#!/usr/bin/env python3
"""
DCAS - è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºå™¨ (ç”Ÿäº§ç‰ˆæœ¬)

æœåŠ¡å™¨éƒ¨ç½²ç‰ˆæœ¬ï¼šå¤„ç†å…¨é‡æ•°æ®ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import os
import json
import pandas as pd
import numpy as np
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import pickle
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import MiniBatchKMeans
import matplotlib
matplotlib.use('Agg')  # æ— GUIç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import gc
import psutil
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionCourseKnowledgeGraphBuilder:
    """ç”Ÿäº§ç¯å¢ƒè¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, course_data_dir: str, output_dir: str = "knowledge_graph_output", 
                 batch_size: int = 100, max_memory_gb: float = 8.0, resume: bool = True):
        """
        åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒæ„å»ºå™¨
        
        Args:
            course_data_dir: è¯¾ç¨‹æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_memory_gb: æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB)
            resume: æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        """
        self.course_data_dir = Path(course_data_dir)
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.max_memory_gb = max_memory_gb
        self.resume = resume
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "embeddings").mkdir(exist_ok=True)
        (self.output_dir / "graphs").mkdir(exist_ok=True)
        (self.output_dir / "analysis").mkdir(exist_ok=True)
        (self.output_dir / "checkpoints").mkdir(exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = self.output_dir / "checkpoints" / "progress.json"
        self.course_cache_file = self.output_dir / "checkpoints" / "courses_cache.pkl"
        self.embedding_cache_file = self.output_dir / "checkpoints" / "embeddings_cache.pkl"
        
        # æ•°æ®å­˜å‚¨
        self.courses = []
        self.embeddings = None
        self.knowledge_graph = None
        self.embedding_model = None
        
        logger.info(f"ç”Ÿäº§ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - æ‰¹å¤§å°: {batch_size}, å†…å­˜é™åˆ¶: {max_memory_gb}GB")
    
    def _check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        memory_gb = process.memory_info().rss / 1024 / 1024 / 1024
        
        if memory_gb > self.max_memory_gb:
            logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_gb:.2f}GB > {self.max_memory_gb}GBï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶")
            gc.collect()
            
        return memory_gb
    
    def _save_checkpoint(self, stage: str, data: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'timestamp': datetime.now().isoformat(),
            'stage': stage,
            'memory_usage_gb': self._check_memory_usage(),
            **data
        }
        
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {stage}")
    
    def _load_checkpoint(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.resume and self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint = json.load(f)
                logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint['stage']} ({checkpoint['timestamp']})")
                return checkpoint
            except Exception as e:
                logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None
    
    def _load_embedding_model(self):
        """åŠ è½½embeddingæ¨¡å‹"""
        if self.embedding_model is not None:
            return True
            
        try:
            logger.info("æ­£åœ¨åŠ è½½Qwen3-Embedding-0.6Bæ¨¡å‹...")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…è­¦å‘Š
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            
            try:
                from transformers import AutoTokenizer, AutoModel
                import torch
                
                model_name = "Qwen/Qwen3-Embedding-0.6B"
                
                # æ£€æŸ¥GPUå¯ç”¨æ€§
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
                
                # åŠ è½½æ¨¡å‹å’Œtokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                self.embedding_model = AutoModel.from_pretrained(
                    model_name, 
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if device == 'cuda' else torch.float32
                ).to(device)
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                self.embedding_model.eval()
                
                logger.info(f"Qwen3-Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
                return True
                
            except ImportError:
                logger.warning("transformersæœªå®‰è£…ï¼Œä½¿ç”¨sentence-transformerså¤‡é€‰æ–¹æ¡ˆ...")
                
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("ä½¿ç”¨SentenceTransformerå¤‡é€‰æ¨¡å‹")
                return True
                    
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_course_data(self) -> List[Dict[str, Any]]:
        """æ‰¹é‡åŠ è½½è¯¾ç¨‹æ•°æ®"""
        checkpoint = self._load_checkpoint()
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»ç¼“å­˜åŠ è½½
        if (checkpoint and checkpoint['stage'] in ['data_loaded', 'embeddings_generated', 'completed'] 
            and self.course_cache_file.exists()):
            try:
                with open(self.course_cache_file, 'rb') as f:
                    self.courses = pickle.load(f)
                logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self.courses)} ä¸ªè¯¾ç¨‹")
                return self.courses
            except Exception as e:
                logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°å¤„ç†: {e}")
        
        logger.info("æ­£åœ¨åŠ è½½è¯¾ç¨‹æ•°æ®...")
        
        course_files = list(self.course_data_dir.glob("*.json"))
        logger.info(f"å‘ç° {len(course_files)} ä¸ªè¯¾ç¨‹æ–‡ä»¶")
        
        courses = []
        processed_count = 0
        
        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        for i in tqdm(range(0, len(course_files), self.batch_size), desc="æ‰¹é‡åŠ è½½è¯¾ç¨‹"):
            batch_files = course_files[i:i+self.batch_size]
            
            for file_path in batch_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        course_data = json.load(f)
                    
                    # æå–å…³é”®ä¿¡æ¯
                    course_info = {
                        'file_name': file_path.stem,
                        'course_name': course_data.get('course_name', ''),
                        'course_description': course_data.get('course_description', ''),
                        'topics': course_data.get('topics', []),
                        'syllabus_content': course_data.get('syllabus_content', ''),
                        'files': course_data.get('files', [])
                    }
                    
                    # åˆ›å»ºç”¨äºembeddingçš„æ–‡æœ¬
                    course_info['embedding_text'] = self._create_embedding_text(course_info)
                    
                    courses.append(course_info)
                    processed_count += 1
                    
                except Exception as e:
                    logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            self._check_memory_usage()
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if processed_count % (self.batch_size * 10) == 0:
                logger.info(f"å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶...")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(courses)} ä¸ªè¯¾ç¨‹")
        self.courses = courses
        
        # ä¿å­˜ç¼“å­˜
        with open(self.course_cache_file, 'wb') as f:
            pickle.dump(courses, f)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        self._save_checkpoint('data_loaded', {'num_courses': len(courses)})
        
        return courses
    
    def _create_embedding_text(self, course_info: Dict[str, Any]) -> str:
        """åˆ›å»ºç”¨äºembeddingçš„ç»¼åˆæ–‡æœ¬"""
        text_parts = []
        
        # è¯¾ç¨‹åç§°ï¼ˆæƒé‡æœ€é«˜ï¼‰
        if course_info['course_name']:
            text_parts.append(f"Course: {course_info['course_name']}")
        
        # ä¸»é¢˜æ ‡ç­¾
        if course_info['topics']:
            unique_topics = list(set(course_info['topics']))
            text_parts.append(f"Topics: {', '.join(unique_topics)}")
        
        # è¯¾ç¨‹æè¿°ï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
        if course_info['course_description']:
            description = course_info['course_description'][:300]
            text_parts.append(f"Description: {description}")
        
        # æ•™å­¦å¤§çº²ï¼ˆæˆªå–å‰500å­—ç¬¦ï¼‰
        if course_info['syllabus_content']:
            syllabus = course_info['syllabus_content'][:500]
            text_parts.append(f"Syllabus: {syllabus}")
        
        return " | ".join(text_parts)
    
    def generate_embeddings(self) -> np.ndarray:
        """æ‰¹é‡ç”Ÿæˆè¯¾ç¨‹åµŒå…¥å‘é‡"""
        checkpoint = self._load_checkpoint()
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»ç¼“å­˜åŠ è½½
        if (checkpoint and checkpoint['stage'] in ['embeddings_generated', 'completed'] 
            and self.embedding_cache_file.exists()):
            try:
                with open(self.embedding_cache_file, 'rb') as f:
                    embeddings_data = pickle.load(f)
                    self.embeddings = embeddings_data['embeddings']
                logger.info(f"ä»ç¼“å­˜åŠ è½½embeddings: {self.embeddings.shape}")
                return self.embeddings
            except Exception as e:
                logger.warning(f"embeddingç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°ç”Ÿæˆ: {e}")
        
        if not self.courses:
            raise ValueError("è¯·å…ˆåŠ è½½è¯¾ç¨‹æ•°æ®")
        
        if not self._load_embedding_model():
            raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
        
        logger.info("æ­£åœ¨æ‰¹é‡ç”Ÿæˆembedding...")
        
        texts = [course['embedding_text'] for course in self.courses]
        
        try:
            # æ‰¹é‡å¤„ç†embedding
            all_embeddings = []
            
            for i in tqdm(range(0, len(texts), self.batch_size), desc="ç”Ÿæˆembeddings"):
                batch_texts = texts[i:i+self.batch_size]
                
                if hasattr(self.embedding_model, 'encode'):
                    # SentenceTransformer
                    batch_embeddings = self.embedding_model.encode(
                        batch_texts, 
                        show_progress_bar=False,
                        batch_size=min(32, len(batch_texts))
                    )
                else:
                    # Transformersæ¨¡å‹
                    batch_embeddings = self._encode_with_transformers(batch_texts)
                
                all_embeddings.append(batch_embeddings)
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                self._check_memory_usage()
            
            # åˆå¹¶æ‰€æœ‰embeddings
            embeddings = np.vstack(all_embeddings)
            self.embeddings = embeddings
            
            # ä¿å­˜embeddingsç¼“å­˜
            embeddings_data = {
                'embeddings': embeddings,
                'metadata': {
                    'model': 'Qwen3-Embedding-0.6B' if not hasattr(self.embedding_model, 'encode') else 'SentenceTransformer',
                    'num_courses': len(self.courses),
                    'embedding_dim': embeddings.shape[1],
                    'created_at': datetime.now().isoformat()
                }
            }
            
            with open(self.embedding_cache_file, 'wb') as f:
                pickle.dump(embeddings_data, f)
            
            # ä¿å­˜åˆ°æœ€ç»ˆè¾“å‡ºç›®å½•
            embedding_file = self.output_dir / "embeddings" / f"course_embeddings_production_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            with open(embedding_file, 'wb') as f:
                pickle.dump({
                    'embeddings': embeddings,
                    'courses': self.courses,
                    **embeddings_data['metadata']
                }, f)
            
            logger.info(f"Embeddingç”Ÿæˆå®Œæˆ: {embeddings.shape}")
            logger.info(f"å·²ä¿å­˜åˆ°: {embedding_file}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self._save_checkpoint('embeddings_generated', {
                'embedding_shape': embeddings.shape,
                'embedding_file': str(embedding_file)
            })
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embeddingç”Ÿæˆå¤±è´¥: {e}")
            raise e
    
    def _encode_with_transformers(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨transformersæ¨¡å‹è¿›è¡Œç¼–ç """
        import torch
        
        embeddings = []
        mini_batch_size = 8  # å‡å°æ‰¹å¤§å°é¿å…å†…å­˜æº¢å‡º
        
        with torch.no_grad():
            for i in range(0, len(texts), mini_batch_size):
                batch_texts = texts[i:i+mini_batch_size]
                
                # åˆ†è¯
                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, 
                                      max_length=512, return_tensors='pt')
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                device = next(self.embedding_model.parameters()).device
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # è·å–embedding
                outputs = self.embedding_model(**inputs)
                
                # ä½¿ç”¨mean pooling
                batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings.extend(batch_embeddings.cpu().numpy())
        
        return np.array(embeddings)
    
    def build_knowledge_graph(self, similarity_threshold: float = 0.85,  # æé«˜é»˜è®¤é˜ˆå€¼
                            max_edges: int = 50000) -> nx.Graph:  # é™ä½è¾¹æ•°é™åˆ¶
        """æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.embeddings is None:
            raise ValueError("è¯·å…ˆç”Ÿæˆembeddings")
        
        logger.info("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œåˆ†å—è®¡ç®—ç›¸ä¼¼åº¦
        num_courses = len(self.courses)
        logger.info(f"è®¡ç®— {num_courses} ä¸ªè¯¾ç¨‹çš„ç›¸ä¼¼åº¦çŸ©é˜µ...")
        
        # åˆ›å»ºå›¾
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for i, course in enumerate(self.courses):
            G.add_node(i, 
                      name=course['course_name'],
                      topics=course['topics'][:5],  # é™åˆ¶ä¸»é¢˜æ•°é‡
                      description=course['course_description'][:100] + "..." if len(course['course_description']) > 100 else course['course_description'])
        
        # åˆ†å—è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ·»åŠ è¾¹
        chunk_size = min(1000, num_courses)  # åŠ¨æ€è°ƒæ•´å—å¤§å°
        edge_count = 0
        max_edges_reached = False
        
        for i in tqdm(range(0, num_courses, chunk_size), desc="è®¡ç®—ç›¸ä¼¼åº¦"):
            if max_edges_reached:
                break
                
            end_i = min(i + chunk_size, num_courses)
            chunk_embeddings_i = self.embeddings[i:end_i]
            
            for j in range(i, num_courses, chunk_size):
                if max_edges_reached:
                    break
                    
                end_j = min(j + chunk_size, num_courses)
                chunk_embeddings_j = self.embeddings[j:end_j]
                
                # è®¡ç®—è¿™ä¸¤ä¸ªå—ä¹‹é—´çš„ç›¸ä¼¼åº¦
                similarity_chunk = cosine_similarity(chunk_embeddings_i, chunk_embeddings_j)
                
                # æ·»åŠ è¾¹
                for ii in range(similarity_chunk.shape[0]):
                    for jj in range(similarity_chunk.shape[1]):
                        global_i = i + ii
                        global_j = j + jj
                        
                        if global_i >= global_j:  # é¿å…é‡å¤å’Œè‡ªè¿æ¥
                            continue
                            
                        similarity = similarity_chunk[ii, jj]
                        if similarity > similarity_threshold:
                            G.add_edge(global_i, global_j, weight=float(similarity))
                            edge_count += 1
                            
                            # é™åˆ¶è¾¹æ•°ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
                            if edge_count >= max_edges:
                                if not max_edges_reached:  # åªåœ¨ç¬¬ä¸€æ¬¡è¾¾åˆ°é™åˆ¶æ—¶è®°å½•è­¦å‘Š
                                    logger.warning(f"è¾¾åˆ°æœ€å¤§è¾¹æ•°é™åˆ¶ {max_edges}ï¼Œåœæ­¢æ·»åŠ è¾¹")
                                max_edges_reached = True
                                break
                    
                    if max_edges_reached:
                        break
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            if not max_edges_reached:
                self._check_memory_usage()
        
        self.knowledge_graph = G
        
        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(G.nodes)} ä¸ªèŠ‚ç‚¹, {edge_count} æ¡è¾¹")
        
        # ä¿å­˜å›¾
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆä¿ç•™å®Œæ•´æ•°æ®ï¼‰
        pickle_file = self.output_dir / "graphs" / f"course_knowledge_graph_production_{timestamp}.pkl"
        with open(pickle_file, 'wb') as f:
            pickle.dump(G, f)
        logger.info(f"å›¾è°±æ•°æ®å·²ä¿å­˜åˆ°: {pickle_file}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        self._save_checkpoint('graph_built', {
            'num_nodes': len(G.nodes),
            'num_edges': len(G.edges),
            'graph_file': str(pickle_file)
        })
        
        return G
    
    def analyze_topics(self) -> Dict[str, Any]:
        """åˆ†æä¸»é¢˜åˆ†å¸ƒå’Œèšç±»ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.embeddings is None:
            raise ValueError("è¯·å…ˆç”Ÿæˆembeddings")
        
        logger.info("æ­£åœ¨è¿›è¡Œä¸»é¢˜åˆ†æ...")
        
        # ä¸»é¢˜ç»Ÿè®¡
        all_topics = []
        for course in self.courses:
            all_topics.extend(course['topics'])
        
        topic_counts = pd.Series(all_topics).value_counts()
        
        # ä½¿ç”¨MiniBatchKMeansè¿›è¡Œèšç±»ï¼ˆå†…å­˜å‹å¥½ï¼‰
        n_clusters = min(20, len(self.courses) // 10)
        if n_clusters > 1:
            kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=100)
            cluster_labels = kmeans.fit_predict(self.embeddings)
            
            # åˆ†ææ¯ä¸ªèšç±»çš„ä¸»è¦ä¸»é¢˜
            cluster_topics = {}
            for i in range(n_clusters):
                cluster_courses = [self.courses[j] for j in range(len(self.courses)) if cluster_labels[j] == i]
                cluster_topic_list = []
                for course in cluster_courses:
                    cluster_topic_list.extend(course['topics'])
                
                if cluster_topic_list:
                    cluster_topics[f"Cluster_{i}"] = pd.Series(cluster_topic_list).value_counts().head(5).to_dict()
        else:
            cluster_labels = np.zeros(len(self.courses))
            cluster_topics = {}
        
        analysis_results = {
            'topic_distribution': topic_counts.head(50).to_dict(),  # å¢åŠ åˆ°50ä¸ªä¸»é¢˜
            'cluster_analysis': cluster_topics,
            'num_clusters': n_clusters,
            'cluster_labels': cluster_labels.tolist()
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.output_dir / "analysis" / f"topic_analysis_production_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¸»é¢˜åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {analysis_file}")
        
        return analysis_results
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ€»ç»“æŠ¥å‘Š"""
        memory_usage = self._check_memory_usage()
        
        report = f"""
# DCAS Course Knowledge Graph - Production Report

## Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Production Environment Stats
- **Processing Mode**: Full Dataset (Production)
- **Total Courses Processed**: {len(self.courses)}
- **Memory Usage**: {memory_usage:.2f} GB
- **Batch Size**: {self.batch_size}

## Embedding Analysis
- **Model Used**: {'Qwen3-Embedding-0.6B' if not hasattr(self.embedding_model, 'encode') else 'SentenceTransformer (fallback)'}
- **Embedding Dimension**: {self.embeddings.shape[1] if self.embeddings is not None else 'N/A'}
- **Processing Batches**: {(len(self.courses) + self.batch_size - 1) // self.batch_size}

## Knowledge Graph Statistics
- **Nodes**: {len(self.knowledge_graph.nodes) if self.knowledge_graph else 'N/A'}
- **Edges**: {len(self.knowledge_graph.edges) if self.knowledge_graph else 'N/A'}
- **Graph Density**: {(nx.density(self.knowledge_graph) if self.knowledge_graph else 0):.6f}

## Performance Metrics
- **Average Text Length**: {np.mean([len(course['embedding_text']) for course in self.courses]):.1f} characters
- **Courses per Batch**: {self.batch_size}
- **Memory Efficiency**: Optimized for large datasets

## Top Course Topics (Production Scale)
"""
        
        # æ·»åŠ ä¸»é¢˜ç»Ÿè®¡
        if self.courses:
            all_topics = []
            for course in self.courses:
                all_topics.extend(course['topics'])
            
            topic_counts = pd.Series(all_topics).value_counts().head(20)
            for topic, count in topic_counts.items():
                report += f"- **{topic}**: {count} courses\n"
        
        report += f"""

## Output Files (Production)
- **Embeddings**: `{self.output_dir}/embeddings/`
- **Knowledge Graph**: `{self.output_dir}/graphs/`
- **Analysis Results**: `{self.output_dir}/analysis/`
- **Checkpoints**: `{self.output_dir}/checkpoints/`

## Deployment Recommendations
1. **Server Requirements**: Minimum 16GB RAM for full dataset
2. **GPU Acceleration**: CUDA-compatible GPU recommended
3. **Storage**: SSD recommended for faster I/O
4. **Monitoring**: Track memory usage during processing

## API Integration Ready
- All outputs saved in pickle format for easy loading
- Embedding vectors can be used for real-time similarity search
- Knowledge graph ready for web visualization

---
*Generated by DCAS Production Course Knowledge Graph Builder*
*Server optimized for datasets of {len(self.courses)} courses*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"production_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ç”Ÿäº§ç¯å¢ƒæŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self._save_checkpoint('completed', {
            'report_file': str(report_file),
            'final_memory_usage': memory_usage
        })
        
        return report

def main():
    """ç”Ÿäº§ç¯å¢ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DCAS Course Knowledge Graph Builder - Production')
    parser.add_argument('--data-dir', default='datasets/Course Details/General', 
                       help='è¯¾ç¨‹æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='knowledge_graph_output_production', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', type=int, default=100, 
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--max-memory', type=float, default=12.0, 
                       help='æœ€å¤§å†…å­˜ä½¿ç”¨(GB)')
    parser.add_argument('--similarity-threshold', type=float, default=0.85, 
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼ (æ¨è: 0.85-0.9)')
    parser.add_argument('--no-resume', action='store_true', 
                       help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ DCAS Course Knowledge Graph Builder - PRODUCTION MODE")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–æ„å»ºå™¨
        builder = ProductionCourseKnowledgeGraphBuilder(
            course_data_dir=args.data_dir,
            output_dir=args.output_dir,
            batch_size=args.batch_size,
            max_memory_gb=args.max_memory,
            resume=not args.no_resume
        )
        
        # 1. åŠ è½½è¯¾ç¨‹æ•°æ®
        print(f"\nğŸ“š Step 1: Loading course data (batch size: {args.batch_size})...")
        courses = builder.load_course_data()
        print(f"âœ… Loaded {len(courses)} courses")
        
        # 2. ç”Ÿæˆembeddings
        print(f"\nğŸ”¢ Step 2: Generating embeddings...")
        embeddings = builder.generate_embeddings()
        print(f"âœ… Generated embeddings: {embeddings.shape}")
        
        # 3. æ„å»ºçŸ¥è¯†å›¾è°±
        print(f"\nğŸ•¸ï¸  Step 3: Building knowledge graph (threshold: {args.similarity_threshold})...")
        knowledge_graph = builder.build_knowledge_graph(similarity_threshold=args.similarity_threshold)
        print(f"âœ… Built graph: {len(knowledge_graph.nodes)} nodes, {len(knowledge_graph.edges)} edges")
        
        # 4. ä¸»é¢˜åˆ†æ
        print(f"\nğŸ“Š Step 4: Analyzing topics...")
        analysis_results = builder.analyze_topics()
        print(f"âœ… Analyzed {len(analysis_results['topic_distribution'])} topics")
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“‹ Step 5: Generating production report...")
        report = builder.generate_summary_report()
        print("âœ… Production report generated")
        
        print(f"\nğŸ‰ Production pipeline completed!")
        print(f"ğŸ“ Results: {args.output_dir}")
        print(f"ğŸ’¾ Memory used: {builder._check_memory_usage():.2f} GB")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        print(f"\nğŸ“ˆ Production Stats:")
        print(f"  - Total courses: {len(courses)}")
        print(f"  - Embedding dims: {embeddings.shape[1]}")
        print(f"  - Graph edges: {len(knowledge_graph.edges)}")
        print(f"  - Processing batches: {(len(courses) + args.batch_size - 1) // args.batch_size}")
        
    except Exception as e:
        logger.error(f"ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()