#!/usr/bin/env python3
"""
DCAS - è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºå™¨

ä½¿ç”¨Qwen3-Embedding-0.6Bå¯¹è¯¾ç¨‹å¤§çº²è¿›è¡ŒåµŒå…¥ï¼Œå¹¶æ„å»ºçŸ¥è¯†å›¾è°±
æ”¯æŒæœ¬åœ°æµ‹è¯•å’ŒæœåŠ¡å™¨éƒ¨ç½²
"""

import os
import json
import pandas as pd
import numpy as np
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import pickle
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CourseKnowledgeGraphBuilder:
    """è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, course_data_dir: str, output_dir: str = "knowledge_graph_output", 
                 test_mode: bool = False, test_sample_size: int = 50):
        """
        åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ„å»ºå™¨
        
        Args:
            course_data_dir: è¯¾ç¨‹æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            test_mode: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
            test_sample_size: æµ‹è¯•æ¨¡å¼ä¸‹çš„æ ·æœ¬æ•°é‡
        """
        self.course_data_dir = Path(course_data_dir)
        self.output_dir = Path(output_dir)
        self.test_mode = test_mode
        self.test_sample_size = test_sample_size
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "embeddings").mkdir(exist_ok=True)
        (self.output_dir / "graphs").mkdir(exist_ok=True)
        (self.output_dir / "analysis").mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.courses = []
        self.embeddings = None
        self.knowledge_graph = None
        self.embedding_model = None
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ - æµ‹è¯•æ¨¡å¼: {test_mode}, æ ·æœ¬æ•°: {test_sample_size if test_mode else 'å…¨éƒ¨'}")
    
    def _load_embedding_model(self):
        """åŠ è½½embeddingæ¨¡å‹"""
        try:
            logger.info("æ­£åœ¨åŠ è½½Qwen3-Embedding-0.6Bæ¨¡å‹...")
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²å®‰è£…transformers
            try:
                from transformers import AutoTokenizer, AutoModel
                import torch
                
                model_name = "Qwen/Qwen3-Embedding-0.6B"
                
                # åŠ è½½æ¨¡å‹å’Œtokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                self.embedding_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                self.embedding_model.eval()
                
                logger.info("Qwen3-Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
                
            except ImportError:
                logger.warning("transformersæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨sentence-transformers...")
                
                # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨sentence-transformers
                try:
                    from sentence_transformers import SentenceTransformer
                    
                    # ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„æ›¿ä»£æ¨¡å‹è¿›è¡Œæµ‹è¯•
                    self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                    logger.info("ä½¿ç”¨SentenceTransformeræ›¿ä»£æ¨¡å‹")
                    return True
                    
                except ImportError:
                    logger.error("è¯·å®‰è£…transformersæˆ–sentence-transformersåº“")
                    return False
                    
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_course_data(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½è¯¾ç¨‹æ•°æ®
        
        Returns:
            List[Dict]: è¯¾ç¨‹æ•°æ®åˆ—è¡¨
        """
        logger.info("æ­£åœ¨åŠ è½½è¯¾ç¨‹æ•°æ®...")
        
        course_files = list(self.course_data_dir.glob("*.json"))
        
        if self.test_mode:
            course_files = course_files[:self.test_sample_size]
            logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šåŠ è½½å‰ {len(course_files)} ä¸ªæ–‡ä»¶")
        
        courses = []
        
        for file_path in tqdm(course_files, desc="åŠ è½½è¯¾ç¨‹æ–‡ä»¶"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    course_data = json.load(f)
                
                # æå–å…³é”®ä¿¡æ¯
                course_info = {
                    'file_name': file_path.stem,
                    'course_name': course_data.get('course_name', ''),
                    'course_description': course_data.get('course_description', ''),
                    'topics': course_data.get('topics', []),
                    'syllabus_content': course_data.get('syllabus_content', ''),
                    'files': course_data.get('files', [])
                }
                
                # åˆ›å»ºç”¨äºembeddingçš„æ–‡æœ¬
                course_info['embedding_text'] = self._create_embedding_text(course_info)
                
                courses.append(course_info)
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                continue
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(courses)} ä¸ªè¯¾ç¨‹")
        self.courses = courses
        return courses
    
    def _create_embedding_text(self, course_info: Dict[str, Any]) -> str:
        """
        åˆ›å»ºç”¨äºembeddingçš„ç»¼åˆæ–‡æœ¬
        
        Args:
            course_info: è¯¾ç¨‹ä¿¡æ¯
            
        Returns:
            str: ç»¼åˆæ–‡æœ¬
        """
        text_parts = []
        
        # è¯¾ç¨‹åç§°ï¼ˆæƒé‡æœ€é«˜ï¼‰
        if course_info['course_name']:
            text_parts.append(f"Course: {course_info['course_name']}")
        
        # ä¸»é¢˜æ ‡ç­¾
        if course_info['topics']:
            unique_topics = list(set(course_info['topics']))  # å»é‡
            text_parts.append(f"Topics: {', '.join(unique_topics)}")
        
        # è¯¾ç¨‹æè¿°
        if course_info['course_description']:
            # æˆªå–å‰500å­—ç¬¦ï¼Œé¿å…æ–‡æœ¬è¿‡é•¿
            description = course_info['course_description'][:500]
            text_parts.append(f"Description: {description}")
        
        # æ•™å­¦å¤§çº²ï¼ˆå…³é”®éƒ¨åˆ†ï¼‰
        if course_info['syllabus_content']:
            # æå–æ•™å­¦å¤§çº²çš„å…³é”®éƒ¨åˆ†ï¼Œé¿å…è¿‡é•¿
            syllabus = course_info['syllabus_content'][:800]
            text_parts.append(f"Syllabus: {syllabus}")
        
        return " | ".join(text_parts)
    
    def generate_embeddings(self) -> np.ndarray:
        """
        ç”Ÿæˆè¯¾ç¨‹åµŒå…¥å‘é‡
        
        Returns:
            np.ndarray: åµŒå…¥çŸ©é˜µ
        """
        if not self.courses:
            raise ValueError("è¯·å…ˆåŠ è½½è¯¾ç¨‹æ•°æ®")
        
        if not self.embedding_model:
            if not self._load_embedding_model():
                raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
        
        logger.info("æ­£åœ¨ç”Ÿæˆembedding...")
        
        texts = [course['embedding_text'] for course in self.courses]
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„embeddingæ–¹æ³•
            if hasattr(self.embedding_model, 'encode'):
                # SentenceTransformer
                embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
            else:
                # Transformersæ¨¡å‹
                embeddings = self._encode_with_transformers(texts)
            
            self.embeddings = embeddings
            
            # ä¿å­˜embeddings
            embedding_file = self.output_dir / "embeddings" / f"course_embeddings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            with open(embedding_file, 'wb') as f:
                pickle.dump({
                    'embeddings': embeddings,
                    'courses': self.courses,
                    'metadata': {
                        'model': 'Qwen3-Embedding-0.6B' if not hasattr(self.embedding_model, 'encode') else 'SentenceTransformer',
                        'num_courses': len(self.courses),
                        'embedding_dim': embeddings.shape[1],
                        'created_at': datetime.now().isoformat()
                    }
                }, f)
            
            logger.info(f"Embeddingç”Ÿæˆå®Œæˆ: {embeddings.shape}")
            logger.info(f"å·²ä¿å­˜åˆ°: {embedding_file}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embeddingç”Ÿæˆå¤±è´¥: {e}")
            raise e
    
    def _encode_with_transformers(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨transformersæ¨¡å‹è¿›è¡Œç¼–ç """
        import torch
        
        embeddings = []
        batch_size = 8  # é¿å…å†…å­˜ä¸è¶³
        
        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size), desc="ç”Ÿæˆembeddings"):
                batch_texts = texts[i:i+batch_size]
                
                # åˆ†è¯
                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, 
                                      max_length=512, return_tensors='pt')
                
                # è·å–embedding
                outputs = self.embedding_model(**inputs)
                
                # ä½¿ç”¨mean pooling
                batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings.extend(batch_embeddings.cpu().numpy())
        
        return np.array(embeddings)
    
    def build_knowledge_graph(self, similarity_threshold: float = 0.7) -> nx.Graph:
        """
        æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            nx.Graph: çŸ¥è¯†å›¾è°±
        """
        if self.embeddings is None:
            raise ValueError("è¯·å…ˆç”Ÿæˆembeddings")
        
        logger.info("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(self.embeddings)
        
        # åˆ›å»ºå›¾
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for i, course in enumerate(self.courses):
            G.add_node(i, 
                      name=course['course_name'],
                      topics=course['topics'],
                      description=course['course_description'][:100] + "..." if len(course['course_description']) > 100 else course['course_description'])
        
        # æ·»åŠ è¾¹ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
        num_edges = 0
        for i in range(len(self.courses)):
            for j in range(i+1, len(self.courses)):
                similarity = similarity_matrix[i][j]
                if similarity > similarity_threshold:
                    G.add_edge(i, j, weight=similarity)
                    num_edges += 1
        
        self.knowledge_graph = G
        
        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(G.nodes)} ä¸ªèŠ‚ç‚¹, {num_edges} æ¡è¾¹")
        
        # ä¿å­˜å›¾
        graph_file = self.output_dir / "graphs" / f"course_knowledge_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.gml"
        
        # ä¸ºGMLæ ¼å¼è½¬æ¢æƒé‡ä¸ºå­—ç¬¦ä¸²
        G_copy = G.copy()
        for u, v, d in G_copy.edges(data=True):
            if 'weight' in d:
                d['weight'] = str(d['weight'])
        
        nx.write_gml(G_copy, graph_file)
        logger.info(f"å›¾è°±å·²ä¿å­˜åˆ°: {graph_file}")
        
        # åŒæ—¶ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆä¿ç•™å®Œæ•´æ•°æ®ï¼‰
        pickle_file = self.output_dir / "graphs" / f"course_knowledge_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        with open(pickle_file, 'wb') as f:
            pickle.dump(G, f)
        logger.info(f"å›¾è°±æ•°æ®å·²ä¿å­˜åˆ°: {pickle_file}")
        
        return G
    
    def analyze_topics(self) -> Dict[str, Any]:
        """
        åˆ†æä¸»é¢˜åˆ†å¸ƒå’Œèšç±»
        
        Returns:
            Dict: åˆ†æç»“æœ
        """
        if self.embeddings is None:
            raise ValueError("è¯·å…ˆç”Ÿæˆembeddings")
        
        logger.info("æ­£åœ¨è¿›è¡Œä¸»é¢˜åˆ†æ...")
        
        # ä¸»é¢˜ç»Ÿè®¡
        all_topics = []
        for course in self.courses:
            all_topics.extend(course['topics'])
        
        topic_counts = pd.Series(all_topics).value_counts()
        
        # K-meansèšç±»
        n_clusters = min(10, len(self.courses) // 5)  # åŠ¨æ€ç¡®å®šèšç±»æ•°
        if n_clusters > 1:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(self.embeddings)
            
            # åˆ†ææ¯ä¸ªèšç±»çš„ä¸»è¦ä¸»é¢˜
            cluster_topics = {}
            for i in range(n_clusters):
                cluster_courses = [self.courses[j] for j in range(len(self.courses)) if cluster_labels[j] == i]
                cluster_topic_list = []
                for course in cluster_courses:
                    cluster_topic_list.extend(course['topics'])
                
                if cluster_topic_list:
                    cluster_topics[f"Cluster_{i}"] = pd.Series(cluster_topic_list).value_counts().head(5).to_dict()
        else:
            cluster_labels = np.zeros(len(self.courses))
            cluster_topics = {}
        
        analysis_results = {
            'topic_distribution': topic_counts.head(20).to_dict(),
            'cluster_analysis': cluster_topics,
            'num_clusters': n_clusters,
            'cluster_labels': cluster_labels.tolist()
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.output_dir / "analysis" / f"topic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¸»é¢˜åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {analysis_file}")
        
        return analysis_results
    
    def visualize_knowledge_graph(self, max_nodes: int = 100):
        """
        å¯è§†åŒ–çŸ¥è¯†å›¾è°±
        
        Args:
            max_nodes: æœ€å¤§æ˜¾ç¤ºèŠ‚ç‚¹æ•°
        """
        if self.knowledge_graph is None:
            raise ValueError("è¯·å…ˆæ„å»ºçŸ¥è¯†å›¾è°±")
        
        logger.info("æ­£åœ¨ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ–...")
        
        G = self.knowledge_graph
        
        # å¦‚æœèŠ‚ç‚¹å¤ªå¤šï¼Œé€‰æ‹©è¿æ¥åº¦æœ€é«˜çš„èŠ‚ç‚¹
        if len(G.nodes) > max_nodes:
            degrees = dict(G.degree())
            top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:max_nodes]
            G = G.subgraph([node for node, _ in top_nodes])
        
        plt.figure(figsize=(15, 10))
        
        # ä½¿ç”¨spring layout
        pos = nx.spring_layout(G, k=1, iterations=50)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        node_sizes = [300 + G.degree(node) * 50 for node in G.nodes()]
        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.7)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, alpha=0.3, width=0.5)
        
        # æ·»åŠ æ ‡ç­¾ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†ï¼‰
        if len(G.nodes) <= 50:
            labels = {node: G.nodes[node]['name'][:20] + "..." if len(G.nodes[node]['name']) > 20 else G.nodes[node]['name'] 
                     for node in G.nodes()}
            nx.draw_networkx_labels(G, pos, labels, font_size=8)
        
        plt.title("Course Knowledge Graph", fontsize=16)
        plt.axis('off')
        
        # ä¿å­˜å›¾ç‰‡
        viz_file = self.output_dir / "graphs" / f"knowledge_graph_viz_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è§†åŒ–å›¾ç‰‡ä¿å­˜åˆ°: {viz_file}")
    
    def visualize_topic_analysis(self, analysis_results: Dict[str, Any]):
        """
        å¯è§†åŒ–ä¸»é¢˜åˆ†æç»“æœ
        
        Args:
            analysis_results: åˆ†æç»“æœ
        """
        logger.info("æ­£åœ¨ç”Ÿæˆä¸»é¢˜åˆ†æå¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ä¸»é¢˜åˆ†å¸ƒæ¡å½¢å›¾
        topic_dist = analysis_results['topic_distribution']
        topics = list(topic_dist.keys())[:15]  # æ˜¾ç¤ºå‰15ä¸ª
        counts = [topic_dist[topic] for topic in topics]
        
        axes[0,0].barh(range(len(topics)), counts)
        axes[0,0].set_yticks(range(len(topics)))
        axes[0,0].set_yticklabels(topics, fontsize=8)
        axes[0,0].set_title('Top Topics Distribution')
        axes[0,0].set_xlabel('Count')
        
        # 2. èšç±»åˆ†æ
        if self.embeddings is not None and len(analysis_results['cluster_labels']) > 1:
            cluster_labels = np.array(analysis_results['cluster_labels'])
            
            # ä½¿ç”¨t-SNEé™ç»´å¯è§†åŒ–ï¼ˆå¦‚æœæ•°æ®é‡ä¸å¤§ï¼‰
            if len(self.embeddings) <= 1000:
                try:
                    from sklearn.manifold import TSNE
                    # è°ƒæ•´perplexityå‚æ•°ï¼Œç¡®ä¿å°äºæ ·æœ¬æ•°
                    perplexity = min(30, len(self.embeddings) - 1, 5)
                    if perplexity >= 2:
                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
                        embeddings_2d = tsne.fit_transform(self.embeddings)
                        
                        scatter = axes[0,1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                                                  c=cluster_labels, cmap='tab10', alpha=0.7)
                        axes[0,1].set_title('Course Clusters (t-SNE)')
                        plt.colorbar(scatter, ax=axes[0,1])
                    else:
                        axes[0,1].text(0.5, 0.5, 'Too few samples for t-SNE visualization', 
                                      ha='center', va='center', transform=axes[0,1].transAxes)
                except ImportError:
                    axes[0,1].text(0.5, 0.5, 'sklearn.manifold.TSNE not available', 
                                  ha='center', va='center', transform=axes[0,1].transAxes)
        
        # 3. èšç±»ä¸»é¢˜è¯äº‘ï¼ˆæ–‡æœ¬å½¢å¼ï¼‰
        cluster_analysis = analysis_results['cluster_analysis']
        if cluster_analysis:
            cluster_text = ""
            for cluster, topics in list(cluster_analysis.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ªèšç±»
                cluster_text += f"{cluster}:\n"
                for topic, count in list(topics.items())[:3]:  # æ¯ä¸ªèšç±»æ˜¾ç¤ºå‰3ä¸ªä¸»é¢˜
                    cluster_text += f"  {topic}: {count}\n"
                cluster_text += "\n"
            
            axes[1,0].text(0.05, 0.95, cluster_text, fontsize=10, verticalalignment='top',
                          transform=axes[1,0].transAxes, fontfamily='monospace')
            axes[1,0].set_title('Cluster Topic Analysis')
            axes[1,0].axis('off')
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"""
Dataset Statistics:
- Total Courses: {len(self.courses)}
- Unique Topics: {len(analysis_results['topic_distribution'])}
- Number of Clusters: {analysis_results['num_clusters']}
- Embedding Dimension: {self.embeddings.shape[1] if self.embeddings is not None else 'N/A'}
- Graph Nodes: {len(self.knowledge_graph.nodes) if self.knowledge_graph else 'N/A'}
- Graph Edges: {len(self.knowledge_graph.edges) if self.knowledge_graph else 'N/A'}
"""
        
        axes[1,1].text(0.05, 0.95, stats_text, fontsize=12, verticalalignment='top',
                      transform=axes[1,1].transAxes, fontfamily='monospace')
        axes[1,1].set_title('Dataset Statistics')
        axes[1,1].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        viz_file = self.output_dir / "analysis" / f"topic_analysis_viz_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ä¸»é¢˜åˆ†æå¯è§†åŒ–ä¿å­˜åˆ°: {viz_file}")
    
    def generate_summary_report(self) -> str:
        """
        ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        report = f"""
# Course Knowledge Graph Analysis Report

## Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Dataset Overview
- **Total Courses Processed**: {len(self.courses)}
- **Test Mode**: {'Yes' if self.test_mode else 'No'}
- **Sample Size**: {self.test_sample_size if self.test_mode else 'All'}

## Embedding Analysis
- **Model Used**: {'Qwen3-Embedding-0.6B' if not hasattr(self.embedding_model, 'encode') else 'SentenceTransformer (fallback)'}
- **Embedding Dimension**: {self.embeddings.shape[1] if self.embeddings is not None else 'N/A'}
- **Average Text Length**: {np.mean([len(course['embedding_text']) for course in self.courses]):.1f} characters

## Knowledge Graph Statistics
- **Nodes**: {len(self.knowledge_graph.nodes) if self.knowledge_graph else 'N/A'}
- **Edges**: {len(self.knowledge_graph.edges) if self.knowledge_graph else 'N/A'}
- **Average Degree**: {np.mean(list(dict(self.knowledge_graph.degree()).values())) if self.knowledge_graph else 'N/A'}
- **Graph Density**: {nx.density(self.knowledge_graph) if self.knowledge_graph else 'N/A'}

## Top Course Topics
"""
        
        # æ·»åŠ ä¸»é¢˜ç»Ÿè®¡
        if self.courses:
            all_topics = []
            for course in self.courses:
                all_topics.extend(course['topics'])
            
            topic_counts = pd.Series(all_topics).value_counts().head(10)
            for topic, count in topic_counts.items():
                report += f"- **{topic}**: {count} courses\n"
        
        report += f"""

## Output Files Generated
- Embeddings: `{self.output_dir}/embeddings/`
- Knowledge Graph: `{self.output_dir}/graphs/`
- Analysis Results: `{self.output_dir}/analysis/`

## Next Steps
1. Review the knowledge graph visualization
2. Analyze topic clusters for curriculum alignment
3. Use similarity scores for course recommendation
4. Export data for further analysis or web interface

---
*Generated by DCAS Course Knowledge Graph Builder*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"summary_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    course_data_dir = "datasets/Course Details/General"
    output_dir = "knowledge_graph_output"
    
    # æ£€æµ‹æ˜¯å¦ä¸ºæœ¬åœ°æµ‹è¯•ç¯å¢ƒ
    test_mode = True  # æœ¬åœ°æµ‹è¯•ï¼Œé‡‡æ ·å°‘é‡æ•°æ®
    test_sample_size = 20  # æµ‹è¯•æ ·æœ¬æ•°é‡
    
    print("=" * 60)
    print("ğŸ“ DCAS Course Knowledge Graph Builder")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ„å»ºå™¨
        builder = CourseKnowledgeGraphBuilder(
            course_data_dir=course_data_dir,
            output_dir=output_dir,
            test_mode=test_mode,
            test_sample_size=test_sample_size
        )
        
        # 1. åŠ è½½è¯¾ç¨‹æ•°æ®
        print("\nğŸ“š Step 1: Loading course data...")
        courses = builder.load_course_data()
        
        if not courses:
            logger.error("æ²¡æœ‰åŠ è½½åˆ°è¯¾ç¨‹æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return
        
        print(f"âœ… Loaded {len(courses)} courses")
        
        # 2. ç”Ÿæˆembeddings
        print("\nğŸ”¢ Step 2: Generating embeddings...")
        embeddings = builder.generate_embeddings()
        print(f"âœ… Generated embeddings with shape: {embeddings.shape}")
        
        # 3. æ„å»ºçŸ¥è¯†å›¾è°±
        print("\nğŸ•¸ï¸  Step 3: Building knowledge graph...")
        knowledge_graph = builder.build_knowledge_graph(similarity_threshold=0.6)
        print(f"âœ… Built graph with {len(knowledge_graph.nodes)} nodes and {len(knowledge_graph.edges)} edges")
        
        # 4. ä¸»é¢˜åˆ†æ
        print("\nğŸ“Š Step 4: Analyzing topics...")
        analysis_results = builder.analyze_topics()
        print(f"âœ… Identified {len(analysis_results['topic_distribution'])} unique topics")
        
        # 5. ç”Ÿæˆå¯è§†åŒ–
        print("\nğŸ¨ Step 5: Generating visualizations...")
        builder.visualize_knowledge_graph()
        builder.visualize_topic_analysis(analysis_results)
        print("âœ… Visualizations generated")
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“‹ Step 6: Generating summary report...")
        report = builder.generate_summary_report()
        print("âœ… Summary report generated")
        
        print(f"\nğŸ‰ All done! Results saved to: {output_dir}")
        
        # æ˜¾ç¤ºä¸»è¦ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“ˆ Quick Stats:")
        print(f"  - Courses processed: {len(courses)}")
        print(f"  - Embedding dimensions: {embeddings.shape[1]}")
        print(f"  - Graph connections: {len(knowledge_graph.edges)}")
        print(f"  - Top topics: {list(analysis_results['topic_distribution'].keys())[:3]}")
        
        if test_mode:
            print(f"\nâš ï¸  Note: Running in test mode with {test_sample_size} samples")
            print("   To process all data, set test_mode=False and run on server")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()