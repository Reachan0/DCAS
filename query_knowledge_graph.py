#!/usr/bin/env python3
"""
DCAS è¯¾ç¨‹çŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·

ç”¨äºåŠ è½½å’ŒæŸ¥è¯¢å·²ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±æ•°æ®
"""

import pickle
import json
import networkx as nx
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any, Tuple

class CourseKnowledgeGraphQuery:
    """è¯¾ç¨‹çŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self, output_dir: str = None):
        """
        åˆå§‹åŒ–æŸ¥è¯¢å·¥å…·
        
        Args:
            output_dir: çŸ¥è¯†å›¾è°±è¾“å‡ºç›®å½•ï¼ŒNoneæ—¶è‡ªåŠ¨æ£€æµ‹
        """
        # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„è¾“å‡ºç›®å½•
        if output_dir is None:
            possible_dirs = [
                "knowledge_graph_output_production",
                "knowledge_graph_output"
            ]
            
            for dir_name in possible_dirs:
                if Path(dir_name).exists():
                    output_dir = dir_name
                    break
            
            if output_dir is None:
                raise ValueError("æ‰¾ä¸åˆ°çŸ¥è¯†å›¾è°±è¾“å‡ºç›®å½•")
        
        self.output_dir = Path(output_dir)
        self.graph = None
        self.embeddings = None
        self.courses = []
        self.metadata = {}
        
        print(f"ğŸ“ ä½¿ç”¨è¾“å‡ºç›®å½•: {self.output_dir}")
        self._load_latest_data()
    
    def _load_latest_data(self):
        """åŠ è½½æœ€æ–°çš„çŸ¥è¯†å›¾è°±æ•°æ®"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„å›¾è°±æ–‡ä»¶
            graph_files = list((self.output_dir / "graphs").glob("*.pkl"))
            if graph_files:
                latest_graph_file = max(graph_files, key=lambda x: x.stat().st_mtime)
                with open(latest_graph_file, 'rb') as f:
                    self.graph = pickle.load(f)
                print(f"âœ… åŠ è½½çŸ¥è¯†å›¾è°±: {latest_graph_file.name}")
            
            # æŸ¥æ‰¾æœ€æ–°çš„embeddingæ–‡ä»¶
            embedding_files = list((self.output_dir / "embeddings").glob("*.pkl"))
            if embedding_files:
                latest_embedding_file = max(embedding_files, key=lambda x: x.stat().st_mtime)
                with open(latest_embedding_file, 'rb') as f:
                    data = pickle.load(f)
                    self.embeddings = data['embeddings']
                    self.courses = data['courses']
                    self.metadata = {k: v for k, v in data.items() if k not in ['embeddings', 'courses']}
                print(f"âœ… åŠ è½½embeddings: {latest_embedding_file.name}")
                print(f"   - è¯¾ç¨‹æ•°é‡: {len(self.courses)}")
                print(f"   - å‘é‡ç»´åº¦: {self.embeddings.shape[1]}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def search_courses_by_keyword(self, keyword: str, top_k: int = 10) -> List[Dict]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢è¯¾ç¨‹
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: åŒ¹é…çš„è¯¾ç¨‹åˆ—è¡¨
        """
        results = []
        keyword_lower = keyword.lower()
        
        for i, course in enumerate(self.courses):
            score = 0
            
            # åœ¨è¯¾ç¨‹åç§°ä¸­æœç´¢
            if keyword_lower in course['course_name'].lower():
                score += 10
            
            # åœ¨æè¿°ä¸­æœç´¢
            if keyword_lower in course['course_description'].lower():
                score += 5
            
            # åœ¨ä¸»é¢˜ä¸­æœç´¢
            for topic in course['topics']:
                if keyword_lower in topic.lower():
                    score += 3
            
            if score > 0:
                results.append({
                    'course_index': i,
                    'course_name': course['course_name'],
                    'score': score,
                    'topics': course['topics'],
                    'description': course['course_description'][:200] + "..." if len(course['course_description']) > 200 else course['course_description']
                })
        
        # æŒ‰å¾—åˆ†æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def find_similar_courses(self, course_name: str, top_k: int = 5) -> List[Dict]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼è¯¾ç¨‹
        
        Args:
            course_name: ç›®æ ‡è¯¾ç¨‹åç§°
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: ç›¸ä¼¼è¯¾ç¨‹åˆ—è¡¨
        """
        if self.embeddings is None:
            return []
        
        # æŸ¥æ‰¾ç›®æ ‡è¯¾ç¨‹
        target_idx = None
        for i, course in enumerate(self.courses):
            if course_name.lower() in course['course_name'].lower():
                target_idx = i
                break
        
        if target_idx is None:
            print(f"âŒ æœªæ‰¾åˆ°è¯¾ç¨‹: {course_name}")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity([self.embeddings[target_idx]], self.embeddings)[0]
        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # æ’é™¤è‡ªå·±
        
        results = []
        for idx in similar_indices:
            results.append({
                'course_index': idx,
                'course_name': self.courses[idx]['course_name'],
                'similarity': float(similarities[idx]),
                'topics': self.courses[idx]['topics'],
                'description': self.courses[idx]['course_description'][:200] + "..." if len(self.courses[idx]['course_description']) > 200 else self.courses[idx]['course_description']
            })
        
        return results
    
    def analyze_course_connections(self, course_name: str) -> Dict[str, Any]:
        """
        åˆ†æè¯¾ç¨‹åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„è¿æ¥æƒ…å†µ
        
        Args:
            course_name: è¯¾ç¨‹åç§°
            
        Returns:
            Dict: è¿æ¥åˆ†æç»“æœ
        """
        if self.graph is None:
            return {}
        
        # æŸ¥æ‰¾è¯¾ç¨‹èŠ‚ç‚¹
        target_node = None
        for node, data in self.graph.nodes(data=True):
            if course_name.lower() in data['name'].lower():
                target_node = node
                break
        
        if target_node is None:
            return {'error': f'æœªæ‰¾åˆ°è¯¾ç¨‹: {course_name}'}
        
        # è·å–è¿æ¥ä¿¡æ¯
        neighbors = list(self.graph.neighbors(target_node))
        edges = [(target_node, neighbor, self.graph[target_node][neighbor]) for neighbor in neighbors]
        
        # æŒ‰æƒé‡æ’åº
        edges.sort(key=lambda x: x[2]['weight'], reverse=True)
        
        connected_courses = []
        for _, neighbor, edge_data in edges[:10]:  # æ˜¾ç¤ºå‰10ä¸ªè¿æ¥
            neighbor_data = self.graph.nodes[neighbor]
            connected_courses.append({
                'course_name': neighbor_data['name'],
                'similarity': float(edge_data['weight']),
                'topics': neighbor_data['topics']
            })
        
        return {
            'course_name': self.graph.nodes[target_node]['name'],
            'total_connections': len(neighbors),
            'degree_centrality': nx.degree_centrality(self.graph)[target_node],
            'betweenness_centrality': nx.betweenness_centrality(self.graph)[target_node],
            'connected_courses': connected_courses
        }
    
    def get_topic_statistics(self) -> Dict[str, Any]:
        """è·å–ä¸»é¢˜ç»Ÿè®¡ä¿¡æ¯"""
        all_topics = []
        for course in self.courses:
            all_topics.extend(course['topics'])
        
        topic_counts = pd.Series(all_topics).value_counts()
        
        return {
            'total_unique_topics': len(topic_counts),
            'top_topics': topic_counts.head(20).to_dict(),
            'topic_distribution': {
                'mean': topic_counts.mean(),
                'std': topic_counts.std(),
                'min': topic_counts.min(),
                'max': topic_counts.max()
            }
        }
    
    def export_graph_summary(self, output_file: str = "graph_summary.json"):
        """å¯¼å‡ºå›¾è°±æ‘˜è¦ä¿¡æ¯"""
        if self.graph is None:
            return
        
        summary = {
            'metadata': self.metadata,
            'graph_statistics': {
                'num_nodes': len(self.graph.nodes),
                'num_edges': len(self.graph.edges),
                'density': nx.density(self.graph),
                'average_degree': sum(dict(self.graph.degree()).values()) / len(self.graph.nodes),
                'is_connected': nx.is_connected(self.graph),
                'number_of_components': nx.number_connected_components(self.graph)
            },
            'topic_statistics': self.get_topic_statistics(),
            'top_connected_courses': []
        }
        
        # æ·»åŠ è¿æ¥åº¦æœ€é«˜çš„è¯¾ç¨‹
        degree_dict = dict(self.graph.degree())
        top_courses = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]
        
        for node, degree in top_courses:
            course_data = self.graph.nodes[node]
            summary['top_connected_courses'].append({
                'course_name': course_data['name'],
                'degree': degree,
                'topics': course_data['topics']
            })
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å›¾è°±æ‘˜è¦å·²å¯¼å‡ºåˆ°: {output_path}")
        return summary

def main():
    """äº¤äº’å¼æŸ¥è¯¢å·¥å…·"""
    print("ğŸ” DCAS è¯¾ç¨‹çŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–æŸ¥è¯¢å·¥å…·
    query_tool = CourseKnowledgeGraphQuery()
    
    if not query_tool.courses:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ„å»ºè„šæœ¬")
        return
    
    while True:
        print("\nğŸ“‹ å¯ç”¨æ“ä½œ:")
        print("1. å…³é”®è¯æœç´¢è¯¾ç¨‹")
        print("2. æŸ¥æ‰¾ç›¸ä¼¼è¯¾ç¨‹")
        print("3. åˆ†æè¯¾ç¨‹è¿æ¥")
        print("4. æŸ¥çœ‹ä¸»é¢˜ç»Ÿè®¡")
        print("5. å¯¼å‡ºå›¾è°±æ‘˜è¦")
        print("6. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-6): ").strip()
        
        if choice == '1':
            keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
            results = query_tool.search_courses_by_keyword(keyword, top_k=10)
            
            if results:
                print(f"\nğŸ” æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³è¯¾ç¨‹:")
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. {result['course_name']} (å¾—åˆ†: {result['score']})")
                    print(f"   ä¸»é¢˜: {', '.join(result['topics'][:3])}")
                    print(f"   æè¿°: {result['description']}")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¯¾ç¨‹")
        
        elif choice == '2':
            course_name = input("è¯·è¾“å…¥è¯¾ç¨‹åç§°ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰: ").strip()
            results = query_tool.find_similar_courses(course_name, top_k=5)
            
            if results:
                print(f"\nğŸ¯ ç›¸ä¼¼è¯¾ç¨‹:")
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. {result['course_name']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
                    print(f"   ä¸»é¢˜: {', '.join(result['topics'][:3])}")
        
        elif choice == '3':
            course_name = input("è¯·è¾“å…¥è¯¾ç¨‹åç§°: ").strip()
            analysis = query_tool.analyze_course_connections(course_name)
            
            if 'error' in analysis:
                print(f"âŒ {analysis['error']}")
            else:
                print(f"\nğŸ•¸ï¸  è¯¾ç¨‹è¿æ¥åˆ†æ: {analysis['course_name']}")
                print(f"æ€»è¿æ¥æ•°: {analysis['total_connections']}")
                print(f"åº¦ä¸­å¿ƒæ€§: {analysis['degree_centrality']:.3f}")
                print(f"ä»‹æ•°ä¸­å¿ƒæ€§: {analysis['betweenness_centrality']:.3f}")
                
                print(f"\nå‰10ä¸ªç›¸å…³è¯¾ç¨‹:")
                for i, course in enumerate(analysis['connected_courses'], 1):
                    print(f"{i}. {course['course_name']} (ç›¸ä¼¼åº¦: {course['similarity']:.3f})")
        
        elif choice == '4':
            stats = query_tool.get_topic_statistics()
            print(f"\nğŸ“Š ä¸»é¢˜ç»Ÿè®¡:")
            print(f"æ€»ä¸»é¢˜æ•°: {stats['total_unique_topics']}")
            print(f"å¹³å‡å‡ºç°æ¬¡æ•°: {stats['topic_distribution']['mean']:.1f}")
            
            print(f"\nå‰20ä¸ªçƒ­é—¨ä¸»é¢˜:")
            for i, (topic, count) in enumerate(list(stats['top_topics'].items())[:20], 1):
                print(f"{i:2d}. {topic}: {count}")
        
        elif choice == '5':
            summary = query_tool.export_graph_summary()
            print(f"\nğŸ“„ å›¾è°±æ‘˜è¦:")
            print(f"è¯¾ç¨‹èŠ‚ç‚¹: {summary['graph_statistics']['num_nodes']}")
            print(f"è¿æ¥è¾¹æ•°: {summary['graph_statistics']['num_edges']}")
            print(f"å›¾è°±å¯†åº¦: {summary['graph_statistics']['density']:.3f}")
        
        elif choice == '6':
            print("ğŸ‘‹ å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    main()