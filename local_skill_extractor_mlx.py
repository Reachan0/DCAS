#!/usr/bin/env python3
"""
DCAS - èŒä½èƒ½åŠ›è¦æ±‚æå–å™¨ï¼ˆMLXç‰ˆæœ¬ï¼‰

ä¸“é—¨ä½¿ç”¨mlx_lmåº“åŠ è½½Qwen/Qwen3-30B-A3B-MLX-4bitæ¨¡å‹
æ ¹æ®å®˜æ–¹æ–‡æ¡£æ­£ç¡®åŠ è½½å’Œä½¿ç”¨MLXé‡åŒ–æ¨¡å‹
"""

import pandas as pd
import time
import os
import logging
from datetime import datetime
import threading
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import random

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LocalMLXSkillExtractor:
    """ä½¿ç”¨MLXæ¨¡å‹çš„æœ¬åœ°æŠ€èƒ½æå–å™¨"""
    
    def __init__(self, max_workers: int = 2):
        """
        åˆå§‹åŒ–æœ¬åœ°MLXæ¨¡å‹æå–å™¨
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆMLXæ¨¡å‹å»ºè®®è¾ƒå°‘ï¼‰
        """
        self.progress_file = "datasets/progress.json"
        self.temp_output_file = "datasets/temp_results.csv"
        self.max_workers = max_workers
        self.results_lock = threading.Lock()
        self.progress_lock = threading.Lock()
        self.processed_count = 0
        self.model = None
        self.tokenizer = None
        self.generate_func = None
        self.model_name = None
        
        # åˆå§‹åŒ–MLXæ¨¡å‹
        self._load_mlx_model()
    
    def _load_mlx_model(self):
        """åŠ è½½MLXæ¨¡å‹"""
        try:
            logger.info("æ­£åœ¨åŠ è½½Qwen3-30B-A3B-MLX-4bitæ¨¡å‹...")
            
            # è®¾ç½®MLXç¯å¢ƒå˜é‡
            os.environ['MLXLM_USE_MODELSCOPE'] = 'True'
            
            # ä½¿ç”¨mlx_lmåŠ è½½æ¨¡å‹
            try:
                from mlx_lm import load, generate
                
                model_name = "Qwen/Qwen3-30B-A3B-MLX-4bit"
                logger.info(f"æ­£åœ¨ä½¿ç”¨mlx_lmåŠ è½½æ¨¡å‹: {model_name}")
                
                # åŠ è½½æ¨¡å‹å’Œtokenizer
                self.model, self.tokenizer = load(model_name)
                self.model_name = model_name
                self.generate_func = generate  # ä¿å­˜generateå‡½æ•°
                
                logger.info("MLXæ¨¡å‹åŠ è½½æˆåŠŸï¼")
                return
                
            except ImportError:
                logger.error("mlx_lmæœªå®‰è£…ï¼è¯·è¿è¡Œ: pip install --upgrade mlx_lm")
                raise Exception("éœ€è¦å®‰è£…mlx_lmåº“")
            except Exception as e:
                logger.error(f"MLXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise e
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def extract_skills_from_job(self, job_title: str, job_description: str) -> str:
        """
        ä½¿ç”¨MLXæ¨¡å‹ä»èŒä½ä¿¡æ¯ä¸­æå–æŠ€èƒ½è¦æ±‚
        
        Args:
            job_title: èŒä½åç§°
            job_description: èŒä½æè¿°
            
        Returns:
            str: æå–çš„æŠ€èƒ½è¦æ±‚ï¼Œé€—å·åˆ†éš”
        """
        try:
            # æ„å»ºprompt
            prompt = f"""è¯·åˆ†æä»¥ä¸‹èŒä½ä¿¡æ¯ï¼Œæå–å‡ºè¯¥èŒä½éœ€è¦çš„æ ¸å¿ƒæŠ€èƒ½è¦æ±‚ã€‚è¯·åªè¿”å›æŠ€èƒ½å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚

èŒä½åç§°: {job_title}
èŒä½æè¿°: {job_description}

æ ¸å¿ƒæŠ€èƒ½è¦æ±‚:"""

            # ä½¿ç”¨èŠå¤©æ¨¡æ¿
            if self.tokenizer.chat_template is not None:
                messages = [{"role": "user", "content": prompt}]
                formatted_prompt = self.tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    enable_thinking=False  # ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼Œæé«˜æ•ˆç‡
                )
            else:
                formatted_prompt = prompt

            # ç”Ÿæˆå“åº”
            response = self.generate_func(
                self.model,
                self.tokenizer,
                prompt=formatted_prompt,
                verbose=False,
                max_tokens=512,  # æŠ€èƒ½æå–ä¸éœ€è¦å¤ªé•¿
                temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
                top_p=0.8,
                top_k=20
            )
            
            # æ¸…ç†å“åº”
            skills = self._clean_skills_response(response)
            return skills
            
        except Exception as e:
            logger.error(f"æŠ€èƒ½æå–å¤±è´¥: {e}")
            return "æå–å¤±è´¥"
    
    def _clean_skills_response(self, response: str) -> str:
        """æ¸…ç†æ¨¡å‹å“åº”ï¼Œæå–çº¯å‡€çš„æŠ€èƒ½åˆ—è¡¨"""
        try:
            # ç§»é™¤å¯èƒ½çš„æ€è€ƒæ ‡ç­¾
            response = response.replace("<think>", "").replace("</think>", "")
            
            # æŒ‰è¡Œåˆ†å‰²ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„è¡Œ
            lines = response.strip().split('\n')
            skills_line = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # è·³è¿‡æ˜æ˜¾çš„è§£é‡Šæ€§æ–‡æœ¬
                if any(word in line.lower() for word in ['åˆ†æ', 'æ€»ç»“', 'èŒä½', 'è¦æ±‚', 'åŒ…æ‹¬', 'éœ€è¦']):
                    continue
                
                # å¦‚æœåŒ…å«é€—å·åˆ†éš”çš„æŠ€èƒ½ï¼Œå¾ˆå¯èƒ½æ˜¯æˆ‘ä»¬è¦çš„
                if ',' in line and len(line.split(',')) > 1:
                    skills_line = line
                    break
                elif not skills_line:  # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œè®°å½•ç¬¬ä¸€ä¸ªå¯èƒ½çš„è¡Œ
                    skills_line = line
            
            if not skills_line:
                skills_line = lines[0] if lines else response
            
            # æ¸…ç†æŠ€èƒ½åˆ—è¡¨
            skills = []
            for skill in skills_line.split(','):
                skill = skill.strip()
                # ç§»é™¤åºå·
                skill = skill.lstrip('0123456789.- ')
                # ç§»é™¤å¼•å·
                skill = skill.strip('"\'')
                
                if skill and len(skill) > 1:
                    skills.append(skill)
            
            return ', '.join(skills[:10])  # é™åˆ¶æœ€å¤š10ä¸ªæŠ€èƒ½
            
        except Exception as e:
            logger.error(f"æ¸…ç†å“åº”å¤±è´¥: {e}")
            return response[:100]  # è¿”å›å‰100ä¸ªå­—ç¬¦ä½œä¸ºå¤‡é€‰
    
    def process_batch(self, jobs_batch):
        """å¤„ç†ä¸€æ‰¹èŒä½æ•°æ®"""
        results = []
        
        for job in jobs_batch:
            try:
                job_id = job.get('job_id', '')
                company_name = job.get('company_name', '')
                title = job.get('title', '')
                description = job.get('description', '')
                
                # æ¸…ç†æ•°æ®
                title = str(title).strip() if title else ''
                description = str(description).strip() if description else ''
                
                if not title or not description:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®: job_id={job_id}")
                    continue
                
                # æå–æŠ€èƒ½
                skills = self.extract_skills_from_job(title, description)
                
                result = {
                    'job_title': title,
                    'job_description': description,
                    'skill_requirements': skills
                }
                
                results.append(result)
                
                # å¢åŠ å¤„ç†è®¡æ•°
                with self.results_lock:
                    self.processed_count += 1
                
                logger.info(f"å·²å¤„ç† {self.processed_count} æ¡è®°å½• - {title}: {skills}")
                
                # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"å¤„ç†è®°å½•å¤±è´¥: {e}")
                continue
        
        return results
    
    def save_progress(self, processed_jobs, output_file):
        """ä¿å­˜è¿›åº¦"""
        try:
            with self.progress_lock:
                # ä¿å­˜ç»“æœåˆ°CSV
                if processed_jobs:
                    df = pd.DataFrame(processed_jobs)
                    
                    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ ï¼›å¦åˆ™åˆ›å»º
                    if os.path.exists(output_file):
                        df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8')
                    else:
                        df.to_csv(output_file, index=False, encoding='utf-8')
                
                # ä¿å­˜è¿›åº¦ä¿¡æ¯
                progress = {
                    'processed_count': self.processed_count,
                    'last_update': datetime.now().isoformat(),
                    'model_name': self.model_name
                }
                
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    self.processed_count = progress.get('processed_count', 0)
                    logger.info(f"å·²åŠ è½½è¿›åº¦: {self.processed_count} æ¡è®°å½•å·²å¤„ç†")
                    return progress
            return {}
        except Exception as e:
            logger.warning(f"åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return {}
    
    def process_dataset(self, input_file: str, output_file: str = None, batch_size: int = 10):
        """
        å¤„ç†æ•´ä¸ªæ•°æ®é›†
        
        Args:
            input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"datasets/job_skills_mlx_{timestamp}.csv"
        
        logger.info(f"å¼€å§‹å¤„ç†æ•°æ®é›†: {input_file}")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        
        # åŠ è½½è¿›åº¦
        self.load_progress()
        
        try:
            # è¯»å–æ•°æ®
            df = pd.read_csv(input_file)
            logger.info(f"æ€»å…± {len(df)} æ¡è®°å½•")
            
            # è·³è¿‡å·²å¤„ç†çš„è®°å½•
            if self.processed_count > 0:
                df = df.iloc[self.processed_count:]
                logger.info(f"è·³è¿‡å·²å¤„ç†çš„ {self.processed_count} æ¡è®°å½•ï¼Œå‰©ä½™ {len(df)} æ¡")
            
            # åˆ†æ‰¹å¤„ç†
            all_results = []
            total_batches = (len(df) + batch_size - 1) // batch_size
            
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size]
                batch_num = i // batch_size + 1
                
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡è®°å½•)")
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                jobs_batch = batch.to_dict('records')
                
                # å¤„ç†æ‰¹æ¬¡
                batch_results = self.process_batch(jobs_batch)
                all_results.extend(batch_results)
                
                # æ¯å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡å°±ä¿å­˜ä¸€æ¬¡
                self.save_progress(batch_results, output_file)
                
                logger.info(f"æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç´¯è®¡å¤„ç† {self.processed_count} æ¡è®°å½•")
            
            logger.info(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç† {len(all_results)} æ¡æœ‰æ•ˆè®°å½•")
            logger.info(f"ç»“æœä¿å­˜åˆ°: {output_file}")
            
            return output_file
            
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®é›†å¤±è´¥: {e}")
            raise e

def main():
    """ä¸»å‡½æ•°"""
    print("=== DCAS æœ¬åœ°MLXæŠ€èƒ½æå–å™¨ ===")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "datasets/Job Descptions/postings.csv"  # ä¿®æ­£æ‹¼å†™é”™è¯¯
    if not os.path.exists(input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    try:
        # è¯¢é—®å‚æ•°
        print(f"\nè¾“å…¥æ–‡ä»¶: {input_file}")
        
        # è®¾ç½®å‚æ•°
        max_workers = 1  # MLXæ¨¡å‹å•çº¿ç¨‹å¤„ç†æ›´ç¨³å®š
        batch_size = 5   # å°æ‰¹æ¬¡ï¼Œé¿å…å†…å­˜é—®é¢˜
        
        print(f"å¹¶å‘æ•°: {max_workers}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # åˆ›å»ºæå–å™¨
        extractor = LocalMLXSkillExtractor(max_workers=max_workers)
        
        # å¤„ç†æ•°æ®é›†
        output_file = extractor.process_dataset(
            input_file=input_file,
            batch_size=batch_size
        )
        
        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_file}")
        print(f"ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {extractor.model_name}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise e

if __name__ == "__main__":
    main()