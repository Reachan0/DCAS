# DCAS è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºæŒ‡å—

## ğŸ“– æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»DCASé¡¹ç›®ä¸­è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºçš„æŠ€æœ¯å®ç°ã€æ¶æ„è®¾è®¡å’Œä½¿ç”¨æ–¹æ³•ã€‚çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»ŸåŸºäºQwen3-Embedding-0.6Bæ¨¡å‹ï¼Œå°†è¯¾ç¨‹å¤§çº²æ•°æ®è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œå¹¶æ„å»ºè¯¾ç¨‹é—´çš„å…³è”å…³ç³»å›¾è°±ã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
è¯¾ç¨‹æ•°æ® â†’ æ–‡æœ¬é¢„å¤„ç† â†’ å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦è®¡ç®— â†’ å›¾è°±æ„å»º â†’ åˆ†æè¾“å‡º
    â†“           â†“          â†“          â†“          â†“          â†“
 JSONæ–‡ä»¶   æ•°æ®æ¸…æ´—   Qwen3åµŒå…¥   ä½™å¼¦ç›¸ä¼¼åº¦   NetworkX   å¯è§†åŒ–
```

### æŠ€æœ¯æ ˆ

- **åµŒå…¥æ¨¡å‹**: Qwen/Qwen3-Embedding-0.6B
- **æ·±åº¦å­¦ä¹ æ¡†æ¶**: Transformers, PyTorch
- **å›¾å¤„ç†**: NetworkX
- **æ•°æ®å¤„ç†**: Pandas, NumPy
- **å¯è§†åŒ–**: Matplotlib, Plotly
- **æœºå™¨å­¦ä¹ **: Scikit-learn

## ğŸ“ æ–‡ä»¶ç»“æ„

```
DCAS/
â”œâ”€â”€ course_knowledge_graph_builder.py      # æµ‹è¯•ç‰ˆæœ¬(20ä¸ªæ ·æœ¬)
â”œâ”€â”€ course_knowledge_graph_production.py   # ç”Ÿäº§ç‰ˆæœ¬(å®Œæ•´æ•°æ®é›†)
â”œâ”€â”€ query_knowledge_graph.py              # æŸ¥è¯¢å·¥å…·
â”œâ”€â”€ streamlit_dashboard.py                # Webå¯è§†åŒ–ç•Œé¢
â”œâ”€â”€ datasets/Course Details/General/      # è¯¾ç¨‹æ•°æ®ç›®å½•
â””â”€â”€ knowledge_graph_output*/              # è¾“å‡ºç»“æœç›®å½•
```

## ğŸ”§ æ ¸å¿ƒè„šæœ¬è¯¦è§£

### 1. course_knowledge_graph_builder.py (æµ‹è¯•ç‰ˆæœ¬)

**ç”¨é€”**: æœ¬åœ°æµ‹è¯•å’Œå¼€å‘ï¼Œå¤„ç†å°‘é‡æ ·æœ¬æ•°æ®

**ä¸»è¦ç‰¹æ€§**:
- å¤„ç†20ä¸ªè¯¾ç¨‹æ ·æœ¬
- å¿«é€ŸéªŒè¯ç®—æ³•æ•ˆæœ
- æœ¬åœ°å¼€å‘è°ƒè¯•

**æ ¸å¿ƒç±»**: `CourseKnowledgeGraphBuilder`

```python
class CourseKnowledgeGraphBuilder:
    def __init__(self, data_dir, model_name="Qwen/Qwen3-Embedding-0.6B"):
        self.data_dir = Path(data_dir)
        self.model_name = model_name
        self.similarity_threshold = 0.7
        self.batch_size = 10
```

**å…³é”®æ–¹æ³•**:

1. **æ•°æ®åŠ è½½** (`_load_course_data`)
   ```python
   def _load_course_data(self):
       """åŠ è½½è¯¾ç¨‹JSONæ•°æ®å¹¶è§£æå­—æ®µ"""
       course_files = list(self.data_dir.glob("*.json"))[:20]  # é™åˆ¶20ä¸ª
       for file_path in course_files:
           with open(file_path, 'r', encoding='utf-8') as f:
               course_data = json.load(f)
   ```

2. **æ–‡æœ¬é¢„å¤„ç†** (`_preprocess_text`)
   ```python
   def _preprocess_text(self, text):
       """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
       text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
       text = re.sub(r'\s+', ' ', text)     # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
       return text.strip()
   ```

3. **å‘é‡åŒ–** (`_get_embeddings`)
   ```python
   def _get_embeddings(self, texts):
       """ä½¿ç”¨Qwen3æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
       embeddings = []
       for i in range(0, len(texts), self.batch_size):
           batch = texts[i:i + self.batch_size]
           batch_embeddings = self._encode_batch(batch)
           embeddings.extend(batch_embeddings)
   ```

4. **å›¾è°±æ„å»º** (`_build_knowledge_graph`)
   ```python
   def _build_knowledge_graph(self, courses, embeddings):
       """åŸºäºç›¸ä¼¼åº¦æ„å»ºçŸ¥è¯†å›¾è°±"""
       similarity_matrix = cosine_similarity(embeddings)
       graph = nx.Graph()
       
       for i, j in combinations(range(len(courses)), 2):
           similarity = similarity_matrix[i][j]
           if similarity >= self.similarity_threshold:
               graph.add_edge(i, j, weight=similarity)
   ```

### 2. course_knowledge_graph_production.py (ç”Ÿäº§ç‰ˆæœ¬)

**ç”¨é€”**: æœåŠ¡å™¨éƒ¨ç½²ï¼Œå¤„ç†å®Œæ•´æ•°æ®é›†

**å¢å¼ºç‰¹æ€§**:
- å†…å­˜ä¼˜åŒ–å’Œæ‰¹å¤„ç†
- æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- GPU/CPUè‡ªåŠ¨åˆ‡æ¢
- è¿›åº¦ç›‘æ§å’Œé”™è¯¯å¤„ç†

**å…³é”®å¢å¼º**:

1. **å†…å­˜ç®¡ç†**
   ```python
   def _monitor_memory(self):
       """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
       process = psutil.Process()
       memory_info = process.memory_info()
       memory_gb = memory_info.rss / (1024**3)
       
       if memory_gb > self.max_memory_gb:
           self._cleanup_memory()
   ```

2. **æ‰¹å¤„ç†ä¼˜åŒ–**
   ```python
   def _process_in_batches(self, course_data):
       """åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡æ•°æ®"""
       for i in range(0, len(course_data), self.batch_size):
           batch = course_data[i:i + self.batch_size]
           batch_embeddings = self._get_embeddings(batch)
           self._save_checkpoint(i, batch_embeddings)
   ```

3. **æ–­ç‚¹ç»­ä¼ **
   ```python
   def _load_checkpoint(self):
       """ä»æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†"""
       checkpoint_file = self.output_dir / "checkpoint.pkl"
       if checkpoint_file.exists():
           with open(checkpoint_file, 'rb') as f:
               return pickle.load(f)
   ```

### 3. query_knowledge_graph.py (æŸ¥è¯¢å·¥å…·)

**ç”¨é€”**: äº¤äº’å¼æŸ¥è¯¢å’Œæ¢ç´¢çŸ¥è¯†å›¾è°±

**ä¸»è¦åŠŸèƒ½**:
- è¯¾ç¨‹æœç´¢å’Œè¿‡æ»¤
- ç›¸ä¼¼åº¦åˆ†æ
- å›¾è°±ç»Ÿè®¡
- ä¸»é¢˜èšç±»æŸ¥è¯¢

**æ ¸å¿ƒæ–¹æ³•**:

1. **è¯¾ç¨‹æœç´¢**
   ```python
   def search_courses(self, keyword, top_k=10):
       """åŸºäºå…³é”®è¯æœç´¢ç›¸å…³è¯¾ç¨‹"""
       scores = []
       for course in self.courses:
           score = self._calculate_relevance(keyword, course)
           scores.append((score, course))
       return sorted(scores, reverse=True)[:top_k]
   ```

2. **ç›¸ä¼¼è¯¾ç¨‹æ¨è**
   ```python
   def find_similar_courses(self, course_name, top_k=5):
       """æŸ¥æ‰¾æŒ‡å®šè¯¾ç¨‹çš„ç›¸ä¼¼è¯¾ç¨‹"""
       target_idx = self._find_course_index(course_name)
       similarities = cosine_similarity([self.embeddings[target_idx]], 
                                      self.embeddings)[0]
       similar_indices = np.argsort(similarities)[::-1][1:top_k+1]
   ```

## ğŸ¯ ç®—æ³•åŸç†

### 1. æ–‡æœ¬åµŒå…¥ç®—æ³•

**Qwen3-Embedding-0.6B**:
- åŸºäºTransformeræ¶æ„
- 6äº¿å‚æ•°è§„æ¨¡
- æ”¯æŒå¤šè¯­è¨€æ–‡æœ¬åµŒå…¥
- è¾“å‡º768ç»´å‘é‡è¡¨ç¤º

**åµŒå…¥è¿‡ç¨‹**:
```python
# 1. æ–‡æœ¬é¢„å¤„ç†
processed_text = self._preprocess_text(course_description)

# 2. åˆ†è¯å’Œç¼–ç 
inputs = self.tokenizer(processed_text, return_tensors="pt", 
                       padding=True, truncation=True)

# 3. æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = self.model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)
```

### 2. ç›¸ä¼¼åº¦è®¡ç®—

**ä½™å¼¦ç›¸ä¼¼åº¦**:
```python
def cosine_similarity(A, B):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(A, B)
    norm_A = np.linalg.norm(A)
    norm_B = np.linalg.norm(B)
    return dot_product / (norm_A * norm_B)
```

**ç›¸ä¼¼åº¦é˜ˆå€¼ç­–ç•¥**:
- é»˜è®¤é˜ˆå€¼: 0.7
- åŠ¨æ€è°ƒæ•´: æ ¹æ®æ•°æ®åˆ†å¸ƒä¼˜åŒ–
- å¤šå±‚é˜ˆå€¼: ä¸åŒç±»å‹å…³ç³»ä½¿ç”¨ä¸åŒé˜ˆå€¼

### 3. å›¾è°±æ„å»ºç®—æ³•

**èŠ‚ç‚¹åˆ›å»º**:
```python
# æ¯ä¸ªè¯¾ç¨‹ä½œä¸ºä¸€ä¸ªèŠ‚ç‚¹
for i, course in enumerate(courses):
    graph.add_node(i, 
                   name=course['course_name'],
                   topics=course['topics'],
                   description=course['course_description'])
```

**è¾¹æƒé‡è®¡ç®—**:
```python
# åŸºäºç›¸ä¼¼åº¦æ·»åŠ è¾¹
for i, j in combinations(range(len(courses)), 2):
    similarity = cosine_similarity(embeddings[i], embeddings[j])
    if similarity >= threshold:
        graph.add_edge(i, j, weight=similarity)
```

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

1. **ç¯å¢ƒå‡†å¤‡**
   ```bash
   pip install transformers torch networkx scikit-learn matplotlib
   ```

2. **æµ‹è¯•è¿è¡Œ**
   ```bash
   python course_knowledge_graph_builder.py
   ```

3. **ç”Ÿäº§éƒ¨ç½²**
   ```bash
   python course_knowledge_graph_production.py
   ```

### é…ç½®å‚æ•°

**æ¨¡å‹é…ç½®**:
```python
config = {
    "model_name": "Qwen/Qwen3-Embedding-0.6B",
    "batch_size": 10,
    "similarity_threshold": 0.7,
    "max_memory_gb": 8.0
}
```

**ç¡¬ä»¶è¦æ±‚**:
- **CPU**: 4æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 8GBä»¥ä¸Š
- **æ˜¾å­˜**: 4GBä»¥ä¸Š(GPUåŠ é€Ÿ)
- **å­˜å‚¨**: 2GBä»¥ä¸Š

### è¾“å‡ºè¯´æ˜

**ç›®å½•ç»“æ„**:
```
knowledge_graph_output/
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ course_embeddings_*.pkl      # å‘é‡æ•°æ®
â”œâ”€â”€ graphs/
â”‚   â”œâ”€â”€ course_knowledge_graph_*.pkl # å›¾è°±å¯¹è±¡
â”‚   â””â”€â”€ course_knowledge_graph_*.gml # å›¾è°±æ ¼å¼
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ topic_analysis_*.json       # ä¸»é¢˜åˆ†æ
â””â”€â”€ summary_report_*.md              # æ€»ç»“æŠ¥å‘Š
```

**æ•°æ®æ ¼å¼**:

1. **åµŒå…¥å‘é‡**:
   ```python
   {
       'embeddings': np.ndarray,      # å½¢çŠ¶: (n_courses, 768)
       'courses': list,               # è¯¾ç¨‹å…ƒæ•°æ®
       'model_info': dict,            # æ¨¡å‹ä¿¡æ¯
       'timestamp': str               # ç”Ÿæˆæ—¶é—´
   }
   ```

2. **çŸ¥è¯†å›¾è°±**:
   ```python
   # NetworkX Graphå¯¹è±¡
   graph.nodes[i] = {
       'name': str,              # è¯¾ç¨‹åç§°
       'topics': list,           # ä¸»é¢˜æ ‡ç­¾
       'description': str        # è¯¾ç¨‹æè¿°
   }
   
   graph.edges[i, j] = {
       'weight': float          # ç›¸ä¼¼åº¦æƒé‡
   }
   ```

## ğŸ”§ é«˜çº§é…ç½®

### 1. æ¨¡å‹ä¼˜åŒ–

**ç²¾åº¦é…ç½®**:
```python
# æ··åˆç²¾åº¦æ¨ç†
model.half()  # FP16ç²¾åº¦ï¼Œå‡å°‘æ˜¾å­˜

# é‡åŒ–æ¨ç†
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)
```

**æ‰¹å¤„ç†ä¼˜åŒ–**:
```python
# åŠ¨æ€æ‰¹å¤„ç†å¤§å°
def adaptive_batch_size(available_memory):
    if available_memory > 8:
        return 20
    elif available_memory > 4:
        return 10
    else:
        return 5
```

### 2. ç›¸ä¼¼åº¦é˜ˆå€¼è°ƒä¼˜

**è‡ªåŠ¨é˜ˆå€¼é€‰æ‹©**:
```python
def optimal_threshold(similarity_matrix, target_density=0.1):
    """åŸºäºç›®æ ‡å›¾å¯†åº¦é€‰æ‹©æœ€ä¼˜é˜ˆå€¼"""
    thresholds = np.arange(0.5, 0.9, 0.05)
    for threshold in thresholds:
        density = calculate_density(similarity_matrix, threshold)
        if density <= target_density:
            return threshold
```

**å¤šå±‚é˜ˆå€¼ç­–ç•¥**:
```python
thresholds = {
    'strong_similarity': 0.8,    # å¼ºç›¸å…³
    'moderate_similarity': 0.7,  # ä¸­ç­‰ç›¸å…³
    'weak_similarity': 0.6       # å¼±ç›¸å…³
}
```

### 3. å†…å­˜ä¼˜åŒ–ç­–ç•¥

**åˆ†å—å¤„ç†**:
```python
def process_large_dataset(courses, chunk_size=1000):
    """åˆ†å—å¤„ç†å¤§æ•°æ®é›†"""
    for chunk in chunks(courses, chunk_size):
        embeddings = get_embeddings(chunk)
        partial_graph = build_partial_graph(embeddings)
        merge_graphs(main_graph, partial_graph)
```

**ç¼“å­˜ç­–ç•¥**:
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text_hash):
    """ç¼“å­˜é‡å¤æ–‡æœ¬çš„åµŒå…¥"""
    return get_embedding(text_hash)
```

## ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³

### 1. å†…å­˜ä¸è¶³

**é—®é¢˜**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°‘æ‰¹å¤„ç†å¤§å°
batch_size = 5

# ä½¿ç”¨CPUæ¨ç†
device = "cpu"

# æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**: `ConnectionError: æ— æ³•ä¸‹è½½æ¨¡å‹`

**è§£å†³æ–¹æ¡ˆ**:
```python
# æœ¬åœ°æ¨¡å‹è·¯å¾„
model_name = "/path/to/local/qwen3-embedding"

# ç¦»çº¿æ¨¡å¼
os.environ["TRANSFORMERS_OFFLINE"] = "1"
```

### 3. å›¾è°±è¿‡äºç¨ å¯†

**é—®é¢˜**: ç”Ÿæˆçš„å›¾è°±è¿æ¥è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**:
```python
# æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼
similarity_threshold = 0.8

# ä½¿ç”¨Top-Kç›¸ä¼¼åº¦
def top_k_similarity(similarities, k=5):
    top_indices = np.argsort(similarities)[-k:]
    return top_indices[similarities[top_indices] > threshold]
```

### 4. å¤„ç†é€Ÿåº¦è¿‡æ…¢

**é—®é¢˜**: å¤§æ•°æ®é›†å¤„ç†æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¹¶è¡Œå¤„ç†
from multiprocessing import Pool
with Pool(processes=4) as pool:
    results = pool.map(process_batch, batches)

# GPUåŠ é€Ÿ
if torch.cuda.is_available():
    model = model.cuda()
    
# é¢„è®¡ç®—ç¼“å­˜
precompute_embeddings(frequent_texts)
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å¤„ç†èƒ½åŠ›

| æ•°æ®è§„æ¨¡ | å¤„ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ | æ˜¾å­˜ä½¿ç”¨ |
|---------|---------|---------|---------|
| 100è¯¾ç¨‹  | 2åˆ†é’Ÿ    | 2GB     | 3GB     |
| 500è¯¾ç¨‹  | 8åˆ†é’Ÿ    | 4GB     | 4GB     |
| 1000è¯¾ç¨‹ | 15åˆ†é’Ÿ   | 6GB     | 5GB     |
| 2000è¯¾ç¨‹ | 30åˆ†é’Ÿ   | 8GB     | 6GB     |

### ç¡¬ä»¶é…ç½®å»ºè®®

**æœ€ä½é…ç½®**:
- CPU: 4æ ¸å¿ƒ 2.0GHz
- å†…å­˜: 8GB
- æ˜¾å¡: 4GBæ˜¾å­˜(å¯é€‰)

**æ¨èé…ç½®**:
- CPU: 8æ ¸å¿ƒ 3.0GHz
- å†…å­˜: 16GB
- æ˜¾å¡: 8GBæ˜¾å­˜

**é«˜æ€§èƒ½é…ç½®**:
- CPU: 16æ ¸å¿ƒ 3.5GHz
- å†…å­˜: 32GB
- æ˜¾å¡: 16GBæ˜¾å­˜

## ğŸ”„ æ‰©å±•å¼€å‘

### 1. è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹

```python
class CustomEmbeddingModel:
    def __init__(self, model_path):
        self.model = self.load_custom_model(model_path)
    
    def encode(self, texts):
        # è‡ªå®šä¹‰ç¼–ç é€»è¾‘
        return custom_embeddings
```

### 2. é«˜çº§ç›¸ä¼¼åº¦è®¡ç®—

```python
def advanced_similarity(emb1, emb2, method="cosine"):
    if method == "cosine":
        return cosine_similarity(emb1, emb2)
    elif method == "euclidean":
        return 1 / (1 + euclidean_distance(emb1, emb2))
    elif method == "weighted":
        return weighted_similarity(emb1, emb2, weights)
```

### 3. åŠ¨æ€å›¾è°±æ›´æ–°

```python
def incremental_update(graph, new_courses):
    """å¢é‡æ›´æ–°çŸ¥è¯†å›¾è°±"""
    new_embeddings = get_embeddings(new_courses)
    existing_embeddings = load_embeddings()
    
    # è®¡ç®—æ–°è¯¾ç¨‹ä¸ç°æœ‰è¯¾ç¨‹çš„ç›¸ä¼¼åº¦
    cross_similarities = cosine_similarity(new_embeddings, 
                                         existing_embeddings)
    
    # æ·»åŠ æ–°èŠ‚ç‚¹å’Œè¾¹
    update_graph(graph, new_courses, cross_similarities)
```

## ğŸ“š å‚è€ƒèµ„æº

### ç›¸å…³è®ºæ–‡
- "Attention Is All You Need" - Transformeræ¶æ„
- "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
- "Graph Neural Networks: A Review of Methods and Applications"

### æŠ€æœ¯æ–‡æ¡£
- [Qwen3æ¨¡å‹æ–‡æ¡£](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)
- [NetworkXå®˜æ–¹æ–‡æ¡£](https://networkx.org/documentation/stable/)
- [Transformersåº“æ–‡æ¡£](https://huggingface.co/docs/transformers)

### å¼€æºé¡¹ç›®
- [SentenceTransformers](https://github.com/UKPLab/sentence-transformers)
- [PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric)
- [DGL (Deep Graph Library)](https://github.com/dmlc/dgl)

---

## ğŸ‰ æ€»ç»“

DCASè¯¾ç¨‹çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿæä¾›äº†å®Œæ•´çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œä»åŸå§‹è¯¾ç¨‹æ•°æ®åˆ°å¯è§†åŒ–çŸ¥è¯†å›¾è°±çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–å¤„ç†ã€‚ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½ä¼˜åŒ–ï¼Œæ”¯æŒä»å°è§„æ¨¡æµ‹è¯•åˆ°å¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½²çš„å„ç§åœºæ™¯ã€‚

**ä¸»è¦ä¼˜åŠ¿**:
- ğŸš€ **é«˜æ€§èƒ½**: åŸºäºå…ˆè¿›çš„Qwen3åµŒå…¥æ¨¡å‹
- ğŸ”§ **æ˜“æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰ç»„ä»¶
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½æ‰¹å¤„ç†å’Œç¼“å­˜æœºåˆ¶
- ğŸ“Š **ä¸°å¯Œè¾“å‡º**: å¤šæ ¼å¼æ•°æ®å¯¼å‡ºå’Œå¯è§†åŒ–
- ğŸ› ï¸ **ç”Ÿäº§å°±ç»ª**: æ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯æ¢å¤æœºåˆ¶

é€šè¿‡æœ¬ç³»ç»Ÿï¼Œç”¨æˆ·å¯ä»¥è½»æ¾æ„å»ºé«˜è´¨é‡çš„è¯¾ç¨‹çŸ¥è¯†å›¾è°±ï¼Œä¸ºæ•™è‚²èµ„æºæ¨èã€è¯¾ç¨‹å…³è”åˆ†æç­‰åº”ç”¨æä¾›å¼ºå¤§çš„æ•°æ®åŸºç¡€ã€‚