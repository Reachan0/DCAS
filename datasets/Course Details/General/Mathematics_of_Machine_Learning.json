{
  "course_name": "Mathematics of Machine Learning",
  "course_description": "Broadly speaking, Machine Learning refers to the automated identification of patterns in data. As such it has been a fertile ground for new statistical and algorithmic developments. The purpose of this course is to provide a mathematically rigorous introduction to these developments with emphasis on methods and their analysis.\nYou can read more about Prof. Rigollet’s work and courses on his website.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Data Mining",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Data Mining",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\n18.100C Real Analysis\n\n18.06SC Linear Algebra\n\n18.05 Introduction to Probability and Statistics\n\nDescription\n\nBroadly speaking, Machine Learning refers to the automated identification of patterns in data. As such it has been a fertile ground for new statistical and algorithmic developments. The purpose of this course is to provide a mathematically rigorous introduction to these developments with emphasis on methods and their analysis.\n\nThe Topics Covered\n\nThe class will be split in three main parts:\n\nThe Statistical Theory of Machine Learning.\n\nClassification, Regression, Aggregation\n\nEmpirical Risk Minimization, Regularization\n\nSuprema of Empirical Processes\n\nAlgorithms and Convexity.\n\nBoosting\n\nKernel Methods\n\nConvex Optimization\n\nOnline Learning.\n\nOnline Convex Optimization\n\nPartial Information: Bandit Problems\n\nBlackwell's Approachability\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\n40%\n\nFinal Project\n\n50%\n\nLecture Notes Scribing\n\n10%\n\nHomework 40%\n\nThere are 3 homework assignments.\n\nFinal project 50%\n\nThe final project should be in any area related to one of the topics of the course or use tools that are developed in class. Examples include: implementing an algorithm for real data, extend an existing method or prove a theoretical result (or a combination of these). You will need to submit a written report (~10 pages) and give a presentation in class in the last week of semester (the duration will depend on the size of the class).\n\nLecture notes scribing 10%",
  "files": [
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/83e771a9b96fa08a6bac7bc783411491_MIT18_657F15_PS1.pdf",
      "content": "18.657. Fall 2105\nRigollet\nSeptember 27, 2015\nProblem set #1 (due Wed., October 7)\nProblem 1. Discriminant analysis\nLet (X, Y ) ∈IRd×{0, 1} be a random pair such that IP(Y = k) = πk > 0 (π0+π1 = 1)\nand the conditional distribution of X given Y is X|Y ∼N (μ , Σ ), where μ = μ ∈IRd\nY\nY\nand Σ0, Σ1 ∈IRd×d are mean vectors and covariance matrices respectively.\n1. What is the (unconditional) density of X?\n2. Assume that Σ0 = Σ1 = Σ is a positive definite matrix. Compute the Bayes classifier\nh∗as a function of μ0, μ1, π0, π1 and Σ. What is the nature of the sets {h∗= 0} and\n{h∗= 1}?\n3. Assume now that Σ0 = Σ1 are two positive definite matrices. What is the nature of\nthe sets {h∗= 0} and {h∗= 1}?\nProblem 2. VC dimensions\n1. Let C be the class of convex polygons in IR2 with d vertices. Show that VC(C) =\n2d + 1.\n2. Let C be the class of convex compact sets in IR2. Show that VC(C) = inf.\n3. Let C be finite. Show that VC(C) ≤log2(card C).\n4. Give an example of a class C such that card C = infand VC(C) = 1.\nProblem 3. Glivenko-Cantelli Theorem\nLet X1, . . . , Xn be n i.i.d copies of X that has cumulative distribution function (cdf)\nF(t) = IP(X ≤t). The empirical cdf of X is defined by\nˆFn(t) =\nn\nX\n1I(Xi\n.\ni=1\n≤t)\nn\nFˆn t\nˆ\n1. Compute the mean and the variance of\n( ) and conclude that Fn(t) →F(t) as\nn →infalmost surely (hint: use Borel-Cantelli).\npage 1 of 2\n\n2. Show that for n ≥2\nˆ\nsup Fn(t)\nF(t)\nC\nt∈IR\n\n-\n≤\nr\nlog(n/δ)\nn\nwith probability 1 -δ.\nProblem 4. Concentration\n1. Let X1, . . . , Xn be n i.i.d copies of X ∈[0, 1]. Each Xi represents the size of a\npackages to be shipped. The shipping containers are bins of size 1 (so that each bin\ncan hold a set of packages whose sizes sum to at most 1). Let Bn be the minimal\nnumber of bins needed to store the n packages. Show that\nIP(|Bn -\nIE[Bn]| ≥t) ≤2e\nt\n-n .\n2. Let X1, . . . , Xn be n i.i.d copies of X ∈IRd, IE[X] = 0 and assume that ∥Xi\nX\n∥≤1\nalmost surely for all i. Let\ndenote the average of the Xis. Prove the following\ninequalities (the constant C may change from one inequality to the other)\n(a) IP\nh\nX\n-IE\n\nC\nX ≥t\ni\n≤e-Cnt2 ,\n(b) IE\n\nX\n\n≤√\n,\n(c) IP\nn\nh\ni\n-Cnt2\nX ≥t ≤2e\n3. Let X1, . . . , Xn be n iid random variables, i.e. such that Xi and -Xi have the same\n\ndistribution. Let X denote the average of the Xis and V = n-1 Pn\ni=1 Xi . Show\nthat\nIP\nh X\n√\n> t\nV\ni\n≤e\nnt\n-\n2 .\n[Hint: introduce Rademacher random variables].\npage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 1 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/d61d6c8e64ed20efff3e6ca4fc88742b_MIT18_657F15_PS1_Sol.pdf",
      "content": "18.657 PS 1 SOLUTIONS\n1. Problem 1\n(1) We expand in terms of conditional probabilities:\nP(X ∈U) = P(X ∈U | Y = 0) P(Y = 0) + P(X ∈U | Y = 1) P(Y = 1)\n= π0 P(X ∈U | Y = 0) + π1 P(X ∈U | Y = 1).\nPassing to densities:\npX = π0 pX|Y =0 + π1 pX|Y =1\n= π0(2π)-d/2(det Σ0)-1/2 exp\n\n-(x\n-μ0)⊤Σ-1\n0 (x -μ0)\nd/2\n1/2\n\n+ π1(2π)-\n(det Σ1)-\nexp\n-(x -μ1)⊤Σ-1\n1 (x -μ1)\n\n.\n(2) By Bayes' rule, we have P(Y = y | X = x) ∝pX Y =y(x) P(Y = y). Thus, given X = x, the\n|\nvalue of the Bayes classifier is 0 if\nπ0pX|Y =0 > π1pX|Y =1,\nand is otherwise 1. Cancelling (2π)-d/2(det Σ)-1/2 on both sides, and taking logs, the in-\nequality reads\nlog π0 -\n(x\n-μ0)⊤Σ-1(x -μ0) > log π1 -\n(x -μ1)⊤Σ-1(x\n-μ1).\nThis asks whether x is at least log π0 -log π1 units closer to μ0 than μ1, in distance measured\nby the quadratic form\n1Σ-1.\nBy the PSD assumption, this distance matches Euclidean\ndistance after a linear transformation (given by the Cholesky factorization of Σ-1). Then,\ngeometrically, the boundary forms a hyperplane, and the two regions {h∗= 0} and {h∗= 1}\nform half-spaces.\n(3) The inequality defining the Bayes classifier remains quadratic in x, so the hypersurface sepa-\nrating {h∗= 0} from {h∗= 1} is a quadric. For example, with\nΣ0 =\nthe\n\n,\n\nΣ1 =\n\n,\nπ0 = π1,\nseparating curve is a hyperbola.\n\n18.657 PS 1 SOLUTIONS\n2. Problem 2\n(1) We see that C can shatter 2d + 1 points on a circle: proceeding clockwise around the circle,\nwe pass a line from last of each run of consecutive 'included' points to start of the next such\nrun, for a total of at most d lines, and define our polygon by these lines (which extend beyond\ntheir defining points).\nSuppose we have 2d + 2 points. If any point is in the convex hull of the others, then C\nfails to shatter: any set in C is convex, and thus can not include the extreme points while\nexcluding an interior point. Otherwise, number the points in their clockwise order as vertices\nof the convex hull, and consider an alternating sign pattern. In order to shatter, there must\nbe a face of the polygon from C passing between each excluded point and its two included\nneighbors; these d + 1 faces must all be distinct, or else we could show that the given points\nviolated our convexity assumption; but no convex polygon with d vertices has more than d\nfaces, a contradiction. So C can not shatter 2d + 2 points.\n(2) The convex compact sets include in particular the convex polygons of the previous section,\nwhich we see can shatter any number of points (once we allow arbitrarily many polygon\nvertices). Thus the VC dimension is infinite.\n(3) We know that C shatters some n = VC(C) points. Then some set in\nn\nC achieves each of the 2n\ninclusion patterns. In particular there exist at least 2\nsets in C. So n ≤log2(card C).\n(4) On the space R, the class of intervals (-inf, t] for t ∈R is infinite. This class certainly shatters\nthe point 0 (the intervals (-inf, -1] and (-inf, 0] suffice), while it fails to shatter any two\npoints a < b, since no such interval contains b but not a. Thus the VC dimension is 1.\n\n18.657 PS 1 SOLUTIONS\n3. Problem 3\n(1) We begin by computing mean and variance:\nX\nn\nX\nn\nE ˆ\n[Fn(t)] =\nP(Xi ≤t) =\nF(t) = F(t),\nn\nn\ni=1\ni=1\nn\nˆ\nVar[Fn(t)] = n2\nX\nVar[1(Xi\ni=1\n≤t)]\n=\nVar[1(X1\nn\n≤t)]\n=\n(E[1(X1 ≤t)2] -E[1(X\n1 ≤t)] ) =\n(F(t)\nn\nn\n-F(t)2).\nˆ\nLet ε > 0 be given. As the Fn(t) are averages of independent 0-1 random variables, we can\napply Hoeffding's inequality:\nP\nˆ\n(|Fn(t) -F(t)| ≥ε) ≤2 exp(-2nε2),\nX\ninf\ninf\nP(| ˆFn(t) -F(t)| ≥ε) ≤\nX\n2 exp(-2nε) =\n<\n,\n=1\nn=1\n-exp(\nn\n-ε)\ninf\nˆ\nso that by Borel-Cantelli, almost surely only finitely many of the events |Fn(t) -F(t)| ≥ε\noccur. Thus Fn(t)\nF(t) almost surely.\nˆ\n→\n(Alternative: Fn(t) is an average of n iid samples of Bern(F(t)); the result follows from the\nstrong law of large numbers. Of course, the proof of that law uses Borel-Cantelli.)\n(2) We can view this is a question of empirical measure versus true measure on the class C of\nhalf-line intervals (inf, t], which have VC dimension 1 (see the solution to 2(d)).\nThe VC\ninequality now asserts that\nsup | ˆFn(t) -F(t) = sup μn(A)\nμ(A)\nt\n|\nA\n|\n-\n∈R\n|\n∈C\n≤2\nr\n2 log(2en) +\nn\nr\nlog(2/δ)\n2n\n≤\n\ns\n2 log 4e\n(\n+\nlog\n√\n! r\nlog n/δ)\nn\nfor all n ≥2.\n\n18.657 PS 1 SOLUTIONS\n4. Problem 4\n(1) Changing the value Xi can only change Bn by at most 1: putting the ith item in a new bin\nis always an option. Applying the bounded differences inequality yields exactly the result:\nP(|Bn -EBn| > t) ≤2 exp(-2t2/n).\n\n(2)\n(a) By the triangle inequality, changing Xi will change ∥X∥by at most 2/n. By the (one-\nsided) bounded differences inequality,\n2t2\nP(∥\n\nX\nE\n-\n∥-\n∥X∥≥t) ≤exp\n\n=\n(2/n)2\n\nexp(-nt /2).\nn\n(b) We apply Jensen's inequality:\nE\n\n[∥X∥] ≤E[∥ X∥2]1/2\nX\n!1/2\nd\n\n=\nE[X(i)2]\n.\ni=1\nBy the assumption E[X] = 0, the cross terms of the squared average will cancel in\nexpectation:\nE\n[X(i)2] =\nX\nn\nn2\nE[Xj(i)2\n] =\nj=1\nE[X(i)2].\nn\nReturning to our computation:\nE[∥ X∥] ≤\nX\nd\nE[X(i)2]\nn\ni=1\n=\n1 E[\nn\n∥X∥2]\n1/2\n≤√.\nn\n(c) Combining (a) and (b), we have\nP\n\n(\n√\n∥X∥≥t) ≤exp(\n\n-n(t -1/\nn)2/2)\n= exp\n-(t√n)2 + t√n\n-2\n\n.\nThe polynomial -1x2 + x -1 is bounded above by 1 -x2/4. Thus,\nP(∥ X∥≥t) ≤e1/2 exp(-nt2/4) ≤2 exp(-nt2/4).\n(3) Let σ1, . . . , σn be independent symmetric Rademacher random variables. By symmetry of\n\nXi, the distribution of X is the same as that of ⟨σ, X⟩/n. Conditioned on X, Hoeffding's\ninequality yields\nP\n⟨σ, X⟩\n√\n> t\n\n≤exp(-nt2\nσ\n/2).\nn\nV\nBy the law of total\n\nprobabilit\n\ny,\nX\n⟨σ, X\nPX\n√\n> t\n= PX,σ\n⟩\nV\nn\n√\n> t\n= EX Pσ\n⟨σ, X⟩> t\nV\n\nn\n√\nV\n\n≤EX exp(-nt2/2)\n= exp(-nt2/2).\n! /2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/8235bec2d7664a3481322938bcff0225_MIT18_657F15_PS2.pdf",
      "content": "18.657. Fall 2105\nRigollet\nOctober 18, 2015\nProblem set #2 (due Wed., October 21)\nShould be typed in LAT X\nE\nProblem 1. Rademacher Complexities and beyond\nLet F be a class of functions from X to IR and let X1, . . . , Xn be iid copies of a\nrandom variable X ∈X. Moreover, let σ1, . . . , σn be n i.i.d. Rad(1/2) random variables\nand let g1, . . . , gn be n i.i.d. N(0, 1). Assume that all these random variables are mutually\nindependent.\n1. Prove the desymmetrization inequality:\nIE\nh\nsup\nf∈F\n\nn\nX\nσi\n\nf(Xi) -IE[f(X)]\ni=1\ni\n≤2IE\nh\nsup\nn\n\nf∈F\n\nn\nn\nX\ni=1\n\nf(Xi) -IE[f(X)]\ni\n\n2. Prove the Rademacher/Gaussian process comparison inequality\nn\nIE\nh\nsup\nf∈F\nX\nσif(Xi)\ni=1\ni\n≤\nrπ\nn\nIE\nh\nsup\nf∈F\nX\ngif(Xi)\ni=1\ni\n=\nh\nDefine Rn(F)\nIE sup\nf∈F\n\nn\nX\nσif(Xi)\ni\n. Let F and G be two set of functions from\nt\nn\nX\no\nIR and recall that\n+\n=\n\ni=1\n\nF\nG\n{f + g : f ∈F , g ∈G}.\n3. Let h ∈IRX be a given function and define F + h = {f + h : f ∈F}. Show that\nh inf\nRn(F + {h}) ≤Rn(F) + ∥∥\n√\n,\nn\nwhere ∥h∥inf= supx∈X |h(x)|.\n4. Let F1, . . . , Fk be k sets of functions from X to IR. Show that\nk\nRn(F1 + · · · , Fk) ≤\nX\nRn(\nj=1\nFj) .\n5. Show that this inequality derived in 4. is in fact an equality when the Fjs are the\nsame.\npage 1 of 4\n\nProblem 2. Covering and packing\nDefinition: A set P ⊂T is called an ε-packing of the metric space (T, d) if d(f, g) > ε\nfor every f, g ∈P, f = g. The largest cardinality of an ε-packing of (T, d) is called\nthe packing number of (T, d):\nD(T, d, ε) = sup\n\ncard(P) : P is an ε packing of (T, d)\n\nRecall that N(T, d, ε) denotes the ε-covering number of (T, d).\n1. Show that\nD(T, d, 2ε) ≤N(T, d, ε) ≤D(T, d, ε)\nLet M be an n × m random matrix with entries that are i.i.d Rad(1/2) entries. We\nare interested in its operator norm\n∥M∥=\nsup\nu⊤Mv .\nu∈IRn : |u|2≤1\nv∈IRm : |v|2≤1\n2. Show that\n∥M∥≤2 max u⊤Mv ,\nu∈Nn\nv∈Nm\nwhere Nn and Nm are 1-nets of the unit Euclidean balls of IRn and IRm\nrespectively.\n3. Conclude that\nIE∥M∥≤C √\nm + √n\n\n.\nProblem 3. Chaining\nLet F be the class of all nondecreasing functions from [0, 1] to [0, 1].\n1. Show that for any x = (x1, . . . , xn) ∈[0, 1]n, the covering number of (F, dx\ninf) satisfy:\nN(F, dx\ninf, ε) ≤n /ε .\n2. Using the chaining bound, show that\nRn(F) ≤C\nr\nlog n\nn\npage 2 of 4\n\n3. Show that there is indeed a strict improvement over the bound obtained using the\ntheorem in section 5.2.1\nProblem 4. Kernel ridge regression\nConsider the regression model:\nYi = f(xi) + ξi,\n, i = 1, . . . , n\nwhere x1, . . . , xn are fixed design points in IRd, ξ = (ξ1, . . . , ξ\nn\nn) ∼N (0, Σ) ∈IR\nwith\nknown covariance matrix Σ ≻0 and f : IRd →IR is an unknown regression function.\nLet W be an RKHS on IRd with reproducing kernel k. Define Y = (Y1, . . . , Yn)⊤and\ng\nˆ\n= [g(x1), . . . , g(xn)]⊤for any function g. Define the estimator f of f by\nˆf = argmin\n\nψ(Y\ng\ng∈W\n-) + μ∥g∥2\nW\n\nwhere ∥· ∥W denotes the Hilbert norm on W, ψ(x) = x⊤Σ-1/2x and μ > 0 is a tuning\nparameter to be chosen later.\n1. Prove the representer theorem, i.e., that there exists a vector θ ∈IRn such that\nn\nˆf(x) =\nX\nθik(xi, x) ,\nfor any x\ni=1\n∈IRd\nˆ\nˆ\n2. Prove that the vector fˆ = [f(x1), . . . , f(x\n⊤\nn)]\nsatisfies\n(KΣ-1/2 + μIn)\n-1/2\nfˆ = KΣ\nY ,\nwhere In is the identity matrix of IRn and K denotes the symmetric n × n matrix\nwith elements Ki,j = k(xi, xj).\n3. Prove that the following inequality holds\nψ(f -\nfˆ) ≤inf\ng∈W\n\nψ(f -g) + 2μ∥g∥W\n\n+\nn\nX\n\nZik(xi,\nμ\ni=1\n·)\n\n,\nW\nwhere Z1, . . . , Zn are iid N (0, 1).\n4. Conclude that\nIEψ(f -\n∥2\nfˆ) ≤inf\ng∈W\n\nψ(f -g) + 2μ∥g\nW\n\n+\nTr(K) ,\nμ\nwhere Tr(K) denotes the trace of K.\npage 3 of 4\n\n5. Assume now that k is the Gaussian kernel:\nk(x, x′) = e-|x-x′|2\nShow that there exists a choice of μ for which\nIEψ(f -fˆ)\n√\n≤∥f∥W\n2n .\npage 4 of 4\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 2 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/091a852cd0d398a3ca4a41446ab2d30f_MIT18_657F15_PS2_Sol.pdf",
      "content": "18.657 PS 2 SOLUTIONS\n1. Problem 1\n(1) Let Y1, . . . , Yn be ghost copies of X1, . . . , Xn. Then we have\nE\n\"\nsup\n\nX\nn\nn\n\nσi[f(Xi) -E[f(X)]]\n#\n\n= E\n\"\nsup\n\nX\n\nσi[f(Xi) -E[f(Yi)]]\nf\nn\nn\n∈F\ni=1\n#\n\n≤E\n\n\"\nf∈F\ni=1\nn\nsup\nσi[f(Xi)\nf(Yi)]\n,\nf\n\nn\n∈F\n\nX\ni=1\n-\n#\n\nby Jensen's inequality,\n\n= E\n\"\nsup\n,\nf\n\nn\n\n[f(Xi)\nf(Yi)]\nn\n∈F\nX\ni=1\n-\n#\n\nas σi[f(Xi) -f(Yi)] and f(Xi)\n\n-f(Yi) have the same distribution,\n\n= E\n\"\nsup\nf∈F\n\nX\nn\n[(f(Xi) -E[f(Xi)]) -(f(Yi)\nE\nn i=1\n-\n[f(Yi)])]\n#\n\n≤2E\n\"\nsup\n\nX\nn\n\n[f(Xi)\n(\n\nf\nn\n∈F\ni=1\n-E[f X\n\ni)]]\n#\nby the triangle inequality.\n,\n(2) The distribution of gi is the same as that of |gi|σi, so we can write\nE\n\"\nn\nn\nsup\nX\ngif(Xi)\ni\ng\ni\n#\n= EXi,σ\nf\n\"\nEgi\n\"\nsup\ni\nf\n=1\nX\ni=1\n|\n|σif(Xi) | Xi, σi\n∈F\n∈F\n##\n≥EXi,σi\n\"\nn\nsup\n[\nf\nX\nE\ni=1\n|gi|]σif(Xi)\n∈F\n#\n,\nby Jensen's inequality,\nr\nn\n=\nE\n\"\nsup\nX\nσif(Xi)\n#\n,\nπ\nf∈F i=1\nusing the first absolute moment of the standard Gaussian.\nDate: Fall 2015.\n\n18.657 PS 2 SOLUTIONS\n(3) We compute:\nRn(F + h) = E\n\"\nsup\nX\nn\n\nσi(f(Xi) + h(Xi))\nf\nn\n∈F\ni=1\n#\nn\n\nn\n≤E\nsup\nσ\n\n\"\nX\n\nif(Xi)\nn\n∈F\ni=1\n#\n\n+ E\nf\n\"\nn\n\nX\n\nh(Xi)\n,\ni=1\n#\n\nby the triangle inequality,\nn\n= Rn(F) + E\n\"\nn\n\n#\nX\n\nh(Xi)\ni=1\n,\nn\n≤Rn(F) +\n\nv\nu\nu\nu\ntE\nn\n\n!\nX\nσih(Xi)\ni=1\n,\nby Jensen's inequality,\n= Rn(F) + n\nv\nu\nuX\nn\nt\nE[h(Xi)2] + 2\nE[σiσjh(Xi)h(Xj)]\nv\ni=1\nX\ni<j\n= Rn(F) +\nu\nu n\ntX\nE[h(X\ni) ],\nn\ni=1\nby symmetry,\n≤Rn(F) + n\np\nn∥h∥2inf\n= Rn(F) + ∥h∥inf.\nn\n(4) By the triangle inequality, we have\nn\nk\nRn(F1 + . . . + Fk) = E\n\nsup\n\nX\nj(Xi\nfj∈Fj n\n\nσi\nX\nf\n)\n\nX\nk\n\nE\n\n≤\n\nj=1\n\"\ni=1\nj=1\nsup\nfj∈Fj n\nX\nn\n\nσifj(Xi)\ni=1\n#\nk\n\n=\n\nX\nRn(\nj=1\nFj).\n\n18.657 PS 2 SOLUTIONS\n(5) The supremum over different choices of fj is at least the supremum over a single repeated\nchoice:\nn\nk\nRn(F| + .{z. . + F}) = E\n\nsup\nσi\nfj(Xi)\nfj\nn\n∈F\n\ni=1\nX\nj=1\n\nk\nX\n\nn\nk\n\n≥E\n\nsup\n\nf\nn\n∈F\nX\n\nσi\ni=1\nX\nf(Xi)\n\"\n\nj=1\n\nσ\n\n= kE\nsup\nX\nn\n\nif(Xi)\nf\nn\n\n∈F\ni=1\n#\n= kRn(F),\n\nwhich provides the reverse bound to that of (4), establishing\n\nequality in this case.\n\n18.657 PS 2 SOLUTIONS\n2. Problem 2\n(1) Let A be a maximum 2ε-packing of (T, d); let B be a minimum ε-covering. As B is an ε-\ncovering, for each a ∈A there exists some choice of b(a) ∈B such that d(a, b(a)) ≤ε. This\nmap b : A →B is an injection: by the triangle inequality, no point in B can be within ε of two\npoints of A, as these are at least 2ε apart. Thus |A| ≤|B|, so that D(T, d, 2ε) ≤N(T, d, ε).\nLet C be maximum ε-packing. Note that C is in fact an ε-covering: if some point of T\nhad distance greater than ε to each point of C, then we could add it to C to produce a larger\nε-packing, contradicting maximality. It follows that N(T, d, ε) ≤D(T, d, ε).\n(2) Let Bn denote the Euclidean ball in Rn.\nLet u ∈Bn and v ∈Bm.\nThen we can write\nu = u + ε\n∗\nu, v = v + ε\n∗\nv, where u∗∈Nn, v∗∈Nm, εu ∈\nBn, and εv\n∈1Bm. We compute:\n∥M∥=\nsup\nu⊤Mv\nu∈Bn,v∈Bm\n=\nsup\nu⊤Mv + ε\n∗\n⊤\nu Mv + u⊤Mε\nu ,v ,ε ,εv\n∗\n∗\nv\n∗\n∗\nu\n≤\n\nsup\nu⊤Mv\n\n+\n\nsup\nε⊤\nu⊤\nu ∈Nn,v∗∈N\n∗\nm\n∗\n∗\nu Mv\nn\n∗\n!\n+\n\nsup\nMεv\nn\n\nεu∈\nB ,v ∈Nm\nu∈B ,ε ∈1\nm\n∗\nv\nB\n!\n≤\nmax\nu⊤Mv\n+\n∥M∥+\n∥M∥.\nu\n∗\n∈Nn,v∗∈Nm\n∗\n∗\nRearranging, we have\n∥M∥≤2\nmax\nu⊤Mv\nu∗∈Nn,v∗∈Nm\n∗\n∗\nas desired.\n(3) By independence and Hoeffding's lemma, we have, for all s > 0,\nE[exp(s u⊤Mv)] =\nY\nE[exp(s uiMijvj)]\ni,j\n≤\nY\nE\ns\ni,j\n\nexp\n\n2 u2\ni v2\nj\n\n= E\n\nexp\n\n1s2\nX\nu2\ni vj\ni,j\n\n≤exp(s2/2),\nwhenever u ∈Bn and v ∈Bm.\nd\nRecall from the notes that Bd has covering number at most\n, so that we are max-\nε\nimizing over |Nn × Nm| ≤12n+m points. It follows from the standard maximal inequality\nfor subgaussian random variables, or else by explicitly using Jensen\n\nwith log and exp and\nreplacing a maximum by a sum, that\nE\n√\n∥M∥≤2\np\n2 log(12m+n) ≤C(\nm + √n).\n\n18.657 PS 2 SOLUTIONS\n3. Problem 3\n(1) Let A be an ε-net for [0, 1]; we can construct one with size at most\n1 + 1. Let Xpre denote\n2ε\nthe set of non-decreasing functions from {x1, . . . , xn, } to A, and let X be the set of functions\n[0, 1] →[0, 1] defined by piecewise linear extension of functions in Xpre (and constant extension\non [0, x1] and [xn, 1]). It is straightforward to see that X is an ε-net for (F, dx ): given f\n,\ninf\n∈F\nwe define g ∈Xpre by taking g(xi) to be the least point of A lying within ε of f(xi), and then\nthe function in X defined by extension of g is within ε of f.\nIt remains to count X. Let a1 < . . . < ak be the elements of A, where k ≤\n1 +1. Functions\n2ε\nf ∈X are uniquely defined by the count for each 1 ≤i ≤k of how many xj satisfy f(xj) = ai.\nA naive count of the possibilities for these values yields\nN(F, dx , ε) ≤|X| ≤(n + 1)1+1/2ε\nn\ninf\n≤\n2ε,\nvalid for n ≥3.\n(2) As N(F, dx\n2, ε) ≤N(F, dx , ε), and f\n1 for all f\n, the chaining bound yields\ninf\n| | ≤\n∈F\nRn ≤inf 4ε + √\nZ 1 q\nlog N(F, dx\nd\nn\n2, t) t\nε>\nε\n≤inf 4ε +\nε>0\n√\nt\nn\nZ\nε\np\n-1 log n dt\nlog n\n= inf 4ε + 24\nε>0\nr\n(1\n√\n-\nε)\n≤lim 4ε + 24\nε→0\nr n\nlog n(1\n√\nn\n-\nε)\n= 24\nr\nlog n.\nn\n(3) We bound N(F, dx\n1, ε) ≤N(F, dx , ε)\ninf\n≤n2/ε\nr\n. The theorem in Section 5.2.1 yields\n2 log(2N(\nRn ≤inf ε +\nF, dx\n1, ε))\nε\nn\n2ε-1 log n + 2 log 2\n≤inf ε +\nε\nr\n.\nn\nSetting the two terms equal, to optimize over ε (in asymptotics), we have\nε3/2\n2 log n + 2ε log 2\n2 log n\n=\nn\n≈\n,\nn\nfor a bound of\nr\nr\nRn ≲\nlog n\nn\n1/3\n,\nwhich is strictly weaker than the chaining bound.\n\n18.657 PS 2 SOLUTIONS\n4. Problem 4\n(1) We adapt the proof from class to the case of a penalized, rather than constrained, norm.\n\nLet Wn be the span of the functions k(xi, -). We can decompose any function g uniquely\n\nas g = gn + g⊥, where gn ∈Wn and g⊥⊥Wn. As in class, g⊥(xi) = ⟨g⊥, k(xi, -)⟩= 0, so\nthat g(xi) = gn(xi).\nPlugging in to the objective function:\nψ(Y -g) + μ∥g∥2\nW = ψ(Y -gn) + μ∥g\nn∥W + μ∥g⊥∥W .\nFor any fixed gn, this is a constant plus μ∥g⊥∥2\nW , which is minimized uniquely at g⊥= 0.\nˆ\n\nThus (unfixing gn) any minimizer\nP\nf must lie in Wn, as desired.\nˆ\n(2) As f ∈\nˆ\nn\nˆ\nWn, write f =\ni=1 θik(xi, -), so that f^ = Kθ and ∥f∥2\nW = θ⊤Kθ.\nThen the\nfirst-order optimalit\n\ny conditions read\n0 = ∇φ(Y -^f) + μ∥ˆf∥2\nW\n= ∇\n\nY⊤Σ-1/2Y\n\n-2Y⊤Σ-1/2Kθ + θ⊤KΣ-1/2Kθ + μθ⊤Kθ\n=\n\n-KΣ-1/2Y + 2KΣ-1/2Kθ + 2μKθ.\nRearranging, and recalling that Kθ = ^f, we have\n(KΣ-1/2 + μIn)^f = KΣ-1/2Y,\nas desired.\n(3) As above, it suffices to consider f, g ∈ Wn, so that we can write f = Kθf, g = Kθg. From\nˆ\nthe definition of f, we have\nψ(f + ξ -^f) + μ∥ˆf∥2\nW ≤ψ(f + ξ -g) + μ∥g∥2\nW .\nSeparating out the contribution of ξ, we obtain\nψ(f -^f) + φ(ξ) + ξ⊤Σ-1/2(f -^f\nˆ\n) + μ∥f∥2\nW ≤ψ(f -g) + ψ(ξ) + ξ⊤Σ-1/2(f -\ng) + μ∥g∥W .\nRearranging,\nψ(f -^f) ≤-ξ⊤Σ-1/2(f -^f\nˆ\n) -μ∥f∥2\nW + ψ(f -) + ξ⊤Σ-1/2\ng\n(f -g) + μ∥g∥2\nW\n= ψ(f -g) + 2μ∥g∥2 + ξ⊤Σ-1/2(f^\nˆ\nW\n-g) -μ∥f∥2\nW -μ∥g∥2\nW\nμ\n≤ψ(f -g) + 2μ∥\nˆ\ng∥2\nW + ξ⊤Σ-1/2(^f -g) -2 ∥f -g∥2\nW ,\nby the inequality ∥a -b∥2\n≤2∥a∥2\n+ 2∥b∥2 . Let Z = Σ-1/2\nW\nW\nW\nξ, which is distributed as\nN(0, I). Continuing the line of algebra,\nμ\nψ(f -^f) ≤\nˆ\nψ( -g) + 2μ∥g∥2\nf\nW + Z⊤(^f -g) -2 ∥f -g∥2\nW\n= ψ(f -g) + 2μ∥g∥2\nμ\nW + Z⊤K(θ ˆ\nθg)\n(θ ˆ\nθg) K(θ ˆ\nθg)\nf -\n-2\nf -\n⊤\nf -\n≤ψ(f -g) + 2μ∥g∥2\nW +\nZ⊤KZ,\nμ\nby the inequality\nμ\na⊤Pb ≤\na⊤Pa +\nb⊤Pb\nμ\nfor any PSD matrix P. Now since\nn\nZ⊤KZ =\nX\nn\nZi⟨k(xi, -), k(xj,\ni,j\n-)⟩W =\n=1\n∥\nX\nZik(xi,\ni=1\n-)∥2\nW ,\n\nwe conclude by taking the infimum over g ∈Wn, which equals the infimum over g ∈W.\n\n18.657 PS 2 SOLUTIONS\n(4) This follows from the previous part together with the observation\nE\n\nX\nn\nn\nZik(xi, -)\n\n= E X\nZ\niZjKij\ni=1\nW\ni,j\n\n(5) It is sufficient to prove the bound for f\nWn, since\n=\nX\nE[Zi ]Kii = Tr(K).\ni=1\n∈\nonly its evaluations at the design points\nmatter. For the Gaussian kernel we have k(x, x) = 1, so that Tr(K) = n. Applying the\nprevious part, and taking the case g = f as an upper bound on the minimizer, we have\nn\nn\nE[ψ(f -^f)] ≤ψ(f -f) + 2μ∥f∥2\nW +\n= 2μ\n.\nμ\n∥f∥2\nW + μ\nTaking μ = ∥f∥W\np\nn/2 gives the result.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/b7fec9284ff39b01661e87e0153b1cdc_MIT18_657F15_PS3.pdf",
      "content": "18.657. Fall 2105\nRigollet\nNovember 6, 2015\nProblem set #3 (due Wed., November 11)\nShould be typed in LAT X\nE\nProblem 1. Kernels\nLet k1 and k2 be two PSD kernels on a space X .\n1. Show that the kernel k defined by k(u, v) = k1(u, v)k2(u, v) for any u, v ∈X is PSD.\n[Hint:\nconsider the Hadamard product between eigenvalue decompositions\nof the Gram matrices associated to k1 and k2].\n2. Let g : C →IR be a given function. Show that the kernel k defined by k(u, v) =\ng(u)g(v) is PSD.\n3. Let Q be a polynomial with nonnegative coefficients. Show that the kernel k defined\nby k(u, v) = Q(k1(u, v)) for any u, v ∈X is PSD.\n4. Show that the kernel k defined by k(u, v) = exp(k1(u, v)) for any u, v ∈X is PSD.\n[Hint:\nuse series expansion].\n5. Let X = IRd and ∥· ∥denote the Euclidean norm on IRd. Show that the kernel k\ndefined by k(u, v) = exp(-∥u -v∥2) is PSD.\nProblem 2. Convexity and Projections\n1. Give an algorithm that computes projections on the set\nC =\n\nx ∈IRd : max |xi| ≤1\n1≤i≤d\n\nand prove a rate of convergence.\n2. Give an algorithm that computes projections on the set\nd\n∆=\n\nx ∈IRd :\nX\nxi = 1 , xi ≥0\n\ni=1\nand prove a rate of convergence.\npage 1 of 3\n\n3. Recall that the Euclidean norm on n×n real matrices is also known as the Frobenius\nnorm and is defined by ∥M∥2 = Trace(M⊤M). Let Sn be the set of n×n symmetric\nmatrices with real entries. Let S+\nn denote the set of n×n symmetric positive definite\nmatrices with real entries, that is M ∈Sn if and only if M ∈Sn and\nx⊤Mx ≥0,\n∀x ∈IRn .\n(a) Show that S+\nn is convex and closed.\n(b) Give an explicit formula for the projection (with respect to the Frobenius norm)\nof a matrix M ∈Sn onto S+\nn\n4. Let C ⊂IRd be a closed convex set and for any x ∈IRd denote by π(x) its projection\nonto C. Show that for any x, y ∈IRd, it holds\n∥π(x) -π(y)∥≤∥x -y∥\nwhere ∥· ∥denotes the Euclidean norm.\nShow that for any y ∈C,\n∥π(x) -y∥≤∥x -y∥,\nProblem 3. Convex conjugate\nFor any function f : D ⊂IRd →IR, define its convex conjugate f ∗by\nf ∗(y) = sup\ny⊤x -f(x)\nx∈\n\n.\n(1)\nC\nThe domain of the function f ∗is taken to be the set D = {y ∈IRd : f ∗(y) < inf}.\n1. Find f ∗and D if\n(a) f(x) = 1/x, C = (0, inf),\n(b) f(x) = 1\n(c) f(x) = log Pd\nj=1 exp(xj), x = (x1, . . . , xd), C = IRd.\nLet f be strictly convex and differentiable and that C = IRd.\n2. Show that f(x) ≥f ∗∗(x) for all x ∈C.\n3. Show that the supremum in (1) is attained at x∗such that ∇f(x∗) = y.\n4. Recall that Df(·, ·) denotes the Bregman divergence associated to f. Show that\nDf(x, y) = Df∗∇f(y), ∇f(x)\n\npage 2 of 3\n|x|2\n2, C = IRd,\n\nProblem 4. Around gradient descent\nIn what follows, we want to solve the constrained problem:\nmin f(x) .\nx∈C\nwhere f is a L-Lipschitz convex function and C ⊂IRd is a compact convex set with\ndiameter at most R (in Euclidean norm). Denote by x∗a minimum of f on C.\n1. Assume that we replace the updates in the projected gradient descent algorithm by\ngs\nys+1 = xs -η\n,\ngs ∈∂f(xs) .\n∥gs∥\nxs+1 = πC(ys+1) ,\nwhere πC(·) is the projection onto C.\nWhat guarantees can you prove for this algorithm under the same assumptions?\n2. Consider the following updates:\nys ∈argmin ∇f(xs)⊤y\ny∈C\nxs+1 = (1 -γs)xs + γsys ,\nwhere γs = 2/(s + 1).\nIn what follows, we assume that f is differentiable and β-smooth:\nβ\nf(y) -f(x) ≤∇f(x)⊤(y -x) +\n|y -x|2\n2 .\n(a) Show that\nβ\nf(x\n∗\ns+1) -f(xs) ≤γs(f(x ) -f(xs)) +\nγ2\nsR\n(b) Conclude that for any k ≥2,\nf(xk) -f(x∗\n2βR\n) ≤\n.\nk + 1\npage 3 of 3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mathematics of Machine Learning Assignment 3 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/285d7726dad0e91cf447da611b84e6d6_MIT18_657F15_PS3_Sol.pdf",
      "content": "18.657 PS\nSOLUTIONS\n1. Problem 1\n(1) Symmetry is clear. Let K1 and K2 be the PSD Gram matrices of k1 and k2, respectively.\nThen the Gram matrix K of k is simply the Hadamard (or Schur) product K1 - K2; we wish\nto see that this is PSD.\nThe Kronecker product K1 ⊗K2 is PSD: its eigenvalues are simply the pair products of an\neigenvalue of K1 with an eigenvalue from K2, as is easily seen from the identity\n(K1 ⊗K2)(v ⊗w) = (K1v) ⊗(K2w)\nwhen v and w are eigenvectors of K1 and K2, respectively.\nNow the Hadamard product\nK = K1 - K2 is a principal submatrix of A ⊗B, and a principal submatrix of a PSD matrix\nis PSD.\n(There are many good approaches to this part.)\n(2) Treating g as a real vector indexed by C, we have K = gg⊤, and a matrix of this form is\nalways PSD.\n(3) The Gram matrix K of k is simply Q(K1), where K1 is the Gram matrix of k1, and where the\nmultiplication in the polynomial is Hadamard product. From (1), the PSD matrices are closed\nunder Hadamard product, and it is a common fact that they are closed under positive scaling\nand addition; thus they are closed under the application of polynomials with non-negative\ncoefficients.\n(4) Let Tr(x) be the rth Taylor approximation to exp(x) about 0, a polynomial with non-negative\ncoefficients. Then Tr(k1) converges to k = exp(k1) as r →inf; equivalently, Tr(K1) converges\nto K = exp(K1), where K and K1 are the Gram matrices of k and k1. Here exp is the\nentry-wise exponential function, not the \"matrix exponential\".. From (3), Tr(K1) is PSD. As\nthe PSD cone is a closed subset of\nn\nR ×n (it's defined by non-strict inequalities x⊤Mx ≥0),\nthe limit K is PSD, and this is the Gram matrix of K.\n(5) Applying (2) to the function f(u) = exp(-∥u∥2), the kernel k1(u, v) = exp(-∥u∥2 -∥v∥2)\nis PSD. Moreover the kernel k2(u, v) = 2u⊤v is PSD: if C is a set of vectors, and we let the\nmatrix U be defined by taking the elements of C as columns, then the Gram matrix of k2 is\n2U ⊤U which is PSD. By (4), the kernel k\n⊤\n3(u, v) = exp(2u v) is PSD. By (1), the kernel\nk(u, v) = k1(u, v)k3(u, v) = exp(-∥u∥2 + 2u⊤v -∥v∥2) exp(-∥u -v∥2)\nis PSD.\nDate: Fall 2015.\n\n18.657 PS 2 SOLUTIONS\n2. Problem 2\nn\n(1) Suppose x ∈\nd\nR ; we wish to find y ∈C minimizing ∥x -y∥2 = P\ni=1(xi -yi) . As the\nconstraints defining C apply to each yi separately, the problem amounts to finding, for each\ni, a value -1 ≤yi ≤1 minimizing (x\ni -yi) . This is clearly achieved at\ni\ny =\n(\nxi/|x\ni\n|\nif |xi| > 1,\nxi\notherwise.\nThis formula is exact, so there is no convergence issue; effectively the method converges\nperfectly after one update on each coordinate.\n(2) Let z ∈\nd\nR\nbe given; we want x ∈∆minimizing the l2 distance f(x) = ∥x -z∥. We apply\nmirror descent, following the corollary on page 7 of the Lecture 13 notes.\nThe objective\nis clearly 1-Lipschitz, with gradient ∇f(x) = (x -z)/∥x -z∥, so we obtain the following\nconvergence guarantee at iteration k:\nf(x*\nk) -f(x∗) ≤\nr\n2 log d.\nk\n(3)\n(a) §+\nn is convex: certainly any convex combination of symmetric matrices is symmetric, and\nif A, B ∈§+\nn , then\nx⊤(λA + (1 -λ)B)x = λx⊤Ax + (1 -λ)x⊤Bx\nis a convex combination of non-negative reals, thus non-negative, so a convex combination\nof PSD matrices is PSD.\n§+\nn is closed in\nn×n\nR\nas it is defined as an intersection of a linear subspace (the symmetric\nmatrices) and the half-spaces ⟨M, v⊤v⟩≥0 for v ∈\nn\nR , all of which are closed; an\nintersection of closed sets is closed.\n(b) Let A ∈§n. As §+\nn is convex and closed, and the function f(B) = 1\n2∥A-B∥2\nF is convex, a\nmatrix B minimizes f over S+\nn iffit satisfies first-order optimality. Specifically, we must\nhave that ∇f(B) = P\ni μ\n⊤\nixixi , for some μi ≥0 and some xi for which B is tight for the\nconstraint x⊤\ni Bxi ≥0 defining S+\nn .\nWe compute the gradient:\n∇(\nA\n2∥\n-B∥2\nF) = ∇\ntr(A2 -2AB + B2) = B\n-A.\nThus, if we can write B -A =\ni μixix⊤\ni as above, then we certify B as optimal.\nWrite the eigendecomposition A = UΣU ⊤, and let B = UΣ+U ⊤, where Σ+ replaces the\nnegative entries of Σ by zero. T\nP\nhen\nB -A = U(Σ\n-Σ)U ⊤\nΣ\n⊤\n+\n=\ni:Σ\nX\n(\nii)UiUi ,\nii<0\n-\nand we have U ⊤\ni BUi = (Σ+)ii = 0, thus fulfilling first-order optimality.\n(4) We begin with the first claim. In class, we proved that\n⟨π(x) -x, π(x) -z⟩≤0,\nwhenever z ∈C. Applying this with z = π(y), we have\n⟨π(x) -x, π(x) -π(y)⟩≤0.\nSumming a copy of this inequality with the same thing with x and y reversed, we have\n⟨π(x) -π(y) -(x -y), π(x) -π(y)⟩≤0,\nπ(x) -π(y)∥2 ≤⟨x -y, π(x) -π(y)⟩≤∥x -y∥∥π(x) -π(y)∥,\nby Cauchy-Schwartz. Cancelling ∥π(x) -π(y)∥from both sides yields the first claim.\nThe second claim follows by specializing to the case y ∈C, for which π(y) = y.\n\n18.657 PS 2 SOLUTIONS\n3. Problem 3\n(1)\n(a) By definition, f ∗(y) = supx>0 xy -1. When y > 0, a sufficiently large choice of x makes\nx\nthe objective arbitrarily large, and the supremum is infinite. When y ≤0, the objective is\nbounded above by 0; for y = 0, this is achieved as x →inf, whereas for y < 0, first-order\noptimality conditions show that the optimum is achieved at x = (-y)-1/2, at which\nf ∗(y) = -2√-y. We have D = (-inf, 0].\n(b) By definition, f ∗(y) = sup\nx∈\nd y⊤\nR\nx -\n2|x|2. The gradient of the objective is y -x, so\nfirst-order optimality conditions are satisfied at x = y, and we have f ∗(y) = 1\n2|x|2 (f is\nself-conjugate). Here D =\nd\nR .\n⊤\nd\n(c) By definition, f ∗(y) = supx∈Rd y x -log P\nj=1 exp(xj). The partial derivative in xi of\nthe objective function is\nexp xi\nyi -\n,\nj exp xj\nso the gradient may be made zero w\nP\nhenever y lies in the simplex ∆, by taking xi = log yi.\nFor such y, we thus have f ∗(y) =\ni yi log yi.\nWe next rule out all y ∈∆, so that\nP\nD = ∆. Consider x of the form (λ, λ, . . . , λ), for which\nthe objective value is λ\ni yi -λ -log d. When\ni yi = 1, this can be made arbitrarily\nlarge, by taking an extreme value of λ. On the other hand, if any coordinate yi of y is\nnegative, then taking x\nP\nto be supported only on t\nP\nhe ith coordinate, the objective value is\nyixi -log((d -1) + expxi), which is arbitrarily large when we take xi to be a sufficiently\nlarge negative number. So y must lie in the simplex ∆.\n(2) For all x ∈C and y ∈D, we have f ∗(y) ≥y⊤x -f(x), so that y⊤x -f ∗(y) ≤f(x). Thus\nf ∗∗(x) = supy∈D y⊤x\nd\nR\n-f ∗(y) ≤f(x).\n(3) Here C =\n, so that the supremum is either achieved in some limit of arbitrarily distant\npoints, or else at at point satisfying first-order optimality. The first case can actually occur,\ne.g. when d = 1, f(x) = -exp(x), and y = 0. In the other case, first-order optimality is that\n0 = ∇x(y⊤x -f(x)) = y\n∗\n-∇f(x),\nso that ∇f(x ) = y.\n(4) We will need the gradient of f ∗. As f is strictly convex, y = ∇f(x) is an injective function of\nx, so we can write x = (∇f)-1(y). Then\nf ∗(y) = y⊤x∗-f(x∗)\n= y⊤(∇f)-1(y) -f((∇f)-1(y)),\n∇f ∗(y) = (∇f)-1(y) + D(∇f)-1(y)[y]\n-1\n-D(∇f)-1(y)[∇f((∇f)-1)(y))]\n= (∇f)\n(y),\nwhere Df(a)[b] denotes the Jacobian of f, taken at a, applied to the vector b.\nWe now compute the Bregman divergence:\nDf ∗(∇f(y), ∇f(x)) = f ∗(∇f(y)) -f ∗(∇f(x)) -(∇f ∗(∇f(x)))⊤(∇f(y) -∇f(x))\n= ∇f(y)⊤y -f(y) -∇f(x)⊤x + f(x) -x⊤(∇f(y) -∇f(x))\n= f(x) -f(y) -(∇f(y))⊤(x -y)\n= Df(x, y).\n\n18.657 PS 2 SOLUTIONS\n4. Problem 4\n(1) Adapting the proof of convergence of projected subgradient descent from the lecture notes,\nwe have:\nf(xs) -f(x∗) ≤g⊤\ns (xs -x∗)\n= ∥gs∥(xs\nη\n-ys+1)⊤(xs -x∗)\n∥g\n≤\ns∥(\n2η\n∥xs -ys+1∥2 + ∥xs -x∗∥2 -∥ys+1 -x∗∥2)\nη\n=\n∥gs∥\ng\n+ ∥s∥\n(\n2η\n∥xs -x∗∥2 -∥ys+1 -x∗∥2),\nas xs -ys+1 has norm η,\nη∥g\n≤\ns∥\ng\n+ ∥s∥\n(\ns\nη\n∥xs -\n∗∥2 -∥xs+1\n-x∗∥2),\nas the projection operator is a contraction.\nSumming over s, and using the bounds ∥x1 -x∗∥≤R and ∥gs∥≤L, we have\n∗\nηL\nLR2\nf(x s) -f(x ) ≤\n+\n.\n2ηk\nTaking η = R/\n√\nk, we obtain the rate LR/\n√\nk.\n(2)\n(a) Starting from the definition of β-smoothness:\nβ\nf(xs+1) -f(xs) ≤∇f(xs)⊤(xs+1 -xs) + 2 ∥xs+1 -xs∥2\nβ\n= γs∇f(x\n⊤\ns) (ys -xs) +\nγ2\ns∥ys -xs∥2\n⊤\n∗\nβ\n≤γs∇f(xs) (x -xs) +\nγ2R2,\ns\nas f(xs)⊤y\n⊤\ns ≤f(xs) y for all y ∈C,\nβ\n≤γs(f(x∗) -f(xs)) +\nγ2\nsR2,\nby convexity.\n(b) We induct on k. Continuing from the inequality above by subtracting f(x∗)-f(xs) from\nboth sides, we have\nβ\nf(xs+1) -f(x∗) ≤(1 -γs)(f(xs) -f(x∗)) +\nγ2\nsR2,\nor, if we define δs = f(x\n∗\ns) -f(x ),\nβ\nδs+1 ≤(1 -γs)δs +\nγ2\nsR2.\n\n18.657 PS 2 SOLUTIONS\nSpecializing to s = 1, and noting that γ1 = 1, we have that δ2 = βR2/2 ≤2βR2/3, so\nthat the base case of k = 2 is satisfied. Now proceeding inductively, we have\nβ\nδk ≤(1 -γk-1)δk-1 +\nγ2\nk-1R\n2(k -2)βR2\n2βR2\n≤\nk2\n+\nk2\n,\nby induction and the definition of γk-1,\n2(k -1)βR2\n=\nk2\n2βR2\n≤\n,\nk + 1\ncompleting the induction.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/81406c87dccb9e873cfafa876a4d69c3_MIT18_657F15_LecNote.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Philippe Rigollet\nSep. 9, 2015\n1. WHAT IS MACHINE LEARNING (IN THIS COURSE)?\nThis course focuses on statistical learning theory, which roughly means understanding the\namount of data required to achieve a certain prediction accuracy. To better understand\nwhat this means, we first focus on stating some differences between statistics and machine\nlearning since the two fields share common goals.\nIndeed, both seem to try to use data to improve decisions. While these fields have evolved\nin the same direction and currently share a lot of aspects, they were at the beginning quite\ndifferent. Statistics was around much before machine learning and statistics was already\na fully developed scientific discipline by 1920, most notably thanks to the contributions of\nR. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for\nstatistical inference. However, MLE requires essentially knowing the probability distribution\nfrom which the data is draw, up to some unknown parameter of interest. Often, the unknown\nparameter has a physical meaning and its estimation is key in better understanding some\nphenomena. Enabling MLE thus requires knowing a lot about the data generating process:\nthis is known as modeling. Modeling can be driven by physics or prior knowledge of the\nproblem. In any case, it requires quite a bit of domain knowledge.\nMore recently (examples go back to the 1960's) new types of datasets (demographics,\nsocial, medical,. . . ) have become available. However, modeling the data that they contain\nis much more hazardous since we do not understand very well the input/output process\nthus requiring a distribution free approach. A typical example is image classification where\nthe goal is to label an image simply from a digitalization of this image. Understanding\nwhat makes an image a cat or a dog for example is a very complicated process. However,\nfor the classification task, one does not need to understand the labelling process but rather\nto replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1).\ninput X\noutput Y\nblackbox\ny = f(x) + ε\ninput X\noutput Y\nFigure 1: The machine learning blackbox (left) where the goal is to replicate input/output\npairs from past observations, versus the statistical approach that opens the blackbox and\nmodels the relationship.\nThese differences between statistics and machine learning have receded over the last\ncouple of decades. Indeed, on the one hand, statistics is more and more concerned with\nfinite sample analysis, model misspecification and computational considerations. On the\nother hand, probabilistic modeling is now inherent to machine learning. At the intersection\nof the two fields, lies statistical learning theory, a field which is primarily concerned with\nsample complexity questions, some of which will be the focus of this class.\n\n2. STATISTICAL LEARNING THEORY\n2.1 Binary classification\nA large part of this class will be devoted to one of the simplest problem of statistical learning\ntheory: binary classification (aka pattern recognition [DGL96]). In this problem, we observe\n(X1, Y1), . . . , (Xn, Yn) that are n independent random copies of (X, Y ) ∈X ×{0, 1}. Denote\nby PX,Y the joint distribution of (X, Y ). The so-called feature X lives in some abstract\nspace X (think IRd) and Y ∈{0, 1} is called label. For example, X can be a collection of\ngene expression levels measured on a patient and Y indicates if this person suffers from\nobesity.\nThe goal of binary classification is to build a rule to predict Y given X using\nonly the data at hand. Such a rule is a function h : X →{0, 1} called a classifier. Some\nclassifiers are better than others and we will favor ones that have low classification error\nR(h) = IP(h(X) = Y ). Let us make some important remarks.\nFist of all, since Y ∈{0, 1} then Y has a Bernoulli distribution: so much for distribution\nfree assumptions! However, we will not make assumptions on the marginal distribution of\nX or, what matters for prediction, the conditional distribution of Y given X. We write,\nY |X ∼Ber(η(X)), where η(X) = IP(Y = 1|X) = IE[Y |X] is called the regression function\nof Y onto X.\nNext, note that we did not write Y = η(X). Actually we have Y = η(X) + ε, where\nε = Y -η(X) is a \"noise\" random variable that satisfies IE[ε|X] = 0. In particular, this noise\naccounts for the fact that X may not contain enough information to predict Y perfectly.\nThis is clearly the case in our genomic example above: it not whether there is even any\ninformation about obesity contained in a patient's genotype. The noise vanishes if and only\nif η(x) ∈{0, 1} for all x ∈X. Figure 2.1 illustrates the case where there is no noise and the\nthe more realistic case where there is noise. When η(x) is close to .5, there is essentially no\ninformation about Y in X as the Y is determined essentially by a toss up. In this case, it\nis clear that even with an infinite amount of data to learn from, we cannot predict Y well\nsince there is nothing to learn. We will see what the effect of the noise also appears in the\nsample complexity.\n\nFigure 2: The thick black curve corresponds to the noiseless case where Y = η(X) ∈{0, 1}\nand the thin red curve corresponds to the more realistic case where η ∈[0, 1]. In the latter\ncase, even full knowledge of η does not guarantee a perfect prediction of Y .\nIn the presence of noise, since we cannot predict Y accurately, we cannot drive the\nclassification error R(h) to zero, regardless of what classifier h we use. What is the smallest\nvalue that can be achieved? As a thought experiment, assume to begin with that we have all\nx\nη(x)\n.5\n\nthe information that we may ever hope to get, namely we know the regression function η(·).\nFor a given X to classify, if η(X) = 1/2 we may just toss a coin to decide our prediction\nand discard X since it contains no information about Y . However, if η(X) = 1/2, we have\nan edge over random guessing: if η(X) > 1/2, it means that IP(Y = 1|X) > IP(Y = 0|X)\nor, in words, that 1 is more likely to be the correct label. We will see that the classifier\nh∗(X) = 1I(η(X) > 1/2) (called Bayes classifier) is actually the best possible classifier in\nthe sense that\nR(h∗) = inf R(h) ,\nh(·)\nwhere the infimum is taken over all classifiers, i.e. functions from X to {0, 1}. Note that\nunless η(x) ∈{0, 1} for all x ∈X (noiseless case), we have R(h∗) = 0. However, we can\nalways look at the excess risk E(h) of a classifier h defined by\nE(h) = R(h) -R(h∗) ≥0 .\nIn particular, we can hope to drive the excess risk to zero with enough observations by\nmimicking h∗accurately.\n2.2 Empirical risk\nThe Bayes classifier h∗, while optimal, presents a major drawback: we cannot compute it\nbecause we do not know the regression function η. Instead, we have access to the data\n(X1, Y1), . . . , (Xn, Yn), which contains some (but not all) information about η and thus h∗.\nIn order to mimic the properties of h∗recall that it minimizes R(h) over all h. But the\nfunction R(·) is unknown since it depends on the unknown distribution PX,Y of (X, Y ). We\nˆ\nestimate it by the empirical classification error, or simply empirical risk Rn(·) defined for\nany classifier h by\nn\nˆRn(h) =\nX\n1I(h(Xi) = Yi) .\nn i=1\nˆ\nˆ\nSince IE[1I(h(Xi) = Yi)] = IP(h(Xi) = Yi) = R(h), we have IE[Rn(h)] = R(h) so Rn(h) is\nan unbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have\nˆ\nˆ\nRn(h) →R(h) as n →inf, almost surely. This indicates that if n is large enough, Rn(h)\nshould be close to R(h).\nAs a result, in order to mimic the performance of h∗, let us use the empirical risk\nˆ\nˆ\nminimizer (ERM) h defined to minimize Rn(h) over all classifiers h. This is an easy enough\nˆ\nˆ\ntask: define h such h(Xi) = Yi for all i = 1, . . . , n and h(x) = 0 if x ∈/ {X1, . . . , Xn}. We\nˆ\nˆ\nhave Rn(h) = 0, which is clearly minimal. The problem with this classifier is obvious: it\ndoes not generalize outside the data. Rather, it predicts the label 0 for any x that is not in\nˆ\nˆ\nthe data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0.\nˆ\nIn particular it is unlikely that IE[R(h)] will be small.\n\nImportant Remark: Recall that R(h) = IP(h(X) = Y ).\nˆ\nˆ\nˆ\nIf h(·) = h({(X1, Y1), . . . , (Xn, Yn)} ; ·) is constructed from the data, R(h) denotes\nthe conditional probability\nˆ\nˆ\nR(h) = IP(h(X) = Y |(X1, Y1), . . . , (Xn, Yn)) .\nˆ\nˆ\nrather than IP(h(X) = Y ). As a result R(h) is a random variable since it depends on the\nrandomness of the data (X1, Y1), . . . , (Xn, Yn). One way to view this is to observe that\nˆ\nwe compute the deterministic function R(·) and then plug in the random classifier h.\nThis problem is inherent to any method if we are not willing to make any assumption\non the distribution of (X, Y ) (again, so much for distribution freeness!). This can actually\nbe formalized in theorems, known as no-free-lunch theorems.\nTheorem:\nˆ\nFor any integer n ≥1, any classifier h built from (X1, Y1), . . . , (Xn, Yn) and\nany ε > 0, there exists a distribution PX,Y for (X, Y ) such that R(h∗) = 0 and\nˆ\nIER(hn) ≥1/2 -ε .\nTo be fair, note that here the distribution of the pair (X, Y ) is allowed to depend on\nn which is cheating a bit but there are weaker versions of the no-free-lunch theorem that\nessentially imply that it is impossible to learn without further assumptions.\nOne such\ntheorem is the following.\nTheorem:\nˆ\nFor any classifier h built from (X1, Y1), . . . , (Xn, Yn) and any sequence\n{an}n > 0 that converges to 0, there exists a distribution PX,Y for (X, Y ) such that\nR(h∗) = 0 and\nˆ\nIER(hn) ≥an ,\nfor all n ≥1\nIn the above theorem, the distribution of (X, Y ) is allowed to depend on the whole sequence\n{an}n > 0 but not on a specific n. The above result implies that the convergence to zero of\nthe classification error may be arbitrarily slow.\n2.3 Generative vs discriminative approaches\nBoth theorems above imply that we need to restrict the distribution PX,Y of (X, Y ). But\nisn't that exactly what statistical modeling is? The is answer is not so clear depending on\nhow we perform this restriction. There are essentially two schools: generative which is the\nstatistical modeling approach and discriminative which is the machine learning approach.\nGenerative: This approach consists in restricting the set of candidate distributions PX,Y .\nThis is what is done in discriminant analysis1where it is assumed that the condition dis-\n1Amusingly, the generative approach is called discriminant analysis but don't let the terminology fool\nyou.\n\ntributions of X given Y (there are only two of them: one for Y = 0 and one for Y = 1) are\nGaussians on X = IRd (see for example [HTF09] for an overview of this approach).\nDiscriminative: This is the machine learning approach. Rather than making assumptions\ndirectly on the distribution, one makes assumptions on what classifiers are likely to perform\ncorrectly. In turn, this allows to eliminate classifiers such as the one described above and\nthat does not generalize well.\nWhile it is important to understand both, we will focus on the discriminative approach\nin this class. Specifically we are going to assume that we are given a class H of classifiers\nsuch that R(h) is small for some h ∈H.\n2.4 Estimation vs. approximation\nAssume that we are given a class H in which we expect to find a classifier that performs well.\nThis class may be constructed from domain knowledge or simply computational convenience.\nˆ\nWe will see some examples in the class. For any candidate classifier hn built from the data,\nwe can decompose its excess risk as follows:\nˆ\nˆ\nˆ\nE(hn) = R(hn) -R(h∗) = R(hn) -inf R(h) + inf R(h) -R(h∗) .\nh∈H\nh∈H\n|\nestimat\n{\nio\nz\nn error\n}\n|\napproxim\n{\na\nz\ntion error\n}\nOn the one hand, estimation error accounts for the fact that we only have a finite\namount of observations and thus a partial knowledge of the distribution PX,Y . Hopefully\nwe can drive this error to zero as n →inf. But we already know from the no-free-lunch\ntheorem that this will not happen if H is the set of all classifiers. Therefore, we need to\ntake H small enough. On the other hand, if H is too small, it is unlikely that we will\nfind classifier with performance close to that of h∗.\nA tradeoffbetween estimation and\napproximation can be made by letting H = Hn grow (but not too fast) with n.\nFor now, assume that H is fixed. The goal of statistical learning theory is to understand\nhow the estimation error drops to zero as a function not only of n but also of H. For the\nfirst argument, we will use concentration inequalities such as Hoeffding's and Bernstein's\ninequalities that allow us to control how close the empirical risk is to the classification error\nby bounding the random variable\n\nn\n\nX\n1I(h(X\n(h\n\ni) = Yi) -IP\n(X) = Y )\n\nn\n\ni=1\n\nwith high probability. More generally we will be interested in results that allow to quantify\nhow close the average of independent and identically distributed (i.i.d) random variables is\nto their common expected value.\nˆ\nˆ\nˆ\nIndeed, since by definition, we have Rn(h) ≤Rn(h) for all h ∈H, the estimation error\n\ncan be controlled as follows. Define h ∈H to be any classifier that minimizes R(·) over H\n(assuming that such a classifier exist).\nˆ\nˆ\n\nR(hn) -inf R(h) = R(hn) -R(h)\nh∈H\nˆ\nˆ\nˆ\n\nˆ\nˆ\n\n= Rn(hn) -Rn(h) +R(hn) -Rˆn(h ) + Rˆ\nn\nn(h) -R(h)\n|\n≤\n{z\n}\n≤\nˆ\nˆ\nˆ\nˆ\n\nRn(hn) -R(hn)\n+\nRn(h) -R(h)\n.\n\nˆ\n\nSince h is deterministic, we can use a concentration inequality to control\nRn(h) -R(h)\n.\nHowever,\nn\nˆ\nˆ\nˆ\nRn(hn) =\nX\n1I(hn(Xi) = Yi)\nn i=1\nis not\nˆ\nthe average of independent random variables since hn depends in a complicated\nmanner on all of the pairs (Xi, Yi), i = 1, . . . , n. To overcome this limitation, we often use\nˆ\na blunt, but surprisingly accurate tool: we \"sup out\" hn,\nˆ\nˆ\nˆ\nˆ\nˆ\nˆ\nRn(hn) -R(hn)\n≤sup\n\nh∈\nRn(hn) -R(hn)\n\nH\n.\nControlling this supremum falls in the scope of suprema of empirical processes that we will\nstudy in quite a bit of detail. Clearly the supremum is smaller as H is smaller but H should\nbe kept large enough to have good approximation properties. This is the tradeoffbetween\napproximation and estimation. It is also know in statistics as the bias-variance tradeoff.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Jonathan Weed\nSep. 14, 2015\nPart I\nStatistical Learning Theory\n1. BINARY CLASSIFICATION\nIn the last lecture, we looked broadly at the problems that machine learning seeks to solve\nand the techniques we will cover in this course. Today, we will focus on one such problem,\nbinary classification, and review some important notions that will be foundational for the\nrest of the course.\nOur present focus on the problem of binary classification is justified because both binary\nclassification encompasses much of what we want to accomplish in practice and because the\nresponse variables in the binary classification problem are bounded. (We will see a very\nimportant application of this fact below.) It also happens that there are some nasty surprises\nin non-binary classification, which we avoid by focusing on the binary case here.\n1.1 Bayes Classifier\nRecall the setup of binary classification: we observe a sequence (X1, Y1), . . . , (Xn, Yn) of n\nindependent draws from a joint distribution PX,Y . The variable Y (called the label) takes\nvalues in {0, 1}, and the variable X takes values in some space X representing \"features\" of\nthe problem. We can of course speak of the marginal distribution PX of X alone; moreover,\nsince Y is supported on {0, 1}, the conditional random variable Y |X is distributed according\nto a Bernoulli distribution. We write Y |X ∼Bernoulli(η(X)), where\nη(X) = IP(Y = 1|X) = IE[Y |X].\n(The function η is called the regression function.)\nWe begin by defining an optimal classifier called the Bayes classifier. Intuitively, the\nBayes classifier is the classifier that \"knows\" η--it is the classifier we would use if we had\nperfect access to the distribution Y |X.\nDefinition: The Bayes classifier of X given Y , denoted h∗, is the function defined by the\nrule\n(\nh\nif η x) > 1/2\n∗(x) =\nif η(x) ≤1/2.\nIn other words, h∗(X) = 1 whenever IP(Y = 1|X) > IP(Y = 0|X).\nOur measure of performance for any classifier h (that is, any function mapping X to\n{0, 1}) will be the classification error: R(h) = IP(Y = h(X)). The Bayes risk is the value\nR∗= R(h∗) of the classification error associated with the Bayes classifier. The following\ntheorem establishes that the Bayes classifier is optimal with respect to this metric.\n\nTheorem: For any classifier h, the following identity holds:\nR(h) -R(h∗) =\nZ\n|2η(x) -1| Px(dx) = IEX[|2η(X) -1|1(h(X) = h∗(X))]\n(1.1)\nh=h∗\nWhere h = h∗is the (measurable) set {x ∈X | h(x) = h∗(x)}.\nIn particular, since the integrand is nonnegative, the classification error R∗of the\nBayes classifier is the minimizer of R(h) over all classifiers h.\nMoreover,\nR(h∗) = IE[min(η(X), 1 -η(X))] ≤\n.\n(1.2)\nProof. We begin by proving Equation (1.2). The definition of R(h) implies\nR(h) = IP(Y = h(X)) = IP(Y = 1, h(X) = 0) + IP(Y = 0, h(X) = 1),\nwhere the second equality follows since the two events are disjoint. By conditioning on X\nand using the tower law, this last quantity is equal to\nIE[IE[1(Y = 1, h(X) = 0)|X]] + IE[IE[1(Y = 0, h(X) = 1)|X]]\nNow, h(X) is measurable with respect to X, so we can factor it out to yield\nIE[1(h(X) = 0)η(X) + 1(h(X) = 1)(1 -η(X))]],\n(1.3)\nwhere we have replaced IE[Y |X] by η(X).\nIn particular, if h = h∗, then Equation 1.3 becomes\nIE[1(η(X) ≤1/2)η(X) + 1(η(x) > 1/2)(1 -η(X))].\nBut η(X) ≤1/2 implies η(X) ≤1 -η(X) and conversely, so we finally obtain\nR(h∗) = IE[1(η(X) ≤1/2)η(X) + 1(η(x) > 1/2)(1 -η(X))]\n= IE[(1(η(X) ≤1/2) + 1(η(x) > 1/2)) min(η(X), 1 -η(X))]\n= IE[min(η(X), 1 -η(X))],\nas claimed. Since min(η(X), 1 -η(X)) ≤1/2, its expectation is also certainly at most 1/2\nas well.\nNow, given an arbitrary h, applying Equation 1.3 to both h and h∗yields\nR(h) -R(h∗) = IE[1(h(X) = 0)η(X) + 1(h(X) = 1)(1 -η(X))]\n-1(h∗(X) = 0)η(X) + 1(h∗(X) = 1)(1 -η(X))]],\nwhich is equal to\nIE[(1(h(X) = 0) -1(h∗(X) = 0))η(X) + (1(h(X) = 1) -1(h∗(X) = 1))(1 -η(X))].\nSince h(X) takes only the values 0 and 1, the second term can be rewritten as -(1(h(X) =\n0) -1(h∗(X) = 0)). Factoring yields\nIE[(2η(X) -1)(1(h(X) = 0) -1(h∗(X) = 0))].\n\nThe term 1(h(X) = 0) -1(h∗(X) = 0) is equal to -1, 0, or 1 depending on whether h\nand h∗agree. When h(X) = h∗(X), it is zero. When h(X) = h∗(X), it equals 1 whenever\nh∗(X) = 0 and -1 otherwise. Applying the definition of the Bayes classifier, we obtain\nIE[(2η(X) -1)1(h(X) = h∗(X)) sign(η -1/2)] = IE[|2η(X) -1|1(h(X) = h∗(X))],\nas desired.\nWe make several remarks. First, the quantity R(h) -R(h∗) in the statement of the\ntheorem above is called the excess risk of h and denoted E(h). (\"Excess,\" that is, above\nthe Bayes classifier.) The theorem implies that E(h) ≥0.\nSecond, the risk of the Bayes classifier R∗equals 1/2 if and only if η(X) = 1/2 almost\nsurely. This maximal risk for the Bayes classifier occurs precisely when Y \"contains no\ninformation\" about the feature variable X. Equation (1.1) makes clear that the excess risk\nweighs the discrepancy between h and h∗according to how far η is from 1/2. When η is\nclose to 1/2, no classifier can perform well and the excess risk is low. When η is far from\n1/2, the Bayes classifier performs well and we penalize classifiers that fail to do so more\nheavily.\nAs noted last time, linear discriminant analysis attacks binary classification by putting\nsome model on the data. One way to achieve this is to impose some distributional assump-\ntions on the conditional distributions X|Y = 0 and X|Y = 1.\nWe can reformulate the Bayes classifier in these terms by applying Bayes' rule:\nIP(X = x Y = 1)IP(Y = 1)\nη(x) = IP(Y = 1\n|\n|X = x) =\n.\nIP(X = x|Y = 1)IP(Y = 1) + IP(X = x|Y = 0)IP(Y = 0)\n(In general, when PX is a continuous distribution, we should consider infinitesimal proba-\nbilities IP(X ∈dx).)\nAssume that X|Y = 0 and X|Y = 1 have densities p0 and p1, and IP(Y = 1) = π is\nsome constant reflecting the underlying tendency of the label Y . (Typically, we imagine\nthat π is close to 1/2, but that need not be the case: in many applications, such as anomaly\ndetection, Y = 1 is a rare event.) Then h∗(X) = 1 whenever η(X) ≥1/2, or, equivalently,\nwhenever\np1(x)\n1 -π.\np0(x) ≥\nπ\nWhen π = 1/2, this rule amounts to reporting 1 or 0 by comparing the densities p1\nand p0. For instance, in Figure 1, if π = 1/2 then the Bayes classifier reports 1 whenever\np1 ≥p0, i.e., to the right of the dotted line, and 0 otherwise.\nOn the other hand, when π is far from 1/2, the Bayes classifier is weighed towards the\nunderlying bias of the label variable Y .\n1.2 Empirical Risk Minimization\nThe above considerations are all probabilistic, in the sense that they discuss properties of\nsome underlying probability distribution. The statistician does not have access to the true\nprobability distribution PX,Y ; she only has access to i.i.d. samples (X1, Y1), . . . , (Xn, Yn).\nWe consider now this statistical perspective. Note that the underlying distribution PX,Y\nstill appears explicitly in what follows, since that is how we measure our performance: we\njudge the classifiers we produced on future i.i.d. draws from PX,Y .\n\nFigure 1: The Bayes classifier when π = 1/2.\nGiven data Dn = {\nˆ\n(X1, Y1), . . . , (Xn, Yn)}, we build a classifier hn(X), which is random\nin two senses: it is a function of a random variable X and also depends implicitly on the\nˆ\nrandom data Dn. As above, we judge a classifier according to the quantity E(hn). This is\na random variable: though we have integrated out X, the excess risk still depends on the\ndata Dn. We therefore will consider bounds both on its expected value and bounds that\nˆ\nhold in high probability. In any case, the bound E(hn) ≥0 always holds. (This inequality\ndoes not merely hold \"almost surely,\" since we proved that R(h) ≥R(h∗) uniformly over\nall choices of classifier h.)\nLast time, we proposed two different philosophical approaches to this problem.\nIn\nparticular, generative approaches make distributional assumptions about the data, attempt\nto learn parameters of these distributions, and then plug the resulting values into the model.\nThe discriminative approach--the one taken in machine learning--will be described in great\ndetail over the course of this semester. However, there is some middle ground, which is worth\nmentioning briefly. This middle ground avoids making explicit distributional assumptions\nabout X while maintaining some of the flavor of the generative model.\nThe central insight of this middle approach is the following: since by definition h∗(x) =\nˆ\n1(η(X) > 1/2), we estimate η by some ηˆn and thereby produce the estimator hn =\n1(ηˆn(X) > 1/2). The result is called a plug-in estimator.\nOf course, achieving good performance with a plug-in estimator requires some assump-\ntions.\n(No-free-lunch theorems imply that we can't avoid making an assumption some-\nwhere!) One possible assumption is that η(X) is smooth; in that case, there are many\nnonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet\nbases, etc.).\nWe could also assume that η(X) is a function of a particular form. Since η(X) is only\nsupported on [0, 1], standard linear models are generally inapplicable; rather, by applying\nthe logit transform we obtain logistic regression, which assumes that η satisfies an identity\nof the form\nlog\n\nη(X)\n1 -η(X)\n\n= θT X.\nPlug-in estimators are called \"semi-paramteric\" since they avoid making any assumptions\nabout the distribution of X. These estimators are widely used because they perform fairly\nwell in practice and are very easy to compute. Nevertheless, they will not be our focus here.\nIn what follows, we focus here on the discriminative framework and empirical risk min-\nimization. Our benchmark continues to be the risk function R(h) = IE1(Y = h(X)), which\n\nis clearly not computable based on the data alone; however, we can attempt to use a na ıve\nstatistical \"hammer\" and replace the expectation with an average.\nDefinition: The empirical risk of a classifier h is given by\nn\nˆRn(h) =\nX\n1(Yi = h(Xi)).\nn i=1\nMinimizing the empirical risk over the family of all classifiers is useless, since we can\nalways minimize the empirical risk by mimicking the data and classifying arbitrarily other-\nwise. We therefore limit our attention to classifiers in a certain family H.\nˆ\nDefinition: The Empirical Risk Minimizer (ERM) over H is any element1 herm of the set\nˆ\nargminh\nRn(h).\n∈H\nIn order for our results to be meaningful, the class H must be much smaller than the\nˆ\nspace of all classifiers. On the other hand, we also hope that the risk of herm will be close\nto the Bayes risk, but that is unlikely if H is too small. The next section will give us tools\nfor quantifying this tradeoff.\n1.3 Oracle Inequalities\nAn oracle is a mythical classifier, one that is impossible to construct from data alone but\n\nwhose performance we nevertheless hope to mimic. Specifically, given H we define h to be\nan element of argminh\nR(h)--a classifier in\n∈H\nH that minimizes the true risk. Of course,\n\nwe cannot determine h, but we can hope to prove a bound of the form\nˆ\nR(h) ≤\n\nR(h) + something small.\n(1.4)\n\nSince h is the best minimizer in H given perfect knowledge of the distribution, a bound of\nˆ\nthe form given in Equation 1.4 would imply that h has performance that is almost best-in-\nclass. We can also apply such an inequality in the so-called improper learning framework,\nˆ\nwhere we allow h to lie in a slightly larger class H′\nˆ\n⊃H; in that case, we still get nontrivial\n\nguarantees on the performance of h if we know how to control R(h)\nThere is a natural tradeoffbetween the two terms on the right-hand side of Equation 1.4.\nWhen H\n\nis small, we expect the performance of the oracle h to suffer, but we may hope\n\nto approximate h quite closely.\n(Indeed, at the limit where H is a single function, the\n\"something small\" in Equation 1.4 is equal to zero.) On the other hand, as H grows the\noracle will become more powerful but approximating it becomes more statistically difficult.\n(In other words, we need a larger sample size to achieve the same measure of performance.)\nˆ\nSince R(h) is a random variable, we ultimately want to prove a bound in expectation\nor tail bound of the form\nˆ\nIP(R(h) ≤\n\nR(h) + ∆n,δ(H)) ≥1 -δ,\nwhere ∆n,δ(H) is some explicit term depending on our sample size and our desired level of\nconfidence.\n1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classifier\nˆ\nˆ\nˆ\nh satisfying Rn(h) ≤infh\nR\n∈H\nn(h) + ε.\n\nIn the end, we should recall that\nE ˆ\nˆ -\n∗\nˆ -\n\n(h) = R(h)\nR(h ) = (R(h)\nR(h)) + (R(h) -R(h∗)).\nThe second term in the above equation is the approximation error, which is unavoidable\nonce we fix the class H. Oracle inequalities give a means of bounding the first term, the\nstochastic error.\n1.4 Hoeffding's Theorem\nOur primary building block is the following important result, which allows us to understand\nhow closely the average of random variables matches their expectation.\nTheorem (Hoeffding's Theorem): Let X1, . . . , Xn be n independent random vari-\nables such that Xi ∈[0, 1] almost surely.\nThen for any t > 0,\nIP\n\nn\n1 X\nXi\nIEXi\nn i=1\n-\n\n>\n!\n\nt\n≤2e-2nt .\nIn other words, deviations from\n\nthe mean deca\n\ny exponentially fast in n and t.\nProof. Define centered random variables Zi = Xi -IEXi. It suffices to show that\n1 X\n\n≤\n-2nt2\nIP\nZi > t\ne\n,\nn\nsince the lower tail bound follows analogously. (Exercise!)\nWe apply Chernoffbounds. Since the exponential function is an order-preserving bijec-\ntion, we have for any s > 0\nIP\n1 X\nZ\nstn\ni > t\n\n= IP\n\nexp\n\ns\nX\nZ\nstn\ns\nZi\ni\n]\nn\n\n> e\n\n≤e-\nIE[e\nP\n(Markov)\n= e-stn\nIE[esZi],\n(1.5)\nwhere in the last equality we have used the independence of the\nY\nZi.\nWe therefore need to control the term IE[esZi], known as the moment-generating func-\ntion of Zi. If the Zi were normally distributed, we could compute the moment-generating\nfunction analytically. The following lemma establishes that we can do something similar\nwhen the Zi are bounded.\nLemma (Hoeffding's Lemma): If Z ∈[a, b] almost surely and IEZ = 0, then\n≤\ns (b-a)\nIEesZ\ne\n.\nProof of Lemma. Consider the log-moment generating function ψ(s) = log IE[esZ], and note\nthat it suffices to show that ψ(s) ≤s2(b -a)2/8. We will investigate ψ by computing the\n\nfirst several terms of its Taylor expansion. Standard regularity conditions imply that we\ncan interchange the order of differentiation and integration to obtain\nIE[ZesZ]\nψ′(s) =\n,\nIE[esZ]\nIE[Z2esZ]IE[esZ]\nIE[ZesZ]2\nesZ\nesZ\nψ′′(s) =\n-\n= IE\n\nZ2\n\n-\n\nIE Z\nIE[esZ]2\nIE[esZ]\n\nIE[esZ]\n\n.\nSince\nesZ\nsZ integrates to 1, we can interpret ψ′′(s) as the variance of Z under the probability\nIE[e\n]\nmeasure dF =\nesZ\nsZ dIE. We obtain\nIE[e\n]\nψ′′(s) = varF(Z) = varF\n\na + b\nZ -\n\n,\nsince the variance is unaffected under shifts.\nBut |Z -a+b\n2 | ≤\nb-a almost surely since\nZ ∈[a, b] almost surely, so\nvarF\n\n+ b\nZ -\n\n≤F\n\"\na\na + b\nZ -\n#\n(b -a)2\n≤\n.\nFinally, the fundamental theorem of calculus yields\ns\nu\ns2(b\na)2\nψ(s) =\nZ\n(u du\nZ\nψ′′\n)\n-\n.\n≤\nThis concludes the proof of the Lemma.\nApplying Hoeffding's Lemma to Equation (1.5), we obtain\nIP\n1 X\nZ > t\n\n≤e-stn Y\nes2/8 = ens /8\nstn\ni\n-\n,\nn\nfor any s > 0. Plugging in s = 4t > 0 yields\nIP\n1 X\nZi > t\n\n≤e-2nt2,\nn\nas desired.\nHoeffding's Theorem implies that, for any classifier h, the bound\nlog(2/δ)\n| ˆRn(h) -R(h)| ≤\nr\n2n\nholds with probability 1 -δ. We can immediately apply this formula to yield a maximal\ninequality: if H is a finite family, i.e., H = {h1, . . . , hM}, then with probability 1 -δ/M\nthe bound\nlog\n| ˆRn(hj) -R(hj)| ≤\nr\n(2M/δ)\n2n\n\nˆ\nholds. The event that maxj |Rn(hj)-R(hj)|\nˆ\n> t is the union of the events |Rn(hj)-R(hj)| >\nt for j = 1, . . . , M, so the union bound immediately implies that\nlog(2M/δ)\nmax | ˆRn(hj) -R(hj)\nj\n| ≤\nr\n2n\nwith probability 1-δ. In other words, for such a family, we can be assured that the empirical\nrisk and the true risk are close. Moreover, the logarithmic dependence on M implies that\nwe can increase the size of the family H exponentially quickly with n and maintain the\nsame guarantees on our estimate.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: James Hirst\nSep. 16, 2015\n1.5 Learning with a finite dictionary\nRecall from the end of last lecture our setup: We are working with a finite dictionary\nH = {h1, . . . , hM} of estimators, and we would like to understand the scaling of this problem\nwith respect to M and the sample size n. Given H, one idea is to simply try to minimize\nˆ\nthe empirical risk based on the samples, and so we define the empirical risk minimizer, herm,\nby\nˆ\nˆ\nherm ∈argmin Rn(h).\nh∈H\nˆ\nˆ\nIn what follows, we will simply write h instead of herm when possible. Also recall the\n\ndefinition of the oracle, h, which (somehow) minimizes the true risk and is defined by\nh ∈argmin R(h).\nh∈H\nˆ\n\nThe following theorem shows that, although h cannot hope to do better than h in\ngeneral, the difference should not be too large as long as the sample size is not too small\ncompared to M.\nˆ\nTheorem: The estimator h satisfies\nˆ\n\nR(h) ≤R(h) +\nr\n2 log(2M/δ)\nn\nwith probability at least 1 -δ. In expectation, it holds that\nr\n2 log(2M)\nˆ\n\nIE[R(h)] ≤R(h) +\n.\nn\nProof.\nˆ\nˆ\nˆ\nˆ\n\nFrom the definition of h, we have Rn(h) ≤Rn(h), which gives\nˆ\n\nˆ\n\nˆ\nˆ\nˆ\nR(h) ≤R(h) + [Rn(h) -R(h)] + [R(h) -Rn(h)].\nThe only term here that we need to control is the second one, but since we don't have\n\nany real information about h, we will bound it by a maximum over H and then apply\nHoeffding:\nlog(2M/δ)\nˆ\n\nˆ\nˆ\nˆ\nˆ\n[Rn(h) -R(h)] + [R(h) -Rn(h)] ≤2 max |Rn(hj) -R(hj)| ≤2\nj\nr\n2n\nwith probability at least 1 -δ, which completes the first part of the proof.\n\nTo obtain the bound in expectation, we start with a standard trick from probability\nwhich bounds a max by its sum in a slightly more clever way. Here, let {Zj}j be centered\nrandom variables, then\n\nIE max |Zj|\n=\nlog exp\nsIE max |Zj|\n\n≤\nlog IE\n\nexp\n\ns max |Zj|\nj\ns\nj\ns\nj\n\n,\nwhere the last inequality comes from applying Jensen's inequality to the convex function\nexp(·). Now we bound the max by a sum to get\n2M\n)\n≤\nlog\nX\ns2\nlog(2M\ns\nIE [exp(sZj)] ≤\nlog\n\n2M exp\n\n=\n+\n,\ns\ns\n8n\ns\n8n\nj=1\nˆ\nwhere we used Zj = Rn(hj) -R(hj) in our case and then applied Hoeffding's Lemma. Bal-\nancing terms by minimizing over s, this gives s = 2\np\n2n log(2M) and plugging in produces\n\nlog(2M)\nˆ\nIE max |Rn(hj) -R(hj)|\n≤\nj\n\nr\n,\n2n\nwhich finishes the proof.\n2. CONCENTRATION INEQUALITIES\nConcentration inequalities are results that allow us to bound the deviations of a function of\nrandom variables from its average. The first of these we will consider is a direct improvement\nto Hoeffding's Inequality that allows some dependence between the random variables.\n2.1 Azuma-Hoeffding Inequality\nGiven a filtration {Fi}i of our underlying space X, recall that {∆i}i are called martingale\ndifferences if, for every i, it holds that ∆i ∈Fi and IE [∆i|Fi] = 0. The following theorem\ngives a very useful concentration bound for averages of bounded martingale differences.\nTheorem (Azuma-Hoeffding): Suppose that {∆i}i are margingale differences with\nrespect to the filtration {Fi}i, and let Ai, Bi ∈Fi-1 satisfy Ai ≤∆i ≤Bi almost surely\nfor every i. Then\nIP\n\"\n1 X\n2n\n∆i > t\n#\n2t2\n≤exp\nn\ni\n\n-Pn\ni=1 ∥Bi -Ai∥2inf\n\n.\nIn comparison to Hoeffding's inequality, Azuma-Hoeffding affords not only the use of\nnon-uniform boundedness, but additionally requires no independence of the random vari-\nables.\nProof. We start with a typical Chernoffbound.\nIP\n\"\n#\nX\n∆i > t\n≤IE\nh\nes P ∆i\ni\ne-st = IE\ni\nh\nIE\nh\nes P ∆i|Fn-1\nii\ne-st\n\nn-1\nn-1\n= IE\nh\nes P\n∆iIE[es∆n|Fn\n1] e-st\n-\n≤IE[es P\n∆i · es (Bn-An) /8]e-st,\nwhere we have used the fact that the ∆\ni\ni, i < n, are all Fn measureable, and then applied\nHoeffding's lemma on the inner expectation.\nIteratively isolating each ∆i like this and\napplying Hoeffding's lemma, we get\nIP\n\"\nn\nX\ns2\n∆> t\n#\n≤exp\n\nX\n∥B -A ∥2\n!\ne-st\ni\ni\ni\ninf\n.\ni\ni=1\nOptimizing over s as usual then gives the result.\n2.2 Bounded Differences Inequality\nAlthough Azuma-Hoeffding is a powerful result, its full generality is often wasted and can\nbe cumbersome to apply to a given problem. Fortunately, there is a natural choice of the\n{Fi}i and {∆i}i, giving a similarly strong result which can be much easier to apply. Before\nwe get to this, we need one definition.\nDefinition (Bounded Differences Condition): Let g : X →IR and constants ci be\ngiven. Then g is said to satisfy the bounded differences condition (with constants ci) if\nsup\n|g(x , . . . , x ) -g(x , . . . , x′\nn\ni, . . . , xn)| ≤ci\nx\n′\n1,...,xn,xi\nfor every i.\nIntuitively, g satisfies the bounded differences condition if changing only one coordinate\nof g at a time cannot make the value of g deviate too far. It should not be too surprising\nthat these types of functions thus concentrate somewhat strongly around their average, and\nthis intuition is made precise by the following theorem.\nTheorem (Bounded Differences Inequality): If g : X →IR satisfies the bounded\ndifferences condition, then\n2t2\nIP [|g(X1, . . . , Xn) -IE[g(X1, . . . , Xn)| > t] ≤2 exp\n\n-P\ni c2\ni\n\n.\nProof. Let {Fi}i be given by Fi = σ(X1, . . . , Xi), and define the martingale differences\n{∆i}i by\n∆i = IE [g(X1, . . . , Xn)|Fi] -IE [g(X1, . . . , Xn)|Fi-1] .\nThen\nIP\n\"\n|\nX\n∆i| > t\n#\n= IP\n\ng(X1, . . . , Xn) -IE[g(X1, . . . , Xn)\ni\n\n> t ,\nexactly the quantity we want to bou\n\nnd. Now, note that\n\n∆i ≤IE\n\nsup g(X1, . . . , xi, . . . , Xn)|Fi\n-IE [g(X1, . . . , Xn)|Fi-1]\nxi\n\n= IE\n\nsup g(X1, . . . , xi, . . . , Xn) -g(X1, . . . , Xn)|Fi-1\nxi\n\n=: Bi.\nSimilarly,\n∆i ≥IE\n\ninf g(X1, . . . , xi, . . . , Xn) -g(X1, . . . , Xn)|Fi-1\n=: Ai.\nxi\n\nAt this point, our assumption on g implies that ∥Bi -Ai∥inf≤ci for every i, and since\nAi ≤∆i ≤Bi with Ai, Bi ∈Fi-1, an application of Azuma-Hoeffding gives the result.\n2.3 Bernstein's Inequality\nHoeffding's inequality is certainly a powerful concentration inequality for how little it as-\nsumes about the random variables. However, one of the major limitations of Hoeffding is\njust this: Since it only assumes boundedness of the random variables, it is completely obliv-\nious to their actual variances. When the random variables in question have some known\nvariance, an ideal concentration inequality should capture the idea that variance controls\nconcentration to some degree. Bernstein's inequality does exactly this.\nTheorem (Bernstein's Inequality): Let X1, . . . , Xn be independent, centered ran-\ndom variables with |X | ≤c for every i, and write σ2 = n-1\ni\ni Var(Xi) for the average\nvariance. Then\nP\nIP\n\"\n1 X\nnt2\nXi > t\n#\n≤exp\n\n-\nn\n2σ2 + 2tc\ni\n!\n.\nHere, one should think of t as being fixed and relatively small compared to n, so that\nstrength of the inequality indeed depends mostly on n and 1/σ2.\nProof. The idea of the proof is to do a Chernoffbound as usual, but to first use our\nassumptions on the variance to obtain a slightly better bound on the moment generating\nfunctions. To this end, we expand\ninf(s\nk\ninf\nX )\nskck-2\ni\nIE[esXi] = 1 + IE[sXi] + IE\n\"\n#\nX\n≤1 + Var(Xi)\nX\n,\nk!\nk!\nk=2\nk=2\nwhere we have used IE[Xk\ni ] ≤IE[X2\ni |Xi|k-2] ≤Var(X\nk-2\ni)c\n.\nRewriting the sum as an\nexponential, we get\nesc\nsXi\n-sc -1\nIE[e\n] ≤s Var(Xi)g(s),\ng(s) :=\n.\nc2s2\nThe Chernoffbound now gives\nIP\n\"\n1 X\nXi > t\n#\n≤exp\n\ninf[s2(\nX\nVar(Xi))g(s) -nst]\n!\n= exp\n\nn · inf[s2σ2g(s) -st]\n,\nn\ns>0\ns>0\ni\ni\n\nand optimizing this over s (a fun calculus exercise) gives exactly the desired result.\n\n3. NOISE CONDITIONS AND FAST RATES\nˆ\nTo measure the effectiveness of the estimator h, we would like to obtain an upper bound\nˆ\nˆ\non the excess risk E(h) = R(h) -R(h∗). It should be clear, however, that this must depend\nsignificantly on the amount of noise that we allow. In particular, if η(X) is identically equal\nˆ\nto 1/2, then we should not expect to be able to say anything meaningful about E(h) in\ngeneral. Understanding this trade-offbetween noise and rates will be the main subject of\nthis chapter.\n3.1 The Noiseless Case\nA natural (albeit somewhat na ıve) case to examine is the completely noiseless case. Here,\nwe will have η(X) ∈{0, 1} everywhere, Var(Y |X) = 0, and\nE(h) = R(h) -R(h∗) = IE[|2η(X) -1|1I(h(X) = h∗(X))] = IP[h(X) = h∗(X)].\nLet us now denote\n\nˆ\nZi = 1I(h(Xi) = Yi) -1I(h(Xi) = Yi),\n\nand write Zi = Zi -IE[Zi]. Then notice that we have\nˆ\n\n|Zi| = 1I(h(Xi) = h(Xi)),\nand\nVar(Zi) ≤IE[Z2\nˆ\n\ni ] = IP[h(Xi) = h(Xi)].\nˆ\nFor any classifier hj ∈H, we can similarly define Zi(hj) (by replacing h with hj through-\nout). Then, to set up an application of Bernstein's inequality, we can compute\nn\n1 X\n\nVar(Zi(hj)) ≤IP[hj(Xi) = h(Xi)] =: σ2\nn\nj .\ni=1\nAt this point, we will make a (fairly strong) assumption about our dictionary H, which\n\nis that h∗∈H, which further implies that h = h∗. Since the random variables Zi compare\n\nˆ\nto h, this will allow us to use them to bound E(h), which rather compares to h∗. Now,\n\napplying Bernstein (with c = 2) to the {Zi(hj)}i for every j gives\n\"\nn\n#\nX\nnt2\nδ\n\nIP\nZi(hj) > t\n≤exp\n\n-\n=\n2σ2\ni=1\nj + 4t\n!\n:\n,\nn\nM\nand a simple computation here shows that it is enough to take\ns\n2σ2\nj log(M/δ)\nt ≥max\n,\nlog(M/δ)\nn\n3n\n\n=: t0(j)\n\nfor this to hold. From here, we may use the assumption h = h∗to conclude that\nˆ\nˆ\nIP\nh\nE(h) > t0(ˆj)\ni\n≤δ,\nhˆ = h.\nj\n\nˆ\nHowever, we also know that σ2\nˆ ≤E(h), which implies that\nj\ns\nˆ\n2E(h) log(M/δ)\nˆ\nE(h) ≤max\n,\nlog(M/δ)\nn\n3n\n\nˆ\nwith probability 1 -δ, and solving for E(h) gives the improved rate\nlog(M/δ)\nˆ\nE(h) ≤2\n.\nn\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Cheng Mao\nSep. 21, 2015\nIn this lecture, we continue to discuss the effect of noise on the rate of the excess risk\nˆ\nˆ\nˆ\nE(h) = R(h) -R(h∗) where h is the empirical risk minimizer. In the binary classification\nmodel, noise roughly means how close the regression function η is from 1\n2. In particular, if\nη = 1 then we observe only noise, and if η ∈{0, 1} we are in the noiseless case which has\nbeen studied last time. Especially, we achieved the fast rate log M in the noiseless case by\nn\n\nassuming h∗∈H which implies that h = h∗. This assumption was essential for the proof\nand we will see why it is necessary again in the following section.\n3.2 Noise conditions\nThe noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess\nrisk is when the noise is present but can be controlled. Instead of the condition η ∈{0, 1},\nwe can control the noise by assuming that η is uniformly bounded away from 1\n2, which is\nthe motivation of the following definition.\nDefinition (Massart's noise condition): The noise in binary classification is said\nto satisfy Massart's condition with constant γ ∈(0, 1\n2] if |η(X) -1| ≥γ almost surely.\nOnce uniform boundedness is assumed, the fast rate simply follows from last proof with\nappropriate modification of constants.\nˆ\nˆ\nˆ\nTheorem: Let cE(h) denote the excess risk of the empirical risk minimizer h = herm.\nIf Massart's noise condition is satisfied with constant γ, then\nlog(M/δ)\nˆ\nE(h) ≤\nγn\nwith probability at least 1 -δ. (In particular γ = 1 gives exactly the noiseless case.)\nProof.\n\nDefine Zi(h) = 1I(h(Xi) = Yi) -1I(h(Xi) = Yi). By the assumption h = h∗and the\nˆ\nˆ\ndefinition of h = herm,\nˆ\nˆ\n\nE(h) = R(h) -R(h)\nˆ\nˆ\nˆ\n\nˆ\n\nˆ\nˆ\n\nˆ\n= Rn(h) -Rn(h) + Rn(h) -Rn(h) -R(h) -R(h)\n(3.1)\nn\nˆ\n≤\n)\n\nX ˆ\nZi(h) -IE[Zi(h ]\n\n.\n(3.2)\nn i=1\nHence it suffices to bound the deviation of P\ni Zi from its expectation. To this end, we\nhope to apply Bernstein's inequality. Since\nVar[Zi(h)] ≤IE[Z\n\ni(h) ] = IP[h(Xi) = h(Xi)],\n\nwe have that for any 1 ≤j ≤M,\nn\n1 X\n\nVar[Zi(hj)] ≤IP[hj(X) = h(X)] =: σ2\nn\nj.\ni=1\nBernstein's inequality implies that\nn\n1 X\n\nnt2\nIP\n(Zi(hj) -IE[Zi(hj)]) > t ≤exp\n-\nn\n2σ2\ni=1\nj + 2\n3t\n\nδ\n=:\n.\nM\nApplying a union bound over 1 ≤j ≤M and taking\n2σ2\nj log(M/δ) 2 log(M/δ)\nt = t0(j) := max\ns\n,\nn\n3n\n\n,\nwe get that\nn\n1 X\n(Zi(hj) -IE[Zi(hj)]) ≤t0(j)\n(3.3)\nn i=1\nfor all 1 ≤j ≤M with probability at least 1 -δ.\nˆ\nSuppose h = hˆ. It follows from (3.2) and (3.3) that with probability at least 1 -δ,\nj\nˆ\nE(h) ≤t\nˆ\n0(j).\n(Note that so far the proof is exactly the same as the noiseless case.) Since |η(X) -1\n2| ≥γ\n\na.s. and h = h∗,\nˆ\nˆ\n\nE(h) = IE[|2η(X) -1|1I(h(X) = h∗(X))] ≥2γIP[hˆ(X) = h(X)] = 2γσ2\nˆ.\nj\nj\nTherefore,\ns\nˆ\nE(h) log(M/δ) 2 log(M/δ)\nˆ\nE(h) ≤max\n,\n,\n(3.4)\nγn\n3n\nso we conclude that with probabilit\ny at least 1 -δ,\n\nlog(M/δ)\nˆ\nE(h) ≤\n.\nγn\n\nThe assumption that h = h∗was used twice in the proof. First it enables us to ignore\nthe approximation error and only study the stochastic error. More importantly, it makes\nthe excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess\nrisk to get the fast rate.\nMassart's noise condition is still somewhat strong because it assumes uniform bounded-\nness of η from 1\n2. Instead, we can allow η to be close to 1\n2 but only with small probability,\nand this is the content of next definition.\n\nDefinition (Tsybakov's noise condition or Mammen-Tsybakov noise condi-\ntion): The noise in binary classification is said to satisfy Tsybakov's condition if there\nexists α ∈(0, 1), C\n0 > 0 and t0 ∈(0, 2] such that\nα\nIP[|η(X) -\n| ≤t] ≤C\n0t\n-α\nfor all t ∈[0, t0].\nα\nIn particular, as α →1, t 1-α →0\nα, so this recovers Massart's condition with γ = t0 and\nwe have the fast rate. As α →0, t 1-α →1, so the condition is void and we have the slow\nrate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose\norder depends on α. We will see that this is indeed the case.\nLemma: Under Tsybakov's noise condition with constants α, C0 and t0, we have\nIP[h(X) = h∗(X)] ≤CE(h)α\nfor any classifier h where C = C(α, C0, t0) is a constant.\nProof. We have\nE(h) = IE[|2η(X) -1|1I(h(X) = h∗(X))]\n≥IE[|2η(X) -1|1I(|η(X) -\n| > t)1I(h(X) = h∗(X))]\n≥2tIP[|η(X) -\n| > t, h(X) = h∗(X)]\n≥2tIP[h(X) = h∗(X)] -2tIP[|η(X) -\n| ≤t]\n≥2tIP[h(X) = h∗(X)] -2C0t 1-α\nwhere Tsybakov's condition was used in the last step. Take t = cIP[h(X) = h∗(X)]\n-α\nα\nfor\nsome positive c = c(α, C0, t0) to be chosen later. We assume that c ≤t0 to guarantee that\nt ∈[0, t0]. Since α ∈(0, 1),\nE(h) ≥2cIP[h(X) = h∗(X)]1/α\n-2C c 1-α IP[h(X) = h∗\n(X)] /α\n≥cIP[h(X) = h∗(X)]1/α\nby selecting c sufficiently small depending on α and C0. Therefore\nIP[h(X) = h∗(X)] ≤\nE(h)α\ncα\nand choosing C = C(α, C0, t0) := c-α completes the proof.\nHaving established the key lemma, we are ready to prove the promised fast rate under\nTsybakov's noise condition.\n\nTheorem: If Tsybakov's noise condition is satisfied with constant α, C0 and t0, then\nthere exists a constant C = C(α, C0, t0) such that\nl\n)\nˆ\nE h) ≤C\nog(M/δ\n(\nn\n\n2-α\nwith probability at least 1 -δ.\nThis rate of excess risk parametrized by α is indeed an interpolation of the slow (α →0)\nˆ\nand the fast rate (α →1). Futhermore, note that the empirical risk minimizer h does not\ndepend on the parameter α at all! It automatically adjusts to the noise level, which is a\nvery nice feature of the empirical risk minimizer.\nProof. The majority of last proof remains valid and we will explain the difference. After\nestablishing that\nˆ\nE(h) ≤t0(ˆj),\nwe note that the lemma gives\nˆ\nσ2\n\nˆ\nˆ = IP[h(X) = h(X)] ≤CE(h)α.\nj\nIt follows that\ns\nˆ\n2CE(h)α log(M/δ) 2 log(M/δ)\nˆ\nE(h) ≤max\n,\nn\n3n\n\n2C log M\nˆ\nE(h) ≤max\nδ\n2-α\nlog(M/δ)\n,\n\n.\nn\n3n\nand thus\n4. VAPNIK-CHERVONENKIS (VC) THEORY\nThe upper bounds proved so far are meaningful only for a finite dictionary H, because if\nM = |H| is infinite all of the bounds we have will simply be infinity. To extend previous\nresults to the infinite case, we essentially need the condition that only a finite number of\nelements in an infinite dictionary H really matter. This is the objective of the Vapnik-\nChervonenkis (VC) theory which was developed in 1971.\n4.1 Empirical measure\nRecall from previous proofs (see (3.1) for example) that the key quantity we need to control\nis\nˆ\n2 sup\nRn(h) -R(h) .\nh∈H\nInstead of the union bound which would not work in th\n\ne infinite case, we seek some bound\nthat potentially depends on n and the complexity of the set H. One approach is to consider\nsome metric structure on H and hope that if two elements in H are close, then the quantity\nevaluated at these two elements are also close. On the other hand, the VC theory is more\ncombinatorial and does not involve any metric space structure as we will see.\n\nBy definition\nn\nˆRn(h) -R(h) =\nX 1I(h(Xi) = Yi) -IE[1I(h(Xi) = Yi)]\nn i=1\n\n.\nLet Z = (X, Y ) and Zi = (Xi, Yi), and let A denote the class of measurable sets in the\nsample space X × {0, 1}. For a classifier h, define Ah ∈A by\n{Zi ∈Ah} = {h(Xi) = Yi}.\nMoreover, define measures μn and μ on A by\nn\nμn(A) =\nX\n1I(Zi ∈A)\nand\nμ(A) = IP[Zi ∈A]\nn i=1\nfor A ∈A. With this notation, the slow rate we proved is just\nlog(2|A|/δ)\nˆ\nsup Rn(h) -R(h) = sup |μn(A) -μ(A)| ≤\nh∈H\nA∈A\nr\n.\n2n\nSince this is not accessible in the infinite case, we hope to use one of the concentration\ninequalities to give an upper bound. Note that μn(A) is a sum of random variables that may\nnot be independent, so the only tool we can use now is the bounded difference inequality.\nIf we change the value of only one zi in the function\nz1, . . . , zn 7→sup |μn(A) -μ(A)|,\nA∈A\nthe value of the function will differ by at most 1/n. Hence it satisfies the bounded difference\nassumption with ci = 1/n for all 1 ≤i ≤n. Applying the bounded difference inequality, we\nget that\n\nlog(2/δ)\nsup |μn(A) -μ(A)| -IE[sup |μn(A) -μ(A)|] ≤\nA∈A\nA∈A\nr\n2n\nwith probability\n\nat least 1 -δ. Note that this already preclu\n\ndes any fast rate (faster than\nn-1/2). To achieve fast rate, we need Talagrand inequality and localization techniques which\nare beyond the scope of this section.\nIt follows that with probability at least 1 -δ,\nlog(2/δ)\nsup |μn(A) -μ(A)| ≤IE[sup |μn(A) -μ(A)|] +\nA\nA∈A\nr\n.\nA∈\n2n\nWe will now focus on bounding the first term on the right-hand side. To this end, we need\na technique called symmetrization, which is the subject of the next section.\n4.2 Symmetrization and Rademacher complexity\nSymmetrization is a frequently used technique in machine learning. Let D = {Z1, . . . , Zn}\nbe the sample set. To employ symmetrization, we take another independent copy of the\nsample set D′ = {Z′\n1, . . . , Z′\nn}. This sample only exists for the proof, so it is sometimes\nreferred to as a ghost sample. Then we have\nn\nn\nμ(A) = IP[Z ∈A] = IE[\nX\n1I(Z′\ni ∈A)] = IE[\n1I(Z′\ni ∈A)|D] = IE[μ′\nn\nn\nn(A)|D]\ni=1\nX\ni=1\n\nn\nwhere μ′\nn := 1 P\ni=1 1I(Z′\ni ∈A). Thus by Jensen's inequality,\nn\nIE[sup |μn(A) -μ(A)|] = IE\n\nsup\nμn(A) -IE[μ′\nn(A)|D]\nA∈A\nA∈A\n≤IE sup IE[|μn(A) -μ′\nn(A)| |D\n\n]\n\n≤\n\nA∈A\nIE\n\nsup |μ\n′\nn(A) -μn(A)|\n\nA∈A\nn\n\n= IE\n\nsup\n\nX 1I(Z\n′\ni ∈A) -1I(Z\nA∈A n\ni ∈A)\ni=1\n\n.\nSince D′ has the same distribution of D, by sy\n\nmmetry 1I(Zi ∈A) -1I(Z′\n\ni ∈A) has the same\ndistribution as σi\n1I(Zi ∈A) -1I(Z′\ni ∈A)\n\nwhere σ1, . . . , σn are i.i.d. Rad(1\n2), i.e.\nIP[σi = 1] = IP[σi = -1] =\n,\nand σi's are taken to be independent of both samples. Therefore,\nn\nIE[sup |μn(A) -μ(A)|] ≤IE\nA A\n\nsup\n′\n∈\nA∈A\n1 X\nσi\n1I(Zi ∈A) -1I(Z\nn\ni ∈A)\ni=1\nn\n\n≤2IE\n\nsup\n\nX\nσi1I(Zi ∈A) .\nA∈A n i=1\n\n(4.5)\nUsing symmetrization we have bounded IE[sup\n\nA∈A |μn(A)-μ(A)|\n\n] by a much nicer quantity.\nYet we still need an upper bound of the last quantity that depends only on the structure\nof A but not on the random sample {Zi}. This is achieved by taking the supremum over\nall zi ∈X × {0, 1} =: Y.\nDefinition: The Rademacher complexity of a family of sets A in a space Y is defined\nto be the quantity\nn\nRn(A) =\nsup\nsup\nIE\n\nX\nσi1I(zi ∈A)\nz1,...,zn∈Y\nA∈A n i=1\n\n.\nThe Rademacher complexity of a set B ⊂I\n\nRn is defined to b\n\ne\nn\nRn(B) = IE\n\nsup\nb∈B\n\nn\nX\nσibi\ni=1\n\n.\nWe conclude from (4.5) and the definition that\nIE[sup |μn(A) -μ(A)|] ≤2Rn(A).\nA∈A\nn\nIn the definition of Rademacher complexity of a set, the quantity\ns\nn\ni=1 σibi measure\nhow well a vector b ∈B correlates with a random sign pattern {σi}. The more complex\nB is, the better some vector in B can replicate a sign pattern. In\n\npa\nP\nrticular, i\n\nf B is the\nfull hypercube [-1, 1]n, then Rn(B) = 1. However, if B ⊂[-1, 1]n contains only k-sparse\n\nvectors, then Rn(B) = k/n. Hence Rn(B) is indeed a measurement of the complexity of\nthe set B.\nThe set of vectors to our interest in the definition of Rademacher complexity of A is\nT(z) := {(1I(z1 ∈A), . . . , 1I(zn ∈A))T , A ∈A}.\nThus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these\nvectors can replicate as A ranges over A. Although the cardinality of A may be infinite,\nthe cardinality of T(z) is bounded by 2n.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Vira Semenova and Philippe Rigollet\nSep. 23, 2015\nIn this lecture, we complete the analysis of the performance of the empirical risk mini-\nmizer under a constraint on the VC dimension of the family of classifiers. To that end, we\nwill see how to control Rademacher complexities using shatter coefficients. Moreover, we\nwill see how the problem of controlling uniform deviations of the empirical measure μn from\nthe true measure μ as done by Vapnik and Chervonenkis relates to our original classification\nproblem.\n4.1 Shattering\nRecall from the previous lecture that we are interested in sets of the form\nT(z) :=\n\n(1I(z1 ∈A), . . . , 1I(zn ∈A)), A ∈A ,\nz = (z1, . . . , zn) .\n(4.1)\nIn particular, the cardinality of T(z), i.e., the numbe\n\nr of binary patterns these vectors\ncan replicate as A ranges over A, will be of critical importance, as it will arise when\ncontrolling the Rademacher complexity. Although the cardinality of A may be infinite, the\ncardinality of T(z) is always at most 2n. When it is of the size 2n, we say that A shatters\nthe set z1, . . . , zn. Formally, we have the following definition.\nDefinition: A collection of sets A shatters the set of points {z1, z2, ..., zn}\ncard{(1I(z1 ∈A), . . . , 1I(zn ∈A)), A ∈A} = 2n .\nThe sets of points {z1, z2, ..., zn} that we are interested are realizations of the pairs Z1 =\n(X1, Y1), . . . , Zn = (Xn, Yn) and may, in principle take any value over the sample space.\nTherefore, we define the shatter coefficient to be the largest cardinality that we may obtain.\nDefinition: The shatter coefficients of a class of sets A is the sequence of numbers\n{SA(n)}n≥1, where for any n ≥1,\nSA(n) =\nsup card (1I(z1\nA), . . . , 1I(zn\nA)), A\nz1,...,zn\n{\n∈\n∈\n∈A}\nand the suprema are taken over the whole sample space.\nBy definition, the nth shatter coefficient SA(n) is equal to 2n if there exists a set {z1, z2, ..., zn}\nthat A shatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di-\nmension.\nDefinition: The Vapnik-Chervonenkis dimension, or VC-dimension of\nd\nVC\nA is the largest\ninteger d such that SA(d) = 2 . We write\n(A) = d.\n\nIf SA(n) = 2n for all positive integers n, then VC(A) := inf\nIn words, A shatters some set of points of cardinality d but shatters no set of points of\ncardinality d+1. In particular, A also shatters no set of points of cardinality d′ > d so that\nthe VC dimension is well defined.\nIn the sequel, we will see that the VC dimension will play the role similar to of cardinality,\nbut on an exponential scale. For interesting classes A such that card(A) = inf, we also may\nhave VC(A) < inf. For example, assume that A is the class of half-lines, A = {(-inf, a], a ∈\nIR} ∪{[a, inf), a ∈IR}, which is clearly infinite. Then, we can clearly shatter a set of size\n2 but we for three points z1, z2, z3, ∈IR, if for example z1 < z2 < z3, we cannot create the\npattern (0, 1, 0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros\nfollowed by ones or with ones followed by zeros but not an alternating pattern like (0, 1, 0).\nFigure 1: If A = {halflines}, then any set of size n = 2 is shattered because we can\ncreate all 2n = 4 0/1 patterns (left); if n = 3 the pattern (0, 1, 0) cannot be reconstructed:\nSA(3) = 7 < 23 (right). Therefore, VC(A) = 2.\n4.2 The VC inequality\nWe have now introduced all the ingredients necessary to state the main result of this section:\nthe VC inequality.\nTheorem (VC inequality): For any family of sets A with VC dimension VC(A) = d,\nit holds\nr\n2d log(2en/d)\nIE sup |μn(A) -μ(A)| ≤2\nA∈A\nn\nNote that this result holds even if A is infinite as long as its VC dimension is finite. Moreover,\nobserve that log(|A|) has been replaced by a term of order d log 2en/d .\nTo prove the VC inequality, we proceed in three steps:\n\n1. Symmetrization, to bound the quantity of interest by the Rademacher complexity:\nIE[sup |μn(A) -μ(A)|] ≤2Rn(\n)\nA∈A\nA .\nWe have already done this step in the previous lecture.\n2. Control of the Rademacher complexity using shatter coefficients. We are going to\nshow that\ng\nR (A) ≤\ns\n2 lo\nn\n2SA(n)\nn\n\n3. We are going to need the Sauer-Shelah lemma to bound the shatter coefficients by\nthe VC dimension. It will yield\nS (n) ≤\nend\n,\nd = VC\nA\n(\nd\nA) .\nPut together, these three steps yield the VC inequality.\nStep 2: Control of the Rademacher complexity\nWe prove the following Lemma.\nLemma: For any B ⊂IRn, such that |B| < inf:, it holds\nn\n\nσ\n\n)\nB\n\n(\nRn(\n) = IE max\nX\nlog 2 B\nibi ≤max\n|\n|\nb∈B\nn\nb∈B\ni=1\n|b|2\np\nn\nwhere | · |2 denotes the Euclidean norm.\nProof. Note that\nRn(B) =\nIE\nn\n\nmax Zb\n,\nb∈B\n|\nwhere Zb = Pn\ni=1 σibi. In particular, since\n\n-|bi| ≤σi|bi| ≤|bi|, a.s., Hoeffding's lemma\nimplies that the moment generating function of Zb is controlled by\nn\nn\nIE\n\nexp(sZb)\n\n=\nY\nIE\ni=1\n\nexp(sσibi)\n\n≤\nY\nexp(s2b2\ni /2) = exp(s2 b 2\n2/2)\n(4.2)\ni=1\n| |\nNext, to control IE maxb∈B Zb| , we use the same technique as in Lecture 3, section 1.5.\n\nTo that end, define\n\nB = B ∪\n\n{-B\n\n} and observe that for any s > 0,\nIE\n\nmax |Zb|\n\n= IE\n\nmax Zb\n\n=\nlog exp\n\nsIE\n\nmax Zb\n\n≤\nlog IE exp\ns max Zb\n,\nb∈B\nb∈B\ns\nb\n\n∈B\ns\n\nb\n\n∈B\n\nwhere the last inequality follows from Jensen's inequality. Now we bound the max by a\nsum to get\n\nX\nlog | B|\ns b 2\nIE max |Zb|\n≤\nlog\nIE [exp(sZb)] ≤\n+\n| |2 ,\nb∈B\ns\ns\n2n\nb∈B\nwhere in the last inequality, we used (4.2).\nOptimizing over s > 0 yields the desired\nresult.\n\nWe apply this result to our problem by observing that\nRn(A) =\nsup\n(\n,\nRn(T z))\nz1,... zn\nwhere T(z) is defined in (4.1).\nIn particular, since T(z) ⊂{0, 1}, we have |b\n√\n|2 ≤\nn\nfor all b ∈T(z).\nMoreover, by definition of the shatter coefficients, if B = T(z), then\n| B| ≤2|T(z)| ≤2SA(n). Together with the above lemma, it yields the desired inequality:\nr\n2 log(2SA(n))\nRn(A) ≤\n.\nn\nStep 3: Sauer-Shelah Lemma\nWe need to use a lemma from combinatorics to relate the shatter coefficients to the VC\ndimension. A priori, it is not clear from its definition that the VC dimension may be at\nall useful to get better bounds. Recall that steps 1 and 2 put together yield the following\nbound\n2 log(2S (n))\nIE[sup\nn(A)\nA\n-μ(A) ]\nA∈\n|μ\n| ≤\nA\nr\n(4.3)\nn\nIn particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does\nnot imply that the uniform deviations go to zero as the sample size n goes to infinity. The\nVC inequality suggest that this is not the case as soon as VC(A) < infbut it is not clear a\npriori. Indeed, it may be the case that\nVC\nSA(n) = 2n for n ≤d and SA(n) = 2n -1 for n > d,\nwhich would imply that\n(A) = d < infbut that the right-hand side in (4.3) is larger than\n2 for all n. It turns our that this can never be the case: if the VC dimension is finite, then\nthe shatter coefficients are at most polynomial in n. This result is captured by the Sauer-\nShelah lemma, whose proof is omitted. The reading section of the course contains pointers\nto various proofs, specifically the one based on shifting which is an important technique in\nenumerative combinatorics.\nLemma (Sauer-Shelah): If VC(A) = d, then ∀n ≥1,\nd\nSA(n) ≤\nX n\nen\nd\n.\nk\n\n≤\nd\nk=0\n\nTogether with (4.3), it clearly yields the VC inequality. By applying the bounded difference\ninequality, we also obtain the following VC inequality that holds with high probability. This\nis often the preferred from for this inequality in the literature.\nCorollary (VC inequality): For any family of sets A such that VC(A) = d and any\nδ ∈(0, 1), it holds with probability at least 1 -δ,\nr\n2d log(2en/d)\nr\nlog(2/δ)\nsup μn A) -μ(A)| ≤2\n+\nA∈A\n|\n(\n.\nn\n2n\n\nNote that the logarithmic term log(2en/d) is actually superfluous and can be replaced\nby a numerical constant using a more careful bounding technique. This is beyond the scope\nof this class and the interested reader should take a look at the recommending readings.\n4.3 Application to ERM\nThe VC inequality provides an upper bound for supA∈A |μn(A) -μ(A)| in terms of the VC\ndimension of the class of sets A. This result translates directly to our quantity of interest:\n2VC(\n) log\n2en\n)\nˆ\nsup |Rn h) -\nVC(A)\nlog(2/δ\n(\nR(h)\nn\n\n+\nh∈H\n≤\ns\nA\n|\nr\n(4.4)\n2n\nwhere A = {Ah : h ∈H} and Ah = {(x, y) ∈X × {0, 1} : h(x) = y}. Unfortunately, the\nVC dimension of this class of subsets of X × {0, 1} is not very natural. Since, a classifier h\nis a {0, 1} valued function, it is more natural to consider the VC dimension of the family\nA =\n\n{h = 1} : h ∈H\n\n.\nDefinition: Let H be a collection of classifiers and define\nA =\n{h = 1} : h ∈H\nWe define the VC d\n\nimension VC(\n) o\n\n=\n\nA : ∃h ∈H, h(·) = 1I(· ∈A) .\nH\n\nf H to be the VC dimension of\n\nA.\n\nIt is not clear how VC(A) relates to the quantity VC(A), where A = {Ah : h ∈H} and\nAh = {(x, y) ∈X × {0, 1} : h(x) = y} that appears in the VC inequality. Fortunately, these\ntwo are actually equal as indicated in the following lemma.\nLemma: Define the two families for sets:\n=\nA\nX×\nh : h\n{0,1} where\n{\n\nA\n{\n∈H} ∈\nAh = (x, y) ∈X × {0, 1} : h(x) = y} and A =\n\n{h = 1\n: h\n2X .\nS\nS\n≥\nVC A\n}\n∈H\n∈\nThen,\nA (n) =\nA (n) for all n\n1. It implies\n(\n) = VC(A\n\n).\nProof. Fix x = (x1, ..., xn) ∈X n and y = (y1, y2, ..., yn) ∈{0, 1}n and define\nT(x, y) = {(1I(h(x1) = y1), . . . , 1I(h(xn) = yn)), h ∈H}\nand\nT(x) = {(1I(h(x1) = 1), . . . , 1I(h(xn) = 1)), h ∈H}\nTo that end, fix v ∈{0, 1} and recall the XOR (exclusive OR) boolean function from {0, 1}\nto {0, 1} defined by u ⊕v = 1I(u = v). It is clearly1 a bijection since (u ⊕v) ⊕v = u.\n1One way to see that is to introduce the \"spinned\" variables u = 2u -1 and v = 2v -1 that live in\n^\n{-1, 1}. Then u ⊕v = u ·v , and the claim follows by observing that (u ·v )·v = u . Another way is to simply\nwrite a truth table.\n\nWhen applying XOR componentwise, we have\n\n1I(h(x1) = y1)\n\n1I(h(x1 = 1)\ny\n\n..\n\n)\n\n..\n\n.\n1I(h(xi)\n\n.\n\n= yi)\n=\n1I(h(xi) = 1)\n\n..\n\n.\n⊕\n.\n\n.\n\n.\n\n1I(h(xn) = yn)\n\n1I(h(xn) = 1)\n\n...\n\nyi\n\n..\n\n.\nyn\n\nSince XOR is a bijection, we must have card[T(x, y)] = card[T(x)]. The lemma follows\nby taking the supremum on each side of the equality.\nIt yields the following corollary to the VC inequality.\nCorollary: Let H be a family of classifiers with VC dimension d. Then the empirical\nˆ\nrisk classifier herm over H satisfies\nerm\nr\n2d log(2en/d)\nˆ\nR(h\n) ≤min R(h) + 4\n+\nh∈H\nn\nr\nlog(2/δ)\n2n\nwith probability 1 -δ.\nProof. Recall from Lecture 3 that\nˆ\nR(herm) -min\n) ≤\nˆ\nR(h\n2 sup\nh∈H\nh∈H\nThe proof follows directly by applyi\nRn(h) -R(h)\n\nng (4.4) and the above lemma.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Ali Makhdoumi\nSep. 28, 2015\n5. LEARNING WITH A GENERAL LOSS FUNCTION\nIn the previous lectures we have focused on binary losses for the classification problem and\ndeveloped VC theory for it. In particular, the risk for a classification function h : X →{0, 1}\nand binary loss function the risk was\nR(h) = IP(h(X) = Y ) = IE[1I(h(X) = Y )].\nIn this lecture we will consider a general loss function and a general regression model where\nY is not necessarily a binary variable. For the binary classification problem, we then used\nthe followings:\n- Hoeffding's inequality: it requires boundedness of the loss functions.\n- Bounded difference inequality: again it requires boundedness of the loss functions.\n- VC theory: it requires binary nature of the loss function.\nLimitations of the VC theory:\n- Hard to find the optimal classification: the empirical risk minimization optimization,\ni.e.,\nn\nmin\nh\nn\nX\n1I(h(Xi) = Yi)\ni=1\nis a difficult optimization.\nEven though it is a hard optimization, there are some\nalgorithms that try to optimize this function such as Perceptron and Adaboost.\n- This is not suited for regression. We indeed know that classification problem is a\nsubset of Regression problem as in regression the goal is to find IE[Y |X] for a general\nY (not necessarily binary).\nIn this section, we assume that Y ∈[-1, 1] (this is not a limiting assumption as all the\nresults can be derived for any bounded Y ) and we have a regression problem where (X, Y ) ∈\nX × [-1, 1]. Most of the results that we preset here are the analogous to the results we had\nin binary classification. This would be a good place to review those materials and we will\nrefer to the techniques we have used in classification when needed.\n5.1 Empirical Risk Minimization\n5.1.1\nNotations\nLoss function: In binary classification the loss function was 1I(h(X) = Y ).\nHere, we\nreplace this loss function by l(Y, f(X)) which we assume is symmetric, where f ∈F,\nf : X →[-1, 1] is the regression functions. Examples of loss function include\n\n- l(a, b) = 1I(a = b) ( this is the classification loss function).\n- l(a, b) = |a -b|.\n- l(a, b) = (a -b)2.\n- l(a, b) = |a -b|p, p ≥1.\nWe further assume that 0 ≤l(a, b) ≤1.\nRisk: risk is the expectation of the loss function, i.e.,\nR(f) = IEX,Y [l(Y, f(X))],\nwhere the joint distribution is typically unknown and it must be learned from data.\nData: we observe a sequence (X1, Y1), . . . , (Xn, Yn) of n independent draws from a joint\ndistribution PX,Y , where (X, Y ) ∈X × [-1, 1].\nWe denote the data points by Dn =\n{(X1, Y1), . . . , (Xn, Yn)}.\nEmpirical Risk: the empirical risk is defined as\nn\nˆRn(f) = n\nX\nl(Yi, f(Xi)),\ni=1\nˆ\nˆ\nand the empirical risk minimizer denoted by f erm (or f) is defined as the minimizer of\nempirical risk, i.e.,\nˆ\nargmin Rn(f).\nf∈F\nˆ\nIn order to control the risk of f we shall compare its performance with the following oracle:\nf ∈argmin R(f).\nf∈F\nNote that this is an oracle as in order to find it one need to have access to PXY and then\nˆ\noptimize R(f) (we only observe the data Dn). Since f is the minimizer of the empirical\nˆ\nˆ\nˆ\n\nrisk minimizer, we have that Rn(f) ≤Rn(f), which leads to\nˆ\nR(f) ≤\nˆ\nR(f) -ˆ\nˆ\nˆ\nˆ\nˆ\n\nˆ\n\nRn(f) + Rn(f) -Rn(f) + Rn(f) -R(f) + R(f)\n≤\n\nˆ\nˆ\nˆ\nˆ\n\nˆ\nR(f) + R(f) -Rn(f) + Rn(f) -R(f) ≤R(f) + 2 sup\nf∈F\n|Rn(f) -R(f)|.\nTherefore, the quantity of interest that we need to bound is\nsup | ˆRn(f) -R(f)\nf∈F\n|.\nMoreover, from the bounded difference inequality, we know that since the loss function l(·, ·)\nˆ\nis bounded by 1, supf∈F |Rn(f) -R(f)| has the bounded difference property with ci = 1\nn\nfor i = 1, . . . , n, and the bounded difference inequality establishes\nP\n\"\n2t2\nsup | ˆ\nˆ\nRn(f) -R(f) | -IE\n\"\nsup |Rn(f) -R(f)\nf∈F\n|\n#\n≥t\nf∈F\n#\n-\n≤exp\n\nx\ni\ni\n\n= e p\nc\n-2nt2 ,\nwhich in turn yields\n\nP\nlog (1/delta)\n| ˆ\nsup Rn(f) -R(f)| ≤I\n| ˆ\nE\n\"\nsup Rn(f) -R(f)\nδ\nf∈F\nf\n|\n#\n+\n∈\nr\n, w.p. 1\nF\n2n\n-.\nˆ\nAs a result we only need to bound the expectation IE[supf∈F |Rn(f) -R(f)|].\n\n5.1.2\nSymmetrization and Rademacher Complexity\nSimilar to the binary loss case we first use symmetrization technique and then intro-\nduce Rademacher random variables. Let Dn = {(X1, Y1), . . . (Xn, Yn)} be the sample set\nand define an independent sample (ghost sample) with the same distribution denoted by\nD′\nn = {(X′\n1, Y ′\n1), . . . (X′\nn, Y ′\nn)}( for each i, (X′\ni, Y ′\ni ) is independent from Dn with the same\ndistribution as of (Xi, Yi)).\nAlso, let σi ∈{-1, +1} be i.i.d.\nRad(1) random variables\nindependent of Dn and D′\nn. We have\nIE\n\"\nn\nsup |\nl\ni\nf∈F n\nX\n(Yi, f(X ))\ni=1\n-IE [l(Yi, f(Xi))] |\n#\nn\nn\n= IE\n\"\nsup\nl(Yi, f(X\nl(Y ′\ni))\nIE\ni , f(X′\ni)) Dn\nf∈F\n|n\nX\ni=1\n-\n\"\nn\nX\ni=1\n|\n#\n|\n#\nn\nn\n= IE\n\"\nsup |IE\n\"\nX\nl(Yi, f(X\n′\ni))\nl(Yi , f(X′\ni)) Dn\nf∈F\nn i=1\n-n\nX\ni=1\n|\n#\n|\n#\nn\n(a)\n≤IE\n\"\nn\nsup IE\n\"\n|\nX\nl(Yi, f(X\n′\ni)) -\nX\nl(Y , f(X′\ni\ni))| |Dn\nf∈F\nn\nn\ni=1\ni=1\n##\n≤IE\n\"\nn\nn\nsup |\nX\nl(Yi, f(Xi))\nl(Y ′\ni , f(X′\nf∈F n\nn\ni))\ni=1\n-\nX\ni=1\n|\n#\n(b)\n= IE\n\"\nn\nsup |\nX\nσi\nl(Yi, f(Xi)) -l(Y ′\nX\nf\nF n\ni , f(\n′\ni))\n∈\ni=1\n\n|\n#\nn\n(c)\n≤2IE\n\"\nsup\nf∈F\n|n\nX\nσil(Yi, f(Xi))\ni=1\n|\n#\nn\n≤2 sup IE\n\"\nsup |\nX\nσil(yi, f(xi))\nDn\nf∈F n i=1\n|\n#\n.\nwhere (a) follows from Jensen's inequality with convex function f(x) = x , (b) follows from\nthe fact that (X , Y ) and (X′\n′\n| |\ni\ni\ni, Yi ) has the same distributions, and (c) follows from triangle\ninequality.\nRademacher complexity: of a class F of functions for a given loss function l(·, ·) and\nsamples Dn is defined as\nn\nRn(l*F) = sup IE\n\"\nsup |\nX\nσil(yi, f(xi))\n.\nDn\nf∈F n i=1\n|\n#\nTherefore, we have\nIE\n\"\nn\nsup |\nX\nl(Yi, f(Xi))\nf∈F n i=1\n-IE[l(Yi, f(Xi))]|\n#\n≤2Rn(l*F)\nand we only require to bound the Rademacher complexity.\n5.1.3\nFinite Class of functions\nSuppose that the class of functions F is finite. We have the following bound.\n\nTheorem: Assume that F is finite and that ltakes values in [0, 1]. We have\nr\n2 log(2\nRn(l*F)\n|F|)\n≤\n.\nn\nProof. From the previous lecture, for B ⊆\nn\nR , we have that\nn\n2 log(2 B )\nRn(B) = IE\n\"\nmax\nb∈B |n\nX\nσibi\ni=1\n|\n#\n|\n|\n≤max\nb∈B |b|2\np\n.\nn\nHere, we have\n\nl(y\n(x\n\n1, f\n1))\n\n.\nB =\n.\n,\n.\n\nl(yn, f(xn )\nf ∈F\n\n.\n)\n\nSince ltakes values in [0, 1], this\n\nim\n\nplies B\n\n⊆{b : |b|2\n√\n≤\n\nn}. Plugging this bound in the\nprevious inequality completes the proof.\n5.2 The General Case\nRecall that for the classification problem, we had F ⊂{0, 1}X . We have seen that the\ncardinality of the set {(f(x1), . . . , f(xn)), f\nˆerm\n∈F} plays an important role in bounding the\nrisk of f\n(this is not exactly what we used but the XOR argument of the previous lecture\nallows us to show that the cardinality of this set is the same as the cardinality of the set\nthat interests us). In this lecture, this set might be uncountable. Therefore, we need to\nintroduce a metric on this set so that we can treat the close points in the same manner. To\nthis end we will define covering numbers (which basically plays the role of VC dimension\nin the classification).\n5.2.1\nCovering Numbers\nDefinition: Given a set of functions F and a pseudo metric d on F ((F, d) is a metric\nspace) and ε > 0. An ε-net of (F, d) is a set V such that for any f ∈F, there exists\ng ∈V such that d(f, g) ≤ε. Moreover, the covering numbers of (F, d) are defined by\nN(F, d, ε) = inf{|V | : V is an ε-net}.\nFor instance, for the F shown in the Figure 5.2.1 the set of points {1, 2, 3, 4, 5, 6} is a\ncovering. However, the covering number is 5 as point 6 can be removed from V and the\nresulting points are still a covering.\nDefinition: Given x = (x1, . . . , xn), the conditional Rademacher average of a class of\n\nfunctions F is defined as\nRˆx\nn = IE\n\"\nn\nsup\nσ\nf∈F\n\nn\nX\nif(xi)\ni=1\n#\n.\nNote that in what follows we consider a general class of functions F.\nHowever, for\napplying the results in order to bound empirical risk minimization, we take xi to be (xi, yi)\nand F to be l*F. We define the empirical l1 distance as\nn\ndx\n1(f, g) = n\nX\ni\n=1\n|f(x )\ni\n-g(xi)|.\nTheorem: If 0 ≤f ≤1 for all f ∈F, then for any x = (x1, . . . , xn), we have\nˆRx\nn(F) ≤inf\nε≥0\nr\nx\n\n2 log (2N(F, d\nε +\n1, ε))\nn\n\n.\nProof. Fix x = (x1, . . . , xn) and ε > 0.\nLet V be a minimal ε-net of (F, dx\n1).\nThus,\nby definition we have that |V | = N(F, dx\n1, ε). For any f ∈F, define f *∈V such that\nF\no\n\ndx\n1(f, f *) ≤ε. We have that\nn\nRx\nn(F) = IE\n\"\nsup\nσif(xi)\nf∈F\n|n\nX\ni=1\n|\n#\n≤IE\n\"\nn\nn\nsup |\nX\nσi(f(xi)\nf *(xi))\n+ IE sup\nσif *(xi)\nf∈F n\nf∈F n\ni=1\n-\n|\n#\n\"\n|\nX\ni=1\n|\n#\n≤ε + IE\n\"\nn\nmax\nσif(xi)\nf∈V |n\nX\ni=1\n|\n#\nr\n2 log(2\n≤ε +\n|V |)\nn\nr\n2 log(2N(\n= ε +\nF, dx\n1, ε)).\nn\nSince the previous bound holds for any ε, we can take the infimum over all ε ≥0 to obtain\nx\nr\n\n2 log(2N(F, dx\nRn(F) ≤inf\nε +\n1, ε))\nε≥0\nn\n\n.\nThe previous bound clearly establishes a trade-offbecause as ε decreases N(F, dx\n1, ε) in-\ncreases.\n5.2.2\nComputing Covering Numbers\nAs a warm-up, we will compute the covering number of the l2 ball of radius 1 in\nd\nR denoted\nby B2. We will show that the covering is at most (3\nε)d. There are several techniques to\nprove this result: one is based on a probabilistic method argument and one is based on\ngreedily finding an ε-net. We will describe the later approach here. We select points in V\none after another so that at step k, we have uk ∈B2 \\ ∪k\nj=1B(uj, ε). We will continue this\nprocedure until we run out of points. Let it be step N. This means that V = {u1, . . . , uN}\nis an ε-net. We claim that the balls B(ui, ε) and B(uj, ε) for any i, j\n∈{ , . . . , N} are\ndisjoint. The reason is that if v ∈B(ui, ε) ∩B(uj, ε), then we would have\nε\nε\n∥ui -uj∥2 ≤∥ui -v∥2 + ∥v -uj∥2 ≤\n+\n= ε,\nwhich contradicts the way we have chosen the points. On the other hand, we have that\n∪N\nj=1B(uj, ε) ⊆(1 + ε)B2. Comparing the volume of these two sets leads to\nε\nε\n|V |( )dvol(B2) ≤(1 +\n)dvol(B2) ,\nwhere vol(B2) denotes the volume of the unit Euclidean ball in d dimensions. It yields,\n|V | ≤\n1 + ε d\nd\nd\n=\n+ 1\n.\nε\nε\n\nd\n\n≤\n\nε\n\nFor any p ≥1, define\ndx\np(f, g) =\n\nn\np\nX\n|f(xi)\ng(x ) p\ni\n,\nn i=1\n-\n|\n!\nand for p = inf, define\ndx\ninf(f, g) = max |f(xi) -g(xi)\ni\n|.\nˆ\nUsing the previous theorem, in order to bound Rx\nn we need to bound the covering number\nwith dx\n1 norm. We claim that it is sufficient to bound the covering number for the infinity-\nnorm. In order to show this, we will compare the covering number of the norms dx\np(f, g) =\nn\nPn\ni=1 |f(x\np\ni) -g(xi)|\n\np for p ≥1 and conclude that a bound on N(F, dx\ninf, ε) implies a\nbound on N(F, dx\np, ε) for any p ≥1.\nProposition: For any 1 ≤p ≤q and ε > 0, we have that\nN(F, dx\np, ε) ≤N(F, dx\nq , ε).\nProof. First note that if q = inf, then the inequality evidently holds. Because, we have\nn\n(\nX\n|zi|p) p ≤max\nn\ni\ni=1\n|zi|,\nwhich leads to B(f, dx\ninf, ε) ⊆B(f, dx\np, ε) and N(f, dinf, ε) ≥N(f, dp, ε). Now suppose that\n1 ≤p ≤q < inf. Using H older's inequality with r = q\np ≥1 we obtain\n\n! 1\n\n!(1\n1 ) 1\n! 1\n\n! 1\n-\nn\nn\nn\np\nr\np\npr\nn\nq\nn\nX\n|z |p\ni\n≤\n-\nn\np\nX\ni\nX\ni=1\n|zi|pr\n=\nn\ni=1\n=\nX\n.\ni\n|zi|q\n=1\nThis inequality yeilds\nB(f, dx\nq, ε) = {g : dx\nq(f, g) ≤ε} ⊆B(f, dx\np, ε),\nwhich leads to N(f, dq, ε) ≥N(f, dp, ε).\nUsing this propositions we only need to bound N(F, dx\ninf, ε).\nLet the function class be F = {f(x) = ⟨f, x⟩, f ∈Bd, x ∈Bd}, where 1\np\nq\n+\n= 1. This\np\nq\nleads to |f| ≤1.\nClaim: N(F, dx\ninf, ε) ≤(2)d.\nε\nThis leads to\nx\nr\n2d log(4/ε)\nˆRn(F) ≤inf\n0{ε +\n.\nε>\nn\n}\nTaking ε = O(\nq\nd log n), we obtain\nn\nˆRx\nd\nn(F) ≤O(\nr\nlog n).\nn\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Zach Izzo\nSep. 30, 2015\nIn this lecture, we continue our discussion of covering numbers and compute upper\nˆ\nbounds for specific conditional Rademacher averages Rx\nn(F). We then discuss chaining and\nconclude by applying it to learning.\nRecall the following definitions. We define the risk function\nR(f) = IE[l(X, f(X))],\n(X, Y ) ∈X × [-1, 1] ,\nfor some loss function l(·, ·). The conditiona Rademacher average that we need to control\nis\nn\nR(ll *F) =\nsup\nIE sup\nσil(yi, f(xi))\n.\n(x1,y1),...,(xn,yn)\n\"\nf\nn\n∈F\nX\ni=1\n#\nFurthermore, we defined the conditional Rademacher average for a poin\n\nt x = (x1, . . . , xn)\nto be\n\nRˆx\nn(F) = IE\n\"\nsup\nf∈F\n\nn\nσif(xi)\nn i=1\n#\n.\nLastly, we define the ε-covering number N(\nX\nF, d,\n\nε) to be the m\n\ninimum number of balls (with\nrespect to the metric d) of radius ε needed to\n\ncover\n\nF. We proved the following theorem:\nTheorem: Assume |f| ≤1 for all f ∈F. Then\n2 log(2N(\n, dx, ε))\nRˆx\nn(F)\n+\nr\nF\n≤inf\nε>0\n(\nε\nn\n)\n,\nwhere dx\n1 is given by\nn\ndx\n1(f, g) = n\nX\ni=1\n|f(xi) -g(xi)|.\nWe make use of this theorem in the following example. Define Bd\np = {x ∈IRd : |x|p ≤1}.\nThen take f(x) = ⟨a, x⟩, set F = {⟨a, ·⟩: a ∈Bd }, and X = Bd\n1. By H older's inequality,\ninf\nwe have\n|f(x)| ≤|a|\nx\ninf| |1 ≤1,\nso the theorem above holds. We need to compute the covering number N(F, dx\n1, ε). Note\nthat for all a ∈Bd , there exists v = (v1, . . . , vn) such that vi = g(xi) and\ninf\nn\nn\nX\na, xi\nvi\nε\ni=1\n|⟨\n⟩-\n| ≤\nfor some function g. For this case, we will take g(x) = ⟨b, x⟩, so vi = ⟨b, xi⟩. Now, note the\nfollowing. Given this definition of g, we have\nn\nn\ndx\n1(f, g) =\na, x1\nb, xi\n=\na\nb, xi\na\nb\nn\nX\nn\ni=1\n|⟨\n⟩-⟨\n⟩|\nX\ni=1\n|⟨-\n⟩| ≤| -|inf\n\nby H older's inequality and the fact that |x|1 = 1. So if |a-b|inf≤ε, we can take vi = ⟨b, xi⟩.\nWe just need to find a set of {b1, . . . , bM} ⊂IRd such that, for any a there exists bj such\nthat |a -bj|\n< inf. We can do this by dividing Bd into cubes with side length ε and\ninf\ninf\ntaking the b 's to be the set of vertices of these cubes. Then any a ∈Bd\nj\nmust land in one\ninf\nof these cubes, so |a -bj|\n≤ε as desired. There are c/εd of such b\ninf\nj's for some constant\nc > 0. Thus\nN(Bd , dx\ninf\n1, ε) ≤c/εd.\nWe now plug this value into the theorem to obtain\nRˆx\n2 log(c/εd)\nn(F) ≤inf\n.\nε\n(\nε +\n≥0\nr\nn\n)\nOptimizing over all choices of ε gives\nr\nd log(n)\nε∗= c\nn\n⇒\nRˆx\nn(F) ≤c\nr\nd log(n).\nn\nNote that in this final inequality, the conditional empirical risk no longer depends on\nx, since we \"sup'd\" x out of the bound during our computations. In general, one should\nignore x unless it has properties which will guarantee a bound which is better than the sup.\nAnother important thing to note is that we are only considering one granularity of F in our\nfinal result, namely the one associated to ε∗. It is for this reason that we pick up an extra\nlog factor in our risk bound. In order to remove this term, we will need to use a technique\ncalled chaining.\n5.4 Chaining\nWe have the following theorem.\nTheorem: Assume that |f| ≤1 for all f ∈F. Then\n\n12 Z 1\nRˆx\nn ≤inf\n4ε +\nε>0\n√\nlog(N(\n, dx\n))dt\n.\nn\n2, t\nε\nq\nF\n\n(Note that the integrand decays with t.)\nProof. Fix x = (x1, . . . , xn), and for all j = 1, . . . , N, let Vj be a minimal 2-j-net of F\nunder the dx\n2 metric. (The number N will be determined later.) For a fixed f ∈F, this\nprocess will give us a \"chain\" of points fi\n*which converges to f: dx\n2(fi\n*, f) ≤2-j.\nDefine F = {(f(x1), . . . , f(xn))⊤, f ∈F} ⊂[-1, 1]n. Note that\nRˆx\nn(F) =\nIE sup\nn\nf∈F\n⟨σ, f⟩\nwhere σ = (σ1, . . . , σn). Observe that for all N, we can rewrite ⟨σ, f⟩as a telescoping sum:\n⟨σ, f⟩= ⟨σ, f -fN\n*⟩+ ⟨σ, fN\n*-fN\n*\n-1⟩+ . . . + ⟨σ, f1\n*-f0\n*⟩\n\nwhere f0\n*:= 0. Thus\nN\nRˆx\nn(F) ≤\nIE sup |⟨σ, f -fN\n*⟩| +\nI\nf\nn\nf∈F\nX\nE sup\nσ,\nn\nj\n*\nfj\n*\nf\nF\n-1\n.\nj=1\n|⟨\n-\n⟩|\n∈\nWe can control the two terms in this inequality separately. Note first that by the Cauchy-\nSchwarz inequality,\ndx\n2(f, fN\n*)\nIE sup |⟨σ, f -fN\n*⟩| ≤|σ|2\nn\nf\n√\n.\nn\n∈F\nSince |σ|2 = √n and dx\n2(f, fN\n*) ≤2-N, we have\n1 IE sup |⟨σ, f -fN\n*\nn\nf∈F\n⟩| ≤\n-N .\nNow we turn our attention to the second term in the inequality, that is\nN\nS =\nX 1 IE sup |⟨σ, fj\n*-fj\n*\nn\nf\nj=1\n∈F\n-1⟩|.\nNote that since fj\n*∈Vj and fj\n*\n-1 ∈Vj\nV\n-1, there are at most | j||Vj-1| possible differences\nfj\n*-fj\n*\n.\n-1\nSince |V\nj\n1| ≤|V\n-\nj|/2, |Vj||Vj\n1| ≤|Vj| /2 and we find ourselves in the finite\n-\ndictionary case. We employ a risk bound from earlier in the course to obtain the inequality\np\n2 log(2\nRn(B) ≤max\nb B |b|2\n|B|) .\n∈\nn\nIn the present case, B = {fj\n*-fj\n*\n-1 , f ∈F} so that |B| ≤|Vj|2/2. It yields\n|2\n2 log( |Vj )\nlog\nR\nVj\nn(B)\n|\n·\nq\n|\n≤r\n= 2r\nn\n·\np\n,\nn\nwhere r = supf∈F |fj\n*-fj\n*\n-1|2. Next, observe that\n|fj\n*-fj\n*\n1|2 = √n d\n-\n·\nx\n2(fj\n*, fj\n*\n-1)\n√\n√\n≤\nn(dx\n2(fj\n*, f) + dx\n2(f, fj\n*\n))\n3 2-j\nn .\n-1\n≤\n·\nby the triangle inequality and the fact that dx(f*, f) ≤2-j\nj\n. Substituting this back into our\nbound for Rn(B), we have\nlog\n(B)\n|Vj\n6 2-j\nn\nr\n||\n,\nj\n(\n))\n≤\n·\n= 6\nn\n·\n-\nr\nlog(N F dx\n2, 2-j\nR\nn\nsince V\nj\nj was chosen to be a minimal 2--net.\nThe proof is almost complete. Note that 2-j = 2(2-j -2-j-1) so that\nN\n√\nX\nN\n2-jq\nlog(N(F, dx\n2, 2-j)) = √\nX\n(2-j -2-j-1)\nq\nlog(N(F, dx\n2, 2-j)) .\nn\nn\nj=1\nj=1\nNext, by comparing sums and integrals (Figure 1), we see that\nX\nN\n(2-j\nj=1\n-2-j-1)\nq\nlog(N(F, dx\n2, 2-j)) ≤\nZ\n/2\nlog(N(\n, dx\n2, t))dt.\n2-(N+1)\nq\nF\n\nFigure 1: A comparison of the sum and integral in question.\nSo we choose N such that 2-(N+2) ≤ε ≤2-(N+1), and by combining our bounds we obtain\n1/2\nRˆx\n) ≤2-N\nn(F\n+ √n\nZ\nq\nlog(N(F, dx\n2, t))dt\n2-(\n+1)\n≤4ε +\nN\nZ\np\nlog(N,\nε\nF, t)dt\nsince the integrand is non-negative. (Note: this integral is known as the \"Dudley Entropy\nIntegral.\")\nReturning to our earlier example, since N(F, dx\n2, ε) ≤c/εd, we have\nRˆx\nn(F) ≤inf\n\n4ε + √\nZ\nq\nlog((c′/t)d)dt\nε>0\nn\nε\n\n.\nSince\nR 1 p\nlog(c/t)dt = c is finite, we then have\nRˆx\nn(F) ≤12c\np\nd/n.\nUsing chaining, we've been able to remove the log factor!\n5.5 Back to Learning\nWe want to bound\nn\nRn(l*F) =\nsup\nIE sup\nσil(yi, f(xi))\n.\n(x1,y1),...,(xn,yn)\n\"\nf∈F\n\nn\nX\ni\nx\n\n=1\n#\n\nRˆ\nWe consider\nn(Φ *F\nn\n) = IE\niΦ\n\nsupf\n1 P\ni=1 σ\n*f(x )\n∈F\ni\nfor some L\nn\n\n-Lipschitz function\nΦ, that is |Φ(a) -Φ(b)| ≤L|a -b| for all a, b ∈[-1, 1]. We\n\nhave the following lemma.\n\nTheorem: (Contraction Inequality) Let Φ be L-Lipschitz and such that Φ(0) = 0,\nthen\nRˆx\nn(Φ *F) ≤\nRˆ\n2L ·\nx\nn(F) .\nThe proof is omitted and the interested reader should take a look at [LT91, Kol11] for\nexample.\nAs a final remark, note that requiring the loss function to be Lipschitz prohibits the use\nof R-valued loss functions, for example l(Y, ·) = (Y -·)2.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Quan Li\nOct. 5, 2015\nPart II\nConvexity\n1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION\nˆ\nIn the previous lectures, we have proved upper bounds on the excess risk R(herm) -R(h∗)\nof the Empirical Risk Minimizer\nˆherm\n= argmin\nh∈H\nn\n1I(Yi = h(Xi)).\n(1.1)\nn\nX\ni=1\n\nHowever due to the nonconvexity of the objective function, the optimization problem\n(1.1) in general can not be solved efficiently. For some choices of H and the classification\nerror function (e.g. 1I(·)), the optimization problem can be NP-hard. However, the problem\nwe deal with has some special features:\n1. Since the upper bound we obtained on the excess risk is O(\nq\nd log n), we only need to\nn\napproximate the optimization problem with error up to O(\nq\nd log n\nn\n).\n2. The optimization problem corresponds to the average case problem where the data\ni.i.d\n(Xi, Yi) ∼PX,Y .\n3. H can be chosen to be some 'natural' classifiers, e.g. H = {half spaces}.\nThese special features might help us bypass the computational issue. Computational\nissue in machine learning have been studied for quite some time (see, e.g. [Kea90]), especially\nin the context of PAC learning. However, many of these problems are somewhat abstract\nand do not shed much light on the practical performance of machine learning algorithms.\nTo avoid the computational problem, the basic idea is to minimize a convex upper bound\nof the classification error function 1I(·) in (1.1). For the purpose of computation, we shall\nalso require that the function class H be a convex set. Hence the resulting minimization\nbecomes a convex optimization problem which can be solved efficiently.\n1.1 Convexity\nDefinition: A set C is convex if for all x, y ∈C and λ ∈[0, 1], λx + (1 -λ)y ∈C.\n\nDefinition: A function f : D →IR on a convex domain D is convex if it satisfies\nf(λx + (1 -λ)y) ≤λf(x) + (1 -λ)f(y),\n∀x, y ∈D, and λ ∈[0, 1].\n1.2 Convex relaxation\nThe convex relaxation takes three steps.\nStep 1: Spinning.\nUsing a mapping Y 7→2Y -1, the i.i.d. data (X1, Y1), (X2, Y2), . . . , (Xn, Yn) is transformed\nto lie in X × {-1, 1}. These new labels are called spinned labels. Correspondingly, the task\nbecomes to find a classifier h : X 7→{-1, 1}. By the relation\nh(X) = Y ⇔-h(X)Y > 0,\nwe can rewrite the objective function in (1.1) by\nn\nn\n1 X\n1I(h(Xi) = Yi) =\nX\nφ1I( h\nn\nn\ni=1\ni=1\n-(Xi)Yi)\n(1.2)\nwhere φ1I(z) = 1I(z > 0).\nStep 2: Soft classifiers.\nThe set H of classifiers in (1.1) contains only functions taking values in {-1, 1}. As a result,\nit is non convex if it contains at least two distinct classifiers. Soft classifiers provide a way\nto remedy this nuisance.\nDefinition: A soft classifier is any measurable function f : X →[-1, 1]. The hard\nclassifier (or simply \"classifier\") associated to a soft classifier f is given by h = sign(f).\nLet F ⊂IRX be a convex set soft classifiers. Several popular choices for F are:\n- Linear functions:\nF := {⟨a, x⟩: a ∈A}.\nfor some convex set A ∈IRd. The associated hard classifier h = sign(f) splits IRd into\ntwo half spaces.\n- Majority votes: given weak classifiers h1, . . . , hM,\nM\nM\nF :=\nn X\nλjhj(x) : λj\nj=\n≥0,\nX\nλj = 1\no\n.\nj=1\n- Let φj, j = 1, 2, . . . a family of functions, e.g., Fourier basis or Wavelet basis. Define\ninf\nF := {\nX\nθjφj(x) : (θ1, θ2, . . .)\nj=1\n∈Θ},\nwhere Θ is some convex set.\n\nStep 3: Convex surrogate.\nGiven a convex set F of soft classifiers, using the rewriting in (1.2), we need to solve that\nminimizes the empirical classification error\nmin\nf∈F\nn\nφ1I( f(Xi)Yi),\nn\nX\ni=1\n-\nHowever, while we are now working with a convex constraint, our objective is still not\nconvex: we need a surrogate for the classification error.\nDefinition: A function φ : IR 7→IR+ is called a convex surrogate if it is a convex\nnon-decreasing function such that φ(0) = 1 and φ(z) ≥φ1I(z) for all z ∈IR.\nThe following is a list of convex surrogates of loss functions.\n- Hinge loss: φ(z) = max(1 + z, 0).\n- Exponential loss: φ(z) = exp(z).\n- Logistic loss: φ(z) = log2(1 + exp(z)).\nTo bypass the nonconvexity of φ1I(·), we may use a convex surrogate φ(·) in place of\nˆ\nφ1I(·) and consider the minimizing the empirical φ-risk Rn,φ defined by\nˆRn,φ(f) = n\nn\nX\ni=1\nφ(-Yif(Xi))\nIt is the empirical counterpart of the φ-risk Rφ defined by\nRφ(f) = IE[φ(-Y f(X))].\n1.3 φ-risk minimization\nIn this section, we will derive the relation between the φ-risk Rφ(f) of a soft classifier f and\nthe classification error R(h) = IP(h(X) = Y ) of its associated hard classifier h = sign(f)\nLet\nf ∗\nφ = argmin E[φ( Y\nf∈IRX\n-\nf(X))]\nwhere the infimum is taken over all measurable functions f : X →IR.\nTo verify that minimizing the φ serves our purpose, we will first show that if the convex\nsurrogate φ(·) is differentiable, then sign(f ∗\nφ(X)) ≥0 is equivalent to η(X) ≥1/2 where\nη(X) = IP(Y = 1 | X). Conditional on {X = x}, we have\nIE[φ(-Y f(X)) | X = x] = η(x)φ(-f(x)) + (1 -η(x))φ(f(x)).\nLet\nHη(α) = η(x)φ(-α) + (1 -η(x))φ(α)\n(1.3)\n\nso that\nf ∗\nφ(x) = argmin H\n∗\nη(α) ,\nand\nRφ = min Rφ(f) = min Hη\n)\nα\nf∈IRX\n(x)(α .\nα∈IR\n∈IR\nSince φ(·) is differentiable, setting the derivative of H\n∗\nη(α) to zero gives fφ(x) = α , where\nH′\nη(α ) = -η(x)φ′(-α ) + (1 -η(x))φ′(α ) = 0,\nwhich gives\nη(x)\nφ′(α )\n=\n1 -η(x)\nφ′(-α )\nSince φ(·) is a convex function, its derivative φ′(·) is non-decreasing. Then from the equation\nabove, we have the following equivalence relation\nη(x) ≥\n⇔α ≥0 ⇔sign(f ∗\nφ(x)) ≥0.\n(1.4)\nSince the equivalence relation holds for all x ∈X,\nη(X) ≥\n⇔sign(f ∗\nφ(X))\n≥0.\nThe following lemma shows that if the excess φ-risk R (f) -R∗\nφ\nφ of a soft classifier f is\nsmall, then the excess-risk of its associated hard classifier sign(f) is also small.\nLemma (Zhang's Lemma [Zha04]): Let φ : IR 7→IR+ be a convex non-decreasing\nfunction such that φ(0) = 1. Define for any η ∈[0, 1],\nτ(η) := inf Hη(α).\nα∈IR\nIf there exists c > 0 and γ ∈[0, 1] such that\n|η -\nc\n2| ≤(1 -τ(η))γ ,\n∀η ∈[0, 1] ,\n(1.5)\nthen\nR(sign(f)) -R∗≤2c(Rφ(f) -R∗\nφ)γ\nProof. Note first that τ(η) ≤Hη(0) = φ(0) = 1 so that condition (2.5) is well defined.\nNext, let h∗= argminh∈{-1,1}X IP[h(X) = Y ] = sign(η-1/2) denote the Bayes classifier,\nwhere η = IP[Y = 1|X = x], . Then it is easy to verify that\nR(sign(f)) -R∗= IE[|2η(X) -1|1I(sign(f(X)) = h∗(X))]\n= IE[|2η(X) -1|1I(f(X)(η(X) -1/2) < 0)]\n≤2cIE[((1 -τ(η(X)))1I(f(X)(η(X) -1/2) < 0))γ]\n≤2c (IE[(1 -τ(η(X)))1I(f(X)(η(X) -1/2) < 0)])γ ,\nwhere the last inequality above follows from Jensen's inequality.\n\nWe are going to show that for any x ∈X, it holds\n(1 -τ(η))1I(f(x)(η(x) -1/2) < 0)] ≤IE[φ(-Y f(x)) | X = x] -R∗\nφ .\n(1.6)\nThis will clearly imply the result by integrating with respect to x.\nRecall first that\nIE[φ(-Y f(x)) | X = x] = Hη(x)(f(x))\nand\nR∗\nφ = min Hη(x)(α) = τ(η(x)) .\nα∈IR\nso that (2.6) is equivalent to\n(1 -τ(η))1I(f(x)(η(x) -1/2) < 0)] ≤Hη(x)(α) -τ(η(x))\nSince the right-hand side above is nonnegative, the case where f(x)(η(x) -1/2) ≥0 follows\ntrivially. If f(x)(η(x)-1/2) < 0, (2.6) follows if we prove that Hη(x)(α) ≥1. The convexity\nof φ(·) gives\nHη(x)(α) = η(x)φ(-f(x)) + (1 -η(x))φ(f(x))\n≥φ(-η(x)f(x) + (1 -η(x))f(x))\n= φ((1 -2η(x))f(x))\n≥φ(0) = 1 ,\nwhere the last inequality follows from the fact that φ is non decreasing and f(x)(η(x) -\n1/2) < 0. This completes the proof of (2.6) and thus of the Lemma.\nIT is not hard to check the following values for the quantities τ(η), c and γ for the three\nlosses introduced above:\n- Hinge loss: τ(η) = 1 -|1 -2η| with c = 1/2 and γ = 1.\n- Exponential loss: τ(η) = 2\np\nη(1 -η) with c = 1/\n√\n2 and γ = 1/2.\n- Logistic loss: τ(η) = -η log η -(1 -η) log(1 -η) with c = 1/\n√\n2 and γ = 1/2.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Xuhong Zhang\nOct. 7, 2015\nRecall that last lecture we talked about convex relaxation of the original problem\nn\nˆh = argmin\n1I(h(Xi) = Yi)\nh∈H\nn\nX\ni=1\nby considering soft classifiers (i.e. whose output is in [-1, 1] rather than in {0, 1}) and\nconvex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss):\nn\nˆ\nˆ\nf = argmin Rφ,n(f) = argmin\nφ( Yif(Xi))\nf∈F\nf∈F\nn\nX\ni=1\n-\nˆ\nˆ\nAnd h = sign(f) will be used as the 'hard' classifier.\nˆ\n\nWe want to bound the quantity Rφ(f) -Rφ(f), where f = argminf∈F Rφ(f).\nˆ\nˆ\n(1) f = argminf∈F Rφ,n(f), thus\nˆ\n\nˆ\n\nˆ\n\nˆ\nˆ\nˆ\nˆ\nˆ\n\nRφ(f) = Rφ(f) + Rφ,n(f) -Rφ,n(f) + Rφ,n(f) -Rφ,n(f) + Rφ(f) -Rφ(f)\n≤\n\nˆ\n\nˆ\nˆ\nˆ\n\nRφ(f) + Rφ,n(f) -Rφ,n(f) + Rφ(f) -Rφ(f)\n≤\n\nˆ\nRφ(f) + 2 sup |Rφ,n(f) -Rφ(f)\nf∈F\n|\nˆ\n(2) Let us first focus on E[supf∈F |Rφ,n(f) -Rφ(f)|]. Using the symmetrization trick as\nbefore, we know it is upper-bounded by 2Rn(φ*F), where the Rademacher complexity\nn\nRn(φ *F) =\nsup\nE[sup |\nX\nσiφ(-Yif(Xi)) ]\nX1,...,Xn,Y1,...,Yn\nf∈F n i=1\n|\nOne thing to notice is that φ(0) = 1 for the loss functions we consider (hinge loss,\nexponential loss and logistic loss), but in order to apply contraction inequality later,\nwe require φ(0) = 0. Let us define ψ(·) = φ(·) -1. Clearly ψ(0) = 0, and\nn\nE[sup |\nX\n(φ(-Yif(Xi)) -E[φ(-Yif(Xi))]) ]\nf∈F n i=1\n|\nn\n= E[sup |\nX\n(ψ(-Yif(X ) -E\ni )\n[ψ(\ni\n-Yif(X ))])\nf∈F n i=\n|]\n≤2Rn(ψ *F)\n(3) The Rademacher complexity of ψ *F is still difficult to deal with. Let us assume\nthat φ(·) is L-Lipschitz, (as a result, ψ(·) is also L-Lipschitz), apply the contraction\ninequality, we have\nRn(ψ *F) ≤2LRn(F)\n\n(4) Let Zi = (Xi, Yi), i = 1, 2, ..., n and\nn\ng(Z1, Z2\nˆ\n, ..., Zn) = sup |Rφ,n(f)-Rφ(f) =\nf\n|\nsup\n∈F\nf∈F\n|n\nX\n(φ(\ni=1\n-Yif(Xi))-E[φ(-Yif(Xi))])|\nSince φ(·) is monotonically increasing, it is not difficult to verify that ∀Z1, Z2, ..., Zn, Z′\ni\n2L\n|g(Z1, ..., Zi, ..., Zn) -g(Z1, ..., Z′\ni, ..., Zn)| ≤\n(φ(1) -φ(\nn\n-1)) ≤n\nThe last inequality holds since g is L-Lipschitz. Apply Bounded Difference Inequality,\n2t2\nP(| s\n| ˆ\nˆ\nup Rφ,n(f) -Rφ(f)| -E[sup |Rφ,n(f) -Rφ(f)|] >\nF\n∈F\n|\nt) ≤2 exp(\n∈\n-\n)\nf\nPn\nf\ni= (2L)2\nn\nSet the RHS of above equation to δ, we get:\nlog(2/δ)\nˆ\nˆ\nsup R\nE\nφ,n(f)\nRφ(f)\n[sup Rφ,n(f)\nf∈F\n|\n-\n| ≤\nf∈F\n|\n-Rφ(f)|] + 2L\nr\n2n\nwith probability 1 -δ.\n(5) Combining (1) - (4), we have\nˆ\n\nRφ(f) ≤Rφ(f) + 8LRn(F) + 2L\nr\nlog(2/δ)\n2n\nwith probability 1 -δ.\n1.4 Boosting\nIn this section, we will specialize the above analysis to a particular learning model: Boosting.\nThe basic idea of Boosting is to convert a set of weak learners (i.e. classifiers that do better\nthan random, but have high error probability) into a strong one by using the weighted\naverage of weak learners' opinions. More precisely, we consider the following function class\nM\nF = {\nX\nθjhj(·) : |θ|1 ≤1, hj : X 7→[-1, 1], j ∈{1, 2, ..., M\na\nj=1\n}\nre classifiers}\nand we want to upper bound Rn(F) for this choice of F.\nn\nM\nn\nRn(F) =\nsup\nE[sup\nσiYif(Xi) ] =\nsup\nE[ sup\nθj\nYiσihj(Xi) ]\nZ1,...,Zn\nf∈F\n|n\nX\nn Z\n|θ|1≤1\n|\n1,...,Z\ni=\nn\n|\nX\nj=1\nX\ni=1\n|\nLet g(θ) = | PM\nj=1 θj\nPn\ni=1 Yiσihj(Xi)|. It is easy to see that g(θ) is a convex function, thus\nsup|θ|1≤1 g(θ) is achieved at a vertex of the unit l1 ball {θ : ∥θ∥1 ≤1}. Define the finite set\nY1h1(X1)\nY1h2(X1)\nY1hM(X1)\n(\nY2h1(X2)\nY2h2(X2)\nY2hM(X2)\nBX,Y ≜\n\n.\n,\n, . . . ,\n.\n\n±\n\n.\n±\n±\n)\nYnh1(Xn)\n\n.\n.\n.\nYnh2(Xn)\n\n.\n.\n.\nYnhM(Xn)\n\nThen\nRn(F) = sup Rn(BX,Y) .\nX,Y\nNotice maxb∈BX,Y b\n√\n| |2 ≤\nn and |BX,Y| = 2M. Therefore, using a lemma from Lecture 5,\nwe get\n2 log(2 B\nR\nX,Y )\nlog( M)\nn(BX,Y) ≤\nmax\nb∈BX,Y |b|2\np\n|\n|\nn\n≤\nr\nn\nThus for Boosting,\n\n2 log(4M)\nlog(2/δ)\nˆ\n\nRφ(f) ≤Rφ(f) + 8L\nr\n+ 2L\nr\nwith probability 1 - δ\nn\n2n\nTo get some ideas of what values L usually takes, consider the following examples:\n(1) for hinge loss, i.e. φ(x) = (1 + x)+, L = 1.\n(2) for exponential loss, i.e. φ(x) = ex, L = e.\n(3) for logistic loss, i.e. φ(x) = log2(1 + ex), L =\ne\n1+e log2(e) ≈2.43\nˆ\n\nNow we have bounded Rφ(f) -Rφ(f), but this is not yet the excess risk. Excess risk is\nˆ\ndefined as R(f) -R(f ∗), where f ∗= argminf Rφ(f). The following theorem provides a\nbound for excess risk for Boosting.\nTheorem: Let F = {PM\nj=1 θjhj : ∥θ∥1 ≤1, hjs are weak classifiers} and φ is an L-\nˆ\nˆ\nˆ\nLipschitz convex surrogate. Define f = argminf∈F Rφ,n(f) and h = sign(f). Then\nγ\nγ\n∗\n∗γ\n\n2 log(4M)\nlog(2/δ)\nˆ\nR(h) -R ≤2c\ninf Rφ(f) -Rφ(f )\n+ 2c\n8L\n+\nf∈F\nr\nn\n!\n2c\n\n2L\nr\n2n\n!\nwith probability 1 -δ\nProof.\nˆ\nR(h) -R∗≤2c\nγ\nRφ(f) -Rφ(f ∗)\n\nγ\n∗\n2 log(4M)\nlog(2/δ)\n≤2c\ninf Rφ(f)\nRφ(f ) + 8L\n+ 2L\nf∈F\n-\nr\nn\nr\n2n\n!\nγ\n∗\nγ\n2 log(4M)\nlog(2/δ)\n≤2c\ninf Rφ(f) -Rφ(f )\n+ 2c\nf∈F\n\n8L\nr\nn\n!\n+ 2c\n\n2L\nr\n2n\n!γ\n\nHere the first inequality uses Zhang's lemma and the last one uses the fact that for ai ≥0\nand γ ∈[0, 1], (a1 + a\nγ\n2 + a3) ≤aγ\n1 + aγ\n2 + aγ\n3.\n1.5 Support Vector Machines\nIn this section, we will apply our analysis to another important learning model: Support\nVector Machines (SVMs). We will see that hinge loss φ(x) = (1 + x)+ is used and the\nassociated function class is F = {f : ∥f∥W ≤λ} where W is a Hilbert space.\nBefore\nanalyzing SVMs, let us first introduce Reproducing Kernel Hilbert Spaces (RKHS).\n\n1.5.1\nReproducing Kernel Hilbert Spaces (RKHS)\nDefinition: A function K : X × X 7→IR is called a positive symmetric definite kernel\n(PSD kernel) if\n(1) ∀x, x′ ∈X, K(x, x′) = K(x′, x)\n(2) ∀n ∈Z+, ∀x1, x2, ..., xn, the n\nth\n× n matrix with K(xi, xj) as its element in ith row\nand j\ncolumn is positive semi-definite. In other words, for any a1, a2, ..., an ∈IR,\nX\naiajK(xi, xj)\ni,j\n≥\nLet us look at a few examples of PSD kernels.\nExample 1\nLet X = IR, K(x, x′) = ⟨x, x′⟩IRd is a PSD kernel, since ∀a1, a2, ..., an ∈IR\nX\naiaj⟨xi, xj⟩IRd =\nX\n⟨aixi, ajxj⟩IRd = ⟨\nX\naixi,\nX\najxj⟩IRd = ∥\nX\na\nixi∥IRd\ni,j\ni,j\ni\nj\ni\n≥\nExample 2\nThe Gaussian kernel K(x, x′) = exp(-1\n∥\n′\n2 x -x ∥IRd) is also a PSD kernel.\nσ\nNote that here and in the sequel, ∥· ∥W and ⟨·, ·⟩W denote the norm and inner product\nof Hilbert space W.\nDefinition: Let W be a Hilbert space of functions X 7→IR. A symmetric kernel K(·, ·)\nis called reproducing kernel of W if\n(1) ∀x ∈X, the function K(x, ·) ∈W.\n(2) ∀x ∈X, f ∈W, ⟨f(·), K(x, ·)⟩W = f(x).\nIf such a K(x, ·) exists, W is called a reproducing kernel Hilbert space (RKHS).\nClaim: If K(·, ·) is a reproducing kernel for some Hilbert space W, then K(·, ·) is a\nPSD kernel.\nProof. ∀a1, a2, ..., an ∈IR, we have\nX\naiajK(xi, xj) =\nX\naiaj⟨K(xi, ·), K(xj, ·)⟩(since K( , ) is reproducing)\ni,j\ni,j\n· ·\n= ⟨\nX\naiK(xi, ),\najK(xj, ) W\ni\n·\nX\nj\n· ⟩\n= ∥\nX\naiK(xi,\ni\n·)∥2\nW ≥0\n\nIn fact, the above claim holds both directions, i.e. if a kernel K(·, ·) is PSD, it is also a\nreproducing kernel.\nA natural question to ask is, given a PSD kernel K(·, ·), how can we build the corresponding\nHilbert space (for which K(·, ·) is a reproducing kernel)? Let us look at a few examples.\nExample 3\nLet φ1, φ2, ..., φM be a set of orthonormal functions in L2([0, 1]), i.e. for any\nj, k ∈{1, 2, ..., M}\nZ\nφj(x)φk(x)dx =\nx\n⟨φj, φk⟩= δjk\nLet K(x, x′) = PM\nj=1 φj(x)φj(x′). We claim that the Hilbert space\nM\nW = {\nX\najφj(\n:\n=1\n·)\na1, a2, ..., aM\nj\n∈IR}\nequipped with inner product ⟨·, ·⟩L2 is a RKHS with reproducing kernel K(·, ·).\nM\nProof.\n(1) K(x, ·) =\nj=1 φj(x)φj(·) ∈W. (Choose aj = φj(x)).\n(2) If f(·) = PM\nj=1 aj\nP\nφj(·),\nM\nM\nM\n⟨f(·), K(x, ·)⟩L2 = ⟨\nX\najφj(·),\nX\nφk(x)φk(·)⟩L2 =\nX\najφj(x) = f(x)\nj=1\nk=1\nj=1\n(3) K(x, x′) is a PSD kernel: ∀a1, a2, ..., an ∈IR,\nX\naiajK(x\ni, xj) =\nX\naiajφk(xi)φk(xj) =\nX\n(\nX\naiφk(xi))\ni,j\ni,j,k\nk\ni\n≥0\nExample 4\nIf X = IRd, and K(x, x′) = ⟨x, x′⟩IRd, the corresponding Hilbert space is\nW = {⟨w, ·⟩: w ∈IRd} (i.e. all linear functions) equipped with the following inner product:\nif f = ⟨w, ·⟩, g = ⟨v, ·⟩, ⟨f, g⟩≜⟨w, v⟩IRd.\nProof.\n(1) ∀x ∈IRd, K(x, ·) = ⟨x, ·⟩IRd ∈W.\n(2) ∀f = ⟨w, ·⟩IRd ∈W, ∀x ∈IRd, ⟨f, K(x, ·)⟩= ⟨w, x⟩IRd = f(x)\n(3) K(x, x′) is a PSD kernel: ∀a1, a2, ..., an ∈IR,\nX\naiajK(xi, xj) =\nX\naiaj\n,\n,j\ni,j\n⟨xi xj\ni\n⟩= ⟨\nX\naixi,\ni\nX\najxj\nj\n⟩IRd = ∥\nX\naix\ni IRd\ni\n∥\n≥\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Aden Forrow\nOct. 13, 2015\nRecall the following definitions from last time:\nDefinition: A function K : X × X 7→R is called a positive symmetric definite kernel\n(PSD kernel) if\n1. ∀x, x′ ∈X, K(x, x′) = K(x′, x)\n2. ∀n ∈Z+, ∀x1, x2, . . . , xn, the n × n matrix with entries K(xi, xj) is positive defi-\nnite. Equivalently, ∀a\nR\n1, a2, . . . , an ∈\n,\nn\nX\naiajK(xi, xj)\ni,j=1\n≥0\nDefinition: Let W be a Hilbert space of functions X 7→R. A symmetric kernel K(·, ·)\nis called a reproducing kernel of W if\n1. ∀x ∈X, the function K(x, ·) ∈W.\n2. ∀x ∈X, ∀f ∈W, ⟨f(·), K(x, ·)⟩W = f(x).\nIf such a K(x, ·) exists, W is called a reproducing kernel Hilbert space (RKHS).\nAs before, ⟨·, ·⟩W and ∥· ∥W respectively denote the inner product and norm of W. The\nsubscript W will occasionally be omitted. We can think of the elements of W as infinite\nlinear combinations of functions of the form K(x, ·). Also note that\n⟨K(x, ·), K(y, ·)⟩W = K(x, y)\nSince so many of our tools rely on functions being bounded, we'd like to be able to\nbound the functions in W. We can do this uniformly over x ∈X if the diagonal K(x, x) is\nbounded.\nProposition: Let W be a RKHS with PSD K such that supx∈X K(x, x) = kmax is\nfinite. Then ∀f ∈W,\nsup |f(x)| ≤∥f∥W\np\nkmax\nx∈X\n.\nProof. We rewrite f(x) as an inner product and apply Cauchy-Schwartz.\nf(x) = ⟨f, K(x, ·)⟩W ≤∥f∥W∥K(x, ·)∥W\nNow ∥K(x, ·)∥2\nW = ⟨K(x, ·), K(x, ·)⟩W = K(x, x) ≤kmax. The result follows immediately.\n\n1.5.2\nRisk Bounds for SVM\nWe now analyze support vector machines (SVM) the same way we analyzed boosting. The\ngeneral idea is to choose a linear classifier that maximizes the margin (distance to classifiers)\nwhile minimizing empirical risk. Classes that are not linearly separable can be embedded\nin a higher dimensional space so that they are linearly separable. We won't go into that,\nhowever; we'll just consider the abstract optimization over a RKHS W.\nExplicitly, we minimize the empirical φ-risk over a ball in W with radius λ:\nˆ\nˆ\nf =\nmin\nRn,φ(f)\nf∈W,∥f∥W ≤λ\nˆ\nˆ\nˆ\nThe soft classifier f is then turned into a hard classifier h = sign(f). Typically in SVM φ\nis the hinge loss, though all our convex surrogates behave similarly. To choose W (the only\nother free parameter), we choose a PSD K(x1, x2) that measures the similarity between two\npoints x1 and x2.\nAs written, this is an intractable minimum over an infinite dimensional ball {f, ∥f∥W ≤\nλ}. The minimizers, however, will all be contained in a finite dimensional subset.\nTheorem: Representer Theorem. Let W be a RKHS with PSD K and let G :\nn\nR 7→R be any function. Then\nmin\nG(f(x1), . . . , f(xn)) =\nmin\nG(f(x1), . . . , f(xn))\nf∈W,∥f∥≤λ\n∈\nf\nWn, ∥f∥≤λ\n=\nmin\nG(gα(x1), . . . , gα(xn)),\nα∈Rn, α⊤IKα≤λ2\nwhere\nn\nWn = {f ∈W|f(·) = gα(·) =\nX\nαiK(xi,\ni=1\n·)}\nand IKij = K(xi, xj).\nProof.\n\nSince Wn is a linear subspace of W, we can decompose any f\nW uniquely as\n\n⊥\n∈\n⊥\n∈\nf = f + f\nwith f\nWn and f\n∈ W ⊥\nn . The Pythagorean theorem then gives\n∥f∥2\nW = ∥ f∥2\nW + ∥f ⊥∥2\nW\n\nMoreover, since K(xi, ·) ∈Wn,\nf ⊥(xi) = ⟨f ⊥, K(xi, ·)⟩W = 0\n\nSo f(xi) = f(xi) and\nG(f(x1\n\n), . . . , f(xn)) = G(f(x1), . . . , f(xn)).\nBecause f ⊥does not contribute to G, we can remove it from the constraint:\n\nmin\nG(f(x1), . . . , f(xn)) =\nmin\nG(f(x1), . . . , f(xn)).\n\nf∈W,∥f∥2+∥f⊥∥2≤λ2\nf∈\n∥\nW, f∥2≤λ2\n\nRestricting to f ∈Wn now does not change the minimum, which gives us the first equality.\nFor the second, we need to show that ∥gα∥W ≤λ is equivalent to α⊤IKα ≤λ2.\n∥gα∥2 = ⟨gα, gα\nn\n⟩\nn\n= ⟨\nX\nαiK(xi,\ni=1\n·),\nn\nX\nαjK(xj,\n=1\n·)\nj\n⟩\n=\nX\nαiαj⟨K(xi,\n(\n,j=1\n·), K xj,\ni\n·)⟩\nn\n=\nX\nαiαjK(xi, xj)\ni,j=1\n= α⊤IKα\nWe've reduced the infinite dimensional problem to a minimization over α ∈\nn\nR . This\nworks because we're only interested in G evaluated at a finite set of points. The matrix\nIK here is a Gram matrix, though we will not not use that. IK should be a measure of the\nsimilarity of the points xi. For example, we could have W = {⟨x, ·⟩Rd, x ∈\nd\nR } with K(x, y\nthe usual inner product K(x, y) = ⟨x, y⟩Rd.\nˆ\nˆ\nWe've shown that f only depends on K through IK, but does Rn,φ depend on K(x, y)\nfor x, y ∈/ {xi}? It turns out not to:\nn\nn\nn\nˆRn,φ =\nX\nφ(-Yigα(xi)) =\nX\nφ(-Yi\nX\nαjK(xj, xi)).\nn\nn\ni=1\ni=1\nj=1\nThe last expression only involves IK. This makes it easy to encode all the knowledge about\nour problem that we need. The hard classifier is\nn\nˆ\nˆ\nh(x) = sign(f(x)) = sign(gαˆ(x)) = sign(\nX\nαˆjK(xj, x))\nj=1\nIf we are given a new point xn+1, we need to compute a new column for IK. Note that\nxn+1 must be in some way comparable or similar to the previous {xi} for the whole idea of\nextrapolating from data to make sense.\nThe expensive part of SVMs is calculating the n × n matrix IK. In some applications,\nIK may be sparse; this is faster, but still not as fast as deep learning. The minimization\nover the ellipsoid α⊤IKα requires quadratic programming, which is also relatively slow. In\npractice, it's easier to solve the Lagrangian form of the problem\nn\nαˆ = argmin\nX\nφ(-Yigα(x\n′\n⊤\ni)) + λ α IKα\nα∈Rn\nn i=1\nThis formulation is equivalent to the constrained one. Note that λ and λ′ are different.\nSVMs have few tuning parameters and so have less flexibility than other methods.\nWe now turn to analyzing the performance of SVM.\n\nTheorem: Excess Risk for SVM. Let φ be an L-Lipschitz convex surrogate and\nˆ\nˆ\nW a RKHS with PSD K such that maxx |K(x, x)| = kmax < inf. Let hn,φ = sign fn,φ,\nˆ\nwhere fn,φ is the empirical φ-risk minimizer over F = {f\nˆ\nˆ\nˆ\n∈W.∥f∥W ≤λ} (that is,\nRn,φ(fn,φ) ≤Rn,φ(f)∀f ∈F). Suppose λ√kmax ≤1. Then\nˆ\nR(hn,φ) -R∗≤2c\n\nγ\nγ\nγ\nkmax\n2 log(2/δ)\ninf (Rφ(f) -R∗\nφ)\n\n+ 2c\n\n8Lλ\n+\nf∈\nr\n2L\nF\nn\n!\n2c\n\nr\nn\n!\nwith probability 1 -δ. The constants c and γ are those from Zhang's lemma. For the\nhinge loss, c = 1\n2 and γ = 1.\nProof. The first term comes from optimizing over a restricted set F instead of all classifiers.\nThe third term comes from applying the bounded difference inequality.\nThese arise in\nexactly the same way as they do for boosting, so we will omit the proof for those parts. For\nthe middle term, we need to show that Rn,φ(F) ≤λ\nkmax\nn .\nFirst, |f(x)| ≤∥f∥W\n√kmax ≤λ√kmax ≤1 for all\nq\nf ∈F, so we can use the contraction\ninequality to replace Rn,φ(F) with Rn(F). Next we'll expand f(xi) inside the Rademacher\ncomplexity and bound inner products using Cauchy-Schwartz.\nn\nRn(F) =\nsup\nE sup\nσif(xi)\nx1,...,xn\n\"\nf∈F\n\nn i=1\n#\nX\n\nn\n=\nsup\nE\n\"\nsup\n\nX\nσ\n\ni K(x\n\n⟨\ni, ·), f\nn x1,...,xn\nf∈F\ni=1\n⟩\n#\nn\n\n=\nsup\nE\n\n\"\nsup\n⟨\nX\nσiK(xi, ·), f\n\nn\n\nx1,...,xn\nf∈F\ni=1\n⟩\n#\n\nλ\n\n≤\nsup\nv\nu\nu\ntE\n\"\nn\n∥\nX\nσiK(x\ni,\n\nn x1,...,xn\ni=1\n·)∥W\n#\nNow,\nn\nn\nn\nE\n\"\n∥\nX\nσiK(xi, ·)∥W\n#\n= E\n\n⟨\nX\nσiK(xi, ·),\nX\nσjK(xj,\ni=1\ni=1\nj=1\n·)⟩W\n\nn\n\n=\nX\n⟨K(x\nE\ni, ·), K(xj, ·)⟩[σiσj]\ni,j=1\nn\n=\ni\nX\nK(xi, xj)δij\n,j=1\n≤nkmax\nSo Rn(F) ≤λ\nkmax\nn\nand we are done with the new parts of the proof. The remainder follows\nas with boosti\nq\nng, using symmetrization, contraction, the bounded difference inequality, and\nZhang's lemma.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Kevin Li\nOct. 14, 2015\n2. CONVEX OPTIMIZATION FOR MACHINE LEARNING\nIn this lecture, we will cover the basics of convex optimization as it applies to machine\nlearning. There is much more to this topic than will be covered in this class so you may be\ninterested in the following books.\nConvex Optimization by Boyd and Vandenberghe\nLecture notes on Convex Optimization by Nesterov\nConvex Optimization: Algorithms and Complexity by Bubeck\nOnline Convex Optimization by Hazan\nThe last two are drafts and can be obtained online.\n2.1 Convex Problems\nA convex problem is an optimization problem of the form min f(x) where f and\nare\nx∈C\nC\nconvex. First, we will debunk the idea that convex problems are easy by showing that\nvirtually all optimization problems can be written as a convex problem. We can rewrite an\noptimization problem as follows.\nmin f(x)\n,\nmin\nt\n,\nmin\nt\nX∈X\nt≥f(x),x∈X\n(x,t)∈epi(f)\nwhere the epigraph of a function is defined by\nepi(f) = f(x, t) 2 X × IR : t ≥f(x)g\n⇔\n⇔\n{\n∈\n×\n≥\n}\nFigure 1:\nAn example of an epigraph.\nSource: https://en.wikipedia.org/wiki/Epigraph_(mathematics)\n\nNow we observe that for linear functions,\nmin c⊤x =\nmin\nc⊤x\nx∈D\nx∈conv(D)\nwhere the convex hull is defined\nN\nN\nconv(D) = fy : 9N 2 Z+, x1, . . . , xN 2 D, αi ≥0,\nX\nαi = 1, y =\ni=1\nX\nαixi\ni=1\ng\nTo prove this, we know that the left side is a least as big as the right side since D ⊂conv(D).\nFor the other direction, we have\nN\nmin\nc⊤x = min\nmin\nmin\nc⊤\nαixi\nx∈conv(D)\nN\nx1,...,xN∈D α1,...,αN\nN\nX\ni=1\n= min\nmin\nmin\nαic⊤xi\nmin c⊤x\nN\nx1,...,xN∈D α1,...,αN\nX\n=1\n≥\nx∈D\ni\nN\n≥min\nmin\nmin\nαi min c⊤x\nN\nx1,...,xN∈D α1,...,αN\nX\nx\ni=1\n∈D\n= min c⊤x\nx∈D\nTherefore we have\nmin f(x)\nmin\nx∈X\n,\nt\n(x,t)∈conv(epi(f))\nwhich is a convex problem.\nWhy do we want convexity? As we will show, convexity allows us to infer global infor-\nmation from local information. First, we must define the notion of subgradient.\nDefinition (Subgradient): Let C ⊂IRd, f : C ! IR. A vector g 2 IRd is called a\nsubgradient of f at x 2 C if\nf(x) -f(y) ≤g⊤(x -y)\n8y 2 C .\nThe set of such vectors g is denoted by ∂f(x).\nSubgradients essentially correspond to gradients but unlike gradients, they always ex-\nist for convex functions, even when they are not differentiable as illustrated by the next\ntheorem.\nTheorem: If f : C ! IR is convex, then for all x, ∂f(x) = ;. In addition, if f is\ndifferentiable at x, then ∂f(x) = frf(x)g.\nProof. Omitted. Requires separating hyperplanes for convex sets.\n{\n∃\n∈\n∈\n≥\n}\n⊂\n≥\n≥\n⇔\n⊂\n→\n∈\n∈\n-\n≤\n-\n∈\n→\n= ∅\n{∇\n}\n∀\n\nTheorem: Let f, C be convex. If x is a local minimum of f on C, then it is also global\nminimum. Furthermore this happens if and only if 0 2 ∂f(x).\nProof. 0 2 ∂f(x) if and only if f(x) -f(y) ≤0 for all y 2 C. This is clearly equivalent to\nx being a global minimizer.\nNext assume\nx is a local minimum. Then for all y 2 C there exists ε small enough such\nthat f(x) ≤f (1 -ε)x + εy\n\n≤(1 -ε)f(x) + εf(y) =) f(x) ≤f(y) for all y 2 C.\nNot only do we know that local minimums are global minimums, looking at the subgra-\ndient also tells us where the minimum can be. If g⊤(x -y) < 0 then f(x) < f(y). This\nmeans f(y) cannot possibly be a minimum so we can narrow our search to ys such that\ng⊤(x -y). In one dimension, this corresponds to the half line fy 2 IR : y ≤xg if g > 0 and\nthe half line fy 2 IR : y ≥xg if g < 0 . This concept leads to the idea of gradient descent.\n2.2 Gradient Descent\ny ≈x and f differentiable the first order Taylor expansion of f at x yields f(y) ≈f(x) +\ng⊤(y -x). This means that\nmin f(x + εμˆ) ≈min f(x) + g⊤(εμˆ)\n|μˆ|2=1\ng\nwhich is minimized at μˆ = -\n. Therefore to minimizes the linear approximation of f at\n|g|2\nx, one should move in direction opposite to the gradient.\nGradient descent is an algorithm that produces a sequence of points fxjgj≥1 such that\n(hopefully) f(xj+1) < f(xj).\n∈\n∈\n≤\n∈\n∈\n≤\n≤\n⇒\n≤\n∈\n{\n∈\n≤\n}\n{\n∈\n≥\n}\n≈\n≈\n≈\n{\n}\nFigure 2: Example where the subgradient of x1 is a singleton and and the subgradient of\nx2 contains multiple elements.\nSource: https://optimization.mccormick.northwestern.edu/index.php/\nSubgradient_optimization\n\nAlgorithm 1 Gradient Descent algorithm\nInput: x1 2 C, positive sequence fηsgs≥1\nfor s = 1 to k -1 do\nxs+1 = xs -ηsgs ,\ngs 2 ∂f(xs)\nend for\nk\nreturn Either x = k\nX\nxs or x*\ns=1\nargmin\nf(x)\nx∈{x1,...,xk}\nTheorem: Let f be a convex L-Lipschitz function on IRd such that x∗2 argminIRd f(x)\nexists. Assume that jx1 -x∗j2 ≤R. Then if η\nR\ns = η = L\n√\nfor all s\nk\n≥1, then\nk\nLR\nf(k\nX\nxs)\ns=1\n-f(x∗) ≤p\nk\nand\nLR\nmin f(xs)\ns\nk\n-f(x∗) ≤p\n≤≤\nk\nProof. Using the fact that gs = 1(x\ns+1\n+\nη\n-xs) and the equality 2a⊤b = kak\nkbk2 -ka-bk2,\nf(xs) -f(x∗) ≤gs\n⊤(xs -x∗) =\n(xs\nη\n-xs+1)⊤(xs -x∗)\n=\nx\ns\nxs+1\n+ x\nx∗2\nx\ns\ns+1\nx∗\n2η\nh\nk\n-\nk\nk\n-\nk -k\n-\nk\nη\ni\n= 2kg\nsk +\n(δ2\n2η\ns -δ2\ns+1)\nwhere we have defined δs = kxs -x∗k. Using the Lipschitz condition\nη\nf(xs) -f(x∗) ≤\nL2 +\n(δ2\n2η\ns -δ2\ns+1)\nTaking the average from 1, to k we get\nk\n1 X\nη\nη\nR2\nf(xs)\nf(x∗) ≤\nL2\nη\n-\n+\n(δ2\nk\n1 -δ2\nk\ns\n)\nη\n≤\nL2\n+1\n+\nδ2\n2kη\n1 ≤\nL2 +\n2kη\ns=1\nTaking η =\nR\nL\n√\nto minimize the expression, we obtain\nk\nk\nk\nX\nLR\nf(xs)\ns=1\n-f(x∗) ≤p\nk\nk\nNoticing that the left-hand side of the inequality is larger than both f(P xs) -f(x∗) by\ns=1\nJensen's inequality and min f(xs)\n1≤s≤k\n-f(x∗) respectively, completes the proof.\n∈\n{\n}\n-\n-\n∈\n∈\n∈\n|\n-\n| ≤\n≥\n-\n≤\n-\n≤√\n-\n∥∥\n∥∥\n∥\n∥\n-\n≤\n-\n-\n-\n∥\n-\n∥\n∥\n-\n∥-∥\n-\n∥\n∥\n∥\n-\n∥\n-\n∥\n-\n≤\n-\n-\n≤\n-\n≤\n≤\n-\n≤√\n-\n-\n√\n-\n-\n\nOne flaw with this theorem is that the step size depends on k. We would rather have\nstep sizes ηs that does not depend on k so the inequalities hold for all k. With the new step\nsizes,\nX\nk\nX\nk\nk\nk\nη2\ns\n[\nX\nR2\nηs f(x ) -f x∗)] ≤\nL\nδ2\nL\ns\n(\n+\n(δ2\ns -\ns+1)\nη2\n+\ns\ns=1\ns=1\ns=1\n≤\nX\ns=1\n\nAfter dividing by Pk\nP\ns=1 ηs, wePwould like the right-hand side to approach 0. For this to\nη2\nhappen we need P\ns ! 0 and\nηs ! 1. One candidate for the step size is ηs =\nG\nηs\n√since\ns\nk\nthen P\nk\nη2\ns ≤c1G2 log(k) and\nηs\nc2G\np\nk. So we get\ns=1\ns\nP\n=1\n≥\nX\nk\nc1\nηs\n\nk\n-1 X\nGL log k\nR2\nηs[f(xs)\nf(x∗)]\n2c2\np\n+\nk\n2c2G\np\nk\ns=1\ns=1\n-\n≤\nChoosing G appropriately, the right-hand side approaches 0 at the rate of LR\nlog k. Notice\np\nk\nthat we get an extra factor of\nlog k. However, if we look at the sum from k/\nq\n2 to k instead\nk\nof 1 to k, P\nk\nη2\ns ≤c′\n1G2 and P ηs ≥c′\n2G\np\nk. Now we have\n= k\ns=1\ns\nk\nk\n-1\ncLR\nmin f(xs)\nf(x∗)\nmin f(xs)\nf(x∗)\nηs\nηs[f(xs)\nf(x∗)]\n1≤s≤k\n-\n≤\nk\ns\nk\n-\n≤\nX\nk\n\nX\nk\n-\n≤p\n≤≤\nk\ns=\ns=\nwhich is the same rate as in the theorem and the step sizes are independent of k.\nImportant Remark: Note this rate only holds if we can ensure that jxk/2 -x∗j2 ≤R\nsince we have replaced x1 by xk/2 in the telescoping sum. In general, this is not true for\ngradient descent, but it will be true for projected gradient descent in the next lecture.\nOne final remark is that the dimension d does not appear anywhere in the proof. How-\never, the dimension does have an effect because for larger dimensions, the conditions f is\nL-Lipschitz and jx1 -x∗j2 ≤R are stronger conditions in higher dimensions.\n-\n≤\n-\n≤\n→\n→inf\n≤\n≥\n√\n-\n≤\n√\n√\n√\n≤\n≥\n√\n-\n≤\n-\n≤\n-\n≤√\n|\n-\n|\n≤\n|\n| ≤\n-\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture 12\nScribe: Michael Traub\nOct. 19, 2015\n2.3 Projected Gradient Descent\nIn the original gradient descent formulation, we hope to optimize minx\nf(x) where\nn\nC\nC a d\n∈\nf are convex, but we did not constrain the intermediate xk. Projected gradient descent will\nincorporate this condition.\n2.3.1\nProjection onto Closed Convex Set\nFirst we must establish that it is possible to always be able to keep xk in the convex set C.\nOne approach is to take the closest point π(xk) ∈C.\nDefinition: Let C be a closed convex subset of IRd. Then ∀x ∈IRd, let π(x) ∈C be\nthe minimizer of\n∥x -π(x)∥= min x\nz\nz∈C ∥-∥\nwhere ∥· ∥denotes the Euclidean norm. Then π(x) is unique and,\n⟨π(x) -x, π(x) -z⟩≤0\n∀z ∈C\n(2.1)\nProof. From the definition of π := π(x), we have ∥x -π∥2 ≤∥x -v∥2 for any v ∈C. Fix\nw ∈C and define v = (1 -t)π + tw for t ∈(0, 1]. Observe that since C is convex we have\nv ∈C so that\n∥-\n∥2 ≤∥-∥2\nx\nπ\nx\nv\n= ∥x -π -t(w -π)∥\nExpanding the right-hand side yields\n∥\nx -π∥≤∥x -π∥-2t ⟨x -π, w -π⟩+ t2 ∥w -π∥\nThis is equivalent to\n⟨x -π, w -\n⟩≤\nπ\nt ∥w -π∥\nSince this is valid for all t ∈(0, 1), letting t →0 yields (2.1).\nProof of Uniqueness. Assume π1, π2 ∈C satisfy\n⟨π1 -x, π1 -z⟩≤0\n∀z ∈C\n⟨π2 -x, π2 -z⟩≤0\n∀z ∈C\nTaking z = π2 in the first inequality and z = π1 in the second, we get\n⟨π1 -x, π1 -π2⟩≤0\n⟨x -π2, π1 -π2⟩≤0\nAdding these two inequalities yields ∥π1 -π2∥2 ≤0 so that π1 = π2.\n\n2.3.2\nProjected Gradient Descent\nAlgorithm 1 Projected Gradient Descent algorithm\nInput: x1 ∈C, positive sequence {ηs}s≥1\nfor s = 1 to k -1 do\nys+1 = xs -ηsgs ,\ngs ∈∂f(xs)\nxs+1 = π(ys+1)\nend for\nk\nreturn Either x =\nX\nxs or x*∈\nargmin\nf(x)\nk\nx\ns=\n{x1,...,x\n∈\nk}\nTheorem: Let C be a closed, nonempty convex subset of IRd such that diam(C) ≤R.\nLet f be a convex L-Lipschitz function on\nR\nC such that x∗∈argminx\nf(x) exists.\n∈C\nThen if ηs ≡η = L\n√\nthen\nk\nLR\nLR\nf(x ) -f(x∗) ≤√\nand\nf(x *) -f(x∗)\nk\n≤√\nk\nMoreover, if ηs =\nR√, then ∃c > 0 such that\nL\ns\nLR\nLR\nf(x ) -f(x∗) ≤c√\nand\nf(x *)\nf(x∗)\nk\n-\n≤c√\nk\nProof. Again we will use the identity that 2a⊤b = ∥a∥2 + ∥b∥2 -∥\na -b∥.\nBy convexity, we have\nf(xs) -f(x∗) ≤gs\n⊤(xs -x∗)\n=\n(xs -ys+1)⊤(xs\nη\n-x∗)\n= 2η\nh\n∥\nxs -ys+1∥+ ∥xs -x∗∥2 -∥ys+1 -x∗∥2i\nNext,\n∥ys+1 -x∗∥2 = ∥\nys+1 -xs+1∥+ ∥xs+1 -x∗∥2 + 2 ⟨ys+1\n-xs+1, xs+1 -x∗⟩\n= ∥ys+1 -xs+1∥+ ∥\nxs+1 -x∗∥+ 2 ⟨ys+1 -π(ys+1), π(ys+1) -x∗⟩\n≥∥xs+1 -x∗∥2\nwhere we used that ⟨x -π(x), π(x) -z⟩≥0 ∀z ∈C, and x∗\n∈C.\nAlso notice that\n∥xs -\ny\ns+1∥\n= η ∥gs∥\n≤η L\nsince f is L-Lipschitz with respect to ∥·∥.\nUsing this\nwe find\nk\nk\nk\nX\n(\n) -\n( ∗) ≤\nX\n2 +\n∗2\n∗2\nf xs\nf x\nη L\nxs\nx\nxs+1\nx\nk\n2η\ns=1\ns=1\nh\n∥\n-\n∥-∥\n-\n∥\ni\nηL2\nηL2\nR2\n≤\n+\nx\n2 k ∥1 -x∗\nη\n∥≤\n+\n2ηk\n\nMinimizing over η we get L =\nR\n=⇒η =\nR√, completing the proof\n2η k\nL\nk\nRL\nf(x ) -f(x∗) ≤√\nk\nMoreover, the proof of the bound for f(Pk\nk xs)-f(x∗) is identical because\nx k\nx\ns=\n\n2 -\n∗\n≤\nR2 as well.\n\n2.3.3\nExamples\nSupport Vector Machines\nThe SVM minimization as we have shown before is\nn\nminn\nX\nmax (0, 1\nYifα(Xi))\nα\nR\n⊤∈I\nn\n≤\ni=1\n-\nα IKα\nC\nwhere fα(Xi) = α⊤IKei = Pn\n=1 αjK(X ,\nj\nj Xi). For convenience, call gi(α) = max (0, 1 -Yifα(Xi)).\nIn this case executing the projection onto the ellipsoid {α : α⊤IKα ≤C2} is not too hard,\nbut we do not know about C, R, or L. We must determine these we can know that our\nbound is not exponential with respect to n. First we find L and start with the gradient of\ngi(α):\n∇gi(α) = 1I(1 -Yifα(Xi) ≥0)YiIKei\nˆ\nWith this we bound the gradient of the φ-risk Rn,φ(fα) = 1\nn\nn\nn\nPn\n=1 gi(α\ni\n).\n∂\n\nRˆn,φ(fα)\n\nX\n=\n\n∇gi(α)\n∂α\n\nn\n\ni=1\n≤\nX\nIKe\n\ni\nn\ni=1\n∥\n∥\nby the triangle inequality and the fact that that 1I(1\n\n-Yifα(Xi) ≥0)Yi ≤1. We can now\nuse the properties of our kernel K. Notice that ∥IKei\n∥is the l2 norm of the ith column so\n∥IKei∥\nn\n2 =\nP\nj=1 K(Xj, Xi)2\n. We also know that\nK(Xj, Xi)2 = ⟨K(X\nj, ·), K(Xi, ·)⟩≤∥K(Xj, ·)∥\nK\nH ∥\n(Xi, ·)∥H ≤kmax\nCombining all of these we get\n\nn\nn\n∂\n\nRˆn,φ(fα)\n\n≤\nmax\n\nX\nX\n\nk2\n\n= kmax\n√n = L\n∂α\n\nn i=1\nj=1\nTo find R we try to evaluate diam{α⊤IKα ≤C2} = 2\nmax\n√\nα\nα⊤\n⊤α. We can use the\nIKα≤C2\ncondition to put bounds on the diameter\nC2\n2C\n≥α⊤IKα ≥λmin(IK)α⊤α =⇒diam{α⊤IKα ≤C2} ≤p\nλmin(IK)\nWe need to understand how small λmin can get. While it is true that these exist random\nsamples selected by an adversary that make λmin = 0, we will consider a random sample of\n\ni.i.d\nX1, . . . , Xn ∼N(0, Id). This we can write these d-dimensional samples as a d × n matrix\nX. We can rewrite the matrix IK with entries IKij = K(Xi, Xj) = ⟨Xi, Xj⟩IRd as a Wishart\nmatrix IK = X⊤X (in particular, 1X\nd\n⊤X is Wishart). Using results from random matrix\ntheory, if we take n, d →infbut hold n as a constant γ, then λ\n(IK\nmin\n)\n(\nd\n→1\n√\n-\nγ) . Taking\nd\nan approximation since we cannot take n, d to infinity, we get\nλmin(IK) ≃d\n\nn\nd\n1 -2\nr\nd\n\n≥2\nusing the fact that d ≫n. This means that λmin becoming too small is not a problem when\nwe model our samples as coming from multivariate Gaussians.\nNow we turn our focus to the number of iterations k. Looking at our bound on the\nexcess risk\nn\nRˆn,φ(f\nˆ\nα*\nR) ≤\nmin\nRn,φ(fα) + C\nr\nkmax\nα⊤IKα≤C2\nkλmin(IK)\nwe notice that our all of the constants in our stochastic term can be computed given the\nnumber of points and the kernel. Since statistical error is often √1 , to be generous we want\nn\nto have precision up to 1\nn to allow for fast rates in special cases. This gives us\nn3k2\nC2\nk ≥\nmax\nλmin(IK)\nwhich is not bad since n is often not very big.\nIn [Bub15], the rates for many a wide rage of problems with various assumptions are\navailable. For example, if we assume strong convexity and Lipschitz we can get an exponen-\ntial rate so k ∼log n. If gradient is Lipschitz, then we get get 1\nk instead of √1 in the bound.\nk\nHowever, often times we are not optimizing over functions with these nice properties.\nBoosting\nWe already know that φ is L-Lipschitz for boosting because we required it before.\nRemember that our optimization problem is\nn\nmin\nX\nφ(-Yifα(Xi))\nα\nRN n\n|α\n∈I\n|1≤\ni=1\nwhere fα =\nN\nj=1 αjfj and fj is the jth weak classifier. Remember before we had some rate\nlike\nq\nlog N\nc\nP\nn\nand we would hope to get some other rate that grows with log N since N can\nbe very large. Taking the gradient of the φ-loss in this case we find\nN\n∇Rˆn,φ(fα) =\nX\nφ′(-Yifα(Xi))(-Yi)F(Xi)\nn i=1\nwhere F(x) is the column vector [f1(x), . . . , fN(x)]⊤. Since |Yi| ≤1 and φ′ ≤L, we can\nbound the l2 norm of the gradient as\n\nn\nL\n∇Rˆ\n\nn,φ(fα)\n\n2 ≤n\n\nX\nF(X\n\ni)\ni=1\n\nn\n\nL\n\n≤n\nX\n)\ni=\n∥F(Xi\n∥≤L\n√\nN\n\nusing triangle inequality and the fact that F(Xi) is a N-dimensional vector with each\ncomponent bounded in absolute value by 1.\nUsing the fact th√at the diameter of the l1 ball is 2, R = 2 and the Lipschitz associated\nwith our φ-risk is L\nN where L is the Lipschitz constant for φ. Our stochastic term R√L\nk\nbecomes 2L\nq\nN\nk . Imposing the same 1\nn error as before we find that k ∼N 2n, which is very\nbad especially since we want log N.\n2.4 Mirror Descent\nBoosting is an example of when we want to do gradient descent on a non-Euclidean space,\nin particular a l1 space. While the dual of the l2-norm is itself, the dual of the l1 norm is\nthe l\nor sup norm. We want this appear if we have an l1 constraint. The reason for this\ninf\nis not intuitive because we are taking about measures on the same space IRd, but when we\nconsider optimizations on other spaces we want a procedure that does is not indifferent to\nthe measure we use. Mirror descent accomplishes this.\n2.4.1\nBregman Projections\nDefinition: If ∥·∥is some norm on IRd, then ∥·∥is its dual norm.\n∗\nExample: If dual norm of the lp norm ∥·∥p is the lq norm ∥·∥q, then 1\np + 1\nq = 1. This is the\nlimiting case of H older's inequality.\nIn general we can also refine our bounds on inner products in IRd to x⊤y ≤∥x∥∥y∥if\n∗\nwe consider x to be the primal and y to be the dual. Thinking like this, gradients live in\nthe dual space, e.g. in gs\n⊤(x -x∗), x -x∗is in the primal space, so gs is in the dual. The\ntranspose of the vectors suggest that these vectors come from spaces with different measure,\neven though all the vectors are in IRd.\nDefinition: Convex function Φ on a convex set D is said to be\n(i) L-Lipschitz with respect to ∥·∥if ∥g∥∗≤L ∀g ∈∂Φ(x) ∀x ∈D\n(ii) α-strongly convex with respect to ∥·∥if\nα\nΦ(y) ≥Φ(x) + g⊤(y -x) + 2 ∥y -x∥2\nfor all x, y ∈D and for g ∈∂f(x)\nExample:\nIf Φ is twice differentiable with Hessian H and ∥·∥is the l2 norm, then all\neig(H) ≥α.\nDefinition (Bregman divergence): For a given convex function Φ on a convex set\nD with x, y ∈D, the Bregman divergence of y from x is defined as\nDΦ(y, x) = Φ(y) -Φ(x) -∇Φ(x)⊤(y -x)\n\nThis divergence is the error of the function Φ(y) from the linear approximation at x.\nAlso note that this quantity is not symmetric with respect to x and y. If Φ is convex then\nDΦ(y, x) ≥0 because the Hessian is positive semi-definite. If Φ is α-strongly convex then\nDΦ(y, x) ≥α\n2 ∥y -x∥2 and if the quadratic approximation is good then this approximately\nholds in equality and this divergence behaves like Euclidean norm.\nProposition: Given convex function Φ on D with x, y, z ∈D\n(∇Φ(x) -∇Φ(y))⊤(x -z) = DΦ(x, y) + DΦ(z, x) -DΦ(z, y)\nProof. Looking at the right hand side\n= Φ(x) -Φ(y) -∇Φ(y)⊤(x -y) + Φ(z) -Φ(x) -∇Φ(x)⊤(z -x)\n-\nh\nΦ(z) -Φ(y) -∇Φ(y)⊤(z -y)\ni\n= ∇Φ(y)⊤(y -x + z -y) -∇Φ(x)⊤(z -x)\n= (∇Φ(x) -∇Φ(y))⊤(x -z)\nDefinition (Bregman projection): Given x ∈IRd, Φ a convex differentiable function\non D ⊂\nD\nIRd and convex C ⊂\n, the Bregman projection of x with respect to Φ is\nπΦ(x) ∈argmin Dφ(x, z)\nz∈C\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Mina Karzand\nOct. 21, 2015\nPreviously, we analyzed the convergence of the projected gradient descent algorithm.\nWe proved that optimizing the convex L-Lipschitz function f on a closed, convex set C with\ndiam(C) ≤R with step sizes ηs =\nR√\nwould give us accuracy of f(x) ≤f(x∗) + LR\nL\nk\n√\nafter\nk\nk iterations.\nAlthough it might seem that projected gradient descent algorithm provides dimension-\nfree convergence rate, it is not always true. Reviewing the proof of convergence rate, we\nrealize that dimension-free convergence is possible when the objective function f and the\nconstraint set C are well-behaved in Euclidean norm (i.e., for all x ∈C and g ∈∂f(x), we\nhave that |x|2 and |g|2 are independent of the ambient dimension). We provide an examples\nof the cases that these assumptions are not satisfied.\n- Consider the differentiable, convex function f on the Euclidean ball B2,n such that\n∥∇f(x)∥\n≤1, ∀x ∈B2,n.\nThis implies that\nf(x)\n√n and the projected\ninf\n|∇\n|2 ≤\ngradient descent converges to the minimum of f in B\nn\n2,q\nn at rate\nk .\nUsing the\nlog(n)\nmethod of mirror descent we can get convergence rate of\np\nk\nTo get better rates of convergence in the optimization problem, we can use the Mirror\nDescent algorithm.\nThe idea is to change the Euclidean geometry to a more pertinent\ngeometry to a problem at hand. We will define a new geometry by using a function which\nis sometimes called potential function Φ(x).\nWe will use Bregman projection based on\nBregman divergence to define this geometry.\nThe geometric intuition behind the mirror Descent algorithm is the following: The\nprojected gradient described in previous lecture works in any arbitrary Hilbert space H so\nthat the norm of vectors is associated with an inner product. Now, suppose we are interested\nin optimization in a Banach space D. In other words, the norm (or the measure of distance)\nthat we use does not derive from an inner product. In this case, the gradient descent does\nnot even make sense since the gradient ∇f(x) are elements of dual space. Thus, the term\nx -η∇f(x) cannot be performed. (Note that in Hilbert space used in projected gradient\ndescent, the dual space of H is isometric to H. Thus, we didn't have any such problems.)\nThe geometric insight of the Mirror Descent algorithm is that to perform the optimiza-\ntion in the primal space D, one can first map the point x ∈D in primal space to the dual\nspace D∗, then perform the gradient update in the dual space and finally map the optimal\npoint back to the primal space. Note that at each update step, the new point in the primal\nspace D might be outside of the constraint set C ⊂D, in which case it should be projected\ninto the constraint set C. The projection associate with the Mirror Descent algorithm is\nBergman Projection defined based on the notion of Bergman divergence.\nDefinition (Bregman Divergence): For given differentiable, α-strongly convex func-\ntion Φ(x) : D →R, we define the Bregman divergence associated with Φ to be:\nDΦ(y, x) = Φ(y) -Φ(x) -∇Φ(x)T (y -x)\n\nWe will use the convex open set D ⊂\nn\nR whose closure contains the constraint set C ⊂D.\nBregman divergence is the error term of the first order Taylor expansion of the function Φ\nin D.\nAlso, note that the function Φ(x) is said to be α-strongly convex w.r.t. a norm ∥.∥if\nΦ(y) -Φ(x) -∇Φ(x)T\nα\n(y -x) ≥2 ∥y -x∥2 .\nWe used the following property of the Euclidean norm:\n2a⊤b = ∥a∥2 + ∥b∥2 -∥a -b∥2\nin the proof of convergence of projected gradient descent, where we chose a = xs -ys+1 and\nb = xs -x∗.\nTo prove the convergence of the Mirror descent algorithm, we use the following property\nof the Bregman divergence in a similar fashion. This proposition shows that the Bregman di-\nvergence essentially behaves as the Euclidean norm squared in terms of projections:\nProposition: Given α-strongly differentiable convex function Φ : D →R, for all\nx, y, z ∈D,\n[∇Φ(x) -∇Φ(y)]⊤(x -z) = DΦ(x, y) + DΦ(z, x) -DΦ(z, y) .\nAs described previously, the Bregman divergence is used in each step of the Mirror descent\nalgorithm to project the updated value into the constraint set.\nDefinition (Bregman Projection): Given α-strongly differentiable convex function\nΦ : D →R and for all x ∈D and closed convex set C ⊂D\nΠΦ(x) = argmin D\nC\nΦ(z, x)\nz∈C∩D\n2.4.2\nMirror Descent Algorithm\nAlgorithm 1 Mirror Descent algorithm\nInput: x1 ∈argmin\nΦ(x), ζ :\nd\nd\nR\nR such that ζ(x) =\nΦ(x)\nC∩D\n→\n∇\nfor s = 1, · · · , k do\nζ(ys+1) = ζ(xs) -ηgs for gs ∈∂f(xs)\nxs+1 = ΠΦ(y\nC\ns+1)\nend for\nreturn Either x = 1\nk\nPk\ns=1 xs or x*∈argminx\nx1,\n,xk f(x)\n∈{\n···\n}\nProposition: Let z ∈C ∩D, then ∀y ∈D,\n(∇Φ(π(y) -∇Φ(y))⊤(π(y) -z) ≤0\n\nMoreover, DΦ(z, π(y)) ≤DΦ(z, y).\nProof. Define π = ΠΦ(y) and h(t) = DΦ(π + t(z -π), y) . Since h(t) is minimized at t = 0\nC\n(due to the definition of projection), we have\nh′(0) = ∇xDΦ(x, y)|x=π(z -π) ≥0\nwhere suing the definition of Bregman divergence,\n∇xDΦ(x, y) = ∇Φ(x) -∇Φ(y)\nThus,\n(∇Φ(π) -∇Φ(y))⊤(π -z) ≤0 .\nUsing proposition 1, we know that\n(∇Φ(π) -∇Φ(y))⊤(π -z) = DΦ(π, y) + DΦ(z, π) -DΦ(z, y) ≤0 ,\nand since DΦ(π, y) ≥0, we would have DΦ(z, π) ≤DΦ(z, y).\nTheorem: Assume that f is convex and L-Lipschitz w.r.t. ∥.∥. Assume that Φ is\nα-strongly convex on C ∩D w.r.t. ∥.∥and\nR2 =\nsup Φ(x)\nm\nx\n-\nin Φ(x)\n∈C∩D\nx∈C∩D\ntaq\nke x1 = argminx\nΦ(x) (assume that it exists). Then, Mirror Descent with η =\n∈C∩D\nR\n2α\nL\nR gives,\nf(x) -f(x∗) ≤RL\nr\nand\nf(x*)\nαk\n-f(x∗) ≤RL\nr\n,\nαk\nProof. Take x♯∈C ∩D. Similar to the proof of the projected gradient descent, we have:\n(i)\nf(xs) -f(x♯) ≤gs\n⊤(xs -x♯)\n(ii) 1\n=\n(ζ(x\n♯\ns) -ζ(ys+1))⊤(xs\n)\nη\n-x\n(iii) 1\n=\n(\nΦ(xs)\nΦ(ys+1))⊤(xs\nx♯)\nη ∇\n-∇\n-\n(iv) 1\n=\nh\nD\n♯\nΦ(xs, ys+1) + D\n♯\nΦ(x , xs)\nη\n-DΦ(x , ys+1)\ni\n(v) 1\n≤\nh\nDΦ(xs, ys+1) + DΦ(x♯, xs) -DΦ(x♯, xs+1)\nη\ni\n(vi) ηL2\n≤\n+\nh\nDΦ(x♯, xs)\n2α2\nη\n-DΦ(x♯, xs+1)\ni\nWhere (i) is due to convexity of the function f.\n\nEquations (ii) and (iii) are direct results of Mirror descent algorithm.\nEquation (iv) is the result of applying proposition 1.\nInequality (v) is a result of the fact that x\n= ΠΦ\n♯\ns+1\n(y\nC\ns+1), thus for x\n♯\n♯\n∈C ∩D, we have\nDΦ(x , ys+1) ≥DΦ(x , xs+1).\nWe will justify the following derivations to prove inequality (vi):\n(a)\nDΦ(xs, ys+1) = Φ(xs) -Φ(ys+1) -∇Φ(ys+1)⊤(xs -ys+1)\n(b)\nα\n≤[∇Φ(x\ns) -∇Φ(ys+1)]⊤(xs -ys+1) -2 ∥ys+1 -xs∥\n(c)\nα\n≤η∥gs∥∗∥xs -ys+1∥-2 ∥ys+1 -xs∥2\n(d) η2L2\n≤\n.\n2α\nEquation (a) is the definition of Bregman divergence.\nTo show inequality (b), we used the fact that Φ is α-strongly convex which implies that\nΦ(ys+1) -Φ(xs) ≥∇Φ(xs)T (ys+1 -xs)α\n2 ∥y\ns+1 -xs∥.\nAccording to the Mirror descent algorithm, ∇Φ(xs) -∇Φ(ys+1) = ηgs. We use H older's\ninequality to show that gs\n⊤(xs -ys+1) ≤∥gs∥∗∥xs -ys+1∥and derive inequality (c).\nLooking at the quadratic term ax-bx2 for a, b > 0 , it is not hard to show that max ax\na\n-bx2 =\n2 . We use this statement with x = ∥ys+1 -xs∥, a = η g\nb\n∥s∥\nL\n∗≤\nand b = α to derive\ninequality (d).\nAgain, we use telescopic sum to get\nk\n1 X\nηL2\nD (x♯\nΦ\n, x1)\n[f(xs)\nf(x♯)]\n+\n.\n(2.1)\nk\n2α\nkη\ns=1\n-\n≤\nWe use the definition of Bregman divergence to get\nDΦ(x♯, x1) = Φ(x♯) -Φ(x1) -∇Φ(x1)(x♯-x1)\n≤Φ(x♯) -Φ(x1)\n≤sup Φ(x)\nmin Φ(x)\nx\nx\n-\n∈C∩D\n∈C∩D\n≤R2 .\nWhere we used the fact x1 ∈argmin\nΦ(x) in the description of the Mirror Descent\n♯\nC∩D\nalgorithm to prove ∇Φ(x1)(x -x1) ≥0. We optimize the right hand side of equation (2.1)\nfor η to get\nk\n1 X\n(x♯\n[f(xs) -f\n)]\nk s=\n≤RL\nr\n.\nαk\nTo conclude the proof, let x♯→x∗∈C.\nNote that with the right geometry, we can get projected gradient descent as an instance\nthe Mirror descent algorithm.\n\n2.4.3\nRemarks\nThe Mirror Descent is sometimes called Mirror Prox. We can write xs+1 as\nxs+1 = argmin DΦ(x, ys+1)\nx∈C∩D\n= argmin Φ(x)\nx\n-∇Φ⊤(ys+1)x\n∈C∩D\n= argmin Φ(x)\nxs\nx\n-[∇Φ(\n) -ηgs]⊤x\n∈C∩D\n= argmin η(gs\n⊤x) + Φ(x)\nx\n-∇Φ⊤(xs)x\n∈C∩D\n= argmin η(gs\n⊤x) + DΦ(x, xs)\nx∈C∩D\nThus, we have\nxs+1 = argmin η(gs\n⊤x) + DΦ(x, xs) .\nx∈C∩D\nTo get xs+1, in the first term on the right hand side we look at linear approximations\nclose to xs in the direction determined by the subgradient gs. If the function is linear, we\nwould just look at the linear approximation term. But if the function is not linear, the\nlinear approximation is only valid in a small neighborhood around xs. Thus, we penalized\nby adding the term DΦ(x, xs).\nWe can penalized by the square norm when we choose\nDΦ(x, xs) = ∥x -xs∥2. In this case we get back the projected gradient descent algorithm\nas an instance of Mirror descent algorithm.\nBut if we choose a different divergence DΦ(x, xs), we are changing the geometry and we\ncan penalize differently in different directions depending on the geometry.\nThus, using the Mirror descent algorithm, we could replace the 2-norm in projected\ngradient descent algorithm by another norm, hoping to get less constraining Lipschitz con-\nstant. On the other hand, the norm is a lower bound on the strong convexity parameter.\nThus, there is trade offin improvement of rate of convergence.\n2.4.4\nExamples\nEuclidean Setup:\nΦ(x) = 1 x 2,\n=\nd\nR ,\nΦ(x) = ζ(x) = x. Thus, the updates will be similar to\n2∥∥\nD\n∇\nthe gradient descent.\nDΦ(y, x) =\n∥y∥2\n-\n∥x\n∥2\nx\n-\n⊤y + ∥x∥\n=\n∥x -y∥2 .\nThus, Bregman projection with this potential function Φ(x) is the same as the usual Eu-\nclidean projection and the Mirror descent algorithm is exactly the same as the projected\ndescent algorithm since it has the same update and same projection operator.\nNote that α = 1 since D\nΦ(y, x) ≥2∥x -y∥2.\nl1 Setup:\nWe look at D =\nd\nR+ \\ {0}.\n\nDefine Φ(x) to be the negative entropy so that:\nd\nΦ(x) =\nX\nxi log(xi),\nζ(x) = ∇Φ(x) = {1 + log(x\nd\ni)\ni=1\n}i=1\n(s+1)\n∇\n(s) -\n(s+1)\nThus, looking at the update function y\n=\nΦ(x\n)\nηgs, we get log(yi\n) =\n(s)\nlog(xi ) -\n(s)\n(s+1)\n(s)\n(s)\nηgi\nand for all i = 1, · · · , d, we have yi\n= xi\nexp(-ηgi ). Thus,\ny(s) = x(s) exp(-ηg(s)) .\nWe call this setup exponential Gradient Descent or Mirror Descent with multiplicative\nweights.\nThe Bregman divergence of this mirror map is given by\nDΦ(y, x) = Φ(y) -Φ(x) -∇Φ⊤(x)(y -x)\nX\nd\nX\nd\nd\n=\nyi log(yi) -\nxi log(xi)\n(1 + log(xi))(yi\nxi)\ni\nX\ni\n-\n=1\ni\n-\n=\n=1\nX\nd\nyi\n=\nyi log(\n) +\nxi\ni=1\nX\nd\n(yi\ni=1\n-xi)\nNote that Pd\ni=1 yi log( yi\ni ) is call the Kullback-Leibler divergence (KL-div) between y\nx\nand x.\nWe show that the projection with respect to this Bregman divergence on the simplex\n∆d = {x ∈\nd\nR :\nd\ni=1 xi = 1, xi ≥0} amounts to a simple renormalization y 7→y/|y|1. To\nprove so, we prov\nP\nide the Lagrangian:\nX\nd\nX\nd\nd\ny\nL\ni\n=\nyi log(\n) +\n(xi\nxi\ni=1\ni=\n-yi) + λ(\nX\nxi\ni=1\n-1) .\nTo find the Bregman projection, for all i = 1, · · · , d we write\n∂\ny\nL\n-\ni\n=\n+ 1 + λ = 0\n∂xi\nxi\nThus, for all i, we have xi = γyi. We know that Pd\ni=1 xi = 1. Thus, γ =\nP yi .\nThus, we have ΠΦ\ny\n∆d(y) =\n1.\nThe Mirror Descent algorithm with this update and\n|y|\nprojection would be:\nys+1 = xs exp(-ηgs)\ny\nxs+1 =\n.\n|y|1\nTo analyze the rate of convergence, we want to study the l1 norm on ∆d. Thus, we have\nto show that for some α, Φ is α-strongly convex w.r.t | · |1 on ∆d.\n\nDΦ(y, x) = KL(y, x) +\nX\n(xi\ni\n-yi)\n= KL(y, x)\n≥2|x -y|2\nWhere we used the fact that x, y ∈∆d to show\ni(xi -yi) = 0 and used Pinsker\ninequality show the result. ThuPs, Φ is 1-strongly conve\nP\nx w.r.t. | · |1 on ∆d.\nRemembering that Φ(x) =\nd\ni=1 xi log(xi) was defined to be negative entropy, we know\nthat -log(d) ≤Φ(x) ≤0 for x ∈∆d. Thus,\nR2 = max Φ(x)\nx∈∆d\n-min Φ(x) = log(d) .\nx∈∆d\nCorollary: Let f be a convex function on ∆d such that\n∥g∥inf≤L,\n∀g ∈∂f(x),\n∀x ∈∆d .\n2 log(d)\nThen, Mirror descent with η = 1\nq\ngives\nL\nk\n2 log(d)\n2 log(d)\nf(xk) -f(x∗) ≤L\nr\n,\nf(x*\nk) -f(x∗)\nk\n≤L\nr\nk\nBoosting: For weak classifiers f1(x), · · · , fN(x) and α ∈∆n, we define\nN\nf1(x)\n.\nfα =\n\nX\nαjfj\nand\nF(x) =\n\nj=1\n\n..\nfN(x)\n\nso that fα(x) is the weighted majority vote classifier. Note that |F|inf≤1.\nAs shown before, in boosting, we have:\nn\ng = ∇Rb\nn,φ(fα) =\nX\nφ′(-yifα(xi))(-yi)F(xi) ,\nn i=1\nSince |F|\n≤1 and |y|\n≤1, then |g|\n≤L where L is the Lipschitz constant of φ\ninf\ninf\ninf\n(e.g., a constant like e or 2).\nr\nb\nb\n2 log(N)\nRn,φ(fα*\nk) -min Rn,φ(fα)\nL\nα∈∆n\n≤\nk\nWe need the number of iterations k ≈n2 log(N).\nThe functions fj's could hit all the vertices. Thus, if we want to fit them in a ball, the\nq\nball has to be radius\n√\nN. This is why the projected gradient descent would give the rate of\nN\nk . But by looking at the gradient we can determine the right geometry. In this case, the\ngradient is bounded by sup-norm which is usually the most constraining norm in projected\n\ngradient descent. Thus, using Mirror descent would be most beneficial.\nOther Potential Functions:\nThere are other potential functions which are strongly convex w.r.t l1 norm. In partic-\nular, for\nΦ(x) =\n|x|p\np,\np = 1 +\np\nlog(d)\nthen Φ is c\np\nlog(d)-strongly convex w.r.t l1 norm.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Sylvain Carpentier\nOct. 26, 2015\nIn this lecture we will wrap up the study of optimization techniques with stochastic\noptimization. The tools that we are going to develop will turn out to be very efficient in\nminimizing the φ-risk when we can bound the noise on the gradient.\n3. STOCHASTIC OPTIMIZATION\n3.1 Stochastic convex optimization\nWe are considering random functions x 7→l(x, Z) where x is the optimization parameter and\nZ a random variable. Let PZ be the distribution of Z and let us assume that x 7→l(x, Z) is\nconvex PZ a.s. In particular, IE[l(x, Z)] will also be convex. The goal of stochastic convex\noptimization is to approach minx\nIE[l(x, Z)] when\nis convex. For our purposes,\nwill\n∈C\nC\nC\nbe a deterministic convex set. However, stochastic convex optimization can be defined more\nbroadly. The constraint can be itself stochastic :\nC = {x, IE[g(x, Z)] ≤0},\ng convex PZ a.s.\nC = {x, IP[g(x, Z) ≤0] ≥1 -ε},\n\"chance constraint\"\nThe second constraint is not convex a priori but remedies are possible (see [NS06, Nem12]).\nIn the following, we will stick to the case where X is deterministic. A few optimization\nproblems we tackled can be interpreted in this new framework.\n3.1.1\nExamples\nBoosting. Recall that the goal in Boosting is to minimize the φ-risk:\nmin IE[φ(\nf\n∈Λ\n-Y\nα(X))] ,\nα\nwhere Λ is the simplex of IRd.\nDefine Z = (X, Y ) and the random function l(α, Z) =\nφ(-Y fα(X)), convex PZ a.s.\nLinear regression. Here the goal is the minimize the l2 risk:\nmin IE[(Y -f\nα(X)) ] .\nα∈IRd\nDefine Z = (X, Y ) and the random function l(α, Z) = (Y -fα(X))2, convex PZ a.s.\nMaximum likelihood. We consider samples Z1, . . . , Zn iid with density pθ, θ ∈Θ. For\ninstance, Z N(θ, 1).\nThe likelihood functions associated to this set of samples is θ\n∼\n7→\nQn\n=1 pθ(Zi). Let p∗(Z) denote the true density of Z\ni\n(it does not have to be of the form pθ\nfor some θ ∈Θ. Then\nn\nY\nZ\np∗(z)\nIE[log\npθ(Zi)] = -\nlog(\n)p∗(z)dz + C =\nn\npθ(z)\ni=1\n-KL(p∗, pθ) + C\n\nwhere C is a constant in θ. Hence maximizing the expected log-likelihood is equivalent to\nminimizing the expected Kullback-Leibler divergence:\nn\nmaxIE[log\nY\npθ(Zi)]\nθ\ni=1\n⇐⇒KL(p∗, pθ)\nExternal randomization. Assume that we want to minimize a function of the form\nf(x) =\nn\nX\nfi(x) ,\nn i=1\nwhere the functions f1, . . . , fn are convex. As we have seen, this arises a lot in empirical\nrisk minimization. In this case, we treat this problem as deterministic problem but inject\nartificial randomness as follows.\nLet I be a random variable uniformly distributed on\n[n] =: {1, . . . , n}. We have the representation f(x) = IE[fI(x)], which falls into the context\nof stochastic convex optimization with Z = I and l(x, I) = fI(x).\nImportant Remark: There is a key difference between the case where we assume that\nwe are given independent random variables and the case where we generate artificial ran-\ndomness. Let us illustrate this difference for Boosting. We are given (X1, Y1), . . . , (Xn, Yn)\ni.i.d from some unknown distribution. In the first example, our aim is to minimize\nIE[φ(-Y fα(X))] based on these n observations and we will that the stochastic gradient\nallows to do that by take one pair (Xi, Yi) in each iteration. In particular, we can use\neach pair at most once. We say that we do one pass on the data.\nWe could also leverage our statistical analysis of the empirical risk minimizer from\nprevious lectures and try to minimize the empirical φ-risk\nRˆn,φ(fα) =\nn\nX\nφ(\nα\ni=1\n-Yif (Xi))\nn\nby generating k independent random variables I1, . . . , Ik uniform over [n] and run the\nstochastic gradient descent to us one random variable Ij in each iteration. The difference\nhere is that k can be arbitrary large, regardless of the number n of observations (we make\nmultiple passes on the data). However, minimizing IEI[φ(-YIfα(XI))|X1, Y1, . . . , Xn, Yn]\nwill perform no better than the empirical risk minimizer whose statistical performance\nis limited by the number n of observations.\n3.2 Stochastic gradient descent\nIf the distribution of Z was known, then the function x 7→IE[l(x, Z)] would be known and\nwe could apply gradient descent, projected gradient descent or any other optimization tool\nseen before in the deterministic setup. However this is not the case in reality where the\ntrue distribution PZ is unknown and we are only given the samples Z1, . . . , Zn and the\nrandom function l(x, Z). In what follows, we denote by ∂l(x, Z) the set of subgradients of\nthe function y 7→l(y, Z) at point x.\n\nAlgorithm 1 Stochastic Gradient Descent algorithm\nInput: x1 ∈C, positive sequence {ηs}s\n1, independent random variables Z , . . . , Z\n≥\nk\nwith distribution PZ.\nfor s = 1 to k -1 do\nys+1 = xs -ηsg s ,\ng s ∈∂l(xs, Zs)\nxs+1 = π (y\nC\ns+1)\nend for\nreturn x k =\nk\nk\nX\nxs\ns=1\nNote the difference here with the deterministic gradient descent which returns either\nx k or x*\nk = argmin f(x). In the stochastic framework, the function f(x) = IE[l(x, ξ)] is\nx1,...,xn\ntypically unknown and x k cannot be computed.\nTheorem: Let C be a closed convex subset of IRd such that diam(C) ≤R. Assume that\nhe convex function f(x) = IE[l(x, Z)] attains its minimum on C at x∗∈IRd. Assume\nthat l(x, Z) is convex PZ a.s. and that IE∥g ∥2 ≤L2 for all g ∈∂l(x, Z) for all x. Then\nif ηs ≡η =\nR\nL\n√,\nk\nLR\nIE[f(x k)] -f(x∗) ≤√\nk\nProof.\nf xs\nf x\ngs\nxs\nx\n= IE[g s\n⊤(xs -x∗)|xs]\n=\nIE[(ys+1\nxs)⊤(xs\nx∗) xs]\nη\n-\n-\n|\n=\nIE[∥x\ns -y\ns+1∥+\nη\n∥xs -x∗\n∥-∥ys+1 -x∗∥|xs]\n≤\n(η2IE[∥g s∥2|xs] + IE[∥x\ns -x∗∥|xs] -IE[∥xs+1 -x∗\nx\nη\n∥2\n| s]\nTaking expectations and summing over s we get\nk\n1 X\nηL2\nR2\nf(xs)\n(\ns=1\n-f x∗)\nk\n≤\n+\n.\n2ηk\nUsing Jensen's inequality and chosing η =\nR\nL\n√, we get\nk\nLR\nIE[f(x k)] -f(x∗) ≤√\nk\n(\n) -\n( ∗) ≤\n⊤(\n-\n∗)\n\n3.3 Stochastic Mirror Descent\nWe can also extend the Mirror Descent to a stochastic version as follows.\nAlgorithm 2 Mirror Descent algorithm\nInput: x1 ∈argmin\nΦ(x), ζ :\nd\nR\n→\nd\nR\nsuch that ζ(x) = ∇Φ(x), independent\nC∩D\nrandom variables Z1, . . . , Zk with distribution PZ.\nfor s = 1, · · · , k do\nζ(ys+1) = ζ(xs) -ηg s for g s ∈∂l(xs, Zs)\nx\nΦ\ns+1 = Π (y\nC\ns+1)\nend for\nreturn x = 1\nk\nk\nP\ns=1 xs\nTheorem: Assume that Φ is α-strongly convex on C ∩D w.r.t. ∥· ∥and\nR2 =\nsup Φ(x)\nΦ x)\nx\n-min\n(\n∈C∩D\nx∈C∩D\ntake x1 = argminx\nΦ(x) (assume that it exists). Then, Stochastic Mirror Descent\n∈C∩D\nwith η = R\nL\nq\n2α\nx\nR outputs k, such that\nIE[f(x k)] -f(x∗) ≤RL\nr\n2 .\nαk\nProof. We essentially reproduce the proof for the Mirror Descent algorithm.\nTake x♯∈C ∩D. We have\nf(xs\ns\ns\nIE[g s\n⊤(xs -x∗)|xs]\n=\nIE[(ζ(xs)\nη\n-ζ(ys+1))⊤(xs -x♯)|xs]\n=\nIE[(∇Φ(xs) -∇Φ(ys+1))⊤(xs\nη\n-x♯)|xs]\n=\nIE\nh\nD\n♯\nΦ(xs, ys+1) + DΦ(x♯, xs) -DΦ(x , ys+1)\nη\nxs\ni\n≤\nIE\nh\nDΦ(x\n♯\ns, ys+1) + DΦ(x , xs) -DΦ(x♯, xs+1) x\nη\ns\ni\nη\n≤\n2 IE[∥g s∥2\n|xs] +\nIE\n2α\n∗\nη\nh\nDΦ(x♯, xs) -DΦ(x♯, xs+1)\nxs\ni\n) -f(x♯) ≤g⊤(x -x♯)\n\nwhere the last inequality comes from\nDΦ(xs, ys+1) = Φ(xs) -Φ(ys+1) -∇Φ(ys+1)⊤(xs -ys+1)\nα\n≤[∇Φ(x\ns) -∇Φ(ys+1)]⊤(xs -ys+1) -2 ∥ys+1 -xs∥\nα\n≤η∥g s∥\nx\ny\ny\nx\n∗∥s -\ns+1∥-2 ∥s+1 -\ns∥\nη2∥g\n≤\ns∥2\n∗.\n2α\nSumming and taking expectations, we get\nk\n1 X\nηL2\nDΦ(x♯, x1)\n[f(x\n♯\ns)\n]\ns=1\n≤\n+\nk\n-f(x )\n.\n(3.1)\n2α\nkη\nWe conclude as in the previous lecture.\n3.4 Stochastic coordinate descent\nLet f be a convex L-Lipschitz and differentiable function on IRd. Let us denote by ∇if the\npartial derivative of f in the direction ei. One drawback of the Gradient Descent Algorithm\nis that at each step one has to update every coordinate ∇if of the gradient. The idea of\nthe stochastic coordinate descent is to pick at each step a direction ej uniformly and to\nchoose that ej to be the direction of the descent at that step. More precisely, of I is drawn\nuniformly on [d], then IE[d∇If(x)eI] = ∇f(x). Therefore, the vector d∇If(x)eI that has\nonly one nonzero coordinate is an unbiased estimate of the gradient ∇f(x). We can use\nthis estimate to perform stochastic gradient descent.\nAlgorithm 3 Stochastic Coordinate Descent algorithm\nInput: x1 ∈C, positive sequence {ηs}s\n1, independent random variables I , . . . , I\n≥\nk\nuniform over [d].\nfor s = 1 to k -1 do\nys+1 = xs -ηsd∇If(x)eI ,\ng s ∈∂l(xs, Zs)\nxs+1 = π (ys+1)\nC\nend for\nk\nreturn x k = k\nX\nxs\ns=1\nIf we apply Stochastic Gradient Descent to this problem for η = Rq\n2 , we directly\nL\ndk\nobtain\n2d\nIE[f(x k)] -f(x∗) ≤RL\nr\nk\nWe are in a trade-offsituation where the updates are much easier to implement but where\nwe need more steps to reach the same precision as the gradient descent alogrithm.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Zach Izzo\nOct. 27, 2015\nPart III\nOnline Learning\nIt is often the case that we will be asked to make a sequence of predictions, rather\nthan just one prediction given a large number of data points. In particular, this situa-\ntion will arise whenever we need to perform online classification: at time t, we have\n(X1, Y1), . . . , (Xt-1, Yt-1) iid random variables, and given Xt, we are asked to predict\nYt ∈{0, 1}. Consider the following examples.\nOnline Shortest Path: We have a graph G = (V, E) with two distinguished vertices\ns and t, and we wish to find the shortest path from s to t. However, the edge weights\nE1, . . . , Et change with time t. Our observations after time t may be all of the edge weights\nE1, . . . , Et; or our observations may only be the weights of edges through which our path\ntraverses; or our observation may only be the sum of the weights of the edges we've traversed.\nDynamic Pricing: We have a sequence of customers, each of which places a value vt\non some product. Our goal is to set a price pt for the tth customer, and our reward for\ndoing so is pt if pt ≤vt (in which case the customer buys the product at our price) or 0\notherwise (in which case the customer chooses not to buy the product). Our observations\nafter time t may be v1, . . . , vt; or, perhaps more realistically, our observations may only be\n1I(p1 < v1), . . . , 1I(pt < vt). (In this case, we only know whether or not the customer bought\nthe product.)\nSequential Investment: Given N assets, a portfolio is ω ∈∆N = {x ∈IRn : xi ≥\n0, PN\nx\ni=1\ni = 1}. (ω tells what percentage of our funds to invest in each stock. We could\nalso allow for negative weights, which would correspond to shorting a stock.) At each time\nt, we wish to create a portfolio ωt ∈∆N to maximize ωT\nt zt, where zt ∈IRN is a random\nvariable which specifies the return of each asset at time t.\nThere are two general modelling approaches we can take: statistical or adversarial.\nStatistical methods typically require that the observations are iid, and that we can learn\nsomething about future points from past data. For example, in the dynamic pricing example,\nwe could assume vt ∼N(v, 1). Another example is the Markowitz model for the sequential\ninvestment example, in which we assume that log(zt) ∼N(μ, Σ).\nIn this lecture, we will focus on adversarial models. We assume that zt can be any\nbounded sequence of numbers, and we will compare our predictions to the performance of\nsome benchmark. In these types of models, one can imagine that we are playing a game\nagainst an opponent, and we are trying to minimize our losses regardless of the moves he\nplays. In this setting, we will frequently use optimization techniques such as mirror descent,\nas well as approaches from game theory and information theory.\n\n1. PREDICTION WITH EXPERT ADVICE\n1.1 Cumulative Regret\nLet A be a convex set of actions we can take. For example, in the sequential investment\nexample, A = ∆N. If our options are discrete-for instance, choosing edges in a graph-then\nthink of A as the convex hull of these options, and we can play one of the choices randomly\naccording to some distribution. We will denote our adversary's moves by Z. At time t,\nwe simultaneously reveal at ∈A and zt ∈Z. Denote by l(at, zt) the loss associated to the\nplayer/decision maker taking action at and his adversary playing zt.\nIn the general case, Pn\nt\nl\n=1 (at, zt) can be arbitrarily large. Therefore, rather than looking\nat the absolute loss for a series of n steps, we will compare our loss to the loss of a benchmark\ncalled an expert. An expert is simply some vector b ∈An, b = (b1, . . . , bt, . . . , b\nT\nn) . If we\nchoose K experts b(1), . . . , b(K), then our benchmark value will be the minimum cumulative\nloss amongst of all the experts:\nn\n(j)\nbenchmark = min\n≤j≤K\nX\nl(bt , zt).\nt=1\nThe cumulative regret is then defined as\nn\nn\nRn =\nX\n(j)\nl(at, zt) -min\nl(bt , zt).\n1≤j≤K\nt=1\nX\nt=1\nAt time t, we have access to the following information:\n1. All of our previous moves, i.e. a1, . . . , at-1,\n2. all of our adversary's previous moves, i.e. z1, . . . , zt-1, and\n3. All of the experts' strategies, i.e. b(1), . . . , b(K).\nNaively, one might try a strategy which chooses a\n= b∗\n∗\nt\nt , where b\nis the expert which\nhas incurred minimal total loss for times 1, . . . , t -1. Unfortunately, this strategy is easily\nexploitable by the adversary: he can simply choose an action which maximizes the loss for\nthat move at each step. To modify our approach, we will instead take a convex combination\nof the experts' suggested moves, weighting each according to the performance of that expert\nthus far. To that end, we will replace l(at, zt) by l(p, (bt, zt)), where p ∈∆K denotes a\n(1)\n(K)\nconvex combination, bt = (bt , . . . , bt\n)T ∈AK is the vector of the experts' moves at time\nt, and zt ∈Z is our adversary's move. Then\nn\nn\nRn =\nX\nl(pt, zt) -min\nl(ej, zt)\n1≤j≤K\nt=1\nX\nt=1\nwhere ej is the vector whose jth entry is 1 and the rest of the entries are 0. Since we are\nrestricting ourselves to convex combinations of the experts' moves, we can write A = ∆K.\nWe can now reduce our goal to an optimization problem:\nK\nn\nmin\n∈∆K\nX\nθj\nθ\nj=1\nX\nl(ej, zt).\nt=1\n\nFrom here, one option would be to use a projected gradient descent type algorithm: we\ndefine\nqt+1 = pt -η(l(e\nT\n1, zt), . . . , l(eK, zT ))\nK\nand then p\n∆\nt+1 = π\n(pt) to be the projection of qt+1 onto the simplex.\n1.2 Exponential Weights\nSuppose we instead use stochastic mirror descent with Φ = negative entropy. Then\nqt\nqt+1,j = pt+1,j exp(-ηl(ej, zt)),\npt+1,j =\n,\nPK\nl\nq\n=1\nt+1,l\nwhere we have defined\nK\nwt,j\npt =\nej\n,\nexp\nK w\nj=1\nl=1\n,l\n!\nwt,j =\nt\n\nt-1\nX\n-η\nX\nl(ej, zs)\ns=1\n!\n.\nThis process looks at the los\nP\ns from each expert and downweights it exponentially according\nto the fraction of total loss incurred. For this reason, this method is called an exponential\nweighting (EW) strategy.\nRecall the definition of the cumulative regret Rn:\nn\nn\nRn =\nX\nl(pt, zt) -min\n1≤j≤K\nt=1\nX\nl(ej, zt).\nt=1\nThen we have the following theorem.\nTheorem: Assume l(·, z) is convex for all z ∈Z and that l(p, z) ∈[0, 1] for all p ∈\n∆K, z ∈Z. Then the EW strategy has regret\nlog K\nηn\nRn ≤\n+\n.\nη\nIn particular, for η =\nq\n2 log K ,\nn\nRn ≤\np\n2n log K.\nProof. We will recycle much of the mirror descent proof. Define\nK\nft(p) =\nX\npjl(ej, zt).\nj=1\nDenote ∥· ∥:= | · |1. Then\nn\nn\n1 X\nη 1\n∥g ∥2\nt\nlog K\nft(\n(\n∗\npt) -f\n∗\nt\n) ≤\nn\nP\nt=1\np\n+\n,\nn\nηn\nt=1\n\nwhere gt ∈∂ft(pt) and ∥· ∥∗is the dual norm (in this case ∥· ∥∗= | · |inf). The 2 in the\ndenominator of the first term of this sum comes from setting α = 1 in the mirror descent\nproof. Now,\ngt ∈∂ft(pt) ⇒gt = (l(e1, zt), . . . , l(e\nT\nK, zt)) .\nFurthermore, since l(p, z) ∈[0, 1], we have ∥gt∥∗= |gt|inf≤1 for all t. Thus\nn\nη 1\nn\nP\nt=1 ∥gt∥2\n∗\nlog K\nη\nlog K\n+\n≤\n+\n.\nnη\nηn\nSubstituting for ft yields\nn\nK\nK\nn\nX X\nηn\nlog K\npt,jl(ej, zt) -\nmin\nX\npjl(ej, zt)\n≤\n+\n.\np∈∆K\nη\nt=1 j=1\nj=1\nX\nt=1\nNote that the boxed term is actually min1≤j≤K\nPn\nl\nt=1 (ej, zt).\nFurthermore, applying\nJensen's to the unboxed term gives\nn\nK\nn\nX X\npt,jl(ej, zt) ≥\nX\nl(pt, zt).\nt=1 j=1\nt=1\nSubstituting these expressions then yields\nηn\nlog K\nRn ≤\n+\n.\nη\nWe optimize over η to reach the desired conclusion.\nWe now offer a different proof of the same theorem which will give us the optimal\nconstant in the error bound. Define\n\nt-1\n!\nK\nK\nX\nX\nP\nwt,je\nj=1\nj\nwt,j = exp\n-η\nl(ej, zs)\n,\nWt =\nwt,j,\npt =\n.\nWt\ns=1\nj=1\nFor t = 1, we initialize w1,j = 1, so W1 = K. It should be noted that the starting values for\nw1,j are uniform, so we're starting at the correct point (i.e. maximal entropy) for mirrored\ndescent. Now we have\n\nK\nexp -\nt-1\nη\nl\n, z\nW\nj=1\ns=1 (ej\ns) exp(-ηl(ej, zt))\nt+1\nlog\ng\nW\nP\n\nK\nt\nt\n\n= lo\n\n·\nP\n-1\nl\nle\n= e\nP\nxp\n\n-η\nP\nj\n, z\n=1\n\n( l\ns)\n\n= log (IEJ∼pt [exp(-ηl(eJ, zt))])\n\nHoeffding's lemma ⇒≤log\n\nη\nIE\ne\ne-η\nJ\nJ\nl(e ,zt)\nη2\n\n=\n-ηIEJl(eJ, zt)\nη2\nη2\nJensen's ⇒≤\n-ηl(IEJeJ, zt) =\n-ηl(pt, zt)\n\nsince IEJej = PK\np\nj=1\nt,jej. If we sum over t, the sum telescopes. Since W1 = K, we are left\nwith\nn\nnη2\nlog(Wn+1) -log(K) ≤\n-η\nX\nl(pt, zt).\nt=1\nWe have\nK\nn\nlog(Wn+1) = log\n\nX\nexp\n\n-η\nX\nl(ej, zs)\nj=1\ns=1\n!\n,\nso setting\nn\nj∗= argmin1≤j≤K\nP\nl(e\n\nt=1\nj, zt), we obtain\nn\nn\nlog(Wn+1) ≥log\n\nexp\n\n-η\n!!\nX\nl(ej∗, zs)\n= -η\nX\nl(ej∗, zt).\ns=1\nt=1\nRearranging, we have\nn\nn\nX\nηn\nlog K\nl(pt, zt) -\nl\nt=\nX\n(ej∗, zt) ≤\n+\n.\nη\nt=1\nFinally, we optimize over η to arrive at\nη =\nr\n8 log K\no\nRn ≤\nn\nr\nn l g K\n⇒\n.\nThe improved constant comes from the assumption that our loss lies in an interval of size\n1 (namely [0, 1]) rather than in an interval of size 2 (namely [-1, 1]).\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Haihao (Sean) Lu\nNov. 2, 2015\nRecall that in last lecture, we talked about prediction with expert advice. Remember\nthat l(ej, zt) means the loss of expert j at time t, where zt is one adversary's move. In this\nlecture, for simplexity we replace the notation zt and denote by zt the loss associated to all\nexperts at time t:\nzt =\n\nl(e1, zt)\n\n...\n,\nl(eK, zt)\n\nwhereby for p ∈∆K, p\n\n⊤zt =\nK\np\ne\nj=1\njl( j, zt). This gives an alternative definition of ft(p)\nin last lecture. Actually it is easy to check ft(p) = p⊤zt, thus we can rewrite the theorem\nfor exponential weighting(EW\nP\n) strategy as\nn\nn\nRn ≤\nX\np⊤\nt zt -min\np\np\nt=1\n∈∆k\nX\n⊤zt\n2n log K,\nt=1\n≤\np\nwhere the first inequality is Jensen inequality:\nn\nn\nX\np⊤\nz zt\nt=1\n≥\nX\nl(pz, zt) .\nt=1\nWe consider EW strategy for bounded convex losses. Without loss of generality, we\nassume l(p, z) ∈[0, 1], for all (p, z) ∈∆K × Z, thus in notation here, we expect pt ∈∆K\nand z\nK\n\nt ∈[0, 1] . Indeed if l(p, z) ∈[m, M] then one can work with a rescaled loss l(a, z) =\nl(a,z)-m. Note that now we have bounded gradient on pt, since zt is bounded.\nM-m\n2. FOLLOW THE PERTURBED LEADER (FPL)\nIn this section, we consider a different strategy, called Follow the Perturbed Leader.\nAt first, we introduce Follow the Leader strategy, and give an example to show that\nFollow the Leader can be hazardous sometimes. At time t, assume that choose\nt-1\npt = argmin\np∈∆K\nX\np⊤zs.\ns=1\nNote that the function to be optimized is linear in p, whereby the optimal solution should\nbe a vertex of the simplex. This method can be viewed as a greedy algorithm, however, it\nmight not be a good strategy.\nConsider the following example. Let K = 2, z1 = (0, ε)⊤, z2 = (0, 1)⊤, z3 = (1, 0)⊤,\nz4 = (0, 1)⊤and so on (alternatively having (0, 1)⊤and (1, 0)⊤when t ≥2), where ε is small\nenough. Then with Following the Leader Strategy, we have that p1 is arbitrary and in the\nbest case p1 = (1, 0)⊤, and p2 = (1, 0)⊤, p3 = (0, 1)⊤, p4 = (1, 0)⊤and so on (alternatively\nhaving (0, 1)⊤and (1, 0)⊤when t ≥2).\n\nIn the above example, we have\nn\nn\nX\nn\nn\np⊤\nt zt\nmin\np⊤zt\nn\n1 ,\np ∆k\nt=1\n-\n≤\n-\n-\n≤\n∈\nX\nt=1\n-\nwhich gives raise to linear regret.\nNow let's consider FPL. FPL regularizes FL by adding a small amount of noise, which\ncan guarantee square root regret under oblivious adversary situation.\nAlgorithm 1 Follow the Perturbed Leader (FPL)\nInput: Let ξ be a random variables uniformly drawn on [0, 1]K.\nη\nfor t = 1 to n do\nt-1\npt = argmin\np∈∆K\nX\ns=1\np⊤zs + ξ\n\n.\nend for\nWe analyze this strategy in oblivious adversaries, which means the sequence zt is chosen\nahead of time, rather than adaptively given. The following theorem gives a bound for regret\nof FPL:\nTheorem: FPL with η = √1\nyields expected regret:\nkn\nIEξ[Rn] ≤2\n√\n2nK .\nBefore proving the theorem, we introduce the so-called Be-The-Leader Lemma at first.\nLemma: (Be-The-Leader)\nFor all loss function l(p, z), let\nt\np∗\nt = arg min\nl(p, zs) ,\np∈∆K\nX\ns=1\nthen we have\nn\nn\nX\nl(p∗\nt, zt)\nt=1\n≤\nX\nl(pn\n∗, zt)\nt=1\nProof. The proof goes by induction on n. For n = 1, it is clearly true. From n to n + 1, it\n\nfollows from:\nn+1\nn\nX\nl(pt\n∗, zt) =\nX\nl(p∗\nt, zt) + l(pn\n∗\n+1, zn+1)\nt=1\ni=1\nn\n≤\nX\nl(p∗\nn, zt) + l(p∗\nn+1, zn+1)\ni=1\nn\n≤\nX\nl(p∗\nn+1, zt) + l(p∗\nn+1, zn+1) ,\ni=1\nwhere the first inequality uses induction and the second inequality follows from the definition\nof p∗\nn.\nProof of Theorem. Define\nt\nqt = argmin p⊤(ξ +\np∈∆K\nX\nzs) .\ns=1\nUsing the Be-The-Leader Lemma with\npT (ξ + z1)\nif t = 1\nl(p, zt) =\npT zt\nif t > 1 ,\nwe have\nn\nn\nq1\n⊤ξ +\nX\nqt\n⊤zt ≤min q⊤(ξ +\nq\nt=\n∈∆K\nX\nzt) ,\nt=1\nwhereby for any q ∈∆K,\nn\nX\nqt\n⊤zt -q⊤zt\n\n≤\n\nq⊤-q1\n⊤\nξ ≤∥q\ni\n-q1\n=1\n∥∥ξ∥inf≤\n,\nη\nwhere the second inequality uses H older's inequality and the third inequality is from the\nfact that q and q1 are on the simplex and ξ is in the box.\nNow let\nt\nqt = arg min p⊤\n\nξ + zt +\nX\nzs\np∈∆K\ns=1\n!\nand\nt\npt = arg min p⊤\n+\n∈∆\n\nξ + 0\np\nK\nX\nzs\ns=1\n!\n.\nTherefore,\nn\nn\nIE[Rn] ≤\nX\np⊤\nt zt\ni\n-min\np⊤zt\np∈∆k\n=1\nX\ni=1\nn\n\nn\n≤\nX\nqt\n⊤zt -p∗T zt\n+\nX\nIE[(pt\nt\ni=1\n=1\n-q )⊤zt]\ni\nn\n≤\n+\nX\nIE[(pt -qt)⊤zt] ,\n(2.1)\nη\ni=1\n\nwhere p∗= arg minp∈∆K Pn\np z\nt=1\n⊤t.\nNow let\nt-1\nh(ξ) = zt\n⊤\n\narg min p⊤[ξ +\np∈∆K\nX\nzs]\n,\ns=1\n!\nthen we have a easy observation that\nIE[zt\n⊤(pt -qt)] = IE[h(ξ)] -IE[h(ξ + zt)] .\nHence,\nIE[zt\n⊤(pt -q\nK\nK\nt)] = η\nZ\nh(ξ)dξ -η\nZ\nh(ξ)dξ\nξ∈[0, 1 ]K\nξ∈z +[0, 1 ]K\nt\nη\nη\n≤ηK\nZ\nh(ξ)dξ\nξ∈[0, 1 ]K\nη\n\\\nn\nz\n, 1 ]K\nt+[0 η\no\n≤ηK\nZ\n1dξ\nξ∈[0, 1 ]K\\\nn\nzt+[0, 1 ]K\nη\nη\no\n= IP (∃i ∈[K], ξ(i) ≤zt(i))\nK\n≤\nX\nIP\n\nUnif\n\n[0,\n]\n\n≤zt(i)\nη\ni=1\n\n≤ηKzt(i) ≤ηK ,\n(2.2)\nwhere the first inequality is from the fact that h(ξ) ≥0, the second inequality uses\nh(ξ) ≤1, the second equation is just geometry and the last inequality is due to zt(i) ≤1.\nCombining (2.1) and (2.2) together, we have\nIE[Rn] ≤\n+ ηKn .\nη\nIn particular, with η =\nq\n2 , we have\nKn\nIE[Rn] ≤2\n√\n2Kn ,\nwhich completes the proof.\n\nonline learning with structured experts-a biased\nsurvey\nG abor Lugosi\nICREA and Pompeu Fabra University, Barcelona\n\non-line prediction\nA repeated game between forecaster and environment.\nAt each round t,\nthe forecaster chooses an action\nt 2 {1, . . . ,\n};\n(actions are often called experts)\nthe environment chooses losses `t(1), . . . , `t(N) 2 [0, 1];\nthe forecaster su↵ers loss `t(It).\nThe goal is to minimize the regret\nRn =\nX\nn\nn\n`t(It) -min\ni\nt\nN\n=1\nX\n`t(i)\nt=1\n!\n.\nI\nN\n\nsimplest example\nIs it possible to make (1/n)Rn ! 0 for all loss assignments?\nLet N = 2 and define, for all t = 1, . . . , n,\n`t(1) =\n⇢0\nif It = 2\nif It = 1\nand `t(2) = 1 -`t(1).\nThen\nX\nn\nn\nn\n`t(It) = n\nand\nmin\n`\ni=1 2\nt=1\nX\nt(i)\n,\n\nt=1\nso\nRn\nn\n≥\n.\n\nrandomized prediction\nKey to solution: randomization.\nAt time t, the forecaster chooses a probability distribution\npt-1 = (p1,t-1, . . . , pN,t-1)\nand chooses action i with probability pi,t-1.\nSimplest model: all losses `s(i), i = 1, . . . , N, s < t, are\nobserved: full information.\n\nHannan and Blackwell\nHannan (1957) and Blackwell (1956) showed that the forecaster\nhas a strategy such that\nX\nn\nn\n`t(It)\nn\n-min\ni\nt\nN\n=1\nX\n`t(i)\nt=1\n!\n! 0\nalmost surely for all strategies of the environment.\n\nbasic ideas\nexpected loss of the forecaster:\nN\n`t(pt-1) =\nX\npi,t\n(\n-1`t i) = Et`t(It)\ni=1\nBy martingale convergence,\nX\nn\nn\n`\n`\n1 2\nt(It) -\nX\n/\nt(p\nn\nt-1)\n= OP(n-\n)\nt=1\nt=1\nso it suffices to study\nn\nX\nn\nn\n`t(pt\n)\nmin\n` (i)\n-1 -\nN\n=1\nX\nt\ni\nt\nt=1\n!\n!\n\nweighted average prediction\nIdea: assign a higher probability to better-performing actions.\nVovk (1990), Littlestone and Warmuth (1989).\nA popular choice is\nexp\n⇣\n-⌘Pt-1\n=1 ` (\ns\ns i)\npi,t\n=\n-1\n⌘\nP\n= 1\nN\n=1 exp\n⇣\n-⌘P\ni\n, . . . , N .\nt-1\n=1 `s( )\nk\ns\nk\nwhere ⌘> 0. Then\n⌘\nX\nn\nn\n\nn\n`t(pt-1) -min\ni\nt=1\nN\nX\n`t(i)\nt=1\n!\n=\nr\nln N\n2n\nwith ⌘=\np\n8 ln N/n.\n\nproof\nt\nLet Li,t = P\ns=1 `s(i) and\nN\nWt =\nX\nX\nN\nwi,t =\ne-⌘Li,t\ni=1\ni=1\nfor t ≥1, and W0 = N. First observe that\nWn\nln\ne\nW\nX\nN\n=\nln\n,\n\n-⌘Li n\ni=1\n!\n-ln N\n≥\nln\n✓\nmax\ne-⌘Li,n\nln\ni=1,...,N\n◆\n-\nN\n=\n-⌘\nmin\nLi,n\ni=1,...,N\n-ln N .\n\nproof\nOn the other hand, for each t = 1, . . . , n\nWt\nPN\ni=1 wi,t\n1e-⌘`t(i)\nln\n=\nln\n-\nWt-1\nPN\nP\nj=1 wj,t-1\nN\n=1 w\nP\ni,t-1` (\n-\ni\nt i)\n⌘2\n\n⌘\n+\nN\nj=1 wj,t-1\n⌘2\n=\n-⌘`t(pt-1) + 8\nby Hoe↵ding's inequality.\nHoe↵ding (1963): if X 2 [0, 1],\nln Ee-⌘\n⌘2\nX -⌘EX + 8\n\nproof\nfor each t = 1, . . . , n\nW\nt\n⌘\nln\n-⌘`t(p\n) +\nW\nt\nt-1\n-1\nSumming over t = 1, . . . , n,\nW\nn\nn\n⌘\nln\n-⌘\nX\n`t(pt\n1) +\nn .\nW0\n-\nt=1\nCombining these, we get\nX\nn\nln N\n⌘\n`t(pt\n1)\nmin\nLi,n +\n+\nn\n-\ni=1,...,N\n⌘\nt=1\n\nlower bound\nThe upper bound is optimal: for all predictors,\nPn\n=1 `t(Itp-\nn\n)\nmin\nt\niN\n`\nt\nsup\n=1\nt(i)\n1 .\nn,N,`t(i)\n(n/2) ln N\nP\n≥\nIdea: choose `t(i) to be i.i.d. symmetric Bernoulli coin flips.\nn\nsup\nX\nn\n`t(It)\nt(i)\n-min\n`\ni\nt=1\nN\nX\n`t(i)\n\"\nt=1\n!\nn\n≥\nE\nX\n`t(It) -min\ni\nt=1\nN\nn\nX\nn\n`t(i)\nt=1\n#\n=\n2 -min Bi\niN\nWhere B1, . . . , BN are independent Binomial (n, 1/2).\nUse the central limit theorem.\n\nfollow the perturbed leader\nt-1\nIt = arg min\nX\n`s(i) + Zi,t\ni=1,...,N s=1\nwhere the Zi,t are random noise variables.\nThe original forecaster of Hannan (1957) is based on this idea.\n\nfollow the perturbed leader\nIf the Zi,t are i.i.d. uniform [0,\np\nnN], then\nN\nRn\nn\nr\n+ Op(n-1/2) .\nn\nIf the Z\nz\ni,t are i.i.d. with density (⌘/2)e-⌘| |, then for\n⌘⇡\np\nlog N/n,\nlog N\nR\n1/2\nn c\nr\n+ Op(n-\n) .\nn\nn\nKalai and Vempala (2003).\n\ncombinatorial experts\nOften the class of experts is very large but has some combinatorial\nstructure. Can the structure be exploited?\npath planning. At each time\ninstance, the forecaster chooses a\npath in a graph between two\nfixed nodes. Each edge has an\nassociated loss. Loss of a path is\nthe sum of the losses over the\nedges in the path.\nN is huge!!!\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\nassignments: learning permutations\nGiven a complete\nbipartite graph\nKm,m, the\nforecaster chooses a\nperfect matching.\nThe loss is the sum\nof the losses over\nthe edges.\nHelmbold and Warmuth (2007): full information case.\nThis image has been removed due to copyright restrictions.\nPlease see the image at\nhttp://38.media.tumblr.com/tumblr_m0ol5tggjZ1qir7tc.gif\n\nspanning trees\nThe forecaster chooses a\nspanning tree in the complete\ngraph Km. The cost is the sum\nof the losses over the edges.\n\ncombinatorial experts\nd\nFormally, the class of experts is a set S ⇢{0, 1}\nof cardinality\n|S| = N.\nt\n2 Rd\nAt each time\n, a loss is assigned to each component: `t\n.\nLoss of expert v 2 S is `t(v) = `>\nt v.\nForecaster chooses It 2 S.\nThe goal is to control the regret\nX\nn\nX\nn\n`t(It) -\nmin\n`t(k) .\nk=1,...,N\nt=1\nt=1\n\ncomputing the exponentially weighted average forecaster\nOne needs to draw a random element of S with distribution\nproportional to\nwt(v) = exp\n.\nt\n-⌘Lt(v)\n/\n= exp\n\n-1\n-⌘\nX\n`>\nt v.\ns=1\n!\nt\nexp\nj\nY\nd\n=\n=1\n\n-1\n-⌘\nX\n`t,jvj\ns=1\n!\n.\n\ncomputing the exponentially weighted average forecaster\npath planning: Sampling may be done by dynamic programming.\nassignments: Sum of weights (partition function) is the permanent\nof a non-negative matrix. Sampling may be done by a FPAS of\nJerrum, Sinclair, and Vigoda (2004).\nspanning trees: Propp and Wilson (1998) define an exact sampling\nalgorithm. Expected running time is the average hitting time of\nthe Markov chain defined by the edge weights wt(v).\n\ncomputing the follow-the-perturbed leader forecaster\nIn general, much easier. One only needs to solve a linear\noptimization problem over S. This may be hard but it is well\nunderstood.\nIn our examples it becomes either a shortest path problem, or an\nassignment problem, or a minimum spanning tree problem.\n\nfollow the leader: random walk perturbation\nSuppose N experts, no structure. Define\nt\nIt = arg min\n(`i,s\n1 + Xs)\ni=1,...,N\nX\n-\ns=1\nwhere the Xs are either i.i.d. normal or ±1 coinflips.\nThis is like follow-the-perturbed-leader but with random walk\nt\nperturbation:\ns=1 Xt.\nAdvantage: fo\nP\nrecaster rarely changes actions!\n\nfollow the leader: random walk perturbation\nIf Rn is the regret and Cn is the number of times It 6= It-1, then\nERn 2ECn 8\np\n2n log N + 16 log n + 16 .\nDevroye, Lugosi, and Neu (2015).\nKey tool: number of leader changes in N independent random\nwalks with drift.\n\nfollow the leader: random walk perturbation\nThis also works in the \"combinatorial\" setting: just add an\nindependent N(0, d) at each time to every component.\nERn = Oe(B3/2p\nn log d)\nand\nECn = O(Bpn log d) ,\nwhere B = maxv2S kvk1.\n\nwhy exponentially weighted averages?\nMay be adapted to many di↵erent variants of the problem,\nincluding bandits, tracking, etc.\n\nmulti-armed bandits\nThe forecaster only observes `t(It) but not `t(i) for i 6= It.\nHerbert Robbins (1952).\nThis image has been removed due to copyright restrictions. Please see the image at\nhttps://en.wikipedia.org/wiki/Herbert_Robbins#/media/File:1966-HerbertRobbins.jpg\n\nmulti-armed bandits\nTrick: estimate `t(i) by\n`e\n`t(It)\nt(i\nI =i\n) =\n{ t\n}\npIt,t-1\nThis is an unbiased estimate:\ne\nX\nN\n`t(j)\nEt`t(i) =\npj,t\n{j=i} = `t(i)\n-\nj\nj\n,\n=1\np t-1\nUse the estimated losses to define exponential weights and mix\nwith uniform (Auer, Cesa-Bianchi, Freund, and Schapire, 2002):\n⇣\n-\nPt\nexp\n⌘\n-1 ` ( )\nP\n⇣\ns=1\npi,t\n1 -γ\nes i\n1 = (\n)\n-\nN\nk=1 exp\n⌘\nγ\n-⌘P\n+\nt-1\ns=1 `s(k)\n⌘\nN\nexploration\ne\n|\n{z\n}\nexploitation\n|{z}\n\nmulti-armed bandits\nX\nn\nE\n\n`t(pt\n1) -min\nX\nn\n`t(i)\n!\n= O\nr\nN ln N\n!\n,\nn\n-\ni\nt=1\nN t=1\nn\n\nmulti-armed bandits\nLower bound:\nn\nN\nsup E\nX\nn\n`t(pt\n1) -min\nX\n`t(i)\n,\n`t(i)\nn\n-\ni\nN\n=1\n\n=1\n!\n≥C\nr\nn\nt\nt\nDependence on N is not logarithmic anymore!\nAudibert and Bubeck (2009) constructed a forecaster with\nn\nn\nN\nmax E\n`t(pt\ni\n-1)\n`t(i)\n= O\n,\nN\nn\n\n-\nt=1\nt=1\n!\nr\nn\n!\nX\nX\n\ncalibration\nSequential probability assignment.\nA binary sequence x1, x2, . . . is revealed one by one.\nAfter observing x1, . . . , xt-1, the forecaster issues prediction\nIt 2 {0, 1, . . . , N}.\nMeaning: \"chance of rain is It/N\".\nForecast is calibrated if\n66P\nn\nt=1 xt\n{It=i}\nPn\nt=1\n{It=i}\n-i\nN\n2N + o(1)\nwhenever lim supn(1/n) Pn\nt=1\n{It=i\n>\n.\n}\nIs there a forecaster that is calibrated for all possible sequences?\nNO. (Dawid, 1985).\n\nrandomized calibration\nHowever, if the forecaster is allowed to randomize then it is\npossible! (Foster and Vohra, 1997).\nThis can be achieved by a simple modification of any regret\nminimization procedure.\nSet of actions (experts): {0, 1, . . . , N}.\nAt time t, assign loss `t(i) = (xt -i/N)2 to action i.\nOne can now define a forecaster. Minimizing regret is not\nsufficient.\n\ninternal regret\nRecall that the (expected) regret is\nX\nn\n`t(pt\nX\nn\nn\n1) -min\n`t(i) = max\n-\ni\ni\nt=1\nt=1\nX\npj,t (`t(j) -`t(i))\nt=1\nX\nj\nThe internal regret is defined by\nn\nmax\nX\npj,t (`t(j) -`t(i))\ni,j\nt=1\npj,t (`t(j) -`t(i)) = Et\n`\n{It=j (\n}\nt(j) -`t(i))\nis the expected regret of having taken action j instead of action i.\n\ninternal regret and calibration\nBy guaranteeing small internal regret, one obtains a calibrated\nforecaster.\nThis can be achieved by an exponentially weighted average\nforecaster defined over N2 actions.\nCan be extended even for calibration with checking rules.\n\nprediction with partial monitoring\nFor each round t = 1, . . . , n,\nthe environment chooses the next outcome Jt 2 {1, . . . , M}\nwithout revealing it;\nthe forecaster chooses a probability distribution pt\nand\n-1\ndraws an action It 2 {1, . . . , N} according to pt-1;\nthe forecaster incurs loss `(It, Jt) and each action i incurs loss\n`(i, Jt). None of these values is revealed to the forecaster;\nthe feedback h(It, Jt) is revealed to the forecaster.\nH = [h(i, j)]N⇥M is the feedback matrix.\nL = [`(i, j)]N⇥M is the loss matrix.\n\nexamples\nDynamic pricing. Here M = N, and L = [`(i, j)]N⇥N where\n(j\n)\ni j\n-i\n{ij\n+ c\n( , ) =\n}\n{i>j\n`\n} .\nN\nand h(i, j) =\n{i>j\nor\n}\nh(i, j) = a\ni\nj\n+ b\ni>j\n,\ni, j = 1, . . . , N .\n{ }\n{\n}\nMulti-armed bandit problem. The only information the forecaster\nreceives is his own loss: H = L.\n\nexamples\nApple tasting.\n=\n= 2.\nL =\nH =\na\na\nb\nc\n.\nThe predictor only receives feedback when he chooses the second\naction.\nLabel efficient prediction. N = 3, M = 2.\nL =\n2 1\n4 0\nH =\n2 a\nb\nc\nc\n.\nN\nM\nc\nc\n\na general predictor\nA forecaster first proposed by Piccolboni and Schindelhauer (2001).\nCrucial assumption: H can be encoded such that there exists an\nN ⇥N matrix K = [k(i, j)]N⇥N such that\nL = K · H .\nThus,\n`(i, j) =\nX\nN\nk(i, l)h(l, j) .\nl=1\nThen we may estimate the losses by\n`e\nk(i, It)h(It, Jt)\n(i, Jt) =\npIt,t\n.\n\na general predictor\nObserve\ne\nX\nN\nk(i, k)h(k, Jt)\nEt`(i, Jt)\n=\npk,t-1\nk=1\npk,t-1\nk\nk\nX\nN\n=\n(i, k)h(k, Jt) = `(i, Jt) ,\n=1\n`e(i, Jt) is an unbiased estimate of `(i, Jt).\nLet\ne-⌘Li,t-1\nγ\npi,t-1 = (1 -γ)\ne\nP\n+\nN\n=1 e-⌘Lek,t-1\nN\nk\nwhere Li,t = Pt\n=1 `(i, Jt).\ne\ns\ne\n\nperformance bound\nWith probability at least 1 -δ,\n1 X\nn\n`(It, Jt)\nn\n-\nmin\ni=1,...,N\nt=1\n-\nX\nn\n`(i, Jt)\nn t=1\n\nCn\n1/3N2/3p\nln(N/δ) .\nwhere C depends on K. (Cesa-Bianchi, Lugosi, Stoltz (2006))\nHannan consistency is achieved with rate O(n-1/3) whenever\nL = K · H.\nThis solves the dynamic pricing problem.\nBart ok, P al, and Szepesv ari (2010): if M = 2, only possible rates\nare n-1/2, n-1/3, 1\n\nimperfect monitoring: a general framework\nS is a finite set of signals.\nFeedback matrix: H : {1, . . . , N} ⇥{1, . . . , M} ! P(S).\nFor each round t = 1, 2 . . . , n,\nthe environment chooses the next outcome Jt 2 {1, . . . , M}\nwithout revealing it;\nthe forecaster chooses pt-1 and draws an action\nIt 2 {1, . . . , N} according to it;\nthe forecaster receives loss `(It, Jt) and each action i su↵ers\nloss `(i, Jt), none of these values is revealed to the forecaster;\na feedback st drawn at random according to H(It, Jt) is\nrevealed to the forecaster.\n\ntarget\nDefine\n`(p, q) =\nX\npiqj`(i, j)\ni,j\nH(·, q) = (H(1, q), . . . , H(N, q))\nwhere H(i, q) =\nj qjH(i, j) .\nDenote by F the set\nP\nof those ∆that can be written as H(·, q) for\nsome q.\nF is the set of \"observable\" vectors of signal distributions ∆.\nThe key quantity is\n⇢(p, ∆) =\nmax\n`(p, q)\nq : H(·,q)=∆\n⇢is convex in p and concave in ∆.\n\nrustichini's theorem\nThe value of the base one-shot game is\nmin max `(p, q) = min max ⇢(p, ∆)\np\nq\np\n∆2F\nIf qn is the empirical distribution of J1, . . . , Jn, even with the\nknowledge of H(·, qn) we cannot hope to do better than\nminp ⇢(p, H(·, qn)).\nRustichini (1999) proved that there exists a strategy such that for\nall strategies of the opponent, almost surely,\nlim sup\n@1\nX\n`(It, Jt) -min ⇢(p, H(\nn\nn\n!1\np\n·, qn))\nt=1,...,n\nA 0\n\nrustichini's theorem\nRustichini's proof relies on an approachability theorem for a\ncontinuum of types (Mertens, Sorin, and Zamir, 1994).\nIt is non-constructive.\nIt does not imply any convergence rate.\nLugosi, Mannor, and Stoltz (2008) construct efficiently computable\nstrategies that guarantee fast rates of convergence.\n\ncombinatorial bandits\nThe class of actions is a set S ⇢{0\nd\n, 1}\nof cardinality |S| = N.\nAt each time t\nd\n, a loss is assigned to each component: `t 2 R .\nLoss of expert v 2 S is `t(v) = `>\nt v.\nForecaster chooses It 2 S.\nThe goal is to control the regret\nX\nn\nn\n`t(It) -\nmin\nk=1,...,N\nt=1\nX\n`t(k) .\nt=1\n\ncombinatorial bandits\nThree models.\n(Full information.) All d components of the loss vector are\nobserved.\n(Semi-bandit.) Only the components corresponding to the chosen\nobject are observed.\n(Bandit.) Only the total loss of the chosen object is observed.\nChallenge: Is O(n-1/2poly(d)) regret achievable for the\nsemi-bandit and bandit problems?\n\ncombinatorial prediction game\nAdversary\nPlayer\n\ncombinatorial prediction game\nAdversary\nPlayer\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\ncombinatorial prediction game\nAdversary\nPlayer\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\ncombinatorial prediction game\nAdversary\nPlayer\n`2\n`6\n`d-1\n`1\n`4\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\ncombinatorial prediction game\nAdversary\nPlayer\n`2\n`6\n`d-1\n`1\n`4\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\nloss su↵ered: `2 + `7 + . . . + `d\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\ncombinatorial prediction game\nAdversary\nPlayer\n`2\n`6\n`d-1\n`1\n`4\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\n<\nFull Info:\n`1, `2, . . . , `d\nFeedback: :\nloss su↵ered: `2 + `7 + . . . + `d\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\ncombinatorial prediction game\nAdversary\nPlayer\n`2\n`6\n`d-1\n`1\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\n`\n<\nn\nd\nb ck:\nFull I fo:\n`1, `2, . . . , `\nFeed a\n: Semi-Bandit:\n`2, `7, . . . , `d\nBandit:\n`2 + `7 + . . . + `d\nloss su↵ered: `2 + `7 + . . . + `d\n(c) Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/fairuse.\n\nnotation\n`2\n`6\n`d-1\n`1\n`4\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\n`2\n`6\n`d-1\n`1\n`4\n`5\n`9\n`d-2\n`d\n`3\n`8\n`7\nS ⇢{0, 1}d\n`t 2 Rd\n+\nVt 2 S\nT\n, loss su↵ered: `t Vt\nregret:\nn\nR\nE\nX\nn\nTV -\nE\nX\nT\nn =\n`t\nt\nmin\n`t u\nu\nt=1\n2S\nt=1\nT\nloss assumption: `t v\n1 for all v\nand t = 1, . . . , n.\n|\n|\n2 S\n\nweighted average forecaster\nAt time t assign a weight wt,i to each i = 1, . . . , d.\nThe weight of each v k 2 S is\nw t(k) =\nwt,i .\ni:vk\nY\n(i)=1\nLet qt-1(k) = w t-1(k\nN\n)/\nk=1 w t-1(k).\nAt each time t, draw Kt from\nP\nthe distribution\npt-1(k) = (1 -γ)qt\nk\n-1( ) + γμ(k)\nwhere μ is a fixed distribution on S and γ > 0. Here\nwt,i = exp -⌘Lt,i\nwhere Lt,i = `1,i +\n+ `t,i and\n.\n`t,i is a\n/\nn estimated loss.\ne\ne\ne\n· · ·\ne\ne\n\nloss estimates\nDani, Hayes, and Kakade (2008).\nDefine the scaled incidence vector\nX t = `t(Kt)V Kt\nwhere Kt is distributed according to pt-1.\nLet Pt-1 = E\n⇥\nV Kt V >\nbe the\nKt\nd ⇥d correlation matrix.\nHence\nPt\n⇤\n-(i, j) =\nk : vk(i\nX\npt-1(k) .\n)=vk(j)=1\nSimilarly, let Qt-1 and M be the correlation matrices of E V V >\nwhen V has law, qt-1 and μ. Then\n⇥\n⇤\nPt-1(i, j) = (1 -γ)Qt-1(i, j) + γ M(i, j) .\nThe vector of loss estimates is defined by\ne`t = P+\nt-1X t\nwhere P+\nt-1 is the pseudo-inverse of Pt-1.\n\nkey properties\nM M+v = v for all v 2 S.\nQt-1 is positive semidefinite for every t.\nPt\n1 P+\nr a\nt\n1v = v fo\nll t\nv\n-\nand\n-\n2 S.\nBy definition,\nEt X t = Pt-1 `t\nand therefore\nEt e`t = P+\nt-1Et X t = `t\nAn unbiased estimate!\n\nperformance bound\nThe regret of the forecaster satisfies\n1 ✓\nln\nELb\n2B2\nd\nN\nn -\nmin\nLn(k)\n+ 1\n.\nn\nk=1,...,N\n◆\n\ns✓\ndλmin(M)\n◆\nn\nwhere\nT\nλmin(M) =\nmin\nx Mx > 0\nx2span(S):kxk=1\nis the smallest \"relevant\" eigenvalue of M. (Cesa-Bianchi and\nLugosi, 2009.)\nLarge λmin(M) is needed to make sure no `t,i is too large.\n|e |\n\nperformance bound\nOther bounds:\nB\np\nd ln N/n (Dani, Hayes, and Kakade). No condition on S.\nSampling is over a barycentric spanner.\nd\np\n(✓ln n)/n (Abernethy, Hazan, and Rakhlin). Computationally\nefficient.\n\neigenvalue bounds\nλmin(M) =\nmin\nE(V , x)2 .\nx2span(S):kxk=1\nwhere V has distribution μ over S.\nIn many cases it suffices to take μ uniform.\n\nmultitask bandit problem\nThe decision maker acts in m games in parallel.\nIn each game, the decision maker selects one of R possible actions.\nAfter selecting the m actions, the sum of the losses is observed.\nλmin = R\nmax E Ln -Ln(k)\nln R .\nk\n2m\np\n3nR\nThe price of only obs\nh\nb\nerving the su\ni\nm of losses is a factor of m.\nGenerating a random joint action can be done in polynomial time.\n\nassignments\nPerfect matchings of Km,m.\nAt each time one of the N = m! perfect matchings of Km,m is\nselected.\nλmin(M) = m -1\nmax E Ln -Ln(k) 2m\n3n ln(m!) .\nk\nOnly a factor of m\nh\nw\nb\norse than n\ni\naive full-i\np\nnformation bound.\n\nspanning trees\nIn a network of m nodes, the cost of communication between two\nnodes joined by edge e is `t(e) at time t. At each time a minimal\nconnected subnetwork (a spanning tree) is selected. The goal is to\nminimize the total cost. N = mm-2.\nλmin(M) =\nO\nm -\n✓\nm2\n◆\n.\nThe entries of M are\nP{Vi = 1}= m\nP\nJ\nVi = 1, Vj = 1\n\n=\nif\nm2\ni ⇠j\nP Vi = 1, Vj = 1 =\nif\n.\ni 6⇠j\nm\nJ\n\nstars\nAt each time a central node of a network of m nodes is selected.\nCost is the total cost of the edges adjacent to the node.\nλmin ≥1 -O\n✓\nm\n◆\n.\n\ncut sets\nA balanced cut in K2m is the collection of all edges between a set\nof m vertices and its comp.lement. Each balanced cut has m2\n2m\nedges and there are N =\nm\n/\nbalanced cuts.\nλmin(M) =\n-O\n.\n✓\nm2\n◆\nChoosing from the exponentially weighted average distribution is\nequivalent to sampling from ferromagnetic Ising model. FPAS by\nRandall and Wilson (1999).\n\nhamiltonian cycles\nA Hamiltonian cycle in Km is a cycle that visits each vertex exactly\nonce and returns to the starting vertex. N = (m -1)!\nλmin ≥m\nEfficient computation is hopeless.\n\nsampling paths\nIn all these examples μ is uniform over S.\nFor path planning it does not always work.\nWhat is the optimal choice of μ?\nWhat is the optimal way of exploration?\n\nminimax regret\nRn =\ninf\nmax\nsup\nRn\nstrategy S⇢{0,1}d adversary\nTheorem\nLet n ≥d 2. In the full information and semi-bandit games, we\nhave\n0.008 dpn Rn d\np\n2n,\nand in the bandit game,\n0.01 d 3/2pn Rn 2 d 5/2p\n2n.\n\nproof\nupper bounds:\nD = [0, +1)d, F(x) = 1\nd\nog\n⌘\ni=1 xi l\nxi works for full\ninformation but it is only opti\nP\nmal up to a logarithmic factor in the\nsemi-bandit case.\nin the bandit case it does not work at all! Exponentially weighted\naverage forecaster is used.\nlower bounds:\ncareful construction of randomly chosen set S in each case.\n\na general strategy\nLet D\nd\nbe a convex subset of R\nwith nonempty interior int(D).\nA function F : D ! R is Legendre if\n- F is strictly convex and admits continuous first partial\nderivatives on int(D),\n- For u 2 @D, and v 2 int(D), we have\nlim\n(u -v T\n) rF\n.\n(1 -s)u + sv\n= +\ns!0,s>0\n1.\nThe Bregman divergence DF : D ⇥int(\n/\nD) associated to a\nLegendre function F is\nDF(u\nT\n, v) = F(u) -F(v) -(u -v) rF(v).\n\nCLEB (Combinatorial LEarning with Bregman divergences)\nParameter: F Legendre on D ⊃Conv(S)\nConv(S)\nD\n∆(S)\npt\nwt\nw 0\nt+1\nwt+1\npt+1\n(1) wt\n+1 2 D :\nrF(w 0\n+1) = rF(\n)\nt\nwt -` t\n(2) wt+1 2 arg min DF(w, wt\n+1)\nw2Conv(S)\n(3) pt+1 2 ∆(S) : wt+1 = EV ⇠pt+1V\n\nGeneral regret bound for CLEB\nTheorem\nIf F admits a Hessian r2F always invertible then,\nn\nRn / diam\nT\n-1\nDF (S) + E\nX\n`t\n=1\n⇣\nr F(wt)\nt\n⌘\n`t.\n\nDi↵erent instances of CLEB: LinExp (Entropy Function)\nD\nPd\n= [0, +\n)d, F(x) = 1\n⌘\ni=1 xi log xi\n>\n>\n< Full Info: Exponentially weighted average\n>\n>\n: Semi-Bandit=Bandit: Exp3\nAuer et al. [2002]\n>\n>\n> Full Info: Component Hedge\n>\n>\n>\n> Koolen, Warmuth and Kivinen [2010]\n>\n<\n>\n>\nSemi-Bandit: MW\n>\n> Kale, Reyzin and Schapire [2010]\nBandit: new algorithm\n>\n>\n>\n>\n:\n\nDi↵erent instances of CLEB: LinINF (Exchangeable\nHessian)\nD = [0, +1)d, F(x) = Pd\ni=1\nR xi\n1( )\n-\ns ds\nINF, Audibert and Bubeck [2009]\n⇢ (x) = exp(⌘x) : LinExp\n(x) = (-⌘x)-q, q > 1 : LinPoly\n\nDi↵erent instances of CLEB: Follow the regularized leader\nD = Conv(S), then\nwt+1 2 arg min\nX\nt\nT\n`s w + F(w)\nw2D\ns=1\n!\nParticularly interesting choice: F self-concordant barrier function,\nAbernethy, Hazan and Rakhlin [2008]\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Haihao (Sean) Lu\nNov. 2, 2015\n3. STOCHASTIC BANDITS\n3.1 Setup\nThe stochastic multi-armed bandit is a classical model for decision making and is defined\nas follows:\nThere are K arms(different actions). Iteratively, a decision maker chooses an arm k ∈\n{1, . . . , K}, yielding a sequence XK,1, . . . , XK,t, . . ., which are i.i.d random variables with\nmean μk. Define μ∗= maxj μj or ∗∈arg max. A policy π is a sequence {πt}t≥1, which\nindicates which arm to be pulled at time t. πt ∈{1, . . . , K} and it depends only on the\nobservations strictly interior to t. The regret is then defined as:\nn\nn\nRn = max IE[\nXK,t] -IE[\nXπt,t]\nk\nX\nt=1\nX\nt=1\nn\n= nμ∗-IE[\nX\nXπt,t]\nt=1\nn\n= nμ∗-IE[IE[\nK\nX\nXπt,t | πt]]\nt=1\n=\nX\n∆kIE[Tk(n)] ,\nk=1\nn\nwhere ∆k = μ∗-μk and Tk(n) = P\nt=1 1I(πt = k) is the number of time when arm k was\npulled.\n3.2 Warm Up: Full Info Case\nX1,t\n.\nAssume in this subsection that K = 2 and we observe the full information\n\n..\n\nat\nXK,t\ntime t after choosing πt. So in each iteration, a normal idea is to choose\n\nthe arm\n\nwith\nhighest average return so far. That is\n\nπt = argmax Xk,t\nk=1,2\nwhere\nXk,t = t\nt\nX\ns=1\nAssume from now on that all random variable Xk,t are subGaussian with variance proxy\nσ2, which means IE[eux\nu σ\n] ≤e\nfor all u ∈IR. For example, N(0, σ2) is subGaussian with\nXk,s\n\nvariance proxy σ2 and any bounded random variable X ∈[a, b] is subGaussian with variance\nproxy (b -a)2/4 by Hoeffding's Lemma.\nTherefore,\nRn = ∆IE[T2(n)] ,\n(3.1)\nwhere ∆= μ1 -μ2. Besides,\nn\n\nT2(n) = 1 +\nX\n1I(X2,t > X1,t)\nt=2\nn\n= 1 +\nX\n\n1I(X2,t -X1,t -(μ2 -μ1) ≥∆) .\nt=2\n\nIt is easy to check that (X2,t -X1,t)-(μ2 -μ1) is centered subGaussian with variance proxy\n2σ2, whereby\n\n-t∆\nIE[1I(X\n2,t > X1,t)] ≤e\n4σ\nby a simple ChernoffBound. Therefore,\ninf\nRn ≤∆(1 +\nX\n4σ2\n-t∆\ne\n4σ ) ≤∆+\n,\n(3.2)\n∆\nt=0\nwhereby the benchmark is\n4σ2\nRn ≤∆+\n.\n∆\n3.3 Upper Confidence Bound (UCB)\nWithout loss of generality, from now on we assume σ = 1. A trivial idea is that after s\npulls on arm k, we use μˆk,s = 1 P\nj∈{pulls of k} XK,j and choose the one with largest μˆk,s.\ns\nThe problem of this trivial policy is that for some arm, we might try it for only limited\ntimes, which give a bad average and then we never try it again. In order to overcome this\nlimitation, a good idea is to choose the arm with highest upper bound estimate on the mean\nof each arm at some probability lever. Note that the arm with less tries would have a large\ndeviations from its mean. This is called Upper Confidence Bound policy.\n\nAlgorithm 1 Upper Confidence Bound (UCB)\nfor t = 1 to K do\nπt = t\nend for\nfor t = K + 1 to n do\nt-1\nTk(t) =\nX\n1I(πt = k)\ns=1\n(number of time we have pull arm k before time t)\nμˆk,t =\nX\nXK,t∧s\nTk(t) s=1\nlog t)\nπt ∈argmax\n(\n(\nμˆk,t + 2\nk∈[K]\ns\nTk(t)\n)\n,\nend for\nTheorem: The UCB policy has regret\nK\nX\nlog n\nπ2\nRn ≤8\n+ (1 +\n)\n∆k\nk,∆k>0\nX\n∆k\nk=1\nProof. From now on we fix k such that ∆k > 0. Then\nn\nIE[Tk(n)] = 1 +\nt=\nX\nIP(πt = k) .\nK+1\nNote that for t > K,\n2 log t\n2 log t\n{πt = k} ⊆{μˆk,t + 2\ns\n≤μˆ∗,t + 2\n}\nTk(t)\ns\nT∗(t)\n(\ns\n2 log t\ns\n[\n2 log t\n2 log t\n⊆\n{μk ≥μˆk,t + 2\n}\n{μ∗≥μˆ∗,t + 2\n}\n[\n{μ∗≤μk + 2\nTk(t)\nT∗(t)\ns\n, πt = k}\nTk(t)\n)\nAnd from a union bound, we have\ns\n2 log t\n2 log t\nIP(μˆk,t -μk < -2\n) = IP(μˆk,t -μk < 2\nTk(t)\ns\n)\nTk(t)\nt\n-s 8 log t\n≤\nX\nexp(\ns\n)\ns=1\n= t3\nt-1\n\n2 log t\n2 log t\nThus IP(μk > μˆk t + 2\nk\n) ≤\n,\n3 and similarly we have IP(μ∗> μˆ∗,t + 2\n) ≤\n1 ,\nT (t)\nt\nT\n∗(t)\nt\nwhereby\nq\nq\nn\nn\nn\nX\n2 log t\nIP(πt = k) ≤2\nμk + 2\ns\nX\n+\nX\nIP(μ∗≤\n, πt = k)\nt3\nTk(t)\nt=K+1\nt=1\nt=1\ninf\nn\n8 log t\n≤2\nX\n+\nX\nIP(Tk(t) ≤\n, πt = k)\nt3\n∆2\nt=1\nt=1\nk\ninf\nn\n≤2\nX 1\ng n\n+\nX\n8 lo\nIP(Tk(t) ≤\n, πt = k)\nt3\n∆2\nt=1\nt=1\nk\ninf\ninf\nX 1\n8 o\n≤2\n+\n(\nt=1\nX\nl g n\nIP s ≤\n)\nt\n∆2\ns=1\nk\ninf\n≤2\nX 1\n8 log n\n+\nt2\n∆2\nt=1\nk\nπ2\n8 log n\n=\n+\n,\n∆2\nk\nwhere s is the counter of pulling arm k. Therefore we have\nK\nRn =\nX\nX\nπ2\n8 log n\n∆kIE[Tk(n)] ≤\n∆k(1 +\n+\n) ,\n∆2\nk\nk,∆k\nk\n=1\n>0\nwhich furnishes the proof.\nConsider the case K = 2 at first, then from the theorem above we know Rn ∼log n,\n∆\nwhich is consistent with intuition that when the difference of two arm is small, it is hard to\ndistinguish which to choose. On the other hand, it always hold that Rn ≤n∆. Combining\nlog n\nlog(n∆2)\nthese two results, we have Rn ≤\n∧n∆, whereby Rn ≤\nup to a constant.\n∆\n∆\nActually it turns out to be the optimal bound. When K ≥3, we can similarly get the\nlog(n∆2)\nresult that Rn ≤P\nk\nk\nk\n. This, however, is not the optimal bound. The optimal bound\n∆\nshould be P\nlog(n/H)\nk\nk\n, which includes the harmonic sum and H =\n1 . See [Lat15].\n∆\nP\nk ∆2\nk\n3.4 Bounded Regret\nFrom above we know UCB policy can give regret that increases with at most rate log n with\nn. In this section we would consider whether it is possible to have bounded regret. Actually\nit turns out that if there is a known separator between the expected reward of optimal arm\nand other arms, there is a bounded regret policy.\nWe would only consider the case when K = 2 here.\nWithout loss of generality, we\nassume μ1 = ∆and μ\n∆\n2 = -\n, then there is a natural separator 0.\n\nAlgorithm 2 Bounded Regret Policy (BRP)\nπ1 = 1 and π2 = 2\nfor t = 3 to n do\nif maxk μˆk,t > 0 then\nthen πt = argmaxk μˆk,t\nelse\nπt = 1, πt+1 = 2\nend if\nend for\nTheorem: BRP has regret\nRn ≤∆+\n.\n∆\nProof.\nIP(πt = 2) = IP(μˆ2,t > 0, πt = 2) + IP(μˆ2,t ≤0, πt = 2)\nNote that\nn\nn\nX\nIP(μˆ2,t > 0, πt = 2) ≤IE\n1I(μˆ2,t > 0, πt = 2)\nt=3\nX\nt=3\nn\n≤IE\nX\n1I(μˆ2,t -μ2 > 0, πt = 2)\nt=3\ninf\n≤\nX\ne-s∆\ns=1\n=\n,\n∆2\nwhere s is the counter of pulling arm 2 and the third inequality is a Chernoffbound.\nSimilarly,\nn\nn\nX\nIP(μˆ2,t ≤0, πt = 2) =\nX\nIP(μˆ1,t ≤0, πt-1 = 1)\nt=3\nt=3\n≤\n,\n∆2\nCombining these two inequality, we have\nRn ≤∆(1 +\n) ,\n∆2\n\n18.657: Mathematics of Machine Learning\nLecturer: Alexander Rakhlin\nLecture\nScribe: Kevin Li\nNov. 16, 2015\n4. PREDICTION OF INDIVIDUAL SEQUENCES\nIn this lecture, we will try to predict the next bit given the previous bits in the sequence.\nGiven completely random bits, it would be impossible to correctly predict more than half\nof the bits. However, certain cases including predicting bits generated by a human can\nbe correct greater than half the time due to the inability of humans to produce truly\nrandom bits. We will show that the existence of a prediction algorithm that can predict\nbetter than a given threshold exists if and only if the threshold satifies certain probabilistic\ninequalities.\nFor more information on this topic, you can look at the lecture notes at\nhttp://stat.wharton.upenn.edu/~rakhlin/courses/stat928/stat928_notes.pdf\n4.1 The Problem\nTo state the problem formally, given a sequence y1, . . . , yn, . . . ∈{-1, +1}, we want to find\na prediction algorithm yˆt = yˆt(y1, . . . , yt\n1) that correctly predicts yt as much as possible.\n-\niid\nIn order to get a grasp of the problem, we will consider the case where y1, . . . , yn ∼Ber(p).\nIt is easy to see that we can get\nn\nIE\n\"\nn\nX\n=\n=1\n{yˆt\nyt\nt\n}\n#\n→min{p, 1 -p}\nby letting yˆt equal majority vote of the first t -1 bits. Eventually, the bit that occurs\nwith higher probability will alway\ns have occurred more times. So the central limit theorem\nshows that our loss will approach min{\np, 1 -p} at the rate of O(√).\nn\nKnowing that the distribution of the bits are iid Bernoulli random variables made the\nprediction problem fairly easy. More surprisingly is the fact that we can achieve the same\nfor any individual sequence.\nClaim: There is an algorithm such that the following holds for any sequence y1, . . . , yn, . . ..\nn\nlim sup\nX\n{yˆt = yt} -min{y n, 1\nn\nn\nn t=1\n-y } ≤0 a.s.\n→inf\nIt is clear that no deterministic strategy can achieve this bound. For any deterministic\nstrategy, we can just choose yt = -yˆt and the predictions would be wrong every time. So\nwe need a non-deterministic algorithm that chooses qˆt = IE[yˆt] ∈[-1, 1].\nTo prove this claim, we will look at a more general problem. Take a fixed horizon n ≥1,\nand function φ : {±1}n →\nR. Does\nthere exist a randomized prediction strategy such that\nfor any y1, . . . , yn\nn\nIE[\nX\n{yˆt = yt\nn t=1\n}] ≤φ(y1, . . . , yn) ?\n\nFor certain φ such as φ ≡0, it is clear that no randomized strategy exists. However for\nφ ≡\n, the strategy of randomly predicting the next bit (qˆt = 0) satisfies the inequality.\nLemma: For a stable φ, the following are equivalent\nn\na) ∃(qˆt)t=1,...,n∀y1, . . . , yn\nIE[\nX\n{yˆt = yt}] ≤φ(y1, . . . , yn)\nn t=1\nb) IE[φ(o1, . . . , on)] ≥\nwhere o1, . . . , on are Rademacher random variables\nwhere stable is defined as follows\nDefinition (Stable Function): A function φ : {±1}n →\nis stable if\n|φ(. . . , yi, . . .) -φ(. . . , -yi, . . . )| ≤n\nProof. (a =\n⇒\nb) Suppose IEφ <\n. Take (y1, . . . , yn) = (o1, . . .\nR\n, on). Then IE[ 1\nn\ny\nn\nt=1\n{ˆt =\not}] =\n> IE[φ] so there must exist a sequence (\nn\no1, . . . , o\nn) such that IE[\nP\nP\nt\n{yˆt = ot}] >\nn\n=1\nφ(o1, . . . , on).\n(b =⇒a) Recu\nrsively define V (y1, . . . , yt) such that ∀y1, . . . , yn\nV (y1, . . . , yt\n1) =\nmin\nmax\n\nIE[ {yˆt = yt}] + V (y1, . . . y\n-\nn)\nqt∈[-1,1] yt∈±1\nn\n\nLooking at the definition, we can see that IE[ 1\nn\nt=1\n{yˆt = yt}] = V ( )\nn\n∅-V (y1, . . . , yn).\nNow we note that V (y1, . . . , yt) = -t -IE[φ(y1,\nP\n.\n. . , yt, ot\nn\n+1, . . . , on)] satisfies the recursive\ndefinition since\nt\nmin max\nIE[\nyˆt = yt ]\nIE[φ(y1, . . . , yt, ot+1, . . . , on)]\nqˆt\nyt\nn\n{\n} -\n-2n\nqˆtyt\nt\n= min max -\n-IE[φ(y1, . . . , yt, ot+1, . . . , on)]\n-\nqˆt\nyt\n2n\n-\n2n\nqˆ\nt\nq\n= min m x{-\nt\n-\nˆ\nt\na\n-\nt\nIE[φ(y1, . . . , yt\n1, 1, ot+1, . . . , on)] -\n,\n-IE[φ(y\n.\n-1,\n-\n1, . . , yt\n-1, ot+1, . . . , on)]\n-\nqˆt\n2n\n2n\n2n\n-\n2n }\nt\n= -IE[φ(y1, . . . , yt\n1, ot, ot+1, . . . , o\n-\nn)]\n-\n-\n2n\n=V (y1, . . . , yt\n1)\n-\nThe first equality uses the fact that for a, b ∈{±1},\n{\na = b} =\n-ab, the second uses the\nfact that yt ∈{±1}, the third minimizes the entire expression by choosing qˆt so that the\ntwo expressions in the max are equal. Here the fact that φ is stable means qˆt ∈[-1, 1] and\nis the only place where we need φ to be stable.\nTherefore we have\nn\nIE[\nX\n{yˆt = yt}] = V (∅)-V (y1, . . . , yn) = -IE[φ(o1, . . . , on)]+ +φ(y1, . . . , yn)\nn\nt=1\n≤φ(y1, . . . , yn)\n\nby b).\nBy choosing φ = min{y , 1 -y } +\nc\n√, this shows there is an algorithm that satisfies our\nn\noriginal claim.\n4.2 Extensions\n4.2.1\nSupervised Learning\nWe can extend the problem to a regression type problem by observing xt and trying to\npredict yt. In this case, the objective we are trying to minimize would be\nl\nn\nX\n(yˆt, yt) -inf\nf∈F\n,\nn\nX\nl(f(xt) yt)\nIt turns out that the best achievable performance in such problems is governed by martin-\ngale (or, sequential) analogues of Rademacher averages, covering numbers, combinatorial\ndimensions, and so on. Much of Statistical Learning techniques extend to this setting of\nonline learning. In addition, the minimax/relaxation framework gives a systematic way of\ndeveloping new prediction algorithms (in a way similar to the bit prediction problem).\n4.2.2\nEquivalence to Tail Bounds\nWe can also obtain probabilistic tail bound on functions φ on hypercube by using part a) of\nthe earlier lemma. Rearranging part a) of the lemma we get 1 -2φ(\ny1, . . . , yn) ≤\nqˆtyt.\nn\nThis implies\nP\nIP\nμ\nμ\nφ(o1, . . . , on) <\n-\n\n= IP\n1 -2φ(o1, . . . , on) > μ ≤IP\nn\nX\nqˆtot > μ ≤e-2n\nSo IEφ ≥1 =⇒\nexistence of a strategy\n=⇒\ntail boun\n\nd for φ\n<\n.\n\nWe can extend the results to higher dimensions. Consider z1, . . . , zn ∈B2 where B2 is\na ball in a Hilbert space. We can define recursively yˆ0 = 0 and yˆt+1 = ProjB2(yˆt -\n√zt).\nn\nBased on the properties of projections, for every\n∗∈\n, we have 1 P⟨ˆ -\n∗\n⟩≤\ny\nB2\nyt\ny , zt\nn\n√.\nn\nz\nTaking y∗\nt\n=\n,\n∥\nP\nP zt∥\nn\nn\n∀z1, . . . , zn,\nX\nzt\n√\n∥\nt=1\n∥-\nn ≤\nX\nyˆt,\nzt\nt=1\n⟨\n-⟩\nTake a martingale difference sequence Z1, . . . , Zn with values in B2. Then\nn\nn\nIP\n∥\nX\nnμ\nZt\n√\nt=1\n∥-\nn > μ\n\n≤IP(\nX\nt=1\n⟨yˆt, -Zt⟩> μ) ≤e-\nIntegrating out the tail,\nn\nIE∥\nX\nZt\nt=1\n∥≤c√n\n\nIt can be shown using Von Neumann minimax theorem that\nn\nn\n∃(yˆt)∀z1, . . . , zn, y∗∈B2\nX\nWt\n√\n⟨yˆt -y∗, zt⟩≤\nsup\nE\nc\nn\nMDSW\nt\n1,...,W\n=1\nn\n∥\nX\nt=1\n∥≤\nwhere the supremum is over all martingale difference sequences (MDS) with values in B2.\nBy the previous part, this upper bound is c√n. We conclude an interesting equivalence of\n(a) deterministic statements that hold for all sequences, (b) tail bounds on the size of a\nmartingale, and (c) in-expectation bound on this size.\nIn fact, this connection between probabilistic bounds and existence of prediction strate-\ngies for individual sequences is more general and requires further investigation.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Vira Semenova\nNov. 23, 2015\nIn this lecture, we talk about the adversarial bandits under limited feedback. Adver-\nsarial bandit is a setup in which the loss function l(a, z) : AxZ is determinitic.\nLim-\nited feedback means that the information available to the DM after the step t is It =\n{l(a1, z1), ..., l(at-1, zt)}, namely consits of the realised losses of the past steps only.\n5. ADVERSARIAL BANDITS\nConsider the problem of prediction with expert advice. Let the set of adversary moves be Z\nand the set of actions of a decision maker A = {e1, ..., eK}. At time t, at ∈A and zt ∈Z are\nsimultaneously revealed.Denote the loss associated to the decision at ∈A and his adversary\nplaying zt by l(at, zt). We compare the total loss after n steps to the minimum expert loss,\nnamely:\nn\nmin\n≤≤\nX\nlt(ej, zt),\nj\nK t=1\nwhere ej is the choice of expert j ∈{1, 2, .., K}.\nThe cumulative regret is then defined as\nn\nn\nRn =\nX\nlt(at, zt) -min\nX\nlt(ej, zt)\n1≤j≤K\nt=1\nt=1\nThe feedback at step t can be either full or limited. The full feedback setup means that\nthe vector f = (l(e1, z\n⊤\nt), ..., l(eK, zt))\nof losses incurred at a pair of adversary's choice zt and\neach bandit ej ∈{e1, ..., eK} is observed after each step t. Hence, the information available\nto the DM after the step t is I = ∪t\n′\nt\n′\n{l(a1, zt), ..., l(aK, z\nt =1\nt′)}. The limited feedback means\nthat the time -t feedback consists of the realised loss l(at, zt) only. Namely, the information\navailable to the DM is It = {l(a1, z1), ..., l(at, zt)}. An example of the first setup is portfolio\noptimization problems, where the loss of all possible portfolios is observed at time t. An\nexample of the second setup is a path planning problem and dynamic pricing, where the\nloss of the chosen decision only is observed. This lecture has limited feedback setup.\nThe two strategies, defined in the past lectures, were exponential weights, which yield\nthe regret of order Rn ≤c√n log K and Follow the Perturbed Leader. We would like to\nplay exponential weights, defined as:\nexp(\nt\nη\npt,j\nP -l\n=\ns=1 (ej, zs))\nPk\n-\nl=1 exp(-η Pt-1 l\ns=1 (ej, zs))\nThis decision rule is not feasible, since the loss l(ej, zt) are not part of the feedback if\nej = at. We will estimate it by\nl(ej, zt)1I(a\nˆ\nt = ej)\nl(ej, zt) =\nP(at = ej)\n\nˆ\nLemma: l(ej, zt) is an unbiased estimator of l(ej, zt)\nK\nI(\nP\nˆ\nl(e\ne =\nroof. E\nk,zt)1\nk\net)\natl(ej, zt) = P\nP\nk=1\n(a = e ) = l(e , z )\nP (at=ej)\nt\nk\nj\nt\nDefinition (Exp 3 Algorithm): Let η > 0 be fixed. Define the exponential weights\nas\n-Pt-1\nη\nˆ\n(j)\nexp(\nl(ej, zs))\np\ns=1\nt+1,j = Pk\nl=1 exp(-η Pt-1 ˆl\ns=1 (ej, zs))\n(Exp3 stands for Exponential weights for Exploration and Exploitation.)\nWe will show that the regret of Exp3 is bounded by √2nK log K. This bound is\n√\nK\ntimes bigger than the bound on the regret under the full feedback. The\n√\nK multiplier is\nthe price of have smaller information set at the time t. The are methods that allow to get\nrid of log K term in this expression. On the other hand, it can be shown that\n√\n2nK is the\noptimal regret.\n-Pt\nPK\n-1 ˆ\nW\ne\nProof. Let Wt,j = exp(\nk\nη\nl(e ,\n=\nj zs)), W\nt = P\nW\ns\nj=1\nt,j, and p =\nj=1\nt,j j\nt\nW\n.\nt\nW\nPK\n(-η P -1\nexp\nt\nlˆ e , z\nηlˆ e , z\nt+1\nj=1\ns=1 ( j\ns)) exp(\n( j\nt))\nlog(\n) = log(\n)\n(5.1)\nW\nPK\ne\nt\n-\nxp(-P -1\nt\nη\nˆl\ns=1 (ej, z\nj=1\ns))\nt-1\nˆ\n= log(IEJ ∼pt exp(-η\nX\nl(eJ , zs)))\n(5.2)\ns=1\n≤∗\nη2\nlog(1 -\nη\nˆ\nIEJ ∼ptl(eJ , z\nˆ\ns) +\nIEJ ∼ptl (eJ , zs)\n(5.3)\n∗\nˆ\nwhere\ninequality is obtained by plugging in IEJ ∼ptl(eJ , zt) into the inequality\nη2x2\nexp x ≥1 -ηx +\n.\nK\nK\nl(ej, zt)1I(a\ne )\nˆ\nIEJ ∼ptl(eJ zt) =\nX\nˆ\nt =\nj\n,\npt,jl(eJ , zt) =\nX\npt,j\n= l(at, zt)\n(5.4)\nP(at = ej)\nj=1\nj=1\nK\nK\nl2\n(ej, zt)1I(a\n)\nˆ\nˆ\nt = ej\nIEJ ∼ptl (eJ , zt) =\nX\npt,jl2(eJ , zt) =\nX\npt,j\n(5.5)\nP 2(at = ej)\nj=1\nj=1\nl2(ej, zt)\n=\nPat,t\n≤\n(5.6)\nPat,t\nSumming from 1 through n, we get\nlog(Wt+1) ≤\nlog(\nn\n1) -η P\nl\nt=1 (at, zt) + η\nW\nP\n1 .\nPa ,t\nt\n\nFor t = 1, we initialize w1,j = 1, so W1 = K.\nSince IE\np\nJ\n=\nPa ,t\nPK\nj,t\nK\nj=1\n=\n, the expression above becomes\np\nt\nj,t\nIE log(\nn\nη2Kn\nWn+1) -log K ≤-η P\nt\nl a\n=1 ( t, zt) +\nˆ\nNoting that log(Wn+1) = log(PK\nj=1 exp(-η Pt-1 l e , z\ns=1 ( j\ns))\nand defining j∗= argmin1≤j≤K\nPn\nl\nt=1 (ej, zt), we obtain:\nK\nt-1\nt-1\nlog(Wn+1) ≥log(\nX\nexp(-η\nX\nl(ej, zs))) = -η\nX\nl(ej, zs)\nj=1\ns=1\ns=1\nTogether:\nn\nn\nlog K\nηKn\nX\nl(at, zt) -min\nt\n≤j≤\nX\nl(ej, z )\nK\nt\nt=1\n≤\n+\nη\n=1\nThe choice of η := √2 log KnK yields the bound Rn\n√\n≤\n2K log Kn.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Ali Makhdoumi\nNov. 25, 2015\n6. LINEAR BANDITS\nRecall form last lectures that in prediction with expert advise, at each time t, the player\nplays at ∈{e1, . . . , ek} and the adversary plays zt such that l(at, zt) ≤1 for some loss\nfunction. One example of such loss function is linear function l(at, zt) = aT\nt zt where |zt|inf≤\n1. Linear bandits are a more general setting where the player selects an action at ∈A ⊂Rk,\nwhere A is a convex set and the adversary selects z\nT\nt ∈Z such that |zt at| ≤1. Similar to\nthe prediction with expert advise, the regret is defined as\nRn = E\n\" n\nn\nX\nT\nAt zt\n#\n-min\nT\na zt,\na∈K\nt=1\nX\nt=1\nwhere At is a random variable in A. Note that in the prediction with expert advise, the set\nA was essentially a polyhedron and we had min\nn\nT\na∈K\naT z\nt=1\nt = min1≤j≤k e z\nj\nt. However, in\nthe linear bandit setting the minimizer of aT zt can be any point of the set A and essentially\nthe umber experts that the player tries to \"comp\nP\nete\" with are infinity.\nSimilar, to the\nprediction with expert advise we have two settings:\n1 Full feedback: after time t, the player observes zt.\n2 Bandit feedback: after time t, the player observes AT\nt zt, where At is the action that\nplayer has chosen at time t.\nWe next, see if we can use the bounds we have developed in the prediction with expert\nadvise in this setting. In particular, we have shown the following bounds for prediction\nwith expert advise:\n1 Prediction with k expert advise, full feedback: Rn\n√\n≤\n2n log k.\n2 Prediction with k expert advise, bandit feedback: Rn\n√\n≤\n2nk log k.\nThe idea to deal with linear bandits is to discretize the set A. Suppose that A is bounded\n(e.g., A ⊂B2, where B2 is the l2 ball in Rk). We can use a 1 -covering of\nn\nA which we\nhave shown to be of size (smaller than)O(nk). This means there exist y1, . . . , y|N | such that\nfor any a ∈A, there exist yi such that ||yi -a|| ≤1 . We now can bound the regret for\nn\ngeneral case, where the experts can be any point in A, based on the regret on the discrete\nset, N = {y1, . . . , y|N |},as follows.\nRn = E\n\" n\n#\nn\nX\nT\nAt zt\nt\n-min\nT\na zt\na∈A\n=\nt=1\n\" n\nX\nn\n= E\nX\nT\nAt zt\n#\n-min\nT\na zt + o(1).\na∈N\nt=1\nX\nt=1\nTherefore, we restrict actions At to a combination of the actions that belong to {y1, . . . , y|N |}\n(we can always do this), then using the bounds for the prediction with expert advise, we\nobtain the following bounds:\n\n1 Linear bandit, full feedback: Rn ≤\np\n2n log(nk) = O(√kn log n), which in terms\nof dependency to n is of order O(√n) that is what we expect to have.\n2 Linear bandit, bandit feedback: Rn ≤\np\n2nnk log(nk) = Ω(n), which is useless in\nterms of dependency of n as we expect to obtain O(√n) behavior.\nThe topic of this lecture is to provide bounds for the linear bandit in the bandit feedback.\nProblem Setup: Let us recap the problem formulation:\n- at time t, player chooses action at ∈A ⊂[-1, 1]k.\n- at time t, adversary chooses zt ∈Z ⊂Rk, where aT\nt zt = ⟨at, zt⟩∈[0, 1].\n- Bandit feedback: player observes ⟨at, zt⟩( rather than zt in the full feedback setup).\nLiterature: O(n3/4) regret bound has been shown in [BB04]. Later on this bound has\nbeen improved to O(n2/3) in [BK04] and [VH06] with \"Geometric Hedge algorithm\", which\nwe will describe and analyze below. We need the following assumption to show the results:\nAssumption: There exist δ such that δe1, . . . , δek ∈A. This assumption guarantees that\nA has full-dimension around zero.\nWe also discretize A with a 1 -net of size Cnk and only consider the resulting discrete set\nn\nand denote it by A, where |A| ≤(3n)k. All we need to do is to bound\nRn = E\n\" n\nX\nT\nAt zt\n#\nn\n-min\nX\nT\na zt.\na∈A\nt=1\nt=1\nFor any t and a, we define\nt-1\nexp\nη\nt(\n\n-\nz\ns=1 ˆT\ns a\np a) =\n,\nt-1\na∈A exp\nP\n\n-η\nz\ns=1 ˆTs a\n\nwhere η is a parameter (that we will\nP\nchoose later) an\nP\nd zˆt is defined to incorporate the idea of\nexploration versus exploitation. The algorithm which is termed Geometric Hedge Algorithm\nis as follows:\nAt time t we have\n- Exploitation: with probability 1 -γ draw at according to pt and let zˆt = 0.\n- Exploration:\nwith probability\nγ let at = δej for some 1\nk\n≤j ≤k and zˆt =\nk\nj)\n2 ⟨\n(\na\nk\nt, zt a\nγ\n⟩t =\nz\ne\nδ\nγ\nt\nj.\nNote that δ is the the parameter that we have by assumption on the set A, and η and γ\nare the parameters of the algorithm that we shall choose later.\nTheorem: Using Geometric Hedge algorithm for linear bandit with bandit feedback,\nwith γ =\n=\ng\n1/3 and η\nq\nlo n\n4/3, we have\nn\nkn\nE[Rn] ≤\n2/3\nCn\np\nlog\n3/2\nn k\n.\n\nProof. Let the overall distribution of at be qt defined as qt = (1 -γ)pt + γU, where U is a\nuniform distribution over the set {δe1, . . . , δek}. Under this distribution, zˆt is an unbiased\nestimator of zt, i.e.,\nk\nγ k\n(\nEat∼qt[zˆt] = 0(1 -\nj)\nγ) +\nX\nzt ej = zt.\nk γ\nj=1\nfollowing the same lines of the proof that we had for analyzing exponential weight algorithm,\nwe will define\nwt =\ne p\nz\na∈A\n\nt-\nx\n-\nT\nη\na ˆs\ns=1\n!\nX\nX\n.\nWe then have\nlog\nwt+1\n\n= log\nX\nT\npt(a) exp\nηa zˆt\nwt\na∈\n-\nA\n!\n\ne-\nx≤1-x+ x\n≤\n2 log\nX\nT\n( )\n\n1 -\nˆ +\n2( T\npt a\nηa zt\nη\na zˆs)\na∈A\n!\n(\n-\nT\n= log\n\n1 +\nX\n)\n\nˆ +\n2( T\npt a\nηa zt\nη\na zˆt)\na∈A\n!\nlog(1+x)≤x\n≤\nX\nT\npt(\nT\na)\n\n-ηa zˆt +\nη (a zˆt)\na∈A\n\n.\nTaking expectation from both sides leads to\nEat∼qt\n\nwt+1\nlog\n\nwt\n\n≤-ηEat∼qt\n\"\npt(a) T\na zˆt\na∈A\n#\n\"\nX\nη2\n+\nT\nEat∼qt\nX\npt(a)(a zˆt)\na∈A\n#\n= -\n\nT\nη\nηE t∼pt at zˆt\n\n+\nE\na\na ∼q\npt(a)( T\na zˆt)\nt\nt\n\"\na\nX\n∈A\n#\nqt=(1-γ)pt+γU\nη\n=\n-\nγ\nEat∼qt\nT\nη\nat zˆ\nT\nE\nt\na\n-γ\n\n+ η\n-\n∼U\nt zˆ\nT\na\nt +\nE\nγ\nt\na\nt∼qt\n\"\npt(a)(a zˆt)\na∈A\n#\naT zt≤1\n-η\n\nηγ\nη2\n\nX\nt ≤\nT\nEat∼qt at zˆt +\n+\n∼qt\n\"X\nT\nEat\npt(a)(a zˆ\n1 -γ\n1 -\nt)\nγ\na∈A\n#\n.\nWe next, take summation of the previous relation for t = 1 up to n and use a telescopic\ncancellation to obtain\nn\nn\nη\nT\nηγ\nη2\nT\nE [log w\nE\n+ ] ≤\n[log w1] -\nE\nn\n\"X\nat zˆt\n#\n+\nn +\nE\npt(a)(a zˆ\n1 -\nt)\nγ\nγ\nt=1\n-\n\"X\nt=1 a\nX\n∈A\n#\nn\nn\nηγ\nη2\n≤E [log\n] -ηE\n\"X\nT\nT\nat zˆ\nw1\nt\n#\n+\nn +\nE\npt(a)(a zˆt)\n.\n(6.1)\nγ\nt=1\n-\n\"X\nt=1 a∈A\n#\nX\n\nNote that for all a∗∈A we have\nn\nlog(wn+1) = log\n\nn\na\n!!\nX\nexp\n-η\n∈A\nX\nT\na zˆs\ns=1\n≥-η\nX\n.\ns=\n⟨∗\na , zˆs\n⟩\nUsing E[zˆs] = zs, leads to\nn\nE [log(wn+1)] ≥-η\nX\n∗\na , zs .\n(6.2)\ns=1\n⟨\n⟩\nWe also have that\nlog(w1) = log |A| ≤2k log n.\n(6.3)\nPlugging (6.2) and (6.3) into (6.1), leads to\nγ\nη\nE[Rn] ≤\nn +\nE\n\" n\n#\nX X\npt a)( T\nn\n(\na\nt)2\nk log\nzˆ\n+\n.\n(6.4)\n1 -γ\nη\nt=1 a∈A\nn\nIt remains to control the quadratic term E\nP\np a aT z\nt=1\na∈A\nt( )(\nˆt)\n. We use the fact that\n| (j)\nzt |, | (j)\nat | ≤1 to obtain\nP\n\nE\n\" n\nn\nX X\nT\nT\npt(a)(a zˆt)\n=\np a)E\nt(\nqt[(a zˆt) ]\nt=1 a∈A\n#\nX\nt=1 a\nX\n∈A\nn\nk\n=\nX X\nγ\nk\n(j)\npt(a)\n\n(1 -γ)0 +\nX\n\n[ j\na zt ]\nk\nγ\nt=1 a∈A\nj=1\n\n(j)\nn\n|ajzt\n|≤1\n\n≤\nX X\nk2\n(a)\n\nt\n\nk\np\n= n\n.\nγ\nγ\nt=1 a∈A\nPlugging this bound into (6.4), we have\nη\nk2\n2k log n\nE[Rn] ≤γn +\nn\n+\n.\nγ\nη\nLetting\nlog n\nγ =\n/3 and η =\nn1\nq\nkn4/3 leads to\nE[\n3/2\n2/3\nRn] ≤Ck\nn\np\nlog n.\nLiterature: The bound we just proved has been improved in [VKH07] where they show\nO(d3/2√n log n) bound with a better exploration in the algorithm. The exploration that we\nused in the algorithm was coordinate-wise. The key is that we have a linear problem and we\ncan use better tools from linear regression such as least square estimation. However, we will\ndescribe a slightly different approach in which we never explore and the exploration is com-\npletely done with the exponential weighting. This approach also gives a better performance\nin terms of the dependency on k. In particular, we obtain the bound O(d√n log n) which\ncoincides with the bound recently shown in [BCK 12] using a John's ellipsoid.\n\n-1\nTheorem: Let Ct = Eat∼qt[a aT\nt ], zˆt = (aT\nt\nt zt)Ct\nat, and γ = 0 (so that pt = qt). Using\nGeometric Hedge algorithm with η = 2\nq\nlog n for linear bandit with bandit feedback\nn\nleads to\nE[Rn] ≤CK\np\nn log n.\nProof. We follow the same lines of the proof as the previous theorem to obtain (6.4). Note\nthat the only fact that we used in order to obtain (6.4) is unbiasedness, i.e., E[zˆt] = zt,\nwhich holds here as well since\nE[zˆ\n-\nE\nt] =\n[\nT\nCt\natat zt] =\n-1\nC\nE\nt\n[\nT\natat ]zt = zt.\nNote that we can use pseudo-inverse instead of inverse so that invertibility is not an issue.\nTherefore, rewriting (6.4) with γ = 0, we obtain\n\" n\nη\nX X\nT\n2k log n\nE[Rn] ≤\nEat∼pt\npt(a)(a zˆt)\n∈\n#\n+\n.\nη\nt=1 a A\nWe now bound the quadratic term as follows\nn\nn\nT\nT\nEat∼pt\n\"X X\npt(a)(a zˆt)\n#\n=\nX X\np a)E\nt(\nat∼pt (a zˆt)\nt=1 a∈A\nt=1 a∈A\n\nCT\nt =C\n-1\nn\nn\nT\nt, zˆt=(at zt)Ct\nat\n=\nX\na) T\np\na E\nt(\nzˆ\nT\ntˆ\na =\npt(\nT\nt\na) T\nz\na E (\n-1\nT\n-1\nat zt) Ct\natat Ct\na\nt=1 a\nX\n∈A\n\nX\nt=1 a\nT\nX\n∈A\nn\nn\n|a\n\nt zt|≤1\nE\n\nX X\nT\n-1\nT\n-1\n[a\n≤\ntaT\np (a)\nt ]=Ct\nT\n-1\na C\nE\nt\nt\natat\nCt\na\n=\npt(a)a Ct\na\nt=1 a∈A\n\nX\nt=1 a\nX\n∈A\nn\nn\nX X\ntr(AB)=tr(BA)\n=\n( )tr( T\n-1\np\nt\nr(\n-\nt a\na C\na)\n=\nX X\npt(a)t\nT\nCt\naa )\nt=1 a∈A\nt=1 a∈A\nn\nn\nn\n=\nX\ntr(\n-1\nC\nE\nt\na∼pt[\nT\naa ]) =\nX\ntr(\n-1\nCt\nCt) =\ntr(Ik) = kn.\nt=1\nt=1\nX\nt=1\nPlugging this bound into previous bound yields\nη\n2k log n\nE[Rn] ≤\nnk +\n.\nη\nlog n\nLetting η = 2\nq\n, leads to E[Rn]\nn\n≤Ck√n log n.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Aden Forrow\nNov. 30, 2015\n7. BLACKWELL'S APPROACHABILITY\n7.1 Vector Losses\nDavid Blackwell introduced approachability in 1956 as a generalization of zero sum game\ntheory to vector payoffs. Born in 1919, Blackwell was the first black tenured professor at\nUC Berkeley and the seventh black PhD in math in the US.\nRecall our setup for online linear optimization. At time t, we choose an action at ∈∆K\nand the adversary chooses zt ∈Binf(1). We then get a loss l(at, zt) = ⟨at, zt⟩. In the full\ninformation case, where we observe zt and not just l(at, zt), this is the same as prediction\nwith expert advice. Exponential weights leads to a regret bound\nRn ≤\nrn\n2 log(K).\nThe setup of a zero sum game is nearly identical:\n- Player 1 plays a mixed strategy p ∈∆n.\n- Player 2 plays q ∈∆m.\n- Player 1's payoffis p⊤Mq.\nHere M is the game's payoffmatrix.\nTheorem: Von Neumann Minimax Theorem\nmax min\n⊤\np Mq = min max\n⊤\np Mq.\np∈∆n q∈∆m\nq∈∆m p∈∆n\nThe minimax is called the value of the game. Each player can prevent the other from doing\nany better than this. The minimax theorem implies that if there is a good response pq to\nany individual q, then there is a silver bullet strategy p that works for any q.\nCorollary: If ∀q ∈∆n, ∃p such that p⊤Mq ≥c, then ∃p such that ∀q, p⊤Mq ≥c.\nVon Neumann's minimax theorem can be extended to more general sets. The following\ntheorem is due to Sion (1958).\nTheorem: Sion's Minimax Theorem Let A and Z be convex, compact spaces, and\nf : A × Z →R. If f(a, ·) is upper semicontinuous and quasiconcave on Z ∀a ∈A and\n\nf(·, z) is lower semicontinuous and quasiconvex on A ∀z ∈Z, then\ninf sup f(a, z) = sup inf f(a, z).\na∈A z∈Z\nz∈\na\nZ\n∈A\n(Note - this wasn't given explicitly in lecture, but we do use it later.) Quasiconvex and\nquasiconcave are weaker conditions than convex and concave respectively.\nBlackwell looked at the case with vector losses. We have the following setup:\n- Player 1 plays a ∈A\n- Player 2 plays z ∈Z\n- Player 1's payoffis l(a, z) ∈\nd\nR\nWe suppose A and Z are both compact and convex, that l(a, z) is bilinear, and that\n∥l(a, z)∥≤R ∀a ∈A, z ∈Z. All norms in this section are Euclidean norms. Can we\ntranslate the minimax theorem directly to this new setting? That is, if we fix a set S ⊂\nd\nR ,\nand if ∀z ∃a such that l(a, z) ∈S, does there exist an a such that ∀z l(a, z) ∈S?\nNo.\nWe'll construct a counterexample.\nLet A = Z = [0, 1], l(a, z) = (a, z), and\nS = {(a, z) ∈[0, 1]2 : a = z}. Clearly, for any z ∈Z there is an a ∈A such that a = z and\nl(a, z) ∈S, but there is no a ∈A such that ∀z, a = z.\nInstead of looking for a single best strategy, we'll play a repeated game. At time t,\nplayer 1 plays at = at(a1, z1, . . . , at-1, zt-1) and player 2 plays zt = zt(a1, z1, . . . , at-1, zt-1).\nPlayer 1's average loss after n iterations is\nl n =\nn\nX\nl(at, zt)\nn t=1\nLet d(x, S) be the distance between a point x ∈\nd\nR and the set S, i.e.\nd(x, S) = inf\ns∈S ∥x -s∥.\nIf S is convex, the infimum is a minimum attained only at the projection of x in S.\nDefinition: A set S is approachable if there exists a strategy at = at(a1, z1, . . . , at-1, zt-1)\n\nsuch that limn→infd(ln, S) = 0.\nWhether a set is approachable depends on the loss function l(a, z). In our example, we can\nchoose a0 = 0 and at = zt-1 to get\nn\nl\nlim\nn = lim\nX\n(zt-1, zt) = (z , z ) ∈S.\nn→inf\nn→infn t=1\nSo this S is approachable.\n7.2 Blackwell's Theorem\nWe have the same conditions on A, Z, and l(a, z) as before.\n\nTheorem: Blackwell's Theorem Let S be a closed convex set of\nR with ∥x∥≤R\n∀x ∈S. If ∀z, ∃a such that l(a, z) ∈S, then S is approachable.\nMoreover, there exists a strategy such that\n2R\nd l\n( n, S) ≤√n\nProof. We'll prove the rate; approachability of S follows immediately. The idea here is to\ntransform the problem to a scalar one where Sion's theorem applies by using half spaces.\nSuppose we have a half space H = {x ∈\nd\nR : ⟨w, x⟩≤c} with S ⊂H. By assumption,\n∀z ∃a such that l(a, z) ∈H. That is, ∀z ∃a such that ⟨w, l(a, z)⟩≤c, or\nmax min⟨w, l(a, z)⟩≤c.\nz∈Z a∈A\nBy Sion's theorem,\nmin max⟨w, l(a, z)\na∈A z∈Z\n⟩≤c.\nSo ∃a∗\nH such that ∀z l(a, z) ∈H.\nThis works for any H containing S. We want to choose Ht so that l(at, zt) brings the\n\naverage l\n\nt closer to S than lt-1. An intuitive choice is to have the hyperplane W bounding\nH\n\nt be the separating hyperplane between S and lt-1 closest to S.\nThis is Blackwell's\n\nstrategy: let W be the hyperplane through πt ∈argminμ∈S ∥lt-1 -μ∥with normal vector\nl t-1 -πt. Then\nH = {x ∈\nd\nR : ⟨x -πt, l t-1 -πt⟩≤0}.\nFind a∗\nH and play it.\nWe need one more equality before proving convergence. The average loss can be ex-\npanded:\nt\nl t\nt-1\nt\nt\nt\nt\n=\n-1\nt\n\n(lt-1 -πt) +\n-\nπt +\nlt\nt\nt\nt\nNow we look at the distance of the average from S, using the above equation and the\ndefinition of πt+1:\nd\n(l, S)2\nl\nt\n= ∥t -πt+1∥2\n≤∥l t -πt∥2\n=\n\nt -1\n\n(lt-1 -πt) +\n(lt -πt)\nt\n\nt\nlt\n\n=\nt -12\nd\n(l\nπ\nt\nt\n\nt-1, S) + ∥\n-\n\n∥+ 2 -\n⟨lt -πt, l\nt\nt2\nt2\nt-1 -πt⟩\nSince lt ∈H, the last term is negative; since lt and πt are both bounded by R, the middle\nterm is bounded by 4R\n2 . Letting μ2\nt = t2d(l\nt, S) , we have a recurrence relation\nt\nμ2\nt ≤μ2\nt-1 + 4R2,\n=\n-\nl\n+ 1l\n\nimplying\nμ2\nn ≤4nR2.\nRewriting in terms of the distance gives the desired bound,\n2R\nd l\n( t, S) ≤√n\nNote that this proof fails for nonconvex S.\n7.3 Regret Minimization via Approachability\nConsider the case A = ∆\nK\nK, Z = Binf(1). As we showed before, exponential weights Rn ≤\nc\np\nn log(K). We can get the same dependence on n with an approachability-based strategy.\nFirst recall that\nn\nn\nRn =\nX\nl(at, zt) -min\nl(ej, zt)\nn\nn\nj\nn\nt=1\nX\nt=1\nn\nn\n= m x\n\"\na\nX\nl(at, zt)\nl(ej, zt)\nj\nn\nn\nt=1\n-\nX\nt=1\n#\nIf we define a vector average loss\nn\nl n =\nX\n(l(at, zt) -l(e1, zt), . . . , l(a\nK\nR\nt, zt)\ne\nt=1\n-l( K, zt))\nn\n∈\n,\nRn\n\nn →0 if and only if all components of ln are nonpositive. That is, we need d(\n-\nln, OK) →0,\nwhere\n-\nO\n= {x ∈\nK\nR\n: -1 ≤xi ≤0, i\nK\n∀} is the nonpositive orthant. Using Blackwell's\napproachability strategy, we get\nRn ≤d(l\n-\nn, O\nr\nK\n) ≤c\n.\nn\nK\nn\nThe K dependence is worse than exponential weights,\n√\nK instead of\nlog(K).\nHow do we find a∗\nH? As a concrete example, let K = 2. We need a\np\n∗\nH tp satisfy\n⟨\n∗\nw, l(\n∗\naH, z)⟩= ⟨w, ⟨aH, z⟩y -z⟩≤c\nfor all z. Here y is the vector of all ones. Note that c ≥0 since 0 is in S and therefore in\nH. Rearranging,\n⟨∗\naH, z⟩⟨w, y⟩≤⟨w, z⟩+ c,\nChoosing a∗\nH =\nw\nwill work; the inequality reduces to\n⟨w,y⟩\n⟨w, z⟩≤⟨w, z⟩+ c.\nApproachability in the bandit setting with only partial feedback is still an open problem.\n\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Jonathan Weed\nDec. 2, 2015\n1. POTENTIAL BASED APPROACHABILITY\nLast lecture, we saw Blackwell's celebrated Approachability Theorem, which establishes a\nprocedure by which a player can ensure that the average (vector) payoffin a repeated game\napproaches a convex set. The central idea was to construct a hyperplane separating the\n\nconvex set from the point lt\n1, the average loss so far. By projecting perpendicular to\n-\nthis hyperplane, we obtained a scalar-valued problem to which von Neumann's minimax\ntheorem could be applied. The set S is approachable as long as we can always find a \"silver\nbullet,\" a choice of action at for which the loss vector lt lies on the side of the hyperplane\ncontaining S. (See Figure 1.)\nFigure 1: Blackwell approachability\nConcretely, Blackwell's Theorem also implied the existence of a regret-minimizing algo-\nrithm for expert advice. Indeed, if we define the vector loss lt by (lt)i = l(at, zt) -l(ei, zt),\nthen the average regret at time t is equivalent to the sup-norm distance between the average\n\nloss lt and the negative orthant. Approaching the negative orthant therefore corresponds\nto achieving sublinear regret.\nHowever, this reduction yielded suboptimal rates. To bound average regret, w\nthe sup-norm distance by the Euclidean distance, which led to an extra factor of\n√e replaced\nk appear-\ning in our bound. In the sequel, we develop a more sophisticated version of approachability\nthat allows us to adapt to the geometry of our problem. (Much of what follows resem-\nbles out development of the mirror descent algorithm, though the two approaches differ in\ncrucial details.)\n1.1 Potential functions\nWe recall the setup of mirror descent, first described in Lecture 13. Mirror descent achieved\naccelerated rates by employing a potential function which was strongly convex with respect\n\nto the given norm. In this case, we seek what is in some sense the opposite: a function\nwhose gradient does not change too quickly. In particular, we make the following definition.\nDefinition: A function Φ : IRd →IR is a potential for S ∈IR if it satisfies the following\nproperties:\n- Φ is convex.\n- Φ(x) ≤0 for x ∈S.\n- Φ(y) = 0 for y ∈∂S.\n- Φ(y) -Φ(x) -⟨∇Φ(x), y -x⟩≤h x\n2∥\n-y∥2, where by abuse of notation we use\n∇Φ(x) to denote a subgradient of Φ at x.\nGiven such a function, we recall two associated notions from the mirror descent algo-\nrithm. The Bregman divergence associated to Φ is given by\nDΦ(y, x) = Φ(y) -Φ(x) -⟨∇Φ(x), y -x⟩.\nLikewise, the associated Bregman projection is\nπ(x) = argmin DΦ(y, x) .\ny∈S\nWe aim to use the function Φ as a stand-in for the Euclidean distance that we employed\nin our proof of Blackwell's theorem. To that end, the following lemma establishes several\nproperties that will allow us to generalize the notion of a separating hyperplane.\nLemma: For any convex, closed set S and z ∈S, x ∈SC, the following properties\nhold.\n- ⟨z -π(x), ∇Φ(x)⟩≤0,\n- ⟨x -π(x), ∇Φ(x)⟩≥Φ(x).\nIn particular, if Φ is positive on SC, then H := {y | ⟨y -Φ(x), ∇Φ(x)⟩= 0} is a\nseparating hyperplane.\nOur proof requires the following proposition, whose proof appears in our analysis of the\nmirror descent algorithm and is omitted here.\nProposition: For all z ∈S, it holds\n⟨∇Φ(π(x)) -∇Φ(x), π(x) -z⟩≤0 .\nProof of Lemma. Denote by π the projection π(x). The first claim follows upon expanding\nthe expression on the left-hand side as follows\n⟨z -π, ∇Φ(x)⟩= ⟨z -π, ∇Φ(x) -∇Φ(π)⟩+ ⟨z -π, ∇Φ(π)⟩.\n\nThe above Proposition implies that the first term is nonpositive. Since the function Φ is\nconvex, we obtain\n0 ≥Φ(z) ≥Φ(π) + ⟨z -π, ∇Φ(π)⟩.\nSince π lies on the boundary of S, by assumption Φ(π) = 0 and the claim follows.\nFor the second claim, we again use convexity:\nΦ(π) ≥Φ(x) + ⟨π -x, ∇Φ(x)⟩.\nSince Φ(π) = 0, the claim follows.\n1.2 Potential based approachability\nWith the definitions in place, the algorithm for approachability is essentially the same as it\nbefore we introduced the potential function. As before, we will use a projection defined by\nthe hyperplane H = {y | ⟨y -\n\nπ(lt-1), ∇Φ(lt\n= 0\nand von Neumann's minmax theorem\n-1⟩\n}\nto find a \"silver bullet\" a∗\nt such that lt = l(a∗\nt , zt) satisfies\n⟨lt -\n\nπt, ∇Φ(lt-1)⟩≤0 .\nAll that remains to do is to analyze this procedure's performance. We have the following\ntheorem.\nTheorem: If ∥l(a, z)∥≤R holds for all z ∈A, z ∈Z and all assumptions above are\nsatisfied, then\n4R2h log n\n\nΦ(ln) ≤\n.\nn\nProof. The definition of the potential Φ required that Φ be upper bounded by a quadratic\nfunction. The proof below is a simple application of that bound.\nAs before, we note the identity\nl\n\nt\nt-1\nlt = lt-1 +\n.\nt\nThis expression and the definition of Φ imply.\nh\n\n≤\n\n)\n⟨\n\nΦ(lt\nΦ(lt-1) +\nlt -\n\nl\nt\n1,\n)\n-\n∇Φ(lt\n1 ⟩+\n∥l\n-\nt\nl\nt\n2 2\n-\nt-1\nt\n∥.\n\nThe last term is the easiest to control. By assumption, lt and lt\n1 are contained in a ball\n-\nof radius R, so ∥lt - lt-1∥2 ≤4R2.\nTo bound the second term, write\n⟨\n\nlt -\n\nlt\n1, ∇Φ(lt\n1)⟩=\n⟨lt -πt, ∇Φ(lt\n1) +\nπ\nl\n,\nΦ(l\n) .\nt\n-\n-\n-\n⟩\nt ⟨t\nt\n-\nt-1 ∇\nt-1 ⟩\nThe first term is nonpositive by assumption, since this is how the algorithm constructs\nthe silver bullet. By the above Lemma, the inner product in the second term is at most\n-\n\nΦ(lt-1).\nWe obtain\nt -1\n\nΦ(lt) ≤\n\nhR2\n\nΦ(lt\n)\n-1 +\n.\nt\nt2\n- l\n\nDefining ut = tΦ(lt) and rearranging, we obtain the recurrence\n2hR2\nut ≤ut-1 +\n,\nt\nSo\nn\nun =\nX\nn\nut -ut-1 ≤2hR2 X 1\nt\nt=1\nt=1\nApplying the definition of un proves the claim.\n1.3 Application to regret minimization\nWe now show that potential based approachability provides\n√\nan improved bound on regret\nminimization. Our ultimate goal is to replace the bound\nnk (which we proved last lecture)\nby √n log k (which we know to be the optimal bound for prediction with expert advice).\nWe will be able to achieve this goal up to logarithmic terms in n. (A more careful analysis\nof the potential defined below does actually yields an optimal rate.)\nRecall that Rn\n\n= d\n(ln, O\nn\nK\n-), where Rn is the cumulative regret after n rounds and O-\ninf\nK\nis the negative orthant. It is not hard to see that d\n= ∥x+∥\n, where x+ is the positive\ninf\ninf\npart of the vector x.\nWe define the following potential function:\nK\nΦ(x) =\nlog\nη\n\nK\nX\neη(xj)+\nj=1\n\n.\nThe function Φ is a kind of \"soft max\" of the\n\npositive entries\n\nof x. (Note that this definition\ndoes not agree with the use of the term soft max in the literature--the difference is the\npresence of the factor\n1 .) The terminology soft max is justified by noting that\nK\n∥x+∥\n= max(x\nlog\n+\nlog K\nlog K\nj)+ ≤max\neη(xj)\n+\n.\nη\n≤Φ(x) +\ninf\nj\nj\nη\nK\nη\nThe potential function therefore serves as an upper bound on the sup distance, up to an\nadditive logarithmic factor.\nThe function Φ defined in this way is clearly convex and zero on the negative orthant.\nTo verify that it is a potential, it remains to show that Φ can be bounded by a quadratic.\nAway from the negative orthant, Φ is twice differentiable and we can compute the\nHessian explicitly:\n∇2Φ(x) = η diag(∇Φ(x)) -η∇Φ∇Φ⊤.\nFor any vector u such that ∥u∥2 = 1, we therefore have\nK\nK\nu⊤∇2Φ(x)u = η\nX\nu2\nj(∇Φ(x))j -η(u⊤\nj\n∇Φ(x))2 ≤η\n=1\nX\nu2\nj(\nj=1\n∇Φ(x))j ≤η ,\nsince ∥u∥2 = 1 and ∥∇Φ(x)\n∥1 ≤1.\nWe conclude that ∇Φ(x) ⪯ηI, which for nonnegative x and y implies the bound\nη\nΦ(y) -Φ(x) -⟨∇Φ(x), y -x⟩≤2∥y -x∥2 .\n≤4hR2 log n .\n\nIn fact, this bound holds everywhere.\nTherefore Φ is a valid potential function for the\nnegative orthant, with h = η.\nThe above theorem then implies that we can ensure\nRn\nlog K\n4R2η log n\nlog K\n≤\n\nΦ(ln) +\n≤\n+\n.\nn\nη\nn\nη\nTo optimize this bound, we pick η =\n1 q\nn log K and obtain the bound\n2R\nlog n\nRn ≤4R\np\nn log n log K .\nAs alluded to earlier, a more careful analysis can remove the log n term. Indeed, for this\nparticular choice of Φ, we can modify the above Lemma to obtain the sharper bound\n⟨x -π(x), ∇Φ(x)⟩≥2Φ(x) .\nWhen we substitute this expression into the above proof, we obtain the recurrence\nrelation\nt\n\nΦ(lt)\n-2\nc\n≤\n\nΦ(lt-1) +\n.\nt\nt2\nThis small change is enough to prevent the appearance of log n in the final bound.\n\nReferences\n[Nem12] Arkadi Nemirovski, On safe tractable approximations of chance constraints, Euro-\npean J. Oper. Res. 219 (2012), no. 3, 707-718. MR 2898951 (2012m:90133)\n[NS06]\nArkadi Nemirovski and Alexander Shapiro, Convex approximations of chance\nconstrained programs, SIAM J. Optim. 17 (2006), no. 4, 969-996. MR 2274500\n(2007k:90077)\n[BB04]\nMcMahan, H. Brendan, and Avrim Blum. Online geometric optimization in the\nbandit setting against an adaptive adversary. Conference on Learning theory\n(COLT) 2004.\n[BCK 12] Bubeck, Sbastien, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards mini-\nmax policies for online linear optimization with bandit feedback. arXiv preprint\narXiv:1202.3079 (2012). APA\n[BK04]\nAwerbuch, Baruch, and Robert D. Kleinberg. Adaptive routing with end-to-end\nfeedback: Distributed learning and geometric approaches.Proceedings of the thirty-\nsixth annual ACM symposium on Theory of computing. ACM, 2004.\n[Bla56]\nD. Blackwell, An analog of the minimax theorem for vector payoffs, Pacific J.\nMath. 6 (1956), no. 1, 1-8\n[Sio58]\nM. Sion, On general minimax theorems. Pacific J. Math. 8 (1958), no. 1, 171-176.\n[VH06]\nDani, Varsha, and Thomas P. Hayes. Robbing the bandit: Less regret in online geo-\nmetric optimization against an adaptive adversary. Proceedings of the seventeenth\nannual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and\nApplied Mathematics, 2006.\n[VKH07] Dani, Varsha, Sham M. Kakade, and Thomas P. Hayes, The price of bandit in-\nformation for online optimization, Advances in Neural Information Processing\nSystems. 2007.\n\n[Bub15] S ebastien Bubeck, Convex optimization: algorithms and complexity, Now Publish-\ners Inc., 2015.\n\n[DGL96] L. Devroye, L. Gyo rfi, and G. Lugosi, A probabilistic theory of pattern recognition,\nApplications of Mathematics (New York), vol. 31, Springer-Verlag, New York,\n1996. MR MR1383093 (97d:68196)\n[HTF09] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statis-\ntical learning, second ed., Springer Series in Statistics, Springer, New York, 2009,\nData mining, inference, and prediction. MR 2722294 (2012d:62081)\n\n[Kol11] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse\n\nrecovery problems. Ecole d'Et e de Probabilit es de Saint-Flour XXXVIII-2008. Lec-\nture Notes in Mathematics 2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011.\n[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of\nErgebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and\nRelated Areas (3)]. Springer-Verlag, Berlin, 1991. Isoperimetry and processes.\n\n[Kea90] Michael J Kearns. The computational complexity of machine learning. PhD thesis,\nHarvard University, 1990.\n\n[Zha04] Tong Zhang. Statistical behavior and consistency of classification methods based\non convex risk minimization. Ann. Statist., 32(1):56-85, 2004.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 1 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/7d229ed907a6d1410a3736c98a9d78d8_MIT18_657F15_L1.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Philippe Rigollet\nSep. 9, 2015\n1. WHAT IS MACHINE LEARNING (IN THIS COURSE)?\nThis course focuses on statistical learning theory, which roughly means understanding the\namount of data required to achieve a certain prediction accuracy. To better understand\nwhat this means, we first focus on stating some differences between statistics and machine\nlearning since the two fields share common goals.\nIndeed, both seem to try to use data to improve decisions. While these fields have evolved\nin the same direction and currently share a lot of aspects, they were at the beginning quite\ndifferent. Statistics was around much before machine learning and statistics was already\na fully developed scientific discipline by 1920, most notably thanks to the contributions of\nR. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for\nstatistical inference. However, MLE requires essentially knowing the probability distribution\nfrom which the data is draw, up to some unknown parameter of interest. Often, the unknown\nparameter has a physical meaning and its estimation is key in better understanding some\nphenomena. Enabling MLE thus requires knowing a lot about the data generating process:\nthis is known as modeling. Modeling can be driven by physics or prior knowledge of the\nproblem. In any case, it requires quite a bit of domain knowledge.\nMore recently (examples go back to the 1960's) new types of datasets (demographics,\nsocial, medical,. . . ) have become available. However, modeling the data that they contain\nis much more hazardous since we do not understand very well the input/output process\nthus requiring a distribution free approach. A typical example is image classification where\nthe goal is to label an image simply from a digitalization of this image. Understanding\nwhat makes an image a cat or a dog for example is a very complicated process. However,\nfor the classification task, one does not need to understand the labelling process but rather\nto replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1).\ninput X\noutput Y\nblackbox\ny = f(x) + ε\ninput X\noutput Y\nFigure 1: The machine learning blackbox (left) where the goal is to replicate input/output\npairs from past observations, versus the statistical approach that opens the blackbox and\nmodels the relationship.\nThese differences between statistics and machine learning have receded over the last\ncouple of decades. Indeed, on the one hand, statistics is more and more concerned with\nfinite sample analysis, model misspecification and computational considerations. On the\nother hand, probabilistic modeling is now inherent to machine learning. At the intersection\nof the two fields, lies statistical learning theory, a field which is primarily concerned with\nsample complexity questions, some of which will be the focus of this class.\n\n2. STATISTICAL LEARNING THEORY\n2.1 Binary classification\nA large part of this class will be devoted to one of the simplest problem of statistical learning\ntheory: binary classification (aka pattern recognition [DGL96]). In this problem, we observe\n(X1, Y1), . . . , (Xn, Yn) that are n independent random copies of (X, Y ) ∈X ×{0, 1}. Denote\nby PX,Y the joint distribution of (X, Y ). The so-called feature X lives in some abstract\nspace X (think IRd) and Y ∈{0, 1} is called label. For example, X can be a collection of\ngene expression levels measured on a patient and Y indicates if this person suffers from\nobesity.\nThe goal of binary classification is to build a rule to predict Y given X using\nonly the data at hand. Such a rule is a function h : X →{0, 1} called a classifier. Some\nclassifiers are better than others and we will favor ones that have low classification error\nR(h) = IP(h(X) = Y ). Let us make some important remarks.\nFist of all, since Y ∈{0, 1} then Y has a Bernoulli distribution: so much for distribution\nfree assumptions! However, we will not make assumptions on the marginal distribution of\nX or, what matters for prediction, the conditional distribution of Y given X. We write,\nY |X ∼Ber(η(X)), where η(X) = IP(Y = 1|X) = IE[Y |X] is called the regression function\nof Y onto X.\nNext, note that we did not write Y = η(X). Actually we have Y = η(X) + ε, where\nε = Y -η(X) is a \"noise\" random variable that satisfies IE[ε|X] = 0. In particular, this noise\naccounts for the fact that X may not contain enough information to predict Y perfectly.\nThis is clearly the case in our genomic example above: it not whether there is even any\ninformation about obesity contained in a patient's genotype. The noise vanishes if and only\nif η(x) ∈{0, 1} for all x ∈X. Figure 2.1 illustrates the case where there is no noise and the\nthe more realistic case where there is noise. When η(x) is close to .5, there is essentially no\ninformation about Y in X as the Y is determined essentially by a toss up. In this case, it\nis clear that even with an infinite amount of data to learn from, we cannot predict Y well\nsince there is nothing to learn. We will see what the effect of the noise also appears in the\nsample complexity.\n\nFigure 2: The thick black curve corresponds to the noiseless case where Y = η(X) ∈{0, 1}\nand the thin red curve corresponds to the more realistic case where η ∈[0, 1]. In the latter\ncase, even full knowledge of η does not guarantee a perfect prediction of Y .\nIn the presence of noise, since we cannot predict Y accurately, we cannot drive the\nclassification error R(h) to zero, regardless of what classifier h we use. What is the smallest\nvalue that can be achieved? As a thought experiment, assume to begin with that we have all\nx\nη(x)\n.5\n\nthe information that we may ever hope to get, namely we know the regression function η(·).\nFor a given X to classify, if η(X) = 1/2 we may just toss a coin to decide our prediction\nand discard X since it contains no information about Y . However, if η(X) = 1/2, we have\nan edge over random guessing: if η(X) > 1/2, it means that IP(Y = 1|X) > IP(Y = 0|X)\nor, in words, that 1 is more likely to be the correct label. We will see that the classifier\nh∗(X) = 1I(η(X) > 1/2) (called Bayes classifier) is actually the best possible classifier in\nthe sense that\nR(h∗) = inf R(h) ,\nh(·)\nwhere the infimum is taken over all classifiers, i.e. functions from X to {0, 1}. Note that\nunless η(x) ∈{0, 1} for all x ∈X (noiseless case), we have R(h∗) = 0. However, we can\nalways look at the excess risk E(h) of a classifier h defined by\nE(h) = R(h) -R(h∗) ≥0 .\nIn particular, we can hope to drive the excess risk to zero with enough observations by\nmimicking h∗accurately.\n2.2 Empirical risk\nThe Bayes classifier h∗, while optimal, presents a major drawback: we cannot compute it\nbecause we do not know the regression function η. Instead, we have access to the data\n(X1, Y1), . . . , (Xn, Yn), which contains some (but not all) information about η and thus h∗.\nIn order to mimic the properties of h∗recall that it minimizes R(h) over all h. But the\nfunction R(·) is unknown since it depends on the unknown distribution PX,Y of (X, Y ). We\nˆ\nestimate it by the empirical classification error, or simply empirical risk Rn(·) defined for\nany classifier h by\nn\nˆRn(h) =\nX\n1I(h(Xi) = Yi) .\nn i=1\nˆ\nˆ\nSince IE[1I(h(Xi) = Yi)] = IP(h(Xi) = Yi) = R(h), we have IE[Rn(h)] = R(h) so Rn(h) is\nan unbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have\nˆ\nˆ\nRn(h) →R(h) as n →inf, almost surely. This indicates that if n is large enough, Rn(h)\nshould be close to R(h).\nAs a result, in order to mimic the performance of h∗, let us use the empirical risk\nˆ\nˆ\nminimizer (ERM) h defined to minimize Rn(h) over all classifiers h. This is an easy enough\nˆ\nˆ\ntask: define h such h(Xi) = Yi for all i = 1, . . . , n and h(x) = 0 if x ∈/ {X1, . . . , Xn}. We\nˆ\nˆ\nhave Rn(h) = 0, which is clearly minimal. The problem with this classifier is obvious: it\ndoes not generalize outside the data. Rather, it predicts the label 0 for any x that is not in\nˆ\nˆ\nthe data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0.\nˆ\nIn particular it is unlikely that IE[R(h)] will be small.\n\nImportant Remark: Recall that R(h) = IP(h(X) = Y ).\nˆ\nˆ\nˆ\nIf h(·) = h({(X1, Y1), . . . , (Xn, Yn)} ; ·) is constructed from the data, R(h) denotes\nthe conditional probability\nˆ\nˆ\nR(h) = IP(h(X) = Y |(X1, Y1), . . . , (Xn, Yn)) .\nˆ\nˆ\nrather than IP(h(X) = Y ). As a result R(h) is a random variable since it depends on the\nrandomness of the data (X1, Y1), . . . , (Xn, Yn). One way to view this is to observe that\nˆ\nwe compute the deterministic function R(·) and then plug in the random classifier h.\nThis problem is inherent to any method if we are not willing to make any assumption\non the distribution of (X, Y ) (again, so much for distribution freeness!). This can actually\nbe formalized in theorems, known as no-free-lunch theorems.\nTheorem:\nˆ\nFor any integer n ≥1, any classifier h built from (X1, Y1), . . . , (Xn, Yn) and\nany ε > 0, there exists a distribution PX,Y for (X, Y ) such that R(h∗) = 0 and\nˆ\nIER(hn) ≥1/2 -ε .\nTo be fair, note that here the distribution of the pair (X, Y ) is allowed to depend on\nn which is cheating a bit but there are weaker versions of the no-free-lunch theorem that\nessentially imply that it is impossible to learn without further assumptions.\nOne such\ntheorem is the following.\nTheorem:\nˆ\nFor any classifier h built from (X1, Y1), . . . , (Xn, Yn) and any sequence\n{an}n > 0 that converges to 0, there exists a distribution PX,Y for (X, Y ) such that\nR(h∗) = 0 and\nˆ\nIER(hn) ≥an ,\nfor all n ≥1\nIn the above theorem, the distribution of (X, Y ) is allowed to depend on the whole sequence\n{an}n > 0 but not on a specific n. The above result implies that the convergence to zero of\nthe classification error may be arbitrarily slow.\n2.3 Generative vs discriminative approaches\nBoth theorems above imply that we need to restrict the distribution PX,Y of (X, Y ). But\nisn't that exactly what statistical modeling is? The is answer is not so clear depending on\nhow we perform this restriction. There are essentially two schools: generative which is the\nstatistical modeling approach and discriminative which is the machine learning approach.\nGenerative: This approach consists in restricting the set of candidate distributions PX,Y .\nThis is what is done in discriminant analysis1where it is assumed that the condition dis-\n1Amusingly, the generative approach is called discriminant analysis but don't let the terminology fool\nyou.\n\ntributions of X given Y (there are only two of them: one for Y = 0 and one for Y = 1) are\nGaussians on X = IRd (see for example [HTF09] for an overview of this approach).\nDiscriminative: This is the machine learning approach. Rather than making assumptions\ndirectly on the distribution, one makes assumptions on what classifiers are likely to perform\ncorrectly. In turn, this allows to eliminate classifiers such as the one described above and\nthat does not generalize well.\nWhile it is important to understand both, we will focus on the discriminative approach\nin this class. Specifically we are going to assume that we are given a class H of classifiers\nsuch that R(h) is small for some h ∈H.\n2.4 Estimation vs. approximation\nAssume that we are given a class H in which we expect to find a classifier that performs well.\nThis class may be constructed from domain knowledge or simply computational convenience.\nˆ\nWe will see some examples in the class. For any candidate classifier hn built from the data,\nwe can decompose its excess risk as follows:\nˆ\nˆ\nˆ\nE(hn) = R(hn) -R(h∗) = R(hn) -inf R(h) + inf R(h) -R(h∗) .\nh∈H\nh∈H\n|\nestimat\n{\nio\nz\nn error\n}\n|\napproxim\n{\na\nz\ntion error\n}\nOn the one hand, estimation error accounts for the fact that we only have a finite\namount of observations and thus a partial knowledge of the distribution PX,Y . Hopefully\nwe can drive this error to zero as n →inf. But we already know from the no-free-lunch\ntheorem that this will not happen if H is the set of all classifiers. Therefore, we need to\ntake H small enough. On the other hand, if H is too small, it is unlikely that we will\nfind classifier with performance close to that of h∗.\nA tradeoffbetween estimation and\napproximation can be made by letting H = Hn grow (but not too fast) with n.\nFor now, assume that H is fixed. The goal of statistical learning theory is to understand\nhow the estimation error drops to zero as a function not only of n but also of H. For the\nfirst argument, we will use concentration inequalities such as Hoeffding's and Bernstein's\ninequalities that allow us to control how close the empirical risk is to the classification error\nby bounding the random variable\n\nn\n\nX\n1I(h(X\n(h\n\ni) = Yi) -IP\n(X) = Y )\n\nn\n\ni=1\n\nwith high probability. More generally we will be interested in results that allow to quantify\nhow close the average of independent and identically distributed (i.i.d) random variables is\nto their common expected value.\nˆ\nˆ\nˆ\nIndeed, since by definition, we have Rn(h) ≤Rn(h) for all h ∈H, the estimation error\n\ncan be controlled as follows. Define h ∈H to be any classifier that minimizes R(·) over H\n(assuming that such a classifier exist).\nˆ\nˆ\n\nR(hn) -inf R(h) = R(hn) -R(h)\nh∈H\nˆ\nˆ\nˆ\n\nˆ\nˆ\n\n= Rn(hn) -Rn(h) +R(hn) -Rˆn(h ) + Rˆ\nn\nn(h) -R(h)\n|\n≤\n{z\n}\n≤\nˆ\nˆ\nˆ\nˆ\n\nRn(hn) -R(hn)\n+\nRn(h) -R(h)\n.\n\nˆ\n\nSince h is deterministic, we can use a concentration inequality to control\nRn(h) -R(h)\n.\nHowever,\nn\nˆ\nˆ\nˆ\nRn(hn) =\nX\n1I(hn(Xi) = Yi)\nn i=1\nis not\nˆ\nthe average of independent random variables since hn depends in a complicated\nmanner on all of the pairs (Xi, Yi), i = 1, . . . , n. To overcome this limitation, we often use\nˆ\na blunt, but surprisingly accurate tool: we \"sup out\" hn,\nˆ\nˆ\nˆ\nˆ\nˆ\nˆ\nRn(hn) -R(hn)\n≤sup\n\nh∈\nRn(hn) -R(hn)\n\nH\n.\nControlling this supremum falls in the scope of suprema of empirical processes that we will\nstudy in quite a bit of detail. Clearly the supremum is smaller as H is smaller but H should\nbe kept large enough to have good approximation properties. This is the tradeoffbetween\napproximation and estimation. It is also know in statistics as the bias-variance tradeoff.\nReferences\n[DGL96] L. Devroye, L. Gy orfi, and G. Lugosi, A probabilistic theory of pattern recognition,\nApplications of Mathematics (New York), vol. 31, Springer-Verlag, New York,\n1996. MR MR1383093 (97d:68196)\n[HTF09] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statis-\ntical learning, second ed., Springer Series in Statistics, Springer, New York, 2009,\nData mining, inference, and prediction. MR 2722294 (2012d:62081)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 2 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/86f311c7073869c5e0c199008787d5c9_MIT18_657F15_L2.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Jonathan Weed\nSep. 14, 2015\nPart I\nStatistical Learning Theory\n1. BINARY CLASSIFICATION\nIn the last lecture, we looked broadly at the problems that machine learning seeks to solve\nand the techniques we will cover in this course. Today, we will focus on one such problem,\nbinary classification, and review some important notions that will be foundational for the\nrest of the course.\nOur present focus on the problem of binary classification is justified because both binary\nclassification encompasses much of what we want to accomplish in practice and because the\nresponse variables in the binary classification problem are bounded. (We will see a very\nimportant application of this fact below.) It also happens that there are some nasty surprises\nin non-binary classification, which we avoid by focusing on the binary case here.\n1.1 Bayes Classifier\nRecall the setup of binary classification: we observe a sequence (X1, Y1), . . . , (Xn, Yn) of n\nindependent draws from a joint distribution PX,Y . The variable Y (called the label) takes\nvalues in {0, 1}, and the variable X takes values in some space X representing \"features\" of\nthe problem. We can of course speak of the marginal distribution PX of X alone; moreover,\nsince Y is supported on {0, 1}, the conditional random variable Y |X is distributed according\nto a Bernoulli distribution. We write Y |X ∼Bernoulli(η(X)), where\nη(X) = IP(Y = 1|X) = IE[Y |X].\n(The function η is called the regression function.)\nWe begin by defining an optimal classifier called the Bayes classifier. Intuitively, the\nBayes classifier is the classifier that \"knows\" η--it is the classifier we would use if we had\nperfect access to the distribution Y |X.\nDefinition: The Bayes classifier of X given Y , denoted h∗, is the function defined by the\nrule\n(\nh\nif η x) > 1/2\n∗(x) =\nif η(x) ≤1/2.\nIn other words, h∗(X) = 1 whenever IP(Y = 1|X) > IP(Y = 0|X).\nOur measure of performance for any classifier h (that is, any function mapping X to\n{0, 1}) will be the classification error: R(h) = IP(Y = h(X)). The Bayes risk is the value\nR∗= R(h∗) of the classification error associated with the Bayes classifier. The following\ntheorem establishes that the Bayes classifier is optimal with respect to this metric.\n\nTheorem: For any classifier h, the following identity holds:\nR(h) -R(h∗) =\nZ\n|2η(x) -1| Px(dx) = IEX[|2η(X) -1|1(h(X) = h∗(X))]\n(1.1)\nh=h∗\nWhere h = h∗is the (measurable) set {x ∈X | h(x) = h∗(x)}.\nIn particular, since the integrand is nonnegative, the classification error R∗of the\nBayes classifier is the minimizer of R(h) over all classifiers h.\nMoreover,\nR(h∗) = IE[min(η(X), 1 -η(X))] ≤\n.\n(1.2)\nProof. We begin by proving Equation (1.2). The definition of R(h) implies\nR(h) = IP(Y = h(X)) = IP(Y = 1, h(X) = 0) + IP(Y = 0, h(X) = 1),\nwhere the second equality follows since the two events are disjoint. By conditioning on X\nand using the tower law, this last quantity is equal to\nIE[IE[1(Y = 1, h(X) = 0)|X]] + IE[IE[1(Y = 0, h(X) = 1)|X]]\nNow, h(X) is measurable with respect to X, so we can factor it out to yield\nIE[1(h(X) = 0)η(X) + 1(h(X) = 1)(1 -η(X))]],\n(1.3)\nwhere we have replaced IE[Y |X] by η(X).\nIn particular, if h = h∗, then Equation 1.3 becomes\nIE[1(η(X) ≤1/2)η(X) + 1(η(x) > 1/2)(1 -η(X))].\nBut η(X) ≤1/2 implies η(X) ≤1 -η(X) and conversely, so we finally obtain\nR(h∗) = IE[1(η(X) ≤1/2)η(X) + 1(η(x) > 1/2)(1 -η(X))]\n= IE[(1(η(X) ≤1/2) + 1(η(x) > 1/2)) min(η(X), 1 -η(X))]\n= IE[min(η(X), 1 -η(X))],\nas claimed. Since min(η(X), 1 -η(X)) ≤1/2, its expectation is also certainly at most 1/2\nas well.\nNow, given an arbitrary h, applying Equation 1.3 to both h and h∗yields\nR(h) -R(h∗) = IE[1(h(X) = 0)η(X) + 1(h(X) = 1)(1 -η(X))]\n-1(h∗(X) = 0)η(X) + 1(h∗(X) = 1)(1 -η(X))]],\nwhich is equal to\nIE[(1(h(X) = 0) -1(h∗(X) = 0))η(X) + (1(h(X) = 1) -1(h∗(X) = 1))(1 -η(X))].\nSince h(X) takes only the values 0 and 1, the second term can be rewritten as -(1(h(X) =\n0) -1(h∗(X) = 0)). Factoring yields\nIE[(2η(X) -1)(1(h(X) = 0) -1(h∗(X) = 0))].\n\nThe term 1(h(X) = 0) -1(h∗(X) = 0) is equal to -1, 0, or 1 depending on whether h\nand h∗agree. When h(X) = h∗(X), it is zero. When h(X) = h∗(X), it equals 1 whenever\nh∗(X) = 0 and -1 otherwise. Applying the definition of the Bayes classifier, we obtain\nIE[(2η(X) -1)1(h(X) = h∗(X)) sign(η -1/2)] = IE[|2η(X) -1|1(h(X) = h∗(X))],\nas desired.\nWe make several remarks. First, the quantity R(h) -R(h∗) in the statement of the\ntheorem above is called the excess risk of h and denoted E(h). (\"Excess,\" that is, above\nthe Bayes classifier.) The theorem implies that E(h) ≥0.\nSecond, the risk of the Bayes classifier R∗equals 1/2 if and only if η(X) = 1/2 almost\nsurely. This maximal risk for the Bayes classifier occurs precisely when Y \"contains no\ninformation\" about the feature variable X. Equation (1.1) makes clear that the excess risk\nweighs the discrepancy between h and h∗according to how far η is from 1/2. When η is\nclose to 1/2, no classifier can perform well and the excess risk is low. When η is far from\n1/2, the Bayes classifier performs well and we penalize classifiers that fail to do so more\nheavily.\nAs noted last time, linear discriminant analysis attacks binary classification by putting\nsome model on the data. One way to achieve this is to impose some distributional assump-\ntions on the conditional distributions X|Y = 0 and X|Y = 1.\nWe can reformulate the Bayes classifier in these terms by applying Bayes' rule:\nIP(X = x Y = 1)IP(Y = 1)\nη(x) = IP(Y = 1\n|\n|X = x) =\n.\nIP(X = x|Y = 1)IP(Y = 1) + IP(X = x|Y = 0)IP(Y = 0)\n(In general, when PX is a continuous distribution, we should consider infinitesimal proba-\nbilities IP(X ∈dx).)\nAssume that X|Y = 0 and X|Y = 1 have densities p0 and p1, and IP(Y = 1) = π is\nsome constant reflecting the underlying tendency of the label Y . (Typically, we imagine\nthat π is close to 1/2, but that need not be the case: in many applications, such as anomaly\ndetection, Y = 1 is a rare event.) Then h∗(X) = 1 whenever η(X) ≥1/2, or, equivalently,\nwhenever\np1(x)\n1 -π.\np0(x) ≥\nπ\nWhen π = 1/2, this rule amounts to reporting 1 or 0 by comparing the densities p1\nand p0. For instance, in Figure 1, if π = 1/2 then the Bayes classifier reports 1 whenever\np1 ≥p0, i.e., to the right of the dotted line, and 0 otherwise.\nOn the other hand, when π is far from 1/2, the Bayes classifier is weighed towards the\nunderlying bias of the label variable Y .\n1.2 Empirical Risk Minimization\nThe above considerations are all probabilistic, in the sense that they discuss properties of\nsome underlying probability distribution. The statistician does not have access to the true\nprobability distribution PX,Y ; she only has access to i.i.d. samples (X1, Y1), . . . , (Xn, Yn).\nWe consider now this statistical perspective. Note that the underlying distribution PX,Y\nstill appears explicitly in what follows, since that is how we measure our performance: we\njudge the classifiers we produced on future i.i.d. draws from PX,Y .\n\nFigure 1: The Bayes classifier when π = 1/2.\nGiven data Dn = {\nˆ\n(X1, Y1), . . . , (Xn, Yn)}, we build a classifier hn(X), which is random\nin two senses: it is a function of a random variable X and also depends implicitly on the\nˆ\nrandom data Dn. As above, we judge a classifier according to the quantity E(hn). This is\na random variable: though we have integrated out X, the excess risk still depends on the\ndata Dn. We therefore will consider bounds both on its expected value and bounds that\nˆ\nhold in high probability. In any case, the bound E(hn) ≥0 always holds. (This inequality\ndoes not merely hold \"almost surely,\" since we proved that R(h) ≥R(h∗) uniformly over\nall choices of classifier h.)\nLast time, we proposed two different philosophical approaches to this problem.\nIn\nparticular, generative approaches make distributional assumptions about the data, attempt\nto learn parameters of these distributions, and then plug the resulting values into the model.\nThe discriminative approach--the one taken in machine learning--will be described in great\ndetail over the course of this semester. However, there is some middle ground, which is worth\nmentioning briefly. This middle ground avoids making explicit distributional assumptions\nabout X while maintaining some of the flavor of the generative model.\nThe central insight of this middle approach is the following: since by definition h∗(x) =\nˆ\n1(η(X) > 1/2), we estimate η by some ηˆn and thereby produce the estimator hn =\n1(ηˆn(X) > 1/2). The result is called a plug-in estimator.\nOf course, achieving good performance with a plug-in estimator requires some assump-\ntions.\n(No-free-lunch theorems imply that we can't avoid making an assumption some-\nwhere!) One possible assumption is that η(X) is smooth; in that case, there are many\nnonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet\nbases, etc.).\nWe could also assume that η(X) is a function of a particular form. Since η(X) is only\nsupported on [0, 1], standard linear models are generally inapplicable; rather, by applying\nthe logit transform we obtain logistic regression, which assumes that η satisfies an identity\nof the form\nlog\n\nη(X)\n1 -η(X)\n\n= θT X.\nPlug-in estimators are called \"semi-paramteric\" since they avoid making any assumptions\nabout the distribution of X. These estimators are widely used because they perform fairly\nwell in practice and are very easy to compute. Nevertheless, they will not be our focus here.\nIn what follows, we focus here on the discriminative framework and empirical risk min-\nimization. Our benchmark continues to be the risk function R(h) = IE1(Y = h(X)), which\n\nis clearly not computable based on the data alone; however, we can attempt to use a na ıve\nstatistical \"hammer\" and replace the expectation with an average.\nDefinition: The empirical risk of a classifier h is given by\nn\nˆRn(h) =\nX\n1(Yi = h(Xi)).\nn i=1\nMinimizing the empirical risk over the family of all classifiers is useless, since we can\nalways minimize the empirical risk by mimicking the data and classifying arbitrarily other-\nwise. We therefore limit our attention to classifiers in a certain family H.\nˆ\nDefinition: The Empirical Risk Minimizer (ERM) over H is any element1 herm of the set\nˆ\nargminh\nRn(h).\n∈H\nIn order for our results to be meaningful, the class H must be much smaller than the\nˆ\nspace of all classifiers. On the other hand, we also hope that the risk of herm will be close\nto the Bayes risk, but that is unlikely if H is too small. The next section will give us tools\nfor quantifying this tradeoff.\n1.3 Oracle Inequalities\nAn oracle is a mythical classifier, one that is impossible to construct from data alone but\n\nwhose performance we nevertheless hope to mimic. Specifically, given H we define h to be\nan element of argminh\nR(h)--a classifier in\n∈H\nH that minimizes the true risk. Of course,\n\nwe cannot determine h, but we can hope to prove a bound of the form\nˆ\nR(h) ≤\n\nR(h) + something small.\n(1.4)\n\nSince h is the best minimizer in H given perfect knowledge of the distribution, a bound of\nˆ\nthe form given in Equation 1.4 would imply that h has performance that is almost best-in-\nclass. We can also apply such an inequality in the so-called improper learning framework,\nˆ\nwhere we allow h to lie in a slightly larger class H′\nˆ\n⊃H; in that case, we still get nontrivial\n\nguarantees on the performance of h if we know how to control R(h)\nThere is a natural tradeoffbetween the two terms on the right-hand side of Equation 1.4.\nWhen H\n\nis small, we expect the performance of the oracle h to suffer, but we may hope\n\nto approximate h quite closely.\n(Indeed, at the limit where H is a single function, the\n\"something small\" in Equation 1.4 is equal to zero.) On the other hand, as H grows the\noracle will become more powerful but approximating it becomes more statistically difficult.\n(In other words, we need a larger sample size to achieve the same measure of performance.)\nˆ\nSince R(h) is a random variable, we ultimately want to prove a bound in expectation\nor tail bound of the form\nˆ\nIP(R(h) ≤\n\nR(h) + ∆n,δ(H)) ≥1 -δ,\nwhere ∆n,δ(H) is some explicit term depending on our sample size and our desired level of\nconfidence.\n1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classifier\nˆ\nˆ\nˆ\nh satisfying Rn(h) ≤infh\nR\n∈H\nn(h) + ε.\n\nIn the end, we should recall that\nE ˆ\nˆ -\n∗\nˆ -\n\n(h) = R(h)\nR(h ) = (R(h)\nR(h)) + (R(h) -R(h∗)).\nThe second term in the above equation is the approximation error, which is unavoidable\nonce we fix the class H. Oracle inequalities give a means of bounding the first term, the\nstochastic error.\n1.4 Hoeffding's Theorem\nOur primary building block is the following important result, which allows us to understand\nhow closely the average of random variables matches their expectation.\nTheorem (Hoeffding's Theorem): Let X1, . . . , Xn be n independent random vari-\nables such that Xi ∈[0, 1] almost surely.\nThen for any t > 0,\nIP\n\nn\n1 X\nXi\nIEXi\nn i=1\n-\n\n>\n!\n\nt\n≤2e-2nt .\nIn other words, deviations from\n\nthe mean deca\n\ny exponentially fast in n and t.\nProof. Define centered random variables Zi = Xi -IEXi. It suffices to show that\n1 X\n\n≤\n-2nt2\nIP\nZi > t\ne\n,\nn\nsince the lower tail bound follows analogously. (Exercise!)\nWe apply Chernoffbounds. Since the exponential function is an order-preserving bijec-\ntion, we have for any s > 0\nIP\n1 X\nZ\nstn\ni > t\n\n= IP\n\nexp\n\ns\nX\nZ\nstn\ns\nZi\ni\n]\nn\n\n> e\n\n≤e-\nIE[e\nP\n(Markov)\n= e-stn\nIE[esZi],\n(1.5)\nwhere in the last equality we have used the independence of the\nY\nZi.\nWe therefore need to control the term IE[esZi], known as the moment-generating func-\ntion of Zi. If the Zi were normally distributed, we could compute the moment-generating\nfunction analytically. The following lemma establishes that we can do something similar\nwhen the Zi are bounded.\nLemma (Hoeffding's Lemma): If Z ∈[a, b] almost surely and IEZ = 0, then\n≤\ns (b-a)\nIEesZ\ne\n.\nProof of Lemma. Consider the log-moment generating function ψ(s) = log IE[esZ], and note\nthat it suffices to show that ψ(s) ≤s2(b -a)2/8. We will investigate ψ by computing the\n\nfirst several terms of its Taylor expansion. Standard regularity conditions imply that we\ncan interchange the order of differentiation and integration to obtain\nIE[ZesZ]\nψ′(s) =\n,\nIE[esZ]\nIE[Z2esZ]IE[esZ]\nIE[ZesZ]2\nesZ\nesZ\nψ′′(s) =\n-\n= IE\n\nZ2\n\n-\n\nIE Z\nIE[esZ]2\nIE[esZ]\n\nIE[esZ]\n\n.\nSince\nesZ\nsZ integrates to 1, we can interpret ψ′′(s) as the variance of Z under the probability\nIE[e\n]\nmeasure dF =\nesZ\nsZ dIE. We obtain\nIE[e\n]\nψ′′(s) = varF(Z) = varF\n\na + b\nZ -\n\n,\nsince the variance is unaffected under shifts.\nBut |Z -a+b\n2 | ≤\nb-a almost surely since\nZ ∈[a, b] almost surely, so\nvarF\n\n+ b\nZ -\n\n≤F\n\"\na\na + b\nZ -\n#\n(b -a)2\n≤\n.\nFinally, the fundamental theorem of calculus yields\ns\nu\ns2(b\na)2\nψ(s) =\nZ\n(u du\nZ\nψ′′\n)\n-\n.\n≤\nThis concludes the proof of the Lemma.\nApplying Hoeffding's Lemma to Equation (1.5), we obtain\nIP\n1 X\nZ > t\n\n≤e-stn Y\nes2/8 = ens /8\nstn\ni\n-\n,\nn\nfor any s > 0. Plugging in s = 4t > 0 yields\nIP\n1 X\nZi > t\n\n≤e-2nt2,\nn\nas desired.\nHoeffding's Theorem implies that, for any classifier h, the bound\nlog(2/δ)\n| ˆRn(h) -R(h)| ≤\nr\n2n\nholds with probability 1 -δ. We can immediately apply this formula to yield a maximal\ninequality: if H is a finite family, i.e., H = {h1, . . . , hM}, then with probability 1 -δ/M\nthe bound\nlog\n| ˆRn(hj) -R(hj)| ≤\nr\n(2M/δ)\n2n\n\nˆ\nholds. The event that maxj |Rn(hj)-R(hj)|\nˆ\n> t is the union of the events |Rn(hj)-R(hj)| >\nt for j = 1, . . . , M, so the union bound immediately implies that\nlog(2M/δ)\nmax | ˆRn(hj) -R(hj)\nj\n| ≤\nr\n2n\nwith probability 1-δ. In other words, for such a family, we can be assured that the empirical\nrisk and the true risk are close. Moreover, the logarithmic dependence on M implies that\nwe can increase the size of the family H exponentially quickly with n and maintain the\nsame guarantees on our estimate.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 3 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/e1c43cdb5ad222e94adbdf639b12b395_MIT18_657F15_L3.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: James Hirst\nSep. 16, 2015\n1.5 Learning with a finite dictionary\nRecall from the end of last lecture our setup: We are working with a finite dictionary\nH = {h1, . . . , hM} of estimators, and we would like to understand the scaling of this problem\nwith respect to M and the sample size n. Given H, one idea is to simply try to minimize\nˆ\nthe empirical risk based on the samples, and so we define the empirical risk minimizer, herm,\nby\nˆ\nˆ\nherm ∈argmin Rn(h).\nh∈H\nˆ\nˆ\nIn what follows, we will simply write h instead of herm when possible. Also recall the\n\ndefinition of the oracle, h, which (somehow) minimizes the true risk and is defined by\nh ∈argmin R(h).\nh∈H\nˆ\n\nThe following theorem shows that, although h cannot hope to do better than h in\ngeneral, the difference should not be too large as long as the sample size is not too small\ncompared to M.\nˆ\nTheorem: The estimator h satisfies\nˆ\n\nR(h) ≤R(h) +\nr\n2 log(2M/δ)\nn\nwith probability at least 1 -δ. In expectation, it holds that\nr\n2 log(2M)\nˆ\n\nIE[R(h)] ≤R(h) +\n.\nn\nProof.\nˆ\nˆ\nˆ\nˆ\n\nFrom the definition of h, we have Rn(h) ≤Rn(h), which gives\nˆ\n\nˆ\n\nˆ\nˆ\nˆ\nR(h) ≤R(h) + [Rn(h) -R(h)] + [R(h) -Rn(h)].\nThe only term here that we need to control is the second one, but since we don't have\n\nany real information about h, we will bound it by a maximum over H and then apply\nHoeffding:\nlog(2M/δ)\nˆ\n\nˆ\nˆ\nˆ\nˆ\n[Rn(h) -R(h)] + [R(h) -Rn(h)] ≤2 max |Rn(hj) -R(hj)| ≤2\nj\nr\n2n\nwith probability at least 1 -δ, which completes the first part of the proof.\n\nTo obtain the bound in expectation, we start with a standard trick from probability\nwhich bounds a max by its sum in a slightly more clever way. Here, let {Zj}j be centered\nrandom variables, then\n\nIE max |Zj|\n=\nlog exp\nsIE max |Zj|\n\n≤\nlog IE\n\nexp\n\ns max |Zj|\nj\ns\nj\ns\nj\n\n,\nwhere the last inequality comes from applying Jensen's inequality to the convex function\nexp(·). Now we bound the max by a sum to get\n2M\n)\n≤\nlog\nX\ns2\nlog(2M\ns\nIE [exp(sZj)] ≤\nlog\n\n2M exp\n\n=\n+\n,\ns\ns\n8n\ns\n8n\nj=1\nˆ\nwhere we used Zj = Rn(hj) -R(hj) in our case and then applied Hoeffding's Lemma. Bal-\nancing terms by minimizing over s, this gives s = 2\np\n2n log(2M) and plugging in produces\n\nlog(2M)\nˆ\nIE max |Rn(hj) -R(hj)|\n≤\nj\n\nr\n,\n2n\nwhich finishes the proof.\n2. CONCENTRATION INEQUALITIES\nConcentration inequalities are results that allow us to bound the deviations of a function of\nrandom variables from its average. The first of these we will consider is a direct improvement\nto Hoeffding's Inequality that allows some dependence between the random variables.\n2.1 Azuma-Hoeffding Inequality\nGiven a filtration {Fi}i of our underlying space X, recall that {∆i}i are called martingale\ndifferences if, for every i, it holds that ∆i ∈Fi and IE [∆i|Fi] = 0. The following theorem\ngives a very useful concentration bound for averages of bounded martingale differences.\nTheorem (Azuma-Hoeffding): Suppose that {∆i}i are margingale differences with\nrespect to the filtration {Fi}i, and let Ai, Bi ∈Fi-1 satisfy Ai ≤∆i ≤Bi almost surely\nfor every i. Then\nIP\n\"\n1 X\n2n\n∆i > t\n#\n2t2\n≤exp\nn\ni\n\n-Pn\ni=1 ∥Bi -Ai∥2inf\n\n.\nIn comparison to Hoeffding's inequality, Azuma-Hoeffding affords not only the use of\nnon-uniform boundedness, but additionally requires no independence of the random vari-\nables.\nProof. We start with a typical Chernoffbound.\nIP\n\"\n#\nX\n∆i > t\n≤IE\nh\nes P ∆i\ni\ne-st = IE\ni\nh\nIE\nh\nes P ∆i|Fn-1\nii\ne-st\n\nn-1\nn-1\n= IE\nh\nes P\n∆iIE[es∆n|Fn\n1] e-st\n-\n≤IE[es P\n∆i · es (Bn-An) /8]e-st,\nwhere we have used the fact that the ∆\ni\ni, i < n, are all Fn measureable, and then applied\nHoeffding's lemma on the inner expectation.\nIteratively isolating each ∆i like this and\napplying Hoeffding's lemma, we get\nIP\n\"\nn\nX\ns2\n∆> t\n#\n≤exp\n\nX\n∥B -A ∥2\n!\ne-st\ni\ni\ni\ninf\n.\ni\ni=1\nOptimizing over s as usual then gives the result.\n2.2 Bounded Differences Inequality\nAlthough Azuma-Hoeffding is a powerful result, its full generality is often wasted and can\nbe cumbersome to apply to a given problem. Fortunately, there is a natural choice of the\n{Fi}i and {∆i}i, giving a similarly strong result which can be much easier to apply. Before\nwe get to this, we need one definition.\nDefinition (Bounded Differences Condition): Let g : X →IR and constants ci be\ngiven. Then g is said to satisfy the bounded differences condition (with constants ci) if\nsup\n|g(x , . . . , x ) -g(x , . . . , x′\nn\ni, . . . , xn)| ≤ci\nx\n′\n1,...,xn,xi\nfor every i.\nIntuitively, g satisfies the bounded differences condition if changing only one coordinate\nof g at a time cannot make the value of g deviate too far. It should not be too surprising\nthat these types of functions thus concentrate somewhat strongly around their average, and\nthis intuition is made precise by the following theorem.\nTheorem (Bounded Differences Inequality): If g : X →IR satisfies the bounded\ndifferences condition, then\n2t2\nIP [|g(X1, . . . , Xn) -IE[g(X1, . . . , Xn)| > t] ≤2 exp\n\n-P\ni c2\ni\n\n.\nProof. Let {Fi}i be given by Fi = σ(X1, . . . , Xi), and define the martingale differences\n{∆i}i by\n∆i = IE [g(X1, . . . , Xn)|Fi] -IE [g(X1, . . . , Xn)|Fi-1] .\nThen\nIP\n\"\n|\nX\n∆i| > t\n#\n= IP\n\ng(X1, . . . , Xn) -IE[g(X1, . . . , Xn)\ni\n\n> t ,\nexactly the quantity we want to bou\n\nnd. Now, note that\n\n∆i ≤IE\n\nsup g(X1, . . . , xi, . . . , Xn)|Fi\n-IE [g(X1, . . . , Xn)|Fi-1]\nxi\n\n= IE\n\nsup g(X1, . . . , xi, . . . , Xn) -g(X1, . . . , Xn)|Fi-1\nxi\n\n=: Bi.\nSimilarly,\n∆i ≥IE\n\ninf g(X1, . . . , xi, . . . , Xn) -g(X1, . . . , Xn)|Fi-1\n=: Ai.\nxi\n\nAt this point, our assumption on g implies that ∥Bi -Ai∥inf≤ci for every i, and since\nAi ≤∆i ≤Bi with Ai, Bi ∈Fi-1, an application of Azuma-Hoeffding gives the result.\n2.3 Bernstein's Inequality\nHoeffding's inequality is certainly a powerful concentration inequality for how little it as-\nsumes about the random variables. However, one of the major limitations of Hoeffding is\njust this: Since it only assumes boundedness of the random variables, it is completely obliv-\nious to their actual variances. When the random variables in question have some known\nvariance, an ideal concentration inequality should capture the idea that variance controls\nconcentration to some degree. Bernstein's inequality does exactly this.\nTheorem (Bernstein's Inequality): Let X1, . . . , Xn be independent, centered ran-\ndom variables with |X | ≤c for every i, and write σ2 = n-1\ni\ni Var(Xi) for the average\nvariance. Then\nP\nIP\n\"\n1 X\nnt2\nXi > t\n#\n≤exp\n\n-\nn\n2σ2 + 2tc\ni\n!\n.\nHere, one should think of t as being fixed and relatively small compared to n, so that\nstrength of the inequality indeed depends mostly on n and 1/σ2.\nProof. The idea of the proof is to do a Chernoffbound as usual, but to first use our\nassumptions on the variance to obtain a slightly better bound on the moment generating\nfunctions. To this end, we expand\ninf(s\nk\ninf\nX )\nskck-2\ni\nIE[esXi] = 1 + IE[sXi] + IE\n\"\n#\nX\n≤1 + Var(Xi)\nX\n,\nk!\nk!\nk=2\nk=2\nwhere we have used IE[Xk\ni ] ≤IE[X2\ni |Xi|k-2] ≤Var(X\nk-2\ni)c\n.\nRewriting the sum as an\nexponential, we get\nesc\nsXi\n-sc -1\nIE[e\n] ≤s Var(Xi)g(s),\ng(s) :=\n.\nc2s2\nThe Chernoffbound now gives\nIP\n\"\n1 X\nXi > t\n#\n≤exp\n\ninf[s2(\nX\nVar(Xi))g(s) -nst]\n!\n= exp\n\nn · inf[s2σ2g(s) -st]\n,\nn\ns>0\ns>0\ni\ni\n\nand optimizing this over s (a fun calculus exercise) gives exactly the desired result.\n\n3. NOISE CONDITIONS AND FAST RATES\nˆ\nTo measure the effectiveness of the estimator h, we would like to obtain an upper bound\nˆ\nˆ\non the excess risk E(h) = R(h) -R(h∗). It should be clear, however, that this must depend\nsignificantly on the amount of noise that we allow. In particular, if η(X) is identically equal\nˆ\nto 1/2, then we should not expect to be able to say anything meaningful about E(h) in\ngeneral. Understanding this trade-offbetween noise and rates will be the main subject of\nthis chapter.\n3.1 The Noiseless Case\nA natural (albeit somewhat na ıve) case to examine is the completely noiseless case. Here,\nwe will have η(X) ∈{0, 1} everywhere, Var(Y |X) = 0, and\nE(h) = R(h) -R(h∗) = IE[|2η(X) -1|1I(h(X) = h∗(X))] = IP[h(X) = h∗(X)].\nLet us now denote\n\nˆ\nZi = 1I(h(Xi) = Yi) -1I(h(Xi) = Yi),\n\nand write Zi = Zi -IE[Zi]. Then notice that we have\nˆ\n\n|Zi| = 1I(h(Xi) = h(Xi)),\nand\nVar(Zi) ≤IE[Z2\nˆ\n\ni ] = IP[h(Xi) = h(Xi)].\nˆ\nFor any classifier hj ∈H, we can similarly define Zi(hj) (by replacing h with hj through-\nout). Then, to set up an application of Bernstein's inequality, we can compute\nn\n1 X\n\nVar(Zi(hj)) ≤IP[hj(Xi) = h(Xi)] =: σ2\nn\nj .\ni=1\nAt this point, we will make a (fairly strong) assumption about our dictionary H, which\n\nis that h∗∈H, which further implies that h = h∗. Since the random variables Zi compare\n\nˆ\nto h, this will allow us to use them to bound E(h), which rather compares to h∗. Now,\n\napplying Bernstein (with c = 2) to the {Zi(hj)}i for every j gives\n\"\nn\n#\nX\nnt2\nδ\n\nIP\nZi(hj) > t\n≤exp\n\n-\n=\n2σ2\ni=1\nj + 4t\n!\n:\n,\nn\nM\nand a simple computation here shows that it is enough to take\ns\n2σ2\nj log(M/δ)\nt ≥max\n,\nlog(M/δ)\nn\n3n\n\n=: t0(j)\n\nfor this to hold. From here, we may use the assumption h = h∗to conclude that\nˆ\nˆ\nIP\nh\nE(h) > t0(ˆj)\ni\n≤δ,\nhˆ = h.\nj\n\nˆ\nHowever, we also know that σ2\nˆ ≤E(h), which implies that\nj\ns\nˆ\n2E(h) log(M/δ)\nˆ\nE(h) ≤max\n,\nlog(M/δ)\nn\n3n\n\nˆ\nwith probability 1 -δ, and solving for E(h) gives the improved rate\nlog(M/δ)\nˆ\nE(h) ≤2\n.\nn\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 4 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/f9bb2571a80299876669c67fd1aff897_MIT18_657F15_L4.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Cheng Mao\nSep. 21, 2015\nIn this lecture, we continue to discuss the effect of noise on the rate of the excess risk\nˆ\nˆ\nˆ\nE(h) = R(h) -R(h∗) where h is the empirical risk minimizer. In the binary classification\nmodel, noise roughly means how close the regression function η is from 1\n2. In particular, if\nη = 1 then we observe only noise, and if η ∈{0, 1} we are in the noiseless case which has\nbeen studied last time. Especially, we achieved the fast rate log M in the noiseless case by\nn\n\nassuming h∗∈H which implies that h = h∗. This assumption was essential for the proof\nand we will see why it is necessary again in the following section.\n3.2 Noise conditions\nThe noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess\nrisk is when the noise is present but can be controlled. Instead of the condition η ∈{0, 1},\nwe can control the noise by assuming that η is uniformly bounded away from 1\n2, which is\nthe motivation of the following definition.\nDefinition (Massart's noise condition): The noise in binary classification is said\nto satisfy Massart's condition with constant γ ∈(0, 1\n2] if |η(X) -1| ≥γ almost surely.\nOnce uniform boundedness is assumed, the fast rate simply follows from last proof with\nappropriate modification of constants.\nˆ\nˆ\nˆ\nTheorem: Let cE(h) denote the excess risk of the empirical risk minimizer h = herm.\nIf Massart's noise condition is satisfied with constant γ, then\nlog(M/δ)\nˆ\nE(h) ≤\nγn\nwith probability at least 1 -δ. (In particular γ = 1 gives exactly the noiseless case.)\nProof.\n\nDefine Zi(h) = 1I(h(Xi) = Yi) -1I(h(Xi) = Yi). By the assumption h = h∗and the\nˆ\nˆ\ndefinition of h = herm,\nˆ\nˆ\n\nE(h) = R(h) -R(h)\nˆ\nˆ\nˆ\n\nˆ\n\nˆ\nˆ\n\nˆ\n= Rn(h) -Rn(h) + Rn(h) -Rn(h) -R(h) -R(h)\n(3.1)\nn\nˆ\n≤\n)\n\nX ˆ\nZi(h) -IE[Zi(h ]\n\n.\n(3.2)\nn i=1\nHence it suffices to bound the deviation of P\ni Zi from its expectation. To this end, we\nhope to apply Bernstein's inequality. Since\nVar[Zi(h)] ≤IE[Z\n\ni(h) ] = IP[h(Xi) = h(Xi)],\n\nwe have that for any 1 ≤j ≤M,\nn\n1 X\n\nVar[Zi(hj)] ≤IP[hj(X) = h(X)] =: σ2\nn\nj.\ni=1\nBernstein's inequality implies that\nn\n1 X\n\nnt2\nIP\n(Zi(hj) -IE[Zi(hj)]) > t ≤exp\n-\nn\n2σ2\ni=1\nj + 2\n3t\n\nδ\n=:\n.\nM\nApplying a union bound over 1 ≤j ≤M and taking\n2σ2\nj log(M/δ) 2 log(M/δ)\nt = t0(j) := max\ns\n,\nn\n3n\n\n,\nwe get that\nn\n1 X\n(Zi(hj) -IE[Zi(hj)]) ≤t0(j)\n(3.3)\nn i=1\nfor all 1 ≤j ≤M with probability at least 1 -δ.\nˆ\nSuppose h = hˆ. It follows from (3.2) and (3.3) that with probability at least 1 -δ,\nj\nˆ\nE(h) ≤t\nˆ\n0(j).\n(Note that so far the proof is exactly the same as the noiseless case.) Since |η(X) -1\n2| ≥γ\n\na.s. and h = h∗,\nˆ\nˆ\n\nE(h) = IE[|2η(X) -1|1I(h(X) = h∗(X))] ≥2γIP[hˆ(X) = h(X)] = 2γσ2\nˆ.\nj\nj\nTherefore,\ns\nˆ\nE(h) log(M/δ) 2 log(M/δ)\nˆ\nE(h) ≤max\n,\n,\n(3.4)\nγn\n3n\nso we conclude that with probabilit\ny at least 1 -δ,\n\nlog(M/δ)\nˆ\nE(h) ≤\n.\nγn\n\nThe assumption that h = h∗was used twice in the proof. First it enables us to ignore\nthe approximation error and only study the stochastic error. More importantly, it makes\nthe excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess\nrisk to get the fast rate.\nMassart's noise condition is still somewhat strong because it assumes uniform bounded-\nness of η from 1\n2. Instead, we can allow η to be close to 1\n2 but only with small probability,\nand this is the content of next definition.\n\nDefinition (Tsybakov's noise condition or Mammen-Tsybakov noise condi-\ntion): The noise in binary classification is said to satisfy Tsybakov's condition if there\nexists α ∈(0, 1), C\n0 > 0 and t0 ∈(0, 2] such that\nα\nIP[|η(X) -\n| ≤t] ≤C\n0t\n-α\nfor all t ∈[0, t0].\nα\nIn particular, as α →1, t 1-α →0\nα, so this recovers Massart's condition with γ = t0 and\nwe have the fast rate. As α →0, t 1-α →1, so the condition is void and we have the slow\nrate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose\norder depends on α. We will see that this is indeed the case.\nLemma: Under Tsybakov's noise condition with constants α, C0 and t0, we have\nIP[h(X) = h∗(X)] ≤CE(h)α\nfor any classifier h where C = C(α, C0, t0) is a constant.\nProof. We have\nE(h) = IE[|2η(X) -1|1I(h(X) = h∗(X))]\n≥IE[|2η(X) -1|1I(|η(X) -\n| > t)1I(h(X) = h∗(X))]\n≥2tIP[|η(X) -\n| > t, h(X) = h∗(X)]\n≥2tIP[h(X) = h∗(X)] -2tIP[|η(X) -\n| ≤t]\n≥2tIP[h(X) = h∗(X)] -2C0t 1-α\nwhere Tsybakov's condition was used in the last step. Take t = cIP[h(X) = h∗(X)]\n-α\nα\nfor\nsome positive c = c(α, C0, t0) to be chosen later. We assume that c ≤t0 to guarantee that\nt ∈[0, t0]. Since α ∈(0, 1),\nE(h) ≥2cIP[h(X) = h∗(X)]1/α\n-2C c 1-α IP[h(X) = h∗\n(X)] /α\n≥cIP[h(X) = h∗(X)]1/α\nby selecting c sufficiently small depending on α and C0. Therefore\nIP[h(X) = h∗(X)] ≤\nE(h)α\ncα\nand choosing C = C(α, C0, t0) := c-α completes the proof.\nHaving established the key lemma, we are ready to prove the promised fast rate under\nTsybakov's noise condition.\n\nTheorem: If Tsybakov's noise condition is satisfied with constant α, C0 and t0, then\nthere exists a constant C = C(α, C0, t0) such that\nl\n)\nˆ\nE h) ≤C\nog(M/δ\n(\nn\n\n2-α\nwith probability at least 1 -δ.\nThis rate of excess risk parametrized by α is indeed an interpolation of the slow (α →0)\nˆ\nand the fast rate (α →1). Futhermore, note that the empirical risk minimizer h does not\ndepend on the parameter α at all! It automatically adjusts to the noise level, which is a\nvery nice feature of the empirical risk minimizer.\nProof. The majority of last proof remains valid and we will explain the difference. After\nestablishing that\nˆ\nE(h) ≤t0(ˆj),\nwe note that the lemma gives\nˆ\nσ2\n\nˆ\nˆ = IP[h(X) = h(X)] ≤CE(h)α.\nj\nIt follows that\ns\nˆ\n2CE(h)α log(M/δ) 2 log(M/δ)\nˆ\nE(h) ≤max\n,\nn\n3n\n\n2C log M\nˆ\nE(h) ≤max\nδ\n2-α\nlog(M/δ)\n,\n\n.\nn\n3n\nand thus\n4. VAPNIK-CHERVONENKIS (VC) THEORY\nThe upper bounds proved so far are meaningful only for a finite dictionary H, because if\nM = |H| is infinite all of the bounds we have will simply be infinity. To extend previous\nresults to the infinite case, we essentially need the condition that only a finite number of\nelements in an infinite dictionary H really matter. This is the objective of the Vapnik-\nChervonenkis (VC) theory which was developed in 1971.\n4.1 Empirical measure\nRecall from previous proofs (see (3.1) for example) that the key quantity we need to control\nis\nˆ\n2 sup\nRn(h) -R(h) .\nh∈H\nInstead of the union bound which would not work in th\n\ne infinite case, we seek some bound\nthat potentially depends on n and the complexity of the set H. One approach is to consider\nsome metric structure on H and hope that if two elements in H are close, then the quantity\nevaluated at these two elements are also close. On the other hand, the VC theory is more\ncombinatorial and does not involve any metric space structure as we will see.\n\nBy definition\nn\nˆRn(h) -R(h) =\nX 1I(h(Xi) = Yi) -IE[1I(h(Xi) = Yi)]\nn i=1\n\n.\nLet Z = (X, Y ) and Zi = (Xi, Yi), and let A denote the class of measurable sets in the\nsample space X × {0, 1}. For a classifier h, define Ah ∈A by\n{Zi ∈Ah} = {h(Xi) = Yi}.\nMoreover, define measures μn and μ on A by\nn\nμn(A) =\nX\n1I(Zi ∈A)\nand\nμ(A) = IP[Zi ∈A]\nn i=1\nfor A ∈A. With this notation, the slow rate we proved is just\nlog(2|A|/δ)\nˆ\nsup Rn(h) -R(h) = sup |μn(A) -μ(A)| ≤\nh∈H\nA∈A\nr\n.\n2n\nSince this is not accessible in the infinite case, we hope to use one of the concentration\ninequalities to give an upper bound. Note that μn(A) is a sum of random variables that may\nnot be independent, so the only tool we can use now is the bounded difference inequality.\nIf we change the value of only one zi in the function\nz1, . . . , zn 7→sup |μn(A) -μ(A)|,\nA∈A\nthe value of the function will differ by at most 1/n. Hence it satisfies the bounded difference\nassumption with ci = 1/n for all 1 ≤i ≤n. Applying the bounded difference inequality, we\nget that\n\nlog(2/δ)\nsup |μn(A) -μ(A)| -IE[sup |μn(A) -μ(A)|] ≤\nA∈A\nA∈A\nr\n2n\nwith probability\n\nat least 1 -δ. Note that this already preclu\n\ndes any fast rate (faster than\nn-1/2). To achieve fast rate, we need Talagrand inequality and localization techniques which\nare beyond the scope of this section.\nIt follows that with probability at least 1 -δ,\nlog(2/δ)\nsup |μn(A) -μ(A)| ≤IE[sup |μn(A) -μ(A)|] +\nA\nA∈A\nr\n.\nA∈\n2n\nWe will now focus on bounding the first term on the right-hand side. To this end, we need\na technique called symmetrization, which is the subject of the next section.\n4.2 Symmetrization and Rademacher complexity\nSymmetrization is a frequently used technique in machine learning. Let D = {Z1, . . . , Zn}\nbe the sample set. To employ symmetrization, we take another independent copy of the\nsample set D′ = {Z′\n1, . . . , Z′\nn}. This sample only exists for the proof, so it is sometimes\nreferred to as a ghost sample. Then we have\nn\nn\nμ(A) = IP[Z ∈A] = IE[\nX\n1I(Z′\ni ∈A)] = IE[\n1I(Z′\ni ∈A)|D] = IE[μ′\nn\nn\nn(A)|D]\ni=1\nX\ni=1\n\nn\nwhere μ′\nn := 1 P\ni=1 1I(Z′\ni ∈A). Thus by Jensen's inequality,\nn\nIE[sup |μn(A) -μ(A)|] = IE\n\nsup\nμn(A) -IE[μ′\nn(A)|D]\nA∈A\nA∈A\n≤IE sup IE[|μn(A) -μ′\nn(A)| |D\n\n]\n\n≤\n\nA∈A\nIE\n\nsup |μ\n′\nn(A) -μn(A)|\n\nA∈A\nn\n\n= IE\n\nsup\n\nX 1I(Z\n′\ni ∈A) -1I(Z\nA∈A n\ni ∈A)\ni=1\n\n.\nSince D′ has the same distribution of D, by sy\n\nmmetry 1I(Zi ∈A) -1I(Z′\n\ni ∈A) has the same\ndistribution as σi\n1I(Zi ∈A) -1I(Z′\ni ∈A)\n\nwhere σ1, . . . , σn are i.i.d. Rad(1\n2), i.e.\nIP[σi = 1] = IP[σi = -1] =\n,\nand σi's are taken to be independent of both samples. Therefore,\nn\nIE[sup |μn(A) -μ(A)|] ≤IE\nA A\n\nsup\n′\n∈\nA∈A\n1 X\nσi\n1I(Zi ∈A) -1I(Z\nn\ni ∈A)\ni=1\nn\n\n≤2IE\n\nsup\n\nX\nσi1I(Zi ∈A) .\nA∈A n i=1\n\n(4.5)\nUsing symmetrization we have bounded IE[sup\n\nA∈A |μn(A)-μ(A)|\n\n] by a much nicer quantity.\nYet we still need an upper bound of the last quantity that depends only on the structure\nof A but not on the random sample {Zi}. This is achieved by taking the supremum over\nall zi ∈X × {0, 1} =: Y.\nDefinition: The Rademacher complexity of a family of sets A in a space Y is defined\nto be the quantity\nn\nRn(A) =\nsup\nsup\nIE\n\nX\nσi1I(zi ∈A)\nz1,...,zn∈Y\nA∈A n i=1\n\n.\nThe Rademacher complexity of a set B ⊂I\n\nRn is defined to b\n\ne\nn\nRn(B) = IE\n\nsup\nb∈B\n\nn\nX\nσibi\ni=1\n\n.\nWe conclude from (4.5) and the definition that\nIE[sup |μn(A) -μ(A)|] ≤2Rn(A).\nA∈A\nn\nIn the definition of Rademacher complexity of a set, the quantity\ns\nn\ni=1 σibi measure\nhow well a vector b ∈B correlates with a random sign pattern {σi}. The more complex\nB is, the better some vector in B can replicate a sign pattern. In\n\npa\nP\nrticular, i\n\nf B is the\nfull hypercube [-1, 1]n, then Rn(B) = 1. However, if B ⊂[-1, 1]n contains only k-sparse\n\nvectors, then Rn(B) = k/n. Hence Rn(B) is indeed a measurement of the complexity of\nthe set B.\nThe set of vectors to our interest in the definition of Rademacher complexity of A is\nT(z) := {(1I(z1 ∈A), . . . , 1I(zn ∈A))T , A ∈A}.\nThus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these\nvectors can replicate as A ranges over A. Although the cardinality of A may be infinite,\nthe cardinality of T(z) is bounded by 2n.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 5 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/c804dde9f0fcaac2abc68696147b3ee6_MIT18_657F15_L5.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Vira Semenova and Philippe Rigollet\nSep. 23, 2015\nIn this lecture, we complete the analysis of the performance of the empirical risk mini-\nmizer under a constraint on the VC dimension of the family of classifiers. To that end, we\nwill see how to control Rademacher complexities using shatter coefficients. Moreover, we\nwill see how the problem of controlling uniform deviations of the empirical measure μn from\nthe true measure μ as done by Vapnik and Chervonenkis relates to our original classification\nproblem.\n4.1 Shattering\nRecall from the previous lecture that we are interested in sets of the form\nT(z) :=\n\n(1I(z1 ∈A), . . . , 1I(zn ∈A)), A ∈A ,\nz = (z1, . . . , zn) .\n(4.1)\nIn particular, the cardinality of T(z), i.e., the numbe\n\nr of binary patterns these vectors\ncan replicate as A ranges over A, will be of critical importance, as it will arise when\ncontrolling the Rademacher complexity. Although the cardinality of A may be infinite, the\ncardinality of T(z) is always at most 2n. When it is of the size 2n, we say that A shatters\nthe set z1, . . . , zn. Formally, we have the following definition.\nDefinition: A collection of sets A shatters the set of points {z1, z2, ..., zn}\ncard{(1I(z1 ∈A), . . . , 1I(zn ∈A)), A ∈A} = 2n .\nThe sets of points {z1, z2, ..., zn} that we are interested are realizations of the pairs Z1 =\n(X1, Y1), . . . , Zn = (Xn, Yn) and may, in principle take any value over the sample space.\nTherefore, we define the shatter coefficient to be the largest cardinality that we may obtain.\nDefinition: The shatter coefficients of a class of sets A is the sequence of numbers\n{SA(n)}n≥1, where for any n ≥1,\nSA(n) =\nsup card (1I(z1\nA), . . . , 1I(zn\nA)), A\nz1,...,zn\n{\n∈\n∈\n∈A}\nand the suprema are taken over the whole sample space.\nBy definition, the nth shatter coefficient SA(n) is equal to 2n if there exists a set {z1, z2, ..., zn}\nthat A shatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di-\nmension.\nDefinition: The Vapnik-Chervonenkis dimension, or VC-dimension of\nd\nVC\nA is the largest\ninteger d such that SA(d) = 2 . We write\n(A) = d.\n\nIf SA(n) = 2n for all positive integers n, then VC(A) := inf\nIn words, A shatters some set of points of cardinality d but shatters no set of points of\ncardinality d+1. In particular, A also shatters no set of points of cardinality d′ > d so that\nthe VC dimension is well defined.\nIn the sequel, we will see that the VC dimension will play the role similar to of cardinality,\nbut on an exponential scale. For interesting classes A such that card(A) = inf, we also may\nhave VC(A) < inf. For example, assume that A is the class of half-lines, A = {(-inf, a], a ∈\nIR} ∪{[a, inf), a ∈IR}, which is clearly infinite. Then, we can clearly shatter a set of size\n2 but we for three points z1, z2, z3, ∈IR, if for example z1 < z2 < z3, we cannot create the\npattern (0, 1, 0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros\nfollowed by ones or with ones followed by zeros but not an alternating pattern like (0, 1, 0).\nFigure 1: If A = {halflines}, then any set of size n = 2 is shattered because we can\ncreate all 2n = 4 0/1 patterns (left); if n = 3 the pattern (0, 1, 0) cannot be reconstructed:\nSA(3) = 7 < 23 (right). Therefore, VC(A) = 2.\n4.2 The VC inequality\nWe have now introduced all the ingredients necessary to state the main result of this section:\nthe VC inequality.\nTheorem (VC inequality): For any family of sets A with VC dimension VC(A) = d,\nit holds\nr\n2d log(2en/d)\nIE sup |μn(A) -μ(A)| ≤2\nA∈A\nn\nNote that this result holds even if A is infinite as long as its VC dimension is finite. Moreover,\nobserve that log(|A|) has been replaced by a term of order d log 2en/d .\nTo prove the VC inequality, we proceed in three steps:\n\n1. Symmetrization, to bound the quantity of interest by the Rademacher complexity:\nIE[sup |μn(A) -μ(A)|] ≤2Rn(\n)\nA∈A\nA .\nWe have already done this step in the previous lecture.\n2. Control of the Rademacher complexity using shatter coefficients. We are going to\nshow that\ng\nR (A) ≤\ns\n2 lo\nn\n2SA(n)\nn\n\n3. We are going to need the Sauer-Shelah lemma to bound the shatter coefficients by\nthe VC dimension. It will yield\nS (n) ≤\nend\n,\nd = VC\nA\n(\nd\nA) .\nPut together, these three steps yield the VC inequality.\nStep 2: Control of the Rademacher complexity\nWe prove the following Lemma.\nLemma: For any B ⊂IRn, such that |B| < inf:, it holds\nn\n\nσ\n\n)\nB\n\n(\nRn(\n) = IE max\nX\nlog 2 B\nibi ≤max\n|\n|\nb∈B\nn\nb∈B\ni=1\n|b|2\np\nn\nwhere | · |2 denotes the Euclidean norm.\nProof. Note that\nRn(B) =\nIE\nn\n\nmax Zb\n,\nb∈B\n|\nwhere Zb = Pn\ni=1 σibi. In particular, since\n\n-|bi| ≤σi|bi| ≤|bi|, a.s., Hoeffding's lemma\nimplies that the moment generating function of Zb is controlled by\nn\nn\nIE\n\nexp(sZb)\n\n=\nY\nIE\ni=1\n\nexp(sσibi)\n\n≤\nY\nexp(s2b2\ni /2) = exp(s2 b 2\n2/2)\n(4.2)\ni=1\n| |\nNext, to control IE maxb∈B Zb| , we use the same technique as in Lecture 3, section 1.5.\n\nTo that end, define\n\nB = B ∪\n\n{-B\n\n} and observe that for any s > 0,\nIE\n\nmax |Zb|\n\n= IE\n\nmax Zb\n\n=\nlog exp\n\nsIE\n\nmax Zb\n\n≤\nlog IE exp\ns max Zb\n,\nb∈B\nb∈B\ns\nb\n\n∈B\ns\n\nb\n\n∈B\n\nwhere the last inequality follows from Jensen's inequality. Now we bound the max by a\nsum to get\n\nX\nlog | B|\ns b 2\nIE max |Zb|\n≤\nlog\nIE [exp(sZb)] ≤\n+\n| |2 ,\nb∈B\ns\ns\n2n\nb∈B\nwhere in the last inequality, we used (4.2).\nOptimizing over s > 0 yields the desired\nresult.\n\nWe apply this result to our problem by observing that\nRn(A) =\nsup\n(\n,\nRn(T z))\nz1,... zn\nwhere T(z) is defined in (4.1).\nIn particular, since T(z) ⊂{0, 1}, we have |b\n√\n|2 ≤\nn\nfor all b ∈T(z).\nMoreover, by definition of the shatter coefficients, if B = T(z), then\n| B| ≤2|T(z)| ≤2SA(n). Together with the above lemma, it yields the desired inequality:\nr\n2 log(2SA(n))\nRn(A) ≤\n.\nn\nStep 3: Sauer-Shelah Lemma\nWe need to use a lemma from combinatorics to relate the shatter coefficients to the VC\ndimension. A priori, it is not clear from its definition that the VC dimension may be at\nall useful to get better bounds. Recall that steps 1 and 2 put together yield the following\nbound\n2 log(2S (n))\nIE[sup\nn(A)\nA\n-μ(A) ]\nA∈\n|μ\n| ≤\nA\nr\n(4.3)\nn\nIn particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does\nnot imply that the uniform deviations go to zero as the sample size n goes to infinity. The\nVC inequality suggest that this is not the case as soon as VC(A) < infbut it is not clear a\npriori. Indeed, it may be the case that\nVC\nSA(n) = 2n for n ≤d and SA(n) = 2n -1 for n > d,\nwhich would imply that\n(A) = d < infbut that the right-hand side in (4.3) is larger than\n2 for all n. It turns our that this can never be the case: if the VC dimension is finite, then\nthe shatter coefficients are at most polynomial in n. This result is captured by the Sauer-\nShelah lemma, whose proof is omitted. The reading section of the course contains pointers\nto various proofs, specifically the one based on shifting which is an important technique in\nenumerative combinatorics.\nLemma (Sauer-Shelah): If VC(A) = d, then ∀n ≥1,\nd\nSA(n) ≤\nX n\nen\nd\n.\nk\n\n≤\nd\nk=0\n\nTogether with (4.3), it clearly yields the VC inequality. By applying the bounded difference\ninequality, we also obtain the following VC inequality that holds with high probability. This\nis often the preferred from for this inequality in the literature.\nCorollary (VC inequality): For any family of sets A such that VC(A) = d and any\nδ ∈(0, 1), it holds with probability at least 1 -δ,\nr\n2d log(2en/d)\nr\nlog(2/δ)\nsup μn A) -μ(A)| ≤2\n+\nA∈A\n|\n(\n.\nn\n2n\n\nNote that the logarithmic term log(2en/d) is actually superfluous and can be replaced\nby a numerical constant using a more careful bounding technique. This is beyond the scope\nof this class and the interested reader should take a look at the recommending readings.\n4.3 Application to ERM\nThe VC inequality provides an upper bound for supA∈A |μn(A) -μ(A)| in terms of the VC\ndimension of the class of sets A. This result translates directly to our quantity of interest:\n2VC(\n) log\n2en\n)\nˆ\nsup |Rn h) -\nVC(A)\nlog(2/δ\n(\nR(h)\nn\n\n+\nh∈H\n≤\ns\nA\n|\nr\n(4.4)\n2n\nwhere A = {Ah : h ∈H} and Ah = {(x, y) ∈X × {0, 1} : h(x) = y}. Unfortunately, the\nVC dimension of this class of subsets of X × {0, 1} is not very natural. Since, a classifier h\nis a {0, 1} valued function, it is more natural to consider the VC dimension of the family\nA =\n\n{h = 1} : h ∈H\n\n.\nDefinition: Let H be a collection of classifiers and define\nA =\n{h = 1} : h ∈H\nWe define the VC d\n\nimension VC(\n) o\n\n=\n\nA : ∃h ∈H, h(·) = 1I(· ∈A) .\nH\n\nf H to be the VC dimension of\n\nA.\n\nIt is not clear how VC(A) relates to the quantity VC(A), where A = {Ah : h ∈H} and\nAh = {(x, y) ∈X × {0, 1} : h(x) = y} that appears in the VC inequality. Fortunately, these\ntwo are actually equal as indicated in the following lemma.\nLemma: Define the two families for sets:\n=\nA\nX×\nh : h\n{0,1} where\n{\n\nA\n{\n∈H} ∈\nAh = (x, y) ∈X × {0, 1} : h(x) = y} and A =\n\n{h = 1\n: h\n2X .\nS\nS\n≥\nVC A\n}\n∈H\n∈\nThen,\nA (n) =\nA (n) for all n\n1. It implies\n(\n) = VC(A\n\n).\nProof. Fix x = (x1, ..., xn) ∈X n and y = (y1, y2, ..., yn) ∈{0, 1}n and define\nT(x, y) = {(1I(h(x1) = y1), . . . , 1I(h(xn) = yn)), h ∈H}\nand\nT(x) = {(1I(h(x1) = 1), . . . , 1I(h(xn) = 1)), h ∈H}\nTo that end, fix v ∈{0, 1} and recall the XOR (exclusive OR) boolean function from {0, 1}\nto {0, 1} defined by u ⊕v = 1I(u = v). It is clearly1 a bijection since (u ⊕v) ⊕v = u.\n1One way to see that is to introduce the \"spinned\" variables u = 2u -1 and v = 2v -1 that live in\n^\n{-1, 1}. Then u ⊕v = u ·v , and the claim follows by observing that (u ·v )·v = u . Another way is to simply\nwrite a truth table.\n\nWhen applying XOR componentwise, we have\n\n1I(h(x1) = y1)\n\n1I(h(x1 = 1)\ny\n\n..\n\n)\n\n..\n\n.\n1I(h(xi)\n\n.\n\n= yi)\n=\n1I(h(xi) = 1)\n\n..\n\n.\n⊕\n.\n\n.\n\n.\n\n1I(h(xn) = yn)\n\n1I(h(xn) = 1)\n\n...\n\nyi\n\n..\n\n.\nyn\n\nSince XOR is a bijection, we must have card[T(x, y)] = card[T(x)]. The lemma follows\nby taking the supremum on each side of the equality.\nIt yields the following corollary to the VC inequality.\nCorollary: Let H be a family of classifiers with VC dimension d. Then the empirical\nˆ\nrisk classifier herm over H satisfies\nerm\nr\n2d log(2en/d)\nˆ\nR(h\n) ≤min R(h) + 4\n+\nh∈H\nn\nr\nlog(2/δ)\n2n\nwith probability 1 -δ.\nProof. Recall from Lecture 3 that\nˆ\nR(herm) -min\n) ≤\nˆ\nR(h\n2 sup\nh∈H\nh∈H\nThe proof follows directly by applyi\nRn(h) -R(h)\n\nng (4.4) and the above lemma.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 6 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/5ebb42429b252cbd2f711cd03e01b97f_MIT18_657F15_L6.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Ali Makhdoumi\nSep. 28, 2015\n5. LEARNING WITH A GENERAL LOSS FUNCTION\nIn the previous lectures we have focused on binary losses for the classification problem and\ndeveloped VC theory for it. In particular, the risk for a classification function h : X →{0, 1}\nand binary loss function the risk was\nR(h) = IP(h(X) = Y ) = IE[1I(h(X) = Y )].\nIn this lecture we will consider a general loss function and a general regression model where\nY is not necessarily a binary variable. For the binary classification problem, we then used\nthe followings:\n- Hoeffding's inequality: it requires boundedness of the loss functions.\n- Bounded difference inequality: again it requires boundedness of the loss functions.\n- VC theory: it requires binary nature of the loss function.\nLimitations of the VC theory:\n- Hard to find the optimal classification: the empirical risk minimization optimization,\ni.e.,\nn\nmin\nh\nn\nX\n1I(h(Xi) = Yi)\ni=1\nis a difficult optimization.\nEven though it is a hard optimization, there are some\nalgorithms that try to optimize this function such as Perceptron and Adaboost.\n- This is not suited for regression. We indeed know that classification problem is a\nsubset of Regression problem as in regression the goal is to find IE[Y |X] for a general\nY (not necessarily binary).\nIn this section, we assume that Y ∈[-1, 1] (this is not a limiting assumption as all the\nresults can be derived for any bounded Y ) and we have a regression problem where (X, Y ) ∈\nX × [-1, 1]. Most of the results that we preset here are the analogous to the results we had\nin binary classification. This would be a good place to review those materials and we will\nrefer to the techniques we have used in classification when needed.\n5.1 Empirical Risk Minimization\n5.1.1\nNotations\nLoss function: In binary classification the loss function was 1I(h(X) = Y ).\nHere, we\nreplace this loss function by l(Y, f(X)) which we assume is symmetric, where f ∈F,\nf : X →[-1, 1] is the regression functions. Examples of loss function include\n\n- l(a, b) = 1I(a = b) ( this is the classification loss function).\n- l(a, b) = |a -b|.\n- l(a, b) = (a -b)2.\n- l(a, b) = |a -b|p, p ≥1.\nWe further assume that 0 ≤l(a, b) ≤1.\nRisk: risk is the expectation of the loss function, i.e.,\nR(f) = IEX,Y [l(Y, f(X))],\nwhere the joint distribution is typically unknown and it must be learned from data.\nData: we observe a sequence (X1, Y1), . . . , (Xn, Yn) of n independent draws from a joint\ndistribution PX,Y , where (X, Y ) ∈X × [-1, 1].\nWe denote the data points by Dn =\n{(X1, Y1), . . . , (Xn, Yn)}.\nEmpirical Risk: the empirical risk is defined as\nn\nˆRn(f) = n\nX\nl(Yi, f(Xi)),\ni=1\nˆ\nˆ\nand the empirical risk minimizer denoted by f erm (or f) is defined as the minimizer of\nempirical risk, i.e.,\nˆ\nargmin Rn(f).\nf∈F\nˆ\nIn order to control the risk of f we shall compare its performance with the following oracle:\nf ∈argmin R(f).\nf∈F\nNote that this is an oracle as in order to find it one need to have access to PXY and then\nˆ\noptimize R(f) (we only observe the data Dn). Since f is the minimizer of the empirical\nˆ\nˆ\nˆ\n\nrisk minimizer, we have that Rn(f) ≤Rn(f), which leads to\nˆ\nR(f) ≤\nˆ\nR(f) -ˆ\nˆ\nˆ\nˆ\nˆ\n\nˆ\n\nRn(f) + Rn(f) -Rn(f) + Rn(f) -R(f) + R(f)\n≤\n\nˆ\nˆ\nˆ\nˆ\n\nˆ\nR(f) + R(f) -Rn(f) + Rn(f) -R(f) ≤R(f) + 2 sup\nf∈F\n|Rn(f) -R(f)|.\nTherefore, the quantity of interest that we need to bound is\nsup | ˆRn(f) -R(f)\nf∈F\n|.\nMoreover, from the bounded difference inequality, we know that since the loss function l(·, ·)\nˆ\nis bounded by 1, supf∈F |Rn(f) -R(f)| has the bounded difference property with ci = 1\nn\nfor i = 1, . . . , n, and the bounded difference inequality establishes\nP\n\"\n2t2\nsup | ˆ\nˆ\nRn(f) -R(f) | -IE\n\"\nsup |Rn(f) -R(f)\nf∈F\n|\n#\n≥t\nf∈F\n#\n-\n≤exp\n\nx\ni\ni\n\n= e p\nc\n-2nt2 ,\nwhich in turn yields\n\nP\nlog (1/delta)\n| ˆ\nsup Rn(f) -R(f)| ≤I\n| ˆ\nE\n\"\nsup Rn(f) -R(f)\nδ\nf∈F\nf\n|\n#\n+\n∈\nr\n, w.p. 1\nF\n2n\n-.\nˆ\nAs a result we only need to bound the expectation IE[supf∈F |Rn(f) -R(f)|].\n\n5.1.2\nSymmetrization and Rademacher Complexity\nSimilar to the binary loss case we first use symmetrization technique and then intro-\nduce Rademacher random variables. Let Dn = {(X1, Y1), . . . (Xn, Yn)} be the sample set\nand define an independent sample (ghost sample) with the same distribution denoted by\nD′\nn = {(X′\n1, Y ′\n1), . . . (X′\nn, Y ′\nn)}( for each i, (X′\ni, Y ′\ni ) is independent from Dn with the same\ndistribution as of (Xi, Yi)).\nAlso, let σi ∈{-1, +1} be i.i.d.\nRad(1) random variables\nindependent of Dn and D′\nn. We have\nIE\n\"\nn\nsup |\nl\ni\nf∈F n\nX\n(Yi, f(X ))\ni=1\n-IE [l(Yi, f(Xi))] |\n#\nn\nn\n= IE\n\"\nsup\nl(Yi, f(X\nl(Y ′\ni))\nIE\ni , f(X′\ni)) Dn\nf∈F\n|n\nX\ni=1\n-\n\"\nn\nX\ni=1\n|\n#\n|\n#\nn\nn\n= IE\n\"\nsup |IE\n\"\nX\nl(Yi, f(X\n′\ni))\nl(Yi , f(X′\ni)) Dn\nf∈F\nn i=1\n-n\nX\ni=1\n|\n#\n|\n#\nn\n(a)\n≤IE\n\"\nn\nsup IE\n\"\n|\nX\nl(Yi, f(X\n′\ni)) -\nX\nl(Y , f(X′\ni\ni))| |Dn\nf∈F\nn\nn\ni=1\ni=1\n##\n≤IE\n\"\nn\nn\nsup |\nX\nl(Yi, f(Xi))\nl(Y ′\ni , f(X′\nf∈F n\nn\ni))\ni=1\n-\nX\ni=1\n|\n#\n(b)\n= IE\n\"\nn\nsup |\nX\nσi\nl(Yi, f(Xi)) -l(Y ′\nX\nf\nF n\ni , f(\n′\ni))\n∈\ni=1\n\n|\n#\nn\n(c)\n≤2IE\n\"\nsup\nf∈F\n|n\nX\nσil(Yi, f(Xi))\ni=1\n|\n#\nn\n≤2 sup IE\n\"\nsup |\nX\nσil(yi, f(xi))\nDn\nf∈F n i=1\n|\n#\n.\nwhere (a) follows from Jensen's inequality with convex function f(x) = x , (b) follows from\nthe fact that (X , Y ) and (X′\n′\n| |\ni\ni\ni, Yi ) has the same distributions, and (c) follows from triangle\ninequality.\nRademacher complexity: of a class F of functions for a given loss function l(·, ·) and\nsamples Dn is defined as\nn\nRn(l*F) = sup IE\n\"\nsup |\nX\nσil(yi, f(xi))\n.\nDn\nf∈F n i=1\n|\n#\nTherefore, we have\nIE\n\"\nn\nsup |\nX\nl(Yi, f(Xi))\nf∈F n i=1\n-IE[l(Yi, f(Xi))]|\n#\n≤2Rn(l*F)\nand we only require to bound the Rademacher complexity.\n5.1.3\nFinite Class of functions\nSuppose that the class of functions F is finite. We have the following bound.\n\nTheorem: Assume that F is finite and that ltakes values in [0, 1]. We have\nr\n2 log(2\nRn(l*F)\n|F|)\n≤\n.\nn\nProof. From the previous lecture, for B ⊆\nn\nR , we have that\nn\n2 log(2 B )\nRn(B) = IE\n\"\nmax\nb∈B |n\nX\nσibi\ni=1\n|\n#\n|\n|\n≤max\nb∈B |b|2\np\n.\nn\nHere, we have\n\nl(y\n(x\n\n1, f\n1))\n\n.\nB =\n.\n,\n.\n\nl(yn, f(xn )\nf ∈F\n\n.\n)\n\nSince ltakes values in [0, 1], this\n\nim\n\nplies B\n\n⊆{b : |b|2\n√\n≤\n\nn}. Plugging this bound in the\nprevious inequality completes the proof.\n5.2 The General Case\nRecall that for the classification problem, we had F ⊂{0, 1}X . We have seen that the\ncardinality of the set {(f(x1), . . . , f(xn)), f\nˆerm\n∈F} plays an important role in bounding the\nrisk of f\n(this is not exactly what we used but the XOR argument of the previous lecture\nallows us to show that the cardinality of this set is the same as the cardinality of the set\nthat interests us). In this lecture, this set might be uncountable. Therefore, we need to\nintroduce a metric on this set so that we can treat the close points in the same manner. To\nthis end we will define covering numbers (which basically plays the role of VC dimension\nin the classification).\n5.2.1\nCovering Numbers\nDefinition: Given a set of functions F and a pseudo metric d on F ((F, d) is a metric\nspace) and ε > 0. An ε-net of (F, d) is a set V such that for any f ∈F, there exists\ng ∈V such that d(f, g) ≤ε. Moreover, the covering numbers of (F, d) are defined by\nN(F, d, ε) = inf{|V | : V is an ε-net}.\nFor instance, for the F shown in the Figure 5.2.1 the set of points {1, 2, 3, 4, 5, 6} is a\ncovering. However, the covering number is 5 as point 6 can be removed from V and the\nresulting points are still a covering.\nDefinition: Given x = (x1, . . . , xn), the conditional Rademacher average of a class of\n\nfunctions F is defined as\nRˆx\nn = IE\n\"\nn\nsup\nσ\nf∈F\n\nn\nX\nif(xi)\ni=1\n#\n.\nNote that in what follows we consider a general class of functions F.\nHowever, for\napplying the results in order to bound empirical risk minimization, we take xi to be (xi, yi)\nand F to be l*F. We define the empirical l1 distance as\nn\ndx\n1(f, g) = n\nX\ni\n=1\n|f(x )\ni\n-g(xi)|.\nTheorem: If 0 ≤f ≤1 for all f ∈F, then for any x = (x1, . . . , xn), we have\nˆRx\nn(F) ≤inf\nε≥0\nr\nx\n\n2 log (2N(F, d\nε +\n1, ε))\nn\n\n.\nProof. Fix x = (x1, . . . , xn) and ε > 0.\nLet V be a minimal ε-net of (F, dx\n1).\nThus,\nby definition we have that |V | = N(F, dx\n1, ε). For any f ∈F, define f *∈V such that\nF\no\n\ndx\n1(f, f *) ≤ε. We have that\nn\nRx\nn(F) = IE\n\"\nsup\nσif(xi)\nf∈F\n|n\nX\ni=1\n|\n#\n≤IE\n\"\nn\nn\nsup |\nX\nσi(f(xi)\nf *(xi))\n+ IE sup\nσif *(xi)\nf∈F n\nf∈F n\ni=1\n-\n|\n#\n\"\n|\nX\ni=1\n|\n#\n≤ε + IE\n\"\nn\nmax\nσif(xi)\nf∈V |n\nX\ni=1\n|\n#\nr\n2 log(2\n≤ε +\n|V |)\nn\nr\n2 log(2N(\n= ε +\nF, dx\n1, ε)).\nn\nSince the previous bound holds for any ε, we can take the infimum over all ε ≥0 to obtain\nx\nr\n\n2 log(2N(F, dx\nRn(F) ≤inf\nε +\n1, ε))\nε≥0\nn\n\n.\nThe previous bound clearly establishes a trade-offbecause as ε decreases N(F, dx\n1, ε) in-\ncreases.\n5.2.2\nComputing Covering Numbers\nAs a warm-up, we will compute the covering number of the l2 ball of radius 1 in\nd\nR denoted\nby B2. We will show that the covering is at most (3\nε)d. There are several techniques to\nprove this result: one is based on a probabilistic method argument and one is based on\ngreedily finding an ε-net. We will describe the later approach here. We select points in V\none after another so that at step k, we have uk ∈B2 \\ ∪k\nj=1B(uj, ε). We will continue this\nprocedure until we run out of points. Let it be step N. This means that V = {u1, . . . , uN}\nis an ε-net. We claim that the balls B(ui, ε) and B(uj, ε) for any i, j\n∈{ , . . . , N} are\ndisjoint. The reason is that if v ∈B(ui, ε) ∩B(uj, ε), then we would have\nε\nε\n∥ui -uj∥2 ≤∥ui -v∥2 + ∥v -uj∥2 ≤\n+\n= ε,\nwhich contradicts the way we have chosen the points. On the other hand, we have that\n∪N\nj=1B(uj, ε) ⊆(1 + ε)B2. Comparing the volume of these two sets leads to\nε\nε\n|V |( )dvol(B2) ≤(1 +\n)dvol(B2) ,\nwhere vol(B2) denotes the volume of the unit Euclidean ball in d dimensions. It yields,\n|V | ≤\n1 + ε d\nd\nd\n=\n+ 1\n.\nε\nε\n\nd\n\n≤\n\nε\n\nFor any p ≥1, define\ndx\np(f, g) =\n\nn\np\nX\n|f(xi)\ng(x ) p\ni\n,\nn i=1\n-\n|\n!\nand for p = inf, define\ndx\ninf(f, g) = max |f(xi) -g(xi)\ni\n|.\nˆ\nUsing the previous theorem, in order to bound Rx\nn we need to bound the covering number\nwith dx\n1 norm. We claim that it is sufficient to bound the covering number for the infinity-\nnorm. In order to show this, we will compare the covering number of the norms dx\np(f, g) =\nn\nPn\ni=1 |f(x\np\ni) -g(xi)|\n\np for p ≥1 and conclude that a bound on N(F, dx\ninf, ε) implies a\nbound on N(F, dx\np, ε) for any p ≥1.\nProposition: For any 1 ≤p ≤q and ε > 0, we have that\nN(F, dx\np, ε) ≤N(F, dx\nq , ε).\nProof. First note that if q = inf, then the inequality evidently holds. Because, we have\nn\n(\nX\n|zi|p) p ≤max\nn\ni\ni=1\n|zi|,\nwhich leads to B(f, dx\ninf, ε) ⊆B(f, dx\np, ε) and N(f, dinf, ε) ≥N(f, dp, ε). Now suppose that\n1 ≤p ≤q < inf. Using H older's inequality with r = q\np ≥1 we obtain\n\n! 1\n\n!(1\n1 ) 1\n! 1\n\n! 1\n-\nn\nn\nn\np\nr\np\npr\nn\nq\nn\nX\n|z |p\ni\n≤\n-\nn\np\nX\ni\nX\ni=1\n|zi|pr\n=\nn\ni=1\n=\nX\n.\ni\n|zi|q\n=1\nThis inequality yeilds\nB(f, dx\nq, ε) = {g : dx\nq(f, g) ≤ε} ⊆B(f, dx\np, ε),\nwhich leads to N(f, dq, ε) ≥N(f, dp, ε).\nUsing this propositions we only need to bound N(F, dx\ninf, ε).\nLet the function class be F = {f(x) = ⟨f, x⟩, f ∈Bd, x ∈Bd}, where 1\np\nq\n+\n= 1. This\np\nq\nleads to |f| ≤1.\nClaim: N(F, dx\ninf, ε) ≤(2)d.\nε\nThis leads to\nx\nr\n2d log(4/ε)\nˆRn(F) ≤inf\n0{ε +\n.\nε>\nn\n}\nTaking ε = O(\nq\nd log n), we obtain\nn\nˆRx\nd\nn(F) ≤O(\nr\nlog n).\nn\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 7 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/af336bcec45f238d85d84cea1a04fd3b_MIT18_657F15_L7.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Zach Izzo\nSep. 30, 2015\nIn this lecture, we continue our discussion of covering numbers and compute upper\nˆ\nbounds for specific conditional Rademacher averages Rx\nn(F). We then discuss chaining and\nconclude by applying it to learning.\nRecall the following definitions. We define the risk function\nR(f) = IE[l(X, f(X))],\n(X, Y ) ∈X × [-1, 1] ,\nfor some loss function l(·, ·). The conditiona Rademacher average that we need to control\nis\nn\nR(ll *F) =\nsup\nIE sup\nσil(yi, f(xi))\n.\n(x1,y1),...,(xn,yn)\n\"\nf\nn\n∈F\nX\ni=1\n#\nFurthermore, we defined the conditional Rademacher average for a poin\n\nt x = (x1, . . . , xn)\nto be\n\nRˆx\nn(F) = IE\n\"\nsup\nf∈F\n\nn\nσif(xi)\nn i=1\n#\n.\nLastly, we define the ε-covering number N(\nX\nF, d,\n\nε) to be the m\n\ninimum number of balls (with\nrespect to the metric d) of radius ε needed to\n\ncover\n\nF. We proved the following theorem:\nTheorem: Assume |f| ≤1 for all f ∈F. Then\n2 log(2N(\n, dx, ε))\nRˆx\nn(F)\n+\nr\nF\n≤inf\nε>0\n(\nε\nn\n)\n,\nwhere dx\n1 is given by\nn\ndx\n1(f, g) = n\nX\ni=1\n|f(xi) -g(xi)|.\nWe make use of this theorem in the following example. Define Bd\np = {x ∈IRd : |x|p ≤1}.\nThen take f(x) = ⟨a, x⟩, set F = {⟨a, ·⟩: a ∈Bd }, and X = Bd\n1. By H older's inequality,\ninf\nwe have\n|f(x)| ≤|a|\nx\ninf| |1 ≤1,\nso the theorem above holds. We need to compute the covering number N(F, dx\n1, ε). Note\nthat for all a ∈Bd , there exists v = (v1, . . . , vn) such that vi = g(xi) and\ninf\nn\nn\nX\na, xi\nvi\nε\ni=1\n|⟨\n⟩-\n| ≤\nfor some function g. For this case, we will take g(x) = ⟨b, x⟩, so vi = ⟨b, xi⟩. Now, note the\nfollowing. Given this definition of g, we have\nn\nn\ndx\n1(f, g) =\na, x1\nb, xi\n=\na\nb, xi\na\nb\nn\nX\nn\ni=1\n|⟨\n⟩-⟨\n⟩|\nX\ni=1\n|⟨-\n⟩| ≤| -|inf\n\nby H older's inequality and the fact that |x|1 = 1. So if |a-b|inf≤ε, we can take vi = ⟨b, xi⟩.\nWe just need to find a set of {b1, . . . , bM} ⊂IRd such that, for any a there exists bj such\nthat |a -bj|\n< inf. We can do this by dividing Bd into cubes with side length ε and\ninf\ninf\ntaking the b 's to be the set of vertices of these cubes. Then any a ∈Bd\nj\nmust land in one\ninf\nof these cubes, so |a -bj|\n≤ε as desired. There are c/εd of such b\ninf\nj's for some constant\nc > 0. Thus\nN(Bd , dx\ninf\n1, ε) ≤c/εd.\nWe now plug this value into the theorem to obtain\nRˆx\n2 log(c/εd)\nn(F) ≤inf\n.\nε\n(\nε +\n≥0\nr\nn\n)\nOptimizing over all choices of ε gives\nr\nd log(n)\nε∗= c\nn\n⇒\nRˆx\nn(F) ≤c\nr\nd log(n).\nn\nNote that in this final inequality, the conditional empirical risk no longer depends on\nx, since we \"sup'd\" x out of the bound during our computations. In general, one should\nignore x unless it has properties which will guarantee a bound which is better than the sup.\nAnother important thing to note is that we are only considering one granularity of F in our\nfinal result, namely the one associated to ε∗. It is for this reason that we pick up an extra\nlog factor in our risk bound. In order to remove this term, we will need to use a technique\ncalled chaining.\n5.4 Chaining\nWe have the following theorem.\nTheorem: Assume that |f| ≤1 for all f ∈F. Then\n\n12 Z 1\nRˆx\nn ≤inf\n4ε +\nε>0\n√\nlog(N(\n, dx\n))dt\n.\nn\n2, t\nε\nq\nF\n\n(Note that the integrand decays with t.)\nProof. Fix x = (x1, . . . , xn), and for all j = 1, . . . , N, let Vj be a minimal 2-j-net of F\nunder the dx\n2 metric. (The number N will be determined later.) For a fixed f ∈F, this\nprocess will give us a \"chain\" of points fi\n*which converges to f: dx\n2(fi\n*, f) ≤2-j.\nDefine F = {(f(x1), . . . , f(xn))⊤, f ∈F} ⊂[-1, 1]n. Note that\nRˆx\nn(F) =\nIE sup\nn\nf∈F\n⟨σ, f⟩\nwhere σ = (σ1, . . . , σn). Observe that for all N, we can rewrite ⟨σ, f⟩as a telescoping sum:\n⟨σ, f⟩= ⟨σ, f -fN\n*⟩+ ⟨σ, fN\n*-fN\n*\n-1⟩+ . . . + ⟨σ, f1\n*-f0\n*⟩\n\nwhere f0\n*:= 0. Thus\nN\nRˆx\nn(F) ≤\nIE sup |⟨σ, f -fN\n*⟩| +\nI\nf\nn\nf∈F\nX\nE sup\nσ,\nn\nj\n*\nfj\n*\nf\nF\n-1\n.\nj=1\n|⟨\n-\n⟩|\n∈\nWe can control the two terms in this inequality separately. Note first that by the Cauchy-\nSchwarz inequality,\ndx\n2(f, fN\n*)\nIE sup |⟨σ, f -fN\n*⟩| ≤|σ|2\nn\nf\n√\n.\nn\n∈F\nSince |σ|2 = √n and dx\n2(f, fN\n*) ≤2-N, we have\n1 IE sup |⟨σ, f -fN\n*\nn\nf∈F\n⟩| ≤\n-N .\nNow we turn our attention to the second term in the inequality, that is\nN\nS =\nX 1 IE sup |⟨σ, fj\n*-fj\n*\nn\nf\nj=1\n∈F\n-1⟩|.\nNote that since fj\n*∈Vj and fj\n*\n-1 ∈Vj\nV\n-1, there are at most | j||Vj-1| possible differences\nfj\n*-fj\n*\n.\n-1\nSince |V\nj\n1| ≤|V\n-\nj|/2, |Vj||Vj\n1| ≤|Vj| /2 and we find ourselves in the finite\n-\ndictionary case. We employ a risk bound from earlier in the course to obtain the inequality\np\n2 log(2\nRn(B) ≤max\nb B |b|2\n|B|) .\n∈\nn\nIn the present case, B = {fj\n*-fj\n*\n-1 , f ∈F} so that |B| ≤|Vj|2/2. It yields\n|2\n2 log( |Vj )\nlog\nR\nVj\nn(B)\n|\n·\nq\n|\n≤r\n= 2r\nn\n·\np\n,\nn\nwhere r = supf∈F |fj\n*-fj\n*\n-1|2. Next, observe that\n|fj\n*-fj\n*\n1|2 = √n d\n-\n·\nx\n2(fj\n*, fj\n*\n-1)\n√\n√\n≤\nn(dx\n2(fj\n*, f) + dx\n2(f, fj\n*\n))\n3 2-j\nn .\n-1\n≤\n·\nby the triangle inequality and the fact that dx(f*, f) ≤2-j\nj\n. Substituting this back into our\nbound for Rn(B), we have\nlog\n(B)\n|Vj\n6 2-j\nn\nr\n||\n,\nj\n(\n))\n≤\n·\n= 6\nn\n·\n-\nr\nlog(N F dx\n2, 2-j\nR\nn\nsince V\nj\nj was chosen to be a minimal 2--net.\nThe proof is almost complete. Note that 2-j = 2(2-j -2-j-1) so that\nN\n√\nX\nN\n2-jq\nlog(N(F, dx\n2, 2-j)) = √\nX\n(2-j -2-j-1)\nq\nlog(N(F, dx\n2, 2-j)) .\nn\nn\nj=1\nj=1\nNext, by comparing sums and integrals (Figure 1), we see that\nX\nN\n(2-j\nj=1\n-2-j-1)\nq\nlog(N(F, dx\n2, 2-j)) ≤\nZ\n/2\nlog(N(\n, dx\n2, t))dt.\n2-(N+1)\nq\nF\n\nFigure 1: A comparison of the sum and integral in question.\nSo we choose N such that 2-(N+2) ≤ε ≤2-(N+1), and by combining our bounds we obtain\n1/2\nRˆx\n) ≤2-N\nn(F\n+ √n\nZ\nq\nlog(N(F, dx\n2, t))dt\n2-(\n+1)\n≤4ε +\nN\nZ\np\nlog(N,\nε\nF, t)dt\nsince the integrand is non-negative. (Note: this integral is known as the \"Dudley Entropy\nIntegral.\")\nReturning to our earlier example, since N(F, dx\n2, ε) ≤c/εd, we have\nRˆx\nn(F) ≤inf\n\n4ε + √\nZ\nq\nlog((c′/t)d)dt\nε>0\nn\nε\n\n.\nSince\nR 1 p\nlog(c/t)dt = c is finite, we then have\nRˆx\nn(F) ≤12c\np\nd/n.\nUsing chaining, we've been able to remove the log factor!\n5.5 Back to Learning\nWe want to bound\nn\nRn(l*F) =\nsup\nIE sup\nσil(yi, f(xi))\n.\n(x1,y1),...,(xn,yn)\n\"\nf∈F\n\nn\nX\ni\nx\n\n=1\n#\n\nRˆ\nWe consider\nn(Φ *F\nn\n) = IE\niΦ\n\nsupf\n1 P\ni=1 σ\n*f(x )\n∈F\ni\nfor some L\nn\n\n-Lipschitz function\nΦ, that is |Φ(a) -Φ(b)| ≤L|a -b| for all a, b ∈[-1, 1]. We\n\nhave the following lemma.\n\nTheorem: (Contraction Inequality) Let Φ be L-Lipschitz and such that Φ(0) = 0,\nthen\nRˆx\nn(Φ *F) ≤\nRˆ\n2L ·\nx\nn(F) .\nThe proof is omitted and the interested reader should take a look at [LT91, Kol11] for\nexample.\nAs a final remark, note that requiring the loss function to be Lipschitz prohibits the use\nof R-valued loss functions, for example l(Y, ·) = (Y -·)2.\nReferences\n[Kol11] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse\n\nrecovery problems. Ecole d'Et e de Probabilit es de Saint-Flour XXXVIII-2008. Lec-\nture Notes in Mathematics 2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011.\n[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of\nErgebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and\nRelated Areas (3)]. Springer-Verlag, Berlin, 1991. Isoperimetry and processes.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 8 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/a86e2ffbc8a505b53f9051b60587763c_MIT18_657F15_L8.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Quan Li\nOct. 5, 2015\nPart II\nConvexity\n1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION\nˆ\nIn the previous lectures, we have proved upper bounds on the excess risk R(herm) -R(h∗)\nof the Empirical Risk Minimizer\nˆherm\n= argmin\nh∈H\nn\n1I(Yi = h(Xi)).\n(1.1)\nn\nX\ni=1\n\nHowever due to the nonconvexity of the objective function, the optimization problem\n(1.1) in general can not be solved efficiently. For some choices of H and the classification\nerror function (e.g. 1I(·)), the optimization problem can be NP-hard. However, the problem\nwe deal with has some special features:\n1. Since the upper bound we obtained on the excess risk is O(\nq\nd log n), we only need to\nn\napproximate the optimization problem with error up to O(\nq\nd log n\nn\n).\n2. The optimization problem corresponds to the average case problem where the data\ni.i.d\n(Xi, Yi) ∼PX,Y .\n3. H can be chosen to be some 'natural' classifiers, e.g. H = {half spaces}.\nThese special features might help us bypass the computational issue. Computational\nissue in machine learning have been studied for quite some time (see, e.g. [Kea90]), especially\nin the context of PAC learning. However, many of these problems are somewhat abstract\nand do not shed much light on the practical performance of machine learning algorithms.\nTo avoid the computational problem, the basic idea is to minimize a convex upper bound\nof the classification error function 1I(·) in (1.1). For the purpose of computation, we shall\nalso require that the function class H be a convex set. Hence the resulting minimization\nbecomes a convex optimization problem which can be solved efficiently.\n1.1 Convexity\nDefinition: A set C is convex if for all x, y ∈C and λ ∈[0, 1], λx + (1 -λ)y ∈C.\n\nDefinition: A function f : D →IR on a convex domain D is convex if it satisfies\nf(λx + (1 -λ)y) ≤λf(x) + (1 -λ)f(y),\n∀x, y ∈D, and λ ∈[0, 1].\n1.2 Convex relaxation\nThe convex relaxation takes three steps.\nStep 1: Spinning.\nUsing a mapping Y 7→2Y -1, the i.i.d. data (X1, Y1), (X2, Y2), . . . , (Xn, Yn) is transformed\nto lie in X × {-1, 1}. These new labels are called spinned labels. Correspondingly, the task\nbecomes to find a classifier h : X 7→{-1, 1}. By the relation\nh(X) = Y ⇔-h(X)Y > 0,\nwe can rewrite the objective function in (1.1) by\nn\nn\n1 X\n1I(h(Xi) = Yi) =\nX\nφ1I( h\nn\nn\ni=1\ni=1\n-(Xi)Yi)\n(1.2)\nwhere φ1I(z) = 1I(z > 0).\nStep 2: Soft classifiers.\nThe set H of classifiers in (1.1) contains only functions taking values in {-1, 1}. As a result,\nit is non convex if it contains at least two distinct classifiers. Soft classifiers provide a way\nto remedy this nuisance.\nDefinition: A soft classifier is any measurable function f : X →[-1, 1]. The hard\nclassifier (or simply \"classifier\") associated to a soft classifier f is given by h = sign(f).\nLet F ⊂IRX be a convex set soft classifiers. Several popular choices for F are:\n- Linear functions:\nF := {⟨a, x⟩: a ∈A}.\nfor some convex set A ∈IRd. The associated hard classifier h = sign(f) splits IRd into\ntwo half spaces.\n- Majority votes: given weak classifiers h1, . . . , hM,\nM\nM\nF :=\nn X\nλjhj(x) : λj\nj=\n≥0,\nX\nλj = 1\no\n.\nj=1\n- Let φj, j = 1, 2, . . . a family of functions, e.g., Fourier basis or Wavelet basis. Define\ninf\nF := {\nX\nθjφj(x) : (θ1, θ2, . . .)\nj=1\n∈Θ},\nwhere Θ is some convex set.\n\nStep 3: Convex surrogate.\nGiven a convex set F of soft classifiers, using the rewriting in (1.2), we need to solve that\nminimizes the empirical classification error\nmin\nf∈F\nn\nφ1I( f(Xi)Yi),\nn\nX\ni=1\n-\nHowever, while we are now working with a convex constraint, our objective is still not\nconvex: we need a surrogate for the classification error.\nDefinition: A function φ : IR 7→IR+ is called a convex surrogate if it is a convex\nnon-decreasing function such that φ(0) = 1 and φ(z) ≥φ1I(z) for all z ∈IR.\nThe following is a list of convex surrogates of loss functions.\n- Hinge loss: φ(z) = max(1 + z, 0).\n- Exponential loss: φ(z) = exp(z).\n- Logistic loss: φ(z) = log2(1 + exp(z)).\nTo bypass the nonconvexity of φ1I(·), we may use a convex surrogate φ(·) in place of\nˆ\nφ1I(·) and consider the minimizing the empirical φ-risk Rn,φ defined by\nˆRn,φ(f) = n\nn\nX\ni=1\nφ(-Yif(Xi))\nIt is the empirical counterpart of the φ-risk Rφ defined by\nRφ(f) = IE[φ(-Y f(X))].\n1.3 φ-risk minimization\nIn this section, we will derive the relation between the φ-risk Rφ(f) of a soft classifier f and\nthe classification error R(h) = IP(h(X) = Y ) of its associated hard classifier h = sign(f)\nLet\nf ∗\nφ = argmin E[φ( Y\nf∈IRX\n-\nf(X))]\nwhere the infimum is taken over all measurable functions f : X →IR.\nTo verify that minimizing the φ serves our purpose, we will first show that if the convex\nsurrogate φ(·) is differentiable, then sign(f ∗\nφ(X)) ≥0 is equivalent to η(X) ≥1/2 where\nη(X) = IP(Y = 1 | X). Conditional on {X = x}, we have\nIE[φ(-Y f(X)) | X = x] = η(x)φ(-f(x)) + (1 -η(x))φ(f(x)).\nLet\nHη(α) = η(x)φ(-α) + (1 -η(x))φ(α)\n(1.3)\n\nso that\nf ∗\nφ(x) = argmin H\n∗\nη(α) ,\nand\nRφ = min Rφ(f) = min Hη\n)\nα\nf∈IRX\n(x)(α .\nα∈IR\n∈IR\nSince φ(·) is differentiable, setting the derivative of H\n∗\nη(α) to zero gives fφ(x) = α , where\nH′\nη(α ) = -η(x)φ′(-α ) + (1 -η(x))φ′(α ) = 0,\nwhich gives\nη(x)\nφ′(α )\n=\n1 -η(x)\nφ′(-α )\nSince φ(·) is a convex function, its derivative φ′(·) is non-decreasing. Then from the equation\nabove, we have the following equivalence relation\nη(x) ≥\n⇔α ≥0 ⇔sign(f ∗\nφ(x)) ≥0.\n(1.4)\nSince the equivalence relation holds for all x ∈X,\nη(X) ≥\n⇔sign(f ∗\nφ(X))\n≥0.\nThe following lemma shows that if the excess φ-risk R (f) -R∗\nφ\nφ of a soft classifier f is\nsmall, then the excess-risk of its associated hard classifier sign(f) is also small.\nLemma (Zhang's Lemma [Zha04]): Let φ : IR 7→IR+ be a convex non-decreasing\nfunction such that φ(0) = 1. Define for any η ∈[0, 1],\nτ(η) := inf Hη(α).\nα∈IR\nIf there exists c > 0 and γ ∈[0, 1] such that\n|η -\nc\n2| ≤(1 -τ(η))γ ,\n∀η ∈[0, 1] ,\n(1.5)\nthen\nR(sign(f)) -R∗≤2c(Rφ(f) -R∗\nφ)γ\nProof. Note first that τ(η) ≤Hη(0) = φ(0) = 1 so that condition (2.5) is well defined.\nNext, let h∗= argminh∈{-1,1}X IP[h(X) = Y ] = sign(η-1/2) denote the Bayes classifier,\nwhere η = IP[Y = 1|X = x], . Then it is easy to verify that\nR(sign(f)) -R∗= IE[|2η(X) -1|1I(sign(f(X)) = h∗(X))]\n= IE[|2η(X) -1|1I(f(X)(η(X) -1/2) < 0)]\n≤2cIE[((1 -τ(η(X)))1I(f(X)(η(X) -1/2) < 0))γ]\n≤2c (IE[(1 -τ(η(X)))1I(f(X)(η(X) -1/2) < 0)])γ ,\nwhere the last inequality above follows from Jensen's inequality.\n\nWe are going to show that for any x ∈X, it holds\n(1 -τ(η))1I(f(x)(η(x) -1/2) < 0)] ≤IE[φ(-Y f(x)) | X = x] -R∗\nφ .\n(1.6)\nThis will clearly imply the result by integrating with respect to x.\nRecall first that\nIE[φ(-Y f(x)) | X = x] = Hη(x)(f(x))\nand\nR∗\nφ = min Hη(x)(α) = τ(η(x)) .\nα∈IR\nso that (2.6) is equivalent to\n(1 -τ(η))1I(f(x)(η(x) -1/2) < 0)] ≤Hη(x)(α) -τ(η(x))\nSince the right-hand side above is nonnegative, the case where f(x)(η(x) -1/2) ≥0 follows\ntrivially. If f(x)(η(x)-1/2) < 0, (2.6) follows if we prove that Hη(x)(α) ≥1. The convexity\nof φ(·) gives\nHη(x)(α) = η(x)φ(-f(x)) + (1 -η(x))φ(f(x))\n≥φ(-η(x)f(x) + (1 -η(x))f(x))\n= φ((1 -2η(x))f(x))\n≥φ(0) = 1 ,\nwhere the last inequality follows from the fact that φ is non decreasing and f(x)(η(x) -\n1/2) < 0. This completes the proof of (2.6) and thus of the Lemma.\nIT is not hard to check the following values for the quantities τ(η), c and γ for the three\nlosses introduced above:\n- Hinge loss: τ(η) = 1 -|1 -2η| with c = 1/2 and γ = 1.\n- Exponential loss: τ(η) = 2\np\nη(1 -η) with c = 1/\n√\n2 and γ = 1/2.\n- Logistic loss: τ(η) = -η log η -(1 -η) log(1 -η) with c = 1/\n√\n2 and γ = 1/2.\nReferences\n[Kea90] Michael J Kearns. The computational complexity of machine learning. PhD thesis,\nHarvard University, 1990.\n[Zha04] Tong Zhang. Statistical behavior and consistency of classification methods based\non convex risk minimization. Ann. Statist., 32(1):56-85, 2004.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Mathematics of Machine Learning Lecture 9 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/c8805f7adeb6fe19382e8341878595c8_MIT18_657F15_L9.pdf",
      "content": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet\nLecture\nScribe: Xuhong Zhang\nOct. 7, 2015\nRecall that last lecture we talked about convex relaxation of the original problem\nn\nˆh = argmin\n1I(h(Xi) = Yi)\nh∈H\nn\nX\ni=1\nby considering soft classifiers (i.e. whose output is in [-1, 1] rather than in {0, 1}) and\nconvex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss):\nn\nˆ\nˆ\nf = argmin Rφ,n(f) = argmin\nφ( Yif(Xi))\nf∈F\nf∈F\nn\nX\ni=1\n-\nˆ\nˆ\nAnd h = sign(f) will be used as the 'hard' classifier.\nˆ\n\nWe want to bound the quantity Rφ(f) -Rφ(f), where f = argminf∈F Rφ(f).\nˆ\nˆ\n(1) f = argminf∈F Rφ,n(f), thus\nˆ\n\nˆ\n\nˆ\n\nˆ\nˆ\nˆ\nˆ\nˆ\n\nRφ(f) = Rφ(f) + Rφ,n(f) -Rφ,n(f) + Rφ,n(f) -Rφ,n(f) + Rφ(f) -Rφ(f)\n≤\n\nˆ\n\nˆ\nˆ\nˆ\n\nRφ(f) + Rφ,n(f) -Rφ,n(f) + Rφ(f) -Rφ(f)\n≤\n\nˆ\nRφ(f) + 2 sup |Rφ,n(f) -Rφ(f)\nf∈F\n|\nˆ\n(2) Let us first focus on E[supf∈F |Rφ,n(f) -Rφ(f)|]. Using the symmetrization trick as\nbefore, we know it is upper-bounded by 2Rn(φ*F), where the Rademacher complexity\nn\nRn(φ *F) =\nsup\nE[sup |\nX\nσiφ(-Yif(Xi)) ]\nX1,...,Xn,Y1,...,Yn\nf∈F n i=1\n|\nOne thing to notice is that φ(0) = 1 for the loss functions we consider (hinge loss,\nexponential loss and logistic loss), but in order to apply contraction inequality later,\nwe require φ(0) = 0. Let us define ψ(·) = φ(·) -1. Clearly ψ(0) = 0, and\nn\nE[sup |\nX\n(φ(-Yif(Xi)) -E[φ(-Yif(Xi))]) ]\nf∈F n i=1\n|\nn\n= E[sup |\nX\n(ψ(-Yif(X ) -E\ni )\n[ψ(\ni\n-Yif(X ))])\nf∈F n i=\n|]\n≤2Rn(ψ *F)\n(3) The Rademacher complexity of ψ *F is still difficult to deal with. Let us assume\nthat φ(·) is L-Lipschitz, (as a result, ψ(·) is also L-Lipschitz), apply the contraction\ninequality, we have\nRn(ψ *F) ≤2LRn(F)\n\n(4) Let Zi = (Xi, Yi), i = 1, 2, ..., n and\nn\ng(Z1, Z2\nˆ\n, ..., Zn) = sup |Rφ,n(f)-Rφ(f) =\nf\n|\nsup\n∈F\nf∈F\n|n\nX\n(φ(\ni=1\n-Yif(Xi))-E[φ(-Yif(Xi))])|\nSince φ(·) is monotonically increasing, it is not difficult to verify that ∀Z1, Z2, ..., Zn, Z′\ni\n2L\n|g(Z1, ..., Zi, ..., Zn) -g(Z1, ..., Z′\ni, ..., Zn)| ≤\n(φ(1) -φ(\nn\n-1)) ≤n\nThe last inequality holds since g is L-Lipschitz. Apply Bounded Difference Inequality,\n2t2\nP(| s\n| ˆ\nˆ\nup Rφ,n(f) -Rφ(f)| -E[sup |Rφ,n(f) -Rφ(f)|] >\nF\n∈F\n|\nt) ≤2 exp(\n∈\n-\n)\nf\nPn\nf\ni= (2L)2\nn\nSet the RHS of above equation to δ, we get:\nlog(2/δ)\nˆ\nˆ\nsup R\nE\nφ,n(f)\nRφ(f)\n[sup Rφ,n(f)\nf∈F\n|\n-\n| ≤\nf∈F\n|\n-Rφ(f)|] + 2L\nr\n2n\nwith probability 1 -δ.\n(5) Combining (1) - (4), we have\nˆ\n\nRφ(f) ≤Rφ(f) + 8LRn(F) + 2L\nr\nlog(2/δ)\n2n\nwith probability 1 -δ.\n1.4 Boosting\nIn this section, we will specialize the above analysis to a particular learning model: Boosting.\nThe basic idea of Boosting is to convert a set of weak learners (i.e. classifiers that do better\nthan random, but have high error probability) into a strong one by using the weighted\naverage of weak learners' opinions. More precisely, we consider the following function class\nM\nF = {\nX\nθjhj(·) : |θ|1 ≤1, hj : X 7→[-1, 1], j ∈{1, 2, ..., M\na\nj=1\n}\nre classifiers}\nand we want to upper bound Rn(F) for this choice of F.\nn\nM\nn\nRn(F) =\nsup\nE[sup\nσiYif(Xi) ] =\nsup\nE[ sup\nθj\nYiσihj(Xi) ]\nZ1,...,Zn\nf∈F\n|n\nX\nn Z\n|θ|1≤1\n|\n1,...,Z\ni=\nn\n|\nX\nj=1\nX\ni=1\n|\nLet g(θ) = | PM\nj=1 θj\nPn\ni=1 Yiσihj(Xi)|. It is easy to see that g(θ) is a convex function, thus\nsup|θ|1≤1 g(θ) is achieved at a vertex of the unit l1 ball {θ : ∥θ∥1 ≤1}. Define the finite set\nY1h1(X1)\nY1h2(X1)\nY1hM(X1)\n(\nY2h1(X2)\nY2h2(X2)\nY2hM(X2)\nBX,Y ≜\n\n.\n,\n, . . . ,\n.\n\n±\n\n.\n±\n±\n)\nYnh1(Xn)\n\n.\n.\n.\nYnh2(Xn)\n\n.\n.\n.\nYnhM(Xn)\n\nThen\nRn(F) = sup Rn(BX,Y) .\nX,Y\nNotice maxb∈BX,Y b\n√\n| |2 ≤\nn and |BX,Y| = 2M. Therefore, using a lemma from Lecture 5,\nwe get\n2 log(2 B\nR\nX,Y )\nlog( M)\nn(BX,Y) ≤\nmax\nb∈BX,Y |b|2\np\n|\n|\nn\n≤\nr\nn\nThus for Boosting,\n\n2 log(4M)\nlog(2/δ)\nˆ\n\nRφ(f) ≤Rφ(f) + 8L\nr\n+ 2L\nr\nwith probability 1 - δ\nn\n2n\nTo get some ideas of what values L usually takes, consider the following examples:\n(1) for hinge loss, i.e. φ(x) = (1 + x)+, L = 1.\n(2) for exponential loss, i.e. φ(x) = ex, L = e.\n(3) for logistic loss, i.e. φ(x) = log2(1 + ex), L =\ne\n1+e log2(e) ≈2.43\nˆ\n\nNow we have bounded Rφ(f) -Rφ(f), but this is not yet the excess risk. Excess risk is\nˆ\ndefined as R(f) -R(f ∗), where f ∗= argminf Rφ(f). The following theorem provides a\nbound for excess risk for Boosting.\nTheorem: Let F = {PM\nj=1 θjhj : ∥θ∥1 ≤1, hjs are weak classifiers} and φ is an L-\nˆ\nˆ\nˆ\nLipschitz convex surrogate. Define f = argminf∈F Rφ,n(f) and h = sign(f). Then\nγ\nγ\n∗\n∗γ\n\n2 log(4M)\nlog(2/δ)\nˆ\nR(h) -R ≤2c\ninf Rφ(f) -Rφ(f )\n+ 2c\n8L\n+\nf∈F\nr\nn\n!\n2c\n\n2L\nr\n2n\n!\nwith probability 1 -δ\nProof.\nˆ\nR(h) -R∗≤2c\nγ\nRφ(f) -Rφ(f ∗)\n\nγ\n∗\n2 log(4M)\nlog(2/δ)\n≤2c\ninf Rφ(f)\nRφ(f ) + 8L\n+ 2L\nf∈F\n-\nr\nn\nr\n2n\n!\nγ\n∗\nγ\n2 log(4M)\nlog(2/δ)\n≤2c\ninf Rφ(f) -Rφ(f )\n+ 2c\nf∈F\n\n8L\nr\nn\n!\n+ 2c\n\n2L\nr\n2n\n!γ\n\nHere the first inequality uses Zhang's lemma and the last one uses the fact that for ai ≥0\nand γ ∈[0, 1], (a1 + a\nγ\n2 + a3) ≤aγ\n1 + aγ\n2 + aγ\n3.\n1.5 Support Vector Machines\nIn this section, we will apply our analysis to another important learning model: Support\nVector Machines (SVMs). We will see that hinge loss φ(x) = (1 + x)+ is used and the\nassociated function class is F = {f : ∥f∥W ≤λ} where W is a Hilbert space.\nBefore\nanalyzing SVMs, let us first introduce Reproducing Kernel Hilbert Spaces (RKHS).\n\n1.5.1\nReproducing Kernel Hilbert Spaces (RKHS)\nDefinition: A function K : X × X 7→IR is called a positive symmetric definite kernel\n(PSD kernel) if\n(1) ∀x, x′ ∈X, K(x, x′) = K(x′, x)\n(2) ∀n ∈Z+, ∀x1, x2, ..., xn, the n\nth\n× n matrix with K(xi, xj) as its element in ith row\nand j\ncolumn is positive semi-definite. In other words, for any a1, a2, ..., an ∈IR,\nX\naiajK(xi, xj)\ni,j\n≥\nLet us look at a few examples of PSD kernels.\nExample 1\nLet X = IR, K(x, x′) = ⟨x, x′⟩IRd is a PSD kernel, since ∀a1, a2, ..., an ∈IR\nX\naiaj⟨xi, xj⟩IRd =\nX\n⟨aixi, ajxj⟩IRd = ⟨\nX\naixi,\nX\najxj⟩IRd = ∥\nX\na\nixi∥IRd\ni,j\ni,j\ni\nj\ni\n≥\nExample 2\nThe Gaussian kernel K(x, x′) = exp(-1\n∥\n′\n2 x -x ∥IRd) is also a PSD kernel.\nσ\nNote that here and in the sequel, ∥· ∥W and ⟨·, ·⟩W denote the norm and inner product\nof Hilbert space W.\nDefinition: Let W be a Hilbert space of functions X 7→IR. A symmetric kernel K(·, ·)\nis called reproducing kernel of W if\n(1) ∀x ∈X, the function K(x, ·) ∈W.\n(2) ∀x ∈X, f ∈W, ⟨f(·), K(x, ·)⟩W = f(x).\nIf such a K(x, ·) exists, W is called a reproducing kernel Hilbert space (RKHS).\nClaim: If K(·, ·) is a reproducing kernel for some Hilbert space W, then K(·, ·) is a\nPSD kernel.\nProof. ∀a1, a2, ..., an ∈IR, we have\nX\naiajK(xi, xj) =\nX\naiaj⟨K(xi, ·), K(xj, ·)⟩(since K( , ) is reproducing)\ni,j\ni,j\n· ·\n= ⟨\nX\naiK(xi, ),\najK(xj, ) W\ni\n·\nX\nj\n· ⟩\n= ∥\nX\naiK(xi,\ni\n·)∥2\nW ≥0\n\nIn fact, the above claim holds both directions, i.e. if a kernel K(·, ·) is PSD, it is also a\nreproducing kernel.\nA natural question to ask is, given a PSD kernel K(·, ·), how can we build the corresponding\nHilbert space (for which K(·, ·) is a reproducing kernel)? Let us look at a few examples.\nExample 3\nLet φ1, φ2, ..., φM be a set of orthonormal functions in L2([0, 1]), i.e. for any\nj, k ∈{1, 2, ..., M}\nZ\nφj(x)φk(x)dx =\nx\n⟨φj, φk⟩= δjk\nLet K(x, x′) = PM\nj=1 φj(x)φj(x′). We claim that the Hilbert space\nM\nW = {\nX\najφj(\n:\n=1\n·)\na1, a2, ..., aM\nj\n∈IR}\nequipped with inner product ⟨·, ·⟩L2 is a RKHS with reproducing kernel K(·, ·).\nM\nProof.\n(1) K(x, ·) =\nj=1 φj(x)φj(·) ∈W. (Choose aj = φj(x)).\n(2) If f(·) = PM\nj=1 aj\nP\nφj(·),\nM\nM\nM\n⟨f(·), K(x, ·)⟩L2 = ⟨\nX\najφj(·),\nX\nφk(x)φk(·)⟩L2 =\nX\najφj(x) = f(x)\nj=1\nk=1\nj=1\n(3) K(x, x′) is a PSD kernel: ∀a1, a2, ..., an ∈IR,\nX\naiajK(x\ni, xj) =\nX\naiajφk(xi)φk(xj) =\nX\n(\nX\naiφk(xi))\ni,j\ni,j,k\nk\ni\n≥0\nExample 4\nIf X = IRd, and K(x, x′) = ⟨x, x′⟩IRd, the corresponding Hilbert space is\nW = {⟨w, ·⟩: w ∈IRd} (i.e. all linear functions) equipped with the following inner product:\nif f = ⟨w, ·⟩, g = ⟨v, ·⟩, ⟨f, g⟩≜⟨w, v⟩IRd.\nProof.\n(1) ∀x ∈IRd, K(x, ·) = ⟨x, ·⟩IRd ∈W.\n(2) ∀f = ⟨w, ·⟩IRd ∈W, ∀x ∈IRd, ⟨f, K(x, ·)⟩= ⟨w, x⟩IRd = f(x)\n(3) K(x, x′) is a PSD kernel: ∀a1, a2, ..., an ∈IR,\nX\naiajK(xi, xj) =\nX\naiaj\n,\n,j\ni,j\n⟨xi xj\ni\n⟩= ⟨\nX\naixi,\ni\nX\najxj\nj\n⟩IRd = ∥\nX\naix\ni IRd\ni\n∥\n≥\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}