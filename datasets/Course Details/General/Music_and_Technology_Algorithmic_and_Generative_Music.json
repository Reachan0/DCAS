{
  "course_name": "Music and Technology: Algorithmic and Generative Music",
  "course_description": "This course examines the history, techniques, and aesthetics of mechanical and computer-aided approaches to algorithmic music composition and generative music systems. Through creative hands-on projects, readings, listening assignments, and lectures, students will explore a variety of historical and contemporary approaches. Diverse tools and systems will be employed, including applications in Python, MIDI, Csound, SuperCollider, and Pure Data.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Fine Arts",
    "Media Studies",
    "Digital Media",
    "Music",
    "Music History",
    "Music Theory",
    "Mathematics",
    "Applied Mathematics",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Fine Arts",
    "Media Studies",
    "Digital Media",
    "Music",
    "Music History",
    "Music Theory",
    "Mathematics",
    "Applied Mathematics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: Two sessions / week, 1.5 hours / session.\n\nCourse meetings are divided into two categories. Most sessions are topic meetings, which combine lecture, demonstration, listening, and discussion to explore particular areas of focus. Every 5th session is a workshop meeting, which enriches and extends topics through presentation of student work, criticism, and collaborative hands-on experimentation.\n\nPrerequisites\n\nNone but curiosity, willingness to experiment\n\nProgramming in Python or other languages useful, but not required\n\nExperience with digital audio and DAW software desirable, but not required\n\nDescription\n\nThis course examines the history, techniques, and aesthetics of mechanical and computer-aided approaches to algorithmic music composition and generative music systems. Through creative hands-on projects, readings, listening assignments, and lectures, students will explore a variety of historical and contemporary approaches. Diverse tools and systems will be employed, including applications in Python, MIDI, Csound, SuperCollider, and Pure Data.\n\nObjectives\n\nTo gain a critical understanding of the history, techniques, and designs of algorithmic and generative music systems.\n\nTo develop musical creativity and expression in the use and design of algorithmic and generative music systems.\n\nTo critically evaluate claims of aesthetic and technological advancement, quality, and promise.\n\nRequired Texts\n\nReadings for this class are comprised of various papers, articles, and book chapters. See the\nReadings and Listening\npage for a detailed list.\n\nRequired Digital Materials\n\nAll students are expected to have regular access to a computer (Windows, Macintosh, or GNU/Linux) with an internet connection, be able to listen to sounds on this computer (with or without headphones), and regularly check their MIT email account and the course Web site. Course announcements and comments on submitted work will always be distributed via email.\n\nLecture notes will be provided online, but these notes do not contain all necessary course information and are not a substitute for attending class and taking notes. Online lecture notes may be made unavailable at any time.\n\nThis course will use several free, cross-platform, stand-alone, or web-based applications and resources, including\n\nAudacity\n\nathenaCL\n\nThe Freesound Project\n\nMartingale\n\nPureData\n\nSuperCollider\n\nAssignments, projects, and demonstrations may be facilitated by the use and installation of these software\ntools\n.\n\nReference Materials\n\nThese reference works may be useful for terms, people, and concepts presented in this course.\n\nOxford Music Online/Grove Music Online [subscription service]\n\nRoads, C.\nThe Computer Music Tutorial\n. Cambridge, MA: MIT Press, 1996. ISBN: 9780262680820.\n\nAssignments\n\nReading Assignments\n\nAll reading assignments, unless marked optional, are required. Reading assignments should be completed\nprior to\nthe scheduled course meeting. Note the specified page numbers, as complete chapters are not always assigned. Taking notes while reading is strongly recommended.\n\nListening Assignments\n\nAll listening assignments, unless marked optional, are required. Listening assignments should be completed\nprior to\nthe scheduled course meeting. Focused and critical listening is required, giving attention to duration, instrumentation, method of production, recording or performance context, notable sonic events, form, temporal design and proportions, aesthetic or historical contexts, and/or critical and subjective responses. Taking notes while listening is strongly recommended. The sonic materials engaged in this class require broad-bandwidth speakers or headphones; small computer or laptop speakers may mask critical sonic details.\n\nReading and Listening Discussion\nLeaders\n\nFor each class, students will be assigned to deliver in-class summaries and commentary on assigned readings and/or listening assignments. For reading assignments, discussion leaders are required to post notes, outlines, key terms, concepts, and/or critical responses. For listening assignments, discussion leaders are required to post commentary on duration, instrumentation, method of production, recording or performance context, notable sonic events, form, temporal design and proportions, aesthetic or historical contexts, and/or critical and subjective responses. Posts should be around 300 words, or about 1 double-spaced page in a 12 point serif font with one inch margins; and must be completed before the start of class.\n\nMusical Design Report\n\nThis report is an original sonic sketch or musical work, lasting from two to five minutes, realized in notation, MIDI, digital audio, or code, and based on approaches, techniques, and/or models presented for each assignment. The sonic submission must be accompanied by written commentary (around 300 words, or about 1 double-spaced page in a 12 point serif font with one inch margins) consisting of at least the following components.\n\nA discussion of personal, musical, and/or aesthetic motivations.\n\nCommentary on how the techniques and/or tools shape and/or control the music versus how intuitive and/or aesthetic choices shape and/or control the music.\n\nAn evaluation of the aesthetic quality of the results and a description of how the work might be improved and/or expanded.\n\nFor each assignment, a group of students will be selected to present and discuss their results.\n\nSonic System Project and Presentation\n\nThis project includes the creation of an original sonic system that functions as either a generative instrument with or without a performance interface or as a static or dynamic musical work employing techniques and/or tools of algorithmic composition. This project can be created with mechanical and/or acoustic tools, electrical circuits, creative extensions of conventional musical instruments and processors, or original software designs in any language or system. Students will present and demonstrate their system to the class and provide a written report describing their approach.\n\nTwo weeks prior to the project's final due date, students must bring to class and submit a working prototype or minimal implementation of their sonic system, and be prepared to demonstrate and discuss their goals and plans as they approach completion.\n\nAssignment Submission and Late Work\n\nAll written assignments, unless otherwise indicated, must be submitted digitally via email attachment. Upon receipt by the instructor students will receive an email confirmation within twelve hours. If a student does not receive an email confirmation, it is the student's responsibility to contact the instructor and/or re-send the assignment.\n\nDigital media assignments, when required, must be submitted (if smaller than 5 MB) via email attachment or (if larger than 5 MB) via a digital delivery service like Pando (free\nBasic Version\n) or YouSendIt (free Lite\naccount\n). Upon receipt by the instructor students will receive an email confirmation within twelve hours. If a student does not receive email confirmation, it is the student's responsibility to contact the instructor and/or re-send the assignment. All digital media assignments must be submitted as MIDI (.mid), AIFF (.aif), or WAVE (.wav) files.\n\nLate assignments will receive a grade reduction. Students are encouraged to submit all assignments, even if late. Assignments turned in within seven days after the due date will be deducted 20 percent of the total points possible. Assignments will not be accepted one week after the due date or after the last scheduled course meeting.\n\nAttendance and Participation\n\nAttendance and participation, as integral parts of this course, are required. Excused absences include illness or emergencies communicated to the instructor before the absence. Students are permitted one unexcused absence without penalty. Beginning with the second unexcused absence, the final course grade will be reduced 3% with each additional absence. If a class is missed, it is the student's responsibility to make-up any missed work. As all assignments are digitally-submitted, assignment deadlines remain regardless of attendance.\n\nAlways arrive to class on time. Early departures are not permitted. Frequent tardiness may negatively affect final course grade.\n\nQuizzes and Exams\n\nThroughout the semester there will be occasional in-class quizzes. These quizzes will require short, written responses. These quizzes will ask questions about material presented in lecture, readings, and listening assignments.\n\nIf a quiz is missed, it is the student's responsibility to take the quiz at the beginning of the next (and only the next) class meeting. Quizzes cannot be taken more than one class meeting after the quiz was originally given. There will not be a final exam for this subject.\n\nGrading\n\nThe final course grade will be determined from the following components:\n\nACTIVITIES\n\nPERCENTAGES\n\nReading and listening discussion leaders\n\n20%\n\nMusical design reports (3)\n\n30%\n\nSonic system project and presentation\n\n20%\n\nSonic system project draft\n\n5%\n\nQuizzes\n\n15%\n\nParticipation\n\n10%\n\nGrading policies, the use of grade modifiers, and additional grades will be given in accordance with policies set forth in the MIT Course Bulletin,\nAcademic Procedures and Institute Regulations.\n\nGrades are given on written assignments based on the following criteria. An\nF\nis given for incorrect, incomplete, and unsatisfactory work that demonstrates neither effort nor critical thought. A\nD\nis given for incomplete and unsatisfactory work that demonstrates some effort and minimal critical thought. A\nC\nis given for complete and satisfactory work with little or no creative or critical thought. A\nB\nis given for thorough, well-written, and well-presented work with some creative and critical thought. An\nA\nis given for substantial and creative original work and critical insight, executed without flaw.\n\nGrades will be reduced for poor writing and/or an unreasonable number of grammatical errors. Grades are given for class participation based on the quality, relevance, creativity, and insight of aural questions, answers, and discussion points based on assignments, lectures, in-class demonstrations, or other student's work. As much as possible, participation grades follow the standards for written assignments as presented above.\n\nAcademic Integrity, Intellectual Property, and Plagiarism\n\nStudents are encouraged to discuss course content with other students taking the course. Each student must, however, produce their own original work. Students are expected to observe the highest levels of academic integrity. All cases of academic dishonesty will be taken very seriously. For more information on academic integrity, citing sources, and plagiarism see\nAcademic Integrity at MIT\n.\n\nAssignments may involve using digital media or intellectual property produced by others. Materials used in such situations, and provided by the instructor or obtained from the internet, must be either in the public domain or licensed specifically for shared use. Students are expected to follow all relevant copyright and intellectual property laws.\n\nPlagiarism includes using the words, ideas, or creative works of another writer or commentator without acknowledgment. It does not matter where these words or ideas are found or if they are signed or anonymous. When using or referencing ideas that are not your own, a citation must be provided. It is the student's responsibility to understand what is plagiarism and how to cite sources. Parenthetical in-text MLA-style citations are acceptable. Footnotes are optional.\n\nIn the case of unattributed and/or suspicious student work, software may be used to search the internet, literature archives, and current and past assignments for possibly-plagiarized material.\n\nSuspected cases of academic misconduct will be handled according to section 10.2 of\nMIT Policies and Procedures\n.\n\nSchedule\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nFoundations: Algorithmic and generative music systems\n\nFoundations: Musical parameters, mappings, and tools\n\nApproaches: Distributions and stochastics\n\nFoundations: Historical and categorical perspectives\n\nHistory: Serialism, loops, tiling, and phasing\n\nWorkshop\n\nMusical design\n\nreport 1 due\n\nHistory: Michele Gottfried Koenig\n\nQuiz 1\n\nApproaches: Permutations, generators, and chaos\n\nHistory: Lejaren Hiller\n\nApproaches: Probability and Markov chains\n\nWorkshop\n\nMusical design\n\nreport 2 due\n\nQuiz 2\n\nHistory: Iannis Xenakis\n\nApproaches: Non-standard synthesis\n\nApproaches: Granular and concatenative synthesis\n\nApproaches: Mapping, sonification, and data bending\n\nQuiz 3\n\nWorkshop\n\nMusical design\n\nreport 3 due\n\nApproaches: Cellular automata\n\nApproaches: Genetic algorithms\n\nApproaches: Grammars and L-Systems\n\nQuiz 4\n\nHistory: Mechanical musical automata\n\nWorkshop\n\nSonic system\n\nproject draft due\n\nApproaches: Agents and ecological models\n\nApproaches: Expert systems and style emulation\n\nQuiz 5\n\nDiscussion: Aesthetics and evaluations\n\nSonic system project presentations\n\nSonic system\n\nproject report\n\ndue\n\nSonic system project presentations (cont.)",
  "files": [
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec_cvr_toc.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/4ecb7f798a87ba9e0fbe95f3b63697e1_MIT21M_380S10_lec_cvr_toc.pdf",
      "content": "21M.380\nMusic and Technology: Algorithmic and Generative Music\nSystems\nChristopher Ariza\n\nTable of Contents\n1. Meeting 1, Foundations: Algorithmic and Generative Music Systems ................................. 1\n2. Meeting 2, Foundations: Musical Parameters, Mappings, and Tools................................20\n3. Meeting 3, Approaches: Distributions and Stochastics ......................................................32\n4. Meeting 4, Foundations: Historical and Categorical Perspectives.....................................47\n5. Meeting 5, History: Serialism, Loops, Tiling, and Phasing................................................48\n6. Meeting 6, Workshop...........................................................................................................60\n7. Meeting 7, History: Gottfried Michael Koenig ...................................................................62\n8. Meeting 8, Approaches: Permutations, Generators, and Chaos .........................................72\n9. Meeting 9, History: Lejaren Hiller ......................................................................................82\n10. Meeting 10, Approaches: Probability and Markov Chains ................................................96\n11. Meeting 11, Workshop .......................................................................................................118\n12. Meeting 12, History: Iannis Xenakis................................................................................ 125\n13. Meeting 13, Approaches: Non-Standard Synthesis.......................................................... 136\n14. Meeting 14, Approaches: Granular and Concatenative Synthesis ................................... 152\n15. Meeting 15, Approaches: Mapping, Sonification, and Data Bending............................. 165\n16. Meeting 16, Workshop...................................................................................................... 173\n17. Meeting 17, Approaches: Cellular Automata ................................................................... 174\n18. Meeting 18, Approaches: Genetic Algorithms ................................................................. 197\n19. Meeting 19, Approaches: Grammars and L-Systems....................................................... 206\n20. Meeting 20, History: Mechanical Musical Automata...................................................... 223\n21. Meeting 21, Workshop...................................................................................................... 254\n22. Meeting 22, Approaches: Agents and Ecological Models............................................... 255\n23. Meeting 23, Approaches: Expert Systems and Style Emulation ..................................... 263\n24. Meeting 24, Discussion: Aesthetics and Evaluations...................................................... 267\n25. Meeting 25........................................................................................................................ 278\n26. Meeting 26........................................................................................................................ 279\nA. 21M.380: Quiz 1 ................................................................................................................. 280\nB. 21M.380: Quiz 2................................................................................................................. 281\nC. 21M.380: Quiz 3................................................................................................................. 282\nD. 21M.380: Quiz 4 ................................................................................................................ 283\nE. 21M.380: Quiz 5................................................................................................................. 284\nReferences ............................................................................................................................. 285\niii\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec_ref.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/5b647dd599011daa3c1bc62e0c65f928_MIT21M_380S10_lec_ref.pdf",
      "content": "References\nAmes, C. 1987. \"Automated Composition in Retrospect: 1956-1986.\" Leonardo 20(2): 169-185.\nAmes, C. 1989. \"The Markov Process as a Compositional Model: A Survey and Tutorial.\" Leonardo\n22(2): 175-187.\nAmes, C. 1991. \"A Catalog of Statistical Distributions: Techniques for Transforming Random,\nDeterminate and Chaotic Sequences.\" Leonardo Music Journal 1(1): 55-70.\nAmes, C. 1992. \"A Catalog of Sequence Generators: Accounting for Proximity, Pattern, Exclusion,\nBalance and/or Randomness.\" Leonardo Music Journal 2(1): 55-72.\nAnders, T. and E. R. Miranda. 2009. \"Interfacing Manual and Machine Composition.\" Contemporary\nMusic Review 28(2): 133-147.\nAriza, C. 2005a. An Open Design for Computer-Aided Algorithmic Music Composition: athenaCL. Ph.D.\nDissertation, New York University.\nAriza, C. 2005b. \"Navigating the Landscape of Computer-Aided Algorithmic Composition Systems:\nA Definition, Seven Descriptors, and a Lexicon of Systems and Research.\" In Proceedings of the\nInternational Computer Music Conference. San Francisco: International Computer Music Association.\n765-772.\nAriza, C. 2005c. \"The Xenakis Sieve as Object: A New Model and a Complete Implementation.\"\nComputer Music Journal 29(2): 40-60.\nAriza, C. 2006. \"Beyond the Transition Matrix: A Language-Independent, String-Based Input\nNotation for Incomplete, Multiple-Order, Static Markov Transition Values.\" Internet:\nhttp://www.flexatone.net/docs/btmimosmtv.pdf.\nAriza, C. 2007a. \"Automata Bending: Applications of Dynamic Mutation and Dynamic Rules in\nModular One-Dimensional Cellular Automata.\" Computer Music Journal 31(1): 29-49.\nAriza, C. 2007b. \"Serial RSS Sound Installation as Open Work: The babelcast.\" In Proceedings of the\nInternational Computer Music Conference. San Francisco: International Computer Music Association.\n1: 275-278.\nAriza, C. 2009a. \"The Interrogator as Critic: The Turing Test and the Evaluation of Generative\nMusic Systems.\" Computer Music Journal 33(2): 48-70.\nAriza, C. 2009b. \"Pure Data Object Glossary.\" Internet: http://flexatone.net/docs/pdg.\nAriza, C. 2010. \"Two Experiments in the Early History of Computer-Aided Algorithmic\nComposition.\"\nAssayag, G. and C. Rueda, M. Laurson, C. Agon, O. Delerue. 1999. \"Computer-Assisted\nComposition at IRCAM: From PatchWork to OpenMusic.\" Computer Music Journal 23(3): 59-72.\n\nBabbitt, M. 1958. \"Who Cares if you Listen.\" High Fidelity 8(2): 38.\nBel, B. 1998. \"Migrating Musical Concepts: An Overview of the Bol Processor.\" Computer Music\nJournal 22(2): 56-64.\nBen-Tal, O. and J. Berger. 2004. \"Creative Aspects of Sonification.\" Leonardo Music Journal 37(3):\n229-232.\nBerg, P. and R. Rowe, D. Theriault. 1980. \"SSP and Sound Description.\" Computer Music Journal 4(1):\n25-35.\nBerg, P. 1996. \"Abstracting the Future: The Search for Musical Constructs.\" Computer Music Journal\n20(3): 24-27.\nBerg, P. 2003. Using the AC Toolbox. Den Haag: Institute of Sonology, Royal Conservatory.\nBerg, P. 2009. \"Composing Sound Structures with Rules.\" Contemporary Music Review 28(1): 75-87.\nBeyls, P. 1989. \"The Musical Universe of Cellular Automata.\" In Proceedings of the International\nComputer Music Conference. San Francisco: International Computer Music Association. 34-41.\nBoulanger, R. C. 2000. The Csound Book: Perspectives in Software Synthesis, Sound Design, Signal Processing,\nand Programming. Cambridge: MIT Press.\nBurt, W. 1996. \"Some Parentheses Around Algorithmic Composition.\" Organised Sound 1(3): 167\n172.\nChadabe, J. 1997. Electric Sound: The Past and Promise of Electronic Music. New Jersey: Prentice-Hall.\nChareyron, J. 1988. \"Sound Synthesis and Processing by Means of Linear Cellular Automata.\"\nUnpublished poster presented at the International Computer Music Conference.\nChareyron, J. 1990. \"Digital Synthesis of Self-Modifying Waveforms by Means of Linear Automata.\"\nComputer Music Journal 14(4): 25-41.\nChernoff, J. M. 1979. African Rhythm and African Sensibility. Chicago: University of Chicago Press.\nChilds, E. 2002. \"Achorripsis: A Sonification of Probability Distributions.\" Proceedings of the 2002\nInternational Conference on Auditory Display.\nCollins, N. 2006. Towards Autonomous Agents for Live Computer Music: Realtime Machine Listening and\nInteractive Music Systems. Ph.D. thesis, University of Cambridge.\nCollins, N. 2008. \"Infno: Generating Synth Pop and Electronic Dance Music On Demand.\" In\nProceedings of the International Computer Music Conference. San Francisco: International Computer\nMusic Association.\nCollins, N. 2009. \"Musical Form and Algorithmic Composition.\" Contemporary Music Review 28(1):\n103-114.\n\nCope, D. 1992. \"Computer Modeling of Musical Intelligence in EMI.\" Computer Music Journal 16(2):\n69-83.\nCope, D. 1996. Experiments in Musical Intelligence. Madison, WI: A-R Editions.\nCope, D. 2000. The Algorithmic Composer. Madison, WI: A-R Editions.\nCope, D. 2001. Virtual Music: Computer Synthesis of Musical Style. Cambridge: MIT Press.\nCope, D. 2004. \"A Musical Learning Algorithm.\" Computer Music Journal 28(3): 12-27.\nCope, D. 2005. Computer Models of Musical Creativity. Cambridge: MIT Press.\nDoornbusch, P. 2002. \"Composers Views on Mapping in Algorithmic Composition.\" Organised\nSound 7(2): 145-156.\nEbcioglu, K. 1988. \"An Expert System for Harmonizing Four-part Chorales.\" Computer Music Journal\n12(3): 43-51.\nEco, U. 1989. The Open Work. Translated by A. Cancogni. Cambridge: Harvard University Press.\nFarbood, M. and H. Kaufman, K. Jennings. 2007. \"Composing with Hyperscore: An Intuitive\nInterface for Visualizing Musical Structure.\" In Proceedings of the International Computer Music\nConference. San Francisco: International Computer Music Association. 2: 111-117.\nGardner, M. 1974. \"Mathematical Games: The Arts as Combinatorial Mathematics, or, How to\nCompose Like Mozart with Dice.\" Scientific American 231(6): 132-136.\nHarnad, S. 2000. \"Minds, Machines and Turing.\" Journal of Logic, Language and Information 9(4): 425\n445.\nHedges, S. A. 1978. \"Dice Music in the Eighteenth Century.\" Music and Letters 180-187.\nHiller, L. 1956. \"Abstracts: Some Structural Principles of Computer Music.\" Journal of the American\nMusicological Society 9(3): 247-248.\nHiller, L. 1970. \"Music Composed with Computers: An Historical Survey.\" In The Computer and\nMusic. H. B. Lincoln, ed. Ithaca: Cornell University Press. 42-96.\nHiller, L. 1981. \"Composing with Computers: A Progress Report.\" Computer Music Journal 5(4): 7-21.\nHiller, L. and L. Isaacson. 1959. Experimental Music. New York: McGraw-Hill.\nHoffman, P. 2000. \"A New GENDYN Program.\" Computer Music Journal 24(2): 31-38.\nHoffman, P. 2002. \"Towards an 'Automated Art': Algorithmic Processes in Xenakis'\nCompositions.\" Contemporary Music Review 21(2-3): 121-131.\nHofstadter, D. R. 1979. Godel, Escher, Bach: an eternal golden braid . New York: Vintage.\n\nKoenig, G. M. 1968. \"Remarks on Composition Theory.\"\nKoenig, G. M. 1970a. \"Project One.\" In Electronic Music Report. Utrecht: Institute of Sonology. 2: 32\n46.\nKoenig, G. M. 1970b. \"Project Two - A Programme for Musical Composition.\" In Electronic Music\nReport. Utrecht: Institute of Sonology. 3.\nKoenig, G. M. 1971. \"The Use of Computer Programs in Creating Music.\" In Music and Technology\n(Proceedings of the Stockholm Meeting organized by UNESCO). Paris: La Revue Musicale. 93-115.\nKoenig, G. M. 1983. \"Aesthetic Integration of Computer-Composed Scores.\" Computer Music Journal\n7(4): 27-32.\nKoenig, G. M. 1991. \"Working with 'Project One': My Experiences with Computer Composition.\"\nInterface 20(3-4): 175-180.\nKurzweil, R. 1990. The Age of Intelligent Machines. Cambridge: MIT Press.\nLaske, O. 1973. \"In Search of a Generative Grammar for Music.\" Perspectives of New Music 12(1): 351\n378.\nLovelace, A. 1842. \"Translator's notes to an article on Babbage's Analytical Engine.\" In Scientific\nmemoirs: selected from the transactions of foreign academies of science and learned societies, and from foreign\njournals. R. Taylor, ed. London: printed by Richard and John E. Taylor. 3: 691-731.\nLuque, S. 2006. Stochastic Synthesis: Origins and Extensions. Masters Thesis, Institute of Sonology.\nMagnus, C. 2004. \"Evolving electroacoustic music: the application of genetic algorithms to time-\ndomain waveforms.\" In Proceedings of the International Computer Music Conference. San Francisco:\nInternational Computer Music Association. 173-176.\nManousakis, S. 2006. Musical L-Systems. Masters Thesis, Institute of Sonology.\nMarino, G. and M. Serra, J. Raczinski. 1993. \"The UPIC System: Origins and Innovations.\"\nPerspectives of New Music 31(1): 258-269.\nMcCartney, J. 1996. \"SuperCollider: a New Real Time Synthesis Language.\" In Proceedings of the\nInternational Computer Music Conference. San Francisco: International Computer Music Association.\nMcCartney, J. 1998. \"Continued Evolution of the SuperCollider Real Time Synthesis Environment.\"\nIn Proceedings of the International Computer Music Conference. San Francisco: International Computer\nMusic Association.\nMcCracken, D. 1955. \"Monte Carlo Method.\" Scientific American 192(5): 90-96.\nMiranda, E. R. 1995. \"Granular Synthesis of Sounds by Means of a Cellular Automaton.\" Leonardo\n28(4): 297-300.\n\nMiranda, E. R. 2000. Composing Music With Computers. Burlington: Focal Press.\nMiranda, E. R. 2002. \"Emergent Sound Repertoires in Virtual Societies.\" Computer Music Journal\n26(2): 77-90.\nMiranda, E. R. 2003. \"On the Music of Emergent Behavior: What Can Evolutionary Computation\nBring to the Musician?.\" Leonardo 36(1): 55-59.\nMozart, W. A. 1793. Anleitung zum Componiren von Walzern so viele man will vermittlest zweier Wurfel ohne\netwas von der Musik oder Composition zu verstehen. Berlin: Juhan Julius Hummel.\nOlson, H. F. and H. Belar. 1961. \"Aid to Music Composition Employing a Random Probability\nSystem.\" Journal of the Acoustical Society of America 33(9): 1163-1170.\nPinkerton, R. C. 1956. \"Information Theory and Melody.\" Scientific American 194(2): 77-86.\nPrusinkiewicz, P. and A. Lindenmayer. 1990. The Algorithmic Beauty of Plants (The Virtual Laboratory).\nLondon: Springer Verlag.\nPuckette, M. 1985. \"A real-time music performance system.\" MIT Experimental Music Studio.\nPuckette, M. 1988. \"The Patcher.\" In Proceedings of the International Computer Music Conference. San\nFrancisco: International Computer Music Association. 420-429.\nPuckette, M. 1997. \"Pure Data.\" In Proceedings of the International Computer Music Conference. San\nFrancisco: International Computer Music Association. 224-227.\nPuckette, M. 2002. \"Max at 17.\" Computer Music Journal 26(4): 31-43.\nRiskin, J. 2003. \"The Defecating Duck, or, the Ambiguous Origins of Artificial Life.\" Critical Inquiry\n29(4): 599-633.\nRoads, C. 1979. \"Grammars as Representations for Music.\" Computer Music Journal 3(1): 48-55.\nRoads, C. 1980. \"Interview with Max Mathews.\" Computer Music Journal 4(4): 15-22.\nRoads, C. 1988. \"Introduction to Granular Synthesis.\" Computer Music Journal 12(2): 11-13.\nRowe, R. 1992. \"Machine Listening and Composing with Cypher.\" Computer Music Journal 16(1): 43\n63.\nSchillinger, J. 1941. The Schillinger System of Musical Composition. New York: Carl Fischer.\nSchillinger, J. 1948. The Mathematical Basis of the Arts. New York: Carl Fischer.\nSerra, M. 1993. \"Stochastic Composition and Stochastic Timbre: GENDY3 by Iannis Xenakis.\"\nPerspectives of New Music 31(1): 236-257.\nSoldier, D. 2002. \"Eine Kleine Naughtmusik: How Nefarious Nonartists Cleverly Imitate Music.\"\nLeonardo Music Journal 12: 53-58.\n\nSowa, J. F. 1957. \"A machine to compose music.\" In Geniac Manual. New York: Oliver Garfield\nCompany.\nStandage, T. 2002. The Turk. New York: Walker & Company.\nStandage, T. 2003. \"Monster in a Box.\" Wired. Internet:\nhttp://www.wired.com/wired/archive/10.03/turk_pr.html.\nSturm, B. L. 2006. \"Adaptive Concatenative Sound Synthesis and Its Application to Micromontage\nComposition.\" Computer Music Journal 30(4): 46-66.\nTaube, H. 1997. \"An Introduction to Common Music.\" Computer Music Journal 21(1): 29-34.\nTaube, H. 2004. Notes from the Metalevel: An Introduction to Computer Composition. Amsterdam: Swets &\nZeitlinger Publishing.\nTipei, S. 1989. \"Manifold Compositions: A (Super)Computer-Assisted Composition Experiment in\nProgress.\" In Proceedings of the International Computer Music Conference. San Francisco: International\nComputer Music Association. 324-327.\nTruax, B. 1985. \"The PODX System: Interactive Compositional Software for the DMX-1000.\"\nComputer Music Journal 9(1): 29-38.\nVercoe, B. 1986. CSOUND: A Manual for the Audio Processing System and Supporting Programs.\nCambridge: MIT Media Lab.\nVoss, R. F. and J. Clarke. 1978. \"1/f Noise in Music: Music from 1/f Noise.\" Journal of the Acoustical\nSociety of America 63(1): 258-263.\nWeinberg, G. and S. Driscoll. 2006. \"Toward Robotic Musicianship.\" Computer Music Journal 30(4):\n28-45.\nWimsatt, W. K. and M. C. Beardsley. 1946. \"The Intentional Fallacy.\" Sewanee Review 54: 468-488.\nWinkler, T. 1998. Composing Interactive Music. Cambridge: MIT Press.\nXenakis, I. 1955. \"La crise de la musique serielle.\" Gravesaner Blatter 1.\nXenakis, I. 1960. \"Elements of Stochastic Music.\" Gravesaner Blatter 18: 84-105.\nXenakis, I. 1965. \"Free Stochastic Music from the Computer. Programme of Stochastic music in\nFortran.\" Gravesaner Blatter 26.\nXenakis, I. 1971. \"Free stochastic Music.\" In Cybernetics, art and ideas. J. Reichardt, ed. Greenwich:\nNew York Graphic Society. 124-142.\nXenakis, I. 1985. \"Music Composition Treks.\" In Composers and the Computer. C. Roads, ed. Los Altos:\nWilliam Kaufmann, Inc.\nXenakis, I. 1987. \"Xenakis on Xenakis.\" Perspectives of New Music 25(1-2): 16-63.\n\nXenakis, I. 1990. \"Sieves.\" Perspectives of New Music 28(1): 58-78.\nXenakis, I. 1992. Formalized Music: Thought and Mathematics in Music. Indiana: Indiana University Press.\nXenakis, I. 1996. \"Determinacy and Indeterminacy.\" Organised Sound 1(3): 143-155.\nZicarelli, D. 1987. \"M and Jam Factory.\" Computer Music Journal 11(4): 13-29.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/1618a4785beed4f6c26bcb847795f00e_MIT21M_380S10_lec01.pdf",
      "content": "Chapter 1. Meeting 1, Foundations: Algorithmic and\nGenerative Music Systems\n1.1. Announcements\n- 21M.380: Music Technology: Algorithmic and Generative Music Systems\n1.2. Overview\n- The last 10 years of algorithmic and generative music systems\n- What are algorithmic and generative music systems?\n- Two examples\n- About this course\n1.3. Generative Systems: Definitions\n- Machines that make music\n- Humans that use or make machines to make music\n- Humans that use or make machines to help them make music\n- Humans that use or make machines to help them make components of their music\n1.4. A New Field of Compositional Research\n- Generative music with a computer took many names:\n- Algorithmic composition\n- Computer music\n- Score synthesis\n- Computer-aided (or -assisted) composition\n- Computer-aided algorithmic composition (CAAC)\n- A new type of generative (rather than reductive) music theory\n\n1.5. Computer-Aided Algorithmic Composition: Definition\n- A negative definition\n- A CAAC system is software that facilitates the generation of new music by means other than the\nmanipulation of a direct music representation (Ariza 2005b)\n- New music: a unique musical variant, not just as copy\n- Output may be in the form of any sound or sound parameter data, from a sequence of samples to\nthe notation of a complete composition\n- A \"direct music representation\" refers to a linear, literal, or symbolic representation of complete\nmusical events, such as an event list (a score in Western notation or a MIDI file) or an ordered list\nof amplitude values (a digital audio file or stream)\n- If the representation provided to the user is the same as the output, the representation may\nreasonably be considered direct.\n- Anything that is not a direct representation employs CAAC\n1.6. A Wide Range of Interactions and Collaborations\n- Machines can be used to create complete structucres\n- Machines can be used to create small fragments that are manually integrated\n- Machines can be used to create guidelines, contexts, or situations for human music making\n1.7. Two Examples\n- I: Minuets and Contredances\n- II: babelcast\n1.8. I: Minuets and Contredances\n- Minuet: a French dance in moderate triple meter, popular in aristocratic society from mid 17th\ncentury to late 18th century (Grove Music Online)\n- Textbook composition method: two or four bar groups, each section being 8 or 16 bars long\n- Audio played in class: Bach: Minuet in G, MWV Anh 114\n\n- Audio played in class: Mozart: Minuet in G, K. 1\n1.9. I: Minutes and Contredances: Musical Dice Games\n- 1757-1812: at least 20 musical dices games published (Kirnberger, CPE Bach, J Haydn, Mozart,\nothers)\n- Musical composition game, one of many 18th-century parlor games (Hedges 1978, p. 180)\n- A table is used to translate the sum of two dice to appropriate score positions\n- Score positions specify complete measure-length segments for each possible phrase position\n- German composer Kirnberger published one of the first in 1757\n\n- Numerous versions of Musikalisches Wurfelspiel attributed to Mozart\n- The version attributed to Mozart was first published two years after his death by Juhan Julius\nHummel (1793) and includes two similar games: one for Minuets and another for contredances\n- Two 8-bar phrases are created from combining 176 pre-composed measures\n- The last bar of each phrase always uses the same measure\n1.10. I: Minuets and Contredances: The First Computer\nImplementation\n- 1955: David Caplin and Dietrich Prinz write a program to generate and synthesize the Mozart\nDice Game for contredances on a Ferranti Mark 1* (MIRACLE) at Shell laboratories in\nAmsterdam (Ariza 2010)\n- Likely the first use of a computer to generate music\n- Ferranti Mark 1* (MIRACLE)\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nAudio sample played in class.\n1.11. I: Minutes and Contredances: Motivations and Meanings\n- Why do this? How is this possible?\n- Is new music being made?\n- What meaning, if any, is conveyed?\n1.12. II: The babelcast\n- An algorithmic, computer generated podcast series (Ariza 2007b)\nAudio RSS URL: (http://www.flexatone.net/babelcast.xml)\nVideo RSS URL: (http://www.flexatone.net/babelcast-zoetrope.xml)\n- First released 5 August 2005, around one episode a month since\n- Created with athenaCL, Python, and Csound\n- Distributed in three formats: mp3, (-mosaic) m4a, and (-zoetrope) m4v\n1.13. II: The babelcast: Information Abduction and Reduction\n- Gather sounds of politicians and political commentators\n- Gather images of politicians and political commentators\n- Favor primary sources\n- Favor massively redundant surplus media: images and sounds that are obtained by many sources\n1.14. II: The babelcast: The Process\n- Sounds are manually collected with minimal editing\n- images are automatically downloaded and then manually filtered\n- Around 40 Texture-generating procedures for athenaCL are configured for each episode\n- Some Textures create noises\n\n- Some Textures process samples\n- Csound instruments use vocoders, granular synthesis methods, and other techniques\n- Between 100 and 200 Textures are generated and mixed into a single audio file\n- Images are randomly selected, cropped, and zoomed\n1.15. II: Listening\n- babelcast-zoetrope-2009.12.27\n(http://www.flexatone.net/video/m4v/babelcast-zoetrope-2009.12.27.m4v)\n1.16. II: The babelcast: Precedents\n- 1989: Umberto Ecco, The Open Work\n- Leaving parts of a work to chance\n- Works that \"reject the definitive, concluded message and multiply the formal possibilities of the\ndistribution of their elements\" (Eco 1989, p. 3).\n- 1986: William Gibson, Count Zero\n- Artificial intelligence that sends randomly constructed human junk, found in space, back down\nto earth, which is assumed to be forged works of artists Joseph Cornell\n- American \"assemblage\" artist Joseph Cornell (1903-1972)\n- Cornell: Object (Roses des Vents) (1942-53)\n\n1.17. II: The babelcast: Motivations and Meanings\n- Why do this?\n- What meaning, if any, is conveyed?\n(c) The Joseph and Robert Cornell Memorial Foundation / Visual Artists\nand Galleries Association, Inc. (VAGA). This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\n1.18. 21M.380: Objectives\n- To gain a critical understanding of the history, techniques, and designs of algorithmic and\ngenerative music systems\n- To develop musical creativity and expression in the use and design of algorithmic and generative\nmusic systems\n- To critically evaluate claims of aesthetic and technological advancement, quality, and promise\n1.19. 21M.380: Areas of Focus\n- History: Mechanical Musical Automata, Serialism, Phasing, Gottfried Michael Koenig, Lejaren\nHiller, Iannis Xenakis\n- Approaches: Distributions and Stochastics, Probability and Markov Chains, Cellular Automata,\nGenetic Algorithms, Grammars and L-Systems, Agents and Ecological Models, Expert Systems\nand Style Emulation, Non-Standard Synthesis, Granular and Concatenative Synthesis, Mapping,\nSonification, and Data Bending\n- Workshops and Discussion\n1.20. 21M.380: Prerequisites\n- None but curiosity, willingness to experiment\n- Programming in Python or other languages useful, but not required\n- Experience with digital audio and DAW software desirable, but not required\n1.21. 21M.380: Course Meetings and Materials\n- Syllabus:\n- Two types of meetings\n- Topic meetings: focused on material in readings, listening, and themes, combining lecture,\ndiscussion, demonstration, and listening\n- Workshop meetings: focus on discussion of projects and techniques, hands-on experimentation\n- If possible, bring laptops to all class meetings\n- Software: core tools\n- athenaCL\n\n- Python\n- Csound\n- SuperCollider\n- PD\n- DAWs and virtual instruments\n- Lecture notes\n1.22. 21M.380: Assignments: Reading\n- Numerous carefully selected readings\nAmes, C. 1987. \"Automated Composition in Retrospect: 1956-1986.\" Leonardo 20(2): 169-185.\nAmes, C. 1992. \"A Catalog of Sequence Generators: Accounting for Proximity, Pattern,\nExclusion, Balance and/or Randomness.\" Leonardo Music Journal 2(1): 55-72.\nAmes, C. 1991. \"A Catalog of Statistical Distributions: Techniques for Transforming Random,\nDeterminate and Chaotic Sequences.\" Leonardo Music Journal 1(1): 55-70.\nAmes, C. 1989. \"The Markov Process as a Compositional Model: A Survey and Tutorial.\"\nLeonardo 22(2): 175-187.\nAriza, C. 2007a. \"Automata Bending: Applications of Dynamic Mutation and Dynamic Rules in\nModular One-Dimensional Cellular Automata.\" Computer Music Journal 31(1): 29-49. Internet:\nhttp://www.mitpressjournals.org/doi/abs/10.1162/comj.2007.31.1.29.\nAriza, C. 2006. \"Beyond the Transition Matrix: A Language-Independent, String-Based Input\nNotation for Incomplete, Multiple-Order, Static Markov Transition Values.\" Internet:\nhttp://www.flexatone.net/docs/btmimosmtv.pdf.\nAriza, C. 2009a. \"The Interrogator as Critic: The Turing Test and the Evaluation of Generative\nMusic Systems.\" Computer Music Journal 33(2): 48-70. Internet:\nhttp://www.mitpressjournals.org/doi/abs/10.1162/comj.2009.33.2.48.\n\nAriza, C. 2005b. \"Navigating the Landscape of Computer-Aided Algorithmic Composition\nSystems: A Definition, Seven Descriptors, and a Lexicon of Systems and Research.\" In Proceedings\nof the International Computer Music Conference. San Francisco: International Computer Music\nAssociation. 765-772. Internet: http://www.flexatone.net/docs/nlcaacs.pdf.\nAriza, C. 2005c. \"The Xenakis Sieve as Object: A New Model and a Complete Implementation.\"\nComputer Music Journal 29(2): 40-60. Internet:\nhttp://www.mitpressjournals.org/doi/abs/10.1162/0148926054094396.\nBen-Tal, O. and J. Berger. 2004. \"Creative Aspects of Sonification.\" Leonardo Music Journal 37(3):\n229-232.\nBerg, P. 2009. \"Composing Sound Structures with Rules.\" Contemporary Music Review 28(1): 75-87.\nBiles, J. A. 2003. \"GenJam in Perspective: A Tentative Taxonomy for GA Music and Art\nSystems.\" Leonardo 36(1): 43-45.\nCope, D. 1992. \"Computer Modeling of Musical Intelligence in EMI.\" Computer Music Journal\n16(2): 69-83.\nEbcioglu, K. 1988. \"An Expert System for Harmonizing Four-part Chorales.\" Computer Music\nJournal 12(3): 43-51.\nHiller, L. and L. Isaacson. 1958. \"Musical Composition with a High-Speed Digital Computer.\"\nJournal of the Audio Engineering Society 6(3): 154-160.\nHoffman, P. 2000. \"A New GENDYN Program.\" Computer Music Journal 24(2): 31-38.\nKoenig, G. M. 1971. \"The Use of Computer Programs in Creating Music.\" In Music and Technology\n(Proceedings of the Stockholm Meeting organized by UNESCO). Paris: La Revue Musicale. 93-115.\nInternet: http://www.koenigproject.nl/Computer_in_Creating_Music.pdf.\nKoenig, G. M. 1983. \"Aesthetic Integration of Computer-Composed Scores.\" Computer Music\nJournal 7(4): 27-32.\nMagnus, C. 2004. \"Evolving electroacoustic music: the application of genetic algorithms to time-\ndomain waveforms.\" In Proceedings of the International Computer Music Conference. San Francisco:\nInternational Computer Music Association. 173-176.\nMarino, G. and M. Serra, J. Raczinski. 1993. \"The UPIC System: Origins and Innovations.\"\nPerspectives of New Music 31(1): 258-269.\nMason, S. and M. Saffle. 1994. \"L-Systems, Melodies and Musical Structure.\" Leonardo Music\nJournal 4: 31-38.\nMiranda, E. R. 2003. \"On the Music of Emergent Behavior: What Can Evolutionary\nComputation Bring to the Musician?.\" Leonardo 36(1): 55-59.\n\nRiskin, J. 2003. \"The Defecating Duck, or, the Ambiguous Origins of Artificial Life.\" Critical\nInquiry 29(4): 599-633.\nRoads, C. 1988. \"Introduction to Granular Synthesis.\" Computer Music Journal 12(2): 11-13.\nRowe, R. 1992. \"Machine Listening and Composing with Cypher.\" Computer Music Journal 16(1):\n43-63.\nSerra, M. 1993. \"Stochastic Composition and Stochastic Timbre: GENDY3 by Iannis Xenakis.\"\nPerspectives of New Music 31(1): 236-257.\nSoldier, D. 2002. \"Eine Kleine Naughtmusik: How Nefarious Nonartists Cleverly Imitate Music.\"\nLeonardo Music Journal 12: 53-58.\nSturm, B. L. 2006. \"Adaptive Concatenative Sound Synthesis and Its Application to\nMicromontage Composition.\" Computer Music Journal 30(4): 46-66.\nVoss, R. F. and J. Clarke. 1978. \"1/f Noise in Music: Music from 1/f Noise.\" Journal of the\nAcoustical Society of America 63(1): 258-263.\nXenakis, I. 1971. \"Free stochastic Music.\" In Cybernetics, art and ideas. J. Reichardt, ed. Greenwich:\nNew York Graphic Society. 124-142.\nXenakis, I. 1987. \"Xenakis on Xenakis.\" Perspectives of New Music 25(1-2): 16-63.\n1.23. 21M.380: Assignments: Listening\n- Reading notation and scores not required\n- Take notes when you listen\n- What to listen for: duration, instrumentation, method of production, recording or performance\ncontext, notable sonic events, form, temporal design and proportions, aesthetic or historical\ncontexts, and/or critical and subjective responses\n1.24. 21M.380: Assignments: Discussion Leaders\n- Students are assigned to cover each reading and listening assignments for each class\n- Must be available to lead discussion, answer questions, and provide a resource to class\n- Must post minimal notes in the class website forum: Reading and Listening Notes\n\n1.25. 21M.380: Assignments: Musical Design Report\n- An original sonic sketch or musical work, lasting from two to five minutes, realized in notation,\nMIDI, digital audio, or code, and based on approaches, techniques, and/or models presented for\neach assignment\n- Includes a very short written report describing approaches and design\n- A group of 3 to 4 students will be selected to present their projects to the class during Workshop\nsessions\n- Three spaced evenly throughout the semester\n1.26. 21M.380: Assignments: Sonic System Project and Presentation\n- An original sonic system that functions as either a generative instrument with or without a\nperformance interface or as a static or dynamic musical work employing techniques and/or tools\nof algorithmic composition.\n- May explore any software or hardware system or interface; can extend class examples or produce\ncompletely original works\n- Includes a short written report describing approaches and design\n- Draft workshop meeting: 27 April\n- Final presentations: 11 and 13 May\n1.27. 21M.380: Assignments: Submission\n- All assignments are submitted digitally via email attachment (or as Forum posts)\n- All assignments, except as noted, are due at 11:59:59 PM on due date\n- Late within 1 week: 20% reduction; no assignments accepted after 1 week\n1.28. 21M.380: Attendance\n- Mandatory and essential\n- More than one unexcused absence incurs a 3% grade reduction\n1.29. 21M.380: Exams and Quizzes\n- Quizzes will be announced, and frequent\n\n- All short written answers\n- Quizzes will be based on reading, listening, and course content\n- No final exam\n1.30. 21M.380: Grading\n- Reading and Listening Discussion Leader: 20%\n- Musical Design Report (3): 30%\n- Sonic System Project and Presentation: 20%\n- Sonic System Project Draft: 5%\n- Quizzes: 15%\n- Participation: 10%\n1.31. 21M.380: Additional Policies\n- Read entire syllabus\n- Common courtesies\n- Computers in class\n- Academic integrity\n1.32. 21M.380: Contact\n- Always feel free to contact me with any problem or concern with this class\n1.33. Us\n- Backgrounds, experiences, goals\n\n1.34. For Next Class\n- Download and read entire syllabus\n- Respond to my email questionnaire\n- Bring computers\n[pp. 17-19 deleted from these notes, due to privacy considerations]\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/5ef90e6fc258b0c193609dc1b62b19f4_MIT21M_380S10_lec02.pdf",
      "content": "Chapter 2. Meeting 2, Foundations: Musical Parameters,\nMappings, and Tools\n2.1. Announcements\n- If you have not downloaded and installed Python and PD-Extended, please do so now\n- Download: most recent athenaCL\nhttp://code.google.com/p/athenacl\n2.2. Overview\n- Events\n- Parameters\n- Containers\n- Instruments\n- Generative software tools\n- athenaCL and Python\n- Digital Audio Workstations\n2.3. Musical Events\n- The event is the fundamental unit of music\n- An event can be single sample lasting 0.0000227 seconds\n- An event can be a note\n- An event can be a continuous sound encompassing a complete work\n- The minimum definition of an event is a start and end time\n\n2.4. Events and Parameters\n- An event can be described as with one or more parameters\n- Parameters may be duration, pitch, amplitude, or any other collection of specifiers\n- Parameters may be coordinated or independent\n- Human musical production often coordinates parameters\n- Independent musical paramerers can make interest musical structures\n- The parameterization of musical events has been critical to the development of modern music\n2.5. Event Lists\n- Events, defined by an array of parameters, can be collected in a list\n- Musical data is stored in various arrangements of event lists\n2.6. Fundamental Musical Parameters\n- Duration and rhythm\n- Frequency and pitch\n- Amplitude and dynamics\n2.7. Parameters: Duration and Rhythm\n- Can be measured in absolute or relative values\n- Absolute values: seconds, milliseconds\n- Relative values\n- Notation: quarter, sixteenth, whole\n- Pulse triples: (divisor, multiplier, accent)\n- Relative values proportional to a beat rate (tempo)\n- Tempi are often thought of in beats per minute (BPM)\n- A range of durations at different tempi [py/demo/parameterDuration.py]\n\n2.8. Parameters: Frequency and Pitch\n- Pitch is a human interpretation of frequency\n- Pitch asserts the octave as referential unit of equivalence\n- An octave is 12 half steps, 8 diatonic steps (white notes on the piano), and a 2:1 frequency ratio\n- Numerous other distances between pitches (intervals) have names: fifths, thirds, 13ths, quarter\ntones\n\n- Pitch names can carry octave designation, where C4 is middle C\n- MIDI pitch values place C4 at 60, use 1 as a half step, and range from 0 to 127\n- athenaCL pitch space values place C4 at 0 and use 1 as a half step\n- A range of fundamental pitches [py/demo/parameterPitch.py]\n- The (ideal) audible frequency range: 20 Hz (MIDI 16, E0) to 20000 Hz (MIDI 135, D#10)\n- Top three octaves (from 3-6k, 6-12k, 12-24k) contain spectral frequencies\n2.9. Parameters: Amplitude and Dynamics\n- Bits: discrete digital audio amplitude levels\n- dB SPL: acoustic power\n- dBv: voltage amplitude\n- Unit interval spacings: between 0 and 1\n- Notation: from ppp to fff\n\n- MIDI velocity values from 0 to 127\n- A range of amplitude levels [py/demo/parameterAmplitude.py]\n2.10. Storing Event Lists in Containers\n- Events can be streamed in real-time or stored in containers\n- Western notation (scores, MusicXML)\n- Musical Insturment Digital Interface (MIDI)\n- Open sound control (OSC)\n- Digital audio files\n2.11. Containers: Western Notation\n- Events are organized around notes that specify pitch and duration\n- Parameter values are limited (mostly) to symbols\n- Parameters are isolated for instruments by staves\n- Parallel staves express simultaneous events\n- Timbral specification relies mostly on instrument assignments\n\n- MusicXML offers a standard for encoding notation\n- Software permits opening, editing, and playing MusicXML files\n- Finale and Sibelius\n- Finale reader\nhttp://www.finalemusic.com/Reader\n2.12. Containers: MIDI\n- A binary representation of musical parameters\n- Parameter values are often 7 bit, or 128 discrete values\n- Parameters are isolated by numerical tags, called channels\n- Timbral specification relies mostly on instrument assignments (programs)\n- Software permits performing MIDI files\n- QuickTime and Windows Media Player\n- Virtual instruments\n2.13. Containers: OSC\n- A hierarchical representation of musical parameters\n- Parameter values can be numbers or strings\n- Parameters are organized hierarchically with URL-like syntax\n- Timbral specification relies mostly on receiving device\n- Sending and receiving OSC data\n- Hardware controllers\n- Software controllers\n2.14. Containers: Digital Audio\n- A micro mono-parameter representation\n- Store amplitude values within a dynamic range taken at a sampling rate\n\n- Signals can be mixed or stored in isolated channels\n- Digital audio is a timbral specification\n- Software permits playing and editing digital audio\n- QuickTime and Windows Media Player\n- Audacity\nhttp://audacity.sourceforge.net\n- Digital Audio Workstations\n2.15. Synthesizers, Samplers, and Virtual Instruments\n- Acoustic instruments translate parameters into acoustic sound\n- Electronic instruments synthesize tones with oscillators or stored samples\n- Digital electronic instruments are built by combining basic software components\n- Virtual instruments are software synthesizers or samplers that respond to MIDI or OSC\nparameters\n2.16. Digital Synthesizers\n- Built from combing fundamental signal generators and processors (unit generators or Ugens)\n- Can be designed to accept any number of initial event parameters\n- Can be designed to accept dynamic parameters over the course of an event\n2.17. Digital Synthesis Languages: Csound\n- Developed in part from the first synthesis language Music 1 in 1957 (Roads 1980)\n- Extended and ported by Barry Vercoe at MIT (1986)\n- A huge library of processors and instrument models (Boulanger 2000)\n- A low-level language for defining instruments\n- A flat list of data for event lists\n\n2.18. Digital Synthesis Languages: PureData\n- Over 20 years of development in synthesis, sampling, and a visual programming envrionment\n- Numerous related alternatives: Max/MSP, jMax, Open Sound World (OSW)\n- Developed by Miller Puckette, creator of the first Max (Puckette 1985, 1988, 1997, 2002)\n2.19. Digital Synthesis Languages: SuperCollider\n- An extension of Csound archetypes into a modern language and network archetype\n- First released in 1996 by James McCartney (McCartney 1996; McCartney 1998)\n- A complete object-oriented language: create objects, manipulate, and reuse code\n- A server-based architecture: SynthDefs live on a server and send and receive messages and signals\n- Designed for real-time performance and experimentation\n2.20. Virtual Instruments\n- Software plug-ins that can receive MIDI or OSC messages\n- Distributed as VST, AU, or other plug-in formats\n- Can employ any internal software and synthesis model\n2.21. Algorithmic Composition and Generative Music Systems\n- May be built within a synthesis language\n- May be stand-alone systems\n- Numerous systems support multiple output formats from a single interface\n- athenaCL\nhttp://code.google.com/p/athenacl\n- AC Toolbox: Lisp based Macintosh application/environment\nhttp://www.koncon.nl/downloads/ACToolbox/\n- Open Music: Lisp based visual programming language\n\nhttp://recherche.ircam.fr/equipes/repmus/OpenMusic/\n- Common Music: Lisp based programming language\nhttp://commonmusic.sourceforge.net\n2.22. A Brief History of athenaCL\n- Started as a way of automating the production of Csound scores in 2001\n- Originally attempted to integrate a variety of post-tonal music theory tools\n- Gradually became a more general tool for composition\n- A way to test and deploy modular approaches to generating music parameters and structures\n- Support for output in MIDI, SuperCollider, and other formats incrementally added\n- Version 2 strips away post-tonal music theory tools, focuses on compositional tasks\n- Present alpha releases may have bugs: please report any problems to me immediately\n2.23. Installing and Running athenaCL\n- Download the most-recent version\n- A distribution from Google Code\nhttp://code.google.com/p/athenacl\n- Via SVN command-line argument:\nsvn checkout http://athenacl.googlecode.com/svn/trunk/ athenacl-read-only\n- Install in Python's site packages\n- Windows: run athenaCL.exe installer\n- Others: extract athenaCL.tar.gz\nWith terminal, cd to athenaCL directory\nEnter: python setup.py install\nIf permissions error, try: sudo python setup.py install\n- Start Python\n\n- Windows: run python.exe or IDLE.py\n- Others: open terminal, enter: python\n- Start athenaCL\n- From within Python, enter: from athenaCL import athenacl\n- Others: open terminal, enter: python\nEnter: from athenaCL import athenacl\n2.24. Running athenaCL Without Installing\n- Download athenaCL (as above)\n- Launch the file athenaCL/athenacl.py with Python\n2.25. athenaCL: System Overview\n- Create and edit Textures (TextureInstances) and Paths (PathInstances)\n- Paths are static pitch collections\n- Textures are dynamic variable parameter event list generators\n- TextureModules define various approaches to create Textures\n- ParameterObjects are used to configure and generate parameters within Textures\n- EventModes define orchestras of instruments and available output formats\n- EventOutputs are output formats, some available with all EventModes, others available from only\none\n- EventLists can be created, rendered, and heard\n2.26. Interactive athenaCL Commands\n- athenaCL as an interactive command line program\n- Commands can be provided with space delimited arguments, or the user can be prompted for all\nnecessary arguments\n- Acronyms are always accepted for arguments\n\n- cmd: view all commands\n- ?: get help for any command\n- EMo: select EventMode midiPercussion\n- EMi: list available instruments\n- TIn: create a new TextureInstance (provide name and instrument number)\n- ELn: create a new EventList\n- ELh: hear (or open) a new EventList\n- Commands with full arguments and sample output\npi{}ti{} :: emo mp\nEventMode mode set to: midiPercussion.\npi{}ti{} :: tin a 50\nTI a created.\npi{auto-highTom}ti{a} :: eln\ncommand.py: temporary file: /Volumes/xdisc/_scratch/ath2010.02.04.09.45.48.xml\nEventList ath2010.02.04.09.45.48 complete:\n/Volumes/xdisc/_scratch/ath2010.02.04.09.45.48.mid\n/Volumes/xdisc/_scratch/ath2010.02.04.09.45.48.xml\npi{auto-highTom}ti{a} :: elh\nEventList hear initiated: /volumes/xdisc/_scratch/ath2010.02.04.09.48.11.mid\n- Setting the scratch directory to \"/Volumes/xdisc/_scratch\"\npi{}ti{} :: apdir x /Volumes/xdisc/_scratch\nuser fpScratchDir directory set to /volumes/xdisc/_scratch.\n- Editing the Texture's temp with a WaveSine generator (space divisions matter)\npi{auto-highTom}ti{a} :: tie b ws,t,10,0,40,400\nTI a: parameter bpm updated.\npi{auto-highTom}ti{a} :: eln; elh\n2.27. Automating athenaCL Commands with Python\n- athenaCL command can be scripted and controlled in Python script\n- Permits reuse and extensions\n- Must create an athenaCL Interpreter object and send string commands\n- Creating a Python script file\n- Windows: use IDLE.py or another text editor\n\n- Others: use emacs, vi, or other text editor\n- Mac: use TextWranger (free)\nhttp://www.barebones.com/products/TextWrangler\n- Automating the production of one Texture: create file 02a.py and run with python [02a.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\nath.cmd('tin a 45')\nath.cmd('tie b ws,t,10,0,40,400')\nath.cmd('eln')\nath.cmd('elh')\n- If Python cannot find the athenaCL directory (because you were not able to do an install) you\nmust provide to python the file path to the directory containing athenaCL\nimport sys\nsys.path.append(\"/path/to/dir/that/contains/athenacl\")\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\nath.cmd('tin a 45')\nath.cmd('tie b ws,t,10,0,40,400')\nath.cmd('eln')\nath.cmd('elh')\n- Automating the production of three Textures [02b.py]\nfrom athenaCL.libATH import athenaObj\nimport random\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\nfor x in [45, 51, 75]:\nath.cmd('tin t%s %s' % (x, x))\nath.cmd('tie t %s,%s' % (random.choice(range(0,10)),\nrandom.choice(range(20,30))))\nath.cmd(\"tie b ws,t,10,0,40,400\")\nath.cmd('eln')\nath.cmd('elh')\n- If you have trouble running a Python script on Windows, visit:\nhttp://www.python.org/doc/faq/windows/\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/81fdecbca189c8491dd8bfca72130618_MIT21M_380S10_lec03.pdf",
      "content": "Chapter 3. Meeting 3, Approaches: Distributions and\nStochastics\n3.1. Announcements\n- Download: most recent athenaCL\nhttp://code.google.com/p/athenacl\n3.2. Reading: Ames: A Catalog of Statistical Distributions\n- Ames, C. 1991. \"A Catalog of Statistical Distributions: Techniques for Transforming Random,\nDeterminate and Chaotic Sequences.\" Leonardo Music Journal 1(1): 55-70.\n- What does Ames mean by balance, and that there can be a balance that is not fair?\n- What is meant by a weight? Why is this term preferable to alternatives?\n- The use of statistics here might be considered outside of the discipline of statistics: why?\n- Which musical parameters are better suited for discrete values? Which for continuous values?\n- Are any distributions dependent on past occurrences?\n- Why might the Law of Large Numbers make working with distributions difficult in musical\ncontexts?\n- In terms of the distribution output, what are time domain and frequency domain graphs?\n- What is the relationship between the Poisson distribution and the Exponential distribution?\n- Ames notes that, when working with some distributions, values may have to be discarded: why?\nWhat does this say about working with distributions?\n3.3. ParameterObjects\n- Reusable value selectors and generators\n\n- Can be created and controlled with strings of comma-separated lists\n- Values in ParameterObjects can be strings (without quotes), numbers, or lists (delimited by\nparenthesis or brackets)\n- In some cases ParameterObjects, enclosed as a list, can be used inside of other ParameterObjects\nto generate values\n- Three types of ParameterObjects\n- Generator: produce values based on arguments alone\n- Rhythm: specialized for rhythm creation\n- Filter: specialized for transforming values produced from a Texture\n- Complete documentation for ParameterObjects, and samples, can be found here:\nhttp://www.flexatone.net/athenaDocs/www/ax03.htm\n- ParameterObject names and string values can always be provided with acronyms\n- Trailing arguments, when not provided, are automatically supplied\n3.4. ParameterObjects: Viewing Arguments and Output\n- TPls: view a list of all available ParameterObjects\n- TPv: vie detailed documentation for one or more ParameterObjects\npi{}ti{} :: tpv ru\nGenerator ParameterObject\n{name,documentation}\nRandomUniform\nrandomUniform, min, max\nDescription: Provides random numbers between 0 and 1 within an\nuniform distribution.\nThis value is scaled within the range designated by min and max;\nmin and max may be\nspecified with ParameterObjects. Note: values are evenly\ndistributed between min and\nmax. Arguments: (1) name, (2) min, (3) max\n- TPmap: create a graphical output providing a number of values and a ParameterObject name\nNote that, when providing arguments from the command-line, spaces cannot be used between\nParameterObject arguments\npi{}ti{} :: tpmap 100 ru,3,8\nrandomUniform, (constant, 3), (constant, 8)\nTPmap display complete.\n\n- With a nested ParameterObject for the maximum value\npi{}ti{} :: tpmap 100 ru,3,(ru,8,15)\nrandomUniform, (constant, 3), (randomUniform, (constant, 8), (constant, 15))\nTPmap display complete.\n3.5. Configuring Graphical Outputs in athenaCL\n- athenaCL supports numerous types of graphical outputs, some with various dependencies\n- Output formats:\n- JPG, PNG: requires working installation of the Python Imaging Library (PIL)\nWindows: http://www.pythonware.com/products/pil\nOthers: not so easy for Python 2.6 (easier for Python 2.5)\n- TK: uses the TK GUI system that ships with Python\nWorks for full installs of Python 2.6 on Windows, Mac, Others\n- EPS: works on all Pythons on all platforms\n- APgfx: set graphical output preferences\npi{}ti{} :: apgfx\nactive graphics format: png.\nselect text, eps, tk, jpg, png. (t, e, k, j, or p): p\ngraphics format changed to png.\n- Use APea to set the imageViewer and psViewer applications if not already set properly\n\n3.6. The Constant ParameterObject\n- The most simple ParameterObject\npi{}ti{} :: tpv constant\nGenerator ParameterObject\n{name,documentation}\nConstant\nconstant, value\nDescription: Return a constant string or numeric value.\nArguments: (1) name, (2)\nvalue\n3.7. Continuous and Discrete Stochastic Distributions as\nParameterObjects\n- Discrete\n- BasketGen\n- Continuous POs put through the Quantize PO or other POs\n- Continuous\n- RandomUniform\n- RandomGauss\n- RandomBeta\n- RandomExponential and RandomInverseExponential\n- Many others...\n3.8. Discrete Stochastic Distributions as ParameterObjects\n- BasketGen: the ball and urn (or basket) paradigm\n- Documentation with TPv\n\n:: tpv bg\nGenerator ParameterObject\n{name,documentation}\nBasketGen\nbasketGen, selectionString, valueList\nDescription: Chooses values from a user-supplied list\n(valueList). Values can be strings or numbers. Values are\nchosen from this list using the selector specified by the\nselectionString argument. Arguments: (1) name, (2)\nselectionString {'randomChoice', 'randomWalk',\n'randomPermutate', 'orderedCyclic',\n'orderedCyclicRetrograde', 'orderedOscillate'}, (3)\nvalueList\n- Selection methods\n- randomChoice: random selection with replacement\npi{}ti{} :: tpmap 100 bg,rc,(0,.2,.4,.6,.8,1)\nbasketGen, randomChoice, (0,0.2,0.4,0.6,0.8,1)\nTPmap display complete.\n- randomPermutate: random selection without replacement\npi{}ti{} :: tpmap 100 bg,rp,(0,.2,.4,.6,.8,1)\nbasketGen, randomPermutate, (0,0.2,0.4,0.6,0.8,1)\nTPmap display complete.\n- randomWalk: random up/down movement along order of list, with wrapping\npi{}ti{} :: tpmap 100 bg,rw,(0,.2,.4,.6,.8,1)\nbasketGen, randomChoice, (0,0.2,0.4,0.6,0.8,1)\nTPmap display complete.\n\n- orderedCyclic: looping\npi{}ti{} :: tpmap 100 bg,oc,(0,.2,.4,.6,.8,1)\nbasketGen, orderedCyclic, (0,0.2,0.4,0.6,0.8,1)\nTPmap display complete.\n- orderedOscillate: oscillating\npi{}ti{} :: tpmap 100 bg,oo,(0,.2,.4,.6,.8,1)\nbasketGen, orderedOscillate, (0,0.2,0.4,0.6,0.8,1)\nTPmap display complete.\n- By configuring the values drawn from, discrete uniform, Bernoulli, and binomial distributions can\nbe modeled\n3.9. Continuous Stochastic Distributions as ParameterObjects\n- RandomUniform: continuous uniform distribution\nscaled between 0 and 10\n\npi{}ti{} :: tpmap 100 ru,0,10\nrandomUniform, (constant, 0), (constant, 10)\nTPmap display complete.\n- RandomGauss: normal distribution, arguments mu and sigma\n- mu: center of distribution, between 0 and 1\n- sigma: deviation around center, where .001 is little deviation\n- mu at .3, sigma at .01, scaled between 0 and 10\npi{}ti{} :: tpmap 100 rg,.3,.01,0,10\nrandomGauss, 0.3, 0.01, (constant, 0), (constant, 10)\nTPmap display complete.\n- mu at .7, sigma at .2, scaled between 0 and 10\npi{}ti{} :: tpmap 100 rg,.7,.2,0,10\nrandomGauss, 0.7, 0.2, (constant, 0), (constant, 10)\nTPmap display complete.\n\n- RandomBeta: arguments alpha and beta\n- This implementation is different than Ames (1991)\n- alpha and beta: low values increase draw to boundaries\n- alpha and beta: large values approach a uniform distribution\n- alpha at .1, beta at .1, scaled between 0 and 10\npi{}ti{} :: tpmap 100 rb,0.1,0.1,0,10\nrandomBeta, 0.1, 0.1, (constant, 0), (constant, 10)\nTPmap display complete.\n- alpha at .3, beta at .3, scaled between 0 and 10\npi{}ti{} :: tpmap 100 rb,.3,.3,0,10\nrandomBeta, 0.3, 0.3, (constant, 0), (constant, 10)\nTPmap display complete.\n- RandomExponential and RandomInverseExponential\n- lambda: larger values create a tighter pull to to one boundary\n- exponential, lambda at 5, scaled between 0 and 10\npi{}ti{} :: tpmap 100 re,5,0,10\nrandomExponential, 5.0, (constant, 0), (constant, 10)\nTPmap display complete.\n\n- inverse exponential, lambda at 20, scaled between 0 and 10\npi{}ti{} :: tpmap 100 rie,20,0,10\nrandomInverseExponential, 20.0, (constant, 0), (constant, 10)\nTPmap display complete.\n- For all generators min and max can be embedded POs\n3.10. Working with athenaCL\n- Often best to use interactive mode for testing values, quick sketches, setting preferences\n- Best to use a Python script for composing or other work\n- Same preferences used in interactive mode are used in scripts\n- For examples, the presence of the command prompt designates that athenaCL is in interactive\nmode\npi{}ti{} ::\n3.11. Configuring Amplitudes\n- Amplitudes in athenaCL are represented within the unit interval (0, 1)\n- After creating texture, we can edit the amplitude with the TIe command\n- The TIe command needs an argument for what Texture parameter to edit: enter \"a\" is for\namplitude\n\n- Parameter abbreviations can be found with the TIv command\n- Setting the amplitude to a RandomUniform value between 0 and 1 [03a.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\n# create a new texture with instrument 45\nath.cmd('tin a 45')\n# edit the amplitude of the texture to be RandomUniform between .1 and 1\nath.cmd('tie a ru,.1,1')\nath.cmd('eln')\nath.cmd('elh')\n- Two parts, one with RandomUniform amplitudes, another with RandomExponential [03b.py]\nNote that textures have to have different names\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\n# create a new texture with instrument 45\nath.cmd('tin a 45')\nath.cmd('tie a ru,.1,1')\n# create a new texture with instrument 65\n# texture must have a different name\nath.cmd('tin b 65')\nath.cmd('tie a re,15,.2,1')\nath.cmd('eln')\nath.cmd('elh')\n- Three parts, RandomUniform, RandomExponential, and RandomBeta amplitudes [03c.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd('emo mp')\n# create a new texture with instrument 45\nath.cmd('tin a 45')\nath.cmd('tie a ru,.1,1')\n# create a new texture with instrument 65\nath.cmd('tin b 65')\nath.cmd('tie a re,15,.2,1')\n# create a new texture with instrument 53\nath.cmd('tin c 53')\nath.cmd('tie a rb,.1,.1,.3,.7')\nath.cmd('eln')\nath.cmd('elh')\n\n3.12. Duration and Sustain\n- Duration\n- The temporal space of an event\n- If events are packed end to end, the time of the next event\n- If a notated event, the written rhythm\n- Sustain\n- The sounding (actual) time of the event\n- A scalar applied to the duration\n- A scalar of 0.2 would suggest a staccato (shortened) event\n- A scalar of 1.2 would create overlapping events\n3.13. The Pulse Triple\n- athenaCL supports both absolute and relative rhythm values\n- The PulseTriple is relative to the beat-defining tempo and made of three values\n- Divisor: divides the tempo beat duration\n- Multiplier: scales the value divided\n- Accent: a rhythm-specific amplitude value, between 0 (o) and 1 (+) (or with symbolic\ndynamics: mp, mf, etc)\n- Conventional rhythms can be easily expressed\n- (4,1,1): 1/4th of a beat (if the beat is a quarter, a sixteenth note)\n- (4,3,1): 3/4ths of a beat (if the beat is a quarter, a dotted eighth note)\n- (1,4,1): 4 beats (if the beat is a quarter, a whole note)\n- (3,1,1): 1/3rd of a beat (if the beat is a quarter, a triplet eighth)\n- (5,8,1): 8/5ths of a beat\n- Representational redundancy may be useful\n- (4,2,1) is the same as (2,1,1)\n\n- (1,5,1) is the same as (4,20,1)\n3.14. Basic Rhythm ParameterObjects\n- PulseTriple: create PulseTriples from embedded ParameterObjects\npi{}ti{} :: tpv pulsetriple\nRhythm Generator ParameterObject\n{name,documentation}\nPulseTriple\npulseTriple, parameterObject, parameterObject, parameterObject,\nparameterObject\nDescription: Produces Pulse sequences with four Generator\nParameterObjects that\ndirectly specify Pulse triple values and a sustain scalar. The\nGenerators specify\nPulse divisor, multiplier, accent, and sustain scalar. Floating-\npoint divisor and\nmultiplier values are treated as probabilistic weightings. Note:\ndivisor and\nmultiplier values of 0 are not permitted and are replaced by 1;\nthe absolute value\nis taken of all values. Arguments: (1) name, (2) parameterObject\n{pulse divisor},\n(3) parameterObject {pulse multiplier}, (4) parameterObject\n{accent value between 0\nand 1}, (5) parameterObject {sustain scalar greater than 0}\n- ConvertSecond: create durations form values in seconds\npi{}ti{} :: tpv cs\nRhythm Generator ParameterObject\n{name,documentation}\nConvertSecond\nconvertSecond, parameterObject\nDescription: Allows the use of a Generator ParameterObject to\ncreate rhythm\ndurations. Values from this ParameterObject are interpreted as\nequal Pulse duration\nand sustain values in seconds. Accent values are fixed at 1.\nNote: when using this\nRhythm Generator, tempo information (bpm) has no effect on event\ntiming. Arguments:\n(1) name, (2) parameterObject {duration values in seconds}\n3.15. Configuring Rhythms\n- After creating texture, we can edit the rhythm with the TIe command\n- The TIe command needs an argument for what Texture parameter to edit: enter \"r\" for rhythm\n- Using basketGen to control the multiplier [03d.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd(\"emo mp\")\nath.cmd(\"tin a 45\")\n\nath.cmd(\"tie a rb,.3,.3,.5,.8\")\nath.cmd(\"tie r pt,(c,4),(bg,oc,(3,3,2)),(c,1)\")\nath.cmd(\"eln\")\nath.cmd(\"elh\")\n- Using two basketGens to control multiplier and divisor independently [03e.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd(\"emo mp\")\nath.cmd(\"tin a 45\")\nath.cmd(\"tie a rb,.3,.3,.4,.8\")\nath.cmd(\"tie r pt,(c,4),(bg,oc,(3,3,2)),(c,1)\")\nath.cmd(\"tin b 65\")\nath.cmd(\"tie a re,15,.3,1\")\nath.cmd(\"tie r pt,(bg,rp,(2,1,1,1)),(c,1),(c,1)\")\nath.cmd(\"eln\")\nath.cmd(\"elh\")\n- Using two basketGens to control multiplier and divisor independently [03f.py]\nfrom athenaCL.libATH import athenaObj\nath = athenaObj.Interpreter()\nath.cmd(\"emo mp\")\nath.cmd(\"tin a 45\")\nath.cmd(\"tie a rb,.3,.3,.4,.8\")\nath.cmd(\"tie r pt,(c,4),(bg,oc,(3,3,2)),(c,1)\")\nath.cmd(\"tin b 65\")\nath.cmd(\"tie a re,15,.3,1\")\nath.cmd(\"tie r pt,(bg,rp,(2,1,1,1)),(c,1),(c,1)\")\nath.cmd(\"tin c 67\")\nath.cmd(\"tie a rb,.1,.1,.4,.6\")\nath.cmd(\"tie r cs,(rb,.2,.2,.01,1.5)\")\nath.cmd(\"eln\")\nath.cmd(\"elh\")\n3.16. Configuring Time Range\n- After creating texture, we can edit the time range with the TIe command\n- The TIe command needs an argument for what Texture parameter to edit: enter \"t\" for time\nrange\n- Enter two values in seconds separated by a comma\n- Staggering the entrances of three parts [03g.py]\nfrom athenaCL.libATH import athenaObj\n\nath = athenaObj.Interpreter()\nath.cmd(\"emo mp\")\nath.cmd(\"tin a 45\")\nath.cmd(\"tie t 0,20\")\nath.cmd(\"tie a rb,.3,.3,.4,.8\")\nath.cmd(\"tie r pt,(c,4),(bg,oc,(3,3,2)),(c,1)\")\nath.cmd(\"tin b 65\")\nath.cmd(\"tie t 10,20\")\nath.cmd(\"tie a re,15,.3,1\")\nath.cmd(\"tie r pt,(bg,rp,(2,1,1,1)),(c,1),(c,1)\")\nath.cmd(\"tin c 67\")\nath.cmd(\"tie t 15,25\")\nath.cmd(\"tie a rb,.1,.1,.4,.6\")\nath.cmd(\"tie r cs,(rb,.2,.2,.01,1.5)\")\nath.cmd(\"eln\")\nath.cmd(\"elh\")\n3.17. Musical Design Report 1\n- Must be primarily rhythmic in nature\n- Must employ at least 4 different timbre sources\n- Should have at least an AB or ABA form\n- Must prominently feature both the beta and exponential distributions\n- Can be composed with athenaCL, athenaCL and other tools, or other tools alone\n- See syllabus for details on other aspects\n3.18. Digital Audio Workstations\n- The merger of software for editing MIDI and notation with software for editing digital audio\n- Numerous commercial varieties: ProTools, Digital Performer, Cubase, FL, Logic, GarageBand\n- Inexpensive varieties: Reaper\n- Free varieties: Ardour, Rosegarden\n- Having access to a DAW with virtual instruments will greatly assist your projects in this class\n3.19. Digital Audio Workstations: Importing and Mixing Digital Audio\n- Create tracks to store audio\n\n- Drag and drop digital audio into a track\n- Adjust levels, process, and edit\n- Bounce to disc to mix down to a single audio file\n3.20. Digital Audio Workstations: Importing MIDI and Rendering\nDigital Audio\n- Create tracks to store MIDI or for virtual instruments\n- Drag and drop MIDI into a track\n- Render, freeze, or bounce realization of virtual instrument\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/4b320d6bb4908f40ab7b1d76866891f8_MIT21M_380S10_lec04.pdf",
      "content": "Chapter 4. Meeting 4, Foundations: Historical and Categorical\nPerspectives\n4.1. Announcements\n- Musical Design Report 1 due Tuesday, 23 February\n4.2. Reading: Ames: Automated Composition in Retrospect: 1956-1986\n- Ames, C. 1987. \"Automated Composition in Retrospect: 1956-1986.\" Leonardo 20(2): 169-185.\n- Is it surprising that Ames writes: \"it is therefore not surprising that these developments have met\nwith continuing -- and often virulent -- resistance\" (1987, p. 169)?\n- How was the DATATRON used to generate a melody?\n- How was MUSICOMP different from the work on the Illiac Suite?\n- How does Ames isolate the contribution of Koenig and Xenkakis as contributing to modularity in\nsystem design?\n- What trends does Ames describe in systems that were contemporary to his article?\n4.3. Reading: Ariza: Navigating the Landscape of Computer-Aided\nAlgorithmic Composition Systems: A Definition, Seven Descriptors,\nand a Lexicon of Systems and Research\n- Ariza, C. 2005b. \"Navigating the Landscape of Computer-Aided Algorithmic Composition\nSystems: A Definition, Seven Descriptors, and a Lexicon of Systems and Research.\" In Proceedings\nof the International Computer Music Conference. San Francisco: International Computer Music\nAssociation. 765-772. Internet: http://www.flexatone.net/docs/nlcaacs.pdf.\n- What is the definition of CAAC proposed in this article?\n- Why does the definition of CAAC exclude notation software and DAWs?\n- What are the seven descriptors proposed, and which seem the most important?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/dd7d565739e3e1e0e3dcc7cd70ec08fe_MIT21M_380S10_lec05.pdf",
      "content": "Chapter 5. Meeting 5, History: Serialism, Loops, Tiling, and\nPhasing\n5.1. Announcements\n- Musical Design Report 1 due Tuesday, 23 February\n- Review readings from last class\n5.2. Trigonometric Functions and Break-Point Graphs as\nParameterObjects\n- WaveSine: A scalable sine oscillator controlled by seconds or events per cycle\npi{}ti{} :: tpmap 100 ws,e,20,0,0,10\nwaveSine, event, (constant, 20), 0, (constant, 0), (constant, 10)\nTPmap display complete.\n- BreakPointLinear: Break point segments defined by seconds or events\npi{}ti{} :: tpmap 100 bpl,e,l,((0,.5),(8,0),(16,1),(24,.75),(32,.9),(40,.5))\nbreakPointLinear, event, loop, ((0,0.5),(8,0),(16,1),(24,0.75),(32,0.9),(40,0.5))\nTPmap display complete.\n\n- Numerous alternative trigonometric function generators exist as ParameterObjects: WaveCosine,\nWavePulse, WaveSawDown, WaveSine, WaveTriangle\n- Numerous alternative break-point function generators exist as ParameterObjects: BreakPointFlat,\nBreakPointHalfCosine, BreakPointLinear, BreakPointPower\n5.3. Configuring Tempo\n- The TIe command can be use to edit tempo by specifying \"b\" for BPM\n- Tempo can be controlled by any ParameterObject\n5.4. Approaches to Composing Time\n- Creating overlapping repeats of the same material\n- Creating overlapping repeats of transformed material\n- Creating ordered material that is then transformed in ways that retain order\n5.5. Canons and Tiling\n- Create an initial line and repeat it with staggered entrances\n- An approach to polyphony\n- The initial line can be temporally shifted and temporally transformed\n- Can be seen as an approach to musical tiling\n5.6. Listening: Andriessen\n- Louis Andriessen (1939-)\n- Dutch composer notable for combining American Minimalism with (at times) more diverse\nharmonic language\n- Andriessen: \"Hout\" (1991)\n\n5.7. Building a Basic Beat\n- Kick, snare, and hats\n- Command sequence:\n- emo mp\n- tin a 36\n- tie r pt,(c,2),(bg,oc,(7,5,2,1,1)),(c,1)\n- tin b 37\n- tie r pt,(c,2),(bg,oc,(3,5)),(bg,oc,(0,1))\n- tin c 42\n- tie r pt,(c,2),(c,1),(bg,oc,(0,1))\n- eln; elh\n5.8. A Basic Beat with More Complex Snare Part\n- Continued command sequence:\n- tio b\n- tie r pt,(c,4),(bg,rp,(3,3,5,4,1)),(bg,oc,(0,1,1))\n- eln; elh\n5.9. Adding Canonic Snare Imitation: Texture Copying\n- Copying a texture creates a new, independent, and dynamic part\n- While having identically configured ParameterObjects, if randomness is employed, unique\nstructures will be created\n- Continued command sequence:\n- tio b\n- ticp b b1\n- tie t .25, 20.25\n\n- tie i 76\n- ticp b b2\n- tie t .5, 20.5\n- tie i 77\n- eln; elh\n5.10. Saving and Loading the AthenaObject\n- An athenaCL XML file can be loaded in to athenaCL to restore Textures\n- These XML files can be automatically created whenever an event list is created\n- Continued command sequence:\n- eoo xao\n- eln\n5.11. Building an Extended Rhythmic Line with Canonic Imitation\n- Using different length ordered cyclic generators will create complex but non-random sequences\n- Command sequence:\n- aorm confirm\n- emo mp\n- tin a 77\n- tie r pt,(c,1),(c,1),(c,1)\n- tin b 67\n- tie r pt,(bg,oc,(2,4,1)),(bg,oc,(3,5,1,7,1,3)),(c,1)\n- ticp b b1\n- tie t 0.125,20.125\n- tie i 60\n- ticp b b2\n\n- tie t 0.25,20.25\n- tie i 68\n- eln; elh\n5.12. Creating Mensural Canons\n- Mensural canons use ratio-base time signatures for each part\n- Continued command sequence:\n- tio b1\n- tie b c,90\n- tio b2\n- tie b c,180\n- eln; elh\n5.13. Extensions\n- We can generate complex, deterministic patterns by combining cycles at high ratios\n- The same musical rhythm at different (low ratio related) rates produces interesting musical results\n5.14. Tonal, Atonal, and Post-Tonal\n- Tonal music employs functional harmony\n- Harmonies (chords) have a trajectory, expectation, and a resolution\n- One (or two) chords are more than others\n- Atonal music does not employ functional harmony\n- The expectations and priorities of chords are removed\n- Ideally, no pitch is more important than any other\n- Post-tonal refers approaches to harmony other than tonal\n- May be atonal, or may employ other approaches to pitch\n- Pitch centers may be developed and exploited\n\n5.15. Serialism\n- An approach to atonality that serialized (ordered) elements of musical parameters, developed by\nArnold Schoenberg\n- An alternative approach to atonality employed chords that completed the aggregate (all 12\npitches), developed by Josef Matthias Haur\n- By serializing the order of all 12-tone pitches, all get equal usage\n- Pitch groups smaller than 12 can be used\n- A series of all 12 tones is used as a motivic origin\n- The series can be transposed to any of 12 pitch levels: prime\n- The series can be reversed: retrograde\n- The series can be inverted ((12-n) % 12): inversion\n- The inverted series can be reversed: retrograde inversion\n- The 12 x 4 possible rows can be presented in a matrix\nGenerated with Python tools in music21: http://code.google.com/p/music21/\nfrom music21 import serial\np = [8,1,7,9,0,2,3,5,4,11,6,10]\nprint serial.rowToMatrix(p)\n0 5 11 1 4 6 7 9 8 3 10 2\n7 0 6 8 11 1 2 4 3 10 5 9\n1 6 0 2 5 7 8 10 9 4 11 3\n11 4 10 0 3 5 6 8 7 2 9 1\n8 1 7 9 0 2 3 5 4 11 6 10\n6 11 5 7 10 0 1 3 2 9 4 8\n5 10 4 6 9 11 0 2 1 8 3 7\n3 8 2 4 7 9 10 0 11 6 1 5\n4 9 3 5 8 10 11 1 0 7 2 6\n9 2 8 10 1 3 4 6 5 0 7 11\n2 7 1 3 6 8 9 11 10 5 0 4\n10 3 9 11 2 4 5 7 6 1 8 0\n- Milton Babbitt and Pierre Boulez extended serial techniques to new parameters and alternative\norganizations\n- Karlheinz Stockhausen and others attempted to employ serial techniques to organize parameters\nin the early Electronic Music studio\n- Total serialism orders amplitudes, rhythms, and other musical parameters\n\n5.16. Listening: Boulez\n- Pierre Boulez (1925-)\n- Post WWII and total serialism\n- Boulez: \"Structures, Book I\" (1952)\n5.17. Extensions\n- The algorithmic opportunities of serialism led many composers to generalize such techniques with\nthe computer\n- athenaCL features Paths as a way for Textures to share source Pitch data\n- One Path might be shared by multiple Textures, each transposing, reversing, and inverting this\nPath to create serial arrangements\n- While some have tried (Babbitt 1958), serial rhythm techniques have not been widely embraced\n5.18. Phasing\n- Musical material shifting in and out of time, or moving at different rates\n- Developed out of manipulations to recording reels: flanging and phasing\n\n- Can be used as a canon-like technique\n5.19. Listening: Reich\n- Steve Reich (1936-)\n- Influenced by techniques of minimalism based in part on music of Terry Riley, La Monte Young,\nand others\n- Reich: \"It's gonna rain\" (1965)\n- \"Scorification\" of a technological process for acoustic instruments\n- Reich: \"Piano Phase\" (1967)\n5.20. Phasing with athenaCL Python Libraries\n- pianoPhase.py\nimport os\nfrom athenaCL.libATH import midiTools\nfrom athenaCL.libATH import osTools\nfrom athenaCL.libATH import pitchTools\nfrom athenaCL.libATH import rhythm\nfrom athenaCL.libATH.libOrc import generalMidi\nfrom athenaCL.libATH.libPmtr import parameter\nOUTDIR = '/Volumes/xdisc/_scratch'\nBEATDUR = rhythm.bpmToBeatTime(225) # provide bpm value\ndef getInstName(nameMatch):\nfor name, pgm in generalMidi.gmProgramNames.items():\nif name.lower().startswith(nameMatch.lower()):\nreturn pgm # an integer\nreturn None\ndef getSource(repeat):\n\"\"\"get source melody and rhythm\"\"\"\npitchSequence = ['E4','F#4','B4','C#5','D5','F#4',\n'E4','C#5','B4','F#4','D5','C#5']\nrhythmSequence = [.5, .5, .5, .5, .5]\nampGen = parameter.factory(['ws','e',14,0,90,120]) # sine osc b/n 90 and 120\n\nscore = []\ntStart = 0.0\nfor i in range(len(pitchSequence) * repeat):\nps = pitchTools.psNameToPs(pitchSequence[i%len(pitchSequence)])\npitch = pitchTools.psToMidi(ps)\ndur = BEATDUR * rhythmSequence[i%len(rhythmSequence)]\namp = int(round(ampGen(0)))\npan = 30\nevent = [tStart, dur, amp, pitch, pan]\nscore.append(event)\ntStart = tStart + dur\nreturn score, len(pitchSequence)\ndef transformSource(score, srcLength):\n\"\"\"transform source, srcLength is size of each melodic unit\n\"\"\"\npost = []\noctaveShift = -1\npanShift = 60\nshiftUnit = BEATDUR / 16.\neCount = 0\nrepCount = 0 # starting at zero means first cycle will be in phase\nfor event in score:\nif eCount % srcLength == 0:\nshift = shiftUnit * repCount\nrepCount = repCount + 1 # increment after using\nnewEvent = [event[0]+shift, event[1], event[2],\nevent[3]+(octaveShift*12), (event[4]+panShift)%128]\npost.append(newEvent)\neCount = eCount + 1 # increment for each event\nreturn post\ndef main():\nrepeat = 33\npartA, seqLen = getSource(repeat)\npartB = transformSource(partA, seqLen)\ntrackList = [('part-a', getInstName('piano'), None, partA),\n('part-b', getInstName('piano'), None, partB),]\npath = os.path.join(OUTDIR, 'test.midi')\nmObj = midiTools.MidiScore(trackList)\nmObj.write(path)\nosTools.openMedia(path)\nif __name__ == '__main__':\nmain()\n5.21. Beats with athenaCL Python Libraries\n- basicBeat.py\nimport os, random\nfrom athenaCL.libATH import midiTools\nfrom athenaCL.libATH import osTools\nfrom athenaCL.libATH import pitchTools\nfrom athenaCL.libATH import rhythm\nfrom athenaCL.libATH.libOrc import generalMidi\nfrom athenaCL.libATH.libPmtr import parameter\n\nOUTDIR = '/Volumes/xdisc/_scratch' # provide output directory\nBEATDUR = rhythm.bpmToBeatTime(160) # provide bpm value\ndef getInstPitch(nameMatch):\nfor name, pgm in generalMidi.gmPercussionNames.items():\nif name.lower().startswith(nameMatch.lower()):\nreturn pgm # an integer\nraise NameError('bad pitch name')\ndef getKickSnare(repeat):\nrhythmA = [1, 1.5, .5, 1]\nrhythmB = [1.5, .5, 1.5, .5]\nrhythmC = [1.75, .25, 1.5, .125, .125, .125, .125]\ninstA\n= ['acousticBassDrum','sideStick']\ninstB\n= ['sideStick']\nampGen = parameter.factory(['rb',.2,.2,110,127])\nscore = []\ntStart = 0.0\nfor q in range(repeat):\nif q % 3 == 0:\nrhythmSequence = rhythmB\ninstSequence = instA\nelif q % 11 == 10:\nrhythmSequence = rhythmC\ninstSequence = instB\nrandom.shuffle(rhythmSequence)\nelse:\nrhythmSequence = rhythmA\ninstSequence = instA\nfor i in range(len(rhythmSequence)):\ninst = instSequence[i % len(instSequence)]\npitch = getInstPitch(inst)\ndur = BEATDUR * rhythmSequence[i % len(rhythmSequence)]\namp = int(round(ampGen(0)))\npan = 63\nevent = [tStart, dur, amp, pitch, pan]\nscore.append(event)\ntStart = tStart + dur\nreturn score, len(rhythmSequence)\ndef getHats(repeat):\nrhythmSequence = [.5, .5, .25, .25, .5, .5, .5, .5]\ninstSequence\n= ['closedHiHat','closedHiHat',\n'closedHiHat','closedHiHat',\n'closedHiHat','openHiHat']\nampGen = parameter.factory(['rb',.2,.2,50,80])\nscore = []\ntStart = 0.0\nfor q in range(repeat):\nfor i in range(len(rhythmSequence)):\ninst = instSequence[i % len(instSequence)]\npitch = getInstPitch(inst)\ndur = BEATDUR * rhythmSequence[i % len(rhythmSequence)]\namp = int(round(ampGen(0)))\npan = 63\nevent = [tStart, dur, amp, pitch, pan]\nscore.append(event)\ntStart = tStart + dur\nreturn score, len(rhythmSequence)\n\ndef main():\nrepeat = 33\npartA, seqLen = getKickSnare(repeat)\npartB, seqLen = getHats(repeat)\ntrackList = [('part-a', 0, 10, partA),\n('part-b', 0, 10, partB),]\npath = os.path.join(OUTDIR, 'test.midi')\nmObj = midiTools.MidiScore(trackList)\nmObj.write(path) # writes in cwd\nosTools.openMedia(path)\nif __name__ == '__main__':\nmain()\n5.22. Building an Extended Rhythmic Line with Fixed Tempo Phasing\n- Using different tempi will create shifting rhythmic patterns\n- Command sequence:\n- aorm confirm\n- emo mp\n- tin a 70\n- tie r pt,(bg,oc,(2,4,4)),(bg,oc,(4,1,1,2,1)),(c,1)\n- tie t 0,60\n- ticp a a1\n- tie b c,124\n- ticp a a2\n- tie b c,128\n- eln; elh\n5.23. Building an Extended Rhythmic Line with Dynamic Tempo\nPhasing\n- Oscillating the tempo at different rates will create dynamic changes\n- Command sequence:\n- aorm confirm\n\n- emo mp\n- tin a 64\n- tie r pt,(bg,oc,(2,4,4)),(bg,oc,(4,1,1,2,1)),(c,1)\n- tie t 0,60\n- ticp a a1\n- tie i 60\n- tie b ws,t,20,0,115,125\n- ticp a a2\n- tie i 69\n- tie b ws,t,30,0,100,140\n- eln; elh\n5.24. Extensions\n- Many works have been built with slow and gradual tempo changes\n- Tempos might slowly deviate with a BreakPointLinear or similar generator\n- Tempos might be randomly perturbed by adding in randomness: PO OperatorAdd can sum two\nParameterObjects\npi{}ti{} :: tpmap 100 oa,(ws,e,20,0,0,10),(ru,-2,2)\noperatorAdd, (waveSine, event, (constant, 20), 0, (constant, 0), (constant, 10)),\n(randomUniform,\n(constant, -2), (constant, 2))\nTPmap display complete.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/2554bea1cb3f52eabdb9eb3f780874e2_MIT21M_380S10_lec06.pdf",
      "content": "Chapter 6. Meeting 6, Workshop\n6.1. Announcements\n- Musical Design Report 1 due Today, 23 February\n- Quiz on Thursday\n- Download Martingale:\nhttp://code.google.com/p/martingale/\n6.2. Workshop: Musical Design Report 1\n- Four students will present their reports today\n6.3. Installing and Configuring Csound\n- Download and install most recent Csound 5\nhttp://sourceforge.net/projects/csound/files/\n- Test installation\n- Windows: run Csound.exe\n- Others: open a terminal, enter: csound\n6.4. Testing Csound in athenaCL\n- athenaCL can write separate score and orchestra files, or a combined .scd file; depends on\nEventOutput settings (select csd with EOo)\n- athenaCL may need to have a user preference set for where the Csound binary is located (use the\nAPea command)\n- athenaCL will create a batch file (.bat) to automate rendering of Csound files to audio\n- The audio file, after rendering, will be stored and named in the same location as other output files\n- Command sequence:\n\n- emo cn\n- tin a 82\n- tie x6 ws,e,14,0,200,16000\n- eln\n- elr\n- elh\n- With the ELauto command, rendering (ELr) and hearing (ELh) can be automatically executed\nfollowing the use of ELn\n6.5. Testing PD and Martingale\n- Download and install PD-Extended\nhttp://puredata.info/downloads\n- Download Martingale manually:\nhttp://code.google.com/p/martingale/\n- Place martingale anywhere on your file system\n- Add the \"martingale/pd/lib\" directory to Preferences > Path; this permits loading abstractions\nfrom the martingale library\n- Open pd/demo/earLimits.pd\n- Make sure \"compute audio\" is on, click check boxes, and select frequencies\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/20296227925e86d224f840cf36aa88b7_MIT21M_380S10_lec07.pdf",
      "content": "Chapter 7. Meeting 7, History: Gottfried Michael Koenig\n7.1. Announcements\n- Test direct rendering of CSD files with Csound if ELr is not working\n- Make sure you have PD-extended installed and Martingale on your system\n7.2. Quiz\n- 10 Minutes\n7.3. Gottfried Michael Koenig\n- Gottfried Michael Koenig (1926-)\n- 1954-1964: Worked with Stockhausen and others at West German Radio in Cologne\n- Composed for tape and acoustic instruments\n- 1963-1964: Studied programming, began developing software for CAAC\n- 1964-1986: Director of the Institute of Sonology in the Netherlands\n- Employed CAAC at three different levels\n- Symbolic: output from computer used to transcribe notation\n- Control: create sequences of control voltage mapped to synthesis parameters\n- Direct: employed direct creation of waveforms to create non-standard synthesis techniques\n7.4. Reading: Koenig: The Use of Computer Programs in Creating\nMusic\n- Koenig, G. M. 1971. \"The Use of Computer Programs in Creating Music.\" In Music and Technology\n(Proceedings of the Stockholm Meeting organized by UNESCO). Paris: La Revue Musicale. 93-115.\nInternet: http://www.koenigproject.nl/Computer_in_Creating_Music.pdf.\n- Koenig states that the use of the computer does not herald a new musical epoch: instead, what\ndoes he see the computer as offering?\n- Koenig describes a variable function generator: what is this?\n\n- Koenig sees work in the electronic music studio as suggesting some of the practices of\nalgorithmic composition: how so?\n- Koenig introduces the term composition theory: what might this mean?\n- What role did Koenig imagine for the computer in the work of composers and music students?\n- Koenig describes a technique of \"sound production\": what is this?\n7.5. Koenig: CAAC for Acoustic Instruments\n- Two early software systems\n- 1964: Project 1 (PR1)\n- 1969: Project 2 (PR2)\n- Favored discrete value generation and selection\n7.6. PR1: Concepts\n- 1964: Project 1 (PR1): Programmed in Fortran for the IBM 7090\n- A closed system, providing output based on user parameters\n- A user specified six tempo values, twenty-eight entry delays (rhythmic values), a random generator\nseed value, and the length of the composition\n- Materials were algorithmically selected\n- Series: random permutations, selection without replacement\n- Alea: random selection\n- Koenig saw series generation as an abstraction of twelve-tone techniques: \"the need for variation\nis satisfied without there having to be the pretense that somewhere deep inside the work the\ntwenty-fifth permutation is still being systematically derived from an original series\" (1970a, p.\n34).\n- At a larger level, 12-tone rows are created and deployed and three-note aggregate completing\ntrichords are created.\n- A seven-section formal structure controls the large-scale form, defining a position in a range from\nregular/periodic to irregular/aperiodic.\n- Output is provided for six parameters: (1) timbre (instrument or instrument group), (2) rhythm,\n(3) pitch, (4) sequence, (5) octave register, and (6) dynamic.\n\n- Sequence is spare parameter, applied to chord formation or timbre component within a timbre\ngroup\n- All parameters are independent\n7.7. PR2: Concepts\n- 1969: Project 2 (PR2): Algol, then Fortran for the PDP-15\n- A closed system, but more general and user-configurable\n- Eight parameters are generated: (1) instrument, (2) harmony, (3) register, (4) entry delay, (5)\nduration, (6) rest, (7) dynamics, and (8) mode of performance.\n- Expanded tools for algorithmic selection\n- Series\n- Alea\n- Ratio: weighted random selection\n- Group: repetition of values\n- Sequence: ordered selection\n- Tendency: random selection within dynamic boundaries\n7.8. PR2: The List-Table-Ensemble Principle\n- Selection procedures can be used on user-specified numeric or symbolic values (lists, stockpiles,\nor tables), or new, algorithmically generated expansions of user-specified numeric or symbolic\nvalues (ensembles).\n- Lists: raw stockpiles of data (assigned index values for access)\n- Tables: user-organized collection of indexes pointing to data in Lists\n- Ensembles: selection methods are used to create intermediary groups of data that are then drawn\nfrom to produce parameter values\n- A techniques of meta-selection that constrains values within distinct representations (distributions\nand orderings)\n- IterateHold: a rough analogy to the list-table-ensemble principle: select a number values from a\nPO, employ these for a number of times, and then regenerate a new selection\n:: tpmap 120 ih,(ru,0,1),(bg,oc,(2,4,13)),(bg,oc,(10,15))\n\niterateHold, (randomUniform, (constant, 0), (constant, 1)), (basketGen,\norderedCyclic, (2,4,13)), (basketGen, orderedCyclic, (10,15)), orderedCyclic\nTPmap display complete.\n7.9. Listening: Koenig\n- Three Asko Pieces\n- Koenig: Three Asko Pieces (1982)\n7.10. PR2 Selection Principles: Ratio\n- Weighted randomness can be achieved by configuration of BasketGen values\n- More control can obtained by configuring a zero-order Markov chain, to be discussed later\n7.11. Controlling Pitch in athenaCL\n- Paths provide ordered collections of pitch groups (Multisets) with proportional durations\n\n- A Texture is assigned a Path based on the last-created Path, an assigned Path, or an automatically\ncreated Path (if none exist)\n- The default Path is a single pitch, C4\n- A Texture can transform the Path with ParameterObjects assigned to the field (transposition) and\noctave (register shift) parameters\n- Different TextureModules can deploy Paths in very diverse ways\n7.12. PR2 Selection Principles: Group\n- IterateGroup: Two POs, one generating values, the other selecting how many times those values\nare repeated before a new selection is made\n:: tpmap 100 ig,(bg,oc,(0,5,10)),(bg,rc,(3,5,7))\niterateGroup, (basketGen, orderedCyclic, (0,5,10)), (basketGen, randomChoice,\n(3,5,7))\nTPmap display complete.\n- Create a collection of values, select a value, and then repeat a selected number of times\n- Command sequence:\n- emo m\n- tin a 6\n- tie r cs,(rb,.2,.2,.02,.25)\n- tie f ig,(bg,rc,(2,4,7,9,11)),(bg,rp,(2,3,5,8,13))\n- tie o ig,(bg,oc,(-2,-1,0,1)),(ru,20,30)\n- ticp a b c d\n- eln; elh\n\n7.13. PR2 Selection Principles: Tendency Mask\n- Random values selected from within dynamic minimum and maximum value range\n- Can be implemented with any Generator PO that has min/max parameter\n- Boundaries can be controlled by BreakPoint, Wave, or similar ParameterObjects\n- A powerful technique for creating long range behavior\n- Here, a break-point function and a wave sine generator form the boundaries of a random beta\nselection to control pitch\n- Command sequence:\n- emo m\n- tin a 15\n- tie r cs,(ig,(ru,.01,.25),(ru,4,12))\n- tie a ru,.2,(cg,u,.3,.9,.005)\n- tie f rb,.2,.2,(bpl,t,l,((0,-12),(30,12))),(ws,t,29,0,0,24)\n- eln; elh\n- A powerful technique for creating long range behavior\n- Here, random octave values are chosen between two wave triangle generators\n- Command sequence:\n- emo m\n- pin a d,e,g,a,b\n- tin a 107\n- tie r pt,(c,16),(ig,(bg,rc,(1,2,3,5,7)),(bg,rc,(3,6,9,12))),(c,1)\n- tie o ru,(wt,t,25,0,-2,4),(wt,t,20,0,-3,1)\n- eln; elh\n\n7.14. Reading: Koenig: Aesthetic Integration of Computer-Composed\nScores\n- Koenig, G. M. 1983. \"Aesthetic Integration of Computer-Composed Scores.\" Computer Music\nJournal 7(4): 27-32.\n- Koenig states that \"... to react functionally means ... to refrain from imitation of a particular\nproduction mode in another medium\": what is he suggesting?\n- What is Koenig suggesting about the use of histograms, where the composer supplies histograms\nand the computer program takes care of the data connections?\n- What is the process of transcription that Koenig describes? How is this different than\nconventional transcription?\n- What is aesthetic integration? Does Koenig suggest that this step can also be automated?\n- Koenig talks about composer having a sense of responsibility for the aesthetic result: why is this\nsignificant?\n7.15. Koenig: CAAC for Voltage Control\n- Used PR1 to generate events that were encoded in voltage control data\n- Voltage control data processed and translated to various musical parameters at different speeds\n- Used \"variable function generator\" (1966) to set and deploy values from the control rate to the\naudio rate\n\n- Produced Funktion pieces with this method: Funktion Grun (1967), Funktion Gelb (1968),\nFunktion Orange (1968), Funktion Rot (1968), Funktion Blau (1969), Funktion Indigo (1969),\nFunktion Violett (1969), Funktion Grau (1969)\n- Similar methods will be employed by outputting athenaCL generators to PD Arrays\n7.16. Listening: Koenig\n- Employed techniques of Funktion pieces\n- Koenig: \"Terminus X\" (1967)\n\n7.17. Alternative Approaches to Grouping and Masking\n- BasketSelect: Select values form a list using another PO providing values within the unit interval\n:: tpmap 100 bs,(-3,-2,\n1,0,1,2,3),(ru,(bpl,e,l,((0,.5),(100,1))),(bpl,e,l,((0,.5),(100,0))))\nbasketSelect, (-3,-2,-1,0,1,2,3), (randomUniform, (breakPointLinear, event,\nloop, ((0,0.5),(100,1))), (breakPointLinear, event, loop, ((0,0.5),(100,0)))),\nTPmap display complete.\n- IterateWindow: Select from a list of POs, and then draw a selected number of values from that\nPO\n:: tpmap 100 iw,((ru,.2,.8),(re,15,0,1),(ws,e,12,0,0,1)),(bg,rp,(14,20,26)),oc\niterateWindow, ((randomUniform, (constant, 0.2), (constant, 0.8)),\n(randomExponential, 15.0, (constant, 0), (constant, 1)), (waveSine, event,\n(constant, 12), 0, (constant, 0), (constant, 1))), (basketGen, randomPermutate,\n(14,20,26)), orderedCyclic\nTPmap display complete.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT21M_380S10_lec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/a3f3b3750b8fef345b8cc0fbafbbe6c2_MIT21M_380S10_lec08.pdf",
      "content": "Chapter 8. Meeting 8, Approaches: Permutations, Generators,\nand Chaos\n8.1. Announcements\n- KIOKU concert this Friday, 6:30, in the MIT Lewis Music Library\n- Musical Design Report 2 due 11 March: details to follow\n- Sonic System Project Draft due 27 April: start thinking\n8.2. A Line Segment Generator\n- Often we need to vary a parameter linearly over time or events\n- Break point functions require defining individual points\n- LineSegment: A dynamic line generator between minimum and maximum values over a duration\n:: tpmap 100 ls,e,30,0,1\nlineSegment, (constant, 30), (constant, 0), (constant, 1)\nTPmap display complete.\n:: tpmap 100 ls,e,(bg,oc,(3,6,9)),(ru,0,.7),(ru,.3,1)\nlineSegment, (basketGen, orderedCyclic, (3,6,9)), (randomUniform, (constant, 0),\n(constant, 0.7)), (randomUniform, (constant, 0.3), (constant, 1))\nTPmap display complete.\n\n8.3. Large Scale Amplitude Behavior with Operators\n- By multiplying or summing multiple behaviors, dynamic large-scale shapes are possible\n- Multiplying amplitudes by zero can create periods of inactivity\n- Techniques derived from modular synthesis\n- OperatorMultiply used to scale LineSegment and WavePulse\n:: tpmap 120 om,(ls,e,9,(ru,.2,1),(ru,.2,1)),(wp,e,23,0,0,1)\noperatorMultiply, (lineSegment, (constant, 9), (randomUniform, (constant, 0.2),\n(constant, 1)), (randomUniform, (constant, 0.2), (constant, 1))), (wavePulse,\nevent, (constant, 23), 0, (constant, 0), (constant, 1))\nTPmap display complete.\n- Command sequence:\n- emo mp\n- tin a 64\n- tie r pt,(bg,rp,(16,16,8)),(bg,rp,(2,2,1,4)),(c,1)\n- tie a om,(ls,e,9,(ru,.2,1),(ru,.2,1)),(wp,e,23,0,0,1)\n- eln; elh\n- Related ParameterObjects: OperatorAdd, OperatorMultiply, OperatorDivide, OperatorPower,\nOperatorSubtract, OperatorCongruence\n8.4. Reading: Ames. A Catalog of Sequence Generators: Accounting for\nProximity, Pattern, Exclusion, Balance and/or Randomness\n- Ames, C. 1992. \"A Catalog of Sequence Generators: Accounting for Proximity, Pattern,\nExclusion, Balance and/or Randomness.\" Leonardo Music Journal 2(1): 55-72.\n- What does ames mean by dependence, exclusion, and balance\n\n- Why does Ames have so many varieties of random uniform generators, such as LEHMER,\nSPREAD, FILL, and others?\n- How is Brownian motion related to random walks?\n- How does Ames characterize the artistic opportunities of using 1/f noise?\n- What are the characteristics of output provided by chaotic generators such as LOGISTIC and\nBAKER\n- What is the idea of a chaos knob?\n8.5. Continuous Random Walks\n- We can use BasketGen for discrete random walks\n- We can use Accumulator for continuous random walks\n- Accumulator\n:: tpmap 100 a,.5,(ru,-.1,.1)\naccumulator, 0.5, (randomUniform, (constant, -0.1), (constant, 0.1))\nTPmap display complete.\n8.6. Chaos and the Logistic Map\n- Complex dynamical systems\n- Deterministic systems that exhibit complex behavior\n- Most employ iterative processing and result in sensitivity to initial conditions (butterfly effect)\n- The logistic map was developed as a model of population growth by Pierre Verhulst\nxn+1 = rxn(1 - xn)\nr is a positive number between 0 and 4 that represents a combined rate for reproduction and\nstarvation\n\n- States produces constant outputs, oscillating behavior, and complex behavior\nPublic domain image (Wikipedia)\n- LogisticMap: most interesting output available from p (or r, lambda, or chaos knob) between 2.75\nand 4\n:: tpmap 100 lm,.5,(ls,e,100,2.75,4),0,1\nlogisticMap, 0.5, (lineSegment, (constant, 100), (constant, 2.75), (constant,\n4)), (constant, 0), (constant, 1)\nTPmap display complete.\n- Related ParameterObjects: henonBasket, lorenzBasket\n\n8.7. Reading: Voss and Clarke. 1/f Noise in Music: Music from 1/f\nNoise\n- Voss, R. F. and J. Clarke. 1978. \"1/f Noise in Music: Music from 1/f Noise.\" Journal of the\nAcoustical Society of America 63(1): 258-263.\n- What is a 1/f noise, and what is the variations of noise from 1/f0, 1/f1, 1/f2, 1/f3?\n- The sound and shape of correlated noise: [noiseColors.pd]\n- What technique did Voss and Clarke use to analyze music?\n- What sort of data did Voss and Clarke collect?\n- Extracting an amplitude envelope from an audio signal: [vossClarke.pd]\n- What conclusions do Voss and Clarke make about 1/f spectral densities?\n- Is music (or the averaged spectral analysis of amplitude envelopes) intelligent behavior\n- What technique did Voss and Clarke use to generate melodies? Is this technique parallel to the\nanalysis technique?\n- What conclusions did they draw from human evaluation of their generated melodies? Were these\nconclusions based on the 1/f noise source?\n8.8. 1/f Noise\n- Rather than just one type of 1/f noise, use many\n- Gamma can move between 0 (white), 1 (pink), 2 (brown), 3 (black)\n- 1/f noise: gamma == 1\n:: tpmap 100 n,100,1,0,1\nnoise, 100, (constant, 1), (constant, 0), (constant, 1)\nTPmap display complete.\n- 1/f noise: gamma == 2\n\n:: tpmap 100 n,100,2,0,1\nnoise, 100, (constant, 2), (constant, 0), (constant, 1)\nTPmap display complete.\n- 1/f noise: gamma == 3\n:: tpmap 100 n,100,3,0,1\nnoise, 100, (constant, 3), (constant, 0), (constant, 1)\nTPmap display complete.\n- Noise: dynamically varying the gamma\n:: tpmap 100 n,100,(ls,e,100,0,3),0,1\nnoise, 100, (lineSegment, (constant, 100), (constant, 0), (constant, 3)),\n(constant, 0), (constant, 1)\nTPmap display complete.\n8.9. 1/f Noise in Melodic Generation: LineGroove\n- Using BasketSelect to select discrete values from a continuous generator\n:: tpmap 100 bs,(2,4,7,9,11,14,16,19,21,23),(n,100,1,0,1)\nbasketSelect, (2,4,7,9,11,14,16,19,21,23), (noise, 100, (constant, 1),\n\n(constant, 0), (constant, 1)),\nTPmap display complete.\n- Command sequence using TM LineGroove:\n- emo m\n- tmo lg\n- tin a 108\n- tie r cs,(ls,e,10,(ru,.01,.2),(ru,.01,.2))\n- tie f bs,(2,4,7,9,11,14,16,19,21,23),(n,100,1,0,1)\n- eln; elh\n8.10. 1/f Noise in Melodic Generation: HarmonicAssembly\n- Command sequence using TM Harmonic Assembly:\n- emo m\n- pin a d3,e3,g3,a3,b3,d4,e4,g4,a4,b4,d5,e5,g5,a5,b5\n- tmo ha\n- tin a 27\n- tie r pt,(c,16),(ig,(bg,rc,(1,2,3,5,7)),(bg,rc,(3,6,9,12))),(c,1)\n- tie a om,(ls,e,9,(ru,.2,1),(ru,.2,1)),(wp,e,23,0,0,1)\n- tie d0 c,0\n- tie d1 n,100,2,0,14\n- tie d2 c,1\n- tie d3 c,1\n\n- eln; elh\n- Continued command sequence: with chord size generation between 1 through 4\n- tie d3 ru,1,4\n- eln; elh\n8.11. Tutorial: PD Arrays as Parameters: Filtered Noise\n- [mgEnvlMtAr] creates a mono-triggered, attack-release envelope\n[mgEnvlMtAr] arguments: attack time, release time, duration\n[mgEnvlMtAr] trigger: a floating point value that sets the peak amp\n8.12. Tutorial: PD Arrays as Parameters: Cyclical Amplitude Values\n- Looping through an array with amplitude values with [counter]\n[pow 4] provides non-linear to linear amplitude scaling\n\n8.13. Tutorial: PD Arrays as Parameters: Cyclical Cutoff Frequency\nValues\n- Looping through an array if values scaled to MIDI pitch values (60-140) with [mgScaleMap]\nMIDI pitch values are scaled to frequency values with [mtof]\nData values are converted to signals with [mtof] and [lop~ 30]\n[moog~] provides a signal controlled low pass filter with variable resonance\n\n8.14. Tutorial: PD Arrays as Parameters: Cyclical Pulse Multipliers\n- [metro] provides regularly spaced triggers\n[counter] and [sel 1] are used to select the first of each cycle\nAn array of values is scaled to pulse multipliers with [mgScaleMap]\nThe [counter] max value is dynamically set after reading and mapping a value from the array\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT21M_380S10_read_btmimos.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/2de9e6cf71010ccf299663076761a7fb_MIT21M_380S10_read_btmimos.pdf",
      "content": "Beyond the Transition Matrix: A Language-Independent, String-Based Input Notation for\nIncomplete, Multiple-Order, Static Markov Transition Values\nChristopher Ariza\nAbstract: While Markov chain generators have been employed throughout the history of\ncomputer music as a tool for the creation of musical parameter values, input notations for\nMarkov transition values are often cumbersome and opaque. Rejecting the transition matrix\nas an input notation, this paper offers a new language-independent, string-based input\nnotation for incomplete, multiple-order, static Markov transition values. Transition values\nare given greater generality by accommodating multiple orders simultaneously, as well as the\nspecification of transitions with the use of limited single-operator regular expressions. A\ncomplete Python implementation of this model is introduced, and high-level utilities and\nobject interfaces are demonstrated in athenaCL.\n1. Introduction\nThroughout the history of computer music Markov chain generators have been employed as a tool\nfor the creation of musical parameter values. There are two common approaches to configuring\nthese Markov processors. In the first case, a data sequence is analyzed, and then the resulting\ntransition data is stored in a transition matrix and used to generate new values. In the second case,\ntransition values are directly specified without derivation from analysis. In both cases, an intuitive\nand transparent notation of transition values, beyond the common tabular matrix, is advantageous.\nIn the second case, a practical notation for Markov transition values offers a powerful and efficient\ninput notation for a wide range of Markov applications.\nRejecting the transition matrix as an input notation, this paper offers a language-independent, string-\nbased input notation for incomplete, multiple-order, static Markov transition values. Transition\nvalues are given greater generality by accommodating multiple orders simultaneously, as well as the\nspecification of transitions with the use of limited single-operator regular expressions. A complete\nPython implementation of this model is introduced, and high-level utilities and object interfaces are\ndemonstrated in athenaCL (Ariza 2005). These specialized object interfaces offer flexible rhythm\nand parameter value generation. Additionally, the use of dynamic Markov order values is shown to\noffer a flexible and previously unexplored resource.\nAs Charles Ames states, \"by no means can Markov chains be said to occupy the cutting edge of\nprogress in automated composition...\" (1989, p. 186). A Markov-based generator, further, has well-\nknown limitations: it is \"incapable of generating self-embedded structures\" (Cohen 1962, p. 155)\nand, in general, \"... is not complete enough by itself to consistently produce high quality music\"\n(Moorer 1972, p. 111). Nonetheless, Markov chains offer a practical tool: they can be used to\ngenerate any type of value or value collection, they are discrete, they offer greater control than\nuniform randomness, and, at higher orders, they produce sequentially related structures. This\npracticality, however, is often encumbered by the parametric complexity of the traditional transition\nariza@flexatone.net\n\nmatrix. A challenge of computer-aided algorithmic composition (CAAC) system design is the\nreduction of parametric complexity. This challenge can be met in part by the development of\nintuitive, flexible, and language-independent string notations. Such notations permit the user to\nsupply complex specifications within a single argument, rather than supplying numerous arguments\nto a function or text-entry form.\n2. The Markov Chain and the Markov Transition String\nA Markov chain, as used here, is a technique for generating a one-dimensional series of values or\nsymbols based on probabilities, where probabilities for each possible outcome are selected based on\nzero or more past symbol formations. The number of past symbols a Markov chain uses to specify a\nnew symbol is the \"order\" of the Markov chain. In the case of orders greater than zero, it is useful\nto label these past values as a \"source,\" and the probabilities as directing to possible \"destinations\"\n(Ames 1989, p. 175). Source formations are referred to as \"transitions.\"\nThe history of Markov models is well documented in the mathematical and scientific literature\n(Norris 1998, Bermaud 1999). Their origins are traced to their namesake, Russian mathematician A.\nA. Markov (1856-1922). While some recent research has explored the Hidden Markov Model\n(HMM) and the Markov Chain Monte Carlo, this article focuses only on the conventional Markov\n\"chain\": a discrete-time stochastic process. Only Markov chains with finite state spaces (or a finite\ncollection of possible symbols) are considered. While Markov chains are frequently displayed with\nvarious state diagrams or as directed graphs, are often presented in the context of random walks, and\nare frequently defined as regular (type 3) finite state grammars (Roads 1984, p. 14), this discussion\nwill only consider elementary models.\nA Markov chain will be defined with a new string notation. This notation encodes key and value\npairs with brace-delimited values. For example, a key \"x,\" assigned a value of 0.3, is notated as\nx{0.3}. Multiple key and value pairs can be defined in sequence without the use of spaces or\ncommas: x{0.3}y{0.7}.\nA Markov chain produces output based on examination of zero or more past symbols. A zeroth\norder Markov chain thus examines zero previous values; a third order Markov chain examines three\nprevious values. A \"transition\" defines a possible past symbol formation. Thus, a second order\nMarkov chain with two symbols (x, y) must define four transitions, or the possible past symbol\nformations x:x, x:y, y:x, y:y (where \":\" separates past symbols, and symbols read from left to\nright as most remote to most recent). For each transition, probabilities may be assigned for the\ngeneration of a new symbol. These probabilities may be specified as weights or as unit-interval\nproportions. In the notation presented here, weights are defined by providing a destination symbol,\nan \"=\" sign, and a numeric value; multiple weights, separated by the \"|\" symbol, may be specified.\nContinuing the example above, the y:x transition may define an equal probability of producing\neither symbol \"x\" or \"y\" with the following notation: x:y{x=1|y=1}. Alternatively, the x:x transition\ncould define the production of a \"y\" symbol nine out of ten times: x:x{x=1|y=9}. If a weight for a\nsymbol is not specified within a transition, the weight is assumed to be zero.\nThe zero order Markov chain has one transition, that of zero previous values. The \":\" character\nalone signifies the single transition of a zero order Markov chain. For example, a zero order\ntransition specification for the symbols above may be defined as :{x=3|y=4}.\n\nA complete Markov transition string consists of two combined parts: symbol definitions and\ntransition weight definitions. Symbols are named with lower-case letters, and weight definitions\ncannot refer to undefined symbols. Transition weight definitions may be provided for any number\nof orders. For example, all weights defined in the previous examples may be combined with symbol\ndefinitions to demonstrate a complete Markov transition string:\nExample 1. A complete Markov transition string\nx{0.3}y{0.7}:{x=3|y=4}x:y{x=1|y=1}x:x{x=1|y=9}\nWith numerous symbols and with orders greater than zero, the number of possible transitions\nbecomes large. To facilitate more compact transition string definitions, two features are\nincorporated. First, all transitions not specified are assigned an equal-weight distribution for all\ndefined symbols. Second, transition weight definition keys may employ limited single-operator\nregular expressions. Three operators are permitted: \"*\", \"-\", and \"|\". The \"*\" operator matches any\ndefined symbol: using the symbols (x, y) defined above, the transition x:* will match x:x and x:y.\nThe \"-\" operator matches any symbol other than that specified: the transition x:-x will match x:y.\nThe \"|\" operator matches any number of symbols specified: the transition x:x|y will match x:x and\nx:y.\nThe athenaCL system offers utility commands to both encode Markov strings (AUma) and use them\nas generators (AUmg). The AUma command (AthenaUtility Markov Analysis), given a maximum\norder and a sequence of symbols, returns the corresponding Markov string for all orders less than\nand equal to the order specified. For example, the following athenaCL session encodes a simple\nsequence of two symbols at orders zero, one, and two. Note that symbols (a and b) are automatically\nassigned and that the symbol sequence, under analysis, is automatically wrapped.\nExample 2. Creating a Markov transition string in athenaCL with AUma\n:: auma 2 x x x x x x y y y y y y\nAthenaUtility Markov Analysis\na{x}b{y}:{a=6|b=6}a:{a=5|b=1}b:{a=1|b=5}a:a:{a=4|b=1}a:b:{b=1}b:a:{a=1}b:b:{a=1|b=4}\nThe AUmg command (AthenaUtility Markov Generator) may be used to test the generation of\nvalues from a Markov transition string with a static order. In the example below, a Markov transition\nstring, employing limited single-operator regular expressions to define a compact second order\nMarkov generator, is used to produce thirty values.\nExample 3. Testing a Markov transition string in athenaCL with AUmg\n:: aumg 30 2 x{a}y{b}z{c}z:-z{z=1}y:y|z{z=1|x=2}*:x{y=2|z=1}\nAthenaUtility Markov Generator\nb,a,c,a,b,a,b,a,b,b,a,c,b,c,a,b,b,a,c,c,a,c,a,b,b,a,b,c,a,b\n\n3. The Markov Chain in the History of Algorithmic Composition\nThe Markov chain is one of the earliest techniques of CAAC. In the production of Experiment IV\nof the Illiac Suite (1956), Lejaren Hiller and Leonard Isaacson used zero, first, and higher order\nMarkov chains (1959, pp. 141-148). Markov chains, with probabilities determined by analysis of\nexcerpts from Charles Ives's Three Places in New England, were employed by Hiller and Robert Baker\nin the Computer Cantata (1963; Hiller and Baker 1964, p. 68; Hiller 1970, p. 54) and implemented as\nthe ORD.n subroutine in MUSICOMP (Hiller 1969, pp. 72-73). An early and extensive exploration\nof computer-based Markov analysis and generation is provided by Brooks et al. (1957). Additionally,\nGottfried Michael Koenig offers the related \"ratio\" selection method in Project Two (PR2, 1966);\nwhile not labeled as a Markov model, this generator is equivalent to a zero order Markov chain\n(1970). A similar utility is found in Barry Truax's POD Programs with the ratio selection mode\n(Buxton 1975, p. 224).\nPrior to computer implementation, however, there was significant interest in generating one-\ndimensional sequences with Markov chains. Claude E. Shannon and Warren Weaver's 1949 text A\nMathematical Theory of Communication, based on an earlier text by Shannon (1948) and influenced by\nthe work of Norbert Wiener and his Cybernetics (1948), demonstrates the application of Markov\nchains for the algorithmic generation of English sentences. Shannon and Weaver, calling these\ngenerators stochastic processes, frequently suggest application to musical structures: \"a system\nwhich produces a sequence of symbols (which may, of course, be letters or musical notes, say, rather\nthan words) according to certain probabilities is called a stochastic process ...\" (1949, p. 11).\nFollowing Shannon and Weaver, numerous studies employed Markov chains as musical generators.\nThese studies were done with manual, mechanical, and primitive computer calculations, and include\nthe analysis and generation of Western cowboy songs by Fred and Carolyn Attneave (Cohen 1962,\np. 143; Quastler 1955), the \"Banal Tune-Maker\" of Richard C. Pinkerton (1956), and the Markov\ngenerator created in 1956 by John F. Sowa with a Geniac \"Electronic Brain Kit\" (Sowa 1957, 2005;\nCohen 1962, p. 143). Harry Olson and Herbert Belar, in 1961, built a sophisticated electronic\nmachine that, based on Markovian pitch and rhythm analysis of eleven Stephen Collins Foster\nsongs, produced and synthesized new melodies (1961). The analysis data of these Foster songs has\nbeen frequently reproduced (Dodge and Jerse 1997, pp. 364-367). Additionally, the first edition of\nIannis Xenakis's Musiques Formelles (1963) contained chapters on \"Markovian Stochastic Music\";\nthese chapters detail Xenakis's application of first order Markov chains for the selection of screens\nand for the generation of intensity and density values in Analogique A and Analogique B (1958-1959).\nThese techniques were not implemented on a computer and pre-date Xenakis's Stochastic Music\nProgram (Xenakis 1965).\nLater computer-based Markov implementations are numerous and widespread. Implementations are\nfound in Sever Tipei's MP1 (1975), David Zicarelli's Jam Factory and Joel Chadabe and Zicarelli's M\n(Zicarelli 1987, Chadabe 1997), the Max system, Clarence Barlow's MIDIDESK (1996 Roads 1996,\np. 849), Larry Polansky and David Rosenboom's HMSL (1985) and JMSL (Didkovsky 2004),\nHeinrich Taube's Common Music (Taube 1989), Eduardo Reck Miranda's CAMUS 3D (McAlpine\net al. 1999), Paul Berg's AC Toolbox (2003), Tim Thompson's KeyKit (Phillips 2005), and Francois\nPachet's Continuator (2002). Additional studies and examples of Markov models are provided by W.\nRoss Ashby (1956), J. E. Youngblood (1958), J. E. Cohen (1962), Pierre Barbaud (1968), James\nAnderson Moorer (1972), Kevin Jones (1981), Ames (1989), E. Cambouropoulos (1994), D. Lyon\n(1995), Curtis Roads (1996, p. 878), Jon McCormack (1996), and Miranda (2000, pp. 69-72). More\n\nrecently, J. L. Trivino-Rodriguez and R. Morales-Bueno review applications of Probabilistic Suffix\nAutomata (PSA) and related work of Assayag et al. (1999), and propose a Multiattribute Prediction\nSuffix Graph (MPSG) as an improvement over both Markov chains and PSA (2001), Diemo\nSchwarz, after a method of score following demonstrated by Orio and Dechelle (2001), employs\nHMMs to solve music alignment problems in concatenative synthesis (2004), David Michael Cottle\ndemonstrates Markov models in SuperCollider 3 (2005), Miranda and Adolfo Maia Junior,\nintroducing the Fuzzkov system, employ Markov chains with dynamic transition matrices to control\ngrain selection (2005), and Rene Wooller and Andrew R. Brown discuss a technique of Markov\nmorphing (2005). While not comprehensive, these listings demonstrate the abundance and diversity\nof Markov models.\n4. Contemporary Markov Implementations\nThe Markov chain is one of the earliest techniques of CAAC (Hiller and Isaacson 1959, pp. 141\n148). A few of the many contemporary Markov implementations found in CAAC systems will be\nexamined in detail. In systems that provide modular Markov generators, input notations often\ndirectly enumerate coordinate pairs of the transition matrix as a list of two or three elements. The\nnew string notation presented above, as well as the extended use of regular expressions, provides\ngreater compactness and readability than these alternative notations.\nReleased as part of the Max library as early as 1995, the Max \"prob\" object provides a first order\nMarkov generator that supports dynamic transition weights and proportional integer weight values\n(Dobrian 1995, pp. 318-319). Symbol and transition weight values are supplied as three-element lists\nwith values specified in the following order: source, destination, weight. Only a single weight, to a\nsingle destination, may be defined in each list. The Max \"anal\" object provides a corresponding tool\nfor first order Markov analysis, returning data lists in the appropriate format. An example,\ndistributed with Max 4.5 (\"prob.help\"), demonstrates a \"prob\" object receiving five Markov\ntransition weight definitions from Max message boxes. These messages, here delimited with\nbrackets, are as follows: [1 2 1], [2 1 1], [1 1 0], [2 2 0], [1 3 1].\nOffering greater clarity, a single Markov transition string can encode these five messages:\na{1}b{2}c{3}a:{b=1|c=1}b:{a=1}\nCommon Music (Taube 1989) offers Markov functionality with nth order Markov generation, static\norders, dynamic transition weights, and unit-interval weight values. The customizable \"markov\"\nclass exposes Markov functionality to the user. Common Music (CM) provides a \"markov-analyze\"\nfunction to generate lists of transition weights (or to configure and return a \"markov\" object) from a\nuser-supplied list of data. When directly specified, transition weight definitions are provided as a\nspace-separated Lisp list in the following form: (source -> (destination weight)), where\ndestinations with equal weights may omit weight specification. In the case of higher-order rules,\nlonger source transition patterns may be specified before the \"->\" symbol. The following incomplete\nexample demonstrates the definition of numerous second order rules.\n\nExample 4. Second order input in CM\n(new markov\n:of '((d4 c4 -> (f4 .5) (g4 .5))\n(d4 bf4 -> bf4)\n(c4 d4 -> c4)\n(c4 c4 -> (d4 .667) (c5 0.333))))\nThe above example can be encoded as a Markov transition string:\nm{c4}n{d4}o{f4}p{g4}q{bf4}r{c5}n:m{o=1|p=1}n:q{q=1}m:n{m=1}m:m{n=2|r=1}\nIn CM, weight values may be supplied by dynamic pattern objects, permitting the production of\nMarkov generators with dynamic weights. A significant feature of CM's transition specifications is\nsupport for the \"*\" wild-card identifier, matching any possible symbol at the designated position. A\ntransition rule can thus be specified in the following form: (* d4 -> c4). This feature inspired the\nmore flexible single-operator regular expression matching presented in this study.\nThe AC Toolbox (Berg 2003) offers Markov functionality with nth order Markov generation, static\norders, static transition weights, and unit-interval weight values. Markov-based software objects are\npartitioned between a \"transition\" Generator and numerous system Tools for creating the necessary\ntransition value table. The \"transition\" Generator, given this table of Markov transition probability\nvalues, allows the generation of new values according to these probabilities. There are three Tools\nfor generating transition value tables: \"Derive-Transition-Table\" produces a table for a user-supplied\norder based on analysis of a wide variety of data objects within the AC Toolbox (such as a list,\nstockpile, note structure, or section); \"Make-Unconditional-Table\" converts a user-supplied\nrepresentation of symbol weights for zeroth order transition tables; and \"Make-Conditional-Table\"\nconverts a user-supplied representation of symbol weights for first and higher order transition\ntables. The input notation for zeroth order transitions (unconditional tables) is a Lisp list of value,\nweight pairs. For example:\nExample 5. Zeroth order input in the AC Toolbox\n(make-unconditional-table 'a .4 'b .4 'c .2)\nThe input notation for first and higher order transitions (conditional tables) is a Lisp list of list pairs,\nwhere each pair is a source symbol sequence and a destination weight list. This weight list follows\nthe same format as the zero order transition above. For example:\nExample 6. First order input in the AC Toolbox\n(make-conditional-table '(a) '(b .3 c .7) '(b) '(c 1) '(c) '(a .5 b .5))\nDemonstrating the ability to combine multiple orders in a single notation, a Markov transition string\ncan be used to encode both examples from above:\na{a}b{b}c{c}:{a=4|b=4|c=2}a:{b=3|c=7}b:{c=1}c:{a=1|b=1}\n\nThe Markov transition string offers three advantages over these alternative representations. First, the\nsyntax is clean and compact. Second, unique symbols are defined for referenced data. By doing so,\nall symbols are defined separately from weight specifications, permitting symbols to refer to complex\ndata or long strings without obfuscating the presentation of weight assignments. Further, with all\nsymbols explicitly defined, incomplete sets of transition weights are permitted. Third, none of the\nmodels above permit defining multiple-order weights simultaneously; with such a facility, multiple\norders may be deployed from the same transition specification.\n5. The Markov Transition String in athenaCL\nThe core athenaCL Markov implementation, as well as a complete Markov transition string parser\nand analysis-based string generator, is modeled as the Transition object and is defined in the Python\nmodule markov.py. This module is distributed as part of the cross platform and open source (GPL)\nathenaCL libATH library. Complete implementation details and object design analysis are beyond\nthe scope of this study.\nIn athenaCL, ParameterObjects, as models of one-dimensional generators, offer high-level object\ninterfaces to CAAC tools and procedures (Ariza 2005, pp. 205-207). Musical parts within athenaCL\nare deployed as specialized TextureModules, or multi-dimensional generators (Ariza 2005, p. 227);\neach Texture is configured with the assignment of numerous ParameterObjects. ParameterObjects\ncan be supplied as arguments to other ParameterObjects, permitting complex, embedded dynamic\ngenerators.\nTwo ParameterObjects are provided for generating general numeric or symbolic parameter values:\nMarkovValue and MarkovGeneratorAnalysis. MarkovValue takes three arguments: (1) name, (2)\ntransitionString, (3) parameterObject {order value}. The name of a ParameterObject must be\nprovided as a first argument. A Markov transition string, of any complexity and order, is provided as\na second argument. The third argument is an embedded ParameterObject to supply the order of\nMarkov generation. As this ParameterObject may by dynamic, various alterations to Markov\ngeneration are possible. Floating-point order values are accepted, and are treated as probabilistic\nweightings toward surrounding integers. Thus a generated order value of 1.5 will be interpreted as an\nequal probabilistic weighting between 1 or 2. For each Markov value generated, these weights are\nevaluated and an integer order value is determined.\nThe MarkovGeneratorAnalysis ParameterObject takes five arguments: (1) name, (2)\nparameterObject {source Generator}, (3) valueCount, (4) maxAnalysisOrder, (5) parameterObject\n{output order value}. Rather than taking a Markov string as an argument, an embedded\nParameterObject is used to produce as many values as specified by the valueCount argument, and\nthese values are analyzed at every order up to and including the order specified by the\nmaxAnalysisOrder argument. With this analysis data, the generator produces new values, embedding\nanother ParameterObject to control the Markov order.\nThe athenaCL system features specialized ParameterObjects for rhythm generation. These\ngenerators employ Pulses, objects that specify a duration (as a ratio of an externally supplied tempo)\nand an accent (as an amplitude scalar between 0 and 1, or between a rest and a fully sounding event).\nThe input notation for a Pulse is a Python list of three elements: a divisor, a multiplier, and an accent\n\n(Ariza 2005, p. 163). Two Rhythm ParameterObjects with Markov functionality, analogous to those\nfor general parameter values, are offered: MarkovPulse and MarkovRhythmAnalysis.\nMarkovPulse takes three arguments: (1) name, (2) transitionString, (3) parameterObject {order\nvalue}. The only difference between MarkovPulse and MarkovValue is that the transition string in\nMarkovPulse must define symbols that refer to Pulse objects. MarkovRhythmAnalysis takes five\narguments (1) name, (2) parameterObject {source Rhythm Generator}, (3) pulseCount, (4)\nmaxAnalysisOrder, (5) parameterObject {output order value}. Similarly, the only difference between\nMarkovRhythmAnalysis and MarkovGeneratorAnalysis is that the analyzed ParameterObject must\nbe a Rhythm ParameterObject.\n6. Examples of Markov Generators with Dynamic Order Specifications\nAs cited above, musical applications and demonstrations of Markov generators are abundant in the\nliterature. The use of dynamic and probabilistic order values, however, is uncommon, if not\ncompletely without precedent. This is in part because traditional transition matrices specify only a\nsingle order. With the Markov transition string presented here, multiple orders can be\naccommodated in the same representation.\nFor example, a sequence formed of six values spanning the unit interval may be constructed. The\nsequence used here is designed to present repeated oscillation followed by a narrowing of the\nminimum and maximum values. This sequence could be further scaled and then used for controlling\namplitude, panning, or signal processing parameters. The athenaCL AUma command is used to\nanalyze this sequence and produce a Markov transition string for orders two and lower:\nExample 7. Creating a Markov transition string in athenaCL with AUma\n:: auma 2 0 .2 .4 .6 .8 1 .8 .6 .4 .2 0 .2 .4 .6 .8 1 .8 .6 .4 .2 0 .2 .4 .6 .8 .6 .4\n.2 .4 .6 .4 .6\nAthenaUtility Markov Analysis\na{0}b{.2}c{.4}d{.6}e{.8}f{1}:{a=3|b=6|c=8|d=8|e=5|f=2}a:{b=3}b:{a=2|c=4}c:{b=3|d=5}d:{\na=1|c=4|e=3}e:{d=3|f=2}f:{e=2}a:b:{c=3}b:a:{b=2}b:c:{d=4}c:b:{a=2|c=1}c:d:{a=1|c=1|e=3\n}d:a:{b=1}d:c:{b=3|d=1}d:e:{d=1|f=2}e:d:{c=3}e:f:{e=2}f:e:{d=2}\nUsing the MarkovValue ParameterObject, this Markov transition string, and a constant order value\nof two, new values are generated and are graphed in the example below. This event-domain graph,\nwhere the x axis refers to event steps and the y axis refers to generated values, is produced with the\nathenaCL TPmap (TextureParameter Map) command. Note that upward and downward oscillation\nis retained, with a slightly higher frequency of oscillation between values in the middle of the range.\n\nExample 8. MarkovValue generation at order 2\nIn the next example, the same transition string is used, while the order is set to a constant value of\none. Note that oscillation gets \"stuck\" within value pairs, demonstrating that only one previous\nvalue is taken into the context of generating new values.\nExample 9. MarkovValue generation at order 1\nIn the example below, the same transition string is used while the order is set to a constant value of\nzero. No longer is oscillation clearly visible: instead, a distribution of values proportional to their\nanalyzed frequency is created.\nExample 10. MarkovValue generation at order 0\nIn the next example, the order parameter is linearly increased from 0 to 2 over the span of 120\nevents. As a clear movement between different behaviors is apparent, this example demonstrates,\neven within the small range of 120 events, the utility of employing dynamic order values.\n\nExample 11. MarkovValue generation with a dynamic order\nmarkovValue,\na{0}b{.2}c{.4}d{.6}e{.8}f{1}:{a=3|b=6|c=8|d=8|e=5|f=2}a:{b=3}b:{a=2|c=4}c:{b=3|d=5}d:{\na=1|c=4|e=3}e:{d=3|f=2}f:{e=2}a:b:{c=3}b:a:{b=2}b:c:{d=4}c:b:{a=2|c=1}c:d:{a=1|c=1|e=3\n}d:a:{b=1}d:c:{b=3|d=1}d:e:{d=1|f=2}e:d:{c=3}e:f:{e=2}f:e:{d=2}, (breakPointLinear,\nevent, single, ((0,0),(119,2)))\nA final example uses the MarkovGeneratorAnalysis ParameterObject to demonstrate the use of\ndynamic order values with a Markov transition string generated by analysis of an embedded\nParameterObject. In this example second order and lower Markov analysis is performed on thirty\nvalues from a sine wave with a period of thirty events. The resulting transition data is used to\ngenerate new values, with the output Markov generator order determined by an embedded\nMarkovValue ParameterObject. This ParameterObject, using a zero order Markov chain, produces\norder values weighted toward second order, with less frequent first and zero order values. The\nvalues produced favor continuous segments of a re-generated sine wave, with interruptions of first\nand zero order re-generated fragments.\nExample 12. MarkovGeneratorAnalysis generation with a MarkovValue-controlled dynamic\norder\nmarkovGeneratorAnalysis, (waveSine, event, 30, 0, (constant, 0), (constant, 1)), 30,\n2, (markovValue, a{0}b{1}c{2}:{a=3|b=7|c=12}, (constant, 0))\n7. Conclusion\nMarkov chains offer a flexible CAAC tool for the probabilistic generation of parameter values. As a\ntool with a long history, opportunities for innovation are limited. As this study demonstrates, the\ndevelopment of more flexible parametric interfaces through powerful notations offers an avenue of\nexploration.\n\n8. Acknowledgments\nThanks to Paul Berg, Elizabeth Hoffman, and Paula Matthusen for offering comments on early\nversions of this paper.\nReferences\nAmes, C. 1989. \"The Markov Process as a Compositional Model: A Survey and Tutorial.\" Leonardo\n22(2): 175-187.\nAriza, C. 2005. An Open Design for Computer-Aided Algorithmic Music Composition: athenaCL. Ph.D.\nDissertation, New York University.\nAshby, R. 1956. An Introduction to Cybernetics. London: Chapman & Hall Ltd.\nAssayag, G. and S. Dubnov, O. Delerue. 1999. \"Guessing the Composer's Mind: Applying Universal\nPrediction to Musical Style.\" In Proceedings of the International Computer Music Conference. San\nFrancisco: International Computer Music Association. 496-499.\nBarbaud, P. 1968. La musique discipline scientifique. Paris: Dunod.\nBerg, P. 2003. Using the AC Toolbox. Den Haag: Institute of Sonology, Royal Conservatory.\nBremaud, P. 1999. Markov Chains. London: Springer.\nBrooks, F. P. and A. Hopkins, P. Neumann, W. V. Wright. 1957. \"An Experiment in Musical\nComposition.\" IRE Transcripts on Electronic Computers 6: 175-182.\nBuxton, W. 1975. Manual for the POD Programs. Utrecht: Institute of Sonology, University of Utrecht.\nCambouropoulos, E. 1994. \"Markov Chains As an Aid to Computer Assisted Composition.\" Musical\nPraxis 1(1): 41-52.\nChadabe, J. 1997. Electric Sound: The Past and Promise of Electronic Music. New Jersey: Prentice-Hall.\nCohen, J. E. 1962. \"Information Theory and Music.\" Behavioral Science 7(2): 137-163.\nCottle, D. M. 2005. \"Computer Music with examples in SuperCollider 3.\"\nDidkovsky, N. 2004. \"Java Music Specification Language, v103 update.\" In Proceedings of the\nInternational Computer Music Conference. San Francisco: International Computer Music Association.\n742-745.\nDobrian, C. 1995. MAX Reference. Mountain View: Opcode Systems.\nDodge, C. and T. A. Jerse. 1997. Computer Music: Synthesis, Composition, and Performance. New York:\nShirmer Books.\n\nHiller, L. 1969. \"Some Compositional Techniques Involving the Use of Computers.\" In Music by\nComputers. H. von Foerster and J. W. Beauchamp, eds. New York: John Wiley & Sons, Inc. 71\n83.\nHiller, L. 1970. \"Music Composed with Computers: An Historical Survey.\" In The Computer and\nMusic. H. B. Lincoln, ed. Ithaca: Cornell University Press. 42-96.\nHiller, L. and R. Baker. 1964. \"Computer Cantata: A Study in Compositional Method.\" Perspectives of\nNew Music 3(1): 62-90.\nHiller, L. and L. Isaacson. 1959. Experimental Music. New York: McGraw-Hill.\nJones, K. 1981. \"Compositional Applications of Stochastic Processes.\" Computer Music Journal 5(2):\n45-61.\nKoenig, G. M. 1970. \"Project Two - A Programme for Musical Composition.\" In Electronic Music\nReport. Utrecht: Institute of Sonology. 3.\nLyon, D. 1995. \"Using Stochastic Petri Nets for Real-Time Nth-Order Stochastic Composition.\"\nComputer Music Journal 19(4): 13-22.\nMcAlpine, K. and E. Miranda, S. Hoggar. 1999. \"Making Music with Algorithms: A Case-Study.\"\nComputer Music Journal 23(2): 19-30.\nMcCormack, J. 1996. \"Grammar Based Music Composition.\" In Complex Systems 96: From Local\nInteractions to Global Phenomena. R. Stocker, ed. Amsterdam: ISO Press.\nMiranda, E. R. 2000. Composing Music With Computers. Burlington: Focal Press.\nMiranda, E. R. and A. M. Junior. 2005. \"Granular Synthesis of Sounds Through Markov Chains with\nFuzzy Control.\" In Proceedings of the International Computer Music Conference. San Francisco:\nInternational Computer Music Association. 193-196.\nMoorer, J. 1972. \"Music and Computer Composition.\" Communications of the ACM 15(2): 104-113.\nNorris, J. R. 1998. Markov Chains. Cambridge: Cambridge University Press.\nOlson, H. F. and H. Belar. 1961. \"Aid to Music Composition Employing a Random Probability\nSystem.\" Journal of the Acoustical Society of America 33(9): 1163-1170.\nOrio, N. and F. Dechelle. 2001. \"Score Following Using Spectral Analysis and Hidden Markov\nModels.\" In Proceedings of the International Computer Music Conference. San Francisco: International\nComputer Music Association. 125-129.\nPachet, F. 2002. \"The Continuator: Musical Interaction with Style.\" In Proceedings of the International\nComputer Music Conference. San Francisco: International Computer Music Association. 211-218.\nPhillips, D. 2005. \"At the Sounding Edge: Introducing KeyKit.\" LINUX Journal. Internet:\nhttp://www.linuxjournal.com/article/8153\n\nPinkerton, R. C. 1956. \"Information Theory and Melody.\" Scientific American 194(2): 77-86.\nPolansky, L. and D. Rosenboom. 1985. \"HMSL.\" In Proceedings of the International Computer Music\nConference. San Francisco: International Computer Music Association. 243-250.\nQuastler, H. 1955. \"Discussion, following Mathematical theory of word formation, by W. Fucks.\" In\nInformation Theory: Third London Symposium. E. C. Cherry, ed. New York: Academic Press. 168.\nRoads, C. 1984. \"An Overview of Music Representations.\" In Musical Grammars and Computer\nAnalysis. Firenze: Leo S. Olschki. 7-37.\nRoads, C. 1996. The Computer Music Tutorial. Cambridge: MIT Press.\nSchwarz, D. 2004. Data-Driven Concatenative Sound Synthesis. Ph.D. Thesis, Ircam, University of Paris 6.\nShannon, C. E. 1948. \"A Mathematical Theory of Communication.\" Bell Systems Technical Journal 27:\n379-423, 623-656.\nShannon, C. E. and W. Weaver. 1949. A Mathematical Theory of Communication. Urbana: University of\nIllinois Press.\nSowa, J. F. 1957. \"A machine to compose music.\" In Geniac Manual. New York: Oliver Garfield\nCompany.\nSowa, J. F. 2005. Personal correspondence. 25 July 2005.\nTaube, H. 1989. \"Common Music: A Compositional Language in Common Lisp and CLOS.\" In\nProceedings of the International Computer Music Conference. San Francisco: International Computer\nMusic Association. 316-319.\nTipei, S. 1975. \"MP1 -- a Computer Program for Music Composition.\" In Proceedings of the Second\nAnnual Music Computation Conference. J. Beauchamp and J. Melby, eds. Urbana, Illinois: Office of\nContinuing Education and Public Service in Music. 68-82.\nTrivino-Rodriguez, J. L. and R. Morales-Bueno. 2001. \"Using Multiattribute Prediction Suffix\nGraphs to Predict and Generate Music.\" Computer Music Journal 25(3): 62-79.\nWiener, N. 1948. Cybernetics. Cambridge: MIT Press.\nWooller, R. and A. R. Brown. 2005. \"Investigating morphing algorithms for generative music.\" Third\nIteration: Third International Conference on Generative Systems in the Electronic Arts.\nXenakis, I. 1963. Musiques Formelles. Paris: Editions Richard-Masse.\nXenakis, I. 1965. \"Free Stochastic Music from the Computer. Programme of Stochastic music in\nFortran.\" Gravesaner Blatter 26.\nYoungblood, J. E. 1958. \"Style as Information.\" Journal of Music Theory 2(24).\nZicarelli, D. 1987. \"M and Jam Factory.\" Computer Music Journal 11(4): 13-29.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT21M_380S10_read_nlcaacs.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/21m-380-music-and-technology-algorithmic-and-generative-music-spring-2010/0d5e3b2b127074e9c5bb7fea1a783e31_MIT21M_380S10_read_nlcaacs.pdf",
      "content": "NAVIGATING THE LANDSCAPE OF COMPUTER AIDED\nALGORITHMIC COMPOSITION SYSTEMS: A\nDEFINITION, SEVEN DESCRIPTORS, AND A LEXICON\nOF SYSTEMS AND RESEARCH\nChristopher Ariza\nNew York University Graduate\nSchool of Arts and Sciences\nNew York, New York\n\nABSTRACT\nTowards developing methods of software comparison\nand analysis, this article proposes a definition of a\ncomputer aided algorithmic composition (CAAC)\nsystem and offers seven system descriptors: scale,\nprocess-time,\nidiom-affinity,\nextensibility,\nevent\nproduction, sound source, and user environment. The\npublic internet resource algorithmic.net is introduced,\nproviding a lexicon of systems and research in computer\naided algorithmic composition.\n1.\nDEFINITION OF A COMPUTER-AIDED\nALGORITHMIC COMPOSITION SYSTEM\nLabels such as algorithmic composition, automatic\ncomposition, composition pre-processing, computer-\naided\ncomposition\n(CAC),\ncomputer\ncomposing,\ncomputer music, procedural composition, and score\nsynthesis have all been used to describe overlapping, or\nsometimes identical, projects in this field. No attempt\nwill be made to distinguish these terms, though some\nhave tried (Spiegel 1989; Cope 1991, p. 220; Burns\n1994, p. 195; Miranda 2000, pp. 9-10; Taube 2004;\nGerhard and Hepting 2004, p. 505). In order to provide\ngreater specificity, a hybrid label is introduced: CAAC,\nor computer aided algorithmic composition. (This term\nis used in passing by Martin Supper (2001, p. 48).) This\nlabel is derived from the combination of two labels,\neach too vague for continued use. The label \"computer\naided composition\" lacks the specificity of using\ngenerative algorithms. Music produced with notation or\nsequencing software could easily be considered\ncomputer aided composition. The label \"algorithmic\ncomposition\" is likewise too broad, particularly in that it\ndoes not specify the use of a computer. Although Mary\nSimoni has suggested that \"because of the increased role\nof the computer in the compositional process,\nalgorithmic composition has come to mean the use of\ncomputers...\" (2003), there remain many historical and\ncontemporary compositional techniques that, while not\nemploying the computer, are properly described as\nalgorithmic. David Cope supports this view, stating that\n\"... the term 'computer' is not requisite to a definition\nof algorithmic composition...\" (1993, p. 24).\nSince 1955 a wide variety of CAAC systems have been\ncreated. Towards the aim of providing tools for software\ncomparison and analysis, this article proposes seven\nsystem descriptors. Despite Lejaren Hiller's well-known\nclaim that \"computer-assisted composition is difficult to\ndefine, difficult to limit, and difficult to systematize\"\n(Hiller 1981, p. 75), a definition is proposed.\nA CAAC system is software that facilitates the\ngeneration of new music by means other than the\nmanipulation of a direct music representation. Here,\n\"new music\" does not designate style or genre; rather,\nthe output of a CAAC system must be, in some manner,\na unique musical variant. An output, compared to the\nuser's representation or related outputs, must not be a\n\"copy,\" accepting that the distinction between a copy\nand a unique variant may be vague and contextually\ndetermined. This output may be in the form of any\nsound or sound parameter data, from a sequence of\nsamples to the notation of a complete composition. A\n\"direct music representation\" refers to a linear, literal, or\nsymbolic representation of complete musical events,\nsuch as an event list (a score in Western notation or a\nMIDI file) or an ordered list of amplitude values (a\ndigital audio file or stream). Though all representations\nof aural entities are necessarily indirect to some degree,\nthe distinction made here is not between these\nrepresentations and aural entities. Rather, a distinction is\nmade between the representation of musical entities\nprovided to the user and the system output. If the\nrepresentation provided to the user is the same as the\noutput, the representation may reasonably be considered\ndirect.\nA CAAC system permits the user to manipulate indirect\nmusical representations: this may take the form of\nincomplete musical materials (a list of pitches or\nrhythms), an equation, non-music data, an image, or\nmeta-musical descriptions. Such representations are\nindirect in that they are not in the form of complete,\nordered musical structures. In the process of algorithmic\ngeneration these indirect representations are mapped or\ntransformed into a direct music representation for\noutput. When working with CAAC software, the\ncomposer\narranges\nand\nedits\nthese\nindirect\nrepresentations. The software interprets these indirect\nmusic representations to produce musical structures.\nThis definition does not provide an empirical measure\nby which a software system, removed from use, can be\nisolated as a CAAC system. Rather, a contextual\nariza@flexatone.net\n\ndelineation of scope is provided, based in part on use\ncase. Consideration must be given to software design,\nfunctionality, and classes of user interaction.\nThis definition is admittedly broad, and says only what a\nCAAC system is not. This definition includes historic\nsystems such as the Experiments of Hiller and Isaacson\n(1959), Iannis Xenakis's SMP (1965), and Gottfried\nMichael Koenig's PR1 (1970a) and PR2 (1970b). In\nthese cases the user provides initial musical and non\nmusical data (parameter settings, value ranges, stockpile\ncollections), and these indirect representations are\nmapped into score tables. This definition likewise\nencompasses Xenakis's GENDYN (1992) and Koenig's\nSSP (Berg et al 1980). This definition includes any\nsystem that converts images (an indirect representation)\nto sound, such as Max Mathews and L. Rosler's Graphic\n1 system (1968) or Xenakis's UPIC (1992; Marino et al.\n1993). It does not matter how the images are made; they\nmight be from a cellular automaton, a digital\nphotograph, or hand-drawn. What matters is that the\nprimary user-interface is an indirect representation.\nSome systems may offer the user both direct and\nindirect music representations. If one representation is\nprimary, that representation may define the system; if\nboth representations are equally presented to the user, a\nclear distinction may not be discernible.\nThis definition excludes, in most use cases, notation\nsoftware. Notation software is primarily used for\nmanipulating and editing a direct music representation,\nnamely Western notation. New music is not created by\nnotation software: the output, the score, is the user-\ndefined representation. Recently, systems such as the\npopular\nnotation\napplications\nSibelius\n(Sibelius\nSoftware Limited) and Finale (MakeMusic! Inc.) have\nadded user-level interfaces for music data processing in\nthe form of specialized scripting languages or plug-ins.\nThese tools allow the user to manipulate and generate\nmusic data as notation. In this case, the script and its\nparameters are an indirect music representation and can\nbe said to have attributes of a CAAC system. This is\nnot, however, the primary user-level interface.\nThis definition excludes, in most use cases, digital audio\nworkstations, sequencers, and digital mixing and\nrecording environments. These tools, as with notation\nsoftware, are designed to manipulate and output a direct\nmusic representation. The representation, in this case, is\nMIDI note data, digital audio files, or sequences of\nevent data. Again, new music is not created. The output\nis the direct representation that has been stored, edited,\nand processed by the user. Such systems often have\nmodular processors (plug-ins or effects) for both MIDI\nand digital audio data. Some of these processors allow\nthe user to control music data with indirect music\nrepresentations. For example, a MIDI processor might\nimplement an arpeggiator, letting the user, for a given\nbase note, determine the scale, size, and movement of\nthe arpeggio. In this case the configuration of the\narpeggio is an indirect representation, and can be said to\nhave attributes of a CAAC system. This is not, however,\nthe primary user-level interface.\n2.\nRESEARCH IN CATEGORIZING\nCOMPOSITION SYSTEMS\nThe number and diversity of CAAC systems, and the\ndiversity of interfaces, platforms, and licenses, have\nmade\ncategorization\nelusive.\nSignificant\ngeneral\noverviews of computer music systems have been\nprovided by Curtis Roads (1984, 1985), Loy and Curtis\nAbbott (1985), Bruce Pennycook (1985), Loy (1989),\nand Stephen Travis Pope (1993). These surveys,\nhowever,\nhave\nnot\nfocused\non\ngenerative\nor\ntransformational systems.\nPennycook (1985) describes five types of computer\nmusic interfaces: (1) composition and synthesis\nlanguages, (2) graphics and score editing environments,\n(3)\nperformance\ninstruments,\n(4)\ndigital\naudio\nprocessing tools, and (5) computer-aided instruction\nsystems. This division does not attempt to isolate CAAC\nsystems\nfrom\ntools\nused\nin\nmodifying\ndirect\nrepresentations, such as score editing and digital audio\nprocessing. Loy (1989, p. 323) considers four types of\nlanguages: (1) languages used for music data input, (2)\nlanguages for editing music, (3) languages for\nspecification of compositional algorithms, and (4)\ngenerative\nlanguages.\nThis\ndivision\nlikewise\nintermingles tools for direct representations (music data\ninput and editing) with tools for indirect representations\n(compositional algorithms and generative languages).\nPope's \"behavioral taxonomy\" (1993, p. 29), in focusing\non how composers interact with software, is near to the\ngoals of this study, but is likewise concerned with a\nmuch broader collection of systems, including \"...\nsoftware-\nand hardware-based systems for music\ndescription, processing, and composition ...\" (1993, p.\n26). Roads survey of \"algorithmic composition systems\"\ndivides software systems into four categories: (1) self-\ncontained\nautomated\ncomposition\nprograms,\n(2)\ncommand languages, (3) extensions to traditional\nprogramming languages, (4) and graphical or textual\nenvironments including music programming languages\n(1996, p. 821). This division also relates to the\nperspective taken here, though neither degrees of \"self\ncontainment\" nor distinctions between music languages\nand language extensions are considered.\nTexts that have attempted to provide an overview of\nCAAC systems in particular have generally used one of\nthree modes of classification: (1) chronological (Hiller\n1981; Burns 1994), (2) division by algorithm type\n(Dodge and Jerse 1997, p. 341; Miranda 2000), or (3)\ndivision by output format or output scale (Buxton 1978,\np. 10; Laske 1981, p. 120). All of these methods,\nhowever, fail to isolate important attributes from the\nperspective of the user and developer. A chronological\napproach offers little information on similarities\nbetween historically disparate systems, and suggests,\n\nincorrectly, that designs have developed along a linear\ntrajectory.\nMany\ncontemporary\nsystems\nsupport\nnumerous types of algorithms, and numerous types of\noutput formats. This article proposes seven possible, and\nequally valid, descriptors of CAAC systems.\n3.\nPRIMARY DESCRIPTORS\n3.1. The Difficulty of Distinctions\nComparative software analysis is a difficult task, even if\nthe software systems to be compared share a common\npurpose. Despite these challenges, such a comparison\noffers a useful vantage. Not only does a comparative\nframework demonstrate the diversity of systems\navailable, it exposes similarities and relationships that\nmight not otherwise be perceived.\nIn order to describe the landscape of software systems, it\nis necessary to establish distinctions. Rather than\nfocusing on chronology, algorithms, or output types, this\narticle proposes seven descriptors of CAAC system\ndesign. These descriptors are scale, process-time, idiom-\naffinity, extensibility, event production, sound source,\nand user environment. All systems can, in some fashion,\nbe defined by these descriptors. For each descriptor, a\nrange of specifications are given. These specifications,\nin some cases, represent a gradient. In all cases these\nspecifications are non-exclusive: some systems may\nhave aspects of more than one specification for a single\ndescriptor. Importantly, all CAAC systems have some\naspect of each descriptor. The use of multiple\ndescriptors to describe a diverse field of systems is\ndemonstrated by Pope in his \"taxonomy of composer's\nsoftware\"\n(1993),\nwhere\neighteen\ndifferent\n\"dimensions\" are proposed and accompanied by fifteen\ntwo-dimensional system graphs. Unlike the presentation\nhere, however, some of Pope's dimensions are only\napplicable to certain systems. John Biles, in his\n\"tentative taxonomy\" of evolutionary music systems\n(2003), likewise calls such descriptors \"dimensions.\"\nIt is unlikely that an objective method for deriving and\napplying a complete set of software descriptors is\npossible in any application domain, let alone in one that\nintegrates\nwith\nthe\ncreative\nprocess\nof\nmusic\ncomposition. Consideration of use case, technological\nchange, and the nature of creative production requires\nbroad categories with specifications that are neither\nmutually exclusive nor quantifiable. The assignment of\nspecifications, further, is an interpretation open to\nalternatives. Though this framework is broad, its\nimprecision permits greater flexibility than previous\nattempts, while at the same time clearly isolating\nessential aspects of closely related systems from the\nentire history of the field.\n3.2. Scale: Micro and Macro Structures\nThe scale of a CAAC system refers to the level of\nmusical structures the system produces. Two extremes\nof a gradient are defined: micro and macro. Micro\nstructures are musical event sequences commonly\nreferred to as sound objects, gestures, textures, or\nphrases: small musical materials that require musical\ndeployment in larger structures. Micro structures scale\nfrom the level of samples and grains to collections of\nnote events. In contrast, macro structures are musical\nevent sequences that approach complete musical works.\nMacro structures often articulate a musical form, such as\na sonata or a chorale, and may be considered complete\ncompositions. The concept of micro and macro\nstructures closely relates to what Eduardo Reck Miranda\n(2000) calls bottom-up and top-down organizations,\nwhere bottom-up composition begins with micro\nstructures, and top-down composition begins with\nmacro structures.\nAlternative time-scale labels for musical structures have\nbeen proposed. Horacio Vaggione has defined the lower\nlimit of the macro-time domain as the note, while the\nmicro-time domain is defined as sub-note durations on\nthe order of milliseconds (2001, p. 60). Roads, in\nMicrosound (2002, pp. 3-4), expands time into nine\nscales: infinite, supra, macro, meso, sound object,\nmicro, sample, subsample, and infinitesimal. Macro, in\nthe usage proposed here, refers to what Roads calls both\nmacro and meso, while micro refers to what Roads calls\nmeso, sound object, micro, and sample. Unlike the\nboundaries defined by Roads and Vaggione, the\ndistinctions here are more fluid and highly dependent on\ncontext and musical deployment. Musical structure and\ntemporal scales are, in part, a matter of interpretation. A\ncomposer may choose to create a piece from a single\ngesture, or to string together numerous large-scale\nforms.\nSuch a coarse distinction is useful for classifying the\nspectrum of possible outputs of CAAC systems. A few\nexamples demonstrate the context-dependent nature of\nthis descriptor. Xenakis's GENDYN, for instance, is a\nsystem specialized toward the generation of micro\nstructures: direct waveform break-points at the level of\nthe sample. Although Xenakis used this system to\ncompose entire pieces (GENDY3 (1991), S709 (1994)),\nthe design of the software is specialized for micro\nstructures. Though the system is used to generate music\nover a large time-span, there is little control over large-\nscale\nform\n(Hoffman\n2000).\nKemal\nEbcioglu's\nCHORAL system (1988), at the other extreme, is a\nsystem designed to create a complete musical form: the\nBach chorale. Though the system is used to generate\nmusic over a relatively short time-span, concepts of\nlarge-scale form are encoded in the system.\n\n3.3. Process Model: Real-Time and Non-Real-Time\nThe process model of a CAAC system refers to the\nrelationship between the computation of musical\nstructures and their output. A real-time (RT) system\noutputs each event after generation along a scheduled\ntime line. A non-real-time (NRT) system generates all\nevents first, then provides output. In the context of a RT\nCAAC system, the calculation of an event must be\ncompleted before its scheduled output. Some systems\noffer a single process model while others offer both.\nWhether a system is RT or NRT determines, to a certain\nextent, the types of operations that can be completed.\nRT processes are a subset of NRT processes: some\nprocesses that can be done in NRT cannot be done in\nRT. For example, a sequence of events cannot be\nreversed or rotated in RT (this would require knowledge\nof future events). Mikael Laurson, addressing the\nlimitations of RT compositional processes, points out\nthat a RT process model \"can be problematic, or even\nharmful\": \"composition is an activity that is typically\n'out-of-time'\" and further, \"there are many musical\nproblems that cannot be solved in real time ... if we\ninsist on real-time performance, we may have to\nsimplify the musical result\" (1996, p. 19). Though a\nCAAC system need not model traditional cognitive\ncompositional\nactivities\n(whether\nout-of-time\nor\notherwise),\na\nRT process\nmodel does\nenforce\ncomputational limits.\nIn general, a RT system is limited to linear processes:\nonly one event, or a small segment of events (a buffer, a\nwindow, or a frame), can be processed at once. A NRT\nsystem is not limited to linear processes: both linear and\nnonlinear processing is available. A nonlinear process\nmight create events in a sequential order different than\ntheir eventual output order. For example, event start\ntimes might be determined by a Gaussian distribution\nwithin defined time boundaries: the events will not be\ncreated in the order of their ultimate output. A RT\nsystem, however, has the obvious advantage of\nimmediate interaction. This interaction may be in\nresponse to the composer or, in the case of an interactive\nmusic system, in response to other musicians or physical\nenvironments.\nAs with other distinctions, these boundaries are not\nrigid. A RT system might, instead of one event at a time,\ncollect events into a frame and thus gain some of the\nfunctionality of NRT processing. Similarly, a NRT\nsystem, instead of calculating all events at once, might\nlikewise calculate events in frames and then output these\nframes in RT, incurring a small delay but simulating RT\nperformance.\nLeland Smith's SCORE system (1972), for example, has\na NRT process model: music, motives, and probabilities\nare specified in a text file for each parameter, and this\nfile is processed to produce a score. James McCartney's\nSuperCollider language (1996) has a RT process model:\nwith SuperCollider3 (SC3), instrument definitions\n(SynthDefs) are instantiated as nodes on a server and\nrespond to RT messages (McCartney 2002).\n3.4. Idiom-Affinity: Singular and Plural\nIdiom-affinity refers to the proximity of a system to a\nparticular musical idiom, style, genre, or form. Idiom,\nan admittedly broad term, is used here to refer\ncollectively to many associated terms. All CAAC\nsystems, by incorporating some minimum of music-\nrepresentation constructs, have an idiom-affinity. A\nsystem with a singular idiom-affinity specializes in the\nproduction of one idiom (or a small collection of related\nidioms), providing tools designed for the production of\nmusic in a certain form, from a specific time or region,\nor by a specific person or group. A system with a plural\nidiom-affinity allows the production of multiple musical\nstyles, genres, or forms.\nThe idea of idiom-affinity is general. If a system offers\nonly one procedural method of generating event lists,\nthe system has a singular idiom-affinity. Idiom-affinity\ntherefore relates not only to the design of low-level\nrepresentations, but also to the flexibility of the large-\nscale music generators. The claim that all CAAC\nsystems have an idiom-affinity has been affirmed by\nmany researchers. Barry Truax states that, regardless of\na system designer's claims, \"all computer music systems\nexplicitly and implicitly embody a model of the musical\nprocess that may be inferred from the program and data\nstructure of the system...\" (1976, p. 230). The claim that\nall systems have an idiom-affinity challenges the goal of\n\"musical neutrality,\" a term used by Laurson to suggest\nthat \"the hands of the user should not be tied to some\npredefined way of thinking about music or to a certain\nmusical style\" (1996, p. 18). Laurson claims, contrary to\nthe view stated here, that by creating primitives that\nhave broad applicability and allowing for the creation of\nnew primitives, a system can maintain musical\nneutrality despite the incorporation of \"powerful tools\nfor representing musical phenomena\" (1996, p. 18).\nMusical neutrality can be approached, but it can never\nbe fully realized.\nKoenig's PR1 (1970a), for example, is a system with a\nsingular idiom-affinity: the system, designed primarily\nfor personal use by Koenig, exposes few configurable\noptions to the user and, in its earliest versions, offers the\nuser no direct control over important musical parameters\nsuch as form and pitch. Paul Berg's AC Toolbox (2003)\nhas a plural idiom-affinity: low level tools and objects\n(such as data sections, masks, and stockpiles) are\nprovided, but are very general, are not supplied with\ndefaults, and can be deployed in a variety of\nconfigurations.\n3.5. Extensibility: Closed and Open\nExtensibility refers to the ability of a software system to\nbe extended. This often means adding code, either in the\n\nform of plug-ins or other modular software components.\nIn terms of object-oriented systems, this is often done by\ncreating a subclass of a system-defined object, inheriting\nlow-level\nfunctionality\nand\na\nsystem-compatible\ninterface. An open system allows extensibility: new\ncode can be added to the system by the user. A closed\nsystem does not allow the user to add code to the system\nor change its internal processing in any way other than\nthe parameters exposed to the user.\nIn terms of CAAC systems, a relationship often exists\nbetween the extensibility of a system and its idiom-\naffinity. Systems that have a singular idiom-affinity tend\nto be closed; systems that have a plural idiom-affinity\ntend to be open. All open-source systems, by allowing\nusers to manipulate system source code, have open\nextensibility. Closed-source systems may or may not\nprovide open extensibility.\nJoel Chadabe's and David Zicarelli's M (Zicarelli 1987;\nChadabe 1997, p. 316), for instance, is a closed, stand\nalone application: though highly configurable, new\ncode, objects, or models cannot be added to the system\nor interface. Miller Puckette's cross-platform PureData\n(1997) is an open system: the language is open source\nand extensible through the addition of compiled\nmodules programmed in C.\n3.6. Event Production: Generation and\nTransformation\nA distinction can be made between the generation of\nevents from indirect music representations (such as\nalgorithms or lists of musical materials) and the\ntransformation of direct music representations (such as\nMIDI files) with indirect models. Within some CAAC\nsystems, both processes are available, allowing the user\nto work with both the organization of generators and the\nconfiguration of transformers. Some systems, on the\nother hand, focus on one form over another. The\ndivision between generators and transformers, like other\ndistinctions, is fluid and contextual.\nAndre Bartetzki's Cmask system (1997) allows the\ngeneration of event parameters with a library of\nstochastic functions, generators, masks, and quantizers.\nTools for transformation are not provided. Cope's EMI\nsystem (1996) employs a transformational model,\nproducing new music based on analyzed MIDI files,\nextracting and transforming compositional patterns and\nsignatures. Tools are not provided to generate events\nwithout relying on structures extracted from direct\nrepresentations.\n3.7. Sound Source: Internal, Exported, Imported,\nExternal\nAll CAAC systems produce event data for sound\nproduction. This event data can be realized by different\nsound sources. In some cases a system contains both the\ncomplete definition of sound-production components\n(instrument algorithms), and is capable of internally\nproducing the sound through an integrated signal\nprocessing engine. The user may have complete\nalgorithmic control of not only event generation, but\nsignal processing configuration. Such a system has an\ninternal sound source. In other cases a system may\nexport\ncomplete\ndefinitions\nof\nsound-production\ncomponents (instrument algorithms) to another system.\nThe user may have limited or complete control over\nsignal\nprocessing\nconfiguration,\nbut\nthe\nactual\nprocessing is exported to an external system. For\nexample, a system might export Csound instrument\ndefinitions or SuperCollider SynthDefs. Such a system\nhas an exported sound source. In a related case a CAAC\nsystem may import sound source information from an\nexternal system, automatically performing necessary\ninternal configurations. For example, loading instrument\ndefinitions into a synthesis system might automatically\nconfigure their availability and settings in a CAAC\nsystem. Such a system has an imported sound source. In\nthe last case a system may define the sound source only\nwith a label and a selection of sound-source parameters.\nThe user has no control over the sound source except\nthrough values supplied to event parameters. Examples\ninclude a system that produces Western notation for\nperformance by acoustic instruments, or a system that\nproduces a Csound score for use with an external\nCsound orchestra. Such a system has an external sound\nsource. As with other descriptors, some systems may\nallow for multiple specifications.\nRoger Dannenberg's Nyquist (1997a, 1997b) is an\nexample of a system with an internal sound source: the\nlanguage provides a complete synthesis engine in\naddition\nto\nindirect\nmusic\nrepresentations.\nThe\nathenaCL system (Ariza 2005) is an example of a\nsystem that uses an exported sound source: a Csound\norchestra file can be dynamically constructed and\nconfigured each time an event list is generated. Heinrich\nTaube's Common Music (1991) supports an imported\nsound source: Common Lisp Music (CLM) instruments,\nonce loaded, are automatically registered within CM\n(1997, p. 30). Clarence Barlow's Autobusk system\n(1990) uses an external sound source: the system\nprovides RT output for MIDI instruments.\n3.8. User Environment: Language, Batch, Interactive\nThe user environment is the primary form in which a\nCAAC system exposes its abstractions to the user, and it\nis the framework in which the user configures these\nabstractions. A CAAC system may provide multiple\nenvironments, or allow users to construct their own\nenvironments and interfaces. The primary environment\nthe system presents to the user can, however, be\nisolated.\nLoy (1989, p. 319) attempts to distinguish languages,\nprograms, and (operating) systems. Contemporary\nsystems, however, are not so discrete: a \"program\" may\nallow internal scripting or external coding through the\n\nprogram's API; a \"language\" may only run within a\nplatform-specific program. Particularly in the case of\nCAAC systems, where minimal access to code-level\ninterfaces is common, the division between \"language\"\nand \"program\" is not useful. Such features are here\nconsidered aspects of user environment. Language,\nbatch, and interactive environments are isolated (and not\ndiscrete) because they involve different types of\ncomputer-user interaction. Loy even considers some\nsystems, such as Koenig's PR1 and PR2, to be\nlanguages (1989, p. 324), even though, in the context of\ncomputer-user interaction, it has never been possible to\nprogram in the \"language\" of either system.\nA language interface provides the user with an artificial\nlanguage to design and configure music abstractions.\nThere are two forms of languages: text and graphic. A\ntext language is composed with standard text-editors,\nand\nincludes\nprogramming\nlanguages,\nmarkup-\nlanguages, or formal languages and grammars. A\ngraphic language (sometimes called a visual language)\nis used within a program that allows the organization of\nsoftware\ncomponents\nas\nvisual\nentities,\nusually\nrepresented as a network of interconnected boxes upon a\ntwo-dimensional plane. A box may have a set of inputs\nand\noutputs;\ncommunication\nbetween\nboxes\nis\nconfigured by drawing graphic lines from inputs to\noutputs.\nLaurson\n(1996)\nprovides\na\nthorough\ncomparison of text and graphic languages. He\nsummarizes differences between the two paradigms: text\nlanguages\noffer\ncompliance\nwith\nstandards,\ncompactness, and speed, whereas graphic languages\noffer intuitive programming logic, intuitive syntax,\ndefaults, and error checking (1996, p. 16). These\ndifferences are not true for all languages: some visual\nlanguages offer speed, while some text languages offer\nan intuitive syntax.\nA batch interface is a system that only permits the user\nto provide input data, usually in the form of a text file or\na list of command-line options. The input data, here\ncalled a manifest, is processed and the program returns a\nresult. As Roads points out, batch processes refer \"... to\nthe earliest computer systems that ran one program at a\ntime; there was no interaction with the machine besides\nsubmitting a deck of punched paper cards for execution\nand picking up the printed output\" (1996, p. 845).\nModern batch systems, in addition to being very fast,\noffer\nconsiderably\ngreater\nflexibility\nof\ninput\nrepresentation. Though an old model, batch processing\nis still useful and, for some tasks, superior to interaction.\nThe manifest may resemble a text programming\nlanguage, but often lacks the expressive flexibility of a\ncomplete language. A batch system does not permit an\ninteractive-session: input is processed and returned in\none operation. What is desired from the software must\nbe completely specified in the manifest. Curiously, Pope\ndefines a batch system as distinct from RT and \"rapid\nturnaround\" systems not by its particular interface or\nuser environment, but by \"... the delay between the\ncapture or description of signal, control, or event and its\naudible effect\" (1993, p. 29). More than just a\nperformance constraint, modern batch environments\ndefine a particular form of user interaction independent\nof performance time or process model.\nAn interactive interface allows the user to issue\ncommands and, for each command, get a response.\nInteractive\ninterfaces\nusually\nrun\nin\na\nsession\nenvironment: the user works inside the program,\nexecuting discrete commands and getting discrete\nresponses. Interactive interfaces often have tools to help\nthe user learn the system, either in the form of help\nmessages, error messages, or user syntax correction.\nInteractive interfaces often let the user browse the\nmaterials that they are working with and the resources\navailable in the system, and may provide numerous\ndifferent representations of these materials. Such a\nsystem may be built with text or graphics. Focusing on\ninteractive systems over interactive interfaces, Roads\ndistinguishes between (1) \"... light interactions\nexperienced in a studio-based 'composing environment,'\nwhere there is time to edit and backtrack...\" and (2) \"...\nreal-time interaction experienced in working with a\nperformance system onstage, where ... there is no time\nfor editing\" (1996, p. 846). While this distinction is\nvaluable for discussing context-based constraints of\nsystem use, many CAAC systems, with either language\ninterfaces or interactive interfaces, support both types of\nsystem interaction as described by Roads. Here, use of\ninteraction refers more to user-system interaction in\nNRT or RT production, rather than user-music\ninteraction in RT production.\nAn interactive text interface is a program that takes\ninput from the user as text, and provides text output.\nThese systems often operate within a virtual terminal\ndescended from the classic DEC VT05 (1975) and\nVT100 (1978) hardware. The UNIX shell is a common\ntext interface. Contemporary text interfaces interact with\nthe operating system and window manager, allowing a\nbroad range of functionality including the production of\ngraphics. These graphics, in most cases, are static and\ncannot be used to manipulate internal representations.\nAn interactive text interface system may have a graphic\nuser interface (GUI). Such a system, despite running in\na graphic environment, conducts user interaction\nprimarily with text. An interactive graphics interface\nemploys a GUI for the configuration and arrangement of\nuser-created\nentities.\nUsers\ncan\nalter\nmusical\nrepresentations by directly designing and manipulating\ngraphics.\nAs with other descriptors, these specifications are not\nexclusive. A CAAC system may offer aspects of both a\ngraphical and a textual programming language. The\nmanifest syntax of a batch system may approach the\nflexibility of a complete text language. An interactive\ntext or graphics system may offer batch processing or\naccess to underlying system functionality as a language-\nbased\nApplication\nProgramming\nInterface (API).\nDespite\nthese\noverlapping\nenvironments,\nit\nis\n\nnonetheless useful, when possible, to classify a system\nby its primary user-level interface.\nWilliam Schottstaedt's Pla system (1983) is an example\nof a text language. Laurson's Patchwork system\n(Laurson and Duthen 1989) provides an example of a\ngraphical language. Mikel Kuehn's nGen (2001) is a\nbatch user environment: the user creates a manifest, and\nthis file is processed to produced Csound scores. Joel\nChadabe's PLAY system demonstrates an interactive\ntext\ninterface,\nproviding\nthe\nuser\na\nshell-like\nenvironment for controlling the system (1978). Finally,\nLaurie Spiegel's Music Mouse system (1986) provides\nan example of an interactive graphic system.\n4.\nALGORITHMIC.NET\nThe definition and seven descriptors presented above\nare the result of extensive research in CAAC systems,\nmuch of which is beyond the scope of this article. This\nresearch has been made publicly available in the form of\na website titled \"algorithmic.net.\" This site provides a\nbibliography of over one thousand resources in CAAC\nand a listing of over eighty contemporary and historic\nsoftware systems. For each system, references, links,\ndescriptions, and specifications for the seven descriptors\ndescribed above are provided. Flexible web-based tools\nallow users to search and filter systems and references,\nas well as to contribute or update information in the\nalgorithmic.net database. The ultimate goal of this site is\na collaborative lexicon of research in computer aided\nalgorithmic music composition.\n5.\nACKNOWLEDGEMENTS\nThis research was funded in part by a grant from the\nUnited States Fulbright program, the Institute for\nInternational Education (IIE), and the Netherlands-\nAmerica Foundation (NAF) for research at the Institute\nof Sonology, The Hague, the Netherlands. Thanks to\nPaul Berg and Elizabeth Hoffman for commenting on\nearlier versions of this article, and to the ICMC's\nanonymous reviewers for valuable commentary and\ncriticism.\n6.\nREFERENCES\nAriza, C. 2005. An Open Design for Computer-Aided\nAlgorithmic Music Composition: athenaCL. Ph.D.\nDissertation, New York University.\nBarlow, C. 1990. \"Autobusk: An algorithmic real-time\npitch and rhythm improvisation programme.\" In\nProceedings of the International Computer Music\nConference. San Francisco: International Computer\nMusic Association. 166-168.\nBartetzki, A. 1997. \"CMask, a Stochastic Event\nGenerator for Csound.\" Internet: http://web.archive.org/web/\n20080501095420/http://gigant.kgw.tu-berlin.de/~abart/CMaskMan/\nCMask-Manual.htm\nBerg, P. and R. Rowe, D. Theriault. 1980. \"SSP and\nSound Description.\" Computer Music Journal 4(1): 25\n35.\nBerg, P. 2003. Using the AC Toolbox. Den Haag:\nInstitute of Sonology, Royal Conservatory.\nBiles, J. A. 2003. \"GenJam in Perspective: A Tentative\nTaxonomy for GA Music and Art Systems.\" Leonardo\n36(1): 43-45.\nBurns, K. H. 1994. The History and Development of\nAlgorithms in Music Composition, 1957-1993. D.A.\nDissertation, Ball State University.\nChadabe, J. 1978. \"An Introduction to the Play\nProgram.\" Computer Music Journal 2(1).\n------. 1997. Electric Sound: The Past and Promise of\nElectronic Music. New Jersey: Prentice-Hall.\nCope, D. 1991. Computers and Musical Style. Oxford:\nOxford University Press.\n------. 1993. \"Algorithmic Composition [re]Defined.\"\nIn Proceedings of the International Computer Music\nConference. San Francisco: International Computer\nMusic Association. 23-25.\n------. 1996. Experiments in Music Intelligence.\nMadison, WI: A-R Editions.\nDannenberg, R. B. 1997a. \"The Implementation of\nNyquist, A Sound Synthesis Language.\" Computer\nMusic Journal 21(3): 71-82.\n------. 1997b. \"Machine Tongues XIX: Nyquist, a\nLanguage for Composition and Sound Synthesis.\"\nComputer Music Journal 21(3): 50-60.\nDodge, C. and T. A. Jerse. 1997. Computer Music;\nSynthesis, Composition, and Performance. Wadsworth\nPublishing Company.\nEbcioglu, K. 1988. \"An Expert System for Harmonizing\nFour-part Chorales.\" Computer Music Journal 12(3):\n43-51.\nGerhard, D. and D. H. Hepting. 2004. \"Cross-Modal\nParametric Composition.\" In Proceedings of the\nInternational\nComputer\nMusic\nConference.\nSan\nFrancisco: International Computer Music Association.\n505-512.\nHiller, L. 1981. \"Composing with Computers: A\nProgress Report.\" Computer Music Journal 5(4).\nHiller, L. and L. Isaacson. 1959. Experimental Music.\nNew York: McGraw-Hill.\nHoffman, P. 2000. \"A New GENDYN Program.\"\nComputer Music Journal 24(2): 31-38.\nKoenig, G. M. 1970a. \"Project One.\" In Electronic\nMusic Report. Utrecht: Institute of Sonology. 2: 32-46.\n\n------. 1970b. \"Project Two - A Programme for\nMusical Composition.\" In Electronic Music Report.\nUtrecht: Institute of Sonology. 3.\nKuehn, M. 2001. \"The nGen Manual.\" Internet:\nhttp://mustec.bgsu.edu/~mkuehn/ngen/man/\nngenman.htm.\nLaske, O. 1981. \"Composition Theory in Koenig's\nProject One and Project Two.\" Computer Music Journal\n5(4).\nLaurson, M. and J. Duthen. 1989. \"PatchWork, a\nGraphical Language in PreForm.\" In Proceedings of the\nInternational\nComputer\nMusic\nConference.\nSan\nFrancisco: International Computer Music Association.\n172-173.\nLaurson, M. 1996. Patchwork. Helsinki: Sibelius\nAcademy.\nLoy, D. G. 1989. \"Composing with Computers: a\nSurvey of Some Compositional Formalisms and Music\nProgramming Languages.\" In Current Directions in\nComputer Music Research. M. V. Mathews and J. R.\nPierce, eds. Cambridge: MIT Press. 291-396.\nLoy, D. G. and C. Abbott. 1985. \"Programming\nLanguages for Computer Music Synthesis, Performance,\nand Composition.\" ACM Computing Surveys 17(2).\nMarino, G. and M. Serra, J. Raczinski. 1993. \"The UPIC\nSystem: Origins and Innovations.\" Perspectives of New\nMusic 31(1): 258-269.\nMathews, M. V. and L. Rosler. 1968. \"Graphical\nLanguage for the Scores of Computer-Generated\nSounds.\" Perspectives of New Music 6(2): 92-118.\nMcCartney, J. 1996. \"SuperCollider: a New Real Time\nSynthesis\nLanguage.\"\nIn\nProceedings\nof\nthe\nInternational\nComputer\nMusic\nConference.\nSan\nFrancisco: International Computer Music Association.\n------. 2002. \"Rethinking the Computer Music\nLanguage.\" Computer Music Journal 26(4): 61-68.\nMiranda, E.\nR. 2000.\nComposing\nMusic With\nComputers. Burlington: Focal Press.\nPennycook, B. W. 1985. \"Computer Music Interfaces: A\nSurvey.\" In ACM Computing Surveys. New York: ACM\nPress. 17(2): 267-289.\nPope, S. T. 1993. \"Music Composition and Editing by\nComputer.\" In Music Processing. G. Haus, ed. Oxford:\nOxford University Press. 25-72.\nPuckette, M. 1997. \"Pure Data.\" In Proceedings of the\nInternational\nComputer\nMusic\nConference.\nSan\nFrancisco: International Computer Music Association.\n224-227.\nRoads,\nC.\n1984.\n\"An\nOverview\nof\nMusic\nRepresentations.\" In Musical Grammars and Computer\nAnalysis. Firenze: Leo S. Olschki. 7-37.\n------. 1985. \"Research in music and artificial\nintelligence.\" In ACM Computing Surveys. New York:\nACM Press. 17(2): 163-190.\n------.\n1996.\nThe\nComputer\nMusic\nTutorial.\nCambridge: MIT Press.\n------. 2002. Microsound. Cambridge: MIT Press.\nSchottstaedt, W. 1983. \"Pla: A Composer's Idea of a\nLanguage.\" Computer Music Journal 7(1).\nSimoni, M. 2003. Algorithmic Composition: A Gentle\nIntroduction to Music Composition Using Common\nLISP and Common Music. Ann Arbor: Scholarly\nPublishing\nOffice,\nthe\nUniversity\nof\nMichigan\nUniversity Library.\nSmith, L. 1972. \"SCORE - A Musician's Approach to\nComputer Music.\" Journal of the Audio Engineering\nSociety 20(1): 7-14.\nSpiegel, L. 1986. \"Music Mouse: An Intelligent In\nstrument.\" Internet: http://retiary.org/ls/programs.html.\n------. 1989. \"Distinguishing Random, Algorithmic,\nand Intelligent Music.\" Internet: http://retiary.org/ls/\nwritings/alg_comp_ltr_to_cem.html.\nSupper, M. 2001. \"A Few Remarks on Algorithmic\nComposition.\" Computer Music Journal 25(1): 48-53.\nTaube,\nH.\n1991.\n\"Common\nMusic:\nA\nMusic\nComposition Language in Common Lisp and CLOS.\"\nComputer Music Journal 15(2): 21-32.\n------. 1997. \"An Introduction to Common Music.\"\nComputer Music Journal 21(1): 29-34.\n------.\n2004.\nNotes\nfrom\nthe\nMetalevel:\nAn\nIntroduction to Computer Composition. Swets Zeitlinger\nPublishing.\nTruax, B. 1976. \"A Communicational Approach to\nComputer Sound Programs.\" Journal of Music Theory\n20(2): 227-300.\nVaggione, H. 2001. \"Some Ontological Remarks about\nMusic Composition Processes.\" Computer\nMusic\nJournal 25(1): 54-61.\nXenakis, I. 1965. \"Free Stochastic Music from the\nComputer. Programme of Stochastic music in Fortran.\"\nGravesaner Blatter 26.\n------. 1992. Formalized Music: Thought and\nMathematics in Music. Indiana: Indiana University\nPress.\nZicarelli, D. 1987. \"M and Jam Factory.\" Computer\nMusic Journal 11(4): 13-29.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n21M.380 Music and Technology: Algorithmic and Generative Music\nSpring 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}