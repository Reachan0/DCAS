{
  "course_name": "Database Systems",
  "course_description": "This course relies on primary readings from the database community to introduce graduate students to the foundations of database systems, focusing on basics such as the relational algebra and data model, schema normalization, query optimization, and transactions. It is designed for students who have taken 6.033 (or equivalent); no prior database experience is assumed, though students who have taken an undergraduate course in databases are encouraged to attend.",
  "topics": [
    "Business",
    "Information Technology",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Data Mining",
    "Software Design and Engineering",
    "Business",
    "Information Technology",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Data Mining",
    "Software Design and Engineering"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nOverview\n\nThis course is designed to introduce graduate students to the foundations of database systems, focusing on basics such as the relational algebra and data model, query optimization, query processing, and transactions. This is not a course on database design or SQL programming (though we will discuss these issues briefly). It is designed for students who have taken\n6.033\n(or equivalent); no prior database experience is assumed, though students who have taken an undergraduate course in databases are encouraged to attend.\n\nClasses will consist of lectures and discussions based on readings from the database literature. For 6.830, there will be a semester-long project, as well as two exams and 7 additional assignments -- 3 Java-based programming \"labs\" and 4 problem sets. For 6.814, exams and assignments are the same as 6.830, but instead of the semester project, there will be 2 additional labs.\n\nLectures\n\nAttendance at lectures is mandatory and you are expected to show up prepared to answer questions and participate in discussion.\n\nTopics Covered\n\nTopics related to the engineering and design of database systems, including: data models; database and schema design; schema normalization and integrity constraints; query processing; query optimization and cost estimation; transactions; recovery; concurrency control; isolation and consistency; distributed, parallel, and heterogeneous databases; adaptive databases; trigger systems; key-value stores; object-relational mappings; streaming databases; DB as a service. Lecture and readings from original research papers. 6.830 includes semester-long project and paper.\n\nPrerequisites\n\nStudents should have taken\n6.033 Introduction to Systems\nor equivalent as well as\n6.006 Introduction to Algorithms\nor equivalent. If you do not have experience in these subjects and would like to take the course, please contact the instructor. Prior database experience is not required.\n\nUnits\n\n3-0-9. 6.830 is a Grad-H class. It counts as an engineering concentration (EC) subject in Systems. For Area II Ph.D. students in EECS, it satisfies the Systems Technical Qualifying Exam requirement.\n\n6.814 is an undergraduate class designed to satisfy the Advanced Undergraduate Subject requirement in the EECS curriculum. It counts as an engineering concentration (EC) subject in Systems.\n\nGrading\n\nGrades are assigned based on labs, homeworks, 2 quizzes, final project (for 6.830), and class participation. The grading breakdown is as follows:\n\n6.830\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments (problem sets, labs)\n\n35% total\n\nExams\n\n20% each\n\nFinal project\n\n20%\n\nClass participation\n\n5%\n\n6.814\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments (problem sets, labs)\n\n50% total\n\nExams\n\n20% each\n\nClass participation\n\n10%\n\nEach student is allowed 3 \"late days\", each of which may be used to turn in one problem set or lab one class meeting later than it is due without penalty. After all three late days are used, assignments will be docked one letter grade for each class meeting they are late.\n\nLate days may not be used for the final project, lab 5 or exams. Regardless of late days, problem sets must be handed in before problem set solutions are posted, usually a week after the problem set is due.\n\nCollaboration Policy\n\nFor problem sets and labs, you are allowed to discuss your answers with other students, but please write up your own answers and list your collaborators. Copying solutions from other students is never allowed. For the group project you will work in teams and hand in only one written report.\n\nText\n\nThe course readings will primarily be drawn from the following sources:\n\nHellerstein, Joseph, and Michael Stonebraker.\nReadings in Database Systems (The Red Book)\n. 4th ed. MIT Press, 2005. ISBN: 9780262693141.\n\nRamakrishnan, Raghu, and Johannes Gehrke.\nDatabase Management Systems\n. 3rd ed. McGraw-Hill, 2002. ISBN: 9780072465631.\n\nNote that the 3rd edition of\nReadings in Database Systems\nis a substantially different text (it does not include the same readings as the 4th edition.)\n\nSupplemental Readings\n\nPlease see the\nReadings\nsection for other readings and discussion questions.\n\n6.033 is a prerequisite. You may wish to review the 6.033 class notes, especially during our discussion of transactions and logging.",
  "files": [
    {
      "category": "Resource",
      "title": "MIT6_830F10_lab1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/6d426ed098eb42a869a5db6d2d506a10_MIT6_830F10_lab1.pdf",
      "content": "6.830 Lab 1: SimpleDB\n6.830 Lab 1: SimpleDB\nAssigned: Wednesday, September 21\nDue: Friday, October 1 11:59 PM EDT\nVersion History:\n‚óè September 17: First revision.\nIn the lab assignments in 6.830 you will write a basic database management system called SimpleDB. For this\nlab, you will focus on implementing the core modules required to access stored data on disk; in future labs, you\nwill add support for various query processing operators, as well as transactions, locking, and concurrent queries.\nSimpleDB is written in Java. We have provided you with a set of mostly unimplemented classes and interfaces.\nYou will need to write the code for these classes. We will grade your code by running a set of system tests\nwritten using JUnit. We have also provided a number of unit tests, which we will not use for grading but that\nyou may find useful in verifying that your code works.\nThe remainder of this document describes the basic architecture of SimpleDB, gives some suggestions about\nhow to start coding, and discusses how to hand in your lab.\nWe strongly recommend that you start as early as possible on this lab. It requires you to write a fair amount\nof code!\n0. Find bugs, be patient, earn candy bars\nSimpleDB is a relatively complex piece of code. It is very possible you are going to find bugs, inconsistencies,\nand bad, outdated, or incorrect documentation, etc.\nWe ask you, therefore, to do this lab with an adventurous mindset. Don't get mad if something is not clear, or\neven wrong; rather, try to figure it out yourself or send us a friendly email. We promise to help out by posting\nbug fixes, new tarballs, etc., as bugs and issues are reported.\n...and if you find a bug in our code, we'll give you a candy bar (see Section 3.3)!\n1. Getting started\nThese instructions are written for Athena or any other Unix-based platform (e.g., Linux, MacOS, etc.) Because\nthe code is written in Java, it should work under Windows as well, though the directions in this document may\nnot apply.\nWe have included Section 1.2 on using the project with Eclipse.\nDownload the code from http://db.csail.mit.edu/6.830/6.830-lab1.tar.gz and untar it. For example:\n$ wget http://db.csail.mit.edu/6.830/6.830-lab1.tar.gz\n$ tar xvzf 6.830-lab1.tar.gz\n$ cd 6.830-lab1\n[on Athena: add sipb]\n[on Athena: add -f java]\n*Athena is MIT's UNIX-based computing environment. OCW does not provide access to it.\n*\n\n6.830 Lab 1: SimpleDB\nSimpleDB uses the Ant build tool to compile the code and run tests. Ant is similar to make, but the build file\nis written in XML and is somewhat better suited to Java code. Most modern Linux distributions include Ant.\nUnder Athena, it is included in the sipb locker, which you can get to by typing add sipb at the Athena\nprompt. Note that on some versions of Athena you must also run add -f java to set the environment correctly\nfor Java programs. See the Athena documentation on using Java for more details.\nTo help you during development, we have provided a set of unit tests in addition to the end-to-end tests that we\nuse for grading. These are by no means comprehensive, and you should not rely on them exclusively to verify\nthe correctness of your project (put those 6.170 skills to use!).\nTo run the unit tests use the test build target:\n$ cd 6.830-lab1\n$ # run all unit tests\n$ ant test\n$ # run a specific unit test\n$ ant runtest -Dtest=TupleTest\nYou should see output similar to:\n# build output...\ntest:\n[junit] Running simpledb.CatalogTest\n[junit] Testsuite: simpledb.CatalogTest\n[junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 0.037 sec\n[junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 0.037 sec\n# ... stack traces and error reports ...\nThe output above indicates that two errors occurred during compilation; this is because the code we have given\nyou doesn't yet work. As you complete parts of the lab, you will work towards passing additional unit tests. If\nyou wish to write new unit tests as you code, they should be added to the test/simpledb directory.\nFor more details about how to use Ant, see the manual. The Running Ant section provides details about using\nthe ant command. However, the quick reference table below should be sufficient for working on the labs.\nCommand\nDescription\nant\nBuild the default target (for simpledb, this is dist).\nant -projecthelp\nList all the targets in build.xml with descriptions.\nant dist\nCompile the code in src and package it in dist/simpledb.jar.\nant test\nCompile and run all the unit tests.\nant runtest -Dtest=testname\nRun the unit test named testname.\nant systemtest\nCompile and run all the system tests.\n*\n*\n*\n*\n*Athena is MIT's UNIX-based computing environment. OCW does not provide access to it.\n\n6.830 Lab 1: SimpleDB\nant runsystest -Dtest=testname\nCompile and run the system test named testname.\n1.1. Running end-to-end tests\nWe have also provided a set of end-to-end tests that will eventually be used for grading. These tests are\nstructured as JUnit tests that live in the test/simpledb/systemtest directory. To run all the system tests,\nuse the systemtest build target:\n$ ant systemtest\n# ... build output ...\n[junit] Testcase: testSmall took 0.017 sec\n[junit] Caused an ERROR\n[junit] expected to find the following tuples:\n[junit] 19128\n[junit]\n[junit] java.lang.AssertionError: expected to find the following tuples:\n[junit] 19128\n[junit]\n[junit] at simpledb.systemtest.SystemTestUtil.matchTuples(SystemTestUtil.\njava:122)\n[junit] at simpledb.systemtest.SystemTestUtil.matchTuples(SystemTestUtil.\njava:83)\n[junit] at simpledb.systemtest.SystemTestUtil.matchTuples(SystemTestUtil.\njava:75)\n[junit] at simpledb.systemtest.ScanTest.validateScan(ScanTest.java:30)\n[junit] at simpledb.systemtest.ScanTest.testSmall(ScanTest.java:40)\n# ... more error messages ...\nThis indicates that this test failed, showing the stack trace where the error was detected. To debug, start by\nreading the source code where the error occurred. When the tests pass, you will see something like the following:\n$ ant systemtest\n# ... build output ...\n[junit] Testsuite: simpledb.systemtest.ScanTest\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 7.278 sec\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 7.278 sec\n[junit]\n[junit] Testcase: testSmall took 0.937 sec\n[junit] Testcase: testLarge took 5.276 sec\n[junit] Testcase: testRandom took 1.049 sec\nBUILD SUCCESSFUL\nTotal time: 52 seconds\n\n6.830 Lab 1: SimpleDB\n1.1.1 Creating dummy tables\nIt is likely you'll want to create your own tests and your own data tables to test your own implementation\nof SimpleDB. You can create any .txt file and convert it to a .dat file in SimpleDB's HeapFile format using\nthe command:\n$ java -jar dist/simpledb.jar convert file.txt N\nwhere file.txt is the name of the file and N is the number of columns in the file. Notice that file.txt has to\nbe in the following format:\nint1,int2,...,intN\nint1,int2,...,intN\nint1,int2,...,intN\nint1,int2,...,intN\n...where each intN is a non-negative integer.\nTo view the contents of a table, use the print command:\n$ java -jar dist/simpledb.jar print file.dat N\nwhere file.dat is the name of a table created with the convert command, and N is the number of columns\nin the file.\n1.2. Working in Eclipse\nEclipse is a graphical software development environment that you might be more comfortable with working in.\nThe instructions we provide were generated by using Eclipse 3.4.0 (Ganymede) for Java Developers (not\nthe enterprise edition) with Java 1.5.0_13 on Ubuntu 7.10. They should also work under Windows or on MacOS.\nSetting the Lab Up in Eclipse\n‚óè Once Eclipse is installed, start it, and note that the first screen asks you to select a location for your workspace\n(we will refer to this directory as $W).\n‚óè On the file system, copy 6.830-lab1.tar.gz to $W/6.830-lab1.tar.gz. Un-GZip and un-tar it, which will create\na directory $W/6.830-lab1 (to do this, you can type tar -pzxvf 6.830-lab1.tar.gz).\n‚óè With Eclipse running, select File->New->Project->Java->Java Project, and push Next.\n‚óè Enter \"6.830-lab1\" as the project name.\n‚óè On the same screen that you entered the project name, select \"Create project from existing source,\" and browse\nto $W/6.830-lab1.\n‚óè Click finish, and you should be able to see \"6.830-lab1\" as a new project in the Project Explorer tab on the left-\nhand side of your screen. Opening this project reveals the directory structure discussed above -\nimplementation code can be found in \"src,\" and unit tests and system tests found in \"test.\"\n\n6.830 Lab 1: SimpleDB\nNote that this class assumes that you are using the official Sun release of Java. This is the default on MacOS\nX, and for most Windows Eclipse installs; but many Linux distributions default to alternate Java compilers.\nThese instructions for Ubuntu Linux may be of help in switching Java versions. If you don't switch, you may\nsee spurious test failures in some of the performance tests in later labs.\nRunning Individual Unit and System Tests\nTo run a unit test or system test (both are JUnit tests, and can be initialized the same way), go to the\nPackage Explorer tab on the left side of your screen. Under the \"6.830-lab1\" project, open the \"test\" directory.\nUnit tests are found in the \"simpledb\" package, and system tests are found in the \"simpledb.systemtests\"\npackage. To run one of these tests, select the test (they are all called *Test.java - don't select TestUtil.java\nor SystemTestUtil.java), right click on it, select \"Run As,\" and select \"JUnit Test.\" This will bring up a JUnit\ntab, which will tell you the status of the individual tests within the JUnit test suite, and will show you exceptions\nand other errors that will help you debug problems.\nRunning Ant Build Targets\nIf you want to run commands such as \"ant test\" or \"ant systemtest,\" right click on build.xml in the Package\nExplorer. Select \"Run As,\" and then \"Ant Build...\" (note: select the option with the ellipsis (...), otherwise you\nwon't be presented with a set of build targets to run). Then, in the \"Targets\" tab of the next screen, check off\nthe targets you want to run (probably \"dist\" and one of \"test\" or \"systemtest\"). This should run the build targets\nand show you the results in Eclipse's console window.\n1.3. Implementation hints\nBefore beginning to write code, we strongly encourage you to read through this entire document to get a\nfeel for the high-level design of SimpleDB.\nYou will need to fill in any piece of code that is not implemented. It will be obvious where we think you should\nwrite code. You may need to add private methods and/or helper classes. You may change APIs, but make sure\nour grading tests still run and make sure to mention, explain, and defend your decisions in your writeup.\nIn addition to the methods that you need to fill out for this lab, the class interfaces contain numerous methods\nthat you need not implement until subsequent labs. These will either be indicated per class:\n// Not necessary for lab1.\npublic class Insert implements DbIterator {\nor per method:\npublic boolean deleteTuple(Tuple t) throws DbException {\n// some code goes here\n// not necessary for lab1\nreturn false;\n}\nThe code that you submit should compile without having to modify these methods.\n\n6.830 Lab 1: SimpleDB\nWe suggest exercises along this document to guide your implementation, but you may find that a different\norder makes more sense for you. Here's a rough outline of one way you might proceed with your\nSimpleDB implementation:\n‚óè Implement the classes to manage tuples, namely Tuple, TupleDesc. We have already implemented\nField, IntField, StringField, and Type for you. Since you only need to support integer and (fixed\nlength) string fields and fixed length tuples, these are straightforward.\n‚óè Implement the Catalog (this should be very simple).\n‚óè Implement the BufferPool constructor and the getPage() method.\n‚óè Implement the access methods, HeapPage and HeapFile and associated ID classes. A good portion of these\nfiles has already been written for you.\n‚óè Implement the operator SeqScan.\n‚óè At this point, you should be able to pass the ScanTest system test, which is the goal for this lab.\nSection 2 below walks you through these implementation steps and the unit tests corresponding to each one\nin more detail.\n1.4. Transactions, locking, and recovery\nAs you look through the interfaces we have provided you, you will see a number of references to\nlocking, transactions, and recovery. You do not need to support these features in this lab, but you should\nkeep these parameters in the interfaces of your code because you will be implementing transactions and locking\nin a future lab. The test code we have provided you with generates a fake transaction ID that is passed into\nthe operators of the query it runs; you should pass this transaction ID into other operators and the buffer pool.\n2. SimpleDB Architecture and Implementation Guide\nSimpleDB consists of:\n‚óè Classes that represent fields, tuples, and tuple schemas;\n‚óè Classes that apply predicates and conditions to tuples;\n‚óè One or more access methods (e.g., heap files) that store relations on disk and provide a way to iterate\nthrough tuples of those relations;\n‚óè A collection of operator classes (e.g., select, join, insert, delete, etc.) that process tuples;\n‚óè A buffer pool that caches active tuples and pages in memory and handles concurrency control and\ntransactions (neither of which you need to worry about for this lab); and,\n‚óè A catalog that stores information about available tables and their schemas.\nSimpleDB does not include many things that you may think of as being a part of a \"database.\" In\nparticular, SimpleDB does not have:\n‚óè (In this lab), a SQL front end or parser that allows you to type queries directly into SimpleDB. Instead, queries\nare built up by chaining a set of operators together into a hand-built query plan (see Section 2.7). We will provide\na simple parser for use in later labs.\n‚óè Views.\n‚óè Data types except integers and fixed length strings.\n‚óè Query optimizer.\n‚óè Indices.\nIn the rest of this Section, we describe each of the main components of SimpleDB that you will need to\nimplement in this lab. You should use the exercises in this discussion to guide your implementation. This\ndocument is by no means a complete specification for SimpleDB; you will need to make decisions about how\nto design and implement various parts of the system. Note that for Lab 1 you do not need to implement\nany operators (e.g., select, join, project) except sequential scan. You will add support for additional operators\nin future labs.\n\n6.830 Lab 1: SimpleDB\nYou may also wish to consult the JavaDoc for SimpleDB.\n2.1. The Database Class\nThe Database class provides access to a collection of static objects that are the global state of the database.\nIn particular, this includes methods to access the catalog (the list of all the tables in the database), the buffer\npool (the collection of database file pages that are currently resident in memory), and the log file. You will not\nneed to worry about the log file in this lab. We have implemented the Database class for you. You should take\na look at this file as you will need to access these objects.\n2.2. Fields and Tuples\nTuples in SimpleDB are quite basic. They consist of a collection of Field objects, one per field in the\nTuple. Field is an interface that different data types (e.g., integer, string) implement. Tuple objects are\ncreated by the underlying access methods (e.g., heap files, or B-trees), as described in the next section.\nTuples also have a type (or schema), called a tuple descriptor, represented by a TupleDesc object. This\nobject consists of a collection of Type objects, one per field in the tuple, each of which describes the type of\nthe corresponding field.\nExercise 1. Implement the skeleton methods in:\n‚óè src/simpledb/TupleDesc.java\n‚óè src/simpledb/Tuple.java\nAt this point, your code should pass the unit tests TupleTest and TupleDescTest. At this point,\nmodifyRecordId() should fail because you havn't implemented it yet.\n2.3. Catalog\nThe catalog (class Catalog in SimpleDB) consists of a list of the tables and schemas of the tables that\nare currently in the database. You will need to support the ability to add a new table, as well as getting\ninformation about a particular table. Associated with each table is a TupleDesc object that allows operators\nto determine the types and number of fields in a table.\nThe global catalog is a single instance of Catalog that is allocated for the entire SimpleDB process. The\nglobal catalog can be retrieved via the method Database.getCatalog(), and the same goes for the\nglobal buffer pool (using Database.getBufferPool()).\nExercise 2. Implement the skeleton methods in:\n‚óè src/simpledb/Catalog.java\nAt this point, your code should pass the unit tests in CatalogTest.\n2.4. BufferPool\nThe buffer pool (class BufferPool in SimpleDB) is responsible for caching pages in memory that have\nbeen recently read from disk. All operators read and write pages from various files on disk through the buffer pool.\nIt consists of a fixed number of pages, defined by the numPages parameter to the BufferPool constructor.\nIn later labs, you will implement an eviction policy. For this lab, you only need to implement the constructor and\nthe BufferPool.getPage() method used by the SeqScan operator. The BufferPool should store up\n\n6.830 Lab 1: SimpleDB\nto numPages pages. For this lab, if more than numPages requests are made for different pages, then instead\nof implementing an eviction policy, you may throw a DbException. In future labs you will be required to\nimplement an eviction policy.\nThe Database class provides a static method, Database.getBufferPool(), that returns a reference to\nthe single BufferPool instance for the entire SimpleDB process.\nExercise 3. Implement the getPage() method in:\n‚óè src/simpledb/BufferPool.java\nWe have not provided unit tests for BufferPool. The functionality you implemented will be tested in the\nimplementation of HeapFile below. You should use the DbFile.readPage method to access pages\nof a DbFile.\n2.5. HeapFile access method\nAccess methods provide a way to read or write data from disk that is arranged in a specific way. Common\naccess methods include heap files (unsorted files of tuples) and B-trees; for this assignment, you will\nonly implement a heap file access method, and we have written some of the code for you.\nA HeapFile object is arranged into a set of pages, each of which consists of a fixed number of bytes for\nstoring tuples, (defined by the constant BufferPool.PAGE_SIZE), including a header. In SimpleDB, there is\none HeapFile object for each table in the database. Each page in a HeapFile is arranged as a set of slots,\neach of which can hold one tuple (tuples for a given table in SimpleDB are all of the same size). In addition to\nthese slots, each page has a header that consists of a bitmap with one bit per tuple slot. If the bit corresponding\nto a particular tuple is 1, it indicates that the tuple is valid; if it is 0, the tuple is invalid (e.g., has been deleted or\nwas never initialized.) Pages of HeapFile objects are of type HeapPage which implements the Page\ninterface. Pages are stored in the buffer pool but are read and written by the HeapFile class.\nSimpleDB stores heap files on disk in more or less the same format they are stored in memory. Each file consists\nof page data arranged consecutively on disk. Each page consists of one or more bytes representing the\nheader, followed by the BufferPool.PAGE_SIZE - # header bytes bytes of actual page content.\nThe number tuples that can fit onto a page is defined by the formula:\ntupsPerPage = floor((BufferPool.PAGE_SIZE * 8) / (tuple size * 8 + 1))\nWhere tuple size is the size of a tuple in the page in bytes. The idea here is that each tuple requires one\nadditional bit of storage in the header. We compute the number of bits in a page (by mulitplying PAGE_SIZE by\n8), and divide this quantity by the number of bits in a tuple (including this extra header bit) to get the number\nof tuples per page. The floor operation rounds down to the nearest integer number of tuples (we don't want to\nstore partial tuples on a page!)\nOnce we know the number of tuples per page, the number of bytes required to store the header is simply:\nheaderBytes = ceiling(tupsPerPage/8)\nThe ceiling operation rounds up to the nearest integer number of bytes (we never store less than a full byte\nof header information.)\nThe low (least significant) bits of each byte represents the status of the slots that are earlier in the file. Hence,\nthe lowest bit of the first byte represents whether or not the first slot in the page is in use. Also, note that the\nhigh-order bits of the last byte may not correspond to a slot that is actually in the file, since the number of slots\nmay not be a multiple of 8. Also note that all Java virtual machines are big-endian.\n\n6.830 Lab 1: SimpleDB\nExercise 4. Implement the skeleton methods in:\n‚óè src/simpledb/HeapPageId.java\n‚óè src/simpledb/RecordID.java\n‚óè src/simpledb/HeapPage.java\nAlthough you will not use them directly in Lab 1, we ask you to implement getNumEmptySlots() and\ngetSlot() in HeapPage. These require pushing around bits in the page header. You may find it helpful\nto look at the other methods that have been provided in HeapPage or in src/simpledb/\nHeapFileEncoder.java to understand the layout of pages.\nYou will also need to implement an Iterator over the tuples in the page, which may involve an auxiliary\nclass or data structure.\nAt this point, your code should pass the unit tests in HeapPageIdTest, RecordIDTest, and\nHeapPageReadTest.\nAfter you have implemented HeapPage, you will write methods for HeapFile in this lab to calculate the number\nof pages in a file and to read a page from the file. You will then be able to fetch tuples from a file stored on disk.\nExercise 5. Implement the skeleton methods in:\n‚óè src/simpledb/HeapFile.java\nTo read a page from disk, you will first need to calculate the correct offset in the file. Hint: you will\nneed random access to the file in order to read and write pages at arbitrary offsets. You should not\ncall BufferPool methods when reading a page from disk.\nYou will also need to implement the HeapFile.iterator() method, which should iterate through\nthrough the tuples of each page in the HeapFile. The iterator must use the BufferPool.getPage()\nmethod to access pages in the HeapFile. This method loads the page into the buffer pool and will\neventually be used (in a later lab) to implement locking-based concurrency control and recovery. Do\nnot load the entire table into memory on the open() call -- this will cause an out of memory error for\nvery large tables.\nAt this point, your code should pass the unit tests in HeapFileReadTest.\n2.6. Operators\nOperators are responsible for the actual execution of the query plan. They implement the operations of\nthe relational algebra. In SimpleDB, operators are iterator based; each operator implements the\nDbIterator interface.\nOperators are connected together into a plan by passing lower-level operators into the constructors of higher-\nlevel operators, i.e., by 'chaining them together.' Special access method operators at the leaves of the plan\nare responsible for reading data from the disk (and hence do not have any operators below them).\nAt the top of the plan, the program interacting with SimpleDB simply calls getNext on the root operator;\nthis operator then calls getNext on its children, and so on, until these leaf operators are called. They fetch\ntuples from disk and pass them up the tree (as return arguments to getNext); tuples propagate up the plan in\nthis way until they are output at the root or combined or rejected by another operator in the plan.\n\n6.830 Lab 1: SimpleDB\nFor this lab, you will only need to implement one SimpleDB operator.\nExercise 6. Implement the skeleton methods in:\n‚óè src/simpledb/SeqScan.java\nThis operator sequentially scans all of the tuples from the pages of the table specified by the\ntableid in the constructor. This operator should access tuples through the DbFile.iterator()\nmethod.\nAt this point, you should be able to complete the ScanTest system test. Good work!\nYou will fill in other operators in subsequent labs.\n2.7. A simple query\nThe purpose of this section is to illustrate how these various components are connected together to process\na simple query. The following code implements a simple selection query over a data file consisting of three\ncolumns of integers. (The file some_data_file.dat is a binary representation of the pages from this\nfile, generated with a command like java -jar dist/simpledb.jar convert some_data_file.txt\n3). This code is equivalent to the SQL statement SELECT * FROM some_data_file.\npackage simpledb;\nimport java.io.*;\npublic class test {\npublic static void main(String[] argv) {\n// construct a 3-column table schema\nType types[] = new Type[]{ Type.INT_TYPE, Type.INT_TYPE, Type.INT_TYPE };\nString names[] = new String[]{ \"field0\", \"field1\", \"field2\" };\nTupleDesc descriptor = new TupleDesc(types, names);\n\n// create the table, associate it with some_data_file.dat\n// and tell the catalog about the schema of this table.\nHeapFile table1 = new HeapFile(new File(\"some_data_file.dat\"), descriptor);\nDatabase.getCatalog().addTable(table1, \"test\");\n\n// construct the query: we use a simple SeqScan, which spoonfeeds\n// tuples via its iterator.\nTransactionId tid = new TransactionId();\nSeqScan f = new SeqScan(tid, table1.getId(), \"test\");\n\ntry {\n// and run it\nf.open();\nwhile (f.hasNext()) {\nTuple tup = f.next();\nSystem.out.println(tup);\n}\nf.close();\nDatabase.getBufferPool().transactionComplete(tid);\n\n6.830 Lab 1: SimpleDB\n} catch (Exception e) {\nSystem.out.println (\"Exception : \" + e);\n}\n}\n}\nThe table we create has three integer fields. To express this, we create a TupleDesc object and pass it an array\nof Type objects, and optionally an array of String field names. Once we have created this TupleDesc,\nwe initialize a HeapFile object representing the table stored in some_data_file.dat. Once we have\ncreated the table, we add it to the catalog. If this were a database server that was already running, we would\nhave this catalog information loaded. We need to load it explicitly to make this code self-contained.\nOnce we have finished initializing the database system, we create a query plan. Our plan consists only of\nthe SeqScan operator that scans the tuples from disk. In general, these operators are instantiated with\nreferences to the appropriate table (in the case of SeqScan) or child operator (in the case of e.g. Filter). The\ntest program then repeatedly calls hasNext and next on the SeqScan operator. As tuples are output from\nthe SeqScan, they are printed out on the command line.\n3. Logistics\nYou must submit your code (see below) as well as a short (2 pages, maximum) writeup describing your\napproach. This writeup should:\n‚óè Describe any design decisions you made. These may be minimal for Lab 1.\n‚óè Discuss and justify any changes you made to the API.\n‚óè Describe any missing or incomplete elements of your code.\n‚óè Describe how long you spent on the lab, and whether there was anything you found particularly difficult\nor confusing.\n3.1. Collaboration\nThis lab should be manageable for a single person, but if you prefer to work with a partner, this is also OK.\nLarger groups are not allowed. Please indicate clearly who you worked with, if anyone, on your individual writeup.\n3.2. Submitting your assignment\nTo submit your code, please create a 6.830-lab1.tar.gz tarball (such that, untarred, it creates a 6.830-\nlab1/src/simpledb directory with your code) . You may submit your code multiple times; we will use\nthe latest version you submit that arrives before the deadline (before 11:59 PM on the due date).\nIf applicable, please indicate your partner in your email. Please also attach your individual writeup as a\nPDF or text file.\n3.3. Submitting a bug\nPlease submit (friendly!) bug reports . When you do, please try to include:\n‚óè A description of the bug.\n‚óè A .java file we can drop in the test/simpledb directory, compile, and run.\n‚óè A .txt file with the data that reproduces the bug. We should be able to convert it to a .dat file\nusing HeapFileEncoder.\n\n6.830 Lab 1: SimpleDB\nIf you are the first person to report a particular bug in the code, we will give you a candy bar!\n3.4 Grading\n75% of your grade will be based on whether or not your code passes the system test suite we will run over it.\nThese tests will be a superset of the tests we have provided. Before handing in your code, you should make\nsure produces no errors (passes all of the tests) from both ant test and ant systemtest.\nImportant: before testing, we will replace your build.xml and the entire contents of the test directory with\nour version of these files. This means you cannot change the format of .dat files! You should also be\ncareful changing our APIs. You should test that your code compiles the unmodified tests. In other words, we\nwill untar your tarball, replace the files mentioned above, compile it, and then grade it. It will look roughly like this:\n$ tar xvzf 6.830-lab1.tar.gz\n$ cd ./6.830-lab1\n[replace build.xml and test]\n$ ant test\n$ ant systemtest\n[additional tests]\nIf any of these commands fail, we'll be unhappy, and, therefore, so will your grade.\nAn additional 25% of your grade will be based on the quality of your writeup and our subjective evaluation of\nyour code.\nWe've had a lot of fun designing this assignment, and we hope you enjoy hacking on it!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_830F10_lab2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/3783128417fee9d2e8149c434a102c0c_MIT6_830F10_lab2.pdf",
      "content": "6.830 Lab 2: SimpleDB Operators\n6.830 Lab 2: SimpleDB Operators\nAssigned: September 30, 2010\nDue: Friday, October 15, 2010\nVersion History:\n‚óè 9/30/10 : Initial version\n‚óè 10/5/10 : Updated due date\nIn this lab assignment, you will write a set of operators for SimpleDB to implement table modifications (e.g.,\ninsert and delete records), selections, joins, and aggregates. These will build on top of the foundation that\nyou wrote in Lab 1 to provide you with a database system that can perform simple queries over multiple tables.\nAdditionally, we ignored the issue of buffer pool management in Lab 1; we have not dealt with the problem\nthat arises when we reference more pages than we can fit in memory over the lifetime of the database. In Lab\n2, you will design an eviction policy to flush stale pages from the buffer pool.\nYou do not need to implement transactions or locking in this lab.\nThe remainder of this document gives some suggestions about how to start coding, describes a set of exercises\nto help you work through the lab, and discusses how to hand in your code. This lab requires you to write a\nfair amount of code, so we encourage you to start early!\n0. Find bugs, be patient, earn candy bars\nSimpleDB is a relatively complex piece of code. It is very possible you are going to find bugs, inconsistencies,\nand bad, outdated, or incorrect documentation, etc.\nWe ask you, therefore, to do this lab with an adventurous mindset. Don't get mad if something is not clear, or\neven wrong; rather, try to figure it out yourself or send us a friendly email. We promise to help out by posting\nbug fixes, new tarballs, etc., as bugs and issues are reported.\n...and if you find a bug in our code, we'll give you a candy bar (see Section 3.3)!\n1. Getting started\nYou should begin with the code you submitted for Lab 1 (if you did not submit code for Lab 1, or your solution\ndidn't work properly, contact us to discuss options.) We have provided you with extra test cases for this lab that\nare not in the original code distribution you received. We reiterate that the unit tests we provide are to help\nguide your implementation along, but they are not intended to be comprehensive or to establish correctness.\nYou will need to add these new test cases to your release. The easiest way to do this is to untar the new code\nin the same directory as your top-level simpledb directory, as follows:\n‚óè Make a backup of your Lab 1 solution by typing :\n$ tar -cvzf 6.830-lab1-submitted.tar.gz 6.830-lab1\n‚óè Change to the directory that contains your top-level simpledb code:\n$ cd 6.830-lab2\n‚óè Download the new tests and skeleton code for Lab 2 from http://db.csail.mit.edu/6.830/6.830-lab2-supplement.\n\n6.830 Lab 2: SimpleDB Operators\ntar.gz:\n$ wget http://db.csail.mit.edu/6.830/6.830-lab2-supplement.tar.gz\n‚óè Extract the new files for Lab 2 by typing:\ntar -xvzf 6.830-lab2-supplement.tar.gz\n‚óè Eclipse users will have to take one more step for their code to compile. (First, make sure the project is in\nyour workspace in Eclipse following similar steps as done in lab1.) Under the package explorer, right click\nthe project name (probably 6.830-lab1), and select Properties. Choose Java Build Path on the left-\nhand-side, and click on the Libraries tab on the right-hand-side. Push the Add JARs... button, select zql.\njar and jline-0.9.94.jar, and push OK, followed by OK. Your code should now compile.\n1.3. Implementation hints\nAs before, we strongly encourage you to read through this entire document to get a feel for the high-\nlevel design of SimpleDB before you write code.\nWe suggest exercises along this document to guide your implementation, but you may find that a different\norder makes more sense for you. As before, we will grade your assignment by looking at your code and\nverifying that you have passed the test for the ant targets test and systemtest. See Section 3.4 for a\ncomplete discussion of grading and list of the tests you will need to pass.\nHere's a rough outline of one way you might proceed with your SimpleDB implementation; more details on the\nsteps in this outline, including exercises, are given in Section 2 below.\n‚óè Implement the operators Filter and Join and verify that their corresponding tests work. The Javadoc\ncomments for these operators contain details about how they should work. We have given you implementations\nof Project and OrderBy which may help you understand how other operators work.\n‚óè Implement IntAggregator and StringAggregator. Here, you will write the logic that actually computes\nan aggregate over a particular field across multiple groups in a sequence of input tuples. Use integer division\nfor computing the average, since SimpleDB only supports integers. StringAggegator only needs to support\nthe COUNT aggregate, since the other operations do not make sense for strings.\n‚óè Implement the Aggregate operator. As with other operators, aggregates implement the DbIterator interface\nso that they can be placed in SimpleDB query plans. Note that the output of an Aggregate operator is\nan aggregate value of an entire group for each call to next(), and that the aggregate constructor takes\nthe aggregation and grouping fields.\n‚óè Implement the methods related to tuple insertion, deletion, and page eviction in BufferPool. You do not need\nto worry about transactions at this point.\n‚óè Implement the Insert and Delete operators. Like all operators, Insert and Delete implement\nDbIterator, accepting a stream of tuples to insert or delete and outputting a single tuple with an integer field\nthat indicates the number of tuples inserted or deleted. These operators will need to call the appropriate methods\nin BufferPool that actually modify the pages on disk. Check that the tests for inserting and deleting tuples\nwork properly.\nNote that SimpleDB does not implement any kind of consistency or integrity checking, so it is possible to\ninsert duplicate records into a file and there is no way to enforce primary or foreign key constraints.\nAt this point you should be able to pass all of the tests in the ant systemtest target, which is the goal of this lab.\nYou'll also be able to use the provided SQL parser to run SQL queries against your database! See Section 2.7 for\na brief tutorial and a description of an optional contest to see who can write the fastest SimpleDB implementation.\nFinally, you might notice that the iterators in this lab extend the AbstractDbIterator class instead of implementing\nthe DbIterator interface. Because the implementation of next/hasNext is often repetitive, annoying, and\nerror-prone, AbstractDbIterator implements this logic generically, and only requires that you implement a\n\n6.830 Lab 2: SimpleDB Operators\nsimpler readNext. Feel free to use this style of implementation, or just implement the DbIterator interface if\nyou prefer. To implement the DbIterator interface, remove extends AbstractDbIterator from iterator\nclasses, and in its place put implements DbIterator.\n2. SimpleDB Architecture and Implementation Guide\n2.1. Filter and Join\nRecall that SimpleDB DbIterator classes implement the operations of the relational algebra. You will now\nimplement two operators that will enable you to perform queries that are slightly more interesting than a table scan.\n‚óè Filter: This operator only returns tuples that satisfy a Predicate that is specified as part of its constructor.\nHence, it filters out any tuples that do not match the predicate.\n‚óè Join: This operator joins tuples from its two children according to a JoinPredicate that is passed in as part of\nits constructor. We only require a simple nested loops join, but you may explore more interesting\njoin implementations. Describe your implementation in your lab writeup.\nExercise 1. Implement the skeleton methods in:\n‚óè src/simpledb/Predicate.java\n‚óè src/simpledb/JoinPredicate.java\n‚óè src/simpledb/Filter.java\n‚óè src/simpledb/Join.java\nAt this point, your code should pass the unit tests in PredicateTest, JoinPredicateTest, FilterTest, and\nJoinTest. Furthermore, you should be able to pass the system tests FilterTest and JoinTest.\n2.2. Aggregates\nAn additional SimpleDB operator implements basic SQL aggregates with a GROUP BY clause. You\nshould implement the five SQL aggregates (COUNT, SUM, AVG, MIN, MAX) and support grouping. You only need\nto support aggregates over a single field, and grouping by a single field.\nIn order to calculate aggregates, we use an Aggregator interface which merges a new tuple into the\nexisting calculation of an aggregate. The Aggregator is told during construction what operation it should use\nfor aggregation. Subsequently, the client code should call Aggregator.merge() for every tuple in the\nchild iterator. After all tuples have been merged, the client can retrieve a DbIterator of aggregation results.\nEach tuple in the result is a pair of the form (groupValue, aggregateValue), unless the value of the group\nby field was Aggregator.NO_GROUPING, in which case the result is a single tuple of the\nform (aggregateValue).\nNote that this implementation requires space linear in the number of distinct groups. For the purposes of this\nlab, you do not need to worry about the situation where the number of groups exceeds available memory.\nExercise 2. Implement the skeleton methods in:\n‚óè src/simpledb/IntAggregator.java\n‚óè src/simpledb/StringAggregator.java\n‚óè src/simpledb/Aggregate.java\nAt this point, your code should pass the unit tests IntAggregatorTest, StringAggregatorTest, and\nAggregateTest. Furthermore, you should be able to pass the AggregateTest system test.\n2.3. HeapFile Mutability\nNow, we will begin to implement methods to support modifying tables. We begin at the level of individual pages\nand files. There are two main sets of operations: adding tuples and removing tuples.\nRemoving tuples: To remove a tuple, you will need to implement deleteTuple. Tuples contain\nRecordIDs which allow you to find the page they reside on, so this should be as simple as locating the page\na tuple belongs to and modifying the headers of the page appropriately.\n\n6.830 Lab 2: SimpleDB Operators\nAdding tuples: The addTuple method in HeapFile.java is responsible for adding a tuple to a heap file.\nTo add a new tuple to a HeapFile, you will have to find a page with an empty slot. If no such pages exist in\nthe HeapFile, you need to create a new page and append it to the physical file on disk. You will need to ensure\nthat the RecordID in the tuple is updated correctly.\nExercise 3. Implement the remaining skeleton methods in:\n‚óè src/simpledb/HeapPage.java\n‚óè src/simpledb/HeapFile.java (note that you do not necessarily need to implement writePage at this point.)\nTo implement HeapPage, you will need to modify the header bitmap for methods such as addTuple()\nand deleteTuple(). You may find that the getNumEmptySlots() and getSlot() methods we asked you\nto implement in Lab 1 serve as useful abstractions. Note that there is a setSlot method provided as\nan abstraction to modify the filled or cleared status of a tuple in the page header.\nNote that it is important that the HeapFile.addTuple() and HeapFile.deleteTuple() methods\naccess pages using the BufferPool.getPage() method; otherwise, your implementation of transactions in\nthe next lab will not work properly.\nImplement the following skeleton methods in src/simpledb/BufferPool.java:\n‚óè insertTuple()\n‚óè deleteTuple()\nThese methods should call the appropriate methods in the HeapFile that belong to the table being modified\n(this extra level of indirection is needed to support other types of files -- like indices -- in the future).\nAt this point, your code should pass the unit tests in HeapPageWriteTest and HeapFileWriteTest. We have\nnot provided additional unit tests for HeapFile.deleteTuple() or BufferPool.\n2.4. Insertion and deletion\nNow that you have written all of the HeapFile machinery to add and remove tuples, you will implement the\nInsert and Delete operators.\nFor plans that implement insert and delete queries, the top-most operator is a special Insert or\nDelete operator that modifies the pages on disk. These operators return the number of affected tuples. This\nis implemented by returning a single tuple with one integer field, containing the count.\n‚óè Insert: This operator adds the tuples it reads from its child operator to the tableid specified in its constructor.\nIt should use the BufferPool.insertTuple() method to do this.\n‚óè Delete: This operator deletes the tuples it reads from its child operator from the tableid specified in\nits constructor. It should use the BufferPool.deleteTuple() method to do this.\nExercise 4. Implement the skeleton methods in:\n‚óè src/simpledb/Insert.java\n‚óè src/simpledb/Delete.java\nAt this point, your code should pass the unit tests in InsertTest. We have not provided unit tests for\nDelete. Furthermore, you should be able to pass the InsertTest and DeleteTest system tests.\n2.5. Page eviction\nIn Lab 1, we did not correctly observe the limit on the maximum number of pages in the buffer pool defined by\nthe constructor argument numPages. Now, you will choose a page eviction policy and instrument any\nprevious code that reads or creates pages to implement your policy.\nWhen more than numPages pages are in the buffer pool, one page should be evicted from the pool before the\nnext is loaded. The choice of eviction policy is up to you; it is not necessary to do something\nsophisticated. Describe your policy in the lab writeup.\n\n6.830 Lab 2: SimpleDB Operators\nNotice that BufferPool asks you to implement a flushAllPages() method. This is not something you\nwould ever need in a real implementation of a buffer pool. However, we need this method for testing purposes.\nYou should never call this method from any real code. Because of the way we have implemented\nScanTest.cacheTest, you will need to ensure that your flushPage and flushAllPages methods do no evict\npages from the buffer pool to properly pass this test. flushAllPages should call flushPage on all pages in\nthe BufferPool, and flushPage should write any dirty page to disk and mark it as not dirty, while leaving it in\nthe BufferPool. The only method which should remove page from the buffer pool is evictPage, which should\ncall flushPage on any dirty page it evicts.\nExercise 5. Fill in the flushPage() method and additional helper methods to implement page eviction in:\n‚óè src/simpledb/BufferPool.java\nIf you did not implement writePage() in HeapFile.java above, you will also need to do that here.\nAt this point, your code should pass the EvictionTest system test.\nSince we will not be checking for any particular eviction policy, this test works by creating a BufferPool with\n16 pages (NOTE: while DEFAULT_PAGES is 50, we are initializing the BufferPool with less!), scanning a file\nwith many more than 16 pages, and seeing if the memory usage of the JVM increases by more than 5 MB. If\nyou do not implement an eviction policy correctly, you will not evict enough pages, and will go over the\nsize limitation, thus failing the test.\nYou have now completed this lab. Good work!\n2.6. Query walkthrough\nThe following code implements a simple join query between two tables, each consisting of three columns\nof integers. (The file some_data_file1.dat and some_data_file2.dat are binary representation of\nthe pages from this file). This code is equivalent to the SQL statement:\nSELECT *\nFROM some_data_file1, some_data_file2\nWHERE some_data_file1.field1 = some_data_file2.field1\nAND some_data_file1.id > 1\nFor more extensive examples of query operations, you may find it helpful to browse the unit tests for joins,\nfilters, and aggregates.\npackage simpledb;\nimport java.io.*;\npublic class jointest {\npublic static void main(String[] argv) {\n// construct a 3-column table schema\nType types[] = new Type[]{ Type.INT_TYPE, Type.INT_TYPE, Type.INT_TYPE };\nString names[] = new String[]{ \"field0\", \"field1\", \"field2\" };\nTupleDesc td = new TupleDesc(types, names);\n// create the tables, associate them with the data files\n// and tell the catalog about the schema the tables.\nHeapFile table1 = new HeapFile(new File(\"some_data_file1.dat\"), td);\nDatabase.getCatalog().addTable(table1, \"t1\");\nHeapFile table2 = new HeapFile(new File(\"some_data_file2.dat\"), td);\nDatabase.getCatalog().addTable(table2, \"t2\");\n// construct the query: we use two SeqScans, which spoonfeed\n\n6.830 Lab 2: SimpleDB Operators\n// tuples via iterators into join\nTransactionId tid = new TransactionId();\nSeqScan ss1 = new SeqScan(tid, table1.getId(), \"t1\");\nSeqScan ss2 = new SeqScan(tid, table2.getId(), \"t2\");\n// create a filter for the where condition\nFilter sf1 = new Filter(\nnew Predicate(0,\nPredicate.Op.GREATER_THAN, new IntField(1)), ss1);\nJoinPredicate p = new JoinPredicate(1, Predicate.Op.EQUALS, 1);\nJoin j = new Join(p, sf1, ss2);\n// and run it\ntry {\nj.open();\nwhile (j.hasNext()) {\nTuple tup = j.next();\nSystem.out.println(tup);\n}\nj.close();\nDatabase.getBufferPool().transactionComplete(tid);\n} catch (Exception e) {\ne.printStackTrace();\n}\n}\n}\nBoth tables have three integer fields. To express this, we create a TupleDesc object and pass it an array of\nType objects indicating field types and String objects indicating field names. Once we have created\nthis TupleDesc, we initialize two HeapFile objects representing the tables. Once we have created the tables,\nwe add them to the Catalog. (If this were a database server that was already running, we would have this\ncatalog information loaded; we need to load this only for the purposes of this test).\nOnce we have finished initializing the database system, we create a query plan. Our plan consists of two\nSeqScan operators that scan the tuples from each file on disk, connected to a Filter operator on the\nfirst HeapFile, connected to a Join operator that joins the tuples in the tables according to the JoinPredicate.\nIn general, these operators are instantiated with references to the appropriate table (in the case of SeqScan)\nor child operator (in the case of e.g., Join). The test program then repeatedly calls next on the Join\noperator, which in turn pulls tuples from its children. As tuples are output from the Join, they are printed out on\nthe command line.\n2.7. Query Parser and Contest\nWe've provided you with a query parser for SimpleDB that you can use to write and run SQL queries against\nyour database once you have completed the exercises in this lab.\nThe first step is to create some data tables and a catalog. Suppose you have a file data.txt with the\nfollowing contents:\n1,10\n2,20\n3,30\n4,40\n5,50\n5,50\n\n6.830 Lab 2: SimpleDB Operators\nYou can convert this into a SimpleDB table using the convert command (make sure to type ant first!):\njava -jar dist/simpledb.jar convert data.txt 2 \"int,int\"\nThis creates a file data.dat. In addition to the table's raw data, the two additional parameters specify that\neach record has two fields and that their types are int and int.\nNext, create a catalog file, catalog.txt, with the follow contents:\ndata (f1 int, f2 int)\nThis tells SimpleDB that there is one table, data (stored in data.dat) with two integer fields named f1 and f2.\nFinally, invoke the parser. You must run java from the command line (ant doesn't work properly with\ninteractive targets.) From the simpledb/ directory, type:\njava -jar dist/simpledb.jar parser catalog.txt\nYou should see output like:\nAdded table : data with schema INT(f1), INT(f2),\nSimpleDB>\nFinally, you can run a query:\nSimpleDB> select d.f1, d.f2 from data d;\nStarted a new transaction tid = 1221852405823\nADDING TABLE d(data) TO tableMap\nTABLE HAS tupleDesc INT(d.f1), INT(d.f2),\n1 10\n2 20\n3 30\n4 40\n5 50\n5 50\n6 rows.\n----------------\n0.16 seconds\nSimpleDB>\nThe parser is relatively full featured (including support for SELECTs, INSERTs, DELETEs, and transactions),\nbut does have some problems and does not necessarily report completely informative error messages. Here\nare some limitations to bear in mind:\n‚óè You must preface every field name with its table name, even if the field name is unique (you can use table\n\n6.830 Lab 2: SimpleDB Operators\nname aliases, as in the example above, but you cannot use the AS keyword.)\n‚óè Nested queries are supported in the WHERE clause, but not the FROM clause.\n‚óè No arithmetic expressions are supported (for example, you can't take the sum of two fields.)\n‚óè At most one GROUP BY and one aggregate column are allowed.\n‚óè Set-oriented operators like IN, UNION, and EXCEPT are not allowed.\n‚óè Only AND expressions in the WHERE clause are allowed.\n‚óè UPDATE expressions are not supported.\n‚óè The string operator LIKE is allowed, but must be written out fully (that is, the Postgres tilde [~] shorthand is\nnot allowed.)\nContest (Optional)\nThe first-place winner in the contest will receive a $50 gift certificate to Amazon. The second-place winner\nwill receive a $25 gift certificate to Amazon.\nWe have built SimpleDB-encoded version of the NSF database you used in problem set 1; the needed files\nare located at: http://db.csail.mit.edu/nsf_data.tar.gz\nYou should download the file and unpack it. It will create four files in the nsf_data directory. Move them into\nthe simpledb directory. The following commands will acomplish this, if you run them from the simpledb directory:\nwget http://db.csail.mit.edu/nsf_data.tar.gz\ntar xvzf nsf_data.tar.gz\nmv nsf_data/* .\nrm -r nsf_data.tar.gz nsf_data\nYou can then run the parser with:\njava -jar dist/simpledb.jar parser nsf.schema\nWe will give a prize to the submission that has shortest total runtime for the following three queries (where\ntotal runtime is the sum of the runtime of each of the individual queries):\n1. SELECT g.title\nFROM grants g\nWHERE g.title LIKE 'Monkey';\n2. SELECT g.title\nFROM grants g, researchers r, grant_researchers gr\nWHERE r.name = 'Samuel Madden'\nAND gr.researcherid = r.id\nAND gr.grantid = g.id;\n\n6.830 Lab 2: SimpleDB Operators\n3. SELECT r2.name, count(g.id)\nFROM grants g, researchers r, researchers r2, grant_researchers gr,\ngrant_researchers gr2\nWHERE r.name = 'Samuel Madden'\nAND gr.researcherid = r.id\nAND gr.grantid = g.id\nAND gr2.researcherid = r2.id\nAND gr.grantid = gr2.grantid\nGROUP BY r2.name\nORDER BY r2.name;\n\nNote that each query will print out its runtime after it executes.\nYou may wish to create optimized implementations of some of the operators; in particular, a fast join operator (e.\ng., not nested loops) will be essential for good performance on queries 2 and 3.\nThere is currently no optimizer in the parser, so the queries above have been written to cause the parser\ngenerate reasonable plans. Here are some helpful hints about how the parser works that you may wish to\nexploit while running these queries:\n‚óè The table on the left side of the joins in these queries is passed in as the first DbIterator parameter to Join.\n‚óè Expressions in the WHERE clause are added to the plan from top to bottom, such that first expression will be the\nbottom-most operator in the generated query plan. For example, the generated plan for Query 2 is:\nProject(Join(Join(Filter(a),pa),p))\nOur reference implementation can run Query 1 in about .1 seconds, Query 2 in about .4 seconds, and Query 3\nin about .7 seconds. We implemented a special-purpose join operator for equality joins but did little else to\noptimize performance.\n3. Logistics\nYou must submit your code (see below) as well as a short (2 pages, maximum) writeup describing your\napproach. This writeup should:\n‚óè Describe any design decisions you made, including your choice of page eviction policy. If you used something\nother than a nested-loops join, describe the tradeoffs of the algorithm you chose.\n‚óè Discuss and justify any changes you made to the API.\n‚óè Describe any missing or incomplete elements of your code.\n‚óè Describe how long you spent on the lab, and whether there was anything you found particularly difficult\nor confusing.\n3.1. Collaboration\nThis lab should be manageable for a single person, but if you prefer to work with a partner, this is also OK.\nLarger groups are not allowed. Please indicate clearly who you worked with, if anyone, on your individual writeup.\n3.2. Submitting your assignment\nTo submit your code, please create a 6.830-lab2.tar.gz tarball (such that, untarred, it creates a 6.830-\nlab2/src/simpledb directory with your code) and submit it . You may submit your code multiple times; we will use\nthe latest version you submit that arrives before the deadline (before 11:59pm on the due date). If applicable, please\nindicate your partner in your writeup. Please also submit your individual writeup as a PDF or plain text file (.txt).\nPlease do not submit a .doc or .docx. Make sure your code is packaged so the instructions outlined in section 3.4 work.\n\n6.830 Lab 2: SimpleDB Operators\n3.3. Submitting a bug\nPlease submit (friendly!) bug reports. When you do, please try to include:\n‚óè A description of the bug.\n‚óè A .java file we can drop in the test/simpledb directory, compile, and run.\n‚óè A .txt file with the data that reproduces the bug. We should be able to convert it to a .dat file\nusing HeapFileEncoder.\nIf you are the first person to report a particular bug in the code, we will give you a candy bar!\n3.4 Grading\n50% of your grade will be based on whether or not your code passes the system test suite we will run over it.\nThese tests will be a superset of the tests we have provided. Before handing in your code, you should make\nsure produces no errors (passes all of the tests) from both ant test and ant systemtest.\nImportant: before testing, we will replace your build.xml, HeapFileEncoder.java, and the entire\ncontents of the test/ directory with our version of these files! This means you cannot change the format of .\ndat files! You should therefore be careful changing our APIs. This also means you need to test whether your\ncode compiles with our test programs. In other words, we will untar your tarball, replace the files mentioned\nabove, compile it, and then grade it. It will look roughly like this:\n$ gunzip 6.830-lab2.tar.gz\n$ tar xvf 6.830-lab2.tar\n$ cd ./6.830-lab2\n[replace build.xml, HeapFileEncoder.java, and test]\n$ ant test\n$ ant systemtest\n[additional tests]\nIf any of these commands fail, we'll be unhappy, and, therefore, so will your grade.\nAn additional 50% of your grade will be based on the quality of your writeup and our subjective evaluation of\nyour code.\nWe've had a lot of fun designing this assignment, and we hope you enjoy hacking on it!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_830F10_lab3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/a0d8109c7a456dce7787f445cf1500ca_MIT6_830F10_lab3.pdf",
      "content": "6.830 Lab 3: SimpleDB Transactions\n6.830 Lab 3: SimpleDB Transactions\nAssigned: Wed, October 20\nDue: Tue, Nov 2\nIn this lab, you will implement a simple locking-based transaction system in SimpleDB. You will need to add lock\nand unlock calls at the appropriate places in your code, as well as code to track the locks held by each\ntransaction and grant locks to transactions as they are needed.\nThe remainder of this document describes what is involved in adding transaction support and provides a basic\noutline of how you might add this support to your database.\nAs with the previous lab, we recommend that you start as early as possible. Locking and transactions can be\nquite tricky to debug!\n0. Find bugs, be patient, earn candybars\nIt is very possible you are going to find bugs, inconsistencies, and bad, outdated, or incorrect documentation, etc.\nWe apologize profusely. We did our best, but, alas, we are fallible human beings.\nWe ask you, therefore, to do this lab with an adventurous mindset. Don't get mad if something is not clear, or\neven wrong; rather, try to figure it out yourself or send us a friendly email. We promise to help out by sending\nbugfixes, new tarballs, etc.\n...and if you find a bug in our code, we'll give you a candybar (see Section 3.3)!\n1. Getting started\nYou should begin with the code you submitted for Lab 2 (if you did not submit code for Lab 2, or your solution\ndidn't work properly, contact us to discuss options.) We have provided you with extra test cases for this lab that\nare not in the original code distribution you received. We reiterate that the unit tests we provide are to help guide\nyour implementation along, but they are not intended to be comprehensive or to establish correctness.\nYou will need to add these new test cases to your release. The easiest way to do this is to untar the new code in\nthe same directory as your top-level simpledb directory, as follows:\n‚óè Make a backup of your Lab 2 solution by typing :\n$ tar -cvzf 6.830-lab2-submitted.tar.gz 6.830-lab2\n‚óè Change to the directory that contains your top-level simpledb code:\n\n6.830 Lab 3: SimpleDB Transactions\n$ cd 6.830-lab2\n‚óè Download the new tests and skeleton code for Lab 3 from http://db.csail.mit.edu/6.830/6.830-lab3-\nsupplement.tar.gz:\n$ wget http://db.csail.mit.edu/6.830/6.830-lab3-supplement.tar.gz\n‚óè Extract the new files for Lab 3 by typing:\ntar -xvzf 6.830-lab3-supplement.tar.gz\nThis will not overwrite any existing files, but will just add new tests to the test/simpledb and test/\nsimpledb/systemtest directories, as well as adding the new file src/simpledb/Transaction.\njava.\n2. Transactions, Locking, and Concurrency Control\nYou do not need to write a great deal of code for this lab, but the code you do have to write is quite tricky. Before\nstarting, you should make sure you understand what a transaction is and how strict two-phase locking (which you\nwill use to ensure isolation and atomicity of your transactions) works.\nIn the remainder of this section, we briefly overview these concepts and discuss how they relate to SimpleDB.\n2.1. Transactions\nA transaction is a group of database actions (e.g., inserts, deletes, and reads) that are executed atomically; that\nis, either all of the actions complete or none of them do, and it is not apparent to an outside observer of the\ndatabase that these actions were not completed as a part of a single, indivisible action.\n2.2. The ACID Properties\nTo help you understand how transaction management works in SimpleDB, we briefly review how it ensures that\nthe ACID properties are satisfied:\n‚óè Atomicity: Strict two-phase locking and careful buffer management ensure atomicity.\n‚óè Consistency: The database is transaction consistent by virtue of atomicity. Other consistency issues (e.\n\n6.830 Lab 3: SimpleDB Transactions\ng., key constraints) are not addressed in SimpleDB.\n‚óè Isolation: Strict two-phase locking provides isolation.\n‚óè Durability: A FORCE buffer management policy ensures durability (see Section 2.3 below).\n2.3. Recovery and Buffer Management\nTo simplify your job, we recommend that you implement a NO STEAL/FORCE buffer management policy. As we\ndiscussed in class, this means that:\n‚óè You shouldn't evict dirty (updated) pages from the buffer pool if they are locked by an uncommitted\ntransaction (this is NO STEAL).\n‚óè On transaction commit, you should force dirty pages to disk (e.g., write the pages out) (this is FORCE).\nTo further simplify your life, you may assume that SimpleDB will not crash while processing a\ntransactionComplete command. Note that these three points mean that you do not need to implement log-\nbased recovery in this lab, since you will never need to undo any work (since you never evict dirty pages) and\nyou will never need to redo any work (since you force updates on commit and will not crash during commit\nprocessing).\n2.4. Granting Locks\nYou will need to add calls to SimpleDB (in BufferPool, for example), that allow a caller to request or release a\n(shared or exclusive) lock on a specific object on behalf of a specific transaction.\nWe recommend locking at page granularity, though you should be able to implement locking at tuple granularity if\nyou wish (please do not implement table-level locking). The rest of this document and our unit tests assume page-\nlevel locking.\nYou will need to create data structures that keep track of which locks each transaction holds and that check to\nsee if a lock should be granted to a transaction when it is requested.\nYou will need to implement shared and exclusive locks; recall that these work as follows:\n‚óè Before a transaction can read an object, it must have a shared lock on it.\n‚óè Before a transaction can write an object, it must have an exclusive lock on it.\n‚óè Multiple transactions can have a shared lock on an object.\n‚óè Only one transaction may have an exclusive lock on an object.\n‚óè No transaction may have a shared lock on an object if another transaction has an exclusive lock on it.\n‚óè If transaction t is the only transaction holding a lock on an object o, t may upgrade its lock on o to a\nexclusive lock.\nIf a transaction requests a lock that it should not be granted, your code should block, waiting for that lock to\nbecome available (i.e., be released by another transaction running in a different thread).\nYou need to be especially careful to avoid race conditions when writing the code that acquires locks -- think about\nhow you will ensure that correct behavior results if two threads request the same lock at the same time (you way\nwish to read about Synchronizing Threads in Java).\n\n6.830 Lab 3: SimpleDB Transactions\nExercise 1. Write the methods that acquire and release locks in BufferPool. Assuming\nyou are using page-level locking, you will need to complete the following:\n‚óè Modify getPage() to block and acquire the desired lock before returning a page.\n‚óè Implement releasePage(). This method is primarily used for testing, and at the\nend of transactions.\n‚óè Implement holdsLock() so that logic in Exercise 2 can determine whether a\npage is already locked by a transaction.\nYou may find it helpful to define a class that is responsible for maintaining state about\ntransactions and locks, but the design is up to you.\nYou may need to implement the next exercise before your code passes the unit tests in\nLockingTest.\n2.5. Lock Lifetime\nYou will need to implement strict two-phase locking. This means that transactions should acquire the appropriate\ntype of lock on any object before accessing that object and shouldn't release any locks until after the transaction\ncommits.\nFortunately, the SimpleDB design is such that is possible obtain locks on pages in BufferPool.getPage()\nbefore you read or modify them. So, rather than adding calls to locking routines in each of your operators, we\nrecommend acquiring locks in getPage(). Depending on your implementation, it is possible that you may not\nhave to acquire a lock anywhere else. It is up to you to verify this!\nYou will need to acquire a shared lock on any page (or tuple) before you read it, and you will need to acquire an\nexclusive lock on any page (or tuple) before you write it. You will notice that we are already passing around\nPermissions objects in the BufferPool; these objects indicate the type of lock that the caller would like to have\non the object being accessed (we have given you the code for the Permissions class.)\nNote that your implementation of HeapFile.addTuple() and HeapFile.deleteTuple(), as well as the\nimplementation of the iterator returned by HeapFile.iterator() should access pages using BufferPool.\ngetPage(). Double check that that these different uses of getPage() pass the correct permissions object (e.g.,\nPermissions.READ_WRITE or Permissions.READ_ONLY). You may also wish to double check that your\nimplementation of BufferPool.insertTuple() and BufferPool.deleteTupe() call markDirty() on\nany of the pages they access (you should have done this when you implemented this code in lab 2, but we did\nnot test for this case.)\nAfter you have acquired locks, you will need to think about when to release them as well. It is clear that you\nshould release all locks associated with a transaction after it has committed or aborted, but it is possible for there\nto be other scenarios in which releasing a lock before a transaction ends might be useful.\nExercise 2. Ensure that you acquire and release locks throughout SimpleDB. Some\n(but not necessarily all) actions that you should verify work properly:\n‚óè Reading tuples off of pages during a SeqScan (if you implemented locking in\n\n6.830 Lab 3: SimpleDB Transactions\nBufferPool.getPage(), this should work correctly as long as your HeapFile.\niterator() uses getPage().)\n‚óè Inserting and deleting tuples through BufferPool and HeapFile methods (if you\nimplemented locking in BufferPool.getPage(), this should work correctly as\nlong as HeapFile.addTuple() and HeapFile.deleteTuple() uses\ngetPage().)\nYou will also want to think especially hard about acquiring and releasing locks in the\nfollowing situations:\n‚óè Adding a new page to a HeapFile. When do you physically write the page to\ndisk? Are there race conditions with other transactions (on other threads) that\nmight need special attention at the HeapFile level, regardless of page-level\nlocking?\n‚óè Looking for an empty slot into which you can insert tuples. Most implementations\nscan pages looking for an empty slot, and will need a READ_ONLY lock to do this.\nSurprisingly, however, if a transaction t finds no free slot on a page p, t may\nimmediately release the lock on p. Although this apparently contradicts the rules of\ntwo-phase locking, it is ok because t did not use any data from the page, such that\na concurrent transaction t' which updated p cannot possibly effect the answer or\noutcome of t.\nAt this point, your code should pass the unit tests in LockingTest.\n2.6. Implementing NO STEAL\nModifications from a transaction are written to disk only after it commits. This means we can abort a transaction\nby discarding the dirty pages and rereading them from disk. Thus, we must not evict dirty pages. This policy is\ncalled NO STEAL.\nYou will need the flushPage method BufferPool. In particular, it must never evict a dirty page. If your eviction\npolicy prefers a dirty page for eviction, you will have to find a way to evict an alternative page. In the case where\nall pages in the buffer pool are dirty, you should throw a DbException.\nNote that, in general, evicting a clean page that is locked by a running transaction is OK when using NO STEAL,\nas long as your lock manager keeps information about evicted pages around, and as long as none of your\noperator implementations keep references to Page objects which have been evicted.\nExercise 3. Implement the necessary logic for page eviction without evicting dirty\npages in the evictPage method in BufferPool.\n2.7. Transactions\nIn SimpleDB, a TransactionId object is created at the beginning of each query. This object is passed to each\nof the operators involved in the query. When the query is complete, the BufferPool method\ntransactionComplete is called.\n\n6.830 Lab 3: SimpleDB Transactions\nCalling this method either commits or aborts the transaction, specified by the parameter flag commit. At any\npoint during its execution, an operator may throw a TransactionAbortedException exception, which\nindicates an internal error or deadlock has occurred. The test cases we have provided you with create the\nappropriate TransactionId objects, pass them to your operators in the appropriate way, and invoke\ntransactionComplete when a query is finished. We have also implemented TransactionId.\nExercise 4. Implement the transactionComplete() method in BufferPool. Note\nthat there are two versions of transactionComplete, one which accepts an additional\nboolean commit argument, and one which does not. The version without the additional\nargument should always commit and so can be implemented by calling\ntransactionComplete(tid, true).\nWhen you commit, you should flush dirty pages associated to the transaction to disk.\nWhen you abort, you should revert any changes made by the transaction by restoring the\npage to its on-disk state.\nWhether the transaction commits or aborts, you should also release any state the\nBufferPool keeps regarding the transaction, including releasing any locks that the\ntransaction held.\nAt this point, your code should pass the TransactionTest unit test and the\nAbortEvictionTest system test. You may find the TransactionTest system test illustrative,\nbut it will likely fail until you complete the next exercise.\n2.8. Deadlocks and Aborts\nIt is possible for transactions in SimpleDB to deadlock (if you do not understand why, we recommend reading\nabout deadlocks in Ramakrishnan & Gehrke). You will need to detect this situation and throw a\nTransactionAbortedException.\nThere are many possible ways to detect deadlock. For example, you may implement a simple timeout policy that\naborts a transaction if it has not completed after a given period of time. Alternately, you may implement cycle-\ndetection in a dependency graph data structure. In this scheme, you would check for cycles in a dependency\ngraph whenever you attempt to grant a new lock, and abort something if a cycle exists.\nAfter you have detected that a deadlock exists, you must decide how to improve the situation. Assume you have\ndetected a deadlock while transaction t is waiting for a lock. If you're feeling homicidal, you might abort all\ntransactions that t is waiting for; this may result in a large amount of work being undone, but you can guarantee\nthat t will make progress.\nAlternately, you may decide to abort t to give other transactions a chance to make progress. This means that the\nend-user will have to retry transaction t.\nExercise 5. Implement deadlock detection and resolution in src/simpledb/\nBufferPool.java. Most likely, you will want to check for deadlock whenever a\ntransaction attempts to acquire a lock and finds another transaction is holding the lock\n\n6.830 Lab 3: SimpleDB Transactions\n(note that this by itself is not a deadlock, but may be symptomatic of one.) You have\nmany design decisions for your deadlock resolution system, but it is not necessary to do\nsomething complicated. Please describe your choices in your writeup.\nYou should ensure that your code aborts transactions properly when a deadlock occurs,\nby throwing a TransactionAbortedException exception. This exception will be\ncaught by the code executing the transaction (e.g., TransactionTest.java), which\nshould call transactionComplete() to cleanup after the transaction. You are not\nexpected to automatically restart a transaction which fails due to a deadlock -- you can\nassume that higher level code will take care of this.\nWe have provided some (not-so-unit) tests in test/simpledb/DeadlockTest.java.\nThey are actually a bit involved, so they make take more than a few seconds to run\n(depending on your policy). If they seem to hang indefinitely, then you probably have an\nunresolved deadlock. These tests construct simple deadlock situations that your code\nshould be able to escape.\nNote that there are two timing parameters near the top of DeadLockTest.java; these\ndetermine the frequency at which the test checks if locks have been acquired and the\nwaiting time before an aborted transaction is restarted. You may observe different\nperformance characteristics by tweaking these parameters if you use a timeout-based\ndetection method. The tests will output TransactionAbortedExceptions\ncorresponding to resolved deadlocks to the console.\nYour code should now should pass the TransactionTest system test (which may also run\nfor quite a long time).\nAt this point, you should have a recoverable database, in the sense that if the database\nsystem crashes (at a point other than transactionComplete()) or if the user\nexplicitly aborts a transaction, the effects of any running transaction will not be visible\nafter the system restarts (or the transaction aborts.) You may wish to verify this by\nrunning some transactions and explicitly killing the database server.\n2.9. Design alternatives\nDuring the course of this lab, we have identified three substantial design choices that you have to make:\n‚óè Locking granularity: page-level versus tuple-level\n‚óè Deadlock detection: timeouts versus dependency graphs\n‚óè Deadlock resolution: aborting yourself versus aborting others\nBonus Exercise 6. (10% extra credit) For one or more of these choices,\nimplement both alternatives and briefly compare their performance characteristics in your\nwriteup.\nYou have now completed this lab. Good work!\n\n6.830 Lab 3: SimpleDB Transactions\n3. Logistics\nYou must submit your code (see below) as well as a short (2 pages, maximum) writeup describing your\napproach. This writeup should:\n‚óè Describe any design decisions you made, including your deadlock detection policy, locking granularity, etc.\n‚óè Discuss and justify any changes you made to the API.\n3.1. Collaboration\nThis lab should be manageable for a single person, but if you prefer to work with a partner, this is also OK. Larger\ngroups are not allowed. Please indicate clearly who you worked with, if anyone, on your writeup.\n3.2. Submitting your assignment\nTo submit your code, please create a 6.830-lab3.tar.gz tarball (such that, untarred, it creates a 6.830-\nlab3/src/simpledb directory with your code) and submit it. You may submit your code multiple times; we wil\nl use the latest version you submit that arrives before the deadline (before 11:59pm on the due date).\nIf applicable, please indicate your partner in your writeup. Please also submit your individual writeup as a\nPDF or plain text file (.txt). Please do not submit a .doc or .docx.\n3.3. Submitting a bug\nPlease submit (friendly!) bug reports. When you do, please try to include:\n‚óè A description of the bug.\n‚óè A .java file we can drop in the src/simpledb/test directory, compile, and run.\n‚óè A .txt file with the data that reproduces the bug. We should be able to convert it to a .dat file using\nPageEncoder.\nIf you are the first person to report a particular bug in the code, we will give you a candy bar!\n3.4 Grading\n50% of your grade will be based on whether or not your code passes the system test suite we will run over it.\nThese tests will be a superset of the tests we have provided. Before handing in your code, you should make sure\nproduces no errors (passes all of the tests) from both ant test and ant systemtest.\nImportant: before testing, we will replace your build.xml, HeapFileEncoder.java, and the entire\ncontents of the test/ directory with our version of these files! This means you cannot change the format of .dat\nfiles! You should therefore be careful changing our APIs. This also means you need to test whether your code\ncompiles with our test programs. In other words, we will untar your tarball, replace the files mentioned above,\ncompile it, and then grade it. It will look roughly like this:\n\n6.830 Lab 3: SimpleDB Transactions\n$ gunzip 6.830-lab3.tar.gz\n$ tar xvf 6.830-lab3.tar\n$ cd 6.830-lab3\n[replace build.xml, HeapFileEncoder.java, and test]\n$ ant test\n$ ant systemtest\n[additional tests]\nIf any of these commands fail, we'll be unhappy, and, therefore, so will your grade.\nAn additional 50% of your grade will be based on the quality of your writeup and our subjective evaluation of your\ncode.\nWe've had a lot of fun designing this assignment, and we hope you enjoy hacking on it!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT6_830F10_quiz01_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/4ef68d0623e26e1ccd86d74f576d71c3_MIT6_830F10_quiz01_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.830 Database Systems: Fall 2008 Quiz I Solutions\nThere are 17 questions and 13 pages in this quiz booklet. To receive credit for a question, answer\nit according to the instructions given. You can receive partial credit on questions. You have 80\nminutes to answer the questions.\nWrite your name on this cover sheet AND at the bottom of each page of this booklet.\nSome questions may be harder than others. Attack them in the order that allows you to make\nthe most progress. If you find a question ambiguous, be sure to write down any assumptions you\nmake. Be neat. If we can't understand your answer, we can't give you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES QUIZ.\nNO PHONES, NO LAPTOPS, NO PDAS, ETC.\nDo not write in the boxes below\n1-4 (xx/20)\n5-7 (xx/16)\n8-10 (xx/20)\n11-13 (xx/21)\n14-17 (xx/23)\nTotal (xx/100)\nName: Solutions\n\nI\n6.830 Fall 2008, Quiz 1 Solutions\nPage 2 of 13\nShort Answer\n1. [6 points]: To reduce the number of plans the query optimizer must consider, the Selinger Opti\nmizer employs a number of heuristics to reduce the search space. List three:\n(Fill in the blanks below.)\n- Push selections down to the leaves.\n- Push projections down.\n- Consider only left deep plans.\n- Join tables without join predicates (cross product joins) as late as possible.\n2. [4 points]: Give one reason why the REDO pass of ARIES must use physical logging.\n(Write your answer in the space below.)\nThe pages on disk may be in any state between \"no updates applied\" and \"all updates applied.\"\nLogical redo cannot be used on an indeterminate state, whereas physically redo logging just over\nwrites the data on disk with the logged values. In other words, physical logging is idempotent\nwhereas logical logging is not.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 3 of 13\nII Optimistic Concurrency Control\nFor the following transaction schedules, indicate which transactions would commit and which would abort\nwhen run using the parallel validation scheme described in the Kung and Robinson paper on Optimistic\nConcurrency Control. Also give a brief justification for your answer. You may assume that if a transaction\naborts, it does not execute its write phase, rolling back instantly at the end of the validation phase.\n3. [4 points]:\nRead\nValidate\nWrite\nRead set: {A,B} Write set: {B,C}\nT1\nRead set: {A,D} Write set: {A,B}\nT2\nTime\n(Write your answer in the spaces below.)\nTransactions that commit: T1,T2 or T1\nTransactions that abort: {} or T2\nJustification:\nThere are two correct solutions:\nT1 and T2 both commit. T1 commits because it validates first. T2 will also commit because while\nits write set intersects T1's write set, T1 finishes its write phase before T2 starts its write phase\n(condition 2 in section 3.1 of the paper).\nT1 commits, T2 aborts. T1 commits because it validates first. In the pseudocode from the paper\n(section 5), T1 is executing the line marked \"write phase\" when T2 starts its validation phase. T2\nfinds T1 in the \"finish active\" set, and aborts due to a write/write conflict. This is sort of a \"false\"\nconflict, since it would be safe for T2 to commit. However, the parallel algorithm assumes that T1\nis still writing at the time that T2 detects the conflict, so it must abort.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 4 of 13\n4. [6 points]:\nRead set: {D} Write set: {D}\nT3\nRead\nValidate\nWrite\nRead set: {A,B} Write set: {B,C}\nT1\nRead set: {A,B} Write set: {B,D}\nT2\n(Write your answer in the spaces below.)\nTransactions that commit: T1,T3\nTransactions that abort:T2\nJustification:\nT1 and T3 commit, T2 aborts. T1 commits because it validates first. T2 must abort because T1's\nwrite set intersects T2's read set, and T2 was started reading before T1 finished writing. T3 then\ncommit because although it has a conflict with T2, when it begins the validation algorithm, T2 has\nalready aborted (it finishes aborting at the end of the validation phase).\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 5 of 13\nIII Schema Design\nConsider a relational table:\nProfessor(\nprofessor name, professor id,\nprofessor office id, student id,\nstudent name, student office id,\nstudent designated refrigerator id, refrigerator owner id,\nrefrigerator id, refrigerator size, secretary name,\nsecretary id, secretary office )\nSuppose the data has the following properties:\nA. Professors and secretaries have individual offices, students share offices.\nB. Students can work for multiple professors.\nC. Refrigerators are owned by one professor.\nD. Professors can own multiple refrigerators.\nE. Students can only use one refrigerator.\nF. The refrigerator the student uses must be owned by one of the professors they work for.\nG. Secretaries can work for multiple professors.\nH. Professors only have a single secretary.\n5. [10 points]: Put this table into 3rd normal form by writing out the decomposed tables; designate\nkeys in your tables by underlining them. Designate foreign keys by drawing an arrow from a foreign\nkey to the primary key it refers to. Note that some of the properties listed above may not be enforced\n(i.e., guaranteed to be true) by a 3NF decomposition.\n(Write your answer in the space below.)\nprof: pid pname poffice psec\nstudent: sid sname soffice sfridge\nsec: secid secname secoffice\nfridge: fridgeid fridgesize fridgeowner\nsworksfor: sid profid\nName:\n\nl\nm\n6.830 Fall 2008, Quiz 1 Solutions\nPage 6 of 13\n6. [3 points]: Which of the eight properties (A-H) of the data are enforced (i.e., guaranteed to be true)\nby the 3NF decomposition and primary and foreign keys you gave above?\n(Write your answer in the space below.)\nB,C,D,E,G,H; the schema does not enfore that profs/secretaries have individual offices or that stu\ndents use the fridge of the professor they work for.\n7. [3 points]: What could a database administrator do to make sure the properties not explicitly en\nforced by your schema are enforced by the database?\n(Write your answer in the space below.)\nUse triggers or stored procedures to ensure that these constraints are checked when data is in\nserted or updated.\nIV External Aggregation\nSuppose you are implementing an aggregation operator for a database system, and you need to support aggre\ngation over very large data sets with more groups than can fit into memory.\nYour friend Johnny Join begins writing out how he would implement the algorithm. Unfortunately, he stayed\nup all night working on Lab 3 and so trails off in an incoherent stream of obscenities and drool before finishing\nhis code. His algorithm begins as follows:\ninput : Table T, aggregation function fname, aggregation field aggf , group by field gbyf\n/* Assume T has |T | pages and your system has |M| pages of memory\n*/\n/* Partition phase\n*/\nn\n|T | ;\n‚Üê\n|M|\nAllocate bu f s, n pages of memory for output buffers ;\nAllocate files, an array of n files for partitions ;\nforeach record r ‚àà T do\nh ‚Üê hash(r.gby f ) ;\n/* h ‚àà [1 . . . n] */\nWrite r into page bu f s[h] ;\nif page bu f s[h] is full then\nAdd bu f s[h] to file files[h], and set the contents of buf s[h] to empty ;\nend\nend\n/* Aggregate phase\n*/\nforeach f ‚àà files do\n/* Your code goes here.\n*/\nend\nName:\n\nl\nm\nl\nm\n6.830 Fall 2008, Quiz 1 Solutions\nPage 7 of 13\n8. [6 points]: Relative to |T|, how big must |M| be for the algorithm to function correctly? Justify your\nanswer with a sentence or brief analytical argument.\n(Write your answer in the space below.)\nBecause buf s should fit in memory, n ‚â§|M|.\nT\nThe number of hash buckets is computed as n =\n|\nM\n| .\n|\n|\nT\nPlugging in for n, we get that\n|\n|\nM\n|\n| ‚â§|M|.\nTherefore, |T | ‚â§|M|2 .\nIt follows that |M| ‚â• ‚àö|T|.\n9.\n[8 points]: Assuming the aggregate phase does only one pass over the data, and that fname\n= 'AVG', describe briefly what should go in the body of the for loop (in place of \"Your code goes\nhere\"). You may write pseudocode or a few short sentences. Suppose the system provides a function\nemit(group,val) to output the value of a group.\n(Write your answer in the space below.)\nAll tuples with the same gby f will end up in the same partition, but there may (very likely will) be multiple\ngbyf values per partition.\nThe basic algorithm should be something like:\nH = new HashTable // stores (gbyf, <sum,cnt>) pairs\nfor tuple t in f:\nif ((<sum,cnt> = H.get(t.gbyf)) == null)\n<sum,cnt> = <0,0>\nsum = sum + t.aggf\ncnt = cnt + 1\nT.put(t.gbyf,<sum,cnt>)\nfor (group, <sum,cnt>) in T:\nemit(group,sum/cnt)\nAny in-memory aggregation would work here, so if you prefer to sort the values in memory and then\nbucket them as is done in the next question, that is also an acceptable answer.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 8 of 13\n10. [6 points]: Describe an alternative to this hashing-based algorithm. Your answer shouldn't require\nmore than a sentence or two.\n(Write your answer in the space below.)\nRemember that the tuples do not fit in memory.\nOne solution would be to use an external memory sort on the table, where the sort key for each tuple is\nthe gbyf field.\nAfter sorting, sequentially scan the sorted file's tuples, keeping a running average for each gbyf grouping\n(they will appear in contiguous chunks in the file). The average should be calculated on each aggf field.\nOther solutions included using external memory indices, such as B+Trees, on gbyf to group the tuples on\nwhich to calculate an average.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 9 of 13\nV Cost Estimation\nSuppose you are given a database with the following tables and sizes, and that each data page holds 100\ntuples, that both leaf and non-leaf B+Tree pages are dense-packed and hold 100 keys, and that you have 102\npages of memory. Assume that the buffer pool is managed as described in the DBMIN Paper (\"An Evaluation\nof Buffer Management Strategies for Relational Database Systems.\", VLDB 1985.)\nTable\nSize, in pages\nT1\nT2\nT3\n11. [15 points]: Estimate the minimum number of I/Os required for the following join operations. Ig\nnore the difference between random and sequential I/O. Assume that B+Trees store pointers to records\nin heap files in their leaves (i.e., B+Tree pages only store keys, not tuples.)\n(Write your answers in the spaces below. Justify each answer\nwith a short analytical argument.)\n- Nested loops join between T1 and T2, no indices.\nT1 fits in memory. Put it as the inner and read all of T1 once. Then scan through T2 as the outer.\nTotal: |T 1| + |T2| = 1100 I/Os.\n- Grace hash join between T2 and T3, no indices.\nGrace join hashes all of T2, T3 in one pass, outputting records as it goes. It then scans through\nthe hashed output to do the join. Therefore it reads all tuples twice and writes them once for a total\ncost of: 3(|T 2| + |T 3|) = 18000 I/Os.\n- Index nested loops join between a foreign key of T2 and the primary key of T3, with a B+Tree index on\nT3 and no index on T2.\nPut T2 as the outer and T3 as the inner. Assume the matching tuples are always on a single\nB+Tree page and that selectivity is 1 due to foreign-key primary-key join. Cache the upper levels\nof the B+Tree. T3 is 500,000 tuples, so it needs 3 levels (top level = 100 pointers, second level =\n1002 = 10, 000, third level 1003 = 1, 000, 000 pointers). Cache the root plus all but one of the level 2\npages (100 pages). Read all of T2 once (one page at a time). For each tuple in T2, we do a lookup\nin the B+Tree and read one B+Tree leaf page (at level 3), then follow the pointer to fetch the actual\ntuple from the heap file (using the other page of memory).\nTotal cost is: 1000(|T 2|) + 100(top o f B + Tree) + {T 2} √ó No.BTree lookups\nFor 99/100 of the B+Tree lookups, two levels will be cached, so {T2} √ó No.BTree lookups is:\n99/100 ‚àó (2 √ó |T2|) = 99/100 ‚àó 2 √ó 100000 = 198000 pages.\nFor 1/100 of the B+Tree lookups, only the root level will be cached:\n1/100 ‚àó (3 √ó |T 2|) = 1/100 ‚àó 3 √ó 100000 = 3000 pages.\nSo the total is :\n1000 + 100 + 198000 + 3000 = 202100 I/Os\nWe were flexible with the exact calculation here due to the complexity introduced by not being able\nto completely cache both levels of the B+Tree.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 10 of 13\nVI ARIES with CLRs\nSuppose you are given the following log file.\nLSN\nTID\nPrevLsn\nType\nData\n-\nSOT\n-\nUP\nA\nUP\nB\n-\nSOT\n-\nUP\nC\n-\n-\nCP\ndirty, trans\n-\nSOT\n-\nUP\nD\nUP\nE\nCOMMIT\n-\nUP\nB\nCLR\nB\nCLR\nE\n12. [2 points]: After recovery, which transactions will be committed and which will be aborted?\n(Write your answers in the spaces below)\nCommitted:\nAborted:\nT1 commits, while T2 and T3 abort, since T1 is the only transaction that has a COMMIT record in\nthe log.\n13. [4 points]: Suppose the dirty page table in the CP record has only page A in it. At what LSN will\nthe REDO pass of recovery begin?\n(Write your answer in the space below)\nLSN:\nThe LSN that the REDO pass will start at is 2.\nThe LSN selected is the earliest recoveryLSN in the dirty page table for any dirty page. Since only\npage A is in the dirty page table, and the first log record in which it was modified (its recoveryLSN)\nis 2, the REDO pass will begin at LSN 2. Pages B and C might have been stolen (STEAL) by some\nbackground flush process before the checkpoint was written out, and so they do not appear in the\ndirty page table.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 11 of 13\n14. [4 points]: Again, suppose the dirty page table in the CP record has only page A in it. What pages\nmay be written during the REDO pass of recovery?\n(Write your answer in the space below)\nPages:\nPages A, B, D, E. REDO will start at LSN 2, and re-apply any UP record in the log. Since B and C\nare not in the dirty page table, any change to them before the checkpoint must already be on disk,\nso they are not written by REDO before the checkpoint. Since B is updated after the checkpoint, it\nwill be written at that point.\n15. [4 points]: Once again, suppose the dirty page table in the CP record has only page A in it. What\npages may be written during the UNDO pass of recovery?\n(Write your answer in the space below)\nPages:\nPages C and D. The UNDO stage starts at the last LSN for the last transaction to be aborted, and\nproceeds backward through the log. LSN 13 is read first, but since it is a CLR (previous recovery\nhandled it), E was correctly updated on disk, and is not rewritten. LSN 12 is also a CLR, so\nwe skip the update to B for the same reason. Following LSN 13's prevLSN pointer, we see that\nLSN 7 was the start of transaction 3, so we are done undoing transaction 3. Following LSN 12's\nprevLSN pointer leads us to LSN 8, and so we undo the update to D (overwriting D). Following LSN\n8's prevLSN pointer to LSN 5, we again undo the update to C (overwriting C). Following LSN 5's\nprevLSN, we go to LSN 4, which is the start of transaction 2, and thus the end of the UNDO pass.\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 12 of 13\nVII Snapshot Isolation\nOracle and Postgres both use a form of transaction isolation called snapshot isolation. One possible imple\nmentation of snapshot isolation is as follows:\n- Every object (e.g., tuple or page) in the database has a timestamp; multiple copies (\"versions\") of\nobjects with old timestamps are kept until no transaction needs to read them again. (For this question,\nyou don't need to worry about how such old versions are maintained or discarded.)\n- When a transaction begins, the system records the transaction's start time stamp, tss. Timestamps are\nmonotonically increasing, such that no two transactions have the same timestamp value.\n- When a transaction T writes an object O, it adds the new version of the O to T 's local write set. Versions\nin the local write set are not read by other transactions until after T has committed.\n- When a transaction T reads an object O, it reads the most recent committed version with timestamp\n‚â§ tss, reading O from T's own local write set if O has been previously written by T .\n- When a transaction T commits, a new timestamp, tsc is taken. For every object O in T's local write set,\nif the most recent version of O in the database has timestamp ‚â§ tss, then O is written into the database\nwith timestamp tsc. Otherwise, T aborts. Only one transaction commits at a time.\nFor example, consider the following schedule:\nInitial database: objects A and B, both version 0 (denoted A0 and B0)\nT 1(tss = 1)\nT 2(tss = 2)\nRead(A0)\nWrite(A)\nRead(A0)\nWrite(A)\ncommit (tsc = 3)\ninstall A3\nattempt to commit with tsc = 4,\nbut abort, because last version of A (3) > tss = 2\nHere, T2 aborts because it tried to write A concurrently with T1.\n16. [10 points]: Is snapshot isolation conflict serializable? If yes, state briefly why. If not, give an\nexample of a non-serializable schedule.\n(Write your answer in the space below.)\nNo. Snapshot isolation ignores read/write conflicts. Consider T1: (RA, WB) and T2: (RB, WA).\nThese execute in the following order:\nRA0 (T1), RB0 (T2), WB1 (T1), WA2 (T2)\nUnder snapshot isolation, this order is acceptable and both transactions can commit. However, this\nis neither serial orders T1, T2 (where T2 would RB1) or T2, T1 (where T1 would RA2).\nName:\n\n6.830 Fall 2008, Quiz 1 Solutions\nPage 13 of 13\n17. [5 points]: Oracle claims that snapshot isolation is much faster than traditional concurrency con\ntrol. Why?\n(Write your answer in the space below.)\nWrites never block readers. For example, if a long running transaction has written value A, with\ntraditional concurrency control any transaction that wants to read A will need to wait to acquire\na lock on A. With snapshot isolation, the reading transactions will read the previous version and\ncontinue.\nThis is similar to optimistic concurrency control in that there are no locks and transactions do not\nblock each other. However, with snapshot isolation, a transaction sees a consistent snapshot of\nthe database, determined when it begins. With optimistic concurrency control, a transaction can\nsee values from transactions that commit after it begins.\nEnd of Quiz I\nName:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT6_830F10_quiz01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/65e4ed7599357838d7300e3195f05fa0_MIT6_830F10_quiz01.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.830 Database Systems: Fall 2008 Quiz I\nThere are 17 questions and 10 pages in this quiz booklet. To receive credit for a question, answer\nit according to the instructions given. You can receive partial credit on questions. You have 80\nminutes to answer the questions.\nWrite your name on this cover sheet AND at the bottom of each page of this booklet.\nSome questions may be harder than others. Attack them in the order that allows you to make\nthe most progress. If you find a question ambiguous, be sure to write down any assumptions you\nmake. Be neat. If we can't understand your answer, we can't give you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES QUIZ.\nNO PHONES, NO LAPTOPS, NO PDAS, ETC.\nDo not write in the boxes below\n1-4 (xx/20)\n5-7 (xx/16)\n8-10 (xx/20)\n11-13 (xx/21)\n14-17 (xx/23)\nTotal (xx/100)\nName:\n\nI\n6.830 Fall 2008, Quiz 1\nPage 2 of 10\nShort Answer\n1. [6 points]: To reduce the number of plans the query optimizer must consider, the Selinger Opti\nmizer employs a number of heuristics to reduce the search space. List three:\n(Fill in the blanks below.)\nA.\nB.\nC.\n2. [4 points]: Give one reason why the REDO pass of ARIES must use physical logging.\n(Write your answer in the space below.)\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 3 of 10\nII Optimistic Concurrency Control\nFor the following transaction schedules, indicate which transactions would commit and which would abort\nwhen run using the parallel validation scheme described in the Kung and Robinson paper on Optimistic\nConcurrency Control. Also give a brief justification for your answer. You may assume that if a transaction\naborts, it does not execute its write phase, rolling back instantly at the end of the validation phase.\n3. [4 points]:\nRead\nValidate\nWrite\nRead set: {A,B} Write set: {B,C}\nT1\nRead set: {A,D} Write set: {A,B}\nT2\nTime\n(Write your answer in the spaces below.)\nTransactions that commit:\nTransactions that abort:\nJustification:\n4. [6 points]:\nRead set: {D} Write set: {D}\nT3\nRead\nValidate\nWrite\nRead set: {A,B} Write set: {B,C}\nT1\nRead set: {A,B} Write set: {B,D}\nT2\n(Write your answer in the spaces below.)\nTransactions that commit:\nTransactions that abort:\nJustification:\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 4 of 10\nIII Schema Design\nConsider a relational table:\nProfessor(\nprofessor name, professor id,\nprofessor office id, student id,\nstudent name, student office id,\nstudent designated refrigerator id, refrigerator owner id,\nrefrigerator id, refrigerator size, secretary name,\nsecretary id, secretary office )\nSuppose the data has the following properties:\nA. Professors and secretaries have individual offices, students share offices.\nB. Students can work for multiple professors.\nC. Refrigerators are owned by one professor.\nD. Professors can own multiple refrigerators.\nE. Students can only use one refrigerator.\nF. The refrigerator the student uses must be owned by one of the professors they work for.\nG. Secretaries can work for multiple professors.\nH. Professors only have a single secretary.\n5. [10 points]: Put this table into 3rd normal form by writing out the decomposed tables; designate\nkeys in your tables by underlining them. Designate foreign keys by drawing an arrow from a foreign\nkey to the primary key it refers to (see example below.) Note that some of the properties listed above\nmay not be enforced (i.e., guaranteed to be true) by a 3NF decomposition.\n(Write your answer in the space below.)\nemp : eid, name, dno\ndept : did, bldg, name\nExample decomposition of\nemp/dept tables showing\nprimary and foreign keys.\nName:\n\nl\nm\n6.830 Fall 2008, Quiz 1\nPage 5 of 10\n6. [3 points]: Which of the eight properties (A-H) of the data are enforced (i.e., guaranteed to be true)\nby the 3NF decomposition and primary and foreign keys you gave above?\n(Write your answer in the space below.)\n7. [3 points]: What could a database administrator do to make sure the properties not explicitly en\nforced by your schema are enforced by the database?\n(Write your answer in the space below.)\nIV External Aggregation\nSuppose you are implementing an aggregation operator for a database system, and you need to support aggre\ngation over very large data sets with more groups than can fit into memory.\nYour friend Johnny Join begins writing out how he would implement the algorithm. Unfortunately, he stayed\nup all night working on Lab 3 and so trails off in an incoherent stream of obscenities and drool before finishing\nhis code. His algorithm begins as follows:\ninput : Table T, aggregation function fname, aggregation field aggf , group by field gbyf\n/* Assume T has |T | pages and your system has |M| pages of memory\n*/\n/* Partition phase\n*/\nn\n|T | ;\n‚Üê\n|M|\nAllocate bu f s, n pages of memory for output buffers ;\nAllocate files, an array of n files for partitions ;\nforeach record r ‚àà T do\nh ‚Üê hash(r.gby f ) ;\n/* h ‚àà [1 . . . n] */\nWrite r into page bu f s[h] ;\nif page bu f s[h] is full then\nAdd bu f s[h] to file files[h], and set the contents of buf s[h] to empty ;\nend\nend\n/* Aggregate phase\n*/\nforeach f ‚àà files do\n/* Your code goes here.\n*/\nend\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 6 of 10\n8. [6 points]: Relative to |T|, how big must |M| be for the algorithm to function correctly? Justify your\nanswer with a sentence or brief analytical argument.\n(Write your answer in the space below.)\n9.\n[8 points]: Assuming the aggregate phase does only one pass over the data, and that fname\n= 'AVG', describe briefly what should go in the body of the for loop (in place of \"Your code goes\nhere\"). You may write pseudocode or a few short sentences. Suppose the system provides a function\nemit(group,val) to output the value of a group.\n(Write your answer in the space below.)\n10. [6 points]: Describe an alternative to this hashing-based algorithm. Your answer shouldn't require\nmore than a sentence or two.\n(Write your answer in the space below.)\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 7 of 10\nV Cost Estimation\nSuppose you are given a database with the following tables and sizes, and that each data page holds 100\ntuples, that both leaf and non-leaf B+Tree pages are dense-packed and hold 100 keys, and that you have 102\npages of memory. Assume that the buffer pool is managed as described in the DBMIN Paper (\"An Evaluation\nof Buffer Management Strategies for Relational Database Systems.\", VLDB 1985.)\nTable\nSize, in pages\nT1\nT2\nT3\n11. [15 points]: Estimate the minimum number of I/Os required for the following join operations. Ig\nnore the difference between random and sequential I/O. Assume that B+Trees store pointers to records\nin heap files in their leaves (i.e., B+Tree pages only store keys, not tuples.)\n(Write your answers in the spaces below. Justify each answer\nwith a short analytical argument.)\nNested loops join between T1 and T2, no indices.\n-\nGrace hash join between T2 and T3, no indices.\n-\nIndex nested loops join between a foreign key of T2 and the primary key of T3, with a B+Tree index on\n-\nT3 and no index on T2.\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 8 of 10\nVI ARIES with CLRs\nSuppose you are given the following log file.\nLSN\nTID\nPrevLsn\nType\nData\n-\nSOT\n-\nUP\nA\nUP\nB\n-\nSOT\n-\nUP\nC\n-\n-\nCP\ndirty, trans\n-\nSOT\n-\nUP\nD\nUP\nE\nCOMMIT\n-\nUP\nB\nCLR\nB\nCLR\nE\n12. [2 points]: After recovery, which transactions will be committed and which will be aborted?\n(Write your answers in the spaces below)\nCommitted:\nAborted:\n13. [4 points]: Suppose the dirty page table in the CP record has only page A in it. At what LSN will\nthe REDO pass of recovery begin?\n(Write your answer in the space below)\nLSN:\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 9 of 10\n14. [4 points]: Again, suppose the dirty page table in the CP record has only page A in it. What pages\nmay be written during the REDO pass of recovery?\n(Write your answer in the space below)\nPages:\n15. [4 points]: Once again, suppose the dirty page table in the CP record has only page A in it. What\npages may be written during the UNDO pass of recovery?\n(Write your answer in the space below)\nPages:\nVII Snapshot Isolation\nOracle and Postgres both use a form of transaction isolation called snapshot isolation. One possible imple\nmentation of snapshot isolation is as follows:\n- Every object (e.g., tuple or page) in the database has a timestamp; multiple copies (\"versions\") of\nobjects with old timestamps are kept until no transaction needs to read them again. (For this question,\nyou don't need to worry about how such old versions are maintained or discarded.)\n- When a transaction begins, the system records the transaction's start time stamp, tss. Timestamps are\nmonotonically increasing, such that no two transactions have the same timestamp value.\n- When a transaction T writes an object O, it adds the new version of the O to T 's local write set. Versions\nin the local write set are not read by other transactions until after T has committed.\n- When a transaction T reads an object O, it reads the most recent committed version with timestamp\n‚â§ tss, reading O from T's own local write set if O has been previously written by T .\n- When a transaction T commits, a new timestamp, tsc is taken. For every object O in T's local write set,\nif the most recent version of O in the database has timestamp ‚â§ tss, then O is written into the database\nwith timestamp tsc. Otherwise, T aborts. Only one transaction commits at a time.\nName:\n\n6.830 Fall 2008, Quiz 1\nPage 10 of 10\nFor example, consider the following schedule:\nInitial database: objects A and B, both version 0 (denoted A0 and B0)\nT 1(tss = 1)\nT 2(tss = 2)\nRead(A0)\nWrite(A)\nRead(A0)\nWrite(A)\ncommit (tsc = 3)\ninstall A3\nattempt to commit with tsc = 4,\nbut abort, because last version of A (3) > tss = 2\nHere, T2 aborts because it tried to write A concurrently with T1.\n16. [10 points]: Is snapshot isolation conflict serializable? If yes, state briefly why. If not, give an\nexample of a non-serializable schedule.\n(Write your answer in the space below.)\n17. [5 points]: Oracle claims that snapshot isolation is much faster than traditional concurrency con\ntrol. Why?\n(Write your answer in the space below.)\nEnd of Quiz I\nName:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT6_830F10_quiz02_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/a9ec7687aec0c4c084b7fb7c1b25d83d_MIT6_830F10_quiz02_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.830 Database Systems: Fall 2008 Quiz II\nThere are 14 questions and 11 pages in this quiz booklet. To receive credit for a question, answer\nit according to the instructions given. You can receive partial credit on questions. You have 80\nminutes to answer the questions.\nWrite your name on this cover sheet AND at the bottom of each page of this booklet.\nSome questions may be harder than others. Attack them in the order that allows you to make\nthe most progress. If you find a question ambiguous, be sure to write down any assumptions you\nmake. Be neat. If we can't understand your answer, we can't give you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES QUIZ.\nNO PHONES, NO LAPTOPS, NO PDAS, ETC.\nYOU MAY USE A CALCULATOR.\nDo not write in the boxes below\n1-3 (xx/14)\n4-6 (xx/26)\n7-8 (xx/10)\n9-11 (xx/26)\n12-14 (xx/24)\nTotal (xx/100)\nName: Solutions\n\nI\n6.830 Fall 2008, Quiz 2\nPage 2 of 11\nShort Answer\n1. [4 points]: List two reasons why a column-oriented design is advantageous in a data warehouse\nenvironment.\n(Write your answer in the spaces below.)\nA. Data warehouse workloads involve large scans of a few columns, meaning that column-stores\nread less data from disk\nB. Columns tend to compress better than rows, which is good in a read intensive setting.\nOther answers are possible.\n2. [4 points]:\nWhich of the following statements about the MapReduce system are true:\n(Circle T or F for each statement.)\nT\nF\nA MapReduce job will complete even if one of the worker nodes fails.\nWorker tasks are restarted at other nodes when a failure occurs.\nT\nF\nA MapReduce job will complete even if the master node fails.\nThere is only a single master node, so if it fails the entire MapReduce job must be restarted.\nT\nF\nMap workers send their results directly to reduce workers.\nMap workers write their results to their local disk. The reducers pull from the disks when needed. Due\nto ambiguity about if this is \"sending directly\" or not, this question was excluded.\nT\nF\nIf there are M map tasks, using more than M map workers may improve MapReduce performance.\nMapReduce will start multiple copies of the last few map or reduce tasks, which can avoid issues with\nslow nodes.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 3 of 11\n3. [6 points]:\nWhich of the following statements about the two-phase commit (2PC) protocol are true:\n(Circle T or F for each statement.)\nT\nF\nIn the \"no information\" case (i.e., when a subordinate asks the coordinator about the outcome of a\ntransaction that the coordinator has no information about), the coordinator running the presumed abort\nversion of 2PC returns the same response to a subordinate as it would in basic 2PC.\nBoth versions answer \"ABORT.\"\nT\nF\nThe Presumed Commit protocol reduces the number of forced writes required by the coordinator com\npared to basic 2PC for transactions that successfully commit.\nPresumed commit forces a COLLECTING record to disk (and on the root coordinator, a COMMIT\nrecord), while the traditional protocol forces a COMMIT record.\nT\nF\nThe Presumed Abort protocol reduces the number of forced writes required of the coordinator compared\n\nto basic 2PC for transactions that abort.\nWith presumed abort the coordinator does not need to force write any entries in the abort case.\nT\nF\nIn basic 2PC, a subordinate that votes to abort a transaction T receives no more messages pertaining to\nT, assuming neither T's coordinator nor any of T's subordinate nodes fail.\nThe coordinator does not need to alert the aborted subordinate about the abort.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 4 of 11\nII C-Store\nDana Bass is trying to understand the performance difference between row- and column- oriented databases.\nHer computer's disk can perform sequential I/O at 20 MB/sec, and takes 10 ms per seek; it has 10 MB of\nmemory. She is performing range queries on a table with 10 4-byte integer columns, and 100 million rows.\nData is not compressed in either system, and all columns are sorted in the same order. Assume each column\nis laid out completely sequentially on disk, as is the row-oriented table.\n4. [16 points]: Fill in the table below with your estimates of the minimal I/O time for each operation,\nin a row-store and a column-store. Assume that all reads are on contiguous groups of rows, and that\nqueries output row-oriented tuples.\nSolution:\nThe cost for scanning sequential rows in a row-store is a seek to the first row, then a sequential scan of\nthe remaining rows. We assume that the data can be processed as it is scanned - one thread sequentially\nloads tuples from disk while another applies any predicates or materialization in parallel. In this\nscenario, having 10MB of RAM is the same as having 1GB or 1MB of RAM.\nIn a column-store, for each extra column one reads in parallel, the disk head must seek to a different\nfile containing that column at a given offset. Thus, the best strategy is to read in\nMB of each\ncolumns read\ncolumn in to memory (incurring one seek for each such read), materialize the row, process it in memory,\nand move to the next sequential chunk of columns. In the special case where you only read one column,\ntreat the behavior of the datbase as you would in a row-store with one column.\nNo. Columns\nRead\nNo. Rows\nRead\nColumn-Oriented System\nRow-Oriented System\n10 Million\n10Mrows‚àó4bytes‚àó1column\n20MB/sec\n= 2sec\n+ 10 ms seek\n2.1 sec total\n10Mrows‚àó4bytes‚àó10columns\n20MB/sec\n= 20sec\n+ 10 ms seek\n20.1 sec total\n10 Million\n20 sec sequential read (400MB of data)\n10MB RAM ‚Üí 1 MB/column\nread 400 MB in 1 MB chunks\n400 seeks * 10 msec = 4 sec\n24 sec total\n20 sec, 10 ms seek\n20.1 sec total\n(same as above)\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 5 of 11\nIII BigTable\nConsider an example table storing bank accounts in Google's BigTable system:\nKey\nValue\n\"adam\"\n$300\n\"evan\"\n$600\n\"sam\"\n$9000\nThe bank programmer implements the deposit function as follows:\nfunction deposit(account, amount):\ncurrentBalance = bigtable.get(account)\ncurrentBalance += amount\nbigtable.put(account, currentBalance)\n5. [4 points]: Given the table above, a user executes deposit(\"sam\", $500). The program crashes\nsomewhere during execution. What are the possible states of the \"sam\" row in the table? Provide a\nshort explanation for your answer.\n(Write your answer in the space below.)\nThe row could either have \"sam\" = $9000 or \"sam\" = $9500, depending on if the program crashed\nbefore or after it wrote the result.\n6. [6 points]: Given the table above, a user executes deposit(\"sam\", $500). The program completes.\nImmediately after the program completes, the power goes out in the BigTable data center, and all the\nmachines reboot. When BigTable recovers, what are the possible states of the \"sam\" row in the table?\nProvide a short explanation for your answer.\n(Write your answer in the space below.)\nThe row can only have the value $9500, since BigTable does not inform a client that a write has been\ncompleted until it logs the update to disk on multiple physical machines. Thus, when BigTable recovers\nit will find the write in the log.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 6 of 11\nNow consider the following pair of functions:\nfunction transfer(sourceAccount, destinationAccount, amount):\nfunction totalAssets():\nsourceBalance = bigtable.get(sourceAccount)\nassets = 0\nif sourceBalance < amount:\nfor account, balance in bigtable:\nraise Exception(\"not enough money to transfer\")\nassets += balance\nsourceBalance -= amount\nreturn assets\ndestinationBalance = bigtable.get(destinationAccount)\ndestinationBalance += amount\nbigtable.put(sourceAccount, sourceBalance)\nbigtable.put(destinationAccount, destinationBalance)\n7. [6 points]: Given the table above, a program executes transfer(\"evan\", \"adam\", 200). While that\nfunction is executing, another program executes totalAssets(). What return values from totalAssets()\nare possible? For the purposes of this question, assume there is only one version of each value, and that\nthe values in BigTable are stored and processed in the order given above (i.e., \"adam\", \"evan\", \"sam\").\nProvide a short explanation for your answer.\n(Write your answer in the space below.)\n$9,900 is possible if totalAssets either reads all the balances before or after the transfer modifies any of\nthem. $9,700 is possible if totalAssets reads \"adam\" before the $200 is added, but reads \"evan\" after\n$200 is deducted.\nIt might seem like $10,100 is possible, by reading \"adam\" after adding $200, and reading \"evan\"\nbefore the subtract. However, this is not possible. The transfer function performs the subtract, then the\nadd. The scan in totalAssets reads the rows in order. Thus, if it sees the modified version of \"adam\" it\nwill see the modified version of \"evan.\" However, it would not be a good idea to write an application\nthat depends on this behavour, as it depends on the order of the keys and the operations in the code,\nwhich could easily change.\n8. [4 points]: If the data were stored in a traditional relational database system using strict two-phase\nlocking, and transfer() and totalAssets() were each implemented as single transactions, what would the\npossible results of totalAssets() be? Provide a short explanation for your answer.\n(Write your answer in the space below.)\n$9,900 is the only possible solution because the DBMS will use concurrency control to execute the to\ntalAssets transaction either strictly before or after the transfer transaction.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 7 of 11\nIV Distributed Query Execution\nSuppose you have a database with two tables with the following schema:\nT1 : (A int PRIMARY KEY, B int, C varchar)\nT2 : (D int REFERENCES T1.A, E varchar)\nAnd suppose you are running the following query:\nSELECT T1.A,COUNT(*)\nFROM T1,T2\nAND T1.A = T2.D\nAND T1.B > n\nGROUP BY T1.A\nYour system has the following configuration:\nParameter\nDescription\nValue\n|T1|\n|T2|\n|M|\n||T1||\n|int|\nT\nSize of table T1\nSize of table T2\nSize of memory of a single node\nNumber of records in T1\nSize of integer, in bytes\nDisk throughput, in bytes / sec\n300 MB\n600 MB\n100 MB\n10 million\n20 MB/sec\nN\nNetwork transmit rate, in bytes / sec\n10 MB/sec\nf\nFraction of tuples selected by T1.B > n predicate\n0.5\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 8 of 11\n9. [9 points]: Suppose you run the query on a single node. Assuming there are no indices, and the\ntables are not sorted, indicate what join algorithm (of the algorithms we have studied in class) would be\nmost efficient, and estimate the time to process the query, ignoring disk seeks and CPU costs. Include a\nbrief justification or calculation.\n(Write your answers in the spaces below.)\nJoin algorithm:\nRead all of A (300MB), applying the predicate and projecting to keep only T1.A. Build an in-memory\nhash table with the results: 10 million tuples in T1 √ó 1 = 5 million output tuples √ó 4 bytes per tuple\n= 20 MB of data. Sequentially scan T2 (600 MB), probing T2.D in the hash table. Emit T1.A for each\nmatching join tuple, and build a hash aggregate to count the number of times T1.A appears (5 million\noutput tuples √ó (4 bytes id + 4 bytes count) = 40 MB). Finally iterate over the aggregate and output the\nresults to the user.\nEstimated time, in seconds:\nRead T1 in 300MB = 15s. Read T2 in 600MB = 30s.\n20MB/s\n20MB/s\nTotal time: 45 seconds.\n10. [10 points]: Now suppose you switch to a cluster of 5 machines. If the above query is the only\nquery running in the system, describe a partitioning for the tables and join algorithm that will provide\nthe best performance, and estimate the total processing time (again, ignoring disk seeks and CPU cost).\nYou may assume about the same number of T2.D values join with each T1.A value. Include a brief\njustification or calculation. You can assume the coordinator can receive an aggregate 50 MB/sec over\nthe network from the five workers transmitting simultaneously at 10 MB/sec.\n(Write your answers in the spaces below.)\nPartitioning strategy:\nHash tables on T1.A/T2.D. Each node will have approximately 60 MB of T1 and 120 MB of T2. 30 MB\nof T1 will pass the predicate.\nJoin algorithm:\nSame as above, but with smaller result set on each machine. Final aggregation sent to coordinator.\nSince tables are properly partitioned, no networking is needed to process joins.\nEstimated time, in seconds:\nEach node has to read 180 MB from disk, which takes 9 seconds. Each node has 2 million T1 records\non it, which are filtered to 1 million by the predicate (4 MB in-memory hash table for probing 4 byte\nintegers). The aggregation takes 8 MB in RAM, since each integer also needs a 4-byte counter. Once\nthe local aggregation is complete, the results are sent over the network to the coordinator in parallel\nwith the other nodes at 10 MB/sec, taking 0.8 seconds to send over the network.\nTotal time: 9.8 seconds.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 9 of 11\nV Adaptive Query Processing\nJenny Join, after hearing the lecture about adaptive query processing, decides to add several types of adaptivity\nto her database system.\nFirst, Jenny decides to add simple plan-based adaptivity in the style of the Kabra and DeWitt paper mentioned\nin the \"Adaptive Query Processing Through the Looking Glass\" paper and discussed in class.\nJenny runs the following query:\nD\nœÉ\nGrace\nHash\nSort-based\nAggregate\nC\nGrace\nHash\nA\nB\nA.f1 = B.f1\nGrace\nHash\nA.f2 = C.f1\nA.f3 = D.f1\n11. [6 points]: To help Jenny understand the Kabra and DeWitt approach, describe one way in which\nit might adapt to each of the following issues that arise during execution of the query shown above:\n(Write your answer in the space below each issue.)\nA. After running Join 3, the system observes that many fewer tuples will be input to the sort-based aggre\ngate than predicted by the optimizer.\nIf sufficient memory has been allocated to the aggregate node, it might switch the aggregate to a hash-\nbased aggregate (or in-memory sort).\nB. After running Join 1, the system observes that many fewer tuples will be input to Join 2 than predicted\nby the optimizer.\nIf sufficient memory has been allocated to the sort-merge join (Join 2), it might switch the join to an\nin-memory hash join. Switching Join 2 and Join 3 is likely not a good idea as the cardinality of the\ninput to Join 2 has gone down, not up, so it is unlikely that Join 2 will now be more expensive than Join\n3.\nC. After running Join 1, the optimizer predicts that many more tuples will be output by Join 2 than pre\ndicted before Join 1 ran (you may assume other optimizer estimates do not change significantly).\nIf the cost of Join 2 is now predicted to be greater than Join 3, the system might switch Join 2 and Join\n3.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 10 of 11\nNow Jenny decides to add even more adaptivity to her system by allowing it to adapt between using secondary\n(unclustered) indices and sequential scans during query processing.\nHer query optimizer uses the following simple approximate cost model to estimate the time to perform a\nrange scan of T tuples from a table A containing |A| disk pages, where the disk can read m pages per second\nand takes s seconds per seek. Assume that a disk page (either from the index or heap file) holds t tuples and\nthat the scan is over a contiguous range of the key attribute of the index.\nFor sequential scans, each range scan requires one seek to the beginning of the heap file plus a scan of the\nentire file:\nCseqscan = s + |\nm\nA|\nFor a secondary (unclustered) B+Tree, each range scan involves reading t√ó\nT\nm leaf index pages (assuming the\ntop levels of the index are cached) plus one seek and one page read from the heap file per tuple (assuming no\nheap file pages are cached).\nCsecondary-btreescan = t√ó\nT\nm + T √ó (s + 1 )\nm\nJenny finds that, when there are correlations between the key of an secondary B+Tree and the key of a\nclustered B+Tree (or the sort order of a table), the performance of the secondary B+Tree is sometimes much\nbetter than the cost formula given above.\n12. [6 points]: Explain why correlations between secondary and clustered index keys might overesti\nmate the cost of the Csecondary-btreescan model given above.\n(Write your answer in the space below.)\nIf there is a correlation between the secondary and clustered index, then consecutive tuples in the leaf\npages of the secondary index may be consecutive (or nearly consecutive) in the clustered index. This\nwill means that in reality there won't be seeks or page reads for every tuple, as in the model.\n13. [6 points]: Using the parameters given above, give a better approximation of Csecondary-btreescan in\nthe presence of correlations.\n(Write your answer in the space below.)\nIf there is a perfect correlation, and assuming the cost model above for scanning the secondary index\nleaf pages is correct, then the cost will be approximately t√ó\nT\nm + T\nt √ó (s + m\n1 ). We didvide T by t because\nthere are t tuples per page, and we only have to seek and read additional pages from the clustered index\nevery t tuples.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 11 of 11\nInspired by the idea of adaptive query processing, Jenny wants to devise a technique to exploit her observation\nabout correlations without having to maintain correlation statistics between the clustered attribute and all\nattributes over which there is an secondary index.\n14. [12 points]: Suggest an adaptive query processing technique that changes the plan while it is\nrunning to switch between a sequential scan and a (possibly correlated) secondary index, depending\non which is better. Your solution should not require precomputing any additional statistics. Be sure to\nindicate how your approach avoids producing duplicate results when it switches plans.\n(Write your answer in the space below.)\nStart using secondary indices, assuming correlations are present.\nIf the number of seeks performed while scanning some small fraction of the range (e.g., the first 1%) is\nclose to what the original secondary model predicts, switch to sequential scan if the overall cost will be\nlower.\nAvoid duplicates by keeping a list of secondary index key values that the system has already scanned,\nand filtering out tuples in that list. Since the scan is done in order of the secondary index key, this list\nbe efficiently represented as a range of keys (e.g., a (lower bound, upper bound) pair).\nEnd of Quiz II\nName:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT6_830F10_quiz02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/9d9aaba7f4cfcf0607dc8893b7a839c3_MIT6_830F10_quiz02.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.830 Database Systems: Fall 2008 Quiz II\nThere are 14 questions and 11 pages in this quiz booklet. To receive credit for a question, answer\nit according to the instructions given. You can receive partial credit on questions. You have 80\nminutes to answer the questions.\nWrite your name on this cover sheet AND at the bottom of each page of this booklet.\nSome questions may be harder than others. Attack them in the order that allows you to make\nthe most progress. If you find a question ambiguous, be sure to write down any assumptions you\nmake. Be neat. If we can't understand your answer, we can't give you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES QUIZ.\nNO PHONES, NO LAPTOPS, NO PDAS, ETC.\nYOU MAY USE A CALCULATOR.\nDo not write in the boxes below\n1-3 (xx/14)\n4-6 (xx/26)\n7-8 (xx/10)\n9-11 (xx/26)\n12-14 (xx/24)\nTotal (xx/100)\nName: 2008 Quiz 2\n\nI\n6.830 Fall 2008, Quiz 2\nPage 2 of 11\nShort Answer\n1. [4 points]: List two reasons why a column-oriented design is advantageous in a data warehouse\nenvironment.\n(Write your answer in the spaces below.)\nA.\nB.\n2. [4 points]:\nWhich of the following statements about the MapReduce system are true:\n(Circle T or F for each statement.)\nT\nF\nA MapReduce job will complete even if one of the worker nodes fails.\nT\nF\nA MapReduce job will complete even if the master node fails.\nT\nF\nMap workers send their results directly to reduce workers.\nT\nF\nIf there are M map tasks, using more than M map workers may improve MapReduce performance.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 3 of 11\n3. [6 points]:\nWhich of the following statements about the two-phase commit (2PC) protocol are true:\n(Circle T or F for each statement.)\nT\nF\nIn the \"no information\" case (i.e., when a subordinate asks the coordinator about the outcome of a\ntransaction that the coordinator has no information about), the coordinator running the presumed abort\nversion of 2PC returns the same response to a subordinate as it would in basic 2PC.\nT\nF\nThe Presumed Commit protocol reduces the number of forced writes required by the coordinator com\npared to basic 2PC for transactions that successfully commit.\nT\nF\nThe Presumed Abort protocol reduces the number of forced writes required of the coordinator compared\nto basic 2PC for transactions that abort.\nF\nF\nIn basic 2PC, a subordinate that votes to abort a transaction T receives no more messages pertaining to\nT , assuming neither T 's coordinator nor any of T 's subordinate nodes fail.\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 4 of 11\nII C-Store\nDana Bass is trying to understand the performance difference between row- and column- oriented databases.\nHer computer's disk can perform sequential I/O at 20 MB/sec, and takes 10 ms per seek; it has 10 MB of\nmemory. She is performing range queries on a table with 10 4-byte integer columns, and 100 million rows.\nData is not compressed in either system, and all columns are sorted in the same order. Assume each column\nis laid out completely sequentially on disk, as is the row-oriented table.\n4. [16 points]: Fill in the table below with your estimates of the minimal I/O time for each operation,\nin a row-store and a column-store. Assume that all reads are on contiguous groups of rows, and that\nqueries output row-oriented tuples.\nNum. Columns\nRead\nNum. Rows\nRead\nColumn-Oriented System\nRow-Oriented System\n10 Million\n10 Million\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 5 of 11\nIII BigTable\nConsider an example table storing bank accounts in Google's BigTable system:\nKey\nValue\n\"adam\"\n$300\n\"evan\"\n$600\n\"sam\"\n$9000\nThe bank programmer implements the deposit function as follows:\nfunction deposit(account, amount):\ncurrentBalance = bigtable.get(account)\ncurrentBalance += amount\nbigtable.put(account, currentBalance)\n5. [4 points]: Given the table above, a user executes deposit(\"sam\", $500). The program crashes\nsomewhere during execution. What are the possible states of the \"sam\" row in the table? Provide a\nshort explanation for your answer.\n(Write your answer in the space below.)\n6. [6 points]: Given the table above, a user executes deposit(\"sam\", $500). The program completes.\nImmediately after the program completes, the power goes out in the BigTable data center, and all the\nmachines reboot. When BigTable recovers, what are the possible states of the \"sam\" row in the table?\nProvide a short explanation for your answer.\n(Write your answer in the space below.)\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 6 of 11\nNow consider the following pair of functions:\nfunction transfer(sourceAccount, destinationAccount, amount):\nfunction totalAssets():\nsourceBalance = bigtable.get(sourceAccount)\nassets = 0\nif sourceBalance < amount:\nfor account, balance in bigtable:\nraise Exception(\"not enough money to transfer\")\nassets += balance\nsourceBalance -= amount\nreturn assets\ndestinationBalance = bigtable.get(destinationAccount)\ndestinationBalance += amount\nbigtable.put(sourceAccount, sourceBalance)\nbigtable.put(destinationAccount, destinationBalance)\n7. [6 points]: Given the table above, a program executes transfer(\"evan\", \"adam\", 200). While that\nfunction is executing, another program executes totalAssets(). What return values from totalAssets()\nare possible? For the purposes of this question, assume there is only one version of each value, and that\nthe values in BigTable are stored and processed in the order given above (i.e., \"adam\", \"evan\", \"sam\").\nProvide a short explanation for your answer.\n(Write your answer in the space below.)\n8. [4 points]: If the data were stored in a traditional relational database system using strict two-phase\nlocking, and transfer() and totalAssets() were each implemented as single transactions, what would the\npossible results of totalAssets() be? Provide a short explanation for your answer.\n(Write your answer in the space below.)\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 7 of 11\nIV Distributed Query Execution\nSuppose you have a database with two tables with the following schema:\nT1 : (A int PRIMARY KEY, B int, C varchar)\nT2 : (D int REFERENCES T1.A, E varchar)\nAnd suppose you are running the following query:\nSELECT T1.A,COUNT(*)\nFROM T1,T2\nAND T1.A = T2.D\nAND T1.B > n\nGROUP BY T1.A\nYour system has the following configuration:\nParameter\nDescription\nValue\n|T1|\n|T2|\n|M|\n||T1||\n|int|\nT\nSize of table T1\nSize of table T2\nSize of memory of a single node\nNumber of records in T 1\nSize of integer, in bytes\nDisk throughput, in bytes / sec\n300 MB\n600 MB\n100 MB\n10 million\n20 MB/sec\nN\nNetwork transmit rate, in bytes / sec\n10 MB/sec\nf\nFraction of tuples selected by T1.B > n predicate\n0.5\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 8 of 11\n9. [9 points]: Suppose you run the query on a single node. Assuming there are no indices, and the\ntables are not sorted, indicate what join algorithm (of the algorithms we have studied in class) would be\nmost efficient, and estimate the time to process the query, ignoring disk seeks and CPU costs. Include a\nbrief justification or calculation.\n(Write your answers in the spaces below.)\nJoin algorithm:\nEstimated time, in seconds:\n10. [10 points]: Now suppose you switch to a cluster of 5 machines. If the above query is the only\nquery running in the system, describe a partitioning for the tables and join algorithm that will provide\nthe best performance, and estimate the total processing time (again, ignoring disk seeks and CPU cost).\nYou may assume about the same number of T2.D values join with each T1.A value. Include a brief\njustification or calculation. You can assume the coordinator can receive an aggregate 50 MB/sec over\nthe network from the five workers transmitting simultaneously at 10 MB/sec.\n(Write your answers in the spaces below.)\nPartitioning strategy:\nJoin algorithm:\nEstimated time, in seconds:\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 9 of 11\nV Adaptive Query Processing\nJenny Join, after hearing the lecture about adaptive query processing, decides to add several types of adaptivity\nto her database system.\nFirst, Jenny decides to add simple plan-based adaptivity in the style of the Kabra and DeWitt paper mentioned\nin the \"Adaptive Query Processing Through the Looking Glass\" paper and discussed in class.\nJenny runs the following query:\nD\nœÉ\nGrace\nHash\nSort-based\nAggregate\nC\nGrace\nHash\nA\nB\nA.f1 = B.f1\nGrace\nHash\nA.f2 = C.f1\nA.f3 = D.f1\n11. [6 points]: To help Jenny understand the Kabra and DeWitt approach, describe one way in which\nit might adapt to each of the following issues that arise during execution of the query shown above:\n(Write your answer in the space below each issue.)\nA. After running Join 3, the system observes that many fewer tuples will be input to the sort-based aggre\ngate than predicted by the optimizer.\nB. After running Join 1, the system observes that many fewer tuples will be input to Join 2 than predicted\nby the optimizer.\nC. After running Join 1, the optimizer predicts that many more tuples will be output by Join 2 than pre\ndicted before Join 1 ran (you may assume other optimizer estimates do not change significantly).\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 10 of 11\nNow Jenny decides to add even more adaptivity to her system by allowing it to adapt between using secondary\n(unclustered) indices and sequential scans during query processing.\nHer query optimizer uses the following simple approximate cost model to estimate the time to perform a\nrange scan of T tuples from a table A containing |A| disk pages, where the disk can read m pages per second\nand takes s seconds per seek. Assume that a disk page (either from the index or heap file) holds t tuples and\nthat the scan is over a contiguous range of the key attribute of the index.\nFor sequential scans, each range scan requires one seek to the beginning of the heap file plus a scan of the\nentire file:\nCseqscan = s + |\nm\nA|\nFor a secondary (unclustered) B+Tree, each range scan involves reading t√ó\nT\nm leaf index pages (assuming the\ntop levels of the index are cached) plus one seek and one page read from the heap file per tuple (assuming no\nheap file pages are cached).\nCsecondary-btreescan = t√ó\nT\nm + T √ó (s + m\n1 )\nJenny finds that, when there are correlations between the key of an secondary B+Tree and the key of a\nclustered B+Tree (or the sort order of a table), the performance of the secondary B+Tree is sometimes much\nbetter than the cost formula given above.\n12. [6 points]: Explain why correlations between secondary and clustered index keys might overesti\nmate the cost of the Csecondary-btreescan model given above.\n(Write your answer in the space below.)\n13. [6 points]: Using the parameters given above, give a better approximation of Csecondary-btreescan in\nthe presence of correlations.\n(Write your answer in the space below.)\nName:\n\n6.830 Fall 2008, Quiz 2\nPage 11 of 11\nInspired by the idea of adaptive query processing, Jenny wants to devise a technique to exploit her observation\nabout correlations without having to maintain correlation statistics between the clustered attribute and all\nattributes over which there is an secondary index.\n14. [12 points]: Suggest an adaptive query processing technique that changes the plan while it is\nrunning to switch between a sequential scan and a (possibly correlated) secondary index, depending\non which is better. Your solution should not require precomputing any additional statistics. Be sure to\nindicate how your approach avoids producing duplicate results when it switches plans.\n(Write your answer in the space below.)\nEnd of Quiz II\nName:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/ce269ab3b1fe758df40aec6cad51371e_MIT6_830F10_lec01.pdf",
      "content": "6.830/6.814 -- Notes‚àó for Lecture 1:\nIntroduction to Database Systems\nCarlo A. Curino\nSeptember 10, 2010\nIntroduction\nREADING MATERIAL: Ramakrishnan and Gehrke Chapter 1\nWhat is a database?\nA database is a collection of structured data. A\ndatabase captures an abstract representation of the domain of an application.\nTypically organized as \"records\" (traditionally, large numbers, on disk)\n-\nand relationships between records\n-\nThis class is about database management systems (DBMS): systems for cre\nating, manipulating, accessing a database.\nA DBMS is a (usually complex) piece of software that sits in front of a\ncollection of data, and mediates applications accesses to the data, guaranteeing\nmany properties about the data and the accesses.\nWhy should you care? There are lots of applications that we don't offer\nclasses on at MIT. Why are databases any different?\n\nDB\n\"a collection of structure data\"\nDBMS\n\"a system to create,\nmanipulate, access databases\n(mediate access to the data)\"\nAPP1\nAPP2\nFigure 1: What is a database management system?\n- Ubiquity (anywhere from your smartphone to Wikipedia)\n- Real world impact: software market (roughly same size as OS market\nroughly $20B/y). Web sites, big companies, scientific projects, all manage\nboth day to day operations as well as business intelligence + data mining.\n- You need to know about databases if you want to be happy!\nThe goal of a DBMS is to simplify the storing and accessing of data. To\nthis purpose DBMSs provide facilities that serve the most common operations\nperformed on data. The database community has devoted significant effort in\nformalizing few key concepts that most applications exploit to manipulate data.\nThis provides a formal ground for us to discuss the application requirements\non data storage and access, and compare ways for the DBMS to meet such\nrequirements. This will provide you with powerful conceptual tools that go\nbeyond the specific topics we tackle in this class, and are of general use for any\napplication that needs to deal with data.\nNow we proceed in showing an example, and show how hard is doing things\nwithout a DB, later we will introduce formal DB concepts and show how much\neasier things are using a DB.\nMafia Example\nToday we cover the user perspective, trying to detail the many reason we want\nto use a DBMS rather than organizing and accessing data directly, for example\nas files.\nLet us assume I am a Mafia Boss (Note: despite the accent this is not the\ncase, but only hypothetical!) and I want to organize my group of \"picciotti\"\n(sicilian for the criminals/bad guys working for the boss, a.k.a the soldiers, see\nFigure 2) to achieve more efficiency in all our operations. I will also need a\nlot of book-keeping, security/privacy etc.. Note that my organization is very\n\nFigure 2: Mafia hierarchy.\nlarge, so there is quite a bit of things going on at any moment (i.e., many people\naccessing the database to record or read information).\nI need to store information about:\n- people that work for me (soldiers, caporegime, etc..)\n- organizations I do business with (police, 'Ndrangheta, politicians)\n- completed and open operations:\n- protection rackets\n- arms trafficking\n- drug trafficking\n- loan sharking\n- control of contracting/politics\n- I need to avoid that any of may man is involved in burglary, mugging,\nkidnapping (too much police attention)\n- cover-up operations/businesses\n- money laundry and funds tracking\n- assignment of soldiers to operations\netc...\n-\nI will need to share some of this information with external organizations I\nwork with, protecting some of the information.\nTherefore I need:\n- the boss, underboss and consigliere should be able to access all the data\nand do any kind of operations (assign soldiers to operations, create or\nshutdown operations, pay cops, check the total state of money movements,\netc...)\n- the accountants (20 of them) access to perform money book-keeping (track\nmoney laundering operations, move money from bank to bank, report\nbribing expenses)\nDiagram of\nMafia hier\narchy.\nImage by MIT OpenCourseWare.\n\n- the soldiers (5000) need to report daily misdeeds in a daily-log, and report\nmoney expenses and collections\n- the semi-public interface accessible by other bosses I collaborate with\n(search for cops on our books, check areas we already cover, etc..)\nperson\norganization\nlog\noperation\naccounts\nname\nnickname\nphone\nlog_id\nauthor\ntitle\nsummary\nname\ndesc\n$$\ncoverup-name\ninvolve\ncollaboration_with\nname\nboss\nrank\naccount-number\nfalse-identity\nbalance\nFigure 3: What data to store in my Mafia database.\n3.1\nAn offer you cannot refuse\nI make you an offer you cannot refuse: \"you are hired to create my Mafia\nInformation System, if you get it right you will have money, sexy cars, and a\ngreat life. If you get it wrong... well you don't want to get it wrong\".\nAs a first attempt, you think about just using a file system:\n1. What to represent:, what are the key entities in the real world I need\nto represent? how many details?\n2. How to store data: maybe we can use just files: people.txt, organiza\ntions.txt, operations.txt, money.txt, daily-log.txt. Each files contains a\ntextual representation of the information with one item per line.\n3. Control access credentials at low granularity: accountants should\nknow about money movement, but not the names and addresses of our\nsoldiers. Soldiers should know about operations, but not access money\ninformation\n4. How to access data: we could write a separate procedural program\nopening one or more files, scanning through them and reading/writing\ninformation in them.\n5. Access patterns and performance: how to find shop we didn't col\nlected money from for the longest time (and at least 1 month)? scan the\nhuge operation file, sort by time, pick the oldest, measure time? (need to\nbe timely or they will stop paying, and this get the boss mad... you surely\n\ndon't want that, and make sure no one is accessing it right now). \"Tony\nSchifezza\" is a mole, we need to find all the operations and people he was\ninvolved or knew about and shut them down... quick... like REAL quick!!!\n6. Atomicity: when an accountant moves money from one place to another\nyou need to guarantee that either money are removed from account A and\nadded to account B, or nothing at all happens... (You do not want to have\nmoney vanishing, unless you plan to vanish too!).\n7. Consistency: guarantee that the data are always in a valid state (e.g.,\nthere are no two operations with the same name)\n8. Isolation: multiple soldiers need to add to daily-log.txt at the same time\n(risk is that they override each other work, and someone get \"fired\" be\ncause not productive!!)\n9. Durability: in case of a computer crash we need to make sure we don't\nlose any data, nor that data get scrambled (e.g., If the system says the\npayment of a cop went through, we must guarantee that after reboot the\noperation will be present in the system and completed. The risk is police\ntaking down our operation!)\nUsing the file system, you realize that most probably you will fail, and that\ncan be very dangerous... Luckily you are enrolled in 6.830/6.814 and you just\nlearned that: Databases address all of these issues!! you might have a chance!\nIn fact, you might notice that the issues listed above are already related to the\nthree concepts we mentioned before: 1-3 are problems related to Data Model,\n4-5 are problems related to the Query language and 6-9 are problems related to\nTransactions.\nSo let's try to do the same with a \"database\" and get the boss what he needs.\n3.2\nMore on fundamental concepts\nDatabase are a microcosm of computer science, their study covers: languages,\ntheory, operating systems, concurrent programming, user interfaces, optimiza\ntion, algorithms, artificial intelligence, system design, parallel and distributed\nsystems, statistical techniques, dynamic programming. Some of the key con\ncepts we will investigate are:\nRepresenting Data We need a consistent structured way to represent data,\nthis is important for consistency, sharing, efficiency of access. From database\ntheory we have the right concepts.\n- Data Model: a set of constructs (or a paradigm) to describe the organiza\ntion of data. For example tables (or more precisely relations), but we could\nalso choose graph, hierarchies, objects, triples <subject,predicate,object>,\netc..\n\n- Conceptual/Logical Schema: is a description of a particular collection\nof data, using the a given data model (e.g., the schema of our Mafia\ndatabase).\n- Physical Schema: is the physical organization of the data (e.g., data and\nindex files on disk for our Mafia database).\nDeclarative Querying and Query Processing a high-level (typically declar\native) language to describe operations on data (e.g., queries, updates). The goal\nis to guarantee Data independence (logical and physical), by separating \"what\"\nyou want to do with data from \"how\" to achieve that (more later).\n- High level language for accessing data\n- \"Data Independence\" (logical and physical)\n- Optimization Techniques for efficiently accessing data\nTransactions\n- a way to group actions that must happen atomically (all or nothing)\n- guarantees to move the DB content from a consistent state to another\n- isolate from parallel execution of other actions/transactions\n- recoverable in case of failure (e.g., power goes out)\nThis provide the application with guarantees about a group of actions even\nin presence of concurrency and failures. It is a unit of access and manipulation\nof data. And significantly simplify the work of application developers.\nThis course covers these concepts, and goes deep into the investigation of\nhow modern DBMS are designed to achieve all that. We will not cover the more\nartificial-inteligence / statistical / mining related areas that are also part of\ndatabase research. Instead, we will explore some of the recent advanced topics\nin database research--see class schedule to get an idea of the topics.\n3.3\nBack to our Mafia database\nWhat features of our organization shall we store? How do we want to capture\nthem? Choose a level of abstraction and describe only the relevant details (e.g.,\nI don't care about favorite movies for my soldiers, but I need to store their\nphone numbers). Let's focus on a subset:\n- each person has real name, nickname, phone number\n- each operation has a name, description, economical value, cover-up name\n- info about the persons involved in an operation and their role,\n\nWe could represent this data according to many different data models:\nhierarchies\n-\n- objects\n- graph\n- triples\netc..\n-\nLet's try using an XML hierarchical file:\n<person>\n<name> </name>\n<nickname> </nickname>\n<phone> </phone>\n<operation>\n<op_name> </op_name>\n<description> </description>\n<econ_value> </econ_value>\n<coverup_name> </coverup_name>\n</operation>\n</person>\nOperations are duplicated in each person, this might make the update very\ntricky (inconsistencies) and the representation very verbose and redundant.\nOtherwise we can organize the other way around with people inside operations,\nwell we would have people replicated.\nAnother possibility is using a graph structure with people, names, nick\nnames,phones, operation names etc.. as nodes, and edges to represent relation\nships between them. Or we could have objects and methods on them, or triples\nlike <carlo,is a,person>, <carlo,phone,5554348882> etc..\nDifferent data models are more suited for different problems.\nThey different expressive power and different strengths depending on what\ndata you want to represent and how you need to access them.\nLet's choose the relational data model and represent this problem using \"ta\nbles\". Again there are many ways to structure the representation, i.e., different\n\"conceptual/logical schemas\" that could capture the reality are modeling. For\nexample we can have a single big table with all info together... again, is redun\ndant and might slow down all the access to data.\nThe \"database design\" is the art of capturing a set of real world concepts\nand their relations in the best possible organization in a database. A good\nrepresentation is shown in Figure 4. It is not redundant and contains all the\ninformation we care about.\n\nshifezza\ntony\nlungo\nmike\nbaffo\ncarlo\nname\nnickname\nphone\nperson\noperation\n..\nlaundromat\nirish pub\nirish pub\nchocolate\nsnowflake\ncoverup\ncaffe\n$10M\necon_val\ntitle\ndescr.\n...\n$2M\n...\n$5M\nchocolate\nmike\nchief\nsnowflake\nsold\ntony\ncarlo\nsnowflake\nchief\npers_name\noper_name\nrols\ninvolved\nFigure 4: Simple Logical Schema for a portion of our Mafia database.\nWhat about the physical organization of the data? As a database user you\ncan ignore the problem, thanks to the physical independence! As a student of\nthis class you will devote a lot of effort in learning how to best organize data\nphysically to provide great performance to access data.\n3.4\nAccessing the data (transactionally)\nAs we introduced before databases provide high-level declarative query lan\nguages. The key idea is that you describe \"what\" you want to access, rather\nthan \"how\" to access it.\nLet's consider the following operations you want to do on data, and how we\ncan represent them using the standard relational query language SQL:\n- Which operations involve \"Tony Schifezza\"?\nSELECT oper_name\nFROM involved\nWHERE person = \"tony\";\n- Given the \"laundromat\" operation, get the phone numbers of all the people\ninvolved in operations using it as a cover up.\nSELECT p.phone\nFROM\nperson p, operation o, involve i\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\no.coverup_name = \"laundromat\";\n- Reassign Tony's operations to Sam and remove Tony from the database\n(he was the mole).\nBEGIN\nUPDATE involved i SET pers_name=\"sam\" WHERE pers_name=\"tony\";\nDELETE FROM person WHERE name = \"tony\";\nCOMMIT\n\n- Create a new operation with \"Sam Astuto\" in charge of it.\nBEGIN\nINSERT INTO operation VALUES ('newop1','',0,'Sam's bakery');\nINSERT INTO involve VALUES ('newop1','sam','chief');\nCOMMIT\nLet us reconsider the procedural approach. You might organize data into\nfiles: one record of each table in a file, and maybe sort the data by one of the\nfields. Now every different access to the data, i.e., every \"query\" should become\na different program opening the files, scanning them, reading or writing certain\nfields, saving the files.\nExtras\nThe two following concepts have been broadly mentioned but not discussed in\ndetails in class.\nOptimization The goal of a DBMS is to provide a library of sophisticated\ntechniques and strategy to store, access, update data that also guarantees per\nformance, atomicity, consistency, isolation, durability. DBMS automatically\ncompile the user declarative queries into an execution plan (i.e., a strategy that\napplies various steps to achieve the compute the user queries), looks for equiv\nalent but more efficient ways to obtain the same result query optimization, and\nexecute it, see example in Figure 5.\nscan(person)\nscan(involved)\nscan(operations)\nproduct\nproduct\nfilter(p.name=i.person)\nfilter(i.oper_name=o.name)\nfilter(o.coverup=\"laundromat\")\nproject(p.phone)\nscan(person)\nscan(involved)\nlookup(operations, coverup=\"laundromat\")\nproduct\nproduct\nfilter(p.name=i.person)\nproject(p.phone)\nfilter(i.oper_name=o.name)\nproject(p.name,p.phone)\nproject(i.oper_name, i.person)\nproject(o.name)\nBASIC PLAN\nOPTIMIZED PLAN\nFigure 5: Two equivalent execution plan, a basic and an optimized one.\n\nExternal schema A set of views over the logical schema, that predicates how\nusers see/access data. (e.g., a set of views for the accountants). It is often not\nphysically materialized, but maintain as a view/query on top of the data.\nLet try to show only coverup names of operations worth less or equal to $5M\nand the nicknames of all people involved using a view (see Figure 6):\nCREATE VIEW nick-cover AS\nSELECT\nnickname, coverup_name\nFROM operation o, involved i, person p\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\no.econ_val <= 5M;\nschifezza\nlaundromat\nlungo\nirish pub\nbaffo\nlaundromat\nnickname\ncoverup\nnick-cover\nFigure 6: Simple External Schema for a portion of our Mafia database.\nWhat's next?\nNext week lessons introduce more formally the relational model (and some of\nits history) and how to design the schema of a database. After that we will dive\ninto the DBMS internals and study \"how\" DBMS are internally architected\nto achieve all the functionalities we discussed. Later on we will study how to\nguarantee transactional behaviors, and how to scale a DBMS beyond a single\nnode. The last portion of the course is devoted to more esoteric topics from\nrecent advances in database research.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/8d7652b95f032eab2dcd6320d8a9f1c7_MIT6_830F10_lec02.pdf",
      "content": "3 data models from the 1970s\nHierarchical (IMS + DL/1)\nNetwork (Codasyl + Codasyl DML)\nRelations (Codd proposal + DL/alpha\nRelational algebra\nSQL\nQuel)\nThemes:\nData redundancy\nPhysical data independence\nLogical data independence\nHigh level language\nWhy study ancient history?\n\"Those that do not understand the mistakes of their ancestors will end up\nrepeating them\"\nUse Zoo example (with one more kind of object)\n3 objects\n2 relationships\nAnimals (Name, species, age, feeding_time)-------------------\n| lives_in\n|\nCages (id, size) ----------------------\n| cared_for_by\n|\nKeepers (Name, address)<-----------------------------------------\nEach animal in ONE cage, multiple animals can share a cage\nEach animal cared for by ONE keeper, a keeper cares for multiple animals\nIMS (IBM 1968)\nSegment types (record types)\nSegments (instances)\nSchema (hierarchical collection of segment types - must be a tree)\nPossible schemas\n\nKeepers\nKeepers\n1)\n|\n2)\n|\nAnimals\nCages\n|\n|\nCages\nAnimals\nInstance of 1)\nSam\nFreddie\nJimmy\nSally\nAll have redundancy!\n1) repeat cage info for animals which share a cage\n2) repeat cage info for animals in a shared cage with different keepers\nBad: possibility of inconsistency\nFundamental problem:\nKeepers\nCages\n|\n|\nAnimals\nCannot be represented as a hierarchy!\nIMS Storage\nRoot\ndependents\n*****\n*********\nSequential\nSequential\nIndex\nSequential\nHash\nPointer spaghetti\nIndex\nPointer spaghetti\nNote: no indexes on dependent segments!\nDL/1\n\nEvery segment has a hierarchical sequential key (HSK)\nKey of the segment, prepended by keys of path back to the root\nAll segments are logically in HSK order (for purposes of DL/1)\nCommands\nGU [segment type] [predicate]\nGN\nGNP\nD\nI\nFind all cages that Sam enters:\nGU Keepers (name = 'Sam')\nUntil no more\nGNP Cages\nFind the keepers that enter cage 6\nGU Keepers\nGNP Cages (id = 6)\nUntil no more\nGN Keepers\nGNP Cages (id = 6)\nNotes: GU is really get first\nSome commands are fast; some are slow; depends on the storage chosen and the schema\nchosen. IMS wizards make gobs of money; even today.\nIMS problems:\na) duplication of data (we talked about this above)\nb) painful low level programming interface - have to program the search algorithm\nc) limited physical data independence\nchange root from indexed to hash --- programs that do GN on the root segment\nwill fail\n\ncannot do inserts into sequential root structure\nmaintenance required if you change the tuning knobs! Hard to tell how bad it will\nbe.\nOne of Codd's key points!!! This is terrible system design\nd) limited logical data independence\nZoo acquires a pair of Pandas - they have 2 feeding times!\nKeepers\n|\nAnimals\n|\n|\nFeedings\nCages\nMust change the schema!\nAs a cost cutting measure, Zoo management decides a keeper will be responsible for a\ncage - and all the animals in that cage.\nKeepers\n|\nCages\n|\nAnimals\nShould change the schema to match the business problem\nManagement decides to have \"patrons\" who buy cages\nKeepers\n|\nAnimals\n|\nCages\n|\nPatrons\nSchemas change for all these reasons - plus\nFeds change the rules (OSHA)\nTax rules change (IRS)\nMerge with another zoo\n\nWhenever the logical schema changes - you need program maintenance - unknown\ncomplexity!! In the worst case, toss everything; begin again!!\nThis was what was motivating Codd -\nCodasyl (Committee on Data Systems Languages)\nBF Goodrich (the guys without the blimp) built a prototype\nCommercialized by Cullinet Corp. as IDS\nCodasyl committee wrote standardization documents closely linked to IDS\nRecord types (like IMS)\nConnected by named Sets (1::n relationships)\nArranged in a graph\nKeepers\nCages\n|\n|\nCared_for_by |\n|\nLives_in\n\\/\n\\/\nAnimals\nModels network data directly. Ought to be better than IMS.\nRecord types\n1) hashed\n2) clustered with the owner record in some set\nPointer spaghetti for set implementation\nIf a record is not hashed, then it can only be accessed through set membership\n.\nCodasyl DML\nFind the cages that Sam enters\nFind Keepers (name = 'Sam')\nUntil nomore\nFind next Animal in Cared_for_by\nFind parent in Lives_in\nIMS has current segment; current parent ( 2 pins)\nCodasyl has\n\nCurrent of app\nCurrent of every record type\nCurrent of every set type\n(6 pins)\nProgramming is\nFind an entry point\nNavigate in N-D space\nFor a defense of this programming style see 1973 Turing award lecture by Charlie\nBachmann.\nCodasyl issues\nHorrible complexity.\nNo physical data independence - change most anything recode\nNo logical data independence - change most anything -> recode\nIf you screw up the data structure inadvertently, then must reload everything. (No\nisolation)\nInitial load must be done all at once. - many hours.\nCodd: relations\nUnordered collections of tuples\nAnimals (name, species, age, feeding_time)\nCages (id, size)\nKeepers (id, name, address)\n**************************************\nAnimals (name, species, age, feeding_time, cid, kid)\nCages (id, size)\nKeepers (id, name, address)\nOr\nAnimals (name, species, age, feeding_time)\nCages (id, size)\nKeepers (id, name, address)\nLives_in (aname, cid)\nCared_for_by (aname, kid)\n\nData base design problem - which one to choose\nHigh level language for access (physical data independence)\nWhat you want not how to get it\nCodd's proposals (he did cellular automata previously)\nData language alpha (basically 1st order predicate calculus)\nRelational algebra (a collection of operations chained together - APL style)\nNo mere mortals could understand either language\nSQL (and Quel) were much more accessible\nFind the cages that Sam touches\nSelect cid\nFrom Animals\nWhere zid in\nSelect id\nFrom Keepers\nWhere name = 'Sam'\nComplete physical data independence\nEliminates redundancy\nBetter change at logical data independence (views)\nDefine view (Sam_Cages) as\nSelect cid\nFrom Animals\nWhere zid in\nSelect id\nFrom Keepers\nWhere name = 'Sam'\nSelect ...\nFrom Sam_Cages\nWhere ...\nAll queries and many updates can be supported on views. Interested reader is referred to\nSIGMOD 1976 and a litany of papers that have followed.\nDebate raged throughout the 1970's over:\n\nEfficiency\nProgrammability of high level languages\nCobol\nWon hands down by relational model.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/a9c47ed7d80c1bfc51d0baf7ac545398_MIT6_830F10_lec03.pdf",
      "content": "Legacy - SQL is a terrible language -- Date paper in 1985\nAnimals (name, species, age, feeding_time, cid, kid)\nCages (id, size)\nKeepers (id, name, address)\nOriginal idea (1974)\nBlock\nBlock\nBlock\ne.g. find the name of Freddies's keeper\nselect name\nfrom keepers\nwhere id =\nselect kid\nfrom Animals\nwhere name = \"Freddie\"\nEvaluation from inside-out\nFirst: this may not be the best plan\nSecond: not powerful enough\nFind animals older than Freddie who share the same cage\nSelect name\nFrom animals A\nWhere age >\nSelect age\nFrom animals\nWhere name = \"Freddie\"\nAnd cid = A.cid\nFind all pairs of animals cared for by the same keeper\nSelect A.name, B.name\nFrom Animals A\nWhere a.zid =\nSelect B. zid\n\nFrom Animals B\nWhere B.name != A.name\nRequires refs from inside out and outside in\nNo obvious query processing strategy disallowed\nOkay for an inner to outer reference, but not the other way around.\nSQL solution (1976) -- multi-table blocks\nSelect A.name, B.name\nFrom Aminals A, Animals B\nWhere A.zid = B.zid and A.name != B.name\nNet result: horrible\n2 ways to express most queries, e.g. Freddie's keeper:\n(nested)\nselect name\nfrom keepers\nwhere id in\nselect kid\nfrom Animals\nwhere name = \"Freddie\"\n(flat)\nSelect k.name\nFrom Animals A, Keepers K\nWhere A.kid = K.id and A.name = 'Freddie'\nWhich one to choose?\n1980's:\nHierarchical got you inside-out evaluation\nFlat got you a complete optimization obviously better!\nDon't use hierarchical representation!!!!\nMore recently, SQL engines rewrite hierarchical queries to flat ones, if they can. Big\npain for the implementation!!!\n\nThere are queries that cannot be flattened.\ne.g. ones with an \"=\" between the inner and out block.\nThere are ones that cannot be expressed in nested fashion, e.g. \"pairs of\nAnimals query\" above\nTwo collections of queries: nested and flat: venn diagram almost identical but not quite.\nAwful language design.\nLessons\nNever too late to toss everything and start over -- glueing warts is never a good idea\nSimple is always good\nLanguage design should be done by PL folks not DB folks - we are no good at it\n**************\nOODB (1980s): persistent C++\nMotivation:\nProgramming language world (C++)\nstruct animals {\nstring name;\nstring feed_time;\nstring species;\nint age;\n};\nstruct keeper_data {\nstring name;\nstring address;\nanimals charges[20];\n}; DBobject;\ndata base world:\nKeepers (name, address)\nAnimals (name, feed_time, species, age, k_id)\nThere is an obvious impedance mismatch.\nQuery returns a table - not a struct. You have to convert DB return to data structures of\nclient program.\nOODBs were focused on removing this impedance mismatch.\n\nData model: C++ data structures. Declare them to be persistent:\ne.g. persistent keeper_data DB [500];\nto query data base:\ntemp = DB[1].keeper_data.name\nto do an update:\nDB [3].keeper_data.name = \"Joseph\"\nQuery language: none - use C++ appropriate for CAD -style apps\nAssumes programming language Esperanto (C++ only)\nLater adopted a QL (OQL)\nTransaction systems very weak - big downside\nNever found a sweet spot.\n***********\nSemi-structured data problem\nHobbies of employees\nSam: bicycle (brand, derailer, maximum slope hill, miles/week)\nMike: hike (number of 4000 footers, boot brand, speed)\nBike (builder, frame-size, kind-of-seat)\n\"semi-structured data\"\nNot well suited to RM (or any other model we have talked about so far)\nXML ( hierarchical, self-describing tagged data)\n<employee>\n<name> Sam </name>\n<hobbies>\n<bike>\n<brand> XXX </brand>\n.\n.\n<next hobby>\nXML is anything you want.\nDocument guys proposed this stuff as simplified SGML.\n\nAdapted by DBMS guys. Always a bad idea to morph something to another purpose.\nIf you want a structured collection, they proposed XML-schema\n<employee: schema>\n<employee:element name = \"empname\" type = \"string\"/>\n<employee:complextype name = \"hobbies\", type = \"hobbiestype\"/>\nXML representation for structure of the XML document\nMost complex thing on the planet...\nTables (RM)\nHierarchies (IMS style)\nRefs (Codasyl style)\nSet valued attributes (color = {red, green brown})\nUnion types (value can be an X or a Y)\nXQuery is one of the query languages (with XPath)\nFor $X in Employee\nWhere $x/name = Sam\nReturn $X/hobbies/bike\nHuge language\nRelational elephants have added (well behaved subsets) to their engines\ngetting some traction.\n******8\nData base design\n***************\nAnimals (name, species, age, feeding_time, cid, kid)\nCages (id, size)\nKeepers (id, name, address)\nOr\nAnimals (name, species, age, feeding_time)\nCages (id, size)\nKeepers (id, name, address)\nLives_in (aname, cid)\nCared_for_by (aname, kid)\nData base design problem - which one to choose\n\nTwo ways to data base design\na) Normalization\nb) E-R models\nNormalization: start with an initial collection of tables, (for this example)\nAnimals (name, species, age, cid, cage_size)\nFunctional dependency: for any two collections of columns, second set is determined by\nthe first set; i.e. it is a function.\nWritten A -> B\nIn our example:\n(name) is a key. Everything is FD on this key\nPlus\ncid -> size\nProblems:\n1) redundancy: cage_size repeated for each animal in the case\n2) cannot have an empty cage\nIssue:\nCage_size is functionally dependent on cid which in turn is functionally dependent on\nname. So called transitive dependency.\nSolution normaliziation;\nCage (id, cage_size)\nAnimals (name, species, age, c_id)\n1NF (Codd) -- all tables \"flat\"\n2NF (Codd) -- no transitive dependencies\n3NF (Codd) -- fully functional on key\nBCNF\n4NF\n\n5NF\nP-J NF\n....\nTheoreticians had a field day....\nTotally worthless\n1) mere mortals can't understand FDs\n2) have to have an initial set of tables - how to come up with these?\nUsers are clueless...\nPlus, if you start with:\nAnimals (name, species, age, feeding_time)\nCages (id, size)\nKeepers (id, name, address)\nLives_in (aname, cid)\nCared_for_by (aname, kid)\nNo way to get\nAnimals (name, species, age, feeding_time, cid, kid)\nCages (id, size)\nKeepers (id, name, address)\n******\nUniversal solution:\nE-R model:\nEntities (things with independent existence)\nKeepers\nCages\nAnimals\nEntities have attributes\nEntities have a key\nAnimals have name (key), species, age, ...\nEntities participate in relationships\n\nLives_in\nCared_for_by\nRelationships are 1:1, 1::N or M::N (use crows feet on the arcs to represent visually)\nRelationships can have attributes\nDraw an E-R diagram\nKeepers\nCages\n(id (key), name, address)\n(id (key), size)\n|\n|\n|\n|\n|\n|\n\\/\n\\/\nAnimals\n(name (key), species, age, feeding_time)\nAutomatic algorithm generates 3NF (Wong and Katz 1979)\nEach entity is a table with the key\nM::N relationships are a table with their attributes\n1::N relationships - add the key on the N side to the one side with all of the relationship\nattributes\nGenerates:\nAnimals (name, species, age, feeding_time, cid, kid)\nCages (id, size)\nKeepers (id, name, address)\nOver the years has been extended with\nWeak entities (no key - inherits the key of some other entity) (learn to drive)\nInheritance hierarchies (generalization) (student is a specialization of person)\nAggregation (attribute of all of the participants in a relationship) (e.g count)\nMore than binary relationships (e.g. marriage ceremony)\nDetails in Ramakrishnan.....\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/8d820170818e123c0f078729a6712bc1_MIT6_830F10_lec04.pdf",
      "content": "6.830/6.814 -- Notes‚àó for Lecture 4:\nDatabase Internals Overview\nCarlo A. Curino\nSeptember 22, 2010\nAnnouncements\n- Problem Set 1 is due today! (For next time please submit some open\nnon-esoteric format .txt .rtf .pdf)\n- Lab 1 is out today... start it right away, is due in 8 days! Do not copy...\nwe perform automatic checking of plagiarism... it is not a good gamble!\nProjects ideas and rules are posted online.\n-\nReadings\nFor this class the suggested readings are:\n- Joseph Hellerstein, Michael Stonebraker and James Hamilton. Architec\nture of a Database System. Online at: http://db.cs.berkeley.edu/\npapers/fntdb07-architecture.pdf\nIt is a rather long paper (don't be too scared by the 119 pages, the page\nformat makes it look much longer than it is) that is in general worth reading,\nhowever we only require you too read sections: 1, 2 (skim through it), 3, 4 (up\nto subsection 4.5 included), 5. You can also skim through section 6 that we will\ndiscuss later on. Probably doesn't all make sense right now - you should look\nat this paper again to this paper through the semester for context.\nA bit of history\nComplementing Mike's historical overview...Projects ideas and rules are posted online.\n‚àóThese notes are only meant to be a guide of the topics we touch in class. Future notes\nare likely to be more terse and schematic, and you are required to read/study the papers and\nbook chapters we mention in class, do homeworks and Labs, etc.. etc..\n\n1970's : Several camps of proponents argue about merits of these competing\nsystems while the theory of databases leads to mainstream research projects.\nTwo main prototypes for relational systems were developed during 1974-77.\n- Ingres: Developed at UCB by (including guess who? Stonebraker and\nWong). This ultimately led to Ingres Corp., Sybase, MS SQL Server,\nBritton-Lee, Wang's PACE. This system used QUEL as query language.\n- System R: Developed at IBM San Jose (now Almaden) and led to IBM's\nSQL/DS & DB2, Oracle, HP's Allbase, Tandem's Non-Stop SQL. This\nsystem used SEQUEL as query language (later SQL). Lots of Berkeley\nfolks on the System R team, including Gray (1st CS PhD @ Berkeley),\nBruce Lindsay, Irv Traiger, Paul McJones, Mike Blasgen, Mario Schkol\nnick, Bob Selinger , Bob Yost.\nEarly 80's : commercialization of relational systems\n- Ellison's Oracle beats IBM to market by reading white papers.\n- IBM releases multiple RDBMSs, settles down to DB2. Gray (System\nR), Jerry Held (Ingres) and others join Tandem (Non-Stop SQL), Kapali\nEswaran starts EsVal, which begets HP Allbase and Cullinet\n- Relational Technology Inc (Ingres Corp), Britton-Lee/Sybase, Wang PACE\ngrow out of Ingres group\n- CA releases CA-Universe, a commercialization of Ingres\n- Informix started by Cal alum Roger Sippl (no pedigree to research).\n- Teradata started by some Cal Tech alums, based on proprietary network\ning technology (no pedigree to software research)\nMid 80's :\n- SQL becomes \"intergalactic standard\".\n- DB2 becomes IBM's flagship product.\n1990's:\n- Postgres project at UC Berkeley turned into successful open source project\nby a large community, mostly driven by a group in russia\nIllustra (from Postgres)\nInformix\nIBM\n-\n‚Üí\n‚Üí\n- MySQL\n\n2000's:\nPostgres\nNetezza, Vertica, Greenplum, EnterpriseDB...\n-\n‚Üí\nMySQL\nInfobright\n-\n‚Üí\nIngres\nDATAllegro\n-\n‚Üí\nSystem R is generally considered the more influential of the two - you can\nsee how many of the things they proposed are still in a database system today.\nHowever, Ingres probably had more \"impact\" by virtue of training a bunch of\ngrad students who went on to fund companies + build products (e.g., Berke\nleyDB, Postgres, etc.)\nIntroduction\nFigure 1 shows the general architecture of a database.\nFigure 1: Architecture of a DBMS\nToday we will mainly look at the big picture, and go through the relational\nquery rewriting and execution, the following lessons will focus on each of the\npieces in more details.\nShow flow of a query\nDiagram of d\natabase a\nrchitecture.\nImage by MIT OpenCourseWare.\n\nProcess Models\nParallelism is a key to performance, in particular when I/O waits might stall\ncomputation. To maximize throughput you need to have enough stuff going on\nin parallel to avoid waiting/stalling.\nProcess models:\n- Back in the days there was no good OS thread support, DB pioneered this\nground (also due to the need of supporting many OSs)\n- Process per DBMS worker (need for shared memory [ASK: is it clear why\nwe need to share across multiple workers?], context switch is expensive,\neasy to port, limited scalability)\n- Thread per DBMS worker (great if good OS thread support, or using\nDBMS separate implementation of threads... pro: portability, cons: du\nplicate functionalities)\n- Process/Thread pool, and scheduling/allocation of DBMS workers to pro\ncesses or threads.\nParallel Architecture\n- Shared Memory: typically inside one machine, for large installation high\ncosts. All process models are applicable. Great for OLTP, many imple\nmentation form almost every vendor.\n- Shared Nothing: typically as a cluster of nodes. Require good partitioning,\nwhich is easier for OLAP workloads (Teradata, Greenplum, DB2 Parallel\nEdition,...).\n- Shared Disk: cluster of nodes with a SAN. Simple model, because ev\nery node can access all the data, but requires cache-coherence protocols.\nOracle RAC, DB2 SYSPLEX.\n- NUMA: not that common, we will not discuss. (Often DBMS treat it as\neither shared nothing or shared memory, depending how non-uniform it\nis).\nDifferent failure modes... Partial failure is good to have when possible.\nWe will go back to parallel architectures later on, and dis\nQuery Processing\nQuery parsing (correctness check)\nQuery admission control / authorization\n\n7.1\nQuery Rewrite:\nView Rewrite\nRemember the other day schema:\nshifezza\ntony\nlungo\nmike\nbaffo\ncarlo\nname\nnickname\nphone\nperson\noperation\n..\nlaundromat\nirish pub\nirish pub\nchocolate\nsnowflake\ncoverup\ncaffe\n$10M\necon_val\ntitle\ndescr.\n...\n$2M\n...\n$5M\nchocolate\nmike\nchief\nsnowflake\nsold\ntony\ncarlo\nsnowflake\nchief\npers_name\noper_name\nrols\ninvolved\nFigure 2: Simple Schema for a portion of our Mafia database.\nWhat are views? A \"named-query\", or a \"virtual-table\" (sometimes mate\nrialized).\nCREATE VIEW nick-cover AS\nSELECT\nnickname, coverup_name\nFROM operation o, involved i, person p\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\no.econ_val <= 5M;\nschifezza\nlaundromat\nlungo\nirish pub\nbaffo\nlaundromat\nnickname\ncoverup\nnick-cover\nFigure 3: Simple External Schema for a portion of our Mafia database.\nSELECT\nnickname\nFROM nick-cover nc\nWHERE nc.coverup_name=\"laundromat\";\nAfter view rewriting:\nSELECT\nnickname\nFROM (\nSELECT\nnickname, coverup_name\nFROM operation o, involved i, person p\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\n\no.econ_val <= 5M\n) as n\nWHERE n.coverup_name=\"laundromat\"\n7.1.1\nContraint Elimination / Logical Predicates manipulation\nAnother important step of query rewriting consists of Constant Elimination and\nLogical Predicates manipulation\nWHERE (a > 50+57 OR a =107) AND b > 105 and a=b AND b < 108\nbecomes (after constant elimination, and logical predicate manipulations):\nWHERE a = 107 and a = b and b = 107\n7.1.2\nSubquery Flattening\nAs Mike mentioned the last class another key step is Subquery flattening (Not\nevery optimizer will successfully do this, so you should always try to think of a\nnon nested query if you can find one):\nSELECT\nnickname\nFROM operation o, involved i, person p\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\no.econ_val <= 5M AND\no.coverup_name=\"laundromat\";\n7.1.3\nSemantic Optimization (Integrity Constraints)\nSometimes, knowledge of integrity constraints, in particular, foreign keys, can\nbe leveraged to avoid performing joins. As an example consider the following\nquery:\nSELECT\nnickname\nFROM operation o, involved i, person p\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\nIf we know from the foreign keys that every person that appear in the involved\ntuple is involved in some operation, we can skip the join with operations.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/3dde7e38f73479c8486a579c760881f7_MIT6_830F10_lec05.pdf",
      "content": "6.830/6.814 -- Notes‚àó for Lecture 5:\nDatabase Internals Overview (2/2)\nCarlo A. Curino\nSeptember 27, 2010\nAnnouncements\n- LABS: Do not copy... we perform automatic checking of plagiarism... it\nis not a good gamble!\n- Projects ideas and rules are posted online.\nReadings\nFor this class the suggested readings are:\n- Joseph Hellerstein, Michael Stonebraker and James Hamilton. Architec\nture of a Database System. Online at: http://db.cs.berkeley.edu/\npapers/fntdb07-architecture.pdf\nIt is a rather long paper (don't be too scared by the 119 pages, the page\nformat makes it look much longer than it is) that is in general worth reading,\nhowever we only require you too read sections: 1, 2 (skim through it), 3, 4 (up\nto subsection 4.5 included), 5. You can also skim through section 6 that we will\ndiscuss later on. Probably doesn't all make sense right now - you should look\nat this paper again to this paper through the semester for context.\nQuery Optimization\n3.1\nPlan Formation (SQL\nrelational algebra)\n‚Üí\nFirst step in which we start thinking of \"how\" this query will be solved. It still\nabstracts many implementation details (e.g., various implementations of each\n‚àóThese notes are only meant to be a guide of the topics we touch in class. Future notes\nare likely to be more terse and schematic, and you are required to read/study the papers and\nbook chapters we mention in class, do homeworks and Labs, etc.. etc..\n\noperator).\n- Projection: Œ†a,b(T )\n- Selection: œÉa=7(T )\n- Cross-Product: R √ó S\n- Join: R 1a1=a2 S\n- Rename: œÅa/b(R)\n...\n-\nLet's use the example query from the introduction lecture (it's in the notes\nbut we didn't discuss it last time):\nSELECT p.phone\nFROM person p, involve i, operation o\nWHERE p.name = i.person AND\ni.oper_name = o.name AND\no.coverup_name = \"laundromat\";\nCorresponding Algebra expression:\nŒ†phone(œÉo.coverupname=\"laundromat\"(œÉp.name=i.person(œÉi.opern ame=o.name(person√ó\n(involved √ó operation))\nLet's show this in a more visual way (the query plan):\nscan(person)\nscan(involved)\nscan(operations)\nproduct\nproduct\nfilter(p.name=i.person)\nfilter(i.oper_name=o.name)\nfilter(o.coverup=\"laundromat\")\nproject(p.phone)\nscan(person)\nscan(involved)\nlookup(operations, coverup=\"laundromat\")\nproduct\nproduct\nfilter(p.name=i.person)\nproject(p.phone)\nfilter(i.oper_name=o.name)\nproject(p.name,p.phone)\nproject(i.oper_name, i.person)\nproject(o.name)\nBASIC PLAN\nOPTIMIZED PLAN\nFigure 1: Two equivalent query plans for the same query.\nThe goal of the optimizer is two find equivalent query plans that lead to\nhigher performance. This is done in two ways:\n- logical: re-ordering operators in a semantic-preserving way\n\n- physical: choosing among multiple implementation of the operators the\none that for the specific scenario supposedly will give best performance\nThere are two main approaches to do this:\n- heuristic: uses a set of rules or strategies that typically pay back, e.g.,\npush all selections before joins.\n- cost-based: define a cost model for the execution (e.g., number of CPU\noperations + amount of disk I/O) and compare various plans according\nto the expected cost. Pick lowest cost.\nRestricting to certain plans (left deep) allows to avoid temporary buffering\nof results or re-computation.\nOptimization is often based on statistical information about the data... e.g.,\ndata distributions... often maintained as histograms. This can allows us to\npredict selectivity of operators, and thus have an idea of size of intermediate\nresults and the cost of the following operators.\nQuery (Pre)Compilation\nMost DBMS support query pre-compilation. I.e., the application can declare\nthat is going to use (frequently?) a certain type of query. The structure of\nthe query is provided in advance and later on the application only passes to\nthe DBMS the \"parameters\" to specialize this template into a specific query.\nThis allows to capitalize query parsing, some authorization work, and optimiza\ntion. However, predicate selectivity becomes a static choice. This can lead to\nsignificant performance improvement for very regular workloads (e.g., Web /\nOLTP).\nPhysical Storage\nAll records are stored in a region on disk; probably easiest to just think of each\ntable being in a file in the file system.\nTuples are arranged in pages in some order. Simple append to the file or\n\"heap file\" access path is a way to access these tuples on disk.\nNow how to access the tuple (the access path)?\n- heap scan\n- index scan provide an efficient way to find particular tuples\nWhat is an index? What does it do?\nIndex is an auxiliary data structure that aims at improving a specific type\nof access to the data. In particular an index is defined on one of the attributes,\nand for each value points at the corresponding positions on disk. Operations:\n- insert (key, recordid) points from a key to a record on disk records\n\n- lookup (key) given a single key return the location of records matching\nthe key.\n- lookup(lowkey,highkey) returns the location of records in the range of\nkeys given as input.\nHierarchical indices are the most common type used--e.g., B+-Trees indices\ntypically point from key values to records in the heap file\nShall we always have indexes? What are the pros and cons? (keep the index\nup-to-date, extra space required)\nFigure 2: B+Tree graphical representation.\nSpecial case, is a \"clustered\" index, i.e., when the order of the tuples on disk\ncorrespond to the order in which they are stored in the index. What is it good\nfor? (range selections, scans).\nSo now we have 2 options:\n- a heap scan (sequential, but load entire table),\n- index scan (load less data, but might lead to a lot of Random I/O).\nIn query plan we can mix multiple of this choosing the best for each basic\ntable we need to read. Why this might be useful? Consider a case in which\nalmost all tuples are needed from one table, but only very few from another\ntable?\nLater on we will get more into optimization details, for now let's keep in mind\nthat for many basic relational algebra operators we might have multiple imple\nmentations, and that depending on data distribution and query characteristics\n(selectivity), different plans might be better than others.\nQuery Execution\nIt is very common for DBMS to implement the Iterator model. The key idea is\nthat each operator implements the same interface:\n- open\nCourtesy of Grundprinzip on Wikipedia.\n\n- (hasNext)\nnext\n-\nclose\n-\nThis allows chaining of operators without any special logic to be imple\nmented. Also it is a natural way to propagate data and control through the\nstack of calls. There are many variations around this principal to allow for\nmore batch processing of data.\nIterator code for a simple Select:\nclass Select extends Iterator {\nIterator child;\nPredicate pred;\nSelect (Iterator child, Predicate pred) {\nthis.child = child;\nthis.pred = pred;\n}\nTuple next() {\nTuple t;\nwhile ((t = child.next()) != null ) {\nif (pred.matches(t)) {\nreturn t;\n}\n}\nreturn null;\n}\nvoid open() {\nchild.open();\n}\nvoid close() {\nchild.close();\n}\n}\nPlan Types (only mentioned in class)\nDifferent type of plans.\nBushy:\n\nfor t1 in outer\nfor t2 in inner\nif p(t1,t2) emit join(t1,t2)\nBushy requires temporarily memorizing results or recompute.\nLeft Deep:\nResults can be pipelined. No materialization of intermediate steps necessary.\nMany database systems restrict themselves to left or right deep plans for this\nreason\n7.1\nCost of an execution plan\nIn class we went over another example (the one above of basic and advanced\nplan). That example was mainly about CPU costs, in the example below, we\nexplore more the importance of Disk I/O. This is also good to prepare our next\nclass, which will be all about access methods.\nWhat's the \"cost\" of a particular plan?\nCPU cost (# of instructions)\n1Ghz == 1 billions instrs/sec\n1 nsec / instr\nI/Ocost(#ofpagesread,#ofseeks)\n100MB/sec\n10nsec/byte\n(RandomI/O=pageread+seek)\n10msec/seek\n100seeks/sec\nRandom I/O can be a real killer (10 million instrs/seek). (You probably\nexperience it yourself on your laptop sometimes).\nWhen does a disk need to seek?\nWhich do you think dominates in most database systems?\nDepends. Not always disk I/O. Typically vendors try to configure machines\nso they are 'balanced'. Can vary from workload to workload.\nBushy\nplan\n\nd\ni\nagram.\nL\ne\nf\nt deep plan diagram.\nImage by MIT OpenCourseWare.\nImage by MIT OpenCourseWare.\n\nFor example, fastest TPC-H benchmark result (data warehousing bench\nmark), on 10 TB of data, uses 1296 √ó 74 GB disks, which is 100 TB of storage.\nAdditional storage is partly for indices, but partly just because they needed\nadditional disk arms. 72 processors, 144 cores - 10 disks / processor!\nBut, if we do a bad job, random I/O can kill us!\n7.2\nAn example\nSchema:\nDEPT(dno,name);\nEMPL(eid,dno,sal);\nKIDS (kidname,eid);\nSome characteristics of this hypothetical scenario:\n- 100 tuples/page\n- 10 pages in RAM 10 KB/page\n10 ms seek time\n-\n- 100 MB/sec I/O\n- cardinality DEPT = 100 tuples = 1 page\n- cardinality EMPL = 10K tuples = 100 pages\n- cardinality KIDS = 30K tuples = 300 pages\nSELECT dept.name,kidname\nFROM emp, dept, kids\nWHERE e.sal > 10k AND\nemp.dno = dept.dno AND\ne.eid = kids.eid;\nŒ†name,kidname(dept ./dno=dno (œÉsal>10k(emp)) ./eno=eno kids)\nMore graphically:\nCPU operations:\nGraphica\nl\n\nr\nepre\nsen\ntati\non of exa\nmple.\nImage by MIT OpenCourseWare.\n\n- selection - 10,000 predicate ops\n- 1st Nested loops join - 100,000 predicate ops\n- 2nd nested loops join - 3,000,000 predicate ops\nLet's look at number of disk I/Os assuming LRU and no indices\nif d is outer:\n1 scan of DEPT\n-\n- 100 consecutive scans of EMP (100 x 100 pg. reads) - cache doesn't benefit\nsince e doesn't fit\n- 1 scan of EMP: 1 seek + read in 1MB =10 ms + 1 MB / 100 MB/sec\n= 20 msec\n- 20 ms x 100 depts = 2 sec\nTOTAL: 10 msec seek to start of d and read into memory 2.1 secs\nif d is inner:\n- read page of e - 10 msec\nread all of d into RAM - 10 msec\n-\nseek back to e - 10 msec\n-\n- scan rest of e - 10 msec,\njoining with d in memory... Because d fits into memory\nTOTAL: total cost is just 40 msec\nNo options if plan is pipelined, k must be inner:\n- 1000 scans of 300 pages 3 / 100 = 30 msec + 10 msec seek = 40 x 1000 =\n40 sec\nSo how do we know what will be cached? That's the job of the buffer pool.\nWhat about indexes? The DBMS uses indexes when it is possible (i.e., when\nan index exists and it support the required operation, e.g., hash-indexes do not\nsupport range search)\nBuffer Management and Storage Subsystem\nBuffer Manager or Buffer pool caches memory accesses... Why is it better if the\nDBMS does this instead of relying on OS-level caching? DBMS knows more\nabout the query workload, and can predict which pages will be accessed next.\nMoreover, it can avoid cases in which LRU fails.\nAlso explicit management of pages is helpful to the locking / logging neces\nsary to guarantee ACID properties.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/1343b2699f7d4c76a768b8f45b1674b1_MIT6_830F10_lec06.pdf",
      "content": "6.830/6.814 -- Notes‚àó for Lecture 6:\nAccess Methods\nCarlo A. Curino\nSeptember 29, 2010\nAnnouncements\n- Problem Set 2 is out today... due in two weeks... be aware that is going\nto overlap to LAB 2.\nProjects ideas and rules are posted online.\n-\nReadings\nFor this class the suggested readings are:\n- In Database Management Systems, read:\n- Pages 273-289. If you are using another book, this is the introduc\ntion to the Section on Storage and Indexing which discusses different\naccess methods and their relative performance.\n- Pages 344-358. This is an in-depth discussion of the B+Tree data\nstructure and its implementation. Most database books, as well as\nany algorithms text (such as CLR or Knuth) will provide an equiva\nlent discussion of B+Trees.\n- (was not assigned, but is an important reading) Pages 370-378 on\nStatic Hashing and Extensible Hashing\n\"The R*-Tree: An Efficient and Robust Access Method for Points and\n-\nRectangles.\" Beckmann et al, in The Red Book.\n‚àóThese notes are only meant to be a guide of the topics we touch in class. Future notes\nare likely to be more terse and schematic, and you are required to read/study the papers and\nbook chapters we mention in class, do homeworks and Labs, etc.. etc..\n\nRecap\nWe presented a broad overview of the DBMS internals.\nWe point out how important coming up with a good plan is.\nI claimed disk I/O can be very important.\nRemember I mentioned this last time:\nCPU cost (# of instructions)\n1Ghz == 1 billions instrs/sec\n1 nsec / instr\nRAM access\n50ns\nI/Ocost(#ofpagesread,#ofseeks)\n100MB/sec\n10nsec/byte\n(RandomI/O=pageread+seek)\n10msec/seek\n100seeks/sec\n1 seek = 10M instructions!!!\nMoreover there is a big difference from sequential and random IO. Example:\n- Read 10KB from a random location on disk: 10ms + 100Œºs = 10.1ms;\n- Read 10KB+10KB from a random location on disk (two blocks are next\nto each other): 10ms + 200Œºs = 10.2ms;\n- Read 10KB+10KB from two random locations on disk: 10ms + 100Œºs +\n10ms +100Œºs= 20.2ms;\nWOW! So saving disk I/O, and in particular random ones is VERY impor\ntant!! DB are usually well designed to: 1) avoid excessive disk I/O and try to be\nsequential, and 2) have a LOT of drives available (TPC-H competing machines:\n144 cores AND 1296 disks!!)\nToday we study how to do a good job at reducing Disk I/O by organize stuff\nintelligently on disk, next lecture we study what we should keep in RAM.\nAccess Methods\nToday we get into more details on Access Methods, i.e., on the portion of the\nDBMS in charge of managing the data on disk. We will show a bunch of\norganization of data, and their performance in supporting typical accesses we\nneed to support queries. Next lecture we will study the Buffer Manager, which\ntries to reduce the access to disk.\nWhat are the functionalities we need to support:\nscan\n-\n- search (equality)\n\n- search (range)\ninsert\n-\ndelete\n-\nVarious access methods:\n- heap file: unordered, typically implemented as a linked list of pages\n- sorted file: ordered records, expensive to maintain\n- index file: data + extra structures around to quickly access data\n- might contain data (primary index)\n- or point at the data, often stored in a heapfile or other index (sec\nondary index)\n- if the data are sorted in the same order of the field is index, we say is\na clustered index (we will see this is good for scans since disk accesses\nare sequential)\nType of indexes:\nhash\n-\nB+trees\n-\nR*trees\n-\n4.1\nData organization within file\nfile organization:\n- pages\n- records (record ids: page id, slot id)\npage layout:\n- fixed length records page of slots,\n- free bit map \"slotted page\" structure for var length records or slot direc\ntory (slot offset, len)\nWhat about big records? Hard to place, and might overflow on another\npage.\ntuple layout (similar story):\n- fixed length (structure know by the system catalog, can predict where\nfields start)\n\n- variable length, field slots, two options: delimiters or directory with point\ners/offsets\nWhat happen when the field size changes? Need to move stuff... so if we\nhave a mix of fixed/variable, the fixed fields are best to be stored first. Null\nvalues? Good trick is using the pointers/offset.. if two have same value, it\nmeans the field in between is null... this makes storing nulls 0 space overhead.\nCost model (enhanced one, from the book)\n- Heap Files: Equality selection on key; exactly one match.\n- Sorted Files: Files compacted after deletions.\nIndexes:\n-\n- Alt (2), (3): data entry size = 10% size of record\n- Hash: No overflow buckets. 80%pageoccupancy\nFilesize=1.25datasize\n‚Üí\n- Tree: 67% occupancy (this is typical).\nFile size = 1.5 data size\n‚Üí\nWe use:\n- B: number of data pages\n- R: number of record per page\n- D: average time to read/write from disk\n- C: average time to process a record (e.g., equality check)\nExtensible hashing\nGood read on the book: pages... 370-378\n6 Cost of the scan, equality, range, insert, and delete operations.\nImage by MIT OpenCourseWare.\nCo\nst\nof t\nhe scan, eq\nuality, rang\ne, in\nsert,\nand delet\ne opera\ntions.\n\nB+trees\nHierarchical indices are the most common type used--e.g., B+-Trees indices\ntypically point from key values to records in the heap file. Shall we always have\nindexes? What are the pros and cons? (keep the index up-to-date, extra space\nrequired)\nFigure 1: B+Tree graphical representation.\nSpecial case, is a \"clustered\" index, i.e., when the order of the tuples on disk\ncorrespond to the order in which they are stored in the index. What is it good\nfor? (range selections, scans).\nCourtesy of Grundprinzip on Wikipedia.\nDiagram\nof extensibl\ne h\nashing.\nImage by MIT OpenCourseWare.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/a8f0c23af21fc07f3c696ce4df4e4582_MIT6_830F10_lec07.pdf",
      "content": "6.830/6.814 -- Notes‚àó for Lecture 7:\nBuffer Management\nCarlo A. Curino\nSeptember 30, 2010\nAnnouncements\n- Lab 2 is going to go out today... Same as before... do not copy! (I mean\nit!)\n- Project teams were due last time. Anyone still without a team?\nReadings\nFor this class the suggested readings are:\n- Hong-Tai Chou and David DeWitt. An Evaluation of Buffer Management\nStrategies for Relational Database Systems. VLDB, 1985.\n- If you are interested in simple current implementation of bufferpool man\nagement: http://dev.mysql.com/doc/refman/5.5/en/innodb-buffer-pool.\nhtml and\nRecap\nWe are given many access methods. None of this is uniformly better than others\nacross the map, but each has some particular case in which is best. B+Trees\n(both clustered and unclustered) and Heapfile are the most commonly used.\nWe made a big case about the fact that Disk accesses are very expensive and\nthat the DBMS has 2 ways to reduce their impact on performance: i) reduce\nthe number of disk accesses by having smart access methods, ii) do a good job\nat caching in RAM data from disk.\n‚àóThese notes are only meant to be a guide of the topics we touch in class. Future notes\nare likely to be more terse and schematic, and you are required to read/study the papers and\nbook chapters we mention in class, do homeworks and Labs, etc.. etc..\n\nLast lecture we assumed every access was off of disk, and we tried our best\nto minimize the number of pages accessed, and to maximize the sequentiality of\nthe accesses (due to the large cost of seeks) by designing smart access methods.\nToday we get into the investigation of \"what\" to try to keep in RAM to further\navoid Disk I/O. The assumption is that we can't keep everything, since the DB\nis in general bigger than the available RAM.\nToday's topic\nWhy don't we just trust the OS?\n(DBMS knows more about the data accesses, and thus can make a better\njob about what to keep in RAM and what to pre-fetch, given an execution plan\nis rather clear what we are going to need next).\nDBMS manages its own memory: Buffer management / Buffer pool.\nBuffer pool:\n- cache of recently used pages (and more importantly plans ahead of which\none are likely to be accessed again, and what could be prefetched)\n- convenient \"bottleneck\" through which references to underlying pages go\nuseful when checking to see if locks can be acquired or not\n- shared between all queries running on the system (important! the goal is\nto globally optimize the query workload)\nFinal goal is to achieve better overall system performance... often correlated\nto minimize physical disk accesses (e.g., executing 1 query at a time guarantees\nminimum number of accesses, but lead to poor throughput).\nGood place to keep locks:\nCache - so what is the best eviction policy?\nUSE SAM NOTES FROM HERE ON...\n\nWhat about MySQL?\n(From MySQL documentation online: http://dev.mysql.com/doc/refman/5.\n5/en/innodb-buffer-pool.html)\nA variation of the LRU algorithm operates as follows by default:\n- 3/8 of the buffer pool is devoted to the old sublist.\n- The midpoint of the list is the boundary where the tail of the new sublist\nmeets the head of the old sublist.\n- When InnoDB reads a block into the buffer pool, it initially inserts it at\nthe midpoint (the head of the old sublist). A block can be read in because\nit is required for a user-specified operation such as a SQL query, or as part\nof a read-ahead operation performed automatically by InnoDB.\n- Accessing to a block in the old sublist makes it ?young?, moving it to the\nhead of the buffer pool (the head of the new sublist). If the block was\nread in because it was required, the first access occurs immediately and\nthe block is made young. If the block was read in due to read-ahead, the\nfirst access does not occur immediately (and might not occur at all before\nthe block is evicted).\n- As the database operates, blocks in the buffer pool that are not accessed\n?age? by moving toward the tail of the list. Blocks in both the new and\nold sublists age as other blocks are made new. Blocks in the old sublist\nalso age as blocks are inserted at the midpoint. Eventually, a block that\nremains unused for long enough reaches the tail of the old sublist and is\nevicted.\nYou can control:\n- innodb old blocks pct for the portion of new-old\n- innodb old blocks time Specifies how long in milliseconds (ms) a block\ninserted into the old sublist must stay there after its first access before it\ncan be moved to the new sublist.\nLRU Cache misses in typical scenarios\nFrom the paper: I/O Reference Behavior of Production Database Workloads\nand the TPC Benchmarks An Analysis at the Logical Level by Windsor W.\nHsu, Alan Jay Smith, and Honesty C. Young. We report the LRU miss ratios\nfor increasingly large bufferpool sizes, and for typical production databases and\npopular benchmarks. This should give you an idea of the fact that some portion\nof the DB are very \"hot\" while other are rather \"cold\", thus throwing more and\nmore RAM at the problem will provide less and less returns. On the other side\nchoosing the \"right\" things to keep in RAM is clearly vital!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/cbab1dd746579df2cf20fe5027fbf95a_MIT6_830F10_lec07b.pdf",
      "content": "LRU? What if a query just does one sequential scan of a file -- then putting it in the cache at all would be\npointless. So you should only do LRU if you are going to access a page again, e.g., if it is in the inner loop of a\nNL join.\nFor the inner loop of a nested loops join, is LRU always the best policy? No, if the inner doesn't fit into\nmemory, then LRU is going to evict the record over and over.\nE.g., 3 pages of memory, scanning a 4 page file:\npages\nA\nB\nC\nread\nhit/miss\nm\nm\nm\n1 4\nm\n1 4\n2 1\nm\nm\nAlways misses?. What would have been a better eviction policy? MRU!\npages\nA\nB\nC\nread\nhit/miss?\nm\nm\nm\n3 4\nm\nh\nh\n2 3\nm\nh\nh\n1 2\nm\nHere, MRU hits 2/3 times.\nDBMIN tries to do a better job of managing buffer pool by\n1) allocating buffer pools on a per-file-instance basis, rather than a single pool for all files\n2) using different eviction policies per file\nWhat is a \"file instance\"?\n(Open instance of a file by some access method.)\nEach time a file is opened, assign it one of several access patterns, and use that pattern to derive a buffer\nmanagement policy.\n(What does a policy consist of?)\nPolicy for a file consists of a number of pages to allocate as well as a page replacement policy.\n(What are the different types of policies?)\nPolicies vary according to access patterns for pages.\nWhat are the different access patterns for\npages in a database system?\nSS - Straight Sequential (sequential scan)\nCS - Clustered Sequential (merge join) (skip)\nLS - Looping sequential (nested loops)\n\nSR - Straight Random (index scan through secondary index)\nCR - Clustered Random (index NL join with with secondary index on inner, with repeat foreign keys on outer)\n(skiP\nSH - Straight Hierarchical (index lookup)\nLH - Looping Hierarchical (repeated btree lookups)\nSo what's the right policy:\nSH - 1 page, any access method\nCS - size of cluster pages, LRU\nLS - size of file pages, any policy, or MRU plus however many pages you can spare\nSR - 1 page, any access method\nCR - size of cluster pages, LRU\nSH - 1 page, any access method\nLH - top few pages, priority levels, any access method for bottom level\nHow do you know which policy to use?\n(Not said, presumably the query parser/optimizer has a table and can figure this out.)\nMultipage interactions.\nDiagram:\nBuffer pool per file instance, with locality set for that instance, plus \"global table\" that contains all pages.\nEach page is \"owned\" by a at most one query. Each query has a \"locality set\" of pages for each file instance it\nis accessing as a part of its operation, and each locality set is managed according to one of the above\npolicies.\nAlso store current number of pages associated with a file instance (r) and the maximum number of pages\nassociated with it (l).\nHow do you determine the maximum number of pages?\nUsing numbers above.\nWhat happens when the same page is accessed by multiple different queries?\n1) Already in buffer pool and owned locally\n2) Already in buffer pool, but not owned\n\na) If someone else owns, nothing to be done\nb) If no owner, requester becomes owner\n3) Not in buffer pool - requester becomes owner, evict something from requester's memory\nHow do you avoid running out memory?\nDon't admit queries into the system that will make the total sum of all of the l_ij variables > total system\nmemory.\nMetacomments about performance study.\n(It's good.) Interesting approach. What did they do?\nCollect real access patterns and costs, use them to drive a simulation of the buffer pool.\n(Why?) Real system would take a very long time to run, would be hard to control.\nHow much difference did they conclude this makes?\nAs much as a factor of 3 for workload with lots of concurrent queries and not much sharing. Seems to be\nmostly due to admission control. With admission control, simple fifo is about 60% as good as DBMIN.\nDBMIN is not used in practice.\nWhat is? (Love hate hints).\nWhat's that? (When an operator finishes with a page, it declares its love or hate for it. Buffer pool preferen\ntially evicts hated pages.)\nNot clear why (this would make a nice class project.)\nPerhaps love hate hints perform almost as well as DBMIN and are a lot simpler. They don't capture the need\nfor different buffer management policies for different types of files.\n(What else might you want the buffer manager to do?)\nPrefetch.\n(Why does that matter.)\nSequential I/O is a lot faster. If you are doing a scan, you should keep scanning for awhile before servicing some other\nrequest, even if the database hasn't yet requested the next page.\nDepending on the access method, you may want to selectively enable prefetching.\nInteraction with the operating system\n(What's the relationship between the database buffer manager and the operating system?)\nLong history of tension between database designers and OS writers. These days databases are an important enough\napplication that some OSes have support for them.\n(What can go wrong?)\n\n- Double buffering -- both OS and database may have a page in memory, wasting RAM.\n- Failure to write back -- the OS may not write back a page the database has evicted, which can cause problems if, for\nexample, the database tries to write a log page and then crashes.\n- Performance issues -- the OS may perform prefetching, for example, when the database knows it may not need it.\nDisk controllers have similar issues (cache, performance tricks.)\n(What are some possible solutions?)\n- Add hooks to the OS to allow the database to tell it what to do.\n- Modify the database to try to avoid caching things the OS is going to cache anyway.\nIn general, a tension in layered systems that can lead to performance anomalies.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec09_selinger.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/7b28a2ebf609323f8ac6783cae73beb5_MIT6_830F10_lec09_selinger.pdf",
      "content": "Selinger Op+mizer\n6.814/6.830 Lecture 10\nOctober 07, 2010\n(Slides gently offered by Sam Madden)\n\nThe Problem\n- How to order a series of N joins, e.g.,\nA.a = B.b AND A.c = D.d AND B.e = C.f\nN! ways to order joins (e.g., ABCD, ACBD, ....)\n(N-‚Äê1)! plans per ordering (e.g., (((AB)C)D), ((AB)(CD), ...)\nMul+ple implementa+ons (e.g., hash, nested loops, etc)\n- Naive approach doesn't scale, e.g., for 20-‚Äêway join\n- 10! x 9! = 1.3 x 10 ^ 12\n- 20! x 19! = 2.9 x 10 ^ 35\n\nSelinger Op+miza+ons\n- Le^-‚Äêdeep only (((AB)C)D) (eliminate (N-‚Äê1)!)\n- Push-‚Äêdown selec+ons\n- Don't consider cross products\n- Dynamic programming algorithm\n\nDynamic Programming\nR set of rela+ons to join (e.g., ABCD)\nFor ‚àÇ in {1...|R|}:\nfor S in {all length ‚àÇ subsets of R}:\noptjoin(S) = a join (S-‚Äêa),\nwhere a is the single rela+on that minimizes:\n\ncost(optjoin(S-‚Äêa)) +\n\nmin. cost to join (S-‚Äêa) to a +\n\nmin. access cost for a\noptjoin(S-‚Äêa) is cached from previous itera+on\n\nExample\noptjoin(ABCD) - assume all joins are NL\n‚àÇ=1\nA = best way to access A\n\n(e.g., sequen+al scan, or predicate pushdown into index...)\nB = best way to access B\nC = best way to access C\nD = best way to access D\nTotal cost computa+ons: choose(N,1), where N\nis number of rela+ons\nSubplan Best\nchoice\nCost\nA\nindex\nB\nseq scan\n...\nCache\n\nSubplan Best\nchoice\nCost\nA\nindex\nB\nseq scan\n...\nCache\nExample\noptjoin(ABCD)\n‚àÇ=2\n{A,B} = AB or BA\n\n(using previously computed best way to access A and B)\n{B,C} = BC or CB\n{C,D} = CD or DC\n{A,C} = AC or CA\n{A,D} = AD or DA\n{B,D} = BD or DB\nTotal cost computa+ons: choose(N,2) x 2\nSubplan Best\nchoice\nCost\nA\nindex\nB\nseq scan\n{A,B}\nBA\n{B,C}\nBC\n...\n\nExample\noptjoin(ABCD)\n‚àÇ=3\n{A,B,C} = remove A, compare A({B,C}) to ({B,C})A\nremove B, compare B({A,C}) to ({A,C})B\nremove C, compare C({A,B}) to ({A,B})C\n{A,B,D} = remove A, compare A({B,D}) to ({B,D})A\n\n....\n{A,C,D} = ...\n{B,C,D} = ...\nAlready computed -\nlookup in cache\nTotal cost computa+ons: choose(N,3) x 3 x 2\nCache\nSubplan Best\nchoice\nCost\nA\nindex\nB\nseq scan\n{A,B}\nBA\n{B,C}\nBC\n...\nSubplan Best\nchoice\nCost\nA\nindex\nB\nseq scan\n{A,B}\nBA\n{B,C}\nBC\n{A,B,C}\nBCA\n...\n{B,C,D}\nBCD\n\nSubplan\nBest\nchoice\nCost\nA\nindex\nB\nseq scan\n{A,B}\nBA\n{B,C}\nBC\n{A,B,C}\nBCA\n...\n{B,C,D}\nBCD\nExample\noptjoin(ABCD)\n‚àÇ=4\n{A,B,C,D} = remove A, compare A({B,C,D}) to ({B,C,D})A\nremove B, compare B({A,C,D}) to ({A,C,D})B\nremove C, compare C({A,B,D}) to ({A,B,D})C\nremove D, compare D({A,B,C}) to ({A,B,C})D\nTotal cost computa+ons: choose(N,4) x 4 x 2\nAlready computed -\nlookup in cache\nCache\nFinal answer is plan with minimum cost of\nthese four\nSubplan\nBest\nchoice\nCost\nA\nindex\nB\nseq scan\n{A,B}\nBA\n{B,C}\nBC\n{A,B,C}\nBCA\n{B,C,D}\nBCD\n{A,B,C,D} ABCD\n\nComplexity\nchoose(n,1) + choose(n,2) + ... + choose(n,n) total\nsubsets considered\nAll subsets of a size n set = power set of n = 2^n\nEquiv. to compu+ng all binary strings of size n\n000,001,010,100,011,101,110,111\nEach bit represents whether an item is in or out of set\n\nComplexity (con+nued)\nFor each subset,\nk ways to remove 1 join\nk < n\nm ways to join 1 rela+on with remainder\nTotal cost: O(nm2^n) plan evalua+ons\nn = 20, m = 2\n4.1 x 10^7\n\nInteres+ng Orders\n- Some queries need data in sorted order\n- Some plans produce sorted data (e.g., using an index scan or merge join\n- May be non-‚Äêop+mal way to join data, but overall op+mal plan\n- Avoids final sort\n- In cache, maintain best overall plan, plus best plan for each\ninteres+ng order\n- At end, compare cost of\n\nbest plan + sort into order\n\nto\n\nbest in order plan\n- Increases complexity by factor of k+1, where k is number of\ninteres+ng orders\n\nExample\nSELECT A.f3, B.f2 FROM A,B where A.f3 = B.f4\nORDER BY A.f3\nSubplan\nBest choice\nCost\nBest in A.f3 order Cost\nA\nindex\nindex\nB\nseq scan\nseqscan\n{A,B}\nBA hash\nAB merge\ncompare:\n\ncost(sort(output)) + 156\nto\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_830F10_lec09.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/cf3d2f95cf16fa717f571771c3792d38_MIT6_830F10_lec09.pdf",
      "content": "Lecture 9\n10/8/09\nQuery Optimization\nLab 2 due next Thursday.\nM pages memory\nS and R, with |S| |R| pages respectively; |S| > |R|\nM > sqrt(|S|)\nExternal Sort Merge\nsplit |S| and |R| into memory sized runs\nsort each\nmerge all runs simultaneously\ntotal I/O 3 |R| + |S|\n(read, write, read)\n\"Simple\" hash\ngiven hash function h(x), split h(x) values in N ranges\nN = ceiling(|R|/M)\nfor (i = 1...N)\nfor r in R\nif h(r) in range i, put in hash table Hr\no.w. write out\nfor s in S\nif h(s) in range i, lookup in Hr\no.w. write out\ntotal I/O\nN (|R| + |S|)\nGrace hash:\nfor each of N partitions, allocate one page per partition\nhash r into partitions, flushing pages as they fill\nhash s into partitions, flushing pages as they fill\nfor each partition p\nbuild a hash table Hr on r tuples in p\nhash s, lookup on Hr\nexample:\nR = 1, 4, 3, 6, 9, 14, 1, 7, 11\nS = 2, 3, 7, 12, 9, 8, 4, 15, 6\nh(x) = x mod 3\nR1 = 3 6 9\nR2 = 1 4 1 7\nR3 = 14 11\nS1 = 3 12 9 15 6\nS2 = 7 4\nS3 = 2 8\n\nNow, join R1 with S1, R2 with S2, R3 with S3\nNote -- need 1 page of memory per partition. Do we have enough memory?\nWe have |R| / M partitions\nM ‚â• sqrt(|R|)\nworst case\n|R| / sqrt(|R|) = sqrt(|R|) partitions\nNeed sqrt(|R|) pages of memory b/c we need at least one page per partition as we write out (note that simple\nhash doesn't have this requirement)\nI/O:\nread R+S (seq)\nwrite R+S (semi-random)\nread R+S (seq)\nalso 3(|R|+|S|) I/OS\nWhat's hard about this?\nWhen does grace outperform simple?\n(When there are many partitions, since we avoid the cost of re-reading tuples from disk in building partitions )\nWhen does simple outperform grace?\n(When there are few partitions, since grace re-reads hash tables from disk )\nSo what does Hybrid do?\nM = sqrt(|R|) + E\nMake first partition of size E, do it on the fly (as in simple)\nDo remaining partitions as in grace.\n|R|/M\nWhy does grace/hybrid outperform sort-merge?\nI/O (relative to simple with |R| = M)\nGrace\nSimple\nHybrid\n\nCPU Costs!\nI/O costs are comparable\n690 / 1000 seconds in sort merge are due to the costs of sorting\n17.4 in the case of CPU for grace/hybrid!\nWill this still be true today?\n(Yes)\nSelinger\nFamous paper. Pat Selinger was one of the early System R researchers; still active today.\nLays the foundation for modern query optimization. Some things are weak but have since been improved\nupon.\nIdea behind query optimization:\n(Find query plan of minimum cost )\nHow to do this?\n(Need a way to measure cost of a plan (a cost model) )\nsingle table operations\nhow do i compute the cost of a particular predicate?\ncompute it's \"selectivity\" - fraction F of tuples it passes\nhow does selinger define these? -- based on type of predicate and available statistics\nwhat statistics does system R keep?\n- relation cardinalities NCARD\n- # pages relation occupies TCARD\n- keys in index ICARD\n- pages occupied by index NINDX\nEstimating selectivity F:\ncol = val\nF = 1/ICARD()\nF = 1/10 (where does this come from?)\ncol > val\nhigh key - value / high key - low key\n1/3 o.w.\ncol1 = col2 (key-foreign key)\n1/MAX(ICARD(col1, col2))\n1/10 o.w.\nex: suppose emp has 10000 records, dept as 1000 records\ntotal records is 10000 * 1000, selectivity is 1/10000, so 1000 tuples expected to pass join\nnote that selectivity is defined relative to size of cross product for joins!\np1 and p2\nF1 * F2\n\np1 or p2\n1 - (1-F1) * (1-F2)\nthen, compute access cost for scanning the relation.\nhow is this defined?\n(in terms of number of pages read)\nequal predicate with unique index: 1 [btree lookup] + 1 [heapfile lookup] + W\n(W is CPU cost per predicate eval in terms of fraction of a time to read a page )\nrange scan:\nclustered index, boolean factors: F(preds) * (NINDX + TCARD) + W*(tuples read)\nunclustered index, boolean factors: F(preds) * (NINDX + NCARD) + W*(tuples read)\nunless all pages fit in buffer -- why?\n...\nseq (segment) scan: TCARD + W*(NCARD)\nIs an index always better than a segment scan? (no)\nmulti-table operations\nhow do i compute the cost of a particular join?\nalgorithms:\nNL(A,B,pred)\nC-outer(A) + NCARD(outer) * C-inner(B)\nNote that inner is always a relation; cost to access depends on access methods for B; e.g.,\nw/ index -- 1 + 1 + W\nw/out index -- TCARD(B) + W*NCARD(B)\nC-outer is cost of subtree under outer\nHow to estimate # NCARD(outer)? product of F factors of children, cardinalities of children\nexample:\nMerge_Join_x(P,A,B), equality pred\nC-outer + C-inner + sort cost\n(Saw cost models for these last time)\nAt time of paper, didn't believe hashing was a good idea\nOverall plan cost is just sum of costs of all access methods and join operators\nThen, need a way to enumerate plans\nE\nstim\nat\ning NCARD(outer).\nImage by MIT OpenCourseWare.\n\nIterate over plans, pick one of minimum cost\nProblem:\nHuge number of plans. Example:\nsuppose I am joining three relations, A, B, C\nCan order them as:\n(AB)C\nA(BC)\n(AC)B\nA(CB)\n(BA)C\nB(AC)\n(BC)A\nB(AC)\n(CA)B\nC(AB)\n(CB)A\nC(BA)\nIs C(AB) different from (CA)B?\nIs (AB)C different from C(AB)?\nyes, inner vs. outer\nn! strings * # of parenthetizations\nhow many parenthetizations are there?\nABCD --> (AB)CD A(BC)D AB(CD) 3\nXCD\nAXD\nABX\n* 2\n===\n6 --> (n-1)!\n==> n! * (n-1)!\n6 * 2 == 12 for 3 relations\nOk, so what does Selinger do?\nPush down selections and projections to leaves\nNow left with a bunch of joins to order.\nSelinger simplifies using 2 heuristics? What are they?\n- only left deep; e.g., ABCD => (((AB)C)D) show\n- ignore cross products\ne.g., if A and B don't have a join predicate, doing consider joining them\nstill n! orderings. can we just enumerate all of them?\n10! -- 3million\n20! -- 2.4 * 10 ^ 18\nso how do we get around this?\n\nEstimate cost by dynamic programming:\nidea: if I compute join (ABC)DE -- I can find the best way to combine ABC and then consider all the ways to\ncombine that with DE.\ni can remember the best way to compute (ABC), and then I don't have to re-evaluate it. best way to do ABC\nmay be ACB, BCA, etc -- doesn't matter for purposes of this decision.\nalgorithm: compute optimal way to generate every sub-join of size 1, size 2, ... n (in that order).\nR <--- set of relations to join\nfor ‚àÇ in {1...|R|}:\nfor S in {all length ‚àÇ subsets of R}:\noptjoin(S) = a join (S-a), where a is the single relation that minimizes:\ncost(optjoin(S-a)) +\nmin cost to join (S-a) to a +\nmin. access cost for a\nexample: ABCD\nonly look at NL join for this example\nA = best way to access A (e.g., sequential scan, or predicate pushdown into index...)\nB = \"\n\"\n\"\n\" B\nC = \"\n\"\n\"\n\" C\nD = \"\n\"\n\"\n\" D\n{A,B} = AB or BA\n{A,C} = AC or CA\n{B,C} = BC or CB\n{A,D}\n{B,D}\n{C,D}\n{A,B,C} = remove A - compare A({B,C}) to ({B,C})A\nremove B - compare ({A,C})B to B({A,C})\nremove C - compare C({A,B}) to ({A,B})C\n{A,C,D}\n{A,B,D}\n{B,C,D}\n{A,B,C,D} =\nremove A - compare A({B,C,D}) to ({B,C,D})A\n....\nremove B\nremove C\nremove D\nComplexity:\nnumber of subsets of size 1 * work per subset = W+\nnumber of subsets of size 2 * W +\n...\nnumber of subsets of size n * W+\nn + n + n ... n\n1 2 3\nn\n\nnumber of subsets of set of size n = power set of n = 2^n\n(string of length n, 0 if element is in, 1 if it is out; clearly, 2^n such strings)\n(reduced an n! problem to a 2^n problem)\nwhat's W? (n)\nso actual cost is: 2^n * n\nSo what's the deal with sort orders? Why do we keep interesting sort orders?\nSelinger says: although there may be a 'best' way to compute ABC, there may also be ways that produce\ninteresting orderings -- e.g., that make later joins cheaper or that avoid final sorts.\nSo we need to keep best way to compute ABC for different possible join orders.\nso we multiply by \"k\" -- the number of interesting orders\nhow are things different in the real world?\n- real optimizers consider bushy plans (why?)\nA\nD\nB\nC\nE\n- selectivity estimation is much more complicated than selinger says\nand is very important.\nhow does selinger estimate the size of a join?\n- selinger just uses rough heuristics for equality and range predicates.\n- what can go wrong?\nconsider ABCD\nsuppose sel (A join B) = 1\neverything else is .1\nIf I don't leave A join B until last, I'm off by a factor of 10\n- how can we do a better job?\n(multi-d) histograms, sampling, etc.\nexample: 1d hist Histogram of salaries, with salaries over 25K shaded.\nImage by MIT OpenCourseWare.\nH\nist\nogr\nam\nof\nsalaries, wi\nth salaries\nov\ner\n5K shaded.\n\nexample: 2d hist\nHis\ntog\nram of\ns\nal\nary\nand age.\nImage by MIT OpenCourseWare.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.830 / 6.814 Database Systems\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}