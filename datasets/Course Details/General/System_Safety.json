{
  "course_name": "System Safety",
  "course_description": "This course covers important concepts and techniques in designing and operating safety-critical systems. Topics include the nature of risk, formal accident and human error models, causes of accidents, fundamental concepts of system safety engineering, system and software hazard analysis, designing for safety, fault tolerance, safety issues in the design of human-machine interaction, verification of safety, creating a safety culture, and management of safety-critical projects. Includes a class project involving the high-level system design and analysis of a safety-critical system.",
  "topics": [
    "Business",
    "Operations Management",
    "Engineering",
    "Aerospace Engineering",
    "Systems Engineering",
    "Business",
    "Operations Management",
    "Engineering",
    "Aerospace Engineering",
    "Systems Engineering"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1 session / week, 3 hours / session\n\nTeaching Objectives for the Class\n\nWe are increasingly designing and operating potentially very dangerous systems while, at the same time, our systems are becoming much more complex than in the past. Events like Air France 477, the Columbia Space Shuttle losses, Deepwater Horizon, Fukushima and Chernobyl, and many other lesser known accidents, are contributing to a demand for greater skills and accountability on the part of engineers and managers.\n\nWhen engineers were creating systems that were relatively simple and the impact of design errors and failures was limited, learning on the job was adequate. But the situation is changing: Complexity is increasing rapidly in the systems we are building, partly because of the extensive use of software. The result is an increase in the physical and environmental harm that engineers can cause. This class will teach s how to use state-of-the-art system engineering techniques to build safer systems and to operate them in a safe manner.\n\nTopics include:\n\nUnderstanding the cause of past major accidents\n: Preventing future accidents requires knowing why accidents in the past have occurred. We will look at well-known accidents in the past to understand the common causal factors.\n\nLearning to perform a causal analysis of new accidents / incidents\n: Too often, accidents are blamed on the system operators without adequate consideration of the role of poor system design and poor management decision-making. You will learn how to analyze the cause of accidents in a blame-free context.\n\nPerforming hazard analysis\n: Hazard analysis has been described as \"investigating an accident before it occurs.\" The goal is to identify potential causes of accidents so the system can be designed and operated to avoid those causes. The first step in designing safety into the high-tech, complex systems we are building today is to perform a hazard analysis, including the potential of the system design to induce human errors in its use. You will learn both traditional and new state-of-the-art hazard analysis techniques.\n\nUsing the results of hazard analysis in the design process\n: The results of the hazard analysis should be used during system design to either eliminate the hazards or to control and mitigate them in some way. Potential hazards in the physical, software, and human-automation interaction need to all be considered to accomplish this goal. You will learn how to design for safety.\n\nOperating and managing safety-critical systems and projects\n: Operations is not taken into account enough in designing systems. In addition, our new systems are so complex that engineers are increasingly involved in the operation, management, and regulation of complex systems. Regulators are increasingly requiring companies to have a Safety Management System. You will learn how to create an operations safety management plan and some of the unique requirements for designing and operating a safety management system.\n\nTextbooks and Readings\n\nThere are two textbooks for the class:\n\nLeveson, Nancy G.\nSafeware: System Safety and Computers\n. Addison-Wesley Professional, 1995. ISBN: 9780201119725.\n\n------.\nEngineering a Safer World: Systems Thinking Applied to Safety\n. MIT Press, 2012. ISBN: 9780262016629. This book is available for purchase or as a\nfree download\nfrom the MIT Press website. [Preview with\nGoogle Books\n]\n\nIn addition, a few other readings will be assigned through out the semester. Our new\nSTPA Primer (draft)\nwill also be provided for reference, but you are not required to read it.\n\nGrading Policy\n\nEach homework assignment will be graded with a number from 1-10 as listed in the following table:\n\nGRADES\n\nGRADING NUMBERS\n\nA\n\n8-10\n\nB\n\n6-7\n\nC\n\n4-5\n\nF\n\n1-3\n\nFor the final grade, there are 4 assignments (Ethics, Citichem, Reading questions, Accident report), a CAST (accident analysis), and a class project. The breakdown of the final grade is summarized as follows:\n\nACTIVITIES\n\nPERCENTAGES\n\nClass Project\n\n30%\n\nCAST Analysis\n\n25%\n\nTake-home Exam\n\n25%\n\nFour Assignments\n\n20% (5% of each)",
  "files": [
    {
      "category": "Resource",
      "title": "System Safety: Accident Models, STAMP, Systems Theory",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/9b08616e217ffd3fafc2dd1c19fff8be_MIT16_863JS16_LecNotes2.pdf",
      "content": "Week 2 Class Notes\n\nPlan for Today\n- Accident Models\n- Introduction to Systems Thinking\n- STAMP: A new loss causality model\n\nAccident Causality Models\n- Underlie all our efforts to engineer for safety\n- Explain why accidents occur\n- Determine the way we prevent and investigate accidents\n- May not be aware you are using one, but you are\n- Imposes patterns on accidents\n\"All models are wrong, some models are useful\"\nGeorge Box\n\nTraditional Ways to Cope with Complexity\n1. Analytic Reduction\n2. Statistics\n\nAnalytic Reduction\n-\nDivide system into distinct parts for analysis\nPhysical aspects Separate physical components or functions\nBehavior Events over time\n-\nExamine parts separately and later combine analysis\nresults\n-\nAssumes such separation does not distort phenomenon\n-\nEach component or subsystem operates independently\n-\nAnalysis results not distorted when consider components\nseparately\n-\nComponents act the same when examined singly as when\nplaying their part in the whole\n-\nEvents not subject to feedback loops and non-linear interactions\n\nStandard Approach to Safety\n-\nReductionist\n- Divide system into components\n- Assume accidents are caused by component failure\n- Identify chains of directly related physical or logical component\nfailures that can lead to a loss\n- Assume randomness in the failure events so can derive\nprobabilities for a loss\n-\nForms the basis for most safety engineering and reliability\nengineering analysis and design\nRedundancy and barriers (to prevent failure propagation),\nhigh component integrity and overdesign, fail-safe design, ....\n\nDomino \"Chain of events\" Model\nEvent-based\nCargo\ndoor fails\nCauses\nFloor\ncollapses\nCauses\nHydraulics\nfail\nCauses\nAirplane\ncrashes\nDC-10:\nImage by MIT OpenCourseWare.A finger pushing a set of dominoes.\n\nThe Domino Model in action\nImage removed due to copyright restrictions.\n\nChain-of-events example\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nEvent Chain\n- E1: Worker washes pipes without inserting a slip blind.\n- E2: Water leaks into MIC tank\n- E3: Gauges do not work\n- E4: Operator does not open valve to relief tank\n- E3: Explosion occurs\n- E4: Relief valve opens\n- E5: Flare tower, vent scrubber, water curtain do not work\n- E5: MIC vented into air\n- E6: Wind carries MIC into populated area around plant.\nWhat was the \"root cause\"?\n\nVariants of Domino Model\n-\nBird and Loftus (1976)\n- Lack of control by management, permitting\n- Basic causes (personal and job factors) that lead to\n- Immediate causes (substandard practices/conditions/errors), which are\nthe proximate cause of\n- An accident or incident, which results in\n- A loss.\n-\nAdams (1976)\n- Management structure (objectives, organization, and operations)\n- Operational errors (management or supervisor behavior)\n- Tactical errors (caused by employee behavior and work conditions)\n- Accident or incident\n- Injury or damage to persons or property.\n\nReason Swiss Cheese\n(c) Cambridge University Press. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\n(c) Cambridge University Press. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSwiss Cheese Model Limitations\n- Ignores common cause failures of defenses (systemic\naccident factors)\n- Does not include migration to states of high risk\n- Assumes accidents are random events coming together\naccidentally\n- Assumes some (linear) causality or precedence in the\ncheese slices (and holes)\n- Just a chain of events, no explanation of \"why\" events\noccurred\n\nAccident with No Component Failures\n- Mars Polar Lander\n- Have to slow down spacecraft to land safely\n- Use Martian gravity, parachute, descent engines\n(controlled by software)\n- Software knows landed because of sensitive sensors on\nlanding legs. Cut off engines when determine have landed.\n- But \"noise\" (false signals) by sensors generated when\nparachute opens\n- Software not supposed to be operating at that time but\nsoftware engineers decided to start early to even out load\non processor\n- Software thought spacecraft had landed and shut down\ndescent engines\n\nTypes of Accidents\n- Component Failure Accidents\n- Single or multiple component failures\n- Usually assume random failure\n- Component Interaction Accidents\n- Arise in interactions among components\n- Related to interactive and dynamic complexity\n- Behavior can no longer be\n- Planned\n- Understood\n- Anticipated\n- Guarded against\n- Exacerbated by introduction of computers and software\n\nAccident with No Component Failure\n- Navy aircraft were ferrying missiles from one location to\nanother.\n- One pilot executed a planned test by aiming at aircraft in\nfront and firing a dummy missile.\n- Nobody involved knew that the software was designed to\nsubstitute a different missile if the one that was\ncommanded to be fired was not in a good position.\n- In this case, there was an antenna between the dummy\nmissile and the target so the software decided to fire a\nlive missile located in a different (better) position instead.\n\nAnalytic Reduction does not Handle\n- Component interaction accidents\n- Systemic factors (affecting all components and barriers)\n- Software and software requirements errors\n- Human behavior (in a non-superficial way)\n- System design errors\n- Indirect or non-linear interactions and complexity\n- Migration of systems toward greater risk over time (e.g.,\nin search for greater efficiency and productivity)\n\nSummary\n- New levels of complexity, software, human factors do not\nfit into a reductionist, reliability-oriented world.\n- Trying to shoehorn new technology and new levels of\ncomplexity into old methods will not work\n\nImages removed due to copyright restrictions.\n\n- \"But the world is too complex to look at the\nwhole, we need analytic reduction\"\n- Right?\n\nSystems Theory\n- Developed for systems that are\n- Too complex for complete analysis\n- Separation into (interacting) subsystems distorts the results\n- The most important properties are emergent\n- Too organized for statistics\n- Too much underlying structure that distorts the statistics\n- New technology and designs have no historical information\n- Developed for biology and engineering\n- First used on ICBM systems of 1950s/1960s\n\nSystems Theory (2)\n-\nFocuses on systems taken as a whole, not on parts\ntaken separately\n-\nEmergent properties\n-\nSome properties can only be treated adequately in their\nentirety, taking into account all social and technical aspects\n\"The whole is greater than the sum of the parts\"\n-\nThese properties arise from relationships among the parts of\nthe system\nHow they interact and fit together\n\nEmergent properties\n(arise from complex interactions)\nProcess\nProcess components interact in\ndirect and indirect ways\nSafety is an emergent property\n\nController\nControlling emergent properties\n(e.g., enforcing safety constraints)\nProcess\nControl Actions\nFeedback\nIndividual component behavior\nComponent interactions\nProcess components interact in\ndirect and indirect ways\n\nController\nControlling emergent properties\n(e.g., enforcing safety constraints)\nProcess\nControl Actions\nFeedback\nIndividual component behavior\nComponent interactions\nProcess components interact in\ndirect and indirect ways\nAir Traffic Control:\nSafety\nThroughput\n\nControls/Controllers Enforce Safety Constraints\n- Power must never be on when access door open\n- Two aircraft must not violate minimum separation\n- Aircraft must maintain sufficient lift to remain airborne\n- Public health system must prevent exposure of public to\ncontaminated water and food products\n- Pressure in a deep water well must be controlled\n- Truck drivers must not drive when sleep deprived\n\nExample\nSafety\nControl\nStructure\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nCourtesy of Qi D. Van Eikema Hommes. Used with permission.\n\n(c) Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nControl Structure Diagram - Level 0\n(c) Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nControl Structure Diagram - ISS Level 1\n(c) Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nevel\nre for\nExample High-L\nControl Structu\nITP\n\nControlled Process\n\nProcess\nModel\nControl\nActions\nFeedback\nThe Role of Process Models in Control\n- Accidents often occur when process\nmodel inconsistent with state of\ncontrolled process (SA)\n- A better model for role of software and\nhumans in accidents than random\nfailure model\n- Four types of unsafe control actions:\n-\nControl commands required for safety\nare not given\n-\nUnsafe ones are given\n-\nPotentially safe commands given too\nearly, too late\n-\nControl stops too soon or applied too\nlong\nController\n(Leveson, 2003); (Leveson, 2011)\nControl\nAlgorithm\n\nSTAMP:\nSystem-Theoretic Accident\nModel and Processes\nBased on Systems Theory\n(vs. Reliability Theory)\n\nApplying Systems Theory to Safety\n- Accidents involve a complex, dynamic \"process\"\n- Not simply chains of failure events\n- Arise in interactions among humans, machines and the\nenvironment\n- Treat safety as a dynamic control problem\n- Safety requires enforcing a set of constraints on system\nbehavior\n- Accidents occur when interactions among system\ncomponents violate those constraints\n- Safety becomes a control problem rather than just a\nreliability problem\n\nSafety as a Dynamic Control Problem\n- Examples\n- O-ring did not control propellant gas release by sealing gap in field\njoint of Challenger Space Shuttle\n- Software did not adequately control descent speed of Mars Polar\nLander\n- At Texas City, did not control the level of liquids in the ISOM tower;\n- In DWH, did not control the pressure in the well;\n- Financial system did not adequately control the use of financial\ninstruments\n\nSafety as a Dynamic Control Problem (2)\n- Events are the result of the inadequate control\n- Result from lack of enforcement of safety constraints\nin system design and operations\n- A change in emphasis:\n\"prevent failures\"\n↓\n\"enforce safety constraints on system behavior\"\n\nAccident Causality\nUsing STAMP\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: CAST Analysis",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/7dc4344ef259e4bde4acc1bb2d578a95_MIT16_863JS16_LecNotes3-2.pdf",
      "content": "Plant\nOperators\nControl\nRoom\nOperators\nPhysical\nEquipment\nPlant\nManager\nOperations\nManager\nMaintenance\nManager\nHSSE\n(Plant\nProcess\nSafety)\nLine\nManagement\nHSSE\n(Corporate\nProcess\nSafety)\nMaintenance\nWorkers\nCorporate\nManagement\nWhiting\n\nFor the Tank Overflow Accident\n- Examine the physical level.\n- What were the responsibilities (requirements) of the\nphysical equipment?\n- What emergency and safety equipment (controls)\nexisted? How did these relate to the requirements\n(constraints)?\n- What failures or unsafe interactions occurred in the\naccident?\n- Evaluate the physical level controls.\n- What additional questions were raised by your analysis\nso far? (What would you ask if you were investigating\nthis accident?)\n\nPhysical Process in SO2 Overflow\nRequirements (roles/responsibilities): Provide physical protection\nagainst hazards (protection for employees and others within the\nvicinity);\n1. Protect against runaway reactions\n2. Protect against inadvertent release of toxic chemicals or explosion\n3 Convert released chemicals into a non-hazardous or less hazardous\nform\n4 Contain inadvertently released toxic chemicals\n5 Provide feedback to operators and others about the state of safety-\ncritical equipment\n6 Provide indicators (alarms ) of the existence of hazardous conditions\n7 Provide protection against human or environmental exposure after\nrelease\n8 Provide emergency treatment of exposed individuals\n\nPhysical Equipment (2)\nEmergency and Safety Equipment (controls): Only those related to\nthe Tank 731 overflow and subsequent events are included.\n- Flow meter and level transmitter\n- Block valves, bypass valve\n- SO2 alarm\n- High level alarms\n- SO2 alarm (analyzer): Strobe light\n- Unit evacuation alarm\n- Drain from containment area to process sewers\n- Process vent routed to T-707 from T-731.\n- Overflow pipe with gooseneck\n- RV\n\nFailures and Inadequate controls: (the links below refer to the requirements\nabove)\n- SO2 released to atmosphere (→ 2)\n- Control flow valve may have stuck open (→ 2)\n- Level transmitter L47731A for Tank 731 was not working properly. Readings\nhad been erratic for a year and a half. This meant that one of the high level\nalarms was effectively disabled. (→ 5)\n- Flow meter FT47706 was not working properly (→ 5)\n- Drain to emergency containment sewer clogged. (could not send excess gas to\nsafe containment area) (→ 4)\n- Alert for harmful release of toxic SO2 is visual and could not be seen by workers\nin path of released gas.\n-SO2 analyzers on the SVS alarm trigger flashing strobe lights on the unit, but no\naudible alarm so they are only effective if they are within the workers line of sight.\n-Several of exposed workers were over 100 yards from the unit and were not able to\nsee the flashing lights. (Because SO2 is a gas, it has the potential to travel away from\nthe unit and around objects to reach workers who may not be able to see the flashing\nstrobe lights.) (→ 5)\nPhysical Contextual Factors:\n- Wind was from NNE at about 9 mph.\n\nEvaluation of Physical Level Controls\n-\nReasonable amount provided but much was inadequate or non-\noperational, e.g.,\n- Tank level transmitter not working properly\n- Flow meter not working properly\n- Drain to emergency containment sewer clogged\n-\nQuestions:\n- Why was sewer clogged? Is this a common occurrence?\n- Were non-functional or inadequately functioning controls common at the\nplant?\n- What types of policy existent about operating plant with non-functioning\nsafety equipment? Is risk assessment done when this occurs?\n- What types of inspections done on safety-critical equipment?\n- How is safety-critical equipment identified?\n- What is maintenance policy? Why was safety-critical equipment non-\noperational or operating erratically for relatively long periods of time?\n\nHindsight Bias at Operator Level\n- What are some examples of hindsight bias in the report?\n\nHindsight Bias Examples\n- Data availability vs. data observability (Dekker)\n- \"The available evidence should have been sufficient to give\nthe Board Operator a clear indication that Tank 731 was\nindeed filling and required immediate attention.\"\n\n- \"Operators could have trended the data\" on the control board\n\nBoard Control Valve Position: closed Flow Meter: shows no flow\nManual Control Valve Position: open Flow: none\nBypass Valve: closed SO2 alarm: off\nLevel in tank: 7.2 feet High level alarm: off\n\nHindsight Bias Examples\n-\nAnother example\n- \"Interviews with operations personnel did not produce a clear\nreason why the response to the SO2 alarm took 31 minutes.\nThe only explanation was that there was not a sense of\nurgency since, in their experience, previous SO2 alarms\nwere attributed to minor releases that did not require a unit\nevacuation.\"\n\nAnalyze Board Operator\n-\nStart from assumption that most people want to do the right\nthing and not purposely cause accidents\n-\nSo why did wrong thing in situation in which they found\nthemselves?\n- Contextual and systemic factors\n- Mental model flaws\n- Missing feedback\n-\nTo minimize hindsight bias, try to understand why it made\nsense for them to act the way they did.\n- For example, why didn't evacuate immediately?\n- Did higher levels of control structure know about previous\ninstances of this behavior?\n\nBoard Operator Analysis\n- I separated contextual issues into those related to:\n- Tank level\n- Didn't know tank was filling. Responded incorrectly to alarm.\nWhy?\n- Procedures and Alarms\n- Didn't evacuate plant immediately. Why?\n\nContextual Factors for Board Operator:\nRelated to Tank Level\n- Flow meter broken. Indicated no flow.\n- Level transmitter and high-level alarm not functioning\n- Erratic behavior since January 2006 but work order not\nwritten to repair it until July 2008 (year and a half later).\nWhy?\n- Another level transmitter and high-level alarm (8.5 ft)\nwere functioning\n- But level transmitters gave conflicting information\nregarding tank level\n\nContextual Factors for Board Operator:\nRelated to Alarms\n- Distracted by other duties related to transferring pit sweep\n- Another alarm in plant he had to attend. Multiple alarms at\nsame time.\n- Previous SO2 alarms attributed to minor releases did not\nrequire an evacuation alarm. Occur approximately once a\nmonth.\n- None of alarms designated as critical alarms \"which may\nhave elicited a higher degree of attention ...\"\n\nContextual Factors for Board Operator:\nRelated to Alarms (2)\n- Upper limit of SO2 analyzers is 25 ppm which occurred\nalmost immediately. No way to determine actual SO2\nconcentration during incident.\n- In past, units not evaluated by blowing horn but by\noperations personnel walking through unit and stopping\nwork.\n- No written procedure for sounding alarm.\n\nContextual Factors for Board Operator:\nRelated to Procedures\n- No written unit procedure for responding to SO2 alarm.\n- No written procedure for ordering evacuation when SO2\nalarm sounds nor criteria established for level of SO2\nthat should trigger an evacuation alarm.\n- Unit training materials contains info about hazards of\nSO2 but no standard operating/emergency procedures\n- Block valves normally left open to facilitate remote\noperations.\n\nCompany Safety Policy\n\"At units, any employee shall assess the situation and\ndetermine what level of evacuation and what equipment\nshutdown is necessary to ensure the safety of all\npersonnel, mitigate the environmental impact and\npotential for equipment/property damage. When in\ndoubt, evacuate.\"\n\nWhat problems do you see with this policy?\n\nProblems with Policy\n-\nResponsibility not assigned to anyone.\n- Need someone with responsibility, accountability, and authority\n- Plus backup procedures for others to step in when necessary\n-\nNormal human behavior is to try to diagnose situation first.\n- When overwhelmed with information, will try to digest and\nunderstand it first.\n- If want immediate behavior, then need to require it (or automate\nit) and not leave it up to employee to \"evaluate situation.\"\n-\nIf want flexibility inherent in real-time decision making then will\nneed to provide\n- More extensive training\n- Better real-time information to operators\n\nOutside Operator\n-\nNo more info than board operator and in hurry to get to\nsimultaneous (but unrelated) trip of equipment in another part\nof unit\n-\nPrimary mistake (in hindsight) seems to be delay in\nevacuation alarm and attempt to clean up instead of\nimmediately seeking help.\n- Report says he was not sure conditions bad enough to make\nthat call\n- \"Poor understanding of risks of an SO2 release\"\n- Is this unique to these two operators?\n- Is this unique to risks associated with SO2 and not other risks?\n- Normal response is to try to fix problem rather than call\nemergency personnel immediately\n\nOther Things Not Mentioned\n- Very likely coordination problems about who should be\ndoing what, but not enough info in report\n- Dynamics (migration):\n- When I asked about why no criteria for SO2 alarm levels,\ntold that \"didn't think of it before - perhaps not needed\nbefore when lots of experienced personnel in units\"\n- Had experience level decreased?\n\nRecommendations\n- Report recommendations very limited\n- We came up with lots more even without additional\ninformation (see STAMP analysis of same accident)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: CAST Class Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/ed4495e833c6ddc73616ae4cd7638620_MIT16_863JS16_LecNotes3-1.pdf",
      "content": "Analyzing Accidents and Incidents\nwith CAST\n\nCommon Traps in Understanding\nAccident Causes\n- Root cause seduction\n- Hindsight bias\n- Narrow views of human error\n- Focus on blame\n\nRoot Cause Seduction\n- Assuming there is a root cause gives us an illusion of\ncontrol.\n- Usually focus on operator error or technical failures\n- Ignore systemic and management factors\n- Leads to a sophisticated \"whack a mole\" game\n- Fix symptoms but not process that led to those symptoms\n- In continual fire-fighting mode\n- Having the same accident over and over\n\nOversimplification of Causes\n- Almost always there is:\n- Operator \"error\"\n- Flawed management decision making\n- Flaws in the physical design of equipment\n- Safety culture problems\n- Regulatory deficiencies\n- Etc.\n\n\"Blame is the Enemy of Safety\"\n- To prevent accidents in the future, need to focus on why\nit happened, not who to blame\n- Blame is for the courts, prevents understanding what\noccurred and how to fix it.\n\nOperator Error: Traditional View\n- Human error is cause of incidents and accidents\n- So do something about human involved (suspend,\nretrain, admonish)\n- Or do something about humans in general\n- Marginalize them by putting in more automation\n- Rigidify their work by creating more rules and\nprocedures\n\nOperator Error: Systems View (1)\n- Human error is a symptom, not a cause\n- All behavior affected by context (system) in which occurs\n- Role of operators in our systems is changing\n- Supervising rather than directly controlling\n- Systems are stretching limits of comprehensibility\n- Designing systems in which operator error inevitable and then\nblame accidents on operators rather than designers\n\nOperator Error: Systems View (2)\n- To do something about error, must look at system in which\npeople work:\n- Design of equipment\n- Usefulness of procedures\n- Existence of goal conflicts and production pressures\n- Human error is a symptom of a system that needs to\nbe redesigned\n\nHindsight Bias\nCourtesy of Sidney Dkker. Used with permission.\n(Sidney Dekker, 2009)\n\"should have, could have, would have\"\n\nOvercoming Hindsight Bias\n- Assume nobody comes to work to do a bad job.\n- Assume were doing reasonable things given the complexities,\ndilemmas, tradeoffs, and uncertainty surrounding them.\n- Simply finding and highlighting people's mistakes explains\nnothing.\n- Saying what did not do or what should have done does not\nexplain why they did what they did.\n\nOvercoming Hindsight Bias\n- Need to consider why it made sense for people to do what\nthey did\n- Some factors that affect behavior\n- Goals person pursuing at time and whether may have conflicted\nwith each other (e.g., safety vs. efficiency, production vs.\nprotection)\n- Unwritten rules or norms\n- Information availability vs. information observability\n- Attentional demands\n- Organizational context\n\nGoals for an Accident Analysis Technique\n- Minimize hindsight bias\n- Provide a framework or process to assist in\nunderstanding entire accident process and identifying\nsystemic factors\n- Get away from blame (\"who\") and shift focus to \"why\"\nand how to prevent in the future\n- Goal is to determine\n- Why people behaved the way they did\n- Weaknesses in the safety control structure that allowed\nthe loss to occur\n\nAnalysis Results Format\n- For each component, will identify:\n- Safety responsibilities\n- Unsafe control actions that occurred\n- Contextual reasons for the behavior\n- Mental (process) model flaws that contributed to it\n- Two examples will be done in tutorial. Lots of examples\nin the ESW book (chapters 6 and 11 as well as the ESW\nappendices).\n- Comair Lexington crash\n- Train Derailment (Niels Smit)\n\nComAir 5191 (Lexington) Sept. 2006\nAnalysis using CAST by Paul Nelson,\nComAir pilot and human factors expert\n(for report: http://sunnyday.mit.edu/papers/nelson-thesis.pdf\n\nIdentify Hazard and Safety\nConstraint Violated\n- Accident: death or injury, hull loss\n- System hazard: Runway incursions and operations on\nwrong runways or taxiways.\n- System safety constraint: The safety control structure\nmust prevent runway incursions and operations on\nwrong runways or taxiways\nGoal: Figure out why the safety control structure did\nnot do this\n\nIdentifying Components to Include\n- Start with physical process\n- What inadequate controls allowed the physical events?\n- Physical\n- Direct controller\n- Indirect controllers\n- Add controls and control components as required to\nexplain the inadequate controls already identified.\n\nPhysical System (Aircraft)\n- Failures: None\n- Unsafe Interactions\n- Took off on wrong runway\n- Runway too short for that aircraft to become safely\nairborne\nThen add direct controller of aircraft to determine why\nthey were on that runway\n\nAircraft\nFlight Crew\n\n5191 Flight Crew\nSafety Requirements and Constraints:\n-\nOperate the aircraft in accordance with company procedures, ATC\nclearances and FAA regulations.\n-\nSafely taxi the aircraft to the intended departure runway.\n-\nTake off safely from the planned runway\nUnsafe Control Actions:\n-\nTaxied to runway 26 instead of continuing to runway 22.\n-\nDid not use the airport signage to confirm their position short of the\nrunway.\n-\nDid not confirm runway heading and compass heading matched\n(high threat taxi procedures0\n-\n40 second conversation violation of \"sterile cockpit\"\n\nMental Model Flaws:\n-\nBelieved they were on runway 22 when the takeoff was initiated.\n-\nThought the taxi route to runway 22 was the same as previously\nexperienced.\n-\nBelieved their airport chart accurately depicted the taxi route to\nrunway 22.\n-\nBelieved high-threat taxi procedures were unnecessary.\n-\nBelieved \"lights were out all over the place\" so the lack of runway\nlights was expected.\n\nContext in Which Decisions Made:\n-\nNo communication that the taxi route to the departure runway was\ndifferent than indicated on the airport diagram\n-\nNo known reason for high-threat taxi procedures\n-\nDark out\n-\nComair had no specified procedures to confirm compass heading\nwith runway\n-\nSleep loss fatigue\n-\nRunways 22 and 26 looked very similar from that position\n-\nComair in bankruptcy, tried to maximize efficiency\n- Demanded large wage concessions from pilots\n- Economic pressures a stressor and frequent topic of conversation for\npilots (reason for cockpit discussion)\n\nThe Airport Diagram\nWhat The Crew Had\nWhat the Crew Needed\n\nSome Questions to Answer\n- Why was the crew not told about the construction?\n- Why didn't ATC detect the aircraft was in the wrong\nplace and warn the pilots?\n- Why didn't the pilots confirm they were in the right\nplace?\n- Why didn't they detect they were in the wrong place?\n\nAircraft\nFlight Crew\nComair/Delta\nConnection\n\nComair (Delta Connection) Airlines\nSafety Requirements and Constraints\n-\nResponsible for safe, timely transport of passengers within their\nestablished route system\n-\nEnsure crews have available all necessary information for each\nflight\n-\nFacilitate a flight deck environment that enables crew to focus on\nflight safety actions during critical phases of flight\n-\nDevelop procedures to ensure proper taxi route progression and\nrunway confirmation\n\nComair (Delta Connection) Airlines (2)\nUnsafe Control Actions:\n-\nInternal processes did not provide LEX local NOTAM on the flight\nrelease, even though it was faxed to Comair from LEX\n-\nIn order to advance corporate strategies, tactics were used that\nfostered work environment stress precluding crew focus ability\nduring critical phases of flight.\n-\nDid not develop or train procedures for take off runway confirmation.\n\nComair (3)\nProcess Model Flaws:\n-\nTrusted the ATIS broadcast would provide local NOTAMs to crews.\n-\nBelieved tactics promoting corporate strategy had no connection to\nsafety.\n-\nBelieved formal procedures and training emphasis of runway\nconfirmation methods were unnecessary.\nContext in Which Decisions Made:\n-\nIn bankruptcy.\n\nFederal Aviation\nAdministration\nComair: Delta\nConnection\nAirport Safety &\nStandards District\nOffice\nLEX ATC\nFacility\nNational\nFlight Data\nCenter\nJeppesen\nFlight\nCrew\nCertification, Regulation,\nMonitoring & Inspection\nProcedures, Staffing, Budget\nAircraft Clearance and\nMonitoring\nCharts, NOTAM Data\n(except \"L\") to Customer\nRead backs, Requests\nLocal\nNOTAMs\nReports, Project Plans\nNOTAM Data\nChart Discrepancies\nATIS & \"L\" NOTAMs\nOperational Reports\nALPA\nSafety\nALR\nAirport\nDiagram\nAirport Diagram\nVerification\nOptional construction signage\n= missing feedback lines\nCertification, Inspection,\nFederal Grants\nComposite Flight Data, except \"L\" NOTAM\nGraphical Airport Data\nATO:\nTerminal\nServices\nPilot perspective\ninformation\nConstruction information\nBlue Grass Airport\nAuthority\nProcedures &\nStandards\nFlight release, Charts etc.\nNOTAMs except \"L\"\nIOR, ASAP\nReports\nCertification & Regulation\nCourtesy of Lund University. Used with permission.\n\nJeppesen\nSafety Requirements and Constraints\n-\nCreation of accurate aviation navigation charts and information\ndata for safe operation of aircraft in the NAS.\n-\nAssure Airport Charts reflect the most recent NFDC data\nUnsafe Control Actions\n-\nInsufficient analysis of the software which processed incoming\nNFDC data to assure the original design assumptions matched\nthose of the application.\n-\nNot making available to the NAS Airport structure the type of\ninformation necessary to generate the 10-8 \"Yellow Sheet\" airport\nconstruction chart.\n\nJeppesen (2)\nProcess Model Flaws\n-\nBelieved Document Control System software always generated\nnotice of received NFDC data requiring analyst evaluation.\n-\nAny extended airport construction included phase and time data as\na normal part of FAA submitted paper work.\nContext in Which Decisions Made\n-\nThe Document Control System software generated notices of\nreceived NFDC data.\n-\nPreferred Chart provider to airlines.\nFeedback\n-\nCustomer feedback channels are inadequate for providing\ninformation about charting inaccuracies.\n\nNational Flight Data Center\nSafety Requirements and Constraints\n-\nCollect, collate, validate, store, and disseminateaeronautical\ninformation detailing the physical description and operational status\nof all components of the National Airspace System (NAS).\n-\nOperate the US NOTAM system to create, validate, publish and\ndisseminate NOTAMS.\n-\nProvide safety critical NAS information in a format which is\nunderstandable to pilots.\n-\nNOTAM dissemination methods will ensure pilot operators receive\nall necessary information.\n\nUnsafe Control Actions\n-\nDid not use the FAA Human Factors Design Guide principles to\nupdate the NOTAM text format.\n-\nLimited dissemination of local NOTAMs (NOTAM-L).\n-\nUsed multiple and various publications to disseminate NOTAMs,\nnone of which individually contained all NOTAM information.\nProcess Model Flaws:\n-\nBelieved NOTAM system successfully communicated NAS\nchanges.\nContext in Which Decisions Made\n-\nThe NOTAM systems over 70 year history of operation. Format\nbased on teletypes\nCoordination:\n-\nNo coordination between FAA human factors branch and the NFDC\nfor use of HF design principle for NOTAM format revision.\n\nBlue Grass Airport Authority (LEX)\nSafety Requirements and Constraints:\n-\nEstablish and maintain a facility for the safe arrival and departure of\naircraft to service the community.\n-\nOperate the airport according to FAA certification standards, FAA\nregulations (FARs) and airport safety bulletin guidelines (ACs).\n-\nEnsure taxiway changes are marked in a manner to be clearly\nunderstood by aircraft operators.\n\nAirport Authority\nUnsafe Control Actions:\n-\nRelied solely on FAA guidelines for determining adequate signage\nduring construction.\n-\nDid not seek FAA acceptable options other than NOTAMs to inform\nairport users of the known airport chart inaccuracies.\n-\nChanged taxiway A5 to Alpha without communicating the change\nby other than minimum signage.\n-\nDid not establish feedback pathways to obtain operational safety\ninformation from airport users.\n\nAirport Authority\nProcess Model Flaws:\n-\nBelieved compliance with FAA guidelines and inspections would\nequal adequate safety.\n-\nBelieved the NOTAM system would provide understandable\ninformation about inconsistencies of published documents.\n-\nBelieved airport users would provide feedback if they were\nconfused.\nContext in Which Decisions Made:\n-\nThe last three FAA inspections demonstrated complete compliance\nwith FAA regulations and guidelines.\n-\nLast minute change from Safety Plans Construction Document\nphase III implementation plan.\n\nFederal Aviation\nAdministration\nComair: Delta\nConnection\nAirport Safety &\nStandards District\nOffice\nLEX ATC\nFacility\nNational\nFlight Data\nCenter\nJeppesen\nFlight\nCrew\nCertification, Regulation,\nMonitoring & Inspection\nProcedures, Staffing, Budget\nAircraft Clearance and\nMonitoring\nCharts, NOTAM Data\n(except \"L\") to Customer\nRead backs, Requests\nLocal\nNOTAMs\nReports, Project Plans\nNOTAM Data\nChart Discrepancies\nATIS & \"L\" NOTAMs\nOperational Reports\nALPA\nSafety\nALR\nAirport\nDiagram\nAirport\nDiagram\nVerification\nOptional construction\nsignage\n= missing feedback lines\nCertification, Inspection,\nFederal Grants\nComposite Flight Data, except \"L\" NOTAM\nGraphical Airport Data\nATO:\nTerminal\nServices\nPilot perspective\ninformation\nConstruction information\nBlue Grass Airport\nAuthority\nProcedures &\nStandards\nFlight release, Charts etc.\nNOTAMs except \"L\"\nIOR, ASAP\nReports\nCertification & Regulation\nCourtesy of Lund University. Used with permission.\n\nFAA Airport Safety & Standards Office\nSafety Requirements and Constraints:\n-\nEstablish airport design, construction, maintenance, operational\nand safety standards and issue operational certificates accordingly.\n-\nEnsure airport improvement project grant compliance and release\nof grant money accordingly.\n-\nPerform airport inspections and surveillance. Enforce compliance if\nproblems found.\n-\nReview and approve Safety Plans Construction Documents in a\ntimely manner, consistent with safety.\n-\nAssure all stake holders participate in developing methods to\nmaintain operational safety during construction periods.\n\nAirport Safety & Standards Office\nUnsafe Control Actions:\n-\nThe FAA review/acceptance process was inconsistent, accepting\nthe original phase IIIA (Paving and Lighting) Safety Plans\nConstruction Documents and then rejecting them during the\ntransition between phases II and IIIA.\n-\nDid not require all stake holders (i.e. a Pilot representative was not\npresent) be part of the meetings where methods of maintaining\noperational safety during construction were decided.\n-\nFocused on inaccurate runway length depiction without\nconsideration of taxiway discrepancies.\n-\nDid not require methods in addition to NOTAMs to assure safety\nduring periods of construction when difference between LEX\nAirport physical environment and LEX Airport charts.\n\nAirport Safety & Standards Office\nProcess Model Flaws\n-\nDid not believe pilot input was necessary for development of safe\nsurface movement operations.\n-\nNo recognition of negative effects of changes on safety.\n-\nBelief that the accepted practice of using NOTAMs to advise crews\nof charting differences was sufficient for safety.\nContext in Which Decisions Made:\n-\nPriority was to keep Airport Facility Directory accurate.\n\nStandard and Enhanced Hold Short\nMarkings\nCourtesy of Lund University. Used with permission.\n\nFederal Aviation\nAdministration\nComair: Delta\nConnection\nAirport Safety &\nStandards District\nOffice\nLEX ATC\nFacility\nNational\nFlight Data\nCenter\nJeppesen\nFlight\nCrew\nCertification, Regulation,\nMonitoring & Inspection\nProcedures, Staffing, Budget\nAircraft Clearance and\nMonitoring\nCharts, NOTAM Data\n(except \"L\") to Customer\nRead backs, Requests\nLocal\nNOTAMs\nReports, Project Plans\nNOTAM Data\nChart Discrepancies\nATIS & \"L\" NOTAMs\nOperational Reports\nALPA\nSafety\nALR\nAirport\nDiagram\nAirport\nDiagram\nVerification\nOptional construction\nsignage\nCertification, Inspection,\nFederal Grants\nComposite Flight Data, except \"L\" NOTAM\nGraphical Airport Data\nATO:\nTerminal\nServices\nPilot perspective\ninformation\nConstruction information\nBlue Grass Airport\nAuthority\nProcedures &\nStandards\nFlight release, Charts etc.\nNOTAMs except \"L\"\nIOR, ASAP\nReports\nCertification & Regulation\n= missing feedback lines\nCourtesy of Lund University. Used with permission.\n\nLEX Controller Operations\nSafety Requirements and Constraints\n-\nContinuously monitor all aircraft in the jurisdictional airspace and\ninsure clearance compliance.\n-\nContinuously monitor all aircraft and vehicle movement on the\nairport surface and insure clearance compliance.\n-\nProvide clearances that clearly direct aircraft for safe arrivals and\ndepartures.\n-\nProvide clearances that clearly direct safe aircraft and vehicle\nsurface movement.\n-\nInclude all Local NOTAMs on the ATIS broadcast.\n\nLEX Controller Operations (2)\nUnsafe Control Actions\n-\nIssued non-specific taxi instructions; i.e. \"Taxi to runway 22\" instead\nof \"Taxi to runway 22 via Alpha, cross runway 26\".\n-\nDid not monitor and confirm 5191 had taxied to runway 22.\n-\nIssued takeoff clearance while 5191 was holding short of the wrong\nrunway.\n-\nDid not include all local NOTAMs on the ATIS\n\nMental Model Flaws\n-\nHazard of pilot confusion during North end taxi operations was\nunrecognized.\n-\nBelieved flight 5191 had taxied to runway 22.\n-\nDid not recognize personal state of fatigue.\nContext in Which Decisions Made\n-\nSingle controller for the operation of Tower and Radar functions.\n-\nThe controller was functioning at a questionable performance level\ndue to sleep loss fatigue\n-\nFrom control tower, thresholds of runways 22 and 26 appear to\noverlap\n\nLEX Air Traffic Control Facility\nSafety Requirements and Constraints\n-\nResponsible for the operation of Class C airspace at LEX airport.\n-\nSchedule sufficient controllers to monitor all aircraft with in\njurisdictional responsibility; i.e. in the air and on the ground.\nUnsafe Control Actions\n-\nDid not staff Tower and Radar functions separately.\n-\nUsed the fatigue inducing 2-2-1 schedule rotation for controllers.\n\nLEX Air Traffic Control Facility (2)\nMental Model Flaws\n-\nBelieved \"verbal\" guidance requiring 2 controllers was merely a\npreferred condition.\n-\nControllers would manage fatigue resulting from use of the 2-2-1\nrotating shift.\nContext in Which Decisions Made\n-\nRequests for increased staffing were ignored.\n-\nOvertime budget was insufficient to make up for the reduced\nstaffing.\n\nAir Traffic Organization: Terminal Services\nSafety Requirements and Constraints\n-\nEnsure appropriate ATC Facilities are established to safely and\nefficiently guide aircraft in and out of airports.\n-\nEstablish budgets for operation and staffing levels which maintain\nsafety guidelines.\n-\nEnsure compliance with minimum facility staffing guidelines.\n-\nProvide duty/rest period policies which ensure safe controller\nperformance functioning ability.\nUnsafe Control Actions\n-\nIssued verbal guidance that Tower and Radar functions were to be\nseparately manned, instead of specifying in official staffing policies.\n-\nDid not confirm the minimum 2 controller guidance was being\nfollowed.\n-\nDid not monitor the safety effects of limiting overtime.\n\nProcess Model Flaws\n-\nBelieved \"verbal\" guidance (minimum staffing of 2 controllers) was\nclear.\n-\nBelieved staffing with one controller was rare and if it was\nunavoidable due to sick calls etc., that the facility would coordinate\nthe with Air Route Traffic Control Center (ARTCC) to control traffic.\n-\nBelieved limiting overtime budget was unrelated to safety.\n-\nBelieved controller fatigue was rare and a personal matter, up to\nthe individual to evaluate and mitigate.\nContext in Which Decisions Made\n-\nBudget constraints.\n-\nAir Traffic controller contract negotiations.\nFeedback\n-\nVerbal communication during quarterly meetings.\n-\nNo feedback pathways for monitoring controller fatigue.\n\nFederal Aviation\nAdministration\nComair: Delta\nConnection\nAirport Safety &\nStandards District\nOffice\nLEX ATC\nFacility\nNational\nFlight Data\nCenter\nJeppesen\nFlight\nCrew\nCertification, Regulation,\nMonitoring & Inspection\nProcedures, Staffing, Budget\nAircraft Clearance\nand Monitoring\nCharts, NOTAM Data\n(except \"L\") to Customer\nRead backs, Requests\nLocal\nNOTAMs\nReports, Project Plans\nNOTAM Data\nChart Discrepancies\nATIS & \"L\" NOTAMs\nOperational Reports\nALPA\nSafety\nALR\nAirport\nDiagram\nAirport\nDiagram\nVerification\nOptional construction\nsignage\n= missing feedback lines\nCertification, Inspection,\nFederal Grants\nComposite Flight Data, except \"L\" NOTAM\nGraphical Airport Data\nATO:\nTerminal\nServices\nPilot perspective\ninformation\nConstruction information\nBlue Grass Airport\nAuthority\nProcedures &\nStandards\nFlight release, Charts etc.\nNOTAMs except \"L\"\nIOR, ASAP\nReports\nCertification & Regulation\nCourtesy of Lund University. Used with permission.\n\nFederal Aviation Administration\nSafety Requirements and Constraints\n-\nEstablish and administer the National Aviation Transportation\nSystem.\n-\nCoordinate the internal branches of the FAA, to monitor and\nenforce compliance with safety guidelines and regulations.\n-\nProvide budgets which assure the ability of each branch to operate\naccording to safe policies and procedures.\n-\nProvide regulations to ensure safety critical operators can function\nunimpaired.\n-\nProvide and require components to prevent runway incursions.\n\nUnsafe Control Actions:\n-\nController and Crew duty/rest regulations were not updated to be\nconsistent with modern scientific knowledge about fatigue and its\ncauses.\n-\nRequired enhanced taxiway markings at only 15% of air carrier\nairports: those with greater than 1.5 million passenger\nenplanements per year.\nMental Model Flaws\n-\nEnhanced taxiway markings unnecessary except for the largest US\nairports.\n-\nCrew/controller duty/rest regulations are safe.\nContext in Which Decisions Made\n-\nFAA funding battles with the US congress.\n-\nIndustry pressure to leave duty/rest regulations alone.\n\nNTSB \"Findings\"\nProbable Cause:\n-\nFC's failure to use available cues and aids to identify the\nairplane's location on the airport surface during taxi\n- FC's failure to cross-check and verify that the airplane was\non the correct runway before takeoff.\n- Contributing to the accident were the flight crew's\nnonpertinent conversation during taxi, which resulted in a\nloss of positional awareness,\n- Federal Aviation Administration's (FAA) failure to require that\nall runway crossings be authorized only by specific air traffic\ncontrol (ATC) clearances.\n\nCopyright Nancy Leveson, Aug. 2006\n\nhospital reports, input from medical community\nreports\nreports\nIACEsl\nI t\nv10E int\n~cl In\nBGOS Medical\n!\nMinistry of\nrei 'lrts\nAdvisories, warnings\nbudgets, laws\nHealth\nDept. of Health\nPublic Health\nreOI lations ,\nregulatory polic\nGuidelines\nrr port\" ... .. ... .. ... ...... .....\nstatus\n\".......... .. : . L water samples :\nrequests\nFederal\n:\n... .......: GovernmL'nt :\nand\nProvincial\nguidelines\n: T ,t'\nL b :\nreports\nreport\n. e ling a ..................\nGovernment\n:\nrep.01.s···.·: .. .. .. .. .. ..\n:\ncontaminants\nwater samples\n................. .... .... .... .... .... :\n:\n: ,, In' pection and other reports\nI\nbudgets, laws\n:\nchlorine residual measurement\nregulatory polic\n:\n:\nWater system\nMinistry of\n,\nODWO,Chlorination Bulletin\nthe Envi ronment\nCertificates of Approval\nreports\nOperator certification\nwater\nWalkerton PUC\nchlorination\nMinistry of\noperations\nWell 7\nWell 5\nPoli(\n~s\nAgriculture,\nWPUC Commissioners\nWell\nDesign flaw:\nDesign flaw:\nBudget\nselection\nNo chlorinator\nShallow location\nbudgets, laws\nFood, and\nRural Affairs\nOversight\nI\nPorous bedrock\nFinancial Info.\nMinimal overburden\nHeav rains\nWalkerton\nPrivate\nResidents\nTesting Lab\nFarm\n\nCommunication Links Theoretically in\nPlace in Uberlingen Accident\n\nCommunication Links Actually in Place\n\nSummary\n- A \"why\" analysis, not a \"blame\" analysis\n- Construct the safety control structure as it was designed to\nwork\n- Component responsibilities (requirements)\n- Control actions and feedback loops\n- For each component, determine if it fulfilled its responsibilities\nor provided inadequate control.\n- If inadequate control, why? (including changes over time)\n- Context\n- Process Model Flaws\n- For humans, why did it make sense for them to do what they\ndid (to reduce hindsight bias)\n- Examine coordination and communication\n\nSummary (2)\n- Consider dynamics (changes in control structure) and\nmigration to higher risk\n- Determine the changes that could eliminate the inadequate\ncontrol (lack of enforcement of system safety constraints) in\nthe future.\n- Generate recommendations\n- Continuous Improvement\n- Assigning responsibility for implementing recommendations\n- Follow-up to ensure implemented\n- Feedback channels to determine whether changes effective\n- If not, why not?\n\nConclusions\n- The model used in accident or incident analysis determines\nwhat we what look for, how we go about looking for \"facts\", and\nwhat facts we see as relevant.\n- A linear chain of events promotes looking for something that\nbroke or went wrong in the proximal sequence of events prior\nto the accident.\n- A stopping point, often, is arbitrarily determined at the point\nwhen something physically broke or an operator \"error\" (in\nhindsight) occurred.\n- Unless we look further, we limit our learning and almost\nguarantee future accidents related to the same factors.\n- Goal should be to learn how to improve the safety control\nstructure\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: Design for Safety",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/3285d0ecac6cb1ff5cef4dbdca5afda2_MIT16_863JS16_LecNotes7.pdf",
      "content": "Basic Design for Safety\nPrinciples\n\nDesigning to Prevent Accidents\n- Standards and codes of practice contain lessons learned\nfrom the past\n- Standard precedence\n- Try to eliminate hazards from the design\n- Identify causes of hazards and try to reduce their likelihood\nof occurring through design\n- Control hazards once they occur\n- Design to reduce damage\n\n(c) Copyright Nancy Leveson, Aug. 2006\n\nHazard Elimination\n- Substitution\n- Use safe or safer materials\n- Nontoxins, non-explosives\n- Chlorine blower example\n- Simplification\n- Minimize parts, modes, interfaces\n- Reduce \"unknowns\"\n- Computers make it easy to build dangerously complex\nsystems\n\nA cartoon from Rube Goldberg vs. the Machine Age by Reuben L. Goldberg is removed due to copyright restrictions.\n\nElimination (2)\n-\nDecoupling:\n- Tightly coupled system is one that is highly interdependent\n- Coupling increases number of interfaces and leads to unplanned\ninteractions\n- Computers tend to increase system coupling unless very careful.\n-\nReduce opportunities for human error\n- Make impossible or possible to detect immediately\n- Examples: wiring errors (color code, female/male connectors),\ntypos, making displays readable, showing status of plant\n-\nReduce hazardous materials or conditions\n- Example: keep fewer hazardous chemicals on hand\n\nHazard Reduction\n- Try to minimize likelihood of hazard occurring\n\nPassive vs. Active Protection\n-\nPassive safeguards:\n- Maintain safety by their presence\n- Fail into safe states\n-\nActive safeguards:\n- Require hazard or condition to be detected and corrected\nTradeoffs\n- Passive rely on physical principles\n- Active depend on less reliable detection and recovery\nmechanisms\nBUT\n- Passive tend to be more restrictive in terms of design freedom\nand not always feasible to implement\n\nFail-Safe (Passive) Safeguards Examples\n- Design so system fails into a safe state\nExamples:\n- Deadman switch\n- Magnetic latch on refrigerators\n- Railroad semaphores: if cable breaks, fails into STOP position\n- Cover over a high-energy source with circuit run through it\n- Relays or valves designed to fail open or fail safe\n- Air brakes: held in off position by air pressure. If line breaks, lose\nair pressure and brakes applied\n- Early Apollo program: use free return trajectory. If engines failed\nat lunar orbit insertion, spacecraft safely coasts back to earth\n\nMore Examples\n-\nRetractible landing gear: wheels drop and lock into position if\nsystem that raises and lowers them fails (e.g., pneumatic pressure\nsystem)\n-\nElevator: if hoist cables fail, safety mechanism wedges into guide\nrails\n-\nBathyscope: ballast held in place by magnets. If electrical power\nlost, ballast released and ascends to surface\n-\nRailway signalling systems: signals not in use kept in \"danger\"\nposition. Positive action required (setting signal to clear) is required\nbefore train can pass.\n-\nDesign cars so drivable with one flat tire. Also \"run-flat tires\" with\nsolid rubber core\n\nDesign for Controllability\n- Make system easier to control, both for humans and\ncomputers\n- Use incremental control\n- Perform critical steps incrementally rather than in one step\n- Provide feedback\n- To test validity of assumptions and models upon which\ndecisions are made\n- To allow taking corrective action before significant damage is\ndone\n- Provide various types of fallback or intermediate states\n- Lower time pressures\n- Provide decision aids\n\nMonitoring\n-\nTo detect a problem need to\n- Check conditions that are assumed to indicate a potential\nproblem\n- Validate or refute assumptions made during design and analysis\n-\nCan be used to indicate\n- Whether a specific condition exists\n- Whether a device ready for operation or operating satisfactorily\n- Whether required input is being provided\n- Whether a desired or undesired output is being generated\n- Whether a specific limit being exceeded or whether a measured\nparameter is abnormal\n-\nNeed to design for checkability and inspectability\n\nMonitoring (2)\n-\nTwo ways to detect equipment malfunction:\n- Monitor equipment performance (requires redundant info)\n- Monitor equipment condition\n-\nIn general, monitors should\n- Detect problems as soon as possible\n- Be independent from devices they are monitoring\n- Add as little complexity to system as possible\n- Be easy to maintain, check, and calibrate\n- Be easily interpreted by operators (e.g., mark limits on dials)\n\nLimitations of Monitoring\n- Difficult to make monitors independent\n- Checks usually require access to information being\nmonitored, but usually involves possibility of corrupting that\ninformation\n- Depends on assumptions about behavior of system and\nabout errors that may or may not occur\n- May be incorrect under certain conditions\n- Common incorrect assumptions may be reflected both in\ndesign of monitor and devices being monitored.\n\nBarriers\nLockout\n-\nMake access to dangerous state difficult or impossible\n-\nFences and physical barriers to block access to a dangerous\ncondition (sharp blades, heated surfaces, high-voltage\nequipment)\n-\nLogical barriers (authority limiting, software firewalls)\n\nBarriers (2)\nLockin\n-\nMake it difficult or impossible to leave a safe state, maintain a\nsafe condition\n-\nPossible uses:\n- Keep humans within an enclosure, e.g., seatbelts and\nharnesses, doors on elevators\n- Contain harmful products or byproducts, e.g., electromagnetic\nradiation, pressure, noise, toxins, ionizing radiation\n- Contain potentially harmful objects, e.g., cages around an\nindustrial robot in case it throws something\n- Maintain a controlled environment (e.g., buildings, spacecraft,\nspace suits, diving suits)\n- Maintain a safe state (e.g. speed governors, relief valves to\nmaintain pressure below dangerous levels)\n\nBarriers (3)\nInterlock\n-\nUsed to enforce a sequence of actions or events\n1.\nEvent A does not occur inadvertently\n2.\nEvent A does not occur while condition C exists\n3.\nEvent A occurs before event D\n-\n(1) and (2) are called \"inhibits\", (3) is a \"sequencer\"\n-\nExamples:\n-\nPressure sensitive mat or light curtain that shuts off a robot if someone\ncomes near\n-\nDeadman switch\n-\nGuard gates and signals at railway crossings\n\nBarriers (4)\n-\nExamples (con't):\n-\nDevice on machinery to ensure all prestart conditions met, correct\nstartup sequence followed, conditions for transitions between phases\nare met\n-\nDevice to ensure correct sequencing of valve turn-off or turn-on or both\nnot on or off at same time.\n-\nDevices to preventing disarming a trip (protection) system unless\ncertain conditions occur first or to prevent system from being left in\ndisabled state after testing or maintenance\n-\nDisabling car ignition unless automatic shift in PARK\n-\nFreeze plug in a car's engine cooling system (expansion will force plug\nout rather than crack cylinder if water in block freezes)\n-\nFusible plug in boiler becomes exposed if excessive heat and water\nlevel drops below predetermined level. Plug melts, opening permits\nsteam to escape, reduces pressure in boiler, and prevents explosion.\n\nBarriers (5)\n-\nDesign Considerations for interlocks\n-\nDesign so hazardous functions stop if interlock fails\n-\nIf interlock brings something to a halt, provide status and alarm\ninformation to indicate which interlock failed.\n-\nIf use interlock during maintenance or testing, must preclude\ninadvertent interlock overrides or being left inoperative once system\nbecomes operational again.\n-\nWhen computers introduced, physical interlocks may be defeated or\nomitted.\n-\nSoftware programmers may not understand physical devices they are\nreplacing.\n-\nMay still need physical interlocks to protect against software errors.\n-\nMake sure in safe state when resume operation, don't just start from where\nleft off.\nRemember, the more complex the design, the more likely errors\nor hazards will be introduced by the protection facilities\nthemselves.\n\nFault or Failure Tolerance\n-\nGoal is to \"tolerate\" faults so they have no or little negative\nimpact\n- Isolation or independence: so that misbehavior of one\ncomponent does not negatively affect behavior of another\n- Failure warnings and indicators: to provide early detection of\nfailures so preventive actions can be taken\n- Carefully designed and practiced flight crew procedures to\nenable safe flight and landing when problems occur\n- Design to tolerate human error\n- Physical damage tolerance: ability to sustain damage without\nhazard resulting.\n- Eliminate impact of common hardware failures on software\n- E.g., do not use 1 or 0 to denote safe vs. armed\n\nRedundancy\n-\nGoal is to increase component reliability and reduce\ncomponent failures\n-\nStandby spares vs. concurrent use of multiple devices (with\nvoting)\n-\nIdentical designs or intentionally different ones (diversity)\n-\nDiversity must be carefully planned to reduce dependencies\n-\nCan also introduce dependencies in maintenance, testing,\nrepair\n\nRedundancy\n\nRedundancy (2)\n- Identical designs or intentionally different ones (diversity)\n- Diversity must be carefully planned to reduce\ndependencies\n- Problem is potential lack of independence\n- Common mode failures: fail in same way, causes may be\ndifferent\n- Common cause failure: Fail due to same cause\n- Can also introduce dependencies in maintenance,\ntesting, repair\n\nRedundancy (3)\n-\nLimitations\n- Common-cause and common-mode failures\n- May add so much complexity that causes failures\n- More likely to operate spuriously\n- May lead to false confidence (Challenger)\n- Extra costs including maintenance and extra weight\n-\nUseful to reduce hardware failures. But what about software?\n- Ariane 5 loss\n- Design redundancy vs. design diversity\n- Bottom line: Claims that multiple version software will achieve\nultra-high reliability levels are not supported by empirical data or\ntheoretical models\n\nSoftware Redundancy\n-\nSoftware errors are design errors\n-\nData redundancy: extra data for detecting errors:\ne.g., parity bit and other codes\nchecksums\nmessage sequence numbers\nduplicate pointers and other structural information\n-\nAlgorithmic redundancy:\n1. Acceptance tests (hard to write)\n2. Multiple versions with voting on results\n3. Found to have lots of common faults\n\nSoftware Recovery\n-\nBackward\n- Assume can detect error before does any damage\n- Assume alternative will be more effective\n-\nForward\n- Robust data structures\n- Dynamically altering flow of control\n- Ignoring single cycle errors\n-\nBut real problem is detecting erroneous states\n\nExample: Nuclear Detonation\n-\nSafety depends on NOT working\n-\nThree basic techniques (callled \"positive measures\")\n1.\nIsolation\n-\nSeparate critical elements\n2.\nInoperability\n-\nKeep in inoperable state, e.g., remove ignition device or\narming pin\n3.\nIncompatibility\n-\nDetonation requires an unambiguous indication of human\nintent be communicated to weapon\n-\nProtecting entire communication system against all credible\nabnormal environments (including sabotage) not practical.\n-\nInstead, use unique signal of sufficient information\ncomplexity that unlikely to be generated by an abnormal\nenvironment\n\nExample: Nuclear Detonation (2)\n-\nUnique signal discriminators must\n1.\nAccept proper unique signal while rejecting spurious inputs\n2.\nHave rejection logic that is highly immune to abnormal\nenvironments\n3.\nProvide predictable safe response to abnormal\nenvironment\n4.\nBe analyzable and testable\n-\nProtect unique signal sources by barriers\n-\nRemovable barrier between these sources and\ncommunication channels\n\nExample: Nuclear Detonation (3)\nImage by MIT OpenCourseWare.\nDiagram\nshowing\nthe var\nious\nsafety\nsystems protectin\ng against nuclear deto\nnation.\n\nExample: Nuclear Detonation (4)\nFlowchar\nt wit\nh thre\ne columns show\ning the stimul\ni sour\nce, co\nmmunic\nation\nsyste\nm, and\nsafi\nng and\nfirin\ng system.\nImage by MIT OpenCourseWare.\n\nHazard Control\n- Detect hazard and control it before damage occurs\n- May be able to reverse it before necessary environmental\nconditions occur\n- Resources (physical and informational, such as diagnostics\nand status information) may be needed to control hazard\n- First need to detect hazard\n- Warning signals should be not present for too long or too\nfrequently (people become insensitive to constant stimuli)\n- Do not assume hazard will never occur because of other\nprotection devices or because software \"never fails\"\n\nHazard Control\nLIMITING EXPOSURE (level or duration of hazard)\n- Stay in safe state as long and as much as possible\ne.g., nitroglycerine used to be manufactured in a large batch\nreactor. Now made in small continuous reactor and residence\ntime reduced from 2 hours to 2 minutes.\n- Start out in safe state and require deliberate change to\nunsafe state\ne.g., arm missile only when near target\nNPP shutdown software keeps variables in \"trip\" state and\nrequires change to non-trip.\n- Critical conditions should not be complementary, e.g.,\nabsence of an arm condition should not be used to indicate\nsystem is unarmed\n\nHazard Control\nISOLATION AND CONTAINMENT\n- Provide barriers between system and environment\ne.g., containment vessels and shields\n- Isolate away from people: Very hard to maintain over time\n\nPROTECTION SYSTEMS AND FAIL-SAFE DESIGN\n- Move system to a safe or safer state\n- Requires existence of a safe state (shutdown in NPP, sleep state in\nspacecraft cruise mode)\n- Also requires an early warning with enough time between detection\nof hazard and actual loss event\n\nFail-Safe Design in Aviation\n-\nDesign integrity and quality\n-\nRedundancy\n-\nIsolation (so failure in one component does not affect another)\n-\nComponent reliability enhancement\n-\nFailure indications (telling pilot a failure has occurred, may\nneed to fly plane differently)\n-\nSpecified flight crew procedures\n\nFail-Safe Design in Aviation (2)\n-\nDesign for checkability and inspectability\n-\nFailure containment\n-\nDamage tolerance\n- Systems surrounding failures should be able to tolerate\nthem in case failure cannot be contained\n-\nDesigned failure paths\n- Direct high energy failure that cannot be tolerated or\ncontained to a safe path\n- E.g. use of structure \"fuses\" in pylons so engine will fall off\nbefore it damages the structure\n\nProtection Systems and Fail-Safe Design\n-\nMay have multiple safe states, depending on process\nconditions\n-\nGeneral rule is hazardous states should be hard to get into\nand safe states should be easy\n-\nTypical protective equipment:\n- Gas detectors\n- Emergency isolation valves\n- Trips and alarms\n- Relief valves and flare stacks\n- Water curtains\n- Firefighting equipment\n- Nitrogen blanketing\n\nProtection Systems and\nFail-Safe Design (2)\n-\nPanic Button: stops a device quickly, perhaps by cutting off power\n- Must be within reach when needed\n- Operators must be trained to react quickly to unexpected events\n-\nPassive devices better than active again\n-\nWatchdog timer: Timer that system must keep restarting. If not then\ntakes protective action\n-\nSanity checks (I'm alive signals): detects failure of computers\n-\nProtection system should provide information about its control\nactions and status to operators or bystanders.\n-\nFailure containment: limit effects of failure or hazard to local area\n\n-\nDesigned failure path: direct failure along a less critical path\n- Example: jet engine mounted on wing by a pylon structure. Severe\nengine unbalance caused by loss of a number of fan blades from\n\"foreign object ingestion\" could destroy wing. But pylon and engine\nmount system designed to fail under these loads before main wing\nstructure, allowing engine to fall harmlessly from airplane.\n-\nThe easier and faster is return of system to operational state, the\nless likely protection system will be purposely bypassed or turned off\n-\nTry to control hazard while causing least damage in process\n-\nMay need to do more than simply shut down, e.g., blowing up an\nerrant rocket.\n- Such facilities may do harm themselves, e.g., French weather balloon\nemergency destruct facility, if inadvertently initiated\n\nProtection Systems and\nFail-Safe Design (3)\n\nProtection Systems and\nFail-Safe Design (4)\n-\nMay design various types of fallback states\n- e.g., traffic lights that fail to blinking red or yellow states, unstable\naircraft have analog backup devices because cannot be flown manually\n(but less functionality)\n-\nTypes of fallback states:\n- Partial shutdown (partial or degraded functionality)\n- Hold (no functionality provided, but steps taken to maintain safety or\nlimit amount of damage)\n- Emergency shutdown (system shut down completely)\n- Manually or externally controlled\n- Restart (system in transitional state from non-normal to normal)\n-\nConditions under which each of fallback states should be invoked\nmust be determined, along with how transitions between states will\nbe implemented and controlled.\n\nProtection Systems and\nFail-Safe Design (5)\n-\nMay need multiple types of shutdown procedures\n- Normal emergency stop (cut power from all circuits)\n- Production stop (stop after current task completed)\n- Protection stop (shut down immediately but not necessarily by cutting\npower from circuits, which could result in damage).\n-\nIf cannot design to fail into safe state or passively change to safe\nstate, the hazard detectors must be of ultra-high reliability.\n- May add equipment to test detection system periodically by simulating\ncondition sensor is supposed to detect (e.g., challenge system)\n- Challenge system must not obscure a real hazard and must be\nindependent from monitor system\n\nProtection Systems and\nFail-Safe Design (6)\n-\nHazard detection system may have three subsystems:\n- Sensor to detect hazardous condition\n- Challenge subsystem to exercise and test sensor\n- Monitor subsystem to watch for any interruption of challenge-\nand-response sequence.\n-\nNote that complexity creeping up, decreasing probability\nthese protection facilities will work when needed.\n\nDamage Reduction\n-\nIn emergency, may not be time to assess situation, diagnose what is\nwrong, determine correct action, and then carry out action.\n- Need to prepare emergency procedures and practice them\n- May need to determine a \"point of no return\" where recovery no longer\npossible or likely and should just try to minimize damage.\n-\nDistinguish between warnings used for drills and those for real\nemergencies\n-\nDamage minimization includes\n- Escape routes\n- Safe abandonment of products and materials (e.g., hazardous waste\ndisposal)\n- Devices for limiting damage to people or equipment (e.g., blowout panels\nand frangible walls, collapsible steering columns on cars, sheer pins in\nmotor-driven equipment\n\nDesign Modification and Maintenance\n-\nNeed to re-analyze safety for every proposed/implemented\nchange\n-\nRecording design rationale from beginning and traceability will\nhelp.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: Introduction, Causality, Bhopal, Hindsight Bias",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/3bd61d02522835a834be4b6e245b0961_MIT16_863JS16_LecNotes1.pdf",
      "content": "System Safety Introduction\n\nUberlingen Mid-Air Collision\n\nhttps://www.youtube.com/watch?v=CHjqun9c4Pc\n\n(khchung787)\n\nWhat were some of the causal factors\nin the Uberlingen accident?\n\nUncoordinated \"Control Agents\"\nControl Agent\n(ATC)\nInstructions\nInstructions\n\"SAFE STATE\"\nATC provides coordinated instructions to both planes\n\"SAFE STATE\"\nTCAS provides coordinated instructions to both planes\nControl Agent\n(TCAS)\nInstructions\nInstructions\n\"UNSAFE STATE\"\nBOTH TCAS and ATC provide uncoordinated & independent instructions\nControl Agent\n(ATC)\nInstructions\nInstructions\nNo Coordination\n\nCommunication Links Theoretically in\nPlace in Uberlingen Accident\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nCommunication Links Actually in Place\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nUberlingen continued\n- A year prior there was a near miss due to conflicting\nTCAS and ATC commands\n- Two Japanese airliners\n- One pilot made evasive maneuvers based on visual\njudgement.\n- Aircraft came within 300 ft\n- Evasive maneuvers caused ~100 injuries\n- Japan called for changes, but ICAO did not take action\nuntil after Uberlingen\n- Four other near misses in Europe before Uberlingen\ncollision\n\nUberlingen continued\n- TCAS Pilot's Guide was ambiguous about TCAS / ATC\nprecedence\n- Tu-154 Flight Operations Manual had contradictory\nsections\n- Chapter 8.18.3.2 forbids maneuvers contrary to TCAS\n- Chapter 8.18.3.4 says \"most important tool\" is executing\nATC instructions. TCAS described as an additional\ninstrument.\n\nSyllabus\nAssignments and Grading\n- Reading assignments and exercises\n- Two group assignments: accident analysis and project\n- One take home exam (on accident analysis and hazard\nanalysis)\nTextbooks and Readings:\n- \"Safeware\"\n- \"Engineering a Safer World\" (published by MIT Press\n-\nDownload free from the MIT Press website\n- STPA Primer (draft) for reference only\n- Optional readings just if you are interested\n\nAcident Causes are Complex\n-\nThe vessel Baltic Star, registered in Panama, ran aground at full\nspeed on the shore of an island in the Stockholm waters on\naccount of thick fog. One of the boilers had broken down, the\nsteering system reacted only slowly, the compass was\nmaladjusted, the captain had gone down into the ship to\ntelephone, the lookout man on the bow took a coffee break, and\nthe pilot had given an erroneous order in English to the sailor\nwho was tending the rudder. The latter was hard of hearing and\nunderstood only Greek.\nLe Monde\n\nWere there also larger organizational and economic factors?\n\nComponents of System Safety\nEngineering\n- Investigating accidents\n- Preventing Accidents\n- Hazard Analysis\n- Design for Safety\n- Operations\n- Management\n\nInvestigating/Understanding Accidents\n- What are ALL the factors involved?\n- Are there tools to help us find all the factors?\n- How do we minimize hindsight bias?\n- How do we learn from accidents in order to prevent them\nin the future?\n\nHazard Analysis\n- \"Investigating an accident before it occurs\"\n- Identify potential scenarios\n- Worst case analysis vs. average (expected) case\nanalysis\n- Use results to prevent losses\n\nDesign for Safety\n- Eliminate or control scenarios (causal factors) identified\nby hazard analysis\n- Fault Tolerance\n- Failures will occur\n- Need to make sure they don't result in an accident\n- Design to prevent operator error\n- Human errors will occur\n- Need to make sure they don't result in an accident\n- Design so that don't induce human error\n\nUnderstanding Accident\nCausality\n\nBhopal\n- Worst industrial accident in history\n- Conservative estimate of 2000-3000 killed, 10,000\npermanent disabilities (including blindness), and 200,000\ninjured.\n- Blamed by management on operator error\n- Union Carbide blamed on sabotage\n- MIC (methyl isocyanate) used in production of\npesticides and polyurathanes (plastics, varnishes, and\nfoams)\n- Highly volatile, vapor heavier than air\n- A major hazard is contact with water, which results in\nlarge amounts of heat.\n- Gas burns any moist part of body (throat, eyes, lungs)\n\nSafety Features\n- UC specified requirements to reduce hazards:\n- MIC was to be stored in underground tanks encased in\nconcrete\n- Bhopal used three double-walled, stainless steel tanks,\neach with a capacity of 60 tons.\n- Operating manual specified that tanks were never to\ncontain more than half their maximum volume or a standby\ntank was to be available to which some of chemical could\nbe transferred in case of trouble.\n- Bhopal tanks were interconnected so that MIC in one tank\ncould be bled into another tank.\n- As specified in operating manual, tanks embedded in\nconcrete.\n\nSafety Features (con't)\n- Several backup protection systems and lines of defense\n- Vent gas scrubber designed to neutralize any escaping gas\nwith caustic soda. Scrubber was capable of neutralizing\nabout 8 tons of MIC per hour at full capacity\n- Flare tower to burn off any escaping gas missed by scrubber;\ntoxic gases would be burned high in the air, making them\nharmless\n- Small amounts of gas missed by scrubber and flare tower\nwere to be knocked down by a water curtain that reached 40\nto 50 feet above ground. Water jets could reach as high as\n115 feet, but only if operated individually.\n- In case of an uncontrolled leak, a siren was installed to warn\nworkers and surrounding community.\n\nSafety Features (con't)\n- MIC was to be stored in an inert atmosphere of nitrogen\ngas at 2 to 10 psi over atmospheric pressure.\n- Regularly scheduled inspection and cleaning of valves\nspecified as imperative\n- Storage limited to 12 months maximum.\n- If staff were doing sampling, testing, or maintenance at a\ntime when there was a possibility of a leak or spill,\noperating manual specified they were to use protective\nrubber suits and air-breathing equipment.\n- To limit its reactivity, MIC was to be maintained at a\ntemperature near 0 C.\n- Refrigeration unit provided for this purpose\n- High temperature alarm if MIC reached 11 C.\n\nHierarchical models\n\nEvents at Bhopal\n- Dec. 2, 1984, relatively new worker assigned to wash out\nsome pipes and filters, which were clogged.\n- Pipes being cleaned were connected to the MIC tanks by\na relief valve vent header, normally closed\n- Worker closed valve to isolate tanks but nobody inserted\nrequired safety disk (slip blind) to back up valves in case\nthey leaked\n- Maintenance sheet contained no instruction to insert disk\n- Worker assigned task did not check to see whether pipe\nproperly isolated because said it was not his job to do so.\n- He knew valves leaked, but safety disks were job of\nmaintenance department.\n\n- Night shift came on duty at 11 pm.\n- Pressure gauge indicated pressure was rising (10 psi\ninstead of recommended 2 to 3 psi). But at upper end of\nnormal range.\n- Temperature in tank about 20 C.\n- Both instruments were ignored because believed to be\ninaccurate. Operators told instead to use eye irritation as\nfirst sign of exposure.\n- 11:30 pm: detected leak of liquid from an overhead line\nafter some workers noticed slight eye irritation.\n- Leaky valves were common and were not considered\nsignificant\n\n- Workers looked for leak and saw a continuous drip on\noutside of MIC unit.\n- Reported it to the MIC supervisor\n- Shift supervisor did not consider it urgent and postponed an\ninvestigation until after the tea break.\n- 12:40 am on Dec. 3: Control room operator noticed tank\n610 pressure gauge was approaching 40 psi and\ntemperature was at top of scale (25 C)\n- 12:45 am: Loud rumbling noises heard from tank.\nConcrete around tank cracked.\n- Temperature in tank rose to 400 C, causing an increase in\npressure that ruptured relief valve.\n- Pressurized gas escaped in a fountain from top of vent\nstack and continued to escape until 2:30 am.\n\n- MIC vented from stack 108 feet above ground. 50,000\npounds of MIC gas would escape.\n- Operator turned off water-washing line when first heard\nloud noises at 12:45 am and turned on vent scrubber\nsystem, but flow meter showed no circulation of caustic\nsoda.\n- He was unsure whether meter was working\n- To verify flow had started, he would have to check pump\nvisually.\n- He refused to do so unless accompanied by supervisor\n- Supervisor declined to go with him.\n- Operator never opened valve connecting tank 610 to the\nspare tank 619 because level gauge showed it to be\npartially full.\n\n- Assistant plant manager called at home at 1 am and\nordered vent flare turned on. He was told it was not\noperational (out of service for maintenance). A section of\npipe connecting it to the tank was being repaired.\n- Plant manager learned of leak at 1:45 am when called by\nthe city magistrate.\n- When MIC leak was serious enough to cause physical\ndiscomfort to workers, they panicked and fled, ignoring four\nbuses intended for evacuating employees and nearby\nresidents.\n- A system of walkie-talkies, kept for such emergencies,\nnever used.\n\n- MIC supervisor could not find his oxygen mask and ran to\nboundary fence, where he broke his leg attempting to\nclimb over it.\n- Control room supervisor stayed in control room until the\nnext afternoon, when he emerged unharmed.\n- Toxic gas warning siren not activated until 12:50 am when\nMIC seen escaping from vent stack.\n- Turned off after only 5 minutes, which was Union Carbide\npolicy.\n- Remained off until turned on again at 2:30 am.\n- Police were not notified and when they called between 1\nand 2, were given no useful information.\n\n- No information given to public about protective measures\nin case of an emergency or other info on hazards.\n- If had known to stay home, close their eyes, and breathe\nthrough a wet cloth, deaths could have been prevented.\n- Army eventually came and tried to help by transporting\npeople out of area and to medical facilities.\n- This help was delayed because nobody at plant notified\nauthorities about the release\n- Weather and wind contributed to consequences.\n- Because happened in middle of night, most people asleep\nand it was difficult to see what was happening.\n\nWhat were the causes of this accident\ngiven what you know so far?\nWhat additional questions were raised\nby what you have seen so far?\n\nHierarchical models\n\nWhat about all the safety devices\nand procedures?\n- How could the vent scrubber, flare tower, water spouts,\nrefrigeration unit, alarms, and monitoring instruments all\nfail simultanously?\n- Not uncommon for a company to turn off passive safety\ndevices to save money; gauges are frequently out of\nservice.\n- At Bhopal, few alarms, interlocks, or automatic shutoff\nsystems in critical locations that might have warned\noperators of abnormal conditions or stopped the gas leak\nbefore it spread.\n- Thresholds established for production of MIC routinely\nexceeded. e.g., workers said it was common to leave MIC\nin the spare tank.\n\n- Operating manual said refrigeration unit must be operating\nwhenever MIC was in the system\n- Chemical has to be maintained at a temp no higher than 5 C.\nto avoid uncontrolled reactions.\n- High temperature alarm to sound if MIC reached 11 C.\n- Refrigeration unit turned off and MIC usually stored at nearly\n20 C.\n- Plant management adjusted threshold of alarm, accordingly,\nfrom 11 C to 20 C., thus eliminating possibility of an early\nwarning of rising temperatures.\n- Flare tower was totally inadequate to deal with estimated\n40 tons of MIC that escaped during accident.\n- Could not be used anyway because pipe was corroded and\nhad not been replaced.\n\n- Vent scrubber (had it worked) was designed to\nneutralize only small quantities of gas at fairly low\npressures and temperatures.\n- Pressure of escaping gas during accident exceeded\nscrubber's design by nearly 2 1⁄2 times\n- Temperature of escaping gas at least 80 degrees more\nthan scrubber could handle.\n- Shut down for maintenance\n- Water curtain designed to reach height of 40 to 50 feet.\nMIC vapor vented over 100 feet above ground.\n- Practice alerts did not seem to be effective in preparing\nfor an emergency (ran from contaminated areas and\nignored buses sitting idle and ready to evacuate them)\n\n- Pipe-washing operation should have been supervised\nby second shift operator, but that position had been\neliminated due to cost cutting.\n- Tank 610 contained 40 to 50 tons of MIC out of total\ncapacity of 60 tons, which violated safety\nrequirements.\n- Tanks were not to be more than half filled\n- Spare tank was to be available to take excess\n- Adjacent tank thought to contain 15 tons according to\nshipping records, but contained nearer to 21 tons\n- Spare tank (619) contained less than 1 ton, but level\ngauge showed it was 20 percent full\n- Many of gauges not working properly or were\nimproperly set.\n\n- Alarms sounded so many times a week (20 to 30) that no\nway to know what the siren signified\n- Emergency signal was identical to that used for other\npurposes, including practice drills.\n- Not turned on until 2 hours after MIC leak started and then\nturned off after 5 minutes (company policy)\n- Plant workers had only bare minimum of emergency\nequipment, e.g., shortage of oxygen masks discovered\nafter accident started.\n- They had almost no knowledge or training about how to\nhandle non-routine events.\n- Police were not notified when chemical release began\n- When called by police and reporters, plant spokesmen first\ndenied accident and then claimed MIC was not dangerous.\n- Surrounding community not warned or prepared\n\nHas your view of this accident changed with\nthis additional information ?\n\nWhat additional causal factors would you\nnow include?\n\nWhat additional questions would you want\nanswered?\n\nHierarchical models\n\nAdditional Information about\nSystemic Factors\n- Demand for MIC dropped sharply after 1981, leading to\nreductions in production and pressure on company to cut\ncosts.\n- Plant operated at less than half capacity when accident occurred.\n- UC put pressure on Indian subsidiary to reduce losses, but gave\nno specific details about how this was to be done.\n- In response, maintenance and operating personnel cut in\nhalf.\n- Top management justified cuts as merely reducing avoidable and\nwasteful expenditures without affecting overall safety.\n- As plant lost money, many of skilled workers left for more\nsecure jobs. They either were not replaced or replaced by\nunskilled workers.\n\n- Maintenance procedures severely cut back and shift\nrelieving system suspended (if no replacement showed\nup at end of shift, following shift went unmanned).\n- Indian government required plant to be operated\ncompletely by Indians\n- At first, UC flew plant personnel to West Virginia for\nintensive training and had teams of U.S. engineers make\nregular on-site safety inspections.\n- By 1982, financial pressures led UC to give up direct\nsupervision of safety at the plant, even though it retained\ngeneral financial and technical control.\n- No American advisors resident at Bhopal after 1982.\n- Minimal training of many of workers in how to handle\nnon-routine emergencies.\n\n- Several Indian staff who were trained in U.S. resigned and\nwere replaced by less experienced technicians.\n- When plant first built, operators and technicians had\nequivalent of two years of college education in chemistry or\nchemical engineering.\n- In addition, UC provided them with 6 months training.\n- When plant began to lose money, educational standards and\nstaffing levels were reportedly reduced.\n- In 1983, chemical engineer managing MIC plant resigned\nbecause he disapproved of falling safety standards. He\nwas replaced by an electrical engineer.\n\n- Morale at the plant was low. Management and labor\nproblems followed the financial losses.\n\"There was widespread belief among employees that the\nmanagement had taken drastic and imprudent measures to cut\ncosts and that attention to the details that ensure safe operation\nwere absent.\"\n- Five months before accident, local UC India management\ndecided to shut down refrigeration system.\n- Most common reason given was cost cutting.\n- Local management claimed unit was too small and never\nworked satisfactorily.\n- Disagreement about whether UC in U.S. approved this\nmeasure.\n- High temperature alert reset and logging of tank\ntemperatures discontinued.\n\n- Other examples of unsafe conditions that were permitted\nto exist:\n- At time of accident, chloroform contamination of MIC was 4\nto 5 times higher than specified in operating manual, but no\ncorrective action taken.\n- MIC tanks were not leak-tight to a required pressure test.\n- Workers regularly did not wear safety equipment, such as\ngloves or masks because of high temperatures in plant.\nThere was no air conditioning.\n- Inspections and safety audits at the plant were few and\nsuperficial.\n\n- A review and audit of Bhopal plant in 1982 noted many\nof deficiencies involved in accident\n- No follow-up to ensure deficiencies were corrected.\n- A number of hazardous conditions were known and allowed to\npersist for considerable amounts of time or inadequate\nprecautions were taken against them.\n- Report noted such things as filter-cleaning operations without\nusing slip blinds, leaking valves, possibility of contaminating\nthe tank with material from the vent gas scrubber, bad\npressure gauges.\n- Report recommended raising capacity of water curtain.\nPointed out that alarm at flare tower was non-operational and\nthus any leakage could go unnoticed for a long time.\n- According to Bhopal manager, all improvements called for in\nthe report had been taken care of, but obviously not true.\n\n- Prior warnings and events presaging the accident were\nignored:\n- 6 serious incidents between 1981 and 1984, several of which\ninvolved MIC\n- One worker killed in 1981, but official inquiries required by law\nwere shelved or tended to minimize government's or company's\nrole.\n- A leak similar to one involved in the big one had occurred the\nyear before.\n- Journalists and others tried to warn of dangers\n- At least one person within government tried to bring up hazards\nof plant. He was forced to resign.\n- Local authorities and plant managers did nothing in response.\n\n-\nUC went into large-scale production of MIC without having\nperformed adequate research on stability of the chemical. Did\nnot know of an effective inhibitor for the type of reaction that\noccurred.\n-\nAfter the accident, both UC and OSHA announced same type\nof accident could not occur at Institute WV plant because of\nplant's better equipment, better personnel, and America's\ngeneral \"higher level of technical culture.\"\n- Eight months later a similar accident occurred there. Led to brief\nhospital stays for 100 people. Consequences less serious only\nbecause of incidental factors such as direction of wind and tank\ncontained a less toxic substance at the time.\n- Warning siren delayed and company slow in making information\navailable to public.\n\n- A few months later, leak at another UC plant created a toxic cloud\nthat traveled to a shopping center.\n- Several people had to be given emergency treatment, but for two\ndays, doctors and health officials did not know what toxic\nchemical was or where it came from because UC denied leak's\nexistence.\n- OSHA fined UC $1.4 million after Institute accident charging\n\"constant, willful, and overt violations at the plant and a general\natmosphere and attitude that \"a few accidents here and there are\nthe price of production.\"\n\nDo you see any additional factors you\ndid not note before?\n\nAre these factors unique to the\nBhopal accident?\n\nTo understand and prevent accidents,\nmust consider system as a whole\n\nAnd so these men of Hindustan\nDisputed loud and long,\nEach in his own opinion\nExceeding stiff and strong,\nThough each was partly in the right\nAnd all were in the wrong.\nJohn Godfrey Saxe (1816-1887)\n47A spear.A spear.\nImage by MIT OpenCourseWare.Several men touching an elephant.\n.\n.A rope.A fan.A snake.A wall.A spear.A tree.\nImage by MIT OpenCourseWare\nImage by MIT OpenCourseWare.\nImage by MIT OpenCourseWare\nImage by MIT OpenCourseWare.\nImage by MIT OpenCourseWare.\nImage by MIT OpenCourseWare.\n\nJerome Lederer (1968)\n\"Systems safety covers the total spectrum of risk management.\nIt goes beyond the hardware and associated procedures of\nsystems safety engineering. It involves:\n-\nAttitudes and motivation of designers and production people,\n-\nEmployee/management rapport,\n-\nThe relation of industrial associations among\nthemselves and with government,\n-\nHuman factors in supervision and quality control\n-\nThe interest and attitudes of top management,\n(c) New Mexico Museum of Space\nHistory. All rights reserved. This\ncontent is excluded from our\nCreative Commons license. For\nmore information, see https://\nocw.mit.edu/help/faq-fair-use/.\n\n-\nThe effects of the legal system on accident investigations and\nexchange of information,\n-\nThe certification of critical workers,\n-\nPolitical considerations\n-\nResources\n-\nPublic sentiment\nAnd many other non-technical but vital influences on the\nattainment of an acceptable level of risk control. These non-\ntechnical aspects of system safety cannot be ignored.\"\n\nRoot Cause Seduction\n- Accidents always complex, but usually blamed on\nhuman operators\n- Cannot prevent them unless understand ALL the factors\nthat contributed\n- Always additional factors (sometimes never identified)\n- Equipment failure and design\n- Procedures\n- Management decisions\n- Etc.\n\nRoot Cause Seduction\n- Assuming there is a root cause gives us an illusion of\ncontrol.\n- Usually focus on operator error or technical failures\n- Ignore systemic and management factors\n- Leads to a sophisticated \"whack a mole\" game\n- Fix symptoms but not process that led to those symptoms\n- In continual fire-fighting mode\n- Having the same accident over and over\n\nBlame is the Enemy of Safety\n-\nGoal of the courts is to establish blame\n-\nPeople stop reporting errors\n-\nInformation is hidden\n-\nGoal of engineering is to understand why accidents\noccur in order to prevent them\n\nExxon Valdez\n-\nShortly after midnight, March 24, 1989, tanker Exxon Valdez ran\naground on Bligh Reef (Alaska)\n- 11 million gallons of crude oil released\n- Over 1500 miles of shoreline polluted\n-\nExxon and government put responsibility on tanker Captain\nHazelwood, who was disciplined and fired\n-\nWas he to \"blame\"?\n- State-of-the-art iceberg monitoring equipment promised by oil\nindustry, but never installed. Exxon Valdez traveling outside normal\nsea lane in order to avoid icebergs thought to be in area\n- Radar station in city of Valdez, which was responsible for monitoring\nthe location of tanker traffic in Prince William Sound, had replaced its\nradar with much less powerful equipment. Location of tankers near\nBligh reef could not be monitored with this equipment.\n\n-\nCongressional approval of Alaska oil pipeline and tanker transport\nnetwork included an agreement by oil corporations to build and use\ndouble-hulled tankers. Exxon Valdez did not have a double hull.\n-\nCrew fatigue was typical on tankers\n- In 1977, average oil tanker operating out of Valdez had a crew of 40\npeople. By 1989, crew size had been cut in half.\n- Crews routinely worked 12-14 hour shifts, plus extensive overtime\n- Exxon Valdez had arrived in port at 11 pm the night before. The crew\nrushed to get the tanker loaded for departure the next evening\n-\nCoast Guard at Valdez assigned to conduct safety inspections of\ntankers. It did not perform these inspections. It's staff had been cut\nby one-third.\n\n-\nTanker crews relied on the Coast Guard to plot their position\ncontinually.\n- Coast Guard operating manual required this.\n- Practice of tracking ships all the way out to Bligh reef had been\ndiscontinued.\n- Tanker crews were never informed of the change.\n-\nSpill response teams and equipment were not readily available.\nSeriously impaired attempts to contain and recover the spilled oil.\nSummary:\n- Safeguards designed to avoid and mitigate effects of an oil spill\nwere not in place or were not operational\n- By focusing exclusively on blame, the opportunity to learn from\nmistakes is lost\nPostscript:\nCaptain Hazelwood was tried for being drunk the night the Exxon\nValdez went aground. He was found \"not guilty\"\n\nDo Operators Really Cause Most\nAccidents?\n- When say human error, usually mean \"operator error\"\n- Operator error vs. design error\n- Hindsight bias\n\nOperator Error: Traditional View\n- Operator error is cause of most incidents and accidents\n- So do something about operator involved (suspend,\nretrain, admonish)\n- Or do something about operators in general\n- Marginalize them by putting in more automation\n- Rigidify their work by creating more rules and procedures\n\nFumbling for his recline button Ted\nunwittingly instigates a disaster\nThe second important factor is\nsystem design vs. operator error\n\nAll human behavior is affected by\nthe context in which it occurs.\n\n(c) Gary Larson. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nOperator Error: Systems View (1)\n-\nHuman error is a symptom, not a cause\n-\nAll behavior affected by context (system) in which occurs\n-\nRole of operators in our systems is changing\n- Supervising rather than directly controlling\n- Systems are stretching limits of comprehensibility\n- Designing systems in which operator error inevitable and then\nblame accidents on operators rather than designers\n\nOperator Error: Systems View (2)\n-\nTo do something about error, must look at system in which\npeople work:\n- Design of equipment\n- Usefulness of procedures\n- Existence of goal conflicts and production pressures\n- Human error is a symptom of a system that needs to\nbe redesigned\n\n(Sidney Dekker, 2009)\nHindsight Bias\n\"should have, could have, would have\"\nCourtesy of Sidney Dkker. Used with permission.\n\nHindsight Bias\n- After an incident\n- Easy to see where people went wrong, what they should\nhave done or avoided\n- Easy to judge about missing a piece of information that\nturned out to be critical\n- Easy to see what people should have seen or avoided\n\nHindsight Bias\n- Almost impossible to go back and understand how world\nlooked to somebody not having knowledge of outcome\n- Oversimplify causality because start from outcome and reason\nbackward\n- Overestimate likelihood of the outcome and people's ability to\nforesee it because already know outcome\n- Overrate rule or procedure \"violations\"\n- Misjudge prominence or relevance of data presented to people\nat the time\n- Match outcomes with actions that went before it: if outcome bad,\nactions leading to it must have been bad too (missed\nopportunities, bad assessments, wrong decisions, and\nmisperceptions)\n\nOvercoming Hindsight Bias\n- Assume nobody comes to work to do a bad job.\n- Assume were doing reasonable things given the complexities,\ndilemmas, tradeoffs, and uncertainty surrounding them.\n- Simply finding and highlighting people's mistakes explains\nnothing.\n- Saying what did not do or what should have done does not\nexplain why they did what they did.\n\nOvercoming Hindsight Bias\n-\nNeed to consider why it made sense for people to do what\nthey did\n-\nSome factors that affect behavior\n- Goals person pursuing at time and whether may have conflicted\nwith each other (e.g., safety vs. efficiency, production vs.\nprotection)\n- Unwritten rules or norms\n- Information availability vs. information observability\n- Attentional demands\n- Organizational context\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: Operations, Regulation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/4f526628bb020fedf8125549fca600dc_MIT16_863JS16_LecNotes10.pdf",
      "content": "Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.\nMIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nSafety in Operations\nLeveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.\nMIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nManaging/Controlling Change\n- Adaptation or change is an inherent part of any system\n- A common factor in accidents\n- Was a change involved in any of the accidents you\nstudied?\n- Controls needed to:\n- Prevent unsafe changes\n- Detect them if they occur\n\nControls for Planned Changes\n- Safety control structure must continue to be effective\ndespite changes (including changes in environment,\nhuman behavior, organization)\n- Most companies have management of change (MOC)\nprocedures to evaluate impact on safety\n- Cost will depend on quality of documentation and how\noriginal hazard analysis done\n- MOC procedures are often skipped. Need to establish\nresponsibility for enforcement\n\nControls for Unplanned Changes\n- How might deal with unplanned and unsafe changes?\n\nControls for Unplanned Changes\n- How might deal with unplanned and unsafe changes?\n1.\nNeed to identify potential unsafe changes\n2.\nNeed to respond (reduce risk)\n- What is a leading indicator?\n\nControls for Unplanned Changes (2)\n- Need to interrupt risk re-evaluation process before safety\nmargins seriously eroded\n- Requires an alerting function to person with responsibility\n- Want to allow change as long as does not violate safety\nconstraints\n- Key is to allow flexibility in how safety goals achieved and\nprovide information that allows accurate risk assessment\nby decision makers.\n- Don't allow waiving requirements. Re-evaluate them.\n- Establish appropriate feedback loops\n\nFeedback Channels\n- Information flow is key to maintaining safety\n- Probably not general \"leading indicators\" but can identify\nsystem specific ones using safety constraints.\n- Also need to ensure feedback channels are operating\neffectively. Cultural problems can interfere with feedback\n- Three general types:\n- Audits and performance assessments\n- Reporting systems\n- Accident/incident causal analysis\n\nAudits and Performance Assessments\n- Starts from safety constraints and assumptions in safety\ndesign\n- Need to audit entire safety control structure, not just\nlower levels\n- Audit teams must be free of conflicts of interest\n- Participatory and non-punitive audits\n\nAccident/Incident Investigation\n- CAST must be embedded in organizational structure that\nallows exploitation of results\n- Training: Analysts must be managerially and financially\nindependent.\n- Could use trained teams with independent budgets\n- Need to get away from blame\n- Follow-up:\n- Ensure recommendations implemented and are effective\n- Findings should be input to future audits and performance\nassessments\n- If reoccurrence of same factors, investigate why\n\nReporting Systems\n- If not being used, then find out why.\n- Common reasons why not used:\n- Difficult or awkward to use\n- Information appears to go into a black hole. No point in\nreporting because organization won't do anything anyway\n- Fear information will be used against them\n- Examples of successful systems:\n- Nuclear Power\n- Commercial Aviation\n\nEncouraging Reporting\n-\nMaximize accessibility\n- Reporting forms easily and ubiquitously available\n- Not cumbersome to fill in or send up\n-\nMinimize anxiety\n- Written policy that explains\n- What reporting process looks like\n- Consequences of reporting\n- Rights, privileges, protections, and obligations\n- Without written policy, ambiguity exists and people will disclose less\n-\nAct on reports and send information back (provide feedback)\n\nBlame is the Enemy of Safety\n- \"My UK safety customers are incredibly spooked by [the\nNimrod accident report] because of the way it singled out\nindividuals in the safety assessment chain for criticism. It\nhas made a very difficult process of assessing safety risk\neven more difficult.\"\n\n- People stop reporting errors and problems\n- Just Culture movement\n\nUsing the Feedback\n- Information must be presented in form that people can\nlearn from, apply to daily jobs, and use through system\nlife cycle.\n- Precursors almost always exist before major accidents\n- Use to update process models, change control\nalgorithms, modify safety control structure, update\ntraining and education\n(Your operations plan for your project should include how\nfeedback will be obtained and how it will be used)\n\nSafety Information System\n-\nSecond in importance only to management commitment\n-\nCreating and maintaining a successful one requires a\nculture that values the sharing of knowledge learned from\nexperience (learning culture)\n-\nImportant source for identifying leading indicators of\npotential safety problems and as feedback on hazard\nanalysis process.\n-\nNeed communication channels for getting info to those who\ncan understand it and to those making decisions.\n\nSafety Information System: Contents\n- Updated safety plan\n- Status of activities\n- Hazard analysis (HAZOP) results and hazard logs\n- Tracking and status information on all known hazards\n- Incident and accident tracking\n- Reports\n- Corrective Actions (status)\n- Trend analysis\n- Lessons learned\n\nOther Operations Topics\n- Education and training\n- Operations Safety Management Plan\n- Implications of STAMP for occupational (workplace) safety\n\nLeveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.\nMIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nMajor Ingredients of Effective\nSafety Management\n- Commitment and leadership\n- Corporate safety policy\n- Risk awareness and communication channels\n- Controls on system migration toward higher risk\n- Strong corporate safety culture\n- Safety control structure with appropriate assignment of\nresponsibility, authority, and accountability\n- Safety information system\n- Continual improvement and training\n- Education, training, and capability development\n\nCommitment and Leadership\n- Management open and sincere concern for safety in\neveryday dealings\n- Studies show management support for and participation\nin safety activities is most effective way to control and\nreduce accidents.\n- Support shown by:\n- Personal involvement\n- Assigning capable people\n- Providing resources\n- Creating appropriate control structure\n- Responding to initiatives by others\n\nWhat is Safety Culture?\nShein: The Three Levels of Organizational Culture\nSafety culture is set by the leaders who establish\nthe values under which decisions will be made.\n\nSafety Culture\n- Safety culture is a subset of culture that reflects general\nattitude and approaches to safety and risk management\n- Trying to change culture without changing environment\nin which it is embedded is doomed to failure\n- Simply changing organizational structures may lower risk\nover short term, but superficial fixes that do not address\nthe set of shared values and social norms are likely to be\nundone over time.\n\nExamples of Positive Cultural Values and\nAssumptions\n- Incidents and accidents are valued as an important\nwindow into systems that are not functioning as they\nshould - triggering causal analysis and improvement\nactions.\n- Safety information is surfaced without fear\n- Safety analysis is conducted without blame\n- Safety commitment is valued\n\nExample Cultural Values and\nAssumptions (2)\n- There is a feeling of openness and honesty, where\neveryone's voice is valued. Employees feel managers\nare listening.\n- Trust among all parties (hard to establish, easy to break).\n- Employees feel psychologically safe about reporting\nconcerns\n- Employees believe that managers can be trusted to hear\ntheir concerns and will take appropriate action\n- Managers believe employees are worth listening to and are\nworthy of respect.\n\nTypes of Flawed Safety Cultures\n- Culture of Denial\n- Risk assessment is unrealistic\n- Credible risks and warnings are dismissed without\nappropriate investigation (only want to hear good news)\n- Believe accidents are inevitable, the price of productivity\n- Compliance Culture\n- Focus on complying with government regulations\n- Produce extensive \"safety case\" arguments\n- Paperwork Culture\n- Produce lots of paper analyses with little impact on design\nand operations\n\nCulture of Denial Examples\n- \"Our accident rates are going down\"\n- Look at worker injury rates: personal or occupational\nsafety vs. system or process safety\n- Choose statistics that give best result\n- \"Accidents are the price of productivity. A dangerous\ndomain\"\n- Mines: \"Everyone has lots of safety violations\"\n\nLeadership is Key to Changing Culture\n- Safety requires passionate and effective leadership\n- Tone is set at the top of the organization\n- Not just sloganeering but real commitment\n- Setting priorities\n- Adequate resources assigned\n- A designated, high-ranking leader\n- Minimize blame (\"Just Culture\")\n- Understand that safety and productivity are not\nconflicting if take a long-term view\n\nPaul O'Neill and Alcoa\n- \"I intend to make Alcoa the safest company in America. I\nintend to go for zero injuries.\"\n- \"The board put a crazy hippie in charge and he's going\nto kill the company\"\n\"I ordered my clients to sell their stock immediately, before\neveryone else in the room started calling their clients and\ntelling them the same thing. It was literally the worst piece of\nadvice I gave in my entire career.\"\n\nPaul O'Neill and Alcoa (2)\n- Within a year of O'Neill's speech, Alcoa's profits hit a\nrecord high and continued that way until he retired in\n2000.\n- All that growth occurred while Alcoa became one of the\nsafest companies in the world.\n- Understood that safety and productivity are not\nconflicting\n\nLeadership is Key to Changing Culture (2)\n- Minimize blame (\"Just Culture\")\n- Blame is the enemy of safety\n- Peer pressure can be effective\n- Moratorium after DWH\n- Customers have more power than government\n- Engineer the incentive structure to encourage the\nbehavior you want\n\nFood Safety Example\n- New alliance of retailers, food growers, and farm workers\n- Workers had little incentive to report safety problems. Paid\nat a piece rate and taking even 10 minutes to report a\nsafety problem would reduce their pay. One manager said\nthat if workers spotted animal feces in an area where ripe\nstrawberries were ready to be plucked, they might have\nstill simply picked those berries.\n- Teach workers how to spot signs of food contamination\nand train in good practices in exchange for better pay and\nworking conditions\n- \"This program means that instead of one auditor coming\naround once in a while to check on things, we have 400\nauditors on the job all the time.\"\n\nFood Safety Example (2)\n- Unexpected benefit is worker retention\n\"Sure, the money is important, but I also feel good\nbecause I am helping to improve quality and safety,\" Mr.\nEsteban said. \"Those things are important to my family,\ntoo.\"\n- Products carry certification to inform consumers\n\nSafety Policy\n- Reflects how the company or group values safety\n- Should be easy to understand, easily operationalized\n- Based on the way the company views safety: guiding\nprinciples (safety philosophy)\n\nCorporate Safety Policy\n- Provides a clear shared vision of organization's safety\ngoals and values and a way to achieve them.\n- Two parts:\n- Short and concise statement of\n- Safety values of organization\n- What is expected of employees with respect to safety\n- Details about how policy will be implemented\n- Needs to be followed\n- Establish feedback channels\n- Monitor improvements\n- Identify, prioritize, and implement improvements\n\nExample Operational Safety Philosophy (1)\n(Colonial Pipeline)\n-\nAll injuries and accidents are preventable.\n-\nWe will not compromise safety to achieve any business objective.\n-\nLeaders are accountable for the safety of all employees, contractors,\nand the public.\n-\nEach employee has primary responsibility for his/her safety and the\nsafety of others.\n-\nEffective communication and the sharing of information is essential\nto achieving an accident-free workplace.\n-\nEmployees and contractor personnel will be properly trained to\nperform their work safely.\n\nExample Operational Safety Philosophy (2)\n(Colonial Pipeline)\n-\nExposure to workplace hazards shall be minimized and/or\nsafeguarded.\n-\nWe will empower and encourage all employees and contractors to\nstop, correct and report any unsafe condition.\n-\nEach employee will be evaluated on his/her performance and\ncontribution to our safety efforts.\n-\nWe will design, construct, operate and maintain facilities and\npipelines with safety in mind.\n-\nWe believe preventing accidents is good business.\n\nCommunication and Risk Awareness\n- In general, risk is unknowable (severity X likelihood)\n- In absence of hard evidence, tends to be evaluated\ndownward over time\n- Delays between relaxation of controls and accidents\n- Complacency results from inadequate feedback and\nprocess models\n- Using STAMP, risk is defined as a function of\neffectiveness of controls to enforce safe behavior\nNote this is potentially knowable\n- Key is communication and feedback (reporting systems)\n\nControls on System Migration\nto Higher Risk\n- Adaptation is predictable and potentially controllable\n- Identify potential causes and institute controls\n- Perform audits and performance assessments based on\nsafety constraints identified during system development\n- Anchor safety efforts beyond short-term program\nmanagement pressures\n\nDesign Principles for Safety\nControl Structures\n- Need clear definition of expectations, responsibilities,\nauthority, and accountability at all levels of safety control\nstructure\n- See new book for list of responsibilities that need to be\nassigned.\n\nSafety Control Structure Design (2)\n-\nWhere should safety activities be put?\n-\nSafety permeates every part of development and\noperations\n-\nNeed not be located in one place, but common methods and\napproach will strengthen the separate disciplines\n-\nIf distributed, need a clear focus and coordinating body. Don't\nwant fragmented, uncoordinated efforts.\n-\nBasic Principles:\n1.\nSystem safety needs a direct link to decision makers and influence\non decision-making (influence and prestige)\n2.\nSystem safety needs to have independence from project\nmanagement (but not engineering)\n3.\nDirect communication channels are needed to most of the\norganization (oversight and communication)\n\nSafety Control Structure Design (3)\n- Use of working groups for communication\n- Very effective in DoD\n- Different groups at different levels\n- Responsible for coordinating safety efforts at each level,\nreporting status of outstanding safety issues, providing\ninformation to other levels and to external review boards\n- Provides important information sharing: Changes in one\nsubsystem may affect other subsystems and system as a\nwhole\n\nSummary: Safety Management System\n- Key components\n- Management commitment\n- Management involvement\n- Employee empowerment\n- Incentive structures\n- Reporting systems\n- Organizational learning and improvement process\n\nEffective Safety Management Systems\n-\nProcess safety is integrated into the dominant culture, not a\nseparate sub-culture\n-\nSafety is integrated into line operations: a mixture of top-down\nre-engineering and bottom-up process improvement\n-\nIndividuals have required knowledge, skills, and ability\n-\nOrganization has clearly articulated safety vision, values and\nprocedures, shared among stakeholders\n-\nTensions between safety priorities and other system priorities\nare addressed through a constructive, negotiated process.\n-\nKey stakeholders (e.g., unions) have full partnership roles and\nresponsibilities regarding system safety\n-\nPassionate, effective leadership at all levels committed to\nsafety as a high priority for the organization\n\nSafety Management System (2)\n-\nEarly warning systems for migration toward states of high risk\nare established and effective\n-\nEffective communication channels exist for disseminating safety\ninformation\n-\nVisibility of state of safety at all levels through appropriate\nfeedback\n-\nResults of operating experience, process hazard analyses,\naudits, near misses, or accident investigations are used to\nimprove process operations and process safety management\nsystem.\n-\nDeficiencies found during assessments, audits, inspections and\nincident investigation are addressed promptly and tracked to\ncompletion\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "System Safety: Systems Theoretic Process Analysis (STPA) Introduction, Basic Components (hazard, constraints, HCS)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/f302ef24b466fa803e1fffc8d24ee7b6_MIT16_863JS16_LecNotes5.pdf",
      "content": "Systems Theoretic Process Analysis\n(STPA)\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nITP Exercise\na new in-trail procedure\nfor trans-oceanic flights\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nExample System: Aviation\nSystem-level Accident (Loss): ?\nImage removed due to copyright restrictions.\n\nExample System: Aviation\nSystem-level Accident (Loss): Two aircraft collide\nImage removed due to copyright restrictions.\n\nSystem-level Accident (Loss): Two aircraft collide\nSystem-level Hazard: ?\nImage removed due to copyright restrictions.\n\nHazard\n- Definition: A system state or set of conditions\nthat, together with a particular set of worst-case\nenvironmental conditions, will lead to an accident\n(loss).\n- Something we can control\n- Examples:\nAccident\nHazard\nSatellite becomes lost or\nSatellite maneuvers out of orbit\nunrecoverable\nPeople die from exposure to toxic\nToxic chemicals are released into\nchemicals\nthe atmosphere\nPeople die from radiation\nNuclear power plant releases\nsickness\nradioactive materials\nPeople die from food poisoning\nFood products containing\npathogens are sold\n\nSystem-level Accident (Loss): Two aircraft collide\nSystem-level Hazard: Two aircraft violate minimum\nseparation\nImage removed due to copyright restrictions.\n\nAviation Examples\n- System-level Accident (loss)\n- Two aircraft collide\n- Aircraft crashes into terrain / ocean\n- System-level Hazards\n- Two aircraft violate minimum separation\n- Aircraft enters unsafe atmospheric region\n- Aircraft enters uncontrolled state\n- Aircraft enters unsafe attitude\n- Aircraft enters prohibited area\n\nAviation Examples\n- System-level Accident (loss)\n- A-1: Two aircraft collide\n- A-2: Aircraft crashes into terrain / ocean\n- System-level Hazards\n- H-1: Two aircraft violate minimum separation\n- H-2: Aircraft enters unsafe atmospheric region\n- H-3: Aircraft enters uncontrolled state\n- H-4: Aircraft enters unsafe attitude\n- H-5: Aircraft enters prohibited area\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nNorth Atlantic Tracks\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSTPA application:\nNextGen In-Trail Procedure (ITP)\nCurrent State\nProposed Change\n- Pilots will have separation\ninformation\n- Pilots decide when to\nrequest a passing maneuver\n- Air Traffic Control\napproves/denies request\n\nSTPA Analysis\n- High-level (simple) Control Structure\n- Main components and controllers?\n?\n?\n?\n\nSTPA Analysis\n- High-level (simple) Control Structure\n- Who controls who?\nFlight Crew?\nAircraft?\nAir Traffic\nController?\n\nSTPA Analysis\n- High-level (simple)\nControl Structure\n- What commands are\nsent?\nAircraft\nFlight Crew\nAir Traffic\nControl\n?\n?\n?\n?\n\nSTPA Analysis\n- High-level (simple)\nControl Structure\nAircraft\nFlight Crew\nAir Traffic\nControl\nIssue\nclearance\nto pass\nExecute\nmaneuver\nFeedback?\nFeedback?\n\nSTPA Analysis\n- More complex control\nstructure\n\nFAA\nCongress\nATC\nAircraft\nExample High-level control structure\nPilots\nDirectives, funding\nRegulations, procedures\nInstructions\nExecute maneuvers\nReports\nReports\nAircraft status, position, etc\nAcknowledgement, requests\n\nATC Ground\nController\nUpdates and\nacknowledgements\nAircraft\nInstructions\nAircraft\nOther Ground\nControllers\nATC Front Line Manager (FLM)\nCompany\nDispatch\nATC Radio\nACARS Text Messages\nInstructions\nStatus\nUpdates\nInstructions\nStatus\nUpdates\nInstructions\nStatus\nUpdates\nStatus\nQuery\nInstructions\nStatus\nUpdates\nAircraft\nAircraft\nPilots\nPilots\nPilots\nPilots\nExecute\nmaneuvers\nExecute\nmaneuvers\nExecute\nmaneuvers\nExecute\nmaneuvers\nAir Traffic Control (ATC)\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nIdentify Unsafe Control Actions\nFlight Crew\nAction (Role)\nNot providing\ncauses hazard\nProviding\nCauses hazard\nIncorrect\nTiming/\nOrder\nStopped Too\nSoon\nExecute\nPassing\nManeuver\nPilots perform\nITP when ITP\ncriteria are not\nmet or request\nhas been refused\n[H-1]\nATC\nPilots\nInstructions\nExecute maneuvers\nAircraft status, position, etc\nAcknowledgement, requests\nAircraft\n\nStructure of a Hazardous Control\nAction\nFour parts of a hazardous control action\n- Source Controller: the controller that can provide the control action\n- Type: whether the control action was provided or not provided\n- Control Action: the controller's command that was provided /\nmissing\n- Context: conditions for the hazard to occur\n-\n(system or environmental state in which command is provided)\nSource Controller\nExample:\n\"Pilots provide ITP maneuver when ITP criteria not met\"\nType\nControl Action\nContext\n\nDefining Safety Constraints\nUnsafe Control Action\nSafety Constraint\nPilot performs ITP when ITP\ncriteria are not met or request\nhas been refused\nPilot must not perform ITP\nwhen criteria are not met or\nrequest has been refused\nPilot starts maneuver late\nafter having re-verified ITP\ncriteria\nPilot must start maneuver\nwithin X minutes of re-verifying\nITP criteria\nEtc.\nEtc.\n\nSTPA\n(System-Theoretic Process Analysis)\n- Identify accidents\nand hazards\n- Draw the control\nstructure\n- Step 1: Identify\nunsafe control\nactions\n- Step 2: Identify\ncausal factors and\ncreate scenarios\nControlled\nprocess\nControl\nActions\nFeedback\nController\n(Leveson, 2012)\n\nSTPA Step 2: Causal scenarios\nUCA: Pilot executes\nmaneuver when\ncriteria are not met\n[H-1]\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "System Safety: Traditional HA Lecture on Quantification",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/03760d67bb97d13372e3b7cd6bba6d53_MIT16_863JS16_LecNotes4.pdf",
      "content": "Traditional Hazard Analysis\n\nAgenda\n- Today\n- Intro to Hazard Analysis\n- Traditional Qualitative Methods\n- FMEA\n- FTA\n- ETA\n- HAZOP\n- Strengths / Limitations\n- Next: Traditional Quantitative Methods\n- FMECA\n- FTA\n- PRA\n- Strengths / Limitations\n\nHazard (Causal) Analysis\n- \"Investigating an accident before it happens\"\n- Goal is to identify causes of accidents (before they occur) so\ncan eliminate or control them in\n- Design\n- Operations\n- Requires\n- A system design model\n- An accident model\n(even if only in the mind\nof the analyst)\n\nPhysical System Design Model\n(simplified)\nPressurized\nMetal Tank\nValve control input\nValve control input\nWater\nSupply\nDrain\n\nChain-of-events example\nFrom Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to\nSafety. MIT Press, (c) Massachusetts Institute of Technology. Used with permission.\nHow do you find the chain of events before an accident?\n\nForward vs. Backward Search\n(c) Copyright Nancy Leveson, Aug. 2006\n\nInput\nOutput\nForward search?\n\nFMEA: A Forward\nSearch Technique\nThis figure is in the public domain.\n\nForward vs. Backward Search\n(c) Copyright Nancy Leveson, Aug. 2006\n\n5 Whys Example (A Backwards Analysis)\nProblem: The Washington\nMonument is disintegrating.\nWhy is it disintegrating?\nBecause we use harsh chemicals\nWhy do we use harsh chemicals?\nTo clean pigeon droppings off the monument\nWhy are there so many pigeons?\nThey eat spiders and there are a lot of spiders at\nmonument\nWhy are there so many spiders?\nThey eat gnats and lots of gnats at monument\nWhy so many gnats?\nThey are attracted to the lights at dusk\nSolution:\nTurn on the lights at a later time.\n(c) Diliff. License: CC-BY-SA. This content is excluded from\nour Creative Commons license. For more information,\nsee https://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All\nrights reserved. This content\nis excluded from our Creative\nCommons license. For more\ninformation, see https://ocw.\nmit.edu/help/faq-fair-use/.\n\nhttp://www.lean.ohio.gov/Portals/0/docs/trai\nning/GreenBelt/GB_Fishbone%20Diagram.pdf\n\"Breaking the\naccident chain of\nevents\" (see\nvideo)\n(c) LeanOhio, Ohio Department of Administrative Services. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nBottom-Up Search\n\nTop-Down Search\n\nTop-Down Example\nImage from Vesely\nThis image is in the public domain.\n\nTraditional Qualitative Methods\nFMEA (Failure Modes and Effects\nAnalysis)\n\nFMEA: Failure Modes and Effects Analysis\n- 1949: MIL-P-1629\n- Forward search\ntechnique\n- Initiating event:\ncomponent failure\n- Goal: identify effect of\neach failure\nCourtesy of John Thomas. Used with permission.\n\nGeneral FMEA Process\n1. Identify individual components\n2. Identify failure modes\n3. Identify failure mechanisms (causes)\n4. Identify failure effects\n\nFailure Mode and Effect Analysis\nProgram:_________ System:_________ Facility:________\nEngineer:_________\nDate:___________ Sheet:_________\nComponent Name\nFailure Modes\nFailure Mechanisms\nFailure effects\n(local)\nFailure effects\n(system)\nMain hoist motor\nInoperative,\ndoes not move\nDefective bearings\nMotor brushes worn\nBroken springs\nMain hoist cannot\nbe raised. Brake\nwill hold hoist\nstationary\nLoad held\nstationary, cannot\nbe raised or\nlowered.\nFMEA worksheet\n*FMEA example adapted from (Vincoli, 2006)\nExample: Bridge crane system\nCourtesy of John Thomas. Used with permission.\n(c) Wiley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/fairuse.\n\nFMECA: A Forward\nSearch Technique\n\nFailure Mode and Effect Analysis\nProgram:_________ System:_________ Facility:________\nEngineer:_________\nDate:___________ Sheet:_________\nComponent Name\nFailure Modes\nFailure\nMechanisms\nFailure effects\n(local)\nFailure effects\n(system)\nMain Hoist Motor\nInoperative, does\nnot move\nDefective bearings\nLoss of power\nBroken springs\nMain hoist cannot\nbe raised. Brake\nwill hold hoist\nstationary\nLoad held\nstationary, cannot\nbe raised or\nlowered.\nFMEA uses an accident model\n*FMEA example adapted from (Vincoli, 2006)\nDefective\nbearings\nCauses\nInoperative\nhoist motor\nCauses\nMain hoist\nfrozen\nCauses\nMain load held\nstationary\nFMEA method:\nAccident model:\nAccident model: Chain-of-events\nCourtesy of John Thomas. Used with permission.\n\nFMEA Exercise\nAutomotive brakes\nSystem components\n- Brake pedal\n- Brake lines\n- Rubber seals\n- Master cylinder\n- Brake pads\nRubber seals\nFMEA worksheet columns\n- Component\n- Failure mode\n- Failure mechanism\n- Failure effect (local)\n- Failure effect (system)\nRubber Seals\nCourtesy of John Thomas. Used with permission.\n\nFMEA Exercise\nAutomotive brakes\nSystem components\n- Brake pedal\n- Brake lines\n- Rubber seals\n- Master cylinder\n- Brake pads\nRubber seals\nFMEA worksheet columns\n- Component\n- Failure mode\n- Failure mechanism\n- Failure effect (local)\n- Failure effect (system)\nRubber Seals\nHow would you make this system safe?\nCourtesy of John Thomas. Used with permission.\n\nActual automotive brakes\n- FMEA heavily used in mechanical engineering\n- Tends to promote redundancy\n- Useful for physical/mechanical systems to identify\nsingle points of failure\nBrake\nPedal\nBrake fluid\nCourtesy of John Thomas. Used with permission.\n\nA real accident: Toyota's unintended\nacceleration\n-\n2004-2009\n- 102 incidents of stuck accelerators\n- Speeds exceed 100 mph despite stomping on the brake\n- 30 crashes\n- 20 injuries\n-\n2009, Aug:\n- Car accelerates to 120 mph\n- Passenger calls 911, reports stuck accelerator\n- Some witnesses report red glow / fire behind wheels\n- Car crashes killing 4 people\n- 2010, Jul:\n- Investigated over 2,000 cases of unintended\nacceleration\nCaptured by FMEA?\n\nFailure discussion\n- Component Failure\nVs.\n- Design problem\nVs.\n- Requirements problem\n\nFMEA Limitations\n-\nComponent failure incidents only\n- Unsafe interactions? Design issues? Requirements issues?\n-\nSingle component failures only\n- Multiple failure combinations not considered\n-\nRequires detailed system design\n- Limits how early analysis can be applied\n-\nWorks best on hardware/mechanical components\n- Human operators? (Driver? Pilot?)\n- Software failure?\n- Organizational factors (management pressure? culture?)\n-\nInefficient, analyzes unimportant + important failures\n- Can result in 1,000s of pages of worksheets\n-\nTends to encourage redundancy\n- Often leads to inefficient solutions\n-\nFailure modes must already be known\n- Best for standard parts with few and well-known failure modes\n\nSafety vs. Reliability\n- Common assumption:\nSafety = reliability\n- How to improve safety?\n- Make everything more\nreliable!\n*Image from midas.com\n- Making car brakes safe\n- Make every component reliable\n- Include redundant components\nIs this a good assumption?\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see https://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of John Thomas. Used with permission.\n\nSafety vs. reliability\nReliability Failures\nSafety Incidents\nComponent\nproperty\nSystem\nproperty\nCourtesy of John Thomas. Used with permission.\n\nA simpler example\nSafe or unsafe?\n*Image: bluecashewkitchen.com\n\nSafety is not a component property\n- Safety is an emergent property of the system\n- Depends on context and environment!\nIndividual components are not inherently safe or unsafe\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSafety vs. Reliability\nUnsafe\nUnreliable\nscenarios\nscenarios\n\nSafe = Reliable\n- Safety often means making sure X never happens\n- Reliability usually means making sure Y always\nhappens\nSafe\nUnsafe\nReliable\n-Typical commercial flight\nUnreliable\n-Aircraft engine fails in flight\n\nSafe = Reliable\n- Safety often means making sure X never happens\n- Reliability usually means making sure Y always\nhappens\nSafe\nUnsafe\nReliable\n-Typical commercial flight\n-Computer reliably executes unsafe\ncommands\n-Increasing tank burst pressure\n-A nail gun without safety lockout\nUnreliable\n-Aircraft engine won't start\non ground\n-Missile won't fire\n-Aircraft engine fails in flight\n\nSafety vs. Reliability\n- FMEA is a reliability technique\n- Explains the inefficiency\n- FMEA sometimes used to identify unsafe outcomes\nUnsafe\nUnreliable\nscenarios\nscenarios\nFMEA can\nonly\nidentify\nFMEA identifies these\nthese\nsafe scenarios too\nunsafe\nscenarios\nCourtesy of John Thomas. Used with permission.\n\nFailure Modes, Mechanisms, Effects\n- Examples and definitions of \"Failure modes,\nmechanisms, effects\"\n\nFTA\nFault Tree Analysis\n\nFTA: Fault Tree Analysis\n- Top-down search\nmethod\n- Top event:\nundesirable event\n- Goal is to identify\ncauses of hazardous\nevent\n- 1961: Bell labs analysis of Minuteman missile\nsystem\n- Today one of the most popular hazard\nanalysis techniques\nCourtesy of John Thomas. Used with permission.\n\nFTA Process\n1. Definitions\n-\nDefine top event\n-\nDefine initial\nstate/conditions\n2. Fault tree construction\n3. Identify cut-sets and\nminimal cut-sets\nVesely\n\nFault tree examples\nExample from original 1961 Bell Labs study\nPart of an actual TCAS fault tree (MITRE, 1983)\nCourtesy of John Thomas. Used with permission.\n\nFault tree symbols\nFrom NUREG-0492 (Vesely, 1981)\nThis image is in the public domain.\n\nFault Tree cut-sets\n- Cut-set: combination of\nbasic events (leaf nodes)\nsufficient to cause the top-\nlevel event\n- Ex: (A and B and C)\n- Minimum cut-set: a cut-set\nthat does not contain\nanother cut-set\n- Ex: (A and B)\n- Ex: (A and C)\nCourtesy of John Thomas. Used with permission.\n\nFTA uses an accident model\nRelay spring\nfails\nCauses\nRelay contacts\nfail closed\nCauses\nExcessive\ncurrent provided\nFault Tree:\nAccident model:\nAccident model: Chain-of-failure-events\nCourtesy of John Thomas. Used with permission.\n\nThrust reversers\n-\n1991 Accident\n-\nB767 in Thailand\n-\nLauda Air Flight 004\n- Thrust reversers deployed in flight, caused\nin-flight breakup and killing all 223 people.\nDeadliest aviation accident involving B767\n- Simulator flights at Gatwick Airport which\nappeared to show that deployment of a\nthrust reverser was a survivable incident.\n- Boeing had insisted that a deployment was\nnot possible in flight. In 1982 Boeing\nestablished a test where the aircraft was\nslowed to 250 knots, and the test pilots then\nused the thrust reverser. The control of the\naircraft had not been jeopardized. The FAA\naccepted the results of the test.\n- Recovery from the loss of lift from the\nreverser deployment \"was uncontrollable for\nan unexpecting flight crew\". The incident led\nBoeing to modify the thrust reverser system\nto prevent similar occurrences by adding\nsync-locks, which prevent the thrust\nreversers from deploying when the main\nlanding gear truck tilt angle is not at the\nground position.\nCourtesy of John Thomas. Used with permission.\n\nFTA example\n-\nAircraft reverse thrust\n- Engines\n- Engine reverse thrust panels\n- Computer\n- Open reverse thrust panels after\ntouchdown\n- Fault handling: use 2/3 voting. (Open\nreverse thrust panels if 2/3 wheel weight\nsensors AND 2/3 wheel speed sensors\nindicate landing)\n- Wheel weight sensors (x3)\n- Wheel speed sensors (x3)\nCreate a fault tree for the top-level event:\nReverse thrusters don't operate on landing.\nImage from: http://en.wikipedia.org/wiki/File:Klm_f100_ph-kle_arp.jpg\nCourtesy of John Thomas. Used with permission.\n\nWarsaw\n- Warsaw\n- Crosswind landing (one\nwheel first)\n- Wheels hydroplaned\n- Thrust reverser would not\ndeploy\n- Pilots could not override and\nmanually deploy\n- Thrust reverser logic\n- Must be 6.3 tons on each\nmain landing gear strut\n- Wheel must be spinning at\nleast 72 knots\nCourtesy of John Thomas. Used with permission.\n\nFTA Strengths\n- Captures combinations of failures\n- More efficient than FMEA\n- Analyzes only failures relevant to top-level event\n- Provides graphical format to help in\nunderstanding the system and the analysis\n- Analyst has to think about the system in great\ndetail during tree construction\n- Finding minimum cut sets provides insight\ninto weak points of complex systems\nCourtesy of John Thomas. Used with permission.\n\nFTA Limitations\n- Independence between\nevents is often assumed\n- Common-cause failures\nnot always obvious\n- Difficult to capture non-\ndiscrete events\n- E.g. rate-dependent events,\ncontinuous variable changes\n- Doesn't easily capture\nsystemic factors\n\nFTA Limitations (cont)\n- Difficult to capture delays and\nother temporal factors\n- Transitions between states or\noperational phases not\nrepresented\n- Can be labor intensive\n- In some cases, over 2,500 pages of\nfault trees\n- Can become very complex very\nquickly, can be difficult to review\n\nFault tree examples\nExample from original 1961 Bell Labs study\nPart of an actual TCAS fault tree (MITRE, 1983)\nGas valve stays open\nMissing:\nConflict alert\ndisplayed, but\nnever observed\nby controller\nCourtesy of John Thomas. Used with permission.\n\nVesely FTA Handbook\n- Considered by many to be the textbook\ndefinition of fault trees\n\nFailure-based methods\n- Tend to treat safety as a component property\n- Use divide-and-conquer strategies\n- Reductionism\nReasonable?\n\n(c) Associated Press. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nToyota Unintended Acceleration\n- 2004-2009: 102 incidents\n\nToyota Unintended Acceleration\n- 2004: Push-button ignition\n- 2004-2009\n- 102 incidents of uncontrolled acceleration\n- Speeds exceed 100 mph despite stomping on\nthe brake\n- 30 crashes\n- 20 injuries\n- Today\n- Software fixes for pushbutton ignition, pedals\nhttp://www.reuters.com/article/2010/07/14/us-toyota-idUSTRE66D0FR20100714\nhttp://www.statesman.com/business/u-s-toyota-cite-driver-error-in-many-803504.html\nPushbutton was reliable!\nSoftware was reliable!\n\nToyota\n- 2004: Push-button ignition\n- 2004-2009\n- 102 incidents of uncontrolled acceleration\n- Speeds exceed 100 mph despite stomping on the\nbrake\n- 30 crashes\n- 20 injuries\n- 2009, Aug:\n- Car accelerates to 120 mph\n- Passenger calls 911, reports stuck accelerator\n- Car crashes killing 4 people\n- Driver was offensive driving instructor for police\n- Today\n- Software fixes for pushbutton ignition, pedals\nhttp://www.reuters.com/article/2010/07/14/us-toyota-idUSTRE66D0FR20100714\nhttp://www.statesman.com/business/u-s-toyota-cite-driver-error-in-many-803504.html\nAll component requirements were met...\nYet system behavior was unexpected, unsafe!\n\nSystems-Theoretic Approaches\n- Focus of next class\n- Need to identify and prevent failures, but also:\n- Go beyond the failures\n- Why weren't the failures detected and mitigated?\n- By operators\n- By engineers\n- Prevent issues that don't involve failures\n- Human-computer interaction issues\n- Software-induced operator error\n- Etc.\nCourtesy of John Thomas. Used with permission.\n\nEvent Tree Analysis\n\nEvent Tree Analysis\n- 1967: Nuclear power\nstations\n- Forward search technique\n- Initiating event: component\nfailure (e.g. pipe rupture)\n- Goal: Identify all possible\noutcomes\n\nEvent Tree Analysis: Process\n1. Identify initiating\nevent\n2. Identify barriers\n3. Create tree\n4. Identify outcomes\n\nEvent Tree Example\nSmall\nrelease\nNo accident\nNo release\nModerate\nrelease\nNo release\nMajor\nrelease\n\nEvent Trees\nvs.\nFault Trees\nEvent Tree\n- Shows what failed, but not how.\n- Shows order of events\nFault Tree\n- Complex, but shows how failure occurred\n- Does not show order of events\n\nETA uses an accident model\nPressure\ntoo high\nRelief valve\n1 fails\nRelief valve\n2 fails\nExplosion\nEvent Tree:\nAccident model:\nAccident model: Chain-of-events\n\nEvent Tree Analysis: Exercise\nElevator\n1. Identify initiating event\n-\nCable breaks\n2. List Barriers\n3. Create Tree\n4. Identify outcomes\nImage from official U.S. Dept of Labor, Mine Safety and Health Administration paper:\nhttp://www.msha.gov/S&HINFO/TECHRPT/HOIST/PAPER4.HTM\nThis image is in the public domain.\n\nEvent Tree Analysis: Exercise\nWhat are the\nbarriers?\n(c) HowStuffWorks. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nEvent Tree Analysis: Strengths\n- Handles ordering of events better than fault trees\n- Most practical when events can be ordered in\ntime (chronology of events is stable)\n- Most practical when events are independent of\neach other.\n- Designed for use with protection systems\n(barriers)\n\nEvent Tree Analysis: Limitations\n- Not practical when chronology of events is not\nstable (e.g. when order of columns may change)\n- Difficult to analyze non-protection systems\n- Can become exceedingly complex and require\nsimplification\n- Separate trees required for each initiating event\n- Difficult to represent interactions among events\n- Difficult to consider effects of multiple initiating\nevents\n\nEvent Tree Analysis: Limitations (cont)\n- Can be difficult to define functions across top of\nevent tree and their order\n- Requires ability to define set of initiating events that\nwill produce all important accident sequences\n- Most applicable to systems where:\n- All risk is associated with one hazard\n- (e.g. overheating of fuel)\n- Designs are fairly standard, very little change over time\n- Large reliance on protection and shutdown systems\n\nHAZOP\nHazard and Operability Analysis\n\nHAZOP: Hazards and Operability Analysis\n- Developed by Imperial\nChemical Industries in early\n1960s\n- Not only for safety, but\nefficient operations\nAccident model:\n- Chain of failure events (that\ninvolve deviations from\ndesign/operating intentions)\nAn image of a chemical plant is removed\ndue to copyright restrictions.\n\nHAZOP\n- Guidewords applied to\nvariables of interest\n- E.g. flow, temperature, pressure, tank\nlevels, etc.\n- Team considers potential\ncauses and effects\n- Questions generated from guidewords\n- Could there be no flow?\n- If so, how?\n- How will operators know there is no flow?\n- Are consequences hazardous or cause inefficiency?\nHAZOP: Generate the right questions,\nnot just fill in a tree\nImage removed due to copyright restrictions.\n\nHAZOP Process\nGuidewords\nMeaning\nNO, NOT,\nNONE\nThe intended result is not achieved, but nothing\nelse happens (such as no forward flow when\nthere should be)\nMORE\nMore of any relevant property than there\nshould be (such as higher pressure, higher\ntemperature, higher flow, or higher viscosity)\nLESS\nLess of a relevant physical property than there\nshould be\nAS WELL\nAS\nAn activity occurs in addition to what was\nintended, or more components are present in\nthe system than there should be (such as extra\nvapors or solids or impurities, including air,\nwater, acids, corrosive products)\nPART OF\nOnly some of the design intentions are\nachieved (such as only one of two components\nin a mixture)\nREVERSE\nThe logical opposite of what was intended\noccurs (such as backflow instead of forward\nflow)\nOTHER\nTHAN\nNo part of the intended result is achieved, and\nsomething completely different happens (such\nas the flow of the wrong material)\nFigure removed due to copyright restrictions.\nSee: Leveson, Nancy. Safeware: System\nSafety and Computers. Addison-Wesley\nProfessional, 1995. pp. 337.\n\nHAZOP Strengths\n- Easy to apply\n- A simple method that can uncover complex\naccidents\n- Applicable to new designs and new design\nfeatures\n- Performed by diverse study team, facilitator\n- Method defines team composition, roles\n- Encourages cross-fertilization of different\ndisciplines\n\nHAZOP Limitations\n- Requires detailed plant information\n- Flowsheets, piping and instrumentation diagrams, plant layout,\netc.\n- Tends to result in protective devices rather than real design\nchanges\n- Developed/intended for chemical industry\n- Labor-intensive\n- Significant time and effort due to search pattern\n- Relies very heavily on judgment of engineers\n- May leave out hazards caused by stable factors\n- Unusual to consider deviations for systemic factors\n- E.g. organizational, managerial factors, management systems,\netc.\n- Difficult to apply to software\n- Human behavior reduces to compliance/deviation from\nprocedures\n- Ignores why it made sense to do the wrong thing\n\nSummary\n- Well-established methods\n- Time-tested, work well for the problems they were\ndesigned to solve\n- Strengths include\n- Ease of use\n- Graphical representation\n- Ability to analyze many failures and failure combinations\n- Application to well-understood mechanical or physical systems\n- Limitations include\n- Inability to consider accidents without failures\n- Difficulty incorporating systemic factors like managerial\npressures, complex human behavior, and design/requirements\nflaws\n- Other methods may be better suited to deal with the\nchallenges introduced with complex systems\n\nQuantitative Hazard Analysis\n\nAgenda\n- Traditional hazard analysis\n- Qualitative techniques\n- Failure Modes and Effects Analysis\n- Fault Tree Analysis\n- Event Tree Analysis\n- HAZOP\n- Quantitative techniques\n- FMECA\n- Quant. Fault Tree Analysis\n- Quant. ETA\nT\n-\n\nQuantitative analysis\n- How do you include numbers and math?\n- What do you quantify?\n- Tends to focus on two parameters\n- Severity\n- Probability\n\nQuantitative methods\n- The quantification is\nusually based on\nprobability theory and\nstatistics\n- Common assumptions\n- Behavior is random\n- Each behavior independent\nGood assumptions?\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see https://ocw.mit.edu/help/faq-fair-use/.\n\nQuantitative methods\nGood assumptions?\n-Hardware?\n-Humans?\n-Software?\n- The quantification is\nusually based on\nprobability theory and\nstatistics\n- Common assumptions\n- Behavior is random\n- Each behavior independent\n- Identical distributions / EV\nAn image of a pinball table removed due to copyright restrictions.\n\nRisk\n- Common idea:\n- Some combination of severity and likelihood\n- How would you combine severity and\nlikelihood mathematically?\n- Risk = f(Severity, Likelihood)\n- What is f ?\n\nRisk Matrix\n- Based on common quantification:\nRisk = Severity * Likelihood\nLikelihood\nVery Likely\nLikely\nPossible\nUnlikely\nRare\nNegligible\nMinor\nModerate\nSignificant\nSevere\nSeverity\n\nRisk Matrix\n- Based on common quantification:\nRisk = Severity * Likelihood\nLikelihood\nVery Likely\nLow Med\nMedium\nMed Hi\nHigh\nHigh\nLikely\nLow\nLow Med\nMedium\nMed Hi\nHigh\nPossible\nLow\nLow Med\nMedium\nMed Hi\nMed Hi\nUnlikely\nLow\nLow Med\nLow Med\nMedium\nMed Hi\nRare\nLow\nLow\nLow Med\nMedium\nMedium\nNegligible\nMinor\nModerate\nSignificant\nSevere\nSeverity\n\nAutomotive Severity Levels\n- Level 0: No injuries\n- Level 1: Light to moderate injuries\n- Level 2: Severe to life-threatening injuries\n(survival probable)\n- Level 3: Life-threatening to fatal injuries\n(survival uncertain)\nFrom ISO26262\n\nAviation Severity Levels\n-\nLevel 1: Catastrophic\n- Failure may cause crash.\n- Failure conditions prevent continued safe flight and landing\n-\nLevel 2: Severe\n- Failure has negative impact on safety, may cause serious or fatal\ninjuries\n- Large reduction in functional capabilities\n-\nLevel 3: Major\n- Failure is significant, but less impact than severe\n- Significant reduction in functional capabilities\n-\nLevel 4: Minor\n- Failure is noticeable, but less impact than Major\n- Slight reduction in safety margins; more workload or inconvenience\n-\nLevel 5: No effect on safety\nFrom ARP4671, DO-178B\n\nRisk Matrix\n- Based on common quantification:\nRisk = Severity * Likelihood\nAviation Severity Levels\n- Level 1: Catastrophic\n- Level 2: Severe\n- Level 3: Major\n- Level 4: Minor\n- Level 5: No effect on safety\nHow to quantify?\n\nNumerical Scales\n- Severity is usually ordinal\n- Only guarantees ordering along increasing\nseverity\n- Distance between levels not comparable\n- Ordinal multiplication can result in\nreversals\n- Multiplication assumes equal distance\n- ...and fixed 0\n- Assumes severity 4 is 2x worse than severity 2\n- A \"Med Hi\" result may actually be worse\nthan \"High\"\nAnother challenge\nOrdinal\nInterval\nRatio\n\nReversal Example\n- Event A\n- Likelihood = 20%\n- Event B\n- Likelihood = 10%\n- Event C\n- Likelihood = 3%\nCalculate risk\nOrdinal\nRatio\nA\nB\nC\n\nReversal Example\nUsing Ordinal Scale:\n- Event A\n- Likelihood = 20%\n- Severity = 1\n- Event B\n- Likelihood = 10%\n- Severity = 2\n- Event C\n- Likelihood = 3%\n- Severity = 4\nOrdinal\nRatio\nC\nRisk = 0.20\nRisk = 0.20\nRisk = 0.12\nB\nA\n\nReversal Example\nUsing Ratio Scale:\n- Event A\n- Likelihood = 20%\n- Severity = 0\n- Event B\n- Likelihood = 10%\n- Severity = 1\n- Event C\n- Likelihood = 3%\n- Severity = 7\nOrdinal\nRatio\nC\nRisk = 0.00\nB\nRisk = 0.10\nRisk = 0.21\nA\n\nReversal Example\nOrdinal\nRatio\nC\nB\nA\nRisk (using\nordinal scale)\nRisk (using\nratio scale)\nEvent A\n0.20\n0.00\nEvent B\n0.20\n0.10\nEvent C\n0.12\n0.21\n\nRisk Matrix\n- Based on common idea:\nRisk = Severity * Likelihood\nLikelihood\nVery Likely\nLow Med\nMedium\nMed Hi\nHigh\nHigh\nLikely\nLow\nLow Med\nMedium\nMed Hi\nHigh\nPossible\nLow\nLow Med\nMedium\nMed Hi\nMed Hi\nUnlikely\nLow\nLow Med\nLow Med\nMedium\nMed Hi\nRare\nLow\nLow\nLow Med\nMedium\nMedium\nNegligible\nMinor\nModerate\nSignificant\nSevere\nSeverity\nUses expected\nvalues (averages)\n\nExpected Value Fallacy\nP-value Fallacy\nFlaw of Averages\nJensen's Law\nSimpson's paradox\n- Beware when averages are used to simplify\nthe problem!\n- Can make adverse decisions appear correct\n\nAnother Example Hazard Level Matrix\n(c) Addison-Wesley Professional. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nHazard Level: A combination of severity (worst potential damage in\ncase of an accident) and likelihood of occurrence of the hazard.\nRisk: The hazard level combined with the likelihood of the hazard\nleading to an accident plus exposure (or duration) of the hazard.\nSafeware p179. (c) Copyright Nancy Leveson\nRISK\nHAZARD LEVEL\nHazard\nseverity\nLikelihood of\nhazard occurring\nHazard\nExposure\nLikelihood of hazard\nLeading to an accident\nSafety: Freedom from accidents or losses.\n(c) Addison-Wesley Professional. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nHazard Level Assessment\n-\nCombination of Severity and Likelihood\n-\nDifficult for complex, human/computer controlled\nsystems\n-\nChallenging to determine likelihood for these\nsystems\n- Software behaves exactly the same way every time\n-\nNot random\n- Humans adapt, and can change behavior over time\n-\nAdaptation is not random\n-\nDifferent humans behave differently\n-\nNot I.I.D (independent and identically distributed)\n- Modern systems almost always involve new designs and\nnew technology\n-\nHistorical data may be irrelevant\n-\nSeverity is usually adequate to determine effort to spend\non eliminating or mitigating hazard.\nHigh\nMed Hi\nMedium\nLow Med\nLow\nHazard Level or\nRisk Level:\n\nFMECA\nFailure Modes Effects and Criticality Analysis\n\nFMECA\n- Same as FMEA, but with \"criticality\"\ninformation\n- Criticality\n- Can be ordinal severity values\n- Can be likelihood probabilities\n- An expression of concern over the effects of failure\nin the system*\n*Vincoli, 2006, Basic Guide to System Safety\n\nFailure Mode and Effect Analysis\nProgram:_________ System:_________ Facility:________\nEngineer:_________\nDate:___________ Sheet:_________\nComponent\nName\nFailure Modes\nFailure\nMechanisms\nFailure effects\n(local)\nFailure effects\n(system)\nCriticality\nLevel\nMain hoist\nmotor\nInoperative,\ndoes not move\nDefective\nbearings\nLoss of power\nBroken springs\nMain hoist\ncannot be\nraised. Brake\nwill hold hoist\nstationary\nLoad held\nstationary,\ncannot be\nraised or\nlowered.\n(5) High,\ncustomers\ndissatisfied\nFMEA worksheet\n*FMEA example adapted from (Vincoli, 2006)\nBridge crane system\n(c) Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSeverity Level Examples\nRating\nMeaning\nNo effect\nVery minor (only noticed by discriminating customers)\nMinor (affects very little of the system, noticed by average\ncustomer)\nModerate (most customers are annoyed)\nHigh (causes a loss of primary function; customers are dissatisfied)\nVery high and hazardous (product becomes inoperative; customers\nangered; the failure may result unsafe operation and possible\ninjury)\n*Otto et al., 2001, Product Design\n(c) Pearson. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSeverity Level Examples\nRating\nSeverity of Effect\nSafety issue and/or non-compliance with government regulation without warning.\nSafety issue and/or non-compliance with government regulation with warning.\nLoss of primary function.\nReduction of primary function.\nLoss of comfort/convenience function.\nReduction of comfort/convenience function.\nReturnable appearance and/or noise issue noticed by most customers.\nNon-returnable appearance and/or noise issue noticed by customers.\nNon-returnable appearance and/or noise issue rarely noticed by customers.\nNo discernable effect.\n*http://www.harpcosystems.com/Design-FMEA-Ratings-PartI.htm\n(c) Harpco Systems. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nFailure Mode and Effect Analysis\nProgram:_________ System:_________ Facility:________\nEngineer:_________\nDate:___________ Sheet:_________\nComponent\nName\nFailure Modes\nFailure\nMechanisms\nFailure effects\n(local)\nFailure effects\n(system)\nProbability of\noccurrence\nMain hoist\nmotor\nInoperative,\ndoes not move\nDefective\nbearings\nLoss of power\nBroken springs\nMain hoist\ncannot be\nraised. Brake\nwill hold hoist\nstationary\nLoad held\nstationary,\ncannot be\nraised or\nlowered.\n0.001 per\noperational\nhour\nFMECA worksheet\n*FMEA example adapted from (Vincoli, 2006)\nBridge crane system\nCould also\nspecify\nlikelihood\n(c) Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse/.\n\nFMECA Exercise: Actual automotive brakes\nBrake\nPedal\nBrake fluid\nFMEA worksheet columns\n- Component\n- Failure mode\n- Failure mechanism\n- Failure effect (local)\n- Failure effect (system)\n- Criticality (Severity)\nSeverity Levels\n1.\nNo effect\n2.\nMinor, not noticed by average\ncustomer\n3.\nMajor, loss of primary function\n4.\nCatastrophic, injury/death\nCourtesy of John Thomas. Used with permission.\n\nQuantitative ETA\n\nQuantitative Event Tree Analysis\n-\nQuantify p(success) for each barrier\n-\nLimitations\n- P(success) may not be random\n- May not be independent\n- May depend on order of events and context\n- Ex: Fukushima\n\nFukushima Diesel Generators\n\nQuantitative results are affected by the\nway barriers are chosen\n-\nBarrier 1a\n- Initial conditions keep aircraft > 10NM apart\n- P(success) = 0.99\n-\nBarrier 1b\n- Initial conditions keep aircraft > 5NM apart\n- P(success) = 0.99\n-\nBarrier 1c\n- Initial conditions keep aircraft > 1NM apart\n- P(success) = 0.99\n-\nBarrier 2\n- Flight crew detects traffic by means other than visual, avoid NMAC\n- P(success) = 0.90\n-\nBarrier 3\n- Flight crew detects traffic by visual acquisition, avoid NMAC\n- P(success) = 0.80\nRTCA DO-312\n(c) RTCA Inc. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee https://ocw.mit.edu/help/faq-fair-use/.\n\nQuantitative FTA\n\nQuantitative Fault Tree Analysis\n- If we can assign probabilities to lowest\nboxes...\n- Can propagate up using probability theory\n- Can get overall total probability of hazard!\n- AND gate\n- P(A and B) = P(A) * P(B)\n- OR gate\n- P(A or B) = P(A) + P(B)\nAny assumptions being made?\n\nQuantitative Fault Tree Analysis\n- If we can assign probabilities to lowest\nboxes...\n- Can propagate up using probability theory\n- Can get overall total probability of hazard!\n- AND gate\n- P(A and B) = P(A) * P(B)\n- OR gate\n- P(A or B) = P(A) + P(B)\nOnly if events A,B are\nindependent!\n\nQuantitative Fault Tree Analysis\n- If we can assign probabilities to lowest\nboxes...\n- Can propagate up using probability theory\n- Can get overall total probability of hazard!\n- AND gate\n- P(A and B) = P(A) * P(B)\n- OR gate\n- P(A or B) = P(A) + P(B)\n- Is independence a good assumption?\n- Hardware?\n- Software?\n- Humans?\n\nQuantitative Fault Tree Analysis\nActual fault trees from RTCA DO-312\n(c) RTCA Inc. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee https://ocw.mit.edu/help/faq-fair-use/.\n\nQuantitative Fault Tree Analysis\n- Where do the probabilities come from?\n- Historical data\n- Simulations\n- Expert judgment\nAre there any issues\nusing these sources?\n*Actual qualitative-quantitative conversion from RTCA DO-312\n(c) RTCA Inc. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee https://ocw.mit.edu/help/faq-fair-use/.\n\nRisk Assessment and Preliminary Hazard\nAnalysis (PHA)\n\nPreliminary Hazard Analysis\n(c) Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse/.\n\nRisk Assessment Matrix\nThis table is in the public domain.\n\nHardware Example\nThis image is in the public domain.\n\nSoftware Example\nThis image is in the public domain.\n\nHuman Error Example\nThis image is in the public domain.\n\nNo.\nTask\nHazard\nRisk\nRisk Reduction\nFinal Risk\nPosition 3 Tasks, Install ECS\nFalling object crushing person or body part\nYellow\nInvestigate process improvements\nGreen\nSite Acceptance\nTest/Qualification Testing\nPasserby unauthorized entry\nPerson entering cell exposed to significant risks from robot,\netc,\nyellow\nIML workstand gates to stop process when entered; Interlocked gates at Brand\nScaffolding; access control, signage\nGreen\nRobots Crossing Ped aisle\nin & out of replenishment\ncell (K)\nAGV/moblie equipment\nimpacts person\nYellow\nAGV control system, signage, crossing markings on\npedestrian aisle,\nGreen\nLight Curtain Alternatives\nAnalysis\nExposure to impact,\ncrushing, etc. when safety scanners are deactivated when OML's \"leapfrog\"\nYellow\nEstablish safe procedure, use\nof spotters, hand guiding\nGreen\nAll Sub-processes\nAll Users\nnormal operation\nExposure to movement of\nrobots, motors and cylinders.\nRed\nSafety perimeter, category 4, that stops automation when\nviolated; investigate use of Kuka.safesolutions, e-stop control, access control, procedures,\ntraining\nGreen\nnormal operation\nmechanical: Drill penetration of fuselage\nOperator exposes body part\nto drill penetration\nYellow\nOnly one operator in\nworkspace, proper training\nGreen\nAFB movement systems\nAGV trapping person against immovable object or running\nsomeone over\nYellow\nAGV safety system with scanners\nGreen\nTraffic management\nmechanical : Impact,\npinching, crushing\nExposure to impact,\npinching, crushing by AGV, OML's, etc\nyellow\nAGV's equipped with safety Laser scanners with 360 degrees coverage, hand guiding, use of\nspotters, procedures\nGreen\nMaintenance activities\ningress / egress : Exposure\nto being hit by robot\nperforming maintenance\nMaintenance person exposed\nwhile working on machinery\nYellow\nLock out auto to enter, lock out other sources as required\nAGV's & Movement Systems\nmechanical : Collision-impact two robots same side of\nbarrier\nAGV impacts person\nYellow\nAGV safeguarding using SICK area scanners 360 degree coverage to stop AGV\nif violated; people will be clear of cell(another line); walls (Anacortes) or ?light curtains? to\nstop motion if violated; Training and Amin\nprocedures\nGreen\n\nExample Risk Assessment:\nManufacturing Robot\nN\no\n.\nTask\nHazard\nRisk\nRisk Reduction\nFinal Risk\n2 Site Acceptance\nTest/Qualification Testing\nPasserby unauthorized entry\nPerson entering cell exposed to\nsignificant risks from robot,\netc,\nYellow\nAccess control, signage\nGreen\n3 Robots Crossing Ped aisle\nin & out of replenishment cell\nMobile equipment\nimpacts person\nYellow\nAGV control system, signage,\ncrossing markings on pedestrian\naisle,\nGreen\n4 Light Curtain Alternatives\nAnalysis\nExposure to impact,\ncrushing, etc. when safety\nscanners are deactivated when\nOML's \"leapfrog\"\nYellow\nEstablish safe procedure, use\nof spotters, hand guiding\nGreen\nPosition 3 Tasks, Install ECS\nFalling object crushing person\nor body part\nRed\nInvestigate process\nimprovements\nGreen\n\nUH-60MU SAR Hazard Classification\nUH-60MU SAR marginal hazards\n-\nLoss of altitude indication in DVE\n-\nLoss of heading indication in DVE\n-\nLoss of airspeed indication in DVE\n-\nLoss of aircraft health information\n-\nLoss of external communications\n-\nLoss of internal communications\nSTPA Unsafe Control Action\nThe Flight Crew does not provide collective\ncontrol input necessary for level flight, resulting\nin controlled flight into terrain\nScenario 1: The Flight Crew has a flawed process\nmodel and believes they are providing sufficient\ncontrol input to maintain level flight. This flawed\nprocess model could result from:\na)The altitude indicator and attitude indicator are\nmalfunctioning during IFR flight and the pilots are\nunable to maintain level flight\nb)The Flight Crew believes the aircraft is trimmed\nin level flight when it is not\nc)The Flight Crew has excessive workload due to\nother tasks and cannot control the aircraft\nd)The Flight Crew has degraded visual conditions\nand cannot perceive slow rates of descent that\nresult in a continuous descent\ne)The Flight Crew does not perceive rising terrain\nand trims the aircraft for level flight that results in\ncontrolled flight into terrain\nUH-60MU SAR identifies various hazards as\nmarginal that actually could lead to a\ncatastrophic accident\nThis content is in the public domain.\n\nCurrent State of the Art: PRA\n- Risk and Risk Assessment\n- Little data validating PRA or methods for calculating\nit\n- Other problems\n- May be significant divergence between modeled system\nand as-built and as-operated system\n- Interactions between social and technical part of system\nmay invalidate technical assumptions underlying analysis\n- Effectiveness of mitigation measures may change over time\n- Why are likelihood estimates inaccurate in practice?\n- Important factors left out (operator error, flawed decision\nmaking, software) because don't have probability\nestimates\n- Non-stochastic factors involved in events\n- Heuristic biases\n\nHeuristic Biases\n-\nConfirmation bias (tend to deny uncertainty and vulnerability)\n- People look for evidence that supports their hypothesis\n- Reject evidence that does not\n-\nConstruct simple causal scenarios\n- If none comes to mind, assume impossible\n-\nTend to identify simple, dramatic events rather than events that are\nchronic or cumulative\n-\nIncomplete search for causes\n- Once one cause identified and not compelling, then stop search\n-\nDefensive avoidance\n- Downgrade accuracy or don't take seriously\n- Avoid topic that is stressful or conflicts with other goals\n\nControlling Heuristic Biases\n-\nCannot eliminate completely but can reduce\n-\nUse structured method for assessing and managing \"risk\"\n- Following a structured process and rules to follow can diminish power\nof biases and encourage more thorough search\n- Concentrate on causal mechanisms vs. likelihood\n- Require action or procedures (to avoid defensive avoidance)\n- Use worst case analysis (vs. \"design basis\naccident\")\n- \"Prove\" unsafe rather than \"safe\"\n- Hazard analysis vs. safety case\n\nMisinterpreting Risk\nRisk assessments can easily be misinterpreted:\n\nCost Benefit Analysis\n\nCost-benefit analysis\n- Goes beyond identifying risk\n- Is it worth fixing?\n$\nHow much does it\n$\ncost NOT to fix?\nHow much does it\ncost to fix?\n\nFord Pinto\n-\nFord noticed design flaw too late to eliminate\n- Fuel tank directly behind axle\n- Rear-end collision can cause disaster\n-\nEngineers developed a patch\n- $11 per car, reinforced structure\n-\nCost-benefit analysis\n- Total cost to fix: $137.5 million\n- Human life is worth $200,000\n- 180 expected burn deaths\n- Serious human injury is worth $67,000\n- 180 expected serious burn injuries\n- Burned out vehicle is worth $700\n- 2,100 expected burned out vehicles\n- Total cost if not fixed: $49 million\nOne lawsuit ruling (1972):\n- Ford to pay $2.5 million compensatory damages\n- Ford to pay $3.5 million because Ford was aware of design defects before production but did\nnot fix the design\n\nFord Pinto\n- Cost of human life was based on National Highway Traffic\nSafety Administration regulations\n- $200,725 per life\n- Fuel tank location was commonplace at that time in\nAmerican cars\n- California supreme court had tolerated and encouraged\nmanufacturers to trade off safety for cost\n- NHTSA recorded 27 Pinto rear-impact fires\n- Lower than average for compact cars at the time\n\nGeneral Motors\n-\n13 deaths, 130 reported incidents\n-\nDesign flaws\n- Ignition switches easily switch to \"off\" position\n- Bumps, vehicle collision, heavy keychain, etc.\n- Keys have wide slot, increased torque\n- Airbags and other safety systems immediately disabled when key is off\n-\nCost-benefit analysis\n- GM aware of problem for over a decade\n- Developed a fix, costs $0.57 per car\n- Recommended no further action because there was \"no acceptable\nbusiness case\"\n- Tooling cost and piece price was too high\n-\nCEO response\n- That is very disturbing if true\n- This is not how GM does business\n- If there is a safety issue we take action. We do not look at the cost\nassociated with it.\n\nGeneral Motors\n- Systemic factors\n- Wrote service bulletin to fix key slot, but kept it\nprivate\n- Knew in 2001 that ignition switches did not meet\nspecification\n- 4-10 vs. 15-25\n- Updated part in 2006\n- Kept old part number, confusion\n- Still didn't meet specification (10-15 vs. 15-25)\n\nBoeing\n- Boeing 787 LiCo Batteries\n- Prediction/Certification:\n- No fires within 107 flight hours\n- Followed 4761 certification\nparadigm\n- Actual experience:\n- Within 52,000 flight hours - 2 such\nevents\n- 2.6 x 104 flight hours [NTSB 2013]\n[http://upload.wikimedia.org/wikipedia/commons/9/95/Boeing_Dreamliner_battery_original_and_damaged.jpg]\nCody Fleming, 2014\nThese images are in the public domain.\n\nBoeing 787 Lithium Battery Fires\n- A module monitors for\nsmoke in the battery bay,\ncontrols fans and ducts to\nexhaust smoke overboard.\n- Power unit experienced\nlow battery voltage, shut\ndown various electronics\nincluding ventilation.\n- Smoke could not be\nredirected outside cabin\nAll software requirements were satisfied!\nThe requirements were inadequate\nCourtesy of John Thomas. Used with permission.\n\nLord Kelvin quote\n- \"I often say that when you can measure what\nyou are speaking about, and express it in\nnumbers, you know something about it; but\nwhen you cannot measure it, when you\ncannot express it in numbers, your knowledge\nis of a meagre and unsatisfactory kind; it may\nbe the beginning of knowledge, but you have\nscarcely in your thoughts advanced to the\nstate of Science, whatever the matter may be.\"\n- [PLA, vol. 1, \"Electrical Units of Measurement\",\n1883-05-03]\n\nA response\n- \"In truth, a good case could be made that if\nyour knowledge is meagre and unsatisfactory,\nthe last thing in the world you should do is\nmake measurements; the chance is negligible\nthat you will measure the right things\naccidentally.\"\n- George Miller (a psychologist)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n16.683J / ESD.863J System Safety\nSpring 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}