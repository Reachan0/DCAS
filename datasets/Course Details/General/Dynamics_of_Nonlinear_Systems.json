{
  "course_name": "Dynamics of Nonlinear Systems",
  "course_description": "This course provides an introduction to nonlinear deterministic dynamical systems. Topics covered include: nonlinear ordinary differential equations; planar autonomous systems; fundamental theory: Picard iteration, contraction mapping theorem, and Bellman-Gronwall lemma; stability of equilibria by Lyapunov’s first and second methods; feedback linearization; and application to nonlinear circuits and control systems.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Theory of Computation",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Systems Engineering",
    "Engineering",
    "Computer Science",
    "Theory of Computation",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Systems Engineering"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCourse Description\n\nThis course studies state-of-the-art methods for modeling, analysis, and design of nonlinear dynamical systems with applications in control. Topics include:\n\nNonlinear Behavior\n\nMathematical Language for Modeling Nonlinear Behavior\n\nDiscrete Time State Space Equations\n\nDifferential Equations on Manifolds\n\nInput/Output Models\n\nFinite State Automata and Hybrid Systems\n\nLinearization\n\nLinearization Around a Trajectory\n\nSingular Perturbations\n\nHarmonic Balance\n\nModel Reduction\n\nFeedback Linearization\n\nSystem Invariants\n\nStorage Functions and Lyapunov Functions\n\nImplicitly Defined Storage Functions\n\nSearch for Lyapunov Functions\n\nLocal Behavior of Differential Equations\n\nLocal Stability\n\nCenter Manifold Theorems\n\nBifurcations\n\nControllability of Nonlinear Differential Equations\n\nFrobenius Theorem\n\nExistence of Feedback Linearization\n\nLocal Controllability of Nonlinear Systems\n\nNonlinear Feedback Design Techniques\n\nControl Lyapunov Functions\n\nFeedback Linearization: Backstepping, Dynamic Inversion, etc.\n\nAdaptive Control\n\nInvariant Probability Density Functions\n\nOptimal Control and Dynamic Programming\n\nPrerequisite:\n6.241\nor an equivalent course.\n\nInformation Resources and Literature\n\nThis year, there will be no required textbook. All necessary information will be supplied in the lecture notes.\n\nThe books\nNonlinear Systems\nby Hassan K. Khalil, published by Prentice Hall, and the more advanced\nNonlinear Systems: Analysis, Stability, and Control\nby Shankar Sastry, published by Springer, can both serve as basic references on Nonlinear Systems Theory, frequently covering the topics skipped in the lectures.\n\nInstructor\n\nProf. Alexandre Megretski\n\nClass Schedule\n\nHomework\n\nHomework assignments are usually given on Wednesdays. Homework papers are to be submitted during the lecture hours on the following Wednesday. The homework will be corrected, graded, and returned as soon as possible. Solutions to the homework will be distributed when the corrected homework is returned.\n\nTeam work on home assignments is strictly encouraged, as far as generating ideas and arriving at the best possible solution is concerned. However, you have to write your own solution texts (and your own code, when needed).\n\nMATLAB(r)\n\nMATLAB(r), the \"language of technical computing'', will be used in some assignments. We will need Simulink(r), Control Systems, and LMI Control Toolboxes. You may wish to consult its online help for general information and for specific commands for simulating and analysing systems.\n\nExaminations\n\nThere will be two take-home quizzes, to be completed and returned within 24 hours, but no final exam. The quizzes will cover the theory of 6.243J (divided as equally as possible). The questions will be based on the ideas used in the problem set solutions made available at least a week before the test. No homework will be given on the last Wednesdays before the quizzes. No cooperation is allowed on take-home quizzes.\n\nGrading\n\nThe letter grade will be determined at the end of the semester from a numerical grade N, obtained from the formula\n\nN=0.5*H+0.25*Q1+0.25*Q2\n\nwhere H is the average homework grade, and Q1, Q2 are quiz grades (H, Q1, Q2 are numbers between 0 and 100). From the distribution of N for the entire class, boundaries will be chosen to define letter grades. For students near the boundaries, other factors may be taken into account to determine the letter grade, such as effort, classroom activity, etc.",
  "files": [
    {
      "category": "Resource",
      "title": "hw1_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/3100110a1338332946cbefcc5c46144b_hw1_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 11\nProblem 1.1\nBehavior set B of an autonomous system with a scalar binary DT output consists of all\nDT signals w = w(t) →{0, 1} which change value at most once for 0 · t < 1.\n(a) Give an example of two signals w1, w2 → B which commute at t = 3, but do not\ndefine same state of B at t = 3.\n(b) Give an example of two different signals w1, w2 →B which define same state of B at\nt = 4.\n(c) Find a time-invariant discrete-time finite state-space \"difference inclusion\" model\nfor B, i.e. find a finite set X and functions g : X ∈! {0, 1}, f : X ∈! S(X),\nwhere S(X) denotes the set of all non-empty subsets of X, such that a sequence\nw(0), w(1), w(2), . . . can be obtained by sampling a signal w →B if and only if there\nexists a sequence x(0), x(1), x(2), . . . of elements from X such that\nx(t + 1) →f(x(t)) and w(t) = g(x(t)) for t = 0, 1, 2, . . . .\n(Figuring out which pairs of signals define same state of B at a given time is one\npossible way to arrive at a solution.)\nProblem 1.2\nConsider differential equation\ny (t) + sgn( y(t) + y(t)) = 0.\n1Posted September 10, 2003. Due date September 17, 2003\n\n(a) Write down an equivalent ODE x(t) = a(x(t)) for the state vector x(t) = [y(t); y(t)].\n(b) Find all vectors x0 → R2 for which the ODE from (a) does not have a solution\nx : [t0, t1] ∈! R2 (with t1 > t0) satisfying initial condition x(t0) = x0.\n(c) Define a semicontinuous convex set-valued function : R2 ∈! 2R\nsuch that a( x) →\nx) for all x. Make sure the sets (\n(\nx) are the smallest possible subject to these\nconstraints.\n(d) Find explicitly all solutions of the differential inclusion x (t) → (x(t)) satisfying\ninitial conditions x(0) = x0, where x0 are the vectors found in (b). Such solutions\nare calles sliding modes.\n(e) Repeat (c) for a : R2 ∈! R2 defined by\na([x1; x2]) = [sgn(x1); sgn(x2)].\nProblem 1.3\nFor the statements below, state whether they are true or false. For true statements,\ngive a brief proof (can refer to lecture notes or books). For false statements, give a\ncounterexample.\n(a) All maximal solutions of ODE x (t) = exp(-x(t)2) are defined on the whole time\naxis {t} = R.\n(b) All solutions x : R ∈! R of the ODE\n1⁄2\nx(t) =\nx(t)/t,\n0,\nt inf= 0,\nt = 0\nare such that x(t) = -x(-t) for all t → R.\n(c) If constant signal w(t) ≤ 1 belongs to a system behavior set B, but constant signal\nw(t) ≤ -1 does not then the system is not linear."
    },
    {
      "category": "Resource",
      "title": "ps1sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/3f677225efcf490bc36975021acdb130_ps1sol_6243_2003.pdf",
      "content": "1⁄2\n1⁄2\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 1 Solutions1\nProblem 1.1\nBehavior set B of an autonomous system with a scalar binary DT output\nconsists of all DT signals w = w(t) ≤ {0, 1} which change value at most\nonce for 0 ∀t < →.\n(a) Give an example of two signals w1, w2 ≤B which commute at t = 3, but\ndo not define same state of B at t = 3.\nTo answer this and the following questions, let us begin with formulating necessary\nand sufficient conditions for two signals z1, z2 ≤B to commute and to define same\nstate of B at a given time t.\nFor w ≤B, t ≤[0, →) let\n1⁄2\nw[t] =\nlim¿ !t,¿ <t w(¿),\nw(0),\nif t > 0,\nif t = 0\nbe the left side limit value of w at t. Let\n0, if w(t) = lim¿ !1 w(¿),\nN+(w, t) =\n1, otherwise\nbe the number of discontinuities of w(¿) between ¿ = t and ¿ = →. Similarly, let\n0, if w(0) = w[t],\nN-(w, t) =\n1, otherwise\nbe the number of discontinuities of w(¿) between ¿ = 0 and ¿ = t -0.\n1Version of October 2, 2003\n\n1⁄2\nLemma 1.1 Signals z1, z2 ≤B commute at time t ≤ [0, →) if and only if z1(t) =\nz2(t) and\nN-(z1, t) + N+(z2, t) + |z2(t) -z1[t]| ∀1\n(1.1)\nand\nN-(z2, t) + N+(z1, t) + |z1(t) -z2[t]| ∀1.\n(1.2)\nProof First note that the \"hybrid\" signal z12, obtained by \"gluing\" the past of z1\n(before time t) to the future of z2 (from t to →), is a discrete time signal if and only\nif z1(t) = z2(t). Moreover, since he discontinuities of z12 result from three causes:\ndiscontinuities of z1(¿) before ¿ = t, discontinuities of z2 between ¿ = t and ¿ = →,\nand the inequality between z1[t] and z1(t), condition (1.1) is necessary and sufficient\nfor z12 ≤ B (subject to z1(t) = z2(t)). Similarly, considering the discontinuities of\nthe other \"hybrid\" obtained by \"gluing\" the past of z2 to the future of z2 yields\n(1.2).\nIt follows immediately from Lemma 1.1 that signals z1, z2 ≤B define same state of\nB at time t ≤[0, →) if and only if\nN-(z1, t) = N-(z2, t), D(z1, t) = D(z2, t), N+(z1, t) = N+(z2, t), z1(t) = z2(t),\n(1.3)\nwhere for w ≤B\nD(w, t) = |w(t) -w[t]|\nis the indicator of a discontinuity at t.\nFor k ≤Z+ let uk ≤B be defined by\n0, t < k,\nuk (t) =\n1, t ⊃k.\nThen u1 and u0 commute but do not define same state of B at time t = 3.\n(b) Give an example of two different signals w1, w2 ≤B which define same\nstate of B at t = 4.\nu1 and u2.\n(c) Find a time-invariant discrete-time finite state-space \"difference in\nclusion\" model for B, i.e. find a finite set X and functions g : X ∈!\n{0, 1}, f : X ∈! S(X), where S(X) denotes the set of all non-empty\nsubsets of X, such that a sequence w(0), w(1), w(2), . . . can be obtained\nby sampling a signal w ≤ B if and only if there exists a sequence\nx(0), x(1), x(2), . . . of elements from X such that\nx(t + 1) ≤f(x(t)) and w(t) = g(x(t)) for t = 0, 1, 2, . . . .\n\n(Figuring out which pairs of signals define same state of B at a given\ntime is one possible way to arrive at a solution.)\nCondition (1.3) naturally calls for X to be the set of all possible combinations\nx(t) = [N-(w, t); N+(w, t); D(w, t); w(t)].\nNote that not more than one of the first three components can be non-zero at a\ngiven time instance, and hence the total number of possible values of x(t) is eight,\nwhich further reduces to four at t = 0, since\nN-(w, 0) = D(w, 0) = 0 8 w ≤B.\nThe dynamics of x(t) is given by\nf([0; 0; 0; 0]) = {[0; 0; 0; 0]},\nf([0; 0; 0; 1]) = {[0; 0; 0; 1]},\nf([1; 0; 0; 0]) = {[1; 0; 0; 0]},\nf([1; 0; 0; 1]) = {[1; 0; 0; 1]},\nf([0; 1; 0; 0]) = {[1; 0; 0; 0]},\nf([0; 1; 0; 1]) = {[1; 0; 0; 1]},\nf([0; 0; 1; 0]) = {[0; 0; 1; 0], [0; 1; 0; 1]},\nf([0; 0; 1; 1]) = {[0; 0; 1; 1], [0; 1; 0; 0]},\nwhile g(x(t)) is simply the last bit of x(t).\nThis model is not the minimal state space model of B. Note that last two bits of\nx(t + 1), as well as w(t), depend only on the last two bits of x(t). Hence a model of\nB with a two-bit state space X¤ = {0, 1} × {0, 1} can be given by\nf¤([0; 0]) = {[0; 0]}, f¤([0; 1]) = {[0; 1]}, f¤([1; 0]) = {[0; 1], [1; 0]}, f¤([1; 1]) = {[0; 0], [1; 1]},\nand\ng¤([x1; x2]) = x2.\n\n·\n\n·\n\nProblem 1.2\nConsider differential equation\ny (t) + sgn( y(t) + y(t)) = 0.\n(a) Write down an equivalent ODE x (t) = a(x(t)) for the state vector\nx(t) = [y(t); y(t)].\n⎩·\n¶\n·\n\nx1\nx2\na\n=\n.\n\nx1 +\nx2\n-sgn(\nx2)\n(b) Find all vectors x0 ≤ R2 for which the ODE from (a) does not have\n\na solution x :\n[t0, t1] ∈! R2 (with t1 > t0) satisfying initial condition\nx(t0) = x0.\nSolutions (forward in time) do not exist for\n1⁄2·\n\n⎧\n\nx0 ≤ X0 =\n\nx02 = 0,\nx01 inf\n\nx01\n≤ R2 : x01 +\nx01 ≤ [-1, 1],\n= 0 .\nx02\nTo show this, note first that, for\n\nx02 ⊃ 0,\nx01 +\nx02 > 1,\na solution is given by\nx01 + t\nx(t) =\n\nx02 - t2/2\n,\nt ≤ [0, 2( x02 - 1)].\nx02 - t\nSimilarly, for\n\nx02 ∀ 0,\nx01 +\nx02 < -1,\na solution is given by\nx01 + t\nx(t) =\n\nx02 + t2/2\n,\nt ≤ [0, 2(- x02 - 1)].\nx02 + t\nFinally, for x0 = 0 there is the equilibrium solution x(t) ≥ 0.\nNow it is left to prove that no solutions with x(0) ≤ X0 exist. Assume that, to\nthe contrary, x : [0, δ] ∈! R is a solution with δ > 0 and x(0) = [-t, t] for some\nt ≤ [-1, 1], t inf= 0. Without loss of generality, assume that 0 < t ∀ 1.\nSince x is continuous, there exist ± ≤ (0, δ) such that x2(t) > 0 for all t ≤ [0, ±]. Let\nt0 be the argument of minimum of x1(t) + x2(t) for t ≤ [0, ±]. If x1(t0) + x2(t0) < 0\nthen x2(t) - sgn(x1(t) + x2(t)) ⊃ 1 for t in a neigborhood of t0, which contradicts\n\n:\nthe assumption that t0 is an argument of a minimum. Hence x1(t) + x2(t) ⊃ 0 for all\nt ≤ [0, ±]. Moreover, since x1 is an integral of x2 > 0, x1(t) is strictly monotonically\nnon-increasing on [0, ±], and hence x1(t) > -1 for all t ≤ (0, ±].\nLet t0 be the argument of maximum of x1(t) + x2(t) on [0, ±]. If x1(t0) + x2(t0) > 0\nthen x1(t) + x2(t) > 0 in a neigborhood of t0. Combined with x1(t) > -1, this yields\nd(t) = x2(t) - sgn(x1(t) + x2(t)) < -x1(t) - sgn(x1(t) + x2(t)) < 1 - 1 = 0.\nSince x1 + x2 is an integral of d, this contradicts the assumption that t0 is an\nargument of a maximum. Hence x1(t) + x2(t) = 0 for t ≤ [0, ±], which implies that\nx2(t) is a constant. Hence x1(t) is a constant as well, which contradicts the strict\nmonotonicity of x1(t).\n(c) Define a semicontinuous convex set-valued function ν : R2 ∈! 2R\nx) ≤ ν(\nx) are the\nsuch that a(\nx) for all x. Make sure the sets ν(\nsmallest possible subject to these constraints.\nx2 ! 0\nFirst note that a([\nx2]) converges to [x2; 1] as\nx2 within the open half plane\n\nx2 < 0. Similarly, a([\nx2]) converges to [x2; -1] as\nx1,\nx1 +\nx1,\nx2 ! x2 subject to\n\nx2 > 0. Hence one must have ν(\nx), where\nx1 +\nx) 3⁄4 ν0(\n⎩·\n¶\n1⁄2·\n\n⎧\n\nx1\nx2\nx1 +\n=\n: t ≤ o(\nx2) ,\nν0\nx2\n-t\n< {1},\ny > 0,\no(y) =\n{-1}, y < 0, .\n[-1,1],\ny = 0.\nOn the other hand, it is easy to check that the compact convex set-valued function\nν0 is semicontinuous. Hence ν = ν0.\n(d) Find explicitly all solutions of the differential inclusion x (t) ≤ ν(x(t))\nsatisfying initial conditions x(0) = x0, where x0 are the vectors found\nin (b). Such solutions are calles sliding modes.\nThe proof in (b) can be repeated to show that all such solutions will stay on the\nhyperplane x1(t) + x2(t) = 0. Hence\nx1(t) = x1(0)e -t ,\nx2(t) = x2(0)e -t .\n(e) Repeat (c) for a : R2 ∈! R2 defined by\na([x1; x2]) = [sgn(x1); sgn(x2)].\n⎩·\n¶\n1⁄2·\n\n⎧\nx1\nc1\nx1), c2 ≤ o(\nν\n\n=\n: c1 ≤ o(\nx2) .\nx2\nc2\n\n1⁄2\n1⁄2\nProblem 1.3\nFor the statements below, state whether they are true or false. For\ntrue statements, give a brief proof (can refer to lecture notes or books).\nFor false statements, give a counterexample.\n(a) All maximal solutions of ODE x (t) = exp(-x(t)2) are defined on the\nwhole time axis {t} = R.\nThis statement is true. Indeed, a maximal solution x = x(t) is defined on an interval\nwith a finite bound t¤ only when |x(t)| ! →as t ! t¤. However, x(t) is an integral\nof a function not exceeding 1 by absolute value. Hence |x(t) -x(t0)| ∀|t -t0| for\nall t, and therefore |x(t)| cannot approach infinity on a finite time interval.\n(b) All solutions x : R ∈! R of the ODE\nx(t)/t, t inf= 0,\nx (t) =\n0,\nt = 0\nare such that x(t) = -x(-t) for all t ≤R.\nThis statement is false. Indeed, for every pair c1, c2 ≤R the function\nc1t, t ∀0,\nx(t) =\nc2t, t > 0\nis a solution of the ODE, which can be verified by checking that\n⎨ t2 x(t)\nx(t2) -x(t1) =\ndt 8 t1, t2.\nt\nt1\n(c) If constant signal w(t) ≥ 1 belongs to a system behavior set B, but\nconstant signal w(t) ≥-1 does not then the system is not linear.\nThis statement is true. Indeed, if B is linear then cw ≤ B for all c ≤ R, w ≤ B.\nWith c = -1 this means that, for a linear system, w ≤B if and only if -w ≤B."
    },
    {
      "category": "Resource",
      "title": "hw2_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/777d690256a56ebd9fe0d6c4af10ef4a_hw2_6243_2003.pdf",
      "content": "3⁄4\nZ\n+ 6\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 21\nProblem 2.1\nConsider the feedback system with external input r = r(t), a causal linear time invariant\nforward loop system G with input u = u(t), output v = v(t), and impulse response\ng(t) = 0.1±(t) + (t + a)-1/2e-t, where a → 0 is a parameter, and a memoryless nonlinear\nfeedback loop u(t) = r(t) + A(v(t)), where A(y) = sin(y). It is customary to require well-\nu\nr\nh\n\nG\n\nv\nA(y)\nFigure 2.1: Feedback setup for Problem 2.1\nposedness of such feedback models, which will usually mean existence and uniqueness of\nsolutions v = v(t), u = u(t) of system equations\nt\nv(t) = 0.1u(t) +\nh(t - π )u(π )dπ, u(t) = r(t) + A(v(t))\non the time interval t ≤ [0, ⊂) for every bounded input signal r = r(t).\n1Posted September 17, 2003. Due date September 24, 2003\n\n(a) Show how Theorem 3.1 from the lecture notes can be used to prove well-posedness\nin the case when a > 0. Hint: it may be a good idea to begin with getting rid\nof the algebraic part of the system equations by introducing a new signal e(t) =\nv(t) - 0.1A(v(t)) - 0.1r(t).\n(b) Propose a generalization of Theorem 3.1 which can be applied when a = 0 as well.\n(You are not required to write down the proof of your generalization, but make\nevery effort to ensure the statement is correct.)\nProblem 2.2\nRead the section of Lecture 4 handouts on limit sets of trajectories of ODE (it was not\ncovered in the classroom).\n(a) Give an example of a continuously differentiable function a : R2 ≥! R2 , and a\nsolution of ODE\nx (t) = a(x(t)),\n(2.1)\nfor which the limit set consists of a single trajectory of a non-periodic and non-\nequilibrium solution of (2.1).\nRn\n(b) Give an example of a continuously differentiable function a :\n≥! Rn, and a\nbounded solution of ODE (2.1), for which the limit set contains no equilibria and no\ntrajectories of periodic solutions. Hint: it is possible to do this with a 4th order\nlinear time-invariant system with purely imaginary poles.\n(c) Use Theorem 4.3 from the lecture notes to derive the Poincare-Bendixon theorem:\nif a set X 1⁄2 R2 is compact (i.e. closed and bounded), positively invariant for system\n(2.1) (i.e. x(t,\n\nx) ≤ X for all t → 0 and x ≤ X), and contains no equilibria, then\nthe limit set of every solution starting in X is a closed orbit (i.e. the trajectory of\na periodic solution). Assume that a : R2 ≥! R2 is continuously differentiable.\nProblem 2.3\nUse the index theory to prove the following statements.\nSn\n(a) If n > 1 is even and F :\n≥! Sn is continuous then there exists x ≤ Sn such that\nx = F(x) or x = -F(x).\n(b) The equations for the harmonically forced nonlinear oscillator\ny (t) + y(t) + (1 + y(t)2)y(t) = 100 cos(t)\nhave at least one 21⁄4-periodic solution. Hint: Show first that, for\nV (t) = y (t)2 + y(t)2 + y(t) y(t) + 0.5y(t)2 ,\n\nthe inequality\nV (t) · -c1V (t) + c2,\nwhere c1, c2 are some positive constants, holds for all t."
    },
    {
      "category": "Resource",
      "title": "ps2sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/475f86b00eafb9ed4732fb2f25a0bdc8_ps2sol_6243_2003.pdf",
      "content": "3⁄4\n+ 6\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 2 Solutions1\nProblem 2.1\nConsider the feedback system with external input r = r(t), a causal linear\ntime invariant forward loop system G with input u = u(t), output v = v(t),\na)-1/2e-t, where\nand impulse response g(t) = 0.1±(t) + (t +\na\n0 is a parameter,\n→\nand a memoryless nonlinear feedback loop u(t) = r(t) + π(v(t)), where π(y) =\nsin(y).\nIt is customary to require well-posedness of such feedback models,\nr\nh u\n\nG\n\nv\nπ(y)\nFigure 2.1: Feedback setup for Problem 2.1\nwhich will usually mean existence and uniqueness of solutions v = v(t),\nu = u(t) of system equations\n⎬ t\nv(t) = 0.1u(t) +\nh(t - δ )u(δ )dδ, u(t) = r(t) + π(v(t))\non the time interval t ≤ [0, ⊂) for every bounded input signal r = r(t).\n1Version of October 8, 2003\n\n1⁄2\n(a) Show how Theorem 3.1 from the lecture notes can be used to prove\nwell-posedness in the case when a > 0.\nIn terms of the new signal variable\ny(t) = v(t) - 0.1π(v(t)) - 0.1r(t)\nsystem equations can be re-written as\n⎬ t\ny(t) =\nh(t - δ)[r(δ) + μ(y(δ) + 0.1r(δ))]dδ,\nwhere\n-t\n(t + a)-1/2e\n,\nt\nh(t) =\n→\n0,\notherwise,\nand μ : R inf! R is the function which maps z ≤ R into π(q), with q being the\nsolution of\nq - 0.1π(q) = z.\nSince π is continuously differentiable, and its derivative ranges in [-1, 1], μ is con\ntinuously differentiable as well, and its derivative ranges between 1/1.1 and 1/0.9.\nFor every constant T ≤ [0, ⊂), the equation for y(t) with t\nT can be re-written\n→\nas\n⎬ t\ny(t) = y(T ) +\naT (y(δ), δ, t)dδ,\nT\nwhere\naT (\ny, δ, t) = h(t - δ)[r(δ) + μ(y(δ) + 0.1r(δ))] + hT (t),\n⎬ T\nhT (t) =\nh (t - δ)[r(δ) + μ(y(δ) + 0.1r(δ))]dδ.\nWhen parameter a takes a positive value, function a = aT satisfies conditions of\n\nTheorem 3.1 with X = Rn,\na) being a\nx0 = y(T), r = 1, and t0 = T, with K = K(\nfunction of a = 0, and\n≥\nM = MT = M0(a)(1 + max y(t) ).\nt2[0,T ] |\n|\nHence a solution y = y(·) defined on an interval t ≤ [0, T] can be extended in a\nunique way to the interval t ≤ [0, T+], where\nT+ - T = min{1/MT , 1/(2K)},\nand\nmax y(t) √ MT (T+ - T) + max y(t) ).\nt2[0,T+ ] |\n|\nt2[0,T ] |\n|\n\nStarting with T = T (0) = 0, for k = 0, 1, 2, . . . define T (k + 1) as the T+ calculated\nfor T = T (k). To finish the proof of well posedness, we have to show that T (k) ! ⊂\n. Indeed, since\nas k ! ⊂\nMT (k)(T (k + 1) -T (k)) = MT (k) min{1/MT (k), 1/(2K)} √1,\nMT(k) grows not faster than linearly with k. Hence T (k + 1) -T (k) decrease not\nfaster than c/k, and therefore T (k) ! ⊂as k ! ⊂.\n(b) Propose a generalization of Theorem 3.1 which can be applied when\na = 0 as well.\nAn appropriate generalization, relying on integral time-varying bounds for a and its\nincrements, rather than their maximal values, is suggested at the end of proof of\nTheorem 3.1 in the lecture notes.\nProblem 2.2\nRead the section of Lecture 4 handouts on limit sets of trajectories of\nODE (it was not covered in the classroom).\n(a) Give an example of a continuously differentiable function a : R2\nR2\ninf!\n, and a solution of ODE\nx (t) = a(x(t)),\n(2.1)\nfor which the limit set consists of a single trajectory of a non-\nperiodic and non-equilibrium solution of (2.1).\nThe limit trajectory should be that of a maximal solution x : (t1, t2) inf! R2 such\nthat x(t) ! ⊂as t ! t1 or t ! t2.\n|\n|\nTo construct a system with such limit trajectory, start with a planar ODE for which\nevery solution, except the equilibrium solution at the origin, converges to a periodic\nsolution which trajectory is the unit circle. Considering R2 as the set of all complex\nnumbers, one such ODE can be written as\nz (t) = (1 -z(t) + j) z(t) z(t), where j =\np\n-1,\n|\n|\n|\n|\nwhere every solution with z(0) = 0 converges to the trajectory of periodic solution\n≥\nz0(t) = ejt . Now apply the substitution\nz =\n+ 1,\nw\nwhich moves the point z = 1 to w = ⊂(and also moves z = ⊂to w = 0). For the\nresulting system\nw (t) = -w(t)(1 + w(t))(1 + j -(1 + w(t))/w(t) ) (1 + w(t))/w(t) ,\n(2.2)\n|\n| |\n|\n\nevery solution w(·) with w(0) = 0 will have the straight line passing through the\npoints w = -1/2 and w = 1/(j\n≥\n- 1) (trajectory of the solution w0(t) = 1/(ejt - 1),\ndefined for t ≤ (0, 21⁄4)), as its limit set. However, the right side of (2.2) is not a\ncontinuously differentiable function of w: there is a discontinuity at w = 0. To fix\nthis problem, multiply the right side by the real number |w(t) 4, which yields\n|\na(w) = -w(1 + w)((1 + j) w 2\n(1 + w)w ) (1 + w)w .\n| | -|\n| |\n|\nFor the resulting system, every trajectory except the equilibrium at w = 0 has the\nsame limit set as defined before.\nRn\n(b) Give an example of a continuously differentiable function a :\ninf!\nRn, and a bounded solution of ODE (2.1), for which the limit set con\ntains no equilibria and no trajectories of periodic solutions.\nIt is possible to do this with a 4th order linear time-invariant system with purely\nimaginary poles:\nx 1(t)\n= x2(t),\nx 2(t)\n= -x1(t),\nx 3(t)\n= 1⁄4x4(t),\nx 4(t)\n= -1⁄4x3(t).\nThe solution\n⎫\n⎢\nsin(t)\n⎧\n6 cos(t)\n⎧\nx(t) = 4 sin(1⁄4t) ⎨\ncos(1⁄4t)\nof this ODE has the limit set\n⎩\n⎫\n⎢\n⎥\nsin(t1)\n⎥\n⎥\n⎥\n⎣\n⎦\n⎧\n6 cos(t1)\n=\n⎧ : t1, t2 ≤ R\n.\n⎥4 sin(t2) ⎨\n⎥\n⎥\n⎥\n⎤\n⎪\ncos(t2)\nIndeed, since 1⁄4 is not a rational number, every real number can be approximated\narbitrarily well by 21⁄4k - 2q where k, q are arbitrarily large positive integers. Hence\nthe difference between t1 + 21⁄4k and t2/1⁄4 + 2q can be made arbitrarily small for every\ngiven pair t1, t2 ≤ R. For t = t1 + 21⁄4k this implies that\nsin(t) = sin(t1), cos(t) = cos(t1), sin(1⁄4t) 1⁄4 sin(t2+21⁄4q) = sin(t2), cos(1⁄4t) 1⁄4 cos(t2).\n\nEvery solution with x(0) in has the form\n⎫\n⎢\nsin(t + t1)\n⎧\n6 cos(t + t1)\n⎧\nx(t) = 4 sin(1⁄4t + t2) ⎨ ,\ncos(1⁄4t + t2)\nand hence is not periodic.\nAn example with n = 3 is also possible. However, such example would require more\nwork, since it cannot be given by a linear system.\n(c) Use Theorem 4.3 from the lecture notes to derive the Poincare-\nBendixon theorem: if a set X ≈ R2 is compact (i.e. closed and bounded),\nx) ≤ X for all t\n0 and\npositively invariant for system (2.1) (i.e. x(t,\n→\nx ≤ X), and\ncontains no equilibria, then the limit set of every solution starting in X is a closed\norbit (i.e. the trajectory of a periodic solution). Assume that a : R2 inf! R2 is\ncontinuously differentiable.\nLet x0 : (t1, t2) inf! R2 be a maximal solution of (2.1) such that t1 < 0 < t2 and\nx(0) ≤ X. Then, by the invariance of X, x(t) ≤ X for all t\n0. Hence x(t) is\n→\nbounded for t → 0, and hence t2 =\nAppllying Theorem 4.3 to x0, note first that\n⊂.\nscenario (a) cannot take place (since x(t) is bounded for t → 0). On the other hand,\nscenario (c) also cannot take place. Indeed, otherwise let x1 : (t1\n1, t1\nbe a\nmaximal solution of (2.1) such that x1(t) is a limit point of x0(·) for all t ≤ (t1\n2).\n2) inf! R2\n1, t1\nSince X is closed and x0(t) ≤ X for t → 0, all limit points of x0 lie in X. Hence\nx1(t) is in X, and t1 = ⊂. According to scenario (c), the limit\nx = lim x1(t)\nt!1\nexists, which implies a( x) = 0, contradicting the assumptions. Hence only scenario\n(b) takes place, which is what we had to prove.\nProblem 2.3\nUse the index theory to prove the following statements.\n(a) If n > 1 is even and F : Sn\nis continuous then there exists x ≤ Sn\ninf! Sn\nsuch that x = F (x) or x = -F (x).\nAssume, to the contrary, that x = F (x) and -x = F (x) for all x ≤ Sn . Then\n≥\n≥\nH(x, t) = (2t - 1)x + t(1 - t)F (x)\n(2t - 1)x + t(1 - t)F (x)\n|\n|\nis a continuous homotopy between H(x, 0) = -x and H(x, 1) = x. Since index\nof the map x inf! -x equals (-1)n+1 , and index of the map x\nx equals 1, a\ninf!\ncontradiction results.\n\n(b) The equations for the harmonically forced nonlinear oscillator\ny (t) + y(t) + (1 + y(t)2)y(t) = 100 cos(t)\nhave at least one 21⁄4-periodic solution. Hint: Show first that, for\nV (t) = y (t)2 + y(t)2 + y(t) y(t) + 0.5y(t)4 ,\nthe inequality\nc1V (t) + c2,\nV (t) √-\nwhere c1, c2 are some positive constants, holds for all t.\nDifferentiating V (t) along a system solution y = y(t) yields, for w(t) = 100 cos(t),\n-y 2 -yy - 2\nV\n=\ny -y + 2( y + y/2)w\n= -0.5V -0.5( y + y/2)2 + 2( y + y/2)w -3/8y\n= -0.5V + 2w 2 -0.5( y + y/2 + 2w)2 -3/8y\n√ -0.5V + 20000.\nHence the derivative of\nr(t) = e 0.5t(V (t) -40000)\nis non-positive at all times, i.e. r = r(t) is monotonically non-increasing.\nConsider the function G0 : R2\nwhich maps the vector of initial conditions\ninf! R2\nx(0) = [y(0); y(0)] to the vector x(T ) = [y(T ); y(T )], where T = 21⁄4k and k > 0 is\nan integer parameter to be chosen later. By continuity of dependence of solutions\nof ODE on parameters, G0 is continuous. Also, since\nV (t) √3 x(t) 2 + 0.5 x(t)| √|x(t) 4 + 5,\n|\n|\n|\n|\nit follows that\n0.5T\ne\n(V (T ) -40000) √V (0) -40000 √x(0) 4 ,\n|\n|\nwhich implies\nV (T ) √40000 + e -1⁄4k x(0) .\n|\n|\nSince V (t) →0.5 x(t) 2, it follows that\n|\n|\nx(T ) √80000 + 2e -1⁄4k( x(0)\n-39995).\n|\n|\n|\n|\nHence, if x(0) √300 and\n|\n|\nlog(2) + 4 log(30)\nk →\n1⁄4\n1⁄4 4.55\n\nthen x(T ) √ 300.\n|\n|\nNow consider the function G : B2 inf! B2, where B2 is the unit ball in R2, defined\nby\nG(\nx)/300.\nx) = G0(300\nThe function satisfies the conditions of the Brower's fixed point theorem, and hence\nthere exists\nx) =\nx ≤ B2 such that G(\nx. By the definition of G, the solution of the\nnonlinear oscillator equations with\n⎭\n⎡\ny(0)\n= 300 x\ny (0)\nwill be periodic with period T = 101⁄4."
    },
    {
      "category": "Resource",
      "title": "hw3_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/5e770942a25e5eab7e7365291fb7275d_hw3_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 31\nProblem 3.1\nFind out which of the functions V : R2 ! R,\n(a) V (x1, x2) = x 2 + x2;\n(b) V (x1, x2) = |x1| + |x2|;\n(c) V (x1, x2) = max |x1|, |x2|;\nare valid Lyapunov functions for the systems\n(1) x 1 = -x1 + (x1 + x2)3 , x 2 = -x2 - (x1 + x2)3;\nx1 = -x2 - x1(x 2\nx2 = -x1 - x2(x 2\n(2)\n+ x2),\n+ x2);\n(3) x 1 = x2|x1|, x 2 = -x1|x2|.\nProblem 3.2\nShow that the following statement is not true. Formulate and prove a correct version: if\nV : Rn ≤! R is a continuously differentiable functional and a : Rn ≤! Rn is a continuous\nfunction such that\ninfV (\nx) · 0 8\nx) = 1,\n(3.1)\nx)a(\nx : V (\nthen V (x(t)) · 1 for every solution x : [0, ∀) ! Rn of\nx (t) = a(x(t))\n(3.2)\nwith V (x(0)) · 1.\n1Posted September 24, 2003. Due date October 1, 2003\n\n1⁄2\nProblem 3.3\nThe optimal minimal-time controller for the double integrator system with bounded con\ntrol\n1⁄2 x 1(t) = x2(t),\n|u(t)| · 1\nx 2(t) = u(t),\nhas the form\nu(t) = sgn(x1(t) + 0.5x2(t)2sgn(x2(t))).\n(Do you know why ?)\n(a) Find a Lyapunov function V : R2 ≤! R2 for the closed loop system, such that\nV (x(t)) is strictly decreasing along all solutions of system equations except the\nequilibrium solution x(t) → 0.\n(b) Find out whether the equilibrium remains asymptotically stable when the same\ncontroller is used for the perturbed system\nx 1(t) = x2(t),\n|u(t)| · 1,\nx 2(t) = -2x1(t) + u(t),\nwhere 2 > 0 is small."
    },
    {
      "category": "Resource",
      "title": "ps3sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/00f08564a17698f0e2198b8d7b78ff8e_ps3sol_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 3 Solutions1\nProblem 3.1\nFind out which of the functions V : R2 ! R,\n(a) V (x1, x2) = x2 + x2;\n(b) V (x1, x2) = |x1| + |x2|;\n(c) V (x1, x2) = max |x1|, |x2|;\nare valid Lyapunov functions for the systems\n(1) x 1 = -x1 + (x1 + x2)3 , x 2 = -x2 - (x1 + x2)3;\n(2) x 1 = -x2 - x1(x1 + x2), x2 = -x1 - x2(x1 + x2);\n(3) x 1 = x2|x1|, x 2 = -x1|x2|.\nThe answer is: (b) is a Lyapunov function for system (3) - and no other valid pairs\nSystem/Lyapunov function in the list. Please note that, when we say that a Lyapunov\nfunction V is defined on a set U, then we expect that V (x(t)) should non-increase along\nall system trajectories in U. In the formulation of Problem 3.1, V is said to be defined on\nthe whole phase space R2 . Therefore, V (x(t)) must be non-increasing along all system\ntrajectories, in order for V to be a valid Lyapunov function.\nTo show that (b) is a valid Lyapunov function for (3), note first that system (3) is\ndefined by an ODE with a Lipschitz right side, and hence has the uniqueness of solutions\nproperty. Now, every point (x1, x2) ≤ R2 with x1 = 0 or x2 = 0 is an equilibrium of\n(3). Hence V is automatically valid at those points. At every other point in R2 , V is\n1Version of October 10, 2003\n\ndifferentiable, with dV/dx = [sgn(x1); sgn(x2)] being the derivative. Hence ≡V (x)f(x) =\nx1x2 - x1x2 = 0 at every such point, which proves that V (x(t)) is non-increasing (and\nnon-decreasing either) along all non-equilibrium trajectories.\nBelow we list the \"reasons\" why no other pair yields a valid Lyapunov function. Of\ncourse, there are many other ways to show that.\nFor system (1) at x = (2, 0), we have x 1 > 0, x 2 < 0, hence both |x1| and |x2| are\nincreasing along system trajectories in a neigborhood of x = (2, 0). Since all Lyapunov\nfunction candidates (a)-(c) increase when both |x1| and |x2| increase, (a)-(c) are not valid\nLyapunov functions for system (1).\nFor system (2) at x = (0.5, -0.5), we have x 1 > 0, x 2 < 0, hence both |x1| and |x2|\nincrease along system trajectories in a neigborhood of x = (0.5, -0.5).\nFor system (3) at x = (2, 1), we have x = (2, -2), hence both x1 + x2 and max(x1, x2)\nare increasing along system trajectories in a neigborhood of x = (2, 1).\nProblem 3.2\nShow that the following statement is not true. Formulate and prove a correct version: if\nV : Rn inf! R is a continuously differentiable functional and a : Rn inf! Rn is a continuous\nfunction such that\n≡V (\nx) · 0 8\nx) = 1,\n(3.1)\nx)a(\nx : V (\nthen V (x(t)) · 1 for every solution x : [0, →) ! Rn of\nx (t) = a(x(t))\n(3.2)\nwith V (x(0)) · 1.\nThere are two important reasons why the statement is not true: first, ≡V ( x) should be\nx such that V (\nnon-zero for all\nx) = 1; second, solution of x = a(x) with initial condition\nx(0) =\nx0) = 1 should be unique. Simple counterexamples based on these\nx0 such that V (\nconsiderations are given by\nV (x) = x 2 + 1, a( x) = 1, x(t) = t,\nand\nx) = 1.5 1/3 , x(t) = t\n.\nV (x) = x + 1, a(\nx\n1.5\nOne correct way to fix the problem is by requiring a strict inequality in (3.1). Here is\na less obvious correction.\nTheorem 3.1 Let V : Rn ! R be a continuously differentiable functional such that\n≡V ( = 0 for all\nx) = 1, and let a : Rn ! Rn be a locally Lipschitz func\nx) 6\nx satisfying V (\ntion such that condition (3.1) holds. Then V (x(t)) · 1 for every solution x : [t0, t1) ! Rn\nof (3.2) with V (x(0)) · 1.\n\nx0 ≤ Rn satisfying the condition V (\nProof It is sufficient to prove that for every\nx0) = 1\nthere exists d > 0 such that V (x(t)) · 1 for 0 · t · d for the solution x(t) of (3.2) with\nx(0) = x0. Indeed, for 2 ≤ (0, 1) define x2 as a solution of equation\n∗\nx (t) = -2≡V (x(t)) + a(x(t)), x(0) = x0.\n(3.3)\nBy the existence theorem, solutions x2 are defined on a non-empty interval t ≤ [0, d] which\ndoes not depend on 2. Note that\n∗\ndV (x 2(t))/dt = ≡V (x (t))(-2≡V (x (t)) + a(x (t))) · -2∈≡V (x (t))∈2 < 0\nwhenever V (x2(t)) = 1, and hence the same inequality holds whenever x2(t) is close enough\nto the set {x : V (x) = 1}. Hence V (x2(t)) · 1 for t ≤ [0, d] for all 2. Now, continuous\ndependence on parameters implies that x2(t) converges for all t ≤ [0, d] to x(t). Hence\nV (x(t)) = lim V (x 2(t)) · 1.\n2!0\nProblem 3.3\nThe optimal minimal-time controller for the double integrator system\nwith bounded control\n⎪ x 1(t) = x2(t),\n|u(t)| · 1\nx 2(t) = u(t),\nhas the form\nu(t) = -sgn(x1(t) + 0.5x2(t)2sgn(x2(t))).\n(a) Find a Lyapunov function V : R2 inf! R2 for the closed loop system,\nsuch that V (x(t)) is strictly decreasing along all solutions of system\nequations except the equilibrium solution x(t) ≥ 0.\nThe original problem set contained a typo: a \"-\" sign in the expression for u(t) was\nmissing. For completeness, a solution which applies to this case is supplied in the\nnext section.\nA hint was given in the problem formulation, stressing that u is a minimal time\ncontrol. What is important here is that it takes only finite time for for a system\nsolution to reach the origin. Therefore, the amount of time it takes for the system to\nreach the origin can be used as a Lyapunov function. Let us verify this by inspection.\nSystem equations are Lipschitz continuous outside the curve\n0 = {x = [x1; x2] : x1 = -0.5x2|x2|},\n\n:\np\nSolving them explicitly (outside ) yields\n⎨\n⎩\nc1 + c2t - 0.5t2\nx(t) =\nfor x(t) ≤ + = {x = [x1; x2] : x1 > -0.5x2|x2|},\nc2 - t\n⎨\n⎩\nc1 + c2t + 0.5t2\nx(t) =\nfor x(t) ≤ - = {x = [x1; x2] : x1 < -0.5x2|x2|}.\nc2 + t\nIn addition, no solutions with initial condition x(0) = [-0.5r2; r] or x(0) = [0.5r2; -r],\nwhere r > 0, exists, unless the sgn(·) function is understood as the set-valued sign\n< {1},\ny > 0,\nsgn(y) =\n[-1, 1],\ny = 0,\n{-1},\ny < 0,\nin which case the corresponding soltion trajectories lie in 0. Finally, there is an\nequilibrium solution x(t) ≥ 0.\nThe corresponding Lyapunov function (time it take to reach the origin) is now easy\nto calculate, and is given by\n⎪\nx2 + 2 x2/2 + x1,\nfor x1 + x2|x2|/2 ∀ 0,\nV (x) =\np 2\n-x2 + 2 x2/2 - x1, for x1 + x2|x2|/2 · 0.\nAs expected, dV/dt = -1 along system trajectories, and x = 0 is the only global\nminimum of V .\n(b) Find out whether the equilibrium remains asymptotically stable when\nthe same controller is used for the perturbed system\n⎪ x 1(t) = x2(t),\n|u(t)| · 1,\nx 2(t) = -2x1(t) + u(t),\nwhere 2 > 0 is small.\nThe Lyapunov function V (x) designed for the case 2 = 0 is not monotonically non-\nincreasing along trajectories of the perturbed system (2 > 0). Indeed, when\nx1 = -0.5r 2 + r , x2 = r > 0,\nwe have\nV (x(t)) = -2x1 - 1 - p x1x\n,\n0.5x2 + x1\nwhich is positive when r > 0 is small enough.\n\nHowever, the stability can be established for the case 2 > 0 using an alternative\nLyapunov function. One such function is\n⎪\n22x2 + (1 + 24|x1|)2 ,\nfor |x1| ∀x2/2,\nV1(x) =\n22x2 + (1 + 24x2/2)2 , for |x1| · x2/2.\nBy considering the two regions |x1| ∀x2/2 and |x1| · x2/2 separately, it is easy to\nsee that dV1(x(t))/dt · 0, and dV1(x(t))/dt = 0 only for\nx(t) ≤N = {[x1; x2] : |x1| ∀x2/2}.\nNote that the origin is the only global minimum of V1. Also, V1 is continuous and\nall level sets of V1 are bounded. Hence, if a solution of the system equations does\nx¤ 6\nnot converge to the origin as t ! →, it must have a limit point = 0 such that,\nfor the solution x¤(t) of the system equations with x¤(0) = x¤,\nx¤) > min V (\nV (x¤(t)) = V (\nx) 8 t ∀0.\nx→R\n\nThis implies that x¤(t) ≤N for all t ∀0. However, no solution except the equilib\nrium can remain forever in N . Hence the equilibrium x = 0 is globally asymptoti\ncally stable.\nUsing the fact that a non-equilibrium solution of system equations cannot stay for\never in the region where V (x(t)) = 0, in order to prove stability of the equilibrium\nas demonstrated above, is referred to as the La Salle's invariance principle. Essen\ntially, the formulation and a proof of this popular general result are contained in\nthe solution above.\nProblem 3.3 with typo\nThe optimal minimal-time controller for the double integrator system\nwith bounded control\n⎪ x 1(t) = x2(t),\n|u(t)| · 1\nx 2(t) = u(t),\nhas the form\nu(t) = sgn(x1(t) + 0.5x2(t)2sgn(x2(t))).\n(a) Find a Lyapunov function V : R2 inf! R2 for the closed loop system,\nsuch that V (x(t)) is strictly decreasing along all solutions of system\nequations except the equilibrium solution x(t) ≥0.\n\n<\n:\nThe system is unstable (all solutions except x(t) ≥0 converge to infinity). However,\nthis does not affect existence of strictly decreasing Lyapunov functions. For example,\n⎧ -x2,\nx1 + 0.5x2|x2| > 0,\n⎧\n-x2,\nx1 + 0.5x2|x2| = 0, x2 ∀0,\nV ([x1; x2]) = ⎧ x2,\nx1 + 0.5x2|x2| < 0,\n⎧\nx2,\nx1 + 0.5x2|x2| = 0, x2 · 0.\nTo show that V is valid, note that the trajectories of this system are given by\n⎨\n⎩\nc1 + c2t + 0.5t2\nx(t) =\nc2 + t\nwhen x1 + 0.5x2|x2| > 0 or x1 + 0.5x2|x2| = 0 and x2 ∀0, and by\n⎨\n⎩\nc1 + c2t -0.5t2\nx(t) =\nc2 -t\nwhen x1 + 0.5x2|x2| < 0 or x1 + 0.5x2|x2| = 0 and x2 · 0.\n(b) Find out whether the equilibrium remains asymptotically stable when\nthe same controller is used for the perturbed system\n⎪ x 1(t) = x2(t),\n|u(t)| · 1,\nx 2(t) = -2x1(t) + u(t),\nwhere 2 > 0 is small.\nAs can be expected, the equilibrium of the perturbed system is unstable just as the\nequilibrium of the unperturbed one is. To show this, note that for\nx ≤K = {[x1; x2] : x1 ≤(0, 1/(22)), x2 ∀0}\nwe have x 1 > 0 and x2 ∀0.5. Hence, a solution x = x(t) such that x(0) ≤K cannot\nsatisfy the inequality |x(t)| < 1/(2e) for all t ∀0."
    },
    {
      "category": "Resource",
      "title": "hw4_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/043739f67a45cfe40bf395958f8875b1_hw4_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 41\nProblem 4.1\n\nFind a function V : R3 →! R+ which has a unique minimum at x = 0, and is strictly\nmonotonically decreasing along all non-equilibrium trajectories of system\nx 1(t)\n= -x1(t) + x2(t)2 ,\nx 2(t)\n= -x2(t)3 + x3(t)4 ,\nx 3(t)\n= -x3(t)5 .\nProblem 4.2\nSystem ¢ takes arbitrary continuous input signals v : [0, 1) →! R and produces contin\nuous outputs w : [0, 1) →! R in such a way that the series connection of ¢ and the LTI\nsystem with transfer function G0(s) = 1/(s + 1), described by equations\nx 0(t) = -x0(t) + w(t),\nw(·) = ¢(v(·)),\nhas a non-negative storage function with supply rate\n3⁄40(\nv,\nw - 0.9\nv - w).\nx0, w) = (\nx0)(\n\n(a) Find at least one nonlinear system ¢ which fits the description.\n(b) Derive constraints to be imposed on the values G(j!) of a transfer function\nG(s) = C(sI - A)-1B\n1Posted October 1, 2003. Due date October 8, 2003\n\nwith a Hurwitz matrix A, which guarantee that x(t) ! 0 as t ! 1 for every\nsolution of\nx (t) = Ax(t) + Bw(t), v(t) = Cx(t), w(·) = ¢(v(·)).\nMake sure that your conditions are satisfied at least for one non-zero transfer func\ntion G = G(s).\nProblem 4.3\nFor the pendulum equation\ny (t) + y + sin(y) = 0,\nfind a single continuously differentiable Lyapunov function V = V (y, y ) that yields the\nmaximal region of attraction of the equilibrium y = y = 0. (In other words, the level set\nx 2 R2 : V (\n{\nx) < 1}\nschould be a union of disjoint open sets, one of which is the attractor of the zero\nequilibrium, and V (y(t), y (t)) schould have negative derivative at all points of except\nthe origin.)"
    },
    {
      "category": "Resource",
      "title": "ps4sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/93e82ee931af87b4c14b67be41dc137f_ps4sol_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 4 Solutions1\nProblem 4.1\n\nFind a function V : R3 ≥! R+ which has a unique minimum at x = 0, and\nis strictly monotonically decreasing along all non-equilibrium trajecto\nries of system\nx 1(t)\n= -x1(t) + x2(t)2 ,\nx 2(t)\n= -x2(t)3 + x3(t)4 ,\nx 3(t)\n= -x3(t)5 .\nLet us begin with collecting storage function and compatible quadratic supply rate\npairs for the system. Naturally, positive definite functions of system states are a good\nstarting point. For V1(x) = x2 we have\nV1 = -2x1 + 2x1x2 · -x1 + w 2 = 3⁄41,\nwhere w1 = x2, and the classical inequality\n2ab · a 2 + b2\nwas used. For V2 = x2 we have\nV 2 = -2x2 + 2x2x3 · -w1 + 2w2 = 3⁄42,\n8/3\nwhere w2 = x3\nand the inequality\n2ab3 · a 4 + 2b4\n1Version of October 11, 2003\n\n(a weakened version of a classical inequality) was used. Finally, for V3 = 3x 4/3 we have\n16/3\nV 3 = -4x3\n= -4w2 .\nNow, for\nV = c1V1 + c2V2 + c3V3\nwe have\nV · c13⁄41 + c23⁄42 + c33⁄43 = -c1x1 + (c1 - c2)w1 + (2c2 - 4c3)w2 .\nTaking c1 = 1, c2 = c3 = 2 yields a continuously differentiable Lyapunov function\n4/3\nV (x) = x 2 + 2x2 + 6x3\nfor which the derivatives along system trajectories are bounded by\n16/3\nV (x) · -x 2 - x2 - 4x3\n.\nProblem 4.2\nSystem ¢ takes arbitrary continuous input signals v : [0, →) ≥! R and\nproduces continuous outputs w : [0, →) ≥! R in such a way that the series\nconnection of ¢ and the LTI system with transfer function G0(s) = 1/(s +\n1), described by equations\nx 0(t) = -x0(t) + w(t),\nw(·) = ¢(v(·)),\nhas a non-negative storage function with supply rate\n3⁄40(\nv,\nw - 0.9\nv - w).\nx0, w) = (\nx0)(\n\n(a) Find at least one nonlinear system ¢ which fits the description.\nThe ideal saturation nonlinearity\n1⁄2\nsat(y) =\ny/|y|,\ny,\n|y| ∀ 1,\n|y| · 1,\nis a nice example of ¢ satisfying the conditions. Indeed, if\nx 0(t) = -x0(t) + sat(v(t)),\nx0(0) = 0\nthen |x0(t)| · 1 for all t ∀ 0. Hence\n(v(t) - sat(v(t)))(sat(v(t)) - x0(t)) ∀ 0 8 t ∀ 0\n(if v(t) ≤ [-1, 1] then the product equals zero, otherwise the multipliers have same\nsign).\n\n·\n\n·\n\n·\n·\n\n·\n·\n\n·\n·\n\n(b) Derive constraints to be imposed on the values G(j!) of a transfer\nfunction\nG(s) = C(sI -A)-1B\nwith a Hurwitz matrix A, which guarantee that x(t) ! 0 as t ! →for\nevery solution of\nx (t) = Ax(t) + Bw(t), v(t) = Cx(t), w(·) = ¢(v(·)).\nMake sure that your conditions are satisfied at least for one non\nzero transfer function G = G(s).\nLet us prove that condition\n0.1 -j!\nRe (1 -G(j!))\n> 0 8 ! ≤R\n(4.1)\n1 -j!\nis sufficient to guarantee that x(t) ! 0 as t ! →. Indeed, since A is a Hurwitz\nmatrix, and G is strictly proper, there exists π > 0 such that\n0.1 -j!\nRe (1 -G(j!))\n> π(1 + |(j!I -A)-1B| ) 8 ! ≤R.\n1 -j!\nTherefore, the frequency inequality conditions of the KYP Lemma are satisfied for\nthe existence of a matrix P = P 0 such that\nx\nP\nAx + Bw\n· (w-Cx)(w-0.9x0)-π(|x|2+|w| ) 8 w, x0 ≤R, x ≤Rn .\nx0\nw -x0\nTo show that P is positive definite, substitute w = x0 into the last inequality, which\nyields\nx\nP\nAx + Bx0\n· -π(|x|2 + |0.9x0| ) 8 x0 ≤R, x ≤Rn ,\nx0\n-0.1x0\nwhich is equivalent to the Lyapunov inequality\nP ˆA + Aˆ0P = -Q,\nwhere\n·\n\n·\n\nˆA =\nA\nB\n-0.1\n, Q = π\nI\n0.81\n.\nSince Aˆ is a Hurwitz matrix, and Q = Q0 > 0, it follows that P > 0.\nNow\nx\nx\nV = V0 +\nP\nx0\nx0\n\nis a non-negative storage function for the closed loop system, with supply rate\n3⁄4 = -π(|x|2 + |w| ).\nHence w is square integrable over the interval [0, →). Since\nx = Ax + Bw,\nand A is a Hurwitz matrix, this implies that x(t) ! 0 as t ! →.\nSince\n0.1 -j!\nRe\n∀0.1 8 ! ≤R,\n1 -j!\ncondition (4.1) is satisfied for all G with sufficiently small H-Infinity norm (maximal\nabsolute value of the frequency response).\nProblem 4.3\nFor the pendulum equation\ny (t) + y + sin(y) = 0,\nfind a single continuously differentiable Lyapunov function V = V (y, y )\nthat yields the maximal region of attraction of the equilibrium y = y = 0.\n(In other words, the level set\nx ≤R2 : V (\n{\nx) < 1}\nschould be a union of disjoint open sets, one of which is the attractor\nof the zero equilibrium, and V (y(t), y (t)) schould have negative derivative\nat all points of except the origin.)\nNote that the problem can be interpreted as follows: given the initial angular position\nand angular velocity of a pendulum, find the number of complete rotations it will have\nbefore settling at an equilibrium position. An \"exact analytical\" answer can be obtained\nby stating that the maximal region of attraction is the area bounded by the four separa\ntrix solutions of the system equation, converging to the two unstable equilibria (0, ±1⁄4).\nHowever, this \"exact\" answer (which cannot be expressed in elementary functions) will\nbe of no use in the case when the pendulum model is slightly modified (a different friction\nmodel, flexibility of the pendulum taken into account, etc.) On the other hand, one can\nexpect that an estimate obtained by using a Lyapunov function will be more \"robust\"\nwith respect to various perturbations of the model.\nAn obvious Lyapunov function is given by the system energy (potential plus kinetic)\nV0(y, y ) = 0.5 y 2 -cos(y), dV/dt = -y (t)2 .\n\n1⁄2\nTo estimate the region of attraction of the equilibrium at the origin, using this Lyapunov\nfunction, one may can find a constant c such that the level set\nL(V0, c) = {(y0, y1) : V0(y0, y1) < c}\ndoes not contain a path connecting the origin with any other equilibrium of the system.\nIt is easy to see that taking c = 1 does the job, and yields the region of attraction 0\ngiven by\n0 = {(y, y ) : 0.5 y 2 - cos(y) < 1, -1⁄4 < y < 1⁄4}.\nThis appears to be a very poor estimate, taking into account what we know about the\ntrue maximal region of attraction.\nTo get a better Lyapunov function, one can try to construct it in such a way that the\nlevel sets are polytopes centered at the origin. Remember that a function V is a Lyapunov\nfunction if and only if the system trajectories never leave any of its level sets. Since the\nboundary of a polytope in R2 is a segment, it is especially easy to check this condition\nfor the Lyapunov functions candidates with polytopic level sets.\nOne of the simplest examples of a Lyapunov function constructed this way is given by\n|y| + |y |,\nyy ∀ 0,\nV1(y, y ) =\nmax{y, y }, yy · 0.\nIt is easy to check that V1 is a Lyapunov function for the pendulum system in the area\n1 = {(y, y ) : V1(y, y ) < 1⁄4},\nwhich is also the resulting estimate of the region of attraction.\nThe previous estimate 0 is contained in 1. Even better estimates can be obtained\nby using other lyapunov functions with polytopic level sets."
    },
    {
      "category": "Resource",
      "title": "hw5_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/d3af478e643c1c37567a89f423804e78_hw5_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nProblem Set 51\nProblem 5.1\ny(t) →a is an equilibrium solution of the differential equation\ny(3)(t) + y(t) + y(t) + 2 sin(y(t)) = 2 sin(a),\nwhere a 2 R and y(3) denotes the third derivative of y. For which values of a 2 R is this\nequilibrium locally exponentially stable?\nProblem 5.2\nIn order to solve a quadratic matrix equation X 2 + AX + B = 0, where A, B are given\nn-by-n matrices and X is an n-by-n matrix to be found, it is proposed to use an iterative\nscheme\nXk+1 = Xk\n2 + AXk + Xk + B.\nAssume that matrix X¤ satisfies X¤\n2 + AX¤ + B = 0. What should be required of the\neigenvalues of X¤ and A+X¤ in order to guarantee that Xk ! X¤ exponentially as k ! 1\nwhen ∈X0 -X¤∈is small enough? You are allowed to use the fact that matrix equation\nay + yb = 0,\nwhere a, b, y are n-by-n matrices, has a non-zero solution y if and only if det(sI -a) =\ndet(sI + b) for some s 2 C.\n1Posted October 22, 2003. Due date October 29, 2003\n\n:\nProblem 5.3\nUse the Center manifold theory to prove local asymptotic stability of the equilibrium at\nthe origin of the Lorentz system\n< x = - x + yz,\ny = -3⁄4y + 3⁄4z,\nz = -yx + 1⁄2y - z,\nwhere , 3⁄4 are positive parameters and 1⁄2 = 1. Estimate the rate of convergence of\nx(t), y(t), z(t) to zero.\nProblem 5.4\nCheck local asymptotic stability of the periodic trajectory y(t) = sin(t) of system\ny (t) + y(t) + y 3 = - sin(t) + cos(t) + sin3(t).\nProblem 5.5\nFind all values of parameter a 2 R such that every solution x : [0, 1) inf! R2 of the ODE\n⎧\n⎨\ncos(2t)\na\nx (t) = β\ncos4(t) sin4(t)\nx(t)\nconverges to zero as t ! 1 when β > 0 is a sufficiently small constant."
    },
    {
      "category": "Resource",
      "title": "t1_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/e29f891085e77d45bd9cb032d6cf5fd1_t1_6243_2003.pdf",
      "content": "μ·\n\n1⁄2\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nTake-Home Test 11\nFor each problem, give an answer and provide supporting arguments, not to exceed one\npage per problem. Return your test paper by 11.05 am on Friday October 17, in the\nclassroom. Remember that collaboration is not allowed on test assignments.\nProblem T1.1\nFind all values of μ → R for which the function V : R2 inf! R, defined by\nx1\nV\n\n= max{|\n\nx1|, |x2|}\nx2\nis monotonically non-increasing along solutions of the ODE\nx 1(t)\n= μx1(t) + sin(x2(t)),\nx 2(t)\n= μx2(t) - sin(x1(t)).\nHint: | sin(y)| < |y| for all y ≤= 0, and sin(y)/y ! 1 as y ! 0.\n1Posted October 16, 2003. Due at 11.05 am on October 17, 2003\n\nProblem T1.2\nFind all values of r → R for which differential inclusion of the form\nx (t) → (x(t)),\nx(0) = x0,\nwhere : R2 inf! 2R\nis defined by\nx) = {f( x|)} for x ≤\n(\nx/|\n= 0,\n(0) = {f(y) : y = [y1; y2] → R2 , |y1| + |y2| · r},\nhas a solution x : [0, ∀) inf! R2 for every continuous function f : R2 inf! R2 and for every\ninitial condition x0 → R2 .\nProblem T1.3\nFind all values q, r → R for which x0 = 0 is not a (locally) stable equilibrium of the ODE\nx (t) = Ax(t) + B(Cx(t))1/3\nfor every set of matrices A, B, C of dimensions n-by-n, n-by-1, and 1-by-n respectively,\nsuch that A is a Hurwitz matrix and\nRe[(1 + j!q)G(j!)] > r 8 ! → R\nfor\nG(s) = C(sI - A)-1B."
    },
    {
      "category": "Resource",
      "title": "t1sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/50dbc16ab8557fbba8b270442a55cb58_t1sol_6243_2003.pdf",
      "content": "μ·\n\n1⁄2\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nTake-Home Test 1 Solutions1\nProblem T1.1\nFind all values of μ ≤ R for which the function V : R2 inf≈ R, defined by\nx1\nV\n\n= max{| x1 ,\nx2\n| |x2|}\nis monotonically non-increasing along solutions of the ODE\nx 1(t)\n= μx1(t) + sin(x2(t)),\nx 2(t)\n= μx2(t) - sin(x1(t)).\nAnswer: μ ∀ -1.\nProof For μ ∀ -1, x1 = 0 we have\n∈\n1 d\nx1 = μx1 + x1 sin(x2)\n2 dt\n<\n( x1\nx2 ),\n-|x1| |\n| -|\n|\nand hence x1 is strictly monotonically decreasing when x = 0 and x1\nx2 . Similarly,\n|\n|\n∈\n|\n| √|\n|\nx2 is strictly monotonically decreasing when x = 0 and x2\nx1 . Hence, when μ ∀ -1,\n|\n|\n∈\n|\n| √|\n|\nV (x) is strictly monotonically decreasing along non-equilibrium trajectories of the system.\nFor μ > 1, x1(0) = r, x2(0) = r, where r > 0 is sufficiently small we have\nx 1(0) = μr - sin(r) > 0,\nhence\nV (x(t)) √ x1(t) > r = V (x(0))\nwhen t > 0 is small enough, which proves that V is not monotonically decreasing.\n1Version of October 20, 2003\n\nProblem T1.2\nFind all values of r ≤ R for which differential inclusion of the form\nx (t) ≤ (x(t)),\nx(0) = x0,\nwhere : R2\nis defined by\ninf≈ 2R\n(\nx/|x|)} for ∈\nx) = {f(\n\nx = 0,\n(0) = {f(y) : y = [y1; y2] ≤ R2 ,\ny1 + y2 ∀ r},\n|\n|\n|\n|\nhas a solution x : [0, →) inf≈ R2 for every continuous function f : R2 inf≈ R2\nand for every initial condition\nAnswer: r √\np\n2.\nx0 ≤ R2 .\nProof First, let us show that existence of solutions is not guaranteed when r <\np\n2. Let\nτ > 0 be such that 2τ <\np\n2 - r. Define\nμ·\n\n·\n\nx1\n0.5\np\n2(1 - τ) - x1\nf\n=\n.\nx2\n0.5\np\n2(1 - τ) - x2\nLet us show that, for this f, the differential inclusion x(t) =≤ (x(t)) will have no solutions\nx : [0, →) inf≈ R2 with x(0) = 0. Indeed, since\nx 0f(x/|x|) < 0\nx = 0, x ≤ R2 ,\n∈\nx(t) is strictly monotonically decreasing when x(t) = 0. Therefore x(0) = 0 implies\n|\n|\n∈\nx(t) = 0 for t √ 0. Hence x(t) = 0 ≤ (0). However, for r <\np\n2, and for this particular\nselection of f(·), zero is not an element of (0). The contradiction shows that no solution\nwith x(0) = 0 exists.\nTo prove existence of solutions for r √\np\n2, one is tempted to use the existence theorem\nrelying on convexity and semicontinuity of (·). However, these assumptions are not\nnecessarily satisfied in this case, since the set (0) does not have to be convex. Instead,\nnote that, by the continuity of f, existence of a solution x : [t0, t0 + x0\nwith\n|\n|/M) inf≈ R2\nx0 is guaranteed for all\nx(t0) =\nx0 = 0. Hence, it is sufficient to show that a solution\n∈\nx0 : [0, →) inf≈ R2 with x0(0) = 0 exists.\nTo do this, consider two separate cases: 0 ≤ (0) and 0 ∈≤ (0). If 0 ≤ (0) then\nx(t) ≥ 0 is the desired solution of the differential inclusion. Let us show that 0 ∈≤ (0)\nimplies existence of a solution q ≤ (0, →), u = 1 of the equation f(u) = qu. Indeed, if\n0 ∈≤ (0) and r √\np\n2 then 0 = f(¿u) for all\n|\n¿\n|\n≤ [0, 1], u = 1, and hence\n∈\n| |\nf(¿u)\n(¿, u) inf≈ f(¿u)\n|\n|\n\nis a homotopy between the vector fields f1 :\nf(u) and f0 : u inf≈ f(0)/ f(0) .\nu inf≈ f(u)/|\n|\n|\n|\nSince the index of the constant map f0 is zero, the index of f1 is zero as well. However,\nassuming that f(u) = qu for q ≤ (0, →), u = 1 yields a homotopy\n∈\n| |\n(¿, u) inf≈ ¿u + (1 - ¿)f(u)\n¿u + (1 - ¿)f(u)\n|\n|\nbetween f1 and the identity map, which is impossible, since the identity map has index 1.\nHence f(u) = qu for some q > 0, u = 1, which yields x0(t) = qtu as as a valid\n| |\nsolution x0 : [0, →) inf≈ R2 of the differential inclusion.\nProblem T1.3\nFind all values q, r ≤ R for which x0 = 0 is not a (locally) stable equilibrium\nof the ODE\nx (t) = Ax(t) + B(Cx(t))1/3\n(1.1)\nfor every set of matrices A, B, C of dimensions n-by-n, n-by-1, and 1-by-n\nrespectively, such that A is a Hurwitz matrix and\nRe[(1 + j!q)G(j!)] > r 8 ! ≤ R\n(1.2)\nfor\nG(s) = C(sI - A)-1B.\nAnswer: r √ 0, q ≤ R arbitrary (note, however, that for r √ 0 (1.2) implies q √ 0).\nProof If r < 0, take A = -1, B = 0, C = 1 to get an example of A, B, C satisfying the\nconditions and such that x0 = 0 is a (globally asymptotically) stable equilibrium of (1.1).\nNow consider the case r\n0. Then, informally speaking, the frequency domain\n√\ncondition means some sort of \"passivity\" of G, while (1.1) describes a positive feedback\ny1/3\ninterconnection of G with nonlinearity y inf≈ w =\n, which can be characterized as\nhaving arbitrarily large positive gain for x 1⁄4 0. Hence one expects instability of the zero\nequilibrium of (1.1).\nTo show local instability, let us prove existence of a Lyapunov function V = V (x) for\nwhich 0 is not a local minimum, and\nd V (x(t)) < 0 whenever Cx(t) ≤ (0, τ0)\ndt\n|\n|\nfor some τ0 > 0. Note that this will imply instability of the equilibrium x0 = 0, since\nevery solution with V (x(0)) < V (0) and x(0) < τ0/2 C will eventually cross the sphere\n|\n|\n| |\nx(0) = τ0/2 C (otherwise Cx(t) ∀ τ0/2 for all t √ 0, hence V (x(t)) is monotonically\n|\n|\n| |\n|\n|\n\nx of x(·) satisfy C\nnon-increasing, and all limit points\nx = 0, therefore every solution x¤(t)\nof (1.1) beginning at such limit point satisfies Cx¤(t) = 0 and hence converges to the\norigin, which contradicts V (x(0)) < V (0)).\nBy introducing w(t) = (Cx(t))1/3, system equations can be re-written in the form\nx (t) = Ax(t) + Bw(t).\nConsider first the (simpler) case when r > 0 (and hence q > 0). Then one can use the\ninequality\nw(t)Cx(t) ∀r w(t) 2 ,\n|\n|\nfor sufficiently small Cx(t) . Condition (1.2) together with the KYP Lemma yields exis\n|\n|\ntence of a matrix P = P 0 such that\nwC\nwC(A\nw) -r w\n√2 0P(A\nw) 8\nw ≤R.\n\nx + q\nx + B\nx\nx + B\nx ≤Rn ,\n| | 2\nSubstituting w = (Cx)1/3, we get\nd [x 0Px -0.75q Cx| 4/3] ∀|Cx 4/3 -r Cx| 2/3 ,\ndt\n|\n|\n|\nwhich is exactly what is needed, because y4/3 -ry2/3 < 0 for y ≤(0, pr). In addition, for\nx0 ≤Rn such that C\nevery\nx0 = 0 the expression\n∈\nx) = 0P\nx\nV (\nx\nx -0.75q C 4/3\n|\n|\nx = r\nis negative when\nx0 and r > 0 is small enough.\nTo prove the answer in the general case, note that the inequality\nw(t)Cx(t) > R Cx(t) 2\n|\n|\nis satisfied whenever Cx(t) ≤ (0, τ) with τ > 0 and R = τ-2/3, i.e. R can be made\n|\n|\narbitrarily large by selecting an appropriate τ > 0. Therefore the derivative bound for\nV (x(t)) = x(t)0Px(t) will hold if\nx + B\nx 2 -\nx\nx ≤Rn ,\nx 0P(A\nw) ∀R C\nwC 8\nw ≤R.\n(1.3)\n|\n|\nAccording to the KYP Lemma, such P = P 0 exists if\nR G(j!) 2 -Re(G(j!)) > 0 8 ! ≤R,\n|\n|\nor, equivalently,\nRe(1/G(j!)) < R 8 ! ≤R.\nMoreover, substituting w = (R + K)Cx, where K > 0 is a constant, into (1.3) yields\nP(A + BKC) + (A + BKC)0P ∀-KC0C.\n\nTherefore, P = P 0 cannot be positive semidefinite if A + BKC has eigenvalues with\npositive real part.\nWe will rely on the following statement from the linear system theory: if H(s) is a\nstable proper rational transfer function which is positive real (i.e. Re(H(j!)) > 0 for all\n! ≤ R) then Re(s) > 0 whenever Re(s) > 0, and the relative degree of H is not larger\nthan one.\nConsider H(s) = (1 + qs)G(s). By assumption, H is positive real and proper. Hence\nq √ 0 (otherwise H(-1/q) = 0). If relative degree of H is zero then q > 0, and hence\nsG(s) converges to a non-zero limit H¤ as s ≈→. Since (1 + qr)G(r) > 0 for r > 0, it\nfollows that H¤ > 0, and hence\nRe\n= Re j!\nG(j!)\nG(j!)\nis bounded as ! ≈→.\nIf relative degree of H is one then sH(s) converges to a positive limit as s ≈→, and\nhence\nj! -(j!)2\nRe\n= Re\nG(j!)\n(j!)2G(j!)\nis bounded from above as ! ≈→. Finally, since G(r) > 0 and G(r) ≈0 as r ≈+→,\nit follows that the equation 1 = KG(r) has a positive solution r for all sufficiently large\nK > 0. Hence matrix A + BKC has a positive eigenvalue for all sufficiently large K > 0."
    },
    {
      "category": "Resource",
      "title": "t2_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/91aeb0fdcbfc127ba5696d4283c1f1dd_t2_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nTake-Home Test 21\nFor each problem, give an answer and provide supporting arguments, not to exceed two\npages per problem. Return your test paper by 11.05 am on Wednesday November 19, in\nthe classroom. Remember that collaboration is not allowed on test assignments.\nProblem T2.1\nSystem of ODE equations\nx (t) = Ax(t) + BA(Cx(t) + cos(t)),\n(1.1)\nwhere A, B, C are constant matrices such that CB = 0, and A : Rk 7! Rq is continuously\ndifferentiable, is known to have a locally asymptotically stable non-equilibrium periodic\nsolution x = x(t). What can be said about trace(A) ? In other words, find the set ¤ of all\nreal numbers such that = trace(A) for some A, B, C, A such that (1.1) has a locally\nasymptotically stable non-equilibrium periodic solution x = x(t).\nProblem T2.2\nFunction g1 : R3 7! R3 is defined by\n⎨⎦\n⎣1\n⎦\n⎣\nx1\n⎩4 x2 ⎤A = 4\ng1\nx1 ⎤ .\nx3\n1Posted November 18, 2003. Due at 11.05 am on November 19, 2003\n\n(a) Find a continuously differentiable function g2 : R3 7! R3 such that the driftless\nsystem\nx (t) = g1(x(t))u1(t) + g2(x(t))u2(t)\n(1.2)\nis completely controllable on R3 .\nR3\nR3\n(b) Find continuously differentiable functions g2 :\n7! R3 and h :\n7! R such that\n→h( = 0 for all\nx) ∈\nx 2 R3 and h(x(t)) is constant on all solutions of (1.2). (Note:\nfunction g2 in (b) does not have to be (and cannot be) the same as g2 in (a).)\n(c) Find a continuously differentiable function g2 : R3 7! R3 such that the driftless\nsystem (1.2) is not completely controllable on R3, but, on the other hand, there\nexists no continuously differentiable function h : R3 7! R such that →h( = 0 for\nx) ∈\nall x 2 R3 and h(x(t)) is constant on all solutions of (1.2).\nProblem T2.3\nAn ODE control system model is given by equations\n⎞\n⎠ x 1(t)\n=\nx2(t)2 + u(t),\nx 2(t)\n=\nx3(t)2 + u(t),\n(1.3)\n⎧ x 3(t)\n= p(x1(t)) + u(t).\n(a) Find all polynomials p : R 7! R such that system (1.3) is full state feedback\nlinearizable in a neigborhood of x = 0.\n(b) For each polynomial p found in (a), design a feedback law\nu(t) = K(x1(t), x2(t), x3(t)) = Kp(x1(t), x2(t), x3(t))\nwhich makes the origin a locally asymptotically stable equilibrium of (1.3).\n(c) Find a C1 function p : R 7! R for which system (1.3) is globally full state feedback\nlinearizable, or prove that such p(·) does not exist."
    },
    {
      "category": "Resource",
      "title": "t2sol_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/f03b7d3e6b11f3c6a44cbf93b3d1ba95_t2sol_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nTake-Home Test 2 Solutions1\nProblem T2.1\nSystem of ODE equations\nx (t) = Ax(t) + Bψ(Cx(t) + cos(t)),\n(1.1)\nwhere A, B, C are constant matrices such that CB = 0, and ψ : Rk\nis\n∈! Rq\ncontinuously differentiable, is known to have a locally asymptotically\nstable non-equilibrium periodic solution x = x(t). What can be said about\ntrace(A) ? In other words, find the set ¤ of all real numbers such that\n= trace(A) for some A, B, C, ψ such that (1.1) has a locally asymptotically\nstable non-equilibrium periodic solution x = x(t).\nAnswer: trace(A) < 0.\nLet x0(t) be the periodic solution. Linearization of (1.1) around x0(·) yields\nα (t) = Aα(t) + Bh(t)Cα(t),\nwhere h(t) is the Jacobian of ψ at x0(t), and\nx(t) = x0(t) + α(t) + o( α(t) ).\n|\n|\nPartial information about local stability of x0(·) is given by the evolution matrix M(T),\nwhere T > 0 is the period of x0(·): if the periodic solution is asymptotically stable then\nall eigenvalues of M(T ) have absolute value not larger than one. Here\nM (t) = (A + Bh(t)C)M(t),\nM(0) = I,\n1Version of November 25, 2003\n\n°\n°\n°\n°\nand hence\n⎝\n⎛Z T\ndet M(T) = exp\ntrace(A + Bh(t)C)dt .\nSince\ntrace(A + Bh(t)C) = trace(A + CBh(t)) = trace(A),\ndet(M(T )) > 1 whenever trace(A) > 0. Hence trace(A) ≈0 is a necessary condition for\nlocal asymptotic stability of x0(·).\nSince system (1.1) with k = q = 1, ψ(y) ≥y,\n⎡\n\n⎡\n\nA =\n-a\n£\n¤\n, B =\n, C =\na\n-\nhas periodic stable steady state solution\n⎡\n\n(1 + a2)-1 cos(t) + a(1 + a2)-1 sin(t))\nx0(t) =\nfor all a > 0, the trace of A can take every negative value. Thus, to complete the solution,\none has to figure out whether trace of A can take the zero value.\nIt appears that the volume contraction techniques are better suited for solving the\nquestion completely. Indeed, consider the autonomous ODE\n⎞\n⎨ z 1(t)\n=\nz2(t),\n⎨\n⎠ z 2(t)\n=\nz1(t),\n⎛ -\n⎝\n(1.2)\n⎨\nz1(t)\n⎨\n⎧ z 3(t)\n= Az3(t) + Bψ Cz3(t) + p\nz1(t)2+z2(t)2\n,\ndefined for z1 + z2\n2 inf= 0. If (1.1) has an asymptotically stable periodic solution x0 = x0(t)\nthen, for δ > 0 small enough, solutions of (1.2) with\n°⎪\n⎣\n°\n⎪\n⎣\n°\nz1(0)\n°\n°4 z2(0) ⎤ -z ° ≈δ, z 4\n⎤\n°\nz3(0)\n°\nx0(0)\nsmall enough satisfy\nlim z3(t) -x0(t + β) = 0,\nt!1\nwhere β 1⁄4 0 is defined by z2(-β) = 0. In particular, the Euclidean volume of the image of\nthe the ball of radius δ centered at z under the differential flow defined by (1.2) converges\nto zero as t ! →. Since the volume is non-increasing when trace(A) √0, we conclude\nthat trace(A) < 0.\n\nProblem T2.2\nFunction g1 : R3 ∈! R3 is defined by\n⎩⎪\n⎣1\n⎪\n⎣\nx1\ng1 ⎦4 x2 ⎤A = 4 x1 ⎤ .\nx3\n(a) Find a continuously differentiable function g2 : R3 ∈! R3 such that\nthe driftless system\nx (t) = g1(x(t))u1(t) + g2(x(t))u2(t)\n(1.3)\nis completely controllable on R3 .\nFor\n⎪\n⎣\ng2(x) = 4 0 ⎤ = const,\nwe have\n⎪\n⎣\ng3 = [g1, g2] = 4 1 ⎤ .\nSince g1(x), g2, g3 form a basis in R3 for all x, the resulting system (1.3) is completely\ncontrollable on R3 .\n(b) Find continuously differentiable functions g2 : R3 ∈! R3 and h : R3 ∈!\nx) = 0 for all\nR such that ≡h( inf\nx ≤ R3 and h(x(t)) is constant on all\nsolutions of (1.3). (Note: function g2 in (b) does not have to be (and\ncannot be) the same as g2 in (a).)\nFor example,\n⎪\n⎣\n⎩⎪\n⎣1\nx1\ng2(x) = 4 1 ⎤ = const,\nh ⎦4 x2 ⎤A = x3.\nx3\n(c) Find a continuously differentiable function g2 : R3 ∈! R3 such that\nthe driftless system (1.3) is not completely controllable on R3, but,\non the other hand, there exists no continuously differentiable func-\nx) = 0 for all\ntion h : R3 ∈! R such that ≡h(\ninf\nx ≤ R3 and h(x(t)) is\nconstant on all solutions of (1.3).\nFor\n⎪\n⎣\ng2(x) = 4 x1 ⎤ ,\nx3\n\nwe have\n⎪\n⎣\ng3 = [g2, g1] = 4 1 ⎤ ,\nand hence g1(x), g2, g3 form a basis in R3 whenever x3 = 0. This contradicts the\ninf\ncondition that ≡h(x) must be non-zero ad orthogonal to g1(x), g2 (and hence to g3)\nfor all x.\nProblem T2.3\nAn ODE control system model is given by equations\n⎞\n⎠ x 1(t)\n=\nx2(t)2 + u(t),\nx 2(t)\n=\nx3(t)2 + u(t),\n(1.4)\n⎧ x 3(t)\n= p(x1(t)) + u(t).\n(a) Find all polynomials p : R ∈! R such that system (1.4) is full state\nfeedback linearizable in a neigborhood of x = 0.\n\nSystem (1.4) has the form\nx (t) = f(x(t)) + g(x(t))u(t),\n(1.5)\nwhere\n⎩⎪\n⎣1\n⎪\n⎣\n⎩⎪\n⎣1\n⎪\n⎣\nx1\nx2\nx1\nf ⎦4 x2 ⎤A = 4 x2\n⎤ ,\ng ⎦4 x2 ⎤A = 4 1 ⎤ .\nx3\np(x1)\nx3\nDefine\ng1 = g, g2 = [f, g1],\ng3 = [f, g2],\ng21 = [g2, g1],\ni.e.\n⎩⎪\n⎣1\n⎪\n⎣\n⎩⎪\n⎣1\n⎪\n⎣\n⎩⎪\n⎣1\n⎪\n⎣\nx1\n2x2\nx1\n4x2x3 - 2x3\nx1\n⎦4 x2 ⎤A = 4 2x3 ⎤ , g3 ⎦4\n⎤A = 4 2x3p (x1) - 2p(x1) ⎤ , g21 ⎦4 x2 ⎤A = 4\n⎤ .\ng2\nx2\nx3\np (x1)\nx3\n2x2p (x1) - p (x1)x2\nx3\np (x1)\nFor local full state feedback linearizability at x = 0 it is necessary and sufficient\nfor vectors g1(0), g2(0), g3(0) to be linearly independent (which is equivalent to\np(0) p(0) = 0) and for g21(x) to be a linear combination of g1(x) and g2(x) for\nall x in a neigborhood of x = 0 (which is equivalent to p (x1) ≥ 2). Hence\np(x1) = x1 + p1x1 + p0,\np0p1 = 0\ninf\nis necessary and sufficient for local full state feedback linearizability at x = 0.\n\n(b) For each polynomial p found in (a), design a feedback law\nu(t) = K(x1(t), x2(t), x3(t)) = Kp(x1(t), x2(t), x3(t))\nwhich makes the origin a locally asymptotically stable equilibrium\nof (1.4).\nSince p(0) = 0, x = 0 cannot be made into a locally asymptotically stable equilib-\ninf\nrium of (1.4). However, the origin z = 0 (i.e. with respect to the new coordinates\nz = A(x)) of the feedback linearized system can be made locally asymptotically\nstable, as long as 0 ≤ A() where is the domain of A. Actually, this does not\nrequire any knowledge of the coordinate transform A, and can be done under an\nassumption substantially weaker than full state feedback linearizability!\nLet\nz (t) = Az(t) + Bv(t)\n(1.6)\nbe the feedback linearized equations (1.5), where\nz(t) = A(x(t)),\nx(t) ≤ ,\nv(t) = (r)(x(t))(u - λ(x(t))).\nIn other words, let\n\nf(x) = [A(x)]-1[AA(x) - B(r)(x)λ(x)],\ng(x) = [A(x)]-1B(r)(x).\nIf\nx) = 0 then\nx ≤ satisfies A(\nx is a conditional equilibrium of (1.5), in the sense\nthat\nf(\nx)\nx) + g( u = 0\nfor u = λ(\n\nx). Moreover, since the pair (A, B) is assumed to be controllable, the\nconditional equilibrium has a controllable linearization, in the sense that the pair\n(f (\nx)\nx)) is controllable as well, because\nx) + g( u, g(\nf (\nx)\nx) = S-1B(r)(\nx) + g( u = S-1(AS - BF), g(\nx)\nfor\nx), F = (r)(\nx).\nS = A (\nx)λ (\nIt is easy to see that every conditional equilibrium\nx of (1.5) with a controllable lin\nearization can be made into a locally exponentially stable equilibrium by introducing\nfeedback control\nu(t) =\nx),\nu + K(x(t) -\nwhere K is a constant gain matrix such that\nf (\nx)\nx)K\nx) + g( u + g(\n\nis a Hurwitz matrix. Indeed, by assumption x is an equilibriun of\nx (t) = fK (x) = f (x(t)) + g(x(t))(\nx)),\nu + K(x(t) -\nand\nf K (\nx) + g( u + g(\nx) = f (\nx)\nx)K.\nIn the case of system (1.4) let\n⎪\n⎣\nx1\nx = 4 x2 ⎤\n\nx3\nbe a conditional equilibrium, i.e.\nx1 = x = p(\nu.\nx1) = -\nThen\n⎪\n⎣\n⎪\n⎣\n\nx2\nf (\nx)\nx2 ⎤ ,\ng(\nx) + g( u = 4\nx) = 4 1 ⎤ .\np ( x1)\nHence a locally stabilizing controller is given by\nu(t) = - 1 + k1(x1(t) -\nx2) + k3(x3(t) -\nx\nx1) + k2(x2(t) -\nx3),\nwhere the coefficients k1, k2, k3 are chosen in such a way that\n⎪\n⎣\n⎪\n⎣\nx2\n£\n¤\n2 x2 ⎤ + 4 1 ⎤ k1 k2 k3\np( x1)\nis a Hurwitz matrix.\n(c) Find a C1 function p : R ∈! R for which system (1.4) is globally full\nstate feedback linearizable, or prove that such p(·) does not exist.\nSuch p(·) does not exist. Indeed, otherwise vectors\n⎪\n⎣\n⎪\n⎣\n2x2\n4 1 ⎤ and 4 2x3 ⎤\np (x1)\nare linearly independent for all real\nx2,\nx1,\nx3, which is impossible for\n\np(\nx2 = x3 = 0.5 x1)."
    },
    {
      "category": "Lecture Notes",
      "title": "lec1_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/a571353d3b09548b01295fb171330a05_lec1_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 1: Input/Output and State-Space Models\nThis lecture presents some basic definitions and simple examples on nonlinear dynam\nical systems modeling.\n1.1\nBehavioral Models.\nThe most general (though rarely the most convenient) way to define a system is by using\na behavioral input/output model.\n1.1.1\nWhat is a signal?\nIn these lectures, a signal is a locally integrable function z : R+ ≤! Rk , where R+ denotes\nthe set of all non-negative real numbers. The notion of \"local integrability\" comes from\nthe Lebesque measure theory, and means simply that the function can be safely and\nmeaningfully integrated over finite intervals. Generalized functions, such as the delta\nfunction (t), are not allowed. The argument t → R+ of a signal function will be referred\nto as \"time\" (which it usually is).\nExample 1.1 Function z = z(·) defined by\nt-0.9sgn(cos(1/t)) for t > 0,\nz(t) =\nfor t = 0\n1Version of September 3, 2003\n\nis a valid signal, while\n1/t for t > 0,\nz(t) =\nfor t = 0\nand z(t) = (t) are not.\nThe definition above formally covers the so-called continuous time (CT) signals. Dis\ncrete time (DT) signals can be represented within this framework as special CT signals.\nMore precisely, a signal z : R+ ≤! Rk is called a DT signal if it is constant on every\ninterval [k, k + 1) where k = 0, 1, 2, . . . .\n1.1.2\nWhat is a system?\nSystems are objects producing signals (called output signals), usually depending on other\nsignals (inputs) and some other parameters (initial conditions). In most applications,\nmathematical models of systems are defined (usually implicitly) by behavior sets. For an\nautonomous system (i.e. for a system with no inputs), a behavior set is just a set B = {z}\nconsisting of some signals z : R+ ≤! Rk (k must be the same for all signals from B). For\na system with input v and output w, the behavior set consists of all possible input/output\npairs z = (v(·), w(·)). There is no real difference between the two definitions, since the\npair of signals z = (v(·), w(·)) can be interpreted as a single vector signal z(t) = [v(t); w(t)]\ncontaining both input and output stacked one over the other.\nNote that in this definition a fixed input v(·) may occur in many or in no pairs\n(v, w) → B, which means that the behavior set does not necessarily define system output\nas a function of an arbitrary system input. Typically, in addition to knowing the input,\none has to have some other information (initial conditions and/or uncertain parameters)\nto determine the output in a unique way.\nExample 1.2 The familiar ideal integrator system (the one with the transfer function\nG(s) = 1/s) can be defined by its behavioral set of all input/output scalar signal pairs\n(v, w) satisfying\nt2\nw(t2) - w(t1) =\nv()d, 8 t1, t2 → [0, ∀).\nt1\nIn this example, to determine the output uniquely it is sufficient to know v and w(0).\nIn Example 1.1.2 a system is characterised by an integral equation. There is a variety\nof other ways to define the same system (by specifying a transfer function, by writing a\ndifferential equation, etc.)\n\n1.1.3\nWhat is a linear/nonlinear system?\nA system is called linear if its behavior set satisfies linear superposition laws, i.e. when\nfor every z1, z2 →B and c →R we have z1 + z2 →B and cz1 →B.\nExcluding some absurd examples2, linear systems are those defined by equations which\nare linear with respect to v and w. In particular, the ideal integrator system from Exam\nple 1.1.2 is linear.\nA nonlinear system is simply a system which is not linear.\n1.2\nSystem State.\nIt is important to realize that system state can be defined for an arbitrary behavioral\nmodel B = {z(·}.\n1.2.1\nTwo signals defining same state at time t.\nSystem state at a given time instance t0 is supposed to contain all information relating\npast (t < t0) and future (t > t0) behavior. This leads us to the following definitions.\nDefinition Let B be a behavior set. Signals z1, z2 →B are said to commute at time t0 if\nthe signals\nz1(t) for t ∩t0,\nz12(t) =\nz2(t) for t > t0\nand\nz2(t) for t ∩t0,\nz21(t) =\nz1(t) for t > t0\nalso belong to the behavior set.\nDefinition Let B be a behavior set. Signals z1, z2 →B are said to define same state of\nB at time t0 if the set of z →B commuting with z1 at t0 is the same as the set of z →B\ncommuting with z2 at t0.\nDefinition Let B be a behavior set. Let X be any set. A function x : R × B ≤! X\nis called a state of system B if z1 and z2 define same state of B at time t whenever\nx(t, z1(·)) = x(t, z2(·)).\nExample 1.3 Consider a system in which both input v and output w are binary signals,\ni.e. DT signals taking values from the set {0, 1}. Define the input/output relation by\nthe following rules: w(t) = 1 only if v(t) = 1, and for every t1, t2 → Z+ such that\n2Such as the (linear) system defined by the nonlinear equation (v(t) - w(t))2 = 0 8 t\n\nw(t1) = w(t2) = 1 and w(t) = 0 for all t →(t1, t2) \\ Z, there are exactly two integers t in\nthe interval (t1, t2) such that v(t) = 1.\nIn other words, the system counts the 1's in the input and, every time the count\nreaches three, the system resets its counter to zero, and outputs 1 (otherwise producing\n0's).\nIt is easy to see that two input/output pairs z1 = (v1, w1) and z2 = (v2, w2) commute\nat a (discrete) time t0 if and only if N(t0, z1) = N(t0, z2), where N(t0, z) for z = (v, w) →B\nis the number of 1's in v(t) for t →(t0, t1) \\ Z, where t1 means the next (after t0) integer\ntime t when w(t) = 1. Hence the state of the system can be defined by a function\nx : R+ × B ≤! {0, 1, 2}, x(t, z) = N(t, z).\nIn this example, knowing a system state allows one to write down state space equations\nfor the system:\nx(t + 1) = f(x(t), v(t)), w(t) = g(x(t), v(t)),\n(1.1)\nwhere\nf(x, v) = (x + v)mod3,\nand g(x, v) = 1 if and only if x = 2 and v = 1."
    },
    {
      "category": "Lecture Notes",
      "title": "lec2_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/5a1c1ee61722a192c4479093ffb612d0_lec2_6243_2003.pdf",
      "content": "Z\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 2: Differential Equations As System Models1\nOrdinary differential requations (ODE) are the most frequently used tool for modeling\ncontinuous-time nonlinear dynamical systems. This section presens results on existence\nof solutions for ODE models, which, in a systems context, translate into ways of proving\nwell-posedness of interconnections.\n2.1\nODE models and their solutions\nOrdinary differential equations are used to describe responses of a dynamical system to\nall possible inputs and initial conditions. Equations which do not have a solution for some\nvalid inputs and initial conditions do not define system's behavior completely, and, hence,\nare inappropriate for use in analysis and design. This is the reason a special attention is\npaid in this lecture to the general question of existence of solution of differential equation.\n2.1.1\nODE and their solutions\nAn ordinary differential equation on a subset Z 1⁄2 Rn × R is defined by a function\na : Z ∈! Rn . Let T be a non-empty convex subset of R (i.e. T can be a single point\nset, or an open, closed, or semi-open interval in R). A function x : T ∈! Rn is called a\nsolution of the ODE\nx (t) = a(x(t), t)\n(2.1)\nif (x(t), t) ⊂ Z for all t ⊂ T, and\nt2\nx(t2) - x(t1) =\na(x(t), t)dt 8 t1, t2 ⊂ T.\n(2.2)\nt1\n1Version of September 10, 2003\n\n:\nThe variable t is usually referred to as the \"time\".\nNote the use of an integral form in the formal definition (2.2): it assumes that the\nfunction t ∈! a(x(t), t) is integrable on T, but does not require x = x(t) to be differentiable\nat any particular point, which turns out to be convenient for working with discontinuous\ninput signals, such as steps, rectangular impulses, etc.\nExample 2.1 Let sgn denote the \"sign\" function sgn : R ! {0, -1, 1} defined by\n< 1,\ny > 0,\nsgn(y) =\n0,\ny = 0,\n-1, y < 0.\nThe notation\nx = -sgn(x),\n(2.3)\nwhich can be thought of as representing the action of an on/off negative feedback (or\ndescribing behavior of velocity subject to dry friction), refers to a differential equation\ndefined as above with n = 1, Z = R × R (since sgn(x) is defined for all real x, and no\nrestrictions on x or the time variable are explicitly imposed in (2.3)), and a(x, t) = sgn(x).\nIt can be verified2 that all solutions of (2.3) have the form\nx(t) = max{c - t, 0} or x(t) = min{t - c, 0},\nwhere c is an arbitrary real constant. These solutions are not differentiable at the critical\n\"stopping moment\" t = c.\n2.1.2\nStandard ODE system models\nOrdinary differential equations can be used in many ways for modeling of dynamical\nsystems. The notion of a standard ODE system model describes the most straightforward\nway of doing this.\nDefinition A standard ODE model B = ODE(f, g) of a system with input v = v(t) ⊂\nV 1⁄2 Rm and output w(t) ⊂ W 1⁄2 Rk is defined by a subset X 1⁄2 Rn, two functions\nf : X × V × R+ ∈! Rn and g : X × V × R+ ∈! W, and a subset X0 1⁄2 X, so that the\nbehavior set B of the system consists of all pairs (v, w) of signals such that v(t) ⊂ V for\nall t, and there exist a solution x : R+ ∈! X of the differential equation\nx (t) = f(x(t), v(t), t)\n(2.4)\nsuch that x(0) ⊂ X0 and\nw(t) = g(x(t), v(t), t).\n(2.5)\nA special case of this definition, when the input v is not present, defines an autonomous\nsystem.\n2Do it as an excercise!\n\n2.1.3\nWell-posedness of standard ODE system models\nAs it was mentioned before, not all ODE models are adequate for design and analysis\npurposes. The notion of well-posedness introduces some typical constraints aimed at\ninsuring their applicability.\nDefinition A standard ODE model ODE(f, g) is called well posed if for every signal\nv(t) ⊂ V and for every solution x1 : [0, t1] ∈! X of (2.4) with x1(0) ⊂ X0 there exists a\nsolution x : R+ ∈! X of (2.4) such that x(t) = x1(t) for all t ⊂ [0, t1].\nThe ODE from Example 2.1.1 can be used to define a standard autonomous ODE\nsystem model\nx (t) = -sgn(x(t)),\nw(t) = x(t),\nwhere V = X = X0 = R, f(x, v, t) = -sgn(x) and g(x, v, t) = x. It can be verified that\nthis autonomous system is well-posed. However, introducing an input into the model\ndestroys well-posedness, as shown in the following example.\nExample 2.2 Consider the standard ODE model\nx (t) = -sgn(x(t)) + v(t),\nw(t) = x(t),\n(2.6)\nwhere v(t) is an unconstrained scalar input. Here\nV = X = X0 = R,\nf(x, v, t) = -sgn(x) + v, g(x, v, t) = x.\nWhile this model appears to describe a physically plausible situation (velocity dynamics\nsubject to dry friction and external force input v), the model is not well-posed.\nTo prove this, consider the input v(t) = 0.5 = const. It is sufficient to show that no\nsolution of the ODE\nx (t) = 0.5 - sgn(x(t))\nsatisfying x(0) = 0 exists on a time interval [0, tf ] for tf > 0. Indeed, let x = x(t) be such\nsolution. As an integral of a bounded function, x = x(t) witll be a continuous function\nof time. A continuous function over a compact interval always achieves a maximum. Let\ntm ⊂ [0, tf ] be an argument of the maximum over t ⊂ [0, tf ].\nIf x(tm) > 0 then tm > 0 and, by continuity, x(t) > 0 in a neighborhood of tm, hence\nthere exists π > 0 such that x(t) > 0 for all t ⊂ [tm - π, tm]. According to the differential\nequation, this means that x(tm -π) = x(tm)+0.5π > x(tm), which contradicts the selection\nof tm as an argument of maximum. Hence max x(t) = 0. Similarly, min x(t) = 0. Hence\nx(t) = 0 for all t. But the constant zero function does not satisfy the differentlial equation.\nHence, no solution exists.\nIt can be shown that the absense of solutions in Example 2.1.3 is caused by lack of\ncontinuity of function f = f(x, v, t) with respect to x (discontinuity with respect to v and\nt would not cause as much trouble).\n\n2.2\nExistence of solutions for continuous ODE\nThis section contains fundamental results establishing existence of solutions of differential\nequations with a continuous right side.\n2.2.1\nLocal existence of solutions for continuous ODE\nIn this subsection we study solutions x : [t0, tf ] ∈! Rn of the standard ODE\nx (t) = a(x(t), t)\n(2.7)\n(same as (2.1)), subject to a given initial condition\nx(t0) = x0.\n(2.8)\nHere a : Z ∈! Rn is a given continuous function, defined on Z 1⁄2 Rn × R. It turns out\nthat a solution x = x(t) of (2.7) with initial condition (2.8) exists, at least on a sufficiently\nshort time interval, whenever the point z0 = (x0, t0) lies, in a certain sense, in the interior\nof Z.\nTheorem 2.1 Assume that for some r > 0\nDr (x0, t0) = {(\n\nx, t) ⊂ Rn × R : |x - x0| ∀ r, t ⊂ [t0, t0 + r]}\nis a subset of Z. Let\nx, t)| : (\nM = max{|a(\nx, t) ⊂ Dr (x0, t0)}.\nThen, for\ntf = min{t0 + r/M, t0 + r},\nthere exists a solution x :\n[t0, tf ] ∈! Rn of (2.7) satisfying (2.8). Moreover, any such\nsolution also satisfies |x(t) - x0| ∀ r for all t ⊂ [t0, tf ].\nExample 2.3 The ODE\nx (t) = c0 + c1 cos(t) + x(t)2 ,\nwhere c0, c1 are given constants, belongs to the class of Riccati equations, which play a\nprominent role in the linear system theory. According to Theorem 2.1, for any initial\ncondition x(0) = x0 there exists a solution of the Riccati equation, defined on some time\ninterval [0, tf ] of positive length. This does not mean, however, that the correspond\ning autonomous system model (producing output w(t) = x(t)) is well-posed, since such\nsolutions are not necessarily extendable to the complete time half-line [0, →).\n\n2.2.2\nMaximal solutions\nIf x1 : [t0, t1] ∈! Rn and x2 : [t1, t2] ∈! Rn are both solutions of (2.7), and x1(t1) = x2(t1),\nthen the function x : [t0, t2] ∈! Rn, defined by\n⎩ x1(t),\nt ⊂ [t0, t1],\nx(t) =\nx2(t),\nt ⊂ [t1, t2],\n(i.e. the result of concatenating x1 and x2) is also a solution of (2.7). This means that\nsome solutions of (2.7) can be extended to a larger time interval.\nA solution x : T ∈! Rn of (2.7) is called maximal if there exists no other solution\n\nx : T ∈! Rn for which T is a proper subset of T, and x(t) = x(t) for all t ⊂ T. In\nparticular, well-posedness of standard ODE system models contains the requirement that\nall maximal solutions must be defined on the whole time-line t ⊂ [0, →).\nThe following theorem gives a useful characterization of maximal solutions.\nTheorem 2.2 Let X be an open subset of Rn . Let a : X × R ∈! Rn be a continuous\nfunction. Then all maximal solutions of (2.7) are defined on open intervals and, whenever\n\nsuch solution x : (t0, t1) ∈! X has a finite interval end t = t0 ⊂ R or t = t1 ⊂ R (as\nopposed to t0 = -→ or t1 = →), there exists no sequence tk ⊂ (t0, t1) such that tk\nconverges to t while x(tk) converges to a limit in X.\nIn other words, in the absense of a-priori constraints on the time variable, a solution is\nnot extendable only if x(t) converges to the boundary of the set on which a is defined. In\nthe most typical situation, the domain Z of f in (2.4) is Rn × R+, which means no a-priori\nconstraints on either x or t. In this case, according to Theorem 2.2, a solution x = x(t)\nnot extendable over a finite time interval [0, tf ), tf < →, must satisfy the condition\nlim |x(t)| = →.\nt!tf\nIn Example 2.2.1 with c0 = 1, c1 = 0, one maximal ODE solution is x(t) = tan(t),\ndefined for t ⊂ (-1⁄4/2, 1⁄4/2). It cannot be extended on either side because |x(t)| ! → as\nt ! 1⁄4/2 or t ! -1⁄4/2.\n2.2.3\nDiscontinuous dependence on time\nThe ODE describing systems dynamics are frequently discontinuous with respect to the\ntime variable. Indeed, the standard ODE system model includes\nx (t) = f(x(t), v(t), t),\nwhere v = v(t) is an input, and the ODE becomes discontinuous with respect to t when\never v is a rectangular impulse etc. As long as the time instances at which a(x, t) is\n\nZ\nZ\nZ\ndiscontinuous for a fixed finite set t1 < t2 < · · · < tn, Theorem 2.1 can be applied\nseparately to the time intervals [tk-1, tk ]. However, when the location of discontinuities\ndepends on x, or when they cannot be counted in an increasing order, a stronger result\nis needed. It turns out that the dependence on time needs only be integrable, as long as\ndependence on x is continuous.\nTheorem 2.3 Assume that for some r > 0\n(a) the set\nDr (x0, t0) = {(x, t) ⊂ Rn × R : |x - x0| ∀ r, t ⊂ [t0, t0 + r]}\nis a subset of Z;\n(b) the function t ∈! a(x(t), t) is integrable on [t0, t0 + r] for every continuous function\nx : [t0, t0 + r] ∈! Rn satisfying |x(t) - x0| ∀ r for all t ⊂ [t0, t0 + r];\n(c) for every π > 0 there exists ± > 0 such that\nt0+r\n|a(x1(t), t) - a(x2(t), t)|dt < π\nt0\nwhenever x1, x2 : [t0, t0 +r] ∈! Rn are continuous functions satisfying |xk (t)-x0| ∀ r\nand |x1(t) - x2(t)| < ± for all t ⊂ [t0, t0 + r].\nThen, for some tf ⊂ (t0, t0 + r) there exists a solution x : [t0, tf ] ∈! Rn of (2.7) satisfying\n(2.8).\nExample 2.4 Theorem 2.3 can be used to show that the differential equation\n⎩\nt-1/3x(t), t > 0\nx (t) =\nx(0) = x0\n0,\nt = 0,\ndoes have a solution on [0, →) for every x0 ⊂ R (in this particular case the solutions can\nbe found analytically). Indeed, for every continuous function x : [0, →) ∈! R the function\nt ∈! t-1/3x(t) for t > 0 is integrable over every finite interval, and the inequality\nt1\nt1\n|t-1/3 x1(t) - t-1/3 x2(t)|dt ∀\nt-1/3dt max |x1(t) - x2(t)|\nt2[0,t1]\nholds.\nOn the contrary, the differential equation\n⎩ t-1x(t), t > 0\nx (t) =\nx(0) = x0\n0,\nt = 0,\ndoes not have a solution on [0, →) for every x0 inf= 0. Indeed, if x : [0, t1] ∈! R is a solution\nfor some t1 > 0 then\n⎧\n⎨\nd\nx(t)\n= 0\ndt\nt\nfor all t inf= 0. Hence x(t) = ct for some constant c, and x(0) = 0.\n\n2.2.4\nDifferential inclusions\nLet X be a subset of Rn, and let : X ! 2R\nn\nbe a function which maps every point of\nX to a subset of Rn . Such a function defines a differential inclusion\nx (t) ⊂ (x(t)).\n(2.9)\nBy a solution of (2.1) on a convex subset T of R we mean a function x : T ∈! X such\nthat\nZ t2\nx(t2) - x(t1) =\nu(t)dt 8 t1, t2 ⊂ T\nt1\nfor some integrable function u : T ∈! Rn satisfying the inclusion u(t) ⊂ (x(t)) for\nall t ⊂ T. It turns out that differential inclusions are a convenient, though not always\nadequate, way of re-defining discontinuous ODE to guarantee existence of solutions.\nIt turns out that differential inclusion (2.9) subject to fixed initial condition x(t0) = x0\nhas a solution on a sufficiently small interval T = [t0, t1] whenever the set-valued function\nis compact convex set-valued and semicontinuous with respect to its argument (plus, as\nusually, x0 must be an interior point of X).\nTheorem 2.4 Assume that for some r > 0\n(a) the set\nBr (x0) = {x ⊂ Rn : |x - x0| ∀ r}\nis a subset of X;\nx ⊂ Br (x0) the set (\n(b) for every\nx) is convex;\n(c) for every sequence of\n\nxk ⊂ Br (x0) converging to a limit x ⊂ Br (x0) and for every\nsequence uk ⊂ (\n\nxk ) there exists a subsequence k = k(q) ! → as q ! → such that\nthe subsequence\nx).\nuk(q) has a limit in (\nThen the supremum\nM = sup{|\n\nx), x ⊂ Dr (x0, t0)}\nu| : u ⊂ (\n\nis finite, and, for\ntf = min{t0 + r/M, t0 + r},\nthere exists a solution x :\n[t0, tf ] ∈! Rn of (2.9) satisfying x(t0) = x0. Moreover, any\nsuch solution also satisfies |x(t) - x0| ∀ r for all t ⊂ [t0, tf ].\nThe discontinuous differential equation\nx (t) = -sgn(x(t)) + c,\n\n:\nwhere c is a fixed constant, can be re-defined as a continuous differential inclusion (2.9)\nby introducing\n< {c - 1},\ny > 0,\n(y) =\n[c - 1, c + 1],\ny = 0,\n{c + 1},\ny < 0.\nThe newly obtained differential inclusion has the \"existence of solutions\" property, and\nappears to be compatible with the \"dry friction\" interpretation of the sign nonlinearity.\nIn particular, with the initial condition x(0) = 0, the equation has solutions for every\nvalue of c ⊂ R. If c ⊂ [-1, 1], the unique maximal solution is x(t) ≤ 0, which corresponds\nto the friction force \"adapting\" itself to equalize the external force, as long as it is not\ntoo large.\nThe differential inclusion model is not as compatible with the \"on/off controller\"\ninterpretation of the sign nonlinearity. In this case, due to the unmodeled feedback\nloop delays, one expects some \"chattering\" solutions oscillating rapidly around the point\nx0 = 0. It is possible to say that, in this particular case, the solutions of (2.9) describe\nthe limit behavior of the closed loop solutions as the loop delay approaches zero."
    },
    {
      "category": "Lecture Notes",
      "title": "lec3_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/41d1588bb21baf8002a8b9b56ed1047c_lec3_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 3: Continuous Dependence On Parameters1\nArguments based on continuity of functions are common in dynamical system analysis.\nThey rarely apply to quantitative statements, instead being used mostly for proofs of\nexistence of certain objects (equilibria, open or closed invariant set, etc.) Alternatively,\ncontinuity arguments can be used to show that certain qualitative conditions cannot be\nsatisfied for a class of systems.\n3.1\nUniqueness Of Solutions\nIn this section our main objective is to establish sufficient conditions under which solutions\nof ODE with given initial conditions are unique.\n3.1.1\nA counterexample\nContinuity of the function a : Rn ∈! Rn on the right side of ODE\nx (t) = a(x(t)),\nx(t0) = x0\n(3.1)\ndoes not guarantee uniqueness of solutions.\nExample 3.1 The ODE\nx (t) = 3|x(t)|2/3 ,\nx(0) = 0\nhas solutions x(t) ≥ 0 and x(t) ≥ t3 (actually, there are infinitely many solutions in this\ncase).\n1Version of September 12, 2003\n\n3.1.2\nA general uniqueness theorem\nThe key issue for uniqueness of solutions turns out to be the maximal slope of a = a(x):\nto guarantee uniqueness on time interval T = [t0, tf ], it is sufficient to require existence\nof a constant M such that\n|a(\nx2)| ∃ M|x1 -\nx1) - a(\n\nx2|\nx1,\nfor all\nx2 from a neigborhood of a solution x : [t0, tf ] ∈! Rn of (3.1). The proof of both\nexistence and uniqueness is so simple in this case that we will formulate the statement\nfor a much more general class of integral equations.\nTheorem 3.1 Let X be a subset of Rn containing a ball\nx0) = {\nx -\nBr (\nx ≤ Rn : |\nx0| ∃ r}\nof radius r > 0, and let t1 > t0 be real numbers. Assume that function a : X × [t0, t1] ×\n[t0, t1] ∈! Rn is such that there exist constants M, K satisfying\n|a( x1, , t) - a( x2, , t)| ∃ K| x1 - x2| 8 x1, x2 ≤ Br ( x0), t0 ∃ ∃ t ∃ t1,\n(3.2)\nand\n|a( x, , t)| ∃ M 8 x ≤ Br ( x0), t0 ∃ ∃ t ∃ t1.\n(3.3)\nThen, for a sufficiently small tf > t0, there exists unique function x : [t0, tf ] ∈! X\nsatisfying\nt\nx(t) = x0 +\na(x(), , t)d 8 t ≤ [t0, tf ].\n(3.4)\nt0\nA proof of the theorem is given in the next section. When a does not depend on the\nthird argument, we have the standard ODE case\nx (t) = a(x(t), t).\nIn general, Theorem 3.1 covers a variety of nonlinear systems with an infinite dimensional\nstate space, such as feedback interconnections of convolution operators and memoryless\nnonlinear transformations. For example, to prove well-posedness of a feedback system in\nwhich the forward loop is an LTI system with input v, output w, and transfer function\ne-s - 1\nG(s) =\n,\ns\nand the feedback loop is defined by v(t) = sin(w(t)), one can apply Theorem 3.1 with\nsin( x) + h(t),\nt - 1 ∃ ∃ t,\na( x, , t) =\nh(t),\notherwise,\nwhere h = h(t) is a given continuous function depending on the initial conditions.\n\n3.1.3\nProof of Theorem 3.1.\nFirst prove existence. Choose tf > t1 such that tf - t0 ∃ r/M and tf - t0 ∃ 1/(2K).\nDefine functions xk : [t0, tf ] ∈! X by\nt\nx0,\nxk+1(t) =\nx0(t) ≥\nx0 +\na(xk (), , t)d.\nt0\nBy (3.3) and by tf - t0 ∃ r/M we have xk (t) ≤ Br ( x0) for all t ≤ [t0, tf ]. Hence by (3.2)\nand by tf - t0 ∃ 1/(2K) we have\nt\n|xk+1(t) - xk (t)| ∃\n|a(xk (), , t) - a(xk-1(), , t)|d\nt0\nt\n∃\nK|xk () - xk-1()|d\nt0\n∃ 0.5 max {|xk (t) - xk-1(t)|}.\nt2[t0,tf ]\nTherefore one can conclude that\nmax {|xk+1(t) - xk (t)|} ∃ 0.5 max {|xk (t) - xk-1(t)|}.\nt2[t0,tf ]\nt2[t0,tf ]\nHence xk (t) converges exponentially to a limit x(t) which, due to continuity of a with\nrespoect to the first argument, is the desired solution of (3.4).\nNow let us prove uniqueness. Note that, due to tf - t0 ∃ r/M, all solutions of (3.4)\nmust satisfy x(t) ≤ Dr ( x0) for t ≤ [t0, tf ]. If xa and xb are two such solutions then\nt\n|xa(t) - xb(t)| ∃\n|a(xa(), , t) - a(xb(), , t)|d\nt0\nt\n∃\nK|xa() - xb()|d\nt0\n∃ 0.5 max {|xa(t) - xb(t)|},\nt2[t0,tf ]\nwhich immediately implies\nmax {|xa(t) - xb(t)|} = 0.\nt2[t0 ,tf ]\nThe proof is complete now. Note that the same proof applies when (3.2),(3.3) are\nreplaced by the weaker conditions\nx1, , t) - a(\nx1 -\nx1,\nx0), t0 ∃ ∃ t ∃ t1,\n|a(\nx2, , t)| ∃ K()|\nx2| 8\nx2 ≤ Br (\nand\nx, , t)| ∃ m(t) 8\nx0), t0 ∃ ∃ t ∃ t1,\n|a(\nx ≤ Br (\nwhere the functions K(·) and M(·) are integrable over [t0, t1].\n\n3.2\nContinuous Dependence On Parameters\nIn this section our main objective is to establish sufficient conditions under which solutions\nof ODE depend continuously on initial conditions and other parameters.\nConsider the parameterized integral equation\nt\nx(t, q) = x0(q) +\na(x(, q), , t, q)d, t ≤[t0, t1],\n(3.5)\nt0\nwhere q ≤ R is a parameter. For every fixed value of q integral equation (3.5) has the\nform of (3.4).\nTheorem 3.2 Let x0 : [t0, tf ] ∈! Rn be a solution of (3.5) with q = q0. For some d > 0\nlet\nXd = {\n\nx ≤Rn : 9 t ≤[t0, tf ] : |x -x 0(t)| < d}\nbe the d-neigborhood of the solution. Assume that\n(a) there exists K ≤R such that\nx1, , t, q)-a(\n\nx1,\n|a(\nx2, , t, q)| ∃K|x1-x2| 8\nx2 ≤Xd, t0 ∃ ∃t ∃tf , q ≤(q0-d, q0+d);\n(3.6)\n(b) there exists K ≤R such that\nx, , t, q)| ∃M 8\n|a(\nx ≤Xd, t0 ∃ ∃t ∃tf , q ≤(q0 -d, q0 + d);\n(3.7)\n(c) for every > 0 there exists > 0 such that\n|x0(q1) -\n\nx0(q2)| ∃ 8 q1, q2 ≤(q0 -d, q0 + d) : |q1 -q2| < ,\n|a(\nx, , t, q2)| ∃ 8 q1, q2 ≤(q0 -d, q0 + d) : |q1 -q2| < ,\nx, , t, q1) -a(\nx ≤Xd .\nThen there exists d1 ≤(0, d) such that the solution x(t, q) of (3.5) is continuous on\n{(t, q)} = [t0, tf ] × (q0 -d1, q0 + d1).\nCondition (a) of Theorem 3.2 is the familiar Lipschitz continuity requirement of the\ndependence of a = a(x, , t, q) on x in a neigborhood of the trajectory of x0 . Condition\n(b) simply bounds a uniformly. Finally, condition (c) means continuous dependence of\nequations and initial conditions on parameter q.\nThe proof of Theorem 3.2 is similar to that of Theorem 3.1.\n\n3.3\nImplications of continuous dependence on parameters\nThis section contains some examples showing how the general continuous dependence\nof solutions on parameters allows one to derive qualitative statements about nonlinear\nsystems.\n3.3.1\nDifferential flow\nConsider a time-invariant autonomous ODE\nx (t) = a(x(t)),\n(3.8)\nwhere a : Rn ∈! Rm is satisfies the Lipschitz constraint\n|a(\nx2)| ∃ M|x1 -\nx1) - a(\n\nx2|\n(3.9)\non every bounded subset of Rn . According to Theorem 3.1, this implies existence and\nuniqueness of a maximal solution x :\n(t-, t+) ∈! Rn of (3.8) subject to given initial\nconditions x(t0) = x0 (by this definition, t- < t0 < t+, and it is possible that t- = -⊂\nand/or t+ = ⊂). To specify the dependence of this solution on the initial conditions,\nwe will write x(t) = x(t, t0, x0). Due to the time-invariance of (3.8), this notation can\nbe further simplified to x(t) = x(t - t0,\nx) means \"the value x(t) of the\nx0), where x(t,\nsolution of (3.8) with initial conditions x(0) = x\". Remember that this definition makes\nsense only when uniqueness of solutions is guaranteed, and that x(t, x) may by undefined\nwhen |t| is large, in which case we will write x(t, x) = ⊂.\nAccording to Theorem 3.2, x :\n∈! Rn is a continuous function defined on an open\n\nx) defines a family of\nsubset\n∀ R × Rn . With x considered a parameter, t ∈! x(t,\nsmooth curves in Rn . When t is fixed,\nx) defines a continuous map form an open\nx ∈! x(t,\nsubset of Rn and with values in Rn . Note that x(t1, x(t2,\nx) whenever\nx)) = x(t1 + t2,\nx) inf\nx(t2, = ⊂. The function x :\n∈! Rn is sometimes called \"differential flow\" defined\nby (3.8).\n3.3.2\nAttractors of asymptotically stable equilibria\nx0 ≤ Rn is called an equilibrium of (3.8) when a(\nx0) ≥\nA poiint\nx0) = 0, i.e. x(t,\nx0 is a\nconstant solution of (3.8).\nDefinition An equilibrium x0 of (3.8) is called asymptotically stable if the following two\nconditions are satisfied:\n(a) there exists d > 0 such that x(t,\nx0 as t ! ⊂ for all\nx0 -\nx) !\nx satisfying |\nx| < d;\nx) -\n(b) for every > 0 there exists > 0 such that |x(t,\nx0| < whenever t → 0 and\n|\nx0| < .\nx -\n\nIn other words, all solutions starting sufficiently close to an asymptotically stable\nequilibrium x0 converge to it as t ! ⊂, and none of such solutions can escape far away\nbefore finally converging to x0.\nTheorem 3.3 Let x0 ≤ Rn be an asymptotically stable equilibrium of (3.8). The set\nx0) of all\nx) !\nA = A(\nx ≤ Rn such that x(t,\nx0 as t ! ⊂ is an open subset of Rn, and\nits boundary is invariant under the transformations\nx).\nx ∈! x(t,\nThe proof of the theorem follows easily from the continuity of x(·, ·).\n3.3.3\nLimit points of a trajectory\nx0 ≤ Rn , the set of all possible limits x(tk ,\nas k ! ⊂, where\nFor a fixed\nx0) ! x\nthe sequence {tk } also converges to infinity, is called the limit set of the \"trajectory\"\nt ∈! x(t, x0).\nTheorem 3.4 The limit set of a given trajectory is always closed and invariant under\nthe transformations\nx).\nx ∈! x(t,"
    },
    {
      "category": "Lecture Notes",
      "title": "lec4_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/211daf3eed37a6add2fd19dcdb7bd015_lec4_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 4: Analysis Based On Continuity 1\nThis lecture presents several techniques of qualitative systems analysis based on what is\nfrequently called topological arguments, i.e. on the arguments relying on continuity of\nfunctions involved.\n4.1\nAnalysis using general topology arguments\nThis section covers results which do not rely specifically on the shape of the state space,\nand thus remain valid for very general classes of systems. We will start by proving gener\nalizations of theorems from the previous lecture to the case of discrete-time autonomous\nsystems.\n4.1.1\nAttractor of an asymptotically stable equilibrium\nConsider an autonomous time invariant discrete time system governed by equation\nx(t + 1) = f(x(t)),\nx(t) ⊂ X, t = 0, 1, 2, . . . ,\n(4.1)\nwhere X is a given subset of Rn , f : X inf! X is a given function. Remember that f\nis called continuous if f(xk ) ! f(x1) as k ! → whenever xk , x1 ⊂ X are such that\nxk ! x1 as k ! →). In particular, this means that every function defined on a finite set\nX is continuous.\nOne important source of discrete time models is discretization of differential equations.\nRn\nAssume that function a :\ninf! Rn is such that solutions of the ODE\nx (t) = a(x(t)),\n(4.2)\n1Version of September 17, 2003\n\nwith x(0) = x exist and are unique on the time interval t ⊂[0, 1] for all\n\nx ⊂Rn . Then\ndiscrete time system (4.1) with f( ) = x(1, ) describes the evolution of continuous time\nx\nx\nsystem (4.2) at discrete time samples. In particular, if a is continuous then so is f.\nLet us call a point in the closure of X locally attractive for system (4.1) if there exists\nd > 0 such that x(t) !\n\nx0 as t ! →for every x = x(t) satisfying (4.1) with |x(0)-x0| < d.\nNote that locally attractive points are not necessarily equilibria, and, even if they are,\nthey are not necessarily asymptotically stable equilibria.\nx0 ⊂Rn the set A = A(\nx ⊂X in (4.1) which define a\nFor\nx0) of all initial conditions\nsolution x(t) converging to\nx\nx0 as t ! →is called the attractor of 0.\nTheorem 4.1 If f is continuous and x0 is locally attractive for (4.1) then the attractor\nA = A( x0) is a (relatively) open subset of X, and its boundary d(A) (in X) is f-invariant,\ni.e. f( ) ⊂d(A) whenever\nx\nx ⊂d(A).\nRemember that a subset Y 1⁄2 X 1⁄2 Rn is called relatively open in X if for every y ⊂Y\nthere exists r > 0 such that all x ⊂X satisfying |x -y| < r belong to Y . A boundary of a\nsubset Y 1⁄2 X 1⁄2 Rn in X is he set of all x ⊂X such that for every r > 0 there exist y ⊂Y\nand z ⊂X/Y such that |y -x| < r and z -x| < r. For example, the half-open interval\nY = (0, 1] is a relatively closed subset of X = (0, 2), and its boundary in X consists of a\nsingle point x = 1.\nExample 4.1 Assume system (4.1), defined on X = Rn by a continuous function\nf : Rn inf! Rn, is such that all solutions with |x(0)| < 1 converge to zero as t ! →,\nand all solutions with |x(0)| > 100 converge to infinity as t ! →. Then, according to\nTheorem 4.1, the boundary of the attractor A = A(0) is a non-empty f-invariant set. By\n\nassumptions, 1 ∀ |x| ∀ 100 for all x ⊂ A(0). Hence we can conclude that there exist\nsolutions of (4.1) which satisfy the constraints 1 ∀|x(t)| ∀100 for all t.\nExample 4.2 For system (4.1), defined on X = Rn by a continuous function f : Rn inf!\nRn, it is possible to have every trajectory to converge to one of two equilibria. However,\nit is not possible for both equilibria to be locally attractive. Otherwise, according to The\norem 4.1, Rn would be represented as a union of two disjoint open sets, which contradicts\nthe notion of connectedness of Rn .\n4.1.2\nProof of Theorem 4.1\nAccording to the definition of local attractiveness, there exists d > 0 such that x(t) ! x0\nas t ! → for every x = x(t) satisfying (4.1) with |x(0) - x0| < d. Take an arbitrary\nx1 ⊂A(\nx1. Then x1(t) ! 0 as\n\nx0). Let x1 = x1(t) be the solution of (4.1) with x(0) =\nx\nt ! →, and hence |x1(t1)| < d/2 for a sufficiently large t1. Since f is continuous, x(t) is a\ncontinuous function of x(0) for every fixed t ⊂{0, 1, 2, . . . }. Hence there exists ± > 0 such\nx\nx\nthat |x(t1) -x1(t1)| < d/2 whenever |x(0) - 1| < ±. Since this implies |x(t1) - 0| < d,\n\nwe have\nx\nx ⊂X such that |x - 1| < ±, which proves that A = A( 0)\nx ⊂A( 0) for every\n\nx\nx\nis open.\nTo show that d(A) is f-invariant, note first that A is itself f-invariant. Now take an\narbitrary x ⊂ d(A). By the definition of the boundary, there exists a sequence\n\nxk ⊂ A\nx\nxk ) converges to f(\nconverging to . Hence, by the continuity of f, the sequence f(\nx). If\nf( ) ≤⊂A, this implies f(\nx\nx) ⊂d(A). Let us show that the opposite is impossible. Indeed,\nif f( ) ⊂ A then, since A is proven open, there exists 2 > 0 such that z ⊂ A for every\nx\nz ⊂ X such that |z -f( )| < 2. Since f is continuous, there exists ± > 0 such that\nx\n|f(y) -f(\nx\nx)| < 2 whenever y ⊂ X is such that |y - | < ±. Hence f(y) ⊂ A whenever\nx\n|y - | < ±. Since, by the definition of attractor, f(y) ⊂A imlies y ⊂A, y ⊂A whenever\nx| < ±, which contradicts the assumption that\n|y -\nx ⊂d(A).\n4.1.3\nLimit points of planar trajectories\nFor a given solution x = x(t) of (4.2), the set lim(x) 1⁄2 Rn of all possible limits x(tk ) ! x\nas k ! →, where {tk } converges to infinity, is called the limit set of x.\nTheorem 4.2 Assume that a : Rn inf! Rn is a locally Lipschitz function. If x : [0, →) inf!\nRn is a solution of (4.2) then the set lim(x) of its limit points is a closed subset of Rn ,\nand every solution of (4.2) with initial conditions in lim(x) lies completely in lim(x).\nx\nxq !\nas\nProof First, if tk,q ! →and x(tk,q , x(0)) ! q as k ! →for every q, and\nx1\nq ! → then one can select q = q(k) such that tk,q(k) ! → and x(tk,q(k), x(0) !\nas\nx1\nk ! →. This proves the closedness (continuity of solutions was not used yet).\nSecond, by assumption\nx0 = lim x(tk , x(0)).\nk!1\nHence, by the continuous dependence of solutions on initial conditions,\nx(t, x0) = lim x(t, x(tk , x(0))) = lim x(t + tk , x(0)).\nk!1\nk!1\nIn general, limit sets of ODE solutions can be very complicated. However, in the case\nwhen n = 2, a relatively simple classification exists.\nTheorem 4.3 Assume that a : R2 inf! R2 is a locally Lipschitz function. Let x0 :\n[0, →) inf! R2 be a solution of (4.2). Then one of the following is true:\n(a) |x0(t)| ! →as t ! →;\n(b) there exists T > 0 and a non-constant solution xp :\n(-→, +→) inf! R2 such that\nxp(t + T) = xp(t) for all t, and the set of limit points of x is the trajectory (the\nrange) of xp;\n\nZ\n(c) the limit set is a union of trajectories of maximal solutions x :\n(t1, t2) inf! R2 of\n(4.2), each of which has a limit (possibly infinite) as t ! t1 or t ! t2.\nThe proof of Theorem 4.3 is based on the more specific topological arguments, to be\ndiscussed in the next section.\n4.2\nMap index in system analysis\nThe notion of index of a continuous function is a remarkably powerful tool for proving\nexistence of mathematical objects with certain properties, and, as such, is very useful in\nqualitative system analysis.\n4.2.1\nDefinition and fundamental properties of index\nFor n = 1, 2, . . . let\nSn = {x ⊂ Rn+1 : |x| = 1}\ndenote the unit sphere in Rn+1 . Note the use of n, not n + 1, in the S-notation: it\nindicates that locally the sphere in Rn+1 looks like Rn . There exists a way to define the\nindex ind(F) of every continuous map F : Sn inf! Sn in such a way that the following\nconditions will be satisfied:\n(a) if H : Sn × [0, 1] inf! Sn is continuous then\nind(H(·, 0)) = ind(H(·, 1))\n(such maps H is called a homotopy between H(·, 0) and H(·, 1));\n(b) if the map Fˆ : Rn+1 inf! Rn+1 defined by\nFˆ(z) = |z|F(z/|z|)\nis continuously differentiable in a neigborhood of Sn then\nind(F) =\ndet(Jx(Fˆ))dm(x),\nx2Sn\nwhere Jx(Fˆ) is the Jacobian of Fˆ at x, and m(x) is the normalized Lebesque measure\non Sn (i.e. m is invariant with respect to unitary coordinate transformations, and\nthe total measure of Sn equals 1).\nOnce it is proven that the integral in (b) is always an integer (uses standard vol-\nume/surface integration relations), it is easy to see that conditions (a),(b) define ind(F)\ncorrectly and uniquelly. For n = 1, the index of a continuous map F : S1 inf! S1 turns\nout to be simply the winding number of F, i.e. the number of rotations around zero the\ntrajectory of F makes.\nIt is also easy to see that ind(FI ) = 1 for the identity map FI (x) = x,and ind(Fc) = 0\nfor every constant map Fc(x) = x0 = const.\n\n4.2.2\nThe Brower's fixed point theorem\nOne of the classical mathematical results that follow from the very existence of the index\nfunction is the famous Brower's fixed point theorem, which states that for every continuous\nfunction G : Bn inf! Bn, where\nBn = {x ⊂ Rn+1 : |x| ∀ 1},\nequation F(x) = x has at least one solution.\nThe statement is obvious (though still very useful) when n = 1. Let us prove it for\nˆ\nn > 1, starting with assume the contrary. Then the map G : Bn inf! Bn which maps\nx ⊂ Bn to the point of Sn-1 which is the (unique) intersection of the open ray starting\nfrom G(x) and passing through x with Sn-1 . Then H : Sn-1 × [0, 1] inf! Sn-1 defined by\nH(x, t) = ˆG(tx)\nis a homotopy between the identity map H(·, 1) and the constant map H(·, 0). Due to\nexistence of the index function, such a homotopy does not exist, which proves the theorem.\n4.2.3\nExistence of periodic solutions\nLet a : Rn × R inf! Rn be locally Lipschitz and T-periodic with respect to the second\nargument, i.e.\nx, t + T) = a(\na(\nx, t) 8 x, t\nwhere T > 0 is a given number. Assume that solutions of the ODE\nx (t) = a(x(t), t)\n(4.3)\nwith initial conditions x(0) ⊂ Bn remain in Bn for all times. Then (4.3) has a T-periodic\nsolution x = x(t) = x(t + T) for all t ⊂ R.\nx inf! x(T, 0,\nIndeed, the map\nx) is a continuous function G : Bn inf! Bn . The solution\nx = G(\nof\nx) defines the initial conditions for the periodic trajectory."
    },
    {
      "category": "Lecture Notes",
      "title": "lec5_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/b71c001af1b43a6fa8c096049073e73e_lec5_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 5: Lyapunov Functions and Storage Functions\nThis lecture gives an introduction into system analysis using Lyapunov functions and\ntheir generalizations.\n5.1\nRecognizing Lyapunov functions\nThere exists a number of slightly different ways of defining what constitutes a Lyapunov\nfunction for a given system. Depending on the strength of the assumptions, a variety of\nconclusions about a system's behavior can be drawn.\n5.1.1\nAbstract Lyapunov and storage functions\nIn general, Lyapunov functions are real-valued functions of system's state which are mono\ntonically non-increasing on every signal from the system's behavior set. More gener\nally, stotage functions are real-valued functions of system's state for which explicit upper\nbounds of increments are available.\nLet B = {z} be a behavior set of a system (i.e. elements of B are are vector sig\nnals, which represent all possible outputs for autonomous systems, and all possible in-\nput/output pairs for systems with an input). Remember that by a state of a system\nwe mean a function x : B × [0, ⊂) ≡! X such that two signals z1, z2 ≤ B define same\nstate of B at time t whenever x(z1(·), t) = x(z2(·), t) (see Lecture 1 notes for details and\nexamples). Here X is a set which can be called the state space of B. Note that, given the\nbehavior set B, state space X is not uniquelly defined.\n1Version of September 19, 2003\n\nZ\nDefinition A real-valued function V : X ≡! R defined on state space X of a system\nwith behavior set B and state x : B × [0, ⊂) ≡! X is called a Lyapunov function if\nt ≡! V (t) = V (x(t)) = V (x(z(·), t)) is a non-increasing function of time for every z ≤ B.\nAccording to this definition, Lyapunov functions provide limited but very explicit\ninformation about system behavior. For example, if X = Rn and V (x(t)) = |x(t)|2 is a\nLyapunov function then we now that system state x(t) remains bounded for all times,\nthough we may have no idea of what the exact value of x(t) is.\nFor conservative systems in physics, the total energy is always a Lyapunov function.\nEven for non-conservative systems, it is frequently important to look for energy-like ex\npressions as Lyapunov function candidates.\nOne can say that Lyapunov functions have an explicit upper bound (zero) imposed on\ntheir increments along system trajectories:\nV (x(z(·), t1)) - V (x(z(·), t0)) ∀ 0 8 t1 → t0 → 0, z ≤ B.\nA useful generalization of this is given by storage functions.\nDefinition Let B be a set of n-dimensional vector signals z : [0, ⊂) ≡! Rn . Let\n3⁄4 : Rn ≡! R be a given function such that 3⁄4(z(t)) is locally integrable for all z(·) ≤ B. A\nreal-valued function V : X ≡! R defined on state space X of a system with behavior set\nB and state x : B × [0, ⊂) ≡! X is called a storage function with supply rate 3⁄4 if\nt1\nV (x(z(·), t1)) - V (x(z(·), t0)) ∀\n3⁄4(z(t))dt 8 t1 → t0 → 0, z ≤ B.\n(5.1)\nt0\nIn many applications 3⁄4 is a function comparing the instantaneous values of input and\noutput. For example, if B = {z(t) = [v(t); w(t)]} is the set of all possible input/output\npairs of a given system, existence of a non-negative storage function with supply rate\n3⁄4(z(t)) = |v(t)|2 -|w(t)|2 proves that power of the output, defined as\n1 Z t\n*w(·)*p = lim sup\n|w(¿)|2d¿,\nT !1 t·T t\nnever exceed power of the input.\nExample 5.1 Let behavior set B = {(i(t), v(t))} descrive the (dynamcal) voltage-current\nrelation of a passive single port electronic circuit. Then the total energy E = E(t)\naccumulated in the circuit can serve as a storage function with supply rate\n3⁄4(i(t), v(t)) = i(t)v(t).\n\n5.1.2\nLyapunov functions for ODE models\nIt is important to have tools for verifying that a given function of a system's state is\nmonotonically non-increasing along system trajectories, without explicitly calculating so\nlutions of system equations. For systems defined by ODE models, this can usually be\ndone.\nConsider an autonomous system defined by ODE model\nx (t) = a(x(t)),\n(5.2)\nwhere a : X ≡! Rn is a function defined on a subset of Rn . A functional V : X ≡! R is\na Lyapunov function for system (5.2) if t ≡! V (x(t)) is monotonically non-increasing for\nevery solution of (5.2). Remember that x : [t0, t1] ! X is called a solution of (5.2) if the\ncomposition a inf x is absolutely integrable on [t0, t1] and equality\nZ t\nx(t) = x(t0) +\na(x(¿))d¿\nt0\nholds for all t ≤ [t0, t1].\nTo check that a given function V is a Lyapunov function for system (5.2), one usually\nattempts to differentiate V (x(t)) with respect to t. If X is an open set, and both V and x\nare differentiable (note that the differentiability of x is assured by the continuity of a), the\ncomposition t ≡! V (x(t)) is also differentiable, and the monotonicity condition is given by\n∈V (\nx) ∀ 0 8\nx)a(\nx ≤ X,\n(5.3)\nwhere ∈V (x) denotes the gradient of V at x.\nIn some applications one may be forced to work with systems that have non-differentiable\nsolutions (for example, because of a jump in an external input signal). The convenient\nLyapunov function candidates V may also be non-differentiable at some points. In such\nsituations, it is tempting to consider, for every\nx ≤ X in\nx ≤ X, the subgradient of V at\nthe direction a( x). One may expect that non-positivity of such subgradients, which can\nbe expressed as\nV (\nx)) - V (\nx + ta(\nx)\nlim\nsup\n∀ 0 8 x ≤ X,\n(5.4)\n2!0,2>0 0<t<2\nt\nimplies that V is a valid Lyapunov function. However, this is not always true.\nExample 5.2 Using the famous example of a Kantor function, one can construct a\nbounded integrable function a : R ≡! R and a continuous function V : R ≡! R such\nx + ta(\n\nthat t ≡! V (\nx)) is constant in a neigborhood of t = 0 for every x ≤ R, but the\nODE (5.2) has a solution for which V (x(t)) is strictly monotonically increasing!\nHere by a Kantor function we mean a continuous strictly monotonic function k :\n[0, 1] ≡! R such that k(0) = 0 and k(1) = 1 despite the fact that k(t) is constant on a\n\n±\n\n±\n±\n±\n±\n\n±\n\nfamily T = {T} of open disjoint intervals T 1⁄2 [0, 1] of total length 1. Indeed, for a fixed\nKantor function k define\nV (\nx) + k(1 - floor(\nx) = floor(\nx)),\nx) denotes the largest integer not larger than x\nx) be zero on every\nwhere floor(\n. Let a(\ninterval (m + t1, m + t2), where m is an integer and (t1, t2) ≤ T , and a( x) = 0 otherwise.\nThen x(t) ≥ t is a solution of ODE (5.2), but V (x(t)) is strictly monotonically increasing,\nx + ta(\ndespite the fact that t ≡! V (\nx)) is constant in a neigborhood of t = 0 for every\nx ≤ R.\nHowever, if V and all solutions of (5.2) are \"smooth enough\", condition (5.4) is suffi\ncient for V to be a Lyapunov function.\nTheorem 5.1 If X is an open set in Rn , V : X ≡! R is locally Lipschitz, a : X ≡! Rn is\ncontinuous, and condition (5.4) is satisfied then V (x(t)) is monotonically non-increasing\nfor all solutions x : [t0, t1] ≡! X of (5.2).\nProof We will use the following statement: if h : [t0, t1] ≡! R is continuous and satisfies\nh(t + ±) - h(t)\nlim\nsup\n∀ 0 8 t ≤ [t0, t1),\n(5.5)\nd!0,d>0 ±→(0,d)\n±\nthen h is monotonically non-increasing. Indeed, for every r > 0 let hr (t) = h(t) - rt.\nIf hr is monotonically non-increasing for all r > 0 then so is h. Otherwise, assume that\nhr (t3) > hr (t2) for some t0 ∀ t2 < t3 ∀ t1 and r > 0. Let t4 be the maximal solution of\nequation hr (t) = hr (t2) with t ≤ [t2, t3]. Then hr (t) > hr (t4) for all t ≤ (t4, t3], and hence\n(5.5) is violated at t = t4.\nNow let M be the Lipschitz constant for V in a neigborhood of the trajectory of x.\nSince a is continuous,\n\nx(t + ±) - x(t)\n\nlim\n- a(x(t)) = 0 8 t.\n±!0,±>0\nHence the maximum (over t ≤ [t0, t1 - ±]) of\nV (x(t + ±)) - V (x(t))\nV (x(t) + ±a(x(t))) - V (x(t)) V (x(t + ±)) - V (x(t) + ±a(x(t)))\n=\n+\n\nV (x(t) + ±a(x(t))) - V (x(t))\nx(t + ±) - x(t) - ±a(x(t))\n\n∀\n+ M\nconverges to a non-positive limit as ± ! 0.\n\nZ\nA time-varying ODE model\nx 1(t) = a1(x1(t), t)\n(5.6)\ncan be converted to (5.2) by introducing\nx(t) = [x1(t); t],\na([barx; ¿]) = [a1( x, ¿); 1],\nin which case the Lyapunov function V = V (x(t)) = V (x1(t), t) can naturally depend on\ntime.\n5.1.3\nStorage functions for ODE models\nConsider the ODE model\nx (t) = f(x(t), u(t))\n(5.7)\nwith state vector x(t) ≤ X 1⁄2 Rn, input u(t) ≤ U 1⁄2 Rm, where f : X × U ≡! Rn is a\ngiven function. Let 3⁄4 : X × U ≡! R be a given functional. A function V : X ≡! R is\ncalled a storage function with supply rate 3⁄4 for system (5.7)\nt1\nV (x(t1)) - V (x(t0)) ∀\n3⁄4(x(t), u(t))dt\nt0\nfor every pair of integrable functions x :\n[t0, t1] ≡! X, u :\n[t0, t1] ≡! U such that the\ncomposition t ≡! f(x(t), u(t)) satisfies the identity\nZ t\nx(t) = x(t0) +\nf(x(t), u(t))dt\nt0\nfor all t ≤ [t0, t1].\nWhen X is an open set, f and 3⁄4 are continuous, and V is continuously differentiable,\nverifying that a given f is a valid storage function with supply rate 3⁄4 is straightforward:\nit is sufficient to check that\n∈V · f( u) ∀ 3⁄4( u) 8\nu ≤ U.\nx,\nx,\nx ≤ X,\nWhen V is locally Lipschitz, the following generalization of Theorem 5.1 is available.\nTheorem 5.2 If X is an open set in Rn , V : X ≡! R is locally Lipschitz, f, 3⁄4 : X×U ≡!\nRn are continuous, and condition\nV (\nx,\nx)\nx + tf( u)) - V (\nx,\nx ≤ X,\nlim\nsup\n∀ 3⁄4( u) 8\nu ≤ U\n(5.8)\n2!0,2>0 0<t<2\nt\nis satisfied then V (x(t)) is a storage function with supply rate 3⁄4 for system (5.7).\nThe proof of the theorem follows the lines of Theorem 5.1. Further generalizations to\ndiscontinuous functions f, etc., are possible."
    },
    {
      "category": "Lecture Notes",
      "title": "lec6_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/ef45f15ff495194117083d0aafb13285_lec6_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 6: Storage Functions And Stability Analysis1\nThis lecture presents results describing the relation between existence of Lyapunov or\nstorage functions and stability of dynamical systems.\n6.1\nStability of an equilibria\nIn this section we consider ODE models\nx (t) = a(x(t)),\n(6.1)\nwhere a : X 7! Rn is a continuous function defined on an open subset X of Rn . Remem\nber that a point\nx0) = 0, i.e. if x(t) ≥\nx0 ≤ X is an equilibrium of (6.1) if a(\nx0 is a solution\nof (6.1). Depending on the behavior of other solutions of (6.1) (they may stay close to\nx0, or converge to\n\nx0 as t ! →, or satisfy some other specifications) the equilibrium may\nbe called stable, asymptotically stable, etc. Various types of stability of equilibria can be\nderived using storage functions. On the other hand, in many cases existence of storage\nfunctions with certain properties is impled by stability of equilibria.\n6.1.1\nLocally stable equilibria\nRemember that a point x0 ≤ X is called a (locally) stable equilibrium of ODE (6.1) if\nfor every 2 > 0 there exists ± > 0 such that all maximal solutions x = x(t) of (6.1) with\nx0| · ± are deinfed for all t ∀ 0, and satisfy |x(t) - 0| < 2 for all t ∀ 0.\n|x(0) -\nx\nThe statement below uses the notion of a lower semicontinuity: a function f : Y 7! R,\ndefined on a subset Y of Rn, is called lower semicontinuous if\nlim\ninf\nf( ) ∀ f(\nx\nx\nx¤) 8 ¤ ≤ Y.\nr!0,r>0 x→Y : |\nx¤ |<r\n\nx-\n1Version of September 24, 2003\n\n1⁄2\nTheorem 6.1 x0 ≤ X is a locally stable equilibrium of (6.1) if and only if there exist\nc > 0 and a lower semicontinuous function V : Bc( x0) 7! R, defined on\nx0) = {\nx0inf < c}\nBc(\nx : infx -\nand continuous at x0, such that V (x(t)) is monotonically non-increasing along the solu\n\ntions of (6.1), and\nV (\nx) 8\nx0)/{\nx0) < V (\nx ≤ Bc(\nx0}.\nProof To prove that (ii) implies (i), define\nx) - V (\nx -\nVˆ(r) = inf{V (\nx0) : |\nx0| = r\nfor r ≤ (0, c). Since V is assumed lower semicontinuous, the infimum is actually a min\nimum, and hence is strictly positive for all r ≤ (0, c). On the other hand, since V is\ncontinuous at x0, Vˆ(r) converges to zero as r ! 0. Hence, for a given 2 > 0, one can find\n± > 0 such that\nx) 8\nx -\nVˆ(min{2, c/2}) > V (\nx : |\nx0| < ±.\nHence a solution x = x(t) of (6.1) with an initial condition such that |x(0) - x0| < ± (and\nhence V (x(0)) < Vˆ(min{2, c/2}) cannot cross the sphere |x -\n\nx0| = min{2, c/2}.\nTo prove that (i) implies (ii), define V by\nV (\nx0inf : t ∀ 0, x(0) =\nx) = sup{infx(t) -\nx, x(·) satisfies (6.1) }.\n(6.2)\nSince, by assumption, solutions starting close enough to x0 never leave a given disc cen\ntered at x0, V is well defined in a neigborhood X0 of x0. Then, by its very definition,\n\nV (x(t)) is not increasing for every solution of (6.1) starting in X0. Since V is a supremum,\nit is lower semicontinuous (actually, here we use the fact, not mentioned before, that if\n1 and xk (t1) ! x1\n1 then there exists\n= xk (t) are solutions of (6.1) such that xk (t0) ! x\nxk\n1 and x(t1) = x 1\na solution of (6.1) with x(t0) = x\n). Moreover, V is continuous at x0,\nbecause of stability of the equilibrium x0.\nOne can ask whether existence of a Lyapunov function from a better class (say, con\ntinuous functions) is possible. The answer, in general, is negative, as demonstrated by\nthe following example.\nExample 6.1 The equilibrium x0 = 0 of the first order ODE Let a : R 7! R be defined\nby\nx2)sgn(\nx),\nx ≡\nexp(-1/\nx) sin2(\n= 0,\na( x) =\n\n0,\nx = 0.\nThen a is arbitrary number of times differentialble and the equilibrium x0 = 0 of (6.1) is\nlocally stable. However, every continuous function V : R 7! R which does not increase\nalong system trajectories will achieve a maximum at x0 = 0.\n\nFor the case of a linear system, however, local stability of equilibrium x0 = 0 implies\nexistence of a Lyapunov function which is a positive definite quadratic form.\nTheorem 6.2 If a : Rn 7! Rn is defined by\na( ) = Ax\nx\n\nwhere A is a given n- by-n matrix, then equilibrium x0 = 0 of (6.1) is locally stable if and\n∗\nonly if there exists a matrix Q = Q∗ > 0 such that V (x(t)) = x(t) Qx(t) is monotonically\nnon-increasing along the solutions of (6.1).\nThe proof of this theorem, which can be based on considering a Jordan form of A, is\nusually a part of a standard linear systems class.\n6.1.2\nLocally asymptotically stable equilibria\nA point x0 is called a (locally) asymptotically stable equilibrium of (6.1) if it is a stable\nequilibria, and, in addition, there exists e0 > 0 such that every solution of (6.1) with\nx0| < 20 converges to\n|x(0) -\nx0 as t ! →.\nTheorem 6.3 If V : X 7! R is a continuous function such that\nV (\nx) 8\nx\nx0) < V (\nx ≤ X/{ 0},\nand V (x(t)) is strictly monotonically decreasing for every solution of (6.1) except x(t) ≥\nx0 then\n\nx0 is a locally asymptotically stable equilibrium of (6.1).\nProof From Theorem 6.1, x0 is a locally stable equilibrium. It is sufficient to show\nthat every solution x = x(t) of (6.1) starting sufficiently close to x0 will converge to\nx0 as t ! →. Assume the contrary. Then x(t) is bounded, and hence will have at\n\nx¤ which is not x\nleast one limit point\n0. In addition, the limit V of V (x(t)) will exist.\nConsider a solution x¤ = x¤(t) starting from that point. By continuous dependence on\n\ninitial conditions we conclude that V (x¤(t)) = V is constant along this solution, which\ncontradicts the assumptions.\nA similar theorem deriving existence of a smooth Lyapunov function is also valid.\nTheorem 6.4 If 0 is an asymptotically stable equilibrium of system (6.1) where a : X 7!\nx\nRn is a continuously differentiable function defined on an open subset X of Rn then there\nx\nx\nx\nexists a continuously differentiable function V : B2( 0) 7! R such that V ( 0) < V ( ) for\nall =\nx ≡\nx0 and\n∈V (\nx) < 0 8\nx0)/{\nx)a(\nx ≤ B2(\nx0}.\n\nProof Define V by\nV (x(0)) =\n1⁄2(|x(t)|2)dt,\nwhere 1⁄2 : [0, →) 7! [0, →) is positive for positive arguments and continuously differen\ntiable. If V is correctly defined and differentiable, differentiation of V (x(t)) with respect\nto t at t = 0 yields\n∈V (x(0))a(x(0)) = -1⁄2(|x(0)|2),\nwhich proves the theorem. To make the integral convergent and continuously differen\ntiable, it is sufficient to make 1⁄2(y) converging to zero quickly enough as y ! 0.\nFor the case of a linear system, a classical Lyapunov theorem shows that local stability\nof equilibrium x0 = 0 implies existence of a strict Lyapunov function which is a positive\ndefinite quadratic form.\nTheorem 6.5 If a : Rn 7! Rn is defined by\na(\nx\nx) = A\nwhere A is a given n- by-n matrix, then equilibrium x0 = 0 of (6.1) is locally asymptotically\nstable if and only if there exists a matrix Q = Q∗ > 0 such that, for V (\nx\nx,\nx) = ∗Q\n∈V (\nx = -|\nx)A\nx|2 .\n6.1.3\nGlobally asymptotically stable equilibria\nHere we consider the case when a : Rn 7! Rn in defined for all vectors. An equilibrium\nx0 of (6.1) is called globally asymptotically stable if it is locally stable and every solution\nof (6.1) converges to x0 as t ! →.\n\nTheorem 6.6 If function V : Rn 7! R has a unique minimum at x0, is strictly mono\ntonically decreasing along every trajectory of (6.1)except x(t) ≥ x0, and has bounded level\nsets then x0 is a globally asymptotically stable equilibrium of (6.1).\nThe proof of the theorem follows the lines of the proof of Theorem 6.4. Note that the\nassumption that the level sets of V are bounded is critically important: without it, some\nsolutions of (6.1) may converge to infinity instead of x0."
    },
    {
      "category": "Lecture Notes",
      "title": "lec7_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/a7e38533cf83040a59e6270bee15fd64_lec7_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 7: Finding Lyapunov Functions1\nThis lecture gives an introduction into basic methods for finding Lyapunov functions and\nstorage functions for given dynamical systems.\n7.1\nConvex search for storage functions\nThe set of all real-valued functions of system state which do not increase along system\ntrajectories is convex, i.e. closed under the operations of addition and multiplication by a\npositive constant. This serves as a basis for a general procedure of searching for Lyapunov\nfunctions or storage functions.\n7.1.1\nLinearly parameterized storage function candidates\nConsider a system model given by discrete time state space equations\nx(t + 1) = f (x(t), w(t)),\ny(t) = g(x(t), w(t)),\n(7.1)\nwhere x(t) ≤ X ∀ Rn is the system state, w(t) ≤ W ∀ Rm is system input, y(t) ≤ Y ∀ Rk\nis system output, and f : X × W ∈! X, g : X × W ∈! Y are given functions. A functional\nV : X ∈! R is a storage function for system (7.1) with supply rate ψ : Y × W ∈! R if\nV (x(t + 1)) - V (x(t)) → ψ(y(t))\n(7.2)\nfor every solution of (7.1), i.e. if\nx,\nx) → ψ(g( w),\nx ≤ X, w ≤ W.\n(7.3)\nV (f ( w)) - V (\nx,\nw) 8\n\n1Version of September 26, 2003\n\n1⁄2\nIn particular, when ψ inf0, this yields the definition of a Lyapunov function.\nFinding, for a given supply rate, a valid storage function (or at least proving that one\nexists) is a major challenge in constructive analysis of nonlinear systems. The most com\nmon approach is based on considering a linearly parameterized subset of storage function\ncandidates V defined by\nN\n\nx) =\nφq Vq (\nV = {V (\nx),\n(7.4)\nq=1\nwhere {Vq } is a fixed set of basis functions, and φk are parameters to be determined. Here\nevery element of V is considered as a storage function candidate, and one wants to set up\nan efficient search for the values of φk which yield a function V satisfying (7.3).\nExample 7.1 Consider the finite state automata defined by equations (7.1) with value\nsets\nX = {1, 2, 3},\nW = {0, 1},\nY = {0, 1},\nand with dynamics defined by\nf (1, 1) = 2, f (2, 1) = 3, f (3, 1) = 1, f (1, 0) = 1, f (2, 0) = 2, f (3, 0) = 2,\ng(1, 1) = 1, g( w) = 0 8 ( w) ≡\nx,\nx, = (1, 1).\nIn order to show that the amount of 1's in the output is never much larger than one third\nof the amount of 1's in the input, one can try to find a storage function V with supply\nrate\nψ( w) = w -3\ny,\n\ny.\nTaking three basis functions V1, V2, V3 defined by\n1,\nx = k,\n\nVk ( x) =\n0,\nx = k,\n≡\nthe conditions imposed on φ1, φ2, φ3 can be written as the set of six affine inequalities (7.3),\ntwo of which (with ( w) = (1, 0) and ( w) = (2, 0)) will be satisfied automatically, while\nx,\nx,\nthe other four are\nx,\nφ2 -φ3 →1 at ( w) = (3, 0),\nx,\nφ2 -φ1 →-2 at ( w) = (1, 1),\nx,\nφ3 -φ2 →1 at ( w) = (2, 1),\nx,\nφ1 -φ3 →1 at ( w) = (3, 1).\nSolutions of this linear program are given by\nφ1 = c, φ2 = c -2,\nφ3 = c -1,\n\nwhere c ≤R is arbitrary. It is customary to normalize storage and Lyapunov functions\nso that their minimum equals zero, which yields c = 2 and\nφ1 = 2, φ2 = 0, φ3 = 1.\nNow, summing the inequalities (7.2) from t = 0 to t = T yields\nT -1\nT -1\n\ny(t) →V (x(0)) -V (x(T )) +\nw(t),\nt=0\nt=0\nwhich is implies the desired relation between the numbers of 1's in the input and in the\noutput, since V (x(0)) -V (x(T )) cannot be larger than 2.\n7.1.2\nStorage functions via cutting plane algorithms\nThe possibility to reduce the search for a valid storage function to convex optimization,\nas demonstrated by the example above, is a general trend. One general situation in which\nan efficient search for a storage function can be performed is when a cheap procedure of\nchecking condition (7.3) (an oracle) is available.\nAssume that for every given element V ≤V it is possible to find out whether condition\n(7.3) is satisfied, and, in the case when the answer is negative, to produce a pair of vectors\nx ≤X, w ≤W for which the inequality in (7.3) does not hold. Select a sufficiently large\n\nset T0 (a polytope or an ellipsoid) in the space of parameter vector φ = (φq )q\nN\n=1 (this set\nwill limit the search for a valid storage function). Let φ ¤ be the \"center\" of T0. Define\nV by the φ ¤, and apply the verification \"oracle\" to it. If V is a valid storage function,\nthe search for storage function ends successfully. Otherwise, the \"invalidity certificate\"\nx,\n( w) produced by the oracle yields a hyperplane separating φ ¤ and the (unknown) set\nof φ defining valid storage functions, thus cutting a substantial portion from the search\nset T0, reducing it to a smaller set T1. Now re-define φ ¤ as the center of T1 and repeat\nthe process by constructing a sequence of monotonically decreasing search sets Tk , until\neither a valid storage function is found, or Tk shrinks to nothing.\nWith an appropriate selection of a class of search sets Tk (ellipsoids or polytopes\nare most frequently used) and with an adequate definition of a \"center\" (the so-called\n\"analytical center\" is used for polytopes), the volume of Tk can be made exponentially\ndecreasing, which constitutes fast convergence of the search algorithm.\n7.1.3\nCompletion of squares\nThe success of the search procedure described in the previous section depends heavily\non the choice of the basis functions Vk . A major difficulty to overcome is verification of\n(7.3) for a given V . It turns out that the only known large linear space of functionals\n\n·\n\nF : Rn ∈! R which admits efficient check of non-negativity of its elements is the set of\nquadratic forms\n·\n∗\n·\n\nx\nx\n∗\n\nF ( x) =\nQ\n, (Q = Q )\nfor which nonnegativity is equivalent to positive semidefiniteness of the coefficient matrix\nQ.\nThis observation is exploited in the linear-quadratic case, when f, g are linear functions\nf( w) = A\nw, g( w) = C\nw,\nx,\nx + B\nx,\nx + D\nand ψ is a quadratic form\n·\n∗\n·\n\nx\nx\n\nψ( w) =\n\n§\n.\nx,\n\nw\nw\nThen it is natural to consider quadratic storage function candidates\nV (\nx\nx\nx) = ∗P\nonly, and (7.3) transforms into the (symmetric) matrix inequality\nP A + A∗P P B\n→ §.\n(7.5)\nB∗P\nSince this inequality is linear with respect to its parameters P and §, it can be solved\nrelatively efficiently even when additional linear constraints are imposed on P and §.\nNote that a quadratic functional is non-negative if and only if it can be represented as\na sum of squares of linear functionals. The idea of checking non-negativity of a functional\nby trying to represent it as a sum of squares of functions from a given linear set can be\nused in searching for storage functions of general nonlinear systems as well. Indeed, let\nˆH : Rn × Rm ∈! RM and Vˆ : Rn ∈! RN be arbitrary vector-valued functions. For every\nφ ≤ RN , condition (7.3) with\nx) = φ ∗Vˆ(\nV (\nx)\nis implied by the identity\nx,\n∗ ˆ x) + ˆ x,\nH( w) = ψ( w) 8\n\nφ ∗Vˆ(f( w)) - φ V (\nH( w)∗S ˆ x,\nx,\nx ≤ X, w ≤ W,\n(7.6)\nas long as S = S∗ ∗ 0 is a positive semidefinite symmetric matrix. Note that both the\nstorage function candidate parameter φ and the \"sum of squares\" parameter S = S∗ ∗ 0\nenter constraint (7.6) linearly. This, the search for a valid storage function is reduced to\nsemidefinite program.\nIn practice, the scalar components of vector ˆH should include enough elements so that\nidentity (7.6) can be achieved for every φ ≤ RN by choosing an appropriate S = S∗ (not\nnecessarily positivie semidefinite). For example, if f, g, ψ are polynomials, it may be a\ngood idea to use a polynomial Vˆ and to define ˆH as the vector of monomials up to a given\ndegree.\n\n7.2\nStorage functions with quadratic supply rates\nAs described in the previous section, one can search for storage functions by considering\nlinearly parameterized sets of storage function candidates. It turns out that storage\nfunctions derived for subsystems of a given system can serve as convenient building blocks\n(i.e. the components Vq of Vˆ). Indeed, assume that Vq = Vq (x(t)) are storage functions\nwith supply rates ψq = ψq (z(t)). Typically, z(t) includes x(t) as its component, and has\nsome additional elements, such as inputs, outputs, and othe nonlinear combinations of\nsystem states and inputs. If the objective is to find a storage function V¤ with a given\nsupply rate ψ¤, one can search for V¤ in the form\nN\n\nV (x(t)) =\nVq (x(t)),\nφq ∗ 0,\n(7.7)\nq=1\nwhere φq are the search parameters. Note that in this case it is known a-priori that every\nV¤ in (7.7) is a storage function with supply rate\nN\n\nψ(z(t)) =\nφq ψq (z(t)).\n(7.8)\nq=1\nTherefore, in order to find a storage function with supply rate ψ¤ = ψ¤(z(t)), it is sufficient\nto find φq ∗ 0 such that\nN\n\nφ1ψq (\nz) 8\nz) → ψ¤(\nz.\n(7.9)\nq=1\nWhen ψ¤, ψq are generic functions, even this simplified task can be difficult. However, in\nthe important special case when ψ¤ and ψq are quadratic functionals, the search for φq in\n(7.9) becomes a semidefinite program.\nIn this section, the use of storage functions with quadratic supply rates is discussed.\n7.2.1\nStorage functions for LTI systems\nx) =\nx is a storage function for LTI system\nA quadratic form V (\nx ∗P\nx = Ax + Bw\n(7.10)\nwith quadratic supply rate\n·\n∗\n·\n\nx\nx\n\nψ( w) =\n\n§\nx,\n\nw\nw\nif and only if matrix inequality (7.5) is satisfied.\nThe well-known Kalman-Popov-Yakubovich Lemma, or positive real lemma gives useful\nfrequency domain condition for existence of such P = P ∗ for given A, B, §.\n\nTheorem 7.1 Assume that the pair (A, B) is controllable. A symmetric matrix P = P ∗\nsatisfying (7.5) exists if and only if\n·\n∗\n·\n\nx\nw\n§\nx\nw\n∗0\nwhenever jσ x = A x + B w for some σ ≤R.\n(7.11)\nMoreover, if there exists a matrix K such that A + BK is a Hurwitz matrix, and\n·\n∗\n·\n\nI\nI\n§\n→0,\nK\nK\nthen all such matrices P = P ∗ are positive semidefinite.\nExample 7.2 Let G(s) = C(sI -A)-1B + D be a stable transfer function (i.e. matrix\nA is a Huewitz matrix) with a controllable pair (A, B). Then |G(jσ)| →1 for all σ ≤R\nif and only if there exists P = P ∗ ∗0 such that\nw|2\nw|2\nx\nx + B\nx + D\n\n2 ∗P(A\nw) →|\n-|C\nx ≤Rn , w ≤Rm .\nThis can be proven by applying Theorem 7.1 with\nψ( w) = |\nx + D\nx,\nw|2 -|C\nw|2\nand K = 0.\n7.2.2\nStorage functions for sector nonlinearities\nWhenever two components v = v(t) and w = w(t) of the system trajectory z = z(t)\nare related in such a way that the pair (v(t), w(t)) lies in the cone between the two lines\nw = k1v and v = k2v, V inf0 is a storage function for\nψ(z(t)) = (w(t) -k1v(t))(k2v(t) -w(t)).\nFor example, if w(t) = v(t)3 then ψ(z(t)) = v(t)w(t). If w(t) = sin(t) sin(v(t)) then\nψ(z(t)) = |v(t)|2 -|w(t)| .\n7.2.3\nStorage for scalar memoryless nonlinearity\nWhenever two components v = v(t) and w = w(t) of the system trajectory z = z(t) are\nrelated by w(t) = A(v(t)), where A : R ∈! R is an integrable function, and v(t) is a\ncomponent of system state, V (x(t)) = A(v(t)) is a storage function with supply rate\nψ(z(t)) = v (t)w(t),\nwhere\nZ y\nA(y) =\nA(φ)dφ.\n\n7.3\nImplicit storage functions\nA number of important results in nonlinear system analysis rely on storage functions for\nwhich no explicit formula is known. It is frequently sufficient to provide a lower bound for\nthe storage function (for example, to know that it takes only non-negative values), and\nto have an analytical expression for the supply rate function ψ.\nIn order to work with such \"implicit\" storage functions, it is helpful to have theorems\nwhich guarantee existence of non-negative storage functions for a given supply rate. In this\nregard, Theorem 7.1 can be considered as an example of such result, stating existence of\na storage function for a linear and time invariant system as an implication of a frequency-\ndependent matrix inequality. In this section we present a number of such statements\nwhich can be applied to nonlinear systems.\n7.3.1\nImplicit storage functions for abstract systems\nConsider a system defined by behavioral set B = {z} of functions z : [0, ⊂) ∈! Rq . As\nusually, the system can be autonomous, in which case z(t) is the output at time t, or with\nan input, in which case z(t) = [v(t); w(t)] combines vector input v(t) and vector output\nw(t).\nTheorem 7.2 Let ψ : Rq ∈! R be a function and let B be a behavioral set, consisting of\nsome functions z : [0, ⊂) ∈! Rq . Assume that the composition ψ(z(t)) is integrable over\nevery bounded interval (t0, t1) in R+ for all z ≤ B. For t0, t ≤ R+ define\nZ t\nI(z, t0, t) =\nψ(z(φ))dφ.\nt0\nThe following conditions are equivalent:\n(a) for every z0 ≤ B and t0 ≤ R+ the set of values I(z, t0, t), taken for all t ∗ t0 and\nfor all z ≤ B defining same state as z0 at time t0, is bounded from below;\n(b) there exists a non-negative storage function V : B×R+ ∈! R+ (such that V (z1, t) =\nV (z2, t) whenever z1 and z2 define same state of B at time t) with supply rate ψ.\nMoreover, when condition (a) is satisfied, a storage function V from (b) can be defined by\nV (z0(·), t0) = -inf I(z, t0, t),\n(7.12)\nwhere the infimum is taken over all t ∗ t0 and over all z ≤ B defining same state as z0 at\ntime t0.\nProof Implication (b)≥(a) follows directly from the definition of a storage function,\nwhich requires\nV (z0, t1) - V (z0, t0) → I(z, t0, t1)\n(7.13)\n\n1⁄2\nZ\nfor t1 ∗ t0, z0 ≤ B. Combining this with V ∗ 0 yields\nI(z, t0, t1) ∗ -V (z, t0) = -V (z0, t0)\nfor all z, z0 defining same state of B at time t0.\nNow let us assume that (a) is valid. Then a finite infimum in (7.12) exists (as an\ninfimum over a non-empty set bounded from below) and is not positive (since I(z0, t0, t0) =\n0). Hence V is correctly defined and not negative. To finish the proof, let us show that\n(7.13) holds. Indeed, if z1 defines same state as z0 at time t1 then\nz0(t),\nt → t1,\nz01(t) =\nz1(t), t > t1\ndefines same state as z0 at time t0 < t1 (explain why). Hence the infimum of I(z, t0, t) in\nthe definition of V is not larger than the infimum of integrals of all such z01, over intervals\nof length not smaller than t1 - t0. These integrals can in turn be decomposed into two\nintegrals\nI(z01, t0, t) = I(z0, t0, t1) + I(z1, t1, t),\nwhich yields the desired inequality.\n7.3.2\nStorage functions for ODE models\nAs an important special case of Theorem 7.2, consider the ODE model\nx (t) = f(x(t), w(t)),\n(7.14)\ndefined by a function f : X × W ∈! Rn , where X, W are subsets of Rn and Rm\nrespectively. Cxonsider the behavior model B consisting of all functions z(t) = [x(t); v(t)]\nwhere x : [0, ⊂) ∈! X is a solution of (7.14). In this case, two signals z1 = [x1; v1] and\nz2 = [x2; v2] define same state of B at time t0 if and only if x1(t0) = x2(t0). Therefore,\naccording to Theorem 7.2, for a given function ψ : X × W ∈! R, existence of a function\nV : X × R+ ∈! R+ such that\nt2\nV (x(t2), t2) - V (x(t1), t1) →\nψ(x(t), v(t))dt\nt1\nfor all 0 → t1 → t2, [x; v] ≤ B is equivalent to finiteness of the infimum of the integrals\nZ t\nψ(x(φ), v(φ))dφ\nt0\nover all solutions of (7.14) with a fixed x(t0) = x0 which can be extended to the time\ninterval [0, ⊂).\n\nZ 1\nZ 1\nIn the case when X = Rn, and f : Rn ∈! Rn is such that existence and uniqueness\nof solutions x : [0, ⊂) ∈! Rn is guaranteed for all locally integrable inputs w : [0, ⊂) ∈!\nW and all initial conditions x(t0) = x0 ≤ Rn , the infimum in (7.12) (and hence, the\ncorresponding storage function) do not depend on time. If, in addition, f is continuous\nand V is continuously differentiable, the well-known dynamic programming condition\nx,\nx)f( w)}0 → inf {ψ(\nw)-⇒V (\nx0,\nx0 ≤ Rn\nlim\ninf\n{ψ( w)-⇒V (\nx,\nx0,\nx0)f(\nw)} 8\n2!0,2>0 w→W,\nx0)\nw→W\n\nx→B2 (\n\n(7.15)\nwill be satisfied. However, using (7.15) requires a lot of caution in most cases, since, even\nfor very smooth f, ψ, the resulting storage function V does not have to be differentiable.\n7.3.3\nZames-Falb quadratic supply rate\nA non-trivial and powerful case of an implicitly defined storage function with a quadratic\nsupply rate was introduced in late 60-s by G. Zames and P. Falb.\nTheorem 7.3 Let A, B, C be matrices such that A is a Hurwitz matrix, and\n|Ce AtB|dt < 1.\nLet A : R ∈! R be a monotonic odd function such that\n\nw) → |\n\n0 → wA(\nw|2 8 w ≤ R.\nThen for all μ < 1 system\nx (t) = Ax(t) + Bw(t)\nhas a non-negative storage function with supply rate\nx,\nw - μA(\nw - C\nψ+( w) = (\nw))(\nx),\nand system\nx (t) = Ax(t) + B(w(t) - μA(w(t))\nhas a non-negative storage function with supply rate\nx,\nw - μA(\nx)\nψ-( w) = (\nw) - C w.\nThe proof of Theorem 7.3 begins with establishing that, for every function h : R ∈! R\nwith L1 norm not exceeding 1, and for every square integrable function w : R ∈! R the\nintegral\n(w(t) - A(w(t)))y(t)dt,\n-1\nwhere y = h ¤ w, is non-negative. This verifies that the assumptions of Theorem 7.2\nare satisfied, and proves existence of the corresponding storage function without actually\nfinding it. Combining the Zames-Falb supply rate with the statement of the Kalman-\nYakubovich-Popov lemma yields the following stability criterion.\n\nZ 1\nZ 1\nTheorem 7.4 Assume that matrices Ap, Bp, Cp are such that Ap is a Hurwitz matrix,\nand there exists γ > 0 such that\nRe(1 - G(jσ))(1 - H(jσ)) ∗ γ 8 σ ≤ R,\nwhere H is a Fourier transform of a function with L1 norm not exceeding 1, and\nG(s) = Cp(sI - Ap)-1Bp.\nThen system\nx (t) = Apx(t) + BpA(Cx(t) + v(t))\nhas finite L2 gain, in the sense that there exists θ > 0 such that\n|x(t)|2dt → θ(|x(0)|2 +\n|v(t)|2dt\nfor all solutions.\n7.4\nExample with cubic nonlinearity and delay\nConsider the following system of differential equations2 with an uncertain constant delay\nparameter φ:\nx 1(t)\n= -x1(t)3 - x2(t - φ)3\n(7.16)\nx 2(t)\n= x1(t) - x2(t)\n(7.17)\nAnalysis of this system is easy when φ = 0, and becomes more difficult when φ is an\narbitrary constant in the interval [0, φ0]. The system is not exponentially stable for any\nvalue of φ. Our objective is to show that, despite the absence of exponential stability, the\nmethod of storage functions with quadratic supply rates works.\nThe case φ = 0\nFor φ = 0, we begin with describing (7.16),(7.17) by the behavior set\nZ = {z = [x1; x2; w1; w2]},\nwhere\nw1 = x1, w2 = x2, x 1 = -w1 - w2, x 2 = x1 - x2.\nQuadratic supply rates for which follow from the linear equations of Z are given by\n·\n∗\n·\n\nx1\n-w1 - w2\nψLT I (z) = 2\nP\nx1 - x2\n,\nx2\n2Suggested by Petar Kokotovich\n\nwhere P = P ∗ is an arbitrary symmetric 2-by-2 matrix defining storage function\nVLTI(z(·), t) = x(t)∗Px(t).\nAmong the non-trivial quadratic supply rates ψ valid for Z, the simplest are defined by\nψNL(z) = d1x1w1 + d2x2w2 + q1w1(-w1 - w2) + q2w2(x1 - x2),\nwith the storage function\nVNL(z(·), t) = 0.25(q1x1(t)4 + q2x2(t)4),\nwhere dk ∗ 0. It turns out (and is easy to verify) that the only convex combinations of\nthese supply rates which yield ψ → 0 are the ones that make ψ = ψLTI + ψNL = 0, for\nexample\n·\n\n0.5 0\nP =\n,\nd1 = d2 = q2 = 1,\nq1 = 0.\nThe absence of strictly negative definite supply rates corresponds to the fact that the\nsystem is not exponentially stable. Nevertheless, a Lyapunov function candidate can be\nconstructed from the given solution:\nV (x) = x ∗Px + 0.25(q1x1 + q2x2) = 0.5x1 + 0.25x2.\nThis Lyapunov function can be used along the standard lines to prove global asymptotic\nstability of the equilibrium x = 0 in system (7.16),(7.17).\n7.4.1\nThe general case\nNow consider the case when φ ≤ [0, 0.2] is an uncertain parameter. To show that the\ndelayed system (7.16),(7.17) remains stable when φ → 0.2, (7.16),(7.17) can be represented\nby a more elaborate behavior set Z = {z(·)} with\nz = [x1; x2; w1; w2; w3; w4; w5; w6] ≤ R8 ,\nsatisfying LTI relations\nx 1 = -w1 - w2 + w3,\nx 2 = x1 - x2\nand the nonlinear/infinite dimensional relations\nw1(t) = x1, w2 = x2, w3 = x2 - (x2 + w4)3 ,\nw4(t) = x2(t - φ) - x2(t), w5 = w4 , w6 = (x1 - x2)3 .\nSome additional supply rates/storage functions are needed to bound the new variables.\nThese will be selected using the perspective of a small gain argument. Note that the\n\n(\n)\n\n·\n\nperturbation w4 can easily be bounded in terms of x2 = x1 - x2. In fact, the LTI system\nwith transfer function (exp(-φs) - 1)/s has a small gain (in almost any sense) when φ is\nsmall. Hence a small gain argument would be applicable provided that the gain \"from w4\nto x2\" could be bounded as well.\nIt turns out that the L2-induced gain from w4 to x 2 is unbounded. Instead, we can\nuse the L4 norms. Indeed, the last two components w5, w6 of w were introduced in order\nto handle L4 norms within the framework of quadratic supply rates. More specifically, in\naddition to the usual supply rate\n·\n∗\n·\n\nx1\n-w1 - w2 + w3\nψLT I (z) = 2\nP\nx1 - x2\n,\nx2\nthe set Z has supply rates\nψ(z) =d1x1w1 + d2x2w2 + q1w1(-w1 - w2 + w3) + q2w2(x1 - x2)\n+ d3[0.99(x1w1 + x2w2) - x1w3 + 2.54 w4w5 - 0.54(x1 - x2)w6]\n+ q3[0.24(x1 - x2)w6 - w4w5],\ndi ∗ 0. Here the supply rates with coefficients d1, d2, q1, q2 are same as before. The term\nwith d3, based on a zero storage function, follows from the inequality\nμ\n¶4\nμ\n¶4\nx1 - x2\n0.99(x1 + x2) - x1(x2 - (x2 + w4)3) +\n5w4\n-\n∗ 0\n(which is satisfied for all real numbers x1, x2, w4, and can be checked numerically).\nThe term with q3 follows from a gain bound on the transfer function G¿ (s) = (exp(-φs)-\n1)/s from x1 - x2 to w4. It is easy to verify that the L1 norm of its impulse response\nequals φ, and hence the L4 induced gain of the causal LTI system with transfer function\nG¿ will not exceed 1. Consider the function\n\nZ 1\nZ t\nVd(v(·), T) = - inf\n0.24|v1(t)|4 -\nv1(r)dr\ndt,\n(7.18)\nT\nt-¿\nwhere the infimum is taken over all functions v1 which are square integrable on (0, ⊂)\nand such that v1(t) = v(t) for t → T. Because of the L4 gain bound of G¿ with φ ≤ [0, 0.2]\ndoes not exceed 0.2, the infimum in (7.18) is bounded. Since we can always use v1(t) = 0\nfor t > T, the infimum is non-positive, and hence Vd is non-negative. The IQC defined\nby the \"q3\" term holds with V3⁄4 = q3Vd(x1 - x2, t).\nLet\nψ0(z) = -0.01(x1w1 + x2w2) = -0.01(x1 + x2),\nwhich reflects our intention to show that x1, x2 will be integrable with fourth power over\n(0, ⊂). Using\n0.5\nP =\n,\nd1 = d2 = 0.01, d3 = q2 = 1, q1 = 0, q3 = 2.54\n\nyields a Lyapunov function\nV (xe(t)) = 0.5x1(t)2 + 0.25x2(t)4 + 2.54Vd(x1 -x2, t),\nwhere xe is the \"total state\" of the system (in this case, xe(T) = [x(T); vT (·)], where\nvT (·) ≤L2(0, φ) denotes the signal v(t) = x1(T -φ + t) -x2(T -φ + t) restricted to the\ninterval t ≤(0, φ)). It follows that\ndV (xe(t)) →-0.01(x1(t)4 + x2(t)4).\ndt\nOn the other hand, we saw previously that V (xe(t)) ∗0 is bounded from below. There\nfore, x1(·), x2(·) ≤¤4 (fourth powers of x1, x2 are integrable over (0, ⊂)) as long as the\ninitial conditions are bounded. Thus, the equilibrium x = 0 in system (7.16),(7.17) is\nstable for 0 →φ →0.2."
    },
    {
      "category": "Lecture Notes",
      "title": "lec8_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/da3ee58a1bcd2becd6920e201eeea7b1_lec8_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 8: Local Behavior at Eqilibria1\nThis lecture presents results which describe local behavior of autonomous systems in terms\nof Taylor series expansions of system equations in a neigborhood of an equilibrium.\n8.1\nFirst order conditions\nThis section describes the relation between eigenvalues of a Jacobian a0( x0) and behavior\nof ODE\nx (t) = a(x(t))\n(8.1)\nor a difference equation\nx(t + 1) = a(x(t))\n(8.2)\nin a neigborhood of equilibrium x0.\nIn the statements below, it is assumed that a : X ≥! Rn is a continuous function\ndefined on an open subset X 1⁄2 Rn . It is further assumed that x0 ⊂ X, and there exists\nan n-by-n matrix A such that\n|a(\nx0) - Aλ|\nx0 + λ) - a(\n! 0 as |λ| ! 0.\n(8.3)\n|λ|\nIf derivatives dak /dxi of each component ak of a with respect to each cpomponent xi\nof x exist at x0, A is the matrix with coefficients dak /dxi, i.e. the Jacobian of the system.\nHowever, differentiability at a single point x0 does not guarantee that (8.3) holds. On the\nother hand, (8.3) follows from continuous differentiability of a in a neigborhood of x0.\n1Version of October 3, 2003\n\nμ·\n\nExample 8.1 Function a : R2 ≥! R2, defined by\n\n2 2 - ( 2\nx2)2 ·\nx1\nx1x2\nx1 - 2\nx1\n\na\n= 2 2\nx2\n\nx2\nx1x2 + ( 2 - 2)2\nx2\nx1\nfor x ≤\nx1 and\n= 0, and by a(0) = 0, is differentiable with respect to\nx2 at every point\nx ⊂ R2, and its Jacobian a0(0) = A equals minus identity matrix. However, condition\n(8.3) is not satisfied (note that a0(\nx = 0).\nx) is not continuous at\n8.1.1\nThe continuous time case\nLet us call an equilibrium x0 of (8.1) exponentially stable if there exist positive real num\nbers σ, r, C such that every solution x : [0, T] ≥! X with |x(0) - x0| < σ satisfies\nx0| ∀ Ce -rt|x(0) -\n|x(t) -\nx0| 8 t → 0.\nThe following theorem can be attributed directly to Lyapunov.\nTheorem 8.1 Assume that a( x0) = 0 and condition (8.3) is satisfied. Then\n(a) if A = a0( x0) is a Hurwitz matrix (i.e. if all eigenvalues of A have negative real\npart) then x0 is a (locally) exponentially stable equilibrium of (8.1);\nx0) has an eigenvalue with a non-negative real part then\n(b) if A = a0(\nx0 is not an\nexponentially stable equilibrium of (8.1);\nx0) has an eigenvalue with a positive real part then\n(c) if A = a0(\nx0 is not a stable\nequilibrium of (8.1).\nNote that Theorem 8.1 does not cover all possible cases: if A is not a Hurwitz matrix\nand does not have eigenvalues with positive real part then the statement says very little,\nand for a good reason: the equilibrium may turn out to be asymptotically stable or\nunstable. Note also that the equilibrium x = 0 from Example 8.1 (where a is differentiable\nbut does not satisfy (8.3)) is not stable, despite the fact that A = -I has all eigenvalues\nat -1.\nExample 8.2 The equilibrium x = 0 of the ODE\nx (t) = (r)x(t) + x(t)3\nis asympotically stable when (r) < 0 (this is due to Theorem 8.1), but also when (r) = 0 and\n< 0. The equilibrium is not stable when (r) > 0 (due to Theorem 8.1), but also when\n(r) = 0 and > 0. In addition, the equilibrium is stable but not asymptotically stable\nwhen (r) = = 0.\n\n8.1.2\nProof of Theorem 8.1\nThe proof of (a) can be viewed as an excercise in \"storage function construction\" outlined\nin the previous lecture. Indeed, assuming, for simplicity, that x0 = 0, (8.1) can be re\nwritten as\nx (t) = Ax(t) + w(t),\nw(t) = a(x(t)) -Ax(t).\nHere the linear part has standard storage functions\nVLTI(\nx 0P\nx) =\nx, P = P 0\nwith supply rates\nαLTI( w) = 2 0P(A\n\nx,\nx\nx + w).\nIn addition, due to (8.3), for every λ > 0 there exists σ > 0 such that the nonlinear\ncomponent w(t) satisfies the sector constraint\nαNL(x(t), w(t)) = λ|x(t)|2 -|w(t)| →0,\nas long as |x(t)| < σ. Since A is a Hurwitx matrix, P = P 0 can be chosen positive definite\nand such that\nPA + A0P = -I.\nThen\nα( w) = αLTI( w) + δαNL( w)\nx,\nx,\nx,\nx|2 + 2δ\nw -δ|\n\nw| -δ|w|2 ,\n= (δλ -1)|\nx 0P\nw|2 ∀(δλ -1)|x|2 -2∈P∈· |x| · |\n\nwhere ∈P∈ is the largest singular value of P, is a supply rate for the storage function\nV = VLTI for every constant δ →0. When δ = 16∈P∈ and λ = 0.25/δ, we have\nx,\nx\nα( w) ∀-0.5| |2 ,\nwhich proves that, for |x(t)| < σ, the inequality\nV (x(t)) ∀-0.5|x(t)|2 ∀-\nV (x(t)).\n2∈P -1∈\nHence\nV (x(t)) ∀e -dtV (x(0)) 8 t →0,\nwhere d = 1/2∈P -1∈, as long as |x(t)| < σ. Since\n-1\n∈P∈· |x(t)| →V (x(t)) →∈P -1∈\n· |x(t)|,\nthis implies (a).\nThe proofs of (b) and (c) are more involved, based on showing that solutions which\nstart at x0 + λv, where v is an eigenvector of A corresponding to an eigenvalue with a non\nnegative (strictly positive) real part, cannot converge to x0 quickly enough (respectively,\ndiverge from x0).\n\nTo prove (b), take a real number d ⊂(0, r/2) such that no two eigenvalues of A sum\nup to -2d. Then P = P 0 be the unique solution of the Lyapunov equation\nP(A + dI) + (A0 + dI)P = -I.\nNote that P is non-singular: otherwise, if Pv = 0 for some v ≤= 0, it follows that\n0(A0\n-|v|2 = v (P(A + dI) + (A0 + dI)P)v = (Pv)0(A + dI)v + v\n+ dI)(Pv) = 0.\nIn addition, P = P 0 is not positive semidefinite: since, by assumption, A + dI has an\neigenvector u ≤= 0 which corresponds to an eigenvalue with a positive real part, we have\n-|u|2 = -2Re( )u Pu,\nhence u0Pu < 0.\nLet σ > 0 be small enough so that\n\nx 0Pw ∀0.5|x|\nfor |w| ∀σ|x|.\nBy assumption, there exists λ > 0 such that\nx) -A\n\n|a(\nx| ∀σ|x| for |x| ∀λ.\nThen\nd (e 2dt\n2dt\nx(t)0Px(t)) = e (2dx(t)0Px(t) + 2x(t)0PAx(t) + 2x(t)0P(a(x(t)) -Ax(t)))\ndt\n∀-0.5e 2dt|x(t)|\nas long as x(t) is a solution of (8.1) and |x(t)| ∀ λ. In particular, this means that if\nx(0)0Px(0) ∀-R < 0 and |x(0)| ∀λ then e2dtx(t)0Px(t) ∀-R for as long as |x(t)| ∀λ,\nwhich contradicts exponential stability with rate r > 2d.\nThe proof of (c) is similar to that of (a).\n8.1.3\nThe discrete time case\nThe results for the discrete time case are similar to Theorem 8.1, with the real parts of\nthe eigenvalues being replaced by the difference between their absolute values and 1.\nLet us call an equilibrium x0 of (8.2) exponentially stable if there exist positive real\nnumbers σ, r, C such that every solution x : [0, T] ≥! X with |x(0) - x0| < σ satisfies\nx0| ∀Ce -rt|x(0) -\n|x(t) -\nx0| 8 t = 0, 1, 2, . . . .\nTheorem 8.2 Assume that a( x0) = 0 and condition (8.3) is satisfied. Then\n\n·\n\n·\n\n(a) if A = a0( 0) is a Schur matrix (i.e. if all eigenvalues of A have absolute value less\nx\nthan one) then 0 is a (locally) exponentially stable equilibrium of (8.2);\nx\n(b) if A = a0( 0) has an eigenvalue with absolute value greater than 1 then 0 is not an\nx\nx\nexponentially stable equilibrium of (8.2);\n(c) if A = a0( 0) has an eigenvalue with absolute value strictly larger than 1 then\nx\nx0 is\nnot a stable equilibrium of (8.2).\n8.2\nHigher order conditions\nWhen the Jacobian A = a0(\nx0 has no eigenvalues\nx0) of (8.1) evaluated at the equilibrium\nwith positive real part, but has some eigenvalues on the imaginary axis, local stability\nanalysis becomes much more complicated. Based on the proof of Theorem 8.1, it is natural\nto expect that system states corresponding to strictly stable eigenvalues will behave in\na predictably stable fashion, and hence the behavior of system states corresponding to\nthe eigenvalues on the imaginary axis will determine local stability or instability of the\nequilibrium.\n8.2.1\nA Center Manifold Theorem\nIn this subsection we assume for simplicity that x0 = 0 is the studied equilibrium of (8.1),\ni.e. a(0) = 0. Assume also that a is k times continuously differentiable in a neigborhood\nof x0 = 0, where k → 1, and that A = a0(0) has no eigenvalues with positive real part, but\nhas eigenvalues on the imaginary axis, as well as in the open left half plane Re(s) < 0.\nThen a linear change of coordinates brings A into a block-diagonal form\nAc\nA =\nAs\n,\nwhere As is a Hurwitz matrix, and all eigenvalues of Ac have zero real part.\nTheorem 8.3 Let a : Rn ≥! Rn be k → 2 times continuously differentiable in a neigbor\nhood of x0 = 0. Assume that a(0) = 0 and\nAc\na (0) = A =\nAs\n,\nwhere As is a Hurwitz p-by-p matrix, and all eigenvalues of the q-by-q matrix Ac have\nzero real part. Then\n(a) there exists σ > 0 and a function h : Rq ≥! Rp , k - 1 times continuously differ\nentiable in a neigborhood of the origin, such that h(0) = 0, h0(0) = 0, and every\nsolution x(t) = [xc(t); xs(t)] of (8.1) with xs(0) = h(xc(0)) and with |xc(0)| < σ\nsatisfies xs(t) = h(x0(t)) for as long as |xc(t)| < σ;\n\n(b) for every function h from (a), the equilibrium x0 = 0 of (8.1) is locally stable\n(asymptotically stable) [unstable] if and only if the equilibrium xc = 0 of the ODE\ndotxc(t) = a([xc(t); h(xc(t))])\n(8.4)\nis locally stable (asymptotically stable) [unstable];\n(c) if the equilibrium xc = 0 of (8.4) is stable then there exist constants r > 0, β > 0\nsuch that for every solution x = x(t) of (8.1) with |x(0)| < r there exists a solution\nxc = xc(t) of (8.4) such that\nlim e °t|x(t) - [xc(t); h(xc(t))]| = 0.\nt!1\nThe set of points\nx = [xc; h(\nx\nMc = {\n\nxc)] : | c| < σ},\nwhere σ > 0 is small enough, is called the central manifold of (8.1). Theorem 8.3, called\nfrequently the center manifold theorem, allows one to reduce the dimension of the system\nto be analyzed from n to q, as long as the function h defining the central manifold can be\ncalculated exactly or to a sufficient degree of accuracy to judge local stability of (8.4).\nExample 8.3 This example is taken from Sastry, p. 312. Consider system\nx 1(t)\n= -x1(t) + kx2(t)2 ,\nx 2(t)\n= x1(t)x2(t),\nwhere k is a real parameter. In this case n = 2, p = q = 1, Ac = 0, As = -1, and k\ncan be arbitrarily large. According to Theorem 8.3, there exists a k times differentiable\nfunction h : R ≥! R such that x1 = h(x2) is an invariant manifold of the ODE (at least,\nin a neigborhood of the origin). Hence\nky 2 = h(y) + h (y)h(y)y\nfor all sufficiently small y. For the 4th order Taylor series expansion\n\nh(y) = h2y 2 + h3y + h4y 4 + o(y ),\nh(y) = 2h2y + 3h3y 2 + 4h4y + o(y 3),\ncomparing the coefficients on both sides of the ODE for h yields h2 = k, h3 = 0, h4 =\n-2k2 . Hence the center manifols ODE has the form\nx c(t) = kxc(t)3 + o(xc(t)3),\nwhich means stability for k < 0 and instability for k > 0."
    },
    {
      "category": "Lecture Notes",
      "title": "lec9_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/ee864f770e61d6c29805fb2ddd45305b_lec9_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 9: Local Behavior Near Trajectories1\nThis lecture presents results which describe local behavior of ODE models in a neigbor\nhood of a given trajectory, with main attention paid to local stability of periodic solutions.\n9.1\nSmooth Dependence on Parameters\nIn this section we consider an ODE model\nx (t) = a(x(t), t, μ),\nx(t0) = x0(μ),\n(9.1)\nwhere μ is a parameter. When a and x0 are differentiable with respect to μ, the solution\nx(t) = x(t, μ) is differentiable with respect to μ as well. Moreover, the derivative of x(t, μ)\nwith respect to μ can be found by solving linear ODE with time-varying coefficients.\nTheorem 9.1 Let a : Rn × R × Rk inf! Rn be a continuous function, μ0 ≤ Rk . Let\nx0 :\n[t0, t1] inf! Rn be a solution of (9.1) with μ = μ0. Assume that a is continuously\ndifferentiable with respect to its first and third arguments on an open set X such that\n(x0(t), t, μ0) ≤ X for all t ≤ [t0, t1]. Then for all μ in a neigborhood of μ0 the ODE in (9.1)\nhas a unique solution x(t) = x(t, μ). This solution is a continuously differentiable function\nof μ, and its derivative with respect to μ at μ = μ0 equals ¢(t), where ¢ : [t0, t1] inf! Rn,k\nis the n-by-k matrix-valued solution of the ODE\n¢(t) = A(t)¢(t) + B(t), ¢(t0) = ¢0,\n(9.2)\nx inf! a(\nx at\nwhere A(t) is the derivative of the map\nx, t, μ0) with respect to\nx = x0(t), B(t)\nis the derivative of the map μ inf! a(x0(t), t, μ) at μ = μ0, and ¢0 is the derivative of the\nmap μ inf! x0(μ) at μ = μ0.\n1Version of October 10, 2003\n\nProof Existence and uniqueness of x(t, μ) and ¢(t) follow from Theorem 3.1. Hence, in\norder to prove differentiability and the formula for the derivative, it is sufficient to show\nthat there exist a function C : R+ inf! R+ such that C(r)/r ! 0 as r ! 0 and δ > 0 such\nthat\n|x(t, μ) -¢(t)(μ -μ0) -x0(t)| ≈C(|μ -μ0|)\nwhenever |μ -μ0| ≈δ. Indeed, due to continuous differentiability of a, there exist C1, δ0\nsuch that\n|a(\nx -x0(t)) -B(t)(μ -μ0)| ≈C1(|x -\nx, t, μ) -a(x0(t), t, μ0) -A(t)(\n\nx0(t)| + |μ -μ0|)\nand\n|x0(μ) -\n\nx0(μ0) -¢0(μ -μ0)| ≈C1(|μ -μ0|)\nwhenever\n|x -\n\nx0(t)| + |μ -μ0| ≈δ,\nt ≤[t0, t1].\nHence, for\n±(t) = x(t, μ) -x0(t) -¢(t)(μ -μ0)\nwe have\n|± (t)| ≈C2|±(t)| + C3(|μ -μ0|),\nas long as |±(t)| ≈δ1 and |μ -μ0| ≈δ1, where δ1 > 0 is sufficiently small. Together with\n|±(t0)| ≈C4(|μ -μ0|),\nthis implies the desired bound.\nExample 9.1 Consider the differential equation\ny (t) = 1 + μ sin(y(t)),\ny(0) = 0,\nwhere μ is a small parameter. For μ = 0, the equation can be solved explicitly: y0(t) = t.\nDifferentiating yμ(t) with respect to μ at μ = 0 yields ¢(t) satisfying\n¢(t) = sin(t), ¢(0) = 0,\ni.e. ¢(t) = 1 -cos(t). Hence\nyμ(t) = t + μ(1 -cos(t)) + O(μ 2)\nfor small μ.\n\n9.2\nStability of periodic solutions\nIn the previous lecture, we were studying stability of equilibrium solutions of differential\nequations. In this section, stability of periodic solutions of nonlinear differential equations\nis considered. Our main objective is to derive an analog of the Lyapunov's first method,\nstating that a periodic solution is asymptotically stable if system's linearization around\nthe solution is stable in a certain sense.\n9.2.1\nPeriodic solutions of time-varying ODE\nConsider system equations given in the form\nx (t) = f(x(t), t),\n(9.3)\nwhere f : Rn ×R inf! Rn is continuous. Assume that a is (π, ˆx)-periodic, in the sense that\nthere exist π > 0 and ˆx ≤Rn such that\nf(t + π, r) = f(t, r),\nf(t, r + ˆx) = f(t, r) 8 t ≤R, r ≤Rn .\n(9.4)\nNote that while the first equation in (9.4) means that f is periodic in t with a period π,\nit is possible that ˆx = 0, in which case the second equation in (9.4) does not bring any\nadditional information.\nDefinition A solution x0 : R inf! Rn of a (π, ˆ\nx)\nx)-periodic system (9.3) is called (π, ˆ\nperiodic if\nx0(t + π) = x0(t) + ˆx 8 t ≤R.\n(9.5)\nExample 9.2 According to the definition, the solution y(t) = t of the forced pendulum\nequation\ny (t) + y(t) + sin(y(t)) = 1 + sin(t)\n(9.6)\nˆ\nas a periodic one (use π = x = 21⁄4). This is reasonable, since y(t) in the pendulum\nequation represents an angle, so that shifting y by 21⁄4 does not change anything in the\nsystem equations.\nDefinition A solution x0 : [t0, →) inf! Rn of (9.3) is called stable if for every ± > 0 there\nexists δ > 0 such that\n|x(t) -x0(t)| ≈± 8 t ∀0,\n(9.7)\nwhenever x(·) is a solution of (9.3) such that |x(0) -x0(0)| < δ. x0(·) is asymptotically\nstable if it is stable and the convergence |x(t) -x0(t)| ! 0 is guaranteed as long as\n|x(0)-x0(0)| is small enough. x0(·) exponentially stable if, in addition, there exist 3⁄4, C > 0\nsuch that\n∈x(t) -x0(t)∈≈C exp(-3⁄4t)|x(0) -x0(0)| 8 t ∀0\n(9.8)\nwhenever |x(0) -x0(0)| is small enough.\n\nTo derive a stability criterion for periodic solutions x0 : R inf! Rn of (9.3), assume\ncontinuous differentiability of function f = f(\n\nx, t) with respect to the first argument x\nfor | x - x0(t)| ≈ δ, where δ > 0 is small, and differentiate the solution as a function of\ninitial conditions x(0) 1⁄4 x0(0).\nTheorem 9.2 Let f : Rn × R inf! Rn be a continuous (π, ˆx)-periodic function. Let\nx0 : R inf! Rn be a (π, ˆx)-periodic solution of (9.3). Assume that there exists δ > 0 such\n\nthat f is continuously differentiable with respect to its first argument for |x - x0(t)| < δ\nand t ≤ R. For\ndf\nA(t) =\n|x=x0(t),\n(9.9)\ndx\ndefine ¢ : [0, π] inf! Rn,n be the n-by-n matrix solution of the linear ODE\n¢(t) = A(t)¢(t), ¢(0) = I.\n(9.10)\nThen\n(a) x0(·) is exponentially stable if ¢(π) is a Schur matrix (i.e. if all eigenvalues of ¢(π)\nhave absolute value less than one);\n(b) x0(·) is not exponentially stable if ¢(π) has an eigenvalue with absolute value greater\nor equal than 1;\n(c) x0(·) is not stable if ¢(π) has an eigenvalue with absolute value greater than 1.\nThe matrix-valued function ¢ = ¢(t) is called the evolution matrix of linear system\n(9.10). The proof of Theorem 9.2 follows the same path as the proof of a similar theorem\nfor stability of equilibria, using time-varying quadratic Lyapunov functions.\n9.2.2\nStable limit cycles time-invariant ODE\nConsider system equations given in the form\nx (t) = a(x(t)),\n(9.11)\nRn\nwhere a :\n× R inf! Rn is continuous. Let ˆx ≤ Rn be such that\na(\nx) = a(\nx ≤ Rn\nx + ˆ\nx) 8\n(in particular, ˆx = 0 always satisfies this condition).\nDefinition Let π > 0. A non-constant (π, ˆx)-periodic solution x0 : R inf! Rn of system\n(9.11) is called a stable limit cycle if\n\n(a) for every δ > 0 there exists ± > 0 such that dist(x(t), x0(·)) < δ for all t ∀ 0 and all\nsolutions x = x(t) of (9.11) such that dist(x(0), x0(·)) < ±, where\nx, x0(·)) = min |\ndist(\nx - x0(t)|;\nt2R\n(b) there exists δ > 0 such that dist(x(t), x0(·)) ! 0 as t ! → for every solution of\n(9.11) such that dist(x(0), x0(·)) < ±.\nNote that a non-constant periodic solution x0 = x0(t) of time-invariant ODE equations\nis never asymptotically stable, because, as ± ! 0, the initial conditions for the solution\nx± (t) = x0(t + ±) approach the initial conditions for x0(·), but the difference x± (t) - x0(t)\ndoes not converge to 0 as t ! → unless x± ≥ x0. Therefore, the notion of a stable limit\ncycle is a relaxed version of asymptotic stability of a solution.\nTheorem 9.3 Let a : Rn inf! Rn be a continuous (ˆx)-periodic function. Let x0 : R inf! Rn\nbe a non-constant (π, ˆx)-periodic solution of (9.11). Assume that there exists δ > 0 such\nthat a is continuously differentiable on the set\nX = {\n\nx ≤ Rn : |x - x0(t)| < δ for some t ≤ R.\nLet ¢ : [0, π] inf! Rn,n be defined by (9.9),(9.10). Then\n(a) if all eigenvalues of ¢(π) except one have absolute value less than 1, x0(·) is a stable\nlimit cycle;\n(b) if one eigenvalue of ¢(π) has absolute value greater than 1, x0(·) is not a stable\nlimit cycle."
    },
    {
      "category": "Lecture Notes",
      "title": "lec10_6243_2003.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-243j-dynamics-of-nonlinear-systems-fall-2003/b7dec307a00c12d0ee95aaba56727c6d_lec10_6243_2003.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.243j (Fall 2003): DYNAMICS OF NONLINEAR SYSTEMS\nby A. Megretski\nLecture 10: Singular Perturbations and Averaging1\nThis lecture presents results which describe local behavior of parameter-dependent ODE\nmodels in cases when dependence on a parameter is not continuous in the usual sense.\n10.1\nSingularly perturbed ODE\nIn this section we consider parameter-dependent systems of equations\nx (t)\n= f(x(t), y(t), t),\n(10.1)\n2y\n= g(x(t), y(t), t),\nwhere 2 → [0, 20] is a small positive parameter. When 2 > 0, (10.1) is an ODE model.\nFor 2 = 0, (10.1) is a combination of algebraic and differential equations. Models such\nas (10.1), where y represents a set of less relevant, fast changing parameters, are fre\nquently studied in physics and mechanics. One can say that singular perturbations is the\n\"classical\" approach to dealing with uncertainty, complexity, and nonlinearity.\n10.1.1\nThe Tikhonov's Theorem\nA typical question asked about the singularly perturbed system (10.1) is whether its\nsolutions with 2 > 0 converge to the solutions of (10.1) with 2 = 0 as 2 ! 0. A suffi\ncient condition for such convergence is that the Jacobian of g with respect to its second\nargument should be a Hurwitz matrix in the region of interest.\nTheorem 10.1 Let x0 :\n[t0, t1] inf! Rn , y0 :\n[t0, t1] inf! Rm be continuous functions\nsatisfying equations\nx 0(t) = f(x0(t), y0(t), t), 0 = g(x0(t), y0(t), t),\n1Version of October 15, 2003\n\nwhere f : Rn × Rm × R inf! Rn and g : Rn × Rm × R inf! Rm are continuous functions.\nAssume that f, g are continuously differentiable with respect to their first two arguments\nin a neigborhood of the trajectory x0(t), y0(t), and that the derivative\nA(t) = g2\n0 (x0(t), y0(t), t)\nis a Hurwitz matrix for all t → [t0, t1]. Then for every t2 → (t0, t1) there exists d > 0 and\nC > 0 such that inequalities |x0(t) - x(t)| ≈ C2 for all t → [t0, t1] and |y0(t) - y(t)| ≈ C2\nfor all t → [t2, t1] for all solutions of (10.1) with |x(t0) - x0(t0)| ≈ 2, |y(t0) - y0(t0)| ≈ d,\nand 2 → (0, d).\nThe theorem was originally proven by A. Tikhonov in 1930-s. It expresses a simple\nprinciple, which suggests that, for small 2 > 0, x = x(t) can be considered a constant\nwhen predicting the behavior of y. From this viewpoint, for a given t → (t0, t1), one can\nexpect that\ny(t + 2¿) 1⁄4 y1(¿),\nwhere y1 : [0, ∀) is the solution of the \"fast motion\" ODE\ny 1(¿) = g(x0(t ), y1(¿)),\ny1(0) = y(t ).\nSince y0(t ) is an equilibrium of the ODE, and the standard linearization around this\nequilibrium yields\n± (¿) 1⁄4 A(t )±(¿)\nwhere ±(¿) = y1(¿) - y0(t ), one can expect that y1(¿) ! y0(t ) exponentially as ¿ ! ∀\nwhenever A(t ) is a Hurwitz matrix and |y(t ) - y0(t )| is small enough. Hence, when 2 > 0\nis small enough, one can expect that y(t) 1⁄4 y0(t).\n10.1.2\nProof of Theorem 10.1\nFirst, let us show that the interval [t0, t1] can be subdivided into subintervals ¢k =\n[¿k-1, ¿k ], where k → {1, 2, . . . , N} and t0 = ¿0 < ¿1 < · · · < ¿N = t1 in such a way that\nfor every k there exists a symmetric matrix Pk = Pk\n0 > 0 for which\nPk A(t) + A(t)0Pk < -I 8 t → [¿k-1, ¿k ].\nIndeed, since A(t) is a Hurwitz matrix for every t → [t0, t1], there exists P(t) = P(t)0 > 0\nsuch that\nP(t)A(t) + A(t)0P(t) < -I.\nSince A depends continuously on t, there exists an open interval ¢(t) such that t → ¢(t)\nand\nP(t)A(¿) + A(¿)0P(t) < -I 8 ¿ → ¢(t).\n\nNow the open intervals ¢(t) with t → [t0, t1] cover the whole closed bounded interval\n[t0, t1], and taking a finite number of t k , k = 1, . . . , m such that [t0, t1] is completely\ncovered by ¢(t k ) yields the desired partition subdivision of [t0, t1].\nSecond, note that, due to the continuous differentiability of f, g, for every μ > 0 there\nexist C, r > 0 such that\n\n|f(x0(t) + ±x, y0(t) + ±y , t) -f(x0(t), y0(t), t)| ≈C(|±x| + |±y |)\nand\n\n|g(x0(t) + ±x, y0(t) + ±y , t) -A(t)±y | ≈C|±x| + μ|±y |\n\nfor all t →R, ± x →Rn , ±y →Rm satisfying\n\nt →[t0, t1], |±x -x0(t)| ≈r, |±y -y0(t)| ≈r.\nFor t →¢k let\n|±y |k = (±y\n0 Pk ±y )1/2 .\nThen, for\n±x(t) = x(t) -x0(t),\n±y (t) = y(t) -y0(t),\nwe have\n|± x| ≈C1(|±x| + |±y |k ),\n2|± y |k ≈-q|±y |k + C1|±x| + 2C1\n(10.2)\nas long as ±x, ±y are sufficiently small, where C1, q are positive constants which do not\ndepend on k. Combining these two derivative bounds yields\nd (|±x| + (2C1/q)|±y |) ≈C2|±x| + 2C2\ndt\nfor some constant C2 independent of k. Hence\n|±x(¿k-1 + ¿)| ≈e C3¿ (|±x(¿k-1)| + (2C1/q)|±y (¿k-1)|) + C32\nfor ¿ →[0, ¿k -¿k-1]. With the aid of this bound for the growth of |±x|, inequality (10.2)\nyields a bound for |±y |k :\n|±y (¿k-1 + ¿)| ≈exp(-q¿/2)|±y (¿k-1)| + C4(|±x(¿k-1)| + (2C1/q)|±y (¿k-1)|) + C42,\nwhich in turn yields the result of Theorem 10.1.\n\n10.2\nAveraging\nAnother case of \"potentially discontinuous\" dependence on parameters is covered by the\nfollowing \"averaging\" result.\nTheorem 10.2 Let f : Rn × R × R inf! Rn be a continuous function which is ¿-periodic\nwith respect to its second argument t, and continuously differentiable with respect to its\nfirst argument. Let x0 → Rn be such that f( x0, t, 2) = 0 for all t, 2. For x → Rn define\nZ ¿\nf( x, 2) =\nf( x, t, 2).\n\nIf df/dx|x=0,2=0 is a Hurwitz matrix, then, for sufficiently small 2 > 0, the equilibrium\nx ≤ 0 of the system\nx (t) = 2f(x, t, 2)\n(10.3)\nis exponentially stable.\nThough the parameter dependence in Theorem 10.2 is continuous, the question asked\nis about the behavior at t = ∀, which makes system behavior for 2 = 0 not a valid\nindicator of what will occur for 2 > 0 being sufficiently small. (Indeed, for 2 = 0 the\nequilibrium x0 is not asymptotically stable.)\nTo prove Theorem 10.2, consider the function S : Rn × R inf! Rn which maps x(0), 2\nto x(¿) = S(x(0), 2), where x(·) is a solution of (10.3). It is sufficient to show that the\nx, 2) of S with respect to its first argument, evaluated at\nx0\nderivative (Jacobian) S (\nx =\nand 2 > 0 sufficiently small, is a Schur matrix. Note first that, according to the rules on\ndifferentiating with respect to initial conditions, S ( x0, 2) = ¢(¿, 2), where\nd¢(t, 2)\ndf\n= 2\n(0, t, 2)¢(t, 2), ¢(0, 2) = I.\ndt\ndx\n\nConsider D(t, 2) defined by\n\nd¢(t, 2)\ndf\n\n= 2\n(0, t, 0)¢(t, 2), ¢(0, 2) = I.\ndt\ndx\n\nLet ±(t) be the derivative of ¢(t, 2) with respect to 2 at 2 = 0. According to the rule for\ndifferentiating solutions of ODE with respect to parameters,\nZ t df\n±(t) =\n(0, t1, 0)dt1.\ndx\nHence\n\n±(¿) = df/dx|x=0,2=0\n\nis by assumption a Hurwitz matrix. On the other hand,\n\n¢(¿, 2) - ¢(¿, 2) = o(2).\nCombining this with\n¢(¿, 2) = I + ±(¿)2 + o(2)\nyields\n¢(¿, 2) = I + ±(¿)2 + o(2).\nSince ±(¿) is a Hurwitz matrix, this implies that all eigenvalues of ¢(¿, 2) have absolute\nvalue strictly less than one for all sufficiently small 2 > 0."
    }
  ]
}