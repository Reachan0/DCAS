{
  "course_name": "Introduction to Convex Optimization",
  "course_description": "This course aims to give students the tools and training to recognize convex optimization problems that arise in scientific and engineering applications, presenting the basic theory, and concentrating on modeling aspects and results that are useful in applications. Topics include convex sets, convex functions, optimization problems, least-squares, linear and quadratic programs, semidefinite programming, optimality conditions, and duality theory. Applications to signal processing, control, machine learning, finance, digital and analog circuit design, computational geometry, statistics, and mechanical engineering are presented. Students complete hands-on exercises using high-level numerical software.\nAcknowledgements\nThe course materials were developed jointly by Prof. Stephen Boyd (Stanford), who was a visiting professor at MIT when this course was taught, and Prof. Lieven Vanderberghe (UCLA).",
  "topics": [
    "Engineering",
    "Electrical Engineering",
    "Systems Engineering",
    "Systems Optimization",
    "Mathematics",
    "Applied Mathematics",
    "Engineering",
    "Electrical Engineering",
    "Systems Engineering",
    "Systems Optimization",
    "Mathematics",
    "Applied Mathematics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1 hour / session\n\nCourse Description\n\nThis course aims to give students the tools and training to recognize convex optimization problems that arise in scientific and engineering applications, presenting the basic theory, and concentrating on modeling aspects and results that are useful in applications. Topics include convex sets, convex functions, optimization problems, least-squares, linear and quadratic programs, semidefinite programming, optimality conditions, and duality theory. Applications to signal processing, control, machine learning, finance, digital and analog circuit design, computational geometry, statistics, and mechanical engineering are presented. Students complete hands-on exercises using high-level numerical software.\n\nSoftware\n\nWe'll use CVX throughout the course.\n\nCVX: Matlab Software for Disciplined Convex Programming\n\nCalendar\n\nHomework assignments were due at the end of the week noted.\n\nWEEK #\n\nKEY DATES\n\nHomework 1 due\n\nHomework 2 due\n\nHomework 3 due\n\nHomework 4 due\n\nHomework 5 due\n\nHomework 6 due\n\nMidterm exam\n\nHomework 7 due\n\nHomework 8 due\n\nHomework 9 due\n\nHomework 10 due\n\nFinal exam",
  "files": [
    {
      "category": "Resource",
      "title": "MIT6_079F09_hw03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/cefd1ed0f6029064d27a8a61bbd6bf2e_MIT6_079F09_hw03.pdf",
      "content": "6.079/6.975, Fall 2009-10\nS. Boyd & P. Parrilo\nHomework 3 additional problems\n1. Reverse Jensen inequality. Suppose f is convex, λ1 > 0, λi ≤ 0, i = 2, . . ., k, and\nλ1 + · · · + λn = 1, and let x1, . . ., xn ∈ dom f. Show that the inequality\nf(λ1x1 + · · · + λnxn) ≥ λ1f(x1) + · · · + λnf(xn)\nalways holds. Hints. Draw a picture for the n = 2 case first. For the general case,\nexpress x1 as a convex combination of λ1x1 +· · · +λnxn and x2, . . . , xn, and use Jensen's\ninequality.\n2. Reformulating constraints in cvx. Each of the following cvx code fragments describes\na convex constraint on the scalar variables x, y, and z, but violates the cvx rule set,\nand so is invalid. Briefly explain why each fragment is invalid. Then, rewrite each one\nin an equivalent form that conforms to the cvx rule set. In your reformulations, you\ncan use linear equality and inequality constraints, and inequalities constructed using\ncvx functions. You can also introduce additional variables, or use LMIs. Be sure to\nexplain (briefly) why your reformulation is equivalent to the original constraint, if it is\nnot obvious.\nCheck your reformulations by creating a small problem that includes these constraints,\nand solving it using cvx. Your test problem doesn't have to be feasible; it's enough to\nverify that cvx processes your constraints without error.\nRemark. This looks like a problem about 'how to use cvx software', or 'tricks for\nusing cvx'. But it really checks whether you understand the various composition rules,\nconvex analysis, and constraint reformulation rules.\n(a) norm([x + 2*y, x - y]) == 0\n(b) square(square(x + y)) <= x - y\n(c) 1/x + 1/y <= 1; x >= 0; y >= 0\n(d) norm([max(x,1), max(y,2)]) <= 3*x + y\n(e) x*y >= 1; x >= 0; y >= 0\n(f) (x + y)^2/sqrt(y) <= x - y + 5\n(g) x^3 + y^3 <= 1; x >= 0; y >= 0\n(h) x + z <= 1 + sqrt(x*y - z^2); x >= 0; y >= 0\n\n3. Optimal activity levels. Solve the optimal activity level problem described in exercise\n4.17 in Convex Optimization, for the instance with problem data\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\nmax\n⎢\n⎥\n⎢\n⎢ 2 ⎥\n⎥\ndisc\n⎢\n⎢ 1 ⎥\n⎥\n⎢\n⎢ 10 ⎥\n⎥\nA = ⎢ 0\n1 ⎥ ,\nc\n= ⎢ 100 ⎥ ,\np = ⎢\n⎥ ,\np\n= ⎢\n⎥ ,\nq = ⎢\n⎥ .\n⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎣\n⎦\n⎣\n⎦\n⎢ 2\n5 ⎥\n⎢ 100 ⎥\n⎣\n⎦\n⎣\n⎦\nYou can do this by forming the LP you found in your solution of exercise 4.17, or\nmore directly, using cvx. Give the optimal activity levels, the revenue generated by\neach one, and the total revenue generated by the optimal solution.\nAlso, give the\naverage price per unit for each activity level, i.e., the ratio of the revenue associated\nwith an activity, to the activity level. (These numbers should be between the basic\nand discounted prices for each activity.) Give a very brief story explaining, or at least\ncommenting on, the solution you find.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_079F09_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/770600f9ad077835a7ddb61d5204805e_MIT6_079F09_final.pdf",
      "content": "6.079/6.975\nS. Boyd & P. Parrilo\nDecember 10-11, 2009.\nFinal exam\nThis is a 24 hour take-home final exam. Please turn it in to Professor Stephen Boyd,\n(Stata Center), on Friday December 11, at 5PM (or before).\nYou may use any books, notes, or computer programs (e.g., Matlab, CVX), but you may not\ndiscuss the exam with anyone until December 11 after 5PM, after everyone has taken the\nexam. The only exception is that you can ask us for clarification, via email. Please address\nyour emails to both professors and the TA.\nPlease make a copy of your exam before handing it in.\nWhen a problem involves computation you must give all of the following: a clear discussion\nand justification of exactly what you did, the Matlab source code that produces the result,\nand the final numerical results or plots.\nMatlab files containing problem data are available on Stellar.\nAll problems have equal weight. Some are easier than they might appear at first glance.\nAnd others are harder than they might appear at first glance.\nBe sure to check your email and the course web site on Stellar often during the exam, just\nin case we need to send out an important announcement.\nAnd one technical comment. For problems that require you to work out a numerical solution,\nyou are welcome to use a solution method that involves solving more than just a single convex\noptimization problem. (Of course, only when this is necessary.)\n\nX\nX X\nX X\n1. Optimal generator dispatch. In the generator dispatch problem, we schedule the elec\ntrical output power of a set of generators over some time interval, to minimize the\ntotal cost of generation while exactly meeting the (assumed known) electrical demand.\nOne challenge in this problem is that the generators have dynamic constraints, which\ncouple their output powers over time. For example, every generator has a maximum\nrate at which its power can be increased or decreased.\nWe label the generators i = 1, . . ., n, and the time periods t = 1, . . . , T . We let pi,t\ndenote the (nonnegative) power output of generator i at time interval t. The (positive)\nelectrical demand in period t is dt. The total generated power in each period must\nequal the demand:\nn\npi,t = dt,\nt = 1, . . . , T.\ni=1\nEach generator has a minimum and maximum allowed output power:\nPi\nmin ≤ pi,t ≤ Pi\nmax ,\ni = 1, . . . , n,\nt = 1, . . ., T.\nThe cost of operating generator i at power output u is φi(u), where φi is an increasing\nstrictly convex function. (Assuming the cost is mostly fuel cost, convexity of φi says\nthat the thermal efficiency of the generator decreases as its output power increases.)\nWe will assume these cost functions are quadratic: φi(u) = αiu + βiu2, with αi and βi\npositive.\nEach generator has a maximum ramp-rate, which limits the amount its power output\ncan change over one time period:\n|pi,t+1 -pi,t| ≤Ri,\ni = 1, . . ., n,\nt = 1, . . . , T -1.\nIn addition, changing the power output of generator i from ut to ut+1 incurs an addi\ntional cost ψi(ut+1 -ut), where ψi is a convex function. (This cost can be a real one,\ndue to increased fuel use during a change of power, or a fictitious one that accounts\nfor the increased maintenance cost or decreased lifetime caused by frequent or large\nchanges in power output.) We will use the power change cost functions ψi(v) = γi|v|,\nwhere γi are positive.\nPower plants with large capacity (i.e., Pi\nmax ) are typically more efficient (i.e., have\nsmaller αi, βi), but have smaller ramp-rate limits, and higher costs associated with\nchanging power levels. Small gas-turbine plants ('peakers') are less efficient, have less\ncapacity, but their power levels can be rapidly changed.\nThe total cost of operating the generators is\nn\nT\nn\nT -1\nC =\nφi(pi,t) +\nψi(pi,t+1 -pi,t).\ni=1 t=1\ni=1 t=1\nChoosing the generator output schedules to minimize C, while respecting the con\nstraints described above, is a convex optimization problem. The problem data are dt\n\n(the demands), the generator power limits Pi\nmin and Pi\nmax , the ramp-rate limits Ri, and\nthe cost function parameters αi, βi, and γi. We will assume that problem is feasible,\nand that pi,t\n⋆ are the (unique) optimal output powers.\n(a) Price decomposition. Show that there are power prices Q1, . . . , QT for which the\nfollowing holds: For each i, pi,t\n⋆ solves the optimization problem\nPT\nPT -1\nminimize\nt=1 (φi(pi,t) -Qtpi,t) +\nt=1 ψi(pi,t+1 -pi,t)\nsubject to\nPi\nmin ≤ pi,t ≤ Pi\nmax ,\nt = 1, . . . , T\n|pi,t+1 -pi,t| ≤Ri,\nt = 1, . . . , T -1.\nThe objective here is the portion of the objective for generator i, minus the revenue\ngenerated by the sale of power at the prices Qt. Note that this problem involves\nonly generator i; it can be solved independently of the other generators (once the\nprices are known). How would you find the prices Qt?\nYou do not have to give a full formal proof; but you must explain your argument\nfully. You are welcome to use results from the text book.\n(b) Solve the generator dispatch problem with the data given in gen_dispatch_data.m,\nwhich gives (fake, but not unreasonable) demand data for 2 days, at 15 minute\nintervals. This file includes code to plot the demand, optimal generator powers,\nand prices. (You must replace these variables with their correct values.) Com\nment on anything you see in your solution that might at first seem odd. Using\nthe prices found, solve the problems in part (a) for the generators separately, to\nbe sure they give the optimal powers (up to some small numerical errors).\nRemark. While beyond the scope of this course, we mention that there are very simple\nprice update mechanisms that adjust the prices in such a way that when the generators\nindependently schedule themselves using the prices (as described above), we end up\nwith the total power generated in each period matching the demand, i.e., the opti\nmal solution of the whole (coupled) problem. This gives a decentralized method for\ngenerator dispatch.\n\n2. Internal rate of return for cash streams with a single initial investment. We use the\nnotation of example 3.34 in the textbook. Let x ∈ Rn+1 be a cash flow over n periods,\nwith x indexed from 0 to n, where the index denotes period number.\nWe assume\nthat x0 < 0, xj ≥ 0 for j = 1, . . . , n, and x0 + · · · + xn > 0.\nThis means that\nthere is an initial positive investment; thereafter, only payments are made, with the\ntotal of the payments exceeding the initial investment. (In the more general setting of\nexample 3.34, we allow additional investments to be made after the initial investment.)\n(a) Show that IRR(x) is quasilinear in this case.\n(b) Blending initial investment only streams. Use the result in part (a) to show the\nfollowing. Let x(i) ∈ Rn+1 , i = 1, . . . , k, be a set of k cash flows over n periods,\neach of which satisfies the conditions above. Let w ∈ Rk , with 1T w = 1, and\n+\nconsider the blended cash flow given by x = w1x(1) +· · ·+wkx(k). (We can think of\nthis as investing a fraction wi in cash flow i.) Show that IRR(x) ≤ maxi IRR(x(i)).\nThus, blending a set of cash flows (with initial investment only) will not improve\nthe IRR over the best individual IRR of the cash flows.\n3. Infimal convolution. Let f1, . . . , fm be convex functions on Rn . Their infimal con\nvolution, denoted g = f1 ⋄ · · · ⋄ fm (several other notations are also used), is defined\nas\ng(x) = inf{f1(x1) + · · · + fm(xm) | x1 + · · · + xm = x},\nwith the natural domain (i.e., defined by g(x) < inf). In one simple interpretation,\nfi(xi) is the cost for the ith firm to produce a mix of products given by xi; g(x) is\nthen the optimal cost obtained if the firms can freely exchange products to produce,\nall together, the mix given by x. (The name 'convolution' presumably comes from the\nobservation that if we replace the sum above with the product, and the infimum above\nwith integration, then we obtain the normal convolution.)\n(a) Show that g is convex.\n∗\n∗\n∗\n(b) Show that g = f1 + · · · + fm .\nIn other words, the conjugate of the infimal\nconvolution is the sum of the conjugates.\n(c) Verify the identity in part (b) for the specific case of two strictly convex quadratic\nfunctions, fi(x) = (1/2)xT Pix, with Pi ∈ S++\nn , i = 1, 2.\nHint: Depending on how you work out the conjugates, you might find the matrix\nidentity (X + Y )-1Y = X-1(X-1 + Y -1)-1 useful.\n\nX\n4. Robust minimum volume covering ellipsoid. Suppose z is a point in Rn and E is an\nellipsoid in Rn with center c. The Mahalanobis distance of the point to the ellipsoid\ncenter is defined as\nM(z, E) = inf{t ≥ 0 | z ∈ c + t(E -c)},\nwhich is the factor by which we need to scale the ellipsoid about its center so that z is\non its boundary. We have z ∈E if and only if M(z, E) ≤ 1. We can use (M(z, E) -1)+\nas a measure of the Mahalanobis distance of the point z to the ellipsoid E.\nNow we can describe the problem. We are given m points x1, . . . , xm ∈ Rn . The goal\nis to find the optimal trade-off between the volume of the ellipsoid E and the total\nMahalanobis distance of the points to the ellipsoid, i.e.,\nm\n(M(z, E) -1)+ .\ni=1\nNote that this can be considered a robust version of finding the smallest volume ellipsoid\nthat covers a set of points, since here we allow one or more points to be outside the\nellipsoid.\n(a) Explain how to solve this problem. You must say clearly what your variables are,\nwhat problem you solve, and why the problem is convex.\n(b) Carry out your method on the data given in rob_min_vol_ellips_data.m. Plot\nthe optimal trade-offcurve of ellipsoid volume versus total Mahalanobis distance.\nFor some selected points on the trade-off curve, plot the ellipsoid and the points\n(which are in R2). We are only interested in the region of the curve where the\nellipsoid volume is within a factor of ten (say) of the minimum volume ellipsoid\nthat covers all the points.\nImportant. Depending on how you formulate the problem, you might encounter\nproblems that are unbounded below, or where CVX encounters numerical diffi\nculty. Just avoid these by appropriate choice of parameter.\nVery important. If you use Matlab version 7.0 (which is filled with bugs) you\nmight find that functions involving determinants don't work in CVX. If you use\nthis version of Matlab, then you must download the file blkdiag.m on the course\nwebsite and put it in your Matlab path before the default version (which has a\nbug).\n\n5. Fitting a vector field to given directions. This problem concerns a vector field on Rn ,\ni.e., a function F : Rn → Rn . We are given the direction of the vector field at points\nx(1), . . ., x(N) ∈ Rn ,\nq(i) =\nF (x(i)),\ni = 1, . . . , N.\n∥F (x(i))∥2\n(These directions might be obtained, for example, from samples of trajectories of the\ndifferential equation z = F (z).) The goal is to fit these samples with a vector field of\nthe form\nFˆ = α1F1 + · · · + αmFm,\nwhere F1, . . ., Fm : Rn → Rn are given (basis) functions, and α ∈ Rm is a set of\ncoefficients that we will choose.\nWe will measure the fit using the maximum angle error,\nJ = max (q(i), Fˆ(x(i))) ,\ni=1,...,N\nwhere (z, w) = cos-1((zT w)/∥z∥2∥w∥2) denotes the angle between nonzero vectors z\nand w. We are only interested in the case when J is smaller than π/2.\n(a) Explain how to choose α so as to minimize J using convex optimization. Your\nmethod can involve solving multiple convex problems. Be sure to explain how\nyou handle the constraints Fˆ(x(i)) = 0.\n(b) Use your method to solve the problem instance with data given in vfield_fit_data.m,\nwith an affine vector field fit, i.e., Fˆ(z) = Az + b. (The matrix A and vector b\nare the parameters α above.)\nGive your answer to the nearest degree, as in\n'20* < J⋆ ≤ 21*'.\nThis file also contains code that plots the vector field directions, and also (but\ncommented out) the directions of the vector field fit, Fˆ(x(i))/∥Fˆ(x(i))∥2. Create\nthis plot, with your fitted vector field.\n\n\"\n#\n6. Efficient solution of basic portfolio optimization problem. This problem concerns the\nsimplest possible portfolio optimization problem:\nmaximize\nμT w -(λ/2)wT Σw\nsubject to\n1T w = 1,\nwith variable w ∈ Rn (the normalized portfolio, with negative entries meaning short\npositions), and data μ (mean return), Σ ∈ S++\nn\n(return covariance), and λ > 0 (the risk\naversion parameter). The return covariance has the factor form Σ = FQF T +D, where\nF ∈ Rn×k (with rank K) is the factor loading matrix, Q ∈ S++\nk\nis the factor covariance\nmatrix, and D is a diagonal matrix with positive entries, called the idiosyncratic risk\n(since it describes the risk of each asset that is independent of the factors). This form\nfor Σ is referred to as a 'k-factor risk model'. Some typical dimensions are n = 2500\n(assets) and k = 30 (factors).\n(a) What is the flop count for computing the optimal portfolio, if the low-rank plus\ndiagonal structure of Σ is not exploited? You can assume that λ = 1 (which can\nbe arranged by absorbing it into Σ).\n(b) Explain how to compute the optimal portfolio more efficiently, and give the flop\ncount for your method. You can assume that k ≪ n. You do not have to give the\nbest method; any method that has linear complexity in n is fine. You can assume\nthat λ = 1.\nHints. You may want to introduce a new variable y = F T w (which is called the\nvector of factor exposures). You may want to work with the matrix\nG =\nF\n∈ R(n+k)×(1+k) ,\n-I\ntreating it as dense, ignoring the (little) exploitable structure in it.\n(c) Carry out your method from part (b) on some randomly generated data with\ndimensions n = 2500, k = 30. For comparison (and as a check on your method),\ncompute the optimal portfolio using the method of part (a) as well. Give the\n(approximate) CPU time for each method, using tic and toc. Hints. After you\ngenerate D and Q randomly, you might want to add a positive multiple of the\nidentity to each, to avoid any issues related to poor conditioning. Also, to be able\nto invert a block diagonal matrix efficiently, you'll need to recast it as sparse.\n(d) Risk return trade-off curve. Now suppose we want to compute the optimal portfo\nlio for M values of the risk aversion parameter λ. Explain how to do this efficiently,\nand give the complexity in terms of M, n, and k. Compare to the complexity of\nusing the method of part (b) M times. Hint. Show that the optimal portfolio is\nan affine function of 1/λ.\n\nZ\nZ\nZ\n7. Optimizing the inertia matrix of a 2D mass distribution. An object has density ρ(z)\nat the point z = (x, y) ∈ R2, over some region R ⊂ R2 . Its mass m ∈ R and center of\ngravity c ∈ R2 are given by\nm =\nρ(z) dxdy,\nc =\nρ(z)z dxdy,\nR\nm R\nand its inertia matrix M ∈ R2×2 is\nM =\nρ(z)(z -c)(z -c)T dxdy.\nR\n(You do not need to know the mechanics interpretation of M to solve this problem,\nbut here it is, for those interested. Suppose we rotate the mass distribution around\na line passing through the center of gravity in the direction q ∈ R2 that lies in the\nplane where the mass distribution is, at angular rate ω. Then the total kinetic energy\nis (ω2/2)qT Mq.)\nThe goal is to choose the density ρ, subject to 0 ≤ ρ(z) ≤ ρmax for all z ∈R, and a\nfixed total mass m = mgiven, in order to maximize λmin(M).\nTo solve this problem numerically, we will discretize R into N pixels each of area a,\nwith pixel i having constant density ρi and location (say, of its center) zi ∈ R2 . We\nwill assume that the integrands above don't vary too much over the pixels, and from\nnow on use instead the expressions\nN\nN\nN\nm = a\nX\nρi,\nc = a X\nρizi,\nM = a\nX\nρi(zi -c)(zi -c)T .\nm\ni=1\ni=1\ni=1\nThe problem below refers to these discretized expressions.\n(a) Explain how to solve the problem using convex (or quasiconvex) optimization.\n(b) Carry out your method on the problem instance with data in inertia_dens_data.m.\nThis file includes code that plots a density. Give the optimal inertia matrix and\nits eigenvalues, and plot the optimal density.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/f33960a29683274ff87e352219d54c76_MIT6_079F09_lec01.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n1. Introduction\n- mathematical optimization\n- least-squares and linear programming\n- convex optimization\n- example\n- course goals and topics\n- nonlinear optimization\n- brief history of convex optimization\n1-1\n\nMathematical optimization\n(mathematical) optimization problem\nminimize\nf0(x)\nsubject to\nfi(x) ≤ bi,\ni = 1, . . . , m\n- x = (x1, . . . , xn): optimization variables\n- f0 : Rn → R: objective function\n- fi : Rn → R, i = 1, . . . , m: constraint functions\n⋆\noptimal solution x has smallest value of f0 among all vectors that\nsatisfy the constraints\nIntroduction\n1-2\n\nExamples\nportfolio optimization\n- variables: amounts invested in different assets\n- constraints: budget, max./min. investment per asset, minimum return\n- objective: overall risk or return variance\ndevice sizing in electronic circuits\n- variables: device widths and lengths\n- constraints: manufacturing limits, timing requirements, maximum area\n- objective: power consumption\ndata fitting\n- variables: model parameters\n- constraints: prior information, parameter limits\n- objective: measure of misfit or prediction error\nIntroduction\n1-3\n\nSolving optimization problems\ngeneral optimization problem\n- very difficult to solve\n- methods involve some compromise, e.g., very long computation time, or\nnot always finding the solution\nexceptions: certain problem classes can be solved efficiently and reliably\n- least-squares problems\n- linear programming problems\n- convex optimization problems\nIntroduction\n1-4\n\nLeast-squares\nminimize\nkAx -bk2\nsolving least-squares problems\n- analytical solution: x ⋆ = (ATA)-1ATb\n- reliable and efficient algorithms and software\n- computation time proportional to n2k (A ∈ Rk×n); less if structured\n- a mature technology\nusing least-squares\n- least-squares problems are easy to recognize\n- a few standard techniques increase flexibility (e.g., including weights,\nadding regularization terms)\nIntroduction\n1-5\n\nLinear programming\nminimize\ncTx\nsubject to\nai\nTx ≤ bi,\ni = 1, . . . , m\nsolving linear programs\n- no analytical formula for solution\n- reliable and efficient algorithms and software\n- computation time proportional to n2m if m ≥ n; less with structure\n- a mature technology\nusing linear programming\n- not as easy to recognize as least-squares problems\n- a few standard tricks used to convert problems into linear programs\n(e.g., problems involving l1 - or linf-norms, piecewise-linear functions)\nIntroduction\n1-6\n\nConvex optimization problem\nminimize\nf0(x)\nsubject to\nfi(x) ≤ bi,\ni = 1, . . . , m\n- objective and constraint functions are convex:\nfi(αx + βy) ≤ αfi(x) + βfi(y)\nif α + β = 1, α ≥ 0, β ≥ 0\n- includes least-squares problems and linear programs as special cases\nIntroduction\n1-7\n\nsolving convex optimization problems\n- no analytical solution\n- reliable and efficient algorithms\n- computation time (roughly) proportional to max{n3, n2m, F}, where F\nis cost of evaluating fi's and their first and second derivatives\n- almost a technology\nusing convex optimization\n- often difficult to recognize\n- many tricks for transforming problems into convex form\n- surprisingly many problems can be solved via convex optimization\nIntroduction\n1-8\n\nExample\nm lamps illuminating n (small, flat) patches\nintensity Ik at patch k depends linearly on lamp powers pj:\nm\n-2\nIk =\nX\nakjpj,\nakj = rkj max{cos θkj, 0}\nj=1\nproblem: achieve desired illumination Ides with bounded lamp powers\nminimize\nmaxk=1,...,n | log Ik -log Ides|\nsubject to\n0 ≤ pj ≤ pmax,\nj = 1, . . . , m\nlamp power pj\nillumination Ik\nrkj\nθkj\nIntroduction\n1-9\n\nhow to solve?\n1. use uniform power: pj = p, vary p\n2. use least-squares:\nminimize\nP\nk\nn\n=1(Ik -Ides)2\nround pj if pj > pmax or pj < 0\n3. use weighted least-squares:\nminimize\nPn\nk=1(Ik -Ides)2 + Pm\nj=1 wj(pj -pmax/2)2\niteratively adjust weights wj until 0 ≤ pj ≤ pmax\n4. use linear programming:\nminimize\nmaxk=1,...,n |Ik -Ides|\nsubject to\n0 ≤ pj ≤ pmax,\nj = 1, . . . , m\nwhich can be solved via linear programming\nof course these are approximate (suboptimal) 'solutions'\nIntroduction\n1-10\n\n5. use convex optimization: problem is equivalent to\nminimize\nf0(p) = maxk=1,...,n h(Ik/Ides)\nsubject to\n0 ≤ pj ≤ pmax,\nj = 1, . . . , m\nwith h(u) = max{u, 1/u}\nh(u)\n0 0\nu\nf0 is convex because maximum of convex functions is convex\nexact solution obtained with effort ≈ modest factor × least-squares effort\nIntroduction\n1-11\n\nadditional constraints: does adding 1 or 2 below complicate the problem?\n1. no more than half of total power is in any 10 lamps\n2. no more than half of the lamps are on (pj > 0)\n- answer: with (1), still easy to solve; with (2), extremely difficult\n- moral: (untrained) intuition doesn't always work; without the proper\nbackground very easy problems can appear quite similar to very difficult\nproblems\nIntroduction\n1-12\n\nCourse goals and topics\ngoals\n1. recognize/formulate problems (such as the illumination problem) as\nconvex optimization problems\n2. develop code for problems of moderate size (1000 lamps, 5000 patches)\n3. characterize optimal solution (optimal power distribution), give limits of\nperformance, etc.\ntopics\n1. convex sets, functions, optimization problems\n2. examples and applications\n3. algorithms\nIntroduction\n1-13\n\nNonlinear optimization\ntraditional techniques for general nonconvex problems involve compromises\nlocal optimization methods (nonlinear programming)\n- find a point that minimizes f0 among feasible points near it\n- fast, can handle large problems\n- require initial guess\n- provide no information about distance to (global) optimum\nglobal optimization methods\n- find the (global) solution\n- worst-case complexity grows exponentially with problem size\nthese algorithms are often based on solving convex subproblems\nIntroduction\n1-14\n\nBrief history of convex optimization\ntheory (convex analysis): ca1900-1970\nalgorithms\n- 1947: simplex algorithm for linear programming (Dantzig)\n- 1960s: early interior-point methods (Fiacco & McCormick, Dikin, . . . )\n- 1970s: ellipsoid method and other subgradient methods\n- 1980s: polynomial-time interior-point methods for linear programming\n(Karmarkar 1984)\n- late 1980s-now: polynomial-time interior-point methods for nonlinear\nconvex optimization (Nesterov & Nemirovski 1994)\napplications\n- before 1990: mostly in operations research; few in engineering\n- since 1990: many new applications in engineering (control, signal\nprocessing, communications, circuit design, . . . ); new problem classes\n(semidefinite and second-order cone programming, robust optimization)\nIntroduction\n1-15\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/26c4c530c9db63a12b898d720dd89a44_MIT6_079F09_lec02.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n2. Convex sets\n- affine and convex sets\n- some important examples\n- operations that preserve convexity\n- generalized inequalities\n- separating and supporting hyperplanes\n- dual cones and generalized inequalities\n2-1\n\nAffine set\nline through x1, x2: all points\nx = θx1 + (1 -θ)x2\n(θ ∈ R)\nθ = 1.2\nx1\nθ = 1\nθ = 0.6\nx2\nθ = 0\nθ = -0.2\naffine set: contains the line through any two distinct points in the set\nexample: solution set of linear equations {x | Ax = b}\n(conversely, every affine set can be expressed as solution set of system of\nlinear equations)\nConvex sets\n2-2\n\nConvex set\nline segment between x1 and x2: all points\nx = θx1 + (1 -θ)x2\nwith 0 ≤ θ ≤ 1\nconvex set: contains line segment between any two points in the set\nx1, x2 ∈ C,\n0 ≤ θ ≤ 1\n=⇒\nθx1 + (1 -θ)x2 ∈ C\nexamples (one convex, two nonconvex sets)\nConvex sets\n2-3\n\nConvex combination and convex hull\nconvex combination of x1,. . . , xk: any point x of the form\nx = θ1x1 + θ2x2 + · · · + θkxk\nwith θ1 + · · · + θk = 1, θi ≥ 0\nconvex hull conv S: set of all convex combinations of points in S\nConvex sets\n2-4\n\nConvex cone\nconic (nonnegative) combination of x1 and x2: any point of the form\nx = θ1x1 + θ2x2\nwith θ1 ≥ 0, θ2 ≥ 0\nx1\nx2\nconvex cone: set that contains all conic combinations of points in the set\nConvex sets\n2-5\n\nHyperplanes and halfspaces\nhyperplane: set of the form {x | aTx = b} (a = 0\n)\na\nx0\nx\naTx = b\nhalfspace: set of the form {x | aT\nx ≤ b} (a = 0)\na\na T x ≥ b\na T x ≤ b\nx0\n- a is the normal vector\n- hyperplanes are affine and convex; halfspaces are convex\nConvex sets\n2-6\n\nEuclidean balls and ellipsoids\n(Euclidean) ball with center xc and radius r:\nB(xc, r) = {x | kx -xck2 ≤ r} = {xc + ru | kuk2 ≤ 1}\nellipsoid: set of the form\n{x | (x -xc)TP -1(x -xc) ≤ 1}\nwith P ∈ Sn\n(i.e., P symmetric positive definite)\n++\nxc\nother representation: {xc + Au | kuk2 ≤ 1} with A square and nonsingular\nConvex sets\n2-7\n\nNorm balls and norm cones\nnorm: a function k · k that satisfies\n- kxk ≥ 0; kxk = 0 if and only if x = 0\n- ktxk = |t| kxk for t ∈ R\n- kx + yk ≤kxk + kyk\nnotation: k · k is general (unspecified) norm; k · ksymb is particular norm\nnorm ball with center xc and radius r: {x | kx -xck ≤ r}\nnorm cone: {(x, t) | kxk ≤ t}\n0.5\nEuclidean norm cone is called second-\norder cone\nx2 -1 -1 x1\nt\nnorm balls and cones are convex\nConvex sets\n2-8\n\nPolyhedra\nsolution set of finitely many linear inequalities and equalities\nAx b,\nCx = d\n(A ∈ Rm×n , C ∈ Rp×n , is componentwise inequality)\na1\na5\na2\na4\nP\na3\npolyhedron is intersection of finite number of halfspaces and hyperplanes\nConvex sets\n2-9\n\nPositive semidefinite cone\nnotation:\n- Sn is set of symmetric n × n matrices\n- S+\nn = {X ∈ Sn | X 0}: positive semidefinite n × n matrices\nX ∈ Sn\n⇐⇒\nz TXz ≥ 0 for all z\n+\nSn is a convex cone\n+\n- S++\nn\n= {X ∈ Sn | X ≻ 0}: positive definite n × n matrices\nexample:\nx\ny\n∈ S2\n0.5\n+\ny\nz\nz\n0.5\ny\n-1 0\nx\nConvex sets\n2-10\n\nOperations that preserve convexity\npractical methods for establishing convexity of a set C\n1. apply definition\nx1, x2 ∈ C,\n0 ≤ θ ≤ 1\n=⇒\nθx1 + (1 -θ)x2 ∈ C\n2. show that C is obtained from simple convex sets (hyperplanes,\nhalfspaces, norm balls, . . . ) by operations that preserve convexity\n- intersection\n- affine functions\n- perspective function\n- linear-fractional functions\nConvex sets\n2-11\n\nIntersection\nthe intersection of (any number of) convex sets is convex\nexample:\nS = {x ∈ Rm | |p(t)| ≤1 for |t| ≤π/3}\nwhere p(t) = x1 cos t + x2 cos 2t + · · · + xm cos mt\nfor m = 2:\nS\n-2\n-1\nπ/3\n2π/3\nπ\nx2\np(t)\n-1\n-1\n-2\nt\nx1\nConvex sets\n2-12\n\nAffine function\nsuppose f : Rn → Rm is affine (f(x) = Ax + b with A ∈ Rm×n , b ∈ Rm)\n- the image of a convex set under f is convex\nS ⊆ Rn convex\n=⇒\nf(S) = {f(x) | x ∈ S} convex\n- the inverse image f -1(C) of a convex set under f is convex\nC ⊆ Rm convex\n=⇒\nf -1(C) = {x ∈ Rn | f(x) ∈ C} convex\nexamples\n- scaling, translation, projection\n- solution set of linear matrix inequality {x | x1A1 + · · · + xmAm B}\n(with Ai, B ∈ Sp)\n- hyperbolic cone {x | xTPx ≤ (cTx)2, cTx ≥ 0} (with P ∈ Sn )\n+\nConvex sets\n2-13\n\nPerspective and linear-fractional function\nperspective function P : Rn+1 → Rn:\nP(x, t) = x/t,\ndom P = {(x, t) | t > 0}\nimages and inverse images of convex sets under perspective are convex\nlinear-fractional function f : Rn → Rm:\nf(x) = Ax + b ,\ndom f = {x | c T x + d > 0}\ncTx + d\nimages and inverse images of convex sets under linear-fractional functions\nare convex\nConvex sets\n2-14\n\nexample of a linear-fractional function\nf(x) =\nx\nx1 + x2 + 1\nf(C)\nC\nx2\nx2\n-1\n-1\n-1\n-1\nx1\nx1\nConvex sets\n2-15\n\nGeneralized inequalities\na convex cone K ⊆ Rn is a proper cone if\n- K is closed (contains its boundary)\n- K is solid (has nonempty interior)\n- K is pointed (contains no line)\nexamples\n- nonnegative orthant K = Rn = {x ∈ Rn | xi ≥ 0, i = 1, . . . , n}\n+\n- positive semidefinite cone K = Sn\n+\n- nonnegative polynomials on [0, 1]:\nK = {x ∈ Rn | x1 + x2t + x3t2 + · · · + xntn-1 ≥ 0 for t ∈ [0, 1]}\nConvex sets\n2-16\n\ngeneralized inequality defined by a proper cone K:\nx K y\n⇐⇒\ny -x ∈ K,\nx ≺K y\n⇐⇒\ny -x ∈ int K\nexamples\n= Rn\n+\n- componentwise inequality (K\n)\nx Rn\n- matrix inequality (K = Sn\n+\n+ y\n⇐⇒\nxi ≤ yi,\ni = 1, . . . , n\n)\nX Sn\nthese two types are so common that we drop the subscript in K\nproperties: many properties of K are similar to ≤ on R, e.g.,\nx K y,\nu K v\n=⇒\nx + u K y + v\n+ Y\n⇐⇒\nY -X positive semidefinite\nConvex sets\n2-17\n\nMinimum and minimal elements\nK is not in general a linear ordering: we can have x 6K y and y 6K x\nx ∈ S is the minimum element of S with respect to K if\ny ∈ S\n=⇒\nx K y\nx ∈ S is a minimal element of S with respect to K if\ny ∈ S,\ny K x\n=⇒\ny = x\nexample (K = R+\n2 )\nx1 is the minimum element of S1\nx2 is a minimal element of S2\nx1\nx2\nS1\nS2\nConvex sets\n2-18\n\nSeparating hyperplane theorem\nif C and D are disjoint convex sets, then there exists a 6= 0, b such that\na T x ≤ b for x ∈ C,\na T x ≥ b for x ∈ D\naTx ≥ b\naTx ≤ b\na\nD\nC\nthe hyperplane {x | aTx = b} separates C and D\nstrict separation requires additional assumptions (e.g., C is closed, D is a\nsingleton)\nConvex sets\n2-19\n\nSupporting hyperplane theorem\nsupporting hyperplane to set C at boundary point x0:\n{x | a T x = a T x0}\nwhere a 6\nTx ≤\nTx0 for all x ∈ C\n= 0 and a\na\nC\na\nx0\nsupporting hyperplane theorem: if C is convex, then there exists a\nsupporting hyperplane at every boundary point of C\nConvex sets\n2-20\n\nDual cones and generalized inequalities\ndual cone of a cone K:\nK ∗ = {y | y T x ≥ 0 for all x ∈ K}\nexamples\n- K = Rn : K∗ = Rn\n+\n+\n- K = Sn : K∗ = Sn\n+\n+\n- K = {(x, t) | kxk2 ≤ t}: K∗ = {(x, t) | kxk2 ≤ t}\n- K = {(x, t) | kxk1 ≤ t}: K∗ = {(x, t) | kxkinf ≤ t}\nfirst three examples are self-dual cones\ndual cones of proper cones are proper, hence define generalized inequalities:\ny K∗ 0\n⇐⇒\ny T x ≥ 0 for all x K 0\nConvex sets\n2-21\n\nMinimum and minimal elements via dual inequalities\nminimum element w.r.t. K\nx is minimum element of S iff for all\nλ ≻K∗ 0, x is the unique minimizer\nof λTz over S\nminimal element w.r.t. K\n- if x minimizes λTz over S for some λ ≻K∗ 0, then x is minimal\nx\nS\nS\nx1\nx2\nλ1\nλ2\n- if x is a minimal element of a convex set S, then there exists a nonzero\nλ K∗ 0 such that x minimizes λTz over S\nConvex sets\n2-22\n\noptimal production frontier\n- different production methods use different amounts of resources x ∈ Rn\n- production set P: resource vectors x for all possible production methods\n- efficient (Pareto optimal) methods correspond to resource vectors x\nthat are minimal w.r.t. Rn\n+\nfuel\nexample (n = 2)\nx1, x2, x3 are efficient; x4, x5 are not\nx3\nx2 x5 x4\nx1\nλ\nP\nlabor\nConvex sets\n2-23\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/2ae23d35685ff402473b36011138149a_MIT6_079F09_lec03.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n3. Convex functions\n- basic properties and examples\n- operations that preserve convexity\n- the conjugate function\n- quasiconvex functions\n- log-concave and log-convex functions\n- convexity with respect to generalized inequalities\n3-1\n\nDefinition\nf : Rn\nR is convex if dom f is a convex set and\n→\nf(θx + (1 -θ)y) ≤θf(x) + (1 -θ)f(y)\nfor all x, y ∈dom f, 0 ≤θ ≤1\n(x, f(x))\n(y, f(y))\n- f is concave if -f is convex\n- f is strictly convex if dom f is convex and\nf(θx + (1 -θ)y) < θf(x) + (1 -θ)f(y)\nfor x, y ∈dom f, x =6\ny, 0 < θ < 1\nConvex functions\n3-2\n\nExamples on R\nconvex:\n- affine: ax + b on R, for any a, b ∈R\n- exponential: eax, for any a ∈R\npowers: xα on R++, for α ≥1 or α ≤0\n-\n- powers of absolute value: |x|p on R, for p ≥1\n- negative entropy: x log x on R++\nconcave:\n- affine: ax + b on R, for any a, b ∈R\n- powers: xα on R++, for 0 ≤α ≤1\n- logarithm: log x on R++\nConvex functions\n3-3\n\nX X\nExamples on Rn and Rm×n\naffine functions are convex and concave; all norms are convex\nexamples on Rn\naffine function f(x) = aTx + b\n-\n- norms: kxkp = ( Pn\n|xi|p)1/p for p ≥1; kxkinf = maxk |xk|\ni=1\nexamples on Rm×n (m\nn matrices)\n×\naffine function\n-\nm\nn\nf(X) = tr(ATX) + b =\nAijXij + b\ni=1 j=1\n- spectral (maximum singular value) norm\nf(X) = kXk2 = σmax(X) = (λmax(XTX))1/2\nConvex functions\n3-4\n\nX\nRestriction of a convex function to a line\nf : Rn\nR is convex if and only if the function g : R\nR,\n→\n→\ng(t) = f(x + tv),\ndom g = {t | x + tv ∈dom f}\nis convex (in t) for any x ∈dom f, v ∈Rn\ncan check convexity of f by checking convexity of functions of one variable\nexample. f : Sn\nR with f(X) = log det X, dom f = S++\nn\n→\ng(t) = log det(X + tV )\n=\nlog det X + log det(I + tX-1/2V X-1/2)\nn\n=\nlog det X +\nlog(1 + tλi)\ni=1\nwhere λi are the eigenvalues of X-1/2V X-1/2\ng is concave in t (for any choice of X ≻0, V ); hence f is concave\nConvex functions\n3-5\n\nExtended-value extension\nextended-value extension f of f is\nf (x) = f(x),\ndom f,\nf (x) =\ndom f\nx ∈\ninf,\nx 6∈\noften simplifies notation; for example, the condition\n0 ≤θ ≤ 1\n=\nf (θx + (1 -θ)y) ≤θf (x) + (1 -θ)f (y)\n⇒\n(as an inequality in R ∪{inf}), means the same as the two conditions\ndom f is convex\n-\n- for x, y ∈dom f,\n0 ≤θ ≤1\n= ⇒\nf(θx + (1 -θ)y) ≤θf(x) + (1 -θ)f(y)\nConvex functions\n3-6\n\nFirst-order condition\nf is differentiable if dom f is open and the gradient\n∂f(x) ∂f(x)\n∂f(x)\n=\n,\n, . . . ,\n∇f(x)\n∂x1\n∂x2\n∂xn\nexists at each x ∈dom f\n1st-order condition: differentiable f with convex domain is convex iff\nf(y) ≥f(x) + ∇f(x)T(y -x)\nfor all x, y ∈dom f\n(x, f(x))\nf(y)\nf(x) + ∇f(x)T(y -x)\nfirst-order approximation of f is global underestimator\nConvex functions\n3-7\n\nSecond-order conditions\nf is twice differentiable if dom f is open and the Hessian ∇2f(x) ∈Sn ,\n2f(x)ij = ∂2f(x) ,\ni, j = 1, . . . , n,\n∇\n∂xi∂xj\nexists at each x ∈dom f\n2nd-order conditions: for twice differentiable f with convex domain\n- f is convex if and only if\n∇ 2f(x) 0\nfor all x ∈dom f\n- if ∇2f(x) ≻0 for all x ∈dom f, then f is strictly convex\nConvex functions\n3-8\n\nExamples\nquadratic function: f(x) = (1/2)xTPx + qTx + r (with P ∈Sn)\n∇f(x) = Px + q,\n∇ 2f(x) = P\nconvex if P 0\nleast-squares objective: f(x) = kAx -bk2\n∇f(x) = 2AT(Ax -b),\n∇ 2f(x) = 2ATA\nconvex (for any A)\nquadratic-over-linear: f(x, y) = x2/y\n\nT\n∇ 2f(x, y) = y\n-\ny\nx\n-\ny\nx\nconvex for y > 0\ny\n0 -2\nx\nf(x, y)\nConvex functions\n3-9\n\nlog-sum-exp: f(x) = log P\nk\nn\n=1 exp xk is convex\n∇ 2f(x) = 1T\nz diag(z) -(1T\nz)2zz T\n(zk = exp xk)\nto show ∇2f(x) 0, we must verify that vT∇2f(x)v ≥0 for all v:\nT\n( P\nk zkvk\n2)( P\nk zk) -( P\nk vkzk)2\nv ∇ 2f(x)v =\n( P\nk zk)2\n≥0\nsince ( P\nk vkzk)2 ≤( P\nk zkvk\n2)( P\nk zk) (from Cauchy-Schwarz inequality)\ngeometric mean: f(x) = ( Q\nk\nn\n=1 xk)1/n on Rn\nis concave\n++\n(similar proof as for log-sum-exp)\nConvex functions\n3-10\n\nEpigraph and sublevel set\nα-sublevel set of f : Rn\nR:\n→\nCα = {x ∈dom f | f(x) ≤α}\nsublevel sets of convex functions are convex (converse is false)\nepigraph of f : Rn\nR:\n→\nepi f = {(x, t) ∈Rn+1 | x ∈dom f, f(x) ≤t}\nepi f\nf\nf is convex if and only if epi f is a convex set\nConvex functions\n3-11\n\nJensen's inequality\nbasic inequality: if f is convex, then for 0 ≤θ ≤1,\nf(θx + (1 -θ)y) ≤θf(x) + (1 -θ)f(y)\nextension: if f is convex, then\nf(E z) ≤E f(z)\nfor any random variable z\nbasic inequality is special case with discrete distribution\nprob(z = x) = θ,\nprob(z = y) = 1 -θ\nConvex functions\n3-12\n\nOperations that preserve convexity\npractical methods for establishing convexity of a function\n1. verify definition (often simplified by restricting to a line)\n2. for twice differentiable functions, show ∇2f(x) 0\n3. show that f is obtained from simple convex functions by operations\nthat preserve convexity\n- nonnegative weighted sum\n- composition with affine function\n- pointwise maximum and supremum\n- composition\nminimization\n-\n- perspective\nConvex functions\n3-13\n\nX\nPositive weighted sum & composition with affine function\nnonnegative multiple: αf is convex if f is convex, α ≥0\nsum: f1 + f2 convex if f1, f2 convex (extends to infinite sums, integrals)\ncomposition with affine function: f(Ax + b) is convex if f is convex\nexamples\n- log barrier for linear inequalities\nm\nf(x) = -\nlog(bi -a T\ni x),\ndom f = {x | ai\nTx < bi, i = 1, . . . , m}\ni=1\n- (any) norm of affine function: f(x) = kAx + bk\nConvex functions\n3-14\n\nPointwise maximum\nif f1, . . . , fm are convex, then f(x) = max{f1(x), . . . , fm(x)} is convex\nexamples\npiecewise-linear function: f(x) = maxi=1,...,m(ai\nTx + bi) is convex\n-\n- sum of r largest components of x ∈Rn:\nf(x) = x[1] + x[2] +\n+ x[r]\n· · ·\nis convex (x[i] is ith largest component of x)\nproof:\nf(x) = max{xi1 + xi2 + · · · + xir | 1 ≤i1 < i2 < · · · < ir ≤n}\nConvex functions\n3-15\n\nPointwise supremum\nif f(x, y) is convex in x for each y ∈A, then\ng(x) = sup f(x, y)\ny∈A\nis convex\nexamples\n- support function of a set C: SC(x) = supy∈C yTx is convex\n- distance to farthest point in a set C:\nf(x) = sup\ny∈C\nkx -yk\n- maximum eigenvalue of symmetric matrix: for X ∈Sn ,\nλmax(X) =\nsup y TXy\n∥y∥2=1\nConvex functions\n3-16\n\nComposition with scalar functions\ncomposition of g : Rn\nR and h : R\nR:\n→\n→\nf(x) = h(g(x))\ng convex, h convex, h nondecreasing\nf is convex if\ng concave, h convex, h nonincreasing\n- proof (for n = 1, differentiable g, h)\nf ′′ (x) = h ′′ (g(x))g ′ (x)2 + h ′ (g(x))g ′′ (x)\nnote: monotonicity must hold for extended-value extension h\n-\nexamples\n- exp g(x) is convex if g is convex\n- 1/g(x) is convex if g is concave and positive\nConvex functions\n3-17\n\nP\nP\nVector composition\ncomposition of g : Rn\nRk and h : Rk\nR:\n→\n→\nf(x) = h(g(x)) = h(g1(x), g2(x), . . . , gk(x))\ngi convex, h convex, h nondecreasing in each argument\nf is convex if\ngi concave, h convex, h nonincreasing in each argument\nproof (for n = 1, differentiable g, h)\nf ′′ (x) = g ′ (x)T ∇ 2h(g(x))g ′ (x) + ∇h(g(x))T g ′′ (x)\nexamples\nm log gi(x) is concave if gi are concave and positive\n-\ni=1\nm\n- log\ni=1 exp gi(x) is convex if gi are convex\nConvex functions\n3-18\n\nMinimization\nif f(x, y) is convex in (x, y) and C is a convex set, then\ng(x) = inf f(x, y)\ny∈C\nis convex\nexamples\nf(x, y) = xTAx + 2xTBy + yTCy with\n-\nA\nB\nBT\nC\n0,\nC ≻0\nminimizing over y gives g(x) = infy f(x, y) = xT(A -BC-1BT)x\ng is convex, hence Schur complement A -BC-1BT 0\n- distance to a set: dist(x, S) = infy∈S kx -yk is convex if S is convex\nConvex functions\n3-19\n\nPerspective\nthe perspective of a function f : Rn\nR is the function g : Rn × R\nR,\n→\n→\ng(x, t) = tf(x/t),\ndom g = {(x, t) | x/t ∈dom f, t > 0}\ng is convex if f is convex\nexamples\nf(x) = xTx is convex; hence g(x, t) = xTx/t is convex for t > 0\n-\n- negative logarithm f(x) = -log x is convex; hence relative entropy\ng(x, t) = t log t -t log x is convex on R2\n++\n- if f is convex, then\ng(x) = (c T x + d)f\n\n(Ax + b)/(c T x + d)\n\nis convex on {x | cTx + d > 0, (Ax + b)/(cTx + d) ∈dom f}\nConvex functions\n3-20\n\nThe conjugate function\nthe conjugate of a function f is\nf ∗ (y) =\nsup (y T x -f(x))\nx∈dom f\nf(x)\n(0, -f ∗ (y))\nxy\nx\nf ∗ is convex (even if f is not)\n-\n- will be useful in chapter 5\nConvex functions\n3-21\n\nexamples\n- negative logarithm f(x) = -log x\nf ∗ (y)\n=\nsup (xy + log x)\nx>0\n-1 -log(-y)\ny < 0\n=\ninf\notherwise\nstrictly convex quadratic f(x) = (1/2)xTQx with Q ∈Sn\n-\n++\nf ∗ (y)\n=\nsup (y T x -(1/2)x TQx)\nx\n=\ny TQ-1 y\nConvex functions\n3-22\n\nQuasiconvex functions\nf : Rn\nR is quasiconvex if dom f is convex and the sublevel sets\n→\nSα = {x ∈dom f | f(x) ≤α}\nare convex for all α\nα\nβ\na\nb\nc\n- f is quasiconcave if -f is quasiconvex\n- f is quasilinear if it is quasiconvex and quasiconcave\nConvex functions\n3-23\n\np\nExamples\n-\n|x| is quasiconvex on R\n- ceil(x) = inf{z ∈Z | z ≥x} is quasilinear\n- log x is quasilinear on R++\nf(x1, x2) = x1x2 is quasiconcave on R2\n-\n++\nlinear-fractional function\n-\naTx + b\nf(x) = cTx + d,\ndom f = {x | c T x + d > 0}\nis quasilinear\ndistance ratio\n-\nf(x) = k\nk\nx\nx\n-\n-\na\nbk\nk\n2 ,\ndom f = {x | kx -ak2 ≤kx -bk2}\nis quasiconvex\nConvex functions\n3-24\n\nX\nX\ninternal rate of return\n- cash flow x = (x0, . . . , xn); xi is payment in period i (to us if xi > 0)\n- we assume x0 < 0 and x0 + x1 + · · · + xn > 0\n- present value of cash flow x, for interest rate r:\nn\nPV(x, r) =\n(1 + r)-i xi\ni=0\n- internal rate of return is smallest interest rate for which PV(x, r) = 0:\nIRR(x) = inf{r ≥0 | PV(x, r) = 0}\nIRR is quasiconcave: superlevel set is intersection of halfspaces\nn\nIRR(x) ≥R\n(1 + r)-i xi ≥0 for 0 ≤r ≤R\n⇐⇒\ni=0\nConvex functions\n3-25\n\nProperties\nmodified Jensen inequality: for quasiconvex f\n0 ≤θ ≤1\n= ⇒\nf(θx + (1 -θ)y) ≤max{f(x), f(y)}\nfirst-order condition: differentiable f with cvx domain is quasiconvex iff\n=\nf(y) ≤f(x)\n⇒\n∇f(x)T(y -x) ≤0\nx\n∇f(x)\nsums of quasiconvex functions are not necessarily quasiconvex\nConvex functions\n3-26\n\nZ\nLog-concave and log-convex functions\na positive function f is log-concave if log f is concave:\nf(θx + (1 -θ)y) ≥f(x)θf(y)1-θ\nfor 0 ≤θ ≤1\nf is log-convex if log f is convex\n- powers: xa on R++ is log-convex for a ≤0, log-concave for a ≥0\n- many common probability densities are log-concave, e.g., normal:\n-1(x-x )T Σ-1(x-x )\nf(x) = p\ne\n(2π)n det Σ\n- cumulative Gaussian distribution function Φ is log-concave\nΦ(x) =\nx\ne -u 2/2 du\n√\n2π\n-inf\nConvex functions\n3-27\n\nZ\nProperties of log-concave functions\n- twice differentiable f with convex domain is log-concave if and only if\nf(x)∇ 2f(x) ∇f(x)∇f(x)T\nfor all x ∈dom f\n- product of log-concave functions is log-concave\n- sum of log-concave functions is not always log-concave\n- integration: if f : Rn × Rm →R is log-concave, then\ng(x) =\nf(x, y) dy\nis log-concave (not easy to show)\nConvex functions\n3-28\n\nZ\nZ\n\nconsequences of integration property\n- convolution f ∗g of log-concave functions f, g is log-concave\n(f ∗g)(x) =\nf(x -y)g(y)dy\n- if C ⊆Rn convex and y is a random variable with log-concave pdf then\nf(x) = prob(x + y ∈C)\nis log-concave\nproof: write f(x) as integral of product of log-concave functions\nf(x) =\ng(x + y)p(y) dy,\ng(u) =\nu ∈C\nu 6∈ C,\np is pdf of y\nConvex functions\n3-29\n\nexample: yield function\nY (x) = prob(x + w ∈S)\n- x ∈Rn: nominal parameter values for product\n- w ∈Rn: random variations of parameters in manufactured product\n- S: set of acceptable values\nif S is convex and w has a log-concave pdf, then\n- Y is log-concave\n- yield regions {x | Y (x) ≥α} are convex\nConvex functions\n3-30\n\nConvexity with respect to generalized inequalities\nf : Rn\nRm is K-convex if dom f is convex and\n→\nf(θx + (1 -θ)y) K θf(x) + (1 -θ)f(y)\nfor x, y ∈dom f, 0 ≤θ ≤1\nexample f : Sm\nSm , f(X) = X2 is Sm -convex\n→\n+\nproof: for fixed z ∈Rm , zTX2z = kXzk2\n2 is convex in X, i.e.,\nz T(θX + (1 -θ)Y )2 z ≤θzTX2 z + (1 -θ)z TY 2 z\nfor X, Y ∈Sm , 0 ≤θ ≤1\ntherefore (θX + (1 -θ)Y )2 θX2 + (1 -θ)Y 2\nConvex functions\n3-31\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/ff36aa8c82776e1ac51115ea32d3a0f7_MIT6_079F09_lec04.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n4. Convex optimization problems\n- optimization problem in standard form\n- convex optimization problems\n- quasiconvex optimization\n- linear optimization\n- quadratic optimization\n- geometric programming\n- generalized inequality constraints\n- semidefinite programming\n- vector optimization\n4-1\n\nOptimization problem in standard form\nminimize\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\n- x ∈Rn is the optimization variable\nf0 : Rn\nR is the objective or cost function\n-\n→\nfi : Rn\nR, i = 1, . . . , m, are the inequality constraint functions\n-\n→\nhi : Rn\nR are the equality constraint functions\n-\n→\noptimal value:\n⋆\np = inf{f0(x) | fi(x) ≤0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p}\n⋆\n- p = infif problem is infeasible (no x satisfies the constraints)\n⋆\n- p = -inf if problem is unbounded below\nConvex optimization problems\n4-2\n\nOptimal and locally optimal points\nx is feasible if x ∈dom f0 and it satisfies the constraints\na feasible x is optimal if f0(x) = p ⋆ ; Xopt is the set of optimal points\nx is locally optimal if there is an R > 0 such that x is optimal for\nminimize (over z)\nf0(z)\nsubject to\nfi(z) ≤0,\ni = 1, . . . , m,\nhi(z) = 0,\ni = 1, . . . , p\nkz -xk2 ≤R\nexamples (with n = 1, m = p = 0)\n⋆\n- f0(x) = 1/x, dom f0 = R++: p = 0, no optimal point\n⋆\n- f0(x) = -log x, dom f0 = R++: p = -inf\n⋆\n- f0(x) = x log x, dom f0 = R++: p = -1/e, x = 1/e is optimal\nf0(x) = x3 -3x, p ⋆ = -inf, local optimum at x = 1\n-\nConvex optimization problems\n4-3\n\n\\\n\\\nImplicit constraints\nthe standard form optimization problem has an implicit constraint\nm\np\nx ∈D =\ndom fi ∩\ndom hi,\ni=0\ni=1\n- we call D the domain of the problem\n- the constraints fi(x) ≤0, hi(x) = 0 are the explicit constraints\n- a problem is unconstrained if it has no explicit constraints (m = p = 0)\nexample:\nminimize\nf0(x) = - P\ni\nk\n=1 log(bi -ai\nTx)\nis an unconstrained problem with implicit constraints ai\nTx < bi\nConvex optimization problems\n4-4\n\nFeasibility problem\nfind\nx\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\ncan be considered a special case of the general problem with f0(x) = 0:\nminimize\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\n⋆\n- p = 0 if constraints are feasible; any feasible x is optimal\n⋆\n- p = infif constraints are infeasible\nConvex optimization problems\n4-5\n\nConvex optimization problem\nstandard form convex optimization problem\nminimize\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nai\nTx = bi,\ni = 1, . . . , p\n- f0, f1, . . . , fm are convex; equality constraints are affine\n- problem is quasiconvex if f0 is quasiconvex (and f1, . . . , fm convex)\noften written as\nminimize\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nAx = b\nimportant property: feasible set of a convex optimization problem is convex\nConvex optimization problems\n4-6\n\nexample\nminimize\nf0(x) = x2\n1 + x2\nsubject to\nf1(x) = x1/(1 + x2\n2) ≤0\nh1(x) = (x1 + x2)2 = 0\n- f0 is convex; feasible set {(x1, x2) | x1 = -x2 ≤0} is convex\n- not a convex problem (according to our definition): f1 is not convex, h1\nis not affine\n- equivalent (but not identical) to the convex problem\nminimize\nx1\n2 + x2\nsubject to\nx1 ≤0\nx1 + x2 = 0\nConvex optimization problems\n4-7\n\nLocal and global optima\nany locally optimal point of a convex problem is (globally) optimal\nproof: suppose x is locally optimal and y is optimal with f0(y) < f0(x)\nx locally optimal means there is an R > 0 such that\nz feasible,\nkz -xk2 ≤R\n= ⇒\nf0(z) ≥f0(x)\nconsider z = θy + (1 -θ)x with θ = R/(2ky -xk2)\n- ky -xk2 > R, so 0 < θ < 1/2\n- z is a convex combination of two feasible points, hence also feasible\n- kz -xk2 = R/2 and\nf0(z) ≤θf0(x) + (1 -θ)f0(y) < f0(x)\nwhich contradicts our assumption that x is locally optimal\nConvex optimization problems\n4-8\n\nOptimality criterion for differentiable f0\nx is optimal if and only if it is feasible and\n∇f0(x)T(y -x) ≥0\nfor all feasible y\n-∇f0(x)\nX\nx\nif nonzero, ∇f0(x) defines a supporting hyperplane to feasible set X at x\nConvex optimization problems\n4-9\n\n- unconstrained problem: x is optimal if and only if\nx ∈dom f0,\n∇f0(x) = 0\n- equality constrained problem\nminimize\nf0(x)\nsubject to\nAx = b\nx is optimal if and only if there exists a ν such that\nx ∈dom f0,\nAx = b,\n∇f0(x) + ATν = 0\n- minimization over nonnegative orthant\nminimize\nf0(x)\nsubject to\nx 0\nx is optimal if and only if\n∇f0(x)i ≥0\nxi = 0\nx ∈dom f0,\nx 0,\n∇f0(x)i\nxi > 0\n= 0\nConvex optimization problems\n4-10\n\nEquivalent convex problems\ntwo problems are (informally) equivalent if the solution of one is readily\nobtained from the solution of the other, and vice-versa\nsome common transformations that preserve convexity:\n- eliminating equality constraints\nminimize\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nAx = b\nis equivalent to\nminimize (over z)\nf0(Fz + x0)\nsubject to\nfi(Fz + x0) ≤0,\ni = 1, . . . , m\nwhere F and x0 are such that\nAx = b\nx = Fz + x0 for some z\n⇐⇒\nConvex optimization problems\n4-11\n\n- introducing equality constraints\nminimize\nf0(A0x + b0)\nsubject to\nfi(Aix + bi) ≤0,\ni = 1, . . . , m\nis equivalent to\nminimize (over x, yi)\nf0(y0)\nsubject to\nfi(yi) ≤0,\ni = 1, . . . , m\nyi = Aix + bi,\ni = 0, 1, . . . , m\n- introducing slack variables for linear inequalities\nminimize\nf0(x)\nsubject to\nai\nTx ≤bi,\ni = 1, . . . , m\nis equivalent to\nminimize (over x, s)\nf0(x)\nsubject to\naT\ni x + si = bi,\ni = 1, . . . , m\nsi ≥0,\ni = 1, . . . m\nConvex optimization problems\n4-12\n\n- epigraph form: standard form convex problem is equivalent to\nminimize (over x, t)\nt\nsubject to\nf0(x) -t ≤0\nfi(x) ≤0,\ni = 1, . . . , m\nAx = b\n- minimizing over some variables\nminimize\nf0(x1, x2)\nsubject to\nfi(x1) ≤0,\ni = 1, . . . , m\nis equivalent to\nminimize\nf 0(x1)\nsubject to\nfi(x1) ≤0,\ni = 1, . . . , m\nwhere f 0(x1) = infx2 f0(x1, x2)\nConvex optimization problems\n4-13\n\nQuasiconvex optimization\nminimize\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nAx = b\nwith f0 : Rn\nR quasiconvex, f1, . . . , fm convex\n→\ncan have locally optimal points that are not (globally) optimal\n(x, f0(x))\nConvex optimization problems\n4-14\n\nconvex representation of sublevel sets of f0\nif f0 is quasiconvex, there exists a family of functions φt such that:\n- φt(x) is convex in x for fixed t\n- t-sublevel set of f0 is 0-sublevel set of φt, i.e.,\nf0(x) ≤t\nφt(x) ≤0\n⇐⇒\nexample\np(x)\nf0(x) = q(x)\nwith p convex, q concave, and p(x) ≥0, q(x) > 0 on dom f0\ncan take φt(x) = p(x) -tq(x):\n- for t ≥0, φt convex in x\n- p(x)/q(x) ≤t if and only if φt(x) ≤0\nConvex optimization problems\n4-15\n\nquasiconvex optimization via convex feasibility problems\nφt(x) ≤0,\nfi(x) ≤0,\ni = 1, . . . , m,\nAx = b\n(1)\n- for fixed t, a convex feasibility problem in x\nif feasible, we can conclude that t ≥p ⋆ ; if infeasible, t ≤p ⋆\n-\nBisection method for quasiconvex optimization\n⋆\n⋆\ngiven l ≤ p , u ≥ p , tolerance o > 0.\nrepeat\n1. t := (l + u)/2.\n2. Solve the convex feasibility problem (1).\n3. if (1) is feasible, u := t;\nelse l := t.\nuntil u -l ≤ o.\nrequires exactly ⌈log2((u -l)/o)⌉iterations (where u, l are initial values)\nConvex optimization problems\n4-16\n\nLinear program (LP)\nminimize\ncTx + d\nsubject to\nGx h\nAx = b\n- convex problem with affine objective and constraint functions\n- feasible set is a polyhedron\nP\nx ⋆\n-c\nConvex optimization problems\n4-17\n\nExamples\ndiet problem: choose quantities x1, . . . , xn of n foods\n- one unit of food j costs cj, contains amount aij of nutrient i\n- healthy diet requires nutrient i in quantity at least bi\nto find cheapest healthy diet,\nminimize\ncTx\nsubject to\nAx b,\nx 0\npiecewise-linear minimization\nminimize\nmaxi=1,...,m(ai\nTx + bi)\nequivalent to an LP\nminimize\nt\nsubject to\nai\nTx + bi ≤t,\ni = 1, . . . , m\nConvex optimization problems\n4-18\n\nChebyshev center of a polyhedron\nChebyshev center of\nP = {x | ai\nT x ≤bi, i = 1, . . . , m}\nis center of largest inscribed ball\nB = {xc + u | kuk2 ≤r}\nxc\nx he\nh b\nc eb\n- ai\nTx ≤bi for all x ∈B if and only if\nsup{ai\nT(xc + u) | kuk2 ≤r} = ai\nT xc + rkaik2 ≤bi\n- hence, xc, r can be determined by solving the LP\nmaximize\nr\nsubject to\nai\nTxc + rkaik2 ≤bi,\ni = 1, . . . , m\nConvex optimization problems\n4-19\n\n(Generalized) linear-fractional program\nminimize\nf0(x)\nsubject to\nGx h\nAx = b\nlinear-fractional program\ncTx + d\nT\nf0(x) = eTx + f ,\ndom f0(x) = {x | e x + f > 0}\n- a quasiconvex optimization problem; can be solved by bisection\n- also equivalent to the LP (variables y, z)\nminimize\ncTy + dz\nsubject to\nGy hz\nAy = bz\neTy + fz = 1\nz ≥0\nConvex optimization problems\n4-20\n\ngeneralized linear-fractional program\nci\nTx + di\nT\nf0(x) = max\nT\n,\ndom f0(x) = {x\nei x+fi > 0, i = 1, . . . , r}\ni=1,...,r ei x + fi\n|\na quasiconvex optimization problem; can be solved by bisection\nexample: Von Neumann model of a growing economy\nmaximize (over x, x+)\nmini=1,...,n x +\ni /xi\nsubject to\nx+ 0,\nBx+ Ax\n- x, x+ ∈Rn: activity levels of n sectors, in current and next period\n- (Ax)i, (Bx+)i: produced, resp. consumed, amounts of good i\n+\n- xi /xi: growth rate of sector i\nallocate activity to maximize growth rate of slowest growing sector\nConvex optimization problems\n4-21\n\nQuadratic program (QP)\nminimize\n(1/2)xTPx + qTx + r\nsubject to\nGx h\nAx = b\n- P ∈Sn , so objective is convex quadratic\n+\n- minimize a convex quadratic function over a polyhedron\nP\nx ⋆\n-∇f0(x ⋆ )\nConvex optimization problems\n4-22\n\nExamples\nleast-squares\nminimize\nkAx -bk2\nanalytical solution x ⋆ = A+b (A+ is pseudo-inverse)\n-\n- can add linear constraints, e.g., l x u\nlinear program with random cost\nminimize\nc Tx + γxTΣx = E cTx + γ var(cTx)\nsubject to\nGx h,\nAx = b\nc is random vector with mean c and covariance Σ\n-\nhence, cTx is random variable with mean c Tx and variance xTΣx\n-\n- γ > 0 is risk aversion parameter; controls the trade-off between\nexpected cost and variance (risk)\nConvex optimization problems\n4-23\n\nQuadratically constrained quadratic program (QCQP)\nminimize\n(1/2)xTP0x + q0\nTx + r0\nsubject to\n(1/2)xTPix + qi\nTx + ri ≤0,\ni = 1, . . . , m\nAx = b\n- Pi ∈S+\nn ; objective and constraints are convex quadratic\n- if P1, . . . , Pm ∈Sn , feasible region is intersection of m ellipsoids and\n++\nan affine set\nConvex optimization problems\n4-24\n\nSecond-order cone programming\nminimize\nf Tx\nsubject to\nkAix + bik2 ≤cT\ni x + di,\ni = 1, . . . , m\nFx = g\n(Ai ∈Rni×n , F ∈Rp×n)\n- inequalities are called second-order cone (SOC) constraints:\n(Aix + bi, c i\nT x + di) ∈second-order cone in Rni+1\n- for ni = 0, reduces to an LP; if ci = 0, reduces to a QCQP\n- more general than QCQP and LP\nConvex optimization problems\n4-25\n\nRobust linear programming\nthe parameters in optimization problems are often uncertain, e.g., in an LP\nminimize\ncTx\nsubject to\naT\ni x ≤bi,\ni = 1, . . . , m,\nthere can be uncertainty in c, ai, bi\ntwo common approaches to handling uncertainty (in ai, for simplicity)\n- deterministic model: constraints must hold for all ai ∈Ei\nminimize\ncTx\nsubject to\nai\nTx ≤bi for all ai ∈Ei,\ni = 1, . . . , m,\n- stochastic model: ai is random variable; constraints must hold with\nprobability η\nminimize\ncTx\nsubject to\nprob(ai\nTx ≤bi) ≥η,\ni = 1, . . . , m\nConvex optimization problems\n4-26\n\ndeterministic approach via SOCP\n- choose an ellipsoid as Ei:\nEi = {a i + Piu | kuk2 ≤1}\n( ai ∈Rn ,\nPi ∈Rn×n)\ncenter is a i, semi-axes determined by singular values/vectors of Pi\nrobust LP\n-\nminimize\ncTx\nsubject to\nai\nTx ≤bi\n∀ai ∈Ei,\ni = 1, . . . , m\nis equivalent to the SOCP\nminimize\ncTx\nsubject to\na i\nTx + kPi\nTxk2 ≤bi,\ni = 1, . . . , m\n(follows from sup∥u∥2≤1( ai + Piu)Tx = a i\nTx + kPi\nTxk2)\nConvex optimization problems\n4-27\n\n!\nstochastic approach via SOCP\n- assume ai is Gaussian with mean a i, covariance Σi (ai ∼N( ai, Σi))\naT\ni x is Gaussian r.v. with mean a T\ni x, variance xTΣix; hence\n-\ni x\nprob(ai\nT x ≤bi) = Φ\nbi -a T\nkΣi\n1/2 xk2\nR x\n-t2\nwhere Φ(x) = (1/\n√\n2π)\ne\n/2 dt is CDF of N(0, 1)\n-inf\nrobust LP\n-\nminimize\ncTx\nsubject to\nprob(ai\nTx ≤bi) ≥η,\ni = 1, . . . , m,\nwith η ≥1/2, is equivalent to the SOCP\nminimize\ncTx\nsubject to\na i\nTx + Φ-1(η)kΣ1\ni\n/2 xk2 ≤bi,\ni = 1, . . . , m\nConvex optimization problems\n4-28\n\nX\nGeometric programming\nmonomial function\nf(x) = cx a1x a2\nx an ,\ndom f = Rn\nn\n++\n· · ·\nwith c > 0; exponent αi can be any real number\nposynomial function: sum of monomials\nK\nf(x) =\nckx1\na1kx2\na2k\nxn\nank ,\ndom f = R++\nn\n· · ·\nk=1\ngeometric program (GP)\nminimize\nf0(x)\nsubject to\nfi(x) ≤1,\ni = 1, . . . , m\nhi(x) = 1,\ni = 1, . . . , p\nwith fi posynomial, hi monomial\nConvex optimization problems\n4-29\n\n!\n\nGeometric program in convex form\nchange variables to yi = log xi, and take logarithm of cost, constraints\n- monomial f(x) = cx 1\na1 · · · xn\nan transforms to\nlog f(ey1 , . . . , eyn) = a T y + b\n(b = log c)\nPK\na1k\na2k\nank\n- posynomial f(x) =\nk=1 ckx1 x2 · · · xn\ntransforms to\nK\na\nlog f(ey1 , . . . , eyn) = log\nX\ne k\nT y+bk\n(bk = log ck)\nk=1\n- geometric program transforms to convex problem\nminimize\nlog PK\nk=1 exp(aT\n0ky + b0k)\nPK\nT\nsubject to\nlog\nk=1 exp(aiky + bik)\n≤0,\ni = 1, . . . , m\nGy + d = 0\nConvex optimization problems\n4-30\n\nDesign of cantilever beam\nF\nsegment 4 segment 3 segment 2 segment 1\n- N segments with unit lengths, rectangular cross-sections of size wi × hi\n- given vertical force F applied at the right end\ndesign problem\nminimize\ntotal weight\nsubject to\nupper & lower bounds on wi, hi\nupper bound & lower bounds on aspect ratios hi/wi\nupper bound on stress in each segment\nupper bound on vertical deflection at the end of the beam\nvariables: wi, hi for i = 1, . . . , N\nConvex optimization problems\n4-31\n\nobjective and constraint functions\n- total weight w1h1 + · · · + wNhN is posynomial\n- aspect ratio hi/wi and inverse aspect ratio wi/hi are monomials\nmaximum stress in segment i is given by 6iF/(wih2\ni), a monomial\n-\n- the vertical deflection yi and slope vi of central axis at the right end of\nsegment i are defined recursively as\nF\nvi\n=\n12(i -1/2) Ewih3\ni\n+ vi+1\nF\nyi\n=\n6(i -1/3) Ewih3 + vi+1 + yi+1\ni\nfor i = N, N -1, . . . , 1, with vN+1 = yN+1 = 0 (E is Young's modulus)\nvi and yi are posynomial functions of w, h\nConvex optimization problems\n4-32\n\nformulation as a GP\nminimize\nw1h1 + · · · + wNhN\nsubject to\nw-1\nmaxwi ≤1,\nwminw -1\ni\n≤1,\ni = 1, . . . , N\nh-1\nmaxhi ≤1,\nhminh-1\ni\n≤1,\ni = 1, . . . , N\nS-1\nmaxw -1\ni hi ≤1,\nSminwih-1\ni\n≤1,\ni = 1, . . . , N\n6iFσ-1\nmaxw -1\ni h-2\ni\n≤1,\ni = 1, . . . , N\n-1\ny\ny1 ≤1\nmax\nnote\n- we write wmin ≤wi ≤wmax and hmin ≤hi ≤hmax\nwmin/wi ≤1,\nwi/wmax ≤1,\nhmin/hi ≤1,\nhi/hmax ≤1\n- we write Smin ≤hi/wi ≤Smax as\nSminwi/hi ≤1,\nhi/(wiSmax) ≤1\nConvex optimization problems\n4-33\n\nMinimizing spectral radius of nonnegative matrix\nPerron-Frobenius eigenvalue λpf (A)\nexists for (elementwise) positive A ∈Rn×n\n-\n- a real, positive eigenvalue of A, equal to spectral radius maxi |λi(A)|\n- determines asymptotic growth (decay) rate of Ak: Ak ∼λpf\nk as k →inf\n- alternative characterization: λpf(A) = inf{λ | Av λv for some v ≻0}\nminimizing spectral radius of matrix of posynomials\n- minimize λpf (A(x)), where the elements A(x)ij are posynomials of x\n- equivalent geometric program:\nminimize\nλ\nsubject to\nPn\nA(x)ijvj/(λvi) ≤1,\ni = 1, . . . , n\nj=1\nvariables λ, v, x\nConvex optimization problems\n4-34\n\nGeneralized inequality constraints\nconvex problem with generalized inequality constraints\nminimize\nf0(x)\nsubject to\nfi(x) Ki 0,\ni = 1, . . . , m\nAx = b\nf0 : Rn\nR convex; fi : Rn\nRki Ki-convex w.r.t. proper cone Ki\n-\n→\n→\n- same properties as standard convex problem (convex feasible set, local\noptimum is global, etc.)\nconic form problem: special case with affine objective and constraints\nminimize\ncTx\nsubject to\nFx + g K 0\nAx = b\nextends linear programming (K = Rm) to nonpolyhedral cones\n+\nConvex optimization problems\n4-35\n\nSemidefinite program (SDP)\nminimize\ncTx\nsubject to\nx1F1 + x2F2 +\n+ xnFn + G 0\n· · ·\nAx = b\nwith Fi, G ∈Sk\n- inequality constraint is called linear matrix inequality (LMI)\n- includes problems with multiple LMI constraints: for example,\nx1Fˆ1 +\n+ xnFˆn + Gˆ 0,\nx1F 1 +\n+ xnF n + G 0\n· · ·\n· · ·\nis equivalent to single LMI\nFˆ1\nFˆ2\nFˆn\nGˆ\nx1\n+x2\n+\n+xn\n+\nF 1\nF 2\n· · ·\nF n\nG\nConvex optimization problems\n4-36\n\nLP and SOCP as SDP\nLP and equivalent SDP\nLP:\nminimize\ncTx\nSDP:\nminimize\ncTx\nsubject to\nAx b\nsubject to\ndiag(Ax -b) 0\n(note different interpretation of generalized inequality )\nSOCP and equivalent SDP\nSOCP:\nminimize\nf Tx\nsubject to\nkAix + bik2 ≤ci\nTx + di,\ni = 1, . . . , m\nSDP:\nminimize\nf Tx\n(ci\nTx + di)I\nAix + bi\nsubject to\n(Aix + bi)T\ncT\ni x + di\n0,\ni = 1, . . . , m\nConvex optimization problems\n4-37\n\nEigenvalue minimization\nminimize\nλmax(A(x))\nwhere A(x) = A0 + x1A1 +\n+ xnAn (with given Ai ∈Sk)\n· · ·\nequivalent SDP\nminimize\nt\nsubject to\nA(x) tI\n- variables x ∈Rn , t ∈R\nfollows from\n-\nλmax(A) ≤t\n⇐⇒\nA tI\nConvex optimization problems\n4-38\n\nMatrix norm minimization\nminimize\nkA(x)k2 =\n\nλmax(A(x)TA(x))\n1/2\nwhere A(x) = A0 + x1A1 +\n+ xnAn (with given Ai ∈Rp×q)\n· · ·\nequivalent SDP\nminimize\nt\nsubject to\nA(\ntI\nx)T\nA\ntI\n(x)\n- variables x ∈Rn , t ∈R\nconstraint follows from\n-\nkAk2 ≤t\nATA t2I,\nt ≥0\n⇐⇒\n\ntI\nA\n⇐⇒\nAT\ntI\nConvex optimization problems\n4-39\n\nVector optimization\ngeneral vector optimization problem\nminimize (w.r.t. K)\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nhi(x) ≤0,\ni = 1, . . . , p\nvector objective f0 : Rn\nRq, minimized w.r.t. proper cone K ∈Rq\n→\nconvex vector optimization problem\nminimize (w.r.t. K)\nf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nAx = b\nwith f0 K-convex, f1, . . . , fm convex\nConvex optimization problems\n4-40\n\nOptimal and Pareto optimal points\nset of achievable objective values\nO = {f0(x) | x feasible}\n- feasible x is optimal if f0(x) is a minimum value of O\n- feasible x is Pareto optimal if f0(x) is a minimal value of O\nO\nf0(x ⋆ )\nO\nf0(xpo)\nx⋆ is optimal\nxpo is Pareto optimal\nConvex optimization problems\n4-41\n\nMulticriterion optimization\nvector optimization problem with K = Rq\n+\nf0(x) = (F1(x), . . . , Fq(x))\n- q different objectives Fi; roughly speaking we want all Fi's to be small\n⋆\n- feasible x is optimal if\ny feasible\n= ⇒\nf0(x ⋆ ) f0(y)\nif there exists an optimal point, the objectives are noncompeting\n- feasible xpo is Pareto optimal if\ny feasible,\nf0(y) f0(xpo)\n=\nf0(xpo) = f0(y)\n⇒\nif there are multiple Pareto optimal values, there is a trade-off between\nthe objectives\nConvex optimization problems\n4-42\n\nRegularized least-squares\nF2 (x) = k x k 2\nO\nminimize (w.r.t. R2 )\n(kAx -bk2\n2)\n+\n2, kxk\nF1(x) = kAx -bk2\nexample for A ∈R100×10; heavy line is formed by Pareto optimal points\nConvex optimization problems\n4-43\n\nRisk return trade-off in portfolio optimization\nminimize (w.r.t. R2\npTx, xTΣx)\n+)\n(-\nsubject to\n1Tx = 1,\nx 0\n- x ∈Rn is investment portfolio; xi is fraction invested in asset i\n- p ∈Rn is vector of relative asset price changes; modeled as a random\nvariable with mean p , covariance Σ\np Tx = E r is expected return; xTΣx = var r is return variance\n-\nexample\n15%\nmean return\n10%\n5%\nallocation x\n0.5\n0% 0%\n10%\n20%\nx(1)\nx(4)\nx(2)\nx(3)\n0%\n10%\n20%\nstandard deviation of return\nstandard deviation of return\nConvex optimization problems\n4-44\n\nScalarization\nto find Pareto optimal points: choose λ ≻K∗ 0 and solve scalar problem\nminimize\nλTf0(x)\nsubject to\nfi(x) ≤0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\nif x is optimal for scalar problem,\nthen it is Pareto-optimal for vector\noptimization problem\nO\nf0(x1)\nλ1\nf0(x2) λ2\nf0(x3)\nfor convex vector optimization problems, can find (almost) all Pareto\noptimal points by varying λ ≻K∗ 0\nConvex optimization problems\n4-45\n\nScalarization for multicriterion problems\nto find Pareto optimal points, minimize positive weighted sum\nλTf0(x) = λ1F1(x) +\n+ λqFq(x)\n· · ·\nexamples\n- regularized least-squares problem of page 4-43\ntake λ = (1, γ) with γ > 0\nγ = 1\nk x k\nminimize\nkAx -bk + γkxk\nfor fixed γ, a LS problem\nkAx -bk 2\nConvex optimization problems\n4-46\n\n- risk-return trade-off of page 4-44\nminimize\n-p Tx + γxTΣx\nsubject to\n1Tx = 1,\nx 0\nfor fixed γ > 0, a quadratic program\nConvex optimization problems\n4-47\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/c3e06e62a1d59d8c61efe93dd7ec9e12_MIT6_079F09_lec05.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n5. Duality\n- Lagrange dual problem\n- weak and strong duality\n- geometric interpretation\n- optimality conditions\n- perturbation and sensitivity analysis\n- examples\n- generalized inequalities\n5-1\n\nX\nX\nLagrangian\nstandard form problem (not necessarily convex)\nminimize\nf0(x)\nsubject to\nfi(x) ≤ 0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\nvariable x ∈ Rn, domain D, optimal value p ⋆\nLagrangian: L : Rn × Rm × Rp → R, with dom L = D × Rm × Rp ,\nm\np\nL(x, λ, ν) = f0(x) +\nλifi(x) +\nνihi(x)\ni=1\ni=1\n- weighted sum of objective and constraint functions\n- λi is Lagrange multiplier associated with fi(x) ≤ 0\n- νi is Lagrange multiplier associated with hi(x) = 0\nDuality\n5-2\n\n!\nX\nX\nLagrange dual function\nLagrange dual function: g : Rm × Rp → R,\ng(λ, ν) =\ninf L(x, λ, ν)\nx∈D\nm\np\n=\ninf\nf0(x) +\nλifi(x) +\nνihi(x)\nx∈D\ni=1\ni=1\ng is concave, can be -inf for some λ, ν\n⋆\nlower bound property: if λ 0, then g(λ, ν) ≤ p\nproof: if x is feasible and λ 0, then\nf0( x) ≥ L( x, λ, ν) ≥ inf L(x, λ, ν) = g(λ, ν)\nx∈D\nminimizing over all feasible x gives p ⋆ ≥ g(λ, ν)\nDuality\n5-3\n\nLeast-norm solution of linear equations\nminimize\nxTx\nsubject to\nAx = b\ndual function\n- Lagrangian is L(x, ν) = xTx + νT(Ax -b)\n- to minimize L over x, set gradient equal to zero:\n∇xL(x, ν) = 2x + ATν = 0\n=⇒\nx = -(1/2)ATν\n- plug in in L to obtain g:\ng(ν) = L((-1/2)ATν, ν) = - 1 νTAATν - bTν\na concave function of ν\nlower bound property: p ⋆ ≥-(1/4)νTAATν -bTν for all ν\nDuality\n5-4\n\nStandard form LP\nminimize\ncTx\nsubject to\nAx = b,\nx 0\ndual function\n- Lagrangian is\nL(x, λ, ν)\n= c T x + νT(Ax -b) -λT x\n= -bTν + (c + ATν -λ)T x\n- L is affine in x, hence\n-bTν\nATν -λ + c = 0\ng(λ, ν) = inf L(x, λ, ν) =\nx\n-inf\notherwise\ng is linear on affine domain {(λ, ν) | ATν -λ + c = 0}, hence concave\nlower bound property: p ⋆ ≥-bTν if ATν + c 0\nDuality\n5-5\n\nEquality constrained norm minimization\nminimize\nkxk\nsubject to\nAx = b\ndual function\ng(ν) = inf (kxk -νTAx + bTν) =\nbTν\nkATνk∗ ≤ 1\nx\n-inf\notherwise\nwhere kvk∗ = sup∥u∥≤1 uTv is dual norm of k · k\nproof: follows from infx(kxk -yTx) = 0 if kyk∗ ≤ 1, -inf otherwise\n- if kyk∗ ≤ 1, then kxk -yTx ≥ 0 for all x, with equality if x = 0\n- if kyk∗ > 1, choose x = tu where kuk ≤ 1, uTy = kyk∗ > 1:\nkxk -y T x = t(kuk -kyk∗) →-inf\nas t →inf\nlower bound property: p ⋆ ≥ bTν if kATνk∗ ≤ 1\nDuality\n5-6\n\nX\n\nTwo-way partitioning\nminimize\nxTWx\nsubject to\nxi\n2 = 1,\ni = 1, . . . , n\n- a nonconvex problem; feasible set contains 2n discrete points\n- interpretation: partition {1, . . . , n} in two sets; Wij is cost of assigning\ni, j to the same set; -Wij is cost of assigning to different sets\ndual function\ng(ν) = inf (x TWx +\nνi(xi\n2 -1)) = inf x T(W + diag(ν))x -1Tν\nx\nx\ni\n-1Tν\nW + diag(ν) 0\n=\n-inf\notherwise\nlower bound property: p ⋆ ≥-1Tν if W + diag(ν) 0\nexample: ν = -λmin(W)1 gives bound p ⋆ ≥ nλmin(W)\nDuality\n5-7\n\nX\nX\nLagrange dual and conjugate function\nminimize\nf0(x)\nsubject to\nAx b,\nCx = d\ndual function\ng(λ, ν) =\ninf\n\nf0(x) + (ATλ + CTν)T x -bTλ -dTν\n\nx∈dom f0\n= -f0\n∗ (-ATλ -CTν) -bTλ -dTν\n- recall definition of conjugate f ∗(y) = supx∈dom f(yTx -f(x))\n- simplifies derivation of dual if conjugate of f0 is kown\nexample: entropy maximization\nn\nn\nf0(x) =\nxi log xi,\nf0\n∗ (y) =\neyi-1\ni=1\ni=1\nDuality\n5-8\n\nThe dual problem\nLagrange dual problem\nmaximize\ng(λ, ν)\nsubject to\nλ 0\n- finds best lower bound on p ⋆, obtained from Lagrange dual function\n- a convex optimization problem; optimal value denoted d⋆\n- λ, ν are dual feasible if λ 0, (λ, ν) ∈ dom g\n- often simplified by making implicit constraint (λ, ν) ∈ dom g explicit\nexample: standard form LP and its dual (page 5-5)\nminimize\ncTx\nmaximize\n-bTν\nsubject to\nAx = b\nsubject to\nATν + c 0\nx 0\nDuality\n5-9\n\nWeak and strong duality\nweak duality: d⋆ ≤ p ⋆\n- always holds (for convex and nonconvex problems)\n- can be used to find nontrivial lower bounds for difficult problems\nfor example, solving the SDP\nmaximize\n-1Tν\nsubject to\nW + diag(ν) 0\ngives a lower bound for the two-way partitioning problem on page 5-7\nstrong duality: d⋆ = p ⋆\n- does not hold in general\n- (usually) holds for convex problems\n- conditions that guarantee strong duality in convex problems are called\nconstraint qualifications\nDuality\n5-10\n\nSlater's constraint qualification\nstrong duality holds for a convex problem\nminimize\nf0(x)\nsubject to\nfi(x) ≤ 0,\ni = 1, . . . , m\nAx = b\nif it is strictly feasible, i.e.,\n∃x ∈ int D :\nfi(x) < 0,\ni = 1, . . . , m,\nAx = b\n- also guarantees that the dual optimum is attained (if p ⋆ > -inf)\n- can be sharpened: e.g., can replace int D with relint D (interior\nrelative to affine hull); linear inequalities do not need to hold with strict\ninequality, . . .\n- there exist many other types of constraint qualifications\nDuality\n5-11\n\nInequality form LP\nprimal problem\nminimize\ncTx\nsubject to\nAx b\ndual function\ng(λ) = inf\n\n(c + ATλ)T x -bTλ\n\n=\n-bTλ\nATλ + c = 0\nx\n-inf\notherwise\ndual problem\nmaximize\n-bTλ\nsubject to\nATλ + c = 0,\nλ 0\n- from Slater's condition: p ⋆ = d⋆ if Ax ≺ b for some x\n- in fact, p ⋆ = d⋆ except when primal and dual are infeasible\nDuality\n5-12\n\nQuadratic program\nprimal problem (assume P ∈ Sn )\n++\nminimize\nxTPx\nsubject to\nAx b\ndual function\ng(λ) = inf\n\nx TPx + λT(Ax -b)\n\n= - 1 λTAP -1ATλ -bTλ\nx\ndual problem\nmaximize\n-(1/4)λTAP -1ATλ -bTλ\nsubject to\nλ 0\n- from Slater's condition: p ⋆ = d⋆ if Ax ≺ b for some x\n- in fact, p ⋆ = d⋆ always\nDuality\n5-13\n\nA nonconvex problem with strong duality\nminimize\nxTAx + 2bTx\nsubject to\nxTx ≤ 1\nA 6 0, hence nonconvex\ndual function: g(λ) = infx(xT(A + λI)x + 2bTx -λ)\n- unbounded below if A + λI 6 0 or if A + λI 0 and b 6∈R(A + λI)\n- minimized by x = -(A + λI)+b otherwise: g(λ) = -bT(A + λI)+b -λ\ndual problem and equivalent SDP:\nmaximize\n-bT(A + λI)+b -λ\nmaximize\n-t -λ\nsubject to\nA + λI 0\nA + λI\nb\nb ∈R(A + λI)\nsubject to\nbT\nt\nstrong duality although primal problem is not convex (not easy to show)\nDuality\n5-14\n\nGeometric interpretation\nfor simplicity, consider problem with one constraint f1(x) ≤ 0\ninterpretation of dual function:\ng(λ) = inf (t + λu),\nwhere\nG = {(f1(x), f0(x)) | x ∈D}\n(u,t)∈G\nG\np ⋆\ng(λ)\nλu + t = g(λ)\nt\nu\nG\np ⋆\nd ⋆\nt\nu\n- λu + t = g(λ) is (non-vertical) supporting hyperplane to G\n- hyperplane intersects t-axis at t = g(λ)\nDuality\n5-15\n\nepigraph variation: same interpretation if G is replaced with\nA = {(u, t) | f1(x) ≤ u, f0(x) ≤ t for some x ∈D}\nt\nλu + t = g(λ)\nA\np ⋆\ng(λ)\nu\nstrong duality\n- holds if there is a non-vertical supporting hyperplane to A at (0, p ⋆)\n- for convex problem, A is convex, hence has supp. hyperplane at (0, p ⋆)\n- Slater's condition: if there exist ( t) ∈A with\nu,\nu < 0, then supporting\nhyperplanes at (0, p ⋆) must be non-vertical\nDuality\n5-16\n\n!\nX\nX\nX\nX\nComplementary slackness\nassume strong duality holds, x ⋆ is primal optimal, (λ⋆, ν⋆) is dual optimal\nm\np\nf0(x ⋆ ) = g(λ ⋆ , ν ⋆ ) = inf f0(x) +\nλi\n⋆ fi(x) +\nνi\n⋆ hi(x)\nx\ni=1\ni=1\nm\np\n≤\nf0(x ⋆ ) +\nλi\n⋆ fi(x ⋆ ) +\nνi\n⋆ hi(x ⋆ )\ni=1\ni=1\n≤\nf0(x ⋆ )\nhence, the two inequalities hold with equality\n- x ⋆ minimizes L(x, λ⋆, ν⋆)\n- λi\n⋆fi(x ⋆) = 0 for i = 1, . . . , m (known as complementary slackness):\nλ ⋆\ni > 0 =⇒ fi(x ⋆ ) = 0,\nfi(x ⋆ ) < 0 =⇒ λi\n⋆ = 0\nDuality\n5-17\n\nX\nX\nKarush-Kuhn-Tucker (KKT) conditions\nthe following four conditions are called KKT conditions (for a problem with\ndifferentiable fi, hi):\n1. primal constraints: fi(x) ≤ 0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p\n2. dual constraints: λ 0\n3. complementary slackness: λifi(x) = 0, i = 1, . . . , m\n4. gradient of Lagrangian with respect to x vanishes:\nm\np\n∇f0(x) +\nλi∇fi(x) +\nνi∇hi(x) = 0\ni=1\ni=1\nfrom page 5-17: if strong duality holds and x, λ, ν are optimal, then they\nmust satisfy the KKT conditions\nDuality\n5-18\n\nKKT conditions for convex problem\nif x , λ , ν satisfy KKT for a convex problem, then they are optimal:\n\n- from complementary slackness: f0( x) = L( x, λ, ν )\n- from 4th condition (and convexity): g( ν) = L( λ, ν )\nλ,\nx,\nhence, f0( x) = g( ν)\nλ,\nif Slater's condition is satisfied:\nx is optimal if and only if there exist λ, ν that satisfy KKT conditions\n- recall that Slater implies strong duality, and dual optimum is attained\n- generalizes optimality condition ∇f0(x) = 0 for unconstrained problem\nDuality\n5-19\n\nexample: water-filling (assume αi > 0)\nminimize\n- Pn\ni=1 log(xi + αi)\nsubject to\nx 0,\n1Tx = 1\nx is optimal iff x 0, 1Tx = 1, and there exist λ ∈ Rn , ν ∈ R such that\nλ 0,\nλixi = 0,\n+ λi = ν\nxi + αi\n- if ν < 1/αi: λi = 0 and xi = 1/ν -αi\n- if ν ≥ 1/αi: λi = ν -1/αi and xi = 0\n- determine ν from 1Tx = Pn\ni=1 max{0, 1/ν -αi} = 1\ninterpretation\n- n patches; level of patch i is at height αi\n1/ν ⋆\nxi\n- flood area with unit amount of water\nαi\n- resulting level is 1/ν⋆\ni\nDuality\n5-20\n\nPerturbation and sensitivity analysis\n(unperturbed) optimization problem and its dual\nminimize\nf0(x)\nmaximize\ng(λ, ν)\nsubject to\nfi(x) ≤ 0,\ni = 1, . . . , m\nsubject to\nλ 0\nhi(x) = 0,\ni = 1, . . . , p\nperturbed problem and its dual\nmin.\nf0(x)\nmax.\ng(λ, ν) -uTλ -vTν\ns.t.\nfi(x) ≤ ui,\ni = 1, . . . , m\ns.t.\nλ 0\nhi(x) = vi,\ni = 1, . . . , p\n- x is primal variable; u, v are parameters\n- p ⋆(u, v) is optimal value as a function of u, v\n- we are interested in information about p ⋆(u, v) that we can obtain from\nthe solution of the unperturbed problem and its dual\nDuality\n5-21\n\nglobal sensitivity result\nassume strong duality holds for unperturbed problem, and that λ⋆ , ν⋆ are\ndual optimal for unperturbed problem\napply weak duality to perturbed problem:\np ⋆ (u, v) ≥\ng(λ ⋆ , ν ⋆ ) -u Tλ ⋆ -v Tν ⋆\n= p ⋆ (0, 0) -u Tλ ⋆ -v Tν ⋆\nsensitivity interpretation\n- if λ⋆\ni large: p ⋆ increases greatly if we tighten constraint i (ui < 0)\n- if λ⋆\ni small: p ⋆ does not decrease much if we loosen constraint i (ui > 0)\n- if νi\n⋆ large and positive: p ⋆ increases greatly if we take vi < 0;\nif νi\n⋆ large and negative: p ⋆ increases greatly if we take vi > 0\n- if νi\n⋆ small and positive: p ⋆ does not decrease much if we take vi > 0;\nif νi\n⋆ small and negative: p ⋆ does not decrease much if we take vi < 0\nDuality\n5-22\n\nlocal sensitivity: if (in addition) p ⋆(u, v) is differentiable at (0, 0), then\nλ ⋆\ni = -∂p⋆(0, 0) ,\nν ⋆ = -∂p⋆(0, 0)\n∂ui\ni\n∂vi\nproof (for λ⋆\ni ): from global sensitivity result,\n∂p⋆(0, 0)\np ⋆(tei, 0) -p ⋆(0, 0)\n= lim\n≥-λ ⋆\n∂ui\ntց0\nt\ni\n∂p⋆(0, 0) = lim p ⋆(tei, 0) -p ⋆(0, 0) ≤-λ ⋆\ni\n∂ui\ntր0\nt\nhence, equality\np ⋆(u) for a problem with one (inequality)\nconstraint:\nu\np ⋆ (u)\nu = 0\np⋆ (0) - λ⋆u\nDuality\n5-23\n\nDuality and problem reformulations\n- equivalent formulations of a problem can lead to very different duals\n- reformulating the primal problem can be useful when the dual is difficult\nto derive, or uninteresting\ncommon reformulations\n- introduce new variables and equality constraints\n- make explicit constraints implicit or vice-versa\n- transform objective or constraint functions\ne.g., replace f0(x) by φ(f0(x)) with φ convex, increasing\nDuality\n5-24\n\nIntroducing new variables and equality constraints\nminimize\nf0(Ax + b)\n⋆\n- dual function is constant: g = infx L(x) = infx f0(Ax + b) = p\n- we have strong duality, but dual is quite useless\nreformulated problem and its dual\nminimize\nf0(y)\nmaximize\nbTν -f0\n∗(ν)\nsubject to\nAx + b -y = 0\nsubject to\nATν = 0\ndual function follows from\ng(ν) = inf (f0(y) -νT y + νTAx + bTν)\nx,y\n-f0\n∗(ν) + bTν\nATν = 0\n=\n-inf\notherwise\nDuality\n5-25\n\nnorm approximation problem: minimize kAx -bk\nminimize\nkyk\nsubject to\ny = Ax -b\ncan look up conjugate of k · k, or derive dual directly\ng(ν) = inf (kyk + νT y -νTAx + bTν)\nx,y\nbTν + infy(kyk + νTy) ATν = 0\n=\n-inf\notherwise\nbTν\nATν = 0,\nkνk∗ ≤ 1\n=\n-inf\notherwise\n(see page 5-4)\ndual of norm approximation problem\nmaximize\nbTν\nsubject to\nATν = 0,\nkνk∗ ≤ 1\nDuality\n5-26\n\nImplicit constraints\nLP with box constraints: primal and dual problem\nminimize\ncTx\nmaximize\n-bTν -1Tλ1 -1Tλ2\nsubject to\nAx = b\nsubject to\nc + ATν + λ1 -λ2 = 0\n-1 x 1\nλ1 0,\nλ2 0\nreformulation with box constraints made implicit\ncTx\n-1 x 1\nminimize\nf0(x) =\ninf\notherwise\nsubject to\nAx = b\ndual function\ng(ν)\n=\ninf (c T x + νT(Ax -b))\n-1⪯x⪯1\n= -bTν -kATν + ck1\ndual problem: maximize -bTν -kATν + ck1\nDuality\n5-27\n\nX\nX\nProblems with generalized inequalities\nminimize\nf0(x)\nsubject to\nfi(x) Ki 0,\ni = 1, . . . , m\nhi(x) = 0,\ni = 1, . . . , p\nKi is generalized inequality on Rki\ndefinitions are parallel to scalar case:\n- Lagrange multiplier for fi(x) Ki 0 is vector λi ∈ Rki\n- Lagrangian L : Rn × Rk1 × · · · × Rkm × Rp → R, is defined as\nm\np\nL(x, λ1, · · · , λm, ν) = f0(x) +\nλi\nTfi(x) +\nνihi(x)\ni=1\ni=1\n- dual function g : Rk1 × · · · × Rkm × Rp → R, is defined as\ng(λ1, . . . , λm, ν) = inf L(x, λ1, · · · , λm, ν)\nx∈D\nDuality\n5-28\n\nX\nX\n⋆\nlower bound property: if λi K∗ 0, then g(λ1, . . . , λm, ν) ≤ p\ni\nproof: if x is feasible and λ K∗ 0, then\ni\nm\np\nf0( x) ≥\nf0( x) +\nλT\ni fi( x) +\nνihi( x)\ni=1\ni=1\n≥\ninf L(x, λ1, . . . , λm, ν)\nx∈D\n= g(λ1, . . . , λm, ν)\nminimizing over all feasible x gives p ⋆ ≥ g(λ1, . . . , λm, ν)\ndual problem\nmaximize\ng(λ1, . . . , λm, ν)\nsubject to\nλi K∗ 0,\ni = 1, . . . , m\ni\n- weak duality: p ⋆ ≥ d⋆ always\n- strong duality: p ⋆ = d⋆ for convex problem with constraint qualification\n(for example, Slater's: primal problem is strictly feasible)\nDuality\n5-29\n\nSemidefinite program\nprimal SDP (Fi, G ∈ Sk)\nminimize\ncTx\nsubject to\nx1F1 + · · · + xnFn G\n- Lagrange multiplier is matrix Z ∈ Sk\n- Lagrangian L(x, Z) = cTx + tr (Z(x1F1 + · · · + xnFn -G))\n- dual function\n-tr(GZ) tr(FiZ) + ci = 0,\ni = 1, . . . , n\ng(Z) = inf L(x, Z) =\nx\n-inf\notherwise\ndual SDP\nmaximize\n-tr(GZ)\nsubject to\nZ 0,\ntr(FiZ) + ci = 0,\ni = 1, . . . , n\np ⋆ = d⋆ if primal SDP is strictly feasible (∃x with x1F1 + · · · + xnFn ≺ G)\nDuality\n5-30\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/f7ba10f977638a036515bb8e180fc7a7_MIT6_079F09_lec06.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n6. Approximation and fitting\n- norm approximation\n- least-norm problems\n- regularized approximation\n- robust approximation\n6-1\n\nNorm approximation\nminimize\nkAx -bk\n(A ∈Rm×n with m ≥n, k · k is a norm on Rm)\n⋆\ninterpretations of solution x = argminx kAx -bk:\ngeometric: Ax⋆ is point in R(A) closest to b\n-\nestimation: linear measurement model\n-\ny = Ax + v\ny are measurements, x is unknown, v is measurement error\n⋆\ngiven y = b, best guess of x is x\n- optimal design: x are design variables (input), Ax is result (output)\n⋆\nx is design that best approximates desired result b\nApproximation and fitting\n6-2\n\nexamples\n- least-squares approximation (k · k2): solution satisfies normal equations\nATAx = ATb\n(x ⋆ = (ATA)-1ATb if rank A = n)\n- Chebyshev approximation (k · kinf): can be solved as an LP\nminimize\nt\nsubject to\n-t1 Ax -b t1\n- sum of absolute residuals approximation (k · k1): can be solved as an LP\nminimize\n1Ty\nsubject to\n-y Ax -b y\nApproximation and fitting\n6-3\n\nPenalty function approximation\nminimize\nφ(r1) +\n+ φ(rm)\n· · ·\nsubject to\nr = Ax -b\n(A ∈Rm×n , φ : R\nR is a convex penalty function)\n→\nexamples\n- quadratic: φ(u) = u\ndeadzone-linear\nquadratic\nlog barrier\n-1.5 -1\n-0.5\n0.5\n1.5\n0.5\n1.5\ndeadzone-linear with width a:\n-\nφ(u) = max{0, |u| -a}\nφ(u)\n- log-barrier with limit a:\nu\nφ(u) =\n-a2 log(1 -(u/a)2)\n|u| < a\ninf\notherwise\nApproximation and fitting\n6-4\n\nexample (m = 100, n = 30): histogram of residuals for penalties\nφ(u) = |u|,\nφ(u) = u 2 ,\nφ(u) = max{0, |u|-a},\nφ(u) = -log(1-u 2)\nDeadzone\np = 2\np = 1\n-2\n-1\n-2\n-1\n-2\n-1\nLog barrier\n-2\n-1\nr\nshape of penalty function has large effect on distribution of residuals\nApproximation and fitting\n6-5\n\ncHuber penalty function (with parameter M)\nu\nu\nφhub(u) =\n| | ≤M\nM(2|u| -M)\n|u| > M\nlinear growth for large u makes approximation less sensitive to outliers\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.5\n1.5\nf(t)\nφhub(u)\n-10\n-20\n-10\n-5\nu\nt\n- left: Huber penalty for M = 1\n- right: affine function f(t) = α + βt fitted to 42 points ti, yi (circles)\nusing quadratic (dashed) and Huber (solid) penalty\nApproximation and fitting\n6-6\n\nLeast-norm problems\nminimize\nkxk\nsubject to\nAx = b\n(A ∈Rm×n with m ≤n, k · k is a norm on Rn)\n⋆\ninterpretations of solution x = argminAx=b kxk:\n⋆\ngeometric: x is point in affine set {x\nAx = b} with minimum\n-\ndistance to 0\n|\n⋆\n- estimation: b = Ax are (perfect) measurements of x; x is smallest\n('most plausible') estimate consistent with measurements\n- design: x are design variables (inputs); b are required results (outputs)\n⋆\nx is smallest ('most efficient') design that satisfies requirements\nApproximation and fitting\n6-7\n\nexamples\n- least-squares solution of linear equations (k · k2):\ncan be solved via optimality conditions\n2x + ATν = 0,\nAx = b\n- minimum sum of absolute values (k · k1): can be solved as an LP\nminimize\n1Ty\nsubject to\n-y x y,\nAx = b\n⋆\ntends to produce sparse solution x\nextension: least-penalty problem\nminimize\nφ(x1) +\n+ φ(xn)\n· · ·\nsubject to\nAx = b\nφ : R\nR is convex penalty function\n→\nApproximation and fitting\n6-8\n\nRegularized approximation\nminimize (w.r.t. R2 )\n(kAx -bk, kxk)\n+\nA ∈Rm×n , norms on Rm and Rn can be different\ninterpretation: find good approximation Ax ≈b with small x\n- estimation: linear measurement model y = Ax + v, with prior\nknowledge that kxk is small\n- optimal design: small x is cheaper or more efficient, or the linear\nmodel y = Ax is only valid for small x\n- robust approximation: good approximation Ax ≈b with small x is\nless sensitive to errors in A than good approximation with large x\nApproximation and fitting\n6-9\n\nScalarized problem\nminimize\nkAx -bk + γkxk\n- solution for γ > 0 traces out optimal trade-off curve\nother common method: minimize kAx -bk2 + δkxk2 with δ > 0\n-\nTikhonov regularization\nminimize\nkAx -bk\ncan be solved as a least-squares problem\nδ\n+ k k\nx\n\nA\nb\n\nminimize\n\n√\nδI\nx -\nsolution x ⋆ = (ATA + δI)-1ATb\nApproximation and fitting\n6-10\n\nX\nOptimal input design\nlinear dynamical system with impulse response h:\nt\ny(t) =\nh(τ)u(t -τ),\nt = 0, 1, . . . , N\nτ=0\ninput design problem: multicriterion problem with 3 objectives\n1. tracking error with desired output ydes: Jtrack = PN (y(t) -ydes(t))2\nt=0\nPN\n2. input magnitude: Jmag =\nt=0 u(t)2\n3. input variation: Jder = PN-1(u(t + 1) -u(t))2\nt=0\ntrack desired output using a small and slowly varying input signal\nregularized least-squares formulation\nminimize\nJtrack + δJder + ηJmag\nfor fixed δ, η, a least-squares problem in u(0), . . . , u(N)\nApproximation and fitting\n6-11\n\nexample: 3 solutions on optimal trade-off curve\nu(t)\n(top) δ = 0, small η; (middle) δ = 0, larger η; (bottom) large δ\n0.5\ny(t)\n-5\n-0.5\n-1\n-10 0\nt\nt\n0.5\ny(t)\nu(t)\n-0.5\n-2\n-1\n-4 0\ny(t)\nt\nt\n0.5\n-0.5\n-2\n-1\n-4 0\nt\nt\nApproximation and fitting\n6-12\nu(t)\n\nX\nX\nSignal reconstruction\nminimize (w.r.t. R2\nxcork2, φ(ˆx))\n+)\n(kxˆ -\n- x ∈Rn is unknown signal\n- xcor = x + v is (known) corrupted version of x, with additive noise v\n- variable xˆ (reconstructed signal) is estimate of x\nφ : Rn\nR is regularization function or smoothing objective\n-\n→\nexamples: quadratic smoothing, total variation smoothing:\nn-1\nn-1\nφquad(ˆx) =\n(ˆxi+1 -xˆi)2 ,\nφtv(ˆx) =\n|xˆi+1 -xˆi|\ni=1\ni=1\nApproximation and fitting\n6-13\n\n0.5\nquadratic smoothing example\n0.5\n-0.5\n0.5\nxˆ\nx\n-0.5\nxˆ\n-0.5\n0.5\n0.5\nxcor\n-0.5\n-0.5\ni\ni\noriginal signal x and noisy\nthree solutions on trade-off curve\nsignal xcor\nkxˆ -xcork2 versus φquad(ˆx)\nxˆ\nApproximation and fitting\n6-14\n\ntotal variation reconstruction example\nxˆi\n-2\n-1\nxˆi\n-2 0\n-2\n-2\ni\n-2\ni\noriginal signal x and noisy\nthree solutions on trade-off curve\nsignal xcor\nkˆx -xcork2 versus φquad(ˆx)\nquadratic smoothing smooths out noise and sharp transitions in signal\nApproximation and fitting\n6-15\nxcor\nxˆi\n-1\nx\n\nxˆ\n-2\n-1\n-2\nxˆ\n-2\nxˆ\n-1\n-2\n-2 0\ni\ni\noriginal signal x and noisy\nthree solutions on trade-off curve\nsignal xcor\nkxˆ -xcork2 versus φtv(ˆx)\nxcor\nx\ntotal variation smoothing preserves sharp transitions in signal\nApproximation and fitting\n6-16\n\nRobust approximation\nminimize kAx -bk with uncertain A\ntwo approaches:\nstochastic: assume A is random, minimize E kAx -bk\n-\nworst-case: set A of possible values of A, minimize supA∈A kAx -bk\n-\ntractable only in special cases (certain norms k · k, distributions, sets A)\nexample: A(u) = A0 + uA1\nxnom minimizes kA0x -bk2\n-\nxstoch minimizes E kA(u)x -bk2\n-\nwith u uniform on [-1, 1]\nxwc minimizes sup-1≤u≤1 kA(u)x -bk2\n-\nfigure shows r(u) = kA(u)x -bk2\nu\nr(u)\nxnom\nxstoch\nxwc\n-2\n-1\nApproximation and fitting\n6-17\n\nk\nk\nstochastic robust LS with A = A + U, U random, E U = 0, E U TU = P\n\nminimize\nE k(A + U)x -bk\n- explicit expression for objective:\nE kAx -bk 2\n\n=\nE k\n\nAx -b + Uxk\nTU TUx\n=\nkAx -bk\nAx -bk\n+ E x\n+ x TPx\n=\n- hence, robust LS problem is equivalent to LS problem\nminimize\nAx -bk2\n2 + kP 1/2 xk 2\n- for P = δI, get Tikhonov regularized problem\n\nminimize\nkAx -bk2\n2 + δkxk 2\nApproximation and fitting\n6-18\n\nworst-case robust LS with A = {A + u1A1 + · · · + upAp | kuk2 ≤1}\nminimize\nsupA∈A kAx -bk2 = sup∥u∥2≤1 kP(x)u + q(x)k2\n\nwhere P(x) =\nA1x\nA2x\nApx\n, q(x) = Ax -b\n· · ·\n- from page 5-14, strong duality holds between the following problems\nmaximize\nkPu + qk2\nminimize\nt + λ\n\nsubject to\nkuk2\n2 ≤1\nI\nP\nq\nsubject to\nP T\nλI\n0 0\nqT\nt\n- hence, robust LS problem is equivalent to SDP\nminimize\nt + λ\n\nI\nP(x)\nq(x)\nsubject to\nP(x)T\nλI\nq(x)T\nt\nApproximation and fitting\n6-19\n\nexample: histogram of residuals\nr(u) = k(A0 + u1A1 + u2A2)x -bk2\nwith u uniformly distributed on unit disk, for three values of x\nr(u)\n- xls minimizes kA0x -bk2\n- xtik minimizes kA0x -bk2\n2 + δkxk2\n2 (Tikhonov solution)\n- xwc minimizes sup∥u∥2≤1 kA0x -bk2\n2 + kxk2\nApproximation and fitting\n6-20\nxls\nxtik\nxrls\nfrequency\n0.05\n0.1\n0.15\n0.2\n0.25\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/dbef8613c8b8c946eb258c25cb532a80_MIT6_079F09_lec07.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n7. Statistical estimation\n- maximum likelihood estimation\n- optimal detector design\n- experiment design\n7-1\n\nParametric distribution estimation\n- distribution estimation problem: estimate probability density p(y) of a\nrandom variable from observed values\n- parametric distribution estimation: choose from a family of densities\npx(y), indexed by a parameter x\nmaximum likelihood estimation\nmaximize (over x)\nlog px(y)\n- y is observed value\n- l(x) = log px(y) is called log-likelihood function\n- can add constraints x ∈ C explicitly, or define px(y) = 0 for x 6∈ C\n- a convex optimization problem if log px(y) is concave in x for fixed y\nStatistical estimation\n7-2\n\nLinear measurements with IID noise\nlinear measurement model\nyi = ai\nT x + vi,\ni = 1, . . . , m\n- x ∈ Rn is vector of unknown parameters\n- vi is IID measurement noise, with density p(z)\n∈ Rm\nQm\nT\n- yi is measurement: y\nhas density px(y) =\ni=1 p(yi -ai x)\nmaximum likelihood estimate: any solution x of\nPm\nmaximize\nl(x) =\ni=1 log p(yi -ai\nTx)\n(y is observed value)\nStatistical estimation\n7-3\n\nX\nX\n\nexamples\n- Gaussian noise N(0, σ2): p(z) = (2πσ2)-1/2e-z 2/(2σ2),\nm\nm\nT\nl(x) = -\nlog(2πσ2) -\n(ai x -yi)2\n2σ2\ni=1\nML estimate is LS solution\n- Laplacian noise: p(z) = (1/(2a))e-|z|/a,\nm\nl(x) = -m log(2a) -\n|ai\nT x -yi|\na i=1\nML estimate is l1-norm solution\n- uniform noise on [-a, a]:\n-m log(2a) |ai\nTx -yi| ≤a,\ni = 1, . . . , m\nl(x) =\n-inf\notherwise\nML estimate is any x with |ai\nTx -yi| ≤a\nStatistical estimation\n7-4\n\nX\nX\nLogistic regression\nrandom variable y ∈{0, 1} with distribution\nexp(aTu + b)\np = prob(y = 1) = 1 + exp(aTu + b)\n- a, b are parameters; u ∈ Rn are (observable) explanatory variables\n- estimation problem: estimate a, b from m observations (ui, yi)\nlog-likelihood function (for y1 = · · · = yk = 1, yk+1 = · · · = ym = 0):\n\nk\nT\nm\nY\nexp(a ui + b)\nY\nl(a, b) = log\n\n1 + exp(aTui + b)\n1 + exp(aTui + b)\ni=1\ni=k+1\nk\nm\n=\n(a T ui + b) -\nlog(1 + exp(a T ui + b))\ni=1\ni=1\nconcave in a, b\nStatistical estimation\n7-5\n\nexample (n = 1, m = 50 measurements)\nprob(y = 1)\n0.8\n0.6\n0.4\n0.2\nu\n- circles show 50 points (ui, yi)\n- solid curve is ML estimate of p = exp(au + b)/(1 + exp(au + b))\nStatistical estimation\n7-6\n\n(Binary) hypothesis testing\ndetection (hypothesis testing) problem\ngiven observation of a random variable X ∈{1, . . . , n}, choose between:\n- hypothesis 1: X was generated by distribution p = (p1, . . . , pn)\n- hypothesis 2: X was generated by distribution q = (q1, . . . , qn)\nrandomized detector\n- a nonnegative matrix T ∈ R2×n, with 1TT = 1T\n- if we observe X = k, we choose hypothesis 1 with probability t1k,\nhypothesis 2 with probability t2k\n- if all elements of T are 0 or 1, it is called a deterministic detector\nStatistical estimation\n7-7\n\ndetection probability matrix:\n\n1 -Pfp\nPfn\nD =\nTp\nTq\n=\nPfp\n1 -Pfn\n- Pfp is probability of selecting hypothesis 2 if X is generated by\ndistribution 1 (false positive)\n- Pfn is probability of selecting hypothesis 1 if X is generated by\ndistribution 2 (false negative)\nmulticriterion formulation of detector design\nminimize (w.r.t. R2 )\n(Pfp, Pfn) = ((Tp)2, (Tq)1)\n+\nsubject to\nt1k + t2k = 1,\nk = 1, . . . , n\ntik ≥ 0,\ni = 1, 2,\nk = 1, . . . , n\nvariable T ∈ R2×n\nStatistical estimation\n7-8\n\nscalarization (with weight λ > 0)\nminimize\n(Tp)2 + λ(Tq)1\nsubject to\nt1k + t2k = 1,\ntik ≥ 0,\ni = 1, 2,\nk = 1, . . . , n\nan LP with a simple analytical solution\n(1, 0) pk ≥ λqk\n(t1k, t2k) =\n(0, 1) pk < λqk\n- a deterministic detector, given by a likelihood ratio test\n- if pk = λqk for some k, any value 0 ≤ t1k ≤ 1, t1k = 1 -t2k is optimal\n(i.e., Pareto-optimal detectors include non-deterministic detectors)\nminimax detector\nminimize\nmax{Pfp, Pfn} = max{(Tp)2, (Tq)1}\nsubject to\nt1k + t2k = 1,\ntik ≥ 0,\ni = 1, 2,\nk = 1, . . . , n\nan LP; solution is usually not deterministic\nStatistical estimation\n7-9\n\nexample\n\n0.70 0.10\n0.20 0.10\n\nP = 0.05 0.70\n0.05 0.10\n0.8\n0.6\n0.4\n0.2\nPfp\nPfn\n0.2\n0.4\n0.6\n0.8\nsolutions 1, 2, 3 (and endpoints) are deterministic; 4 is minimax detector\nStatistical estimation\n7-10\n\nX\nX\nX\nExperiment design\nm linear measurements yi = ai\nTx + wi, i = 1, . . . , m of unknown x ∈ Rn\n- measurement errors wi are IID N(0, 1)\n- ML (least-squares) estimate is\n\n!-1\nm\nm\nxˆ =\naia T\ni\nyiai\ni=1\ni=1\n- error e = xˆ -x has zero mean and covariance\n\n!-1\nm\nE = E ee T =\naia T\ni\ni=1\nconfidence ellipsoids are given by {x | (x -xˆ)TE-1(x -xˆ) ≤ β}\nexperiment design: choose ai ∈{v1, . . . , vp} (a set of possible test\nvectors) to make E 'small'\nStatistical estimation\n7-11\n\nvector optimization formulation\nminimize (w.r.t. Sn )\nE = Pp\nT-1\n+\nk=1 mkvkvk\nsubject to\nmk ≥ 0,\nm1 + · · · + mp = m\nmk ∈ Z\n- variables are mk (# vectors ai equal to vk)\n- difficult in general, due to integer constraint\nrelaxed experiment design\nassume m ≫ p, use λk = mk/m as (continuous) real variable\nPp\nT\nminimize (w.r.t. Sn )\nE = (1/m)\nλkvkv\n-1\n+\nk=1\nk\nsubject to\nλ 0,\n1Tλ = 1\n- common scalarizations: minimize log det E, tr E, λmax(E), . . .\n- can add other convex constraints, e.g., bound experiment cost cTλ ≤ B\nStatistical estimation\n7-12\n\nD-optimal design\nminimize\nlog det\nP\nk\np\n=1 λkvkvk\nT-1\nsubject to\nλ 0,\n1Tλ = 1\ninterpretation: minimizes volume of confidence ellipsoids\ndual problem\nmaximize\nlog det W + n log n\nsubject to\nvk\nTWvk ≤ 1,\nk = 1, . . . , p\ninterpretation: {x | xTWx ≤ 1} is minimum volume ellipsoid centered at\norigin, that includes all test vectors vk\ncomplementary slackness: for λ, W primal and dual optimal\nλk(1 -vk\nTWvk) = 0,\nk = 1, . . . , p\noptimal experiment uses vectors vk on boundary of ellipsoid defined by W\nStatistical estimation\n7-13\n\nexample (p = 20)\nλ1\n0.5\n= 0.5\nλ2 =\ndesign uses two vectors, on boundary of ellipse defined by optimal W\nStatistical estimation\n7-14\n\n!!\nX\nderivation of dual of page 7-13\nfirst reformulate primal problem with new variable X:\nminimize\nlog det X-1\nsubject to\nX = P\nk\np\n=1 λkvkvk\nT ,\nλ 0,\n1Tλ = 1\np\nL(X, λ, Z, z, ν) = log det X-1+tr Z\nX -\nλkvkvk\nT\n-z Tλ+ν(1Tλ-1)\nk=1\n- minimize over X by setting gradient to zero: -X-1 + Z = 0\n- minimum over λk is -inf unless -vk\nTZvk -zk + ν = 0\ndual problem\nmaximize\nn + log det Z -ν\nsubject to\nvk\nTZvk ≤ ν,\nk = 1, . . . , p\nchange variable W = Z/ν, and optimize over ν to get dual of page 7-13\nStatistical estimation\n7-15\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/01bc28bea61ac801e76e9bcdf7971cdb_MIT6_079F09_lec08.pdf",
      "content": "Convex Optimization -- Boyd & Vandenberghe\n8. Geometric problems\n- extremal volume ellipsoids\n- centering\nclassification\n-\n- placement and facility location\n8-1\n\nMinimum volume ellipsoid around a set\nL owner-John ellipsoid of a set C: minimum volume ellipsoid E s.t. C ⊆E\n- parametrize E as E\n| kAv + bk2 ≤1}; w.l.o.g. assume A ∈Sn\n= {v\n++\nvol E is proportional to det A-1; to compute minimum volume ellipsoid,\n-\nminimize (over A, b)\nlog det A-1\nsubject to\nsupv∈C kAv + bk2 ≤1\nconvex, but evaluating the constraint can be hard (for general C)\nfinite set C = {x1, . . . , xm}:\nminimize (over A, b)\nlog det A-1\nsubject to\nkAxi + bk2 ≤1,\ni = 1, . . . , m\nalso gives L owner-John ellipsoid for polyhedron conv{x1, . . . , xm}\nGeometric problems\n8-2\n\nMaximum volume inscribed ellipsoid\nmaximum volume ellipsoid E inside a convex set C ⊆Rn\n- parametrize E as E = {Bu + d | kuk2 ≤1}; w.l.o.g. assume B ∈Sn\n++\n- vol E is proportional to det B; can compute E by solving\nmaximize\nlog det B\nsubject to\nsup∥u∥2≤1 IC(Bu + d) ≤0\n(where IC(x) = 0 for x ∈C and IC(x) = inffor x 6∈ C)\nconvex, but evaluating the constraint can be hard (for general C)\npolyhedron {x | ai\nT x ≤bi, i = 1, . . . , m}:\nmaximize\nlog det B\nsubject to\nkBaik2 + ai\nT d ≤bi,\ni = 1, . . . , m\n(constraint follows from sup∥u∥2≤1 aT (Bu + d) = kBaik2 + aT d)\ni\ni\nGeometric problems\n8-3\n\nEfficiency of ellipsoidal approximations\nC ⊆Rn convex, bounded, with nonempty interior\n- L owner-John ellipsoid, shrunk by a factor n, lies inside C\n- maximum volume inscribed ellipsoid, expanded by a factor n, covers C\nexample (for two polyhedra in R2)\nfactor n can be improved to √n if C is symmetric\nGeometric problems\n8-4\n\nCentering\nsome possible definitions of 'center' of a convex set C:\n- center of largest inscribed ball ('Chebyshev center')\nfor polyhedron, can be computed via linear programming (page 4-19)\n- center of maximum volume inscribed ellipsoid (page 8-3)\nxcheb\nxcheb\nxmve\nMVE center is invariant under affine coordinate transformations\nGeometric problems\n8-5\n\nAnalytic center of a set of inequalities\nthe analytic center of set of convex inequalities and linear equations\nfi(x) ≤0,\ni = 1, . . . , m,\nFx = g\nis defined as the optimal point of\nminimize\n- Pm log(-fi(x))\ni=1\nsubject to\nFx = g\n- more easily computed than MVE or Chebyshev center (see later)\n- not just a property of the feasible set: two sets of inequalities can\ndescribe the same set, but have different analytic centers\nGeometric problems\n8-6\n\nanalytic center of linear inequalities ai\nT x ≤bi, i = 1, . . . , m\nxac is minimizer of\nm\nX\nxac\nφ(x) = -\nlog(bi -ai\nT x)\ni=1\ninner and outer ellipsoids from analytic center:\nEinner ⊆{x | ai\nT x ≤bi, i = 1, . . . , m} ⊆Eouter\nwhere\nEinner\n= {x | (x -xac)T ∇ 2φ(xac)(x -xac) ≤1}\nEouter\n= {x | (x -xac)T ∇ 2φ(xac)(x -xac) ≤m(m -1)}\nGeometric problems\n8-7\n\nLinear discrimination\nseparate two sets of points {x1, . . . , xN }, {y1, . . . , yM } by a hyperplane:\na T xi + b > 0,\ni = 1, . . . , N,\na T yi + b < 0,\ni = 1, . . . , M\nhomogeneous in a, b, hence equivalent to\na T xi + b ≥1,\ni = 1, . . . , N,\na T yi + b ≤-1,\ni = 1, . . . , M\na set of linear inequalities in a, b\nGeometric problems\n8-8\n\nRobust linear discrimination\n(Euclidean) distance between hyperplanes\nH1\n= {z | a T z + b = 1}\nH2\n= {z | a T z + b = -1}\nis dist(H1, H2) = 2/kak2\nto separate two sets of points by maximum margin,\nminimize\n(1/2)kak2\nsubject to\naT xi + b ≥1,\naT yi + b ≤-1,\n(after squaring objective) a QP in a, b\ni = 1, . . . , N\ni = 1, . . . , M\n(1)\nGeometric problems\n8-9\n\nLagrange dual of maximum margin separation problem (1)\nmaximize\n1T λ + 1T μ\nPN\nPM\n\nsubject to\ni=1 λixi -\ni=1 μiyi\n2 ≤1\n(2)\n1T λ = 1T μ,\nλ 0,\nμ 0\nfrom duality, optimal value is inverse of maximum margin of separation\ninterpretation\n- change variables to θi = λi/1T λ, γi = μi/1T μ, t = 1/(1T λ + 1T μ)\n- invert objective to minimize 1/(1T λ + 1T μ) = t\nminimize\nt\nPN\nPM\n\nsubject to\n\ni=1 θixi -\ni=1 γiyi\n2 ≤t\nθ 0,\n1T θ = 1,\nγ 0,\n1T γ = 1\noptimal value is distance between convex hulls\nGeometric problems\n8-10\n\nApproximate linear separation of non-separable sets\nminimize\n1T u + 1T v\nsubject to\naT xi + b ≥1 -ui,\ni = 1, . . . , N\naT yi + b ≤-1 + vi,\ni = 1, . . . , M\nu 0,\nv 0\n- an LP in a, b, u, v\n- at optimum, ui = max{0, 1 -aT xi -b}, vi = max{0, 1 + aT yi + b}\n- can be interpreted as a heuristic for minimizing #misclassified points\nGeometric problems\n8-11\n\nSupport vector classifier\nminimize\nkak2 + γ(1T u + 1T v)\nsubject to\naT xi + b ≥1 -ui,\ni = 1, . . . , N\naT yi + b ≤-1 + vi,\ni = 1, . . . , M\nu 0,\nv 0\nproduces point on trade-off curve between inverse of margin 2/kak2 and\nclassification error, measured by total slack 1T u + 1T v\nsame example as previous page,\nwith γ = 0.1:\nGeometric problems\n8-12\n\nNonlinear discrimination\nseparate two sets of points by a nonlinear function:\nf(xi) > 0,\ni = 1, . . . , N,\nf(yi) < 0,\ni = 1, . . . , M\n- choose a linearly parametrized family of functions\nf(z) = θT F(z)\nF = (F1, . . . , Fk) : Rn\nRk are basis functions\n→\n- solve a set of linear inequalities in θ:\nθT F(xi) ≥1,\ni = 1, . . . , N,\nθT F(yi) ≤-1,\ni = 1, . . . , M\nGeometric problems\n8-13\n\nquadratic discrimination: f(z) = zT Pz + qT z + r\nx T Pxi + q T xi + r ≥1,\ny T Pyi + q T yi + r ≤-1\ni\ni\ncan add additional constraints (e.g., P -I to separate by an ellipsoid)\npolynomial discrimination: F(z) are all monomials up to a given degree\nseparation by ellipsoid\nseparation by 4th degree polynomial\nGeometric problems\n8-14\n\nP\nPlacement and facility location\nN points with coordinates xi ∈R2 (or R3)\n-\n- some positions xi are given; the other xi's are variables\n- for each pair of points, a cost function fij(xi, xj)\nplacement problem\nminimize\ni\nfij(xi, xj)\n=j\nvariables are positions of free points\ninterpretations\n- points represent plants or warehouses; fij is transportation cost between\nfacilities i and j\n- points represent cells on an IC; fij represents wirelength\nGeometric problems\n8-15\n\nP\nexample: minimize\n(i,j)∈A h(kxi -xjk2), with 6 free points, 27 links\noptimal placement for h(z) = z, h(z) = z2 , h(z) = z4\n-1\n-1\n-1\n-1\n-1\n-1\nhistograms of connection lengths kxi -xjk2\n0.5\n1.5\n0.5\n1.5\n0.5\n1.5\nGeometric problems\n8-16\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec09.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/40b33990b6fdba13b9d464c143b07fb4_MIT6_079F09_lec09.pdf",
      "content": "Filter design\nFIR filters\n-\n- Chebychev design\n- linear phase filter design\n- equalizer design\n- filter magnitude specifications\n\nFIR filters\nfinite impulse response (FIR) filter:\nn-1\ny(t) =\nX\nhτu(t -τ),\nt ∈Z\nτ=0\n(sequence) u : Z\nR is input signal\n-\n→\n(sequence) y : Z\nR is output signal\n-\n→\n- hi are called filter coefficients\n- n is filter order or length\nFilter design\n\nX\nX\nfilter frequency response: H : R\nC\n→\nH(ω)\n= h0 + h1e -iω +\n+ hn-1e -i(n-1)ω\n· · ·\nn-1\nn-1\n=\nht cos tω + i\nht sin tω\nt=0\nt=0\n(EE tradition uses j = √-1 instead of i)\n-\n- H is periodic and conjugate symmetric, so only need to know/specify\nfor 0 ≤ω ≤π\nFIR filter design problem: choose h so it and H satisfy/optimize specs\nFilter design\n\nexample: (lowpass) FIR filter, order n = 21\nimpulse response h:\nh(t)\n0.2\n0.1\n-0.1\n-0.2\nt\nFilter design\n\nfrequency response magnitude (i.e., H(ω) ):\n|\n|\n0.5\n1.5\n2.5\n|H(ω)|\n-1\n-2\n-3\nω\nfrequency response phase (i.e., 6 H(ω)):\nH(ω)\n0.5\n1.5\n2.5\n-3\n-2\n-1\nω\nFilter design\n\nChebychev design\nminimize max H(ω) -Hdes(ω)\nω∈[0,π] |\n|\n- h is optimization variable\nHdes : R\nC is (given) desired transfer function\n-\n→\n- convex problem\n- can add constraints, e.g., |hi| ≤1\nsample (discretize) frequency:\nminimize\nmax\nk=1,...,m |H(ωk) -Hdes(ωk)|\n- sample points 0 ≤ω1 < · · · < ωm ≤π are fixed (e.g., ωk = kπ/m)\n- m ≫n (common rule-of-thumb: m = 15n)\n- yields approximation (relaxation) of problem above\nFilter design\n\nChebychev design via SOCP:\nminimize\nt\nsubject to\nA(k)h -b(k) ≤t,\nk = 1, . . . , m\nwhere\ncos ωk\ncos(n-1)ωk\n\nA(k)\n=\n· · ·\n0 -sin ωk\n-sin(n-1)ωk\n· · ·\n\nRHdes(ωk)\n\nb(k)\n=\nIHdes(ωk)\n\nh0\n\n.\nh\n=\n..\n\nhn-1\nFilter design\n\nLinear phase filters\nsuppose\nn = 2N + 1 is odd\n-\n- impulse response is symmetric about midpoint:\nht = hn-1-t,\nt = 0, . . . , n -1\nthen\nH(ω) = h0 + h1e -iω +\n+ hn-1e -i(n-1)ω\n· · ·\n= e -iNω (2h0 cos Nω + 2h1 cos(N -1)ω +\n+ hN)\n· · ·\nΔ\n-iNω\n= e\nHe(ω)\nFilter design\n\nterm e-iNω represents N-sample delay\n-\n- He(ω) is real\n- |H(ω)| = |He(ω)|\n- called linear phase filter (6 H(ω) is linear except for jumps of ±π)\nFilter design\n\nLowpass filter specifications\nδ1\n1/δ1\nδ2\nωp\nωs\nπ\nω\nidea:\n- pass frequencies in passband [0, ωp]\n- block frequencies in stopband [ωs, π]\nFilter design\n\nspecifications:\n- maximum passband ripple (±20 log10 δ1 in dB):\n1/δ1 ≤|H(ω)| ≤δ1,\n0 ≤ω ≤ωp\n- minimum stopband attenuation (-20 log10 δ2 in dB):\n|H(ω)| ≤δ2,\nωs ≤ω ≤π\nFilter design\n\nLinear phase lowpass filter design\n- sample frequency\n- can assume wlog He(0) > 0, so ripple spec is\n1/δ1 ≤He(ωk) ≤δ1\ndesign for maximum stopband attenuation:\nminimize\nδ2\nsubject to\n1/δ1 ≤He(ωk) ≤δ1,\n0 ≤ωk ≤ωp\n-δ2 ≤He(ωk) ≤δ2,\nωs ≤ωk ≤π\nFilter design\n\n- passband ripple δ1 is given\n- an LP in variables h, δ2\n- known (and used) since 1960's\n- can add other constraints, e.g., |hi| ≤α\nvariations and extensions:\n- fix δ2, minimize δ1 (convex, but not LP)\n- fix δ1 and δ2, minimize ωs (quasiconvex)\n- fix δ1 and δ2, minimize order n (quasiconvex)\nFilter design\n\nexample\n- linear phase filter, n = 21\n- passband [0, 0.12π]; stopband [0.24π, π]\n- max ripple δ1 = 1.012 (±0.1dB)\n- design for maximum stopband attenuation\nimpulse response h:\nh(t)\n0.2\n0.1\n-0.1\n-0.2\nt\nFilter design\n\nfrequency response magnitude (i.e., H(ω) ):\n|\n|\n0.5\n1.5\n2.5\n|H(ω)|\n-1\n-2\n-3\nω\nfrequency response phase (i.e., 6 H(ω)):\n0.5\n1.5\n2.5\n-3\n-2\n-1\nω\n6 H(ω)\nFilter design\n\nEqualizer design\nG(ω)\nH(ω)\nequalization: given\n- G (unequalized frequency response)\n- Gdes (desired frequency response)\nΔ\ndesign (FIR equalizer) H so that Ge = GH ≈Gdes\ncommon choice: Gdes(ω) = e-iDω (delay)\n-\ni.e., equalization is deconvolution (up to delay)\n- can add constraints on H, e.g., limits on |hi| or maxω |H(ω)|\nFilter design\n\nChebychev equalizer design:\nminimize max\nGe(ω) -Gdes(ω)\n\nω∈[0,π]\nconvex; SOCP after sampling frequency\nFilter design\n\ntime-domain equalization: optimize impulse response g of equalized\nsystem\ne.g., with Gdes(ω) = e-iDω ,\n\n1 t = D\ngdes(t) =\n0 t = D\nsample design:\nminimize\nmaxt6\ng (t)\n=D |\n|\nsubject to\ng (D) = 1\nan LP\n-\n- can use P\nt6\ng (t)2 or P\nt6\n|g (t)|\n=D\n=D\nFilter design\n\nextensions:\n- can impose (convex) constraints\n- can mix time- and frequency-domain specifications\n- can equalize multiple systems, i.e., choose H so\nG(k)H ≈Gdes,\nk = 1, . . . , K\n- can equalize multi-input multi-output systems\n(i.e., G and H are matrices)\n- extends to multidimensional systems, e.g., image processing\nFilter design\n\nEqualizer design example\nunequalized system G is 10th order FIR:\ng(t)\n0.8\n0.6\n0.4\n0.2\n-0.2\n-0.4\nt\nFilter design\n\n6 G(ω)\n|G(ω)|\n-1\n0.5\n1.5\n2.5\nω\n0.5\n1.5\n2.5\n-3\n-2\n-1\nω\ndesign 30th order FIR equalizer with Ge(ω) ≈e-i10ω\nFilter design\n\nChebychev equalizer design:\n\nminimize max\nG(ω) -e -i10ω\nω\nequalized system impulse response g\ng (t)\n0.8\n0.6\n0.4\n0.2\n-0.2\nt\nFilter design\n\nequalized frequency response magnitude |Ge|\n|Ge(ω)|\n-1\n0.5\n1.5\n2.5\nω\nequalized frequency response phase 6 Ge\n0.5\n1.5\n2.5\n6 Ge(ω)\n-1\n-2\n-3\nω\nFilter design\n\ntime-domain equalizer design:\nminimize max g (t)\n=10\nt6\n|\n|\nequalized system impulse response g\ng (t)\n0.8\n0.6\n0.4\n0.2\n-0.2\nt\nFilter design\n\nequalized frequency response magnitude |Ge|\n|Ge(ω)|\n-1\n0.5\n1.5\n2.5\nω\nequalized frequency response phase 6 Ge\nGe(ω)\n-1\n-2\n-3\n0.5\n1.5\n2.5\nω\nFilter design\n\nFilter magnitude specifications\ntransfer function magnitude spec has form\nL(ω) ≤|H(ω)| ≤U(ω),\nω ∈[0, π]\nwhere L, U : R\nR+ are given\n→\nlower bound is not convex in filter coefficients h\n-\n- arises in many applications, e.g., audio, spectrum shaping\n- can change variables to solve via convex optimization\nFilter design\n\nAutocorrelation coefficients\nautocorrelation coefficients associated with impulse response\nh = (h0, . . . , hn-1) ∈Rn are\nrt =\nX\nhτhτ+t\nτ\n(we take hk = 0 for k < 0 or k ≥n)\n- rt = r-t; rt = 0 for |t| ≥n\n- hence suffices to specify r = (r0, . . . , rn-1) ∈Rn\nFilter design\n\nX\nFourier transform of autocorrelation coefficients is\nn-1\nX\nτ\n=1\nt\nalways have R(ω)\n0 for all ω\n≥\n-\ncan express magnitude specification as\n-\n-iωτ\nR(ω) =\n2rt cos ωt = H(ω)\n|\n|\ne\nrτ = r0 +\nL(ω)2 ≤R(ω) ≤U(ω)2 ,\nω ∈[0, π]\n. . . convex in r\nFilter design\n\nSpectral factorization\nquestion: when is r ∈Rn the autocorrelation coefficients of some h ∈Rn?\nanswer: (spectral factorization theorem) if and only if R(ω) ≥0 for all ω\n- spectral factorization condition is convex in r\nmany algorithms for spectral factorization, i.e., finding an h s.t.\n-\nR(ω) = |H(ω)|2\nmagnitude design via autocorrelation coefficients:\n- use r as variable (instead of h)\n- add spectral factorization condition R(ω) ≥0 for all ω\n- optimize over r\n- use spectral factorization to recover h\nFilter design\n\nlog-Chebychev magnitude design\nchoose h to minimize\nmax 20 log10 H(ω) -20 log10 D(ω)\nω\n|\n|\n|\n|\n- D is desired transfer function magnitude\n(D(ω) > 0 for all ω)\n- find minimax logarithmic (dB) fit\nreformulate as\nminimize\nt\nsubject to\nD(ω)2/t ≤R(ω) ≤tD(ω)2 ,\n0 ≤ω ≤π\n- convex in variables r, t\n- constraint includes spectral factorization condition\nFilter design\n\nexample: 1/f (pink noise) filter (i.e., D(ω) = 1/√ω), n = 50,\nlog-Chebychev design over 0.01π ≤ω ≤π\n|H(ω)| 2\n-1\n-2\n-1\nω\noptimal fit: ±0.5dB\nFilter design\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_079F09_lec10.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-079-introduction-to-convex-optimization-fall-2009/9050ed14e4e85dedb159ffd9306d9c1d_MIT6_079F09_lec10.pdf",
      "content": "Convex optimization examples\n- multi-period processor speed scheduling\n- minimum time optimal control\n- grasp force optimization\n- optimal broadcast transmitter power allocation\n- phased-array antenna beamforming\n- optimal receiver location\n\nX\nMulti-period processor speed scheduling\nprocessor adjusts its speed st ∈[smin, smax] in each of T time periods\n-\nPT\nenergy consumed in period t is φ(st); total energy is E =\nφ(st)\n-\nt=1\n- n jobs\n- job i available at t = Ai; must finish by deadline t = Di\n- job i requires total work Wi ≥0\n- θti ≥0 is fraction of processor effort allocated to job i in period t\nDi\n1Tθt = 1,\nθtist ≥Wi\nt=Ai\n- choose speeds st and allocations θti to minimize total energy E\n\nX\nX\nP\nMinimum energy processor speed scheduling\n- work with variables Sti = θtist\nn\nDi\nst =\nSti,\nSti ≥Wi\ni=1\nt=Ai\n- solve convex problem\nPT\nminimize\nE =\nt=1 φ(st)\nsubject to\nsmin ≤s\nnt ≤smax ,\nt = 1, . . . , T\nst =\ni=1 Sti,\nt = 1, . . . , T\nPDi\nSti ≥Wi,\ni = 1, . . . , n\nt=Ai\n- a convex problem when φ is convex\n- can recover θt\n⋆ as θ⋆ = (1/s⋆\nt)S⋆\nti\nti\n\nExample\n- T = 16 periods, n = 12 jobs\nsmin = 1, smax = 6, φ(st) = s2\n-\nt\n- jobs shown as bars over [Ai, Di] with area ∝Wi\nφt (st )\njob i\nst\nt\n\nOptimal and uniform schedules\nuniform schedule: Sti = Wi/(Di -Ai + 1); gives Eunif = 204.3\n-\n-\nti; gives\noptimal schedule: S⋆\nE⋆ = 167.1\noptimal\nuniform\nst\ns t\nt\nt\n\nMinimum-time optimal control\n- linear dynamical system:\nxt+1 = Axt + But,\nt = 0, 1, . . . , K,\nx0 = x init\n- inputs constraints:\numin ut umax,\nt = 0, 1, . . . , K\n- minimum time to reach state xdes:\nf(u0, . . . , uK) = min {T | xt = xdes for T ≤t ≤K + 1}\n\nstate transfer time f is quasiconvex function of (u0, . . . , uK):\nf(u0, u1, . . . , uK) ≤T\nif and only if for all t = T, . . . , K + 1\nxt = At x init + At-1Bu0 +\n+ But-1 = xdes\n· · ·\ni.e., sublevel sets are affine\nminimum-time optimal control problem:\nminimize\nf(u0, u1, . . . , uK)\nsubject to\numin ut umax,\nt = 0, . . . , K\nwith variables u0, . . . , uK\na quasiconvex problem; can be solved via bisection\n\nMinimum-time control example\nu1\nu2\n- force (ut)1 moves object modeled as 3 masses (2 vibration modes)\n- force (ut)2 used for active vibration suppression\n- goal: move object to commanded position as quickly as possible, with\n|(ut)1| ≤1,\n|(ut)2| ≤0.1,\nt = 0, . . . , K\n\n(xt )3\nIgnoring vibration modes\n- treat object as single mass; apply only u1\n- analytical ('bang-bang') solution\n-2\n0.5\n1.5\n2.5\nt\n(ut ) 1\n(ut ) 2\n0.5\n-0.5\n-1\n-2\nt\n0.1\n0.05\n-0.05\n-0.1\n-2\nt\n\n0.5\n2.5\nWith vibration modes\n- no analytical solution\n- a quasiconvex problem; solved using bisection\n-2\nt\n(ut ) 2\n(ut ) 1\n-1\n-2\n1.5\nt\n0.1\n0.5\n-0.5\n0.05\n-0.05\n-0.1\n-2\nt\n(xt )3\n\nP\nGrasp force optimization\n- choose K grasping forces on object\n- resist external wrench\n- respect friction cone constraints\n- minimize maximum grasp force\n- convex problem (second-order cone program):\nminimize\nmaxi kf (i)k2\nmax contact force\nsubject to\nP\ni Q(i)f (i) = f ext\nforce equillibrium\ni p(i) × (Q(i)f (i)) = τ ext\ntorque equillibrium\nμif3\n(i) ≥\n\nf1\n(i)2 + f2\n(i)2 1/2\nfriction cone contraints\nvariables f (i) ∈R3 , i = 1, . . . , K (contact forces)\n\nExample\n\nOptimal broadcast transmitter power allocation\n- m transmitters, mn receivers all at same frequency\n- transmitter i wants to transmit to n receivers labeled (i, j), j = 1, . . . , n\n- Aijk is path gain from transmitter k to receiver (i, j)\n- Nij is (self) noise power of receiver (i, j)\n- variables: transmitter powers pk, k = 1, . . . , m\ntransmitter i\ntransmitter k\nreceiver (i, j)\n\nX\nat receiver (i, j):\n- signal power:\nSij = Aijipi\n- noise plus interference power:\nIij =\nAijkpk + Nij\nk=i\n- signal to interference/noise ratio (SINR): Sij/Iij\nproblem: choose pi to maximize smallest SINR:\nAijipi\nmaximize\nmin P\ni,j\nk\nAijkpk + Nij\n=i\nsubject to\n0 ≤pi ≤pmax\n. . . a (generalized) linear fractional program\n\nPhased-array antenna beamforming\n(xi, yi)\nθ\n- omnidirectional antenna elements at positions (x1, y1), . . . , (xn, yn)\nunit plane wave incident from angle θ induces in ith element a signal\n-\nj(xi cos θ+yi sin θ-ωt)\ne\n(j = √-1, frequency ω, wavelength 2π)\n\nX\ndemodulate to get output ej(xi cos θ+yi sin θ) ∈C\n-\n- linearly combine with complex weights wi:\nn\ny(θ) =\nwiej(xi cos θ+yi sin θ)\ni=1\n- y(θ) is (complex) antenna array gain pattern\n- |y(θ)| gives sensitivity of array as function of incident angle θ\n- depends on design variables Re w, Im w\n(called antenna array weights or shading coefficients)\ndesign problem: choose w to achieve desired gain pattern\n\nP\nSidelobe level minimization\nmake |y(θ)| small for |θ -θtar| > α\n(θtar: target direction; 2α: beamwidth)\nvia least-squares (discretize angles)\nminimize\ni y(θi) 2\n|\n|\nsubject to\ny(θtar) = 1\n(sum is over angles outside beam)\nleast-squares problem with two (real) linear equality constraints\n\nθtar = 30*\n10*\n50*\n@\n@\n@\nR\n|y(θ)|\n@@\nR\nsidelobe level\n\nminimize sidelobe level (discretize angles)\nminimize\nmaxi y(θi)\n|\n|\nsubject to\ny(θtar) = 1\n(max over angles outside beam)\ncan be cast as SOCP\nminimize\nt\nsubject to\ny(θi)\nt\n|\n| ≤\ny(θtar) = 1\n\nθtar = 30*\n10*\n50*\n@\n@\n@\nR\n|y(θ)|\n@@\nR\nsidelobe level\n\nExtensions\nconvex (& quasiconvex) extensions:\n- y(θ0) = 0 (null in direction θ0)\n- w is real (amplitude only shading)\n- |wi| ≤1 (attenuation only shading)\n- minimize σ2 P\ni\nn\n=1 |wi|2 (thermal noise power in y)\n- minimize beamwidth given a maximum sidelobe level\nnonconvex extension:\n- maximize number of zero weights\n\nOptimal receiver location\n- N transmitter frequencies 1, . . . , N\ntransmitters at locations ai, bi ∈R2 use frequency i\n-\n- transmitters at a1, a2, . . . , aN are the wanted ones\n- transmitters at b1, b2, . . . , bN are interfering\nreceiver at position x ∈R2\n-\nq b3\na3 a\na2 a\nq b2\nx\na1 a\nq b1\n\n-α\n- (signal) receiver power from ai: kx -aik2\n(α ≈2.1)\n- (interfering) receiver power from bi: kx -bik -α (α ≈2.1)\n- worst signal to interference ratio, over all frequencies, is\n-α\nS/I = min kx -aik2\n-α\ni\nkx -bik2\n- what receiver location x maximizes S/I?\n\nS/I is quasiconcave on {x | S/I ≥1}, i.e., on\n{x | kx -aik2 ≤kx -bik2, i = 1, . . . , N}\na1 a\na2 a\na3 a\nq b3\nq b2\nq b1\ncan use bisection; every iteration is a convex quadratic feasibility problem\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.079 / 6.975 Introduction to Convex Optimization\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}