{
  "course_name": "Topics in Mathematics of Data Science",
  "course_description": "This is a mostly self-contained research-oriented course designed for undergraduate students (but also extremely welcoming to graduate students) with an interest in doing research in theoretical aspects of algorithms that aim to extract information from data. These often lie in overlaps of two or more of the following: Mathematics, Applied Mathematics, Computer Science, Electrical Engineering, Statistics, and / or Operations Research.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Mathematics",
    "Applied Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Mathematics",
    "Applied Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\nWorking knowledge of\n18.06SC Linear Algebra\nand\n18.05 Introduction to Probability and Statistics\nis required. Some familiarity with the basics of optimization and algorithms is also recommended.\n\nDescription\n\nThis is a mostly self-contained research-oriented course designed for undergraduate students (but also extremely welcoming to graduate students) with an interest in doing research in theoretical aspects of algorithms that aim to extract information from data. These often lie in overlaps of two or more of the following: Mathematics, Applied Mathematics, Computer Science, Electrical Engineering, Statistics, and / or Operations Research.\n\nThe topics covered include:\n\nPrincipal Component Analysis (PCA) and some random matrix theory that will be used to understand the performance of PCA in high dimensions, through spike models.\n\nManifold Learning and Diffusion Maps: A nonlinear dimension reduction tool, alternative to PCA. Semisupervised Learning and its relations to Sobolev Embedding Theorem.\n\nSpectral Clustering and a guarantee for its performance: Cheeger's Inequality.\n\nConcentration of Measure and tail bounds in probability, both for scalar variables and matrix variables.\n\nDimension reduction through Johnson-Lindenstrauss Lemma and Gordon's Escape Through a Mesh Theorem.\n\nCompressed Sensing / Sparse Recovery, Matrix Completion, etc. If time permits, I will present Number Theory inspired constructions of measurement matrices.\n\nGroup Testing. Here we will use combinatorial tools to establish lower bounds on testing procedures and, if there is time, I might give a crash course on error-correcting codes and show a use of them in group testing.\n\nApproximation algorithms in Theoretical Computer Science and the Max-Cut problem.\n\nClustering on random graphs: Stochastic Block Model. Basics of duality in optimization.\n\nSynchronization, inverse problems on graphs, and estimation of unknown variables from pairwise ratios on compact groups.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\n40%\n\nProject\n\n60%\n\n40% of the grade is based on a handful of homework problem sets (to be handed out roughly bi-weekly). You are welcome to work on the problem sets in groups, but you have to write your own solutions.\n\n60% of the grade is based on a project. The project (which can be done individually or in groups of two) can be a literature review, but I would recommended attempting to do original research, either by trying to make partial progress on (or completely solve!) one of the open problems, or by pursuing another research direction. The project report is due on the last week of classes. A preliminary abstract will be due roughly a month before the project due date and each group is expected to make a 5 minute presentation on class about their project before the due date.",
  "files": [
    {
      "category": "Assignment",
      "title": "Topics in Mathematics of Data Science Assignment 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/001f7340ddb813766def0f2e64d2c8a7_MIT18_S096F15_Homework_1.pdf",
      "content": "18.S096: Homework Problem Set 1 (revised)\nTopics in Mathematics of Data Science (Fall 2015)\nAfonso S. Bandeira\nDue on October 6, 2015\nExtended to: October 8, 2015\nThis homework problem set is due on October 6, at the start of class\nTry not to look up the answers, you'll learn much more if you try to\nthink about the problems without looking up the solutions.\nYou can work in groups but each student must write his/her own\nsolution based on his/her own understanding of the problem.\nIf you need to impose extra conditions on a problem to make it easier,\nstate explicitly that you have done so. Solutions where extra conditions\nwere assumed will also be graded (probably scored as a partial answer).\n1.1\nLinear Algebra\nProblem 1.1 Show the resut we used in class: If M ∈Rn×n is a symmetric\nmatrix and d ≤n then\nmax\nTr\nUT MU\nU∈Rn×d\nUT U=Id×d\n\nX\nd\n(+)\n=\nλ\n(M),\nk\nk=1\n(+)\nwhere λ\nis the largest k-th eigenvalue of M.\nk\n.\n\n1.2\nEstimators\nProblem 1.2 Given x1, · · · , xn i.i.d. samples from a distribution X with\nmean μ and covariance Σ, show that\nμn =\nX\nn\nxk,\nand\nΣn =\nn k=1\nX\nn\nT\n(xk\nμn) (xk\nμn) ,\nn\n-\n-\n-1 k=1\nare unbiased estimators for μ and Σ, i.e., show that E [μn] = μ and E [Σn] =\nΣ.\n1.3\nRandom Matri\nRecall the definition of a\nrandom matrix W ∈Rn\nindependent Wii ∼N(0,\ntrix emsemble is invaria\nany U ∈O(n). Also, the\nto the so-called semicirc\nces\nstandard gaussian Wigner Matrix W: a symmetric\n×n whose diagonal and upper-diagonal entries are\n2) and, for i < j, Wij ∼N(0, 1). This random ma-\nnt under orthogonal conjugation: UT WU ∼W for\ndistribution of the eigenvalues of\n√W converges\nn\nular law with support [-2, 2]\ndSC(x) =\np\n4 -x21[-2,2](x).\n(try it in\ndraw an histogram of the distribution of the eigenvalues\nof\n√W for, say n = 500.)\nn\nIn the next problem, you will show that the largest eigenvalue of\n√W\nn\nhas expected value at most 2.1\nFor that, we will make use of Slepian's\nComparison Lemma.\nSlepian's Comparison Lemma is a crucial tool to compare Gaussian Pro-\ncesses. A Gaussian process is a family of gaussian random variables indexed\nby some set T, more precisely is a family of gaussian random variables\n{Xt}t T (if T is finite this is simply a gaussian vector). Given a gaussian pro-\n∈\ncess Xt, a particular quantity of interest is E [maxt T X\nIn\n∈\nt].\ntuitively, if we\nhave two Gaussian processes Xt and\nYt with mean zero E [Xt] = E [Yt] = 0,\nfor all t ∈T and same variances E X2\nt\n\n= E Y 2\nt\nthen the process that has\nthe \"least correlations\" should have a larger maxim\n\num (think the maximum\nentry of vector with i.i.d. gaussian entries versus one always with the same\n1Note that, a priori, there could be a very large eigenvalue and it would still not\ncontradict the semicircular law, since it does not predict what happens to a vanishing\nfraction of the eigenvalues.\nMATLAB(r)\n\ngaussian entry). A simple version of Slepian's Lemma makes this intuition\nprecise:2\nIn the conditions above, if for all t1, t2 ∈T\nE [Xt1Xt2] ≤E [Yt1Yt2] ,\nthen\nE\n\nmax Xt\nt∈T\n\n≥E\n\nmax Yt\nt∈T\n\n.\nA slightly more general version of it asks that the two Gaussian pro-\ncesses Xt and Yt have mean zero E [Xt] = E [Yt] = 0, for all t ∈T but not\nnecessarily the same variances. In that case it says that: If or all t1, t2 ∈T\nE\n[Xt1 -\nXt2] ≥E [Yt1 -Yt2] ,\n(1)\nthen\nE\n\nmax Xt\n\n≥E\n\nmax Yt .\nt∈T\nt∈T\n\nProblem 1.3 We will use Slepian's Comparison Lemma to show that\nEλmax(W) ≤2√n.\n1. Note that\nλmax(W) =\nmax\nvT Wv,\nv: ∥v∥2=1\nwhich means that, if we take for unit-norm v, Yv := vT Wv we have\nthat\nλmax(W) = E\n\nmax Yv ,\nv∈Sn-1\n\n2. Use Slepian to compare Yv with 2Xv defined as\nXv = vT g,\nwhere g ∼N (0, In×n)\n3. Use Jensen's inequality to upperbound E [maxv\n.\n∈n-1 Xv]\nS\n2Although intuitive in some sense, this is a delicate statement about Gaussian random\nvariables, it turns out not to hold for other distributions.\n\nProblem 1.4 In this problem you'll derive the limit of the largest eigenvalue\nof a rank 1 perturbation of a Wigner matrix.\nFor this problem, you don't have to justify all of the steps rig-\norously.\nYou can use the same level of rigor that was used in\nclass to derive the analogue result for sample covariance matri-\nces. Deriving this phenomena rigorously would take considerably\nmore work and is outside of the scope of this homework.\nConsider the matrix M =\n√W + βvvT for ∥v\nn\n∥2 = 1 and W a stan-\ndard Gaussian Wigner matrix. The purpose of this homework problem is to\nunderstand the behavior of λmax(M). Because W is invariant to orthogonal\nconjugation we can focus on understanding\nλmax\n√W + βe1eT\nn\n\n.\nUse the same techniques as used in class to derive the behavior of this\nquantity.\n√\n(Hint: at some point, you'll probably have to inte ate\nR 2\ngr\n-2\n4-x2\ny-x dx. You\ncan use the fact that, for y > 2,\nR 2\n-2\n√\n4-x2\ny-x dx = π\n\ny -\np\ny2 -4\n(you can\nalso use an integrator software, such as Mathematica, for this).\n\n1.4\nDiffusion Maps and other embeddings\nProblem 1.5 The ring graph on n nodes is a graph where node 1 < k <\nn is connected to node k -1 and k + 1 and node 1 is connected to node\nn. Derive the two-dimensional diffusion map embedding for the ring graph\n(if the eigenvectors are complex valued, try creating real valued ones using\nmultiplicity of the eigenvalues). Is it a reasonable embedding of this graph\nin two dimensions?\nProblem 1.6 (Multidimensional Scaling\nRevised) Suppose you want\nto represent n data points in Rd and all you are given is estimates for their\nEuclidean distances δij ≈∥xi -xj∥2\n2. Multiimensional scaling attempts to\nfind an d dimensions that agrees, as much as possible, with these estimates.\nOrganizing X = [x1, . . . , xn] and consider the matrix ∆whose entries are\nδij.\n1. Show that, if δij = ∥xi -xj∥2\n2 then there is a choice of xi (note that\nthe solution is not unique, as a translation of the points will preserve\nthe pairwise distances, e.g.) for which\nXT\nX = -2H∆H,\n\nwhere H = I -1 11T .\nn\n2. If the goal is to find points in Rd, how would you do it (keep part 1 of\nthe question in mind)?\n(The procedure you have just derived is known as Multidimensional Scal-\ning)\nThis motivates a way to embed a graph in d dimensions. Given two nodes\nwe take δij to be the square of some natural distance on a graph such as, for\nexample, the geodesic distance (the distance of the shortest path between the\nnodes) and then use the ideas above to find an embedding in Rd for which\nEuclidean distances most resemble geodesic distances on the graph. This is\nthe motivation behind a dimension reduction technique called ISOMAP (J.\nB. Tenenbaum, V. de Silva, and J. C. Langford, Science 2000).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Topics in Mathematics of Data Science Assignment 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/57c79829f3269daaac3ab97a4c7505c5_MIT18_S096F15_Homework_2.pdf",
      "content": "18.S096: Homework Problem Set 2\nTopics in Mathematics of Data Science (Fall 2015)\nAfonso S. Bandeira\nDue on October 20, 2015\nProblem 2.1 Given a graph G = (V, E, W) consider the random walk on\nV with transition probabilities\nw\nMij = Prob {X\n}\nij\n(t + 1) = j|X(t) = i =\n.\ndeg(i)\nPartition the vertex set as V = V+ ∪V-∪V . Suppose that every node in\n∗\nV\nis connected to at least a node in either V\n∗\n+ or V . Given a node i\n-\n∈V\nlet g(i) be the probability that a random walker starting at i reaches a node\nin V+ before reaching one in V , i.e.:\n-\ng(i) = Prob\n\ninf\nt <\ninf\nt\n.\nt≥0: X(t)∈V+\n|X(0) = i\nt≥0: X(t)∈V-\n\nNote that if i ∈V+ then g(i) = 1 and, if i ∈V , then g(i) = 0. What is the\n-\nvalue of g in V ? How would you compute it?\n∗\nProblem 2.2 For a graph G let h(G) denote its Cheeger constant and\nλ2 (LG) the second smallest eigenvalue of its normalized graph Laplacian.\nRecall that Cheeger inequality guarantees that\n1λ2 (\nLG) ≤hG ≤\np\n2λ2 (LG).\nThis exercise shows that this inequality is tight (at least up to constants).\n1. Construct a family of graphs for which λ2 (LG) →0 and for which\nthere exists a constant C > 0 for which, for every G in the family,\nhG ≤Cλ2 (LG)\n\n2. Construct a family of graphs for which λ2 (LG) →0 and for which\nthere exists a constant c > 0 for which, for every G in the family\nhG ≥c\np\nλ2 (LG)\nProblem 2.3 Given a graph G show that the dimension of the nullspace of\nLG corresponds to the number of connected components of G.\nProblem 2.4 Given a connected unweighted graph G = (V, E), its diameter\nis equal to\ndiam(G) = max\nmin\nlength of p.\nu,v∈V path p from u to v\nShow that\ndiam(G) ≥\n.\nvol(G)λ2 (LG)\nProblem 2.5\n1. Prove the Courant Fisher Theorem: Given a symmetric\nmatrix A ∈Rn×n with eigenvalues λ1 ≤· · · ≤λn, for k ≤n,\nxT Ax\nλk(A) =\nmin\nU: dim(U\nk\n\nmax\n)=\nx∈U\n.\nxT x\n\n2. Show also that:\n\nxT Ax\nλ2(A) = max\nmin\ny∈Rn\nx∈Rn: x⊥y\n.\nxT x\n\nProblem 2.6 Given a set of points x , . . . , x ∈Rp\nn\nand a partition of them\nin k clusters S1, . . . , Sk recall the k-means objective\nk\nmin\nmin\nxi\nμl\n.\nS1,...,Sk μ1,...,μk\nX\nl=1\nX\n∥\n-\n∥\ni∈Si\nShow that this is equivalent to\nk\nmin\nS1,...,Sk\nX\nl=1 |Sl|\nX\ni,j∈Sl\n∥xi -xj∥2 .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Topics in Mathematics of Data Science Assignment 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/de970f3b6895be957102f827b7b922e2_MIT18_S096F15_Homework_3.pdf",
      "content": "18.S096: Homework Problem Set 3\nTopics in Mathematics of Data Science (Fall 2015)\nAfonso S. Bandeira\nDue on November 3, 2015\nProblem 3.1 Given n i.i.d. non-negative random variables x1, . . . , xn, show that\nE max xi ≲\ni\n\nE\nh\nlog n\nx1\ni\nlog n .\nProblem 3.2 Let y1, . . . , yn ∈Rd be i.i.d. random variables such that Eyk = 0 and\nEykyT\nk = Id×d.\nShow that, if\nlog d\nn\n\nE∥y1∥2 log n\nlog n ≲1,\nthen\nE\n\nn\nn\nX\nk=1\nykyT\nk\n!\n-Id×d\n≲\nr\nlog d\nn\n\nE∥y1∥2 log n\n2 log n\nNote that ∥· ∥denotes spectral norm, and ≲means smaller up to constants.\nProblem 3.3 Given a centered1 random symmetric matrix X ∈Rd×d, we define\nσ =\np\n∥EX2∥,\nand\nσ =\nmax\n∗\nv: ∥v∥=1\nq\nE\n(vT Xv) ,\n1. Show that σ ≥σ .\n∗\n2. If X has independent entries (except for the fact that Xij = Xji) such that Xij ∼N\n\n0, b2\nij\n\n,\nshow that\n1Meaning that EX = 0.\n\nσ\nX\nn\n- σ2 = max\nb2\nij\ni\nj=1\n-\n∗≤2 max\nij\n|bij|\nNote that ∥·∥denotes spectral norm, and in expressions with E and a power, the power binds first.\nFor example, by EX2, we mean E\n\nX2\nand, by E\nvT Xv\n\n, we mean E\nh\nvT Xv\ni\nProblem 3.4 (Norm concentration of projection) Let y1, . . . , y b\n\nd e i.i.d standard Gaussian ran-\ndom variables\n\nand Y = (y1, . . . , yd). Let g : Rd →Rk be the projection into the first k coordinates\nand Z = g\nY\n∥Y ∥\n\n=\n∥Y ∥(y1, . . . , yk) and L = ∥Z∥2. It is clear that EL = k. Prove that L is very\nd\nconcentrated around its mean\n- If β < 1,\nPr\n\nk\nL ≤β d\n\n≤exp\nk(1\n-β + log β)\n\n.\n- If β > 1,\nPr\n\nk\nL ≥β d\n\n≤exp\nk(1\n-β + log β)\n\n.\n1. Prove\nPr\n\"X\nk\ny2\nk\ni\ni=1\n≤β d\nd\nX\ni=1\ny2\ni\n#\n≤(1 -2tkβ)-(d-k)/2\n(1 -2t(kβ -d))k/2\nfor any t > 0 such that 1 -2tkβ > 0 and 1 -2t(kβ -d) > 0.\n(Hint: Prove, and use, that E(esX2) =\n√\n, for\n<\n-2s\n-inf\ns < 1/2, where X is standard normal.)\n2. Find a suitable t and conclude the proof of the inequality for β < 1.\n3. Use the same idea as above to prove the β > 1 case.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Topics in Mathematics of Data Science Assignment 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/0652e0df1a00ac93199921e375cab387_MIT18_S096F15_Homework_4.pdf",
      "content": "18.S096: Homework Problem Set 4\nTopics in Mathematics of Data Science (Fall 2015)\nAfonso S. Bandeira\nDue on November 17, 2015\nConnectivity of the Erd os-R enyi random graph\nThe Erd os-R enyi random graph G(n, p) is a graph with n nodes, where each edge (i, j) appears\n(independently) with probability p. In this problem set, you will show a remarkable phase transition:\nif λ < 1, then G(n, λlnn) has, with high probability, isolated nodes while, if λ > 1, the graph is\nn\nconnected (with high probability).\nProblem 4.1 Let Ii be a random variable\nP indicating whether node i is isolated: Ii = 1 if node i is\nn\nisolated, and Ii = 0 otherwise. Let X =\ni=1 Ii be the number of isolated nodes.\nThe goal is to show that Pr{X = 0} is small when λ < 1 (meaning that there are isolated notes,\nwith high probability). In the proof you can use the approximation\n(1 -λ/n)n ≈e-λ\n(for large n)\n1. Show that E[X] ≈n-λ+1.\nNote: The fact that E[X] →infis not sufficient to show Pr{X =\n0} →0 (why? Can you give a counter-example?). We need to ensure that X concentrates\naround its mean.\n2. Use (a simple) concentration inequality derived in class to finish the proof. (The techinque you\nhave just derived is known as the second moment method)\nProblem 4.2 Prove that, if λ ≥1, G(n, λlnn) is connected with high probability:\nn\n1. Derive the probability for a set of k nodes (k ≤n/2) being disconnected from the rest of the\ngraph.\n2. Prove the probability of graph G having a disconnected component goes to zero as n grows (hint:\nuse union bound).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Topic in Mathematics of Data Science Assignment 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/429af5651e2860de79b5ef1321624d13_MIT18_S096F15_Homework_5.pdf",
      "content": "18.S096: Homework Problem Set 5\nTopics in Mathematics of Data Science (Fall 2015)\nAfonso S. Bandeira\nDue on December 1, 2015\nProblem 5.1 (Little Grothendieck problem) Let C ⪰0 (C is positive semidefinite).\nIn this\nhomework you'll show an approximation ratio of 2 to the problem\nπ\nn\nmax\nxi=±1 i,j\nX\nCijxixj.\n=1\nSimilarly to Max-Cut, we consider\nn\nmax\nvi∈Rn\n∥vi∥2\ni,j\n=1\nX\nCijvT\ni vj.\n=1\n♮\nThe goal is to show that, for r ∼N (0, In\nn), taking xi = sign(vT\ni r) a randomized rounding,\n×\nE\n\nn\nX\n♮\n♮\nCijxixj\ni,j=1\n\n≥\nC\ni,j\nX\nn\nijvT\nπ\ni vj\n=1\nHints:\n1. The main difficulty is that E sign(vT\nT\nT\ni r) sign(vj r) is not linear in vi vj and Cij might be negative\nfor some (i, j)'s.\nh\ni\n2. Show that that E\nh\nsign(vT\ni r)vT\nj r\ni\nis linear in vT\ni vj. What is it equal to?\n3. Construct S with entries Sij =\n\nvT\ni r -\nq\nπ sign(vT\ni r)\n\nvT\nj r -\nq\n2 sign(vT\nπ\nj r)\n4. Show that Tr(CS) ≥0.\n\nProblem 5.2 Consider the problem of angular synchronization. Suppose we have n clocks. Between\nany two clocks, we observe the relative hourly difference. The goal is to tell the time on each clock, up\n\nto a global time shift. Let z = [eiθ1, . . . , eiθn]T ∈Cn represents the ground truth time vector we try to\nrecover (θk ∈[0, 2π], k = 1, . . . , n are effectively the time on the clocks). Then the true pairwise time\ndifference between clock k and l can be represented by\nzkzl\n∗= ei(θk-θl)\n.\nConsider Akl the observed time difference between clock k and l. Akl equals zkzl\n∗with probability\np and Akl = eiφ with probability 1 -p. φ is a random uniform variable on the interval [0, 2π].\n1. Since we know that when p is small Akl ≈zkzl\n∗, a way to solve the synchronization problem is\nby solving\nn\nmin\nX X\nn\n|ukul\n∗-Akl|2,\ns.t. |uk =\nu\n|\n1, k = 1, . . . , n\nk=1 l=1\nRelax this to a problem of computing the eigenvector associated with the maximum eigenvalue of\na matrix.\n2. The goal is to understand when the eigenvector method returns meaningful solution. Without\nloss of generality we can assume zi = 1 for all i.\n- In that case what is EA?\n- Are the entries of A -EA independent (and identically distributed)?\n- If the entries of A were Gaussian (but with the same mean and variance they have), argue\nthat\np\np\n>\n1 -p2\n√n\nis needed so that the eigenvector method returns a meaningful solution.\nRemark: It turns out that the fact that the entries are not Gaussian, in this case, does not dras-\ntically affect the behavior of the top eigenvector and so this prediction is very accurate. Also, the\ncondition is not only needed but sufficient.\nProblem 5.3 (Z2 Synchronization) Consider a similar synchronization-type problem.\nLet z ∈\n{-1, 1}n denote the faces of n coins (or a clock that only gives noon and 6 o'clock). From the noisy\nobservations Aij of the relative faces between the n coins (or the relative time) and we want to solve\nn\nmin\nx∈{-1,1}n\nX\nn\ni=1\nX\nj=1\n|xixj -Aij|2\nwhere\nAij = zizj + σW,\nwhere Wij ∼N(0, 1) with independent entries (expect for the fact that W T = W).\nThe purpose is to understand for which values of σ is a Semidefinite programming relaxation exact.\n\n1. Show that this problem is equivalent to\nmax\nX\nn X\nn\nAijxixj\nx∈{-1,1}n\ni=1 j=1\n2. Derive an SDP relaxation for this problem.\n3. Show that\nmin Tr(D)\nsuch that D is diagonal, and D -A ⪰0\nsatisfies weak duality: for any feasible D, the objective of this program is at least the one of the\noriginal SDP.\n4. Without Loss of generality take zi = 1 for all i.\n5. Take D = DA where (DA)ii = Pn\nj=1 Aij. When does this choice of D is able to prove that 11T\nis an optimal solution of the original SDP? For which values of σ do we expect this to happen\nwith high probability? Can it also be shown that 11T is the unique solution? (Such a D is known\nas a dual certificate or a dual witness).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Topics in Mathematics of Data Science Lecture Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/5f0f7205d1cf274e80d77345a7edbf2a_MIT18_S096F15_TenLec.pdf",
      "content": "Ten Lectures and Forty-Two Open Problems in the Mathematics of\nData Science\nAfonso S. Bandeira\nDecember, 2015\nPreface\nThese are notes from a course I gave at MIT on the Fall of 2015 entitled: \"18.S096: Topics in\nMathematics of Data Science\". These notes are not in final form and will be continuously\nedited and/or corrected (as I am sure they contain many typos). Please use at your own\nrisk and do let me know if you find any typo/mistake.\nPart of the content of this course is greatly inspired by a course I took from Amit Singer while a\ngraduate student at Princeton. Amit's course was inspiring and influential on my research interests.\nI can only hope that these notes may one day inspire someone's research in the same way that Amit's\ncourse inspired mine.\nThese notes also include a total of forty-two open problems (now 41, as in meanwhile Open\nProblem 1.3 has been solved [MS15]!).\nThis list of problems does not necessarily contain the most important problems in the field (al-\nthough some will be rather important). I have tried to select a mix of important, perhaps approachable,\nand fun problems. Hopefully you will enjoy thinking about these problems as much as I do!\nI would like to thank all the students who took my course, it was a great and interactive audience!\nI would also like to thank Nicolas Boumal, Ludwig Schmidt, and Jonathan Weed for letting me know\nof several typos. Thank you also to Nicolas Boumal, Dustin G. Mixon, Bernat Guillen Pegueroles,\nPhilippe Rigollet, and Francisco Unda for suggesting open problems.\nContents\n0.1\nList of open problems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.2\nA couple of Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.2.1\nKoml os Conjecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.2.2\nMatrix AM-GM inequality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.3\nBrief Review of some linear algebra tools . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.3.1\nSingular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.3.2\nSpectral Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n0.3.3\nTrace and norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n0.4\nQuadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nPrincipal Component Analysis in High Dimensions and the Spike Model\n1.1\nDimension Reduction and PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.1\nPCA as best d-dimensional affine fit . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.2\nPCA as d-dimensional projection that preserves the most variance . . . . . . .\n1.1.3\nFinding the Principal Components . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.4\nWhich d should we pick?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.5\nA related open problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2\nPCA in high dimensions and Marcenko-Pastur\n. . . . . . . . . . . . . . . . . . . . . .\n1.2.1\nA related open problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3\nSpike Models and BBP transition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3.1\nA brief mention of Wigner matrices\n. . . . . . . . . . . . . . . . . . . . . . . .\n1.3.2\nAn open problem about spike models\n. . . . . . . . . . . . . . . . . . . . . . .\nGraphs, Diffusion Maps, and Semi-supervised Learning\n2.1\nGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.1.1\nCliques and Ramsey numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2\nDiffusion Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.1\nA couple of examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.2\nDiffusion Maps of point clouds\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.3\nA simple example\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.4\nSimilar non-linear dimensional reduction techniques\n. . . . . . . . . . . . . . .\n2.3\nSemi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.1\nAn interesting experience and the Sobolev Embedding Theorem\n. . . . . . . .\nSpectral Clustering and Cheeger's Inequality\n3.1\nClustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1.1\nk-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2\nSpectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3\nTwo clusters\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3.1\nNormalized Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3.2\nNormalized Cut as a spectral relaxation . . . . . . . . . . . . . . . . . . . . . .\n3.4\nSmall Clusters and the Small Set Expansion Hypothesis . . . . . . . . . . . . . . . . .\n3.5\nComputing Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.6\nMultiple Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nConcentration Inequalities, Scalar and Matrix Versions\n4.1\nLarge Deviation Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.1\nSums of independent random variables . . . . . . . . . . . . . . . . . . . . . . .\n4.2\nGaussian Concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.1\nSpectral norm of a Wigner Matrix . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.2\nTalagrand's concentration inequality . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.3\nOther useful large deviation inequalities . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.1\nAdditive ChernoffBound\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.2\nMultiplicative ChernoffBound\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.3\nDeviation bounds on χ2 variables . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.4\nMatrix Concentration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5\nOptimality of matrix concentration result for gaussian series . . . . . . . . . . . . . . .\n4.5.1\nAn interesting observation regarding random matrices with independent matrices 68\n4.6\nA matrix concentration inequality for Rademacher Series\n. . . . . . . . . . . . . . . .\n4.6.1\nA small detour on discrepancy theory\n. . . . . . . . . . . . . . . . . . . . . . .\n4.6.2\nBack to matrix concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.7\nOther Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.7.1\nOblivious Sparse Norm-Approximating Projections . . . . . . . . . . . . . . . .\n4.7.2\nk-lifts of graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.8\nAnother open problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nJohnson-Lindenstrauss Lemma and Gordons Theorem\n5.1\nThe Johnson-Lindenstrauss Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1.1\nOptimality of the Johnson-Lindenstrauss Lemma . . . . . . . . . . . . . . . . .\n5.1.2\nFast Johnson-Lindenstrauss . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2\nGordon's Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2.1\nGordon's Escape Through a Mesh Theorem . . . . . . . . . . . . . . . . . . . .\n5.2.2\nProof of Gordon's Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3\nSparse vectors and Low-rank matrices . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3.1\nGaussian width of k-sparse vectors . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3.2\nThe Restricted Isometry Property and a couple of open problems . . . . . . . .\n5.3.3\nGaussian width of rank-r matrices . . . . . . . . . . . . . . . . . . . . . . . . .\nCompressed Sensing and Sparse Recovery\n6.1\nDuality and exact recovery\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2\nFinding a dual certificate\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3\nA different approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.4\nPartial Fourier matrices satisfying the Restricted Isometry Property\n. . . . . . . . . .\n6.5\nCoherence and Gershgorin Circle Theorem . . . . . . . . . . . . . . . . . . . . . . . . .\n6.5.1\nMutually Unbiased Bases\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.5.2\nEquiangular Tight Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.5.3\nThe Paley ETF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.6\nThe Kadison-Singer problem\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nGroup Testing and Error-Correcting Codes\n7.1\nGroup Testing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2\nSome Coding Theory and the proof of Theorem 7.3\n. . . . . . . . . . . . . . . . . . .\n7.2.1\nBoolean Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2.2\nThe proof of Theorem 7.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3\nIn terms of linear Bernoulli algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n7.3.1\nShannon Capacity\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3.2\nThe deletion channel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nApproximation Algorithms and Max-Cut\n8.1\nThe Max-Cut problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.2\nCan αGW be improved? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3\nA Sums-of-Squares interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.4\nThe Grothendieck Constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.5\nThe Paley Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.6\nAn interesting conjecture regarding cuts and bisections . . . . . . . . . . . . . . . . . .\nCommunity detection and the Stochastic Block Model\n9.1\nCommunity Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.2\nStochastic Block Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.3\nWhat does the spike model suggest? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.3.1\nThree of more communities . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.4\nExact recovery\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.5\nThe algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.6\nThe analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.6.1\nSome preliminary definitions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.7\nConvex Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.8\nBuilding the dual certificate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.9\nMatrix Concentration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.10 More communities\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.11 Euclidean Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.12 Probably Certifiably Correct algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.13 Another conjectured instance of tightness\n. . . . . . . . . . . . . . . . . . . . . . . . .\n10 Synchronization Problems and Alignment\n10.1 Synchronization-type problems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.2 Angular Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.2.1 Orientation estimation in Cryo-EM . . . . . . . . . . . . . . . . . . . . . . . . .\n10.2.2 Synchronization over Z2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3 Signal Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3.1 The model bias pitfall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3.2 The semidefinite relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3.3 Sample complexity for multireference alignment . . . . . . . . . . . . . . . . . .\n0.1\nList of open problems\n- 0.1: Komlos Conjecture\n- 0.2: Matrix AM-GM Inequality\n- 1.1: Mallat and Zeitouni's problem\n\n- 1.2: Monotonicity of eigenvalues\n- 1.3: Cut SDP Spike Model conjecture →SOLVED here [MS15].\n- 2.1: Ramsey numbers\n- 2.2: Erdos-Hajnal Conjecture\n- 2.3: Planted Clique Problems\n- 3.1: Optimality of Cheeger's inequality\n- 3.2: Certifying positive-semidefiniteness\n- 3.3: Multy-way Cheeger's inequality\n- 4.1: Non-commutative Khintchine improvement\n- 4.2: Latala-Riemer-Schutt Problem\n- 4.3: Matrix Six deviations Suffice\n- 4.4: OSNAP problem\n- 4.5: Random k-lifts of graphs\n- 4.6: Feige's Conjecture\n- 5.1: Deterministic Restricted Isometry Property matrices\n- 5.2: Certifying the Restricted Isometry Property\n- 6.1: Random Partial Discrete Fourier Transform\n- 6.2: Mutually Unbiased Bases\n- 6.3: Zauner's Conjecture (SIC-POVM)\n- 6.4: The Paley ETF Conjecture\n- 6.5: Constructive Kadison-Singer\n- 7.1: Gilbert-Varshamov bound\n- 7.2: Boolean Classification and Annulus Conjecture\n- 7.3: Shannon Capacity of 7 cycle\n- 7.4: The Deletion Channel\n- 8.1: The Unique Games Conjecture\n- 8.2: Sum of Squares approximation ratio for Max-Cut\n\n- 8.3: The Grothendieck Constant\n- 8.4: The Paley Clique Problem\n- 8.5: Maximum and minimum bisections on random regular graphs\n- 9.1: Detection Threshold for SBM for three of more communities\n- 9.2: Recovery Threshold for SBM for logarithmic many communities\n- 9.3: Tightness of k-median LP\n- 9.4: Stability conditions for tightness of k-median LP and k-means SDP\n- 9.5: Positive PCA tightness\n- 10.1: Angular Synchronization via Projected Power Method\n- 10.2: Sharp tightness of the Angular Synchronization SDP\n- 10.3: Tightness of the Multireference Alignment SDP\n- 10.4: Consistency and sample complexity of Multireference Alignment\n0.2\nA couple of Open Problems\nWe start with a couple of open problems:\n0.2.1\nKoml os Conjecture\nWe start with a fascinating problem in Discrepancy Theory.\nOpen Problem 0.1 (Koml os Conjecture) Given n, let K(n) denote the infimum over all real\nnumbers such that: for all set of n vectors u1, . . . , un ∈Rn satisfying ∥ui∥2 ≤1, there exist signs\nεi = ±1 such that\n∥ε1u1 + ε2u2 + · · · + εnun∥inf≤K(n).\nThere exists a universal constant K such that K(n) ≤K for all n.\nAn early reference for this conjecture is a book by Joel Spencer [Spe94]. This conjecture is tightly\nconnected to Spencer's famous Six Standard Deviations Suffice Theorem [Spe85]. Later in the course\nwe will study semidefinite programming relaxations, recently it was shown that a certain semidefinite\nrelaxation of this conjecture holds [Nik13], the same paper also has a good accounting of partial\nprogress on the conjecture.\n- It is not so difficult to show that K(n)\n√\n≤\nn, try it!\n\n0.2.2\nMatrix AM-GM inequality\nWe move now to an interesting generalization of arithmetic-geometric means inequality, which has\napplications on understanding the difference in performance of with- versus without-replacement sam-\npling in certain randomized algorithms (see [RR12]).\nOpen Problem 0.2 For any collection of d × d positive semidefinite matrices A1, · · · , An, the fol-\nlowing is true:\n(a)\nn!\nX\nσ∈Sym(n)\nn\nY\nj=1\nAσ(j)\n\n≤\n\nnn\nn\nX\nk1,...,kn=1\nn\nY\nj=1\nAkj\n\n,\nand\n(b)\nn!\nX\nσ∈Sym(n)\n\nn\nY\nj=1\nAσ(j)\n\n≤1\nA\nj\nY\nn\n,\nk\nX\nn\nnn\n1,...,kn=1\n\nkj\n=1\n\nwhere Sym(n) denotes the group of permutations of n elements,\n\nand\n\n∥· ∥the spectral norm.\nMorally, these conjectures state that products of matrices with repetitions are larger than with-\nout. For more details on the motivations of these conjecture (and their formulations) see [RR12] for\nconjecture (a) and [Duc12] for conjecture (b).\nRecently these conjectures have been solved for the particular case of n = 3, in [Zha14] for (a)\nand in [IKW14] for (b).\n0.3\nBrief Review of some linear algebra tools\nIn this Section we'll briefly review a few linear algebra tools that will be important during the course.\nIf you need a refresh on any of these concepts, I recommend taking a look at [HJ85] and/or [Gol96].\n0.3.1\nSingular Value Decomposition\nThe Singular Value Decomposition (SVD) is one of the most useful tools for this course! Given a\nmatrix M ∈Rm×n, the SVD of M is given by\nM = UΣV T ,\n(1)\nwhere U ∈O(m), V ∈O(n) are orthogonal matrices (meaning that UT U = UUT = I and V T V =\nV V T = I) and Σ ∈Rm×n is a matrix with non-negative entries in its diagonal and otherwise zero\nentries.\nThe columns of U and V are referred to, respectively, as left and right singular vectors of M and\nthe diagonal elements of Σ as singular values of M.\nRemark 0.1 Say m ≤n, it is easy to see that we can also think of the SVD as having U ∈Rm×n\nwhere UUT = I, Σ ∈Rn×n a diagonal matrix with non-negative entries and V ∈O(n).\n\n0.3.2\nSpectral Decomposition\nIf M ∈Rn×n is symmetric then it admits a spectral decomposition\nM = V ΛV T ,\nwhere V ∈O(n) is a matrix whose columns vk are the eigenvectors of M and Λ is a diagonal matrix\nwhose diagonal elements λk are the eigenvalues of M. Similarly, we can write\nn\nM =\nX\nλkvkvT\nk .\nk=1\nWhen all of the eigenvalues of M are non-negative we say that M is positive semidefinite and write\nM ⪰0. In that case we can write\nT\nM =\n\nV Λ1/2\nV Λ1/2\n.\nA decomposition of M of the form M = UUT (such as the one above) is called a Cholesky decompo-\nsition.\nThe spectral norm of M is defined as\n∥M∥= max |λk(M) .\nk\n|\n0.3.3\nTrace and norm\nGiven a matrix M ∈Rn×n, its trace is given by\nTr(M) =\nX\nn\nn\nMkk =\nX\nλk (M) .\nk=1\nk=1\nIts Frobeniues norm is given by\n∥M∥F =\nsX\nM2\nij = Tr(MT M)\nij\nA particularly important property of the trace is that:\nn\nTr(AB) =\nX\nAijBji = Tr(BA).\ni,j=1\nNote that this implies that, e.g., Tr(ABC) = Tr(CAB), it does not imply that, e.g., Tr(ABC) =\nTr(ACB) which is not true in general!\n\n0.4\nQuadratic Forms\nDuring the course we will be interested in solving problems of the type\nmax\nTr V\nV ∈Rn×d\nV T V =Id\nd\nT MV\n\n,\n×\nwhere M is a symmetric n × n matrix.\nNote that this is equivalent to\nd\nmax\nvT\nk Mvk,\n(2)\nv1,...,vd∈Rn\nvT\nX\nk=1\nv\ni\nj=δij\nwhere δij is the Kronecker delta (is 1 is i = j and 0 otherwise).\nWhen d = 1 this reduces to the more familiar\nmax vT Mv.\n(3)\nv∈Rn\n∥v∥2=1\nIt is easy to see (for example, using the spectral decomposition of M) that (3) is maximized by\nthe leading eigenvector of M and\nmax vT Mv = λmax(M).\nv\nv\n∈Rn\n∥∥2=1\nIt is also not very difficult to see (it follows for example from a Theorem of Fan (see, for example,\npage 3 of [Mos11]) that (2) is maximized by taking v1, . . . , vd to be the k leading eigenvectors of M\nand that its value is simply the sum of the k largest eigenvalues of M. The nice consequence of this\nis that the solution to (2) can be computed sequentially: we can first solve for d = 1, computing v1,\nthen v2, and so on.\nRemark 0.2 All of the tools and results above have natural analogues when the matrices have complex\nentries (and are Hermitian instead of symmetric).\n\n0.1\nSyllabus\nThis will be a mostly self-contained research-oriented course designed for undergraduate students\n(but also extremely welcoming to graduate students) with an interest in doing research in theoretical\naspects of algorithms that aim to extract information from data. These often lie in overlaps of\ntwo or more of the following: Mathematics, Applied Mathematics, Computer Science, Electrical\nEngineering, Statistics, and/or Operations Research.\nThe topics covered include:\n1. Principal Component Analysis (PCA) and some random matrix theory that will be used to\nunderstand the performance of PCA in high dimensions, through spike models.\n2. Manifold Learning and Diffusion Maps: a nonlinear dimension reduction tool, alternative to\nPCA. Semisupervised Learning and its relations to Sobolev Embedding Theorem.\n3. Spectral Clustering and a guarantee for its performance: Cheeger's inequality.\n4. Concentration of Measure and tail bounds in probability, both for scalar variables and matrix\nvariables.\n5. Dimension reduction through Johnson-Lindenstrauss Lemma and Gordon's Escape Through\na Mesh Theorem.\n6. Compressed Sensing/Sparse Recovery, Matrix Completion, etc. If time permits, I will present\nNumber Theory inspired constructions of measurement matrices.\n7. Group Testing. Here we will use combinatorial tools to establish lower bounds on testing\nprocedures and, if there is time, I might give a crash course on Error-correcting codes and\nshow a use of them in group testing.\n8. Approximation algorithms in Theoretical Computer Science and the Max-Cut problem.\n9. Clustering on random graphs: Stochastic Block Model. Basics of duality in optimization.\n10. Synchronization, inverse problems on graphs, and estimation of unknown variables from pair-\nwise ratios on compact groups.\n11. Some extra material may be added, depending on time available.\n0.4\nOpen Problems\nA couple of open problems will be presented at the end of most lectures. They won't necessarily\nbe the most important problems in the field (although some will be rather important), I have tried\nto select a mix of important, approachable, and fun problems. In fact, I take the opportunity to\npresent two problems below (a similar exposition of this problems is also available on my blog [?]).\n\nPrincipal Component Analysis in High Dimensions and the Spike\nModel\n1.1\nDimension Reduction and PCA\nWhen faced with a high dimensional dataset, a natural approach is to try to reduce its dimension,\neither by projecting it to a lower dimension space or by finding a better representation for the data.\nDuring this course we will see a few different ways of doing dimension reduction.\nWe will start with Principal Component Analysis (PCA). In fact, PCA continues to be one of the\nbest (and simplest) tools for exploratory data analysis. Remarkably, it dates back to a 1901 paper by\nKarl Pearson [Pea01]!\nLet's say we have n data points x1, . . . , xn in Rp, for some p, and we are interested in (linearly)\nprojecting the data to d < p dimensions. This is particularly useful if, say, one wants to visualize\nthe data in two or three dimensions. There are a couple of different ways we can try to choose this\nprojection:\n1. Finding the d-dimensional affine subspace for which the projections of x1, . . . , xn on it best\napproximate the original points x1, . . . , xn.\n2. Finding the d dimensional projection of x1, . . . , xn that preserved as much variance of the data\nas possible.\nAs we will see below, these two approaches are equivalent and they correspond to Principal Com-\nponent Analysis.\nBefore proceeding, we recall a couple of simple statistical quantities associated with x1, . . . , xn,\nthat will reappear below.\nGiven x1, . . . , xn we define its sample mean as\nμn = 1 X\nn\nxk,\n(4)\nn k=1\nand its sample covariance as\nΣn =\n(\nn\nX\nn\nxk\n(xk\n-μn)\n-\nT\nμ\n-\nn) .\n(5)\nk=1\nRemark 1.1 If x1, . . . , xn are independently sampled from a distribution, μn and Σn are unbiased\nestimators for, respectively, the mean and covariance of the distribution.\nWe will start with the first interpretation of PCA and then show that it is equivalent to the second.\n1.1.1\nPCA as best d-dimensional affine fit\nWe are trying to approximate each xk by\nd\nxk ≈μ +\nX\n(βk)i vi,\n(6)\ni=1\n\nwhere v1, . . . , vd is an orthonormal basis for the d-dimensional subspace, μ ∈Rp represents the transla-\ntion, and βk corresponds to the coefficients of xk. If we represent the subspace by V = [v\np\n1 · · · vd] ∈R ×d\nthen we can rewrite (7) as\nxk ≈μ + V βk,\n(7)\nwhere V T V = Id×d as the vectors vi are orthonormal.\nWe will measure goodness of fit in terms of least squares and attempt to solve\nmin\nX\nn\n∥\nxk -(μ + V βk)\nμ, V, βk\n∥2\n(8)\nT\nk=1\nV\nV =I\nWe start by optimizing for μ. It is easy to see that the first order conditions for μ correspond to\nn\n∇μ\nX\nn\n∥xk -(μ + V βk)∥2\n2 = 0 ⇔\nX\n(xk -(μ + V βk)) = 0.\nk=1\nk=1\nThus, the optimal value μ∗of μ satisfies\nX\nn\nn\nxk\n-nμ∗-V\nX\nβk\n= 0.\nk=1\nk=1\nBecause Pn\nk=1 βk = 0 we have that the optimal μ is given by\nn\nμ∗=\nX\nxk = μn,\nn k=1\nthe sample mean.\nWe can then proceed on finding the solution for (9) by solving\nn\nmin\nk\nμn\nV\nV, βk\n∥x -\n-\nβk∥2\n2 .\n(9)\nV T\nk=1\nV =I\nX\nLet us proceed by optimizing for βk. Since the problem decouples for each k, we can focus on, for\neach k,\nd\nmin ∥xk -μn -V βk∥2 = min\n\nxk -μn\n(\nβ\n-\n.\n(10)\nk\nβ\nX\nβk)i vi\nk\ni=1\n\nSince v1, . . . , v\n\nd are orthonormal, it is easy to see that\n\nthe solution is given\n\nby (βk\n∗) = vT\ni (xk -μn)\ni\nwhich can be succinctly written as β\nT\nk = V\n(xk -μn). Thus, (9) is equivalent to\nX\nn\nmin\nn)\nV T V =I\n(xk -μ\n-\nV V T (xk -μn)\nk=1\n.\n(11)\n!\n\n!\n\nNote that\n(xk -μn) -V V T (xk -\nT\nμn)\n\n=\n(xk\n-μn) (xk -μn)\n-2 (xk -\nT\nμn) V V T\n(xk\nμ\nT\n-\nn)\n+ (x -μ ) V\nV T V\nV T\nk\nn\n(xk -μn)\n=\n(xk -\nT\nμn) (xk -μn)\n-(xk -\nT\nμn) V V T (xk -μn) .\nSince (xk -\nT\nμn) (xk -μn) does not depend on V , minimizing (9) is equivalent to\nn\nT\nmax\nX\n(xk -μn) V V T (xk -μn) .\n(12)\nV T V =I k=1\nA few more simple algebraic manipulations using properties of the trace:\nX\nn\nn\nT\nT\n(xk -μn) V V T (xk -μn)\n=\nX\nTr\nh\n(x\nV V T\nk -μn)\n(xk -μn)\nk=1\nk=1\nn\ni\n=\nX\n-\nT\nTr\nh\nV T (xk\nμn) (xk -μn) V\nk=1\ni\n=\nTr\n\"\nn\nV T X\n(xk -\nT\nμn) (xk -μn) V\nk=1\n#\n=\n(n -1) Tr\n\nV T ΣnV\ny\n\n.\nThis means that the solution to (13) is given b\nmax Tr\n\nV T ΣnV\n.\nV T V =I\n\n(13)\nAs we saw above (recall (2)) the solution is given by V = [v1, · · · , vd] where v1, . . . , vd correspond\nto the d leading eigenvectors of Σn.\nLet us first show that interpretation (2) of finding the d-dimensional projection of x1, . . . , xn that\npreserves the most variance also arrives to the optimization problem (13).\n1.1.2\nPCA as d-dimensional projection that preserves the most variance\nWe aim to find an orthonormal basis v1, . . . , v\nT\nd (organized as V = [v1, . . . , vd] with V\nV = Id×d) of\na d-dimensional space such that the projection of x1, . . . , xn projected on this subspace has the most\nvariance. Equivalently we can ask for the points\n\nvT\n\n1 xk\n.\n\nn\n\n..\nvT\nd xk\n\n,\nk=1\n\nto have as much variance as possible. Hence, we are interested in solving\nmax\nX\nn\nX\nn\nV T\nT\n\nxk -\nV\nxr\n.\nV T V =I\nn\nk=1\nr=1\n\n(14)\nNote that\nX\nn\nn\n\nn\n\nT\nV T x\nT\nT\nk -\nX\nV\nxr\n\n=\nX\nV\n(xk -μn)\n= Tr V\nΣnV\n,\nn\nk=1\nr=1\nk=1\nshowing that (14) is equivalent to (13) and\n\nthat the\n\ntwo interpretations\n\nof\nPCA\n\nare indeed equivalent.\n1.1.3\nFinding the Principal Components\nWhen given a dataset x1, . . . , xn ∈Rp, in order to compute the Principal Components one needs to\nfind the leading eigenvectors of\nn\nT\nΣn =\n(\n-\nX\nxk\nμ\nn\n-\nn) (xk -μn) .\nk=1\nA naive way of doing this would be to construct Σn (which takes O(np2) work) and then finding its\nspectral decomposition (which takes O(p3) work). This means that the computational complexity of\nthis procedure is\nmax np2, p3\n(see [HJ85] and/or [Gol96]).\nO\n\nAn alternative is to use the Singular Value Decomposition (1). Let X = [x1 · · · xn] recall that,\nΣn =\nX -\nT\nμ\nT\nT\nn1\nX -μn1\n.\nn\nLet us take the SVD of X -μ 1T = U DUT with\n\nn\nL\nR\nUL ∈O(p), D diagonal, and UT\nRUR = I. Then,\nT\nΣn =\nX -μ 1T X -μ 1T\n= U DUT U DUT = U D2\nT\nn\nn\nL\nR\nR\nL\nL\nU\nn\nL ,\nmeaning that UL correspond to the eigenvectors of Σ\nT\nn.\nComputing the SVD of X -μn1\ntakes\nO(min n2p, p2n) but if one is interested in simply computing the top d eigenvectors then this compu-\ntational costs reduces to O(dnp). This can be further improved with randomized algorithms. There\nare randomized algorithms that compute an approximate solution in\nO\npn log d + (p + n)d2\ntime\n(see for example [HMT09, RST09, MM15]).\n1.1.4\nWhich d should we pick?\nGiven a dataset, if the objective is to visualize it then picking d = 2 or d = 3 might make the\nmost sense. However, PCA is useful for many other purposes, for example: (1) often times the data\nbelongs to a lower dimensional space but is corrupted by high dimensional noise. When using PCA\nit is oftentimess possible to reduce the noise while keeping the signal. (2) One may be interested\nin running an algorithm that would be too computationally expensive to run in high dimensions,\n1If there is time, we might discuss some of these methods later in the course.\n\ndimension reduction may help there, etc. In these applications (and many others) it is not clear how\nto pick d.\n(+)\nIf we denote the k-th largest eigenvalue of Σn as λ\n(Σ\nk\nn), then the k-th principal component has\n(+)\nλ\n(Σ\na\nn)\nk\nproportion of the variance. 2\nTr(Σn)\nA fairly popular heuristic is to try to choose the cut-offat a component that has significantly more\nvariance than the one immediately after. This is usually visualized by a scree plot: a plot of the values\nof the ordered eigenvalues. Here is an example:\nIt is common to then try to identify an \"elbow\" on the scree plot to choose the cut-off. In the\nnext Section we will look into random matrix theory to try to understand better the behavior of the\neigenvalues of Σn and it will help us understand when to cut-off.\n1.1.5\nA related open problem\nWe now show an interesting open problem posed by Mallat and Zeitouni at [MZ11]\nOpen Problem 1.1 (Mallat and Zeitouni [MZ11]) Let g ∼N(0, Σ) be a gaussian random vector\nin Rp with a known covariance matrix Σ and d < p. Now, for any orthonormal basis V = [v1, . . . , vp]\nof Rp, consider the following random variable ΓV : Given a draw of the random vector g, ΓV is the\nsquared l2 norm of the largest projection of g on a subspace generated by d elements of the basis V .\nThe question is:\nWhat is the basis V for which E [ΓV ] is maximized?\n2Note that Tr (Σn) = Pp\nk=1 λk (Σn).\n\nThe conjecture in [MZ11] is that the optimal basis is the eigendecomposition of Σ. It is known\nthat this is the case for d = 1 (see [MZ11]) but the question remains open for d > 1. It is not very\ndifficult to see that one can assume, without loss of generality, that Σ is diagonal.\nA particularly intuitive way of stating the problem is:\n1. Given Σ ∈Rp×p and d\n2. Pick an orthonormal basis v1, . . . , vp\n3. Given g ∼N(0, Σ)\n4. Pick d elements v 1, . . . , v d of the basis\nd\n5. Score: P\ni=1\nv T\ni g\nThe objective is to pic\n\nk the basis in order to maximize the expected value of the Score.\nNotice that if the steps of the procedure were taken in a slightly different order on which step\n4 would take place before having access to the draw of g (step 3) then the best basis is indeed\nthe eigenbasis of Σ and the best subset of the basis is simply the leading eigenvectors (notice the\nresemblance with PCA, as described above).\nMore formally, we can write the problem as finding\n\nargmax\nE\nmax\nvT\ni g\n,\nV ∈Rp×p\n\nV T V =I\n\nS\n[p\n|S\n⊂\n]\n\n|=d\nX\ni∈S\n\nwhere g ∼N(0, Σ). The observation regarding the different ordering of the steps amounts to saying\nthat the eigenbasis of Σ is the optimal solution for\n\ni g\nargmax\nmax E\nX vT\n.\np\nS⊂[p]\nV ∈Rp×\nS|\ni\n=d\n∈S\nV T V =I\n|\n\n\"\n#\n\n1.2\nPCA in high dimensions and Marcenko-Pastur\nLet us assume that the data points x\np\n1, . . . , xn ∈R\nare independent draws of a gaussian random\nvariable g ∼N(0, Σ) for some covariance Σ ∈Rp×p. In this case when we use PCA we are hoping\nto find low dimensional structure in the distribution, which should correspond to large eigenvalues of\nΣ (and their corresponding eigenvectors). For this reason (and since PCA depends on the spectral\nproperties of Σn) we would like to understand whether the spectral properties of Σn (eigenvalues and\neigenvectors) are close to the ones of Σ.\nSince EΣn = Σ, if p is fixed and n →infthe law of large numbers guarantees that indeed Σn →Σ.\nHowever, in many modern applications it is not uncommon to have p in the order of n (or, sometimes,\neven larger!). For example, if our dataset is composed by images then n is the number of images and\np the number of pixels per image; it is conceivable that the number of pixels be on the order of the\nnumber of images in a set. Unfortunately, in that case, it is no longer clear that Σn →Σ. Dealing\nwith this type of difficulties is the realm of high dimensional statistics.\n\nFor simplicity we will instead try to understand the spectral properties of\nSn =\nXXT .\nn\nSince x ∼N(0, Σ) we know that μn →0 (and, clearly,\nn\nn-1 →1) the spectral properties of Sn will be\nessentially the same as Σn.3\nLet us start by looking into a simple example, Σ = I. In that case, the distribution has no low\ndimensional structure, as the distribution is rotation invariant. The following is a histogram (left) and\na scree plot of the eigenvalues of a sample of Sn (when Σ = I) for p = 500 and n = 1000. The red\nline is the eigenvalue distribution predicted by the Marchenko-Pastur distribution (15), that we will\ndiscuss below.\nAs one can see in the image, there are many eigenvalues considerably larger than 1 (and some\nconsiderably larger than others). Notice that , if given this profile of eigenvalues of Σn one could\npotentially be led to believe that the data has low dimensional structure, when in truth the distribution\nit was drawn from is isotropic.\nUnderstanding the distribution of eigenvalues of random matrices is in the core of Random Matrix\nTheory (there are many good books on Random Matrix Theory, e.g. [Tao12] and [AGZ10]). This\nparticular limiting distribution was first established in 1967 by Marchenko and Pastur [MP67] and is\nnow referred to as the Marchenko-Pastur distribution. They showed that, if p and n are both going\nto infwith their ratio fixed p/n = γ ≤1, the sample distribution of the eigenvalues of Sn (like the\nhistogram above), in the limit, will be\np\n(γ+ -λ) (λ -γ )\ndFγ(λ) =\n-1[γ\n2π\nγλ\n-,γ+](λ)dλ,\n(15)\n3In this case, Sn is actually the Maximum likelihood estimator for Σ, we'll talk about Maximum likelihood estimation\nlater in the course.\n\nwith support [γ , γ+]. This is plotted as the red line in the figure above.\n-\nRemark 1.2 We will not show the proof of the Marchenko-Pastur Theorem here (you can see, for\nexample, [Bai99] for several different proofs of it), but an approach to a proof is using the so-called\nmoment method. The core of the idea is to note that one can compute moments of the eigenvalue\ndistribution in two ways and note that (in the limit) for any k,\n\"\nk\n#\n\np\n1 X\nZ γ+\nE Tr\nXXT\n=\nE Tr Sk\nn\n= E\nλk\nk\ni (Sn) =\nλ dFγ(λ),\np\nn\np\np\nγ\ni=1\n-\nand that the quantities 1E Tr\nh 1 XXT ki\ncan be estimated (these estimates rely essentially in combi-\np\nn\nnatorics). The distribution dFγ(λ) can then be computed from its moments.\n1.2.1\nA related open problem\nOpen Problem 1.2 (Monotonicity of singular values [BKS13a]) Consider the setting above but\nwith p = n, then X ∈Rn×n is a matrix with iid N(0, 1) entries. Let\nσi\n√X\nn\n\n,\ndenote the i-th singular value4 of √1 X, and define\nn\nαR(n) := E\n\"\nX\nn\nσi\n√X\n,\nn\nn\ni=1\n#\nas the expected value of the average singular value of √1 X.\nn\nThe conjecture is that, for every n ≥1,\nαR(n + 1) ≥αR(n).\nMoreover, for the analogous quantity αC(n) defined over the complex numbers, meaning simply\nthat each entry of X is an iid complex valued standard gaussian CN(0, 1) the reverse inequality is\nconjectured for all n ≥1:\nαC(n + 1) ≤αC(n).\nNotice that the singular values of √1 X are simply the square roots of the eigenvalues of Sn,\nn\nσi\n√X\n\n=\nλ\nn\np\ni (Sn).\n4The i-th diagonal element of Σ in the SVD √1 X = UΣV .\nn\n\nThis means that we can compute αR in the limit (since we know the limiting distribution of λi (Sn))\nand get (since p = n we have γ = 1, γ\n= 0, and γ\n-\n+ = 2)\n(2\nλ) λ\nlim αR(n) =\nZ\nλ 2 dF1(λ) = 2π\nZ\nλ 2\n→inf\np\n-\n=\n0.8488.\nn\nλ\n3π ≈\nAlso, αR(1) simply corresponds to the expected value of the absolute value of a standard gaussian\ng\nαR(1) = E|g| =\nr\nπ ≈0.7990,\nwhich is compatible with the conjecture.\nOn the complex valued side, the Marchenko-Pastur distribution also holds for the complex valued\ncase and so limn\nαC(n) = lim\n→inf\nn\nα\n→inf\nR(n) and αC(1) can also be easily calculated and seen to be\nlarger than the limit.\n1.3\nSpike Models and BBP transition\nWhat if there actually is some (linear) low dimensional structure on the data? When can we expect to\ncapture it with PCA? A particularly simple, yet relevant, example to analyse is when the covariance\nmatrix Σ is an identity with a rank 1 perturbation, which we refer to as a spike model Σ = I + βvvT ,\nfor v a unit norm vector and β ≥0.\nOne way to think about this instance is as each data point x consisting of a signal part √βg0v\nwhere g0 is a one-dimensional standard gaussian (a gaussian multiple of a fixed vector √βv and a\nnoise part g ∼N(0, I) (independent of g0. Then x = g + √βg0v is a gaussian random variable\nx ∼N(0, I + βvvT ).\nA natural question is whether this rank 1 perturbation can be seen in Sn. Let us build some\nintuition with an example, the following is the histogram of the eigenvalues of a sample of Sn for\np = 500, n = 1000, v is the first element of the canonical basis v = e1, and β = 1.5:\n\nThe images suggests that there is an eigenvalue of Sn that \"pops out\" of the support of the\nMarchenko-Pastur distribution (below we will estimate the location of this eigenvalue, and that es-\ntimate corresponds to the red \"x\"). It is worth noticing that the largest eigenvalues of Σ is simply\n1 + β = 2.5 while the largest eigenvalue of Sn appears considerably larger than that. Let us try now\nthe same experiment for β = 0.5:\nand it appears that, for β = 0.5, the distribution of the eigenvalues appears to be undistinguishable\nfrom when Σ = I.\nThis motivates the following question:\nQuestion 1.3 For which values of γ and β do we expect to see an eigenvalue of Sn popping out of the\nsupport of the Marchenko-Pastur distribution, and what is the limit value that we expect it to take?\nAs we will see below, there is a critical value of β below which we don't expect to see a change\nin the distribution of eivenalues and above which we expect one of the eigenvalues to pop out of the\nsupport, this is known as BBP transition (after Baik, Ben Arous, and P ech e [BBAP05]). There are\nmany very nice papers about this and similar phenomena, including [Pau, Joh01, BBAP05, Pau07,\nBS05, Kar05, BGN11, BGN12].5\nIn what follows we will find the critical value of β and estimate the location of the largest eigenvalue\nof Sn. While the argument we will use can be made precise (and is borrowed from [Pau]) we will\nbe ignoring a few details for the sake of exposition.\nIn short, the argument below can be\ntransformed into a rigorous proof, but it is not one at the present form!\nFirst of all, it is not difficult to see that we can assume that v = e1 (since everything else is rotation\ninvariant). We want to understand the behavior of the leading eigenvalue of\nn\nSn =\nX\nx\nT\nix\nn\ni =\nXXT ,\nn\ni=1\n5Notice that the Marchenko-Pastur theorem does not imply that all eigenvalues are actually in the support of the\nMarchenk-Pastur distribution, it just rules out that a non-vanishing proportion are. However, it is possible to show that\nindeed, in the limit, all eigenvalues will be in the support (see, for example, [Pau]).\n\nwhere\nX = [x1, . . . , xn] ∈Rp×n.\nWe can write X as\nX =\n√1 + βZT\nZT\n,\n\nwhere Z1 ∈Rn×1 and Z2 ∈Rn×(p-1), both populated with i.i.d. standard gaussian entries (N(0, 1)).\nThen,\nβ)ZT\nT\n(1 +\n1 Z\nT\n√\n√1 + βZ\nn =\nXX\n=\n1 Z2\nS\nn\nn\n1 + βZT\n2 Z\nT\n.\nZ2 Z2\n\nˆ\nNow, let λ and v =\nv1\n\nwhere v2 ∈Rp-1 and v1 ∈R, denote, respectively, an eigenvalue and\nv2\nassociated eigenvector for Sn. By the definition of eigenvalue and eigenvector we have\n1 (1 + β)ZT\n1 Z1\n√1 + βZT\n1 Z2\nv1\nˆ\n= λ\nn\n√1 + βZT\n2 Z1\nZT\n2 Z2\n\nv2\n\nv1\nv2\n\n,\nwhich can be rewritten as\nˆ\n(1 + β)ZT\n1 Z1v1 +\np\n1 + βZT\n1 Z2v2\n=\nλv1\n(16)\nn\nn\nˆ\n1 + βZT\n2 Z1v1 +\nZT\nn\n2 Z2v2\n=\nλv2.\n(17)\nn\n(17) is equivalent to\np\n1 + βZT\n2 Z1v1 =\n\nˆλ I -\nZT\nn\nn\n2 Z2\n\nv2.\nˆ\nIf λ I -1 ZT\n2 Z2 is invertible (this w\np\non't be justified here, but it is in [Pau]) then we can rewrite it as\nn\n\n-\nˆ\nv2 =\nλ I -\nZT\n2 Z2\n\n1 p\n1 + βZT\n2 Z1v1,\nn\nn\nwhich we can then plug in (16) to get\nT\n-1\nˆ\nˆ\n(1 + β)Z1 Z1v1 +\np\n1 + βZT\n1 Z2\n\nλ I -\nZT\n2 Z2\n\np\n1 + βZT\n=\nn\nn\n2 Z1v1\nλv1\nn\nn\nIf v1 = 0 (again, not properly justified here, see [Pau]) then this means that\n-1\nˆ\nˆ\nλ =\n(1 + β)ZT\n1 Z1 +\np\n1 + βZT\n1 Z2\n\nλ I -\nZT\n2 Z2\n\np\n1 + βZT\n(18)\nn\nn\nn\n2 Z1\nn\nFirst observation is that because Z1 ∈Rn has standard gaussian entries then 1 ZT\nn\n1 Z1 →1, meaning\nthat\n\"\nT\n\nT\n-1\nˆ\nˆ\nλ = (1 + β) 1 +\nZ\nn\n1 Z2\nλ I -\nZ\nT\nn\n2 Z2\nZ\nn\n2 Z1\n#\n.\n(19)\n\nConsider the SVD of Z\n= UΣV T where U ∈Rn×p and V ∈Rp\np\n×\nhave orthonormal columns\n(meaning that UT U = Ip\np and V T V = I\n×\np\n),\n×p\nand Σ is a diagonal matrix. Take D =\nΣ then\nn\nZT\n2 Z2 =\nV Σ2V T = V DV T ,\nn\nn\nmeaning that the diagonal entries of D correspond to the eigenvalues of 1 ZT\n2 Z2 which we expect to\nn\np\nbe distributed (in the limit) according to the Marchenko-Pastur distribution for\n-\nn\n≈γ. Replacing\nback in (19)\nT\nˆ\nˆ\n-1 1\nλ\n=\n(1 + β)\n\n1 +\nZT\n1/2\nT\nT\n1/2\nT\nn\nλ -\n\n√nUD\nV\nI\nV DV\n√nUD\nV\nZ1\nn\n\nT\nˆ\n=\nU\nV T\n\n(1 + β) 1\nT Z\nD1/\n-\n+\nλ I -V DV T\nV D1/2 UT Z1\n\nn\n1 -\n\nT\n\nˆ\n\n=\n(1 + β)\n+\nUT Z\n\nD1/2V T\nD1\n\nV\nh\nλ I -D\ni\nV T\nV\n/2 UT\n\nZ1\nn\n\nT\n=\n(1 + β)\n\n1 +\nUT Z\n\nD1/2\nˆ\nh\nλ I -D\ni-\nD1/2 UT Z1\n\n.\n\nn\nSince the columns of U are orthonormal, g := UT Z1 ∈Rp-1 is an isotropic gaussian (g ∼N(0, 1)), in\nfact,\nEggT = EUT Z\nUT Z\nT = EUT\nT\nZ1Z1 U = UT E\n\nZ1ZT\nT\n\nU = U U = I(p-1)×(p\n.\n-1)\nWe proceed\nˆλ\n=\n(1 + β)\n\n1 +\ngT D1/2 h\nˆ\n-1\nλ I -D\ni\nD1/2g\n\n=\n(1 + β)\n\nn\np\n\n-1\n1 + n\nX\ng2\nDjj\nj ˆλ -Djj\nj=1\n\nBecause we expect the diagonal entries of D to be distributed\n\naccording to the Marchenko-Pastur\ndistribution and g to be independent to it we expect that (again, not properly justified here, see [Pau])\nX\np-1\nj\ng2\nD j\nx\np -1\nj ˆ\nˆ\nλ -Djj\n→\nZ γ+\ndFγ(x).\nγ\nλ\nj=1\n-\n-x\nˆ\nWe thus get an equation for λ:\nγ\nˆλ = (1 + β)\n\n1 + γ\nZ\n+\nx\ndF (\nˆ\nγ x)\nγ\nλ\n-\n-x\n\n,\nwhich can be easily solved with the help of a program that computes integrals symbolically (such as\nMathematica) to give (you can also see [Pau] for a derivation):\nγ\nˆλ = (1 + β)\n\n1 + β\n\n,\n(20)\n\nwhich is particularly elegant (specially considering the size of some the equations used in the deriva-\ntion).\nAn important thing to notice is that for β = √γ we have\n√\n\nγ\nˆλ = (1 +\nγ)\n1 + √γ\n\n= (1 + √\nγ) = γ+,\nsuggesting that β = √γ is the critical point.\nIndeed this is the case and it is possible to make the above argument rigorous6 and show that in\nthe model described above,\n- If β\n√\n≤\nγ then\nλmax (Sn) →γ+,\n- and if β > √γ then\nλmax (Sn) →(1 + β)\n\nγ\n1 + β\n\n> γ+.\nAnother important question is wether the leading eigenvector actually correlates with the planted\nperturbation (in this case e1). Turns out that very similar techniques can answer this question as\nwell [Pau] and show that the leading eigenvector vmax of Sn will be non-trivially correlated with e1 if\nand only if β > √γ, more precisely:\n- If β\n√\n≤\nγ then\n|⟨vmax, e1⟩|2 →0,\n- and if β > √γ then\nγ\n|⟨\nβ2\nvmax, e1\n-\n⟩| →1 -γ .\nβ\n1.3.1\nA brief mention of Wigner matrices\nAnother very important random matrix model is the Wigner matrix (and it will show up later in this\ncourse). Given an integer n, a standard gaussian Wigner matrix W ∈Rn×n is a symmetric matrix\nwith independent\n(0, 1) entries (except for the fact that Wij = Wji). In the limit, the eigenvalues\nof √1\nN\nW are distributed according to the so-called semi-circular law\nn\ndSC(x) =\np\n4 -x21[\n2,2](x)dx,\n2π\n-\nand there is also a BBP like transition for this matrix ensemble [FP06]. More precisely, if v is a\nunit-norm vector in Rn and ξ ≥0 then the largest eigenvalue of √1 W + ξvvT satisfies\nn\n6Note that in the argument above it wasn't even completely clear where it was used that the eigenvalue was actually\nthe leading one. In the actual proof one first needs to make sure that there is an eigenvalue outside of the support and\nthe proof only holds for that one, you can see [Pau]\n\n- If ξ ≤1 then\nλ\n√W + ξvvT\nmax\nn\n\n→2,\n- and if ξ > 1 then\nλmax\n√W + ξvvT\n\n→ξ +\n.\n(21)\nn\nξ\n1.3.2\nAn open problem about spike models\nOpen Problem 1.3 (Spike Model for cut-SDP [MS15]. As since been solved [MS15]) Let\nW denote a symmetric Wigner matrix with i.i.d. entries Wij ∼N(0, 1). Also, given B ∈Rn×n sym-\nmetric, define:\nQ(B) = max {Tr(BX) : X ⪰0, Xii = 1} .\nDefine q(ξ) as\nξ\nq(ξ) = lim\nEQ\nn→infn\n\n11T +\nn\n√W\nn\n\n.\nWhat is the value of ξ , defined as\n∗\nξ = inf\n∗\n{ξ ≥0 : q(ξ) > 2}.\nIt is known that, if 0 ≤ξ ≤1, q(ξ) = 2 [MS15].\nOne can show that 1 Q(B)\nn\n≤λmax(B). In fact,\nmax {Tr(BX) : X ⪰0, Xii = 1} ≤max {Tr(BX) : X ⪰0, Tr X = n} .\nIt is also not difficult to show (hint: take the spectral decomposition of X) that\n(\nX\nn\nmax\nTr(BX) : X ⪰0,\nXii = n\ni=1\n)\n= λmax(B).\nThis means that for ξ > 1, q(ξ) ≤ξ + 1.\nξ\nRemark 1.4 Optimization problems of the type of max {Tr(BX) : X ⪰0, Xii = 1} are semidefinite\nprograms, they will be a major player later in the course!\nSince 1 E Tr 11T\nξ 11T + √1 W\nξ.\nn\nn\nn\n≈ξ, by taking X = 11T we expect that q(ξ) ≥\nThese observ\nh\nations\n\nimply that 1\ni\n≤ξ < 2 (see [MS15]). A reasonable conjecture is that it is equal\n∗\nto 1. This would imply that a certain semidefinite programming based algorithm for clustering under\nthe Stochastic Block Model on 2 clusters (we will discuss these things later in the course) is optimal\nfor detection (see [MS15]).7\nRemark 1.5 We remark that Open Problem 1.3 as since been solved [MS15].\n7Later in the course we will discuss clustering under the Stochastic Block Model quite thoroughly, and will see how\nthis same SDP is known to be optimal for exact recovery [ABH14, HWX14, Ban15c].\n\nGraphs, Diffusion Maps, and Semi-supervised Learning\n2.1\nGraphs\nGraphs will be one of the main objects of study through these lectures, it is time to introduce them.\nA graph G = (V, E) contains a set of nodes V = {v\nV\n1, . . . , vn} and edges E ⊆\n. An edge (i, j)\n∈E\nif vi and vj are connected. Here is one of the graph theorists favorite examples,\n\nthe Petersen graph8:\nFigure 1: The Petersen graph\nGraphs are crucial tools in many fields, the intuitive reason being that many phenomena, while\ncomplex, can often be thought about through pairwise interactions between objects (or data points),\nwhich can be nicely modeled with the help of a graph.\nLet us recall some concepts about graphs that we will need.\n- A graph is connected if, for all pairs of vertices, there is a path between these vertices on the\ngraph.\nThe number of connected components is simply the size of the smallest partition of\nthe nodes into connected subgraphs. The Petersen graph is connected (and thus it has only 1\nconnected component).\n- A clique of a graph G is a subset S of its nodes such that the subgraph corresponding to it is\ncomplete. In other words S is a clique if all pairs of vertices in S share an edge. The clique\nnumber c(G) of G is the size of the largest clique of G. The Petersen graph has a clique number\nof 2.\n- An independence set of a graph G is a subset S of its nodes such that no two nodes in S share\nan edge. Equivalently it is a clique of the complement graph Gc := (V, Ec). The independence\nnumber of G is simply the clique number of Sc. The Petersen graph has an independence number\nof 4.\n8The Peterson graph is often used as a counter-example in graph theory.\nThis graph is in public domain.\nSource: https://commons.wikimedia.org/\nwiki/File:Petersen_graph_3-coloring.svg.\n\nA particularly useful way to represent a graph is through its adjacency matrix. Given a graph\nG = (V, E) on n nodes (|V | = n), we define its adjacency matrix A ∈Rn×n as the symmetric matrix\nwith entries\n(\nij =\nif i, j)\nA\n∈E,\notherwise.\nSometime, we will consider weighted graphs G = (V, E, W), where edges may have weights wij,\nwe think of the weights as non-negative wij ≥0 and symmetric wij = wji.\n2.1.1\nCliques and Ramsey numbers\nCliques are important structures in graphs and may have important application-specific applications.\nFor example, in a social network graph (e.g., where people correspond to vertices and two vertices are\nconnected if the respective people are friends) cliques have a clear interpretation.\nA natural question is whether it is possible to have arbitrarily large graphs without cliques (and\nwithout it's complement having cliques), Ramsey answer this question in the negative in 1928 [Ram28].\nLet us start with some definitions: given a graph G we define r(G) as the size of the largest clique of\nindependence set, i.e.\nr(G) := max {c(G), c (Gc)} .\nGiven r, let R(r) denote the smallest integer n such that every graph G on n nodes must have r(G) ≥r.\nRamsey [Ram28] showed that R(r) is finite, for every r.\nRemark 2.1 It is easy to show that R(3) ≤6, try it!\nWe will need a simple estimate for what follows (it is a very useful consequence of Stirling's\napproximation, e.g.).\nProposition 2.2 For every k ≤n positive integers,\nn\nk\nk\n≤\nn\nr\n\n≤\nne\nk\nk\n\n.\nWe will show a simple lower bound on R(r). But first we introduce a random graph construction,\nan Erd os-Reny ı graph.\nDefinition 2.3 Given n and p, the random Erd os-Reny ı graph G(n, p) is a random graph on n vertices\nwhere each possible edge appears, independently, with probability p.\nThe proof of the lower bound on R(r) is based on the probabilistic method, a beautiful non-\nconstructive method pioneered by Paul Erd os to establish the existence of certain objects. The core\nidea is the simple observation that if a random variable has a certain expectation then there must exist\na draw of it whose value is at least that of the expectation. It is best understood with an example.\nTheorem 2.4 For every r ≥2,\nR( ) ≥\nr-\nr\n2 .\n\nProof.\nLet G be drawn from the G\nn, 1\n\ndistribution, G ∼G\nn, 1\nS\n\n. For every set\nof r nodes, let\nX(S) denote the random variable\nif S is a clique or independent set,\nX(S) =\notherwise.\nAlso, let X denote the random variable\nX =\nX\nX(S).\nV\nS∈( r)\nWe will proceed by estimating E [X]. Note that, by linearity of expectation,\nE [X] =\nX\nE [X(S)] ,\nS∈(V\nr)\nand E [X(S)] = Prob {S is a clique or independent set} =\n.\n(|S\nThis means that\n|\n2 )\nE [X] =\nS\nX\n∈(V\nr) 2(|S|\n2 ) =\nn\nr\n2(r\n2) =\nn\nr\n\nr(r-1) .\nBy Proposition 2.2 we have,\nE [X] ≤\nne\nr\nr\nr(r-1)\n= 2\nn\nr-1\ne\nr\nr\n.\nThat means that if n ≤2\nr-1\nand r ≥3 then E [X] < 1. This means that Prob{X < 1} > 0 and since\nX is a non-negative integer we must have Prob{X = 0} = Prob{X < 1} > 0 (another way of saying\nthat is that if E [X] < 1 then there must be an instance for which X < 1 and since X is a non-negative\nr-1\ninteger, we must have X = 0). This means that there exists a graph with 2\nnodes that does not\nhave cliques or independent sets of size r which implies the theorem.\nRemarkably, this lower bound is not very different from the best known. In fact, the best known\nlower and upper bounds known [Spe75, Con09] for R(r) are\n√\n(1 + o(1))\n2r\ne\n√\nr\n≤R(r) ≤r-c log r\nlog log r 4r.\n(22)\nOpen Problem 2.1 Recall the definition of R(r) above, the following questions are open:\n- What is the value of R(5)?\n- What are the asymptotics\nthe lower bound (\n√\nof R(s)? In particular, improve on the base of the exponent on either\n2) or the upper bound (4).\n\n- Construct a family of graphs G = (V, E) with increasing number of vertices for which there exists\nε > 0 such that9\n|V | ≲(1 + ε)r.\nIt is known that 43 ≤R(5) ≤49. There is a famous quote in Joel Spencer's book [Spe94] that\nconveys the difficulty of computing Ramsey numbers:\n\"Erd os asks us to imagine an alien force, vastly more powerful than us, landing on Earth and\ndemanding the value of R(5) or they will destroy our planet.\nIn that case, he claims, we should\nmarshal all our computers and all our mathematicians and attempt to find the value. But suppose,\ninstead, that they ask for R(6). In that case, he believes, we should attempt to destroy the aliens.\"\nThere is an alternative useful way to think about 22, by taking log2 of each bound and rearranging,\nwe get that\n1 + o(1)\n\nlog2 n ≤\nmin\nr(G)\n(2 + o(1)) log2 n\nG=(V,E), |V |=n\n≤\nThe current \"world record\" (see [CZ15, Coh15]) for deterministic construction of families of graphs with\n≲\n|\n| c\nsmall r(G) achieves r(G)\n2(log log V ) , for some constant c > 0. Note that this is still considerably\nlarger than polylog|V |. In contrast, it is very easy for randomized constructions to satisfy r(G) ≤\n2 log2 n, as made precise by the folloing theorem.\nTheorem 2.5 Let G ∼G\nn, 1\n\nbe and Erd os-Reny ı graph with edge probability 1. Then, with high\nprobability,10\nR(G) ≤2 log2(n).\nProof.\nGiven n, we are interested in upper bounding Prob {R(G) ≥⌈2 log2 n⌉}. and we proceed by\nunion bounding (and making use of Proposition 2.2):\nProb {R(G) ≥⌈2 log2 n⌉}\n=\nProb\n\n∃S⊂V, S\nS is a clique or independent set\n| |=⌈2 log2 n⌉\n\n=\nProb\n\n[\n{S is a clique or independent set}\nS∈(\nV\n⌈2 log\nn\n⌉\n\n)\n≤\nProb {S is a clique or independent set}\n\n=\n\n∈(\nV\nS\nX\n2 log\nn )\n⌈\n⌉\nn\n⌈2 log2 n⌉\n\n2(⌈2 log2 n⌉\n)\n≤\n\nn\n⌈2 log2 n⌉-1\ne\n⌈\n⌈2 log2 n⌉\n! 2 log2 n⌉\n≤\n\ne\n√\n⌈\n2 log2 n\n! 2 log2 n⌉\n≲\nn-Ω(1).\n9By ak ≲bk we mean that there exists a constant c such that ak ≤c bk.\n10We say an event happens with high probability if its probability is ≥1 -n-Ω(1).\n\nThe following is one of the most fascinating conjectures in Graph Theory\nOpen Problem 2.2 (Erd os-Hajnal Conjecture [EH89]) Prove or disprove the following:\nFor any finite graph H, there exists a constant δH > 0 such that any graph on n nodes that does\nnot contain H as a subgraph (is a H-free graph) must have\nr(G) ≳nδH.\nIt is known that r(G) ≳exp\ncH\n√log n , for some constant cH > 0 (see [Chu13] for a survey\non this conjecture).\nNote that this lower\n\nbound already shows that H-free graphs need to have\nconsiderably larger r(G). This is an amazing local to global effect, where imposing a constraint on\nsmall groups of vertices are connected (being H-free is a local property) creates extremely large cliques\nor independence sets (much larger than polylog(n) as in random Erd os-Reny ı graphs).\nSince we do not know how to deterministically construct graphs with r(G) ≤polylogn, one ap-\nproach could be to take G ∼G n, 1\nand check that indeed it has small clique and independence\nnumber. However, finding the largest\n\nclique on a graph is known to be NP-hard (meaning that there\nis no polynomial time algorithm to solve it, provided that the widely believed conjecture NP = P\nholds). That is a worst-case statement and thus it doesn't necessarily mean that it is difficult to find\nthe clique number of random graphs. That being said, the next open problem suggests that this is\nindeed still difficult.\nFirst let us describe a useful construct.\nGiven n and ω, let us consider a random graph G that\nconsists of taking a graph drawn from G n, 1 , picking ω of its nodes (say at random) and adding an\nedge between every pair of those ω nodes, thus \"planting\" a clique of size ω. This will create a clique\nof size ω in G. If ω > 2 log2 n this clique is larger\n\nthan any other clique that was in the graph before\nplanting. This means that, if ω > 2 log2 n, there is enough information in the graph to find the planted\nclique. In fact, one can simply look at all subsets of size 2 log2 n + 1 and check wether it is clique: if\nit is a clique then it very likely these vertices belong to the planted clique. However, checking all such\nsubgraphs takes super-polynomial time ∼nO(log n). This motivates the natural question of whether\nthis can be done in polynomial time.\nSince the degrees of the nodes of a G\nn, 1\n\nhave expected value n-1\nand standard deviation ∼√n,\nif ω > c√n (for sufficiently large constant c) then the degrees of the nodes involved in the planted\nclique will have larger degrees and it is easy to detect (and find) the planted clique. Remarkably,\nthere is no known method to work for ω significant smaller than this. There is a quasi-linear\np\ntime\nalgorithm [DM13] that finds the largest clique, with high probability, as long as ω ≥\nn\ne + o(√n).11\nOpen Problem 2.3 (The planted clique problem) Let G be a random graph constructed by tak-\ning a G\nn, 1\nand\n\nplanting a clique of size ω.\n1. Is there a polynomial time algorithm that is able to find the largest clique of G (with high prob-\nability) for ω\n√\n≪\nn. For example, for ω ≈\n√n\nlog n.\n11There is an amplification technique that allows one to find the largest clique for ω ≈c√n for arbitrarily small c in\npolynomial time, where the exponent in the runtime depends on c. The rough idea is to consider all subsets of a certain\nfinite size and checking whether the planted clique contains them.\n\n2. Is there a polynomial time algorithm that is able to distinguish, with high probability, G from a\ndraw of G\nn, 1\n\nfor ω ≪√n. For example, for ω ≈\n√n\nlog n.\n3. Is there a quasi-linear time algorithm able to find the largest clique of G (with high probability)\nfor ω ≤\n\n√e -ε\n√n, for some ε > 0.\nThis open problem is particularly important. In fact, the hypothesis that finding planted cliques\nfor small values of ω is behind several cryptographic protocols, and hardness results in average case\ncomplexity (hardness for Sparse PCA being a great example [BR13]).\n2.2\nDiffusion Maps\nDiffusion Maps will allows us to represent (weighted) graphs G = (V, E, W) in Rd, i.e. associating,\nto each node, a point in Rd. As we will see below, oftentimes when we have a set of data points\nx1, . . . , xn ∈Rp it will be beneficial to first associate to each a graph and then use Diffusion Maps to\nrepresent the points in d-dimensions, rather than using something like Principal Component Analysis.\nBefore presenting Diffusion Maps, we'll introduce a few important notions. Given G = (V, E, W)\nwe consider a random walk (with independent steps) on the vertices of V with transition probabilities:\nw\nProb {\nij\nX(t + 1) = j|X(t) = i} =\n,\ndeg(i)\nwhere deg(i) = P\nj wij. Let M be the matrix of these probabilities,\nwij\nMij =\n.\ndeg(i)\nIt is easy to see that Mij ≥0 and M1 = 1 (indeed, M is a transition probability matrix). Defining D\nas the diagonal matrix with diagonal entries Dii = deg(i) we have\nM = D-1W.\nIf we start a random walker at node i (X(0) = 1) then the probability that, at step t, is at node j\nis given by\nProb {X(t) = j|X(0) = i} =\nMt\n.\nij\nIn other words, the probability cloud of the random walker at poin\n\nt t, given that it started at node i\nis given by the row vector\nProb {X(t)|X(0) = i} = eT\ni Mt = Mt[i, :].\nRemark 2.6 A natural representation of the graph would be to associate each vertex to the probability\ncloud above, meaning\ni →Mt[i, :].\nThis would place nodes i1 and i2 for which the random walkers starting at i1 and i2 have, after t steps,\nvery similar distribution of locations. However, this would require d = n. In what follows we will\nconstruct a similar mapping but for considerably smaller d.\n\nM is not symmetric, but a matrix similar to M, S = D 2 MD-1\n2 is, indeed S = D-1\n2 WD-1\n2 . We\nconsider the spectral decomposition of S\nS = V ΛV T ,\nwhere V = [v1, . . . , vn] satisfies V T V = In\nn and Λ is diagonal with diagonal elements Λkk = λk (and\n×\nwe organize them as λ1 ≥λ2 ≥· · · ≥λn). Note that Svk = λkvk. Also,\nM = D-1\n2 SD\n2 = D-1\n2 V ΛV T D\n2 =\n\nD-1\n2 V\n\nΛ\n\nD\n2 V\nT\n.\nWe define Φ = D-1\n2 V with columns Φ = [φ1, . . . , φn] and Ψ = D 2 V with columns Ψ = [ψ1, . . . , ψn].\nThen\nM = ΦΛΨT ,\nand Φ, Ψ form a biorthogonal system in the sense that ΦT Ψ = In\nn or, equivalently, φT\nj ψk = δ\n×\njk.\nNote that φk and ψk are, respectively right and left eigenvectors of M, indeed, for all 1 ≤k ≤n:\nMφk = λkφk\nand ψT\nk M = λkψT\nk .\nAlso, we can rewrite this decomposition as\nM =\nX\nn\nλkφkψT\nk .\nk=1\nand it is easy to see that\nn\nMt =\nX\nλt\nkφkψT\nk .\n(23)\nk=1\nLet's revisit the embedding suggested on Remark 2.6. It would correspond to\nn\nvi →Mt[i, :] =\nX\nλt\nkφk(i)ψT\nk ,\nk=1\nit is written in terms of the basis ψk. The Diffusion Map will essentially consist of the representing a\nnode i by the coefficients of the above map\nλt\n1φ1(i)\nλt\n2φ2(i)\nv →\nt\ni\nM [i, :] =\n\n..\n\n.\nλt φn(i)\n\n,\n(24)\nn\nNote that M1 = 1, meaning that one of the right eigenvectors φk is simply a multiple of 1 and so it\ndoes not distinguish the different nodes of the graph. We will show that this indeed corresponds to\nthe the first eigenvalue.\nProposition 2.7 All eigenvalues λk of M satisfy |λk| ≤1.\n\nProof.\nLet φk be a right eigenvector associated with λk whose largest entry in magnitude is positive\nφk (imax). Then,\nn\nλkφk (imax) = Mφk (imax) =\nX\nMimax,jφk (j) .\nj=1\nThis means, by triangular inequality that, that\nn\n|λk| =\nX\nφ\nM max,j\n|\n|\nj=1\n|\n|\nk (j)\ni\nX\nn\n|\n)\n|\ni\nφ (i\n≤\nM\n|\nmax,j\nk\nmax\nj=1\n| = 1.\nRemark 2.8 It is possible that there are other eigenvalues with magnitude 1 but only if G is dis-\nconnected or if G is bipartite. Provided that G is disconnected, a natural way to remove potential\nperiodicity issues (like the graph being bipartite) is to make the walk lazy, i.e. to add a certain proba-\nbility of the walker to stay in the current node. This can be conveniently achieved by taking, e.g.,\nM′ = 2M + 1I.\nBy the proposition above we can take φ1 = 1, meaning that the first coordinate of (24) does not\nhelp differentiate points on the graph. This suggests removing that coordinate:\nDefinition 2.9 (Diffusion Map) Given a graph G = (V, E, W) construct M and its decomposition\nM = ΦΛΨT as described above. The Diffusion Map is a map φt : V →Rn-1 given by\nλt\n2φ2(i)\nλt\nφt (vi) =\n\nφ3(i)\n.\n.\n..\nλt\nnφn(i)\nThis map is still a map to n -1 dimensions. But note now that each coordinate has a factor of\nλt\nk which, if λk is small will be rather small for moderate values of t. This motivates truncating the\nDiffusion Map by taking only the first d coefficients.\nDefinition 2.10 (Truncated Diffusion Map) Given a graph G = (V, E, W) and dimension d, con-\nstruct M and its decomposition M = ΦΛΨT as described above. The Diffusion Map truncated to d\ndimensions is a map φt : V →Rd given by\n(d)\nφt\n(vi) =\n\nλt\n\n2φ2(i)\n\nλt\n3φ3(i)\n\n.\n.\n.\n\n.\nλt\nd+1φd+1(i)\n\nIn the following theorem we show that the euclidean distance in the diffusion map coordinates\n(called diffusion distance) meaningfully measures distance between the probability clouds after t iter-\nations.\nTheorem 2.11 For any pair of nodes vi1, vi2 we have\nX\nn\n∥φt (vi1) -φt (vi2)∥=\nj=1 deg(j) [Prob {X(t) = j|X(0) = i1} -Prob {X(t) = j|X(0) = i2}]2 .\nProof.\nNote that Pn\nj=1\n{\n[Prob X(t) = j|X(0) = i1} -Prob {X(t) = j\ndeg(j\n|X(0) = i2}] can be rewritten\n)\nas\nX\nn\nj=1 deg(j)\n\" n\nX\nk=1\nλt\nkφk(i1)ψk(j) -\nn\nX\nk=1\nλt\nkφk(i2)ψk(j)\n#2\n=\nn\nX\nj=1\ndeg(j)\n\" n\nX\nk=1\nλt\nk (φk(i1) -φk(i2)) ψk(j)\n#2\nand\nn\nX\nj=1\n\"X\nn\nX\nn \" n\nt\n#2\nX\nt\nψk(j)\nλk (φk(i1) -φk(i2)) ψk(j)\n=\nλk (φk(i1)\n(j)\nj=1\n-φk(i2))\ndeg\nk=1\nk=1\np\ndeg(j)\n#\nX\nn\n\nt\n-\n-1\n=\n\nλk (φk(i1)\nφk(i2)) D\nk=1\n2 ψk\n\n.\nNote that D-1\n2 ψk = vk which forms an orthonormal basis, meaning that\nX\nn\n\nλt\nk (φk(i1) -φk(i2)) D-\nk=1\n2 ψk\n\nn\n\n=\nX 2\nλt\nk (φk(i1) -φk(i2))\nk=1\nn\n\n=\nX 2\nλt\nkφ\nt\nk(i1) -λkφk(i2)\nk=2\n\n,\nwhere the last inequality follows from the fact that φ1 = 1 and concludes the proof of the theorem.\n2.2.1\nA couple of examples\nThe ring graph is a graph on n nodes {1, . . . , n} such that node k is connected to k -1 and k + 1 and\n1 is connected to n. Figure 2 has the Diffusion Map of it truncated to two dimensions\nAnother simple graph is Kn, the complete graph on n nodes (where every pair of nodes share an\nedge), see Figure 3.\n\nFigure 2: The Diffusion Map of the ring graph gives a very natural way of displaying (indeed, if one is\nasked to draw the ring graph, this is probably the drawing that most people would do). It is actually\nnot difficult to analytically compute the Diffusion Map of this graph and confirm that it displays the\npoints in a circle.\n2.2.2\nDiffusion Maps of point clouds\nVery often we are interested in embedding in Rd a point cloud of points x1, . . . , xn ∈Rp and necessarily\na graph. One option (as discussed before in the course) is to use Principal Component Analysis (PCA),\nbut PCA is only designed to find linear structure of the data and the low dimensionality of the dataset\nmay be non-linear. For example, let's say our dataset is images of the face of someone taken from\ndifferent angles and lighting conditions, for example, the dimensionality of this dataset is limited by\nthe amount of muscles in the head and neck and by the degrees of freedom of the lighting conditions\n(see Figure ??) but it is not clear that this low dimensional structure is linearly apparent on the pixel\nvalues of the images.\nLet's say that we are given a point cloud that is sampled from a two dimensional swiss roll embedded\nin three dimension (see Figure 4). In order to learn the two dimensional structure of this object we\nneed to differentiate points that are near eachother because they are close by in the manifold and not\nsimply because the manifold is curved and the points appear nearby even when they really are distant\nin the manifold (see Figure 4 for an example). We will achieve this by creating a graph from the data\npoints.\nOur goal is for the graph to capture the structure of the manifold. To each data point we will\nassociate a node. For this we should only connect points that are close in the manifold and not points\nthat maybe appear close in Euclidean space simply because of the curvature of the manifold. This\nis achieved by picking a small scale and linking nodes if they correspond to points whose distance\nis smaller than that scale. This is usually done smoothly via a kernel Kε, and to each edge (i, j)\nassociating a weight\nwij = Kε (∥xi -xj∥2) ,\na common example of a Kernel is K (u) = exp\n-1\nε\n2εu2\n, that gives essentially zero weight to edges\ncorresponding to pairs of nodes for which ∥xi -xj∥2 ≫√ε. We can then take the the Diffusion Maps\nof the resulting graph.\n\nFigure 3: The Diffusion Map of the complete graph on 4 nodes in 3 dimensions appears to be a regular\ntetrahedron suggesting that there is no low dimensional structure in this graph. This is not surprising,\nsince every pair of nodes is connected we don't expect this graph to have a natural representation in\nlow dimensions.\n2.2.3\nA simple example\nA simple and illustrative example is to take images of a blob on a background in different positions\n(image a white square on a black background and each data point corresponds to the same white\nsquare in different positions). This dataset is clearly intrinsically two dimensional, as each image\ncan be described by the (two-dimensional) position of the square. However, we don't expect this\ntwo-dimensional structure to be directly apparent from the vectors of pixel values of each image; in\nparticular we don't expect these vectors to lie in a two dimensional affine subspace!\nLet's start by experimenting with the above example for one dimension. In that case the blob is\na vertical stripe and simply moves left and right. We think of our space as the in the arcade game\nAsteroids, if the square or stripe moves to the right all the way to the end of the screen, it shows\nup on the left side (and same for up-down in the two-dimensional case). Not only this point cloud\nshould have a one dimensional structure but it should also exhibit a circular structure. Remarkably,\nthis structure is completely apparent when taking the two-dimensional Diffusion Map of this dataset,\nsee Figure 5.\nFor the two dimensional example, we expect the structure of the underlying manifold to be a\ntwo-dimensional torus. Indeed, Figure 6 shows that the three-dimensional diffusion map captures the\ntoroidal structure of the data.\n2.2.4\nSimilar non-linear dimensional reduction techniques\nThere are several other similar non-linear dimensional reduction methods. A particularly popular one\nis ISOMAP [?]. The idea is to find an embedding in Rd for which euclidean distances in the embedding\ncorrespond as much as possible to geodesic distances in the graph. This can be achieved by, between\npairs of nodes vi, vj finding their geodesic distance and then using, for example, Multidimensional\n\nFigure 4: A swiss roll point cloud (see, for example, [TdSL00]). The points are sampled from a two\ndimensional manifold curved in R3 and then a graph is constructed where nodes correspond to points.\nScaling to find points y\nd\ni ∈R that minimize (say)\nmin\nX ∥y\ni\ny\ny1,...,yn\n-\nj∥\n∈\ni,j\n-δij\nRd\n\n,\nwhich can be done with spectral methods (it is a good exercise to compute the optimal solution to\nthe above optimization problem).\n2.3\nSemi-supervised learning\nClassification is a central task in machine learning. In a supervised learning setting we are given many\nlabelled examples and want to use them to infer the label of a new, unlabeled example. For simplicity,\nlet's say that there are two labels, {-1, +1}.\nLet's say we are given the task of labeling point \"?\" in Figure 10 given the labeled points. The\nnatural label to give to the unlabeled point would be 1.\nHowever, let's say that we are given not just one unlabeled point, but many, as in Figure 11; then\nit starts being apparent that -1 is a more reasonable guess.\nIntuitively, the unlabeled data points allowed us to better learn the geometry of the dataset. That's\nthe idea behind Semi-supervised learning, to make use of the fact that often one has access to many\nunlabeled data points in order to improve classification.\nThe approach we'll take is to use the data points to construct (via a kernel Kε) a graph G =\n(V, E, W) where nodes correspond to points. More precisely, let l denote the number of labeled points\nwith labels f1, . . . , fl, and u the number of unlabeled points (with n = l + u), the first l nodes\nv1, . . . , vl correspond to labeled points and the rest vl+1, . . . , vn are unlabaled. We want to find a\nfunction f : V →{-1, 1} that agrees on labeled points: f(i) = fi for i = 1, . . . , l and that is \"as\nsmooth as possible\" the graph. A way to pose this is the following\nmin\nf:V →{-1,1}: f(i)=fi i=1,...,l\nX\nwij (f(i) -f(j)) .\ni<j\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 5: The two-dimensional diffusion map of the dataset of the datase where each data point is\nan image with the same vertical strip in different positions in the x-axis, the circular structure is\napparent.\nInstead of restricting ourselves to giving {-1, 1} we allow ourselves to give real valued labels, with the\nintuition that we can \"round\" later by, e.g., assigning the sign of f(i) to node i.\nWe thus are interested in solving\nmin\nX\nwij (f(i) -\nf(j)) .\nf:V →R: f(i)=fi i=1,...,l i<j\nIf we denote by f the vector (in Rn with the function values) then we are can rewrite the problem\nas\nX\nwij (f(i) -f(j))\n=\nX\nT\nwij [(ei -ej) f] [(ei -ej) f]\ni<j\ni<j\n=\nX\nT\nT\nT\nwij (\ni<j\nh\nei -ej) f\ni h\n(ei -ej) f\ni\n=\nX\nT\nwijfT (ei -ej) (ei -ej) f\ni<j\n=\nfT\n\nX\nT\nwij (ei -ej) (ei -ej)\ni<j\n\nf\nT\nThe matrix P\ni<j wij (ei -ej) (ei -ej)\nwill play a central role throughout this course, it is called\nthe graph Laplacian [Chu97].\nLG :=\nX\nT\nwij (ei -ej) (ei -ej) .\ni<j\nNote that the entries of LG are given by\nwij\nif i = j\n(LG)ij =\n-\n\ndeg(i)\nif i = j,\n\nFigure 6: On the left the data set considered and on the right its three dimensional diffusion map, the\nfact that the manifold is a torus is remarkably captured by the embedding.\nmeaning that\nLG = D -W,\nwhere D is the diagonal matrix with entries Dii = deg(i).\nRemark 2.12 Consider an analogous example on the real line, where one would want to minimize\nZ\nf′(x)2dx.\nIntegrating by parts\nZ\nf′(x)2dx = Boundary Terms -\nd\nZ\nf(x)f′′(x)dx.\nAnalogously, in R :\nZ\ndx\nZ\nd\nf\n∥∇f(x)∥2\n=\nX\nk=1\n∂\n∂xk\n(x)\ndx = B. T. -\nZ\nf(x)\nd\nX\nk=1\n∂2f (x)dx = B. T. -\nZ\nf(x)∆f(x)dx,\n∂x2\nk\nwhich helps motivate the use of the term graph Laplacian.\nLet us consider our problem\nmin\nfT LGf.\nf:V →R: f(i)=fi i=1,...,l\nWe can write\nDl\nl\nWl\nD =\n\n,\nW =\nWll\nWlu\n\nDl\nWl\nu\nfl\n,\nLG =\n-\n-\n,\nand f =\nDu\nWul\nWuu\n\n.\n-Wul\nDu -Wuu\n\nfu\n\nThen we want to find (recall that Wul = Wlu)\nmin fT\nl [Dl -Wll] fl -2fT\nfu∈Ru\nu Wulfl + fT\nu [Du -Wuu] fu.\n\nFigure 7: The two dimensional represention of a data set of images of faces as obtained in [TdSL00]\nusing ISOMAP. Remarkably, the two dimensionals are interpretable\nby first-order optimality conditions, it is easy to see that the optimal satisfies\n(Du -Wuu) fu = Wulfl.\nIf Du -Wuu is invertible12 then\n∗\n-\n-1\nfu = (Du\nWuu)\nWulfl.\nRemark 2.13 The function f function constructed is called a harmonic extension. Indeed, it shares\nproperties with harmonic functions in euclidean space such as the mean value property and maximum\nprinciples; if vi is an unlabeled point then\nf\n\n(i) = D-1\nu (Wulfl + Wuufu)\n\n=\ni\nw\ndeg\nX\nn\nijf(j),\n(i) j=1\nwhich immediately implies that the maximum and minimum value of f needs to be attained at a labeled\npoint.\n2.3.1\nAn interesting experience and the Sobolev Embedding Theorem\nLet us try a simple experiment. Let's say we have a grid on [-1, 1]d dimensions (with say md points\nfor some large m) and we label the center as +1 and every node that is at distance larger or equal\n12It is not difficult to see that unless the problem is in some form degenerate, such as the unlabeled part of the graph\nbeing disconnected from the labeled one, then this matrix will indeed be invertible.\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 8: The two dimensional represention of a data set of images of human hand as obtained\nin [TdSL00] using ISOMAP. Remarkably, the two dimensionals are interpretable\nto 1 to the center, as -1. We are interested in understanding how the above algorithm will label the\nremaining points, hoping that it will assign small numbers to points far away from the center (and\nclose to the boundary of the labeled points) and large numbers to points close to the center.\nSee the results for d = 1 in Figure 12, d = 2 in Figure 13, and d = 3 in Figure 14. While for d ≤2\nit appears to be smoothly interpolating between the labels, for d = 3 it seems that the method simply\nlearns essentially -1 on all points, thus not being very meaningful. Let us turn to Rd for intuition:\nLet's say that we wan\nR t to find a function in Rd that takes the value 1 at zero and -1 at the unit\nsphere, that minimizes\n∥∇f(x)∥2dx. Let us consider the following function on B0(1) (the ball\nB0(1)\ncentered at 0 with unit radius)\n|\nfε( ) =\n\n1 -2|x\nx\nif\nε\n|x| ≤ε\n-1\notherwise.\nA quick calculation suggest that\nZ\nZ\ndx\nB\n∥∇fε(x)∥\n=\n0(1)\nB0(ε) ε2 dx = vol(B0(ε)) 1 dx\n≈εd-2,\nε\nmeaning that, if d > 2, the performance of this function is improving as ε →0, explaining the results\nin Figure 14.\nOne way of thinking about what is going on is through the Sobolev Embedding Theorem. Hm Rd\nis the space of function whose derivatives up to order m are square-integrable in Rd, Sobolev Em\nbed-\nding Theorem says that if m > d\n\n2 then, if f ∈Hm Rd\nthen f must be continuous, which would rule\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 9: The two dimensional represention of a data set of handwritten digits as obtained in [TdSL00]\nusing ISOMAP. Remarkably, the two dimensionals are interpretable\nFigure 10: Given a few labeled points, the task is to label an unlabeled point.\nout the behavior observed in Figure 14. It also suggests that if we are able to control also second\nderivates of f then this phenomenon should disappear (since 2 > 3). While we will not describe\nit here in detail, there is, in fact, a way of doing this by minimizing not fT Lf but fT L2f instead,\nFigure 15 shows the outcome of the same experiment with the fT Lf replaced by fT L2f and con-\nfirms our intuition that the discontinuity issue should disappear (see, e.g., [NSZ09] for more on this\nphenomenon).\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 11: In this example we are given many unlabeled points, the unlabeled points help us learn\nthe geometry of the data.\nFigure 12: The d = 1 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 1 it appears to smoothly interpolate between the labeled\npoints.\nSpectral Clustering and Cheeger's Inequality\n3.1\nClustering\nClustering is one of the central tasks in machine learning. Given a set of data points, the purpose of\nclustering is to partition the data into a set of clusters where data points assigned to the same cluster\ncorrespond to similar data points (depending on the context, it could be for example having small\ndistance to each other if the points are in Euclidean space).\n3.1.1\nk-means Clustering\nOne the most popular methods used for clustering is k-means clustering. Given x , . . . , x\n∈Rp\nn\nthe\nk-means clustering partitions the data points in clusters S1 ∪· · · ∪Sk with centers μ1, . . . , μk ∈Rp as\nthe solution to:\nk\nmin\nX X\n∥xi -μl\n.\n,...,S\npartition\nk\n∥\n(25)\nS\nμ1,...,μk l=1 i∈Si\n\nFigure 13: The d = 2 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 2 it appears to smoothly interpolate between the labeled\npoints.\nNote that, given the partition, the optimal centers are given by\nμl =\nx\n|Sl|\nX\ni.\ni∈Sl\nLloyd's algorithm [Llo82] (also known as the k-means algorithm), is an iterative algorithm that\nalternates between\n- Given centers μ1, . . . , μk, assign each point xi to the cluster\nl = argminl=1,...,k ∥xi -μl∥.\n- Update the centers μl =\nx\n|Sl|\ni∈S\ni.\nl\nUnfortunately, Lloyd's algorithm\nP\nis not guaranteed to converge to the solution of (25). Indeed,\nLloyd's algorithm oftentimes gets stuck in local optima of (25). A few lectures from now we'll discuss\nconvex relaxations for clustering, which can be used as an alternative algorithmic approach to Lloyd's\nalgorithm, but since optimizing (25) is NP-hard there is not polynomial time algorithm that works\nin the worst-case (assuming the widely believed conjecture P = NP)\nWhile popular, k-means clustering has some potential issues:\n- One needs to set the number of clusters a priori (a typical way to overcome this issue is by trying\nthe algorithm for different number of clusters).\n- The way (25) is defined it needs the points to be defined in an Euclidean space, oftentimes\nwe are interested in clustering data for which we only have some measure of affinity between\ndifferent data points, but not necessarily an embedding in Rp (this issue can be overcome by\nreformulating (25) in terms of distances only).\n\nFigure 14: The d = 3 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 3 the solution appears to only learn the label -1.\nFigure 15: The d = 3 example of the use of this method with the extra regularization fT L2f to the\nexample described above, the value of the nodes is given by color coding. The extra regularization\nseems to fix the issue of discontinuities.\n- The formulation is computationally hard, so algorithms may produce suboptimal instances.\n- The solutions of k-means are always convex clusters. This means that k-means may have diffi-\nculty in finding cluster such as in Figure 17.\n3.2\nSpectral Clustering\nA natural way to try to overcome the issues of k-means depicted in Figure 17 is by using Diffusion\nMaps: Giventhe data points we construct a weighted graph G = (V, E, W) using a kernel Kε, such as\nKε(u) = exp\n1 u2\n, by associating each point to a vertex and, for which pair of nodes, set the edge\n2ε\nweight as\nwij = Kε (∥xi -xj∥) .\n\nFigure 16: Examples of points separated in clusters.\nRecall the construction of a matrix M = D-1W as the transition matrix of a random walk\nw\nProb {\nij\nX(t + 1) = j|X(t) = i} =\n= Mij,\ndeg(i)\nwhere D is the diagonal with Dii = deg(i). The d-dimensional Diffusion Maps is given by\n(d)\nφt (i) =\n\nt\n\nwhere\n\nλ φ2(i)\n...\nλt\nd+1φd+1(i)\nM\n,\n= ΦΛΨT where Λ is the diagonal matrix with the eigenvalues of M and Φ and Ψ are,\nrespectively, the right and left eigenvectors of M (note that they form a bi-orthogonal system, ΦT Ψ =\nI).\nIf we want to cluster the vertices of the graph in k clusters, then it is natural to truncate the\nDiffusion Map to have k -1 dimensions (since in k -1 dimensions we can have k linearly separable\nsets). If indeed the clusters were linearly separable after embedding then one could attempt to use\nk-means on the embedding to find the clusters, this is precisely the motivation for Spectral Clustering.\nAlgorithm 3.1 (Spectral Clustering) Given a graph G = (V, E, W) and a number of clusters k\n(and t), Spectral Clustering consists in taking a (k -1) dimensional Diffusion Map\n\nλt\n2φ2(i)\n(k\nφ\n-1)\n.\nt\n(i) =\n..\nλt\n\nkφk(i)\n\n(k\n1)\n(k\n1)\n\n(k\n1)\n\nand clustering the points φt\n-(1), φt\n-(2), . . . , φt\n-(n) ∈Rk-1 using, for example, k-means clus-\ntering.\n\nFigure 17: Because the solutions of k-means are always convex clusters, it is not able to handle some\ncluster structures.\n3.3\nTwo clusters\nWe will mostly focus in the case of two cluster (k = 2). For k = 2, Algorithm 3.1 consists in assigning\nto each vertex i a real number φ2(i) and then clustering the points in the real line. Note in R, clustering\nreduces to setting a threshold τ and taking S = {i ∈V : φ2(i) ≤τ}. Also, it is computationally\ntractable to try all possible thresholds (there are ≤n different possibilities).\nFigure 18: For two clusters, spectral clustering consists in assigning to each vertex i a real number\nφ2(i), then setting a threshold τ and taking S = {i ∈V : φ2(i) ≤τ}.\nAlgorithm 3.2 (Spectral Clustering for two clusters) Given a graph G = (V, E, W), consider\nthe two-dimensional Diffusion Map\ni →φ2(i).\nset a threshold τ (one can try all different possibilities) and set\nS = {i ∈V : φ2(i) ≤τ}.\n\nIn what follows we'll give a different motivation for Algorithm 3.2.\n3.3.1\nNormalized Cut\nGiven a graph G = (V, E, W), a natural measure to measure a vertex partition (S, Sc) is\ncut(S) =\nX X\nwij.\ni∈S j∈Sc\nNote however that the minimum cut is achieved for S = ∅(since cut(∅) = 0) which is a rather\nmeaningless choice of partition.\nRemark 3.3 One way to circumvent this issue is to ask that |S| = |Sc| (let's say that the number of\nvertices n = |V | is even), corresponding to a balanced partition. We can then identify a partition with\na label vector y ∈P\n{±1}n where yi = 1 is i ∈S, and yi = -1 otherwise. Also, the balanced condition\nn\ncan be written as\ni=1 yi = 0. This means that we can write the minimum balanced cut as\nmin cut(S) =\nmin\nS⊂V\ny\n|S|=|Sc\n∈{-1,1}n\n|\n1T y=0\nX\ni≤j\nwij (yi -yj)2 = 1\nmin\nyT LGy,\n4 y∈{-1,1}n\n1T y=0\nwhere L\nG = D -W is the graph Laplacian.\n.\nSince asking for the partition to be balanced is too restrictive in many cases, there are several\nways to evaluate a partition that are variations of cut(S) that take into account the intuition that one\nwants both S and Sc to not be too small (although not necessarily equal to |V |/2). A prime example\nis Cheeger's cut.\nDefinition 3.4 (Cheeger's cut) Given a graph and a vertex partition (S, Sc), the cheeger cut (also\nknown as conductance, and sometimes expansion) of S is given by\ncut(S)\nh(S) =\n,\nmin{vol(S), vol(Sc)}\nwhere vol(S) = P\ni∈S deg(i).\nAlso, the Cheeger's constant of G is given by\nhG = min h(S).\nS⊂V\nA similar object is the Normalized Cut, Ncut, which is given by\ncut(S)\nNcut(S) = vol(S) + cut(Sc).\nvol(Sc)\nNote that Ncut(S) and h(S) are tightly related, in fact it is easy to see that:\nh(S) ≤Ncut(S) ≤2h(S).\n13W is the matrix of weights and D the degree matrix, a diagonal matrix with diagonal entries Dii = deg(i).\n\nBoth h(S) and Ncut(S) favor nearly balanced partitions, Proposition 3.5 below will give an inter-\npretation of Ncut via random walks.\nLet us recall the construction form previous lectures of a random walk on G = (V, E, W):\nw\nProb {X\n}\nij\n(t + 1) = j|X(t) = i =\n= Mij,\ndeg(i)\nwhere M = D-1W. Recall that M = ΦΛΨT where Λ is the diagonal matrix with the eigenvalues λk\nof M and Φ and Ψ form a biorthogonal system ΦT Ψ = I and correspond to, respectively, the right\n-1\nand left eigenvectors of M. Moreover they are given by Φ = D\n2 V and Ψ = D\n2 V where V T V = I\nand D-1\n2 WD-1\nT\n2 = V ΛV\nis the spectral decomposition of D-2 WD-1\n2 .\nRecall also that M1 = 1, corresponding to Mφ1 = φ1, which means that ψT\n1 M = ψT\n1 , where\nψ1 = D 2 v1 = Dφ1 = [deg(i)]1≤i≤n .\nThis means that\nh\ndeg(i)\nis the stationary distribution of this random walk. Indeed it is easy\nvol(G) 1≤i≤n\nto check that, if X(t) has\ni\na certain distribution pt then X(t + 1) has a distribution pt+1 given by\npT\nt+1 = pT\nt M\nProposition 3.5 Given a graph G = (V, E, W) and a partition (S, Sc) of V , Ncut(S) corresponds\nto the probability, in the random walk associated with G, that a random walker in the stationary\ndistribution goes to Sc conditioned on being in S plus the probability of going to S condition on being\nin Sc, more explicitly:\nNcut(S) = Prob {X(t + 1) ∈Sc|X(t) ∈S} + Prob {X(t + 1) ∈S|X(t) ∈Sc} ,\n)\nwhere Prob{X(t) = }\ndeg(i\ni =\n.\nvol(G)\nProof.\nWithout loss of generality we can take t = 0. Also, the second term in the sum corresponds\nto the first with S replaced by Sc and vice-versa, so we'll focus on the first one. We have:\nProb X(1)\nSc\nX(0)\nS\nProb {X(1) ∈Sc\n{\n∈\n}\n|X(0)\n∩\n∈S\n∈\n}\n=\nProb {X(0) ∈S}\n=\nP\ni∈S\nP\nj∈Sc Prob {X(1) ∈j ∩X(0) ∈i}\ndeg\ni\n=\nP\ni∈S Prob {X(0) = i}\nP\n∈S\nP\n(i)\nj∈Sc vol(G)\nwij\ndeg(i)\nP\ni∈S\ndeg(i)\nP\nvol(G)\ni\n=\n∈S\nP\nj∈Sc wij\nP\ni∈S deg(i)\n=\ncut(S).\nvol(S)\nAnalogously,\nProb {X(t + 1) ∈S|X(t) ∈Sc\ncut(S)\n} =\n,\nvol(Sc)\nwhich concludes the proof.\n\n3.3.2\nNormalized Cut as a spectral relaxation\nBelow we will show that Ncut can be written in terms of a minimization of a quadratic form involving\nthe graph Laplacian LG, analogously to the balanced partition.\nRecall that balanced partition can be written as\nmin\nyT LGy.\n4 y∈{-1,1}n\n1T y=0\nAn intuitive way to relax the balanced condition is to allow the labels y to take values in two\ndifferent real values a and b (say yi = a if i ∈S and yj = b if i ∈/ S) but not necessarily ±1. We can\nthen use the notion of volume of a set to ensure a less restrictive notion of balanced by asking that\na vol (S) + b vol (Sc) = 0,\nwhich corresponds to 1T Dy = 0.\nWe also need to fix a scale/normalization for a and b:\na2 vol (S) + b2 vol (Sc) = 1,\nwhich corresponds to yT Dy = 1.\nThis suggests considering\nmin\nyT LGy.\ny∈{a,b}n\n1T Dy=0, yT Dy=1\nAs we will see below, this corresponds precisely to Ncut.\nProposition 3.6 For a and b to satisfy a vol (S) + b vol (Sc) = 0 and a2 vol (S) + b2 vol (Sc) = 1 it\nmust be that\na =\n\nvol(Sc)\nvol(S) vol(G)\nand\nb = -\n\nvol(S)\nvol(Sc) vol(G)\n,\ncorresponding to\nyi =\n\nvol(Sc)\nvol(S) vol(G)\n\nif i ∈S\n-\n\nvol(S)\nvol(Sc) vol(G)\nif i ∈Sc.\nProof.\nThe proof involves only doing simple algebraic manipulations together with noticing that\nvol(S) + vol(Sc) = vol(G).\nProposition 3.7\nNcut(S) = yT LGy,\nwhere y is given by\nyi =\n\nvol(Sc)\n\nvol(S) vol(G)\nif i ∈S\n-\n\nvol(S)\nvol(Sc) vol(G)\nif i ∈Sc.\n\nProof.\nyT LGy\n=\nwij(y\ni\n2 i,j\n-yj)\n=\nX X\nw\nij(yi -yj)\ni∈S j∈Sc\n=\nX X\nvol(Sc)\nvol(S)\nwij\n\"\n+\nc\n\nvol(S) vol(G)\n\nvol(Sc) vol(G)\ni∈S j S\n#\n=\nX\ni∈S j\nX\n∈\nv\nwij\n+\nc\nvol(G)\nvol(Sc)\nol(S) + 2\nvol(S)\nvol(Sc)\n∈S\n\nX X\nvol(Sc)\nvol(S)\nvol(S)\nvol(Sc)\n=\nwij\n+\n+\n+\nc\nvol(G)\nvol(S)\nvol(Sc)\nvol(S)\nvol(Sc)\ni∈S j∈S\n\n1 X\n=\nX\ni∈S\nX\nj∈Sc\nwij\n\n+\nvol(S)\nvol(Sc)\n\n=\ncut(S)\n\nvol(S) +\nvol(Sc)\n=\nNcut(S).\n\nThis means that finding the minimum Ncut corresponds to solving\nmin\nyT LGy\ns. t.\ny ∈{a, b}n for some a and b\nyT\n(26)\nDy = 1\nyT D1 = 0.\nSince solving (26) is, in general, NP-hard, we consider a similar problem where the constraint that\ny can only take two values is removed:\nmin\nyT LGy\ns. t.\ny ∈Rn\nyT\n(27)\nDy = 1\nyT D1 = 0.\nGiven a solution of (27) we can round it to a partition by setting a threshold τ and taking\nS = {i ∈V : yi ≤τ}.\nWe will see below that (27) is an eigenvector problem (for this reason we\ncall (27) a spectral relaxation) and, moreover, that the solution corresponds to y a multiple of φ2\nmeaning that this approach corresponds exactly to Algorithm 3.2.\nIn order to better see that (27) is an eigenvector problem (and thus computationally tractable),\nset z = D 2 y and LG = D-1\n2 LGD-1\n2 , then (27) is equivalent\n\nmin\nzT LGz\ns. t.\nz ∈Rn\n∥z∥2 = 1\n(28)\nT\nD 2 1\nz = 0.\nNote that LG = I -D-1\n2 WD-1\n2 . We order\n\nits\n\neigenvalues in increasing order 0 = λ1 (LG) ≤\nλ2 (LG) ≤· · · ≤λn (L\nG). The eigenvector associated to the smallest eigenvector is given by D 2 1 this\nmeans that (by the variational interpretation of the eigenvalues) that the minimum of (28) is λ2 (LG)\nand the minimizer is given by the second smallest eigenvector of LG = I -D-2 WD-2 , which is the\nsecond largest eigenvector of D-2 WD-2 which we know is v2. This means that the optimal y in (27)\nis given by φ2 = D-1\n2 v2. This confirms that this approach is equivalent to Algorithm 3.2.\nBecause the relaxation (27) is obtained from (26) by removing a constraint we immediately have\nthat\nλ2 (LG) ≤min Ncut(S).\nS⊂V\nThis means that\n1λ2 (LG) ≤hG.\nIn what follows we will show a guarantee for Algorithm 3.2.\nLemma 3.8 There is a threshold τ producing a partition S such that\nh(S) ≤\np\n2λ2 (LG).\nThis implies in particular that\nh(S) ≤\np\n4hG,\nmeaning that Algorithm 3.2 is suboptimal at most by a square root factor.\nNote that this also directly implies the famous Cheeger's Inequality\nTheorem 3.9 (Cheeger's Inequality) Recall the definitions above. The following holds:\n1λ2 (LG) ≤hG ≤\np\n2λ2 (\nG\nL ).\nCheeger's inequality was first established for manifolds by JeffCheeger in 1970 [Che70], the graph\nversion is due to Noga Alon and Vitaly Milman [Alo86, AM85] in the mid 80s.\nThe upper bound in Cheeger's inequality (corresponding to Lemma 3.8) is more interesting but\nmore difficult to prove, it is often referred to as the \"the difficult part\" of Cheeger's inequality. We\nwill prove this Lemma in what follows. There are several proofs of this inequality (see [Chu10] for\nfour different proofs!). The proof that follows is an adaptation of the proof in this blog post [Tre11]\nfor the case of weighted graphs.\nProof. [of Lemma 3.8]\nWe will show that given y ∈Rn satisfying\nyT L\nR\nGy\n(y) := yT Dy ≤δ,\n\nand yT D1 = 0. there is a \"rounding of it\", meaning a threshold τ and a corresponding choice of\npartition\nS = {i ∈V : yi ≤τ}\nsuch that\nh(S)\n√\n≤\n2δ,\nsince y = φ2 satisfies the conditions and gives δ = λ2 (LG) this proves the Lemma.\nWe will pick this threshold at random and use the probabilistic method to show that at least one\nof the thresholds works.\nFirst we can, without loss of generality, assume that y1 ≤· ≤yn (we can simply relabel the\nvertices). Also, note that scaling of y does not change the value of R(y). Also, if yD1 = 0 adding\na multiple of 1 to y can only decrease the value of\n(y): the numerator does not change and the\ndenominator (y + c1)T D(y + c1) = yT Dy + c2\nT\nR\n1 D1 ≥yT Dy.\nThis means that we can construct (from y by adding a multiple of 1 and scaling) a vector x such\nthat\nx1 ≤... ≤xn, xm = 0,\nand x2\n1 + x2\nn = 1,\nand\nxT LGx ≤δ,\nxT Dx\nwhere m be the index for which vol({1, . . . , m-1}) ≤vol({m, . . . , n}) but vol({1, . . . , m}) > vol({m, . . . , n}).\nWe consider a random construction of S with the following distribution. S = {i ∈V : xi ≤τ}\nwhere τ ∈[x1, xn] is drawn at random with the distribution\nb\nProb {τ ∈[a, b]} =\nZ\na\n|τ|dτ,\nwhere x1 ≤a ≤b ≤xn.\nIt is not difficult to check that\nProb {τ ∈[a, b]} =\nb2 -a2\nif a and b have the same sign\na2 + b2\nif a and b have different signs\nLet us start by estimating E cut(S).\nE cut(S)\n=\nE\nw\nX\ni∈V j\nX\nij1(S,Sc) cuts the edge (i,j)\n∈V\n=\nw\nX\nij Prob{(S, Sc) cuts the edge (i, j)}\ni∈V j\nX\n∈V\nNote that Prob{(S, Sc) cuts the edge (i, j)} is\nx2\ni -x2\nj\nis x\ni and xj have the same sign and xi +xj\notherwise. Both cases can be conveniently upper bounded by |xi -xj| (|xi| + |xj|). This means that\nE cut(S)\n≤\nw\nX\nij\ni,j\n|xi -xj| (|xi| + |xj|)\n≤\nsX\nw (x -x )2\nij\ni\nj\nij\nsX\nwij(\nij\n|xi| + |xj|) ,\n\nwhere the second inequality follows from the Cauchy-Schwarz inequality.\nFrom the construction of xX\nwe know that\nw\nij(xi\nxj) = 2xT LGx\n2δxT Dx.\nij\n-\n≤\nAlso,\nX\nwij(\nij\n|xi| + |x |)2 ≤\nX\nw 2x2 + 2x2. = 2\nX\ndeg(i)x2\nT\nj\nij\ni\nj\ni\n+ 2\ndeg(j)xj\n= 4x Dx.\nij\ni\n!\n\nX\nj\n\nThis means that\n\nE cut(S) ≤2\n√\n2δxT Dx\n√\n4xT Dx =\n√\n2δ xT Dx.\nOn the other hand,\nn\nE min{vol S, vol Sc} =\ndeg(i) Prob\ni=1\n{xi is in the smallest set (in terms of volume)},\nto break ties, if vol(S) = vol(S\nX\nc) we take the \"smallest\" set to be the one with the first indices.\nNote that m is always in the largest set. Any vertex j < m is in the smallest set if xj ≤τ ≤xm = 0\nand any j > m is in the smallest set if 0 = xm ≤τ ≤xj. This means that,\nProb{xi is in the smallest set (in terms of volume) = x2\nj.\nWhich means that\nn\nE min{vol S, vol Sc} =\nX\ndeg(i)x2\ni = xT Dx.\ni=1\nHence,\nE cut(S)\nE min{vol S, vol Sc} ≤\n√\n2δ.\nE cut(S)\ncut(S)\nNote however that because\nis not necessarily the same as\nand so,\nE min{vol S,vol Sc\nE\n}\nmin{vol S,vol Sc}\nwe do not necessarily have\ncut(S)\nE\n√\nmin{vol S, vol Sc\n≤\n2δ.\n}\nHowever, since both random variables are positive,\nE cut(S) ≤E min{vol S, vol Sc √\n}\n2δ,\nor equivalently\nE\nh\ncut(S) -min{vol S, vol Sc √\n}\n2δ ≤0,\nwhich guarantees, by the probabilistic method, the existence of S\ni\nsuch that\ncut(S) ≤min{vol S, vol Sc √\n}\n2δ,\nwhich is equivalent to\ncut(S)\nh(S) =\n√\n2δ,\nmin{vol S, vol Sc\n≤\n}\nwhich concludes the proof of the Lemma.\n\n3.4\nSmall Clusters and the Small Set Expansion Hypothesis\nWe now restrict to unweighted regular graphs G = (V, E).\nCheeger's inequality allows to efficiently approximate its Cheeger number up to a square root\nfactor. It means in particular that, given G = (V, E) and φ we can efficiently between the cases where:\nhG ≤φ or hG ≥2√φ. Can this be improved?\nOpen Problem 3.1 Does there exists a constant c > 0 such that it is NP-hard to, given φ, and G\ndistinguis between the cases\n1. hG ≤φ, and\n2. hG ≥c√φ?\nIt turns out that this is a consequence [RST12] of an important conjecture in Theoretical Computer\nScience (see [BS14] for a nice description of it). This conjecture is known [RS10] to imply the Unique-\nGames Conjecture [Kho10], that we will discuss in future lectures.\nConjecture 3.10 (Small-Set Expansion Hypothesis [RS10]) For every ε > 0 there exists δ > 0\nsuch that it is NP-hard to distinguish between the cases\n)\n1. There exists a subset S ⊂\ncut(S\nV with vol(S) = δ vol(V ) such that vol(S) ≤ε,\ncut(S)\n2.\n≥1\nol(\nS\n-ε, for every S\nv\nvol( )\n⊂V satisfying\nS) ≤δ vol(V ).\n3.5\nComputing Eigenvectors\nSpectral clustering requires us to compute the second smallest eigenvalue of LG. One of the most\nefficient ways of computing eigenvectors is through the power method. For simplicity we'll consider\nthe case on which we are computing the leading eigenvector of a matrix A ∈Rn×n with m non-\nzero entries, for which |λmax(A)| ≥|λmin(A)| (the idea is easily adaptable).\nThe power method\nproceeds by starting with a guess y0\nAyt\nand taking iterates yt+1 = ∥Ayt . One can show [KW92] that the\n∥\nvariantes of the power method can find a vector x in randomized time O δ-1(m + n) log n satisfying\nxT Ax ≥λmax(A)(1 -δ)xT x. Meaning that an approximate solution can\nbe found in quasi-linear\ntime.14\n\nOne drawback of the power method is that when using it, one cannot be sure, a posteriori, that\nthere is no eigenvalue of A much larger than what we have found, since it could happen that all our\nguesses were orthogonal to the corresponding eigenvector. It simply guarantees us that if such an\neigenvalue existed, it would have been extremely likely that the power method would have found it.\nThis issue is addressed in the open problem below.\nOpen Problem 3.2 Given a symmetric matrix M with small condition number, is there a quasi-\nlinear time (on n and the number of non-zero entries of M) procedure that certifies that M ⪰0. More\nspecifically, the procedure can be randomized in the sense that it may, with some probably not certify\nthat M ⪰0 even if that is the case, what is important is that it never produces erroneous certificates\n(and that it has a bounded-away-from-zero probably of succeeding, provided that M ⪰0).\n14Note that, in spectral clustering, an error on the calculation of φ2 propagates gracefully to the guarantee given by\nCheeger's inequality.\n\nThe Cholesky decomposition produces such certificates, but we do not know how to compute it\nin quasi-linear time. Note also that the power method can be used in αI -M to produce certifi-\ncates that have arbitrarily small probability of being false certificates. Later in these lecture we will\ndiscuss the practical relevance of such a method as a tool to quickly certify solution produced by\nheuristics [Ban15b].\n3.6\nMultiple Clusters\nGiven a graph G = (V, E, W), a natural way of evaluating k-way clusterign is via the k-way expansion\nconstant (see [LGT12]):\n,...,S\ncut(S)\nρG(k) =\nmin\nmax\nS1\nk l=1,...,k\nv\n\n,\nol(S)\nwhere the maximum is over all choice of k disjoin subsets of V (but not necessarily forming a partition).\nAnother natural definition is\nφG(k) =\nmin\nS:vol S≤1\nk vol(G)\ncut(S).\nvol(S)\nIt is easy to see that\nφG(k) ≤ρG(k).\nThe following is known.\nTheorem 3.11 ([LGT12]) Let G = (V, E, W) be a graph and k a positive integer\nρG(k) ≤O\nk2 p\nλk,\n(29)\nAlso,\nρG(k) ≤O\np\nλ2k log k\n\n.\nOpen Problem 3.3 Let G = (V, E, W) be a graph and k a positive integer, is the following true?\nρG(k) ≤polylog(k)\np\nλk.\n(30)\nWe note that (30) is known not to hold if we ask that the subsets form a partition (meaning that\nevery vertex belongs to at least one of the sets) [LRTV12]. Note also that no dependency on k would\ncontradict the Small-Set Expansion Hypothesis above.\n\nConcentration Inequalities, Scalar and Matrix Versions\n4.1\nLarge Deviation Inequalities\nConcentration and large deviations inequalities are among the most useful tools when understanding\nthe performance of some algorithms. In a nutshell they control the probability of a random variable\nbeing very far from its expectation.\nThe simplest such inequality is Markov's inequality:\nTheorem 4.1 (Markov's Inequality) Let X ≥0 be a non-negative random variable with E[X] <\ninf. Then,\nE[X]\nProb{X > t} ≤\nt\n.\n(31)\nProof.\nLet t > 0. Define a random variable Yt as\nYt =\nif X ≤t\nt\nif X > t\nClearly, Yt ≤X, hence E[Yt] ≤E[X], and\nt Prob{X > t} = E[Yt] ≤E[X],\nconcluding the proof.\nMarkov's inequality can be used to obtain many more concentration inequalities. Chebyshev's\ninequality is a simple inequality that control fluctuations from the mean.\nTheorem 4.2 (Chebyshev's inequality) Let X be a random variable with E[X2] < inf. Then,\nVar(X)\nProb{|X -EX| > t} ≤\n.\nt2\nProof. Apply Markov's inequality to the random variable (X -E[X])2 to get:\n(X\nProb X\nEX > t = Prob\nt2\nE\n(X\nEX)2 >\n\n-EX)2\n{|\n-\n|\n}\n{\n-\n} ≤\nt2\n\nVar(X)\n=\n.\nt2\n4.1.1\nSums of independent random variables\nIn what follows we'll show two useful inequalities involving sums of independent random variables.\nThe intuitive idea is that if we have a sum of independent random variables\nX = X1 + · · · + Xn,\nwhere Xi are iid centered random variables, then while the value of X can be of order O(n) it will very\nlikely be of order O(√n) (note that this is the order of its standard deviation). The inequalities that\nfollow are ways of very precisely controlling the probability of X being larger than O(√n). While we\ncould use, for example, Chebyshev's inequality for this, in the inequalities that follow the probabilities\nwill be exponentially small, rather than quadratic, which will be crucial in many applications to come.\n\nTheorem 4.3 (Hoeffding's Inequality) Let X1, X2, . . . , Xn be independent bounded random vari-\nables, i.e., |Xi| ≤a and E[Xi] = 0. Then,\n(X\nn\n\n)\n\nt2\nProb\n\nXi > t\n=1\n≤2 exp\ni\n-2na2\n\n.\nThe inequality implies that fluctuations larger than O (√n) have small probability. For example,\nfor t = a√2n log n we get that the probability is at most 2 .\nn n\nProof.\nWe first get a probability bound for the event\ni=1 Xi > t. The proof, again, will follow\nfrom Markov. Since we want an exponentially small probability, we use a classical trick that involves\nexponentiating with any λ > 0 and then choosing the optim\nP\nal λ.\nProb\n(X\nn\nXi > t\ni=1\n)\n=\nProb\n(X\nn\nXi > t\ni=1\n)\n(32)\n=\nProb\nn\neλ Pn\nX\nλt\ni=1\ni > e\nE[eλ\no\n≤\nPn\ni=1 Xi]\netλ\nn\n=\ne-tλ Y\nE[eλXi],\n(33)\ni=1\nwhere the penultimate step follows from Markov's inequality and the last equality follows from inde-\npendence of the Xi's.\nWe now use the fact that |Xi| ≤a to bound E[eλXi]. Because the function f(x) = eλx is convex,\nλx\na + x\ne\n≤\n2a eλa + a -xe-λa,\n2a\nfor all x ∈[-a, a].\nSince, for all i, E[Xi] = 0 we get\nE[eλXi] ≤E\na + Xi\n2a\neλa + a -Xi\n2a\ne-λa\n\n≤1\n\neλa + e-λa\n= cosh(λa)\nNote that15\ncosh(x) ≤ex2/2,\nfor all x ∈R\nHence,\nE[eλXi] ≤E[e(λXi)2/2] ≤e(λa)2/2.\nTogether with (32), this gives\nProb\n(X\nn\nn\nXi > t\n)\n≤\ne-tλ\n=1\nY\ne(λa) /2\ni\ni=1\n=\ne-tλen(λa)2/2\n15This follows immediately from the Taylor expansions: cosh(x) = Pinf\nn\nx\nn=0 (2n)!, ex2/2 = Pinf\nn=0\nx2n ,\n2n\nand (2n)!\nn!\n≥2nn!.\n\nThis inequality holds for any choice of λ ≥0, so we choose the value of λ that minimizes\nmin\nλ\n(λa)2\nn\ntλ\n-\n\nDifferentiating readily shows that the minimizer is given by\nt\nλ =\n,\nna2\nwhich satisfies λ > 0. For this choice of λ,\nn(λa)2\n/2 -tλ = n\nt2\n2a2 -t2\na2\n\n= -t2\n2na2\nThus,\nProb\n(X\nn\nXi > t\ni=1\n)\n≤\ne-\nt\n2na\nBy using the same argument on Pn\ni=1 (-Xi), and union bounding over the two events we get,\nProb\n(X\nn\n\nXi\ni\n> t\n)\n≤\n2e-\nt\n=1\n2na\nRemark 4.4 Let's say that we have random variables r1, . . . , rn i.i.d. distributed as\n-\nri =\n\nwith probability p/2\n\nwith probability 1 -p\nwith probability p/2.\nThen, E(ri) = 0 and |ri| ≤1 so Hoeffding's inequality gives:\nProb\n(X\nn\n\nri\ni=1\n> t\n)\n2 exp\n\nt\n≤\n-\n.\n2n\n\nIntuitively, the smallest p is the more concentrated |Pn\ni=1 ri| should be, however Hoeffding's in-\nequality does not capture this behavior.\nn\nA natural way to quantify this intuition is by noting that the variance of\ni=1 ri depends on p as\nVar(ri) = p. The inequality that follows, Bernstein's inequality, uses the variance of the summands to\nimprove over Hoeffding's inequality.\nP\nThe way this is going to be achieved is by strengthening the proof above, more specifically in\nstep (33) we will use the bound on the variance to get a better estimate on E[eλXi] essentially by\nrealizing that if Xi is centered, EX2\ni = σ2\n, and |Xi| ≤a then, for k ≥2, EXk\ni ≤σ2ak-2 =\n\nσ\na\na2\n\nk.\n\nTheorem 4.5 (Bernstein's Inequality) Let X1, X2, . . . , Xn be independent centered bounded ran-\ndom variables, i.e., |Xi| ≤a and E[X\ni] = 0, with variance E[Xi ] = σ . Then,\nn\nt2\nProb\n(\nX\n\nXi > t\n2 exp\ni=1\n)\n\n≤\n\n-2nσ2 + 2\n3at\n!\n.\nRemark 4.6 Before proving Bernstein's Inequality, note that on the example of Remark 4.4 we get\nProb\n(\nn\nX\ni=1\nri\n> t\n)\n≤2 exp\n\n-\nt2\n2np + 2\n,\nt\n!\nwhich exhibits a dependence on p and, for small values of p is considerably smaller than what Hoeffd-\ning's inequality gives.\nProof.\nAs before, we will prove\n(X\nn\nProb\nXi\nt\ni=1\n)\n≤exp\n\nt\n>\n-2nσ2 + 2\n,\nat\n!\n-\nn\nand then union bound with the same result for\ni=1 Xi, to prove the Theorem.\nFor any λ > 0 we have\nP\nProb\n(X\nn\nXi > t\ni=1\n)\n=\nProb{eλ P Xi > eλt}\nE[eλ\n≤\nP Xi]\neλt\nn\n=\ne-λt Y\nE[eλXi]\ni=1\nNow comes the source of the improvement over Hoeffding's,\nE[eλXi]\n=\nE\n\"\ninf\n1 + λXi +\nX λmXm\ni\nm=2\nm!\n#\n≤\n1 +\ninf\nX\nm=2\nλmam-2σ2\nm!\nσ2\n=\n1 +\n(\nm\nX\ninf\nλa)m\na2\n=2\nm!\n=\n1 + σ2\ne\na2\n\nλa -1 -λa\nTherefore,\n\nProb\n(X\nn\nXi > t\ni=1\n)\n≤e-λt\n\nσ2\n1 + a2\n\neλa -1 -λa\nn\n\nWe will use a few simple inequalities (that can be easily proved with calculus) such as16 1 + x ≤\nex, for all x ∈R.\nThis means that,\nσ2\nσ (eλa\nλa)\n1 +\neλa\na\n\n-\n-λa\n\n≤e\na\n--\n,\nwhich readily implies\n( n\n)\n)\nProb\nXi > t\ni=1\n≤e-\nnσ\nλt\n2 (eλa\ne a\n-1-λa .\nAs before, we try to find the value\nX\nof λ > 0 that minimizes\nnσ2\nmin\nλt +\n(eλa\nλa)\nλ\n\n-\na\n-\n-\n\nDifferentiation gives\nnσ2\n-t +\n(aeλa -a) = 0\na2\nwhich implies that the optimal choice of λ is given by\nat\nλ∗=\nlog\n1 +\na\n\nnσ2\n\nIf we set\nat\nu =\n,\n(34)\nnσ2\nthen λ∗= 1 log(1 + u).\na\nNow, the value of the minimum is given by\nnσ2\nnσ2\n-∗\n∗\nλ t +\n(eλ a -1 -λ∗a) = -\n[(1 + u) log(1 + u)\na2\na2\n-u] .\nWhich means that,\nProb\n( n\nnσ\nXi > t\ni=1\n)\n≤\nexp\n\n-a2 {(1 + u) log(1 + u) -u}\n\nThe rest of the proof follo\nX\nws by noting that, for every u > 0,\nu\n(1 + u) log(1 + u) -u ≥\n,\n(35)\n2 + 2\nu\nwhich implies:\n(X\nn\n)\n\nnσ2\nu\nProb\nXi > t\ni=1\n≤\nexp\n-a2\n2 + 2\nu\n!\n=\nexp\n\nt2\n-\n.\n2nσ2 + 2at\n!\n16In fact y = 1 + x is a tangent line to the graph of f(x) = ex.\n\n4.2\nGaussian Concentration\nOne of the most important results in concentration of measure is Gaussian concentration, although\nbeing a concentration result specific for normally distributed random variables, it will be very useful\nthroughout these lectures. Intuitively it says that if F : Rn →R is a function that is stable in terms\nof its input then F(g) is very well concentrated around its mean, where g ∈N(0, I). More precisely:\nTheorem 4.7 (Gaussian Concentration) Let X = [X1, . . . , Xn]T be a vector with i.i.d. standard\nGaussian entries and F : Rn →R a σ-Lipschitz function (i.e.: |F(x) -F(y)| ≤σ∥x -y∥, for all\nx, y ∈Rn). Then, for every t ≥0\n≤\n\nt2\nProb {|F(X) -EF(X)| ≥t}\n2 exp\n-2σ2\n\n.\nFor the sake of simplicity we will show the proof for a slightly weaker bound (in terms of the constant\ninside the exponent): Prob {|F(X) -EF(X)| ≥t} ≤2 exp\n\n-2\nπ2 t2\nσ2 . This exposition follows closely\nthe proof of Theorem 2.1.12 in [Tao12] and the original argument\n\nis due to Maurey and Pisier. For\na proof with the optimal constants see, for example, Theorem 3.25 in these notes [vH14]. We will\nalso assume the function F is smooth -- this is actually not a restriction, as a limiting argument can\ngeneralize the result from smooth functions to general Lipschitz functions.\nProof.\nIf F is smooth, then it is easy to see that the Lipschitz property implies that, for every x ∈Rn,\n∥∇F(x)∥2 ≤σ. By subtracting a constant to F, we can assume that EF(X) = 0. Also, it is enough\nto show a one-sided bound\nProb {F(X) -EF(X) ≥t} ≤exp\n\n-π2\nt2\n,\nσ2\n\nsince obtaining the same bound for -F(X) and taking a union bound would gives the result.\nWe start by using the same idea as in the proof of the large deviation inequalities above; for any\nλ > 0, Markov's inequality implies that\nProb {F(X) ≥t}\n=\nProb {exp (λF(X)) ≥exp (λt)}\nE [exp (λF(X))]\n≤\nexp (λt)\nThis means we need to upper bound E [exp (λF(X))] using a bound on ∥∇F∥. The idea is to\nintroduce a random independent copy Y of X. Since exp (λ·) is convex, Jensen's inequality implies\nthat\nE [exp (-λF(Y ))] ≥exp (-EλF(Y )) = exp(0) = 1.\nHence, since X and Y are independent,\nE [exp (λ [F(X) -F(Y )])] = E [exp (λF(X))] E [exp (-λF(Y ))] ≥E [exp (λF(X))]\nNow we use the Fundamental Theorem of Calculus in a circular arc from X to Y :\nπ\nF(X) -F(Y ) =\nZ\n∂\n∂θF (Y cos θ + X sin θ) dθ.\n\nThe advantage of using the circular arc is that, for any θ, Xθ := Y cos θ + X sin θ is another random\nvariable with the same distribution. Also, its derivative with respect to θ, Xθ\n′ = -Y sin θ + X cos θ\nalso is. Moreover, Xθ and Xθ\n′ are independent. In fact, note that\nE\nh\nXθXθ\n′ T i\n= E\nT\n[Y cos θ + X sin θ] [-Y sin θ + X cos θ]\n= 0.\nWe use Jensen's again (with respect to the integral now) to get:\nπ\nexp (λ [F(X) -F(Y )])\n=\nexp\n\nλ 2\nπ/2\nZ π/2\n∂\n∂θF (Xθ) dθ\n!\n≤\nπ/2\nZ π/2\nexp\n\nλπ\n∂F (Xθ)\n∂θ\n\ndθ\nUsing the chain rule,\nexp (λ [F(X) -F(Y )]) ≤π\nZ π/2\nexp\n\nλπ\nF\n2 ∇\n(Xθ) · Xθ\n′\ndθ,\nand taking expectations\nE exp (λ [F(X) -F(Y )]) ≤π\nZ π/2\nE exp\n\nλπ\nF\n2 ∇\n(Xθ) · Xθ\n′\ndθ,\nIf we condition on Xθ, since\nλ π\n2 ∇F (Xθ)\n≤λ π\n2 σ, λ π\n2 ∇F (Xθ) · X′\nθ is a gaussian random variable\nwith variance at most\nλ π\n2 σ\n2. This directly implies that, for every value of Xθ\nEX′\nθ exp\n\nλπ\n2 ∇F (Xθ) · X′\nθ\n\n≤exp\n\nλπ\nσ\n\nTaking expectation now in Xθ, and putting everything together, gives\nE [exp (λF(X))] ≤exp\n\nλπ\nσ\n\n,\nwhich means that\nProb {F(X) ≥t} ≤exp\n\nλπ\nσ\n\n-λt\n\n,\nOptimizing for λ gives λ∗=\nπ\nt ,\nσ2 which gives\nProb {F(X) ≥t} ≤exp\n\n-π2\nt2\nσ2\n\n.\n\n4.2.1\nSpectral norm of a Wigner Matrix\nWe give an illustrative example of the utility of Gaussian concentration. Let W ∈Rn×n be a standard\nGaussian Wigner matrix, a symmetric matrix with (otherwise) independent gaussian entries, the off-\nn(n+1)\ndiagonal entries have unit variance and the diagonal entries have variance 2. ∥W∥depends on\nindependent (standard) gaussian random variables and it is easy to see that it is a\n√\n2-Lipschitz\nfunction of these variables, since\n∥W (1)∥-∥W (2)∥≤\nW (1) -W (2)\n≤\nW (1) -W (2)\n.\nF\nThe symmetry\n√\nof the matrix and the variance\n\n2 of the diagon\n\nal entries are\n\nresponsible for an extra\nfactor of\n2.\nUsing Gaussian Concentration (Theorem 4.7) we immediately get\nProb {∥W∥≥E∥W∥+ t} ≤exp\n\nt2\n-4\n\n.\nSince17 E∥W∥≤2√n we get\nProposition 4.8 Let W ∈Rn×n be a standard Gaussian Wigner matrix, a symmetric matrix with\n(otherwise) independent gaussian entries, the off-diagonal entries have unit variance and the diagonal\nentries have variance 2. Then,\nProb\n\n∥W∥≥2√n + t\n\n≤exp\n\n-t2\n.\n\nNote that this gives an extremely precise control of the fluctuations of ∥W∥. In fact, for t = 2√log n\nthis gives\nProb\nn\n∥W∥≥2√n + 2\np\nlog n\no\n≤exp\n\n-4 log n\n\n= 1 .\nn\n4.2.2\nTalagrand's concentration inequality\nA remarkable result by Talagrand [Tal95], Talangrad's concentration inequality, provides an analogue\nof Gaussian concentration to bounded random variables.\nTheorem 4.9 (Talangrand concentration inequality, Theorem 2.1.13 [Tao12]) Let K > 0,\nand let X1, . . . , Xn be independent bounded random variables, |Xi| ≤K for all 1\nn\n≤i ≤n.\nLet\nF : R →R be a σ-Lipschitz and convex function. Then, for any t ≥0,\nt2\nProb {|F(X) -E [F(X)]| ≥tK} ≤c1 exp\n\n-c2\n,\nσ2\n\nfor positive constants c1, and c2.\nOther useful similar inequalities (with explicit constants) are available in [Mas00].\n17It is an excellent exercise to prove E∥W∥≤2√n using Slepian's inequality.\n\n4.3\nOther useful large deviation inequalities\nThis Section contains, without proof, some scalar large deviation inequalities that I have found useful.\n4.3.1\nAdditive ChernoffBound\nThe additive Chernoffbound, also known as Chernoff-Hoeffding theorem concerns Bernoulli random\nvariables.\nTheorem 4.10 Given 0 < p < 1 and X1, . . . , Xn i.i.d. random variables distributed as Bernoulli(p)\nrandom variable (meaning that it is 1 with probability p and 0 with probability 1 -p), then, for any\nε > 0:\n-\nProb\n(\nn\nn\nX\ni=1\nXi ≥p + ε\n)\n≤\n\"\np\np + ε\np+ε\n1 -p\n1 -p -ε\n1-p-ε#n\n-\nProb\n(\nn\nn\nX\ni=1\nXi ≤p -ε\n)\n≤\n\"\np\np -ε\np-ε\n1 -p\nn\np\n1-+ε\n-p + ε\n#\n4.3.2\nMultiplicative ChernoffBound\nThere is also a multiplicative version (see, for example Lemma 2.3.3. in [Dur06]), which is particularly\nuseful.\nTheorem 4.11 Let X1, . . . , Xn be independent random variables taking values is {0, 1} (meaning they\nare Bernoulli distributed but not necessarily identically distributed). Let μ = E\nδ\nPn\ni=1 Xi, then, for any\n> 0:\nδ\n-\nProb {X > (1 + )μ} <\n\ne\nδ\n(1 + δ)(1+δ)\nμ\n-\nProb {X < (1 -δ)μ} <\n\ne-δ\nμ\n(1 -δ)(1-δ)\n\n4.3.3\nDeviation bounds on χ2 variables\nA particularly useful deviation inequality is Lemma 1 in Laurent and Massart [LM00]:\nTheorem 4.12 (Lemma 1 in Laurent and Massart [LM00]) Let X1, . . . , Xn be i.i.d. standard\ngaussian random variables (N(0, 1)), and a1, . . . , an non-negative numbers. Let\nn\nZ =\nX\na\nk\nk=1\nXk -1\n\n.\nThe following inequalities hold for any t > 0:\n- Prob Z\n√\n{\n≥2∥a∥2\nx + 2∥a∥infx} ≤exp(-x),\n\n- Prob {Z ≤-2∥a∥2\n√x} ≤exp(-x),\n∥∥2\nPn\nwhere\na 2 =\nk=1 a2\nk and ∥a∥\n= max\ninf\n1≤k\na .\n≤n | k|\nNote that if ak = 1, for all k, then Z is a χ2 with n degrees of freedom, so this theorem immediately\ngives a deviation inequality for χ2 random variables.\n4.4\nMatrix Concentration\nIn many important applications, some of which we will see in the proceeding lectures, one needs to\nuse a matrix version of the inequalities above.\nGiven {Xk}n\nk=1 independent random symmetric d × d matrices one is interested in deviation in-\nequalities for\nλmax\nn\nXk\nk=1\n!\n.\nFor example, a very useful adaptation of Bernstein's\nX\ninequality exists for this setting.\nTheorem 4.13 (Theorem 1.4 in [Tro12]) Let {Xk}n\nk=1 be a sequence of independent random sym-\nmetric d × d matrices. Assume that each Xk satisfies:\nEXk = 0 and λmax (Xk) ≤R almost surely.\nThen, for all t ≥0,\nProb\n(\nλmax\nX\nn\nXk\nk=1\n!\n≥t\n)\n≤d · exp\n\n-t2\n2σ2 + 2\nX\nn\nwher\nRt\n!\ne σ2 =\n\nE\nk=1\nX2\nk\n\n.\nNote that ∥A∥denotes the spectral norm of A.\n\nIn what follows we will state and prove various matrix concentration results,\n\nsomewhat\n\nsimilar to\nTheorem 4.13. Motivated by the derivation of Proposition 4.8, that allowed us to easily transform\nbounds on the expected spectral norm of a random matrix into tail bounds, we will mostly focus on\nbounding the expected spectral norm. Tropp's monograph [Tro15b] is a nice introduction to matrix\nconcentration and includes a proof of Theorem 4.13 as well as many other useful inequalities.\nA particularly important inequality of this type is for gaussian series, it is intimately related to\nthe non-commutative Khintchine inequality [Pis03], and for that reason we will often refer to it as\nNon-commutative Khintchine (see, for example, (4.9) in [Tro12]).\nTheorem 4.14 (Non-commutative Khintchine (NCK)) Let A1, . . . , An ∈Rd×d be symmetric\nmatrices and g1, . . . , gn ∼N(0, 1) i.i.d., then:\nE\nX\nn\n\ngkAk\nk=1\n≤\n\n2 + 2 log(2d)\n\n2 σ,\nwhere\nσ2 =\nX\nn\n\nA2\nk\nk=1\n\n.\n(36)\n\nPNote that, akin to Proposition 4.8, we can also use Gaussian Concentration to get a tail bound on\n∥\nn\nk=1 gkAk∥. We consider the function\nF : Rn →\nX\nn\n\ngkAk\nk=1\n\nits\n.\nWe now estimate\nLipschitz constant; let g, h ∈Rn then\n\nX\nn\nX\nn\ngkAk\nhk\nk=1\n\n-\n\nAk\nk=1\n\n!\n\n!\n\n≤\n\nX\nn\nn\n\ngkAk\n-\nk=1\nX\nhkAk\n\nk=1\nX\nn\n\n=\n\n(gk -hk)Ak\nk=1\n\nv\nX\nn\n=\n\nmax\nT\n\n(gk\nh\nv: ∥v∥=1\n-\nk)Ak\nk=1\n!\nv\nn\n=\nmax\nX\n(gk -hk)\nvT Akv\nv: ∥v∥=1 k=1\n\n≤\nmax\nv: ∥v∥=1\nv\nu\nu\nt\nn\nX\nk=1\n(gk -hk)2\nv\nu\nu\nt\nn\nX\nk=1\n(vT Akv)2\n=\nv\nu\nu\nt\nn\nmax\n(vT Akv)\nv: ∥v\n∥g -h∥2,\n∥=1\nX\nk=1\nwhere the first inequality made use of the triangular inequality and the last one of the Cauchy-Schwarz\ninequality.\nThis motivates us to define a new parameter, the weak variance σ .\n∗\nDefinition 4.15 (Weak Variance (see, for example, [Tro15b])) Given A , . . . , A ∈Rd\nd\nn\n× sym-\nmetric matrices. We define the weak variance parameter as\nn\nσ2 =\nmax\nX 2\nvT Akv\n.\n∗\nv: ∥v∥=1 k=1\n\nThis means that, using Gaussian concentration (and setting t = uσ ), we have\n∗\n(X\nn\n≥\n\nProb\n\ngkAk\nk=1\n\n2 + 2 log(2d)\n\n2 σ + uσ∗\n)\n≤exp\n\n-1u2\n\n.\n(37)\nThis means that although the expected value of ∥Pn\nk=1 gkAk∥is controlled by the parameter σ, its\nfluctuations seem to be controlled by σ . We compare the two quantities in the following Proposition.\n∗\n\nProposition 4.16 Given A1, . . . , An ∈Rd×d symmetric matrices, recall that\nσ =\nv\nu\nu\nt\n\nn\nX\nk=1\nA2\nk\n\nand σ∗=\nv\nu\nu\nt\nT\n=1\nX\nn\nmax\n(v Akv) .\nv: ∥v∥\nk=1\nWe have\nσ ≤σ.\n∗\nProof.\nUsing the Cauchy-Schwarz inequality,\nσ2\n=\nmax\n∗\nX\nn 2\nvT Akv\nv: ∥v∥=1 k=1\nn\n\n=\nmax\nvT [Akv]\nv: ∥v∥=1\nX\nk=1\nn\n\n≤\nmax\n( v\nAkv )\nv:\n∥\n∥v\n∥∥∥\n∥=1\nX\nk=1\nX\nn\n=\nmax\nv: ∥v∥=1\n∥Akv∥2\nk=1\nn\n=\nmax\nvT A2\nkv\nv: ∥v∥=1\nX\nk=1\n=\nX\nn\nA2\nk\nk=1\n\n=\n\nσ2.\n\n4.5\nOptimality of matrix concentration result for gaussian series\nThe following simple calculation is suggestive that the parameter σ in Theorem 4.14 is indeed the\ncorrect parameter to understand E ∥\nX\nn\nPn\nk=1 gkAk∥.\nE\n\nn\nn\n\ngkAk\n\n=\nE\n\nX\n\ng\nk\n!\nT\nkA\nk=1\nk=1\n\n= E max v\ngkAk\nv\nv: ∥v∥=1\nX\nk=1\n!\n≥\nmax EvT\nv: ∥v∥=1\nX\nn\ngkAk\nk=1\n!\nv =\nmax vT\nv: ∥v∥=1\nX\nn\nA2\nk\nk=1\n!\nv = σ2\n(38)\nBut a natural question is whether the logarithmic term is needed. Motivated by this question we'll\nexplore a couple of examples.\n\nExample 4.17 We can write a d × d Wigner matrix W as a gaussian series, by taking Aij for i ≤j\ndefined as\nAij = eieT\nj + ejeT\ni ,\nif i = j, and\nAii =\n√\n2eieT\ni .\nIt is not difficult to see that, in this case, P\ni\nj A2\nij = (d + 1)I\n≤\nd×d, meaning that σ =\n√\nd + 1. This\nmeans that Theorem 4.14 gives us\nE∥W∥≲\np\nd log d,\nhowever, we know that E∥W\n√\n∥≍\nd, meaning that the bound given by NCK (Theorem 4.14) is, in this\ncase, suboptimal by a logarithmic factor.18\nThe next example will show that the logarithmic factor is in fact needed in some examples\nT ∈Rd×d\nn\nExample 4.18 Consider Ak = ekek\nfor k = 1, . . . , d. The matrix P\nk=1 gkAk corresponds to\na diagonal matrix with independent standard gaussian random variables as diagonal entries, and so\nit's spectral norm is given by maxk |gk|. It is known that max1≤k≤d |gk\n√\n| ≍\nlog d. On the other hand,\na direct calculation shows that σ = 1. This shows that the logarithmic factor cannot, in general, be\nremoved.\nThis motivates the question of trying to understand when is it that the extra dimensional factor\nn\nis needed. For both these examples, the resulting matrix X = P\nk=1 gkAk has independent entries\n(except for the fact that it is symmetric). The case of independent entries [RS13, Seg00, Lat05, BvH15]\nis now somewhat understood:\nTheorem 4.19 ([BvH15]) If X is a d × d random symmetric matrix with gaussian independent\nentries (except for the symmetry constraint) whose entry i, j has variance b2\nij then\nE∥X∥≲\nv\nu\nu\nt max\n1≤i≤d\nd\nX\nj=1\nb2\nij + max\nij\n|bij|\np\nlog d.\nRemark 4.20 X in the theorem above can be written in terms of a Gaussian series by taking\nAij = bij eieT\nj + e\nT\njei\n,\nfor i < j and Aii = biieieT\ni . One can then compute\nσ and σ\n\n:\n∗\nd\nσ2 = max\nX\nb2 and σ2\n∗≍b2\nij\nij.\n1≤i≤d j=1\nThis means that, when the random matrix in NCK (Theorem 4.14) has negative entries (modulo\nsymmetry) then\nE∥X∥≲σ +\np\nlog dσ∗.\n(39)\n18By a ≍b we mean a ≲b and a ≳b.\n\nTheorem 4.19 together with a recent improvement of Theorem 4.14 by Tropp [Tro15c]19 motivate\nthe bold possibility of (39) holding in more generality.\nConjecture 4.21 Let A , . . . , A ∈Rd\nd\nn\n× be symmetric matrices and g1, . . . , gn ∼N(0, 1) i.i.d., then:\nX\nn\nE\n\ngkAk\nk=1\n≲σ + (log d) 2 σ ,\n∗\nWhile it may very will be that this Conjecture 4.21 is false, no counter example is known, up to\ndate.\nOpen Problem 4.1 (Improvement on Non-Commutative Khintchine Inequality) Prove or\ndisprove Conjecture 4.21.\nI would also be pretty excited to see interesting examples that satisfy the bound in Conjecture 4.21\nwhile such a bound would not trivially follow from Theorems 4.14 or 4.19.\n4.5.1\nAn interesting observation regarding random matrices with independent matrices\nFor the inndepoendent entries setting, Theorem 4.19 is tight (up to constants) for a wide range of variance\nprofiles\nb2\nij\n- the details are available as Corollary 3.15 in [BvH15]; the basic idea is that if the\ni≤j\nlargest variance is comparable to the variance of a sufficient number of entries, then the bound in\nTheorem 4.19 is tight up to constants.\nHowever, the situation is not as well understood when the variance profiles\nb2\nij\nare arbitrary.\ni≤j\nSince the spectral norm of a matrix is always at least the l2 norm of a row, the\nn\nfollo\no\nwing lower bound\nholds (for X a symmetric random matrix with independent gaussian entries):\nE∥X∥≥E max\nk\n∥Xek∥2.\nObservations in papers of Lata la [Lat05] and Riemer and Schutt [RS13], together with the results\nin [BvH15], motivate the conjecture that this lower bound is always tight (up to constants).\nOpen Problem 4.2 (Lata la-Riemer-Schutt) Given X a symmetric random matrix with indepen-\ndent gaussian entries, is the following true?\nE∥X∥≲E max ∥Xek∥2.\nk\nThe results in [BvH15] answer this in the positive for a large range of variance profiles, but not in\nfull generality. Recently, van Handel [vH15] proved this conjecture in the positive with an extra factor\nof √log log d. More precisely, that\nE∥X∥≲\np\nlog log dE max\nk\n∥Xek∥2,\nwhere d is the number of rows (and columns) of X.\n19We briefly discuss this improvement in Remark 4.32\n\n4.6\nA matrix concentration inequality for Rademacher Series\nIn what follows, we closely follow [Tro15a] and present an elementary proof of a few useful matrix\nconcentration inequalities.\nWe start with a Master Theorem of sorts for Rademacher series (the\nRademacher analogue of Theorem 4.14)\nTheorem 4.22 Let H1, . . . , Hn ∈Rd×d be symmetric matrices and ε1, . . . , εn i.i.d.\nRademacher\nrandom variables (meaning = +1 with probability 1/2 and = -1 with probability 1/2), then:\nE\nX\nn\n\nεkHk\n≤\nk=1\n\n1 + 2⌈log(d)⌉\n\n2 σ,\nwhere\nσ2 =\nX\nn\nH2\nk\n(40)\nk=1\n.\nBefore proving this theorem, we take first a small\n\ndetour\n\nin discrepancy theory followed by deriva-\ntions, using this theorem, of a couple of useful matrix concentration inequalities.\n4.6.1\nA small detour on discrepancy theory\nThe following conjecture appears in a nice blog post of Raghu Meka [Mek14].\nConjecture 4.23 [Matrix Six-Deviations Suffice] There exists a universal constant C such that, for\nany choice of n symmetric matrices H\nn\nn\n1, . . . , Hn ∈R ×\nsatisfying ∥Hk∥≤1 (for all k = 1, . . . , n),\nthere exists ε1, . . . , εn ∈{±1} such that\nX\nn\n\n√\n\nεkHk\nk=1\n\n≤C\nn.\nOpen Problem 4.3 Prove or disprove Conjecture 4.23.\nNote that, when the matrices Hk are diagonal, this problem corresponds to Spencer's Six Standard\nDeviations Suffice Theorem [Spe85].\nRemark 4.24 Also, using Theorem 4.22, it is easy to show that if one picks εi as i.i.d. Rademacher\nrandom variables, then with positive probability (via the probabilistic method) the inequality will be\nsatisfied with an extra √log n term. In fact one has\nE\n\nn\nX\nk=1\nεkHk\n≲\np\nlog n\nv\nu\nu\nt\nX\nn\n\nH2\nk\nk=1\n≤\np\nlog n\nv\nu\nu\nt\nn\nX\nk=1\n∥Hk∥2 ≤\np\nlog n√n.\nRemark 4.25 Remark 4.24 motivates asking whether Conjecture 4.23 can be strengthened to ask for\nε1, . . . , εn such that\n\nX\nn\n\nεkHk\nk=1\n\nX\nn\n≲\nH2\n\nk\nk=1\n\n.\n(41)\n\n4.6.2\nBack to matrix concentration\nUsing Theorem 4.22, we'll prove the following Theorem.\nTheorem 4.26 Let T1, . . . , Tn ∈Rd×d be random independent positive semidefinite matrices, then\nE\n\nX\nn\nTi\ni=1\n≤\nX\nn\n\nETi\ni=1\n\n+\np\nC(d)\n\nE max\ni\n∥Ti∥\n\n,\nwhere\nC(d) := 4 + 8⌈log d⌉.\n(42)\nA key step in the proof of Theorem 4.26 is an idea that is extremely useful in Probability, the trick\nof symmetrization. For this reason we isolate it in a lemma.\nLemma 4.27 (Symmetrization) Let T1, . . . , Tn be independent random matrices (note that they\ndon't necessarily need to be positive semidefinite, for the sake of this lemma) and ε1, . . . , εn random\ni.i.d. Rademacher random variables (independent also from the matrices). Then\nE\nX\nn\n\nTi\ni=1\n\nX\nn\n≤\n\nETi\nE\nεiT\ni=1\n\n+ 2\nX\nn\ni\ni=1\n\nProof.\nTriangular inequality gives\n\nE\nX\nn\nn\nn\n\nTi\nE\n(\n=1\n≤\nX\n\nTi\n+ E\nTi\nETi) .\ni\ni=1\n\nX\ni=1\n-\n\nLet us now introduce, for each i, a random matrix\n\nTi\n′ iden\n\ntically distributed\n\nto Ti and independent\n(all 2n matrices are independent). Then\nE\n\nX\nn\n\n(Ti -ETi)\n\n=\nET\nX\nn\n\nTi -ETi\nE\ni=1\ni=1\n-\nTi\n′\nh\nTi\n′ -ETi\n′Ti\n′i\nn\nn\n\n=\nET\nET ′\nX Ti -Ti\n′\ni\n\nTi\n\n=1\n≤E\nX\n-Ti\n′\ni=1\n\nmean that\n\nthe expectation\n\n,\nwhere we use the notation Ea to\nis taken\n\nwith respect\n\nto the variable a\nand the last step follows from Jensen's inequality with respect to ET ′.\nSince Ti -Ti\n′ is a symmetric random variable, it is identically distributed to εi (Ti -Ti\n′) which\ngives\nE\nX\nn\n\nX\nn\nn\nn\nn\n\nTi -Ti\n′\n= E\n\nεi\nTi -Ti\n′\n\n≤E\nX\nX\n\nεiTi\n+ E\n\nεiTi\n′\ni=1\ni=1\ni=1\ni\n\n= 2E\n=1\nX\nεiTi\ni=1\n\n,\nconcluding the proof.\n\nProof. [of Theorem 4.26]\nUsing Lemma 4.27 and Theorem 4.22 we get\nE\nX\nn\nn\n\nTi\nE\ni=1\n≤\nX\n\nTi\ni=1\n+\np\nC(d)E\n\nn\nX\ni=1\nT 2\ni\n\nThe trick now is to make a term like the one in the LHS appear in the RHS. For that we start by\nnoting (you can see Fact 2.3 in [Tro15a] for an elementary proof) that, since Ti ⪰0,\nX\nn\nT 2\ni\ni=1\n\n≤max\ni\n∥Ti∥\n\nX\nn\n\nTi\ni=1\n.\nThis means that\nE\nX\nn\nTi\ni=1\n\n≤\n\nX\nn\n\nETi\ni=1\n+\np\nC(d)E\n\nmax\ni\n∥Ti∥\n\nn\nX\ni=1\nTi\n\n.\nFurther applying the Cauchy-Schwarz inequality for E gives,\nE\n\nX\nn\n\nTi\n≤\nX\nn\n\nETi\n+\ni=1\ni=1\n\np\n\nC(d)\n\nE max\ni\n∥Ti∥\n\nE\n\nn\nX\ni=1\nTi\n\n! 1\n,\nNow that the term E ∥Pn\ni=1 Ti∥appears in the RHS, the proof can be finished with a simple application\nof the quadratic formula (see Section 6.1. in [Tro15a] for details).\nWe now show an inequality for general symmetric matrices\nTheorem 4.28 Let Y1, . . . , Yn ∈Rd×d be random independent positive semidefinite matrices, then\nE\nX\nn\np\n\nYi\ni=1\n\n≤\nC(d)σ + C(d)L,\nwhere,\nσ2 =\nX\nn\n\nEY 2\ni\ni=1\n\nand L = E max\ni\n∥Yi∥2\n(43)\nand, as in (42),\nC(d) := 4 + 8⌈log d⌉.\nProof.\nUsing Symmetrization (Lemma 4.27) and Theorem 4.22, we get\nE\nX\nn\nYi\n≤2E\ni=1\n\nY\n\"\n\nEε\nX\nn\n\nεiYi\ni=1\n#\n\n≤\np\nC(d)E\n\nn\nX\ni=1\nY 2\ni\n\n.\n\nJensen's inequality gives\nE\nX\nn\n\nY 2\ni\ni=1\n\n≤\n\nE\n\nn\nX\ni=1\nY 2\ni\n\n! 1\n,\nand the proof can be concluded by noting that Y 2\ni ⪰0 and using Theorem 4.26.\nRemark 4.29 (The rectangular case) One can extend Theorem 4.28 to general rectangular ma-\ntrices S1, . . . , S\nd1\nd2\nn ∈R\n×\nby setting\nYi =\n\nSi\nST\n,\ni\n\nand noting that\n\nSi\n\nS ST\nY\n\ni\n=\n=\ni\ni\n\nT\n\nT\n\n= max\nST\nSiST\ni Si\n,\n\ni\n\n.\nSi\nSi Si\nWe defer the details to [Tro15a]\nIn order to prove Theorem 4.22, we will use an AM-GM like inequality for matrices for which,\nunlike the one on Open Problem 0.2. in [Ban15d], an elementary proof is known.\nLemma 4.30 Given symmetric matrices H, W, Y ∈Rd×d and non-negative integers r, q satisfying\nq ≤2r,\nTr\n\nHW qHY 2r-q\n+ Tr\n\nHW 2r-qHY q\n≤Tr\n\nH2 W 2r + Y 2r\n,\nand summing over q gives\nX\n2r\nr\nTr HW qHY 2 -q\n2r +\nq=0\n\n≤\n\n+\n\nTr\n\nH2 W 2r\nY 2r\nWe refer to Fact 2.4 in [Tro15a] for an elementary proof but note that it is a matrix analogue to\nthe inequality,\nμθλ1-θ + μ1-θλθ ≤λ + θ\nfor μ, λ ≥0 and 0 ≤θ ≤1, which can be easily shown by adding two AM-GM inequalities\nμθλ1-θ ≤θμ + (1 -θ)λ and μ1-θλθ ≤(1 -θ)μ + θλ.\nProof. [of Theorem 4.22]\nLet X = Pn\nk=1 εkHk, then for any positive integer p,\nE∥X∥≤\nE∥X∥2p\n2p =\nE∥X2p∥\n2p ≤\nE Tr X2p 1\n2p ,\nwhere the first inequality follows from Jensen's inequality and the last from X2p ⪰0 and the obser-\nvation that the trace of a positive semidefinite matrix is at least its spectral norm. In the sequel, we\n\nupper bound E Tr X2p. We introduce X+i and X-i as X conditioned on εi being, respectively +1 or\n-1. More precisely\nX+i = Hi +\nX\nεjHj and X-i = -Hi +\nj=i\nX\nεjHj.\nj=\n\ni\nThen, we have\nE Tr X2p = E Tr\n\nXX2p-1\nn\n= E\nX\nTr εiHiX2p-1.\ni=1\nNote that E\nTr\n\nε H X2p-1\n= 1\nεi\ni\ni\n2 Tr\nh\nHi\n\nX2p-1\n+i\n-X2p-1\n-i\ni\n, this means that\nE Tr X2p =\nn\nX\ni=1\nE1\nh\np\nTr Hi\n\np\nX\n-1\n+i\n-\nX\ni\n-1\n-\ni\n,\nwhere the expectation can be taken over εj for j = i.\n2p\nNow we rewrite X+i\n-1 -\n2p\nX\n-1\n-i\nas a telescopic sum:\n2X\np\np\n-2\n+i\n--\n2p\nX\nX-i\n-\nq\n2p\nq\n=\nX+i (X+i\nq=0\n-X\n) X\n--\n-i\n-i\n.\nWhich gives\nn\n2p-2\nE Tr X2p =\nX\ni=1\nX\nE\nq=0\nq\nTr\nh\np\nHiX+i (X+i -\nq\nX\ni) X\ni\n--\n-\n-\ni\n.\nSince X+i -X-i = 2Hi we get\nX\nn\n2X\np-2\nE Tr X2p\nq\n2p\n=\nE Tr\nh\nq\nHiX+iHiX\n--\n-i\ni=1 q=0\ni\n.\n(44)\nWe now make use of Lemma 4.30 to get20 to get\nn\nE Tr X2p\nX 2p -\n≤\ni=1\nE Tr\nh\nH2\ni\n\nX2p-2\n+i\n+ X2p-2\n-i\ni\n.\n(45)\n20See Remark 4.32 regarding the suboptimality of this step.\n\nHence,\nX\nn 2p -1\ni=1\n2p-2\n2p-2\nX\nE Tr\nh\nn\nX+i\n+\nH2\n2p\n2p\n-i\ni\n\nX+i\n-+ X\ni\n-i\n=\n(2p\n1)\nH\n-\n-\nX\nE Tr\ni=1\n\ni\n\nn\n\n=\n(2p -1)\nX\nE Tr\ni=1\n\nH2\ni Eεi\n\nX2p-2\nn\n\n=\n(2p -1)\nX\nE Tr\ni=1\n\nH2\ni X2p-2\n=\n(2p -1)E Tr\n\" X\nn\nH2\ni\ni=1\n!\nX2p-2\n#\nSince X2p-2 ⪰0 we\nTr\n\"\nhave\nX\nn\nH2\ni\n!\nX2p-2\n#\n≤\nX\nn\n\n2p\n\nH2\n2p\ni\n\n-\ni=1\nTr X\n= σ Tr X\ni=1\n\n-,\n(46)\nwhich gives\nE Tr X2p\n\n≤σ2(2p -1)E Tr X2p-2.\n(47)\nApplying this inequality, recursively, we get\nE Tr X2p ≤[(2p -1)(2p -3) · · · (3)(1)] σ2pE Tr X0 = (2p -1)!!σ2pd\nHence,\nE∥X∥≤\nE Tr X2p\n2p ≤[(2p -1)!!]\n2p σd\n2p .\nTaking p = ⌈log d⌉and using the fact that (2p -1)!! ≤\n\n2p+1\np\n(see [Tro15a] for an elementary proof\ne\nconsisting essentially of taking logarithms and comparing the\n\nsum with an integral) we get\nE∥X∥≤\n2⌈log d⌉+ 1\ne\nσd\n2⌈log d⌉≤(2⌈log d⌉+ 1)\n2 σ.\nRemark 4.31 A similar argument can be used to prove Theorem 4.14 (the gaussian series case) based\non gaussian integration by parts, see Section 7.2. in [Tro15c].\nRemark 4.32 Note that, up until the step from (44) to (45) all steps are equalities suggesting that\nthis step may be the lossy step responsible by the suboptimal dimensional factor in several cases (al-\nthough (46) can also potentially be lossy, it is not uncommon that\nH2\ni is a multiple of the identity\nmatrix, which would render this step also an equality).\nIn fact, Joel Tropp [Tro15c] recently proved an improvement over\nP\nthe NCK inequality that, essen-\ntially, consists in replacing inequality (45) with a tighter argument. In a nutshell, the idea is that, if\nthe Hi's are non-commutative, most summands in (44) are actually expected to be smaller than the\nones corresponding to q = 0 and q = 2p -2, which are the ones that appear in (45).\n\n4.7\nOther Open Problems\n4.7.1\nOblivious Sparse Norm-Approximating Projections\nThere is an interesting random matrix problem related to Oblivious Sparse Norm-Approximating\nProjections [NN], a form of dimension reduction useful for fast linear algebra. In a nutshell, The\nidea is to try to find random matrices Π that achieve dimension reduction, meaning Π ∈Rm×n with\nm ≪n, and that preserve the norm of every point in a certain subspace [NN], moreover, for the\nsake of computational efficiency, these matrices should be sparse (to allow for faster matrix-vector\nmultiplication).\nIn some sense, this is a generalization of the ideas of the Johnson-Lindenstrauss\nLemma and Gordon's Escape through the Mesh Theorem that we will discuss next Section.\nOpen Problem 4.4 (OSNAP [NN]) Let s ≤d ≤m ≤n.\n1. Let Π ∈Rm×n be a random matrix with i.i.d. entries\nδriσri\nΠri =\n√\n,\ns\nwhere σri is a Rademacher random variable and\nδri =\n(\n√s\nwith probability\ns\nm\nwith probability\n1 -s\nm\nProve or disprove: there exist positive universal constants c1 and c2 such that\nFor any U ∈Rn×d for which UT U = Id×d\nProb\n(ΠU)T (ΠU) -I\n≥ε\n< δ,\nd+log( 1\nfor m\n\n≥c\n\nδ)\nε2\nand s ≥c2\nlog( d\nδ).\nε2\n2. Same setting as in (1) but conditioning on\nX\nm\nδri = s,\nfor all i,\nr=1\nmeaning that each column of Π has exactly s non-zero elements, rather than on average. The\nconjecture is then slightly different:\nProve or disprove: there exist positive universal constants c1 and c2 such that\nFor any U ∈Rn×d for which UT U = Id×d\nProb\n(ΠU)T (ΠU) -I\n≥ε\n\n< δ,\nfor m ≥\nd+log(\nc\n\nδ)\nε2\nand s ≥c2\nlog( d\nδ)\nε\n.\n\n3. The conjecture in (1) but for the specific choice of U:\nU =\n\nId×d\n.\n0(n-d)×d\n\nIn this case, the object in question is a sum of rank 1 independent matrices. More precisely,\nz1, . . . , zm ∈Rd (corresponding to the first d coordinates of each of the m rows of Π) are i.i.d.\nrandom vectors with i.i.d. entries\n(zk)j\n\n-\n=\n\n√s\nwith probability\ns\n2m\nwith probability\n1 -s\nm\n√s\nwith probability\ns\n2m\nNote that EzkzT\nk = 1 I\n. The conjecture is then that, there exists c1 and c2 positive universal\nm d×d\nconstants such that\n(X\nm\nProb\nT\nk\nk=1\n\nzkzT\nk -Ezkz\n\n)\n\n≥ε\n< δ,\nfor m ≥\nd+log( 1\nc1\nδ)\nε2\nand s ≥c2\nlog( d\nδ).\nε2\nI think this would is an interesting question even for fixed δ, for say δ = 0.1, or even simply\nunderstand the value of\nE\nX\nm\n.\nk=1\n\nzkzT\nk -EzkzT\nk\n\n4.7.2\nk-lifts of graphs\n\nGiven a graph G, on n nodes and with max-degree ∆, and an integer k ≥2 a random k lift G⊗k of G\nis a graph on kn nodes obtained by replacing each edge of G by a random k\nk bipartite matching.\nMore precisely, the adjacency matrix A⊗k\nk\n×\nof G⊗\nis a nk × nk matrix with k × k blocks given by\nA⊗k\nij = AijΠij,\nwhere Πij is uniformly randomly drawn from the set of permutations on k elements, and all the edges\nare independent, except for the fact that Πij = Πji. In other words,\nA⊗k =\nX\nAij\neieT\nj ⊗Πij + e\nT\njei\ni<j\n⊗ΠT\nij\n\n,\nwhere ⊗corresponds to the Kronecker product. Note that\nEA⊗k = A ⊗\n\nJ\nk\n\n,\nwhere J = 11T is the all-ones matrix.\n\nOpen Problem 4.5 (Random k-lifts of graphs) Give a tight upperbound to\nE\nA⊗k -EA⊗k .\nOliveira [Oli10] gives a bound that is essentially of the form\np\n∆log(nk), while the results in [ABG12]\nsuggest that one may expect more concentration for large k. It is worth noting that the case of k = 2\ncan essentially be reduced to a problem where the entries of the random matrix are independent and\nthe results in [BvH15] can be applied to, in some case, remove the logarithmic factor.\n4.8\nAnother open problem\nFeige [Fei05] posed the following remarkable conjecture (see also [Sam66, Sam69, Sam68])\nConjecture 4.33 Given n independent random variables X1, . . . , Xn s.t., for all i, Xi ≥0 and EXi =\n1 we have\nProb\nX\nn\nXi\ni=1\n≥n + 1\n!\n≤1 -e-1\nNote tP\nhat, if Xi are i.i.d. andXi = n + 1 with probability 1/(n + 1) and Xi = 0 otherwise, then\nn\nProb (\ni=1 Xi ≥n + 1) = 1 -\nn\nn\nn+1\n\n≈1 -e-1.\nOpen Problem 4.6 Prove or disprove Conjecture 4.33.21\n21We thank Francisco Unda and Philippe Rigollet for suggesting this problem.\n\nJohnson-Lindenstrauss Lemma and Gordons Theorem\n5.1\nThe Johnson-Lindenstrauss Lemma\nSuppose one has n points, X = {x1, . . . , xn}, in Rd (with d large). If d > n, since the points have\nto lie in a subspace of dimension n it is clear that one can consider the projection f : Rd →Rn of\nthe points to that subspace without distorting the geometry of X. In particular, for every xi and xj,\n∥f(xi) -f(xj)∥2 = ∥xi -x\nj∥, meaning that f is an isometry in X.\nSuppose now we allow a bit of distortion, and look for f : Rd →Rk that is an ε-isometry, meaning\nthat\n(1 -ε)∥xi -x\nj∥≤∥f(xi) -f(xj)∥≤(1 + ε)∥xi -xj∥.\n(48)\nCan we do better than k = n?\nIn 1984, Johnson and Lindenstrauss [JL84] showed a remarkable Lemma (below) that answers this\nquestion positively.\nTheorem 5.1 (Johnson-Lindenstrauss Lemma [JL84]) For any 0 < ε < 1 and for any integer\nn, let k be such that\nk ≥4\nlog n.\nε2/2 -ε3/3\nThen, for any set X of n points in Rd, there is a linear map f : Rd →Rk that is an ε-isometry for\nX (see (48)). This map can be found in randomized polynomial time.\nWe borrow, from [DG02], an elementary proof for the Theorem. We need a few concentration of\nmeasure bounds, we will omit the proof of those but they are available in [DG02] and are essentially\nthe same ideas as those used to show Hoeffding's inequality.\nLemma 5.2 (see [DG02]) Let y1, . . . , yd be i.i.d standard Gaussian random variables and Y\n=\n(y , . . . , y ).\nLet g : Rd\nk\nd\n→R\nbe the projection into the first k coordinates and Z = g\n\nY\n∥Y ∥\n\n=\n∥Y ∥(y1, . . . , yk) and L = ∥Z∥2. It is clear that EL = k. In fact, L is very concentrated around its\nd\nmean\n- If β < 1,\nPr\n\nk\nL ≤β\nk\nexp\nd\n\n≤\n\n2(1 -β + log β)\n\n.\n- If β > 1,\nPr\n\nL ≥β k\nd\n\n≤exp\nk(1 -β + log β)\n\n.\nProof. [ of Johnson-Lindenstrauss Lemma ]\nWe will start by showing that, given a pair xi, xj a projection onto a random subspace of dimension\nk will satisfy (after appropriate scaling) property (48) with high probability. WLOG, we can assume\nthat u = xi -xj has unit norm. Understanding what is the norm of the projection of u on a random\nsubspace of dimension k is the same as understanding the norm of the projection of a (uniformly)\n\nrandom point on Sd-1 the unit sphere in Rd on a specific k-dimensional subspace, let's say the one\ngenerated by the first k canonical basis vectors.\nThis means that we are interested in the distribution of the norm of the first k entries of a random\nvector drawn from the uniform distribution over Sd-1 - this distribution is the same as taking a\nstandard Gaussian vector in Rd and normalizing it to the unit sphere.\nLet g : Rd →Rk be the projection on a random k\nd\n-dimensional subspace and let f : Rd →Rk\ndefined as f = kg. Then (by the above discussion), given a pair of distinct xi and xj, ∥f(xi)-f(xj)∥2\nd\n∥xi-xj∥2\nhas the same distribution as kL, as defined in Lemma 5.2. Using Lemma 5.2, we have, given a pair\nxi, xj,\nPr\n∥f(xi) -f(xj)∥2\nk\n≤(1\n∥xi -xj∥2\n-ε)\n\n≤exp\n\n(1 -(1 -ε) + log(1 -ε))\n\n,\nsince, for ε ≥0, log(1 -ε) ≤-ε -ε2/2 we have\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≤(1 -ε)\n\n≤\nexp\n\n-kε2\n\n≤\nexp (-2 log n) = 1 .\nn2\nOn the other hand,\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≥(1 + ε)\n\n≤exp\nk(1 -(1 + ε) + log(1 + ε))\n\n.\nsince, for ε ≥0, log(1 + ε) ≤ε -ε2/2 + ε3/3 we have\nProb\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≤(1 -ε)\n\n≤\nexp\n\n-k\nε2 -2ε3/3\n\n!\n≤\nexp (-2 log n) = 1 .\nn2\nBy union bound it follows that\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n/∈[1 -ε, 1 + ε]\n\n≤2 .\nn2\nSince there exist\nn\nsuch pairs, again, a simple union bound gives\nPr\n\n∃i,j : ∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n/∈[1 -ε, 1 + ε]\n\n≤2\nn2\nn(n -1)\n= 1 -1 .\nn\nTherefore, choosing f as a properly scaled projection onto a random k\n-dimensional subspace is an\nε-isometry on X (see (48)) with probability at least\n.\nWe can achieve any desirable constant\nn\nprobability of success by trying O(n) such random projections, meaning we can find an ε-isometry\nin randomized polynomial time.\nNote that by considering k slightly larger one can get a good projection on the first random attempt\nwith very good confidence. In fact, it's trivial to adapt the proof above to obtain the following Lemma:\n\nLemma 5.3 For any 0 < ε < 1, τ > 0, and for any integer n, let k be such that\nk ≥(2 + τ)\nlog n.\nε2/2 -ε3/3\nThen, for any set X of n points in Rd, take f : Rd →Rk to be a suitably scaled projection on a random\nsubspace of dimension k, then f is an ε-isometry for X (see (48)) with probability at least 1 -\n1 .\nnτ\nLemma 5.3 is quite remarkable. Think about the situation where we are given a high-dimensional\ndata set in a streaming fashion - meaning that we get each data point at a time, consecutively. To run\na dimension-reduction technique like PCA or Diffusion maps we would need to wait until we received\nthe last data point and then compute the dimension reduction map (both PCA and Diffusion Maps\nare, in some sense, data adaptive). Using Lemma 5.3 you can just choose a projection at random in\nthe beginning of the process (all ones needs to know is an estimate of the log of the size of the data\nset) and just map each point using this projection matrix which can be done online - we don't need\nto see the next point to compute the projection of the current data point. Lemma 5.3 ensures that\nthis (seemingly na ıve) procedure will, with high probably, not distort the data by more than ε.\n5.1.1\nOptimality of the Johnson-Lindenstrauss Lemma\nIt is natural to ask whether the dependency on ε and n in Lemma 5.3 can be improved.\nNoga\nAlon [Alo03] showed that there are n points for which the smallest dimension k on which they can\nbe embedded with a distortion as in Lemma 5.3, satisfies k = Ω\n\nε\nlog\n(1/ε)\n-2\nn , this was recently\nlog\nimproved by Larsen and Nelson [?], for linear maps, to Ω\n\nε-2 log n\n\n, closing the gap.22\n5.1.2\nFast Johnson-Lindenstrauss\n(Disclaimer: the purpose of this section is just to provide a bit of intuition, there is a lot of hand-\nwaving!!)\nLet's continue thinking about the high-dimensional streaming data. After we draw the random\nprojection matrix, say M, for each data point x, we still have to compute Mx which, since M has\nO(ε-2 log(n)d) entries, has a computational cost of O(ε-2 log(n)d). In some applications this might\nbe too expensive, can one do better? There is no hope of (significantly) reducing the number of rows\n(Recall Open Problem ?? and the lower bound by Alon [Alo03]). The only hope is to speed up the\nmatrix-vector multiplication. If we were able to construct a sparse matrix M then we would definitely\nspeed up the computation of Mx but sparse matrices tend to distort sparse vectors, and the data\nset may contain. Another option would be to exploit the Fast Fourier Transform and compute the\nFourier Transform of x (which takes O(d log d) time) and then multiply the Fourier Transform of x by\na sparse matrix. However, this again may not work because x might have a sparse Fourier Transform.\nThe solution comes from leveraging an uncertainty principle -- it is impossible for both x and the FT\nof x to be sparse simultaneously. The idea is that if, before one takes the Fourier Transform of x, one\nflips (randomly) the signs of x, then the probably of obtaining a sparse vector is very small so a sparse\nmatrix can be used for projection. In a nutshell the algorithm has M be a matrix of the form PHD,\n22An earlier version of these notes marked closing the gap as an open problem, this has been corrected.\n\nwhere D is a diagonal matrix that flips the signs of the vector randomly, H is a Fourier Transform\n(or Hadamard transform) and P a sparse matrix. This method was proposed and analysed in [AC09]\nand, roughly speaking, achieves a complexity of O(d log d), instead of the classical O(ε-2 log(n)d).\nThere is a very interesting line of work proposing fast Johnson Lindenstrauss projections based on\nsparse matrices. In fact, this is, in some sense, the motivation for Open Problem 4.4. in [Ban15d].\nWe recommend these notes Jelani Nelson's notes for more on the topic [Nel].\n5.2\nGordon's Theorem\nIn the last section we showed that, in order to approximately preserve the distances (up to 1 ± ε)\nbetween n points it suffices to randomly project them to Θ\nε-2 log n dimensions. The key argument\nwas that a random projection approximately preserves the norm of every point in a set S, in this case\nthe set of differences between pairs of n points. What we showed is\n\nthat, in order to approximately\npreserve the norm of every point in S it is enough to project to Θ\nε-2 log |S|\ndimensions.\nThe\nquestion this section is meant to answer is: can this improved if S has a special structure? Given a\nset S, what is the measure of complexity of S that explains how many dimensions\n\none needs to take\non the projection to still approximately preserve the norms of points in S. Was we will see below, this\nwill be captured, via Gordon's Theorem, by the so called Gaussian Width of S.\nDefinition 5.4 (Gaussian Width) Given a closed set S ⊂Rd, its gaussian width ω(S) is define\nas:\nω(S) = E max gT\nd x\nx∈S\nwhere gd\n(0, Id\nd).\n\n,\n∼N\n×\nSimilarly to what we did in the proof of Theorem 5.1 we will restrict our attention to sets S of\nunit norm vectors, meaning that S ⊂Sd-1.\nAlso, we will focus our attention not in random projections but in the similar model of random\nlinear maps G : Rd →Rk that are given by matrices with i.i.d. gaussian entries. For this reason the\nfollowing Proposition will be useful:\nProposition 5.5 Let gk ∼N (0, Ik×k), and define\nak := E∥gk∥.\nThen\nq\nk\nk+1\n√\nk ≤ak ≤\n√\nk.\nWe are now ready to present Gordon's Theorem.\nTheorem 5.6 (Gordon's Theorem [Gor88]) Let G ∈Rk×d a random matrix with independent\nN(0, 1) entries and S ⊂Sd-1 be a closed subset of the unit sphere in d dimensions. Then\nE max\nx∈S\nak\nGx\n≤1 + ω(S)\nak\n,\n\nand\nE min\nx∈S\nak\nGx\n≥1 -ω(S),\nak\nwhere ak = E∥gk∥and ω(S) is the gaussian width of S. Recall that\nq\nk\nk+1\n√\nk ≤ak ≤\n√\nk.\nBefore proving Gordon's Theorem we'll note some of it's direct implications. It suggest that\n1 G\nak\npreserves the norm of the points in S up to 1 ± ω(S), indeed we can make this precise with Gaussian\nak\nConcentration.\nNote that the function F(G) = maxx∈S\n1 Gx\nak\nis 1-Lipschitz. Indeed\nmax ∥G1x1∥-max ∥G2x2∥\n\n≤\nmax |∥G1x∥-∥G2x\nmax\nx1∈S\nx2∈S\nx∈S\n∥| ≤\nx∈S ∥(G1 -G2) x∥\n=\n∥G1 -G2∥≤∥G1 -G2∥F .\nSimilarly, one can show that F(G) = minx∈S\n1 Gx\nis 1-Lipschitz. Thus, one can use Gaussian\nak\nConcentration to get:\n\nProb\n\nt2\nmax ∥Gx∥≥ak + ω(S) + t\nx S\n\nexp\n∈\n≤\n\n-2\n\n,\n(49)\nand\nProb\n\nmin\nx∈S ∥Gx∥≤ak -ω(S) -t\n\n≤exp\n\n-t2\n.\n\n(50)\nThis gives us the following Theorem.\nTheorem 5.7 Let G ∈Rk×d a random matrix with independent N(0, 1) entries and S ⊂Sd-1 be\na closed subset of the unit sphere in d dimensions. Then, for ε >\nr\nω(S)2\na2\nk , with probability ≥1 -\n2 exp\n\n-k\n\nε -ω(S)\nak\n\n:\n(1 -ε)∥x∥≤\nak\nGx\n≤(1 + ε)∥x∥,\nfor all x ∈S.\nRecall that k -\nk\na\nk+1 ≤\nk ≤k.\nω(S)+t\nProof.\nThis is readily obtained by taking ε =\n, using (49), (50), and recalling that a2\nak\nk ≤k.\nRemark 5.8 Note that a simple use of a union bound23 shows that ω(S) ≲\np\n2 log |S|, which means\nthat taking k to be of the order of log |S| suffices to ensure that\nak G to have the Johnson Lindenstrauss\nproperty. This observation shows that Theorem 5.7 essentially directly implies Theorem 5.1 (although\nnot exacly, since\n1 G is not a projection).\nak\n23This follows from the fact that the maximum of n standard gaussian random variables is ≲\np\n2 log |S|.\n\n5.2.1\nGordon's Escape Through a Mesh Theorem\nTheorem 5.7 suggests that, if ω(S) ≤ak, a uniformly chosen random subspace of Rn of dimension\n(n -k) (which can be seen as the nullspace of G) avoids a set S with high probability. This is indeed\nthe case and is known as Gordon's Escape Through a Mesh Theorem, it's Corollary 3.4. in Gordon's\noriginal paper [Gor88]. See also [Mix14b] for a description of the proof. We include the Theorem\nbelow for the sake of completeness.\nTheorem 5.9 (Corollary 3.4. in [Gor88]) Let S ⊂Sd-1 be a closed subset of the unit sphere in\nd dimensions. If ω(S) < ak, then for a (n -k)-dimensional subspace Λ drawn uniformly from the\nGrassmanian manifold we have\nProb {Λ ∩S = ∅} ≤2 exp\n\n-1\n(ak -ω(S))\n\n,\nwhere ω(S) is the gaussian width of S and ak = E∥gk∥where gk ∼N(0, Ik×k).\n5.2.2\nProof of Gordon's Theorem\nIn order to prove this Theorem we will use extensions of the Slepian's Comparison Lemma.\nSlepian's Comparison Lemma, and the closely related Sudakov-Fernique inequality, are crucial\ntools to compare Gaussian Processes. A Gaussian process is a family of gaussian random variables\nindexed by some set T, {Xt}t T (if T is finite this is simply a gaussian vector). Given a gaussian\n∈\nprocess Xt, a particular quantity of interest is E [maxt T Xt]. Intuitively, if we have two Gaussian\n∈\nprocesses Xt and Yt with mean zero E [Xt] = E [Yt] = 0, for all t ∈T, and the same variance, then the\nprocess that has the \"least correlations\" should have a larger maximum (think the maximum entry\nof vector with i.i.d. gaussian entries versus one always with the same gaussian entry). The following\ninequality makes this intuition precise and extends it to processes with different variances. 24\nTheorem 5.10 (Slepian/Sudakov-Fernique inequality) Let {Xu}u∈U and {Yu}u∈U be two (al-\nmost surely bounded) centered Gaussian processes indexed by the same (compact) set U. If, for every\nu1, u2 ∈U:\nE [Xu1 -\nXu2] ≤E [Yu1 -Yu2] ,\n(51)\nthen\nE\n\nmax Xu\n\n≤E\n\nmax Yu .\nu∈U\nu∈U\n\nThe following extension is due to Gordon [Gor85, Gor88].\nTheorem 5.11 [Theorem A in [Gor88]] Let {Xt,u}\nand Yt,u\nbe two (almost surely\n(t,u)∈T×U\n{\n}(t,u)∈T×U\nbounded) centered Gaussian processes indexed by the same (compact) sets T and U. If, for every\nt1, t2 ∈T and u1, u2 ∈U:\nE [Xt1,u1 -\nXt1,u2] ≤E\n[Yt1,u1 -Yt1,u2] ,\n(52)\n24Although intuitive in some sense, this turns out to be a delicate statement about Gaussian random variables, as it\ndoes not hold in general for other distributions.\n\nand, for t1 = t2,\nE [Xt1,u1 -\nXt2,u2] ≥E [Yt1,u1 -Yt2,u2] ,\n(53)\nthen\nE\n\nmin max Xt,u\nt∈T u∈U\n\n≤E\n\nmin max Yt,u .\nt∈T u∈U\n\nNote that Theorem 5.10 easily follows by setting |T| = 1.\nWe are now ready to prove Gordon's theorem.\nProof. [of Theorem 5.6]\nLet G ∈Rk×d with i.i.d. N(0, 1) entries. We define two gaussian processes: For v ∈S ⊂Sd-1 and\nu ∈Sk-1 let g ∼N (0, Ik\nk) and h ∼N (0, Id\nd) and define the following processes:\n×\n×\nAu,v = gT u + hT v,\nand\nBu,v = uT Gv.\nFor all v, v′ ∈S ⊂Sd-1 and u, u′ ∈Sk-1,\nE\n\n-\n2 -E\n\nT\nT\nAv,u\nAv′,u′\nBv,u -Bv′,u′\n\n=\n4 -2 u u′ + v v′ -\nX\nviuj\nj\nij\n-vi\n′u′\n=\n-2\n\nuT u′ + vT v′ -2 -2 vT\nv′\nuT\n\nu′\n=\n2 -2 uT u′\n\nvT\n+\nv′\n\n-u\n\nT u′vT v\n′\n\n=\n2 1 -uT u′\n1 -vT v′ .\n\nThis means that E Av,u -\nAv′,u′\n-E Bv,u -\nBv′,u′\n\n≥\nand E Av,u -Av′,u′\n-E Bv,u -Bv′,u′\n=\n0 if v = v′.\nThis means that w\n\ne can use Theorem\n\n5.11 with X\n\n= A and Y\n\n= B, to get\n\nE min max Av,u\nv∈S u∈Sk-1\n≤E min max Bv,u.\nv∈S u∈Sk-1\nNoting that\nE min max B\nE min max uT\nv,u =\nGv = E min ∥Gv\nv∈S u Sk-1\nv∈S u Sk-1\nv∈S\n∥,\n∈\n∈\nand\nE\n\nmin max A\nE max g u + E min hT\nv,u\n\n=\nT\nv = E max gT u\nE max( hT v) = ak\nω(S),\nv∈S u∈Sk-1\nu∈Sk-1\nv∈S\nu\n-\n∈Sk-1\n-\nv∈S\n-\ngives the second part of the Theorem.\nOn the other hand, since E\nAv,u -Av′,u′\n2 -E\nBv,u -Bv′,u′\n2 ≥0 then we can similarly use\nTheorem 5.10 with X = B and Y = A, to get\nE max max Av,u\nmax max Bv,u.\nv∈S u∈Sk\nE\n-\n≥\nv∈S u∈Sk-1\n\nNoting that\nE max max Bv,u =\nv∈S u∈Sk-1\nE max max uT Gv = E max Gv ,\nv∈S u∈Sk-1\nv∈S ∥\n∥\nand\nE\n\nmax max Av,u\n\n= E max gT u + E max hT v = ak + ω(S),\nv∈S u∈Sk-1\nu∈Sk-1\nv∈S\nconcludes the proof of the Theorem.\n5.3\nSparse vectors and Low-rank matrices\nIn this Section we illustrate the utility of Gordon's theorem by undertanding which projections are\nexpected to keep the norm of sparse vectors and low-rank matrices.\n5.3.1\nGaussian width of k-sparse vectors\nSay we have a signal (or image) x ∈RN that we are interested in measuring with linear measurements\ny\nT\nN\ni = ai x, for ai ∈R . In general, it is clear that we would need N measurements to find x. The\nidea behind Compressed Sensing [CRT06a, Don06] is that one may be able to significantly decrease\nthe number of measurements needed if we know more about the structure of x, a prime example is\nwhen x is known to have few non-zero entries (being sparse). Sparse signals do arise in countless\napplications (for example, images are known to be sparse in the Wavelet basis; in fact this is the basis\nof the JPEG2000 compression method).\nWe'll revisit sparse recovery and Compressed Sensing next lecture but for now we'll see how\nGordon's Theorem can suggest us how many linear measurements are needed in order to reconstruct\na sparse vector. An efficient way of representing the measurements is to use a matrix\nA =\n--\naT\n--\n\n--\naT\n--\n...\n--\naT\n--\n\nM\n,\nand represent the linear measurements as\ny = Ax.\nIn order to hope to be able to reconstruct x from y we need that A is injective on sparse vectors.\nLet us assume that x is s-sparse, meaning that x has at most s non-zero entries (often written as\n∥x∥0 ≤s, where ∥· ∥0 is called the 0-norm and counts the number of non-zero entries in a vector25).\nIt is also intuitive that, in order for reconstruction to be stable, one would like that not only A is\ninjective in s-sparse vectors but actually almost an isometry, meaning that the l2 distance between\nAx1 and Ax2 should be comparable to the distances between x1 and x2 if they are s-sparse. Since the\ndifference between two s-sparse vectors is a 2s-sparse vector, we can alternatively ask for A to keep\nthe norm of 2s sparse vectors. Gordon's Theorem above suggests that we can take A ∈RM×N to have\n25It is important to note that ∥· ∥0 is not actually a norm\n\ni.i.d. gaussian entries and to take M ≈ω (S2s), where Sk =\nx : x ∈SN-1, ∥x∥0 ≤k\nis the set of 2s\nsparse vectors, and ω (S2s) the gaussian width of S2s.\n\nProposition 5.12 If s ≤N, the Gaussian Width ω (Ss) of Ss, the set of unit-norm vectors that are\nat most s sparse, satisfies\nN\nω (Ss) ≲s log\n\ns\n\n.\nProof.\nω (Ss) =\nmax\nv∈SSN-1, ∥v∥0≤s gT v, log\nN\n,\ns\n\nwhere g ∼N(0, IN\nN). We have\n×\nω (Ss) =\nmax\ngΓ ,\nΓ⊂[N], |Γ|=s ∥\n∥\nwhere gΓ is the restriction of g to the set of indices Γ.\nGiven a set Γ, Theorem 4.12 gives\nProb\nn\ngΓ\n√\n∥\n∥2 ≥s + 2\ns\n√\nt + t\no\n≤exp(-t).\nUnion bounding over all Γ ⊂[N], |Γ| = s gives\nProb\n\nmax\nΓ⊂[N], |Γ|=s ∥gΓ∥2 ≥s + 2√s\n√\nt\n\nN\n+ t\n≤\n\ns\n\nexp(-t)\nTaking u such that t = su, gives\nProb\n\nmax\n√\nΓ⊂[N], |Γ|= ∥gΓ∥2\ns\ns\n≥\n1 + 2\nu + u\n\n≤exp\n\n-su + s log\nN\ns\n\n.\n(54)\nTaking u > log\nN\ns\n\nit can be readily seen that the typical size of maxΓ⊂[N], |Γ|=s ∥gΓ∥2 is ≲\ns log\nN\nq\n.\ns\n\nThe proof can be finished by integrating (54) in order to get a bound of the expectation of\nmaxΓ⊂[N], |Γ|=s ∥gΓ∥2.\nThis suggests that ≈2s log\nN\nmeasurements suffice to identify a 2s-sparse vector. As we'll see,\n2s\nnot only such a number of measuremen\n\nts suffices to identify a sparse vector but also for certain efficient\nalgorithms to do so.\n5.3.2\nThe Restricted Isometry Property and a couple of open problems\nMatrices perserving the norm of sparse vectors do play a central role in sparse recovery, they are said\nto satisfy the Restricted Isometry Property. More precisely:\nDefinition 5.13 (The Restricted Isometry Property) An M × N matrix A (with either real or\ncomplex valued entries) is said to satisfy the (s, δ)-Restricted Isometry Property (RIP),\n(1 -δ)∥x∥2 ≤∥\nAx∥≤(1 + δ)∥x∥2,\nfor all s-sparse x.\n\nUsing Proposition 5.12 and Theorem 5.7 one can readily show that matrices with Gaussian entries\nsatisfy the restricted isometry property with M ≈s log\nN .\ns\nTheorem 5.14 Let A be an M\n\n× N matrix with i.i.d.\nstandard gaussian entries, there exists a\nconstant C such that, if\nM ≥Cs log\nN\ns\n\n,\nthen\n1 A satisfies the\naM\ns, 1\n\n-RIP, with high probability.\nTheorem 5.14 suggests that RIP matrices are abundant for s ≈\nM\nlog(N), however it appears to be\nvery difficult to deterministically construct matrices that are RIP for s ≫\n√\nM, known as the square\nbottleneck [Tao07, BFMW13, BFMM14, BMM14, B+11, Mix14a]. The only known unconditional\nconstruction that is able to break this bottleneck is due to Bourgain et al. [B+11] that achieves\ns ≈\nM\n+ε\nfor a small, but positive, ε.\nThere is a conditional construction, based on the Paley\nEquiangular Tight Frame, that will be briefly described in the next Lecture [BFMW13, BMM14].\nOpen Problem 5.1 Construct deterministic matrices A ∈CM×N (or A ∈CM×N) satisfying (s, 1\n3)-\nRIP for s ≳\nM0.6\n.\npolylog(N\nOpen Problem 5.2 Theorem 5.14 guarantees that if we take A to have i.i.d. Gaussian entries then\nit should be RIP for s ≈\nM\n. If we were able to, given A, certify that it indeed is RIP for some s\nlog(N)\nthen one could have a randomized algorithm to build RIP matrices (but that is guaranteed to eventually\nfind one). This motives the following question\n1. Let N = 2M, for which s is there a polynomial time algorithm that is guaranteed to, with high\nprobability, certify that a gaussian matrix A is\ns, 1\n\n-RIP?\n2. In particular, a\ns, 1 -RIP matrix has to not have s sparse vectors in its nullspace. This mo-\ntivates a second question: Let N = 2M, for which s is there a polynomial time algorithm that\nis guaranteed to, with\n\nhigh probability, certify that a gaussian matrix A does not have s-sparse\nvectors in its nullspace?\nThe second\n√\nquestion is tightly connected to the question of sparsest vector on a subspace (for\nwhich s ≈\nM is the best known answer), we refer the reader to [SWW12, QSW14, BKS13b] for\nmore on this problem and recent advances. Note that checking whether a matrix has RIP or not is,\nin general, NP-hard [BDMS13, TP13].\n5.3.3\nGaussian width of rank-r matrices\nAnother structured set of interest is the set of low rank matrices.\nLow-rank matrices appear in\ncountless applications, a prime example being the Netflix Prize. In that particular example the matrix\nin question is a matrix indexed by users of the Netflix service and movies. Given a user and a movie,\nthe corresponding entry of the matrix should correspond to the score that user would attribute to that\nmovie. This matrix is believed to be low-rank. The goal is then to estimate the score for user and\n\nmovie pairs that have not been rated yet from the ones that have, by exploiting the low-rank matrix\nstructure. This is known as low-rank matrix completion [CT10, CR09, Rec11].\nIn this short section, we will not address the problem of matrix completion but rather make a\ncomment about the problem of low-rank matrix sensing, where instead of observing some of the entries\nof the matrix X ∈Rn1×n2 one has access to linear measuremetns of it, of the form yi = Tr(AT\ni X).\nIn order to understand the number of measurements needed for the measurement procedure to\nbe a nearly isometry for rank r matrices, we can estimate the Gaussian Width of the set of matrices\nX ∈∈Rn1×n2 whose rank is smaller or equal to 2r (and use Gordon's Theorem).\nProposition 5.15\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n≲\np\nr(d1 + d2).\nProof.\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n= E\nmax\nTr(GX).\nX∥F\nrank(\n∥\n=1\nX)≤r\nLet X = UΣV T be the SVD decomposition of X, then\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n= E\nmax\nTr(Σ V T GU ).\nUT U=V T V =Ir×r\nΣ∈Rr×r diagonal ∥Σ∥F =1\n\nThis implies that\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n≤(Tr Σ) (E∥G∥) ≲√r (√n1 + √n1) ,\nwhere the last inequality follows from bounds on the largest eigenvalue of a Wishart matrix, such as\nthe ones used on Lecture 1.\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING\nTREES\nMUSTAZEE RAHMAN\n1. Introduction\nA spanning tree in a connected graph G is a subgraph that contains every vertex of G and is\nitself a tree. Clearly, if G is a tree then it has only one spanning tree. Every connected graph\ncontains at least one spanning tree: iteratively remove an edge from any cycle that is present until\nthe graph contains no cycles. Counting spanning trees is a very natural problem. Following Lyons\n[5] we will see how the theory of graph limits does this in an asymptotic sense. There are many\nother interesting questions that involve understanding spanning trees in large graphs, for example,\nwhat is a 'random spanning tree' of Zd? We will not discuss these questions in this note, however,\nthe interested reader should see chapters 4, 10 and 11 of Lyons and Peres [7].\nLet us begin with some motivating examples. Let Pn denote the path on n vertices. Each Pn\nnaturally embeds into the bi-infinite path whose vertices are the set of integers Z with edges between\nconsecutive integers. By an abuse of notation we denote the bi-infiite path as Z. It is intuitive to\nsay that Pn converges to Z as these paths can be embedded into Z in a nested manner such that\nthey exhaust Z. Clearly, both Pn and Z contain only one spanning tree.\nFigure 1. Extending a spanning tree in Z[-1, 1]2 to a spanning tree in Z[-2, 2]2.\nBlack edges form a spanning tree in Z[-1, 1]2. Red vertices form the cluster of\nchosen vertices on each side and isolated blue vertices are not chosen.\nCorner\nvertices are matched arbitrarily to one of their neighbours.\nThe previous example was too simple. Let us move to the infinite planar grid Z2 where things\nare more interesting. Let Z[-n, n]2 denote the square grid graph on [-n, n]2, that is, the subgraph\n\nMUSTAZEE RAHMAN\nspanned by [-n, n]2 in Z2. There are exponentially many spanning trees in Z[-n, n]2 in terms of\nits size. Indeed, let us see that any spanning tree in Z[-n + 1, n -1]2 can be extended to at least\n28n different spanning trees in Z[-n, n]2. Consider the boundary of Z[-n, n]2 which has 8n vertices\nof the form (±n, y) or (x, ±n). There are four corner vertices (±n, ±n) and vertices on the four\nsides (±n, y) or (x, ±n) where |x|, |y| < n. Consider any subset of vertices S on the right hand side\n{(n, y) : |y| < n}, say. The edges along this side partition S into clusters of paths; two vertices are\nin the same cluster if they lie on a common path (see the red vertices in Figure 1). Pick exactly\none vertex from each cluster, say the median vertex. Connect each such vertex, say (n, y), to the\nvertex (n -1, y) via the edge (n, y) ↔(n\n-1, y), which is the unique edge connecting (n, y) to\nZ[-n + 1, n -1] . If a vertex (n, y′) on the right hand side is not in S then connect it directly to\nZ[-n + 1, n -1]2 via the edge (n, y′) ↔(n -1, y′) (see blue vertices in Figure 1). Do this for each of\nthe four sides and also connect each of the four corner vertices to any one of its two neighbours. In\nthis manner we may extend any spanning tree T in Z[-n+1, n-1]2 to (22n-1)4 ·24 = 28n spanning\ntrees in Z[-n, n]2.\nLet sptr(Z[-n, n]2) denote the number of spanning trees in Z[-n, n]2. The argument above shows\nthat sptr(Z[-n, n]2) ≥28nsptr(Z[-n + 1, n -1]2), from which it follows that sptr(Z[ n, n]2)\n24n(n+1). As |Z[-n, n]2| = (2n + 1)2 we deduce that log sptr(Z[-n, n]2\n-\n≥\n)/|Z[-n, n] | ≥log 2(1 +\nO(n-2)). It turns out that there is a limiting value of log sptr(Z[-n, n]2)/|Z[-n, n]2| as n →inf,\nwhich is called the tree entropy of Z2. We will see that the limiting value depends on Z2, which in\nan intuitively sense is the limit of the grids Z[-n, n]2. We will in fact calculate the tree entropy.\nThe tree entropy of a sequence of bounded degree connected graphs {Gn} is the limiting value of\nlog sptr(Gn)/|Gn| provided it exists. It measures the exponential rate of growth of the number of\nspanning trees in Gn. We will see that that tree entropy exists whenever the graphs Gn converge to\na limit graph in a precise local sense. In particular, this will allow us to calculate the tree entropy\nof the d-dimensional grids Z[-n, n]d and of random d-regular graphs.\n2. Local weak convergence of graphs\nWe only consider connected labelled graphs with a countable number of vertices and of bounded\ndegree. A rooted graph (G, x) is a graph with a distinguished vertex x called the root. Two rooted\ngraphs (G, x) and (H, y) are isomorphic if there is a graph isomorphism φ : G →H such that\nφ(x) = y. In this case we write (G, x) ∼= (H, y). We consider isomorphism classes of rooted graphs,\nalthough we will usually just refer to the graphs instead of their isomorphism class. Given any graph\nG we denote Nr(G, x) as the r-neighbourhood of x in G rooted at x. The distance between two\n(isomorphism classes of) rooted graphs (G, x) and (H, y) is 1/(1+R) where R = min{r : Nr(G, x) ∼=\nNr(H, y)}.\nLet G denote the set of isomorphism classes of connected rooted graphs such that all degrees are\nbounded by ∆. For concreteness we may assume that all these graphs have a common vertex set,\nnamely, {1, 2, 3, . . .}. Then G∆is a metric space with the aforementioned distance function. By\na diagonalization argument it is easy to see that G is a compact metric space. Let F denote the\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nBorel σ-algebra of G under this metric; it is generated by sets of the from A(H, y, r) = {(G, x) ∈G :\nNr(G, x) ∼= Nr(H, y)}. A random rooted graph (G, *) is a probability space (G, F, μ); we think of\n(G, *) as a G-valued random variable such that P\n\n(G, *) ∈A\n\n= μ(A) for every A ∈F.\nLet us see some examples. Suppose G is a finite connected graph of maximum degree ∆. If *G is a\nuniform random vertex of G then (G, *G) is a random rooted graph. We have P\n\n(G, *G) = (H, y)\n=\n(1/|G|) × |{x ∈V (G) : (G, x) ∼= (H, y)}|. If G is a vertex transitive graph, for example Zd, then\n\nfor any vertex *∈G we have a random rooted graph (G, *) which is simply the delta measure\nsupported on the isomorphism class of (G, *). The isomorphism class of G consists of G rooted at\ndifferent vertices. It is conventional in this case to simply think of (G, *) as the fixed graph G. So,\nfor example, Zd is a 'random' rooted graph with root at the origin.\nLet Gn be a sequence of finite connected graphs of maximum degree at most ∆. Let *n denote a\nuniform random vertex of Gn. We say Gn converges in the local weak limit if the law of the random\nrooted graphs (Gn, *n) converge in distribution to the law of a random rooted graph (G, *) ∈G. For\nthose unfamiliar with the notion of converge of probability measures here is an alternative definition.\nFor every r > 0 and any finite connected\n∼\nthat |x ∈V (Gn) : Nr(G, x) ∼= (H, y)|\nand compactness of G it can be shown\n\nrooted graph (H, y) with that Nr(H, y) = (H, y) we require\n/|Gn| converges as n →inf. Using tools from measure theory\nthat there is a random rooted graph (\nif the ratios in the previous sentence converge then P Nr(Gn, *n) =∼Nr(G, *)\nevery r. This is what it means for (Gn, *n) to converge\n\nin distribution to (G, *\nG, *) ∈G such that\n→1 as n →inffor\n).\nThis notion of local weak convergence was introduced by Benjamini and Schramm [3] in order\nto study random planar graphs. Readers interested in a detailed study of local weak convergence\nshould see Aldous and Lyons [1] and the references therein.\nExercise 2.1. Show that the d-dimensional grid graphs Z[-n, n]d converge to Zd in the local weak\nlimit. Show that the same convergence holds for the d-dimensional discrete tori (Z/nZ)d, where two\nvertices x = (x1, . . . , xd) and y = (y1, . . . , yd) are connected if xi = yi ± 1 (mod n) for exactly one i\nand xi = yi for all other i.\nExercise 2.2. Suppose the graphs Gn have maximum degree at most ∆and converge in the local\nweak limit to (G, *). Show that\n\ndeg(*n) conver\nges in distribution (as integer valued random variables)\nto deg(*). Conclude that E deg(*n)\n→E deg(*)\n\n.\n2.1. Local weak limit and simple random walk. Let (G, x) be a rooted graph. The simple\nrandom walk (SRW) on (G, x) (started as x) is a V (G)-valued stochastic process X0 = x, X1, X2 . . .\nsuch that Xk is a uniform random neighbour of Xk\n1 picked independently of X0, . . . , X\n-\nk-1. The\nSRW is a Markov process given by the transition matrix P(u, v) =\n1u∼v where u\ndeg(u)\n∼v means that\n{u, v} is an edge of G. If G has bounded degree then if is easily verified that P\n\nXk = y | X0 = x\n=\nP k(x, y). The k-step return probability to x is pk\nG(x) = P k(x, x) for k ≥0.\n\nSuppose that Gn is a sequence of bounded degree graphs that converge to (G, *) in the local weak\nlimit. We show that the expected k-step return probability of the SRW on (Gn, *n) converges to the\nexpected k-step return probability of the SRW on (G, *). Note that if Nr(G, x) ∼= Nr(H, y) then\n\nMUSTAZEE RAHMAN\npk (x) = pk\nG\nH(y) for all 0 ≤k ≤2r since in order for the SRW to return in k steps it must remain in\nthe (k/2)-neighbourhood on the starting point.\nIf Gn converges to (G, *) then there is a probability space (Ω, Σ, μ) and G-valued random variables\n(G′\nn, *′\nn), (G′, *′) on (Ω, Σ, μ) such that (Gn, *n) has the law of (G′\nn, *′\nn), (G, *) has the law of (G, *),\nand for every r ≥0 the probability μ(Nr(G′\nn, *′\nn) ∼= Nr(G′, *′)) →1 as n →inf. This common\nprobability space where all the graphs can be jointly defined and satisfy the stated claim follows\nfrom Shorokhod's representation theorem. On the event {Nk/2(G′\nn, *′\nn) ∼= Nk/2(G′, *′)} we have\npk\nG (\n′\nn *n) = pk\nG (\n′ *). Therefore,\nE\n\npk (*)\n\n-E\n\npk (*)\n=\nE\n\npk (\nk\nGn\nn\nG\n\nG\n′ )\np\n( ′)\n′\nn *n -\nG′ *\n=\n(\n\nE\n\npk\nG\n*′\nn) -pk\nG (*′); N\n\nk/2(G\n′\nn\n′\n′\nn, *′\nn) ≅Nk/2(G′, *′)\n≤2P\n\nNk/2(G′\nn, *′\nn) ≅Nk/2(G′, *′)\n\n-→0\nas n\n\n→inf\n\n.\n2.2. Local weak limit of random regular graphs. In this section we will show a classical result\nthat random d-regular graphs converge to the d-regular tree Td in the local weak sense (see Bollob as\n[4]). There are a finite number of d-regular graphs on n vertices so we can certainly consider a\nuniform random d-regular graph on n vertices whenever nd is even. However, how do we calculate\nprobabilities and expectations involving a uniform random d-regular graph on n vertices?\nFirst, we would have to calculate the number of d-regular graphs on n vertices. This is no easy\ntask. To get around this issue we will consider a method for sampling (or generating) a random\nd-regular multigraph (that is, graphs with self loops and multiple edges between vertices), This\nsampling procedure is simple enough that we can calculate the expectations and probabilities that\nare of interest to us. We will then relate this model of random d-regular multigraphs to uniform\nrandom d-regular graphs.\nThe configuration model starts with n labelled vertices and d labelled half edges emanating from\neach vertex.\nWe assume that nd is even with d being fixed.\nWe pair up these nd half edges\nuniformly as random and glue every matched pair of half edges into a full edge.\nThis gives a\nrandom d-regular multigraph (see Figure 2). The number of possible matchings of nd half edges is\n(nd -1)!! = (nd -1)(nd -3) · · · 3 · 1. Let Gn,d denote the random multigraph obtained this way.\nThe probability that Gn,d is a simple graph is uniformly bounded away from zero at n →inf. In\nfact, Bender and Canfield [2] showed that as n →inf,\nP\n\nGn,d is simple\n\n→\n1-2\nd\ne\n.\nAlso, conditioned on Gn,d being simple its distribution is a uniform random d-regular graph on\nn vertices.\nIt follows from these observations that any sequence of graph properties An whose\nprobability under Gn,d tends to 1 as n →infalso tends to 1 under the uniform random d-regular\ngraph model. In particular, if Gn,d converges to Td in the local weak limit then so does a sequence\nof uniform random d-regular graphs.\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nFigure 2. A matching of 12 half edges on 4 vertices giving rise to a 3 regular multigraph.\nNow we show that Gn,d converges to Td in the local weak limit. Unpacking the definition of local\nweak limit this means that for every r > 0 we must show that\n(1)\nE\nh |v ∈V (Gn,d) : Nr(Gn,d, v) ∼= Nr(Td, *)|\nn\ni\n→1 as n →inf,\nwhere *is any fixed vertex of Td (note that Td is vertex transitive). Notice that if Nr(Gn,d, v)\ncontains no cycles then it must be isomorphic to Nr(Td, *) due to Gn,d being d-regular.\nNow\nsuppose that Nr(Gn,d, v) contains a cycle. Then this cycle has length at most 2r and v lies within\ndistance r of some vertex of this cycle. Thus the number of vertices v such that Nr(Gn,d, v) is not\na cycle is at most the number of vertices in Gn,d that are within distance r of any cycle of length\n2r in Gn,d. Let us call such vertices bad vertices. The number of vertices within distance r of any\nvertex x ∈V (Gn,d) is at most dr. Therefore, the number of bad vertices is at most dr(2r)C≤2r\nwhere C\n2r is the (random) number of cycles in Gn,d of length at most 2r. It follows from this\n≤\nargument that\n∼\nE |v ∈V (Gn,d) : Nr(Gn,d, v) = Nr(Td, *)|\n≤2rdrE C\nfollo\n≤2r . The\nwing lemma\nshows that E C\n2r\n≤2r\n\n≤2r(3d -3)\nif d ≥3, and more precisely\n\n, E\n\nC≤2\n\nr\nconverges to a finite\nlimit as n →inffor every d. This establishes (1), and thus, Gn,d conv\n\nerges to\n\nTd in the local weak\nlimit.\n(\n1)l\nLemma 2.3. Let\n\nd-\n\nClbe the number of cycles of length lin Gn,d. Then limn\nE C\n→inf\nl\n=\n.\n2l\nMoreover, E Cl\n≤(3d -3)lif d ≥3.\nProof. Given a set of ldistinct vertices {v1, . . . , vl} the number of ways to arrange them in cyclic\norder is (l-1)!/2. Given a cyclic ordering, the number of ways to pair half edges in the configuration\nmodel such that these vertices form a cycle is (d(d\n}\n(\nd-1))\n-1))l(nd\n2l\n1)!!. Therefore, the probability that\n{\nl-1)!(d(\nl(nd-2l\n-\n-\nv1, . . . , vl\nform an l-cycle in Gn,d is\n-1)!!. From the linearity of expectation we\n2(nd-1)!!\nconclude that\n\nn\n(l\n1)!(d(d\n1))l(nd\n2l\n1)!!\nE Cl\n=\nP {v1, . . . , vl} forms an l-cycle\n=\n-\n-\n-\n-\n.\nl\n2(nd\n1)!!\n{v1\nX\n,...,vl}\n\n-\nNote that\nn\n≤\nl\nnl/l!, and in fact if lis fixed then\nn\n= (1 + o(1)) n\nas n →inf. Similarly,\nl\nl\nl!\n(nd-2l-1)!!/((nd-1)!!) = (1+o(1))(nd)-las n →infand it is at most 3l(nd)-lif d ≥3 (provided\n\nMUSTAZEE RAHMAN\nthat lis fixed). It follows from these observations that E\n\nCl\n\n→(d -1)l/(2l), and is at most\n(3d -3)lif d ≥3.\n3. Enumeration of spanning trees\nThe Matrix-Tree Theorem allows us to express the number of spanning trees in a finite graph G\nin terms of the eigenvalues of the SRW transition matrix P of G. As we will see, this expression\nit turn can be written in terms of the return probabilities of the SRW on G. This is good for our\npurposes because if a sequence of bounded degree graphs Gn converges in the local weak limit to\na random rooted graph (G, *\nlog sptr(G\n) then we will be able to express\nn) in terms of the expected\n|Gn|\nreturn probabilities of the SRW on (G, *). In particular, we shall see that\nlog sptr(Gn)\nlim\n= E\nh\nlog deg(*) -\nX pk\nG(*) i\n.\nn→inf\n|Gn|\nk\nk≥1\nThe quantity of the r.h.s. is called the tree entropy of (G, *). If the limiting graph G is deterministic\nand vertex transitive, for example Zd or Td, then the above simplifies to\nlog sptr(Gn)\nlim\n= log d\nn→inf\n|Gn\n-\n|\nk\nX pk\nG(*),\nk\n≥1\nwhere d is the degree of G and *is any fixed vertex. In this manner we will be able to find expressions\nfor the tree entropy of Zd and Td and asymptotically enumerate the number of spanning trees in\nthe grid graphs Z[-n, n]d and random regular graphs Gn,d.\n3.1. The Matrix-Tree Theorem. Let G be a finite graph. Let D be the diagonal matrix consisting\nof the degrees of the vertices of G. The Laplacian of G is the |G| × |G| matrix L = D(I -P), where\nI is the identity matrix and P is the transition matrix of the SRW on G. It is easily seen that\nL(x, x) = deg(x), L(x, y) = -1 if x ∼y in G and L(x, y) = 0 otherwise (if G is a multigraph then\nL(x, y) equals negative of the number of edges from x to y).\nExercise 3.1. The Laplacian L of a graph G is a matrix acting on the vector space RV (G). Let\n(f, g) = P\nx V (G) f(x)g(x) denote the standard inner product on RV (G). Prove each of the following\n∈\nstatements.\n(1) (Lf, g) = 1 P\n(x,y)(f(x) -f(y))(g(x)\n(\nx∼y\n-g y)).\n(2) L is self-adjoint and positive semi-definite: (Lf, g) = (f, Lg) and (Lf, f) ≥0 for all f, g.\n(3) Lf = 0 if and only if f is constant on the connected components of f.\n(4) The dimension of the eigenspace of L corresponding to eigenvalue 0 equals the number of\nconnected components of G.\n(5) If G is connected and has maximum degree ∆then L has |G| eigenvalues 0 = λ0 < λ1 ≤\n· · · ≤λ G\n.\n|-1 ≤2∆\n|\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nLet G be a finite connected graph. From part (5) of exercise 2.2 we see that the Laplacian L of\nG has n = |G| eigenvalues 0 = λ0 < λ1 ≤· · · ≤λn-1. The Matrix-Tree Theorem states that\n(2)\nsptr(G) =\nn-1\nn\ni\nY\nλi.\n=1\nIn other words, the number of spanning trees in G is the product of the non-zero eigenvalues of\nthe Laplacian of G. In fact, the Matrix-Tree Theorem states something a bit more precise. Let Li\nbe the (n-1)×(n-1) matrix obtained from L by removing its i-th row and column; Li is called the\n(i, i)-cofactor of L. The Matrix-Tree Theorem states that det(Li) = sptr(G) for every i. To derive\n(2)\nP\nwe consider the characteristic polynomial det(L -tI) of L and note that the coefficient of t is\n-\ni det(Li) = -nsptr(G). On the other hand, if we write the characteristic polynomial in terms\nn\nof its roots, which are the eigenvaluesof L, then we can deduce that the coefficient of t is -\n-\ni=1 λi.\nExercise 3.2. Let G be a connected finite graph and suppose\nQ\n{x, y} is an edge of G. Let G \\ {x, y}\nbe the graph obtained from removing the edge {x, y} from G. Let G · {x, y} be the graph obtained\nfrom contracting the edge {x, y}. Prove that sptr(G) = sptr(G \\ {x, y}) + sptr(G · {x, y}).\nTry to prove the Matrix-Tree Theorem by induction on the number of edges of G, the identity\nabove, and the expression for the determinant in terms of the cofactors along any row.\nIt is better for us to express (2) in terms of the eigenvalues of the SRW transition matrix P of G.\nThe matrix P also has n real eigen\nP\nvalues. Perhaps the easiest way to see this is to define a new inner\nproduct on RV (G) by (f, g)π =\nx∈V (G) π(x)f(x)g(x) where π(x) = deg(x)/2e and e is the number\nof edges in G. The vector π is\nP\ncalled the stationary measure of the SRW on G. It is a probability\ndistribution on V (G), that is,\nx π(x) = 1. Also, π(x)P(x, y) = π(y)P(y, x). The latter condition\nis equivalent to (Pf, g)π = (f, Pg)π for all f, g ∈RV (G), which means that P is self-adjoint w.r.t. the\ninner product (·, ·)π. Due to being self-adjoint it has n real eigenvalues and an orthonormal basis of\neigenvector w.r.t. the new inner product.\nNotice that the eigenvalues of P lie in the interval [-1, 1] since ||Pf||\n≤||f||\nwhere\ninf\ninf\n||f||\n=\ninf\nmaxx V (G){|f(x)|}. If G is connected then the largest eigenvalue of P is 1 and it has multiplicity\n∈\n1 as well. The eigenfuctions for the eigenvalue 1 are constant functions over V (G). Suppose that\n-1 ≤μ1 ≤μ2 ≤· · · ≤μn\n1 < μn = 1 are the n eigenvalues of P. If e is the number of edges in G\n-\nthen we may rewrite (2) as\n(\n(3)\nsptr(G =\nQ\nx∈V G) deg(x)\n)\nn-1\n)\n2e\nY\n(1\n=1\n-μi .\ni\nThis formula is derived from determining the coefficient of t in the characteristic polynomial of I -P,\nwhich equals (det(D))-1det(L -tD). This is a rather tedious exercise so we leave the derivation to\nthe interested reader.\nFrom 3 and |G| being n we deduce that\nlog sptr(G)\nlog 2e(G)\nx V\n=\n+\n∈\n(G)\n+\ni=1 log(1 -μi).\n|G|\nn\nn\nn\nP\nlog deg(x)\nPn-1\n\nMUSTAZEE RAHMAN\nSince\nP\nlog(1-x) = -\nk\n1 xk/k for -1 ≤\nn\nn\nx < 1 we see that\n-\n≥\ni=1 log(1-μi) = -\n-\nk\nk≥1\ni=1 μi /k.\nn\nNow,\n-1\ni=1 μk\ni = TrP k\nP\n-1 since we exclude the eigenvalue 1\nP\nof P that occurs with\nP\nmultiplicit\nP\ny one.\nNote that TrP k = P\nx V (G) pk\nG(x) where pk\nG(x) in the k-step return probability of the SRW in G\n∈\nstarted from x. Consequently, we conclude that\nlog sptr(G)\n(G)\nP\nk\nlog 2e\nx V\n(4)\n=\n∈\n(G) log deg(x)\n+\n|G|\nn\nn\n-\nX 1 (\nx∈V (G) pG(x)) -1\n.\nk\nn\nk≥1\nP\nTheorem 3.3. Let Gn be a sequence of finite, connected graphs with maximum degree bounded by ∆\nand |Gn| →inf. Suppose that Gn converges in the local weak limit to a random rooted graph (G, *).\nlog sptr(G\nThen\nn) converges to\n|Gn|\nh(G, *) = E\nh\nlog deg(*) -\nk\nX\npk\nk\nG(\n≥1\n*)\ni\n.\nIn particular, suppose that G is a deterministic, vertex transitive graph of degree d. If *∈V (G) is\nany fixed vertex then the tree entropy of G is defined to be\nX 1\nh(G) = log d -\npk\nk\nG(\nk≥1\n*).\nThe tree entropy h(G) does not depend on the choice of the sequence of graphs Gn converging to G\nin the local weak limit.\nTo prove this theorem let *n be a uniform random vertex of Gn. Then from (3) we get that\nlog sptr(Gn)\n2e(Gn)\n=\n+ E log\n|Gn\n|Gn|\n\ndeg(*n)\n\n-\n|\nk\nX\nE pk\nk\nGn( n)\nGn\n-1 .\n≥1\n\n*\n\n-|\n|\n\nAs Gn has degree bounded by ∆we have 2e(Gn) = P\nx V (G\ndeg(x)\n∆n. Thus, log(2e(Gn))/ G\n∈\nn\nn)\n≤\n|\n|\nconverges to 0. Also, deg(*n) converges in distribution to the degree deg(*) of (G, *) (exercise 2.2).\nThe function x →log x is\n\nbounded and continuous if 1 ≤x ≤∆.\nTherefore, E log deg( n)\nconverges to E log deg(*) . Following the discussion is Section 2.1 we conclude that E pk\n*\nG (\nn *n) -\n\n|Gn|-1 converges to E\n\npk\nG(*)\n\nas well. To conclude the proof it suffices to show that\n\nE pk\nGn(*n)\n-|G\n-α\nn|-1 ≤k\nfor some α > 0.\nThen it follows from the dominated\n\ncon\n\nvergence\n\ntheorem that P\nk≥1\n(E\n\npk\nG (*\nn)\nk\n\n-|Gn\nn\n|-) con-\nverges to E\nk\nas required.\n≥1 pk\nG(*)/k ,\nLemma 3.4.\nP\nLet G be a fini\n\nte connected graph of maximum degree ∆. Let pk\nG(x) denote the k-step\nreturn probability of the SRW on G starting at x. Let π(x) = deg(x)/2e for x ∈V (G), where e is\nthe number of edges in G. Then for every x ∈V (G) and k ≥0,\npk\nG(x)\n∆\nπ(x\n-1\n\nn\n≤\n.\n)\n(k + 1)1/4\nProof. The vector π is a probability\n\nmeasure on\n\nV (G). Let (f, g)π =\nx V (G) π(x)f(x)g(x) for\n∈\nf, g ∈RV (G). Let P denote the transition matrix of the SRW on G; thus,\nP\npk\nG(x) = P k(x, x). Note\nthat π(x)P(x, y) = 1x\ny/(2e) = π(y)P(y, x). From this we conclude that (Pf, g)π = (f, Pg)π. Let\n∼\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nU ⊂RV (G) be the subspace of vectors f such that P\nx π(x)f(x) = 0. Then U is a P invariant\nsubspace.\nSuppose f ∈RV (G) takes both positive and negative values. Suppose that |f(x0)| = ||f||\n=\ninf\nmaxx V (G) |f(x)|, and by replacing f with -f if necessary we may assume that f(x\n∈\n0) ≥0. Let z be\nsuch that f(z) ≤0. Then ||f||\n≤|f(x0) -f(z)|. There is a path x0, x1, . . . , x\ninf\nt = z in G from x0\nto z. Therefore,\nt\n||f||inf≤\nX\ni=1\n|f(xi-1) -f(xi)| ≤2\nX\n(x,y)∈V (G)×V (G)\nx∼y\n|f(x) -f(y)|.\nLet K(x, y) = π(x)P(x, y) = 1x ∼y/(2e). The sum above becomes e\n(x,y) K(x, y)|f(x) -f(y)|.\nConsider an f ∈U, which must take both positive and negative values. Apply the inequality\nabove to the function sgn(f)f 2 and use the inequality |sgn(s)s2 -sgn(t)t2| ≤|s -t|(|s| + |t|) to\nconclude that ||f||2 ≤e P\n(x,y) K(x, y)\n\n|f(x) -f(y)\ny\ninf\n|(|f(x)| + |f( )|)\n\n. Straightforward calculations\nshow that\nX\nK(x, y)|f(x) -f(y)|2 =\n(I -P)f, f\n\nand\nX\nK(x, y)|f(x) + f(y)|2 =\n(I + P)f, f\nπ\n(x,y)\n(x,y)\n\n.\nπ\nIf we apply the Cauchy-Schwarz inequality to the terms\np\nK(x, y)|f(x)-f(y)| and\np\nK(x, y)(|f(x)|+\n|f(y)|) then we deduce that\n||f||4\ninf≤e2(I -P)f, f\n\nπ ·\n(I + P)|f|, |f|\n.\nπ\nNotice that(Pf, f)π ≤(f, f)π because all eigenvalues of P lies in\n\nthe interval [0, 1]. Therefore,\n(I +P)|f|, |f|\n≤2( f\nπ\n| |, |f|)π. If (f, f)π ≤1 then we see that\nm\n||f||4\ninf≤2e2 (I -P)f, f\n. Applying\nπ\nthis to the function P\nf and using that P is self-adjoint we deduce that\n\n||P mf||4 ≤2e2\nf\ninf\n(I -P)P mf, P m\n= 2e2 (P 2m\nπ\n-P 2m+1)f, f .\nSince ||Pg||\n≤||g||\n, if we sum the inequality abo\n\nve over 0\n\ninf\ninf\n≤m ≤k we get\nk\n(k + 1)||P kf||4 ≤2e2 X\n||P mf||\n≤2e2(I -P 2k+1)f, f\ninf\nm=0\n\nπ ≤2e .\ninf\nThe\nlast inequalit\n\ny holds because every eigenvalue of I\nP m lies in the interval [0, 1] and thus\n(I -P m\n-\n)f, f\n≤(f, f)π.\nπ\nWe have concluded that ||P kf\n√\n||\n≤\n2e(k + 1)-1/4 if f ∈U and (f, f)π ≤1. Let us now apply\ninf\n√1x(y)-π(x)\nthis to the function f(y) =\n. Then |P kf(x)\n√\n| ≤\n2e(k + 1)-1/4. The value of P kf(x) is\nπ(x)(1-π(x))\nP k(x, x)(1 -pπ(x)) -π(x) P\ny=x P k(y, x)\nP k(x, x)(1 -pπ(x)) -π(x)(1\nP\n=\n-\nk(x, x))\nπ(x)(1 -π(x))\nπ(x)(1 -π(x))\nP k(x, x)\n= p\n-π(x) .\nπ(x)(1 -π(x))\nP\n\nMUSTAZEE RAHMAN\nTherefore,\nP k(x,x) -1\n≤\np\n2eπ(x)-1(1 -π(x))(k + 1)-1/4. However, π(x)-1 = 2e/deg(x)\nπ(x)\n≤2e\nand 1 -π(x) ≤\nP k(x,x)\n1. Thus, we conclude that\n\n-1\n≤2e(k + 1)-1/4. As 2e equals the sum of\nπ(x)\nthe degrees in G we have that 2e ≤∆n, and this establishes the statement in the lemma.\nLemma 3.4 implies that if G is a finite connected graph of maximum degree ∆then\n|G|-1x∈\nX\npk\nG(x)\nV (G)\n\n-1\n\nX\n\n= |G|-1\npk\nG(x) -π(x)\nx∈V (G)\n\nX\npk\n\n≤|G|-\nπ(x)\nG(x)\nπ(x) -1\nx∈V (G)\n\n∆\n≤\n.\n(k + 1)1/4\nThis proves that\nE\n\npk (*)\n\n-|G |-1\nG\nn\nn\nn\n≤∆k-/4 and completes the proof of Theorem 3.3.\n3.2. Tree entropy of T\nZd\nd and\n. In order\n\nto calculate the tree entropy of a graph we have to be\nable to compute the return probability of the SRW on the graph. There is a rich enough theory that\ndoes this for d-regular tree and Zd. We begin\nP\nwith the d-regular tree Td.\nConsider the generating function F(t) =\nk\n0 pk (*)tk. Actually note that pk (\nif\n≥\n*) = 0\nk is\nTd\nTd\nodd because whenever the SRW takes a step along an edge that moves it away from the root it\nmust traverse that edge backwards in order to return. So it is not possible to return in an odd\nnumber of steps. (This holds in any bipartite graph, for example, Zd as well.) There is a classical\nproblem called the Ballot Box problem that allows us to compute p2k(\nTd *) explicitly. It turns out that\n2k\n2k\nk\np2k\n( ) (d\n1) -\n( )\n(*) =\nk\n-\n2k\n. The numbers\nk\nare called the Catalan numbers. From this it is possible\nTd\nk+1\nd\n-\nk+1\nto find a closed form of F(t) (see Wo\n\ness [9\n\n] Lemma 1.24):\n2(d\nF(t) =\n-1)\nd -1 +\np\n.\nd2 -4(d -1)t2\nNote that P\nk\n1 pk (*) =\nR 1 F (t)-1 dt. It turns out that this integrand has an antiderivative and\n≥\nTd\nt\nit can be shown that\n(d\nh(Td) = log\n-1)d-1\n.\n(d -2d)(d/2)-1\nThis result was proved by McKay[8]. Since the\nh\nrandom d-regular\ni\ngraphs Gn,d converge to Td in\nthe local weak limit we see that E log sptr(Gn,d)\n\n= nh(Td) + o(n).\nA rigorous calculation of the tree entropy of Zd requires an excursion into operator theory that is\noutside the scope of these notes. We will sketch the argument; for details see Lyons [5] Section 4 or\nLyons [6]. Recall the Matrix-Tree Theorem for finite graphs which states that if λ1, . . .\nP\n, λn-1 are the\nn\npositive eigenvalues of the Laplacian of a graph G of size n then log sptr(G)/n = (1/n)\n-\ni=1 log λi-\n(log n)/n. There is an infinitary version of this representation for the tree entropy of Zd. If L is\nthe Laplacian on Zd, which acts on l2(Zd), then one can define an operator log L on l2(Zd) that\nsatisfies\nh(Zd) =\n\n(log L) 1o, 1o\n\n.\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nIn the above, (·, ·) is the inner product on l2(Zd) and 1o is the indicator function of the origin.\nThe inner product above may be calculated via the Fourier transform. The Fourier transform\nstates that l2(Zd) is isomorphic as a Hilbert space to L2([0, 1]d) in that any function f\nl2(Zd)\n\n∈\nˆ\n\nˆ\ncorresponds to an unique f ∈L2([0, 1]d) such that f(k) =\nR\nf(x)e2πi x·k dx. This correspondence\npreserves the inner product between functions. The Fourier transform maps 1o to the function that\nis identically 1 in [0, 1]d. It also transforms the operator log L to the operator on L2([0, 1]d) which\nis multiplication by the function (x1, . . . , xd) →log(2d -2 P\ni cos(2πxi)). As the Fourier transform\npreserves inner products we get that\nd\nh(Zd) =\nZ\nlog\n[0,1]d\n\n2d -2\nX\ncos(2πxi)\ni=1\n\ndx.\n4. Open problems\nOne can consider the space of (isomorphism classes of) doubly rooted graphs (G, x, y) of bounded\ndegree, analogous to the space G. It is also a compact metric space where the distance between\n(G, x, y) and (H, u, v) is 1/(1 + R) where R is the minimal r such that the r-neighborhood of (x, y)\nin G is isomorphic to the r-neighbourhood of (u, v) in H. Consider a Borel measurable function F\nfrom the space of double rooted graphs into [0, inf). Note that F is defined on isomorphism classes\nof such graphs, so F(G, x, y) = F(H, u, v) if φ : G →H is a graph isomorphism satisfying φ(x) = u\nand φ(y) = v.\nA random rooted graph (G, *) is unimodular if for all F as above the following equality holds\nE\nh\nX\nF(*, x)\nE\n∼*in G\ni\n=\nh\nX\nF(x, *)\nx\nx∼*in G\ni\n.\nHere is an example. Let G be a finite connected graph and suppose *∈G is a uniform random root.\nThen\nP\n(G, *) is unimodular because both the left and right hand side of the equation above equals\n(x,y) F(x, y), where the sum is over all pair of vertices in G. Unimodularity is preserved under\nx∼y\ntaking local weak limits and so any random rooted graph (G, *) that is a local weak limit of finite\nconnected graphs is unimodular.\nIt is not known whether the converse is true: is a unimodular random rooted graph (G, *) a local\nweak limit of finite connected graphs. This is known to be true for unimodular trees (see Aldous\nand Lyons [1]). This is a major open problem in the field.\nHere is a problem on tree entropy. Bernoulli bond percolation on Td at density p is a random\nforest of Td obtained by deleting each edge independently with probability 1 -p. Let *be a fixed\nvertex and denote by Cp(*) the component of *in the percolation process. Then Cp(*) is finite with\nprobability one if p ≤1/(d -1) and infinite with positive probability otherwise. Let Cp\ninf(*) be the\nrandom rooted tree obtained from Cp(*) by conditioning it to be infinite if p > 1/(d -1). In fact,\nCp\ninf(*) is unimodular. What is the tree entropy of Cp\ninf(*)? Is it strictly increasing in p?\n\nMUSTAZEE RAHMAN\nReferences\n[1] D. J. Aldous and R. Lyons, Processes on unimodular networks, Electron. J. Probab. 12 #54 (2007), pp. 1454-\n1508.\n[2] E. A. Bender and E. R. Canfield, The asymptotic number of labelled graphs with given degree sequences, Journal\nof Combinatorial Theory Series A 24 (1978), pp. 296-307.\n[3] I. Benjamini and O. Schramm, Recurrence of distributional limits of finite planar graphs, Electron. J. Probab. 6\n#23 (2001).\n[4] B. Bollob as, Random graphs, 2nd ed., Cambridge University Press, Cambridge, 2001.\n[5] R. Lyons, Asymptotic enumeration of spanning trees. Combin. Probab. Comput. 14 (2005), pp. 491-522.\n[6] R. Lyons, Identities and inequalities for tree entropy, Combin. Probab. Comput. 19 (2010), pp. 303-313.\n[7] R. Lyons and Y. Peres, Probability on Trees and Networks, Cambridge University Press, 2014, in preparation.\nCurrent version available at\n.\n[8] B. D. McKay, Spanning trees in regular graphs, Europ. J. Combin. 4 (1983), pp. 149-160.\n[9] W. Woess, Random Walks on Infinite Graphs and Groups, Cambridge University Press, Cambridge, 2000.\nDepartment of Mathematics, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cam-\nbridge, MA 02139\nhttp://mypage.iu.edu/~rdlyons/\n\nCompressed Sensing and Sparse Recovery\nMost of us have noticed how saving an image in JPEG dramatically reduces the space it occupies in\nour hard drives (as oppose to file types that save the pixel value of each pixel in the image). The idea\nbehind these compression methods is to exploit known structure in the images; although our cameras\nwill record the pixel value (even three values in RGB) for each pixel, it is clear that most collections of\npixel values will not correspond to pictures that we would expect to see. This special structure tends\nto exploited via sparsity. Indeed, natural images are known to be sparse in certain bases (such as the\nwavelet bases) and this is the core idea behind JPEG (actually, JPEG2000; JPEG uses a different\nbasis).\nLet us think of x ∈RN as the signal corresponding to the image already in the basis for which it is\nsparse. Let's say that x is s-sparse, or ∥x∥0 ≤s, meaning that x has, at most, s non-zero components\nand, usually, s ≪N. The l0 norm26 ∥x∥0 of a vector x is the number of non-zero entries of x. This\nmeans that, when we take a picture, our camera makes N measurements (each corresponding to a pixel)\nbut then, after an appropriate change of basis, it keeps only s ≪N non-zero coefficients and drops\nthe others. This motivates the question: \"If only a few degrees of freedom are kept after compression,\nwhy not measure in a more efficient way and take considerably less than N measurements?\". This\nquestion is in the heart of Compressed Sensing [CRT06a, CRT06b, CT05, CT06, Don06, FR13]. It is\nparticularly important in MRI imaging [?] as less measurements potentially means less measurement\ntime. The following book is a great reference on Compressed Sensing [FR13].\nMore precisely, given a s-sparse vector x, we take s < M ≪N linear measurements yi = aT\ni x and\nthe goal is to recover x from the underdetermined system:\n\ny\n\n=\n\nA\n\nx\n\n.\nLast lecture\nwe used Gordon's theorem to show that, if w\n\ne to\n\nok random measurements, on the\norder of s log\nN\ns\n\nmeasurements suffice to have all considerably different s-sparse signals correspond\nto considerably different sets of measurements. This suggests that ≈s log\nN\nmay be enough to\ns\nrecover x, we'll see (later) in this lecture that this intuition is indeed correct.\nSince the system is underdetermined and we know x is sparse, the natural thing\n\nto try, in order\nto recover x, is to solve\nmin\n∥z∥0\n(55)\ns.t.\nAz = y,\nand hope that the optimal solution z corresponds to the signal in question x. Unfortunately, (55) is\nknown to be a computationally hard problem in general. Instead, the approach usually\nP\ntaken in sparse\nN\nrecovery is to consider a convex relaxation of the l0 norm, the l1 norm: ∥z∥1 =\ni=1 |zi|. Figure 19\n26The l0 norm is not actually a norm though.\n\ndepicts how the l1 norm can be seen as a convex relaxation of the l0 norm and how it promotes\nsparsity.\nFigure 19: A two-dimensional depiction of l0 and l1 minimization. In l1 minimization (the picture\nof the right), one inflates the l1 ball (the diamond) until it hits the affine subspace of interest, this\nimage conveys how this norm promotes sparsity, due to the pointy corners on sparse vectors.\nThis motivates one to consider the following optimization problem (surrogate to (55)):\nmin\n∥z∥1\n(56)\ns.t.\nAz = y,\nIn order for (56) we need two things, for the solution of it to be meaningful (hopefully to coincide\nwith x) and for (56) to be efficiently solved.\nWe will formulate (56) as a Linear Program (and thus show that it is efficiently solvable). Let us\nthink of ω+ as the positive part of x and ω-as the symmetric of the negative part of it, meaning that\nx = ω+ -ω-and, for each i, either ωi\n-or ω+\ni is zero. Note that, in that case,\nN\n∥x∥1 =\nX\nω+\nT\ni + ωi\n-= 1\nω+ + ω-\ni=1\n\n.\nMotivated by this we consider:\nmin\n1T (ω+ + ω-)\ns.t.\nA (ω+ -ω-) = y\nω+\n(57)\n≥0\nω-≥0,\nwhich is a linear program. It is not difficult to see that the optimal solution of (57) will indeed satisfy\nthat, for each i, either ωi\n-or ω+\ni is zero and it is indeed equivalent to (56). Since linear programs are\nefficiently solvable [VB04], this means that (56) efficiently.\n\n6.1\nDuality and exact recovery\nThe goal now is to show that, under certain conditions, the solution of (56) indeed coincides with x.\nWe will do this via duality (this approach is essentially the same as the one followed in [Fuc04] for the\nreal case, and in [Tro05] for the complex case.)\nLet us start by presenting duality in Linear Programming with a game theoretic view point. The\nidea will be to reformulate (57) without constraints, by adding a dual player that wants to maximize\nthe objective and would exploit a deviation from the original constraints (by, for example, giving the\ndual player a variable u and adding to to the objective uT (y -A (ω+ -ω-))). More precisely consider\nthe following\nmin max\nv+ T\n1T ω+ + ω-\nω+\n+\nu\nω\nv+\n\n-\n\n-\nT\nv-\nω-+ uT y -A ω+ -ω-\n.\n(58)\nω-\nv\n≥\n-≥0\n\nIndeed, if the primal player (picking ω+ and ω-and attempting to minimize the objective) picks\nvariables that do not satisfy the original constraints, then the dual player (picking u, v+, and v-and\ntrying to maximize the objective) will be able to make the objective value as large as possible. It is\nthen clear that (57) = (58).\nNow image that we switch the order at which the players choose variable values, this can only\nbenefit the primal player, that now gets to see the value of the dual variables before picking the primal\nvariables, meaning that (58) ≥(59), where (59) is given by:\nmax min 1T ω+ + ω-\n-\nv+T\nT\nω+\n-\nu\n+\nv+\nω\n≥0\n-\nω-\n-≥\nv\nv\n\nω-+ uT y -A\nω+ -ω-\n.\n(59)\nRewriting\nT\nT\nmax min 1\nv+\nAT u\nω+ + 1\nv-+ AT u\nω-+ uT y\n(60)\nu\nω+\nv+≥0\n-\nω-\nv-≥0\n-\n\n-\n\nWith this formulation, it becomes clear that the dual players needs to set 1 -v+ -AT u = 0,\n1 -v-+ AT u = 0 and thus (60) is equivalent to\nmax\nuT y\nu\nv+\nv\n≥\n-≥0\n1-v+-AT u=0\n1-v-+AT u=0\nor equivalently,\nmaxu\nuT y\ns.t.\n-1 ≤AT\n(61)\nu ≤1.\nThe linear program (61) is known as the dual program to (57). The discussion above shows that\n(61) ≤(57) which is known as weak duality. More remarkably, strong duality guarantees that the\noptimal values of the two programs match.\n\nThere is a considerably easier way to show weak duality (although not as enlightening as the one\nabove). If ω-and ω+ are primal feasible and u is dual feasible, then\n0 ≤\n1T -uT A\n\nω+ +\n1T + uT A\n\nω-= 1T ω+ + ω-\n-uT\nA\nω+ -ω-\n= 1T\nshowing that (61)\n(57).\nω+ + ω-\n-uT y,\n(62)\n≤\n6.2\nFinding a dual certificate\nIn order to show that ω+ -ω-= x is an optimal solution27 to (57), we will find a dual feasible point\nu for which the dual matches the value of ω+ -ω-= x in the primal, u is known as a dual certificate\nor dual witness.\nFrom (62) it is clear that u must satisfy\n1T -uT A ω+ = 0 and 1T + uT A ω-= 0, this is known\nas complementary slackness. This means that we must\n\nhave the en\ntries of AT u\n\nbe +1 or -1 when x\nis non-zero (and be +1 when it is positive and -1 when it is negative), in other words\nAT u\n\n= sign (xS) ,\nS\nwhere S = supp(x), and\nAT u\n\n≤1 (in order to be dual feasible).\ninf\nRemark 6.1 It is not difficult to see that if we further ask that\nsolution would have to have its support contained in the support\nAT u\n\nc\n< 1 any optimal primal\nS\ninf\nof x. This observation gives us the\nfollowing Lemma.\nLemma 6.2 Consider the problem of sparse recovery discussed this lecture. Let S = supp(x), if AS\nis injective and there exists u ∈RM such that\nAT u\n\n= sign (xS) ,\nS\nand\nAT u Sc\n< 1,\ninf\nthen x is the unique optimal solution to the\nl1 minimization\n\nprogram 56.\nSince we know that\nAT u\n\n=sign\n(xS) (and\n\nthat AS is injective), we'll try to construct u by least\nS\nsquares and hope that it satisfies AT u\nc\n< 1. More precisely, we take\nS\ninf\nu = AT\n+\nS\nsign (xS) ,\nwhere\nAT + = A\nAT\nS\n-\nS\nSAS\n\nis the Moore Penrose pseudo-inverse of AT\nS. This gives the following\nCorollary.\nCorollary 6.3 Consider the problem of sparse recovery discussed this lecture. Let S = supp(x), if\nAS is injective and\nAT\nAT\nScAS\nSAS\n-sign (xS)\n< 1,\nSc\ninf\nthen x is the unique optimal solution\n\nto\nthe l1\n\nminimization pr\n\nogram 56.\n27For now we will focus on showing that it is an optimal solution, see Remark 6.1 for a brief discussion of how to\nstrengthen the argument to show uniqueness\n\nRecall the definition of A ∈RM×N satisfying the restricted isometry property from last Lecture.\nDefinition 6.4 (Restricted Isometry Property) A matrix A ∈RM×N is said to satisfy the (s, δ)\nrestricted isometry property if\n(1 -δ) ∥x∥≤∥Ax∥2 ≤(1 + δ) ∥x∥2 ,\nfor all two s-sparse vectors x.\nLast lecture (Lecture 5) we showed that if M ≫s log\nN\ns\n\nand A ∈RM×N is drawn with i.i.d.\ngaussian entries N\n0, 1\nM\n\nthen it will, with high probability, satisfy the (s, 1/3)-RIP. Note that, if A\nsatisfies the (s, δ)-RIP then, for any |S| ≤s one has ∥AS∥≤\nq\n1 + 1\n3 and l\nAT\nSAS\n-1 ∥≤\n1 -1\n-1 =\n3, where\n∥· ∥denotes the operator norm ∥B∥= max\nBx .\n∥x∥=1 ∥\n∥\nThis means that, if we take A random with i.i.d. N\n0, 1\nM\n\nentries then, for any |S ≤s| we have\nthat\n∥AS\nAT\nSAS\n-1 sign (xS) ∥≤\nr\n1 + 1\n2 =\n√\n3√s,\nand because of the independency among the entries of A, ASc is independent of this vector and so for\neach j ∈Sc we have\nProb\n\nAT A\nAT\n(\n\nS\n-\nj\nSAS\n\nsign xS) ≥√\nM\n√\n3√st\n\n≤2 exp\n\n-t2\n,\n\nwhere Aj is the j-th column of A.\nUnion bound gives\n\nT\nT\n-1\n\nProb\nASAS ASAS\nsign (xS)\ninf≥√\nM\n√\n3√st\n\n≤2N exp\n\n-t2\n\n,\nwhich implies\nProb\nAT\nSAS\nAT\nSAS\n-1 sign (xS)\n\ninf≥1\n\n≤2N exp\n\n-\n√\nM\n√\n3s\n\n= exp\n\n-1\nM\n-2 log(2N)\n,\ns\n\nwhich means that we expect to exactly recover x via l1 minimization when M ≫s log(N), similarly\nto what was predicted by Gordon's Theorem last Lecture.\n6.3\nA different approach\nGiven x a sparse vector, we want to show that x is the unique optimal solution to\nmin\n∥z∥1\n(63)\ns.t.\nAz = y,\nLet S = supp(x) and suppose that z = x is an optimal solution of the l1 minimization problem.\nLet v = z -x, it satisfies\n∥v + x∥1 ≤∥x∥1\nand\nA(v + x) = Ax,\n\nthis means that Av = 0. Also, ∥x∥S = ∥x∥1 ≥∥v+x∥1 = ∥(v + x)S ∥1+∥vSc∥1 ≥∥x∥S-∥vS∥1+∥v∥Sc,\nwhere the last inequality follows by triangular inequality. This means that ∥vS∥1 ≥∥vSc∥1, but since\n|S| ≪N it is unlikely for A to have vectors in its nullspace that are this concentrated on such few\nentries. This motivates the following definition.\nDefinition 6.5 (Null Space Property) A is said to satisfy the s-Null Space Property if, for all v\nin ker(A) (the nullspace of A) and all |S| = s we have\n∥vS∥1 < ∥vSc∥1.\nFrom the argument above, it is clear that if A satisfies the Null Space Property for s, then x will\nindeed be the unique optimal solution to (56). Also, now that recovery is formulated in terms of certain\nvectors not belonging to the nullspace of A, one could again resort to Gordon's theorem. And indeed,\nGordon's Theorem can be used to understand the number of necessary measurements to do sparse\nrecovery28 [CRPW12]. There is also an interesting\napproach based on Integral Geometry [ALMT14].\nAs it turns out one can show that the\n2s, 1\ne\n\n-RIP implies s-NSP [FR13]. W omit that proof as\nit does not appear to be as enlightening (or adaptable) as the approach that was shown here.\n6.4\nPartial Fourier matrices satisfying the Restricted Isometry Property\nWhile the results above are encouraging, rarely one has the capability of designing random gaussian\nmeasurements. A more realistic measurement design is to use rows of the Discrete Fourier Transform:\nConsider the random M × N matrix obtained by drawing rows uniformly with replacement from the\nN × N discrete Fourier transform matrix. It is known [CT06] that if M = Ωδ(K polylog N), then the\nresulting partial Fourier matrix satisfies the restricted isometry property with high probability.\nA fundamental problem in compressed sensing is determining the order of the smallest number M\nof random rows necessary. To summarize the progress to date, Cand`es and Tao [CT06] first found that\nM = Ωδ(K log6 N) rows suffice, then Rudelson and Vershynin [RV08] proved M = Ωδ(K log4 N), and\nmore recently, Bourgain [Bou14] achieved M = Ωδ(K log3 N); Nelson, Price and Wootters [NPW14]\nalso achieved M = Ωδ(K log3 N), but using a slightly different measurement matrix. The latest result\nis due to Haviv and Regev [HR] giving an upper bound of M = Ωδ(K log2 k log N). As far as lower\nbounds, in [BLM15] it was shown that M = Ωδ(K log N) is necessary. This draws a contrast with\nrandom Gaussian matrices, where M = Ωδ(K log(N/K)) is known to suffice.\nOpen Problem 6.1 Consider the random M × N matrix obtained by drawing rows uniformly with\nreplacement from the N × N discrete Fourier transform matrix. How large does M need to be so that,\nwith high probability, the result matrix satisfies the Restricted Isometry Property (for constant δ)?\n6.5\nCoherence and Gershgorin Circle Theorem\nLast lectures we discussed the problem of building deterministic RIP matrices (building deterministic\nRIP matrices is particularly important because checking whether a matrix is RIP is computationally\n28In these references the sets considered are slightly different than the one described here, as the goal is to ensure\nrecovery of just one sparse vector, and not all of them simultaneously.\n\nhard [BDMS13, TP13]).\nDespite suboptimal, coherence based methods are still among the most\npopular ways of building RIP matrices, we'll briefly describe some of the ideas involved here.\nRecall the definition of the Restricted Isometry Property (Definition 6.4). Essentially, it asks that\nany S ⊂[N], |S| ≤s satisfies:\n(1 -δ)∥x∥2 ≤∥A x∥2 ≤(1 + δ)∥x∥2\nS\n,\nfor all x ∈R|S|. This is equivalent to\nxT\nmax\nx\nAT\nSAS -I\n\nx\nx\n≤δ,\nT x\nor equivalently\nAT\nSAS -I\n≤δ.\nIf the columns of A are unit-norm vectors (in RM), then the diagonal of AT\nSAS is all-ones, this\nmeans that AT\nSAS -I consists only of the non-diagonal elements of AT\nSAS. If, moreover, for any two\ncolumns\n\na\nT\n\ni,aj, of A we have\nai aj\n≤μ for some μ then, Gershgorin's circle theorem tells us that\nAT\nSAS -I ≤δ(s -1).\nMore precisely, given a symmetric matrix B, the Gershgorin's circle theorem [HJ85] tells that all\nof the eigenvalues\nn of B are contained\nP\nin thoe so called Gershgorin discs (for each i, the Gershgorin disc\ncorresponds to\nλ : |λ -Bii| ≤\nj=i |Bij| . If B has zero diagonal, then this reads:\nB\n\n∥\n∥≤maxi |Bij|.\nGiven a set of N vectors a\nM\n1, . . . , aN ∈R\nwe define its worst-case coherence μ as\nμ = max aT\ni aj\ni=j\nGiven a set of unit-norm vectors a\n\n1, . . . , aNRM with w\n\norst-case coherence μ, if we form a matrix\nwith these vectors as columns, then it will be (s, μ(s -1)μ)-RIP, meaning that it will be\ns, 1\n\n- RIP\nfor s ≤1\n1.\nμ\n6.5.1\nMutually Unbiased Bases\nWe note that now we will consider our vectors to be complex valued, rather than real valued, but all\nof the results above hold for either case.\nConsider the following 2d vectors: the d vectors from the identity basis and the d orthonormal\nvectors corresponding to columns of the Discrete Fourier Transform F. Since these basis are both\northonormal the vectors in question are unit-norm and within the basis are orthogonal, it is also easy\nto see that the inner product between any two vectors, one from each basis, has absolute value\n√,\nd\nmeaning that the worst case coherence of this set of vectors is μ =\n√\nd this corresponding matrix [I F]\nis RIP for s ≈\n√\nd.\nIt is easy to see that\n√\ncoherence is the minimum possible between two orthonormal bases in\nd,\nd\nC\nsuch bases are called unbiased (and are important in Quantum Mechanics, see for example [BBRV01])\nThis motivates the question of how many orthonormal basis can be made simultaneously (or mutually)\nunbiased in Cd, such sets of bases are called mutually unbiased bases. Let M(d) denote the maximum\nnumber of such bases. It is known that M(d) ≤d + 1 and that this upper bound is achievable when\nd is a prime power, however even determining the value of M(6) is open [BBRV01].\n\nOpen Problem 6.2 How many mutually unbiased bases are there in 6 dimensions? Is it true that\nM(6) < 7?29\n6.5.2\nEquiangular Tight Frames\nAnother natural question is whether one can get better coherence (or more vectors) by relaxing the\ncondition that the set of vectors have to be formed by taking orthonormal basis. A tight frame (see,\nfor example, [CK12] for more on Frame Theory) is a set of N vectors in CM (with N ≥M) that spans\nCM \"equally\". More precisely:\nDefinition 6.6 (Tight Frame) v1, . . . , vN ∈CM is a tight frame if there exists a constant α such\nthat\nX\nN\n|⟨vk, x⟩|2 = α∥x∥2,\n∀x∈CM ,\nk=1\nor equivalently\nX\nN\nvkvT\nk = αI.\nk=1\nThe smallest coherence of a set of N unit-norm vectors in M dimensions is bounded below by the\nWelch bound (see, for example, [BFMW13]) which reads:\nμ ≥\ns\nN -M\n.\nM(N -1)\nOne can check that, due to this limitation, deterministic constructions based on coherence cannot\nyield matrices that are RIP for s\n√\n≫\nM, known as the square-root bottleneck [BFMW13, Tao07].\nThere are constructions that achieve the Welch bound, known as Equiangular Tight Frames\n(ETFs), theseqare tight frames for which all inner products between pairs of vectors have the same\nmodulus μ =\nN-M , meaning that they are \"equiangular\". It is known that for there to exist an\nM(N-1)\nETF in CM one needs N ≤M2 and ETF's for which N = M2 are important in Quantum Mechanics,\nand known as SIC-POVM's. However, they are not known to exist in every dimension (see here for\nsome recent computer experiments [SG10]). This is known as Zauner's conjecture.\nOpen Problem 6.3 Prove or disprove the SIC-POVM / Zauner's conjecture: For any d, there exists\nan Equiangular tight frame with d2 vectors in Cd dimensions. (or, there exist d2 equiangular lines in\nCd).\nWe note that this conjecture was recently shown [Chi15] for d = 17 and refer the reader to\nthis interesting remark [Mix14c] on the description length of the constructions known for different\ndimensions.\n29The author thanks Bernat Guillen Pegueroles for suggesting this problem.\n\n6.5.3\nThe Paley ETF\nThere is a simple construction of an ETF made of 2M vectors in M dimensions (corresponding to\na M × 2M matrix) known as the Paley ETF that is essentially a partial Discrete Fourier Transform\nmatrix. While we refer the reader to [BFMW13] for the details the construction consists of picking\nrows of the p × p Discrete Fourier Transform (with p ∼= 1 mod 4 a prime) with indices corresponding\nto quadratic residues modulo p. Just by coherence considerations this construction is known to be\nRIP for s\n√\n≈\np but conjectured [BFMW13] to be RIP for s ≈\np\n, which would be predicted if\npolylogp\nthe choice os rows was random (as discussed above)30. Although partial conditional (conditioned on\na number theory conjecture) progress on this conjecture has been made [BMM14] no unconditional\nresult is known for s\n√\n≪\np. This motivates the following Open Problem.\nOpen Problem 6.4 Does the Paley Equiangular tight frame satisfy the Restricted Isometry Property\npass the square root bottleneck? (even by logarithmic factors?).\nWe note that [BMM14] shows that improving polynomially on this conjecture implies an improve-\nment over the Paley clique number conjecture (Open Problem 8.4.)\n6.6\nThe Kadison-Singer problem\nThe Kadison-Singer problem (or the related Weaver's conjecture) was one of the main questions in\nframe theory, it was solved (with a non-constructive proof) in the recent breakthrough of Marcus,\nSpielman, and Srivastava [MSS15b], using similar techniques to their earlier work [MSS15a]. Their\ntheorem guarantees the existence of universal constants η\nM\n≥2 and θ > 0 s.t. for any tight frame\nω1, . . . , ωN ∈C\nsatisfying ∥ωk∥≤1 and\nX\nN\nωkωT\nk = ηI,\nk=1\nthere exists a partition of this tight frame S1, S2 ⊂[N] such that each is \"almost a tight frame\" in the\nsense that,\nX\nωkωT\nk ⪯(η -θ) I.\nk∈Sj\nHowever, a constructive prove is still not known and there is no known (polynomial time) method\nthat is known to construct such partitions.\nOpen Problem 6.5 Give a (polynomial time) construction of the tight frame partition satisfying the\nproperties required in the Kadison-Singer problem (or the related Weaver's conjecture).\n30We note that the quadratic residues are known to have pseudorandom properties, and indeed have been leveraged\nto reduce the randomness needed in certain RIP constructions [BFMM14]\n\nGroup Testing and Error-Correcting Codes\n7.1\nGroup Testing\nDuring the Second World War the United States was interested in weeding out all syphilitic soldiers\ncalled up for the army. However, syphilis testing back then was expensive and testing every soldier\nindividually would have been very costly and inefficient. A basic breakdown of a test is: 1) Draw\nsample from a given individual, 2) Perform required tests, and 3) Determine presence or absence of\nsyphilis.\nIf there are n soldiers, this method of testing leads to n tests.\nIf a significant portion of the\nsoldiers were infected then the method of individual testing would be reasonable. The goal however,\nis to achieve effective testing in the more likely scenario where it does not make sense to test n (say\nn = 100, 000) people to get k (say k = 10) positives.\nLet's say that it was believed that there is only one soldier infected, then one could mix the samples\nof half of the soldiers and with a single test determined in which half the infected soldier is, proceeding\nwith a binary search we could pinpoint the infected individual in log n tests. If instead of one, one\nbelieves that there are at most k infected people, then one could simply run k consecutive binary\nsearches and detect all of the infected individuals in k log n tests. Which would still be potentially\nmuch less than n.\nFor this method to work one would need to observe the outcome of the previous tests before\ndesigning the next test, meaning that the samples have to be prepared adaptively. This is often not\npractical, if each test takes time to run, then it is much more efficient to run them in parallel (at\nthe same time). This means that one has to non-adaptively design T tests (meaning subsets of the n\nindividuals) from which it is possible to detect the infected individuals, provided there are at most k\nof them. Constructing these sets is the main problem in (Combinatorial) Group testing, introduced\nby Robert Dorfman [Dor43] with essentially the motivation described above.31\nLet Ai be a subset of [T] = {1, . . . , T} that indicates the tests for which soldier i participates.\nConsider A the family of n such sets A = {A1, . . . , An}.\nWe say that A satisfies the k-disjunct\nproperty if no set in A is contained in the union of k other sets in A. A test set designed in such a\nway will succeed at identifying the (at most k) infected individuals - the set of infected tests is also a\nsubset of [T] and it will be the union of the Ai's that correspond to the infected soldiers. If the set\nof infected tests contains a certain Ai then this can only be explained by the soldier i being infected\n(provided that there are at most k infected people).\nTheorem 7.1 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nT = O\nk2 log n\nProof.\nWe will use the probabilistic method. We will\n\n.\nshow that, for T = Ck2 log n (where C is\na universal constant), by drawing the family A from a (well-chosen) distribution gives a k-disjunct\nfamily with positive probability, meaning that such a family must exist (otherwise the probability\nwould be zero).\n31in fact, our description for the motivation of Group Testing very much follows the description in [Dor43].\n\nLet 0 ≤p ≤1 and let A be a collection of n random (independently drawn) subsets of [T].\nThe distribution for a random set A is such that each t ∈[T] belongs to A with probability p (and\nindependently of the other elements).\nConsider k + 1 independent draws of this random variable, A0, . . . , Ak. The probability that A0 is\ncontained in the union of A1 through Ak is given by\nT\nPr [A0 ⊆(A1 ∪· · · ∪Ak)] =\n\n1 -p(1 -p)k\n.\nThis is minimized for p =\n\nk+1. For this choice of p, we have\n1 -p(1 -p)k = 1 -\nk + 1\n\n1 -\nk\nk + 1\n\nGiven that there are n such sets, there are (k + 1)\nn\ndifferent ways of picking a set and k\nk+1\nothers to test whether the first is contained in the union of the other k. Hence, using a union bound\nargument, the probability that A is k-disjunct can be bounded\n\nas\nPr[k -disjunct] ≥1 -( + 1)\n\nn\nk\nk\n\n+\n-k + 1\n\n1 -\n.\nk\n\nT\nk\n+ 1\n!\nIn order to show that one of the elements in A is k-disjunct we show that this probability is strictly\npositive. That is equivalent to\n\n1 -k + 1\n\n1 -\nk + 1\nk!T\n≤\n.\n(k + 1)\nn\nk+1\nNote that\n\n1 -\nk+1\nk\n→e-1\n1-\nk+1 = e-1 k+1\nk , as k →inf. Thus, we only need\nT ≥\nlog\n\n(k + 1)\nn\nk+1\n\n-log\n\n1 -\nk+1e-1 k+1\nk\n=\nlog\n\nk\nn\nk+1\n\n=\n-log (1 -(ek)-1)\nO(k2 log(n/k)),\nwhere the last inequality uses the fact that log\nn\nk+1\n\n= O\nk log\nn\ndue to Stirling's formula and\nk\nthe Taylor expansion -log(1 -x-1)-1 = O(x)\n\nThis argument simply shows the existence of a family satisfying the k-disjunt property. However,\nit is easy to see that by having T slightly larger one can ensure that the probability that the random\nfamily satisfies the desired property can be made very close to 1.\nRemarkably, the existence proof presented here is actually very close to the best known lower\nbound.\nTheorem 7.2 Given n and k, if there exists a family A of subsets of [T] satisfying the k-disjunct\nproperty, then\nT = Ω\nk2 log n\nlog k\n\n.\n\nProof.\nFix a u such that 0 < u <\nT ; later it will be fixed to u :=\nl\n(T\n-k)/\nk-1\n.\nm\nWe start by\nconstructing a few auxiliary family of sets. Let\nA0 = {A ∈A : |A| < u},\nand let A1 ⊆A denote the family of sets in A that contain their own unique u-subset,\nA1 :=\n\nA ∈A : ∃F ⊆A : |F| = u and, for all other A′ ∈A, F ⊆A′ .\nWe will procede by giving an upper bound to A0 ∪A1. For that, we will need a coup\n\nle of auxiliary\nfamily of sets. Let F denote the family of sets F in the definition of A1. More precisely,\nF := {F ∈[T] : |F| = u and ∃!A ∈A : F ⊆A} .\nBy construction |A1| ≤|F|\nAlso, let B be the family of subsets of [T] of size u that contain an element of A0,\nB = {B ⊆[T] : |B| = u and ∃A ∈A0 such that A ⊆B} .\nWe now prove that |A0| ≤|B|. Let B′ denote the family of subsets of [T] of size u that are not in\nB,\nB′ =\nB′ ⊆[T] : |B′| = u and B′ ∈/ B .\nBy construction of A0 and B, no\n\nset in B′ contains a set in A0 nor\n\ndoes a set in A0 contain a set\nin B′. Also, both A0 and B′ are antichains (or Sperner family), meaning that no pair of sets in each\nfamily contains each other. This implies that A0 ∪B′ is an antichain containing only sets with u or\nless elements. The Lubell-Yamamoto-Meshalkin inequality [Yam54] directly implies that (as long as\nu < T ) the largest antichain whose sets contain at most u elements is the family of subsets of [T] of\nsize u. This means that\n|A0| +\n\nT\nB′\n\n=\nA0 ∪B′ ≤\n\n=\nB\nB\nu\n∪\n′ = |B| +\nB′\nThis implies that A\n.\n\n.\n|\n0| ≤|B|\nBecause A satisfies the k-disjunct property, no two sets in A can contain eachother. This implies\nthat the families B and F of sets of size u are disjoint which implies that\nT\n|A0 ∪A1| = |A0| + |A1| ≤|B| + |F| ≤\n\nu\n\n.\nLet A2 := A \\ (A0 ∪A1). We want to show that if A ∈A2 and A1, . . . , Aj ∈A we have\n\nj\nA \\\n[\nAi\n> u(k\nj\ni=1\n-).\n(64)\nThis is readily shown by\n\\ Snoting that if (64) did not hold then one could find Bj+1, . . . , Bk subsets of\nj\nk\nA of size t such that A\ni=1 Ai ⊆S\ni=j+1 Bi. Since A as no unique subsets of size t there must exist\n\nk\nAj+1, . . . , Ak ∈A such that Bi ⊆Ai for i = j + 1, . . . , k. This would imply that A ⊆\ni=1 Ai which\nwould contradict the k-disjunct property.\nIf |A2| > k then we can take A0, A1, . . . , Ak distinct elements of A2.\nFor this choice\nS\nand any\nj = 0, . . . , k\n\nAj \\\n[\nAi\n1 + u(k\nj).\n≤i<j\n\n≥\n\n-\nThis means that\n\nk\nAj\nj=0\n\n[\n=\nj=0\nX\n,...,k\n\nAj \\\n≤\n[\nAi\ni<j\n\nX\nk + 1\n\n≥\n\n[1 + u(k -j)] = 1 + k + u\n\n.\nj=0,...,k\n\nSince all sets in A are subsets of [T] we must have 1 + k + u\nhand, taking\nk + 1\nk+1\n≤\nk\n\nS\nj=0 Aj\n≤T. On the other\nu :=\n\n(T -k)/\n\ngives a contradition (note that this choice of u is smaller than T as long as k > 2). This implies that\n|A2| ≤k which means that\nT\nT\nn = |A| = |A0| + |A1| + |A2| ≤k +\n\n= k +\nu\n\nl\n(T -k)/\nk+1\n\nThis means that\nm .\nlog n ≤log\n\nk +\n\n!\n\nl\nT\n-k)/\nk+1\n\nT\n= O\n(T\nm\nlog k\nk2\n\n,\nwhich concludes the proof of the theorem.\nWe essentially borrowed the proof of Theorem 7.2 from [Fur96]. We warn the reader however that\nthe notation in [Fur96] is drasticly different than ours, T corresponds to the number of people and n\nto the number of tests.\nThere is another upper bound, incomparable to the one in Theorem 7.1 that is known.\nTheorem 7.3 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nn\n= O\n\nk2\nlog\nT\nlog k\n!\n.\nThe proof of this Theorem uses ideas of Coding Theory (in particular Reed-Solomon codes) so we\nwill defer it for next section, after a crash course on coding theory.\nThe following Corollary follows immediately.\n\nCorollary 7.4 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nT = O\nk2 log n\nlog k\nmin\n\nlog k, log n\n.\nlog k\n\nWhile the upper bound in Corollary 7.4 and the lower bound in Theorem 7.2 are quite close, there\nis still a gap. This gap was recently closed and Theorem 7.2 was shown to be optimal [DVPS14]\n(original I was not aware of this reference and closing this gap was posed as an open problem).\nRemark 7.5 We note that the lower bounds established in Theorem 7.2 are not an artifact of the\nrequirement of the sets being k-disjunct. For the measurements taken in Group Testing to uniquely\ndetermine a group of k infected individuals it must be that the there are no two subfamilies of at most\nk sets in A that have the same union. If A is not k -1-disjunct then there exists a subfamily of k -1\nsets that contains another set A, which implies that the union of that subfamily is the same as the\nunion of the same subfamily together with A. This means that a measurement system that is able to\nuniquely determine a group of k infected individuals must be k -1-disjunct.\n7.2\nSome Coding Theory and the proof of Theorem 7.3\nIn this section we (very) briefly introduce error-correcting codes and use Reed-Solomon codes to prove\nTheorem 7.3. We direct the reader to [GRS15] for more on the subject.\nLets say Alice wants to send a message to Bob but they can only communicate through a channel\nthat erases or replaces some of the letters in Alice's message. If Alice and Bob are communicating with\nan alphabet Σ and can send messages with lenght N they can pre-decide a set of allowed messages\n(or codewords) such that even if a certain number of elements of the codeword gets erased or replaced\nthere is no risk for the codeword sent to be confused with another codeword. The set C of codewords\n(which is a subset of ΣN) is called the codebook and N is the blocklenght.\nIf every two codewords in the codebook differs in at least d coordinates, then there is no risk of\nconfusion with either up to d -1 erasures or up to ⌊d-1\n2 ⌋replacements. We will be interested in\ncodebooks that are a subset of a finite field, meanign that we will take Σ to be Fq for q a prime power\nand C to be a linear subspace of Fq.\nThe dimension of the code is given by\nm = logq |C|,\nand the rate of the code by\nm\nR =\n.\nN\nGiven two code words c1, c2 the Hamming distance ∆(c1, c2) is the number of entries where they\ndiffer. The distance of a code is defined as\nd =\nmin\n∆(c1, c2).\nc1=c2∈C\nFor linear codes, it is the same as the minimum weight\nω(C) =\nmin\n∆(c).\nc∈C\\{0}\n\nWe say that a linear code C is a [N, m, d]q code (where N is the blocklenght, m the dimension, d\nthe distance, and Fq the alphabet.\nOne of the main goals of the theory of error-correcting codes is to understand the possible values\nof rates, distance, and q for which codes exist. We simply briefly mention a few of the bounds and\nrefer the reader to [GRS15]. An important parameter is given by the entropy function:\nlog(q\n1)\nHq(x) = x\n-\nlog q\n-xlog x\nlog q -(1 -x)log(1 -x).\nlog q\n- Hamming bound follows essentially by noting that if a code has distance d then balls of radius\n⌊d-1\n2 ⌋centered at codewords cannot intersect. It says that\nR ≤1 -Hq\nd\n+\nN\n\no(1)\n- Another particularly simple bound is Singleton bound (it can be easily proven by noting that\nthe first n + d + 2 of two codewords need to differ in at least 2 coordinates)\nd\nR ≤1 -\n+ o(1).\nN\nThere are probabilistic constructions of codes that, for any ε > 0, satisfy\nR ≥1 -Hq\nd\nN\n\n-ε.\nThis means that R∗the best rate achievable satisties\nR∗≥1 -Hq\nd\n,\nN\n\n(65)\nknown as the GilbertVarshamov (GV) bound [Gil52, Var57]. Even for q = 2 (corresponding to binary\ncodes) it is not known whether this bound is tight or not, nor are there deterministic constructions\nachieving this Rate. This motivates the following problem.\nOpen Problem 7.1\n1. Construct an explicit (deterministic) binary code (q = 2) satisfying the\nGV bound (65).\n2. Is the GV bound tight for binary codes (q = 2)?\n7.2.1\nBoolean Classification\nA related problem is that of Boolean Classification [AABS15]. Let us restrict our attention to In\nerror-correcting codes one wants to build a linear codebook that does not contain a codeword with\nweight ≤d -1. In other words, one wants a linear codebook C that does intersect B(d -1) = {x ∈\n{0, 1}n : 0 < ∆(x) ≤d -1} the pinched Hamming ball of radius d (recall that ∆(d) is the Hamming\nweight of x, meaning the number of non-zero entries). In the Boolean Classification problem one is\nwilling to confuse two codewords as long as they are sufficiently close (as this is likely to mean they are\n\nin the same group, and so they are the same from the point of view of classification). The objective\nthen becomes understanding what is the largest possible rate of a codebook that avoids an Annulus\nA(a, b) = {x ∈{0, 1}n : a ≤∆(x) ≤b}. We refer the reader to [AABS15] for more details. Let us call\nthat rate\nRA\n∗(a, b, n).\nNote that RA\n∗(1, d-1, n) corresponds to the optimal rate for a binary error-correcting code, conjectured\nto be 1 -H\nd\nq\n(The\nN\n\nGV bound).\nOpen Problem 7.2 It is conjectured in [AABS15] (Conjecture 3 in [AABS15]) that the optimal rate\nin this case is given by\nRA\n∗(αn, βn, n) = α + (1 -α) RA\n∗(1, βn, (1 -α)) + o(1),\nwhere o(1) goes to zero as n goes to infinity.\nThis is established in [AABS15] for β ≥2α but open in general.\n7.2.2\nThe proof of Theorem 7.3\nReed-Solomon codes[RS60] are [n, m, n -m + 1]q codes, for m ≤n ≤q. They meet the Singleton\nbound, the drawback is that they have very large q (q > n).\nWe'll use their existence to prove\nTheorem 7.3\nProof. [of Theorem 7.3]\nWe will construct a family A of sets achieving the upper bound in Theorem 7.3. We will do this\nby using a Reed-Solomon code [q, m, q -m+1]q. This code has qm codewords. To each codework c we\nwill correspond a binary vector a of length q2 where the i-th q-block of a is the indicator of the value\nof c(i). This means that a is a vector with exactly q ones (and a total of q2 entries)32. We construct\nthe family A for T = q2 and n = qm (meaning qm subsets of q2 ) by constructing, for each codeword\nc, the set of non-zero entries of the corresponding binary vector a.\nThese sets have the following properties,\n\nmin\nj∈[n] |Aj| = q,\nand\nmax\n|Aj\n\n∈\nAj2 = q\nmin\n∆(c1, c2)\nq\n(q\nm + 1) = m\n1.\nj1=j2 [n]\n∩\n|\n-\nc1=\n\nc2∈C\n≤\n-\n-\n-\nThis readily implies that A is k-disjunct for\nk =\nq -1\n,\nm -1\n\nbecause the union of\nj\nq-1\nm-1\nk\nsets can only contain (m -1)\nj\nq-1\nm-1\nk\n< q elements of another set.\nNow we pick q ≈2k log n (q has to be a prime but there is always a prime between this number and\nlog k\nits double by Bertrand's postulate (see [?] for a particularly nice proof)). Then m = log n (it can be\nlog q\ntaken to be the ceiling of this quantity and then n gets updated accordingly by adding dummy sets).\n32This is precisely the idea of code concatenation [GRS15]\n\nThis would give us a family (for large enough parameters) that is k-disjunct for\nq -1\nm -1\n\n≥\n$\n2k log n\nlog k -1\nlog n\nlog q + 1 -1\n%\n=\n\n2k log q\nlog q\nlog k -log n\n\n≥\nk.\nNoting that\nT ≈\n\nlog n\n2k\nlog k\n\n.\nconcludes the proof.\n7.3\nIn terms of linear Bernoulli algebra\nWe can describe the process above in terms of something similar to a sparse linear system. let 1Ai be\nthe t -dimensional indicator vector of Ai, 1i:n be the (unknown) n-dimensional vector of infected\nsoldiers and 1t:T the T-dimensional vector of infected (positive) tests. Then\n|\n|\n1A\n1A\n\n· · ·\nn\n\n|\n|\n⊗\n|\n\n|\n=\n|\n\n,\nwhere\nis\n\ni:n\n\n|\n\nt:T\n|\n|\n⊗\nmatrix-vector multiplication in the Bernoulli algebra, basically the only thing that is\ndifferent from the standard matrix-vector multiplications is that the addition operation is replaced by\nbinary \"or\", meaning 1 ⊕1 = 1.\nThis means that we are essentially solving a linear system (with this non-standard multiplication).\nSince the number of rows is T = O(k2 log(n/k)) and the number or columns n ≫T the system is\nunderdetermined. Note that the unknown vector, 1i:n has only k non-zero components, meaning it\nis k-sparse. Interestingly, despite the similarities with the setting of sparse recovery discussed in a\n\nprevious lecture, in this case, O(k2) measurements are needed, instead of O(k) as in the setting of\nCompressed Sensing.\n7.3.1\nShannon Capacity\nThe goal Shannon Capacity is to measure the amount of information that can be sent through a noisy\nchannel where some pairs of messages may be confused with eachother. Given a graph G (called\nthe confusion graph) whose vertices correspond to messages and edges correspond to messages that\nmay be confused with each other. A good example is the following: say one has a alphabet of five\nsymbols 1, 2, 3, 4, 5 and that each digit can be confused with the immediately before and after (and\n1 and 5 can be confused with eachother). The confusion graph in this case is C5, the cyclic graph\n\non 5 nodes.\nIt is easy to see that one can at most send two messages of one digit each without\nconfusion, this corresponds to the independence number of C5, α(C5) = 2. The interesting question\narises when asking how many different words of two digits can be sent, it is clear that one can send at\nleast α(C5)2 = 4 but the remarkable fact is that one can send 5 (for example: \"11\",\"23\",\"35\",\"54\",\nor \"42\"). The confusion graph for the set of two digit words C⊕2\ncan be described by a product of\nthe original graph C5 where for a graph G on n nodes G⊕2 is a graph on n nodes where the vertices\nare indexed by pairs ij of vertices of G and\n(ij, kl) ∈E G⊕2\nif both a) i = k or i, k\n=\n\n∈E and b) j\nl or j, l\n∈E hold.\nThe above observation can then be written as α C⊕2\n= 5.\nThis motivates the definition of\nShannon Capacity [Sha56]\n\nθS (G) sup\nk\n\nG⊕k 1\nk .\nLovasz, in a remarkable paper [Lov79], showed that θS (C5) =\n√\n5, but determining this quantity is\nan open problem for many graphs of interested [AL06], including C7.\nOpen Problem 7.3 What is the Shannon Capacity of the 7 cycle?\n7.3.2\nThe deletion channel\nIn many applications the erasures or errors suffered by the messages when sent through a channel are\nrandom, and not adversarial. There is a beautiful theory understanding the amount of information\nthat can be sent by different types of noisy channels, we refer the reader to [CT] and references therein\nfor more information.\nA particularly challenging channel to understand is the deletion channel.\nThe following open\nproblem will envolve a particular version of it. Say we have to send a binary string \"10010\" through\na deletion channel and the first and second bits get deleted, then the message receive would be \"010\"\nand the receiver would not know which bits were deleted. This is in contrast with the erasure channel\nwhere bits are erased but the receiver knows which bits are missing (in the case above the message\nreceived would be \"??010\"). We refer the reader to this survey on many of the interesting questions\n(and results) regarding the Deletion channel [Mit09].\nA particularly interesting instance of the problem is the Trace Reconstruction problem, where the\nsame message is sent multiple times and the goal of the receiver is to find exactly the original message\nsent from the many observed corrupted version of it. We will be interested in the following quantity:\nDraw a random binary stringwithn bits, suppose the channel has a deletion probability of 1 for each\nbit (independently), define D n; 1\nhas the number of times the receiver needs to receive the message\n(with independent corruptions)\n\nso that she can decode the message exactly, with high probability.\nIt is easy to see that D n; 1\n≤2n, since roughly once in every 2n times the whole message will go\n√\nthrough the channel unharmed. It is possible to show (see [HMPW]) that D\nn; 1\nknown whether this bound is tight.\n\n≤2\nn but it is not\nOpen Problem 7.4\n1. What are the asymptotics of D\nn; 1\n\n?\n\n2. An interesting aspect of the Deletion Channel is that different messages may have different\ndifficulties of decoding.\nThis motivates the following question: What are the two (distinct)\nbinary sequences x(2) and x(2) that are more difficult to distinguish (let's say that the receiver\nknows that either x(1) or x(2) was sent but not which)?\n\nApproximation Algorithms and Max-Cut\n8.1\nThe Max-Cut problem\nUnless the widely believed P = NP conjecture is false, there is no polynomial algorithm that can\nsolve all instances of an NP-hard problem. Thus, when faced with an NP-hard problem (such as the\nMax-Cut problem discussed below) one has three options: to use an exponential type algorithm that\nsolves exactly the problem in all instances, to design polynomial time algorithms that only work for\nsome of the instances (hopefully the typical ones!), or to design polynomial algorithms that, in any\ninstance, produce guaranteed approximate solutions. This section is about the third option. The\nsecond is discussed in later in the course, in the context of community detection.\nThe Max-Cut problem is the following: Given a graph G = (V, E) with non-negative weights wij on\nthe edges, find a set S ⊂V for which cut(S) is maximal. Goemans and Williamson [GW95] introduced\nan approximation algorithm that runs in polynomial time and has a randomized component to it, and\nis able to obtain a cut whose expected value is guaranteed to be no smaller than a particular constant\nαGW times the optimum cut. The constant αGW is referred to as the approximation ratio.\nLet V = {1, . . . , n}. One can restate Max-Cut as\nmax\ni\nP\ni<j wij(1 -y yj)\n(66)\ns.t.\n|yi| = 1\nThe yi's are binary variables that indicate set membership, i.e., yi = 1 if i ∈S and yi = -1 otherwise.\nConsider the following relaxation of this problem:\nmax\nw\nP\ni<j\nij(1 -uT\ni uj)\ns.t.\nui ∈Rn\n(67)\n, ∥ui∥= 1.\nThis is in fact a relaxation because if we restrict ui to be a multiple of e1, the first element of the\ncanonical basis, then (77) is equivalent to (66). For this to be a useful approach, the following two\nproperties should hold:\n(a) Problem (77) needs to be easy to solve.\n(b) The solution of (77) needs to be, in some way, related to the solution of (66).\nDefinition 8.1 Given a graph G, we define MaxCut(G) as the optimal value of (66) and RMaxCut(G)\nas the optimal value of (77).\nWe start with property (a). Set X to be the Gram matrix of u1, . . . , un, that is, X = UT U where\nthe i'th column of U is ui. We can rewrite the objective function of the relaxed problem as\nij\nX\nwij(1\nX\ni<j\n-\n)\nOne can exploit the fact that X having a decomposition of the form X = Y T Y is equivalent to being\npositive semidefinite, denoted X ⪰0. The set of PSD matrices is a convex set. Also, the constraint\n\n∥ui∥= 1 can be expressed as Xii = 1. This means that the relaxed problem is equivalent to the\nfollowing semidefinite program (SDP):\nmax\nX\nP\ni<j wij(1 -\nij)\n(68)\ns.t.\nX ⪰0 and Xii = 1, i = 1, . . . , n.\nThis SDP can be solved (up to ε accuracy) in time polynomial on the input size and log(ε-1)[VB96].\nThere is an alternative way of viewing (68) as a relaxation of (66). By taking X = yyT one can\nformulate a problem equivalent to (66)\nmax\nij\nP\ni<j wij(1 -X )\n(69)\ns.t.\nX ⪰0 , Xii = 1, i = 1, . . . , n, and rank(X) = 1.\nThe SDP (68) can be regarded as a relaxation of (69) obtained by removing the non-convex rank\nconstraint. In fact, this is how we will later formulate a similar relaxation for the minimum bisection\nproblem.\nWe now turn to property (b), and consider the problem of forming a solution to (66) from a\nsolution to (68). From the solution {ui}i=1,...,n of the relaxed problem (68), we produce a cut subset\nS′ by randomly picking a vector r ∈Rn from the uniform distribution on the unit sphere and setting\nS′ = {i|rT ui ≥0}\nIn other words, we separate the vectors u1, . . . , un by a random hyperplane (perpendicular to r). We\nwill show that the cut given by the set S′ is comparable to the optimal one.\nFigure 20: θ = arccos(uT\ni uj)\nLet W be the value of the cut produced by the procedure described above. Note that W is a\nrandom variable, whose expectation is easily seen (see Figure 20) to be given by\nE[W]\n=\nX\nwij Pr sign(rT ui) = sign(rT uj)\ni<j\n\nX\n\n=\nwij\ni<j\narccos(uT\nπ\ni uj).\n\nIf we define αGW as\nαGW =\nmin\n-1≤x≤1\nπ arccos(x)\n2(1 -x) ,\nit can be shown that αGW > 0.87.\nIt is then clear that\nE[W] =\nX\nwij\narccos(uT\ni uj)\nα\nπ\n≥\nGW 2\ni<j\nX\nwij(1 -uT\ni uj).\n(70)\ni<j\nLet MaxCut(G) be the maximum cut of G, meaning the maximum of the original problem (66).\nNaturally, the optimal value of (77) is larger or equal than MaxCut(G). Hence, an algorithm that\nsolves (77) and uses the random rounding procedure described above produces a cut W satisfying\nMaxCut(G) ≥E[W] ≥αGW\nX\nwij(1 -uT\ni uj)\n2 i<j\n≥αGW MaxCut(G),\n(71)\nthus having an approximation ratio (in expectation) of αGW . Note that one can run the randomized\nrounding procedure several times and select the best cut.\nNote that the above gives\nMaxCut(G) ≥E[W] ≥αGW RMaxCut(G) ≥αGW MaxCut(G)\n8.2\nCan αGW be improved?\nA natural question is to ask whether there exists a polynomial time algorithm that has an approxi-\nmation ratio better than αGW .\nFigure 21: The Unique Games Problem\nThe unique games problem (as depicted in Figure 21) is the following: Given a graph and a set\nof k colors, and, for each edge, a matching between the colors, the goal in the unique games problem\nis to color the vertices as to agree with as high of a fraction of the edge matchings as possible. For\nexample, in Figure 21 the coloring agrees with 3 of the edge constraints, and it is easy to see that one\ncannot do better.\nThe Unique Games Conjecture of Khot [Kho02], has played a major role in hardness of approxi-\nmation results.\n(c) Thore Husfeldt. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nConjecture 8.2 For any ε > 0, the problem of distinguishing whether an instance of the Unique\nGames Problem is such that it is possible to agree with a ≥1 -ε fraction of the constraints or it is\nnot possible to even agree with a ε fraction of them, is NP-hard.\nThere is a sub-exponential time algorithm capable of distinguishing such instances of the unique\ngames problem [ABS10], however no polynomial time algorithm has been found so far. At the moment\none of the strongest candidates to break the Unique Games Conjecture is a relaxation based on the\nSum-of-squares hierarchy that we will discuss below.\nOpen Problem 8.1 Is the Unique Games conjecture true?\nIn particular, can it be refuted by a\nconstant degree Sum-of-squares relaxation?\nRemarkably, approximating Max-Cut with an approximation ratio better than αGW is has hard\nas refuting the Unique Games Conjecture (UG-hard) [KKMO05].\nMore generality, if the Unique\nGames Conjecture is true, the semidefinite programming approach described above produces optimal\napproximation ratios for a large class of problems [Rag08].\nNot depending on the Unique Games Conjecture, there is a NP-hardness of approximation of 16\nfor Max-Cut [Has02].\nRemark 8.3 Note that a simple greedy method that assigns membership to each vertex as to maximize\nthe number of edges cut involving vertices already assigned achieves an approximation ratio of 1\n2 (even\nof 1 of the total number of edges, not just of the optimal cut).\n8.3\nA Sums-of-Squares interpretation\nWe now give a different interpretation to the approximation ratio obtained above. Let us first slightly\nreformulate the problem (recall that wii = 0).\nmax\nyi=±1 2\nX\ni<j\nwij(1 -yiyj)\n=\nmax\nyi=±1\nX\ni,j\nwij(1 -yiyj)\ni\nj\n=\nmax\nwij\n-yiyj\nyi=±1 4\ni,j\n=\nmax\n\n-\nX\nw y y +\nw\ny2\nij i j\nij\n+\nwij\ny2\nyi=±1 4\nj\ni,j\nj\n\"\n\ni\ni\n\nX\nX\n\nX\nj\nX\ni\n#\n\n=\nmax\nyi=±1 4\n\n-\nX\nw\nijyiyj + 2\nX\ndeg(i)y2\ni + 2\nX\ndeg(j)yj\ni,j\ni\nj\n\n=\nmax\nw\nijyiyj +\ndeg(i)yi\nyi=±1 4\n\n-\nX\ni,j\nX\ni\n\n=\nmax\nyT L\n,\n\nGy\nyi=±1 4\nX\n\ny2 + y2\n!\n\nwhere LG = DG -W is the Laplacian matrix, DG is a diagonal matrix with (DG)ii = deg(i) = P\nj wij\nand Wij = wij.\nThis means that we rewrite (66) as\nmax\n1yT LGy\n(72)\nyi = ±1, i = 1, . . . , n.\nSimilarly, (68) can be written (by taking X = yyT ) as\nmax\n1 Tr (LGX)\ns.t.\nX ⪰0\n(73)\nXii = 1, i = 1, . . . , n.\nIndeed, given\nNext lecture we derive the formulation of the dual program to (73) in the context of recovery in\nthe Stochastic Block Model. Here we will simply show weak duality. The dual is given by\nmin\nTr (D)\ns.t.\nD is a diagonal matrix\nD -1\n4LG ⪰0.\n(74)\nIndeed, if X is a feasible solution to (73) and D a feasible solution to (74) then, since X and\nD -1LG are both positive semidefinite Tr\n\nX\nD -1\n4LG\n\n≥0 which gives\n0 ≤Tr\n\nX\n\nD -1\n4LG\n\n= Tr(XD) -1\n4 Tr (LGX) = Tr(D) -1 Tr (LGX) ,\nsince D is diagonal and Xii = 1. This shows weak duality, the fact that the value of (74) is larger\nthan the one of (73).\nIf certain conditions, the so called Slater conditions [VB04, VB96], are satisfied then the optimal\nvalues of both programs are known to coincide, this is known as strong duality. In this case, the\nSlater conditions ask whether there is a matrix strictly positive definite that is feasible for (73) and\nthe identity is such a matrix. This means that there exists D♮feasible for (74) such that\nTr(D♮) = RMaxCut.\nHence, for any y ∈Rn we have\n4yT LGy = RMaxCut -yT\n\nD♮-1\nD\nT\nn\nLG\n+\nX\nii\ni=1\ny2\ni -1\n\n.\n(75)\nNote that (75) certifies that no cut of G is larger than RMaxCut. Indeed, if y ∈{±1}2 then y2\ni = 1\nand so\nRMaxCut -4yT LGy = yT\n\nD♮-1\n4LG\nT\n.\n\nSince D♮-1\n4LG ⪰0, there exists V such that D♮-1LG = V V T with the columns of V denoted by\nv1, . . . , vn. This means that meaning that yT D♮-1\n4LG\nT =\nV T y\n2 = Pn\nk=1(vT\nk y)2. This means\nthat, for y ∈{±1}2,\nRMaxCut -1\nX\nn\nyT LGy =\n(vT\nk y)2.\nk=1\nIn other words, RMaxCut -\nyT LGy is, in the hypercube (y\n1 2) a sum-of-squares of degree 2.\n∈{± }\nThis is known as a sum-of-squares certificate [BS14, Bar14, Par00, Las01, Sho87, Nes00]; indeed, if a\npolynomial is a sum-of-squares naturally it is non-negative.\nNote that, by definition, MaxCut -1yT LGy is always non-negative on the hypercube. This does\nnot mean, however, that it needs to be a sum-of-squares33 of degree 2.\n(A Disclaimer: the next couple of paragraphs are a bit hand-wavy, they contain some of intuition\nfor the Sum-of-squares hierarchy but for details and actual formulations, please see the references.)\nThe remarkable fact is that, if one bounds the degree of the sum-of-squares certificate, it can be\nfound using Semidefinite programming [Par00, Las01]. In fact, SDPs (74) and (74) are finding the\nsmallest real number Λ such that Λ -1yT LGy is a sum-of-squares of degree 2 over the hypercube,\nthe dual SDP is finding a certificate as in (75) and the primal is constraining the moments of degree\n2 of y of the form Xij = yiyj (see [Bar14] for some nice lecture notes on Sum-of-Squares, see also\nRemark 8.4). This raises a natural question of whether, by allowing a sum-of-squares certificate of\ndegree 4 (which corresponds to another, larger, SDP that involves all monomials of degree ≤4 [Bar14])\none can improve the approximation of αGW to Max-Cut. Remarkably this is open.\nOpen Problem 8.2\n1. What is the approximation ratio achieved by (or the integrality gap of) the\nSum-of-squares degree 4 relaxation of the Max-Cut problem?\n2. The relaxation described above (of degree 2) (74) is also known to produce a cut of 1 -O (√ε)\nwhen a cut of 1 -ε exists. Can the degree 4 relaxation improve over this?\n3. What about other (constant) degree relaxations?\nRemark 8.4 (triangular inequalities and Sum of squares level 4) A (simpler) natural ques-\ntion is wether the relaxation of degree 4 is actually strictly tighter than the one of degree 2 for Max-Cut\n(in the sense of forcing extra constraints). What follows is an interesting set of inequalities that degree\n4 enforces and that degree 2 doesn't, known as triangular inequalities.\nSince yi ∈{±1} we naturally have that, for all i, j, k\nyiyj + yjyk + ykyi ≥-1,\nthis would mean that, for Xij = yiyj we would have,\nXij + Xjk + Xik ≥-1,\nhowever it is not difficult to see that the SDP (73) of degree 2 is only able to constraint\nXij + Xjk + Xik ≥-2,\n33This is related with Hilbert's 17th problem [Sch12] and Stengle's Positivstellensatz [Ste74]\n\nwhich is considerably weaker. There are a few different ways of thinking about this, one is that the\nthree vector ui, uj, uk in the relaxation may be at an angle of 120 degrees with each other. Another way\nof thinking about this is that the inequality yiyj +y y\nj k +ykyi ≥-2 can be proven using sum-of-squares\nproof with degree 2:\n(yi + yj + yk)2 ≥0\n⇒\nyiyj + yjyk + ykyi ≥-3\nHowever, the stronger constraint cannot.\nOn the other hand, if degree 4 monomials are involved, (let's say XS = Q\ns S ys, note that X = 1\n∈\n∅\nand XijXik = Xjk) then the constraint\nX∅\nXij\n\nT\nX\nX\nX\n\n∅\nij\njk\nXki\nXij\n\nXij\nXik\nX\n\njk\n=\nXjk\nXjk\nXjk\nXik\nXij\n⪰\nXki\n\nXki\n\nXki\nXjk\nXij\n\nimplies X\n\nij + Xjk + Xik ≥-1 just by taking\n\nX\n\nij\nXjk\nXki\n1T X\n\nij\nXik\nXjk\n0.\nXjk\nXik\nXij\n\n≥\nXki\nXjk\nXij\n\nAlso, note that the inequality yiyj + yjyk + ykyi ≥-1 can inde\n\ned be proven using sum-of-squares proof\nwith degree 4 (recall that y2\ni = 1):\n(1 + yiyj + yjyk + ykyi)2 ≥0\n⇒\nyiyj + yjyk + ykyi ≥-1.\nInterestingly, it is known [KV13] that these extra inequalities alone will not increase the approximation\npower (in the worst case) of (68).\n8.4\nThe Grothendieck Constant\nThere is a somewhat similar remarkable problem, known as the Grothendieck problem [AN04, AMMN05].\nGiven a matrix A ∈Rn×m the goal is to maximize\nmax\nxT Ay\ns.t.\nxi = ±, ∀i\n(76)\ns.t.\nyj = ±, ∀j\nNote that this is similar to problem (66). In fact, if A ⪰0 it is not difficult to see that the optimal\nsolution of (76) satisfies y = x and so if A = LG, since LG ⪰0, (76) reduces to (66). In fact, when\nA ⪰0 this problem is known as the little Grothendieck problem [AN04, CW04, BKS13a].\nEven when A is not positive semidefinite, one can take zT = [xT yT ] and the objective can be\nwritten as\nzT\n\nA\nAT\n\nz.\n\nSimilarly to the approximation ratio in Max-Cut, the Grothendieck constant [Pis11] KG is the\nmaximum ratio (over all matrices A) between the SDP relaxation\nmax\nP\nij AijuT\ni vj\ns.t.\nui ∈Rn+m,\n(77)\nn\n∥ui∥= 1,\nvj ∈R +m, ∥vj∥= 1\nand 76, and its exact value is still unknown, the best known bounds are available here [] and are 1.676 <\nKG <\nπ\n2 log(1+\n√. See also page 21 here [F+14]. There is also a complex valued analogue [Haa87].\n2)\nOpen Problem 8.3 What is the real Grothendieck constant KG?\n8.5\nThe Paley Graph\nLet p be a prime such that p ∼= 1 mod 4. The Paley graph of order p is a graph on p nodes (each\nnode associated with an element of Zp) where (i, j) is an edge if i -j is a quadratic residue modulo\np. In other words, (i, j) is an edge is there exists a such that a2 ∼= i -j mod p. Let ω(p) denote the\nclique number of the Paley graph of order p, meaning the size of its largest clique. It is conjectured\nthat ω(p) ≲pollywog(n) but the best known bound is ω(p)\n√\n≤\np (which can be easily obtained). The\nonly improvement to date is that, infinitely often, ω(p) ≤√p -1, see [BRM13].\nThe theta function of a graph is a Semidefinite programming based relaxation of the independence\nnumber [Lov79] (which is the clique number of the complement graph). As such, it provides an upper\nbound on the clique number. In fact, this upper bound for Paley graph matches ω(p)\n√\n≤\np.\nSimilarly to the situation above, one can define a degree 4 sum-of-squares analogue to θ(G) that, in\nprinciple, has the potential to giving better upper bounds. Indeed, numerical experiments in [GLV07]\nseem to suggest that this approach has the potential to improve on the upper bound ω(p)\n√\n≤\np\nOpen Problem 8.4 What are the asymptotics of the Paley Graph clique number ω(p) ? Can the the\nSOS degree 4 analogue of the theta number help upper bound it? 34\nInterestingly, a polynomial improvement on Open Problem 6.4. is known to imply an improvement\non this problem [BMM14].\n8.6\nAn interesting conjecture regarding cuts and bisections\nGiven d and n let Greg(n, d) be a random d-regular graph on n nodes, drawn from the uniform\ndistribution on all such graphs. An interesting question is to understand the typical value of the\nMax-Cut such a graph. The next open problem is going to involve a similar quantity, the Maximum\nBisection. Let n be even, the Maximum Bisection of a graph G on n nodes is\nMaxBis(G) =\nmax\nS: |S|= n cut(S),\nand the related Minimum Bisection (which will play an important role in next lectures), is given by\nMinBis(G) =\nmin\nS: |S|= n\ncut(S),\n34The author thanks Dustin G. Mixon for suggesting this problem.\n\nA typical bisection will cut half the edges, meaning dn. It is not surprising that, for large n,\nMaxBis(G) and MinBis(G) will both fluctuate around this value, the amazing conjecture [ZB09] is\nthat their fluctuations are the same.\nConjecture 8.5 ([ZB09]) Let G ∼Greg(n, d), then for all d, as n grows\nn (MaxBis(G) + MinBis(G)) = d + o(1),\nwhere o(1) is a term that goes to zero with n.\nOpen Problem 8.5 Prove or disprove Conjecture 8.5.\nRecently, it was shown that the conjecture holds up to o(\n√\nd) terms [DMS15]. We also point the\nreader to this paper [Lyo14], that contains bounds that are meaningful already for d = 3.\n\nCommunity detection and the Stochastic Block Model\n9.1\nCommunity Detection\nCommunity detection in a network is a central problem in data science. A few lectures ago we discussed\nclustering and gave a performance guarantee for spectral clustering (based on Cheeger's Inequality)\nthat was guaranteed to hold for any graph. While these guarantees are remarkable, they are worst-case\nguarantees and hence pessimistic in nature. In what follows we analyze the performance of a convex\nrelaxation based algorithm on typical instances of the community detection problem (where typical is\ndefined through some natural distribution of the input).\nWe focus on the problem of minimum graph bisection. The objective is to partition a graph in\ntwo equal-sized disjoint sets (S, Sc) while minimizing cut(S) (note that in the previous lecture, for the\nMax-Cut problem, we were maximizing it instead!).\n9.2\nStochastic Block Model\nWe consider a random graph model that produces graphs that have a clustering structure. Let n be\nan even positive integer. Given two sets of m = n nodes consider the following random graph G: For\neach pair (i, j) of nodes, (i, j) is an edge of G with probability p if i and j are in the same set, and\nwith probability q if they are in different sets. Each edge is drawn independently and p > q. This is\nknown as the Stochastic Block Model on two communities.\n(Think of nodes as habitants of two different towns and edges representing friendships, in this\nmodel, people leaving in the same town are more likely to be friends)\nThe goal will be to recover the original partition. This problem is clearly easy if p = 1 and q = 0\nand hopeless if p = q. The question we will try to answer is for which values of p and q is it possible\nto recover the partition (perhaps with high probability). As p > q, we will try to recover the original\npartition by attempting to find the minimum bisection of the graph.\n9.3\nWhat does the spike model suggest?\nMotivated by what we saw in previous lectures, one approach could be to use a form of spectral\nclustering to attempt to partition the graph.\nLet A be the adjacency matrix of G, meaning that\nAij =\nif (i, j) ∈E(G)\n(78)\notherwise.\nNote that in our model, A is a random matrix. We would like to solve\nmax\ns.t. x\nX\nAijxixj\ni,j\nX\ni = ±1, ∀i\n(79)\nxj = 0,\nj\nThe intended solution x takes the value +1 in one cluster and -1 in the other.\n\nRelaxing the condition xi = ±1, ∀i to ∥x∥2\n2 = n would yield a spectral method\nmax\nX\nAijxixj\ni,j\ns.t.\nx\n√\n∥∥2 =\nn\n(80)\n1T x = 0\nThe solution consists of taking the top eigenvector of the projection of A on the orthogonal of the\nall-ones vector 1.\nThe matrix A is a random matrix whose expectation is given by\np\nif (i, j)\nE[A] =\n∈E(G)\nq\notherwise.\nLet g denote a vector that is +1 in one of the clusters and -1 in the other (note that this is the vector\nwe are trying to find!). Then we can write\np + q\nE[A] =\n11T + p -qggT ,\nand\nA =\nA -E[A]\n\np + q\n+\n11T + p -qggT .\np+q\nIn order to remove the term\n2 11T we consider the random matrix\nA = A -p + q11T .\nIt is easy to see that\nA =\nA -E[A]\n\np\n+\n-qggT .\nThis means that A is a superposition of a random matrix whose expected value is zero and a rank-1\nmatrix, i.e.\nA = W + λvvT\nwhere W =\nA -E[A]\n\nq\nλvvT\np\nand\n=\n-\n2 n\n\ng\n√n\n\ng\n√\nT\n. In previous lectures we saw that for large\nn\nenough λ, the eigenvalue associated with λ pops outside\n\nthe distribution of eigenvalues of W and\nwhenever this happens, the leading eigenvector has a non-trivial correlation with g (the eigenvector\nassociated with λ).\nNote that since to obtain A we simply subtracted a multiple of 11T from A, problem (80) is\nequivalent to\nmax\nX\ni,j\nAijxixj\ns.t. ∥x∥2 = √n\n(81)\n1T x = 0\n\nNow that we removed a suitable multiple of 11T we will even drop the constraint 1T x = 0, yielding\nmax\nX\ni,j\nAijxixj\ns.t. ∥x∥2 = √n,\n(82)\nwhose solution is the top eigenvector of A.\nRecall that if A -E[A] was a Wigner matrix with i.i.d entries with zero mean and variance σ2\nthen its empirical spectral density would follow the semicircle law and it will essentially be supported\nin [-2σ√n, 2σ√n]. We would then expect the top eigenvector of A to correlate with g as long as\np -q\nn > 2σ√n.\n(83)\nUnfortunately A-E[A] is not a Wigner matrix in general. In fact, half of its entries have variance\np(1 -p) while the variance of the other half is q(1\nq).\np(1\nere\n-p)+q(1\n)\n-\nIf we w\nto take σ =\n-q and use (83) it would suggest that the leading eigenvector of\nA correlates with the true partition vector g as long as\np -q\n>\n√n\nr\np(1 -p) + q(1 -q),\n(84)\nHowever, this argument is not necessarily valid because the matrix is not a Wigner matrix. For the\nspecial case q = 1 -p, all entries of A -E[A] have the same variance and they can be made to\nbe identically distributed by conjugating with ggT . This is still an impressive result, it says that if\np = 1 -q then p -q needs only to be around\n√\nto be able to make an estimate that correlates with\nn\nthe original partitioning!\nAn interesting regime (motivated by friendship networks in social sciences) is when the average\ndegree of each node is constant. This can be achieved by taking p = a\nn and q = b for constants a and\nn\nb. While the argument presented to justify condition (84) is not valid in this setting, it nevertheless\nsuggests that the condition on a and b needed to be able to make an estimate that correlates with the\noriginal partition is\n(a -b)2 > 2(a + b).\n(85)\nRemarkably this was posed as conjecture by Decelle et al. [DKMZ11] and proved in a series of\nworks by Mossel et al. [MNS14b, MNS14a] and Massoulie [Mas14].\n9.3.1\nThree of more communities\nThe stochastic block model can be similarly defined for any k ≥2 communities: G is a graph on\nn = km nodes divided on k groups of m nodes each. Similarly to the k = 2 case, for each pair (i, j) of\nnodes, (i, j) is an edge of G with probability p if i and j are in the same set, and with probability q if\nthey are in different sets. Each edge is drawn independently and p > q. In the sparse regime, p = a\nn\nand q = b , the threshold at which it is possible to make an estimate that correlates with the original\nn\npartition is open.\n\nOpen Problem 9.1 Consider the balanced Stochastic Block Model for k > 3 (constant) communities\nwith inner probability p = a\nn and outer probability q = b , what is the threshold at which it becomes\nn\npossible to make an estimate that correlates with the original partition is open (known as the par-\ntial recovery or detection threshold). We refer the reader to [DKMZ11, ZMZ14, GZC+15] for more\ninformation on this and many other interesting conjectures often motivated from statistical physics.\n9.4\nExact recovery\nWe now turn our attention to the problem of recovering the cluster membership of every single node\ncorrectly, not simply having an estimate that correlates with the true labels. We'll restrict to two\ncommunities for now. If the probability of intra-cluster edges is p = a then it is not hard to show that\nn\neach cluster will have isolated nodes making it impossible to recover the membership of every possible\nnode correctly. In fact this is the case whenever p ≪2 log n\nn\n. For that reason we focus on the regime\np = α log(n)\nn\nand q = β log(n),\n(86)\nn\nfor some constants α > β.\nLet x ∈Rn with xi = ±1 representing the partition (note there is an ambiguity in the sense that\nx and -x represent the same partition). Then, if we did not worry about efficiency then our guess\n(which corresponds to the Maximum Likelihood Estimator) would be the solution of the minimum\nbissection problem (79).\nIn fact, one can show (but this will not be the main focus of this lecture, see [ABH14] for a proof)\nthat if\n√α -\np\nβ >\n√\n2,\n(87)\nthen, with high probability, (79) recovers the true partition. Moreover, if\n√α -\np\nβ <\n√\n2,\nno algorithm (efficient or not) can, with high probability, recover the true partition.\nWe'll consider a semidefinite programming relaxation algorithm for SBM and derive conditions for\nexact recovery. The main ingredient for the proof will be duality theory.\n9.5\nThe algorithm\nNote that if we remove the constraint that\nj xj = 0 in (79) then the optimal solution becomes x = 1.\nLet us define B = 2A -(11T -I), meaning\nP\nthat\nBij =\n\nif i = j\n\nif (i, j) ∈E(G)\n(88)\n-1\notherwise\nIt is clear that the problem\n\nmax\nX\nBijxixj\ni,j\ns.t. x\nX\ni = ±1, ∀i\n(89)\nxj = 0\nj\nhas the same solution as (79). However, when the constraint is dropped,\nmax\nX\nBijxixj\ni,j\ns.t. xi = ±1, ∀i,\n(90)\nx = 1 is no longer an optimal solution. Intuitively, there is enough \"-1\" contribution to discourage\nunbalanced partitions. In fact, (90) is the problem we'll set ourselves to solve.\nUnfortunately (90) is in general NP-hard (one can encode, for example, Max-Cut by picking an\nappropriate B). We will relax it to an easier problem by the same technique used to approximate the\nMax-Cut problem in the previous section (this technique is often known as matrix lifting). If we write\nX = xxT then we can formulate the objective of (90) as\nX\nBijxixj = xT Bx = Tr(xT Bx) = Tr(BxxT ) = Tr(BX)\ni,j\nAlso, the condition xi = ±1 implies Xii = x2\ni = 1. This means that (90) is equivalent to\nmax\nTr(BX)\ns.t.\nXii = 1, ∀i\n(91)\nX = xxT for some x ∈Rn.\nThe fact that X = xxT for some x ∈Rn is equivalent to rank(X) = 1 and X ⪰0.This means that\n(90) is equivalent to\nmax\nTr(BX)\ns.t.\nXii = 1, ∀i\n(92)\nX ⪰0\nrank(X) = 1.\nWe now relax the problem by removing the non-convex rank constraint\nmax\nTr(BX)\ns.t.\nXii = 1, ∀i\n(93)\nX ⪰0.\n\nThis is an SDP that can be solved (up to arbitrary precision) in polynomial time [VB96].\nSince we removed the rank constraint, the solution to (93) is no longer guaranteed to be rank-1.\nWe will take a different approach from the one we used before to obtain an approximation ratio for\nMax-Cut, which was a worst-case approximation ratio guarantee. What we will show is that, for some\nvalues of α and β, with high probability, the solution to (93) not only satisfies the rank constraint\nbut it coincides with X = ggT where g corresponds to the true partition. After X is computed, g is\nsimply obtained as its leading eigenvector.\n9.6\nThe analysis\nWithout loss of generality, we can assume that g = (1, . . . , 1,\nn\n-1, . . . , -1)T , meaning that the true\npartition corresponds to the first 2 nodes on one side and the other n on the other.\n9.6.1\nSome preliminary definitions\nRecall that the degree matrix D of a graph G is a diagonal matrix where each diagonal coefficient Dii\ncorresponds to the number of neighbours of vertex i and that λ2(M) is the second smallest eigenvalue\nof a symmetric matrix M.\nDefinition 9.1 Let G+ (resp. G ) be the subgraph of G that includes the edges that link two nodes in\n-\nthe same community (resp. in different communities) and A the adjacency matrix of G. We denote by\nD+ (resp. D-) the degree matrix of\nG\nG\nG+ (resp. G ) and define the Stochastic Block Model Laplacian\n-\nto be\nL\n= D+\nSBM\nG -D-\nG -A\n9.7\nConvex Duality\nA standard technique to show that a candidate solution is the optimal one for a convex problem is to\nuse convex duality.\nWe will describe duality with a game theoretical intuition in mind. The idea will be to rewrite (93)\nwithout imposing constraints on X but rather have the constraints be implicitly enforced. Consider\nthe following optimization problem.\nmax\nmin\nTr(BX) + Tr(QX) + Tr (Z (In\nX\nZ, Q\n×n -X))\n(94)\nZ is diagonal\nQ⪰0\nLet us give it a game theoretical interpretation. Suppose that is a primal player (picking X) whose\nobjective is to maximize the objective and a dual player, picking Z and Q after seeing X, trying to\nmake the objective as small as possible. If the primal player does not pick X satistying the constraints\nof (93) then we claim that the dual player is capable of driving the objective to -inf. Indeed, if there\nis an i for which Xii = 1 then the dual player can simply pick Zii = -c\nand make the objective\n1-Xii\nas small as desired by taking large enough c. Similarly, if X is not positive semidefinite, then the\n\ndual player can take Q = cvvT where v is such that vT Xv < 0. If, on the other hand, X satisfy the\nconstraints of (93) then\nTr(BX) ≤\nmin\nTr(BX) + Tr(QX) + Tr (Z (In\nZ, Q\n×n -X)) ,\nZ is diagonal\nQ⪰0\nsince equality can be achieve if, for example, the dual player picks Q = 0n\nn, then it is clear that the\n×\nvalues of (93) and (94) are the same:\nmax Tr(BX) = max\nmin\nTr(BX) + Tr(QX) + Tr (Z (In\nn\nX))\nX,\nX\nZ, Q\n× -\nXii\nX\n∀i\nZ is diagonal\n⪰0\nQ⪰0\nWith this game theoretical intuition in mind, it is clear that if we change the \"rules of the game\" and\nhave the dual player decide their variables before the primal player (meaning that the primal player\ncan pick X knowing the values of Z and Q) then it is clear that the objective can only increase, which\nmeans that:\nmax Tr(BX) ≤\nmin\nmax Tr(BX) + Tr(QX) + Tr (Z (In\nX)) .\nX,\nZ, Q\nX\n×n -\nXii ∀i\nZ is diagonal\nX⪰0\nQ⪰0\nNote that we can rewrite\nTr(BX) + Tr(QX) + Tr (Z (In\nn -X)) = Tr ((B + Q -Z) X) + Tr(Z).\n×\nWhen playing:\nmin\nmax Tr ((B + Q -Z) X) + Tr(Z),\nZ, Q\nX\nZ is diagonal\nQ⪰0\nif the dual player does not set B + Q -Z = 0n×n then the primal player can drive the objective value\nto +inf, this means that the dual player is forced to chose Q = Z -B and so we can write\nmin\nmax Tr ((B + Q -Z) X) + Tr(Z) =\nmin\nmax Tr(Z),\nZ, Q\nX\nZ,\nX\nZ is diagonal\nZ is diagonal\nQ⪰0\nZ-B⪰0\nwhich clearly does not depend on the choices of the primal player. This means that\nmax Tr(BX)\nX,\n≤\nmin\nTr(Z).\nZ,\nXii\ni\nX\n∀\nZ is diagonal\n⪰0\nZ-B⪰0\nThis is known as weak duality (strong duality says that, under some conditionsm the two optimal\nvalues actually match, see, for example, [VB96], recall that we used strong duality when giving a\nsum-of-squares interpretation to the Max-Cut approximation ratio, a similar interpretation can be\ngiven in this problem, see [Ban15b]).\n\nAlso, the problem\nmin\nTr(Z)\ns.t.\nZ is diagonal\n(95)\nZ -B ⪰0\nis called the dual problem of (93).\nThe derivation above explains why the objective value of the dual is always larger or equal to\nthe primal. Nevertheless, there is a much simpler proof (although not as enlightening): let X, Z be\nrespectively a feasible point of (93) and (95). Since Z is diagonal and Xii = 1 then Tr(ZX) = Tr(Z).\nAlso, Z -B ⪰0 and X ⪰0, therefore Tr[(Z -B)X] ≥0. Altogether,\nTr(Z) -Tr(BX) = Tr[(Z -B)X] ≥0,\nas stated.\nRecall that we want to show that ggT is the optimal solution of (93). Then, if we find Z diagonal,\nsuch that Z -B ⪰0 and\nTr[(Z -B)ggT ] = 0,\n(this condition is known as complementary slackness)\nthen X = ggT must be an optimal solution of (93). To ensure that ggT is the unique solution we\njust have to ensure that the nullspace of Z -B only has dimension 1 (which corresponds to multiples\nof g). Essentially, if this is the case, then for any other possible solution X one could not satisfy\ncomplementary slackness.\nThis means that if we can find Z with the following properties:\n1. Z is diagonal\n2. Tr[(Z -B)ggT ] = 0\n3. Z -B ⪰0\n4. λ2(Z -B) > 0,\nthen ggT is the unique optima of (93) and so recovery of the true partition is possible (with an efficient\nalgorithm).\nZ is known as the dual certificate, or dual witness.\n9.8\nBuilding the dual certificate\nThe idea to build Z is to construct it to satisfy properties (1) and (2) and try to show that it satisfies\n(3) and (4) using concentration.\nIf indeed Z -B ⪰0 then (2) becomes equivalent to (Z -B)g = 0. This means that we need to\nconstruct Z such that Zii = 1\ngi B[i, :]g. Since B = 2A -(11T -I) we have\nZii = 1\n(2A\ngi\n-(11T -I))[i, :]g = 2gi\n(Ag)i + 1,\n\nmeaning that\nZ = 2(D+\nG -D-) + I\nG\nis our guess for the dual witness. As a result\nZ -B = 2(D+\nG -D-)\nG\n-I -\n\n2A -(11T -I)\n\n= 2LSBM + 11T\nIt trivially follows (by construction) that\n(Z -B)g = 0.\nTherefore\nLemma 9.2 If\nλ2(2LSBM + 11T ) > 0,\n(96)\nthen the relaxation recovers the true partition.\nNote that 2LSBM + 11T is a random matrix and so this boils down to \"an exercise\" in random matrix\ntheory.\n9.9\nMatrix Concentration\nClearly,\nE\n\n2LSBM + 11T\n= 2ELSBM + 11T = 2ED+\nG -2ED-\nG -2EA + 11T ,\nand ED+ = n\nG\nα log(n)\nn\nI, ED-\nG = n\nβ log(n)\nn\nI, and EA is a matrix such with 4 n\n2 × n\n2 blocks where the\ndiagonal blocks have α log(n)\nn\nand the off-diagonal blocks have β log(n)\nn\n. We can write this as EA =\n\nα log(n)\nn\n+ β log(n)\nn\n\n11T + 1\n\nα log(n)\nn\n-β log(n)\ng\nn\n\ngT\nThis means that\n\nT\n\nlog n\nE 2LSBM + 11\n= ((α -β) log n) I +\n1 -(α + β)\nn\n\n11T -(α -β)log n\nn\nggT .\nSince 2LSBMg = 0 we can ignore what happens in the span of g and it is not hard to see that\nλ2\n\n((α -β) log n) I +\n\n1 -(α + β)log n\nn\n\n11T -(α -β)log nggT\nn\n\n= (α -β) log n.\nThis means that it is enough to show that\nα\n∥LSBM -E [LSBM]∥<\n-β log n,\n(97)\nwhich is a large deviations inequality. (∥· ∥denotes operator norm)\nWe will skip the details here (and refer the reader to [Ban15c] for the details), but the main idea is\nto use an inequality similar to the ones presented in the lecture about concentration of measure (and,\nin particular, matrix concentration). The main idea is to separate the diagonal from the non-diagonal\npart of LSBM -E [LSBM]. The diagonal part depends on in and out-degrees of each node and can\nbe handled with scalar concentration inequalities for trinomial distributions (as it was in [ABH14] to\nobtain the information theoretical bounds). The non-diagonal part has independent entries and so its\nspectral norm can be controlled by the following inequality:\n\nLemma 9.3 (Remark 3.13 in [BvH15]) Let X be the n × n symmetric matrix with independent\ncentered entries. Then there exists a universal constant c′, such that for every t ≥0\n∥\n∥\n≤\n-t2\n′\nProb[ X\n> 3σ + t]\nne\n/c σ2\ninf,\n(98)\nwhere we have defined\nσ := max\ni\nsX\nE[X2\nij],\nσ\n:= max\ninf\nij\nj\n∥Xij∥\n.\ninf\nUsing these techniques one can show (this result was independently shown in [Ban15c] and [HWX14],\nwith a slightly different approach)\nTheorem 9.4 Let G be a random graph with n nodes drawn accordingly to the stochastic block model\non two communities with edge probabilities p and q. Let p = α log n\nn\nand q = β log n\nn\n, where α > β are\nconstants. Then, as long as\n√α -\np\nβ >\n√\n2,\n(99)\nthe semidefinite program considered above coincides with the true partition with high probability.\nNote that, if\n√α -\np\nβ <\n√\n2,\nthen exact recovery of the communities is impossible, meaning that the SDP algorithm is optimal.\nFurthermore, in this regime one can show that there will be a node on each community that is more\nconnected to the other community that to its own, meaning that a partition that swaps them would\nhave more likelihood. In fact, the fact that the SDP will start working essentially when this starts\nhappening appears naturally in the analysis; the diagonal part corresponds exactly to differences\nbetween in and out-degrees and Lemma 9.3 allows one to show that the contributions of the off-\ndiagonal part are of lower order.\nRemark 9.5 A simpler analysis (and seemingly more adaptable to other problems) can be carried out\nby using by Matrix Bernstein's inequality [Tro12] (described in the lecture about Matrix Concentration).\nThe idea is simply to write LSBM -E [LSBM] as a sum of independent matrices (where each matrix\ncorresponds to a pair of nodes) and to apply Matrix Bernstein (see [ABH14]). Unfortunately, this only\nshows exact recovery of a suboptimal threshold (suboptimal essentially by a factor of 2).\n9.10\nMore communities\nA natural question is to understand what is the exact recovery threshold for the Stochastic Block\nModel on k ≥2 communities. Recall the definition: The stochastic block model can be similarly\ndefined for any k ≥2 communities: G is a graph on n = km nodes divided on k groups of m nodes\neach. Similarly to the k = 2 case, for each pair (i, j) of nodes, (i, j) is an edge of G with probability p\nif i and j are in the same set, and with probability q if they are in different sets. Each edge is drawn\nindependently and p > q. In the logarithmic degree regime, we'll define the parameters in a slightly\ndifferent way: p = α′ log m\nm\nand q = β′ log m. Note that, for k = 2, we roughly have α = 2α′ and β = 2β\nm\n′,\nwhich means that the exact recovery threshold, for k = 2, reads as: for\n√\nα′ -\np\nβ′ > 1\n\nrecovery is possible (and with the SDP algorithm), and for\n√\nα′-√β′ < 1 exact recovery is impossible.\nClearly, for any k > 2, if\n√\nα′\n√\n-\nβ′ < 1 then exact recovery will also be impossible (simply imagine\nthat n oracle tells us all of the community memberships except for those of two of the clusters, then\nthe problem reduces to the k = 2 case). The remarkable fact is that, for k = o(log m) this is enough,\nnot only for exact recovery to be possible, but also for an SDP based algorithm (very similar to the\none above) to achieve exact recovery (see [AS15, ABKK15, HWX15, PW15]). However, for k ≈log n,\nthe situation is not understood.\nOpen Problem 9.2 What is the threshold for exact recovery on the balanced symmetric Stochas-\ntic Block Model in k ≈log n communities and at what threshold does the SDP succeed at exactly\ndetermining the communities? (see [ABKK15]).\n9.11\nEuclidean Clustering\nThe stochastic block model, although having fascinating phenomena, is not always an accurate model\nfor clustering. The independence assumption assumed on the connections between pairs of vertices\nmay sometimes be too unrealistic. Also, the minimum bisection of multisection objective may not be\nthe most relevant in some applications.\nOne particularly popular form of clustering is k-means clustering.\nGiven n points x1, . . . , xn\nand pairwise distances d(xi, xj), the k-means objective attempts to partition the points in k clusters\nA1, . . . , Ak (not necessarily of the same size) as to minimize the following objective35\nX\nk\nmin\nt=1\nd\n|At| xi,x\nX\n2(xi, xj).\nj∈At\nA similar objective is the one in k-medians clustering, where for each cluster a center is picked (the\ncenter has to be a point in the cluster) and the sum of the distances from all points in the cluster to\nthe center point are to be minimized, in other words, the objective to be minimized is:\nk\nmin\nX\nmin\nct\nt=1\n∈At x\nX\nd(xi, ct).\ni∈At\nIn [ABC+15] both an Linear Programming (LP) relaxation for k-medians and a Semidefinite\nProgramming (SDP) relaxation for k-means are analyzed for a points in a generative model on which\nthere are k disjoint balls in Rd and, for every ball, points are drawn according to a isotropic distribution\non each of the balls. The goal is to establish exact recovery of these convex relaxations requiring the\nleast distance between the balls. This model (in this context) was first proposed and analyzed for\nk-medians in [NW13], the conditions for k-medians were made optimal in [ABC+15] and conditions\nfor k-means were also given. More recently, the conditions on k-means were improved (made optimal\nfor large dimensions) in [IMPV15a, IMPV15b] which also coined the term \"Stochastic Ball Model\".\nFor P the set of points, in order to formulate the k-medians LP we use variables yp indicat-\ning whether p is a center of its cluster or not and zpq indicating whether q is assigned to p or not\n(see [ABC+15] for details), the LP then reads:\n35When the points are in Euclidean space there is an equivalent more common formulation in which each cluster is\nassign a mean and the objective function is the sum of the distances squared to the center.\n\nmin\nP\np,q d(p, q)zpq,\ns.t.\nP\np∈P zpq = 1,\n∀q ∈P\nPzpq ≤yp\np∈P yp = k\nzpq, yp ∈[0, 1],\n∀p, q ∈P.\nthe solution corresponds to an actual k-means solution if it is integral.\nThe semidefinite program for k-means is written in terms of a PSD matrix X ∈Rn×n (where n is\nthe total number of points), see [ABC+15] for details. The intended solution is\nX =\nX\nk\n1A\nn\nt1T\nAt,\nt=1\nwhere 1At is the indicator vector of the cluster At. The SDP reads as follows:\nminX\nP\ni,j d(i, j)Xij,\ns.t.\nTr(X) = k,\nX1 = 1\nX ≥0\nX ⪰0.\nInspired by simulations in the context of [NW13] and [ABC+15], Rachel Ward observed that the\nk-medians LP tends to be integral even for point configurations where no planted partition existed,\nand proposed the conjecture that k-medians is tight for typical point configurations. This was recorded\nas Problem 6 in [Mix15]. We formulate it as an open problem here:\nOpen Problem 9.3 Is the LP relaxation for k-medians tight for a natural (random) generative model\nof points even without a clustering planted structure (such as, say, gaussian independent points)?\nIdeally, one would like to show that these relaxations (both the k-means SDP and the k-medians\nLP) are integral in instances that have clustering structure and not necessarily arising from generative\nrandom models. It is unclear however how to define what is meant by \"clustering structure\". A\nparticularly interesting approach is through stability conditions (see, for example [AJP13]), the idea\nis that if a certain set of data points has a much larger k -1-means (or medians) objective than a\nk-means (or medians) one, and there is not much difference between the k and the k + 1 objectives,\nthen this is a good suggestion that the data is well explained by k clusters.\nOpen Problem 9.4 Given integrality conditions to either the k-medians LP or the k-means SDP\nbased on stability like conditions, as described above.\n9.12\nProbably Certifiably Correct algorithms\nWhile the SDP described in this lecture for recovery in the Stochastic Block Model achieves exact\nrecovery in the optimal regime, SDPs (while polynomial time) tend to be slow in practice. There\nare faster (quasi-linear) methods that are also able to achieve exact recovery at the same threshold.\n\nHowever, the SDP has an added benefit of producing a posteriori certificates. Indeed, if the solution\nfrom the SDP is integral (rank 1) then one is (a posteriori) sure to have found the minimum bisection.\nThis means that the SDP (above the threshold) will, with high probability, not only find the minimum\nbisection but will also produce a posteriori certificate of such,. Such an algorithms are referred to as\nProbably Certifiably Correct (PCC) [Ban15b]. Fortunately, one can get (in this case) get the best\nof both worlds and get a fast PCC method for recovery in the Stochastic Block Model essentially by\nusing a fas method to find the solution and then using the SDP to only certify, which can be done\nconsiderably faster (see [Ban15b]). More recently, a PCC algorithm was also analyzed for k-means\nclustering (based on the SDP described above) [IMPV15b].\n9.13\nAnother conjectured instance of tightness\nThe following problem is posed, by Andrea Montanari, in [Mon14], a description also appears in [Ban15a].\nWe briefly describe it here as well:\nGiven a symmetric matrix W ∈Rn×n the positive principal component analysis problem can be\nwritten as\nmax\nxT Wx\ns. t.\n∥x∥= 1\n(100)\nx ≥0\nx ∈Rn.\nIn the flavor of the semidefinite relaxations considered in this section, (100) can be rewritten (for\nX ∈Rn×n) as\nmax\nTr(WX)\ns. t.\nTr(X) = 1\nX ≥0\nX ⪰0\nrank(X) = 1,\nand further relaxed to the semidefinite program\nmax\nTr(WX)\ns. t.\nTr(X) = 1\n(101)\nX ≥0\nX ⪰0.\nThis relaxation appears to have a remarkable tendency to be tight. In fact, numerical simulations\nsuggest that if W is taken to be a Wigner matrix (symmetric with i.i.d. standard Gaussian entries),\nthen the solution to (101) is rank 1 with high probability, but there is no explanation of this phe-\nnomenon. If the Wigner matrix is normalized\n√\nto have entries N(0, 1/n), it is known that the typical\nvalue of the rank constraint problem is\n2 (see [MR14]).\nThis motivates the last open problem of this section.\nOpen Problem 9.5 Let W be a gaussian Wigner matrix with entries N(0, 1/n). Consider the fol-\n\nlowing Semidefinite Program:\nmax\nTr(WX)\ns. t.\nTr(X) = 1\n(102)\nX ≥0\nX ⪰0.\nProve or disprove the following conjectures.\n1. The expected value of this program is\n√\n2 + o(1).\n2. With high probability, the solution of this SDP is rank 1.\nRemark 9.6 The dual of this SDP motivates a particularly interesting statement which is implied by\nthe conjecture. By duality, the value of the SDP is the same as the value of\nmin λmax (W + Λ) ,\nΛ≥0\nwhich is thus conjectured to be\n√\n2 + o(1), although no bound better than 2 (obtained by simply taking\nΛ = 0) is known.\n\nSynchronization Problems and Alignment\n10.1\nSynchronization-type problems\nThis section will focuses on synchronization-type problems.36\nThese are problems where the goal\nis to estimate a set of parameters from data concerning relations or interactions between pairs of\nthem.\nA good example to have in mind is an important problem in computer vision, known as\nstructure from motion: the goal is to build a three-dimensional model of an object from several\ntwo-dimensional photos of it taken from unknown positions. Although one cannot directly estimate\nthe positions, one can compare pairs of pictures and gauge information on their relative positioning.\nThe task of estimating the camera locations from this pairwise information is a synchronization-type\nproblem. Another example, from signal processing, is multireference alignment, which is the problem\nof estimating a signal from measuring multiple arbitrarily shifted copies of it that are corrupted with\nnoise.\nWe will formulate each of these problems as an estimation problem on a graph G = (V, E). More\nprecisely, we will associate each data unit (say, a photo, or a shifted signal) to a graph node i ∈V . The\nproblem can then be formulated as estimating, for each node i ∈V , a group element gi ∈G, where the\ngroup G is a group of transformations, such as translations, rotations, or permutations. The pairwise\ndata, which we identify with edges of the graph (i, j) ∈E, reveals information about the ratios gi(gj)-1.\nIn its simplest form, for each edge (i, j) ∈E of the graph, we have a noisy estimate of gi(g\nj)-\nand\nthe synchronization problem consists of estimating the individual group elements g : V →G that are\nthe most consistent with the edge estimates, often corresponding to the Maximum Likelihood (ML)\nestimator. Naturally, the measure of \"consistency\" is application specific. While there is a general\nway of describing these problems and algorithmic approaches to them [BCS15, Ban15a], for the sake\nof simplicity we will illustrate the ideas through some important examples.\n10.2\nAngular Synchronization\nThe angular synchronization problem [Sin11, BSS13] consist in estimating n unknown angles θ1, . . . , θn\nfrom m noisy measurements of their offsets θi -θj mod 2π. This problem easily falls under the scope\nof synchronization-type problem by taking a graph with a node for each θi, an edge associated with\neach measurement, and taking the group to be G ∼= SO(2), the group of in-plane rotations. Some of its\napplications include time-synchronization of distributed networks [GK06], signal reconstruction from\nphaseless measurements [ABFM12], surface reconstruction problems in computer vision [ARC06] and\noptics [RW01].\nLet us consider a particular instance of this problem (with a particular noise model).\nLet z1, . . . , zn ∈C satisfying |za| = 1 be the signal (angles) we want to estimate (za = exp(iθa)).\nSuppose for every pair (i, j) we make a noisy measurement of the angle offset\nYij = zizj + σWij,\nwhere Wij ∼N(0, 1). The maximum likelihood estimator for z is given by solving (see [Sin11, BBS14])\nmax x∗Y x.\n(103)\n|xi|2=1\n36And it will follow somewhat the structure in Chapter 1 of [Ban15a]\n\n-x i\nxj\ng\ngj\ni\n~gg\ni j\n-1\nFigure 22: Given a graph G = (V, E) and a group G, the goal in synchronization-type problems is to\nestimate node labels g : V →G from noisy edge measurements of offsets gig-1\nj .\nThere are several approaches to try to solve (103). Using techniques very similar to the study\nof the spike model in PCA on the first lecture one can (see [Sin11]), for example, understand the\nperformance of the spectral relaxation of (103) into\nmax x∗Y x.\n(104)\n∥x∥2=n\nNotice that, since the solution to (104) will not necessarily be a vector with unit-modulus entries,\na rounding step will, in general, be needed. Also, to compute the leading eigenvector of A one would\nlikely use the power method. An interesting adaptation to this approach is to round after each iteration\nof the power method, rather than waiting for the end of the process, more precisely:\nAlgorithm 10.1 Given Y . Take a original (maybe random) vector x(0). For each iteration k (until\nconvergence or a certain number of iterations) take x(k+1) to be the vector with entries:\n\nx(k+1)\n=\ni\nY x(k)\ni\nmetho\n\n.\ni\nthis\nY x(k)\nAlthough\nd appears to perform very well in n\n\numeric\n\nexperiments, its analysis is still an\nopen problem.\nOpen Problem 10.1 In the model where Y = zz∗+ σW as described above, for which values of σ\nwill the Projected Power Method (Algorithm 10.1) converge to the optimal solution of (103) (or at\nleast to a solution that correlates well with z), with high probability?37\n37We thank Nicolas Boumal for suggesting this problem.\n\nFigure 23: An example of an instance of a synchronization-type problem. Given noisy rotated copies\nof an image (corresponding to vertices of a graph), the goal is to recover the rotations. By comparing\npairs of images (corresponding to edges of the graph), it is possible to estimate the relative rotations\nbetween them. The problem of recovering the rotation of each image from these relative rotation\nestimates is an instance of Angular synchronization.\nWe note that Algorithm 10.1 is very similar to the Approximate Message Passing method presented,\nand analyzed, in [MR14] for the positive eigenvector problem.\nAnother approach is to consider an SDP relaxation similar to the one for Max-Cut and minimum\nbisection.\nmax\nTr(Y X)\ns.t.\nXii = 1, ∀i\n(105)\nX ⪰0.\n\nIn [BBS14] it is shown that, in the model of Y = zz∗zz∗+ σW, as long as σ = O(n1/4) then (105)\nis tight, meaning that the optimal solution is rank 1 and thus it corresponds to the optimal solution\nof (103).38. It is conjecture [BBS14] however that σ = O (n1/2) should suffice. It is known (see [BBS14])\nthat this is implied by the following conjecture:\nIf x♮\n\nis the optimal solution to (103), then with high probability ∥Wx♮∥\n=\n).\ninf\nO(n1/2\nThis is the\ncontent of the next open problem.\nOpen Problem 10.2 Prove or disprove: With high probability the SDP relaxation (105) is tight as\n\nlong as σ = O(n1/2). This would follow from showing that, with high probability ∥Wx♮∥\n= O(n1/2),\ninf\nwhere x♮is the optimal solution to (103).\n38Note that this makes (in this regime) the SDP relaxation a Probably Certifiably Correct algorithm [Ban15b]\n\nFigure 24: Illustration of the Cryo-EM imaging process: A molecule is imaged after being frozen\nat a random (unknown) rotation and a tomographic 2-dimensional projection is captured. Given a\nnumber of tomographic projections taken at unknown rotations, we are interested in determining such\nrotations with the objective of reconstructing the molecule density. Images courtesy of Amit Singer\nand Yoel Shkolnisky [SS11].\nWe note that the main difficulty seems to come from the fact that W and x♮are not independent\nrandom variables.\n10.2.1\nOrientation estimation in Cryo-EM\nA particularly challenging application of this framework is the orientation estimation problem in\nCryo-Electron Microscopy [SS11].\nCryo-EM is a technique used to determine the three-dimensional structure of biological macro-\nmolecules. The molecules are rapidly frozen in a thin layer of ice and imaged with an electron micro-\nscope, which gives 2-dimensional projections. One of the main difficulties with this imaging process is\nthat these molecules are imaged at different unknown orientations in the sheet of ice and each molecule\ncan only be imaged once (due to the destructive nature of the imaging process). More precisely, each\nmeasurement consists of a tomographic projection of a rotated (by an unknown rotation) copy of the\nmolecule. The task is then to reconstruct the molecule density from many such measurements. As\nthe problem of recovering the molecule density knowing the rotations fits in the framework of classical\ntomography--for which effective methods exist-- the problem of determining the unknown rotations,\nthe orientation estimation problem, is of paramount importance. While we will not go into details\nhere, there is a mechanism that, from two such projections, obtains information between their ori-\nentation. The problem of finding the orientation of each projection from such pairwise information\nnaturally fits in the framework of synchronization and some of the techniques described here can be\nadapted to this setting [BCS15].\nImage courtesy of Prof. Amit Singer, Princeton University. Used with permission.\n\n10.2.2\nSynchronization over Z2\nThis particularly simple version already includes many applications of interest. Similarly to before,\ngiven a graph G = (V, E), the goal is recover unknown node labels g : V →Z2 (corresponding to\nmemberships to two clusters) from pairwise information. Each pairwise measurement either suggests\nthe two involved nodes are in the same cluster or in different ones (recall the problem of recovery in\nthe stochastic block model). The task of clustering the graph in order to agree, as much as possible,\nwith these measurements is tightly connected to correlation clustering [BBC04] and has applications\nto determining the orientation of a manifold [SW11].\nIn the case where all the measurements suggest that the involved nodes belong in different com-\nmunities, then this problem essentially reduces to the Max-Cut problem.\n10.3\nSignal Alignment\nIn signal processing, the multireference alignment problem [BCSZ14] consists of recovering an unknown\nsignal u ∈RL from n observations of the form\nyi = Rliu + σξi,\n(106)\nwhere Rli is a circulant permutation matrix that shifts u by li ∈ZL coordinates, ξi is a noise vector\n(which we will assume standard gaussian i.i.d. entries) and li are unknown shifts.\nIf the shifts were known, the estimation of the signal u would reduce to a simple denoising problem.\nFor that reason, we will focus on estimating the shifts {li}n\ni=1. By comparing two observations yi and\nyj we can obtain information about the relative shift li -lj mod L and write this problem as a\nSynchronization problem\n10.3.1\nThe model bias pitfall\nIn some of the problems described above, such as the multireference alignment of signals (or the orien-\ntation estimation problem in Cryo-EM), the alignment step is only a subprocedure of the estimation\nof the underlying signal (or the 3d density of the molecule). In fact, if the underlying signal was\nknown, finding the shifts would be nearly trivial: for the case of the signals, one could simply use\nmatch-filtering to find the most likely shift li for measurement yi (by comparing all possible shifts of\nit to the known underlying signal).\nWhen the true signal is not known, a common approach is to choose a reference signal z that is not\nthe true template but believed to share some properties with it. Unfortunately, this creates a high risk\nof model bias: the reconstructed signal uˆ tends to capture characteristics of the reference z that are\nnot present on the actual original signal u (see Figure 10.3.1 for an illustration of this phenomenon).\nThis issue is well known among the biological imaging community [SHBG09, Hen13] (see, for example,\n[Coh13] for a particularly recent discussion of it). As the experiment shown on Figure 10.3.1 suggests,\nthe methods treated in this paper, based solely on pairwise information between observations, do not\nsuffer from model bias as they do not use any information besides the data itself.\nIn order to recover the shifts li from the shifted noisy signals (106) we will consider the following\nestimator\nargminl1,...,ln\nL\nX\nR\nliyi -\nR\ny\n∈Z\n-\n-lj\nj\ni,j∈[n]\n,\n(107)\n\nFigure 25: A simple experiment to illustrate the model bias phenomenon: Given a picture of the\nmathematician Hermann Weyl (second picture of the top row) we generate many images consisting\nof random rotations (we considered a discretization of the rotations of the plane) of the image with\nadded gaussian noise. An example of one such measurements is the third image in the first row. We\nthen proceeded to align these images to a reference consisting of a famous image of Albert Einstein\n(often used in the model bias discussions). After alignment, an estimator of the original image was\nconstructed by averaging the aligned measurements. The result, first image on second row, clearly\nhas more resemblance to the image of Einstein than to that of Weyl, illustration the model bias issue.\nOne the other hand, the method based on the synchronization approach produces the second image of\nthe second row, which shows no signs of suffering from model bias. As a benchmark, we also include\nthe reconstruction obtained by an oracle that is given the true rotations (third image in the second\nrow).\nwhich is related to the maximum likelihood estimator of the shifts. While we refer to [Ban15a] for a\nderivation we note that it is intuitive that if li is the right shift for yi and lj for yj then R-liyi -R-ljyj\nshould be random gaussian noise, which motivates the estimator considered.\nSince a shift does not change the norm of a vector, (107) is equivalent to\nargmax\nX\n⟨R-liyi, R-ljyj ,\n(108)\nl1,...,ln∈ZL\n⟩\ni,j∈[n]\nwe will refer to this estimator as the quasi-MLE.\nIt is not surprising that solving this problem is NP-hard in general (the search space for this\noptimization problem has exponential size and is nonconvex). In fact, one can show [BCSZ14] that,\nconditioned on the Unique Games Conjecture, it is hard to approximate up to any constant.\n\n10.3.2\nThe semidefinite relaxation\nWe will now present a semidefinite relaxation for (108) (see [BCSZ14]).\nLet us identify Rl with the L×L permutation matrix that cyclicly permutes the entries fo a vector\nby li coordinates:\n\nu\n\nu1-l\n.\n.\nRl\n.\n=\n.\n.\n.\n\nuL\nuL-l\n.\nThis corresponds to an L-dimensional represen\n\ntation\n\nof the cyclic\n\ngroup. Then, (108) can be rewritten:\nX\n⟨R-liyi, R-ljyj⟩\n=\nX\nT\n(R-liyi) R-ljyj\ni,j∈[n]\ni,j∈[n]\n=\ni,j\nX\nTr\n∈[n]\nh\nT\n(R-liyi) R-ljyj\ni\n=\nX\nTr\n\nyT RT\ni\n-l R\ni\n-ljyj\ni,j∈[n]\n\n=\nX\nTr\nhy yT\ni j\nT R\nT\nliRlj\ni,j∈[n]\ni\n.\nWe take\nX =\nRl1\n\nRl2\nnL\nnL\n\n.\nRT\nT\n.\nl\nRT\nl\nRl\nR\n×\n,\n(109)\n.\nl\n\n· · ·\nn\nR\n\n∈\nn\nand can rewrite (108) as\nmax\nTr(CX)\ns. t.\nXii = IL×L\nXij is a circulant permutation matrix\n(110)\nX ⪰0\nrank(X) ≤L,\nwhere C is the rank 1 matrix given by\ny\nC\ny\n\n=\n..\n\n.\nyn\n\nT\nT\nT\nnL\nnL\nT\n\ny1\ny2\n· · ·\nyn\n∈R\n×\n,\n(111)\nwith blocks Cij = yiyj .\n\nThe constraints Xii = IL\nL and rank(X) ≤L imply that rank(X) = L and X\n×\nij ∈O(L). Since the\nonly doubly stochastic matrices in O(L) are permutations, (110) can be rewritten as\nmax\nTr(CX)\ns. t.\nXii = IL×L\nXij1 = 1\nXij is circulant\n(112)\nX ≥0\nX ⪰0\nrank(X) ≤L.\nRemoving the nonconvex rank constraint yields a semidefinite program, corresponding to (??),\nmax\nTr(CX)\ns. t.\nXii = IL×L\nXij1 = 1\n(113)\nXij is circulant\nX ≥0\nX ⪰0.\nNumerical simulations (see [BCSZ14, BKS14]) suggest that, below a certain noise level, the semidef-\ninite program (113) is tight with high probability.\nHowever, an explanation of this phenomenon\nremains an open problem [BKS14].\nOpen Problem 10.3 For which values of noise do we expect that, with high probability, the semidef-\ninite program (113) is tight? In particular, is it true that for any σ by taking arbitrarily large n the\nSDP is tight with high probability?\n10.3.3\nSample complexity for multireference alignment\nAnother important question related to this problem is to understand its sample complexity. Since\nthe objective is to recover the underlying signal u, a larger number of observations n should yield\na better recovery (considering the model in (??)). Another open question is the consistency of the\nquasi-MLE estimator, it is known that there is some bias on the power spectrum of the recovered\nsignal (that can be easily fixed) but the estimates for phases of the Fourier transform are conjecture\nto be consistent [BCSZ14].\nOpen Problem 10.4\n1. Is the quasi-MLE (or the MLE) consistent for the Multireference align-\nment problem? (after fixing the power spectrum appropriately).\n2. For a given value of L and σ, how large does n need to be in order to allow for a reasonably\naccurate recovery in the multireference alignment problem?\nRemark 10.2 One could design a simpler method based on angular synchronization: for each pair\nof signals take the best pairwise shift and then use angular synchronization to find the signal shifts\nfrom these pairwise measurements.\nWhile this would yield a smaller SDP, the fact that it is not\n\nusing all of the information renders it less effective [BCS15]. This illustrates an interesting trade-off\nbetween size of the SDP and its effectiveness. There is an interpretation of this through dimensions of\nrepresentations of the group in question (essentially each of these approaches corresponds to a different\nrepresentation), we refer the interested reader to [BCS15] for more one that.\nReferences\n[AABS15]\nE. Abbe, N. Alon, A. S. Bandeira, and C. Sandon. Linear boolean classification, coding\nand \"the critical problem\". Available online at arXiv:1401.6528v3 [cs.IT], 2015.\n[ABC+15]\nP. Awasthi, A. S. Bandeira, M. Charikar, R. Krishnaswamy, S. Villar, and R. Ward. Relax,\nno need to round: integrality of clustering formulations. 6th Innovations in Theoretical\nComputer Science (ITCS 2015), 2015.\n[ABFM12]\nB. Alexeev, A. S. Bandeira, M. Fickus, and D. G. Mixon. Phase retrieval with polarization.\navailable online, 2012.\n[ABG12]\nL. Addario-Berry and S. Griffiths.\nThe spectrum of random lifts.\navailable at\narXiv:1012.4097 [math.CO], 2012.\n[ABH14]\nE. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model.\nAvailable online at arXiv:1405.3267 [cs.SI], 2014.\n[ABKK15]\nN. Agarwal, A. S. Bandeira, K. Koiliaris, and A. Kolla. Multisection in the stochastic block\nmodel using semidefinite programming.\nAvailable online at arXiv:1507.02323 [cs.DS],\n2015.\n[ABS10]\nS. Arora, B. Barak, and D. Steurer. Subexponential algorithms for unique games related\nproblems. 2010.\n[AC09]\nNir Ailon and Bernard Chazelle. The fast Johnson-Lindenstrauss transform and approx-\nimate nearest neighbors. SIAM J. Comput, pages 302-322, 2009.\n[AGZ10]\nG. W. Anderson, A. Guionnet, and O. Zeitouni. An introduction to random matrices.\nCambridge studies in advanced mathematics. Cambridge University Press, Cambridge,\nNew York, Melbourne, 2010.\n[AJP13]\nM. Agarwal, R. Jaiswal, and A. Pal. k-means++ under approximation stability. The 10th\nannual conference on Theory and Applications of Models of Computation, 2013.\n[AL06]\nN. Alon and E. Lubetzky. The shannon capacity of a graph and the independence numbers\nof its powers. IEEE Transactions on Information Theory, 52:21722176, 2006.\n[ALMT14]\nD. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: phase\ntransitions in convex programs with random data. 2014.\n[Alo86]\nN. Alon. Eigenvalues and expanders. Combinatorica, 6:83-96, 1986.\n\n[Alo03]\nN. Alon. Problems and results in extremal combinatorics i. Discrete Mathematics, 273(1-\n3):31-53, 2003.\n[AM85]\nN. Alon and V. Milman. Isoperimetric inequalities for graphs, and superconcentrators.\nJournal of Combinatorial Theory, 38:73-88, 1985.\n[AMMN05] N. Alon, K. Makarychev, Y. Makarychev, and A. Naor.\nQuadratic forms on graphs.\nInvent. Math, 163:486-493, 2005.\n[AN04]\nN. Alon and A. Naor. Approximating the cut-norm via Grothendieck's inequality. In\nProc. of the 36 th ACM STOC, pages 72-80. ACM Press, 2004.\n[ARC06]\nA. Agrawal, R. Raskar, and R. Chellappa. What is the range of surface reconstructions\nfrom a gradient field?\nIn A. Leonardis, H. Bischof, and A. Pinz, editors, Computer\nVision - ECCV 2006, volume 3951 of Lecture Notes in Computer Science, pages 578-591.\nSpringer Berlin Heidelberg, 2006.\n[AS15]\nE. Abbe and C. Sandon. Community detection in general stochastic block models: fun-\ndamental limits and efficient recovery algorithms. to appear in FOCS 2015, also available\nonline at arXiv:1503.00609 [math.PR], 2015.\n[B+11]\nJ. Bourgain et al. Explicit constructions of RIP matrices and related problems. Duke\nMathematical Journal, 159(1), 2011.\n[Bai99]\nZ. D. Bai. Methodologies in spectral analysis of large dimensional random matrices, a\nreview. Statistics Sinica, 9:611-677, 1999.\n[Ban15a]\nA. S. Bandeira. Convex relaxations for certain inverse problems on graphs. PhD thesis,\nProgram in Applied and Computational Mathematics, Princeton University, 2015.\n[Ban15b]\nA. S. Bandeira.\nA note on probably certifiably correct algorithms.\nAvailable at\narXiv:1509.00824 [math.OC], 2015.\n[Ban15c]\nA. S. Bandeira. Random Laplacian matrices and convex relaxations. Available online at\narXiv:1504.03987 [math.PR], 2015.\n[Ban15d]\nA. S. Bandeira. Relax and Conquer BLOG: Ten Lectures and Forty-two Open Problems\nin Mathematics of Data Science. 2015.\n[Bar14]\nB. Barak. Sum of squares upper bounds, lower bounds, and open questions. Available\nonline at http: // www. boazbarak. org/ sos/ files/ all-notes. pdf , 2014.\n[BBAP05]\nJ. Baik, G. Ben-Arous, and S. P ech e. Phase transition of the largest eigenvalue for nonnull\ncomplex sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.\n[BBC04]\nN. Bansal, A. Blum, and S. Chawla. Correlation clustering. Machine Learning, 56(1-\n3):89-113, 2004.\n[BBRV01]\nS. Bandyopadhyay, P. O. Boykin, V. Roychowdhury, and F. Vatan. A new proof for the\nexistence of mutually unbiased bases. Available online at arXiv:quant-ph/0103162, 2001.\n\n[BBS14]\nA. S. Bandeira, N. Boumal, and A. Singer.\nTightness of the maximum likelihood\nsemidefinite relaxation for angular synchronization. Available online at arXiv:1411.3272\n[math.OC], 2014.\n[BCS15]\nA. S. Bandeira, Y. Chen, and A. Singer. Non-unique games over compact groups and\norientation estimation in cryo-em. Available online at arXiv:1505.03840 [cs.CV], 2015.\n[BCSZ14]\nA. S. Bandeira, M. Charikar, A. Singer, and A. Zhu.\nMultireference alignment using\nsemidefinite programming. 5th Innovations in Theoretical Computer Science (ITCS 2014),\n2014.\n[BDMS13]\nA. S. Bandeira, E. Dobriban, D.G. Mixon, and W.F. Sawin. Certifying the restricted\nisometry property is hard. IEEE Trans. Inform. Theory, 59(6):3448-3450, 2013.\n[BFMM14] A. S. Bandeira, M. Fickus, D. G. Mixon, and J. Moreira.\nDerandomizing restricted\nisometries via the Legendre symbol.\nAvailable online at arXiv:1406.4089 [math.CO],\n2014.\n[BFMW13] A. S. Bandeira, M. Fickus, D. G. Mixon, and P. Wong. The road to deterministic matrices\nwith the restricted isometry property.\nJournal of Fourier Analysis and Applications,\n19(6):1123-1149, 2013.\n[BGN11]\nF. Benaych-Georges and R. R. Nadakuditi. The eigenvalues and eigenvectors of finite,\nlow rank perturbations of large random matrices. Advances in Mathematics, 2011.\n[BGN12]\nF. Benaych-Georges and R. R. Nadakuditi. The singular values and vectors of low rank\nperturbations of large rectangular random matrices. Journal of Multivariate Analysis,\n2012.\n[BKS13a]\nA. S. Bandeira, C. Kennedy, and A. Singer. Approximating the little grothendieck problem\nover the orthogonal group. Available online at arXiv:1308.5207 [cs.DS], 2013.\n[BKS13b]\nB. Barak, J. Kelner, and D. Steurer. Rounding sum-of-squares relaxations. Available\nonline at arXiv:1312.6652 [cs.DS], 2013.\n[BKS14]\nA. S. Bandeira, Y. Khoo, and A. Singer. Open problem: Tightness of maximum likeli-\nhood semidefinite relaxations. In Proceedings of the 27th Conference on Learning Theory,\nvolume 35 of JMLR W&CP, pages 1265-1267, 2014.\n[BLM15]\nA. S. Bandeira, M. E. Lewis, and D. G. Mixon. Discrete uncertainty principles and sparse\nsignal processing. Available online at arXiv:1504.01014 [cs.IT], 2015.\n[BMM14]\nA. S. Bandeira, D. G. Mixon, and J. Moreira. A conditional construction of restricted\nisometries. Available online at arXiv:1410.6457 [math.FA], 2014.\n[Bou14]\nJ. Bourgain. An improved estimate in the restricted isometry problem. Lect. Notes Math.,\n2116:65-70, 2014.\n\n[BR13]\nQ. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal com-\nponent detection. Conference on Learning Theory (COLT), 2013.\n[BRM13]\nC. Bachoc, I. Z. Ruzsa, and M. Matolcsi.\nSquares and difference sets in finite fields.\nAvailable online at arXiv:1305.0577 [math.CO], 2013.\n[BS05]\nJ. Baik and J. W. Silverstein. Eigenvalues of large sample covariance matrices of spiked\npopulation models. 2005.\n[BS14]\nB. Barak and D. Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.\nSurvey, ICM 2014, 2014.\n[BSS13]\nA. S. Bandeira, A. Singer, and D. A. Spielman.\nA Cheeger inequality for the graph\nconnection Laplacian. SIAM J. Matrix Anal. Appl., 34(4):1611-1630, 2013.\n[BvH15]\nA. S. Bandeira and R. v. Handel. Sharp nonasymptotic bounds on the norm of random\nmatrices with independent entries. Annals of Probability, to appear, 2015.\n[Che70]\nJ. Cheeger. A lower bound for the smallest eigenvalue of the Laplacian. Problems in\nanalysis (Papers dedicated to Salomon Bochner, 1969), pp. 195-199. Princeton Univ.\nPress, 1970.\n[Chi15]\nT.-Y. Chien. Equiangular lines, projective symmetries and nice error frames. PhD thesis,\n2015.\n[Chu97]\nF. R. K. Chung. Spectral Graph Theory. AMS, 1997.\n[Chu10]\nF. Chung. Four proofs for the cheeger inequality and graph partition algorithms. Fourth\nInternational Congress of Chinese Mathematicians, pp. 331-349, 2010.\n[Chu13]\nM. Chudnovsky. The erdos-hajnal conjecture - a survey. 2013.\n[CK12]\nP. G. Casazza and G. Kutyniok. Finite Frames: Theory and Applications. 2012.\n[Coh13]\nJ. Cohen. Is high-tech view of HIV too good to be true?\nScience, 341(6145):443-444,\n2013.\n[Coh15]\nG. Cohen.\nTwo-source dispersers for polylogarithmic entropy and improved ramsey\ngraphs. Electronic Colloquium on Computational Complexity, 2015.\n[Con09]\nDavid Conlon. A new upper bound for diagonal ramsey numbers. Annals of Mathematics,\n2009.\n[CR09]\nE.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations\nof Computational Mathematics, 9(6):717-772, 2009.\n[CRPW12] V. Chandrasekaran, B. Recht, P.A. Parrilo, and A.S. Willsky. The convex geometry of\nlinear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.\n\n[CRT06a]\nE. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal recon-\nstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory,\n52:489-509, 2006.\n[CRT06b]\nE. J. Cand`es, J. Romberg, and T. Tao.\nStable signal recovery from incomplete and\ninaccurate measurements. Comm. Pure Appl. Math., 59:1207-1223, 2006.\n[CT]\nT. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience.\n[CT05]\nE. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory,\n51:4203-4215, 2005.\n[CT06]\nE. J. Cand`es and T. Tao. Near optimal signal recovery from random projections: universal\nencoding strategies? IEEE Trans. Inform. Theory, 52:5406-5425, 2006.\n[CT10]\nE. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix comple-\ntion. Information Theory, IEEE Transactions on, 56(5):2053-2080, May 2010.\n[CW04]\nM. Charikar and A. Wirth. Maximizing quadratic programs: Extending grothendieck's\ninequality. In Proceedings of the 45th Annual IEEE Symposium on Foundations of Com-\nputer Science, FOCS '04, pages 54-60, Washington, DC, USA, 2004. IEEE Computer\nSociety.\n[CZ15]\nE. Chattopadhyay and D. Zuckerman. Explicit two-source extractors and resilient func-\ntions. Electronic Colloquium on Computational Complexity, 2015.\n[DG02]\nS. Dasgupta and A. Gupta. An elementary proof of the johnson-lindenstrauss lemma.\nTechnical report, 2002.\n[DKMZ11] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov a. Asymptotic analysis of the stochas-\ntic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84,\nDecember 2011.\n[DM13]\nY. Deshpande and A. Montanari. Finding hidden cliques of size\np\nN/e in nearly linear\ntime. Available online at arXiv:1304.7047 [math.PR], 2013.\n[DMS15]\nA. Dembo, A. Montanari, and S. Sen. Extremal cuts of sparse random graphs. Available\nonline at arXiv:1503.03923 [math.PR], 2015.\n[Don06]\nD. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52:1289-1306, 2006.\n[Dor43]\nR. Dorfman. The detection of defective members of large populations. 1943.\n[Duc12]\nJ. C. Duchi. Commentary on \"towards a noncommutative arithmetic-geometric mean\ninequality\" by b. recht and c. re. 2012.\n[Dur06]\nR. Durrett. Random Graph Dynamics (Cambridge Series in Statistical and Probabilistic\nMathematics). Cambridge University Press, New York, NY, USA, 2006.\n\n[DVPS14]\nA. G. D'yachkov, I. V. Vorob'ev, N. A. Polyansky, and V. Y. Shchukin. Bounds on the\nrate of disjunctive codes. Problems of Information Transmission, 2014.\n[EH89]\nP. Erdos and A. Hajnal. Ramsey-type theorems. Discrete Applied Mathematics, 25, 1989.\n[F+14]\nY. Filmus et al. Real analysis in computer science: A collection of open problems. Available\nonline at http: // simons. berkeley. edu/ sites/ default/ files/ openprobsmerged.\npdf , 2014.\n[Fei05]\nU. Feige. On sums of independent random variables with unbounded variance, and esti-\nmating the average degree in a graph. 2005.\n[FP06]\nD. F eral and S. P ech e. The largest eigenvalue of rank one deformation of large wigner\nmatrices. Communications in Mathematical Physics, 272(1):185-228, 2006.\n[FR13]\nS. Foucart and H. Rauhut.\nA Mathematical Introduction to Compressive Sensing.\nBirkhauser, 2013.\n[Fuc04]\nJ. J. Fuchs. On sparse representations in arbitrary redundant bases. Information Theory,\nIEEE Transactions on, 50(6):1341-1344, 2004.\n[Fur96]\nZ. Furedia. On r-cover-free families. Journal of Combinatorial Theory, Series A, 1996.\n[Gil52]\nE. N. Gilbert.\nA comparison of signalling alphabets.\nBell System Technical Journal,\n31:504-522, 1952.\n[GK06]\nA. Giridhar and P.R. Kumar. Distributed clock synchronization over wireless networks:\nAlgorithms and analysis. In Decision and Control, 2006 45th IEEE Conference on, pages\n4915-4920. IEEE, 2006.\n[GLV07]\nN. Gvozdenovic, M. Laurent, and F. Vallentin. Block-diagonal semidefinite programming\nhierarchies for 0/1 programming. Available online at arXiv:0712.3079 [math.OC], 2007.\n[Gol96]\nG. H. Golub. Matrix Computations. Johns Hopkins University Press, third edition, 1996.\n[Gor85]\nY. Gordon. Some inequalities for gaussian processes and applications. Israel J. Math,\n50:109-110, 1985.\n[Gor88]\nY. Gordon. On milnan's inequality and random subspaces which escape through a mesh\nin Rn. 1988.\n[GRS15]\nV. Guruswami, A. Rudra, and M. Sudan. Essential Coding Theory. Available at: http:\n//www.cse.buffalo.edu/faculty/atri/courses/coding-theory/book/, 2015.\n[GW95]\nM. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum\ncut and satisfiability problems using semidefine programming. Journal of the Association\nfor Computing Machinery, 42:1115-1145, 1995.\n\n[GZC+15]\nAmir Ghasemian, Pan Zhang, Aaron Clauset, Cristopher Moore, and Leto Peel.\nDe-\ntectability thresholds and optimal algorithms for community structure in dynamic net-\nworks. Available online at arXiv:1506.06179 [stat.ML], 2015.\n[Haa87]\nU. Haagerup. A new upper bound for the complex Grothendieck constant. Israel Journal\nof Mathematics, 60(2):199-224, 1987.\n[Has02]\nJ. Hastad. Some optimal inapproximability results. 2002.\n[Hen13]\nR. Henderson. Avoiding the pitfalls of single particle cryo-electron microscopy: Einstein\nfrom noise. Proceedings of the National Academy of Sciences, 110(45):18037-18041, 2013.\n[HJ85]\nR. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.\n[HMPW]\nT. Holenstein, T. Mitzenmacher, R. Panigrahy, and U. Wieder. Trace reconstruction with\nconstant deletion probability and related results. In Proceedings of the Nineteenth Annual\nACM-SIAM.\n[HMT09]\nN. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: proba-\nbilistic algorithms for constructing approximate matrix decompositions. Available online\nat arXiv:0909.4061v2 [math.NA], 2009.\n[HR]\nI. Haviv and O. Regev. The restricted isometry property of subsampled fourier matrices.\nSODA 2016.\n[HWX14]\nB. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite\nprogramming. Available online at arXiv:1412.6156, 2014.\n[HWX15]\nB. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite\nprogramming: Extensions. Available online at arXiv:1502.07738, 2015.\n[IKW14]\nA. Israel, F. Krahmer, and R. Ward. An arithmetic-geometric mean inequality for prod-\nucts of three matrices. Available online at arXiv:1411.0333 [math.SP], 2014.\n[IMPV15a] T. Iguchi, D. G. Mixon, J. Peterson, and S. Villar. On the tightness of an sdp relaxation\nof k-means. Available online at arXiv:1505.04778 [cs.IT], 2015.\n[IMPV15b] T. Iguchi, D. G. Mixon, J. Peterson, and S. Villar. Probably certifiably correct k-means\nclustering. Available at arXiv, 2015.\n[JL84]\nW. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.\nIn Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26\nof Contemporary Mathematics, pages 189-206. American Mathematical Society, 1984.\n[Joh01]\nI. M. Johnston. On the distribution of the largest eigenvalue in principal components\nanalysis. The Annals of Statistics, 29(2):295-327, 2001.\n[Kar05]\nN. E. Karoui. Recent results about the largest eigenvalue of random covariance matrices\nand statistical application. Acta Physica Polonica B, 36(9), 2005.\n\n[Kho02]\nS. Khot. On the power of unique 2-prover 1-round games. Thiry-fourth annual ACM\nsymposium on Theory of computing, 2002.\n[Kho10]\nS. Khot. On the unique games conjecture (invited survey). In Proceedings of the 2010\nIEEE 25th Annual Conference on Computational Complexity, CCC '10, pages 99-121,\nWashington, DC, USA, 2010. IEEE Computer Society.\n[KKMO05] S. Khot, G. Kindler, E. Mossel, and R. O'Donnell. Optimal inapproximability results for\nmax-cut and other 2-variable csps? 2005.\n[KV13]\nS. A. Khot and N. K. Vishnoi.\nThe unique games conjecture, integrality gap for\ncut problems and embeddability of negative type metrics into l1.\nAvailable online at\narXiv:1305.4581 [cs.CC], 2013.\n[KW92]\nJ. Kuczynski and H. Wozniakowski.\nEstimating the largest eigenvalue by the power\nand lanczos algorithms with a random start.\nSIAM Journal on Matrix Analysis and\nApplications, 13(4):1094-1122, 1992.\n[Las01]\nJ. B. Lassere. Global optimization with polynomials and the problem of moments. SIAM\nJournal on Optimization, 11(3):796-817, 2001.\n[Lat05]\nR. Lata la.\nSome estimates of norms of random matrices.\nProc. Amer. Math. Soc.,\n133(5):1273-1282 (electronic), 2005.\n[LGT12]\nJ.R. Lee, S.O. Gharan, and L. Trevisan. Multi-way spectral partitioning and higher-order\ncheeger inequalities. STOC '12 Proceedings of the forty-fourth annual ACM symposium\non Theory of computing, 2012.\n[Llo82]\nS. Lloyd. Least squares quantization in pcm. IEEE Trans. Inf. Theor., 28(2):129-137,\n1982.\n[LM00]\nB. Laurent and P. Massart.\nAdaptive estimation of a quadratic functional by model\nselection. Ann. Statist., 2000.\n[Lov79]\nL. Lovasz. On the shannon capacity of a graph. IEEE Trans. Inf. Theor., 25(1):1-7, 1979.\n[LRTV12]\nA. Louis, P. Raghavendra, P. Tetali, and S. Vempala. Many sparse cuts via higher eigen-\nvalues. STOC, 2012.\n[Lyo14]\nR. Lyons. Factors of IID on trees. Combin. Probab. Comput., 2014.\n[Mas00]\nP. Massart. About the constants in Talagrand's concentration inequalities for empirical\nprocesses. The Annals of Probability, 28(2), 2000.\n[Mas14]\nL. Massouli e. Community detection thresholds and the weak ramanujan property. In\nProceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC '14,\npages 694-703, New York, NY, USA, 2014. ACM.\n\n[Mek14]\nR.\nMeka.\nWindows\non\nTheory\nBLOG:\nDiscrepancy\nand\nBeat-\ning\nthe\nUnion\nBound.\nhttp: // windowsontheory. org/ 2014/ 02/ 07/\ndiscrepancy-and-beating-the-union-bound/ , 2014.\n[Mit09]\nM. Mitzenmacher. A survey of results for deletion channels and related synchronization\nchannels. Probability Surveys, 2009.\n[Mix14a]\nD. G. Mixon.\nExplicit matrices with the restricted isometry property: Breaking the\nsquare-root bottleneck. available online at arXiv:1403.3427 [math.FA], 2014.\n[Mix14b]\nD. G. Mixon. Short, Fat matrices BLOG: Gordon's escape through a mesh theorem. 2014.\n[Mix14c]\nD. G. Mixon. Short, Fat matrices BLOG: Gordon's escape through a mesh theorem. 2014.\n[Mix15]\nD. G. Mixon. Applied harmonic analysis and sparse approximation. Short, Fat Matrices\nWeb blog, 2015.\n[MM15]\nC. Musco and C. Musco. Stronger and faster approximate singular value decomposition\nvia the block lanczos method. Available at arXiv:1504.05477 [cs.DS], 2015.\n[MNS14a]\nE. Mossel, J. Neeman, and A. Sly.\nA proof of the block model threshold conjecture.\nAvailable online at arXiv:1311.4115 [math.PR], January 2014.\n[MNS14b]\nE. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. Probability\nTheory and Related Fields (to appear), 2014.\n[Mon14]\nA. Montanari. Principal component analysis with nonnegativity constraints. http: //\nsublinear. info/ index. php? title= Open_ Problems: 62 , 2014.\n[Mos11]\nM. S. Moslehian. Ky Fan inequalities. Available online at arXiv:1108.1467 [math.FA],\n2011.\n[MP67]\nV. A. Marchenko and L. A. Pastur. Distribution of eigenvalues in certain sets of random\nmatrices. Mat. Sb. (N.S.), 72(114):507-536, 1967.\n[MR14]\nA. Montanari and E. Richard. Non-negative principal component analysis: Message pass-\ning algorithms and sharp asymptotics.\nAvailable online at arXiv:1406.4775v1 [cs.IT],\n2014.\n[MS15]\nA. Montanari and S. Sen. Semidefinite programs on sparse random graphs. Available\nonline at arXiv:1504.05910 [cs.DM], 2015.\n[MSS15a]\nA. Marcus, D. A. Spielman, and N. Srivastava. Interlacing families i: Bipartite ramanujan\ngraphs of all degrees. Annals of Mathematics, 2015.\n[MSS15b]\nA. Marcus, D. A. Spielman, and N. Srivastava. Interlacing families ii: Mixed characteristic\npolynomials and the kadison-singer problem. Annals of Mathematics, 2015.\n[MZ11]\nS. Mallat and O. Zeitouni. A conjecture concerning optimality of the karhunen-loeve basis\nin nonlinear reconstruction. Available online at arXiv:1109.0489 [math.PR], 2011.\n\n[Nel]\nJ. Nelson.\nJohnson-lindenstrauss notes.\nhttp: // web. mit. edu/ minilek/ www/ jl_\nnotes. pdf .\n[Nes00]\nY. Nesterov. Squared functional systems and optimization problems. High performance\noptimization, 13(405-440), 2000.\n[Nik13]\nA. Nikolov.\nThe komlos conjecture holds for vector colorings.\nAvailable online at\narXiv:1301.4039 [math.CO], 2013.\n[NN]\nJ. Nelson and L. Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser\nsubspace embeddings. Available at arXiv:1211.1002 [cs.DS].\n[NPW14]\nJ. Nelson, E. Price, and M. Wootters.\nNew constructions of RIP matrices with fast\nmultiplication and fewer rows. SODA, pages 1515-1528, 2014.\n[NSZ09]\nB. Nadler, N. Srebro, and X. Zhou. Semi-supervised learning with the graph laplacian:\nThe limit of infinite unlabelled data. 2009.\n[NW13]\nA. Nellore and R. Ward. Recovery guarantees for exemplar-based clustering. Available\nonline at arXiv:1309.3256v2 [stat.ML], 2013.\n[Oli10]\nR. I. Oliveira. The spectrum of random k-lifts of large graphs (with possibly large k).\nJournal of Combinatorics, 2010.\n[Par00]\nP. A. Parrilo. Structured semidefinite programs and semialgebraic geometry methods in\nrobustness and optimization. PhD thesis, 2000.\n[Pau]\nD. Paul. Asymptotics of the leading sample eigenvalues for a spiked covariance model.\nAvailable online at http: // anson. ucdavis. edu/ ~ debashis/ techrep/ eigenlimit.\npdf .\n[Pau07]\nD. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance\nmodel. Statistics Sinica, 17:1617-1642, 2007.\n[Pea01]\nK. Pearson. On lines and planes of closest fit to systems of points in space. Philosophical\nMagazine, Series 6, 2(11):559-572, 1901.\n[Pis03]\nG. Pisier.\nIntroduction to operator space theory, volume 294 of London Mathematical\nSociety Lecture Note Series. Cambridge University Press, Cambridge, 2003.\n[Pis11]\nG. Pisier. Grothendieck's theorem, past and present. Bull. Amer. Math. Soc., 49:237-323,\n2011.\n[PW15]\nW. Perry and A. S. Wein. A semidefinite program for unbalanced multisection in the\nstochastic block model. Available online at arXiv:1507.05605 [cs.DS], 2015.\n[QSW14]\nQ. Qu, J. Sun, and J. Wright. Finding a sparse vector in a subspace: Linear sparsity\nusing alternating directions. Available online at arXiv:1412.4659v1 [cs.IT], 2014.\n\n[Rag08]\nP. Raghavendra. Optimal algorithms and inapproximability results for every CSP? In\nProceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC '08,\npages 245-254. ACM, 2008.\n[Ram28]\nF. P. Ramsey. On a problem of formal logic. 1928.\n[Rec11]\nB. Recht. A simpler approach to matrix completion. Journal of Machine Learning Re-\nsearch, 12:3413-3430, 2011.\n[RR12]\nB. Recht and C. Re. Beneath the valley of the noncommutative arithmetic-geometric\nmean inequality: conjectures, case-studies, and consequences. Conference on Learning\nTheory (COLT), 2012.\n[RS60]\nI. S. Reed and G. Solomon. Polynomial codes over certain finite fields. Journal of the\nSociety for Industrial and Applied Mathematics (SIAM), 8(2):300-304, 1960.\n[RS10]\nP. Raghavendra and D. Steurer.\nGraph expansion and the unique games conjecture.\nSTOC, 2010.\n[RS13]\nS. Riemer and C. Schutt.\n\nOn the expectation of the norm of random matrices with\nnon-identically distributed entries. Electron. J. Probab., 18, 2013.\n[RST09]\nV. Rokhlin, A. Szlam, and M. Tygert. A randomized algorithm for principal component\nanalysis. Available at arXiv:0809.2274 [stat.CO], 2009.\n[RST12]\nP. Raghavendra, D. Steurer, and M. Tulsiani. Reductions between expansion problems.\nIEEE CCC, 2012.\n[RV08]\nM. Rudelson and R. Vershynin. On sparse reconstruction from Fourier and Gaussian\nmeasurements. Comm. Pure Appl. Math., 61:1025-1045, 2008.\n[RW01]\nJ. Rubinstein and G. Wolansky. Reconstruction of optical surfaces from ray data. Optical\nReview, 8(4):281-283, 2001.\n[Sam66]\nS. M. Samuels. On a chebyshev-type inequality for sums of independent random variables.\nAnn. Math. Statist., 1966.\n[Sam68]\nS. M. Samuels. More on a chebyshev-type inequality. 1968.\n[Sam69]\nS. M. Samuels. The markov inequality for sums of independent random variables. Ann.\nMath. Statist., 1969.\n[Sch12]\nK. Schmudgen. Around hilbert's 17th problem. Documenta Mathematica - Extra Volume\nISMP, pages 433-438, 2012.\n[Seg00]\nY. Seginer. The expected norm of random matrices. Combin. Probab. Comput., 9(2):149-\n166, 2000.\n[SG10]\nA. J. Scott and M. Grassl. Sic-povms: A new computer study. J. Math. Phys., 2010.\n\n[Sha56]\nC. E. Shannon. The zero-error capacity of a noisy channel. IRE Transactions on Infor-\nmation Theory, 2, 1956.\n[SHBG09]\nM. Shatsky, R. J. Hall, S. E. Brenner, and R. M. Glaeser. A method for the alignment of\nheterogeneous macromolecules from electron microscopy. Journal of Structural Biology,\n166(1), 2009.\n[Sho87]\nN. Shor. An approach to obtaining global extremums in polynomial mathematical pro-\ngramming problems. Cybernetics and Systems Analysis, 23(5):695-700, 1987.\n[Sin11]\nA. Singer. Angular synchronization by eigenvectors and semidefinite programming. Appl.\nComput. Harmon. Anal., 30(1):20 - 36, 2011.\n[Spe75]\nJ. Spencer. Ramsey's theorem - a new lower bound. J. Combin. Theory Ser. A, 1975.\n[Spe85]\nJ. Spencer. Six standard deviations suffice. Trans. Amer. Math. Soc., (289), 1985.\n[Spe94]\nJ. Spencer. Ten Lectures on the Probabilistic Method: Second Edition. SIAM, 1994.\n[SS11]\nA. Singer and Y. Shkolnisky. Three-dimensional structure determination from common\nlines in Cryo-EM by eigenvectors and semidefinite programming. SIAM J. Imaging Sci-\nences, 4(2):543-572, 2011.\n[Ste74]\nG. Stengle. A nullstellensatz and a positivstellensatz in semialgebraic geometry. Math.\nAnn. 207, 207:87-97, 1974.\n[SW11]\nA. Singer and H.-T. Wu. Orientability and diffusion maps. Appl. Comput. Harmon. Anal.,\n31(1):44-58, 2011.\n[SWW12]\nD. A Spielman, H. Wang, and J. Wright. Exact recovery of sparsely-used dictionaries.\nCOLT, 2012.\n[Tal95]\nM. Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.\nInst. Hautes Etudes Sci. Publ. Math., (81):73-205, 1995.\n[Tao07]\nT. Tao. What's new blog: Open question: deterministic UUP matrices. 2007.\n[Tao12]\nT. Tao. Topics in Random Matrix Theory. Graduate studies in mathematics. American\nMathematical Soc., 2012.\n[TdSL00]\nJ. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for\nnonlinear dimensionality reduction. Science, 290(5500):2319-2323, 2000.\n[TP13]\nA. M. Tillmann and M. E. Pfefsch.\nThe computational complexity of the restricted\nisometry property, the nullspace property, and related concepts in compressed sensing.\n2013.\n[Tre11]\nL. Trevisan. in theory BLOG: CS369G Llecture 4: Spectral Partitionaing. 2011.\n\n[Tro05]\nJ. A. Tropp. Recovery of short, complex linear combinations via l1 minimization. IEEE\nTransactions on Information Theory, 4:1568-1570, 2005.\n[Tro12]\nJ. A. Tropp.\nUser-friendly tail bounds for sums of random matrices.\nFoundations of\nComputational Mathematics, 12(4):389-434, 2012.\n[Tro15a]\nJ. A. Tropp. The expected norm of a sum of independent random matrices: An elementary\napproach. Available at arXiv:1506.04711 [math.PR], 2015.\n[Tro15b]\nJ. A. Tropp.\nAn introduction to matrix concentration inequalities.\nFoundations and\nTrends in Machine Learning, 2015.\n[Tro15c]\nJ. A. Tropp. Second-order matrix concentration inequalities. In preparation, 2015.\n[Var57]\nR. R. Varshamov. Estimate of the number of signals in error correcting codes. Dokl.\nAcad. Nauk SSSR, 117:739-741, 1957.\n[VB96]\nL. Vanderberghe and S. Boyd. Semidefinite programming. SIAM Review, 38:49-95, 1996.\n[VB04]\nL. Vanderberghe and S. Boyd. Convex Optimization. Cambridge University Press, 2004.\n[vH14]\nR. van Handel.\nProbability in high dimensions.\nORF 570 Lecture Notes, Princeton\nUniversity, 2014.\n[vH15]\nR. van Handel. On the spectral norm of inhomogeneous random matrices. Available online\nat arXiv:1502.05003 [math.PR], 2015.\n[Yam54]\nK. Yamamoto. Logarithmic order of free distributive lattice. Journal of the Mathematical\nSociety of Japan, 6:343-353, 1954.\n[ZB09]\nL. Zdeborova and S. Boettcher. Conjecture on the maximum cut and bisection width in\nrandom regular graphs. Available online at arXiv:0912.4861 [cond-mat.dis-nn], 2009.\n[Zha14]\nT. Zhang. A note on the non-commutative arithmetic-geometric mean inequality. Avail-\nable online at arXiv:1411.5058 [math.SP], 2014.\n[ZMZ14]\nPan Zhang, Cristopher Moore, and Lenka Zdeborova. Phase transitions in semisupervised\nclustering of sparse networks. Phys. Rev. E, 90, 2014.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 1: Overview and Two Open Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/594e3ae91cc8e865f25d07dcbd2dd460_MIT18_S096F15_Ses1.pdf",
      "content": "Overview\nThis will be a mostly self-contained research-oriented course designed for undergraduate students\n(but also extremely welcoming to graduate students) with an interest in doing research in theoretical\naspects of algorithms that aim to extract information from data. These often lie in overlaps of\ntwo or more of the following: Mathematics, Applied Mathematics, Computer Science, Electrical\nEngineering, Statistics, and/or Operations Research.\nThe topics covered include:\n1. Principal Component Analysis (PCA) and some random matrix theory that will be used to\nunderstand the performance of PCA in high dimensions, through spike models.\n2. Manifold Learning and Diffusion Maps: a nonlinear dimension reduction tool, alternative to\nPCA. Semisupervised Learning and its relations to Sobolev Embedding Theorem.\n3. Spectral Clustering and a guarantee for its performance: Cheeger's inequality.\n4. Concentration of Measure and tail bounds in probability, both for scalar variables and matrix\nvariables.\n5. Dimension reduction through Johnson-Lindenstrauss Lemma and Gordon's Escape Through\na Mesh Theorem.\n6. Compressed Sensing/Sparse Recovery, Matrix Completion, etc. If time permits, I will present\nNumber Theory inspired constructions of measurement matrices.\n7. Group Testing. Here we will use combinatorial tools to establish lower bounds on testing\nprocedures and, if there is time, I might give a crash course on Error-correcting codes and\nshow a use of them in group testing.\n8. Approximation algorithms in Theoretical Computer Science and the Max-Cut problem.\n9. Clustering on random graphs: Stochastic Block Model. Basics of duality in optimization.\n10. Synchronization, inverse problems on graphs, and estimation of unknown variables from pair-\nwise ratios on compact groups.\n11. Some extra material may be added, depending on time available.\nOpen Problems\nA couple of open problems will be presented at the end of most lectures. They won't necessarily\nbe the most important problems in the field (although some will be rather important), I have tried\nto select a mix of important, approachable, and fun problems. In fact, I take the opportunity to\npresent two problems below.\n0.2.1 Koml os Conjecture\nWestartwithafascinatingprobleminDiscrepancyTheory.\nOpen Problem 0.1 (Koml os Conjecture) Given n, let K(n) denote the infimum over all real\nn\nnumbers such that: for all set of n vectors u1, . . . , un ∈ R satisfying luil2 ≤ 1, there exist signs\nEi =±1suchthat\nlE1u1 +E2u2 +· · ·+Enunlinf ≤K(n).\n\nThere exists a universal constant K such that K(n) ≤ K for all n.\nAn early reference for this conjecture is a book by Joel Spencer [Spe94]. This conjecture is\ntightly connected to Spencer's famous Six Standard Deviations Suffice Theorem [Spe85]. Later in\nthe course we will study semidefinite programming relaxations, recently it was shown that a certain\nsemidefinite relaxation of this conjecture holds [Nik13], the same paper also has a good accounting\nof partial progress on the conjecture.\n√\n- It is not so difficult to show that K(n) ≤\nn, try it!\n0.4.2\nMatrix AM-GM inequality\nWe move now to an interesting generalization of arithmetic-geometric means inequality, which has\napplications on understanding the difference in performance of with- versus without-replacement\nsampling in certain randomized algorithms (see [RR12]).\nOpen Problem 0.2 For any collection of d × d positive semidefinite matrices A1, · · · , An, the\nfollowing is true:\n(a)\n\n≤\n\nn\nn\nk1,...,kn=1 j=1\nAkj\n\nn\nj=1\n\nnn\nAσ(j)\n,\nn! σ∈Sym(n)\nand\n(b)\n\nn\n\nn\nj=1\n\nn\nk1,...,kn=1\n≤\nn\nn\n\nAσ(j)\nAkj ,\nn!\nj=1\nσ∈Sym(n)\nwhere Sym(n) denotes the group of permutations of n elements, and 1 · 1 the spectral norm.\nMorally, these conjectures state that products of matrices with repetitions are larger than\nwithout. For more details on the motivations of these conjecture (and their formulations) see [RR12]\nfor conjecture (a) and [Duc12] for conjecture (b).\nRecently these conjectures have been solved for the particular case of n = 3, in [Zha14] for (a)\nand in [IKW14] for (b).\nReferences\n[Duc12] J. C. Duchi. Commentary on \"towards a noncommutative arithmetic-geometric mean\ninequality\" by b. recht and c. re. 2012.\n\n[IKW14] A. Israel, F. Krahmer, and R. Ward. An arithmetic-geometric mean inequality for prod\nucts of three matrices. Available online at arXiv:1411.0333 [math.SP], 2014.\n[Nik13]\nA. Nikolov.\nThe komlos conjecture\narXiv:1301.4039 [math.CO], 2013.\nholds for vector colorings.\nAvailable online at\n[RR12]\nB. Recht and C. Re. Beneath the valley of the noncommutative arithmetic-geomet\nmean inequality: conjectures, case-studies, and consequences. Conference on Learni\nTheory (COLT), 2012.\nric\nng\n[Spe85]\nJ. Spencer. Six standard deviations suffice. Trans. Amer. Math. Soc., (289), 1985.\n[Spe94]\nJ. Spencer. Ten Lectures on the Probabilistic Method: Second Edition. SIAM, 1994.\n[Zha14] T. Zhang. A note on the non-commutative arithmetic-geometric mean inequality. Ava\nable online at arXiv:1411.5058 [math.SP], 2014.\nil\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 2-4: Principal Component Analysis in High Dimensions and the Spike Model",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/4c9fa7ce658a63174f562fdf44b55626_MIT18_S096F15_Ses2_4.pdf",
      "content": "Principal Component Analysis in High Dimensions and the Spike\nModel\n1.1\nDimension Reduction and PCA\nWhen faced with a high dimensional dataset, a natural approach is to try to reduce its dimension,\neither by projecting it to a lower dimension space or by finding a better representation for the data.\nDuring this course we will see a few different ways of doing dimension reduction.\nWe will start with Principal Component Analysis (PCA). In fact, PCA continues to be one of the\nbest (and simplest) tools for exploratory data analysis. Remarkably, it dates back to a 1901 paper by\nKarl Pearson [Pea01]!\nLet's say we have n data points x1, . . . , xn in Rp, for some p, and we are interested in (linearly)\nprojecting the data to d < p dimensions. This is particularly useful if, say, one wants to visualize\nthe data in two or three dimensions. There are a couple of different ways we can try to choose this\nprojection:\n1. Finding the d-dimensional affine subspace for which the projections of x1, . . . , xn on it best\napproximate the original points x1, . . . , xn.\n2. Finding the d dimensional projection of x1, . . . , xn that preserved as much variance of the data\nas possible.\nAs we will see below, these two approaches are equivalent and they correspond to Principal Com-\nponent Analysis.\nBefore proceeding, we recall a couple of simple statistical quantities associated with x1, . . . , xn,\nthat will reappear below.\nGiven x1, . . . , xn we define its sample mean as\nμn = 1 X\nn\nxk,\n(4)\nn k=1\nand its sample covariance as\nΣn =\n(\nn\nX\nn\nxk\n(xk\n-μn)\n-\nT\nμ\n-\nn) .\n(5)\nk=1\nRemark 1.1 If x1, . . . , xn are independently sampled from a distribution, μn and Σn are unbiased\nestimators for, respectively, the mean and covariance of the distribution.\nWe will start with the first interpretation of PCA and then show that it is equivalent to the second.\n1.1.1\nPCA as best d-dimensional affine fit\nWe are trying to approximate each xk by\nd\nxk ≈μ +\nX\n(βk)i vi,\n(6)\ni=1\n\nwhere v1, . . . , vd is an orthonormal basis for the d-dimensional subspace, μ ∈Rp represents the transla-\ntion, and βk corresponds to the coefficients of xk. If we represent the subspace by V = [v\np\n1 · · · vd] ∈R ×d\nthen we can rewrite (7) as\nxk ≈μ + V βk,\n(7)\nwhere V T V = Id×d as the vectors vi are orthonormal.\nWe will measure goodness of fit in terms of least squares and attempt to solve\nmin\nX\nn\n∥\nxk -(μ + V βk)\nμ, V, βk\n∥2\n(8)\nT\nk=1\nV\nV =I\nWe start by optimizing for μ. It is easy to see that the first order conditions for μ correspond to\nn\n∇μ\nX\nn\n∥xk -(μ + V βk)∥2\n2 = 0 ⇔\nX\n(xk -(μ + V βk)) = 0.\nk=1\nk=1\nThus, the optimal value μ∗of μ satisfies\nX\nn\nn\nxk\n-nμ∗-V\nX\nβk\n= 0.\nk=1\nk=1\nBecause Pn\nk=1 βk = 0 we have that the optimal μ is given by\nn\nμ∗=\nX\nxk = μn,\nn k=1\nthe sample mean.\nWe can then proceed on finding the solution for (9) by solving\nn\nmin\nk\nμn\nV\nV, βk\n∥x -\n-\nβk∥2\n2 .\n(9)\nV T\nk=1\nV =I\nX\nLet us proceed by optimizing for βk. Since the problem decouples for each k, we can focus on, for\neach k,\nd\nmin ∥xk -μn -V βk∥2 = min\n\nxk -μn\n(\nβ\n-\n.\n(10)\nk\nβ\nX\nβk)i vi\nk\ni=1\n\nSince v1, . . . , v\n\nd are orthonormal, it is easy to see that\n\nthe solution is given\n\nby (βk\n∗) = vT\ni (xk -μn)\ni\nwhich can be succinctly written as β\nT\nk = V\n(xk -μn). Thus, (9) is equivalent to\nX\nn\nmin\nn)\nV T V =I\n(xk -μ\n-\nV V T (xk -μn)\nk=1\n.\n(11)\n!\n\n!\n\nNote that\n(xk -μn) -V V T (xk -\nT\nμn)\n\n=\n(xk\n-μn) (xk -μn)\n-2 (xk -\nT\nμn) V V T\n(xk\nμ\nT\n-\nn)\n+ (x -μ ) V\nV T V\nV T\nk\nn\n(xk -μn)\n=\n(xk -\nT\nμn) (xk -μn)\n-(xk -\nT\nμn) V V T (xk -μn) .\nSince (xk -\nT\nμn) (xk -μn) does not depend on V , minimizing (9) is equivalent to\nn\nT\nmax\nX\n(xk -μn) V V T (xk -μn) .\n(12)\nV T V =I k=1\nA few more simple algebraic manipulations using properties of the trace:\nX\nn\nn\nT\nT\n(xk -μn) V V T (xk -μn)\n=\nX\nTr\nh\n(x\nV V T\nk -μn)\n(xk -μn)\nk=1\nk=1\nn\ni\n=\nX\n-\nT\nTr\nh\nV T (xk\nμn) (xk -μn) V\nk=1\ni\n=\nTr\n\"\nn\nV T X\n(xk -\nT\nμn) (xk -μn) V\nk=1\n#\n=\n(n -1) Tr\n\nV T ΣnV\ny\n\n.\nThis means that the solution to (13) is given b\nmax Tr\n\nV T ΣnV\n.\nV T V =I\n\n(13)\nAs we saw above (recall (2)) the solution is given by V = [v1, · · · , vd] where v1, . . . , vd correspond\nto the d leading eigenvectors of Σn.\nLet us first show that interpretation (2) of finding the d-dimensional projection of x1, . . . , xn that\npreserves the most variance also arrives to the optimization problem (13).\n1.1.2\nPCA as d-dimensional projection that preserves the most variance\nWe aim to find an orthonormal basis v1, . . . , v\nT\nd (organized as V = [v1, . . . , vd] with V\nV = Id×d) of\na d-dimensional space such that the projection of x1, . . . , xn projected on this subspace has the most\nvariance. Equivalently we can ask for the points\n\nvT\n\n1 xk\n.\n\nn\n\n..\nvT\nd xk\n\n,\nk=1\n\nto have as much variance as possible. Hence, we are interested in solving\nmax\nX\nn\nX\nn\nV T\nT\n\nxk -\nV\nxr\n.\nV T V =I\nn\nk=1\nr=1\n\n(14)\nNote that\nX\nn\nn\n\nn\n\nT\nV T x\nT\nT\nk -\nX\nV\nxr\n\n=\nX\nV\n(xk -μn)\n= Tr V\nΣnV\n,\nn\nk=1\nr=1\nk=1\nshowing that (14) is equivalent to (13) and\n\nthat the\n\ntwo interpretations\n\nof\nPCA\n\nare indeed equivalent.\n1.1.3\nFinding the Principal Components\nWhen given a dataset x1, . . . , xn ∈Rp, in order to compute the Principal Components one needs to\nfind the leading eigenvectors of\nn\nT\nΣn =\n(\n-\nX\nxk\nμ\nn\n-\nn) (xk -μn) .\nk=1\nA naive way of doing this would be to construct Σn (which takes O(np2) work) and then finding its\nspectral decomposition (which takes O(p3) work). This means that the computational complexity of\nthis procedure is\nmax np2, p3\n(see [HJ85] and/or [Gol96]).\nO\n\nAn alternative is to use the Singular Value Decomposition (1). Let X = [x1 · · · xn] recall that,\nΣn =\nX -\nT\nμ\nT\nT\nn1\nX -μn1\n.\nn\nLet us take the SVD of X -μ 1T = U DUT with\n\nn\nL\nR\nUL ∈O(p), D diagonal, and UT\nRUR = I. Then,\nT\nΣn =\nX -μ 1T X -μ 1T\n= U DUT U DUT = U D2\nT\nn\nn\nL\nR\nR\nL\nL\nU\nn\nL ,\nmeaning that UL correspond to the eigenvectors of Σ\nT\nn.\nComputing the SVD of X -μn1\ntakes\nO(min n2p, p2n) but if one is interested in simply computing the top d eigenvectors then this compu-\ntational costs reduces to O(dnp). This can be further improved with randomized algorithms. There\nare randomized algorithms that compute an approximate solution in\nO\npn log d + (p + n)d2\ntime\n(see for example [HMT09, RST09, MM15]).\n1.1.4\nWhich d should we pick?\nGiven a dataset, if the objective is to visualize it then picking d = 2 or d = 3 might make the\nmost sense. However, PCA is useful for many other purposes, for example: (1) often times the data\nbelongs to a lower dimensional space but is corrupted by high dimensional noise. When using PCA\nit is oftentimess possible to reduce the noise while keeping the signal. (2) One may be interested\nin running an algorithm that would be too computationally expensive to run in high dimensions,\n1If there is time, we might discuss some of these methods later in the course.\n\ndimension reduction may help there, etc. In these applications (and many others) it is not clear how\nto pick d.\n(+)\nIf we denote the k-th largest eigenvalue of Σn as λ\n(Σ\nk\nn), then the k-th principal component has\n(+)\nλ\n(Σ\na\nn)\nk\nproportion of the variance. 2\nTr(Σn)\nA fairly popular heuristic is to try to choose the cut-offat a component that has significantly more\nvariance than the one immediately after. This is usually visualized by a scree plot: a plot of the values\nof the ordered eigenvalues. Here is an example:\nIt is common to then try to identify an \"elbow\" on the scree plot to choose the cut-off. In the\nnext Section we will look into random matrix theory to try to understand better the behavior of the\neigenvalues of Σn and it will help us understand when to cut-off.\n1.1.5\nA related open problem\nWe now show an interesting open problem posed by Mallat and Zeitouni at [MZ11]\nOpen Problem 1.1 (Mallat and Zeitouni [MZ11]) Let g ∼N(0, Σ) be a gaussian random vector\nin Rp with a known covariance matrix Σ and d < p. Now, for any orthonormal basis V = [v1, . . . , vp]\nof Rp, consider the following random variable ΓV : Given a draw of the random vector g, ΓV is the\nsquared l2 norm of the largest projection of g on a subspace generated by d elements of the basis V .\nThe question is:\nWhat is the basis V for which E [ΓV ] is maximized?\n2Note that Tr (Σn) = Pp\nk=1 λk (Σn).\n\nThe conjecture in [MZ11] is that the optimal basis is the eigendecomposition of Σ. It is known\nthat this is the case for d = 1 (see [MZ11]) but the question remains open for d > 1. It is not very\ndifficult to see that one can assume, without loss of generality, that Σ is diagonal.\nA particularly intuitive way of stating the problem is:\n1. Given Σ ∈Rp×p and d\n2. Pick an orthonormal basis v1, . . . , vp\n3. Given g ∼N(0, Σ)\n4. Pick d elements v 1, . . . , v d of the basis\nd\n5. Score: P\ni=1\nv T\ni g\nThe objective is to pic\n\nk the basis in order to maximize the expected value of the Score.\nNotice that if the steps of the procedure were taken in a slightly different order on which step\n4 would take place before having access to the draw of g (step 3) then the best basis is indeed\nthe eigenbasis of Σ and the best subset of the basis is simply the leading eigenvectors (notice the\nresemblance with PCA, as described above).\nMore formally, we can write the problem as finding\n\nargmax\nE\nmax\nvT\ni g\n,\nV ∈Rp×p\n\nV T V =I\n\nS\n[p\n|S\n⊂\n]\n\n|=d\nX\ni∈S\n\nwhere g ∼N(0, Σ). The observation regarding the different ordering of the steps amounts to saying\nthat the eigenbasis of Σ is the optimal solution for\n\ni g\nargmax\nmax E\nX vT\n.\np\nS⊂[p]\nV ∈Rp×\nS|\ni\n=d\n∈S\nV T V =I\n|\n\n\"\n#\n\n1.2\nPCA in high dimensions and Marcenko-Pastur\nLet us assume that the data points x\np\n1, . . . , xn ∈R\nare independent draws of a gaussian random\nvariable g ∼N(0, Σ) for some covariance Σ ∈Rp×p. In this case when we use PCA we are hoping\nto find low dimensional structure in the distribution, which should correspond to large eigenvalues of\nΣ (and their corresponding eigenvectors). For this reason (and since PCA depends on the spectral\nproperties of Σn) we would like to understand whether the spectral properties of Σn (eigenvalues and\neigenvectors) are close to the ones of Σ.\nSince EΣn = Σ, if p is fixed and n →infthe law of large numbers guarantees that indeed Σn →Σ.\nHowever, in many modern applications it is not uncommon to have p in the order of n (or, sometimes,\neven larger!). For example, if our dataset is composed by images then n is the number of images and\np the number of pixels per image; it is conceivable that the number of pixels be on the order of the\nnumber of images in a set. Unfortunately, in that case, it is no longer clear that Σn →Σ. Dealing\nwith this type of difficulties is the realm of high dimensional statistics.\n\nFor simplicity we will instead try to understand the spectral properties of\nSn =\nXXT .\nn\nSince x ∼N(0, Σ) we know that μn →0 (and, clearly,\nn\nn-1 →1) the spectral properties of Sn will be\nessentially the same as Σn.3\nLet us start by looking into a simple example, Σ = I. In that case, the distribution has no low\ndimensional structure, as the distribution is rotation invariant. The following is a histogram (left) and\na scree plot of the eigenvalues of a sample of Sn (when Σ = I) for p = 500 and n = 1000. The red\nline is the eigenvalue distribution predicted by the Marchenko-Pastur distribution (15), that we will\ndiscuss below.\nAs one can see in the image, there are many eigenvalues considerably larger than 1 (and some\nconsiderably larger than others). Notice that , if given this profile of eigenvalues of Σn one could\npotentially be led to believe that the data has low dimensional structure, when in truth the distribution\nit was drawn from is isotropic.\nUnderstanding the distribution of eigenvalues of random matrices is in the core of Random Matrix\nTheory (there are many good books on Random Matrix Theory, e.g. [Tao12] and [AGZ10]). This\nparticular limiting distribution was first established in 1967 by Marchenko and Pastur [MP67] and is\nnow referred to as the Marchenko-Pastur distribution. They showed that, if p and n are both going\nto infwith their ratio fixed p/n = γ ≤1, the sample distribution of the eigenvalues of Sn (like the\nhistogram above), in the limit, will be\np\n(γ+ -λ) (λ -γ )\ndFγ(λ) =\n-1[γ\n2π\nγλ\n-,γ+](λ)dλ,\n(15)\n3In this case, Sn is actually the Maximum likelihood estimator for Σ, we'll talk about Maximum likelihood estimation\nlater in the course.\n\nwith support [γ , γ+]. This is plotted as the red line in the figure above.\n-\nRemark 1.2 We will not show the proof of the Marchenko-Pastur Theorem here (you can see, for\nexample, [Bai99] for several different proofs of it), but an approach to a proof is using the so-called\nmoment method. The core of the idea is to note that one can compute moments of the eigenvalue\ndistribution in two ways and note that (in the limit) for any k,\n\"\nk\n#\n\np\n1 X\nZ γ+\nE Tr\nXXT\n=\nE Tr Sk\nn\n= E\nλk\nk\ni (Sn) =\nλ dFγ(λ),\np\nn\np\np\nγ\ni=1\n-\nand that the quantities 1E Tr\nh 1 XXT ki\ncan be estimated (these estimates rely essentially in combi-\np\nn\nnatorics). The distribution dFγ(λ) can then be computed from its moments.\n1.2.1\nA related open problem\nOpen Problem 1.2 (Monotonicity of singular values [BKS13a]) Consider the setting above but\nwith p = n, then X ∈Rn×n is a matrix with iid N(0, 1) entries. Let\nσi\n√X\nn\n\n,\ndenote the i-th singular value4 of √1 X, and define\nn\nαR(n) := E\n\"\nX\nn\nσi\n√X\n,\nn\nn\ni=1\n#\nas the expected value of the average singular value of √1 X.\nn\nThe conjecture is that, for every n ≥1,\nαR(n + 1) ≥αR(n).\nMoreover, for the analogous quantity αC(n) defined over the complex numbers, meaning simply\nthat each entry of X is an iid complex valued standard gaussian CN(0, 1) the reverse inequality is\nconjectured for all n ≥1:\nαC(n + 1) ≤αC(n).\nNotice that the singular values of √1 X are simply the square roots of the eigenvalues of Sn,\nn\nσi\n√X\n\n=\nλ\nn\np\ni (Sn).\n4The i-th diagonal element of Σ in the SVD √1 X = UΣV .\nn\n\nThis means that we can compute αR in the limit (since we know the limiting distribution of λi (Sn))\nand get (since p = n we have γ = 1, γ\n= 0, and γ\n-\n+ = 2)\n(2\nλ) λ\nlim αR(n) =\nZ\nλ 2 dF1(λ) = 2π\nZ\nλ 2\n→inf\np\n-\n=\n0.8488.\nn\nλ\n3π ≈\nAlso, αR(1) simply corresponds to the expected value of the absolute value of a standard gaussian\ng\nαR(1) = E|g| =\nr\nπ ≈0.7990,\nwhich is compatible with the conjecture.\nOn the complex valued side, the Marchenko-Pastur distribution also holds for the complex valued\ncase and so limn\nαC(n) = lim\n→inf\nn\nα\n→inf\nR(n) and αC(1) can also be easily calculated and seen to be\nlarger than the limit.\n1.3\nSpike Models and BBP transition\nWhat if there actually is some (linear) low dimensional structure on the data? When can we expect to\ncapture it with PCA? A particularly simple, yet relevant, example to analyse is when the covariance\nmatrix Σ is an identity with a rank 1 perturbation, which we refer to as a spike model Σ = I + βvvT ,\nfor v a unit norm vector and β ≥0.\nOne way to think about this instance is as each data point x consisting of a signal part √βg0v\nwhere g0 is a one-dimensional standard gaussian (a gaussian multiple of a fixed vector √βv and a\nnoise part g ∼N(0, I) (independent of g0. Then x = g + √βg0v is a gaussian random variable\nx ∼N(0, I + βvvT ).\nA natural question is whether this rank 1 perturbation can be seen in Sn. Let us build some\nintuition with an example, the following is the histogram of the eigenvalues of a sample of Sn for\np = 500, n = 1000, v is the first element of the canonical basis v = e1, and β = 1.5:\n\nThe images suggests that there is an eigenvalue of Sn that \"pops out\" of the support of the\nMarchenko-Pastur distribution (below we will estimate the location of this eigenvalue, and that es-\ntimate corresponds to the red \"x\"). It is worth noticing that the largest eigenvalues of Σ is simply\n1 + β = 2.5 while the largest eigenvalue of Sn appears considerably larger than that. Let us try now\nthe same experiment for β = 0.5:\nand it appears that, for β = 0.5, the distribution of the eigenvalues appears to be undistinguishable\nfrom when Σ = I.\nThis motivates the following question:\nQuestion 1.3 For which values of γ and β do we expect to see an eigenvalue of Sn popping out of the\nsupport of the Marchenko-Pastur distribution, and what is the limit value that we expect it to take?\nAs we will see below, there is a critical value of β below which we don't expect to see a change\nin the distribution of eivenalues and above which we expect one of the eigenvalues to pop out of the\nsupport, this is known as BBP transition (after Baik, Ben Arous, and P ech e [BBAP05]). There are\nmany very nice papers about this and similar phenomena, including [Pau, Joh01, BBAP05, Pau07,\nBS05, Kar05, BGN11, BGN12].5\nIn what follows we will find the critical value of β and estimate the location of the largest eigenvalue\nof Sn. While the argument we will use can be made precise (and is borrowed from [Pau]) we will\nbe ignoring a few details for the sake of exposition.\nIn short, the argument below can be\ntransformed into a rigorous proof, but it is not one at the present form!\nFirst of all, it is not difficult to see that we can assume that v = e1 (since everything else is rotation\ninvariant). We want to understand the behavior of the leading eigenvalue of\nn\nSn =\nX\nx\nT\nix\nn\ni =\nXXT ,\nn\ni=1\n5Notice that the Marchenko-Pastur theorem does not imply that all eigenvalues are actually in the support of the\nMarchenk-Pastur distribution, it just rules out that a non-vanishing proportion are. However, it is possible to show that\nindeed, in the limit, all eigenvalues will be in the support (see, for example, [Pau]).\n\nwhere\nX = [x1, . . . , xn] ∈Rp×n.\nWe can write X as\nX =\n√1 + βZT\nZT\n,\n\nwhere Z1 ∈Rn×1 and Z2 ∈Rn×(p-1), both populated with i.i.d. standard gaussian entries (N(0, 1)).\nThen,\nβ)ZT\nT\n(1 +\n1 Z\nT\n√\n√1 + βZ\nn =\nXX\n=\n1 Z2\nS\nn\nn\n1 + βZT\n2 Z\nT\n.\nZ2 Z2\n\nˆ\nNow, let λ and v =\nv1\n\nwhere v2 ∈Rp-1 and v1 ∈R, denote, respectively, an eigenvalue and\nv2\nassociated eigenvector for Sn. By the definition of eigenvalue and eigenvector we have\n1 (1 + β)ZT\n1 Z1\n√1 + βZT\n1 Z2\nv1\nˆ\n= λ\nn\n√1 + βZT\n2 Z1\nZT\n2 Z2\n\nv2\n\nv1\nv2\n\n,\nwhich can be rewritten as\nˆ\n(1 + β)ZT\n1 Z1v1 +\np\n1 + βZT\n1 Z2v2\n=\nλv1\n(16)\nn\nn\nˆ\n1 + βZT\n2 Z1v1 +\nZT\nn\n2 Z2v2\n=\nλv2.\n(17)\nn\n(17) is equivalent to\np\n1 + βZT\n2 Z1v1 =\n\nˆλ I -\nZT\nn\nn\n2 Z2\n\nv2.\nˆ\nIf λ I -1 ZT\n2 Z2 is invertible (this w\np\non't be justified here, but it is in [Pau]) then we can rewrite it as\nn\n\n-\nˆ\nv2 =\nλ I -\nZT\n2 Z2\n\n1 p\n1 + βZT\n2 Z1v1,\nn\nn\nwhich we can then plug in (16) to get\nT\n-1\nˆ\nˆ\n(1 + β)Z1 Z1v1 +\np\n1 + βZT\n1 Z2\n\nλ I -\nZT\n2 Z2\n\np\n1 + βZT\n=\nn\nn\n2 Z1v1\nλv1\nn\nn\nIf v1 = 0 (again, not properly justified here, see [Pau]) then this means that\n-1\nˆ\nˆ\nλ =\n(1 + β)ZT\n1 Z1 +\np\n1 + βZT\n1 Z2\n\nλ I -\nZT\n2 Z2\n\np\n1 + βZT\n(18)\nn\nn\nn\n2 Z1\nn\nFirst observation is that because Z1 ∈Rn has standard gaussian entries then 1 ZT\nn\n1 Z1 →1, meaning\nthat\n\"\nT\n\nT\n-1\nˆ\nˆ\nλ = (1 + β) 1 +\nZ\nn\n1 Z2\nλ I -\nZ\nT\nn\n2 Z2\nZ\nn\n2 Z1\n#\n.\n(19)\n\nConsider the SVD of Z\n= UΣV T where U ∈Rn×p and V ∈Rp\np\n×\nhave orthonormal columns\n(meaning that UT U = Ip\np and V T V = I\n×\np\n),\n×p\nand Σ is a diagonal matrix. Take D =\nΣ then\nn\nZT\n2 Z2 =\nV Σ2V T = V DV T ,\nn\nn\nmeaning that the diagonal entries of D correspond to the eigenvalues of 1 ZT\n2 Z2 which we expect to\nn\np\nbe distributed (in the limit) according to the Marchenko-Pastur distribution for\n-\nn\n≈γ. Replacing\nback in (19)\nT\nˆ\nˆ\n-1 1\nλ\n=\n(1 + β)\n\n1 +\nZT\n1/2\nT\nT\n1/2\nT\nn\nλ -\n\n√nUD\nV\nI\nV DV\n√nUD\nV\nZ1\nn\n\nT\nˆ\n=\nU\nV T\n\n(1 + β) 1\nT Z\nD1/\n-\n+\nλ I -V DV T\nV D1/2 UT Z1\n\nn\n1 -\n\nT\n\nˆ\n\n=\n(1 + β)\n+\nUT Z\n\nD1/2V T\nD1\n\nV\nh\nλ I -D\ni\nV T\nV\n/2 UT\n\nZ1\nn\n\nT\n=\n(1 + β)\n\n1 +\nUT Z\n\nD1/2\nˆ\nh\nλ I -D\ni-\nD1/2 UT Z1\n\n.\n\nn\nSince the columns of U are orthonormal, g := UT Z1 ∈Rp-1 is an isotropic gaussian (g ∼N(0, 1)), in\nfact,\nEggT = EUT Z\nUT Z\nT = EUT\nT\nZ1Z1 U = UT E\n\nZ1ZT\nT\n\nU = U U = I(p-1)×(p\n.\n-1)\nWe proceed\nˆλ\n=\n(1 + β)\n\n1 +\ngT D1/2 h\nˆ\n-1\nλ I -D\ni\nD1/2g\n\n=\n(1 + β)\n\nn\np\n\n-1\n1 + n\nX\ng2\nDjj\nj ˆλ -Djj\nj=1\n\nBecause we expect the diagonal entries of D to be distributed\n\naccording to the Marchenko-Pastur\ndistribution and g to be independent to it we expect that (again, not properly justified here, see [Pau])\nX\np-1\nj\ng2\nD j\nx\np -1\nj ˆ\nˆ\nλ -Djj\n→\nZ γ+\ndFγ(x).\nγ\nλ\nj=1\n-\n-x\nˆ\nWe thus get an equation for λ:\nγ\nˆλ = (1 + β)\n\n1 + γ\nZ\n+\nx\ndF (\nˆ\nγ x)\nγ\nλ\n-\n-x\n\n,\nwhich can be easily solved with the help of a program that computes integrals symbolically (such as\nMathematica) to give (you can also see [Pau] for a derivation):\nγ\nˆλ = (1 + β)\n\n1 + β\n\n,\n(20)\n\nwhich is particularly elegant (specially considering the size of some the equations used in the deriva-\ntion).\nAn important thing to notice is that for β = √γ we have\n√\n\nγ\nˆλ = (1 +\nγ)\n1 + √γ\n\n= (1 + √\nγ) = γ+,\nsuggesting that β = √γ is the critical point.\nIndeed this is the case and it is possible to make the above argument rigorous6 and show that in\nthe model described above,\n- If β\n√\n≤\nγ then\nλmax (Sn) →γ+,\n- and if β > √γ then\nλmax (Sn) →(1 + β)\n\nγ\n1 + β\n\n> γ+.\nAnother important question is wether the leading eigenvector actually correlates with the planted\nperturbation (in this case e1). Turns out that very similar techniques can answer this question as\nwell [Pau] and show that the leading eigenvector vmax of Sn will be non-trivially correlated with e1 if\nand only if β > √γ, more precisely:\n- If β\n√\n≤\nγ then\n|⟨vmax, e1⟩|2 →0,\n- and if β > √γ then\nγ\n|⟨\nβ2\nvmax, e1\n-\n⟩| →1 -γ .\nβ\n1.3.1\nA brief mention of Wigner matrices\nAnother very important random matrix model is the Wigner matrix (and it will show up later in this\ncourse). Given an integer n, a standard gaussian Wigner matrix W ∈Rn×n is a symmetric matrix\nwith independent\n(0, 1) entries (except for the fact that Wij = Wji). In the limit, the eigenvalues\nof √1\nN\nW are distributed according to the so-called semi-circular law\nn\ndSC(x) =\np\n4 -x21[\n2,2](x)dx,\n2π\n-\nand there is also a BBP like transition for this matrix ensemble [FP06]. More precisely, if v is a\nunit-norm vector in Rn and ξ ≥0 then the largest eigenvalue of √1 W + ξvvT satisfies\nn\n6Note that in the argument above it wasn't even completely clear where it was used that the eigenvalue was actually\nthe leading one. In the actual proof one first needs to make sure that there is an eigenvalue outside of the support and\nthe proof only holds for that one, you can see [Pau]\n\n- If ξ ≤1 then\nλ\n√W + ξvvT\nmax\nn\n\n→2,\n- and if ξ > 1 then\nλmax\n√W + ξvvT\n\n→ξ +\n.\n(21)\nn\nξ\n1.3.2\nAn open problem about spike models\nOpen Problem 1.3 (Spike Model for cut-SDP [MS15]. As since been solved [MS15]) Let\nW denote a symmetric Wigner matrix with i.i.d. entries Wij ∼N(0, 1). Also, given B ∈Rn×n sym-\nmetric, define:\nQ(B) = max {Tr(BX) : X ⪰0, Xii = 1} .\nDefine q(ξ) as\nξ\nq(ξ) = lim\nEQ\nn→infn\n\n11T +\nn\n√W\nn\n\n.\nWhat is the value of ξ , defined as\n∗\nξ = inf\n∗\n{ξ ≥0 : q(ξ) > 2}.\nIt is known that, if 0 ≤ξ ≤1, q(ξ) = 2 [MS15].\nOne can show that 1 Q(B)\nn\n≤λmax(B). In fact,\nmax {Tr(BX) : X ⪰0, Xii = 1} ≤max {Tr(BX) : X ⪰0, Tr X = n} .\nIt is also not difficult to show (hint: take the spectral decomposition of X) that\n(\nX\nn\nmax\nTr(BX) : X ⪰0,\nXii = n\ni=1\n)\n= λmax(B).\nThis means that for ξ > 1, q(ξ) ≤ξ + 1.\nξ\nRemark 1.4 Optimization problems of the type of max {Tr(BX) : X ⪰0, Xii = 1} are semidefinite\nprograms, they will be a major player later in the course!\nSince 1 E Tr 11T\nξ 11T + √1 W\nξ.\nn\nn\nn\n≈ξ, by taking X = 11T we expect that q(ξ) ≥\nThese observ\nh\nations\n\nimply that 1\ni\n≤ξ < 2 (see [MS15]). A reasonable conjecture is that it is equal\n∗\nto 1. This would imply that a certain semidefinite programming based algorithm for clustering under\nthe Stochastic Block Model on 2 clusters (we will discuss these things later in the course) is optimal\nfor detection (see [MS15]).7\nRemark 1.5 We remark that Open Problem 1.3 as since been solved [MS15].\n7Later in the course we will discuss clustering under the Stochastic Block Model quite thoroughly, and will see how\nthis same SDP is known to be optimal for exact recovery [ABH14, HWX14, Ban15c].\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 5-7: Graphs, Diffusion Maps, and Semi-supervised Learning",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/96c4c2d9f8b72433633af7762f0c742d_MIT18_S096F15_Ses5_7.pdf",
      "content": "Graphs, Diffusion Maps, and Semi-supervised Learning\n2.1\nGraphs\nGraphs will be one of the main objects of study through these lectures, it is time to introduce them.\nA graph G = (V, E) contains a set of nodes V = {v\nV\n1, . . . , vn} and edges E ⊆\n. An edge (i, j)\n∈E\nif vi and vj are connected. Here is one of the graph theorists favorite examples,\n\nthe Petersen graph8:\nFigure 1: The Petersen graph\nGraphs are crucial tools in many fields, the intuitive reason being that many phenomena, while\ncomplex, can often be thought about through pairwise interactions between objects (or data points),\nwhich can be nicely modeled with the help of a graph.\nLet us recall some concepts about graphs that we will need.\n- A graph is connected if, for all pairs of vertices, there is a path between these vertices on the\ngraph.\nThe number of connected components is simply the size of the smallest partition of\nthe nodes into connected subgraphs. The Petersen graph is connected (and thus it has only 1\nconnected component).\n- A clique of a graph G is a subset S of its nodes such that the subgraph corresponding to it is\ncomplete. In other words S is a clique if all pairs of vertices in S share an edge. The clique\nnumber c(G) of G is the size of the largest clique of G. The Petersen graph has a clique number\nof 2.\n- An independence set of a graph G is a subset S of its nodes such that no two nodes in S share\nan edge. Equivalently it is a clique of the complement graph Gc := (V, Ec). The independence\nnumber of G is simply the clique number of Sc. The Petersen graph has an independence number\nof 4.\n8The Peterson graph is often used as a counter-example in graph theory.\nThis graph is in public domain.\nSource: https://commons.wikimedia.org/\nwiki/File:Petersen_graph_3-coloring.svg.\n\nA particularly useful way to represent a graph is through its adjacency matrix. Given a graph\nG = (V, E) on n nodes (|V | = n), we define its adjacency matrix A ∈Rn×n as the symmetric matrix\nwith entries\n(\nij =\nif i, j)\nA\n∈E,\notherwise.\nSometime, we will consider weighted graphs G = (V, E, W), where edges may have weights wij,\nwe think of the weights as non-negative wij ≥0 and symmetric wij = wji.\n2.1.1\nCliques and Ramsey numbers\nCliques are important structures in graphs and may have important application-specific applications.\nFor example, in a social network graph (e.g., where people correspond to vertices and two vertices are\nconnected if the respective people are friends) cliques have a clear interpretation.\nA natural question is whether it is possible to have arbitrarily large graphs without cliques (and\nwithout it's complement having cliques), Ramsey answer this question in the negative in 1928 [Ram28].\nLet us start with some definitions: given a graph G we define r(G) as the size of the largest clique of\nindependence set, i.e.\nr(G) := max {c(G), c (Gc)} .\nGiven r, let R(r) denote the smallest integer n such that every graph G on n nodes must have r(G) ≥r.\nRamsey [Ram28] showed that R(r) is finite, for every r.\nRemark 2.1 It is easy to show that R(3) ≤6, try it!\nWe will need a simple estimate for what follows (it is a very useful consequence of Stirling's\napproximation, e.g.).\nProposition 2.2 For every k ≤n positive integers,\nn\nk\nk\n≤\nn\nr\n\n≤\nne\nk\nk\n\n.\nWe will show a simple lower bound on R(r). But first we introduce a random graph construction,\nan Erd os-Reny ı graph.\nDefinition 2.3 Given n and p, the random Erd os-Reny ı graph G(n, p) is a random graph on n vertices\nwhere each possible edge appears, independently, with probability p.\nThe proof of the lower bound on R(r) is based on the probabilistic method, a beautiful non-\nconstructive method pioneered by Paul Erd os to establish the existence of certain objects. The core\nidea is the simple observation that if a random variable has a certain expectation then there must exist\na draw of it whose value is at least that of the expectation. It is best understood with an example.\nTheorem 2.4 For every r ≥2,\nR( ) ≥\nr-\nr\n2 .\n\nProof.\nLet G be drawn from the G\nn, 1\n\ndistribution, G ∼G\nn, 1\nS\n\n. For every set\nof r nodes, let\nX(S) denote the random variable\nif S is a clique or independent set,\nX(S) =\notherwise.\nAlso, let X denote the random variable\nX =\nX\nX(S).\nV\nS∈( r)\nWe will proceed by estimating E [X]. Note that, by linearity of expectation,\nE [X] =\nX\nE [X(S)] ,\nS∈(V\nr)\nand E [X(S)] = Prob {S is a clique or independent set} =\n.\n(|S\nThis means that\n|\n2 )\nE [X] =\nS\nX\n∈(V\nr) 2(|S|\n2 ) =\nn\nr\n2(r\n2) =\nn\nr\n\nr(r-1) .\nBy Proposition 2.2 we have,\nE [X] ≤\nne\nr\nr\nr(r-1)\n= 2\nn\nr-1\ne\nr\nr\n.\nThat means that if n ≤2\nr-1\nand r ≥3 then E [X] < 1. This means that Prob{X < 1} > 0 and since\nX is a non-negative integer we must have Prob{X = 0} = Prob{X < 1} > 0 (another way of saying\nthat is that if E [X] < 1 then there must be an instance for which X < 1 and since X is a non-negative\nr-1\ninteger, we must have X = 0). This means that there exists a graph with 2\nnodes that does not\nhave cliques or independent sets of size r which implies the theorem.\nRemarkably, this lower bound is not very different from the best known. In fact, the best known\nlower and upper bounds known [Spe75, Con09] for R(r) are\n√\n(1 + o(1))\n2r\ne\n√\nr\n≤R(r) ≤r-c log r\nlog log r 4r.\n(22)\nOpen Problem 2.1 Recall the definition of R(r) above, the following questions are open:\n- What is the value of R(5)?\n- What are the asymptotics\nthe lower bound (\n√\nof R(s)? In particular, improve on the base of the exponent on either\n2) or the upper bound (4).\n\n- Construct a family of graphs G = (V, E) with increasing number of vertices for which there exists\nε > 0 such that9\n|V | ≲(1 + ε)r.\nIt is known that 43 ≤R(5) ≤49. There is a famous quote in Joel Spencer's book [Spe94] that\nconveys the difficulty of computing Ramsey numbers:\n\"Erd os asks us to imagine an alien force, vastly more powerful than us, landing on Earth and\ndemanding the value of R(5) or they will destroy our planet.\nIn that case, he claims, we should\nmarshal all our computers and all our mathematicians and attempt to find the value. But suppose,\ninstead, that they ask for R(6). In that case, he believes, we should attempt to destroy the aliens.\"\nThere is an alternative useful way to think about 22, by taking log2 of each bound and rearranging,\nwe get that\n1 + o(1)\n\nlog2 n ≤\nmin\nr(G)\n(2 + o(1)) log2 n\nG=(V,E), |V |=n\n≤\nThe current \"world record\" (see [CZ15, Coh15]) for deterministic construction of families of graphs with\n≲\n|\n| c\nsmall r(G) achieves r(G)\n2(log log V ) , for some constant c > 0. Note that this is still considerably\nlarger than polylog|V |. In contrast, it is very easy for randomized constructions to satisfy r(G) ≤\n2 log2 n, as made precise by the folloing theorem.\nTheorem 2.5 Let G ∼G\nn, 1\n\nbe and Erd os-Reny ı graph with edge probability 1. Then, with high\nprobability,10\nR(G) ≤2 log2(n).\nProof.\nGiven n, we are interested in upper bounding Prob {R(G) ≥⌈2 log2 n⌉}. and we proceed by\nunion bounding (and making use of Proposition 2.2):\nProb {R(G) ≥⌈2 log2 n⌉}\n=\nProb\n\n∃S⊂V, S\nS is a clique or independent set\n| |=⌈2 log2 n⌉\n\n=\nProb\n\n[\n{S is a clique or independent set}\nS∈(\nV\n⌈2 log\nn\n⌉\n\n)\n≤\nProb {S is a clique or independent set}\n\n=\n\n∈(\nV\nS\nX\n2 log\nn )\n⌈\n⌉\nn\n⌈2 log2 n⌉\n\n2(⌈2 log2 n⌉\n)\n≤\n\nn\n⌈2 log2 n⌉-1\ne\n⌈\n⌈2 log2 n⌉\n! 2 log2 n⌉\n≤\n\ne\n√\n⌈\n2 log2 n\n! 2 log2 n⌉\n≲\nn-Ω(1).\n9By ak ≲bk we mean that there exists a constant c such that ak ≤c bk.\n10We say an event happens with high probability if its probability is ≥1 -n-Ω(1).\n\nThe following is one of the most fascinating conjectures in Graph Theory\nOpen Problem 2.2 (Erd os-Hajnal Conjecture [EH89]) Prove or disprove the following:\nFor any finite graph H, there exists a constant δH > 0 such that any graph on n nodes that does\nnot contain H as a subgraph (is a H-free graph) must have\nr(G) ≳nδH.\nIt is known that r(G) ≳exp\ncH\n√log n , for some constant cH > 0 (see [Chu13] for a survey\non this conjecture).\nNote that this lower\n\nbound already shows that H-free graphs need to have\nconsiderably larger r(G). This is an amazing local to global effect, where imposing a constraint on\nsmall groups of vertices are connected (being H-free is a local property) creates extremely large cliques\nor independence sets (much larger than polylog(n) as in random Erd os-Reny ı graphs).\nSince we do not know how to deterministically construct graphs with r(G) ≤polylogn, one ap-\nproach could be to take G ∼G n, 1\nand check that indeed it has small clique and independence\nnumber. However, finding the largest\n\nclique on a graph is known to be NP-hard (meaning that there\nis no polynomial time algorithm to solve it, provided that the widely believed conjecture NP = P\nholds). That is a worst-case statement and thus it doesn't necessarily mean that it is difficult to find\nthe clique number of random graphs. That being said, the next open problem suggests that this is\nindeed still difficult.\nFirst let us describe a useful construct.\nGiven n and ω, let us consider a random graph G that\nconsists of taking a graph drawn from G n, 1 , picking ω of its nodes (say at random) and adding an\nedge between every pair of those ω nodes, thus \"planting\" a clique of size ω. This will create a clique\nof size ω in G. If ω > 2 log2 n this clique is larger\n\nthan any other clique that was in the graph before\nplanting. This means that, if ω > 2 log2 n, there is enough information in the graph to find the planted\nclique. In fact, one can simply look at all subsets of size 2 log2 n + 1 and check wether it is clique: if\nit is a clique then it very likely these vertices belong to the planted clique. However, checking all such\nsubgraphs takes super-polynomial time ∼nO(log n). This motivates the natural question of whether\nthis can be done in polynomial time.\nSince the degrees of the nodes of a G\nn, 1\n\nhave expected value n-1\nand standard deviation ∼√n,\nif ω > c√n (for sufficiently large constant c) then the degrees of the nodes involved in the planted\nclique will have larger degrees and it is easy to detect (and find) the planted clique. Remarkably,\nthere is no known method to work for ω significant smaller than this. There is a quasi-linear\np\ntime\nalgorithm [DM13] that finds the largest clique, with high probability, as long as ω ≥\nn\ne + o(√n).11\nOpen Problem 2.3 (The planted clique problem) Let G be a random graph constructed by tak-\ning a G\nn, 1\nand\n\nplanting a clique of size ω.\n1. Is there a polynomial time algorithm that is able to find the largest clique of G (with high prob-\nability) for ω\n√\n≪\nn. For example, for ω ≈\n√n\nlog n.\n11There is an amplification technique that allows one to find the largest clique for ω ≈c√n for arbitrarily small c in\npolynomial time, where the exponent in the runtime depends on c. The rough idea is to consider all subsets of a certain\nfinite size and checking whether the planted clique contains them.\n\n2. Is there a polynomial time algorithm that is able to distinguish, with high probability, G from a\ndraw of G\nn, 1\n\nfor ω ≪√n. For example, for ω ≈\n√n\nlog n.\n3. Is there a quasi-linear time algorithm able to find the largest clique of G (with high probability)\nfor ω ≤\n\n√e -ε\n√n, for some ε > 0.\nThis open problem is particularly important. In fact, the hypothesis that finding planted cliques\nfor small values of ω is behind several cryptographic protocols, and hardness results in average case\ncomplexity (hardness for Sparse PCA being a great example [BR13]).\n2.2\nDiffusion Maps\nDiffusion Maps will allows us to represent (weighted) graphs G = (V, E, W) in Rd, i.e. associating,\nto each node, a point in Rd. As we will see below, oftentimes when we have a set of data points\nx1, . . . , xn ∈Rp it will be beneficial to first associate to each a graph and then use Diffusion Maps to\nrepresent the points in d-dimensions, rather than using something like Principal Component Analysis.\nBefore presenting Diffusion Maps, we'll introduce a few important notions. Given G = (V, E, W)\nwe consider a random walk (with independent steps) on the vertices of V with transition probabilities:\nw\nProb {\nij\nX(t + 1) = j|X(t) = i} =\n,\ndeg(i)\nwhere deg(i) = P\nj wij. Let M be the matrix of these probabilities,\nwij\nMij =\n.\ndeg(i)\nIt is easy to see that Mij ≥0 and M1 = 1 (indeed, M is a transition probability matrix). Defining D\nas the diagonal matrix with diagonal entries Dii = deg(i) we have\nM = D-1W.\nIf we start a random walker at node i (X(0) = 1) then the probability that, at step t, is at node j\nis given by\nProb {X(t) = j|X(0) = i} =\nMt\n.\nij\nIn other words, the probability cloud of the random walker at poin\n\nt t, given that it started at node i\nis given by the row vector\nProb {X(t)|X(0) = i} = eT\ni Mt = Mt[i, :].\nRemark 2.6 A natural representation of the graph would be to associate each vertex to the probability\ncloud above, meaning\ni →Mt[i, :].\nThis would place nodes i1 and i2 for which the random walkers starting at i1 and i2 have, after t steps,\nvery similar distribution of locations. However, this would require d = n. In what follows we will\nconstruct a similar mapping but for considerably smaller d.\n\nM is not symmetric, but a matrix similar to M, S = D 2 MD-1\n2 is, indeed S = D-1\n2 WD-1\n2 . We\nconsider the spectral decomposition of S\nS = V ΛV T ,\nwhere V = [v1, . . . , vn] satisfies V T V = In\nn and Λ is diagonal with diagonal elements Λkk = λk (and\n×\nwe organize them as λ1 ≥λ2 ≥· · · ≥λn). Note that Svk = λkvk. Also,\nM = D-1\n2 SD\n2 = D-1\n2 V ΛV T D\n2 =\n\nD-1\n2 V\n\nΛ\n\nD\n2 V\nT\n.\nWe define Φ = D-1\n2 V with columns Φ = [φ1, . . . , φn] and Ψ = D 2 V with columns Ψ = [ψ1, . . . , ψn].\nThen\nM = ΦΛΨT ,\nand Φ, Ψ form a biorthogonal system in the sense that ΦT Ψ = In\nn or, equivalently, φT\nj ψk = δ\n×\njk.\nNote that φk and ψk are, respectively right and left eigenvectors of M, indeed, for all 1 ≤k ≤n:\nMφk = λkφk\nand ψT\nk M = λkψT\nk .\nAlso, we can rewrite this decomposition as\nM =\nX\nn\nλkφkψT\nk .\nk=1\nand it is easy to see that\nn\nMt =\nX\nλt\nkφkψT\nk .\n(23)\nk=1\nLet's revisit the embedding suggested on Remark 2.6. It would correspond to\nn\nvi →Mt[i, :] =\nX\nλt\nkφk(i)ψT\nk ,\nk=1\nit is written in terms of the basis ψk. The Diffusion Map will essentially consist of the representing a\nnode i by the coefficients of the above map\nλt\n1φ1(i)\nλt\n2φ2(i)\nv →\nt\ni\nM [i, :] =\n\n..\n\n.\nλt φn(i)\n\n,\n(24)\nn\nNote that M1 = 1, meaning that one of the right eigenvectors φk is simply a multiple of 1 and so it\ndoes not distinguish the different nodes of the graph. We will show that this indeed corresponds to\nthe the first eigenvalue.\nProposition 2.7 All eigenvalues λk of M satisfy |λk| ≤1.\n\nProof.\nLet φk be a right eigenvector associated with λk whose largest entry in magnitude is positive\nφk (imax). Then,\nn\nλkφk (imax) = Mφk (imax) =\nX\nMimax,jφk (j) .\nj=1\nThis means, by triangular inequality that, that\nn\n|λk| =\nX\nφ\nM max,j\n|\n|\nj=1\n|\n|\nk (j)\ni\nX\nn\n|\n)\n|\ni\nφ (i\n≤\nM\n|\nmax,j\nk\nmax\nj=1\n| = 1.\nRemark 2.8 It is possible that there are other eigenvalues with magnitude 1 but only if G is dis-\nconnected or if G is bipartite. Provided that G is disconnected, a natural way to remove potential\nperiodicity issues (like the graph being bipartite) is to make the walk lazy, i.e. to add a certain proba-\nbility of the walker to stay in the current node. This can be conveniently achieved by taking, e.g.,\nM′ = 2M + 1I.\nBy the proposition above we can take φ1 = 1, meaning that the first coordinate of (24) does not\nhelp differentiate points on the graph. This suggests removing that coordinate:\nDefinition 2.9 (Diffusion Map) Given a graph G = (V, E, W) construct M and its decomposition\nM = ΦΛΨT as described above. The Diffusion Map is a map φt : V →Rn-1 given by\nλt\n2φ2(i)\nλt\nφt (vi) =\n\nφ3(i)\n.\n.\n..\nλt\nnφn(i)\nThis map is still a map to n -1 dimensions. But note now that each coordinate has a factor of\nλt\nk which, if λk is small will be rather small for moderate values of t. This motivates truncating the\nDiffusion Map by taking only the first d coefficients.\nDefinition 2.10 (Truncated Diffusion Map) Given a graph G = (V, E, W) and dimension d, con-\nstruct M and its decomposition M = ΦΛΨT as described above. The Diffusion Map truncated to d\ndimensions is a map φt : V →Rd given by\n(d)\nφt\n(vi) =\n\nλt\n\n2φ2(i)\n\nλt\n3φ3(i)\n\n.\n.\n.\n\n.\nλt\nd+1φd+1(i)\n\nIn the following theorem we show that the euclidean distance in the diffusion map coordinates\n(called diffusion distance) meaningfully measures distance between the probability clouds after t iter-\nations.\nTheorem 2.11 For any pair of nodes vi1, vi2 we have\nX\nn\n∥φt (vi1) -φt (vi2)∥=\nj=1 deg(j) [Prob {X(t) = j|X(0) = i1} -Prob {X(t) = j|X(0) = i2}]2 .\nProof.\nNote that Pn\nj=1\n{\n[Prob X(t) = j|X(0) = i1} -Prob {X(t) = j\ndeg(j\n|X(0) = i2}] can be rewritten\n)\nas\nX\nn\nj=1 deg(j)\n\" n\nX\nk=1\nλt\nkφk(i1)ψk(j) -\nn\nX\nk=1\nλt\nkφk(i2)ψk(j)\n#2\n=\nn\nX\nj=1\ndeg(j)\n\" n\nX\nk=1\nλt\nk (φk(i1) -φk(i2)) ψk(j)\n#2\nand\nn\nX\nj=1\n\"X\nn\nX\nn \" n\nt\n#2\nX\nt\nψk(j)\nλk (φk(i1) -φk(i2)) ψk(j)\n=\nλk (φk(i1)\n(j)\nj=1\n-φk(i2))\ndeg\nk=1\nk=1\np\ndeg(j)\n#\nX\nn\n\nt\n-\n-1\n=\n\nλk (φk(i1)\nφk(i2)) D\nk=1\n2 ψk\n\n.\nNote that D-1\n2 ψk = vk which forms an orthonormal basis, meaning that\nX\nn\n\nλt\nk (φk(i1) -φk(i2)) D-\nk=1\n2 ψk\n\nn\n\n=\nX 2\nλt\nk (φk(i1) -φk(i2))\nk=1\nn\n\n=\nX 2\nλt\nkφ\nt\nk(i1) -λkφk(i2)\nk=2\n\n,\nwhere the last inequality follows from the fact that φ1 = 1 and concludes the proof of the theorem.\n2.2.1\nA couple of examples\nThe ring graph is a graph on n nodes {1, . . . , n} such that node k is connected to k -1 and k + 1 and\n1 is connected to n. Figure 2 has the Diffusion Map of it truncated to two dimensions\nAnother simple graph is Kn, the complete graph on n nodes (where every pair of nodes share an\nedge), see Figure 3.\n\nFigure 2: The Diffusion Map of the ring graph gives a very natural way of displaying (indeed, if one is\nasked to draw the ring graph, this is probably the drawing that most people would do). It is actually\nnot difficult to analytically compute the Diffusion Map of this graph and confirm that it displays the\npoints in a circle.\n2.2.2\nDiffusion Maps of point clouds\nVery often we are interested in embedding in Rd a point cloud of points x1, . . . , xn ∈Rp and necessarily\na graph. One option (as discussed before in the course) is to use Principal Component Analysis (PCA),\nbut PCA is only designed to find linear structure of the data and the low dimensionality of the dataset\nmay be non-linear. For example, let's say our dataset is images of the face of someone taken from\ndifferent angles and lighting conditions, for example, the dimensionality of this dataset is limited by\nthe amount of muscles in the head and neck and by the degrees of freedom of the lighting conditions\n(see Figure ??) but it is not clear that this low dimensional structure is linearly apparent on the pixel\nvalues of the images.\nLet's say that we are given a point cloud that is sampled from a two dimensional swiss roll embedded\nin three dimension (see Figure 4). In order to learn the two dimensional structure of this object we\nneed to differentiate points that are near eachother because they are close by in the manifold and not\nsimply because the manifold is curved and the points appear nearby even when they really are distant\nin the manifold (see Figure 4 for an example). We will achieve this by creating a graph from the data\npoints.\nOur goal is for the graph to capture the structure of the manifold. To each data point we will\nassociate a node. For this we should only connect points that are close in the manifold and not points\nthat maybe appear close in Euclidean space simply because of the curvature of the manifold. This\nis achieved by picking a small scale and linking nodes if they correspond to points whose distance\nis smaller than that scale. This is usually done smoothly via a kernel Kε, and to each edge (i, j)\nassociating a weight\nwij = Kε (∥xi -xj∥2) ,\na common example of a Kernel is K (u) = exp\n-1\nε\n2εu2\n, that gives essentially zero weight to edges\ncorresponding to pairs of nodes for which ∥xi -xj∥2 ≫√ε. We can then take the the Diffusion Maps\nof the resulting graph.\n\nFigure 3: The Diffusion Map of the complete graph on 4 nodes in 3 dimensions appears to be a regular\ntetrahedron suggesting that there is no low dimensional structure in this graph. This is not surprising,\nsince every pair of nodes is connected we don't expect this graph to have a natural representation in\nlow dimensions.\n2.2.3\nA simple example\nA simple and illustrative example is to take images of a blob on a background in different positions\n(image a white square on a black background and each data point corresponds to the same white\nsquare in different positions). This dataset is clearly intrinsically two dimensional, as each image\ncan be described by the (two-dimensional) position of the square. However, we don't expect this\ntwo-dimensional structure to be directly apparent from the vectors of pixel values of each image; in\nparticular we don't expect these vectors to lie in a two dimensional affine subspace!\nLet's start by experimenting with the above example for one dimension. In that case the blob is\na vertical stripe and simply moves left and right. We think of our space as the in the arcade game\nAsteroids, if the square or stripe moves to the right all the way to the end of the screen, it shows\nup on the left side (and same for up-down in the two-dimensional case). Not only this point cloud\nshould have a one dimensional structure but it should also exhibit a circular structure. Remarkably,\nthis structure is completely apparent when taking the two-dimensional Diffusion Map of this dataset,\nsee Figure 5.\nFor the two dimensional example, we expect the structure of the underlying manifold to be a\ntwo-dimensional torus. Indeed, Figure 6 shows that the three-dimensional diffusion map captures the\ntoroidal structure of the data.\n2.2.4\nSimilar non-linear dimensional reduction techniques\nThere are several other similar non-linear dimensional reduction methods. A particularly popular one\nis ISOMAP [?]. The idea is to find an embedding in Rd for which euclidean distances in the embedding\ncorrespond as much as possible to geodesic distances in the graph. This can be achieved by, between\npairs of nodes vi, vj finding their geodesic distance and then using, for example, Multidimensional\n\nFigure 4: A swiss roll point cloud (see, for example, [TdSL00]). The points are sampled from a two\ndimensional manifold curved in R3 and then a graph is constructed where nodes correspond to points.\nScaling to find points y\nd\ni ∈R that minimize (say)\nmin\nX ∥y\ni\ny\ny1,...,yn\n-\nj∥\n∈\ni,j\n-δij\nRd\n\n,\nwhich can be done with spectral methods (it is a good exercise to compute the optimal solution to\nthe above optimization problem).\n2.3\nSemi-supervised learning\nClassification is a central task in machine learning. In a supervised learning setting we are given many\nlabelled examples and want to use them to infer the label of a new, unlabeled example. For simplicity,\nlet's say that there are two labels, {-1, +1}.\nLet's say we are given the task of labeling point \"?\" in Figure 10 given the labeled points. The\nnatural label to give to the unlabeled point would be 1.\nHowever, let's say that we are given not just one unlabeled point, but many, as in Figure 11; then\nit starts being apparent that -1 is a more reasonable guess.\nIntuitively, the unlabeled data points allowed us to better learn the geometry of the dataset. That's\nthe idea behind Semi-supervised learning, to make use of the fact that often one has access to many\nunlabeled data points in order to improve classification.\nThe approach we'll take is to use the data points to construct (via a kernel Kε) a graph G =\n(V, E, W) where nodes correspond to points. More precisely, let l denote the number of labeled points\nwith labels f1, . . . , fl, and u the number of unlabeled points (with n = l + u), the first l nodes\nv1, . . . , vl correspond to labeled points and the rest vl+1, . . . , vn are unlabaled. We want to find a\nfunction f : V →{-1, 1} that agrees on labeled points: f(i) = fi for i = 1, . . . , l and that is \"as\nsmooth as possible\" the graph. A way to pose this is the following\nmin\nf:V →{-1,1}: f(i)=fi i=1,...,l\nX\nwij (f(i) -f(j)) .\ni<j\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 5: The two-dimensional diffusion map of the dataset of the datase where each data point is\nan image with the same vertical strip in different positions in the x-axis, the circular structure is\napparent.\nInstead of restricting ourselves to giving {-1, 1} we allow ourselves to give real valued labels, with the\nintuition that we can \"round\" later by, e.g., assigning the sign of f(i) to node i.\nWe thus are interested in solving\nmin\nX\nwij (f(i) -\nf(j)) .\nf:V →R: f(i)=fi i=1,...,l i<j\nIf we denote by f the vector (in Rn with the function values) then we are can rewrite the problem\nas\nX\nwij (f(i) -f(j))\n=\nX\nT\nwij [(ei -ej) f] [(ei -ej) f]\ni<j\ni<j\n=\nX\nT\nT\nT\nwij (\ni<j\nh\nei -ej) f\ni h\n(ei -ej) f\ni\n=\nX\nT\nwijfT (ei -ej) (ei -ej) f\ni<j\n=\nfT\n\nX\nT\nwij (ei -ej) (ei -ej)\ni<j\n\nf\nT\nThe matrix P\ni<j wij (ei -ej) (ei -ej)\nwill play a central role throughout this course, it is called\nthe graph Laplacian [Chu97].\nLG :=\nX\nT\nwij (ei -ej) (ei -ej) .\ni<j\nNote that the entries of LG are given by\nwij\nif i = j\n(LG)ij =\n-\n\ndeg(i)\nif i = j,\n\nFigure 6: On the left the data set considered and on the right its three dimensional diffusion map, the\nfact that the manifold is a torus is remarkably captured by the embedding.\nmeaning that\nLG = D -W,\nwhere D is the diagonal matrix with entries Dii = deg(i).\nRemark 2.12 Consider an analogous example on the real line, where one would want to minimize\nZ\nf′(x)2dx.\nIntegrating by parts\nZ\nf′(x)2dx = Boundary Terms -\nd\nZ\nf(x)f′′(x)dx.\nAnalogously, in R :\nZ\ndx\nZ\nd\nf\n∥∇f(x)∥2\n=\nX\nk=1\n∂\n∂xk\n(x)\ndx = B. T. -\nZ\nf(x)\nd\nX\nk=1\n∂2f (x)dx = B. T. -\nZ\nf(x)∆f(x)dx,\n∂x2\nk\nwhich helps motivate the use of the term graph Laplacian.\nLet us consider our problem\nmin\nfT LGf.\nf:V →R: f(i)=fi i=1,...,l\nWe can write\nDl\nl\nWl\nD =\n\n,\nW =\nWll\nWlu\n\nDl\nWl\nu\nfl\n,\nLG =\n-\n-\n,\nand f =\nDu\nWul\nWuu\n\n.\n-Wul\nDu -Wuu\n\nfu\n\nThen we want to find (recall that Wul = Wlu)\nmin fT\nl [Dl -Wll] fl -2fT\nfu∈Ru\nu Wulfl + fT\nu [Du -Wuu] fu.\n\nFigure 7: The two dimensional represention of a data set of images of faces as obtained in [TdSL00]\nusing ISOMAP. Remarkably, the two dimensionals are interpretable\nby first-order optimality conditions, it is easy to see that the optimal satisfies\n(Du -Wuu) fu = Wulfl.\nIf Du -Wuu is invertible12 then\n∗\n-\n-1\nfu = (Du\nWuu)\nWulfl.\nRemark 2.13 The function f function constructed is called a harmonic extension. Indeed, it shares\nproperties with harmonic functions in euclidean space such as the mean value property and maximum\nprinciples; if vi is an unlabeled point then\nf\n\n(i) = D-1\nu (Wulfl + Wuufu)\n\n=\ni\nw\ndeg\nX\nn\nijf(j),\n(i) j=1\nwhich immediately implies that the maximum and minimum value of f needs to be attained at a labeled\npoint.\n2.3.1\nAn interesting experience and the Sobolev Embedding Theorem\nLet us try a simple experiment. Let's say we have a grid on [-1, 1]d dimensions (with say md points\nfor some large m) and we label the center as +1 and every node that is at distance larger or equal\n12It is not difficult to see that unless the problem is in some form degenerate, such as the unlabeled part of the graph\nbeing disconnected from the labeled one, then this matrix will indeed be invertible.\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 8: The two dimensional represention of a data set of images of human hand as obtained\nin [TdSL00] using ISOMAP. Remarkably, the two dimensionals are interpretable\nto 1 to the center, as -1. We are interested in understanding how the above algorithm will label the\nremaining points, hoping that it will assign small numbers to points far away from the center (and\nclose to the boundary of the labeled points) and large numbers to points close to the center.\nSee the results for d = 1 in Figure 12, d = 2 in Figure 13, and d = 3 in Figure 14. While for d ≤2\nit appears to be smoothly interpolating between the labels, for d = 3 it seems that the method simply\nlearns essentially -1 on all points, thus not being very meaningful. Let us turn to Rd for intuition:\nLet's say that we wan\nR t to find a function in Rd that takes the value 1 at zero and -1 at the unit\nsphere, that minimizes\n∥∇f(x)∥2dx. Let us consider the following function on B0(1) (the ball\nB0(1)\ncentered at 0 with unit radius)\n|\nfε( ) =\n\n1 -2|x\nx\nif\nε\n|x| ≤ε\n-1\notherwise.\nA quick calculation suggest that\nZ\nZ\ndx\nB\n∥∇fε(x)∥\n=\n0(1)\nB0(ε) ε2 dx = vol(B0(ε)) 1 dx\n≈εd-2,\nε\nmeaning that, if d > 2, the performance of this function is improving as ε →0, explaining the results\nin Figure 14.\nOne way of thinking about what is going on is through the Sobolev Embedding Theorem. Hm Rd\nis the space of function whose derivatives up to order m are square-integrable in Rd, Sobolev Em\nbed-\nding Theorem says that if m > d\n\n2 then, if f ∈Hm Rd\nthen f must be continuous, which would rule\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 9: The two dimensional represention of a data set of handwritten digits as obtained in [TdSL00]\nusing ISOMAP. Remarkably, the two dimensionals are interpretable\nFigure 10: Given a few labeled points, the task is to label an unlabeled point.\nout the behavior observed in Figure 14. It also suggests that if we are able to control also second\nderivates of f then this phenomenon should disappear (since 2 > 3). While we will not describe\nit here in detail, there is, in fact, a way of doing this by minimizing not fT Lf but fT L2f instead,\nFigure 15 shows the outcome of the same experiment with the fT Lf replaced by fT L2f and con-\nfirms our intuition that the discontinuity issue should disappear (see, e.g., [NSZ09] for more on this\nphenomenon).\n(c) Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFigure 11: In this example we are given many unlabeled points, the unlabeled points help us learn\nthe geometry of the data.\nFigure 12: The d = 1 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 1 it appears to smoothly interpolate between the labeled\npoints.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 8-11: Spectral Clustering and Cheeger's Inequality",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/251ccb47d3003505f06d27c8f91f831d_MIT18_S096F15_Ses8_11.pdf",
      "content": "Spectral Clustering and Cheeger's Inequality\n3.1\nClustering\nClustering is one of the central tasks in machine learning. Given a set of data points, the purpose of\nclustering is to partition the data into a set of clusters where data points assigned to the same cluster\ncorrespond to similar data points (depending on the context, it could be for example having small\ndistance to each other if the points are in Euclidean space).\n3.1.1\nk-means Clustering\nOne the most popular methods used for clustering is k-means clustering. Given x1, . . . , x\np\nn ∈R\nthe\nk-means clustering partitions the data points in clusters S1 ∪· · · ∪Sk with centers μ1, . . . , μk ∈Rp as\nthe solution to:\nk\nmin\nxi\nμl\n.\n(25)\nS\npartition\n1,...,Sk\nX\n∥\n1,...,μk\ni\nX\n-\n∥\nμ\nl=1 ∈Si\n\nFigure 13: The d = 2 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 2 it appears to smoothly interpolate between the labeled\npoints.\nNote that, given the partition, the optimal centers are given by\nμl =\nx\n|Sl|\nX\ni.\ni∈Sl\nLloyd's algorithm [Llo82] (also known as the k-means algorithm), is an iterative algorithm that\nalternates between\n- Given centers μ1, . . . , μk, assign each point xi to the cluster\nl = argminl=1,...,k ∥xi -μl∥.\n- Update the centers μl =\nx\n|Sl|\ni∈S\ni.\nl\nUnfortunately, Lloyd's algorithm\nP\nis not guaranteed to converge to the solution of (25). Indeed,\nLloyd's algorithm oftentimes gets stuck in local optima of (25). A few lectures from now we'll discuss\nconvex relaxations for clustering, which can be used as an alternative algorithmic approach to Lloyd's\nalgorithm, but since optimizing (25) is NP-hard there is not polynomial time algorithm that works\nin the worst-case (assuming the widely believed conjecture P = NP)\nWhile popular, k-means clustering has some potential issues:\n- One needs to set the number of clusters a priori (a typical way to overcome this issue is by trying\nthe algorithm for different number of clusters).\n- The way (25) is defined it needs the points to be defined in an Euclidean space, oftentimes\nwe are interested in clustering data for which we only have some measure of affinity between\ndifferent data points, but not necessarily an embedding in Rp (this issue can be overcome by\nreformulating (25) in terms of distances only).\n\nFigure 14: The d = 3 example of the use of this method to the example described above, the value of\nthe nodes is given by color coding. For d = 3 the solution appears to only learn the label -1.\nFigure 15: The d = 3 example of the use of this method with the extra regularization fT L2f to the\nexample described above, the value of the nodes is given by color coding. The extra regularization\nseems to fix the issue of discontinuities.\n- The formulation is computationally hard, so algorithms may produce suboptimal instances.\n- The solutions of k-means are always convex clusters. This means that k-means may have diffi-\nculty in finding cluster such as in Figure 17.\n3.2\nSpectral Clustering\nA natural way to try to overcome the issues of k-means depicted in Figure 17 is by using Diffusion\nMaps: Giventhe data points we construct a weighted graph G = (V, E, W) using a kernel Kε, such as\nKε(u) = exp\n1 u2\n, by associating each point to a vertex and, for which pair of nodes, set the edge\n2ε\nweight as\nwij = Kε (∥xi -xj∥) .\n\nFigure 16: Examples of points separated in clusters.\nRecall the construction of a matrix M = D-1W as the transition matrix of a random walk\nw\nProb {\nij\nX(t + 1) = j|X(t) = i} =\n= Mij,\ndeg(i)\nwhere D is the diagonal with Dii = deg(i). The d-dimensional Diffusion Maps is given by\n(d)\nφt (i) =\n\nt\n\nwhere\n\nλ φ2(i)\n...\nλt\nd+1φd+1(i)\nM\n,\n= ΦΛΨT where Λ is the diagonal matrix with the eigenvalues of M and Φ and Ψ are,\nrespectively, the right and left eigenvectors of M (note that they form a bi-orthogonal system, ΦT Ψ =\nI).\nIf we want to cluster the vertices of the graph in k clusters, then it is natural to truncate the\nDiffusion Map to have k -1 dimensions (since in k -1 dimensions we can have k linearly separable\nsets). If indeed the clusters were linearly separable after embedding then one could attempt to use\nk-means on the embedding to find the clusters, this is precisely the motivation for Spectral Clustering.\nAlgorithm 3.1 (Spectral Clustering) Given a graph G = (V, E, W) and a number of clusters k\n(and t), Spectral Clustering consists in taking a (k -1) dimensional Diffusion Map\n\nλt\n2φ2(i)\n(k\nφ\n-1)\n.\nt\n(i) =\n..\nλt\n\nkφk(i)\n\n(k\n1)\n(k\n1)\n\n(k\n1)\n\nand clustering the points φt\n-(1), φt\n-(2), . . . , φt\n-(n) ∈Rk-1 using, for example, k-means clus-\ntering.\n\nFigure 17: Because the solutions of k-means are always convex clusters, it is not able to handle some\ncluster structures.\n3.3\nTwo clusters\nWe will mostly focus in the case of two cluster (k = 2). For k = 2, Algorithm 3.1 consists in assigning\nto each vertex i a real number φ2(i) and then clustering the points in the real line. Note in R, clustering\nreduces to setting a threshold τ and taking S = {i ∈V : φ2(i) ≤τ}. Also, it is computationally\ntractable to try all possible thresholds (there are ≤n different possibilities).\nFigure 18: For two clusters, spectral clustering consists in assigning to each vertex i a real number\nφ2(i), then setting a threshold τ and taking S = {i ∈V : φ2(i) ≤τ}.\nAlgorithm 3.2 (Spectral Clustering for two clusters) Given a graph G = (V, E, W), consider\nthe two-dimensional Diffusion Map\ni →φ2(i).\nset a threshold τ (one can try all different possibilities) and set\nS = {i ∈V : φ2(i) ≤τ}.\n\nIn what follows we'll give a different motivation for Algorithm 3.2.\n3.3.1\nNormalized Cut\nGiven a graph G = (V, E, W), a natural measure to measure a vertex partition (S, Sc) is\ncut(S) =\nX X\nwij.\ni∈S j∈Sc\nNote however that the minimum cut is achieved for S = ∅(since cut(∅) = 0) which is a rather\nmeaningless choice of partition.\nRemark 3.3 One way to circumvent this issue is to ask that |S| = |Sc| (let's say that the number of\nvertices n = |V | is even), corresponding to a balanced partition. We can then identify a partition with\na label vector y ∈P\n{±1}n where yi = 1 is i ∈S, and yi = -1 otherwise. Also, the balanced condition\nn\ncan be written as\ni=1 yi = 0. This means that we can write the minimum balanced cut as\nmin cut(S) =\nmin\nS⊂V\ny\n|S|=|Sc\n∈{-1,1}n\n|\n1T y=0\nX\ni≤j\nwij (yi -yj)2 = 1\nmin\nyT LGy,\n4 y∈{-1,1}n\n1T y=0\nwhere L\nG = D -W is the graph Laplacian.\n.\nSince asking for the partition to be balanced is too restrictive in many cases, there are several\nways to evaluate a partition that are variations of cut(S) that take into account the intuition that one\nwants both S and Sc to not be too small (although not necessarily equal to |V |/2). A prime example\nis Cheeger's cut.\nDefinition 3.4 (Cheeger's cut) Given a graph and a vertex partition (S, Sc), the cheeger cut (also\nknown as conductance, and sometimes expansion) of S is given by\ncut(S)\nh(S) =\n,\nmin{vol(S), vol(Sc)}\nwhere vol(S) = P\ni∈S deg(i).\nAlso, the Cheeger's constant of G is given by\nhG = min h(S).\nS⊂V\nA similar object is the Normalized Cut, Ncut, which is given by\ncut(S)\nNcut(S) = vol(S) + cut(Sc).\nvol(Sc)\nNote that Ncut(S) and h(S) are tightly related, in fact it is easy to see that:\nh(S) ≤Ncut(S) ≤2h(S).\n13W is the matrix of weights and D the degree matrix, a diagonal matrix with diagonal entries Dii = deg(i).\n\nBoth h(S) and Ncut(S) favor nearly balanced partitions, Proposition 3.5 below will give an inter-\npretation of Ncut via random walks.\nLet us recall the construction form previous lectures of a random walk on G = (V, E, W):\nw\nProb {X\n}\nij\n(t + 1) = j|X(t) = i =\n= Mij,\ndeg(i)\nwhere M = D-1W. Recall that M = ΦΛΨT where Λ is the diagonal matrix with the eigenvalues λk\nof M and Φ and Ψ form a biorthogonal system ΦT Ψ = I and correspond to, respectively, the right\n-1\nand left eigenvectors of M. Moreover they are given by Φ = D\n2 V and Ψ = D\n2 V where V T V = I\nand D-1\n2 WD-1\nT\n2 = V ΛV\nis the spectral decomposition of D-2 WD-1\n2 .\nRecall also that M1 = 1, corresponding to Mφ1 = φ1, which means that ψT\n1 M = ψT\n1 , where\nψ1 = D 2 v1 = Dφ1 = [deg(i)]1≤i≤n .\nThis means that\nh\ndeg(i)\nis the stationary distribution of this random walk. Indeed it is easy\nvol(G) 1≤i≤n\nto check that, if X(t) has\ni\na certain distribution pt then X(t + 1) has a distribution pt+1 given by\npT\nt+1 = pT\nt M\nProposition 3.5 Given a graph G = (V, E, W) and a partition (S, Sc) of V , Ncut(S) corresponds\nto the probability, in the random walk associated with G, that a random walker in the stationary\ndistribution goes to Sc conditioned on being in S plus the probability of going to S condition on being\nin Sc, more explicitly:\nNcut(S) = Prob {X(t + 1) ∈Sc|X(t) ∈S} + Prob {X(t + 1) ∈S|X(t) ∈Sc} ,\n)\nwhere Prob{X(t) = }\ndeg(i\ni =\n.\nvol(G)\nProof.\nWithout loss of generality we can take t = 0. Also, the second term in the sum corresponds\nto the first with S replaced by Sc and vice-versa, so we'll focus on the first one. We have:\nProb X(1)\nSc\nX(0)\nS\nProb {X(1) ∈Sc\n{\n∈\n}\n|X(0)\n∩\n∈S\n∈\n}\n=\nProb {X(0) ∈S}\n=\nP\ni∈S\nP\nj∈Sc Prob {X(1) ∈j ∩X(0) ∈i}\ndeg\ni\n=\nP\ni∈S Prob {X(0) = i}\nP\n∈S\nP\n(i)\nj∈Sc vol(G)\nwij\ndeg(i)\nP\ni∈S\ndeg(i)\nP\nvol(G)\ni\n=\n∈S\nP\nj∈Sc wij\nP\ni∈S deg(i)\n=\ncut(S).\nvol(S)\nAnalogously,\nProb {X(t + 1) ∈S|X(t) ∈Sc\ncut(S)\n} =\n,\nvol(Sc)\nwhich concludes the proof.\n\n3.3.2\nNormalized Cut as a spectral relaxation\nBelow we will show that Ncut can be written in terms of a minimization of a quadratic form involving\nthe graph Laplacian LG, analogously to the balanced partition.\nRecall that balanced partition can be written as\nmin\nyT LGy.\n4 y∈{-1,1}n\n1T y=0\nAn intuitive way to relax the balanced condition is to allow the labels y to take values in two\ndifferent real values a and b (say yi = a if i ∈S and yj = b if i ∈/ S) but not necessarily ±1. We can\nthen use the notion of volume of a set to ensure a less restrictive notion of balanced by asking that\na vol (S) + b vol (Sc) = 0,\nwhich corresponds to 1T Dy = 0.\nWe also need to fix a scale/normalization for a and b:\na2 vol (S) + b2 vol (Sc) = 1,\nwhich corresponds to yT Dy = 1.\nThis suggests considering\nmin\nyT LGy.\ny∈{a,b}n\n1T Dy=0, yT Dy=1\nAs we will see below, this corresponds precisely to Ncut.\nProposition 3.6 For a and b to satisfy a vol (S) + b vol (Sc) = 0 and a2 vol (S) + b2 vol (Sc) = 1 it\nmust be that\na =\n\nvol(Sc)\nvol(S) vol(G)\nand\nb = -\n\nvol(S)\nvol(Sc) vol(G)\n,\ncorresponding to\nyi =\n\nvol(Sc)\nvol(S) vol(G)\n\nif i ∈S\n-\n\nvol(S)\nvol(Sc) vol(G)\nif i ∈Sc.\nProof.\nThe proof involves only doing simple algebraic manipulations together with noticing that\nvol(S) + vol(Sc) = vol(G).\nProposition 3.7\nNcut(S) = yT LGy,\nwhere y is given by\nyi =\n\nvol(Sc)\n\nvol(S) vol(G)\nif i ∈S\n-\n\nvol(S)\nvol(Sc) vol(G)\nif i ∈Sc.\n\nProof.\nyT LGy\n=\nwij(y\ni\n2 i,j\n-yj)\n=\nX X\nw\nij(yi -yj)\ni∈S j∈Sc\n=\nX X\nvol(Sc)\nvol(S)\nwij\n\"\n+\nc\n\nvol(S) vol(G)\n\nvol(Sc) vol(G)\ni∈S j S\n#\n=\nX\ni∈S j\nX\n∈\nv\nwij\n+\nc\nvol(G)\nvol(Sc)\nol(S) + 2\nvol(S)\nvol(Sc)\n∈S\n\nX X\nvol(Sc)\nvol(S)\nvol(S)\nvol(Sc)\n=\nwij\n+\n+\n+\nc\nvol(G)\nvol(S)\nvol(Sc)\nvol(S)\nvol(Sc)\ni∈S j∈S\n\n1 X\n=\nX\ni∈S\nX\nj∈Sc\nwij\n\n+\nvol(S)\nvol(Sc)\n\n=\ncut(S)\n\nvol(S) +\nvol(Sc)\n=\nNcut(S).\n\nThis means that finding the minimum Ncut corresponds to solving\nmin\nyT LGy\ns. t.\ny ∈{a, b}n for some a and b\nyT\n(26)\nDy = 1\nyT D1 = 0.\nSince solving (26) is, in general, NP-hard, we consider a similar problem where the constraint that\ny can only take two values is removed:\nmin\nyT LGy\ns. t.\ny ∈Rn\nyT\n(27)\nDy = 1\nyT D1 = 0.\nGiven a solution of (27) we can round it to a partition by setting a threshold τ and taking\nS = {i ∈V : yi ≤τ}.\nWe will see below that (27) is an eigenvector problem (for this reason we\ncall (27) a spectral relaxation) and, moreover, that the solution corresponds to y a multiple of φ2\nmeaning that this approach corresponds exactly to Algorithm 3.2.\nIn order to better see that (27) is an eigenvector problem (and thus computationally tractable),\nset z = D 2 y and LG = D-1\n2 LGD-1\n2 , then (27) is equivalent\n\nmin\nzT LGz\ns. t.\nz ∈Rn\n∥z∥2 = 1\n(28)\nT\nD 2 1\nz = 0.\nNote that LG = I -D-1\n2 WD-1\n2 . We order\n\nits\n\neigenvalues in increasing order 0 = λ1 (LG) ≤\nλ2 (LG) ≤· · · ≤λn (L\nG). The eigenvector associated to the smallest eigenvector is given by D 2 1 this\nmeans that (by the variational interpretation of the eigenvalues) that the minimum of (28) is λ2 (LG)\nand the minimizer is given by the second smallest eigenvector of LG = I -D-2 WD-2 , which is the\nsecond largest eigenvector of D-2 WD-2 which we know is v2. This means that the optimal y in (27)\nis given by φ2 = D-1\n2 v2. This confirms that this approach is equivalent to Algorithm 3.2.\nBecause the relaxation (27) is obtained from (26) by removing a constraint we immediately have\nthat\nλ2 (LG) ≤min Ncut(S).\nS⊂V\nThis means that\n1λ2 (LG) ≤hG.\nIn what follows we will show a guarantee for Algorithm 3.2.\nLemma 3.8 There is a threshold τ producing a partition S such that\nh(S) ≤\np\n2λ2 (LG).\nThis implies in particular that\nh(S) ≤\np\n4hG,\nmeaning that Algorithm 3.2 is suboptimal at most by a square root factor.\nNote that this also directly implies the famous Cheeger's Inequality\nTheorem 3.9 (Cheeger's Inequality) Recall the definitions above. The following holds:\n1λ2 (LG) ≤hG ≤\np\n2λ2 (\nG\nL ).\nCheeger's inequality was first established for manifolds by JeffCheeger in 1970 [Che70], the graph\nversion is due to Noga Alon and Vitaly Milman [Alo86, AM85] in the mid 80s.\nThe upper bound in Cheeger's inequality (corresponding to Lemma 3.8) is more interesting but\nmore difficult to prove, it is often referred to as the \"the difficult part\" of Cheeger's inequality. We\nwill prove this Lemma in what follows. There are several proofs of this inequality (see [Chu10] for\nfour different proofs!). The proof that follows is an adaptation of the proof in this blog post [Tre11]\nfor the case of weighted graphs.\nProof. [of Lemma 3.8]\nWe will show that given y ∈Rn satisfying\nyT L\nR\nGy\n(y) := yT Dy ≤δ,\n\nand yT D1 = 0. there is a \"rounding of it\", meaning a threshold τ and a corresponding choice of\npartition\nS = {i ∈V : yi ≤τ}\nsuch that\nh(S)\n√\n≤\n2δ,\nsince y = φ2 satisfies the conditions and gives δ = λ2 (LG) this proves the Lemma.\nWe will pick this threshold at random and use the probabilistic method to show that at least one\nof the thresholds works.\nFirst we can, without loss of generality, assume that y1 ≤· ≤yn (we can simply relabel the\nvertices). Also, note that scaling of y does not change the value of R(y). Also, if yD1 = 0 adding\na multiple of 1 to y can only decrease the value of\n(y): the numerator does not change and the\ndenominator (y + c1)T D(y + c1) = yT Dy + c2\nT\nR\n1 D1 ≥yT Dy.\nThis means that we can construct (from y by adding a multiple of 1 and scaling) a vector x such\nthat\nx1 ≤... ≤xn, xm = 0,\nand x2\n1 + x2\nn = 1,\nand\nxT LGx ≤δ,\nxT Dx\nwhere m be the index for which vol({1, . . . , m-1}) ≤vol({m, . . . , n}) but vol({1, . . . , m}) > vol({m, . . . , n}).\nWe consider a random construction of S with the following distribution. S = {i ∈V : xi ≤τ}\nwhere τ ∈[x1, xn] is drawn at random with the distribution\nb\nProb {τ ∈[a, b]} =\nZ\na\n|τ|dτ,\nwhere x1 ≤a ≤b ≤xn.\nIt is not difficult to check that\nProb {τ ∈[a, b]} =\nb2 -a2\nif a and b have the same sign\na2 + b2\nif a and b have different signs\nLet us start by estimating E cut(S).\nE cut(S)\n=\nE\nw\nX\ni∈V j\nX\nij1(S,Sc) cuts the edge (i,j)\n∈V\n=\nw\nX\nij Prob{(S, Sc) cuts the edge (i, j)}\ni∈V j\nX\n∈V\nNote that Prob{(S, Sc) cuts the edge (i, j)} is\nx2\ni -x2\nj\nis x\ni and xj have the same sign and xi +xj\notherwise. Both cases can be conveniently upper bounded by |xi -xj| (|xi| + |xj|). This means that\nE cut(S)\n≤\nw\nX\nij\ni,j\n|xi -xj| (|xi| + |xj|)\n≤\nsX\nw (x -x )2\nij\ni\nj\nij\nsX\nwij(\nij\n|xi| + |xj|) ,\n\nwhere the second inequality follows from the Cauchy-Schwarz inequality.\nFrom the construction of xX\nwe know that\nw\nij(xi\nxj) = 2xT LGx\n2δxT Dx.\nij\n-\n≤\nAlso,\nX\nwij(\nij\n|xi| + |x |)2 ≤\nX\nw 2x2 + 2x2. = 2\nX\ndeg(i)x2\nT\nj\nij\ni\nj\ni\n+ 2\ndeg(j)xj\n= 4x Dx.\nij\ni\n!\n\nX\nj\n\nThis means that\n\nE cut(S) ≤2\n√\n2δxT Dx\n√\n4xT Dx =\n√\n2δ xT Dx.\nOn the other hand,\nn\nE min{vol S, vol Sc} =\ndeg(i) Prob\ni=1\n{xi is in the smallest set (in terms of volume)},\nto break ties, if vol(S) = vol(S\nX\nc) we take the \"smallest\" set to be the one with the first indices.\nNote that m is always in the largest set. Any vertex j < m is in the smallest set if xj ≤τ ≤xm = 0\nand any j > m is in the smallest set if 0 = xm ≤τ ≤xj. This means that,\nProb{xi is in the smallest set (in terms of volume) = x2\nj.\nWhich means that\nn\nE min{vol S, vol Sc} =\nX\ndeg(i)x2\ni = xT Dx.\ni=1\nHence,\nE cut(S)\nE min{vol S, vol Sc} ≤\n√\n2δ.\nE cut(S)\ncut(S)\nNote however that because\nis not necessarily the same as\nand so,\nE min{vol S,vol Sc\nE\n}\nmin{vol S,vol Sc}\nwe do not necessarily have\ncut(S)\nE\n√\nmin{vol S, vol Sc\n≤\n2δ.\n}\nHowever, since both random variables are positive,\nE cut(S) ≤E min{vol S, vol Sc √\n}\n2δ,\nor equivalently\nE\nh\ncut(S) -min{vol S, vol Sc √\n}\n2δ ≤0,\nwhich guarantees, by the probabilistic method, the existence of S\ni\nsuch that\ncut(S) ≤min{vol S, vol Sc √\n}\n2δ,\nwhich is equivalent to\ncut(S)\nh(S) =\n√\n2δ,\nmin{vol S, vol Sc\n≤\n}\nwhich concludes the proof of the Lemma.\n\n3.4\nSmall Clusters and the Small Set Expansion Hypothesis\nWe now restrict to unweighted regular graphs G = (V, E).\nCheeger's inequality allows to efficiently approximate its Cheeger number up to a square root\nfactor. It means in particular that, given G = (V, E) and φ we can efficiently between the cases where:\nhG ≤φ or hG ≥2√φ. Can this be improved?\nOpen Problem 3.1 Does there exists a constant c > 0 such that it is NP-hard to, given φ, and G\ndistinguis between the cases\n1. hG ≤φ, and\n2. hG ≥c√φ?\nIt turns out that this is a consequence [RST12] of an important conjecture in Theoretical Computer\nScience (see [BS14] for a nice description of it). This conjecture is known [RS10] to imply the Unique-\nGames Conjecture [Kho10], that we will discuss in future lectures.\nConjecture 3.10 (Small-Set Expansion Hypothesis [RS10]) For every ε > 0 there exists δ > 0\nsuch that it is NP-hard to distinguish between the cases\n)\n1. There exists a subset S ⊂\ncut(S\nV with vol(S) = δ vol(V ) such that vol(S) ≤ε,\ncut(S)\n2.\n≥1\nol(\nS\n-ε, for every S\nv\nvol( )\n⊂V satisfying\nS) ≤δ vol(V ).\n3.5\nComputing Eigenvectors\nSpectral clustering requires us to compute the second smallest eigenvalue of LG. One of the most\nefficient ways of computing eigenvectors is through the power method. For simplicity we'll consider\nthe case on which we are computing the leading eigenvector of a matrix A ∈Rn×n with m non-\nzero entries, for which |λmax(A)| ≥|λmin(A)| (the idea is easily adaptable).\nThe power method\nproceeds by starting with a guess y0\nAyt\nand taking iterates yt+1 = ∥Ayt . One can show [KW92] that the\n∥\nvariantes of the power method can find a vector x in randomized time O δ-1(m + n) log n satisfying\nxT Ax ≥λmax(A)(1 -δ)xT x. Meaning that an approximate solution can\nbe found in quasi-linear\ntime.14\n\nOne drawback of the power method is that when using it, one cannot be sure, a posteriori, that\nthere is no eigenvalue of A much larger than what we have found, since it could happen that all our\nguesses were orthogonal to the corresponding eigenvector. It simply guarantees us that if such an\neigenvalue existed, it would have been extremely likely that the power method would have found it.\nThis issue is addressed in the open problem below.\nOpen Problem 3.2 Given a symmetric matrix M with small condition number, is there a quasi-\nlinear time (on n and the number of non-zero entries of M) procedure that certifies that M ⪰0. More\nspecifically, the procedure can be randomized in the sense that it may, with some probably not certify\nthat M ⪰0 even if that is the case, what is important is that it never produces erroneous certificates\n(and that it has a bounded-away-from-zero probably of succeeding, provided that M ⪰0).\n14Note that, in spectral clustering, an error on the calculation of φ2 propagates gracefully to the guarantee given by\nCheeger's inequality.\n\nThe Cholesky decomposition produces such certificates, but we do not know how to compute it\nin quasi-linear time. Note also that the power method can be used in αI -M to produce certifi-\ncates that have arbitrarily small probability of being false certificates. Later in these lecture we will\ndiscuss the practical relevance of such a method as a tool to quickly certify solution produced by\nheuristics [Ban15b].\n3.6\nMultiple Clusters\nGiven a graph G = (V, E, W), a natural way of evaluating k-way clusterign is via the k-way expansion\nconstant (see [LGT12]):\n,...,S\ncut(S)\nρG(k) =\nmin\nmax\nS1\nk l=1,...,k\nv\n\n,\nol(S)\nwhere the maximum is over all choice of k disjoin subsets of V (but not necessarily forming a partition).\nAnother natural definition is\nφG(k) =\nmin\nS:vol S≤1\nk vol(G)\ncut(S).\nvol(S)\nIt is easy to see that\nφG(k) ≤ρG(k).\nThe following is known.\nTheorem 3.11 ([LGT12]) Let G = (V, E, W) be a graph and k a positive integer\nρG(k) ≤O\nk2 p\nλk,\n(29)\nAlso,\nρG(k) ≤O\np\nλ2k log k\n\n.\nOpen Problem 3.3 Let G = (V, E, W) be a graph and k a positive integer, is the following true?\nρG(k) ≤polylog(k)\np\nλk.\n(30)\nWe note that (30) is known not to hold if we ask that the subsets form a partition (meaning that\nevery vertex belongs to at least one of the sets) [LRTV12]. Note also that no dependency on k would\ncontradict the Small-Set Expansion Hypothesis above.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 12-14: Concentration Inequalities, Scalar and Matrix Versions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/87db5678b0405c1087b0e85181b64c1a_MIT18_S096F15_Ses12_14.pdf",
      "content": "Concentration Inequalities, Scalar and Matrix Versions\n4.1\nLarge Deviation Inequalities\nConcentration and large deviations inequalities are among the most useful tools when understanding\nthe performance of some algorithms. In a nutshell they control the probability of a random variable\nbeing very far from its expectation.\nThe simplest such inequality is Markov's inequality:\nTheorem 4.1 (Markov's Inequality) Let X ≥0 be a non-negative random variable with E[X] <\ninf. Then,\nE[X]\nProb{X > t} ≤\nt\n.\n(31)\nProof.\nLet t > 0. Define a random variable Yt as\nYt =\nif X ≤t\nt\nif X > t\nClearly, Yt ≤X, hence E[Yt] ≤E[X], and\nt Prob{X > t} = E[Yt] ≤E[X],\nconcluding the proof.\nMarkov's inequality can be used to obtain many more concentration inequalities. Chebyshev's\ninequality is a simple inequality that control fluctuations from the mean.\nTheorem 4.2 (Chebyshev's inequality) Let X be a random variable with E[X2] < inf. Then,\nVar(X)\nProb{|X -EX| > t} ≤\n.\nt2\nProof. Apply Markov's inequality to the random variable (X -E[X])2 to get:\n(X\nProb X\nEX > t = Prob\nt2\nE\n(X\nEX)2 >\n\n-EX)2\n{|\n-\n|\n}\n{\n-\n} ≤\nt2\n\nVar(X)\n=\n.\nt2\n4.1.1\nSums of independent random variables\nIn what follows we'll show two useful inequalities involving sums of independent random variables.\nThe intuitive idea is that if we have a sum of independent random variables\nX = X1 + · · · + Xn,\nwhere Xi are iid centered random variables, then while the value of X can be of order O(n) it will very\nlikely be of order O(√n) (note that this is the order of its standard deviation). The inequalities that\nfollow are ways of very precisely controlling the probability of X being larger than O(√n). While we\ncould use, for example, Chebyshev's inequality for this, in the inequalities that follow the probabilities\nwill be exponentially small, rather than quadratic, which will be crucial in many applications to come.\n\nTheorem 4.3 (Hoeffding's Inequality) Let X1, X2, . . . , Xn be independent bounded random vari-\nables, i.e., |Xi| ≤a and E[Xi] = 0. Then,\n(X\nn\n\n)\n\nt2\nProb\n\nXi > t\n=1\n≤2 exp\ni\n-2na2\n\n.\nThe inequality implies that fluctuations larger than O (√n) have small probability. For example,\nfor t = a√2n log n we get that the probability is at most 2 .\nn n\nProof.\nWe first get a probability bound for the event\ni=1 Xi > t. The proof, again, will follow\nfrom Markov. Since we want an exponentially small probability, we use a classical trick that involves\nexponentiating with any λ > 0 and then choosing the optim\nP\nal λ.\nProb\n(X\nn\nXi > t\ni=1\n)\n=\nProb\n(X\nn\nXi > t\ni=1\n)\n(32)\n=\nProb\nn\neλ Pn\nX\nλt\ni=1\ni > e\nE[eλ\no\n≤\nPn\ni=1 Xi]\netλ\nn\n=\ne-tλ Y\nE[eλXi],\n(33)\ni=1\nwhere the penultimate step follows from Markov's inequality and the last equality follows from inde-\npendence of the Xi's.\nWe now use the fact that |Xi| ≤a to bound E[eλXi]. Because the function f(x) = eλx is convex,\nλx\na + x\ne\n≤\n2a eλa + a -xe-λa,\n2a\nfor all x ∈[-a, a].\nSince, for all i, E[Xi] = 0 we get\nE[eλXi] ≤E\na + Xi\n2a\neλa + a -Xi\n2a\ne-λa\n\n≤1\n\neλa + e-λa\n= cosh(λa)\nNote that15\ncosh(x) ≤ex2/2,\nfor all x ∈R\nHence,\nE[eλXi] ≤E[e(λXi)2/2] ≤e(λa)2/2.\nTogether with (32), this gives\nProb\n(X\nn\nn\nXi > t\n)\n≤\ne-tλ\n=1\nY\ne(λa) /2\ni\ni=1\n=\ne-tλen(λa)2/2\n15This follows immediately from the Taylor expansions: cosh(x) = Pinf\nn\nx\nn=0 (2n)!, ex2/2 = Pinf\nn=0\nx2n ,\n2n\nand (2n)!\nn!\n≥2nn!.\n\nThis inequality holds for any choice of λ ≥0, so we choose the value of λ that minimizes\nmin\nλ\n(λa)2\nn\ntλ\n-\n\nDifferentiating readily shows that the minimizer is given by\nt\nλ =\n,\nna2\nwhich satisfies λ > 0. For this choice of λ,\nn(λa)2\n/2 -tλ = n\nt2\n2a2 -t2\na2\n\n= -t2\n2na2\nThus,\nProb\n(X\nn\nXi > t\ni=1\n)\n≤\ne-\nt\n2na\nBy using the same argument on Pn\ni=1 (-Xi), and union bounding over the two events we get,\nProb\n(X\nn\n\nXi\ni\n> t\n)\n≤\n2e-\nt\n=1\n2na\nRemark 4.4 Let's say that we have random variables r1, . . . , rn i.i.d. distributed as\n-\nri =\n\nwith probability p/2\n\nwith probability 1 -p\nwith probability p/2.\nThen, E(ri) = 0 and |ri| ≤1 so Hoeffding's inequality gives:\nProb\n(X\nn\n\nri\ni=1\n> t\n)\n2 exp\n\nt\n≤\n-\n.\n2n\n\nIntuitively, the smallest p is the more concentrated |Pn\ni=1 ri| should be, however Hoeffding's in-\nequality does not capture this behavior.\nn\nA natural way to quantify this intuition is by noting that the variance of\ni=1 ri depends on p as\nVar(ri) = p. The inequality that follows, Bernstein's inequality, uses the variance of the summands to\nimprove over Hoeffding's inequality.\nP\nThe way this is going to be achieved is by strengthening the proof above, more specifically in\nstep (33) we will use the bound on the variance to get a better estimate on E[eλXi] essentially by\nrealizing that if Xi is centered, EX2\ni = σ2\n, and |Xi| ≤a then, for k ≥2, EXk\ni ≤σ2ak-2 =\n\nσ\na\na2\n\nk.\n\nTheorem 4.5 (Bernstein's Inequality) Let X1, X2, . . . , Xn be independent centered bounded ran-\ndom variables, i.e., |Xi| ≤a and E[X\ni] = 0, with variance E[Xi ] = σ . Then,\nn\nt2\nProb\n(\nX\n\nXi > t\n2 exp\ni=1\n)\n\n≤\n\n-2nσ2 + 2\n3at\n!\n.\nRemark 4.6 Before proving Bernstein's Inequality, note that on the example of Remark 4.4 we get\nProb\n(\nn\nX\ni=1\nri\n> t\n)\n≤2 exp\n\n-\nt2\n2np + 2\n,\nt\n!\nwhich exhibits a dependence on p and, for small values of p is considerably smaller than what Hoeffd-\ning's inequality gives.\nProof.\nAs before, we will prove\n(X\nn\nProb\nXi\nt\ni=1\n)\n≤exp\n\nt\n>\n-2nσ2 + 2\n,\nat\n!\n-\nn\nand then union bound with the same result for\ni=1 Xi, to prove the Theorem.\nFor any λ > 0 we have\nP\nProb\n(X\nn\nXi > t\ni=1\n)\n=\nProb{eλ P Xi > eλt}\nE[eλ\n≤\nP Xi]\neλt\nn\n=\ne-λt Y\nE[eλXi]\ni=1\nNow comes the source of the improvement over Hoeffding's,\nE[eλXi]\n=\nE\n\"\ninf\n1 + λXi +\nX λmXm\ni\nm=2\nm!\n#\n≤\n1 +\ninf\nX\nm=2\nλmam-2σ2\nm!\nσ2\n=\n1 +\n(\nm\nX\ninf\nλa)m\na2\n=2\nm!\n=\n1 + σ2\ne\na2\n\nλa -1 -λa\nTherefore,\n\nProb\n(X\nn\nXi > t\ni=1\n)\n≤e-λt\n\nσ2\n1 + a2\n\neλa -1 -λa\nn\n\nWe will use a few simple inequalities (that can be easily proved with calculus) such as16 1 + x ≤\nex, for all x ∈R.\nThis means that,\nσ2\nσ (eλa\nλa)\n1 +\neλa\na\n\n-\n-λa\n\n≤e\na\n--\n,\nwhich readily implies\n( n\n)\n)\nProb\nXi > t\ni=1\n≤e-\nnσ\nλt\n2 (eλa\ne a\n-1-λa .\nAs before, we try to find the value\nX\nof λ > 0 that minimizes\nnσ2\nmin\nλt +\n(eλa\nλa)\nλ\n\n-\na\n-\n-\n\nDifferentiation gives\nnσ2\n-t +\n(aeλa -a) = 0\na2\nwhich implies that the optimal choice of λ is given by\nat\nλ∗=\nlog\n1 +\na\n\nnσ2\n\nIf we set\nat\nu =\n,\n(34)\nnσ2\nthen λ∗= 1 log(1 + u).\na\nNow, the value of the minimum is given by\nnσ2\nnσ2\n-∗\n∗\nλ t +\n(eλ a -1 -λ∗a) = -\n[(1 + u) log(1 + u)\na2\na2\n-u] .\nWhich means that,\nProb\n( n\nnσ\nXi > t\ni=1\n)\n≤\nexp\n\n-a2 {(1 + u) log(1 + u) -u}\n\nThe rest of the proof follo\nX\nws by noting that, for every u > 0,\nu\n(1 + u) log(1 + u) -u ≥\n,\n(35)\n2 + 2\nu\nwhich implies:\n(X\nn\n)\n\nnσ2\nu\nProb\nXi > t\ni=1\n≤\nexp\n-a2\n2 + 2\nu\n!\n=\nexp\n\nt2\n-\n.\n2nσ2 + 2at\n!\n16In fact y = 1 + x is a tangent line to the graph of f(x) = ex.\n\n4.2\nGaussian Concentration\nOne of the most important results in concentration of measure is Gaussian concentration, although\nbeing a concentration result specific for normally distributed random variables, it will be very useful\nthroughout these lectures. Intuitively it says that if F : Rn →R is a function that is stable in terms\nof its input then F(g) is very well concentrated around its mean, where g ∈N(0, I). More precisely:\nTheorem 4.7 (Gaussian Concentration) Let X = [X1, . . . , Xn]T be a vector with i.i.d. standard\nGaussian entries and F : Rn →R a σ-Lipschitz function (i.e.: |F(x) -F(y)| ≤σ∥x -y∥, for all\nx, y ∈Rn). Then, for every t ≥0\n≤\n\nt2\nProb {|F(X) -EF(X)| ≥t}\n2 exp\n-2σ2\n\n.\nFor the sake of simplicity we will show the proof for a slightly weaker bound (in terms of the constant\ninside the exponent): Prob {|F(X) -EF(X)| ≥t} ≤2 exp\n\n-2\nπ2 t2\nσ2 . This exposition follows closely\nthe proof of Theorem 2.1.12 in [Tao12] and the original argument\n\nis due to Maurey and Pisier. For\na proof with the optimal constants see, for example, Theorem 3.25 in these notes [vH14]. We will\nalso assume the function F is smooth -- this is actually not a restriction, as a limiting argument can\ngeneralize the result from smooth functions to general Lipschitz functions.\nProof.\nIf F is smooth, then it is easy to see that the Lipschitz property implies that, for every x ∈Rn,\n∥∇F(x)∥2 ≤σ. By subtracting a constant to F, we can assume that EF(X) = 0. Also, it is enough\nto show a one-sided bound\nProb {F(X) -EF(X) ≥t} ≤exp\n\n-π2\nt2\n,\nσ2\n\nsince obtaining the same bound for -F(X) and taking a union bound would gives the result.\nWe start by using the same idea as in the proof of the large deviation inequalities above; for any\nλ > 0, Markov's inequality implies that\nProb {F(X) ≥t}\n=\nProb {exp (λF(X)) ≥exp (λt)}\nE [exp (λF(X))]\n≤\nexp (λt)\nThis means we need to upper bound E [exp (λF(X))] using a bound on ∥∇F∥. The idea is to\nintroduce a random independent copy Y of X. Since exp (λ·) is convex, Jensen's inequality implies\nthat\nE [exp (-λF(Y ))] ≥exp (-EλF(Y )) = exp(0) = 1.\nHence, since X and Y are independent,\nE [exp (λ [F(X) -F(Y )])] = E [exp (λF(X))] E [exp (-λF(Y ))] ≥E [exp (λF(X))]\nNow we use the Fundamental Theorem of Calculus in a circular arc from X to Y :\nπ\nF(X) -F(Y ) =\nZ\n∂\n∂θF (Y cos θ + X sin θ) dθ.\n\nThe advantage of using the circular arc is that, for any θ, Xθ := Y cos θ + X sin θ is another random\nvariable with the same distribution. Also, its derivative with respect to θ, Xθ\n′ = -Y sin θ + X cos θ\nalso is. Moreover, Xθ and Xθ\n′ are independent. In fact, note that\nE\nh\nXθXθ\n′ T i\n= E\nT\n[Y cos θ + X sin θ] [-Y sin θ + X cos θ]\n= 0.\nWe use Jensen's again (with respect to the integral now) to get:\nπ\nexp (λ [F(X) -F(Y )])\n=\nexp\n\nλ 2\nπ/2\nZ π/2\n∂\n∂θF (Xθ) dθ\n!\n≤\nπ/2\nZ π/2\nexp\n\nλπ\n∂F (Xθ)\n∂θ\n\ndθ\nUsing the chain rule,\nexp (λ [F(X) -F(Y )]) ≤π\nZ π/2\nexp\n\nλπ\nF\n2 ∇\n(Xθ) · Xθ\n′\ndθ,\nand taking expectations\nE exp (λ [F(X) -F(Y )]) ≤π\nZ π/2\nE exp\n\nλπ\nF\n2 ∇\n(Xθ) · Xθ\n′\ndθ,\nIf we condition on Xθ, since\nλ π\n2 ∇F (Xθ)\n≤λ π\n2 σ, λ π\n2 ∇F (Xθ) · X′\nθ is a gaussian random variable\nwith variance at most\nλ π\n2 σ\n2. This directly implies that, for every value of Xθ\nEX′\nθ exp\n\nλπ\n2 ∇F (Xθ) · X′\nθ\n\n≤exp\n\nλπ\nσ\n\nTaking expectation now in Xθ, and putting everything together, gives\nE [exp (λF(X))] ≤exp\n\nλπ\nσ\n\n,\nwhich means that\nProb {F(X) ≥t} ≤exp\n\nλπ\nσ\n\n-λt\n\n,\nOptimizing for λ gives λ∗=\nπ\nt ,\nσ2 which gives\nProb {F(X) ≥t} ≤exp\n\n-π2\nt2\nσ2\n\n.\n\n4.2.1\nSpectral norm of a Wigner Matrix\nWe give an illustrative example of the utility of Gaussian concentration. Let W ∈Rn×n be a standard\nGaussian Wigner matrix, a symmetric matrix with (otherwise) independent gaussian entries, the off-\nn(n+1)\ndiagonal entries have unit variance and the diagonal entries have variance 2. ∥W∥depends on\nindependent (standard) gaussian random variables and it is easy to see that it is a\n√\n2-Lipschitz\nfunction of these variables, since\n∥W (1)∥-∥W (2)∥≤\nW (1) -W (2)\n≤\nW (1) -W (2)\n.\nF\nThe symmetry\n√\nof the matrix and the variance\n\n2 of the diagon\n\nal entries are\n\nresponsible for an extra\nfactor of\n2.\nUsing Gaussian Concentration (Theorem 4.7) we immediately get\nProb {∥W∥≥E∥W∥+ t} ≤exp\n\nt2\n-4\n\n.\nSince17 E∥W∥≤2√n we get\nProposition 4.8 Let W ∈Rn×n be a standard Gaussian Wigner matrix, a symmetric matrix with\n(otherwise) independent gaussian entries, the off-diagonal entries have unit variance and the diagonal\nentries have variance 2. Then,\nProb\n\n∥W∥≥2√n + t\n\n≤exp\n\n-t2\n.\n\nNote that this gives an extremely precise control of the fluctuations of ∥W∥. In fact, for t = 2√log n\nthis gives\nProb\nn\n∥W∥≥2√n + 2\np\nlog n\no\n≤exp\n\n-4 log n\n\n= 1 .\nn\n4.2.2\nTalagrand's concentration inequality\nA remarkable result by Talagrand [Tal95], Talangrad's concentration inequality, provides an analogue\nof Gaussian concentration to bounded random variables.\nTheorem 4.9 (Talangrand concentration inequality, Theorem 2.1.13 [Tao12]) Let K > 0,\nand let X1, . . . , Xn be independent bounded random variables, |Xi| ≤K for all 1\nn\n≤i ≤n.\nLet\nF : R →R be a σ-Lipschitz and convex function. Then, for any t ≥0,\nt2\nProb {|F(X) -E [F(X)]| ≥tK} ≤c1 exp\n\n-c2\n,\nσ2\n\nfor positive constants c1, and c2.\nOther useful similar inequalities (with explicit constants) are available in [Mas00].\n17It is an excellent exercise to prove E∥W∥≤2√n using Slepian's inequality.\n\n4.3\nOther useful large deviation inequalities\nThis Section contains, without proof, some scalar large deviation inequalities that I have found useful.\n4.3.1\nAdditive ChernoffBound\nThe additive Chernoffbound, also known as Chernoff-Hoeffding theorem concerns Bernoulli random\nvariables.\nTheorem 4.10 Given 0 < p < 1 and X1, . . . , Xn i.i.d. random variables distributed as Bernoulli(p)\nrandom variable (meaning that it is 1 with probability p and 0 with probability 1 -p), then, for any\nε > 0:\n-\nProb\n(\nn\nn\nX\ni=1\nXi ≥p + ε\n)\n≤\n\"\np\np + ε\np+ε\n1 -p\n1 -p -ε\n1-p-ε#n\n-\nProb\n(\nn\nn\nX\ni=1\nXi ≤p -ε\n)\n≤\n\"\np\np -ε\np-ε\n1 -p\nn\np\n1-+ε\n-p + ε\n#\n4.3.2\nMultiplicative ChernoffBound\nThere is also a multiplicative version (see, for example Lemma 2.3.3. in [Dur06]), which is particularly\nuseful.\nTheorem 4.11 Let X1, . . . , Xn be independent random variables taking values is {0, 1} (meaning they\nare Bernoulli distributed but not necessarily identically distributed). Let μ = E\nδ\nPn\ni=1 Xi, then, for any\n> 0:\nδ\n-\nProb {X > (1 + )μ} <\n\ne\nδ\n(1 + δ)(1+δ)\nμ\n-\nProb {X < (1 -δ)μ} <\n\ne-δ\nμ\n(1 -δ)(1-δ)\n\n4.3.3\nDeviation bounds on χ2 variables\nA particularly useful deviation inequality is Lemma 1 in Laurent and Massart [LM00]:\nTheorem 4.12 (Lemma 1 in Laurent and Massart [LM00]) Let X1, . . . , Xn be i.i.d. standard\ngaussian random variables (N(0, 1)), and a1, . . . , an non-negative numbers. Let\nn\nZ =\nX\na\nk\nk=1\nXk -1\n\n.\nThe following inequalities hold for any t > 0:\n- Prob Z\n√\n{\n≥2∥a∥2\nx + 2∥a∥infx} ≤exp(-x),\n\n- Prob {Z ≤-2∥a∥2\n√x} ≤exp(-x),\n∥∥2\nPn\nwhere\na 2 =\nk=1 a2\nk and ∥a∥\n= max\ninf\n1≤k\na .\n≤n | k|\nNote that if ak = 1, for all k, then Z is a χ2 with n degrees of freedom, so this theorem immediately\ngives a deviation inequality for χ2 random variables.\n4.4\nMatrix Concentration\nIn many important applications, some of which we will see in the proceeding lectures, one needs to\nuse a matrix version of the inequalities above.\nGiven {Xk}n\nk=1 independent random symmetric d × d matrices one is interested in deviation in-\nequalities for\nλmax\nn\nXk\nk=1\n!\n.\nFor example, a very useful adaptation of Bernstein's\nX\ninequality exists for this setting.\nTheorem 4.13 (Theorem 1.4 in [Tro12]) Let {Xk}n\nk=1 be a sequence of independent random sym-\nmetric d × d matrices. Assume that each Xk satisfies:\nEXk = 0 and λmax (Xk) ≤R almost surely.\nThen, for all t ≥0,\nProb\n(\nλmax\nX\nn\nXk\nk=1\n!\n≥t\n)\n≤d · exp\n\n-t2\n2σ2 + 2\nX\nn\nwher\nRt\n!\ne σ2 =\n\nE\nk=1\nX2\nk\n\n.\nNote that ∥A∥denotes the spectral norm of A.\n\nIn what follows we will state and prove various matrix concentration results,\n\nsomewhat\n\nsimilar to\nTheorem 4.13. Motivated by the derivation of Proposition 4.8, that allowed us to easily transform\nbounds on the expected spectral norm of a random matrix into tail bounds, we will mostly focus on\nbounding the expected spectral norm. Tropp's monograph [Tro15b] is a nice introduction to matrix\nconcentration and includes a proof of Theorem 4.13 as well as many other useful inequalities.\nA particularly important inequality of this type is for gaussian series, it is intimately related to\nthe non-commutative Khintchine inequality [Pis03], and for that reason we will often refer to it as\nNon-commutative Khintchine (see, for example, (4.9) in [Tro12]).\nTheorem 4.14 (Non-commutative Khintchine (NCK)) Let A1, . . . , An ∈Rd×d be symmetric\nmatrices and g1, . . . , gn ∼N(0, 1) i.i.d., then:\nE\nX\nn\n\ngkAk\nk=1\n≤\n\n2 + 2 log(2d)\n\n2 σ,\nwhere\nσ2 =\nX\nn\n\nA2\nk\nk=1\n\n.\n(36)\n\nPNote that, akin to Proposition 4.8, we can also use Gaussian Concentration to get a tail bound on\n∥\nn\nk=1 gkAk∥. We consider the function\nF : Rn →\nX\nn\n\ngkAk\nk=1\n\nits\n.\nWe now estimate\nLipschitz constant; let g, h ∈Rn then\n\nX\nn\nX\nn\ngkAk\nhk\nk=1\n\n-\n\nAk\nk=1\n\n!\n\n!\n\n≤\n\nX\nn\nn\n\ngkAk\n-\nk=1\nX\nhkAk\n\nk=1\nX\nn\n\n=\n\n(gk -hk)Ak\nk=1\n\nv\nX\nn\n=\n\nmax\nT\n\n(gk\nh\nv: ∥v∥=1\n-\nk)Ak\nk=1\n!\nv\nn\n=\nmax\nX\n(gk -hk)\nvT Akv\nv: ∥v∥=1 k=1\n\n≤\nmax\nv: ∥v∥=1\nv\nu\nu\nt\nn\nX\nk=1\n(gk -hk)2\nv\nu\nu\nt\nn\nX\nk=1\n(vT Akv)2\n=\nv\nu\nu\nt\nn\nmax\n(vT Akv)\nv: ∥v\n∥g -h∥2,\n∥=1\nX\nk=1\nwhere the first inequality made use of the triangular inequality and the last one of the Cauchy-Schwarz\ninequality.\nThis motivates us to define a new parameter, the weak variance σ .\n∗\nDefinition 4.15 (Weak Variance (see, for example, [Tro15b])) Given A , . . . , A ∈Rd\nd\nn\n× sym-\nmetric matrices. We define the weak variance parameter as\nn\nσ2 =\nmax\nX 2\nvT Akv\n.\n∗\nv: ∥v∥=1 k=1\n\nThis means that, using Gaussian concentration (and setting t = uσ ), we have\n∗\n(X\nn\n≥\n\nProb\n\ngkAk\nk=1\n\n2 + 2 log(2d)\n\n2 σ + uσ∗\n)\n≤exp\n\n-1u2\n\n.\n(37)\nThis means that although the expected value of ∥Pn\nk=1 gkAk∥is controlled by the parameter σ, its\nfluctuations seem to be controlled by σ . We compare the two quantities in the following Proposition.\n∗\n\nProposition 4.16 Given A1, . . . , An ∈Rd×d symmetric matrices, recall that\nσ =\nv\nu\nu\nt\n\nn\nX\nk=1\nA2\nk\n\nand σ∗=\nv\nu\nu\nt\nT\n=1\nX\nn\nmax\n(v Akv) .\nv: ∥v∥\nk=1\nWe have\nσ ≤σ.\n∗\nProof.\nUsing the Cauchy-Schwarz inequality,\nσ2\n=\nmax\n∗\nX\nn 2\nvT Akv\nv: ∥v∥=1 k=1\nn\n\n=\nmax\nvT [Akv]\nv: ∥v∥=1\nX\nk=1\nn\n\n≤\nmax\n( v\nAkv )\nv:\n∥\n∥v\n∥∥∥\n∥=1\nX\nk=1\nX\nn\n=\nmax\nv: ∥v∥=1\n∥Akv∥2\nk=1\nn\n=\nmax\nvT A2\nkv\nv: ∥v∥=1\nX\nk=1\n=\nX\nn\nA2\nk\nk=1\n\n=\n\nσ2.\n\n4.5\nOptimality of matrix concentration result for gaussian series\nThe following simple calculation is suggestive that the parameter σ in Theorem 4.14 is indeed the\ncorrect parameter to understand E ∥\nX\nn\nPn\nk=1 gkAk∥.\nE\n\nn\nn\n\ngkAk\n\n=\nE\n\nX\n\ng\nk\n!\nT\nkA\nk=1\nk=1\n\n= E max v\ngkAk\nv\nv: ∥v∥=1\nX\nk=1\n!\n≥\nmax EvT\nv: ∥v∥=1\nX\nn\ngkAk\nk=1\n!\nv =\nmax vT\nv: ∥v∥=1\nX\nn\nA2\nk\nk=1\n!\nv = σ2\n(38)\nBut a natural question is whether the logarithmic term is needed. Motivated by this question we'll\nexplore a couple of examples.\n\nExample 4.17 We can write a d × d Wigner matrix W as a gaussian series, by taking Aij for i ≤j\ndefined as\nAij = eieT\nj + ejeT\ni ,\nif i = j, and\nAii =\n√\n2eieT\ni .\nIt is not difficult to see that, in this case, P\ni\nj A2\nij = (d + 1)I\n≤\nd×d, meaning that σ =\n√\nd + 1. This\nmeans that Theorem 4.14 gives us\nE∥W∥≲\np\nd log d,\nhowever, we know that E∥W\n√\n∥≍\nd, meaning that the bound given by NCK (Theorem 4.14) is, in this\ncase, suboptimal by a logarithmic factor.18\nThe next example will show that the logarithmic factor is in fact needed in some examples\nT ∈Rd×d\nn\nExample 4.18 Consider Ak = ekek\nfor k = 1, . . . , d. The matrix P\nk=1 gkAk corresponds to\na diagonal matrix with independent standard gaussian random variables as diagonal entries, and so\nit's spectral norm is given by maxk |gk|. It is known that max1≤k≤d |gk\n√\n| ≍\nlog d. On the other hand,\na direct calculation shows that σ = 1. This shows that the logarithmic factor cannot, in general, be\nremoved.\nThis motivates the question of trying to understand when is it that the extra dimensional factor\nn\nis needed. For both these examples, the resulting matrix X = P\nk=1 gkAk has independent entries\n(except for the fact that it is symmetric). The case of independent entries [RS13, Seg00, Lat05, BvH15]\nis now somewhat understood:\nTheorem 4.19 ([BvH15]) If X is a d × d random symmetric matrix with gaussian independent\nentries (except for the symmetry constraint) whose entry i, j has variance b2\nij then\nE∥X∥≲\nv\nu\nu\nt max\n1≤i≤d\nd\nX\nj=1\nb2\nij + max\nij\n|bij|\np\nlog d.\nRemark 4.20 X in the theorem above can be written in terms of a Gaussian series by taking\nAij = bij eieT\nj + e\nT\njei\n,\nfor i < j and Aii = biieieT\ni . One can then compute\nσ and σ\n\n:\n∗\nd\nσ2 = max\nX\nb2 and σ2\n∗≍b2\nij\nij.\n1≤i≤d j=1\nThis means that, when the random matrix in NCK (Theorem 4.14) has negative entries (modulo\nsymmetry) then\nE∥X∥≲σ +\np\nlog dσ∗.\n(39)\n18By a ≍b we mean a ≲b and a ≳b.\n\nTheorem 4.19 together with a recent improvement of Theorem 4.14 by Tropp [Tro15c]19 motivate\nthe bold possibility of (39) holding in more generality.\nConjecture 4.21 Let A , . . . , A ∈Rd\nd\nn\n× be symmetric matrices and g1, . . . , gn ∼N(0, 1) i.i.d., then:\nX\nn\nE\n\ngkAk\nk=1\n≲σ + (log d) 2 σ ,\n∗\nWhile it may very will be that this Conjecture 4.21 is false, no counter example is known, up to\ndate.\nOpen Problem 4.1 (Improvement on Non-Commutative Khintchine Inequality) Prove or\ndisprove Conjecture 4.21.\nI would also be pretty excited to see interesting examples that satisfy the bound in Conjecture 4.21\nwhile such a bound would not trivially follow from Theorems 4.14 or 4.19.\n4.5.1\nAn interesting observation regarding random matrices with independent matrices\nFor the inndepoendent entries setting, Theorem 4.19 is tight (up to constants) for a wide range of variance\nprofiles\nb2\nij\n- the details are available as Corollary 3.15 in [BvH15]; the basic idea is that if the\ni≤j\nlargest variance is comparable to the variance of a sufficient number of entries, then the bound in\nTheorem 4.19 is tight up to constants.\nHowever, the situation is not as well understood when the variance profiles\nb2\nij\nare arbitrary.\ni≤j\nSince the spectral norm of a matrix is always at least the l2 norm of a row, the\nn\nfollo\no\nwing lower bound\nholds (for X a symmetric random matrix with independent gaussian entries):\nE∥X∥≥E max\nk\n∥Xek∥2.\nObservations in papers of Lata la [Lat05] and Riemer and Schutt [RS13], together with the results\nin [BvH15], motivate the conjecture that this lower bound is always tight (up to constants).\nOpen Problem 4.2 (Lata la-Riemer-Schutt) Given X a symmetric random matrix with indepen-\ndent gaussian entries, is the following true?\nE∥X∥≲E max ∥Xek∥2.\nk\nThe results in [BvH15] answer this in the positive for a large range of variance profiles, but not in\nfull generality. Recently, van Handel [vH15] proved this conjecture in the positive with an extra factor\nof √log log d. More precisely, that\nE∥X∥≲\np\nlog log dE max\nk\n∥Xek∥2,\nwhere d is the number of rows (and columns) of X.\n19We briefly discuss this improvement in Remark 4.32\n\n4.6\nA matrix concentration inequality for Rademacher Series\nIn what follows, we closely follow [Tro15a] and present an elementary proof of a few useful matrix\nconcentration inequalities.\nWe start with a Master Theorem of sorts for Rademacher series (the\nRademacher analogue of Theorem 4.14)\nTheorem 4.22 Let H1, . . . , Hn ∈Rd×d be symmetric matrices and ε1, . . . , εn i.i.d.\nRademacher\nrandom variables (meaning = +1 with probability 1/2 and = -1 with probability 1/2), then:\nE\nX\nn\n\nεkHk\n≤\nk=1\n\n1 + 2⌈log(d)⌉\n\n2 σ,\nwhere\nσ2 =\nX\nn\nH2\nk\n(40)\nk=1\n.\nBefore proving this theorem, we take first a small\n\ndetour\n\nin discrepancy theory followed by deriva-\ntions, using this theorem, of a couple of useful matrix concentration inequalities.\n4.6.1\nA small detour on discrepancy theory\nThe following conjecture appears in a nice blog post of Raghu Meka [Mek14].\nConjecture 4.23 [Matrix Six-Deviations Suffice] There exists a universal constant C such that, for\nany choice of n symmetric matrices H\nn\nn\n1, . . . , Hn ∈R ×\nsatisfying ∥Hk∥≤1 (for all k = 1, . . . , n),\nthere exists ε1, . . . , εn ∈{±1} such that\nX\nn\n\n√\n\nεkHk\nk=1\n\n≤C\nn.\nOpen Problem 4.3 Prove or disprove Conjecture 4.23.\nNote that, when the matrices Hk are diagonal, this problem corresponds to Spencer's Six Standard\nDeviations Suffice Theorem [Spe85].\nRemark 4.24 Also, using Theorem 4.22, it is easy to show that if one picks εi as i.i.d. Rademacher\nrandom variables, then with positive probability (via the probabilistic method) the inequality will be\nsatisfied with an extra √log n term. In fact one has\nE\n\nn\nX\nk=1\nεkHk\n≲\np\nlog n\nv\nu\nu\nt\nX\nn\n\nH2\nk\nk=1\n≤\np\nlog n\nv\nu\nu\nt\nn\nX\nk=1\n∥Hk∥2 ≤\np\nlog n√n.\nRemark 4.25 Remark 4.24 motivates asking whether Conjecture 4.23 can be strengthened to ask for\nε1, . . . , εn such that\n\nX\nn\n\nεkHk\nk=1\n\nX\nn\n≲\nH2\n\nk\nk=1\n\n.\n(41)\n\n4.6.2\nBack to matrix concentration\nUsing Theorem 4.22, we'll prove the following Theorem.\nTheorem 4.26 Let T1, . . . , Tn ∈Rd×d be random independent positive semidefinite matrices, then\nE\n\nX\nn\nTi\ni=1\n≤\nX\nn\n\nETi\ni=1\n\n+\np\nC(d)\n\nE max\ni\n∥Ti∥\n\n,\nwhere\nC(d) := 4 + 8⌈log d⌉.\n(42)\nA key step in the proof of Theorem 4.26 is an idea that is extremely useful in Probability, the trick\nof symmetrization. For this reason we isolate it in a lemma.\nLemma 4.27 (Symmetrization) Let T1, . . . , Tn be independent random matrices (note that they\ndon't necessarily need to be positive semidefinite, for the sake of this lemma) and ε1, . . . , εn random\ni.i.d. Rademacher random variables (independent also from the matrices). Then\nE\nX\nn\n\nTi\ni=1\n\nX\nn\n≤\n\nETi\nE\nεiT\ni=1\n\n+ 2\nX\nn\ni\ni=1\n\nProof.\nTriangular inequality gives\n\nE\nX\nn\nn\nn\n\nTi\nE\n(\n=1\n≤\nX\n\nTi\n+ E\nTi\nETi) .\ni\ni=1\n\nX\ni=1\n-\n\nLet us now introduce, for each i, a random matrix\n\nTi\n′ iden\n\ntically distributed\n\nto Ti and independent\n(all 2n matrices are independent). Then\nE\n\nX\nn\n\n(Ti -ETi)\n\n=\nET\nX\nn\n\nTi -ETi\nE\ni=1\ni=1\n-\nTi\n′\nh\nTi\n′ -ETi\n′Ti\n′i\nn\nn\n\n=\nET\nET ′\nX Ti -Ti\n′\ni\n\nTi\n\n=1\n≤E\nX\n-Ti\n′\ni=1\n\nmean that\n\nthe expectation\n\n,\nwhere we use the notation Ea to\nis taken\n\nwith respect\n\nto the variable a\nand the last step follows from Jensen's inequality with respect to ET ′.\nSince Ti -Ti\n′ is a symmetric random variable, it is identically distributed to εi (Ti -Ti\n′) which\ngives\nE\nX\nn\n\nX\nn\nn\nn\nn\n\nTi -Ti\n′\n= E\n\nεi\nTi -Ti\n′\n\n≤E\nX\nX\n\nεiTi\n+ E\n\nεiTi\n′\ni=1\ni=1\ni=1\ni\n\n= 2E\n=1\nX\nεiTi\ni=1\n\n,\nconcluding the proof.\n\nProof. [of Theorem 4.26]\nUsing Lemma 4.27 and Theorem 4.22 we get\nE\nX\nn\nn\n\nTi\nE\ni=1\n≤\nX\n\nTi\ni=1\n+\np\nC(d)E\n\nn\nX\ni=1\nT 2\ni\n\nThe trick now is to make a term like the one in the LHS appear in the RHS. For that we start by\nnoting (you can see Fact 2.3 in [Tro15a] for an elementary proof) that, since Ti ⪰0,\nX\nn\nT 2\ni\ni=1\n\n≤max\ni\n∥Ti∥\n\nX\nn\n\nTi\ni=1\n.\nThis means that\nE\nX\nn\nTi\ni=1\n\n≤\n\nX\nn\n\nETi\ni=1\n+\np\nC(d)E\n\nmax\ni\n∥Ti∥\n\nn\nX\ni=1\nTi\n\n.\nFurther applying the Cauchy-Schwarz inequality for E gives,\nE\n\nX\nn\n\nTi\n≤\nX\nn\n\nETi\n+\ni=1\ni=1\n\np\n\nC(d)\n\nE max\ni\n∥Ti∥\n\nE\n\nn\nX\ni=1\nTi\n\n! 1\n,\nNow that the term E ∥Pn\ni=1 Ti∥appears in the RHS, the proof can be finished with a simple application\nof the quadratic formula (see Section 6.1. in [Tro15a] for details).\nWe now show an inequality for general symmetric matrices\nTheorem 4.28 Let Y1, . . . , Yn ∈Rd×d be random independent positive semidefinite matrices, then\nE\nX\nn\np\n\nYi\ni=1\n\n≤\nC(d)σ + C(d)L,\nwhere,\nσ2 =\nX\nn\n\nEY 2\ni\ni=1\n\nand L = E max\ni\n∥Yi∥2\n(43)\nand, as in (42),\nC(d) := 4 + 8⌈log d⌉.\nProof.\nUsing Symmetrization (Lemma 4.27) and Theorem 4.22, we get\nE\nX\nn\nYi\n≤2E\ni=1\n\nY\n\"\n\nEε\nX\nn\n\nεiYi\ni=1\n#\n\n≤\np\nC(d)E\n\nn\nX\ni=1\nY 2\ni\n\n.\n\nJensen's inequality gives\nE\nX\nn\n\nY 2\ni\ni=1\n\n≤\n\nE\n\nn\nX\ni=1\nY 2\ni\n\n! 1\n,\nand the proof can be concluded by noting that Y 2\ni ⪰0 and using Theorem 4.26.\nRemark 4.29 (The rectangular case) One can extend Theorem 4.28 to general rectangular ma-\ntrices S1, . . . , S\nd1\nd2\nn ∈R\n×\nby setting\nYi =\n\nSi\nST\n,\ni\n\nand noting that\n\nSi\n\nS ST\nY\n\ni\n=\n=\ni\ni\n\nT\n\nT\n\n= max\nST\nSiST\ni Si\n,\n\ni\n\n.\nSi\nSi Si\nWe defer the details to [Tro15a]\nIn order to prove Theorem 4.22, we will use an AM-GM like inequality for matrices for which,\nunlike the one on Open Problem 0.2. in [Ban15d], an elementary proof is known.\nLemma 4.30 Given symmetric matrices H, W, Y ∈Rd×d and non-negative integers r, q satisfying\nq ≤2r,\nTr\n\nHW qHY 2r-q\n+ Tr\n\nHW 2r-qHY q\n≤Tr\n\nH2 W 2r + Y 2r\n,\nand summing over q gives\nX\n2r\nr\nTr HW qHY 2 -q\n2r +\nq=0\n\n≤\n\n+\n\nTr\n\nH2 W 2r\nY 2r\nWe refer to Fact 2.4 in [Tro15a] for an elementary proof but note that it is a matrix analogue to\nthe inequality,\nμθλ1-θ + μ1-θλθ ≤λ + θ\nfor μ, λ ≥0 and 0 ≤θ ≤1, which can be easily shown by adding two AM-GM inequalities\nμθλ1-θ ≤θμ + (1 -θ)λ and μ1-θλθ ≤(1 -θ)μ + θλ.\nProof. [of Theorem 4.22]\nLet X = Pn\nk=1 εkHk, then for any positive integer p,\nE∥X∥≤\nE∥X∥2p\n2p =\nE∥X2p∥\n2p ≤\nE Tr X2p 1\n2p ,\nwhere the first inequality follows from Jensen's inequality and the last from X2p ⪰0 and the obser-\nvation that the trace of a positive semidefinite matrix is at least its spectral norm. In the sequel, we\n\nupper bound E Tr X2p. We introduce X+i and X-i as X conditioned on εi being, respectively +1 or\n-1. More precisely\nX+i = Hi +\nX\nεjHj and X-i = -Hi +\nj=i\nX\nεjHj.\nj=\n\ni\nThen, we have\nE Tr X2p = E Tr\n\nXX2p-1\nn\n= E\nX\nTr εiHiX2p-1.\ni=1\nNote that E\nTr\n\nε H X2p-1\n= 1\nεi\ni\ni\n2 Tr\nh\nHi\n\nX2p-1\n+i\n-X2p-1\n-i\ni\n, this means that\nE Tr X2p =\nn\nX\ni=1\nE1\nh\np\nTr Hi\n\np\nX\n-1\n+i\n-\nX\ni\n-1\n-\ni\n,\nwhere the expectation can be taken over εj for j = i.\n2p\nNow we rewrite X+i\n-1 -\n2p\nX\n-1\n-i\nas a telescopic sum:\n2X\np\np\n-2\n+i\n--\n2p\nX\nX-i\n-\nq\n2p\nq\n=\nX+i (X+i\nq=0\n-X\n) X\n--\n-i\n-i\n.\nWhich gives\nn\n2p-2\nE Tr X2p =\nX\ni=1\nX\nE\nq=0\nq\nTr\nh\np\nHiX+i (X+i -\nq\nX\ni) X\ni\n--\n-\n-\ni\n.\nSince X+i -X-i = 2Hi we get\nX\nn\n2X\np-2\nE Tr X2p\nq\n2p\n=\nE Tr\nh\nq\nHiX+iHiX\n--\n-i\ni=1 q=0\ni\n.\n(44)\nWe now make use of Lemma 4.30 to get20 to get\nn\nE Tr X2p\nX 2p -\n≤\ni=1\nE Tr\nh\nH2\ni\n\nX2p-2\n+i\n+ X2p-2\n-i\ni\n.\n(45)\n20See Remark 4.32 regarding the suboptimality of this step.\n\nHence,\nX\nn 2p -1\ni=1\n2p-2\n2p-2\nX\nE Tr\nh\nn\nX+i\n+\nH2\n2p\n2p\n-i\ni\n\nX+i\n-+ X\ni\n-i\n=\n(2p\n1)\nH\n-\n-\nX\nE Tr\ni=1\n\ni\n\nn\n\n=\n(2p -1)\nX\nE Tr\ni=1\n\nH2\ni Eεi\n\nX2p-2\nn\n\n=\n(2p -1)\nX\nE Tr\ni=1\n\nH2\ni X2p-2\n=\n(2p -1)E Tr\n\" X\nn\nH2\ni\ni=1\n!\nX2p-2\n#\nSince X2p-2 ⪰0 we\nTr\n\"\nhave\nX\nn\nH2\ni\n!\nX2p-2\n#\n≤\nX\nn\n\n2p\n\nH2\n2p\ni\n\n-\ni=1\nTr X\n= σ Tr X\ni=1\n\n-,\n(46)\nwhich gives\nE Tr X2p\n\n≤σ2(2p -1)E Tr X2p-2.\n(47)\nApplying this inequality, recursively, we get\nE Tr X2p ≤[(2p -1)(2p -3) · · · (3)(1)] σ2pE Tr X0 = (2p -1)!!σ2pd\nHence,\nE∥X∥≤\nE Tr X2p\n2p ≤[(2p -1)!!]\n2p σd\n2p .\nTaking p = ⌈log d⌉and using the fact that (2p -1)!! ≤\n\n2p+1\np\n(see [Tro15a] for an elementary proof\ne\nconsisting essentially of taking logarithms and comparing the\n\nsum with an integral) we get\nE∥X∥≤\n2⌈log d⌉+ 1\ne\nσd\n2⌈log d⌉≤(2⌈log d⌉+ 1)\n2 σ.\nRemark 4.31 A similar argument can be used to prove Theorem 4.14 (the gaussian series case) based\non gaussian integration by parts, see Section 7.2. in [Tro15c].\nRemark 4.32 Note that, up until the step from (44) to (45) all steps are equalities suggesting that\nthis step may be the lossy step responsible by the suboptimal dimensional factor in several cases (al-\nthough (46) can also potentially be lossy, it is not uncommon that\nH2\ni is a multiple of the identity\nmatrix, which would render this step also an equality).\nIn fact, Joel Tropp [Tro15c] recently proved an improvement over\nP\nthe NCK inequality that, essen-\ntially, consists in replacing inequality (45) with a tighter argument. In a nutshell, the idea is that, if\nthe Hi's are non-commutative, most summands in (44) are actually expected to be smaller than the\nones corresponding to q = 0 and q = 2p -2, which are the ones that appear in (45).\n\n4.7\nOther Open Problems\n4.7.1\nOblivious Sparse Norm-Approximating Projections\nThere is an interesting random matrix problem related to Oblivious Sparse Norm-Approximating\nProjections [NN], a form of dimension reduction useful for fast linear algebra. In a nutshell, The\nidea is to try to find random matrices Π that achieve dimension reduction, meaning Π ∈Rm×n with\nm ≪n, and that preserve the norm of every point in a certain subspace [NN], moreover, for the\nsake of computational efficiency, these matrices should be sparse (to allow for faster matrix-vector\nmultiplication).\nIn some sense, this is a generalization of the ideas of the Johnson-Lindenstrauss\nLemma and Gordon's Escape through the Mesh Theorem that we will discuss next Section.\nOpen Problem 4.4 (OSNAP [NN]) Let s ≤d ≤m ≤n.\n1. Let Π ∈Rm×n be a random matrix with i.i.d. entries\nδriσri\nΠri =\n√\n,\ns\nwhere σri is a Rademacher random variable and\nδri =\n(\n√s\nwith probability\ns\nm\nwith probability\n1 -s\nm\nProve or disprove: there exist positive universal constants c1 and c2 such that\nFor any U ∈Rn×d for which UT U = Id×d\nProb\n(ΠU)T (ΠU) -I\n≥ε\n< δ,\nd+log( 1\nfor m\n\n≥c\n\nδ)\nε2\nand s ≥c2\nlog( d\nδ).\nε2\n2. Same setting as in (1) but conditioning on\nX\nm\nδri = s,\nfor all i,\nr=1\nmeaning that each column of Π has exactly s non-zero elements, rather than on average. The\nconjecture is then slightly different:\nProve or disprove: there exist positive universal constants c1 and c2 such that\nFor any U ∈Rn×d for which UT U = Id×d\nProb\n(ΠU)T (ΠU) -I\n≥ε\n\n< δ,\nfor m ≥\nd+log(\nc\n\nδ)\nε2\nand s ≥c2\nlog( d\nδ)\nε\n.\n\n3. The conjecture in (1) but for the specific choice of U:\nU =\n\nId×d\n.\n0(n-d)×d\n\nIn this case, the object in question is a sum of rank 1 independent matrices. More precisely,\nz1, . . . , zm ∈Rd (corresponding to the first d coordinates of each of the m rows of Π) are i.i.d.\nrandom vectors with i.i.d. entries\n(zk)j\n\n-\n=\n\n√s\nwith probability\ns\n2m\nwith probability\n1 -s\nm\n√s\nwith probability\ns\n2m\nNote that EzkzT\nk = 1 I\n. The conjecture is then that, there exists c1 and c2 positive universal\nm d×d\nconstants such that\n(X\nm\nProb\nT\nk\nk=1\n\nzkzT\nk -Ezkz\n\n)\n\n≥ε\n< δ,\nfor m ≥\nd+log( 1\nc1\nδ)\nε2\nand s ≥c2\nlog( d\nδ).\nε2\nI think this would is an interesting question even for fixed δ, for say δ = 0.1, or even simply\nunderstand the value of\nE\nX\nm\n.\nk=1\n\nzkzT\nk -EzkzT\nk\n\n4.7.2\nk-lifts of graphs\n\nGiven a graph G, on n nodes and with max-degree ∆, and an integer k ≥2 a random k lift G⊗k of G\nis a graph on kn nodes obtained by replacing each edge of G by a random k\nk bipartite matching.\nMore precisely, the adjacency matrix A⊗k\nk\n×\nof G⊗\nis a nk × nk matrix with k × k blocks given by\nA⊗k\nij = AijΠij,\nwhere Πij is uniformly randomly drawn from the set of permutations on k elements, and all the edges\nare independent, except for the fact that Πij = Πji. In other words,\nA⊗k =\nX\nAij\neieT\nj ⊗Πij + e\nT\njei\ni<j\n⊗ΠT\nij\n\n,\nwhere ⊗corresponds to the Kronecker product. Note that\nEA⊗k = A ⊗\n\nJ\nk\n\n,\nwhere J = 11T is the all-ones matrix.\n\nOpen Problem 4.5 (Random k-lifts of graphs) Give a tight upperbound to\nE\nA⊗k -EA⊗k .\nOliveira [Oli10] gives a bound that is essentially of the form\np\n∆log(nk), while the results in [ABG12]\nsuggest that one may expect more concentration for large k. It is worth noting that the case of k = 2\ncan essentially be reduced to a problem where the entries of the random matrix are independent and\nthe results in [BvH15] can be applied to, in some case, remove the logarithmic factor.\n4.8\nAnother open problem\nFeige [Fei05] posed the following remarkable conjecture (see also [Sam66, Sam69, Sam68])\nConjecture 4.33 Given n independent random variables X1, . . . , Xn s.t., for all i, Xi ≥0 and EXi =\n1 we have\nProb\nX\nn\nXi\ni=1\n≥n + 1\n!\n≤1 -e-1\nNote tP\nhat, if Xi are i.i.d. andXi = n + 1 with probability 1/(n + 1) and Xi = 0 otherwise, then\nn\nProb (\ni=1 Xi ≥n + 1) = 1 -\nn\nn\nn+1\n\n≈1 -e-1.\nOpen Problem 4.6 Prove or disprove Conjecture 4.33.21\n21We thank Francisco Unda and Philippe Rigollet for suggesting this problem.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 15-16: Johnson-Lindenstrauss Lemma and Gordon's Theorem",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/f9261308512f6b90e284599f94055bb4_MIT18_S096F15_Ses15_16.pdf",
      "content": "Johnson-Lindenstrauss Lemma and Gordons Theorem\n5.1\nThe Johnson-Lindenstrauss Lemma\nSuppose one has n points, X = {x1, . . . , xn}, in Rd (with d large). If d > n, since the points have\nto lie in a subspace of dimension n it is clear that one can consider the projection f : Rd →Rn of\nthe points to that subspace without distorting the geometry of X. In particular, for every xi and xj,\n∥f(xi) -f(xj)∥2 = ∥xi -x\nj∥, meaning that f is an isometry in X.\nSuppose now we allow a bit of distortion, and look for f : Rd →Rk that is an ε-isometry, meaning\nthat\n(1 -ε)∥xi -x\nj∥≤∥f(xi) -f(xj)∥≤(1 + ε)∥xi -xj∥.\n(48)\nCan we do better than k = n?\nIn 1984, Johnson and Lindenstrauss [JL84] showed a remarkable Lemma (below) that answers this\nquestion positively.\nTheorem 5.1 (Johnson-Lindenstrauss Lemma [JL84]) For any 0 < ε < 1 and for any integer\nn, let k be such that\nk ≥4\nlog n.\nε2/2 -ε3/3\nThen, for any set X of n points in Rd, there is a linear map f : Rd →Rk that is an ε-isometry for\nX (see (48)). This map can be found in randomized polynomial time.\nWe borrow, from [DG02], an elementary proof for the Theorem. We need a few concentration of\nmeasure bounds, we will omit the proof of those but they are available in [DG02] and are essentially\nthe same ideas as those used to show Hoeffding's inequality.\nLemma 5.2 (see [DG02]) Let y1, . . . , yd be i.i.d standard Gaussian random variables and Y\n=\n(y , . . . , y ).\nLet g : Rd\nk\nd\n→R\nbe the projection into the first k coordinates and Z = g\n\nY\n∥Y ∥\n\n=\n∥Y ∥(y1, . . . , yk) and L = ∥Z∥2. It is clear that EL = k. In fact, L is very concentrated around its\nd\nmean\n- If β < 1,\nPr\n\nk\nL ≤β\nk\nexp\nd\n\n≤\n\n2(1 -β + log β)\n\n.\n- If β > 1,\nPr\n\nL ≥β k\nd\n\n≤exp\nk(1 -β + log β)\n\n.\nProof. [ of Johnson-Lindenstrauss Lemma ]\nWe will start by showing that, given a pair xi, xj a projection onto a random subspace of dimension\nk will satisfy (after appropriate scaling) property (48) with high probability. WLOG, we can assume\nthat u = xi -xj has unit norm. Understanding what is the norm of the projection of u on a random\nsubspace of dimension k is the same as understanding the norm of the projection of a (uniformly)\n\nrandom point on Sd-1 the unit sphere in Rd on a specific k-dimensional subspace, let's say the one\ngenerated by the first k canonical basis vectors.\nThis means that we are interested in the distribution of the norm of the first k entries of a random\nvector drawn from the uniform distribution over Sd-1 - this distribution is the same as taking a\nstandard Gaussian vector in Rd and normalizing it to the unit sphere.\nLet g : Rd →Rk be the projection on a random k\nd\n-dimensional subspace and let f : Rd →Rk\ndefined as f = kg. Then (by the above discussion), given a pair of distinct xi and xj, ∥f(xi)-f(xj)∥2\nd\n∥xi-xj∥2\nhas the same distribution as kL, as defined in Lemma 5.2. Using Lemma 5.2, we have, given a pair\nxi, xj,\nPr\n∥f(xi) -f(xj)∥2\nk\n≤(1\n∥xi -xj∥2\n-ε)\n\n≤exp\n\n(1 -(1 -ε) + log(1 -ε))\n\n,\nsince, for ε ≥0, log(1 -ε) ≤-ε -ε2/2 we have\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≤(1 -ε)\n\n≤\nexp\n\n-kε2\n\n≤\nexp (-2 log n) = 1 .\nn2\nOn the other hand,\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≥(1 + ε)\n\n≤exp\nk(1 -(1 + ε) + log(1 + ε))\n\n.\nsince, for ε ≥0, log(1 + ε) ≤ε -ε2/2 + ε3/3 we have\nProb\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n≤(1 -ε)\n\n≤\nexp\n\n-k\nε2 -2ε3/3\n\n!\n≤\nexp (-2 log n) = 1 .\nn2\nBy union bound it follows that\nPr\n∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n/∈[1 -ε, 1 + ε]\n\n≤2 .\nn2\nSince there exist\nn\nsuch pairs, again, a simple union bound gives\nPr\n\n∃i,j : ∥f(xi) -f(xj)∥2\n∥xi -xj∥2\n/∈[1 -ε, 1 + ε]\n\n≤2\nn2\nn(n -1)\n= 1 -1 .\nn\nTherefore, choosing f as a properly scaled projection onto a random k\n-dimensional subspace is an\nε-isometry on X (see (48)) with probability at least\n.\nWe can achieve any desirable constant\nn\nprobability of success by trying O(n) such random projections, meaning we can find an ε-isometry\nin randomized polynomial time.\nNote that by considering k slightly larger one can get a good projection on the first random attempt\nwith very good confidence. In fact, it's trivial to adapt the proof above to obtain the following Lemma:\n\nLemma 5.3 For any 0 < ε < 1, τ > 0, and for any integer n, let k be such that\nk ≥(2 + τ)\nlog n.\nε2/2 -ε3/3\nThen, for any set X of n points in Rd, take f : Rd →Rk to be a suitably scaled projection on a random\nsubspace of dimension k, then f is an ε-isometry for X (see (48)) with probability at least 1 -\n1 .\nnτ\nLemma 5.3 is quite remarkable. Think about the situation where we are given a high-dimensional\ndata set in a streaming fashion - meaning that we get each data point at a time, consecutively. To run\na dimension-reduction technique like PCA or Diffusion maps we would need to wait until we received\nthe last data point and then compute the dimension reduction map (both PCA and Diffusion Maps\nare, in some sense, data adaptive). Using Lemma 5.3 you can just choose a projection at random in\nthe beginning of the process (all ones needs to know is an estimate of the log of the size of the data\nset) and just map each point using this projection matrix which can be done online - we don't need\nto see the next point to compute the projection of the current data point. Lemma 5.3 ensures that\nthis (seemingly na ıve) procedure will, with high probably, not distort the data by more than ε.\n5.1.1\nOptimality of the Johnson-Lindenstrauss Lemma\nIt is natural to ask whether the dependency on ε and n in Lemma 5.3 can be improved.\nNoga\nAlon [Alo03] showed that there are n points for which the smallest dimension k on which they can\nbe embedded with a distortion as in Lemma 5.3, satisfies k = Ω\n\nε\nlog\n(1/ε)\n-2\nn , this was recently\nlog\nimproved by Larsen and Nelson [?], for linear maps, to Ω\n\nε-2 log n\n\n, closing the gap.22\n5.1.2\nFast Johnson-Lindenstrauss\n(Disclaimer: the purpose of this section is just to provide a bit of intuition, there is a lot of hand-\nwaving!!)\nLet's continue thinking about the high-dimensional streaming data. After we draw the random\nprojection matrix, say M, for each data point x, we still have to compute Mx which, since M has\nO(ε-2 log(n)d) entries, has a computational cost of O(ε-2 log(n)d). In some applications this might\nbe too expensive, can one do better? There is no hope of (significantly) reducing the number of rows\n(Recall Open Problem ?? and the lower bound by Alon [Alo03]). The only hope is to speed up the\nmatrix-vector multiplication. If we were able to construct a sparse matrix M then we would definitely\nspeed up the computation of Mx but sparse matrices tend to distort sparse vectors, and the data\nset may contain. Another option would be to exploit the Fast Fourier Transform and compute the\nFourier Transform of x (which takes O(d log d) time) and then multiply the Fourier Transform of x by\na sparse matrix. However, this again may not work because x might have a sparse Fourier Transform.\nThe solution comes from leveraging an uncertainty principle -- it is impossible for both x and the FT\nof x to be sparse simultaneously. The idea is that if, before one takes the Fourier Transform of x, one\nflips (randomly) the signs of x, then the probably of obtaining a sparse vector is very small so a sparse\nmatrix can be used for projection. In a nutshell the algorithm has M be a matrix of the form PHD,\n22An earlier version of these notes marked closing the gap as an open problem, this has been corrected.\n\nwhere D is a diagonal matrix that flips the signs of the vector randomly, H is a Fourier Transform\n(or Hadamard transform) and P a sparse matrix. This method was proposed and analysed in [AC09]\nand, roughly speaking, achieves a complexity of O(d log d), instead of the classical O(ε-2 log(n)d).\nThere is a very interesting line of work proposing fast Johnson Lindenstrauss projections based on\nsparse matrices. In fact, this is, in some sense, the motivation for Open Problem 4.4. in [Ban15d].\nWe recommend these notes Jelani Nelson's notes for more on the topic [Nel].\n5.2\nGordon's Theorem\nIn the last section we showed that, in order to approximately preserve the distances (up to 1 ± ε)\nbetween n points it suffices to randomly project them to Θ\nε-2 log n dimensions. The key argument\nwas that a random projection approximately preserves the norm of every point in a set S, in this case\nthe set of differences between pairs of n points. What we showed is\n\nthat, in order to approximately\npreserve the norm of every point in S it is enough to project to Θ\nε-2 log |S|\ndimensions.\nThe\nquestion this section is meant to answer is: can this improved if S has a special structure? Given a\nset S, what is the measure of complexity of S that explains how many dimensions\n\none needs to take\non the projection to still approximately preserve the norms of points in S. Was we will see below, this\nwill be captured, via Gordon's Theorem, by the so called Gaussian Width of S.\nDefinition 5.4 (Gaussian Width) Given a closed set S ⊂Rd, its gaussian width ω(S) is define\nas:\nω(S) = E max gT\nd x\nx∈S\nwhere gd\n(0, Id\nd).\n\n,\n∼N\n×\nSimilarly to what we did in the proof of Theorem 5.1 we will restrict our attention to sets S of\nunit norm vectors, meaning that S ⊂Sd-1.\nAlso, we will focus our attention not in random projections but in the similar model of random\nlinear maps G : Rd →Rk that are given by matrices with i.i.d. gaussian entries. For this reason the\nfollowing Proposition will be useful:\nProposition 5.5 Let gk ∼N (0, Ik×k), and define\nak := E∥gk∥.\nThen\nq\nk\nk+1\n√\nk ≤ak ≤\n√\nk.\nWe are now ready to present Gordon's Theorem.\nTheorem 5.6 (Gordon's Theorem [Gor88]) Let G ∈Rk×d a random matrix with independent\nN(0, 1) entries and S ⊂Sd-1 be a closed subset of the unit sphere in d dimensions. Then\nE max\nx∈S\nak\nGx\n≤1 + ω(S)\nak\n,\n\nand\nE min\nx∈S\nak\nGx\n≥1 -ω(S),\nak\nwhere ak = E∥gk∥and ω(S) is the gaussian width of S. Recall that\nq\nk\nk+1\n√\nk ≤ak ≤\n√\nk.\nBefore proving Gordon's Theorem we'll note some of it's direct implications. It suggest that\n1 G\nak\npreserves the norm of the points in S up to 1 ± ω(S), indeed we can make this precise with Gaussian\nak\nConcentration.\nNote that the function F(G) = maxx∈S\n1 Gx\nak\nis 1-Lipschitz. Indeed\nmax ∥G1x1∥-max ∥G2x2∥\n\n≤\nmax |∥G1x∥-∥G2x\nmax\nx1∈S\nx2∈S\nx∈S\n∥| ≤\nx∈S ∥(G1 -G2) x∥\n=\n∥G1 -G2∥≤∥G1 -G2∥F .\nSimilarly, one can show that F(G) = minx∈S\n1 Gx\nis 1-Lipschitz. Thus, one can use Gaussian\nak\nConcentration to get:\n\nProb\n\nt2\nmax ∥Gx∥≥ak + ω(S) + t\nx S\n\nexp\n∈\n≤\n\n-2\n\n,\n(49)\nand\nProb\n\nmin\nx∈S ∥Gx∥≤ak -ω(S) -t\n\n≤exp\n\n-t2\n.\n\n(50)\nThis gives us the following Theorem.\nTheorem 5.7 Let G ∈Rk×d a random matrix with independent N(0, 1) entries and S ⊂Sd-1 be\na closed subset of the unit sphere in d dimensions. Then, for ε >\nr\nω(S)2\na2\nk , with probability ≥1 -\n2 exp\n\n-k\n\nε -ω(S)\nak\n\n:\n(1 -ε)∥x∥≤\nak\nGx\n≤(1 + ε)∥x∥,\nfor all x ∈S.\nRecall that k -\nk\na\nk+1 ≤\nk ≤k.\nω(S)+t\nProof.\nThis is readily obtained by taking ε =\n, using (49), (50), and recalling that a2\nak\nk ≤k.\nRemark 5.8 Note that a simple use of a union bound23 shows that ω(S) ≲\np\n2 log |S|, which means\nthat taking k to be of the order of log |S| suffices to ensure that\nak G to have the Johnson Lindenstrauss\nproperty. This observation shows that Theorem 5.7 essentially directly implies Theorem 5.1 (although\nnot exacly, since\n1 G is not a projection).\nak\n23This follows from the fact that the maximum of n standard gaussian random variables is ≲\np\n2 log |S|.\n\n5.2.1\nGordon's Escape Through a Mesh Theorem\nTheorem 5.7 suggests that, if ω(S) ≤ak, a uniformly chosen random subspace of Rn of dimension\n(n -k) (which can be seen as the nullspace of G) avoids a set S with high probability. This is indeed\nthe case and is known as Gordon's Escape Through a Mesh Theorem, it's Corollary 3.4. in Gordon's\noriginal paper [Gor88]. See also [Mix14b] for a description of the proof. We include the Theorem\nbelow for the sake of completeness.\nTheorem 5.9 (Corollary 3.4. in [Gor88]) Let S ⊂Sd-1 be a closed subset of the unit sphere in\nd dimensions. If ω(S) < ak, then for a (n -k)-dimensional subspace Λ drawn uniformly from the\nGrassmanian manifold we have\nProb {Λ ∩S = ∅} ≤2 exp\n\n-1\n(ak -ω(S))\n\n,\nwhere ω(S) is the gaussian width of S and ak = E∥gk∥where gk ∼N(0, Ik×k).\n5.2.2\nProof of Gordon's Theorem\nIn order to prove this Theorem we will use extensions of the Slepian's Comparison Lemma.\nSlepian's Comparison Lemma, and the closely related Sudakov-Fernique inequality, are crucial\ntools to compare Gaussian Processes. A Gaussian process is a family of gaussian random variables\nindexed by some set T, {Xt}t T (if T is finite this is simply a gaussian vector). Given a gaussian\n∈\nprocess Xt, a particular quantity of interest is E [maxt T Xt]. Intuitively, if we have two Gaussian\n∈\nprocesses Xt and Yt with mean zero E [Xt] = E [Yt] = 0, for all t ∈T, and the same variance, then the\nprocess that has the \"least correlations\" should have a larger maximum (think the maximum entry\nof vector with i.i.d. gaussian entries versus one always with the same gaussian entry). The following\ninequality makes this intuition precise and extends it to processes with different variances. 24\nTheorem 5.10 (Slepian/Sudakov-Fernique inequality) Let {Xu}u∈U and {Yu}u∈U be two (al-\nmost surely bounded) centered Gaussian processes indexed by the same (compact) set U. If, for every\nu1, u2 ∈U:\nE [Xu1 -\nXu2] ≤E [Yu1 -Yu2] ,\n(51)\nthen\nE\n\nmax Xu\n\n≤E\n\nmax Yu .\nu∈U\nu∈U\n\nThe following extension is due to Gordon [Gor85, Gor88].\nTheorem 5.11 [Theorem A in [Gor88]] Let {Xt,u}\nand Yt,u\nbe two (almost surely\n(t,u)∈T×U\n{\n}(t,u)∈T×U\nbounded) centered Gaussian processes indexed by the same (compact) sets T and U. If, for every\nt1, t2 ∈T and u1, u2 ∈U:\nE [Xt1,u1 -\nXt1,u2] ≤E\n[Yt1,u1 -Yt1,u2] ,\n(52)\n24Although intuitive in some sense, this turns out to be a delicate statement about Gaussian random variables, as it\ndoes not hold in general for other distributions.\n\nand, for t1 = t2,\nE [Xt1,u1 -\nXt2,u2] ≥E [Yt1,u1 -Yt2,u2] ,\n(53)\nthen\nE\n\nmin max Xt,u\nt∈T u∈U\n\n≤E\n\nmin max Yt,u .\nt∈T u∈U\n\nNote that Theorem 5.10 easily follows by setting |T| = 1.\nWe are now ready to prove Gordon's theorem.\nProof. [of Theorem 5.6]\nLet G ∈Rk×d with i.i.d. N(0, 1) entries. We define two gaussian processes: For v ∈S ⊂Sd-1 and\nu ∈Sk-1 let g ∼N (0, Ik\nk) and h ∼N (0, Id\nd) and define the following processes:\n×\n×\nAu,v = gT u + hT v,\nand\nBu,v = uT Gv.\nFor all v, v′ ∈S ⊂Sd-1 and u, u′ ∈Sk-1,\nE\n\n-\n2 -E\n\nT\nT\nAv,u\nAv′,u′\nBv,u -Bv′,u′\n\n=\n4 -2 u u′ + v v′ -\nX\nviuj\nj\nij\n-vi\n′u′\n=\n-2\n\nuT u′ + vT v′ -2 -2 vT\nv′\nuT\n\nu′\n=\n2 -2 uT u′\n\nvT\n+\nv′\n\n-u\n\nT u′vT v\n′\n\n=\n2 1 -uT u′\n1 -vT v′ .\n\nThis means that E Av,u -\nAv′,u′\n-E Bv,u -\nBv′,u′\n\n≥\nand E Av,u -Av′,u′\n-E Bv,u -Bv′,u′\n=\n0 if v = v′.\nThis means that w\n\ne can use Theorem\n\n5.11 with X\n\n= A and Y\n\n= B, to get\n\nE min max Av,u\nv∈S u∈Sk-1\n≤E min max Bv,u.\nv∈S u∈Sk-1\nNoting that\nE min max B\nE min max uT\nv,u =\nGv = E min ∥Gv\nv∈S u Sk-1\nv∈S u Sk-1\nv∈S\n∥,\n∈\n∈\nand\nE\n\nmin max A\nE max g u + E min hT\nv,u\n\n=\nT\nv = E max gT u\nE max( hT v) = ak\nω(S),\nv∈S u∈Sk-1\nu∈Sk-1\nv∈S\nu\n-\n∈Sk-1\n-\nv∈S\n-\ngives the second part of the Theorem.\nOn the other hand, since E\nAv,u -Av′,u′\n2 -E\nBv,u -Bv′,u′\n2 ≥0 then we can similarly use\nTheorem 5.10 with X = B and Y = A, to get\nE max max Av,u\nmax max Bv,u.\nv∈S u∈Sk\nE\n-\n≥\nv∈S u∈Sk-1\n\nNoting that\nE max max Bv,u =\nv∈S u∈Sk-1\nE max max uT Gv = E max Gv ,\nv∈S u∈Sk-1\nv∈S ∥\n∥\nand\nE\n\nmax max Av,u\n\n= E max gT u + E max hT v = ak + ω(S),\nv∈S u∈Sk-1\nu∈Sk-1\nv∈S\nconcludes the proof of the Theorem.\n5.3\nSparse vectors and Low-rank matrices\nIn this Section we illustrate the utility of Gordon's theorem by undertanding which projections are\nexpected to keep the norm of sparse vectors and low-rank matrices.\n5.3.1\nGaussian width of k-sparse vectors\nSay we have a signal (or image) x ∈RN that we are interested in measuring with linear measurements\ny\nT\nN\ni = ai x, for ai ∈R . In general, it is clear that we would need N measurements to find x. The\nidea behind Compressed Sensing [CRT06a, Don06] is that one may be able to significantly decrease\nthe number of measurements needed if we know more about the structure of x, a prime example is\nwhen x is known to have few non-zero entries (being sparse). Sparse signals do arise in countless\napplications (for example, images are known to be sparse in the Wavelet basis; in fact this is the basis\nof the JPEG2000 compression method).\nWe'll revisit sparse recovery and Compressed Sensing next lecture but for now we'll see how\nGordon's Theorem can suggest us how many linear measurements are needed in order to reconstruct\na sparse vector. An efficient way of representing the measurements is to use a matrix\nA =\n--\naT\n--\n\n--\naT\n--\n...\n--\naT\n--\n\nM\n,\nand represent the linear measurements as\ny = Ax.\nIn order to hope to be able to reconstruct x from y we need that A is injective on sparse vectors.\nLet us assume that x is s-sparse, meaning that x has at most s non-zero entries (often written as\n∥x∥0 ≤s, where ∥· ∥0 is called the 0-norm and counts the number of non-zero entries in a vector25).\nIt is also intuitive that, in order for reconstruction to be stable, one would like that not only A is\ninjective in s-sparse vectors but actually almost an isometry, meaning that the l2 distance between\nAx1 and Ax2 should be comparable to the distances between x1 and x2 if they are s-sparse. Since the\ndifference between two s-sparse vectors is a 2s-sparse vector, we can alternatively ask for A to keep\nthe norm of 2s sparse vectors. Gordon's Theorem above suggests that we can take A ∈RM×N to have\n25It is important to note that ∥· ∥0 is not actually a norm\n\ni.i.d. gaussian entries and to take M ≈ω (S2s), where Sk =\nx : x ∈SN-1, ∥x∥0 ≤k\nis the set of 2s\nsparse vectors, and ω (S2s) the gaussian width of S2s.\n\nProposition 5.12 If s ≤N, the Gaussian Width ω (Ss) of Ss, the set of unit-norm vectors that are\nat most s sparse, satisfies\nN\nω (Ss) ≲s log\n\ns\n\n.\nProof.\nω (Ss) =\nmax\nv∈SSN-1, ∥v∥0≤s gT v, log\nN\n,\ns\n\nwhere g ∼N(0, IN\nN). We have\n×\nω (Ss) =\nmax\ngΓ ,\nΓ⊂[N], |Γ|=s ∥\n∥\nwhere gΓ is the restriction of g to the set of indices Γ.\nGiven a set Γ, Theorem 4.12 gives\nProb\nn\ngΓ\n√\n∥\n∥2 ≥s + 2\ns\n√\nt + t\no\n≤exp(-t).\nUnion bounding over all Γ ⊂[N], |Γ| = s gives\nProb\n\nmax\nΓ⊂[N], |Γ|=s ∥gΓ∥2 ≥s + 2√s\n√\nt\n\nN\n+ t\n≤\n\ns\n\nexp(-t)\nTaking u such that t = su, gives\nProb\n\nmax\n√\nΓ⊂[N], |Γ|= ∥gΓ∥2\ns\ns\n≥\n1 + 2\nu + u\n\n≤exp\n\n-su + s log\nN\ns\n\n.\n(54)\nTaking u > log\nN\ns\n\nit can be readily seen that the typical size of maxΓ⊂[N], |Γ|=s ∥gΓ∥2 is ≲\ns log\nN\nq\n.\ns\n\nThe proof can be finished by integrating (54) in order to get a bound of the expectation of\nmaxΓ⊂[N], |Γ|=s ∥gΓ∥2.\nThis suggests that ≈2s log\nN\nmeasurements suffice to identify a 2s-sparse vector. As we'll see,\n2s\nnot only such a number of measuremen\n\nts suffices to identify a sparse vector but also for certain efficient\nalgorithms to do so.\n5.3.2\nThe Restricted Isometry Property and a couple of open problems\nMatrices perserving the norm of sparse vectors do play a central role in sparse recovery, they are said\nto satisfy the Restricted Isometry Property. More precisely:\nDefinition 5.13 (The Restricted Isometry Property) An M × N matrix A (with either real or\ncomplex valued entries) is said to satisfy the (s, δ)-Restricted Isometry Property (RIP),\n(1 -δ)∥x∥2 ≤∥\nAx∥≤(1 + δ)∥x∥2,\nfor all s-sparse x.\n\nUsing Proposition 5.12 and Theorem 5.7 one can readily show that matrices with Gaussian entries\nsatisfy the restricted isometry property with M ≈s log\nN .\ns\nTheorem 5.14 Let A be an M\n\n× N matrix with i.i.d.\nstandard gaussian entries, there exists a\nconstant C such that, if\nM ≥Cs log\nN\ns\n\n,\nthen\n1 A satisfies the\naM\ns, 1\n\n-RIP, with high probability.\nTheorem 5.14 suggests that RIP matrices are abundant for s ≈\nM\nlog(N), however it appears to be\nvery difficult to deterministically construct matrices that are RIP for s ≫\n√\nM, known as the square\nbottleneck [Tao07, BFMW13, BFMM14, BMM14, B+11, Mix14a]. The only known unconditional\nconstruction that is able to break this bottleneck is due to Bourgain et al. [B+11] that achieves\ns ≈\nM\n+ε\nfor a small, but positive, ε.\nThere is a conditional construction, based on the Paley\nEquiangular Tight Frame, that will be briefly described in the next Lecture [BFMW13, BMM14].\nOpen Problem 5.1 Construct deterministic matrices A ∈CM×N (or A ∈CM×N) satisfying (s, 1\n3)-\nRIP for s ≳\nM0.6\n.\npolylog(N\nOpen Problem 5.2 Theorem 5.14 guarantees that if we take A to have i.i.d. Gaussian entries then\nit should be RIP for s ≈\nM\n. If we were able to, given A, certify that it indeed is RIP for some s\nlog(N)\nthen one could have a randomized algorithm to build RIP matrices (but that is guaranteed to eventually\nfind one). This motives the following question\n1. Let N = 2M, for which s is there a polynomial time algorithm that is guaranteed to, with high\nprobability, certify that a gaussian matrix A is\ns, 1\n\n-RIP?\n2. In particular, a\ns, 1 -RIP matrix has to not have s sparse vectors in its nullspace. This mo-\ntivates a second question: Let N = 2M, for which s is there a polynomial time algorithm that\nis guaranteed to, with\n\nhigh probability, certify that a gaussian matrix A does not have s-sparse\nvectors in its nullspace?\nThe second\n√\nquestion is tightly connected to the question of sparsest vector on a subspace (for\nwhich s ≈\nM is the best known answer), we refer the reader to [SWW12, QSW14, BKS13b] for\nmore on this problem and recent advances. Note that checking whether a matrix has RIP or not is,\nin general, NP-hard [BDMS13, TP13].\n5.3.3\nGaussian width of rank-r matrices\nAnother structured set of interest is the set of low rank matrices.\nLow-rank matrices appear in\ncountless applications, a prime example being the Netflix Prize. In that particular example the matrix\nin question is a matrix indexed by users of the Netflix service and movies. Given a user and a movie,\nthe corresponding entry of the matrix should correspond to the score that user would attribute to that\nmovie. This matrix is believed to be low-rank. The goal is then to estimate the score for user and\n\nmovie pairs that have not been rated yet from the ones that have, by exploiting the low-rank matrix\nstructure. This is known as low-rank matrix completion [CT10, CR09, Rec11].\nIn this short section, we will not address the problem of matrix completion but rather make a\ncomment about the problem of low-rank matrix sensing, where instead of observing some of the entries\nof the matrix X ∈Rn1×n2 one has access to linear measuremetns of it, of the form yi = Tr(AT\ni X).\nIn order to understand the number of measurements needed for the measurement procedure to\nbe a nearly isometry for rank r matrices, we can estimate the Gaussian Width of the set of matrices\nX ∈∈Rn1×n2 whose rank is smaller or equal to 2r (and use Gordon's Theorem).\nProposition 5.15\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n≲\np\nr(d1 + d2).\nProof.\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n= E\nmax\nTr(GX).\nX∥F\nrank(\n∥\n=1\nX)≤r\nLet X = UΣV T be the SVD decomposition of X, then\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n= E\nmax\nTr(Σ V T GU ).\nUT U=V T V =Ir×r\nΣ∈Rr×r diagonal ∥Σ∥F =1\n\nThis implies that\nω\n\nX : X ∈Rn1×n2, rank(X) ≤r\n\n≤(Tr Σ) (E∥G∥) ≲√r (√n1 + √n1) ,\nwhere the last inequality follows from bounds on the largest eigenvalue of a Wishart matrix, such as\nthe ones used on Lecture 1.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 17: Local Convergence of Graphs and Enumeration of Spanning Trees",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/55ff9f23be313f3beefe692dda95aff9_MIT18_S096F15_Ses17.pdf",
      "content": "LOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING\nTREES\nMUSTAZEE RAHMAN\n1. Introduction\nA spanning tree in a connected graph G is a subgraph that contains every vertex of G and is\nitself a tree. Clearly, if G is a tree then it has only one spanning tree. Every connected graph\ncontains at least one spanning tree: iteratively remove an edge from any cycle that is present until\nthe graph contains no cycles. Counting spanning trees is a very natural problem. Following Lyons\n[5] we will see how the theory of graph limits does this in an asymptotic sense. There are many\nother interesting questions that involve understanding spanning trees in large graphs, for example,\nwhat is a 'random spanning tree' of Zd? We will not discuss these questions in this note, however,\nthe interested reader should see chapters 4, 10 and 11 of Lyons and Peres [7].\nLet us begin with some motivating examples. Let Pn denote the path on n vertices. Each Pn\nnaturally embeds into the bi-infinite path whose vertices are the set of integers Z with edges between\nconsecutive integers. By an abuse of notation we denote the bi-infiite path as Z. It is intuitive to\nsay that Pn converges to Z as these paths can be embedded into Z in a nested manner such that\nthey exhaust Z. Clearly, both Pn and Z contain only one spanning tree.\nFigure 1. Extending a spanning tree in Z[-1, 1]2 to a spanning tree in Z[-2, 2]2.\nBlack edges form a spanning tree in Z[-1, 1]2. Red vertices form the cluster of\nchosen vertices on each side and isolated blue vertices are not chosen.\nCorner\nvertices are matched arbitrarily to one of their neighbours.\nThe previous example was too simple. Let us move to the infinite planar grid Z2 where things\nare more interesting. Let Z[-n, n]2 denote the square grid graph on [-n, n]2, that is, the subgraph\n\nMUSTAZEE RAHMAN\nspanned by [-n, n]2 in Z2. There are exponentially many spanning trees in Z[-n, n]2 in terms of\nits size. Indeed, let us see that any spanning tree in Z[-n + 1, n -1]2 can be extended to at least\n28n different spanning trees in Z[-n, n]2. Consider the boundary of Z[-n, n]2 which has 8n vertices\nof the form (±n, y) or (x, ±n). There are four corner vertices (±n, ±n) and vertices on the four\nsides (±n, y) or (x, ±n) where |x|, |y| < n. Consider any subset of vertices S on the right hand side\n{(n, y) : |y| < n}, say. The edges along this side partition S into clusters of paths; two vertices are\nin the same cluster if they lie on a common path (see the red vertices in Figure 1). Pick exactly\none vertex from each cluster, say the median vertex. Connect each such vertex, say (n, y), to the\nvertex (n -1, y) via the edge (n, y) ↔(n\n-1, y), which is the unique edge connecting (n, y) to\nZ[-n + 1, n -1] . If a vertex (n, y′) on the right hand side is not in S then connect it directly to\nZ[-n + 1, n -1]2 via the edge (n, y′) ↔(n -1, y′) (see blue vertices in Figure 1). Do this for each of\nthe four sides and also connect each of the four corner vertices to any one of its two neighbours. In\nthis manner we may extend any spanning tree T in Z[-n+1, n-1]2 to (22n-1)4 ·24 = 28n spanning\ntrees in Z[-n, n]2.\nLet sptr(Z[-n, n]2) denote the number of spanning trees in Z[-n, n]2. The argument above shows\nthat sptr(Z[-n, n]2) ≥28nsptr(Z[-n + 1, n -1]2), from which it follows that sptr(Z[ n, n]2)\n24n(n+1). As |Z[-n, n]2| = (2n + 1)2 we deduce that log sptr(Z[-n, n]2\n-\n≥\n)/|Z[-n, n] | ≥log 2(1 +\nO(n-2)). It turns out that there is a limiting value of log sptr(Z[-n, n]2)/|Z[-n, n]2| as n →inf,\nwhich is called the tree entropy of Z2. We will see that the limiting value depends on Z2, which in\nan intuitively sense is the limit of the grids Z[-n, n]2. We will in fact calculate the tree entropy.\nThe tree entropy of a sequence of bounded degree connected graphs {Gn} is the limiting value of\nlog sptr(Gn)/|Gn| provided it exists. It measures the exponential rate of growth of the number of\nspanning trees in Gn. We will see that that tree entropy exists whenever the graphs Gn converge to\na limit graph in a precise local sense. In particular, this will allow us to calculate the tree entropy\nof the d-dimensional grids Z[-n, n]d and of random d-regular graphs.\n2. Local weak convergence of graphs\nWe only consider connected labelled graphs with a countable number of vertices and of bounded\ndegree. A rooted graph (G, x) is a graph with a distinguished vertex x called the root. Two rooted\ngraphs (G, x) and (H, y) are isomorphic if there is a graph isomorphism φ : G →H such that\nφ(x) = y. In this case we write (G, x) ∼= (H, y). We consider isomorphism classes of rooted graphs,\nalthough we will usually just refer to the graphs instead of their isomorphism class. Given any graph\nG we denote Nr(G, x) as the r-neighbourhood of x in G rooted at x. The distance between two\n(isomorphism classes of) rooted graphs (G, x) and (H, y) is 1/(1+R) where R = min{r : Nr(G, x) ∼=\nNr(H, y)}.\nLet G denote the set of isomorphism classes of connected rooted graphs such that all degrees are\nbounded by ∆. For concreteness we may assume that all these graphs have a common vertex set,\nnamely, {1, 2, 3, . . .}. Then G∆is a metric space with the aforementioned distance function. By\na diagonalization argument it is easy to see that G is a compact metric space. Let F denote the\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nBorel σ-algebra of G under this metric; it is generated by sets of the from A(H, y, r) = {(G, x) ∈G :\nNr(G, x) ∼= Nr(H, y)}. A random rooted graph (G, *) is a probability space (G, F, μ); we think of\n(G, *) as a G-valued random variable such that P\n\n(G, *) ∈A\n\n= μ(A) for every A ∈F.\nLet us see some examples. Suppose G is a finite connected graph of maximum degree ∆. If *G is a\nuniform random vertex of G then (G, *G) is a random rooted graph. We have P\n\n(G, *G) = (H, y)\n=\n(1/|G|) × |{x ∈V (G) : (G, x) ∼= (H, y)}|. If G is a vertex transitive graph, for example Zd, then\n\nfor any vertex *∈G we have a random rooted graph (G, *) which is simply the delta measure\nsupported on the isomorphism class of (G, *). The isomorphism class of G consists of G rooted at\ndifferent vertices. It is conventional in this case to simply think of (G, *) as the fixed graph G. So,\nfor example, Zd is a 'random' rooted graph with root at the origin.\nLet Gn be a sequence of finite connected graphs of maximum degree at most ∆. Let *n denote a\nuniform random vertex of Gn. We say Gn converges in the local weak limit if the law of the random\nrooted graphs (Gn, *n) converge in distribution to the law of a random rooted graph (G, *) ∈G. For\nthose unfamiliar with the notion of converge of probability measures here is an alternative definition.\nFor every r > 0 and any finite connected\n∼\nthat |x ∈V (Gn) : Nr(G, x) ∼= (H, y)|\nand compactness of G it can be shown\n\nrooted graph (H, y) with that Nr(H, y) = (H, y) we require\n/|Gn| converges as n →inf. Using tools from measure theory\nthat there is a random rooted graph (\nif the ratios in the previous sentence converge then P Nr(Gn, *n) =∼Nr(G, *)\nevery r. This is what it means for (Gn, *n) to converge\n\nin distribution to (G, *\nG, *) ∈G such that\n→1 as n →inffor\n).\nThis notion of local weak convergence was introduced by Benjamini and Schramm [3] in order\nto study random planar graphs. Readers interested in a detailed study of local weak convergence\nshould see Aldous and Lyons [1] and the references therein.\nExercise 2.1. Show that the d-dimensional grid graphs Z[-n, n]d converge to Zd in the local weak\nlimit. Show that the same convergence holds for the d-dimensional discrete tori (Z/nZ)d, where two\nvertices x = (x1, . . . , xd) and y = (y1, . . . , yd) are connected if xi = yi ± 1 (mod n) for exactly one i\nand xi = yi for all other i.\nExercise 2.2. Suppose the graphs Gn have maximum degree at most ∆and converge in the local\nweak limit to (G, *). Show that\n\ndeg(*n) conver\nges in distribution (as integer valued random variables)\nto deg(*). Conclude that E deg(*n)\n→E deg(*)\n\n.\n2.1. Local weak limit and simple random walk. Let (G, x) be a rooted graph. The simple\nrandom walk (SRW) on (G, x) (started as x) is a V (G)-valued stochastic process X0 = x, X1, X2 . . .\nsuch that Xk is a uniform random neighbour of Xk\n1 picked independently of X0, . . . , X\n-\nk-1. The\nSRW is a Markov process given by the transition matrix P(u, v) =\n1u∼v where u\ndeg(u)\n∼v means that\n{u, v} is an edge of G. If G has bounded degree then if is easily verified that P\n\nXk = y | X0 = x\n=\nP k(x, y). The k-step return probability to x is pk\nG(x) = P k(x, x) for k ≥0.\n\nSuppose that Gn is a sequence of bounded degree graphs that converge to (G, *) in the local weak\nlimit. We show that the expected k-step return probability of the SRW on (Gn, *n) converges to the\nexpected k-step return probability of the SRW on (G, *). Note that if Nr(G, x) ∼= Nr(H, y) then\n\nMUSTAZEE RAHMAN\npk (x) = pk\nG\nH(y) for all 0 ≤k ≤2r since in order for the SRW to return in k steps it must remain in\nthe (k/2)-neighbourhood on the starting point.\nIf Gn converges to (G, *) then there is a probability space (Ω, Σ, μ) and G-valued random variables\n(G′\nn, *′\nn), (G′, *′) on (Ω, Σ, μ) such that (Gn, *n) has the law of (G′\nn, *′\nn), (G, *) has the law of (G, *),\nand for every r ≥0 the probability μ(Nr(G′\nn, *′\nn) ∼= Nr(G′, *′)) →1 as n →inf. This common\nprobability space where all the graphs can be jointly defined and satisfy the stated claim follows\nfrom Shorokhod's representation theorem. On the event {Nk/2(G′\nn, *′\nn) ∼= Nk/2(G′, *′)} we have\npk\nG (\n′\nn *n) = pk\nG (\n′ *). Therefore,\nE\n\npk (*)\n\n-E\n\npk (*)\n=\nE\n\npk (\nk\nGn\nn\nG\n\nG\n′ )\np\n( ′)\n′\nn *n -\nG′ *\n=\n(\n\nE\n\npk\nG\n*′\nn) -pk\nG (*′); N\n\nk/2(G\n′\nn\n′\n′\nn, *′\nn) ≅Nk/2(G′, *′)\n≤2P\n\nNk/2(G′\nn, *′\nn) ≅Nk/2(G′, *′)\n\n-→0\nas n\n\n→inf\n\n.\n2.2. Local weak limit of random regular graphs. In this section we will show a classical result\nthat random d-regular graphs converge to the d-regular tree Td in the local weak sense (see Bollob as\n[4]). There are a finite number of d-regular graphs on n vertices so we can certainly consider a\nuniform random d-regular graph on n vertices whenever nd is even. However, how do we calculate\nprobabilities and expectations involving a uniform random d-regular graph on n vertices?\nFirst, we would have to calculate the number of d-regular graphs on n vertices. This is no easy\ntask. To get around this issue we will consider a method for sampling (or generating) a random\nd-regular multigraph (that is, graphs with self loops and multiple edges between vertices), This\nsampling procedure is simple enough that we can calculate the expectations and probabilities that\nare of interest to us. We will then relate this model of random d-regular multigraphs to uniform\nrandom d-regular graphs.\nThe configuration model starts with n labelled vertices and d labelled half edges emanating from\neach vertex.\nWe assume that nd is even with d being fixed.\nWe pair up these nd half edges\nuniformly as random and glue every matched pair of half edges into a full edge.\nThis gives a\nrandom d-regular multigraph (see Figure 2). The number of possible matchings of nd half edges is\n(nd -1)!! = (nd -1)(nd -3) · · · 3 · 1. Let Gn,d denote the random multigraph obtained this way.\nThe probability that Gn,d is a simple graph is uniformly bounded away from zero at n →inf. In\nfact, Bender and Canfield [2] showed that as n →inf,\nP\n\nGn,d is simple\n\n→\n1-2\nd\ne\n.\nAlso, conditioned on Gn,d being simple its distribution is a uniform random d-regular graph on\nn vertices.\nIt follows from these observations that any sequence of graph properties An whose\nprobability under Gn,d tends to 1 as n →infalso tends to 1 under the uniform random d-regular\ngraph model. In particular, if Gn,d converges to Td in the local weak limit then so does a sequence\nof uniform random d-regular graphs.\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nFigure 2. A matching of 12 half edges on 4 vertices giving rise to a 3 regular multigraph.\nNow we show that Gn,d converges to Td in the local weak limit. Unpacking the definition of local\nweak limit this means that for every r > 0 we must show that\n(1)\nE\nh |v ∈V (Gn,d) : Nr(Gn,d, v) ∼= Nr(Td, *)|\nn\ni\n→1 as n →inf,\nwhere *is any fixed vertex of Td (note that Td is vertex transitive). Notice that if Nr(Gn,d, v)\ncontains no cycles then it must be isomorphic to Nr(Td, *) due to Gn,d being d-regular.\nNow\nsuppose that Nr(Gn,d, v) contains a cycle. Then this cycle has length at most 2r and v lies within\ndistance r of some vertex of this cycle. Thus the number of vertices v such that Nr(Gn,d, v) is not\na cycle is at most the number of vertices in Gn,d that are within distance r of any cycle of length\n2r in Gn,d. Let us call such vertices bad vertices. The number of vertices within distance r of any\nvertex x ∈V (Gn,d) is at most dr. Therefore, the number of bad vertices is at most dr(2r)C≤2r\nwhere C\n2r is the (random) number of cycles in Gn,d of length at most 2r. It follows from this\n≤\nargument that\n∼\nE |v ∈V (Gn,d) : Nr(Gn,d, v) = Nr(Td, *)|\n≤2rdrE C\nfollo\n≤2r . The\nwing lemma\nshows that E C\n2r\n≤2r\n\n≤2r(3d -3)\nif d ≥3, and more precisely\n\n, E\n\nC≤2\n\nr\nconverges to a finite\nlimit as n →inffor every d. This establishes (1), and thus, Gn,d conv\n\nerges to\n\nTd in the local weak\nlimit.\n(\n1)l\nLemma 2.3. Let\n\nd-\n\nClbe the number of cycles of length lin Gn,d. Then limn\nE C\n→inf\nl\n=\n.\n2l\nMoreover, E Cl\n≤(3d -3)lif d ≥3.\nProof. Given a set of ldistinct vertices {v1, . . . , vl} the number of ways to arrange them in cyclic\norder is (l-1)!/2. Given a cyclic ordering, the number of ways to pair half edges in the configuration\nmodel such that these vertices form a cycle is (d(d\n}\n(\nd-1))\n-1))l(nd\n2l\n1)!!. Therefore, the probability that\n{\nl-1)!(d(\nl(nd-2l\n-\n-\nv1, . . . , vl\nform an l-cycle in Gn,d is\n-1)!!. From the linearity of expectation we\n2(nd-1)!!\nconclude that\n\nn\n(l\n1)!(d(d\n1))l(nd\n2l\n1)!!\nE Cl\n=\nP {v1, . . . , vl} forms an l-cycle\n=\n-\n-\n-\n-\n.\nl\n2(nd\n1)!!\n{v1\nX\n,...,vl}\n\n-\nNote that\nn\n≤\nl\nnl/l!, and in fact if lis fixed then\nn\n= (1 + o(1)) n\nas n →inf. Similarly,\nl\nl\nl!\n(nd-2l-1)!!/((nd-1)!!) = (1+o(1))(nd)-las n →infand it is at most 3l(nd)-lif d ≥3 (provided\n\nMUSTAZEE RAHMAN\nthat lis fixed). It follows from these observations that E\n\nCl\n\n→(d -1)l/(2l), and is at most\n(3d -3)lif d ≥3.\n3. Enumeration of spanning trees\nThe Matrix-Tree Theorem allows us to express the number of spanning trees in a finite graph G\nin terms of the eigenvalues of the SRW transition matrix P of G. As we will see, this expression\nit turn can be written in terms of the return probabilities of the SRW on G. This is good for our\npurposes because if a sequence of bounded degree graphs Gn converges in the local weak limit to\na random rooted graph (G, *\nlog sptr(G\n) then we will be able to express\nn) in terms of the expected\n|Gn|\nreturn probabilities of the SRW on (G, *). In particular, we shall see that\nlog sptr(Gn)\nlim\n= E\nh\nlog deg(*) -\nX pk\nG(*) i\n.\nn→inf\n|Gn|\nk\nk≥1\nThe quantity of the r.h.s. is called the tree entropy of (G, *). If the limiting graph G is deterministic\nand vertex transitive, for example Zd or Td, then the above simplifies to\nlog sptr(Gn)\nlim\n= log d\nn→inf\n|Gn\n-\n|\nk\nX pk\nG(*),\nk\n≥1\nwhere d is the degree of G and *is any fixed vertex. In this manner we will be able to find expressions\nfor the tree entropy of Zd and Td and asymptotically enumerate the number of spanning trees in\nthe grid graphs Z[-n, n]d and random regular graphs Gn,d.\n3.1. The Matrix-Tree Theorem. Let G be a finite graph. Let D be the diagonal matrix consisting\nof the degrees of the vertices of G. The Laplacian of G is the |G| × |G| matrix L = D(I -P), where\nI is the identity matrix and P is the transition matrix of the SRW on G. It is easily seen that\nL(x, x) = deg(x), L(x, y) = -1 if x ∼y in G and L(x, y) = 0 otherwise (if G is a multigraph then\nL(x, y) equals negative of the number of edges from x to y).\nExercise 3.1. The Laplacian L of a graph G is a matrix acting on the vector space RV (G). Let\n(f, g) = P\nx V (G) f(x)g(x) denote the standard inner product on RV (G). Prove each of the following\n∈\nstatements.\n(1) (Lf, g) = 1 P\n(x,y)(f(x) -f(y))(g(x)\n(\nx∼y\n-g y)).\n(2) L is self-adjoint and positive semi-definite: (Lf, g) = (f, Lg) and (Lf, f) ≥0 for all f, g.\n(3) Lf = 0 if and only if f is constant on the connected components of f.\n(4) The dimension of the eigenspace of L corresponding to eigenvalue 0 equals the number of\nconnected components of G.\n(5) If G is connected and has maximum degree ∆then L has |G| eigenvalues 0 = λ0 < λ1 ≤\n· · · ≤λ G\n.\n|-1 ≤2∆\n|\n□\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nLet G be a finite connected graph. From part (5) of exercise 2.2 we see that the Laplacian L of\nG has n = |G| eigenvalues 0 = λ0 < λ1 ≤· · · ≤λn-1. The Matrix-Tree Theorem states that\n(2)\nsptr(G) =\nn-1\nn\ni\nY\nλi.\n=1\nIn other words, the number of spanning trees in G is the product of the non-zero eigenvalues of\nthe Laplacian of G. In fact, the Matrix-Tree Theorem states something a bit more precise. Let Li\nbe the (n-1)×(n-1) matrix obtained from L by removing its i-th row and column; Li is called the\n(i, i)-cofactor of L. The Matrix-Tree Theorem states that det(Li) = sptr(G) for every i. To derive\n(2)\nP\nwe consider the characteristic polynomial det(L -tI) of L and note that the coefficient of t is\n-\ni det(Li) = -nsptr(G). On the other hand, if we write the characteristic polynomial in terms\nn\nof its roots, which are the eigenvaluesof L, then we can deduce that the coefficient of t is -\n-\ni=1 λi.\nExercise 3.2. Let G be a connected finite graph and suppose\nQ\n{x, y} is an edge of G. Let G \\ {x, y}\nbe the graph obtained from removing the edge {x, y} from G. Let G · {x, y} be the graph obtained\nfrom contracting the edge {x, y}. Prove that sptr(G) = sptr(G \\ {x, y}) + sptr(G · {x, y}).\nTry to prove the Matrix-Tree Theorem by induction on the number of edges of G, the identity\nabove, and the expression for the determinant in terms of the cofactors along any row.\nIt is better for us to express (2) in terms of the eigenvalues of the SRW transition matrix P of G.\nThe matrix P also has n real eigen\nP\nvalues. Perhaps the easiest way to see this is to define a new inner\nproduct on RV (G) by (f, g)π =\nx∈V (G) π(x)f(x)g(x) where π(x) = deg(x)/2e and e is the number\nof edges in G. The vector π is\nP\ncalled the stationary measure of the SRW on G. It is a probability\ndistribution on V (G), that is,\nx π(x) = 1. Also, π(x)P(x, y) = π(y)P(y, x). The latter condition\nis equivalent to (Pf, g)π = (f, Pg)π for all f, g ∈RV (G), which means that P is self-adjoint w.r.t. the\ninner product (·, ·)π. Due to being self-adjoint it has n real eigenvalues and an orthonormal basis of\neigenvector w.r.t. the new inner product.\nNotice that the eigenvalues of P lie in the interval [-1, 1] since ||Pf||\n≤||f||\nwhere\ninf\ninf\n||f||\n=\ninf\nmaxx V (G){|f(x)|}. If G is connected then the largest eigenvalue of P is 1 and it has multiplicity\n∈\n1 as well. The eigenfuctions for the eigenvalue 1 are constant functions over V (G). Suppose that\n-1 ≤μ1 ≤μ2 ≤· · · ≤μn\n1 < μn = 1 are the n eigenvalues of P. If e is the number of edges in G\n-\nthen we may rewrite (2) as\n(\n(3)\nsptr(G =\nQ\nx∈V G) deg(x)\n)\nn-1\n)\n2e\nY\n(1\n=1\n-μi .\ni\nThis formula is derived from determining the coefficient of t in the characteristic polynomial of I -P,\nwhich equals (det(D))-1det(L -tD). This is a rather tedious exercise so we leave the derivation to\nthe interested reader.\nFrom 3 and |G| being n we deduce that\nlog sptr(G)\nlog 2e(G)\nx V\n=\n+\n∈\n(G)\n+\ni=1 log(1 -μi).\n|G|\nn\nn\nn\nP\nlog deg(x)\nPn-1\n\nMUSTAZEE RAHMAN\nSince\nP\nlog(1-x) = -\nk\n1 xk/k for -1 ≤\nn\nn\nx < 1 we see that\n-\n≥\ni=1 log(1-μi) = -\n-\nk\nk≥1\ni=1 μi /k.\nn\nNow,\n-1\ni=1 μk\ni = TrP k\nP\n-1 since we exclude the eigenvalue 1\nP\nof P that occurs with\nP\nmultiplicit\nP\ny one.\nNote that TrP k = P\nx V (G) pk\nG(x) where pk\nG(x) in the k-step return probability of the SRW in G\n∈\nstarted from x. Consequently, we conclude that\nlog sptr(G)\n(G)\nP\nk\nlog 2e\nx V\n(4)\n=\n∈\n(G) log deg(x)\n+\n|G|\nn\nn\n-\nX 1 (\nx∈V (G) pG(x)) -1\n.\nk\nn\nk≥1\nP\nTheorem 3.3. Let Gn be a sequence of finite, connected graphs with maximum degree bounded by ∆\nand |Gn| →inf. Suppose that Gn converges in the local weak limit to a random rooted graph (G, *).\nlog sptr(G\nThen\nn) converges to\n|Gn|\nh(G, *) = E\nh\nlog deg(*) -\nk\nX\npk\nk\nG(\n≥1\n*)\ni\n.\nIn particular, suppose that G is a deterministic, vertex transitive graph of degree d. If *∈V (G) is\nany fixed vertex then the tree entropy of G is defined to be\nX 1\nh(G) = log d -\npk\nk\nG(\nk≥1\n*).\nThe tree entropy h(G) does not depend on the choice of the sequence of graphs Gn converging to G\nin the local weak limit.\nTo prove this theorem let *n be a uniform random vertex of Gn. Then from (3) we get that\nlog sptr(Gn)\n2e(Gn)\n=\n+ E log\n|Gn\n|Gn|\n\ndeg(*n)\n\n-\n|\nk\nX\nE pk\nk\nGn( n)\nGn\n-1 .\n≥1\n\n*\n\n-|\n|\n\nAs Gn has degree bounded by ∆we have 2e(Gn) = P\nx V (G\ndeg(x)\n∆n. Thus, log(2e(Gn))/ G\n∈\nn\nn)\n≤\n|\n|\nconverges to 0. Also, deg(*n) converges in distribution to the degree deg(*) of (G, *) (exercise 2.2).\nThe function x →log x is\n\nbounded and continuous if 1 ≤x ≤∆.\nTherefore, E log deg( n)\nconverges to E log deg(*) . Following the discussion is Section 2.1 we conclude that E pk\n*\nG (\nn *n) -\n\n|Gn|-1 converges to E\n\npk\nG(*)\n\nas well. To conclude the proof it suffices to show that\n\nE pk\nGn(*n)\n-|G\n-α\nn|-1 ≤k\nfor some α > 0.\nThen it follows from the dominated\n\ncon\n\nvergence\n\ntheorem that P\nk≥1\n(E\n\npk\nG (*\nn)\nk\n\n-|Gn\nn\n|-) con-\nverges to E\nk\nas required.\n≥1 pk\nG(*)/k ,\nLemma 3.4.\nP\nLet G be a fini\n\nte connected graph of maximum degree ∆. Let pk\nG(x) denote the k-step\nreturn probability of the SRW on G starting at x. Let π(x) = deg(x)/2e for x ∈V (G), where e is\nthe number of edges in G. Then for every x ∈V (G) and k ≥0,\npk\nG(x)\n∆\nπ(x\n-1\n\nn\n≤\n.\n)\n(k + 1)1/4\nProof. The vector π is a probability\n\nmeasure on\n\nV (G). Let (f, g)π =\nx V (G) π(x)f(x)g(x) for\n∈\nf, g ∈RV (G). Let P denote the transition matrix of the SRW on G; thus,\nP\npk\nG(x) = P k(x, x). Note\nthat π(x)P(x, y) = 1x\ny/(2e) = π(y)P(y, x). From this we conclude that (Pf, g)π = (f, Pg)π. Let\n∼\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nU ⊂RV (G) be the subspace of vectors f such that P\nx π(x)f(x) = 0. Then U is a P invariant\nsubspace.\nSuppose f ∈RV (G) takes both positive and negative values. Suppose that |f(x0)| = ||f||\n=\ninf\nmaxx V (G) |f(x)|, and by replacing f with -f if necessary we may assume that f(x\n∈\n0) ≥0. Let z be\nsuch that f(z) ≤0. Then ||f||\n≤|f(x0) -f(z)|. There is a path x0, x1, . . . , x\ninf\nt = z in G from x0\nto z. Therefore,\nt\n||f||inf≤\nX\ni=1\n|f(xi-1) -f(xi)| ≤2\nX\n(x,y)∈V (G)×V (G)\nx∼y\n|f(x) -f(y)|.\nLet K(x, y) = π(x)P(x, y) = 1x ∼y/(2e). The sum above becomes e\n(x,y) K(x, y)|f(x) -f(y)|.\nConsider an f ∈U, which must take both positive and negative values. Apply the inequality\nabove to the function sgn(f)f 2 and use the inequality |sgn(s)s2 -sgn(t)t2| ≤|s -t|(|s| + |t|) to\nconclude that ||f||2 ≤e P\n(x,y) K(x, y)\n\n|f(x) -f(y)\ny\ninf\n|(|f(x)| + |f( )|)\n\n. Straightforward calculations\nshow that\nX\nK(x, y)|f(x) -f(y)|2 =\n(I -P)f, f\n\nand\nX\nK(x, y)|f(x) + f(y)|2 =\n(I + P)f, f\nπ\n(x,y)\n(x,y)\n\n.\nπ\nIf we apply the Cauchy-Schwarz inequality to the terms\np\nK(x, y)|f(x)-f(y)| and\np\nK(x, y)(|f(x)|+\n|f(y)|) then we deduce that\n||f||4\ninf≤e2(I -P)f, f\n\nπ ·\n(I + P)|f|, |f|\n.\nπ\nNotice that(Pf, f)π ≤(f, f)π because all eigenvalues of P lies in\n\nthe interval [0, 1]. Therefore,\n(I +P)|f|, |f|\n≤2( f\nπ\n| |, |f|)π. If (f, f)π ≤1 then we see that\nm\n||f||4\ninf≤2e2 (I -P)f, f\n. Applying\nπ\nthis to the function P\nf and using that P is self-adjoint we deduce that\n\n||P mf||4 ≤2e2\nf\ninf\n(I -P)P mf, P m\n= 2e2 (P 2m\nπ\n-P 2m+1)f, f .\nSince ||Pg||\n≤||g||\n, if we sum the inequality abo\n\nve over 0\n\ninf\ninf\n≤m ≤k we get\nk\n(k + 1)||P kf||4 ≤2e2 X\n||P mf||\n≤2e2(I -P 2k+1)f, f\ninf\nm=0\n\nπ ≤2e .\ninf\nThe\nlast inequalit\n\ny holds because every eigenvalue of I\nP m lies in the interval [0, 1] and thus\n(I -P m\n-\n)f, f\n≤(f, f)π.\nπ\nWe have concluded that ||P kf\n√\n||\n≤\n2e(k + 1)-1/4 if f ∈U and (f, f)π ≤1. Let us now apply\ninf\n√1x(y)-π(x)\nthis to the function f(y) =\n. Then |P kf(x)\n√\n| ≤\n2e(k + 1)-1/4. The value of P kf(x) is\nπ(x)(1-π(x))\nP k(x, x)(1 -pπ(x)) -π(x) P\ny=x P k(y, x)\nP k(x, x)(1 -pπ(x)) -π(x)(1\nP\n=\n-\nk(x, x))\nπ(x)(1 -π(x))\nπ(x)(1 -π(x))\nP k(x, x)\n= p\n-π(x) .\nπ(x)(1 -π(x))\nP\n\nMUSTAZEE RAHMAN\nTherefore,\nP k(x,x) -1\n≤\np\n2eπ(x)-1(1 -π(x))(k + 1)-1/4. However, π(x)-1 = 2e/deg(x)\nπ(x)\n≤2e\nand 1 -π(x) ≤\nP k(x,x)\n1. Thus, we conclude that\n\n-1\n≤2e(k + 1)-1/4. As 2e equals the sum of\nπ(x)\nthe degrees in G we have that 2e ≤∆n, and this establishes the statement in the lemma.\n□\nLemma 3.4 implies that if G is a finite connected graph of maximum degree ∆then\n|G|-1x∈\nX\npk\nG(x)\nV (G)\n\n-1\n\nX\n\n= |G|-1\npk\nG(x) -π(x)\nx∈V (G)\n\nX\npk\n\n≤|G|-\nπ(x)\nG(x)\nπ(x) -1\nx∈V (G)\n\n∆\n≤\n.\n(k + 1)1/4\nThis proves that\nE\n\npk (*)\n\n-|G |-1\nG\nn\nn\nn\n≤∆k-/4 and completes the proof of Theorem 3.3.\n3.2. Tree entropy of T\nZd\nd and\n. In order\n\nto calculate the tree entropy of a graph we have to be\nable to compute the return probability of the SRW on the graph. There is a rich enough theory that\ndoes this for d-regular tree and Zd. We begin\nP\nwith the d-regular tree Td.\nConsider the generating function F(t) =\nk\n0 pk (*)tk. Actually note that pk (\nif\n≥\n*) = 0\nk is\nTd\nTd\nodd because whenever the SRW takes a step along an edge that moves it away from the root it\nmust traverse that edge backwards in order to return. So it is not possible to return in an odd\nnumber of steps. (This holds in any bipartite graph, for example, Zd as well.) There is a classical\nproblem called the Ballot Box problem that allows us to compute p2k(\nTd *) explicitly. It turns out that\n2k\n2k\nk\np2k\n( ) (d\n1) -\n( )\n(*) =\nk\n-\n2k\n. The numbers\nk\nare called the Catalan numbers. From this it is possible\nTd\nk+1\nd\n-\nk+1\nto find a closed form of F(t) (see Wo\n\ness [9\n\n] Lemma 1.24):\n2(d\nF(t) =\n-1)\nd -1 +\np\n.\nd2 -4(d -1)t2\nNote that P\nk\n1 pk (*) =\nR 1 F (t)-1 dt. It turns out that this integrand has an antiderivative and\n≥\nTd\nt\nit can be shown that\n(d\nh(Td) = log\n-1)d-1\n.\n(d -2d)(d/2)-1\nThis result was proved by McKay[8]. Since the\nh\nrandom d-regular\ni\ngraphs Gn,d converge to Td in\nthe local weak limit we see that E log sptr(Gn,d)\n\n= nh(Td) + o(n).\nA rigorous calculation of the tree entropy of Zd requires an excursion into operator theory that is\noutside the scope of these notes. We will sketch the argument; for details see Lyons [5] Section 4 or\nLyons [6]. Recall the Matrix-Tree Theorem for finite graphs which states that if λ1, . . .\nP\n, λn-1 are the\nn\npositive eigenvalues of the Laplacian of a graph G of size n then log sptr(G)/n = (1/n)\n-\ni=1 log λi-\n(log n)/n. There is an infinitary version of this representation for the tree entropy of Zd. If L is\nthe Laplacian on Zd, which acts on l2(Zd), then one can define an operator log L on l2(Zd) that\nsatisfies\nh(Zd) =\n\n(log L) 1o, 1o\n\n.\n\nLOCAL CONVERGENCE OF GRAPHS AND ENUMERATION OF SPANNING TREES\nIn the above, (·, ·) is the inner product on l2(Zd) and 1o is the indicator function of the origin.\nThe inner product above may be calculated via the Fourier transform. The Fourier transform\nstates that l2(Zd) is isomorphic as a Hilbert space to L2([0, 1]d) in that any function f\nl2(Zd)\n\n∈\nˆ\n\nˆ\ncorresponds to an unique f ∈L2([0, 1]d) such that f(k) =\nR\nf(x)e2πi x·k dx. This correspondence\npreserves the inner product between functions. The Fourier transform maps 1o to the function that\nis identically 1 in [0, 1]d. It also transforms the operator log L to the operator on L2([0, 1]d) which\nis multiplication by the function (x1, . . . , xd) →log(2d -2 P\ni cos(2πxi)). As the Fourier transform\npreserves inner products we get that\nd\nh(Zd) =\nZ\nlog\n[0,1]d\n\n2d -2\nX\ncos(2πxi)\ni=1\n\ndx.\n4. Open problems\nOne can consider the space of (isomorphism classes of) doubly rooted graphs (G, x, y) of bounded\ndegree, analogous to the space G. It is also a compact metric space where the distance between\n(G, x, y) and (H, u, v) is 1/(1 + R) where R is the minimal r such that the r-neighborhood of (x, y)\nin G is isomorphic to the r-neighbourhood of (u, v) in H. Consider a Borel measurable function F\nfrom the space of double rooted graphs into [0, inf). Note that F is defined on isomorphism classes\nof such graphs, so F(G, x, y) = F(H, u, v) if φ : G →H is a graph isomorphism satisfying φ(x) = u\nand φ(y) = v.\nA random rooted graph (G, *) is unimodular if for all F as above the following equality holds\nE\nh\nX\nF(*, x)\nE\n∼*in G\ni\n=\nh\nX\nF(x, *)\nx\nx∼*in G\ni\n.\nHere is an example. Let G be a finite connected graph and suppose *∈G is a uniform random root.\nThen\nP\n(G, *) is unimodular because both the left and right hand side of the equation above equals\n(x,y) F(x, y), where the sum is over all pair of vertices in G. Unimodularity is preserved under\nx∼y\ntaking local weak limits and so any random rooted graph (G, *) that is a local weak limit of finite\nconnected graphs is unimodular.\nIt is not known whether the converse is true: is a unimodular random rooted graph (G, *) a local\nweak limit of finite connected graphs. This is known to be true for unimodular trees (see Aldous\nand Lyons [1]). This is a major open problem in the field.\nHere is a problem on tree entropy. Bernoulli bond percolation on Td at density p is a random\nforest of Td obtained by deleting each edge independently with probability 1 -p. Let *be a fixed\nvertex and denote by Cp(*) the component of *in the percolation process. Then Cp(*) is finite with\nprobability one if p ≤1/(d -1) and infinite with positive probability otherwise. Let Cp\ninf(*) be the\nrandom rooted tree obtained from Cp(*) by conditioning it to be infinite if p > 1/(d -1). In fact,\nCp\ninf(*) is unimodular. What is the tree entropy of Cp\ninf(*)? Is it strictly increasing in p?\n\nMUSTAZEE RAHMAN\nReferences\n[1] D. J. Aldous and R. Lyons, Processes on unimodular networks, Electron. J. Probab. 12 #54 (2007), pp. 1454-\n1508.\n[2] E. A. Bender and E. R. Canfield, The asymptotic number of labelled graphs with given degree sequences, Journal\nof Combinatorial Theory Series A 24 (1978), pp. 296-307.\n[3] I. Benjamini and O. Schramm, Recurrence of distributional limits of finite planar graphs, Electron. J. Probab. 6\n#23 (2001).\n[4] B. Bollob as, Random graphs, 2nd ed., Cambridge University Press, Cambridge, 2001.\n[5] R. Lyons, Asymptotic enumeration of spanning trees. Combin. Probab. Comput. 14 (2005), pp. 491-522.\n[6] R. Lyons, Identities and inequalities for tree entropy, Combin. Probab. Comput. 19 (2010), pp. 303-313.\n[7] R. Lyons and Y. Peres, Probability on Trees and Networks, Cambridge University Press, 2014, in preparation.\nCurrent version available at\n.\n[8] B. D. McKay, Spanning trees in regular graphs, Europ. J. Combin. 4 (1983), pp. 149-160.\n[9] W. Woess, Random Walks on Infinite Graphs and Groups, Cambridge University Press, Cambridge, 2000.\nDepartment of Mathematics, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cam-\nbridge, MA 02139\nhttp://mypage.iu.edu/~rdlyons/.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 18-19: Compressed Sensing and Sparse Recovery",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/e819e378cf61f25069e3d38d30e0d520_MIT18_S096F15_Ses18_19.pdf",
      "content": "Compressed Sensing and Sparse Recovery\nMost of us have noticed how saving an image in JPEG dramatically reduces the space it occupies in\nour hard drives (as oppose to file types that save the pixel value of each pixel in the image). The idea\nbehind these compression methods is to exploit known structure in the images; although our cameras\nwill record the pixel value (even three values in RGB) for each pixel, it is clear that most collections of\npixel values will not correspond to pictures that we would expect to see. This special structure tends\nto exploited via sparsity. Indeed, natural images are known to be sparse in certain bases (such as the\nwavelet bases) and this is the core idea behind JPEG (actually, JPEG2000; JPEG uses a different\nbasis).\nLet us think of x ∈RN as the signal corresponding to the image already in the basis for which it is\nsparse. Let's say that x is s-sparse, or ∥x∥0 ≤s, meaning that x has, at most, s non-zero components\nand, usually, s ≪N. The l0 norm26 ∥x∥0 of a vector x is the number of non-zero entries of x. This\nmeans that, when we take a picture, our camera makes N measurements (each corresponding to a pixel)\nbut then, after an appropriate change of basis, it keeps only s ≪N non-zero coefficients and drops\nthe others. This motivates the question: \"If only a few degrees of freedom are kept after compression,\nwhy not measure in a more efficient way and take considerably less than N measurements?\". This\nquestion is in the heart of Compressed Sensing [CRT06a, CRT06b, CT05, CT06, Don06, FR13]. It is\nparticularly important in MRI imaging [?] as less measurements potentially means less measurement\ntime. The following book is a great reference on Compressed Sensing [FR13].\nMore precisely, given a s-sparse vector x, we take s < M ≪N linear measurements yi = aT\ni x and\nthe goal is to recover x from the underdetermined system:\n\ny\n\n=\n\nA\n\nx\n\n.\nLast lecture\nwe used Gordon's theorem to show that, if w\n\ne to\n\nok random measurements, on the\norder of s log\nN\ns\n\nmeasurements suffice to have all considerably different s-sparse signals correspond\nto considerably different sets of measurements. This suggests that ≈s log\nN\nmay be enough to\ns\nrecover x, we'll see (later) in this lecture that this intuition is indeed correct.\nSince the system is underdetermined and we know x is sparse, the natural thing\n\nto try, in order\nto recover x, is to solve\nmin\n∥z∥0\n(55)\ns.t.\nAz = y,\nand hope that the optimal solution z corresponds to the signal in question x. Unfortunately, (55) is\nknown to be a computationally hard problem in general. Instead, the approach usually\nP\ntaken in sparse\nN\nrecovery is to consider a convex relaxation of the l0 norm, the l1 norm: ∥z∥1 =\ni=1 |zi|. Figure 19\n26The l0 norm is not actually a norm though.\n\ndepicts how the l1 norm can be seen as a convex relaxation of the l0 norm and how it promotes\nsparsity.\nFigure 19: A two-dimensional depiction of l0 and l1 minimization. In l1 minimization (the picture\nof the right), one inflates the l1 ball (the diamond) until it hits the affine subspace of interest, this\nimage conveys how this norm promotes sparsity, due to the pointy corners on sparse vectors.\nThis motivates one to consider the following optimization problem (surrogate to (55)):\nmin\n∥z∥1\n(56)\ns.t.\nAz = y,\nIn order for (56) we need two things, for the solution of it to be meaningful (hopefully to coincide\nwith x) and for (56) to be efficiently solved.\nWe will formulate (56) as a Linear Program (and thus show that it is efficiently solvable). Let us\nthink of ω+ as the positive part of x and ω-as the symmetric of the negative part of it, meaning that\nx = ω+ -ω-and, for each i, either ωi\n-or ω+\ni is zero. Note that, in that case,\nN\n∥x∥1 =\nX\nω+\nT\ni + ωi\n-= 1\nω+ + ω-\ni=1\n\n.\nMotivated by this we consider:\nmin\n1T (ω+ + ω-)\ns.t.\nA (ω+ -ω-) = y\nω+\n(57)\n≥0\nω-≥0,\nwhich is a linear program. It is not difficult to see that the optimal solution of (57) will indeed satisfy\nthat, for each i, either ωi\n-or ω+\ni is zero and it is indeed equivalent to (56). Since linear programs are\nefficiently solvable [VB04], this means that (56) efficiently.\n\n6.1\nDuality and exact recovery\nThe goal now is to show that, under certain conditions, the solution of (56) indeed coincides with x.\nWe will do this via duality (this approach is essentially the same as the one followed in [Fuc04] for the\nreal case, and in [Tro05] for the complex case.)\nLet us start by presenting duality in Linear Programming with a game theoretic view point. The\nidea will be to reformulate (57) without constraints, by adding a dual player that wants to maximize\nthe objective and would exploit a deviation from the original constraints (by, for example, giving the\ndual player a variable u and adding to to the objective uT (y -A (ω+ -ω-))). More precisely consider\nthe following\nmin max\nv+ T\n1T ω+ + ω-\nω+\n+\nu\nω\nv+\n\n-\n\n-\nT\nv-\nω-+ uT y -A ω+ -ω-\n.\n(58)\nω-\nv\n≥\n-≥0\n\nIndeed, if the primal player (picking ω+ and ω-and attempting to minimize the objective) picks\nvariables that do not satisfy the original constraints, then the dual player (picking u, v+, and v-and\ntrying to maximize the objective) will be able to make the objective value as large as possible. It is\nthen clear that (57) = (58).\nNow image that we switch the order at which the players choose variable values, this can only\nbenefit the primal player, that now gets to see the value of the dual variables before picking the primal\nvariables, meaning that (58) ≥(59), where (59) is given by:\nmax min 1T ω+ + ω-\n-\nv+T\nT\nω+\n-\nu\n+\nv+\nω\n≥0\n-\nω-\n-≥\nv\nv\n\nω-+ uT y -A\nω+ -ω-\n.\n(59)\nRewriting\nT\nT\nmax min 1\nv+\nAT u\nω+ + 1\nv-+ AT u\nω-+ uT y\n(60)\nu\nω+\nv+≥0\n-\nω-\nv-≥0\n-\n\n-\n\nWith this formulation, it becomes clear that the dual players needs to set 1 -v+ -AT u = 0,\n1 -v-+ AT u = 0 and thus (60) is equivalent to\nmax\nuT y\nu\nv+\nv\n≥\n-≥0\n1-v+-AT u=0\n1-v-+AT u=0\nor equivalently,\nmaxu\nuT y\ns.t.\n-1 ≤AT\n(61)\nu ≤1.\nThe linear program (61) is known as the dual program to (57). The discussion above shows that\n(61) ≤(57) which is known as weak duality. More remarkably, strong duality guarantees that the\noptimal values of the two programs match.\n\nThere is a considerably easier way to show weak duality (although not as enlightening as the one\nabove). If ω-and ω+ are primal feasible and u is dual feasible, then\n0 ≤\n1T -uT A\n\nω+ +\n1T + uT A\n\nω-= 1T ω+ + ω-\n-uT\nA\nω+ -ω-\n= 1T\nshowing that (61)\n(57).\nω+ + ω-\n-uT y,\n(62)\n≤\n6.2\nFinding a dual certificate\nIn order to show that ω+ -ω-= x is an optimal solution27 to (57), we will find a dual feasible point\nu for which the dual matches the value of ω+ -ω-= x in the primal, u is known as a dual certificate\nor dual witness.\nFrom (62) it is clear that u must satisfy\n1T -uT A ω+ = 0 and 1T + uT A ω-= 0, this is known\nas complementary slackness. This means that we must\n\nhave the en\ntries of AT u\n\nbe +1 or -1 when x\nis non-zero (and be +1 when it is positive and -1 when it is negative), in other words\nAT u\n\n= sign (xS) ,\nS\nwhere S = supp(x), and\nAT u\n\n≤1 (in order to be dual feasible).\ninf\nRemark 6.1 It is not difficult to see that if we further ask that\nsolution would have to have its support contained in the support\nAT u\n\nc\n< 1 any optimal primal\nS\ninf\nof x. This observation gives us the\nfollowing Lemma.\nLemma 6.2 Consider the problem of sparse recovery discussed this lecture. Let S = supp(x), if AS\nis injective and there exists u ∈RM such that\nAT u\n\n= sign (xS) ,\nS\nand\nAT u Sc\n< 1,\ninf\nthen x is the unique optimal solution to the\nl1 minimization\n\nprogram 56.\nSince we know that\nAT u\n\n=sign\n(xS) (and\n\nthat AS is injective), we'll try to construct u by least\nS\nsquares and hope that it satisfies AT u\nc\n< 1. More precisely, we take\nS\ninf\nu = AT\n+\nS\nsign (xS) ,\nwhere\nAT + = A\nAT\nS\n-\nS\nSAS\n\nis the Moore Penrose pseudo-inverse of AT\nS. This gives the following\nCorollary.\nCorollary 6.3 Consider the problem of sparse recovery discussed this lecture. Let S = supp(x), if\nAS is injective and\nAT\nAT\nScAS\nSAS\n-sign (xS)\n< 1,\nSc\ninf\nthen x is the unique optimal solution\n\nto\nthe l1\n\nminimization pr\n\nogram 56.\n27For now we will focus on showing that it is an optimal solution, see Remark 6.1 for a brief discussion of how to\nstrengthen the argument to show uniqueness\n\nRecall the definition of A ∈RM×N satisfying the restricted isometry property from last Lecture.\nDefinition 6.4 (Restricted Isometry Property) A matrix A ∈RM×N is said to satisfy the (s, δ)\nrestricted isometry property if\n(1 -δ) ∥x∥≤∥Ax∥2 ≤(1 + δ) ∥x∥2 ,\nfor all two s-sparse vectors x.\nLast lecture (Lecture 5) we showed that if M ≫s log\nN\ns\n\nand A ∈RM×N is drawn with i.i.d.\ngaussian entries N\n0, 1\nM\n\nthen it will, with high probability, satisfy the (s, 1/3)-RIP. Note that, if A\nsatisfies the (s, δ)-RIP then, for any |S| ≤s one has ∥AS∥≤\nq\n1 + 1\n3 and l\nAT\nSAS\n-1 ∥≤\n1 -1\n-1 =\n3, where\n∥· ∥denotes the operator norm ∥B∥= max\nBx .\n∥x∥=1 ∥\n∥\nThis means that, if we take A random with i.i.d. N\n0, 1\nM\n\nentries then, for any |S ≤s| we have\nthat\n∥AS\nAT\nSAS\n-1 sign (xS) ∥≤\nr\n1 + 1\n2 =\n√\n3√s,\nand because of the independency among the entries of A, ASc is independent of this vector and so for\neach j ∈Sc we have\nProb\n\nAT A\nAT\n(\n\nS\n-\nj\nSAS\n\nsign xS) ≥√\nM\n√\n3√st\n\n≤2 exp\n\n-t2\n,\n\nwhere Aj is the j-th column of A.\nUnion bound gives\n\nT\nT\n-1\n\nProb\nASAS ASAS\nsign (xS)\ninf≥√\nM\n√\n3√st\n\n≤2N exp\n\n-t2\n\n,\nwhich implies\nProb\nAT\nSAS\nAT\nSAS\n-1 sign (xS)\n\ninf≥1\n\n≤2N exp\n\n-\n√\nM\n√\n3s\n\n= exp\n\n-1\nM\n-2 log(2N)\n,\ns\n\nwhich means that we expect to exactly recover x via l1 minimization when M ≫s log(N), similarly\nto what was predicted by Gordon's Theorem last Lecture.\n6.3\nA different approach\nGiven x a sparse vector, we want to show that x is the unique optimal solution to\nmin\n∥z∥1\n(63)\ns.t.\nAz = y,\nLet S = supp(x) and suppose that z = x is an optimal solution of the l1 minimization problem.\nLet v = z -x, it satisfies\n∥v + x∥1 ≤∥x∥1\nand\nA(v + x) = Ax,\n\nthis means that Av = 0. Also, ∥x∥S = ∥x∥1 ≥∥v+x∥1 = ∥(v + x)S ∥1+∥vSc∥1 ≥∥x∥S-∥vS∥1+∥v∥Sc,\nwhere the last inequality follows by triangular inequality. This means that ∥vS∥1 ≥∥vSc∥1, but since\n|S| ≪N it is unlikely for A to have vectors in its nullspace that are this concentrated on such few\nentries. This motivates the following definition.\nDefinition 6.5 (Null Space Property) A is said to satisfy the s-Null Space Property if, for all v\nin ker(A) (the nullspace of A) and all |S| = s we have\n∥vS∥1 < ∥vSc∥1.\nFrom the argument above, it is clear that if A satisfies the Null Space Property for s, then x will\nindeed be the unique optimal solution to (56). Also, now that recovery is formulated in terms of certain\nvectors not belonging to the nullspace of A, one could again resort to Gordon's theorem. And indeed,\nGordon's Theorem can be used to understand the number of necessary measurements to do sparse\nrecovery28 [CRPW12]. There is also an interesting\napproach based on Integral Geometry [ALMT14].\nAs it turns out one can show that the\n2s, 1\ne\n\n-RIP implies s-NSP [FR13]. W omit that proof as\nit does not appear to be as enlightening (or adaptable) as the approach that was shown here.\n6.4\nPartial Fourier matrices satisfying the Restricted Isometry Property\nWhile the results above are encouraging, rarely one has the capability of designing random gaussian\nmeasurements. A more realistic measurement design is to use rows of the Discrete Fourier Transform:\nConsider the random M × N matrix obtained by drawing rows uniformly with replacement from the\nN × N discrete Fourier transform matrix. It is known [CT06] that if M = Ωδ(K polylog N), then the\nresulting partial Fourier matrix satisfies the restricted isometry property with high probability.\nA fundamental problem in compressed sensing is determining the order of the smallest number M\nof random rows necessary. To summarize the progress to date, Cand`es and Tao [CT06] first found that\nM = Ωδ(K log6 N) rows suffice, then Rudelson and Vershynin [RV08] proved M = Ωδ(K log4 N), and\nmore recently, Bourgain [Bou14] achieved M = Ωδ(K log3 N); Nelson, Price and Wootters [NPW14]\nalso achieved M = Ωδ(K log3 N), but using a slightly different measurement matrix. The latest result\nis due to Haviv and Regev [HR] giving an upper bound of M = Ωδ(K log2 k log N). As far as lower\nbounds, in [BLM15] it was shown that M = Ωδ(K log N) is necessary. This draws a contrast with\nrandom Gaussian matrices, where M = Ωδ(K log(N/K)) is known to suffice.\nOpen Problem 6.1 Consider the random M × N matrix obtained by drawing rows uniformly with\nreplacement from the N × N discrete Fourier transform matrix. How large does M need to be so that,\nwith high probability, the result matrix satisfies the Restricted Isometry Property (for constant δ)?\n6.5\nCoherence and Gershgorin Circle Theorem\nLast lectures we discussed the problem of building deterministic RIP matrices (building deterministic\nRIP matrices is particularly important because checking whether a matrix is RIP is computationally\n28In these references the sets considered are slightly different than the one described here, as the goal is to ensure\nrecovery of just one sparse vector, and not all of them simultaneously.\n\nhard [BDMS13, TP13]).\nDespite suboptimal, coherence based methods are still among the most\npopular ways of building RIP matrices, we'll briefly describe some of the ideas involved here.\nRecall the definition of the Restricted Isometry Property (Definition 6.4). Essentially, it asks that\nany S ⊂[N], |S| ≤s satisfies:\n(1 -δ)∥x∥2 ≤∥A x∥2 ≤(1 + δ)∥x∥2\nS\n,\nfor all x ∈R|S|. This is equivalent to\nxT\nmax\nx\nAT\nSAS -I\n\nx\nx\n≤δ,\nT x\nor equivalently\nAT\nSAS -I\n≤δ.\nIf the columns of A are unit-norm vectors (in RM), then the diagonal of AT\nSAS is all-ones, this\nmeans that AT\nSAS -I consists only of the non-diagonal elements of AT\nSAS. If, moreover, for any two\ncolumns\n\na\nT\n\ni,aj, of A we have\nai aj\n≤μ for some μ then, Gershgorin's circle theorem tells us that\nAT\nSAS -I ≤δ(s -1).\nMore precisely, given a symmetric matrix B, the Gershgorin's circle theorem [HJ85] tells that all\nof the eigenvalues\nn of B are contained\nP\nin thoe so called Gershgorin discs (for each i, the Gershgorin disc\ncorresponds to\nλ : |λ -Bii| ≤\nj=i |Bij| . If B has zero diagonal, then this reads:\nB\n\n∥\n∥≤maxi |Bij|.\nGiven a set of N vectors a\nM\n1, . . . , aN ∈R\nwe define its worst-case coherence μ as\nμ = max aT\ni aj\ni=j\nGiven a set of unit-norm vectors a\n\n1, . . . , aNRM with w\n\norst-case coherence μ, if we form a matrix\nwith these vectors as columns, then it will be (s, μ(s -1)μ)-RIP, meaning that it will be\ns, 1\n\n- RIP\nfor s ≤1\n1.\nμ\n6.5.1\nMutually Unbiased Bases\nWe note that now we will consider our vectors to be complex valued, rather than real valued, but all\nof the results above hold for either case.\nConsider the following 2d vectors: the d vectors from the identity basis and the d orthonormal\nvectors corresponding to columns of the Discrete Fourier Transform F. Since these basis are both\northonormal the vectors in question are unit-norm and within the basis are orthogonal, it is also easy\nto see that the inner product between any two vectors, one from each basis, has absolute value\n√,\nd\nmeaning that the worst case coherence of this set of vectors is μ =\n√\nd this corresponding matrix [I F]\nis RIP for s ≈\n√\nd.\nIt is easy to see that\n√\ncoherence is the minimum possible between two orthonormal bases in\nd,\nd\nC\nsuch bases are called unbiased (and are important in Quantum Mechanics, see for example [BBRV01])\nThis motivates the question of how many orthonormal basis can be made simultaneously (or mutually)\nunbiased in Cd, such sets of bases are called mutually unbiased bases. Let M(d) denote the maximum\nnumber of such bases. It is known that M(d) ≤d + 1 and that this upper bound is achievable when\nd is a prime power, however even determining the value of M(6) is open [BBRV01].\n\nOpen Problem 6.2 How many mutually unbiased bases are there in 6 dimensions? Is it true that\nM(6) < 7?29\n6.5.2\nEquiangular Tight Frames\nAnother natural question is whether one can get better coherence (or more vectors) by relaxing the\ncondition that the set of vectors have to be formed by taking orthonormal basis. A tight frame (see,\nfor example, [CK12] for more on Frame Theory) is a set of N vectors in CM (with N ≥M) that spans\nCM \"equally\". More precisely:\nDefinition 6.6 (Tight Frame) v1, . . . , vN ∈CM is a tight frame if there exists a constant α such\nthat\nX\nN\n|⟨vk, x⟩|2 = α∥x∥2,\n∀x∈CM ,\nk=1\nor equivalently\nX\nN\nvkvT\nk = αI.\nk=1\nThe smallest coherence of a set of N unit-norm vectors in M dimensions is bounded below by the\nWelch bound (see, for example, [BFMW13]) which reads:\nμ ≥\ns\nN -M\n.\nM(N -1)\nOne can check that, due to this limitation, deterministic constructions based on coherence cannot\nyield matrices that are RIP for s\n√\n≫\nM, known as the square-root bottleneck [BFMW13, Tao07].\nThere are constructions that achieve the Welch bound, known as Equiangular Tight Frames\n(ETFs), theseqare tight frames for which all inner products between pairs of vectors have the same\nmodulus μ =\nN-M , meaning that they are \"equiangular\". It is known that for there to exist an\nM(N-1)\nETF in CM one needs N ≤M2 and ETF's for which N = M2 are important in Quantum Mechanics,\nand known as SIC-POVM's. However, they are not known to exist in every dimension (see here for\nsome recent computer experiments [SG10]). This is known as Zauner's conjecture.\nOpen Problem 6.3 Prove or disprove the SIC-POVM / Zauner's conjecture: For any d, there exists\nan Equiangular tight frame with d2 vectors in Cd dimensions. (or, there exist d2 equiangular lines in\nCd).\nWe note that this conjecture was recently shown [Chi15] for d = 17 and refer the reader to\nthis interesting remark [Mix14c] on the description length of the constructions known for different\ndimensions.\n29The author thanks Bernat Guillen Pegueroles for suggesting this problem.\n\n6.5.3\nThe Paley ETF\nThere is a simple construction of an ETF made of 2M vectors in M dimensions (corresponding to\na M × 2M matrix) known as the Paley ETF that is essentially a partial Discrete Fourier Transform\nmatrix. While we refer the reader to [BFMW13] for the details the construction consists of picking\nrows of the p × p Discrete Fourier Transform (with p ∼= 1 mod 4 a prime) with indices corresponding\nto quadratic residues modulo p. Just by coherence considerations this construction is known to be\nRIP for s\n√\n≈\np but conjectured [BFMW13] to be RIP for s ≈\np\n, which would be predicted if\npolylogp\nthe choice os rows was random (as discussed above)30. Although partial conditional (conditioned on\na number theory conjecture) progress on this conjecture has been made [BMM14] no unconditional\nresult is known for s\n√\n≪\np. This motivates the following Open Problem.\nOpen Problem 6.4 Does the Paley Equiangular tight frame satisfy the Restricted Isometry Property\npass the square root bottleneck? (even by logarithmic factors?).\nWe note that [BMM14] shows that improving polynomially on this conjecture implies an improve-\nment over the Paley clique number conjecture (Open Problem 8.4.)\n6.6\nThe Kadison-Singer problem\nThe Kadison-Singer problem (or the related Weaver's conjecture) was one of the main questions in\nframe theory, it was solved (with a non-constructive proof) in the recent breakthrough of Marcus,\nSpielman, and Srivastava [MSS15b], using similar techniques to their earlier work [MSS15a]. Their\ntheorem guarantees the existence of universal constants η\nM\n≥2 and θ > 0 s.t. for any tight frame\nω1, . . . , ωN ∈C\nsatisfying ∥ωk∥≤1 and\nX\nN\nωkωT\nk = ηI,\nk=1\nthere exists a partition of this tight frame S1, S2 ⊂[N] such that each is \"almost a tight frame\" in the\nsense that,\nX\nωkωT\nk ⪯(η -θ) I.\nk∈Sj\nHowever, a constructive prove is still not known and there is no known (polynomial time) method\nthat is known to construct such partitions.\nOpen Problem 6.5 Give a (polynomial time) construction of the tight frame partition satisfying the\nproperties required in the Kadison-Singer problem (or the related Weaver's conjecture).\n30We note that the quadratic residues are known to have pseudorandom properties, and indeed have been leveraged\nto reduce the randomness needed in certain RIP constructions [BFMM14]\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Session 20: Group Testing and Error-Correcting Codes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/eeddd14f99618988d18b47ef4d72e8c6_MIT18_S096F15_Ses20.pdf",
      "content": "Group Testing and Error-Correcting Codes\n7.1\nGroup Testing\nDuring the Second World War the United States was interested in weeding out all syphilitic soldiers\ncalled up for the army. However, syphilis testing back then was expensive and testing every soldier\nindividually would have been very costly and inefficient. A basic breakdown of a test is: 1) Draw\nsample from a given individual, 2) Perform required tests, and 3) Determine presence or absence of\nsyphilis.\nIf there are n soldiers, this method of testing leads to n tests.\nIf a significant portion of the\nsoldiers were infected then the method of individual testing would be reasonable. The goal however,\nis to achieve effective testing in the more likely scenario where it does not make sense to test n (say\nn = 100, 000) people to get k (say k = 10) positives.\nLet's say that it was believed that there is only one soldier infected, then one could mix the samples\nof half of the soldiers and with a single test determined in which half the infected soldier is, proceeding\nwith a binary search we could pinpoint the infected individual in log n tests. If instead of one, one\nbelieves that there are at most k infected people, then one could simply run k consecutive binary\nsearches and detect all of the infected individuals in k log n tests. Which would still be potentially\nmuch less than n.\nFor this method to work one would need to observe the outcome of the previous tests before\ndesigning the next test, meaning that the samples have to be prepared adaptively. This is often not\npractical, if each test takes time to run, then it is much more efficient to run them in parallel (at\nthe same time). This means that one has to non-adaptively design T tests (meaning subsets of the n\nindividuals) from which it is possible to detect the infected individuals, provided there are at most k\nof them. Constructing these sets is the main problem in (Combinatorial) Group testing, introduced\nby Robert Dorfman [Dor43] with essentially the motivation described above.31\nLet Ai be a subset of [T] = {1, . . . , T} that indicates the tests for which soldier i participates.\nConsider A the family of n such sets A = {A1, . . . , An}.\nWe say that A satisfies the k-disjunct\nproperty if no set in A is contained in the union of k other sets in A. A test set designed in such a\nway will succeed at identifying the (at most k) infected individuals - the set of infected tests is also a\nsubset of [T] and it will be the union of the Ai's that correspond to the infected soldiers. If the set\nof infected tests contains a certain Ai then this can only be explained by the soldier i being infected\n(provided that there are at most k infected people).\nTheorem 7.1 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nT = O\nk2 log n\nProof.\nWe will use the probabilistic method. We will\n\n.\nshow that, for T = Ck2 log n (where C is\na universal constant), by drawing the family A from a (well-chosen) distribution gives a k-disjunct\nfamily with positive probability, meaning that such a family must exist (otherwise the probability\nwould be zero).\n31in fact, our description for the motivation of Group Testing very much follows the description in [Dor43].\n\nLet 0 ≤p ≤1 and let A be a collection of n random (independently drawn) subsets of [T].\nThe distribution for a random set A is such that each t ∈[T] belongs to A with probability p (and\nindependently of the other elements).\nConsider k + 1 independent draws of this random variable, A0, . . . , Ak. The probability that A0 is\ncontained in the union of A1 through Ak is given by\nT\nPr [A0 ⊆(A1 ∪· · · ∪Ak)] =\n\n1 -p(1 -p)k\n.\nThis is minimized for p =\n\nk+1. For this choice of p, we have\n1 -p(1 -p)k = 1 -\nk + 1\n\n1 -\nk\nk + 1\n\nGiven that there are n such sets, there are (k + 1)\nn\ndifferent ways of picking a set and k\nk+1\nothers to test whether the first is contained in the union of the other k. Hence, using a union bound\nargument, the probability that A is k-disjunct can be bounded\n\nas\nPr[k -disjunct] ≥1 -( + 1)\n\nn\nk\nk\n\n+\n-k + 1\n\n1 -\n.\nk\n\nT\nk\n+ 1\n!\nIn order to show that one of the elements in A is k-disjunct we show that this probability is strictly\npositive. That is equivalent to\n\n1 -k + 1\n\n1 -\nk + 1\nk!T\n≤\n.\n(k + 1)\nn\nk+1\nNote that\n\n1 -\nk+1\nk\n→e-1\n1-\nk+1 = e-1 k+1\nk , as k →inf. Thus, we only need\nT ≥\nlog\n\n(k + 1)\nn\nk+1\n\n-log\n\n1 -\nk+1e-1 k+1\nk\n=\nlog\n\nk\nn\nk+1\n\n=\n-log (1 -(ek)-1)\nO(k2 log(n/k)),\nwhere the last inequality uses the fact that log\nn\nk+1\n\n= O\nk log\nn\ndue to Stirling's formula and\nk\nthe Taylor expansion -log(1 -x-1)-1 = O(x)\n\nThis argument simply shows the existence of a family satisfying the k-disjunt property. However,\nit is easy to see that by having T slightly larger one can ensure that the probability that the random\nfamily satisfies the desired property can be made very close to 1.\nRemarkably, the existence proof presented here is actually very close to the best known lower\nbound.\nTheorem 7.2 Given n and k, if there exists a family A of subsets of [T] satisfying the k-disjunct\nproperty, then\nT = Ω\nk2 log n\nlog k\n\n.\n\nProof.\nFix a u such that 0 < u <\nT ; later it will be fixed to u :=\nl\n(T\n-k)/\nk-1\n.\nm\nWe start by\nconstructing a few auxiliary family of sets. Let\nA0 = {A ∈A : |A| < u},\nand let A1 ⊆A denote the family of sets in A that contain their own unique u-subset,\nA1 :=\n\nA ∈A : ∃F ⊆A : |F| = u and, for all other A′ ∈A, F ⊆A′ .\nWe will procede by giving an upper bound to A0 ∪A1. For that, we will need a coup\n\nle of auxiliary\nfamily of sets. Let F denote the family of sets F in the definition of A1. More precisely,\nF := {F ∈[T] : |F| = u and ∃!A ∈A : F ⊆A} .\nBy construction |A1| ≤|F|\nAlso, let B be the family of subsets of [T] of size u that contain an element of A0,\nB = {B ⊆[T] : |B| = u and ∃A ∈A0 such that A ⊆B} .\nWe now prove that |A0| ≤|B|. Let B′ denote the family of subsets of [T] of size u that are not in\nB,\nB′ =\nB′ ⊆[T] : |B′| = u and B′ ∈/ B .\nBy construction of A0 and B, no\n\nset in B′ contains a set in A0 nor\n\ndoes a set in A0 contain a set\nin B′. Also, both A0 and B′ are antichains (or Sperner family), meaning that no pair of sets in each\nfamily contains each other. This implies that A0 ∪B′ is an antichain containing only sets with u or\nless elements. The Lubell-Yamamoto-Meshalkin inequality [Yam54] directly implies that (as long as\nu < T ) the largest antichain whose sets contain at most u elements is the family of subsets of [T] of\nsize u. This means that\n|A0| +\n\nT\nB′\n\n=\nA0 ∪B′ ≤\n\n=\nB\nB\nu\n∪\n′ = |B| +\nB′\nThis implies that A\n.\n\n.\n|\n0| ≤|B|\nBecause A satisfies the k-disjunct property, no two sets in A can contain eachother. This implies\nthat the families B and F of sets of size u are disjoint which implies that\nT\n|A0 ∪A1| = |A0| + |A1| ≤|B| + |F| ≤\n\nu\n\n.\nLet A2 := A \\ (A0 ∪A1). We want to show that if A ∈A2 and A1, . . . , Aj ∈A we have\n\nj\nA \\\n[\nAi\n> u(k\nj\ni=1\n-).\n(64)\nThis is readily shown by\n\\ Snoting that if (64) did not hold then one could find Bj+1, . . . , Bk subsets of\nj\nk\nA of size t such that A\ni=1 Ai ⊆S\ni=j+1 Bi. Since A as no unique subsets of size t there must exist\n\nk\nAj+1, . . . , Ak ∈A such that Bi ⊆Ai for i = j + 1, . . . , k. This would imply that A ⊆\ni=1 Ai which\nwould contradict the k-disjunct property.\nIf |A2| > k then we can take A0, A1, . . . , Ak distinct elements of A2.\nFor this choice\nS\nand any\nj = 0, . . . , k\n\nAj \\\n[\nAi\n1 + u(k\nj).\n≤i<j\n\n≥\n\n-\nThis means that\n\nk\nAj\nj=0\n\n[\n=\nj=0\nX\n,...,k\n\nAj \\\n≤\n[\nAi\ni<j\n\nX\nk + 1\n\n≥\n\n[1 + u(k -j)] = 1 + k + u\n\n.\nj=0,...,k\n\nSince all sets in A are subsets of [T] we must have 1 + k + u\nhand, taking\nk + 1\nk+1\n≤\nk\n\nS\nj=0 Aj\n≤T. On the other\nu :=\n\n(T -k)/\n\ngives a contradition (note that this choice of u is smaller than T as long as k > 2). This implies that\n|A2| ≤k which means that\nT\nT\nn = |A| = |A0| + |A1| + |A2| ≤k +\n\n= k +\nu\n\nl\n(T -k)/\nk+1\n\nThis means that\nm .\nlog n ≤log\n\nk +\n\n!\n\nl\nT\n-k)/\nk+1\n\nT\n= O\n(T\nm\nlog k\nk2\n\n,\nwhich concludes the proof of the theorem.\nWe essentially borrowed the proof of Theorem 7.2 from [Fur96]. We warn the reader however that\nthe notation in [Fur96] is drasticly different than ours, T corresponds to the number of people and n\nto the number of tests.\nThere is another upper bound, incomparable to the one in Theorem 7.1 that is known.\nTheorem 7.3 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nn\n= O\n\nk2\nlog\nT\nlog k\n!\n.\nThe proof of this Theorem uses ideas of Coding Theory (in particular Reed-Solomon codes) so we\nwill defer it for next section, after a crash course on coding theory.\nThe following Corollary follows immediately.\n\nCorollary 7.4 Given n and k, there exists a family A satisfying the k-disjunct property for a number\nof tests\nT = O\nk2 log n\nlog k\nmin\n\nlog k, log n\n.\nlog k\n\nWhile the upper bound in Corollary 7.4 and the lower bound in Theorem 7.2 are quite close, there\nis still a gap. This gap was recently closed and Theorem 7.2 was shown to be optimal [DVPS14]\n(original I was not aware of this reference and closing this gap was posed as an open problem).\nRemark 7.5 We note that the lower bounds established in Theorem 7.2 are not an artifact of the\nrequirement of the sets being k-disjunct. For the measurements taken in Group Testing to uniquely\ndetermine a group of k infected individuals it must be that the there are no two subfamilies of at most\nk sets in A that have the same union. If A is not k -1-disjunct then there exists a subfamily of k -1\nsets that contains another set A, which implies that the union of that subfamily is the same as the\nunion of the same subfamily together with A. This means that a measurement system that is able to\nuniquely determine a group of k infected individuals must be k -1-disjunct.\n7.2\nSome Coding Theory and the proof of Theorem 7.3\nIn this section we (very) briefly introduce error-correcting codes and use Reed-Solomon codes to prove\nTheorem 7.3. We direct the reader to [GRS15] for more on the subject.\nLets say Alice wants to send a message to Bob but they can only communicate through a channel\nthat erases or replaces some of the letters in Alice's message. If Alice and Bob are communicating with\nan alphabet Σ and can send messages with lenght N they can pre-decide a set of allowed messages\n(or codewords) such that even if a certain number of elements of the codeword gets erased or replaced\nthere is no risk for the codeword sent to be confused with another codeword. The set C of codewords\n(which is a subset of ΣN) is called the codebook and N is the blocklenght.\nIf every two codewords in the codebook differs in at least d coordinates, then there is no risk of\nconfusion with either up to d -1 erasures or up to ⌊d-1\n2 ⌋replacements. We will be interested in\ncodebooks that are a subset of a finite field, meanign that we will take Σ to be Fq for q a prime power\nand C to be a linear subspace of Fq.\nThe dimension of the code is given by\nm = logq |C|,\nand the rate of the code by\nm\nR =\n.\nN\nGiven two code words c1, c2 the Hamming distance ∆(c1, c2) is the number of entries where they\ndiffer. The distance of a code is defined as\nd =\nmin\n∆(c1, c2).\nc1=c2∈C\nFor linear codes, it is the same as the minimum weight\nω(C) =\nmin\n∆(c).\nc∈C\\{0}\n\nWe say that a linear code C is a [N, m, d]q code (where N is the blocklenght, m the dimension, d\nthe distance, and Fq the alphabet.\nOne of the main goals of the theory of error-correcting codes is to understand the possible values\nof rates, distance, and q for which codes exist. We simply briefly mention a few of the bounds and\nrefer the reader to [GRS15]. An important parameter is given by the entropy function:\nlog(q\n1)\nHq(x) = x\n-\nlog q\n-xlog x\nlog q -(1 -x)log(1 -x).\nlog q\n- Hamming bound follows essentially by noting that if a code has distance d then balls of radius\n⌊d-1\n2 ⌋centered at codewords cannot intersect. It says that\nR ≤1 -Hq\nd\n+\nN\n\no(1)\n- Another particularly simple bound is Singleton bound (it can be easily proven by noting that\nthe first n + d + 2 of two codewords need to differ in at least 2 coordinates)\nd\nR ≤1 -\n+ o(1).\nN\nThere are probabilistic constructions of codes that, for any ε > 0, satisfy\nR ≥1 -Hq\nd\nN\n\n-ε.\nThis means that R∗the best rate achievable satisties\nR∗≥1 -Hq\nd\n,\nN\n\n(65)\nknown as the GilbertVarshamov (GV) bound [Gil52, Var57]. Even for q = 2 (corresponding to binary\ncodes) it is not known whether this bound is tight or not, nor are there deterministic constructions\nachieving this Rate. This motivates the following problem.\nOpen Problem 7.1\n1. Construct an explicit (deterministic) binary code (q = 2) satisfying the\nGV bound (65).\n2. Is the GV bound tight for binary codes (q = 2)?\n7.2.1\nBoolean Classification\nA related problem is that of Boolean Classification [AABS15]. Let us restrict our attention to In\nerror-correcting codes one wants to build a linear codebook that does not contain a codeword with\nweight ≤d -1. In other words, one wants a linear codebook C that does intersect B(d -1) = {x ∈\n{0, 1}n : 0 < ∆(x) ≤d -1} the pinched Hamming ball of radius d (recall that ∆(d) is the Hamming\nweight of x, meaning the number of non-zero entries). In the Boolean Classification problem one is\nwilling to confuse two codewords as long as they are sufficiently close (as this is likely to mean they are\n\nin the same group, and so they are the same from the point of view of classification). The objective\nthen becomes understanding what is the largest possible rate of a codebook that avoids an Annulus\nA(a, b) = {x ∈{0, 1}n : a ≤∆(x) ≤b}. We refer the reader to [AABS15] for more details. Let us call\nthat rate\nRA\n∗(a, b, n).\nNote that RA\n∗(1, d-1, n) corresponds to the optimal rate for a binary error-correcting code, conjectured\nto be 1 -H\nd\nq\n(The\nN\n\nGV bound).\nOpen Problem 7.2 It is conjectured in [AABS15] (Conjecture 3 in [AABS15]) that the optimal rate\nin this case is given by\nRA\n∗(αn, βn, n) = α + (1 -α) RA\n∗(1, βn, (1 -α)) + o(1),\nwhere o(1) goes to zero as n goes to infinity.\nThis is established in [AABS15] for β ≥2α but open in general.\n7.2.2\nThe proof of Theorem 7.3\nReed-Solomon codes[RS60] are [n, m, n -m + 1]q codes, for m ≤n ≤q. They meet the Singleton\nbound, the drawback is that they have very large q (q > n).\nWe'll use their existence to prove\nTheorem 7.3\nProof. [of Theorem 7.3]\nWe will construct a family A of sets achieving the upper bound in Theorem 7.3. We will do this\nby using a Reed-Solomon code [q, m, q -m+1]q. This code has qm codewords. To each codework c we\nwill correspond a binary vector a of length q2 where the i-th q-block of a is the indicator of the value\nof c(i). This means that a is a vector with exactly q ones (and a total of q2 entries)32. We construct\nthe family A for T = q2 and n = qm (meaning qm subsets of q2 ) by constructing, for each codeword\nc, the set of non-zero entries of the corresponding binary vector a.\nThese sets have the following properties,\n\nmin\nj∈[n] |Aj| = q,\nand\nmax\n|Aj\n\n∈\nAj2 = q\nmin\n∆(c1, c2)\nq\n(q\nm + 1) = m\n1.\nj1=j2 [n]\n∩\n|\n-\nc1=\n\nc2∈C\n≤\n-\n-\n-\nThis readily implies that A is k-disjunct for\nk =\nq -1\n,\nm -1\n\nbecause the union of\nj\nq-1\nm-1\nk\nsets can only contain (m -1)\nj\nq-1\nm-1\nk\n< q elements of another set.\nNow we pick q ≈2k log n (q has to be a prime but there is always a prime between this number and\nlog k\nits double by Bertrand's postulate (see [?] for a particularly nice proof)). Then m = log n (it can be\nlog q\ntaken to be the ceiling of this quantity and then n gets updated accordingly by adding dummy sets).\n32This is precisely the idea of code concatenation [GRS15]\n\nThis would give us a family (for large enough parameters) that is k-disjunct for\nq -1\nm -1\n\n≥\n$\n2k log n\nlog k -1\nlog n\nlog q + 1 -1\n%\n=\n\n2k log q\nlog q\nlog k -log n\n\n≥\nk.\nNoting that\nT ≈\n\nlog n\n2k\nlog k\n\n.\nconcludes the proof.\n7.3\nIn terms of linear Bernoulli algebra\nWe can describe the process above in terms of something similar to a sparse linear system. let 1Ai be\nthe t -dimensional indicator vector of Ai, 1i:n be the (unknown) n-dimensional vector of infected\nsoldiers and 1t:T the T-dimensional vector of infected (positive) tests. Then\n|\n|\n1A\n1A\n\n· · ·\nn\n\n|\n|\n⊗\n|\n\n|\n=\n|\n\n,\nwhere\nis\n\ni:n\n\n|\n\nt:T\n|\n|\n⊗\nmatrix-vector multiplication in the Bernoulli algebra, basically the only thing that is\ndifferent from the standard matrix-vector multiplications is that the addition operation is replaced by\nbinary \"or\", meaning 1 ⊕1 = 1.\nThis means that we are essentially solving a linear system (with this non-standard multiplication).\nSince the number of rows is T = O(k2 log(n/k)) and the number or columns n ≫T the system is\nunderdetermined. Note that the unknown vector, 1i:n has only k non-zero components, meaning it\nis k-sparse. Interestingly, despite the similarities with the setting of sparse recovery discussed in a\n\nprevious lecture, in this case, O(k2) measurements are needed, instead of O(k) as in the setting of\nCompressed Sensing.\n7.3.1\nShannon Capacity\nThe goal Shannon Capacity is to measure the amount of information that can be sent through a noisy\nchannel where some pairs of messages may be confused with eachother. Given a graph G (called\nthe confusion graph) whose vertices correspond to messages and edges correspond to messages that\nmay be confused with each other. A good example is the following: say one has a alphabet of five\nsymbols 1, 2, 3, 4, 5 and that each digit can be confused with the immediately before and after (and\n1 and 5 can be confused with eachother). The confusion graph in this case is C5, the cyclic graph\n\non 5 nodes.\nIt is easy to see that one can at most send two messages of one digit each without\nconfusion, this corresponds to the independence number of C5, α(C5) = 2. The interesting question\narises when asking how many different words of two digits can be sent, it is clear that one can send at\nleast α(C5)2 = 4 but the remarkable fact is that one can send 5 (for example: \"11\",\"23\",\"35\",\"54\",\nor \"42\"). The confusion graph for the set of two digit words C⊕2\ncan be described by a product of\nthe original graph C5 where for a graph G on n nodes G⊕2 is a graph on n nodes where the vertices\nare indexed by pairs ij of vertices of G and\n(ij, kl) ∈E G⊕2\nif both a) i = k or i, k\n=\n\n∈E and b) j\nl or j, l\n∈E hold.\nThe above observation can then be written as α C⊕2\n= 5.\nThis motivates the definition of\nShannon Capacity [Sha56]\n\nθS (G) sup\nk\n\nG⊕k 1\nk .\nLovasz, in a remarkable paper [Lov79], showed that θS (C5) =\n√\n5, but determining this quantity is\nan open problem for many graphs of interested [AL06], including C7.\nOpen Problem 7.3 What is the Shannon Capacity of the 7 cycle?\n7.3.2\nThe deletion channel\nIn many applications the erasures or errors suffered by the messages when sent through a channel are\nrandom, and not adversarial. There is a beautiful theory understanding the amount of information\nthat can be sent by different types of noisy channels, we refer the reader to [CT] and references therein\nfor more information.\nA particularly challenging channel to understand is the deletion channel.\nThe following open\nproblem will envolve a particular version of it. Say we have to send a binary string \"10010\" through\na deletion channel and the first and second bits get deleted, then the message receive would be \"010\"\nand the receiver would not know which bits were deleted. This is in contrast with the erasure channel\nwhere bits are erased but the receiver knows which bits are missing (in the case above the message\nreceived would be \"??010\"). We refer the reader to this survey on many of the interesting questions\n(and results) regarding the Deletion channel [Mit09].\nA particularly interesting instance of the problem is the Trace Reconstruction problem, where the\nsame message is sent multiple times and the goal of the receiver is to find exactly the original message\nsent from the many observed corrupted version of it. We will be interested in the following quantity:\nDraw a random binary stringwithn bits, suppose the channel has a deletion probability of 1 for each\nbit (independently), define D n; 1\nhas the number of times the receiver needs to receive the message\n(with independent corruptions)\n\nso that she can decode the message exactly, with high probability.\nIt is easy to see that D n; 1\n≤2n, since roughly once in every 2n times the whole message will go\n√\nthrough the channel unharmed. It is possible to show (see [HMPW]) that D\nn; 1\nknown whether this bound is tight.\n\n≤2\nn but it is not\nOpen Problem 7.4\n1. What are the asymptotics of D\nn; 1\n\n?\n\n2. An interesting aspect of the Deletion Channel is that different messages may have different\ndifficulties of decoding.\nThis motivates the following question: What are the two (distinct)\nbinary sequences x(2) and x(2) that are more difficult to distinguish (let's say that the receiver\nknows that either x(1) or x(2) was sent but not which)?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Topics in Mathematics of Data Science Lecture Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/cd754449860c6337d03260226fcb52a8_MIT18_S096F15_Open7.2.pdf",
      "content": "7.2.1\nBoolean Classification\nA related problem is that of Boolean Classification [AABS15]. Let us restrict our attention to In\nerror-correcting codes one wants to build a linear codebook that does not contain a codeword with\nweight ≤d -1. In other words, one wants a linear codebook C that does intersect B(d -1) = {x ∈\n{0, 1}n : 0 < ∆(x) ≤d -1} the pinched Hamming ball of radius d (recall that ∆(d) is the Hamming\nweight of x, meaning the number of non-zero entries). In the Boolean Classification problem one is\nwilling to confuse two codewords as long as they are sufficiently close (as this is likely to mean they are\nin the same group, and so they are the same from the point of view of classification). The objective\nthen becomes understanding what is the largest possible rate of a codebook that avoids an Annulus\nA(a, b) = {x ∈{0, 1}n : a ≤∆(x) ≤b}. We refer the reader to [AABS15] for more details. Let us call\nthat rate\nRA\n∗(a, b, n).\nNote that RA\n∗(1, d-1, n) corresponds to the optimal rate for a binary error-correcting code, conjectured\nto be 1 -H\nd\nq\nOp\n\n(The GV bound).\nN\nen Problem 7.2 It is conjectured in [AABS15] (Conjecture 3 in [AABS15]) that the optimal rate\nin this case is given by\nRA\n∗(αn, βn, n) = α + (1 -α) RA\n∗(1, βn, (1 -α)) + o(1),\nwhere o(1) goes to zero as n goes to infinity.\nThis is established in [AABS15] for β ≥2α but open in general.\n\nReference\n[AABS15]\nE. Abbe, N. Alon, A. S. Bandeira, and C. Sandon. Linear boolean classification, coding\nand \"the critical problem\". Available online at arXiv:1401.6528v3 [cs.IT], 2015.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 0.1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/563e30feff737262df22446786dcd471_MIT18_S096F15_Open0.1.pdf",
      "content": "0.2.1\nKoml os Conjecture\nWe start with a fascinating problem in Discrepancy Theory.\nOpen Problem 0.1 (Koml os Conjecture) Given n, let K(n) denote the infimum over all real\nnumbers such that: for all set of n vectors u\nn\n1, . . . , un ∈ R satisfying luil2 ≤ 1, there exist signs\nEi = ±1 such that\nlE1u1 + E2u2 + · · · + Enunlinf ≤ K(n).\nThere exists a universal constant K such that K(n) ≤ K for all n.\nAn early reference for this conjecture is a book by Joel Spencer [Spe94]. This conjecture is tightly\nconnected to Spencer's famous Six Standard Deviations Suffice Theorem [Spe85]. Later in the course\nwe will study semidefinite programming relaxations, recently it was shown that a certain semidefinite\nrelaxation of this conjecture holds [Nik13], the same paper also has a good accounting of partial\nprogress on the conjecture.\n\n- It is not so difficult to show that K(n)\n√\n≤\nn, try it!\nReferences\n[Nik13]\nA. Nikolov.\nThe komlos conjecture holds for vector colorings.\nAvailable online at\narXiv:1301.4039 [math.CO], 2013.\n[Spe85]\nJ. Spencer. Six standard deviations suffice. Trans. Amer. Math. Soc., (289), 1985.\n[Spe94]\nJ. Spencer. Ten Lectures on the Probabilistic Method: Second Edition. SIAM, 1994.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 0.2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/f4bd396a2d0e0668134c9f9a7d658a9f_MIT18_S096F15_Open0.2.pdf",
      "content": "Open Problem 0.2 For any collection of d × d positive semidefinite matrices A1, · · · , An, the fol-\nlowing is true:\n(a)\n\nn\nAσ(j\nn! σ∈Sym(\nX\n)\nn) j=1\n\nY\n\n≤\n\nn\nn\n\nAk\n,\nnn\nj\nk1,...,k\n\nX\nn=1 j\nY\n=1\n\nand\n\n(b)\n\nn\nn\nn\nAσ(j)\nAk\n,\nn! σ\nX\n≤nn\nj\n∈Sym(n)\n\nj\n\nY\n=1\n\nk1,...,k\nX\nn=1\n\nj\nY\n=1\n\nwhere Sym(n) denotes the group of\n\npermutations\n\nof n elements,\n\nand\n\n∥· ∥the spectral norm.\nMorally, these conjectures state that products of matrices with repetitions are larger than with-\nout. For more details on the motivations of these conjecture (and their formulations) see [RR12] for\nconjecture (a) and [Duc12] for conjecture (b).\nRecently these conjectures have been solved for the particular case of n = 3, in [Zha14] for (a)\nand in [IKW14] for (b).\n0.2.2\nMatrix AM-GM inequality\nWe move now to an interesting generalization of arithmetic-geometric means inequality, which has\napplications on understanding the difference in performance of with- versus without-replacement sam-\npling in certain randomized algorithms (see [RR12]).\nReferences\n[Duc12]\nJ. C. Duchi. Commentary on \"towards a noncommutative arithmetic-geometric mean\ninequality\" by b. recht and c. re. 2012.\n[IKW14]\nA. Israel, F. Krahmer, and R. Ward. An arithmetic-geometric mean inequality for prod-\nucts of three matrices. Available online at arXiv:1411.0333 [math.SP], 2014.\n[RR12]\nB. Recht and C. Re. Beneath the valley of the noncommutative arithmetic-geometric\nmean inequality: conjectures, case-studies, and consequences. Conference on Learning\nTheory (COLT), 2012.\n[Zha14]\nT. Zhang. A note on the non-commutative arithmetic-geometric mean inequality. Avail-\nable online at arXiv:1411.5058 [math.SP], 2014.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 1.1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/ecbad9d39ad5e2a918f969068f54c47b_MIT18_S096F15_Open1.1.pdf",
      "content": "Open Problem 1.1 (Mallat and Zeitouni [MZ11]) Let g ∼N(0, Σ) be a gaussian random vector\nin Rp with a known covariance matrix Σ and d < p. Now, for any orthonormal basis V = [v1, . . . , vp]\nof Rp, consider the following random variable ΓV : Given a draw of the random vector g, ΓV is the\nsquared l2 norm of the largest projection of g on a subspace generated by d elements of the basis V .\nThe question is:\nWhat is the basis V for which E [ΓV ] is maximized?\nThe conjecture in [MZ11] is that the optimal basis is the eigendecomposition of Σ. It is known\nthat this is the case for d = 1 (see [MZ11]) but the question remains open for d > 1. It is not very\ndifficult to see that one can assume, without loss of generality, that Σ is diagonal.\nA particularly intuitive way of stating the problem is:\n1. Given Σ ∈Rp×p and d\n2. Pick an orthonormal basis v1, . . . , vp\n3. Given g ∼N(0, Σ)\n4. Pick d elements v 1, . . . , v d of the basis\nd\n5. Score:\ni=1 v T\ni g\nThe objectiv\nP\ne is\nto pic\n\nk the basis in order to maximize the expected value of the Score.\nNotice that if the steps of the procedure were taken in a slightly different order on which step\n4 would take place before having access to the draw of g (step 3) then the best basis is indeed\nthe eigenbasis of Σ and the best subset of the basis is simply the leading eigenvectors (notice the\nresemblance with PCA, as described above).\nMore formally, we can write the problem as finding\n\nargmax\n\nE\n\nmax\nvT\ni g\n,\n∈Rp×p\nS\n[p\nV T\n⊂\n]\nV\n\nV =I\n∈\n| |\ni\nS =d\nX\nS\n\nwhere g ∼N(0, Σ). The observation regarding the different ordering\n\nof the steps amounts to saying\nthat the eigenbasis of Σ is the optimal solution for\nE\nargmax\nmax\nvT\ni g\n.\nV ∈Rp×p\n\nS⊂[p]\nS|\nX\ni\n=d\nV T V =I\n\"\n\n∈\n|\nS\n\n#\n\n1.1.5\nA related open problem\nWe now show an interesting open problem posed by Mallat and Zeitouni at [MZ11]\n[MZ11]\nS. Mallat and O. Zeitouni. A conjecture concerning optimality of the karhunen-loeve basis\nin nonlinear reconstruction. Available online at arXiv:1109.0489 [math.PR], 2011.\nReference\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 1.2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/a9963e8f7bd9c10b4d48df8115f63116_MIT18_S096F15_Open1.2.pdf",
      "content": "1.2.1\nA related open problem\nOpen Problem 1.2 (Monotonicity of singular values [BKS13a]) Consider the setting above but\nwith p = n, then X ∈Rn×n is a matrix with iid N(0, 1) entries. Let\nσi\n√X\nn\n\n,\ndenote the i-th singular value4 of √1 X, and define\nn\nαR(n) := E\n\"\nX\nn\nσi\n√X\n,\nn\nn\ni=1\n#\nas the expected value of the average singular value of √1 X.\nn\nThe conjecture is that, for every n ≥1,\nαR(n + 1) ≥αR(n).\nMoreover, for the analogous quantity αC(n) defined over the complex numbers, meaning simply\nthat each entry of X is an iid complex valued standard gaussian CN(0, 1) the reverse inequality is\nconjectured for all n ≥1:\nαC(n + 1) ≤αC(n).\nNotice that the singular values of √1 X are simply the square roots of the eigenvalues of Sn,\nn\nσi\n√X\n\n=\nλ\nn\np\ni (Sn).\n4The i-th diagonal element of Σ in the SVD √1 X = UΣV .\nn\n\nThis means that we can compute αR in the limit (since we know the limiting distribution of λi (Sn))\nand get (since p = n we have γ = 1, γ\n= 0, and γ\n-\n+ = 2)\n(2\nλ) λ\nlim αR(n) =\nZ\nλ 2 dF1(λ) = 2π\nZ\nλ 2\n→inf\np\n-\n=\n0.8488.\nn\nλ\n3π ≈\nAlso, αR(1) simply corresponds to the expected value of the absolute value of a standard gaussian\ng\nαR(1) = E|g| =\nr\nπ ≈0.7990,\nwhich is compatible with the conjecture.\nOn the complex valued side, the Marchenko-Pastur distribution also holds for the complex valued\ncase and so limn\nαC(n) = lim\n→inf\nn\nα\n→inf\nR(n) and αC(1) can also be easily calculated and seen to be\nlarger than the limit.\nReference\n[BKS13a]\nA. S. Bandeira, C. Kennedy, and A. Singer. Approximating the little grothendieck problem\nover the orthogonal group. Available online at arXiv:1308.5207 [cs.DS], 2013.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 1.3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/f45f0a31d53a81f7354e4cfe79e6c4d0_MIT18_S096F15_Open1.3.pdf",
      "content": "Open Problem 1.3 (Spike Model for cut-SDP [MS15]. As since been solved [MS15]) Let\nW denote a symmetric Wigner matrix with i.i.d. entries Wij ∼N(0, 1). Also, given B ∈Rn×n sym-\nmetric, define:\nQ(B) = max {Tr(BX) : X ⪰0, Xii = 1} .\nDefine q(ξ) as\nξ\nq(ξ) = lim\nEQ\nn→infn\n\n11T +\nn\n√W\nn\n\n.\nWhat is the value of ξ , defined as\n∗\nξ = inf\n∗\n{ξ ≥0 : q(ξ) > 2}.\nIt is known that, if 0 ≤ξ ≤1, q(ξ) = 2 [MS15].\nOne can show that 1 Q(B)\nn\n≤λmax(B). In fact,\nmax {Tr(BX) : X ⪰0, Xii = 1} ≤max {Tr(BX) : X ⪰0, Tr X = n} .\nIt is also not difficult to show (hint: take the spectral decomposition of X) that\n(\nX\nn\nmax\nTr(BX) : X ⪰0,\nXii = n\ni=1\n)\n= λmax(B).\nThis means that for ξ > 1, q(ξ) ≤ξ + 1.\nξ\n1.3.2\nAn open problem about spike models\n[MS15]\nA. Montanari and S. Sen. Semidefinite programs on sparse random graphs. Available\nonline at arXiv:1504.05910 [cs.DM], 2015.\nReference\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 2.1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/ccf9cc9cb3bb8fba9ead00464c5dbace_MIT18_S096F15_Open2.1.pdf",
      "content": "Open Problem 2.1 Recall the definition of R(r) above, the following questions are open:\n- What is the value of R(5)?\n- What are the asymptotics\nthe lower bound (\n√\nof R(s)? In particular, improve on the base of the exponent on either\n2) or the upper bound (4).\n- Construct a family of graphs G = (V, E) with increasing number of vertices for which there exists\nε > 0 such that9\n|V | ≲(1 + ε)r.\nIt is known that 43 ≤R(5) ≤49. There is a famous quote in Joel Spencer's book [Spe94] that\nconveys the difficulty of computing Ramsey numbers:\n\"Erd os asks us to imagine an alien force, vastly more powerful than us, landing on Earth and\ndemanding the value of R(5) or they will destroy our planet.\nIn that case, he claims, we should\nmarshal all our computers and all our mathematicians and attempt to find the value. But suppose,\ninstead, that they ask for R(6). In that case, he believes, we should attempt to destroy the aliens.\"\nThere is an alternative useful way to think about 22, by taking log2 of each bound and rearranging,\nwe get that\n1 + o(1)\n\nlog2 n ≤\nmin\nr(G)\n(2 + o(1)) log2 n\nG=(V,E), |V |=n\n≤\nThe current \"world record\" (see [CZ15, Coh15]) for deterministic construction of families of graphs with\nc\nsmall r(G) achieves r(G) ≲2(log log |V |) , for some constant c > 0. Note that this is still considerably\nlarger than polylog|V |. In contrast, it is very easy for randomized constructions to satisfy r(G) ≤\n2 log2 n, as made precise by the folloing theorem.\n9By ak ≲bk we mean that there exists a constant c such that ak ≤c bk.\nReferences\n[CZ15]\nE. Chattopadhyay and D. Zuckerman. Explicit two-source extractors and resilient func-\ntions. Electronic Colloquium on Computational Complexity, 2015.\n[Coh15]\nG. Cohen.\nTwo-source dispersers for polylogarithmic entropy and improved ramsey\ngraphs. Electronic Colloquium on Computational Complexity, 2015.\n[Spe94]\nJ. Spencer. Ten Lectures on the Probabilistic Method: Second Edition. SIAM, 1994.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 2.2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/489bd825a7ba8e2a2161c9cf9259a567_MIT18_S096F15_Open2.2.pdf",
      "content": "Open Problem 2.2 (Erd os-Hajnal Conjecture [EH89]) Prove or disprove the following:\nFor any finite graph H, there exists a constant δH > 0 such that any graph on n nodes that does\nnot contain H as a subgraph (is a H-free graph) must have\nr(G) ≳nδH.\nIt is known that r(G) ≳exp\ncH\n√log n , for some constant cH > 0 (see [Chu13] for a survey\non this conjecture).\nNote that this lower\n\nbound already shows that H-free graphs need to have\nconsiderably larger r(G). This is an amazing local to global effect, where imposing a constraint on\nsmall groups of vertices are connected (being H-free is a local property) creates extremely large cliques\nor independence sets (much larger than polylog(n) as in random Erd os-Reny ı graphs).\nSince we do not know how to deterministically construct graphs with r(G)\n≤polylogn, one ap-\nproach could be to take G ∼G n,\nand check that indeed it has small clique and independence\nnumber. However, finding the largest\n\nclique on a graph is known to be NP-hard (meaning that there\nis no polynomial time algorithm to solve it, provided that the widely believed conjecture NP = P\nholds). That is a worst-case statement and thus it doesn't necessarily mean that it is difficult to find\nthe clique number of random graphs. That being said, the next open problem suggests that this is\nindeed still difficult.\nFirst let us describe a useful construct.\nGiven n and ω, let us consider a random graph G that\nconsists of taking a graph drawn from G n, 1 , picking ω of its nodes (say at random) and adding an\nedge between every pair of those ω nodes, thus \"planting\" a clique of size ω. This will create a clique\nof size ω in G. If ω > 2 log2 n this clique is larger\n\nthan any other clique that was in the graph before\nplanting. This means that, if ω > 2 log2 n, there is enough information in the graph to find the planted\nclique. In fact, one can simply look at all subsets of size 2 log2 n + 1 and check wether it is clique: if\nit is a clique then it very likely these vertices belong to the planted clique. However, checking all such\nsubgraphs takes super-polynomial time ∼nO(log n). This motivates the natural question of whether\nthis can be done in polynomial time.\nSince the degrees of the nodes of a G\nn, 1\n\nhave expected value n-1\nand standard deviation ∼√n,\nif ω > c√n (for sufficiently large constant c) then the degrees of the nodes involved in the planted\nclique will have larger degrees and it is easy to detect (and find) the planted clique. Remarkably,\nthere is no known method to work for ω significant smaller than this. There is a quasi-linear\nalgorithm\np\ntime\n[DM13] that finds the largest clique, with high probability, as long as ω ≥\nn\ne + o(√n).11\n\n11There is an amplification technique that allows one to find the largest clique for ω ≈c√n for arbitrarily small c in\npolynomial time, where the exponent in the runtime depends on c. The rough idea is to consider all subsets of a certain\nfinite size and checking whether the planted clique contains them.\n[EH89]\nP. Erdos and A. Hajnal. Ramsey-type theorems. Discrete Applied Mathematics, 25, 1989.\n[DM13]\nY. Deshpande and A. Montanari. Finding hidden cliques of size\np\nN/e in nearly linear\ntime. Available online at arXiv:1304.7047 [math.PR], 2013.\nReferences\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 2.3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/94a647cf712ddbc10f881e7bc106e58b_MIT18_S096F15_Open2.3.pdf",
      "content": "Open Problem 2.3 (The planted clique problem) Let G be a random graph constructed by tak-\ning a G\nn, 1\nand\n\nplanting a clique of size ω.\n1. Is there a polynomial time algorithm that is able to find the largest clique of G (with high prob-\nability) for ω\n√\n√\n≪\nn. For example, for ω ≈\nn .\nlog n\n2. Is there a polynomial\nG\n\ntime algorithm that is able to distinguish, with high probability, G from a\nn\ndraw of\nn, 1\n√\nfor ω\n.\n√\n≪\nn. For example, for ω ≈log n\n3. Is there a quasi-linear time algorithm able to find the largest clique of G (with high probability)\nfor ω ≤\n√1\ne -ε √n, for some ε > 0.\nThis open\n\nproblem\n\nis particularly important. In fact, the hypothesis that finding planted cliques\nfor small values of ω is behind several cryptographic protocols, and hardness results in average case\ncomplexity (hardness for Sparse PCA being a great example [BR13]).\nReference\n[BR13]\nQ. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal com-\nponent detection. Conference on Learning Theory (COLT), 2013.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Topics in Mathematics of Data Science Open Question 3.1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/6403e71189c7739a756c80846d240463_MIT18_S096F15_Open3.1.pdf",
      "content": "Open Problem 3.1 Does there exists a constant c > 0 such that it is NP-hard to, given φ, and G\ndistinguis between the cases\n1. hG ≤φ, and\n2. hG ≥c√φ?\nIt turns out that this is a consequence [RST12] of an important conjecture in Theoretical Computer\nScience (see [BS14] for a nice description of it). This conjecture is known [RS10] to imply the Unique-\nGames Conjecture [Kho10], that we will discuss in future lectures.\nConjecture 3.10 (Small-Set Expansion Hypothesis [RS10]) For every ε > 0 there exists δ > 0\nsuch that it is NP-hard to distinguish between the cases\n1. There exists a subset S ⊂\ncut(S)\nV with vol(S) = δ vol(V ) such that vol(S) ≤ε,\n2.\ncut(S) ≥1 -ε, for every S ⊂V satisfying vol(S)\nvol(S)\n≤δ vol(V ).\nReferences\n[RST12]\nP. Raghavendra, D. Steurer, and M. Tulsiani. Reductions between expansion problems.\nIEEE CCC, 2012.\n[RS10]\nP. Raghavendra and D. Steurer.\nGraph expansion and the unique games conjecture.\nSTOC, 2010.\n[Kho10]\nS. Khot. On the unique games conjecture (invited survey). In Proceedings of the 2010\nIEEE 25th Annual Conference on Computational Complexity, CCC '10, pages 99-121,\nWashington, DC, USA, 2010. IEEE Computer Society.\n[BS14]\nB. Barak and D. Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.\nSurvey, ICM 2014, 2014.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.S096 Topics in Mathematics of Data Science\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}