{
  "course_name": "Performance Engineering of Software Systems",
  "course_description": "6.172 is an 18-unit class that provides a hands-on, project-based introduction to building scalable and high-performance software systems. Topics include performance analysis, algorithmic techniques for high performance, instruction-level optimizations, caching optimizations, parallel programming, and building scalable systems. The course programming language is C.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering",
    "Systems Engineering",
    "Systems Optimization",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering",
    "Systems Engineering",
    "Systems Optimization"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week; 1.5 hours / session\n\nPrerequisites\n\nThe prerequisites for this class are\n6.004 Computation Structures\n,\n6.006 Introduction to Algorithms\n, and 6.031 Software Construction (formerly\n6.005 Software Construction\n). The course programming language is C.\n\nCourse Description\n\nModern computing platforms provide unprecedented amounts of raw computational power. But with great power comes great complexity, to the point that making useful computations exploit even a fraction of the computing platform's potential becomes a substantial challenge. Indeed, obtaining good performance requires a comprehensive understanding of all layers of the underlying platform, deep insight into the computation at hand, and the ingenuity and creativity required to obtain an effective mapping of the computation onto the machine. The reward for mastering these sophisticated and challenging topics is the ability to make computations that can process large amounts of data orders of magnitude more quickly and efficiently and to obtain\n\nresults that are unavailable with standard practice.\n\n6.172 is an 18-unit class that provides a hands-on, project-based introduction to building scalable and high-performance software systems. Topics include performance analysis, algorithmic techniques for high performance, instruction-level optimizations, caching optimizations, parallel programming, and building scalable systems.\n\nTextbook\n\nReading materials will be posted on the class website.\n\nGrading\n\nEach assignment will describe how you will be graded. The scores you receive on each assignment will be combined to produce your final grade after being weighted approximately.\n\nAssignments\n\nPercentages\n\nWeekly Homeworks\n\n10%\n\nQuizzes\n\n30%\n\nProject 1\n\n10%\n\nProject 2\n\n12%\n\nProject 3\n\n12%\n\nProject 4\n\n24%\n\nClass Participation\n\n2%",
  "files": [
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Services, Homework 8: Cache-Oblivious Algorithms",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/5bc7ce0d33656fd3783abcdc3dc4eb92_MIT6_172F18hw8.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 13\nHomework 8: Cache-Oblivious Algorithms\nThen, answer the writeup questions in this handout and submit an individual writeup.\nSee the following paper for more information on cache-oblivious algorithms:\nhttps:// dl.acm.org/citation.cfm?id=2071383.\nFor this homework, assume that all matrices are stored in row-major layout.\n1 Cache Complexity of Matrix Multiplication\nDuring Lecture 14 we discussed the cache complexity of matrix multiplication of dimension n,\nwith tall cache assumption of size M and cache line size B. For the naive approach, there\nwere two cases: 1) If n > M/B, then Θ(n3) cache misses occur, and 2) if M1/2 < n < M/B, then\n3/BM1/2)\nΘ(n3/B) cache misses occur. For the blocking approach, with block size s < M1/2, Θ(n\ncache misses occur. The cache-oblivious approach achieves the same complexity as the blocking\napproach without the need of the voodoo parameter s.\nCheckoff Item 1: Assume we want to multiply two rectangular matrices: m × n with n × r.\nGiven the same tall cache assumption, please analyze the complexity for one of the\nfollowing four cases: the two cases for the naive approach (n > M/B and\nM/r < n < M/B), the block approach, and the cache-oblivious approach. You may pick\nwhichever case you want to analyze.\n2 Tableau Construction\nConsider the tableau-construction problem from Lecture 8. The problem involves filling an N × N\ntableau, where each entry of the tableau is calculated as a function of some of its neighbors. To\nbe specific, the equation to fill an element of the tableau would take the form\nA[i][j] = f (A[i - 1][j - 1], A[i][j - 1], A[i - 1][j])\nwhere f is an arbitrary function.\n\nHandout 13 -- Homework 8: Cache-Oblivious Algorithms\n2.1 Iterative Formulation\nConsider the code snippet in Figure 1 below.\n01 #define A(i, j) A[N + (i) - (j) - 1]\n03 void tableau(double *A, size_t N) {\nfor (size_t i = 1; i < N; i++) {\nfor (size_t j = 1; j < N; j++) {\nA(i, j) = f(A(i-1, j-1), A(i, j-1), A(i-1, j));\n}\n}\n09 }\nFigure 1: A simple, iterative loop for filling a tableau.\nIn this problem, we are only interested in computing the final value of the tableau, stored in\nA(N-1,N-1), and hence we really only need 2N - 1 amount of space during computation. Thus,\nthe algorithm declares A as an array of size 2N - 1.\nThe algorithm initializes the first row and first column of the tableau, and invokes the tableau\nfunction as shown in Figure 2.\n10 for (size_t i = 0; i < N; i++) {\nA(i, 0) = INIT_VAL;\n12 }\n13 for (size_t j = 0; j < N; j++) {\nA(0, j) = INIT_VAL;\n15 }\n16 tableau(A, N);\n17 res = A(N - 1, N - 1);\nFigure 2: Initializing and calling the iterative tableau function.\nWrite-up 1: Explain why 2N - 1 space is sufficient and how the tableau function utilizes\nthe 2N - 1 space.\nRecall the tall cache assumption, which states that B2 < αM, where B is the size of the cache\nline, M is the size of the cache, and α ≤ 1 is a constant.\n\nHandout 13 -- Homework 8: Cache-Oblivious Algorithms\nWrite-up 2: Assuming that an optimal replacement strategy holds and that the cache is tall,\ngive a tight upper bound on the cache complexity Q(n) for each of the following cases using\nO notation, where c ≤ 1 is a sufficiently small constant:\n1. n ≥ cM\n2. n < cM\n2.2 Recursive Formulation\nNow consider the code snippet for a recursive tableau implementation, as shown in Figure 3. This\n18 #define A(i, j) A[N + (i) - (j) - 1]\n20 void recursive_tableau(double *A, size_t rbegin, size_t rend, size_t cbegin,\nsize_t cend) {\nif (rend-rbegin == 1 && cend-cbegin == 1) {\nsize_t i = rbegin, j = cbegin;\nA(i, j) = f(A(i-1, j-1), A(i, j-1), A(i-1, j));\n} else {\nsize_t rmid = rend-rbegin > 1 ? (rbegin + (rend-rbegin) / 2) : rend;\nsize_t cmid = cend-cbegin > 1 ? (cbegin + (cend-cbegin) / 2) : cend;\nrecursive_tableau(A, rbegin, rmid, cbegin, cmid);\nif (cend > cmid)\nrecursive_tableau(A, rbegin, rmid, cmid, cend);\nif (rend > rmid)\nrecursive_tableau(A, rmid, rend, cbegin, cmid);\nif (rend > rmid && cend > cmid)\nrecursive_tableau(A, rmid, rend, cmid, cend);\n}\n36 }\nFigure 3: A recursive implementation for filling in a tableau.\nalgorithm similarly uses only 2N - 1 amount of space, initializes the array A, and invokes the\nrecursive_tableau function as shown in Figure 4. This recursive algorithm divides the tableau\ninto four quadrants to compute. As discussed in Lecture 8 (slide 88), after the first quadrant is\ndone computing, we can then compute the second and third quadrants in parallel. Parallelizing\n2-lg3).\nthis way gives us work as Θ(n2) and span as Θ(nlg3) with parallelism as Θ(n\nWe also\ndiscussed (slide 92) a more parallel construction that divides up the tableau 9 ways.\n\nHandout 13 -- Homework 8: Cache-Oblivious Algorithms\n37 for (size_t i = 0; i < N; i++) {\nA(i, 0) = INIT_VAL;\n39 }\n40 for (size_t j = 0; j < N; j++) {\nA(0, j) = INIT_VAL;\n42 }\n43 if (N > 1) {\nrecursive_tableau(A, 1, N, 1, N);\n45 }\n46 res = A(N-1, N-1);\nFigure 4: Initializing and calling the recursive_tableau function.\nWrite-up 3: Derive the general formula for work and span, assuming a k2-way tableau\nconstruction (i.e., the tableau is divided up into k2 pieces of size n/k × n/k).\nWrite-up 4: Answer the following questions assuming that an optimal replacement strategy\nholds and that the cache is tall.\n1. Show the recurrence relation for the cache complexity Q(n) using the 4-way\nconstruction of the recursive_tableau function.\n2. Draw the recursion tree and label the internal nodes and leaves with their cache\ncomplexity Q(n). What's the height of the recursion tree?\n3. How many leaves are in the recursion tree?\n4. Using the recursion tree and the recurrence relation, derive a simplified expression for\nQ(n).\nWrite-up 5: Answer the following question assuming that an optimal replacement strategy\nholds and that the cache is tall. Assuming a k2-way tableau construction, show that if we are\n\"unlucky,\" where a subpiece is just slightly above the cache size, then we have\nQ(n) = Θ(n2k/MB). Also show that if we are lucky and this situation does not arise, then\nwe have Q(n) = Θ(n2/MB).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 1: Getting Started",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/2724d8594cb413754669fc4e9c6ce7db_MIT6_172F18hw1.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 2\nHomework 1: Getting Started\nThis homework introduces the environment and tools you will be using to complete your future project\nassignments. It includes a quick C primer. You should use this assignment to familiarize yourself with the\ntools you will be using throughout the course.\n1 Software engineering\nBest practices\nA good software engineer strives to write programs that are fast, correct, and maintainable. Here\nare a few best practices which we feel are worth reminding you of:\n- Maintainability: comment your code, use meaningful variable names, insert whitespaces,\nand follow a consistent style.\n- Code organization: break up large functions into smaller subroutines, write reusable helper\nfunctions, and avoid duplicate code.\n- Version control: write descriptive commit messages, and commit your changes frequently\n(but don't commit anything which doesn't compile).\n- Assertions: frequently make assertions within your code so that you know quickly when\nsomething goes wrong.\nPair programming\nPair programming is a technique in which two programmers work on the same machine. Ac\ncording to Laurie Williams from North Carolina State University, \"One of the programmers, the\ndriver, has control of the keyboard/mouse and actively implements the program. The other\nprogrammer, the observer, continuously observes the work of the driver to identify tactical (syn\ntactic, spelling, etc.) defects, and also thinks strategically about the direction of the work.\" The\nprogrammers work equally to develop a piece of software as they periodically switch roles.\nYou will gain more experience with pair programming during Project 1.\n\n[Note: This course makes use of AWS and Git features which may not be available to all OCW users.]\n4 C Primer\nThis section will be a short introduction to the C programming language. The code used in the\nexercises is located in homework1/c-primer in your repository.\nWhy use C?\n- Simple: No complicated object-oriented abstractions like Java/C++.\n- Powerful: Offers direct access to memory (but does not offer protection in accessing mem\nory).\n- Fast: No overhead of a runtime or JIT compiler, and no behind-the-scenes runtime features\n(like garbage collection) that use machine resources.\n- Ubiquitous: C is the most popular language for low-level and performance-intensive soft\nware like device drivers, operating system kernels, and microcontrollers.\n\nPreprocessing\nThe C preprocessor modifies source code before it is passed to the compilation phase. Specifically,\nit performs string substitution of #define macros, conditionally omits sections of code, processes\n#include directives to import entire files' worth of code, and strips comments from code.\nAs an example, consider the code in preprocess.c, which is replicated in Figure 1.\n01 // All occurrences of ONE will be replaced by 1.\n02 #define ONE 1\n04 // Macros can also behave similar to inline functions.\n05 // Note that parentheses around parameters are required to preserve order of\n06 // operations. Otherwise, you can introduce bugs when substitution happens.\n07 #define MIN(a,b) ((a) < (b) ? (a) : (b))\n09 int c = ONE, d = ONE + 5;\n10 int e = MIN(c, d);\n12 #ifndef NDEBUG\n13 // This code will be compiled only when\n14 // the macro NDEBUG is not defined.\n15 // Recall that if clang is passed -DNDEBUG on the command line,\n16 // then NDEBUG will be defined.\n17 if (something) {}\n18 #endif\nFigure 1: A sample C program. If -DNDEBUG is not on, the preprocessor will omit the if statement\nin line 17.\nExercise: Direct clang to preprocess preprocess.c.\n$ clang -E preprocess.c\nThe preprocessed code will be output to the console. Now rerun the C preprocessor with the\nfollowing command:\n$ clang -E -DNDEBUG preprocess.c\nYou will notice that the if statement won't appear in the preprocessor output.\nData types and their sizes\nC supports a variety of primitive types, including the types listed in Figure 2.\nNote: On most 64-bit machines and compilers, a standard-precision value (e.g. int, float) is\n32 bits. A short is usually 16 bits, and a long or a double is usually 64. However, the precisions of\nthese types are weakly defined by the C standard, and may vary across compilers and machines.\nConfusingly, sometimes int and long are the same precision, and sometimes long and long long\nare the same, both longer than int. Sometimes, int, long, and long long all mean the exact same\nthing!\n\n19 short s;\n// short signed integer\n20 unsigned int i; // standard-length unsigned integer\n21 long l;\n// long signed integer\n22 long long l;\n// extra-long signed integer\n23 char c;\n// represents 1 ASCII character (1 byte)\n24 float f;\n// standard-precision floating point number\n25 double d;\n// double-precision floating point number\nFigure 2: Some of the primitive types in C.\nFor throwaway variables or variables which will stay well under precision limits, use a regular\nint. The precisions of these values are set in order to maximize performance on machines with\ndifferent word sizes. If you are working with bit-level manipulation, it is better to use unsigned\ndata types such as uint64_t (unsigned 64 bit int). Otherwise, it is often better to use a non-\nexplicit variable such as a regular int.\nFurthermore, if you know the architecture you're working with, it is often better to write code\nwith explicit data types instead (such as the ones in Figure 3).\n26 #include <stdint.h>\n28 uint64_t unsigned_64_bit_int;\n29 int16_t signed_16_bit_int;\nFigure 3: Examples of explicit types in C.\nYou can define more complex data types by composing primitive types into a struct. For\nexample, one example of a struct definition in C is provided in Figure 4.\n30 typedef struct {\nint id;\nint year;\n33 } student;\n35 student you;\n36 // access values on a struct with .\n37 you.id = 12345;\n38 you.year = 3;\nFigure 4: Examples of a struct declaration in C.\nExercise: Edit sizes.c to print the sizes of each of the following types: int, short, long, char,\nfloat, double, unsigned int, long long, uint8_t, uint16_t, uint32_t, uint64_t, uint_fast8_t,\nuint_fast16_t, uintmax_t, intmax_t, __int128, int[] and student. Note that __int128 is a clang\n\nC extension, and not part of standard C. To check the size of an int array, print the size of the\narray x declared in the provided code.\nTo compile and run this code, use the following command:\n$ make sizes && ./sizes\nTo avoid creating repetitive code, you may find it useful to define a macro and call it for each of\nthe types.\nIf you are interested in learning more about built-in types, check out http://en.cppreference.com/\nw/c/types/integer .\nPointers\nPointers are first-class data types that store addresses into memory. A pointer can store the\naddress of anything in memory, including another pointer. In other words, it is possible to have\na pointer to a pointer.\nArrays behave very similarly to pointers: both hold information about the type and loca\ntion of values in memory. There are a few gotchas involved with treating pointers and arrays\nequivalently, however.1\nConsider the following (buggy) snippet of code from pointer.c in Figure 5.\n1For further\nreading\non\nthis,\ntry\nout\nthe\nchallenge\nat https://blogs.oracle.com/ksplice/entry/\nthe_ksplice_pointer_challenge after class.\n\nint main(int argc, char* argv[]) {\n// What is the type of argv?\nint i = 5;\n// The & operator here gets the address of i and stores it into pi\nint* pi = &i;\n// The * operator here dereferences pi and stores the value -- 5 -\n// into j.\nint j = *pi;\nchar c[] = \"6.172\";\nchar* pc = c;\n// Valid assignment: c acts like a pointer to c[0] here.\nchar d = *pc;\nprintf(\"char d = %c\\n\", d); // What does this print?\n// compound types are read right to left in C.\n// pcp is a pointer to a pointer to a char, meaning that\n// pcp stores the address of a char pointer.\nchar** pcp;\npcp = argv; // Why is this assignment valid?\nconst char* pcc = c;\n// pcc is a pointer to char constant\nchar const* pcc2 = c;\n// What is the type of pcc2?\n// For each of the following, why is the assignment:\n*pcc = '7'; // invalid?\npcc = *pcp; // valid?\npcc = argv[0]; // valid?\nchar* const cp = c;\n// cp is a const pointer to char\n// For each of the following, why is the assignment:\ncp = *pcp; // invalid?\ncp = *argv; // invalid?\n*cp = '!'; // valid?\nconst char* const cpc = c;\n// cpc is a const pointer to char const\n// For each of the following, why is the assignment:\ncpc = *pcp; // invalid?\ncpc = argv[0]; // invalid?\n*cpc = '@'; // invalid?\nreturn 0;\n}\nFigure 5: An example of valid and invalid pointer usage in C.\n\nExercise: Compile pointer.c using the following command:\n$ make pointer\nYou will see compilation errors corresponding to the invalid statements mentioned in the above\nprogram. Why are these statements invalid? Comment out those invalid statements and recom\npile the program. (Do not worry if you see additional warnings about unused variables.)\nWrite-up 2: Answer the questions in the comments in pointer.c. For example, why are\nsome of the statements valid and some are not?\nWrite-up 3: For each of the types in the sizes.c exercise above, print the size of a pointer to\nthat type. Recall that obtaining the address of an array or struct requires the & operator.\nProvide the output of your program (which should include the sizes of both the actual type\nand a pointer to it) in the writeup.\nArgument passing\nIn C, arguments2 to a function are passed by value. That means that if you pass an integer to\nfunction foo(int f), a new variable f will be initialized inside foo with the same value as the\ninteger you passed in.\nFor instance, consider the code in Figure 6 that swaps two integers. Why doesn't it work as\nexpected?\n80 void swap(int i, int j) {\nint temp = i;\ni = j;\nj = temp;\n84 }\n86 int main() {\nint k = 1;\nint m = 2;\nswap(k, m);\n// What does this print?\nprintf(\"k = %d, m = %d\\n\", k, m);\n92 }\nFigure 6: An incorrect implementation of swap in C.\n2In general, parameters are the variables that appear in a function definition, and arguments are the data that are\nactually passed in at runtime.\n\nThere are two ways to fix this code. One way is to change swap() to be a macro, causing the\noperations to be evaluated in the scope of the macro invocation. Another way is to change swap()\nto use pointers. We will now ask you to fix the code by using pointers.\nWrite-up 4: File swap.c contains the code to swap two integers. Rewrite the swap() function\nusing pointers and make appropriate changes in main() function so that the values are\nswapped with a call to swap(). Compile the code with make swap and run the program with\n./swap. Provide your edited code in the writeup. Verify that the results of both sizes.c and\nswap.c are correct by using the python script verifier.py.\n5 Basic tools\nThe code that we will be using in this section is located in homework1/matrix-multiply.\nBuilding and running your code\nYou can build the code by going to the homework1/matrix-multiply directory and typing make.\nThe program will compile using Tapir, a cutting-edge derivative of Clang/LLVM. Notice that we\nare only compiling with optimization level 1 (i.e., -O1).\nExercise: Modify your Makefile so that the program is compiled using optimization level 3 (i.e.,\n-O3).\nWrite-up 5: Now, what do you see when you type make clean; make?\nYou can then run the built binary by typing ./matrix_multiply. The program should print\nout something and then crash with a segmentation fault.\nUsing a debugger\nWhile running your program, if you encounter a segmentation fault, bus error, or assertion\nfailure, or if you just want to set a breakpoint, you can use the debugging tool GDB.\nExercise: Start a debugging session in GDB:\n$ gdb --args ./matrix_multiply\nThis command should give you a GDB prompt, at which you should type run or r:\n$ (gdb) run\n\nYour program will crash, giving you back a prompt, where you can type backtrace or bt to get\na stack trace:\n93 Program received signal SIGSEGV, Segmentation fault.\n94 0x0000000000400de4 in matrix_multiply_run ()\n95 (gdb) bt\n96 #0 0x0000000000400de4 in matrix_multiply_run ()\n97 #1 0x0000000000400bbe in main ()\nThis stack trace says that the program crashes in matrix_multiply_run, but doesn't tell any\nother information about the error. In order to get more information, build a \"debug\" version of\nthe code. First, quit GDB by typing quit or q:\n98 (gdb) q\n99 A debugging session is active.\nInferior 1 [process 26817] will be killed.\n103 Quit anyway? (y or n) y\nNext, build a \"debug\" version of the code by typing make DEBUG=1:\n104 $ make DEBUG=1\n105 clang -g -DDEBUG -O0 -Wall -std=c99 -D_POSIX_C_SOURCE=200809L\n-c -o testbed.o testbed.c\n106 clang -g -DDEBUG -O0 -Wall -std=c99 -D_POSIX_C_SOURCE=200809L\n-c -o matrix_multiply.o \\\n107 matrix_multiply.c\n108 clang -o matrix_multiply testbed.o matrix_multiply.o -lrt -flto -fuse-ld=gold\nThe major differences from the optimized build are '-g' (add debug symbols to your program)\nand '-O0' (compile without any optimizations). Once you have created a debug build, you can\nstart a debugging session again:\n109 $ gdb --args ./matrix_multiply\n110 (gdb) r\n111 ...\n112 Program received signal SIGSEGV, Segmentation fault.\n113 0x00000000004011cf in matrix_multiply_run (A=0x603270, B=0x6031d0, C=0x603130)\nat matrix_multiply.c:90\n115 90\nC->values[i][j] += A->values[i][k] * B->values[k][j];\nNow, GDB can tell that a segmentation fault occurs at at matrix_multiply.c line 90. You can\nask GDB to print values using print or p:\n\n116 gdb) p A->values[i][k]\n117 $1 = 7\n118 (gdb) p B->values[k][j]\n119 Cannot access memory at address 0x0\n120 (gdb) p B->values[k]\n121 $2 = (int *) 0x0\n122 (gdb) p k\n123 $3 = 4\nThis suggests that B->values[4] is 0x0, which means B doesn't have row 5. There is something\nwrong with the matrix dimensions.\nUsing assertions\nThe tbassert package is a useful tool for catching bugs before your program goes off into the\nweeds. If you look at matrix_multiply.c, you should see some assertions in matrix_multiply_run\nroutine that check that the matrices have compatible dimensions.\nExercise: Uncomment these lines and a line to include tbassert.h at the top of the file. Then,\nbuild and run the program again using GDB. Make sure that you build using make DEBUG=1. You\nshould see:\n124 (gdb) r\n125 ...\n126 Running matrix_multiply_run()...\n127 matrix_multiply.c:80 (int matrix_multiply_run(const matrix *, const matrix *, matrix *))\nAssertion A->cols == B->rows failed: A->cols = 5, B->rows = 4\n130 Program received signal SIGABRT, Aborted.\n131 0x00007ffff7843c37 in raise () from /lib/x86_64-linux-gnu/libc.so.6\nNow, GDB tells that \"Assertion 'A->cols == B->rows' failed\", which is much better than\nthe former segmentation fault. The assertion provides a printf-like API that allows you to print\nvalues in your own output, as above. However, even if you don't print values in your assertions,\nthe debug build still has the symbols for GDB, as above. Unlike the above, however, if you try to\nprint A->cols, you will fail. The reason is that GDB is not in the stack frame you want. You can\nget the stack trace to see which frame you want (#3 in this case), and type frame 3 or f 3 to move\nto frame #3. After that, you can print A->cols and B->cols.\n\n132 (gdb) bt\n133 #0 0x00007ffff7843c37 in raise () from /lib/x86_64-linux-gnu/libc.so.6\n134 #1 0x00007ffff7847028 in abort () from /lib/x86_64-linux-gnu/libc.so.6\n135 #2 0x000000000040121b in matrix_multiply_run (A=0x603270, B=0x6031d0, C=0x603130)\nat matrix_multiply.c:79\n137 #3 0x0000000000400db2 in main (argc=1, argv=0x7fffffffdf58) at testbed.c:127\n138 (gdb) f 3\n139 #3 0x0000000000400db2 in main (argc=1, argv=0x7fffffffdf58) at testbed.c:127\n140 127\nmatrix_multiply_run(A, B, C);\n141 (gdb) p A->cols\n142 $1 = 5\n143 (gdb) p B->rows\n144 $2 = 4\nYou should see the values 5 and 4, which indicates that we are multiplying matrices of in\ncompatible dimensions.\nYou will also see an assertion failure with a line number for the failing assertion without\nusing GDB. Since the extra checks performed by assertions can be expensive, they are disabled\nfor optimized builds, which are the default in our Makefile. As a result, if you make the program\nwithout DEBUG=1, you will not see an assertion failure.\nYou should consider sprinkling assertions throughout your code to check important invari\nants in your program, since they will make your life easier when debugging. In particular,\nmost nontrivial loops and recursive functions should have an assertion of the loop or recursion\ninvariant.\nExercise: Fix testbed.c, which creates the matrices, rebuild your program, and verify that it now\nworks. You should see \"Elapsed execution time...\" after running\n$ ./matrix_multiply\nCommit and push your changes to the Git repository:\n$ git commit -am 'Your commit message'\n$ git push origin master\nNext, check the result of the multiplication. Run\n$ ./matrix_multiply -p\nThe program will print out the result. The result seems to be wrong, however. You can check the\nmultiplication of zero matrices by running\n$ ./matrix_multiply -pz\nUsing a memory checker\nSome memory bugs do not crash the program, so GDB cannot tell you where the bug is. You can\nuse the memory checking tools AddressSanitizer and Valgrind to track these bugs.\n\nAddressSanitizer\nAddressSanitizer is a quick memory error checker that uses compiler instrumentation and a\nrun-time library. It can detect a wide variety of bugs (including memory leaks).\nTo use AddressSanitizer, we need to pass the appropriate flags. First, do\n$ make clean\nto get rid of the existing build.\nNext, do\n$ make ASAN=1\nto build with AddressSanitizer's instrumentation.\n145 $ make ASAN=1\n146 clang -O1 -g -fsanitize=address -Wall -std=c99 -D_POSIX_C_SOURCE=200809L -c \\\n147 testbed.c -o testbed.o\n148 clang -O1 -g -fsanitize=address -Wall -std=c99 -D_POSIX_C_SOURCE=200809L -c \\\nmatrix_multiply.c -o matrix_multiply.o\n150 clang -o matrix_multiply testbed.o matrix_multiply.o -lrt -flto -fuse-ld=gold \\\n151 -fsanitize=address\nFinally, run the program with\n$ ./matrix_multiply\nWrite-up 6: What output do you see from AddressSanitizer regarding the memory bug?\nPaste it into your writeup here.\nValgrind\nValgrind is another tool for checking memory leaks. If you want to check a program but are not\nable to instrument it, Valgrind is a good option for detecting memory bugs.\nExercise: First, do\n$ make clean && make\nto get rid of the existing build and get a fresh build. Run Valgrind using\n$ valgrind ./matrix_multiply -p\nYou need the -p switch, since Valgrind only detects memory bugs that affect outputs. You should\nalso use a \"debug\" version to get a good result. This command should print out many lines. The\nimportant ones are\n\n152 ==43644== Use of uninitialised value of size 8\n153 ==43644==\nat 0x508899B: _itoa_word (_itoa.c:179)\n154 ==43644==\nby 0x508C636: vfprintf (vfprintf.c:1660)\n155 ==43644==\nby 0x50933D8: printf (printf.c:33)\n156 ==43644==\nby 0x401137: print_matrix (matrix_multiply.c:68)\n157 ==43644==\nby 0x400E0E: main (testbed.c:133)\nThis output indicates that the program used a value before initializing it. The stack trace\nindicates that the bug occurs in testbed.c:133, which is where the program prints out matrix C.\nExercise: Fix matrix_multiply.c to initialize values in matrices before using them. Keep in mind\nthat the matrices are stored in structs. Rebuild your program, and verify that it outputs a correct\nanswer. Again, commit and push your changes to the Git repository.\nWrite-up 7: After you fix your program, run ./matrix_multiply -p. Paste the program\noutput showing that the matrix multiplication is working correctly.\nMemory management\nThe C programming language requires you to free memory after you are done using it, or else\nyou will have a memory leak. Valgrind can track memory leaks in the program. Run the same\nValgrind command, and you will see these lines at the very end:\n158 ==2158== LEAK SUMMARY:\n159 ==2158==\ndefinitely lost: 48 bytes in 3 blocks\n160 ==2158==\nindirectly lost: 288 bytes in 15 blocks\n161 ==2158==\npossibly lost: 0 bytes in 0 blocks\n162 ==2158==\nstill reachable: 0 bytes in 0 blocks\n163 ==2158==\nsuppressed: 0 bytes in 0 blocks\nThis output suggests that there are indeed memory leaks in the program. To get more in\nformation, you can build your program in debug mode and again run Valgrind, using the flag\n--leak-check=full\n$ valgrind --leak-check=full ./matrix_multiply -p\nThe trace shows that all leaks are from the creations of matrices A, B, and C.\nExercise: Fix testbed.c by freeing these matrices after use with the function free_matrix. Re\nbuild your program, and verify that Valgrind doesn't complain about anything. Commit and\npush your changes to the Git repository.\n\nWrite-up 8: Paste the output from Valgrind showing that there is no error in your program.\nChecking code coverage\nBugs may exist in code that doesn't get executed in your tests. You may find it surprising when\nsomeone testing your code (like a professor or a TA) uncovers a crash on a line that you never\nexercised. Additionally, lines that are frequently executed are good candidates for optimization.\nThe Gcov tool provides a line-by-line execution count for your program.\nExercise: To use Gcov, modify your Makefile and add the flags -fprofile-arcs and -ftest-coverage\nto the CFLAGS and LDFLAGS variables. You will have to rebuild from scratch using make clean\nfollowed by make DEBUG=1. Try running your code normally with ./matrix_multiply -p. Note\nthat a number of new .gcda and .gcno files were created during your execution.\nNow use the llvm-cov commandline utility on testbed.c:\n$ llvm-cov gcov testbed.c\nA new file, testbed.c.gcov was created that is identical to the original testbed.c, except that it\nhas the number of times each line was executed in the code. In that file, you will see:\n164 1:\n99:\n165 if (use_zero_matrix) {\n166 #####: 100:\nfor (int i = 0; i < A->rows; i++) {\n167 #####: 101:\nfor (int j = 0; j < A->cols; j++) {\n168 #####: 102:\nA->values[i][j] = 0;\n169 #####: 103:\n}\n170 #####: 104:\n}\n171 #####: 105:\nfor (int i = 0; i < B->rows; i++) {\n172 #####: 106:\nfor (int j = 0; j < B->cols; j++) {\n173 #####: 107:\nB->values[i][j] = 0;\n174 #####: 108:\n}\n175 #####: 109:\n}\nThe hash-marks indicate lines that were never executed. In general, it is unusual to run a\ncode-coverage utility on a testbed, but a set of untested lines in your core code could lead to\nunexpected results when executed by someone else.\nAnother handy use of Gcov is identifying which lines got executed the most frequently. Code\nthat gets run the most is often the most costly in terms of performance. Run llvm-cov gcov\nmatrix_multiply.c and look at the output:\n\n10:\n93: for (int i = 0; i < A->rows; i++) {\n40:\n94:\nfor (int j = 0; j < B->cols; j++) {\n178 160:\n95:\nfor (int k = 0; k < A->cols; k++) {\n64:\n96:\nC->values[i][j] += A->values[i][k] * B->values[k][j];\n64:\n97:\n}\n16:\n98:\n}\n4:\n99:\n}\nThese are the loops in matrix_multiply_run. Clearly, this function is a good candidate for\noptimization.\nWhen you are done using Gcov, remove the flags you added to the Makefile because they add\ncostly overhead to the execution, and will negatively impact your actual performance numbers.\nYou should never run benchmarks on code that is instrumented with Gcov. Don't forget to\nmake clean to remove the instrumented object files.\n6 Using AWSRUN\nYou share the athena.dialup.mit.edu machines with all of MIT. Do not perform computationally\nintensive tasks on these machines. Directly running and timing the execution on your own\nAmazon VMs may give inaccurate results; you may be running a small VM, and there might also\nbe measurement errors due to interference with your editor or other programs you are running.\nTo get an accurate timing measure on a dedicated machine, you can use the AWSRUN utilities.\nWhen you are ready to do a performance run you can use\n$ awsrun ./matrix_multiply\nand your job will be queued. The command will not return until the job has been completed and\nyou get results unless you control-C twice (canceling the process). Once the job is complete, your\noutput should look something like below:\n183 $ awsrun ./matrix_multiply\n185 Submitting Job: ./matrix_multiply\n186 Waiting for job to finish...\n187 ==== Standard Error ====\n188 Setup\n189 Running matrix_multiply_run()...\n193 ==== Standard Output ====\n194 Elapsed execution time: 0.000001 sec\n\nPerformance enhancements\nTo get an idea of the extent to which performance optimizations can affect the performance of\nyour program, we will first increase the size of the input to demonstrate the effects of changes in\nthe code.\nExercise: Increase the size of all matrices to 1000 × 1000.\nNow let's try one of the techniques from the first lecture. Right now, the inner loop produces\na sequential access pattern on A and skips through memory on B.\nLet's rearrange the loops to produce a better access pattern.\nExercise: First, you should run the program as is to get a performance measurement. Next, swap\nthe j and k loops, so that the inner loop strides sequentially through the rows of the C and B\nmatrices. Rerun the program, and verify that you have produced a speedup. Commit and push\nyour changes to the Git repository.\nWrite-up 9: Report the execution time of your programs before and after the optimization.\nCompiler optimizations\nTo get an idea of the extent to which compiler optimizations can affect the performance of your\nprogram, rebuild your program in \"debug\" mode and run it with AWSRUN.\nExercise: Rebuild it again with optimizations (just type make), and run it with AWSRUN. Both\nversions should print timing information, and you should verify that the optimized version is\nfaster.\nWrite-up 10: Report the execution time of your programs compiled in debug mode with -O0\nand in non-debug mode with -O3.\n7 C style guidelines\nCode that adheres to a consistent style is easier to read and debug. Google provides a style guide\nfor C++ which you may find useful: https://google.github.io/styleguide/cppguide.html\nWe have provided a Python script clint.py, which is designed to check a subset of Google's\nstyle guidelines for C code. To run this script on all source files in your current directory use the\ncommand:\n$ python clint.py *\n\nThe code the staff provides has no style errors. We suggest, but do not require, that you use\nthis tool to clean up your source code. Part of your code-quality grade on projects is based on the\nreadability of your code. It can be difficult to maintain a consistent style when multiple people are\nworking on the same codebase. For this reason, you may find it useful to use the style checkers\nprovided by the 6.172 staff during your group projects to keep your code readable.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 10: Data Synchronization",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/64f4db5020ca72ab205e458d40bbdfe3_MIT6_172F18hw10.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 16\nHomework 10: Data Synchronization\nIntroduction\nThe focus of this problem set is the theoretical side of the material taught in class. This prob\nlem set focuses on data synchronization and comparing lock-based and lock-free FIFO queue\nimplementations.\n1 Data synchronization\nFigures 1 and 2 present two implementations of a FIFO queue. Figure 1 is a lock-based imple\nmentation, and Figure 2 is a lock-free implementation. In both queue implementations, a pool\nof nodes is allocated in advance. A call to new_node() grabs a free node from the pool of nodes,\nand free_node(node) returns the node node to the pool. In the implementation of the lock-free\nqueue, the compare-and-swap instruction CAS(addr, old_val, new_val) is an atomic instruction\nthat has the following effect:\nif (*addr == old_val) {\n*addr = new_val;\nreturn true;\n}\nreturn false;\nFor the questions below, assume that CAS can operate on the entire pointer_t, that the compiler\ncannot change the order of instructions, and that there are always enough free nodes in the pool\nto perform all enqueue operations. Assume also that the nodes in the queue do not cross cache\nlines, and thus all writes are atomic.\nRead both implementations carefully. Before you start answering the questions, you may\nfind it helpful to draw diagrams of an empty queue and a queue with a few nodes. Using\nthese diagrams, try to understand how nodes are inserted and deleted from the queue in both\nimplementations.\nNote that the first node added in initialization of both the lock-based and lock-free version of\nthe deque is a dummy value and never dequeued. It is only used to denote an empty deque.\n2 Check-off Questions\n1. What are constraints on enqueue and dequeue in the FIFO queue? You do not need to look\nat the code yet.\n\nHandout 16 -- Homework 10: Data Synchronization\n2. What is the advantage of using two locks over one lock?\n3. In the style of comments of the lock-based FIFO queue code, add comments to the lock-free\ncode (on paper), explaining what each line does. The comments should be short and precise\n(not more than 10 words each). We have provided you a copy of the code in Figure 2.\n4. Explain how a new node is inserted into the lock-free queue. How many successful CASes\nare needed per node? What happens if the CAS in line 96 fails? How far can the tail lag\nbehind? Is the program correct without line 96?\n5. Carefully look at the code for the lock-free dequeue operation and answer the following\nquestions:\n(a) Line 104 checks what was already assigned in line 101. Why do we need line line 104?\n(b) In line 111 the value of the node is read before the head is updated in line 112. Why\nis this important? What can happen if we change the order of the lines?\n(c) What happens if the CAS in line 112 is unsuccessful?\n6. Which implementation do you expect to run faster -- the lock-based or the lock-free? Ex\nplain your answer in terms of cost of the synchronization primitives, contention, synchro\nnization overhead, etc.\n7. Show how to simplify the lock-based code if only one thread may enqueue nodes to the\nqueue. Write the pseudocode and comment it. Explain in your own words why your\nsolution is correct (i.e. any execution sequence keeps the FIFO ordering).\n8. Show how to simplify the lock-free code if only one thread may dequeue nodes from the\nqueue. Write the pseudocode and comment it. Explain in your own words why your\nsolution is correct (i.e. any execution sequence keeps the FIFO ordering) and why it is\nnon-blocking.\n9. Explain how count is used to handle the ABA problem discussed in recitation.\n\nHandout 16 -- Homework 10: Data Synchronization\nstruct node_t {\ndata_t value;\nnode_t* next;\n};\nstruct queue_t {\nnode_t* head;\nnode_t* tail;\nmutex_t h_lock;\nmutex_t t_lock;\n};\nvoid initialize(queue_t* q, data_t value) {\nnode_t* node = new_node(); // Allocate a new node\nnode->value = value;\nnode->next = NULL;\n// Make it the only node in the queue\nq->head = node;\n// Both head and tail point to it\nq->tail = node;\nq->h_lock = FREE;\n// Locks are initially free\nq->t_lock = FREE;\n}\nvoid enqueue(queue_t* q, data_t value) {\nnode_t* node = new_node(); // Allocate a new node\nnode->value = value;\n// Copy enqueued value into node\nnode->next = NULL;\n// Set next pointer of node to NULL\nlock(&q->t_lock);\n// Acquire t_lock to access tail\nq->tail->next = node;\n// Append node at the end of queue\nq->tail = node;\n// Swing tail to node\nunlock(&q->t_lock);\n// Release t_lock\n}\nbool dequeue(queue_t* q, data_t* pvalue) {\nlock(&q->h_lock);\n// Acquire h_lock to access head\nnode_t* node = q->head;\n// Read head\nnew_head = node->next;\n// Read next pointer\nif (new_head == NULL) {\n// Is queue empty?\nunlock(&q->h_lock);\n// Release h_lock before return\nreturn false;\n// Queue was empty\n}\n*pvalue = new_head->value; // Queue not empty. Read value\nq->head = new_head;\n// Swing head to next node\nunlock(&q->h_lock);\n// Release h_lock\nfree_node(node);\n// Free node\nreturn true;\n// Dequeue succeeded\n}\nFigure 1: C-like pseudocode declaring, initializing, and adding for lock-based FIFO queue.\n\nHandout 16 -- Homework 10: Data Synchronization\nstruct pointer_t {\nnode_t* ptr;\nunsigned int count;\n};\nstruct node_t {\ndata_t value;\npointer_t next;\n};\nstruct queue_t {\npointer_t head;\npointer_t tail;\n};\nvoid initialize(queue_t* q, data_t value) {\nnode_t* node = new_node();\nnode->value = value;\nnode->next.ptr = NULL;\nq->head.ptr = node;\nq->tail.ptr = node;\n}\nvoid enqueue(queue_t* q, data_t value) {\nnode_t* node = new_node();\nnode->value = value;\nnode->next.ptr = NULL;\npointer_t tail;\nwhile (true) {\ntail = q->tail;\npointer_t next = tail.ptr->next;\nif (tail == q->tail) {\nif (next.ptr == NULL) {\nif (CAS(&tail.ptr->next, next, (struct pointer_t) {node, next.count + 1 })) {\nbreak;\n}\n} else {\nCAS(&q->tail, tail, (struct pointer_t) {next.ptr, tail.count + 1 });\n}\n}\n}\nCAS(&q->tail, tail, (struct pointer_t) { node, tail.count + 1 });\n}\nFigure 2: C-like pseudocode for declaring, initializing, and adding to a lock-free FIFO queue.\n\nHandout 16 -- Homework 10: Data Synchronization\nbool dequeue(queue_t* q, data_t* pvalue) {\npointer_t head;\nwhile (true) {\nhead = q->head;\npointer_t tail = q->tail;\npointer_t next = head.ptr->next;\nif (head == q->head) {\nif (head.ptr == tail.ptr) {\nif (next.ptr == NULL) {\nreturn false;\n}\nCAS(&q->tail, tail, (struct pointer_t) { next.ptr, tail.count + 1});\n} else {\n*pvalue = next.ptr->value;\nif (CAS(&q->head, head, (struct pointer_t) { next.ptr, head.count + 1})) {\nbreak;\n}\n}\n}\n}\nfree_node(head.ptr);\nreturn true;\n}\nFigure 3: C-like pseudocode for dequeueing in a lock-free FIFO queue.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 2: Profiling Serial Merge Sort",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/796439e646c02f44348d50b1836ff7f9_MIT6_172F18hw2.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 3\nHomework 2: Profiling Serial Merge Sort\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\nIn this homework you will be introduced to important profiling tools. You'll learn to use Perf, which\ngives you details about where time is being spent in your program, and Cachegrind, a cache and branch-\nprediction profiler in the Valgrind tool suite. These tools will help you identify parts of your code that would\nbenefit from further optimization. Finally, you'll use these tools to optimize a serial merge sort routine.\nGenerally, when you are concerned about the performance of a program, the best approach is to imple\nment something correct and then evaluate it. In some cases, many parts of this initial implementation (or\neven the entire thing) may be adequate for your needs. However, when you need to improve performance,\nyou must first decide where to focus your efforts. This is where profiling becomes useful. Profiling can help\nyou identify the performance bottlenecks in your program.\n1 Getting Started\nCode structure\nThe code for the recitation is in the recitation directory. You'll be looking at the files isort.c,\nwhich contains code for an insertion sorting routine, and sum.c, which allocates a large array of\nintegers and sums a sample of them. For the homework, the program you are responsible for\nimproving is in the homework directory.\nBuilding the code\nIn general, make <target> will build the code for a specific target binary. Remember that to build\nwith debugging symbols and assertions (useful for debugging with gdb), you must build with\n$ make <target> DEBUG=1\nWhenever a question asks about performance, we want you to run your programs on the\nAWS job machines using awsrun.\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\nSubmitting your solutions\nRemember to explicitly add new files to your repository before committing and pushing your\nfinal changes.\n$ git add\n$ git commit -a\n$ git status\n$ git push\nIf git status shows any modified files, then you probably haven't checked your code into\nyour repository properly. We recommend you commit often.\n2 Recitation: Perf and Cachegrind\nFirst, we will explore how to use two extremely useful performance profiling tools: Perf and\nCachegrind. These tools allow you to identify the performance bottlenecks in a program and\nmeasure salient performance properties, such as cache and branch misses. Used correctly, they\ncan help you figure out exactly why your code is running slower than it should.\n2.1 Perf\nperf is a profiler tool for Linux 2.6+ based systems. It uses sampling to gather data about\nimportant software, kernel, and hardware events in order to locate performance bottlenecks in a\nprogram. It generates a detailed record of where the time is spent in your code.\nNote: If perf is not installed on your AWS VM, run\n$ sudo apt install linux-tools-common linux-tools-aws\nYou can use perf record and perf report to analyze the performance of your program.\nperf record generates a record of the events that occur when you run your code, and perf report\nallows you to view it interactively.\nNote: For perf to annotate your code properly, you must pass the -g flag to clang when\ncompiling your program. This flag, which does not affect performance, tells the compiler to\ngenerate debugging symbols, which allow tools like perf and gdb to associate lines of machine\ncode with lines of source code. The provided Makefile will do this if you run it with DEBUG=1.\nTo record performance events on a program, run the following:\n$ perf record <program_name> <program_arguments>\nYou can then view the results by running perf report from the same directory:\n$ perf report\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\nNote: You may see a warning screen that says something like \"Kernel address maps were\nrestricted.\" This is OK, just press any key to continue to the report.\nThe personal AWS machines are generally less performant (and less consistent) than the\ndedicated AWSRUN machines, which are accessed via awsrun. You can also profile performance\nin the cloud:\n$ awsrun perf record <program_name> <program_arguments>\nThis will generate a report on an AWS cloud queue machine rather than on your personal\ninstance, so you need to pass some extra arguments to perf report in order to view it. We\nprovide the command aws-perf-report to do this for you:\n$ aws-perf-report\nNote: aws-perf-report should be run without awsrun.\nSpend some time exploring the contents of the report, and try to figure out what it means.\nYou can see both the C code and the assembly instructions in the annotated output. The\nperformance counter events are usually associated with the instruction right after or a few in\nstructions below the one that causes extra stalls.\nFile isort.c contains an insertion sort routine. Compile the program with\n$ make isort DEBUG=1\nNow, running ./isort n k will sort an array of n elements k times. Run:\n$ awsrun perf record ./isort 10000 10\n$ aws-perf-report\nIdentify branch misses, clock cycles and instructions. Diagnose the performance bottlenecks in\nthe program.\nCheckoff Item 1: Make note of one bottleneck.\n2.2 Cachegrind\nCachegrind (a Valgrind tool) is a cache and branch-prediction profiler. Recall from class that a\nread from the L1 cache can be around 100x faster than a read from RAM! Optimizing for cache\nhits is a critical part of perfomance engineering.\nOn virtual environments like those on AWS, hardware events providing information about\nbranches and cache misses are often unavailable, so perf may not be helpful. Cachegrind sim\nulates how your program interacts with a machine's cache hierarchy and branch predictor and\ncan be used even in the absence of available hardware performance counters.\nHere is an example on how to identify cache misses, branch misses, clock cycles, and instruc\ntions executed by your program using Cachegrind:\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\n$ valgrind --tool=cachegrind --branch-sim=yes <program_name> <program_arguments>\nNote: Although valgrind --tool=cachegrind measures cache and branch predictor behavior\nusing a simulator, it bases its simulation upon the architecture on which it is run. You should\nexpect different results when running on different machines, e.g. when running on your personal\ninstance vs. the awsrun machines.\nFile sum.c contains a program that allocates an array of U = 10 million elements, and then\nsums N = 100 million elements chosen at random. Make it with:\n$ make sum\nCheckoff Item 2: Run sum under cachegrind to identify cache performance. It may take a\nlittle while. In the output, look at the D1 and LLd misses. D1 represents the lowest-level\ncache (L1), and LL represents the last (highest) level data cache (on most machines, L3). Do\nthese numbers correspond with what you would expect? Try playing around with the\nvalues N and U in sum.c. How can you bring down the number of cache misses?\nHint: to find information about your CPU and its caches, use lscpu.\nCheckoff: Explain to a TA your responses to the previous two Checkoff Items.\n3 Homework: Sorting\nProfiling code can often give you valuable insight into how a program works and why it performs\nthe way it does. In this exercise, we have provided a simple implementation of merge sort in\nhomework/sort_a.c. You can make the code for all sort implementations by just typing make.\nAfter making, run the code with ./sort <num_elements> <num_trials>.\nWrite-up 1: Compare the Cachegrind output on the DEBUG=1 code versus DEBUG=0 compiler\noptimized code. Explain the advantages and disadvantages of using instruction count as a\nsubstitute for time when you compare the performance of different versions of this program.\nMake sure you're evaluating the non-debug version for the rest of your experiments.\nWe want to identify performance bottlenecks and incrementally improve the performance of\nthe merge sort as explained in the tasks below. As you make improvements to the sort routine\nin each task, we would like you to keep a copy of the sort routine before the improvements and\nkeep adding new versions of the sort routine in the testing suite (with different names), so we can\nprofile and compare the performance of different versions. The testing code is set up so that you\ncan easily add new sorting routines to test, as long as you keep the same function signature for\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\nthe sort routine, i.e., each sort routine you make should have the same type as sort_a provided\nin sort_a.c. In addition, do not remove the static keyword in any of the internal functions,\npreserve the structure of the merge sort algorithm as coded, and do not change it radically for\nthis homework.\n3.1 Inlining\nYou would like to see how much inline functions can help. Copy over the code from sort_a.c\ninto sort_i.c, and change all the routine names from <function>_a to <function>_i. Using the\ninline keyword, inline one or more of the functions in sort_i.c and util.c. To add the sort_i\nroutine to the testing suite, uncomment the line in main.c, under testFunc, that specifies sort_i.\nProfile and annotate the inlined program.\nWrite-up 2: Explain which functions you chose to inline and report the performance\ndifferences you observed between the inlined and uninlined sorting routines.\nThe compiler does not always inline functions with the inline keyword. Use the annotated\nassembly code to help you verify whether the compiler really inlined the functions you wanted\nto inline.\nTry to inline the sort_i recursive function (You may find forward declarations useful). When\nyou inline a recursive function, it will expand the recursion only a fixed number of times, and\nthen will recurse.\nWrite-up 3: Explain the possible performance downsides of inlining recursive functions.\nHow could profiling data gathered using cachegrind help you measure these negative\nperformance effects?\n3.2 Pointers vs Arrays\nYou learn that pointer access can be more efficient than array indexing when manipulating arrays,\nand you want to implement this efficiency in the sorting algorithm, in addition to the inlining\nchanges you did in the previous task. To accomplish this, you first copy your sort routine in\nsort_i.c into sort_p.c and update the function names in sort_p.c to <function>_p. Then you\nmodify the code in sort_p.c to use pointers instead of arrays and include sort_p into the test\nsuite.\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\nWrite-up 4: Give a reason why using pointers may improve performance. Report on any\nperformance differences you observed in your implementation.\n3.3 Coarsening\nThe base case in the recursion of sort_p routine in sort_p.c is sorting just one element. You\nwant to coarsen the recursion so that the recursion base case sorts more than one element. As\nbefore, copy all your changes into sort_c.c, and update your function names. Use the fastest\ncorrect sort routine so far as a base for sort_c.c. Then coarsen the recursion in sort_c.c. We\nhave provided an insertion sort routine in isort.c, which you can use as the sorting algorithm in\nyour base case. You can also write your favorite sorting algorithm and use it for your base case.\nRemember that in order to use a function defined elsewhere, you first need to write a function\nprototype. Include sort_c into your test suite.\nWrite-up 5: Explain what sorting algorithm you used and how you chose the number of\nelements to be sorted in the base case. Report on the performance differences you observed.\n3.4 Reducing Memory Usage\nObserve that two temporary memory scratch spaces left and right are used in the merge_c func\ntion. You want to optimize memory by using just one temporary memory scratch space (thereby\nreducing the total temporary memory usage by half) and using the input array to be sorted as\nthe other memory scratch space in merge operation. As before, copy your newest changes into\nsort_m.c and update the function names. Then implement the memory optimization described\nabove.\nWrite-up 6: Explain any difference in performance in your sort_m.c. Can a compiler\nautomatically make this optimization for you and save you all the effort? Why or why not?\n3.5 Reusing Temporary Memory\nThough you have reduced temporary memory usage in sort_m.c by half, you find that it is un\nnecessary to allocate and deallocate the temporary memory scratch space in the merge_m function\nevery time it is called. Instead, you would like to allocate the required memory once in the be\nginning and deallocate it at the end after sorting is done. To do this final optimization, copy your\n\nHandout 3 -- Homework 2: Profiling Serial Merge Sort\nsorting routine into sort_f.c. Then, implement the memory enhancement described above in\nsort_f.c and include sort_f in your test suite.\nWrite-up 7: Report any differences in performance in your sort_f.c, and explain the\ndifferences using profiling data.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 3: Vectorization",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/072651a8229a63376d5720c9a500ae45_MIT6_172F18hw3.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 5\nHomework 3: Vectorization\nIn this homework and recitation you will experiment with Intel Vector Extensions. You will learn how\nto vectorize your code, figure out when vectorization has succeeded and debug when vectorization seems to\nhave worked but you aren't seeing speedup.\nVectorization is a general optimization technique that can buy you an order of magnitude performance\nincrease in some cases. It is also a delicate operation. On the one hand, vectorization is automatic: when\nclang is told to optimize aggressively, it will automatically try to vectorize every loop in your program.\nOn the other hand, very small changes to loop structure cause clang to give up and not vectorize at all.\nFurthermore, these small changes may allow your code to vectorize but not yield the expected speedup. We\nwill discuss how to identify these cases so that you can get the most out of your vector units.\n1 Getting started\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\nSubmitting your solutions\nFor each question we ask (i.e., each sentence with a question mark), respond with a short (1-3 sentence)\nresponses or a code snippet (if requested). Please ensure that all the times you quote are obtained\nfrom the awsrun machines.\n2 Vectorization in clang\nConsider a loop that performs elementwise addition between two arrays A and B, storing the\nresult in array C. This loop is data parallel because the operation during any iteration i1 is inde\npendent of the operation during any iteration i2 where i1 6= i2. In short, the compiler should be\n\nHandout 5 -- Homework 3: Vectorization\nallowed to schedule each iteration in any order, or pack multiple iterations into a single clock\ncycle. The first option will be covered in the next homework. The second case is covered by\nvectorization, also known as \"single instruction, multiple data\" or SIMD.\nVectorization is a delicate operation: very small changes to loop structure may cause clang\nto give up and not vectorize at all, or to vectorize your code but not yield the expected speedup.\nOccasionally, unvectorized code may be faster than vectorized code. Before we can understand\nthis fragility, we must get a handle on how to interpret what clang is actually doing when it\nvectorizes code; in Section 3, you will see the actual performance impacts of vectorizing code.\n2.1 Example 1\nWe will start with the following simple loop:\n01 #include <stdint.h>\n02 #include <stdlib.h>\n03 #include <math.h>\n05 #define SIZE (1L << 16)\n07 void test(uint8_t * a, uint8_t * b) {\nuint64_t i;\nfor (i = 0; i < SIZE; i++) {\na[i] += b[i];\n}\n13 }\n$ make clean; make ASSEMBLE=1 VECTORIZE=1 example1.o\nYou should see the following output, informing you that the loop has been vectorized. Al\nthough clang does tell you this, you should always look at the assembly to see exactly how it has\nbeen vectorized, since it is not guaranteed to be using the vector registers optimally.\n14 example1.c:12:3: remark: vectorized loop (vectorization width: 16, interleaved count: 2)\n[-Rpass=loop-vectorize]\nfor (i = 0; i < SIZE; i++) {\nNow, let's inspect the assembly code in example1.s. You should see something similar to the\nfollowing:\n\nHandout 5 -- Homework 3: Vectorization\n# %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:i <- 0\n.loc\n1 12 3 prologue_end\n# example1.c:12:3\nleaq\ncmpq\njbe\n# %bb.1:\n65536(%rsi), %rax\n%rdi, %rax\n.LBB0_2\n# %entry\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\nleaq\n65536(%rdi), %rax\ncmpq\n%rsi, %rax\njbe\n.LBB0_2\n# %bb.4:\n# %for.body.preheader\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 0 3 is_stmt 0\n# example1.c:0:3\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n.p2align\n4, 0x90\n.LBB0_5:\n# %for.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.Ltmp0:\n.loc\n1 13 13 is_stmt 1\n# example1.c:13:13\nmovzbl\n65536(%rsi,%rax), %ecx\n.loc\n1 13 10 is_stmt 0\n# example1.c:13:10\naddb\n%cl, 65536(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovzbl\n65537(%rsi,%rax), %ecx\n.loc\n1 13 10\n# example1.c:13:10\naddb\n%cl, 65537(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovzbl\n65538(%rsi,%rax), %ecx\n.loc\n1 13 10\n# example1.c:13:10\naddb\n%cl, 65538(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovzbl\n65539(%rsi,%rax), %ecx\n.loc\n1 13 10\n# example1.c:13:10\naddb\n%cl, 65539(%rdi,%rax)\n\nHandout 5 -- Homework 3: Vectorization\n.Ltmp1:\n.loc\n1 12 17 is_stmt 1\n# example1.c:12:17\naddq\n.Ltmp2:\n.loc\n$4, %rax\n1 12 3 is_stmt 0\n# example1.c:12:3\njne .LBB0_5\njmp .LBB0_6\n.LBB0_2:\n# %vector.body.preheader\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 0 3\n# example1.c:0:3\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n.p2align\n4, 0x90\n.LBB0_3:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.Ltmp3:\n.loc\n1 13 13 is_stmt 1\n# example1.c:13:13\nmovdqu 65536(%rsi,%rax), %xmm0\nmovdqu 65552(%rsi,%rax), %xmm1\n.loc\n1 13 10 is_stmt 0\n# example1.c:13:10\nmovdqu 65536(%rdi,%rax), %xmm2\npaddb\n%xmm0, %xmm2\nmovdqu 65552(%rdi,%rax), %xmm0\nmovdqu 65568(%rdi,%rax), %xmm3\nmovdqu 65584(%rdi,%rax), %xmm4\nmovdqu %xmm2, 65536(%rdi,%rax)\npaddb\n%xmm1, %xmm0\nmovdqu %xmm0, 65552(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovdqu 65568(%rsi,%rax), %xmm0\n.loc\n1 13 10\n# example1.c:13:10\npaddb\n%xmm3, %xmm0\n.loc\n1 13 13\n# example1.c:13:13\nmovdqu 65584(%rsi,%rax), %xmm1\n.loc\n1 13 10\n# example1.c:13:10\nmovdqu %xmm0, 65568(%rdi,%rax)\npaddb\n%xmm4, %xmm1\nmovdqu %xmm1, 65584(%rdi,%rax)\n.Ltmp4:\n.loc\n1 12 26 is_stmt 1\n# example1.c:12:26\naddq\n$64, %rax\njne .LBB0_3\n\nHandout 5 -- Homework 3: Vectorization\nWrite-up 1: Look at the assembly code above. The compiler has translated the code to set\nthe start index at -216 and adds to it for each memory access. Why doesn't it set the start\nindex to 0 and use small positive offsets?\nThis code first checks if there is a partial overlap between array a and b. If there is an overlap,\nthen it does a simple non-vectorized code. If there is overlap, then go to .LBB0_2, and do a\nvectorized version. The above can, at best, be called partially vectorized. The problem is that the\ncompiler is constrained by what we tell it about the arrays. If we tell it more, then perhaps it\ncan do more optimization. The most obvious thing is to inform the compiler that no overlap is\npossible. This is done in standard C by using the restrict qualifier for the pointers.\n125 void test(uint8_t * restrict a, uint8_t * restrict b) {\nuint64_t i;\nfor (i = 0; i < SIZE; i++) {\na[i] += b[i];\n}\n131 }\nNow you should see the following assembly code:\n\nHandout 5 -- Homework 3: Vectorization\n# %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:b <- %rsi\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n.Ltmp0:\n#DEBUG_VALUE: test:i <- 0\n.p2align\n4, 0x90\n.LBB0_1:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 13 13 prologue_end\n# example1.c:13:13\nmovdqu\n65536(%rsi,%rax), %xmm0\n.loc\n1 13 10 is_stmt 0\n# example1.c:13:10\nmovdqu\n65536(%rdi,%rax), %xmm1\npaddb\n%xmm0, %xmm1\nmovdqu\n65552(%rdi,%rax), %xmm0\nmovdqu\n65568(%rdi,%rax), %xmm2\nmovdqu\n65584(%rdi,%rax), %xmm3\nmovdqu\n%xmm1, 65536(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovdqu\n65552(%rsi,%rax), %xmm1\n.loc\n1 13 10\n# example1.c:13:10\npaddb\n%xmm1, %xmm0\nmovdqu\n%xmm0, 65552(%rdi,%rax)\n.loc\n1 13 13\n# example1.c:13:13\nmovdqu\n65568(%rsi,%rax), %xmm0\n.loc\n1 13 10\n# example1.c:13:10\npaddb\n%xmm2, %xmm0\n.loc\n1 13 13\n# example1.c:13:13\nmovdqu\n65584(%rsi,%rax), %xmm1\n.loc\n1 13 10\n# example1.c:13:10\nmovdqu\n%xmm0, 65568(%rdi,%rax)\npaddb\n%xmm3, %xmm1\nmovdqu\n%xmm1, 65584(%rdi,%rax)\n.Ltmp1:\n.loc\n1 12 26 is_stmt 1\n# example1.c:12:26\naddq\n$64, %rax\njne\n.LBB0_1\n\nHandout 5 -- Homework 3: Vectorization\nThe generated code is better, but it is assuming the data are NOT 16 bytes aligned (movdqu is\nunaligned move). It also means that the loop above can not assume that both arrays are aligned.\nIf clang were smart, it could test for the cases where the arrays are either both aligned, or both\nunaligned, and have a fast inner loop. However, it does not do that currently.\nSo in order to get the performance we are looking for, we need to tell clang that the arrays are\naligned. There are a couple of ways to do that. The first is to construct a (non-portable) aligned\ntype, and use that in the function interface. The second is to add an intrinsic or two within the\nfunction itself. The second option is easier to implement on older code bases, as other functions\ncalling the one to be vectorized do not have to be modified. The intrinsic has for this is called\n__builtin_assume_aligned:\n182 void test(uint8_t * restrict a, uint8_t * restrict b) {\nuint64_t i;\na = __builtin_assume_aligned(a, 16);\nb = __builtin_assume_aligned(b, 16);\nfor (i = 0; i < SIZE; i++) {\na[i] += b[i];\n}\n191 }\nAfter you add the instruction __builtin_assume_aligned, you should see something similar\nto the following output:\n\nHandout 5 -- Homework 3: Vectorization\n192 # %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:b <- %rsi\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n198 .Ltmp0:\n#DEBUG_VALUE: test:i <- 0\n.p2align\n4, 0x90\n201 .LBB0_1:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 16 10 prologue_end\n# example1.c:16:10\nmovdqa\n65536(%rdi,%rax), %xmm0\nmovdqa\n65552(%rdi,%rax), %xmm1\nmovdqa\n65568(%rdi,%rax), %xmm2\nmovdqa\n65584(%rdi,%rax), %xmm3\npaddb\n65536(%rsi,%rax), %xmm0\npaddb\n65552(%rsi,%rax), %xmm1\nmovdqa\n%xmm0, 65536(%rdi,%rax)\nmovdqa\n%xmm1, 65552(%rdi,%rax)\npaddb\n65568(%rsi,%rax), %xmm2\npaddb\n65584(%rsi,%rax), %xmm3\nmovdqa\n%xmm2, 65568(%rdi,%rax)\nmovdqa\n%xmm3, 65584(%rdi,%rax)\n219 .Ltmp1:\n.loc\n1 15 26\n# example1.c:15:26\naddq\n$64, %rax\njne\n.LBB0_1\n224 .Ltmp2:\nNow finally, we get the nice tight vectorized code (movdqa is aligned move) we were looking\nfor, because clang has used packed SSE instructions to add 16 bytes at a time. It also manages\nto load and store two at a time, which it did not do last time. The question is now that we\nunderstand what we need to tell the compiler, how much more complex can the loop be before\nauto-vectorization fails.\nNext, we try to turn on AVX2 instructions using the following command:\n$ make clean; make ASSEMBLE=1 VECTORIZE=1 AVX2=1 example1.o\n\nHandout 5 -- Homework 3: Vectorization\n225 # %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:b <- %rsi\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n231 .Ltmp0:\n#DEBUG_VALUE: test:i <- 0\n.p2align\n4, 0x90\n234 .LBB0_1:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 16 10 prologue_end\n# example1.c:16:10\nvmovdqu\n65536(%rdi,%rax), %ymm0\nvmovdqu\n65568(%rdi,%rax), %ymm1\nvmovdqu\n65600(%rdi,%rax), %ymm2\nvmovdqu\n65632(%rdi,%rax), %ymm3\nvpaddb\n65536(%rsi,%rax), %ymm0, %ymm0\nvpaddb\n65568(%rsi,%rax), %ymm1, %ymm1\nvpaddb\n65600(%rsi,%rax), %ymm2, %ymm2\nvmovdqu\n%ymm0, 65536(%rdi,%rax)\nvmovdqu\n%ymm1, 65568(%rdi,%rax)\nvmovdqu\n%ymm2, 65600(%rdi,%rax)\nvpaddb\n65632(%rsi,%rax), %ymm3, %ymm0\nvmovdqu\n%ymm0, 65632(%rdi,%rax)\n252 .Ltmp1:\n.loc\n1 15 26\n# example1.c:15:26\naddq\n$128, %rax\njne\n.LBB0_1\n257 .Ltmp2:\nWrite-up 2: This code is still not aligned when using AVX2 registers. Fix the code to make\nsure it uses aligned moves for the best performance.\n2.2 Example 2\nTake a look at the second example below in example2.c:\n\nHandout 5 -- Homework 3: Vectorization\n258 void test(uint8_t * restrict a, uint8_t * restrict b) {\nuint64_t i;\nuint8_t * x = __builtin_assume_aligned(a, 16);\nuint8_t * y = __builtin_assume_aligned(b, 16);\nfor (i = 0; i < SIZE; i++) {\n/* max() */\nif (y[i] > x[i]) x[i] = y[i];\n}\n268 }\nCompile example 2 with the following command:\n$ make clean; make ASSEMBLE=1 VECTORIZE=1 example2.o\nNote that the assembly does not vectorize nicely. Now, change the function to look like the\nfollowing:\n269 void test(uint8_t * restrict a, uint8_t * restrict b) {\nuint64_t i;\na = __builtin_assume_aligned(a, 16);\nb = __builtin_assume_aligned(b, 16);\nfor (i = 0; i < SIZE; i++) {\n/* max() */\na[i] = (b[i] > a[i]) ? b[i] : a[i];\n}\n279 }\nNow, you actually see the vectorized assembly with the movdqa and pmaxub instructions.\n\nHandout 5 -- Homework 3: Vectorization\n# %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:b <- %rsi\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\n286 .Ltmp0:\n#DEBUG_VALUE: test:i <- 0\n.p2align\n4, 0x90\n289 .LBB0_1:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:b <- %rsi\n#DEBUG_VALUE: test:a <- %rdi\n.loc\n1 17 15 prologue_end\n# example2.c:17:15\nmovdqa\n65536(%rsi,%rax), %xmm0\nmovdqa\n65552(%rsi,%rax), %xmm1\n.loc\n1 17 14 is_stmt 0\n# example2.c:17:14\npmaxub\n65536(%rdi,%rax), %xmm0\npmaxub\n65552(%rdi,%rax), %xmm1\n.loc\n1 17 12\n# example2.c:17:12\nmovdqa\n%xmm0, 65536(%rdi,%rax)\nmovdqa\n%xmm1, 65552(%rdi,%rax)\n.loc\n1 17 15\n# example2.c:17:15\nmovdqa\n65568(%rsi,%rax), %xmm0\nmovdqa\n65584(%rsi,%rax), %xmm1\n.loc\n1 17 14\n# example2.c:17:14\npmaxub\n65568(%rdi,%rax), %xmm0\npmaxub\n65584(%rdi,%rax), %xmm1\n.loc\n1 17 12\n# example2.c:17:12\nmovdqa\n%xmm0, 65568(%rdi,%rax)\nmovdqa\n%xmm1, 65584(%rdi,%rax)\n317 .Ltmp1:\n.loc\n1 15 28 is_stmt 1\n# example2.c:15:28\naddq\n$64, %rax\njne\n.LBB0_1\n322 .Ltmp2:\nWrite-up 3: Provide a theory for why the compiler is generating dramatically different\nassembly.\n\nHandout 5 -- Homework 3: Vectorization\n2.3 Example 3\nOpen up example3.c and run the following command:\n$ make clean; make ASSEMBLE=1 VECTORIZE=1 example3.o\n323 void test(uint8_t * restrict a, uint8_t * restrict b) {\nuint64_t i;\nfor (i = 0; i < SIZE; i++) {\na[i] = b[i + 1];\n}\n329 }\nWrite-up 4: Inspect the assembly and determine why the assembly does not include\ninstructions with vector registers. Do you think it would be faster if it did vectorize?\nExplain.\n2.4 Example 4\nTake a look at example4.c.\n330 double test(double * restrict a) {\nsize_t i;\ndouble *x = __builtin_assume_aligned(a, 16);\ndouble y = 0;\nfor (i = 0; i < SIZE; i++) {\ny += x[i];\n}\nreturn y;\n341 }\n$ make clean; make ASSEMBLE=1 VECTORIZE=1 example4.o\nYou should see the non-vectorized code with the addsd instruction.\n\nHandout 5 -- Homework 3: Vectorization\n342 .LBB0_1:\n# %for.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:x <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n346 .Ltmp1:\n#DEBUG_VALUE: test:y <- %xmm0\n.loc\n1 18 7 prologue_end\n# example4.c:18:7\naddsd\n524288(%rdi,%rax,8), %xmm0\n351 .Ltmp2:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524296(%rdi,%rax,8), %xmm0\n354 .Ltmp3:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524304(%rdi,%rax,8), %xmm0\n357 .Ltmp4:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524312(%rdi,%rax,8), %xmm0\n360 .Ltmp5:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524320(%rdi,%rax,8), %xmm0\n363 .Ltmp6:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524328(%rdi,%rax,8), %xmm0\n366 .Ltmp7:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524336(%rdi,%rax,8), %xmm0\n369 .Ltmp8:\n#DEBUG_VALUE: test:y <- %xmm0\naddsd\n524344(%rdi,%rax,8), %xmm0\n372 .Ltmp9:\n#DEBUG_VALUE: test:y <- %xmm0\n.loc\n1 17 17\n# example4.c:17:17\naddq\n$8, %rax\n377 .Ltmp10:\n.loc\n1 17 3 is_stmt 0\n# example4.c:17:3\njne\n.LBB0_1\nNotice that this does not actually vectorize as the xmm registers are operating on 8 byte chunks.\nThe problem here is that clang is not allowed to re-order the operations we give it. Even though\nthe the addition operation is associative with real numbers, they are not with floating point\nnumbers. (Consider what happens with signed zeros, for example.)\nFurthermore, we need to tell clang that reordering operations is okay with us. To do this, we\nneed to add another compile-time flag, -ffast-math. Add the compilation flag -ffast-math to\nthe Makefile and compile the program again.\n\nHandout 5 -- Homework 3: Vectorization\nWrite-up 5: Check the assembly and verify that it does in fact vectorize properly. Also what\ndo you notice when you run the command\n$ clang -O3 example4.c -o example4; ./example4\nwith and without the -ffast-math flag? Specifically, why do you a see a difference in the\noutput.\n\nHandout 5 -- Homework 3: Vectorization\n381 # %bb.0:\n# %entry\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n#DEBUG_VALUE: test:x <- %rdi\n#DEBUG_VALUE: test:x <- %rdi\nxorpd\n%xmm0, %xmm0\n387 .Ltmp0:\n#DEBUG_VALUE: test:i <- 0\n#DEBUG_VALUE: test:y <- 0.000000e+00\nmovq\n$-65536, %rax\n# imm = 0xFFFF0000\nxorpd\n%xmm1, %xmm1\n.p2align\n4, 0x90\n393 .LBB0_1:\n# %vector.body\n# =>This Inner Loop Header: Depth=1\n#DEBUG_VALUE: test:x <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n397 .Ltmp1:\n.loc\n1 18 7 prologue_end\n# example4.c:18:7\naddpd\n524288(%rdi,%rax,8), %xmm0\naddpd\n524304(%rdi,%rax,8), %xmm1\naddpd\n524320(%rdi,%rax,8), %xmm0\naddpd\n524336(%rdi,%rax,8), %xmm1\naddpd\n524352(%rdi,%rax,8), %xmm0\naddpd\n524368(%rdi,%rax,8), %xmm1\naddpd\n524384(%rdi,%rax,8), %xmm0\naddpd\n524400(%rdi,%rax,8), %xmm1\n408 .Ltmp2:\n.loc\n1 17 26\n# example4.c:17:26\naddq\n$16, %rax\njne\n.LBB0_1\n413 # %bb.2:\n# %middle.block\n#DEBUG_VALUE: test:x <- %rdi\n#DEBUG_VALUE: test:a <- %rdi\n416 .Ltmp3:\n.loc\n1 18 7\n# example4.c:18:7\naddpd\n%xmm0, %xmm1\nmovapd\n%xmm1, %xmm0\nmovhlps\n%xmm0, %xmm0\n# xmm0 = xmm0[1,1]\naddpd\n%xmm1, %xmm0\n3 Performance Impacts of Vectorization\nWe will now familiarize ourselves with what code does/does not vectorize, and discuss how to\nincrease speedup from vectorization.\n\nHandout 5 -- Homework 3: Vectorization\n3.1 The Many Facets of a Data Parallel Loop\nIn loop.c, we have written a loop that performs elementwise an operation -- by default, addition\n-- between two arrays A and B, storing the result in array C. If you examine the code, you will\nsee that our loop does no useful work (in the sense that A and B are not filled with any initial\nvalues). We are just using this loop to demonstrate concepts. Further, we have added an outer\nloop over I whose purpose is to eliminate measurement error in gettime().\nLet's see what speedup we get from vectorization. Run make and run awsrun ./loop. Record\nthe elapsed execution time. Then run make VECTORIZE=1 and run awsrun ./loop again. Record\nthe vectorized elapsed execution time. The flag -mavx2 tells clang to use advanced vector exten\nsions with larger vector registers. Run make VECTORIZE=1 AVX2=1 and run awsrun ./loop again.\nNote that you must use the awsrun machines for this; you may otherwise get a message like\nIllegal instruction (core dumped). You can check whether or not a machine supports the AVX2\ninstructions by looking for avx2 in the flags section of the output of cat /proc/cpuinfo. Record\nthe vectorized elapsed execution time.\nWrite-up 6: What speedup does the vectorized code achieve over the unvectorized code?\nWhat additional speedup does using -mavx2 give? You may wish to run this experiment\nseveral times and take median elapsed times; you can report answers to the nearest 100%\n(e.g., 2×, 3×, etc). What can you infer about the bit width of the default vector registers on\nthe awsrun machines? What about the bit width of the AVX2 vector registers? Hint: aside\nfrom speedup and the vectorization report, the most relevant information is that the data\ntype for each array is uint32_t.\n3.1.1 Flags to enable and debug vectorization\nVectorization is enabled by default, but can be explicitly turned on with the -fvectorize flag1.\nWhen vectorization is enabled, the -Rpass=loop-vectorize flag identifies loops that were suc\ncessfully vectorized, and the -Rpass-missed=loop-vectorize flag identifies loops that failed vec\ntorization and indicates if vectorization was specified (see Makefile). Further, you can add the\nflag -Rpass-analysis=loop-vectorize to identify the statements that caused vectorization to fail.\n3.1.2 Debugging through assembly code inspection\nAnother way to see how code is vectorized is to look at the assembly output from the compiler.\nRun\n$ make ASSEMBLE=1 VECTORIZE=1\n1If you open Makefile, you will see we set up things in a slightly different way. We set -O3 regardless of\nvectorization--because we want a fair comparison when the vectorization flag is enabled/disabled. We then disable\nvectorization for when VECTORIZE=0 by setting the flag -fno-vectorize.\n\nHandout 5 -- Homework 3: Vectorization\nThis will produce loop.s, which contains human-readable x86 assembly like perf annotate -f\nfrom Recitation 2. Note that the compilation may \"fail\" with ASSEMBLE=1 because this flag tells\nclang to not produce loop.o.\nWrite-up 7: Compare the contents of loop.s when the VECTORIZE flag is set/not set. Which\ninstruction (copy its text here) is responsible for the vector add operation? Which\ninstruction (copy its text here) is responsible for the vector add operation when you\nadditionally pass AVX2=1? You can find an x86 instruction manual on LMOD. Look for\nMMX and SSE2 instructions, which are vector operations. To make the assembly code more\nreadable it may be a good idea to remove debug symbols from release builds by moving the\n-g and -gdwarf-3 CFLAGS in your Makefile. It might also be a good idea to turn off loop\nunrolling with the -fno-unroll-loops flag while you study the assembly code.\n3.1.3 Flavors of vector arithmetic\nAs discussed in lecture, the vector unit is built directly in hardware. To support more flavors of\nvector operations (e.g., vector subtract or multiply), additional hardware must be added for each\noperation.\nWrite-up 8: Use the __OP__ macro to experiment with different operators in the data parallel\nloop. For some operations, you will get division by zero errors because we initialize array B\nto be full of zeros--fix this problem in any way you like. Do any versions of the loop not\nvectorize with VECTORIZE=1 AVX2=1? Study the assembly code for << with just VECTORIZE=1\nand explain how it differs from the AVX2 version.\nThe results may surprise you. For example, compare the results for * and << (shift). The\nproblem is that shifting by a variable amount (B[j]) is not a supported vector instruction unless\nwe pass -mavx2. Changing B[j] to a constant value should allow the code to be vectorizable\nagain.\n3.1.4 Packing smaller words into vectors\nA big class of optimizations you will use in future projects is optimizing data type width for\nyour application. Consider the arrays A, B, and C which have data type uint32_t (given by the\n__TYPE__ macro). Changing the data type for each array has an impact in two places:\n1. Memory requirements. A smaller data type per element leads to a smaller memory foot\nprint per array.\n\nHandout 5 -- Homework 3: Vectorization\n2. Vector packing. A smaller data type allows more elements to be packed into a single vector\nregister.\nLet's experiment with the vector packing idea:\nWrite-up 9: What is the new speedup for the vectorized code, over the unvectorized code,\nand for the AVX2 vectorized code, over the unvectorized code, when you change __TYPE__\nto uint64_t, uint32_t, uint16_t and uint8_t? For each experiment, set __OP__ to + and do\nnot change N.\nIn general, speedup should increase as data type size decreases. This is a fundamental advan\ntage over unvectorized codes where for fixed N, the number of instructions needed to perform\nelementwise operations over an array of N elements is mostly independent of the data type width.2\n3.1.5 To vectorize or not to vectorize\nPerformance potential from vectorization is also impacted by what operation you wish to per\nform. Of the operations that vectorize (Section 3.1.3), multiply (*) takes the most clock cycles per\noperation.\nWrite-up 10: You already determined that uint64_t yields the least performance\nimprovement for vectorized codes (Section 3.1.4). Test a vector multiplication (i.e., __OP__ is\n*) using uint64_t arrays. What happens to the AVX2 vectorized code's speedup relative to\nthe unvectorized code (also using uint64_t and *)? What about when you set the data type\nwidth to be smaller -- say uint8_t?\nWrite-up 11: Open up the aws-perf-report tool for the AVX2 vectorized multiply code\nusing uint64_t (as you did in Recitation 2). Remember to first use the awsrun perf record\ntool to collect a performance report. Does the vector multiply take the most time? If not,\nwhere is time going instead? Now change __OP__ back to +, rerun the experiment and\ninspect aws-perf-report again. How does the percentage of time taken by the AVX2 vector\nadd instruction compare to the time spent on the AVX2 vector multiply instruction?\n2We say \"mostly\" because depending on your processor's architecture, arrays with large data types (e.g., 64 bit and\n128 bit) are processed in different ways. For example, you can use 128 bit data types using gcc and the type __int128.\nBut since ALUs in the awsrun machines are only 64 bits wide, the compiler turns each 128 bit operation into several\n64 bit operations.\n\nHandout 5 -- Homework 3: Vectorization\nYou will see that where time goes changes dramatically when you change * to +. This is partly\ndue to the data type width (uint64_t) and partly due to the * operation itself. In particular,\nthe awsrun machine vector units only support 32 × 32 bit multiplication--wider data types are\nsynthesized from smaller operations. If you experiment with smaller (uint16_t and below) data\ntypes, you should see that the assembly code for * and + look more similar\n3.2 Vector Patterns\nWe will now explore some common vector code patterns. We also recommend https://llvm.\norg/docs/Vectorizers.html as a reference guide for when you are optimizing your projects.\n3.2.1 Loops with Runtime Bounds\nUp to this point, our data parallel loop has been simple for the compiler to handle because N was\nknown beforehand and was a power of 2. What about when the loop bound is not known ahead\nof time?\nWrite-up 12: Get rid of the #define N 1024 macro and redefine N as: int N = atoi(argv[1]);\n(at the beginning of main()). (Setting N through the command line ensures that the compiler\nwill make no assumptions about it.) Rerun (with various choices of N) and compare the\nAVX2 vectorized, non-AVX2 vectorized, and unvectorized codes. Does the speedup change\ndramatically relative to the N = 1024 case? Why?\nHint: If you look at loop.s when you apply this change, you will see the compiler adding\ntermination case code to handle the final loop iterations (i.e., the iterations that do not align\nwith the vector register width). Test this yourself: as you set __TYPE__ to smaller data types,\nyou should see that the amount of termination-related assembly code emitted by the compiler\nincreases.\n3.2.2 Striding\nAnother simplifying feature in our loop is that its stride (or step) equals 1. Stride corresponds\nto how big our steps through the array are; e.g., j++, j+=2, etc. The awsrun machine vector units\nhave some hardware support to accelerate different strides.\nFor example,\n424 for (j = 0; j < N; j+=2) {\nC[j] = A[j] + B[j];\n426 }\n\nHandout 5 -- Homework 3: Vectorization\nWrite-up 13: Set __TYPE__ to uint32_t and __OP__ to +, and change your inner loop to be\nstrided. Does clang vectorize the code? Why might it choose not to vectorize the code?\nclang provides a #pragma Clang loop directive that can be used to control the optimization\nof loops, including vectorization. These are described at the following webpage: http://Clang.\nllvm.org/docs/LanguageExtensions.html#extensions-for-loop-hint-optimizations\nWrite-up 14: Use the #vectorize pragma described in the clang language extensions\nwebpage above to make clang vectorize the strided loop. What is the speedup over\nnon-vectorized code for non-AVX2 and AVX2 vectorization? What happens if you change\nthe vectorize_width to 2? Play around with the clang loop pragmas and report the best you\nfound (that vectorizes the loop). Did you get a speedup over the non-vectorized code?\nOnce again, inspecting the assembly code to see how striding is vectorized can be insightful.\n3.2.3 Strip Mining\nA very common operation is to combine elements in an array (somehow) into a single value. For\ninstance, one might wish to sum up the elements in an array. Replace the data parallel inner loop\nwith such a reduction:\n427 for (j = 0; j < N; j++) {\ntotal += A[j];\n429 }\nTo ensure that clang vectorizes the inner loop rather than the outer loop, comment out the\nouter loop.\nWrite-up 15: This code vectorizes, but how does it vectorize? Turn on ASSEMBLE=1, look at\nthe assembly dump, and explain what the compiler is doing.\nAs discussed in lecture, this reduction will only vectorize if the combination operation (+) is\nassociative.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 4: Reducer Hyperobjects",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/b2ff5659ae776c6fa6834967f73a2ab4_MIT6_172F18hw4.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 7\nHomework 4: Reducer Hyperobjects\nIn this homework you will experiment with parallelizing in Cilk. You will learn how to detect and solve\ndeterminacy races in your multithreaded code using Cilk Sanitizer, and how to measure a program's\nparallelism using the Cilkscale profiler.\n1 Getting started\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\nSubmitting your solutions\nFor each question we ask give short responses of 1-3 sentences. Paste program outputs where\nnecessary.\nReporting performance numbers\nTo test the performance of parallel code, you should use awsrun8, a variant of awsrun that submits\njobs to cloud machines with 8 cores. It's important to use awsrun8 for all performance numbers\ninvolving parallel code.\n2 Recitation: Parallelism and race detection using Cilk\nPlease answer the checkoff questions on your own and show a TA after you have completed all\nof the questions in this section.\n2.1 Introduction to Cilk\nThe cilk_spawn and cilk_sync keywords\nCompile the fib program in the fib/ subdirectory. Then, time the execution for finding fib(45)\nusing the time command:\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\n$ awsrun8 time ./fib 45\nYou will get 3 different times as outputs, labeled real, user, and sys. The real time is wall\ntime, which is what you see from a clock. The user time is the time a CPU spent in user mode.\nThe sys time is the time a CPU spent in the kernel. The sum of user and sys is the actual CPU\ntime of the command. Since the current fib is a serial program, you will find that wall time is\nslightly higher than CPU time.\nNext, we want to parallelize the program to take advantage of the other 7 processors on the\nawsrun machines. You can do this by adding cilk_spawn in front of function calls that you want\nto execute in parallel. You also need to add cilk_sync to wait for all spawning tasks to finish.\nLastly, you should include the Cilk header using\n01 #include <cilk/cilk.h>\nYou can specify the number of Cilk workers that execute a program by setting the envi\nronment variable CILK_NWORKERS. You can specify this variable globally for all Cilk programs\nexecuted in your bash session by running:\n$ export CILK_NWORKERS=8\nYou may also specify the number of Cilk workers to 8 for a single execution of a program by\nprepending CILK_NWORKERS=8 before the command you wish to run. Setting the number of Cilk\nworkers is especially useful when executing a Cilk program when using awsrun8. For example,\nyou may execute\n$ awsrun8 CILK_NWORKERS=4 ./fib 45\nThis command will execute fib on the AWS job queue machines using 4 Cilk workers. You can\nsafely ignore errors about not finding the command to set CILK_NWORKERS with awsrun8 such as\nthe following:\n[Warning] Cannot find path for command: CILK_NWORKERS=8\nCheckoff Item 1: Parallelize fib using cilk_spawn and cilk_sync, and report the runtime\nwhen using 1, 4, and 8 Cilk workers.\nYou might find that the new version is not faster than the first one (in terms of real time).\nFurthermore, the CPU time is much higher than wall time, because the program uses multiple\nprocessors to run. Under what circumstances would the parallel version be slower? Try to fix\nyour program using \"coarsening\" to get a parallel speedup.\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nCheckoff Item 2: Describe your approach to coarsening the program, and report your\nparallel runtime for the coarsened version of fib on 1, 4, and 8 Cilk workers.\nThe cilk_for keyword\nThe transpose/ subdirectory contains the source code for transpose, an in-place matrix-transpose\nprogram. As you saw in lecture, you can replace the for loops with cilk_for loops to parallelize\ntranspose.\nCheckoff Item 3: Parallelize transpose using cilk_for, and report your runtime with input\nsize 10000 for 1, 4, and 8 Cilk workers.\n2.2 The Cilksan race detector\nThe Cilksan race detector allows you to check whether your Cilk program has a determinacy\nrace. It provides detailed output that specifies the line numbers of two memory locations, often a\nread and a write, that were involved in the race. The Cilksan tool needs the libsnappy package,\nwhich you can install on your AWS instance with:\n$ sudo apt-get install libsnappy-dev\nCompile and run the qsort-race program in the qsort-race/ subdirectory. This code was\nparallelized by naively adding cilk_spawn and cilk_sync to a serial quicksort program. This\ncode has a race!\nBefore you run Cilksan, take a look at the quicksort code and see if you can identify the\ndeterminacy race. See if you can expose the race condition by running a few tests on awsrun8\ntoo. Don't be discouraged if you are unable to expose the race by running tests -- but also do not\nallow yourself to be fooled! This code does, indeed, have a race. Like many subtle determinacy\nraces, the one present in quicksort is difficult to identify without the use of tools.\nWe can use Cilksan to detect the race as follows:\n$ make clean; make CILKSAN=1\nWe can expose this race on a fairly small input of 10 elements:\n$ ./qsort-race 10 1\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nCheckoff Item 4: Use Cilksan to find this race. Then, fix the race, and use Cilksan to\nconfirm that no more races exist. Report the line numbers in the code where the read/write\nrace occurs, and give a brief description of what was happening to cause this race.\n2.3 The Cilkscale scalability analyzer\nThe qsort/ subdirectory contains qsort, another parallel quicksort program. This version of\nquicksort should not have any races, but you should of course verify this by using Cilksan.\nYou can use Cilkscale to analyze the scalability of this quicksort program. We have printed\nthe scalability information at the end of main in qsort.c:\n02 #ifdef CILKSCALE\n03 print_total();\n04 #endif\nWe can build qsort with Cilkscale as follows:\n$ make CILKSCALE=1\nSince Cilkscale uses timing measurements to compute parallelism, it is usually a good idea\nto run it on a quiesced machine -- such as those provided in the AWS job queue. For this\nassignment, however, we recommend you run Cilkscale locally on your AWS instance instead of\nusing awsrun.\nCheckoff Item 5: Report the parallelism computed by Cilkscale on the quicksort program\nfor a few different sized inputs.\nCilkscale's command-line output includes work and span measurements for the Cilk program\nin terms of empirically measured cycle counts, as well as parallelism measurements based on the\nmeasured work and span. For some programs, such as fib, there may be some variability in the\nreported parallelism numbers.\nCheckoff: Explain your responses to the previous five Checkoff Items to a TA.\n3 Homework: N queens problem and reducers\nWe introduced the N Queens Problem in Lecture 3: Bit Hacks. The problem is to place N queens\non a N × N chessboard so that no queen attacks another (i.e., no two queens in any row, column,\nor diagonal). Review the slides to get a feel for how the backtracking procedure works.\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nFor this homework, our recursive implementation in queens.c (only 12 lines of code!) looks\nfor all possible solutions and appends them to a list. It works for N = 8, a standard 8 × 8\nchessboard. We chose N = 8 for two reasons:\n1. Tractable number of solutions. N = 8 only has 92 distinct solutions. N = 16, for example,\nhas 14772512 distinct solutions.\n2. Simple board representation. The 8 × 8 squares of the chessboard fit into the bits of a\nuint64_t.\nLecture 3 represented the board as 3 bit vectors down, left, and right of size N, 2N - 1, and\n2N - 1, respectively. The implementation does better! It uses the same 3 bit vectors, but of size\nN, N, and N only. This result was invented by Tony Lezard and can be found in an email from\n1991:\nPath: gmdzi!unido!mcsun!uknet!slxsys!ibmpcug!mantis!tony\nFrom: to...@mantis.co.uk (Tony Lezard)\nNewsgroups: rec.puzzles\nSubject: Re: 8 Queens (NO *SPOILER*)\nMessage-ID: <eeFmBB1w164w@mantis.co.uk>\nDate: 18 Nov 91 18:47:49 GMT\nReferences: <1991Nov16.033939.70781@cs.cmu.edu>\nOrganization: Mantis Consultants, Cambridge. UK.\nLines: 40\nredm...@cs.cmu.edu (Redmond English) writes:\n> I wrote a little C program a while ago, which after exhaustively testing\n> every position with exactly one queen on each rank (taking about 1 minute\n> on an 8MHz 68000 machine), came up with 92 solutions.\nErk. Exhaustive testing? Count me out on that. The following program,\nwritten in ANSI C, uses a backtracking algorithm and gives the correct\nanswer instantaneously:\n#include <stdio.h>\nvoid try(int, int, int);\nstatic int count = 0;\nvoid main() {\ntry(0,0,0);\nprintf(\"There are \\%d solutions.\\n\", count);\n}\nvoid try(int row, int left, int right) {\nint poss, place;\n\n--\n6L\nHandout-7----Homework-4:-Reducer-Hyperobjects-\nif (row == 0xFF) ++count;\nelse {\nposs = ~(row|left|right) & 0xFF;\nwhile (poss != 0) {\nplace = poss & -poss;\ntry(row|place, (left|place)<<1, (right|place)>>1);\nposs &= ~place;\n}\n}\n}\nObPuzzle: Generalize the above to n queens.\nTony Lezard IS to...@mantis.co.uk OR tony\\%man...@uknet.ac.uk\nOR EVEN ar...@phx.cam.ac.uk if all else fails. Great! Kept my .sig down to two\nlines!\nWrite-up 1: (Bonus) HowLdoesLtheLcodeLsnippetLaboveLwork?LWhyLdoLyouLonlyLneedLN-bitsL\nforLeachLvector?L\nOurLqueens implementationLusesLthreeLbitLvectors,LwhereL0sLrepresentLavailableLsquaresLandL1sL\nrepresentLunavailableLsquares.LTherefore,LtheLdown,Lleft,LandLright startLasLallL0sLandLgetLfilledL\nwithL1sLasLtheLqueensLareLplaced.L\n3.1\nIt's a race, but no one is winning.\nCompileLandLrunL./queens.LRecordLtheLserialLexecutionLtime.L\nLet'sLseeLifLweLcanLgetLanyLspeedupsLfromLparallelization.L InLtheLqueens function,LaddL\ncilk_spawn andLcilk_sync keywordsLtoLparallelizeLtheLrecursiveLcallsLtoLqueens.LCompileLandL\nrunL./queens withLawsrun8.L\nWhenLrunningLaLCilkLprogram,LyouLmayLspecifyLtheLnumberLofLworkerLthreadsLyou'dLlikeLtoL\nuseLbyLsettingLtheLenvironmentLvariableLCILK_NWORKERS.LSpecifyingLtheLnumberLofLCilkLworkersL\ncanLbeLdoneLbyLaddingLexport CILK_NWORKERS=W (whereLW-isLanLintegerLgreaterLthanL0)LinLtheL\ncommandLlineLbeforeL./queens.LMostLofLtheLtime,LyouLwon'tLhaveLtoLexplicitlyLsetLthisLvariableL\nbecauseLitLdefaultsLtoLtheLnumberLofLcoresLonLtheLsystem.L ALgoodLplaceLtoLfindLCilk-relatedL\nreferenceLinformationLisLhttps://www.cilkhub.org.\nhttps://www.cilkhub.org.\ncilk.mit.edu.\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nWrite-up 2: What happened when you ran ./queens? Did parallelization do what you\nexpected?\nSomething went wrong. Let's use the Cilksan tool to detect the problem. Cilksan will take too\nlong to finish with the current build, so comment out the loop which runs run_queens I times in\nthe main function. Remember to uncomment the loop before recording performance numbers\nfor the remainder of the homework.\nNow compile with Cilksan. The clang option -fsanitize=cilk triggers compilation with\nCilksan, and you can build with it automatically by running make CILKSAN=1. The resulting\nbinary will run with Cilksan code built-in.\nWrite-up 3: Was Cilksan able to detect the problem? Describe the race condition briefly in\nwords, and report the relevant line numbers for the read and write involved in the race.\n3.2 Fixing the race\nRemove cilk_spawn and cilk_sync keywords from the queens function. Before re-parallelizing\nour code, we want to make any accesses to the list of solutions, board_list, thread-safe.\nHere's one strategy that doesn't require mutual exclusion locks: instead of passing in board_list\nto the recursive calls to queens, we can create a bunch of temporary BoardList objects and pass\nin a different one to each recursive thread. When the threads synchronize, all of the temporary\nlists will have been filled, and we can concatenate them all together with the original board_list.\nIn queens.c, implement\n05 void merge_lists(BoardList* list1, BoardList* list2);\nThe function should merge list2 into list1 and then reset list2 to be empty. For example,\nsay list1 has 3 nodes and list2 has 5 nodes. After a call to merge_lists(list1, list2), list1\nshould have 8 nodes and list2 should be empty. We'll use this call to concatenate several lists\ntogether. Remember to handle cases where one or both of the lists are empty and to update all 3\nfields of BoardList.\nWrite-up 4: What is the most efficient way to concatenate two singly linked lists? What is\nthe asymptotic running time of your merge_lists function?\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nNow, implement the strategy of passing in temporary lists to the recursive calls and merging\nthem at the end. Compile and run ./queens to make sure your serial code is still correct. If it is,\nre-parallelize the code with cilk_spawn and cilk_sync. Run Cilksan to verify that there are no\nraces. Then record the parallel execution time.\nWrite-up 5: Briefly explain your implementation. How does the parallel code perform\ncompared to the serial code? Try to explain any difference in performance.\nRecall that, in Homework 2, you coarsened the recursion of merge sort. Coarsen the recursion\nof queens so that if it hits a base case, then it uses the original implementation. Record the\nnew parallel execution time. Remember to uncomment the loop before recording performance\nnumbers for the remainder of the homework.\nWrite-up 6: What is the base case you used? How does the parallel code with coarsening\nperform compared to the parallel code without? Try to explain any difference in\nperformance. How does it perform compared to the serial code?\n3.3 Cilk reducers\nBackground\nCilk provides a unique programming construct called a reducer hyperobject, or reducer. Concep\ntually, a reducer is a variable that can be safely used by multiple Cilk strands running in parallel.\nIn a parallel execution of a program, the Cilk runtime maintains multiple views of a reducer.\nThe runtime eliminates the possibility of a race by ensuring that each view can be accessed by\nonly one Cilk strand at a time. After all parallel strands have been synchronized (i.e., after a\ncilk_sync), Cilk guarantees that all views of a reducer have been merged together into a single\nview. In Cilk, reducers are based on \"monoids,\" and thus Cilk guarantees that a parallel exe\ncution of a program updates a reducer and merge views in a way that is equivalent to a serial\nexecution of the same program.\nMore precisely, a monoid is an algebraic structure on a set of elements T with an associative\nbinary operator ⊕ and an identity element e. As a simple example, for an integer sum monoid,\nT is Z (the set of integers), ⊕ is + (integer addition), and the identity is e = 0.\nThe serial execution of a Cilk program typically performs a sequence of updates to a reducer\nX, which correspond to a sequence of ⊕ operations with elements xi drawn from T. For example,\nconsider a program with a reducer X with initial value x0, and suppose that a serial execution\nof the program performs a sequence of updates x1, x2,...,x7 to a reducer X. The serial execution\ncombines these updates one at a time, i.e., it produces a final value for X of\n(((((((x0 ⊕ x1) ⊕ x2) ⊕ x3) ⊕ x4) ⊕ x5) ⊕ x6) ⊕ x7) .\n(1)\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\nBecause ⊕ is associative and has an identity element e, however, other ways to combine these\nupdates also produce the same final result. For example, one could also combine updates as\nfollows:\n((((x0 ⊕ x1) ⊕ x2) ⊕ x3) ⊕ x4) ⊕ (((e ⊕ x5) ⊕ x6) ⊕ x7) ,\n(2)\nand still produce the same result. The original serial execution order in Equation (1) uses a single\nview of X, updates occur sequentially. In contrast, the execution represented by Equation (2) uses\ntwo views, one for value (x0 ⊕ x1 ⊕ x2 ⊕ x3 ⊕ x4), and one for the value (x5 ⊕ x6 ⊕ x7). Having\ntwo views allows two processors to update X in parallel without races.\nMore generally, a parallel execution may create more reducer views and combine them in\nmore complicated but equivalent ways. For example, Equation (3) uses three views, and Equa\ntion (4) uses five views.\n(((x0 ⊕ x1) ⊕ x2) ⊕ ((e ⊕ x3) ⊕ x4)) ⊕ (((e ⊕ x5) ⊕ x6) ⊕ x7)\n(3)\n((x0 ⊕ ((e ⊕ x1) ⊕ x2)) ⊕ ((e ⊕ x3) ⊕ x4)) ⊕ (((e ⊕ x5) ⊕ (e ⊕ x6)) ⊕ x7)\n(4)\nIn summary, reducers exhibit several attractive properties:\n- Multiple strands can access a reducer without races.\n- Reducers are shared without the need for mutual exclusion locks.\n- Reducers can be used without significantly restructuring existing code.\n- Defined and used correctly, reducers retain serial semantics. The result of a Cilk program\nthat uses reducers is the same as the serial version, and the result does not depend on the\nnumber of processors or how the work is scheduled.\n- Reducers are implemented efficiently, incurring little or no runtime overhead for synchro\nnization. The cost of accessing a reducer is a hash-table lookup.\nImplementing your own reducer\nCilk comes with a library of predefined reducers which support many commonly used monoids.\nHere's an example usage of the + reducer:\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\n06 #include <cilk/reducer_opadd.h>\n08 // ...\nCILK_C_REDUCER_OPADD(r, double, 0);\nCILK_C_REGISTER_REDUCER(r);\ncilk_for(int i = 0; i != n; ++i) {\nREDUCER_VIEW(r) += A[i];\n}\nprintf(\"The sum of the elements of A is %f\\n\", REDUCER_VIEW(r));\nCILK_C_UNREGISTER_REDUCER(r);\nSince the list of boards queens() generates is not ordered, we can use a reducer to assemble\nit.\nWrite-up 7: How can a BoardListReducer help you to avoid creating the temporary lists in\nSection 3.2? Define the monoid: What type of objects will the reducer operate on? What is\nthe associative binary operator? What is the identity object?\nUnfortunately, Cilk did not come with a BoardListReducer, so let's implement our own in 6 easy\nsteps:\n1. Include the Cilk reducer header in your code:\n16 #include <cilk/reducer.h>\n2. Define the behavior of the reducer in 3 functions:\n17 // Evaluates *left = *left OPERATOR *right.\n18 void board_list_reduce(void* key, void* left, void* right) { ... }\n20 // Sets *value to the the identity value.\n21 void board_list_identity(void* key, void* value) { ... }\n23 // Destroys any dynamically allocated memory. Hint: delete_nodes.\n24 void board_list_destroy(void* key, void* value) { ... }\nRemember to cast the void* arguments to BoardList* before using them. You don't have to\nworry about the key argument (although you might see why it's there, since we previously\nmentioned a hash-table lookup).\n\nHandout 7 -- Homework 4: Reducer Hyperobjects\n3. Instantiate the reducer type:\n25 typedef CILK_C_DECLARE_REDUCER(BoardList) BoardListReducer;\n4. Initialize the reducer for use (it's easiest to put it in global scope):\n26 BoardListReducer X = CILK_C_INIT_REDUCER(BoardList,\n// type\nboard_list_reduce, board_list_identity, board_list_destroy,\n// functions\n(BoardList) { .head = NULL, .tail = NULL, .size = 0 });\n// initial value\n5. You must register and unregister the reducer before and after use. After the declaration of\nthe reducer, but before the first use, insert CILK_C_REGISTER_REDUCER(X) to register. When\nthe reducer is no longer needed, insert CILK_C_UNREGISTER_REDUCER(X) to unregister.\n6. Use REDUCER_VIEW(X) to access the value. In this case, it will return a BoardList.\n7. Once all parallel execution has completed, use X.value to query the final value of the\nreducer.\nThe full details are at:\nhttp://software.intel.com/en-us/articles/using-an-intel-cilk-plus-reducer-in-c\nYou can find a good tutorial with a working example at:\nhttps://www.cilkplus.org/docs/doxygen/include-dir/page_reducers_in_c.html\nWrite-up 8: Use a reducer to parallelize queens. Record the parallel execution time using\nawsrun8 ./queens. Verify that the answers you're getting are consistent with the serial code\nfrom before. Note: Cilksan may report races if you do not explicitly call\n__cilksan_disable_checking() and __cilksan_enable_checking() before and after\naccesses to the reducer object. How does the parallel code with reducers perform\ncompared to the parallel code without?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 5: Theory of Performance Engineering",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cd57dbdecbce3bd4da39d620c1e61d14_MIT6_172F18hw5.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 9\nHomework 5: Theory of Performance Engineering\nIntroduction\nThe focus of this problem set is on the theoretical side of the material taught in class and as well\nserves as a warm-up exercise for the quiz. This problem set should be done individually. Please\ntreat it like a take-home quiz. Do not discuss problems with classmates, and clearly cite any\nexternal resources you use (e.g., books, published papers, wikipedia).\nAs preparation for the homework and quiz, read Chapter 27 of Introduction to Algorithms by\nCormen, Leiserson, Rivest, and Stein (CLRS).\nPlease submit a write-up of your answers on the class site.\n1 Greedy scheduling\nWrite-up 1: Solve Exercise 27.1-3 on Page 791 in Chapter 27 of CLRS.\nBen Bitdiddle measures his deterministic program on 4 and 64 processors on an ideal parallel\ncomputer using a greedy scheduler. He obtains running times of T4 = 100 seconds and T64 = 10\nseconds.\nIn the previous problem, we proved that any greedy scheduler schedules a computation with\nwork T1 and span Tinf in time TP ≤ (T1 - Tinf)/P + Tinf on a P-processor ideal parallel computer.\nWe also showed in lecture that TP ≥ T1/P (Work Law) and TP ≥ Tinf (Span Law). Based on these\nformulas, please answer the following questions.\nWrite-up 2:\n1. What is the lowest possible value for the parallelism of the program?\n2. What is the highest possible value for the parallelism of the program?\n\nHandout 9 -- Homework 5: Theory of Performance Engineering\nBen Bitdiddle measures the running time of his deterministic parallel program scheduled\nusing a greedy scheduler on an ideal parallel computer with 4, 10, and 64 processors. Ben\nobtains the following running times:\nT4 : 80\nT10 : 42\nT64 : 9\nWrite-up 3: Argue that Ben messed up at least one of his measurements.\nWrite-up 4: Solve Exercise 27.1-5 on Page 791 in Chapter 27 of CLRS.\n2 Multithreaded matrix multiplication\nWrite-up 5: Solve Exercise 27.2-5 on Page 796 in Chapter 27 of CLRS.\nWrite-up 6: Solve Exercise 27.2-6 on Page 797 in Chapter 27 of CLRS.\n3 Multithreading reductions and prefix computations\nWrite-up 7: Solve Problem 27-4 on page 807 in Chapter 27 of CLRS.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 6: Custom Memory Allocators",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/e8e29570ad0673ec3460af46536a1e04_MIT6_172F18hw6.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 10\nHomework 6: Custom Memory Allocators\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\n1 Introduction\nProject 3 requires you to examine the complex real-world problem of high-performance memory\nmanagement. You will implement a serial memory allocator that implements the malloc(),\nfree(), and realloc() functions (the C memory management API). In this homework, you will\nimplement different versions of such an allocator.\nThen, you will explore extensions to the memory management API for cases in which a cus\ntom memory allocator is useful. In particular, you will complete implementations for a \"wrapped\nallocator,\" a \"packed allocator,\" a \"fixed aligned allocator,\" and a \"smart allocator.\"\nYou should start installing OpenTuner immediately; it will take about 10 minutes to install,\nand starting it before you look at the code will let you have it ready once you need it. Please\nensure that you are logged into your Amazon VM.\n$ sudo apt-get install python-setuptools\n$ sudo easy_install pip\n$ sudo ./install_opentuner.sh\nMore detailed instructions are in the README. You can open up another ssh connection to your\nVM to work with while OpenTuner is being installed.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nPerformance\nUnlike in previous assignments, you will be evaluating your custom allocators in terms of space\nutilization and throughput.\n- Space utilization is the peak ratio between the aggregate amount of currently allocated\nmemory (M) (i.e., allocated via your custom malloc() and not yet freed via your custom\nfree()) and the size of the heap (H) used by your allocator. The optimal ratio is, of course, 1.\nThe space utilization U is calculated as follows:\nU = max{M,40KB} /max{H,40KB}\n- Throughput is the average number of operations completed per second.\nTo summarize these two performance metrics for your allocator, we define a performance index\nP to be a weighted sum of the space utilization and throughput:\nP = wU + (1 - w) min{1,T/Tlibc}\nwhere U is your space utilization, T is your throughput, and Tlibc is the estimated throughput of\nlibc's malloc() on your system on the default traces.\nRunning the code\nYour allocator implementations will be tested on traces, which are text files that encode a series\nof calls to malloc() and free(). You can use the provided mdriver program to test and evaluate\nyour custom allocator on a given trace. Here is an example of how to compile and run the driver\nprogram on a particular trace:\n$ make clean mdriver; awsrun ./mdriver -g -v -B -f traces/trace_c0_v0\nYou can also run the driver on all traces in the traces directory as follows:\n$ awsrun ./mdriver -g -v -B\nThe mdriver program accepts the following command-line arguments:\n- -t <tracedir>: Look for the default trace files in directory <tracedir> instead of the default\ndirectory (./traces).\n- -f <tracefile>: Use one particular trace file for testing instead of the default set of trace\nfiles.\n- -h: Print a summary of the command line arguments.\n- -l: Run and measure libc's malloc() in addition to the custom malloc() implementation.\n- -g: Generate summary info for the autograder.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\n- -v: Verbose output. Print a performance breakdown for each trace file in a compact table.\n- -V: More verbose output. Prints additional diagnostic information as each trace file is\nprocessed. Useful during debugging for determining which trace file is causing you to fail.\n- -B: Use the custom \"simple allocator.\"\n- -W: Use the custom \"wrapped allocator.\"\n- -P: Use the custom \"packed allocator.\"\n- -F: Use the custom \"fixed aligned allocator.\"\n- -S: Use the custom \"smart allocator.\"\n2 Code layout\nThis homework uses code that resembles the code that you will use for Project 3. Let's review\nthe functions whose implementations you will complete to implement your custom allocators, as\nwell as some methods that you will use in those implementations.\nHeap memory allocator interface\nYour storage allocators will implement different versions of init(), malloc(), and free() using\nvarious allocation strategies. These functions are described below and (among other functions)\nare declared in allocator_interface.h. The specific versions of the functions to implement and\nmodify are specified in each question.\n- int init(void);\nBefore calling the corresponding malloc() or free(), the application program (i.e., the\ntrace-driven driver program that you will use to evaluate your implementation) calls init().\nYou may use this function to perform any necessary initialization, such as allocating the\ninitial heap area. The return value should be -1 if there was a problem in performing the\ninitialization and 0 if everything went smoothly. The specific versions you will encounter\nin this homework are\nint simple_init(void);\nint wrapped_init(void);\nint packed_init(void);\nint fixed_aligned_init(void);\nint smart_init(void);\n- void *malloc(size_t size);\nThis call must return a pointer to a contiguous block of newly allocated memory which is at\nleast size bytes long. This entire block must lie within the heap region and must not overlap\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nany other currently allocated chunk. The pointers returned by malloc() must always be\naligned to 8-byte boundaries; you'll notice that the libc implementation of malloc() does\nthe same. If the requested size is zero or an error occurs and the requested block cannot be\nallocated, a NULL pointer must be returned. The specific versions you will encounter in this\nhomework are\nvoid *simple_malloc(size_t size);\nvoid *wrapped_malloc(size_t size);\nvoid *packed_malloc(size_t size);\nvoid *fixed_aligned_malloc(size_t size);\nvoid *smart_malloc(size_t size);\n- void free(void *ptr);\nThis call notifies your storage allocator that a currently allocated block of memory should\nbe deallocated. The argument must be a pointer previously returned by malloc() and\nnot previously freed. You are not required to detect or handle either of these error cases.\nHowever, you should handle freeing a NULL pointer - it is defined to have no effect. The\nspecific versions you will encounter in this homework are\nvoid simple_free(void *ptr);\nvoid wrapped_free(void *ptr);\nvoid packed_free(void *ptr);\nvoid fixed_aligned_free(void *ptr);\nvoid smart_free(void *ptr);\nAll of this behavior matches the semantics of the corresponding libc routines. Type man malloc\nat the shell to see additional documentation, if you're curious.\nThe provided memory allocator in simple_allocator.c is very fast. On simple_malloc(), it\nincreases the heap size and returns the newly allocated memory, while on simple_free(), it does\nnothing. Compile and run it using mdriver on rec_traces/trace_c0_v0. Unsurprisingly, the\nreference allocator has nearly 0% space utilization (because it doesn't reuse freed memory) and\n100% throughput.\nSupport routines\nThe code in memlib.c simulates the memory system for your dynamic memory allocators. You\ncan invoke the following functions in memlib.c:\n- void* mem_sbrk(int incr);\nExpands the heap by incr bytes, where incr is a positive non-zero integer and returns a\ngeneric pointer to the first byte of the newly allocated heap area. The semantics are identical\nto the Unix sbrk() function, except that mem_sbrk() accepts only a positive non-zero integer\nargument.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\n- void* mem_heap_lo(void);\nReturns a generic pointer to the first byte in the heap.\n- void* mem_heap_hi(void);\nReturns a generic pointer to the last byte in the heap.\n- size_t mem_heapsize(void);\nReturns the current size of the heap in bytes.\n- size_t mem_pagesize(void);\nReturns the system page size in bytes (4 KB on Linux systems).\nIn addition to these functions, there are several macros defined in allocator_interface.h to\nhelp you implement your custom allocators.\n3 Fixed-size blocks\nTo improve the space utilization, let's implement the fixed-size allocation strategy from Lecture\n11: Storage Allocation. In this strategy, all blocks are allocated with the same size. The allocator\nuses a free list to track the freed blocks. The free list can be implemented as a singly linked list,\nwith the next pointers stored inside the freed blocks.\nFor this first part, assume that the fixed block size that we use is 1024 bytes. Add the following\nlines to the top of simple_allocator.c:\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 1024 // default value\n#endif\nYou should use the variable BLOCK_SIZE in your code rather than hardcoding 1024 as you will\nbe modifying BLOCK_SIZE in later parts. Notice that next to each block, the provided memory\nallocator stores the size of the block because it is needed for memory reallocation. Even though\nyou are not using any reallocation functionality in this homework (that will be in Project 3!), you\ncan choose whether or not you still want to store the size next to each block (which requires\nallocating slightly extra memory).\nCheckoff Item 1: Implement the fixed-size block allocation strategy in simple_allocator.c\nby modifying simple_init(), simple_malloc(), and simple_free() as necessary. Report that\nthe space utilization (and score) increases to over 99% when run on\nrec_traces/trace_c0_v0. Hint: Make a struct for the nodes of the free list, and remember\nto initialize the head in simple_init(). Use the -B flag when you run mdriver.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\n4 Autotuning\nUnlike rec_traces/trace_c0_v0, trace rec_traces/trace_c1_v0 only requests memory of a fixed\nsize BLOCK_SIZE = 4096. Your allocator should support all allocation sizes less than or equal to\nBLOCK_SIZE. Add the following lines to the top of the simple_malloc() function in simple_allocator.c:\nif (size > BLOCK_SIZE)\nreturn NULL;\nelse // size <= BLOCK_SIZE\nsize = BLOCK_SIZE;\nThe simple_malloc() function is now able to handle all sizes less than or equal to BLOCK_SIZE,\nand it returns an error (as a NULL pointer) for sizes greater than BLOCK_SIZE. (You may argue\nthat this error checking shouldn't be necessary in simple_malloc(): someone else should check\nthat it's broken if it returns a block of size BLOCK_SIZE when a size greater than BLOCK_SIZE was\nrequested. We agree, and that will be your job in Project 3 when you fill in validator.h.) Confirm\nthat there is an error when you recompile and run on rec_traces/trace_c1_v0.\n$ make clean mdriver; ./mdriver -g -v -B -f rec_traces/trace_c1_v0\nNext, let's manually override BLOCK_SIZE and confirm that there is no longer an error.\n$ make clean mdriver PARAMS=\"-D BLOCK_SIZE=4096\"\n$ ./mdriver -g -v -B -f rec_traces/trace_c1_v0\nBLOCK_SIZE is an example of a tunable parameter of the code, but how do we determine\nthe best value for BLOCK_SIZE? In this case, we could easily determine the value by manually\ninspecting the traces, but in general, it can be very difficult to tune these parameters by hand,\nparticularly when there are multiple different parameters (as you will see in Project 3). This is\nwhere autotuning is useful.\nOpenTuner is a autotuning tool that, by running an optimization, automatically finds the best\nvalues for the parameters that you tell it about. You will use OpenTuner next to see if you can\nautomatically determine the appropriate values for BLOCK_SIZE on different traces.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nCheckoff Item 2: Use OpenTuner to find the best value of BLOCK_SIZE for\nrec_traces/trace_c0_v0 and rec_traces/trace_c1_v0.\n1. Add BLOCK_SIZE as a power-of-two parameter in opentuner_params.py, varying it from\n25 to 215.\n2. Run the OpenTuner script (which takes 1-2 minutes):\n$ ./opentuner_run.py --test-limit=300 --no-dups --display-frequency=20 \\\n--trace-file=<trace-file>\nHow did OpenTuner know that BLOCK_SIZE should be 1024 and 4096, respectively? Is\nit just really good at reading traces, or is the value of BLOCK_SIZE somehow affecting\nthe value of the optimization's objective function?\n3. Add a target autotune to your Makefile so that you can run OpenTuner by running\n$ make clean autotune TRACE_FILE=<trace_dir>/<tracefile>\nHint: Your Makefile target needs to run the opentuner_run.py script with the\nnecessary flags shown above, as well as the trace file passed in on the command line.\nCheckoff\nCommit your changes to your local repository, then verify your work using verifier.py and\ncheck your code quality by running clint.py. If these scripts pass, show your work to a TA or\nUTA to complete the checkoff for the recitation.\n5 Cache-friendly allocation\nAligning objects on a cache-line boundary limits the number of cache lines needed to access an\nobject. For randomly accessed objects, ensuring that an access uses the fewest cache lines possible\nis especially important.\nIn implementing the allocators in this section, assume that objects never need to be freed with\nwrapped_free(). You can evaluate your allocators empirically on traces/trace_c0_nofree. Any\nexisting allocator can be used to ensure that objects start at a cache-line boundary with sufficient\npadding.\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nWrite-up 1: In wrapped_allocator.c, wrap the call to unaligned_malloc() inside of\nwrapped_malloc() to ensure that each object starts at a cache-line boundary. The wrapper\ncode cannot assume anything about the state of unaligned_malloc(). The macros in\nallocator_interface.h may prove useful. From analyzing your allocator's code and\nlooking at its utilization when run through mdriver, argue about how much memory is\nwasted for aligned allocations. Use the -W flag when you run mdriver.\nAlthough each object will need fewer cache lines, inefficient cache utilization can lead to more\ncache loads overall.\nNow consider allocating memory in a more cache-friendly way that allows for a more com\npact packing. In other words, you can pack multiple objects into a single cache-line. Even though\nsome of your objects may no longer be cache-aligned, don't forget that objects still need to be\n8-byte aligned. The allocator in packed_allocator.c is targeting a single-threaded workload and\naims to ensure that each object spans a minimal number of cache lines. The provided memory\nallocator stores the size of a block next to that block. Let's keep this block header and think about\nwhere we need to store it in relation to the pointer returned to the caller and what we might need\nto store in it.\nWrite-up 2: Implement packed_malloc() in packed_allocator.c. Where did you allocate the\nblock header? Report the utilization and performance scores. Based on analyzing your code\nand running it through mdriver, how much memory is wasted overall by your aligned\nallocator? Use the -P flag when you run mdriver.\n6 Allocator overheads\nWrite-up 3: Do we need to allocate the header in packed_allocator.c if, instead of the\nclassic free(p) interface, programmers were in charge of passing the original size as in\nfree(p, size)?\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nWrite-up 4: Assume that we have only one object size for our next allocator: 64 bytes. Do\nwe need to allocate a size header at all? Fill in the fixed_aligned_init(),\nfixed_aligned_malloc(), and fixed_aligned_free() methods in\nfixed_aligned_allocator.c to implement a cache-aligned fixed-size allocator. (In particular,\nmake sure you implement a free list this time.) Show the utilization and performance of\nyour allocator. Use the -F flag when you run mdriver.\nNow assume that we need to support large (64-byte) and small (32-byte) object sizes. We still\nwish for these objects to be cache-aligned, that is, at least one endpoint of each object must lie\non a cache-line boundary. Although we could use large allocations for all objects, this internal\nfragmentation can double the memory requirements. Can we support allocation and deallocation\nof two sizes with zero space overhead?\nLet's implement these ideas in smart_allocator.c. Suppose that you can change the interface\nfor your allocator such that any pointer returned by smart_malloc() needs to be accessed through\nthe SMART_PTR() macro.\nWrite-up 5: Where and how can you store the size of each allocation? Following this\nassumption, the smart_free() implementation in smart_allocator.c uses the SMART_PTR()\nand IS_SMALL() macros to operate on a given pointer. Implement the SMART_PTR() and\nIS_SMALL() macro definitions in allocator_interface.h.\nWrite-up 6: Using a single allocator for both different sizes might still suffer from external\nfragmentation. If the small and large objects need to be cache-aligned, then we might need\nto waste space to allocate a 64 byte object on a cache line boundary. What would you do\nwith the \"wasted space\"? Implement alloc_aligned() and smart_malloc() in\nsmart_allocator.c, and show your utilization. (Use the -S flag with mdriver.)\nWrite-up 7: (No implementation required.) What can you do when you need to allocate a\n32-byte object when you have free-list entries of size 64? What would happen once you have\nrun out of space and keep breaking up large objects, but never coalesce small, adjacent\nobjects into large ones? What can you do when you have run out of space and need to\nallocate a 64-byte object when you have free list entries of size 32?\n\nHandout 10 -- Homework 6: Custom Memory Allocators\nWrite-up 8: (No implementation required.) Allocating space has to be done in contiguous\nregions of memory. Is it possible to coalesce two 32-byte objects into a 64-byte object if the\n32-byte objects are not adjacent in memory? How would you implement this coalescing if\npossible?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 7: Dynamic-Analysis Tools",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/1ca096c3f8a039f3b782479c1deca8a4_MIT6_172F18hw7.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 12\nHomework 7: Dynamic-Analysis Tools\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\n1 Introduction\nKey to understanding and improving the behavior of any system is visibility -- the ability to\nknow what is going on inside the system. For application and system software, compiler-inserted\nprogram instrumentation (or simply compiler instrumentation) -- where the compiler inserts\nspecial code into the program-under-test to monitor its execution -- has emerged as a popular\nway for programmers to gain visibility into how their programs are operating. In your previous\nassignments, you have already availed yourself of a variety of dynamic-analysis tools, such as\nthe Cilksan race detector, Valgrind, AddressSanitizer, Cachegrind, and perf. These tools\ngenerally operate as shadow computations -- executing behind the scenes while the program\nunder-test runs. In this homework, you will investigate how to build your own dynamic-\nanalysis tool using compiler instrumentation.\nTraditionally, to write a dynamic-analysis tool that uses compiler instrumentation, a tool\nwriter must modify the source code of the compiler. But modifying a compiler requires sub\nstantial mastery of the compiler's source code that is beyond the scope of this course. In this\nhomework, you will use CSI, a prototype framework supported by the Tapir/LLVM compiler for\nwriting dynamic-analysis tools.\nThe CSI framework\nCSI aims to simplify many aspects of writing and using tools that employ compiler instrumenta\ntion by providing comprehensive static instrumentation and removing the need for tool writers to\ninteract directly with the compiler codebase. Specifically, CSI provides a simple application\nprogram interface (API) consisting of functions, called hooks, which are automatically inserted\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nthroughout the compiled code of the program-under-test. The CSI API exposes generic instru\nmentation points throughout the program-under-test, thereby hiding the complexity of the com\npiler's codebase from tool writers.\nUsing CSI, tool writers insert their own instrumentation into the program-under-test by writ\ning a library, called a CSI-tool, that defines the semantics of relevant hooks. All unimplemented\nhooks in the CSI-tool default to null behavior. A tool user uses the CSI framework to link the\ncompiled CSI-tool with the program-under-test to produce a tool-instrumented executable (TIX).\nWhen the TIX executes, the program-under-test runs normally, except that whenever a hook is\ninvoked, the tool performs its shadow computation. In order to support a wide variety of tools,\nCSI inserts hooks to instrument many events that a tool might care about, for instance, before\nand after a memory access, function entry and function exit, beginning and end of a basic block,\netc.\nFigure 1 presents CSI-cov, an example tool written using the CSI framework. CSI-cov reports\ncode-coverage information for an execution of the TIX of a program-under-test. As the TIX\nexecutes, CSI-cov records when each basic block runs. When the TIX terminates, CSI-cov reports\nhow many times each block was executed, including its location in the source code. The code\nfor CSI-cov, shown in Figure 1 in its entirety, is both short and simple.1 In just 40 lines, the CSI\nframework allows this useful compiler-based tool to be implemented as a simple C library.\nCSI targets program points that are visible in LLVM's intermediate representation (IR). In\nparticular, CSI instruments six categories of IR objects: memory loads, memory stores, basic\nblocks, function entry points, function exits, and call instructions.\nThe design of the CSI API is centered around two main considerations: (1) to allow for simple\nlibrary-based implementations of a wide-variety of dynamic-analysis tools, and (2) to enable\ntools to run quickly by exploiting standard compiler optimizations and analyses for precision\nand efficiency. To these ends, the CSI API provides four key features: hooks, flat and compact\nCSI ID spaces, forensic tables, and properties. We overview each of the features in turn.\nFlat CSI ID spaces. To each IR object, CSI assigns a CSI ID, an integer identifier for the IR\nobject that is unique within its category. At runtime, these ID's are passed to hooks to identify\nthe particular IR object or objects being instrumented. CSI assigns these ID's in such a way as to\nmaintain a flat and compact ID space for each IR-object category, even when translation units\n-- components of a program that are separately compiled -- are dynamically loaded. These flat\nand compact ID's greatly simplify tool design by allowing a tool to use simple arrays, rather\nthan hash tables, to store information for each IR object. Such arrays are simpler to maintain\nthan hash tables, especially in multithreaded programs, and they often perform more efficiently\nthan hash tables.\nHooks. CSI defines two kinds of hooks: initialization hooks and IR-object hooks. The ini\ntialization hooks allow tools to perform actions immediately before main is invoked and when a\ntranslation unit is loaded. The IR-object hooks allow tools to perform actions before and/or after\nan IR object is executed at runtime. The compiler automatically elides any hooks left undefined\nby the tool writer.\n1This code as written assumes that the program-under-test executes serially. Additional care must be taken for\nallocating, initializing, and accessing block_executed in order for CSI-cov to handle a multithreaded program-under\ntest. Thread-safety is an issue that tool writers must address explicitly.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n01 #include <stdio.h>\n02 #include <stdlib.h>\n03 #include <csi.h>\nlong *block_executed = NULL;\n06 csi_id_t num_basic_blocks = 0;\n08 void report() {\ncsi_id_t num_basic_blocks_executed = 0;\nfprintf(stderr, \"CSI-cov report:\\n\");\nfor (csi_id_t i = 0; i < num_basic_blocks; i++) {\nif (block_executed[i] > 0)\nnum_basic_blocks_executed++;\nconst source_loc_t *source_loc = __csi_get_bb_source_loc(i);\nif (NULL != source_loc)\nfprintf(stderr, \"%s:%d:%d executed %d times\\n\",\nsource_loc->filename, source_loc->line_number,\nsource_loc->column_number, block_executed[i]);\n}\nfprintf(stderr, \"Total: %ld of %ld basic blocks executed\\n\",\nnum_basic_blocks_executed, num_basic_blocks);\nfree(block_executed);\n23 }\nvoid __csi_init() {\natexit(report);\n27 }\n29 void __csi_unit_init(const char * const name,\nconst instrumentation_counts_t counts) {\nblock_executed = (long *)realloc(block_executed,\n(num_basic_blocks + counts.num_bb) *\nsizeof(long));\nmemset(block_executed + num_basic_blocks, 0, counts.num_bb * sizeof(long));\nnum_basic_blocks += counts.num_bb;\n36 }\n38 void __csi_bb_entry(const csi_id_t bb_id, const bb_prop_t prop) {\nblock_executed[bb_id]++;\n}\nFigure 1: A serial version of the CSI-cov code-coverage tool, which reports the number of times every\nbasic block in a TIX is executed.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nForensic tables. In addition to hooks for IR objects, the CSI API defines forensic tables which\nstore static information created by the compiler. A CSI-tool can access these tables at runtime\nthrough abstract accessor functions. The prototype CSI implementation in Tapir/LLVM provides\nforensic tables that associate IR objects with locations in the source code, as well as tables that\nspecify the number of LLVM IR instructions in each basic block.\nThe code of CSI-cov in Figure 1 demonstrates that CSI ID's, hooks, and forensic tables can\nbe used in a tool. CSI-cov maintains a table block_executed, defined in line 5, in which each\nbasic block in the TIX has a unique slot indexed by the basic block's CSI ID. The number of basic\nblocks is stored in the variable num_basic_blocks, defined in line 6.\nThe initialization hook __csi_init is invoked before main of the program-under-test is called.\nThis hook, defined in lines 25-27, registers the function report, defined in lines 8-23, to run\nwhen the program terminates. The report function prints out a report summarizing the results\nof the tool's analysis. For each basic block, report retrieves its source location from CSI's forensic\ntables using the accessor function __csi_get_bb_source_loc in line 14. It prints the file name,\nthe initial source line, and the number of times the basic block has been executed in lines 16-18.\nFinally, report prints the total number of basic blocks executed and the total number of basic\nblocks in the program in lines 20-21.\nThe hook __csi_unit_init is invoked whenever a new translation unit is loaded, either stati\ncally or dynamically. The hook is called with the name of the translation unit and a structure that\nprovides the number of IR objects in each IR-object category that the translation unit contains.\nCSI-cov defines this hook in lines 29-36 to reallocate the array block_executed to incorporate the\nnew basic blocks in lines 31-33, after which it updates the number num_basic_blocks of basic\nblocks in line 35.\nThe hook __csi_bb_entry, defined in lines 38-40, is called every time a basic block is executed.\nThe hook has two arguments: the CSI ID of the basic block and a \"property,\" which CSI-cov\nignores. At runtime, CSI-cov indexes block_executed and increments the counter for that basic\nblock.\nProperties. The one key feature of CSI that the CSI-cov example does not illustrate is the\nuse of properties. For each IR object, CSI computes a property, which is a compile-time constant\nthat encodes the results of standard compiler analyses. The property for a memory operation,\nfor example, specifies that the location is guaranteed to be on the stack and has a particular\nalignment. The CSI framework uses properties to optimize the inserted instrumentation. In\nparticular, if a CSI-tool branches based on the value of a property, CSI can constant-fold the test\nand optimize the instrumentation accordingly.\nAs an example of how a CSI tool might use properties, imagine that a tool writer builds a race\ndetector capable of detecting races on shared variables. If she were using conventional compiler\ninstrumentation, she could avoid instrumenting locations that could not possibly be involved in a\nrace, such as a variable declared const or a variable on the stack whose address does not escape\nthe frame. In CSI, the property argument to an IR-object hook can give her access to similar\nspecific compile-time information concerning the memory operation being instrumented.\nFigure 2 shows a snippet of the code that the writer of a race-detection tool might produce.\nIn defining the hook __csi_before_load, the code safely skips the instrumentation of a load\nif it satisfies certain criteria: it accesses a const value (line 45), it accesses a memory location\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n41 void __csi_before_load(const csi_id_t load_id,\nconst void *addr,\nconst int32_t num_bytes,\nconst load_prop_t prop) {\nif (prop.is_const ||\nprop.is_not_shared ||\nprop.is_read_before_write_in_bb)\nreturn;\ncheck_race_on_load(addr, num_bytes);\n50 }\nFigure 2: An example memory-load hook for a race detector that uses properties to avoid unnecessary\ninstrumentation.\nwhose address is not captured and is thus guaranteed not to be shared (line 46), or it reads a\nmemory address that is written to within the same basic block without any intervening function\ncalls (line 47). Because these conditions are known at compile time, the CSI framework can\nevaluate the condition when it inserts this load hook before each memory load. As a result, the\nCSI framework will preclude the compiler from inserting runtime calls to __csi_before_load\nbefore loads that satisfy these conditions. The function is analyzed at compile time and executed\nat runtime only when necessary. If a given property holds at link time and a tool branches\ndepending on that condition, CSI can constant-fold the test and optimize the instrumentation.\nArchitecture of CSI\nAt first glance, CSI's brute-force method of inserting hooks at every salient location in the\nprogram-under-test seems replete with overheads. For example, the CSI instrumentation of a\nmemory operation involves calls to two hooks, one before the operation and one after. If a tool\ndoes not use these hooks, the cost of the function calls can contribute significantly and unneces\nsarily to runtime overhead.\nThe CSI prototype overcomes overheads through the use of modern compiler technology,\nspecifically, through the use of link-time optimization (LTO). Conceptually, LTO allows compiler\noptimizations to run when the program's compiled units are statically linked together, thereby\nallowing the compiler to perform optimizations between these separate units. Because all unim\nplemented hooks in a CSI-tool default to a null behavior, LTO can remove calls to these hooks\nas dead code. Furthermore, LTO can can constant-fold the test and optimize the instrumentation\nbased on property values and perform a variety of optimizations on the instrumentation code\nitself.\nFrom the point of view of the tool writer, CSI works as follows. The tool writer defines\nrelevant hooks for her CSI-tool, and then she statically links her tool with a null tool, which\nprovides a no-op implementation of every hook. Linking these two tools thus produces a null-\ndefault CSI-tool, where the functions defined in the tool writer's CSI-tool override the corre\nsponding functions in the null tool, but for hooks not defined in the tool writer's CSI-tool, the\nnull-hook definition defaults. When a tool user statically links a null-default CSI-tool with a\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nprogram-under-test, LTO automatically elides calls to null hooks during its optimization pass.\n2 Recitation: Analyzing function executions\nIn recitation, you will use CSI to develop tools to analyze the execution of functions in a serial\nprogram. You will first develop a simple tool to measure function coverage, that is, to count\nhow many times each function in the program executes. Time permitting, you will then extend\nthe function-coverage tool to compute a profile for the program-under-test, which specifies how\nmuch of the total execution time of the program-under-test is spent in each function. For this\nrecitation assignment, refer to Section 4 for documentation on the CSI API.\nThe file func-analysis.c provides a boiler-plate for the function-analysis CSI-tool. The check\noff items in this homework will walk you through completing the implementation of this CSI-tool.\nCheckoff Item 1: Add global variables to func-analysis.c for recording the number of\ntimes each function in the program executes. Add code to the __csi_init and\n__csi_unit_init instrumentation hooks to initialize these global variables.\nCheckoff Item 2: Add a function report to func-analysis.c for outputting the results of\nthe analysis. In particular, report should use CSI forensic tables to output the name of each\nfunction in the program and how many times that function was executed. Register the\nreport function to run at program termination by adding the statement atexit(report); to\n__csi_init.\nCheckoff Item 3: Which CSI IR-object hooks should be implemented to observe when each\nfunction is executed? Implement those IR-object hooks in func-analysis.c to record the\nnumber of times each function executes.\nCheckoff Item 4: Compile the function-analysis CSI-tool and null tool and instrument\nfib_serial with the function-analysis tool as follows:\n51 $ make CSI=1 FUNCANALYSIS=1 fib_serial\nFinally, run the instrumented fib_serial executable.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nCheckoff Item 5: Once you have confirmed that function-coverage CSI-tool is working, use\nit to instrument and analyze bzip2. We have already modified the Makefile for bzip2 to\noptionally make use of a specified CSI-tool. Hence, you can instrument bzip2 with your\nfunction-coverage CSI-tool and test the executable as follows:\n52 $ cd bzip2-1.0.6\n53 $ make test CSI=../func-analysis.o\n54 $ ./bzip2 -z -9 -c ../bzip2-1.0.6.tar > /dev/null\nCheckoff: Demonstrate to a TA that the function-coverage tool correctly counts the number\nof times each function in bzip2 is executed.\nCheckoff: (Time permitting.) Modify the function-coverage tool to profile each function, that\nis, to record the amount of the running time of the program is spent within each function. Try out\nthe tool on fib and on bzip2. Make sure the tool doesn't over-count the time spent in recursive\nfunctions! Demonstrate to a TA that the profiling tool works.\n3 Homework: Linear-regression analysis\nIn this homework, you will write some simple CSI-tools to perform linear-regression analysis.\nLinear regression is a linear approach to modelling the relationship between a scalar dependent\nvariable (in this case runtime) and one or more independent variables. The idea behind linear-\nregression analysis is to develop a simple, predictive linear model to describe the performance of\na program. Such a performance model provides guidance for how one can tune the performance\nof a program. There are many possible independent variables in a program that can affect it's\nrunning time, including:\n- Lines of code. Some lines of code are more expensive than others.\n- C language primitives. Some primitives are more expensive. Vectorizable instructions can\nmake things cheaper.\n- Machine instructions. Some instructions are more expensive than others e.g. square root com\npared to increment.\n- Machine instructions weighted by their cost. Some instructions vary in cost e.g. a load with a\ncache hit vs. a cache miss.\n- Cache Misses. TLB misses.\nIn this homework, we will guess that we can model the time T to run a program as following\nequation:\nT = a · I + b · C ,\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nwhere I is the number of instructions, C is the number of cache misses, and a and b are con\nstant values. Linear-regression analysis involves solving for the constants a and b in this model\nin two steps. First, one accumulates a variety of measurements of the program's running-time\nand corresponding instruction and cache-miss counts. Given those measurements, one can use\na program such as gnuplot or Excel to perform a linear-regression fit of this model to the mea\nsurements, thereby solving for the constants a and b in the model. The constant a describes the\ncost of an instruction, and the constant b describes the cost of a cache miss in the program.\nThis homework uses linear-regression analysis to measure the cost of two short-running pro\ngram events. First, you will measure the cost of a function call. Second, you will measure the cost\nof a cilk_spawn. You will study to programs, fib_serial and fib_cilk, to perform the linear-\nregression analyses in this homework. You can compile these programs without instrumentation\nas follows:\n55 $ make fib_serial fib_cilk\nAs with the recitation assignment, refer to the documentation of the CSI API in Section 4 when\nwriting CSI-tools for this homework.\nMeasuring the cost of a function call\nTo estimate the cost of a single function call, let us consider the serial recursive exponential-time\nfib function in the file fib_serial.c. This execution of this fib function is comprised of function\ncalls and integer arithmetic and logic. Hence, we can model the execution time T of this code\nusing the following linear-regression model:\nT = a · I + b · F ,\n(1)\nwhere I is the number of (non-function-call) instructions, F is the number of function calls, and a\nand b are constants. Using this model, we can estimate the cost of a function call by performing\na linear-regression analysis to solve for the constants a and b.\nTo perform this analysis, we need to observe several measurements of the running-time T of\ndifferent executions of fib that each perform different, uncorrelated numbers of instructions I\nand function calls F. To control the number of instructions and function calls that an execution\nof fib performs, the program fib_serial.c takes two parameters: the nth Fibonacci number to\ncompute, and a base-case size, which must be at least 2. The program then computes the nth\nFibonacci number using an exponential-time recursive routine until fib is recursively called on\nan input value n smaller than the base-case size. At that point, the routine uses a simple loop to\ncompute the requested Fibonacci number.\nWrite-up 1: Implement a CSI-tool in linreg.c to count the number of LLVM IR instructions\nand function calls that a program execution performs. You can get the number of LLVM IR\ninstructions in a basic block using __csi_get_bb_sizeinfo. Use the non-empty-size value\nfor the LLVM IR instruction count for a basic block. Submit your CSI-tool for this write-up.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nNote: The prototype CSI implementation currently contains a bug that affects its\ninstrumentation on simple recursive functions, such as fib. To get around this bug, declare\nall counter variables in your tool as volatile.\nOne issue with compiler instrumentation is that the instrumentation code can perturb the\nexecution of the program-under-test. In particular, a program instrumented with the CSI-tool\nfrom Write-up 1 will perform more operations than the original uninstrumented program. As a\nresult, the instrumented program might run more slowly than the uninstrumented program.\nWrite-up 2: Describe your strategy for collecting the necessary measurements to perform\nthis linear-regression analysis using timers (e.g., calls to clock_gettime) and the CSI-tool\nfrom Write-up 1. Your strategy should address the following questions:\n- What inputs will you use to run the program such that the counts of instructions and\nfunction calls are not highly correlated?\n- How will you measure the running time of the program? What timers will you use? How\nmany times will you run the program on each input? How will you aggregate your\nmeasurements to ensure you have reliable timings (e.g., using min, mean, or median)?\n- How will you accommodate the fact that program instrumentation can perturb the\nrunning time of the program?\nNow use your CSI-tool from Write-ups 1 and 2 to perform the linear-regression analysis.\nWrite-up 3: Following the strategy you outlined in Write-up 2, use timers and the CSI-tool\nfrom Write-up 1 to collect the running-time measurements and instruction and function-call\ncounts for the fib_serial.c program. You can compile the linear-regression CSI-tool and\ninstrument fib_serial with it as follows:\n56 $ make CSI=1 LINREG=1 fib_serial\nUse these measurements to perform the linear-regression analysis to solve for constants a\nand b in (1). You can use any data-analysis program you prefer (e.g., gnuplot or Excel) to\nperform the linear-regression fit on the collected measurements. What values does your\nanalysis find for the constants a and b? How correlated are the counts of instructions and\nfunction calls in your measurements?\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\nMeasuring the cost of a Cilk spawn\nNow you will use linear-regression analysis to measure the cost of a cilk_spawn statement. To\nperform this analysis, consider the Cilk program in fib_cilk.c, which computes Fibonacci num\nbers in exponential time. The serial running time of this program T1 can be modeled using the\nfollowing linear-regression model:\nT1 = a · I + b · S,\n(2)\nwhere I is the number of instructions (including serial function calls), S is the number of\ncilk_spawn's, and a and b are constants. Using this model, one can estimate the cost of a\ncilk_spawn by performing a linear-regression analysis to solve for the constants a and b. For\nthis analysis, we are primarily interested in solving for the constant b.\nTo solve (2) for the constant b, the fib_cilk.c program can be run using different numbers of\ncilk_spawn statements to compute a particular Fibonacci number. The fib routine in fib_cilk.c\ntakes a base-case size along with the value n specifying which Fibonacci number to compute.\nWhen fib is called on a value of n larger than the base-case size, then the first recursive call to\nfib is spawned using a cilk_spawn. Otherwise this recursive call is called serially.\nWrite-up 4: Modify the CSI-tool you implemented in Write-up 1 to additionally count the\nnumber of cilk_spawns executed by a program-under-test. The prototype CSI framework in\nthe Tapir/LLVM compiler inserts the following hook at the point in LLVM's IR\ncorresponding to a cilk_spawn:\nvoid __csi_detach(const csi_id_t detach_id);\nTo simplify the design of your tool, you can assume that you will run the\nprogram-under-test with CILK_NWORKERS=1. Submit your CSI-tool for this write-up.\nWrite-up 5: Mirroring the steps outlined in Write-ups 2 and 3, use the CSI-tool developed in\nWrite-up 4 to perform the linear-regression analysis to estimate the cost of a cilk_spawn.\nYou can compile the linear-regression CSI-tool and instrument fib_cilk with it as follows:\n03 $ make CSI=1 LINREG=1 fib_cilk\nWhen taking measurements for this analysis, run the fib_cilk executable using\nCILK_NWORKERS=1. Describe your strategy for taking the necessary timing measurements and\ninstruction and cilk_spawn counts. What values does your analysis compute for the\nconstant b? How correlated are the counts of instructions and spawns in your\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n04 typedef int64_t csi_id_t;\n05 // Value representing unknown CSI ID\n06 #define UNKNOWN_CSI_ID ((csi_id_t) -1)\n08 typedef struct {\ncsi_id_t num_func;\ncsi_id_t num_func_exit;\ncsi_id_t num_callsite;\ncsi_id_t num_bb;\ncsi_id_t num_load;\ncsi_id_t num_store;\n15 } instrumentation_counts_t;\n17 // Hooks to be defined by tool writer\n18 void __csi_init();\n19 void __csi_unit_init(const char * const file_name,\nconst instrumentation_counts_t counts);\nFigure 3: CSI hooks for initialization.\nmeasurements? Based on the linear-regression analysis from Write-up 3, what is the cost of\na cilk_spawn in terms of function calls?\n4 Reference: The CSI API\nThis section overviews the CSI API implemented in the Tapir/LLVM compiler. The CSI API\nconsists of four key features: CSI ID spaces, hooks, properties, and forensic tables. The API\nemploys flat and compact CSI ID spaces to identify IR objects. The API provides hooks that\nare invoked for IR objects, as well as hooks for initializing a CSI tool. The API exports results\nof standard compiler analysis through property parameters, allowing the tool writer to avoid\nunnecessary instrumentation. Forensic tables associate IR objects with locations in the source\ncode and relate IR objects to each other. This section describes each of these four key features.\nCSI ID's\nCSI identifies six categories of IR objects: function entries, function exits, basic blocks, call sites,\nloads, and stores. Each IR object is assigned a unique number, called a CSI ID, within each\ncategory. The CSI ID's are consecutively numbered from 0 up to 1 less than the number of IR\nobjects in the category. A CSI ID has type csi_id_t.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n21 void __csi_func_entry(const csi_id_t func_id,\nconst func_prop_t prop);\n23 void __csi_func_exit(const csi_id_t func_exit_id,\nconst csi_id_t func_id,\nconst func_exit_prop_t prop);\n27 void __csi_before_call(const csi_id_t callsite_id,\nconst csi_id_t func_id,\nconst call_prop_t prop);\n30 void __csi_after_call(const csi_id_t callsite_id,\nconst csi_id_t func_id,\nconst call_prop_t prop);\nFigure 4: CSI hooks for functions.\n33 void __csi_bb_entry(const csi_id_t bb_id,\nconst bb_prop_t prop);\n35 void __csi_bb_exit(const csi_id_t bb_id,\nconst bb_prop_t prop);\nFigure 5: CSI hooks for basic blocks.\nTool initialization\nCSI provides two initialization hooks, shown in Figure 3. The global initialization hook __csi_init\nexecutes exactly once immediately before the program-under-test invokes the main function and\nbefore it initializes global variables. The unit-initialization hook __csi_unit_init is executed\nonce whenever a translation unit is loaded into the TIX, whether statically or dynamically.\nIR object hooks\nCSI provides hooks for the each of the six categories of IR objects it identifies. To provide\nflexibility to tool writers, CSI generally inserts a hook both just before and just after each IR\nobject. This flexibility allows, for example, a memory location's value to be queried before a\nstore, after a store, or both.\nFunctions are instrumented on entry and exit. The hook __csi_func_entry is invoked at the\nbeginning of every instrumented function instance after the function has been entered and ini\ntialized but before any user code has run -- in LLVM terminology, at the first insertion point\nof the entry block of the function. The func_id parameter identifies the function being entered\nor exited. Correspondingly, the hook __csi_func_exit is invoked just before the function re\nturns (normally).2 Its parameters include both a function ID func_id and a function-exit ID\nfunc_exit_id, which allows tool writers to distinguish the potentially multiple exit points from\na function.\n2We have not yet defined the API for exceptions.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n37 void __csi_before_load(const csi_id_t load_id,\nconst void *addr,\nconst int32_t num_bytes,\nconst load_prop_t prop);\n41 void __csi_after_load(const csi_id_t load_id,\nconst void *addr,\nconst int32_t num_bytes,\nconst load_prop_t prop);\n46 void __csi_before_store(const csi_id_t store_id,\nconst void *addr,\nconst int32_t num_bytes,\nconst store_prop_t prop);\n50 void __csi_after_store(const csi_id_t store_id,\nconst void *addr,\nconst int32_t num_bytes,\nconst store_prop_t prop);\nFigure 6: CSI hooks for loads and stores.\nThe __csi_before_call and __csi_after_call hooks instrument call sites. The callsite_id\nparameter identifies the call site, and the func_id parameter identifies the callee -- the function\nbeing called. It is not possible at compile time to identify with certainty the callee of a call site if\nthe callee is called indirectly via a function pointer or if the callee is not instrumented by CSI. In\nthese cases, the func_id argument is set to UNKNOWN_CSI_ID, a macro defined by the CSI library,\nas shown in Figure 3.\nFigure 5 shows the two CSI hooks for basic blocks. The hook __csi_bb_entry is called when\ncontrol enters a basic block, and __csi_bb_exit is called just before control leaves the basic block.\nThe bb_id parameter identifies the basic block being entered or exited.\nFigure 6 shows the four CSI hooks for memory operations. The hooks __csi_before_load\nand __csi_after_load are called before and after memory loads, respectively, and likewise,\n__csi_before_store and __csi_after_store are called before and after memory stores. The\nargument addr is the location in memory, and num_bytes is the number of bytes loaded or stored.\nProperties\nEvery IR-object hook contains a property parameter prop: a bit-field struct that encodes static\ninformation for optimizing instrumentation. CSI defines a distinct bit-field struct type for ev\nery IR-object category: func_prop_t, func_exit_prop_t, call_prop_t, bb_prop_t, load_prop_t,\nstore_prop_t. In the prototype CSI implementation in Tapir/LLVM, Figures 7 and 8 presents the\nproperty struct types that have been implemented in the existing CSI prototype. As an example,\nconsider the load_prop_t type for loads, which is shown in Figure 8. The property encodes the\nalignment of the load (alignment) -- an integer k indicating that the address must be a multiple\nof k -- whether the load is volatile (is_volatile), whether the loaded location is guaranteed not\nto be shared (is_not_shared), etc.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n54 typedef struct {\n// The call is indirect.\nunsigned may_spawn : 1;\n// Pad struct to 64 total bits.\nuint64_t _unused : 63;\n59 } func_prop_t;\n61 typedef struct {\n// The function might spawn.\nunsigned may_spawn : 1;\n// Pad struct to 64 total bits.\nuint64_t _unused : 63;\n66 } func_exit_prop_t;\n68 typedef struct {\n// The function might spawn.\nunsigned is_indirect : 1;\n// Pad struct to 64 total bits.\nuint64_t _unused : 63;\n73 } call_prop_t;\nFigure 7: Definition of the CSI property for function entry points, function exits, and call sites.\nForensic tables\nIn addition to properties, CSI passes information known at compile time through its forensic\ntables. The forensic tables map CSI ID's to static information associated with the instrumented\nIR objects. CSI encapsulates the tables with a set of accessor functions, which allow the tool writer\nto map IR objects to their source-code locations, such as line numbers and containing file name,\nand to get instruction counts associated with each basic block. Figure 9 presents the accessors\nfor the forensic tables implemented in the CSI prototype implementation in Tapir/LLVM.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\ntypedef struct {\n// The alignment of the load.\nunsigned alignment : 8;\n// The loaded address is in a vtable.\nunsigned is_vtable_access : 1;\n// The loaded address points to constant data.\nunsigned is_constant : 1;\n// The loaded address is on the stack.\nunsigned is_on_stack : 1;\n// The loaded address cannot be captured.\nunsigned may_be_captured : 1;\n// The loaded address is read before it is written in the same basic block.\nunsigned is_read_before_write_in_bb : 1;\n// Pad struct to 64 total bits.\nuint64_t _unused : 51;\n} load_prop_t;\ntypedef struct {\n// The alignment of the store.\nunsigned alignment : 8;\n// The stored address is in a vtable.\nunsigned is_vtable_access : 1;\n// The stored address points to constant data.\nunsigned is_constant : 1;\n// The stored address is on the stack.\nunsigned is_on_stack : 1;\n// The stored address cannot be captured.\nunsigned may_be_captured : 1;\n// Pad struct to 64 total bits.\nuint64_t _unused : 52;\n} store_prop_t;\nFigure 8: Definition of the CSI property for memory loads and memory stores.\n\nHandout 12 -- Homework 7: Dynamic-Analysis Tools\n// Structure for the source-location information of an instrumented IR\n106 // object.\n107 typedef struct {\n// The name of the instrumented IR object, such as the name of the\n// function. This field is NULL if no name is found.\nchar * name;\nint32_t line_number;\nint32_t column_number;\nchar * filename;\n114 } source_loc_t;\n116 // Accessors for various CSI forensic tables that associate IR objects\n117 // with locations in the source code. These accessors return NULL\n118 // when given an invalid ID.\n119 const source_loc_t *__csi_get_func_source_loc(const csi_id_t func_id);\nconst source_loc_t *__csi_get_func_exit_source_loc(const csi_id_t func_exit_id);\n121 const source_loc_t *__csi_get_bb_source_loc(const csi_id_t bb_id);\n122 const source_loc_t *__csi_get_call_source_loc(const csi_id_t call_id);\n123 const source_loc_t *__csi_get_load_source_loc(const csi_id_t load_id);\n124 const source_loc_t *__csi_get_store_source_loc(const csi_id_t store_id);\n126 // Structure for the count of LLVM IR instructions within a basic\n127 // block.\n128 typedef struct {\n// Total LLVM IR instructions.\nint32_t full_ir_size;\n// Number of LLVM IR instructions that are implemented with at least\n// one instruction in a machine architecture, e.g., no debug\n// intrinsic function calls or phi instructions.\nint32_t non_empty_size;\n} sizeinfo_t;\n137 const sizeinfo_t *__csi_get_bb_sizeinfo(const csi_id_t bb_id);\nFigure 9: CSI accessor functions for reading forensic tables for source location information and for reading\nthe number of LLVM IR instructions in each basic block.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Homework 9: Deterministic Execution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/ddcc1a4046b1893ca38211dc54be44fc_MIT6_172F18hw9.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 14\nHomework 9: Deterministic Execution\nPlease answer the questions in this handout and submit an individual writeup.\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\n1 Introduction\nWe'll explore the power of deterministic execution, which may or may not help you debug your\nfinal project. Before we even get to parallel code, there will be plenty of bugs to root out in your\nserial code.\n2 Environment dependencies\nThe most entertaining bugs to debug are the ones that depend on the environment. Why would\na bug happen only on awsrun but not on your development machine? Why would code that\nworks great for you, occasionally crash for your partner, and always for your TA?\nLet's dig deeper into this simple program excerpt from undef.c.\n01 main() {\nint i;\nprintf(\"value of i=%d\\n\", i);\nprintf(\"address &i=%p\\n\", &i);\n05 }\nFigure 1: An undef.c excerpt\nThe Makefile contains a number of useful targets for this exercise. Whenever you use a target,\nyou may find it useful to examine the Makefile in detail and understand why certain targets have\nthe result they do.\nRun it a few times using make undef-compare and compare the results. Are they the same?\nWhy or why not? Now, let's fix these nondeterministic outputs line by line.\n\nHandout 14 -- Homework 9: Deterministic Execution\nWhenever your program is using undefined state, it's often going to produce non-deterministic\nresults. Undefined variables (or bits within one) are one such nondeterminism source. Fix the\ncode to define the variable and rerun make undef-compare to make sure that it worked.\nNext, we will look at nondeterminism in addresses. The higher order bits of a pointer are\nrandom due to Address Space Layout Randomization (ASLR) (look on Wikipedia for more de\ntails), which is an important security feature now in all modern operating systems. However,\nfor testing how it impacts your address-dependent nondeterministic program, you can run your\nprogram without ASLR with the command setarch x86_64 -R ./undef. Run make undef-noaslr\nto run the program a few times without ASLR. Is this program deterministic now?\nEven after considering these two factors, there are even more aspects that affect a nondeter\nministic program. Run make undef-env to run the command with differing environments. What\nis printed now? To see why, run make whoami and take a look at user.c :\n06 int main(int argc, char *argv[], char* envp[]) {\nchar *username=genenv(\"USER\");\n08 }\nFigure 2: An excerpt from user.c\nYou can see that the stack location is perturbed by the environment block, which affects the\nlower order bits of the stack variable i in undef.c.\nThus, we learned that program addresses are thus an important source of nondeterminism.\nThese are a derivative of environmental, time, and randomness sources beyond our control.\n3 Nondeterministic Hashtable\nNow let's see a more complicated example in hashtable.c. It has a mostly working implementa\ntion of an open address hashtable with linear probing. This technique is a lot more cache-friendly\ncompared to hashtables with linked lists.\nRun make hashtable1 a few times. It may work for you most of the time. How about\nmake hashtable10 or make hashtable1000?\nCheckoff Item 1: What do you need to make this program deterministic? Modify the\nMakefile target for hashtable1000-good so that all runs succeed.\nTo fix our bugs, we want the bugs to be reproducible.\n\nHandout 14 -- Homework 9: Deterministic Execution\nCheckoff Item 2: Modify hashtable1-bad target so the program always fails. Show your\nmodified Makefile target. You may find it useful to examine hashtable.c to see what system\narguments it takes.\n4 Replay Debugging\nTo fix serial code, you can use the GCC Record/Replay debugging facilities:\nhttp://www.sourceware.org/gdb/wiki/ProcessRecord/Tutorial\n(gdb) break main\n(gdb) run\n(gdb) record\n(gdb) continue\n(gdb) reverse-next\n(gdb) reverse-next\n(gdb) reverse-continue\nIf you don't like typing, you can just do\n(gdb) b main\n(gdb) r\n(gdb) rec\n(gdb) c\n(gdb) rn\n(gdb) rn\n(gdb) rc\nYou may find it useful to watch some variables when debugging:\n(gdb) set can-use-hw-watchpoints 0\n(gdb) watch some_variable_name\nYou don't necessarily have to use good tools--if you have plenty of time, you can always\nfigure out any bug by staring at the code long enough (static analysis by eyeballing). However,\na much easier approach is to run your code with asserts, printf statements, or replay debugging\ntechniques like gdb. Use any of the above techniques to find and fix the bug in hashtable_insert.\nCheckoff Item 3: What was the bug? What is your fix? Rerun make hashtable1000 to ensure\nthat your fix always works.\n\nHandout 14 -- Homework 9: Deterministic Execution\nRemember that this same bug may appear in other functions that have similar routines. This\nis an important reason why you should try not to write the same piece of code twice. An easy\nway to fix this is to refactor your code or use macros.\n5 Hashlocking\nAssuming that the serial code works, let's move on to the parallel version with make hashtable-mt.\nIf you run make hashtable-mt1000, you may find some errors. We will need some synchronization\nhere to fix the errors.\nOne possibility is to use a single hashtable lock, as in hashtable_insert_locked. However,\nthis lock would be very highly contended. A good technique for reducing lock contention on\na shared data structure is to split the lock into multiple locks. For example, instead of a single\n\"tablelock\", we consider having a separate lock for each section of the hashtable. A lock per\nhashtable line (\"rowlock\") would have too much memory and cache overhead, but a lock per\nhash (\"hashlock\") gives us the amount of overhead that we want.\nModify hashtable_fill to use hashtable_insert_fair, which is a rather basic implementa\ntion of a hashlock.\nWrite-up 1: Is the hashlock implementation in hashlock.c vulnerable to false sharing? Why\nor why not?\nWrite-up 2: What problem is the fairness solution in hashtable_insert_fair introducing?\nExplain or demonstrate the behavior you see.\n6 Lockless Hashtable\nNow, let's try to avoid using a lock altogether. On modern processors, the assembly instruction\nCMPXCHG16B supports the compare-and-swap operation for 16 bytes, and similar instructions exist\nfor 4 bytes and 8 bytes. However, early 64-bit processors didn't support the 16 byte instruction.\nIn this vein, we'll use the CMPXCHG8B in our implementation to only work with 64-bit words. This\ndoes mean that we can't atomically write a whole entry_t. Assume that size can be a positive\ninteger only. The relevant definitions are in common.h.\n\nHandout 14 -- Homework 9: Deterministic Execution\nWrite-up 3: Use InterlockedCompareExchange64 to implement your changes in\nhashtable_insert_lockless. Don't forget to modify hashtable_fill to use your new code.\nRun make hashtable-mt1000 to ensure that it works. What changes are necessary in\nhashtable_lookup so you can use just a 8-byte compare-and-swap in\nhashtable_insert_lockless?\n7 Deterministic Hashtable\nWrite-up 4: (No implementation required.) How would you implement a deterministic\nhashtable structure, such that the order of insertions does not change the final hashtable\nstate?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/6dd24f1d927b16e1e0eec707a44cf6a4_MIT6_172F18_practicequiz1.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nPractice Quiz 1\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 1\nName:\nInstructions\n- DO NOT open this quiz booklet until you are instructed to do so.\n- This quiz booklet contains 14 pages, including this one. You have 80 minutes to earn 80\npoints.\n- This quiz is closed book, but you may use one handwritten, double-sided 8 1/200 × 1100 crib\nsheet and the Master Method card handed out in lecture.\n- When the quiz begins, please write your name on this coversheet, and write your name\non the top of each page, since the pages may be separated for grading.\n- Some of the questions are true/false, and some are multiple choice. You need not explain\nthese answers unless you wish to receive partial credit if your answer is wrong. For these\nkinds of questions, incorrect answers will be penalized, so do not guess unless you are\nreasonably sure.\n- Good luck!\nNumber\nQuestion\nParts\nPoints\nScore\nGrader\nName on Every Page\nTrue or False\nBit Tricks for the Queens Problem\nParallel PageRank\nBitonic Sort\nHeap Allocation\nCompilers & Assembly\nTotal\n\n6.172 Practice Quiz 1\nName\n1 True or False (8 parts, 16 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer unless you want to leave open the possibility of receiving partial credit if\nyour answer is wrong.\n1.1\nPacking is an optimization that reduces data movement but may increase computation.\nTrue\nFalse\n1.2\nThere can never be a true-, anti- or output-data dependence between the following two lines of\ncode:\n\nmovl %eax , (%esi)\nmovl (%edi), %ecx\n\nTrue\nFalse\n1.3\nThe time command can more readily diagnose kernel-mode performance variations (e.g., page\nzeroing) than clock_gettime().\nTrue\nFalse\n1.4\nUsing taskset for a serial program diminishes performance variations caused by NUMA (nonuni\nform memory access).\nTrue\nFalse\n\n1.5\n6.172 Practice Quiz 1\nName\nWhen using reference counting for garbage collection, cyclic data structures are never garbage\ncollected.\nTrue\nFalse\n1.6\nIn the free-list heap-allocation algorithm, allocating to the least-full page maximizes the proba\nbility that two random accesses hit the same page.\nTrue\nFalse\n1.7\nMemory-allocator performance is more important when requesting a large amount of memory\nthan when requesting a small amount.\nTrue\nFalse\n1.8\nEliminating common subexpressions generally improves the performance of code unless it causes\ntoo much register pressure.\nTrue\nFalse\n\n6.172 Practice Quiz 1\nName\n2 Bit Tricks for the Queens Problem (5 parts, 13 points)\nRecall the Queens Problem from lecture: Place n queens on an n × n chessboard such that no two\nqueens can attack each other, i.e., only one queen in placed each row, column, or diagonal:\nWe saw in lecture that this problem could be solved by a backtracking search that marches up\nand down the rows of the chessboard using bit tricks to represent columns and diagonals. In fact,\nthe problem can be solved by an algorithm even more simple than the one Professor Leiserson\npresented in class.\nThe function queens() performs the backtrack search, returning the number of solutions to the\nqueens problem. The four arguments to the function are (1) a bit mask mask representing the\nwidth of the board as n columns, (2) a bit mask down representing which columns contain queens,\n(3) a bit mask left representing which left-going diagonals ending on the current row contain\nqueens, and (4) a bit mask right representing which right-going diagnoals ending on the current\nrow contain queens.\nThe queens() function is called from main() as follows:\n\nprintf (\" The %d -queens problem has %d solutions .\\n\",\nn,\nqueens ((1 << n) - 1, 0, 0, 0));\n\nThe next page shows code for queens() with 5 blanks labeled with letters, as well as a collection\nof 20 expressions beneath.\n\n6.172 Practice Quiz 1\nName\n\nint32_t queens ( uint32_t mask ,\nuint32_t down ,\nuint32_t left ,\nuint32_t right ) {\nint32_t count = 0;\nuint32_t possible , place ;\nif ( down ==\n(A)\n) return 1;\nfor ( possible = ( down | left | right ) & mask ;\npossible !=\n(B)\n;\npossible &= place ) {\nplace =\n(C)\n;\ncount += queens ( mask ,\ndown | place ,\n(D)\n,\n(E)\n);\n}\nreturn count ;\n\n}\n\nFor each blank in the code, write its label next to the expression that best fits. (Hint: Some blanks\ncan take more than one expression, but only one is \"best.\")\nplace\n-1\n-place\ndown\nplace\nleft\npossible\nleft << 1\npossible & (-possible)\n(left|place) << 1\n-possible\n((left|place) << 1) & mask\nright\nmask\nright >> 1\nmask + 1\n(right|place) >> 1\nmask\n((right|place) >> 1) & mask\n\n6.172 Practice Quiz 1\nName\n3 Parallel PageRank (3 parts, 9 points)\nIn the following code, the variables contribution and rank are arrays of doubles, and in_degree\nand out_degree are arrays of integers. neighbor is a two dimensional array that stores the edges.\nneighbor[i][j] is the jth neighbor of the ith node.\n\nvoid pagerank(double * rank , double * contribution ,\nint ** neighbor , int * in_degree ,\nint * out_degree , int num_vertices) {\nfor (size_t iter = 0; iter < 10; iter ++) {\ncilk_for (size_t i = 0; i < num_vertices; i++){\nfor (int j = 0; j < in_degree[i]; j++){\nrank[i] += contribution[neighbor[i][j]];\n}\n}\nfor (size_t i = 0; i < num_vertices; i++){\ncontribution[i] = rank[i]/ out_degree[i];\nrank[i] = 0.0;\n}\n}\n15 }\n\nFor each of the following code modifications designed to improve performance, circle the appro\npriate option to specify whether it is safe to make the indicated change, whether it is safe if a\nreducer is used, or whether it is unsafe. (Note: \"safe\" means that the output must be exactly the same\nas for the original code.)\n3.1\nReplace the for in line 4 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\n3.2\nReplace the for in line 6 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\n\nName\n6.172 Practice Quiz 1\n3.3\nReplace the for in line 10 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\n\n6.172 Practice Quiz 1\nName\n4 Bitonic Sort (6 parts, 18 points)\nConsider the following multithreaded implementation of bitonic sort, a sorting algorithm based\non bitonic sequences, which are sequences that can be cyclically shifted to be nonincreasing and\nthen nondecreasing. Fortunately, for this problem you must understand neither bitonic sequences\nnor how the algorithm works. You must only understand its parallelism structure.\n\n// Swap a[i] and a[j] if they are out of order , assuming that\n// i < j and the boolean ascending indicates whether the\n// sequence should be ascending (true) or descending (false)\nvoid bitonic_swap(int *array , size_t i, size_t j, bool ascending) {\nif ((array[i] > array[j]) == ascending) {\nint temp = array[i];\narray[i] = array[j];\narray[j] = temp;\n}\n10 }\n// Sort the elements in the subarray a[lo, .., hi -1] into\n// ascending/descending order , assuming that the subarray forms\n// a bitonic sequence of power -of -2 length.\nvoid bitonic_merge(int *a, size_t lo, size_t hi, bool ascending) {\nif (lo >= hi - 1) return;\nsize_t len = (hi - lo) / 2;\nsize_t mid = lo + len;\ncilk_for (size_t i = lo; i < mid; i++) {\nbitonic_swap(a, i, i + len , ascending);\n}\ncilk_spawn bitonic_merge(a, lo, mid , ascending);\nbitonic_merge(a, mid , hi, ascending);\ncilk_sync;\n27 }\n// Sort the elements in a[lo, .., hi -1] in ascending/descending order\n// assuming that the length of the subarray is a power of 2.\nvoid bitonic_sort(int *a, size_t lo, size_t hi, bool ascending) {\nif (lo >= hi - 1) return;\nsize_t mid = (hi + lo) / 2;\ncilk_spawn bitonic_sort(a, lo, mid , true);\nbitonic_sort(a, mid , hi, false);\ncilk_sync;\nbitonic_merge(a, lo, hi, ascending);\n40 }\n\n4.1\n6.172 Practice Quiz 1\nName\nFor the following questions, let n = hi - lo, and assume that n is a power of 2.\nGive a recurrence for the work M1(n) of bitonic_merge(), solve the recurrence, and express\nM1(n) in simple terms.\n4.2\nGive a recurrence for the span Minf(n) of bitonic_merge(), solve the recurrence, and express\nMinf(n) in simple terms.\n4.3\nGive the parallelism of bitonic_merge() in simple terms.\n\n4.4\n6.172 Practice Quiz 1\nName\nGive a recurrence for the work S1(n) of bitonic_sort() in terms of M1(n) and Minf(n), solve the\nrecurrence, and express S1(n) in simple terms.\n4.5\nGive a recurrence for the span Sinf(n) of bitonic_sort() in terms of M1(n) and Minf(n), solve the\nrecurrence, and express Sinf(n) in simple terms.\n4.6\nGive the parallelism of bitonic_sort() in simple terms.\n\n6.172 Practice Quiz 1\nName\n5 Heap Allocation (3 parts, 13 points)\nA certain program needs to allocate memory chunks that have alternating sizes of 60 bytes and\n120 bytes. In other words, the program will allocate a chunk of 60 bytes, followed by 120 bytes,\nfollowed by 60 bytes, and so on. We shall consider two schemes for heap allocation.\nScheme F is a fixed-size allocator that uses a free-list of 120-byte blocks. Scheme V is a variable-\nsized allocator that uses binned free lists with blocks that are exact powers of 2.\n5.1\nWhat are likely the advantages of Scheme V over Scheme F? (Circle all that apply.)\nA\nFaster allocation.\nB\nLess internal fragmentation.\nC\nLess external fragmentation.\nD\nFewer TLB (translation lookaside buffer) misses.\nE\nLess false sharing when parallelized.\n5.2\nBen Bitdiddle pushes an update to the program. Now, after 100,000 allocations of blocks with\nalternating sizes of 60 and 120 bytes (as described above), the program frees all blocks of size 60\nbytes and proceeds to allocate 100,000 more blocks of size 120 bytes. After the update, which\nscheme would you prefer to use and why? (Circle all that apply.)\nA\nScheme V, because freeing blocks is faster with a variable-sized allocator.\nB\nScheme F, because there is less external fragmentation.\nC\nScheme F, because there is less internal fragmentation.\nD\nScheme F, because there is better space utilization and, therefore, fewer TLB (translation\nlookaside buffer) misses.\nE\nScheme V, because there is better space utilization and, therefore, fewer TLB (translation\nlookaside buffer) misses.\n\n6.172 Practice Quiz 1\nName\n5.3\nLem E. Tweakit sends you a mystery Cilk program, in which each allocated object is labeled with\nthe worker thread that created it (its owner), and freed objects are returned to the owner's heap.\nWhat is the bound on blowup B for this mystery Cilk program? (Hint: Blowup is the maximum\nof allocated storage across all workers divided by the maximum of allocated storage in the serial\nexecution.)\nA\nB = 1.\nB\nB = K for some constant K > 1.\nC\nB ≤ P, where P = # workers.\nD\nP < B ≤ KP for some constant K > 1.\nE\nB > KP for any constant K > 1.\n\n6.172 Practice Quiz 1\nName\n6 Compilers & Assembly (2 parts, 9 points)\nConsider the four code snippets below. Assume all four examples are compiled with Tapir/Clang\non a 64-bit AVX2-enabled Linux machine, using the flags -Rpass=loop-vectorize -mavx2 (the\nsame conditions you used to complete Homework 3).\nA\n\nvoid func_A ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i ++) {\nX[i] = X[i] + Y[i ];\n}\n}\n* restrict Y) {\n\nB\nvoid func_B ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i +=4) {\nX[i] = X[i] + Y[i ];\n}\n}\n* restrict Y) {\n\nC\nvoid func_C ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i ++) {\nX[i] = X[i] / Y[i ];\n}\n}\n* restrict Y) {\n\nD\n\nvoid func_D ( int32_t * restrict X , int32_t * restrict Y) {\nfor ( int i = 0; i < 1000*1000 - 1; i ++) {\nX[i] = X[i +1] + Y[i ];\n}\n}\n\n6.172 Practice Quiz 1\nName\n6.1\nFor which of the functions is the loop likely to be vectorized without further compiler directives?\n(Circle all that apply.)\nA\nfunc_A\nB\nfunc_B\nC\nfunc_C\nD\nfunc_D\n6.2\nThe following text is the assembly code for one of the loops:\n\nmovq\n%rdi , -8(% rbp )\n# X\nmovq\n%rsi , -16(% rbp )\n# Y\nmovl\n$0 , -20(% rbp )\n. LBB0_1 :\ncmpl\n$1000000 , -20(% rbp )\njge\n. LBB0_4\nmovslq -20(% rbp ), % rax\nmovq\n-8(% rbp ) , % rcx\nmovl\n(% rcx ,% rax ,4) , % edx\nmovslq -20(% rbp ), % rax\nmovq\n-16(% rbp ) , % rcx\naddl\n(% rcx ,% rax ,4) , % edx\nmovslq -20(% rbp ), % rax\nmovq\n-8(% rbp ) , % rcx\nmovl\n%edx , (% rcx ,% rax ,4)\nmovl\n-20(% rbp ) , % eax\naddl\n$4 , % eax\nmovl\n%eax , -20(% rbp )\njmp\n. LBB0_1\n. LBB0_4 :\n\nCircle the letter for the function containing the loop corresponding to the assembly code.\nA\nfunc_A\nB\nfunc_B\nC\nfunc_C\nD\nfunc_D\n\nIntel x86 Assembly Language Cheat Sheet\nInstruction\nEffect\nExample\nData movement\nmov src, dest\nCopy src to dest\nmov $10,%eax\nArithmetic\nadd src, dest\nDest = dest + src\nadd $10, %esi\nmul reg\nedx:eax = eax * reg (colon means the\nresult spans across two registers)\nmul %esi\ndiv reg\nidiv reg\nedx = edx:eax mod reg\neax = edx:eax / reg\ndiv %edi\ninc dest\nIncrement destination\nInc %eax\ndec dest\nDecrement destination\ndec (%esi)\nsbb arg1, arg2\nIf CF = 1, (this is set by cmp instruction;\nrefer cmp)\narg2 = arg2 - (arg1 + 1)\nelse\narg2 = arg2 - arg1\nsbb %eax, %ebx\nFunction Calls\ncall label\nPush eip, transfer control\ncall _fib\nret\nPop eip and return\nret\npush item\nPush item (constant or register) to stack\npushl $32\npushl %eax\npop [reg]\nPop item from stack; optionally store to\nregister\npop %eax\npopl\nBitwise Operations\nand src,dest\nDest = src & dest\nand %ebx, %eax\nor src, dest\nDest = src | dest\norl (0x2000), %eax\nxor src, dest\nDest = src ^ dest\nxor $0xffffff, %eax\nshl count, dest\nDest = dest << count\nshl $2, %eax\nshr count, dest\nDest = dest >> count\nshr $4, (%eax)\nsal count, dest\nSame as shl, shifted bits will be the sign\nbit\nConditionals and jumps\ncmp arg1, arg2\nIf arg1 > arg2 sets\nCF=1 (carry flag =1)\nThis compares arg1 and arg2; you can\nuse any conditionals jumps below to act\nupon the result of this comparison\ncmp $0, %eax\ntest reg,imm/reg\nBitwise and of register and\nconstant/register; the next jump command\nuses the result of this; consider this\nessentially as same as compare\ntest %rax, %rcx\nje label\nJump to label if arg2 = arg1\nje endloop\njne label\nJump to label if arg2 != arg1\njne loopstart\njg label / ja label\nJump to label if arg2 > arg1\njg exit / ja exit\njge label\nJump to label if arg2 >= arg1\njge format_disk\njl label\nJump to label if arg2 < arg1\njl error\njle label\nJump to label if arg2 <= arg1\njle finish\njz label\nJump to label if bits were not set\njz looparound\njnz label\nJump to label if bits were set\njnz error\njump label\nUnconditional jump\njmp exit\nMiscellaneous\nnop\nNo-op\nnop\nlea addr, dest\nMove the address calculated to the dest\nlea 23(%eax, %ecx,8),%eax\ncqto\n%rdx:%rax← sign-extend of %rax.\ncqto\nsuffixes b=byte(8), w=word(16), l=long(32), q=quad(64)\nbase indexed scale displacement 172(%rdi, %rdx,8) = %rdi + 8 * %rdx + 172\nNote that not both src and dest can be memory operands at the same time.\nregister - %eax\n\nfixed address - (0x1000)\nconstant - $10\n\ndynamic address - (%rsi)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 1 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/5a4a0e523f93da26593c3a1b2d6ca6e5_MIT6_172F18_practicequiz1answers.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 1 ANSWERS\nPractice Quiz 1 ANSWERS\n\n6.172 Practice Quiz 1 ANSWERS\nName\n1 True or False (8 parts, 16 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer unless you want to leave open the possibility of receiving partial credit if\nyour answer is wrong.\n1.1\nPacking is an optimization that reduces data movement but may increase computation.\nTrue\nFalse\nAnswer: True\n1.2\nThere can never be a true-, anti- or output-data dependence between the following two lines of\ncode:\n\nmovl %eax , (%esi)\nmovl (%edi), %ecx\n\nTrue\nFalse\nAnswer: False\n1.3\nThe time command can more readily diagnose kernel-mode performance variations (e.g., page\nzeroing) than clock_gettime().\nTrue\nFalse\nAnswer: True\n1.4\nUsing taskset for a serial program diminishes performance variations caused by NUMA (nonuni\nform memory access).\nTrue\nFalse\nAnswer: True\n\n6.172 Practice Quiz 1 ANSWERS\nName\n1.5\nWhen using reference counting for garbage collection, cyclic data structures are never garbage\ncollected.\nTrue\nFalse\nAnswer: True\n1.6\nIn the free-list heap-allocation algorithm, allocating to the least-full page maximizes the proba\nbility that two random accesses hit the same page.\nTrue\nFalse\nAnswer: False\n1.7\nMemory-allocator performance is more important when requesting a large amount of memory\nthan when requesting a small amount.\nTrue\nFalse\nAnswer: False\n1.8\nEliminating common subexpressions generally improves the performance of code unless it causes\ntoo much register pressure.\nTrue\nFalse\nAnswer: True\n\n6.172 Practice Quiz 1 ANSWERS\nName\n2 Bit Tricks for the Queens Problem (5 parts, 13 points)\nRecall the Queens Problem from lecture: Place n queens on an n × n chessboard such that no two\nqueens can attack each other, i.e., only one queen in placed each row, column, or diagonal:\nWe saw in lecture that this problem could be solved by a backtracking search that marches up\nand down the rows of the chessboard using bit tricks to represent columns and diagonals. In fact,\nthe problem can be solved by an algorithm even more simple than the one Professor Leiserson\npresented in class.\nThe function queens() performs the backtrack search, returning the number of solutions to the\nqueens problem. The four arguments to the function are (1) a bit mask mask representing the\nwidth of the board as n columns, (2) a bit mask down representing which columns contain queens,\n(3) a bit mask left representing which left-going diagonals ending on the current row contain\nqueens, and (4) a bit mask right representing which right-going diagnoals ending on the current\nrow contain queens.\nThe queens() function is called from main() as follows:\n\nprintf (\" The %d -queens problem has %d solutions .\\n\",\nn,\nqueens ((1 << n) - 1, 0, 0, 0));\n\nThe next page shows code for queens() with 5 blanks labeled with letters, as well as a collection\nof 20 expressions beneath.\n\n6.172 Practice Quiz 1 ANSWERS\nName\n\nint32_t queens ( uint32_t mask ,\nuint32_t down ,\nuint32_t left ,\nuint32_t right ) {\nint32_t count = 0;\nuint32_t possible , place ;\nif ( down ==\n(A)\n) return 1;\nfor ( possible = ( down | left | right ) & mask ;\npossible !=\n(B)\n;\npossible &= place ) {\nplace =\n(C)\n;\ncount += queens ( mask ,\ndown | place ,\n(D)\n,\n(E)\n);\n}\nreturn count ;\n\n}\n\nFor each blank in the code, write its label next to the expression that best fits. (Hint: Some blanks\ncan take more than one expression, but only one is \"best.\")\n0 Ans: B\nplace\n-1\n-place\ndown\nplace\nleft\npossible\nleft << 1\npossible & (-possible) Ans: C\n(left|place) << 1 Ans: D\n-possible\n((left|place) << 1) & mask Ans: D\nright\nmask Ans: A\nright >> 1\nmask + 1\n(right|place) >> 1 Ans: E\nmask\n((right|place) >> 1) & mask\n\n6.172 Practice Quiz 1 ANSWERS\nName\n3 Parallel PageRank (3 parts, 9 points)\nIn the following code, the variables contribution and rank are arrays of doubles, and in_degree\nand out_degree are arrays of integers. neighbor is a two dimensional array that stores the edges.\nneighbor[i][j] is the jth neighbor of the ith node.\n\nvoid pagerank(double * rank , double * contribution ,\nint ** neighbor , int * in_degree ,\nint * out_degree , int num_vertices) {\nfor (size_t iter = 0; iter < 10; iter ++) {\ncilk_for (size_t i = 0; i < num_vertices; i++){\nfor (int j = 0; j < in_degree[i]; j++){\nrank[i] += contribution[neighbor[i][j]];\n}\n}\nfor (size_t i = 0; i < num_vertices; i++){\ncontribution[i] = rank[i]/ out_degree[i];\nrank[i] = 0.0;\n}\n}\n15 }\n\nFor each of the following code modifications designed to improve performance, circle the appro\npriate option to specify whether it is safe to make the indicated change, whether it is safe if a\nreducer is used, or whether it is unsafe. (Note: \"safe\" means that the output must be exactly the same\nas for the original code.)\n3.1\nReplace the for in line 4 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\nAnswer: Unsafe\n3.2\nReplace the for in line 6 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\nAnswer: Unsafe\n\n6.172 Practice Quiz 1 ANSWERS\nName\n3.3\nReplace the for in line 10 with cilk_for.\nSafe\nSafe with reducer\nUnsafe\nAnswer: Safe\n\n6.172 Practice Quiz 1 ANSWERS\nName\n4 Bitonic Sort (6 parts, 18 points)\nConsider the following multithreaded implementation of bitonic sort, a sorting algorithm based\non bitonic sequences, which are sequences that can be cyclically shifted to be nonincreasing and\nthen nondecreasing. Fortunately, for this problem you must understand neither bitonic sequences\nnor how the algorithm works. You must only understand its parallelism structure.\n\n// Swap a[i] and a[j] if they are out of order , assuming that\n// i < j and the boolean ascending indicates whether the\n// sequence should be ascending (true) or descending (false)\nvoid bitonic_swap(int *array , size_t i, size_t j, bool ascending) {\nif ((array[i] > array[j]) == ascending) {\nint temp = array[i];\narray[i] = array[j];\narray[j] = temp;\n}\n10 }\n// Sort the elements in the subarray a[lo, .., hi -1] into\n// ascending/descending order , assuming that the subarray forms\n// a bitonic sequence of power -of -2 length.\nvoid bitonic_merge(int *a, size_t lo, size_t hi, bool ascending) {\nif (lo >= hi - 1) return;\nsize_t len = (hi - lo) / 2;\nsize_t mid = lo + len;\ncilk_for (size_t i = lo; i < mid; i++) {\nbitonic_swap(a, i, i + len , ascending);\n}\ncilk_spawn bitonic_merge(a, lo, mid , ascending);\nbitonic_merge(a, mid , hi, ascending);\ncilk_sync;\n27 }\n// Sort the elements in a[lo, .., hi -1] in ascending/descending order\n// assuming that the length of the subarray is a power of 2.\nvoid bitonic_sort(int *a, size_t lo, size_t hi, bool ascending) {\nif (lo >= hi - 1) return;\nsize_t mid = (hi + lo) / 2;\ncilk_spawn bitonic_sort(a, lo, mid , true);\nbitonic_sort(a, mid , hi, false);\ncilk_sync;\nbitonic_merge(a, lo, hi, ascending);\n40 }\n\n6.172 Practice Quiz 1 ANSWERS\nName\nFor the following questions, let n = hi - lo, and assume that n is a power of 2.\n4.1\nGive a recurrence for the work M1(n) of bitonic_merge(), solve the recurrence, and express\nM1(n) in simple terms.\nAnswer: O(nlog(n))\n4.2\nGive a recurrence for the span Minf(n) of bitonic_merge(), solve the recurrence, and express\nMinf(n) in simple terms.\nAnswer: O(log2(n))\n4.3\nGive the parallelism of bitonic_merge() in simple terms.\nAnswer: O(n/log(n))\n\n4.4\n6.172 Practice Quiz 1 ANSWERS\nName\nGive a recurrence for the work S1(n) of bitonic_sort() in terms of M1(n) and Minf(n), solve the\nrecurrence, and express S1(n) in simple terms.\nAnswer: O(nlog2(n))\n4.5\nGive a recurrence for the span Sinf(n) of bitonic_sort() in terms of M1(n) and Minf(n), solve the\nrecurrence, and express Sinf(n) in simple terms.\nAnswer: O(log3(n))\n4.6\nGive the parallelism of bitonic_sort() in simple terms.\nAnswer: O(n/log(n))\n\n6.172 Practice Quiz 1 ANSWERS\nName\n5 Heap Allocation (3 parts, 13 points)\nA certain program needs to allocate memory chunks that have alternating sizes of 60 bytes and\n120 bytes. In other words, the program will allocate a chunk of 60 bytes, followed by 120 bytes,\nfollowed by 60 bytes, and so on. We shall consider two schemes for heap allocation.\nScheme F is a fixed-size allocator that uses a free-list of 120-byte blocks. Scheme V is a variable-\nsized allocator that uses binned free lists with blocks that are exact powers of 2.\n5.1\nWhat are likely the advantages of Scheme V over Scheme F? (Circle all that apply.)\nA\nFaster allocation.\nB\nLess internal fragmentation.\nC\nLess external fragmentation.\nD\nFewer TLB (translation lookaside buffer) misses.\nE\nLess false sharing when parallelized.\nAnswer: B, D, E\n5.2\nBen Bitdiddle pushes an update to the program. Now, after 100,000 allocations of blocks with\nalternating sizes of 60 and 120 bytes (as described above), the program frees all blocks of size 60\nbytes and proceeds to allocate 100,000 more blocks of size 120 bytes. After the update, which\nscheme would you prefer to use and why? (Circle all that apply.)\nA\nScheme V, because freeing blocks is faster with a variable-sized allocator.\nB\nScheme F, because there is less external fragmentation.\nC\nScheme F, because there is less internal fragmentation.\nD\nScheme F, because there is better space utilization and, therefore, fewer TLB (translation\nlookaside buffer) misses.\nE\nScheme V, because there is better space utilization and, therefore, fewer TLB (translation\nlookaside buffer) misses.\nAnswer: B, D\n\n6.172 Practice Quiz 1 ANSWERS\nName\n5.3\nLem E. Tweakit sends you a mystery Cilk program, in which each allocated object is labeled with\nthe worker thread that created it (its owner), and freed objects are returned to the owner's heap.\nWhat is the bound on blowup B for this mystery Cilk program? (Hint: Blowup is the maximum\nof allocated storage across all workers divided by the maximum of allocated storage in the serial\nexecution.)\nA\nB = 1.\nB\nB = K for some constant K > 1.\nC\nB ≤ P, where P = # workers.\nD\nP < B ≤ KP for some constant K > 1.\nE\nB > KP for any constant K > 1.\nAnswer: C\n\n6.172 Practice Quiz 1 ANSWERS\nName\n6 Compilers & Assembly (2 parts, 9 points)\nConsider the four code snippets below. Assume all four examples are compiled with Tapir/Clang\non a 64-bit AVX2-enabled Linux machine, using the flags -Rpass=loop-vectorize -mavx2 (the\nsame conditions you used to complete Homework 3).\nA\n\nvoid func_A ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i ++) {\nX[i] = X[i] + Y[i ];\n}\n}\n* restrict Y) {\n\nB\nvoid func_B ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i +=4) {\nX[i] = X[i] + Y[i ];\n}\n}\n* restrict Y) {\n\nC\nvoid func_C ( int32_t * restrict X , int32_t\nfor ( int i = 0; i < 1000*1000; i ++) {\nX[i] = X[i] / Y[i ];\n}\n}\n* restrict Y) {\n\nD\n\nvoid func_D ( int32_t * restrict X , int32_t * restrict Y) {\nfor ( int i = 0; i < 1000*1000 - 1; i ++) {\nX[i] = X[i +1] + Y[i ];\n}\n}\n\n6.172 Practice Quiz 1 ANSWERS\nName\n6.1\nFor which of the functions is the loop likely to be vectorized without further compiler directives?\n(Circle all that apply.)\nA\nfunc_A\nB\nfunc_B\nC\nfunc_C\nD\nfunc_D\nAnswer: A, D\n6.2\nThe following text is the assembly code for one of the loops:\n\nmovq\n%rdi , -8(%rbp)\n# X\nmovq\n%rsi , -16(%rbp)\n# Y\nmovl\n$0, -20(%rbp)\n.LBB0_1:\ncmpl\n$1000000 , -20(%rbp)\njge\n.LBB0_4\nmovslq -20(%rbp), %rax\nmovq\n-8(%rbp), %rcx\nmovl\n(%rcx ,%rax ,4), %edx\nmovslq -20(%rbp), %rax\nmovq\n-16(%rbp), %rcx\naddl\n(%rcx ,%rax ,4), %edx\nmovslq -20(%rbp), %rax\nmovq\n-8(%rbp), %rcx\nmovl\n%edx , (%rcx ,%rax ,4)\nmovl\n-20(%rbp), %eax\naddl\n$4, %eax\nmovl\n%eax , -20(%rbp)\njmp\n.LBB0_1\n.LBB0_4:\n\nCircle the letter for the function containing the loop corresponding to the assembly code.\nA\nfunc_A\nB\nfunc_B\nC\nfunc_C\nD\nfunc_D\nAnswer: B\n\nIntel x86 Assembly Language Cheat Sheet\nInstruction\nEffect\nExample\nData movement\nmov src, dest\nCopy src to dest\nmov $10,%eax\nArithmetic\nadd src, dest\nDest = dest + src\nadd $10, %esi\nmul reg\nedx:eax = eax * reg (colon means the\nresult spans across two registers)\nmul %esi\ndiv reg\nidiv reg\nedx = edx:eax mod reg\neax = edx:eax / reg\ndiv %edi\ninc dest\nIncrement destination\nInc %eax\ndec dest\nDecrement destination\ndec (%esi)\nsbb arg1, arg2\nIf CF = 1, (this is set by cmp instruction;\nrefer cmp)\narg2 = arg2 - (arg1 + 1)\nelse\narg2 = arg2 - arg1\nsbb %eax, %ebx\nFunction Calls\ncall label\nPush eip, transfer control\ncall _fib\nret\nPop eip and return\nret\npush item\nPush item (constant or register) to stack\npushl $32\npushl %eax\npop [reg]\nPop item from stack; optionally store to\nregister\npop %eax\npopl\nBitwise Operations\nand src,dest\nDest = src & dest\nand %ebx, %eax\nor src, dest\nDest = src | dest\norl (0x2000), %eax\nxor src, dest\nDest = src ^ dest\nxor $0xffffff, %eax\nshl count, dest\nDest = dest << count\nshl $2, %eax\nshr count, dest\nDest = dest >> count\nshr $4, (%eax)\nsal count, dest\nSame as shl, shifted bits will be the sign\nbit\nConditionals and jumps\ncmp arg1, arg2\nIf arg1 > arg2 sets\nCF=1 (carry flag =1)\nThis compares arg1 and arg2; you can\nuse any conditionals jumps below to act\nupon the result of this comparison\ncmp $0, %eax\ntest reg,imm/reg\nBitwise and of register and\nconstant/register; the next jump command\nuses the result of this; consider this\nessentially as same as compare\ntest %rax, %rcx\nje label\nJump to label if arg2 = arg1\nje endloop\njne label\nJump to label if arg2 != arg1\njne loopstart\njg label / ja label\nJump to label if arg2 > arg1\njg exit / ja exit\njge label\nJump to label if arg2 >= arg1\njge format_disk\njl label\nJump to label if arg2 < arg1\njl error\njle label\nJump to label if arg2 <= arg1\njle finish\njz label\nJump to label if bits were not set\njz looparound\njnz label\nJump to label if bits were set\njnz error\njump label\nUnconditional jump\njmp exit\nMiscellaneous\nnop\nNo-op\nnop\nlea addr, dest\nMove the address calculated to the dest\nlea 23(%eax, %ecx,8),%eax\ncqto\n%rdx:%rax← sign-extend of %rax.\ncqto\nsuffixes b=byte(8), w=word(16), l=long(32), q=quad(64)\nbase indexed scale displacement 172(%rdi, %rdx,8) = %rdi + 8 * %rdx + 172\nNote that not both src and dest can be memory operands at the same time.\nregister - %eax\n\nfixed address - (0x1000)\nconstant - $10\n\ndynamic address - (%rsi)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/26d0fe05ef1e719ae599c4e4c9ccf5b4_MIT6_172F18_practicequiz2.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 2\nPractice Quiz 2\nName:\nInstructions\n- DO NOT open this quiz booklet until you are instructed to do so.\n- This quiz booklet contains 17 pages, including this one. You have 80 minutes to earn 80\npoints.\n- This quiz is closed book, but you may use one handwritten, double-sided 8 1/200 × 1100 crib\nsheet and the Master Method card handed out in lecture.\n- When the quiz begins, please write your name and Kerberos on this coversheet, and write\nyour name or Kerberos on the top of each page, since the pages may be separated for\ngrading.\n- Some of the questions are true/false, and some are multiple choice. You need not explain\nthese answers unless you wish to receive partial credit if your answer is wrong. For these\nkinds of questions, since incorrect answers will be penalized, do not guess unless you are\nreasonably sure.\n- Good luck!\nNumber\nQuestion\nParts\nPoints\nScore\nGrader\nName on Every Page\nTrue or False\nHeap space for Cilk programs\nConcurrent free lists\nParallel Strassen\nSpace usage for Strassen\nCache-oblivious Strassen\nTotal\n\n6.172 Practice Quiz 2\nName\n1 True or false (9 parts, 18 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer, unless you want to leave open the possibility of receiving partial credit\nif your answer is wrong. Comments will have no impact on a correct answer.\n1.1\nThere are generally fewer conflict misses in a set-associative cache than a direct-mapped cache.\nTrue\nFalse\n1.2\nIf there is a TLB miss, then the corresponding cache line is not already in cache.\nTrue\nFalse\n1.3\nWhen dealing with multiple pages, a new block should be allocated from the free list for the\nemptiest page to incur the fewest TLB misses.\nTrue\nFalse\n1.4\nIt is generally more important for a memory allocator to quickly allocate small blocks than to\nquickly allocate large blocks.\nTrue\nFalse\n\n1.5\n6.172 Practice Quiz 2\nName\nStreaming writes allow for higher memory bandwidth utilization by bypassing cache with the\nadditional cost of higher latency.\nTrue\nFalse\n1.6\nOn x86, sequentially consistent behavior can always be achieved through the use of memory\nfences.\nTrue\nFalse\n1.7\nIf a program that runs on 2 threads protects its critical sections using Peterson's algorithm, then\nit is free of determinacy races.\nTrue\nFalse\n1.8\nIf a program that runs on 2 threads protects its critical sections using Peterson's algorithm, then\nit is free of data races.\nTrue\nFalse\n1.9\nIgnoring the effects of true and false sharing, on x86, a compare-and-swap operation is just as\nfast as an equivalent comparison and assignment.\nTrue\nFalse\n\n2.2\n6.172 Practice Quiz 2\nName\n2 Heap space for Cilk programs (3 parts, 7 points)\nConsider the following parallel Cilk code.\n\nvoid foo(int depth , size_t X) {\nif (depth < 1) return;\ncilk_spawn foo(depth -1, X);\nvoid *ptr = malloc(X);\ncilk_sync;\nfree(ptr);\n7 }\n\nAssume that the stack is large enough such that for all inputs of depth the code does not overflow\nthe stack. Furthermore, assume that foo is run using an optimal allocator that achieves perfect\nutilization.\n2.1\nSuppose that foo is executed using just 1 worker. Argue that the space needed to satisfy the calls\nto malloc from foo is X.\nNow suppose that foo is executed using 2 workers. In terms of depth and X, what is the maximum\namount of space needed to satisfy the calls to malloc from foo? (Hint: Consider an execution of\nfoo in which one worker steals line 4 from the other worker in every invocation of foo.)\n\n6.172 Practice Quiz 2\nName\n2.3\nIn lecture, we saw a theorem stating that a P-worker execution of any Cilk program uses at most\nP times the space used in a serial execution of that Cilk program. Why does the theorem not\napply to foo, or does it?\n\n6.172 Practice Quiz 2\nName\n3 Concurrent free lists (2 parts, 8 points)\nBen Bitdiddle is parallelizing a program that uses a free list to handle the allocation of objects of\na fixed size X. Ben's serial code uses a global free-list data structure of type FreeList_t, defined\nas follows:\n\ntypedef struct Node_t {\n/* A */\nstruct Node_t *next;\n11 } Node_t;\ntypedef struct {\n/* B */\nNode_t *head;\n16 } FreeList_t;\nvoid push(FreeList_t *fl, Node_t *node) {\n/* C */\nnode ->next = fl ->head;\n/* D */\nfl ->head = node;\n/* E */\n24 }\nNode_t *pop(FreeList_t *fl) {\n/* F */\nNode_t *node = fl ->head;\n/* G */\nif (NULL != node) {\n/* H */\nfl ->head = node ->next;\n/* I */\n}\n/* J */\nreturn node;\n37 }\n\n3.1\n3.2\n6.172 Practice Quiz 2\nName\nBen considers parallelizing his free list by implementing thread-local free lists. Each thread\nwould only push or pop storage from its local free list. If the thread's local free list is empty, then\nthe thread simply calls the system's malloc.\nDescribe a performance problem that Ben might see with thread-local free lists that can cause\nBen's parallel program to use much more space than his original serial program.\nBen decides to modify his serial free-list code into a global free list, with push and pop oper\nations synchronized using mutex locks. Ben wants to add as few instructions as possible to his\nfree-list code to implement a correct and efficient global free list. Ben's free list code contains\nconvenient labels A through J at lines where Ben might insert a new statement.\nWrite an X in each table cell below where Ben should insert the statement in the corresponding\ncolumn at the line specified by the corresponding row. Each row should contain at most one X.\nmutex_t L;\nlock(fl->L);\nunlock(fl->L);\nlock(node->L);\nunlock(node->L);\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n\n6.172 Practice Quiz 2\nName\n4 Parallel Strassen (5 parts, 15 points)\nRecall the (cache-oblivious) divide-and-conquer matrix multiplication algorithm from lecture and\nfrom Homework 8. Given two n × n matrices A and B, where n is a power of 2, the algorithm\ncomputes the product C = A · B in Θ(n3) work as follows. First, the algorithm partitions the\nmatrices A, B, and C into four quadrants:\n\nA1,1\nA1,2\nB1,1\nB1,2\nC1,1 C1,2\nA =\n, B =\n, C =\nA2,1\nA2,2\nB2,1\nB2,2\nC2,1 C2,2\nThe algorithm then computes the quadrants of C using eight recursive multiplications:\nC1,1 = A1,1 · B1,1 + A1,2 · B2,1\nC1,2 = A1,1 · B1,2 + A1,2 · B2,2\nC2,1 = A2,1 · B1,1 + A2,2 · B2,1\nC2,2 = A2,1 · B1,2 + A2,2 · B2,2 .\nIn 1969 Volker Strassen invented an asymptotically faster matrix-multiplication algorithm and\nshowed that the n3 matrix multiplication algorithm was not optimal. Strassen's algorithm par\ntitions the matrices into quadrants as before, but then recursively computes seven intermediate\nmatrix products: (Do not waste time verifying that these formulae are correct.)\nM1 = (A1,1 + A2,2) · (B1,1 + B2,2)\nM2 = (A2,1 + A2,2) · B1,1\nM3 = A1,1 · (B1,2 - B2,2)\nM4 = A2,2 · (B2,1 - B1,1)\nM5 = (A1,1 + A1,2) · B2,2\nM6 = (A2,1 - A1,1) · (B1,1 + B1,2)\nM7 = (A1,2 - A2,2) · (B2,1 + B2,2) .\nThe quadrants of the C matrix can be computed in terms of M1, M2,..., M7 as follows:\nC1,1 = M1 + M4 - M5 + M7\nC1,2 = M3 + M5\nC2,1 = M2 + M4\nC2,2 = M1 - M2 + M3 + M6 .\nThe next page presents pseudocode for a parallel implementation of Strassen's algorithm for\nn × n matrices, where n is an even power of 2. Do not waste time trying to understand how the\ncode works. You only need to understand its parallel structure.\n\n6.172 Practice Quiz 2\nName\n\n// Pseudocode for a parallel version of Strassen 's algorithm to\n// multiply two square matrices A , B. The matrices have side length n\n// in each recursive call , where n is an even power of 2.\n// Assume that A11 ... A22 , B11 ... B22 , C11 ... C22 are quadrants of A , B, C.\nstrassen (A , B , n) {\n// Coarsened base case\nif (n < BASE_CASE_SIZE ) return base_case (A , B, n);\n// Compute the quadrants A11 , A12 , etc . for A, B , and C.\n// Create 8 temporary matrices of size n ˆ2/4.\nTemp [8];\nfor ( int i = 0; i < 8; ++ i) Temp [i] = malloc (n * n / 4);\n//////// Block 1\ncilk_spawn { Temp [0] = A21 + A22 ;\nTemp [1] = strassen ( Temp [0] , B11 , n /2) }; // M2\ncilk_spawn { Temp [2] = B12 - B22 ;\nTemp [3] = strassen ( A11 , Temp [2] , n /2) }; // M3\ncilk_spawn { Temp [4] = B21 - B11 ;\nTemp [5] = strassen ( A22 , Temp [4] , n /2) }; // M4\nTemp [6] = A11 + A12 ;\nTemp [7] = strassen ( Temp [6] , B22 , n /2) ;\n// M5\ncilk_sync ; // End block 1\n//////// Block 2\ncilk_spawn { C11 = Temp [5] - Temp [7] }; // C11 = M4 - M5\ncilk_spawn { C12 = Temp [3] + Temp [7] }; // C12 = M3 + M5\ncilk_spawn { C21 = Temp [1] + Temp [5] }; // C21 = M2 + M4\nC22 = Temp [3] - Temp [1];\n// C22 = M3 - M2\ncilk_sync ; // End block 2\n//////// Block 3\ncilk_spawn { Temp [0] = A11 + A22 ;\nTemp [1] = B11 + B22 ;\nTemp [2] = strassen ( Temp [0] , Temp [1] , n /2) }; // M1\ncilk_spawn { Temp [3] = A12 - A22 ;\nTemp [4] = B21 + B22 ;\nC11 += strassen ( Temp [3] , Temp [4] , n /2) };\n// C11 += M7\nTemp [5] = A21 - A11 ;\nTemp [6] = B11 + B12 ;\nC22 += strassen ( Temp [5] , Temp [6] , n /2) ;\n// C22 += M6\ncilk_sync ; // End block 3\n//////// Block 4\ncilk_spawn { C11 += Temp [2] }; // C11 += M1\nC22 += Temp [2];\n// C22 += M1\ncilk_sync ; // End block 4\nfor ( int i = 0; i < 8; ++ i) free ( Temp [i ]) ;\nreturn C;\n88 }\n\n4.1\n6.172 Practice Quiz 2\nName\nWe will first analyze the work and span of this Strassen code. Assume that this Strassen code\nuses a routine for adding or subtracting two n × n matrices in Θ(n2) work and Θ(lgn) span, such\nas in line 52.\nExplain why the recurrence for the work T1(n) of the parallel Strassen code satisfies\nT1(n) = 7T1(n/2) + Θ(n2).\n(1)\n4.2\nThe solution to the work recurrence (1) is\nT1(n) = Θ(na lgb n)\nfor some values a and b. Select the correct values for a and b from the choices below.\nA\na = 2, b = 0\nB\na = 2, b = 1\nC\na = lg7, b = 0\nD\na = lg7, b = 1\nE\na = 3, b = 0\nF\nNone of the above\n\n4.3\n4.4\n6.172 Practice Quiz 2\nName\nThe recurrence for the span Tinf(n) of this parallel Strassen code is composed from the spans of\nfour blocks in the code: Block 1 (lines 51-60), Block 2 (lines 62-67), Block 3 (lines 69-79), and\nBlock 4 (lines 81-84). For each block, write an X in the table below to identify the span of that\nblock.\nΘ(lgn)\nTinf(n/2) + Θ(lgn)\n3Tinf(n/2) + Θ(lgn)\n4Tinf(n/2) + Θ(lgn)\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nDescribe how the spans of those four blocks compose to make the recurrence for the span of this\nparallel Strassen code equal to\nTinf(n) = 2Tinf(n/2) + Θ(lgn).\n(2)\n\n6.172 Practice Quiz 2\nName\n4.5\nThe solution to the span recurrence (2) is\nTinf(n) = Θ(na lgb n)\nfor some values a and b. Select the correct values for a and b from the choices below.\nA\na = 0, b = 1\nC\na = 1, b = 0\nE\na = 2, b = 0\nG\na = lg7, b = 0\nI\nNone of the above\nB\na = 0, b = 2\nD\na = 1, b = 1\nF\na = 2, b = 1\nH\na = lg7, b = 1\n\n5.2\n6.172 Practice Quiz 2\nName\n5 Space usage for Strassen (6 parts, 18 points)\nThis question studies the space usage during an execution of the parallel Strassen code described\nin Question 4. As lines 48-49 in the pseudocode show, this parallel Strassen code uses Θ(n2)\ntemporary space per recursive call.\n5.1\nDescribe why the total amount of temporary space S1(n) used by a 1-worker execution of this\nparallel Strassen program satisfies the following recurrence:\nS1(n) = S1(n/2) + Θ(n2).\n(3)\nSolve the recurrence (3), and write your answer using Θ-notation.\n\n6.172 Practice Quiz 2\nName\nAlthough the Cilk scheduler provides a weak bound on the space used by a P-worker execu\ntion of this parallel Strassen code, we can prove a stronger bound by considering the worst-case\nrecursion tree for this code. It turns out that this worst-case recursion tree for space usage is as\nillustrated below:\n...\nΘ((n/2)2)\nΘ((n/2k)2)\nΘ((n/2k)2)\nΘ((n/2k)2)\n...\nΘ((n/2)2)\nΘ((n/2)2)\nΘ(1)\nΘ(1)\nΘ(1)\n...\nP nodes\nLevel 0\nΘ(n2)\nLevel 1\nLevel k\n...\nThis recursion tree recursively branches 4 ways until it reaches the first level k that contains P\nnodes. Each worker then serially executes the computation under a distinct level-k node, which\nthe recursion tree models with 1-way branching after level k.\n5.3\nWhy is the branching factor 4 for the top part of this recursion tree?\n5.4\nArgue that the total space usage in the top part of the recursion tree, in all levels i ≤ k, is\nΘ(n2 log4 P).\n\n5.5\n5.6\n6.172 Practice Quiz 2\nName\nArgue that the total space usage in the bottom part of the recursion tree, in all levels i > k, is\nΘ(n2).\nWhat is the total worst-case space used by an execution of parallel Strassen? Express your answer\nin Θ-notation.\n\n6.172 Practice Quiz 2\nName\n6 Cache-oblivious Strassen (3 parts, 13 points)\nThis question examines the cache-efficiency of Strassen's algorithm for matrix multiplication. For\nthis question, we will consider a serial implementation of Strassen's algorithm, such as the serial\nelision of the parallel implementation described in Question 4. For this problem, assume an\nideal cache model (fully associative with an optimal or LRU replacement policy, as appropriate)\nwith cache size M and cache-line length B. Assume that the cache is tall (i.e. M = ω(B2)).\nFurthermore, assume that Strassen's algorithm is run using an optimal allocator that achieves\nperfect utilization. Assume that the input matrix is stored in row-major order, and also assume\nthat the routine for adding or subtracting two m-by-m submatrices is Θ(m2/B).\nThe recurrence for the worst-case number Q(n) of cache misses incurred by Strassen's algorithm\nwhen multiplying two n × n matrices is given by\n(\n√\nΘ(n2/B)\nif 0 ≤ n < c M for sufficiently small constant c ≤ 1;\nQ(n) =\n(4)\n7Q(n/2) + Θ(n2/B) otherwise.\n6.1\nWhy is Q(n) = 7Q(n/2) + Θ(n2/B) for n sufficiently large?\n6.2\n√\nWhy is Q(n) = Θ(n2/B) for n < c M?\n\n6.3\n6.172 Practice Quiz 2\nName\nSketch the recursion tree for the recurrence for (4). Compute the height and the number of leaves\nof the recursion tree, and label the internal nodes and leaves of the tree with the corresponding\nnumber of cache misses incurred at them. Solve the recurrence, providing a tight asymptotic\nbound in simple form.\nSketch of recursion tree for Q(n):\nHeight =\nNumber of leaves =\nMisses per leaf =\nQ(n) =\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 2 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/3bf4579a108c9e41753851e2dee4adee_MIT6_172F18_practicequiz2answers.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 2\nPractice Quiz 2 Solutions\nName:\nInstructions\n- DO NOT open this quiz booklet until you are instructed to do so.\n- This quiz booklet contains 17 pages, including this one. You have 80 minutes to earn 80\npoints.\n- This quiz is closed book, but you may use one handwritten, double-sided 8 1/200 × 1100 crib\nsheet and the Master Method card handed out in lecture.\n- When the quiz begins, please write your name on this coversheet, and write your name on\nthe top of each page, since the pages may be separated for grading.\n- Some of the questions are true/false, and some are multiple choice. You need not explain\nthese answers unless you wish to receive partial credit if your answer is wrong. For these\nkinds of questions, since incorrect answers will be penalized, do not guess unless you are\nreasonably sure.\n- Good luck!\nNumber\nQuestion\nParts\nPoints\nScore\nGrader\nName on Every Page\nTrue or False\nHeap space for Cilk programs\nConcurrent free lists\nParallel Strassen\nSpace usage for Strassen\nCache-oblivious Strassen\nTotal\n\n6.172 Practice Quiz 2\nName\n1 True or false (9 parts, 18 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer, unless you want to leave open the possibility of receiving partial credit\nif your answer is wrong. Comments will have no impact on a correct answer.\n1.1\nThere are generally fewer conflict misses in a set-associative cache than a direct-mapped cache.\nTrue\nFalse\nAnswer: True\n1.2\nIf there is a TLB miss, then the corresponding cache line is not already in cache.\nTrue\nFalse\nAnswer: False\n1.3\nWhen dealing with multiple pages, a new block should be allocated from the free list for the\nemptiest page to incur the fewest TLB misses.\nTrue\nFalse\nAnswer: False\n1.4\nIt is generally more important for a memory allocator to quickly allocate small blocks than to\nquickly allocate large blocks.\nTrue\nFalse\nAnswer: True\n\n6.172 Practice Quiz 2\nName\n1.5\nStreaming writes allow for higher memory bandwidth utilization by bypassing cache with the\nadditional cost of higher latency.\nTrue\nFalse\nAnswer: True\n1.6\nOn x86, sequentially consistent behavior can always be achieved through the use of memory\nfences.\nTrue\nFalse\nAnswer: True\n1.7\nIf a program that runs on 2 threads protects its critical sections using Peterson's algorithm, then\nit is free of determinacy races.\nTrue\nFalse\nAnswer: False\n1.8\nIf a program that runs on 2 threads protects its critical sections using Peterson's algorithm, then\nit is free of data races.\nTrue\nFalse\nAnswer: True\n1.9\nIgnoring the effects of true and false sharing, on x86, a compare-and-swap operation is just as\nfast as an equivalent comparison and assignment.\nTrue\nFalse\nAnswer: False\n\n2.2\n6.172 Practice Quiz 2\nName\n2 Heap space for Cilk programs (3 parts, 7 points)\nConsider the following parallel Cilk code.\n\nvoid foo(int depth , size_t X) {\nif (depth < 1) return;\ncilk_spawn foo(depth -1, X);\nvoid *ptr = malloc(X);\ncilk_sync;\nfree(ptr);\n7 }\n\nAssume that the stack is large enough such that for all inputs of depth the code does not overflow\nthe stack. Furthermore, assume that foo is run using an optimal allocator that achieves perfect\nutilization.\n2.1\nSuppose that foo is executed using just 1 worker. Argue that the space needed to satisfy the calls\nto malloc from foo is X.\nAnswer: X\nNow suppose that foo is executed using 2 workers. In terms of depth and X, what is the maximum\namount of space needed to satisfy the calls to malloc from foo? (Hint: Consider an execution of\nfoo in which one worker steals line 4 from the other worker in every invocation of foo.)\nAnswer: depth · X\n\n2.3\n6.172 Practice Quiz 2\nName\nIn lecture, we saw a theorem stating that a P-worker execution of any Cilk program uses at most\nP times the space used in a serial execution of that Cilk program. Why does the theorem not\napply to foo, or does it?\nAnswer: The theorem applies to stack allocations, not heap allocations\n\n6.172 Practice Quiz 2\nName\n3 Concurrent free lists (2 parts, 8 points)\nBen Bitdiddle is parallelizing a program that uses a free list to handle the allocation of objects of\na fixed size X. Ben's serial code uses a global free-list data structure of type FreeList_t, defined\nas follows:\n\ntypedef struct Node_t {\n/* A */\nstruct Node_t *next;\n11 } Node_t;\ntypedef struct {\n/* B */\nNode_t *head;\n16 } FreeList_t;\nvoid push(FreeList_t *fl, Node_t *node) {\n/* C */\nnode ->next = fl ->head;\n/* D */\nfl ->head = node;\n/* E */\n24 }\nNode_t *pop(FreeList_t *fl) {\n/* F */\nNode_t *node = fl ->head;\n/* G */\nif (NULL != node) {\n/* H */\nfl ->head = node ->next;\n/* I */\n}\n/* J */\nreturn node;\n37 }\n\n3.1\n3.2\n6.172 Practice Quiz 2\nName\nBen considers parallelizing his free list by implementing thread-local free lists. Each thread\nwould only push or pop storage from its local free list. If the thread's local free list is empty, then\nthe thread simply calls the system's malloc.\nDescribe a performance problem that Ben might see with thread-local free lists that can cause\nBen's parallel program to use much more space than his original serial program.\nAnswer: memory drift\nBen decides to modify his serial free-list code into a global free list, with push and pop oper\nations synchronized using mutex locks. Ben wants to add as few instructions as possible to his\nfree-list code to implement a correct and efficient global free list. Ben's free list code contains\nconvenient labels A through J at lines where Ben might insert a new statement.\nWrite an X in each table cell below where Ben should insert the statement in the corresponding\ncolumn at the line specified by the corresponding row. Each row should contain at most one X.\nmutex_t L;\nlock(fl->L);\nunlock(fl->L);\nlock(node->L);\nunlock(node->L);\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nAnswer: X's at (B, mutex_t L;), (C, lock(fl->L)), (E, unlock(fl->L)), (F, lock(fl->L)), (J, unlock(fl->L)).\n\n6.172 Practice Quiz 2\nName\n4 Parallel Strassen (5 parts, 15 points)\nRecall the (cache-oblivious) divide-and-conquer matrix multiplication algorithm from lecture and\nfrom Homework 8. Given two n × n matrices A and B, where n is a power of 2, the algorithm\ncomputes the product C = A · B in Θ(n3) work as follows. First, the algorithm partitions the\nmatrices A, B, and C into four quadrants:\n\nA1,1\nA1,2\nB1,1\nB1,2\nC1,1 C1,2\nA =\n, B =\n, C =\nA2,1\nA2,2\nB2,1\nB2,2\nC2,1 C2,2\nThe algorithm then computes the quadrants of C using eight recursive multiplications:\nC1,1 = A1,1 · B1,1 + A1,2 · B2,1\nC1,2 = A1,1 · B1,2 + A1,2 · B2,2\nC2,1 = A2,1 · B1,1 + A2,2 · B2,1\nC2,2 = A2,1 · B1,2 + A2,2 · B2,2 .\nIn 1969 Volker Strassen invented an asymptotically faster matrix-multiplication algorithm and\nshowed that the n3 matrix multiplication algorithm was not optimal. Strassen's algorithm par\ntitions the matrices into quadrants as before, but then recursively computes seven intermediate\nmatrix products: (Do not waste time verifying that these formulae are correct.)\nM1 = (A1,1 + A2,2) · (B1,1 + B2,2)\nM2 = (A2,1 + A2,2) · B1,1\nM3 = A1,1 · (B1,2 - B2,2)\nM4 = A2,2 · (B2,1 - B1,1)\nM5 = (A1,1 + A1,2) · B2,2\nM6 = (A2,1 - A1,1) · (B1,1 + B1,2)\nM7 = (A1,2 - A2,2) · (B2,1 + B2,2) .\nThe quadrants of the C matrix can be computed in terms of M1, M2,..., M7 as follows:\nC1,1 = M1 + M4 - M5 + M7\nC1,2 = M3 + M5\nC2,1 = M2 + M4\nC2,2 = M1 - M2 + M3 + M6 .\nThe next page presents pseudocode for a parallel implementation of Strassen's algorithm for\nn × n matrices, where n is an even power of 2. Do not waste time trying to understand how the\ncode works. You only need to understand its parallel structure.\n\n6.172 Practice Quiz 2\nName\n\n// Pseudocode for a parallel version of Strassen 's algorithm to\n// multiply two square matrices A , B. The matrices have side length n\n// in each recursive call , where n is an even power of 2.\n// Assume that A11 ... A22 , B11 ... B22 , C11 ... C22 are quadrants of A , B, C.\nstrassen (A , B , n) {\n// Coarsened base case\nif (n < BASE_CASE_SIZE ) return base_case (A , B, n);\n// Compute the quadrants A11 , A12 , etc . for A, B , and C.\n// Create 8 temporary matrices of size n ˆ2/4.\nTemp [8];\nfor ( int i = 0; i < 8; ++ i) Temp [i] = malloc (n * n / 4);\n//////// Block 1\ncilk_spawn { Temp [0] = A21 + A22 ;\nTemp [1] = strassen ( Temp [0] , B11 , n /2) }; // M2\ncilk_spawn { Temp [2] = B12 - B22 ;\nTemp [3] = strassen ( A11 , Temp [2] , n /2) }; // M3\ncilk_spawn { Temp [4] = B21 - B11 ;\nTemp [5] = strassen ( A22 , Temp [4] , n /2) }; // M4\nTemp [6] = A11 + A12 ;\nTemp [7] = strassen ( Temp [6] , B22 , n /2) ;\n// M5\ncilk_sync ; // End block 1\n//////// Block 2\ncilk_spawn { C11 = Temp [5] - Temp [7] }; // C11 = M4 - M5\ncilk_spawn { C12 = Temp [3] + Temp [7] }; // C12 = M3 + M5\ncilk_spawn { C21 = Temp [1] + Temp [5] }; // C21 = M2 + M4\nC22 = Temp [3] - Temp [1];\n// C22 = M3 - M2\ncilk_sync ; // End block 2\n//////// Block 3\ncilk_spawn { Temp [0] = A11 + A22 ;\nTemp [1] = B11 + B22 ;\nTemp [2] = strassen ( Temp [0] , Temp [1] , n /2) }; // M1\ncilk_spawn { Temp [3] = A12 - A22 ;\nTemp [4] = B21 + B22 ;\nC11 += strassen ( Temp [3] , Temp [4] , n /2) };\n// C11 += M7\nTemp [5] = A21 - A11 ;\nTemp [6] = B11 + B12 ;\nC22 += strassen ( Temp [5] , Temp [6] , n /2) ;\n// C22 += M6\ncilk_sync ; // End block 3\n//////// Block 4\ncilk_spawn { C11 += Temp [2] }; // C11 += M1\nC22 += Temp [2];\n// C22 += M1\ncilk_sync ; // End block 4\nfor ( int i = 0; i < 8; ++ i) free ( Temp [i ]) ;\nreturn C;\n88 }\n\n4.1\n6.172 Practice Quiz 2\nName\nWe will first analyze the work and span of this Strassen code. Assume that this Strassen code\nuses a routine for adding or subtracting two n × n matrices in Θ(n2) work and Θ(lgn) span, such\nas in line 52.\nExplain why the recurrence for the work T1(n) of the parallel Strassen code satisfies\nT1(n) = 7T1(n/2) + Θ(n2).\n(1)\nAnswer: We have seven recursive multiplications of matrices of size n/2 × n/2. At each level\nof the recursion, there are a constant number of matrix additions / subtractions to produce the\nintermediate matrices to pass into the next recursive calls.\n4.2\nThe solution to the work recurrence (1) is\nT1(n) = Θ(na lgb n)\nfor some values a and b. Select the correct values for a and b from the choices below.\nA\na = 2, b = 0\nB\na = 2, b = 1\nC\na = lg7, b = 0\nD\na = lg7, b = 1\nE\na = 3, b = 0\nF\nNone of the above\nAnswer: C\n\n4.3\n4.4\n6.172 Practice Quiz 2\nName\nThe recurrence for the span Tinf(n) of this parallel Strassen code is composed from the spans of\nfour blocks in the code: Block 1 (lines 51-60), Block 2 (lines 62-67), Block 3 (lines 69-79), and\nBlock 4 (lines 81-84). For each block, write an X in the table below to identify the span of that\nblock.\nΘ(lgn)\nTinf(n/2) + Θ(lgn)\n3Tinf(n/2) + Θ(lgn)\n4Tinf(n/2) + Θ(lgn)\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nAnswer: Block 1 has span Tinf(n/2) + Θ(log2 n). Block 2 has span Θ(log2 n). Block 3 has span\nTinf(n/2) + Θ(log2 n). Block 4 has span Θ(log2 n).\nDescribe how the spans of those four blocks compose to make the recurrence for the span of this\nparallel Strassen code equal to\nTinf(n) = 2Tinf(n/2) + Θ(lgn).\n(2)\nAnswer: The spans of the four blocks sum to give Tinf(n) = 2Tinf(n/2) + Θ(log2 n).\n\n6.172 Practice Quiz 2\nName\n4.5\nThe solution to the span recurrence (2) is\nTinf(n) = Θ(na lgb n)\nfor some values a and b. Select the correct values for a and b from the choices below.\nA\na = 0, b = 1\nC\na = 1, b = 0\nE\na = 2, b = 0\nG\na = lg7, b = 0\nI\nNone of the above\nB\na = 0, b = 2\nD\na = 1, b = 1\nF\na = 2, b = 1\nH\na = lg7, b = 1\nAnswer: C\n\n6.172 Practice Quiz 2\nName\n5 Space usage for Strassen (6 parts, 18 points)\nThis question studies the space usage during an execution of the parallel Strassen code described\nin Question 4. As lines 48-49 in the pseudocode show, this parallel Strassen code uses Θ(n2)\ntemporary space per recursive call.\n5.1\nDescribe why the total amount of temporary space S1(n) used by a 1-worker execution of this\nparallel Strassen program satisfies the following recurrence:\nS1(n) = S1(n/2) + Θ(n2).\n(3)\nAnswer: In a serial execution, at any point in time, the execution of parallel Strassen involves\njust a single stack of frames. Each frame on the stack uses Θ(n2) space. Hence the recurrence is\nas stated.\n5.2\nSolve the recurrence (3), and write your answer using Θ-notation.\nAnswer: The solution to the recurrence is Θ(n2).\n\n6.172 Practice Quiz 2\nName\nAlthough the Cilk scheduler provides a weak bound on the space used by a P-worker execu\ntion of this parallel Strassen code, we can prove a stronger bound by considering the worst-case\nrecursion tree for this code. It turns out that this worst-case recursion tree for space usage is as\nillustrated below:\n...\nΘ((n/2)2)\nΘ((n/2k)2)\nΘ((n/2k)2)\nΘ((n/2k)2)\n...\nΘ((n/2)2)\nΘ((n/2)2)\nΘ(1)\nΘ(1)\nΘ(1)\n...\nP nodes\nLevel 0\nΘ(n2)\nLevel 1\nLevel k\n...\nThis recursion tree recursively branches 4 ways until it reaches the first level k that contains P\nnodes. Each worker then serially executes the computation under a distinct level-k node, which\nthe recursion tree models with 1-way branching after level k.\n5.3\nWhy is the branching factor 4 for the top part of this recursion tree?\nAnswer: The code spawns off at most four recursive calls in parallel at any point during its\nexecution, i.e., either in block 1 or block 2. Therefore, the branching factor of the space overhead\nis four because the space allocated is reused across all blocks.\n5.4\nArgue that the total space usage in the top part of the recursion tree, in all levels i ≤ k, is\nΘ(n2 log4 P).\nAnswer: The height of the tree is k = log4P. Each level uses Θ(n2) space. The result follows from\nthe product of these terms.\n\n5.5\n5.6\n6.172 Practice Quiz 2\nName\nArgue that the total space usage in the bottom part of the recursion tree, in all levels i > k, is\nΘ(n2).\nAnswer: As shown in 5.2, the total space used at the lower levels of the recursion is Θ(n2).\nWhat is the total worst-case space used by an execution of parallel Strassen? Express your answer\nin Θ-notation.\nAnswer: The space usage below any particular node at level k is S1(n/2k) = Θ(n2/4k). Because\nk = log4 P, this equation simplifies to Θ(n2/P). Because there are P nodes at level k, the total\nspace usage below level k is Θ(n2).\n\n6.172 Practice Quiz 2\nName\n6 Cache-oblivious Strassen (3 parts, 13 points)\nThis question examines the cache-efficiency of Strassen's algorithm for matrix multiplication. For\nthis question, we will consider a serial implementation of Strassen's algorithm, such as the serial\nelision of the parallel implementation described in Question 4. For this problem, assume an\nideal cache model (fully associative with an optimal or LRU replacement policy, as appropriate)\nwith cache size M and cache-line length B. Assume that the cache is tall (i.e. M = ω(B2)).\nFurthermore, assume that Strassen's algorithm is run using an optimal allocator that achieves\nperfect utilization. Assume that the input matrix is stored in row-major order, and also assume\nthat the routine for adding or subtracting two m-by-m submatrices is Θ(m2/B).\nThe recurrence for the worst-case number Q(n) of cache misses incurred by Strassen's algorithm\nwhen multiplying two n × n matrices is given by\n(\n√\nΘ(n2/B)\nif 0 ≤ n < c M for sufficiently small constant c ≤ 1;\nQ(n) =\n(4)\n7Q(n/2) + Θ(n2/B) otherwise.\n6.1\nWhy is Q(n) = 7Q(n/2) + Θ(n2/B) for n sufficiently large?\nAnswer: We have seven recursive calls on matrices of size n/2. In addition, there are a constant\nnumber of matrix additions and subtractions to construct the inputs for the recursive calls, so the\ncache misses to create the inputs is Θ(n2/B).\n6.2\n√\nWhy is Q(n) = Θ(n2/B) for n < c M?\nAnswer: All the matrices used by the algorithm have Θ(n2) elements. When n is sufficiently\nsmall that all these matrices fit into cache, we have n2 ≤ cM. The number of cache misses to\nbring them all into cache is Θ(n2/B). Once in memory, the recursive nature of the program is\nsuch that no more cache misses are incurred on any subproblems.\n\n6.3\n6.172 Practice Quiz 2\nName\nSketch the recursion tree for the recurrence for (4). Compute the height and the number of leaves\nof the recursion tree, and label the internal nodes and leaves of the tree with the corresponding\nnumber of cache misses incurred at them. Solve the recurrence, providing a tight asymptotic\nbound in simple form.\nSketch of recursion tree for Q(n):\nHeight =\nNumber of leaves =\nMisses per leaf =\nQ(n) =\nAnswer: Height = lgn - lg(cM).\n√\n√\nNumber of leaves = 7lgn-lg(c M) = Θ(nlg7/ M\nlg7).\nMisses per leaf = Θ(M/B).\nlg7\nn\nQ(n) = Θ( BM(log7)/2-1 ).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cba3aa03dc8216608c6ed7c95d6310e4_MIT6_172F18_practicequiz3.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 3\nPractice Quiz 3\nName:\nInstructions\n- DO NOT open this quiz booklet until you are instructed to do so.\n- This quiz booklet contains 14 pages, including this one. You have 80 minutes to earn 80\npoints.\n- This quiz is closed book, but you may use one handwritten, double-sided 8 1/200 × 1100 crib\nsheet and the Master Method card handed out in lecture.\n- When the quiz begins, please write your name on this coversheet, and write your name\non the top of each page, since the pages may be separated for grading.\n- Some of the questions are true/false, and some are multiple choice. You need not explain\nthese answers unless you wish to receive partial credit if your answer is wrong. For these\nkinds of questions, since incorrect answers will be penalized, do not guess unless you are\nreasonably sure.\n- Good luck!\nNumber\nQuestion\nParts\nPoints\nScore\nGrader\nName on Every Page\nTrue or False\nCaching in Matrix Multiplication\nDining Philosophers\nCaching for Karatsuba\nLock-free FIFO Queue\nTotal\n\n6.172 Practice Quiz 3\nName\n1 True or False (10 parts, 20 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer, unless you want to leave open the possibility of receiving partial credit\nif your answer is wrong. Comments will have no impact on a correct answer.\n1.1\nParallel pull algorithms are generally slower than parallel push algorithms because they require\nstate updates to be atomic.\nTrue\nFalse\n1.2\nStreaming writes avoid the latency of reading cache blocks into the cache.\nTrue\nFalse\n1.3\nSuppose that a program runs on a machine with a fully associative cache of size M and incurs n\nconflict misses. Then we must have n > M.\nTrue\nFalse\n1.4\nIn the ideal-cache model, an n × n submatrix of an underlying matrix stored in row-major order\nalways fits in a cache of size Θ(n2).\nTrue\nFalse\n1.5\nSuppose that a parallel Cilk program exhibits poor speedup when run on a multicore machine\nwith P processing cores. If the cause is insufficient memory bandwidth, it can often be detected\nby executing P copies of the serial elision of the program simultaneously and comparing the\nruntime with that of a single serial elision.\nTrue\nFalse\n\n1.6\n6.172 Practice Quiz 3\nName\nConcurrent programs that are data-race free and obey atomicity constraints are deterministic.\nTrue\nFalse\n1.7\nA yielding mutex lock should generally be preferred over a spinning mutex lock when a critical\nsection is large.\nTrue\nFalse\n1.8\nHardware can reorder memory operations by allowing a store to bypass (move earlier in the\nexecution order) a load to a different location in order to compensate for memory-system latency.\nTrue\nFalse\n1.9\nRunning on a machine with a relaxed-consistency memory model can cause Peterson's algorithm\nfor mutual exclusion to operate incorrectly unless memory fences are used.\nTrue\nFalse\n1.10\nThe storage cost of a Compressed Sparse Rows (CSR) representation of a sparse graph with n\nvertices and m edges is Θ(m + n).\nTrue\nFalse\n\n6.172 Practice Quiz 3\nName\n2 Caching in Matrix Multiplication (3 parts, 12 points)\nBen Bitdiddle writes the following code to multiply two matrices. Ben's machine has a 32KB\n(32,768 byte) L1 data cache with 64-byte cache lines. Assume that Ben's machine has a fully\nassociative cache with least-recently-used (LRU) replacement and code executes on a single pro\ncessor.\n\nvoid matrix_multiply ( int32_t *a, int32_t *b , int32_t *c ,\nfor ( size_t row = 0; row < n; row ++)\nfor ( size_t col = 0; col < n; col ++)\nfor ( size_t i = 0; i < n; i ++)\nc[ row *n + col ] += a[ row *n + i] * b[i*n + col ];\n}\nsize_t n) {\n\nEstimate the number of L1-cache misses each of the following three calls incurs.\n2.1\nmatrix_multiply (a ,\n\nb, c , 64) ;\n\nA\nB\nC\nD\n(64 · 3)/16 = 12\n64 · 3 = 192\n(642 · 3)/16 = 768\n642 + (642 · 2)/16 = 4,608\n\n6.172 Practice Quiz 3\nName\n2.2\nmatrix_multiply (a ,\n\nb, c , 256) ;\n\nA\nB\nC\nD\n(2562 · 3)/16 = 12,288\n(2563 + 2562 · 2)/16 = 1,056,768\n(2563 · 3)/16 = 3,145,728\n2563 + 2562 + 2562/16 = 16,846,848\n2.3\n\nmatrix_multiply(a, b, c, 1024);\n\nA\n(10242 · 3)/16 = 196,608\nB\n(10243 + 10242 · 2)/16 = 67,239,936\nC\n(10243 · 2)/16 + 10242 = 135,266,304\nD\n10243 + (10243 + 10242)/16 = 1,140,916,224\n\n6.172 Practice Quiz 3\nName\n3 Dining Philosophers (4 parts, 16 points)\nIn the Dining Philosophers problem introduced in lecture, each of n philosophers needs the two\nchopsticks on either side of his/her plate to eat his/her noodles. Consider the pseudocode for\nphilosopher i below, which uses fair mutexes:\n\nwhile (1) {\nthink();\nlock(& chopstick[i].L);\nlock (& chopstick [(i+1)%n].L);\neat ();\nunlock (& chopstick [i].L);\nunlock (& chopstick [(i +1)%n].L);\n8 }\n\nWe saw in lecture that a deadlock may occur with this code.\n3.1\nIn a first attempt to solve this problem, Ben Bitdiddle sets a time limit t. If a philosopher holds\nhis or her first chopstick for t seconds without acquiring the second chopstick, the philosopher\nputs down the first chopstick, waits another t seconds, and then restarts the chopstick-acquisition\nprotocol.\nWhich of the following may happen?\nA\nA critical-section violation may occur\nYes\nNo\nB\nDeadlock may occur\nYes\nNo\nC\nStarvation may occur\nYes\nNo\nD\nLivelock may occur\nYes\nNo\n\n6.172 Practice Quiz 3\nName\n3.2\nIn a second attempt to solve the Dining Philosophers problem, Alyssa P. Hacker decides to keep\ntrack of how many philosophers are attempting to eat, and only allow at most n - 1 philoso\nphers to attempt eating at the same time. Alyssa uses the atomic compare-and-swap operation\nCAS(address, old, new).\n\nvolatile int count = 0;\nwhile (1) {\nthink ();\nint c = count;\nif (c < n-1 && CAS(&count , c, c+1)) {\nlock(& chopstick[i].L);\nlock (& chopstick [(i+1)%n].L);\neat ();\nunlock (& chopstick [i].L);\nunlock (& chopstick [(i +1)%n].L);\nc = count;\nwhile (!CAS(&count , c, c-1)) {\nc = count;\n}\n}\n16 }\n\nWhich of the following may happen?\nA\nA critical-section violation may occur\nYes\nNo\nB\nDeadlock may occur\nYes\nNo\nC\nStarvation may occur\nYes\nNo\nD\nLivelock may occur\nYes\nNo\n\n6.172 Practice Quiz 3\nName\nIn a third attempt to solve the Dining Philosophers problem, Professor Kung writes the following\ncode:\n\nwhile (1) {\nthink();\nif (i & 1) {\nlock (& chopstick [i].L);\nlock (& chopstick [(i+1) %n].L);\n} else {\nlock (& chopstick [(i+1) %n].L);\nlock (& chopstick [i].L);\n}\neat ();\nunlock (& chopstick [i].L);\nunlock (& chopstick [(i +1)%n].L);\n13 }\n\n3.3\nWhich of the following may happen?\nA\nA critical-section violation may occur\nYes\nNo\nB\nDeadlock may occur\nYes\nNo\nC\nStarvation may occur\nYes\nNo\nD\nLivelock may occur\nYes\nNo\n3.4\nAssume that n is an even number. Which of the following may happen?\nA\nA critical-section violation may occur\nYes\nNo\nB\nDeadlock may occur\nYes\nNo\nC\nStarvation may occur\nYes\nNo\nD\nLivelock may occur\nYes\nNo\n\n6.172 Practice Quiz 3\nName\n4 Caching for Karatsuba (4 parts, 17 points)\nFor this problem, assume an ideal cache model (fully associative with an optimal or LRU replace\nment policy, as appropriate) with cache size M and cache-line length B.\nRecall that a polynomial in the variable x is a formal sum\nn-1\nX\nA(x) =\najxj ,\nj=0\nwhere the values a0, a1,..., an-1 are the coefficients of the polynomial and n - 1 is its degree. Any\ninteger strictly greater than the degree of a polynomial is a degree-bound of that polynomial.\nComputing the product of two polynomials with degree-bound n takes Θ(n2) work if the normal\ngrade-school multiplication algorithm is used. In 1962, however, Karatsuba invented a faster\nway based on a clever scheme for multiplying two linear polynomials a1x + a2 and b1x + b2.\nThe product is a1b1x2 + (a1b2 + a2b1)x + a2b2, which normally involves four multiplications of\ncoefficients. Karatsuba's algorithm accomplishes it in three:\nm1 = a1 · b1 ,\nm2 = a2 · b2 ,\nm3 =\n(a1 + a2) · (b1 + b2) .\nThe product of the two polynomials is then m1x2 + (m3 - m1 - m2)x + m2.\nThis algorithm can be performed recursively using divide-and-conquer, similar to Strassen's\nalgorithm for multiplying matrices. The code for Karatsuba polynomial multiplication is given\non the following page, where each polynomial is represented as an array of its coefficients. (Do\nnot spend time trying to understand the code. You only need to understand its performance\ncharacteristics.)\n\n6.172 Practice Quiz 3\nName\n\n// Compute the polynomial product C(x) = A(x) * B(x).\n// A(x) and B(x) each have degree bound n ( and n elements ).\n// C(x) has degree bound 2n -1 ( and 2n -1 elements ).\n// Each polynomial is represented as an array of coefficients .\nvoid karatsuba ( double * A , double * B, int n, double * C) {\nassert (( n & (-n)) == n);\n// n must be an exact power of 2.\nif (n == 1) // degree 0\nC [0] = A [0] * B [0];\nelse {\ndouble M1 [n -1] , M2 [n -1] , M3[n -1] , AA [n /2] , BB [n /2];\ndouble * A1 = A;\ndouble * A2 = A + n /2;\ndouble * B1 = B;\ndouble * B2 = B + n /2;\nkaratsuba (A1 , B1 , n /2 , M1 );\n// M1(x) = A1 (x) * B1(x)\nkaratsuba (A2 , B2 , n /2 , M2 );\n// M2(x) = A2 (x) * B2(x)\nfor ( int i = 0; i < n /2; i ++)\n// AA(x) = A1 (x) + A2(x)\nAA[i] = A1 [i] + A2 [i ];\nfor ( int i = 0; i < n /2; i ++)\n// BB(x) = B1 (x) + B2(x)\nBB[i] = B1 [i] + B2 [i ];\nkaratsuba (AA , BB , n /2 , M3 );\n// M3(x) = AA (x) * BB(x)\nfor ( int i = 0; i < n -1; i ++) { // C(x) = M3 (x) - M2 (x) - M1(x)\nC[i] = M1 [i];\nC[i + n] = M2[i ];\n}\nC[n -1] = 0;\nfor ( int i = 0; i < n -1; i ++)\nC[i + n /2] += M3 [i] - M1[i] - M2 [i ];\n}\n39 }\n\n4.3\n6.172 Practice Quiz 3\nName\n4.1\nThe work T(n) required by this program to multiply two polynomials of degree-bound n satisfies\nthe recurrence\nT(n) = 3T(n/2) + Θ(n) .\nWhat is the solution to this recurrence?\nA\nΘ(n)\nB\nΘ(n lgn)\nlog3 2)\nC\nΘ(n\nlg3)\nD\nΘ(n\nE\nNone of the above\nThe worst-case number Q(n) of cache misses incurred by the Karatsuba algorithm is given by\nΘ(n/B)\nif n < cM, where c ≤ 1 is a sufficiently small constant,\nQ(n) =\n3Q(n/2) + Θ(n/B) otherwise.\n4.2\nExplain why Q(n) = 3Q(n/2) + Θ(n/B) if n is sufficiently large.\nExplain why Q(n) = Θ(n/B) if n < cM.\n\n6.172 Practice Quiz 3\nName\n4.4\nSketch a recursion tree for the recurrence for Q(n). Compute the height and the number of leaves\nof the recursion tree, and label the internal nodes and leaves of the tree with the corresponding\nnumber of cache misses incurred at them. Solve the recurrence, providing a tight asymptotic\nbound in simple form.\nHeight =\nNumber of leaves =\nQ(n) =\n\n6.172 Practice Quiz 3\nName\n5 Lock-free FIFO Queue (1 part, 14 points)\nConsider the following lock-free FIFO queue with blocking semantics for a single enqueuer and\na single dequeuer.\n\nint head = 0;\nint tail = 0;\ndouble *items = malloc(CAPACITY*sizeof(double));\nvoid enq(double x) {\nwhile (tail - head == CAPACITY) {}\nitems[tail % CAPACITY] = x;\ntail ++;\n11 }\nvoid deq(double x) {\nwhile (tail - head == 0) {}\ndouble x = items[head % CAPACITY ];\nhead ++;\nreturn x;\n20 }\n\nThe queue is implemented as an array. Initially, the head and tail fields are equal, and the\nqueue is empty. If head and tail differ by exactly CAPACITY, then the queue is full. The enq()\nfunction reads the head field, and if the queue is full, it repeatedly checks head until the queue is\nno longer full. It then stores the double x in the items array and increments the tail field. The\ndec() function works in a symmetric way.\n\n6.172 Practice Quiz 3\nName\nFor this problem, assume the x86-64 total-store-order memory model discussed in lecture.\nSince the queue is blocking, removing an item from an empty queue or inserting an item into\na full one causes the thread to block (or wait). The surprising thing about this queue is that it\nrequires only atomic loads and stores and not an atomic read-modify-write operation. It may,\nhowever, require the use of volatile variables, a compiler fence (asm volatile(\"\": : :\"memory\")),\nor a hardware fence (asm volatile(\"mfence\": : :\"memory\")).\nFor each of the seven missing code snippets, choose one of the following such that the code is\ncorrect and has the highest performance:\n- /* Nothing */\n- volatile\n- COMPILER_FENCE();\n- HARDWARE_FENCE();\nLine 1:\nLine 2:\nLine 3:\nLine 7:\nLine 9:\nLine 15:\nLine 17:\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/13654259f421b165debaaed3b79e0bed_MIT6_172F18_practicequiz4.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 4\nPractice Quiz 4\nName:\nInstructions\n- DO NOT open this quiz booklet until you are instructed to do so.\n- This quiz booklet contains 13 pages, including this one. You have 80 minutes to earn 80\npoints.\n- This quiz is closed book, but you may use one handwritten, double-sided 8 1/200 × 1100 crib\nsheet and the Master Method card handed out in lecture.\n- When the quiz begins, please write your name on this coversheet, and write your name\non the top of each page, since the pages may be separated for grading.\n- Some of the questions are true/false, and some are multiple choice. You need not explain\nthese answers unless you wish to receive partial credit if your answer is wrong. For these\nkinds of questions, incorrect answers will be penalized, so do not guess unless you are\nreasonably sure.\n- Good luck!\nNumber\nQuestion\nParts\nPoints\nScore\nGrader\nName on Every Page\nTrue or False\nBack Of The Envelope\nBranch Prediction\nVariable Byte Compression\nLLVM\nAlgorithm Analysis\nTotal\n\n1 TRUE OR FALSE (8 PARTS, 16 POINTS)\n1 True or False (8 parts, 16 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer unless you want to leave open the possibility of receiving partial credit if\nyour answer is wrong.\n1.1\nBecause thermal limits prevented CPU manufacturers from increasing clock frequencies signifi\ncantly, they began to produce multicore microprocessors.\nTrue\nFalse\n1.2\nInlining a function tends to improve performance by reducing the number of instruction cache\nmisses.\nTrue\nFalse\n1.3\nSuppose that a recursive function performs work on a problem of size N according to the recur\nrence T(N) = 2T(N/3) + Θ(N lg N). Then coarsening the recursion is unlikely to significantly\nimprove performance.\nTrue\nFalse\n1.4\nCompilers eliminate data dependencies through register renaming by replacing x86 logical reg\nisters with physical registers.\nTrue\nFalse\n\n1.5\n1 TRUE OR FALSE (8 PARTS, 16 POINTS)\n1.5\nThere can never be a true-, anti-, or output-data dependence between the following two lines of\ncode:\nmovl %eax , 8(%esi)\nlea 24(%esi, %edi, 8) , %ecx\nTrue\nFalse\n1.6\nWhen using the MSI (modified, shared, invalid) cache coherence protocol, the state of a cache\nline in a processor's cache may transition directly from shared to invalid.\nTrue\nFalse\n1.7\nIf a memory location in a program is read by two logically parallel instructions, then a read-read\ndeterminacy race exists.\nTrue\nFalse\n1.8\nA greedy scheduler schedules a computation with work T1 and span Tinf in time Tp ≤ max(T1/P,Tinf)\non a P-processor ideal parallel computer.\nTrue\nFalse\n\n2 BACK OF THE ENVELOPE CALCULATIONS (3 PART, 10 POINTS)\n2 Back Of The Envelope Calculations (3 Part, 10 Points)\nPerform a back-of-the-envelope calculation for the work of a serial execution of get_row_sums on\na 1000-by-1000 matrix. Assume that you have one 3 GHz scalar processing core which executes\nonly one instruction per cycle, and that everything is in cache.\n\n# define ROWS 1000\n# define COLS 1000\ntypedef int32_t element_t ;\nvoid get_row_sums ( element_t M[ ROWS ][ COLS ],\nelement_t row_sums [ ROWS ]) {\nfor ( int32_t i = 0; i < ROWS ; i ++) {\nelement_t sum = 0;\nfor ( int32_t j = 0; j < COLS ; j ++) {\nsum += M[i ][ j];\n}\nrow_sums [i] = sum ;\n}\n15 }\n\n2.1\nHow long does get_row_sums take assuming that vectorization is disabled? Please circle the letter\nof your answer.\nA\n0.00001-0.001 seconds.\nB\n0.001-0.1 seconds.\nC\n0.1-10 seconds.\nD\n10-1000 seconds.\nE\nNone of the above.\n\n2.2\n2.2\n2 BACK OF THE ENVELOPE CALCULATIONS (3 PART, 10 POINTS)\nAfter the code is compiled with vectorization enabled (with 128-bit vector registers), what is the\nperformance of the program compared to the original program? Please circle the letter of your\nanswer.\nA\nMore than twice as slow as the code without vectorization.\nB\nSlower, but less than twice as slow as the code without vectorization.\nC\nAbout the same as the code without vectorization.\nD\nFaster, but less than twice as fast as the code without vectorization.\nE\nMore than twice as fast as the code without vectorization.\n2.3\nWith vectorization enabled (with 128-bit vector registers), what is the performance of the program\nif you change element_t from int32_t to int8_t? Please circle the letter of your answer.\nA\nMore than twice as slow as the vectorized code with int32_t's.\nB\nSlower, but less than twice as slow as the vectorized code with int32_t's.\nC\nAbout the same as the vectorized code with int32_t's.\nD\nFaster, but less than twice as fast as the vectorized code with int32_t's.\nE\nMore than twice as fast the vectorized code with int32_t's.\n\n3 BRANCH PREDICTION (3 PARTS, 9 POINTS)\n3 Branch Prediction (3 parts, 9 points)\nUsing asymptotic Θ-notation, give the expected number of branch misses for the following sort\ning algorithm on an array of N distinct integers on (1) a sorted input (increasing order); (2) a\nreverse sorted input (decreasing order); and (3) a randomly ordered input.\n\nvoid insertion_sort ( int * A ,\nfor ( int j = 0; j < N; j ++)\nint insert = A[j];\nint slot = j;\nwhile ( slot > 0 && insert\nA[ slot ] = A[ slot -1];\nslot -;\n}\nA[ slot ] = insert ;\n}\n}\nint N) {\n{\n< A[ slot -1]) {\n\n3.1 Sorted\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\n3.2 Reverse Sorted\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\n3.3 Random Order\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\n\n4 VARIABLE BYTE COMPRESSION (2 PARTS, 11 POINTS)\n4 Variable Byte Compression (2 parts, 11 points)\nByte codes are used as a way to compress sequences of positive integers of varying magnitudes.\nEach integer is represented as a series of bytes, where the most significant bit of a byte is called\nthe continuation bit, and the remaining 7 bits are the payload. To encode an integer, we take its\nbinary representation ignoring leading zeros and group the remaining bits in 7-bit payloads. The\nremaining payloads are placed in bytes, with the least significant bits being in the first byte and\nthe most significant bits being in the last byte. The continuation bit in each byte is set to 1 except\nfor the last byte where it is set to 0.\nFor example, to encode the integer 6172, we inspect its binary representation (without the leading\n0's), which is 0b1100000011100. We create two payloads, h0011100i and h0110000i, and place\nthem into bytes with the first byte's continuation bit set to 1 and the second byte's continuation\nbit set to 0. The resulting bytes that encode the integer 6172 are 0b10011100 and 0b00110000,\nwhere the first byte encodes the lower-order bits and the second byte encodes the higher-order\nbits.\n4.1\nSuppose that the sequence of bytes that resulted from encoding some integer x was 0b11001010,\n0b10001011, and 0b00101100. What is x in hexadecimal notation ignoring the leading zeros?\nA\n0x1645CA\nB\n0x160BCA\nC\n0x2C8BCA\nD\n0xB05CA\nE\nNone of the above.\n\n4.2\n4 VARIABLE BYTE COMPRESSION (2 PARTS, 11 POINTS)\n4.2\nThe following program encodes an array In of N positive integers into a sequence of bytes, stored\nin the array Out. Assume that sufficient memory has been allocated for Out.\n\nvoid encode(uint32_t* In, int32_t N, unsigned char* Out) {\nfor(int32_t i=0; i < N; i++) {\nuint32_t x = In[i];\nwhile(x) {\nchar byte =\n(A)\n;\nx =\n(B)\n;\nif(x)\n{\n(C)\n;}\n*Out++ = byte;\n}\n}\n12 }\n\nFor each blank in the code, write its label (A, B, or C) next to the expression that best fits. (Hint:\nSome blanks can take more than one expression, but only one is \"best.\")\nbyte & 0x80\nx << 7\nbyte & 0x8\nx = x << 7\nbyte & 0x80\nx = x >> 1\nbyte &= 0x80\nx >> 8\nbyte ˆ 0x80\nx ˆ 0x7f\nbyte |= 0x80\nx ˆ 0x80\nbyte\nx | 0x7f\nx >> 7\nx | 0x80\nx & 0x7f\nx++\nx & 0x7\nx-7\nx & 0x80\nx\n\nC\n5 LLVM (6 PARTS, 12 POINTS)\n5 LLVM (6 parts, 12 points)\nThis question explores your understanding of control-flow graphs and Bentley optimizations.\nConsider the following complete C source file.\n\nint value1 () ;\nint value2 () ;\n__attribute__ (( const ))\nint bar ( bool p) {\nint val ;\nif (p)\nval = value1 ();\nelse\nval = value2 ();\nreturn val ;\n}\nvoid foo ( int * restrict Y,\nconst int * restrict X ,\nint n , bool p) {\nfor ( int i = 0; i < n; ++ i)\nY[i] += bar (p) * X[i ];\n19 }\n\nWhen compiling this C code, LLVM can perform optimizations involving the functions foo and\nbar defined in this file, but not on the functions value1 and value2, which are only declared in\nthis file. Suppose that LLVM compiles the following functions with the specified optimizations:\nA\nThe function bar with no optimization.\nB\nThe function foo with no optimization.\nThe function foo with function inlining and no other optimizations.\nD\nThe function foo with loop unrolling and no other optimizations.\nE\nThe function foo with code hoisting followed by function inlining and no other optimiza\ntions.\nThe next page contains several pictures of control-flow graphs. For each control-flow graph,\ncircle either the unique letter of the function-and-optimization scenario from above that it corre\nsponds to, or circle \"None\" if it does not correspond to any of the scenarios. While not strictly\nnecessary to solve this question, the LLVM IR is provided on Page 11 for your reference.\n\n5 LLVM (6 PARTS, 12 POINTS)\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\n\n5 LLVM (6 PARTS, 12 POINTS)\nHere is the LLVM IR for the two functions bar and foo without function inlining, loop un\nrolling, or code hoisting.\n\ndefine i32 @bar(i1 zeroext) #0 {\nbr i1 %0, label %2, label %4\n; <label >:2:\n; preds = %1\n%3 = call i32 (...) @value1 () #3\nbr label %6\n; <label >:4:\n; preds = %1\n%5 = call i32 (...) @value2 () #3\nbr label %6\n; <label >:6:\n; preds = %4, %2\n%.0 = phi i32 [ %3, %2 ], [ %5, %4 ]\nret i32 %.0\n}\ndefine void @foo(i32* noalias , i32* noalias , i32 , i1 zeroext) #2 {\n%5 = icmp sgt i32 %2, 0\nbr i1 %5, label %4, label %. _ret_edge\n21 ; <label >:6:\n; preds = %4, %6\n%.01 = phi i32 [ 0, %4 ], [ %16, %6 ]\n%7 = call i32 @bar(i1 zeroext %3) #4\n%8 = sext i32 %.01 to i64\n%9 = getelementptr inbounds i32 , i32* %1, i64 %8\n%10 = load i32 , i32* %9, align 4\n%11 = mul nsw i32 %7, %10\n%12 = sext i32 %.01 to i64\n%13 = getelementptr inbounds i32 , i32* %0, i64 %12\n%14 = load i32 , i32* %13, align 4\n%15 = add nsw i32 %14, %11\nstore i32 %15, i32* %13, align 4\n%16 = add nsw i32 %.01, 1\n%17 = icmp slt i32 %16, %2\nbr i1 %17, label %6, label %. _ret_edge\n37 ._ret_edge:\n; preds = %6, %4\nret void\n39 }\n\n6 ALGORITHM ANALYSIS (4 PARTS, 20 POINTS)\n6 Algorithm Analysis (4 parts, 20 points)\nRecall the prefix sum algorithm from Homework 5, and consider the following alternative parallel\nprefix sum algorithm, where A is the input array with N elements. The array element A[N + 1]\nis also allocated to store the total sum. Assume that N is an exact power of 2.\nNote that you do not need to understand why the code is correct -- just its structure.\n\nvoid upsweep(int64_t* A, int64_t N) {\nfor (int64_t d = 1; d < N; d*=2) {\ncilk_for (int64_t k = 0; k < N; k += 2*d) {\nA[k + 2*d - 1] = A[k + d - 1] + A[k + 2*d - 1];\n}\n}\n}\nvoid downsweep(int64_t* A, int64_t N) {\nA[N] = A[N - 1] //total sum\nA[N - 1] = 0;\nfor (int64_t d = N / 2; d >= 1; d = d / 2) {\ncilk_for(int64_t i = 0; i < N; i += 2*d) {\nint64_t temp = A[i + d - 1];\nA[i + d - 1] = A[i + 2*d - 1];\nA[i + 2*d - 1] = temp + A[i + 2*d - 1];\n}\n}\n19 }\nvoid prefix_sum(int64_t* A, int64_t N) {\nupsweep(A, N);\ndownsweep(A, N);\n24 }\n\n6.1\nWhat is the work of prefix_sum? Give your answer in terms of N using Θ-notation.\nWhat is the span of prefix_sum? Give your answer in terms of N using Θ-notation.\n6.2\n\n6.3\n6 ALGORITHM ANALYSIS (4 PARTS, 20 POINTS)\nConsider the following algorithm for computing a histogram on a length-N array A of in\ntegers in the range {0,1,...,k - 1}, where k ≤ N. The algorithm uses a two-dimensional array\nH, whose dimensions are (N/k) + 1 rows by k + 1 columns. Assume for simplicity that N is\ndivisible by k. Here is the algorithm:\n1. Partition the array into N/k length-k subarrays A0, A1,..., AN/k-1.\n2. Initialize each element of the matrix H to 0.\n3. For all r = 0,1,..., N/k - 1, sequentially count the number of occurrences of each integer\nin Ar, and store the number of occurrences of each integer c in H[r][c] (which fills in a row\nof H). Each row is processed sequentially but different rows can be processed in parallel.\n4. For each c = 0,1,...,k - 1, perform a prefix sum on the cth column of H, that is, on the values\nH[r][c] for all 0 ≤ r < N/k. Different columns can be processed in parallel. H[N/k][c] now\nstores the number of integers of value c in A.\nThe code for this algorithm is shown below. Assume that vertical_prefix_sum(H, c, N/k)\ncomputes the prefix sum on the cth column of H with N/k entries in parallel.\n\n// H has dimension (n/k)+1 by k+1\nvoid histogram(int64_t* A, int64_t ** H, int64_t N, int64_t k) {\ncilk_for (int64_t r = 0; r < N/k; r++) {\nfor (int64_t c = 0; c < k; c++) {\nH[r][c] = 0;\n}\nfor (int64_t c = 0; c < k; c++) {\nH[r][A[r * N/k + c]]++;\n}\n}\ncilk_for (int64_t c = 0; c < k; c++) {\nvertical_prefix_sum(H, c, N/k);\n}\n// resulting histogram is stored in H[N/k]\n15 }\n\n6.3\nWhat is the asymptotic work of histogram? Give your answer using Θ-notation in terms of N, k,\nand Wps(N), where Wps(N) is the work of prefix sum on N elements.\nWhat is the asymptotic span of the histogram? Give your answer using Θ-notation in terms of\nN, k, and Sps(N), where Sps(N) is the span of prefix sum on N elements.\n6.4\n\nIntel x86 Assembly Language Cheat Sheet\nInstruction\nEffect\nExample\nData movement\nmov src, dest\nCopy src to dest\nmov $10,%eax\nArithmetic\nadd src, dest\nDest = dest + src\nadd $10, %esi\nmul reg\nedx:eax = eax * reg (colon means the\nresult spans across two registers)\nmul %esi\ndiv reg\nidiv reg\nedx = edx:eax mod reg\neax = edx:eax / reg\ndiv %edi\ninc dest\nIncrement destination\nInc %eax\ndec dest\nDecrement destination\ndec (%esi)\nsbb arg1, arg2\nIf CF = 1, (this is set by cmp instruction;\nrefer cmp)\narg2 = arg2 - (arg1 + 1)\nelse\narg2 = arg2 - arg1\nsbb %eax, %ebx\nFunction Calls\ncall label\nPush eip, transfer control\ncall _fib\nret\nPop eip and return\nret\npush item\nPush item (constant or register) to stack\npushl $32\npushl %eax\npop [reg]\nPop item from stack; optionally store to\nregister\npop %eax\npopl\nBitwise Operations\nand src,dest\nDest = src & dest\nand %ebx, %eax\nor src, dest\nDest = src | dest\norl (0x2000), %eax\nxor src, dest\nDest = src ^ dest\nxor $0xffffff, %eax\nshl count, dest\nDest = dest << count\nshl $2, %eax\nshr count, dest\nDest = dest >> count\nshr $4, (%eax)\nsal count, dest\nSame as shl, shifted bits will be the sign\nbit\nConditionals and jumps\ncmp arg1, arg2\nIf arg1 > arg2 sets\nCF=1 (carry flag =1)\nThis compares arg1 and arg2; you can\nuse any conditionals jumps below to act\nupon the result of this comparison\ncmp $0, %eax\ntest reg,imm/reg\nBitwise and of register and\nconstant/register; the next jump command\nuses the result of this; consider this\nessentially as same as compare\ntest %rax, %rcx\nje label\nJump to label if arg2 = arg1\nje endloop\njne label\nJump to label if arg2 != arg1\njne loopstart\njg label / ja label\nJump to label if arg2 > arg1\njg exit / ja exit\njge label\nJump to label if arg2 >= arg1\njge format_disk\njl label\nJump to label if arg2 < arg1\njl error\njle label\nJump to label if arg2 <= arg1\njle finish\njz label\nJump to label if bits were not set\njz looparound\njnz label\nJump to label if bits were set\njnz error\njump label\nUnconditional jump\njmp exit\nMiscellaneous\nnop\nNo-op\nnop\nlea addr, dest\nMove the address calculated to the dest\nlea 23(%eax, %ecx,8),%eax\ncqto\n%rdx:%rax← sign-extend of %rax.\ncqto\nsuffixes b=byte(8), w=word(16), l=long(32), q=quad(64)\nbase indexed scale displacement 172(%rdi, %rdx,8) = %rdi + 8 * %rdx + 172\nNote that not both src and dest can be memory operands at the same time.\nregister - %eax\n\nfixed address - (0x1000)\nconstant - $10\n\ndynamic address - (%rsi)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.172 Performance Engineering of Software Systems, Practice Quiz 4 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/c5d28baace762f3d6fbae10d4ee639fd_MIT6_172F18_practicequiz4answers.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nPractice Quiz 4 ANSWERS\nPractice Quiz 4 ANSWERS\n\n1 True or False (8 parts, 16 points)\nIncorrect answers will be penalized, so do not guess unless you are reasonably sure. You need\nnot justify your answer unless you want to leave open the possibility of receiving partial credit if\nyour answer is wrong.\n1.1\nBecause thermal limits prevented CPU manufacturers from increasing clock frequencies signifi\ncantly, they began to produce multicore microprocessors.\nTrue\nFalse\nAnswer: True\n1.2\nInlining a function tends to improve performance by reducing the number of instruction cache\nmisses.\nTrue\nFalse\nAnswer: False\n1.3\nSuppose that a recursive function performs work on a problem of size N according to the recur\nrence T(N) = 2T(N/3) + Θ(N lg N). Then coarsening the recursion is unlikely to significantly\nimprove performance.\nTrue\nFalse\nAnswer: True\n1.4\nCompilers eliminate data dependencies through register renaming by replacing x86 logical reg\nisters with physical registers.\nTrue\nFalse\nAnswer: False\n\n1.5\nThere can never be a true-, anti-, or output-data dependence between the following two lines of\ncode:\nmovl %eax , 8(%esi)\nlea 24(%esi, %edi, 8) , %ecx\nTrue\nFalse\nAnswer: True\n1.6\nWhen using the MSI (modified, shared, invalid) cache coherence protocol, the state of a cache\nline in a processor's cache may transition directly from shared to invalid.\nTrue\nFalse\nAnswer: True\n1.7\nIf a memory location in a program is read by two logically parallel instructions, then a read-read\ndeterminacy race exists.\nTrue\nFalse\nAnswer: False\n1.8\nA greedy scheduler schedules a computation with work T1 and span Tinf in time Tp ≤ max(T1/P,Tinf)\non a P-processor ideal parallel computer.\nTrue\nFalse\nAnswer: False\n\n2 Back Of The Envelope Calculations (3 Part, 10 Points)\nPerform a back-of-the-envelope calculation for the work of a serial execution of get_row_sums on\na 1000-by-1000 matrix. Assume that you have one 3 GHz scalar processing core which executes\nonly one instruction per cycle, and that everything is in cache.\n\n# define ROWS 1000\n# define COLS 1000\ntypedef int32_t element_t ;\nvoid get_row_sums ( element_t M[ ROWS ][ COLS ],\nelement_t row_sums [ ROWS ]) {\nfor ( int32_t i = 0; i < ROWS ; i ++) {\nelement_t sum = 0;\nfor ( int32_t j = 0; j < COLS ; j ++) {\nsum += M[i ][ j];\n}\nrow_sums [i] = sum ;\n}\n15 }\n\n2.1\nHow long does get_row_sums take assuming that vectorization is disabled? Please circle the letter\nof your answer.\nA\n0.00001-0.001 seconds.\nB\n0.001-0.1 seconds.\nC\n0.1-10 seconds.\nD\n10-1000 seconds.\nE\nNone of the above.\nAnswer: A\n\n2.2\nAfter the code is compiled with vectorization enabled (with 128-bit vector registers), what is the\nperformance of the program compared to the original program? Please circle the letter of your\nanswer.\nA\nMore than twice as slow as the code without vectorization.\nB\nSlower, but less than twice as slow as the code without vectorization.\nC\nAbout the same as the code without vectorization.\nD\nFaster, but less than twice as fast as the code without vectorization.\nE\nMore than twice as fast as the code without vectorization.\nAnswer: E\n2.3\nWith vectorization enabled (with 128-bit vector registers), what is the performance of the program\nif you change element_t from int32_t to int8_t? Please circle the letter of your answer.\nA\nMore than twice as slow as the vectorized code with int32_t's.\nB\nSlower, but less than twice as slow as the vectorized code with int32_t's.\nC\nAbout the same as the vectorized code with int32_t's.\nD\nFaster, but less than twice as fast as the vectorized code with int32_t's.\nE\nMore than twice as fast the vectorized code with int32_t's.\nAnswer: E\n\n3 Branch Prediction (3 parts, 9 points)\nUsing asymptotic Θ-notation, give the expected number of branch misses for the following sort\ning algorithm on an array of N distinct integers on (1) a sorted input (increasing order); (2) a\nreverse sorted input (decreasing order); and (3) a randomly ordered input.\n\nvoid insertion_sort ( int * A ,\nfor ( int j = 0; j < N; j ++)\nint insert = A[j];\nint slot = j;\nwhile ( slot > 0 && insert\nA[ slot ] = A[ slot -1];\nslot -;\n}\nA[ slot ] = insert ;\n}\n}\nint N) {\n{\n< A[ slot -1]) {\n\n3.1 Sorted\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\nAnswer: A\n3.2 Reverse Sorted\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\nAnswer: B\n3.3 Random Order\nA\nB\nC\nD\nE\nΘ(1)\nΘ(N)\nΘ(N lg N)\nΘ(N2)\nNone of the above.\nAnswer: B\n\n4.1\n4 Variable Byte Compression (2 parts, 11 points)\nByte codes are used as a way to compress sequences of positive integers of varying magnitudes.\nEach integer is represented as a series of bytes, where the most significant bit of a byte is called\nthe continuation bit, and the remaining 7 bits are the payload. To encode an integer, we take its\nbinary representation ignoring leading zeros and group the remaining bits in 7-bit payloads. The\nremaining payloads are placed in bytes, with the least significant bits being in the first byte and\nthe most significant bits being in the last byte. The continuation bit in each byte is set to 1 except\nfor the last byte where it is set to 0.\nFor example, to encode the integer 6172, we inspect its binary representation (without the leading\n0's), which is 0b1100000011100. We create two payloads, h0011100i and h0110000i, and place\nthem into bytes with the first byte's continuation bit set to 1 and the second byte's continuation\nbit set to 0. The resulting bytes that encode the integer 6172 are 0b10011100 and 0b00110000,\nwhere the first byte encodes the lower-order bits and the second byte encodes the higher-order\nbits.\nSuppose that the sequence of bytes that resulted from encoding some integer x was 0b11001010,\n0b10001011, and 0b00101100. What is x in hexadecimal notation ignoring the leading zeros?\nA\n0x1645CA\nB\n0x160BCA\nC\n0x2C8BCA\nD\n0xB05CA\nE\nNone of the above.\nAnswer: D\n\n4.2\nThe following program encodes an array In of N positive integers into a sequence of bytes, stored\nin the array Out. Assume that sufficient memory has been allocated for Out.\n\nvoid encode(uint32_t* In, int32_t N, unsigned char* Out) {\nfor(int32_t i=0; i < N; i++) {\nuint32_t x = In[i];\nwhile(x) {\nchar byte =\n(A)\n;\nx =\n(B)\n;\nif(x)\n{\n(C)\n;}\n*Out++ = byte;\n}\n}\n12 }\n\nFor each blank in the code, write its label (A, B, or C) next to the expression that best fits. (Hint:\nSome blanks can take more than one expression, but only one is \"best.\")\nbyte & 0x80\nx << 7\nbyte & 0x8\nx = x << 7\nbyte & 0x80\nx = x >> 1\nbyte &= 0x80\nx >> 8\nbyte ˆ 0x80\nx ˆ 0x7f\nbyte |= 0x80 Answer: C\nx ˆ 0x80\nbyte\nx | 0x7f\nx >> 7 Answer: B\nx | 0x80\nx & 0x7f Answer: A\nx++\nx & 0x7\nx-7\nx & 0x80\nx\n\nC\n5 LLVM (6 parts, 12 points)\nThis question explores your understanding of control-flow graphs and Bentley optimizations.\nConsider the following complete C source file.\n\nint value1 () ;\nint value2 () ;\n__attribute__ (( const ))\nint bar ( bool p) {\nint val ;\nif (p)\nval = value1 ();\nelse\nval = value2 ();\nreturn val ;\n}\nvoid foo ( int * restrict Y,\nconst int * restrict X ,\nint n , bool p) {\nfor ( int i = 0; i < n; ++ i)\nY[i] += bar (p) * X[i ];\n19 }\n\nWhen compiling this C code, LLVM can perform optimizations involving the functions foo and\nbar defined in this file, but not on the functions value1 and value2, which are only declared in\nthis file. Suppose that LLVM compiles the following functions with the specified optimizations:\nA\nThe function bar with no optimization.\nB\nThe function foo with no optimization.\nThe function foo with function inlining and no other optimizations.\nD\nThe function foo with loop unrolling and no other optimizations.\nE\nThe function foo with code hoisting followed by function inlining and no other optimiza\ntions.\nThe next page contains several pictures of control-flow graphs. For each control-flow graph,\ncircle either the unique letter of the function-and-optimization scenario from above that it corre\nsponds to, or circle \"None\" if it does not correspond to any of the scenarios. While not strictly\nnecessary to solve this question, the LLVM IR is provided on Page 11 for your reference.\n\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\nA B C D E None\n\nHere is the LLVM IR for the two functions bar and foo without function inlining, loop un\nrolling, or code hoisting.\n\ndefine i32 @bar(i1 zeroext) #0 {\nbr i1 %0, label %2, label %4\n; <label >:2:\n; preds = %1\n%3 = call i32 (...) @value1 () #3\nbr label %6\n; <label >:4:\n; preds = %1\n%5 = call i32 (...) @value2 () #3\nbr label %6\n; <label >:6:\n; preds = %4, %2\n%.0 = phi i32 [ %3, %2 ], [ %5, %4 ]\nret i32 %.0\n}\ndefine void @foo(i32* noalias , i32* noalias , i32 , i1 zeroext) #2 {\n%5 = icmp sgt i32 %2, 0\nbr i1 %5, label %4, label %. _ret_edge\n21 ; <label >:6:\n; preds = %4, %6\n%.01 = phi i32 [ 0, %4 ], [ %16, %6 ]\n%7 = call i32 @bar(i1 zeroext %3) #4\n%8 = sext i32 %.01 to i64\n%9 = getelementptr inbounds i32 , i32* %1, i64 %8\n%10 = load i32 , i32* %9, align 4\n%11 = mul nsw i32 %7, %10\n%12 = sext i32 %.01 to i64\n%13 = getelementptr inbounds i32 , i32* %0, i64 %12\n%14 = load i32 , i32* %13, align 4\n%15 = add nsw i32 %14, %11\nstore i32 %15, i32* %13, align 4\n%16 = add nsw i32 %.01, 1\n%17 = icmp slt i32 %16, %2\nbr i1 %17, label %6, label %. _ret_edge\n37 ._ret_edge:\n; preds = %6, %4\nret void\n39 }\n\n6 Algorithm Analysis (4 parts, 20 points)\nRecall the prefix sum algorithm from Homework 5, and consider the following alternative parallel\nprefix sum algorithm, where A is the input array with N elements. The array element A[N + 1]\nis also allocated to store the total sum. Assume that N is an exact power of 2.\nNote that you do not need to understand why the code is correct -- just its structure.\n\nvoid upsweep(int64_t* A, int64_t N) {\nfor (int64_t d = 1; d < N; d*=2) {\ncilk_for (int64_t k = 0; k < N; k += 2*d) {\nA[k + 2*d - 1] = A[k + d - 1] + A[k + 2*d - 1];\n}\n}\n}\nvoid downsweep(int64_t* A, int64_t N) {\nA[N] = A[N - 1] //total sum\nA[N - 1] = 0;\nfor (int64_t d = N / 2; d >= 1; d = d / 2) {\ncilk_for(int64_t i = 0; i < N; i += 2*d) {\nint64_t temp = A[i + d - 1];\nA[i + d - 1] = A[i + 2*d - 1];\nA[i + 2*d - 1] = temp + A[i + 2*d - 1];\n}\n}\n19 }\nvoid prefix_sum(int64_t* A, int64_t N) {\nupsweep(A, N);\ndownsweep(A, N);\n24 }\n\n6.1\nWhat is the work of prefix_sum? Give your answer in terms of N using Θ-notation.\nAnswer: Θ(N)\n6.2\nWhat is the span of prefix_sum? Give your answer in terms of N using Θ-notation.\nAnswer: Θ(log2 N)\n\nConsider the following algorithm for computing a histogram on a length-N array A of in\ntegers in the range {0,1,...,k - 1}, where k ≤ N. The algorithm uses a two-dimensional array\nH, whose dimensions are (N/k) + 1 rows by k + 1 columns. Assume for simplicity that N is\ndivisible by k. Here is the algorithm:\n1. Partition the array into N/k length-k subarrays A0, A1,..., AN/k-1.\n2. Initialize each element of the matrix H to 0.\n3. For all r = 0,1,..., N/k - 1, sequentially count the number of occurrences of each integer\nin Ar, and store the number of occurrences of each integer c in H[r][c] (which fills in a row\nof H). Each row is processed sequentially but different rows can be processed in parallel.\n4. For each c = 0,1,...,k - 1, perform a prefix sum on the cth column of H, that is, on the values\nH[r][c] for all 0 ≤ r < N/k. Different columns can be processed in parallel. H[N/k][c] now\nstores the number of integers of value c in A.\nThe code for this algorithm is shown below. Assume that vertical_prefix_sum(H, c, N/k)\ncomputes the prefix sum on the cth column of H with N/k entries in parallel.\n\n// H has dimension (n/k)+1 by k+1\nvoid histogram(int64_t* A, int64_t ** H, int64_t N, int64_t k) {\ncilk_for (int64_t r = 0; r < N/k; r++) {\nfor (int64_t c = 0; c < k; c++) {\nH[r][c] = 0;\n}\nfor (int64_t c = 0; c < k; c++) {\nH[r][A[r * N/k + c]]++;\n}\n}\ncilk_for (int64_t c = 0; c < k; c++) {\nvertical_prefix_sum(H, c, N/k);\n}\n// resulting histogram is stored in H[N/k]\n15 }\n\n6.3\nWhat is the asymptotic work of histogram? Give your answer using Θ-notation in terms of N, k,\nand Wps(N), where Wps(N) is the work of prefix sum on N elements.\nAnswer: Θ(N) + kWps(N/k)\n6.4\nWhat is the asymptotic span of the histogram? Give your answer using Θ-notation in terms of\nN, k, and Sps(N), where Sps(N) is the span of prefix sum on N elements.\nAnswer: Θ(k + lg(N/k)) + Sps(N/k)\n\nIntel x86 Assembly Language Cheat Sheet\nInstruction\nEffect\nExample\nData movement\nmov src, dest\nCopy src to dest\nmov $10,%eax\nArithmetic\nadd src, dest\nDest = dest + src\nadd $10, %esi\nmul reg\nedx:eax = eax * reg (colon means the\nresult spans across two registers)\nmul %esi\ndiv reg\nidiv reg\nedx = edx:eax mod reg\neax = edx:eax / reg\ndiv %edi\ninc dest\nIncrement destination\nInc %eax\ndec dest\nDecrement destination\ndec (%esi)\nsbb arg1, arg2\nIf CF = 1, (this is set by cmp instruction;\nrefer cmp)\narg2 = arg2 - (arg1 + 1)\nelse\narg2 = arg2 - arg1\nsbb %eax, %ebx\nFunction Calls\ncall label\nPush eip, transfer control\ncall _fib\nret\nPop eip and return\nret\npush item\nPush item (constant or register) to stack\npushl $32\npushl %eax\npop [reg]\nPop item from stack; optionally store to\nregister\npop %eax\npopl\nBitwise Operations\nand src,dest\nDest = src & dest\nand %ebx, %eax\nor src, dest\nDest = src | dest\norl (0x2000), %eax\nxor src, dest\nDest = src ^ dest\nxor $0xffffff, %eax\nshl count, dest\nDest = dest << count\nshl $2, %eax\nshr count, dest\nDest = dest >> count\nshr $4, (%eax)\nsal count, dest\nSame as shl, shifted bits will be the sign\nbit\nConditionals and jumps\ncmp arg1, arg2\nIf arg1 > arg2 sets\nCF=1 (carry flag =1)\nThis compares arg1 and arg2; you can\nuse any conditionals jumps below to act\nupon the result of this comparison\ncmp $0, %eax\ntest reg,imm/reg\nBitwise and of register and\nconstant/register; the next jump command\nuses the result of this; consider this\nessentially as same as compare\ntest %rax, %rcx\nje label\nJump to label if arg2 = arg1\nje endloop\njne label\nJump to label if arg2 != arg1\njne loopstart\njg label / ja label\nJump to label if arg2 > arg1\njg exit / ja exit\njge label\nJump to label if arg2 >= arg1\njge format_disk\njl label\nJump to label if arg2 < arg1\njl error\njle label\nJump to label if arg2 <= arg1\njle finish\njz label\nJump to label if bits were not set\njz looparound\njnz label\nJump to label if bits were set\njnz error\njump label\nUnconditional jump\njmp exit\nMiscellaneous\nnop\nNo-op\nnop\nlea addr, dest\nMove the address calculated to the dest\nlea 23(%eax, %ecx,8),%eax\ncqto\n%rdx:%rax← sign-extend of %rax.\ncqto\nsuffixes b=byte(8), w=word(16), l=long(32), q=quad(64)\nbase indexed scale displacement 172(%rdi, %rdx,8) = %rdi + 8 * %rdx + 172\nNote that not both src and dest can be memory operands at the same time.\nregister - %eax\n\nfixed address - (0x1000)\nconstant - $10\n\ndynamic address - (%rsi)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering in Software Systems, Lecture 21: Tuning a TSP Algorithm",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/dd989b76928a0561851e73da6d028763_MIT6_172F18_lec21.pdf",
      "content": "Tuning a TSP Algorithm\nJon Bentley\nBell Labs Research, Retired\nOutline\nReview of Recursive Generation\nThe Traveling Salesperson Problem\nA Sequence of TSP Algorithms\nPrinciples of Algorithm Engineering\n\nRecursive Generation\nA technique for systematically generating all members of a class\nExample: all subsets of the n integers 0, 1, 2, ..., n-1\nRepresentation? {1, 3, 4} or 01011 or ...\nAn Iterative Solution: binary counting\n00000 00001 00010 00011 00100 ... 11111\nA Recursive Solution to Fill p\nvoid allsubsets(int m)\n{ if (m == 0) {\nvisit();\nSm-1\n} else {\nSm =\np[m-1] = 0;\nallsubsets(m-1);\np[m-1] = 1;\nSm-1\nallsubsets(m-1);\n}\n}\nBentley: TSP\n\nThe TSP\nThe Problem\nMr. Lincoln's Eighth Circuit\nScheduling vehicles, drills, plotters\nAutomobile assembly lines\nA Prototypical Problem\nNP-Hard\nHeld-Karp dynamic programming\nApproximation algorithms\nKernighan-Lin heuristics\n(c) GUY C. FRAKER. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/\nBentley: TSP\n\nA Personal History\nShamos's 1978 Thesis\n\"In fact, the tour was obtained by applying the Christofides heuristic several\ntimes and selecting the best result. It is not known to be a shortest tour for the\ngiven set of points.\"\nA 1997 Exercise\nCan simple code now solve a 16-city problem on faster machines?\nA 2016 Talk\nWhat has changed in two more decades?\nBentley: TSP\n\nThis Talk\nAlternative Titles\nA case study in ...\n... implementing algorithms\n... recursive enumeration\n... algorithm engineering\n... applying algorithms and data structures\nA master class in algorithms\nA sampler of performance engineering\nTwo fun days of programming\nNon-Titles\nState-of-the-art TSP algorithms\nExperimental analysis of algorithms\nBentley: TSP\n\nRepresentation Details\nCount of Cities\n#define MAXN 20\nint n;\nPermutation of Cities\nint p[MAXN];\nDistances Between Cities\nTypedef double Dist;\nDist d(int i, int j)\nBentley: TSP\n\nAlgorithm 1\nThe Idea\nRecursively generate all n! permutations\nand choose the best\nImplementation\nvoid search1(int m)\n{ int i;\nif (m == 1)\ncheck1();\nelse\nfor (i = 0; i < m; i++) {\nswap(i, m-1);\nsearch1(m-1);\nswap(i, m-1);\n}\n}\nBentley: TSP\n\nSupporting Code\nvoid check1()\n{ int i;\nDist sum = dist1(p[0], p[n-1]);\nfor (i = 1; i < n; i++)\nsum += dist1(p[i-1], p[i]);\nsave(sum);\n}\nvoid save(Dist sum)\n{ int i;\nIs this code correct?\nif (sum < minsum) {\nminsum = sum;\nfor (i = 0; i < n; i++)\nminp[i] = p[i];\n}\n}\nvoid solve1()\n{ search1(n);\n}\nBentley: TSP\n\nRun Time of Algorithm 1\nAnalysis\nPermutations: n!\nDistance calculations at each: n\nTotal distance calculations: n x n!\nExperiments: Intel Core i7-6500U @ 2.50GHz (Default machine)\nN\nTime\n\n0.05 secs\n0.34 secs\n3.97 secs\n45.47 secs\n9 minutes\n2 hours\nBentley: TSP\n\nConstant Factor Improvements\nExternal to the Program\nCompiler optimizations\nFaster hardware\nInternal Changes\nModify the C code\nBentley: TSP\n\nCompiler Optimizations\nAn Experiment\n0.05 secs\n0.34 secs\n3.97 secs\n0.12 secs\n45.47 secs\n1.62 secs\nN\nNo Opt\n-O3\n\n9 min\n19.81 secs\n2 hours\n4.3 min\ngcc -O3\nFactor of ~25 on this machine\nFactor of ~6 on a Raspberry Pi 3\nOnly full optimization shown from now on\nBentley: TSP\n\nFaster Hardware\nTwo Machines\n1997: Pentium Pro @ 200MHz\n2016: Intel Core i7-6500U @ 2.50GHz\nN\n\n20.9 secs\n0.12 secs\n4 min\n1.62 secs\n48 min\n19.81 secs\n10 hrs\n4.3 min\nSpeedup of about 150\nx12 due to clock speed; x12 due to wider data and deeper pipes\nBentley: TSP\n\nConstant Factor Improvements: Internal\nFaster computation\nChange doubles to floats or (scaled) ints\nMeasure and use fastest size (short, int, long)\nAvoid recomputing math-intensive function\nAlgorithm 1\nDist geomdist(int i, int j) {\nreturn (Dist) (sqrt(sqr(c[i].x-c[j].x) +\nsqr(c[i].y-c[j].y)));\n}\nAlgorithm 2\nPrecompute all n2 distances in a table\n#define dist2(i, j) distarr[i][j]\nSpeedup of about 2.5 or 3\nBentley: TSP\n\nAlgorithm 3\nIdea\nReduce distance calculations by examining fewer permutations\nFix last city in the permutation\nCode\nvoid solve3()\n{ search2(n-1);\n}\nAnalysis\nPermutations: (n-1)!\nDistance calculations at each: n\nTotal distance calculations: n x (n-1)! = n!\nBentley: TSP\n\nAlgorithm 4\nDon't recompute sum; carry along a partial sum instead\nvoid solve4()\n{ search4(n-1, ZERO);\n}\nvoid search4(int m, Dist sum)\n{ int i;\nif (m == 1)\ncheck4(sum + dist2(p[0], p[1]));\nelse\nfor (i = 0; i < m; i++) {\nswap(i, m-1);\nsearch4(m-1, sum + dist2(p[m-1], p[m]));\nswap(i, m-1);\n}\n}\nvoid check4(Dist sum)\n{ sum += dist2(p[0], p[n-1]);\nsave(sum);\n}\nReduces n x (n-1)! to ~(1+e) x (n-1)!\nBentley: TSP\n\nSummary of Four Algorithms\nAlg 1\nAlg 2\nAlg 3\nAlg 4\n\n0.25\nn\nn x n!\nn x n!\nn x (n-1)!\n(1+e) x\n(n-1)!\n0.83\n0.25\n0.02\n10.28\n3.22\n0.28\n33.50\n3.47\n2.59\n34.84\n21.02\nSeconds on an AMD E-450 at 1.65GHz\nAll run times are for initial members of one sequence uniform on the\nunit square\nBentley: TSP\n\nPerspective on Factorial Growth\nEach factor of n allows us to increase the problem size by 1 in about the\nsame wall-clock time\nFast machines, great compilers and code tuning allow us to solve\nproblems into the teens\n14 cities in 5 minutes\n16 cities in 16 hours\nBut can we ever analyze, say, all permutations of a deck of cards?\nBentley: TSP\n\nExponential Growth\nDefinition of \"Growing Exponentially\"\nPopular usage: \"growing real fast, looks like\"\nMathematics: cn for some base c and time period for n\nFactorial Growth\nBy Stirling's approximation,\nln n! = n ln n - n + O(ln n)\nlg n! ~ n lg n - 1.386 n\nSo n! ~ 2 ^ ( n lg n - 1.386 n ) ~ 2 ^ (n lg n) ~ (n / e) ^ n\nFactorial grows faster than any exponential function\nHow Big is 52!?\n52! ~ 8.0658 x 1067 ~\nBentley: TSP\n\n52! in Everyday Terms\nSet a timer to count down 52! = 8.0658 x 1067 nanoseconds.\nStand on the equator, and take one step forward every million years.\nWhen you've circled the earth once, take a drop of water from the\nPacific Ocean, and keep going.\nWhen the Pacific Ocean is empty, lay a sheet of paper down, refill the\nocean and carry on.\nWhen your stack of paper reaches the moon, check the timer. You're\nabout done.\nFact: the universe is about 26! = 4.03 x 1026 nanoseconds old\nMoral: we're going to have to ignore some of the possible tours.\nBentley: TSP\n\nPuzzle Break\nFrom Greg Conti\nFind all permutations of 1..9 such that each initial substring of length m is\ndivisible by m\nFor 1..3, 321 works but not 132\nTwo Main Approaches\nThinking\nComputing\nWhat structures? What language?\nHow to generate all n! permutations of a string?\nRecursive search(left, right)\nStart with left = \"123456789\", right = \"\"; end when left == \"\"\nsearch(\"356\", \"421978\") calls\nsearch(\"56\", \"3421978\"), search(\"36\", \"5421978\"), search(\"35\", \"6421978\")\nBentley: TSP\n\nAn Awk Program\nfunction search(left, right, i) {\nif (left == \"\") {\nfor (i = 1; i <= length(right); i++)\nif (substr(right, 1, i) % i)\nreturn\nprint \" \" right\n} else\nfor (i = 1; i <= length(left); i++)\nsearch(substr(left, 1, i-1) substr(left, i+1), substr(left, i, 1) right)\n}\nAbout 3 seconds to find 381654729\nBEGIN { search(\"123456789\", \"\") }\nBentley: TSP\n\nHow To Make it Faster?\nConstant Factors\nDon't check for divisibility by 1; change language; ...\nPruning the Search - Lessons from 381654729?\nEven digits in even positions\nOdd digits in odd positions\nDigit 5 in position 5\nOld: 9 x 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1 = 9!\n= 362880\nNew: 4 x 4 x 3 x 3 x 1 x 2 x 2 x 1 x 1 = (4!)2 = 576\nCode\ndigit = substr(left, i, 1)\nif (((length(left) % 2) == (digit % 2)) &&\n((length(left) == 5) == (digit == 5)) )\nsearch2(substr(left, 1, i-1) substr(left, i+1), digit right)\nBentley: TSP\n\nLessons from the Break\nFactorial grows very quickly\nWe can never visit the entire search space\nThe key to speed is pruning the search\nSome fancy algorithms can be implemented in very little code\nBentley: TSP\n\nAlgorithm 5\nDon't keep doing what doesn't work; sum never decreases\nCode\nvoid search5(int m, Dist sum)\n{ int i;\nif (sum > minsum)\nreturn;\nif (m == 1) {\n...\nExperiments on an Intel Core i7-3630QM @ 2.40GHz\nAlg 4\n0.59\n5.14\n54.44\n\nAlg 5\n0.01\n0.12\n0.41\n0.99\n3.92\n39.00\nBentley: TSP\n\nAlgorithm 6\nA Better Lower Bound: Add MST of remaining points\nCode\nvoid search6(int m, Dist sum, Mask mask)\n{ int i;\nif (sum + mstdist(mask | bit[p[m]]) > minsum)\nreturn;\n...\nsearch6(m-1,\nsum + dist2(p[m-1], p[m]),\nmask & ~bit[p[m-1]]);\nBentley: TSP\n\nPrim-Dijkstra MST Code\nDist mstdist(Mask mask)\n{ int i, m, mini, newcity, n;\nDist mindist, thisdist, totaldist;\ntotaldist = ZERO;\nn = 0;\nfor (i = 0; i < MAXN; i++)\nif (mask & bit[i])\nq[n++].city = i;\nnewcity = q[n-1].city;\nfor (i = 0; i < n-1; i++)\nq[i].nndist = INF;\nfor (m = n-1; m > 0; m--) {\nmindist = INF;\nfor (i = 0; i < m; i++) {\nthisdist = dist2(q[i].city, newcity);\nif (thisdist < q[i].nndist) {\nq[i].nndist = thisdist;\nq[i].nnfrag = newcity;\n}\nif (q[i].nndist < mindist) {\nmindist = q[i].nndist;\nmini = i;\n}\n}\nnewcity = q[mini].city;\ntotaldist += mindist;\nq[mini] = q[m-1];\n}\nreturn totaldist;\n}\nBentley: TSP\n\nMore Experiments\nAlgorithms 5 and 6\nN\nAlg 5\n0.95\n4.03\n40.00\nAlg 6\n0.00\n0.02\n0.03\n0.05\n0.06\n0.22\n0.20\n2.39\nAlgorithm 6\nN\n\nAlg 6\n2.39\n0.75\n1.81\n5.56\n8.97\n13.36\n7.67\n13.73\nBentley: TSP\n\nAlgorithms 7 and 8\nCache MST distances rather than recomputing them\nAlgorithm 7: Store all (used) distances in a table of size 2n\nnmask = mask | bit[p[m]];\nif (mstdistarr[nmask] < 0.0)\nmstdistarr[nmask] = mstdist(nmask);\nif (sum + mstdistarr[nmask] > minsum)\nreturn;\nAlgorithm 8: Store them in a hash table\nif (sum + mstdistlookup(mask | bit[p[m]]) > minsum)\nreturn;\nBentley: TSP\n\nHash Table Implementation\nDist mstdistlookup(Mask mask)\n{ Tptr p;\nint h;\nh = mask % MAXBIN;\nfor (p = bin[h]; p != NULL; p = p->next)\nif (p->arg == mask)\nreturn p->val;\np = (Tptr) malloc(sizeof(Tnode));\np->arg = mask;\np->val = mstdist(mask);\np->next = bin[h];\nbin[h] = p;\nreturn p->val;\n}\nBentley: TSP\n\nExperiments\nAlgorithms 6 and 8\nN\n\nAlg 6\n13.44\n12.69\n40.39\nAlg 8\n0.50\n0.45\n1.41\n1.88\n5.33\n2.31\n2.44\n48.52\n7.59\nBentley: TSP\n\nAlgorithm 9\nSort edges to visit nearest city first, then others in order\nfor (i = 0; i < m; i++)\nunvis[i] = i;\nfor (top = m; top > 0; top--) {\nmindist = INF;\nfor (j = 0; j < top; j++) {\nthisdist = dist2(p[unvis[j]], p[m]);\nif (thisdist < mindist) {\nmindist = thisdist;\nminj = j;\n}\n}\nswap(unvis[minj], m-1);\nsearch9(m-1,\nsum + dist2(p[m-1], p[m]),\nmask & ~bit[p[m-1]]);\nswap(unvis[minj], m-1);\nunvis[minj] = unvis[top-1];\n}\nBentley: TSP\n\nExperiments\nAlgorithms 8 and 9\nN\n\nAlg 8\n8.27\n11.08\n25.84\n93.61\n55.20\n41.95\nAlg 9\n4.41\n0.86\n3.44\n10.80\n12.69\n10.39\n21.19\nIn 1997, stopped at n=30 and 305.66 seconds\nBentley: TSP\n\nA 45-City Tour\n42 seconds on a 2.50GHz Core i7\nBentley: TSP\n\nHow Far Can Algorithm 9 Go?\nN\nSeconds\n3.59\n11.33\n12.17\n11.62\n23.49\n41.89\n183.70\n1036.28\n1199.11\n7815.83\n2172.03\n6537.12\n11254.17\nMy Thanksgiving 2016 Cyclefest\nWhen to think and when to run programs?\n= 3hrs 7 min\nBentley: TSP\n\nComplete Code: 160 Lines of C\n/* tspfastonly.c -- Cut tsp.c down to only final algorithm 9 */\n#include <stdio.h>\n#include <time.h>\n#include <math.h>\n#include <stdlib.h>\n/* Globals: points and perm vectors; edge operations */\n#define MASKTYPE long\n#define MAXN 60\n#define MAXBIN 9999991\ntypedef double Dist;\nint n = 0;\ntypedef struct point {\nfloat x;\nfloat y;\n} Point;\nPoint c[MAXN];\nint p[MAXN], minp[MAXN];\nDist minsum;\nDist distarr[MAXN][MAXN];\nfloat sqr(float x) { return x*x; }\nDist geomdist(int i, int j)\n{ return (Dist) (sqrt(sqr(c[i].x-c[j].x)\n+ sqr(c[i].y-c[j].y)));\n}\n#define dist2(i, j) (distarr[i][j])\n// Search algs\nvoid swap(int i, int j)\n{ int t = p[i]; p[i] = p[j]; p[j] = t; }\nvoid check4(Dist sum)\n{ sum += dist2(p[0], p[n-1]);\nint i;\nif (sum < minsum) {\nminsum = sum;\nfor (i = 0; i < n; i++)\nminp[i] = p[i];\n}\n}\ntypedef MASKTYPE Mask;\nMask bit[MAXN];\nstruct link {\nint city;\nint nnfrag;\nDist nndist;\n} q[MAXN];\nDist mstdist(Mask mask)\n{ int i, m, mini, newcity, n;\nDist mindist, thisdist, totaldist;\nn = 0;\nfor (i = 0; i < MAXN; i++)\nif (mask & bit[i])\nq[n++].city = i;\nnewcity = q[n-1].city;\nfor (i = 0; i < n-1; i++)\nq[i].nndist = 1e35;\nMST\nfor (m = n-1; m > 0; m--) {\nmindist = 1e35;\nfor (i = 0; i < m; i++) {\nthisdist = dist2(q[i].city, newcity);\nif (thisdist < q[i].nndist) {\nq[i].nndist = thisdist;\nq[i].nnfrag = newcity;\n}\nif (q[i].nndist < mindist) {\nmindist = q[i].nndist;\nmini = i;\n}\n}\nnewcity = q[mini].city;\ntotaldist += mindist;\nq[mini] = q[m-1];\n}\nreturn totaldist;\n}\ntypedef struct tnode *Tptr;\ntypedef struct tnode {\nMask arg;\nDist val;\nTptr next;\n} Tnode;\nTptr bin[MAXBIN];\nDist mstdistlookup(Mask mask)\n{\nTptr p;\nHash\nint h;\nh = mask % MAXBIN;\nfor (p = bin[h]; p != NULL; p = p->next)\nif (p->arg == mask)\nreturn p->val;\np = (Tptr) malloc(sizeof(Tnode));\np->arg = mask;\np->val = mstdist(mask);\np->next = bin[h];\nbin[h] = p;\nreturn p->val;\n}\nvoid search9(int m, Dist sum, Mask mask)\n{ int i;\nif (sum + mstdistlookup(mask | bit[p[m]]) > minsum)\nreturn;\nif (m == 1) {\ncheck4(sum + dist2(p[0], p[1]));\n} else {\nint j, minj, unvis[MAXN], top;\nDist mindist, thisdist;\nSort\nfor (i = 0; i < m; i++)\nunvis[i] = i;\nfor (top = m; top > 0; top--) {\nmindist = 1e35;\nfor (j = 0; j < top; j++) {\nthisdist = dist2(p[unvis[j]], p[m]);\nif (thisdist < mindist) {\nmindist = thisdist;\nminj = j;\n}\n}\nswap(unvis[minj], m-1);\nsearch9(m-1,\nsum + dist2(p[m-1], p[m]), mask & ~bit[p[m-1]]);\nswap(unvis[minj], m-1);\nunvis[minj] = unvis[top-1];\n}\n}\n}\nvoid solve9()\n{ int i, j;\nMask mask;\nfor (i = 0; i < n; i++)\nfor (j = 0; j < n; j++)\ndistarr[i][j] = geomdist(i, j);\nfor (i = 0; i < MAXN; i++)\nbit[i] = (Mask) 1 << i;\nfor (i = 0; i < MAXBIN; i++)\nbin[i] = NULL;\nmask = 0;\n// mask is a bit vector\nfor (i = 0; i < n; i++) {\nmask |= bit[i];\np[i] = i;\n}minsum = 1e35;\nsearch9(n-1, 0.0, mask);\n}\nvoid main()\n{ int i;\nFILE *fp;\nfp = fopen(\"rand60.txt\", \"r\");\nwhile (fscanf(fp, \"%f %f\", &c[n].x, &c[n].y) != EOF)\nn++;\nn = 25;\nsolve9();\nfor (i = 0; i < n; i++)\nprintf(\"%3d\\t%10.8f\\t%10.8f\\n\", minp[i],\nc[minp[i]].x, c[minp[i]].y);\n}\nBentley: TSP\n\nEach Algorithm in under a Minute\n1. Simple\n2. Store precomputed distances\n3. Fix starting city\n4. Accumulate distance\n5. Prune search\n6. Add MST of remaining cities\n7. Store MST distances\n8. Store MST distances in hash table\n9. Visit cities sorted by distance\nN = 11 20 secs\n-\n-\nBentley: TSP\n\nPossible Additional Improvements\nConstant Factor Speedups\nFaster machines\nCode tuning as before\nBetter hashing: larger table, remove malloc\nBetter Pruning\nBetter starting tour\nBetter bounds: MST Length + Nearest Neighbor to each end\nEarlier pruning tests\nBetter Sorting\nTune insertion sort; better algorithms\nPrecompute all sorts\nSort once for each city\nSelect subsequence using mask\nBentley: TSP\n\nComponents\nIncremental Software Development\nTotal of 580 lines of C\nAlgorithmic Techniques\nRecursive permutation generation\nStore precomputed results: distances, MST lengths\nPartial sums\nEarly cutoffs\nAlgorithms and Data Structures\nVectors, strings\nSets: Arrays and bit vectors\nMinimum Spanning Trees (MSTs)\nHash tables\nInsertion sort\nBentley: TSP\n\nCode Tuning Rules\nSpace-for-Time 2: Store precomputed results\nInterpoint distances in a matrix; table of MST lengths\nSpace-for-Time 4: Lazy evaluation\nCompute all n2 distances but only compute MSTs as needed\nLogic Rule 2: Short-circuiting monotone functions\nPrune search when length exceeds best so far\nLogic Rule 3: Reordering tests\nSort cities to visit closest first\nExpression Rule 5: Exploit word parallelism\nBit mask represents a set of cities\nFrom Writing Efficient Programs, 1982\nBentley: TSP\n\nPerformance Tools Behind the Scenes\nDriver to make experiments easy\nVariety of input data: real, uniform, annular, random symmetric matrices, ...\nCount critical operations: distances, MSTs, ...\nSoftware profilers\nCost models\nSpreadsheet as a \"lab notebook\"\nGraphs of performance\nCurve fitting\nBentley: TSP\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 1: Introduction and Matrix Multiplication",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/d0c73dd51c79b95196a2e6faa824e1b4_MIT6_172F18_lec1.pdf",
      "content": "6.172\nPerformance\nEngineering\nof Software\nSystems\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLECTURE 1\nIntroduction &\nMatrix Multiplication\nCharles E. Leiserson\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nWHY PERFORMANCE\nENGINEERING?\n\nSoftware Properties\nWhat software properties are more important\nthan performance?\n∙ Compatibility\n∙ Functionality\n∙ Reliability\n∙ Correctness\n∙ Maintainability\n∙ Robustness\n∙ Clarity\n∙ Debuggability\n∙ Modularity\n∙ Portability\n∙ Testability\n∙ Usability\n... and more.\nIf programmers are\nPerformance is the\nwilling to sacrifice\ncurrency of computing.\nperformance for these\nYou can often \"buy\"\nproperties, why study\nneeded properties with\nperformance?\nperformance.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nComputer Programming in the Early Days\nSoftware performance engineering was common,\nbecause machine resources were limited.\nApple II\nIBM System/360\nDEC PDP-11\nCourtesy of alihodza on Flickr.\nCourtesy of jonrb on Flickr.\nCourtesy of mwichary on Flickr.\nUsed under CC-BY-NC.\nUsed under CC-BY.\nUsed under CC-BY-NC.\nLaunched:\nLaunched:\nLaunched:\nClock rate:\n1.25 MHz\nClock rate: 33 KHz\nClock rate:\n1 MHz\nData path:\n16 bits\nData path:\n32 bits\nData path:\n8 bits\nMemory:\n56 Kbytes\nMemory:\n524 Kbytes\nMemory:\n48 Kbytes\nCost:\n$20,000\nCost:\n$5,000/month\nCost:\n$1,395\nMany programs strained the machine's resources.\n∙ Programs had to be planned around the machine.\n∙ Many programs would not \"fit\" without intense\nperformance engineering.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLessons Learned from the 70's and 80's\nPremature optimization is\nthe root of all evil. [K79]\nDonald Knuth\nWilliam Wulf\nMichael Jackson\nMore computing sins are committed in the\nname of efficiency (without necessarily\nachieving it) than for any other single\nreason -- including blind stupidity. [W79]\nThe First Rule of Program\nOptimization: Don't do it.\nThe Second Rule of Program\nOptimization -- For experts\nonly: Don't do it yet. [J88]\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTechnology Scaling Until 2004\n1,000,000\n100,000\n10,000\n1,000\n\"Moore's Law\"\nNormalized\ntransistor count\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015\nYear\nProcessor data from Stanford's CPU DB [DKM12].\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTechnology Scaling Until 2004\n1,000,000\n100,000\n10,000\n1,000\nClock speed (MHz)\nNormalized\ntransistor count\n\"Dennard scaling\"\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015\nYear\nProcessor data from Stanford's CPU DB [DKM12].\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAdvances in Hardware\nApple computers with similar prices from 1977 to 2004\nCourtesy of mwichary on Flickr.\nUsed under CC-BY.\nApple II\nLaunched:\nClock rate:\n1 MHz\nData path:\n8 bits\nMemory:\n48 KB\nCost:\n$1,395\nCourtesy of compudemano on\nFlickr. Used under CC-BY.\nPower Macintosh G4\nLaunched:\nClock rate:\n400 MHz\nData path:\nMemory:\nCost:\n32 bits\n64 MB\n$1,599\nCourtesy of Bernie Kohl on Wikipedia.\nUsed under CC0.\nPower Macintosh G5\nLaunched:\nClock rate:\n1.8 GHz\nData path:\nMemory:\nCost:\n64 bits\n256 MB\n$1,499\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nUntil 2004\nMoore's Law and the scaling of clock frequency\n= printing press for the currency of performance.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTechnology Scaling After 2004\n1,000,000\n100,000\n10,000\nNormalized\ntransistor count\n1,000\nClock speed (MHz)\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015\nYear\nProcessor data from Stanford's CPU DB [DKM12].\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPower Density\nSource: Patrick Gelsinger, Intel Developer's Forum, Intel Corporation, 2004.\nThe growth of power density, as seen in 2004, if\nthe scaling of clock frequency had continued its\ntrend of 25%-30% increase per year.\n(c) Paul Gelsinger at Intel Corporation. All rights reserved. This content is excluded from our\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCreative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nVendor Solution: Multicore\nIntel Core i7 3960X\n(Sandy Bridge E), 2011\n! 6 cores\n! 3.3 GHz\n! 15-MB L3 cache\n! To scale performance, processor manufacturers put\nmany processing cores on the microprocessor chip.\n! Each generation of Moore's Law potentially doubles\nthe number of cores.\n(c) Intel. All rights reserved. This content is excluded from our Creative Commons license.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nTechnology Scaling\n1,000,000\n100,000\n10,000\nNormalized\ntransistor count\n1,000\nClock speed (MHz)\nProcessor cores\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015\nYear\nProcessor data from Stanford's CPU DB [DKM12].\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Is No Longer Free\n2011 Intel\nSkylake\nprocessor\nNVIDIA\nGT200\nGPU\nMoore's Law continues to\nincrease computer\nperformance.\nBut now that performance\nlooks like big multicore\nprocessors with complex\ncache hierarchies, wide\nvector units, GPU's,\nFPGA's, etc.\nGenerally, software must\nbe adapted to utilize this\nhardware efficiently!\n(c) Intel. All rights reserved. This content is excluded from our Creative Commons license.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nSoftware Bugs Mentioning \"Performance\"\nBug reports for Mozilla \"Core\"\nCommit messages for MySQL\n1.40%\n1.60%\n1.40%\n1.20%\n1.20%\n1.00%\n1.00%\n0.80%\n0.80%\n0.60%\n0.60%\n0.40%\n0.40%\n0.20%\n0.20%\n0.00%\n0.00%\nCommit messages for OpenSSL\nBug reports for the Eclipse IDE\n3.00%\n4.50%\n4.00%\n2.50%\n3.50%\n2.00%\n3.00%\n2.50%\n1.50%\n2.00%\n1.00%\n1.50%\n1.00%\n0.50%\n0.50%\n0.00%\n0.00%\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSoftware Developer Jobs\nMentioning \"performance\"\nMentioning \"optimization\"\n30.00%\n7.00%\n6.00%\n25.00%\n5.00%\n20.00%\n4.00%\n15.00%\n3.00%\n10.00%\n2.00%\n5.00%\n1.00%\n0.00%\n0.00%\n2001 2003 2005 2007 2009 2011 2013\nMentioning \"parallel\"\nMentioning \"concurrency\"\n2.50%\n0.40%\n0.30%\n0.70%\n0.60%\n2.00%\n0.50%\n1.50%\n1.00%\n0.20%\n0.50%\n0.10%\n0.00%\n0.00%\nSource: Monster.com\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Engineering Is Still Hard\nA modern multicore\ndesktop processor contains\nparallel-processing cores,\nvector units, caches,\nprefetchers, GPU's,\nhyperthreading, dynamic\nfrequency scaling, etc., etc.\nHow can we write software\nto utilize modern hardware\nefficiently?\n2017 Intel 7th-generation\ndesktop processor\n(c) Intel. All rights reserved. This content is excluded from our Creative Commons license.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCASE STUDY\nMATRIX MULTIPLICATION\n\nSquare-Matrix Multiplication\nc11 c12 ! c1n\na11 a12 ! a1n\nb11 b12 ! b1n\nc21 c22 ! c2n\na21 a22 ! a2n\n$\nb21 b22 ! b2n\n=\n\"\n\" #\n\"\n\"\n\" #\n\"\n\"\n\" #\n\"\ncn1 cn2 ! cnn\nan1 an2 ! ann\nbn1 bn2 ! bnn\nC\nA\nB\ncij = !\nk = 1\nn\naik bkj\nAssume for simplicity that n = 2k.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAWS c4.8xlarge Machine Specs\nFeature\nSpecification\nMicroarchitecture\nHaswell (Intel Xeon E5-2666 v3)\nClock frequency\n2.9 GHz\nProcessor chips\nProcessing cores\n9 per processor chip\nHyperthreading\n2 way\n8 double-precision operations, including\nFloating-point unit\nfused-multiply-add, per core per cycle\nCache-line size\n64 B\nL1-icache\n32 KB private 8-way set associative\nL1-dcache\n32 KB private 8-way set associative\nL2-cache\n256 KB private 8-way set associative\nL3-cache (LLC)\n25 MB shared 20-way set associative\nDRAM\n60 GB\nPeak = (2.9 ! 109) ! 2 ! 9 ! 16 = 836 GFLOPS\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVersion 1: Nested Loops in Python\n!\"#$%& '(') %*+,$\"\n-%$\" &!\". !\"#$%& /\n+ 0 1234\n5 0 66%*+,$\"7%*+,$\"89\n-$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\n@ 0 66%*+,$\"7%*+,$\"89\n-$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\nA 0 662 -$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\n'&*%& 0 &!\".89\n-$% ! !+ ;%*+<.8+9B\n-$% C !+ ;%*+<.8+9B\n-$% D !+ ;%*+<.8+9B\nA6!=6C= E0 56!=6D= / @6D=6C=\n.+, 0 &!\".89\n#%!+& FG274-F G 8.+, H '&*%&9\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRunning time\n= 21042 seconds\n! 6 hours\nIs this fast?\nShould we expect\nmore from our\nmachine?\n\n;%*+<.8+9=\n66%*+,$\"7%*+,$\"89\n-$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\n662 -$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\n'&*%& 0 &!\".89\n! !+ ;%*+<.8+9B\n-$% C !+ ;%*+<.8+9B\n-$% D !+ ;%*+<.8+9B\nA6!=6C= E0 56!=6D= / @6D=6C=\n0 &!\".89\n< 8 9=\n66%*+,$\"7%*+,$\"89\n-$% %$: !+ ;%*+<.8+9=\n-$% >$? !+ ;%*+<.8+9=\n662 -$% %$: !+ ;%*+<.8+9=\n-$%\n-$% >$?\n>$? !+\n!+ ;%*+<.\n;%*+<.8+9=\n8+9=\n& 0 &!\".89\n! !+ ;%*+<.8+9B\n-$% C !+ ;%*+<.8+9B\n-$% D !+ ;%*+<.8+9B\nA6!=6C= E0 56!=6D= / @6D=6C=\n0 &!\".89\nVersion 1: Nested Loops in Python\n!\"#$%& '(') %*+,$\"\nRunning time\n-%$\" &!\". !\"#$%& /\n= 21042 seconds\n+ 0 1234\n! 6 hours\n5 0 66%*+,$\"7%*+,$\"89\nIs this fast?\n-$% %$: !+ ;%*+<.8+9=\n-$% >$? !+\n@ 0\nBack-of-the-envelope calculation\nA 0\n2n3 = 2(212)3 = 237 floating-point operations\nRunning time = 21042 seconds\n\" Python gets 237/21042 ! 6.25 MFLOPS\n-$%\nPeak ! 836 GFLOPS\nPython gets ! 0.00075% of peak\n.+,\n#%!+& FG274-F G 8.+, H '&*%&9\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"#$%& '()(*+&!,*-(./$\"0\n#+1,!2 2,(33 \"\"4'()( 5\n3&(&!2 !.& . 6 789:0\n3&(&!2 /$+1,;<=<= > 6 .;? /$+1,;<.=<.=0\n3&(&!2 /$+1,;<=<= @ 6 .;? /$+1,;<.=<.=0\n3&(&!2 /$+1,;<=<= A 6 .;? /$+1,;<.=<.=0\n#+1,!2 3&(&!2 )$!/ \"(!.BC&%!.D<= (%D3E 5\n-(./$\" % 6 .;? -(./$\"BE0\nF$% B!.& !680 !G.0 !HHE 5\nF$% B!.& '680 'G.0 'HHE 5\n><!=<'= 6 %*.;I&J$+1,;BE0\n@<!=<'= 6 %*.;I&J$+1,;BE0\nA<!=<'= 6 80\nK\nK\n,$.D 3&(%& 6 CL3&;\"*.(.$M!\";BE0\n!\"#$%& '()(*+&!,*-(./$\"0\n#+1,!2 2,(33 \"\"4'()( 5\n3&(&!2 !.& . 6 789:0\n3&(&!2 /$+1,;<=<= > 6 .;? /$+1,;<.=<.=0\n3&(&!2 /$+1,;<=<= @ 6 .;? /$+1,;<.=<.=0\n3&(&!2 /$+1,;<=<= A 6 .;? /$+1,;<.=<.=0\n#+1,!2 3&(&!2 )$!/ \"(!.BC&%!.D<= (%D3E 5\n-(./$\"\n-(./$\" % 6 .;?\n.;? -(./$\"\n-(./$\"BE0\nBE0\nF$% B!.& !680 !G.0 !HHE 5\nF$% B!.& '680 'G.0 'HHE 5\n><!=<'= 6 %*.;I&J$+1,;BE0\n@<!=<'= 6 %*.;I&J$+1,;BE0\nA<!=<'= 6 80\nK\nK\n,$.D\n,\n3&(%&\n&\n& 6 CL3&;\"\nC\n&\n*.(.$M!\";\nM!\nBE0\nBE\nVersion 2: Java\nRunning time = 2,738 seconds\n! 46 minutes\n... about 8.8\" faster than Python.\nF$% B!.& !680 !G.0 !HHE 5\nF$% B!.& '680 'G.0 'HHE 5\nF$% B!.& !680 !G.0 !HHE 5\nF$% B!.& N680 NG.0 NHHE 5\nF$% B!.& '680 'G.0 'HHE 5\nF$% B!.& N680 NG.0 NHHE 5\nA<!=<'= H6 ><!=<N= O @<N=<'=0\nA<!=<'= H6 ><!=<N= O @<N=<'=0\nK\nK\nK\nK\nK\n,$.D 3&$# 6 CL3&;\"*.(.$M!\";BE0\n/$+1,; &/!FF 6 B3&$# P 3&(%&E O Q;P90\nCL3&;\"*$+&*#%!.&,.B&/!FFE0\nK\nK\n,$.D 3&$# 6 CL3&;\"*.(.$M!\";BE0\n/$+1,; &/!FF 6 B3&$# P 3&(%&E O Q;P90\nCL3&;\"*$+&*#%!.&,.B&/!FFE0\nK\nK\nK\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"#$%&'( )*+'%\",-./\n!\"#$%&'( )*+'\"0-./\n!\"#$%&'( )*1*2+\"3(-./\n!'(4\"#( # 5678\n'0&,%( 9:#;:#;<\n'0&,%( =:#;:#;<\n'0&,%( >:#;:#;<\n4%0?+ +'\"44@*+A&$+ +\"3(B?% C*+?A+D\n*+A&$+ +\"3(B?% C(#'E F\nA(+&A# @(#'G/+BH*($ *+?A+G/+BH*($E I\nJ(G8C@(#'G/+BH&*($ *+?A+G/+BH&*($E<\nK\n\"#+ 3?\"#@\"#+ ?AL$D $0#*+ $.?A C?ALB:;E F\n40A @\"#+ \"\n6< \" ) #< II\"E F\n40A @\"#+ N\n6< N ) #< IINE F\n9:\";:N;\n@'0&,%(EA?#'@E 2 @'0&,%(EO9PQHR9S<\n=:\";:N;\n@'0&,%(EA?#'@E 2 @'0&,%(EO9PQHR9S<\n>:\";:N;\n6<\nK\nK\n*+A&$+ +\"3(B?% *+?A+D (#'<\nL(++\"3(04'?1@T*+?A+D PUVVE<\n!\"#$%&'( )*+'%\",-./\n!\"#$%&'( )*+'\"0-./\n!\"#$%&'( )*1*2+\"3(-./\n!'(4\"#( # 5678\n'0&,%( 9:#;:#;<\n'0&,%( =:#;:#;<\n'0&,%( >:#;:#;<\n4%0?+ +'\"44@*+A&$+ +\"3(B?% C*+?A+D\n*+A&$+ +\"3(B?% C(#'E F\nA(+&A# @(#'G/+BH*($ *+?A+G/+BH*($E I\nJ(G8C@(#'G/+BH&*($ *+?A+G/+BH&*($E<\nK\n\"#+ 3?\"#@\"#+ ?AL$D $0#*+ $.?A C?ALB:;E F\n40A @\"#+ \"\n6< \" ) #< II\"E F\n40A @\"#+ N\n6< N ) #< IINE F\n9:\";:N;\n@'0&,%(EA?#'@E 2 @'0&,%(EO9PQHR9S<\n=:\";:N;\n@'0&,%(EA?#'@E 2 @'0&,%(EO9PQHR9S<\n>:\";:N;\n6<\nK\nK\n*+A&$+ +\"3(B?% *+?A+D (#'<\nL(++\"3(04'?1@T*+?A+D PUVVE<\nVersion 3: C\nG\nG\nM\nM\nM\nM\nM\nUsing the Clang/LLVM 5.0\ncompiler\nRunning time = 1,156 seconds\n! 19 minutes,\nor about 2\" faster than Java and\nabout 18\" faster than Python.\n40A @\"#+ \" M 6< \" ) #< II\"E F\n40A @\"#+ N M 6< N ) #< IINE F\n40A @\"#+ W M 6< W ) #< IIWE F\n>:\";:N; IM 9:\";:W; C =:W;:N;<\nK\nK\nK\nL(++\"3(04'?1@T(#'D PUVVE<\nXA\"#+4@YZ6-84[#YD +'\"44@T*+?A+D T(#'EE<\nA(+&A# 6<\nK\nL(++\"3(04'?1@T(#'D PUVV\nPUVVE<\nXA\"#+4@YZ6-84[#YD +'\"44@T*+?A+D T(#'EE<\nA(+&A# 6<\nK\n40A @\"#+ \" M 6< \" ) #< II\"E F\n40A @\"#+ N M 6< N ) #< IINE F\n40A @\"#+ W M 6< W ) #< IIWE F\n>:\";:N; IM 9:\";:W; C =:W;:N;<\nK\nK\nK\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWhere We Stand So Far\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\n\nPython\n21041.67\n1.00\n0.007\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.119\n0.014\nWhy is Python so slow and C so fast?\n∙Python is interpreted.\n∙C is compiled directly to machine code.\n∙Java is compiled to byte-code, which is then\ninterpreted and just-in-time (JIT) compiled\nto machine code.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nInterpreters are versatile, but slow\n! The interpreter reads, interprets, and performs each\nprogram statement and updates the machine state.\n! Interpreters can easily support high-level\nprogramming features -- such as dynamic code\nalteration -- at the cost of performance.\nInterpreter\nloop\nUpdate\nPerform\nstate\nstatement\nRead next\nstatement\nInterpret\nstatement\nInterpreter\nloop\nte\nPe\ne\nstat\next\nstat\nInt\nent\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nJIT Compilation\n∙JIT compilers can recover some of the performance\nlost by interpretation.\n∙When code is first executed, it is interpreted.\n∙The runtime system keeps track of how often the\nvarious pieces of code are executed.\n∙Whenever some piece of code executes sufficiently\nfrequently, it gets compiled to machine code in real\ntime.\n∙Future executions of that code use the more-\nefficient compiled version.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLoop Order\nWe can change the order of the loops in this program\nwithout affecting its correctness.\n!\"#$\"%$ &' (\"#$\")$ * +\")$\"%$,\n-\n-\n-\n./0 1#23 # ' 4, # 5 2, &&#6 7\n./0 1#23 % ' 4, % 5 2, &&%6 7\n./0 1#23 ) ' 4, ) 5 2, &&)6 7\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLoop Order\nWe can change the order of the loops in this program\nwithout affecting its correctness.\n!\"#$\"%$ &' (\"#$\")$ * +\")$\"%$,\n-\n-\n-\n./0 1#23 # ' 4, # 5 2, &&#6 7\n./0 1#23 % ' 4, % 5 2, &&%6 7\n./0 1#23 ) ' 4, ) 5 2, &&)6 7\nDoes the order of loops matter for performance?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance of Different Orders\ni, j, k\nLoop order\n(outer to inner)\nRunning\ntime (s)\n1155.77\nj, i, k\ni, k, j\n177.68\n1080.61\nk, i, j\nj, k, i\n3056.63\n179.21\nk, j, i\n3032.82\nLoop order affects\nrunning time by a\nfactor of 18!\nWhat's going on!?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHardware Caches\nEach processor reads and writes main memory\nin contiguous blocks, called cache lines.\n! Previously accessed cache lines are stored in a\nsmaller memory, called a cache, that sits near the\nprocessor.\n! Cache hits -- accesses to data in cache -- are fast.\n! Cache misses -- accesses to data not in cache --\nare slow.\nmemory\nP\ncache\nB\nM/B\ncache lines\nprocessor\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMemory Layout of Matrices\nIn this matrix-multiplication code, matrices are\nlaid out in memory in row-major order.\nMatrix\nRow 1\nRow 3\nRow 2\nRow 4\nWhat does this layout imply\nabout the performance of\nRow 5\nRow 6\ndifferent loop orders?\nRow 7\nRow 8\nRow 1\nRow 2\nRow 3\nMemory\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAccess Pattern for Order i, j, k\nC\n=\n!\"# $%&' % ( )* % + &* ,,%-\n!\"# $%&' . ( )* . + &* ,,.-\n!\"# $%&' / ( )* / + &* ,,/-\n01%21.2 ,( 31%21/2 4 51/21.2*\nGood spatial locality\nPoor spatial locality\n!\"# $%&' % ( )* % + &* ,,%-\n!\"# $%&' . ( )* . + &* ,,.-\nRunning time:\n1155.77s\nIn-memory layout\nExcellent spatial locality\nA\nx\nB\n4096 elements apart\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAccess Pattern for Order i, k, j\nC\n!\"# $%&' % ( )* % + &* ,,%-\n!\"# $%&' . ( )* . + &* ,,.-\n!\"# $%&' / ( )* / + &* ,,/-\n01%21/2 ,( 31%21.2 4 51.21/2*\n!\"# $%&' % ( )* % + &* ,,%-\n!\"# $%&' . ( )* . + &* ,,.-\nRunning time:\n177.68s\nIn-memory layout\n=\nx\nB\nA\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAccess Pattern for Order j, k, i\n!\"# $%&' ( ) *+ ( , &+ --(.\n!\"# $%&' / ) *+ / , &+ --/.\n!\"# $%&' % ) *+ % , &+ --%.\n01%21(2 -) 31%21/2 4 51/21(2+\n!\"# $%&' ( ) *+ ( , &+ --(.\n!\"# $%&' / ) *+ / , &+ --/.\nRunning time:\n3056.63s\nIn-memory layout\nC\n=\nA\nx\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance of Different Orders\nWe can measure the effect of different access patterns\nusing the Cachegrind cache simulator:\n! \"#$%&'() **+,,$-.#./0%&'() 1233\ni, j, k\nLoop order\n(outer to inner)\nRunning\ntime (s)\nLast-level-cache\nmiss rate\n1155.77\nj, i, k\ni, k, j\n177.68\n1.0%\n1080.61\n8.6%\nk, i, j\nj, k, i\n3056.63\n15.4%\n179.21\n1.0%\nk, j, i\n3032.82\n15.4%\n(c) 2008-2018 by the MIT 6.172 Lecturers\n7.7%\n\nVersion 4: Interchange Loops\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\nWhat other simple changes we can try?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCompiler Optimization\nClang provides a collection of optimization switches.\nYou can specify a switch to the compiler to ask it to\noptimize.\nOpt. level Meaning\nTime (s)\n!\"#\nDo not optimize\n177.54\n!\"$\nOptimize\n66.24\n!\"%\nOptimize even more\n54.63\n!\"&\nOptimize yet more\n55.58\nClang also supports optimization levels for special\npurposes, such as '\"(, which aims to limit code size,\nand '\"), for debugging purposes.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVersion 5: Optimization Flags\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nWith simple code and compiler technology, we can\nachieve 0.3% of the peak performance of the machine.\nWhat's causing the low performance?\n(c) 2008-2018 by the MIT 6.172 Lecturers\nJava\n\nMulticore Parallelism\nIntel Haswell E5:\n9 cores per chip\nThe AWS test\nmachine has 2 of\nthese chips.\nWe're running on just 1 of the 18 parallel-processing\ncores on this system. Let's use them all!\n(c) Intel. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nParallel Loops\nThe 6%7.8!\"# loop allows all iterations of the loop to\nexecute in parallel.\n6%7.8!\"# $%&' % ( )* % + &* ,,%-\n!\"# $%&' . ( )* . + &* ,,.-\n6%7.8!\"# $%&' / ( )* / + &* ,,/-\n01%21/2 ,( 31%21.2 4 51.21/2*\nThese loops can be\n(easily) parallelized.\nWhich parallel version works best?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExperimenting with Parallel Loops\n!\"#$%&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\nParallel \" loop\nRunning time: 3.18s\nParallel 2 loop\n!\"#$%&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n!\"#$%&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\n&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n!\"#$%&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\nRunning time: 531.71s\nRunning time: 10.64s\nParallel \" and 2 loops\n-.\n/ *. 00\"1\n$\nRunnin\nin\n\" / *. 00\"1\n\" / *. 00\"1\nRunn\nRule of Thumb\nParallelize outer\nloops rather than\ninner loops.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVersion 6: Parallel Loops\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\nUsing parallel loops gets us almost 18× speedup on\n18 cores! (Disclaimer: Not all code is so easy to\nparallelize effectively.)\nWhy are we still getting just 5% of peak?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHardware Caches, Revisited\nIDEA: Restructure the computation to reuse data in the\ncache as much as possible.\n! Cache misses are slow, and cache hits are fast.\n! Try to make the most of the cache by reusing the\ndata that's already there.\nmemory\nprocessor\ncache\nP\nM/B\nB\ncache lines\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nData Reuse: Loops\nHow many memory accesses must the looping code\nperform to fully compute 1 row of C?\n! 4096 * 1 = 4096 writes to C,\n! 4096 * 1 = 4096 reads from A, and\n! 4096 * 4096 = 16,777,216 reads from B, which is\n! 16,785,408 memory accesses total.\nC\nB\nA\n=\nx\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nData Reuse: Blocks\nHow about to compute a 64 ! 64 block of C?\n! 64 · 64 = 4096 writes to C,\n! 64 · 4096 = 262,144 reads from A, and\n! 4096 · 64 = 262,144 reads from B, or\n! 528,384 memory accesses total.\nC\nC\n=\nA\nA\nx\nB\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTiled Matrix Multiplication\ns\n!\"#$%&'( )\"*+ \", - ./ \", 0 */ \", 1- 23\n!\"#$%&'( )\"*+ 4, - ./ 4, 0 */ 4, 1- 23\n&'( )\"*+ $, - ./ $, 0 */ $, 1- 23\n&'( )\"*+ \"# - ./ \"# 0 2/ 11\"#3\n&'( )\"*+ $# - ./ $# 0 2/ 11$#3\n&'( )\"*+ 4# - ./ 4# 0 2/ 114#3\n56\",1\"#764,14#7 1- 86\",1\"#76$,1$#7 9 :6$,1$#764,14#7/\n6$,1$#7\n:6$,1$#764,14#7/\n1$#7 9 :6$,1$#764,14#7/\nTuning parameter\nHow do we find the\nright value of 2?\nExperiment!\ns\n6.74\n2.76\nTile size\ntime (s)\nRunning\nn\n2.49\n1.74\n2.33\nn\n(c) 2008-2018 by the MIT 6.172 Lecturers\n2.13\n\nVersion 7: Tiling\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\n+ tiling\n1.79\n1.70\n11,772\n76.782\n9.184\nImplementation\nCache references\n(millions)\nL1-d cache\nmisses (millions)\nLast-level cache\nmisses (millions)\nParallel loops\n104,090\n17,220\n8,600\n+ tiling\n64,690\n11,777\nThe tiled implementation performs about 62% fewer\ncache references and incurs 68% fewer cache misses.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMulticore Cache Hierarchy\nLevel\nSize\nAssoc. Latency\n(ns)\nMain\n60 GB\nLLC\n25 MB\nL2\n256 KB\nL1-d\n32 KB\nL1-i\n32 KB\n64-byte cache lines\nDRAM\nProcessor chip\nL1\ndata\nL1\ninst\nL1\ndata\nL1\ninst\nL1\ndata\nL1\ninst\nL2\nL2\nL2\nLLC (L3)\nP\nP\n!\nP\nMemory\nController\nNet-\nwork\nDRAM DRAM\n!\n!\n!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTiling for a Two-Level Cache\nn\nt\ns\nn\ns\n∙Two tuning\nparameters, s and t.\n∙Multidimensional\ntuning optimization\ncannot be done with\nbinary search.\nt\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n! Two tuning\nparameters, ! and \".\n! Multidimensional\ntuning optimization\ncannot be done with\nbinary search.\nTiling for a Two-Level Cache\nn\ns\nt\nt\ns\nn\n#$%&'()* +$,\" $- . /0 $- 1 ,0 $- 2. !3\n#$%&'()* +$,\" 4- . /0 4- 1 ,0 4- 2. !3\n()* +$,\" &- . /0 &- 1 ,0 &- 2. !3\n()* +$,\" $5 . /0 $5 1 !0 $5 2. \"3\n()* +$,\" 45 . /0 45 1 !0 45 2. \"3\n()* +$,\" &5 . /0 &5 1 !0 &5 2. \"3\n()* +$,\" $% . /0 $% 1 \"0 22$%3\n()* +$,\" &% . /0 &% 1 \"0 22&%3\n()* +$,\" 4% . /0 4% 1 \"0 224%3\n67$-2$52$%874-24524%8 2.\n97$-2$52$%87&-2&52&%8 : ;7&-2&52&%874-24524%80\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecursive Matrix Multiplication\nIDEA: Tile for every power of 2 simultaneously.\nC00\nC01\nA00\nA01\nB00\nB01\n=\n·\nC10\nC11\nA10\nA11\nB10\nB11\n=\nA00B00\nA00B01\n+\nA01B10\nA11B10\nA01B11\nA11B11\n8 multiplications of n/2 × n/2 matrices.\n1 addition of n × n matrices.\nA10B00\nA10B01\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nE)53B3B73:4&53:E)63B3K73:4&63:49G7?:\nE)53B3B73:4&53:E)63B3B73:4&63:49G7?:\nE)5\nE\n)5\nRecursive Parallel Matrix Multiply\n!\"#$:%%&$'()$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:;<:5:.:6::\n'00-/1))4:=:)>477:<<:47?:\n#@:)4:A<:B7:8:\n.2:;<:.5:.:.6?\nC:-,0-:8:\nD$-@#4-:E)F3/3(7:)F:;:)/.)4&:DD:F7:;:(7.)49G77:\n(#,H&0I'J4:%%&$'()E)23K3K73:4&23:E)53K3K73:4&53:E)63K3K73:4&63:49G7?:\n(#,H&0I'J4:%%&$'()E)23K3B73:4&23:E)53K3K73:4&53:E)63K3B73:4&63:49G7?:\n&5\n(7 )49G77\n7.)49G77\nThe child function call\nis spawned, meaning it\nmay execute in parallel\nwith the parent caller.\n(#,H&0I'J4\n?\n. .6\nE)F\n(#,H&0I'J4:%%&$'()E)23B3K73:4&23:E)53B3K73:4&53:E)63K3K73:4&63:49G7?:\n%%&$'()E)23B3B73:4&23:E)53B3K73:4&53:E)63K3B73:4&63:49G7?:\n(#,H&0L4(?:\n(#,H&0I'J4:%%&$'()E)23K3K73:4&23:E)53K3B73:4&53:E)63B3K73:4&63:49G7?:\n(#,H&0I'J4:%%&$'()E)23K3B73:4&23:E)53K3B73:4&53:E)63B3B73:4&63:49G7?:\n(#,H&0I'J4:%%&$'()E)23B3K73:4&23:\n%%&$'()E)23B3B73:4&23:\n(#,H&0L4(?:\nC:\nC:\nControl may not pass\nthis point until all\nspawned children have\nreturned.\n(#,H&0L4(\n(#,H 0L4(?\n&0\n0I\nI'J4\n%%&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n3B73:4&53:E)63B3B73:4&63:49G7?:\n3B73:4&53:E)63B3K73:4&63:49G7?:\n3B73:4&53:E)63B3B73:4&63:49G7?:\nB7\n4 5\nE)6 B B7\n4 6\n49G7?\nRecursive Parallel Matrix Multiply\n!\"#$:%%&$'()$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:;<:5:.:6::\n'00-/1))4\n#@:)4:A<:B7:8\n)4 A< B7\n47?:\n477:\n)\n?\n=: >\n<<:\n.2:;<:.5:.:.6?\nC:-,0-:8:\nD$-@#4-:E)F3/3(7:)F:;:)/.)4&:DD:F7:;:(7.)49G77:\n(#,H&0I'J4:%%&$'()E)23K3K73:4&23:E)53K3K73:4&53:E)63K3K73:4&63:49G7?:\nE)5 K K7\n4 5\nE)6 K K7\n4 6\n49G7?\n&\nThe base case is too\nsmall. We must coarsen\nthe recursion to overcome\nfunction-call overheads.\n(#,H&0I'J4:%%&$'()E)23K3B73:4&23:E)53K3K73:4&53:E)63K3B73:4&63:49G7?:\n(#,H&0I'J4:%%&$'()E)23B3K73:4&23:E)53B3K73:4&53:E)63K3K73:4&63:49G7?:\n%%&$'()E)23B3B73:4&23:E)53B3K73:4&53:E)63K3B73:4&63:49G7?:\n(#,H&0L4(?:\n(#,H&0I'J4:%%&$'()E)23K3K73:4&23:E)53K3B73:4&53:E)63B3K73:4&63:49G7?:\n(#,H&0I'J4:%%&$'()E)23K3B73:4&23:E)53K:\n(#,H&0I'J4:%%&$'()E)23B3K73:4&23:E)53B:\n%%&$'()E)23B3B73:4&23:E)53B:\n(#,H&0L4(?:\nC:\nC:\nRunning time: 93.93s\n... about 50! slower\nthan the last version!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCoarsening The Recursion\n!\"#$:%%&$'()$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:;<:5:.:6::\n'00-/1))4:=:)>477:<<:47?:\n#@:)4:A<:BCDEFCGHI7:8:\n%%&+'0-)23:4&23:53:4&53:63:4&63:47?:\nJ:-,0-:8:\nK$-@#4-:L)M3/3(7:)M:;:)/.)4&:KK:M7:;:(7.)49N77:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3R73:4&53:L)63R3R73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3R73:4&53:L)63R3S73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3R73:4&53:L)63R3R73:4&63:49N7?:\n%%&$'()L)23S3S73:4&23:L)53S3R73:4&53:L)63R3S73:4&63:49N7?:\n(#,O&0T4(?:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3S73:4&53:L)63S3R73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3S73:4&53:L)63S3S73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3S73:4&53:L)63S3R73:4&63:49N7?:\n%%&$'()L)23S3S73:4&23:L)53S3S73:4&53:L)63S3S73:4&63:49N7?:\n(#,O&0T4(?:\nJ:\nJ:\nJust one tuning\nparameter, for the\nsize of the base case.\n8:\n)4 A< BCDEFCGHI7\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"#$:%%&$'()$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:;<:5:.:6::\n'00-/1))4:=:)>477:<<:47?:\n#@:)4:A<:BCDEFCGHI7:8:\n%%&+'0-)23:4&23:53:4&53:63:4&63:47?:\nJ:-,0-:8:\nK$-@#4-:L)M3/3(7:)M:;:)/.)4&:\nM7:;:(7.)49N77:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3R73:4&53:L)63R3R73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3R73:4&53:L)63R3S73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3R73:4&53:L)63R3R73:4&63:49N7?:\n%%&$'()L)23S3S73:4&23:L)53S3R73:4&53:L)63R3S73:4&63:49N7?:\n(#,O&0T4(?:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3S73:4&53:L)63S3R73:\n3:49N7?:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3S73:4&53:L)63S3S73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3S73:4&53:L)63S3R73:4&63:49N7?:\n%%&$'()L)23S3S73:4&23:L)53S3S73:4&53:L)63S3S73:4&63:49N7?:\n(#,O&0T4(?:\nJ:\nJ:\n!\"#$ %%&$'()$\"*+,- ./-01/#(1 23 #41 4&23\n$\"*+,- ./-01/#(1 53 #41 4&53\n$\"*+,- ./-01/#(1 63 #41 4&63\n#41 47:\n8 99:2:;<:5:.:6:\n'00-/1))4 = )>477 << 47?\n#@ )4 A< BCDEFCGHI7 8\n%%&+'0-)23:4&23 53:4&53 63:4&63 47?\nJ:-,0- 8\nK$-@#4- L)M3/3(7:)M ; )/.)4&:\nM7 ; (7.)49N77\n(#,O&0P'Q4 %%&$'()L)23R3R73:4&23 L)53R3R73:4&53:L)63R3R73:4&63:49N7?\n(#,O&0P'Q4 %%&$'()L)23R3S73:4&23:L)53R3R73:4&53:L)63R3S73:4&63:49N7?\n(#,O&0P'Q4 %%&$'()L)23S3R73:4&23 L)53S3R73:4&53:L)63R3R73:4&63:49N7?\n%%&$'()L)23S3S73:4&23:L)53S3R73:4&53:L)63R3S73:4&63:49N7?\n(#,O&0T4(\n(#,O 0T4(?\n(#,O&0P'Q4 %%&$'()L)23R3R73:4&23 L)53R3S73:4&53:L)63S3R73\n3:49N7?\n(#,O&0P'Q4 %%&$'()L)23R3S73:4&23 L)53R3S73:4&53 L)63S3S73:4&63:49N7?\n(#,O&0P'Q4 %%&$'()L)23S3R73:4&23:L)53S3S73:4&53 L)63S3R73:4&63:49N7?\n%%&$'()L)23S3S73:4&23 L)53S3S73:4&53 L)63S3S73:4&63:49N7?\n(#,O&0T4(?\nJ\nJ\n&\n&\n&\n&\n&\n&\n$\n$\n$\n$\n$\n$\nCoarsening The Recursion\nKK:\n4&6\n!\"#$:%%&+'0-)$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:<:5:.:6:\n@\"/:)#41:#:<:R?:#:A:4?:;;#7:\n@\"/:)#41:O:<:R?:O:A:4?:;;O7:\n@\"/:)#41:U:<:R?:U:A:4?:;;U7:\n2V#.4&2;UW:;<:5V#.4&5;OW:.:6VO.4&6;UW?:\nJ:\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCoarsening The Recursion\n!\"#$:%%&$'()$\"*+,-:./-01/#(1:23:#41:4&23:\n$\"*+,-:./-01/#(1:53:#41:4&53:\n$\"*+,-:./-01/#(1:63:#41:4&63:\n#41:47:\n8:99:2:;<:5:.:6::\n'00-/1))4:=:)>477:<<:47?:\n#@:)4:A<:BCDEFCGHI7:8:\n%%&+'0-)23:4&23:53:4&53:63:4&63:47?:\nJ:-,0-:8:\nK$-@#4-:L)M3/3(7:)M:;:)/.)4&:KK:M7:;:(7.)49N77:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3R73:4&53:L)63R3R73:4&63:49N7?:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3R73:4&53:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3R73:4&53:\n%%&$'()L)23S3S73:4&23:L)53S3R73:4&53:\n(#,O&0T4(?:\n(#,O&0P'Q4:%%&$'()L)23R3R73:4&23:L)53R3S73:4&53:\n(#,O&0P'Q4:%%&$'()L)23R3S73:4&23:L)53R3S73:4&53:\n(#,O&0P'Q4:%%&$'()L)23S3R73:4&23:L)53S3S73:4&53:\n%%&$'()L)23S3S73:4&23:L)53S3S73:4&53:\n(#,O&0T4(?:\nJ:\nJ:\nBase-\ncase size\nRunning\ntime (s)\n3.00\n1.34\n1.34\n1.30\n1.95\n2.08\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n8. Divide-and-Conquer\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\n+ tiling\n1.79\n1.70\n11,772\n76.782\n9.184\nParallel divide-and-conquer\n1.30\n1.38\n16,197 105.722\n12.646\nImplementation\nCache references\n(millions)\nL1-d cache\nmisses (millions)\nLast-level cache\nmisses (millions)\nParallel loops\n104,090\n17,220\n8,600\n+ tiling\n64,690\n11,777\nParallel divide-and-conquer\n58,230\n9,407\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVector Hardware\nModern microprocessors incorporate vector hardware\nto process data in single-instruction stream, multiple-\ndata stream (SIMD) fashion.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMemory an\nLane 0\nLane 1\nLane 2\nLane 3\nWord 0\nWord 1\nWord 2\nWord 3\nVector Load/Store Unit\nALU\nVector Registers\nInstruction decode\nand sequencing\nALU\nALU\nALU\nW\nd 3\nEach vector register\nholds multiple\nwords of data.\nn\n/Store Unit\n/Store Unit\nParallel vector lanes operate\nsynchronously on the words\nin a vector register.\n\nCompiler Vectorization\nClang/LLVM uses vector instructions automatically\nwhen compiling at optimization level !\"#*or higher.\nClang/LLVM can be induced to produce a vectorization\nreport as follows:\n$*%&'()*!\"+*!,-./%00*112%*!3*11*!\"#$%%&'()*+,\n112%45#464*781'794*:8%-37;<8.*&33=*>:8%-37;<'-;3(*?;.-@4*#A\n;(-87&8':8.*%3B(-4*#C*D!E=',,/&33=!:8%-37;<8F\nG37*>;(-*H*/*IJ*H*K*(J*LLHC*M*\nN*\nMany machines don't support the newest set of vector\ninstructions, however, so the compiler uses vector\ninstructions conservatively by default.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVectorization Flags\nProgrammers can direct the compiler to use modern\nvector instructions using compiler flags such as the\nfollowing:\n- -mavx: Use Intel AVX vector instructions.\n- -mavx2: Use Intel AVX2 vector instructions.\n- -mfma: Use fused multiply-add vector instructions.\n- -march=<string>: Use whatever instructions are\navailable on the specified architecture.\n- -march=native: Use whatever instructions are\navailable on the architecture of the machine doing\ncompilation.\nDue to restrictions on floating-point arithmetic,\nadditional flags, such as -ffast-math, might be\nneeded for these vectorization flags to have an effect.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nVersion 9: Compiler Vectorization\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\n+ tiling\n1.79\n1.70\n11,772\n76.782\n9.184\nParallel divide-and-conquer\n1.30\n1.38\n16,197 105.722\n12.646\n+ compiler vectorization\n0.70\n1.87\n30,272 196.341\n23.486\nUsing the flags -march=native -ffast-math nearly\ndoubles the program's performance!\nCan we be smarter than the compiler?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAVX Intrinsic Instructions\nIntel provides C-style functions, called intrinsic\ninstructions , that provide direct access to hardware vector\noperations:\nhttps://software.intel.com/sites/landingpage/IntrinsicsGuide/\n(c) Intel. All rights reserved. This content is excluded from our Creative Commons\n(c) 2008-2018 by the MIT 6.172 Lecturers\nlicense. For more information, see https://ocw.mit.edu/help/faq-fair-use/\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPlus More Optimizations\nWe can apply several more insights and performance-\nengineering tricks to make this code run faster,\nincluding:\n-\nPreprocessing\n-\nMatrix transposition\n-\nData alignment\n-\nMemory-management optimizations\n-\nA clever algorithm for the base case that uses AVX\nintrinsic instructions explicitly\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPlus Performance Engineering\nThink,\ncode,\nrun, run, run...\n...to test and measure many\ndifferent implementations\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nVersion 10: AVX Intrinsics\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\n+ tiling\n1.79\n1.70\n11,772\n76.782\n9.184\nParallel divide-and-conquer\n1.30\n1.38\n16,197\n105.722\n12.646\n+ compiler vectorization\n0.70\n1.87\n30,272\n196.341\n23.486\n+ AVX intrinsics\n0.39\n1.76\n53,292\n352.408\n41.677\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nVersion 11: Final Reckoning\nVersion Implementation\nRunning\ntime (s)\nRelative\nspeedup\nAbsolute\nSpeedup\nGFLOPS\nPercent\nof peak\nPython\n21041.67\n1.00\n0.006\n0.001\nJava\n2387.32\n8.81\n0.058\n0.007\nC\n1155.77\n2.07\n0.118\n0.014\n+ interchange loops\n177.68\n6.50\n0.774\n0.093\n+ optimization flags\n54.63\n3.25\n2.516\n0.301\nParallel loops\n3.04\n17.97\n6,921\n45.211\n5.408\n+ tiling\n1.79\n1.70\n11,772\n76.782\n9.184\nParallel divide-and-conquer\n1.30\n1.38\n16,197\n105.722\n12.646\n+ compiler vectorization\n0.70\n1.87\n30,272\n196.341\n23.486\n+ AVX intrinsics\n0.39\n1.76\n53,292\n352.408\n41.677\nIntel MKL\n0.41\n0.97\n51,497\n335.217\n40.098\nVersion 10 is competitive with Intel's professionally\nengineered Math Kernel Library!\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPerformance Engineering\n53,292!\nGas economy\nMPG\n\" You won't generally see the magnitude\nof performance improvement we\nobtained for matrix multiplication.\n\" But in 6.172, you will learn how to\nprint the currency of performance all\nby yourself.\nCourtesy of pngimg.\nUsed under CC-BY-NC.\nCourtesy of stevepj2009 on Flickr. Used under CC-BY.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 10: Measurement and Timing",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/4d7f4bb31bf1ed90c669a11867d36d36_MIT6_172F18_lec10.pdf",
      "content": "!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n!\"#$%&\n'()*+),-./(&\n0.12.(()2.1&\n+*&3+*45-)(&\n3674(,7&\n3'008&\n9:;:<&\n'0=&>=80=&>?&!\"#$%&!\nLECTURE 10\nMEASUREMENT AND TIMING\nCharles E. Leiserson\n\n#include <stdio.h>\n#include <time.h>\nvoid my_sort(double *A, int n);\nvoid fill(double *A, int n);\nstruct\nint main\ndouble tdiff = (end.tv_sec - start.tv_sec)\n+ 1e-9*(end.tv_nsec - start.tv_nsec);\nprintf(\"size %d, time %f\\n\", n, tdiff);\n}\nreturn 0;\n}\nInspired by a study\ndue to Sivan Toledo.\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\ntimespec start, end;\n() {\n= 4*1000*1000;\n= 1;\n= 20 * 1000;\n[max];\nn=min; n<max; n+=step){\nfill(A, n);\nclock_gettime(CLOCK_MONOTONIC, &start);\nmy sort(A, n);\nclock_gettime(CLOCK_MONOTONIC, &end);\nAuxiliary routine for filling\narray with random numbers.\nTiming a Code for Sorting\nint max\nint min\nint step\ndouble A\nfor (int\nLibrary for clock_gettime()\nSorting routine to be timed.\nUsed by clock_gettime():\nstruct timespec {\ntime_t tv_sec; /* seconds */\nlong tv_nsec; /* nanoseconds */\n};\n\nTiming a Code for Sorting\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n#include <stdio.h>\n#include <time.h>\nvoid my_sort(double *A, int n);\nvoid fill(double *A, int n);\nstruct timespec start, end;\nint main() {\nint max = 4 * 1000 * 1000;\nint min = 500 * 1000;\nint step = 20 * 1000;\ndouble A[max];\nfor (int n=min; n<max; n+=step){\nfill(A, n);\nclock_gettime(CLOCK_MONOTONIC, &start);\nmy sort(A, n);\nclock_gettime(CLOCK_MONOTONIC, &end);\ndouble tdiff = (end.tv_sec - start.tv_sec)\n+ 1e-9*(end.tv_nsec - start.tv_nsec);\nprintf(\"size %d, time %f\\n\", n, tdiff);\n}\nreturn 0;\n}\nLoop over arrays of\nincreasing length.\nMeasure time before sorting.\nSort.\nMeasure time after sorting.\nCompute\nelapsed time.\n\nRunning Times for Sorting\narray size n\nWhat is\ngoing on?\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n0.5e5\n1e6\n1.5e6\n2e6\n2.5e6\n3e6\n3.5e6\n4e6\nRunning time (seconds)\n-10\nMeasured running time\nBest fit to c1 · n lg n\nBest fit to c2 ! n\nMeasured running time\nBest fit to c1· n lg n\nBest fit to c2 ! n\n!\n\"\n\"\n\nDynamic Frequency and Voltage Scaling\nDVFS is a technique to reduce power by adjusting the\nclock frequency and supply voltage to transistors.\n- Reduce operating frequency if chip is too hot or\notherwise to conserve (especially battery) power.\n- Reduce voltage if frequency is reduced.\nC = dynamic capacitance\nPower ∝ C V2 f\n≈ roughly area × activity (how many bits toggle)\nV = supply voltage\nf = clock frequency\nReducing frequency and voltage results\nin a cubic reduction in power (and heat).\nBut it wreaks havoc on performance measurements!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nToday's Lecture\nHow can one reliably measure\nthe performance of software?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nOUTLINE\n! QUIESCING SYSTEMS\n! TOOLS FOR MEASURING\nSOFTWARE PERFORMANCE\n! PERFORMANCE MODELING\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nOUTLINE\n! QUIESCING SYSTEMS\n! TOOLS FOR MEASURING\nSOFTWARE PERFORMANCE\n! PERFORMANCE MODELING\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nGenichi Taguchi and Quality\nQuestion: If you were an Olympic pistol coach, which\nshooter would you recruit for your team?\nAnswer:\nB, because you just need to teach B to\nshoot lower and to the left.\nA\nB\nPerformance-engineering lesson\nIf you can reduce variability, you\ncan compensate for systematic\nand random measurement errors.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSources of Variability\n- Daemons and\nbackground jobs\n- Interrupts\n- Code and data alignment\n- Thread placement\n- Runtime scheduler\n- Hyperthreading\n- Multitenancy\n- Dynamic voltage and\nfrequency scaling (DVFS)\n- Turbo Boost\n- Network traffic\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nUnquiesced System\nExperiment (joint work with Tim Kaler)\n- Cilk program to count the primes in an interval\n- AWS c4 instance (18 cores)\n- 2-way hyperthreading on, Turbo Boost on\n- 18 Cilk workers\n- 100 runs, each about 1 second\n25%\n20%\nPercent above\nMinimum\nPerformance Rank of Run\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n15%\n10%\n5%\n0%\n\nQuiesced System\nExperiment (joint work with Tim Kaler)\n! Cilk program to count the primes in an interval\n! AWS c4 instance (18 cores)\n! 2-way hyperthreading off, Turbo Boost off\n! 18 Cilk workers\n! 100 runs, each about 1 second\n0.0%\n0.1%\n0.2%\n0.3%\n0.4%\n0.5%\n0.6%\n0.7%\n0.8%\nPercent above\nMinimum\nPerformance\nRank of Run\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nQuiescing the System\n- Make sure no other jobs are running.\n- Shut down daemons and cron jobs.\n- Disconnect the network.\n- Don't fiddle with the mouse!\n- For serial jobs, don't run on core 0, where interrupt\nhandlers are usually run.\n- Turn hyperthreading off.\n- Turn off DVFS.\n- Turn off Turbo Boost.\n- Use taskset to pin Cilk workers to cores.\n- Etc., etc. (Already done for you with awsrun.)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCode Alignment\nA small change to one place in the source code can\ncause much of the generated machine code to change\nlocations. Performance can vary due to changes in\ncache alignment and page alignment.\n!\"!\"!\"!\"#\n!\"!!\"!!!#\n\"!!!\"!!\"#\n\"\"\"!!\"!\"#\n!\"!\"!!\"\"#\n!\"!!\"!!!#\n\"!!!!!\"\"#\n\"\"\"!\"\"!!#\n!!!!\"!!!#\n\"!!!\"!!\"#\n!\"\"\"\"\"!\"#\n\"\"\"\"!\"!!#\n\"!!!!!\"\"#\n!\"\"\"\"\"!\"#\n\"\"\"\"!\"!!#\n!!!!!!!\"#\n!\"\"\"\"\"\"\"#\n!\"!\"!\"!\"#\n!\"!!\"!!!#\n\"!!!\"!!\"#\n\"\"\"!!\"!\"#\n\"!\"\"!!!\"#\n!\"!\"!!\"\"#\n!\"!!\"!!!#\n\"!!!!!\"\"#\n\"\"\"!\"\"!!#\n!!!!\"!!!#\n\"!!!\"!!\"#\n!\"\"\"\"\"!\"#\n\"\"\"\"!\"!!#\n\"!!!!!\"\"#\n!\"\"\"\"\"!\"#\n\"\"\"\"!\"!!#\n!!!!!!!\"#\n!\"\"\"\"\"\"\"#\nSimilar: Changing the order in\nwhich the *.o files appear on\nthe linker command line can\nhave a larger effect than going\nbetween -O2 to -O3.\ncache and page\nalignment has\nchanged\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nLLVM Alignment Switches\nLLVM tends to cache-align functions, but it also\nprovides several compiler switches for controlling\nalignment:\n- -align-all-functions=<uint>\n- Force the alignment of all functions.\n- -align-all-blocks=<uint>\n- Force the alignment of all blocks in the function.\n- -align-all-nofallthru-blocks=<uint>\n- Force the alignment of all blocks that have no fall-through\npredecessors (i.e. don't add nops that are executed).\nAligned code is more likely to avoid performance\nanomalies, but it can also sometimes be slower.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nData Alignment\nA program's name can affect its speed!\n- [Mytkowicz, Diwan, Hauswirth, and Sweeney, \"Producing wrong\ndata without doing anything obviously wrong,\" 2009.]\n- The executable's name ends up in an environment\nvariable.\n- Environment variables end up on the call stack.\n- The length of the name affects the stack alignment.\n- Data access slows when crossing page boundaries.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nOUTLINE\n! QUIESCING SYSTEMS\n! TOOLS FOR MEASURING\nSOFTWARE PERFORMANCE\n! PERFORMANCE MODELING\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nWays to Measure a Program\n- Measure the program externally.\n- /usr/bin/time\n- Instrument the program.\n- Include timing calls in the program.\n- E.g., gettimeofday(), clock_gettime(), rdtsc().\n- By hand, or with compiler support.\n- Interrupt the program.\n- Stop the program, and look at its internal state.\n- E.g., gdb, Poor Man's Profiler, gprof.\n- Exploit hardware and operating systems support.\n- Run the program with counters maintained by the hardware\nand operating system, e.g., perf.\n- Simulate the program.\n- E.g., cachegrind.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n/usr/bin/time\nThe time command can measure elapsed time, user\ntime, and system time for an entire program.\nWhat does that mean?\nreal\n0m3.502s\nuser\n0m0.023s\nsys\n0m0.005s\n∙real is wall-clock time.\n∙user is the amount of processor time spent in\nuser-mode code (outside the kernel) within the\nprocess.\n∙sys is the amount of processor time spent in the\nkernel within the process.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nclock_gettime(CLOCK_MONOTONIC, ...)\n#include <time.h>\nstruct timespec start, end;\nclock_gettime(CLOCK_MONOTONIC, &start);\nfunction_to_measure();\nclock_gettime(CLOCK_MONOTONIC, &end);\ndouble tdiff = (end.tv_sec - start.tv_sec)\n+ 1e-9*(end.tv_nsec - start.tv_nsec);\n! On my laptop, clock_gettime(CLOCK_MONOTONIC, ...)\ntakes about 83ns.\n! That's about two orders of magnitude faster than a\nsystem call.\n! clock_gettime(CLOCK_MONOTONIC, ...) guarantees never\nto run backwards.\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nrdtsc()\nx86 processors provide a time-stamp counter (TSC)\nin hardware. You can read TSC as follows:\nstatic __inline__ unsigned long long rdtsc(void)\n{\nunsigned hi, lo;\n__asm__ __volatile__ (\"rdtsc\" : \"=a\"(lo), \"=d\"(hi));\nreturn (\n((unsigned long long)lo)\n| (((unsigned long long)hi)<<32));\n}\n! The time returned is \"clock cycles since boot.\"\n! rdtsc() runs in about 32ns.\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nDon't Use Lousy Timers!\n- rdtsc() may give different answers on different\ncores on the same machine.\n- TSC sometimes runs backwards.\n- The counter may not progress at a constant speed.\n- Converting clock cycles to seconds can be ... tricky.\n- Don't use rdtsc()!\n- And don't use gettimeofday(), either, because it has\nsimilar problems!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nInterrupting\n- IDEA: Run your program under gdb, and type\ncontrol-C at random intervals.\n- Look at the stack each time to determine which\nfunctions are usually being executed.\n- Who needs a fancy profiler?\n- Some people call this strategy the \"Poor Man's\nProfiler.\"\n- pmprof and gprof automate this strategy to provide\nprofile information for all your functions.\n- Neither is accurate if you don't obtain enough\nsamples. (gprof samples only 1OO times per\nsecond.)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHardware Counters\n- libpfm4 virtualizes all the hardware counters\n- Modern kernels make it possible for libraries such\nas libpfm4 to measure all the provided hardware\nevent counters on a per-process basis.\n- perf stat employs libpfm4.\n- There are many esoteric hardware counters. Good\nluck figuring out what they all measure.\n- Watch out: You probably cannot measure more\nthan 4 or 5 counters at a time without paying a\npenalty in performance or accuracy.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSimulation\n- Simulators, such as cachegrind, usually run much\nslower than real time.\n- But they can deliver accurate and repeatable\nperformance numbers.\n- If you want a particular statistic, you can go in\nand collect it without perturbing the simulation.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nOUTLINE\n! QUIESCING SYSTEMS\n! TOOLS FOR MEASURING\nSOFTWARE PERFORMANCE\n! PERFORMANCE MODELING\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nBasic Performance-Engineering Workflow\n1. Measure the performance of Program A.\n2. Make a change to Program A to produce\na hopefully faster Program A!.\n3. Measure the performance of Program A!.\n4. If A! beats A, set A = A!.\n5. If A is still not fast enough, go to Step 2.\nIf you can't measure performance reliably, it is\nhard to make many small changes that add up.\n!\"#$$%&#$'%\"()\"*+,\"-./\"01'2#\"3,4*56,67\"\n\nProblem\nSuppose that you measure the performance of a\ndeterministic program 100 times on a computer with\nsome interfering background noise. What statistic\nbest represents the raw performance of the software?\n- arithmetic mean\n- geometric mean\n- median\n- maximum\n- minimum\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nProblem\nSuppose that you measure the performance of a\ndeterministic program 100 times on a computer with\nsome interfering background noise. What statistic\nbest represents the raw performance of the software?\n- arithmetic mean\n- geometric mean\n- median\n- maximum\n-✓minimum\nMinimum does the best at noise rejection, because\nwe expect that any measurements higher than the\nminimum are due to noise.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSelecting among Summary Statistics\nService as many requests\nas possible\n∙Arithmetic mean\n∙CPU utilization\nAll tasks are completed\nwithin 10 ms\n∙Arithmetic mean\n∙Wall-clock time\nMost service requests are\nsatisfied within 100 ms\n∙90th percentile\n∙Wall clock time\nMeet a customer service-\nlevel agreement (SLA)\n∙Some weighted combination\n∙multiple\nFit into a machine with\n100 MB of memory\n∙Maximum\n∙Memory use\nLeast cost possible\n∙Arithmetic mean\n∙Energy use or CPU utilization\nFastest/biggest/best\nsolution\n∙Arithmetic mean\n∙Speedup of wall clock time\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSummarizing Ratios\n\nConclusion\nProgram B is > 3 times better than A.\nWRONG!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTrial\nProgram A Program B\nA/B\nMean\n7.25\n6.75\n3.03\n3.00\n4.00\n0.10\n5.00\n\nTurn the Comparison Upside-Down\nTrial\nProgram A Program B\nA/B\nB/A\nMean\n7.25\n6.75\n3.03\n2.70\n\nParadox\nIf we look at the ratio B/A, then A is\nbetter by a factor of almost 3.\nObservation\nThe arithmetic mean of A/B is NOT the\ninverse of the arithmetic mean of B/A.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n3.00\n0.33\n4.00\n0.25\n0.10\n10.00\n5.00\n0.20\n\nGeometric Mean\nTrial\nProgram A Program B\nA/B\nB/A\nMean\n(a) 7.25\n(a) 6.75\n(g) 1.57\n(g) 0.64\n\nFormula\nP\nম\nP\nఅ\nn\nভః CK\nCC ੈ CP\nK\nObservation\nThe geometric mean of A/B IS the inverse\nof the geometric mean of B/A.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n3.00\n0.33\n4.00\n0.25\n0.10\n10.00\n5.00\n0.20\n\nComparing Two Programs\nQ. You want to know which of two programs, A and B,\nis faster, and you have a slightly noisy computer on\nwhich to measure their performance. What is your\nstrategy?\nA. Perform n head-to-head comparisons between A\nand B, and suppose A wins more frequently.\nConsider the null hypothesis that B beats A, and\ncalculate the P-value: \"If B beats A, what is the\nprobability that we'd observe that A beats B more\noften than we did?\" If the P-value is low, we can\naccept that A beats B.\n(See Statistics 101.)\nNOTE: With a lot of noise, we need lots of trials.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFitting to a Model\nSuppose that I have gathered this data:\nProgram\nTime (s)\nInstructions\nCache misses\n\npython\njava\nC gcc -O0\nC gcc -O3\nI want to infer how long it takes to run an\ninstruction and how long to take a cache miss.\nI guess that I can model the runtime T as\nT = a⋅I + b⋅C ,\nwhere\n- I is the number of instructions, and\n- C is the number of cache misses.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLeast-Squares Regression\nA least-squares regression can fit the data to the\nmodel\nT = a⋅I + b⋅C ,\nyielding\n- a = 0.2002 ns\n- b = 18.00 ns\nwith R2 = 0.9997, which means that 99.97% of the\ndata is explained by the model.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nIssues with Modeling\nAdding more basis functions to the model improves\nthe fit, but how do I know whether I'm overfitting?\n- Removing a basis function doesn't affect the quality much.\nIs the model predictive?\n- Pick half the data at random.\n- Use that data to find the coefficients.\n- Using those coefficients, fid out how well the model predicts\nthe other half of the data.\nHow can I tell whether I'm fooling myself?\n- Triangulate.\n- Check that different ways of measuring tell a consistent\nstory.\n- Analogously to a spreadsheet, make sure the sum of the row\nsums adds up to the sum of the column sums.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 11: Storage Allocation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/68624442da6047731a33b3590cefd030_MIT6_172F18_lec11.pdf",
      "content": "!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n6.172\nPerformance\nEngineering\nof Software\nSystems\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLECTURE 11\nStorage\nAllocation\nJulian Shun\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSTACKS\n\nStack Allocation\nArray and pointer\nused\nunused\n!\"\n#\nAllocate x bytes\n!\"$%&$'(\n)*+,)-$!\"$. '(\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Allocation\nArray and pointer\nused\nunused\n!\"\n#\n!\"#$%#&'\n$%&'$()!\")* +,\nAllocate x bytes\nShould check for\nstack overflow.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Allocation\nArray and pointer\nused\nunused\n!\"\n#\n!\"$%&$'(\n!\"#$!%&'(&) *+\nAllocate x bytes\nShould check for\nstack overflow.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Deallocation\nArray and pointer\nused\nunused\n!\n\"#\nAllocate x bytes\nFree x bytes\n\"#$%&$'(\n\"#$)&$'(\n*+,-*.$\"#$% '(\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Deallocation\nArray and pointer\nused\nunused\n!\"\n#\nAllocate x bytes\nFree x bytes\n!\"#$%#&'\nShould check for\n!\"$%&$'(\n)*+,)-$!\"$. '(\nstack underflow.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Storage\nArray and pointer\nused\nunused\n!\"\n#\nAllocate x bytes\nFree x bytes\n!\"*+,*-.\n!\"*/,*-.\n012304*!\"*+ -.\n! Allocating and freeing take \"(1) time.\n! Must free consistent with stack discipline.\n! Limited applicability, but great when it works!\n! One can allocate on the call stack using $%%&'$(),\nbut this function is deprecated, and the compiler is\nmore efficient with fixed-size frames.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStacks and Heaps\nImage is in the public domain.\nImage is in the public domain.\nStack\nHeap\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nFIXED-SIZE\nHEAP ALLOCATION\n\nHeap Allocation*\nC provides malloc() and free().\nC++ provides new and delete.\nUnlike Java and Python, C and C++ provide no\ngarbage collector. Heap storage allocated by\nthe programmer must be freed explicitly.\nFailure to do so creates a memory leak. Also,\nwatch for dangling pointers and double freeing.\nMemory checkers (e.g., AddressSanitizer,\nValgrind) can assist in finding these pernicious\nbugs.\n*Do not confuse with a heap data structure.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\nFree list\nused\n! Every piece of storage has the same size\n! Unused storage has a pointer to next unused block\nBitmap mechanism\n! Bit for each block saying whether or not it is free\n! Bit tricks for allocation\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\nFree list\nused\nAllocate 1 object\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\n*\nFree list\nused\nAllocate 1 object\n!\"#\"$%&&'\n\"#$$%&%\"#$$'()$*+,\n#$+-#)%*,%\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\n%\nFree list\nused\nAllocate 1 object\nShould check\n\"#$$&,'&-.//.\n%&'&\"#$$(\n!\"##$%$!\"##&'(#)*+\n#$)*#+&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\n%\ngarbage\nFree list\nused\npointer\nAllocate 1 object\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n!\"#$!%&'(\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Allocation\n!\nused\nused\nused\n\"#$$\nFree list\nused\nused\nAllocate 1 object\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Deallocation\n!\nused\nused\nused\n\"#$$\n%\nFree list\nused\nAllocate 1 object\nfree object %\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n%)*+$%,&'&\"#$$(\n\"#$$&'&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Deallocation\n!\nused\nused\nused\n\"#$$\n%\nFree list\nused\nAllocate 1 object\nfree object %\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n!\"#$%!&'(')*%%+\n\"#$$&'&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Deallocation\n!\nused\nused\nused\n\"#$$\n%\nFree list\nused\nAllocate 1 object\nfree object %\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n%)*+$%,&'&\"#$$(\n!\"##$%$&'\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFixed-Size Deallocation\n!\nused\nused\nused\n\"#$$\nFree list\nused\nAllocate 1 object\nfree object %\n%&'&\"#$$(\n\"#$$&'&\"#$$)*+$%,(\n#$,-#+&%(&\n%)*+$%,&'&\"#$$(\n\"#$$&'&%(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFree Lists\n!\nused\nused\nused\n\"#$$\nFree list\nused\n! Allocating and freeing take \"(1) time.\n! Good temporal locality.\n! Poor spatial locality due to external fragmentation\n-- blocks distributed across virtual memory --\nwhich can increase the size of the page table and\ncause disk thrashing.\n! The translation lookaside buffer (TLB) can also be a\nproblem.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMitigating External Fragmentation\n! Keep a free list (or bitmap) per disk page.\n! Allocate from the free list for the fullest page.\n! Free a block of storage to the free list for the page\non which the block resides.\n! If a page becomes empty (only free-list items), the\nvirtual-memory system can page it out without\naffecting program performance.\n! 90-10 is better than 50-50:\n>\nProbability that 2 random accesses hit the same page\n= .9!.9 + .1!.1 = .82 versus .5!.5 + .5!.5 = .5\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nVARIABLE-SIZE\nHEAP ALLOCATION\n\nVariable-Size Allocation\nBinned free lists\n! Leverage the efficiency of free lists.\n! Accept a bounded amount of internal fragmentation.\n!\n!\nBin k holds memory\nblocks of size 2k.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAllocation for Binned Free Lists\nAllocate % If bin k = \"lg x# is nonempty, return a\nx bytes\nblock.\n% Otherwise, find a block in the next larger\nnonempty bin k& > k, split it up into blocks\nof sizes 2k&-1, 2k&-2, ..., 2k, 2k, and distribute\nthe pieces.\n$\n(c) 2008-2018 by the MIT 6.172 Lecturers\nExample\nx = 3 ! \"lg x# = 2.\nBin 2 is empty.\n\nAllocation for Binned Free Lists\nAllocate \" If bin k = #lg x$ is nonempty, return a\nx bytes\nblock.\n\" Otherwise, find a block in the next larger\nnonempty bin k% > k, split it up into blocks\nof sizes 2k%-1, 2k%-2, ..., 2k, 2k, and distribute\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\nthe pieces.\nExample\nx = 3 & #lg x$ = 2.\nBin 2 is empty.\n\nAllocation for Binned Free Lists\nAllocate \" If bin k = #lg x$ is nonempty, return a\nx bytes\nblock.\n\" Otherwise, find a block in the next larger\nnonempty bin k% > k, split it up into blocks\nof sizes 2k%-1, 2k%-2, ..., 2k, 2k, and distribute\nthe pieces.\n!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nreturn\nExample\nx = 3 & #lg x$ = 2.\nBin 2 is empty.\n\nAllocation for Binned Free Lists\nAllocate \" If bin k = #lg x$ is nonempty, return a\nx bytes\nblock.\n\" Otherwise, find a block in the next larger\nnonempty bin k% > k, split it up into blocks\nof sizes 2k%-1, 2k%-2, ..., 2k, 2k, and distribute\nthe pieces.*\nExample\nx = 3 & #lg x$ = 2.\nBin 2 is empty.\n!\n*If no larger blocks exist, ask the\nreturn\nOS to allocate\nmore memory.\n(c) 2008-2018 by the\n(c) 2008\nMIT 6.172 Lecturers\n2018 b\nth\nM\n\nStorage Layout of a Program\nhigh address\nvirtual\nmemory\ndynamically\nallocated\nstack\nbss\ndata\ntext\nheap\ninitialized to 0 at\nprogram start\nread from disk\ncode\nlow address\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHow Virtual is Virtual Memory?\nQ. Since a 64-bit address space takes over a\ncentury to write at a rate of 4 billion bytes per\nsecond, we effectively never run out of virtual\nmemory. Why not just allocate out of virtual\nmemory and never free?\nA. External fragmentation would be horrendous!\nThe performance of the page table would\ndegrade tremendously leading to disk\nthrashing, since all nonzero memory must be\nbacked up on disk in page-sized blocks.\nGoal of storage allocators\nUse as little virtual memory as possible, and try\nto keep the used portions relatively compact.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Binned Free Lists\nTheorem. Suppose that the maximum amount of\nheap memory in use at any time by a program is M.\nIf the heap is managed by a BFL allocator, the\namount of virtual memory consumed by heap\nstorage is O(M lg M).\nProof. An allocation request for a block of size x\nconsumes 2⌈lg x⌉≤ 2x storage. Thus, the amount\nof virtual memory devoted to blocks of size 2k is at\nmost 2M. Since there are at most lg M free lists,\nthe theorem holds. ■\n⇒In fact, BFL is Θ(1)-competitive with the optimal\nallocator (assuming no coalescing).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCoalescing\nBinned free lists can sometimes be heuristically\nimproved by splicing together adjacent small\nblocks into a larger block.\n●Clever schemes exist for finding adjacent blocks\nefficiently -- e.g., the \"buddy\" system -- but the\noverhead is still greater than simple BFL.\n●No good theoretical bounds exist that prove the\neffectiveness of coalescing.\n●Coalescing seems to reduce fragmentation in\npractice, because heap storage tends to be\ndeallocated as a stack (LIFO) or in batches.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nGARBAGE COLLECTION\nBY REFERENCE COUNTING\n\nGarbage Collectors\nIdea\n∙Free the programmer from freeing objects.\n∙A garbage collector identifies and recycles the\nobjects that the program can no longer access.\n∙GC can be built-in (Java, Python) or do-it-yourself.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nGarbage Collection\nTerminology\n●Roots are objects directly accessible by the\nprogram (globals, stack, etc.).\n●Live objects are reachable from the roots by\nfollowing pointers.\n●Dead objects are inaccessible and can be\nrecycled.\nHow can the GC identify pointers?\n●Strong typing.\n●Prohibit pointer arithmetic (which may slow down\nsome programs).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReference Counting\nKeep a count of the number of pointers referencing\neach object. If the count drops to 0, free the dead\nobject.\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLimitation of Reference Counting\nProblem\nA cycle is never garbage collected!\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLimitation of Reference Counting\nProblem\nA cycle is never garbage collected!\nroot\nroot\nroot\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nLimitation of Reference Counting\nProblem\nA cycle is never garbage collected!\nroot\nroot\nroot\nUncollected\ngarbage\nstinks!\nNevertheless, reference counting\nworks well for acyclic structures.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMARK-AND-SWEEP\nGARBAGE COLLECTION\n\nGraph Abstraction\nIdea\nObjects and pointers\nform a directed graph\nG = (V, E). Live\nobjects are reachable\nfrom the roots. Use\nbreadth-first search to\nfind the live objects.\n!\"#$%! &\"'($)\n*!$%#\"\"+%&(($)\n&,-.#/ 0$12\n3456363%78$&(2\n9$3:;3$&,-.#/ 0$<2\n=>*:3$%7$?0$#($)\n6$0$@356363%7(2\n!\"#$%! &\"' such that %68&(\" A($)\n*!$%&,-.#/ 00$<($)\n&,-.#/ 0$12\n3456363%78$&(2\n9$9$9\nFIFO queue 7\n>3.@\n+.*:\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n(\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n(\n,\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n(\n,\nDone!\n)&\"'\n-\"*.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMark-and-Sweep\nMark stage: Breadth-first search marked all of the live\nobjects.\nSweep stage: Scan over memory to free unmarked\nobjects.\nMark-and-sweep doesn't deal with fragmentation\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSTOP-AND-COPY\nGARBAGE COLLECTION\n\nBreadth-First Search\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n!\n#\n$\n'\n&\n%\n(\n,\nObservation\nAll live vertices are placed in contiguous storage in ,.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\nWhen the FROM space is \"full,\" copy live storage\nusing BFS with the TO space as the FIFO queue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\nWhen the FROM space is \"full,\" copy live storage\nusing BFS with the TO space as the FIFO queue.\nallocation\nTO space\nnext\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nUpdating Pointers\nSince the FROM address of an object is not generally\nequal to the TO address of the object, pointers must\nbe updated.\n∙When an object is copied to the TO space, store a\nforwarding pointer in the FROM object, which\nimplicitly marks it as moved.\n∙When an object is removed from the FIFO queue in\nthe TO space, update all its pointers.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nFROM\nTO\n!\"#$\n%#&'\nRemove an item from the queue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nFROM\nTO\n!\"#$\n%#&'\nRemove an item from the queue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nFROM\nTO\n!\"#$\n%#&'\nEnqueue adjacent vertices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nFROM\nTO\n!\"#$\n%#&'\nEnqueue adjacent vertices.\nPlace forwarding pointers in FROM vertices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\n!\"#$\n%#&'\nFROM\nTO\nUpdate the pointers in the removed item to refer\nto its adjacent items in the TO space.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\n!\"#$\n%#&'\nFROM\nTO\nUpdate the pointers in the removed item to refer\nto its adjacent items in the TO space.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nFROM\nTO\n!\"#$\n%#&'\nLinear time to copy and update all vertices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWhen Is the FROM Space \"Full\"?\nused\nFROM\nheap\n! Request new heap space equal to the used space,\nand consider the FROM space to be \"full\" when\nthis heap space has been allocated.\n! The cost of garbage collection is then\nproportional to the size of the new heap space !\namortized O(1) overhead, assuming that the user\nprogram touches all the memory allocated.\n! Moreover, the VM space required is O(1) times\noptimal by locating the FROM and TO spaces in\ndifferent regions of VM where they cannot\ninterfere with each other.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDynamic Storage Allocation\nLots more is known and unknown about\ndynamic storage allocation. Strategies include\n●buddy system,\n●variants of mark-and-sweep,\n●generational garbage collection,\n●real-time garbage collection,\n●multithreaded storage allocation,\n●parallel garbage collection,\n●etc.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSummary\n●Stack: most basic form of storage and is very\nefficient when it works\n●Heap is the more general form of storage\n●Fixed-size allocation using free lists\n●Variable-sized allocation using binned free lists\n●Garbage collection - reference counting, mark\nand-sweep, stop-and-copy\n●Internal and external fragmentation\n●You will look at storage allocation in Homework 6\nand Project 3\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 12: Parallel Storage Allocation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/24c247c943024dcda40bcc7caa206581_MIT6_172F18_lec12.pdf",
      "content": "SPEED\nLIMITinf\nPER ORDER OF 6.172\n6.172\nPerformance\nEngineering\nof Software\nSystems\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLECTURE 12\nParallel Storage\nAllocation\nJulian Shun\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nREVIEW OF MEMORY-\nALLOCATION PRIMITIVES\n\nHeap Storage in C\nAllocation\nf\nAligned allocation\nf\nDeallocation\nf\nHeap Storage in C\n● Allocation\nvoid* malloc(size_t s);\nE fect: Allocate and return a pointer to a block of\nmemory containing at least s bytes.\n● Aligned allocation\nvoid* memalign(size_t a, size_t s);\nE fect: Allocate and return a pointer to a block of\nmemory containing at least s bytes, aligned to a\nmultiple of a, where a must be an exact power of 2:\n0 == ((size_t) memalign(a, s)) % a .\n● Deallocation\nvoid free(void *p);\nE fect: p is a pointer to a block of memory returned\nby malloc() or memalign(). Deallocate the block.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAllocating Virtual Memory\nThe mmap() system call can be used to allocate\nvirtual memory by memory mapping:\nvoid *p = mmap(0, // Don't care where\nsize, // #bytes\nPROT_READ | PROT_WRITE, // Read/write\nMAP_PRIVATE | MAP_ANON, // Private anonymous\n-1, // no backing file\n0 // offset (N/A)\n);\nThe Linux kernel finds a contiguous, unused region in\nthe address space of the application large enough to\nhold size bytes, modifies the page table, and creates\nthe necessary virtual-memory management structures\nwithin the OS to make the user's accesses to this area\n\"legal\" so that accesses won't result in a segfault.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nProperties of\nProperties of mmap()\n● mmap() is lazy. It does not immediately allocate\nphysical memory for the requested allocation.\n● Instead, it populates the page table with entries\npointing to a special zero page and marks the page\nas read only.\n● The first write into such a page causes a page fault.\n● At that point, the OS allocates a physical page,\nmodifies the page table, and restarts the\ninstruction.\n● You can mmap() a terabyte of virtual memory on a\nmachine with only a gigabyte of DRAM.\n● A process may die from running out of physical\nmemory well after after the mmap() call.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWhat's the Difference...\nWhat's the Difference...\n...between malloc() and mmap() used in this way?\n● The functions malloc() and free() are part of the\nmemory-allocation interface of the heap-\nmanagement code in the C library.\n● The heap-management code uses available\nsystem facilities, including mmap(), to obtain\nmemory (virtual address space) from the kernel.\n● The heap-management code within malloc()\nattempts to satisfy user requests for heap\nstorage by reusing freed memory whenever\npossible.\n● When necessary, the malloc() implementation\ninvokes mmap() and other system calls to\nexpand the size of the user's heap storage.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAddress Translation\n⋮\nvirtual address\nphysical memory\noffset\nsearch\nphysical address\npage table\nvirtual page #\noffset\nframe #\nframe 0\nframe 1\nframe 2\nframe 3\nbl\nframe #\noffset\nIf the virtual page does not reside in\nphysical memory, a page fault occurs.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n8-2018 by the MIT 6.172 Lecturers\nAddress Translation\nvirtual page #\noffset\nvirtual address\nframe #\noffset\nphysical address\nframe 0\nframe 1\nframe 2\nframe 3\n⋮\nphysical memory\nsearch\noffset\nframe #\npage table\nSince page-table lookups are costly, the hardware\ncontains a translation lookaside buffer (TLB) to\ncache recent page-table lookups.\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACTUS STACKS\n\nTraditional Linear Stack\nAn execution of a serial C/C++ program can be\nviewed as a serial walk of an invocation tree.\nB\nA\nC\nE\nD\nA\nA\nB\nA\nC\nA\nC\nD\nA\nC\nE\nC\nB\nA\nD\nE\ninvocation tree\nviews of stack\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTraditional Linear Stack\nRule for pointers: A parent can pass pointers to its\nstack variables down to its children, but not the\nother way around.\nB\nA\nC\nE\nD\nA\nA\nB\nA\nC\nA\nC\nD\nA\nC\nE\nC\nB\nA\nD\nE\ninvocation tree\nviews of stack\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCactus Stack\nA cactus stack supports multiple views in parallel.\nB\nA\nC\nE\nD\nA\nA\nB\nA\nC\nA\nC\nD\nA\nC\nE\nC\nB\nA\nD\nE\ninvocation tree\nviews of stack\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHeap-Based Cactus Stack\nA heap-based cactus stack allocates frames off the heap.\nA\nC\nD\nE\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSpace Bound\nTheorem. Let S1 be the stack space required by a\nserial execution of a Cilk program. The stack space of\na P-worker execution using a heap-based cactus stack\nis at most SP ≤P S1.\nProof. Cilk's work-stealing\nalgorithm maintains the\nbusy-leaves property:\nEvery active leaf frame has\na worker executing it. ∎\nS1\nP = 4\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nD&C Matrix Multiplication\nvoid mm_dac(double *restrict C, int n_C,\ndouble *restrict A, int n_A,\ndouble *restrict B, int n_B,\nint n)\n{ // C = A * B\nassert((n & (-n)) == n);\nif (n <= THRESHOLD) {\nmm_base(C, n_C, A, n_A, B, n_B, n);\n} else {\ndouble *D = malloc(n * n * sizeof(*D));\nassert(D != NULL);\n#define n_D n\n#define X(M,r,c) (M + (r*(n_ ## M) + c)*(n/2))\ncilk_spawn mm_dac(X(C,0,0), n_C, X(A,0,0), n_A, X(B,0,0), n_B, n/2);\ncilk_spawn mm_dac(X(C,0,1), n_C, X(A,0,0), n_A, X(B,0,1), n_B, n/2);\ncilk_spawn mm_dac(X(C,1,0), n_C, X(A,1,0), n_A, X(B,0,0), n_B, n/2);\ncilk_spawn mm_dac(X(C,1,1), n_C, X(A,1,0), n_A, X(B,0,1), n_B, n/2);\ncilk_spawn mm_dac(X(D,0,0), n_D, X(A,0,1), n_A, X(B,1,0), n_B, n/2);\ncilk_spawn mm_dac(X(D,0,1), n_D, X(A,0,1), n_A, X(B,1,1), n_B, n/2);\ncilk_spawn mm_dac(X(D,1,0), n_D, X(A,1,1), n_A, X(B,1,0), n_B, n/2);\nmm_dac(X(D,1,1), n_D, X(A,1,1), n_A, X(B,1,1), n_B, n/2);\ncilk_sync;\nm_add(C, n_C, D, n_D, n);\nfree(D);\n}\n}\nNotice that\nallocations of\nthe temporary\nmatrix D obey a\nstack discipline.\ndouble *D = malloc(n * n * sizeof(*D));\nfree(D);\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of D&C Matrix Mult.\nWe can actually prove a stronger bound.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n(n/2)2\nBranch fully (8\n...\nWorst-Case Recursion Tree\nn2\n\nWorst-Case Recursion Tree\nway) until we\n(n/2)2\n(n/2)2\nget to a level k\nwith P nodes\n...\n(n/2k)2\n(n/2k)2\n(n/2k)2\nand then\nbranch serially\n...\nfrom there on.\nP nodes\nΘ(1)\nΘ(1)\nΘ(1)\nWe have 8k = P, which implies that k = log8P = (lg P)/3.\nThe cost per level grows geometrically from the root to\nlevel k and then decreases geometrically from level k to\nthe leaves. Thus, the space is Θ(P(n/2(lg P)/3)2) = Θ(P1/3n2).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nInteroperability\nProblem: With heap-based linkage, parallel functions\nfail to interoperate with legacy and third-party serial\nbinaries. Our implementation of Cilk uses a less\nspace-efficient strategy that preserves interoperability\nby using a pool of linear stacks.\nA\nC\nD\nE\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nBASIC PROPERTIES OF\nSTORAGE ALLOCATORS\n\nAllocator Speed\nDefinition.\nspeed\nAllocator Speed\nDefinition. Allocator speed is the number of\nallocations and deallocations per second that the\nallocator can sustain.\nQ. Is it more important to maximize allocator\nspeed for large blocks or small blocks?\nA. Small blocks!\nQ. Why?\nA. Typically, a user program writes all the bytes\nof an allocated block. A large block takes so\nmuch time to write that the allocator time has\nlittle effect on the overall runtime. In contrast,\nif a program allocates many small blocks, the\nallocator time can represent a significant\noverhead.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFragmentation\nDefinition.\nuser footprint\nallocator\nfootprint\nfragmentation\nRemark\nTheorem\n.\nRemark.\nFragmentation\nDefinition. The user footprint is the maximum over\ntime of the number U of bytes in use by the user\nprogram (allocated but not freed). The allocator\nfootprint is the maximum over time of the number A\nof bytes of memory provided to the allocator by the\noperating system. The fragmentation is F = A/U.\nRemark. A grows monotonically for many\nallocators.\nTheorem (proved in Lecture 11). The fragmentation\nfor binned free lists is FV = O(lg U). ∎\nRemark. Modern 64-bit processors provide about\n248 bytes of virtual address space. A big server\nmight have 240 bytes of physical memory.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFragmentation Glossary\nFragmentation Glossary\n∙ Space overhead: Space used by the allocator for\nbookkeeping.\n∙ Internal fragmentation: Waste due to allocating\nlarger blocks than the user requests.\n∙ External fragmentation: Waste due to the inability\nto use storage because it is not contiguous.\n∙ Blowup: For a parallel allocator, the additional\nspace beyond what a serial allocator would require.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPARALLEL ALLOCATION\nSTRATEGIES\n\nStrategy 1: Global Heap\n∙Default C allocator.\n∙All threads (processors)\nshare a single heap.\n∙Accesses are mediated\nby a mutex (or lock-free\nsynchronization) to\npreserve atomicity.\nJ Blowup = 1.\nL Slow -- acquiring a\nlock is like an L2-cache\naccess.\nL Contention can inhibit\nscalability.\nglobal heap\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nScalability\nScalability\nIdeally, as the number of threads (processors)\ngrows, the time to perform an allocation or\ndeallocation should not increase.\n∙ The most common reason for loss of scalability\nis lock contention.\nQ. Is lock contention more of a problem for large\nblocks or for small blocks?\nA. Small blocks!\nQ. Why?\nA. Typically, a user program writes all the bytes of\nan allocated block, making it hard for a thread\nallocating large blocks to issue allocation\nrequests at a high rate. In contrast, if a program\nallocates many small blocks in parallel,\ncontention can be a significant issue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStrategy 2: Local Heaps\n∙Each thread allocates\nout of its own heap.\n∙No locking is necessary.\nJ Fast -- no\nsynchronization.\nL Suffers from memory\ndrift: blocks allocated\nby one thread are freed\non another ⇒\nunbounded blowup.\nheap\nheap\nheap\nheap\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStrategy 3: Local Ownership\n∙Each object is labeled\nwith its owner.\n∙Freed objects are\nreturned to the owner's\nheap.\nJ Fast allocation and\nfreeing of local\nobjects.\nL Freeing remote\nobjects requires\nsynchronization.\nK Blowup ≤ P.\nJ Resilience to false\nsharing.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nheap\nheap\nheap\nheap\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nFALSE SHARING\n\nFalse Sharing Example\n...\nP\nP\nP\nWrite x\nx\ny\nThe compiler happens\nto place x and y in the\nsame cache block.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFalse Sharing Example\n...\nP\nP\nP\nx\ny\nWrite y\nThe compiler happens\nto place x and y in the\nsame cache block.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFalse Sharing Example\n...\nP\nP\nP\nx\ny\nWrite x\nThe compiler happens\nto place x and y in the\nsame cache block.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFalse Sharing Example\n...\nP\nP\nP\nx\ny\nWrite y\nThe compiler happens\nto place x and y in the\nsame cache block.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHow False Sharing Can Occur\nHow False Sharing Can Occur\nA program can induce false sharing having\ndifferent threads process nearby objects.\n∙ The programmer can mitigate this problem by\naligning the object on a cache-line boundary and\npadding out the object to the size of a cache line, but\nthis solution can be wasteful of space.\nAn allocator can induce false sharing in two ways:\n∙ Actively, when the allocator satisfies memory\nrequests from different threads using the same cache\nblock.\n∙ Passively, when the program passes objects lying on\nthe same cache line to different threads, and the\nallocator reuses the objects' storage after the objects\nare freed to satisfy requests from those threads.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nBACK TO PARALLEL\nHEAP ALLOCATION\n\nThe Hoard Allocator\n∙P local heaps.\n∙1 global heap.\n∙Memory is organized\ninto large superblocks\nof size S.\n∙Only superblocks are\nmoved between the\nlocal heaps and the\nglobal heap.\nJ Fast.\nJ Scalable.\nJ Bounded blowup.\nJ Resilience to false\nsharing.\nheap\nheap\nheap\nheap\nglobal heap\nheap\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHoard Allocation\nAssume without loss of generality that all\nblocks are the same size (fixed-size allocation).\nif (there exists a free object in heap i) {\nx = an object from the fullest nonfull superblock in i's heap;\n} else {\nif (the global heap is empty) {\nB = a new superblock from the OS;\n} else {\nB = a superblock in the global heap;\n}\nset the owner of B to i;\nx = a free object in B;\n}\nreturn x;\nx = malloc() on thread i\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHoard Deallocation\nLet ui be the in-use storage in heap i, and\nlet ai be the storage owned by heap i.\nHoard maintains the following invariant for\nall heaps i:\nui ≥ min(ai - 2S, ai/2),\nwhere S is the superblock size.\nfree(x), where x is owned by thread i:\nput x back in heap i;\nif (ui < min(ai - 2S, ai/2)) {\nmove a superblock that is at least 1/2 empty from\nheap i to the global heap;\n};\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHoard's Blowup\nLemma.\nTheorem.\nProof.\nHoard's Blowup\nLemma. The maximum storage allocated in\nglobal heap is at most maximum storage\nallocated in local heaps.\nTheorem. Let U be the user footprint for a\nprogram, and let A be Hoard's allocator\nfootprint. We have\nA ≤ O(U + SP) ,\nand hence the blowup is\nA/U = O(1 + SP/U) . ∎\nProof. Analyze storage in local heaps.\nRecall that ui ≥ min(ai - 2S, ai/2).\nFirst term: at most 2S unutilized storage per\nheap for a total of O(SP).\nSecond term: allocated storage is at most twice\nthe used storage for a total of O(U). ∎\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nOther Solutions\nOther Solutions\njemalloc is like Hoard, with a few differences:\n● jemalloc has a separate global lock for each\ndifferent allocation size.\n● jemalloc allocates the object with the smallest\naddress among all objects of the requested size.\n● jemalloc releases empty pages using\nmadvise(p, MADV_DONTNEED, ...) ,\nwhich zeros the page while keeping the virtual\naddress valid.\n● jemalloc is a popular choice for parallel systems\ndue to its performance and robustness.\nSuperMalloc is an up-and-coming contender. (See\npaper by Bradley C. Kuszmaul.)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAllocator Speeds\nAllocator\nSLOC\nthreads\nAllocator Speeds\nAllocator\nSLOC\n32 threads\nDefault\n6,281\n0.97 M/s\nHoard\n16,948\n17.1 M/s\njemalloc\n22,230\n38.2 M/s\nSuperMalloc\n3,571\n131.7 M/s\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSPEED\nLIMITinf\nPER ORDER OF 6.172\n(c) 2008-2018 by the MIT 6.172 Lecturers\nGARBAGE COLLECTION\n\nCopying Garbage Collector\nFROM space\nnext\nallocation\ndead\nlive\nunused\nWhen the FROM space is \"full,\" copy live storage\nusing BFS with the TO space as the FIFO queue.\nTO space\nnext\nallocation\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nUpdating Pointers\nUpdating Pointers\nSince the FROM address of an object is not generally\nequal to the TO address of the object, pointers must\nbe updated.\n∙ When an object is copied to the TO space, store a\nforwarding pointer in the FROM object, which\nimplicitly marks it as moved.\n∙ When an object is removed from the FIFO queue in\nthe TO space, update all its pointers.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nRemove an item from the queue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nRemove an item from the queue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nEnqueue adjacent vertices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nEnqueue adjacent vertices.\nPlace forwarding pointers in FROM vertices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nUpdate the pointers in the removed item to refer\nto its adjacent items in the TO space.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample\nhead\ntail\nFROM\nTO\nUpdate the pointers in the removed item to refer\nto its adjacent items in the TO space.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTypes of Garbage Collectors\nStop-the-world collector\n- Program pauses once in a while and garbage\ncollector (GC) does work across all of memory\n- High program pause times\nProgram\nGC\nProgram\nGC\n...\nIncremental collector\n- Collector cleans up a small part of memory every\ntime it executes\n- Low program pause times\nProgram\nGC\n...\nProgram\nGC\nProgram\nGC\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRunning Collector with Program\nRunning Collector with Program\n∙ Incremental version of copying collector.\n∙ When it is time to collect, application program and\ngarbage collector take turns running.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRunning Collector with Program\nFROM\nTO\nhead\ntail\nIf an object O already dequeued in BFS gains a\nreference to another object O', the BFS may not\nfind O' and it will be freed.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRunning Collector with Program\nFROM\nTO\nhead\ntail\nIf an object O already dequeued in BFS gains a\nreference to another object O', the BFS may not\nfind O' and it will be freed.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBaker's Algorithm\nBaker's Algorithm\n∙ Program follows forward pointer if there is one.\n∙ Whenever the program accesses an object not in\nthe TO space, mark object as explored and copy it\nover to the TO space.\n∙ Whenever the program allocates an object, put it in\nthe TO space.\n∙ Requires a read barrier to intercept every read with\na check, which is expensive.\n∙ This algorithm is conservative in that it does not\nnecessarily collect all garbage. Why?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nNettles-O'Toole Algorithm\nNettles-O'Toole Algorithm\n∙ Program works only in FROM space until garbage\ncollection is finished.\n∙ Replicates the objects by keeping mutations to\nFROM-space objects in a log.\n∙ Garbage collector applies the mutations to\ncorresponding TO-space objects.\n∙ Requires a write barrier to log mutations on every\nwrite\n∙ This is expensive, but writes are usually much\nless frequent than reads.\n∙ Is this algorithm conservative?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nGarbage Collection Glossary\nGarbage Collection Glossary\n∙ Stop-the-world: Garbage collector does all of its\nwork across memory while pausing program.\n∙ Incremental: Garbage collector runs incrementally,\nallowing pause times to be bounded.\n∙ Parallel: Multiple collector threads are running\nsimultaneously.\n∙ Concurrent: At least one program thread and one\ncollector thread are running simultaneously.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nParallel and Concurrent GC\nParallel and Concurrent GC\n∙ Based on Nettles-O'Toole algorithm\n∙ High-level idea\n∙ Use per-processor local stacks for search\n∙ Maintain a shared stack for load balancing\n∙ Processors periodically transfer objects between\nlocal and shared stack\n∙ Use synchronization primitives (test-and-set and\nfetch-and-add) to manage concurrent accesses\nSee \"On Bounding Time and Space for Multiprocessor Garbage\nCollection\" (PLDI 1999), and \"A Parallel, Real-Time Garbage\nCollector\" (PLDI 2001) by Cheng and Blelloch\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSummary\nSummary\n∙ malloc() vs. mmap()\n∙ Cactus stacks\n∙ Cilk space bound of SP ≤ P S1 and better bound for\nmatrix multiply\n∙ Parallel allocation strategies: global heap, local\nheaps, local ownership\n∙ Incremental garbage collection\n∙ Parallel and concurrent garbage collection\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 13: The Cilk Runtime System",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/fe9b746636bea5357ea8269fe6463020_MIT6_172F18_lec13.pdf",
      "content": "6.172\nPerformance\nEngineering\nof Software\nSystems\nLECTURE 13\nThe Cilk Runtime\nSystem\nTao B. Schardl\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Cilk Programming\nCilk allows programmers to make software run faster\nusing parallel processors.\n&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\nSerial matrix multiply\nRunning time TS.\nCilk matrix multiply\nRunning time TP on P\nprocessors.\n!\"#$%&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n...\n$\nP\n$\nP\n$\nP\n$\n$\n$\nNetwork\nRecall: Cilk Scheduling\n! The Cilk concurrency\nplatform allows the\nprogrammer to express\nlogical parallelism in an\napplication.\n! The Cilk scheduler maps\nthe executing program\nonto the processor cores\ndynamically at runtime.\n! Cilk's work-stealing\nscheduling algorithm is\nprovably efficient.\n!\"#$%&#*'!()!\"#$%&#*\"+*,*\n!'*)\"*-*.+*,*\n/0#1/\"*\"2*\n3*0450*,*\n!\"#$%&#*67*82*\n6*9*:!4;&5<=>\"*'!()\"?@+2*\n8*9*'!()\"?.+2*\n:!4;&58\":2***\n/0#1/\"*)6*A*8+2*\n3*\n3*\nMemory\nI/O\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Cilk Platform\n(c) 2008-2018 by the MIT 6.172 Lecturers\nParallel\nperformance\nCilk\ncompiler\n!\"#$%&#*'!()!\"#$%&#*\"+*,*\n!'*)\"*-*.+*,*/0#1/\"*\"2*3*\n0450*,*\n!\"#$%&#*67*82*\n6*9*!\"#$%&'()* '!()\":;+2*\n8*9*'!()\":.+2*\n!\"#$%&+*!2***\n/0#1/\"*)6*<*8+2*\n3*\n3*\nCilk source\nP\n!\nP\nP\nBinary\nProgram\ninput\nThis lecture:\nHow does Cilk\nwork?\n\nA More Accurate Picture\nParallel\nperformance\nCilk\ncompiler\n!\"#$%&#*'!()!\"#$%&#*\"+*,*\n!'*)\"*-*.+*,*/0#1/\"*\"2*3*\n0450*,*\n!\"#$%&#*67*82*\n6*9*!\"#$%&'()* '!()\":;+2*\n8*9*'!()\":.+2*\n!\"#$%&+*!2***\n/0#1/\"*)6*<*8+2*\n3*\n3*\nCilk source\nP\n!\nP\nP\nBinary\nProgram\ninput\nlibcilkrts.so\nThe compiler and\nruntime library\ntogether implement\nthe runtime system.\nCilk runtime-\nsystem library\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWhat the Compiler Generates\nCilk code\nC pseudocode of\ncompiled result\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n!\"# )* +,\n) - .!/012345\" 647&\"',\n+ - 648&\"',\n.!/012+\".,\n79#:7\" ) ; +,\n<\n!\"# $%%&!\"# \"' (\n11.!/07#212#4.01$74=91# 2$,\n11.!/07#219\"#971$74=9&>2$',\n!\"# )* +,\n!$ &?29#@=3&2$A.#)''\n2345\"1647&>)* \"',\n+ - 648&\"',\n!$ &2$A$/4B2 > CDEF1GHIJK1LMNOMCPKQ'\n!$ &?29#@=3&2$A.#)''\n11.!/07#212+\".&>2$',\n!\"# 792:/# - ) ; +,\n11.!/07#213%31$74=9&>2$',\n!$ &2$A$/4B2'\n11.!/07#21/94R91$74=9&>2$',\n79#:7\" 792:/#,\n<\nR%!S 2345\"1647&!\"# T)* !\"# \"' (\n11.!/07#212#4.01$74=9 2$,\n11.!/07#219\"#971$74=91$42#&>2$',\n11.!/07#21S9#4.U&',\nT) - 647&\"',\n11.!/07#213%31$74=9&>2$',\n11.!/07#21/94R91$74=9&>2$',\n<\nCilk\ncompiler\n\nOutline\n-\nREQUIRED FUNCTIONALITY\n-\nPERFORMANCE CONSIDERATIONS\n-\nIMPLEMENTING A WORKER DEQUE\n-\nSPAWNING COMPUTATION\n-\nSTEALING COMPUTATION\n-\nSYNCHRONIZING COMPUTATION\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nREQUIRED FUNCTIONALITY\n\nRecall: Execution Model\n!\"#&$!%&'!\"#&\"(&)&\n!$&'\"&*&+(&,-#.,\"&\"/&\n-01-&)&\n!\"#&23&4/&\n2&5&6!07819:;\"&$!%'\"<=(/&\n4&5&$!%'\"<+(/&\n6!07814\"6/&\n,-#.,\"&2&>&4/&\n?&\n?&\nExample:\n$!%'@(&\n!\n\"\n#\n#\n$\n$\n$\n%\n%\nThe computation dag\nunfolds dynamically.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nP1\nP1\nP1\nA,!9\nA,!9\nA,!9\nSerial Execution\nP1\nA,!9&\nExample:\n$!%'@(&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#&$!%&'!\"#&\"(&)&\n!$&'\"&*&+(&,-#.,\"&\"/&\n-01-&)&\n!\"#&23&4/&\n2&5&6!07819:;\"&$!%'\"<=(/&\n4&5&$!%'\"<+(/&\n6!07814\"6/&\n,-#.,\"&2&>&4/&\n?&\n?&\n!\n\"\n#\n$\n! P1\nP1\nP1\nAvailable for\nexecution.\ne\nAvailable for\nexecution.\nexec\nAvailable for\nexecution.\n\nA,!9\nA,!9\nA,!9\nParallel Execution: Steals\nP3\nP2\nP1\nExample:\nA,!9\nA,!9&\n$!%'@(&\nP2\nP3\nP1\nA,!9&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#&$!%&'!\"#&\"(&)&\n!$&'\"&*&+(&,-#.,\"&\"/&\n-01-&)&\n!\"#&23&4/&\n2&5&6!07819:;\"&$!%'\"<=(/&\n4&5&$!%'\"<+(/\n6!07814\"6/&\n,-#.,\"&2&>&4/&\n?&\n?&\n!\n\"\n#\n#\n$\n$\n$\nP2\n$\n/\n!\nSteal!\n:;\" $!%'\"\n4/ \"\n:\n(\n:;\"&$!%'\n( Steal!\nP3\n$ P1\nHow does a processor start\nexecuting in the middle of\na running function?\n\nParallel Execution: Syncs\nP3\nA,!9&\nExample:\n$!%'@(&\nP2\nA,!9\nP1\nA,!9\nP3\nA,!9&\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#&$!%&'!\"#&\"(&)&\n!$&'\"&*&+(&,-#.,\"&\"/&\n-01-&)&\n!\"#&23&4/&\n2&5&6!07819:;\"&$!%'\"<=(/&\n4&5&$!%'\"<+(/&\n6!07814\"6/&\n,-#.,\"&2&>&4/&\n?&\n?&\n!\n\"\n#\n/\n;\n/\n&\n\"&$!%'\" =(/\nexecute!\n#\n$\n$\n$\nHow does a 6!07814\"6&wait only\nP2\nP1\n$ P3\nCan't\non nested subcomputations?\n\nRequired Functionality\n- A single worker must be able to execute the\ncomputation on its own similarly to an\nordinary serial computation.\n- A thief must be able to jump into the\nmiddle of an executing function to steal a\ncontinuation.\n- A sync must stall a function's execution\nuntil child subcomputations complete.\nWhat other\nfunctionality is\nneeded?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Cactus Stack\nCilk supports C's rule for pointers: A pointer to stack\nspace can be passed from parent to child, but not from\nchild to parent.\nViews of stack\nA\nA\nB\nA\nC\nA\nC\nD\nA\nC\nE\nCilk's cactus stack supports\nmultiple views in parallel.\nP1\nP2\nP3\nP4\nP5\nB\nA\nC\nE\nD\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\ndeque like a stack [MKH90, BL94, FLR98].\nspawned\np\ncalled\ncalled\nspawned\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\np\ncalled\nP\nP\nP\nP\nEach deque contains a mixture of\nspawned frames and called frames.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\ndeque like a stack [MKH90, BL94, FLR98].\nP\nspawned\np\ncalled\ncalled\ncalled\nP\nspawned\np\ncalled\nCall!\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nP\nP\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\ndeque like a stack [MKH90, BL94, FLR98].\nP\nspawned\np\ncalled\ncalled\ncalled\nspawned\nP\nspawned\np\ncalled\nSpawn!\nspawn\nspawn\nspawned\npp\npp\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nP\nP\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\ndeque like a stack [MKH90, BL94, FLR98].\nP\nspawned\np\ncalled\ncalled\ncalled\nspawned\nP\nspawned\nP\np\ncalled\nspawned\np\ncalled\ncalled\np\nspawned\nSteal!\nspawned\np\ncalled\nspawned\nP\nWhen a worker runs out of work, it steals\nfrom the top of a random victim's deque.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\nP\nspawned\ncalled\ncalled\ncalled\nspawned\nP\nspawned\nP\nP\np\ncalled\nspawned\np\ncalled\ncalled\nspawned\np\ncalled\nspawned\nspawned\nSteal!\ndeque like a stack [MKH90, BL94, FLR98].\nWhen a worker runs out of work, it steals\nfrom the top of a random victim's deque.\nA steal takes all frames up\nto the next spawned frame.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nspawned\nP\np\ncalled\nspawned\np\ncalled\ncalled\nspawned\np\ncalled\nspawned\nwork, it steals\nvictim's deque.\nRecall: Work Stealing\nEach worker (processor) maintains a work deque of\nready strands, and it manipulates the bottom of the\ndeque like a stack [MKH90, BL94, FLR98].\nspawned\np\ncalled\ncalled\ncalled\nspawned\np\nspawned\nP\nP\nP\nWhen a worker runs out of\nfrom the top of a random\nWhat is involved in\nstealing frames?\n! What\nsynchronization\nis needed?\n! What happens to\nthe stack?\n! How efficient can\nthis be?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRequired Functionality\n- A single worker must be able to execute the\ncomputation on its own similarly to an\nordinary serial computation.\n- A thief must be able to jump into the\nmiddle of an executing function to steal a\ncontinuation.\n- A sync must stall a function's execution\nuntil child subcomputations complete.\n- The runtime must implement a cactus stack\nfor its parallel workers.\n- Thieves must be able to handle mixtures of\ncalled spawned functions.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPERFORMANCE\nCONSIDERATIONS\n\nRecall: Work-Stealing Bounds\nTheorem [BL94]. The Cilk work-stealing scheduler\nachieves expected running time\nTime workers\nspend working.\nTP ! T1/P + O(T\nTime workers\nspend stealing.\n\")\non P processors.\nIf the program achieves linear speedup, then\nworkers spend most of their time working.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nParallel Speedup\nIdeally, parallelizing a serial code makes it run P times\nfaster on P processors.\n&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\nSerial matrix multiply\nRunning time TS.\nCilk matrix multiply\nWith sufficient\nparallelism, running\ntime is TP ! T1/P.\nGoal: TP ! TS/P,\nmeaning that TS ! T1.\n!\"#$%&'( )\"*+ \" , -. \" / *. 00\"1\n&'( )\"*+ $ , -. $ / *. 00$1\n&'( )\"*+ 2 , -. 2 / *. 0021\n34\"5425 0, 64\"54$5 7 84$5425.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWork Efficiency\nLet TS denote the work of a serial program.\nSuppose the serial program is parallelized.\nLet T1 denote the work of the parallel\nprogram, and let Tinf denote the span of the\nparallel program.\nTo achieve linear speedup on P processors\nover the serial program, i.e., TP ≈ TS/P, the\nparallel program must exhibit:\n- Ample parallelism: T1/Tinf≫ P.\n- High work efficiency: TS/T1 ≈ 1.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nThe Work-First Principle\nTo optimize the execution of programs with\nsufficient parallelism, the implementation of\nthe Cilk runtime system works to maintain high\nwork-efficiency by abiding by the work-first\nprinciple:\nOptimize for the ordinary serial\nexecution, at the expense of some\nadditional computation in steals.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDivision of Labor\nThe work-first principle guides the division of\nthe Cilk runtime system between the compiler\nand the runtime library.\nCompiler\n- Uses a handful of small data structures, e.g.,\nworkers and stack frames.\n- Implements optimized fast paths for\nexecution of functions when no steals have\noccurred.\nRuntime library\n- Uses larger data structures.\n- Handles slow paths of execution, e.g., when a\nsteal occurs.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nIMPLEMENTING A WORKER\nDEQUE\n\nRunning Example\n!\"# $%%&!\"# \"' (\n!\"# )* +,\n) - .!/012345\" 647&\"',\n+ - 648&\"',\n.!/012+\".,\n79#:7\" ) ; +,\n<\n- Function $%% is a spawning function,\nmeaning that $%% contains a .!/012345\".\n- Function 647 is spawned by $%%.\n- The call to 648 occurs in the continuation of\nthe spawn.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRequirements of Worker Deques\nPROBLEM: How do we implement a worker's deque?\nspawned\np\ncalled\ncalled\ncalled\nspawned\nspawned\ncalled\nspawned\ncalled\ncalled\nspawned\ncalled\np\nspawned\nspawned\nspawned\nspawned\np\ncalled\ncalled\nspawned\np\ncalled\ncalled\ncalled\ncalled\nspawned\np\ncalled\ncalled\nspawned\nspawned\nspawned\nspawned\nP\nP\nP\nP\n- The worker should operate its own deque like a\nstack.\n- A steal needs to transfer ownership of several\nconsecutive frames to a thief.\n- A thief needs to be able to resume a continuation.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nBasic Worker-Deque Design\nIDEA: The worker deque is an\nexternal structure with\npointers to stack frames.\n!\nA Cilk worker maintains\nhead and tail pointers to\nits deque.\n!\nStealable frames maintain\na local structure to store\ninformation necessary for\nstealing the frame.\nDesign\nspawned\np\ncalled\ncalled\ncalled\nspawned\np\nspawned\nDeque\nframe\nframe\nframe\nframe\nframe\nframe\nCall Stack\nCilk worker\n!\"#$\n%#&'\nConcept\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nImplementation Details\nThe Intel Cilk Plus runtime elaborates on this\nidea as follows:\n- Every spawned subcomputation runs in its\nown spawn-helper function.\n- The runtime maintains three basic data\nstructures as workers execute work:\n- A worker structure for every worker used\nto execute the program.\n- A Cilk stack-frame structure for each\ninstantiation of a spawning function.\n- A spawn-helper stack frame for each\ninstantiation of a cilk_spawn.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSpawn-Helper Functions\nCilk code\nC pseudocode of\ncompiled result\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n!\"# )* +,\n) - .!/012345\" 647&\"',\n+ - 648&\"',\n.!/012+\".,\n79#:7\" ) ; +,\n<\n!\"# $%%&!\"# \"' (\n11.!/07#212#4.01$74=91# 2$,\n11.!/07#219\"#971$74=9&>2$',\n!\"# )* +,\n!$ &?29#@=3&2$A.#)''\n2345\"1647&>)* \"',\n+ - 648&\"',\n!$ &2$A$/4B2 > CDEF1GHIJK1LMNOMCPKQ'\n!$ &?29#@=3&2$A.#)''\n11.!/07#213%31$74=9&>2$',\n11.!/07#212+\".&>2$',\n!\"# 792:/# - ) ; +,\n!$ &2$A$/4B2'\n11.!/07#21/94R91$74=9&>2$',\n79#:7\" 792:/#,\n<\nR%!S 2345\"1647&!\"# T)* !\"# \"' (\n11.!/07#212#4.01$74=9 2$,\n11.!/07#219\"#971$74=91$42#&>2$',\n11.!/07#21S9#4.U&',\nT) - 647&\"',\n11.!/07#213%31$74=9&>2$',\n11.!/07#21/94R91$74=9&>2$',\n<\nCilk\ncompiler\n!\"# $%%&!\"# \"' (\n!\"# $%%&!\"# \"' (\n) - .!/012345\" 647&\"',\nR%!S 2345\"1647&!\"# T)* !\"# \"' (\n\nCilk Stack-Frame Structures\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n))*!+,-#.).#/*,)$-/01)# .$2\n))*!+,-#.)1\"#1-)$-/01&3.$'2\n!\"# 45 62\n!$ &7.1#809&.$:*#4''\n.9/;\")</-&345 \"'2\n6 = </>&\"'2\n!$ &.$:$+/?. 3 @ABC)DEFGH)IJKLJ@MHN'\n!$ &7.1#809&.$:*#4''\n))*!+,-#.).6\"*&3.$'2\n!\"# -1.O+# = 4 P 62\n))*!+,-#.)9%9)$-/01&3.$'2\n!$ &.$:$+/?.'\n))*!+,-#.)+1/Q1)$-/01&3.$'2\n-1#O-\" -1.O+#2\nR\nQ%!S .9/;\")</-&!\"# T45 !\"# \"' (\n))*!+,-#.).#/*,)$-/01 .$2\n))*!+,-#.)1\"#1-)$-/01)$/.#&3.$'2\n))*!+,-#.)S1#/*U&'2\nT4 = </-&\"'2\n))*!+,-#.)9%9)$-/01&3.$'2\n))*!+,-#.)+1/Q1)$-/01&3.$'2\nR\n))*!+,-#.).#/*,)$-/01)# .$2\n))*!+,-#.).#/*,)$-/01 .$2\nCilk stack-frame\nstructures\nC pseudocode of\ncompiled result\n\nThe Cilk Stack Frame (Simplified)\nEach Cilk stack frame stores:\n- A context buffer, !\"#,\nwhich contains enough\ninformation to resume a\nfunction at a continuation,\ni.e., after a !$%&'()*+, or\n!$%&'(-,!.\n- An integer, .%*/(, that\nsummarizes the state of\nthe Cilk stack frame.\n- A pointer, )*01,\", to its\nparent Cilk stack frame.\nCilk stack frame\n!\"#\n)*01,\"\n.%*/(\nCilk stack frame\nCilk stack frame\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nThe Cilk Worker Structure (Simplified)\nWorker's call stack\nEach Cilk worker maintains:\n- A deque of stack frames that\ncan be stolen.\n- A pointer to the current stack\nframe.\nDeque\nExample:\nFunction %&&\nspawned +\",,\nwhich called\n4..5, which\nspawned\n%,/2.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$\n%&&\n%&&'(%\n()\"*$'+\",\n()\"*$'+\",'(%\n+\",\nCilk worker\n-.,,/$0'(%\n1/\"2\n0\"#3\n4..5\n4..5'(%\n()\"*$'%,/2\n()\"*$'%,/2'(%\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSPAWNING COMPUTATION\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n))*!+,-#.).#/*,)$-/01)# .$2\n))*!+,-#.)1\"#1-)$-/01&3.$'2\n!\"# 45 62\n!$ &7.1#809&.$:*#4''\n.9/;\")</-&345 \"'2\n6 = </>&\"'2\n!$ &.$:$+/?. 3 @ABC)DEFGH)IJKLJ@MHN'\n!$ &7.1#809&.$:*#4''\n))*!+,-#.).6\"*&3.$'2\n!\"# -1.O+# = 4 P 62\n))*!+,-#.)9%9)$-/01&3.$'2\n!$ &.$:$+/?.'\n))*!+,-#.)+1/Q1)$-/01&3.$'2\n-1#O-\" -1.O+#2\nR\n))*!+,-#.).#/*,)$-/01)# .$2\n))*!+,-#.)1\"#1-)$-/01&3.$'2\n!\nCreate and initialize a Cilk\nstack-frame structure.\n7.1#809&.$:*#4'\nPrepare to spawn.\n.9/;\")</-&345 \"'2\nInvoke the spawn helper.\n!$ &.$:$+/?. 3 @ABC)DEFGH)IJKLJ@MHN'\n!$ &7.1#809&.$:*#4''\n))*!+,-#.).6\"*&3.$'2\nPerform a sync.\n))*!+,-#.)9%9)$-/01&3.$'2\nClean up the Cilk\nstack-frame structure.\n!$ &.$:$+/?.'\n))*!+,-#.)+1/Q1)$-/01&3.$'2\nClean up the deque.\nCode for a Spawning Function\nC pseudocode of a spawning function\n\nCode for a Spawn Helper\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'()*+',-#). /01 #). )2 3\n**4#56,.%*%.'46*7,'89 %7:\n**4#56,.%*9).9,*7,'89*7'%.-;%72:\n**4#56,.%*$9.'4<-2:\n/0 = +',-)2:\n**4#56,.%*&\"&*7,'89-;%72:\n**4#56,.%*59'!9*7,'89-;%72:\n>\nC pseudocode of a spawn helper\n**4#56,.%*%.'46*7,'89 %7:\n**4#56,.%*9).9,*7,'89*7'%.-;%72:\nCreate and initialize\na Cilk stack-frame\nstructure.\n**4#56,.%*$9.'4<-2:\n/0 = +',-)2:\n**4#56,.%*&\"&*7,'89-;%72:\nUpate the deque to allow\nthe parent to be stolen.\n**4#56,.%*59'!9*7,'89-;%72:\nClean up the Cilk\nstack-frame structure.\nClean up the deque\nand attempt to return.\nInvoke the spawned subroutine.\n\nEntering a Spawning Function\nWhen execution enters a spawning function, the\nCilk worker's current stack-frame structure is\nupdated.\n!\"#$\n%&&\n%&&'(%\nCilk worker\n)*++,$-'(%\n.,\"/\n-\"#0\nCall stack\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPreparing to Spawn\nCilk code\nCilk uses the +,#-./\nfunction to allow thieves to\nsteal the continuation.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\nC pseudocode\n+,#-./&+$01#2'\n!\"# $%%&!\"# \"' (\n)\n2 < 1!=>5+/34\" 637&\"':\n)\n;\nQUESTION: What information\nneeds to be saved?\nANSWER: Registers\n?7!/, ?76/, ?7+/, and\ncallee-saved registers.\nThe +,#-./ function stores\ninformation necessary for\nresuming the function at the\n+,#-./ into the given buffer.\n\nSpawning a Function\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\n<%!= +/34\"5637&!\"# >29 !\"# \"' (\n551!?@7#+5+#31@5$73., +$:\n551!?@7#+5,\"#,75$73.,5$3+#&8+$':\n551!?@7#+5=,#31A&':\n>2 B 637&\"':\n)\n;\n.3!\"\n$%%\n$%%5+$\nCilk worker\n1C77,\"#5+$\nA,3=\n#3!?\nCall stack\nC pseudocode\n+/34\"5637\n+/34\"56375+$\n+/34\"5637&829 \"':\n\nSpawning a Function\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\n<%!= +/34\"5637&!\"# >29 !\"# \"' (\n551!?@7#+5+#31@5$73., +$:\n551!?@7#+5,\"#,75$73.,5$3+#&8+$':\n551!?@7#+5=,#31A&':\n>2 B 637&\"':\n)\n;\n.3!\"\n$%%\n$%%5+$\nCilk worker\n1C77,\"#5+$\nA,3=\n#3!?\nCall stack\nC pseudocode\n+/34\"5637\n+/34\"56375+$\n551!?@7#+5,\"#,75$73.,5$3+#&8+$':\n\nSpawning a Function\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\n<%!= +/34\"5637&!\"# >29 !\"# \"' (\n551!?@7#+5+#31@5$73., +$:\n551!?@7#+5,\"#,75$73.,5$3+#&8+$':\n551!?@7#+5=,#31A&':\n>2 B 637&\"':\n)\n;\n.3!\"\n$%%\n$%%5+$\nCilk worker\n1C77,\"#5+$\nA,3=\n#3!?\nCall stack\nC pseudocode\n+/34\"5637\n+/34\"56375+$\n551!?@7#+5=,#31A&':\n\nSpawning a Function\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\n<%!= +/34\"5637&!\"# >29 !\"# \"' (\n551!?@7#+5+#31@5$73., +$:\n551!?@7#+5,\"#,75$73.,5$3+#&8+$':\n551!?@7#+5=,#31A&':\n>2 B 637&\"':\n)\n;\n.3!\"\n$%%\n$%%5+$\nCilk worker\n1C77,\"#5+$\nA,3=\n#3!?\nCall stack\nC pseudocode\n+/34\"5637\n+/34\"56375+$\n>2 B 637&\"':\n\nReturning from a Spawn\nDeque\n!\"#$ %&'()*+',-#). /01 #). )2 3\n/0 5 +',-)26\n**7#89,.%*&\"&*:,';<-=%:26\n**7#89,.%*8<'!<*:,';<-=%:26\n>\n;'#)\n:\"\"\n:\"\"*%:\nCilk worker\n7?,,<).*%:\n@<'$\n.'#8\nCall stack\nC pseudocode\n%&'()*+',\n%&'()*+',*%:\n+',\n**7#89,.%*&\"&*:,';<-=%:26\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReturning from a Spawn\nDeque\n!\"#$ %&'()*+',-#). /01 #). )2 3\n/0 5 +',-)26\n**7#89,.%*&\"&*:,';<-=%:26\n**7#89,.%*8<'!<*:,';<-=%:26\n>\n;'#)\n:\"\"\n:\"\"*%:\nCilk worker\n7?,,<).*%:\n@<'$\n.'#8\nCall stack\nC pseudocode\n%&'()*+',\n%&'()*+',*%:\n**7#89,.%*&\"&*:,';<-=%:26\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nReturning from a Spawn\nDeque\nCall stack\nC pseudocode\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'()*+',-#). /01 #). )2 3\n/0 5 +',-)26\n**7#89,.%*&\"&*:,';<-=%:26\n**7#89,.%*8<'!<*:,';<-=%:26\n>\n;'#)\n:\"\"\n:\"\"*%:\nCilk worker\n7?,,<).*%:\n@<'$\n.'#8\n%&'()*+',\n%&'()*+',*%:\n**7#89,.%*8<'!<*:,';<-=%:26\nMay or may not return,\ndepending on what's in\nthe worker's deque.\n\nPopping the Deque\nIn __cilkrts_leave_frame, the worker tries to\npop the stack frame from the tail of the deque.\nThere are two possible outcomes:\n1. If the pop succeeds, then the execution\ncontinues as normal.\n2. If the pop fails, then the worker is out of\nwork to do. It thus becomes a thief and\ntries to steal work from the top of a random\nvictim's deque.\nQuestion: Which case\nis more important to\noptimize?\nAnswer: Case 1.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSTEALING COMPUTATION\n\nRecall: Stealing Work\nConceptually, a thief takes frames off of the\ntop of a victim worker's deque.\nP\nspawned\np\ncalled\ncalled\ncalled\nspawned\nP\nspawned\nP\nP\np\ncalled\nspawned\np\ncalled\ncalled\nspawned\np\ncalled\np\nspawned\nspawned\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStealing a Frame\nA thief steals from the head of the victim\nworker's deque.\nDeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$\n%&&\n%&&'(%\nCilk worker\n)*++,$-'(%\n.,\"/\n-\"#0\nVictim's\nCall stack\n(1\"2$'3\"+\n(1\"2$'3\"+'(%\nDeque\n3\"+\n)*++,$-'(%\nVictim\nCilk worker\n.,\"/\n-\"#0\ndeque!\nNeed to handle\nThief\nconcurrent\naccesses to the\n.,\"/\n\nSynchronizing Deque Accesses\nWorker\nThe worker and thief\nprotocol\ncoordinate operations\non the deque using\nthe THE protocol:\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'()* +\n,-#.//0\n2\"\". %\"%)* +\n,-#.330\n#4 )(5-$ 6 ,-#.* +\n,-#.//0\n.\"78)9*0\n,-#.330\n#4 )(5-$ 6 ,-#.* +\n,-#.//0\n&:.\"78)9*0\n;5,&;: <=>9?@A0\n&:.\"78)9*0\n;5,&;: B?CCABB0\n2\"\". ',5-.)* +\n.\"78)9*0\ndeque\n(5-$//0\n#4 )(5-$ 6 ,-#.* +\n(5-$330\n&:.\"78)9*0\n;5,&;: <=>9?@A0\n&:.\"78)9*0\n;5,&;: B?CCABB0\nThief protocolThe thief\nalways grabs\na lock before\noperating on\nthe deque.\n,-#.330\nThe worker only grabs\na lock if the deque\nappears to be empty.\n%\"\nThe worker\npops the\noptimistically.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n1>77,\"#5+$\nResuming a Continuation\n!\"# $%%&!\"# \"' (\n)\n!$ &*+,#-./&+$01#2''\n+/34\"5637&829 \"':\n)\n;\nC pseudocode\n+,#-./&+$01#2'\nCilk uses the <%\"=-./ function to resume a\nstolen continuation.\n.3!\"\n$%%\nVictim's\nCall stack\nCilk worker\n?,3@\n#3!<\nDeque\nThief\nPreviously, the victim\nperformed a +,#-./ to store\nregister state in $%%5+$01#2.\n$%%5+$\nExecuting <%\"=-./&1>77,\"#5+$AB1#29C'\nsets the thief's registers to start\nexecuting at the location of the +,#-./.\n\nResuming a Continuation\nThe contract between +,#-./@and <%\"=-./@\nensures the thief resumes the continuation.\n- On its direct invocation, +,#-./&6>$$,7'\nreturns ?.\n- Invoking <%\"=-./&6>$$,79@2'@causes the\n+,#-./@to effectively return again with the\ninteger value 2.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#@$%%&!\"#@\"'@(@\n)@\n!$@&*+,#-./&+$01#2''@\n+/34\"5637&829@\"':@\n)@\n;@\nC pseudocode\n+,#-./&+$01#2'\nBecause a thief reaches this\npoint by calling\n<%\"=-./&1>77,\"#5+$AB1#29C',\nthe condition fails, and the thief\njumps to the continuation.\n\nImplementing the Cactus Stack\nThieves maintain their own call stacks and use\npointer tricks to implement the cactus stack.\nThief's !\"-$\nThief's !\"-$\n!\"#$\nThief's call stack\nExample: A thief steals\nthe continuation of\n%&&, and then calls #'(.\n)'*+\n%&&\n%&&,-%\n-$'.+,#'\"\n-$'.+,#'\",-%\n#'\"\nVictim's call stack\nThief's !\"#$\n#'(\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSYNCHRONIZING\nCOMPUTATION\n\nRecall: Nested Synchronization\n!\"#&$!%&'!\"#&\"(&)&\n!$&'\"&*&+(&,-#.,\"&\"/&\n-01-&)&\n!\"#&23&4/&\n2&5&6!07819:;\"&$!%'\"<=(/&\n4&5&$!%'\"<+(/&\n6!07814\"6/&\n,-#.,\"&2&>&4/&\n?&\n?&\nExample:\n$!%'@(&\n!\n\"\n#\n#\n$\n$\n$\n%\n%\n%P2\n% P1\n/&\n:\n(\n/&\n;\n/\nWaiting on\nP1 to sync!\nSynchronization happens\nin a nested fashion.\nP3\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSynchronization Concerns\nIf a worker reaches a cilk_sync before all\nspawned subcomputations are complete, the\nworker should become a thief, but the\nworker's current function frame should not\ndisappear!\n- The existing subcomputations might access\nstate in that frame, which is their parent\nframe.\n- In the future, another worker must resume\nthat frame and execute the cilk_sync.\n- The cilk_sync only applies to nested\nsubcomputations of the frame, not to all\nsubcomputations or workers.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFull-Frame Tree\nThe Cilk runtime maintains a tree of full frames,\nwhich stores state for parallel subcomputations.\n(c) 2008-2018 by the MIT 6.172 Lecturers\nP\nP\nP\nP\nspawned\np\ncalled\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\np\ncalled\nProcessors\nwork on active\nfull frames.\nOther full\nframes are\nsuspended.\nA full frame keeps\ntrack of its parent\nand child frames.\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nP\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nP\nP\nSteal!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nThe thief steals the\nfull frame and\ncreates a new full\nframe for the victim.\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nP\nSteal!\nP\nP\nP\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\n(c) 2008-2018 by the MIT 6.172 Lecturers\nThe thief steals the\nfull frame and\ncreates a new full\nframe for the victim.\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nSteal!\nframes.\nP\nP\nP\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nP\nP\nP\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nSteal!\n*Full-frame illustrations resized for cleanliness.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nP\nP\nP\nP\nSteal!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nP\nP\nP\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nSteal!\nBy stealing the full\nframe, existing\npointers to parent full\nframe are preserved.\n*Full-frame illustrations resized for cleanliness.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nP\nP\nP\nP\n*Full-frame illustrations resized for cleanliness.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nP\nP\nSteal!\nP\nP\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMaintaining Full Frames\nLet's see how steals can produce a tree of full\nframes.\nP\nP\nP\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nSteal!\nspawned\np\ncalled\n*Full-frame illustrations resized for cleanliness.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSuspending Full Frames\n(c) 2008-2018 by the MIT 6.172 Lecturers\nP\nP\nP\nP\nSync?\nThe full frame\ncannot sync because\nof the running child\nsubcomputation.\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\n\nSuspending Full Frames\n(c) 2008-2018 by the MIT 6.172 Lecturers\nP\nP\nP\nP\nSuspend!\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\n\nSuspending Full Frames\n(c) 2008-2018 by the MIT 6.172 Lecturers\nP\nP\nP\nP\nSteal!\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\n\nCommon Case for Sync\nQUESTION: If the program has ample parallelism,\nwhat do we expect will typically happen when\nthe program execution reaches a cilk_sync?\nANSWER: The executing function contains no\noutstanding spawned children.\nHow does the\nruntime optimize\nfor this case?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nStack Frames and Full Frames\nP\nP\nP\nP\nspawned\np\ncalled\nspawned\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nspawned\np\ncalled\nEvery full frame is\nassociated with a\nCilk stack frame.\nThe flags field in\nthe Cilk stack frame\nmaintains the full\nframe's status.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCompiled Code for Sync\nCilk code\nC pseudocode\n!\"# $%%&!\"# \"' (\n)\n!$ &*$+$,-.* /\n01234567894:;<=;0>9?'\n!$ &@*A#BCD&*$+E#F''\n44E!,GH#*4*I\"E&/*$'J\n)\nK\n!\"# $%%&!\"# \"' (\n!\"# FL IJ\nF M E!,G4*D-N\" O-H&\"'J\nI M O-P&\"'J\nE!,G4*I\"EJ\nHA#QH\" F R IJ\nK\n!$ &*$+$,-.* /\n01234567894:;<=;0>9?'\n!$ &@*A#BCD&*$+E#F''\n44E!,GH#*4*I\"E&/*$'J\nE!,G4*I\"EJ\nThe code compiled to implement a E!,G4*I\"E\nchecks the flags field before performing an\nexpensive call to 44E!,GH#*4*I\"E in the Cilk\nruntime library.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMore Cilk Runtime Features\nThe Cilk runtime system implements many\nother features and optimizations:\n- Schemes for making the full-frame tree\nsimpler and easier to maintain.\n- Data structure and protocol enhancements\nto support C++ exceptions.\n- Sibling pointers between full frames to\nsupport reducer hyperobjects.\n- Pedigrees to assign a unique, deterministic\nID to each strand efficiently in parallel.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTHE TAPIR COMPILER\n[SML17]\n\nCompilation Pipeline\nLLVM\noptimizer\n!\"##$%$\"&\n'\"(#)*%\n!\"##$%$\"&\n'\"(#)*##\nLLVM code\ngenerator\nAssembly\nPreprocessed\nsource\n!\"##$%$\"&\n'\"(#)*+\nC source\nClang pre-\nprocessor\n!\"##$%$\"&\n'\"(#)*$\nClang code\ngenerator\n!\"##$%$\"&\n'\"(#)*##\nLLVM IR\nOptimized\nLLVM IR\nQuestion: When do +$#,-%./0&, +$#,-%1&+, and\n+$#,-2\"( get compiled?\nTraditional\nanswer.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nExample: Normalize\n!!\"##$%&'#(!!))*+,-#..\n/+'&0( ,+$1)*+,-# /+'&0( 234 %,# ,.5\n6+%/ ,+$1\"0%7()/+'&0( 2$(-#$%*# 84\n*+,-# /+'&0( 2$(-#$%*# 34\n%,# ,. 9\n:+$ )%,# % ; <5 % = ,5 >>%.\n8?%@ ; 3?%@ A ,+$1)34 ,.5\nB\nTest: Random vector of n=64M elements\nMachine: AWS c4.8xlarge Compiler: GCC 6.2\nRunning time: TS = 0.312 s\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nOptimizing the Serial Code\n!!\"##$%&'#(!!))*+,-#..\n/+'&0( ,+$1)*+,-# /+'&0( 234 %,# ,.5\n6+%/ ,+$1\"0%7()/+'&0( 2$(-#$%*# 84\n*+,-# /+'&0( 2$(-#$%*# 34\n%,# ,. 9\n:+$ )%,# % ; <5 % = ,5 >>%.\n8?%@ ; 3?%@ A ,+$1)34 ,.5\nB\n,+$1)34 ,.\n.\n%\n.\nGCC can move the\ncall to ,+$1 out of\nthe serial loop.\nWork before hoisting:\nT(n) =\nWork after hoisting:\nT(n) =\n%,# ,.5\n%,# ,.5\nThe ,+$1 function\nperforms !(n) work.\n!(n2)\n!(n)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"#$ %\"&'()#*+9E+)F+&,01&-21 (&D091 (&D04 #%1 #6 7\n$\"-.)+ /3 ; (&D0G3=\n$\"-.)+ /5 ; (&D0G5=\n#%1 % ; (&D0G%=\n!\"#$ %\"&'()#*+9E+)F+&,01&-21 (&D0 1 (&D0\n#%1 #6 7\n$\"-.)+ /3 ; (&D0G3=\n$\"-.)+ /5 ; (&D0G5=\n#%1 % ; (&D0G%=\nGCC Compiling Cilk Code\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCilk\ncompiler\n!\"#$ %\"&'()#*+,$\"-.)+ /&+01&#21 34\n2\"%01 $\"-.)+ /&+01&#21 54 #%1 %6 7\n2#)89:\"& ,#%1 # ; <= # > %= ??#6\n3@#A ; 5@#A B %\"&',54 %6=\nC\n!\"#$ %\"&'()#*+,$\"-.)+ /&+01&#21 34\n2\"%01 $\"-.)+ /&+01&#21 54 #%1 %6 7\n01&-21 (&D091 (&D0 ; 7 34 54 % C=\n992#)8&1092#)89:\"&,%\"&'()#*+9E+)F+&4 (&D04 <4 %6=\nC\n3@#A ; 5@#A B %\"&',54 %6=\nC\nCilk code\nC\npseudo-\ncode\n2#)89:\"&\nCall into Cilk runtime\nlibrary to execute a\n2#)89:\"& loop.\n/\n$\n;\nHelper function\nencodes the loop body.\n\"#\n#$\n,\n%\"&',54\n%6\n01&-21 (&D0 1 (&D0\n#%1 #6 7\nThe compiler can't move\n%\"&' out of the loop.\n=\n\nPerformance of Parallel Normalize\n!!\"##$%&'#(!!))*+,-#..\n/+'&0( ,+$1)*+,-# /+'&0( 234 %,# ,.5\n6+%/ ,+$1\"0%7()/+'&0( 2$(-#$%*# 84\n*+,-# /+'&0( 2$(-#$%*# 34\n%,# ,. 9\n*%0:!;+$ )%,# % < =5 % > ,5 ??%.\n8@%A < 3@%A B ,+$1)34 ,.5\nC\nTest: Random vector of n=64M elements\nTerrible work efficiency!\nTS/T1 = 0.312/2600\n~ 1/8600\nThe ,+$1 function\nwas also parallelized.\nMachine: AWS c4.8xlarge\nCompiler: GCC 6.2\nRunning time of serial code: TS = 0.312 s\n18-core running time: T18 = 180.657 s\n1-core running time: T1 = 2600.287 s\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTapir\nTapir's Compilation Pipeline\nTapir embeds fork-join parallelism into LLVM's IR.\nLLVM IR\nPreprocessed\nC source\nsource\nAssembly\n!\"##$%$\"&\n'\"(#)*+\nClang pre-\nprocessor\n!\"##$%$\"&\n'\"(#)*$\nClang code\ngenerator\nLLVM\noptimizer\n!\"##$%$\"&\n'\"(#)*##\n!\"##$%$\"&\n'\"(#)*%\n!\"##$%$\"&\n'\"(#)*##\nLLVM code\ngenerator\nLLVM IR\nTapir\nWith minimal code\nchanges, LLVM can\noptimize Tapir.\nOptimized\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nUnsafe Optimizations\nProblem: There are many examples of\noptimizations on serial code that cannot be\nsafely applied to parallel code [MP90].\nCilk code\n!\"#$<%\"\"&#'(<')<*<\n+#,-.%\"/<&#'(<#<0<12<#<3<'2<44#)<\n56/&78#)2<\n9<\n!\"#$<%\"\"&#'(<')<*<\n#'(<(:;<0<12<\n+#,-.%\"/<&#'(<#<0<12<#<3<'2<44#)<*<\n56/&(:;)2<\n(:;<40<72<\n9<\n9<\nIncorrectly\noptimized\nCilk code\nUnleashing LLVM\non parallel\nprograms requires\nsome care.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nA Tapir CFG\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#@$%%&!\"#@\"'@(@\n!\"#@)*@+,@\n)@-@.!/012345\"@647&\"',@\n+@-@648&\"',@\n.!/012+\".,@\n79#:7\"@)@;@+,@\n<@\n9)!#@\n=9#@\n+@-@>648&'@\n2+\".@9)!#@\n)?@-@/%4=@)@\n79#@)?@;@+@\nTapir control-flow graph\n9\"#7+@\n.%\"#@\n)@-@4//%.4&'@\n=9#4.A@=9#*@.%\"#@\n)B@-@>647&'@\n2#%79@)*@)B@\n794##4.A@.%\"#@\nTapir uses an\nasymmetric\nrepresentation of\nthe parallel tasks\nin the CFG.\nSpawned task\nContinuation\nTapir adds three\nconstructs to LLVM's IR:\n=9#4.A, 794##4.A, and\n2+\"..\n\nSerial Elision\nTapir CFG\nThis asymmetry models\n,*/5!1\nthe program's serial\nelision.\n4,/1\nCFG of serial elision\n,*/5!1\n,-./1\n+3*/1\n,-./1\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!1\"1#$%&'(1\n)!*+1,-./1\n-01\"123%41-1\n5,/1-0161!1\n-1\"1%223+%'(1\n4,/%+714,/81+3*/1\n-91\"1#$%5'(1\n)/35,1-81-91\n5,%//%+71+3*/1\n!1\"1#$%&'(1\n$51,-./1\n-01\"123%41-1\n-91\"1#$%5'(1\n5,/1-0161!1\n-1\"1%223+%'(1\n$514,/1\n)/35,1-81-91\n$51+3*/1\n+3*/1\n4,/1\n4,/%+714,/81+3*/\n5,%//%+71+3*/\n)!*+1,-./\n$5 4,/\n$5 +3*/\n$5 ,-./\n%5'\nIf the program contains no\ndeterminacy races, then it\nis semantically equivalent\nto its serial projection.\n\nParallel Loops in Tapir\nIn Tapir, parallel loops look similar to serial\nloops, with some differences due to parallelism.\nTapir %.!8>-1B\")CFG\nSerial %.!8>-1B\")CFG\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$!%)\n&!)'()*)%+,)-../,)\"01#)\n1()2)3'4(,\"%#!56,417,-../6+)\n%.!8()2)%.!8'9,)%+)\n:41(6)2)941(6);)%.!8()\n17)2)1()<)7)\n&!)'17)*)%+,)-../,)\"01#)\n17)2)1()<)7)\n&!)'17)*)%+,)=\">?\"!,)@5%A)\n1()2)3'4(,\"%#!56,417,->#A=6+)\n!\"#$%& &.?5,)->#A=)\n'()% \"01#)\n&!)'()*)%+,)=\">?\"!,)\"01#)\n%.!8( 2 %.!8'9,)%+\n:41(6)2)941(6); %.!8(\n17 2 1( < 7\n&! '17 * %+,)-../,)\"01#\n17 2 1( < 7\n&! '17 *)%+,)=\">?\"!,)@5%A\n1( 2 3'4(,\"%#!56,417,-../6+\n1( 2 3'4(,\"%#!56,417,->#A=6+\n%.!8()2)%.!8'9,)%+)\n:41(6)2)941(6);)%.!8()\n*\"$##$%& ->#A=)\n%.!8( 2 %.!8'9,)%+\n:41(6)2)941(6);)%.!8(\n!\"#$!%)\n\nImpact on LLVM\nLLVM can reason about a Tapir CFG as a relatively minor\nchange to the CFG of the serial elision.\n-\nMany standard compiler analyses required no\nchanges.\n-\nMemory analysis required a minor change to handle\nTapir's constructs (~450 lines of code).\n-\nSome optimizations, e.g., code hoisting and tail-\nrecursion elimination, required some changes to work\non Tapir CFG's.\nIn total, implementing Tapir involved adding or\nmodifying ~6000 lines of LLVM's 4-million-line\ncodebase.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nParallelize Normalize with Tapir\n!!\"##$%&'#(!!))*+,-#..\n/+'&0( ,+$1)*+,-# /+'&0( 234 %,# ,.5\n6+%/ ,+$1\"0%7()/+'&0( 2$(-#$%*# 84\n*+,-# /+'&0( 2$(-#$%*# 34\n%,# ,. 9\n*%0:!;+$ )%,# % < =5 % > ,5 ??%.\n8@%A < 3@%A B ,+$1)34 ,.5\nC\nGood work efficiency:\nTS/T1 = 97%\nTest: Random vector of n=64M elements\nMachine: AWS c4.8xlarge Compiler: Tapir/LLVM\nRunning time of serial code: TS = 0.312 s\n1-core running time: T1 = 0.321 s\n18-core running time: T18 = 0.081 s\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWork-Efficiency Improvement\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSame as Tapir/LLVM except that\nCilk constructs are compiled early.\nTapir/LLVM doesn't fix everything,\nbut it helps parallel programs\nachieve good work efficiency.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTS\nT1\nIdeal\nefficiency\nReference\nTapir/LLVM\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCASE STUDY: OPENMP\n\nExample: OpenMP Normalize\n!!\"##$%&'#(!!))*+,-#..=\n/+'&0(=,+$1)*+,-#=/+'&0(=234=%,#=,.5=\n6+%/=,+$1\"0%7()/+'&0(=2$(-#$%*#=84=\n*+,-#=/+'&0(=2$(-#$%*#=34=\n%,#=,.=9=\n:;$\"<1\"=+1;=;\"$\"00(0=>+$=\n>+$=)%,#=%=?=@5=%=A=,5=BB%.=\n8C%D=?=3C%D=E=,+$1)34=,.5=\nF=\nTest: Random vector of n=64M elements\nMachine: AWS c4.8xlarge\nCompiler: GCC 6.2\nRunning time of serial code: TS = 0.312 s\n1-core running time: T1 = 0.329 s\n18-core running time: T18 = 0.205 s\nGood work\nefficiency\nwithout Tapir?\nParallel\nspeedup is\nnot great.\nThe ,+$1=function\nwas also parallelized.\nM\nCo\nT\nWhy do we\nget this\nperformance?\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nE;\n!\"#$;%\"&'()#*+,$\"-.)+;/&+01&#21;\n2\"%01;$\"-.)+;\nGCC Compiling OpenMP Code\nOpenMP !\"#$;%\"&'()#*+,$\"-.)+;/&+01&#21;34\ncode\n2\"%01;$\"-.)+;/&+01&#21;54;#%1;%6;7;\n89&(:'(;\"'9;9(&())+);<\"&;\n89&(:'(;\"'9;9(&())+);<\"&;\n(c) 2008-2018 by the MIT 6.172 Lecturers\nOpenMP\ncompiler\n<\"&;,#%1;#;=;>?;#;@;%?;AA#6;\n3B#C;=;5B#C;D;%\"&',54;%6?;\n34;\n/&+01&#21;54;#%1;%6;7\nFFG'92F<\"&GF2()),\"'9F\"-1)#%+$4;%4;34;56?;\nE;\n!\"#$;\"'9F\"-1)#%+$,#%1;%4;$\"-.)+;/&+01&#21;34;\n2\"%01;$\"-.)+;/&+01&#21;56;7;\n#%1;)\"2()F%;=;%?;$\"-.)+;/)\"2()F3;=;34;/)\"2()F5;=;5?;\nFFG'92F<\"&F01(1#2F#%#1,H)\"2()F%4;H)\"2()F34;H)\"2()F56?;\n$\"-.)+;1'9;=;%\"&',54;%6?;\n<\"&;,#%1;#;=;>?;#;@;)\"2()F%?;AA#6;\n)\"2()F3B#C;=;)\"2()F5B#C;D;1'9?;\nFFG'92F<\"&F01(1#2F<#%#,6?;\nE;\n$\"-.)+;1'9;= %\"&',54;%6?;\n<\"& ,#%1 # = >?;#;@\nF\n)\"2()F%?;AA 6\n#6;\n)\"2()F3B#C;=;)\"2()F5B#C;D;1'9?\n8 by the MIT 6 172 Lecturers\nFFG'92F<\"&F01(1#2F<#%#,6?\nE\n8 bE\n8 b\nThe helper function's\nloop on n/P iterations\ncan be optimized.\n%1\n/)\n()\nEach processor invokes\nthis helper method on\nn/P iterations.\n+\n#\n\nAnalysis of OpenMP Normalize\n!\"#$?%\"&'()#*+,$\"-.)+?/&+01&#21?34?\n2\"%01?$\"-.)+?/&+01&#21?54?#%1?%6?7?\n889':28;\"&982()),\"':8\"-1)#%+$4?%4?34?56<?\n=?\n!\"#$?\"':8\"-1)#%+$,#%1?%4?$\"-.)+?/&+01&#21?34?\n2\"%01?$\"-.)+?/&+01&#21?56?7?\n#%1?)\"2()8%?>?%<?$\"-.)+?/)\"2()83?>?34?/)\"2()85?>?5<?\n889':28;\"&801(1#28#%#1,@)\"2()8%4?@)\"2()834?@)\"2()856<?\n$\"-.)+?1':?>?%\"&',54?%6<?\n;\"&?,#%1?#?>?A<?#?B?)\"2()8%<?CC#6?\n)\"2()83D#E?>?)\"2()85D#E?F?1':<?\n889':28;\"&801(1#28;#%#,6<?\n=?\nThe %\"&'?function\nperforms !(n) work.\n;\"&?,#%1 # > A<?#?B\n)\"2()8%<?CC 6\n#6?\n)\"2()83D#E?>?)\"2()85D#E?F 1':<?\n$\"-.)+?1':?> %\"&',54?%6<?\nThe variable )\"2()8%?is\napproximately n/P.\n+01&#21\n\"%01 $\n%<\nEach processor invokes\n\"':8\"-1)#%+$?on n/P\niterations.\nWork of omp_outlined:\nT(n) = !(n)\nTotal work on P processors:\nT(n) = !(Pn)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSummary of OpenMP Normalize\n!!\"##$%&'#(!!))*+,-#..=\n/+'&0(=,+$1)*+,-#=/+'&0(=234=%,#=,.5=\n6+%/=,+$1\"0%7()/+'&0(=2$(-#$%*#=84=\n*+,-#=/+'&0(=2$(-#$%*#=34=\n%,#=,.=9=\n:;$\"<1\"=+1;=;\"$\"00(0=>+$=\n>+$=)%,#=%=?=@5=%=A=,5=BB%.=\n8C%D=?=3C%D=E=,+$1)34=,.5=\nF=\nWork on P\nprocessors:\nT(n) = !(Pn)\n!\nThis code is only work efficient on 1\nprocessor.\n!\nThis code can never achieve more than\nminimal parallel speedup.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTakeaways\nThe work-first principle\nOptimize for ordinary serial\nexecution, at the expense of some\nadditional computation in steals.\nTwo more takeaways:\n!\nThink about the performance model for your\nprogram.\n!\nKnow what your parallel runtime system is\ndoing.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 14: Caching and Cache-Efficient Algorithms",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/329bfc6e1808c375afa517feb3c4c273_MIT6_172F18_lec14.pdf",
      "content": "6.172\nPerformance\nEngineering\nof Software\nSystems\nLECTURE 14\nCaching and Cache-\nEfficient Algorithms\nJulian Shun\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACHE HARDWARE\n\nMulticore Cache Hierarchy\nLevel\nSize\nAssoc. Latency\n(ns)\nMain\n128 GB\nLLC\n30 MB\nL2\n256 KB\nL1-d\n32 KB\nL1-i\n32 KB\n64 B cache blocks\nDRAM\nL1\ndata\nL1\ninst\nL1\ndata\nL1\ninst\nL1\ndata\nL1\ninst\nL2\nL2\nL2\nLLC (L3)\nP\nP\n!\nP\nMemory\nController\nNet-\nwork\nDRAM DRAM\n!\n!\n!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFully Associative Cache\n0x0000\n0x0004\n0x0008\n0x000C\n0x0010\n0x0014\n0x0040\n0x0018\nw-bit\n0x001C\n0x0024\n0x0020\n0x0014\ntag\naddress\n0x0024\n0x003C\n0x0028\n0x0030\nspace\n0x002C\n0x0030\n0x0008\n0x0034\n0x0038\n0x003C\n0x0040\n0x0044\n0x0048\nA cache block can reside\nanywhere in the cache.\nCache size M = 32.\nLine/block size B = 4.\nTo find a block in the cache, the entire cache must be\nsearched for the tag. When the cache becomes full, a\nblock must be evicted to make room for a new block.\nThe replacement policy determines which block to evict.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDirect-Mapped Cache\nw-bit\naddress\nspace\n0x0000\n0x0004\n0x0008\n0x000C\n0x0010\n0x0014\n0x0018\n0x001C\n0x0020\n0x0024\n0x0028\n0x002C\n0x0030\n0x0034\n0x0038\n0x003C\n0x0040\n0x0044\n0x0048\n0x0008\n0x0014\n0x0024\n0x0030\n0x003C\n0x0040\ntag\nA cache block's set determines\nits location in the cache.\nCache size M = 32.\nLine/block size B = 4.\ntag\nset\noffset\nw - lg M\nlg(M/B)\nlg B\nbits\n(c) 2008-2018 by the MIT 6.172 Lecturers\naddress\nTo find a block in the\ncache, only a single\nlocation in the cache\nneed be searched.\n\nSet-Associative Cache\nw-bit\naddress\nspace\n0x0000\n0x0004\n0x0008\n0x000C\n0x0010\n0x0014\n0x0018\n0x001C\n0x0020\n0x0024\n0x0028\n0x002C\n0x0030\n0x0034\n0x0038\n0x003C\n0x0040\n0x0044\n0x0048\naddress\nbits\ntag\nset\noffset\nw - lg(M/k)\nlg(M/kB)\nlg B\n(c) 2008-2018 by the MIT 6.172 Lecturers\n0x0008\n0x0014\n0x0024\n0x0030\n0x003C\n0x0040\ntag\nCache size M = 32.\nLine/block size B = 4.\nk=2-way associativity.\nA cache block's set determines\nk possible cache locations.\nTo find a block in the\ncache, only the k\nlocations of its set\nmust be searched.\n\nTaxonomy of Cache Misses\nCold miss\n∙The first time the cache block is accessed.\nCapacity miss\n∙The previous cached copy would have been evicted even\nwith a fully associative cache.\nConflict miss\n∙Too many blocks from the same set in the cache. The block\nwould not have been evicted with a fully associative cache.\nSharing miss\n∙Another processor acquired exclusive access to the cache\nblock.\n∙True-sharing miss: The two processors are accessing the\nsame data on the cache line.\n∙False-sharing miss: The two processors are accessing\ndifferent data that happen to reside on the same cache line.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConflict Misses for Submatrices\n4096 columns\nof doubles\n= 215 bytes\nA\nAssume:\n! Word width w = 64!\n! Cache size M = 32K!\n! Line (block) size B = 64!\n! k=4-way associativity.\nrows\nConflict misses can be\nproblematic for caches with\nlimited associativity.\naddress\nbits\ntag\nset\noffset\ng\nw - lg(M/k) lg(M/kB)\nlg B\nAnalysis\nLook at a column of submatrix A.\nThe addresses of the elements are\nx, x+215, x+2·215, ..., x+31·215.\nThey all fall into the same set!\nSolutions\nCopy A into a temporary 32!32\nmatrix, or pad rows.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nIDEAL-CACHE MODEL\n\nIdeal-Cache Model\nParameters\n! Two-level hierarchy.\n! Cache size of M\nbytes.\n! Cache-line length\nof B bytes.\n! Fully associative.\n! Optimal, omniscient\nreplacement.\nP\ncache\nmemory\nB\nM/B\ncache lines\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPerformance Measures\n! work W (ordinary running time)\n! cache misses Q\n\nHow Reasonable Are Ideal Caches?\n\"LRU\" Lemma [ST85]. Suppose that an algorithm incurs\nQ cache misses on an ideal cache of size M. Then on\na fully associative cache of size 2M that uses the\nleast-recently used (LRU) replacement policy, it\nincurs at most 2Q cache misses. ∎\nImplication\nFor asymptotic analyses, one can assume optimal or\nLRU replacement, as convenient.\nSoftware Engineering\n∙Design a theoretically good algorithm.\n∙Engineer for detailed performance.\n- Real caches are not fully associative.\n- Loads and stores have different costs\nwith respect to bandwidth and latency.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCache-Miss Lemma\nLemma. Suppose that a program reads a set of r data\nsegments, where the ith segment consists of si\nbytes, and suppose that\nT\nంUK 0 ෬ N0 0T ଯ\nK\nThen all the segments fit into cache, and the number\nof misses to read them all is at most 3N/B.\nProof. A single segment si incurs at most si/B + 2 misses, and\nhence we have\nంUK\nT\nK\nT\n\nT\n\nB\nB\nB\nB\nB\nsi\nମ0\n0 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTall Caches\nP\ncache\nmemory\nB\nM/B\ncache lines\nTall-cache assumption\nB 2 < cM for some sufficiently small constant c \" 1.\nExample: Intel Xeon E5-2666 v3\n! Cache-line length = 64 bytes.\n! L1-cache size = 32 Kbytes.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nWhat's Wrong with Short Caches?\nTall-cache assumption\nB 2 < cM for some sufficiently small constant c ! 1.\nshort\ncache\nB\nM/B\nn\nn\nAn n\"n submatrix stored in row-major order may not\nfit in a short cache even if n2 < cM !\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSubmatrix Caching Lemma\nA\nn\nn\nLemma. Suppose that an n×n submatrix A is read\ninto a tall cache satisfying B 2 < cM , where c ≤ 1 is\nconstant, and suppose that cM ≤ n2 < M /3. Then A\nfits into cache, and the number of misses to read all\nA's elements is at most 3n2/B.\nProof. We have N = n2, n = r = si, B ≤ n = N/r, and N\n< M /3. Thus, the Cache-Miss Lemma applies. ∎\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACHE ANALYSIS OF\nMATRIX MULTIPLICATION\n\nMultiply Square Matrices\nvoid Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i=0; i < n; i++)\nfor (int64_t j=0; j < n; j++)\nfor (int64_t k=0; k < n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAnalysis of work\nW(n) = !(n3).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nvoid Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i=0; i < n; i++)\nfor (int64_t j=0; j < n; j++)\nfor (int64_t k=0; k < n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAssume row major and tall cache\nA\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCase 1\nn > cM/B.\nAnalyze matrix B.\nAssume LRU.\nQ(n) = !(n3), since\nmatrix B misses on\nevery access.\n\nAnalysis of Cache Misses\nvoid Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i=0; i < n; i++)\nfor (int64_t j=0; j < n; j++)\nfor (int64_t k=0; k < n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAssume row major and tall cache\nA\nB\nCase 2\nc'M1/2< n < cM/B.\nAnalyze matrix B.\nAssume LRU.\nQ(n) = n·!(n2/B) =\n!(n3/B), since\nmatrix B can exploit\nspatial locality.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nvoid Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i=0; i < n; i++)\nfor (int64_t j=0; j < n; j++)\nfor (int64_t k=0; k < n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAssume row major and tall cache\nA\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCase 3\nn !\"#$M1/2.\nAnalyze matrix B.\nAssume LRU.\nQ(n) = !(n2/B),\nsince everything\nfits in cache!\n\nSwapping Inner Loop Order\nvoid Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i=0; i < n; i++)\nfor (int64_t k=0; k < n; k++)\nfor (int64_t j=0; j < n; j++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAssume row major and tall cache\nAnalyze matrix B.\nAssume LRU.\nQ(n) = n·!(n2/B) =\n!(n3/B), since\nmatrix B can exploit\nspatial locality.\nC\nB\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTILING\n\nTiled Matrix Multiplication\ns\nvoid Tiled_Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i1=0; i1<n/s; i1+=s)\nfor (int64_t j1=0; j1<n/s; j1+=s)\nfor (int64_t k1=0; k1<n/s; k1+=s)\nfor (int64_t i=i1; i<i1+s && i<n; i++)\nfor (int64_t j=j1; j<j1+s && j<n; j++)\nfor (int64_t k=k1; k<k1+s && k<n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\nAnalysis of work\n! Work W(n)\n= \"((n/s)3(s3))\n= \"(n3).\ns\nn\nn\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTiled Matrix Multiplication\nvoid Tiled_Mult(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i1=0; i1<n; i1+=s)\nfor (int64_t j1=0; j1<n; j1+=s)\nfor (int64_t k1=0; k1<n; k1+=s)\nfor (int64_t i=i1; i<i1+s && i<n; i++)\nfor (int64_t j=j1; j<j1+s && j<n; j++)\nfor (int64_t k=k1; k<k1+s && k<n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\ns\nAnalysis of cache misses\ns\n! Tune s so that the submatrices\njust fit into cache ! s = \"(M1/2).\n! Submatrix Caching Lemma implies\nn\n\"(s2/B) misses per submatrix.\n! Q(n) =\n= \"(n3/(BM1/2)).\n\"((n/s)3(s2/B))\nRemember\nn\n! Optimal [HK81].\nthis!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nfor (int64_t i1=0; i1<n; i1+=s)\nfor (int64_t j1=0; j1<n; j1+=s)\nfor (int64_t k1=0; k1<n; k1+=s)\nfor (int64_t i=i1; i<i1+s && i<n; i++)\nfor (int64_t j=j1; j<j1+s && j<n; j++)\nfor (int64_t k=k1; k<k1+s && k<n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n+j] += A[i*\n*n+\nTiled Matrix Multiplication\nvoid Tiled_Mult(double *C, double *A, double *B, int64_t n) {\nVoodoo!\n}\ns\nAnalysis of cache misses\n! Tune s so that the submatrices\ns\njust fit into cache ! s = \"(M1/2).\n! Submatrix Caching Lemma implies\nn\n\"(s2/B) misses per submatrix.\n! Q(n) =\n= \"(n3/(BM1/2)).\n\"((n/s)3(s2/B))\nRemember\n! Optimal [HK81].\nthis!\nn\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTwo-Level Cache\nn\nt\ns\nn\ns\n∙Two \"voodoo\" tuning\nparameters s and t.\n∙Multidimensional\ntuning optimization\ncannot be done with\nbinary search.\nt\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n! Two \"voodoo\" tuning\nparameters s and t.\n! Multidimensional\ntuning optimization\ncannot be done with\nbinary search.\nTwo-Level Cache\nn\ns\nt\nt\ns\nn\nvoid Tiled_Mult2(double *C, double *A, double *B, int64_t n) {\nfor (int64_t i2=0; i2<n; i2+=s)\nfor (int64_t j2=0; j2<n; j2+=s)\nfor (int64_t k2=0; k2<n; k2+=s)\nfor (int64_t i1=i2; i1<i2+s && i1<n; i1+=t)\nfor (int64_t j1=j2; j1<j2+s && j1<n; j1+=t)\nfor (int64_t k1=k2; k1<k2+s && k1<n; k1+=t)\nfor (int64_t i=i1; i<i1+s && i<i2+t && i<n; i++)\nfor (int64_t j=j1; j<j1+s && j<j2+t && j<n; j++)\nfor (int64_t k=k1; k1<k1+s && k<k2+t && k<n; k++)\nC[i*n+j] += A[i*n+k] * B[k*n+j];\n}\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nThree-Level Cache\nn\n∙Three \"voodoo\"\ntuning parameters.\n∙Twelve nested for loops.\n∙Multiprogrammed environment:\nDon't know the effective cache size\nwhen other jobs are running ⇒\neasy to mistune the parameters!\nu\nu\nt\nn\ns\nt s\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$*\n%&'&(!\n\"#)*+)$#)*+,*-./01*\n(c) 2008-2018 by the MIT 6.172 Lecturers\nDIVIDE & CONQUER\n\nRecursive Matrix Multiplication\nDivide-and-conquer on n × n matrices.\nC11\nC12\nC21\nC22\n=\nA11\nA12\nA21\nA22\n×\nB11\nB12\nB21\nB22\n=\n+\nA11B11\nA11B12\nA21B11\nA21B12\nA12B21\nA12B22\nA22B21\nA22B22\n8 multiply-adds of (n/2) × (n/2) matrices.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecursive Code\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\nRec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\nRec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\nRec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\nRec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\n} }\nCoarsen base case to\novercome function-\ncall overheads.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecursive Code\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\nRec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\nRec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\nRec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\nRec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\n} }\nrowsize\nn\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Work\nW(n) =\n=\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\nRec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\nRec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\nRec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\nRec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\n} }\n8W(n/2) + !(1)\n!(n3)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Work\nW(n) = 8W(n/2) + Θ(1)\nrecursion tree\nW(n)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nW(n)\nAnalysis of Work\nW(n) = 8W(n/2) + Θ(1)\nrecursion tree\nW(n/2)\nW(n/2)\n⋯\nW(n/2)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nW(n/2)\nW(n/2)\nW(n/2)\nAnalysis of Work\nW(n) = 8W(n/2) + Θ(1)\nrecursion tree\n⋯\nW(n/4) W(n/4)\n⋯\nW(n/4)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Work\nW(n) = 8W(n/2) + #(1)\nW(n/2)\nW(n/2)\nW(n/2)\n!\nW(n/4) W(n/4)\nW(n/4)\n!\n\"\n#(1)\n$\n#(n3)\n#leaves = 8lg n = nlg 8 = n3\nlg n\nW(n) = #(n3)\nrecursion tree\nNote: Same work as looping versions.\n#\nGeometric\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nSubmatrix\nCaching\nLemma\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\nRec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\nRec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\nRec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\nRec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\n} }\nQ(n) =\n!(n2/B) if n2<cM for suff. small const c\"1,\n8Q(n/2) + !(1) otherwise.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nQ(n) =\n!(n2/B) if n2<cM for suff. small const c\"1,\n8Q(n/2) + !(1) otherwise.\nrecursion tree\nQ(n)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nQ(n) =\n!(n2/B) if n2<cM for suff. small const c\"1,\n8Q(n/2) + !(1) otherwise.\nrecursion tree\nQ(n/2)\nQ(n/2)\n#\nQ(n/2)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nQ(n) =\n!(n2/B) if n2<cM for suff. small const c\"1,\n8Q(n/2) + !(1) otherwise.\nrecursion tree\n#\nQ(n/4) Q(n/4)\n#\nQ(n/4)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAnalysis of Cache Misses\nQ(n) =\n#(n2/B) if n2<cM for suff. small const c&1,\n8Q(n/2) + #(1) otherwise.\n\"\nSame cache misses as with tiling!\nlg n-!lg(cM)\nQ(n/2) Q(n/2)\nQ(n/2)\n!\nQ(n/4) Q(n/4)\nQ(n/4)\n!\n#(n3/BM1/2)\n#(cM/B)\n$\n#(\nGeometric\nQ(n) = #(n3/BM1/2)\nrecursion tree\n#leaves = 8lg n - %lg(cM)\n= #(n3/M3/2).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nEfficient Cache-Oblivious Algorithms\n- No voodoo tuning parameters.\n- No explicit knowledge of caches.\n- Passively autotune.\n- Handle multilevel caches automatically.\n- Good in multiprogrammed environments.\nMatrix multiplication\nThe best cache-oblivious codes to date work on\narbitrary rectangular matrices and perform\nbinary splitting (instead of 8-way) on the largest\nof i, j, and k.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecursive Parallel Matrix Multiply\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\ncilk_spawn Rec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\ncilk_sync;\ncilk_spawn Rec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\ncilk_sync;\n} }\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCilk and Caching\nTheorem. Let QP be the number of cache misses in a\ndeterministic Cilk computation when run on P\nprocessors, each with a private cache of size M, and\nlet SP be the number of successful steals during the\ncomputation. In the ideal-cache model, we have\nQP = Q1 + O(SPM/B) ,\nwhere M is the cache size and B is the size of a cache\nblock.\nProof. After a worker steals a continuation, its cache is\ncompletely cold in the worst case. But after M/B (cold) cache\nmisses, its cache is identical to that in the serial execution. The\nsame is true when a worker resumes a stolen subcomputation\nafter a cilk_sync. The number of times these two situations can\noccur is at most 2SP. ∎\nSP = O(PTinf) in expectation\nMORAL: Minimizing cache misses in the serial elision\nessentially minimizes them in parallel executions.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecursive Parallel Matrix Multiply\n// Assume that n is an exact power of 2.\nvoid Rec_Mult(double *C, double *A, double *B,\nint64_t n, int64_t rowsize) {\nif (n == 1)\nC[0] += A[0] * B[0];\nelse {\nint64_t d11 = 0;\nint64_t d12 = n/2;\nint64_t d21 = (n/2) * rowsize;\nint64_t d22 = (n/2) * (rowsize+1);\ncilk_spawn Rec_Mult(C+d11, A+d11, B+d11, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d21, A+d22, B+d21, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d12, A+d11, B+d12, n/2, rowsize);\nRec_Mult(C+d22, A+d22, B+d22, n/2, rowsize);\ncilk_sync;\ncilk_spawn Rec_Mult(C+d11, A+d12, B+d21, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d21, A+d21, B+d11, n/2, rowsize);\ncilk_spawn Rec_Mult(C+d12, A+d12, B+d22, n/2, rowsize);\nRec_Mult(C+d22, A+d21, B+d12, n/2, rowsize);\ncilk_sync;\n} }\nSpan:\nT!(n) =\n2!!(n/2) + \"#$%&\n)&\"#*%&\nCache misses:\nQp = '1 + O(SPM(B)\n= \"(n3/BM1/2) + O(PnM(B)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSummary\n●Associativity in caches\n●Ideal cache model\n●Cache-aware algorithms\n●Tiled matrix multiplication\n●Cache-oblivious algorithms\n●Divide-and-conquer matrix multiplication\n●Cache efficiency analysis in Homework 8\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 15: Cache-Oblivious Algorithms",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cef17369f91d3140409f2be4ad9246a4_MIT6_172F18_lec15.pdf",
      "content": "!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n6.172\nPerformance\nEngineering\nof Software\nSystems\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLECTURE 15\nCache-Oblivious\nAlgorithms\nJulian Shun\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSIMULATION OF\nHEAT DIFFUSION\n\nHeat Diffusion\n2D heat equation\nLet u(t, x, y) = temperature\nat time t of point (x, y).\n! is the thermal diffusivity.\nᆄV\nᆄU ᅦ ᆄV\nᆄY\nᆄV\nᆄZ\nAcknowledgment\nSome of the slides in this presentation were inspired\nby originals due to Matteo Frigo.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n2D Heat-Diffusion Simulation\nBefore\nAfter\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n1D Heat Equation\nᆄV\nᆄU ᅦᆄV\nᆄY\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFinite-Difference Approximation\n\nஃ\nᆄ\nᆄU V U Y\nஃ V U U Y\nਲV U Y\nU\n.\nᆄV\nᆄU ᅦᆄV\nᆄY\nᆄ\nV U Y Y\nਲV U Y ਲY\nV U Y\n.\nᆄY\nY\nᇘ\nᆄ\nV U Y Y\nਲᇘ V U Y ਲY\nᆄY V U Y\nஃ\nᇘY\nY\nᇘY\nThe 1D heat equation thus reduces to\nV U U Y\nਲV U Y\nY\nਲV U Y\nV U Y ਲY\nU\nᅦV U Y\nY\nওY\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n3-Point Stencil\nt\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\nx\nA stencil computation\nupdates each point in\nan array by a fixed\npattern, called a stencil.\nboundary\niteration\nspace\nV U U Y\nਲV U Y\nU\nᅦV U Y Y\nਲV U Y\nV U Y ਲY\nY\n\nও\nUpdate rule\nu[t+1][x] = u[t][x] + ALPHA\n* (u[t][x+1] - 2*u[t][x] + u[t][x-1]);\nU Y\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACHE-OBLIVIOUS\nSTENCIL COMPUTATIONS\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nParameters\n! Two-level hierarchy.\n! Cache size of M\nbytes.\n! Cache-line length\n(block size) of B\nbytes.\n! Fully associative.\n! Optimal omniscient\nreplacement, or LRU.\nRecall: Ideal-Cache Model\nMIT 6 172 L\nt\nPerformance Measures\n! work W (ordinary running time)\n! cache misses Q\nP\ncache\nmemory\nB\nM/B\ncache lines\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\n\"\n!\n!\n!\nCache Behavior of Looping\ndouble u[2][N]; // even-odd trick\nstatic inline double kernel(double * w) {\nreturn w[0] + ALPHA * (w[-1] - 2*w[0] + w[1]);\n}\nfor (size_t t = 1; t < T-1; ++t) { // time loop\nfor(size_t x = 1; x < N-1; ++x) // space loop\nu[(t+1)%2][x] = kernel( &u[t%2][x] );\nAssuming LRU,\nif N > M, then\nQ = #(NT/B).\nu\nt\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\nCache-Oblivious 3-Point Stencil\nx\nt\nt0\nt1\nx0\nx1\nheight\nRecursively traverse trapezoidal regions of\nspace-time points (t,x) such that\nt0 ≤ t < t1\nx0+dx0(t-t0) ≤ x < x1+dx1(t-t0)\ndx0, dx1 ∈ {-1, 0, 1}\nwidth\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\nBase Case\nx\nt\nIf height = 1, compute all space-time points in\nthe trapezoid. Any order of computation is valid,\nsince no point depends on another.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\nSpace Cut\nx\nt\nIf width ≥ 2·height, cut the trapezoid with a line\nof slope -1 through the center. Traverse the\ntrapezoid on the left first, and then the one on\nthe right.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTime Cut\nx\nt\nIf width < 2·height, cut the trapezoid with a\nhorizontal line through the center. Traverse the\nbottom trapezoid first, and then the top one.\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nC Implementation\n2008 2018 b\nth\nMIT 6 172 L\nt\nvoid trapezoid(int64_t t0, int64_t t1, int64_t x0, int64_t dx0,\nint64_t x1, int64_t dx1)\n{\nint64_t lt = t1 - t0;\nif (lt == 1) { //base case\nfor (int64_t x = x0; x < x1; x++)\nu[t1%2][x] = kernel( &u[t0%2][x] );\n} else if (lt > 1) {\nif (2 * (x1 - x0) + (dx1 - dx0) * lt >= 4 * lt) { //space cut\nint64_t xm = (2 * (x0 + x1) + (2 + dx0 + dx1) * lt) / 4;\ntrapezoid(t0, t1, x0, dx0, xm, -1);\ntrapezoid(t0, t1, xm, -1, x1, dx1);\n} else { //time cut\nint64_t halflt = lt / 2;\ntrapezoid(t0, t0 + halflt, x0, dx0, x1, dx1);\ntrapezoid(t0 + halflt, t1, x0 + dx0 * halflt, dx0,\nx1 + dx1 * halflt, dx1);\n}\n}\n}\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis\n!(1)\nW(n/2)\nW(n/2)\n!(1)\n!(1)\n!(1)\n!(1)\nRecursion tree\n!(1)\n\" Each leaf represents !(hw) points, where h = !(w).\n\" Each leaf incurs !(w/B) misses, where w = !(M).\n\" !(NT/hw) leaves.\n\" #internal nodes = #leaves - 1 do not contribute substantially to Q.\n\" Q = !(NT/hw)·!(w/B) = !(NT/M2)·!(M/B) = !(NT/MB).\n\" For d dimensions, Q = !(NT/M1/dB)\n#\n#\n#\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nSimulation: 3-Point Stencil\n! Fully associative LRU cache\n\" B = 4 points\n\" M = 32 points\n! Cache-hit latency = 1 cycle\n! Cache-miss latency = 10 cycles\n! Rectangular\nregion\n\" N = 95\n\" T = 87\nLooping\nTrapezoid\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLooping v. Trapezoid on Heat\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nImpact on Performance\nQ. How can the cache-oblivious trapezoidal\ndecomposition have so many fewer cache misses,\nbut the advantage gained over the looping version\nbe so marginal?\nA. Prefetching and a good memory architecture. One\ncore cannot saturate the memory bandwidth.\n...\nMemory\nI/O\n$\nP\n$\nP\n$\nP\n$\n$\n$\nNetwork\n$\nPlenty of\nbandwidth\nfor 1 core\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACHING AND\nPARALLELISM\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCilk and Caching\nTheorem. Let QP be the number of cache misses in a\ndeterministic Cilk computation when run on P\nprocessors, each with a private cache, and let SP be the\nnumber of successful steals during the computation.\nIn the ideal-cache model, we have\nQP = Q1 + O(SPM/B) ,\nwhere M is the cache size and B is the size of a cache\nblock.\nProof. After a worker steals a continuation, its cache is\ncompletely cold in the worst case. But after M/B (cold) cache\nmisses, its cache is identical to that in the serial execution. The\nsame is true when a worker resumes a stolen subcomputation\nafter a cilk_sync. The number of times these two situations can\noccur is at most 2SP. ∎\nMORAL: Minimizing cache misses in the serial elision\nessentially minimizes them in parallel executions.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\n∙\nDoes this work in parallel?\nx\nt\nSpace cut: If width ≥ 2·height, cut the trapezoid\nwith a line of slope -1 through the center.\nTraverse the trapezoid on the left first, and then\nthe one on the right.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nParallel Space Cuts\nA parallel space cut produces two black\ntrapezoids that can be executed in parallel\nand a third gray trapezoid that executes in\nseries with the black trapezoids.\nt\nx\nupright trapezoid\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nParallel Space Cuts\nt\nx\nA parallel space cut produces two black\ntrapezoids that can be executed in parallel\nand a third gray trapezoid that executes in\nseries with the black trapezoids.\ninverted trapezoid\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nParallel Looping v. Parallel Trap.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPerformance Comparison\nHeat equation on a 3000×3000 grid for 1000\ntime steps (4 processor cores with 8MB LLC)\nCode\nTime\nSerial looping\n128.95s\nParallel looping\n66.97s\nSerial trapezoidal\n66.76s\nParallel trapezoidal\n16.86s\nThe parallel looping code achieves less than\nhalf the potential speedup, even though it has\nfar more parallelism.\n1.93x\n3.96x\n}\n}\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMemory Bandwidth\n...\nMemory\nI/O\n$\nP\n$\nP\n$\nP\nNetwork\nM\nPotential\nbottleneck!\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nImpediments to Speedup\n□Insufficient parallelism\n□Scheduling overhead\n□Lack of memory bandwidth\n□Contention (locking and true/false sharing)\nCilkscale can diagnose the first two problems.\nQ. How can we diagnose the third?\nA. Run P identical copies of the serial code in parallel\n-- if you have enough memory.\nTools exist to detect lock contention in an execution,\nbut not the potential for lock contention. Potential for\ntrue and false sharing is even harder to detect.\nP\nP\nP\nP\n\n!\"##$\n%&'&(!\n\"#)*+)$#)*+,*-./01\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCACHE-OBLIVIOUS\nSORTING\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nOUTLINE\n- Simulation of Heat Diffusion\n- Cache-Oblivious Stencil\nComputations\n- Caching and Parallelism\n- Cache-Oblivious Sorting\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)#*+,-.+ /01 #*+,-.+ /21 #*+,-.+ *31\n#*+,-.+ /41 #*+,-.+ *56 7\n89#:& )*3;< == *5;<6 7\n#> )/2 ?@ /46 7\n/0AA @ /2AAB *3CCB\nD &:E& 7\n/0AA @ /4AAB *5CCB\nD\nD\n89#:& )*3;<6 7\n/0AA @ /2AAB *3CCB\nD\n89#:& )*5;<6 7\n/0AA @ /4AAB *5CCB\nD\nD\nMerging Two Sorted Arrays\nTime to merge n\nelements = !(n).\nNumber of cache\nmisses = !(n/B).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\nMerge Sort\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\nMerge Sort\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\nMerge Sort\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\n6 172 Lecture\nMerge Sort\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\n6 172 Lecture\nMerge Sort\nmerge\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\n6 172 Lecture\nMerge Sort\nmerge\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n!\"#$ %&'(&)*\"'+,#-+./)+ 012 #-+./)+ 032 #-+./)+ -4 5\n#6 ,-7784 5\n19:; 7 39:;<\n= &>*& 5\n#-+./)+ ?9-;<\n@#>A)*BCD- %&'(&)*\"'+,?2 32 -EF4<\n%&'(&)*\"'+,?G-EF2 3G-EF2 -H-EF4<\n@#>A)*I-@<\n%&'(&,12 ?2 -EF2 ?G-EF2 -H-EF4<\n=\n=\n6 172 Lecture\nMerge Sort\nmerge\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nvoid merge_sort(int64_t *B, int64_t *A, int64_t n) {\nif (n==1) {\nB[0] = A[0];\n} else {\nint64_t C[n];\nmerge_sort(C, A, n/2);\nmerge_sort(C+n/2, A+n/2, n-n/2);\nmerge(B, C, n/2, C+n/2, n-n/2);\n}\n}\n= !(n lgn)\nWork:\nW(n) = 2W(n/2) + !(n)\nCASE 2\nnlogba = nlog22 = n\nf(n) = !(nlogbalg0n)\nWork of Merge Sort\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nSolve W(n) = 2W(n/2) + Θ(n).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nW(n)\nSolve W(n) = 2W(n/2) + Θ(n).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nW(n/2)\nW(n/2)\nn\nSolve W(n) = 2W(n/2) + Θ(n).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nn\nW(n/4)\nW(n/4)\nW(n/4)\nW(n/4)\nn/2\nn/2\nSolve W(n) = 2W(n/2) + Θ(n).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nn\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nΘ(1)\n...\nSolve W(n) = 2W(n/2) + Θ(n).\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nh = lg n\nn\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nΘ(1)\nSolve W(n) = 2W(n/2) + Θ(n).\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nn\nh = lg n\nn\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nΘ(1)\nSolve W(n) = 2W(n/2) + Θ(n).\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nn\nΘ(1)\nh = lg n\nn\nn\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nSolve W(n) = 2W(n/2) + Θ(n).\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nRecursion Tree\nn\nΘ(1)\nh = lg n\nn\nn\nn\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nSolve W(n) = 2W(n/2) + Θ(n).\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n#leaves = n\nRecursion Tree\n!(n)\nn\n!(1)\nh = lg n\nn\nn\nn\n...\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nSolve W(n) = 2W(n/2) + !(n).\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n#leaves = n\nRecursion Tree\n!(n)\nn\n!(1)\nh = lg n\nn\nn\nn\n...\nn/4\nn/4\nn/4\nn/4\nn/2\nn/2\nSolve W(n) = 2W(n/2) + !(n).\nW(n) = !(n lg n)\n...\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nNow with Caching\nMerge subroutine\nQ(n) = Θ(n/B) .\nMerge sort\nΘ(n/B)\nif n ≤ cM, constant c ≤ 1;\n2Q (n/2) + Θ(n/B)\notherwise.\nQ(n) =\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis of Merge Sort\nQ(n)\nΘ(n/B)\nif n ≤ cM, constant c ≤ 1;\n2Q (n/2) + Θ(n/B)\notherwise.\nQ(n) =\nRecursion tree\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis of Merge Sort\nQ(n/2)\nQ(n/2)\nn/B\nΘ(n/B)\nif n ≤ cM, constant c ≤ 1;\n2Q (n/2) + Θ(n/B)\notherwise.\nQ(n) =\nRecursion tree\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis of Merge Sort\nQ(n/4)\nQ(n/4)\nQ(n/4)\nQ(n/4)\nn/2B\nn/2B\nn/B\nΘ(n/B)\nif n ≤ cM, constant c ≤ 1;\n2Q (n/2) + Θ(n/B)\notherwise.\nQ(n) =\nRecursion tree\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis of Merge Sort\nQ(cM!\n...\nh=lg(n/cM)\nn/4B\nn/4B\nn/4B\nn/4B\nn/2B\nn/2B\nn/B\n!(n/B)\nif n \" cM, constant c \" 1;\n2Q (n/2) + !(n/B)\notherwise.\nQ(n) =\nRecursion tree\n#leaves = n/cM\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n...\n#leaves = n/cM\nCache Analysis of Merge Sort\n!(n/B)\n!(M/B)\n...\nh=lg(n/cM)\nn/B\nn/B\nn/B\nn/4B\nn/4B\nn/4B\nn/4B\nn/2B\nn/2B\nQ(n) = !((n/B) lg(n/M))\nn/B\n!(n/B)\nif n \" cM, constant c \" 1;\n2Q (n/2) + !(n/B)\notherwise.\nQ(n) =\nRecursion tree\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nBottom Line for Merge Sort\n= Θ( (n/B) lg(n/M) ) .\n∙For n ≫ M, we have lg(n/M) ≈ lg n, and thus\nW(n)/Q(n) ≈ Θ(B).\n∙For n ≈ M, we have lg(n/M) ≈ Θ(1), and thus\nW(n)/Q(n) ≈ Θ(B lg n).\nΘ(n/B)\nif n ≤ cM, constant c ≤ 1;\n2Q (n/2) + Θ(n/B)\notherwise;\nQ(n) =\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway Merging\nIDEA: Merge R < n subarrays with a tournament.\nR\nn\nn/R\nlg R\n! Tournament takes \"(R)\nwork to produce the first\noutput.\n#\n$\n$\n$\n$\n$\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway Merging\nR\nn\nn/R\nlg R\n! Tournament takes \"(R)\nwork to produce the first\noutput.\n#\n$\n$\n$\n$\n$\nIDEA: Merge R < n subarrays with a tournament.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway Merging\nR\nn\nn/R\nlg R\n! Tournament takes \"(R)\nwork to produce the first\noutput.\n! Subsequent outputs cost\n\"(lg R) per element.\nIDEA: Merge R < n subarrays with a tournament.\n#\n#\n#\n#\n#\n$\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway Merging\nR\nn\nn/R\nlg R\n! Tournament takes \"(R)\nwork to produce the first\noutput.\n! Subsequent outputs cost\n\"(lg R) per element.\nIDEA: Merge R < n subarrays with a tournament.\n#\n#\n#\n#\n#\n$\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway Merging\nR\nn\nn/R\nlg R\n! Tournament takes \"(R)\nwork to produce the first\noutput.\n! Subsequent outputs cost\n\"(lg R) per element.\n! Total work merging\n= \"(R+n lg R) = \"(n lg R).\nIDEA: Merge R < n subarrays with a tournament.\n#\n#\n#\n#\n#\n$\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nWork of Multiway Merge Sort\n!(1)\nif n = 1;\nR·W(n/R) + !(n lg R)\notherwise.\nW(n) =\nR\n#leaves = n\n\"\n!(1)\nlogRn\nW(n) = !((n lg R) logRn+n)\n= !((n lg R)(lg n)/lg R+n)\n= !(n lg n)\nRecursion tree\nSame as binary merge sort.\n(n/R) lg R\n(n/R) lg R\n(n/R) lg R\nR\nn lg R\nn lg R\nn lg R\n#\n!(n)\nn lg R\n(n/R2) lg R\n(n/R2) lg R\n(n/R2) lg R\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCaching Recurrence\nAssume that we have R < cM/B for a sufficiently\nsmall constant c ≤ 1.\nConsider the R-way merging of contiguous arrays\nof total size n. If R < cM/B, the entire tournament\nplus 1 block from each array can fit in cache.\n⇒Q (n) ≤ Θ(n/B) for merging.\nR-way merge sort\nΘ(n/B)\nif n < cM;\nR·Q(n/R) + Θ(n/B) otherwise.\nQ(n) ≤\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nCache Analysis\nR\n#leaves = n/cM\n!\nlogR(n/cM)\nQ(n) = \"((n/B) logR(n/M))\nRecursion tree\nn/RB\nn/RB\nn/RB\nR\n#\n$\nn/B\nn/B\nn/B\nn/R2B\nn/R2B\nn/R2B\n#\nn/B\n\"(M/B)\n\"(n/B)\n\"(n/B)\nif n < cM;\nR·Q(n/R) + \"(n/B)\notherwise\n.\nQ(n) %\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTuning the Voodoo Parameter\nBy the tall-cache assumption and the fact that\nlogM (n/M) = Θ((lg n)/lg M), we have\nQ(n) = Θ((n/B) logM/B(n/M))\n= Θ((n/B) logM (n/M))\n= Θ((n lg n)/B lg M) .\nHence, we have W(n)/Q(n) ≈ Θ(B lg M).\nWe have\nQ(n) = Θ((n/B) logR(n/M)) ,\nwhich decreases as R ≤ cM/B increases.\nChoosing R as big as possible yields\nR = Θ(M/B) .\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMultiway versus Binary Merge Sort\nWe have\nQmultiway(n) = Θ((n lg n)/B lg M)\nversus\nQbinary(n) = Θ((n/B) lg(n/M))\n= Θ((n lg n)/B) ,\nas long as n ≫M, because then lg(n/M) ≈ lg n.\nThus, multiway merge sort saves a factor of\nΘ(lgM) in cache misses.\nExample (ignoring constants)\n∙L1-cache: M = 215 ⇒ 15× savings.\n∙L2-cache: M = 218 ⇒ 18× savings.\n∙L3-cache: M = 223 ⇒ 23× savings.\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nOptimal Cache-Oblivious Sorting\nFunnelsort [FLPR99]\n1. Recursively sort n1/3 groups of n2/3 items.\n2. Merge the sorted groups with an n1/3-funnel.\nA k-funnel merges k3 items in k sorted lists, incurring\nat most\ncache misses. Thus, funnelsort incurs\ncache misses, which is asymptotically optimal [AV88].\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nk\nConstruction of a k-funnel\nbuffers\nk3\nitems\nTall-cache assumption: M = !(B2).\nCache misses\n= O(k + (k3/B)(1+logMk)).\nSubfunnels in contiguous storage.\nBuffers in contiguous storage.\nRefill buffers on demand.\nSpace = O(k2).\nk3/2\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nOther C-O Algorithms\nMatrix Transposition/Addition\nStraightforward recursive algorithm.\nΘ(1+mn/B)\nFast Fourier Transform\nVariant of Cooley-Tukey [CT65] using cache-\noblivious matrix transpose.\nΘ(1 + (n/B)(1 + logMn))\nLUP-Decomposition\nRecursive algorithm due to Sivan Toledo [T97].\nΘ(1 + n2/B + n3/BM1/2)\nStrassen's Algorithm\nStraightforward recursive algorithm.\nΘ(n + n2/B + nlg 7/BM(lg 7)/2 - 1)\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nC-O Data Structures\nOrdered-File Maintenance\nINSERT/DELETE or delete anywhere in file while\nmaintaining O(1)-sized gaps. Amortized bound\n[BDFC00], later improved in [BCDFC02].\nO(1 + (lg2n)/B)\nB-Trees\nINSERT/DELETE:\nO(1+logB+1n+(lg2n)/B\nSEARCH:\nO(1+logB+1n)\nTRAVERSE:\nO(1+k/B)\nPriority Queues\nFunnel-based solution [BF02]. General scheme\nbased on buffer trees [ABDHMM02] supports\nINSERT/DELETE.\nO(1+(1/B)logM/B(n/B))\nSolution [BDFC00] with later simplifications\n[BDIW02], [BFJ02].\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 16: Nondeterministic Parallel Programming",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/e4fe97d2a250b57bb1e330fba8cf5992_MIT6_172F18_lec16.pdf",
      "content": "(c) 2008-2018 by the MIT 6.172 Lecturers\n6.172\nPerformance\nEngineering\nof Software\nSystems\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\nLECTURE 16\nNONDETERMINISTIC\nPARALLEL PROGRAMMING\nCharles E. Leiserson\n\nDeterminism\nDefinition. A program is deterministic on\na given input if every memory location is\nupdated with the same sequence of values\nin every execution.\n! The program always behaves the same way.\n! Two different memory locations may be updated\nin different orders, but each location always\nsees the same sequence of updates.\nAdvantage: DEBUGGING!\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nGolden Rule of Parallel Programming\nNever r write nondeterministic\nparallel programs.\nThey can exhibit anomalous behaviors,\nand it's hard to debug them.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSilver Rule of Parallel Programming\nNever r write nondeterministic\nparallel programs\np\n--\np\ng\n-- but if you must*\nt* --\ny\nbut\nyou\nust\nb\nt\nalways devise a test strategy\na\nays de\nse a test st ategy\nto manage the nondeterminism!\nTypical test strategies\n- Turn off nondeterminism.\n- Encapsulate nondeterminism.\n- Substitute a deterministic alternative.\n- Use analysis tools.\n*E.g., for performance reasons.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nMUTUAL EXCLUSION\n& ATOMICITY\n\nHash Table\nslot = hash(x->key);\nx->next = table[slot];\ntable[slot] = x;\nInsert x into table\nx:\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConcurrent Hash Table\n(c) 2008-2018 by the MIT 6.172 Lecturers\nx:\ny:\nslot = hash(x->key);\nx->next = table[slot];\ntable[slot] = x;\nslot = hash(y->key);\ny->next = table[slot];\ntable[slot] = y;\nRACE\nBUG!\n\nAtomicity\nDefinition. A sequence of instructions is\natomic if the rest of the system cannot ever\nview them as partially executed. At any\nmoment, either no instructions in the\nsequence have executed or all have executed.\nDefinition. A critical section is a piece of\ncode that accesses a shared data structure\nthat must not be accessed by two or more\nthreads at the same time (mutual exclusion).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMutexes\nDefinition. A mutex is an object with lock and\nunlock member functions. An attempt by a thread\nto lock an already locked mutex causes that thread\nto block (i.e., wait) until the mutex is unlocked.\nModified code: Each slot is a struct with a mutex\nL and a pointer head to the slot contents.\nslot = hash(x->key);\nlock(&table[slot].L);\nx->next = table[slot].head;\ntable[slot].head = x;\nunlock(&table[slot].L);\ncritical\nsection\nMutexes can be used to implement atomicity.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRecall: Determinacy Races\nDefinition. A determinacy race occurs when\ntwo logically parallel instructions access the\nsame memory location and at least one of the\ninstructions performs a write.\n∙ A program execution with no determinacy races\nmeans that the program is deterministic on that\ninput.\n∙ The program always behaves the same on that input,\nno matter how it is scheduled and executed.\n∙ If determinacy races exist in an ostensibly\ndeterministic program (e.g., a program with no\nmutexes), Cilksan guarantees to find such a race.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nData Races\nDefinition. A data race occurs when two logically\nparallel instructions holding no locks in common\naccess the same memory location and at least one\nof the instructions performs a write.\nAlthough data-race-free programs obey atomicity\nconstraints, they can still be nondeterministic,\nbecause acquiring a lock can cause a determinacy\nrace with another lock acquisition.\nnondeterministic by intention, and\nthey invalidate Cilksan's guarantee.\nWARNING: Codes that use locks are\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nNo Data Races ! No Bugs\nExample\nslot = hash(x->key);\nlock(&table[slot].L);\nx->next = table[slot].head;\nunlock(&table[slot].L);\nlock(&table[slot].L);\ntable[slot].head = x;\nunlock(&table[slot].L);\nNevertheless, the presence of mutexes and\nthe absence of data races at least means that\nthe programmer thought about the issue.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n\"Benign\" Races\nExample: Identify the set of digits in an array.\nA: 4, 1, 0, 4, 3, 3, 4, 6, 1, 9, 1, 9, 6, 6, 6, 3, 4\n//benign race\n(int i=0; i<N; ++i) {\nfor (int i=0; i<10; ++i) {\ndigits[i] = 0;\n}\ncilk_for\ndigits[A[i]] = 1;\n}\ndigits:\nCAUTION: This code only works correctly if the\nhardware writes the array elements atomically --\ne.g., it races for byte values on some architectures.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n\"Benign\" Races\nExample: Identify the set of digits in an array.\nfor (int i=0; i<10; ++i) {\ndigits[i] = 0;\n}\ncilk_for (int i=0; i<N; ++i) {\ndigits[A[i]] = 1; //benign race\n}\nA: 4, 1, 0, 4, 3, 3, 4, 6, 1, 9, 1, 9, 6, 6, 6, 3, 4\ndigits:\nCilksan allows you to turn off race detection for\nintentional races, which is dangerous but practical.\nBetter solutions exist, e.g., fake locks in Intel's\nCilkscreen (See Intel Cilk Plus Tools User's Guide.)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nIMPLEMENTATION OF\nMUTEXES\n\nProperties of Mutexes\n∙ Yielding/spinning\nA yielding mutex returns control to the operating\nsystem when it blocks. A spinning mutex consumes\nprocessor cycles while blocked.\n∙ Reentrant/nonreentrant\nA reentrant mutex allows a thread that is already\nholding a lock to acquire it again. A nonreentrant\nmutex deadlocks if the thread attempts to reacquire\na mutex it already holds.\n∙ Fair/unfair\nA fair mutex puts blocked threads on a FIFO queue,\nand the unlock operation unblocks the thread that\nhas been waiting the longest. An unfair mutex lets\nany blocked thread go next.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSimple Spinning Mutex\nSpin_Mutex:\ncmp 0, mutex ; Check if mutex is free\nje Get_Mutex\npause ; x86 hack to unconfuse pipeline\njmp Spin_Mutex\nGet_Mutex:\nmov 1, %eax\nxchg mutex, %eax ; Try to get mutex\ncmp 0, %eax ; Test if successful\njne Spin_Mutex\nCritical_Section:\n<critical-section code>\nmov 0, mutex ; Release mutex\nKey property: xchg is an atomic exchange.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSimple Yielding Mutex\nSpin_Mutex:\ncmp 0, mutex ; Check if mutex is free\nje Get_Mutex\ncall pthread_yield ; Yield quantum\njmp Spin_Mutex\nGet_Mutex:\nmov 1, %eax\nxchg mutex, %eax ; Try to get mutex\ncmp 0, %eax ; Test if successful\njne Spin_Mutex\nCritical_Section:\n<critical-section code>\nmov 0, mutex ; Release mutex\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nCompetitive Mutex\nCompeting goals:\n∙ To claim mutex soon after it is released.\n∙ To behave nicely and waste few cycles.\nIDEA: Spin for a while, and then yield.\nHow long to spin?\nAs long as a context switch takes. Then, you\nnever wait longer than twice the optimal time.\n∙ If the mutex is released while spinning, optimal.\n∙ If the mutex is released after yield, ≤ 2 × optimal.\nRandomized algorithm [KMMO94]\nA clever randomized algorithm can achieve a\ncompetitive ratio of e/(e-1) ≈ 1.58.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLOCKING ANOMALY:\nDEADLOCK\n\nDeadlock\nHolding more than one lock at a time can\nbe dangerous:\nThread 1\nThread 2\nlock(&A);\nlock(&B);\n!critical section\"\nunlock(&B);\nunlock(&A);\nlock(&B);\nlock(&A);\n!critical section\"\nunlock(&A);\nunlock(&B);\nThe ultimate loss of performance!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConditions for Deadlock\n1. Mutual exclusion -- Each thread claims\nexclusive control over the resources it\nholds.\n2. Nonpreemption -- Each thread does\nnot release the resources it holds until\nit completes its use of them.\n3. Circular waiting -- A cycle of threads\nexists in which each thread is blocked\nwaiting for resources held by the next\nthread in the cycle.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDining Philosophers\nC.A.R. (Tony) Hoare\nEdsger Dijkstra\nIllustrative story of deadlock told by Charles\nAntony Richard Hoare based on an examination\nquestion by Edsgar Dijkstra. The story has been\nembellished over the years by many retellers.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDining Philosophers\n(c) 2008-2018 by the MIT 6.172 Lecturers\n(c) 2008-2018 by the MIT 6.172 Lecturers\n2008-2018 by the MIT 6 172 Lecturers\nwhile (1) {\nthink();\nlock(&chopstick[i].L);\nlock(&chopstick[(i+1)%n].L);\neat();\nunlock(&chopstick[i].L);\nunlock(&chopstick[(i+1)%n].L);\n}\nEach of n philosophers needs\nthe two chopsticks on\neither side of his/her\nplate to eat his/her\nnoodles.\nPhilosopher i\nds\n*mage (c) source unknown. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n\n(c) 2008-2018 by the MIT 6.172 Lecturers\nPhilosopher i\n(c) 2008-2018 by the MIT 6.172 Lecturers\n2008-2018 by the MIT 6 172 Lecturers\nwhile (1) {\nthink();\nlock(&chopstick[i].L);\nlock(&chopstick[(i+1)%n].L);\neat();\nunlock(&chopstick[i].L);\nunlock(&chopstick[(i+1)%n].L);\n}\nDining Philosophers\nEach of n philosophers needs\nthe two chopsticks on\neither side of his/her\nplate to eat his/her\nnoodles.\ner i\nOne day they all pick\nup their left chopsticks\nsimultaneously.\nStarving\nImage (c) source unknown. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n\nPreventing Deadlock\nTheorem. Assume that we can linearly order the\nmutexes L1 ⋖ L2 ⋖⋯⋖ Ln so that whenever a\nthread holds a mutex Li and attempts to lock\nanother mutex Lj, we have Li ⋖ Lj. Then, no\ndeadlock can occur.\nProof. Suppose that a cycle of waiting exists. Consider the\nthread in the cycle that holds the \"largest\" mutex Lmax in the\nordering, and suppose that it is waiting on a mutex L held by\nthe next thread in the cycle. Then, we must have Lmax ⋖ L .\nContradiction. ∎\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nDining Philosophers\nPhilosopher i\n*mage (c) source unknown. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see https://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\nwhile (1) {\nthink();\nlock(&chopstick[min(i,(i+1)%n)].L);\nlock(&chopstick[max(i,(i+1)%n)].L);\neat();\nunlock(&chopstick[i].L);\nunlock(&chopstick[(i+1)%n].L);\n}\nmons\n\nDeadlocking Cilk\nvoid main() {\ncilk_spawn foo();\nlock(&L);\ncilk_sync;\nunlock(&L);\n}\nvoid foo() {\nlock(&L);\nunlock(&L);\n}\nmain()\n[\n]\n[\n3 waits.\n[\n]\nfoo() waits.\n! Don't hold mutexes across cilk_sync's!\n! Hold mutexes only within strands.\n! As always, try to avoid nondeterministic\nprogramming (but that's not always possible).\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nTRANSACTIONAL MEMORY\n\nConcurrent Graph Computation\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConcurrent Graph Computation\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConcurrent Graph Computation\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nConcurrent Graph Computation\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHow to Deal with Concurrency?\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nHow to Deal with Concurrency?\nGaussian Elimination\nPhotographs (c) sources unknown. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nTransactional Memory*\nGaussian_Eliminate(G, v) {\natomic {\nS = neighbors[v];\nfor u ! S {\nE(G) = E(G) - {(u, v)};\nE(G) = E(G) - {(v, u)};\n}\nV(G) = V(G) - {v};\nfor u ! S\nfor u\" ! S - {u}\nE(G) = E(G) ! {(u, u\")};\n}\n}\n(c) 2008-2018 by the MIT 6.172 Lecturers\nAtomicity\n! On transaction commit,\nall memory updates in\nthe critical region\nappear to take effect at\nonce.\n! On transaction abort,\nnone of the memory\nupdates appear to take\neffect, and the trans\naction must be\nrestarted.\n! A restarted transaction\nmay take a different\ncode path.\n\nDefinitions\nConflict\nWhen two or more transactions attempt to access\nthe same location of transactional memory\nconcurrently.\nContention resolution\nDeciding which of two conflicting transactions to\nwait or to abort and restart, and under what\nconditions.\nForward progress\nAvoiding deadlock, livelock, and starvation.\nThroughput\nRun as many transactions concurrently as possible.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAlgorithm L [L16]\nAssume that the transactional-memory system\nprovides mechanisms for\n● logging reads and writes,\n● aborting and rolling back transactions,\n● restarting.\nAlgorithm L employs a lock-based approach that\ncombines two ideas:\n● finite ownership array [HF03],\n● release-sort-reacquire [L95, RFF06].\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nFinite Ownership Array\n● An array lock[0..n-1] of antistarvation (queuing)\nmutual-exclusion locks,* which support:\n- ACQUIRE(l): Grab lock l, blocking until it becomes\navailable.\n- TRY_ACQUIRE(l): Try to grab lock l, and return true or\nfalse to indicate success or failure, respectively.\n- RELEASE(l): Release lock l.\n● An owner function h: U → {0, 1, ..., n-1} mapping\nthe space U of memory locations to indexes in lock.\n● To lock location x ∈ U, acquire lock[h(x)].\n*For greater generality, one can use reader/writer locks.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRelease-Sort-Reacquire\nBefore accessing a memory location x, try to acquire\nlock[h(x)] greedily. On conflict (i.e., the lock is\nalready held):\n1. Roll back the transaction (without releasing locks).\n2. Release all locks with indexes larger than h[x].\n3. Acquire lock[h(x)], blocking if already held.\n4. Reacquire the released locks in sorted order,\nblocking if already held.\n5. Restart the transaction.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nAlgorithm L\nSAFE_ACCESS(x, L)\nif h(x) ! L\nM = {i \" L : i > h(x)}\nL = L # {h(x)}\nif M == $\nACQUIRE(lock[h(x)]) // blocking\nelseif TRY_ACQUIRE(lock[h(x)]) // nonblocking\n// do nothing\nelse\nroll back transaction state (without releasing locks)\nfor i \" M\nRELEASE(lock[i])\nACQUIRE(lock[h(x)]) // blocking\nfor i \" M in increasing order\nACQUIRE(lock[i]) // blocking\nrestart transaction // does not return\n16 access location x\nSet of local\nlock-indexes.\nGlobal finite\nownership\narray.\nLocks held with\nindexes larger\nthan h(x).\nOwner\nfunction.\nSafely access a memory location x within a transaction having local\nlock-index set L.\n! At transaction start, the transaction's lock-index set L is initialized\nto the empty set: L = $.\n! When the transaction completes, all locks with indexes in L are\nreleased.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nForward Progress (1)\nBefore accessing a memory location x, try to acquire\nlock[h(x)] greedily. On conflict (i.e., the lock is\nalready held):\n1. Roll back the transaction (without releasing locks).\n2. Release all locks with indexes larger than h[x].\n3. Acquire lock[h(x)], blocking if already held.\n4. Reacquire the released locks in sorted order,\nblocking if already held.\n5. Restart the transaction.\nNo deadlocks\nA transaction only blocks when waiting for a lock larger\nthan any of the locks it already holds ⇒ no deadly embrace,\ni.e., no cycle of blocking.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nForward Progress (2)\nBefore accessing a memory location x, try to acquire\nlock[h(x)] greedily. On conflict (i.e., the lock is\nalready held):\n1. Roll back the transaction (without releasing locks).\n2. Release all locks with indexes larger than h[x].\n3. Acquire lock[h(x)], blocking if already held.\n4. Reacquire the released locks in sorted order,\nblocking if already held.\n5. Restart the transaction.\nNo livelocks or starvation\nEach time a transaction restarts, it holds at least one more\nlock than it held the previous time. Thus, a transaction can\nbe attempted at most n times, where n is the size of the\nownership array.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nRemarks\nProperly choosing the length n of the ownership-array\nis crucial:\n● The smaller n is, the more the false contention.\n● The larger n is, the weaker the forward-progress guarantee.\n● If the owner function h is random, by the birthday paradox,\nthe number of \"false\" conflicts is at most 1 if n = m2/2, where m\nis the total number of shared-memory locations in all\nconcurrently running transactions.\nAs a practical matter, timestamp-based algorithms\nseem to be the preferred method for guaranteeing\nforward progress:\n● wound-wait and wait-die [RSL78],\n● TL2 [DSS06],\n● provable bounds [GHP05].\nBut these algorithms tend to be complex.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLOCKING ANOMALY:\nCONVOYING\n\nConvoying\nA lock convoy occurs when multiple threads of\nequal priority contend repeatedly for the same lock.\nExample: Performance bug in MIT-Cilk\nWhen random work-stealing, each thief grabs a\nmutex on its victim's deque:\n∙ If the victim's deque is empty, the thief releases the\nmutex and tries again at random.\n∙ If the victim's deque contains work, the thief steals\nthe topmost frame and then releases the mutex.\nPROBLEM: At start-up, most thieves quickly\nconverge on the worker containing the initial\nstrand, creating a convoy.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\n: busy worker\n3 : successful steal in progress\n: idle worker\n: dependency from onto\nthe lock on 's\n'\ndeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\n: busy worker\n3 : successful steal in progress\n: idle worker\n: dependency from onto\nthe lock on 's deque\n'\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\n: busy worker\n3 : successful steal in progress\n: idle worker\n: dependency from onto\nthe lock on 's deque\n'\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\n: busy worker\n3 : successful steal in progress\n: idle worker\n: dependency from onto\nthe lock on 's deque\n'\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\n: busy worker\n: idle worker\n: successful steal in progress\n: dependency from\nonto\n\nthe lock on 's\n'\ndeque\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nPerformance Bug in MIT-Cilk\nThe work now gets distributed\nslowly as each thief serially\nobtains Processor 1's mutex.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSolving the Convoying Problem\nUse the nonblocking function try_lock(),\nrather than lock():\n∙ try_lock() attempts to acquire the mutex\nand returns a flag indicating whether it\nwas successful, but it does not block on\nan unsuccessful attempt.\nIn Cilk Plus, when a thief fails to acquire\na mutex, it simply tries to steal again at\nrandom, rather than blocking.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\n!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2008-2018 by the MIT 6.172 Lecturers\nLOCKING ANOMALY:\nCONTENTION\n\nSumming Example\nint compute(const X& v);\nint main() {\nconst size_t n = 1000000;\nextern X myArray[n];\n// ...\nint result = 0;\nfor (size_t i = 0; i < n; ++i) {\nresult += compute(myArray[i]);\n}\nprintf(\"The result is: %d\\n\", result);\nreturn 0;\n}\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSumming Example in Cilk\nint compute(const X& v);\nint main() {\nconst size_t n = 1000000;\nextern X myArray[n];\n// ...\nint result = 0;\ncilk_for (size_t i = 0; i < n; ++i) {\nresult += compute(myArray[i]);\n}\nprintf(\"The result is: %d\\n\", result);\nreturn 0;\n}\nt n = 1000000;\nArray[n];\n= 0;\nize_t i = 0; i < n; ++i) {\ncompute(myArray[i]);\nRace!\nWork/span theory\nT1(n) = !(n)\nT\"(n) = !(lg n)\nTP(n) = O(n/P + lg n)\nAssume !(1)\nwork.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMutex Solution\n#include <pthread.h>\nint compute(const X& v);\nint main() {\nconst size_t n = 1000000;\nextern X myArray[n];\n// ...\nint result = 0;\npthread_spinlock_t slock;\npthread_spin_init(&slock, 0);\ncilk_for (size_t i = 0; i < n; ++i) {\npthread_spin_lock(&slock);\nresult += compute(myArray[i]);\npthread_spin_unlock(&slock);\n}\nprintf(\"The result is: %d\\n\", result);\nreturn 0;\n}\nLock contention\n! no parallelism!\nContention\nT1(n) = \"(n)\nT#(n) = \"(lg n)\nTP(n) = $(n)\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nScheduling with Mutexes\nGreedy scheduler:\nTP ≤ T1/P + Tinf + B ,\nwhere B is the bondage -- the total time of\nall critical sections.\nThis upper bound is weak, especially if\nmany small mutexes each protect different\ncritical regions. Little is known theoretically\nabout lock contention.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nGolden Rule of Parallel Programming\nNever r write nondeterministic\nparallel programs.\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nSilver Rule of Parallel Programming\nNever r write nondeterministic\nparallel programs\np\n--\np\ng\n-- but if you must --\nst\ny\nbut\nyou\nust\nb\nst\nalways devise a test strategy\na\nays de\nse a test st ategy\nto manage the nondeterminism!\n(c) 2008-2018 by the MIT 6.172 Lecturers\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.172 Performance Engineering of Software Systems, Lecture 17: Synchronization Without Locks",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/fc3b846f6e13e15e4982a92369ca3825_MIT6_172F18_lec17.pdf",
      "content": "!\"##$\n%&'&(\n\"#)*+)$#)*+,*-./01 !\n(c) 2012-2018 by the Lecturers of MIT 6.172\n6.172\nPerformance\nEngineering\nof Software\nSystems\nLECTURE 17\nSYNCHRONIZATION\nWITHOUT LOCKS\nCharles E. Leiserson\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nSEQUENTIAL\nCONSISTENCY\n\nMemory Models\nInitially, '&,&(&,&-.\nProcessor 0\nProcessor 1\n!\"#&$%&'&\n!\"#$%&\n!\"#&(%&)*(+&\n!'$()\n!\"#&$%&(&\n!\"#$%&\n!\"#&'%&)*'+&\n!'$()\nQ. Is it possible that Processor 0's )*(+&and\nProcessor 1's )*'+&both contain the value 0 after\nthe processors have both executed their code?\nA. It depends on the memory model: how memory\noperations behave in the parallel computer\nsystem.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nSequential Consistency\n\"[T]he result of any execution is the same as\nif the operations of all the processors were\nexecuted in some sequential order, and the\noperations of each individual processor\nappear in this sequence in the order specified\nby its program.\" -- Leslie Lamport [1979]\n∙ The sequence of instructions as defined by a processor's\nprogram are interleaved with the corresponding sequences\ndefined by the other processors' programs to produce a global\nlinear order of all instructions.\n∙ A LOAD instruction receives the value stored to that address by\nthe most recent STORE instruction that precedes the LOAD,\naccording to the linear order.\n∙ The hardware can do whatever it wants, but for the execution\nto be sequentially consistent, it must appear as if LOAD's and\nSTORE's obey some global linear order.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nExample\nInitially, #&%&'&%&(.\nProcessor 0\nProcessor 1\nInterleavings\n%eax\n%ebx\n)*+&,-&#&\n;Store\n)*+&'-&!\"'$& ;Load\n)*+&,-&'&\n;Store\n)*+&#-&!\"#$& ;Load\n,&\n(&\n,&\n,&\n,&\n,&\n,&\n,&\n,&\n,&\n(&\n,&\nSequential consistency implies that no\nexecution ends with !\"#$&%&!\"'$&%&(.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nReasoning about Sequential Consistency\n∙ An execution induces a \"happens before\" relation,\nwhich we shall denote as -.\n∙ The - relation is linear, meaning that for any two\ndistinct instructions x and y, either x - y or y - x.\n∙ The - relation respects processor order, the order\nof instructions in each processor.\n∙ A LOAD from a location in memory reads the value\nwritten by the most recent STORE to that location\naccording to -.\n∙ For the memory resulting from an execution to be\nsequentially consistent, there must exist such a\nlinear order - that yields that memory state.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nMUTUAL EXCLUSION\nWITHOUT LOCKS\n\nMutual-Exclusion Problem\nRecall\nA critical section is a piece of code that accesses\na shared data structure that must not be accessed\nby two or more threads at the same time (mutual\nexclusion).\nMost implementations of mutual exclusion\nemploy an atomic read-modify-write instruction\nor the equivalent, usually to implement a lock:\n- e.g., xchg, test-and-set, compare-and-swap, load\nlinked-store-conditional.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nMutual-Exclusion Problem\nQ. Can mutual exclusion be implemented with\nLOAD's and STORE's as the only memory\noperations?\nA. Yes, Theodorus J. Dekker and Edsgar Dijkstra\nshowed that it can, as long as the computer\nsystem is sequentially consistent.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nPeterson's Algorithm\nwidget\nAlice\nfrob\nborf\nBob\nx\n(c) 2012-2018 by the Lecturers of MIT 6.172\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\n!\"#$%&'()'!!\"#$%&'%&()*+#,+-.&)\n*++,'-.!/0&1'2'3/,1%)'\n*++,'4.!/0&1'2'3/,1%)'\n%056'7-8'49'&5:0)'\nAlice\nBob\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\n\nPeterson's Algorithm\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\nIntuition\n! If Alice and Bob both try to enter the critical section,\nthen whoever writes last to !\"#$ spins and the other\nprogresses.\n! If only Alice tries to enter the critical section, then she\nprogresses, since %&'($!) is false.\n! If only Bob tries to enter the critical section, then he\nprogresses, since *&'($!) is false.\nBut we can do better!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nTheorem. Peterson's algorithm achieves\nmutual exclusion on the critical section.\nProof.\n∙ Assume for the purpose of contradiction that both\nAlice and Bob find themselves in the critical\nsection together.\n∙ Consider the most-recent time that each of them\nexecuted the code before entering the critical\nsection.\n∙ We shall derive a contradiction.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\n! WLOG, assume that Bob was the last to write to !\"#$:\nwriteA(!\"#$%&%') ! writeB(!\"#$%&%() .\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\n! WLOG, assume that Bob was the last to write to !\"#$:\nwriteA(!\"#$%&%') ! writeB(!\"#$%&%() .\n! Alice's program order:\nwriteA(()*+$!,%&%!#\"-) ! writeA(!\"#$%&%') .\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\n! WLOG, assume that Bob was the last to write to !\"#$:\nwriteA(!\"#$%&%') ! writeB(!\"#$%&%() .\n! Alice's program order:\nwriteA(()*+$!,%&%!#\"-) ! writeA(!\"#$%&%') .\n! Bob's program order:\nwriteB(!\"#$%&%() ! readB(()*+$!,) ! readB(!\"#$) .\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nProof of Mutual Exclusion\nAlice\nBob\nA_wants = true;\nturn = B;\nwhile (B_wants && turn==B);\nfrob(&x); //critical section\nA_wants = false;\n= tr\n= true;\nwants && tu\n= tr\nB_wants = true;\nturn = A;\nwhile (A_wants && turn==A);\nborf(&x); //critical section\nB_wants = false;\nts =\n= A\nrn\nA);\ntrue;\ntrue;\nants && tu\ntttrue\ntrue\ntrue;;;\nA;\n! WLOG, assume that Bob was the last to write to !\"#$:\nwriteA(!\"#$%&%') ! writeB(!\"#$%&%() .\n! Alice's program order:\nwriteA(()*+$!,%&%!#\"-) ! writeA(!\"#$%&%') .\n! Bob's program order:\nwriteB(!\"#$%&%() ! readB(()*+$!,) ! readB(!\"#$) .\n! What did Bob read?\n()*+$!,.%!#\"-%\nBob should spin. Contradiction. \"\n!\"#$.%\n(%\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nStarvation Freedom\nTheorem: Peterson's algorithm guarantees\nstarvation freedom: While Alice wants to execute\nher critical section, Bob cannot execute his critical\nsection twice in a row, and vice versa.\nProof. Exercise. !\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nRELAXED MEMORY\nCONSISTENCY\n\nMemory Models Today\n∙ No modern-day processor implements sequential\nconsistency.\n∙ All implement some form of relaxed consistency.\n∙ Hardware actively reorders instructions.\n∙ Compilers may reorder instructions too.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nInstruction Reordering\n!\"#&$%&'&&&&&&!\"#$%&\n!\"#&(%&)*(+&\n!'$()\n!\"#&(%&)*(+&\n!'$()\n!\"#&$%&'&&&&&&!\"#$%&\nProgram Order\nExecution Order\nQ. Why might the hardware or compiler decide to\nreorder these instructions?\nA. To obtain higher performance by covering load\nlatency -- instruction-level parallelism.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nInstruction Reordering\n$%&\"'(\"!\"\"\"\"\"\"!\"#$%&\n$%&\"#(\")*#+\"\n!'$()\n$%&\"#(\")*#+\"\n!'$()\n$%&\"'(\"!\"\"\"\"\"\"!\"#$%&\nProgram Order\nExecution Order\nQ. When is it safe for the hardware or compiler to\nperform this reordering?\nA. When !\"! #.\nA\". And there's no concurrency.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nHardware Reordering\nMemory\nSystem\nLoad Bypass\nProcessor\nNetwork\nStore Buffer\n! The processor can issue !\"#$%'s faster than the\nnetwork can handle them \" store buffer.\n! Since a &#'( can stall the processor until it is satisfied,\nloads take priority, bypassing the store buffer.\n! If a &#'( address matches an address in the store\nbuffer, the store buffer returns the result.\n! Thus, a &#'( can bypass a !\"#$% to a different address.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nx86-64 Total Store Order\nInstruction Trace\n!\"#$%&\n!\"#$%'\n(#)*&\n!\"#$%+\n!\"#$%,\n(#)*+\n(#)*'\n(#)*,\n(#)*-\nHouse rules:\n!\" #$%&'s are not reordered with #$%&'s.\n'\" ()$*+'s are not reordered with ()$*+'s.\n,\" ()$*+'s are not reordered with prior\n#$%&'s.\n4. A #$%& may be reordered with a prior\n()$*+ to a different location but not\nwith a prior ()$*+ to the same location.\n5. L$%&'s and ()$*+'s are not reordered\nwith #$-. instructions.\n/\" ()$*+'s to the same location respect a\nglobal total order.\n0\" #$-. instructions respect a global total\norder.\n8. Memory ordering preserves transitive\nvisibility (\"causality\").\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nx86-64 Total Store Order\nInstruction Trace\nL\nO\nA\nD\nS\nHouse rules:\n!\" #$%&'s are not reordered with #$%&'s.\n'\" ()$*+'s are not reordered with ()$*+'s.\n,\" ()$*+'s are not reordered with prior\n#$%&'s.\nation.\n5. L\nred\n/\" ()$*+'s to the same location respect a\nglobal total order.\n0\" #$-. instructions respect a global total\norder.\n8. Memory ordering preserves transitive\nvisibility (\"causality\").\n!\"#$%&\n!\"#$%'\n(#)*&\n!\"#$%+\n!\"#$%,\n(#)*+\n(#)*'\n(#)*,\n(#)*-\n4. A #$%&\nb\nrd\nd ith\nprior\n()$*+\nnot\nwi\nwith\nuctions.\nTotal Store Ordering\n(TSO) is weaker than\nsequential consistency.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nImpact of Reordering\nProcessor 0\nProcessor 1\n!\"#&$%&'&&&&&&!\"#$%&\n!\"#&(%&)*(+&\n!'$()\n!\"#&$%&(&&&&&&!\"#$%&\n!\"#&'%&)*'+&\n!'$()\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nImpact of Reordering\nProcessor 0\nProcessor 1\n)*+&,-&#&&&&&&!\"#$%&\n)*+&'-&!\"'$&\n!'$()\n)*+&,-&'&&&&&&!\"#$%&\n)*+&#-&!\"#$&\n!'$()\n)*+&'-&!\"'$&\n!'$()\n)*+&,-&#&&&&&&!\"#$%&\n)*+&#-&!\"#$&\n!'$()\n)*+&,-&'&&&&&&!\"#$%&\nThe ordering !2, 4, 1, 3\" produces !\"#$&%&!\"'$&%&(.\nInstruction reordering violates\nsequential consistency!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nFurther Impact of Reordering\nPeterson's algorithm revisited\nAlice\nBob\n#&'()*+101*23451\n*32)101%51\n'678419%&'()*+1::1*32)00%;51\n<2=>9:?;51!!\"#$%$\"&'()*\"%$+,(\n#&'()*+101<(8+451\n%&'()*+101*23451\n*32)101#51\n'678419#&'()*+1::1*32)00#;51\n>=2<9:?;51!!\"#$%$\"&'()*\"%$+,(\n%&'()*+101<(8+451\n! The !\"#$'s of %&'()*+1and #&'()*+1can be reordered\nbefore the ,-\"./'s of #&'()*+1and %&'()*+,\nrespectively.\n! Both Alice and Bob might enter their critical sections\nsimultaneously!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nMemory Fences\n∙ A memory fence (or memory barrier ) is a hardware\naction that enforces an ordering constraint between the\ninstructions before and after the fence.\n∙ A memory fence can be issued explicitly as an\ninstruction (x86: mfence) or be performed implicitly by\nlocking, exchanging, and other synchronizing\ninstructions.\n∙ The Tapir/LLVM compiler implements a memory fence\nvia the function atomic_thread_fence() defined in the\nC header file stdatomic.h.*\n∙ The typical cost of a memory fence is comparable to\nthat of an L2-cache access.\n*See http://en.cppreference.com/w/c/atomic .\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nRestoring Consistency\nAlice\nBob\n!\"#$%&')()&*+,-)\n&+*%)().-)\n#/01,)2.\"#$%&')33)&+*%((.4-)\n5*672384-)!!\"#$%$\"&'()*\"%$+,(\n!\"#$%&')()5$1',-)\n.\"#$%&')()&*+,-)\n&+*%)()!-)\n#/01,)2!\"#$%&')33)&+*%((!4-)\n76*52384-)!!\"#$%$\"&'()*\"%$+,(\n.\"#$%&')()5$1',-)\nMemory fences can restore sequential consistency.\n!\"#$%&')()&*+,-)\n&+*%)().-)\n$&690:\"&/*,$;\"5,%:,24-)\n#/01,)2.\"#$%&')33)&+*%((.4-)\n5*672384-)!!\"#$%$\"&'()*\"%$+,(\n!\"#$%&')()5$1',-)\n.\"#$%&')()&*+,-)\n&+*%)()!-)\n$&690:\"&/*,$;\"5,%:,24-)\n#/01,)2!\"#$%&')33)&+*%((!4-)\n76*52384-)!!\"#$%$\"&'()*\"%$+,(\n.\"#$%&')()5$1',-)\nWell, sort of. You also need to make sure\nthat the compiler doesn't screw you over.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nRestoring Consistency\nAlice\nBob\n-./$0%1323%)4'53\n%4)0323653\n$%\"7&8.%9)'$:.('08'+,53\n/9&#'3+6./$0%13;;3%4)0226,53\n$173!\"#$%&#'+<<===<7'7\")><,53\n()\"*+;?,53!!\"#$%$\"&'()*\"%$+,(\n$173!\"#$%&#'+<<===<7'7\")><,53\n-./$0%1323($#1'53\n6./$0%1323%)4'53\n%4)0323-53\n$%\"7&8.%9)'$:.('08'+,53\n/9&#'3+-./$0%13;;3%4)022-,53\n$173!\"#$%&#'+<<===<7'7\")><,53\n*\")(+;?,53!!\"#$%$\"&'()*\"%$+,(\n$173!\"#$%&#'+<<===<7'7\")><,53\n6./$0%1323($#1'53\nBack in the day, in addition to the memory fence:\n! you must declare variables as !\"#$%&#'3to prevent the\ncompiler from optimizing away memory references;\n! you need compiler fences around ()\"*+,3and *\")(+,3to\nprevent compiler reordering.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nRestoring Consistency with C11\nAlice\nBob\n!\"#$%&'(\"#)*+,-'.!/\"(01\")2*341\n!\"#$%&'(\"#)*+,\"2)/015341\n.6%7*1+!\"#$%&'7#!8+,5'.!/\"(31,,1\n!\"#$%&'7#!8+,\"2)/3995341\n:)#;+,<341!!\"#$%$\"&'()*\"%$+,(\n!\"#$%&'(\"#)*+,-'.!/\"(01:!7(*341\n!\"#$%&'(\"#)*+,5'.!/\"(01\")2*341\n!\"#$%&'(\"#)*+,\"2)/01-341\n.6%7*1+!\"#$%&'7#!8+,-'.!/\"(31,,1\n!\"#$%&'7#!8+,\"2)/399-341\n;#):+,<341!!\"#$%$\"&'()*\"%$+,(\n!\"#$%&'(\"#)*+,5'.!/\"(01:!7(*341\nThe C11 language standard defines its own weak\nmemory model, in which you can control hardware and\ncompiler reordering of memory operations by:\n! Declaring variables as '-\"#$%&; and\n! Using the functions !\"#$%&'7#!8+3, !\"#$%&'(\"#)*+3,\netc. as needed.\nSee 6\"\"=>??*/@&==)*:*)*/&*@&#$?.?&?!\"#$%&.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nImplementing General Mutexes\nTheorem [Burns-Lynch]. Any !-thread deadlock-free\nmutual-exclusion algorithm using only \"#$% and\n&'#() memory operations requires !(!) space.\nTheorem [Attiya et al.]: Any !-thread deadlock-free\nmutual-exclusion algorithm on a modern\nmachine must use an expensive operation such\nas a memory fence or an atomic compare-and-\nswap operation.\nThus, hardware designers are justified when they\nimplement special operations to support atomicity.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nCOMPARE-AND-SWAP\n\nThe Lock-Free Toolbox\nMemory operations\n! !\"#$\n! %&\"'(\n! )#% (compare-and-swap)\n(c) 2012-2018 by the Lecturers of MIT 6.172\nThis image is in the public domain.\n\nCompare-and-Swap\nThe compare-and-swap operation is provided by the\n!\"#$!%&>instruction on x86-64. The C header file\n'()*(+\",!-%>provides ./0>via the built-in function\n*(+\",!1!+\"#*2313$!%*4&31'(2+4&56>\nwhich can operate on various integer types.*\n! Executes atomically.\n! Implicit fence.\nSpecification\n;++<>./05=>?$@>=>+<)@>=>43:6>A>\n,9>5?$>BB>+<)6>A>\n?$>B>43:C>\n23(D24>(2D3C>\nE>\n23(D24>9*<'3C>\nE>\n* See %((#78834-!##2393234!3-!+\"8:8!##8*(+\",!8*(+\",!1!+\"#*2313$!%*4&3>.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nMutex Using CAS\nTheorem. An *-thread deadlock-free\nmutual-exclusion algorithm using 678%\ncan be implemented using !(1) space.\nProof.\n!\"#$%&\"'()#*+%,&\"'(-!./0%1%\n23#&4%)5678),&\"'(-!./9%:.&;49%+/<400=%\n>%\n!\"#$%<*&\"'()#*+%,&\"'(-!./0%1%\n,&\"'(-!./%?%:.&;4=%\n>%\nJust the space for the mutex itself. \"\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nSumming Problem\n!\"#1$%&'(#)*$%\"+#1,-1./01\n!\"#1&2!\"*/131\n$%\"+#1!\"#1\"141566666601\n)7#)8\"1,1&9:8829;\"<01\n==1>>>1\n!\"#18)+(?#141601\n$!?@AB%81*!\"#1!141601!1C1\"01DD!/131\n8)+(?#1D41$%&'(#)*&9:8829;!</01\nE1\n'8!\"#B*1FGH)18)+(?#1!+I1JBK\"LM18)+(?#1/01\n8)#(8\"1601\nE1\n'\n* 9\nB*1FGH)18)+(?#1!+I1JBK\n\"1601\nRace!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nMutex Solution\n!\"#1$%&'(#)*$%\"+#1,-1./01\n!\"#1&2!\"*/131\n$%\"+#1!\"#1\"141566666601\n)7#)8\"1,1&9:8829;\"<01\n&(#)71=01\n>>1???1\n!\"#18)+(@#141601\n$!@ABC%81*!\"#1!141601!1D1\"01EE!/131\n!\"#1#)&'141$%&'(#)*&9:8829;!</01\n=?@%$A*/01\n8)+(@#1E41#)&'01\n=?(\"@%$A*/01\nF1\n'8!\"#C*1GHI)18)+(@#1!+J1KCL\"MN18)+(@#1/01\n8)#(8\"1601\nF1\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\n!\"#1$%&'(#)*\n!\"#1&2!\"*/131\n$%\"+#1!\"#1\n)7#)8\"1,1\n&(#)71=01\n>>1???1\n!\"#18)+(@#1\n$!@ABC%81\n!\"#1#)&'1\n=?@%$A*/0\n8)+(@#1\n#\n8)+(\n$!@ABC%8\n!\"#1#)\n=?@%$A\n@#\n#\n/\n\"\n,\n(\n(#\n*/\n!\"\n,\n=0\n)+(\nMutex Solution\nYet all we want\nis to atomically\nexecute a =O:P1\nof 71followed\nby a store of 7.\n$%\"+#1,-1./01\n\"141566666601\n&9:8829;\"<01\nQ. What happens if the\noperating system\nswaps out a loop\niteration just after it\nacquires the mutex?\n*!\"#1!141601!1D1\"01EE!/131\n41$%&'(#)*&9:8829;!</01\nE41#)&'01\n=?(\"@%$A*/0\nF1\n'8!\"#C*1GHI)18)+(@#1!+J1KCL\"MN18)+(@#1/01\n8)#(8\"1601\nF1\nA. All other loop\niterations must wait.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nCAS Solution\n!\"#$%&'()#$*$+,$\n-!)./01%$2!\"#$!$*$+,$!$3$\",$44!5$6$\n!\"#$#&78$*$-178(#&279:%%;9<!=5,$\n!\"#$1)>?$\"&@,$\n>1$6$\n1)>$*$%&'()#,$\n\"&@$*$1)>$4$#&78,$\nA$@B!)&$2CD:E2F%&'()#?$1)>?$\"&@55,$\nA$\nQ. Now what happens if\nthe operating system\nswaps out a loop\niteration?\nA. No other loop\niteration needs to\nwait. The algorithm\nis nonblocking.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nLOCK-FREE\nALGORITHMS\n\nLock-Free Stack\nhead:\n!\"#$%\",&'(),*,\n&'()+,-).\"/,\n0-\",(1\"1/,\n2/,\n!\"#$%\",3\"1%4,*,\n&'()+,5)1(/,\n!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nLock-Free Push\n!\"#$-%&'()*\"$+,-.\"$+/-0-\n$\"-0-\n.\"$+12.+34-5-(+6$7-\n8-9(#:+-);<=>)?(+6$@-.\"$+12.+34@-.\"$+//7-\n8-\nhead:\nnode:\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nLock-Free Push with Contention\n!\"#$-%&'()*\"$+,-.\"$+/-0-\n$\"-0-\n.\"$+12.+34-5-(+6$7-\n8-9(#:+-);<=>)?(+6$@-.\"$+12.+34@-.\"$+//7-\n8-\nhead:\nThe compare-and-swap fails!\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nLock-Free Pop\n!\"#$%&'\"'()&*&\n!\"#$%&+,--$./&0&1$2#3&\n4156$&(+,--$./)&*&\n57&(89:(;1$2#<&+,--$./<&+,--$./=>.$?/))&@-$2A3&\n+,--$./&0&1$2#3&\nB&\n-$/,-.&+,--$./3&\nB&\nhead:\ncurrent:\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nCompare and compare-and-swap\nCompare-and-swap acquires a cache line in exclusive\nmode, invalidating the cache line in other caches.\nResult: High contention if all processors are doing\nCAS's to same cache line.\nBetter way: First read if value at memory location\nchanged before doing CAS, and only do CAS if value\ndidn't change.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nLock-Free Push and Pop\n!\"#$%&'\"'()&*&\n!\"#$%&+,--$./&0&1$2#3&\n4156$&(+,--$./)&*&\n57&(1$2#&00&+,--$./&88&\n9:;(81$2#<&+,--$./<&+,--$./=>.$?/))&@-$2A3&\n+,--$./&0&1$2#3&\nB&\n-$/,-.&+,--$./3&\nB&\nC\"5#&',D1(!\"#$%&.\"#$)&*&\n#\"&*&\n.\"#$=>.$?/&0&1$2#3&\nB&4156$&(1$2#&E0&.\"#$=>.$?/&FF&\nE9:;(81$2#<&.\"#$=>.$?/<&.\"#$))3&\nB&\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nLock-Free Data Structures\n! Efficient lock-free algorithms are known for a variety\nof classical data structures (e.g., linked lists, queues,\nskip lists, hash tables).\n! In theory, a thread might starve. Because of\ncontention, its operation might never complete. In\npractice, starvation rarely happens.\n! Transactional memory is revolutionizing this area.\n! Allows executing a block of code atomically.\n(c) 2012-2018 by the Lecturers of MIT 6.172\ners of MIT 6.172\nPractical Issues\n! Memory management.\n! Contention.\n! The ABA problem.\n\n!\"##$*\n%&'&(*\n\"#)*+)$#)*+,*-./01*!\n(c) 2012-2018 by the Lecturers of MIT 6.172\nTHE ABA PROBLEM\n\nABA Example\n!\"#$%\n&'((\")*%\n1. Thread 1 begins to pop the node containing +,, but\nstalls after reading &'((\")*-.)\"/*.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nABA Example\n!\"#$%\n&'((\")*%\n1. Thread 1 begins to pop the node containing +,, but\nstalls after reading &'((\")*-.)\"/*.\n2. Thread 2 pops the node containing +,.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nABA Example\n!\"#$%\n&'((\")*%\n1. Thread 1 begins to pop the node containing +,, but\nstalls after reading &'((\")*-.)\"/*.\n2. Thread 2 pops the node containing +,.\n3. Thread 2 pops the node containing 01.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nABA Example\n!\"#$%\n&'((\")*%\n1. Thread 1 begins to pop the node containing +,, but\nstalls after reading &'((\")*-.)\"/*.\n2. Thread 2 pops the node containing +,.\n3. Thread 2 pops the node containing 01.\n4. Thread 2 pushes the node 7, reusing the node that\ncontained +,.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nABA Example\n!\"#$%\n&'((\")*%\n1. Thread 1 begins to pop the node containing +,, but\nstalls after reading &'((\")*-.)\"/*.\n2. Thread 2 pops the node containing +,.\n3. Thread 2 pops the node containing 01.\n4. Thread 2 pushes the node 7, reusing the node that\ncontained +,.\n5. Thread 1 resumes, and its 234 succeeds, removing\n5, but putting garbage back on the list.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nSolutions to ABA\nVersioning\n∙ Pack a version number with each pointer in the\nsame atomically updatable word.\n∙ Increment the version number every time the\npointer is changed.\n∙ Compare-and-swap both the pointer and the\nversion number as a single atomic operation.\nIssue\n∙ Version numbers may need to be very large.\nReclamation\n∙ Prevent node reuse while pending requests exist.\n∙ For example, prevent node 15 from being reused\nas node 7 while Thread 1 still executing.\n(c) 2012-2018 by the Lecturers of MIT 6.172\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Leiserchess",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/6f1ff098e89d9c6f1e639de093fd9495_MIT6_172F18_leiserchess.pdf",
      "content": "Leiserchess 2018\nA Laser-Chess Game\nCharles E. Leiserson and the MIT 6.172 Staff\nLeiserchess (pronounced \"LYE-sir-chess\") 2018 is a two-player laser-chess\ngame similar to Laser Chess, Khet, and previous versions of Leiserchess.\nThe\nteaching staff of the MIT class 6.172 Performance Engineering of Software Systems\ndeveloped Leiserchess 2018 (henceforth just Leiserchess) for the term final project\nin Fall 2018. The students are given a working implementation of a program to\nplay Leiserchess, and their job is to make it run as fast as possible and otherwise\nimprove its playing ability.\nOn the surface, Leiserchess is much simpler than Laser Chess or Khet in that there\nare only two kinds of pieces -- Kings and Pawns -- and all pieces move the\nsame way. A deeper complexity arises from the dynamics of how pieces interact,\nhowever, because the Kings carry their own lasers to shoot at each other and at\neach other's Pawns.\nThe result is an entertaining and challenging game that\ninvolves both tactics and strategy.\nPieces and Board\nLeiserchess is played on an 8x8 square board. Each player has one King and\nseven Pawns:\nTangerine\nTangerine\nLavender\nLavender\nKing\nPawn\nKing\nPawn\nThe King contains a laser that can be activated to shoot out of its front. Each Pawn\ncontains a mirror oriented at a 45-degree angle to the ranks and files of the board,\nwhich can deflect the beam of the laser from rank to file or vice versa.\nOpening Position\nPlay begins with the following starting position:\nLeiserchess 2018\n\n(\n'\n&\n%\n$\n#\n\"\n!\n)\n*\n+\n,\n-\n.\n/\nThe starting position.\nRules\nTangerine moves first, and then play alternates between the two players. A player\ncan only move his or her own pieces.\nAll pieces in Leiserchess move the same,\nwhether King or Pawn. A turn has two parts: moving, and firing the laser. A move is\neither a basic move or a swap move.\nBasic moves. For the first part of a turn, the player on move chooses a piece. For\na basic move, the piece can either rotate by 90, 180, or 270 degrees, or it can move\nto an empty adjacent square in any of the eight compass directions while\nmaintaining its orientation. A piece cannot both rotate and move as part of the same\nbasic move. The following diagram shows a Pawn on a square and the 11 possible\nbasic moves it can make:\noriginal\nposition\nThe 11 basic moves.\nLeiserchess 2018\n\nSwap moves. If an enemy piece occupies a square adjacent to the player's piece,\nthe two pieces swap positions, maintaining their orientations, and then the player's\npiece can make an extra basic move. Here is an example:\nInitial position with\nIntermediate position\nFinal position after\nTangerine to move.\nafter swap.\nextra basic move.\nFiring the laser.\nFor the second part of a turn, the player fires the laser, which\nshoots out of the top of the King.\nThe beam can safely bounce off the mirrored\nsurfaces of Pawns, but if the laser \"zaps\" an opaque (nonmirrored) side of a piece,\nthe zapped piece is removed from the board, no matter which player owns it. (Yes,\nyou can zap your own Pawns and even commit suicide!) If a King is zapped, the\ngame is over, and the player who owns the zapped King loses.\nAfter a player moves, he or she must always fire the laser, even if it is self-\ndestructive.\nFor example, in the position shown below on the left, if Tangerine\nunwisely rotates the Pawn on !\"/clockwise 90 degrees, as shown in the position on\nthe right, it zaps its own King on #\":\n(\n(\n'\n'\n&\n&\n%\n%\n$\n$\n#\n#\n\"\n\"\n!\n!\n)\n*\n+\n,\n-\n.\n/\n)\n*\n+\n,\n-\n.\n/\nTangerine to move.\nSuicide!\nThe Ko rule. To help ensure that the game makes progress, Leiserchess has a \"Ko\"\nrule similar to the Ko rule in the game of Go. The Leiserchess Ko rule says that a\nmove is illegal if it \"undoes\" the opponent's most recent move by returning the\nposition to the position immediately prior to the current position.\nLeiserchess 2018\n\nDraws. A draw occurs (1) if there have been 50 moves by each side without a Pawn\nbeing zapped; (2) if the same position repeats itself three times with the same side\non move; or (3) if the two players agree to a draw.\nTime control. As players become skilled, they tend to think longer. A chess clock\n(see http://en.wikipedia.org/wiki/Time_control) can be used to keep the pace up.\nIdeally, use a \"delay\" clock, such as a Fischer clock. Free chess-clock applications\nare available for many smart phones.\nLearning Leiserchess\nTo learn the game, start by clearing the board of all Pawns, and play with just the\ntwo Kings, one in each corner of the board.\nYou will discover that this endgame\nsituation can always be won by one of the two players, who can force the enemy\nKing to the edge and zap it.\nPlaying this endgame will give you a feeling for the\npower of the laser-slinging Kings. Afterwards, go on to play normal games.\nTactics. Despite the simplicity of the rules, Leiserchess has remarkably interesting\ntactics. For a King to zap the enemy King, it risks opening itself up to counterattack,\nand so shots must be artfully composed. For example, consider the following\nposition with Tangerine to move:\n(\n'\n&\n%\n$\n#\n\"\n!\n)\n*\n+\n,\n-\n.\n/\nTangerine to move.\nTangerine can zap the Lavender Pawn at $% by moving its Pawn on &' to ():\nLeiserchess 2018\n\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n,\n-\n.\n/\nTangerine's move zaps Lavender's piece on $%.\nDoing so results in immediate disaster, however, as Lavender counters by moving its\nLavender Pawn on *+ to &,, zapping the Tangerine King, and winning the game:\n!\n\"\n#\n$\n%\n&\n'\n(\n)\n*\n+\n,\n-\n.\n/\nLavender counterattacks and wins the game!\nThe Pawns at (,, (), !), !-, and #-, which Tangerine used to reflect the laser to zap\nthe Lavender Pawn at $%, are now used by Lavender in the reverse direction to zap\nthe Tangerine King. Watch out for poison Pawns!\nThis kind of tactic illustrates the \"emergent complexity\" inherent in Leiserchess,\nwhere the interaction of simple pieces engenders complex behavior. The \"reverse-\nLeiserchess 2018\n\npath\" nature of mirrors -- \"If you can see me, I can see you!\" -- produces a wealth\nof tactical issues.\nStrategy. Strategy is required for a player with a dominant position to prevail over\nthe opponent. Without a thoughtful plan, the dominant player may not be able to\nengineer a zap. Although Leiserchess is a young game, some strategic elements\nhave begun to emerge from games played so far:\n●\nKeep some Pawns in the neighborhood of your own King. A \"naked\" King is\ngenerally easy to zap.\n●\nTry to limit the mobility of the enemy King by threatening to attack the\nsquares next to it. That is, be in a position to attack the enemy King should it\nmove to an adjacent square.\n●\nAs the Kings move toward the center, a Pawn on the edge of the board can\neasily find itself unable to \"cooperate\" with other pieces in directing the path\nof the laser, rendering the Pawn next to useless.\n●\nIf the enemy King tries to \"hunker down\" by surrounding itself with its Pawns,\nyou can invade with your Pawns to disrupt the enemy King's defensive\nposition.\nRecording Board Positions and Games\nBoard positions can be recorded using a modified Forsyth-Edwards notation (FEN).\nFrom Tangerine's point of view, list the pieces rank by rank, starting with rank 7 and\nending with rank 0. Within each rank, describe the contents of each square from a\nto h as follows. Each piece is identified by a two-letter sequence describing the way\nit is facing, where upper case letters stand for Tangerine, and lower-case letters\nstand for Lavender. The Tangerine King is identified by NN, EE, SS, and WW, depending\non whether it is facing north (toward the higher-numbered ranks), east (toward\nhigher lettered files), south, or west. The Lavender King is similar, except lower-case\nletters are used. A Tangerine Pawn is identified by NE, SE, SW, and NW, depending on\nwhether its mirror is facing northeast, southeast, southwest, or northwest, and\nsimilarly, using lower case, for the Lavender Pawns.\nThe numbers 1 through 8\nindicate consecutive empty squares, and a forward slash separates ranks. After the\nboard description, the letter W or B (for White and Black, the traditional chess colors),\ndepending on whether Tangerine has the next move in the position, or Lavender,\nrespectively. For example, the opening position can be described with the following\nFEN string:\nss7/3nwse3/2nwse4/1nwse3NW1/1se3NWSE1/4NWSE2/3NWSE3/7NN W\nGames can be recorded using the following simple notation:\n!\nLeiserchess 2018\n\n●\nIf a piece is rotated, write down the square holding the piece followed by\neither L for counterclockwise, R for a clockwise, or U for a 180-degree\nrotation, e.g., g2R.\n●\nIf a piece is moved to an adjacent empty square, write down the source\nsquare followed by the destination square, e.g., h3g4.\n●\nIf a piece swaps with an enemy piece, the two squares involved are recorded\nfollowed by the notation for the additional basic move taken by the piece: L,\nR, U, or the destination square.\n●\nRecord the outcome of the game as 1-0 (Tangerine wins), 0-1 (Lavender\nwins), or 1/2-1/2 (draw).\nHere is an example of a recorded game:\n1. g4g5 e6e7\n18. g2g3 f3g3g2\n35. e5e6 c3c2\n2. g5g4 e7e6\n19. f3e4 d7L\n36. a1b1 c1L\n3. g4L\nb3L\n20. e4d3c4 a7b7\n37. b1R\nd0R\n4. g4h5 b4R\n21. c4b5 e4d5d6\n38. e0L\nc1b2\n5. f3e4 d5e4f5\n22. h4g5 g2f3\n39. b1b2c1 b1a1\n6. f2e3 f5g5\n23. e4f3e2 e4f3\n40. c1b2 a1b2a3\n7. h5g6 g5h4\n24. e2f3g3 b7a7\n41. e6U\na3R\n8. h0g0 c4R\n25. b5a4 b4a4b5\n42. e6d5 a7R\n9. e3e4 b3b2\n26. b4b5a6 d7e7f6\n43. f5e6 c2U\n10. g0f0 d6d7\n27. g5f6e5 a7L\n44. a1b1 c7U\n11. d5c5b5 h4g3f2\n28. g6g5f5 d6d7L\n45. e0f0 c2d2\n12. e2f2g2 c4b3\n29. f0e1 d7c7\n46. b1c1 c7b7\n13. e4d5R d7L\n30. e1e0 b4c3\n47. d5L\nd2e3\n14. d5e6e7 b2b1\n31. f5g6f6 b3c2\n48. f0g1 e3f3\n15. e1d0 e4d3\n32. f6f5 c2d1c0\n49. g1h1 f3g3\n16. b5c4 d5c4c3\n33. c2b2 b1a1\n50. e6f5 g3h3\n17. d5R\ne2f3\n34. d0c0R a1b2c1\n0-1 \n!7\nLeiserchess 2018\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Project 1: Bit Hacks",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/271aca8dd9fe0ff159ae31396a180f17_MIT6_172F18_project1.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 4\nProject 1: Bit Hacks\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\nThis project provides you with an opportunity to learn how to improve the performance of programs\nusing the perf tool and to experiment with word-level parallelism: the abstraction of a computer word\nas a vector of bits on which bitwise arithmetic and logical operations can be performed.\nWord-level parallelism -- more colloquially called bit hacks -- can have a dramatic impact on per\nformance. This project will also give you the opportunity to develop and practice your C programming\nskills.\nGenerally, when you are concerned about the performance of a program, the best approach is to imple\nment something correct and then evaluate it. In some cases, many parts of this initial implementation (or\neven the entire thing) may be adequate for your needs. However, when you need to improve performance,\nyou must first decide where to focus your efforts. This is where profiling becomes useful. Profiling can help\nyou identify the performance bottlenecks in your program.\nDeliverables\n2 Team formation\n2 Team contract\n2 Beta release with 20 tests\n2 Progress reports\n2 Beta write-up\n2 Final release\n2 Final write-up\n2 Getting started\nOn your second week of work at Snailspeed Ltd., your boss asks you build a faster version of a\nprogram sold by Snailspeed's competitor Diddled Bits, Ltd. Snailspeed thinks that there is great\ndemand for a faster bit rotator and would like you to develop a faster product to compete with\nDiddled Bits. You suspect that you can exploit word-level parallelism to greatly speed up their\nbit rotation program. The author of Snailspeed's program, Ben Bitdiddle, is an ex-Snailspeed\nemployee. For this reason Snailspeed has a prototype of his bit rotation program. Currently, the\nprototype runs very slowly. It's your job to analyze its performance, and make it faster.\n\nHandout 4 -- Project 1: Bit Hacks\nMany programs operate under tight constraints, and getting respectable performance under\nthese circumstances can be especially challenging. Part of Snailspeed Ltd.'s product portfolio\ntargets mobile and embedded devices such as cell phones. One such application requires that\nvarious operations on bit strings be performed within a large data buffer.\nSince you can't change Snailspeed's culture overnight, please don't perform multithreaded\nparallelization. We'll learn more about these techniques later in the course. Code in standard\nC -- for example, no inline assembly directives to the compiler. Furthermore, please don't use\ncompiler intrinsics or libraries that use assembly or intrinsics to achieve the same effect. We want\nto see you use standard C. The x86 machines on which you will be running are little endian --\nbytes of words are stored in memory with least significant bytes first. You need not make your\ncode portable to big-endian machines. You can post on Piazza if you have a question about the\nrules.\nForming your team\nYou should begin by finding a teammate.\nTeam contract\nYou and your teammate must agree to a team contract. A team contract is an agreement among\nteammates about how your team will operate -- a set of conventions that you plan to abide by.\nThe questions below will help you consider what might go into your team contract. You should\nalso think back to good or bad aspects of team-project experiences you've already had.\nBelow are some questions to consider for your team contract broken into three categories:\nmeeting and communication norms, work norms, and decision making. You needn't address all\nthe questions, which are simply suggestions. Focus on the issues that your team considers most\nimportant. A skimpy team contract is a bad idea. The TA's will review all contracts. If you have\nlittle in your team contract and it ends up that your team has problems working together later in\nthe group project, your TA will not be as sympathetic to your plight. All team members should\nwrite their names at the end of the contract, to indicate that they agree with it.\nMeeting and communication norms\n- How often will the team plan to meet outside of class? How long do you anticipate meet\nings will be? What will you do if things change?\n- Where and when will outside-class meetings be held? What will you do if someone fails to\nshow for a meeting?\n\nHandout 4 -- Project 1: Bit Hacks\n- How will you communicate outside of meetings? (Email list? Real-time messaging plat\nform?)\n- If someone in the group decides to drop the class, what obligations does that person have\nto the team?\nWork norms\n- How much time per week do you anticipate it will take to make the project successful?\n- How will work be divided among team members? On which parts of the project will you\ndo pair programming?\n- What will happen if someone does not follow through on a commitment, e.g., not doing\ntheir work? What if someone gets sick?\n- How will the work be reviewed? How will you manage your code branches?\n- How will you deal with different work habits of individual team members (e.g., some\npeople like to get work done early, while others like to work under the pressure of a\ndeadline)?\nDecision making\n- Do you need unanimous consent to make a decision? What process for decision-making\nwill you use if you can't agree?\n- How will you prioritize the work to be done? How will you deal with the common situation\nin which different team members have different optimization ideas?\n- What happens if everyone does not agree on the level of commitment, e.g., some team\nmembers want an A, but others are willing to settle for a B.\n- Is it acceptable for some team members to do more or less work than others?\nSubmitting your team contract\nEach member should individually submit the team contract as a PDF document. In addition,\nonce your team repository has been formed, you should commit a copy of your team contract to\nthe top level of the repo under the name team-contract.pdf. Feel free to update the team\ncontract in the repo as needed during the project, as long as all parties agree.\nGetting the code\n\nHandout 4 -- Project 1: Bit Hacks\nWe strongly recommend that groups practice pair programming, where two partners work\ntogether with one person at the keyboard and the other serving as watchful eyes. After an\nagreed-upon time, the partners switch. This style of programming leads to bugs being caught\nearlier, and both programmers retain familiarity with the code.\nAs soon as your repo is setup, add and commit a copy of your team contract to your project\nrepo for future reference by you, the course staff, and your MITPOSSE mentors.\nCode structure\nThe code resides in the everybit directory. Take a look at bitarray.h and bitarray.c. This code\nimplements the functions needed to allocate, access, and process large strings of bits while using\na minimum of memory. In the implementation, bits are packed 8 per byte in memory, but can be\naccessed individually through the public bitarray_get() and bitarray_set() functions.\nYour job is to improve the bitarray_rotate() function programmed by Ben Bitdiddle. For the\nfollowing tasks, do not call bitarray_new() from within your implementations. You can allocate\nsmall buffers on the stack or in the BSS section (e.g., global arrays).\nYour implementation of bitarray_rotate() will be considered correct if the contents of the\nbit array, as accessed through bitarray_get_bit_sz() and bitarray_get(), is the same as after\nrunning the original, slow, implementation.\nThe timing and testing system resides in main.c and tests.c, which call your routines. Do not\nmake modifications to these two files, as they will be replaced with fresh copies when the staff\nruns your code. By the same token, do not remove or change the signatures of any of your\nfunctions that main.c or tests.c calls. In short, the provided files main.c and tests.c should\nalways compile against your code and execute correctly!\nBuilding the code\nYou can build the code by typing make. To build with debugging symbols, useful when debug\nging with gdb, type make DEBUG=1. The everybit binary accepts arguments which allows you to\n\nHandout 4 -- Project 1: Bit Hacks\nto run a benchmark composed of a long running bit rotation operation, run all tests in a given\ntest file , and run a specific test from a test file. For usage instructions you can run\n$ ./everybit\nusage: ./everybit\n-s Run a sample short (0.01s) rotation operation\n-m Run a sample medium (0.1s) rotation operation\n-l Run a sample long (1s) rotation operation\n-t tests/default\nRun all tests in the test file tests/default\n-n 1 -t tests/default\nRun test 1 in the test file tests/default\nBenchmarks\nWhen evaluating your implementation's performance, you should use the awsrun command, as\nthis is what the staff will use to grade you. You can run a sample benchmark using the command\n$ awsrun ./everybit -l\nThe ./everybit -s, -m, and -l options run benchmarks of varying lengths. Each benchmark\ngeometrically increases the number of rotated bits until your algorithm takes longer than a par\nticular threshold (.01 seconds for -s, .1 second for -m, and 1 second for -l). We will grade your\nperformance off of the tier and final tier time for a benchmark similar to -l. (This means that\nincremental improvements, even if they do not move your team to the next tier, will improve\nyour grade.)\nThe naive long-running version will not complete many tiers. But simple optimizations\nshould allow your rotation code to complete much higher tiers. You should do your final bench\nmark using awsrun.\nTesting\nFor everybit, the staff has provided a testing framework which allows you to write tests in a\nsimple textual format. Each set of tests is a file in the tests directory. We have already provided\na few simple tests in the file tests/default. You can add additional tests to this file, as well as\ninto the file tests/mytests, which will be used to determine your test-coverage grade (please\nsee below). You can also add additional test files in the tests directory in the same format.\nRemember to git add any new test files you create! You can run all test files in the tests\ndirectory with the command\n$ make test\nYou may also run the command\n$ make testquiet\nwhich will run all tests, but output information only for tests that fail.\nTesting is a fundamental component of good software engineering. You will find that having a\nregression suite for your projects speeds your ability to make performance optimizations, because\n\nHandout 4 -- Project 1: Bit Hacks\nit is easy to try something out and localize the bug, rather than spending hours trying to figure\nout where it is, or worse, never realize that you have a bug in your code.\nThe provided tests in tests/default do not provide adequate coverage (especially for faster\nand more complex implementations), and we will be looking for you to add more test cases. Your\ngoal with testing is to find bugs not only in your own code, but in the code written by others in\nthe class. In particular, your regression suite will be run against other teams' projects. If another\nteam's buggy program passes all the tests in your regression suite, you will lose points. The\nharder it is to find a bug, the more points the bug is worth, so your goal should be to find as\nmany corner cases as possible.\nWrite test cases for all the edge cases in everybit. Include some general tests, but think\ncreatively about how to keep your test suite small. You can create as many test files as you wish\nin the test directory, and the make test command will run then all.\nAs part of your code submission, edit the file tests/mytests, and place your 20 best tests in\nthe file. The staff will run your 20 tests against everyone's code for the \"test coverage\" part of\nyour grade. The more bugs you uncover in others' code, the higher your grade. (Make sure your\nown code passes your own tests!)\nResearch on software engineering shows that over 90% of bugs in code previously occurred\nin an earlier version of the same software. Whenever you find a bug in your own code that\npasses all the tests in your current regression suite, it is a good idea to add a test case for that\nbug so that you can immediately catch it if it shows up again. Thus, developing a regression\nsuite that checks for previously encountered bugs can vastly accelerate your ability to develop\ngood (and fast) software. Indeed, the course staff has observed in previous terms that the quality\nof students' regression suites has been correlated with the performance of their applications\nthemselves. So, if you want a better grade on the performance of your application, it's a good\nidea to develop a good test suite.\nWhen you encounter a bug, it's tempting to fix the bug and leave the testing for later. That's\nactually an inferior strategy. Most professional software developers follow the following simple\nbut effective methodology when a bug is found:\n1. Write a test case.\n2. Verify that the existing code fails the test case.\n3. Fix the bug.\n4. Verify that the new code passes the test case (and all the other tests in your regression\nsuite).\nIndeed, many software teams require that whenever a developer fixes a bug, a test case for the\nbug be checked into the team's software repository along with the bug fix itself.\n3 Rotating a bit string\nThe function bitarray_rotate() rotates a string of bits within a bit array by some amount to the\nleft or right. See the documentation in bitarray.h. Ben Bitdiddle's prototype is slow, however,\nperforming lots of one-bit rotations over and over again until the correct degree of rotation is\nachieved. If you try to run\n\nHandout 4 -- Project 1: Bit Hacks\n$ ./everybit -l\nyou will see that a couple of rotations on even a small buffer can take a while long to run. As a\nnote, this mode does not perform correctness/error checking, nor is it necessarily the exact set\nof rotations we will use for grading.\nYour task is to come up with a more efficient implementation for bitarray_rotate(), given\nthe rules stated earlier. Your write-up and documentation should explain clearly and succinctly\nhow your method works.\nThe most obvious way to perform a circular left rotation is to consider the string to be rotated\nto be of the form ab, where a and b are bit strings. We wish to transform ab to ba. The simplest\nmethod is to copy a to an auxiliary array, move b to its final location, then copy a from the\nauxiliary array to its final location. This method is simple, but the need for a large auxiliary\narray can be problematic for cache performance when rotating long strings.\nTo minimize the number of bit movements, a cyclic approach can be implemented, where\neach bit moves ahead by the specified amount, modulo the length of the region to be rotated,\neventually looping back to the first bit in the cycle. This strategy places all bits in the correct\nlocations while using a constant amount of auxiliary space, but memory accesses are scattered,\nwhich can adversely impact caching.\nFinally, there is a clever approach that moves every bit twice without using auxiliary memory.\nAgain treating the string to be rotated as ab, observe the identity (aRbR)R = ba, where R is the\noperation that reverses a string. The \"reverse\" operation can be accomplished using only constant\nstorage. Thus, with 3 reversals of bit strings, the string can be rotated.\nThere may also be other competitive approaches. Think, code, and profile! The TAs are happy\nto discuss your ideas with you at office hours.\n4 Evaluation\nGrade Breakdown\nWe will grade your project submission based on the following point distribution:\nBeta\nFinal\nPerformance\nTest coverage\nCorrectness\nAddressing MITPOSSE comments\nWrite-up\nTeam contract\n20%\n12%\n7%\n3%\n3%\n30%\n10%\n10%\n5%\nTotal\n45%\n55%\nThis point distribution serves as a guideline and not as an exact formula. The staff will also review\nyour Git commit logs to assess the dynamics of your team in the final submission. Please ensure\nthat each team member authors a substantial fraction of the project commits. We strongly recommend\nthat you experiment with pair programming for this assignment, as you will find that it is difficult\nto divide the work in this project into independent components.\n\nHandout 4 -- Project 1: Bit Hacks\nPerformance\nYour performance grade for your beta release is based primarily on how fast your correct program\nruns. That means you should first focus on obtaining a correct solution for the project before\noptimizing for performance, since the more test cases a submitted solution fails, the lower grade\nyour solution will get.\nFor the final version, all submissions have the opportunity to receive full credit on the per\nformance criteria. After the beta releases have been submitted, the staff will choose a baseline\nperformance goal. Any correct final submission whose performance matches or exceeds the\nbaseline will receive full credit on performance.\nTest Coverage\nWhile working on your project, you will add test cases by modifying the provided test file,\ntests/default, and by adding additional test files in the same format to the tests directory.\nTo grade your code for test coverage and correctness, we will aggregate the test files submitted\nby every group (appending a prefix to each test file to avoid naming conflicts), and then run all\nsubmitted solutions against the aggregate test suite. We will remove any test files that do not pass\nwhen run against the reference implementation we supply. For this reason, we encourage you\nto put your tests in multiple test files. In addition, we will penalize groups that submit incorrect\ntests which do not pass when run against the initial implementation -- there is no excuse for\nfailing to run your test suite against the program we supply. The more bugs your tests catch, the\nhigher your grade.\nCorrectness\nCorrectness grading is more subjective than coverage to allow us to evaluate the severity of\nbugs. Generally, grades should fall into the following categories. If all tests pass, you will get\nfull marks. If your implementation is essentially correct, but misses some corner cases, you\ncan expect between 80% to 90% of the points. If your implementation fails more tests but the\nperformance tests still run, you can expect 60% to 80%. If you cannot run the performance tests,\nthen you will get a low correctness grade.\nIf you are aware of a correctness bug in your own implementation, you will be penalized less\nif you inform us of the deficiency in your submission write-up. Note what steps you have taken\nto debug it, even though your debugging might not have been 100% successful. If you appear\ncompletely unaware of a bug, you will receive no correctness points for that test case.\nAddressing MITPOSSE Comments\nThe MITPOSSE will give you feedback in GitHub on your code quality. We expect that you\nwill respond thoughtfully to their comments in your final submission. We will review the MIT\nPOSSE comments, your final submission, and your write-up to ensure that you are addressing\nthe MITPOSSE comments.\n\nHandout 4 -- Project 1: Bit Hacks\nAlthough code quality is subjective, good programmers produce programs that are neatly\nformatted, contain descriptive variables and function names, are partitioned well into modular\nunits, are well commented, and contain liberal use of assertions via the assert.h package. Follow\nthe C style guide published by Google: http://code.google.com/p/google-styleguide/. For\nexample, every significant loop and recursive function should have an invariant (whether self-\nexplanatory or documented in a comment) that can be verified with an assertion. You will find\nthat it is easy to write a program that is \"bigger than your head,\" where you return to the code\neven just a few days -- sometimes hours -- later and find it hard to figure out what you yourself\nwere doing without investing a significant amount in time. Comments and assertions can greatly\nimprove your ability to work on your program over a long period of time.\nIt can be difficult to maintain a consistent style when multiple people are working on the\nsame codebase. We have provided a Python script clint.py, which is designed to check a subset\nof Google's style guidelines for C code. To run this script on all source files in your current\ndirectory use the command\n$ python clint.py *\nThe code the staff has provided contains no style errors. You should use this tool to clean up\nyour source code for this project.\n5 Submitting your work\nWhen you complete your beta release, your team will need to submit your code, including your\n20 test cases, with Git and submit a write-up of your project the next day. Similarly, when you\nfinish your final release, you will need to submit your code and a write-up. In addition, every\nThursday you will individually submit a short progress report that summarizes the work you\nhave done over the prior week.\nSubmitting code\nSubmit your code with Git before the beta and final deadlines. Remember to explicitly add new\nfiles to your repository before committing and pushing your final changes:\n$ git add new-files\n$ git commit -a\n$ git status\n$ git push\nIf git status shows any modified files, then you probably haven't checked your code into your\nrepository properly.\nIn keeping with good programming practice, you should check in incremental changes to\nyour repository as you write and test your code. Remember to balance these commits among\nyour team members. We expect you to submit your code promptly by the due date.\nA quick note about git commit -a: this command will commit all modifications to tracked files\nto your local repository. If you only wish to commit changes to certain files, you can git add them\n\nHandout 4 -- Project 1: Bit Hacks\nexplicitly and git commit will commit only files that have been manually staged (use git status\nto check what's been staged). If this is confusing, please just stick to git commit -a.\nPlease do not forget to submit your 20 best tests in the file tests/mytests.\nSubmitting write-ups\nSubmit your write-ups by the due date, which is one day after your code is due.\nProgress reports\nYou must individually submit a personal progress report every Thursday that briefly describes the\nwork you performed during the past week. The reports should be short: typically, a paragraph in\nlength, describing what project work you engaged in and about how much time you spent on the\nvarious activities. Additionally, if necessary, describe any issues that may have arisen, such as with\nteammates -- or rather, especially with teammates.\nAlong with your paragraph description, include a summary of the daily number of lines of\ncode you committed using the following script:\n$ cd <your/projects/base/directory>\n$ loc_summary.py\nRunning loc_summary.py produces individualized summaries on all of the code you committed\nlocally over all branches as well as on any code your partner wrote, pushed, and you subse\nquently pulled locally. The default run option is to produce a summary between the time of\nrunning and a week before. In general, you should run the script at about the same time every\nThursday to ensure that all your commits are counted and none are double-counted.\nPlease do not worry if there are days in which you make no commits. We understand that you\nare taking other classes, that you have other obligations, and that even work in 6.172 sometimes\ninvolves things other than coding, such as thinking. Once again, please balance your commits\namong the team members so that everyone can show a fair share of commits.\nWrite-up\nWe ask you to submit small write-ups for the beta and final releases with the goal of helping the\nstaff (who understand the assignment and its general strategies but have never seen your code)\nto fairly grade your assignment. We suggest you include the following:\n- A brief overview of your design, particularly what improvements you made over the starter\ncode for your beta design. This is not designed to replace appropriate code documentation,\nthough.\n- The general state of completeness/expected performance of your implementation, as well\nas any bugs/gotchas that you are aware of.\n\nHandout 4 -- Project 1: Bit Hacks\n- Any additional information that you feel would be helpful to the staff in evaluating your\nsubmission (e.g., if you spent a lot of time on approaches that didn't result in a speedup,\netc).\nFor the final writeup, you can also include:\n- An overview of changes you made to your beta release, and what motivated you to do\nit (surprised with performance ranking? New revelations after your MITPOSSE meeting?\nIdeas conceived before the beta deadline but ran out of time implementing it?).\n- Some comments on meeting with your MITPOSSE mentor.\n6 Good Luck, and Have Fun!\nWe hope you enjoy the first project of 6.172. Start early, but remember that you can always make\nyour code faster, although there tend to be diminishing returns. Budget your time wisely, both\nwithin 6.172 and to keep a balance with your other classes and activities. Good luck, and happy\ncoding!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Project 2: Collision Detection",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/ac1794b01bec7e14b4fd34d5b1a7d2aa_MIT6_172F18_project2.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 7\nProject 2: Collision Detection\n[Note: This assignment makes use of AWS and/or Git features which may not be available to OCW users.]\nIn this project you will optimize a graphical screensaver program for multicore processors using the\nOpen Cilk parallel programming interface.\nDeliverables\n2 Team formation\n2 Team contract:\n2 Beta submission\n2 Beta write-up\n2 Final submission\n2 Final write-up\nRemember that progress reports are due weekly at 7:00 p.m. every Thursday.\n2 Getting started\nSnailspeed Ltd. has been put onto the map due to its newly patented iRotate algorithm. Due\nto your role in Snailspeed's rise to prominence, you have been promoted to CCTO (Co-Chief\nTechnology Officer). Custom engraved door plates and corner offices aren't cheap; and, unfortu\nnately, Snailspeed didn't have enough revenue left over to hire any additional engineers. Instead,\nSnailspeed has hired a small team of corporate management consultants from InchWorm Advi\nsors.\nInchWorm Advisors has suggested that Snailspeed pivot towards the development of multi-\ncore software in order to remain on the cutting edge. As a result of these discussions, Snailspeed\nis planning to launch SnailSaver, a disruptively high-performance screensaver application de\nsigned for multicore processors.\nIt is up to you and your fellow CCTO to transform SnailSaver from dream to reality. Inch-\nWorm Advisors has suggested that you look into the Open Cilk parallel programming interface\nto parallelize SnailSaver. Open Cilk may be the key, according to Inchworm Advisors, to trans\nforming Snailspeed Ltd. into a major player in the high-performance computing industry.\n\nHandout 7 -- Project 2: Collision Detection\n2.1 Team formation\nYou should begin by finding a teammate (your fellow CCTO).\nIf you do not fill out the form by the deadline, we will randomly assign you to a partner.\n2.2 Team contract\nYou and your teammate must agree to a team contract. A team contract is an agreement among\nteammates about how your team will operate -- a set of conventions that you plan to abide by.\nThe requirements for the team contract are the same as before. For guidance in writing your\nteam contract, reference Project 1's description.\n2.3 Getting the code\nWe strongly recommend groups practice pair programming, where partners are working\ntogether with one person at the keyboard and the other person serves as watchful eyes. This\nstyle of programming will lead to bugs being caught earlier, and both programmers always\nremaining familiar with the code.\n\n3L\nHandout 7 -- Project 2: Collision Detection\nFigure 1: Screenshot from input/explosion.in input.\nOptimizing collision detection\nYou will be optimizing a screensaver. The screensaver consists of a 2D virtual environment filled\nwith colored line segments that bounce off one another according to simplified physics.\nThe first part of the project is to make algorithmic changes to the collision detection algo-\nrithm to reduce the total work performed by the serial code. Currently, the screensaver uses an\nextremely inefficient algorithm to detect collisions between line segments. At each time step, the\nscreensaver iterates through all pairs of line segments, testing each pair to see if they've collided.\nThis pairwise method is expensive, as it requires Θ(N2) collision tests for N line segments.\nYou will begin by implementing a quadtree data structure to reduce the total number of line\nsegment pairs that must be checked each time step. You should not, at this point, use Open Cilk\n(or any other method) to parallelize your collision detection algorithm. The screensaver will be\nparallelized in the second part of this project (described in Section 4).\n3.1\nRunning the screensaver\nThe screensaver can be executed both with and without a graphical display. While collecting\nperformance data, make sure to execute the screensaver without the graphical display to obtain\nmore accurate performance results.\nUsage: ./screensaver [-g] <numFrames> [input-file]\n-g : show graphics over X11\nThe input file is optional and will default to input/mit.in. The input directory has several\nother (fun!) examples. Use the -g option to look at them. Figure 1 shows a screenshot from\ninput/explosion.in.\nHere's the output of one run with the default input:\n\nHandout 7 -- Project 2: Collision Detection\n$ ./screensaver 4000\nNumber of frames = 4000\nInput file path is: input/mit.in\n---- RESULTS ---\nElapsed execution time: 79.925466s\n1262 Line-Wall Collisions\n19806 Line-Line Collisions\n---- END RESULTS ---\nYou should benchmark your serial code on the awsrun compute nodes. The awsrun nodes,\nhowever, are configured to time out after 40 seconds. You should therefore first benchmark\nyour code with 1000 iterations or so. As your code gets faster, you should ramp this up to 4000\niterations or more (on the default input). Be sure to note the number of collisions detected during\nthe screensaver's execution.\n3.2 Things to remember\nBefore you start modifying the implementation, keep the following points in mind:\n- The code is written in C and is compiled with clang -std=gnu99. You may add other\ncompiler flags, but do not change the -std=gnu99.\n- Ordered collision processing: The unmodified reference implementation establishes an unam\nbiguous order in which detected collisions should be processed at the end of a time step\nby ordering lines using unique identifiers.1 As a result, your algorithm should report the\nsame number of collision events as the reference implementation as long as your algorithm\ndetects the same set of collisions at each step. Implementations that do not report the same\nnumber of collisions as the reference implementation will be considered incorrect.\n- Reference testing: A technique that you may find useful in this project is reference testing,\nwhich is comparing the execution of two different implementations of a function to ensure\nthat the two implementations have identical behavior.\nLet's say you have modified the intersect function in intersection_detection.c but aren't\nsure if it is correct. There are two ways you can proceed. The first is to write unit tests,\nas in 6.005 and 6.031. The second is to test it live by having two versions of the function:\nintersect_orig (the original implementation) and intersect_new (your new implementa\ntion). Then, for intersect, you can write:\n09 IntersectionType intersect(Line *l1, Line *l2, double time) {\nassert(intersect_orig(l1, l2, time) == intersect_new(l1, l2, time));\nreturn intersect_new(l1, l2, time);\n12 }\n1Processing the same set of collisions in two different orders can yield different results in discrete time. For\nexample, in collision_world.c, collisionSolver updates the positions and velocities of each of the two lines, and\nthese updates are not commutative (or even associative). To ensure that modified implementations follow the same\norder as the original, the list of intersecting lines is sorted using line IDs before it is processed.\n\nHandout 7 -- Project 2: Collision Detection\nFigure 2: Quadtree partitioning where each partition contains at most three points.\nIf the assertion fails when you run the screensaver, you can find out what arguments caused\nit to fail. Remember to take out the dead code in your submission, however, so that your\nMITPOSSE mentors don't have to look at it!\n- Quadtrees and other data structures: You're free to implement any data structure you like to\nmake the screensaver faster. We ask, however, that you begin the project by implementing\na quadtree and exploring a few questions about its performance characteristics.\n- Serial optimization: You may be tempted to jump directly into parallelizing your code as a\nmeans to gain performance. Generally, however, the best parallel programs are those that\nare first highly optimized serially and then made parallel afterwards. Consequently, we\nrecommend when approaching this project to make the serial version of the screensaver as\nfast as possible before going on with parallelization.\n3.3 Quadtrees\nA quadtree is a spatial data structure that stores elements in a partitioned 2-dimensional space.\nQuadtrees are often used to efficiently store and lookup 2D points. Each quadtree node is\nassociated with a square region in 2D space. During quadtree construction, any node whose\nregion contains more than r points (where r is a tunable parameter) is recursively subdivided\ninto 4 quadrants. Figure 2 illustrates the plane partitioning performed by a quadtree when r = 3.\nBecause of the way 2D space is partitioned in a quadtree, elements at the same location are\nalways placed together in the same partition. This grouping is extremely helpful in collision\ndetection. Since collisions are localized events, in order for two elements to collide, they must\nboth be within the same quadtree partition. Correspondingly, if two elements are in different\nquadtree partitions, they cannot possibly collide.\n\nHandout 7 -- Project 2: Collision Detection\nFigure 3: Quadtree storing line segments.\nCaution! In order to use quadtrees for collision detection, we actually want to think about\nstoring parallelograms instead of line segments. Imagine a line moving very fast through 2D\nspace. During a single time step, this line may exit the region associated with its current quadtree\nnode and collide with lines in other regions. Such collisions would not be detected if we only\nchecked that line against those stored in its current node. A fix for this problem is to instead\nstore the region, a parallelogram, that will be sweeped by the line during the next time step.\nNow, making this change to quadtrees is not a trivial matter. For example, while Figure 3\nsuggests that line segments can at least sometimes be handled by quadtrees, in Figure 4 one of\nthe line segments cannot fit into any of the partitions. What can you do about this problem? Can\nyou still use quadtrees to effectively speed up collision detection? These sort of issues should\nbe discussed in your write-up. Hint: Do all line segments need to be stored in leaf nodes?\nUsing a quadtree, rewrite collision detection to be much more efficient. You should use\nintersect from intersection_detection.c to test if two line segments will intersect in the next\ntime step. For now, use a reasonable value for r. To simplify your implementation, consider\ndestroying the quadtree and reconstructing it at each time step. This way, you will not need to\nworry about updating the quadtree when line segments move to new positions.\nAs you test your quadtree, you may want to consider a few key questions for your write-up:\n- What speedup did you achieve? Was this what you expected?\n- How does varying the maximum number of elements r a quadtree node can store before it\nneeds to be subdivided impact the performance? Why?\n- What key design decisions did you make while rewriting collision detection to use a\n\nHandout 7 -- Project 2: Collision Detection\nFigure 4: Broken quadtree?\nquadtree? For example, once you built a quadtree, how did you use it to extract collisions?\nRemember that the number of collisions detected during the screensaver's execution should be\nthe same as the number you recorded for the unmodified code. Moreover, make sure to run the\nprogram without the graphics flag to get accurate performance results.\n3.4 Further optimization\nLook for other opportunities for optimization within the screensaver and describe what you did.\nHere are some optimization ideas to help you get started:\n- Implement a maximum depth for the quadtree and vary it.\n- A large percentage of calculations are repeated with each time step. For example, the\ncollision detection code recalculates the length of each line segment, a relatively expensive\noperation, in each time step, even though the length never changes. For calculations that\nare repeated in each time step, it might be worthwhile to precompute these calculations\nand store results for reuse.\n- To simplify the implementation, we suggested that you destroy and recreate the quadtree\non each time step to avoid having to figure out how to effectively update the quadtree.\nWhile simple, this method is a bit wasteful. You can try finding an effective way to update\nthe quadtree so that you don't need to destroy it.\n- While our method for testing if two lines intersect is fairly efficient, you could try finding a\nmore efficient way of testing if two lines intersect.\n\nHandout 7 -- Project 2: Collision Detection\nKeep track of the optimizations that you tried and how well they worked.\n4 Parallelization\nOnce you have optimized your serial execution, you can parallelize your code.\n4.1 Profiling the serial program\nIt is useful to profile your application to determine where you should focus your time when\nparallelizing your code. The goal is to identify a region of code that comprises of a large percent\nage of the total execution time but is amenable to a potentially large degree of coarse grained\nparallelism. Run awsrun perf record and aws-perf-report to see the profiling statistics.\n4.2 Converting global variables to reducers\nBefore inserting any cilk_spawn and cilk_sync keywords, you will need to make any accesses to\nyour list of intersecting lines thread-safe. Without such a change, your parallel code may see two\nthreads attempting to concurrently insert an element into the list, causing a determinacy race.\nImplement a solution based on what you learned from Homework 4: Reducer Hyperobjects.\n4.3 Parallelizing the application\nParallelize your code by inserting cilk_spawn and cilk_sync keywords to the regions of code\nyou have identified as worth parallelizing. If you do not see much code suitable for paralleliza\ntion using the Cilk primitives, you may want to modify your collision detection code so that it\nperforms a recursive depth-first search through your quadtree.\nIn order to benchmark your parallel code, use:\n$ awsrun8 ./screensaver 4000\nThe awsrun8 command will run your code on an 8-core AWS machine. Warning: If you try to\nbenchmark your code using awsrun, it will run on a single core machine, and you will get wildly\ninaccurate results.\nFor any changes you make, verify that your code reports the same number of collisions as be\nfore. Additionally, verify that the code is race free by running it through the Cilksan determinacy\nrace detector.\n4.4 Profiling the parallel code\nYou can determine the span and work of your parallel code by executing it through Cilkscale.\nHow much parallelism do you see? Vary the parameters of your algorithm -- such as the maxi\nmum depth and maximum number of nodes per quadrant, if you are still using a quadtree -- as\nwell as any other spawn cut offs you have used in your code. What is the maximum amount of\nparallelism you can achieve?\n\nHandout 7 -- Project 2: Collision Detection\n4.5 Performance tuning\nSince you are running on an 8-core machine, you should tune your code so that it executes as\nfast as possible when running with 8 workers. To tell the Open Cilk runtime that you want 8\nworkers, set the CILK_NWORKERS environment flag:\n$ awsrun8 CILK_NWORKERS=8 ./screensaver 4000\nIn your write-up, describe any decisions, trade-offs, and further optimizations that you made.\n5 Evaluation\nPlease remember to add all new files to your repository explicitly before committing and\npushing your final changes. Your grade will be based on all of the following:\n- Performance (of correct code): As with Project 1, your final performance grade will be com\nputed by comparing the performance of your submission to a baseline to be announced.\nYour code will be benchmarked against inputs not made available to you. Therefore, you\nshould keep your code general and be aware to not over-optimize to any specific type of\nscreensaver patterns. Additionally, we will be benchmarking your code on larger machines\n(> 8 cores) than what you have access to. So be sure your code is processor oblivious. You\ncan use Cilkscale to validate whether your parallel code scales well or not.\nFor correctness, your code should:\n- Yield the same number of collisions as the original code.\n- Compute collisions correctly. When running in graphical mode, the collisions should\nlook semi-realistic as in the code given to you originally.\n- Contain no determinacy races.\n- Progress reports: You must individually submit a personal progress report every Thursday\nthat briefly describes the work you performed during the past week. The reports should be\nshort: typically, a paragraph in length, describing what project work you engaged in and\nabout how much time you spent on the various activities. Additionally, if necessary, describe\nany issues that may have arisen, such as with teammates -- or rather, especially with\nteammates.\nAlong with your paragraph description, include a summary of the daily number of lines of\ncode you committed using the following script:\n$ cd <your/projects/base/directory>\n$ loc_summary\n- Beta group write-up: You are required to submit a group write-up discussing the work that\nyou performed following a similar structure as with Project 1. Important: Please\n\nHandout 7 -- Project 2: Collision Detection\nalso push your write-up to your git repository as a .txt file. This is so that your MITPOSSE\nmentor will have access to the write-ups and can read a description of the optimizations\nyou've made.\n- Addressing MITPOSSE comments: The MITPOSSE will give you feedback on your code\nquality. We expect you to respond thoughtfully to their comments in your final submission.\nWe will review the MITPOSSE comments and your write-up to ensure that you are\naddressing them.\n- Final group write-up: You are required to submit a group write-up discussing the work\nthat you performed since the Beta submission. You should answer all questions asked in\nthis handout and address the following:\n- Provide a clear and concise description of your final strategy and implementation.\n- Discuss any experimentation and optimizations performed.\n- Justify the choices you made.\n- Discuss what you decided to abandon and why.\n- Briefly comment on meeting with your MITPOSSE mentors.\n- Final individual write-up: You are required to submit an individual write-up where you\nstate what work you did, what work your partner did and how you worked together.\nUnderstand that we expect both students to be very familiar with the code sub-mitted.\nThis will show in your write-up and how you describe the project.\n- Screensaver aesthetic test inputs: The staff has provided a few input files in the input directory\nfor your screensaver application. A small portion of your grade is based on the quality of the\nnew inputs you create to test and benchmark your program. These new inputs will be\njudged based on whether they satisfy both of the following criteria: 1. the input has\nchallenging performance or correctness properties; 2. the input is aesthetically interesting,\ni.e., it is entertaining to watch your screensaver run on your input.\nThe staff will curate the most interesting inputs produced by all teams and show them in\nclass.\nWe will grade your project submission based on the following point distribution:\nBeta\nFinal\nPerformance (of correct code)\nTeam Contract\nAddressing MITPOSSE comments\nWrite-up\nScreensaver aesthetic test inputs\n36%\n3%\n5%\n1%\n39%\n10%\n5%\n1%\nTotal\n45%\n55%\nAs with Project 1, this point distribution serves as a guideline and not as an exact formula. The\nstaff will also review your Git commit logs to assess the dynamics of your team. Please ensure\nthat commits are balanced between each team member.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Project 3: Serial Dynamic Memory Allocation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/d2fb8ee337632ab3e256fa1704375053_MIT6_172F18_project3.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 11\nProject 3: Serial Dynamic Memory Allocation\n[Note: This assignment makes use of AWS and/or Git features which may not be available to OCW\nusers.]\nIn this project you will implement a fast and space-efficient, single-core memory allocator that follows\nthe semantics of libc's memory allocator functions.\nDeliverables\n2 Team formation\n2 Team contract\n2 Beta submission\n2 Beta write-up\n2 Final submission\n2 Final write-up\nRemember that progress reports are due weekly at 7:00 p.m. every Thursday.\n2 Getting started\nSnailspeed Ltd. makes liberal use of dynamic storage allocation through the use of malloc(). You\nwould like to speed up as much of Snailspeed's code as possible, but it is not feasible for you\nto rewrite their entire codebase. In an effort to maximize the value of your work, you decide to\nwrite an optimized storage allocator which may be used to improve the performance of a large\nnumber of Snailspeed applications.\nYou have generated a number of memory allocation traces from some Snailspeed applications;\nsince these traces are representative of the sort of work that the dynamic storage allocator will be\nasked to do, you should make sure that your allocator performs well on these traces. Snailspeed\napplications are often run in memory-scarce environments, and so a good solution should be\nfast and have as little memory overhead as possible. Snailspeed has serial programs which use\ndynamic memory allocation.\nYour goal is to build a fast, space-efficient, general purpose, single-core memory allocator.\nYou'll also build a test suite to ensure that your allocator is functionally correct. You should use\nthe techniques discussed in class to balance free-list maintenance time with memory overhead.\nYou are also encouraged to try out autotuning, which allows you to programmatically find\noptimal values for parameters that control the allocator's execution.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n2.1 Team formation\nYou should begin by finding a teammate.\n2.2 Team contract\nYou and your teammate must agree to a team contract. A team contract is an agreement among\nteammates about how your team will operate -- a set of conventions that you plan to abide by.\nThe requirements for the team contract are the same as before. For guidance in writing your\nteam contract, refer to Project 1's description.\n2.3 Getting the code\nRemember that we expect groups to be practicing pair programming, where partners are\nworking together with one person at the keyboard and the other person serves as watchful eyes.\nThis style of programming will lead to bugs being caught earlier, and both programmers always\nhaving familiarity with the code. You will find that it'll be difficult to split this project into two\nindividually-completed chunks, so don't plan to divide work in this manner.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n3 Heap Memory Allocator Interface\nYour dynamic storage allocator will consist of the following four functions, which (among other\nfunctions) are declared in allocator_interface.h and defined in allocator.c. The allocator.c\nfile we have given you implements the simplest functionally correct malloc() package that we\ncould think of. Using this as a starting place, modify these functions (and possibly define other\nprivate static functions), so that they obey the proper semantics.\n- int my_init(void);\nBefore calling my_malloc(), my_realloc(), or my_free(), the application program (i.e., the\ntrace-driven driver program that you will use to evaluate your implementation) calls my_init().\nYou may use this function to perform any necessary initialization, such as allocating the\ninitial heap area. The return value should be -1 if there was a problem in performing the\ninitialization and 0 if everything went smoothly.\n- void* my_malloc(size_t size);\nThis call must return a pointer to a contiguous block of newly allocated memory which\nis at least size bytes long. This entire block must lie within the heap region and must\nnot overlap any other currently allocated chunk. The pointers returned by my_malloc()\nmust always be aligned to 8-byte boundaries; you'll notice that the libc implementation of\nmalloc does the same. If the requested size is zero or an error occurs and the requested\nblock cannot be allocated, a NULL pointer must be returned.\n- void my_free(void* ptr);\nThis call notifies your storage allocator that a currently allocated block of memory should\nbe deallocated. The argument must be a pointer previously returned by my_malloc() or\nmy_realloc(), and not previously freed. You are not required to detect or handle either of\nthese error cases. However, you should handle freeing a NULL pointer -- it is defined to\nhave no effect.\n- void* my_realloc(void* ptr, size_t size);\nThis call returns a pointer to an allocated region, similarly to how my_malloc() behaves.\nThere are two special cases you should be aware of.\n- If ptr is NULL, the call is equivalent to my_malloc(size);.\n- If size is equal to zero, the call is equivalent to my_free(ptr);.\nOtherwise, ptr must meet the same constraints as the argument to my_free(); it must\npoint to a previously allocated block and it must have been previously returned by either\nmy_malloc() or my_realloc(). You do not need to defend against frees to invalid pointers.\nThe return value of my_realloc() must meet all of the same constraints as the return value\nof my_malloc(); namely, it be 8-byte aligned and must point to a block of memory of at\nleast size() bytes.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\nThere is one additional constraint on the behavior of my_realloc(). Any data in the old\nblock must be copied over to the new block. If the new block is smaller, the old values are\ntruncated; if the new block is larger, the value of each of the bytes at the end of the block is\nundefined.\nA naive implementation of my_realloc() might consist of nothing more than a call to\nmy_malloc(), a memory copy, and a call to my_free(). This is, in fact, how the reference\nimplementation works; leaving this solution in place is probably a good way to get started.\nOnce you've made progress on my_malloc() and my_free(), you will want to consider ways\nof improving the performance of my_realloc().\nAll of this behavior matches the semantics of the corresponding libc routines. Type man malloc\nat the shell to see additional documentation, if you're curious.\n4 Checking the Consistency of the Heap\nDynamic memory allocators are notoriously tricky to program correctly and efficiently. One\nreason why they can be difficult to program correctly is because the code involves a lot of untyped\npointer manipulation. Corruption introduced by mishandling pointers might not show up until\nseveral operations later, making it extremely difficult to diagnose the root cause for a crash or\nincorrect output. For this reason, among others, you will find it very helpful to write a heap\nchecker that scans the heap and checks it for consistency. Naturally, exactly what the heap\nchecker can or should check for will depend on how you choose to implement your storage\nallocator. However, here are some examples of questions that the heap checker might ask.\n- Is every block in the free list marked as free?\n- Are there any contiguous free blocks that could be coalesced?\n- Is every free block actually in the free list?\n- Do the pointers in the free list point to valid free blocks?\n- Do any allocated blocks overlap?\n- Do the pointers in a heap block point to valid heap addresses?\nGenerally, as you design the data structures you will use to solve this problem, you should\nmake a note of any relevant invariants. Your heap checker will consist of the function int check(void)\nin allocator.c. It should return a zero if and only if your heap is consistent and return -1 oth\nerwise. You are not limited to the listed suggestions nor are you required to check all of them.\nYou are encouraged to print out error messages when check() fails.\nYou can tell the driver to check the heap after every operation by passing the -c option to the\ndriver, and looking at the \"checked\" column. You should also sprinkle heap check assertions in\nyour code when you feel the heap might go from uncorrupted to corrupted (e.g. before and after\nmajor operations on your internal data structures). Remember that assertions are only checked in\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\ndebug mode, and are not executed in release mode. The heap checker is like your own internal\nsuite of unit tests. We will not run it against anyone else's code, and we will not look at it during\ncross testing or performance testing.\nAlong the same lines, you may find it helpful to write some debugging functions that print\nyour key data structures in some easy-to-read format on your screen.\n5 Support Routines\nThe code in memlib.c simulates the memory system for your dynamic memory allocator. You\ncan invoke any of the the following functions in memlib.c (but you may not modify any of their\nimplementations):\n- void* mem_sbrk(int incr);\nExpands the heap by incr bytes, where incr is a positive non-zero integer and returns a\ngeneric pointer to the first byte of the newly allocated heap area. The semantics are identical\nto the Unix sbrk() function, except that mem_sbrk() accepts only a positive non-zero integer\nargument.\n- void* mem_heap_lo(void);\nReturns a generic pointer to the first byte in the heap.\n- void* mem_heap_hi(void);\nReturns a generic pointer to the last byte in the heap.\n- size_t mem_heapsize(void);\nReturns the current size of the heap in bytes.\n- size_t mem_pagesize(void);\nReturns the system page size in bytes (4 KB on Linux systems).\nIn the code you write, you may NOT invoke any of the functions in memlib.c that are not\nlisted above. Additionally, you may NOT invoke any of the wrappers to these functions in\nmy_allocator_wrappers.c. These functions are for use by the TA's who are grading your submis\nsions.\n6 Writing the External Validator\nFor the cross testing component of this project, you will be writing a blackbox (external) valida\ntor that can test any malloc() implementation. Your validator will be used to test your peers'\nimplementations. Look in validator.h for the skeleton of our validator, which lists all of the\ninvariants we want you to check. They are reproduced here for reference.\n- For the given traces, neither my_malloc() nor my_realloc() should return NULL.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n- Allocated ranges returned by the allocator must be aligned to 8 bytes.\n- Allocated ranges returned by the allocator must be within the heap.\n- Allocated ranges returned by the allocator must not overlap.\n- When calling my_realloc() on an existing allocation, the original data must be intact (up\nto the reallocated size).\nWe've provided a linked list representation for the ranges, but you must provide the add()\nand remove() operations for this data structure.\n7 The Trace-based Driver\nThe driver program mdriver.c tests your allocator.c package for correctness, space utiliza\ntion, and throughput. The driver program is controlled by a trace file. Each trace file con\ntains a sequence of allocate, reallocate, and free commands that instruct the driver to call your\nmy_malloc(), my_realloc(), and my_free() routines in some sequence. It also contains write\ncommands that instruct the driver to read and write the allocated memory. Run mdriver locally\non your VM as opposed to awsrun. The driver mdriver accepts the following command line\narguments:\n- -t <tracedir>: Look for the default trace files in directory <tracedir> instead of the default\ndirectory (./traces).\n- -f <tracefile>: Use one particular trace file for testing instead of the default set of trace\nfiles.\n- -h: Print a summary of the command line arguments.\n- -v: Verbose output. Print a performance breakdown for each tracefile in a compact table.\n- -V: More verbose output. Prints additional diagnostic information as each trace file is pro\ncessed. Useful during debugging for determining which trace file is causing your my_malloc\npackage to fail.\n- -c: Check the heap after every operation using your check() function.\nThe simple implementation given to you will run out of memory on the realloc() trace and\nthrow an error since it does not utilize freed space appropriately. Your implementation will be\nexpected to pass all of the trace files in the traces directory. We will test your implementation\nwith traces other than those provided in the traces directory.\nMore information on the format of the trace files can be found in the README included at the\ntop level of the Project 3 code repository.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n8 Autotuning\nOptimization and implementation strategies in a program can affect performance considerably.\nA tuning framework searches through different strategies and finds the best performing imple\nmentations. If you'd like, you can use the OpenTuner autotuning framework to tune parameters\nof your allocator's execution. This is not required for you to receive full credit.\nTo tune your program, you first need to express the optimizations in your program as param\neters and define a search space of possible values. The job of the autotuner is to automatically\ntraverse the space, searching for the most effective combination of parameters. OpenTuner is a\nopen source framework developed at MIT which we recommend for this project. To run Open-\nTuner, go to the project directory and follow the steps in the README file. Also revisit Homework\n6 for some instructions on running OpenTuner.\nBe careful: Autotuners are designed to find the most optimal configuration for whatever test\ncases you run them on, sometimes in ways that a human coder would never consider. If your\ntests are not general enough, you can overfit your allocator to the autotuning inputs. This may\ncause your code to run slower on the somewhat-different test suite that will actually be used for\ngrading.\nMore information about OpenTuner can be found in the conference paper, (\"OpenTuner: An\nExtensible Framework for Program Autotuning\"), and in the official online tutorial.\n9 Real Program Performance Testing\nIn addition to the traces, we may also to test your allocators on real programs to determine\nthe performance on realistic workloads. We have provided a simple performance test for your\nallocator in allocator_test.c, which can be run by executing the file allocator_test. For the\nbeta, we will not be grading your performance against these additional programs; however, we\ndo recommend you play around with this to get a feel for the performance of your allocator since\nwe may test the your allocator against other real workloads for the final project submission.\n10 Rules and Reminders\n- You should not change any of the sources in the distribution except for the allocator.c,\nvalidator.h and opentuner_params.py files. You are free to add new files and update the\nMakefile appropriately if you wish. All of the other files will be overwritten with fresh\ncopies during cross testing.\n- You should not invoke any memory-management related library calls or system calls. Do\nnot use malloc(), calloc(), free(), realloc(), sbrk(), brk() or any variants of these calls\nin your code.\n- You should not use any parallelization for this project. This project is meant to be single-\ncore only.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n- The total size of all defined global and static scalar variables and compound data structures\nmust not exceed 512 bytes.\n- All data structures that allocate memory on heap MUST use our allocator heap interface.\n- All heap memory space used by your data structures will be counted under space utiliza\ntion.\n11 Evaluation\nPlease remember to add all new files to your repository explicitly before committing and\npushing your final changes. Your grade will be based on all of the following:\n- Performance (of correct code): Two performance metrics will be used to evaluate your solution.\nThis is a little bit different from what we've done in previous projects.\n- Space utilization: The peak ratio between the aggregate amount of currently allocated\nmemory (M) (i.e., allocated via my_malloc() or my_realloc() and not yet freed via\nmy_free()) and the size of the heap (H) used by your allocator. The optimal ratio is, of\ncourse, 1. You should find good policies to minimize fragmentation in order to make\nthis ratio as high as possible. The space utilization U would be calculated as\nU = max(M,40KB)/max(H,40KB)\n- Throughput: The average number of operations completed per second.\nThe driver program summarizes the performance of your allocator by computing a perfor\nmance index, P, which is a weighted geometric mean of the space utilization and through\nput:\nP = exp(wlogU + (1 - w) log(min(1, T/Tlibc)))\nwhere U is your space utilization, T is your throughput, and Tlibc is the estimated through\nput of libc's malloc() on your system on the default traces.\nSince both memory and CPU cycles are expensive system resources, we adopt this formula\nto encourage balanced optimization of both memory utilization and throughput. Ideally,\nthe performance index will reach P = exp(wlog1 + (1 - w) log1) = 1 or 100%. Specifically,\nwe have set the value of w to 0.5 so performance in terms of memory utilization and\nthroughput are equally important. To receive a good score, you must therefore perform\nwell in both categories.\nYou will receive credit for each trace that your allocator successfully handles, as evaluated\nby our validator. We will test your implementation with traces other than those provided\nto you with the code.\nAs mentioned previously, we may also test the performance (both throughput and utiliza\ntion) of your allocator against other real programs. Note that we will not be doing this\nfor the beta submission. We have provided a simple allocator test that uses your allocator,\nwhich you can modify to test against other programs.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n- Progress reports: You must individually submit a personal progress report every Thursday\nthat briefly describes the work you performed during the past week. The reports should be\nshort: typically, a paragraph in length, describing what project work you engaged in and\nabout how much time you spent on the various activities. Additionally, if necessary, describe\nany issues that may have arisen, such as with teammates -- or rather, especially with\nteammates.\nAlong with your paragraph description, include a summary of the daily number of lines of\ncode you committed using the following script:\n$ cd <your/projects/base/directory>\n$ loc_summary\n- Addressing MITPOSSE comments: The MITPOSSE will give you feedback on your code\nquality. We expect you to respond thoughtfully to their comments in your final submission.\nWe will review the MITPOSSE comments and your write-up to ensure that you are\naddressing them.\n- Test coverage: Your my_malloc() validator should catch all violations of the invariants we\nlisted. We will run it on every other team's my_malloc() implementation and compare your\nresults with our own to determine your grade.\n- Beta and final group write-up: You are required to submit (on LMOD) a group write-up for\nboth the beta and the final submissions. Important: Please also push your write-up to your\ngit repository as a .txt file. This requirement gives your MITPOSSE mentor access to the\nwrite-ups and lets them read a description of the optimizations you've made.\nYou will notice that we have not provided you with a list of questions to guide your ex\nploration of this problem. Now that you have a couple of projects under your belts, we\nwill expect you to be able to produce well-documented code and accompanying design\nmaterials without prompting.\nTo supplement the documentation present in your code, you should submit some addi\ntional materials; they should be as concise as possible while still doing an effective job of\nexplaining how your allocator works. Diagrams may be useful (and you should probably\ninclude some)!\nYour written materials should describe the data structures you have chosen and how each of\nyour calls manipulates those data structures to accomplish its goals. While a lot of this\nmaterial will probably overlap the comments present in your code, your write-up should be\nsufficiently detailed as a stand-alone document that we can completely understand what you\nare doing without looking at your code.\nAs always, be sure to include a discussion of any possibilities that you examined and\ndiscarded. If you were forced to make trade-offs, be sure to discuss the possible advantages\nand disadvantages of each choice, and explain why you made the decision that you did.\nExplain the memory allocation strategies you used and what parameters you used for the\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\nautotuning section. You should also report your program's performance before and after\ntuning and report the best parameter configuration found by OpenTuner.\n- Final individual write-up: You are required to submit an individual write-up where you\nstate what work you did, what work your partner did and how you worked together.\nUnderstand that we expect both students to be very familiar with the code submitted.\nThis will show in your write-up and how you describe the project.\nWe will grade your project submission based on the following point distribution:\nBeta\nFinal\nPerformance (of correct code)\nTeam Contract\nAddressing MITPOSSE comments\nTest Coverage\nWrite-up\n27%\n3%\n12%\n3%\n40%\n10%\n5%\nTotal\n45%\n55%\nYou will receive zero points if you break any of the rules or your code is buggy and crashes the\ndriver.\n12 Hints and Tips\n- Spend plenty of time writing your internal consistency checker and other debugging tools.\nThis will save you time in the long run.\n- Use the mdriver -f option. During initial development, using tiny trace files will simplify\ndebugging and testing. We have included two such trace files.\n- Use the mdriver -v and -V options. The -v option will give you a detailed summary for\neach trace file. The -V will also indicate when each trace file is read, which will help you\nisolate errors.\n- Use a debugger, such as gdb. A debugger will help you isolate and identify out-of-bounds\nmemory references.\n- Use assertions. When debugging a failure or a crash, sprinkle assertions in your code\nbefore and around the crash and rebuild it in DEBUG mode. When a program crashes due\nto an assertion failure, it prints out the failed condition and the line number of the failed\nassertion, which is more helpful than \"Segmentation Fault\".\n- You may want to explore encapsulating your pointer arithmetic in static inline functions.\nPointer arithmetic in memory managers is confusing and error-prone because of all of the\ncasting which is necessary. You can reduce this complexity significantly by writing helper\nfunctions (or macros if appropriate) for your pointer operations.\n\nHandout 11 -- Project 3: Serial Dynamic Memory Allocation\n- Do your implementation in stages. We recommend that you start by getting your my_malloc()\nand my_free() routines working correctly and efficiently and test it on the traces. The\nrealloc() trace should be tested after the implementation of my_realloc(). Remember that\nthe reference implementation of my_realloc() is built on top of my_malloc() and my_free().\n- Use a profiler like perf or gprof to identify hot spots. Remember all of the techniques we've\ndiscussed so far!\n- Start very early! It is possible to write an efficient malloc package with a few pages of code.\nHowever, we can guarantee that it will be some of the most difficult and sophisticated code\nyou have written so far, and probably also the most difficult to debug. If you wait until\nthe last minute, you may find that you do not have enough time to produce a worthwhile\nproduct. Good luck!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.172 Performance Engineering of Software Systems, Project 4: Leiserchess",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/b13a935db7ea2a78a16582065bba55be_MIT6_172F18_project4.pdf",
      "content": "Performance Engineering of Software Systems\nMassachusetts Institute of Technology\n6.172\nProf. Charles E. Leiserson and Prof. Julian Shun\nHandout 15\nProject 4: Leiserchess\n[Note: This assignment makes use of AWS and/or Git features which may not be available to\nOCW users.]\n1 Deliverables\n2 Team formation\n2 Team contract\n2 Beta I submission\n2 Beta I write-up\n2 Beta II submission\n2 Beta II write-up\n2 MITPOSSE Beta I Meeting\n2 Presentation Slides\n2 Presentation\n2 Final submission\n2 Final write-up\n2 Exhibition Tournament\nRemember that progress reports are due weekly at 7:00 p.m. every Thursday.\n2 Introduction\nIn this final assignment, you will put the skills you have learned so far to the test. Each group will\nstart with a high quality game-playing AI for Leiserchess--a chess-like game with lasers! The\ngame-playing AI that we provide implements an algorithm called Principal Variation Search (or\nPVS). The PVS algorithm is a refinement of the alpha-beta search algorithm and it is commonly\nused in high-performance chess engines. We recommend that you briefly review the materials\non the Chess Programming Wiki (https://www.chessprogramming.org) to learn a bit about this\nalgorithm.\nUnlike previous projects, the code that you start with is not using a naive algorithm. This\nproject simulates situations that you will encounter in real life. As a performance engineer\nyou will often be tasked with finding opportunities to improve the performance of sizable and\n\nHandout 15 -- Project 4: Leiserchess\ncomplex programs that have been implemented by domain experts. It is your job to improve the\nperformance of such programs without compromising their correctness.\nThis project simulates the situations you will encounter in real life. After designing and\nimplementing a fairly sizable program, you find that it doesn't run fast enough. At this point,\nyou have to figure out what is slow, and decide how to fix the performance issues. We've left in\nsome low-hanging fruit, but you'll find the need to make design changes to the game engine for\nsubstantial performance improvement.\n3 Leiserchess\nThe starter program implements a Leiserchess game engine that plays games through the UCI in\nterface (https://en.wikipedia.org/wiki/Universal_Chess_Interface). Leiserchess (pronounced\n\"LYE-sir-chess\") is a two-player laser-chess game similar to Laser Chess and Khet. The goal is to\nperformance-engineer the game engine so that you improve the player's ability to play the game.\nThere are two common avenues to improve the player's ability.\nThe first is to improve the search algorithm, such as by coming up with more sophisticated\nheuristics for the evaluation function or by improving the PVS algorithm provided. The second\nis to performance-engineer the existing algorithm in order to allow it to search more positions in\nthe game. There are many changes that can be made to the game-engine that do not change its\nsearch behavior, but improve performance. If two game-playing programs implement the same\nsearch algorithm, then the faster program will be the stronger player because it is able to consider\nmore positions per second.\nIn this project, you will find that it is more productive to focus on the second avenue. The\nheuristics used by the evaluation function we have given you are already reasonably good, and\nthe PVS search algorithm is used by domain-experts in chess programming. The implementa\ntions of the evaluation function and the PVS search algorithm, however, have not been heavily\noptimized.\nNevertheless, you are completely free to employ any strategy you wish to improve your\nprogram's game-playing performance. The final measure of performance, will be your program's\nability to beat a selection of game-playing programs in round-robin tournaments using Fischer\nTime Control. We'll discuss these details in greater depth in the Getting Started section.\n4 Background: Alpha-Beta Search Overview\nThe given program is an AI for playing Leiserchess that uses Principal Variation Search (PVS)\nto find the best move from a given position. Principal Variation Search (PVS) is a refinement\nof alpha-beta search that is itself an optimization of the Minimax algorithm. Let us begin with\na brief overview of the alpha-beta search algorithm upon which Principal Variation Search is\nbased.\n\nHandout 15 -- Project 4: Leiserchess\nFigure 1: Alpha-Beta Pruning\n4.1 Game Trees\nIn this algorithm, each possible game state, where a state is usually a board layout, is modeled\nas a node in a tree. The children of a node are the states that are possible by making any of the\nplayer's moves. The children of these nodes are the states possible from the opponent's moves,\nand so forth. For simple games like Tic-Tac-Toe, this tree is small enough that it can be completely\ngenerated, allowing for perfect knowledge. For complex games like chess and Leiserchess, this\ntree is far too large to generate completely. The branching factor, defined as how many states are\npossible from a given state, is several dozens for the opening move of Leiserchess! The height of\nthe tree is the number of moves from start to finish, which is probably far more than 50. The size\nof the tree is branchingheight, which is impossible to generate in any reasonable amount of time.\n4.2 Alpha-Beta Pruning\nAlpha-beta search uses a concept called pruning to avoid searching the entire tree. Imagine a\ngame tree where you are exploring the two subtrees of the root, as in Figure 1, from left to right.\nWe begin to explore the left subtree and analyze the moves our opponent could make. One game\nstate is an immediate loss for us, labeled B in the figure. Assuming our opponent is as intelligent\nas we are, if we make the move corresponding to game state A, our opponent will make us lose\nby making the move which leads to B. There is no point in exploring any of the other children\nof the A, as we already know our opponent already has a very good move B. We stop exploring\nthis subtree, or prune away the subtree, and instead focus on the other subtree in the hopes of\nfinding a better move. Another way of thinking of this is that once you have found out that a\nmove is bad, there is no need to find out exactly how bad it is.\n4.3 Static Evaluation\nPruning helps cut away part of the game tree, but it doesn't address the issue that the tree is\nstill too big to explore to the point of a loss. Each game state is assigned a score by a static\nevaluator function, where this score is a measure of how good the position is for the player. The\nstatic evaluator can be expensive, and it is often difficult to write an accurate one. Instead we\n\nHandout 15 -- Project 4: Leiserchess\nFigure 2: Pruning With Alpha-Beta Values\nexplore the game tree to a given depth, and we use our static evaluator on the leaves. Using\nthese scores, we can establish lower and upper bounds, alpha and beta, respectively, on the\nscores of the moves available to us at any game state. The alpha value is the highest score we are\nguaranteed to get, and the beta is the lowest score our opponent can force us to get. In Figure 2,\nthe alpha and beta values are initialized to negative infinity and positive infinity, since we have\nno knowledge of possible scores. We evaluate the left child to have the score of 5. Now we\nknow we are guaranteed at least a 5 and update our alpha value accordingly. We then begin to\nexplore the right child. The right child's values are initialized to that of the parent, 5 for alpha\nand infinity for beta. Node A evaluates to -3, meaning our opponent can force us into a score at\nleast as bad as -3 if we go the right child's state. We update the beta value accordingly. Pruning\noccurs whenever alpha exceeds beta. In this example we saw that we had a move with a score\nof 5 available in the left subtree. By entering the right tree, our opponent can force us to get at\nmost -3. There is no point in exploring the rest of the moves to find out as no better outcome\ncan occur.\nWhile having an accurate static evaluator looks to be the most important thing, there is a\nbig trade-off between having an accurate static evaluator and a fast one. A fast static evaluator\nallows for a deeper search of the tree in a given time limit, which usually translates into a stronger\nplayer, as it can look more moves ahead. The provided evaluation function is the best the course\nstaff could come up with. There is probably room for improvement, but you are advised to focus\non producing high-performance code first.\n4.4 Parallelization\nWe do not expect any students to parallelize their program for the first beta deadline. It may,\nin fact, be counter-productive to attempt this too early since there are significant performance\noptimizations that are possible in the serial code.\nParallelization, however, will be a necessary change in order to achieve good performance on\nthe second beta and final versions of your program. The staff has designed the code to be fairly\n\nHandout 15 -- Project 4: Leiserchess\neasy to parallelize, but we leave the details of the parallelization to you. These changes require\nsome insight into the structure of the program, however, that you will build as you improve the\nperformance of the serial program.\nWe will release more instructions on how to parallelize your program after the first beta\nsubmission. Let us briefly mention a few concepts, however, so that you might keep them in\nmind as you are optimizing the serial code.\nThe idea behind parallelizing alpha-beta and Principal Variation Search is to search some\nselected subtrees in parallel. This is an example of speculative parallelism because you might\nsearch a subtree that would have been pruned due to a beta cutoff.\nAs we mentioned in the lecture on speculative parallelism, it is necessary to consider two\nthings when introducing speculative parallelism into your program. The first is to ensure that\nyou abort computations that are not needed as soon as possible. This is necessary in order to\nensure that a beta cutoff correctly terminates searches of subtrees. The second is to avoid the\nexecution of computations that are likely to be aborted. The second consideration is needed\nto allow your parallel program to obtain the full benefits of parallelization. Afterall, even if\nsearches are correctly aborted your program will not actually search deeper in the game-tree if\nyour additional processing cores are used to search subtrees that are highly likely to be aborted.\nAn additional consideration when parallelizing your code is contending with races. Care\nmust be taken, for example, to ensure there are no races on the alpha and beta values. You\nwill also find that the search algorithm maintains some tables for memoizing moves, etc. Races\ncan occur on these tables and you will have to decide whether the races are tolerable. Nota\nbene: unlike other programs you have seen so far, some races in game engines are OK. In\nparticular, you should consider how concurrency impacts the transposition table, killer-move\ntable, and best-move history used by your programs. Some races may be tolerable, others may\nresult in incorrect behavior, and others might be correct but compromise the benefits of various\nheuristics.\n5 Administrivia and Deliverables\nCompared to previous projects, this project is loosely structured in order to cut the overhead and\ngive you the most time to work on this project. Here is an overview of the deliverables. Each\ngroup needs to submit three versions of the project codebase: Beta I, Beta II, and final. Please\nalso submit a Beta I report describing strategies, progress, issues concerning Beta I. Both the\nBeta I codebase and the Beta I report will be the basis for MITPOSSE meeting, which happens\nafter the Beta I submission. For the final code base submission, we also require each group to\ncreate slides and deliver a 10-minute presentation with the course staff. Each group also needs\nto submit a final report, which contains a conclusive description of the project and a review of\nthe MITPOSSE meeting. The following provides a breakdown of the details for each item.\n5.1 Groups\nYou are expected to work in groups of three or four students. You may work with anybody in the\nclass, including previous project partners and classmates who have the same MITPOSSE mentor.\n\nHandout 15 -- Project 4: Leiserchess\nWe suggest you choose and form your group as soon as possible.\n5.2 Beta Submissions\nThere are two Beta submissions. During Beta I, you will optimize the game engine any way\nyou wish with the constraint that all parallelism is disabled. During Beta II, you will optimize a\nparallel version of the code. After each Beta submission, the course staff will run the autotester\non all possible pairs of submitted binaries. The staff will publish a performance report on the\nsubmitted binaries so that you can get a sense of where your player stands at the point of the\nBeta submission. Use this information to evaluation what strategy your group should take for\nthe next milestone.\n5.2.1 Beta I: Serial optimizations\nBeta I is about serial optimizations. In expectation that students will parallelize their program,\nhowever, we will evaluate your implementation by running\n$ make PARALLEL=0\nSo you may use that preprocessor definition in your code to turn features on and off depend\ning on whether you want your serial or parallel code to be evaluated. Feel free to experiment\nwith the parallel code if you have time. Just be aware that you will be judged only on the serial\nversion for this checkpoint.\n5.2.2 Beta I Writeup and MITPOSSE Meeting\nOne the due date of Beta I Writeup, please submit a written report on your Beta I software. This\nreport should be around 1000 words (roughly two pages in a word processor), and will be read\nby your MITPOSSE mentor and TAs.\nAt this point, you should have a strong understanding of the game engine's structure and\nperformance characteristics. You should also have made some progress with optimizations, but\nmost importantly, you should have a plan for what optimizations you are going to implement.\nThere are many potential changes you could make, but you should focus on the areas that will\ngive you the best return on time spent. To summarize, your review should contain:\n\nHandout 15 -- Project 4: Leiserchess\n- Profiling data on the reference implementation.\n- The changes you've implemented so far, and their impact (supported by profiling measure\nments).\n- Optimizations you plan to make (and how you prioritize them), supported by profiling\ndata. You should also estimate the impact of each optimization based on your profiling.\n- A work breakdown of how your team plans on dividing the work. You should convince\nthe course staff that your group members are performing equal work and that everyone is\ndoing work worthy of a final project for 6.172.\nPlease arrange a meeting with your mentor before the MITPOSSE meeting due date. Remem\nber that you are required to meet with your MITPOSSE mentor. The meeting provides a valuable\nopportunity to receive feedback both from an industry expert regarding your plans for the final\nproject. Please include a summary of your meeting in your final written report.\n5.2.3 Beta II: Parallel optimizations\nBeta II will proceed exactly as Beta I, except, your bot will be run on a multicore machine. You're\nencouraged to parallelize your bot to obtain parallel speedup. Since there may be some changes\nto your bot that you only want active when running on multiple cores, we will compile your\ncode using the command:\n$ make PARALLEL=1\n5.2.4 Beta II Writeup\nOne the due date of Beta II Writeup, please submit a written report on your Beta II software.\nThis writeup has the same structure as the Beta I Writeup, and will be read by your TAs to assess\nyour progress.\n5.3 Final Deliverable\nThe final deliverables for this class consists of a final presentation and a final write-up, consisting\nof your final code submission and an overall final report on the project. Each component is\ndetailed below.\n5.3.1 Final Presentation\nBefore the code is due, we expect each group to deliver a 10-minute presentation. Suggested\ntopics to cover in your presentation include:\n- Your overall optimization strategies.\n- How you implemented or approached your plan.\n\nHandout 15 -- Project 4: Leiserchess\n- What kind of performance bottlenecks you observed and solved.\n- Brief overview of the performance results.\n- Analysis, if there is any.\n- Brief description of the work breakdown.\n- Other thoughts, such as those strategies that you tried but didn't work.\nPlease submit your slides the day before the presentation. They will be compiled by the staff\nin order to reduce the startup time required for each presentation. The presentations will run\nfor the entire day, and each team will be assigned a 10-minute slot. All group members must\nattend the participation. We will provide more information about the presentation format after\nthe second beta deadline.\n5.3.2 Final Code Submission, Tournament\nPlease turn in your code by the final turn-in deadline. We will grade your program by autotesting,\nbut we will hold a live exhibition tournament with prizes for the winners. Come and see how\nyou and your peers did!\n5.3.3 Final Report\nTo help us with grading your submission, please submit a final project report that briefly outlines:\n- The optimizations in your submission.\n- Optimizations that you tried, but didn't work.\n- The work breakdown within your group (to be submitted individually).\n- Any extra information you think would be helpful for us in assigning a grade.\n- Description of your meeting with your POSSE mentor, if you attended the meeting.\nUnlike the Beta writeups, there is no page limit to the final writeup, but as always don't\nwaste words. Feel free to reuse (copy/paste) material from your Beta I and II writeups where\napplicable.\n6 Evaluation\nGrade breakdown for Project 4 is shown in the table below.\n\nHandout 15 -- Project 4: Leiserchess\nBeta 1\nBeta 2\nFinal\nTeam Contract\nPerformance (of correct code)\nWriteup\nMITPOSSE of Beta 1\nPresentation\n2%\n18%\n5%\n5%\n20%\n5%\n35%\n5%\n5%\nTotal\n30%\n25%\n45%\nGetting Started\nYou will also need to install Java and Tcl on your personal Amazon instances to use some of the\nproject scripts, which you can do by running:\n$ sudo apt-get install openjdk-8-jdk tcl\nThere is significantly more documentation in this project than in other projects; for conve\nnience, here are additional supporting documents you will find useful:\n1. /README: description of the code base structure, how to launch a game server and how to\ntest your code.\n2. player/README: summary of code files that you may find yourself modifying.\n3. doc/Leiserchess.pdf: introduction to Leiserchess rules.\n4. doc/engine-interface.txt: description of commands you can send Leiserchess at the com\nmandline. See /player/leiserchess.c for more information.\n5. Slides from Leiserchess code walk.\nThe main codebase that you will be working with is the game engine in the player directory.\nYou can use make to compile the player code, which generates leiserchess. This program fol\nlows the Universal Chess Interface (UCI), which is a set of text commands sent via stdin that a\nLeiserchess engine must respond to at any point during runtime. Those commands are described\nin the UCI document, doc/engine-interface.txt. The given implementation supports an addi\ntional command perft. This command counts all possible moves to a given depth, which makes\nit a valuable debugging tool for your move-generation code.\n\nHandout 15 -- Project 4: Leiserchess\n7.1 Testing Tips\n- For correctness checking, run your bot and the reference bot together and verify that they\nmake the same moves at each time step for the same starting board state. Both bots should\nsearch at the same depth; otherwise, it would make sense for your bot to make a different\nmove because it is faster and able to search deeper in the tree.\n- You can use FEN strings/move lists to facilitate debugging. If your AI is exhibiting a bug\nof some kind, you can use a FEN string or move list for the board right before the bug\nappears to quickly reset the board and hopefully more quickly elicit the bug. You can use\nthe output of the autotester to see games where the AI crashes and to get the relevant move\nlists for those games.\n- You might be tempted to evaluate your performance by running the UCI command go depth\nand looking at the Nodes Per Second (NPS) metric. This is generally a bad idea, as NPS\nmay vary at different points in the game depending on the complexity of the search tree\ntraversal. It's better when possible to run many games with the autotester and see whether\nthere's a significant difference in ELO rating.\n- To spot egregious gameplay bugs, we recommend that you watch some games on the\nscrimmage server.\n- Use git branches to isolate various optimization strategies.\n8 Other considerations\nThe other directories contain tools that will assist you to performance engineer the player code. In\nparticular, we have provided an autotester framework and Elo-rating software which will allow\nyou to evaluate two different versions of your player to determine whether the modifications you\nhave made indeed help the player to perform better. Instructions on how to run the autotester\nand the rating software are also included in the top-level README file.\nThe Leiserchess engine consists of a fair amount of code. You will have to decide how to use\nyour limited time to achieve the best results. Here are some suggestions from us about how to\nbegin:\n- Make sure you are comfortable with the concepts behind how alpha-beta search works.\nWithout this background, it will be difficult to think of substantial optimizations or trou\nbleshoot.\n- Make sure you understand how the code is organized. Although it's probably a waste of\ntime to read through every line of code before beginning, make sure you understand the\nbig-picture components of the code and how they interact.\n- Use the profiling tools you've learned in this class to identify hotspots. Profiling should\nreveal substantial low-hanging fruit.\n\nHandout 15 -- Project 4: Leiserchess\n- Use Cilksan to make sure you don't have race conditions. Program crashes count as losses\nwhen we autotest your program.\n- Never lose sight of the big picture. Once you find one hotspot, it's easy to get sucked into\noptimizing it to death. Recognize when you've made sufficient progress and move on!\n- Make incremental changes. Avoid structuring your changes such that you cannot compile\nor run your code for extended periods of time. Performance surprises tend to be unpleasant\nin nature.\n- We recommend that you make any changes to the board representation as early as possible.\nThese changes may affect the behavior of other parts of the code base.\n- The static evaluator in the codebase uses the best heuristics the staff could devise. Although\nthere could very well be better heuristics, there is plenty of low-hanging fruit from simply\noptimizing the program before you bother coming up with your own. Do keep in mind\nthere is a trade-off between having an accurate static evaluator versus a fast one. A program\nusing a less-accurate evaluator can be stronger than a program with a more accurate one if\nit is faster and can search deeper in the tree.\n- Keep an open mind. When looking at preexisting code, it's easy to be lured into following\nthe footsteps of the original authors. You should ask yourself whether the data structures\nand algorithms chosen are appropriate. Is there a faster (or more parallelizable) method?\n- The chess-programming wiki (https://www.chessprogramming.org) is an invaluable resource.\nWhile Leiserchess differs from chess in many important ways, many concepts in writing\nfast chess engines should apply to Leiserchess as well.\n- Your team has a share of the computing resource on the Amazon machines. Explore ways\nto have it do useful work, even while no team member is actively developing.\n8.1 Familiarize Yourself with the Leiserchess Game\nBesides getting the code to run fast, we also want you to have lots of fun--the project is about\nperformance engineering a game engine after all! It is a good idea to start out by playing a few\ngames to familiarize yourself with the rules of the game. Once you download the codebase, find\nthe accompanying document in doc/Leiserchess.pdf, which describes the rules for Leiserchess.\nRun the game in your own browser using instructions included in the top-level README file in the\ncodebase.\n8.2 Rules and Fine Print\nYour program must obey the following ground rules:\n- You may not rely on other people's code for any significant functionality in your submis\nsion. Reference materials and borrowed code snippets should be cited in your final report.\n\nHandout 15 -- Project 4: Leiserchess\n- You need not use the Cilk extensions for parallelization. You may use TBB, Pthreads,\nOpenMP, or any other platform.\n- Your program must run on Amazon resources. You may not, for example, use a distributed-\ncomputing package to source external computational resources.\nWhen in doubt, don't hesitate to ask the course staff for clarification.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.172 Performance Engineering of Software Systems\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    }
  ]
}