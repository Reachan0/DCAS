{
  "course_name": "Topics in Applied Mathematics: Waves and Imaging",
  "course_description": "No description found.",
  "topics": [
    "Mathematics",
    "Applied Mathematics",
    "Computation",
    "Mathematics",
    "Applied Mathematics",
    "Computation"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\nThere are no official prerequisites for this course, but permission of the instructor is required. Student should have some undergraduate familiarity with partial differential equations, Fourier transforms, distributions (the Dirac delta), linear algebra and least squares, as well as some basic physics. A knowledge of basic computer programming is also needed.\n\nDescription\n\nThis class covers the mathematics of inverse problems involving waves, with examples taken from reflection seismology, synthetic aperture radar, and computerized tomography. The course is suitable for graduate students from all departments who have affinities with applied mathematics.\n\nTextbook\n\nThere is not one textbook. The material will be inspired from various sources. See the\nreadings\nsection for a list of references that sometimes go way beyond what we'll do in class.\n\nRequirements\n\nThere will be occasional problem sets and an oral presentation of a good (landmark, foundational) paper from the literature. The presentations will take place during the last two class sessions.\n\nGrading\n\nThe lowest homework score will be dropped.\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem Sets\n\n70%\n\nOral Presentation\n\n30%\n\nTopics\n\nWEEK #\n\nTOPICS\n\n1-2\n\nAcoustic, Elastic, Electromagnetic Wave Equations\n\nScattering Series and Inversion\n\n4-5\n\nMigration and Backprojection: Adjoint-state Methods\n\n6-7\n\nRadar Imaging, Filtered Backprojection, Ambiguity and Resolution\n\nComputerized Tomography, Radon Transform\n\n9-11\n\nSeismic Imaging, Geometrical Optics, Generalized Radon Transform\n\n12-13\n\nOptimization, Regularization, Velocity Estimation, Autofocus\n\nStudent Presentations",
  "files": [
    {
      "category": "Assignment",
      "title": "Waves and Imaging Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/43df046d11ac96c0eb4a3ac04d928006_MIT18_325F15_hw3.pdf",
      "content": "18.325 :: Homework 3 :: Fall\nIn this problem set we will form an image from a fan-beam CT dataset.\nDownload and load the dataset in MATLAb(r) with load siemens.mat\nThe array g is a sinogram. It has 513 rows, corresponding to uniformly sampled offsets t, and 360 columns,\ncorresponding to uniform, all-around angular sampling with 1-degree steps in θ. The acquisition is fan-\nbeam: a transformation is needed to recover the parallel-beam geometry. The fan-beam geometry man-\nifests itself in that the angle depends on the offset t in a linear fashion.\nInstead of being just θ, it is\n(1 ≤t ≤513 is the row index)\nt\nθ\n-257\n+\nα,\nwith\nsin α =\n.\n2.87\nImaging from a parallel-beam sinogram is done by filtered backprojection. Filtering is multiplication by ω in\nthe ω domain dual to the offset t. Backprojection of a sinogram g(t, θ) is\nI(x) =\nX\ng(x · eθ, θ),\nθ\nwhere eθ is (cos θ, sin θ)T . (Why is this the same thing as what we saw in class?) Form the image on a grid\nwhich has at least 100 by 100 grid points (preferably 200 by 200). You will need an interpolation routin\nsince x · eθ may not be an integer; piecewise linear interpolation is accurate enough (interp1 in MATLAb).\nIn your writeup, show your best image, your code, and write no more than one page to explain your choices.\n\ne\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Adjoint-state Methods",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/d7f2deb3f507b44b4d853cd229bf96c5_MIT18_325F15_Chapter4.pdf",
      "content": "Chapter 4\nAdjoint-state methods\nAs explained in section (3.4), the adjoint F ∗of the linearized forward (mod-\neling) operator F plays an important role in the formula of the functional\ngradient δJ\nδm of the least-squares cost function J:\nδJ [m] = F ∗(F[m] -d).\nδm\nWhile F is the basic linear map from model space to data space, F ∗is\nthe basic linear map from data space to model space. F ∗is not only the\nbuilding block of iterative optimization schemes, but the mere application of\nF ∗to data is the simplest form of \"imaging\". For instance, when the initial\nguess m(0) = m0 is a smooth background model reasonably close to the true\nsolution m, and when there is a sufficiently large number of receivers and/or\nsources, the first iteration of gradient descent,\nm(1) = αF ∗(d -F[m0]),\noften gives a good \"image\" of the scatterers (somewhat close to the actual\nεm1). For this reason, F ∗is often called the imaging operator.\nIt should also be noted that F ∗behaves not entirely unlike F -1, i.e., F\nis somewhat close to being unitary. This statement does not have a rigorous\nformulation of the form ∥F ∗F -I∥≤(. . .), but rather of the form \"F ∗F\ndoes not move singularities around like F or F ∗do\". More details on the\nmicrolocal aspects of this question will be given in chapter 8.1.\nForming the full matrix F = δF and transposing it is not a practical way\nδm\nto compute F ∗. The adjoint-state method provides an elegant solution to\nthis problem, resulting in what is called the \"imaging condition\".\n\nCHAPTER 4. ADJOINT-STATE METHODS\n4.1\nThe imaging condition\nFor any dr(t) function of the receiver index r and time t, and m(x) function\nof position x (here m and d are two arbitrary functions, not necessarily linked\nto one another by the forward model), we have\n⟨d, Fm⟩= ⟨F ∗d, m⟩.\nThe inner product on the left is in data space,\nX\n⟨d, Fm⟩=\nˆ T\ndr(t)u(xr, t) dt,\nu = Fm,\nr\nwhile the inner product on the right is in model space.\n⟨F ∗d, m⟩=\nˆ\n(F ∗d)(x)m(x) dx.\nRn\nThe relation u = Fm is implicitly encoded by the two equations\n\n∂2\nm0∂t2 -∆\n\nu = -m∂2u0\n∂t2 ,\n\nm0\n∂2\n\n-∆\nu0 = f.\n∂t2\nNote that a single right-hand side generates u0, and that we have omitted the\nsource subscript s in this section; we will return to multiples sources shortly.\nP The argument that isolates and makes explicit the contribution of m in\nr\nT dr(t)u(xr, t) dt is one of integration by parts. In order to integrate by\nparts in x, we need to turn the sum over receivers into an integral. This\ncan be achieved by considering a distributional extended dataset where each\nmeasurement dr(t) is accompanied by a Dirac delta located at xr:\nX\ndext(x, t) =\ndr(t)δ(x -xr).\nr\nWe then have\nT\n⟨d, Fm⟩=\nˆ\nRn\nˆ\ndext(x, t)u(x, t) dxdt.\nIn\norder to use the wave equation for u, a copy of the differential operator\nm\n∂2\n0 ∂t2 -∆\n\nneeds to materialize. This is done by considering an auxiliary\n\n4.1. THE IMAGING CONDITION\nfield q(x, t) that solves the same wave equation with dext(x, t) as a right-hand\nside:\n\n∂2\nm0\n\n-∆\nq(x, t) = dext(x, t),\nx ∈\nRn,\n(4.1)\n∂t\nwith as-yet unspecified \"boundary conditions\" in time.\nSubstituting this\nexpression for dext(x, t), and integrating by parts both in space and in time\nreveals\nˆ ˆ T\n\n∂2\n⟨d, Fm⟩=\nq(x, t)\nm0\nV\n∂t2 -∆\n\nu(x, t) dxdt\n+\nˆ\nV\nm0\n∂q\n∂t u|T\n0 dx -\nˆ\nV\nm0q∂u|T\n∂t 0 dx\n+\nˆ\n∂V\nˆ T ∂q\n∂nu dSxdt -\nˆ\n∂V\nˆ T\nq∂u dSxdt,\n∂n\nwhere V is a volume that extends to the whole of Rn, and ∂V is the boundary\nof V -- the equality then holds in the limit of V = Rn.\nThe boundary terms over ∂V vanish in the limit of large V by virtue\nof the fact that they involve u - a wavefield created by localized functions\nf, m, u0 and which does not have time to travel arbitrarily far within a time\n[0, T]. The boundary terms at t = 0 vanish due to u|t=0 = ∂u|t=0 = 0. As for\n∂t\nthe boundary terms at t = T, they only vanish if we impose\n∂q\nq|t=T =\n|t=T = 0.\n(4.2)\n∂t\nSince we are only interested in the values of q(x, t) for 0 ≤t ≤T, the\nabove are final conditions rather than initial conditions, and the equation\n(4.1) is run backward in time.\nThe wavefield q is called adjoint field, or\nadjoint state. The equation (4.1) is itself called adjoint equation. Note that\nq is not in general the physical field run backward in time (because of the\nlimited sampling at the receivers), instead, it is introduced purely out of\ncomputational convenience.\nThe analysis needs to be modified when boundary conditions are present.\nTypically, homogeneous Dirichlet and Neumann boundary conditions should\nthe same for u0 and for q - a choice which will manifestly allow to cancel\nout the boundary terms in the reasoning above - but absorbing boundary\nconditions involving time derivatives need to be properly time-reversed as\nwell. A systematic, painless way of solving the adjoint problem is to follow\n\nCHAPTER 4. ADJOINT-STATE METHODS\nthe following sequence of three steps: (i) time-reverse the data dext at each\nreceiver, (ii) solve the wave equation forward in time with this new right-\nhand-side, and (iii) time-reverse the result at each point x.\nWe can now return to the simplification of the left-hand-side,\n⟨d, Fm⟩=\nˆ\nRn\nˆ T\n\n∂2\nq(x, t)\nm0\n∂t2 -∆\n\nu(x, t) dxdt\n= -\nˆ\nRn\nˆ T\nq(x, t)m(x)∂2u0 dxdt\n∂t2\nThis quantity is also supposed to be ⟨m, F ∗d⟩, regardless of m, so we conclude\n(F ∗\nd)(x) =\nˆ T\n∂2u\n-\nq(x, t)\ndt.\n(4.3)\n∂t2\nThis equation is called the imaging condition: it expresses the action of F ∗\non d as the succession of the following steps:\n1. Place data dr(t) at the location of the receivers with point masses to\nget dext;\n2. Use dext as the right-hand side in the adjoint wave equation to get the\nadjoint, backward field q;\n3. Simulate the incident, forward field u0; and finally\n4. Take the time integral of the product of the forward field u0 (differen-\ntiated twice in t), and the backward field q, for each x independently.\nThe result is a function of x which sometimes serves the purpose of image,\nand may sometimes be called Im(x). Note that we have not performed a\nfull inversion; if d are measured data, then Im is not the model m that gave\nrise to d. In seismology, the imaging condition (4.3) is called reverse-time\nmigration, or simply migration. In radar, the imaging condition does not have\na particular name, but in the next chapter we will encounter a simplification\nof (4.3) called backprojection.\nIf we now restore the presence of multiple sources, the wavefields u, u0,\nand u1 will depend on the source index s. The source term fs -- typically of\nthe form w(t)δ(x -xs) -- is in the right-hand side of the wave equations for\n\n4.1. THE IMAGING CONDITION\nu0 and u, while u1 implicitly depends on fs through u0. For a fixed source\ns, we denote\nus = Fs[m],\nu0,s = Fs[m0],\nu1,s = Fsm1,\nwhile we continue to denote u = F[m], u0 = F[m0] and u1 = Fm1 for the\ncollection of such wavefields over s.\nThe data inner-product now has an additional sum over s, namely\nX X\nT\n⟨d, Fm⟩=\nd\ns\nˆ\nr,s(t)us(xr, t) dt.\nr\nThe formula for F ∗can be obtained by taking adjoints one s at a time,\nnamely\nX\n⟨F ∗d, m⟩= ⟨d, Fm⟩=\n⟨ds, Fsm⟩\nX\ns\n=\n⟨F ∗\ns ds, m⟩\nX\ns\n= ⟨\nF ∗\ns ds, m⟩,\ns\nhence\nX\nF ∗=\nF ∗\ns .\ns\nMore explicitly, in terms of the imaging condition,\nX\n(F ∗\n,s\nd)(x) =\ns\nˆ T\n∂2u0\n-\nqs(x, t)\n(x, t) dt,\n(4.4)\n∂t2\nwhere the adjoint field qs is relative to the source s:\n\n∂2\nm0\n\n-∆\nqs(x, t) = dext,s(x, t).\n∂t2\nThe sum over s in the new imaging condition (4.4) is sometimes called a stack.\nIt is often the case that particular images F ∗\ns d are not very informative on\ntheir own, but a stack uses the redundancy in the data to bring out the\ninformation and reveal more details.\nThe mathematical tidbit underlying stacks is that the operation of creat-\ning a vector (x, x, . . . , x) out of a single number x has for adjoint the operation\nof summing the components of a vector.\n\nCHAPTER 4. ADJOINT-STATE METHODS\n4.2\nThe imaging condition in the frequency\ndomain\nWe now modify the exposition to express both the adjoint-state equation\nand the imaging condition in the frequency (ω) domain. The nugget in this\nsection is that complex conjugation in ω corresponds to time reversal. We\nassume a single source for simplicity.\nWe are again interested in finding F ∗such that ⟨d, Fm⟩= ⟨F ∗d, m⟩for\nall generic d and m. The data inner product ⟨d, Fm⟩can be expressed in the\nfrequency domain by means of the Parseval formula,\nX\n⟨d, Fm⟩= 2π\nr\nˆ\ndbr(ω)\nR\nX\n(\\\nFm)(xr, ω)dω =\nˆ\ndr(t)(Fm)(xr, t)dt.\nr\nThe complex conjugate is important, now that we are in the frequency do-\nmain - though since the overall quantity is real, it does not matter which\nof the two integrand's factors it is placed on. As previously, we pass to the\nextended dataset\nX\ndd(x, ω) =\ndb\next\nr(ω)δ(x -xr),\nr\nand turn the sum over r into an integral over x. The linearized scattered\nfield is\n(\\\nFm)(x , ω) =\nˆ\nGb\nr\n(x, y; ω)m(y)ω ub0(y, ω) dy.\n(4.5)\nTo simplify the resulting expression of ⟨d, Fm⟩, we let\nqb(x, ω) =\nˆ\nGb(y, x; ω)dd\next(y, ω) dy.\n(4.6)\n(Note that Green's functions are always symmetric under the swap of x and\ny, as we saw in a special case in one of the exercises in chapter 1.) It follows\nthat\n\n⟨d, Fm⟩=\nˆ\nm(y) 2π\nˆ\nqb(y, ω)ω2\nR\n\nub0(y, ω) dω\ndy,\nhence\nF ∗d(y) = 2π\nˆ\nqb(y, ω)ω2\nR\nbu0(y, ω) dω.\n(4.7)\n\nThe integral in t in (4.3) is over [0, T] because such is the support of\nq ∂2u0.\nThe integral in ω in (4.7) is over\n.\nIt is tempting to truncate\n∂t2\nR\nthis integral to \"the frequencies that have been measured\" -- but that is\nstrictly speaking incompatible with the limits on t (for the same reason that a\nfunction compactly supported in time cannot also be compactly supported in\nfrequency.) Careful consideration of cutoffs is needed to control the accuracy\nof a truncation in ω.\nEquation (4.7) is valuable for a few different reasons:\n- It can be further simplified to highlight its geometrical content as an\napproximate curvilinear integral, such as explained in the next chapter;\n- The integral over ω can be deliberately restricted in the scope of descent\niterations, so as to create sweeps over frequencies. This is sometimes\nimportant to deal with the lack of convexity of full inversion; see chapter\n9.\n4.3\nThe general adjoint-state method\nIn this section we explain how to use the adjoint-state method to compute the\nfirst and second variations of an objective function J[u(m)] in a parameter\nm, when u is constrained by the equation L(m)u = f, where L(m) is a linear\noperator that depends on m.\n1Time reversal of any real-valued function becomes conjugation in the Fourier domain:\nbf(ω) =\nˆ\neiωtf(t) dt =\nˆ\ne-iωtf(t)dt =\nˆ\neiωtf(-t) dt.\n4.3. THE GENERAL ADJOINT-STATE METHOD\nThis equation is the same as (4.3), by Parseval's identity. Equation (4.6) is\nthe integral version of (4.1) in the frequency domain. The complex conju-\ngation of Gb in (4.6) is the expression in the frequency domain of the fact\nthat the adjoint equation is solved backwards in time1. We can alternatively\ninterpret qb = bGd\ndext by applying an extra conjugate, namely bq = bGdd\next, which\ncan be read as the sequence of operations: (i) time-reverse dext, (ii) propa-\ngate it forward in time, and (iii) time-reverse the result. This is the same\nprescription as in the time-domain case, and offers the added advantage of\nnot having to rethink the boundary condition for the backward equation.\n\nCHAPTER 4. ADJOINT-STATE METHODS\nWe refer to \"u space\", \"m space\", and \"f space\" for the respective L2\nspaces containing u, m, and f. The first variation of J is simply\nδJ\nδm = ⟨δJ\nδu, δu ⟩u,\n(4.8)\nδm\nwhere the inner product pairs δu in each equation, hence acts in u space.\nNotice that δu/δm is an operator acting in m space and returning a function in\nu space2.\nIf we were interested in computing the directional derivative of J in some\ndirection m, namely ⟨δJ\nδm, m⟩, then we would simply swap the m-space inner\nproduct with the u-space inner product from (4.8), and recognize that u =\nδu m is easily accessible by solving the linearized version of the equation\nδm\nL(m)u = f. This result is straightforward to obtain, and the adjoint-state\nmethod is not necessary.\nThe interesting problem is that of computing\nδJ as a function in m-\nδm\nspace.\nIn principle, equation (4.8) is all that is needed for this purpose,\nexcept that explicitly computing the full kernel of the operator\nδu\nδm can be\nhighly inefficient in comparison to the complexity of specifying the function\nδJ .\nδm\nThe adjoint-state method is a very good way of eliminating\nδu\nδm so that\nδJ can be computed in more favorable complexity. In order to achieve this,\nδm\ndifferentiate the \"state equation\" L(m)u = f with respect to m to get\nδL\nδmu + L δu = 0.\nδm\nWe see that\nδu can be eliminated by composition with L on the left. The\nδm\nmain idea of the adjoint-state method is that a copy of L can materialize\nin (4.8) provided the other factor, δJ , is seen as the adjoint of L applied to\nδu\nsome field q,\nL∗\nδJ\nq =\n,\n(adjoint-state equation)\n(4.9)\nδu\nwith q naturally called the adjoint field. Then,\nδJ\nδm = ⟨L∗q, δu\nδm⟩u = ⟨q, L δu ⟩f\n(4.10)\nδm\nδL\n= -⟨q, δmu⟩f\n(imaging condition)\n2It relates to what we called F = δF/δm earlier by the operator S of sampling at the\nreceivers, via F = Su or δF/δm = Sδu/δm.\n\n4.3. THE GENERAL ADJOINT-STATE METHOD\nThis latter expression is often much easier to compute than (4.8).\nExample 1. In the setting of section 4.1, m is a function of x; u and f\nare functions of (x, t). The state equation L(m)u = f is the forward wave\nequation m∂2\nt u -∆u = f with zero initial conditions. When we evaluate all\nour quantites at m0, then u becomes u0, the incident field. The adjoint-state\nequation L∗q =\nδJ\nδu is the backward wave equation m∂2\nt q -∆q =\nδJ\nδu with\nzero final conditions. The least-squares cost function is J[u] = 1∥Su -d∥2\nwith S the operator of sampling at the receivers, so that the adjoint source\nδJ = S∗(Su -d) is the data residual extended to a function of x and t.\nδu\nThe quantity\nδL u is a multiplication operator from m-space to f-space\nδm\n(which takes the function m to the function m∂2\nt u), expressed in coordinates\nas\nδL\n(δm(y)u)(x, t) = δ(x -y) ∂2\nt u(x, t),\n(x ∈R3, y ∈R3.)\nUsing the formula above, -⟨q, δL u⟩f becomes the usual imaging condition\nδm\n-\n\nq(x, t)∂2\nt u(x, t) dt.\nThe adjoint-state method also allows to compute second variations. We\nreadily compute the functional Hessian of J as\nδ2J\nδmδm′ = ⟨δu\nδm, δ2J\nδuδu′\nδu′\nδm′⟩u + ⟨δJ\nδu,\nδ2u\n⟩u,\nδmδm′\nδ2J\n⟨u, δuδu′u′⟩u + ⟨δJ\nδu, v⟩u.\nThe three functions u =\nδu\nδmm, u′ =\nδu′\nm′\nδm′\n, and v = ⟨m,\nδ u m′\nδmδm′\n⟩can be\ncomputed by solving simple (linearized) equations derived from the state\nequation L(m)u = f. (See a few exercises at the end of this chapter.) The\n3Here m and m′ are functions, hence can be seen as independent variables, i.e., points,\nin a function space. Free, or unpaired variables are to functional calculus what free indices\nare to vector calculus: they are \"open slots\" available to be filled by pairing with a\nfunction/vector in an inner product.\nwhere the u-space inner product pairs the δu factors in each expression.\nThis Hessian is an object with two free variables3 m and m′. If we wish\nto view it as a bilinear form, and compute its action ⟨m,\nδ2J\nδmδm′m′⟩m on two\nfunctions m and m′, then it suffices to pass those functions inside the u-space\ninner product to get\n\nCHAPTER 4. ADJOINT-STATE METHODS\nadjoint-state method is not needed for evaluating the Hessian as a bilinear\nform.\nOn the other hand, we are generally not interested in computing the full\nmatrix representation of\nδ2J\n′\n′ with row index m and column index m either:\nδmδm\nthis object is too large to store.\nThe most interesting question is the computation of the action of the\nHessian as an operator on a function, say m′. While m′ is paired, the variable\nm remains free, hence the result is a function in m-space. This problem can\nbe solved by the second-order adjoint-state method, which we now explain.\nAs earlier, we can pass m′ inside the u-space inner product to get\nδ2J\nδmδm′m′ = ⟨δu\nδm, δ2J\nδuδu′u′⟩+ ⟨δJ\nδu,\nδ2u\nm′⟩,\n(4.11)\nδmδm′\nwith u′ = δu′ m′\nδm′\neasily computed. However, two quantities are not immedi-\nately accessible:\n1. the remaining δu\nδm factor with the un-paired m variable, and\n2. the second-order\nδ2u m′\nδmδm′\nfactor.\nBoth quantities have two free variables u and m, hence are too large to store,\nlet alone compute.\nThe second-order factor can be handled as follows. The second variation\nof the equation L(m)u = f is\nδ2L\nδmδm′u + δL\nδm\nδu\nδm′ + δL\nδm′\nδu\nδm + L\nδ2u\n= 0.\nδmδm′\nWe see that the factor\nδ2u\napplied\nδ\n′ can be eliminated provided L is\nto it\nmδm\non the left. We follow the same prescription as in the first-order case, and\ndefine a first adjoint field q1 such that\n∗\nδJ\nL q1 =\n.\n(1st adjoint-state equation)\n(4.12)\nδu\nA substitution in (4.11) reveals\nδ2J\nδmδm′m′ = ⟨δu\nδm, δ2J\nδuδu′u′⟩u -⟨q1,\nδL\nδm′m′\nδu ⟩f\nδm\nδ2L\n-⟨q1,\nδmδm′m′\n\nu + δL\nδmu′⟩f,\n\n4.3. THE GENERAL ADJOINT-STATE METHOD\nwith u′ =\nδu\n′\n′m , as earlier. The term on the last row can be computed as is;\nδm\nall the quantities it involves are accessible. The two terms in the right-hand-\nside of the first row can be rewritten to isolate δu/δm, as\nδ2J\n⟨δuδu′u′ -\nδL\nδm′m′\n∗\nq1, δu\nδm⟩u.\nIn order to eliminate\nδu by composing it with L on the left, we are led to\nδm\ndefining a second adjoint-state field q2 via\nL∗\nδ2J\nq2 = δuδu′u′ -\nδL\n∗\nm′\nq1.\n(2nd adjoint-state equation)\n(4.13)\nδm′\nAll the quantities in the right-hand side are available. It follows that\n⟨L∗\nδu\nq2, δm⟩u = ⟨q2, L δu\nδm⟩= -⟨q2, δL u⟩f.\nδm\nGathering all the terms, we have\nδ2J\nδmδm′m′ = -⟨q2, δL\nδmu⟩f -⟨q1,\nδ2L\nδmδm′m′\n\nu + δL u′⟩f,\nδm\nwith q1 obeying (4.12) and q2 obeying (4.13).\nExample 2. In the setting of section 4.1, call the base model m = m0 and the\nmodel perturbation m′ = m1, so that u′ = δu\nδm[m0]m′ is the solution u1 of the\nlinearized wave equation m0∂2\nt u1 -∆u1 = -m1∂2\nt u0 with u0 = L(m0)u0 = f\nas previously. The first variation\nδL is the same as explained earlier, while\nδm\nthe second variation\nδ2L\nδmδm′ vanishes since the wave equation is linear in m.\nThus if we let H =\nδ2J\n-sp\nδ\n′ for the Hessian of J as an operator in m\nace, we\nmδm\nget\nHm1 = -\nˆ\nq2(x, t)∂2\nt u0(x, t) dt -\nˆ\nq1(x, t)∂2\nt u1(x, t) dt.\nThe first adjoint field is the same as in the previous example, namely q1 solves\nthe backward wave equation\nm0∂2\nt q1 -∆q\n∗\n1 = S (Su0 -d),\nzero f.c.\nTo get the equation for q , notice that\nδ2J\nδuδu′ = S∗S and\nδL\nδm′m1\n∗=\nδL m\nδm′\n1 =\nm1∂2\nt . Hence q2 solves the backward wave equation\nm0∂2\nt q2 -∆q2 = S∗Su1 -m1∂2\nt q1,\nzero f.c.\n\nCHAPTER 4. ADJOINT-STATE METHODS\nNote that the formulas (3.14) (3.15) for H that we derived in an ex-\nercise in the previous chapter are still valid, but they do not directly allow\nthe computation of H as an operator acting in m-space. The approximation\nH ≃F ∗F obtained by neglecting the last term in (3.14) is recovered in the\ncontext of second-order adjoints by letting q1 = 0.\n4.4\nThe adjoint state as a Lagrange multi-\nplier\nThe adjoint field q was introduced in a somewhat opportunistic and artificial\nway in earlier sections. In this section, we show that it has the interpretation\nof a Lagrange multiplier in a constrained optimization framework, where the\nwave equation serves as the constraint. The problem is the same as in the\nprevious section, namely to compute the functional gradient of J[u(m)] with\nrespect to m - and the resulting expression is the same as previously - but\nwe show how the Lagrangian construction offers a new way of interpreting\nit.\nInstead of considering J[u(m)] as a functional to minimize on m, we now\nview J as a function of u only, and accommodate the constraint L(m)u = f\nby pairing it with a function q in f-space in order to form the so-called\nLagrangian4\nL[u, m, q] = J[u] -⟨q, L(m)u -f⟩f.\nThe function q is called Lagrange multiplier in this context, and is arbitrary\nfor the time being. Notice that L[u(m), m, q] = J[u(m)] regardless of q when\nu = u(m), i.e., when the constraint is satisfied.\nThis expression can be\ndifferentiated to give the desired quantity, namely\nd\nδL\nJ[u(m)] = ⟨\ndm\nδu , δu\nδm⟩+ δL .\n(4.14)\nδm\nThe partials of L are computed as\n-\nδL\nδu = δJ -L∗q, since ⟨q, Lu⟩f = ⟨L∗q, u⟩u,\nδu\n-\nδL\nδm = -⟨q, δL\nδmu⟩f,\n-\nδL = L(m)u -f.\nδq\n4We reserve the letter L for the Lagrangian and L for the state equation.\n\n4.5. EXERCISES\nIn convex optimization, the traditional role of the Lagrangian is that putting\nto zero its partials5 is convenient way of deriving the optimality conditions\nthat hold at critical points, both for the primal and the dual problems. In\nparticular, if the partials of L are zero at (u, m, q), then m is a critical point\nof J[u(m)].\nThe way we make use of the Lagrangian in this section is different, because\nwe aim to derive the expression of\nd\ndmJ[u(m)] away from critical points. In\nparticular, we are not going to require δL\nδm to be zero. Still, we find it advan-\ntageous to put δL = 0 as a convenient choice to help simplify the expression\nδu\nof the gradient of J. Indeed, in that case we recover the imaging condition\n(4.10) from\nd\ndmJ[u(m)] = δL\nδm = -⟨q, δL u⟩f.\nδm\nIt is always possible to achieve δL = 0, by defining q to be the solution of\nδu\nL∗q = δJ , the adjoint-state equation (4.9). Note that putting δL\nδu\n= 0 recovers\nδq\nthe state equation L(m)u = f.\n4.5\nExercises\n1. Starting from an initial guess model m0, a known source function f,\nand further assuming that the Born approximation is valid, explain\nhow the inverse problem d = F[m] can be completely solved by means\nof F -1, the inverse of the linearized forward operator (provided F is\ninvertible). The intermediate step consisting in inverting F is called\nthe linearized inverse problem.\nSolution. Form the incident field as u0 = Gf. Subtract from observed\ndata to get d -u0. Since the Born approximation is assumed valid, we\nhave d -u0 ≃εu1. Invert for m1 by solving the system u1 = Fm1, i.e.,\nm1 = F -1u1. Then form m = m0 + εm1.\n2. Consider the forward wave equation for u0 in one spatial dimension\nwith an absorbing boundary condition of the form ( 1 ∂t -∂x)u(0) = 0\nc(0)\nat the left endpoint x = 0 of the interval [0, 1]. Assume that c(x) is\nlocally uniform and equal to c(0) in a neighborhood of x = 0.\n5Or letting 0 be a subgradient of L in the non-smooth case.\n\nCHAPTER 4. ADJOINT-STATE METHODS\n(a) Argue why this choice of boundary condition accommodates left-\ngoing waves, but not right-going waves.\n(b) Find the corresponding boundary condition for the adjoint-state\nequation on the backwards field q.\n3. Snapshot migration. The treatment of reverse-time migration seen ear-\nlier involves data u(xr, t) for an interval in time t, and at fixed receiver\npoints xr. Consider instead the snapshot setup, where t is fixed, and\nthere are receivers everywhere in the domain of interest. (So we have\nfull knowledge of the wavefield at some time t.) Repeat the analysis\nof the imaging operator, adjoint to the forward operator that forms\nsnapshot data from singly scattered waves. In particular, find what\nthe adjoint-state wave equation becomes in this case. [Hint: it involves\nnonzero final data, and a zero right-hand side.]\n4. Sampling. Call S the linear operator that maps a function f(x) to the\nvector of point samples {f(xr)}r. Find a formula for S∗. Note that\nwhen the linearized forward model F has S as its last operation, then\nthe imaging operator F ∗has S∗as its first operation. The presence of\nS∗explains why we passed from dr(t) to dext(x, t) in the first step of\nthe derivation of the imaging operator.\n5. Repeat the general adjoint-state theory by assuming a possibly nonlin-\near state equation of the form L(m, u) = f.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Calculus of Variations, Functional Derivatives",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/834e98a77f64597a422c205b123cd6d7_MIT18_325F15_Appendix_A.pdf",
      "content": "Appendix A\nCalculus of variations,\nfunctional derivatives\nThe calculus of variations is to multivariable calculus what functions are\nto vectors. It answers the question of how to differentiate with respect to\nfunctions, i.e., objects with an uncountable, infinite number of degrees of\nfreedom. Functional calculus is used to formulate linearized forward models\nfor imaging, as well as higher-order terms in Born series. It is also useful\nfor finding stationary-point conditions of Lagrangians, and gradient descent\ndirections in optimization.\nLet X, Y be two function spaces endowed with norms and inner products\n(technically, Hilbert spaces). A functional φ is a map from X to R. We\ndenote its action on a function f as φ(f). An operator F is a map from X\nto Y . We denote its action on a function f as Ff.\nWe say that a functional φ is Fr echet differentiable at f ∈X when there\nexists a linear functional A : X 7→R such that\n|φ(f + h) -φ(f) -A(h)|\nlim\nh→0\n= 0.\n∥h∥\nIf this relation holds, we say that A is the functional derivative, or Fr echet\nderivative, of φ at f, and we denote it as\nδφ\nA =\n[f].\nδf\nIt is also called the first variation of φ. It is the equivalent of the gradient in\nmultivariable calculus. The fact that A is a map from X to R corresponds\n\n124APPENDIX A. CALCULUS OF VARIATIONS, FUNCTIONAL DERIVATIVES\nto the idea that a gradient maps vectors to scalars when paired with the dot\nproduct, to form directional derivatives. If X = Rn and f = (f1, . . . , fn), we\nhave\nδφ[f](h) = ∇φ(f) · h.\nδf\nFor this reason, it is is also fine to write A(h) = ⟨A, h⟩.\nThe differential ratio formula for δφ\nδf is called Gˆateaux derivative,\nδφ\nφ(f + th) -φ(f)\n[f](h) = lim\nδf\nt→0\n,\n(A.1)\nt\nwhich corresponds to the idea of the directional derivative in Rn.\nExamples of functional derivatives:\n- φ(f) = ⟨g, f⟩,\nδφ\nδf [f] = g,\nδφ[f](h) = ⟨g, h⟩\nδf\nBecause φ is linear, δφ = φ. Proof: φ(f + th) -φ(f) = ⟨g, f + th⟩-\nδf\n⟨g, f⟩= t⟨g, h⟩, then use (A.1).\n- φ(f) = f(x0),\nδφ[f] = δ(x -x0),\n(Dirac delta).\nδf\nThis is the special case when g(x) = δ(x -x0). Again, δφ = φ.\nδf\n- φ(f) = ⟨g, f 2⟩,\nδφ[f] = 2fg.\nδf\nProof: φ(f + th) -φ(f) = ⟨g, (f + th)2⟩-⟨g, f⟩= t⟨g, 2fh⟩+ O(t2) =\nt⟨2fg, h⟩+ O(t2), then use (A.1).\nNonlinear operators F[f] can also be differentiated with respect to their\ninput function. We say F : X →Y is Fr echet differentiable when there exists\na linear operator F : X →Y\n∥F[f + h] -F[f] -Fh∥\nlim\nh→0\n∥h∥\n= 0.\n\nF is the functional derivative of F, and we write\nδF\nF =\n[f].\nδf\nWe still have the difference formula\nδF\nδf [f]h = lim\nt→0\nF[f + th] -F[f].\nt\nExamples:\n- F[f] = f. Then\nδF [f] = I,\nδf\nthe identity. Proof: F is linear hence equals its functional derivative.\nAlternatively, apply the difference formula to get δF [f]h = h.\nδf\n- F[f] = f 2. Then\nδF [f] = 2f,\nδf\nthe operator of multiplication by 2f.\nUnder a suitable smoothness assumption, the Fr echet Hessian of an op-\nerator F can also be defined: it takes two functions as input, and returns\na function in a linear manner (\"bilinear operator\"). It is defined through a\nsimilar finite-difference formula\nδ2F\n⟨δf 2 [f]h1, h2⟩= lim\nt→0\nF[f + t(h2 + h1)] -F[f + th2] -F[f + th1] + F[f].\nt2\nThe Hessian is also called second variation of F. For practical calculations\nof the Hessian, the notation δ2F is\nδf2\ntoo cavalier. Instead, it is useful to view\nthe Hessian as the double directional derivative\nδ2F\nδfδf ′\nin two directions f and f ′, and compute those derivatives one at a time. This\nformula is the equivalent of the mixed partial\n∂2f\nwhen the two directions\n∂xi∂xj\nare xi and xj in n dimensions.\n\n126APPENDIX A. CALCULUS OF VARIATIONS, FUNCTIONAL DERIVATIVES\nFunctional derivatives obey all the properties of multivariable calculus,\nsuch as chain rule and derivative of a product (when all the parties are\nsufficiently differentiable).\nWhenever in doubt when faced with calculations involving functional\nderivatives, keep track of free variables vs. integration variables -- the equiv-\nalent of \"free indices\" and \"summation indices\" in vector calculus. For in-\nstance,\n-\nδF\nδf is like δFi\nδfj , with two free indices i and j;\n-\nδF\nδf h is like P\nj\nδFihj, with one free index i and one summation index j.\nδfj\n-\nδ2F\nδf2 is like\nδ2Fi\nδfjδfk , with three free indices i, j, k.\n- ⟨δ2F\nδf2 h1, h2⟩is like P\nj,k\nδ2Fi (h1)j(h2)k, with one free index i and two\nδfjδfk\nsummation indices j and k.\nNo free index indicates a scalar, one free index indicates a function (or a\nfunctional), two free indices indicate an operator, three indices indicate an\n\"object that takes in two functions and returns one\", etc.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Computerized Tomography",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/fe6e815d7abf9c0ca3351174ec6c585d_MIT18_325F15_Chapter6.pdf",
      "content": "Chapter 6\nComputerized tomography\n6.1\nAssumptions and vocabulary\n(...)\nComputerized tomography (CT scans, as well as PET scans) imaging\ninvolves inversion of a Radon or X-ray transform. It is primarily used for\nmedical imaging.\nIn two spatial dimensions, the variables in the Radon domain are t (offset)\nand θ (angle).\nData in the form d(t, θ) corresponds to the parallel beam\ngeometry. More often, data follow the fan-beam geometry, where for a given\nvalue of θ the rays intersect at a point (the source of X-rays), and t indexes\nrays within the fan. The transformation to go from parallel-beam to fan-\nbeam and back is\ndfan(t, θ) = dpara(t, θ + (at + b)),\nfor some numbers a and b that depend on the acquisition geometry. Datasets\nin the Radon domain are in practice called sinograms, because the Radon\ntransform of a Dirac mass is a sine wave1.\n6.2\nThe Radon transform and its inverse\nRadon transform:\n(Rf)(t, θ) =\nˆ\nδ(t -x · eθ)f(x) dx,\n1More precisely, a distribution supported on the graph of a sine wave, see an exercise\nat the end of the chapter.\n\nCHAPTER 6. COMPUTERIZED TOMOGRAPHY\nwith eθ = (cos θ, sin θ)T.\nFourier transform in t / Fourier-slice theorem2:\nRcf(ω, θ) =\nˆ\ne-iωx·eθf(x) dx.\nAdjoint Radon transform / (unfiltered) backprojection:\nR∗d(x) =\nˆ\neiωx·eθdb(ω, θ) dωdθ\n=\nˆ\nδ(t -x · eθ)d(t, θ) dtdθ\n=\nˆ\nd(x · θ, θ) dθ\nInverse Radon transform / filtered backprojection in the case of two spa-\ntial dimensions:\n-1\nR\nd(x) =\nθ\n(2\nˆ\neiωx·eθdb(ω, ) ω dωdθ.\nπ)n\n(notice the factor ω.)\nFiltered backprojection can be computed by the following sequence of\nsteps:\n- Take a Fourier transform to pass from t to ω;\n- Multiply by ω;\n- Take an inverse Fourier transform from ω back to t, call D(t, θ) the\nresult;\n- Compute\n\nd(x · θ, θ) dθ by quadrature and interpolation (piecewise\nlinear interpolation is often accurate enough.)\n2The direct Fourier transform comes with e-iωt. Here t is offset, not time, so we use\nthe usual convention for the FT.\n\n6.3. EXERCISES\n6.3\nExercises\n1. Compute the Radon transform of a Dirac mass, and show that it is\nnonzero along a sinusoidal curve (with independent variable θ and de-\npendent variable t, and wavelength 2π.)\n2. In this problem set we will form an image from a fan-beam CT dataset.\n(Courtesy Frank Natterer)\nDownload the data set at http://math.mit.edu/icg/ct.mat\nand load it in MATLAB(r) with load ct.mat\nThe array g is a sinogram.\nIt has 513 rows, corresponding to uni-\nformly sampled offsets t, and 360 columns, corresponding to uniform,\nall-around angular sampling with 1-degree steps in θ. The acquisition\nis fan-beam: a transformation is needed to recover the parallel-beam\ngeometry. The fan-beam geometry manifests itself in that the angle\ndepends on the offset t in a linear fashion. Instead of being just θ, it is\n(1 ≤t ≤513 is the row index)\nt -257\nθ +\nα,\nwith\nsin α =\n.\n2.87\nImaging from a parallel-beam sinogram is done by filtered backprojec-\ntion. Filtering is multiplication by ω in the ω domain dual to the offset\nt. Backprojection of a sinogram g(t, θ) is\nX\nI(x) =\ng(x · eθ, θ),\nθ\nwhere eθ is (cos θ, sin θ)T . Form the image on a grid which has at\nleast 100 by 100 grid points (preferably 200 by 200). You will need an\ninterpolation routine since x·eθ may not be an integer; piecewise linear\ninterpolation is accurate enough (interp1 in MATLAB).\nIn your writeup, show your best image, your code, and write no more\nthan one page to explain your choices.\n\nCHAPTER 6. COMPUTERIZED TOMOGRAPHY\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Finite Difference Methods for Wave Equations",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/bc9dbfa31205a987e59efb24e1a1c3d3_MIT18_325F15_Appendix_B.pdf",
      "content": "Appendix B\nFinite difference methods for\nwave equations\nMany types of numerical methods exist for computing solutions to wave\nequations - finite differences are the simplest, though often not the most\naccurate ones.\nConsider for illustration the 1D time-dependent problem\n∂2u\nm(x) ∂t2 = ∂2u + f(x, t),\nx ∈[0, 1],\n∂x2\nwith smooth f(x, t), and, say, zero initial conditions. The simplest finite\ndifference scheme for this equation is set up as follows:\n- Space is discretized over N + 1 points as xj = j∆x with ∆x =\n1 and\nN\nj = 0, . . . , N.\n- Time is discretized as tn = n∆t with n = 0, 1, 2, . . .. Call un\nj the com-\nputed approximation to u(xj, tn). (In this appendix, n is a superscript.)\n- The centered finite difference formula for the second-order spatial deriva-\ntive is\n∂2u\n∂x2(xj, tn) = un\nj+1 -2un\nj + un\nj-1 + O((∆x)2),\n(∆x)2\nprovided u is sufficiently smooth - the O(·) notation hides a multiplica-\ntive constant proportional to ∂4u/∂x4.\n\n128APPENDIX B. FINITE DIFFERENCE METHODS FOR WAVE EQUATIONS\n- Similarly, the centered finite difference formula for the second-order\ntime derivative is\n∂2u\n∂t2 (xj, tn) = un+1\nj\n-2un\nj + un-1\nj\n+ O((∆t)2),\n(∆t)2\nprovided u is sufficiently smooth.\n- Multiplication by m(x) is realized by multiplication on the grid by\nm(xj). Gather all the discrete operators to get the discrete wave equa-\ntion.\n- The wave equation is then solved by marching: assume that the values\nof un-1\nj\nand un\nj are known for all j, then isolate un+1\nj\nin the expression\nof the discrete wave equation.\nDirichlet boundary conditions are implemented by fixing. e.g., u0 = a.\nNeumann conditions involve a finite difference, such as u1-u0\n∆x\n= a. The more\naccurate, centered difference approximation u1-u-1 = a with a ghost node at\n2∆x\nu-1 can also be used, provided the discrete wave equation is evaluated one\nmore time at x0 to close the resulting system. In 1D the absorbing boundary\ncondition has the explicit form 1∂tu±∂xu = 0 for left (-) and right-going (+)\nc\nwaves respectively, and can be implemented with adequate differences (such\nas upwind in space and forward in time).\nThe grid spacing ∆x is typically chosen as a small fraction of the rep-\nresentative wavelength in the solution. The time step ∆t is limited by the\nCFL condition ∆t ≤∆x / maxx c(x), and is typically taken to be a fraction\nthereof.\nIn two spatial dimensions, the simplest discrete Laplacian is the 5-point\nstencil which combines the two 3-point centered schemes in x and in y. Its\naccuracy is also O(max{∆x)2, (∆y)2}). Designing good absorbing boundary\nconditions is a somewhat difficult problem that has a long history.\nThe\ncurrently most popular solution to this problem is to slightly expand the\ncomputational domain using an absorbing, perfectly-matched layer (PML).\nMore accurate schemes can be obtained from higher-order finite differ-\nences. Low-order schemes such as the one explained above typically suffer\nfrom unacceptable numerical dispersion at large times. If accuracy is a big\nconcern, spectral methods (spectral elements, Chebyshev polynomials, etc.)\nare by far the best way to solve wave equations numerically with a controlled,\nsmall number of points per wavelength.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Front Matter, including Table of Contents",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/b8376f1e945862668d11626e0f0e55d5_MIT18_325F15_FrontMatter.pdf",
      "content": "Waves and Imaging\nClass notes - 18.325\nLaurent Demanet\nDraft April 7, 2015\n\nPreface\nIn the margins of this text we use\n- the symbol (!) to draw attention when a physical assumption or sim-\nplification is made; and\n- the symbol ($) to draw attention when a mathematical fact is stated\nwithout proof.\nThanks are extended to the following people for discussions, suggestions,\nand contributions to early drafts: William Symes, Thibaut Lienart, Nicholas\nMaxwell, Pierre-David Letourneau, Russell Hewett, and Vincent Jugnon.\nThese notes are accompanied by computer exercises in Python, that\nshow how to code the adjoint-state method in 1D, in a step-by-step fash-\nion, from scratch. They are provided by Russell Hewett, as part of our\nsoftware platform, the Python Seismic Imaging Toolbox (PySIT), available\nat http://pysit.org.\n\nContents\nWave equations\n1.1\nPhysical models . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.1\nAcoustic waves\n. . . . . . . . . . . . . . . . . . . . . .\n1.1.2\nElastic waves\n. . . . . . . . . . . . . . . . . . . . . . .\n1.1.3\nElectromagnetic waves . . . . . . . . . . . . . . . . . .\n1.2\nSpecial solutions\n. . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1\nPlane waves, dispersion relations\n. . . . . . . . . . . .\n1.2.2\nTraveling waves, characteristic equations . . . . . . . .\n1.2.3\nSpherical waves, Green's functions . . . . . . . . . . . .\n1.2.4\nThe Helmholtz equation . . . . . . . . . . . . . . . . .\n1.2.5\nReflected waves . . . . . . . . . . . . . . . . . . . . . .\n1.3\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nGeometrical optics\n2.1\nTraveltimes and Green's functions . . . . . . . . . . . . . . . .\n2.2\nRays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3\nAmplitudes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4\nCaustics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nScattering series\n3.1\nPerturbations and Born series . . . . . . . . . . . . . . . . . .\n3.2\nConvergence of the Born series (math)\n. . . . . . . . . . . . .\n3.3\nConvergence of the Born series (physics) . . . . . . . . . . . .\n3.4\nA first look at optimization\n. . . . . . . . . . . . . . . . . . .\n3.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nCONTENTS\nAdjoint-state methods\n4.1\nThe imaging condition . . . . . . . . . . . . . . . . . . . . . .\n4.2\nThe imaging condition in the frequency domain\n. . . . . . . .\n4.3\nThe general adjoint-state method . . . . . . . . . . . . . . . .\n4.4\nThe adjoint state as a Lagrange multiplier . . . . . . . . . . .\n4.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nSynthetic-aperture radar\n5.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . .\n5.2\nForward model\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3\nFiltered backprojection . . . . . . . . . . . . . . . . . . . . . .\n5.4\nResolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nComputerized tomography\n6.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . .\n6.2\nThe Radon transform and its inverse . . . . . . . . . . . . . .\n6.3\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nSeismic imaging\n7.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . . 103\n7.2\nKirchhoffmodeling and migration . . . . . . . . . . . . . . . . 105\n7.3\nDepth extrapolation\n. . . . . . . . . . . . . . . . . . . . . . . 106\n7.4\nExtended modeling . . . . . . . . . . . . . . . . . . . . . . . . 106\n7.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nMicrolocal analysis of imaging\n8.1\nPreservation of the wavefront set\n. . . . . . . . . . . . . . . . 107\n8.2\nCharacterization of the wavefront set . . . . . . . . . . . . . . 113\n8.3\nPseudodifferential theory . . . . . . . . . . . . . . . . . . . . . 119\n8.4\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nOptimization\n9.1\nRegularization and sparsity\n. . . . . . . . . . . . . . . . . . . 121\n9.2\nDimensionality reduction techniques . . . . . . . . . . . . . . . 121\nA Calculus of variations, functional derivatives\nB Finite difference methods for wave equations\n\nCONTENTS\nC Stationary phase\n\nCONTENTS\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Waves and Imaging, Full set of lecture notes in one file",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/c5db24b1a6d0d3301b1a3f21f6ceba3a_MIT18_325F15_CompleteLect.pdf",
      "content": "Waves and Imaging\nClass notes - 18.325\nLaurent Demanet\nDraft April 7, 2015\n\nPreface\nIn the margins of this text we use\n- the symbol (!) to draw attention when a physical assumption or sim-\nplification is made; and\n- the symbol ($) to draw attention when a mathematical fact is stated\nwithout proof.\nThanks are extended to the following people for discussions, suggestions,\nand contributions to early drafts: William Symes, Thibaut Lienart, Nicholas\nMaxwell, Pierre-David Letourneau, Russell Hewett, and Vincent Jugnon.\nThese notes are accompanied by computer exercises in Python, that\nshow how to code the adjoint-state method in 1D, in a step-by-step fash-\nion, from scratch. They are provided by Russell Hewett, as part of our\nsoftware platform, the Python Seismic Imaging Toolbox (PySIT), available\nat http://pysit.org.\n\nContents\nWave equations\n1.1\nPhysical models . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.1\nAcoustic waves\n. . . . . . . . . . . . . . . . . . . . . .\n1.1.2\nElastic waves\n. . . . . . . . . . . . . . . . . . . . . . .\n1.1.3\nElectromagnetic waves . . . . . . . . . . . . . . . . . .\n1.2\nSpecial solutions\n. . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1\nPlane waves, dispersion relations\n. . . . . . . . . . . .\n1.2.2\nTraveling waves, characteristic equations . . . . . . . .\n1.2.3\nSpherical waves, Green's functions . . . . . . . . . . . .\n1.2.4\nThe Helmholtz equation . . . . . . . . . . . . . . . . .\n1.2.5\nReflected waves . . . . . . . . . . . . . . . . . . . . . .\n1.3\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nGeometrical optics\n2.1\nTraveltimes and Green's functions . . . . . . . . . . . . . . . .\n2.2\nRays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3\nAmplitudes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4\nCaustics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nScattering series\n3.1\nPerturbations and Born series . . . . . . . . . . . . . . . . . .\n3.2\nConvergence of the Born series (math)\n. . . . . . . . . . . . .\n3.3\nConvergence of the Born series (physics) . . . . . . . . . . . .\n3.4\nA first look at optimization\n. . . . . . . . . . . . . . . . . . .\n3.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nCONTENTS\nAdjoint-state methods\n4.1\nThe imaging condition . . . . . . . . . . . . . . . . . . . . . .\n4.2\nThe imaging condition in the frequency domain\n. . . . . . . .\n4.3\nThe general adjoint-state method . . . . . . . . . . . . . . . .\n4.4\nThe adjoint state as a Lagrange multiplier . . . . . . . . . . .\n4.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nSynthetic-aperture radar\n5.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . .\n5.2\nForward model\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3\nFiltered backprojection . . . . . . . . . . . . . . . . . . . . . .\n5.4\nResolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nComputerized tomography\n6.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . .\n6.2\nThe Radon transform and its inverse . . . . . . . . . . . . . .\n6.3\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nSeismic imaging\n7.1\nAssumptions and vocabulary . . . . . . . . . . . . . . . . . . . 103\n7.2\nKirchhoffmodeling and migration . . . . . . . . . . . . . . . . 105\n7.3\nDepth extrapolation\n. . . . . . . . . . . . . . . . . . . . . . . 106\n7.4\nExtended modeling . . . . . . . . . . . . . . . . . . . . . . . . 106\n7.5\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nMicrolocal analysis of imaging\n8.1\nPreservation of the wavefront set\n. . . . . . . . . . . . . . . . 107\n8.2\nCharacterization of the wavefront set . . . . . . . . . . . . . . 113\n8.3\nPseudodifferential theory . . . . . . . . . . . . . . . . . . . . . 119\n8.4\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nOptimization\n9.1\nRegularization and sparsity\n. . . . . . . . . . . . . . . . . . . 121\n9.2\nDimensionality reduction techniques . . . . . . . . . . . . . . . 121\nA Calculus of variations, functional derivatives\nB Finite difference methods for wave equations\n\nCONTENTS\nC Stationary phase\n\nCONTENTS\n\nChapter 1\nWave equations\n1.1\nPhysical models\n1.1.1\nAcoustic waves\nAcoustic waves are propagating pressure disturbances in a gas or liquid. With\np(x, t) the pressure fluctuation (a time-dependent scalar field) and v(x, t) the\nparticle velocity (a time-dependent vector field), the acoustic wave equations\nread\n∂v\n= -\n∂t\nρ0\n∇p,\n(1.1)\n∂p = -κ0∇· v.\n(1.2)\n∂t\nThe two quantities ρ0 and κ0 are the mass density and the bulk modulus,\nrespectively. They are linked to the wave speed c through κ0 = ρ0c2. Initial\nconditions on p and v must be supplied. A forcing term may be added to\nthe dynamic balance equation (1.1) when external forces (rather than initial\nconditions) create the waves.\nLet us now explain how these equations are obtained from a linearization\nof Euler's gas dynamics equations in a uniform background medium. Con- (!)\nsider the mass density ρ as a scalar field. In the inviscid case, conservation (!)\nof momentum and mass respectively read\n∂v\nρ( ∂t + v · ∇v) = -∇p,\n∂ρ\n∂t + ∇· (ρv) = 0.\n\nCHAPTER 1. WAVE EQUATIONS\nAn additional equation, called constitutive relation, must be added to close\nthe system. It typically relates the pressure and the density in an algebraic\nway, and encodes a thermodynamic assumption about compression and dila-\ntion. For instance if the gas is assumed to be ideal, and if the compression-\ndilation process occurring in the wave is adiabatic reversible (no heat trans-\nfer), then p ∼ργ, γ = 1.4, where ∼indicates equality up to a dimensional\nconstant. More generally, assume for the moment that the constitutive rela-\ntion takes the form\np = f(ρ)\nfor some scalar function f, which we assume differentiable and strictly in-\ncreasing (f ′(ρ) > 0 for all ρ > 0).\nConsider small disturbances offof an equilibrium state:\np = p0 + p1,\nρ = ρ0 + ρ1,\nv = v0 + v1.\nIn what follows, neglect quadratic quantities of p1, ρ1, v1. Consider a medium\nat rest ($): p0, ρ0 independent of t, and v0 = 0. After some algebraic simpli-\nfication the conservation of momentum becomes\n∂v1\nρ0\n= -∇p0 -∇p1.\n∂t\nTo zero-th order (i.e., at equilibrium, p1 = ρ1 = v1 = 0,) we have\n∇p0 = 0\n⇒\np0 constant in x.\nTo first order, we get\n∂v1\nρ0\n= -∇p1,\n∂t\nwhich is exactly (1.1) after renaming v1 →v, p1 →p. The constitutive\nrelation must hold at equilibrium, hence p0 constant in x implies that ρ0 is\nalso constant in x (uniform). Conservation of mass becomes\n∂ρ1 + ρ0∇· v1 = 0.\n∂t\nDifferentiate the constitutive relation to obtain p = f ′(ρ )ρ . Call f ′\n(ρ0) =\nc2, a number that we assume positive. Then we can eliminate ρ1 to get\n∂p1 + ρ0c2∇· v1 = 0.\n∂t\n\n1.1. PHYSICAL MODELS\nThis is exactly (1.2) with κ0 = ρ0c2.\nConveniently, the equations for acoustic waves in a variable medium ρ0(x),\nκ0(x) are obvious modifications of (1.1), (1.2):\n∂v\n∂t = -\n∇p,\n(1.3)\nρ0(x)\n∂p = -κ0(x)∇· v.\n(1.4)\n∂t\nA different argument is needed to justify these equations, however.\nThe\nprevious reasoning does not leave room for variable ρ0(x) or κ0(x). Instead,\nit is necessary to introduce a more realistic constitutive relation\np = f(ρ, s),\nwhere s is the entropy. An additional equation for conservation of entropy\nneeds to be considered. The new constitutive relation allows ρ0 and s0 to be\nfunctions of x in tandem, although p0 is still (necessarily) uniform in x. The\nreasoning leading to (1.3), (1.4) is the subject of an exercise in section 1.3.\nAcoustic waves can take the form of a first-order system of equations, or\nelse a second-order scalar equation. Combining (1.3), (1.4), we get\n∂2p\n∂t2 = κ0(x)∇· (\n∇p).\nρ0(x)\nInitial conditions on both p and ∂p/∂t must be supplied. This equation may\ncome with a right-hand side f(x, t) that indicates forcing. When ρ0 and κ0\nare constant, the scalar wave equation reduces to\n∂2p = c2 ∆\n∂t2\np.\nWaves governed by (1.3), (1.4) belong in the category of hyperbolic waves\nbecause they obey conservation of energy. Define\n\nv\n-1\nw =\n,\nL =\np\n\n∇\nρ0\n.\n-κ0∇·\nThen the acoustic system simply reads\n∂w = Lw.\n∂t\nL is called the generator of the evolution.\n\nCHAPTER 1. WAVE EQUATIONS\nDefinition 1. The system ∂w = Lw is said to be hyperbolic if L is a matrix\n∂t\nof first-order differential operators, and there exists an inner product ⟨w, w′⟩\nwith respect to which L∗= -L, i.e., L is anti-self-adjoint.\nAn adjoint operator such as L∗ is defined through the equation1\n⟨Lw, w′⟩= ⟨w, L∗w′⟩,\nfor all w, w′.\nFor instance, in the case of the acoustic system, the proper notion of inner\nproduct is (the factor 1/2 is optional)\n⟨w, w′\n⟩= 2\nˆ\n(ρ0v · v′ + 1 pp′) dx.\nκ0\nIt is an exercise in section 1.3 to show that ⟨Lw, w′⟩= ⟨w, L∗w′⟩for that\ninner product, for all w, w′.\nTheorem 1. If ∂w = Lw is a hyperbolic system, then E = ⟨w, w⟩is con-\n∂t\nserved in time.\nProof.\nd\n∂w\n⟨w, w⟩= ⟨\ndt\n∂t , w⟩+ ⟨w, ∂w\n∂t ⟩\n= 2 ⟨∂w, w⟩\n∂t\n= 2 ⟨Lw, w⟩\n= 2 ⟨w, L∗w⟩\n= 2 ⟨w, (-L)w⟩\n= -2 ⟨Lw, w⟩.\nA quantity is equal to minus itself if and only if it is zero.\nIn the case of acoustic waves,\nE = 2\nˆ\n(ρ0v2 + p2\nκ ) dx,\n1The existence of L∗can be traced back to the Riesz representation theorem once\n⟨Lw, w′⟩is shown to be a continuous functional of w in some adequate Hilbert space\nnorm.\n\n1.1. PHYSICAL MODELS\nwhich can be understood as kinetic plus potential energy. We now see that\nthe factor 1/2 was chosen to be consistent with the physicists' convention for\nenergy.\nIn the presence of external forcings the hyperbolic system reads ∂w/∂t =\nLw + f: in that case the rate of change of energy is determined by f.\nFor reference, common boundary conditions for acoustic waves include\n- Sound-soft boundary condition: Dirichlet for the pressure, p = 0.\n- Sound-hard boundary condition: Neumann for the pressure, ∂p = 0, or\n∂n\nequivalently v · n = 0.\nAnother important physical quantity is related to acoustic waves: the\n√\nacoustic impedance Z =\nρ0κ0. We will see later that impedance jumps\ndetermine reflection and transmission coefficients at medium discontinuities.\n1.1.2\nElastic waves\nElastic waves are propagating pressure disturbances in solids. The interesting\nphysical variables are\n- The displacement u(x, t), a time-dependent vector field. In terms of u,\nthe particle velocity is v = ∂u.\n∂t\n- The strain tensor\nε =\n(∇u + (∇u)T),\na symmetric time-dependent tensor field.\n- The stress tensor σ, also a symmetric time-dependent tensor field.\nFor elastic waves, the density ρ is very often assumed independent of t along\nparticle trajectories, namely ρ0(x, 0) = ρ0(x + u(x, t), t).\nThe equation of elastic waves in an isotropic medium (where all the waves (!)\ntravel at the same speed regardless of the direction in which they propagate)\nreads\n∂2u\nρ\n= ∇(λ∇· u) + ∇· (μ(∇u + (∇u)T)).\n(1.5)\n∂t2\nwhere ρ, λ, and μ may possibly depend on x. As for acoustic waves, a forcing\nterm is added to this equation when waves are generated from external forces.\n\nCHAPTER 1. WAVE EQUATIONS\nTo justify this equation, start by considering the equation of conservation\nof momentum (\"F = ma\"),\n∂v\nρ\n= ∇· σ,\n∂t\npossibly with an additional term f(x, t) modeling external\nPforces. The nota-\n∂σ\ntion ∇· indicates tensor divergence, namely (∇· σ)i =\nij\nj\n. Stress and\n∂xj\nstrain are linked by a constitutive relation called Hooke's law,\nσ = C : ε,\nwhere C is the 4-index elastic tensor. In three spatial dimensions, C has\nP components. The colon indicates tensor contraction, so that (C : ε)ij =\nklCijklεkl.\nThese equations form a closed system when they are complemented by\n∂ε\n∂t = 1(∇v + (∇v)T),\nwhich holds by definition of ε.\nAt this point we can check that the first-order system for v and ε defined\nby the equations above is hyperbolic. Define\n\nv\nL\nw =\n,\nL =\n,\nε\nL1\nwith\nL1v = 2(∇v + (∇v)T),\nL2ε = 1\nρ0\n∇· (C : ε).\nThen, as previously, ∂w = Lw. An exercise in section 1.3 asks to show that\n∂t\nthe matrix operator L is anti-selfadjoint with respect to the inner product\n⟨w, w′⟩= 2\nˆ\n(ρv · v′ + ε : C : ε) dx.\nThe corresponding conserved elastic energy is E = ⟨w, w⟩.\nIsotropic elasticity is obtained where C takes a special form with 2 degrees\nof freedom rather than 81, namely\nCijkl= λδijδkl + μ(δilδjk + δikδjl).\nWe are not delving into the justification of this equation. The two elastic\nparameters λ and μ are also called Lam e parameters:\n\n1.1. PHYSICAL MODELS\n- λ corresponds to longitudinal waves, also known as compressional, pres-\nsure waves (P).\n- μ corresponds to transverse waves, also known as shear waves (S).\nOriginally, the denominations P and S come from \"primary\" and \"secondary\",\nas P waves tend to propagate faster, hence arrive earlier, than S waves.\nWith this parametrization of C, it is easy to check that the elastic system\nreduces to the single equation (1.5). In index notation, it reads\n∂2ui\nρ\n= ∂i(λ∂juj) + ∂j(μ(∂iuj + ∂jui)).\n∂t2\nFor reference, the hyperbolic propagator L2 reduces to\nL2ε =\nX\n(∇(λ tr ε) + 2 ∇· (με)),\ntr ε =\nεii,\nρ\ni\nand the energy inner product is\n⟨w, w′\n⟩=\n(ρv · v′ + 2 μtr(εTε′) + λ(tr ε)(tr ε′)) dx.\nˆ\nThe elastic wave equation looks like an acoustic wave equation with \"2\nterms, hence 2 waves\". To make this observation more precise, assume that λ\nand μ are constant. Use some vector identities2 to reduce (1.5) to\n(!)\n∂2u\nρ\n= (λ + μ)∇(∇· u) + μ∆u,\n∂t2\n= (λ + 2μ)∇(∇· u) -μ∇× ∇× u.\nPerform the Helmholtz (a.k.a. Hodge) decomposition of u in terms of poten-\ntials φ and ψ:\nu = ∇φ + ∇× ψ,\nwhere φ is a scalar field andψis a vector field3. These two potentials are\ndetermined up to a gauge choice, namely\nφ′ = φ + C,\nψ′ = ψ + ∇f.\n2In this section, we make use of ∇× ∇× u = ∇(∇· u) -∆u, ∇· ∇× ψ = 0, and\n∇× ∇ψ = 0.\n3Normally the Helmholtz decomposition comes with a third term h which obeys ∆h =\n0, i.e., h is harmonic, but under suitable assumptions of decay at infinity the only solution\nto ∆h = 0 is h = 0.\n\nCHAPTER 1. WAVE EQUATIONS\nChoose f such that ψ′ has zero divergence:\n∇· ψ′ = 0\n⇒\n∆f = -∇· ψ.\nThis is a well-posed Poisson equation for f. With this choice of ψ′, it holds\nthat\n∇· u = ∆φ,\n∇× u = ∇× ∇× u = -∆ψ.\nThe elastic wave equation can then be rewritten in terms of φ, ψ as\n∂2φ\n∇ρ ∂t2 -(λ + 2μ)∆φ\n\n+ ∇×\n\nρ∂2ψ\n\n-μ∆ψ\n= 0.\n∂t2\nTake the gradient of this equation to conclude that (with a suitable decay\ncondition at infinity)\n∂2φ\nρ\n-(λ + 2μ)∆φ = harmonic = 0.\n∂t2\nNow that the first term is zero, we get\n∂2ψ\nρ\n-μ∆ψ = ∇(something).\n∂t2\nTake the divergence of this equation to conclude that (with the same decay\ncondition at infinity as earlier), \"something\" is harmonic, hence zero. There-\nfore, each potential φ and ψ solve their own scalar wave equation: one for\nthe longitudinal waves (φ) and one for the transverse waves (ψ). They obey\na superposition principle. The two corresponding wave speeds are\ns\ncP =\nλ + 2μ\nρ0\n,\ncS =\nr μ .\nρ0\nIn the limit μ →0, we see that only the longitudinal wave remains, and\nλ reduces to the bulk modulus. In all cases, since λ ≥0 we always have\n√\ncP ≥\n2cS: the P waves are indeed always faster (by a factor at least\n√\n2)\nthan the S waves.\nThe assumption that λ and μ are constant is a very strong one: there is\na lot of physics in the coupling of φ and ψ that the reasoning above does not\ncapture. Most important is mode conversion as a result of wave reflection at\ndiscontinuity interfaces of λ(x) and/or μ(x).\n\n1.1. PHYSICAL MODELS\n1.1.3\nElectromagnetic waves\nThe quantities of interest for electromagnetic waves are:\n- Physical fields: the electric field E, and the magnetic field H,\n- Medium parameters: the electric permittivity ε and the magnetic per-\nmeability μ,\n- Forcings: electric currents j and electric charges ρ.\nThe electric displacement field D and the magnetic induction field B are\nalso considered. In the linearized regime ($), they are assumed to be linked\nto the usual fields E and H by the constitutive relations\nD = εE,\nB = μH.\nMaxwell's equations in a medium with possible space-varying parameters\nε and μ read\n∂B\n∇× E = -∂t\n(Faraday's law)\n(1.6)\n∇× H = ∂D + j\n(Amp`ere's law with Maxwell's correction)\n(1.7)\n∂t\n∇· D = ρ\n(Gauss's law for the electric field)\n(1.8)\n∇· B = 0\n(Gauss's law for the magnetic field)\n(1.9)\nThe integral forms of these equations are obtained by a volume integral,\nfollowed by a reduction to surface equations by Stokes's theorem for (1.6),\n(1.7) and the divergence (Gauss's) theorem for (1.8), (1.9).\nThe integral\nequations are valid when ε and μ are discontinuous, whereas the differential\nequations strictly speaking are not.\nThe total charge in a volume V is\n\nρdV , while the total current through\nV\na surface S is\n\nj·dS. Conservation of charge follows by taking the divergence\nS\nof (1.7) and using (1.8):\n∂ρ + ∇· j = 0.\n∂t\nIn vacuum, or dry air, the parameters are constant and denoted ε = ε0,\nμ = μ0. They have specific numerical values in adequate units.\nWe now take the viewpoint that (1.6) and (1.7) are evolution equations for\nE and H (or D and B) that fully determine the fields when they are solved\n\nCHAPTER 1. WAVE EQUATIONS\nforward (or backward) in time. In that setting, the other two equations (1.8)\nand (1.9) are simply constraints on the initial (or final) condition at t = 0. As\npreviously, we may write Maxwell's equations in the more concise hyperbolic\nform\n∂w\n\n-j/ε\nE\n= Lw +\n,\nwith w =\n,\n∂t\nH\nprovided\n\nL =\nε∇×\n-1\n\n.\n∇×\nμ\nThe \"physical\" inner product that makes L∗= -L is\n⟨w, w′\n⟩=\ndx.\nˆ\n(εEE′ + μHH′)\nThe electromagnetic energy E = ⟨w, w⟩is conserved when j = 0.\nIt is the balanced coupling of E and H through (1.6) and (1.7) that creates\nwave-like solutions to Maxwell's equations (and prompts calling the physical\nphenomenon electromagnetism rather than just electricity and magnetism.)\nCombining both equations, we obtain\n∂2E\n∂t2 = -1\nε∇× (1∇× E),\nμ\n∂2H\n∂t2 = -1\nμ∇× (1∇× H).\nε\nThese wave equations may be stand-alone but E and H are still subject to\nessential couplings.\nA bit of algebra4 reveals the more familiar form\n∂2E\n∆E -εμ ∂t2 + ∇μ\nμ × (∇× E) + ∇(E · ∇ε) = 0.\nε\nWe now see that in a uniform medium, ε and μ are constant and the last two\nterms drop, revealing a wave equation with speed\nc = √εμ.\n4Using the relations ∇× ∇× F = ∇(∇· F) -∆F again, as well as ∇· (F × G) =\nG · (∇× F) -F · (∇× G).\n\n1.2. SPECIAL SOLUTIONS\n√\nThe speed of light is c0 = 1/\nε0μ0. Even when ε and μ vary in x, the last two\nterms are kinematically much less important than the first two because they\ninvolve lower-order derivatives of E. They would not, for instance, change\nthe path of the \"light rays\", a concept that we'll make clear later.\nFor reference, we now list the jump conditions that the electric and mag-\nnetic fields obey at a dielectric interface. These relations can be obtained\nfrom the integral form of Maxwell's equations, posed over a thin volume\nstraddling the interface. Let n be the vector normal to a dielectric interface.\nn × E1 = n × E2\n(continuous tangential components)\nn × H1 = n × H2 + jS\nn · D1 = n · D2 + ρS\nn · H1 = n · H2\n(continuous normal component)\nWe have used jS and ρS for surface currents and surface charges respectively.\nIf the two dielectrics correspond to finite parameters ε1, ε2 and μ1, μ2, then\nthese currents are zero. If material 2 is a perfect electric conductor however,\nthen these currents are not zero, but the fields E2, H2, D2 and H2 are zero.\nThis results in the conditions n × E = 0 (E perpendicular to the interface)\nand n × H = 0 (H parallel to the interface) in the vicinity of a perfect\nconductor.\nMaterials conducting current are best described by a complex electric\npermittivity ε = ε′ + iσ/ω, where σ is called the conductivity. All these\nquantities could be frequency-dependent. It is the ratio σ/ε′ that tends to\ninfinity when the conductor is \"perfect\". Materials for which ε is real are\ncalled \"perfect dielectrics\": no conduction occurs and the material behaves\nlike a capacitor. We will only consider perfect dielectrics in this class. When\nconduction is present, loss is also present, and electromagnetic waves tend\nto be inhibited. Notice that the imaginary part of the permittivity is σ/ω,\nand not just σ, because we want Amp`ere's law to reduce to j = σE (the\ndifferential version of Ohm's law) in the time-harmonic case and when B = 0.\n1.2\nSpecial solutions\n1.2.1\nPlane waves, dispersion relations\nIn this section we study special solutions of wave equations that depend on x\nlike eikx. These solutions are obtained if we assume that the time dependence\n\nCHAPTER 1. WAVE EQUATIONS\nis harmonic, namely if the unknown is w(x, t), then we assume ($)\nw(x, t) = e-iωtfω(x),\nω ∈R.\nThe number ω is called angular frequency, or simply frequency. Choosing\ne+iωt instead makes no difference down the road. Under the time-harmonic\nassumption, the evolution problem ∂w = Lw becomes an eigenvalue problem:\n∂t\n-iωfω = Lfω.\nNot all solutions are time-harmonic, but all solutions are superpositions of\nharmonic waves at different frequencies ω. Indeed, if w(x, t) is a solution,\nconsider it as the inverse Fourier transform of some wb(x, ω):\nw(x, t) =\nx,\nˆ\ne-iωtwb(\nω)dω.\nπ\nThen each wb(x, ω) is what we called fω(x) above. Hence there is no loss of\ngenerality in considering time-harmonic solutions.\nConsider the following examples.\n- The one-way, one-dimensional wave equation\n∂u\n∂t + c∂u = 0,\nx ∈R.\n∂x\nTime harmonic solutions u(x, t) = e-iωtfω(x) obey\nω\ni\nfω = f ′\nc\nω,\nx ∈R.\nThe solution to this equation is\nfω(x) = eikx\nω\n,\nk =\n∈R.\nc\nEvanescent waves corresponding to decaying exponentials in x and t\nare also solutions over a half-line, say, but they are ruled out by our\n(!)\nassumption that ω ∈R.\nWhile ω is the angular frequency (equal to 2π/T where T is the period),\nk is called the wave number (equal to 2π/λ where λ is the wavelength.)\nIt is like a \"spatial frequency\", though it is prudent to reserve the word\n\n1.2. SPECIAL SOLUTIONS\nfrequency for the variable dual to time.\nThe quantity measured in\nHertz [1/s] and also called frequency is ν = ω/(2π).\nThe full solution then takes the form\nu(x, t) = ei(kx-ωt) = eik(x-ct),\nmanifestly a right-going wave at speed c. If the equation had been\n∂u\n∂t -c ∂u = 0 instead, the wave would have been left-going: u(x, t) =\n∂x\neik(x+ct).\n- The n-dimensional wave equation in a uniform medium,\n∂2u = c2∆u,\nx ∈\nt2\nRn.\n∂\nWhen u(x, t) = e-iωtfω(x), the eigenvalue problem is called the (homo-\ngeneous) Helmholtz equation. It is\n-ω2fω(x) = ∆fω(x),\nx ∈Rn.\n(1.10)\nAgain, plane waves are solutions to this equation:\nfω(x) = eik·x,\nprovided ω2 = |k|2c2, i.e., ω = ±|k|c.\nHence fω is a function that\noscillates in the direction parallel to k. The full solution is\nu(x, t) = ei(k·x-ωt),\nwhich are plane waves traveling with speed c, along the direction k.\nWe call k the wave vector and |k| the wave number. The wavelength\nis still 2π/|k|. The relation ω2 = |k|2c2 linking ω and k, and encoding\nthe fact that the waves travel with velocity c, is called the dispersion\nrelation of the wave equation.\nNote that eik·x are not the only (non-growing) solutions of the Helmholtz\nequation in free space; so is any linear combination of eik·x that share\nthe same wave number |k|. This superposition can be a discrete sum\nor a continuous integral. An exercise in section 1.3 deals with the con-\ntinuous superposition with constant weight of all the plane waves with\nsame wave number |k|.\n\nCHAPTER 1. WAVE EQUATIONS\nConsider now the general case of a hyperbolic system ∂w = Lw, with\n∂t\nL∗= -L. The eigenvalue problem is -iωfω = Lfω. It is fine to assume ω\nreal: since L is antiselfadjoint, iL is selfadjoint (Hermitian), hence all the\neigenvalues of L are purely imaginary. This is sometimes how hyperbolic\nsystems are defined -- by assuming that the eigenvalues of the generator L\nare purely imaginary.\nWe still look for eigenfunctions with a eik·x dependence, but since w and\nfω may now be vectors with m components, we should make sure to consider\nf (x) = eik·x\nω\nr,\nr ∈Rm.\nHowever, such fω cannot in general expected to be eigenvectors of L.\nIt\nis only when the equation is translation-invariant that they will be. This\nmeans that the generator L is a matrix of differential operators with constant\ncoefficients - no variability as a function of x is allowed. In this translation-\ninvariant setting, and only in this setting, L is written as a multiplication\nby some matrix P(k) in the Fourier domain. Say that f has m components\n(f1, . . . fm); then\nLf(x) =\n)\n(2\nˆ\neik·xP(k fb(k)dk,\nπ)n\nwhere P (k) is an m-by-m matrix for each k. Here P (k) is called the dispersion\nmatrix. We refer to operators such as L as diagonal in the Fourier domain,\nwith respect to the k variable, because they act like a \"diagonal matrix\" on\nvectors of the continuous index k -- although for each k the small matrix\nP (k) is not in general diagonal5. In pure math, P (k) is called the multiplier,\nand L is said to be a multiplication operator in the Fourier domain.\nFor illustration, let us specialize our equations to the 2D acoustic system\nwith ρ0 = κ0 = c = 1, where\n\n∂\n\n-\nv\n\nw =\n,\nL =\np\n\n∂x1\n-∂\n∂x2\n-∂\n∂x1\n-∂\n∂x2\n\n.\n5Non-diagonal, translation-variant operators would require yet another integral over a\nk′ variable, and would read Lf(x) =\neik·x\nn\nQ(k, k′)fb(k′)dk′, for some more com-\n(2π)\nplicated object Q(k, k′) ∈Rm×m. The name\n\n\"diagonal\"\n\ncomes from the fact that Q(k, k′)\nsimplifies as P(k)δ(k-k′) in the translation-invariant case. You can think of P(k)δ(k-k′)\nas the continuous analogue of diδij: it is a \"diagonal continuous matrix\" as a function of\nk (continuous row index) and k′ (continuous column index).\n\n1.2. SPECIAL SOLUTIONS\nIt can be readily checked that\n\n-ik1\n\nP(k) = 0\n-ik2\n\n,\n-ik1\n-ik2\nfrom which it is apparent that P(k) is a skew-Hermitian matrix: P ∗(k) =\n-P(k).\nWe can now study the conditions under which -iωfω = Lfω: we compute\n(recall that r is a fixed vector)\nL(eik·xr) = (2π)n\nˆ\neik′·xP(k′) \\\n[eik·xr](k′)dk′,\n=\nˆ\neik′·xP(k′)(2π)nδ(k -k′)rdk′,\n= eik·xP(k)r.\n(2π)n\nIn order for this quantity to equal -iωeik·xr for all x, we require (at x = 0)\nP(k) r = -iω r.\nThis is just the condition that -iω is an eigenvalue of P(k), with eigenvector\nr. We should expect both ω and r to depend on k. For instance, in the 2D\nacoustic case, the eigen-decomposition of P(k) is\n\nk2\nλ0(k) = -iω0(k) = 0,\nr0(k) = -k1\nand\n\n±k1/|k|\nλ±(k) = -iω±(k) = -i|k|,\nr±(k) = ±k2/|k|.\n|k|\nOnly the last two eigenvalues correspond to physical waves: they lead to the\nusual dispersion relations ω(k) = ±|k| in the case c = 1. Recall that the\nfirst two components of r are particle velocity components: the form of the\neigenvector indicates that those components are aligned with the direction k\nof the wave, i.e., acoustic waves can only be longitudinal.\nThe general definition of dispersion relation follows this line of reason-\ning: there exists one dispersion relation for each eigenvalue λj of P(k), and\n-iωj(k) = λj(k); for short\ndet [iωI + P(k)] = 0.\n\nCHAPTER 1. WAVE EQUATIONS\n1.2.2\nTraveling waves, characteristic equations\nWe now consider a few examples that build up to the notion of characteristic\ncurve/surface.\n- Let us give a complete solution to the one-way wave equation of one\nspace variable in a uniform medium:\n∂u\n∂t + c∂u = 0,\nu(x, 0) = u0(x).\n(1.11)\n∂x\nThe study of plane wave solutions in the previous section suggests that\nthe variable x -ct may play a role.\nLet us perform the change of\nvariables\nξ = x -ct,\nη = x + ct.\nIt inverts as\nξ + η\nx =\n,\nt = η -ξ.\n2c\nBy the chain rule, e.g.,\n∂\n∂ξ = ∂x\n∂ξ\n∂\n∂x + ∂t\n∂ξ\n∂,\n∂t\nwe get\n∂\n-2c∂ξ = ∂\n∂t -c ∂\n∂x,\n2c ∂\n∂η = ∂\n∂t + c ∂.\n∂x\nWith U(ξ, η) = u(x, t), the wave equation simply becomes\n∂U = 0,\n∂η\nwhose general solution is U(ξ, η) = F(ξ) for some differentiable function\nF. Hence u(x, t) = F(x -ct). In view of the initial condition, this is\nu(x, t) = u0(x -ct).\nThe solutions to (1.11) are all the right-going waves with speed c, and\nnothing else.\nThe wave propagate along the lines ξ(x, t) = x-ct =const. in the (x, t)\nplane. For this reason, we call ξ the characteristic coordinate, and we\ncall the lines ξ(x, t) = const. characteristic curves.\n\n1.2. SPECIAL SOLUTIONS\nNotice that imposing a boundary condition u(0, t) = v0(t) rather than\nan initial condition is also fine, and would result in a solution u(x, t) =\nv0(t -x/c). Other choices are possible; they are called Cauchy data.\nHowever, a problem occurs if we try to specify Cauchy data along a\ncharacteristic curve ξ = constant, as v0(η):\n1. this choice is not in general compatible with the property that the\nsolution should be constant along the characteristic curves; and\nfurthermore\n2. it fails to determine the solution away from the characteristic\ncurve.\nIn other words, there is a problem with both existence and unique-\nness when we try to prescribe Cauchy data on a characteristic curve.\nThis fact will be used in the sequel to define these curves when their\ngeometric intuition becomes less clear.\n- Using similar ideas, let us describe the full solution of the (two-way)\nwave equation in one space dimension,\n∂2u\n∂t2 -c2∂2u\n∂x2 = 0,\nu(x, 0) = u0(x),\n∂u(x, 0) = u1(x).\n∂t\nThe same change of variables leads to the equation\n∂U\n= 0,\n∂ξ∂η\nwhich is solved via\n∂U\nξ\n(ξ, η) = f(ξ),\nU(ξ, η) =\nˆ\nf(ξ′)dξ′ + G(η) = F(ξ) + G(η).\n∂ξ\nThe resulting general solution is a superposition of a left-going wave\nand a right-going wave:\nu(x, t) = F(x -ct) + G(x + ct).\nMatching the initial conditions yields d'Alembert's formula (1746):\nu(x, t) = 2(u0(x -ct) + u0(x + ct)) + 1\nx\n2c\nˆ\n+ct\nu1(y)dy.\nx-ct\n\nCHAPTER 1. WAVE EQUATIONS\nIt is the complete solution to the 1D wave equation in a uniform wave\nspeed c. Notice that we now have two families of criss-crossing charac-\neristic curves, given by ξ(x, t) = const. and η(x, t) = const. Cauchy\ndata cannot be prescribed on either type of characteristic.\n- Consider now the wave equation in a variable medium c(x) (technically,\nacoustic waves on an infinite string with variable bulk modulus):\n∂2u\n∂t2 -c2(x)∂2u\n∂x2 = 0,\nu(x, 0) = u0(x),\n∂u(x, 0) = u1(x).\n∂t\nWe will no longer be able to give an explicit solution to this problem,\nbut the notion of characteristic curve remains very relevant. Consider\nan as-yet-undetermined change of coordinates (x, t) 7→(ξ, η), which\ngenerically changes the wave equation into\n∂2U\nα(x) ∂ξ2 + ∂2U\n∂ξ∂η + β(x)∂2U\n∂η2 +\n\np(x)∂U\n∂ξ + q(x)∂U\n\n+ r(x)U\n= 0,\n∂η\nwith\n∂ξ\nα(x) =\n∂t\n-c2(x)\n∂ξ\n∂x\n,\nβ(x) =\n∂η\n∂t\n-c2(x)\n∂η 2\n.\n∂x\nThe lower-order terms in the square brackets are kinematically less\nimportant than the first three terms6. We wish to define characteristic\ncoordinates as those along which\nU(ξ, η) ≃F(ξ) + G(η),\ni.e., \"directions in which the waves travel\" in space-time. It is in general\nimpossible to turn this approximate equality into an actual equality\n(because of the terms in the square brackets), but it is certainly possible\nto choose the characteristic coordinates so that the ∂2U\n∂ξ2 and ∂2U v\n∂η2\nanish.\nChoosing α(x) = β(x) = 0 yields the same equation for both ξ and η,\nhere expressed in terms of ξ:\n∂ξ\n∂t\n-c2(x)\n∂ξ\n∂x\n= 0.\n(1.12)\n6In a sense that we are not yet ready to make precise. Qualitatively, they affect the\nshape of the wave, but not the character that the waves travel with local speed c(x).\n\n1.2. SPECIAL SOLUTIONS\nThis relation is called the characteristic equation. Notice that ξ = x-ct\nand η = x + ct are both solutions to this equation in the case when\nc(x) = c is a constant. But it can be checked that ξ = x ± c(x)t is\notherwise not a solution of (1.12). Instead, refer to the exercise section\nfor a class of solutions to (1.12).\n- Consider now the n dimensional wave equation\n∂2u\n∂t2 -c2(x)∆u = 0,\nu(x, 0) = u0(x),\n∂u(x, 0) = u1(x).\n∂t\nA change of variables would now read (x1, . . . , xn, t) 7→(ξ, η1, . . . , ηn).\nThe variable ξ is called characteristic when the coefficient of the lead-\ning term\n∂2U v\n∂ξ2\nanishes in the expression of the equation in the new\ncoordinates. This condition leads to the n-dimensional version of the\ncharacteristic equation\n∂ξ2\n-c2(x)|∇\nxξ| = 0.\n(1.13)\n∂t\nThe same relations should hold for the other coordinates η1, . . . , ηn\nif they are to be characteristic as well.\nEquation (1.13) is called a\nHamilton-Jacobi equation.\nWe now speak of characteristic surfaces\nξ(x, t) = const., rather than curves.\nThe set of solutions to (1.13) is very large. In the case of constant c,\nwe can check that possible solutions are\nξ(x, t) = x · k ± ωt,\nω = |k|c,\ncorresponding to more general plane waves u(x, t) = F(x · k ± ωt)\n(which the reader can check are indeed solutions of the n-dimensional\nwave equation for smooth F), and\nξ(x, t) = ∥x -y∥± ct,\nfor some fixed y, and x = y,\ncorresponding to concentric spherical waves originating from y. We\ndescribe spherical waves in more details in the next section. Notice\nthat both formulas for ξ reduce in some sense to x ± ct in the one-\ndimensional case.\n\nCHAPTER 1. WAVE EQUATIONS\nThe choice of characteristic coordinates led to the reduced equation\n∂2U + lower order terms = 0,\n∂ξ∂η\nsometimes called \"first fundamental form\" of the wave equation, on the in-\ntuitive basis that solutions (approximately) of the form F(ξ) + G(η) should\ntravel along the curves ξ = const. and η = const. Let us now motivate this\nchoice of the reduced equation in more precise terms, by linking it to the\nidea that Cauchy data cannot be prescribed on a characteristic curve.\nConsider utt = c2uxx. Prescribing initial conditions u(x, 0) = u0, ut(x, 0) =\nu1 is perfectly acceptable, as this completely and uniquely determines all the\npartial derivatives of u at t = 0. Indeed, u is specified through u0, and all its\nx-partials ux, uxx, uxxx, . . . are obtained from the x-partials of u0. The first\ntime derivative ut at t = 0 is obtained from u1, and so are utx, utxx, . . . by\nfurther x-differentiation. As for the second derivative utt at t = 0, we obtain\nit from the wave equation as c2uxx = c2(u0)xx. Again, this also determines\nuttx, uttxx, . . . The third derivative uttt is simply c2utxx = c2(u1)xx. For the\nfourth derivative u\ntttt, apply the wave equation twice and get it as c (u0)xxxx.\nAnd so on. Once the partial derivatives are known, so is u itself in a neigh-\nborhood of t = 0 by a Taylor expansion -- this is the original argument\nbehind the Cauchy-Kowalevsky theorem.\nThe same argument fails in characteristic coordinates. Indeed, assume\nthat the equation is uξη + puξ + quη + ru = 0, and that the Cauchy data\nis u(ξ, 0) = v0(ξ), uη(ξ, 0) = v1(η).\nAre the partial derivatives of u all\ndetermined in a unique manner at η = 0? We get u from v0, as well as\nuξ, uξξ, uξξξ, . . . by further ξ differentiation. We get uη from v1, as well as\nuηξ, uηξξ, . . . by further ξ differentiation. To make progress, we now need to\nconsider the equation uξη + (l.o.t.) = 0, but two problems arise:\n- First, all the derivatives appearing in the equation have already been\ndetermined in terms of v0 and v1, and there is no reason to believe that\nthis choice is compatible with the equation. In general, it isn't. There\nis a problem of existence.\n- Second, there is no way to determine uηη from the equation, as this term\ndoes not appear. Hence additional data would be needed to determine\nthis partial derivative. There is a problem of uniqueness.\n\n1.2. SPECIAL SOLUTIONS\nThe only way to redeem this existence-uniqueness argument is by making\nsure that the equation contains a uηη term, i.e., by making sure that η is\nnon-characteristic.\nPlease refer to the exercise section for a link between characteristic equa-\ntions, and the notions of traveltime and (light, sound) ray. We will return to\nsuch topics in the scope of geometrical optics, in chapter 7.\n1.2.3\nSpherical waves, Green's functions\nConsider x ∈R3 and c constant. We will only be dealing with solutions in\n3 spatial dimensions for now. We seek radially symmetric solutions of the\nwave equation. In spherical coordinate (r, θ, φ), the Laplacian reads\n∆u = r\n∂2\n(ru) + angular terms.\n∂r2\nFor radially symmetric solutions of the wave equation, therefore,\nc2\n∂2\n∂t2(ru) = ∂2\n(ru).\n∂r2\nThis is a one-dimensional wave equation in the r variable, whose solution we\nderived earlier:\nF(r -ct)\nru(r, t) = F(r -ct) + G(r + ct)\n⇒\nu(r, t) =\nr\n+ G(r + ct).\nr\nSpherical waves corresponding to the F term are called outgoing, while waves\ncorresponding to the G term are called incoming. More generally, spherical\nwaves can be outgoing/incoming with respect to any point y ∈R3, for in-\nstance\nF(∥x -y∥-ct)\nu(x, t) =\n.\n∥x -y∥\nNotice that we had already seen that ∥x -y∥± ct is a characteristic variable\nfor the wave equation, in the previous section. The surfaces ∥x -y∥= ct+\nconst. are often called light cones in the setting of electromagnetic waves.\nIn what follows we will be interested in the special case F(r) = δ(r), the\nDirac delta, for which the wave equation is only satisfied in a distributional\nsense. Superpositions of such spherical waves are still solutions of the wave\nequation.\n\nCHAPTER 1. WAVE EQUATIONS\nIt turns out that any solution of the wave equation in R3, with constant\nc, can be written as a superposition of such spherical waves. Let us consider\na quantity which is not quite the most general yet:\nu(x, t) =\nˆ\nδ(∥x -y∥-ct)\nR3\nψ(y)dy.\n(1.14)\n∥x -y∥\nSince ∥x- y∥ = ct on the support of the delta function, the denominator can\nbe written ct. Denoting by Bx(ct) the ball centered at x and with radius ct,\nwe can rewrite7\nu(x, t) = ct\nˆ\nψ(y)dy.\n∂Bx(ct)\nThe interesting question is that of matching u(x, t) given by such a formula,\nwith the initial conditions. By the mean value theorem,\nu(x, t) ∼4πctψ(x),\nt →0,\nwhich tends to zero as t →0. On the other hand, an application of the\nReynolds transport theorem (or a non-rigorous yet correct derivative in time\nof the equation above) yields\n∂u\nlim\nt→0\n(x, t) = 4πcψ(x).\n∂t\nWe are therefore in presence of initial conditions u0 = 0, and arbitrary u1 =\n4πcψ(x) arbitrary. In that case, the solution of the constant-c wave equation\nin R3 is\nu(x, t) =\nˆ\nG(x, y; t)u1(y) dy,\nwith the so-called Green's function\nδ(∥x -y∥-ct)\nG(x, y; t) =\n,\nt > 0,\n(1.15)\n4πc2t\nand zero when t ≤0.\n7Note that the argument of δ has derivative 1 in the radial variable, hence no Jacobian\nis needed. We could alternatively have written an integral over the unit sphere, such as\nu(x, t) = ct\nˆ\nψ(x + ctξ)dSξ.\nS2\n\n1.2. SPECIAL SOLUTIONS\nLet us now describe the general solution for the other situation when\nu1 = 0, but u0 = 0. The trick is to define v(x, t) by the same formula (1.14),\nand consider u(x, t) = ∂v, which also solves the wave equation:\n∂t\n∂2\n∂t2 -c2∆\n∂v\n∂t = ∂\n∂t\n∂2\n\n-c2∆v = 0.\n∂t2\nThe limits are now\nlim u(x, t) = 4πcψ(x),\nt→0\nand\n∂u\n∂t = ∂2v = c2∆v,\nlim c2∆v(x, t) = c2∆lim v(x, t) = 0\n∂t2\nt→0\nt→0\n(limit and derivative are interchangeable when the function is smooth enough.)\nThe time derivative trick is all that is needed to generate the solution in the\ncase u1 = 0:\nG\nu(x, ) =\nˆ ∂\nt\n(x, y; t)u0(y) dy.\n∂t\nThe general solution, called Kirchhoff's formula8, is obtained by super-\nposition of these two special cases:\nG\nu(\nt) =\nˆ\n∂\nx,\n\n(x, y; t)u0(y) + G(x, y; t)u1(y)\ndy.\n(1.16)\n∂t\nThe concept of Green' function G is much more general than suggested\nby the derivation above. Equation (1.16), for instance, holds in arbitrary\ndimension and for heterogeneous media, albeit with a different Green's func-\ntion -- a claim that we do not prove here. In two dimensions and constant\nc for instance, it can be shown9 that\nG(x, y; t) =\n2πc\np\n,\nwhen t > 0,\nc2t2 -∥x -y∥2\nand zero otherwise. In heterogeneous media, explicit formulas are usually\nnot available.\n8Though credited to Poisson.\n9By the so called \"method of descent\". See the book Introduction to PDE by Gerald\nFolland for a wonderful explanation of wave equations in constant media.\n\nCHAPTER 1. WAVE EQUATIONS\nIn the wider context of linear PDE, Green's functions are more often\nintroduced as linking a right-hand-side forcing f to the solution u upon inte-\ngration. For a linear PDE Lu = f, Green's functions are to the differential\noperator L what the inverse matrix A-1 is to a matrix A. Accordingly, the\nGreen's function describes the solution of the wave equation with a right-hand\nside forcing -- a setting more often encountered in imaging than initial-value\nproblems. The premise of the proposition below is that G is defined10 through\n(1.16), even as x ∈ Rn and c is a function of x.\nProposition 2. (Duhamel principle) For x ∈Rn, and t > 0, the solution of\nthe inhomogeneous problem\n∂2\n∂t2 -c2(x)∆\n\nu(x, t) = f(x, t),\nu(x, 0) = ∂u(x, 0) = 0.\n∂t\nis\nˆ t\nu(x, t) =\nˆ\nG(x, y; t -s)f(y, s) dyds.\n(1.17)\nProof. Let us check that the wave equation holds.\nFor each s > 0, consider the auxiliary problem\n∂2\n∂t2 -c2(x)∆\n\nvs(x, t) = f(x, t),\nvs(x, 0) = 0,\n∂vs(x, 0) = f(x, s).\n∂t\nThen\nvs(x, t) =\nˆ\nG(x, y; t)f(y, s) dy.\nThe candidate formula for u is\nu(x, t) =\nˆ t\nvs(x, t -s) ds.\nLet us now check that this u solves the wave equation. For one, u(x, 0) = 0\nbecause the integral is over an interval of length zero. We compute\n∂u\nt\n(x, t) = vs(x, t -s)|s=t +\n∂t\nˆ\n∂vs\n∂t (x, t -s) ds =\nˆ t\n∂vs\n∂t (x, t -s) ds.\n10The tables could be turned, and G could instead be defined by (1.17). In that case\n(1.16) would be a corollary.\n\n1.2. SPECIAL SOLUTIONS\nFor the same reason as previously, ∂u\n∂t (x, 0) = 0. Next,\n∂2u\n∂t2 (x, t) = ∂vs\nt\n(x, t -s)|s=t +\n∂t\nˆ\n∂2vs\n(x, t -s) ds\n∂t2\n= f(x, t) +\nˆ t\nc2(x)∆vs(x, t -s) ds\n= f(x, t) + c2(x)∆\nˆ t\nvs(x, t -s) ds\n= f(x, t) + c2(x)∆u(x, t).\nSince the solution of the wave equation is unique, the formula is general.\nBecause the Green's function plays such a special role in the description\nof the solutions of the wave equation, it also goes by fundamental solution.\nWe may specialize (1.17) to the case f(x, t) = δ(x -y)δ(t) to obtain the\nequation that the Green's function itself satsfies,\n∂2\n\n-c2(x)∆x G(x, y; t) = δ(x -y)δ(t).\n∂t2\nIn the spatial-translation-invariant case, G is a function of x -y, and we\nmay write G(x, y; t) = g(x -y, t). In that case, the general solution of the\nwave equation with a right-hand side f(x, t) is the space-time convolution of\nf with g.\nA spatial dependence in the right-hand-side such as δ(x -y) may be a\nmathematical idealization, but the idea of a point disturbance is nevertheless\na very handy one. In radar imaging for instance, antennas are commonly\nassumed to be point-like, whether on arrays or mounted on a plane/satellite.\nIn exploration seismology, sources are often modeled as point disturbances\nas well (shots), both on land and for marine surveys.\nThe physical interpretation of the concentration of the Green's function\nalong the cone ∥x-y∥= ct is called the Huygens principle. Starting from an\ninitial condition at t = 0 supported along (say) a curve Γ, this principle says\nthat the solution of the wave equation is mostly supported on the envelope\nof the circles of radii ct centered at all the points on Γ.\n\nCHAPTER 1. WAVE EQUATIONS\n1.2.4\nThe Helmholtz equation\nIt is often convenient to use a formulation of the wave equation in the fre-\nquency domain. If\nub(x, ω) =\nˆ\neiωtu(x, t) dt,\nh\nand if\n∂2\ni\n-c2(x)∆x u = f, then it is immediate to check that the (inho-\n∂t2\nmogeneous) Helmholtz equation holds:\n\n-ω2 + c2(x)∆ub(x, ω) = fb(x, ω).\n(1.18)\nThe notion of Green's function is also very useful for the Helmholtz equation:\nit is the function Gb(x, y; ω) such that\nub(x, ω) =\nˆ\nGb(x, y; ω)fb(y, ω) dy.\nIt is a good exercise to check that Gb(x, y; ω) is indeed the Fourier transform\nof G(x, y; t) in t, by Fourier-transforming (1.17) and applying the convolu-\ntion theorem. By specializing the Helmholtz equation to the right-hand side\nfb(x, ω) = δ(x), we see that the Green's function itself obeys\n\n-ω2 + c2(x)∆Gb(x, y; ω) = δ(x -y).\n(1.19)\nIn particular, for x ∈R3 and constant c, we get (x = y)\nGb\n(\n(x, y; ω)\nˆ inf\neiωtδ ∥x -y∥-ct)\n=\n4πc2t\ndt\n=\nˆ inf\nei ω\nc t′ δ(∥x -y∥-t′)\n4πct′\ndt′\nc\n=\neik∥x-y∥\n,\nk = ω/c.\n4πc2∥x -y∥\nThe Helmholtz Green's function is an outgoing spherical wave generated by\na \"point source\" at x = y.\nIn the literature, the constant-medium Green's function is often encoun-\n∥\ntered in the\neik∥x-y\nform\n,\n4π∥x-y∥without the c2 in the denominator. This version\nof Gb originates instead from the equation -[ω2\nc2 + ∆] bG(x, y; ω) = δ(x -y),\nwhich differs from (1.19) by an overall c2 factor.\n\n1.2. SPECIAL SOLUTIONS\nNote that ω →-ω corresponds to time reversal: e-ik∥x-y∥\na\nalso\nis\nsolution\n4π∥x-y∥\nof the Helmholtz equation for x = y, but it is an incoming rather than\noutgoing wave. The sign in the exponent depends on the choice of convention\nfor the Fourier transform11\nSome mathematical care should be exercised when posing the Helmholtz\nequation in free space. Uniqueness, in particular, is not as easy to guarantee\nas for the time-dependent wave equation. \"Sufficient decay as ∥x∥→inf\" is\nnot a good criterion for uniqueness, since we've just seen an example of two\n∥\nwav\ne±ik∥x-y\nes\n∥-∥which have the same modulus and obey the same equation\n4π x\ny\n(1.19). Instead, it is customary to require the wave to be outgoing in order\nto have a well-posed problem in constant c. We say that ub(x, ω) obeys the\nSommerfeld radiation condition in R3 if (r = ∥x∥)\n∂\n∂r -ik\n\nbu(x, ω) = o( 1\n|x|),\ni.e., lim|x|→inf|x|\n∂\n\n-ik u(x, ω) = 0. It is a good exercise to check that\nb\n∂r\nb\nG(x, y; ω) obeys this radiation conditions, while Gb(x, y; -ω) does not.\nFor reference, the expression of the Green's function in two spatial di-\nmensions, in a uniform medium c(x) = c, is\nGb\ni\n(x, y, ω) =\n(1)\nH0 (k|x -y|),\nk = ω/c.\n(1)\nwhere H0\n= J0 + iY0 is the Hankel function of order zero.\n1.2.5\nReflected waves\nSpatial variability in the physical parameters (ρ, κ; ε, μ; λ, μ, etc.) entering\nthe wave equation generate wave scattering, i.e., changes of the direction of\npropagation of the waves. Of particular interest are discontinuities, or other\nnon-Cinfsingularities, which generate reflected waves alongside transmitted\nwaves.\n11We choose eiωt for the direct transform, and e-iωt for the inverse transform, in ac-\ncordance with practice in signal processing, radar imaging, and seismic imaging. Their\nrationale for this choice is that eikx, in conjunction with the time convention e-iωt, is\na right-going wave. For the spatial Fourier transforms, however, we adopt the opposite\nconvention e-ik·x for the direct transform, and eik·x for the inverse transform.\n\nCHAPTER 1. WAVE EQUATIONS\nLet us study reflection and transmission in the 1D, variable-density acous-\ntics equation\n∂2u\n∂t2 = κ(x) ∂\n∂x\nρ(x)\n∂u\n.\n∂x\nConsider a step discontinuity at x = 0, with ρ(x) = ρ1 and κ(x) = κ1 in\nx < 0, and ρ(x) = ρ2 and κ(x) = κ2 in x > 0. Assume an incident plane\nwave ui(x, t) = ei(k1x-ωt) in x < 0; we are interested in finding the reflection\ncoefficient R and the transmission coefficient T so the solution reads\nu (x, t) + u (x, t) = ei(k1x-ωt) + Rei(k1x+ωt)\ni\nr\n,\nx < 0.\nu (x, t) = Tei(k2x-ωt)\nt\n,\nx > 0.\nThe connection conditions are the continuity of u and 1\nρ\n∂u\n∂x. To justify this,\nremember that u is in fact a pressure disturbance in the acoustic case, while\nρ\n∂u is minus the time derivative of particle velocity, and these two quantities\n∂x\nare continuous on physical grounds. There is also a mathematical justification\nfor the continuity of 1\nρ\n∂u\n∂x: if it weren't, then\n∂\n∂x\n\nρ(x)\n∂u\nwould have a point\n∂x\nmass (Dirac atom) at x = 0, which would pose a problem both for the\nmultiplication by a discontinuous κ(x), and because ∂2u is\n∂t2\nsupposed to be a\nfinite function, not a distribution.\nAt x = 0, the connection conditions give\n1 + R = T,\nρ1\n(-ik1 -ik1R) = 1 (ik2T).\nρ2\nEliminate k1 and k2 by expressing them as a function of ρ1, ρ2 only; for\ninstance\nk1\nρ1\n=\nω\nρ1c1\n=\nω\n√\n,\nρ1κ1\nand similarly for k2. Note that ω is fixed throughout and does not depend\nρ2\non x. The quantity in the denominator is physically very important: it is\n√\nZ = ρc =\nκρ, the acoustic impedance. The R and T coefficients can then\nbe solved for as\nZ2 -Z1\nR = Z2 + Z1\n,\nT =\n2Z2\n.\nZ2 + Z1\n\n1.2. SPECIAL SOLUTIONS\nIt is the impedance jump Z2 -Z1 which mostly determines the magnitude\nof the reflected wave.R = 0 corresponds to an impedance match, even in the\ncase when the wave speeds differ in medium 1 and in medium 2.\nThe same analysis could have been carried out for a more general incoming\nwave f(x -c1t), would have given rise to the same R and T coefficients, and\nto the complete solution\nu(x, t) = f(x -c1t) + Rf(-x -c1t),\nx < 0,\n(1.20)\nc1\nu(x, t) = Tf(\n(x -c2t)),\nx > 0.\n(1.21)\nc2\nThe reader can check the relation\n1 = R2\nZ1\n+ Z2\nT 2,\nwhich corresponds to conservation of energy. An exercise in section 1.3 aims\nto establish this link.\nNote that R = R2 and T =\nZ1T 2 are sometimes\nZ2\nreferred to as reflection and transmission coefficients, though they measure\nintensities rather than amplitudes. The intensity coefficients are even de-\nnoted as R and T in place of R and T in some texts.\nPhysically, the acoustic impedance Z is the proportionality constant be-\ntween the pressure amplitude and the velocity amplitude of an acoustic wave.\nWe do not have direct access to Z in the acoustic equations however, as\np(x, t) = Zv(x, t) pointwise - only combinations of partial derivatives match.\nSo Z is in some sense an \"averaged quantity\" over at least a wavelength.\nOne can derive the expression of Z from the time-harmonic regime. The\nfirst equation (1.1) in the acoustic system reads, in the (k, ω) domain (in one\nspatial dimension),\niωvb(k, ω) = -ρ0\nikbp(k, ω),\nor, if we simplify further,\n|bp| = Z|bv|,\nZ = ρ0c = √ρ0κ0.\nThe same relation would have been obtained from (1.2). The larger Z, the\nmore difficult to move particle from a pressure disturbance, i.e., the smaller\nthe corresponding particle velocity.\n\nCHAPTER 1. WAVE EQUATIONS\nThe definition of acoustic impedance is intuitively in line with the tradi-\ntional notion of electrical impedance for electrical circuits. To describe the\nlatter, consider Amp`ere's law in the absence of a magnetic field:\n∂D\n∂t = -j\n⇒\nε∂E = -j.\n∂t\nIn the time-harmonic setting (AC current), iωεEb = -bj. Consider a conduct-\ning material, for which the permittivity reduces to the conductivity:\nσ\nε = iω\nIt results that Eb = Zbj with the resistivity Z = 1/σ. This is the differential\nversion of Ohm's law. The (differential) impedance is exactly the resistivity\nin the real case, and can accommodate capacitors and inductions in the\ncomplex case. Notice that the roles of E (or V ) and j (or I) in an electrical\ncircuit are quite analogous to p and v in the acoustic case.\nThere are no waves in the conductive regime we just described, so it is\nout of the question to seek to write R and T coefficients, but reflections\nand transmissions of waves do occur at the interface between two dielectric\nmaterials. Such is the case of light propagating in a medium with variable\nindex of refraction. To obtain the R and T coefficients in the optical case,\nthe procedure is as follows:\n- Consider Amp`ere's law again, but this time with a magnetic field H\n(because it is needed to describe waves) but no current (because we are\ndealing with dielectrics):\n∂D = ∇× H.\n∂t\nUse D = εE.\n- Assume plane waves with complex exponentials, or in the form E(k ·\nx -ωt) and H(k · x -ωt).\n- Use continuity of n × E and n × H at the interface (tangential compo-\nnents).\n- Assume no magnetism: μ = const.\n\n1.3. EXERCISES\nThe quantity of interest is not the impedance, but the index of refraction\nc\nn =\nref\nc\n= cref√εμ. Further assuming that the waves are normally incident\nto the interface, we have\nn2 -n1\nR = n2 + n1\n,\nT =\n2n2\n.\nn2 + n1\nThese relations become more complicated when the angle of incidence is not\nzero. In that case R and T also depend on the polarization of the light. The\ncorresponding equations for R and T are then called Fresnel's equations.\nTheir expression and derivation can be found in \"Principles of optics\" by\nBorn and Wolf.\n1.3\nExercises\n1. Derive from first principles the wave equation for waves on a taut string,\ni.e., a string subjected to pulling tension. [Hint: assume that the ten-\nsion is a vector field T(x) tangent to the string.\nAssume that the\nmass density and the scalar tension (the norm of the tension vector)\nare both constant along the string. Consider the string as vibrating in\ntwo, not three spatial dimensions. Write conservation of momentum on\nan infinitesimally small piece of string. You should obtain a nonlinear\nPDE: finish by linearizing it with respect to the spatial derivative of\nthe displacement.]\n2. Continue the reasoning in section 1.1.1 with the entropy to justify the\nequations of variable-density acoustics. [Hints: conservation of entropy\nreads ∂s +v ·∇s = 0. Continue assuming that the background velocity\n∂t\nfield is v0 = 0. Assume a fixed, variable background density ρ0(x).\nThe new constitutive relation is p = f(ρ, s). Consider defining c2(x) =\n∂f (ρ0(x), s0(x)).]\n∂ρ\n3. In a 2011 episode of the TV show Mythbusters, the team tested the\nmyth that you can better survive an underwater explosion by floating\non your back than by treading water. The myth was confirmed, with\ndata showing that the pressure resulting from a 10lb TNT explosion is\nmuch smaller at shallower depths.\n\nCHAPTER 1. WAVE EQUATIONS\nExplain this observation theoretically. [Hint: \"boundary condition\"]\n4. First, show the multivariable rule of integration by parts\n∇f · g =\n-\n\nf ∇· g, when f and g are smooth and decay fast at\n\ninfinity, by\ninvoking the divergence theorem. Second, use this result to show that\nL∗= -L for variable-density acoustics (section 1.1.1), i.e., show that\n⟨Lw, w′⟩= -⟨w, Lw′⟩for all reasonable functions w and w′, and where\n⟨·, ·⟩is the adequate notion of inner product seen in section 1.1.1.\n5. Show that ⟨Lw, w′⟩= -⟨w, Lw′⟩for general elastic waves.\n6. Predict that elastic waves can only propagate at speeds cP or cS, by\nstudying plane wave solutions of the form rei(k·x-ωt) (for some fixed\nvector r ∈R3), for the elastic equation (1.5) with constant parameters\nρ, λ, μ.\n7. In R2, consider\nfω(x) =\nˆ 2π\n\neikθ·x\ncos θ\ndθ,\nkθ = |k|\n,\nsin θ\nwith |k| = ω/c.\nShow that fω is a solution of the homogeneous\nHelmholtz equation (1.10) with constant c, and simplify the expres-\nChar\nt sh\nowin\ng results of t\nhe effects\nof an u\nnderwater bl\nast at various d\nepths.\nImage by MIT OpenCourseWare.\n\n1.3. EXERCISES\nsion of fω by means of a Bessel function. [Hint: show first that fω is\nradially symmetric.]\n8. Find all the functions τ(x) for which\nξ(x, t) = τ(x) -t\nis a solution of (1.12) in the case x ∈R.\nThe function τ(x) has the interpretation of a traveltime.\n9. Consider a characteristic curve as the level set ξ(x, t) =const., where ξ\nis a characteristic coordinate obeying (1.12). Express this curve para-\nmetrically as (X(t), t), and find a differential equation for X(t) of the\n\nform X(t) = . . . How do you relate this X(t) to the traveltime function\nτ(x) of the previous exercise? Justify your answer.\nSuch functions X(t) are exactly the rays -- light rays or sound rays.\nThey encode the idea that waves propagate with local speed c(x).\n10. Give a complete solution to the wave equation in Rn,\n∂2u\n∂t2 = c2∆u,\nu(x, 0) = u0(x),\n∂u(x, 0) = u1(x),\n∂t\nby Fourier-transforming u(x, t) in the x-variable, solving the resulting\nODE to obtain the e±i|k|/ct time dependencies, matching the initial con-\nditions, and finishing with an inverse Fourier transform. The resulting\nformula is a generalization of d'Alembert's formula.\n11. We have seen the expression of the wave equation's Green function in\nthe (x, t) and (x, ω) domains. Find the expression of the wave equa-\ntion's Green function in the (ξ, t) and (ξ, ω) domains, where ξ is dual\nto x and ω is dual to t. [Hint: it helps to consider the expressions of\nthe wave equation in the respective domains, and solve these equations,\nrather than take a Fourier transform.]\n12. Show that the Green's function of the Poisson or Helmholtz equa-\ntion in a bounded domain with homogeneous Dirichlet or Neumann\nboundary condition is symmetric: G(x, y) = G(y, x). [Hint: consider\nG(x, y)∆xG(x, z) -G(x, z)∆xG(x, y). Show that this quantity is the\ndivergence of some function. Integrate it over the domain, and show\nthat the boundary terms drop.]\n\nCHAPTER 1. WAVE EQUATIONS\n13. Check that the relation 1 = R2 + Z1T 2 for the reflection and transmis-\nZ2\nsion coefficiets follows from conservation of energy for acoustic waves.\n[Hint: use the definition of energy given in section 1.1.1, and the gen-\neral form (1.20, 1.21) of a wavefield scattering at a jump interface in\none spatial dimension.]\n14. The wave equation (3.2) can be written as a first-order system\n∂w\nM\n-Lw = fe,\n∂t\nwith\n\n∂u/∂t\nm\n∇·\nf\nw =\n,\nM =\n,\nL =\n,\nfe=\n.\n∇u\n∇\nFirst, check that L∗= -L for the L2 inner product ⟨w, w′⟩=\n\n(w1w′\n1+\nw2 · w′\n2) dx where w = (w\nT\n1, w2) . Then, check that E = ⟨w, Mw⟩is a\nconserved quantity.\n15. Another way to write the wave equation (3.2) as a first-order system is\n∂w\nM\n-Lw = fe,\n∂t\nwith\n\nu\nm\nI\nf\nw =\n,\nM =\n,\nL =\n,\nfe=\n.\nv\n∆\nFirst, check that L∗= -L for the inner product ⟨w, w′⟩=\n(∇u·∇u′+\nvv′) dx. Then, check that E = ⟨w, Mw⟩is a conserved quan\n\ntity.\n\nChapter 2\nGeometrical optics\nThe material in this chapter is not needed for SAR or CT, but it is founda-\ntional for seismic imaging.\nFor simplicity, in this chapter we study the variable-wave speed wave\nequation\n\n(!)\nc2(x)\n∂2\n\n-∆\nu = 0.\n∂t2\nAs explained earlier, this equation models either constant-density acoustics\n(c2(x) is then the bulk modulus), or optics (cref/c(x) is then the index of\nrefraction for some reference level cref). It is a good exercise to generalize\nthe constructions of this chapter in the case of wave equations with several\nphysical parameters.\n2.1\nTraveltimes and Green's functions\nIn a uniform 3D medium, we have seen that the acoustic Green's function\n(propagator) is\nδ(ct -|x -y|)\nG(x, y, t) =\n.\n(2.1)\n4πc|x -y|\nIn a variable (smooth) medium c(x), we can no longer expect an explicit\nformula for G. However, to good approximation, the Green's function can\nbe expressed in terms of a progressing-wave expansion as\nG(x, y, t) = a(x, y)δ(t -τ(x, y)) + R(x, y, t),\n(2.2)\n\nCHAPTER 2. GEOMETRICAL OPTICS\nwhere a is some smooth amplitude function, τ is the so-called traveltime\nfunction, and R is a remainder which is not small, but smoother than a delta\nfunction.\nThe functions a and τ are determined by substituting the expression\nabove in the wave equation\n\nc2(x)\n∂2\n\n-∆x\nG(x, y, t) = 0,\nx =\ny,\n∂t2\nand equating terms that have the same order of smoothness. By this, we\nmean that a δ(x) is smoother than a δ′(x), but less smooth than a Heaviside\nstep function H(x). An application of the chain rule gives\n\nc2(x)\n∂2\n∂t2 -∆x\n\nG = a\n\nc2(x) -|∇xτ|2\n\nδ′′(t -τ)\n+ (2∇xτ · ∇xa -a∆xτ) δ′(t -τ)\n+ ∆xaδ(t -τ) +\n\nc2(x)\n∂2\n\n-∆x\nR.\n∂t2\nThe δ′′ term vanishes if, and in the case a = 0, only if\n|∇xτ(x, y)| =\n,\n(2.3)\nc(x)\na very important relation called the eikonal equation for τ. It determines τ\ncompletely for x in some neighborhood of y. Notice that τ has the units of\na time.\nThe δ′ term vanishes if and only if\n2∇xτ(x, y) · ∇xa(x, y) -a(x, y)∆xτ(x, y) = 0,\n(2.4)\na relation called the transport equation for a. It determines a up to a mul-\ntiplicative scalar, for x in a neighborhood of y.\nAs for the term involving δ, it is a good exercise (see end of chapter) to\ncheck that the multiplicative scalar for the amplitude a can be chosen so that\nthe solution R of\n\n∆xa(x, y)δ(t -τ(x, y)) +\nc2(x)\n∂2\n∂t2 -∆x\n\nR = δ(x -y)δ(t)\n\n2.1. TRAVELTIMES AND GREEN'S FUNCTIONS\nis smoother than G itself. A good reference for progressing wave expansions\nis the book \"Methods of Mathematical Physics\" by Courant and Hilbert (pp.\n622 ff. in volume 2).\nThis type of expansion for solutions of the wave equation is sometimes de-\nrived in the frequency domain ω rather than the time domain t. In that case,\nit often takes on the name geometrical optics. Taking the Fourier transform\nof (2.2), we get the corresponding Ansatz in the ω domain:\nGb(x, y, ω) =\nˆ\neiωtG(x, y, t) dt = a(x, y)eiωτ(x,y) + Rb(x, y, ω).\n(2.5)\nBecause τ appears in a complex exponential, it is also often called a phase.\nThe same exercise of determining a and τ can be done, by substituting this\nexpression in the Helmholtz equation, with the exact same outcome as earlier.\nInstead of matching like derivatives of δ, we now match like powers of ω. The\nω2 term is zero when the eikonal equation is satisfied, the ω term is zero when\nthe transport equation is satisfied, etc.\nDoing the matching exercise in the frequency domain shows the true\nnature of the geometrical optics expression of the Green's function: it is a\nhigh-frequency approximation.\nLet us now inspect the eikonal equation for τ and characterize its solu-\ntions. In a uniform medium c(x) = c0, it is easy to check the following two\nsimple solutions.\n- With the condition τ(y, y) = 0, the solution is the by-now familiar\n|x -y|\nτ(x, y) =\n,\nc0\nwhich defines a forward light cone, (or -|x-y|, which defines a backward\nc0\nlight cone,) and which helps recover the phase of the usual Green's\nfunction (2.1) when plugged in either (2.2) or (2.5).\n- This is however not the only solution. With the condition τ(x) = 0\nfor x1 = 0 (and no need for a parameter y), a solution is τ(x) = |x1|\nc0 .\nAnother one would be τ(x) = x1.\nc0\nFor more general boundary conditions of the form τ(x) = 0 for x on some\ncurve Γ, but still in a uniform medium c(x) = c0, τ(x) takes on the interpre-\ntation of the distance function to the curve Γ.\n\nCHAPTER 2. GEOMETRICAL OPTICS\nNote that the distance function to a curve may develop kinks, i.e., gradi-\nent discontinuities. For instance, if the curve is a parabola x2 = x2\n1, a kink\nis formed on the half-line x = 0, x\n2 ≥\nabove the focus point. This com-\nplication originates from the fact that, for some points x, there exist several\nsegments originating from x that meet the curve at a right angle. At the\nkinks, the gradient is not defined and the eikonal equation does not, strictly\nspeaking, hold. For this reason, the eikonal equation is only locally solvable\nin a neighborhood of Γ. To nevertheless consider a generalized solution with\nkinks, mathematicians resort to the notion of viscosity solution, where the\nequation\n= |∇xτε|2 + ε2∆xτε\nc2(x)\nis solved globally, and the limit as ε →0 is taken. Note that in the case\nof nonuniform c(x), the solution generically develops kinks even in the case\nwhen the boundary condition is τ(y, y) = 0.\nIn view of how the traveltime function appears in the expression of the\nGreen's function, whether in time or in frequency, it is clear that the level\nlines\nτ(x, y) = t\nfor various values of t are wavefronts. For a point disturbance at y at t = 0,\nthe wavefront τ(x, y) = t is the surface where the wave is exactly supported\n(when c(x) = c0 in odd spatial dimensions), or otherwise essentially sup-\nported (in the sense that the wavefield asymptotes there.) It is possible to\nprove that the wavefield G(x, y, t) is exactly zero for τ(x, y) > t, regardless\nof the smoothness of c(x), expressing the idea that waves propagate no faster\nthan with speed c(x).\nFinally, it should be noted that\nφ(x, t) = t -τ(x, y)\nis for each y (or regardless of the boundary condition on τ) a solution of the\ncharacteristic equation\n∂ξ2\n= |∇xξ|2,\n∂t\ncalled a Hamilton-Jacobi equation, and already encountered in chapter 1.\nHence the wavefronts t -τ(x, y) = 0 are nothing but characteristic surfaces\nfor the wave equation. They are the space-time surfaces along which the\nwaves propagate, in a sense that we will make precise in section 8.1.\n\n2.2. RAYS\n2.2\nRays\nWe now give a general solution of the eikonal equation, albeit in a somewhat\nimplicit form, in terms of rays. The rays are the characteristic curves for the\neikonal equation. Since the eikonal equation was already itself characteristic\nfor the wave equation (see the discussion at the end of the preceding section),\nthe rays also go by the name bicharacteristics.\nThe rays are curves X(t) along which the eikonal equation is simplified, in\nthe sense that the total derivative of the traveltime has a simple expression.\nFix y and remove it from the notations. We write\nd\n\nτ(X(t)) = X(t) · ∇τ(X(t)).\n(2.6)\ndt\nThis relation will simplify if we define the ray X(t) such that\n- the speed | X(t)| is c(x), locally at x = X(t);\n-\n\nthe direction of X(t) is perpendicular to the wavefronts, i.e., aligned\nwith ∇τ(x) locally at x = X(t).\nThese conditions are satisfied if we specify the velocity vector as\n∇τ(X(t))\nX(t) = c(X(t))\n.\n(2.7)\n|∇τ(X(t))|\nSince the eikonal equation is |∇τ(x)| = 1/c(x), we can also write\nX(t) = c2(X(t))∇τ(X(t)).\n\nUsing either expression of X(t) in (2.6), we have\nd τ(X(t)) = 1,\ndt\nwhich has for solution\nτ(X(t)) -τ(X(t0)) = t -t0.\nWe now see that τ indeed has the interpretation of time. Provided we can\nsolve for X(t), the formula above solves the eikonal equation.\nThe differential equation (2.7) for X(t) is however not expressed in closed\nform, because it still depends on τ. We cannot however expect closure from\n\nCHAPTER 2. GEOMETRICAL OPTICS\na single equation in X(t). We need an auxiliary quantity that records the\ndirection of the ray, such as\nξ(t) = ∇τ(X(t)).\nThen (all the functions of x are evaluated at X(t))\nξ(t) = ∇∇τ · X(t)\n= ∇∇τ(X(t)) · c2∇τ\nc2\n= 2 ∇|∇τ|2\n= c2\n∇c-2\nc-2\n= -\n∇c2\n|∇τ|2\n= -\n∇c2\n= -|ξ(t)|2\n∇(c2)(X(t)).\nWe are now in presence of a closed, stand-alone system for the rays of geo-\nmetrical optics in the unknowns X(t) and ξ(t):\n(\nX(t)\n= c2(X(t)) ξ(t),\nX(0) = x0,\nξ(t)\n= -∇(c2)(X(t)) |ξ(t)|2,\nξ(0) = ξ0.\nThe traveltime function τ(x) is equivalently determined as the solution of\nthe eikonal equation (the Eulerian viewpoint), or as the time parameter for\nthe ray equations (the Lagrangian viewpoint). While X is a space variable,\ntogether (X, ξ) are called phase-space variables.\nIt is fine to speak of a\ncurve X(t) in space as a ray, although strictly speaking the ray is a curve\n(X(t), ξ(t)) in phase-space. Because of its units, ξ is in this context often\ncalled the slowness vector.\nThe system above is called Hamiltonian because it can be generated as\n\nX(t)\n= ∇ξH(X(t), ξ(t)),\nξ(t)\n= -∇xH(X(t), ξ(t)),\nfrom the Hamiltonian\nH(x, ξ) = 2c2(x)|ξ|2.\n\n2.3. AMPLITUDES\nThis is the proper Hamiltonian for optics or acoustics; the reader is already\np2\naware that the Hamiltonian of mechanics is H(x, p) =\n+ V (x). Note that\n2m\nH is a conserved quantity along the rays1\nIt can be shown that the rays are extremal curves of the action functional ($)\nS(\n) =\nˆ b\nX\na c(x)dl=\nˆ 1\n| X(t)|dt,\ns.t.\nX(0) = a, X(1) = b,\nc(X(t))\na result called the Fermat principle. For this reason, it can also be shown\nthat the rays are geodesics curves in the metric\n($)\nX\nds2 = c-2(x)dx2,\ndx2 =\ndxi ∧dxi.\ni\nThe traveltime τ therefore has yet another interpretation, namely that of\naction in the variational Hamiltonian theory2.\nInspection of the ray equations now gives another answer to the question\nof solvability of τ from the eikonal equation. There is no ambiguity in spec-\nifying τ from |∇τ(x, y)| = 1/c(x) and τ(y, y) = 0 as long as there is a single\nray linking y to x. When there are several such rays -- a situation called mul-\ntipathing -- the traveltime function takes on multiple values τj(x, y) which\neach solve the eikonal equation locally. The function that records the \"num-\nber of arrivals\" from y to x has discontinuities along curves called caustics;\nthe respective eikonal equations for the different branches τj hold away from\ncaustics. The global viscosity solution of the eikonal equation only records\nthe time of the first arrival.\n2.3\nAmplitudes\nWe can now return to the equation (2.4) for the amplitude, for short\n2∇τ · ∇a = -a∆τ.\nIt is called a transport equation because it turns into an ODE in characteristic\ncoordinates, i.e., along the rays.\nAgain, all the functions of x should be\n1So is the symplectic 2-form dx ∧dξ, hence areas are conserved as well.\n2There is no useful notion of Lagrangian in optics, because the photon is massless.\nSee the book on Mathematical methods of classical mechanics by Arnold and the treatise\nby Landau and Lifschitz for the fascinating analogy between the equations of optics and\nLagrangian/Hamiltonian mechanics.\n\nCHAPTER 2. GEOMETRICAL OPTICS\nevaluated at X(t) in the following string of equalities:\nd\n\na(X(t)) = X(t) · ∇a\ndt\n= c2∇τ · ∇a\nc2\n= -\na ∆τ.\nIf τ is assumed known, then this equation specifies a(X(t)) up to a multi-\nplicative constant. If we wish to eliminate τ like we did earlier for the rays,\nthen we need to express ∆τ(X(t)) not just in terms of X(t) and ξ(t), but\nalso in terms of the first partials\n∂X\n∂X0(t), ∂X\n∂ξ0(t),\n∂ξ\n∂X0(t),\n∂ξ (t) with respect to\n∂ξ0\nthe initial conditions3.\nThe transport equation can also be written in divergence form,\n∇· (a2∇τ) = 0,\nwhich suggests that there exists an underlying conserved quantity, which\nintegration will reveal. Assume for now that space is 3-dimensional. Consider\na ray tube R, i.e, an open surface spanned by rays. Close this surface with two\ncross-sections S+ and S-normal to the rays. Apply the divergence theorem\nin the enclosed volume V . This gives\n0 =\n\n∇· (a2∇τ)dV =\n‹\na2∇τ · ndS,\nV\n∂V\nwhere n is the outward normal vector to the surface ∂V = R ∪S+ ∪S-.\n- For x on R, the normal vector n is by definition (of R) perpendicular\nto the ray at x, hence ∇τ · n = 0.\n- For x on S±, the normal vector n is parallel to the ray at x, hence\n∇τ · n = ±|∇τ|.\nAs a result,\nˆ\na2|∇τ|dS =\nS+\nˆ\na2|∇τ|dS,\nS-\n3See for instance the 2006 paper by Candes and Ying on the phase-flow method for\nthese equations.\n\n2.4. CAUSTICS\nthus\nˆ\na2\nS+ c dS =\nˆ\nS-\na2\ndS.\nc\nThis relation is an expression of conservation of energy. Passing to an in-\nfinitesimally thin ray tube linking x0 to x, it becomes\ns\na(x) = a(x0)\nc(x0)\nc(x)\ndS .\ndS0\nIt is again clear that the amplitude is determined up to a multiplicative scalar\nfrom this equation. A similar argument can be made in 2 space dimensions,\nand leads to the same conclusion with the ratio of line elements ds/ds0 in\nplace of the ratio of surface elements dS/dS0.\nExamples of solutions in a uniform medium in R3 include\n- Plane waves, for which dS/dS0 = 1 hence a = constant,\n- Cylindrical waves about r = 0, for which dS/dS0 = r/r0 hence a ∼\n√\n1/\nr,\n- Spherical waves about r = 0, for which dS/dS\n0 = (r/r0) hence a ∼\n1/r,\n- A cylindrical focus or a caustic point at r = a can generically be seen\n√\nas a time-reversed cylindrical wave, hence a ∼1/\nr -a. A spherical\nfocus is a geometrical exception; it would correspond to a ∼1/(r -a).\nIn the infinite-frequency geometrical optics approximation, the amplitude\nindeed becomes infinite at a focus point or caustic curve/surface. In reality,\nthe amplitude at a caustic is an increasing function of the frequency ω of\nthe underlying wave. The rate of growth is generically of the form ω1/6, as\nestablished by Keller in the 1950s.\nCaustics and focus points give rise to bright spots in imaging datasets,\nalthough this information is probably never explicitly used in practice to\nimprove imaging.\n2.4\nCaustics\nFor fixed t, and after a slight change of notation, the Hamiltonian system\ngenerates the so-called phase map (x, ξ) 7→(y(x, ξ), η(x, ξ)). Its differential\n\nCHAPTER 2. GEOMETRICAL OPTICS\nis block partitioned as\n\n∂y\ny\n∇(x,ξ)\n=\nη\n∂x\n∂y\n∂ξ\n∂η\n∂x\n∂η\n!\n.\n∂ξ\nBesides having determinant 1, this Jacobian matrix has the property that\nX ∂ηj\nδkl =\nj\n∂ξl\n∂yj\n∂xk\n-∂ηj\n∂xk\n∂yj .\n(2.8)\n∂ξl\nThis equation is conservation of the second symplectic form dη∧dy = dξ∧dx\nwritten in coordinates. It also follows from writing conservation of the first\nsymplectic form ξ · dx = η · dy as\nX\n∂yi\nξj =\nηi\ni\n∂xj\n,\n0 =\nX\ni\nηi\n∂yi ,\n∂ξj\nand further combining these expressions. (dη ∧dy = dξ ∧dx also follows\nfrom ξ ·dx = η ·dy by Cartan's formula). Equation (2.8) can also be justified\ndirectly, see the exercise section. Note in passing that it is not a Poisson\nbracket.\nIt is instructive to express (2.8) is ray coordinates. Without loss of gen-\nerality choose the reference points x0 = 0 and ξ0 = (1, 0)T. Let x1 and ξ1 be\nthe coordinates of x and ξ along ξ0, and x2, ξ2 along ξ⊥\n0 . Consider (y0, η0) the\nimage of (x0, ξ0) under the phase map. Let y1 and y2 be the coordinates of y\nand η along η0, and x2, ξ2 along η⊥\n0 . In other words, the coordinates labeled\n\"1\" are along the ray (longitudinal), and \"2\" across the ray (transversal).\nIn two spatial dimensions, only the coordinates across the ray give rise to\na nontrivial relation in (2.8). One can check that the k = l = 2 element of\n(2.8) becomes\n∂η2\n1 = ∂ξ2\n∂y2\n∂x2\n-∂η2\n∂x2\n∂y2.\n(2.9)\n∂ξ2\nWhen either of the terms in this equation vanish, we say that (y, η) is on a\ncaustic. Two regimes can be contrasted:\n- If ∂y2/∂x2 = 0, we are in presence of a \"x-caustic\". This means that, as\nthe initial point x is moved infinitesimally in a direction perpendicular\nto the take-offdirection, the resulting location y does not move. A\nx-caustic is at the tip of a swallowtail pattern formed from an initial\nplane wavefront.\n\n2.5. EXERCISES\n- If ∂y2/∂ξ2 = 0, we are in presence of a \"ξ-caustic\". This means that,\nas the initial direction angle arg ξ changes infinitesimally, the resulting\nlocation y does not move. A ξ-caustic is at the tip of a swallowtail\npattern formed from an initial point wavefront.\nEquation (2.9) shows that these two scenarios cannot happen simultaneously.\nIn fact, if the variation of y2 with respect to x2 is zero, then the variation\nof y2 with respect to ξ2 must be maximal to compensate for it; and vice-\nversa. When t is the first time at which either partial derivative vanishes,\nthe caustic is a single point: we speak of a focus instead.\nNotice that ∂η2/∂x2 = 0 and ∂η2/∂ξ2 = 0 are not necessarily caustic\nevents; rather, they are inflection points in the wavefronts (respectively ini-\ntially plane and initially point.)\n2.5\nExercises\n1. (Difficult) Show that the remainder R in the progressing wave expan-\nsion is smoother than the Green's function G itself.\n2. In this exercise we compute the Fr echet derivative of traveltime with\nrespect to the wave speed. For simplicity, let n(x) = 1/c(x).\nx\n(a) In one spatial dimension, we have already seen that τ(x) =\nn(x′)dx′.\nx0\nFind an expression for δτ(x)/δn(y) (or equivalently for the\n\noper-\nator that it generates via ⟨δτ(x)/δn, h⟩for a test function h).\n(b) In several spatial dimensions, τ(x) obeys |∇τ(x)| = n(x) with\nτ(0) = 0, say. First, show that δτ(x)/δn(y) obeys a transport\nequation along the rays. Then solve this equation. Provided there\nis one ray between 0 and x, argue that δτ(x)/δn(y), as a function\nof y, is concentrated along this ray.\n(c) What do your answers become when the derivative is taken with\nrespect to c(x) rather than n(x)?\nThe function δτ(x)/δn(y) of y is often called sensitivity kernel (of τ\nwith respect to n). It's a distribution, really.\n3. Show that the Hamiltonian is conserved along the trajectories of a\nHamiltonian system.\n\nCHAPTER 2. GEOMETRICAL OPTICS\n4. Show that the alternative Hamiltonian H(x, ξ) = c(x)|ξ| generates an\nequivalent system of ODEs for the rays.\n5. Show that the rays are circular in a linear wave velocity model, i.e.,\nc(x) = z in the half-plane z > 0. Note: {z > 0} endowed with ds2 =\ndx2+dz2\nob\nz2\nis called the Poincar e half-plane, a very important\nject in\nmathematics.\n6. Show that the traveltime τ is convex as a function of the underlying\nmedium c(x), by invoking the Fermat principle.\n7. Prove (2.8).\nHint. Show it holds at time zero, and use the Hamiltonian structure\nto show that the time derivative of the whole expression is zero.\nP\n8. Let {y(x, ξ), η(x, ξ)} be the fixed-time phase map. Show that\n∂ηi\ni ∂ξk\n∂yi\n∂ξl\nis symmetric.\nHint. Same hint as above. Show that the time derivative of the dif-\nference of the matrix and its transpose vanishes.\n9. Let τ(x, y) be the 2-point traveltime, and let {y(x, ξ), η(x, ξ)} be the\nfixed-time phase map for the Hamiltonian of isotropic optics. Prove or\ndisprove:\n(a)\nX ∂yk\nk\n∂ξj\n∂τ (x, y(x, ξ)) = 0;\n∂yk\n(b)\nX ∂ηi\nk\n∂ξk\n∂2τ\nX ∂yk\n(x, y(x, ξ)) +\n∂xj∂xk\nk\n∂xj\n∂2τ\n(x, y(x, ξ)) = 0.\n∂yi∂yk\n\nChapter 3\nScattering series\nIn this chapter we describe the nonlinearity of the map c 7→u in terms of a\nperturbation (Taylor) series. To first order, the linearization of this map is\ncalled the Born approximation. Linearization and scattering series are the\nbasis of most inversion methods, both direct and iterative.\nThe idea of perturbation permeates imaging for physical reasons as well.\nIn radar imaging for instance, the background velocity is c0 = 1 (speed\nof light), and the reflectivity of scatterers is viewed as a deviation in c(x).\nThe assumption that c(x) does not depend on t is a strong one in radar:\nit means that the scatterers do not move. In seismology, it is common to\nconsider a smooth background velocity c0(x) (rarely well known), and explain\nthe scattered waves as reflections due to a \"rough\" (singular/oscillatory)\nperturbations to this background. In both cases, we will write\nc2(x) = m(x),\n= m0(x),\nm for \"model\",\nc2\n0(x)\nand, for some small number ε,\nm(x) = m0(x) + εm1(x).\n(3.1)\nNote that, when perturbing c(x) instead of m(x), an additional Taylor\napproximation is necessary:\nc(x) = c0(x) + εc1(x)\n⇒\nc2(x) ≃\nc2\n0(x) -2εc1(x).\nc3\n0(x)\nWhile the above is common in seismology, we avoid making unnecessary\nassumptions by choosing to perturb m(x) = 1/c2(x) instead.\n\nCHAPTER 3. SCATTERING SERIES\nPerturbations are of course not limited to the wave equation with a single\nparameter c. The developments in this chapter clearly extend to more general\nwave equations.\n3.1\nPerturbations and Born series\nLet\n∂2u\nm(x)\n-∆u = f(x, t),\n(3.2)\n∂t2\nwith zero initial conditions and x ∈Rn. Perturb m(x) as in (3.1). The\nwavefield u correspondingly splits into\nu(x) = u0(x) + usc(x),\nwhere u0 solves the wave equation in the undisturbed medium m0,\n∂2u0\nm0(x)\n-∆u0 = f(x, t).\n(3.3)\n∂t2\nWe say u is the total field, u0 is the incident field1, and usc is the scattered field,\ni.e., anything but the incident field.\nWe get the equation for usc by subtracting (3.3) from (3.2), and using\n(3.1):\n∂2usc\nm0(x) ∂t2\n-∆usc = -ε m1(x)∂2u.\n(3.4)\n∂t2\nThis equation is implicit in the sense that the right-hand side still depends\non usc through u. We can nevertheless reformulate it as an implicit integral\nrelation by means of the Green's function:\nˆ t\n∂2u\nusc(x, t) = -ε\nG(x, y; t -s)m1(y)\nˆ\nRn\n(y, s) dyds.\n∂t2\nAbuse notations slightly, but improve conciseness greatly, by letting\n- G for the operator of space-time integration against the Green's func-\ntion, and\n1Here and in the sequel, u0 is not the initial condition. It is so prevalent to introduce\nthe source as a right-hand side f in imaging that it is advantageous to free the notation\nu0 and reserve it for the incident wave.\n\n3.1. PERTURBATIONS AND BORN SERIES\n- m1 for the operator of multiplication by m1.\nThen usc = -ε G m\n∂2u\n1 ∂t2 . In terms of u, we have the implicit relation\nu = u0 -ε G m1\n∂2u,\n∂t2\ncalled a Lippmann-Schwinger equation. The field u can be formally2 ex-\npressed in terms of u0 by writing\n\n∂2\nu =\nI + ε G m1\n-1\nu0.\n(3.5)\n∂t2\nWhile this equation is equivalent to the original PDE, it shines a different\nlight on the underlying physics. It makes explicit the link between u0 and u,\nas if u0 \"generated\" u via scattering through the medium perturbation m1.\nWriting [I + A]-1 for some operator A invites a solution in the form of a\nNeumann series I -A + A2 -A3 + . . ., provided ∥A∥< 1 in some norm. In\nour case, we write\n\n∂2\nu = u0 -ε\nG m1 ∂t2\n\nu0 + ε2\n\nG m1\n∂2\n∂t2\n\nG m1\n∂2\n∂t2\n\nu0 + . . .\nThis is called a Born series. The proof of convergence, based on the \"weak\nscattering\" condition ε∥G m1\n∂2\nb\n∂2∥∗< 1, in some norm to\ne determined, will\nt\nbe covered in the next section. It retroactively justifies why one can write\n(3.5) in the first place.\nThe Born series carries the physics of multiple scattering. Explicitly,\nu = u0\n(incident wave)\n-ε\nˆ ˆt\n∂2u0\nG(x, y; t -s)m1(y)\nRn\n∂t2 (y, s) dyds\n(single scattering)\n+ ε2\nˆ t\nˆ\nRn G(x, y2; t -s2)m1(y2) ∂2 ˆ s2ˆ\n∂2u0\nG(y2, y1; s2 -s1)m1(y1)\n∂s2\nRn\n\n(y1, s1) dy1ds1\ndy2ds2\n∂t2\n(double scattering)\n+ . . .\n2For mathematicians, \"formally\" means that we are a step ahead of the rigorous ex-\nposition: we are only interested in inspecting the form of the result before we go about\nproving it. That's the intended meaning here. For non-mathematicians, \"formally\" often\nmeans rigorous, i.e., the opposite of \"informally\"!\n\nCHAPTER 3. SCATTERING SERIES\nWe will naturally summarize this expansion as\nu = u0 + εu1 + ε2u2 + . . .\n(3.6)\nwhere εu1 represent single scattering, ε2u2 double scattering, etc. For in-\nstance, the expression of u1 can be physically read as \"the incident wave\ninitiates from the source at time t = 0, propagates to y where it scatters due\nto m(y) at time t = s, then further propagates to reach x at time t.\" The\nexpression of u2 can be read as \"the incident wave initiates from the source at\nt = 0, propagates to y1 where it first scatters at time t = s1, them propagates\nto y2 where it scatters a second time at time t = s2, then propagates to x at\ntime t, where it is observed.\" Since scatterings are not a priori prescribed to\noccur at fixed points in space and time, integrals must be taken to account\nfor all physically acceptable scattering scenarios.\nThe approximation\nusc(x) ≃εu1(x)\nis called the Born approximation. From u1 = -Gm ∂2u0\n,\n∂t2\nwe can return to\nthe PDE and obtain the equation for the primary reflections:\n∂2u1\nm0(x) ∂t2 -∆u1 = -m1(x)∂2u0.\n(3.7)\n∂t2\nThe only difference with (3.4) is the presence of u0 in place of u in the right-\nhand side (and ε is gone, by choice of normalization of u1). Unlike (3.4),\nequation (3.7) is explicit: it maps m1 to u1 in a linear way. The incident field\nu0 is determined from m0 alone, hence \"fixed\" for the purpose of determining\nthe scattered fields.\nIt is informative to make explicit the dependence of u1, u2, . . . on m1. To\nthat end, the Born series can be seen as a Taylor series of the forward map\nu = F[m],\nin the sense of the calculus of variations. Denote by δF\nδm[m0] the \"functional\ngradient\" of F with respect to m, evaluated at m0. It is an operator acting\nfrom model space (m) to data space (u). Denote by δ2F\n]\nδ\n2[m0 the \"functional\nm\nHessian\" of F with respect to m, evaluated at m0. It is a bilinear form from\nmodel space to data space. See the appendix for background on functional\nderivatives. Then the functional version of the Taylor expansion enables to\nexpress (3.6) in terms of the various derivatives of F as\nδF\nu = u0 + ε δm[m0] m1 + ε2\n2 ⟨δ2F\nδm2[m0] m1, m1⟩+ . . .\n\n3.2. CONVERGENCE OF THE BORN SERIES (MATH)\nIt is convenient to denote the linearized forward map by (print) F:\nδF\nF = δm[m0],\nor, for short, F = ∂u . It is a linear operator. The point of F is that it makes\n∂m\nexplicit the linear link between m1 and u1:\nu1 = Fm1.\nWhile F is supposed to completely model data (up to measurement errors),\nF would properly explain data only in the regime of the Born approximation.\nLet us show that the two concepts of linearized scattered field coincide,\nnamely\nδF\nu1 = δm[m0] m1 = -Gm1\n∂2u0.\n∂t2\nThis will justify the first term in the Taylor expansion above. For this pur-\npose, let us take the\nδ derivative of (3.2). As previously, write u = F(m)\nδm\nand F = δF\nδm[m]. We get the operator-valued equation\n∂2u\n∂2\nI + m\n∂t2\nF -∆F = 0.\n∂t2\nEvaluate the functional derivatives at the base point m0, so that u = u0.\nApplying each term as an operator to the function m1, and defining u1 =\nFm1, we obtain\n∂2u0\nm1 ∂t2 + m0\n∂2u1 -∆u1 = 0,\n∂t2\nwhich is exactly (3.7).\nApplying G on both sides, we obtain the desired\nconclusion that u1 = -Gm ∂2u0\n.\n∂t2\n3.2\nConvergence of the Born series (math)\nWe are faced with two very interrelated questions: justifying convergence of\nthe Born series, and showing that the Born approximation is accurate when\nthe Born series converges. The answers can either take the form of mathe-\nmatical theorems (this section), or physical explanations (next section). As\nof 2013, the community's mathematical understanding is not yet up to par\nwith the physical intuition!\n\nCHAPTER 3. SCATTERING SERIES\nLet us describe what is known mathematically about convergence of Born\nseries in a simple setting. To keep the notations concise, it is more convenient\nto treat the wave equation in first-order hyperbolic form\n∂w\nM\n-Lw = f,\nL∗= -L,\n(3.8)\n∂t\nfor some inner product ⟨w, w′⟩. The conserved energy is then E = ⟨w, Mw⟩.\nSee one of the exercises at the end of chapter 1 to illustrate how the wave\nequation can be put in precisely this form, with ⟨w, w′⟩the usual L2 inner\nproduct and M a positive diagonal matrix.\nConsider a background medium M0, so that M = M0 + εM1. Let w =\nw0 + εw1 + . . . Calculations very similar to those of the previous section (a\ngood exercise) show that\n- The Lippmann-Schwinger equation is\n∂w\nw = w0 -εGM1\n,\n∂t\nwith the Green's function G = (M\n∂\n-L)-1.\n∂t\n- The Neumann series of interest is\n∂w0\nw = w0 -εGM1 ∂t + ε2GM1\n∂\n∂tGM1\n∂w0\n∂t + . . .\nWe identify w1 = -GM1\n∂w0.\n∂t\n- In differential form, the equations for the incident field w0 and the\nprimary scattered field w1 are\n∂w0\nM0 ∂t -Lw0 = f,\nM0\n∂w1\n∂t -Lw1 = -M1\n∂w0,\n(3.9)\n∂t\n- Convergence of the Born series occurs when\n∂\nε∥GM1\n∥∗< 1,\n∂t\nin some induced operator norm, i.e., when ε∥w1∥∗< ∥w0∥∗for arbitrary\nw0, and w1 = -GM ∂w0\n1 ∂t , for some norm ∥· ∥∗.\n\n3.2. CONVERGENCE OF THE BORN SERIES (MATH)\nNotice that the condition ε∥w1∥∗< ∥w0∥∗is precisely one of weak scat-\ntering, i.e., that the primary reflected wave εw1 is weaker than the incident\nwave w0.\nWhile any induced norm over space and time in principle works for the\nproof of convergence of the Neumann series, it is convenient to use\np\n∥w∥∗= max\n0≤t≤T\n⟨w, M0w⟩= max\n0≤t≤T ∥\np\nM0w∥.\nNote that it is a norm in space and time, unlike ∥w∥=\np\n⟨w, w⟩, which is\nonly a norm in space.\nTheorem 3. (Convergence of the Born series) Assume that the fields w, w0,\nw1 are bandlimited with bandlimit3 Ω. Consider these fields for t ∈ [0, T ]. Then\nthe weak scattering condition ε∥w1∥∗ < ∥w0∥∗ is satisfied, hence the Born\nseries converges, as soon as\nM1\nε ΩT ∥\n∥inf< 1.\nM0\nProof. We compute\nd\n∂w1\n⟨w1, M0w1⟩= 2⟨w1, M0\ndt\n⟩\n∂t\n∂w0\n= 2⟨w1, Lw1 -M1 ∂t ⟩\n= -2⟨w1, M1\n∂w0⟩\nbecause L∗= -L\n∂t\np\n= -2⟨\nM0w1, M1\n√M0\n∂w0\n∂t ⟩.\nSquare roots and fractions of positive diagonal matrices are legitimate oper-\nations. The left-hand-side is also\nd\n√\n⟨w1, M0w1⟩= 2∥\ndt\nM0w1∥2\nd\ndt∥√M0w1∥2.\nBy Cauchy-Schwarz, the right-hand-side is majorized by\n2∥\np\nM1\nM0w1∥2 ∥√M0\n∂w0\n∂t ∥2.\nHence\nd\ndt∥\np\nM1\nM0w1∥2 ≤∥√M0\n∂w0\n∂t ∥2.\n3A function of time has bandlimit Ωwhen its Fourier transform, as a function of ω, is\nsupported in [-Ω, Ω].\n\nCHAPTER 3. SCATTERING SERIES\np\n∥\nt\nM0w1∥2 ≤\nˆ\nM1\n∥\n√M0\n∂w0\n∂t ∥2(s) ds.\n∥w1∥∗= max\n0≤t≤T ∥\np\nM0w1∥2 ≤T max\n0≤t≤T ∥M1\n√\n∂w0\nM0 ∂t ∥2\n≤T∥M1\nM0\n∥infmax\n0≤t≤T ∥\np\nM0\n∂w0∥2.\n∂t\nThis last inequality is almost, but not quite, what we need. The right-\nhand side involves\n∂w0 instead of w0.\nBecause time derivatives can grow\n∂t\narbitrarily large in the high-frequency regime, this is where the bandlimited\nassumption needs to be used. We can invoke a classical result known as Bern-\nstein's inequality4, which says that ∥f′∥inf ≤ Ω∥f∥inf for all Ω-bandlimited f.\nThen\nM1\n∥w1∥∗≤ΩT∥\n∥inf∥w0∥∗.\nM0\nIn view of our request that ε∥w1∥∗< ∥w0∥∗, it suffices to require\nM1\nε ΩT ∥M0\n∥inf< 1.\nSee the book Inverse Acoustic and Electromagnetic Scattering Theory by\nColton and Kress for a different analysis that takes into account the size of\nthe support of M1.\nNote that the beginning of the argument, up to the Cauchy-Scwharz\ninequality, is called an energy estimate in math. See an exercise at the end\nof this chapter. It is a prevalent method to control the size of the solution of\nmany initial-value PDE, including nonlinear ones.\nThe weak scattering condition ε∥w1∥∗< ∥w0∥∗encodes the idea that the\nprimary reflected field εw1 is small compared to the incident field w0. It is\nsatisfied when ε is small, and when w1 is not so large that it would undo the\nsmallness of ε (via the factors ΩT, for instance). It turns out that\n- the full scattered field wsc = w -w0 is also on the order of εΩT∥M1∥inf\n-- namely the high-order terms don't compromise the weak scattering\nsituation; and\n4The same inequality holds with the Lp norm for all 1 ≤p ≤inf.\n\n3.3. CONVERGENCE OF THE BORN SERIES (PHYSICS)\n- the remainder w -εw = w-w -εw is on the order of ε2\nsc\n(ΩT∥M1∥inf) .\nBoth claims are the subject of an exercise at the end of the chapter. The\nsecond claim is the mathematical expression that the Born approximation\nis accurate (small wsc -εw1 on the order of ε2) precisely when scattering is\nweak (εw1 and wsc on the order of ε.)\n3.3\nConvergence of the Born series (physics)\nLet us explain why the criterion εΩT < 1 (assuming the normalization\n∥M1/M0∥inf= 1) is adequate in some cases, and why it is grossly pessimistic\nin others.\n- Instead of m or M, consider the wave speed c0 = 1. Consider a constant\nperturbation c1 = 1, so that c = c0 + εc1 = 1 + ε. In one spatial\ndimension, u(x, T) = f(x -cT). As a Taylor series in ε, this is\nε2\nu(x, T) = f(x-(1+ε)T) = f(x-T)-εTf ′(x-T)+\nT 2f ′′(x-T)+. . .\nWe identify u0(x, T) = f(x -T) and u1(x, T) = -Tf ′(x -T). Assume\nnow that f is a waveform with bandlimit Ω, i.e., wavelength 2π/Ω. The\nBorn approximation\nf(x -(1 + ε)T) -f(x -T) ≃-εTf ′(x -T)\nis only good when the translation step εT between the two waveforms\non the left is a small fraction of a wavelength 2π/Ω, otherwise the\nsubtraction f(x-(1+ε)T)-f(x-T) will be out of phase and will not\ngive rise to values on the order of ε. The requirement is εT ≪2π/Ω,\ni.e.,\nεΩT ≪2π,\nwhich is exactly what theorem 3 is requiring. We could have reached\nthe same conclusion by requiring either the first or the second term\nof the Taylor expansion to be o(1), after noticing that |f ′| = O(Ω) or\n|f ′′| = O(Ω2). In the case of a constant perturbation c1 = 1, the waves\nundergo a shift which quickly becomes nonlinear in the perturbation.\nThis is the worst case: the requirement εΩT < 1 is sharp.\n\nCHAPTER 3. SCATTERING SERIES\n- As a second example, consider c0 = 1 and c1(x) = H(x). The profile\nof reflected and transmitted waves was studied in equations (1.20) and\n(1.21). The transmitted wave will undergo a shift as in the previous\nexample, so we expect εΩT < 1 to be sharp for it. The full reflected\nwave, on the other hand, is\nε\nur(x, T) = Rεf(-x -T),\nRε =\n.\n2 + ε\nNotice that ε only appears in the reflection coefficient Rε, not in the\nwaveform itself. As ε →0, ur expands as\nε\nur(x, T) = 2f(-x -T) -ε2\n4 f(-x -T) + . . .\nWe recognize u1 = 1f(-x -T). The condition for weak scattering and\naccuracy of the Born approximation is now simply ε < 1, which is in\ngeneral much weaker than εΩT < 1.\n- In the case when c0 = 1 and c1 is the indicator function of a thin slab\nin one dimension, or a few isolated scatterers in several dimensions, the\nBorn approximation is often very good. That's when the interpretation\nof the Born series in terms of multiple scattering is the most relevant.\nSuch is the case of small isolated objects in synthetic aperture radar:\ndouble scattering from one object to another is often negligible.\nThe Born approximation is often satisfied in the low-frequency regime\n(small Ω), by virtue of the fact that cycle skipping is not as much of an\nissue. In the high-frequency regime, the heuristics for validity of the Born\napproximation are that\n1. c0 or m0 should be smooth.\n2. c1 or m1 should be localized, or better yet, localized and oscillatory\n(zero mean).\nThe second requirement is the most important one: it prohibits transmitted\nwaves from propagating in the wrong velocity for too long. We do not yet\nhave a way to turn these empirical criteria and claims into rigorous mathe-\nmatical results. Seismologists typically try to operate in the regime of this\nheuristic when performing imaging with migration (see chapter on seismic\nimaging).\n\n3.4. A FIRST LOOK AT OPTIMIZATION\nConversely, there are a few settings in which the Born approximation is\nclearly violated: (i) in radar, when waves bounce multiple times before being\nrecorded (e.g. on the ground and on the face of a building, or in cavities\nsuch as airplane engines), (ii) in seismology, when trying to optimize over\nthe small-wavenumber components of m(x) (model velocity estimation), or\nwhen dealing with multiple scattering (internal multiples). However, note\nthat multiple reflections from features already present in the modeling (such\nas ghosts due to reflections at the ocean-air interface) do not count as non-\nlinearities.\nScattered waves that do not satisfy the Born approximation have long\nbeen considered a nuisance in imaging, but have recently become the subject\nof some research activity.\n3.4\nA first look at optimization\nIn the language of the previous sections, the forward map is denoted\nd = F[m],\nd = data,\nm = model,\nwhere dr,s(t) = us(xr, t),\n- xr is the position of receiver r,\n- s indexes the source,\n- and t is time.\nThe inverse problem of imaging is that of solving for m in the system of\nnonlinear equations d = F[m].\nNo single method will convincingly solve\nsuch a system of nonlinear equations efficiently and in all regimes.\nThe very prevalent least-squares framework formulate the inverse problem\nas finding m as the solution of the minimization problem\nmin J[m],\nwhere\nJ[m] =\nm\n∥d -F[m]∥2\n2,\n(3.10)\nwhere ∥d∥2\nP\nT\n2 =\n,s\n\n|dr,s(t)|\nr\nis the L2 norm squared in the space of vectors\nindexed by r, s (discrete) and t (continuous, say). J is called the output\nleast-squares criterion, or objective, or cost.\n\nCHAPTER 3. SCATTERING SERIES\nIn the sequel we consider iterative schemes based on the variations of J at\na base point m0, namely the functional gradient δJ\nδm[m0], a linear functional\nin m space; and the functional Hessian\nδ2J\nv\nδ\n2[m0], also called wa e-equation\nm\nHessian, an operator (or bilinear form) in m space. The appendix contains\na primer on functional calculus.\nTwo extreme scenarios cause problems when trying to solve for m as the\nminimizer of a functional J:\n- The inverse problem is called ill-posed when there exist directions m1 in\nwhich J(m) has a zero curvature, or a very small curvature, in the vicin-\nity of the solution m∗. Examples of such directions are the eigenvectors\nof the Hessian of J associated to small eigenvalues. The curvature is\nthen twice the eigenvalue, i.e., twice the second directional derivative\nin the eigen-direction. Small perturbations of the data, or of the model\nF, induce modifications of J that may result in large movements of\nits global minimum in problematic directions in the \"near-nullspace\"\nof the Hessian of J.\n- Conversely, the inverse problem may suffer from severe non-convexity\nwhen the abundance of local minima, or local \"valleys\", hinders the\nsearch for the global minimum.\nThis happens when the Hessian of\nJ alternates between having large positive and negative curvatures in\nsome direction m1.\nMany inversion problems in high-frequency imaging suffer from some (not\noverwhelming) amount of ill-posedness, and can be quite non-convex. These\ntopics will be further discussed in chapter 9.\nThe gradient descent method5 applied to J is simply\nm(k+1) = m(k)\nδJ\n-α\n[m(k)].\n(3.11)\nδm\nThe choice of α is a balance between stability and speed of convergence -\nsee two exercises at the end of the chapter. In practice, a line search for α is\noften a good idea.\nThe usual rules of functional calculus give the expression of\nδJ , also\nδm\nknown as the \"sensitivity kernel\" of J with respect to m.\n5Also called Landweber iteration in this nonlinear context.\n\n3.4. A FIRST LOOK AT OPTIMIZATION\nProposition 4. Put F = δF\nδm[m]. Then\nδJ [m] = F ∗(F[m] -d).\nδm\nProof. Since F[m + h] = F[m] + Fh + O(∥h∥2), we have\n⟨F[m+h]-d, F[m+h]-d⟩= ⟨F[m]-d, F[m]-d⟩+2⟨Fh, F[m]-d⟩+O(∥h∥2).\nTherefore\nJ[m + h] -J[m] =\n2⟨Fh, F[m] -d⟩+ O(∥h∥2)\n= ⟨h, F ∗(F[m] -d)⟩+ O(∥h∥2).\nWe conclude by invoking (A.1).\nWith some care, calculations involving functional derivatives are more\nefficiently done using the usual rules of calculus in Rn. For instance, the\nresult above is more concisely justified from\nδ\n⟨δm\n\n⟨F[m] -d, F[m] -d⟩\n, m1⟩= ⟨Fm1, F[m] -d⟩\n= ⟨F ∗(F[m] -d), m1⟩.\nThe reader may still wish to use a precise system for bookkeeping the various\nfree and dummy variables for longer calculations - see the appendix for such\na system.\nThe problem of computing F ∗will be completely addressed in the next\nchapter.\nThe Gauss-Newton iteration is Newton's method applied to J:\n\nm(k+1)\nJ\n=\n(k)\nδ2\nm\n-\nδm2[m(k)]\n-1 δJ\nδm[m(k)].\n(3.12)\nHere\n\nδ2J\n-1\n2[m(k)]\nis an operator: it is the inverse of the functional Hessian\nδm\nof J.\nAny iterative scheme based on a local descent direction may converge to\na wrong local minimum when J is nonconvex. Gradient descent typically\nconverges slowly - a significant impediment for large-scale problems. The\n\nCHAPTER 3. SCATTERING SERIES\nGauss-Newton iteration converges faster than gradient descent in the neigh-\nborhood of a local minimum, when the Hessian of J is (close to being) positive\nsemi-definite, but may otherwise result in wrong update directions. It is in\ngeneral much more complicated to set up Gauss-Newton than a gradient de-\nscent since the wave-equation Hessian is a large matrix, costly to store and\ncostly to invert. Good practical alternatives include quasi-Newton methods\nsuch as LBFGS, which attempt to partially invert the wave-equation Hessian.\n3.5\nExercises\n1. Repeat the development of section (3.1) in the frequency domain (ω)\nrather than in time.\n2. Derive Born series with a multiscale expansion: write m = m0 + εm1,\nu = u0 + εu1 + ε2u2 + . . ., substitute in the wave equation, and equate\nlike powers of ε. Find the first few equations for u0, u1, and u2.\n3. Write the Born series for the acoustic system, i.e., find the linearized\nequations that the first few terms obey. [Hint: repeat the reasoning of\nsection 3.1 for the acoustic system, or equivalently expand on the first\nfew three bullet points in section 3.2.]\n4. At the end of section 3.1 we found the equation that u1 obeys by\ndifferentiating (3.2) with respect to m. Now, differentiate (3.2) twice\nin two different directions m1, m′\n1 to find the equation for the Hessian\nδ2F\n, as a bilinear form of two functions m\nm′\n1 and\n. Check that (up\nδm1δm′\nto a factor 2) your answer reduces to the equation for u2 obtained in\nexercise 2 when m\n′\n1 = m1. The Hessian of F reappears in the next\nchapter as we describe accelerated descent methods for the inversion\nproblem.\nSolution. A first derivative with respect to m1 gives\nδm\nδm1\n∂2F(m)\n∂t2\n+\n\nm ∂2\nδF(m)\n-∆\n∂t2\nδm1\n= 0.\nThe notation\nδm means the linear form that takes a function m1 and\nδm1\nreturns the operator of multiplication by m1. We may also write it as\n\n3.5. EXERCISES\nthe identity Im1 \"expecting\" a trial function m1. A second derivative\nwith respect to m′\n1 gives\nδm\nδm1\n∂2\n∂t2\nδF(m)\nδm′\n+ δm\nδm′\n∂2\n∂t2\nδF(m)\nδm1\n+\n\nm ∂2\n∂t2 -∆\nδ2F(m)\nδ\n′ = .\nm1δm1\nWe now evaluate the result at the base point m = m0, and perform the\npairing with two trial functions m1 and m′\n1. Denote\nδ2F(m0)\nv = ⟨\nm\nδm1δm′\n1, m′\n1⟩.\nThen the equation for v is\n\n∂2\nm0∂t2 -∆\n\nv = -m1\n∂2u′\n∂t2 -m′\n∂2u1,\n∂t2\nwhere u1, u′\n1 are the respective linearized reflection fields generated by\nm\n′\n1, m1. In this formulation, the computation of v requires solving four\nwave equations, for v, u1, u′\n1, and u0 (which appears in the equations\nfor u1 and u′\n′\n1). Notice that v = 2u2 when m1 = m1.\n5. Compute δ2F\nolarization:\nδ\n2 in an alternative way by p\nfind the equations\nm\nfor the second-order field u2 when the respective model perturbations\nare m1 + m′\n1 and m1 -m′\n1, and take a combination of those two fields.\n6. Consider the setting of section 3.2 in the case M = I. No perturbation\nwill be needed for this exercise (no decomposition of M into M0+εM1).\nProve the following energy estimate for the solution of (3.8):\n\nt\nE(t) ≤\nˆ\n∥f∥(s) ds\n,\n(3.13)\nwhere E(t) = ⟨w, Mw⟩and ∥f∥2 = ⟨f, f⟩. [Hint: repeat and adapt the\nbeginning of the proof of theorem 3.]\n7. p\nConsider (3.8) and (3.9) in the special case when M0 = I. Let ∥w∥=\n⟨w, w⟩and ∥w∥∗= max0≤t≤T ∥w∥. In this exercise we show that\nw -w0 = O(ε), and that w -w0 -w1 = O(ε2).\n\nCHAPTER 3. SCATTERING SERIES\n(a) Find an equation for w -w0. Prove that\n∥w -w0∥∗≤ε ∥M1∥infΩT ∥w∥∗\n[Hint: repeat and adapt the proof of theorem 3.]\n(b) Find a similar inequality to control the time derivative of w -w0.\n(c) Find an equation for w -w0 -w1. Prove that\nw\nw\nw\n(ε M\nΩT)2\n∥\n-\n0 -\n1∥∗≤\n∥\n1∥inf\n∥w∥∗\n8. Consider the gradient descent method applied to the linear least-squares\nproblem minx ∥Ax -b∥2. Show that\nα = ∥A∗A∥\nis a safe choice in the sense that the resulting gradient step is a con-\ntraction, i.e., the distance between successive iterates decreases mono-\ntonically.\n9. Consider J(m) any smooth, locally convex function of m.\n(a) Show that the specific choice\n⟨δJ\nα =\nδm[m(k)], δJ\nδm[m(k)]⟩\n⟨δJ\nδm[m(k)], δJ2\nδm2[m(k)] δJ [m(k)]⟩\nδm\nfor the gradient descent method results from approximating J by\na quadratic function in the direction of δJ/δm, near m(k), and\nfinding the minimum of that quadratic function.\n(b) Show that the Gauss-Newton iteration (3.12) results from approx-\nimating J by a quadratic near m(k), and finding the minimum of\nthat quadratic function.\n10. Prove the following formula for the wave-equation Hessian\nδ2J\nin\nδm1δm′\nterms of F and its functional derivatives:\nδ2J\nδm1δm′\n= F ∗F + ⟨\nδ2F\n, F\nδm1δm′\n[m] -d⟩.\n(3.14)\n\n3.5. EXERCISES\nNote: F ∗F is called the normal operator.\nSolution. To compute Hessians, it is important to expand the notation\nto keep track of the different variables, i.e., we compute\nδ2J\n.\nδm1δm′\nA first\nderivative gives\nδJ\nδm1\n= ⟨δF(m), F(m) -d⟩,\nδm1\nwhere the inner product bears on F in each factor. A second derivative\ngives\nδ2J\nδm1δm′\n= ⟨δF(m)\nδm1\n, δF(m)\nδm′\n⟩+ ⟨δ2F(m) , F\nδm1δm′\n(m) -d⟩.\nThis result is then evaluated at the base point m = m0, where δF(m0) =\nδm1\nF. The second term in the right-hand side already has the desired\nform. The first term in the right-hand-side, when paired with m1 and\nm′\n1, gives\n⟨Fm1, Fm′\n1⟩= ⟨F ∗Fm1, m′\n1⟩,\nhence it can be seen as F ∗F, turned into a bilinear form by application\nto m\n′\n1 and inner product with m1. Notice that, if we pair the whole\nequation with m1 and m′\n1, and evaluate at m = m0, we arrive at the\nelegant expression.\nδ2J\n⟨\n′\n′ m1, m1⟩= ⟨u1, u′\n1⟩+ ⟨v, u0 -d⟩,\n(3.15)\nδm1δm1\nwhere v was defined in the solution of an earlier exercise as\nδ2F(m0)\nv = ⟨\nm\nδm1δm′\n1, m′\n1⟩.\n11. Show that the spectral radius of the Hessian operator δ2J ,\nδm2 when data\nare (essentially) limited by t ≤T and ω ≤Ω, is bounded by a constant\ntimes (ΩT)2.\n\nCHAPTER 3. SCATTERING SERIES\n\nChapter 4\nAdjoint-state methods\nAs explained in section (3.4), the adjoint F ∗of the linearized forward (mod-\neling) operator F plays an important role in the formula of the functional\ngradient δJ\nδm of the least-squares cost function J:\nδJ [m] = F ∗(F[m] -d).\nδm\nWhile F is the basic linear map from model space to data space, F ∗is\nthe basic linear map from data space to model space. F ∗is not only the\nbuilding block of iterative optimization schemes, but the mere application of\nF ∗to data is the simplest form of \"imaging\". For instance, when the initial\nguess m(0) = m0 is a smooth background model reasonably close to the true\nsolution m, and when there is a sufficiently large number of receivers and/or\nsources, the first iteration of gradient descent,\nm(1) = αF ∗(d -F[m0]),\noften gives a good \"image\" of the scatterers (somewhat close to the actual\nεm1). For this reason, F ∗is often called the imaging operator.\nIt should also be noted that F ∗behaves not entirely unlike F -1, i.e., F\nis somewhat close to being unitary. This statement does not have a rigorous\nformulation of the form ∥F ∗F -I∥≤(. . .), but rather of the form \"F ∗F\ndoes not move singularities around like F or F ∗do\". More details on the\nmicrolocal aspects of this question will be given in chapter 8.1.\nForming the full matrix F = δF and transposing it is not a practical way\nδm\nto compute F ∗. The adjoint-state method provides an elegant solution to\nthis problem, resulting in what is called the \"imaging condition\".\n\nCHAPTER 4. ADJOINT-STATE METHODS\n4.1\nThe imaging condition\nFor any dr(t) function of the receiver index r and time t, and m(x) function\nof position x (here m and d are two arbitrary functions, not necessarily linked\nto one another by the forward model), we have\n⟨d, Fm⟩= ⟨F ∗d, m⟩.\nThe inner product on the left is in data space,\nX\n⟨d, Fm⟩=\nˆ T\ndr(t)u(xr, t) dt,\nu = Fm,\nr\nwhile the inner product on the right is in model space.\n⟨F ∗d, m⟩=\nˆ\n(F ∗d)(x)m(x) dx.\nRn\nThe relation u = Fm is implicitly encoded by the two equations\n\n∂2\nm0∂t2 -∆\n\nu = -m∂2u0\n∂t2 ,\n\nm0\n∂2\n\n-∆\nu0 = f.\n∂t2\nNote that a single right-hand side generates u0, and that we have omitted the\nsource subscript s in this section; we will return to multiples sources shortly.\nP The argument that isolates and makes explicit the contribution of m in\nr\nT dr(t)u(xr, t) dt is one of integration by parts. In order to integrate by\nparts in x, we need to turn the sum over receivers into an integral. This\ncan be achieved by considering a distributional extended dataset where each\nmeasurement dr(t) is accompanied by a Dirac delta located at xr:\nX\ndext(x, t) =\ndr(t)δ(x -xr).\nr\nWe then have\nT\n⟨d, Fm⟩=\nˆ\nRn\nˆ\ndext(x, t)u(x, t) dxdt.\nIn\norder to use the wave equation for u, a copy of the differential operator\nm\n∂2\n0 ∂t2 -∆\n\nneeds to materialize. This is done by considering an auxiliary\n\n4.1. THE IMAGING CONDITION\nfield q(x, t) that solves the same wave equation with dext(x, t) as a right-hand\nside:\n\n∂2\nm0\n\n-∆\nq(x, t) = dext(x, t),\nx ∈\nRn,\n(4.1)\n∂t\nwith as-yet unspecified \"boundary conditions\" in time.\nSubstituting this\nexpression for dext(x, t), and integrating by parts both in space and in time\nreveals\nˆ ˆ T\n\n∂2\n⟨d, Fm⟩=\nq(x, t)\nm0\nV\n∂t2 -∆\n\nu(x, t) dxdt\n+\nˆ\nV\nm0\n∂q\n∂t u|T\n0 dx -\nˆ\nV\nm0q∂u|T\n∂t 0 dx\n+\nˆ\n∂V\nˆ T ∂q\n∂nu dSxdt -\nˆ\n∂V\nˆ T\nq∂u dSxdt,\n∂n\nwhere V is a volume that extends to the whole of Rn, and ∂V is the boundary\nof V -- the equality then holds in the limit of V = Rn.\nThe boundary terms over ∂V vanish in the limit of large V by virtue\nof the fact that they involve u - a wavefield created by localized functions\nf, m, u0 and which does not have time to travel arbitrarily far within a time\n[0, T]. The boundary terms at t = 0 vanish due to u|t=0 = ∂u|t=0 = 0. As for\n∂t\nthe boundary terms at t = T, they only vanish if we impose\n∂q\nq|t=T =\n|t=T = 0.\n(4.2)\n∂t\nSince we are only interested in the values of q(x, t) for 0 ≤t ≤T, the\nabove are final conditions rather than initial conditions, and the equation\n(4.1) is run backward in time.\nThe wavefield q is called adjoint field, or\nadjoint state. The equation (4.1) is itself called adjoint equation. Note that\nq is not in general the physical field run backward in time (because of the\nlimited sampling at the receivers), instead, it is introduced purely out of\ncomputational convenience.\nThe analysis needs to be modified when boundary conditions are present.\nTypically, homogeneous Dirichlet and Neumann boundary conditions should\nthe same for u0 and for q - a choice which will manifestly allow to cancel\nout the boundary terms in the reasoning above - but absorbing boundary\nconditions involving time derivatives need to be properly time-reversed as\nwell. A systematic, painless way of solving the adjoint problem is to follow\n\nCHAPTER 4. ADJOINT-STATE METHODS\nthe following sequence of three steps: (i) time-reverse the data dext at each\nreceiver, (ii) solve the wave equation forward in time with this new right-\nhand-side, and (iii) time-reverse the result at each point x.\nWe can now return to the simplification of the left-hand-side,\n⟨d, Fm⟩=\nˆ\nRn\nˆ T\n\n∂2\nq(x, t)\nm0\n∂t2 -∆\n\nu(x, t) dxdt\n= -\nˆ\nRn\nˆ T\nq(x, t)m(x)∂2u0 dxdt\n∂t2\nThis quantity is also supposed to be ⟨m, F ∗d⟩, regardless of m, so we conclude\n(F ∗\nd)(x) =\nˆ T\n∂2u\n-\nq(x, t)\ndt.\n(4.3)\n∂t2\nThis equation is called the imaging condition: it expresses the action of F ∗\non d as the succession of the following steps:\n1. Place data dr(t) at the location of the receivers with point masses to\nget dext;\n2. Use dext as the right-hand side in the adjoint wave equation to get the\nadjoint, backward field q;\n3. Simulate the incident, forward field u0; and finally\n4. Take the time integral of the product of the forward field u0 (differen-\ntiated twice in t), and the backward field q, for each x independently.\nThe result is a function of x which sometimes serves the purpose of image,\nand may sometimes be called Im(x). Note that we have not performed a\nfull inversion; if d are measured data, then Im is not the model m that gave\nrise to d. In seismology, the imaging condition (4.3) is called reverse-time\nmigration, or simply migration. In radar, the imaging condition does not have\na particular name, but in the next chapter we will encounter a simplification\nof (4.3) called backprojection.\nIf we now restore the presence of multiple sources, the wavefields u, u0,\nand u1 will depend on the source index s. The source term fs -- typically of\nthe form w(t)δ(x -xs) -- is in the right-hand side of the wave equations for\n\n4.1. THE IMAGING CONDITION\nu0 and u, while u1 implicitly depends on fs through u0. For a fixed source\ns, we denote\nus = Fs[m],\nu0,s = Fs[m0],\nu1,s = Fsm1,\nwhile we continue to denote u = F[m], u0 = F[m0] and u1 = Fm1 for the\ncollection of such wavefields over s.\nThe data inner-product now has an additional sum over s, namely\nX X\nT\n⟨d, Fm⟩=\nd\ns\nˆ\nr,s(t)us(xr, t) dt.\nr\nThe formula for F ∗can be obtained by taking adjoints one s at a time,\nnamely\nX\n⟨F ∗d, m⟩= ⟨d, Fm⟩=\n⟨ds, Fsm⟩\nX\ns\n=\n⟨F ∗\ns ds, m⟩\nX\ns\n= ⟨\nF ∗\ns ds, m⟩,\ns\nhence\nX\nF ∗=\nF ∗\ns .\ns\nMore explicitly, in terms of the imaging condition,\nX\n(F ∗\n,s\nd)(x) =\ns\nˆ T\n∂2u0\n-\nqs(x, t)\n(x, t) dt,\n(4.4)\n∂t2\nwhere the adjoint field qs is relative to the source s:\n\n∂2\nm0\n\n-∆\nqs(x, t) = dext,s(x, t).\n∂t2\nThe sum over s in the new imaging condition (4.4) is sometimes called a stack.\nIt is often the case that particular images F ∗\ns d are not very informative on\ntheir own, but a stack uses the redundancy in the data to bring out the\ninformation and reveal more details.\nThe mathematical tidbit underlying stacks is that the operation of creat-\ning a vector (x, x, . . . , x) out of a single number x has for adjoint the operation\nof summing the components of a vector.\n\nCHAPTER 4. ADJOINT-STATE METHODS\n4.2\nThe imaging condition in the frequency\ndomain\nWe now modify the exposition to express both the adjoint-state equation\nand the imaging condition in the frequency (ω) domain. The nugget in this\nsection is that complex conjugation in ω corresponds to time reversal. We\nassume a single source for simplicity.\nWe are again interested in finding F ∗such that ⟨d, Fm⟩= ⟨F ∗d, m⟩for\nall generic d and m. The data inner product ⟨d, Fm⟩can be expressed in the\nfrequency domain by means of the Parseval formula,\nX\n⟨d, Fm⟩= 2π\nr\nˆ\ndbr(ω)\nR\nX\n(\\\nFm)(xr, ω)dω =\nˆ\ndr(t)(Fm)(xr, t)dt.\nr\nThe complex conjugate is important, now that we are in the frequency do-\nmain - though since the overall quantity is real, it does not matter which\nof the two integrand's factors it is placed on. As previously, we pass to the\nextended dataset\nX\ndd(x, ω) =\ndb\next\nr(ω)δ(x -xr),\nr\nand turn the sum over r into an integral over x. The linearized scattered\nfield is\n(\\\nFm)(x , ω) =\nˆ\nGb\nr\n(x, y; ω)m(y)ω ub0(y, ω) dy.\n(4.5)\nTo simplify the resulting expression of ⟨d, Fm⟩, we let\nqb(x, ω) =\nˆ\nGb(y, x; ω)dd\next(y, ω) dy.\n(4.6)\n(Note that Green's functions are always symmetric under the swap of x and\ny, as we saw in a special case in one of the exercises in chapter 1.) It follows\nthat\n\n⟨d, Fm⟩=\nˆ\nm(y) 2π\nˆ\nqb(y, ω)ω2\nR\n\nub0(y, ω) dω\ndy,\nhence\nF ∗d(y) = 2π\nˆ\nqb(y, ω)ω2\nR\nbu0(y, ω) dω.\n(4.7)\n\nThe integral in t in (4.3) is over [0, T] because such is the support of\nq ∂2u0.\nThe integral in ω in (4.7) is over\n.\nIt is tempting to truncate\n∂t2\nR\nthis integral to \"the frequencies that have been measured\" -- but that is\nstrictly speaking incompatible with the limits on t (for the same reason that a\nfunction compactly supported in time cannot also be compactly supported in\nfrequency.) Careful consideration of cutoffs is needed to control the accuracy\nof a truncation in ω.\nEquation (4.7) is valuable for a few different reasons:\n- It can be further simplified to highlight its geometrical content as an\napproximate curvilinear integral, such as explained in the next chapter;\n- The integral over ω can be deliberately restricted in the scope of descent\niterations, so as to create sweeps over frequencies. This is sometimes\nimportant to deal with the lack of convexity of full inversion; see chapter\n9.\n4.3\nThe general adjoint-state method\nIn this section we explain how to use the adjoint-state method to compute the\nfirst and second variations of an objective function J[u(m)] in a parameter\nm, when u is constrained by the equation L(m)u = f, where L(m) is a linear\noperator that depends on m.\n1Time reversal of any real-valued function becomes conjugation in the Fourier domain:\nbf(ω) =\nˆ\neiωtf(t) dt =\nˆ\ne-iωtf(t)dt =\nˆ\neiωtf(-t) dt.\ninterpret bq = Gdext by applying an extra conjugate, namely bq = Gdext, which\ncan be read as the sequence of operations: (i) time-reverse dext, (ii) propa-\ngate it forward in time, and (iii) time-reverse the result. This is the same\nprescription as in the time-domain case, and offers the added advantage of\nnot having to rethink the boundary condition for the backward equation.\n4.3. THE GENERAL ADJOINT-STATE METHOD\nThis equation is the same as (4.3), by Parseval's identity. Equation (4.6) is\nthe integral version of (4.1) in the frequency domain. The complex conju-\ngation of Gb in (4.6) is the expression in the frequency domain of the fact\nthat the adjoint equation is solved backwards in time1. We can alternatively\n\nCHAPTER 4. ADJOINT-STATE METHODS\nWe refer to \"u space\", \"m space\", and \"f space\" for the respective L2\nspaces containing u, m, and f. The first variation of J is simply\nδJ\nδm = ⟨δJ\nδu, δu ⟩u,\n(4.8)\nδm\nwhere the inner product pairs δu in each equation, hence acts in u space.\nNotice that δu/δm is an operator acting in m space and returning a function in\nu space2.\nIf we were interested in computing the directional derivative of J in some\ndirection m, namely ⟨δJ\nδm, m⟩, then we would simply swap the m-space inner\nproduct with the u-space inner product from (4.8), and recognize that u =\nδu m is easily accessible by solving the linearized version of the equation\nδm\nL(m)u = f. This result is straightforward to obtain, and the adjoint-state\nmethod is not necessary.\nThe interesting problem is that of computing\nδJ as a function in m-\nδm\nspace.\nIn principle, equation (4.8) is all that is needed for this purpose,\nexcept that explicitly computing the full kernel of the operator\nδu\nδm can be\nhighly inefficient in comparison to the complexity of specifying the function\nδJ .\nδm\nThe adjoint-state method is a very good way of eliminating\nδu\nδm so that\nδJ can be computed in more favorable complexity. In order to achieve this,\nδm\ndifferentiate the \"state equation\" L(m)u = f with respect to m to get\nδL\nδmu + L δu = 0.\nδm\nWe see that\nδu can be eliminated by composition with L on the left. The\nδm\nmain idea of the adjoint-state method is that a copy of L can materialize\nin (4.8) provided the other factor, δJ , is seen as the adjoint of L applied to\nδu\nsome field q,\nL∗\nδJ\nq =\n,\n(adjoint-state equation)\n(4.9)\nδu\nwith q naturally called the adjoint field. Then,\nδJ\nδm = ⟨L∗q, δu\nδm⟩u = ⟨q, L δu ⟩f\n(4.10)\nδm\nδL\n= -⟨q, δmu⟩f\n(imaging condition)\n2It relates to what we called F = δF/δm earlier by the operator S of sampling at the\nreceivers, via F = Su or δF/δm = Sδu/δm.\n\n4.3. THE GENERAL ADJOINT-STATE METHOD\nThis latter expression is often much easier to compute than (4.8).\nExample 1. In the setting of section 4.1, m is a function of x; u and f\nare functions of (x, t). The state equation L(m)u = f is the forward wave\nequation m∂2\nt u -∆u = f with zero initial conditions. When we evaluate all\nour quantites at m0, then u becomes u0, the incident field. The adjoint-state\nequation L∗q =\nδJ\nδu is the backward wave equation m∂2\nt q -∆q =\nδJ\nδu with\nzero final conditions. The least-squares cost function is J[u] = 1∥Su -d∥2\nwith S the operator of sampling at the receivers, so that the adjoint source\nδJ = S∗(Su -d) is the data residual extended to a function of x and t.\nδu\nThe quantity\nδL u is a multiplication operator from m-space to f-space\nδm\n(which takes the function m to the function m∂2\nt u), expressed in coordinates\nas\nδL\n(δm(y)u)(x, t) = δ(x -y) ∂2\nt u(x, t),\n(x ∈R3, y ∈R3.)\nUsing the formula above, -⟨q, δL u⟩f becomes the usual imaging condition\nδm\n-\n\nq(x, t)∂2\nt u(x, t) dt.\nThe adjoint-state method also allows to compute second variations. We\nreadily compute the functional Hessian of J as\nδ2J\nδmδm′ = ⟨δu\nδm, δ2J\nδuδu′\nδu′\nδm′⟩u + ⟨δJ\nδu,\nδ2u\n⟩u,\nδmδm′\nδ2J\n⟨u, δuδu′u′⟩u + ⟨δJ\nδu, v⟩u.\nThe three functions u =\nδu\nδmm, u′ =\nδu′\nm′\nδm′\n, and v = ⟨m,\nδ u m′\nδmδm′\n⟩can be\ncomputed by solving simple (linearized) equations derived from the state\nequation L(m)u = f. (See a few exercises at the end of this chapter.) The\n3Here m and m′ are functions, hence can be seen as independent variables, i.e., points,\nin a function space. Free, or unpaired variables are to functional calculus what free indices\nare to vector calculus: they are \"open slots\" available to be filled by pairing with a\nfunction/vector in an inner product.\nwhere the u-space inner product pairs the δu factors in each expression.\nThis Hessian is an object with two free variables3 m and m′. If we wish\nto view it as a bilinear form, and compute its action ⟨m,\nδ2J m′\nδmδm′\n⟩m on two\nfunctions m and m′, then it suffices to pass those functions inside the u-space\ninner product to get\n\nCHAPTER 4. ADJOINT-STATE METHODS\nadjoint-state method is not needed for evaluating the Hessian as a bilinear\nform.\nOn the other hand, we are generally not interested in computing the full\nmatrix representation of\nδ2J\n′\n′ with row index m and column index m either:\nδmδm\nthis object is too large to store.\nThe most interesting question is the computation of the action of the\nHessian as an operator on a function, say m′. While m′ is paired, the variable\nm remains free, hence the result is a function in m-space. This problem can\nbe solved by the second-order adjoint-state method, which we now explain.\nAs earlier, we can pass m′ inside the u-space inner product to get\nδ2J\nδmδm′m′ = ⟨δu\nδm, δ2J\nδuδu′u′⟩+ ⟨δJ\nδu,\nδ2u\nm′⟩,\n(4.11)\nδmδm′\nwith u′ = δu′ m′\nδm′\neasily computed. However, two quantities are not immedi-\nately accessible:\n1. the remaining δu\nδm factor with the un-paired m variable, and\n2. the second-order\nδ2u m′\nδmδm′\nfactor.\nBoth quantities have two free variables u and m, hence are too large to store,\nlet alone compute.\nThe second-order factor can be handled as follows. The second variation\nof the equation L(m)u = f is\nδ2L\nδmδm′u + δL\nδm\nδu\nδm′ + δL\nδm′\nδu\nδm + L\nδ2u\n= 0.\nδmδm′\nWe see that the factor\nδ2u\napplied\nδ\n′ can be eliminated provided L is\nto it\nmδm\non the left. We follow the same prescription as in the first-order case, and\ndefine a first adjoint field q1 such that\n∗\nδJ\nL q1 =\n.\n(1st adjoint-state equation)\n(4.12)\nδu\nA substitution in (4.11) reveals\nδ2J\nδmδm′m′ = ⟨δu\nδm, δ2J\nδuδu′u′⟩u -⟨q1,\nδL\nδm′m′\nδu ⟩f\nδm\nδ2L\n-⟨q1,\nδmδm′m′\n\nu + δL\nδmu′⟩f,\n\n4.3. THE GENERAL ADJOINT-STATE METHOD\nwith u′ =\nδu\n′\n′m , as earlier. The term on the last row can be computed as is;\nδm\nall the quantities it involves are accessible. The two terms in the right-hand-\nside of the first row can be rewritten to isolate δu/δm, as\nδ2J\n⟨δuδu′u′ -\nδL\nδm′m′\n∗\nq1, δu\nδm⟩u.\nIn order to eliminate\nδu by composing it with L on the left, we are led to\nδm\ndefining a second adjoint-state field q2 via\nL∗\nδ2J\nq2 = δuδu′u′ -\nδL\n∗\nm′\nq1.\n(2nd adjoint-state equation)\n(4.13)\nδm′\nAll the quantities in the right-hand side are available. It follows that\n⟨L∗\nδu\nq2, δm⟩u = ⟨q2, L δu\nδm⟩= -⟨q2, δL u⟩f.\nδm\nGathering all the terms, we have\nδ2J\nδmδm′m′ = -⟨q2, δL\nδmu⟩f -⟨q1,\nδ2L\nδmδm′m′\n\nu + δL u′⟩f,\nδm\nwith q1 obeying (4.12) and q2 obeying (4.13).\nExample 2. In the setting of section 4.1, call the base model m = m0 and the\nmodel perturbation m′ = m1, so that u′ = δu\nδm[m0]m′ is the solution u1 of the\nlinearized wave equation m0∂2\nt u1 -∆u1 = -m1∂2\nt u0 with u0 = L(m0)u0 = f\nas previously. The first variation\nδL is the same as explained earlier, while\nδm\nthe second variation\nδ2L\nδmδm′ vanishes since the wave equation is linear in m.\nThus if we let H =\nδ2J\n-sp\nδ\n′ for the Hessian of J as an operator in m\nace, we\nmδm\nget\nHm1 = -\nˆ\nq2(x, t)∂2\nt u0(x, t) dt -\nˆ\nq1(x, t)∂2\nt u1(x, t) dt.\nThe first adjoint field is the same as in the previous example, namely q1 solves\nthe backward wave equation\nm0∂2\nt q1 -∆q\n∗\n1 = S (Su0 -d),\nzero f.c.\nTo get the equation for q , notice that\nδ2J\nδuδu′ = S∗S and\nδL\nδm′m1\n∗=\nδL m\nδm′\n1 =\nm1∂2\nt . Hence q2 solves the backward wave equation\nm0∂2\nt q2 -∆q2 = S∗Su1 -m1∂2\nt q1,\nzero f.c.\n\nCHAPTER 4. ADJOINT-STATE METHODS\nNote that the formulas (3.14) (3.15) for H that we derived in an ex-\nercise in the previous chapter are still valid, but they do not directly allow\nthe computation of H as an operator acting in m-space. The approximation\nH ≃F ∗F obtained by neglecting the last term in (3.14) is recovered in the\ncontext of second-order adjoints by letting q1 = 0.\n4.4\nThe adjoint state as a Lagrange multi-\nplier\nThe adjoint field q was introduced in a somewhat opportunistic and artificial\nway in earlier sections. In this section, we show that it has the interpretation\nof a Lagrange multiplier in a constrained optimization framework, where the\nwave equation serves as the constraint. The problem is the same as in the\nprevious section, namely to compute the functional gradient of J[u(m)] with\nrespect to m - and the resulting expression is the same as previously - but\nwe show how the Lagrangian construction offers a new way of interpreting\nit.\nInstead of considering J[u(m)] as a functional to minimize on m, we now\nview J as a function of u only, and accommodate the constraint L(m)u = f\nby pairing it with a function q in f-space in order to form the so-called\nLagrangian4\nL[u, m, q] = J[u] -⟨q, L(m)u -f⟩f.\nThe function q is called Lagrange multiplier in this context, and is arbitrary\nfor the time being. Notice that L[u(m), m, q] = J[u(m)] regardless of q when\nu = u(m), i.e., when the constraint is satisfied.\nThis expression can be\ndifferentiated to give the desired quantity, namely\nd\nδL\nJ[u(m)] = ⟨\ndm\nδu , δu\nδm⟩+ δL .\n(4.14)\nδm\nThe partials of L are computed as\n-\nδL\nδu = δJ -L∗q, since ⟨q, Lu⟩f = ⟨L∗q, u⟩u,\nδu\n-\nδL\nδm = -⟨q, δL\nδmu⟩f,\n-\nδL = L(m)u -f.\nδq\n4We reserve the letter L for the Lagrangian and L for the state equation.\n\n4.5. EXERCISES\nIn convex optimization, the traditional role of the Lagrangian is that putting\nto zero its partials5 is convenient way of deriving the optimality conditions\nthat hold at critical points, both for the primal and the dual problems. In\nparticular, if the partials of L are zero at (u, m, q), then m is a critical point\nof J[u(m)].\nThe way we make use of the Lagrangian in this section is different, because\nwe aim to derive the expression of\nd\ndmJ[u(m)] away from critical points. In\nparticular, we are not going to require δL\nδm to be zero. Still, we find it advan-\ntageous to put δL = 0 as a convenient choice to help simplify the expression\nδu\nof the gradient of J. Indeed, in that case we recover the imaging condition\n(4.10) from\nd\ndmJ[u(m)] = δL\nδm = -⟨q, δL u⟩f.\nδm\nIt is always possible to achieve δL = 0, by defining q to be the solution of\nδu\nL∗q = δJ , the adjoint-state equation (4.9). Note that putting δL\nδu\n= 0 recovers\nδq\nthe state equation L(m)u = f.\n4.5\nExercises\n1. Starting from an initial guess model m0, a known source function f,\nand further assuming that the Born approximation is valid, explain\nhow the inverse problem d = F[m] can be completely solved by means\nof F -1, the inverse of the linearized forward operator (provided F is\ninvertible). The intermediate step consisting in inverting F is called\nthe linearized inverse problem.\nSolution. Form the incident field as u0 = Gf. Subtract from observed\ndata to get d -u0. Since the Born approximation is assumed valid, we\nhave d -u0 ≃εu1. Invert for m1 by solving the system u1 = Fm1, i.e.,\nm1 = F -1u1. Then form m = m0 + εm1.\n2. Consider the forward wave equation for u0 in one spatial dimension\nwith an absorbing boundary condition of the form ( 1 ∂t -∂x)u(0) = 0\nc(0)\nat the left endpoint x = 0 of the interval [0, 1]. Assume that c(x) is\nlocally uniform and equal to c(0) in a neighborhood of x = 0.\n5Or letting 0 be a subgradient of L in the non-smooth case.\n\nCHAPTER 4. ADJOINT-STATE METHODS\n(a) Argue why this choice of boundary condition accommodates left-\ngoing waves, but not right-going waves.\n(b) Find the corresponding boundary condition for the adjoint-state\nequation on the backwards field q.\n3. Snapshot migration. The treatment of reverse-time migration seen ear-\nlier involves data u(xr, t) for an interval in time t, and at fixed receiver\npoints xr. Consider instead the snapshot setup, where t is fixed, and\nthere are receivers everywhere in the domain of interest. (So we have\nfull knowledge of the wavefield at some time t.) Repeat the analysis\nof the imaging operator, adjoint to the forward operator that forms\nsnapshot data from singly scattered waves. In particular, find what\nthe adjoint-state wave equation becomes in this case. [Hint: it involves\nnonzero final data, and a zero right-hand side.]\n4. Sampling. Call S the linear operator that maps a function f(x) to the\nvector of point samples {f(xr)}r. Find a formula for S∗. Note that\nwhen the linearized forward model F has S as its last operation, then\nthe imaging operator F ∗has S∗as its first operation. The presence of\nS∗explains why we passed from dr(t) to dext(x, t) in the first step of\nthe derivation of the imaging operator.\n5. Repeat the general adjoint-state theory by assuming a possibly nonlin-\near state equation of the form L(m, u) = f.\n\nChapter 5\nSynthetic-aperture radar\nThe object of synthetic aperture radar imaging (SAR) is to infer reflectivity\nprofiles from measurement of scattered electromagnetic waves.\nThe word\n\"aperture\" refers to the perceived angular resolution from the viewpoint of\nthe sensor (antenna). The expression \"synthetic aperture\" refers to the fact\nthat the aperture is created not from a very directional antenna, or array\nof antennas (as in ultrasound), but results from a computational process of\ntriangulation, implicit in the handling of data with a backprojection formula.\nThe goal of the chapter is to gain an understanding of the geometry\nunderlying the operators F and F ∗arising in SAR. Our reference for this\nchapter is the book \"Fundamentals of radar imaging\" by Cheney and Borden.\n5.1\nAssumptions and vocabulary\nWe will make the following basic assumptions:\n(!)\n1. Scalar fields obeying the wave equation, rather than vector fields obey-\ning Maxwell's equation. This disregards polarization (though process-\ning polarization is a sometimes a simple process of addition of images.)\nThe reflectivity of the scatterers is then encoded via m(x) as usual,\nrather than by specifying the shape of the boundary ∂Ωand the type\nof boundary conditions for the exterior Maxwell problem.\n2. The Born approximation, so that data d are proportional to εu1, and\nu1 = Fm1. This disregards multiple scattering. In the sequel we will\nwrite ε = 1 for simplicity.\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\n3. No dispersion, so that all waves travel at the same speed regardless of\nfrequency, as in the wave equation. Dispersion happens for radio waves\nin the ionosphere.\n4. The reflectivity m(x) = m0(x) + εm1(x) is constant in time, with m0\nconstant in time and space.\nThis disregards moving scatterers.\nAs\nmentioned earlier, we put ε = 1. For convenience, we will also drop\nthe subscript 1 from m1, so that in this chapter, m stands for the\nperturbation in squared slowness 1/c2.\nA few other \"working\" assumptions are occasionally made for conve-\n(!)\nnience, but can easily be removed if necessary:\n5. The far field assumption: spherical wavefronts are assumed to be locally\nplanar, for waves at the scatterer originating from the antenna (or vice-\nversa).\n6. Monostatic SAR: the same antenna is used for transmission and re-\nception. It is not difficult to treat the bistatic/multistatic case where\ndifferent antennas play different roles.\n7. Start-stop approximation: in the time it takes for the pulse to travel\nback and forth from the antenna to the scatterers, the antenna is as-\nsumed not to have moved.\n8. Flat topography: the scatterers are located at elevation z = 0.\nSAR typically operates with radio waves or microwaves, with wavelengths\non the order of meters to centimeters. Moving antennas are typically carried\nby planes or satellites. A variant of SAR is to use arrays of fixed antennas,\na situation called MIMO (multiple input, multiple output.) If the frequency\nband is of the form [ω0-∆ω/2, ω0+∆ω/2], we say ω0 is the carrier frequency\nand ∆ω is the bandwidth. We speak of wideband acquisition when ∆ω is a\nlarge fraction of ω0. As usual, ω = 2πν where ν is in Hertz.\nThe direction parallel to the trajectory of the antenna is called along-\ntrack. The vector from the antenna to the scatterer is called range vector,\nits direction is the range direction, and the direction perpendicular to the\nrange direction is called cross-range. The distance from the antenna to the\nscatterer is also called range. The length of the horizontal projection of the\nrange vector is the downrange.\n\n5.2. FORWARD MODEL\nWe will not deal with the very interesting topic of Doppler imaging, where\nfrequency shifts are used to infer velocities of scatterers. We will also not (!)\ncover the important topic of interferometric SAR (InSAR) where the objec-\ntive is to create difference images from time-lapse datasets.\nWe finish this section by describing the nature of the far-field approxima-\ntion in more details, and its consequence for the expression of the Green's\nfunction eik|x-y|\n| -|. Consider an antenna located near the origin. We will as-\n4π x\ny\nsume that a scatterer at x is \"far\" from a point y on the antenna in the sense\nthat\n|y| ≪|x|,\nk|y|2 ≪|x|.\nThen, if we let xb =\nx\n|x|,\n|x -y| =\np\n|x|2 -2x · y + |y|2\n= |x|\ns\nxb · y\n1 -2 |x| + |y|2\n|x|2\n≃|x|\n\n1 -bx · y\n|x| + 1\n|y|2\n\n+ . . .\n|x|2\n= |x| -xb · y + 2\n|y|2\n|x| + . . .\nWe therefore have\neik|x-y| = eik|x|e-ikbx·y\n\n1 + O\nk|y|2\n,\n|x|\n|x -y| = 1\n|x|\n\n1 + O\n|y|\n.\n|x|\nAs a result, in the far field,\neik|x-y|\neik|x|\n≃\n4π|x -y|\nb\ne-ikx·y.\n4π|x|\nThis simplification will cause the y integrals to become Fourier transforms.\n5.2\nForward model\nWe can now inspect the radiation field created by the antenna at the trans-\nmission side. The ≃sign will be dropped for =, although it is understood\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\nthat the approximation is only accurate in the far field. Call j(x, ω) the\nscalar analogue of the vector forcing generated by currents at the antenna,\ncalled current density vector. (The dependence on ω is secondary.) Call pb(ω)\nthe Fourier transform of the user-specified pulse p(t). Then\neik|x|\nub0(x, ω) =\nˆ\n4π|x|e-ikbx·yj(y, ω)bp(ω) dy.\nThis reduces to a spatial Fourier transform of j in its first argument,\nbu0(x, ω) = eik|x|\nbj(1)(kx,\nb ω)pb(ω).\n4π|x|\nFor short, we let\nJ(x,\nb ω) = bj(1)(kx,\nb ω),\nand call it the radiation beam pattern. It is determined by the shape of the\nantenna. As a function of xb, the radiation beam pattern is often quite broad\n(not concentrated).\nFor an antenna centered at position γ(s), parametrized by s (called slow\ntime), the radiation field is therefore\neik|x-γ(s)|\nud\n0,s(x, ω) =\nJ(x\\\n-γ(s), ω)pb(ω).\n4π|x -γ(s)|\nThe scattered field u1(x, ω) is not directly observed. Instead, the recorded\ndata are the linear functionals\ndb(s, ω) =\nˆ\nu1(y, ω)w(y, ω) dy\nAs\nagainst some window function w(x, ω), and where the integral is over the\nantenna As centered at γ(s).\nRecall that u1 obeys (4.5), hence (with m\nstanding for what we used to call m1)\ndb\nA\nˆ\n|\n(s, ω) =\nˆ\neik|x-y\ns\nω2ub0(x, ω)m(x)w(y, ω) dydx.\n4π|x -y|\nIn the regime of the the far-field approximation for an antenna at γ(s), we\nget instead (still using an equality sign)\ne\ndb(s, ω) =\nˆ\nik|x-γ(s)|\n4π|x -γ(s)|ω2 bu0(x, ω)m(x) bw(1)(k( \\\nx -γ(s)), ω).\n\n5.2. FORWARD MODEL\nThe start-stop approximation results in the same γ(s) used at transmission\nand at reception. For short, we let\nW(x,\nb ω) = wb(1)(kx,\nb ω),\nand call it the reception beam pattern. For a perfectly conducting antenna,\nthe two beam patterns are equal by reciprocity:\n($)\nJ(x,\nb ω) = W(x,\nb ω).\nWe can now carry through the substitutions and obtain the expression of the\nlinearized forward model F:\ndb(s, ω) = Fdm(s, ω) =\nˆ\ne2ik|x-γ(s)|A(x, s, ω)m(x) dx,\n(5.1)\nwith amplitude\nJ\n-\n(x\\\nγ(s), ω)W(x\\\n-γ(s), ω)\nA(x, s, ω) = ω pb(ω)\n.\n16π2|x -γ(s)|2\nSo far we have assumed that x = (x1, x2, x3), and that dx a volume\nelement. We could alternatively assume a two-dimensional reflectivity profile\nat a known elevation x3 = h(x1, x2). In that case we write\nxT = (x1, x2, h(x1, x2)),\nassume a reflectivity of the form m(x) = δ(x3 -h(x1, x2))V (x1, x2), and get (!)\ndb(s, ω) =\nˆ\ne2ik|xT -γ(s)|A(xT, s, ω)V (x1, x2) dx1dx2.\nIn the sequel we assume h = 0 for simplicity. We also abuse notations slightly (!)\nand write A(x, s, ω) for the amplitude.\nThe geometry of the formula for F is apparent if we return to the time\nvariable. For illustration, reduce A(x, s, ω) = ω2pb(ω) to its leading ω depen-\ndence. Then\nd(s, t) = 2π\nˆ\ne-iωt bd(s, ω) dω\n= -1\n\np\n2π\nˆ\n′′\n|x -γ(s)|\nt -2\nc0\n\nm(x) dx.\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\nWe have used the fact that k = ω/c0 to help reduce the phase to the simple\nexpression\n|x -γ(s)|\nt -2\nc\nIts physical significance is clear: the time taken for the waves to travel to\nthe scatterer and back is twice the distance |x -γ(s)| divided by the light\nspeed c0. Further assuming p(t) = δ(t), then there will be signal in the data\nd(s, t) only at a time t = 2|x-γ(s)| compatible with the kinematics of wave\nc\npropagation. The locus of possible scatterers giving rise to data d(s, t) is then\na sphere of radius ct/2, centered at the antenna γ(s). It is a good exercise\nto modify these conclusions in case p(t) is a narrow pulse (oscillatory bump)\nsupported near t = 0, or even when the amplitude is returned to its original\nform with beam patterns.\nIn SAR, s is called slow time, t is the fast time, and as we mentioned\nearlier, |x -γ(s)| is called range.\n5.3\nFiltered backprojection\nIn the setting of the assumptions of section 5.1, the imaging operator F ∗ is\ncalled backprojection in SAR. Consider the data inner product1\n⟨d, Fm⟩=\nˆ\ndb(s, ω)Fdm(s, ω) dsdω.\nAs usual, we wish to isolate the dependence on m to identify ⟨d, Fm⟩as\n⟨F ∗d, m⟩. After using (5.1), we get\n⟨d, Fm⟩=\nˆ\nm(x)\nˆˆ\ne-2ik|x-γ(s)|A(x, s, ω)db(s, ω) dsdω dx.\nThis means that\n(F ∗d)(x) =\nˆˆ\ne-2ik|x-γ(s)|A(x, s, ω)db(s, ω) dsdω.\n(5.2)\nNotice that the kernel of F ∗is the conjugate of that of F, and that the\nintegration is over the data variables (s, ω) rather than the model variable x.\n1It could be handy to introduce a multiplicative factor 2π in case the Parseval identity\nwere to be used later.\n\n5.3. FILTERED BACKPROJECTION\nThe physical interpretation is clear if we pass to the t variable, by using\ndb(s, ω) =\n\neiωtd(s, t) dt in (5.2). Again, assume A(x, s, ω) = ω2pb(ω). We\nthen have\n(F ∗d)(x) = -2π\nˆ\np′′\n\nt -2|x -γ(s)|\nd(s, t) dsdt.\nc0\nAssume for the moment that p(t) = δ(t); then F ∗places a contribution to\nthe reflectivity at x if and only if there is signal in the data d(s, t) for s, t, x\nlinked by the same kinematic relation as earlier, namely t = 2|x-γ(s)|. In other\nc\nwords, it \"spreads\" the data d(s, t) along a sphere of radius ct/2, centered at\nγ(s), and adds up those contributions over s and t. In practice p is a narrow\npulse, not a delta, hence those spheres become thin shells. Strictly speaking,\n\"backprojection\" refers to the amplitude-free formulation A = constant, i.e.,\nin the case when p′′(t) = δ(t). But we will use the word quite liberally, and\nstill refer to the more general formula (5.2) as backprojection. So do many\nreferences in the literature.\nBackprojection can also be written in the case when the reflectivity pro-\nfile is located at elevation h(x1, x2). It suffices to evaluate (5.2) at xT =\n(x1, x2, h(x1, x2)).\nWe now turn to the problem of modifying backprojection to give a formula\napproximating F -1 rather than F ∗. Hence the name filtered backprojection.\nIt will only be an approximation of F -1 because of sampling issues, as we\nwill see.\nThe phase -2ik|x - γ(s)| needs no modification: it is already \"kinemat-\nically correct\" (for deep reasons that will be expanded on at length in the\nchapter on microlocal analysis). Only the amplitude needs to be changed, to\nyield a new operator2 B to replace F ∗:\n(Bd)(x) =\nˆˆ\ne-2ik|x-γ(s)|Q(x, s, ω)db(s, ω) dsdω.\nBy composing B with F, we obtain\n(BFm)(x) =\nˆ\nK(x, y)m(y) dy,\n2B for filtered Backprojection, or for Gregory Beylkin, who was the first to propose it\nin 1984.\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\nwith\nK(x, y) =\nˆˆ\ne-2ik|x-γ(s)|+2ik|y-γ(s)|Q(x, s, ω)A(y, s, ω) dsdω.\n(5.3)\nM\nThe integral runs over the so-called data manifold M. We wish to choose Q\nso that BF is as close to the identity as possible, i.e.,\nK(x, y) ≃δ(x -y).\nThis can be done by reducing the oscillatory integral in (5.3) to an integral\nof the form\n,\n(2\nˆ\nei(x-y)·ξ dξ\nπ)2\nwhich, as we know, equals δ(y -x) if the integral is taken over R2. The\nintegral will turn out to be over a bounded set, the characterization of which\nis linked to the question of resolution as explained in the next section, but\nthe heuristic that we want to approach δ(y -x) remains relevant.\nAs the integral in (5.3) is in data space (s, ω), we define ξ ∈R2 as the\nresult of an as-yet undetermined change of variables\n(s, ω) 7→ξ = Ξ(s, ω; x).\n(ξ is xi, Ξ is capital xi.) The additional dependence on x indicates that the\nchange of variables will be different for each x.\nTo find Ξ, we need to introduce some notations.\nWe follow Borden-\nCheney [?] closely. Denote the range vector by\nRy,s = γ(s) -yT\nFor reference, its partials are\n∂Ry,s = γ (s),\n∂s\n\n∇yRy,s = -0\n1= -P2.\nWe understand both R\n,s\ny\nand ∂Ry\n,s\na column 3-vectors in a matrix context.\n∂s\nThe modification to deal with a nonzero elevation h(x1, x2) in xT is simple.\nThen\n∂\n∂s|Ry,s| =\n∂Ry,s\n∂s\nT Ry,s = γ (s) · Rd\ny,s,\n|Ry,s|\n\n5.3. FILTERED BACKPROJECTION\n∂Ry,s\n∇y|Ry,s| =\n∂y\nT Ry,s = P T d,s\nRy,s|\n2 Ry ,\n|\nwhere Rd\ny,s is the unit range vector.\nThe operation of pre-multiplying a\ncolumn 3-vector by P T\n2 corresponds to extraction of the first two components\nof the vector. (Recall that x and y are coordinates in two dimensions, while\ntheir physical realizations xT and yT have a zero third component.)\nWith the partial derivatives in hand we can now apply the principle of\nstationary phase (see appendix C) to the integral (5.3). The coordinates x\nand y are fixed when considering the phase\nφ(s, ω) = 2k(|Ry,s| -|Rx.s|).\nWe can introduce a large parameter α in the phase by normalizing frequencies\nas ω = αω′ (recall k = ω/c); the higher the frequency band of the pulse the\nbetter the approximation from stationary phase asymptotics. The critical\npoints occur when\n∂φ\n∂ω = 2(|Ry,s| -|Rx.s|) = 0,\nc\n∂φ = 2k γ (s) · (Rd\ny,s -Rd\nx,s) = 0.\n∂s\nThe Hessian matrix is singular, which seemingly precludes a direct applica-\ntion of lemma 5 in appendix 4, but the second example following the lemma\nshows the trick needed to remedy the situation: use a trial function f(y) and\nextend the integration variables to also include y. Henceforth we denote the\nphase as φ(s, ω, y) to stress the extra dependence on y.\nThe critical points occur when 1) the ranges are equal, and 2) the down-\nrange velocities are equal. For fixed x, the first condition can be visualized\nin three-dimensional yT-space as a sphere centered about γ(s), and passing\nthrough xT. The second condition corresponds to a cone with symmetry\n\naxis along the tangent vector γ(s) to the trajectory, and with the precise\nopening angle that ensures that xT belongs to the cone. Thirdly, we have\nyT = (y1, y2, 0), so an additional intersection with the horizontal plane z = 0\nshould be taken. The intersection of the sphere, the cone, and the plane,\nconsists of two points: yT = xT, and yT = xT,mirr, the mirror image of xT\nabout the local flight plane (the vertical plane containing γ (s)). In practice,\nthe antenna beam pattern \"looks to one side\", so that A(x, s, ω) ≃0 for x on\nthe \"uninteresting\" side of the flight path, therefore the presence of xT,mirr\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\ncan be ignored. (If not, the consequence would be that SAR images would\nbe symmetric about the flight plane.)\nWith the critical point essentially unique and at y = x, we can invoke\nstationary phase to claim that the main contribution to the integral is due\n(!)\nto points y near x. This allows to simplify the integral (5.3) in two ways: 1)\nthe amplitude A(y, s, ω) is smooth enough in y that we can approximate it\nby A(x, s, ω), and 2) the phase factor can be approximated as locally linear\nin y -x, as\nφ(s, ω, y) = 2k(|Ry,s| -|Rx.s|) ≃(y -x) · ξ.\nA multivariable Taylor expansion reveals that ξ can be chosen as the y-\ngradient of the phase, evaluated at x:\nξ = Ξ(x, ω; x) = ∇\nT d\nyφ(s, ω, y)|y=x = 2kP2 Rx,s.\nWe have therefore reduced the expression of K(y, x) to\nK(x, y) ≃\nˆ\nei(y-x)·Ξ(s,ω;x)Q(x, s, ω)A(x, s, ω) dsdω.\nM\nChanging from (s, ω) to ξ variables, and with a reasonable abuse of notation\nin the arguments of the amplitudes, we get\nK(x, y) ≃\nˆ\nei(y-x)·ξ\n∂(s, ω)\nQ(x, ξ)A(x, ξ) |\n∂ξ\n| dξ.\nThe Jacobian J = |∂(s,ω)| of the change of variables goes by the name Beylkin\n∂ξ\ndeterminant.\nThe proper choise of Q that will make this integral close to\n\nei(y-x)·ξ dξ\nis now clear: we should take\nQ(x, ξ) =\nA(x, ξ)|∂(s,ω) χ(x, ξ),\n(5.4)\n|\n∂ξ\n(!)\nfor some adequate cutoffχ(x, ξ) to prevent division by small numbers. The\npresence of χ owes partly to the fact that A can be small, but also partly\n(and mostly) to the fact that the data variables (s, ω) are limited to the data\nmanifold M. The image of M in the ξ domain is now an x-dependent set\nthat we may denote Ξ(M; x). The cutoffχ(x, ξ) essentially indicates this set\nin the ξ variable, in a smooth way so as to avoid unwanted ringing artifacts.\n\n5.4. RESOLUTION\nThe conclusion is that, when Q is given by (5.4), the filtered backprojec-\ntion operator B acts as an approximate inverse of F, and the kernel of BF\nis well modeled by the approximate identity\nK(x, y) ≃\nˆ\nei(y-x)·ξ dξ.\nΞ(M;x)\n5.4\nResolution\nSee Borden-Cheney chapter 9.\n(...)\nc\n∆x1 = ∆ω sin ψ\n∆x2 = L,\nL ≥λ\n5.5\nExercises\n1. Prove (5.2) in an alternative fashion by substituting in the far-field\napproximation of G in the imaging condition (4.7).\n2. Bistatic SAR: repeat and modify the derivation of (5.1) in the case\nof an antenna γ1(s) for transmission and another antenna γ2(s) for\nreception.\n\nCHAPTER 5. SYNTHETIC-APERTURE RADAR\n\nChapter 6\nComputerized tomography\n6.1\nAssumptions and vocabulary\n(...)\nComputerized tomography (CT scans, as well as PET scans) imaging\ninvolves inversion of a Radon or X-ray transform. It is primarily used for\nmedical imaging.\nIn two spatial dimensions, the variables in the Radon domain are t (offset)\nand θ (angle).\nData in the form d(t, θ) corresponds to the parallel beam\ngeometry. More often, data follow the fan-beam geometry, where for a given\nvalue of θ the rays intersect at a point (the source of X-rays), and t indexes\nrays within the fan. The transformation to go from parallel-beam to fan-\nbeam and back is\ndfan(t, θ) = dpara(t, θ + (at + b)),\nfor some numbers a and b that depend on the acquisition geometry. Datasets\nin the Radon domain are in practice called sinograms, because the Radon\ntransform of a Dirac mass is a sine wave1.\n6.2\nThe Radon transform and its inverse\nRadon transform:\n(Rf)(t, θ) =\nˆ\nδ(t -x · eθ)f(x) dx,\n1More precisely, a distribution supported on the graph of a sine wave, see an exercise\nat the end of the chapter.\n\nCHAPTER 6. COMPUTERIZED TOMOGRAPHY\nwith eθ = (cos θ, sin θ)T.\nFourier transform in t / Fourier-slice theorem2:\nRcf(ω, θ) =\nˆ\ne-iωx·eθf(x) dx.\nAdjoint Radon transform / (unfiltered) backprojection:\nR∗d(x) =\nˆ\neiωx·eθdb(ω, θ) dωdθ\n=\nˆ\nδ(t -x · eθ)d(t, θ) dtdθ\n=\nˆ\nd(x · θ, θ) dθ\nInverse Radon transform / filtered backprojection in the case of two spa-\ntial dimensions:\n-1\nR\nd(x) =\nθ\n(2\nˆ\neiωx·eθdb(ω, ) ω dωdθ.\nπ)n\n(notice the factor ω.)\nFiltered backprojection can be computed by the following sequence of\nsteps:\n- Take a Fourier transform to pass from t to ω;\n- Multiply by ω;\n- Take an inverse Fourier transform from ω back to t, call D(t, θ) the\nresult;\n- Compute\n\nd(x · θ, θ) dθ by quadrature and interpolation (piecewise\nlinear interpolation is often accurate enough.)\n2The direct Fourier transform comes with e-iωt. Here t is offset, not time, so we use\nthe usual convention for the FT.\n\n6.3. EXERCISES\n6.3\nExercises\n1. Compute the Radon transform of a Dirac mass, and show that it is\nnonzero along a sinusoidal curve (with independent variable θ and de-\npendent variable t, and wavelength 2π.)\n2. In this problem set we will form an image from a fan-beam CT dataset.\n(Courtesy Frank Natterer)\nDownload the data set at http://math.mit.edu/icg/ct.mat\nand load it in MATLAB(r) with load ct.mat\nThe array g is a sinogram.\nIt has 513 rows, corresponding to uni-\nformly sampled offsets t, and 360 columns, corresponding to uniform,\nall-around angular sampling with 1-degree steps in θ. The acquisition\nis fan-beam: a transformation is needed to recover the parallel-beam\ngeometry. The fan-beam geometry manifests itself in that the angle\ndepends on the offset t in a linear fashion. Instead of being just θ, it is\n(1 ≤t ≤513 is the row index)\nt -257\nθ +\nα,\nwith\nsin α =\n.\n2.87\nImaging from a parallel-beam sinogram is done by filtered backprojec-\ntion. Filtering is multiplication by ω in the ω domain dual to the offset\nt. Backprojection of a sinogram g(t, θ) is\nX\nI(x) =\ng(x · eθ, θ),\nθ\nwhere eθ is (cos θ, sin θ)T . Form the image on a grid which has at\nleast 100 by 100 grid points (preferably 200 by 200). You will need an\ninterpolation routine since x·eθ may not be an integer; piecewise linear\ninterpolation is accurate enough (interp1 in MATLAB).\nIn your writeup, show your best image, your code, and write no more\nthan one page to explain your choices.\n\nCHAPTER 6. COMPUTERIZED TOMOGRAPHY\n\nChapter 7\nSeismic imaging\nMuch of the imaging procedure was already described in the previous chap-\nters. An image, or a gradient update, is formed from the imaging condition\nby means of the incident and adjoint fields. This operation is called migration\nrather than backprojection in the seismic setting. The two wave equations\nare solved by a numerical method such as finite differences.\nIn this chapter, we expand on the structure of the Green's function in vari-\nable media. This helps us understand the structure of the forward operator\nF, as well as migration F ∗, in terms of a 2-point traveltime function τ(x, y).\nThis function embodies the variable-media generalization of the idea that\ntime equals total distance over wave speed, an aspect that was crucial in the\nexpression of backprojection for SAR. Traveltimes are important for \"trav-\neltime tomography\", or matching of traveltimes for inverting a wave speed\nprofile. Historically, they have also been the basis for Kirchhoffmigration, a\nsimplification of (reverse-time) migration.\n7.1\nAssumptions and vocabulary\nIn seismic imaging, the forward operator F is called Born modeling, and the\nadjoint/imaging operator F ∗is called migration. F is sometimes also called\ndemigration. We will see in a later section that F ∗\"undoes\" most of F in a\nkinematic sense. The set of F ∗\ns ds indexed by the source s is called prestack\nmigration, or prestack depthPmigration (PSDM) by geophysicists. They call\nthe sum over s in F ∗d =\ns F ∗\ns ds a postmigration stack (not poststack\nmigration!).\n\nCHAPTER 7. SEISMIC IMAGING\nA point x inside the Earth is said to be in the subsurface. Usually x =\n(x′, z) where x′ are the horizontal coordinates, and z is depth. Data space\nis indexed by (xr, , xs, t), where xr, xs are usually (but not always) at z = 0.\nDatasets are also called seismograms. Time is usually depicted as a vertical\naxis, pointing down.\nThey also call the source s a shot. When the dataset ds is indexed by shot\ns, it is called a shot gather, or common shot gather (CSG). Alternatively, a\ndataset can be organized by midpoint xm = xs+xr\nvs. half-offset h = xs-xr --\nin that case it is called common midpoint gather (CMP). Since the midpoint\ndoes not generally fall on a grid, doing a CMP requires binning1.\nMany physical assumptions are made to keep the algebra simple in this\n(!)\nchapter. They are as follows:\n1. We use a wave equation that models acoustic waves, rather than elas-\ntic waves.\nAll the waves are treated like P waves, hence mistakes\nwill be made with the S waves, as well as mode conversion. Another\nconsequence of using the acoustic simplification is that the medium\nis assumed to be isotropic, i.e., the waves do not travel at different\nspeeds depending on the direction in which they travel. Laminated\nrocks are usually anisotropic. Finally, we do not model the (frequency-\ndependent) dissipation of energy, and dispersion, of the seismic waves.\n2. We assume constant-density (single-parameter) acoustics.\n3. Boundary conditions are omitted, corresponding to the situation of\nnon-reflecting boundaries. This can be inaccurate in the presence of\nwater-air or rock-air interfaces, better modeled by a Neumann condi-\ntion (see chapter 1). Free boundaries corresponding to topography (or\nbathymetry) can be particularly hard to model properly.\n4. We assume the source is known, and of the form fs(x, t) = δ(x -\nxs)w(t). In practice the source needs to be determined or calibrated,\nusually from a direct arrival. The assumption that the source is a scalar\nfunction of x and t in an acoustic wave equation is itself not necessarily\naccurate (such as in earthquakes, for instance.)\n5. Data are assumed to be point samples of the solution of the wave\nequation, rather than filtered versions thereof (both in space and time).\n1Which can be a very inaccurate operation from a numerical viewpoint, i.e., as inaccu-\nrate as its adjoint, nearest-neighbor interpolation.\n\n7.2. KIRCHHOFF MODELING AND MIGRATION\nTypically, filtered versions of pressure disturbances are measured by\nmicrophones.\n6. We do not deal with the issue of acquisition noise in data (malfunc-\ntioning detectors, ambient seismic noise, incoherent scattering offof\nstructure that are not to be imaged, etc.).\n7.2\nKirchhoffmodeling and migration\nConsider a single, fixed source at location xs. (If there are several sources,\nthey are handled as in the earlier section on \"stacks\".) We have already\nseen how migration F ∗is computed with the so-called imaging condition in\nchapter 4. In that case, F ∗is called reverse-time migration.\nThe \"Kirchhoff\" version of the forward and adjoint operators F and F ∗\nare obtained by using the geometrical optics approximation of the Green's\nfunction, from chapter 2. Starting from the Born approximation, we obtain\nKirchhoffmodeling after a few lines of algebra:\n(Fm)(xr, t) =\nˆ\na(xs, x)a(x, xr)δ′′(t -τ(xr, x, xs))m(x) dx,\nwhere τ(xr, x, xs) = τ(xr, x) + τ(x, xs) is the three-point traveltime. The\ncurve/surface t = τ(xr, x, xs) traced in x-space (model space) is called isochrone.\nIt is an ellipse/ellipsoid when the background wave speed is uniform.\nPassing to the adjoint in a now-familiar manner by equating ⟨d, Fm⟩=\n⟨F ∗d, m⟩, we obtain Kirchhoffmigration as\n(F ∗d)(m) =\nˆˆ\na(xs, x)a(x, xr)δ′′(t -τ(xr, x, xs))d(xr, t) dxrdt.\nThe curve/surface t = τ(xr, x, xs) traced in (xr, t) space (data space) is called\nmoveout curve/surface. It is a hyperbola/hyperboloid when the background\nwave speed is uniform.\nCertain references call Kirchhoffmigration any\nbackprojection-style formula where either the amplitudes a(xs, x), a(x, xr)\nor the derivatives on the Dirac delta are absent, or both. Both F and F ∗, in\ntheir \"Kirchhoff\" version, are generalized Radon transforms.\nHistorically, Kirchhoffmigration (KM) has been very important because\nit is an explicit formula that requires solving ODEs (for τ, mostly), not\nPDE. Hence it is computationally much more attractive than reverse-time\n\nCHAPTER 7. SEISMIC IMAGING\nmigration (RTM). KM is still used in optimization schemes where cheap\ninexact iterations are useful.\n7.3\nDepth extrapolation\n7.4\nExtended modeling\n7.5\nExercises\n1. Show the formula for Kirchhoffmigration directly from the imaging\ncondition (4.3) and the geometrical optics approximation of the Green's\nfunction.\n2. Predict the recorded arrival time, for fixed xs and as a function of xr at\nthe surface, of the wave that undergoes a single reflction offa horizontal\nreflector at depth z. Assume a constant background speed c0.\n\nChapter 8\nMicrolocal analysis of imaging\nIn this chapter we consider that the receiver and source positions (xr, xs) are\nrestricted to a vector subspace Ω⊂R6, such as R2 × R2 in the simple case (!)\nwhen xr,3 = xs,3 = 0. We otherwise let xr and xs range over a continuum of\nvalues within Ω, which allows us to take Fourier transforms of distributions\nof xr and xs within Ω. We call Ωthe acquisition manifold, though in the\nsequel it will be a vector subspace for simplicity1.\n8.1\nPreservation of the wavefront set\nIn this section we show that, in simple situations, the operators F and F ∗de-\nfined earlier have inverse kinematic behaviors, in the sense that the respective\nmappings of singularities that they induce are inverses of one another.\nThe microlocal analysis of F and F ∗hinges on the mathematical notion\nof wavefront set that we now introduce. We start by describing the charac-\nterization of functions with (or without) singularities.\nNonsingular functions are infinitely differentiable, hence have Fourier\ntransforms that decay \"super-algebraically\".\nLemma 1. Let u ∈Cinf\n0 (Rn). Then, for all N > 0,\n|ub(k)| ≤CN(1 + |k|2)-N.\n1The Fourier transform can still be defined for functions on general manifolds, but\nthis involves patches, a partition of unity, and many distractions that we prefer to avoid\nhere. The microlocal theory presented here carries over mostly unchanged to the manifold\nsetting.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nProof. In the Fourier integral defining ub(k), insert N copies of the differential\noperator L =\nI-∆x\nof\nix\n|\nmeans\nk|2 by\nthe identity e\n·k = LNeix·k. Integrate by\n1+\nparts, pull out the (1 + |k|2)-N factor, and conclude by boundedness of u(x)\nand all its derivatives.\nA singularity occurs whenever the function is not locally Cinf.\nDefinition 2. Let u ∈S′(Rn), the space of tempered distributions on Rn.\nWe define the singular support of u as\nsing supp(u) = {x ∈Rn : there does not exist an open\nneighborhood V of x such that u ∈Cinf(V )}.\nIn addition to recording where the function is singular, we also want to\nrecord the direction in which it is singular. This idea gives rise to the notion\nof wavefront set, that we now build up to.\n\"Direction of singularity\" is associated to lack of decay of the Fourier\ntransform in the same direction. For this purpose it is useful to consider sets\ninvariant under rescaling, i.e., cones.\nDefinition 3. A set X is said to be a conic neighborhood of a set Y ⊂Rn if\n- X is open;\n- Y ⊂X;\n- ξ ∈X implies λξ ∈X for all λ > 0.\nDefinition 4. The singular cone of a tempered distribution u ∈S′(Rn) is\nthe set\nΓ(u) = {η ∈Rn\\{0} : there does not exist a conic\nneighborhood W of η such that for ξ ∈W,\n|ub(k)| ≤CN(1 + |k|2)-N,\nfor all N > 0.}\nThe intuition is that Γ(u) records the set of directions ξ in which the\nFourier transform of u decays slowly. The reason for formulating the defini-\ntion in negative terms is that we want the resulting cone to be closed, i.e.,\nisolated directions in which the Fourier transform would accidentally decay\nquickly, while decay is otherwise slow along an open set of nearby directions,\ndo not count.\n\n8.1. PRESERVATION OF THE WAVEFRONT SET\nThe construction of Γ(u) is global - a direction is labeled singular with-\nout regard to the locations x where there are singularities that may have\ncontributed to the direction being labeled singular. To go further we need to\nlocalize the construction.\nLemma 2. Let φ ∈Cinf(Rn) and u ∈S′(Rn\n). Then Γ(φu) ⊂Γ(u).\nDefinition 5. Let x ∈X ⊂Rn. We call singular fiber at x the set\n\\\nΓx(u) = {\nΓ(φu) : φ ∈Cinf\n0 (X), φ(x) = 0}.\nφ\nThe idea of this definition is that we localize u by means of multiplica-\ntion with a smooth function φ of arbitrarily small support, then consider the\nsmallest resulting singular cone. The definition can equivalently be formu-\nlated by means of a family of smooth indicators whose support converges\ntoward the singleton {x} in the sense of sets:\nΓx(u) = { lim Γ(φju) : φj ∈Cinf\n0 (X), φj(x) = 0, supp φj →{x} }.\nj→inf\nNote that Γx(u) is empty if u is smooth (Cinf) at x.\nThe wavefront set then consists of the union of the singular cones, taken\nas fibers over the singular support.\nDefinition 6. Let u ∈S′(Rn). The wavefront set of u is the set\nWF(u) = {(x, ξ) ∈Rn × (Rn\\{0}) : ξ ∈Γx(u)}.\nThe set Rn × (Rn\\{0}) is also abbreviated as T ∗Rn\\0.\nBecause this definition localizes the singularites of u both in position and\nin direction, it is the basis for microlocal analysis.\nIn this context, the product Rn × Rn is the cotangent bundle of Rn, and\ndenoted T ∗Rn. The reason for viewing each singular cones (each fiber) as a\nsubset of the cotangent space at x rather than the tangent space at x will be\napparent in the sequel as we study the behavior of WF(u) under change of\nvariables.\nNotice that the projection of WF(u) on the first Rn (relative to x) is the\nsingular support sing supp(u), while the projection of WF(u) on the second\nRn (relative to ξ) is the singular cone Γ(u).\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nIn the context of imaging, we will use the same notation as introduced\nabove and let ξ be the wave-vector variable, dual to position x ∈Rn in\nmodel space. Usually, n = 3. Data space is indexed by the base variables\n(xs, xr, t) ∈Ω× R ≡Rnd, usually with nd = 5. We denote by (ξr, ξs, ω) the\ncorresponding dual variables, i.e., the independent variables of the Fourier\ntransform of a function of (xs, xr, t).\nThe wavefront set of a distribution\nd(xr, xs, t) in Rnd is then\nWF(d) = {(xr, x , t; ξ\nd\ns\n, ξ\n∗\nn\nr\ns, ω) ∈T R\n\\0 : (ξr, ξs, ω) ∈Γ(xr,xs,t)(d)}.\nWe will also consider wavefront sets of functions in the product space\nRn × Rnd = Rn+nd of model space and data space, such as the distributional\nintegral kernel K of F. We have\nWF(K) = {(xr, xs, t, x; ξr, ξs, ω, ξ) ∈T ∗Rn+nd\\0 : (ξr, ξs, ω, ξ) ∈Γ(xr,xs,t,x)(K)}.\n(8.1)\nIn a product space, a wavefront set has the additional interpretation of being\na relation. The role of WF(K), where K is the kernel of F, is that it provides\na way of predicting WF(Fm1) from WF(m1) using a simple composition\nrule.\nIn order to see WF(K) as a relation, however, two minor modifications\nneed to be made to the definition (8.1). First, the variables first need to be\nre-arranged as ((x, ξ), (xr, xs, t; ξr, ξs, ω)), in order to be seen as elements of\nthe product space T ∗Rn × T ∗Rnd. Second, the sign of the first co-variable\nξ needs to be flipped, so that we instead consider the \"wavefront prime\"\nWF ′(K). In short,\n((x, -ξ), (xr, xs, t; ξr, ξs, ω)) ∈WF ′(k)\n⇔\n(xr, xs, t, x; ξr, ξs, ω, ξ) ∈WF(K)\nWe call WF ′(K) the wavefront relation of the operator whose kernel is K.\nMore generally, the operations that define relations are as follows.\n- The composition of a relation C ⊂X × Y and a set S ⊂X is the set\nC *S = {y ∈Y : there exists x ∈S such that (x, y) ∈C}.\n- The composition of two relations C ⊂X × Y and C′ ∈Y × Z is the\nrelation\nC *C′ = {(x, z) ∈X × Z : there exists y ∈Y\nsuch that (x, y) ∈C, (y, z) ∈C′}.\n\n8.1. PRESERVATION OF THE WAVEFRONT SET\n- The transposition of a relation C is the relation\nCT = {(y, x) ∈Y × X : (x, y) ∈C}.\nWe call identity relation on X the set I = {(x, x) : x ∈ X}. Note that\nall our relations are \"canonical\" in the sense that they preserve areas in a\ngeneralized sense2. We will not purse this topic further here.\nThe high-level idea is that we would like to write WF(Fm1) ⊂WF ′(K)*\nWF(m1), but this identity is not always true.\nThe obstruction is that\nelements of WF ′(K) are in T ∗Rn×nd\\0, which is in general a larger set\nthan (T ∗Rn\\0) × (T ∗Rnd\\0). Hence the interpretation of WF ′(K) as a re-\nlation runs into trouble with those elements such that either ξ = 0 but\n(ξs, ξr, ω) = 0; or conversely (ξs, ξr, ω) = 0 but ξ = 0. We say such elements\nare part of the zero sections of WF ′(K). In the sequel, we simply treat the\ncase when those zero sections are empty.\nThe proper way of composing wavefront sets is the following celebrated\ntheorem, due to H ormander.\nTheorem 5. Let K be the distributional kernel of an operator F, with wave-\nfront set (8.1). Assume that the elements of WF ′(K) obey\nξ = 0\n⇔\n(ξs, ξr, ω) = 0,\ni.e., WF ′(K) does not have zero sections. Then, for all m1 in the domain of\nF,\nWF(Fm1) ⊂WF ′(K) *WF(m1).\nThe physical significance of the zero sections will be made clear below\nwhen we compute WF(K) for the imaging problem.\nWe can now consider the microlocal properties of the imaging operator\nF ∗. Its kernel is simply KT, the transpose of the kernel K of F. In turn, the\nrelation WF ′(KT) is simply the transpose of WF ′(K) seen as a relation in\nthe product space T ∗Rn × T ∗Rnd,\nWF ′(KT) = (WF ′(K))T.\n2Namely, they are Lagrangian manifolds: the second fundamental symplectic form\nvanishes when restricted to canonical relations. The precaution of flipping the sign of the\nfirst covariable ξ in the definition of WF ′(K) translates into the fact that, in variables\n((x, ξ), (y, η)), it is dx ∧dξ -dy ∧dη that vanishes when restricted to the relation, and not\ndx ∧dξ + dy ∧dη.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nOur interest is in whether the normal operator F ∗F preserves the sin-\ngularities of the function m1 it is applied to, i.e., whether imaging \"places\"\nsingularities of the model perturbation at the right location from the knowl-\nedge of the linearized data Fm1. To study the mapping of singularities of\nthe normal operator F ∗F, we are led to the composition\nWF(F ∗Fm ) ⊂(WF ′\nT\n(K)) *WF ′(K) *WF(m1).\nup to the same precaution involving zero-sections as above. The question is\nnow whether the transpose relation can be considered an inverse, i.e., whether\n(WF ′(K))T *WF ′(K) is a subset of the identity on T ∗Rn. A simple condition\nof injectivity is necessary and sufficient for this to be the case.\nDefinition 7. We say a relation C ⊂X × Y is injective if\n(x1, y) ∈C, (x2, y) ∈C\n⇒\nx1 = x2.\nAs a map from X to Y , an injective relation may be multivalued, but as\na map from Y to X, the transpose relation is a graph.\nLemma 3. A relation C is injective if and only if CT *C ⊂I.\nProof. Assume that C ⊂X×Y is injective, and let S ⊂X. By contradiction,\nif CT *C were not a subset of I, then there would exist an element x of\nX, for which there exists an element x′ of CT *C *x such that x′ = x. By\ndefinition of CT *C, this means that there exists y ∈Y such that (y, x′) ∈CT\nand (x, y) ∈C. By definition of transpose relation, we have (x′, y) ∈C.\nInjectivity of C implies x = x′, a contradiction.\nBy contraposition, assume that C ⊂X × Y is not injective, i.e., there\nexist two elements (x, y) and (x′, y) of C for which x = x′. Then (y, x′) ∈CT\nby definition of transpose, and (x, x′) ∈CT *C by definition of composition.\nSince x = x′, CT *C is not contained in the identity relation I = {(x, x) :\nx ∈X}.\nWe have all the pieces to gather the main result on preservation of sin-\ngularities.\nTheorem 6. Let K be the distributional kernel of an operator F, with wave-\nfront set (8.1). Assume that the elements of WF ′(K) obey\nξ = 0\n⇔\n(ξs, ξr, ω) = 0,\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\ni.e., WF(K) does not have zero sections. Further assume that WF ′(K) is\ninjective as a relation in (T ∗Rn\\0) × (T ∗Rnd\\0). Then, for all m1 in the\ndomain of F,\nWF(F ∗Fm1) ⊂WF(m1).\nProof. Notice that the zero sections of (WF ′(K))T are the same as those of\nWF ′(K), hence H ormander's theorem can be applied twice. We get\nWF(F ∗Fm1) ⊂(WF ′(K))T *WF ′(K) *WF(m1).\nUnder the injectivity assumption, lemma 3 allows to conclude.\nThe assumptions of the theorem are tight:\n- If zero sections are present, the composition law is different and adds\nelements away from the diagonal, as shown in the general formulation\nof H ormander's theorem in [?].\n- If injectivity does not hold, lemma 3 shows that the composition of two\nrelations CT *C must have non-diagonal components.\n8.2\nCharacterization of the wavefront set\nIn this section we construct the wavefront relation WF ′(K) explicitly in the\nsimple scenario of section 7.2.\nRecall that the setting of section 7.2 is one in which the background\nmodel m0(x) is heterogeneous and smooth, and enjoys no multipathing in\nthe zone of interest. The source wavelet w(t) (entering the right-hand side (!)\nof the wave equation) still needs to be an impulse δ(t) in order to create (!)\npropagating singularities. If instead the wavelet is essentially bandlimited, then so\nwill the wavefields, but there are still remnants of the microlocal theory in the\nmagnitude and locality of the propagating \"wiggles\"3.\nThe distributional kernel of GRT migration is then\nK(x; x\n′′\nr, xs, t) = a(x, xr, xs)δ (t -τ(xr, x, xs)),\nwhere τ(xr, x, xs) = τ(xr, x) + τ(x, xs) is the three-point traveltime, and a is\na smooth amplitude, except for a singularity at x = xr and x = xs.\n3Microlocal theory can be expressed scale-by-scale, with wave packet decompositions\nand/or the so-called FBI transform. We leave out this topic.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nThe singular support of K is the same as its support as a measure, namely\nsing supp(K) = {(x, xr, xs, t) : t = τ(xr, x, xs)}.\nIn what follows we keep in mind that, for (x, xr, xs, t) to be a candidate\nbase point in the wavefront relation of K, i.e., with a non-empty fiber, it is\nnecessary that t = τ(xr, x, xs).\nTo find this wavefront relation, we first localize K around some refer-\nence point (x0, xr,0, xs,0, t0) by multiplication with increasingly sharper cutoff\nfunctions, such as\nχ(∥x -x0∥) χ(∥xr -xr,0∥) χ(∥xs -xs,0∥) χ(|t -t0|),\nwhere χ is a Cinf\nfunction compactly supported in the ball Bε(0) for some\nε that tends to zero. To keep notations manageable in the sequel, we in-\ntroduce the symbol [ χ ] to refer to any Cinf\nfunction of (x, xr, xs, t) (or any\nsubset of those variables) with support in the ball of radius ε centered at\n(x0, xr,0, xs,0, t0) (or any subset of those variables, resp.).\nWe then take a Fourier transform in every variable, and let\nI(ξ, ξ , ξ , ω) =\nˆˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωt)\nr\ns\nK(x; xr, xs, t)[ χ ]dx dxr dxs dt.\n(8.2)\nAccording to the definition in the previous section, the test of member-\nship in WF ′(K) involves the behavior of I under rescaling (ξ, ξr, ξs, ω) →\n(αξ, αξr, αξs, αω) by a single number α > 0. Namely,\n((x0; ξ), (xr,0, xs,0, t0; ξr, ξs, ω)) ∈/ WF ′(K)\nprovided there exists a sufficiently small ε > 0 (determining the support of\n[ χ ]) such that\nI(αξ, αξr, αξs, αω) ≤Cmα-m\nfor all m > 0.\nNotice that the Fourier transform in (8.2) is taken with a minus sign in the\nξ variable (hence eix·ξ instead of e-ix·ξ) because WF ′(K) - the one useful for\nrelations - precisely differs from WF(K) by a minus sign in the ξ variable.\nBefore we simplify the quantity in (8.2), notice that it has a wave packet\ninterpretation: we may let\nφ(x) = e-ix·ξχ(∥x -x0∥),\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\nψ(x , x .t) = e-i(xr·ξr+xs·ξs+ωt)\nr\ns\nχ(∥xr -xr,0∥) χ(∥xs -xs,0∥) χ(|t -t0|),\nand view\nI(ξ, ξr, ξs, ω) = ⟨ψ, Fφ⟩(xr,xs,t),\nwith the complex conjugate over the second argument. The quantity I(ξ, ξr, ξs, ω)\nwill be \"large\" (slow decay in α when the argument is scaled as earlier) pro-\nvided the wave packet ψ in data space \"matches\", both in terms of location\nand oscillation content, the image of the wave packet φ in model space under\nthe forward map F.\nThe t integral can be handled by performing two integrations by parts,\nnamely\nˆ\ne-iωtδ′′(t -τ(x, xr, xs))χ(∥t -t0∥) dt = e-iωτ(x,xr,xs)χeω(τ(x, xr, xs)),\nfor some smooth function χe (t) = eiωt(e-iωt\nω\nχ(|t-t0|))′′ involving a (harmless)\ndependence on ω0, ω1 and ω2. After absorbing the χeω(τ(x, xr, xs)) factor in\na new cutofffunction of (x, xr, xs) still called [ χ ], the result is\nI(ξ, ξ , ξ , ω) =\nˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωτ(x,xr,xs))\nr\ns\na(x; xr, xs)[ χ ]dx dxr dxs.\n(8.3)\nFirst, consider the case when x0 = xr,0 or x0 = xs,0. In that case, no\nmatter how small ε > 0, the cutofffunction [ χ ] never avoids the singularity of\nthe amplitude. As a result, the Fourier transform is expected to decay slowly\nin some directions. The ampltude's singularity is not the most interesting\nfrom a geometrical viewpoint, so for the sake of simplicity, we just brace\nfor every possible (ξ, ξr, ξs, ω) to be part of the fiber relative (x0, xr,0, xs,0, t0)\nwhere either x0 = xr,0 or x0 = xs,0. It is a good exercise to characterize these\nfibers in more details; see for instance the analysis in [?].\nAssume now that x0 = xr,0 and x0 = xs,0. There exists a sufficiently small\nε for which a(x, xr, xs) is nonsingular and smooth on the support of [ χ ]. We\nmay therefore remove a from (8.3) by absorbing it in [ χ ]:\nI(ξ, ξ , ξ , ω) =\nˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωτ(x,xr,xs))\nr\ns\n[ χ ]dx dxr dxs.\n(8.4)\nWhat is left is an integral that can be estimated by the stationary phase\nlemma, or more precisely, the simplest version of such a result when the\nphase is nonstationary: see lemma 4 in appendix C.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nThe phase in (8.4) is φ(x, xr, xs) = x·ξ-xr·ξr-xs·ξs-ωτ(x, xr, xs). Notice\nthat φ involves the co-variables in a linear manner, hence is homogenenous\nof degree 1 in α, as needed in lemma 4. Its gradients are\n∇xφ = ξ -ω∇xτ(x, xr, xs),\n∇xrφ = -ξr -ω∇xrτ(x, xr, xs),\n∇xsφ = -ξs -ω∇xsτ(x, xr, xs).\nIf either of these gradients is nonzero at (x0, xr,0, xs,0), then it will also be\nzero in a small neighborhood of that point, i.e., over the support of [ χ ]\nfor ε small enough.\nIn that case, lemma 4 applies, and it follows that\nthe decay of the Fourier transform is fast no matter the (nonzero) direc-\ntion (ξ, ξr, ξs, ω).\nHence, if either of the gradients is nonzero, the point\n((x0, ξ), (x\n′\nr,0, xs,0, t0; ξr, ξs, ω)) is not in WF (K).\nHence ((x0, ξ), (xr,0, xs,0, t0; ξr, ξ\n′\ns, ω)) may be an element in WF (K) only\nif the phase has a critical point at (x0, xr,0, xs,0):\nξ = ω∇xτ(x0, xr,0, xs,0),\n(8.5)\nξr = -ω∇xrτ(x0, xr,0, xs,0),\n(8.6)\nξs = -ω∇xsτ(x0, xr,0, xs,0).\n(8.7)\nAdditionally, recall that t0 = τ(x0, xr,0, xs,0). We gather the result as follows:\nWF ′(K) ⊂{ ((x, ξ), (xr, xs, t; ξr, ξs, ω)) ∈(T ∗Rn × T ∗Rnd)\\0 :\nt = τ(x, xr, xs), ω = 0, and, either x = xr, or x = xs,\nor (8.5, 8.6, 8.7) hold at (x, xr, xs) }.\nWhether the inclusion is a proper inclusion, or an equality, (at least away\nfrom x = xr and x = xs) depends on whether the amplitude factor in (8.4)\nvanishes in an open set or not.\nNotice that ω = 0 for the elements in WF ′(K), otherwise (8.5, 8.6, 8.7)\nwould imply that the other covariables are zero as well, which is not allowed\nin the definition of WF ′(K). In the sequel, we may then divide by ω at will.\nThe relations (8.5, 8.6, 8.7) have an important physical meaning. Re-\ncall that t = τ(x, xr, xs) is called isochrone curve/surface when it is consid-\nered in model x-space, and moveout curve/surface when considered in data\n(xr, xs, t)-space.\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\n- The relation ξ = ω∇xτ(x, xr, xs) indicates that ξ is normal to the\nisochrone passing through x, with level set t = τ(x, xr, xs). In terms of\ntwo-point traveltimes, we may write\nξ = ∇xτ(x, xr) + ∇xτ(x, xs).\nω\nObserve that ∇xτ(x, xr) is tangent at x to the ray from xr to x, and\n∇xτ(x, xs) is tangent at x to the ray from xs to x, hence ξ is the bisector\ndirection for those two rays. The (co)vector ξ may be understood as\nthe (co)normal to a \"localized mirror\" about which the incident wave\nreflects in a specular manner to create the scattered wave. The equation\nabove is then the vector expression of Snell's law of reflection, that\nthe angle of incidence ∠(∇xτ(x, xs), ξ) equals the angle of reflection\n∠(∇xτ(x, xr), ξ).\n- The special case ξ = 0 is equally important from a physical viewpoint.\nSince ω = 0, it corresponds to\n∇xτ(x, xr) = -∇xτ(x, xs),\ni.e., the tangents to the incident and reflected rays are collinear and\nopposite in signs. This happens when x is a point on the direct, unbro-\nken ray linking xr to xs. We may call this situation forward scattering:\nit corresponds to transmitted waves rather than reflected waves. The\nreader can check that it is the only way in which a zero section is created\nin the wavefront relation (see the previous section for an explanation\nof zero sections and how they impede the interpretation of WF ′(K) as\na relation.)\n- Consider now (8.6) and (8.7). Pick any η = (ηr, ηs), form the combi-\nnation ηr· (8.6) + ηs· (8.7) = 0, and rearrange the terms to get\n\nξr\nηr\nξs·\nηs\n= 0,\nω\nηr · ∇xrτ + ηs · ∇xsτ\nwith both τ evaluated at (x, xr, xs).\nThe second vector in the dot\nproduct is an arbitrary vector tangent to the moveout surface t =\nτ(x, xr, xs) in (xr, xs, t)-data space. Thus (ξr, ξs, ω) is normal to the\nmoveout surface.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nWe are now ready to interpret WF ′(K) in terms of the mapping of singu-\n(!)\nlarities that it generates. Assume that no forward scattering is taking place,\ni.e., we are only dealing with reflected rather than transmitted waves. From a\nsingularity in model space at a point x and in the direction ξ - a local mirror\nplaced at x with conormal ξ - the wavefront relation predicts that there may\nbe singularities in data space, with location(s) (xr, xs, t) and corresponding\nconormal direction(s) (ξr, ξs, ω) determined as follows.\nFix a couple (x, ξ).\n1. For each xs, find the unique ray that links xs to x.\n2. At x, find the direction of the incoming ray as ∇xτ(x, xs).\n3. Determine ∇xτ(x, xr) as the vector with magnitude 1/c(x) (from the\neikonal equation), and direction so that (8.5) holds for some ω. This\ncan be done by computing the reflection of ∇xτ(x, xs) about the axis\ngenerated by ξ,\n∇xτ(x, xs) · ξ\n∇xτ(x, xr) = ∇xτ(x, xs) -2\nξ.\nξ · ξ\n4. Trace the ray from x and take-offdirection -∇xτ(x, xr):\n- If the ray is closed or exits the domain before reaching a receiver,\ndiscard xs. This particular xs does not give rise to any singularity\n(relative to this particular couple (x, ξ).)\n- If the ray reaches a receiver xr, let t = τ(x, xs) + τ(x, xr). Some\nsingularity may appear at the point (xs, xr, t) in data space.\n5. Determine ω = ∥ξ∥/∥∇xτ(x, xr) + ∇xτ(x, xs)∥. The normalization of\nξ starts as arbitrary, and is undone by division by ω.\n6. Determine ξr and ξs directly from (8.6) and (8.7). Then the covector\nconormal to the singularity at (xr, xs, t) is (ξr, ξs, ω) (as well as all its\npositive multiples.)\nNotice that both the data variables (xr, xs, t) and the covariables (ξr, ξs, ω)\nare determined uniquely from x and ξ in this situation, hence the singularity\nmapping induced by the wavefront relation is one-to-one.\n\n8.3. PSEUDODIFFERENTIAL THEORY\nIf we make the additional two assumptions that the receivers surround\nthe domain of interest (which would require us to give up the assumption\nthat the acquisition manifold Ωis a vector subspace), and that there is no\ntrapped (closed) ray , then the test in point 4 above always succeeds. In (!)\nthat case, the wavefront relation is onto, hence bijective, and can be inverted\nby a sequence of steps with a similar geometrical content as earlier. See an\nexercise at the end of this chapter.\n8.3\nPseudodifferential theory\n8.4\nExercises\n1. Compute the wavefront set of the following functions/distributions of\ntwo variables: (1) the indicator function H(x1) (where H is the Heav-\niside function) of the half-plane x1 > 0, (2) the indicator function of\nthe unit disk, (3) the indicator function of the square [-1, 1]2, (4) the\nDirac delta δ(x).\n2. Find conditions on the acquisition manifold under which forward scat-\ntering is the only situation in which a zero section can be created in\nWF ′(K). [Hint: transversality with the rays from x to xr, and from x\nto xs.]\n3. Perform the geometrical construction of (WF ′(K))T for migration,\nanalogous to the geometrical construction of WF ′(K) for Born mod-\neling done in section 8.2. Assume: 1) τ single-valued, 2) no forward\nscattering, 3) full aperture of receivers, 4) no trapped rays, and 5) the\nacquisition manifold Ωis a vector subspace equal to the Cartesian prod-\nuct of two copies of the same subspace of codimension 1 in R3 (one for\nxr and one for xs). In other words, you may assume that both xr and\nxs lie on the \"surface\" z = 0.\nSolution. Fix (xr, xs, t) and (ξr, ξs, ω) = 0. We aim to determine a\nunique (x, ξ) so that ((x, ξ), (xr, xs, t; ξr, ξs, ω)) ∈WF ′(K).\nStarting with (8.6), notice first that ∇xrτ(x, xr) is a partial gradient:\nit is the projection to the acquisition manifold Ωat xr of the three-\ndimensional gradient of τ in its second argument, say ∇yτ(x, y)|y=(xr,0),\nin coordinates y = (xr, z) so that Ωis represented by z = 0. A similar\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nobservation holds for ∇xsτ(x, xs). Since the norm of each partial gra-\ndient is less than the norm of the full gradient, and since the eikonal\nequation determines this latter norm, ξr and ξs must obey the geometric\ncompatibility relations\nω\n|ξr| ≤c(xr),\n|ξs| ≤\nω\n.\nc(xs)\nThe coordinate z is one-dimensional from our assumption on the ac-\nquisition manifold, so we can now assemble the full gradient of τ at xr.\nWith the help of (8.6) we obtain\n\nξr\n∇yτ(x, y)|y=(xr,0) =\n-ω , ±\ns\nc2(xr) -|ξr|2!\n.\nω2\n(The sign is determined on geometrical grounds, so that ∇yτ points\noutside the domain of interest.) Minus the direction of ∇yτ is also\nthe take-off direction of a ray from xr. We obtain ∇yτ(x, y)|y=(xs,0) in\nthe same fashion from (8.7), which determines the take-off direction\nof a ray from xs. These rays meet at a single point that we call x.\nWe may alternatively intersect either of these rays with the isochrone\nt = τ(x, xr, xs) to obtain x; this piece of information is redundant in the\nwavefront relation4. Finally, we determine ξ = ω∇xτ(x, xr, xs).\n4Only because we have assumed that the acquisition manifold has low codimension.\n\nChapter 9\nOptimization\n9.1\nRegularization and sparsity\n9.2\nDimensionality reduction techniques\nOne way to reduce the dimensionality of a dataset is to scramble data as\nde= Cd, where\nX\ndej,r(t) =\ncj,sdr,s(t -bj,s).\ns\nThe numbers cj,s and bj,s may be random, for instance. The point is that\nusing fewer values of j than s may result in computational savings -- a\nstrategy sometimes called source encoding. By linearity of the wave equation,\nthe scrambled data de can be seen as originating from scrambled shots, or\nsupershots fe= Cf, for\nX\nfej(x, t) =\ncj,sfs(x, t -bj,s).\ns\nScrambled data may be all that's available in practice, in acquisition scenarios\nknown as simultaneous sourcing.\nThe adjoint operation C∗results in twice-scrambled data D = C∗de, where\nX\nDr,s(t) =\nc\ne\nj,sdj,r(t + bj,s).\nj\nThe linearized forward model with scrambling is de = CFm. The ba-\nsic imaging operator is still the adjoint, Im = F ∗C∗de. In addition to the\n\nCHAPTER 9. OPTIMIZATION\ntraditional incident and adjoint fields\nu0,s = Gfs,\nqs = Gds,\nwhere G is the Green's function in the unperturbed medium, and G the\ntime-reversed Green's function, we define the scrambled fields\nue0,j = Gfej,\nqej = Gdej.\nAlso define the twice-scrambled adjoint field\nQ\n∗\ns = G(C de)s.\nThen\nX\nT\nI (x) = (F ∗\n∗\n∂\nC\nˆ\n,s\nde\nu0\nm\n)(x) = -\ns\n(x, t) Qs(x, t) dt.\n∂t2\nAnother formula involving j instead of s (hence computationally more favor-\nable) is\nX\n,j\nIm(x)\n-\nj\nˆ T ∂2ue0\n=\n(x, t) qej(x, t) dt.\n(9.1)\n∂t2\nTo shoPw this latter form\nPula, use Q = C∗qe, pass C∗to the rest of the integrand\nwith\ns v (C∗\ns\nw)s =\nj(Cvj)wj, and combine Cu0 = ue0.\nScrambled data can also be used as the basis of a least-squares misfit,\nsuch as\nJe\n(m) =\n∥de-CF(m)∥2\n2.\nThe gradient of Je is F ∗C∗applied to the residual, hence can be computed\nwith (9.1).\n\nAppendix A\nCalculus of variations,\nfunctional derivatives\nThe calculus of variations is to multivariable calculus what functions are\nto vectors. It answers the question of how to differentiate with respect to\nfunctions, i.e., objects with an uncountable, infinite number of degrees of\nfreedom. Functional calculus is used to formulate linearized forward models\nfor imaging, as well as higher-order terms in Born series. It is also useful\nfor finding stationary-point conditions of Lagrangians, and gradient descent\ndirections in optimization.\nLet X, Y be two function spaces endowed with norms and inner products\n(technically, Hilbert spaces). A functional φ is a map from X to R. We\ndenote its action on a function f as φ(f). An operator F is a map from X\nto Y . We denote its action on a function f as Ff.\nWe say that a functional φ is Fr echet differentiable at f ∈X when there\nexists a linear functional A : X 7→R such that\n|φ(f + h) -φ(f) -A(h)|\nlim\nh→0\n= 0.\n∥h∥\nIf this relation holds, we say that A is the functional derivative, or Fr echet\nderivative, of φ at f, and we denote it as\nδφ\nA =\n[f].\nδf\nIt is also called the first variation of φ. It is the equivalent of the gradient in\nmultivariable calculus. The fact that A is a map from X to R corresponds\n\n124APPENDIX A. CALCULUS OF VARIATIONS, FUNCTIONAL DERIVATIVES\nto the idea that a gradient maps vectors to scalars when paired with the dot\nproduct, to form directional derivatives. If X = Rn and f = (f1, . . . , fn), we\nhave\nδφ[f](h) = ∇φ(f) · h.\nδf\nFor this reason, it is is also fine to write A(h) = ⟨A, h⟩.\nThe differential ratio formula for δφ\nδf is called Gˆateaux derivative,\nδφ\nφ(f + th) -φ(f)\n[f](h) = lim\nδf\nt→0\n,\n(A.1)\nt\nwhich corresponds to the idea of the directional derivative in Rn.\nExamples of functional derivatives:\n- φ(f) = ⟨g, f⟩,\nδφ\nδf [f] = g,\nδφ[f](h) = ⟨g, h⟩\nδf\nBecause φ is linear, δφ = φ. Proof: φ(f + th) -φ(f) = ⟨g, f + th⟩-\nδf\n⟨g, f⟩= t⟨g, h⟩, then use (A.1).\n- φ(f) = f(x0),\nδφ[f] = δ(x -x0),\n(Dirac delta).\nδf\nThis is the special case when g(x) = δ(x -x0). Again, δφ = φ.\nδf\n- φ(f) = ⟨g, f 2⟩,\nδφ[f] = 2fg.\nδf\nProof: φ(f + th) -φ(f) = ⟨g, (f + th)2⟩-⟨g, f⟩= t⟨g, 2fh⟩+ O(t2) =\nt⟨2fg, h⟩+ O(t2), then use (A.1).\nNonlinear operators F[f] can also be differentiated with respect to their\ninput function. We say F : X →Y is Fr echet differentiable when there exists\na linear operator F : X →Y\n∥F[f + h] -F[f] -Fh∥\nlim\nh→0\n∥h∥\n= 0.\n\nF is the functional derivative of F, and we write\nδF\nF =\n[f].\nδf\nWe still have the difference formula\nδF\nδf [f]h = lim\nt→0\nF[f + th] -F[f].\nt\nExamples:\n- F[f] = f. Then\nδF [f] = I,\nδf\nthe identity. Proof: F is linear hence equals its functional derivative.\nAlternatively, apply the difference formula to get δF [f]h = h.\nδf\n- F[f] = f 2. Then\nδF [f] = 2f,\nδf\nthe operator of multiplication by 2f.\nUnder a suitable smoothness assumption, the Fr echet Hessian of an op-\nerator F can also be defined: it takes two functions as input, and returns\na function in a linear manner (\"bilinear operator\"). It is defined through a\nsimilar finite-difference formula\nδ2F\n⟨δf 2 [f]h1, h2⟩= lim\nt→0\nF[f + t(h2 + h1)] -F[f + th2] -F[f + th1] + F[f].\nt2\nThe Hessian is also called second variation of F. For practical calculations\nof the Hessian, the notation δ2F is\nδf2\ntoo cavalier. Instead, it is useful to view\nthe Hessian as the double directional derivative\nδ2F\nδfδf ′\nin two directions f and f ′, and compute those derivatives one at a time. This\nformula is the equivalent of the mixed partial\n∂2f\nwhen the two directions\n∂xi∂xj\nare xi and xj in n dimensions.\n\n126APPENDIX A. CALCULUS OF VARIATIONS, FUNCTIONAL DERIVATIVES\nFunctional derivatives obey all the properties of multivariable calculus,\nsuch as chain rule and derivative of a product (when all the parties are\nsufficiently differentiable).\nWhenever in doubt when faced with calculations involving functional\nderivatives, keep track of free variables vs. integration variables -- the equiv-\nalent of \"free indices\" and \"summation indices\" in vector calculus. For in-\nstance,\n-\nδF\nδf is like δFi\nδfj , with two free indices i and j;\n-\nδF\nδf h is like P\nj\nδFihj, with one free index i and one summation index j.\nδfj\n-\nδ2F\nδf2 is like\nδ2Fi\nδfjδfk , with three free indices i, j, k.\n- ⟨δ2F\nδf2 h1, h2⟩is like P\nj,k\nδ2Fi (h1)j(h2)k, with one free index i and two\nδfjδfk\nsummation indices j and k.\nNo free index indicates a scalar, one free index indicates a function (or a\nfunctional), two free indices indicate an operator, three indices indicate an\n\"object that takes in two functions and returns one\", etc.\n\nAppendix B\nFinite difference methods for\nwave equations\nMany types of numerical methods exist for computing solutions to wave\nequations - finite differences are the simplest, though often not the most\naccurate ones.\nConsider for illustration the 1D time-dependent problem\n∂2u\nm(x) ∂t2 = ∂2u + f(x, t),\nx ∈[0, 1],\n∂x2\nwith smooth f(x, t), and, say, zero initial conditions. The simplest finite\ndifference scheme for this equation is set up as follows:\n- Space is discretized over N + 1 points as xj = j∆x with ∆x =\n1 and\nN\nj = 0, . . . , N.\n- Time is discretized as tn = n∆t with n = 0, 1, 2, . . .. Call un\nj the com-\nputed approximation to u(xj, tn). (In this appendix, n is a superscript.)\n- The centered finite difference formula for the second-order spatial deriva-\ntive is\n∂2u\n∂x2(xj, tn) = un\nj+1 -2un\nj + un\nj-1 + O((∆x)2),\n(∆x)2\nprovided u is sufficiently smooth - the O(·) notation hides a multiplica-\ntive constant proportional to ∂4u/∂x4.\n\n128APPENDIX B. FINITE DIFFERENCE METHODS FOR WAVE EQUATIONS\n- Similarly, the centered finite difference formula for the second-order\ntime derivative is\n∂2u\n∂t2 (xj, tn) = un+1\nj\n-2un\nj + un-1\nj\n+ O((∆t)2),\n(∆t)2\nprovided u is sufficiently smooth.\n- Multiplication by m(x) is realized by multiplication on the grid by\nm(xj). Gather all the discrete operators to get the discrete wave equa-\ntion.\n- The wave equation is then solved by marching: assume that the values\nof un-1\nj\nand un\nj are known for all j, then isolate un+1\nj\nin the expression\nof the discrete wave equation.\nDirichlet boundary conditions are implemented by fixing. e.g., u0 = a.\nNeumann conditions involve a finite difference, such as u1-u0\n∆x\n= a. The more\naccurate, centered difference approximation u1-u-1 = a with a ghost node at\n2∆x\nu-1 can also be used, provided the discrete wave equation is evaluated one\nmore time at x0 to close the resulting system. In 1D the absorbing boundary\ncondition has the explicit form 1∂tu±∂xu = 0 for left (-) and right-going (+)\nc\nwaves respectively, and can be implemented with adequate differences (such\nas upwind in space and forward in time).\nThe grid spacing ∆x is typically chosen as a small fraction of the rep-\nresentative wavelength in the solution. The time step ∆t is limited by the\nCFL condition ∆t ≤∆x / maxx c(x), and is typically taken to be a fraction\nthereof.\nIn two spatial dimensions, the simplest discrete Laplacian is the 5-point\nstencil which combines the two 3-point centered schemes in x and in y. Its\naccuracy is also O(max{∆x)2, (∆y)2}). Designing good absorbing boundary\nconditions is a somewhat difficult problem that has a long history.\nThe\ncurrently most popular solution to this problem is to slightly expand the\ncomputational domain using an absorbing, perfectly-matched layer (PML).\nMore accurate schemes can be obtained from higher-order finite differ-\nences. Low-order schemes such as the one explained above typically suffer\nfrom unacceptable numerical dispersion at large times. If accuracy is a big\nconcern, spectral methods (spectral elements, Chebyshev polynomials, etc.)\nare by far the best way to solve wave equations numerically with a controlled,\nsmall number of points per wavelength.\n\nAppendix C\nStationary phase\nSee Stein's book Harmonic analysis [?], chapter 8, as a reference on station-\nary phase and for proofs of the claims below.\nIf an integrand has a phase factor with no stationary points, and the am-\nplitude is otherwise smooth, then the integral has a very small value because\nthe positive parts cancel out the negative parts. The following result makes\nthis heuristic precise as an asymptotic bound on the value of the integral\nwhen the phase has a large prefactor.\nLemma 4. (The non-stationary phase lemma.)\nLet χ ∈Cinf\n0 (Rn), φ ∈\nCinf(supp χ), and let\nIα =\nˆ\neiαφ(x)χ(x)dx.\nRn\nIf ∇φ(x) = 0 for all x ∈supp χ, then\n|Iα| ≤Cmα-m,\nfor all m > 0.\nProof. Integrate by parts after inserting an m-th power of the differential\noperator\nI -∆x\nL =\n,\n1 + α2|∇xφ(x)|2\nwhich leaves the exponential factor unchanged. A fortiori, 1+α2|∇xφ(x)|2 >\nCα2 for some number C > 0. Deal with the odd values of m by interpolation\n(geometric mean) from the m -1 and m + 1 cases.\nIf the phase otherwise has critical points, then the value of the integral\nis mostly determined by the behavior of the integrand near those critical\npoints.\n\nAPPENDIX C. STATIONARY PHASE\nLemma 5. Consider the same setting as earlier, but consider the presence\nof a point x∗such that\n∇φ(x∗) = 0,\nD2φ(x∗) invertible,\nwhere D2φ denotes the Hessian matrix of φ. Assume that ∇φ(x) = 0 for\nx = x∗. Then, as α →inf,\n2π\nIα =\nα\nn/2\nχ(x∗)eiαφ(x∗) ei π\n4 sgn(D2φ(x∗))\np\ndet(D2φ(x∗))\n+ O(α-n-1\n),\nwhere sgn denotes the signature of a matrix (the number of positive eigenval-\nues minus the number of negative eigenvalues.)\nSee [?] for a proof. More generally, if there exists a point x∗where all the\npartials of φ of order less than or equal to lvanish, but ∂lφ(x∗)/∂xl\n1 = 0 in\nsome direction x1, then it is possible to show that Iα = O(α-1/l).\nHere are a few examples.\n- A good example for the above lemma is\nˆ inf\neiαx2\ndx ∼\n-inf\n√.\nα\nThe real part of the integrand, cos(αx2), is non-oscillatory at the origin,\nbut develops significant oscillations as soon as x is on the order of\n√\n±1/\nα. The extent of the range over which the integrand essentially\ndoes not oscillate (e.g., as measured from the length of the first half\nperiod) determines the order of magnitude of the value of the integral.\n- An important case not immediately handled by any of the previous\nlemmas is the stationary phase explanation of the often-invoked fact\nthat1\n\nˆ\nˆ\nei(y-x)·ξdξ\nf(x)dx ∼f(y).\nRn\nRn\nThe large factor α of the stationary phase lemmas can be placed in the\nexponent as iα(y -x) · ξ. The rescaling ξ′ = αξ quickly helps to get\n1The actual value of the integral is (2π)nf(y). The function f is only required to be\ncontinuous with some decay at infinity for this relation to make sense pointwise. Fourier\nanalysis makes all of this precise, of course.\n\nrid of it by turning it into a multiplicative 1/αn factor for the integral\nabove. Hence the equivalent, stationary-phase-friendly formulation of\nthe relation above is really\n\nˆ\nˆ\neiα(y-x)·ξ\nf(y)\ndξ\nf(x)dx ∼\nRn\nRn\n.\nαn\nAs a function of ξ alone, the phase φ(ξ) = (y -x)·ξ has a critical point\nwhen x = y , but the Hessian is degenerate: φ′′(ξ) = 0. We cannot\napply any of the stationary phase lemmas to the integral on ξ alone.\nThe solution is to consider the double integral over x and ξ: the phase\nφ(x, ξ) = (y -x) · ξ is still critical when x = y, and now ξ = 0, but the\nHessian matrix is\n\n∇x∇xφ\n∇\nφ\nx∇\n=\nξφ\n-I\nD\n=\n,\n∇ξ∇xφ\n∇ξ∇ξφ\n-I\nwhich is invertible independently of the base point (x∗, ξ∗).\nHence\nlemma 5 applies in 2n dimensions, and actually predicts the exact value\nof the integral, namely (2π/α)nf(y). The condition y = x signifies that,\nof all the values of f(x), only that at x = y matters for the result of\nthe integral. The condition ξ = 0 is a manifestation of the fact that\nf(x) was assumed to be minimially smooth (hence it is fb(0) when ξ = 0\nthat matters). The function f may have oscillatory factors like ei 100 ψ(x)\nfor some other phase ψ, but no factors of the form eiαψ(x) involving α\nexplcitly.\n- Another interesting example is the integral\n\nˆ\nˆ\nei(y-x)·ξdξ\nˆ\neix·ηF(η)dη dx\nRn\nRn\nwhich often appears in Fourier analysis. It can be seen as the compo-\nsition of an inverse Fourier transform of F, from η to x, followed by a\nFourier transform, from x to ξ, followed by an inverse Fourier trans-\nform, from ξ to y. Indeed, the integral reduces to (an unimportant\nˇ\nmultiple of 2π times) F(y). For fixed η we can still see the phase as\nhaving two arguments, namely φ(x, ξ) = (y -x) · ξ + x · η, but the\nequations for the critical points now look more symmetric:\n∂φ\n∂φ\n= η -x = 0,\n∂x\n∂ξ = y -x = 0,\n\nAPPENDIX C. STATIONARY PHASE\nand D2φ is the same as previously. We now have x∗= y and ξ∗= η,\nso φ(x∗, ξ∗) = y · η. Stationary phase over the inner (x, ξ) variables\nthen reduces the outer η integral to (a constant times)\n\neiy·ηF(η)dη,\nas needed.\nThe relation η = ξ indicates that, in the course of the first two Fourier\ntransforms taking η to x, then to ξ, it is only the value of F at η =\nξ which matters to determine the result F(ξ).\nThe relation x = y\nindicates that, from the result f(x) of having done the first Fourier\ntransform from η to x, it is only the value f(y) at x = y which matters\nˇ\nto determine the end result f(y) = F(y).\nThe set of equations\nx = y,\nξ = η\nis a simple example of a so-called canonical relation in phase-space,\nthe space made of all the quadruples (x, ξ; y, η). In particular, it is\nprecisely the relation corresponding to the identity map from (x, ξ) to\n(y, η).\nThe adjective \"canonical\" refers to the fact that the map is\nsymplectic, i.e., preserves areas, which is instantiated in our context by\nthe fact that | det D2φ | = 1. Phase-space relations are introduced and\nused in chapter 8.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Geometrical Optics",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/ef8d8479be54561827f93c7006e2a040_MIT18_325F15_Chapter2.pdf",
      "content": "Chapter 2\nGeometrical optics\nThe material in this chapter is not needed for SAR or CT, but it is founda-\ntional for seismic imaging.\nFor simplicity, in this chapter we study the variable-wave speed wave\nequation\n\n(!)\nc2(x)\n∂2\n\n-∆\nu = 0.\n∂t2\nAs explained earlier, this equation models either constant-density acoustics\n(c2(x) is then the bulk modulus), or optics (cref/c(x) is then the index of\nrefraction for some reference level cref). It is a good exercise to generalize\nthe constructions of this chapter in the case of wave equations with several\nphysical parameters.\n2.1\nTraveltimes and Green's functions\nIn a uniform 3D medium, we have seen that the acoustic Green's function\n(propagator) is\nδ(ct -|x -y|)\nG(x, y, t) =\n.\n(2.1)\n4πc|x -y|\nIn a variable (smooth) medium c(x), we can no longer expect an explicit\nformula for G. However, to good approximation, the Green's function can\nbe expressed in terms of a progressing-wave expansion as\nG(x, y, t) = a(x, y)δ(t -τ(x, y)) + R(x, y, t),\n(2.2)\n\nCHAPTER 2. GEOMETRICAL OPTICS\nwhere a is some smooth amplitude function, τ is the so-called traveltime\nfunction, and R is a remainder which is not small, but smoother than a delta\nfunction.\nThe functions a and τ are determined by substituting the expression\nabove in the wave equation\n\nc2(x)\n∂2\n\n-∆x\nG(x, y, t) = 0,\nx =\ny,\n∂t2\nand equating terms that have the same order of smoothness. By this, we\nmean that a δ(x) is smoother than a δ′(x), but less smooth than a Heaviside\nstep function H(x). An application of the chain rule gives\n\nc2(x)\n∂2\n∂t2 -∆x\n\nG = a\n\nc2(x) -|∇xτ|2\n\nδ′′(t -τ)\n+ (2∇xτ · ∇xa -a∆xτ) δ′(t -τ)\n+ ∆xaδ(t -τ) +\n\nc2(x)\n∂2\n\n-∆x\nR.\n∂t2\nThe δ′′ term vanishes if, and in the case a = 0, only if\n|∇xτ(x, y)| =\n,\n(2.3)\nc(x)\na very important relation called the eikonal equation for τ. It determines τ\ncompletely for x in some neighborhood of y. Notice that τ has the units of\na time.\nThe δ′ term vanishes if and only if\n2∇xτ(x, y) · ∇xa(x, y) -a(x, y)∆xτ(x, y) = 0,\n(2.4)\na relation called the transport equation for a. It determines a up to a mul-\ntiplicative scalar, for x in a neighborhood of y.\nAs for the term involving δ, it is a good exercise (see end of chapter) to\ncheck that the multiplicative scalar for the amplitude a can be chosen so that\nthe solution R of\n\n∆xa(x, y)δ(t -τ(x, y)) +\nc2(x)\n∂2\n∂t2 -∆x\n\nR = δ(x -y)δ(t)\n\n2.1. TRAVELTIMES AND GREEN'S FUNCTIONS\nis smoother than G itself. A good reference for progressing wave expansions\nis the book \"Methods of Mathematical Physics\" by Courant and Hilbert (pp.\n622 ff. in volume 2).\nThis type of expansion for solutions of the wave equation is sometimes de-\nrived in the frequency domain ω rather than the time domain t. In that case,\nit often takes on the name geometrical optics. Taking the Fourier transform\nof (2.2), we get the corresponding Ansatz in the ω domain:\nGb(x, y, ω) =\nˆ\neiωtG(x, y, t) dt = a(x, y)eiωτ(x,y) + Rb(x, y, ω).\n(2.5)\nBecause τ appears in a complex exponential, it is also often called a phase.\nThe same exercise of determining a and τ can be done, by substituting this\nexpression in the Helmholtz equation, with the exact same outcome as earlier.\nInstead of matching like derivatives of δ, we now match like powers of ω. The\nω2 term is zero when the eikonal equation is satisfied, the ω term is zero when\nthe transport equation is satisfied, etc.\nDoing the matching exercise in the frequency domain shows the true\nnature of the geometrical optics expression of the Green's function: it is a\nhigh-frequency approximation.\nLet us now inspect the eikonal equation for τ and characterize its solu-\ntions. In a uniform medium c(x) = c0, it is easy to check the following two\nsimple solutions.\n- With the condition τ(y, y) = 0, the solution is the by-now familiar\n|x -y|\nτ(x, y) =\n,\nc0\nwhich defines a forward light cone, (or -|x-y|, which defines a backward\nc0\nlight cone,) and which helps recover the phase of the usual Green's\nfunction (2.1) when plugged in either (2.2) or (2.5).\n- This is however not the only solution. With the condition τ(x) = 0\nfor x1 = 0 (and no need for a parameter y), a solution is τ(x) = |x1|\nc0 .\nAnother one would be τ(x) = x1.\nc0\nFor more general boundary conditions of the form τ(x) = 0 for x on some\ncurve Γ, but still in a uniform medium c(x) = c0, τ(x) takes on the interpre-\ntation of the distance function to the curve Γ.\n\nCHAPTER 2. GEOMETRICAL OPTICS\nNote that the distance function to a curve may develop kinks, i.e., gradi-\nent discontinuities. For instance, if the curve is a parabola x2 = x2\n1, a kink\nis formed on the half-line x = 0, x\n2 ≥\nabove the focus point. This com-\nplication originates from the fact that, for some points x, there exist several\nsegments originating from x that meet the curve at a right angle. At the\nkinks, the gradient is not defined and the eikonal equation does not, strictly\nspeaking, hold. For this reason, the eikonal equation is only locally solvable\nin a neighborhood of Γ. To nevertheless consider a generalized solution with\nkinks, mathematicians resort to the notion of viscosity solution, where the\nequation\n= |∇xτε|2 + ε2∆xτε\nc2(x)\nis solved globally, and the limit as ε →0 is taken. Note that in the case\nof nonuniform c(x), the solution generically develops kinks even in the case\nwhen the boundary condition is τ(y, y) = 0.\nIn view of how the traveltime function appears in the expression of the\nGreen's function, whether in time or in frequency, it is clear that the level\nlines\nτ(x, y) = t\nfor various values of t are wavefronts. For a point disturbance at y at t = 0,\nthe wavefront τ(x, y) = t is the surface where the wave is exactly supported\n(when c(x) = c0 in odd spatial dimensions), or otherwise essentially sup-\nported (in the sense that the wavefield asymptotes there.) It is possible to\nprove that the wavefield G(x, y, t) is exactly zero for τ(x, y) > t, regardless\nof the smoothness of c(x), expressing the idea that waves propagate no faster\nthan with speed c(x).\nFinally, it should be noted that\nφ(x, t) = t -τ(x, y)\nis for each y (or regardless of the boundary condition on τ) a solution of the\ncharacteristic equation\n∂ξ2\n= |∇xξ|2,\n∂t\ncalled a Hamilton-Jacobi equation, and already encountered in chapter 1.\nHence the wavefronts t -τ(x, y) = 0 are nothing but characteristic surfaces\nfor the wave equation. They are the space-time surfaces along which the\nwaves propagate, in a sense that we will make precise in section 8.1.\n\n2.2. RAYS\n2.2\nRays\nWe now give a general solution of the eikonal equation, albeit in a somewhat\nimplicit form, in terms of rays. The rays are the characteristic curves for the\neikonal equation. Since the eikonal equation was already itself characteristic\nfor the wave equation (see the discussion at the end of the preceding section),\nthe rays also go by the name bicharacteristics.\nThe rays are curves X(t) along which the eikonal equation is simplified, in\nthe sense that the total derivative of the traveltime has a simple expression.\nFix y and remove it from the notations. We write\nd\n\nτ(X(t)) = X(t) · ∇τ(X(t)).\n(2.6)\ndt\nThis relation will simplify if we define the ray X(t) such that\n- the speed | X(t)| is c(x), locally at x = X(t);\n-\n\nthe direction of X(t) is perpendicular to the wavefronts, i.e., aligned\nwith ∇τ(x) locally at x = X(t).\nThese conditions are satisfied if we specify the velocity vector as\n∇τ(X(t))\nX(t) = c(X(t))\n.\n(2.7)\n|∇τ(X(t))|\nSince the eikonal equation is |∇τ(x)| = 1/c(x), we can also write\nX(t) = c2(X(t))∇τ(X(t)).\n\nUsing either expression of X(t) in (2.6), we have\nd τ(X(t)) = 1,\ndt\nwhich has for solution\nτ(X(t)) -τ(X(t0)) = t -t0.\nWe now see that τ indeed has the interpretation of time. Provided we can\nsolve for X(t), the formula above solves the eikonal equation.\nThe differential equation (2.7) for X(t) is however not expressed in closed\nform, because it still depends on τ. We cannot however expect closure from\n\nCHAPTER 2. GEOMETRICAL OPTICS\na single equation in X(t). We need an auxiliary quantity that records the\ndirection of the ray, such as\nξ(t) = ∇τ(X(t)).\nThen (all the functions of x are evaluated at X(t))\nξ(t) = ∇∇τ · X(t)\n= ∇∇τ(X(t)) · c2∇τ\nc2\n= 2 ∇|∇τ|2\n= c2\n∇c-2\nc-2\n= -\n∇c2\n|∇τ|2\n= -\n∇c2\n= -|ξ(t)|2\n∇(c2)(X(t)).\nWe are now in presence of a closed, stand-alone system for the rays of geo-\nmetrical optics in the unknowns X(t) and ξ(t):\n(\nX(t)\n= c2(X(t)) ξ(t),\nX(0) = x0,\nξ(t)\n= -∇(c2)(X(t)) |ξ(t)|2,\nξ(0) = ξ0.\nThe traveltime function τ(x) is equivalently determined as the solution of\nthe eikonal equation (the Eulerian viewpoint), or as the time parameter for\nthe ray equations (the Lagrangian viewpoint). While X is a space variable,\ntogether (X, ξ) are called phase-space variables.\nIt is fine to speak of a\ncurve X(t) in space as a ray, although strictly speaking the ray is a curve\n(X(t), ξ(t)) in phase-space. Because of its units, ξ is in this context often\ncalled the slowness vector.\nThe system above is called Hamiltonian because it can be generated as\n\nX(t)\n= ∇ξH(X(t), ξ(t)),\nξ(t)\n= -∇xH(X(t), ξ(t)),\nfrom the Hamiltonian\nH(x, ξ) = 2c2(x)|ξ|2.\n\n2.3. AMPLITUDES\nThis is the proper Hamiltonian for optics or acoustics; the reader is already\np2\naware that the Hamiltonian of mechanics is H(x, p) =\n+ V (x). Note that\n2m\nH is a conserved quantity along the rays1\nIt can be shown that the rays are extremal curves of the action functional ($)\nS(\n) =\nˆ b\nX\na c(x)dl=\nˆ 1\n| X(t)|dt,\ns.t.\nX(0) = a, X(1) = b,\nc(X(t))\na result called the Fermat principle. For this reason, it can also be shown\nthat the rays are geodesics curves in the metric\n($)\nX\nds2 = c-2(x)dx2,\ndx2 =\ndxi ∧dxi.\ni\nThe traveltime τ therefore has yet another interpretation, namely that of\naction in the variational Hamiltonian theory2.\nInspection of the ray equations now gives another answer to the question\nof solvability of τ from the eikonal equation. There is no ambiguity in spec-\nifying τ from |∇τ(x, y)| = 1/c(x) and τ(y, y) = 0 as long as there is a single\nray linking y to x. When there are several such rays -- a situation called mul-\ntipathing -- the traveltime function takes on multiple values τj(x, y) which\neach solve the eikonal equation locally. The function that records the \"num-\nber of arrivals\" from y to x has discontinuities along curves called caustics;\nthe respective eikonal equations for the different branches τj hold away from\ncaustics. The global viscosity solution of the eikonal equation only records\nthe time of the first arrival.\n2.3\nAmplitudes\nWe can now return to the equation (2.4) for the amplitude, for short\n2∇τ · ∇a = -a∆τ.\nIt is called a transport equation because it turns into an ODE in characteristic\ncoordinates, i.e., along the rays.\nAgain, all the functions of x should be\n1So is the symplectic 2-form dx ∧dξ, hence areas are conserved as well.\n2There is no useful notion of Lagrangian in optics, because the photon is massless.\nSee the book on Mathematical methods of classical mechanics by Arnold and the treatise\nby Landau and Lifschitz for the fascinating analogy between the equations of optics and\nLagrangian/Hamiltonian mechanics.\n\nCHAPTER 2. GEOMETRICAL OPTICS\nevaluated at X(t) in the following string of equalities:\nd\n\na(X(t)) = X(t) · ∇a\ndt\n= c2∇τ · ∇a\nc2\n= -\na ∆τ.\nIf τ is assumed known, then this equation specifies a(X(t)) up to a multi-\nplicative constant. If we wish to eliminate τ like we did earlier for the rays,\nthen we need to express ∆τ(X(t)) not just in terms of X(t) and ξ(t), but\nalso in terms of the first partials\n∂X\n∂X0(t), ∂X\n∂ξ0(t),\n∂ξ\n∂X0(t),\n∂ξ (t) with respect to\n∂ξ0\nthe initial conditions3.\nThe transport equation can also be written in divergence form,\n∇· (a2∇τ) = 0,\nwhich suggests that there exists an underlying conserved quantity, which\nintegration will reveal. Assume for now that space is 3-dimensional. Consider\na ray tube R, i.e, an open surface spanned by rays. Close this surface with two\ncross-sections S+ and S-normal to the rays. Apply the divergence theorem\nin the enclosed volume V . This gives\n0 =\n\n∇· (a2∇τ)dV =\n‹\na2∇τ · ndS,\nV\n∂V\nwhere n is the outward normal vector to the surface ∂V = R ∪S+ ∪S-.\n- For x on R, the normal vector n is by definition (of R) perpendicular\nto the ray at x, hence ∇τ · n = 0.\n- For x on S±, the normal vector n is parallel to the ray at x, hence\n∇τ · n = ±|∇τ|.\nAs a result,\nˆ\na2|∇τ|dS =\nS+\nˆ\na2|∇τ|dS,\nS-\n3See for instance the 2006 paper by Candes and Ying on the phase-flow method for\nthese equations.\n\n2.4. CAUSTICS\nthus\nˆ\na2\nS+ c dS =\nˆ\nS-\na2\ndS.\nc\nThis relation is an expression of conservation of energy. Passing to an in-\nfinitesimally thin ray tube linking x0 to x, it becomes\ns\na(x) = a(x0)\nc(x0)\nc(x)\ndS .\ndS0\nIt is again clear that the amplitude is determined up to a multiplicative scalar\nfrom this equation. A similar argument can be made in 2 space dimensions,\nand leads to the same conclusion with the ratio of line elements ds/ds0 in\nplace of the ratio of surface elements dS/dS0.\nExamples of solutions in a uniform medium in R3 include\n- Plane waves, for which dS/dS0 = 1 hence a = constant,\n- Cylindrical waves about r = 0, for which dS/dS0 = r/r0 hence a ∼\n√\n1/\nr,\n- Spherical waves about r = 0, for which dS/dS\n0 = (r/r0) hence a ∼\n1/r,\n- A cylindrical focus or a caustic point at r = a can generically be seen\n√\nas a time-reversed cylindrical wave, hence a ∼1/\nr -a. A spherical\nfocus is a geometrical exception; it would correspond to a ∼1/(r -a).\nIn the infinite-frequency geometrical optics approximation, the amplitude\nindeed becomes infinite at a focus point or caustic curve/surface. In reality,\nthe amplitude at a caustic is an increasing function of the frequency ω of\nthe underlying wave. The rate of growth is generically of the form ω1/6, as\nestablished by Keller in the 1950s.\nCaustics and focus points give rise to bright spots in imaging datasets,\nalthough this information is probably never explicitly used in practice to\nimprove imaging.\n2.4\nCaustics\nFor fixed t, and after a slight change of notation, the Hamiltonian system\ngenerates the so-called phase map (x, ξ) 7→(y(x, ξ), η(x, ξ)). Its differential\n\nCHAPTER 2. GEOMETRICAL OPTICS\nis block partitioned as\n\n∂y\ny\n∇(x,ξ)\n=\nη\n∂x\n∂y\n∂ξ\n∂η\n∂x\n∂η\n!\n.\n∂ξ\nBesides having determinant 1, this Jacobian matrix has the property that\nX ∂ηj\nδkl =\nj\n∂ξl\n∂yj\n∂xk\n-∂ηj\n∂xk\n∂yj .\n(2.8)\n∂ξl\nThis equation is conservation of the second symplectic form dη∧dy = dξ∧dx\nwritten in coordinates. It also follows from writing conservation of the first\nsymplectic form ξ · dx = η · dy as\nX\n∂yi\nξj =\nηi\ni\n∂xj\n,\n0 =\nX\ni\nηi\n∂yi ,\n∂ξj\nand further combining these expressions. (dη ∧dy = dξ ∧dx also follows\nfrom ξ ·dx = η ·dy by Cartan's formula). Equation (2.8) can also be justified\ndirectly, see the exercise section. Note in passing that it is not a Poisson\nbracket.\nIt is instructive to express (2.8) is ray coordinates. Without loss of gen-\nerality choose the reference points x0 = 0 and ξ0 = (1, 0)T. Let x1 and ξ1 be\nthe coordinates of x and ξ along ξ0, and x2, ξ2 along ξ⊥\n0 . Consider (y0, η0) the\nimage of (x0, ξ0) under the phase map. Let y1 and y2 be the coordinates of y\nand η along η0, and x2, ξ2 along η⊥\n0 . In other words, the coordinates labeled\n\"1\" are along the ray (longitudinal), and \"2\" across the ray (transversal).\nIn two spatial dimensions, only the coordinates across the ray give rise to\na nontrivial relation in (2.8). One can check that the k = l = 2 element of\n(2.8) becomes\n∂η2\n1 = ∂ξ2\n∂y2\n∂x2\n-∂η2\n∂x2\n∂y2.\n(2.9)\n∂ξ2\nWhen either of the terms in this equation vanish, we say that (y, η) is on a\ncaustic. Two regimes can be contrasted:\n- If ∂y2/∂x2 = 0, we are in presence of a \"x-caustic\". This means that, as\nthe initial point x is moved infinitesimally in a direction perpendicular\nto the take-offdirection, the resulting location y does not move. A\nx-caustic is at the tip of a swallowtail pattern formed from an initial\nplane wavefront.\n\n2.5. EXERCISES\n- If ∂y2/∂ξ2 = 0, we are in presence of a \"ξ-caustic\". This means that,\nas the initial direction angle arg ξ changes infinitesimally, the resulting\nlocation y does not move. A ξ-caustic is at the tip of a swallowtail\npattern formed from an initial point wavefront.\nEquation (2.9) shows that these two scenarios cannot happen simultaneously.\nIn fact, if the variation of y2 with respect to x2 is zero, then the variation\nof y2 with respect to ξ2 must be maximal to compensate for it; and vice-\nversa. When t is the first time at which either partial derivative vanishes,\nthe caustic is a single point: we speak of a focus instead.\nNotice that ∂η2/∂x2 = 0 and ∂η2/∂ξ2 = 0 are not necessarily caustic\nevents; rather, they are inflection points in the wavefronts (respectively ini-\ntially plane and initially point.)\n2.5\nExercises\n1. (Difficult) Show that the remainder R in the progressing wave expan-\nsion is smoother than the Green's function G itself.\n2. In this exercise we compute the Fr echet derivative of traveltime with\nrespect to the wave speed. For simplicity, let n(x) = 1/c(x).\nx\n(a) In one spatial dimension, we have already seen that τ(x) =\nn(x′)dx′.\nx0\nFind an expression for δτ(x)/δn(y) (or equivalently for the\n\noper-\nator that it generates via ⟨δτ(x)/δn, h⟩for a test function h).\n(b) In several spatial dimensions, τ(x) obeys |∇τ(x)| = n(x) with\nτ(0) = 0, say. First, show that δτ(x)/δn(y) obeys a transport\nequation along the rays. Then solve this equation. Provided there\nis one ray between 0 and x, argue that δτ(x)/δn(y), as a function\nof y, is concentrated along this ray.\n(c) What do your answers become when the derivative is taken with\nrespect to c(x) rather than n(x)?\nThe function δτ(x)/δn(y) of y is often called sensitivity kernel (of τ\nwith respect to n). It's a distribution, really.\n3. Show that the Hamiltonian is conserved along the trajectories of a\nHamiltonian system.\n\nCHAPTER 2. GEOMETRICAL OPTICS\n4. Show that the alternative Hamiltonian H(x, ξ) = c(x)|ξ| generates an\nequivalent system of ODEs for the rays.\n5. Show that the rays are circular in a linear wave velocity model, i.e.,\nc(x) = z in the half-plane z > 0. Note: {z > 0} endowed with ds2 =\ndx2+dz2\nob\nz2\nis called the Poincar e half-plane, a very important\nject in\nmathematics.\n6. Show that the traveltime τ is convex as a function of the underlying\nmedium c(x), by invoking the Fermat principle.\n7. Prove (2.8).\nHint. Show it holds at time zero, and use the Hamiltonian structure\nto show that the time derivative of the whole expression is zero.\nP\n8. Let {y(x, ξ), η(x, ξ)} be the fixed-time phase map. Show that\n∂ηi\ni ∂ξk\n∂yi\n∂ξl\nis symmetric.\nHint. Same hint as above. Show that the time derivative of the dif-\nference of the matrix and its transpose vanishes.\n9. Let τ(x, y) be the 2-point traveltime, and let {y(x, ξ), η(x, ξ)} be the\nfixed-time phase map for the Hamiltonian of isotropic optics. Prove or\ndisprove:\n(a)\nX ∂yk\nk\n∂ξj\n∂τ (x, y(x, ξ)) = 0;\n∂yk\n(b)\nX ∂ηi\nk\n∂ξk\n∂2τ\nX ∂yk\n(x, y(x, ξ)) +\n∂xj∂xk\nk\n∂xj\n∂2τ\n(x, y(x, ξ)) = 0.\n∂yi∂yk\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Microlocal Analysis of Imaging",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/d7a4d8449f9122e52aacc2858eb0bd51_MIT18_325F15_Chapter8.pdf",
      "content": "Chapter 8\nMicrolocal analysis of imaging\nIn this chapter we consider that the receiver and source positions (xr, xs) are\nrestricted to a vector subspace Ω⊂R6, such as R2 × R2 in the simple case (!)\nwhen xr,3 = xs,3 = 0. We otherwise let xr and xs range over a continuum of\nvalues within Ω, which allows us to take Fourier transforms of distributions\nof xr and xs within Ω. We call Ωthe acquisition manifold, though in the\nsequel it will be a vector subspace for simplicity1.\n8.1\nPreservation of the wavefront set\nIn this section we show that, in simple situations, the operators F and F ∗de-\nfined earlier have inverse kinematic behaviors, in the sense that the respective\nmappings of singularities that they induce are inverses of one another.\nThe microlocal analysis of F and F ∗hinges on the mathematical notion\nof wavefront set that we now introduce. We start by describing the charac-\nterization of functions with (or without) singularities.\nNonsingular functions are infinitely differentiable, hence have Fourier\ntransforms that decay \"super-algebraically\".\nLemma 1. Let u ∈Cinf\n0 (Rn). Then, for all N > 0,\n|ub(k)| ≤CN(1 + |k|2)-N.\n1The Fourier transform can still be defined for functions on general manifolds, but\nthis involves patches, a partition of unity, and many distractions that we prefer to avoid\nhere. The microlocal theory presented here carries over mostly unchanged to the manifold\nsetting.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nProof. In the Fourier integral defining ub(k), insert N copies of the differential\noperator L =\nI-∆x\nof\nix\n|\nmeans\nk|2 by\nthe identity e\n·k = LNeix·k. Integrate by\n1+\nparts, pull out the (1 + |k|2)-N factor, and conclude by boundedness of u(x)\nand all its derivatives.\nA singularity occurs whenever the function is not locally Cinf.\nDefinition 2. Let u ∈S′(Rn), the space of tempered distributions on Rn.\nWe define the singular support of u as\nsing supp(u) = {x ∈Rn : there does not exist an open\nneighborhood V of x such that u ∈Cinf(V )}.\nIn addition to recording where the function is singular, we also want to\nrecord the direction in which it is singular. This idea gives rise to the notion\nof wavefront set, that we now build up to.\n\"Direction of singularity\" is associated to lack of decay of the Fourier\ntransform in the same direction. For this purpose it is useful to consider sets\ninvariant under rescaling, i.e., cones.\nDefinition 3. A set X is said to be a conic neighborhood of a set Y ⊂Rn if\n- X is open;\n- Y ⊂X;\n- ξ ∈X implies λξ ∈X for all λ > 0.\nDefinition 4. The singular cone of a tempered distribution u ∈S′(Rn) is\nthe set\nΓ(u) = {η ∈Rn\\{0} : there does not exist a conic\nneighborhood W of η such that for ξ ∈W,\n|ub(k)| ≤CN(1 + |k|2)-N,\nfor all N > 0.}\nThe intuition is that Γ(u) records the set of directions ξ in which the\nFourier transform of u decays slowly. The reason for formulating the defini-\ntion in negative terms is that we want the resulting cone to be closed, i.e.,\nisolated directions in which the Fourier transform would accidentally decay\nquickly, while decay is otherwise slow along an open set of nearby directions,\ndo not count.\n\n8.1. PRESERVATION OF THE WAVEFRONT SET\nThe construction of Γ(u) is global - a direction is labeled singular with-\nout regard to the locations x where there are singularities that may have\ncontributed to the direction being labeled singular. To go further we need to\nlocalize the construction.\nLemma 2. Let φ ∈Cinf(Rn) and u ∈S′(Rn\n). Then Γ(φu) ⊂Γ(u).\nDefinition 5. Let x ∈X ⊂Rn. We call singular fiber at x the set\n\\\nΓx(u) = {\nΓ(φu) : φ ∈Cinf\n0 (X), φ(x) = 0}.\nφ\nThe idea of this definition is that we localize u by means of multiplica-\ntion with a smooth function φ of arbitrarily small support, then consider the\nsmallest resulting singular cone. The definition can equivalently be formu-\nlated by means of a family of smooth indicators whose support converges\ntoward the singleton {x} in the sense of sets:\nΓx(u) = { lim Γ(φju) : φj ∈Cinf\n0 (X), φj(x) = 0, supp φj →{x} }.\nj→inf\nNote that Γx(u) is empty if u is smooth (Cinf) at x.\nThe wavefront set then consists of the union of the singular cones, taken\nas fibers over the singular support.\nDefinition 6. Let u ∈S′(Rn). The wavefront set of u is the set\nWF(u) = {(x, ξ) ∈Rn × (Rn\\{0}) : ξ ∈Γx(u)}.\nThe set Rn × (Rn\\{0}) is also abbreviated as T ∗Rn\\0.\nBecause this definition localizes the singularites of u both in position and\nin direction, it is the basis for microlocal analysis.\nIn this context, the product Rn × Rn is the cotangent bundle of Rn, and\ndenoted T ∗Rn. The reason for viewing each singular cones (each fiber) as a\nsubset of the cotangent space at x rather than the tangent space at x will be\napparent in the sequel as we study the behavior of WF(u) under change of\nvariables.\nNotice that the projection of WF(u) on the first Rn (relative to x) is the\nsingular support sing supp(u), while the projection of WF(u) on the second\nRn (relative to ξ) is the singular cone Γ(u).\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nIn the context of imaging, we will use the same notation as introduced\nabove and let ξ be the wave-vector variable, dual to position x ∈Rn in\nmodel space. Usually, n = 3. Data space is indexed by the base variables\n(xs, xr, t) ∈Ω× R ≡Rnd, usually with nd = 5. We denote by (ξr, ξs, ω) the\ncorresponding dual variables, i.e., the independent variables of the Fourier\ntransform of a function of (xs, xr, t).\nThe wavefront set of a distribution\nd(xr, xs, t) in Rnd is then\nWF(d) = {(xr, x , t; ξ\nd\ns\n, ξ\n∗\nn\nr\ns, ω) ∈T R\n\\0 : (ξr, ξs, ω) ∈Γ(xr,xs,t)(d)}.\nWe will also consider wavefront sets of functions in the product space\nRn × Rnd = Rn+nd of model space and data space, such as the distributional\nintegral kernel K of F. We have\nWF(K) = {(xr, xs, t, x; ξr, ξs, ω, ξ) ∈T ∗Rn+nd\\0 : (ξr, ξs, ω, ξ) ∈Γ(xr,xs,t,x)(K)}.\n(8.1)\nIn a product space, a wavefront set has the additional interpretation of being\na relation. The role of WF(K), where K is the kernel of F, is that it provides\na way of predicting WF(Fm1) from WF(m1) using a simple composition\nrule.\nIn order to see WF(K) as a relation, however, two minor modifications\nneed to be made to the definition (8.1). First, the variables first need to be\nre-arranged as ((x, ξ), (xr, xs, t; ξr, ξs, ω)), in order to be seen as elements of\nthe product space T ∗Rn × T ∗Rnd. Second, the sign of the first co-variable\nξ needs to be flipped, so that we instead consider the \"wavefront prime\"\nWF ′(K). In short,\n((x, -ξ), (xr, xs, t; ξr, ξs, ω)) ∈WF ′(k)\n⇔\n(xr, xs, t, x; ξr, ξs, ω, ξ) ∈WF(K)\nWe call WF ′(K) the wavefront relation of the operator whose kernel is K.\nMore generally, the operations that define relations are as follows.\n- The composition of a relation C ⊂X × Y and a set S ⊂X is the set\nC *S = {y ∈Y : there exists x ∈S such that (x, y) ∈C}.\n- The composition of two relations C ⊂X × Y and C′ ∈Y × Z is the\nrelation\nC *C′ = {(x, z) ∈X × Z : there exists y ∈Y\nsuch that (x, y) ∈C, (y, z) ∈C′}.\n\n8.1. PRESERVATION OF THE WAVEFRONT SET\n- The transposition of a relation C is the relation\nCT = {(y, x) ∈Y × X : (x, y) ∈C}.\nWe call identity relation on X the set I = {(x, x) : x ∈ X}. Note that\nall our relations are \"canonical\" in the sense that they preserve areas in a\ngeneralized sense2. We will not purse this topic further here.\nThe high-level idea is that we would like to write WF(Fm1) ⊂WF ′(K)*\nWF(m1), but this identity is not always true.\nThe obstruction is that\nelements of WF ′(K) are in T ∗Rn×nd\\0, which is in general a larger set\nthan (T ∗Rn\\0) × (T ∗Rnd\\0). Hence the interpretation of WF ′(K) as a re-\nlation runs into trouble with those elements such that either ξ = 0 but\n(ξs, ξr, ω) = 0; or conversely (ξs, ξr, ω) = 0 but ξ = 0. We say such elements\nare part of the zero sections of WF ′(K). In the sequel, we simply treat the\ncase when those zero sections are empty.\nThe proper way of composing wavefront sets is the following celebrated\ntheorem, due to H ormander.\nTheorem 5. Let K be the distributional kernel of an operator F, with wave-\nfront set (8.1). Assume that the elements of WF ′(K) obey\nξ = 0\n⇔\n(ξs, ξr, ω) = 0,\ni.e., WF ′(K) does not have zero sections. Then, for all m1 in the domain of\nF,\nWF(Fm1) ⊂WF ′(K) *WF(m1).\nThe physical significance of the zero sections will be made clear below\nwhen we compute WF(K) for the imaging problem.\nWe can now consider the microlocal properties of the imaging operator\nF ∗. Its kernel is simply KT, the transpose of the kernel K of F. In turn, the\nrelation WF ′(KT) is simply the transpose of WF ′(K) seen as a relation in\nthe product space T ∗Rn × T ∗Rnd,\nWF ′(KT) = (WF ′(K))T.\n2Namely, they are Lagrangian manifolds: the second fundamental symplectic form\nvanishes when restricted to canonical relations. The precaution of flipping the sign of the\nfirst covariable ξ in the definition of WF ′(K) translates into the fact that, in variables\n((x, ξ), (y, η)), it is dx ∧dξ -dy ∧dη that vanishes when restricted to the relation, and not\ndx ∧dξ + dy ∧dη.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nOur interest is in whether the normal operator F ∗F preserves the sin-\ngularities of the function m1 it is applied to, i.e., whether imaging \"places\"\nsingularities of the model perturbation at the right location from the knowl-\nedge of the linearized data Fm1. To study the mapping of singularities of\nthe normal operator F ∗F, we are led to the composition\nWF(F ∗Fm ) ⊂(WF ′\nT\n(K)) *WF ′(K) *WF(m1).\nup to the same precaution involving zero-sections as above. The question is\nnow whether the transpose relation can be considered an inverse, i.e., whether\n(WF ′(K))T *WF ′(K) is a subset of the identity on T ∗Rn. A simple condition\nof injectivity is necessary and sufficient for this to be the case.\nDefinition 7. We say a relation C ⊂X × Y is injective if\n(x1, y) ∈C, (x2, y) ∈C\n⇒\nx1 = x2.\nAs a map from X to Y , an injective relation may be multivalued, but as\na map from Y to X, the transpose relation is a graph.\nLemma 3. A relation C is injective if and only if CT *C ⊂I.\nProof. Assume that C ⊂X×Y is injective, and let S ⊂X. By contradiction,\nif CT *C were not a subset of I, then there would exist an element x of\nX, for which there exists an element x′ of CT *C *x such that x′ = x. By\ndefinition of CT *C, this means that there exists y ∈Y such that (y, x′) ∈CT\nand (x, y) ∈C. By definition of transpose relation, we have (x′, y) ∈C.\nInjectivity of C implies x = x′, a contradiction.\nBy contraposition, assume that C ⊂X × Y is not injective, i.e., there\nexist two elements (x, y) and (x′, y) of C for which x = x′. Then (y, x′) ∈CT\nby definition of transpose, and (x, x′) ∈CT *C by definition of composition.\nSince x = x′, CT *C is not contained in the identity relation I = {(x, x) :\nx ∈X}.\nWe have all the pieces to gather the main result on preservation of sin-\ngularities.\nTheorem 6. Let K be the distributional kernel of an operator F, with wave-\nfront set (8.1). Assume that the elements of WF ′(K) obey\nξ = 0\n⇔\n(ξs, ξr, ω) = 0,\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\ni.e., WF(K) does not have zero sections. Further assume that WF ′(K) is\ninjective as a relation in (T ∗Rn\\0) × (T ∗Rnd\\0). Then, for all m1 in the\ndomain of F,\nWF(F ∗Fm1) ⊂WF(m1).\nProof. Notice that the zero sections of (WF ′(K))T are the same as those of\nWF ′(K), hence H ormander's theorem can be applied twice. We get\nWF(F ∗Fm1) ⊂(WF ′(K))T *WF ′(K) *WF(m1).\nUnder the injectivity assumption, lemma 3 allows to conclude.\nThe assumptions of the theorem are tight:\n- If zero sections are present, the composition law is different and adds\nelements away from the diagonal, as shown in the general formulation\nof H ormander's theorem in [?].\n- If injectivity does not hold, lemma 3 shows that the composition of two\nrelations CT *C must have non-diagonal components.\n8.2\nCharacterization of the wavefront set\nIn this section we construct the wavefront relation WF ′(K) explicitly in the\nsimple scenario of section 7.2.\nRecall that the setting of section 7.2 is one in which the background\nmodel m0(x) is heterogeneous and smooth, and enjoys no multipathing in\nthe zone of interest. The source wavelet w(t) (entering the right-hand side (!)\nof the wave equation) still needs to be an impulse δ(t) in order to create (!)\npropagating singularities. If instead the wavelet is essentially bandlimited, then so\nwill the wavefields, but there are still remnants of the microlocal theory in the\nmagnitude and locality of the propagating \"wiggles\"3.\nThe distributional kernel of GRT migration is then\nK(x; x\n′′\nr, xs, t) = a(x, xr, xs)δ (t -τ(xr, x, xs)),\nwhere τ(xr, x, xs) = τ(xr, x) + τ(x, xs) is the three-point traveltime, and a is\na smooth amplitude, except for a singularity at x = xr and x = xs.\n3Microlocal theory can be expressed scale-by-scale, with wave packet decompositions\nand/or the so-called FBI transform. We leave out this topic.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nThe singular support of K is the same as its support as a measure, namely\nsing supp(K) = {(x, xr, xs, t) : t = τ(xr, x, xs)}.\nIn what follows we keep in mind that, for (x, xr, xs, t) to be a candidate\nbase point in the wavefront relation of K, i.e., with a non-empty fiber, it is\nnecessary that t = τ(xr, x, xs).\nTo find this wavefront relation, we first localize K around some refer-\nence point (x0, xr,0, xs,0, t0) by multiplication with increasingly sharper cutoff\nfunctions, such as\nχ(∥x -x0∥) χ(∥xr -xr,0∥) χ(∥xs -xs,0∥) χ(|t -t0|),\nwhere χ is a Cinf\nfunction compactly supported in the ball Bε(0) for some\nε that tends to zero. To keep notations manageable in the sequel, we in-\ntroduce the symbol [ χ ] to refer to any Cinf\nfunction of (x, xr, xs, t) (or any\nsubset of those variables) with support in the ball of radius ε centered at\n(x0, xr,0, xs,0, t0) (or any subset of those variables, resp.).\nWe then take a Fourier transform in every variable, and let\nI(ξ, ξ , ξ , ω) =\nˆˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωt)\nr\ns\nK(x; xr, xs, t)[ χ ]dx dxr dxs dt.\n(8.2)\nAccording to the definition in the previous section, the test of member-\nship in WF ′(K) involves the behavior of I under rescaling (ξ, ξr, ξs, ω) →\n(αξ, αξr, αξs, αω) by a single number α > 0. Namely,\n((x0; ξ), (xr,0, xs,0, t0; ξr, ξs, ω)) ∈/ WF ′(K)\nprovided there exists a sufficiently small ε > 0 (determining the support of\n[ χ ]) such that\nI(αξ, αξr, αξs, αω) ≤Cmα-m\nfor all m > 0.\nNotice that the Fourier transform in (8.2) is taken with a minus sign in the\nξ variable (hence eix·ξ instead of e-ix·ξ) because WF ′(K) - the one useful for\nrelations - precisely differs from WF(K) by a minus sign in the ξ variable.\nBefore we simplify the quantity in (8.2), notice that it has a wave packet\ninterpretation: we may let\nφ(x) = e-ix·ξχ(∥x -x0∥),\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\nψ(x , x .t) = e-i(xr·ξr+xs·ξs+ωt)\nr\ns\nχ(∥xr -xr,0∥) χ(∥xs -xs,0∥) χ(|t -t0|),\nand view\nI(ξ, ξr, ξs, ω) = ⟨ψ, Fφ⟩(xr,xs,t),\nwith the complex conjugate over the second argument. The quantity I(ξ, ξr, ξs, ω)\nwill be \"large\" (slow decay in α when the argument is scaled as earlier) pro-\nvided the wave packet ψ in data space \"matches\", both in terms of location\nand oscillation content, the image of the wave packet φ in model space under\nthe forward map F.\nThe t integral can be handled by performing two integrations by parts,\nnamely\nˆ\ne-iωtδ′′(t -τ(x, xr, xs))χ(∥t -t0∥) dt = e-iωτ(x,xr,xs)χeω(τ(x, xr, xs)),\nfor some smooth function χe (t) = eiωt(e-iωt\nω\nχ(|t-t0|))′′ involving a (harmless)\ndependence on ω0, ω1 and ω2. After absorbing the χeω(τ(x, xr, xs)) factor in\na new cutofffunction of (x, xr, xs) still called [ χ ], the result is\nI(ξ, ξ , ξ , ω) =\nˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωτ(x,xr,xs))\nr\ns\na(x; xr, xs)[ χ ]dx dxr dxs.\n(8.3)\nFirst, consider the case when x0 = xr,0 or x0 = xs,0. In that case, no\nmatter how small ε > 0, the cutofffunction [ χ ] never avoids the singularity of\nthe amplitude. As a result, the Fourier transform is expected to decay slowly\nin some directions. The ampltude's singularity is not the most interesting\nfrom a geometrical viewpoint, so for the sake of simplicity, we just brace\nfor every possible (ξ, ξr, ξs, ω) to be part of the fiber relative (x0, xr,0, xs,0, t0)\nwhere either x0 = xr,0 or x0 = xs,0. It is a good exercise to characterize these\nfibers in more details; see for instance the analysis in [?].\nAssume now that x0 = xr,0 and x0 = xs,0. There exists a sufficiently small\nε for which a(x, xr, xs) is nonsingular and smooth on the support of [ χ ]. We\nmay therefore remove a from (8.3) by absorbing it in [ χ ]:\nI(ξ, ξ , ξ , ω) =\nˆˆˆ\nei(x·ξ-xr·ξr-xs·ξs-ωτ(x,xr,xs))\nr\ns\n[ χ ]dx dxr dxs.\n(8.4)\nWhat is left is an integral that can be estimated by the stationary phase\nlemma, or more precisely, the simplest version of such a result when the\nphase is nonstationary: see lemma 4 in appendix C.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nThe phase in (8.4) is φ(x, xr, xs) = x·ξ-xr·ξr-xs·ξs-ωτ(x, xr, xs). Notice\nthat φ involves the co-variables in a linear manner, hence is homogenenous\nof degree 1 in α, as needed in lemma 4. Its gradients are\n∇xφ = ξ -ω∇xτ(x, xr, xs),\n∇xrφ = -ξr -ω∇xrτ(x, xr, xs),\n∇xsφ = -ξs -ω∇xsτ(x, xr, xs).\nIf either of these gradients is nonzero at (x0, xr,0, xs,0), then it will also be\nzero in a small neighborhood of that point, i.e., over the support of [ χ ]\nfor ε small enough.\nIn that case, lemma 4 applies, and it follows that\nthe decay of the Fourier transform is fast no matter the (nonzero) direc-\ntion (ξ, ξr, ξs, ω).\nHence, if either of the gradients is nonzero, the point\n((x0, ξ), (x\n′\nr,0, xs,0, t0; ξr, ξs, ω)) is not in WF (K).\nHence ((x0, ξ), (xr,0, xs,0, t0; ξr, ξ\n′\ns, ω)) may be an element in WF (K) only\nif the phase has a critical point at (x0, xr,0, xs,0):\nξ = ω∇xτ(x0, xr,0, xs,0),\n(8.5)\nξr = -ω∇xrτ(x0, xr,0, xs,0),\n(8.6)\nξs = -ω∇xsτ(x0, xr,0, xs,0).\n(8.7)\nAdditionally, recall that t0 = τ(x0, xr,0, xs,0). We gather the result as follows:\nWF ′(K) ⊂{ ((x, ξ), (xr, xs, t; ξr, ξs, ω)) ∈(T ∗Rn × T ∗Rnd)\\0 :\nt = τ(x, xr, xs), ω = 0, and, either x = xr, or x = xs,\nor (8.5, 8.6, 8.7) hold at (x, xr, xs) }.\nWhether the inclusion is a proper inclusion, or an equality, (at least away\nfrom x = xr and x = xs) depends on whether the amplitude factor in (8.4)\nvanishes in an open set or not.\nNotice that ω = 0 for the elements in WF ′(K), otherwise (8.5, 8.6, 8.7)\nwould imply that the other covariables are zero as well, which is not allowed\nin the definition of WF ′(K). In the sequel, we may then divide by ω at will.\nThe relations (8.5, 8.6, 8.7) have an important physical meaning. Re-\ncall that t = τ(x, xr, xs) is called isochrone curve/surface when it is consid-\nered in model x-space, and moveout curve/surface when considered in data\n(xr, xs, t)-space.\n\n8.2. CHARACTERIZATION OF THE WAVEFRONT SET\n- The relation ξ = ω∇xτ(x, xr, xs) indicates that ξ is normal to the\nisochrone passing through x, with level set t = τ(x, xr, xs). In terms of\ntwo-point traveltimes, we may write\nξ = ∇xτ(x, xr) + ∇xτ(x, xs).\nω\nObserve that ∇xτ(x, xr) is tangent at x to the ray from xr to x, and\n∇xτ(x, xs) is tangent at x to the ray from xs to x, hence ξ is the bisector\ndirection for those two rays. The (co)vector ξ may be understood as\nthe (co)normal to a \"localized mirror\" about which the incident wave\nreflects in a specular manner to create the scattered wave. The equation\nabove is then the vector expression of Snell's law of reflection, that\nthe angle of incidence ∠(∇xτ(x, xs), ξ) equals the angle of reflection\n∠(∇xτ(x, xr), ξ).\n- The special case ξ = 0 is equally important from a physical viewpoint.\nSince ω = 0, it corresponds to\n∇xτ(x, xr) = -∇xτ(x, xs),\ni.e., the tangents to the incident and reflected rays are collinear and\nopposite in signs. This happens when x is a point on the direct, unbro-\nken ray linking xr to xs. We may call this situation forward scattering:\nit corresponds to transmitted waves rather than reflected waves. The\nreader can check that it is the only way in which a zero section is created\nin the wavefront relation (see the previous section for an explanation\nof zero sections and how they impede the interpretation of WF ′(K) as\na relation.)\n- Consider now (8.6) and (8.7). Pick any η = (ηr, ηs), form the combi-\nnation ηr· (8.6) + ηs· (8.7) = 0, and rearrange the terms to get\n\nξr\nηr\nξs·\nηs\n= 0,\nω\nηr · ∇xrτ + ηs · ∇xsτ\nwith both τ evaluated at (x, xr, xs).\nThe second vector in the dot\nproduct is an arbitrary vector tangent to the moveout surface t =\nτ(x, xr, xs) in (xr, xs, t)-data space. Thus (ξr, ξs, ω) is normal to the\nmoveout surface.\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nWe are now ready to interpret WF ′(K) in terms of the mapping of singu-\n(!)\nlarities that it generates. Assume that no forward scattering is taking place,\ni.e., we are only dealing with reflected rather than transmitted waves. From a\nsingularity in model space at a point x and in the direction ξ - a local mirror\nplaced at x with conormal ξ - the wavefront relation predicts that there may\nbe singularities in data space, with location(s) (xr, xs, t) and corresponding\nconormal direction(s) (ξr, ξs, ω) determined as follows.\nFix a couple (x, ξ).\n1. For each xs, find the unique ray that links xs to x.\n2. At x, find the direction of the incoming ray as ∇xτ(x, xs).\n3. Determine ∇xτ(x, xr) as the vector with magnitude 1/c(x) (from the\neikonal equation), and direction so that (8.5) holds for some ω. This\ncan be done by computing the reflection of ∇xτ(x, xs) about the axis\ngenerated by ξ,\n∇xτ(x, xs) · ξ\n∇xτ(x, xr) = ∇xτ(x, xs) -2\nξ.\nξ · ξ\n4. Trace the ray from x and take-offdirection -∇xτ(x, xr):\n- If the ray is closed or exits the domain before reaching a receiver,\ndiscard xs. This particular xs does not give rise to any singularity\n(relative to this particular couple (x, ξ).)\n- If the ray reaches a receiver xr, let t = τ(x, xs) + τ(x, xr). Some\nsingularity may appear at the point (xs, xr, t) in data space.\n5. Determine ω = ∥ξ∥/∥∇xτ(x, xr) + ∇xτ(x, xs)∥. The normalization of\nξ starts as arbitrary, and is undone by division by ω.\n6. Determine ξr and ξs directly from (8.6) and (8.7). Then the covector\nconormal to the singularity at (xr, xs, t) is (ξr, ξs, ω) (as well as all its\npositive multiples.)\nNotice that both the data variables (xr, xs, t) and the covariables (ξr, ξs, ω)\nare determined uniquely from x and ξ in this situation, hence the singularity\nmapping induced by the wavefront relation is one-to-one.\n\n8.3. PSEUDODIFFERENTIAL THEORY\nIf we make the additional two assumptions that the receivers surround\nthe domain of interest (which would require us to give up the assumption\nthat the acquisition manifold Ωis a vector subspace), and that there is no\ntrapped (closed) ray , then the test in point 4 above always succeeds. In (!)\nthat case, the wavefront relation is onto, hence bijective, and can be inverted\nby a sequence of steps with a similar geometrical content as earlier. See an\nexercise at the end of this chapter.\n8.3\nPseudodifferential theory\n8.4\nExercises\n1. Compute the wavefront set of the following functions/distributions of\ntwo variables: (1) the indicator function H(x1) (where H is the Heav-\niside function) of the half-plane x1 > 0, (2) the indicator function of\nthe unit disk, (3) the indicator function of the square [-1, 1]2, (4) the\nDirac delta δ(x).\n2. Find conditions on the acquisition manifold under which forward scat-\ntering is the only situation in which a zero section can be created in\nWF ′(K). [Hint: transversality with the rays from x to xr, and from x\nto xs.]\n3. Perform the geometrical construction of (WF ′(K))T for migration,\nanalogous to the geometrical construction of WF ′(K) for Born mod-\neling done in section 8.2. Assume: 1) τ single-valued, 2) no forward\nscattering, 3) full aperture of receivers, 4) no trapped rays, and 5) the\nacquisition manifold Ωis a vector subspace equal to the Cartesian prod-\nuct of two copies of the same subspace of codimension 1 in R3 (one for\nxr and one for xs). In other words, you may assume that both xr and\nxs lie on the \"surface\" z = 0.\nSolution. Fix (xr, xs, t) and (ξr, ξs, ω) = 0. We aim to determine a\nunique (x, ξ) so that ((x, ξ), (xr, xs, t; ξr, ξs, ω)) ∈WF ′(K).\nStarting with (8.6), notice first that ∇xrτ(x, xr) is a partial gradient:\nit is the projection to the acquisition manifold Ωat xr of the three-\ndimensional gradient of τ in its second argument, say ∇yτ(x, y)|y=(xr,0),\nin coordinates y = (xr, z) so that Ωis represented by z = 0. A similar\n\nCHAPTER 8. MICROLOCAL ANALYSIS OF IMAGING\nobservation holds for ∇xsτ(x, xs). Since the norm of each partial gra-\ndient is less than the norm of the full gradient, and since the eikonal\nequation determines this latter norm, ξr and ξs must obey the geometric\ncompatibility relations\nω\n|ξr| ≤c(xr),\n|ξs| ≤\nω\n.\nc(xs)\nThe coordinate z is one-dimensional from our assumption on the ac-\nquisition manifold, so we can now assemble the full gradient of τ at xr.\nWith the help of (8.6) we obtain\n\nξr\n∇yτ(x, y)|y=(xr,0) =\n-ω , ±\ns\nc2(xr) -|ξr|2!\n.\nω2\n(The sign is determined on geometrical grounds, so that ∇yτ points\noutside the domain of interest.) Minus the direction of ∇yτ is also\nthe take-off direction of a ray from xr. We obtain ∇yτ(x, y)|y=(xs,0) in\nthe same fashion from (8.7), which determines the take-off direction\nof a ray from xs. These rays meet at a single point that we call x.\nWe may alternatively intersect either of these rays with the isochrone\nt = τ(x, xr, xs) to obtain x; this piece of information is redundant in the\nwavefront relation4. Finally, we determine ξ = ω∇xτ(x, xr, xs).\n4Only because we have assumed that the acquisition manifold has low codimension.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Optimization",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/75f6561745b3dd2af7f44c89ee34a4bc_MIT18_325F15_Chapter9.pdf",
      "content": "Chapter 9\nOptimization\n9.1\nRegularization and sparsity\n9.2\nDimensionality reduction techniques\nOne way to reduce the dimensionality of a dataset is to scramble data as\nde= Cd, where\nX\ndej,r(t) =\ncj,sdr,s(t -bj,s).\ns\nThe numbers cj,s and bj,s may be random, for instance. The point is that\nusing fewer values of j than s may result in computational savings -- a\nstrategy sometimes called source encoding. By linearity of the wave equation,\nthe scrambled data de can be seen as originating from scrambled shots, or\nsupershots fe= Cf, for\nX\nfej(x, t) =\ncj,sfs(x, t -bj,s).\ns\nScrambled data may be all that's available in practice, in acquisition scenarios\nknown as simultaneous sourcing.\nThe adjoint operation C∗results in twice-scrambled data D = C∗de, where\nX\nDr,s(t) =\nc\ne\nj,sdj,r(t + bj,s).\nj\nThe linearized forward model with scrambling is de = CFm. The ba-\nsic imaging operator is still the adjoint, Im = F ∗C∗de. In addition to the\n\nCHAPTER 9. OPTIMIZATION\ntraditional incident and adjoint fields\nu0,s = Gfs,\nqs = Gds,\nwhere G is the Green's function in the unperturbed medium, and G the\ntime-reversed Green's function, we define the scrambled fields\nue0,j = Gfej,\nqej = Gdej.\nAlso define the twice-scrambled adjoint field\nQ\n∗\ns = G(C de)s.\nThen\nX\nT\nI (x) = (F ∗\n∗\n∂\nC\nˆ\n,s\nde\nu0\nm\n)(x) = -\ns\n(x, t) Qs(x, t) dt.\n∂t2\nAnother formula involving j instead of s (hence computationally more favor-\nable) is\nX\n,j\nIm(x)\n-\nj\nˆ T ∂2ue0\n=\n(x, t) qej(x, t) dt.\n(9.1)\n∂t2\nTo shoPw this latter form\nPula, use Q = C∗qe, pass C∗to the rest of the integrand\nwith\ns v (C∗\ns\nw)s =\nj(Cvj)wj, and combine Cu0 = ue0.\nScrambled data can also be used as the basis of a least-squares misfit,\nsuch as\nJe\n(m) =\n∥de-CF(m)∥2\n2.\nThe gradient of Je is F ∗C∗applied to the residual, hence can be computed\nwith (9.1).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Waves and Imaging, Scattering Series",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2015/15588369c29a866ba59b450518ba486a_MIT18_325F15_Chapter3.pdf",
      "content": "Chapter 3\nScattering series\nIn this chapter we describe the nonlinearity of the map c 7→u in terms of a\nperturbation (Taylor) series. To first order, the linearization of this map is\ncalled the Born approximation. Linearization and scattering series are the\nbasis of most inversion methods, both direct and iterative.\nThe idea of perturbation permeates imaging for physical reasons as well.\nIn radar imaging for instance, the background velocity is c0 = 1 (speed\nof light), and the reflectivity of scatterers is viewed as a deviation in c(x).\nThe assumption that c(x) does not depend on t is a strong one in radar:\nit means that the scatterers do not move. In seismology, it is common to\nconsider a smooth background velocity c0(x) (rarely well known), and explain\nthe scattered waves as reflections due to a \"rough\" (singular/oscillatory)\nperturbations to this background. In both cases, we will write\nc2(x) = m(x),\n= m0(x),\nm for \"model\",\nc2\n0(x)\nand, for some small number ε,\nm(x) = m0(x) + εm1(x).\n(3.1)\nNote that, when perturbing c(x) instead of m(x), an additional Taylor\napproximation is necessary:\nc(x) = c0(x) + εc1(x)\n⇒\nc2(x) ≃\nc2\n0(x) -2εc1(x).\nc3\n0(x)\nWhile the above is common in seismology, we avoid making unnecessary\nassumptions by choosing to perturb m(x) = 1/c2(x) instead.\n\nCHAPTER 3. SCATTERING SERIES\nPerturbations are of course not limited to the wave equation with a single\nparameter c. The developments in this chapter clearly extend to more general\nwave equations.\n3.1\nPerturbations and Born series\nLet\n∂2u\nm(x)\n-∆u = f(x, t),\n(3.2)\n∂t2\nwith zero initial conditions and x ∈Rn. Perturb m(x) as in (3.1). The\nwavefield u correspondingly splits into\nu(x) = u0(x) + usc(x),\nwhere u0 solves the wave equation in the undisturbed medium m0,\n∂2u0\nm0(x)\n-∆u0 = f(x, t).\n(3.3)\n∂t2\nWe say u is the total field, u0 is the incident field1, and usc is the scattered field,\ni.e., anything but the incident field.\nWe get the equation for usc by subtracting (3.3) from (3.2), and using\n(3.1):\n∂2usc\nm0(x) ∂t2\n-∆usc = -ε m1(x)∂2u.\n(3.4)\n∂t2\nThis equation is implicit in the sense that the right-hand side still depends\non usc through u. We can nevertheless reformulate it as an implicit integral\nrelation by means of the Green's function:\nˆ t\n∂2u\nusc(x, t) = -ε\nG(x, y; t -s)m1(y)\nˆ\nRn\n(y, s) dyds.\n∂t2\nAbuse notations slightly, but improve conciseness greatly, by letting\n- G for the operator of space-time integration against the Green's func-\ntion, and\n1Here and in the sequel, u0 is not the initial condition. It is so prevalent to introduce\nthe source as a right-hand side f in imaging that it is advantageous to free the notation\nu0 and reserve it for the incident wave.\n\n3.1. PERTURBATIONS AND BORN SERIES\n- m1 for the operator of multiplication by m1.\nThen usc = -ε G m\n∂2u\n1 ∂t2 . In terms of u, we have the implicit relation\nu = u0 -ε G m1\n∂2u,\n∂t2\ncalled a Lippmann-Schwinger equation. The field u can be formally2 ex-\npressed in terms of u0 by writing\n\n∂2\nu =\nI + ε G m1\n-1\nu0.\n(3.5)\n∂t2\nWhile this equation is equivalent to the original PDE, it shines a different\nlight on the underlying physics. It makes explicit the link between u0 and u,\nas if u0 \"generated\" u via scattering through the medium perturbation m1.\nWriting [I + A]-1 for some operator A invites a solution in the form of a\nNeumann series I -A + A2 -A3 + . . ., provided ∥A∥< 1 in some norm. In\nour case, we write\n\n∂2\nu = u0 -ε\nG m1 ∂t2\n\nu0 + ε2\n\nG m1\n∂2\n∂t2\n\nG m1\n∂2\n∂t2\n\nu0 + . . .\nThis is called a Born series. The proof of convergence, based on the \"weak\nscattering\" condition ε∥G m1\n∂2\nb\n∂2∥∗< 1, in some norm to\ne determined, will\nt\nbe covered in the next section. It retroactively justifies why one can write\n(3.5) in the first place.\nThe Born series carries the physics of multiple scattering. Explicitly,\nu = u0\n(incident wave)\n-ε\nˆ ˆt\n∂2u0\nG(x, y; t -s)m1(y)\nRn\n∂t2 (y, s) dyds\n(single scattering)\n+ ε2\nˆ t\nˆ\nRn G(x, y2; t -s2)m1(y2) ∂2 ˆ s2ˆ\n∂2u0\nG(y2, y1; s2 -s1)m1(y1)\n∂s2\nRn\n\n(y1, s1) dy1ds1\ndy2ds2\n∂t2\n(double scattering)\n+ . . .\n2For mathematicians, \"formally\" means that we are a step ahead of the rigorous ex-\nposition: we are only interested in inspecting the form of the result before we go about\nproving it. That's the intended meaning here. For non-mathematicians, \"formally\" often\nmeans rigorous, i.e., the opposite of \"informally\"!\n\nCHAPTER 3. SCATTERING SERIES\nWe will naturally summarize this expansion as\nu = u0 + εu1 + ε2u2 + . . .\n(3.6)\nwhere εu1 represent single scattering, ε2u2 double scattering, etc. For in-\nstance, the expression of u1 can be physically read as \"the incident wave\ninitiates from the source at time t = 0, propagates to y where it scatters due\nto m(y) at time t = s, then further propagates to reach x at time t.\" The\nexpression of u2 can be read as \"the incident wave initiates from the source at\nt = 0, propagates to y1 where it first scatters at time t = s1, them propagates\nto y2 where it scatters a second time at time t = s2, then propagates to x at\ntime t, where it is observed.\" Since scatterings are not a priori prescribed to\noccur at fixed points in space and time, integrals must be taken to account\nfor all physically acceptable scattering scenarios.\nThe approximation\nusc(x) ≃εu1(x)\nis called the Born approximation. From u1 = -Gm ∂2u0\n,\n∂t2\nwe can return to\nthe PDE and obtain the equation for the primary reflections:\n∂2u1\nm0(x) ∂t2 -∆u1 = -m1(x)∂2u0.\n(3.7)\n∂t2\nThe only difference with (3.4) is the presence of u0 in place of u in the right-\nhand side (and ε is gone, by choice of normalization of u1). Unlike (3.4),\nequation (3.7) is explicit: it maps m1 to u1 in a linear way. The incident field\nu0 is determined from m0 alone, hence \"fixed\" for the purpose of determining\nthe scattered fields.\nIt is informative to make explicit the dependence of u1, u2, . . . on m1. To\nthat end, the Born series can be seen as a Taylor series of the forward map\nu = F[m],\nin the sense of the calculus of variations. Denote by δF\nδm[m0] the \"functional\ngradient\" of F with respect to m, evaluated at m0. It is an operator acting\nfrom model space (m) to data space (u). Denote by δ2F\n]\nδ\n2[m0 the \"functional\nm\nHessian\" of F with respect to m, evaluated at m0. It is a bilinear form from\nmodel space to data space. See the appendix for background on functional\nderivatives. Then the functional version of the Taylor expansion enables to\nexpress (3.6) in terms of the various derivatives of F as\nδF\nu = u0 + ε δm[m0] m1 + ε2\n2 ⟨δ2F\nδm2[m0] m1, m1⟩+ . . .\n\n3.2. CONVERGENCE OF THE BORN SERIES (MATH)\nIt is convenient to denote the linearized forward map by (print) F:\nδF\nF = δm[m0],\nor, for short, F = ∂u . It is a linear operator. The point of F is that it makes\n∂m\nexplicit the linear link between m1 and u1:\nu1 = Fm1.\nWhile F is supposed to completely model data (up to measurement errors),\nF would properly explain data only in the regime of the Born approximation.\nLet us show that the two concepts of linearized scattered field coincide,\nnamely\nδF\nu1 = δm[m0] m1 = -Gm1\n∂2u0.\n∂t2\nThis will justify the first term in the Taylor expansion above. For this pur-\npose, let us take the\nδ derivative of (3.2). As previously, write u = F(m)\nδm\nand F = δF\nδm[m]. We get the operator-valued equation\n∂2u\n∂2\nI + m\n∂t2\nF -∆F = 0.\n∂t2\nEvaluate the functional derivatives at the base point m0, so that u = u0.\nApplying each term as an operator to the function m1, and defining u1 =\nFm1, we obtain\n∂2u0\nm1 ∂t2 + m0\n∂2u1 -∆u1 = 0,\n∂t2\nwhich is exactly (3.7).\nApplying G on both sides, we obtain the desired\nconclusion that u1 = -Gm ∂2u0\n.\n∂t2\n3.2\nConvergence of the Born series (math)\nWe are faced with two very interrelated questions: justifying convergence of\nthe Born series, and showing that the Born approximation is accurate when\nthe Born series converges. The answers can either take the form of mathe-\nmatical theorems (this section), or physical explanations (next section). As\nof 2013, the community's mathematical understanding is not yet up to par\nwith the physical intuition!\n\nCHAPTER 3. SCATTERING SERIES\nLet us describe what is known mathematically about convergence of Born\nseries in a simple setting. To keep the notations concise, it is more convenient\nto treat the wave equation in first-order hyperbolic form\n∂w\nM\n-Lw = f,\nL∗= -L,\n(3.8)\n∂t\nfor some inner product ⟨w, w′⟩. The conserved energy is then E = ⟨w, Mw⟩.\nSee one of the exercises at the end of chapter 1 to illustrate how the wave\nequation can be put in precisely this form, with ⟨w, w′⟩the usual L2 inner\nproduct and M a positive diagonal matrix.\nConsider a background medium M0, so that M = M0 + εM1. Let w =\nw0 + εw1 + . . . Calculations very similar to those of the previous section (a\ngood exercise) show that\n- The Lippmann-Schwinger equation is\n∂w\nw = w0 -εGM1\n,\n∂t\nwith the Green's function G = (M\n∂\n-L)-1.\n∂t\n- The Neumann series of interest is\n∂w0\nw = w0 -εGM1 ∂t + ε2GM1\n∂\n∂tGM1\n∂w0\n∂t + . . .\nWe identify w1 = -GM1\n∂w0.\n∂t\n- In differential form, the equations for the incident field w0 and the\nprimary scattered field w1 are\n∂w0\nM0 ∂t -Lw0 = f,\nM0\n∂w1\n∂t -Lw1 = -M1\n∂w0,\n(3.9)\n∂t\n- Convergence of the Born series occurs when\n∂\nε∥GM1\n∥∗< 1,\n∂t\nin some induced operator norm, i.e., when ε∥w1∥∗< ∥w0∥∗for arbitrary\nw0, and w1 = -GM ∂w0\n1 ∂t , for some norm ∥· ∥∗.\n\n3.2. CONVERGENCE OF THE BORN SERIES (MATH)\nNotice that the condition ε∥w1∥∗< ∥w0∥∗is precisely one of weak scat-\ntering, i.e., that the primary reflected wave εw1 is weaker than the incident\nwave w0.\nWhile any induced norm over space and time in principle works for the\nproof of convergence of the Neumann series, it is convenient to use\np\n∥w∥∗= max\n0≤t≤T\n⟨w, M0w⟩= max\n0≤t≤T ∥\np\nM0w∥.\nNote that it is a norm in space and time, unlike ∥w∥=\np\n⟨w, w⟩, which is\nonly a norm in space.\nTheorem 3. (Convergence of the Born series) Assume that the fields w, w0,\nw1 are bandlimited with bandlimit3 Ω. Consider these fields for t ∈ [0, T ]. Then\nthe weak scattering condition ε∥w1∥∗ < ∥w0∥∗ is satisfied, hence the Born\nseries converges, as soon as\nM1\nε ΩT ∥\n∥inf< 1.\nM0\nProof. We compute\nd\n∂w1\n⟨w1, M0w1⟩= 2⟨w1, M0\ndt\n⟩\n∂t\n∂w0\n= 2⟨w1, Lw1 -M1 ∂t ⟩\n= -2⟨w1, M1\n∂w0⟩\nbecause L∗= -L\n∂t\np\n= -2⟨\nM0w1, M1\n√M0\n∂w0\n∂t ⟩.\nSquare roots and fractions of positive diagonal matrices are legitimate oper-\nations. The left-hand-side is also\nd\n√\n⟨w1, M0w1⟩= 2∥\ndt\nM0w1∥2\nd\ndt∥√M0w1∥2.\nBy Cauchy-Schwarz, the right-hand-side is majorized by\n2∥\np\nM1\nM0w1∥2 ∥√M0\n∂w0\n∂t ∥2.\nHence\nd\ndt∥\np\nM1\nM0w1∥2 ≤∥√M0\n∂w0\n∂t ∥2.\n3A function of time has bandlimit Ωwhen its Fourier transform, as a function of ω, is\nsupported in [-Ω, Ω].\n\nCHAPTER 3. SCATTERING SERIES\np\n∥\nt\nM0w1∥2 ≤\nˆ\nM1\n∥\n√M0\n∂w0\n∂t ∥2(s) ds.\n∥w1∥∗= max\n0≤t≤T ∥\np\nM0w1∥2 ≤T max\n0≤t≤T ∥M1\n√\n∂w0\nM0 ∂t ∥2\n≤T∥M1\nM0\n∥infmax\n0≤t≤T ∥\np\nM0\n∂w0∥2.\n∂t\nThis last inequality is almost, but not quite, what we need. The right-\nhand side involves\n∂w0 instead of w0.\nBecause time derivatives can grow\n∂t\narbitrarily large in the high-frequency regime, this is where the bandlimited\nassumption needs to be used. We can invoke a classical result known as Bern-\nstein's inequality4, which says that ∥f′∥inf ≤ Ω∥f∥inf for all Ω-bandlimited f.\nThen\nM1\n∥w1∥∗≤ΩT∥\n∥inf∥w0∥∗.\nM0\nIn view of our request that ε∥w1∥∗< ∥w0∥∗, it suffices to require\nM1\nε ΩT ∥M0\n∥inf< 1.\nSee the book Inverse Acoustic and Electromagnetic Scattering Theory by\nColton and Kress for a different analysis that takes into account the size of\nthe support of M1.\nNote that the beginning of the argument, up to the Cauchy-Scwharz\ninequality, is called an energy estimate in math. See an exercise at the end\nof this chapter. It is a prevalent method to control the size of the solution of\nmany initial-value PDE, including nonlinear ones.\nThe weak scattering condition ε∥w1∥∗< ∥w0∥∗encodes the idea that the\nprimary reflected field εw1 is small compared to the incident field w0. It is\nsatisfied when ε is small, and when w1 is not so large that it would undo the\nsmallness of ε (via the factors ΩT, for instance). It turns out that\n- the full scattered field wsc = w -w0 is also on the order of εΩT∥M1∥inf\n-- namely the high-order terms don't compromise the weak scattering\nsituation; and\n4The same inequality holds with the Lp norm for all 1 ≤p ≤inf.\n\n3.3. CONVERGENCE OF THE BORN SERIES (PHYSICS)\n- the remainder w -εw = w-w -εw is on the order of ε2\nsc\n(ΩT∥M1∥inf) .\nBoth claims are the subject of an exercise at the end of the chapter. The\nsecond claim is the mathematical expression that the Born approximation\nis accurate (small wsc -εw1 on the order of ε2) precisely when scattering is\nweak (εw1 and wsc on the order of ε.)\n3.3\nConvergence of the Born series (physics)\nLet us explain why the criterion εΩT < 1 (assuming the normalization\n∥M1/M0∥inf= 1) is adequate in some cases, and why it is grossly pessimistic\nin others.\n- Instead of m or M, consider the wave speed c0 = 1. Consider a constant\nperturbation c1 = 1, so that c = c0 + εc1 = 1 + ε. In one spatial\ndimension, u(x, T) = f(x -cT). As a Taylor series in ε, this is\nε2\nu(x, T) = f(x-(1+ε)T) = f(x-T)-εTf ′(x-T)+\nT 2f ′′(x-T)+. . .\nWe identify u0(x, T) = f(x -T) and u1(x, T) = -Tf ′(x -T). Assume\nnow that f is a waveform with bandlimit Ω, i.e., wavelength 2π/Ω. The\nBorn approximation\nf(x -(1 + ε)T) -f(x -T) ≃-εTf ′(x -T)\nis only good when the translation step εT between the two waveforms\non the left is a small fraction of a wavelength 2π/Ω, otherwise the\nsubtraction f(x-(1+ε)T)-f(x-T) will be out of phase and will not\ngive rise to values on the order of ε. The requirement is εT ≪2π/Ω,\ni.e.,\nεΩT ≪2π,\nwhich is exactly what theorem 3 is requiring. We could have reached\nthe same conclusion by requiring either the first or the second term\nof the Taylor expansion to be o(1), after noticing that |f ′| = O(Ω) or\n|f ′′| = O(Ω2). In the case of a constant perturbation c1 = 1, the waves\nundergo a shift which quickly becomes nonlinear in the perturbation.\nThis is the worst case: the requirement εΩT < 1 is sharp.\n\nCHAPTER 3. SCATTERING SERIES\n- As a second example, consider c0 = 1 and c1(x) = H(x). The profile\nof reflected and transmitted waves was studied in equations (1.20) and\n(1.21). The transmitted wave will undergo a shift as in the previous\nexample, so we expect εΩT < 1 to be sharp for it. The full reflected\nwave, on the other hand, is\nε\nur(x, T) = Rεf(-x -T),\nRε =\n.\n2 + ε\nNotice that ε only appears in the reflection coefficient Rε, not in the\nwaveform itself. As ε →0, ur expands as\nε\nur(x, T) = 2f(-x -T) -ε2\n4 f(-x -T) + . . .\nWe recognize u1 = 1f(-x -T). The condition for weak scattering and\naccuracy of the Born approximation is now simply ε < 1, which is in\ngeneral much weaker than εΩT < 1.\n- In the case when c0 = 1 and c1 is the indicator function of a thin slab\nin one dimension, or a few isolated scatterers in several dimensions, the\nBorn approximation is often very good. That's when the interpretation\nof the Born series in terms of multiple scattering is the most relevant.\nSuch is the case of small isolated objects in synthetic aperture radar:\ndouble scattering from one object to another is often negligible.\nThe Born approximation is often satisfied in the low-frequency regime\n(small Ω), by virtue of the fact that cycle skipping is not as much of an\nissue. In the high-frequency regime, the heuristics for validity of the Born\napproximation are that\n1. c0 or m0 should be smooth.\n2. c1 or m1 should be localized, or better yet, localized and oscillatory\n(zero mean).\nThe second requirement is the most important one: it prohibits transmitted\nwaves from propagating in the wrong velocity for too long. We do not yet\nhave a way to turn these empirical criteria and claims into rigorous mathe-\nmatical results. Seismologists typically try to operate in the regime of this\nheuristic when performing imaging with migration (see chapter on seismic\nimaging).\n\n3.4. A FIRST LOOK AT OPTIMIZATION\nConversely, there are a few settings in which the Born approximation is\nclearly violated: (i) in radar, when waves bounce multiple times before being\nrecorded (e.g. on the ground and on the face of a building, or in cavities\nsuch as airplane engines), (ii) in seismology, when trying to optimize over\nthe small-wavenumber components of m(x) (model velocity estimation), or\nwhen dealing with multiple scattering (internal multiples). However, note\nthat multiple reflections from features already present in the modeling (such\nas ghosts due to reflections at the ocean-air interface) do not count as non-\nlinearities.\nScattered waves that do not satisfy the Born approximation have long\nbeen considered a nuisance in imaging, but have recently become the subject\nof some research activity.\n3.4\nA first look at optimization\nIn the language of the previous sections, the forward map is denoted\nd = F[m],\nd = data,\nm = model,\nwhere dr,s(t) = us(xr, t),\n- xr is the position of receiver r,\n- s indexes the source,\n- and t is time.\nThe inverse problem of imaging is that of solving for m in the system of\nnonlinear equations d = F[m].\nNo single method will convincingly solve\nsuch a system of nonlinear equations efficiently and in all regimes.\nThe very prevalent least-squares framework formulate the inverse problem\nas finding m as the solution of the minimization problem\nmin J[m],\nwhere\nJ[m] =\nm\n∥d -F[m]∥2\n2,\n(3.10)\nwhere ∥d∥2\nP\nT\n2 =\n,s\n\n|dr,s(t)|\nr\nis the L2 norm squared in the space of vectors\nindexed by r, s (discrete) and t (continuous, say). J is called the output\nleast-squares criterion, or objective, or cost.\n\nCHAPTER 3. SCATTERING SERIES\nIn the sequel we consider iterative schemes based on the variations of J at\na base point m0, namely the functional gradient δJ\nδm[m0], a linear functional\nin m space; and the functional Hessian\nδ2J\nv\nδ\n2[m0], also called wa e-equation\nm\nHessian, an operator (or bilinear form) in m space. The appendix contains\na primer on functional calculus.\nTwo extreme scenarios cause problems when trying to solve for m as the\nminimizer of a functional J:\n- The inverse problem is called ill-posed when there exist directions m1 in\nwhich J(m) has a zero curvature, or a very small curvature, in the vicin-\nity of the solution m∗. Examples of such directions are the eigenvectors\nof the Hessian of J associated to small eigenvalues. The curvature is\nthen twice the eigenvalue, i.e., twice the second directional derivative\nin the eigen-direction. Small perturbations of the data, or of the model\nF, induce modifications of J that may result in large movements of\nits global minimum in problematic directions in the \"near-nullspace\"\nof the Hessian of J.\n- Conversely, the inverse problem may suffer from severe non-convexity\nwhen the abundance of local minima, or local \"valleys\", hinders the\nsearch for the global minimum.\nThis happens when the Hessian of\nJ alternates between having large positive and negative curvatures in\nsome direction m1.\nMany inversion problems in high-frequency imaging suffer from some (not\noverwhelming) amount of ill-posedness, and can be quite non-convex. These\ntopics will be further discussed in chapter 9.\nThe gradient descent method5 applied to J is simply\nm(k+1) = m(k)\nδJ\n-α\n[m(k)].\n(3.11)\nδm\nThe choice of α is a balance between stability and speed of convergence -\nsee two exercises at the end of the chapter. In practice, a line search for α is\noften a good idea.\nThe usual rules of functional calculus give the expression of\nδJ , also\nδm\nknown as the \"sensitivity kernel\" of J with respect to m.\n5Also called Landweber iteration in this nonlinear context.\n\n3.4. A FIRST LOOK AT OPTIMIZATION\nProposition 4. Put F = δF\nδm[m]. Then\nδJ [m] = F ∗(F[m] -d).\nδm\nProof. Since F[m + h] = F[m] + Fh + O(∥h∥2), we have\n⟨F[m+h]-d, F[m+h]-d⟩= ⟨F[m]-d, F[m]-d⟩+2⟨Fh, F[m]-d⟩+O(∥h∥2).\nTherefore\nJ[m + h] -J[m] =\n2⟨Fh, F[m] -d⟩+ O(∥h∥2)\n= ⟨h, F ∗(F[m] -d)⟩+ O(∥h∥2).\nWe conclude by invoking (A.1).\nWith some care, calculations involving functional derivatives are more\nefficiently done using the usual rules of calculus in Rn. For instance, the\nresult above is more concisely justified from\nδ\n⟨δm\n\n⟨F[m] -d, F[m] -d⟩\n, m1⟩= ⟨Fm1, F[m] -d⟩\n= ⟨F ∗(F[m] -d), m1⟩.\nThe reader may still wish to use a precise system for bookkeeping the various\nfree and dummy variables for longer calculations - see the appendix for such\na system.\nThe problem of computing F ∗will be completely addressed in the next\nchapter.\nThe Gauss-Newton iteration is Newton's method applied to J:\n\nm(k+1)\nJ\n=\n(k)\nδ2\nm\n-\nδm2[m(k)]\n-1 δJ\nδm[m(k)].\n(3.12)\nHere\n\nδ2J\n-1\n2[m(k)]\nis an operator: it is the inverse of the functional Hessian\nδm\nof J.\nAny iterative scheme based on a local descent direction may converge to\na wrong local minimum when J is nonconvex. Gradient descent typically\nconverges slowly - a significant impediment for large-scale problems. The\n\nCHAPTER 3. SCATTERING SERIES\nGauss-Newton iteration converges faster than gradient descent in the neigh-\nborhood of a local minimum, when the Hessian of J is (close to being) positive\nsemi-definite, but may otherwise result in wrong update directions. It is in\ngeneral much more complicated to set up Gauss-Newton than a gradient de-\nscent since the wave-equation Hessian is a large matrix, costly to store and\ncostly to invert. Good practical alternatives include quasi-Newton methods\nsuch as LBFGS, which attempt to partially invert the wave-equation Hessian.\n3.5\nExercises\n1. Repeat the development of section (3.1) in the frequency domain (ω)\nrather than in time.\n2. Derive Born series with a multiscale expansion: write m = m0 + εm1,\nu = u0 + εu1 + ε2u2 + . . ., substitute in the wave equation, and equate\nlike powers of ε. Find the first few equations for u0, u1, and u2.\n3. Write the Born series for the acoustic system, i.e., find the linearized\nequations that the first few terms obey. [Hint: repeat the reasoning of\nsection 3.1 for the acoustic system, or equivalently expand on the first\nfew three bullet points in section 3.2.]\n4. At the end of section 3.1 we found the equation that u1 obeys by\ndifferentiating (3.2) with respect to m. Now, differentiate (3.2) twice\nin two different directions m1, m′\n1 to find the equation for the Hessian\nδ2F\n, as a bilinear form of two functions m\nm′\n1 and\n. Check that (up\nδm1δm′\nto a factor 2) your answer reduces to the equation for u2 obtained in\nexercise 2 when m\n′\n1 = m1. The Hessian of F reappears in the next\nchapter as we describe accelerated descent methods for the inversion\nproblem.\nSolution. A first derivative with respect to m1 gives\nδm\nδm1\n∂2F(m)\n∂t2\n+\n\nm ∂2\nδF(m)\n-∆\n∂t2\nδm1\n= 0.\nThe notation\nδm means the linear form that takes a function m1 and\nδm1\nreturns the operator of multiplication by m1. We may also write it as\n\n3.5. EXERCISES\nthe identity Im1 \"expecting\" a trial function m1. A second derivative\nwith respect to m′\n1 gives\nδm\nδm1\n∂2\n∂t2\nδF(m)\nδm′\n+ δm\nδm′\n∂2\n∂t2\nδF(m)\nδm1\n+\n\nm ∂2\n∂t2 -∆\nδ2F(m)\nδ\n′ = .\nm1δm1\nWe now evaluate the result at the base point m = m0, and perform the\npairing with two trial functions m1 and m′\n1. Denote\nδ2F(m0)\nv = ⟨\nm\nδm1δm′\n1, m′\n1⟩.\nThen the equation for v is\n\n∂2\nm0∂t2 -∆\n\nv = -m1\n∂2u′\n∂t2 -m′\n∂2u1,\n∂t2\nwhere u1, u′\n1 are the respective linearized reflection fields generated by\nm\n′\n1, m1. In this formulation, the computation of v requires solving four\nwave equations, for v, u1, u′\n1, and u0 (which appears in the equations\nfor u1 and u′\n′\n1). Notice that v = 2u2 when m1 = m1.\n5. Compute δ2F\nolarization:\nδ\n2 in an alternative way by p\nfind the equations\nm\nfor the second-order field u2 when the respective model perturbations\nare m1 + m′\n1 and m1 -m′\n1, and take a combination of those two fields.\n6. Consider the setting of section 3.2 in the case M = I. No perturbation\nwill be needed for this exercise (no decomposition of M into M0+εM1).\nProve the following energy estimate for the solution of (3.8):\n\nt\nE(t) ≤\nˆ\n∥f∥(s) ds\n,\n(3.13)\nwhere E(t) = ⟨w, Mw⟩and ∥f∥2 = ⟨f, f⟩. [Hint: repeat and adapt the\nbeginning of the proof of theorem 3.]\n7. p\nConsider (3.8) and (3.9) in the special case when M0 = I. Let ∥w∥=\n⟨w, w⟩and ∥w∥∗= max0≤t≤T ∥w∥. In this exercise we show that\nw -w0 = O(ε), and that w -w0 -w1 = O(ε2).\n\nCHAPTER 3. SCATTERING SERIES\n(a) Find an equation for w -w0. Prove that\n∥w -w0∥∗≤ε ∥M1∥infΩT ∥w∥∗\n[Hint: repeat and adapt the proof of theorem 3.]\n(b) Find a similar inequality to control the time derivative of w -w0.\n(c) Find an equation for w -w0 -w1. Prove that\nw\nw\nw\n(ε M\nΩT)2\n∥\n-\n0 -\n1∥∗≤\n∥\n1∥inf\n∥w∥∗\n8. Consider the gradient descent method applied to the linear least-squares\nproblem minx ∥Ax -b∥2. Show that\nα = ∥A∗A∥\nis a safe choice in the sense that the resulting gradient step is a con-\ntraction, i.e., the distance between successive iterates decreases mono-\ntonically.\n9. Consider J(m) any smooth, locally convex function of m.\n(a) Show that the specific choice\n⟨δJ\nα =\nδm[m(k)], δJ\nδm[m(k)]⟩\n⟨δJ\nδm[m(k)], δJ2\nδm2[m(k)] δJ [m(k)]⟩\nδm\nfor the gradient descent method results from approximating J by\na quadratic function in the direction of δJ/δm, near m(k), and\nfinding the minimum of that quadratic function.\n(b) Show that the Gauss-Newton iteration (3.12) results from approx-\nimating J by a quadratic near m(k), and finding the minimum of\nthat quadratic function.\n10. Prove the following formula for the wave-equation Hessian\nδ2J\nin\nδm1δm′\nterms of F and its functional derivatives:\nδ2J\nδm1δm′\n= F ∗F + ⟨\nδ2F\n, F\nδm1δm′\n[m] -d⟩.\n(3.14)\n\n3.5. EXERCISES\nNote: F ∗F is called the normal operator.\nSolution. To compute Hessians, it is important to expand the notation\nto keep track of the different variables, i.e., we compute\nδ2J\n.\nδm1δm′\nA first\nderivative gives\nδJ\nδm1\n= ⟨δF(m), F(m) -d⟩,\nδm1\nwhere the inner product bears on F in each factor. A second derivative\ngives\nδ2J\nδm1δm′\n= ⟨δF(m)\nδm1\n, δF(m)\nδm′\n⟩+ ⟨δ2F(m) , F\nδm1δm′\n(m) -d⟩.\nThis result is then evaluated at the base point m = m0, where δF(m0) =\nδm1\nF. The second term in the right-hand side already has the desired\nform. The first term in the right-hand-side, when paired with m1 and\nm′\n1, gives\n⟨Fm1, Fm′\n1⟩= ⟨F ∗Fm1, m′\n1⟩,\nhence it can be seen as F ∗F, turned into a bilinear form by application\nto m\n′\n1 and inner product with m1. Notice that, if we pair the whole\nequation with m1 and m′\n1, and evaluate at m = m0, we arrive at the\nelegant expression.\nδ2J\n⟨\n′\n′ m1, m1⟩= ⟨u1, u′\n1⟩+ ⟨v, u0 -d⟩,\n(3.15)\nδm1δm1\nwhere v was defined in the solution of an earlier exercise as\nδ2F(m0)\nv = ⟨\nm\nδm1δm′\n1, m′\n1⟩.\n11. Show that the spectral radius of the Hessian operator δ2J ,\nδm2 when data\nare (essentially) limited by t ≤T and ω ≤Ω, is bounded by a constant\ntimes (ΩT)2.\n\nCHAPTER 3. SCATTERING SERIES\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.325 Topics in Applied Mathematics: Waves and Imaging\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}