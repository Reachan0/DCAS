{
  "course_name": "Computer Graphics",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Graphics and Visualization",
    "Programming Languages",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Graphics and Visualization",
    "Programming Languages"
  ],
  "syllabus_content": "This course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\nCourse Meeting Times\n\nLectures: 2 lectures / week, 1.5 hours / lectures\n\nCourse Description\n\nIn this course, we will concentrate on 3D, not 2D illustration or image processing. You will learn:\n\nFundamentals of computer graphics algorithms\n\nBasics of real-time rendering and graphics hardware\n\nBasic OpenGL\n\nC++ programming experience\n\nYou will NOT learn:\n\nOpenGL and DirectX hacks\n\nSoftware packages\n\nArtistic skills\n\nGame design\n\nGrading Policy\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\n75%\n\nQuiz\n\n10%\n\nFinal Exam\n\n10%\n\nParticipation\n\n5%\n\nPrerequisites\n\nIt is useful if you have knowledge of the following:\n\nC++: All assignments are in C++\n\nCalculus, Linear Algebra: Solving equations, derivatives, integral; vectors, matrices, basis, solving systems of equations\n\nAssignments\n\nTurn in code and executable (Athena Linux).\n\nAlways turn in a README file: Describe problems, explain partially-working code, and say how long the assignment took.\n\nCoding style is important: Some assignments are cumulative.\n\nCollaboration policy: see below.\n\nLate policy:\n\nThe deadline is absolute. All late assignments will be graded 0. So, submit early, even before you might be fully done.\n\nDue Wednesday @ 8pm\n\nExtensions only considered if requested 1 week before due date\n\nMedical problems must be documented\n\nCollaboration Policy\n\nYou can chat, but code on your own (we use automated plagiarism detection software!)\n\nUse Piazza message board\n\nHelp others on Piazza message board (will help your grade!)\n\nAcknowledge your collaboration (in README)\n\nTalk to each other, get a community going\n\nTextbooks\n\nNo textbook is required\n\nRecommendations\n\nWatt, Alan.\n3D Computer Graphics\n. Addison-Wesley, 1999. ISBN: 9780201398557.\n\nBuss, Samuel R.\n3D Computer Graphics: A Mathematical Introduction with OpenGL\n. 2003. ISBN: 9780521821032. [Preview with\nGoogle Books\n]\n\nAkenine-Moller, Tomas, Eric Haines and Naty Hoffman.\nReal-Time Rendering\n. 3rd ed. A K Peters/CRC Press, 2008. ISBN: 9781568814247.\n\nShirley, Peter, Michael Ashikhmin, Steve Marschner.\nFundamentals of Computer Graphics\n. 3rd ed. A K Peters/CRC Press, 2009. ISBN: 9781568814698. [Preview with\nGoogle Books\n]",
  "files": [
    {
      "category": "Resource",
      "title": "Curves & surfaces",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/96f343745a4bf6234a3c4f7cf3049f23_MIT6_837F12_assn1.pdf",
      "content": "6.837: Computer Graphics Fall 2012\nProgramming Assignment 1: Curves and Surfaces\nIn this assignment, you will be implementing splines and swept surfaces to model interesting shapes. The\nprimary goal of this assignment is to introduce you to splines and coordinate systems. Upon successful\ncompletion of this assignment, you will be rewarded with a powerful tool for modeling 3D shapes.\nTo get you motivated, here is an image that was generated using Maya from one of the swept surfaces\nthat your program will create.\nYou may also wish to look at some cool results from a previous year's class. The remainder of this\ndocument is organized as follows.\n1. Getting Started\n2. Summary of Requirements\n3. Starter Code\n4. Implementing Curves\n5. Implementing Surfaces\n6. Extra Credit\n7. Submission Instructions\n8. Helpful Hints\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nGetting Started\nThe sample solution a1soln is included in the starter code distribution. You may need to change the its\npermissions to be able to run it (type chmod u+x a1soln at the Athena prompt).\nFirst, load up core.swp (type ./a1soln swp/core.swp). Note that this is a different way of reading the\ncontents of a file than the method used in assignment zero. In that assignment, we used the < operator to\nput the contents of the file into standard input. Now, we will tell the program the filename as a command\nline argument, and the program will open and read the file itself (the starter code does this already for\nyou). If you look at main(...), you'll see that an ifstream object is created from the filename given on the\ncommand line, and the curves files are read from this stream. In the function parseFile (in parse.cpp),\nthe curve data is read from the file into data structures that the code you write will use. For this assignment,\nparsing the files is already done for you.\nThe file core.swp contains four curves. Their control points are shown in yellow, and the resulting curves\nare shown in white. You can toggle the display of local coordinate frames using the c key, and you can toggle\nthe display of control points using the p key.\nThen, load up weird.swp. This displays a generalized cylinder. You can toggle the display mode of the\nsurface with the s key. By default, it starts with smooth shading, but you can also turn the surface off to\nget a better look at the curves, or render a wireframe surface with normals.\nTry viewing some of the other SWP files to get an idea of the sorts of curves and surfaces that these\ntechniques can generate.\nSummary of Requirements\nThis section summarizes the core requirements of this assignment. You will find more details regarding\nthe implementation of these requirements later in this document. Note that, for this and most future\nassignments, you are free to start your implementation from scratch (in C++). However, the provided\nstarter code implements a number of these requirements already, so we encourage you to either start from\nthat or read through the files to see the implementation.\n2.1\nFile Input\nThis is fully implemented in the starter code. Your program must read in a specific file format (SWP)\nthat allows users to specify curves (Bezier, B-Spline, and circles) and surfaces (surfaces of revolution and\ngeneralized cylinders). Many SWP files are provided in the swp subdirectory. The specification of the file\nformat is given in the header file of the provided parser (parse.h). The starter code distribution also includes\na number of SWP files that your code must be able to read and process (if you implement the assignment\nfrom scratch and file input is broken, you will receive no credit for the assignment). Note that, for grading,\nwe may be using SWP files that are not included with the starter code.\n2.2\nUser Interface\nThis is fully implemented in the starter code. Your program must provide functionality to rotate the model\nusing mouse input and select the display mode of the curves and surfaces. If you choose to implement the\nassignment from scratch, play with the sample solution and support equivalent functionality. Again, if you\nchoose to implement the assignment from scratch and it fails to support the user interface requirements, you\nwill receive no credit.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n2.3\nCurves (45% of grade)\nYour program must be able to generate and display piecewise cubic Bezier and B-spline curves. In addition,\nit must correctly compute local coordinate frames along the curve (as described elsewhere in this document)\nand display them. You should render the curve in white, and the N, B, and T vectors in red, green, and\nblue, respectively. If you use the starter code, you can fill in the evalBezier and evalBspline functions in\ncurve.cpp and the display will be handled for you.\nYou will receive 30% of the total number of points if you implement one type of curve correctly, and 15%\nfor the other type (since you can use one curve type to implement the other using a simple transformation).\n2.4\nSurfaces (50% of grade)\nYour program must be able to generate and display surfaces of revolution (of a curve on the xy-plane around\nthe y-axis) and generalized cylinders. It must compute surface normals correctly. To demonstrate this in\nyour program, you should have two modes of display. One mode should display the surface is drawn in wire-\nframe mode with the vertex normals drawn pointing outwards from the surface. The other should display\nthe surface with smooth shading. The display functions are already implemented for you in the starter code.\nTo generate surfaces, you can fill in the makeSurfRev and makeGenCyl functions in surf.cpp. Note that,\nfor generalized cylinders, you are not required to precisely match the results of the sample solution, as it\nincludes a somewhat arbitrary choice of the initial coordinate frame.\nYou will receive 20% of the total number of points for mesh generation of each type of surface (surface\nof revolution and generalized cylinders). For proper computation and display of normals, you will receive 10%.\n2.5\nArtifact (5% of grade)\nFinally, you are to use your program to create at least two complex geometric models, which are substan\ntially different from those distributed with the assignment. They can model real-life objects or they can be\nabstract forms. Also note that the SWP file format supports multiple surfaces, so you may wish to exploit\nthis functionality.\nIn addition to the two or more SWP files, you should submit screenshots of these shapes in either\nPNG (preferred), JPEG, or GIF format. To take a screenshot on an Athena system, type add graphics;\n/mit/graphics/bin/xv, hit the Grab button, and read the directions. Alternatively, you may create your\nartifact by exporting your surfaces and rendering them using other software. The sample code provided can\nexport your surfaces to OBJ files by passing in a prefix as the second argument on the command line. It\nwill produce with one file per named surface with the prefix prepended (e.g., ./a1 swp/weird.swp foo, will\ncreate foo weird.obj).\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nStarter Code\nTo build the starter code, type make. This will compile a barely-working version of the application into a1.\nGo ahead and run this on swp/circles.swp. The starter code, as-is, is fully functional on this example. If\nyou press c, you'll also notice that the coordinate frames are computed for you.\nIf you try and load any of the other SWP files, the program will complain (and often verbosely; try\nrunning the program on swp/core.swp). Your job is to replace these complaints with real code. In particu\nlar, all the code you should need to write can be placed in curve.cpp and surf.cpp (the starter code was\ngenerated from the solution code by deleting parts from these files only).\nYou can see what functions you need to implement, as well as what they should do, in those files and\nthe associated header files curve.h and surf.h. We also encourage you to take a look at the other parts of\nthe code, especially the functions that draw the curves and surfaces. These functions give a very good idea\nof how to access and modify the various data structures that are used.\nThe starter code uses the vecmath vector library extensively. It provides classes such as Vector3f,\nVector4f and Matrix4f, and all sorts of helpful functions that operate on vectors and matrices (not only\nstuff like addition and multiplication, but dot products, cross products, and so on). Look through some of\nthe starter code to see how it is used, and also look at the individual header files in\n/mit/6.837/public/include/vecmath (it's not too long). The source code is also available on Athena via\ngit by typing:\ngit clone /mit/6.837/public/vecmath.git\nThis code also uses the vector object from the C++ Standard Template Library. There are plenty of\nexamples in the starter code that demonstrate how they're used, but if you'd like to have a gentler introduc\ntion, there is a tutorial here and a full (but less comprehensible) description here.\nNote: If you get compile errors, specifically 'error: GL/glut.h: No such file or directory', type:\nsudo aptitude install freeglut3 freeglut3-dev\n\nImplementing Curves\nYour first task is to implement piecewise cubic Bezier splines and B-splines. Given an array of control points,\nyou are to generate a set of points that lie on the spline curve (which can then be used to draw the spline by\nconnecting them with line segments). For instance, the control points shown in the picture on the left will\nresult in the piecewise uniform cubic B-spline on the right (in this example, the first three control points are\nidentical to the last three, which results in a closed curve).\nComputing points on the spline is sufficient if the only goal is to draw it. For other applications of splines,\nsuch as animation and surface modeling, it is necessary to generate more information. We'll go over these\ndetails first in two dimensions and then go up to three.\n4.1\nTwo Dimensions\nConsider a simple example. Suppose we wanted to animate a car driving around a curve q(t) on the xy-plane.\nEvaluating q(t) tells us where the car should be at any given time, but not which direction the car should\nbe pointing. A natural way to choose this direction is with the first derivative of curve: q'(t).\nTo determine the appropriate transformation at some t, we introduce some shorthand notation. First,\nwe define the position V as:\nV = q(t)\nWe define the unit tangent vector T as:\nT = q'(t)/||q'(t)||\nThen we define the normal vector N as a unit vector that is orthogonal to T. One such vector that will\nsatisfy this property is:\nN = T'/||T'||\nThen, if we assume that the car is pointed up its positive y-axis, the appropriate homogeneous transfor\nmation can computed as:\n\nN\nT\nV\nM =\n\nUnfortunately, there is a problem with this formulation. Specifically, if q has an infection point, the\ndirection of the normal will flip. In other words, the car will instantaneously change orientation by 180 de\ngrees. Furthermore, if the car is traveling along a straight line, N is zero, and the coordinate system is lost.\nThis is clearly undesirable behavior, so we adopt a different approach to finding an N that is orthogonal to T.\nWe introduce a new vector orthogonal to T that we'll call B. This is known as the binormal, and we'll\narbitrarily select it to be in pointing in the positive z-direction. Given B, we can compute the appropriate\nnormal as:\nN = B × T\nNote that N will be unit and orthogonal to both B and T, because B and T are orthogonal unit vectors.\nThis may not seem like the most intuitive method to compute the desired local coordinate frames in two\ndimensions, but we have described it because it generalizes quite nicely to three.\n4.2\nThree Dimensions\nTo find appropriate coordinate systems along three dimensions, we can't just use the old trick of selecting\na fixed binormal B. One reason is that the curve might turn in the direction of the binormal (i.e., it may\nhappen that T = B). In this case, the normal N becomes undefined, and we lose our coordinate system. So\nhere is the solution that we suggest.\nWe will rely on the fact that we will be stepping along q(t) to generate discrete points along the curve.\nIn other words, we might step along q(t) at t1 = 0, t2 = 0.1, and so on. At step i, we can compute the\nfollowing values (just as in the two-dimensional case):\nVi = q(ti)\nTi = q ' (ti).normalized()\nTo select Ni and Bi, we use the following recursive update equations:\nNi = (Bi-1 × Ti).normalized()\nBi = (Ti × Ni).normalized()\nIn these equations, normalized() is a method of Vector3f that returns a unit-length copy of the instance\n(i.e., v.normalized() returns v/||v||). We can initialize the recursion by selecting an arbitrary B0 (well,\nalmost arbitrary; it can't be parallel to T1). This can then be plugged into the update equations to choose\nN1 and B1. Intuitively, this recursive update allows the the normal vector at ti to be as close as possible to\nthe normal vector at ti-1, while still retaining the necessary property that the normal is orthogonal to the\ntangent.\ntt with the two-dimensional case, we can use these three vectors to define a local coordinate system at\neach point along the curve. So, let's say that we wanted to animate this airplane:\n\nAnd let's say that we wanted to align the z-axis of the airplane with the tangent T, the x-axis with the\nnormal N, the y-direction with the binormal B, and have the plane at position V. This can be achieved\nwith the following transformation matrix:\nN\nB\nT\nV\nM = 0\nAnd that summarizes one way to compute local coordinate systems along a piecewise spline. Although\nthe recursive update method that we proposed works fairly well, it has its share of problems. For one, it's\nan incremental technique, and so it doesn't really give you an analytical solution for any value of t that\nyou provide. Another problem with this technique is that there's no guarantee that closed curves will have\nmatching coordinate systems at the beginning and end. While this is perfectly acceptable for animating an\nairplane, it is undesirable when implementing closed generalized cylinders.\n\nImplementing Surfaces\nIn this assignment, you will be using the curves that you generate to create swept surfaces. Specifically, the\ntype of surfaces you are to handle are surfaces of revolution and generalized cylinders.\nThe following images show an example of a surface of revolution. On the left is the profile curve on the\nxy-plane, and on the right is the result of sweeping the surface about the y-axis.\nThe images below show an example of a generalized cylinder. On the left is what we'll call the sweep\ncurve, and the surface is defined by sweeping a profile curve along the sweep curve. Here, the profile curve\nis chosen to be a circle, but your implementation should also support arbitrary two-dimensional curves.\n5.1\nSurfaces of Revolution\nFor this assignment, we define a surface of revolution as the product of sweeping a curve on the xy-plane\ncounterclockwise around the positive y-axis. The specific direction of the revolution will be important, as\nyou will soon see.\nSuppose that you have already evaluated the control points along the profile curve. Then, you can gen\nerate the vertices of the surface by simply duplicating the evaluated curve points at evenly sampled values\nof rotation. This should be your first task during the implementation process.\nHowever, vertices alone do not define a surface. As we saw from the previous assignment, we also need to\ndefine normals and faces. This is where things get a little more challenging. First, let's discuss the normals.\n\nWell, we already have normal vectors N from the evaluation of the curve. So, we can just rotate these normal\nvectors using the same transformation as we used for the vertices, right? Yes, and no. It turns out that, if\nwe transform a vertex by a homogeneous transformation matrix M, its normal should be transformed by\nthe inverse transpose of the top-left 3 × 3 submatrix of M. A discussion of why this is the case appears\nin the Red Book. You can take comfort in the fact that the inverse transpose of a rotation matrix is itself\n(since rotation is a rigid transformation).\nAnother thing that you'll need to worry about is the orientation of the normals. For OpenGL to perform\nproper lighting calculations, the normals need to be facing out of the surface. So, you can't just blindly\nrotate the normals of any curve and expect things to work.\nTo appreciate this issue, observe the following Bezier curves. The difference between them is that the\nnormals (the red lines) are reversed. This is the result of just reversing the order of the control points. In\nother words, even though the curves are the same, the normals depend on the direction of travel.\nIn this assignment, we will assume that, for two-dimensional curves, the normals will always point to\nthe left of the direction of travel (the direction of travel is indicated by the blue lines). In other words, the\nimage on the left is correct. This is to be consistent with the fact that, when you're drawing a circle in\na counterclockwise direction, the analytical normals will always point at the origin. With this convention,\nyou will actually want to reverse the orientation of the curve normals when you are applying them to the\nsurface of revolution. Note that, if you implement two-dimensional curves as we described previously, you\nwill automatically get this behavior.\nWe must also keep in mind that translating the curve may disrupt our convention. Consider what hap\npens if we just take the curve on the left side, and translate it so that it is on the other side of the y-axis.\nThis is shown below (the y-axis is the thick green line).\n\nHere, the normals that were once facing towards the y-axis are now facing away! Rather than try to\nhandle both cases, we will assume for this assignment that the profile curve is always on the left of the y-axis\n(that is, all points on the curve will have a negative x-coordinate).\nSo far, we have ignored the faces. Your task will be to generate triangles that connect each repetition of\nthe profile curve, as shown in the following image. The basic strategy is to zigzag back and forth between\nadjacent repetitions to build the triangles.\nIn OpenGL, you are required to specify the vertices in a specific order. They must form a triangle in\ncounterclockwise order (assuming that you are looking at the front of the triangle). If you generate your\ntriangle faces backwards, your triangles will have incorrect lighting calculations, or even worse, not appear\nat all. So, when you are generating these triangle meshes, keep this in mind. In particular, this is where our\nassumption of counterclockwise rotation about the y-axis comes in handy.\n5.2\nGeneralized Cylinders\nBy now, you should be ready to implement generalized cylinders. They are very much like surfaces of rev\nolution; they are formed by repeating a profile curve and connecting each copy of the profile curve with\ntriangles. The difference is that, rather than sweeping the two-dimensional profile curve around the y-axis,\nyou'll be sweeping it along a three-dimensional sweep curve.\nJust as with surfaces of revolution, each copy of the profile curve is independently transformed. With\nsurfaces of revolution, we used a rotation. With generalized cylinders, we will use the coordinate system\ndefined by the N, B, T, and V vectors of the sweep curve. To put it in the context of a previous discussion,\nimagine dragging a copy of the profile curve behind an airplane that's flying along the sweep curve, thus\nleaving a trail of surface.\nThe process of selecting the correct normals and faces is very similar to how it is done for surfaces of\nrevolution. We recommend that you write your triangle meshing code as a separate function so that it may\nbe reused.\nHere is a neat example of a generalized cylinder. First, let's see the profile curve and the sweep curve.\nBoth of these are shown with local coordinate systems at points along the curves. Specifically, the blue lines\nare the Ts, the red lines are the Ns, and the green lines are the Bs.\n\nThe small curve is chosen to be the profile curve, and the large one is chosen to be the sweep curve. The\nresulting generalized cylinder is shown below.\n\nExtra Credit\nAs with the previous assignment, the extra credits for this assignment will also be ranked easy, medium,\nand hard. However, these are all relative to the individual assignment; an easy extra credit for this assign\nment will probably be will be harder than an easy one from the last assignment (and we will assign credit\naccordingly). Furthermore, these categorizations are only meant as a rough guideline for how much they'll\nbe worth. The actual value will depend on the quality of your implementation. E.g., a poorly-implemented\nmedium may be worth less than a well-implemented easy. We will make sure that you get the credit that\nyou deserve.\nIf you do any of these extra credits, please make sure that your program does not lose support for the\nSWP file format. We will be using these files to evaluate your assignment.\n6.1\nEasy\n- Render your model more interestingly. Change the materials, or even better, change the material as\nsome function of the surface. You might, for instance, have the color depend on the sharpness of the\ncurvature. Or jump ahead of what we are covering in class and read about texture mapping, bump\nmapping, and displacement mapping.\n- Implement another type of spline curve, and extend the SWP format and parser to accommodate\nit. We recommend Catmull-Rom splines, as they are C1-continuous and interpolate all the control\npoints, but feel free to try others (Bessel-Overhauser, Tension-Continuity-Bias). If you implement this\nextension, please make sure that your code can still read standard SWP files, since that's how we're\ngoing to grade it. Additionally, provide some SWP files that demonstrate that they work as they're\nsupposed to.\n- Implement another kind of primitive curve that is not a spline. Preferably, pick something that'll result\nin interesting surfaces. For instance, you might try a corkscrew shape or a Trefoil Knot. Both of these\nshapes would work great as sweep curves for generalized cylinders. Again, you'll probably want to\nextend the SWP format to support these sorts of primitives.\n- As mentioned before, the suggested method of computing coordinate frames has a problem: if a curve\nis closed, the coordinate frames are not guaranteed to line up where the curve meets itself. The sample\nsolution fixes these issues as follows. First, it detects when these errors occur (when the positions and\ntangents are identical but the normals are not). Then, it interpolates the rotational difference between\nthe start and end of the curve, and adds these to the coordinate frames along the curve. Implement\nthis solution in your code. To check your solution, you should be able to display a seamless solid for\nweirder.swp. You should also ensure that your solution does not affect the results for curves that are\nnot closed.\n- The description (and the sample code) only supports swept surfaces with C1-continuous sweep curves.\nThis is all that is required, but it may be desirable to sweep curves with sharp corners as well. Extend\nyour code to handle this functionality for piecewise Bezier curves, and provide some SWP files that\ndemonstrate the correctness of your implementation.\n6.2\nMedium\n- Implement a user interface so that it is easy for users to specify curves using mouse control. A user\nshould be able to interactively add, remove, and move control points to curves, and those curves should\nbe displayed interactively. This application may be implemented as an extension to your assignment\ncode, or as a standalone executable. Either way, it should be able to export SWP files. You may\nchoose to implement a purely two-dimensional version, or, for additional credit, a three-dimensional\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\ncurve editor. If you choose the latter, you should provide an intuitive way of adding and moving control\npoints using mouse input (this is somewhat challenging, since you will be trying to specify and move\nthree-dimensional points using a two-dimensional input device). If you choose to implement this, you\nmay want additional user interface widgets such as menus and sliders. We recommend Qt for this.\n- In this assignment, the suggested strategy for discretizing splines involves making uniform steps along\nthe parametric curve. However, it is difficult to choose the appropriate number of steps, since the\ncurvature of splines can vary quite a bit. In fact, any choice of a uniform step size will result in\neither too few steps at the sharpest turns, or too many in the straight regions. To remedy this\nsituation, implement a recursive subdivision technique to adaptively control the discretization so that\nmore samples are taken when needed. See Buss (Chapter VII.3) for details on one way this can be\nperformed.\n- We can extend the generality of generalized cylinders by allowing the profile curve to be controlled\nby other curves as well. For instance, you may wish to not only control the orientation of the profile\ncurve using a sweep curve, but the relative scale of the profile curve using a scale curve. This is more\nchallenging than it may sound at first, since you must worry about the fact that the sweep curve may\nnot have the same number of control points as the scale curve.\n- Implement a birail surface. Birail surfaces sweep a profile curve along two sweep curves which control\nthe scale and orientation of the profile curve. Specifically, the two profile curves (the rails) are attached\nto control points on the sweep curves. The profile is the swept along both of the rails, while keeping\nthe control points on the rails using scale and rotation.\n- Implement piecewise Bezier and B-spline surfaces. If you do this extension, you should also extend the\nSWP format so that control points can be specified. Additionally, you should provide some sample\nSWP files to demonstrate that your technique will work.\n6.3\nHard\n- The techniques covered in this assignment are just one of many ways that modelers create shapes.\nAnother popular technique is that of subdivision surfaces. At a high level, you can think of this\ntechnique as starting with a polygon mesh and defining the desired surface as the limit of a set of\nsubdivision rules that are applied to the mesh. Implement this technique. You may find this helpful.\nAlso note that, if you plan to take 6.839, you'll have an advantage, since this is one of the projects in\nthat class.\n6.4\nTangential\n- The sample code will export your surfaces to OBJ format. Files in this format can be imported into\na variety of 3D modeling and rendering packages, such as Maya. This software has been used for a\nnumber of video games, feature films (including The Lord of the Rings), and television shows (including,\nbelieve it or not, South Park). You may wish to make your artifact more interesting by rendering it in\nthis software, as it is available to you on Athena. Since 6.837 isn't about learning software tools, you\nwon't receive extra credit for this. However, your artifact will look amazing when we post the images\non the web for everyone to see. The image at the beginning of this document was generated using this\nsoftware.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nSubmission Instructions\nAs with the previous assignment, you are to write a README.txt or optionally a README.html that answers\nthe following questions:\n- How do you compile and run your code? Provide instructions for Athena Linux.\n- Did you collaborate with anyone in the class? If so, let us know who you talked to and what sort of\nhelp you gave or received.\n- Were there any references (books, papers, websites, etc.) that you found particularly helpful for\ncompleting your assignment? Please provide a list.\n- Are there any known problems with your code? If so, please provide a list and, if possible, describe\nwhat you think the cause is and how you might fix them if you had more time or motivation. This\nis very important, as we're much more likely to assign partial credit if you help us understand what's\ngoing on.\n- Did you do any of the extra credit? If so, let us know how to use the additional features. If there was\na substantial amount of work involved, describe how you did it.\n- Do you have any comments about this assignment that you'd like to share? We know this was a tough\none, but did you learn a lot from it? Or was it overwhelming?\nAs with the previous assignment, you should submit your assignment online. Your submission\nshould contain:\n- Your source code.\n- A compiled executable named a1.\n- The README.txt file.\n- At least two SWP files that define your new artifacts.\n- Images of your artifacts in PNG, JPEG, or GIF format.\nWe will NOT accept late submissions.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nHelpful Hints\n- Get started early. This assignment does not need to involve too much additional code (the sample\nsolution adds about 130 lines to the starter code), but it is remarkably easy to make mathematical and\nlogical errors that are very difficult to debug.\n- Use features of the interface to debug your code. For instance, by toggling through the curve drawing\nmodes, you can see the coordinate systems that are computed at each evaluated point and check if they\nmake sense. If your surfaces don't look right, use the wireframe mode to check whether the normals\nare pointing outwards and if your triangulation is correct.\n- Exploit code modularity. B-spline control points can be converted to Bezier control points via a matrix\nmultiplication, so you can reuse your Bezier code to generate the appropriate coordinate frames. This\nalso makes implementing other types of splines (e.g., Catmull-Rom) relatively easy.\n- Look at the headers in the vecmath vector library. It's brief, and the library provides numerous\nfunctions that you will find indispensable. You can just call cross, rather than writing it yourself and\nworrying if you did it right.\n- Study the starter code. It will give you some insight into how to access the relevant data structures,\nand there are numerous helpful comments throughout the code. Furthermore, you may find certain\nparts of code helpful. For instance, the code that draws the local coordinate frames on curves is quite\nsimilar to the code that you'll want to use for generalized cylinders.\n- If you are working on your own computer and you want to use the starter code, you'll probably need\nto recompile the vecmath vector library for your system. The source code is available on Athena via\ngit as mentioned earlier. However, this is the extent of support that we will provide. 6.837 exclusively\nsupports Athena Linux; if you choose to work on your own system, you're still responsible for ensuring\nthat your code runs on Athena.\n- You can test curves and surfaces independently by using the provided circle primitives. Take a look at\ntor.swp, which draws a torus as a swept surface without using any spline curves.\n- Get started early. We know that we're repeating ourselves, but we want to emphasize that this is a\nmuch more difficult project that the previous one.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Hierarchical modeling, skinning",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/59e619e792b658927c8e79f3ab0116ec_MIT6_837F12_assn2.pdf",
      "content": "6.837: Computer Graphics Fall 2012\nProgramming Assignment 2: Hierarchical Modeling and SSD\nIn this assignment, you will construct a hierarchical character model that can be interactively controlled\nwith a user interface. Hierarchical models may include humanoid characters (such as people, robots, or\naliens), animals (such as dogs, cats, or spiders), mechanical devices (watches, tricycles), and so on. You will\nimplement skeletal subspace deformation, a simple method for attaching a \"skin\" to a hierarchical skeleton\nwhich naturally deforms when we manipulate the skeleton's joint angles.\nThis document is organized into the following sections:\n1. Getting Started\n2. Summary of Requirements\n3. Hierarchical Modeling\n4. Skeletal Subspace Deformation\n5. Extra Credit\n6. Submission Instructions\nGetting Started\nDownload the starter code as provided,\nbuild the executable with make, and run the resulting executable on\nthe first test model: (./a2 data/Model1). Two windows will pop up. One will be empty, and will eventually\ncontain a rendering of your character. The other contains a list of articulation variables or simply joints.\nBy clicking on joint names, a slider will appear that lets you manipulate that variable. By shift-clicking and\ncontrol-clicking multiple names, you can pop up multiple sliders. You can change the camera view using\nmouse control just like in previous assignments: the left button rotates, the middle button moves, and the\nright button zooms. You can press a to toggle drawing of the coordinate axes.\nThe sample solution a2soln shows a completed version of the homework, including loading and displaying\na skeleton, loading a mesh that is bound to the skeleton, and deforming the skeleton and mesh based on the\njoint angles. You can press s to toggle between displaying the skeleton and displaying the mesh.\nSummary of Requirements\n2.1\nHierarchical Model (40% of grade)\nFor part one of this assignment, you are required to correctly load, display, and manipulate a hierarchical\nskeleton. Your implementation must be able to correctly parse any of the provided skeleton files (*.skel),\nconstruct a joint hierarchy, and use a matrix stack in conjunction with OpenGL primitives to render the\nskeleton. Finally, you will write the code to set joint transforms based on joint angles passed in from the\nuser interface.\n\n2.2\nSkeletal Subspace Deformation (55% of grade)\nFor the second part of this assignment, you will implement skeletal subspace deformation to attach a \"skin\"\nto your skeleton. SSD will allow you to pose true characters, not just skeletons. This part first requires\nyou to adapt your assignment 0 code to parse a mesh without normals and generate them at display time.\nYou will also need to write another parser to load attachment weights, which specify, for each vertex, the\nimportance of each joint. Finally, you will implement the actual SSD algorithm which requires applying a\nnumber of operations to your transformation hierarchy.\n2.3\nArtifact (5% of grade)\nThe artifact for this assignment will be easy to create: simply take a screenshot of one of the character\nmodels and submit it in PNG, JPEG, or GIF format. You can take a screenshot by selecting \"Save bitmap\nfile\" from the file menu in the user interface. Please take a few minutes to pose your character interestingly\nand choose a reasonable camera position. A straightforward extension would be to load multiple characters\nand pose them interacting together in an interesting way. You may also want to add a floor by drawing a\nflattened cube.\nHierarchical Modeling\nIn previous assignments, we addressed the task of generating static geometric models. As we've seen, this\napproach works quite well for generating objects such as spheres, teapots, wineglasses, statues, and so on.\nHowever, this approach is limited when applied to generating characters that need to be posed and ani\nmated. For instance, in a video game, a model of a human should be able to interact with the environment\nrealistically by moving its limbs to imitate walking or running.\nOne approach to creating these animations is to individually manipulate vertices and control points.\nDoing so quickly becomes tedious. A better approach is to define a hierarchy such as a skeleton for a human\nfigure and few control parameters such as joint angles of the skeleton. By manipulating these parameters,\nsometimes called articulation variables or joints, a user can pose the hierarchical shapes more easily. Fur\nthermore, the surface of the object can also be computed as a function of the same articulation variables.\nAn example of a skeleton hierarchy for a human character is shown below.\nEach joint in the hierarchy is associated with a transformation, which defines its local coordinate frame\nrelative to its parent. These transformations will typically have translational and rotational components.\nTypically, only the rotational components are controlled by articulation variables given by the user (chang\ning the translational component would mean stretching the bone). We can determine the global coordinate\nframe of a node (that is, a coordinate system relative to the world) by multiplying the local transformations\ndown the tree.\n\nThe global coordinate frames of each node can be used to generate a character model by using them\nto transform geometric models for each joint. For instance, the torso of the character can be drawn in the\ncoordinate frame of the pelvis, and the thighs of the character can be drawn in the coordinate frame of the\nhips. Make sure you understand what these global coordinate frames mean; in what space is the input? In\nwhat space is the output?\nIn your code, your model will be drawn in this manner. By placing, say, a cylinder in the coordinate\nframe of the left hip, you can draw a simple thigh for your character. Doing this for all nodes in the hierarchy\nwill result in simple stick figures, such as the ones shown below.\n3.1\nMatrix Stack (5% of grade)\nYour first task is to implement a matrix stack. A matrix stack keeps track of the current transformation\n(encoded in a matrix) that is applied to geometry when it is rendered. It is stored in a stack to allow you\nto keep track of a hierarchy of coordinate frames that are defined relative to one another - e.g., the foot's\ncoordinate frame is defined relative to the leg's coordinate frame.\nOpenGL provides a framework for maintaining a matrix stack. However, in this assignment you will\nbe building your own, and not using OpenGL's. By building our own matrix stack, we will have a much\nmore flexible data structure independent of the rendering system. For instance, we could maintain multiple\nhierarchical characters simultaneously and perform collision detection between them.\nIn your implementation, if no current transformation is applied to the stack, then it should return the\nidentity. Each matix transformation pushed to the stack should be multiplied by the previous transforma\ntion. This puts you in the correct coordinate space with respect to its parent. The interface for the matrix\nstack has been defined for you in MatrixStack.h. The implementation in MatrixStack.cpp is currently\nempty and must be filled in. We recommend you simply use an STL vector for the stack, but you may use\nany data structure you wish.\nWhen rendering, you will be pushing and popping matrices onto and off the stack. After each push or\npop, you should call glLoadMatrixf( m matrixStack.top() ) to tell OpenGL that you want all geometry\n\nto be transformed by the current top matrix. Subsequent OpenGL primitives (glVertex3f, glutSolidCube,\netc) will then be transformed by this matrix. The starter code's SkeletalModel class comes equipped with\nan instance of MatrixStack called m matrixStack. The starter code also pushes the camera matrix as the\nfirst item on the stack.\n3.2\nHierarchical Skeletons (30% of grade)\n3.2.1\nFile Input\nYour next task is to parse a skeleton that has been built for you. The starter code automatically calls\nthe method SkeletalModel::loadSkeleton with the right filename (found in SkeletalModel.cpp). The\nskeleton file format (.skel) is straightforward. It contains a number of lines of text, each with 4 fields\nseparated by a space. The first three fields are floating point numbers giving the joint's translation relative\nto its parent joint. The final field is the index of its parent (where a joint's index is the zero-based order\nit occurs in the .skel file), hence forming a directed acyclic graph or DAG of joint nodes. The root node\ncontains -1 as its parent and its translation is the global position of the character in the world.\nEach line of the .skel file refers to a joint, which you should load as a pointer to a new instance of the\nJoint class. You can initialize a new joint by calling\nJoint *joint = new Joint;\nBecause Joint is a pointer, note that we must initialize it with the 'new' keyword to allocate space in memory\nfor this object that will persist after the function ends. (If you try to create a pointer to a local variable,\nwhen the local variable goes out of scope the pointer will become invalid, and attempting to access it will\ncause a crash.) Also note that when dealing with a pointer to a object, you must access the member variables\nof the object with the arrow operator -> instead of . (e.g., joint->transform), which reflects the fact that\nthere is a memory lookup involved.\nYour implementation of loadSkeleton must create a hierarchy of Joints, where each Joint maintains a\nlist of pointers to Joints that are its children. You must also populate a list of all Joints m joints in the\nSkeletalModel and set m rootJoint to point to the root Joint.\n3.2.2\nDrawing Stick Figures\nTo ensure that your skeleton was loaded correctly, we will draw simple stick figures like the ones above.\nJoints We will first draw a sphere at each joint to see the general shape of the skeleton. The starter code\ncalls SkeletalModel::drawJoints. Your task is to create a separate recursive function that you should call\nfrom drawJoints that traverses the joint hierarchy starting at the root and uses your matrix stack to draw\na sphere at each joint. We recommend using glutSolidSphere( 0.025f, 12, 12 ) to draw a sphere of\nreasonable size.\nYou must use your matrix stack to perform the transformations. You will receive no credit if you use the\nOpenGL matrix stack. To do this, you must push the joint's transform onto the stack, load the transform\nby calling glLoadMatrixf, recusively draw any of its children joints, and then pop it off the stack. You may\nfind it helpful to verify your rendering by comparing it to the sample solution.\nBones A stick figure without bones is not very interesting. In order to draw bones, we will draw elongated\nboxes between each pair of joints in the method SkeletalModel::drawSkeleton. As with joints, it is up to\nyou to define a separate recursive function that will traverse the joint hierarchy. At each joint, you should\ndraw a box between the joint and the joint's parent (unless it is the root node).\n\nUnfortunately, OpenGL's box primitive glutSolidCube can only draw cubes centered around the ori\ngin; therefore, we recommend the following strategy. Start with a cube with side length 1 (simply call\nglutSolidCube( 1.0f )). Translate it in z such that the box ranges from [-0.5, -0.5, 0]T to [0.5, 0.5, 1]T .\nScale the box so that it ranges from [-0.025, -0.025, 0]T to [0.025, 0.025, ]T where is the distance to the\nnext joint in your recursion. Finally, you need to rotate the z-axis so that it is aligned with the direction\nto the parent joint: z = parentOffset.normalized(). Since the x and y axes are arbitrary, we recommend\nmapping y = (z × rnd).normalized(), and x = (y × z).normalized(), with rnd supplied as [0, 0, 1]T .\nFor the translation, scaling, and rotation of the box primitive, you must push the transforms onto the\nstack before calling glutSolidCube, but you must pop it off before drawing any of its children, as these\ntransformations are not part of the skeleton hierarchy. As with the joints, you should verify the correctness\nof your implementation with the sample solution.\n3.3\nUser Interface (5% of grade)\nWhenever a joint rotation slider is dragged, the application calls setJointTransform, passing in the index\nof the joint to be updated and the Euler rotation angles set by the user. You should implement this function\nto set rotation component of the joint's transformation matrix appropriately.\nSkeletal Subspace Deformation\nHierarchical skeletons allowed you to render and pose vaguely human-looking stick figures in 3D. In this sec\ntion, we will use Skeletal Subspace Deformation to attach a mesh that naturally deforms with the skeleton.\nIn the approach used to render the skeleton, body parts (spheres and cubes) were drawn in the coordinate\nsystem of exactly one joint. This method, however, can generate some undesirable artifacts. Observe the\nvertices near a joint.\nThis is a cross-sectional view of a skeleton with a mesh attached to each node. Notice how the two meshes\ncollide with each other as the skeleton bends. Our stick figures hide this artifact by drawing spheres at each\njoint. However, it is only a quick fix for the fact that the aforementioned approach rigidly attaches vertices of\nthe character model to individual nodes of the hierarchy. This is an unrealistic assumption for more organic\ncharacters (such as humans and animals) because skin is not rigidly attached to bones. It instead deforms\nsmoothly according to configuration of the bones, as shown below.\n\nThis result was achieved using skeletal subspace deformation, which positions individual vertices as a\nweighted average of transformations associated with nearby nodes in the hierarchy. For example, a vertex\nnear the elbow of the model is positioned by averaging the transformations associated with the shoulder and\nelbow joints. Vertices near the middle of the bone (far from a joint) are affected by only one joint -- they\nmove rigidly, as they did in the previous setup.\nMore generally, we can assign each vertex a set of attachment weights which describes how closely it\nfollows the movement of each joint. A vertex with a weight of one for a given joint will follow that joint\nrigidly, as it did in the previous setup. A vertex with a weight of zero for a joint is completely unaffected\nby that joint. Vertices in between are blended--we compute their position as if they were rigidly attached\nto each joint, then average these positions according to the weights we've assigned them.\nIn the previous section, a vertex was defined in the local coordinates of a given joint--you probably used\nmethods like glutSolidCube, then transformed the entire object via a translation to the joint's location.\nNow, however, vertices don't belong to a single joint, so we can't define vertices in the local coordinate frame\nof the joint they belong to. Instead, we define the mesh for the entire body, and keep track of the bind\npose--the pose of each joint in the body such that the bones match up with the locations of the vertices\nin the mesh. Imagine taking the skin of a character, then fitting a skeleton inside that skin. The skeleton\nwhich matches up with the skin's position is in the character's bind pose.\nLet's say that p is the position of a vertex in a character's coordinate frame in the bind pose. Say that p\nis affected by joint 1 and joint 2. Let's also say that the bind pose transformation of joint 1 (the transforma\ntion which takes us from the local coordinate frame of joint 1 before the character has been animated to the\ncharacter's coordinate frame) is B1. Finally, the transformation from joint 1's local coordinate frame to the\ncharacter's coordinate frame after animation is T1. Then the position of our vertex after transformation,\nif that vertex were rigidly attached to joint 1, would be T1B-1 p. Notice that we have to first transform\nthe point into the local coordinate system of the joint (B-1 p) before transforming it (remember, p is in\nthe character's bind pose coordinate system). Similarly, the bind transformation of joint 2 is described by\nB2, and T2 describes the transformation from the unanimated local coordinate frame of joint 2 to the an\nimated character coordinate frame. Then the vertex's position, if it were rigidly attached to joint 2, would\nbe T2B-1 p. However, say that the given vertex is near the connection of two bones corresponding to joint\n1 and joint 2, and we want it to be attached to both joints. We assign each joint a weight according to\nhow much influence that joint should have on the vertex. Weights will usually range between 0 and 1 for\neach joint, and the weights for all joints will usually sum to 1. We want it tied to joint 1 with a weight of\nw, so it is tied to joint 2 with a weight of (1 - w). Then we compute the final position of the vertex as\nwT1B-1 p + (1 - w)T2B-1 p.\nNote that since we usually only have one bind pose for a character, the inverse bind transformations\nB-1 need to be computed only once. On the other hand, since we want to animate the character using our\ni\nuser interface, the animation transforms Ti need to be recomputed every time the joint angles change. This\nimplies that the vertex positions will also need to be updated whenever the skeleton changes. (Although\nrecomputing Ti is relatively cheap on a character with few joints, updating the entire mesh can be quite\nexpensive. Modern games typically perform SSD on many vertices in parallel using graphics hardware.)\n4.1\nFile Input: Bind Pose Mesh (5% of grade)\nTo get started, we will first need to adapt your code from assignment 0 to load the bind pose vertices from\nan OBJ file. The starter code automatically calls Mesh::load with the appropriate filename. The only\ndifference between this part and assignment 0 is that the meshes we provide for you do not include normals.\nInstead, we will generate them on-the-fly when we render. Your code should populate the bindVertices\nand faces fields of the mesh. Notice that our Mesh struct comes with two copies of vertices: the bind pose\n\nand the current pose. We will render from the current pose vertices, which are generated by transformations\nof the bind pose vertices. The starter code makes the initial copy for you.\n4.2\nMesh Rendering (5% of grade)\nNext, we will verify the correctness your mesh loader by rendering the mesh. The starter code calls\nMesh::draw automatically with the right filename. Be sure to render from m mesh.currentVertices and\nnot m mesh.bindVertices.\nUnlike meshes from previous assignments, these meshes do not provide any per-vertex normals since they\nwere not computed analytically. Instead, we will generate a single normal for each triangle on-the-fly inside\nrendering loop by taking the cross product of the edges. Don't forget to normalize your normals. Note how\nyour model appears \"faceted\": the lighting is discontinuous between neighboring faces because the normals\nchange abruptly.\n4.3\nFile Input: Attachment Weights (5% of grade)\nThe last thing we must load are the attachment weights. The starter code calls Mesh::loadAttachments\nautomatically with the right filename. The attachment file format (.attach) is straightforward. It contains\na number of lines of text, one per vertex in your mesh. Each line contains as many fields as there are joints,\nminus one, separated by spaces. Each field is a floating point number that indicates how strongly the vertex\nis attached to the (i + 1)-th joint. The weight for the 0th joint, the root, is assumed to be zero.\nYour code should populate the attachments field of the mesh. We recommend the starter code's data\nstructure, where m mesh.attachments is a vector< vector< float > >. The inner vector contains one\nweight per joint, and the outer vector has size equal to the number of vertices.\n4.4\nImplementing SSD (40% of grade)\nFinally, we will implement SSD as described above. We will first compute all the transformations necessary\nfor blending the weights and then use them to update the vertices of the mesh.\n4.4.1\nComputing Transforms\nAs we describe above, we must compute the bind pose world to joint transformations (once) and the animated\npose joint to world transformations (every time the skeleton is changed). The starter code automatically\ncalls computeBindWorldToJointTransforms and updateCurrentJointToWorldTransforms at the appro\npriate points in the code.\ncomputeBindWorldtoJointTransforms should set the bindWorldToJointTransform matrix of each Joint.\nYou should use a recursive algorithm similar to the one you used for rendering the skeleton. Be careful with\nthe order of matrix multiplications. Think carefully about which space is the input and which space is the\noutput.\nupdateCurrentJointToWorldTransforms is called whenever the skeleton changes. Your implementation\nshould update the currentJointToWorldTransform matrix of each Joint, and will be very similar to your\nimplementation of computeBindWorldtoJointTransforms. But once again, be careful about which spaces\nyou're mapping between. One convenient method for debugging is that if your skeleton did not change (i.e.,\nyou did not touch any sliders), the bind world to joint transform for any Joint should be the inverse of the\ncurrent joint to world tranform.\n\n4.4.2\nDeforming the Mesh\nFor the final part your assignment, you will deform the mesh according to the skeleton and the attachment\nweights. Since you've populated all the appropriate data structures, your implementation should be straight\nforward. The starter code calls SkeletalModel::updateMesh whenever the sliders change. Your code should\nupdate the current position of each vertex in m mesh.currentVertices according to the current pose of the\nskeleton by blending together transformations of your bind pose vertex positions in m mesh.bindVertices.\nIf you implemented SSD correctly, your solution should match the sample solution. Feel free to change\nthe appearance of your characters and extend your code to pose multiple characters together to make an\ninteresting scene.\nExtra Credit\nAs with the previous assignment, the extra credits for this assignment will also be ranked easy, medium, and\nhard. These categorizations are only meant as a rough guideline for how much they'll be worth. The actual\nvalue will depend on the quality of your implementation. E.g., a poorly-implemented medium may be worth\nless than a well-implemented easy. We will make sure that you get the credit that you deserve.\n5.1\nEasy\n- Generalize the code to handle multiple characters by storing multiple skeletons, meshes, and attachment\nweights. Optionally, you can reuse data between multiple instances of the same character.\n- Employ OpenGL texture mapping to render parts of your model more interestingly. The OpenGL Red\nBook covers this topic in Chapter 9, and it provides plenty of sample code which you are free to use.\n- Simulate the appearance of a shadow or reflection of your model on a floor using OpenGL. While\nperforming these operations in general is quite complicated, it is relatively easy when you are just\nassuming that a plane is receiving the shadows. The OpenGL Red Book describes one way to do this\nin Chapter 14.\n- For your SSD implementation, use pseudo-colors to display your vertex weights. For example, assign\na color with a distinct hue to each joint, and color each vertex in the model according to the assigned\nweights by computing the corresponding weighted average of joint colors.\n- There are numerous other tricks that you might try to make your model look more interesting. Feel\nfree to implement extensions that are not listed here, and we'll give you an appropriate amount of\nextra credit.\n5.2\nMedium\n- Build your own model out of generalized cylinders and surfaces of revolution, and pose it using SSD.\n- Implement intuitive manipulation of articulation variables through the model display. For instance, if\nthe elbow joint is active, the user should be able to click on the arm and drag it to set the angle (rather\nthan using the slider). For an example of such an interface, give Maya a try.\n- Implement a method of animating your character by using interpolating splines to control articulation\nvariables. This method of animation is known as keyframing. You may either allow input from a text\nfile, or for additional credit, you may implement some sort of interface that allows users to interactively\nmodify the curves.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n5.3\nHard\n- Implement pose space deformation. This method is an alternative to skeletal subspace deformation\nwhich often gives higher-quality results.\n- Implement inverse kinematics, which solves for articulation variables given certain positional con\nstraints. For instance, you can drag the model's hand and the elbow will naturally extend. Your code\nshould allow interactive manipulation of your model through the drawing window.\n- Implement mesh-based inverse kinematics, which allows a model to be posed without any underlying\nskeleton.\nSubmission Instructions\nAs with the previous assignment, you are to write a README.txt,pdf that answers the following questions:\n- How do you compile and run your code? Provide instructions for Athena Linux.\n- Did you collaborate with anyone in the class? If so, let us know who you talked to and what sort of\nhelp you gave or received.\n- Were there any references (books, papers, websites, etc.) that you found particularly helpful for\ncompleting your assignment? Please provide a list.\n- Are there any known problems with your code? If so, please provide a list and, if possible, describe\nwhat you think the cause is and how you might fix them if you had more time or motivation. This\nis very important, as we're much more likely to assign partial credit if you help us understand what's\ngoing on.\n- Did you do any of the extra credit? If so, let us know how to use the additional features. If there was\na substantial amount of work involved, describe what how you did it.\n- Got any comments about this assignment that you'd like to share?\nSubmit your assignment as a single archive (.tar.gz or .zip).\nIt should contain:\n- Your source code.\n- A compiled executable named a2.\n- Any additional files (OBJs, textures) that are necessary.\n- The README file.\n- Your artifact(s) in PNG, JPEG, or GIF format.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Physically-based simulation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/2e73bf4e85143a357755d7da9d42b29a_MIT6_837F12_assn3.pdf",
      "content": "6.837: Computer Graphics Fall 2012\nProgramming Assignment 3: Physical Simulation\nPhysical simulation is used in movies and video games to animate a variety of phenomena: explosions, car\ncrashes, water, cloth, and so on. Such animations are very difficult to keyframe, but relatively easy to\nsimulate given the physical laws that govern their motion. In this assignment, you will be using springs to\nbuild a visually appealing simulation of cloth, as shown below. Start early as this assignment will require\nyou to think about the design of your code!\nThe remainder of this document is organized as follows:\n1. Getting Started\n2. Summary of Requirements\n3. Time Integrators\n4. Physical Simulation\n5. Particle System Cloth\n6. Extra Credit\n7. Submission Instructions\n\nGetting Started\nTake a look at the sample solution for a demonstration of what you'll be implementing.\nRun the a3soln file with parameters 'e', 't' or 'r' and watch as the \"cloth\" falls. You may reset the simulation\nby pressing r, or you can flap the cloth around by pressing s. If you wish to toggle the wireframe view, press\nw. Your task will be to build a similar simulation.\nThe other files in this directory provide a simple skeleton OpenGL application. Compile the code with\nmake and run it with a3. Remember, incremental debugging is your friend! We recommend, for debugging\npurposes, trying to get the code to display a single particle, and then moving on to implementing the\nintegrators.\nFor this assignment, you will have to put thought into the design of your solution. We provide you with\nsome starter code, although feel free to change things as you go- it is merely a way to help guide you through\nthe assignment. In general, you should spend some time thinking about what sorts of functions or classes to\nwrite before you begin. Start early!\nSummary of Requirements\nAgain, you do not need to use all of the provided starter code (but we recommend it). This is a challenging\nassignment that requires you to be a good code designer and tester. To ensure partial credit, we suggest the\nfollowing steps.\nFirst, you will begin by implementing two numerical methods for solving ordinary differential equations:\nEuler and the Trapezoidal Rule. You will test these on a simple first order system that we covered in class.\nIt is important that you abstract the time integrator from the system. A time integrator should be general\nenough to be able to take any step for any system. This step is independent of the next steps as we will\nprovide you with a reasonable integrator (RK4).\nSecond, you will implement a second order system, a simple pendulum, consisting of two particles with a\nspring connecting them. This will require you to implement three types of forces: gravity, viscous drag, and\nsprings. Each of these forces will be necessary later to create your cloth simulation.\nThird, you will extend your simple pendulum to create a string of particles with four particles. This will\nallow you to incrementally test your spring implementation before you begin assembling the cloth.\nFinally, using springs, you are to assemble a piece of cloth. It should be at least an eight-by-eight grid of\nparticles. You will need to implement structural, shear, and flexion springs.\nYour application should display a wireframe animation of the cloth. You will receive extra credit for imple\nmenting smooth-shading similar to the one shown in the sample solution. Your application should also allow\nthe user to move the cloth in some way. It can be as simple as a keystroke that makes the cloth move back\nand forth, as implemented in the sample solution.\nYou should provide an executable called a3 that takes two parameters. The first should be a character, e,\nt or r that selects the solver (Euler , Trapezoidal or RK4). The second is an optional stepsize used by the\n\nsolver. Your application should have a key 't' to togle between showing the cloth, the simple system, and\nthe pendulum. Alternatively, if you are lazy, show all of them simultaneously.\nTime Integrators (TimeStepper.xpp)\nIt is important for you to understand the abstraction between time integrators and particle systems. The\ntime integrator does not know anything about the physics of the system. It can request the particle system\nto compute the derivatives using the system's evalF method. This function is the critical communication\nchannel between a system and an integrator. It takes as input a state vector and returns a derivative vector\nfor this particular state, which are both represented as arrays regardless of the precise type of particle\nsystem (only the size of the array varies). This allows integrators to be general and reusable.\nA particle system stores its current state X, but the time integrator might request the derivatives for\na different state, in particular for the trapezoidal method. It is critical that the particle system uses the\ncorrect state, the one requested by the call to evalF, to compute the derivatives. Make sure you understand\nthe difference between this internal state of the system and that requested by the integrator.\nTo re-emphasize, your integrator should be modular and be able to step any system, while your particle\nsystem should be very careful to compute forces at the requested state, which is potentially different from\nits current state. Do not be misled by the Euler integrator where both states are the same, or you will suffer\nmiserably when implementing the trapezoidal rule.\n3.1\nRefresher on Euler and Trapezoidal Rule\nThe simplest integrator is the explicit Euler method. For an Euler step, given state X, we examine f(X, t)\nat X, then step to the new state value. This requires to pick a step size h, and we take the following step\nbased on f(X, t), which depends on our system.\nX(t + h) =X + hf(X, t)\nThis technique, while easy to implement, can be unstable for all but the simplest particle systems. As a\nresult, one must use small step sizes (h) to achieve reasonable results.\nThere are numerous other methods that provide greater accuracy and stability. For this problem set, we will\nuse the Trapezoidal approach, which works by using the average of the force f0 at the current state and the\nforce f1 after an Euler step of stepsize h:\nf0 =f(X, t)\nf1 =f(X + hf0, t + h)\nh\nX(t + h) =X +\n(f0 + f1)\nThis method makes it critical for the particle system to be able to evaluate the forces at a state X + hf0\nother than its current state X. The state of the particle system should not be updated until reaching the\nlast equation.\n\n3.2\nSimple Example 20% (simpleSystem.cpp)\nYou will implement Euler's and the Trapezoidal Rule. It should use the a particle system's evalF method to\ncalculate the derivative at the appropriate state. It should set the system's state to the updated values. Look\nat the abstract class ParticleSystem. It provides you with methods for getting and setting the system's\nstate.\nYou will test each of your implementations on the simple first-order ODE similar to one we saw in class.\nThis system has a single particle and its state is defined by its x-y-z coordinates:\n⎛\n⎞\nx\nXt =\ny\n⎝\n⎠\nz\nAnd the right-hand side is\n⎛\n⎞\n-y\nf(X, t) = ⎝ x ⎠\nThis is only a first-order system and the right-hand side of the ODE does not describe physical forces.\nYou do not need to use the trick we used in class to turn Newtonian systems into first-order systems. The\narrays passed between the system and the integrator are of length 3. The z coordinate does not do anything\ninteresting but we kept it to make the various systems in this problem set more consistent and use Vector3f.\nImplement evalF in SimpleSystem which inherits from the abstract ParticleSystem class. As iterated\nabove, you should pass the system to the takeStep method, who will take care of updating the system's\nstate.\nIn this case we only have a single particle, but the next problems will require you to handle multiple\nparticles, so make sure you account for that. We have given you a hint by characterizing the state as a 1D\narray of Vector3f (review the lecture notes to see how you should be representing the state of a system).\nevalF evaluates and returns f(X, t) (which is the velocity in this case) given any state X of the system.\nBoth evalF and your methods in Integrator should not be modifying individual particles within the system.\nevalF should take in a system state and return the derivatives associated with that state. The Integrator\nmethods should atomically modify the system's state at each step.\nImplement the simple system and the Euler integrator. Try different values of h and see how the precision\nvaries.\nAs seen in the lecture slides, Euler's method is unstable. The exact solution is a circle with the equation\n\nrcos(t + k)\nX(t) =\nrsin(t + k)\nHowever, Euler's method causes the solution to spiral outward, no matter how small h is. After imple\nmenting Euler's method, you should see the single particle spiral outwardly in a 2D space, similar to the\nimage below.\n\nNext, implement the Trapezoidal Rule. Think carefully about how you are going to implement the Trape\nzoidal Rule. It requires that you evaluate the derivatives f(X, t) at a different time step at different points.\nSo, you should write a function that evaluates all derivatives given any state of the system. Remember that\nyour integrator functions should be separated and abstracted away from the system itself. You'll be using\nthese integrator functions for different systems later on, so it's important that they are modular.\nThe Trapezoidal Rule is still unstable, but it diverges at a much slower rate. You should be able to\ncompare your Euler and Trapezoidal implementations, seeing the particles diverge outwardly at different\nrates. Check that your Euler and Trapezoidal implementation work as expected, with the Euler spiraling\noutward and diverging, and the Trapezoidal doing the same, but at a slower rate. The command line of your\napplication should allow the user to choose the solver and stepsize. Each method implementation is worth\n10%.\nPhysical Simulation\nIn this section, you will implement a simple two particle pendulum and extend that to a multiple particle\nchain. This will require you to implement the different kinds of forces (gravity, viscous drag, and springs). We\nhave provided you with a fourth order Runge-Kutta (RK4) integrator, since the integrators you implemented\nare unstable.\n4.1\nForces\nThe core component of particle system simulations are forces. Suppose we are given a particle's position xi,\nvelocity x , and mass mi. We can then express forces such as gravity:\ni\nF(xi, xi\n0 , mi) = mig\nOr perhaps viscous drag (given a drag constant k):\nF(xi, xi, mi) = -kxi\nWe can also express forces that involve other particles as well. For instance, if we connected particles i and\nj with an with an undamped spring of rest length r and spring constant k, it would yield a force of:\nd\nF(xi, xi, mi) = -k(||d|| - r)\n, where d = xi - xj .\n||d||\n\nSumming over all forces yields the net force, and dividing the net force by the mass gives the acceleration\nx′′\ni .\nThe motion of all the particles can be described in terms of a second-order ordinary differential equation:\nx′′ = F(x, x′)\nIn this expression, x describes the positions of all the particles (x has 3n elements, where n is the number\nof particles). The function F sums over all forces and divides by the masses of the particles.\nThe typical way to solve this equation numerically is by transforming it into a first-order ordinary differential\nequation. We do this by introducing a variable v = x′. This yields the following:\n\nx′\nv\n=\nv′\n\nF(x, v)\n\nIn conclusion, we can define our state X as the position and velocity of all of the particles in our system:\nx\nX =\n\nv\n\nwhich then gives us:\nd\nv\nX = f(X, t) =\ndt\n\nF(x, v)\n\nGiven these system characteristics, you should be able to use your time integrators to approximate this\nsystem. In the next sections, you will implement a simple pendulum and a multiple particle chain to test\nyour implementation. Note that you should not need to modify your code for Euler or Trapezoidal. Your\nintegrator code should be modular and abstracted enough to be able to handle any arbitrary state from any\nsystem! However, you might want to think carefully about how you will store the state. One simple option\nis to store a big Vector3f arry of size 2n where positions are stored at even indices and velocities at odd\nindices. It might help to write helper functions that read the position or velocity of particle i.\n4.2\nSimple Pendulum 20%\nYou will now implement the gravity force, viscous drag force, and spring force. Test this first with a single\nparticle connected to a fixed point by a spring (basically, a pendulum) in pendulumSystem.cpp.\nYour\nimplementation of evalF should return f(X, t), requiring you to calculate the gravity, viscous drag, and the\nspring forces that now act on your particle system.\nWe recommend that you think carefully about the representation of springs, as you'll need to keep track of\nthe spring forces on each of the particles. You can store a list of springs that know the two particles they\nact on, the rest length and the stiffness. You can alternatively store, for each particle, what other particle it\nis connected to and what the stiffness and rest length are.\n(Optional) To help with debugging, make a function that allows you to see which springs are attached\nto a specific particle. Allow the user to specify a number i as a command line parameter that renders the\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nsprings that are connected to the particle with index i (this will be become more useful when we have many\nmore particles).\nYou should make sure that the motion of this particle appears to be correct. Note that, especially with\nthe Euler method, you will need to provide a reasonable amount of drag, or the system will explode. The\nTrapezoidal Rule method should be more stable, but you will still want a little viscous drag to keep the\nmotion in check. If you have everything correct so far, you will already receive 40% of the available points.\n4.3\nMultiple Particle Chain 20%\nThe next step is to extend your test to multiple particles. Try connecting four particles with springs to form\na chain, and fix one of the endpoints (you can fix a particle by zeroing the net force applied to it). Make sure\nthat you can simulate the motion of this chain. As a general rule, more particles and springs will lead to\nmore instability, so you will have to choose your parameters (spring constants, drag coefficients, step sizes)\ncarefully to avoid explosions. If you reach this point and everything is correct, you'll get 60% of the possible\npoints.\nParticle System Cloth 40%\nThe previous section describes how to simulate a collection of particles that are affected by gravity, drag,\nand springs. In this section, we describe how these forces can be combined to yield a reasonable (but not\nnecessarily accurate) model of cloth.\nBefore moving on, we also recommend taking a snapshot of your code, just in case the full cloth implemen\ntation does not work out. We recommend using Git for version control (available on Athena and Github).\nAlthough there is a bit of a learning curve to using a version control system, having a safety net is more\nthan worth it.\nYou should extend ParticleSystem or pendulumSystem and make your own ClothSystem class for the\ncloth simulation.\nFigure 1: Left to right: structural springs, shear springs, and flex springs\nWe begin with a uniform grid of particles and connect them to their vertical and horizonal neighbors with\nsprings. These springs keep the particle mesh together and are known as structural springs. Then, we add\nadditional shear springs to prevent the cloth from collapsing diagonally. Finally, we add flexion springs to\nprevent the cloth from folding over onto itself. Notice that the flex springs are only drawn as curves to make\nit clear that they skip a particle--they are still \"straight\" springs with the same force equation given earlier.\n\nIf you have designed your code reasonably well, it shouldn't be too tough to add the necessary springs. Make\nsure that you use reasonable rest lengths, and start small. Write your code very carefully here; it is easy to\nmake mistakes and connect springs to particles that don't exist. We recommend that you create a helper\nmethod indexOf that given index i, j into a n × n cloth, returns the linear index into our vector of particles.\nFirst, implement structural springs. Draw the springs to make sure you've added the right ones in. Make\nsure it looks as you expect before moving on. Run the simulation and you should obtain something that\nlooks like a net. As usual, viscous drag helps prevent explosions. For faster debugging, use small meshes of\ne.g. 3 × 3 particles.\nOnce you've made sure your structural springs are correct, add in the shear springs. Again, test incrementally\nto avoid mistakes. Finally, add in flex springs.\nTo display your cloth, the simplest approach is to draw a grid or the structural springs (which you should\nhave already done to debug your structural spring implementation!). For extra credit, you can draw it as a\nsmooth surface like the sample solution, but this is not required. If you do choose to draw the cloth as a\nsmooth surface, you'll need to figure out the normal for the cloth at each point.\nDon't be too discouraged if your first test looks terrible, or blows up because of instability. At this point,\nyour Euler solver will be useless for all but the smallest step sizes, and you should be using the Trapezoidal\nsolver almost exclusively.\nIf you manage to have a moving wireframe cloth, then you're at 90%. All that's left is to add the necessary\nuser interface elements, such rendering the cloth, moving it around, and so on. And that's 100%.\nYou may also find these notes on physically based modeling David Baraff helpful, particularly particle system\ndynamics.\n\nExtra Credit\nThe list of extra credits below is a short list of possibilities. In general, visual simulation techniques draw\nfrom numerous engineering disciplines and benefit from a wide variety of techniques in numerical analysis.\nPlease feel free to experiment with ideas that are not listed below. Also make sure that your code for previous\nsections still works after implementing extra credit.\n6.1\nEasy\n- Add a random wind force to your cloth simulation that emulates a gentle breeze. You should be able\nto toggle it on and off using a key.\n- Rather than display the cloth as a wireframe mesh, implement smooth shading. The most challenging\npart of this is defining surface normals at each vertex, which you should approximate using the positions\nof adjacent particles.\n- Implement a different object using the same techniques. For example, by extending the particle mesh\nto a three-dimensional grid, you might create wobbly gelatin.\n- Provide a mouse-based interface for users to interact with the cloth. You may, for instance, allow the\nuser to click on certain parts of the cloth and drag parts around.\n- Implement frictionless collisions of cloth with a simple primitive such as a sphere. This is simpler than\nit may sound at first: just check whether a particle is \"inside\" the sphere; if so, just project the point\nback to the surface.\n- Implement 4th order Runge-Kutta time stepper. This is the same one we provided you with.\n6.2\nMedium\n- Implement an adaptive solver scheme (look up adaptive Runge-Kutta-Felhberg techniques or check out\nthe MATLAB ode45 function).\n- Implement an implicit integration scheme as described in this paper. Such techniques allow much\ngreater stability for stiff systems of differential equations, such as the ones that arise from cloth sim\nulation. An implicit Euler integration technique, for instance, is just as inaccurate as the explicit one\nthat you will implement. However, the inaccuracy tends to bias the solution towards stable solutions,\nthus allowing far greater step sizes. The sample solution demonstrates such a technique, which can be\nactivated with i on the command line.\n- Extend your particle system to support constraints, as described in this document. This extra credit\nis actually an assignment in 6.839.\n6.3\nHard\n- Implement a more robust model of cloth, as described in this paper.\n- Simulate rigid-body dynamics, deformable models, or fluids. In theory, particle systems can be used to\nachieve similar effects. However, greater accuracy and efficiency can be achieved through more complex\nphysical and mathematical models.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nSubmission Instructions\nYou are to write a README.txt that answers the following questions:\n- How do you compile and run your code? Provide instructions for Athena Linux. If your executable\nrequires certain parameters to work well, make sure that these are specified.\n- Did you collaborate with anyone in the class? If so, let us know who you talked to and what sort of\nhelp you gave or received.\n- Were there any references (books, papers, websites, etc.) that you found particularly helpful for\ncompleting your assignment? Please provide a list.\n- Are there any known problems with your code? If so, please provide a list and, if possible, describe\nwhat you think the cause is and how you might fix them if you had more time or motivation. This\nis very important, as we're much more likely to assign partial credit if you help us understand what's\ngoing on.\n- Did you do any of the extra credit? If so, let us know how to use the additional features. If there was\na substantial amount of work involved, describe what how you did it.\n- Got any comments about this assignment that you'd like to share?\nAs with the previous assignment, you should create a single archive (.tar.gz or .zip) containing:\n- All source code necessary to compile your assignment.\n- A compiled executable named a3.\n- The README.txt file.\nSubmit it online.\nThis assignment does not require the submission of an artifact.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Ray casting",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/754e6d77b8eed00ae57f2ef2371c7d00_MIT6_837F12_assn4.pdf",
      "content": "6.837: Computer Graphics Fall 2012\nProgramming Assignment 4: Ray Casting\nIn this assignment, you will implement a ray caster. This will be the basis of your final assignment, so proper\ncode design is quite important. As seen in class, a ray caster sends a ray for each pixel and intersects it with\nall the objects in the scene. Your ray caster will support perspective cameras as well as several primitives\n(spheres, planes, and triangles). You will also have to support phong shading and texture mapping.\nThe remainder of this document is organized as follows:\n1. Getting Started\n2. Summary of Requirements\n3. Starter Code\n4. Implementation Notes\n5. Test Cases\n6. Hints\n7. Extra Credit\n8. Submission Instructions\nGetting Started\nNote that this assignment is non-trivial. Please start as early as possible. One significant way in\nwhich this assignment differs from previous assignments is that you start off with a lot less starter code.\nRun the sample solution a4soln as follows:\n./a4soln -input scene01_plane.txt -size 200 200 -output output01.bmp -depth 8 12 depth01.bmp\nThis will generate an image named output01.bmp. We'll describe the rest of the command-line parameters\nlater. When your program is complete, you will be able to render this scene as well as well as the other test\ncases given below.\n\nSummary of Requirements\nThis section summarizes the core requirements of this assignment. There are a lot of them and you should\nstart early. Let's walk through them.\nYou will use object-oriented design to make your ray caster flexible and extendable. A generic Object3D\nclass will serve as the parent class for all 3D primitives. You will derive subclasses (such as Sphere, Plane,\nTriangle, Group, Transform, Mesh) to implement specialized primitives. Similarly, this assignment requires\nthe implementation of a general Camera class with perspective camera subclasses.\nYou will implement Phong shading with texture mapping. We will focus on diffuse and specular shading.\nDiffuse shading is our first step toward modeling the interaction of light and materials. Specular shading\nwill be explained later when you get the basic components working. Given the direction to the light L and\nthe normal N we can compute the diffuse shading as a clamped dot product:\n\nL · N if L · N > 0\nd =\notherwise\nIf the object has diffuse color kd = (r, g, b) (in case the object has texture, just use the texture color\ninstead), and the light source has color clight = (Lr, Lg, Lb), then the pixel color is cpixel = (rLrd, gLgd, bLbd).\nMultiple light sources are handled by simply summing their contributions. We can also include an ambient\nlight with color cambient, which can be very helpful for debugging. Without it, parts facing away from the\nlight source appear completely black. Putting this all together, the formula is:\n\ncpixel = cambient ∗ ka +\nclamp(Li · N) ∗ clight ∗ kd\ni\nColor vectors are multiplied term by term. Note that if the ambient light color is (1, 1, 1) and the light\nsource color is (0, 0, 0), then you have constant shading.\nYou may optionally implement two visualization modes. One mode will display the distance t of each\npixel to the camera. The other mode is a visualization of the surface normal. For the normal visualization,\nyou will simply display the absolute value of the coordinates of the normal vector as an (r, g, b) color. For\nexample, a normal pointing in the positive or negative z direction will be displayed as pure blue (0, 0, 1).\nYou should use black as the color for the background (undefined normal).\nYour code will be tested using a script on all the test cases below. Make sure that your program handles\nthe exact same arguments as the examples below.\n\nStarter Code\n(As always, you can add files or modify any files. You can even start from scratch.)\nCompile the code with make. You can type make clean; make to rebuild everything from scratch.\nThe Image class is used to initialize and edit the RGB values of images. Be careful--do not try to read\nor write to pixels outside the bounds of the image. The class also includes functions for saving simple .bmp\nimage files (and .tga files).\nFor linear algebra, you should use the vecmath library that you are familiar with from previous assign\nments.\nWe provide you with a Ray class and a Hit class to manipulate camera rays and their intersection points,\nand a skeleton Material class. A Ray is represented by its origin and direction vectors. The Hit class stores\ninformation about the closest intersection point, normal, texture coordinates, the value of the ray parameter\nt and a pointer to the Material of the object at the intersection. The Hit data structure must be initialized\nwith a very large t value (try FLT MAX). It is modified by the intersection computation to store the new\nclosest t and the Material of intersected object.\nYour program should take a number of command line arguments to specify the input file, output image\nsize and output file. Make sure the examples below work, as this is how we will test your program. A simple\nscene file parser for this assignment is provided. Several constructors and the Group::addObject method\nyou will write are called from the parser (and will be a source for many compilation errors initially). Look\nin the scene parser.cpp file for details.\nIf you're interested, the scene description file grammar used in this assignment is included in the source\nfile distribution.\n\nImplementation Notes\nThis is a very large assignment. We can't repeat this enough. Here's a suggested recipe to follow to get as\nfar as possible, as quickly as possible.\n1. Look at the virtual Object3D class (a virtual class in C++ is like an abstract class in Java). It only\nprovides the specification for 3D primitives, and in particular the ability to be intersected with a ray\nvia the virtual method: virtual bool intersect( const Ray& r, Hit& h, float tmin ) = 0;\nSince this method is pure virtual for the Object3D class, the prototype in the header file includes '= 0'.\nThis '= 0' tells the compiler that Object3D won't implement the method, but that subclasses derived\nfrom Object3D must implement this routine. An Object3D stores a pointer to its Material type. The\nObject3D class has:\n- a default constructor and destructor\n- a pointer to a Material instance\n- a pure virtual intersection method\n2. Fill in Sphere, a subclass of Object3D, that additionally stores a center point and a radius. The\nSphere constructor will be given a center, a radius, and a pointer to a Material instance. The Sphere\nclass implements the virtual intersect method mentioned above (but without the '= 0'): virtual\nbool intersect(const Ray& r, Hit& h, float tmin);\nWith the intersect routine, we are looking for the closest intersection along a Ray, parameterized by\nt. tmin is used to restrict the range of intersection. If an intersection is found such that t > tmin and\nt is less than the value of the intersection currently stored in the Hit data structure, Hit is updated\nas necessary. Note that if the new intersection is closer than the previous one, both t and Material\nmust be modified. It is important that your intersection routine verifies that t >= tmin. tmin is not\nmodified by the intersection routine.\n3. Fill in Group, also a subclass of Object3D, that stores a list of pointers to Object3D instances. For\nexample, it will be used to store all objects in the entire scene. You'll need to write the intersect\nmethod of Group which loops through all these instances, calling their intersection methods. The\nGroup constructor should take as input the number of objects under the group. The group should\ninclude a method to add objects: void addObject(int index, Object3D* obj);\n4. Look at the pure virtual Camera class (in Java parlance, an interface). The Camera class has two pure\nvirtual methods:\nvirtual Ray generateRay( const Vector2f& point ) = 0;\nvirtual float getTMin() const = 0;\nThe first is used to generate rays for each screen-space coordinate, described as a Vector2f. The\ngetTMin() method will be useful when tracing rays through the scene. For a perspective camera, the\nvalue of tmin will be zero to correctly clip objects behind the viewpoint (already provided).\n5. Fill in PerspectiveCamera class that inherits Camera. Choose your favorite internal camera repre\nsentation. The scene parser provides you with the center, direction, and up vectors. The field of\nview is specified with an angle (as shown in the diagram). PerspectiveCamera( const Vector3f&\ncenter, const Vector3f& direction, const Vector3f& up, float angle ); Here up and direc\ntion are not necessarily perpendicular. The u, v, w vectors are computed using cross products. w =\ndirection, u = w×up, v = u×w. The camera does not know about screen resolution. Image resolution\nshould be handled in your main loop.\n\nHint: In class, we often talk about a \"virtual screen\" in space. You can calculate the location and\nextents of this \"virtual screen\" using some simple trigonometry. You can then sample points on the\nvirtual screen. Direction vectors can then be calculated by subtracting the camera center point from\nthe screen point. Don't forget to normalize! In contrast, if you iterate over the camera angle to obtain\nyour direction vectors, your scene will look distorted - especially for large camera angles, which will\ngive the appearance of a fisheye lens. Note: the distance to the image plane and the size of the image\nplane are unnecessary. Why?\n6. SceneParse is completely implemented for you. Use it to load the camera, background color and\nobjects of the scene from scene files.\n7. Write the main function that reads the scene (using the parsing code provided), loops over the pixels\nin the image plane, generates a ray using your camera class, intersects it with the high-level Group\nthat stores the objects of the scene, and writes the color of the closest intersected object. Up to this\npoint, you may choose to render some spheres with a single color. If there is an intersection, use one\ncolor and if not, use another color.\n8. (Optional, but good for debugging) Implement an alternative rendering style to visualize the depth\nt of objects in the scene. Two input depth values specify the range of depth values which should be\nmapped to shades of gray in the visualization. Depth values outside this range should be clamped.\n9. Update your sphere intersection routine to pass the correct normal to the Hit.\n10. (Optional) Implement normal visualization. Add code to parse an additional command line option\n-normals <normal file.bmp> to specify the output file for this visualization.\n11. Implement diffuse shading in Material class, ignoring textures for now. We provide the pure virtual\nLight class and two subclasses: directional light and point light. Scene lighting can be accessed with\nthe SceneParser::getLight() and\nSceneParser::getAmbientLight() methods. Use the Light method:\nvoid getIllumination( const Vector3f& p, Vector3f& dir, Vector3f& col );\n\nto find the illumination at a particular location in space. p is the intersection point that you want to\nshade, and the function returns the normalized direction toward the light source in dir and the light\ncolor and intensity in col.\n12. Implement Plane, an infinite plane primitive derived from Object3D. Use the representation of your\nchoice, but the constructor is assumed to be:\nPlane( const Vector3f& normal, float d, Material* m );\nd is the offset from the origin, meaning that the plane equation is P · n = d. You can also implement\nother constructors (e.g., using 3 points). Implement intersect, and remember that you also need to\nupdate the normal stored by Hit, in addition to the intersection distance t and color.\n13. Fill in triangle primitive which also derives from Object3D. The constructor takes 3 vertices:\nTriangle( const Vector3f& a, const Vector3f& b, const Vector3f& c, Material* m );\nUse the method of your choice to implement the ray-triangle intersection, preferably using barycentric\ncoordinates. Suppose we have barycentric coordinates λ0, λ1, λ2 and vertex normals n0, n1, n2, the\ninterpolated normal can be computed as\nλ0n0 + λ1n1 + λ2n2.\nTexture coordinates can be interpolated in the same way.\n14. Fill in subclass Transform from Object3D. Similar to a Group, a Transform will store a pointer to an\nObject3D (but only one, not an array). The constructor of a Transform takes a 4 × 4 matrix as input\nand a pointer to the Object3D modified by the transformation: Transform( const Matrix4f& m,\nObject3D* o ); The intersect routine will first transform the ray, then delegate to the intersect\nroutine of the contained object. Make sure to correctly transform the resulting normal according to\nthe rule seen in lecture. You may choose to normalize the direction of the transformed ray or leave it\nun-normalized. If you decide not to normalize the direction, you might need to update some of your\nintersection code. Instancing.\n15. Implement specular component in the Phong shading model.\nThis is as simple as adding another\nformula.\nThe intensity cs depends on four quantities: shininess s, ray direction d, direction to the light L and\nthe normal N. We can first compute the direction R of the reflected ray using d and N. The specular\nshading intensity is another clamped dot\n(\nproduct:\n(L · R)s\nif L\ncs =\n· R > 0\notherwise\nIf the object has specular color ks = (sr, sg, sb) , and the light source has color clight = (Lr, Lg, Lb),\nthen the pixel color is cpixel = (srLrcs, sgLgcs, sbLbcs)\nCombining this with diffuse and ambient, the formula is:\ncpixel = cambient ∗ka +\nX\nclamp(Li · N) ∗clight ∗kd + clamp(Li\ni\n· R)s ∗clight ∗ks .\n16. Texture mapping. This is the last item. The scene parser will load texture and coordinates\n\nfor you.\nWe also provide a Texture class that facilitates texture look-up. All that remains are interpolating\ntexture coordinate in Triangle's intersect function, and looking up texture coordinates in Material's\nShade function. If the material has valid texture indicated by t.valid(), then simply use the texture\ncolor instead of kd. The texture color can be retrieved by\nVector3f color = t(u,v);\nWhere u, v is the texture coordinate.\n\nTest Cases\nYour assignment will be graded by running a script that runs these examples below. Make sure your ray\ncaster produces the same output (up to visual perception).\n./a4 -input scene01 plane.txt -size 200 200 -output 1.bmp\n./a4 -input scene02 cube.txt -size 200 200 -output 2.bmp\n./a4 -input scene03 sphere.txt -size 200 200 -output 3.bmp\n./a4 -input scene04 axes.txt -size 200 200 -output 4.bmp\n./a4 -input scene05 bunny 200.txt -size 200 200 -output 5.bmp\n./a4 -input scene06 bunny 1k.txt -size 200 200 -output 6.bmp\n\n./a4 -input scene07 shine.txt -size 200 200 -output 7.bmp\n./a4 -input scene08 c.txt -size 200 200 -output 8.bmp\n./a4 -input scene09 s.txt -size 200 200 -output 9.bmp\nHints\n- Incremental debugging. Implement and test one primitive at a time. Test one shading a time. Ambient,\ndiffuse, specular, and then texture.\n- Use a small image size for faster debugging. 64 × 64 pixels is usually enough to realize that something\nmight be wrong.\n- As usual, don't hesitate to print as much information as needed for debugging, such as the direction\nvector of the rays, the hit values, etc.\n- Use assert() to check function preconditions, array indices, etc. See cassert.\n- The \"very large\" negative and positive values for t used in the Hit class and the intersect routine can\nsimply be initialized with large values relative to the camera position and scene dimensions. However,\nto be more correct, you can use the positive and negative values for infinity from the IEEE floating\npoint standard.\n- Parse the arguments of the program in a separate function. It will make your code easier to read.\n- Implement the normal visualization and diffuse shading before the transformations.\n- Use the various rendering modes (normal, diffuse, distance) to debug your code. This helps you locate\nwhich part of your code is buggy.\nExtra Credit\nNote that there isn't much extra credit for this assignment. That's because we want you to focus on a good\ndesign so that your code will survive not only this assignment but the next one as well.\n\n7.1\nEasy\n- Add simple fog to your ray tracer by attenuating rays according to their length. Allow the color of the\nfog to be specified by the user in the scene file.\n- Add other types of simple primitives to your ray tracer, and extend the file format and parser accord\ningly. For instance, how about a cylinder or cone? These can make your scenes much more interesting.\n- Add a new oblique camera type (or some other weird camera). In a standard camera, the projection\nwindow is centered on the z-axis of the camera. By sliding this projection window around, you can get\nsome cool effects.\n7.2\nMedium\n- Implement a torus or higher order implicit surfaces by solving for t with a numerical root finder.\n- Implement texture mapping for spheres. Render a bunch of planets in some space scene for example.\n- Implement environment mapping for objects (sphere maps or cube maps).\n- Load more interesing textured models and put them into new scenes. Note that the starter code only\nloads .bmp image files. The Mesh utility only loads .obj files with a single texture map.\n7.3\nHard\n- Add normal mapping (aka bump mapping).\n- Bloom: render multiple passes and do some blurring.\n- Depth of field blurring. The camera can focus on some distance and objects out of focus are blurred\ndepending on how far it is from the focal plane. It doesn't have to be optically correct, but it needs to\nbe visually pleasing.\nSubmission Instructions\nYou are to write a README.txt (or optionally a PDF) that answers the following questions:\n- How do you compile your code? Provide instructions for Athena Linux. You will not need to provide\ninstructions on how to run your code, because it must run with the exact command line given earlier\nin this document.\n- Did you collaborate with anyone in the class? If so, let us know who you talked to and what sort of\nhelp you gave or received.\n- Were there any references (books, papers, websites, etc.) that you found particularly helpful for\ncompleting your assignment? Please provide a list.\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\n- Are there any known problems with your code? If so, please provide a list and, if possible, describe\nwhat you think the cause is and how you might fix them if you had more time or motivation. This\nis very important, as we're much more likely to assign partial credit if you help us understand what's\ngoing on.\n- Did you do any of the extra credit? If so, let us know how to use the additional features. If there was\na substantial amount of work involved, describe what how you did it.\n- Got any comments about this assignment that you'd like to share?\nSubmit your assignment online. Please submit a single archive (.zip\nor .tar.gz) containing:\n- Your source code.\n- A compiled executable named a4.\n- Any additional files that are necessary.\n- The README file.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Ray tracing",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/c8f44300de5d661810d321d3ae30e916_MIT6_837F12_assn5.pdf",
      "content": "6.837: Computer Graphics Fall 2012\nProgramming Assignment 5: Ray Tracing\nIn this assignment, you will greatly improve the rendering capabilities of your ray caster by adding several\nnew features. First, you will improve the shading model by recursively generating rays to create reflections\nand shadows. Second, you'll add procedural solid texturing. Finally, Implement supersampling to fix aliasing\nproblems.\nThe remainder of this document is organized as follows:\n1. Getting Started\n2. Summary of Requirements\n3. Starter Code\n4. Implementation Notes\n5. Test Cases\n6. Hints\n7. Extra Credit\n8. Submission Instructions\nGetting Started\nThis assignment used to be several one week assignments. Please start as early as possible.\nAn updated parser, sample solution, and new scene files for this project are included in the assignment\ndistribution. Run the sample solution a5soln as follows:\n./a5soln -input scene10 vase.txt -size 300 300 -output 10.bmp -shadows\nThis will generate an image named 10.bmp.\nAlso try this:\n./a5soln -input scene10 vase.txt -size 300 300\n-output 10.bmp -shadows -jitter -filter\nWe'll describe the rest of the command-line parameters later. When your program is complete, you will\nbe able to render this scene as well as well as the other test cases given below.\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\nSummary of Requirements\nAs mentioned above you'll be adding several new features to your ray caster to improve its realism and\nperformance.\nNew features\n- Recursive ray tracing for shadowing and reflections.\n- Procedural solid textures.\n- Supersampling for antialiasing.\nWe go into more detail about these features below in the implementation notes.\nYour code will be tested using a script on all the test cases below. Make sure that your program handles\nthe exact same arguments as the examples below. For your convenience, you can assume that \"-jitter\",\n\"-shadows\" and \"-filter\" are always given. That is you can ignore these arguments and always run your\nray-tracer with these features enabled.\nStarter Code\nFor this assignment, part of the starter code is your own from Assignment 4. We updated some files and\nyou need to merge your code with these updated files. Before start merging, make sure you have a back up\ncopy of your code in case you damage your own files by accident during the merge. Now copy all files from\nthe new starter code to your own code and overwrite everything. Of course, you are always free to change\nwhat we give you, as long as it compiles on Athena.\nThere's a new scene parser to handle new material types. If you had to modify SceneParser.cpp/hpp\nfor assignment four, remember to make the same modifications to the new scene parser we provide for you.\nAlso, there is updated Mesh.cpp/hpp to accelerate intersection test speed. To help build procedural shaders,\nwe provide you with some procedural noise routines. Finally, we give you a new Material class to handle\nnew materials. Please replace your own Material with the new one.\nSceneParser::getBackgroundColor(Vector3f dir) now takes the ray direction as an argument. Please\nupdate your main loop accordingly.\nImplementation Notes\n4.1\nRecursive Rays\nAfter merging, you will add some global illumination effects to your ray caster. Because you cast secondary\nrays to account for shadows, reflection and refraction , you can now call it a ray tracer. You will encapsulate\nthe high-level computation in a RayTracer class that will be responsible for sending rays and recursively\ncomputing colors along them.\n\nTo compute cast shadows, you will send rays from the visible point to each light source. If an intersection\nis reported, the visible point is in shadow and the contribution from that light source is ignored. Note that\nshadow rays must be sent to all light sources. To add reflection (and refraction) effects, you need to send\nsecondary rays in the mirror (and transmitted) directions, as explained in lecture. Refer to the document\nhttp://www.cs.utah.edu/~shirley/books/fcg2/rt.pdf for more details. The computation is recursive\nto account for multiple reflections (and or refractions).\n1. Make sure your Material classes store refraction index, which are necessary for recursive ray tracing.\n2. Create a new class RayTracer that computes the radiance (color) along a ray. Update your main\nfunction to use this class for the rays through each pixel. This class encapsulates the computation of\nradiance (color) along rays. It stores a pointer to an instance of SceneParser for access to the geometry\nand light sources. Your constructor should have these arguments (and maybe others, depending on\nhow you handle command line arguments):\nRayTracer( SceneParser* s, int max bounces, ... );\nwhere max bounces is the maximum number of bounces (recursion depth) a light ray has.\nThe main method of this class is traceRay that, given a ray, computes the color seen from the origin\nalong the direction. This computation is recursive for reflected (or transparent) materials. We therefore\nneed a max bounces to prevent infinite recursion. The maximum recursion depth which is passed as a\ncommand line arguments to the program with\n-bounces max bounces.\nTry that with the solution. traceRay takes the current number of bounces (recursion depth) as one of\nits parameters.\nVector3f traceRay( Ray& ray, float tmin, int bounces, Hit& hit ) const;\n3. Now is a good time to make sure you did not break any prior functionality by testing against scenes\nfrom the last assignment as well as the new scenes. You can do this by setting max bounces=0. Check\nthat you get the same results except that the specular component is removed.\n4. (Optional) Add support for the new command line arguments: -shadows, which indicates that shadow\nrays are to be cast, and -bounces, which control the depth of recursion in your ray tracer.\n5. Implement cast shadows by sending rays toward each light source to test whether the line segment\njoining the intersection point and the light source intersects an object. If there is an intersection, then\ndiscard the contribution of that light source. Recall that you must displace the ray origin slightly away\nfrom the surface, or equivalently set tmin to some E.\n6. Implement mirror reflections for reflective materials by sending a ray from the current intersection\npoint in the mirror direction. For this, we suggest you write a function:\nVector3f mirrorDirection( const Vector3f& normal, const Vector3f& incoming );\nTrace the secondary ray with a recursive call to traceRay using modified recursion depth. Make sure\nthat traceRay checks the appropriate stopping conditions. Add the color seen by the reflected ray\ntimes the reflection color to the color computed for the current ray. If a ray didn't hit anything, simply\nreturn the background by SceneParser::getBackgroundColor(dir).\n7. Simple refraction. Cast refraction rays for transparent materials (if Material::refractionIndex>0).\nAir has refraction Index 1. You can assume you always start in air. The starter code keeps track\nof current refraction index as an additional argument to traceRay. This will not work properly for\ntransparent objects that are inside other transparent objects. Choose your own implementation if you\nwould like. We suggest you write a function:\n\nbool transmittedDirection( const Vector3f& normal, const Vector3f& incoming, float index n,\nfloat index nt,Vector3f & transmitted);\nindex n and nt are refraction indices of the current object and the object the ray is going into. Refer\nto http://www.cs.utah.edu/~shirley/books/fcg2/rt.pdf for a complete explanation.\nLet d be the direction of the incoming light ray, N be the normal. Note that if the ray is currently\ninside the object, d · N will be positive.\nLet n and nt be the refraction indices of the two mediums. The refracted direction t is\n\nn(d - N(d · N))\nn2(1 - (d · N)2)\nt =\n- N\n1 -\n.\nnt\nnt\nFirst, you need to decide whether there is refraction or not and return false if there is no refraction\n(total reflection). This is done by checking that the quantity under the square root is positive.\nThen, if there is refraction, you should use the same traceRay function to get a color of the recursive\nray.\nFinally, you need to blend the reflection color and refraction color using Schlick's approximation to\nFresnel's equation. We are going to compute a weight R for the reflection color and use 1 - R for\nrefraction color.\nR is given by\nnt - n\nR = R0 + (1 - R0)(1 - c)5, R0 = (\n)2 .\nnt + n\n\nabs(d · N),\nn ≤ nt\nc =\n.\nabs(t · N),\nn > nt\n4.2\nPerlin Noise\nNext you'll build materials using Perlin Noise, which lets you to add controllable irregularities to your\nprocedural textures. Here's what Ken Perlin says about his invention (http://www.noisemachine.com/\ntalk1/):\nNoise appears random, but isn't really. If it were really random, then you'd get a different result every\ntime you call it. Instead, it's \"pseudo-random\"--it gives the appearance of randomness. Its appearance\nis similar to what you'd get if you took a big block of random values and blurred it (ie: convolved with a\nGaussian kernel). Although that would be quite expensive to compute.\nSo we'll use his more efficient (and recently improved) implemenation of noise (http://mrl.nyu.edu/\n~perlin/noise/) translated to C++ in PerlinNoise.{h,cpp}.\nPerlinNoise::octaveNoise is implemented for you. It computes\nN(x, y, z) = noise(x, y, z) + noise(2x, 2y, 2z)/2 + noise(4x, 4y, 4z)/4 + . . . ,\nwhere (x, y, z) is simply the global coordinate.\nKen Perlin's original paper and his online notes have many cool examples of procedural textures that can\nbe built from the noise function. You will implement a simple Marble material. This material uses the sin\nfunction to get bands of color that represent the veins of marble. These bands are perturbed by the noise\nfunction as follows:\n\nM(x, y, z) = sin(ωx + aN(x, y, z))\nFill in the class Noise which calls the function PerlinNoise::octaveNoise with proper arguments to\nobtain N(x, y, z). Then compute M(x, y, z). The value of M(x, y, z) will be a floating point number that\nyou should clamp and use to interpolate between the two contained colors. Here's the constructor for Noise:\nNoise( int octaves , const Vector3f & color1, const Vector3f & color2, float frequency,\nfloat amplitude);\nwhere frequency is w and amplitude is a in the equation above.\nTry different parameter settings to understand the variety of appearances you can get. Remember that\nthis is not a physical simulation of marble, but a procedural texture attempting to imitate our observations.\n4.3\nAnti-aliasing\nNext, you will add some simple anti-aliasing to your ray tracer. You will use supersampling and filtering to\nalleviate jaggies and Moire patterns.\n1. Jittered Sampling.\nFor each pixel, instead of getting a color with one ray, we can sample multiple rays perturbed randomly.\nYou are required to subdivide a pixel into 3 × 3 sub-grids. This is equivalent to rendering an image\nwith 3x resolution. Then, for each pixel at (i, j) in the high resolution image, instead of generating a\nray just using (i, j), generate two random numbers ri, rj in range [-0.5, 0.5] to get (i + ri, j + rj ).\n2. Gaussian blur. http://en.wikipedia.org/wiki/Gaussian_blur In the solution, we use the kernel\nK = (0.1201, 0.2339, 0.2931, 0.2339, 0.1201). First blur the image horizontally and then blur the blurred\nimage vertically.\nTo blur an image horizontally Each pixel color I'(i, j) in the new image is computed as a weighed sum\nof pixels in the original image.\nI'(i, j) = I(i, j - 2)K(0) + I(i, j - 1)K(1) + I(i, j)K(2) + I(i, j + 1)K(3) + I'(i, j + 2)K(4).\nNote that you probably want to allocate a new image to do the blurring to avoid bugs. If (i, j - 2) is\nout of image boundary, for example, simply use (i, 0) and so on.\n3. Down-sampling. To get from the high-resolution image back to the original specified resolution, average\neach 3 × 3 neighborhood of pixels to get back 1 pixel.\n4. (Optional) Handle commandline arguments:\n- -jittered Enable jittered sampling.\n- -filter Enable Gaussian smoothing and down-sampling.\nIf you choose to ignore these arguments, your program should always enable these two steps.\n\nTest Cases\nYour assignment will be graded by running a script that runs these new examples below. Make sure your\nray tracer produces similar output.\n./a5 -input scene06 bunny 1k.txt -size 300 300 -output 6.bmp\n-shadows -bounces 4 -jitter -filter\n./a5 -input scene10 sphere.txt -size 300 300 -output 10.bmp\n-shadows -bounces 4 -jitter -filter\n./a5 -input scene11 cube.txt -size 300 300 -output 11.bmp\n-shadows -bounces 4 -jitter -filter\n./a5 -input scene12 vase.txt -size 300 300 -output 12.bmp\n-shadows -bounces 4 -jitter -filter\n\nHints\n- You do not need to declare all methods in a class virtual, only the ones which subclasses will override.\n- Print as much information as you need for debugging. When you get weird results, don't hesitate to\nuse simple cases, and do the calculations manually to verify your results. Perhaps instead of casting\nall the rays needed to create an image, just cast a single ray (and its subsequent recursive rays).\n- Modify the test scenes to reduce complexity for debugging: remove objects, remove light sources, change\nthe parameters of the materials so that you can view the contributions of the different components,\netc.\n- To avoid a segmentation fault, make sure you don't try to access samples in pixels beyond the image\nwidth and height. Pixels on the boundary will have a cropped support area.\nExtra Credit\nMost of these extensions require that you modify the parser to take into account the extra specification\nrequired by your technique. Make sure that you create (and turn in) appropriate input scenes to show off\nyour extension.\n7.1\nEasy\n- Create a wood material or other procedural material that uses Perlin Noise.\n- Add more interesting lights to your scenes, e.g. a spotlight with angular falloff.\n- Render with depth of field (if you didn't do this last time).\n7.2\nMedium\n- Bump mapping (If you didn't do last time): look up the normals for your surface in a height field\nimage or an normal map. This needs the derivation of a tangent frame. There many such free images\nand models online.\n- Bidirectional Texture Functions (BTFs): make your texture lookups depend on the viewing angle.\nThere are datasets available for this online.\n- Load or create more interesting complex scenes. You can download more models and scenes that are\nfreely available online.\n- Add area light sources and Monte-Carlo integration of soft shadows.\n- Render glossy surfaces.\n- Render interesting BRDF such as milled surface.\n- Distribution ray tracing of indirect lighting (very slow). Cast tons of random secondary rays to sample\nthe hemisphere around the visible point. It is advised to stop after one bounce. Sample uniform or\naccording to the cosine term (careful, it's not trivial to sample the hemisphere uniformly).\n\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n- Uniform Grids. Create a 3D grid and \"rasterize\" your object into it. Then, you march each ray through\nthe grid stopping only when you hit an occupied voxel. Difficult to debug.\n- Simulate dispersion (and rainbows). The rainbow is difficult, as is the Newton prism demo.\n- Make a little animation (10 sec at 24fps will suffice). E.g, if you implemented depth of field, show what\nhappens when you change camera focal distance. Move lights and objects around.\n- Add motion blur to moving objects in your animation.\n7.3\nHard\n- Photon mapping with kd-tree acceleration to render caustics.\n- Irradiance caching.\n- Subsurface scattering. Render some milk, jade etc.\n- Path tracing with importance sampling, path termination with Russian Roulette, etc.\n- Raytracing through a volume. Given a regular grid encoding the density of a participating medium\nsuch as fog, step through the grid to simulate attenuation due to fog. Send rays towards the light\nsource and take into account shadowing by other objects as well as attenuation due to the medium.\nThis will give you nice shafts of light.\n- Animate some water pouring into a tank or smoke rising.\nSubmission Instructions\nYou are to write a README.txt (or optionally a PDF) that answers the following questions:\n- How do you compile your code? Provide instructions for Athena Linux. You will not need to provide\ninstructions on how to run your code, because it must run with the exact command line given earlier\nin this document.\n- Did you collaborate with anyone in the class? If so, let us know who you talked to and what sort of\nhelp you gave or received.\n- Were there any references (books, papers, websites, etc.) that you found particularly helpful for\ncompleting your assignment? Please provide a list.\n- Are there any known problems with your code? If so, please provide a list and, if possible, describe\nwhat you think the cause is and how you might fix them if you had more time or motivation. This\nis very important, as we're much more likely to assign partial credit if you help us understand what's\ngoing on.\n- Did you do any of the extra credit? If so, let us know how to use the additional features. If there was\na substantial amount of work involved, describe how you did it.\n- Got any comments about this assignment that you'd like to share?\n\nSubmit your assignment online. Please submit a single archive (.zip\nor\n.tar.gz) containing:\n- Your source code.\n- A compiled executable named a5.\n- Any additional files that are necessary.\n- The README file.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2003",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/4771822e099c02401619b9a1f80448e7_MIT6_837F12_2003_qz_1.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 1\nTuesday October 26, 2003 2:40-4pm\nOne page of notes allowed\nName:\n/ 15\n/ 12\n/ 15\n/ 8\nTotal\n/ 50\nLight and shading [\n/15]\n1.1\nPhong [\n/2]\nWhat is the visual effect of the Phong exponent? Write one or two sentences.\n1.2\nLambertian materials [\n/4]\nFor a diffuse material, the BRDF is constant, yet the intensity varies across a rounded surface. Why?\nWrite one or two sentences.\n1.3\nFalloff[\n/2]\nIn the real world, how fast does light intensity decrease with respect to distance from a point light\nsource?\n\n1.4\nShadows [\n/3]\nWhich algorithm produced the shadows in the following image: ray-casting, shadow maps or shadow\nvolumes? How can you tell?\n1.5\nRecursive ray tracing [\n/4]\nIf the objects in your scene have reflective and transmissive color no \"brighter\" than (0.5, 0.5, 0.5)\nand the recursive ray weight cutoff is 0.05, what is the maximum depth of the ray tree in your ray\ntracer? Explain briefly.\n\nLinear algebra [\n/12]\n2.1\nLinearity [\n/3]\nWhat formal property defines linearity? Write one or two equations that characterize a linear operator.\n2.2\nTranslation [\n/2]\nShow that translation is not linear in Euclidean space.\n2.3\nTranslation in homogeneous coordinates [\n/2]\nGive the 4x4 matrix M for translation by a vector (tx, ty, tz ) in homogeneous coordinates.\n\n2.4\nSum in homogeneous coordinates [\n/5]\nConsider the coordinate-by-coordinate sum of two homogeneous vectors (four components each).\nWhat's the 3D geometric interpretation of this sum? First treat the case where both vectors have\nw = 1, then the more general case where w = 0.\nNow show that translation is a linear operation in homogeneous coordinates.\nRay-cone intersection [\n/15]\n3.1\nCone equation [\n/4]\nGive the implicit equation for a double-cone of angle θ centered on the origin and oriented along the\nz direction.\nz\ny\nx\nθ\n\n3.2\nRay equation [\n/2]\nGive the parametric equation of a ray with origin O and direction dJ.\n3.3\nIntersection [\n/6]\nGive the equation for the parameter t at the ray-cone intersection. Solve for t. Do you always find a\nsolution? Why or why not?\n3.4\nFinite cone [\n/3]\nHow do you need to change the equation or the code to handle half-infinite cones (only the half in the\npositive z half space)?\n\nRasterization [\n/8]\n4.1\nEfficiency [\n/4]\nHere is pseudo-code for triangle rasterization.\nFor each triangle\nSet up coefficients ai, bi, ci for the 3 edge equations\nFor each screen pixel\nFor each edge fi = ai x + bi y + ci\nIf all fi are positive\ndisplay pixel\nGive two possible optimizations.\n4.2\nLine rasterization [\n/4]\nHow are line rasterization and ray-intersection acceleration related?\nHow do they differ?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2003",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/50aad24f5a60893aeb37a6be3fce55ff_MIT6_837F12_2003_qz_2.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 2\nThursday November 20, 2003 2:40-4pm\nOne hand-written sheet of notes allowed\nName:\nAnimation [\n/4]\n1.1\nQuaternions vs. Euler Angles [\n/2]\nQuaternions are generally used for animation instead of Euler Angles. Why? Give 2 reasons.\n1.2\nInterpolation in Quaternion Space [\n/2]\nIf we linearly interpolate the values of two rotations in quaternion space, the speed of animation will\nnot be constant. How does the speed vary? Draw a simple figure to explain this variation.\n/ 4\n/ 15\n/ 5\n/ 5\n/ 12\n/ 2\n/ 7\nTotal\n/ 50\n\nRendering pipeline [\n/15]\n2.1\nRay Casting vs. the Graphics Pipeline [\n/2]\nDescribe the main algorithmic difference between standard ray casting and rendering using the graph\nics pipeline.\n2.2\nPerspective Projection [\n/4]\nGive the 4x4 matrix corresponding to a perspective\nprojection of all points to the z=d plane with the\neyepoint at the origin.\nHint: Use similar triangles.\n(x,y,z)\nZ\nX\nz = d\n(x',y',d)\n\n⎡\n⎡\n\n⎡\n\n⎡\n\n⎡\nx\nx'\n\n⎢\n⎢\n\n⎢\n\n⎢\n\n⎢\n\n⎢\n⎢\n\n⎢\n\n⎢\n\n⎢\n\n⎢\n⎢\n⎢\ny ⎢\n⎢\n⎢ =\n\n⎢\n⎢\n⎢ homogenize\n\n⎢\n⎢\n⎢ =\ny' ⎢\n⎢\n⎢\n\n⎢\n⎢\n\n⎢\n\n⎢\n\n⎢\n\n⎢\n⎢ z ⎢\n⎢\n\n⎢\n⎢\n==\n\n⎢\n⎢\nd ⎢\n⎢\n\n⎢\n⎢\n\n⎢\n\n⎢\n\n⎢\n\n⎣\n⎣\n\n⎣\n\n⎣\n\n⎣\nmatrix\nun-homogenized\nhomogenized\npoint\npoint\n\n2.3\nTriangle Rasterization [\n/5]\nDescribe how to rasterize a triangle with vertices v0, v1, and v2 into the framebuffer, as implemented\non a modern graphics card. Use pseudo-code as appropriate. Make sure that small triangles are\nprocessed efficiently. Be specific about how you determine that a point is inside the triangle.\n2.4\nThe z-buffer [\n/2]\nHow do you modify your answer to the previous question to use the z-buffer to correctly rasterize\nmultiple overlapping triangles?\n2.5\nLinearity of Projection [\n/2]\nCan we interpolate z linearly in screen space? Explain.\n\nProcedural Solid Texturing [\n/5]\nWrite pseudo-code for a procedural wood shader similar to that shown in the image below. The wood\nmaterial is composed of two other materials, m1 and m2, arranged in cylindrical bands around the\ny-axis with thicknesses t1 and t2. Given point p, return the appropriate material. Don't worry about\nthe variations in circular cross-section or band thickness seen in natural wood grain.\ny\nx\nz\nt1\nt2\n\nCurves and surfaces [\n/5]\n4.1\nInterpolation vs. Approximation Splines [\n/1]\nSketch an interpolation and an approximation spline curve for the 4 control points below.\np2\np3\np2\np3\np1\np4\np1\np4\nINTERPOLATION SPLINE\nAPPROXIMATION SPLINE\n4.2\nBezier vs. B-Spline [\n/1]\nLabel each curve below as Bezier or B-spline.\n4.3\nPolynomial Degree [\n/1]\nIf we want a single polynomial to pass through n points, what degree polynomial is required?\n4.4\nModeling with high order polynomials [\n/2]\nWhy can it be difficult to model with high-order interpolation polynomials?\n\nShadows [\n/12]\n5.1\nShadow Volumes [\n/3]\nSketch the shadow volumes corresponding to this 2D scene and explain how the technique is used to\ndetermine whether point p is in shadow.\npoint light source\neyepoint\np\n5.2\nShadow Techniques [\n/5]\nFor each shadow algorithm below, check the boxes to indicate the features and limitations inherent\nin the technique. The features and limitations may be used more than once.\nFeatures / Limitations\nPlanar\nFake\nShadows\nProjective\nTexture\nShadows\nShadow\nMaps\nShadow\nVolumes\nRay\nCasting\nShadows\nAllows objects to cast shadows\non themselves (self shadowing)\nPermits shadows on arbitrary\nsurfaces (i.e. curved)\nRenders geometry from the\nviewpoint of the light\nGenerates extra geometric primitives\nLimited resolution of intermediate\nrepresentation can result in jaggie\nshadow artifacts\n\n5.3\nSoft Shadows [\n/2]\nDescribe how one of these shadowing techniques can be extended to produce soft shadows from an\narea light source.\n5.4\nThe Bias (Epsilon) Problem [\n/2]\nDescribe for one of these shadowing techniques how and why it suffers from the bias (epsilon) problem.\nColor [\n/2]\n6.1\nMetamers [\n/2]\nWhat are metamers?\n\nGlobal Illumination [\n/7]\n7.1\nRadiosity [\n/1]\nWhat photorealistic effects are missing from your ray tracer that can be captured using radiosity?\n7.2\nRadiosity [\n/1]\nWhat other aspect of the radiosity computation makes it a popular choice for interactive architectural\nvisualization and games?\n7.3\nMonte-Carlo Ray Tracing [\n/2]\nWhat additional effects are captured by extending a basic ray tracing algorithm with Monte-Carlo\nray tracing techniques?\n7.4\nForm Factors for Radiosity [\n/3]\nOrder these pairs of patches by the form factor Fij (the fraction of light energy leaving patch j that\narrives at patch i). Label the pair with the largest form factor '1', the 2nd largest '2', etc.\nj\noccluder\ni\nj\nj\nj\nj\ni\ni\ni\ni\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2003_ray tracing",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/bd1dc9cdb744de87238ef9d34735546c_MIT6_837F12_2003qz1_ray_tr.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 1: Ray Tracing\nTuesday October 7, 2003 2:40-4pm\nOne hand-written sheet allowed\nTotal is 50 points\nName:\nRay Tracing [\n/14]\n1.1\nComplexity [\n/3]\nWe want to render a scene of N objects with one single light source onto an image of M pixels (the\nn\nn\nimage resolution is\nM * M). Any object can be reflective and refractive, but we enforce a maximum\nrecursion depth of K.\nWhat is the worst-case complexity of ray tracing for the total image?\n1.2\nLight sources [\n/2]\nHow is this complexity changed when we have L light sources?\n1.3\nForward ray tracing [\n/3]\nGive a 1-to-2-sentence explanation of why forward ray tracing is not directly practical.\n\n1.4\nWhat is the bug that caused the artifacts in this image? [\n/3]\n1.5\nWhich sphere has the bigger index of refraction? [\n/3]\n\nTransformations [\n/6]\n2.1\nMatrix form [\n/3]\nWhat is the 4 × 4 matrix in homogeneous coordinate form corresponding to a 3D translation by\n(a, b, c)?\n2.2\nNormal transform [\n/3]\nIn a ray tracer, when an object is transformed by a linear transformation described by matrix M,\nhow must we transform the surface normal after ray intersection? Give both a one- or two-sentence\nexplanation and a formula.\nLocal shading [\n/8]\n3.1\nCoefficients [\n/3]\nDescribe what the image will look like if the scene contains no real light source but the ambient light\ncolor is (1,1,1).\n3.2\nGive a one-sentence description of the Fresnel effect. [\n/3]\n3.3\nDimensionality of BRDFs [\n/2]\nHow many dimensions does an anisotropic BRDF have?\nHow many dimensions does an isotropic BRDF have?\n\nRay-Cylinder Intersection [\n/22]\ny\nx\n\nr\nR\nD\n4.1\nImplicit cylinder [\n/2]\nGive the implicit equation for an infinite cylinder centered on the z axis and with radius r.\n4.2\nExplicit ray [\n/2]\nGive the explicit (parametric) equation for a ray with origin R and direction D.\n4.3\nRay-cylinder intersection equation [\n/4]\nWrite the quadratic equation for the intersection of a ray with an infinite cylinder centered on the z\naxis. Solve your equation for t.\n\n4.4\nRay-cylinder intersection pseudo code [\n/8]\nUsing the result from the previous question, write the pseudocode for the intersection method\nbool Cylinder::intersect(const Ray &r, Hit &h, float tmin);\nDon't forget to compute the surface normal, but don't worry about the material.\n\n4.5\nGeneral cylinder [\n/3]\nIn your ray tracer, without writing additional code, how would you use the code from question 4.4 to\nrender arbitrary infinite cylinders (arbitrary position and arbitrary orientation)?\n4.6\nNon-infinite cylinders [\n/3]\nHow would you modify the code from question 4.4 to render non-infinite cylinders. That is, the\ncylinder only goes from z1 to z2. You do not need to render the caps of the cylinder. You can assume\nthat z2 z1.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2006",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/14040b6bd1c0cd2936e04c9b2371e4a0_MIT6_837F12_2006_qz_1.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 1\nThursday October 19, 2006 2:40-4pm\nOne sheet of notes (2 pages) allowed\nName:\nTransformations [\n/10]\n1.1\nLinearity [\n/3]\nWhat does it mean for a transformation or an operator to be linear? [\n/3]\n1.2\nHomogeneous coordinates and IFS [\n/7]\nConsider the 2D IFS (Iterated Function System) defined in 2D by\nA = ∪fi(A)\nThat is, this fractal is the set of points A that is equal to the union of its transformed versions by the\ntransformations fi. The fi are described by the following matrices in homogeneous coordinates\n⎛\n⎞\n⎛\n⎞\n⎛\n⎞\n0.5\n0.5\n0.5\n0.5\n⎜\n⎟\n⎜\n⎟\n⎜\n⎟\n⎜\n⎟\n⎜\n⎟\n⎜\n⎟\nf0 = ⎜ 0\n0.5\n⎟\nf1 = ⎜ 0\n0.5\n0 ⎟\nf2 = ⎜ 0\n0.5\n0.5 ⎟\n⎝\n⎠\n⎝\n⎠\n⎝\n⎠\nExplain the effect of each of the three transformations (e.g. translation by something followed by\na rotation by another thing). [\n/4]\n/ 10\n/ 22\n/ 18\nTotal\n/ 50\n\nWhat is the resulting fractal? The name is not necessary, you can roughly draw it. Advice: draw\nthe first few iterations starting with the unit square from (0, 0) to (1, 1). [\n/3]\nCurves and surfaces [\n/22]\nIn class, we have focused on cubic B ezier splines. However, one can similarly define quadratic B ezier\nsplines using the Bernstein polynomials:\nB1(t) = (1 - t)2\nB2(t) = 2t(1 - t)\nB3(t) = t\nHow many control points do we need for a quadratic B ezier spline? [\n/1]\nProve that the weights defined by these basis functions always sum to one. [\n/4]\nWhy is it critical for splines that the weights sum to one? [\n/4]\n\nDoes the curve approximate or interpolate the control points? The answer can be different for the\nvarious points... [\n/3]\nWhat is the derivative (tangent) at the two extremities as a function of the control point?\n[\n/4]\nWhat does it mean geometrically? That is, how is the geometric tangent at the extremities related\nto the control points? [\n/3]\nHow many control points do we need for a tensor-product quadratic B ezier patch? [\n/3]\n\nAnimation [\n/18]\n3.1\nParticles [\n/10]\nd2 x\nConsider a simplified 1D version of the spring equation: dt2 = -kx where x is a scalar function. The\ninitials conditions are x(0) = d and dx (0) = 0.\ndt\nWhat is the rest length of this spring? [\n/1]\nDescribe the system after one step of Euler integration with time step h (that is, give the values\nof x and dx (0) = 0, which you can note v). [\n/3]\ndt\nDescribe the system after two step of Euler integration with time step h. [\n/3]\nFor which value of h does the length of x increase after two iterations compared to the initial length?\nThat is, when do we have |x2| > |x0|, where xi is the value after i iterations. [\n/3]\n\n3.2\nQuaternion [\n/8]\n∗∗\nLet q1 and q2 be two unit quaternions. Prove that (q1q2)∗ = q2 q1 .\nFirst, prove this using quaternion algebra. Recall that (d; iu)∗ = (d; -ui) and\n(d, iu)(d', iu') = (dd' - iu.iu'; diu' + d'iu + iu × iu'). [\n/5]\nSecond, give a geometric or matrix argument. [\n/3]\nEXTRA\n\nCREDIT\n\n4.1\nEasy extra credit\ndX\nConsider a general multivariable linear first-order ODE of the form\n= MX where X is an n\ndt\ndimensional vector and M is an n × n matrix.\nDerive the implicit Euler integration for this case. That is, express X(t + h).\n\n4.2\nHarder extra credit\nWhat limits the stability of the method, that is, how does the maximum stable time step h relate to\nproperties of the matrix?\n4.3\nEven more fun extra credit\ndX\nNow consider a general first-order multivariate ODE of the type\n= f(X) where f is an arbitrary\ndt\nsmooth function. How do you adapt the above implicit integration scheme to this situation?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2009",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/7a869ef0341ee5660060775d333d5acf_MIT6_837F12_2009_final.pdf",
      "content": "MIT EECS 6.837 Computer Graphics, F09\n\nFinal Exam, December 17, 1:30pm-3:30pm\nName:\nTotal: [\n/ 42 ]\nRendering Basics [\n/ 8 ]\n1.1\nRay Casting vs. Rasterization [\n/ 4 ]\nGive pseudocode for rendering an image using a ray caster and a rasterizer.\nRay Casting\nRasterization\n1.2\nVisibility [\n/ 2 ]\nHow is correct visibility ordering between primitives achieved in ray casting? What is the idea in z-buffering?\n1.3\nWorking Set [\n/ 2 ]\nWhat are the main differences between the working sets in rasterization and ray casting? I.e., what needs to\nbe kept in memory during execution in each case?\nfor each pixel for each primitive\ngenerate ray through pixel (project primitive onto screen)\nfor each object for each pixel\ntest if ray intersects object test if pixel inside primitive\n(keep closest intersection) (keep closest intersection)\n(+2) (+2)\nray casting: as we test objects along ray, we only update intersection\nif it's closer than the current closest intersection (+1)\nz-buffer: each pixel keeps a \"closest depth\" value, pixel is only written to\nif current primitive is closer. (+1)\nray caster must have entire scene in memory (+1)\n(can render image in tiles)\nrasterizer must have entire image (and z-buffer) in memory (+1)\ncan stream over primitives\n\nRay Casting/Tracing [\n/ 12 ]\n2.1\nImplicit/explicit surface representations [\n/ 1 ]\nWhat is the difference between implit and explicit surface representations?\n2.2\nRay Representation [\n/ 1 ]\nWhat is the explicit representation of a ray? Give a formula.\n2.3\nRay-Plane Intersection [\n/ 4 ]\nAn infinite plane may be represented by the formula p · n + d = 0 where n is the plane normal, d is a real\nconstant, and p = (x, y, z) is a (variable) 3D point.\na) Is this an implicit or an explicit representation? [\n/ 1 ]\nb) Derive the formula for intersecting a ray and a plane. You can assume that the ray direction is not tangent\nto the plane. [\n/ 3 ]\n2.4\nBarycentric triangle representation [\n/ 2 ]\nGive the barycentric representation of a triangle with vertices {a, b, c} in terms of {α, β, γ}, including possible\nequality and inequality constraints.\nImplicit representation only allows testing if a given point is on the surface.\nExplicit representation lets you generate points on the surface.\nP(t) = O + tD,\nwhere O is the ray origin, D is ray direction, and t>=0 is a scalar.\n(no points off if no definition of terms)\nImplicit (only allows testing, not generation)\nP(t) is on plane\n<=> P(t).n + d = 0 (+1)\n<=> (O + tD).n + d = 0 (+1)\n<=> t(D.n) = -O.n - d\n<=> t = -(O.n + d)/D.n (+1)\nP(alpha,beta,gamma) = alpha*a + beta*b + gamma*c, (+1)\nwith alpha+beta+gamma = 1 and alpha,beta,gamma >= 0. (+1)\nOR P(beta,gamma) = a + beta*(b-a) + gamma*(c-a),\nwith beta+gamma <= 1 and beta,gamma >= 0.\n\n2.5\nBVH Traversal [\n/ 4 ]\nA bounding volume hierarchy (BVH) based on bounding spheres may be represented using a following\nstructure (in pseudocode).\nstruct Node\n{\nSphere\nboundingSphere;\nbool\nisLeaf;\nNode*\nchildren[2];\nlist< Triangle* > primitives;\n// contains stuff in case of a leaf\n};\nThe following pseudocode traverses a BVH. Fill in the two missing pieces. A rough outline will suffice, see\nthe child case for an example.\nbool rayIntersects( Ray* ray, Hit* rayHit, Node* node )\n{\n// a) test if the node intersects the ray at all and act accordingly (\n/ 1)\n// leaf node? test triangles, update hit if necessary, and return\nif node->isLeaf\ntest all triangles in node->primitives\nwhen a triangle is hit\ncompute t and update rayHit if t closer than rayHit\nreturn true if any triangle was hit, false otherwise\nendif\n// b) recurse into the children in the right order, paying attention\n// to handling the case of overlapping nodes correctly (\n/ 3 )\n}\nif node->boundingSphere does not intersect ray\nreturn false\ncompute t's for both child nodes' bounding spheres\nrecurse into closer node first, based on t's (+1)\nif there was a hit in the closer node (+1)\ncheck if hit point is inside the farther node too\nif yes\nrecurse into farther node as well\nif no hit in the closer node (+1)\nrecurse into farther node\nreturn true if anything was hit in either child node, false otherwise\n\nRasterization [\n/ 7 ]\n3.1\nEdge Functions [\n/ 2 ]\nEdge functions ei(x, y) = ax+by +c are 2D line equations that are computed from the three edges (i = 1, 2, 3)\nof a projected triangle. What is the mathematical condition that holds when a pixel/sample at (x, y) is inside\nthe triangle?\n3.2\nRasterization Using Edge Functions [\n/ 5 ]\nGive pseudocode for rasterizing a triangle using edge functions, starting before projection. Your code should\nuse screen bounding boxes for avoiding testing all pixels on the screen and include z-buffering for visibility.\nYou can assume the triangle has a constant color and that clipping has been performed already.\ne_i(x,y) >= 0 for all i=1,2,3\nproject vertices onto screen\ncompute edge functions e_i from projected vertices\ncompute bounding box B from projected vertices (+1 until here)\n(set up interpolation matrix)\nfor all pixels (x,y) inside B (+1)\nevaluate e_i(x,y) for i=1,2,3 (evaluate+test, +1)\nif all positive\ninterpolate z from vertices\nif zbuffer[x,y] > z (z test, +1)\nframebuffer[x,y] = color (z+color update, +1)\nzbuffer[x,y] = z\nendif\nendif\nendfor\n-1 if no z update\n\nShading, Sampling and Textures [\n/ 15 ]\n4.1\nIrradiance\nHow does the irradiance incident on a surface vary with the angle between the surface normal n and incident\nlight direction l? [\n/ 1 ]\n4.2\nThe BRDF [\n/ 1 ]\nThe BRDF stands for \"Bidirectional Reflectance Distribution Function\". It is often denoted by fr(l, v),\nwhere l is incident (light) direction and v is the outgoing (viewing) direction. What does the value fr(l, v)\ntell you?\n4.3\nDiffuse Reflectance [\n/ 1 ]\nHow does the BRDF of an ideally diffuse surface vary with l and v?\n4.4\nTypes of Aliasing [\n/ 2 ]\nWhat is meant by pre-aliasing and post-aliasing?\nWith the cosine (+1). No light from below, i.e., when cosine is negative.\n(No points off for just cosine.)\nf_r(l,v) is the fraction of light that reflects from direction l to direction v.\nIt is a constant, no variation. (+1)\n(What constant? Albedo/pi.)\nPre-aliasing happens when sampling rate is not high enough. This leads to\nthe spectral replicas to overlap, and makes it impossible to reconstruct the\noriginal signal from the samples (+1)\nPost-aliasing means that we perform poor reconstruction based on sampled values (+1)\nThe end result is again that the reconstruction does not match the original.\n\n4.5\nAvoiding Pre-Aliasing [\n/ 4 ]\na) Give the two main ways of preventing or alleviating the effects of pre-aliasing. [\n/ 2 ]\nb) Which one of the two does MIP-mapping approximate? What is the general idea? [\n/ 2 ]\n4.6\nSupersampling, Multisampling [\n/ 6 ]\na) Imagine you are rendering an image with a single sample per pixel. First you sample the image. This\ncreates replicas in the frequency domain. Reconstructing the samples by a low-pass filter that corresponds to\nthe sampling frequency recreates a continuous image. Any original image frequencies above the pixel pitch\nget aliased in the reconstruction. Describe supersampling in similar terms. Which frequencies are aliased in\nthe final output? How does this relate to the two methods for avoiding pre-aliasing? [\n/ 4 ]\nb) How does multisampling differ from supersampling? Why is it useful? [\n/ 2 ]\n1: Sample at a higher rate (+1), this pushes replicas further apart and we can\nrepresent higher frequencies.\n2: Prefilter the signal, i.e., blur before sampling to remove the high\nfrequencies that cannot be represented using the chosen sample rate. (+1)\nMIP-mapping approximates prefiltering (+1) by precomputing a set of different\nlow-pass filtered versions of the texture (+1). (The actual prefilter is\nthen approximated as a combination of the prefiltered results.)\nIn supersampling, we first sample the signal at a higher rate than the output. (+1)\nThis pushes frequency replicas further apart.\nWe then low-pass filter the supersampled signal by a low-pass filter that\ncorresponds to _the final output sampling rate_ and sample the low-passed result\nat the sample locations that correspond to the output sampling rate (+1).\nAliasing only happens for frequencies above the supersampling rate (+1)\nThis approximates a prefilter using a higher sampling rate -- we remove\nfrequencies above the output rate (as we should in prefiltering), but only up\nto the limit given by the supersampling rate (+1)\nIn multisampling, we compute shading results (colors) only once per pixel\nbut supersample visibility (+1) (and share shading results for subpixel samples\nthat fall within the same primitive).\nIt is useful because shading is expensive compared to visibility.\n\nExtra Credit\nNo partial credit for extra credit questions.\n5.1\nCosine Importance Sampling [\n/ 6 ]\nφ\nθ\np\nGiven two uniformly distributed random numbers x1 ∈ [0, 1] and x2 ∈ [0, 1], give the polar coordinates for a\npoint p = (θ, φ) on the hemisphere as functions of x1, x2, such that the distribution of ps on the hemisphere\nis proportional to cos θ. (This is helpful, for instance, when rendering indirect diffuse illumination.)\nAnswer: ( acos(sqrt(x_1)), 2*pi*x_2 )\n(Of course, x_1 <=> x_2.)\n\nYou can use this page for sketching.\n\nYou can use this page for sketching.\n\nYou can use this page for sketching.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2010",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/4d5fa8746ea9838b454519e313d538d7_MIT6_837F12_2010_final.pdf",
      "content": "6.837 Introduction to Computer Graphics\nFinal exam\nMonday, December 13, 2010 1:35-4:30pm\nTwo sheets of notes (4 pages) allowed\nName:\n/ 29\n/ 25\n/ 13\n/ 13\nTotal\n/ 80\n2D Ray Tracing [\n/29]\nRay tracing is usually used to compute 2D images of a 3D scene. We seek to adapt ray tracing to two-\ndimensional flatland, and compute 1D images of a 2D scene. Points now only have two coordinates x\nand y.\n1.1\nRay equation\nGiven an origin (xO, yO) and a direction (xD, yD), what is the parametric equation for the x and y\ncoordinates of a ray? [\n/2]\n1.2\nCircle equation\nThe equivalent of a sphere in 2D is a circle. What is the implicit equation for a 2D circle of radius r\ncentered at the origin? [\n/2]\n\n1.3\nRay-circle intersection\nWrite the equation for the intersection of the above ray and circle, and provide the solution(s). Com\nment on the number of intersections and the test to distinguish the various cases. [\n/9]\n1.4\nNormal\nWhat is the 2D unit normal to the circle at a point (x, y)? [\n/2]\n\n1.5\nTriangles becomes segments\nThe 2D equivalent of a 3D triangle is a segment defined by two vertices. We seek to use barycentric\ncoordinates α and β to perform ray-segment intersection.\nWrite a point P on the line (ab) as a barycentric combination of the two vertices a and b. [\n/2]\nWhat is the conditiont on α and β so that they represent barycentric coordinates?[\n/2]\nWhat extra condition(s) is satisfied when P is inside the segment? [\n/2]\nReduce the above equation to write P as a function of a single barycentric coordinate. [\n/2]\nGiven 1D texture coordinates ua and ub at the two vertices and the barycentric coordinates α and\nβ for P , what is the texture coordinate at P ? [\n/2]\n\n1.6\nRay-segment intersection\nWrite the system of two equations that define the intersection between the segment [ab] and a ray\ndefined by an origin (xO, yO) and a direction (xD, yD). You do not need to solve the system, but write\nit in a matrix form. [\n/4]\n\nKd-tree acceleration [\n/25]\nWe are now back to 3D ray casting and seek to accelerate it. We consider a generalization of kd-trees\nwhere the splitting plane is arbitrary and given by a general plane equation P.N = d\nKdTreeNode:\nKdTreeNode* backNode, * frontNode //children\nVector3D N\nfloat d //the splitting plane is defined by N.P=d\nboolean isLeaf\nListOfTriangles listTri //only for leaves\nWe seek to write the traversal routine float KdTreeNode::intersect(orig, dir, tstart, tend)\nfor a ray (orig, dir) where tstart and tend have already been computed and represent the first\nand last intersection of the ray with the node. It returns the distance to the closest hit. Assume you\nare given a routine\nfloat ListOfTriangles::intersect(orig, dir, tstart, tend)\nthat returns the distance to the closest hit for a list of triangles.\nWe assume that the back and front nodes are ordered correctly with respect to the ray direction\nand we ignore the case where the ray direction is parallel to the splitting plane.\nWrite the base case (both the test and the action). [\n/3]\nWhat is t at the intersection between the ray and the splitting plane? You can use mathematical\nnotations. [\n/3]\nMake a drawing of the three possible cases for the configuration of the ray, the node, and the splitting\nplane. [\n/4]\n\nWrite the full intersect method. [\n/15]\nfloat KdTreeNode::intersect(Vector 3D orig, Vector3D dir, float tstart, float tend)\n\nRasterization [\n/13]\n3.1\nDifference with Ray Casting\nIf the pseudocode for ray casting is\nfor every pixel (ray)\nfor every triangle\ndoes ray hit triangle?\nkeep closest hit?\nWhat is the pseudocode for rasterization?[\n/3]\n3.2\nVisibility\nRay casting maintains the t value for the closest intersection. What is the corresponding mechanism\nin rasterization? [\n/2]\n3.3\nAlias-free shadow maps\nHow do alias-free shadow maps differ from regular shadow maps? [\n/4]\n\n3.4\nInterpolation\nCan we interpolate texture coordinates linearly in screen space? Draw a figure to explain.[\n/4]\nMisc [\n/13]\nWhat does it mean for a function f to be linear? [\n/2]\nIs SSD (skinning) usually implemented in vertex or pixel shaders? [\n/2]\n\nHow many texture pixels must be read in order to perform trilinear lookups? [\n/2]\nWhat is the difference between lerp and slerp? (no formulae, just behavior). [\n/2]\nLabel the curves below as B ezier or B-spline. [\n/2]\nWhat is the 4x4 matrix for a rotation by angle θ around the y axis?[\n/3]\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2010",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/9cffee2a33ce1da69f8d44b19741f648_MIT6_837F12_2010_qz_1.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 1\nTuesday, October 19, 2010 2:40-4pm\nOne hand-written sheet of notes (2 pages) allowed\nName:\nCurves and Surfaces\n1.1\nB ezier curves of degree 1\nIn class, we have studied cubic B ezier curves. In this question, we will simplify it to degree 1 polyno\nmials. This case is rather trivial but will allow us to assess your understanding of splines.\nThe degree 1 Bernstein basis is defined as:\nB1 = t\nB2 = (1 - t)\nHow many control points are needed for a degree-1 B ezier curve? [\n/1]\nWhat is the basis matrix for degree-1 B ezier curves, if the power basis is (1, t)T ? [\n/2]\nDoes the curve interpolate or approximate its control points? [\n/2]\n/ 15\n/ 20\n/ 25\nTotal\n/ 60\nWhat can you say about the tangent at 0 and 1?\nYou do not need to provide derivations. [\n/2]\n\nRecall that the DeCasteljeau construction allows us to subdivide a B ezier curve into two B ezier\ncurves by taking a succession of middle points. What is the corresponding construction for degree-1\nB ezier splines? [\n/5]\n1.2\nB ezier surfaces of degree 1\nWe now consider the extension to bi-parametric 3D surfaces S(u,v) defined as a tensor product of\ndegree-1 B ezier curves.\nHow many control points are needed for such a degree-1 surface? [\n/3]\nTransformations\n2.1\nNormal transformation\nIn this question, we consider standard linear coordinates, not homogenous coordinates, and no trans\nlation. If a 3D object is linearly transformed by the following matrix:\n⎞\n⎛\n⎜\n⎜\n⎜\n⎝\n0.5\n⎟\n⎟\n⎟\n⎠\n\nWhat is the matrix that gives the normal transformation? Do not worry about the final normal\nization. [\n/3]\nIf a 3D object is linearly transformed by the following matrix:\n⎞\n⎛\n⎜\n⎜\n⎜\n⎝\n0.5\n⎟\n⎟\n⎟\n⎠\nWhat is the matrix that gives the normal transformation? Do not worry about the final normal\nization. [\n/5]\n2.2\nRotations\nHow many degrees of freedom for a rotation around the origin in 2D? [\n/1]\nHow many degrees of freedom for a rotation around the origin in 3D? [\n/2]\n\n2.3\nSkinning\nThe skinning or SSD equation for the transformation of a vertex can be given by\n\npi =\nwij Tj Bj\n-1 pi\nj\nWhat does j index ? [\n/1]\nWhat is Bj and why is it needed? [\n/4]\nWhich term(s) vary over time and need to be updated for each frame of the animation?\n[\n/2]\n\nWhat is the problem if\nj wij\n/3]\n= 1? No need for a proof. [\nAnimation\n3.1\nParticle systems\nFor a system of N particles in 3D, how big is the state vector X passed to an ODE solver?\n[\n/1]\nIn a good implementation of particle systems, who is responsible for the computation of forces?\nThe particle system or the ODE solver? [\n/1]\n\nWhat happens if we forget the diagonal springs for cloth simulation? [\n/3]\n3.2\nCollision Detection\nWe want to compute the collision between a single 3D point (e.g. a particle) and a bounding sphere\nhierarchy. The collision method will be called at the root node of the hierarchy. The Node class has the\nfollowing methods already implemented: Node::radius(), Node::center(), and Node::children().\nYou are encouraged to use pseudocode and to assume you can traverse all elements of a list using a\nforeach keyword and that you have access to a good Point3D class.\nWrite the predicate boolean Node::collide(Point3D pt). [\n/8]\n\n3.3\nODE\nWrite the general equation for x(t + h) for the implicit Euler solver, for a generic single-variable x.\n[\n/4]\nRecall that the trapezoid method is the one that does a first (temporary) Euler step, reads the force\nand takes the average of the force at this temporary location and at the origin. Write the equation\nfor x(t + h) using the trapezoid method and our favorite equation: x (t) = -kx(t). [\n/8]\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/0ded3f80a1f1c484000cca061bb4104b_MIT6_837F12_2011_final.pdf",
      "content": "6.837 Introduction to Computer Graphics\nFinal Exam\nTuesday, December 20, 2011 9:05-12pm\nTwo hand-written sheet of notes (4 pages) allowed\nNAME:\n/ 17\n/ 12\n/ 35\n/ 8\n/ 18\nTotal\n/ 90\nSSD\n[\n/17]\nIn this problem we are going to animate a simple character using linear blend skinning.\n1.1\nComputing Bind Pose\nWe are given a skeleton and a skin mesh in a bind pose. Our character has only two bones (red and\ngreen) and we are interested in only one mesh vertex, p. See the diagram below.\nCompute rigid-transformation matrices Bred and Bgreen that transform mesh vertices from local\nbone coordinate system to the global coordinate system.\n[\n/4]\n\nThen compute pred and p\n, the bone space coordinates of vertex p relative to the red and\ngreen\ngreen bones, either geometrically or by inverting Bred and Bgreen.\n[\n/4]\n1.2\nBone Transformations\nWe have transformed each bone of this character according to the diagram below.\nCompute matrices Tred and Tgreen that transform mesh vertices from local bone coordinate system\nto the global coordinate system.\n[\n/4]\n\n1.3\nComputing Vertex Positions\nUsing previously computed pred and pgreen and the new bone matrices Tred and Tgreen, determine\nthe transformed positions of vertex p in the global coordinate system, both for the red and green\nbone.\n[\n/4]\nGiven that the weights for the red and green bone are 0.5, compute the final transformed vertex\nposition in the global coordinate system.\n[\n/1]\n\np\nShading\n[\n/12]\nSuppose we have a sphere centered at the origin, x2 + y2 + z = r . There is a light source at (a,b,c).\nGenerate a formula for finding the color at any point (x,y,z) on the surface of the sphere, assuming\nthat there is diffuse reflection. Define any additional terms you introduce.\n[\n/12]\nRay Tracing\n[\n/35]\n3.1\nRefraction\n[\n/6]\nRecall that the formula for the outgoing angle of a refracted ray is:\n\nT = ηr(N · I) -\n1 - ηr(1 - (N · I)2) N - ηrI\nWhat is the name of the physical phenomenon that causes the term under the square root to be\nnegative?\n[\n/3]\nHow should we deal with the transmitted ray in such a case?\n[\n/3]\n\n3.2\nKd-tree\n[\n/13]\nBelow is the representation of a given 2D Kd-tree with the leaves indicated by upper-case letters. We\nhave drawn some leaf geometry in blue for motivation, but you do not need to consider it, albeit to\nnotice that the particular ray r does not have any intersection with the scene.\nA\nr\nB\nC\nD\nH\nE\nG\nF\nDraw the corresponding tree structure.\n[\n/5]\nWe now consider the traversal of this kd-tree for the ray r, as it would happen for ray-tracing\nacceleration. For warm up, draw the four intersections with the sides of the bounding box of the tree\nthat occur during the initialization of the traversal.\n[\n/2]\nWe now want you to show the order in which ray-plane intersections are computed for the efficient\nhierarchical traversal of the tree. Draw a cross at each intersection point and write its order as a\nnumber next to it. NB: we want the order in which intersections are calculated, not the order along\nthe ray. Make sure you use a smart traversal that only visits relevant nodes and that the order can\nenable early termination if appropriate.\n[\n/6]\n\n3.3\nRay slab intersection\n[\n/16]\nWe seek to compute the intersection between a ray and a convex object defined as the intersection\nof a set of slabs. Slabs are the space between two parallel planes (see figure). A slab with index i\nis defined by a normal Ni and two real numbers di1 and di2. The axis-aligned bounding boxes we\nstudied in class are a special type of such objects where the three slabs have axis-aligned normals.\nWe want to adapt the fast ray-box intersection algorithm to handle general slabs. We parameterize\nour ray as P (t) = O + tD where O is the origin and D the direction. You can assume that the ray is\ngoing in the positive direction (i.e. t1 is always smaller than t2) and you should not worry about the\nray being parallel to a plane or starting inside the slab.\nNi\nO\nx\ny\nD\nd\nd\ni2\ni1\nt1\nt1\nt2\nFirst, we consider a single slab with index i. Write the equation for t1 and t2, the intersection\nparameters for the first and second plane delimiting this slab.\n[\n/6]\n\nWe now turn to the intersection of the ray and the CSG intersection of N slabs. We initialize tstart\nand tend with the values for t1 and t2 given by the first pair of planes. Write pseudocode to update\ntstart and tend with the values t1\n; and t2\n; for a new pair of planes.\n[\n/6]\nFinally, after they have been updated to take into account all slabs, give a criterion on tstart and\ntend that determines if the intersection between the ray and the volume is non-empty. Do not worry\nabout whether the slab is in front or behind the origin.\n[\n/4]\n\nRasterization\n[\n/8]\nWe want to implement two-scale rasterization where rectangular groups of pixels are quickly declared\nfully inside or fully outside a triangle. Assume you are given the three edge equations so that a 2D\npoint P inside the triangle respects P · Ni - di > 0 for i = 0..2. A rectangular region is described by\nits four corners Pj for j = 0..3.\nWhat is the condition for the full rectangle to be entirely inside the triangle? [\n/4]\nThings are more tricky for the test to be fully outside. A na ıve solution would be to say that all\nfour corners fail the edge tests. Find a counter example.\n[\n/4]\n\nGraphics hardware\n[\n/18]\nList one form of task vs. data parallelism in graphics hardware.\n[\n/4]\nExample of task parallelism:\nExample of data parallelism:\nAttribute the following properties to either graphics hardware or CPU (we recommend against\nusing the acronym GPU because we might have a hard time distinguishing your Gs and Cs :-(\n[\n/6]\n- optimized for latency\n- latency hiding\n- extremely long pipeline (1000 stages)\nWould the following algorithm be implemented in a vertex or pixel shader?\n[\n/8]\nSSD skinning\nPhong shading\nBlend shapes\nShadow map query\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "2011 quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/edd87bd64e6febf80aa09c95cb779ac7_MIT6_837F12_2011_qz1.pdf",
      "content": "6.837 Introduction to Computer Graphics\nQuiz 1\nTuesday, October 18, 2011 2:40-4pm\nOne hand-written sheet of notes (2 pages) allowed\nNAME:\n/ 20\n/ 18\n/ 18\n/ 24\nTotal\n/ 80\nThe answer are in blue and extra information that was not required is in green.\nCoordinate systems\n[\n/20]\nt\nGiven that basis a\ncan be expressed in basis abt as a t = abtM, what are the coordinates of the\nvector abtNc with respect to the basis a t, where N and M are matrices and c is a column vector of\ncoordinates?\n[\n/2]\nM -1Nc\nLet a0 be the zero vector. For any linear transformation L, what is L(a0)?\n[\n/2]\nL(a0) = a0\nLet T (av) be the transformation that adds a specific non-zero constant vector ak to av: T (av) = av +ak.\nIs T a linear transformation?\n[\n/2]\nNo\n(because T (a0) is not a0)\n\nSuppose af t is a 2D orthonormal frame, and we apply the transform af t ⇒ af tST , where S is a matrix\nthat applies a uniform scale by a factor of 2, and T translates by 1 along the x axis.\nWrite these two 3 × 3 matrices.\n[\n/4]\n⎤\n⎡\n⎤\n⎡\n⎢⎢⎢⎣\n⎥⎥⎥⎦\nT =\n⎢⎢⎢⎣\n⎥⎥⎥⎦\nS =\nHow far does the frame's origin move, measured in the original units of af t ?\n[\n/4]\nand abt shown below:\nGiven the two orthonormal frames aat\nat\nat\nbt\nbt\nd1\nd1\nd2\nd2\nd3\nd3\nd4\nd4\nθθ\nwith distances given by the positive quantities di. What are the matrices R and T such that\nabt = a tTR? Note: do this without using trigonometric terms in the matrix T .\n[\n/6]\n⎤\n⎡\n⎤\n⎡\ncos θ - sin θ 0\nd3\nT =\n⎢⎢⎢⎣\n⎥⎥⎥⎦\nR =\n⎢⎢⎢⎣\n⎥⎥⎥⎦\nsin θ\ncos θ\nd4\n\nSplines\n[\n/18]\nDraw the DeCasteljau construction for the evaluation of the point t=0.5 of the B ezier spline defined\nby the 4 points below.\n[\n/6]\nP1\nP2\nP3\nP4\nHow can you choose the control points so that a cubic B ezier curve is a closed loop?\n[\n/4]\nP1 = P4\nWhat is the order of continuity that you get, in general, with such closed-loop cubic B ezier spline?\n[\n/4]\nC0\nbut it has a C1 discontinuity\nWhat additional constraint on the control points is needed to get one more order of continuity\n(while keeping it a closed-loop cubic spline defined by four points), and what would the consequence\nbe?\n[\n/4]\nP1P2 and P3P4 should be aligned. Since P1 = P 4, would mean that all four points are aligned and\nthe spline reduces to a segment.\nP1 = P4 should also be between P2 and P3. Furthermore, if we want C1 continuity and not just\nG1, we need the length of P1P2 to be the same as P3P4\n\nColor\n[\n/18]\nImagine that humans were intelligently designed with simpler color bi-chromat vision where the two\ntypes of cones have the following spectral responses:\nλ (nm)\nλ (nm)\nresponse\n(arbitrary unit)\nresponse\n(arbitrary unit)\nS\nL\nWhat would be the pair of cone responses LS for a stimulus of monochromatic light at 500nm with\ntotal power 2.73 in our arbitrary units?\n[\n/2]\n(0, 2.73)\nGive two spectra that are metamers under this set of cones.\n[\n/4]\nλ (nm)\nλ (nm)\nIn fact any spectrum that is symmetric with respect to 550nm is a metamer of these two.\nAnother possible answer is to use parts of the spectrum that have zero response and add them. It's\nkind of correct but in my opinion it's a little bit cheating.\n\nWe seek to reproduce the CIE color matching experiment with primary wavelengths 500nm and\n600 nm. What is the required amount of these two primaries to match unit-power stimuli at the\nfollowing wavelengths:\n[\n/6]\n500nm\n(1 0)\n550nm\n(0.5 0.5)\n600nm\n(0 1)\nGive an example that illustrates what happens if we seek to use the same spectra for analysis (e.g.\nthe two color filters on a camera) and for synthesis (e.g. the two primaries of a video projector) with\nthese 2-cone humans.\n[\n/6]\nSuppose we use the cone spectra both for analysis and synthesis and seek to reproduce a monochro\nmatic stimulus at 500nm (left figure below). The response to our analysis is (1 0). We then scale the\ntwo spectra by these numbers for synthesis, as shown on the right, leading to a new stimulus with the\nshape of a tent. Unfortunately, this stimulus has a non-zero response for the S cone and the color is\nnot accurately reproduced.\nλ (nm)\nλ (nm)\n\nPhysically-based animation\n[\n/24]\nWe are given a single particle with 1D vertical position described by the scalar function x(t) where\nthe positive values of x go up. It follows Newtonian mechanics and is affected by gravity described\ndx\nby the constant g (in ms-2) and a drag force F = -dv where v =\nand d is a constant. Initially, the\ndt\nparticle is at x0 and travels with velocity v0. The particle has mass m.\nWrite the second-order ODE that describes this particle system.\n[\n/4]\nd2x\ndt2 = -g - m dv\nTransform this second-order ODE into a first order ODE system .\n[\n/4]\n⎛\n⎞\n⎛\n⎞\nd\nx\nv\n⎝\n⎠ = ⎝\n⎠\ndt\nv\n-g -\ndv\nm\nUse the Euler method to estimate the state of the system after time step h.\n[\n/8]\nx(h) = x0 + v0h\nv(h)\n= v0 - gh - h dv0\nm\n\nm\nhd\n= v0\n1 -\n- gh\nm\n\nUse the trapezoid method to estimate the value of x after time step h. Note: we do not ask you\nto compute the velocity after this full step.\n[\n/8]\nWe perform a Euler step, read the right-hand side of the equation and use the average of the\nright-hand side at this location and at the original location.\n\nm\n\nx(h)\n= x0 + h\nv0 + v0 1 - hd\n- gh\nm\nm\nhd\n= x0 + hv0 1 -\n- gh2\n2m\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Acceleration Structures for Ray Casting",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/51937f370603b18f259f00e814f96a0c_MIT6_837F12_Lec14.pdf",
      "content": "MIT EECS 6.837 Computer Graphics\nWojciech Matusik, MIT EECS\nAcceleration\nStructures for Ray Casting\nHasan et al. 2007\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nStopping criteria:\n- Recursion depth\n- Stop after a\nnumber\nof bounces\n- Ray contribution\n- Stop if reflected /\ntransmitted\ncontribution\nbecomes too small\ntrace ray\nIntersect all objects\ncolor = ambient term\nFor every light\ncast shadow ray\ncolor += local shading term\nIf mirror\ncolor += colorrefl *\ntrace reflected ray\nIf transparent\ncolor += colortrans *\ntrace transmitted ray\n\n- Does it ever end?\nRecap: Ray Tracing\n\nRecursion For Reflection: None\n0 recursion\n\nRecursion For Reflection: 1\n0 recursion\n\nRecursion For Reflection: 2\n0 recursion\n\nRay tree\n- Visualizing the ray tree for single image pixel\nincoming\nreflected ray\nshadow ray\ntransmitted (refracted) ray\n\nRay tree\n- Visualizing the ray tree for single image pixel\nincoming\nreflected ray\nshadow ray\ntransmitted (refracted) ray\nThis gets pretty complicated\npretty fast!\n\nQuestions?\n\nRay Tracing Algorithm Analysis\n- Lots of primitives\n- Recursive\n- Distributed Ray\nTracing\n- Means using many\nrays for non-\nideal/non-pointlike\nphenomena\n- Soft shadows\n- Anti-aliasing\n- Glossy reflection\n- Motion blur\n- Depth of field\ncost ≈ height * width *\nnum primitives *\nintersection cost *\nsize of recursive ray tree *\n\nnum shadow rays *\n\nnum supersamples *\nnum glossy rays *\nnum temporal samples *\nnum aperture samples *\n. . .\nCan we reduce this?\n\n- Motivation\n- You need LOTS of rays to generate nice pictures\n- Intersecting every ray with every primitive becomes the\nbottleneck\n- Bounding volumes\n- Bounding Volume Hierarchies, Kd-trees\nFor every pixel\nConstruct a ray from the eye\nFor every object in the scene\n\nFind intersection with the ray\n\nKeep if closest\nShade\nToday\n\nAccelerating Ray Casting\n- Goal: Reduce the number\nof ray/primitive\nintersections\n\nConservative Bounding Volume\n- First check for an\nintersection with a\nconservative\nbounding volume\n- Early reject: If ray\ndoesn't hit volume,\nit doesn't hit the\ntriangles!\n\nConservative Bounding Volume\n- What does\n\"conservative\" mean?\n- Volume must be big\nenough to contain all\ngeometry within\n\nConservative Bounding Regions\n- Desiderata\n- Tight →\navoid false positives\n- Fast to intersect\n\nRay-Box Intersection\n- Axis-aligned box\n- Box: (X1, Y1, Z1) → (X2, Y2, Z2)\n- Ray: P(t) = Ro + tRd\ny=Y2\ny=Y1\nx=X1\nx=X2\nRo\nRd\n\nNaive Ray-Box Intersection\n- 6 plane equations: Compute all intersections\n- Return closest intersection inside the box\n- Verify intersections are on the correct side\nof each plane: Ax+By+Cz+D < 0\ny=Y2\ny=Y1\nx=X1\nx=X2\nRo\nRd\n\nReducing Total Computation\n- Pairs of planes have the same normal\n- Normals have only one non-zero component\n- Do computations one dimension at a time\ny=Y2\ny=Y1\nx=X1\nx=X2\nRo\nRd\n\nTest if Parallel\n- If Rdx = 0 (ray is parallel) AND\nRox < X1 or Rox > X2 → no intersection\ny=Y2\ny=Y1\nx=X1\nx=X2\nRo\nRd\n(The same\nfor Y and Z,\nof course)\n\nFind Intersections Per Dimension\n- Basic idea\n- Determine an interval along the ray for each dimension\n- The intersect these 1D intervals (remember CSG!)\n- Done!\nRo\ny=Y2\ny=Y1\nx=X1\nx=X2\n\nFind Intersections Per Dimension\n- Basic idea\n- Determine an interval along the ray for each dimension\n- The intersect these 1D intervals (remember CSG!)\n- Done!\nRo\ny=Y2\ny=Y1\nx=X1\nx=X2\nInterval\nbetween X1\nand X2\n\nFind Intersections Per Dimension\n- Basic idea\n- Determine an interval along the ray for each dimension\n- The intersect these 1D intervals (remember CSG!)\n- Done!\nRo\ny=Y2\ny=Y1\nx=X1\nx=X2\nInterval\nbetween X1\nand X2\nInterval\nbetween Y1\nand Y2\n\nFind Intersections Per Dimension\n- Basic idea\n- Determine an interval along the ray for each dimension\n- The intersect these 1D intervals (remember CSG!)\n- Done!\nRo\ny=Y2\ny=Y1\nx=X1\nx=X2\nInterval\nbetween X1\nand X2\nInterval\nbetween Y1\nand Y2\nIntersection\n\nIntersecting 1D Intervals\n\nIntersecting 1D Intervals\nStart=\nmax of mins\n\nIntersecting 1D Intervals\nStart=\nmax of mins\nEnd=\nmin of maxs\n\nIntersecting 1D Intervals\nStart=\nmax of mins\nEnd=\nmin of maxs\nIf Start > End, the intersection is empty!\n\nFind Intersections Per Dimension\n- Calculate intersection distance t1 and t2\nt1\nt2\nRo\nRd\ny=Y2\ny=Y1\nx=X1\nx=X2\n\nFind Intersections Per Dimension\n- Calculate intersection distance t1 and t2\n- t1 = (X1 - Rox) / Rdx\n- t2 = (X2 - Rox) / Rdx\n- [t1, t2] is the X interval\nt1\nt2\nRo\nRd\ny=Y2\ny=Y1\nx=X1\nx=X2\n\nThen Intersect Intervals\n- Init tstart & tend with X interval\n- Update tstart & tend for each subsequent dimension\ny=Y2\ny=Y1\nx=X1\nx=X2\ntend\ntstart\n\nThen Intersect Intervals\n- Compute t1 and t2 for Y...\nt1\nt2\ny=Y2\ny=Y1\nx=X1\nx=X2\n\nThen Intersect Intervals\n- Update tstart & tend for each subsequent dimension\n- If t1 > tstart, tstart = t1\n- If t2 < tend, tend = t2\ny=Y2\ny=Y1\nx=X1\nx=X2\nt1\nt2\ntend\ntstart\n\nThen Intersect Intervals\n- Update tstart & tend for each subsequent dimension\n- If t1 > tstart, tstart = t1\n- If t2 < tend, tend = t2\ntend\ny=Y2\ny=Y1\nx=X1\nx=X2\ntstart\nt1\nt2\n\nThen Intersect Intervals\n- Update tstart & tend for each subsequent dimension\n- If t1 > tstart, tstart = t1\n- If t2 < tend, tend = t2\ny=Y2\ny=Y1\nx=X1\nx=X2\ntend\ntstart\n:-)\n\nIs there an Intersection?\n- If tstart > tend → box is missed\ny=Y2\ny=Y1\nx=X1\nx=X2\ntend\ntstart\n\nIs the Box Behind the Eyepoint?\n- If tend < tmin → box is behind\ny=Y2\ny=Y1\nx=X1\nx=X2\ntend\ntstart\n\nReturn the Correct Intersection\n- If tstart > tmin → closest intersection at tstart\n- Else → closest intersection at tend\n- Eye is inside box\ny=Y2\ny=Y1\nx=X1\nx=X2\ntend\ntstart\n\nRay-Box Intersection Summary\n- For each dimension,\n- If Rdx = 0 (ray is parallel) AND\nRox < X1 or Rox > X2 → no intersection\n- For each dimension, calculate intersection distances t1 and t2\n- t1 = (X1 - Rox) / Rdx t2 = (X2 - Rox) / Rdx\n- If t1 > t2, swap\n- Maintain an interval [tstart, tend], intersect with current\ndimension\n- If t1 > tstart, tstart = t1 If t2 < tend, tend = t2\n- If tstart > tend → box is missed\n- If tend < tmin → box is behind\n- If tstart > tmin → closest intersection at tstart\n- Else → closest intersection at tend\n\nEfficiency Issues\n- 1/Rdx, 1/Rdy and 1/Rdz can be pre-computed\nand shared for many boxes\n\nBounding Box of a Triangle\n(xmin, ymin, zmin)\n(xmax, ymax, zmax)\n(x0, y0, z0)\n(x1, y1, z1)\n(x2, y2, z2)\n= (min(x0,x1,x2),\nmin(y0,y1,y2),\nmin(z0,z1,z2))\n= (max(x0,x1,x2),\nmax(y0,y1,y2),\nmax(z0,z1,z2))\n\nBounding Box of a Sphere\nr\n(xmin, ymin, zmin)\n(xmax, ymax, zmax)\n(x, y, z)\n= (x-r, y-r, z-r)\n= (x+r, y+r, z+r)\n\nBounding Box of a Plane\n(xmin, ymin, zmin)\n(xmax, ymax, zmax)\n= (-inf, -inf, -inf)*\n= (+inf, +inf, +inf)*\nn = (a, b, c)\nax + by + cz = d\n* unless n is exactly perpendicular to an axis\n\nBounding Box of a Group\n(xmin_b, ymin_b, zmin_b)\n(xmin, ymin, zmin)\n(xmax, ymax, zmax)\n= (min(xmin_a,xmin_b),\nmin(ymin_a,ymin_b),\nmin(zmin_a,zmin_b))\n= (max(xmax_a,xmax_b),\nmax(ymax_a,ymax_b),\nmax(zmax_a,zmax_b))\n(xmin_a, ymin_a, zmin_a)\n(xmax_b, ymax_b, zmax_b)\n(xmax_a, ymax_a, zmax_a)\n\nBounding Box of a Transform\n(x'min, y'min, z'min)\n(x'max, y'max, z'max)\n= (min(x0,x1,x2,x3,x4,x5,x6,x7),\nmin(y0,y1,y2,y3,y4,x5,x6,x7),\nmin(z0,z1,z2,z3,z4,x5,x6,x7))\nM\n(xmin, ymin, zmin)\n(x0,y0,z0) =\nM (xmin,ymin,zmin)\n= (max(x0,x1,x2,x3,x4,x5,x6,x7),\nmax(y0,y1,y2,y3,y4,x5,x6,x7),\nmax(z0,z1,z2,z3,z4,x5,x6,x7))\n(x1,y1,z1) =\nM (xmax,ymin,zmin)\n(x2,y2,z2) =\nM (xmin,ymax,zmin)\n(x3,y3,z3) =\nM (xmax,ymax,zmin)\n(xmax, ymax, zmax)\nBounding box of transformed object IS NOT\nthe transformation of the bounding box!\n\nBounding Box of a Transform\n(x'min, y'min, z'min)\n(x'max, y'max, z'max)\n= (min(x0,x1,x2,x3,x4,x5,x6,x7),\nmin(y0,y1,y2,y3,y4,x5,x6,x7),\nmin(z0,z1,z2,z3,z4,x5,x6,x7))\nM\n(xmin, ymin, zmin)\n(x0,y0,z0) =\nM (xmin,ymin,zmin)\n= (max(x0,x1,x2,x3,x4,x5,x6,x7),\nmax(y0,y1,y2,y3,y4,x5,x6,x7),\nmax(z0,z1,z2,z3,z4,x5,x6,x7))\n(x1,y1,z1) =\nM (xmax,ymin,zmin)\n(x2,y2,z2) =\nM (xmin,ymax,zmin)\n(x3,y3,z3) =\nM (xmax,ymax,zmin)\n(xmax, ymax, zmax)\nBounding box of transformed object IS NOT\nthe transformation of the bounding box!\nQuestions?\n\nAre Bounding Volumes Enough?\n- If ray hits bounding volume,\nmust we test all primitives inside it?\n- Lots of work, think of a 1M-triangle mesh\nbounding\nsphere\n\nBounding Volume Hierarchies\n- If ray hits bounding volume,\nmust we test all primitives inside it?\n- Lots of work, think of a 1M-triangle mesh\n- You guessed it already, we'll split the primitives in\ngroups and build recursive bounding volumes\n- Like collision detection,\nremember?\nbounding\nsphere\nhierarchy\n\nBounding Volume Hierarchy (BVH)\n- Find bounding box of objects/primitives\n- Split objects/primitives into two, compute child BVs\n- Recurse, build a binary tree\n\n- Find bounding box of objects/primitives\n- Split objects/primitives into two, compute child BVs\n- Recurse, build a binary tree\nBounding Volume Hierarchy (BVH)\n\n- Find bounding box of objects/primitives\n- Split objects/primitives into two, compute child BVs\n- Recurse, build a binary tree\nBounding Volume Hierarchy (BVH)\n\n- Find bounding box of objects/primitives\n- Split objects/primitives into two, compute child BVs\n- Recurse, build a binary tree\nBounding Volume Hierarchy (BVH)\n\n- Find bounding box of objects/primitives\n- Split objects/primitives into two, compute child BVs\n- Recurse, build a binary tree\nBounding Volume Hierarchy (BVH)\n\nWhere to Split Objects?\n- At midpoint of current volume OR\n- Sort, and put half of the objects on each side OR\n- Use modeling hierarchy\n\nWhere to Split Objects?\n- At midpoint of current volume OR\n- Sort, and put half of the objects on each side OR\n- Use modeling hierarchy\nQuestions?\n\nRay-BVH Intersection\n\nRay-BVH Intersection\n\nRay-BVH Intersection\n\nIntersection with BVH\n\nIntersection with BVH\n\nIntersection with BVH\n\nBVH Discussion\n- Advantages\n- easy to construct\n- easy to traverse\n- binary tree (=simple structure)\n\n- Disadvantages\n- may be difficult to choose a good split for a node\n- poor split may result in minimal spatial pruning\n\nBVH Discussion\n- Advantages\n- easy to construct\n- easy to traverse\n- binary tree (=simple structure)\n\n- Disadvantages\n- may be difficult to choose a good split for a node\n- poor split may result in minimal spatial pruning\n\n- Still one of the best methods\n- Recommended for your first hierarchy!\n\nBVH Discussion\n- Advantages\n- easy to construct\n- easy to traverse\n- binary tree (=simple structure)\n\n- Disadvantages\n- may be difficult to choose a good split for a node\n- poor split may result in minimal spatial pruning\n\n- Still one of the best methods\n- Recommended for your first hierarchy!\nQuestions?\n\nKd-trees\n- Probably most popular acceleration structure\n- Binary tree, axis-aligned splits\n- Each node splits space in half along an axis-aligned plane\n- A space partition: The nodes do not overlap!\n- This is in contrast to BVHs\n\nData Structure\nKdTreeNode:\nKdTreeNode* backNode, frontNode //children\nint dimSplit // either x, y or z\nfloat splitDistance\n// from origin along split axis\nboolean isLeaf\nList of triangles //only for leaves\n\nhere dimSplit = 0 (x axis)\nbackNode\nfrontNode\nX=splitDistance\n\nKd-tree Construction\n- Start with scene axis-aligned bounding box\n- Decide which dimension to split (e.g. longest)\n- Decide at which distance to split (not so easy)\n\nKd-tree Construction - Split\n- Distribute primitives to each side\n- If a primitive overlaps split plane, assign to both\nsides\n\nKd-tree Construction - Recurse\n- Stop when minimum number of primitives reached\n- Other stopping criteria possible\n\nQuestions?\n- Further reading on efficient Kd-tree construction\n- Hunt, Mark & Stoll, IRT 2006\n- Zhou et al., SIGGRAPH Asia 2008\nZhou et al.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nKd-tree Traversal - High Level\n- If leaf, intersect with list of primitives\n- If intersects back child, recurse\n- If intersects front child, recurse\n\nKd-tree Traversal, Naive Version\n- Could use bounding box test for each child\n- But redundant calculation: bbox similar to that of\nparent node, plus axis aligned, one single split\n\nKd-tree Traversal, Smarter Version\n- Get main bbox intersection from parent\n- tnear, tfar\n- Intersect with splitting plane\n- easy because axis aligned\ntnear\ntfar\nt\n\nKd-tree Traversal - Three Cases\n- Intersects only back, only front, or both\n- Can be tested by examining t, tstart and tend\n\nKd-tree traversal - three cases\n- If t>tend => intersect only front\n- If t<tstart => intersect only back\nNote: \"Back\" and\n\"Front\" depend on\nray direction!\n\nKd-tree Traversal Pseudocode\ntravers(orig, dir, t_start, t_end):\n#adapted from Ingo Wald's thesis\n#assumes that dir[self.dimSplit] >0\nif self.isLeaf:\nreturn intersect(self.listOfTriangles, orig, dir, t_start, t_end)\nt = (self.splitDist - orig[self.dimSplit]) / dir[self.dimSplit];\nif t <= t_start:\n# case one, t <= t_start <= t_end -> cull front side\nreturn self.backSideNode.traverse(orig, dir,t_start,t_end)\nelif t >= t_end:\n# case two, t_start <= t_end <= t -> cull back side\nreturn self.frontSideNode.traverse(orig, dir,t_start,t_end)\nelse:\n# case three: traverse both sides in turn\nt_hit = self.frontSideNode.traverse(orig, dir, t_start, t)\nif t_hit <= t: return t_hit; # early ray termination\nreturn self.backSideNode.traverse(orig, dir, t, t_end)\n\nImportant!\ntravers(orig, dir, t_start, t_end):\n#adapted from Ingo Wald's thesis\n#assumes that dir[self.dimSplit] >0\nif self.isLeaf:\nreturn intersect(self.listOfTriangles, orig, dir, t_start, t_end)\nt = (self.splitDist - orig[self.dimSplit]) / dir[self.dimSplit];\nif t <= t_start:\n# case one, t <= t_start <= t_end -> cull front side\nreturn self.backSideNode.traverse(orig, dir,t_start,t_end)\nelif t >= t_end:\n# case two, t_start <= t_end <= t -> cull back side\nreturn self.frontSideNode.traverse(orig, dir,t_start,t_end)\nelse:\n# case three: traverse both sides in turn\nt_hit = self.frontSideNode.traverse(orig, dir, t_start, t)\nif t_hit <= t: return t_hit; # early ray termination\nreturn self.backSideNode.traverse(orig, dir, t, t_end)\n\nEarly termination is powerful!\ntravers(orig, dir, t_start, t_end):\n#adapted from Ingo Wald's thesis\n#assumes that dir[self.dimSplit] >0\nif self.isLeaf:\nreturn intersect(self.listOfTriangles, orig, dir, t_start, t_end)\nt = (self.splitDist - orig[self.dimSplit]) / dir[self.dimSplit];\nif t <= t_start:\n# case one, t <= t_start <= t_end -> cull front side\nreturn self.backSideNode.traverse(orig, dir,t_start,t_end)\nelif t >= t_end:\n# case two, t_start <= t_end <= t -> cull back side\nreturn self.frontSideNode.traverse(orig, dir,t_start,t_end)\nelse:\n# case three: traverse both sides in turn\nt_hit = self.frontSideNode.traverse(orig, dir, t_start, t)\nif t_hit <= t: return t_hit; # early ray termination\nreturn self.backSideNode.traverse(orig, dir, t, t_end)\n\nEarly termination is powerful\n- If there is an intersection in the first node, don't visit\nthe second one\n- Allows ray casting to be reasonably independent of\nscene depth complexity\n\nRecap: Two main gains\n- Only intersect with triangles \"near\" the line\n- Stop at the first intersection\n\nTwo main gains\ntravers(orig, dir, t_start, t_end):\n#adapted from Ingo Wald's thesis\n#assumes that dir[self.dimSplit] >0\nif self.isLeaf:\nreturn intersect(self.listOfTriangles, orig, dir, t_start, t_end)\nt = (self.splitDist - orig[self.dimSplit]) / dir[self.dimSplit];\nif t <= t_start:\n# case one, t <= t_start <= t_end -> cull front side\nreturn self.backSideNode.traverse(orig, dir,t_start,t_end)\nelif t >= t_end:\n# case two, t_start <= t_end <= t -> cull back side\nreturn self.frontSideNode.traverse(orig, dir,t_start,t_end)\nelse:\n# case three: traverse both sides in turn\nt_hit = self.frontSideNode.traverse(orig, dir, t_start, t)\nif t_hit <= t: return t_hit; # early ray termination\nreturn self.backSideNode.traverse(orig, dir, t, t_end)\n\nOnly near line\nstop at first intersection\n\nImportant Details\n- For leaves, do NOT report\nintersection if t is not in [tnear, tfar].\n- Important for primitives that overlap multiple nodes!\n\n- Need to take direction of ray into account\n- Reverse back and front if the direction has negative\ncoordinate along the split dimension\n- Degeneracies when ray direction\nis parallel to one axis\n\nImportant Details\n- For leaves, do NOT report\nintersection if t is not in [tnear, tfar].\n- Important for primitives that overlap multiple nodes!\n\n- Need to take direction of ray into account\n- Reverse back and front if the direction has negative\ncoordinate along the split dimension\n- Degeneracies when ray direction\nis parallel to one axis\nQuestions?\n\nWhere to split for construction?\n- Example for baseline\n- Note how this ray traverses easily: one leaf only\n\nSplit in the Middle\n- Does not conform to empty vs. dense areas\n- Inefficient traversal - Not so good!\n\nSplit in the Median\n- Tries to balance tree, but does not conform to empty\nvs. dense areas\n- Inefficient traversal - Not good\n\nOptimizing Splitting Planes\n- Most people use the Surface Area Heuristic (SAH)\n- MacDonald and Booth 1990, \"Heuristic for ray tracing\nusing space subdivision\", Visual Computer\n- Idea: simple probabilistic prediction of traversal cost\nbased on split distance\n- Then try different possible splits and keep the one\nwith lowest cost\n- Further reading on efficient Kd-tree construction\n- Hunt, Mark & Stoll, IRT 2006\n- Zhou et al., SIGGRAPH Asia 2008\n\nSurface Area Heuristic\n- Probability that we need to intersect a child\n- Area of the bbox of that child\n(exact for uniformly distributed rays)\n- Cost of the traversal of that child\n- number of primitives (simplistic heuristic)\n- This heuristic likes to put big densities of primitives\nin small-area nodes\n\nIs it Important to Optimize Splits?\n- Given the same traversal code, the quality of Kd-tree\nconstruction can have a big impact on performance,\ne.g. a factor of 2 compared to naive middle split\n- But then, you should consider carefully if you need that\nextra performance\n- Could you optimize something else for bigger gain?\n\nEfficient Implementation\n- Not so easy, need ability to sort primitives along the\nthree axes very efficiently and split them into two\ngroups\n- Plus primitives have an extent (bbox)\n- Extra tricks include smarter tests to check if a\ntriangle is inside a box\nNode\nbbox of triangle\n\nHard-core efficiency considerations\n- See e.g. Ingo Wald's PhD thesis\n-\n\n- Calculation\n- Optimized barycentric ray-triangle intersection\n- Memory\n- Make kd-tree node as small as possible\n(dirty bit packing, make it 8 bytes)\n- Parallelism\n- SIMD extensions, trace 4 rays at a time, mask results\nwhere they disagree\nhttp://www.sci.utah.edu/~wald/PhD/\n\nPros and Cons of Kd trees\n- Pros\n- Simple code\n- Efficient traversal\n- Can conform to data\n\n- Cons\n- costly construction, not great if you work with moving\nobjects\n\nQuestions?\n- For extensions to moving scenes, see Real-Time KD-\nTree Construction on Graphics Hardware, Zhou et\nal., SIGGRAPH 2008\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nStack Studios, Rendered using Maxwell\nQuestions?\n(c) Next Limit S.L. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Basics of Computer Animation — Skinning/ Enveloping",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/9ab6e7460a5b93d6f97b08f95de23ebc_MIT6_837F12_Lec06.pdf",
      "content": "Many slides courtesy of Jovan\nPopovic, Ronen Barzel, and\nJaakko Lehtinen\nBasics of Computer Animation\nSkinning/Enveloping\n\nMIT EECS 6.837 Computer Graphics\n6.837 Matusik\nCourtesy of Blender Foundation. License CC-BY. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nTraditional Animation\nFrom ACM (c) 1997 \"Multiperspective panoramas for cel animation.\"\n- Draw each frame by hand\n- great control, but tedious\n- Reduce burden with cel animation\n- Layer, keyframe, inbetween, ...\n- Example: Cel panoramas (Disney's\nPinocchio)\nImage courtesy of Garrett Albright on Wikimedia\nCommons. License: CC-BY-SA. This content is\nexcluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- The in-betweening, was once a job for apprentice animators.\nSplines accomplish these tasks automatically. However, the\nanimator still has to draw the keyframes. This is an art form and\nprecisely why the experienced animators were spared the in-\nbetweening work even before automatic techniques.\n- The classical paper on animation by John Lasseter from Pixar\nsurveys some the standard animation techniques:\n- \"Principles of Traditional Animation Applied to 3D Computer\nGraphics,\" SIGGRAPH'87, pp. 35-44.\n- See also The Illusion of Life: Disney Animation, by Frank\nThomas and Ollie Johnston.\nTraditional Animation Principles\n\n- Squash: flatten an object or character by pressure or by\nits own power\n\n- Stretch: used to increase the sense of speed and\nemphasize the squash by contrast\nExample: Squash and Stretch\nImage adapted from: Lasseter, John. \"Principles of Traditional Animation applied to 3D Computer Animation.\" ACM SIGGRAPH Computer Graphics 21, no. 4 (July 1987): 35-44.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nExample: Timing\n- Timing affects weight:\n- Light object move quickly\n- Heavier objects move slower\n\n- Timing completely changes the interpretation of the\nmotion.\n(c) ACM. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- How do we describe and generate motion of\nobjects in the scene?\n\n- Two very different contexts:\n- Production (offline)\n- Can be hardcoded, entire sequence know beforehand\n- Interactive (e.g. games, simulators)\n- Needs to react to user interaction, sequence not known\nComputer Animation\n(c) ACM. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- Types of Animation (overview)\n- Keyframing\n- Procedural\n- Physically-based\n\n- Animation Controls\n\n- Character Animation\nusing skinning/enveloping\nPlan\nCERN\n\nTypes of Animation: Keyframing\n-\nSpecify scene only at\nsome instants of time\n-\nGenerate in-betweens automatically\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Describes the motion algorithmically\n- Express animation as a function of\nsmall number of parameters\n- Example\n- a clock/watch with second, minute and hour hands\n- express the clock motions in terms of\na \"seconds\" variable\n- the clock is animated by\nchanging this variable\n- Another example: Grass in the wind,\ntree canopies, etc.\nTypes of Animation: Procedural\n\n- Assign physical properties to objects\n- Masses, forces, etc.\n- Also procedural forces (like wind)\n- Simulate physics by solving equations of motion\n- Rigid bodies, fluids, plastic deformation, etc.\n- Realistic but difficult to control\nTypes of Animation: Physically-Based\nv0\nm\ng\n\n- Physically-Based Character Animation\n- Specify keyframes, solve for physically valid motion\nthat interpolates them by \"spacetime optimization\"\n\n- Anthony C. Fang and Nancy S. Pollard, 2003. Efficient\nSynthesis of Physically Valid Human Motion, ACM\nTransactions on Graphics 22(3) 417-426, Proc. SIGGRAPH\n2003.http://graphics.cs.cmu.edu/nsp/projects/spacetime/space\ntime.html\nAnother Example\n\n- Types of Animation (overview)\n- Keyframing\n- Procedural\n- Physically-based\n\n- Animation Controls\n\n- Character Animation\nusing skinning/enveloping\nPlan\nCERN\n\n- Animation is (usually) specified using some form\nof low-dimensional controls as opposed to\nremodeling the actual geometry for each frame.\nBecause we are Lazy...\nCan you think of examples?\n\n- Animation is (usually) specified using some form\nof low-dimensional controls as opposed to\nremodeling the actual geometry for each frame.\n- Example: The joint angles (bone transformations) in a\nhierarchical character determine the pose\n- Example: A rigid motion is represented by\nchanging the object-to-world transformation\n(rotation and translation).\nBecause we are Lazy...\nCourtesy Robert C. Duvall, Duke\nUniversity. License CC BY-NC-SA.\n\n- Animation is (usually) specified using some form\nof low-dimensional controls as opposed to\nremodeling the actual geometry for each frame.\n- Example: The joint angles (bone transformations) in a\nhierarchical character determine the pose\n- Example: A rigid motion is represented by\nchanging the object-to-world transformation\n(rotation and translation).\n\"Blendshapes\" are\nkeyframes that are just\nsnapshots of the\nentire geometry.\nBecause we are Lazy...\nCourtesy Robert C. Duvall, Duke\nUniversity. License CC BY-NC-SA.\n\n- Ken Perlin's facial expression\napplet\n\n- Lower-level controls are\nmapped to semantically\nmeaningful higher-level ones\n- \"Frown/smile\" etc.\nExample of Higher-Level Controls\nhttp://mrl.nyu.edu/~perlin/experiments/facedemo/\n(c) Ken Perlin. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nBuilding 3D models and their animation controls is\na major component of every animation pipeline.\n\nBuilding the controls is called \"rigging\".\n\n- Forward kinematics\ndescribes the positions of the\nbody parts as a function of\njoint angles\n- Body parts are\nusually called \"bones\"\n- Angles are the low-\ndimensional control.\n- Inverse kinematics specifies\nconstraint locations for bones\nand solves for joint angles.\nArticulated Character Models\nCourtesy Robert C. Duvall, Duke University. License CC BY-NC-SA.\n\nSkinning Characters\n- Embed a skeleton into a\ndetailed character mesh\n\nCourtesy of Blender Foundation. License CC-BY. This content\nis excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use.\n\n- Embed a skeleton into a\ndetailed character mesh\n- Animate \"bones\"\n- Change the joint\nangles over time\n- Keyframing, procedural, etc.\n- Bind skin vertices to bones\n- Animate skeleton, skin will\nmove with it\nSkinning Characters\nCourtesy Robert C. Duvall, Duke University. License CC BY-NC-SA.\nCourtesy of Blender Foundation. License CC-BY. This content\nis excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use.\n\nMotion Capture\n- Usually uses optical markers and multiple\nhigh-speed cameras\n- Triangulate to get marker 3D position\n- (Again, structure from motion and projective\ngeometry, i.e., homogeneous coordinates)\n- Captures style, subtle nuances and realism\n- But need ability to record someone\nCourtesy Robert C. Duvall, Duke University. License CC BY-NC-SA.\n\nMotion Capture\n- Motion capture records\n3D marker positions\n- But character is\ncontrolled using\nanimation controls\nthat affect bone\ntransformations!\n- Marker positions must be\ntranslated into character\ncontrols (\"retargeting\")\n\nThis image is in the public domain. Source: Wikimedia Commons.\n\nQuestions?\n\n- Types of Animation (overview)\n- Keyframing\n- Procedural\n- Physically-based\n\n- Animation Controls\n\n- Character Animation\nusing skinning/enveloping\nPlan\nCERN\n\nSkinning/Enveloping\nCourtesy of Blender Foundation. License CC-BY. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use.\n\nSkinning\n- We know how to animate a\nbone hierarchy\n- Change the joint angles, i.e.,\nbone transformations, over\ntime (keyframing)\nCourtesy Robert C. Duvall, Duke University. License CC BY-NC-SA.\nCourtesy of Blender Foundation. License CC-BY. This content\nis excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use.\n\n- We know how to animate a\nbone hierarchy\n- Change the joint angles, i.e.,\nbone transformations, over\ntime (keyframing)\n- Embed a skeleton into a\ndetailed character mesh\n- Bind skin vertices to bones\n- Animate skeleton, skin will\nmove with it\n- But how?\nSkinning\nCourtesy of Blender Foundation. License CC-BY. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use.\n\n- Need to infer how skin deforms\nfrom bone transformations.\n- Most popular technique:\nSkeletal Subspace Deformation\n(SSD), or simply Skinning\n- Other aliases\n- vertex blending\n- matrix palette skinning\n- linear blend skinning\nSkinning/Enveloping\nThis image is in the public domain. Source: Wikimedia Commons.\n\n- Each bone has a deformation of\nthe space around it (rotation, translation)\n- What if we attach each\nvertex of the skin to a single bone?\n- Skin will be rigid, except at joints where it will\nstretch badly\n- Let's attach a vertex to many bones at once!\n- In the middle of a limb,\nthe skin points follow the bone rotation (near-\nrigidly)\n- At a joint, skin is deformed according to a\n\"weighted combination\" of the bones\nSSD / Skinning\nCourtesy Robert C. Duvall, Duke\nUniversity. License CC BY-NC-SA.\n\nExample\nJames & Twigg, Skinning Mesh Animations, 2005, used with permission from ACM, Inc.\nColored\ntriangles are\nattached to 1\nbone\nBlack triangles\nare attached to\nmore than 1\n\nNote how they\nare near joints\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nExample\nJames & Twigg, Skinning Mesh Animations.\nColored\ntriangles are\nattached to 1\nbone\nBlack triangles\nare attached to\nmore than 1\n\nNote how they\nare near joints\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nVertex Weights\n-\nWe'll assign a weight wij\nfor each vertex pi for each bone Bj.\n-\n\"How much vertex i should move with bone j\"\n-\nwij = 1 means pi is rigidly attached to bone j.\n\nVertex Weights\n-\nWe'll assign a weight wij\nfor each vertex pi for each bone Bj.\n-\n\"How much vertex i should move with bone j\"\n-\nwij = 1 means pi is rigidly attached to bone j.\nFrom Automatic Rigging and Animation of 3D Characters.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- We'll assign a weight wij\nfor each vertex pi for each bone Bj.\n- \"How much vertex i should move with bone j\"\n- wij = 1 means pi is rigidly attached to bone j.\n- Weight properties\n- Usually want weights to be non-negative\nVertex Weights\n\n- We'll assign a weight wij\nfor each vertex pi for each bone Bj.\n- \"How much vertex i should move with bone j\"\n- wij = 1 means pi is rigidly attached to bone j.\n- Weight properties\n- Usually want weights to be non-negative\n- Also, want the sum over all bones\nto be 1 for each vertex\nVertex Weights\n\n- We'll assign a weight wij\nfor each vertex pi for each bone Bj.\n- \"How much vertex i should move with bone j\"\n- wij = 1 means pi is rigidly attached to bone j.\n- We'll limit the number of bones N that can\ninfluence a single vertex\n- N=4 bones/vertex is a usual choice\n- Why?\nVertex Weights cont'd\n\n- We'll assign a weight wij\nfor each vertex pi for each bone Bj.\n- \"How much vertex i should move with bone j\"\n- wij = 1 means pi is rigidly attached to bone j.\n- We'll limit the number of bones N that can\ninfluence a single vertex\n- N=4 bones/vertex is a usual choice\n- Why? You most often don't need very many.\n- Also, storage space is an issue.\n- In practice, we'll store N (bone index j, weight wij)\npairs per vertex.\nVertex Weights cont'd\n\nHow to compute\nvertex positions?\n\n- Basic Idea 1: Transform each vertex pi with each\nbone as if it was tied to it rigidly.\nLinear Blend Skinning\n\n- Basic Idea 1: Transform each vertex pi with each\nbone as if it was tied to it rigidly.\n- Basic Idea 2: Then blend the results using the\nweights.\nLinear Blend Skinning\n\n- Basic Idea 1: Transform each vertex pi with each\nbone as if it was tied to it rigidly.\n- Basic Idea 2: Then blend the results using the\nweights.\n\" \"\nComputing Vertex Positions\np'ij is the vertex i\ntransformed using\nbone j.\nTj is the current\ntransformation of bone\nj.\np'i is the new skinned\nposition of vertex i.\n\n- Vertex p0 has\nweights\nw01=0.5,\nw02=0.5\nRest (\"bind\") pose\np0\nBone 1: T1\nBone 2: T2\nComputing Vertex Positions\n\"Skin\"\n\n- Vertex p0 has\nweights\nw01=0.5,\nw02=0.5\n- Transform by\nT'1 and T'2\nyields p'01,\np'02\nRest (\"bind\") pose\nAfter rotations\np0\np'01\np'02\nBone 1: T1\nBone 2: T2 Bone 1: T'1Bone 2: T'2\nComputing Vertex Positions\n\"Skin\"\n\n- Vertex p0 has\nweights\nw01=0.5,\nw02=0.5\n- Transform by\nT'1 and T'2\nyields p'01, p'02\n- the new position\nis p'0=\n0.5*p'1 +\n0.5*p'2\nRest (\"bind\") pose\nAfter rotations\np0\np'0\nBone 1: T1\nBone 2: T2 Bone 1: T'1Bone 2: T'2\nComputing Vertex Positions\np'01\np'02\n\"Skin\"\n\n- Vertex p0 has\nweights\nw01=0.5,\nw02=0.5\n- Transform by\nT'1 and T'2\nyields p'01, p'02\n- the new position\nis p'0=\n0.5*p'1 +\n0.5*p'2\nRest (\"bind\") pose\nAfter rotations\np0\np'0\nBone 1: T1\nBone 2: T2 Bone 1: T'1Bone 2: T'2\nComputing Vertex Positions\np'01\np'02\n\"Skin\"\n\"Skin\"\n\nSSD is Not Perfect\nAfter rotations\np0\nq0\n\nSSD is Not Perfect\nAfter rotations\np0\nq0\nQuestions?\n\n- We are given a skeleton and a\nskin mesh in a default pose\n- Called \"bind pose\"\n- Undeformed vertices pi are\ngiven in the object space of\nthe skin\n- a \"global\" coordinate system,\nno hierarchy\nBind Pose\nGNU Free Documentation License. Some rights reserved. This content is\nexcluded from our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- We are given a skeleton and a\nskin mesh in a default pose\n- Called \"bind pose\"\n- Undeformed vertices pi are\ngiven in the object space of\nthe skin\n- Previously we conveniently\nforgot that in order for\np'ij = Tj pi to make sense,\ncoordinate systems must\nmatch up.\nBind Pose\nGNU Free Documentation License. Some rights reserved. This content is\nexcluded from our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- Undeformed vertices pi are given\nin the object space of the skin\n- Tj is in local bone coordinate system\n- according to skeleton\nhierarchy\nCoordinate Systems\nGNU Free Documentation License. Some rights reserved. This content is\nexcluded from our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- In the rigging phase, we\nline the skeleton up with the\nundeformed skin.\n- This gives some \"rest pose\"\nbone transformations Bj\nfrom local bone coordinates to global\n- Bj concatenates all hierarchy matrices\nfrom node j up to the root\nBind Pose cont'd\n\n- When we animate the model,\nthe bone transformations\nTj change.\nBind Pose cont'd\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- When we animate the model,\nthe bone transformations\nTj change.\n- What is Tj? It maps from the\nlocal coordinate system of\nbone j to world space.\n- again, concatenates hierarchy matrices\nBind Pose cont'd\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- When we animate the model,\nthe bone transformations\nTj change.\n- What is Tj? It maps from the\nlocal coordinate system of\nbone j to world space.\n- To be able to deform pi according\nto Tj, we must first express pi in the local\ncoordinate system of bone j.\n- This is where the bind pose\nbone transformations Bj come in.\nBind Pose cont'd\n(c) ACM. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nBind Pose cont'd\nThis maps pi from bind pose to the local\ncoordinate system of bone j using B-1j, and\nthen to world space using Tj.\n-\nTo be able to deform pi\naccording to Tj, we must first\nexpress pi in the local\ncoordinate system of bone j.\n-\nThis is where the bind pose\nbone transformations Bj come\nin.\n\nBind Pose cont'd\nThis maps pi from bind pose to the local\ncoordinate system of bone j using B-1j, and\nthen to world space using Tj.\nWhat is Tj B-1j? It is the relative\nchange between the bone\ntransformations between the current\nand the bind pose.\n\nBind Pose cont'd\nThis maps pi from bind pose to the local\ncoordinate system of bone j using B-1j, and\nthen to world space using Tj.\nWhat is Tj B-1j? It is the relative\nchange between the bone\ntransformations between the current\nand the bind pose.\nWhat is the\ntransformation\nwhen the model\nis still in bind\npose?\n\nBind Pose cont'd\nThis maps pi from bind pose to the local\ncoordinate system of bone j using B-1j, and\nthen to world space using Tj.\nWhat is Tj B-1j? It is the relative\nchange between the bone\ntransformations between the current\nand the bind pose.\nWhat is the\ntransformation\nwhen the model\nis still in bind\npose?\nThe\nidentity!\n\nBind Pose cont'd\nThis maps pi from bind pose to the local\ncoordinate system of bone j using B-1j, and\nthen to world space using Tj.\nWhat is Tj B-1j? It is the relative\nchange between the bone\ntransformations between the current\nand the bind pose.\nWhat is the\ntransformation\nwhen the model\nis still in bind\npose?\nThe\nidentity!\nQuestions?\n\n- We then figure out the vertex\nweights wij.\n- How? Usually paint by hand!\n- We'll look at much cooler\nmethods in a while.\nBind Pose & Weights\nFrom Automatic Rigging and Animation of 3D Characters.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Do the usual forward kinematics\n- get a matrix Tj(t) per bone\n(full transformation from local to world)\n- For each skin vertex pi\nSkinning Pseudocode\n\n- Do the usual forward kinematics\n- get a matrix Tj(t) per bone\n(full transformation from local to world)\n- For each skin vertex pi\nSkinning Pseudocode\nDo you remember how to treat normals?\n\n- Do the usual forward kinematics\n- get a matrix Tj(t) per bone\n(full transformation from local to world)\n- For each skin vertex pi\n\n- Inverse transpose for normals!\nSkinning Pseudocode\n\n- Do the usual forward kinematics\n- For each skin vertex pi\n\n- Note that the weights & bind pose vertices are\nconstant over time\n- Only matrices change\n(small number of them, one per bone)\n- This enables implementation on GPU \"vertex\nshaders\"\n(little information to update for each frame)\nSkinning Pseudocode\n\n- This is what we do to get deformed positions\nHmmh...\n\n- This is what we do to get deformed positions\n\n- But wait...\nHmmh...\n\n- This is what we do to get deformed positions\n\n- But wait...\n\n- Rotations are not handled correctly (!!!)\nHmmh...\n\n-\nFrom: Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven\nDeformation, J. P. Lewis, Matt Cordner, Nickson Fong\nIndeed... Limitations\n- Rotations really need to be combined differently\n(quaternions!)\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nReal-time enveloping with rotational regression\nWang, Pulli, Popovic\nWe learn a fast model from exported examples.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Usual approach: Paint them on the skin.\n- Can also find them by optimization from example\nposes and deformed skins.\n- Wang & Phillips, SCA 2002\nFiguring out the Weights\nFrom Automatic Rigging and Animation of 3D Characters.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nSuper Cool: Automatic Rigging\n- When you just have some reference skeleton\nanimation (perhaps from motion capture) and a\nskin mesh, figure out the bone transformations\nand vertex weights!\n- Ilya Baran, Jovan Popovic: Automatic Rigging\nand Animation of 3D Characters,\nSIGGRAPH 2007\n-\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nSuper Cool: Automatic Rigging\nFrom Automatic Rigging and Animation\nof 3D Characters by Baran and Popovic,\nused with permission from ACM, Inc.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nThe Other Direction\nFrom Skinning Mesh Animations.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nMIT EECS 6.837 - Duand\nThat's All for Today!\n- Further reading\n- http://www.okino.com/\nconv/skinning.htm\n\n- Take a look at any\nvideo game -\nbasically all the\ncharacters are\nanimated using\nSSD/skinning.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Bezier Curves and Splines",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/aba8735cf2cf7c71ca5d29b46873309a_MIT6_837F12_Lec01.pdf",
      "content": "6.837 - Matusik\n6.837 Computer Graphics\n\nBezier Curves and Splines\nWojciech Matusik\nMIT CSAIL\nvectorportal.com\n\n- Anything on your mind\nconcerning Assignment 0?\n- Any questions about the course?\n\n- Assignment 1 (Curves & Surfaces)\n- Linear algebra review session\nBefore We Begin\n\n- Smooth curves in 2D\n- Useful in their own right\n- Provides basis for surface\nediting\nToday\nThis image is in the public domain\nSource:Wikimedia Commons\n\n- Polylines\n- Sequence of vertices connected\nby straight line segments\n- Useful, but not for smooth curves\n- This is the representation\nthat usually gets drawn in the end\n(a curve is converted into a polyline)\n- Smooth curves\n- How do we specify them?\n- A little harder (but not too much)\nModeling 1D Curves in 2D\n\n- A type of smooth curve\nin 2D/3D\n- Many different uses\n- 2D illustration (e.g., Adobe Illustrator)\n- Fonts (e.g., PostScript, TrueType)\n- 3D modeling\n- Animation: trajectories\n- In general: interpolation\nand approximation\nSplines\nACM (c) 1987 \"Principles of\ntraditional animation applied to 3D\ncomputer animation\"\n(c) ACM. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nDemo\n\nHow Many Dimensions?\n\nHow Many Dimensions?\nThis curve lies on the 2D plane,\nbut is itself 1D.\n\nHow Many Dimensions?\nThis curve lies on\nthe 2D plane,\nbut is itself 1D.\nYou can just as well\ndefine 1D curves in\n3D space.\n\nTwo Definitions of a Curve\n- A continuous 1D set of points in 2D (or 3D)\n- A mapping from an interval S onto the plane\n- That is, P(t) is the point of the curve at parameter t\n\n- Big differences\n- It is easy to generate points on the curve from the 2nd\n- The second definition can describe trajectories, the\nspeed at which we move on the curve\n\n- User specifies control points\n- We will interpolate the control points\nby a smooth curve\n- The curve is completely\ndetermined by the control points.\nGeneral Principle of Splines\n\nPhysical Splines\nSee http://en.wikipedia.org/wiki/Flat_spline\nCourtesy of The Antique Boat Museum.\n\nTwo Application Scenarios\n- Approximation/interpolation\n- We have \"data points\", how can we interpolate?\n- Important in many applications\n\n- User interface/modeling\n- What is an easy way to specify\n\na smooth curve?\n- Our main perspective today.\nImage courtesy of SaphireS on Wikimedia Commons. License: CC-BY-\nSA. This content is excluded from our Creative Commons license. For\nmore information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\n- Specified by a few control points\n- Good for UI\n- Good for storage\n\n- Results in a smooth parametric curve P(t)\n- Just means that we specify x(t) and y(t)\n- In practice: low-order polynomials, chained together\n- Convenient for animation, where t is time\n- Convenient for tessellation because we can discretize\nt and approximate the curve with a polyline\nSplines\n\n6.837 - Durand\nTessellation\n- It is easy to rasterize mathematical line segments\ninto pixels\n- OpenGL and the graphics hardware can do it for you\n- But polynomials and other parametric functions\nare harder\nImage courtesy of Phrood on Wikimedia Commons. License: CC-BY-SA.This content is excluded from our\nCreative Commons license. For moreinformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nTessellation\nt0\nt1\nt2\ntn\n\nTo display P(t),\ndiscretize it at discrete ts\n\nTessellation\nt0\nt1\nt2\ntn\nIt's clear that adding\nmore points will get\nus closer to the\ncurve.\n\nTessellation\nt0\nt1\nt2\ntn\nIt's clear that adding\nmore points will get\nus closer to the\ncurve.\n\n- Interpolation\n- Goes through all specified points\n- Sounds more logical\n\n- Approximation\n- Does not go through all points\nInterpolation vs. Approximation\nInterpolation\nApproximation\n\n- Interpolation\n- Goes through all specified points\n- Sounds more logical\n- But can be more unstable\n- Approximation\n- Does not go through all points\n- Turns out to be convenient\n\n- We will do something\nin between.\nInterpolation vs. Approximation\nInterpolation\nApproximation\n\nQuestions?\n\n- User specifies 4 control points P1 ... P4\n- Curve goes through (interpolates) the ends P1, P4\n- Approximates the two other ones\n- Cubic polynomial\nCubic Bezier Curve\n\nCubic Bezier Curve\nThat is,\n- P(t) = (1-t)3\nP1\n+ 3t(1-t)2 P2\n\n+ 3t2(1-t) P3\n+ t3\n\nP4\n\nCubic Bezier Curve\nVerify what happens\nfor t=0 and t=1\n- P(t) = (1-t)3\nP1\n+ 3t(1-t)2 P2\n\n+ 3t2(1-t) P3\n+ t3\n\nP4\n\n- 4 control points\n- Curve passes through first & last control point\nCubic Bezier Curve\nCourtesy of Seth Teller.\nUsed with permission.\n\n- 4 control points\n- Curve passes through first & last control point\n- Curve is tangent at P1 to (P1-P2) and at P4 to (P4-P3)\nCubic Bezier Curve\nA Bezier curve is\nbounded by the\nconvex hull of its\ncontrol points.\n\nQuestions?\n\n- Explanation 1:\n- Magic!\n- Explanation 2:\n- These are smart weights that describe the influence of\neach control point\n- Explanation 3:\n- It is a linear combination of basis polynomials.\nWhy Does the Formula Work?\n\n- P(t) is a weighted\ncombination of the 4\ncontrol points with\nweights:\n- B1(t)=(1-t)3\n- B2(t)=3t(1-t)2\n- B3(t)=3t2(1-t)\n- B4(t)=t3\n- First, P1 is the most\ninfluential point,\nthen P2, P3, and P4\nWeights\nP(t) = (1-t)3 P1\n+\n3t(1-t)2 P2\n+\n3t2(1-t)\nP3\n+\nt3\n\nP4\n\n- P2 and P3 never have full\ninfluence\n- Not interpolated!\nWeights\nP(t) = (1-t)3 P1\n+\n3t(1-t)2 P2\n+\n3t2(1-t)\nP3\n+\nt3\n\nP4\n\nQuestions?\n\n- Explanation 1:\n- Magic!\n- Explanation 2:\n- These are smart weights that describe the influence of\neach control point\n- Explanation 3:\n- It is a linear combination of basis polynomials.\n- The opposite perspective:\ncontrol points are the weights of polynomials!!!\n\nWhy Does the Formula Work?\n\n- Understand relationships between types of splines\n- Conversion\n- Express what happens when a spline curve is\ntransformed by an affine transform\n(rotation, translation, etc.)\n- Cool simple example of non-trivial vector space\n- Important to understand for advanced methods\nsuch as finite elements\nWhy Study Splines as Vector Space?\n\n- In 3D, each vector has three components x, y, z\n- But geometrically, each vector is actually the sum\n\n- i, j, k are basis vectors\n\n- Vector addition: just add components\n- Scalar multiplication: just multiply components\nUsual Vector Spaces\ni\nj\nk\n\n- Polynomials\n- Can be added: just add the coefficients\n\n- Can be multiplied by a scalar: multiply the\ncoefficients\nPolynomials as a Vector Space\n\n-\nPolynomials\n\n- In the polynomial vector space, {1, t, ..., tn} are\nthe basis vectors, a0, a1, ..., an are the\ncomponents\nPolynomials as a Vector Space\n\nQuestions?\n\n- Closed under addition & scalar multiplication\n- Means the result is still a cubic polynomial (verify!)\n- Cubic polynomials also compose a vector space\n- A 4D subspace of the full space of polynomials\n- The x and y coordinates of cubic Bezier curves\nbelong to this subspace as functions of t.\nSubset of Polynomials: Cubic\n\nMore precisely:\nWhat's a basis?\n\n- A set of \"atomic\" vectors\n- Called basis vectors\n- Linear combinations of basis vectors span the space\n- i.e. any cubic polynomial is a sum of those basis cubics\n- Linearly independent\n- Means that no basis vector can be obtained from the\nothers by linear combination\n- Example: i, j, i+j is not a basis (missing k direction!)\nBasis for Cubic Polynomials\ni\nj\nk\nIn 3D\n\n- Any cubic polynomial is a\nlinear combination of these:\na0+a1t+a2t2+a3t3 = a0*1+a1*t+a2*t2+a3*t3\n\n- They are linearly independent\n- Means you cannot write any of the four monomials as\na linear combination of the others. (You can try.)\nCanonical Basis for Cubics\nt\nt2\nt3\n\n- For example:\n- {1, 1+t, 1+t+t2, 1+t-t2+t3}\n- {t3, t3+t2, t3+t, t3+1}\n\n- These can all be obtained from\nby linear combination\n- Infinite number of possibilities, just like you have\nan infinite number of bases to span R2\nDifferent Basis\n2D examples\n\n- For example:\n1, 1+t, 1+t+t2, 1+t-t2+t3\nt3, t3+t2, t3+t, t3+1\nMatrix-Vector Notation\nChange-of-basis\nmatrix\n\"Canonical\"\nmonomial\nbasis\nThese\nrelationships\nhold for each\nvalue of t\n\n- For example:\n1, 1+t, 1+t+t2, 1+t-t2+t3\nt3, t3+t2, t3+t, t3+1\nMatrix-Vector Notation\nChange-of-basis\nmatrix\n\"Canonical\"\nmonomial\nbasis\nNot any matrix will do!\nIf it's singular, the basis\nset will be linearly\ndependent, i.e.,\nredundant and\nincomplete.\n\n- For Bezier curves, the\nbasis polynomials/vectors\nare Bernstein polynomials\n\n- For cubic Bezier curve:\nB1(t)=(1-t)3 B2(t)=3t(1-t)2\nB3(t)=3t2(1-t) B4(t)=t3\n(careful with indices, many authors start at 0)\n- Defined for any degree\nBernstein Polynomials\n\nProperties of Bernstein Polynomials\n- for all 0 t 1\n- Sum to 1 for every t\n- called partition of unity\n- These two together are the\nreason why Bezier curves\nlie within convex hull\n- B1(0) =1\n- Bezier curve interpolates P1\n- B4(1) =1\n- Bezier curve interpolates P4\n\n- P(t) = P1B1(t) + P2B2(t) + P3B3(t) + P4B4(t)\n- Pi are 2D points (xi, yi)\n- P(t) is a linear combination of the control points\nwith weights equal to Bernstein polynomials at t\n- But at the same time, the control points\n(P1, P2, P3, P4) are the \"coordinates\" of the\ncurve in the Bernstein basis\n- In this sense, specifying a Bezier curve with control\npoints is exactly like specifying a 2D point with its x\nand y coordinates.\nBezier Curves in Bernstein Basis\n\n- The plane where the curve lies, a 2D vector space\n- The space of cubic polynomials, a 4D space\n- Don't be confused!\n- The 2D control points can be replaced by 3D\npoints - this yields space curves.\n- The math stays the same, just add z(t).\n- The cubic basis can be extended to higher-order\npolynomials\n- Higher-dimensional vector space\n- More control points\nTwo Different Vector Spaces!!!\n\nQuestions?\n\n-\nHow do we go from Bernstein basis\nto the canonical monomial basis\n1, t, t2, t3 and back?\n-\nWith a matrix!\nChange of Basis\nNew basis vectors\n- B1(t)=(1-t)3\n- B2(t)=3t(1-t)2\n- B3(t)=3t2(1-t)\n- B4(t)=t3\n\nCubic Bernstein:\n- B1(t)=(1-t)3\n- B2(t)=3t(1-t)2\n- B3(t)=3t2(1-t)\n- B4(t)=t3\nHow You Get the Matrix\nExpand these out\nand collect powers of t.\nThe coefficients are the entries\nin the matrix B!\n\n- Given B1...B4, how to get back\nto canonical 1, t, t2, t3 ?\nChange of Basis, Other Direction\n\n- Given B1...B4, how to get back\nto canonical 1, t, t2, t3 ?\nChange of Basis, Other Direction\nThat's right, with the inverse matrix!\n\n- Cubic polynomials form a 4D vector space.\n- Bernstein basis is canonical for Bezier.\n- Can be seen as influence function of data points\n- Or data points are coordinates of the curve in the\nBernstein basis\n- We can change between basis with matrices.\nRecap\n\nQuestions?\n\nMore Matrix-Vector Notation\nmatrix of\ncontrol points (2 x 4)\nBernstein polynomials\n(4x1 vector)\npoint on curve\n(2x1 vector)\n\nFlashback\n\nCubic Bezier in Matrix Notation\npoint on curve\n(2x1 vector)\n\"Geometry matrix\"\nof control points P1..P4\n(2 x 4)\n\"Spline matrix\"\n(Bernstein)\nCanonical\nmonomial basis\n\n- Geometry: control points coordinates assembled\ninto a matrix (P1, P2, ..., Pn+1)\n- Spline matrix: defines the type of spline\n- Bernstein for Bezier\n- Power basis: the monomials (1, t, ..., tn)\n- Advantage of general formulation\n- Compact expression\n- Easy to convert between types of splines\n- Dimensionality (plane or space) does not really matter\nGeneral Spline Formulation\n\nQuestions?\n\n- What if you want more control?\nA Cubic Only Gets You So Far\n\n- > 4 control points\n- Bernstein Polynomials as the basis functions\n- For polynomial of order n, the ith basis function is\n\n- Every control point affects the entire curve\n- Not simply a local effect\n- More difficult to control for modeling\n- You will not need this in this class\nHigher-Order Bezier Curves\nCourtesy of Seth Teller. Used with\npermission.\n\n- Can we split a Bezier curve in the middle into\ntwo Bezier curves?\n- This is useful for adding detail\n- It avoids using nasty higher-order curves\nSubdivision of a Bezier Curve\n?\n\n- Can we split a Bezier curve in the middle into\ntwo Bezier curves?\n- The resulting curves are again a cubic\n(Why? A cubic in t is also a cubic in 2t)\n- Hence it must be representable using the Bernstein\nbasis. So yes, we can!\nSubdivision of a Bezier Curve\n?\nt=1\nt=0.5\nt=0\nt2=2t-0.5\nt1=2t\ncubic\n\n- Take the middle point of each of the 3 segments\n- Construct the two segments joining them\n- Take the middle of those two new segments\n- Join them\n- Take the middle point P'''\nDe Casteljau Construction\nP'1\nP'2\nP''1\nP'''\nP'3\nP''2\n\n-\nThe two new curves are defined by\n-\nP1, P'1, P''1, and P'''\n-\nP''', P''2, P'3, and P4\n-\nTogether they exactly replicate the original\ncurve!\n-\nOriginally 4 control points, now 7 (more control)\nResult of Split in Middle\nP'1\nP'2\nP''1\nP'''\nP'3\nP''2\nP4\nP1\n\n-\nDo we actually get the middle point?\n-\nB1(t)=(1-t)3\n-\nB2(t)=3t(1-t)2\n-\nB3(t)=3t2(1-t)\n-\nB4(t)=t3\nSanity Check\n✔\nP'1\nP'2\nP''1\nP'''\nP'3\nP''2\n\n- Actually works to construct a point at any t, not just\n0.5\n- Just subdivide the segments with ratio (1-t), t\n(not in the middle)\nDe Casteljau Construction\nt\nt\nt\nt\nt\nt\n\n- Bezier curves: piecewise polynomials\n- Bernstein polynomials\n- Linear combination of basis functions\n- Basis: control points\n\nweights: polynomials\n- Basis: polynomials\nweights: control points\n- Subdivision by de Casteljau algorithm\n- All linear, matrix algebra\n\nRecap\n\n- Bezier curves: piecewise polynomials\n- Bernstein polynomials\n- Linear combination of basis functions\n- Basis: control points\n\nweights: polynomials\n- Basis: polynomials\nweights: control points\n- Subdivision by de Casteljau algorithm\n- All linear, matrix algebra\n\nRecap\n\nvectorportal.com\nThat's All for Today, Folks\n- Further reading\n- Buss, Chapters 7 and 8\n\n- Fun stuff to know about function/vector spaces\n- http://en.wikipedia.org/wiki/Vector_space\n- http://en.wikipedia.org/wiki/Functional_analysis\n- http://en.wikipedia.org/wiki/Function_space\n\n- Inkscape is an open source vector drawing\nprogram for Mac/Windows. Try it out!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Collision Detection and Response",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/53c5b275d4fb9eba7968e6e80047f2d4_MIT6_837F12_Lec10.pdf",
      "content": "MIT EECS 6.837 - Durand\nMIT EECS 6.837 Computer Graphics\nCollision Detection\nand Response\nPhilippe Halsman: Dali Atomicus\nMIT EECS 6.837 - Matusik\nThis image is in the public domain. Source:Wikimedia Commons.\n\n- Detection\n- Response\n- Overshooting problem\n(when we enter the solid)\nCollisions\n\nCollision Response for Particles\nN\nv\n\nCollision Response for Particles\nN\nv\nvn\nvt\nv=vn+vt\nnormal component\ntangential component\n\n- Tangential velocity vt\noften unchanged\n- Normal velocity vn reflects:\n\n- Coefficient of restitution ε\n\n- When ε = 1, mirror reflection\nCollision Response for Particles\nN\nv\nvn\nvt\nN\nv\nvnew\nN\nv\nvnew\nε=1\nε<1\n\n- Usually, we detect collision when it is too late:\nwe are already inside\nCollisions - Overshooting\nxi\nxi+1\n\n- Usually, we detect collision when it is too late:\nwe are already inside\n- Solution: Back up\n- Compute intersection point\n- Ray-object intersection!\n- Compute response there\n- Advance for remaining\nfractional time step\nCollisions - Overshooting\nbacktracking\nxi\nxi+1\n\n- Usually, we detect collision when it is too late:\nwe are already inside\n- Solution: Back up\n- Compute intersection point\n- Ray-object intersection!\n- Compute response there\n- Advance for remaining\nfractional time step\n- Other solution:\nQuick and dirty hack\n- Just project back to object closest point\nCollisions - Overshooting\nfixing\nbacktracking\nxi\nxi+1\n\n- Pong: ε =?\n- http://www.youtube.com/watch?v=sWY0Q_lMFfw\n- http://www.xnet.se/javaTest/jPong/jPong.html\nQuestions?\n\nAnimation removed due to copyright restrictions.\nImage courtesy of Chris Rand on Wikimedia Commons. License: CC-BY-SA.\nThis content is excluded from our Creative Commons license. For more info\nrmation, see http://ocw.mit.edu/help/faq-fair-use/.\nThis image is in the public domain.\nSource:Wikimedia Commons.\n\n- Imagine we have n objects. Can we test all pairwise\nintersections?\n- Quadratic cost O(n2)!\n\n- Simple optimization: separate static objects\n- But still O(static × dynamic+ dynamic2)\nCollision Detection in Big Scenes\n\n- Use simpler conservative proxies\n(e.g. bounding spheres)\n\n- Recursive (hierarchical) test\n- Spend time only for parts of the scene that are close\n\n- Many different versions, we will cover only one\n\nHierarchical Collision Detection\n\n- Place spheres around objects\n- If spheres do not intersect, neither do the objects!\n- Sphere-sphere collision test is easy.\nBounding Spheres\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\n- Two spheres, centers C1 and C2, radii r1 and r2\n- Intersect only if ||C1C2||<r1+r2\nSphere-Sphere Collision Test\nC1\nC2\nr1\nr2\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\n- Hierarchy of bounding spheres\n- Organized in a tree\n- Recursive test with early pruning\nHierarchical Collision Test\nRoot encloses\nwhole object\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- http://isg.cs.tcd.ie/spheretree/\nExamples of Hierarchy\n(c) Gareth Bradshaw. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nboolean intersect(node1, node2)\n// no overlap? ==> no intersection!\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\n\n// recurse down the larger of the two nodes\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\n\n// no intersection in the subtrees? ==> no intersection!\nreturn false\nPseudocode (simplistic version)\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\nnode 1\nnode 2\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\nif (node1->radius()>node2->radius())\nfor each child c of node1\nif intersect(c, node2) return true\nelse\nfor each child c f node2\nif intersect(c, node1) return true\nreturn false\n(c) Gareth Bradshaw. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\n\nboolean intersect(node1, node2)\nif (!overlap(node1->sphere, node2->sphere)\nreturn false\n\n// if there is nowhere to go, test everything\nif (node1->isLeaf() && node2->isLeaf())\nperform full test between all primitives within nodes\n\n// otherwise go down the tree in the non-leaf path\nif ( !node2->isLeaf() && !node1->isLeaf() )\n// pick the larger node to subdivide, then recurse\nelse\n// recurse down the node that is not a leaf\n\nreturn false\nPseudocode (with leaf case)\n\n- Axis Aligned Bounding Boxes\n- \"R-Trees\"\n\n- Oriented bounding boxes\n- S. Gottschalk, M. Lin, and D. Manocha. \"OBBTree: A hierarchical Structure\nfor rapid interference detection,\" Proc. Siggraph 96. ACM Press, 1996\n\n- Binary space partitioning trees; kd-trees\nOther Options\n\n- http://www.youtube.com/watch?v=b_cGXtc-nMg\n- http://www.youtube.com/watch?v=nFd9BIcpHX4&f\neature=related\n- http://www.youtube.com/watch?v=2SXixK7yCGU\nQuestions?\n\n- Top down\n- Divide and conquer\n\n- Bottom up\n- Cluster nearby objects\n\n- Incremental\n- Add objects one by one, binary-tree style.\nHierarchy Construction\n\n- Trivial given center C\n- radius = maxi ||C-Pi||\nBounding Sphere of a Set of Points\nC\n(c) Gareth Bradshaw. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Using axis-aligned bounding box\n- center=\n((xmin+xmax)/2, (ymin+ymax)/2, (zmin, zmax)/2)\n- Better than the average of the vertices because does not\nsuffer from non-uniform tessellation\nBounding Sphere of a Set of Points\n(c) Gareth Bradshaw. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Using axis-aligned bounding box\n- center=\n((xmin+xmax)/2, (ymin+ymax)/2, (zmin, zmax)/2)\n- Better than the average of the vertices because does not\nsuffer from non-uniform tessellation\nBounding Sphere of a Set of Points\nQuestions?\n(c) Gareth Bradshaw. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Take longest scene dimension\n- Cut in two in the middle\n- assign each object or triangle to one side\n- build sphere around it\nTop-Down Construction\n(c) Oscar Meruvia-Pastor, Daniel Rypl. All rights reserved. This content is\nexcluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Patrick Laug. Used with permission.\nThis image is in the public domain.\nSource: Wikimedia Commons.\n(c) Sara McMains. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\n(c) Gareth Bradshaw. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n3\"%%\n)% %%\n%\"*%\"%\n%/<%\n(c) Sara McMains. All rights reserved. This content\nTop-Down Construction - Recurse\nis excluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n(c) Gareth Bradshaw. All rights reserved.\nThis content is excluded from our Creative\nCommons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\nThis image is in the public domain.\nSource: Wikimedia Commons.\n(c) Oscar Meruvia-Pastor, Daniel Rypl. All rightsreserved. This content is\nexcluded from ourCreative Commons license. For more information,\nCourtesy of Patrick Laug. Used with permission.\nsee http://ocw.mit.edu/help/faq-fair-use/.\n\n3\"%%\n)% %%\n%\"*%\"%\n%/<%\nTop-Down Construction - Recurse\nQuestions?\n(c) Sara McMains. All rights reserved. This content\nis excluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n(c) Gareth Bradshaw. All rights reserved.\nThis content is excluded from our Creative\nCommons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\nThis image is in the public domain.\nSource: Wikimedia Commons.\n(c) Oscar Meruvia-Pastor, Daniel Rypl. All rightsreserved. This content is\nexcluded from ourCreative Commons license. For more information,\nCourtesy of Patrick Laug. Used with permission.\nsee http://ocw.mit.edu/help/faq-fair-use/.\n\nReference\nImage of the cover of the book, \"Real Time Collision Detection,\" by Christer Ericson has been removed\ndue to copyright restrictions.\n\n- A cloth has many points of contact\n- Stays in contact\n- Requires\n- Efficient collision detection\n- Efficient numerical treatment (stability)\nThe Cloth Collision Problem\nImage from Bridson et al.\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nRobust Treatment of Simultaneous Collisions\nDavid Harmon, Etienne Vouga, Rasmus Tamstorf, Eitan Grinspun\nAnimation removed due to copyright restrictions.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Color",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/2d7476b0ce37e364ac728cd4c8ff41f4_MIT6_837F12_Lec05.pdf",
      "content": "Color\nWojciech Matusik MIT EECS\nMany slides courtesy of Victor Ostromoukhov, Leonard McMillan, Bill Freeman, Fredo Durand\nImage courtesy of Chevre on Wikimedia Commons. License: CC-BY-SA. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nDoes color puzzle you?\n\nAnswer\n- It's all linear algebra\n\n- Spectra\n- Cones and spectral response\n- Color blindness and metamers\n- Color matching\n- Color spaces\nPlan\n\nColor\nImage courtesy of Zatonyi Sandor, (ifj.) Fizped on Wikimedia Commons.\nLicense: CC-BY-SA. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nSpectrum\nLight is a wave\nVisible: between 450 and 700nm\n\nSpectrum\nLight is characterized by its\nspectrum:\nthe amount of energy at each\nwavelength\nThis is a full distribution:\none value per wavelength\n(infinite number of values)\n\nLight-Matter Interaction\nWhere spectra come from:\n- light source spectrum\n- object reflectance (aka spectral albedo)\nget multiplied wavelength by wavelength\n\nThere are different physical processes that explain\nthis multiplication\ne.g. absorption, interferences\n.*\n=\nFoundations of Vision, by Brian Wandell, Sinauer Assoc., 1995\n(c) Sinauer Associates, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nSpectrum demo\n- Diffraction grating:\n- shifts light as a function of\nwavelength\n- Allows you to see spectra\n- In particular, using a slit light\nsource, we get a nice band\nshowing the spectrum\n- See the effect of filters\n- See different light source\nspectra\nImage courtesy of Cmglee on Wikimedia Commons. License:\nCC-BY-SA. This content is excluded from our Creative\nCommons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\nThis image is in the public domain. Source: Wikimedia Commons.\n\nSo far, physical side of colors: spectra\nan infinite number of values\n(one per wavelength)\nQuestions?\n(c) Sinauer Associates, Inc. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Spectra\n- Cones and spectral response\n- Color blindness and metamers\n- Color matching\n- Color spaces\nPlan\n\nWhat is Color?\nLight\n\nObject\n\nObserver\n\nWhat is Color?\nIllumination\nReflectance\nStimulus\nCone\nresponses\n\nWhat is Color?\nLight\nIllumination\nObject\nReflectance\nFinal stimulus\nSpectral\nSensibility\nof the\nL, M and S\nCones\nS\nM\nL\nThen the cones in the eye interpret the stimulus\n\nCones\n- We focus on low-level aspects of color\n- Cones and early processing in the retina\n- We won't talk about rods (night vision)\nSpectral\nSensibility\nof the\nL, M and S\nCones\nS\nM\nL\nThis image is in the public domain. Source: Wikimedia Commons.\nImage courtesy of Ivo Kruusamagi on Wikimedia\nCommons. License: CC-BY-SA. This content is\nexcluded from our Creative Commons license. For\nmore information, see http://ocw.mit.edu/help/\nfaq-fair-use/.\n\nSummary (and time for questions)\n- Spectrum: infinite number of values\n- can be multiplied\n- can be added\n- Light spectrum multiplied by reflectance\nspectrum\n- spectrum depends on illuminant\n- Human visual system is complicated\n\n- Short, Medium and Long wavelength\n- Response for a cone\n= ∫ λ stimulus(λ) * response(λ) dλ\nCone spectral sensitivity\n\nCone response\nStart from infinite\nnumber of values\n(one per\nwavelength)\n\nEnd up with 3\nvalues (one per\ncone type)\nCone responses\nStimulus\nMultiply wavelength by wavelength\nIntegrate\n1 number\n1 number\n1 number\n\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFor matrix lovers\n- Spectrum: big long vector size N where N=inf\n- Cone response: 3xN matrix of individual\nresponses\nS\nL\nM\ncone spectral\nresponse\nkind of RGB\nobserved\nspectrum\n\nBig picture\n- It's all linear!\nLight\nreflectance\nCone responses\nStimulus\nmultiply\nMultiply wavelength by wavelength\nIntegrate\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nBig picture\n- It's all linear!\n- multiply\n- add\n- But\n- non-orthogonal\nbasis\n- infinite\ndimension\n- light must be\npositive\n- Depends on light\nsource\nLight\nreflectance\nCone responses\nStimulus\nmultiply\nMultiply wavelength by wavelength\nIntegrate\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\nreflectance\nCone responses\nStimulus\nmultiply\nMultiply wavelength by wavelength\nIntegrate\n\n- Different wavelength, different intensity\n- Same response\nA cone does not \"see\" colors\n\n- Different wavelength, different intensity\n- But different response for different cones\nResponse comparison\n\n- Colors as relative responses\n(ratios)\nvon Helmholtz 1859: Trichromatic theory\nViolet\nBlue\nGreen\nYellow\nOrange\nRed\nShort wavelength receptors\nMedium wavelength receptors\nLong wavelength receptors\nReceptor Responses\nWavelengths (nm)\nViolet\nBlue\nGreen\nYellow\nOrange\nRed\n\nQuestions?\n\n- Spectra\n- Cones and spectral response\n- Color blindness and metamers\n- Color matching\n- Color spaces\nPlan\n\n- Classical case: 1 type of cone is missing (e.g. red)\n- Makes it impossible to distinguish some spectra\nColor blindness\ndifferentiated\nSame responses\n\n- Dalton\n- 8% male, 0.6% female\n- Genetic\n- Dichromate (2% male)\n- One type of cone missing\n- L (protanope), M (deuteranope),\nS (tritanope)\n- Anomalous trichromat\n- Shifted sensitivity\nColor blindness - more general\n\nColor blindness test\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\nImage courtesy of Eddau processed File: Ishihara 2.svg by User:Sakurambo, with\nhttp://www.vischeck.com/vischeck/vischeckURL.php on Wikimedia Commons.\nLicense: CC-BY-SA. This content is excluded from our Creative Commons license.\nFor more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Maze in subtle intensity contrast\n- Visible only to color blinds\n- Color contrast overrides intensity otherwise\nColor blindness test\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n- Links:\n- Vischeck shows you what an image looks like to\nsomeone who is colorblind.\n- http://www.vischeck.com/vischeck/\n- Daltonize, changes the red/green variation to\nbrightness and blue/yellow variations.\n- http://www.vischeck.com/dalton\n\n- http://www.vischeck.com/daltonize/runDaltonize.php\n\nMetamers\n- We are all color blind!\n- These two different\nspectra elicit the same\ncone responses\n- Called metamers\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nGood news: color reproduction\n- 3 primaries are (to a first order) enough to\nreproduce all colors\nImage courtesy of Martin Apolin on Wikimedia Commons. License: CC-BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nRecap\n- Spectrum: infinite number of values\n- projected according to cone spectral response\n=> 3 values\n- metamers: spectra that induce the same response\n(physically different but look the same)\n\n- Questions?\n\n- Metamers under a given light source\n- May not be metamers under a different lamp\nMetamerism & light source\n\nIlluminant metamerism example\n-\nTwo grey patches in Billmeyer & Saltzman's book\nlook the same under daylight\nbut different under neon or halogen\n(& my camera agrees ;-)\nDaylight\nScan (neon)\nHallogen\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nBad consequence: cloth matching\n- Clothes appear to match in store (e.g. under neon)\n- Don't match outdoor\n\nRecap\n- Spectrum is an infinity of numbers\n- Projected to 3D cone-response space\n- for each cone, multiply per wavelength and integrate\n- a.k.a. dot product\n- Metamerism: infinite-D points projected to the\nsame 3D point\n(different spectrum, same perceived color)\n- affected by illuminant\n- enables color reproduction with only 3 primaries\n\nQuestions?\n\nAnalysis & Synthesis\n- Now let's switch to technology\n- We want to measure & reproduce color\nas seen by humans\n- No need for full spectrum\n- Only need to match up to metamerism\n\nAnalysis & Synthesis\n- Focus on additive color synthesis\n- We'll use 3 primaries (e.g. red green and blue) to\nmatch all colors\n\n- What should those primaries be?\n- How do we tell the amount of each primary\nneeded to reproduce a given target color?\nImage courtesy of Pengo on Wikimedia Commons. License: CC-BY-SA.\nThis content is excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nTricky thing with spectra & color:\n- Spectrum for the stimulus / synthesis\n- Light, monitor, reflectance\n- Response curve for receptor /analysis\n- Cones, camera, scanner\nThey are usually not the same\nThere are good reasons for this\nWarning\nThis image is in the public domain. Source:\nhttp://openclipart.org/detail/34051/digicam-\nby-thesaurus.\nImage courtesy of Pengo on Wikimedia Commons. License: CC-BY-SA.\nThis content is excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Take a given stimulus and the corresponding\nresponses s, m, l (here 0.5, 0, 0)\nAdditive Synthesis - wrong way\n\nAdditive Synthesis - wrong way\n- Use it to scale the cone spectra (here 0.5 * S)\n- You don't get the same cone response!\n(here 0.5, 0.1, 0.1)\n\nWhat's going on?\n- The three cone responses are not orthogonal\n- i.e. they overlap and \"pollute\" each other\n\n- Spectra are infinite-dimensional\n- Only positive values are allowed\n- Cones are non-orthogonal/overlap\nFundamental problems\n\n- Physical color\n- Spectrum\n- multiplication of light & reflectance spectrum\n- Perceptual color\n- Cone spectral response: 3 numbers\n- Metamers: different spectrum, same responses\n- Color matching, enables color reproduction with 3 primaries\n- Fundamental difficulty\n- Spectra are infinite-dimensional (full function)\n- Projected to only 3 types of cones\n- Cone responses overlap / they are non-orthogonal\n- Means different primaries for analysis and synthesis\n- Negative numbers are not physical\nSummary\n\nQuestions?\n\n- We need a principled color space\n- Many possible definition\n- Including cone response (LMS)\n- Unfortunately not really used,\n(because not known at the time)\n\n- The good news is that color vision is linear and\n3-dimensional, so any new color space based on\ncolor matching can be obtained using 3x3 matrix\n- But there are also non-linear color spaces\n(e.g. Hue Saturation Value, Lab)\nStandard color spaces\n\n- Most standard color space: CIE XYZ\n- LMS and the various flavor of RGB are just\nlinear transformations of the XYZ basis\n- 3x3 matrices\nOverview\n\nWhy not measure cone sensitivity?\n- Less directly measurable\n- electrode in photoreceptor?\n- not available when color spaces were defined\n- Most directly available measurement:\n- notion of metamers & color matching\n- directly in terms of color reproduction:\ngiven an input color,\nhow to reproduce it with 3 primary colors?\n- Commission Internationale de l'Eclairage\n(International Lighting Commission)\n- Circa 1920\nSpectral\nSensibility\nof the\nL, M and S\nCones\nS\nM\nL\n\n- Choose 3 synthesis primaries\n- Seek to match any monochromatic light (400 to 700nm)\n- Record the 3 values for each wavelength\n- By linearity, this tells us how to match any light\nCIE color matching\n\n- Primaries (synthesis) at 435.8, 546.1 and 700nm\n- Chosen for robust reproduction, good separation in red-green\n- Don't worry, we'll be able to convert it to any other set of\nprimaries (Linear algebra to the rescue!)\n- Resulting 3 numbers for each input wavelength are\ncalled tristimulus values\nCIE color matching\n\nNow, our interactive\nfeature!\nYou are...\nTHE LAB RAT\n\n- Some colors cannot be produced using only\npositively weighted primaries\n- Solution: add light on the other side!\nColor Matching Problem\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Meaning of these curves: a monochromatic\nwavelength λ can be reproduced with\nb(λ) amount of the 435.8nm primary,\n+g(λ) amount of the 546.1 primary,\n+r(λ) amount of the 700 nm primary\n\n- This fully specifies the color\nperceived by a human\n\n- Careful: this is not your usual rgb\nCIE color matching\n\n- Meaning of these curves: a monochromatic\nwavelength λ can be reproduced with\nb(λ) amount of the 435.8nm primary,\n+g(λ) amount of the 546.1 primary,\n+r(λ) amount of the 700 nm primary\n\n- This fully specifies the color\nperceived by a human\n- However, note that one of\nthe responses can be\nnegative\nThose colors cannot be\nreproduced by those 3 primaries.\nCIE color matching\n\n- If I have a given spectrum X\n- I compute its response to the 3 matching curves\n(multiply and integrate)\n- I use these 3 responses to\nscale my 3 primaries\n(435.8, 546.1 and 700nm)\n- I get a metamer of X\n(perfect color reproduction)\nCIE color matching: what does it mean?\n\nRelation to cone curves\n- Project to the same subspace\n- b, g, and r are linear combinations of S, M and L\n- Related by 3x3 matrix.\n- Unfortunately unknown at that time. This would\nhave made life a lot easier!\n\nRecap\n- Spectra : infinite dimensional\n- Cones: 3 spectral responses\n- Metamers: spectra that look the same\n(same projection onto cone responses)\n- CIE measured color response:\n- chose 3 primaries\n- tristimulus curves to reproduce any wavelength\n\n- Questions?\n\nHow to build a measurement device?\n- Idea:\n- Start with light sensor sensitive to all wavelength\n- Use three filters with spectra b, r, g\n- measure 3 numbers\n- This is pretty much what the eyes do!\n\nCIE's problem\n- Idea:\n- Start with light sensor sensitive to all wavelength\n- Use three filters with spectra b, r, g\n- measure 3 numbers\n- But for those primaries, we need negative spectra\n\nCIE's problem\n- Obvious solution:\nuse cone response!\n- but unknown at the time\n- =>new set of tristimulus curves\n- linear combinations of b, g, r\n- pretty much add enough b and g\nuntil r is positive\n\nChromaticity diagrams\n- 3D space are tough to visualize\n- Usually project to 2D for clarity\n- Chromaticity diagram:\n- normalize against X + Y + Z:\n\n- Perspective projection to plane\nX+Y+Z=1\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- THE standard for color specification\n- Lots of legacy decision - I wish it were LMS\n- Based on color matching\n- 3 monochromatic primaries\n- Subjects matched every wavelength\n- Tricks to avoid negative numbers\n- These 3 values \"measure\"\nor describe a perceived color.\nCIE XYZ -recap\n\nQuestions?\n\nOther primaries\n- We want to use a new set of primaries\n- e.g. the spectra of R, G & B in a projector or monitor\n- By linearity of color matching,\ncan be obtained from XYZ by a 3x3 matrix\none example RGB space\n\nOther primaries\n- We want to use a new set of primaries\n- e.g. the spectra of R, G & B in a projector or monitor\n- By linearity of color matching,\ncan be obtained from XYZ by a 3x3 matrix\n- This matrix tells us how to match the 3 primary\nspectra from XYZ using the new 3 primaries\none example RGB space\n\nXYZ to RGB & back\n- e.g.\nhttp://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n- sRGB to XYZ XYZ to sRGB\n\n- Adobe RGB to XYZ XYZ to Adobe RGB\n\n- NTSC RGB to XYZ XYZ to NTSC RGB\n0.412424 0.212656 0.0193324\n0.357579 0.715158 0.119193\n0.180464 0.0721856 0.950444\n3.24071 -0.969258 0.0556352\n-1.53726 1.87599 -0.203996\n0.498571 0.0415557 1.05707\n0.576700 0.297361 0.0270328\n0.185556 0.627355 0.0706879\n0.188212 0.0752847 0.991248\n2.04148 -0.969258 0.0134455\n-0.564977 1.87599 -0.118373\n-0.344713 0.0415557 1.01527\n0.606734 0.298839 0.000000\n0.173564 0.586811 0.0661196\n0.200112 0.114350 1.11491\n1.91049 -0.984310 0.0583744\n-0.532592 1.99845 -0.118518\n-0.288284 -0.0282980 0.898611\n\nColor gamut\n- Given 3 primaries\n- The realizable\nchromaticities lay in the\ntriangle in xy\nchromaticity diagram\n- Because we can only\nadd light, no negative\nlight\nC\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\nThis image is in the public domain. Source: Wikimedia Commons.\n\nImage courtesy of Cpesacreta on Wikimedia Commons. License: CC-BY. This\ncontent is excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\nImage courtesy of Spigget on Wikimedia Commons. License: CC-BY-SA.\nThis content is excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- It's all about linear algebra\n- Projection from infinite-dimensional spectrum to a 3D\nresponse\n- Then any space based on color matching and\nmetamerism can be converted by 3x3 matrix\n- Complicated because\n- Projection from infinite-dimensional space\n- Non-orthogonal basis (cone responses overlap)\n- No negative light\n- XYZ is the most standard color space\n- RGB has many flavors\nIn summary\n\nQuestions?\n\n- Digital images are usually not encoded linearly\n- Instead, the value X1/γ is stored\n\n- Need to be decoded if we want linear values\nGamma encoding overview\n\n- The human visual system is more sensitive to ratios\n- Is a grey twice as bright as another one?\n- If we use linear encoding, we have tons of information\nbetween 128 and 255, but very little between 1 and 2!\n- Ideal encoding?\nLog\n- Problems with log?\nGets crazy around zero\nSolution: gamma\n\nColor quantization gamma\n\n- The human visual system is more sensitive to ratios\n- Is a grey twice as bright as another one?\n- If we use linear encoding, we have tons of information\nbetween 128 and 255, but very little between 1 and 2!\n- This is why a non-linear gamma remapping of about 2.0\nis applied before encoding\n- True also of analog imaging to optimize signal-noise\nratio\n\nColor quantization gamma\n\n- The human visual system is more sensitive to ratios\n- Is a grey twice as bright as another one?\n- If we use linear encoding, we have tons of information\nbetween 128 and 255, but very little between 1 and 2!\n- This is why a non-linear gamma remapping of about 2.0\nis applied before encoding\n- True also of analog imaging to optimize signal-noise\nratio\n\nColor quantization gamma\n\n- From Greg Ward\n- Only 6 bits for emphasis\nGamma encoding\n\n- Digital images are usually gamma encoded\n- Often γ = 2.2 (but 1.8 for Profoto RGB)\n- To get linear values, you must decode\n- apply x => xγ\nImportant Message\n\nQuestions?\n\nSelected Bibliography\nVision and Art : The Biology of Seeing by Margaret Livingstone, David H. Hubel Harry N\nAbrams; ISBN: 0810904063\n208 pages (May 2002)\nVision Science by Stephen E. Palmer\nMIT Press; ISBN: 0262161834\n760 pages (May 7, 1999)\n\nBillmeyer and Saltzman's Principles of Color Technology, 3rd Edition\nby Roy S. Berns, Fred W. Billmeyer, Max Saltzman\nWiley-Interscience; ISBN: 047119459X\n304 pages 3 edition (March 31, 2000)\nThe Reproduction of Color by R. W. G. Hunt\nFountain Press, 1995\nColor Appearance Models by Mark Fairchild\nAddison Wesley, 1998\nColor for the Sciences, by Jan Koenderink\nMIT Press 2010.\n\nQuestions?\nImage courtesy of SharkD on Wikimedia Commons. License: CC-BY. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Coordinates and Transformations",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/5cbb1bf32a92fad91e8ad6c37a473240_MIT6_837F12_Lec03.pdf",
      "content": "Coordinates and\nTransformations\nMIT ECCS 6.837\nWojciech Matusik\n\nmany slides follow Steven Gortler's book\n\nHierarchical modeling\n- Many coordinate systems:\n- Camera\n- Static scene\n- car\n- driver\n- arm\n- hand\n- ...\n- Makes it important to understand coordinate\nsystems\n\nImage courtesy of Gunnar A. Sjogren on Wikimedia Commons. License: CC-BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nCoordinates\n- We are used to represent points with tuples of\ncoordinates such as\n- But the tuples are meaningless without a clear\ncoordinate system\ncould be this point\nin the blue\ncoordinate system\ncould be this point\nin the red\ncoordinate system\n\nDifferent objects\n- Points\n- represent locations\n- Vectors\n- represent movement, force, displacement from A to B\n- Normals\n- represent orientation, unit length\n- Coordinates\n- numerical representation of the above objects\nin a given coordinate system\n\nPoints & vectors are different\n- The 0 vector has a fundamental meaning:\nno movement, no force\n- Why would there be a special 0 point?\n\n- It's meaningful to add vectors, not points\n- Boston location + NYC location =?\n+\n=?\n\nPoints & vectors are different\n- Moving car\n- points describe location of car elements\n- vectors describe velocity, distance between pairs of\npoints\n- If I translate the moving car to a different road\n- The points (location) change\n- The vectors (speed, distance between points) don't\nImage courtesy of Gunnar A. Sjogren on Wikimedia Commons. License: CC-BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nMatrices have two purposes\n- (At least for geometry)\n- Transform things\n- e.g. rotate the car from facing\nNorth to facing East\n- Express coordinate system\nchanges\n- e.g. given the driver's location\nin the coordinate system of the\ncar, express it in the coordinate\nsystem of the world\n\nGoals for today\n- Make it very explicit what coordinate system is\nused\n- Understand how to change coordinate systems\n- Understand how to transform objects\n- Understand difference between points, vectors,\nnormals and their coordinates\n\nQuestions?\n\nReference\n- This lecture follows the new book by\nSteven (Shlomo) Gortler from Harvard:\nFoundations of 3D Computer Graphics\n\nPlan\n- Vectors\n\n- Points\n\n- Homogeneous coordinates\n\n- Normals (in the next lecture)\n\nVectors (linear space)\n- Formally, a set of elements equipped with\naddition and scalar multiplication\n- plus other nice properties\n- There is a special element, the zero vector\n- no displacement, no force\n\nVectors (linear space)\n- We can use a basis to produce all the vectors in\nthe space:\n- Given n basis vectors\nany vector can be written as\nhere:\n\nLinear algebra notation\n- can be written as\n\n- Nice because it makes the basis\n(coordinate system) explicit\n- Shorthand:\n\n- where bold means triplet, t is transpose\n\nQuestions?\n\nLinear transformation\n- Transformation of the vector space\nCourtesy of Prof. Fredo Durand. Used with permission.\n\nLinear transformation\n- Transformation of the vector space so that\n\n- Note that it implies\n- Notation for transformations\nCourtesy of Prof. Fredo Durand. Used with permission.\n\nMatrix notation\n- Linearity implies\n\n?\n\nMatrix notation\n- Linearity implies\n\n- i.e. we only need to know the basis\ntransformation\n- or in algebra notation\n\nAlgebra notation\n- The are also vectors of the space\n- They can be expressed in the basis\nfor example:\n\n- which gives us\n...\n\nAlgebra notation\n- The are also vectors of the space\n- They can be expressed in the basis\nfor example:\n\n- which gives us\n\nRecap, matrix notation\n\n- Given the coordinates c in basis\nthe transformed vector has coordinates Mc in\n\nWhy do we care\n- We like linear algebra\n- It's always good to get back to an abstraction\nthat we know and for which smarter people have\ndeveloped a lot of tools\n- But we also need to keep track of what\nbasis/coordinate system we use\n\nQuestions?\n\nChange of basis\n- Critical in computer graphics\n- From world to car to arm to hand coordinate system\n- From Bezier splines to B splines and back\n\n- problem with basis change:\nyou never remember which is M or M 1\nit's hard to keep track of where you are\n\nChange of basis\n- Assume we have two bases and\n- And we have the coordinates of in\n- e.g.\n\n- i.e.\n\n- which implies\n\nChange of basis\n- We have &\n- Given the coordinate of in :\n\n- What are the coordinates in ?\n\nChange of basis\n- We have &\n- Given the coordinate of in :\n\n- Replace by its expression in\n\n- has coordinates in\n- Note how we keep track of the coordinate\nsystem by having the basis on the left\n\nQuestions?\n\n-L(p + q) = L(p) + L(q)\n-L(ap) = a L(p)\nLinear Transformations\nTranslation\nRotation\nRigid / Euclidean\nLinear\nSimilitudes\nIsotropic Scaling\nScaling\nShear\nReflection\nIdentity\nTranslation is not linear:\nf(p) = p+t\nf(ap) = ap+t = a(p+t) = a f(p)\nf(p+q) = p+q+t = (p+t)+(q+t) = f(p) + f(q)\n\nPlan\n- Vectors\n\n- Points\n\n- Homogenous coordinates\n\n- Normals\n\nPoints vs. Vectors\n- A point is a location\n- A vector is a motion between two points\n- Adding vectors is meaningful\n- going 3km North + 4km East = going 5km North-East\n- Adding points is not meaningful\n- Boston location + New York location = ?\n- Multiplying a point by a scalar?\n- The zero vector is meaningful (no movement)\n- Zero point ?\n\nAffine space\n- Points are elements of an affine space\n- We denote them with a tilde\n\n- Affine spaces are an extension of vector spaces\n\nPoint-vector operations\n- Subtracting points gives a vector\n\n- Adding a vector to a point gives a point\n\nFrames\n- A frame is an origin plus a basis\n- We can obtain any point in the space by adding\na vector to the origin\n\n- using the coordinates c of the vector in\n\nAlgebra notation\n- We like matrix-vector expressions\n- We want to keep track of the frame\n- We're going to cheat a little for elegance\nand decide that 1 times a point is the point\n\n- is represented in by 4 coordinate, where the\nextra dummy coordinate is always 1 (for now)\n\nRecap\n- Vectors can be expressed in a basis\n- Keep track of basis with left notation\n- Change basis\n- Points can be expressed in a frame\n(origin+basis)\n- Keep track of frame with left notation\n- adds a dummy 4th coordinate always 1\n\nAffine transformations\n- Include all linear transformations\n- Applied to the vector basis\n- Plus translation\nCourtesy of Prof. Fredo Durand. Used with permission.\n\nMatrix notation\n- We know how to transform the vector basis\n\n- We will soon add translation by a vector\n\nLinear component\n- Note how we leave the fourth component alone\n\nTranslation component\n- Express translation vector t in the basis\n\nTranslation\n\nFull affine expression\nWhich tells us both how to get a new frame ftM\nor how to get the coordinates Mc after transformation\n\nQuestions?\n\nMore notation properties\n- If the fourth coordinate is zero, we get a vector\n- Subtracting two points:\n\n- Gives us\n\na vector (last coordinate = 0)\n\nMore notation properties\n- Adding a point\n\nto a vector\n\n- Gives us\n\na point (4th coordinate=1)\n\nMore notation properties\n- vectors are not affected by the translation part\n\n- because their 4th coordinate is 0\n- If I rotate my moving car in the world, I want its\nmotion to rotate\n- If I translate it, motion should be unaffected\n\nQuestions?\n\nFrames & hierarchical modeling\n- Many coordinate systems (frames):\n- Camera\n- Static scene\n- car\n- driver\n- arm\n- hand\n- ...\n\n- Need to understand nested transformations\nImage courtesy of Gunnar A. Sjogren on Wikimedia Commons. License: CC-BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFrames & hierarchical modeling\n- Example: what if I rotate the wheel of the moving\ncar:\n- frame 1: world\n- frame 2: car\n- transformation: rotation\nImage courtesy of Gunnar A. Sjogren on Wikimedia Commons. License: CC-BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFrames & transformations\n- Transformation S wrt car frame f\n\n- how is the world frame a affected by this?\n- we have\n- which gives\n\n- i.e. the transformation in a is A-1SA\n- i.e., from right to left, A takes us from a to f, then\nwe apply S, then we go back to a with A-1\n\nQuestions?\n\nHow are transforms combined?\n(0,0)\n(1,1)\n(2,2)\n(0,0)\n(5,3)\n(3,1)\nScale(2,2)\nTranslate(3,1)\nTS =\n=\nScale then Translate\nUse matrix multiplication: p' = T ( S p ) = TS p\nCaution: matrix multiplication is NOT commutative!\n\nNon-commutative Composition\nScale then Translate: p' = T ( S p ) = TS p\nTranslate then Scale: p' = S ( T p ) = ST p\n(0,0)\n(1,1)\n(4,2)\n(3,1)\n(8,4)\n(6,2)\n(0,0)\n(1,1)\n(2,2)\n(0,0)\n(5,3)\n(3,1)\nScale(2,2)\nTranslate(3,1)\nTranslate(3,1)\nScale(2,2)\n\nTS =\nST =\nNon-commutative Composition\nScale then Translate: p' = T ( S p ) = TS p\n=\n=\nTranslate then Scale: p' = S ( T p ) = ST p\n\nQuestions?\n\nPlan\n- Vectors\n\n- Points\n\n- Homogenous coordinates\n\n- Normals\n\nForward reference and eye\n- The fourth coordinate is useful for perspective\nprojection\n- Called homogenous coordinates\n\nHomogeneous Coordinates\n-Add an extra dimension (same as frames)\n- in 2D, we use 3-vectors and 3 x 3 matrices\n- In 3D, we use 4-vectors and 4 x 4 matrices\n-The extra coordinate is now an arbitrary value, w\n- You can think of it as \"scale,\" or \"weight\"\n- For all transformations\nexcept perspective, you can\njust set w=1 and not worry\nabout it\nx'\ny'\na b\nd e\n0 0\nc\nf\n=\nx\ny\n\n- All non-zero scalar multiples of a point are considered\nidentical\n- to get the equivalent Euclidean point, divide by w\n\nProjective Equivalence\nx\ny\nz\nw\nax\nay\naz\naw\na != 0\n=\nx/w\ny/w\nz/w\n=\nw !=0\n\nWhy bother with extra coord?\nw = 1\nw = 2\n- This picture gives away almost\nthe whole story.\n\n- Camera at origin, looking along z, 90 degree\nf.o.v., \"image plane\" at z=1\nPerspective in 2D\nThis image is in the public domain.\nSource: http://openclipart.org/detail/34051/digicam-by-thesaurus.\n\nPerspective in 2D\nThis image is in the public domain.\nSource: http://openclipart.org/detail/34051/digicam-by-thesaurus.\nThe projected point in\nhomogeneous\ncoordinates\n(we just added w=1):\n\nPerspective in 2D\nProjectively\nequivalent\nThis image is in the public domain.\nSource: http://openclipart.org/detail/34051/digicam-by-thesaurus.\n\nPerspective in 2D\nWe'll just copy z to w,\nand get the projected\npoint after\nhomogenization!\nThis image is in the public domain.\nSource: http://openclipart.org/detail/34051/digicam-by-thesaurus.\n\nHomogeneous Visualization\n- Divide by w to normalize (project)\nw = 1\nw = 2\n(0, 0, 1) = (0, 0, 2) = ...\n(7, 1, 1) = (14, 2, 2) = ...\n(4, 5, 1) = (8, 10, 2) = ...\n(0,0,0)\n\nHomogeneous Visualization\n- Divide by w to normalize (project)\n- w = 0?\nw = 1\nw = 2\n(0, 0, 1) = (0, 0, 2) = ...\n(7, 1, 1) = (14, 2, 2) = ...\n(4, 5, 1) = (8, 10, 2) = ...\nPoints at infinity (directions)\n(0,0,0)\n\nProjective Equivalence - Why?\n- For affine transformations,\nadding w=1 in the end proved to be convenient.\n- The real showpiece is perspective.\nThis image is in the public domain.\nSource: http://openclipart.org/detail/34051/digicam-by-thesaurus.\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-us e/.\n\nQuestions?\n\nEye candy: photo tourism\n- Application of homogenous coordinates\n- Goal: given N photos of a scene\n- find where they were taken\n- get 3D geometry for points in the scene\nFrom Photo Tourism:: Exploring Photo Collections in 3D, used with permission from ACM, Inc.\n(c)ACM. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nStep 1: point correspondences\n- Extract salient points (corners) from images\n- Find the same scene point in other images\n- To learn how it's done, take 6.815\n\nStructure from motion\n- Given point correspondences\n- Unknowns: 3D point location, camera poses\n- For each point in each image, write perspective\nequations\nCamera 1 R1,t1\nCamera 2 R2,t2\nCamera 3\nR3,t3\np1\nMinimize f(R,T,P)\n\nEye candy: photo tourism\nQuickTimeTM and a\nMPEG-4 Video decompressor\nare needed to see this picture.\n\nAnd that's it for today\n- The rest on Thursday\n\nNormal\n- Surface Normal: unit vector that is locally\nperpendicular to the surface\n\nWhy is the Normal important?\n- It's used for shading -- makes things look 3D!\nobject color only\nDiffuse Shading\n\nVisualization of Surface Normal\n± x = Red\n± y = Green\n± z = Blue\n\nHow do we transform normals?\nObject Space\nWorld Space\nnOS\nnWS\n\nTransform Normal like Object?\n-translation?\n-rotation?\n-isotropic scale?\n-scale?\n-reflection?\n-shear?\n-perspective?\n\nTransform Normal like Object?\n-translation?\n-rotation?\n-isotropic scale?\n-scale?\n-reflection?\n-shear?\n-perspective?\n\nTransformation for shear and scale\nIncorrect\nNormal\nTransformation\nCorrect\nNormal\nTransformation\n\nMore Normal Visualizations\nIncorrect Normal Transformation\nCorrect Normal Transformation\n\n-Think about transforming the tangent plane\nto the normal, not the normal vector\nSo how do we do it right?\nOriginal\nIncorrect\nCorrect\nnOS\nPick any vector vOS in the tangent plane,\nhow is it transformed by matrix M?\nvOS\nvWS\nnWS\nvWS = M vOS\n\nTransform tangent vector v\nv is perpendicular to normal n:\nnOST vOS = 0\nnOS T (Mˉ1 M) vOS = 0\nnWST = nOST (Mˉ1)\n(nOST Mˉ1) (M vOS) = 0\n(nOST Mˉ1) vWS = 0\nnWST vWS = 0\nvWS is perpendicular to normal nWS:\nnWS = (Mˉ1)T nOS\nnOS\nvWS\nnWS\nvOS\nDot product\n\nDigression\n\n- The previous proof is not quite rigorous; first\nyou'd need to prove that tangents indeed\ntransform with M.\n- Turns out they do, but we'll take it on faith here.\n- If you believe that, then the above formula follows.\nnWS = (Mˉ1)T nOS\n\nComment\n- So the correct way to transform normals is:\n\n- But why did nWS = M nOS work for similitudes?\n- Because for similitude / similarity transforms,\n(Mˉ1)T =λ M\n- e.g. for orthonormal basis:\n\nMˉ1 = M T i.e. (Mˉ1)T = M\n\nnWS = (Mˉ1)T nOS\nSometimes denoted MˉT\n\nConnections\n- Not part of class, but cool\n- \"Covariant\": transformed by the matrix\n- e.g., tangent\n- \"Contravariant\": transformed by the inverse transpose\n- e.g., the normal\n- a normal is a \"co-vector\"\n\n- Google \"differential geometry\" to find out more\n\nQuestions?\n\nThat's All for Today\n- Further Reading\n-Buss, Chapter 2\n\n- Other Cool Stuff\n-Algebraic Groups\n-http://phototour.cs.washington.edu/\n-http://phototour.cs.washington.edu/findingpaths/\n-Free-form deformation of solid objects\n-Harmonic coordinates for character articulation\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Curves Properties and Conversion, Surface Representation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/38682e92015a39734867853f57ccf55b_MIT6_837F12_Lec02.pdf",
      "content": "6.837 - Matusik\n6.837 Computer Graphics\n\nCurve Properties & Conversion,\nSurface Representations\n\nvectorportal.com\n\n- P(t) = (1-t)3\nP1\n+ 3t(1-t)2 P2\n+ 3t2(1-t) P3\n+ t3\n\nP4\nCubic Bezier Splines\n\n- For Bezier curves, the\nbasis polynomials/vectors\nare Bernstein polynomials\n\n- For cubic Bezier curve:\nB1(t)=(1-t)3 B2(t)=3t(1-t)2\nB3(t)=3t2(1-t) B4(t)=t3\n(careful with indices, many authors start at 0)\n- Defined for any degree\nBernstein Polynomials\n\n- Geometry: control points coordinates assembled\ninto a matrix (P1, P2, ..., Pn+1)\n- Power basis: the monomials 1, t, t2, ...\n- Cubic Bezier:\nGeneral Spline Formulation\n\nQuestions?\n\n- What if we want to transform each point on the\ncurve with a linear transformation M?\nLinear Transformations & Cubics\nP'(t)= M\n\n- What if we want to transform each point on the\ncurve with a linear transformation M?\n- Because everything is linear, it is the same as\ntransforming only the control points\nLinear Transformations & Cubics\nP'(t)= M\n= M\n\n- Homogeneous coordinates also work\n- Means you can translate, rotate, shear, etc.\n- Note though that you need to normalize P' by 1/w'\nAffine Transformations\nP'(t)= M\n= M\n\nQuestions?\n\n- Differential Properties of Curves & Continuity\n- B-Splines\n- Surfaces\n- Tensor Product Splines\n- Subdivision Surfaces\n- Procedural Surfaces\n- Other\nThe Plan for Today\n\nDifferential Properties of Curves\n- Motivation\n- Compute normal for surfaces\n- Compute velocity for animation\n- Analyze smoothness\nImage courtesy of Kristian Molhave on Wikimedia Commons. License: CC-\nBY-SA. This content is excluded from our Creative Commons license. For\nmore information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- First derivative w.r.t. t\n- Can you compute this for Bezier curves?\nP(t) = (1-t)3 P1\n\n+ 3t(1-t)2 P2\n\n+ 3t2(1-t) P3\n\n+ t3\nP4\n- You know how to\ndifferentiate polynomials...\nVelocity\n\nVelocity\nSanity check: t=0; t=1\n- First derivative w.r.t. t\n- Can you compute this for Bezier curves?\nP(t) = (1-t)3 P1\n\n+ 3t(1-t)2 P2\n\n+ 3t2(1-t) P3\n\n+ t3\nP4\n- P'(t) = -3(1-t)2\nP1\n\n+ [3(1-t) 2 -6t(1-t)] P2\n\n+ [6t(1-t)-3t 2] P3\n\n+ 3t 2\n\nP4\n\n- Differentiation is a linear operation\n- (f+g)'=f'+g'\n- (af)'=a f'\n- This means that the derivative of the basis is\nenough to know the derivative of any spline.\n- Can be done with matrices\n- Trivial in monomial basis\n- But get lower-order polynomials\nLinearity?\n\n- The tangent to the curve P(t) can be defined as\nT(t)=P'(t)/||P'(t)||\n- normalized velocity, ||T(t)|| = 1\n- This provides us with one orientation for swept\nsurfaces later\nTangent Vector\nCourtesy of Seth Teller.\n\n- Derivative of unit tangent\n- K(t)=T'(t)\n- Magnitude ||K(t)|| is constant for a circle\n- Zero for a straight line\n- Always orthogonal to tangent, ie.\nCurvature Vector\n\n- K is zero for a line, constant for circle\n- What constant? 1/r\n- 1/||K(t)|| is the radius of the circle that touches\nP(t) at t and has the same curvature as the curve\nGeometric Interpretation\n\n- Normalized curvature: T'(t)/||T'(t)||\nCurve Normal\n\nQuestions?\n\n- C0 = continuous\n- The seam can be a sharp kink\n- G1 = geometric continuity\n- Tangents point to the same\ndirection at the seam\n- C1 = parametric continuity\n- Tangents are the same at the\nseam, implies G1\n- C2 = curvature continuity\n- Tangents and their derivatives\nare the same\nOrders of Continuity\nC0\nG1\nC1\n\n- G1 = geometric continuity\n- Tangents point to the same\ndirection at the seam\n- good enough for modeling\n- C1 = parametric continuity\n- Tangents are the same at the\nseam, implies G1\n- often necessary for animation\nOrders of Continuity\nG1\nC1\n\nConnecting Cubic Bezier Curves\n- How can we guarantee C0 continuity?\n- How can we guarantee G1 continuity?\n- How can we guarantee C1 continuity?\n- C2 and above gets difficult\n\nConnecting Cubic Bezier Curves\n- Where is this curve\n-\nC0 continuous?\n-\nG1 continuous?\n-\nC1 continuous?\n- What's the relationship\nbetween:\n-\nthe # of control points, and\nthe # of cubic Bezier\nsubcurves?\n\nQuestions?\n\nCubic B-Splines\n- ≥ 4 control points\n- Locally cubic\n- Cubics chained together, again.\nCourtesy of Seth Teller.\n\nCubic B-Splines\n- ≥ 4 control points\n- Locally cubic\n- Cubics chained together, again.\nCourtesy of Seth Teller.\n\nCubic B-Splines\n- ≥ 4 control points\n- Locally cubic\n- Cubics chained together, again.\nCourtesy of Seth Teller.\n\nCubic B-Splines\n- ≥ 4 control points\n- Locally cubic\n- Cubics chained together, again.\nCourtesy of Seth Teller.\n\n6.837 - Durand\nCubic B-Splines\n- ≥ 4 control points\n- Locally cubic\n- Cubics chained together, again.\n- Curve is not constrained to pass through any\ncontrol points\nCourtesy of Seth Teller.\n\nCubic B-Splines: Basis\nB1\nB4\nB2\nB3\nThese sum to 1, too!\nA B-Spline curve is also\nbounded by the convex\nhull of its control points.\n\nB1\nB4\nB2\nB3\nCubic B-Splines: Basis\n\nB1\nB4\nB2\nB3\nCubic B-Splines: Basis\n\nCubic B-Splines\n- Local control (windowing)\n- Automatically C2, and no need to match tangents!\nCourtesy of Seth Teller. Used with permission.\n\nB-Spline Curve Control Points\nDefault B-Spline\nB-Spline with\nderivative\ndiscontinuity\nB-Spline which passes\nthrough\nend points\nRepeat interior control\npoint\nRepeat end points\n\nBezier = B-Spline\nBezier\nB-Spline\nBut both are cubics, so one can be converted into the other!\n\nConverting between Bezier & BSpline\n\n- Simple with the basis matrices!\n- Note that this only works for\na single segment of 4\ncontrol points\n- P(t) = G B1 T(t) =\nG B1 (B2-1B2) T(t)=\n(G B1 B2-1) B2 T(t)\n- G B1 B2-1 are the control points\nfor the segment in new basis.\n\nMIT EECS 6.837, Popovic\nConverting between Bezier & B-Spline\noriginal\ncontrol\npoints as\nBezier\noriginal\ncontrol\npoints as\nB-Spline\nnew Bezier\ncontrol\npoints to\nmatch\nB-Spline\nnew\nBSpline\ncontrol\npoints to\nmatch\nBezier\n\n- Rational cubics\n- Use homogeneous coordinates, just add w !\n- Provides an extra weight parameter to control points\n\n- NURBS: Non-Uniform Rational B-Spline\n- non-uniform = different spacing between the\nblending functions, a.k.a. \"knots\"\n- rational = ratio of cubic polynomials\n(instead of just cubic)\n- implemented by adding the homogeneous coordinate w into\nthe control points.\nNURBS (Generalized B-Splines)\n\nQuestions?\n\n- Triangle meshes\n- Surface analogue of polylines, this is what GPUs\ndraw\n- Tensor Product Splines\n- Surface analogue of spline curves\n- Subdivision surfaces\n- Implicit surfaces, e.g. f(x,y,z)=0\n- Procedural\n- e.g. surfaces of revolution, generalized cylinder\n- From volume data (medical images, etc.)\nRepresenting Surfaces\n\n- What you've used so far in Assignment 0\n- Triangle represented by 3 vertices\n- Pro: simple, can be rendered directly\n- Cons: not smooth, needs many triangles to\napproximate smooth surfaces (tessellation)\n\nTriangle Meshes\nThis image is in the public domain. Source: Wikimedia Commons.\n\n- P(t) = (1-t)3\nP1\n+ 3t(1-t)2 P2\n+ 3t2(1-t) P3\n+ t3\n\nP4\nSmooth Surfaces?\nWhat's the\ndimensionality of a\ncurve? 1D!\n\nWhat about a\nsurface?\n\n- P(u) = (1-u)3 P1\n+ 3u(1-u)2 P2\n+ 3u2(1-u) P3\n+ u3\nP4\n\nHow to Build Them? Here's an Idea\n(Note! We relabeled\nt to u)\n\n- P(u) = (1-u)3 P1\n+ 3u(1-u)2 P2\n+ 3u2(1-u) P3\n+ u3\nP4\n\nHow to Build Them? Here's an Idea\n(Note! We relabeled\nt to u)\n\n- P(u) = (1-u)3 P1\n+ 3u(1-u)2 P2\n+ 3u2(1-u) P3\n+ u3\nP4\n\nHow to Build Them? Here's an Idea\n(Note! We relabeled\nt to u)\n\n- P(u) = (1-u)3 P1\n+ 3u(1-u)2 P2\n+ 3u2(1-u) P3\n+ u3\nP4\n\nHow to Build Them? Here's an Idea\n(Note! We relabeled\nt to u)\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\nv=1/3\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\nv=1/3 v=2/3\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\nv=1/3 v=2/3\n\n- P(u, v) = (1-u)3\nP1(v)\n+ 3u(1-u)2 P2(v)\n\n+ 3u2(1-u) P3(v)\n+ u3\nP4(v)\n\n- Let's make\nthe Pis move along\ncurves!\nHere's an Idea\nv=0\nv=1\nv=1/3 v=2/3\nA 2D surface patch!\n\n-\nIn the previous, Pis were just some curves\n-\nWhat if we make them Bezier curves?\nTensor Product Bezier Patches\n\n-\nIn the previous, Pis were just some curves\n-\nWhat if we make them Bezier curves?\n-\nEach u=const. and v=const.\ncurve is a Bezier curve!\n-\nNote that the boundary\ncontrol points (except\ncorners) are NOT\ninterpolated!\nTensor Product Bezier Patches\nv=0\nv=1\nv=2/3\n\nTensor Product Bezier Patches\nA bicubic Bezier\nsurface\n\nTensor Product Bezier Patches\nThe \"Control Mesh\"\n16 control points\n\n- P(u,v) = B1(u) * P1(v)\n+ B2(u) * P2(v)\n\n+ B3(u) * P3(v)\n+ B4(u) * P4(v)\n- Pi(v) = B1(v) * Pi,1\n+ B2(v) * Pi,2\n+ B3(v) * Pi,3\n+ B4(v) * Pi,4\n\nBicubics, Tensor Product\nP1,1\nP1,2\nP1,3\nP1,4\nP2,1 P2,2\nP2,3\nP2,4\nP3,1\nP3,2\nP3,3\nP3,4\nP4,1\nP4,2\nP4,3\nP4,4\n\n- P(u,v) = B1(u) * P1(v)\n+ B2(u) * P2(v)\n\n+ B3(u) * P3(v)\n+ B4(u) * P4(v)\n- Pi(v) = B1(v) * Pi,1\n+ B2(v) * Pi,2\n+ B3(v) * Pi,3\n+ B4(v) * Pi,4\n\nBicubics, Tensor Product\n\n- P(u,v) = B1(u) * P1(v)\n+ B2(u) * P2(v)\n\n+ B3(u) * P3(v)\n+ B4(u) * P4(v)\n- Pi(v) = B1(v) * Pi,1\n+ B2(v) * Pi,2\n+ B3(v) * Pi,3\n+ B4(v) * Pi,4\n\nBicubics, Tensor Product\n16 control points Pi,j\n16 2D basis functions Bi,j\n\n- Parametric surface P(u,v) is a bicubic polynomial\nof two variables u & v\n- Defined by 4x4=16 control points P1,1, P1,2....\nP4,4\n- Interpolates 4 corners, approximates others\n- Basis are product of two Bernstein polynomials:\nB1(u)B1(v); B1(u)B2(v);... B4(u)B4(v)\n\nRecap: Tensor Bezier Patches\n(c) Addison-Wesley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\n-\nP(u,v) is a 3D point specified by u, v\n-\nThe partial derivatives and are\n3D vectors\n-\nBoth are tangent to surface at P\n\nTangents and Normals for Patches\n(c) Addison-Wesley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n-\nP(u,v) is a 3D point specified by u, v\n-\nThe partial derivatives and are\n3D vectors\n-\nBoth are tangent to surface at P\n-\nNormal is perpendicular to both, i.e.,\n\nn is usually not\nunit, so must\nnormalize!\nTangents and Normals for Patches\n(c) Addison-Wesley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\n- Cubic Bezier in matrix notation\nRecap: Matrix Notation for Curves\npoint on curve\n(2x1 vector)\n\"Geometry matrix\"\nof control points P1..P4\n(2 x 4)\n\"Spline matrix\"\n(Bernstein)\nCanonical\n\"power basis\"\n\nHardcore: Matrix Notation for Patches\nx coordinate of\nsurface at (u,v)\nRow vector of\nbasis functions (u)\nColumn vector of\nbasis functions (v)\n4x4 matrix of x coordinates\nof the control points\n- Not required,\nbut convenient!\n\n- Curves:\n\n- Surfaces:\n\n- T = power basis\nB = spline matrix\nG = geometry matrix\nHardcore: Matrix Notation for Patches\nA separate 4x4 geometry\nmatrix for x, y, z\n\n- You can stack the Gx, Gy, Gz matrices into a\ngeometry tensor of control points\n- I.e., Gki,j = the kth coordinate of control point Pi,j\n- A cube of numbers!\n\n- \"Definitely not required, but nice!\n- See http://en.wikipedia.org/wiki/Multilinear_algebra\nSuper Hardcore: Tensor Notation\n\nTensor Product B-Spline Patches\n- Bezier and B-Spline curves are both cubics\n- Can change between representations using matrices\n\n- Consequently, you can build tensor product\nsurface patches out of B-Splines just as well\n- Still 4x4 control points for each patch\n- 2D basis functions are pairwise\nproducts of B-Spline basis functions\n- Yes, simple!\n(c) Addison-Wesley. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Pros\n- Smooth\n- Defined by reasonably small set of points\n- Cons\n- Harder to render (usually converted to triangles)\n- Tricky to ensure continuity at patch boundaries\n- Extensions\n- Rational splines: Splines in homogeneous coordinates\n- NURBS: Non-Uniform Rational B-Splines\n- Like curves: ratio of polynomials, non-uniform location of\ncontrol points, etc.\nTensor Product Spline Patches\n\n6.837 - Durand\nUtah Teapot: Tensor Bezier Splines\n- Designed by Martin Newell\nImage courtesy of Dhatfield on Wikimedia Commons. License: CC-BY-SA. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Not all surfaces are smooth...\nCool: Displacement Mapping\n(c) Addison-Wesley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Not all surfaces are smooth...\n- \"Paint\" displacements on a smooth surface\n- For example, in the direction of normal\n- Tessellate smooth patch into fine grid,\nthen add displacement D(u,v) to vertices\n- Heavily used in movies, more and more in games\nCool: Displacement Mapping\n(c) Addison-Wesley. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nDisplacement Mapping Example\nSmooth base surface\nDisplaced Surface\nThis image is in the public domain. Source: Wikimedia Commons.\n\nQuestions?\n\n6.837 - Durand\nSubdivision Surfaces\n- Start with polygonal mesh\n- Subdivide into larger number of polygons,\nsmooth result after each subdivision\n- Lots of ways to do this.\n- The limit surface is smooth!\n(c) IEEE. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\n\nCorner Cutting\ninf\n\nCorner Cutting\n\nCorner Cutting\nIt turns out corner cutting\n(Chaikin's Algorithm)\nproduces a quadratic B-\nSpline curve! (Magic!)\n\nCorner Cutting\n(Well, not totally unexpected,\nremember de Casteljau)\n\n- Idea: cut corners to smooth\n- Add points and compute\nweighted average of neighbors\n- Same for surfaces\n- Special case for irregular vertices\n- vertex with more or less than 6 neighbors in a triangle mesh\nSubdivision Curves and Surfaces\nWarren et al.\n(c) IEEE. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Advantages\n- Arbitrary topology\n- Smooth at boundaries\n- Level of detail, scalable\n- Simple representation\n- Numerical stability, well-behaved meshes\n- Code simplicity\n- Little disadvantage:\n- Procedural definition\n- Not parametric\n- Tricky at special vertices\nWarren et al.\nSubdivision Curves and Surfaces\n(c) IEEE. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\n- Catmull-Clark\n- Quads and triangles\n- Generalizes bicubics to\narbitrary topology!\n- Loop, Butterfly\n- Triangles\n- Doo-Sabin, sqrt(3), biquartic...\n- and a whole host of others\n- Used everywhere in movie and game modeling!\n- See http://www.cs.nyu.edu/~dzorin/sig00course/\nFlavors of Subdivision Surfaces\nImage courtesy of Romainbehar on Wikimedia Commons.\nLicense: CC-BY-SA. This content is excluded from our\nCreative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nSubdivision + Displacement\nOriginal rough mesh\nOriginal mesh with\nsubdivision\nOriginal mesh with\nsubdivision and\ndisplacement\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\nSpecialized Procedural Definitions\n- Surfaces of\nrevolution\n- Rotate given 2D\nprofile curve\n- Generalized\ncylinders\n- Given 2D profile and\n3D curve, sweep the\nprofile along the 3D\ncurve\n- Assignment 1!\n\nSurface of Revolution\nv\ns(u,v)=R(v)q(u)\nwhere R is a matrix,\nq a vector,\nand s is a point on\nthe surface\ns(u,v)\n- 2D curve q(u) provides one dimension\n- Note: works also with 3D curve\n- Rotation R(v) provides 2nd dimension\n\n- Trace out surface by moving a\nprofile curve along a trajectory.\n- profile curve q(u) provides one dim\n- trajectory c(u) provides the other\n- Surface of revolution can be seen\nas a special case where trajectory\nis a circle\nGeneral Swept Surfaces\nwhere M is a matrix that depends on the trajectory c\nq\nc\ns\ns(u,v)=M(c(v))q(u)\n\n- How do we get M?\n- Translation is easy, given by c(v)\n- What about orientation?\n- Orientation options:\n- Align profile curve with an axis.\n- Better: Align profile curve with\nframe that \"follows\" the curve\nGeneral Swept Surfaces\nwhere M is a matrix that depends on the trajectory c\ns(u,v)=M(c(v))q(u)\nq\nc\ns\n\n- Frame defined by 1st\n(tangent), 2nd and 3rd\nderivatives of a 3D curve\n- Looks like a good idea\nfor swept surfaces...\nFrames on Curves: Frenet Frame\nImage courtesy of Kristian Molhave on Wikimedia Commons. License: CC-\nBY-SA. This content is excluded from our Creative Commons license. For\nmore information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Normal flips!\n- Bad to define a smooth swept surface\nFrenet: Problem at Inflection!\nAn inflection is a point\nwhere curvature changes\nsign\n\n- Build triplet of vectors\n- include tangent (it is reliable)\n- orthonormal\n- coherent over the curve\n- Idea:\n- use cross product to create orthogonal vectors\n- exploit discretization of curve\n- use previous frame to bootstrap orientation\n- See Assignment 1 instructions!\nSmooth Frames on Curves\n\n- Need partial derivatives w.r.t.\nboth u and v\n\n- Remember to normalize!\n- One given by tangent of profile\ncurve, the other by tangent of\ntrajectory\nNormals for Swept Surfaces\nwhere M is a matrix that depends on the trajectory c\ns(u,v)=M(c(v))q(u)\nq\nc\ns\ns\ns\n\nQuestions?\n\nImplicit Surfaces\n-\nSurface defined implicitly by a function\nThis image is in the public domain. Source: Wikimedia Commons.\n\n- Pros:\n- Efficient check whether point is inside\n- Efficient Boolean operations\n- Can handle weird topology for animation\n- Easy to do sketchy modeling\n- Cons:\n- Does not allow us to easily generate a\npoint on the surface\nImplicit Surfaces\nImage courtesy of Anders Sandberg on Wikimedia Commons. License: CC-BY-\nSA. This content is excluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\nPoint Set Surfaces\n- Given only a noisy 3D point cloud (no\nconnectivity), can you define a reasonable surface\nusing only the points?\n- Laser range scans only give you points,\nso this is potentially useful\nFrom Point Set Surfaces, (Alexa et al. 2001).\n(c) IEEE. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nPoint Set Surfaces\nAlexa et al. 2001\nFrom Point Set Surfaces, used\nwith permission from ACM, Inc\n(c) IEEE. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Modern take on implicit surfaces\n- Cool math: Moving Least Squares (MLS),\npartitions of unity, etc.\n\n- Not required in this class, but nice to know.\nPoint Set Surfaces\nOhtake et al. 2003\nFrom Multi-Level Partition\nof Unity Implicits\n(c) ACM, Inc. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\n6.837 - Durand\nThat's All for Today\n- Further reading\n- Buss, Chapters 7 & 8\n\n- Subvision curves and surfaces\n- http://www.cs.nyu.edu/~dzorin/sig00course/\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Global Illumination and Monte Carlo",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/1b5985f78c68379e15543fe27d41b72c_MIT6_837F12_Lec18.pdf",
      "content": "Global Illumination and Monte Carlo\nMIT EECS 6.837 Computer Graphics\nWojciech Matusik\nwith many slides from Fredo Durand and Jaakko Lehtinen\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nToday\n- Lots of randomness!\nDunbar & Humphreys\n\nToday\n- Global Illumination\n- Rendering Equation\n- Path tracing\n- Monte Carlo integration\n- Better sampling\n- importance\n- stratification\n(c) ACM. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nGlobal Illumination\n- So far, we've seen only direct lighting (red here)\n- We also want indirect lighting\n- Full integral of all directions (multiplied by BRDF)\n- In practice, send tons of random rays\n\nDirect Illumination\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nGlobal Illumination (with Indirect)\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nGlobal Illumination\n- So far, we only used the BRDF for point lights\n- We just summed over all the point light sources\n- BRDF also describes how indirect illumination\nreflects off surfaces\n- Turns summation into integral over hemisphere\n- As if every direction had a light source\n\nReflectance Equation, Visually\noutgoing light to\ndirection v\nincident light\nfrom direction\nomega\nthe BRDF\ncosine term\nv\nSum (integrate)\nover every\ndirection on the\nhemisphere,\nmodulate incident\nillumination by\nBRDF\nLin\nLin\nLin\nLin\n\nThe Reflectance Equation\n\n- Where does Lin come from?\nx\n\nThe Reflectance Equation\n\n- Where does Lin come from?\n- It is the light reflected towards x from the surface point in\ndirection l ==> must compute similar integral there!\n- Recursive!\n\nx\n\n- Where does Lin come from?\n- It is the light reflected towards x from the surface point in\ndirection l ==> must compute similar integral there!\n- Recursive!\n- AND if x happens\nto be a light source,\nwe add its contribution\ndirectly\n\nThe Rendering Equation\nx\n\n- The rendering equation describes the appearance of\nthe scene, including direct and indirect illumination\n- An \"integral equation\", the unknown solution function L\nis both on the LHS and on the RHS inside the integral\n- Must either discretize or use Monte Carlo integration\n- Originally described by Kajiya and Immel et al. in 1986\n- More on 6.839\n- Also, see book references towards the end\nThe Rendering Equation\n\nThe Rendering Equation\n- Analytic solution is usually impossible\n- Lots of ways to solve it approximately\n- Monte Carlo techniques use random samples for\nevaluating the integrals\n- We'll look at some simple method in a bit...\n- Finite element methods discretize the solution using\nbasis functions (again!)\n- Radiosity, wavelets, precomputed radiance transfer, etc.\n\nQuestions?\n\nHow To Render Global\nIllumination?\nLehtinen et al. 2008\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nRay Casting\n- Cast a ray from the eye through each pixel\n\nRay Tracing\n- Cast a ray from the eye through each pixel\n- Trace secondary rays (shadow, reflection, refraction)\n\n\"Monte-Carlo Ray Tracing\"\n- Cast a ray from the eye through each pixel\n- Cast random rays from the hit point to evaluate\nhemispherical integral using random sampling\n\n\"Monte-Carlo Ray Tracing\"\n- Cast a ray from the eye through each pixel\n- Cast random rays from the visible point\n- Recurse\n\n\"Monte-Carlo Ray Tracing\"\n- Cast a ray from the eye through each pixel\n- Cast random rays from the visible point\n- Recurse\n\n\"Monte-Carlo Ray Tracing\"\n- Systematically sample light sources at each hit\n- Don't just wait the rays will hit it by chance\n\nResults\nHenrik Wann Jensen\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nMonte Carlo Path Tracing\n- Trace only one secondary ray per recursion\n- Otherwise number of rays explodes!\n- But send many primary rays per pixel (antialiasing)\n\nMonte Carlo Path Tracing\n- Trace only one secondary ray per recursion\n- Otherwise number of rays explodes!\n- But send many primary rays per pixel (antialiasing)\nAgain, trace\nshadow rays\nfrom each\nintersection\n\nMonte Carlo Path Tracing\n- We shoot one path from the eye at a time\n- Connect every surface point on the way to the light by a\nshadow ray\n- We are randomly sampling the space of all possible light\npaths between the source and the camera\n\n- 10 paths/pixel\nPath Tracing Results\nHenrik Wann Jensen\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nNote: More noise. This is not a coincidence; the integrand\nhas higher variance (the BRDFs are \"spikier\").\n- 10 paths/pixel\nPath Tracing Results: Glossy Scene\nHenrik Wann Jensen\nCourtesy of Henrik Wann Jensen. Used with permission.\n\n- 100 paths/pixel\nPath Tracing Results: Glossy Scene\nHenrik Wann Jensen\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nImportance of Sampling the Light\nWithout explicit\nlight sampling\nWith explicit\nlight sampling\n1 path\nper pixel\n4 paths\nper pixel\n✔\n✔\n\nWhy Use Random Numbers?\n- Fixed random sequence\n- We see the structure in the error\nHenrik Wann Jensen\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nDemo\n- http://madebyevan.com/webgl-path-tracing/\nImage removed due to copyright restrictions. Please see the above link for further details.\n\nFor more demo/experimentation\n- http://www.mitsuba-renderer.org/\n- http://www.pbrt.org/\n- http://www.luxrender.net/en_GB/index\n\nQuestions?\n- Vintage path tracing by Kajiya\n(c) Jim Kajiya. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nPath Tracing is costly\n- Needs tons of rays per pixel!\n\nGlobal Illumination (with Indirect)\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nIndirect Lighting is Mostly Smooth\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nIrradiance Caching\n- Indirect illumination is smooth\n\nIrradiance Caching\n- Indirect illumination is smooth\n\nIrradiance Caching\n- Indirect illumination is smooth\n==> Sample sparsely, interpolate nearby values\n\nIrradiance Caching\n- Store the indirect illumination\n- Interpolate existing cached values\n- But do full calculation for direct lighting\n\nIrradiance Caching\n- Yellow dots:\nindirect diffuse sample points\nThe irradiance cache tries to\nadapt sampling density to\nexpected frequency content of\nthe indirect illumination (denser\nsampling near geometry)\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nRadiance by Greg Ward\n- The inventor of irradiance caching\n- http://radsite.lbl.gov/radiance/\nImage removed due to copyright restrictions. Please see above link for further details.\n\nQuestions?\nImage: Pure\nImage of Y chair designed by H.J. Wegner has been removed due to copyright restrictions.\nPlease see http://tora_2097.cgsociety.org/portfolio/project-detail/786738/ for further details.\n\nPhoton Mapping\n- Preprocess: cast rays from light sources, let them\nbounce around randomly in the scene\n- Store \"photons\"\n\nPhoton Mapping\n- Preprocess: cast rays from light sources\n- Store photons (position + light power + incoming direction)\n\nThe Photon Map\n- Efficiently store photons for fast access\n- Use hierarchical spatial structure (kd-tree)\n\nPhoton Mapping - Rendering\n-\nCast primary rays\n-\nFor secondary rays\n-\nreconstruct irradiance using adjacent stored photon\n-\nTake the k closest photons\n-\nCombine with irradiance caching and a number of other techniques\nShooting one bounce of\nsecondary rays and\nusing the density\napproximation at those\nhit points is called final\ngathering.\n\nPhoton Map Results\nCourtesy of Henrik Wann Jensen. Used with permission.\n\n- Many materials exhibit subsurface scattering\n- Light doesn't just reflect off the surface\n- Light enters, scatters around, and exits at another point\n- Examples: Skin, marble, milk\nMore Global Illumination Coolness\nImages: Jensen et al.\nCourtesy of Henrik Wann Jensen. Used with permission.\n\nMore Subsurface Scattering\nPhotograph\nRendering\nWeyrich et al. 2006\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nThat Was Just the Beginning\n- Tons and tons of other Monte Carlo techniques\n- Bidirectional Path Tracing\n- Shoot random paths not just from camera but also light, connect\nthe path vertices by shadow rays\n- Metropolis Light Transport\n- And Finite Element Methods\n- Use basis functions instead of random sampling\n- Radiosity (with hierarchies & wavelets)\n- Precomputed Radiance Transfer\n\n- This would warrant a class of its own!\n\nWhat Else Can We Integrate?\n- Pixel: antialiasing\n- Light sources: Soft shadows\n- Lens: Depth of field\n- Time: Motion blur\n- BRDF: glossy reflection\n- (Hemisphere: indirect lighting)\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved.\nThis content is excluded from our Creative\nCommons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\nCourtesy of Henrik Wann Jensen.\nUsed with permission.\n(c) ACM. All rights reserved. This content is\nexcluded from our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nDomains of Integration\n- Pixel, lens (Euclidean 2D domain)\n- Antialiasing filters, depth of field\n- Time (1D)\n- Motion blur\n- Hemisphere\n- Indirect lighting\n- Light source\n- Soft shadows\nFamous motion blur image\nfrom Cook et al. 1984\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Rendering glossy reflections\n- Random reflection rays around mirror direction\n- 1 sample per pixel\nMotivational Eye Candy\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Rendering glossy reflections\n- Random reflection rays around mirror direction\n- 256 samples per pixel\nMotivational Eye Candy\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nError/noise Results in Variance\n- We use random rays\n- Run the algorithm again get different image\n- What is the noise/variance/standard deviation?\n- And what's really going on anyway?\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nIntegration\n- Compute integral of arbitrary function\n- e.g. integral over area light source, over hemisphere, etc.\n- Continuous problem we need to discretize\n- Analytic integration never works because of visibility and other\nnasty details\n\nIntegration\n- You know trapezoid, Simpson's rule, etc.\n\nMonte Carlo Integration\n- Monte Carlo integration: use random samples and\ncompute average\n- We don't keep track of spacing between samples\n- But we kind of hope it will be 1/N on average\n\nMonte Carlo Integration\n- S is the integration domain\n- Vol(S) is the volume (measure) of S\n- {xi} are independent uniform random points in S\n\nMonte Carlo Integration\n- S is the integration domain\n- Vol(S) is the volume (measure) of S\n- {xi} are independent uniform random points in S\n- The integral is the average of f times the volume of S\n- Variance is proportional to 1/N\n- Avg. error is proportional 1/sqrt(N)\n- To halve error, need 4x samples\n\nMonte Carlo Computation of\n- Take a square\n- Take a random point (x,y) in the square\n- Test if it is inside the 1⁄4 disc (x2+y2 < 1)\n- The probability is /4\nx\ny\nIntegral of the function that\nis one inside the circle, zero\noutside\n\nMonte Carlo Computation of\n- The probability is /4\n- Count the inside ratio n = # inside / total # trials\n- n * 4\n- The error depends on the number or trials\nDemo\ndef piMC(n):\nsuccess = 0\nfor i in range(n): x=random.random()\n\ny=random.random()\n\nif x*x+y*y<1: success = success+1 return\n4.0*float(success)/float(n)\n\nWhy Not Use Simpson Integration?\n- You're right, Monte Carlo is not very efficient for\ncomputing\n- When is it useful?\n- High dimensions: Convergence is independent of\ndimension!\n- For d dimensions, Simpson requires Nd domains (!!!)\n- Similar explosion for other quadratures (Gaussian, etc.)\n\nAdvantages of MC Integration\n- Few restrictions on the integrand\n- Doesn't need to be continuous, smooth, ...\n- Only need to be able to evaluate at a point\n- Extends to high-dimensional problems\n- Same convergence\n- Conceptually straightforward\n- Efficient for solving at just a few points\n\nDisadvantages of MC\n- Noisy\n- Slow convergence\n- Good implementation is hard\n- Debugging code\n- Debugging math\n- Choosing appropriate techniques\n\nQuestions?\n- Images by Veach and Guibas, SIGGRAPH 95\nNaive sampling strategy\nOptimal sampling strategy\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nHmmh...\n- Are uniform samples the best we can do?\n\nSmarter Sampling\nSample a non-uniform probability\nCalled \"importance sampling\"\nIntuitive justification: Sample more in places where there are\nlikely to be larger contributions to the integral\n\nExample: Glossy Reflection\n- Integral over hemisphere\n- BRDF times cosine times incoming light\nSlide courtesy of Jason Lawrence\nImage removed due to copyright restrictions - please see Jason Lawrence's slide 9-12 in the talk slides on \"Efficient BRDF\nImportance Sampling Using a Factored Representation,\" available at http://www.cs.virginia.edu/~jdl/.\n\nSampling a BRDF\nSlide courtesy of Jason Lawrence\nImage removed due to copyright restrictions - please see Jason Lawrence's slide 9-12 in the talk slides on \"Efficient BRDF\nImportance Sampling Using a Factored Representation,\" available at http://www.cs.virginia.edu/~jdl/.\n\nSampling a BRDF\nSlide courtesy of Jason Lawrence\nImage removed due to copyright restrictions - please see Jason Lawrence's slide 9-12 in the talk slides on \"Efficient BRDF\nImportance Sampling Using a Factored Representation,\" available at http://www.cs.virginia.edu/~jdl/.\n\nSampling a BRDF\nSlide courtesy of Jason Lawrence\nImage removed due to copyright restrictions - please see Jason Lawrence's slide 9-12 in the talk slides on \"Efficient BRDF\nImportance Sampling Using a Factored Representation,\" available at http://www.cs.virginia.edu/~jdl/.\n\nImportance Sampling Math\n\n- Like before, but now {xi} are not uniform but drawn\naccording to a probability distribution p\n- Uniform case reduces to this with p(x) = const.\n- The problem is designing ps that are easy to sample\nfrom and mimic the behavior of f\n\nMonte Carlo Path Tracing\nhttp://www.youtube.com/watch?v=mYMkAnm-PWw 75\nVideo removed due to copyright restrictions - please see the link below for further details.\n\nQuestions? Traditional importance function\nTraditional importance function Better importance by Lawrence et al.\nBetter importance by Lawrence et al.\n1200 Samples/Pixel\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nStratified Sampling\n- With uniform sampling, we can get unlucky\n- E.g. all samples clump in a corner\n- If we don't know anything of the integrand,\nwe want a relatively uniform sampling\n- Not regular, though, because of aliasing!\n\n- To prevent clumping, subdivide domain\ninto non-overlapping regions i\n- Each region is called a stratum\n- Take one random sample per i\n\nStratified Sampling Example\n- When supersampling, instead of taking KxK regular\nsub-pixel samples, do random jittering within each\nKxK sub-pixel\n\nStratified Sampling Analysis\n- Cheap and effective\n- But mostly for low-dimensional domains\n- Again, subdivision of N-D needs Nd domains like\ntrapezoid, Simpson's, etc.!\n\n- With very high dimensions, Monte Carlo is pretty\nmuch the only choice\n\nQuestions?\n- Image from the ARNOLD Renderer by Marcos Fajardo\nImages removed due to copyright restrictions -- Please see\nhttp://www.3dluvr.com/marcosss/morearni/ for further details.\n\n- 6.839!\n- Eric Veach's PhD dissertation\nhttp://graphics.stanford.edu/papers/veach_thesis/\n\n- Physically Based Rendering\nby Matt Pharr, Greg Humphreys\nFor Further Information...\n\nReferences\nImages of the following book covers have been removed due to copyright restrictions:\n-Advanced Global Illumination by Philip Dutre, Philippe Bekaert, and Kavita Bala\n-Realistic Ray Tracing by Peter Shirley and R. K. Morley\n-Realistic Image Synthesis Using Photon Mapping by Henrik Wann Jensen\nPlease check the books for further details.\n\nThat's All for today\nImage: Fournier and\nReeves, SIGGRAPH 86\nImage removed due to copyright restrictions -- please Fig. 13 in Fournier A. and W.T. Reeves. \"A Simple Model of Ocean Waves.\"\nSIGGRAPH '86 Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques; Pages 75-84.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Graphics Pipeline and Rasterization",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/53d96abf747a3c82fd3497d2fea540f5_MIT6_837F12_Lec21.pdf",
      "content": "Graphics Pipeline & Rasterization\nMIT EECS 6.837 - Matusik\nImage removed due to copyright restrictions.\n\n- Use graphics hardware, via OpenGL or DirectX\n- OpenGL is multi-platform, DirectX is MS only\n\nHow Do We Render Interactively?\nOpenGL rendering\nOur ray tracer\n(c) Khronos Group. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Use graphics hardware, via OpenGL or DirectX\n- OpenGL is multi-platform, DirectX is MS only\n\n- Most global effects available in ray tracing will be\nsacrificed for speed, but some can be approximated\nHow Do We Render Interactively?\nOpenGL rendering\nOur ray tracer\n(c) Khronos Group. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nRay Casting vs. GPUs for Triangles\nRay Casting\nFor each pixel (ray)\nFor each triangle\nDoes ray hit triangle?\nScene\nprimitives\nPixel raster\nKeep closest hit\n\nRay Casting vs. GPUs for Triangles\nRay Casting\nFor each pixel (ray)\nFor each triangle\nDoes ray hit triangle?\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\nScene\nprimitives\nScene\nprimitives\nPixel raster\nPixel raster\nKeep closest hit\nKeep closest hit\n\nRay Casting vs. GPUs for Triangles\nRay Casting\nFor each pixel (ray)\nFor each triangle\nDoes ray hit triangle?\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\nScene\nprimitives\nScene\nprimitives\nPixel raster\nPixel raster\nKeep closest hit\nKeep closest hit\nIt's just a different order of the loops!\n\nGPUs do Rasterization\n- The process of taking a\ntriangle and figuring out\nwhich pixels it covers is\ncalled rasterization\nScene\nprimitives\nPixel raster\nKeep closest hit\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\n\nGPUs do Rasterization\n- The process of taking a\ntriangle and figuring out\nwhich pixels it covers is\ncalled rasterization\n- We've seen acceleration\nstructures for ray\ntracing; rasterization is\nnot stupid either\n- We're not actually going\nto test all pixels for each\ntriangle\nScene\nprimitives\nPixel raster\nKeep closest hit\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\n\nRasterization (\"Scan Conversion\")\nglBegin(GL_TRIANGLES)\nglNormal3f(...)\nglVertex3f(...)\nglVertex3f(...)\nglVertex3f(...)\nglEnd();\n- Given a triangle's vertices &\nextra info for shading, figure\nout which pixels to \"turn on\"\nto render the primitive\n- Compute illumination values to\n\"fill in\" the pixels within the\nprimitive\n- At each pixel, keep track of\nthe closest primitive (z-buffer)\n- Only overwrite if triangle being\ndrawn is closer than the previous\ntriangle in that pixel\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- What needs to be stored in memory in each case?\nWhat are the Main Differences?\nRay Casting\nFor each pixel (ray)\nFor each triangle\nDoes ray hit triangle?\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\nKeep closest hit\nKeep closest hit\nRay-centric\nTriangle-centric\n\n- In this basic form, ray tracing needs the entire scene\ndescription in memory at once\n- Then, can sample the image completely freely\n- The rasterizer only needs one triangle at a time, plus\nthe entire image and associated depth information for\nall pixels\nWhat are the Main Differences?\nRay Casting\nFor each pixel (ray)\nFor each triangle\nDoes ray hit triangle?\nGPU\nFor each triangle\nFor each pixel\nDoes triangle cover pixel?\nKeep closest hit\nKeep closest hit\nRay-centric\nTriangle-centric\n\n- Modern scenes are more complicated than images\n- A 1920x1080 frame at 64-bit color and 32-bit depth per\npixel is 24MB (not that much)\n- Of course, if we have more than one sample per pixel this gets\nlarger, but e.g. 4x supersampling is still a relatively comfortable\n~100MB\n- Our scenes are routinely larger than this\n- This wasn't always true\nRasterization Advantages\n\nRasterization Advantages\nWeiler, Atherton 1977\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Modern scenes are more complicated than images\n- A 1920x1080 frame (1080p) at 64-bit color and 32-bit\ndepth per pixel is 24MB (not that much)\n- Of course, if we have more than one sample per pixel (later) this\ngets larger, but e.g. 4x supersampling is still a relatively\ncomfortable ~100MB\n- Our scenes are routinely larger than this\n- This wasn't always true\n\n- A rasterization-based renderer can stream over the\ntriangles, no need to keep entire dataset around\n- Allows parallelism and optimization of memory systems\nRasterization Advantages\n\n- Restricted to scan-convertible primitives\n- Pretty much: triangles\n- Faceting, shading artifacts\n- This is largely going away\nwith programmable per-pixel\nshading, though\n- No unified handling of\nshadows, reflection,\ntransparency\n- Potential problem of\noverdraw (high depth\ncomplexity)\n- Each pixel touched\nmany times\nRasterization Limitations\nscan conversion\ngouraud shading\nray tracing\nscan conversion\nflat shading\n(c) Khronos Group. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Advantages\n- Generality: can render anything\nthat can be intersected with a ray\n- Easily allows recursion (shadows, reflections, etc.)\n\n- Disadvantages\n- Hard to implement in hardware (lacks computation\ncoherence, must fit entire scene in memory, bad memory\nbehavior)\n- Not such a big point any more given general purpose GPUs\n- Has traditionally been too slow for interactive applications\n- Both of the above are changing rather rapidly right now!\nRay Casting / Tracing\n\nQuestions?\nCall of Duty: Modern Warfare 2 by Infinity Ward\nImage removed due to copyright restrictions.\n\n- Input\n- Geometric model\n- Triangle vertices, vertex normals, texture coordinates\n- Lighting/material model (shader)\n- Light source positions, colors, intensities, etc.\n- Texture maps, specular/diffuse coefficients, etc.\n- Viewpoint + projection plane\n\n- Output\n- Color (+depth) per pixel\nModern Graphics Pipeline\nColbert & Krivanek\nImage of Real-Time Rendering of the Stanford Bunny\nwith 40 Samples per Pixel removed due to copyright\nrestrictions -- please see Fig. 20-1 from http://http.\ndeveloper.nvidia.com/GPUGems3/gpugems3_ch20.html\nfor further details.\n(c) Oscar Meruvia-Pastor, Daniel Rypl.\nAll rights reserved. This content is\nexcluded from our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Test visibility (Z-buffer),\nupdate frame buffer color\n\n- Compute per-pixel color\n\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n- For each pixel,\ntest 3 edge equations\n- if all pass, draw pixel\n\n- Compute per-pixel color\n- Test visibility (Z-buffer),\nupdate frame buffer color\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Perform projection of vertices\n- Rasterize triangle: find which\npixels should be lit\n- Compute per-pixel color\n- Test visibility,\nupdate frame buffer color\n- Store minimum distance to camera\nfor each pixel in \"Z-buffer\"\n- ~same as tmin in ray casting!\n- if newz < zbuffer[x,y]\nzbuffer[x,y]=new_z\nframebuffer[x,y]=new_color\nModern Graphics Pipeline\nZ buffer\nframe buffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFor each triangle\ntransform into eye space\n(perform projection)\nsetup 3 edge equations\nfor each pixel x,y\nif passes all edge equations\ncompute z\nif z<zbuffer[x,y]\nzbuffer[x,y]=z\nframebuffer[x,y]=shade()\nModern Graphics Pipeline\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFor each triangle\ntransform into eye space\n(perform projection)\nsetup 3 edge equations\nfor each pixel x,y\nif passes all edge equations\ncompute z\nif z<zbuffer[x,y]\nzbuffer[x,y]=z\nframebuffer[x,y]=shade()\nModern Graphics Pipeline\nQuestions?\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility (Z-buffer),\nupdate frame buffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nProjection\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility (Z-buffer),\nupdate frame buffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Orthographic\n\n- Perspective\nOrthographic vs. Perspective\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nPerspective in 2D\nThis image is in the public domain. Source: openclipart\n\nPerspective in 2D\nThe projected point in\nhomogeneous\ncoordinates\n(we just added w=1):\nThis image is in the public domain. Source: openclipart\n\nPerspective in 2D\nProjectively\nequivalent\nThis image is in the public domain. Source: openclipart\n\nPerspective in 2D\nWe'll just copy z to w, and\nget the projected point\nafter homogenization!\nThis image is in the public domain. Source: openclipart\n\n- Trivial: Just ass another dimension y and treat it like x\n- Different fields of view and non-square image aspect\nratios can be accomplished by simple scaling of the x\nand y axes.\nExtension to 3D\n\n- These projections matrices work perfectly in the\nsense that you get the proper 2D projections of 3D\npoints.\n- However, since we are flattening the scene onto the\nz=1 plane, we've lost all information about the\ndistance to camera.\n- We need the distance for Z buffering, i.e., figuring out\nwhat is in front of what!\nCaveat\n\nBasic Idea: store 1/z\n\n- z' = 1 before homogenization\n- z'=1/z after homogenization\nBasic Idea: store 1/z\n\n- We can transform the frustum by a modified\nprojection in a way that makes it a square (cube in\n3D) after division by w'.\nFull Idea: Remap the View Frustum\nx\nz\nx'/w'\nz'/w'\nviewpoint\nview frustum\n(visible part of the scene)\n\nThe final image is obtained by merely\ndropping the z coordinate after\nprojection (orthogonal projection)\n- We can transform the frustum by a modified\nprojection in a way that makes it a square (cube in\n3D) after division by w'.\nThe View Frustum in 2D\nx\nz\nx'/w'\nz'/w'\n\n- (In 3D this is a truncated pyramid.)\nThe View Frustum in 2D\nimage xmin\nimage xmax\n\n- Far and near are kind of arbitrary\n- They bound the depth storage precision\nThe View Frustum in 2D\nimage xmin\nimage xmax\n\n- Point of the exercise: This gives screen coordinates\nand depth values for Z-buffering with unified math\n- Caveat: OpenGL and DirectX define Z differently [0,1] vs.[-1,1]\nThe Canonical View Volume\nx = -1\nx = 1\nz = -1\nz = 1\n\nOpenGL Form of the Projection\nInput point in view\ncoordinates\nHomogeneous coordinates\nwithin canonical view volume\n\n- z'=(az+b)/z =a+b/z\n- where a & b depend on near & far\n- Similar enough to our basic idea:\n- z'=1/z\nOpenGL Form of the Projection\n\n- Details/more intuition in handout\n- \"Understanding Projections and Homogenous\nCoordinates\"\nOpenGL Form of the Projection\n\n- Perform rotation/translation/other transforms to put\nviewpoint at origin and view direction along z axis\n- This is the OpenGL \"modelview\" matrix\n\n- Combine with projection matrix (perspective or\northographic)\n- Homogenization achieves foreshortening\n- This is the OpenGL \"projection\" matrix\n\n- Corollary: The entire transform from object space to\ncanonical view volume [-1,1]3 is a single matrix\nRecap: Projection\n\n- Perform rotation/translation/other transforms to put\nviewpoint at origin and view direction along z axis\n- This is the OpenGL \"modelview\" matrix\n\n- Combine with projection matrix (perspective or\northographic)\n- Homogenization achieves foreshortening\n- This is the OpenGL \"projection\" matrix\n\n- Corollary: The entire transform from object space to\ncanonical view volume [-1,1]3 is a single matrix\nRecap: Projection\nQuestions?\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n- We now have screen\ncoordinates\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility (Z-buffer),\nupdate frame buffer\n(c) Khronos Group. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Primitives are \"continuous\" geometric objects;\nscreen is discrete (pixels)\n2D Scan Conversion\n\n- Primitives are \"continuous\" geometric objects;\nscreen is discrete (pixels)\n- Rasterization computes a discrete approximation in\nterms of pixels (how?)\n2D Scan Conversion\n\n- The triangle's 3D edges project to line segments in\nthe image (thanks to planar perspective)\n- Lines map to lines, not curves\nEdge Functions\n\nEdge Functions\n- The triangle's 3D edges project to line segments in\nthe image (thanks to planar perspective)\n- The interior of the triangle is the set of points that is\ninside all three halfspaces defined by these lines\n\nEdge Functions\n- The triangle's 3D edges project to line segments in\nthe image (thanks to planar perspective)\n- The interior of the triangle is the set of points that is\ninside all three halfspaces defined by these lines\n\n- Compute E1, E2 , E3 coefficients from projected\nvertices\n- Called \"triangle setup\", yields ai, bi, ci for i=1,2,3\nBrute Force Rasterizer\n\nBrute Force Rasterizer\nProblem?\n- Compute E1, E2 , E3 coefficients from projected\nvertices\n- For each pixel (x, y)\n- Evaluate edge functions at pixel center\n- If all non-negative, pixel is in!\n\n- Compute E1, E2 , E3 coefficients from projected\nvertices\n- For each pixel (x, y)\n- Evaluate edge functions at pixel center\n- If all non-negative, pixel is in!\n\nBrute Force Rasterizer\nIf the triangle is\nsmall, lots of useless\ncomputation if we\nreally test all pixels\n\n- Improvement: Scan over only the pixels that overlap\nthe screen bounding box of the triangle\n- How do we get such a bounding box?\n- Xmin, Xmax, Ymin, Ymax of the projected triangle vertices\nEasy Optimization\n\nFor every triangle\nCompute projection for vertices, compute the Ei\nCompute bbox, clip bbox to screen limits\nFor all pixels in bbox\nEvaluate edge functions Ei\nIf all > 0\nFramebuffer[x,y ] = triangleColor\nRasterization Pseudocode\nBounding box clipping is easy,\njust clamp the coordinates to\nthe screen rectangle\nNote: No\nvisibility\n\nFor every triangle\nCompute projection for vertices, compute the Ei\nCompute bbox, clip bbox to screen limits\nFor all pixels in bbox\nEvaluate edge functions Ei\nIf all > 0\nFramebuffer[x,y ] = triangleColor\nRasterization Pseudocode\nBounding box clipping is easy,\njust clamp the coordinates to\nthe screen rectangle\nNote: No\nvisibility\nQuestions?\n\nFor every triangle\nCompute projection for vertices, compute the Ei\nCompute bbox, clip bbox to screen limits\nFor all pixels in bbox\nEvaluate edge functions aix + biy + ci\nIf all > 0\nFramebuffer[x,y ] = triangleColor\nCan We Do Better?\n\nFor every triangle\nCompute projection for vertices, compute the Ei\nCompute bbox, clip bbox to screen limits\nFor all pixels in bbox\nEvaluate edge functions aix + biy + ci\nIf all > 0\nFramebuffer[x,y ] = triangleColor\nCan We Do Better?\nThese are linear functions of\nthe pixel coordinates (x,y), i.e.,\nthey only change by a constant\namount when we step from x to\nx+1 (resp. y to y+1)\n\nFor every triangle\nComputeProjection\nCompute bbox, clip bbox to screen limits\nFor all scanlines y in bbox\nEvaluate all Ei's at (x0,y): Ei = aix0 + biy + ci\nFor all pixels x in bbox\nIf all Ei>0\n\nFramebuffer[x,y ] = triangleColor\nIncrement line equations: Ei += ai\n\n- We save ~two multiplications and\ntwo additions per pixel when the\ntriangle is large\nIncremental Edge Functions\n\nFor every triangle\nComputeProjection\nCompute bbox, clip bbox to screen limits\nFor all scanlines y in bbox\nEvaluate all Ei's at (x0,y): Ei = aix0 + biy + ci\nFor all pixels x in bbox\nIf all Ei>0\n\nFramebuffer[x,y ] = triangleColor\nIncrement line equations: Ei += ai\n\n- We save ~two multiplications and\ntwo additions per pixel when the\ntriangle is large\nIncremental Edge Functions\nCan also zig-zag to avoid\nreinitialization per scanline,\njust initialize once at x0, y0\n\n- For a really HC piece of rasterizer engineering, see\nthe hierarchical Hilbert curve rasterizer by McCool,\nWales and Moule.\n- (Hierarchical? We'll look at that next..)\nQuestions?\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- We compute the line equation for many useless\npixels\n- What could we do?\nCan We Do Even Better?\n\nIndeed, We Can Be Smarter\n?\n\n- Hierarchical rasterization!\n- Conservatively test blocks of pixels before\ngoing to per-pixel level (can skip large blocks at once)\n- Usually two levels\nIndeed, We Can Be Smarter\nConservative tests of\naxis-aligned blocks vs.\nedge functions are not\nvery hard, thanks to\nlinearity. See Akenine-\nMoller and Aila, Journal\nof Graphics Tools 10(3),\n2005.\n\n- Hierarchical rasterization!\n- Conservatively test blocks of pixels before\ngoing to per-pixel level (can skip large blocks at once)\n- Usually two levels\nIndeed, We Can Be Smarter\nCan also test if an entire\nblock is inside the\ntriangle; then, can skip\nedge functions tests for\nall pixels for even further\nspeedups.(Must still test\nZ, because they might\nstill be occluded.)\n\n- Henry Fuchs, Jack Goldfeather, Jeff Hultquist, Susan Spach, John\nAustin, Frederick Brooks, Jr., John Eyles and John Poulton, \"Fast\nSpheres, Shadows, Textures, Transparencies, and Image\nEnhancements in Pixel-Planes\", Proceedings of SIGGRAPH '85\n(San Francisco, CA, July 22-26, 1985). In Computer Graphics,\nv19n3 (July 1985), ACM SIGGRAPH, New York, NY, 1985.\n- Juan Pineda, \"A Parallel Algorithm for Polygon Rasterization\",\nProceedings of SIGGRAPH '88 (Atlanta, GA, August 1-5, 1988).\nIn Computer Graphics, v22n4 (August 1988), ACM SIGGRAPH,\nNew York, NY, 1988. Figure 7: Image from the spinning teapot\nperformance test.\n- Marc Olano Trey Greer, \"Triangle Scan Conversion using 2D\nHomogeneous Coordinates\", Graphics Hardware 97\nhttp://www.cs.unc.edu/~olano/papers/2dh-tri/2dh-tri.pdf\nFurther References\n\n- Compute the boundary pixels using line rasterization\nOldschool Rasterization\n\n- Compute the boundary pixels using line rasterization\n- Fill the spans\nOldschool Rasterization\n\n- Compute the boundary pixels using line rasterization\n- Fill the spans\nOldschool Rasterization\nMore annoying to\nimplement than edge\nfunctions\n\nNot faster unless\ntriangles are huge\n\n- Compute the boundary pixels using line rasterization\n- Fill the spans\nOldschool Rasterization\nMore annoying to\nimplement than edge\nfunctions\n\nNot faster unless\ntriangles are huge\nQuestions?\n\nWhat if the pz is > eyez?\n(eyex, eyey, eyez)\nimage plane\nz axis → +\n\nWhat if the pz is < eyez?\n(eyex, eyey, eyez)\nimage plane\nz axis → +\n\nWhat if the pz = eyez?\n(eyex, eyey, eyez)\nimage plane\n???\nz axis → +\nWhen w' = 0, point projects to infinity\n(homogenization is division by w')\n\nA Solution: Clipping\n(eyex, eyey, eyez)\nimage plane\n\"clip\" geometry to\nview frustum, discard\noutside parts\nz axis → +\nz=near\nz=far\n\nClipping\nbottom\ntop\nright\nleft\nnear\nfar\n- Eliminate portions of objects\noutside the viewing frustum\n- View Frustum\n- boundaries of the image\nplane projected in 3D\n- a near & far\nclipping plane\n- User may define\nadditional clipping\nplanes\nLeonard McMillan, Computer Science at the University of North Carolina in Chapel Hill.\n\n- Avoid degeneracies\n- Don't draw stuff\nbehind the eye\n- Avoid division\nby 0 and overflow\nWhy Clip?\nz=near\nz=far\n\n- \"View Frustum Culling\"\n- Use bounding volumes/hierarchies to test whether any\npart of an object is within the view frustum\n- Need \"frustum vs. bounding volume\" intersection test\n- Crucial to do hierarchically when scene has lots of objects!\n- Early rejection (different from clipping)\nRelated Idea\nSee e.g. Optimized view\nfrustum culling\nalgorithms for bounding\nboxes, Ulf Assarsson\nand Tomas Moller,\njournal of graphics\ntools, 2000.\n(c) Oscar Meruvia-Pastor, Daniel Rypl. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- \"View Frustum Culling\"\n- Use bounding volumes/hierarchies to test whether any\npart of an object is within the view frustum\n- Need \"frustum vs. bounding volume\" intersection test\n- Crucial to do hierarchically when scene has lots of objects!\n- Early rejection (different from clipping)\nRelated Idea\nSee e.g. Optimized view\nfrustum culling\nalgorithms for bounding\nboxes, Ulf Assarsson\nand Tomas Moller,\njournal of graphics\ntools, 2000.\nQuestions?\n(c) Oscar Meruvia-Pastor, Daniel Rypl. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Idea: avoid projection (and division by zero) by\nperforming rasterization in 3D\n- Or equivalently, use 2D homogenous coordinates\n(w'=z after the projection matrix, remember)\n\n- Motivation: clipping is annoying\n\n- Marc Olano, Trey Greer: Triangle scan conversion\nusing 2D homogeneous coordinates, Proc. ACM\nSIGGRAPH/Eurographics Workshop on Graphics\nHardware 1997\nHomogeneous Rasterization\n\nHomogeneous Rasterization\n2D rasterization\n\nHomogeneous Rasterization\n2D rasterization\n3D (homogenous)\nrasterization\n- Replace 2D edge equation by 3D plane equation\n- Plane going through 3D edge and viewpoint\n- Still a halfspace, just 3D\n\n- Replace 2D edge equation by 3D plane equation\n- Treat pixels as 3D points (x, y, 1) on image plane, test for\ncontainment in 3 halfspaces just like edge functions\nHomogeneous Rasterization\n2D rasterization\n3D (homogenous)\nrasterization\n\nGiven 3D triangle\nsetup plane equations\n(plane through viewpoint & triangle edge)\nFor each pixel x,y\ncompute plane equations for (x,y,1)\nif all pass, draw pixel\nHomogeneous Rasterization\n3D triangle\n2D pixel\n(x, y, 1)\nplane equation\nplane equation\n\n- Works for triangles behind eye\n- Still linear, can evaluate incrementally/hierarchically\nlike 2D\nHomogeneous Rasterization\n3D triangle\n2D pixel\n(x', y', 1)\n\n- Rasterizes with plane tests instead of edge tests\n- Removes the need for clipping!\nHomogeneous Rasterization Recap\n3D triangle\n2D pixel\n(x', y', 1)\n\n- Rasterizes with plane tests instead of edge tests\n- Removes the need for clipping!\nHomogeneous Rasterization Recap\n3D triangle\n2D pixel\n(x', y', 1)\nQuestions?\n\nModern Graphics Pipeline\n- Perform projection of\nvertices\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility, update frame\nbuffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- Modern graphics hardware enables the execution of\nrather complex programs to compute the color of every\nsingle pixel\n- More later\nPixel Shaders iridescence\n\niridescence\nProcedural texture,\nAnisotropic brdf Translucence Backlighting\nTranslucence\nBacklighting\n(c) NVIDIA. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Perform projection of\nvertices\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility, update frame\nbuffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- How do we know which parts are visible/in front?\nVisibility\n\n- Maintain intersection with closest object\nRay Casting\n\n- In ray casting, use intersection with closest t\n- Now we have swapped the loops (pixel, object)\n- What do we do?\nVisibility\n\n- In addition to frame buffer (R, G, B)\n- Store distance to camera (z-buffer)\n- Pixel is updated only if newz is closer\nthan z-buffer value\nZ buffer\n\nFor every triangle\nCompute Projection, color at vertices\nSetup line equations\nCompute bbox, clip bbox to screen limits\nFor all pixels in bbox\nIncrement line equations\nCompute curentZ\nCompute currentColor\nIf all line equations>0 //pixel [x,y] in triangle\n\nIf currentZ<zBuffer[x,y] //pixel is visible\nFramebuffer[x,y]=currentColor\nzBuffer[x,y]=currentZ\nZ-buffer pseudo code\n\nWorks for hard cases!\n\n- How do we get Z?\n- Texture Mapping?\nMore questions for next time\n\n- Next time:\nScreen-space interpolation, visibility, shading\nThat's All For Today!\nUncharted 2 by Naughty Dog / Sony\nScreenshot from the video game Uncharted 2 has been removed due to copyright restrictions.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Graphics Pipeline and Rasterization II",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-837-computer-graphics-fall-2012/e8a6314637f3ca051209b068984d3f46_MIT6_837F12_Lec22.pdf",
      "content": "Graphics Pipeline & Rasterization II\nMIT EECS 6.837\nComputer Graphics\nWojciech Matusik\nImage removed due to copyright restrictions.\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n\n- Compute per-pixel color\n\n- Test visibility (Z-buffer),\nupdate frame buffer color\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Project vertices to 2D\n(image)\n\n- Rasterize triangle: find\nwhich pixels should be lit\n- For each pixel,\ntest 3 edge equations\n- if all pass, draw pixel\n\n- Compute per-pixel color\n- Test visibility (Z-buffer),\nupdate frame buffer color\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\n- Perform projection of vertices\n- Rasterize triangle: find which\npixels should be lit\n- Compute per-pixel color\n- Test visibility,\nupdate frame buffer color\n- Store minimum distance to camera\nfor each pixel in \"Z-buffer\"\n- ~same as tmin in ray casting!\n- if new_z < zbuffer[x,y]\nzbuffer[x,y]=new_z\nframebuffer[x,y]=new_color\nZ buffer\nframe buffer\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\nFor each triangle\ntransform into eye space\n(perform projection)\nsetup 3 edge equations\nfor each pixel x,y\nif passes all edge equations\ncompute z\nif z<zbuffer[x,y]\nzbuffer[x,y]=z\nframebuffer[x,y]=shade()\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nModern Graphics Pipeline\nFor each triangle\ntransform into eye space\n(perform projection)\nsetup 3 edge equations\nfor each pixel x,y\nif passes all edge equations\ncompute z\nif z<zbuffer[x,y]\nzbuffer[x,y]=z\nframebuffer[x,y]=shade()\nQuestions?\n(c) source unknown. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n(c) Khronos Group. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/.\n\n- How do we get that Z value for each pixel?\n- We only know z at the vertices...\n- (Remember, screen-space z is actually z'/w')\n- Must interpolate from vertices into triangle interior\nInterpolation in Screen Space\nFor each triangle\nfor each pixel (x,y)\nif passes all edge equations\ncompute z\nif z<zbuffer[x,y]\nzbuffer[x,y]=z\nframebuffer[x,y]=shade()\n\nInterpolation in Screen Space\n- Also need to interpolate color, normals, texture coordinates,\netc. between vertices\n‒\nWe did this with barycentrics in ray casting\n-\nLinear interpolation in object space\n‒\nIs this the same as linear interpolation on the screen?\n\nInterpolation in Screen Space\nTwo regions of same\nsize in world space\n\nInterpolation in Screen Space\nThe farther region\nshrinks to a smaller\narea of the screen\nTwo regions of same\nsize in world space\n\nNope, Not the Same\n- Linear variation in world space does not yield linear\nvariation in screen space due to projection\n- Think of looking at a checkerboard at a steep angle; all\nsquares are the same size on the plane, but not on screen\nlinear screen-space\n(\"Gouraud\") interpolation\nBAD\nPerspective-correct\nInterpolation\nHead-on view\nThis image is in the public domain. Source: Wikipedia.\n\nBack to the basics: Barycentrics\n- Barycentric coordinates for a triangle (a, b, c)\n\n- Remember,\n\n- Barycentrics are very general:\n- Work for x, y, z, u, v, r, g, b\n- Anything that varies linearly in object space\n- including z\n\nBasic strategy\n- Given screen-space x', y'\n- Compute barycentric coordinates\n- Interpolate anything specified at the three vertices\n\nBasic strategy\n- How to make it work\n- start by computing x', y' given barycentrics\n- invert\n- Later: shortcut barycentrics, directly build interpolants\n\nFrom barycentric to screen-space\n- Barycentric coordinates for a triangle (a, b, c)\n\n- Remember,\n\n- Let's project point P by projection matrix C\na', b', c' are the\nprojected\nhomogeneous\nvertices before\ndivision by w\n\nProjection\n- Let's use simple formulation of projection going\nfrom 3D homogeneous coordinates to 2D\nhomogeneous coordinates\n\n- No crazy near-far or storage of 1/z\n- We use ' for screen space coordinates\n\nFrom barycentric to screen-space\n- From previous slides:\n\n- Seems to suggest it's linear in screen space.\nBut it's homogenous coordinates\na', b', c' are the\nprojected\nhomogeneous\nvertices\n\nFrom barycentric to screen-space\n- From previous slides:\n\n- Seems to suggest it's linear in screen space.\nBut it's homogenous coordinates\n- After division by w, the (x,y) screen coordinates are\na', b', c' are the\nprojected\nhomogeneous\nvertices\n\nRecap: barycentric to screen-space\n\nFrom screen-space to barycentric\n\n- It's a projective mapping from\nthe barycentrics onto screen coordinates!\n- Represented by a 3x3 matrix\n- We'll take the inverse mapping to get from (x, y, 1)\nto the barycentrics!\n\n- Recipe\n- Compute projected homogeneous coordinates a', b', c'\n- Put them in the columns of a matrix, invert it\n- Multiply screen coordinates (x, y, 1) by inverse matrix\n- Then divide by the sum of the resulting coordinates\n- This ensures the result is sums to one like barycentrics should\n- Then interpolate value (e.g. Z) from vertices using them!\nFrom Screen to Barycentrics\nprojective\nequivalence\n\nFrom Screen to Barycentrics\n\n- Notes:\n- matrix is inverted once per triangle\n- can be used to interpolate z, color, texture coordinates, etc.\n\nPseudocode - Rasterization\nFor every triangle\nComputeProjection\nCompute interpolation matrix\nCompute bbox, clip bbox to screen limits\nFor all pixels x,y in bbox\nTest edge functions\nIf all Ei>0\ncompute barycentrics\ninterpolate z from vertices\nif z < zbuffer[x,y ]\ninterpolate UV coordinates from vertices\nlook up texture color kd\nFramebuffer[x,y ] = kd\n//or more complex shader\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nPseudocode - Rasterization\nFor every triangle\nComputeProjection\nCompute interpolation matrix\nCompute bbox, clip bbox to screen limits\nFor all pixels x,y in bbox\nTest edge functions\nIf all Ei>0\ncompute barycentrics\ninterpolate z from vertices\nif z < zbuffer[x,y ]\ninterpolate UV coordinates from vertices\nlook up texture color kd\nFramebuffer[x,y ] = kd\n//or more complex shader\nQuestions?\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nThe infamous half pixel\n- I refuse to teach it, but it's an annoying issue you\nshould know about\n- Do a line drawing of a rectangle\nfrom [top, right] to [bottom,left]\n- Do we actually draw the columns/rows of pixels?\nLeonard McMillan, Computer Science at the University of North Carolina in Chapel Hill.\n\nThe infamous half pixel\n- Displace by half a pixel so that top, right, bottom, left\nare in the middle of pixels\n- Just change the viewport transform\nLeonard McMillan, Computer Science at the University of North Carolina in Chapel Hill.\n\nQuestions?\n\nSupersampling\n✔\n✘\n- Trivial to do with rasterization as well\n- Often rates of 2x to 8x\n- Requires to compute per-pixel average at the end\n- Most effective against edge jaggies\n- Usually with jittered sampling\n- pre-computed pattern for a big block of pixels\n\n1 Sample / Pixel\n\n4 Samples / Pixel\n\n16 Samples / Pixel\n\n100 Samples / Pixel\nEven this\nsampling rate\ncannot get rid\nof all aliasing\nartifacts!\n\nWe are really\nonly pushing\nthe problem\nfarther.\n\n- Problem\n- Shading is very expensive today (complicated shaders)\n- Full supersampling has linear cost in #samples (k*k)\n- Goal: High-quality edge antialiasing at lower cost\n- Solution\n- Compute shading only once per pixel for each primitive,\nbut resolve visibility at \"sub-pixel\" level\n- Store (k*width, k*height) frame and z buffers, but share shading\nresults between sub-pixels within a real pixel\n- When visibility samples within a pixel hit different\nprimitives, we get an average of their colors\n- Edges get antialiased without large shading cost\nRelated Idea: Multisampling\n\nMultisampling, Visually\n= sub-pixel visibility sample\nOne pixel\n\nOne pixel\nMultisampling, Visually\n= sub-pixel visibility sample\n\nMultisampling, Visually\n= sub-pixel visibility sample\nThe color is only\ncomputed once\nper pixel per\ntriangle and\nreused for all the\nvisibility samples\nthat are covered\nby the triangle.\nOne pixel\n\nSupersampling, Visually\n= sub-pixel visibility sample\nWhen\nsupersampling,\nwe compute\ncolors\nindependently for\nall the visibility\nsamples.\nOne pixel\n\nMultisampling Pseudocode\nFor each triangle\nFor each pixel\nif pixel overlaps triangle\ncolor=shade() // only once per pixel!\nfor each sub-pixel sample\ncompute edge equations & z\nif subsample passes edge equations\n&& z < zbuffer[subsample]\nzbuffer[subsample]=z\nframebuffer[subsample]=color\n\nFor each triangle\nFor each pixel\nif pixel overlaps triangle\ncolor=shade() // only once per pixel!\nfor each sub-pixel sample\ncompute edge equations & z\nif subsample passes edge equations\n&& z < zbuffer[subsample]\nzbuffer[subsample]=z\nframebuffer[subsample]=color\nAt display time: //this is called \"resolving\"\nFor each pixel\ncolor = average of subsamples\nMultisampling Pseudocode\n\nMultisampling vs. Supersampling\n- Supersampling\n- Compute an entire image at a higher resolution, then\ndownsample (blur + resample at lower res)\n- Multisampling\n- Supersample visibility, compute expensive shading only\nonce per pixel, reuse shading across visibility samples\n- But Why?\n- Visibility edges are where supersampling really works\n- Shading can be prefiltered more easily than visibility\n- This is how GPUs perform antialiasing these days\n\nQuestions?\n\nExamples of Texture Aliasing\npoint sampling\nMagnification\nMinification\n\nTexture Filtering\n- Problem: Prefiltering is impossible when you can\nonly take point samples\n- This is why visibility (edges) need supersampling\n- Texture mapping is simpler\n- Imagine again we are looking at an infinite textured plane textured plane\n\nTexture Filtering\n- We should pre-filter image function before sampling\n- That means blurring the image function with a low-pass\nfilter (convolution of image function and filter) textured plane\nLow-pass filter\n\nTexture Filtering\n- We can combine low-pass and sampling\n- The value of a sample is the integral of the product of the\nimage f and the filter h centered at the sample location\n- \"A local average of the image f weighted by the filter h\" textured plane\nLow-pass filter\n\nTexture Filtering\n- Well, we can just as well change variables and\ncompute this integral on the textured plane instead\n- In effect, we are projecting the pre-filter onto the plane textured plane\nLow-pass filter\n\nTexture Filtering\n- Well, we can just as well change variables and\ncompute this integral on the textured plane instead\n- In effect, we are projecting the pre-filter onto the plane\n- It's still a weighted average of the texture under filter textured plane\nLow-pass filter\n\n- Must still integrate product of projected filter and\ntexture - That doesn't sound any easier...\nTexture Pre-Filtering, Visually\nimage\nplane\ntextured surface\n(texture map)\nimage-space filter\nimage-space filter\nprojected onto plane\nImage adapted from\nMcCormack et al.\n\nSolution: Precomputation\n- We'll precompute and store a set of prefiltered results\nfrom each texture with different sizes of prefilters\n\nSolution: Precomputation\n- We'll precompute and store a set of prefiltered results\nfrom each texture with different sizes of prefilters\n\nSolution: Precomputation\n- We'll precompute and store a set of prefiltered results\nfrom each texture with different sizes of prefilters\n- Because it's low-passed, we can also subsample\n\nSolution: Precomputation\n- We'll precompute and store a set of prefiltered results\nfrom each texture with different sizes of prefilters\n- Because it's low-passed, we can also subsample\n\nThis is Called \"MIP-Mapping\"\n- Construct a pyramid\nof images that are\npre-filtered and\nre-sampled at\n1/2, 1/4, 1/8, etc.,\nof the original\nimage's sampling\n- During rasterization\nwe compute the index of the decimated image that is sampled at\na rate closest to the density of our desired sampling rate\n- MIP stands for multum in parvo which means\nmany in a small place\n\nMIP-Mapping\n- When a pixel wants an integral of the pre-filtered\ntexture, we must find the \"closest\" results from the\nprecomputed MIP-map pyramid\n- Must compute the \"size\" of\nthe projected pre-filter in\nthe texture UV domain\nProjected pre-filter\n\nMIP-Mapping\n- Simplest method: Pick the scale closest,\nthen do usual reconstruction on that level\n(e.g. bilinear between 4 closest texture pixels)\nProjected pre-filter\nclosest-available\nfilter in pyramid\nCorresponding\npyramid level\n\nMIP-Mapping\n- Simplest method: Pick the scale closest,\nthen do usual reconstruction on that level\n(e.g. bilinear between 4 closest texture pixels)\n- Problem: discontinuity when switching scale\nProjected pre-filter\nclosest-available\nfilter in pyramid\nCorresponding\npyramid level\n\n2 closest-available\nfilters in pyramid\nTri-Linear MIP-Mapping\n- Use two closest scales,\ncompute reconstruction results from both,\nand linearly interpolate between them\nProjected pre-filter\nBlurrier\npyramid level\nSharper\npyramid level\n\nProjected pre-filter\nTri-Linear MIP-Mapping\n- Use two closest scales,\ncompute reconstruction results from both,\nand linearly interpolate between them\n- Problem: our filter might not be circular, because of\nforeshortening\n\nProjected pre-filter\nAnisotropic filtering\n- Approximate Elliptical filter with multiple circular\nones (usually 5)\n- Perform trilinear lookup at each one\n- i.e. consider five times eight values\n- fair amount of computation\n- this is why graphics hardware\nhas dedicated units to compute\ntrilinear mipmap reconstruction\n\nMIP Mapping Example\nMIP Mapped (Tri-Linear)\nNearest Neighbor\n\nMIP Mapping Example\nnearest neighbor/\npoint sampling\nmipmaps & linear interpolation\n(tri-linear)\n\nQuestions\n\nStoring MIP Maps\n- Can be stored compactly: Only 1/3 more space!\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nFinding the MIP Level\n- Often we think of the\npre-filter as a box\n- What is the projection\nof the square\npixel \"window\"\nin texture space?\nProjected pre-filter\n\nFinding the MIP Level\n- Often we think of the\npre-filter as a box\n- What is the projection\nof the square\npixel \"window\"\nin texture space?\n- Answer is in the partial\nderivatives px and py\nof (u,v) w.r.t. screen (x,y)\nProjected pre-filter\nProjection of pixel center\npy = (du/dy, dv/dy)\npx = (du/dx, dv/dx)\n\nFor isotropic trilinear mipmapping\n- No right answer,\ncircular approximation\n- Two most common\napproaches are\n- Pick level according to\nthe length (in texels) of\nthe longer partial\n\n- Pick level according to\nthe length of their sum\nProjected pre-filter\nProjection of pixel center\npy = (du/dy, dv/dy)\npx = (du/dx, dv/dx)\nw x h\n\nAnisotropic filtering\n- Pick levels according\nto smallest partial\n- well, actually max of the\nsmallest and the largest/5\n- Distribute circular\n\"probes\" along\nlongest one\n- Weight them\nby a Gaussian\nProjected pre-filter\nProjection of pixel center\npy = (du/dy, dv/dy)\npx = (du/dx, dv/dx)\n\nHow Are Partials Computed?\n- You can derive closed form formulas based on the uv\nand xyw coordinates of the vertices...\n- This is what used to be done\n- ..but shaders may compute texture coordinates\nprogrammatically, not necessarily interpolated\n- No way of getting analytic derivatives!\n\n- In practice, use finite differences\n- GPUs process pixels in blocks of (at least) 4 anyway\n- These 2x2 blocks are called quads\n\nImage Quality Comparison\nanisotropic filtering\ntrilinear mipmapping\n(excessive blurring)\n\n- Paul Heckbert published seminal work on texture\nmapping and filtering in his master's thesis (!)\n- Including EWA\n- Highly recommended reading!\n- See http://www.cs.cmu.edu/~ph/texfund/texfund.pdf\n- More reading\n- Feline: Fast Elliptical Lines for\nAnisotropic Texture Mapping,\nMcCormack, Perry, Farkas, Jouppi\nSIGGRAPH 1999\n- Texram: A Smart Memory for Texturing\nSchilling, Knittel, Strasser,. IEEE CG&A, 16(3): 32-41\nFurther Reading\nArf!\n(c) Marc Levoy. All rights reserved. This\ncontent is excluded from our Creative\nCommons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\nImage removed due to copyright restrictions.\n\nRay Casting\nFor each pixel\nFor each object\n- Ray-centric\n- Needs to store scene in\nmemory\n- (Mostly) Random access\nto scene\nRendering Pipeline\nFor each triangle\nFor each pixel\n- Triangle centric\n- Needs to store image\n(and depth) into memory\n- (Mostly) random access to\nframe buffer\nRay Casting vs. Rendering Pipeline\nWhich is smaller? Scene or Frame?\nFrame\nWhich is easiest to access randomly?\nFrame because regular sampling\n\nRay Casting\nFor each pixel\nFor each object\n-\nWhole scene must be in memory\n-\nNeeds spatial acceleration to be\nefficient\n+ Depth complexity: no computation\nfor hidden parts\n+ Atomic computation\n+ More general, more flexible\n- Primitives, lighting effects,\nadaptive antialiasing\nRendering Pipeline\nFor each triangle\nFor each pixel\n-\nHarder to get global illumination\n-\nNeeds smarter techniques to address\ndepth complexity (overdraw)\n+ Primitives processed one at a time\n+ Coherence: geometric transforms for\nvertices only\n+ Good bandwidth/computation ratio\n+ Minimal state required, good memory\nbehavior\nRay Casting vs. Rendering Pipeline\n\nhttp://xkcd.com/386/\nImage removed due to copyright restrictions - please see the link above for further details.\n\nBad example\nImage removed due to copyright restrictions -- please see\nhttps://blogs.intel.com/intellabs/2007/10/10/real_time_raytracing_the_end_o/ for further details.\n\nRay-triangle intersection\n- Triangle ABC\n- Ray O+t*D\n- Barycentric coordinates α, β, γ\n- Ray-triangle intersection\n\n- or in matrix form\n\nRay-triangle\n\n- Cramer's rule (where | | is the determinant)\n\nDeterminant\n- Cross product and dot product\n- i.e., for a matrix with 3 columns vectors: M=UVW\n\nBack to ray-triangle\n\nRay-triangle recap\n\n- And\n\n- Intersection if\n\nRasterization\n- Viewpoint is known and fixed\n- Let's extract what varies per pixel\n\n- Only D!\n\nRasterization\n\n- Cache redundant computation independent of D:\n\n- And for each pixel\nEquivalent to the setup of edge equations and\ninterpolants in rasterization\nPer-pixel calculation of\nedge equations and z (=t)\n\nConclusions\n- Rasterization and ray casting do the same thing\n- Just swap the two loops\n- And cache what is independent of pixel location\n\nRay casting (Python)\n\nMain loops\n\nGood References\n- http://www.tomshardware.com/reviews/ray-tracing-\nrasterization,2351.html\n- http://c0de517e.blogspot.com/2011/09/raytracing-\nmyths.html\n- http://people.csail.mit.edu/fredo/tmp/rendering.pdf\n\nGraphics Hardware\n- High performance through\n- Parallelism\n- Specialization\n- No data dependency\n- Efficient pre-fetching\n\n- More next week\nG\nR\nT\nF\nD\nG\nR\nT\nF\nD\nG\nR\nT\nF\nD\nG\nR\nT\nF\nD\ntask\nparallelism\ndata parallelism\n\nQuestions?\n\nMovies\nboth rasterization and ray tracing\nImages removed due to copyright restrictions.\n\nGames\nrasterization\nImages removed due to copyright restrictions.\n\nSimulation\nrasterization\n(painter for a long time)\nImages removed due to copyright restrictions.\n\nCAD-CAM & Design\nrasterization for GUI,\nanything for final image\nImages removed due to copyright restrictions.\n\nArchitecture\nray-tracing, rasterization with\npreprocessing for complex lighting\nImages removed due to copyright restrictions.\n\nVirtual Reality\nrasterization\nImages removed due to copyright restrictions.\n\nVisualization\nmostly rasterization,\ninteractive ray-tracing is starting\nImages removed due to copyright restrictions.\n\nMedical Imaging\nsame as\nvisualization\nImages removed due to copyright restrictions.\n\nQuestions?\n\n- Transparency\n- Difficult, pretty much unsolved!\n- Alternative\n- Reyes (Pixar's Renderman)\n- deferred shading\n- pre-Z pass\n- tile-based rendering\n\n- Shadows\n- Next time\n- Reflections, global illumination\nMore issues\n\nTransparency\n- Triangles and pixels can have transparency (alpha)\n- But the result depends on the order in which triangles\nare sent\n\n- Big problem: visibility\n- There is only one depth stored per pixel/sample\n- transparent objects involve multiple depth\n- full solutions store a (variable-length) list of visible objects\nand depth at each pixel\n- see e.g. the A-buffer by Carpenter\nhttp://portal.acm.org/citation.cfm?id=808585\n\nDeferred shading\n- Avoid shading fragments that are eventually hidden\n- shading becomes more and more costly\n- First pass: rasterize triangles, store information such\nas normals, BRDF per pixel\n- Second pass: use stored information to compute\nshading\n\n- Advantage: no useless shading\n- Disadvantage: storage, antialiasing is difficult\n\nPre z pass\n- Again, avoid shading hidden fragment\n- First pass: rasterize triangles, update only z buffer,\nnot color buffer\n- Second pass: rasterize triangles again, but this time,\ndo full shading\n\n- Advantage over deferred shading: less storage, less\ncode modification, more general shading is possible,\nmultisampling possible\n- Disadvantage: needs to rasterize twice\n\nTile-based rendering\n- Problem: framebuffer is a lot of memory, especially\nwith antialiasing\n- Solution: render subsets of the screen at once\n- For each tile of pixels\n- For each triangle\n- for each pixel\n\n- Might need to handle a triangle in multiple tiles\n- redundant computation for projection and setup\n- Used in mobile graphics cards\n\nReyes - Pixar's Renderman\n- Cook et al. http://graphics.pixar.com/library/Reyes/\n- Based on micropolygons\n- each primitive gets diced into polygons as small as a pixel\n- Enables antialiasing motion blur, depth of field\n- Shading is computed at the micropolygon level,\nnot pixel\n- related to multisampling: shaded value will be used for\nmultiple visibility sample\n\nDicing and rasterization\n(c) source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nReyes - Pixar's Renderman\n- Tile-based to save memory and maximize texture\ncoherence\n- Order-independent transparency\n- stores list of fragments and depth per pixel\n- Micropolygons get rasterized in space, lens and time\n- frame buffer has multiple samples per pixel\n- each sample has lens coordinates and time value\n\nReyes - ignoring transparency\n- For each tile of pixels\n- For each geometry\n- Dice into micropolygons adaptively\n- For each micropolygon\n- compute shaded value\n- For each sample in tile at coordinates x, y, u, v, t\n» reproject micropolygon to its position at time t, and lens position uv\n» determine if micropolygon overlaps samples\n» if yes, test visibility (z-buffer)\n» if z buffer passes, update framebuffer\n\nREYES results\n(c) ACM. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see http://ocw.mit.edu/help/faq-fair-use/.\n\nQuestions?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.837 Computer Graphics\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}