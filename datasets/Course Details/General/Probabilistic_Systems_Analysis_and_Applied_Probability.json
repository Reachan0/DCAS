{
  "course_name": "Probabilistic Systems Analysis and Applied Probability",
  "course_description": "Welcome to 6.041/6.431, a subject on the modeling and analysis of random phenomena and processes, including the basics of statistical inference. Nowadays, there is broad consensus that the ability to think probabilistically is a fundamental component of scientific literacy. For example:\n\nThe concept of statistical significance (to be touched upon at the end of this course) is considered by the Financial Times as one of “The Ten Things Everyone Should Know About Science”.\nA recent Scientific American article argues that statistical literacy is crucial in making health-related decisions.\nFinally, an article in the New York Times identifies statistical data analysis as an upcoming profession, valuable everywhere, from Google and Netflix to the Office of Management and Budget.\n\nThe aim of this class is to introduce the relevant models, skills, and tools, by combining mathematics with conceptual understanding and intuition.",
  "topics": [
    "Engineering",
    "Systems Engineering",
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Systems Engineering",
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1 hour / session\n\nRecitations: 2 sessions / week, 1 hour / session\n\nTutorials: 1 session / week, 1 hour / session\n\nGeneral Information\n\nWelcome to 6.041/6.431! This fundamental subject is concerned with the nature, formulation, and analysis of probabilistic situations. No previous experience with probability is assumed. This course is fun, but also demanding.\n\nStudents intending to take the undergraduate version of the course need to sign up for 6.041, while those intending to take the graduate version should sign up for 6.431, which includes full participation in 6.041, together with some additional homework problems, additional topics, and possibly different quiz and exam questions.\n\n6.041/6.431 has three types of class sessions: lectures, recitations, and tutorials. The lectures and recitations each meet twice a week. In addition, there will be a tutorial once a week, which is not mandatory, but is highly recommended.\n\nLectures serve to introduce new concepts. They have an overview character, but also include some derivations and motivating applications. In recitation, your instructor elaborates on the theory, works through new examples with your participation, and answers your questions about them. In tutorial, you discuss and solve new examples with a little help from your classmates and your instructor. Tutorials are active sessions to help you develop confidence in thinking about probabilistic situations in real time. Tutorials are highly recommended; past students have found them to be very helpful.\n\nPrerequisites\n\nThe prerequisite for 6.041/6.431 is\n18.02\n, or a year of college-level calculus for those with undergraduate degrees from other universities.\n\nText\n\nThe text for this course is:\n\nBertsekas, Dimitri, and John Tsitsiklis.\nIntroduction to Probability\n. 2nd ed. Athena Scientific, 2008. ISBN: 9781886529236.\n\nSolutions to end-of-chapter problems are available: (\nPDF - 1.5MB\n)\n\nA few of these problems will be covered in recitation and tutorial. The remaining ones can be used for self-study (for best results, always try to solve a problem on your own, before reading the solution).\n\nAdditionally, the following books may be useful as references. They cover many of the topics in this course, although in a different style. You may wish to consult them to get a different perspective on particular topics.\n\nDrake, Alvin.\nFundamentals of Applied Probability Theory\n. McGraw-Hill, 1967. ISBN: 9780070178151.\n\nRoss, Sheldon.\nA First Course in Probability\n. 8th ed. Prentice Hall, 2009. ISBN: 9780136033134.\n\nGrading\n\nWe grade homework, but often only a small, randomly chosen subset of the problems. We do post detailed solutions on the course Web site. Your TA is available to discuss your work with you, both before and after it is due. You may encounter difficulty figuring out where your own solution of a homework problem went astray. There are many ways to approach most probability problems. Just agreeing with our problem solutions may not explain why your approach didn't work.\n\nGrades will be determined by your work in all aspects of this subject.\n\nACTIVITIES\n\nPERCENTAGES\n\nFirst quiz\n\n20%\n\nSecond quiz\n\n28%\n\nFinal exam\n\n37%\n\nHomework (best 9 out of 10 problem sets)\n\n10%\n\nAttendance and participation\n\n5%\n\nStudy Habits\n\nIn order to get the most out of the course, it is important to not fall behind. It is also important to read the text carefully before attempting to solve the homework problems. A very good practice is to review the lecture notes before attending the next lecture or recitation; this way, recitations and tutorials will be much more informative and meaningful.",
  "files": [
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn01_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/587d22d7cf4ba6c1fc79dac7aaaf29d9_MIT6_041F10_assn01_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 1: Solutions\nDue: September 15, 2010\n1.\n(a) A ∪B ∪C\n(b) (A ∩Bc ∩Cc) ∪(Ac ∩B ∩Cc) ∪(Ac ∩Bc ∩C) ∪(Ac ∩Bc ∩Cc)\n(c) (A ∪B ∪C)c = Ac ∩Bc ∩Cc\n(d) A ∩B ∩C\n(e) (A ∩Bc ∩Cc) ∪(Ac ∩B ∩Cc) ∪(Ac ∩Bc ∩C)\n(f) A ∩B ∩Cc\n(g) A ∪(Ac ∩Bc)\nΩ\nA\nB\nC\nA\nB\nC\nΩ\nA\nB\nC\nΩ\nA\nC\nΩ\nB\n(a)\n(b)\n(c)\n(d)\nA\nC\nΩ\nB\nA\nC\nΩ\nB\nB\nA\nΩ\n(e)\n(f)\n(g)\n2. Since all outcomes are equally likely we apply the discrete uniform probability law to solve\nthe problem. To solve for any event we simply count the number of elements in the event\nand divide by the total number of elements in the sample space.\nThere are 2 possible outcomes for each flip, and 3 flips. Thus there are 23 = 8 elements (or\nsequences) in the sample space.\n(a) Any sequence has probability of 1/8. Therefore P({H, H, H}) = 1/8 .\n(b) This is still a single sequence, thus P({H, T, H}) = 1/8 .\n(c) The event of interest has 3 unique sequences, thus P({HHT, HTH, THH}) = 3/8 .\n(d) The sequences where there are more heads than tails are A : {HHH, HHT, HTH, THH}.\n4 unique sequences gives us P(A) = 1/2 .\n3. The easiest way to solve this problem is to make a table of some sort, similar to the one\nbelow.\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nDie 1\nDie 2\nSum\nTotal\nP(Sum)\n2p\n3p\n4p\n5p\n3p\n4p\n5p\n6p\n4p\n5p\n6p\n7p\n5p\n6p\n7p\n8p\n80p\nP(All outcomes) = 80p (Total from the table)\nand therefore\np = 80\n(a)\nP(Even sum)\n=\n2p + 4p + 4p + 6p + 4p + 6p + 6p + 8p = 40p = 1/2\n(b)\nP(Rolling a 2 and a 3)\n=\nP(2, 3) + P(3, 2) = 5p + 5p = 10p = 1/8\n4. P(B)\nThe shaded area in the following figure is the union of Alice's pick being greater than 1/3\nand Bob's pick being greater than 1/3.\nBob\n1/3\n1/3\nAlice\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nP(B)\n=\n1 -P(both numbers are smaller than 1/3)\narea of small square\n=\n1 -\ntotal sample area\n(1/3)(1/3)\n=\n1 -\n= 1 -\n=\n35/36\nP(C)\nIn the following figure, the diagonal line represents the set of points where the two selected\nnumbers are equal.\nBob\n1/3\n1/3\nAlice\nThe line has an area of 0. Thus,\narea of line\nP(C)\n=\n=\n=\ntotal sample area\nP(A ∩D)\nOverlapping the diagrams we would get for P(A) and P(D),\nBob\n1/3\n1/3\nAlice\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\ndouble shaded area\nP(A ∩D)\n=\ntotal sample area\n(5/3)(5/3)(1/2) + (4/3)(4/3)(1/2)\n25/18 + 16/18\n=\n=\n=\n41/72\n5.\n(a) The probability of Mike scoring 50 points is proportional to the area of the inner disk.\nHence, it is equal to απR2 = απ, where α is a constant to be determined.\nSince the probability of landing the dart on the board is equal to one, απ102 = 1, which\nimplies that α = 1/(100π).\nTherefore, the probability that Mike scores 50 points is equal to π/(100π) = 0.01\n(b) In order to score exactly 30 points, Mike needs to place the dart between 1 and 3 inches\nfrom the origin. An easy way to compute this probability is to look first at that of\nscoring more than 30 points, which is equal to απ32 = 0.09.\nNext, since the 30 points ring is disjoint from the 50 points disc, probability of scoring\nmore than 30 points is equal to the probability of scoring 50 points plus that of scoring\nexactly 30 points. Hence, the probability of Mike scoring exactly 30 points is equal to\n0.09 -0.01 = 0.08\n(c) For the part (a) question. The probability of John scoring 50 points is equal to the\nprobability of throwing in the right half of the board and scoring 50 points plus that of\nthrowing in the left half and scoring 50 points.\nThe first term in the sum is proportional to the area of the right half of the inner disk\nand is equal to απR2/2 = απ/2, where α is a constant to be determined.\nSimilarly, the probability of him throwing in the left half of the board and scoring 50\npoints is equal to βπ/2, where β is a constant (not necessarily equal to α).\nIn order to determine α and β, let us compute the probability of throwing the dart in\nthe right half of the board. This probability is equal to\nαπR2/2 = απ102/2 = α50π.\nSince that probability is equal to 2/3, α = 1/(75π). In a similar fashion, β can be\ndetermined to be 1/(150π).\nConsequently, the total probability is equal to 1/150 +\n1/300 = 0.01\nFor the part (b), The probability of scoring exactly 30 points is equal to that of scoring\nmore than 30 points minus that of scoring exactly 50. By applying the same type of\nanalysis as in (b) above, the probability is found to be equal to 0.08\nThese numbers suggest that John and Mike have similar skills, and are equally likely\nto win the game. The fact that Mike's better control (or worst, depending on how you\nlook at it) of the direction of his throw does not increase his chances of winning can be\nexplained by the observation that both players' control over the distance from the origin\nis identical.\n6. See the textbook, Problem 1.11 page 55, which proves the general version of Bonferroni's\ninequality.\nG1+ .\n(a) If we define An = [an, bn] for all n, it is easy to see that the sequence A1, A2, . . . is\n\"monotonically decreasing,\" i.e., An+1 ⊂ An for all n:\n\n+\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nOmega\n. . .\nAn\nA3\nA2\nA1\nFurthermore, ∩inf\nn An = [a, b].\nBy the continuity property of probabilities (see Problem 1.13, page 56 of the text),\nlim P([an, bn]) = P([a, b]).\nn→inf\n(b) No. Consider the following example. Let an = a + n\n1 , bn = b - n\n1 for all n. Then {an}\nis a decreasing sequence that converges to a, and {bn} is an increasing sequence that\nconverges to b. If we define a probability law that places non-zero probability only on\npoints a and b, then limn→inf P([an, bn]) = 0, but P([a, b]) = 1.\nThis example is closely related to the continuity property of probabilities. In this case, if\nwe define An = [an, bn], then A1, A2, . . . is \"monotonically increasing,\" i.e., An ⊂ An+1,\nbut A = (∪inf\nn An) = (a, b), which is an open interval whose probability is 0 under our\nprobability law.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/3f30d4cd7d743ed019765e5adfd1c3d1_MIT6_041F10_assn01.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Spring 2010)\nProblem Set 1\nDue: September 15, 2010\n1. Express each of the following events in terms of the events A, B and C as well as the operations\nof complementation, union and intersection:\n(a) at least one of the events A, B, C occurs;\n(b) at most one of the events A, B, C occurs;\n(c) none of the events A, B, C occurs;\n(d) all three events A, B, C occur;\n(e) exactly one of the events A, B, C occurs;\n(f) events A and B occur, but not C;\n(g) either event A occurs or, if not, then B also does not occur.\nIn each case draw the corresponding Venn diagrams.\n2. You flip a fair coin 3 times, determine the probability of the below events. Assume all\nsequences are equally likely.\n(a) Three heads: HHH\n(b) The sequence head, tail, head: HTH\n(c) Any sequence with 2 heads and 1 tail\n(d) Any sequence where the number of heads is greater than or equal to the number of tails\n3. Bob has a peculiar pair of four-sided dice. When he rolls the dice, the probability of any\nparticular outcome is proportional to the sum of the results of each die. All outcomes that\nresult in a particular sum are equally likely.\n(a) What is the probability of the sum being even?\n(b) What is the probability of Bob rolling a 2 and a 3, in any order?\n4. Alice and Bob each choose at random a number in the interval [0, 2]. We assume a uniform\nprobability law under which the probability of an event is proportional to its area. Consider\nthe following events:\nA : The magnitude of the difference of the two numbers is greater than 1/3.\nB : At least one of the numbers is greater than 1/3.\nC : The two numbers are equal.\nD : Alice's number is greater than 1/3.\nFind the probabilities P(B), P(C), and P(A ∩ D).\n\n+\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Spring 2010)\n5. Mike and John are playing a friendly game of darts where the dart board is a disk with radius\nof 10in.\nWhenever a dart falls within 1in of the center, 50 points are scored. If the point of impact is\nbetween 1 and 3in from the center, 30 points are scored, if it is at a distance of 3 to 5in 20\npoints are scored and if it is further that 5in, 10 points are scored.\nAssume that both players are skilled enough to be able to throw the dart within the boundaries\nof the board.\nMike can place the dart uniformly on the board (i.e., the probability of the dart falling in a\ngiven region is proportional to its area).\n(a) What is the probability that Mike scores 50 points on one throw?\n(b) What is the probability of him scoring 30 points on one throw?\n(c) John is right handed and is twice more likely to throw in the right half of the board\nthan in the left half. Across each half, the dart falls uniformly in that region. Answer\nthe previous questions for John's throw.\n6. Prove that for any three events A, B and C, we have\nP(A ∩ B ∩ C) ≥ P(A) + P(B) + P(C) - 2.\nG1+. Consider an experiment whose sample space is the real line.\n(a) Let {an} be an increasing sequence of numbers that converges to a and {bn} a decreasing\nsequence that converges to b. Show that\nlim P([an, bn]) = P([a, b]).\nn→inf\nHere, the notation [a, b] stands for the closed interval {x | a ≤ x ≤ b}. Note: This\nresult seems intuitively obvious. The issue is to derive it using the axioms of probability\ntheory.\n(b) Let {an} be a decreasing sequence that converges to a and {bn} an increasing sequence\nthat converges to b. Is it true that\nlim P([an, bn]) = P([a, b])?\nn→inf\nNote: You may use freely the results from the problems in the text in your proofs.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn02_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/9ff5eda7b7032ccefe4270d5f8e2cab0_MIT6_041F10_assn02_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 2: Solutions\nDue September 22, 2010\n1.\n(a) The tree representation during the winter can be drawn as the following:\n0.8\nRain\nThe forecast is\n\"Rain\"\np\n0.2\nNo Rain\n0.1\nRain\n1-p\nThe forecast is\n\"No Rain\"\n0.9\nNo Rain\nLet A be the event that the forecast was \"Rain,\"\nlet B be the event that it rained, and\nlet p be the probability that the forecast says \"Rain.\" If it is in the winter, p = 0.7 and\nP(B | A)P(A)\n(0.8)(0.7)\nP(A | B) =\n=\n=\n.\nP(B)\n(0.8)(0.7) + (0.1)(0.3)\nSimilarly, if it is in the summer, p = 0.2 and\nP(B | A)P(A)\n(0.8)(0.2)\nP(A | B) =\n=\n=\n.\nP(B)\n(0.8)(0.2) + (0.1)(0.8)\n(b) Let C be the event that Victor is carrying an umbrella.\nLet D be the event that the forecast is no rain.\nThe tree diagram in this case is:\n0.5\nUmbrella\nMissed the forecast\n0.2\n0.5\nNo umbrella\nRain (umbrella)\n0.8\np\nSaw the forecast\n1-p\nNo Rain (no umbrella)\nP(D)\n=\n1 -p\nP(C)\n=\n(0.8)p + (0.2)(0.5) = 0.8p + 0.1\nP(C | D)\n=\n(0.8)(0) + (0.2)(0.5) = 0.1\nPage 1 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTherefore, P(C) = P(C | D) if and only if p = 0. However, p can only be 0.7 or 0.2, which\nimplies the events C and D can never be independent, and this result does not depend on\nthe season.\n(c) Let us first find the probability of rain if Victor missed the forecast.\nP(actually rains | missed forecast) = (0.8)p + (0.1)(1 -p) = 0.1 + 0.7p.\nThen, we can extend the tree in part (b) as follows:\nActually no rain\n0.1+0.7p\nActually rain\n0.5\nUmbrella\n0.9-0.7p\nMissed the forecast\nActually rain\n0.1+0.7p\n0.2\n0.5\nNo umbrella\n0.9-0.7p\nActually no rain\nActually no rain\n0.8\nActually rain\nRain (umbrella)\n0.8\np\n0.2\nSaw the forecast\n1-p\n0.1\nActually rain\nNo Rain (no umbrella)\n0.9\nActually no rain\nTherefore, given that Victor is carrying an umbrella and it is not raining, we are looking at\nthe two shaded cases.\n(0.8)p(0.2)\nP(saw forecast | umbrella and not raining) = (0.8)p(0.2) + (0.2)(0.5)(0.9 -0.7p)\nIn fall and winter, p = 0.7, so the probability is 112 .\nIn summer and spring, p = 0.2, so the probability is\n8 .\n2.\n(a)\ni.\nNo\nDie 1\nOverall, there are 25 different outcomes in the sample space. For a total of 10, we\nshould get a 5 on both rolls. Therefore A ⊂ B, and\nP(A ∩B)\nP(A)\nP(B|A) =\n=\n= 1\nP(A)\nP(A)\nDie 2\ntotal is 10\nPage 2 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nWe observe that to get at least one 5 showing, we can have 5 on the first roll, 5 on the\nsecond roll, or 5 on both rolls, which corresponds to 9 distinct outcomes in the sample\nspace. Therefore\nP(B) =\nP(B|A)\n=\nii.\nNo Given event A, we know that both roll outcomes must be 5. Therefore, we could\nnot have event C occur, which would require at least one 1 showing. Formally, there\nare 9 outcomes in C, and\nP(C) = 25\nBut\nP(C|A) = 0 6\nP(C)\n=\n(b)\ni.\nNo Out of the total 25 outcomes, 5 outcomes correspond to equal numbers in the two\nrolls. In half of the remaining 20 outcomes, the second number is higher than the first\none. In the other half, the first number is higher than the second. Therefore,\nP(F ) = 25\nThere are eight outcomes that belong to event E:\nE = {(1, 2), (2, 3), (3, 4), (4, 5), (2, 1), (3, 2), (4, 3), (5, 4)}.\nTo find P(F |E), we need to compute the proportion of outcomes in E for which the\nsecond number is higher than the first one:\nP(F |E) =\nP(F )\n=\nii.\nYes Conditioning on event D reduces the sample space to just four outcomes\n{(2, 5), (3, 4), (4, 3), (5, 2)}\nwhich are all equally likely. It is easy to see that\nP(E|D) =\n=\n,\nP(F |D) =\n=\n,\nP(E ∩F |D) =\n= P(E|D)P(F |D)\n3.\n(a) Suppose we choose old widgets. Before we choose any widgets, there are 500 · 0.15 = 75\ndefective old widgets. The probability that we choose two defective widgets is\nP(two defective|old)\n=\nP(first is defective|old) · P(second is defective|first is defective, old)\n75 74\n=\n= 0.02224\n500 499\nNow let's consider the new widgets. Before we choose any widgets, there are 1500· 0.05 = 75\ndefective old widgets. Similar to the calculations above,\nP(two defective|new)\n=\nP(first is defective|new) · P(second is defective|first is defective, new)\n=\n= 0.002568\n1500 1499\nPage 3 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nBy the total probability law,\nP(two defective)\n=\nP(old) · P(two defective|old)\n+P(new) · P(two defective|new)\n=\n· 0.02224 +\n· 0.002568 = 0.01240.\nNote that this number is very close to what we would get if we ignored the effects of removing\none defective widget before choosing the second widget:\nP(two defective)\n=\nP(old) · P(two defective|old)\n+\nP(new) · P(two defective|new)\n≈\n· 0.152 +\n· 0.052 = 0.0125.\n(b) Using Bayes' rule,\nP(old) · P(two defective|old)\nP(old|two defective)\n=\nP(old) · P(two defective|old) + P(new) · P(two defective|new)\n2 · 0.02224\n=\n= 0.8965\n2 ·\n2 · 0.002568\n0.02224 +\n4.\n(a)\nP(find in A and in A) = P(in A) · P(find in A|in A) = 0.4 · 0.25 = 0.1\nP(find in B and in B) = P(in B) · P(find in B|in B) = 0.6 · 0.15 = 0.09\nOscar should search in Forest A first.\n(b) Using Bayes' Rule,\nP(not find in A|in A) · P(in A)\nP(in A|not find in A)\n=\nP(not find in A|in A) · P(in A) + P(not find in A|in B) · P(in B)\n(0.75) · (0.4)\n=\n=\n(0.4) · (0.75) + (1) · (0.6)\n(c) Again, using Bayes' Rule,\nP(find dog|looked in A) · P(looked in A)\nP(looked in A|find dog)\n=\nP(find dog)\n(0.25) · (0.4) · (0.5)\n=\n=\n(0.25) · (0.4) · (0.5) + (0.15) · (0.6) · (0.5)\n(d) In order for Oscar to find the dog, it must be in Forest A, not found on the first day, alive,\nand found on the second day. Note that this calculation requires conditional independence\nof not finding the dog on different days and the dog staying alive.\nP(find live dog in A day 2)\n=\nP(in A) · P(not find in A day 1|in A)\n·P(alive day 2) · P(find day 2|in A)\n=\n0.4 · 0.75 · (1 -\n) · 0.25 = 0.05\nPage 4 of 7\n\nY\nY\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n5.\n(a) We proceed as follows:\nP(A ∩(B ∪C))\n=\nP((A ∩B) ∪(A ∩C))\n=\nP(A ∩B) + P(A ∩C) -P(A ∩B ∩C)\n* =\nP(A)P(B) + P(A)P(C) -P(A)P(B)P(C)\n=\nP(A) [P(B) + P(C) -P(B)P(C)]\n=\nP(A)P(B ∪C),\nwhere the equality marked with ∗ follows from the independence of A, B, and C.\n(b) Proof 1: If A and B are independent, then Ac and Bc are also independent (see Problem\n1.43, page 63 for the proof).\nFor any two independent events U and V , DeMorgan's Law implies\nP(U ∪V )\n=\nP((Uc ∩V c)c) = 1 -P(Uc ∩V c) = 1 -P(Uc) · P(V c)\n=\n1 -(1 -P(U))(1 -P(V )).\nWe proceed to prove the statement by induction. Letting U = A1 and V = A2, the base\ncase is proven above. Now we assume that the result holds for any n and show that it holds\nfor n + 1. For independent {A1, . . . , An, An+1}, let B = ∪n\ni=1Ai. It is easy to show that B\nand An+1 are independent. Therefore,\nP(A1 ∪A2 ∪. . . ∪An+1)\n=\n1 -(1 -P(B)) · (1 -P(An+1))\nn+1\n=\n1 -\n(1 -P(Ai)),\ni=1\nwhich completes the proof.\nProof 2: Alternatively, we can use the version of the DeMorgan's Law for n events:\nP(A1 ∪A2 ∪. . . ∪An)\n=\nP((Ac\n1 ∩Ac\n2 ∩. . . ∩Ac\nn)c)\n=\n1 -P(Ac\n1 ∩Ac\n2 ∩. . . ∩Ac\nn).\nBut we know that Ac\n1, Ac\n2, . . . , Ac\nn are independent. Therefore\nP(A1 ∪A2 ∪. . . ∪An)\n=\n1 -P(Ac\n1)P(Ac\n2) . . . P(Ac\nn)\nn\n=\n1 -\n(1 -P(Ai)).\ni=1\nG1+ .\n(a) The figure below describes the sample space via an infinite tree. The leaves of this tree\nare exactly all finite tournament histories; in addition, the two infinite paths represent the\ntwo infinite tournament histories that are possible. Note that the winner of the first game\nis either Alice or Bob; from then on, the winner of a game is either the winner of the\nprevious game (in which case we have reached a leaf and the tournament has ended) or the\nplayer that sat out the previous game.The outcomes of the sample space correspond to the\nfinite histories (which are identified with the leafs of the tree) and the two infinite histories:\nACBACB... and BCABCA...\nPage 5 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nA\nC\nB\nA\nC\nB\nA\nC\nB\nA\nA\nC\nB\nA\nC\nB\nA\nC\nB\nA\n...\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\n...\n(b) The probability of an event is 1/2k times the number of finite histories contained in the\nevent. The probability of the event consisting of one or both infinite histories is 0. We\nhave to show that this probability law satisfies the three probability axioms.\nIt clearly\nsatisfies nonnegativity and additivity. To check normalization, we have to verify that the\nprobabilities of all tournament histories sum up to 1.\nStart by noticing that two of the histories are infinite and have probability 0. Each one of\nthe remaining histories has some finite length k ≥ 2 (and hence is represented by one of the\ntwo leaves of the tree of the figure above at depth k) and probability 1/2k . Hence, summing\nall probabilities we get\ninf\nX\n·\n=\n2k\ninf\nX\n2k-1 =\ninf\nX\n=\n2k+1\ninf\nX 1\n2 · 0 +\n1.\n=\n=\n2k\n2 1 -1/2\nk=2\nk=2\nk=0\nk=0\n(c) The probability that exactly 2 games will be played is the sum of the probabilities of the\ntwo leaves at depth 2; that is,\nP (exactly 2 games) =\n+\n=\n.\nSimilarly, the probability that exactly i games will be played, for i = 3, 4, 5, is\nP (exactly 3 games)\n=\n1 ,\n+\n=\nP (exactly 4 games)\n=\n8 ,\n+\n=\nP (exactly 5 games)\n=\n16 .\n+\n=\nHence, the probability that the tournament lasts no more than 5 games is\nP (at most 5 games) =\n+\n+\n+\n=\n.\nHence, it's pretty probable that the tournament will last at most that much.\nThe probability that Alice wins the tournament is the sum of the probabilities of the leaves\nof the tree that are labeled \"A\"; that is,\n(22\n28 + · · ·) + ( 24\n+\n+\n+\n+\n+ · · ·),\nwhere the first summation includes all leaves from the upper part of the tree, while the\nsecond one takes care of the leaves on the lower part. Calculating, we have\ninf\nX\n(1 +\n+\n+ · · ·) +\n(1 +\n+\n+ · · ·) =\n=\n=\n.\n8j\n16 1 -1/8\n16 j=0\nPage 6 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nBy symmetry (note the correspondence between the histories where Alice wins and the\nhistories where Bob does), Bob's probability of winning is\n5 , as well.\nThen, since the\noutcomes where nobody wins (these are the two infinite tournament histories) have total\nprobability 0, Carol wins with probability 1 - 5 - 5\n=\n. Hence, by not participating in\nthe first game, Carol enters the tournament with a disadvantage.\n+Required for 6.431; optional for 6.041\nPage 7 of 7\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/8db952c5e194f18b295eb199b66a1d54_MIT6_041F10_assn02.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 2\nDue September 22, 2010\n1. Most mornings, Victor checks the weather report before deciding whether to carry an umbrella.\nIf the forecast is \"rain,\" the probability of actually having rain that day is 80%. On the other\nhand, if the forecast is \"no rain,\" the probability of it actually raining is equal to 10%. During\nfall and winter the forecast is \"rain\" 70% of the time and during summer and spring it is 20%.\n(a) One day, Victor missed the forecast and it rained. What is the probability that the forecast\nwas \"rain\" if it was during the winter? What is the probability that the forecast was \"rain\"\nif it was during the summer?\n(b) The probability of Victor missing the morning forecast is equal to 0.2 on any day in the\nyear. If he misses the forecast, Victor will flip a fair coin to decide whether to carry an\numbrella. On any day of a given season he sees the forecast, if it says \"rain\" he will always\ncarry an umbrella, and if it says \"no rain,\" he will not carry an umbrella. Are the events\n\"Victor is carrying an umbrella,\" and \"The forecast is no rain\" independent? Does your\nanswer depend on the season?\n(c) Victor is carrying an umbrella and it is not raining. What is the probability that he saw\nthe forecast? Does it depend on the season?\n2. You have a fair five-sided die. The sides of the die are numbered from 1 to 5. Each die roll is\nindependent of all others, and all faces are equally likely to come out on top when the die is\nrolled. Suppose you roll the die twice.\n(a) Let event A to be \"the total of two rolls is 10\", event B be \"at least one roll resulted in 5\",\nand event C be \"at least one roll resulted in 1\".\ni. Is event A independent of event B?\nii. Is event A independent of event C?\n(b) Let event D be \"the total of two rolls is 7\", event E be \"the difference between the two roll\noutcomes is exactly 1\", and event F be \"the second roll resulted in a higher number than\nthe first roll\".\ni. Are events E and F independent?\nii. Are events E and F independent given event D?\n3. The local widget factory is having a blowout widget sale. Everything must go, old and new. The\nfactory has 500 old widgets, and 1500 new widgets in stock. The problem is that 15% of the old\nwidgets are defective, and 5% of the new ones are defective as well. You can assume that widgets\nare selected at random when an order comes in. You are the first customer since the sale was\nannounced.\n(a) You flip a fair coin once to decide whether to buy old or new widgets. You order two widgets\nof the same type, chosen based on the outcome of the coin toss. What is the probability\nthat they will both be defective?\n(b) Given that both widgets turn out to be defective, what is the probability that they were\nold widgets?\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n4. Oscar has lost his dog in either forest A (with a priori probability 0.4) or in forest B (with a\npriori probability 0.6).\nOn any given day, if the dog is in A and Oscar spends a day searching for it in A, the conditional\nprobability that he will find the dog that day is 0.25. Similarly, if the dog is in B and Oscar\nspends a day looking for it there, the conditional probability that he will find the dog that day\nis 0.15.\nThe dog cannot go from one forest to the other. Oscar can search only in the daytime, and he\ncan travel from one forest to the other only at night.\n(a) In which forest should Oscar look to maximize the probability he finds his dog on the first\nday of the search?\n(b) Given that Oscar looked in A on the first day but didn't find his dog, what is the probability\nthat the dog is in A?\n(c) If Oscar flips a fair coin to determine where to look on the first day and finds the dog on\nthe first day, what is the probability that he looked in A?\n(d) If the dog is alive and not found by the Nth day of the search, it will die that evening\nwith probability N\nN\n+2 . Oscar has decided to look in A for the first two days. What is the\nprobability that he will find a live dog for the first time on the second day?\n5. In solving this problem, feel free to browse problems 43-45 in Chapter 1 of the text for ideas. If\nyou need to, you may quote the results of these problems.\n(a) Suppose that A, B, and C are independent. Use the definition of independence to show\nthat A and B ∪ C are independent.\n(b) Prove that if A1, . . . , An are independent events, then\nn\nP(A1 ∪ A2 ∪ . . . ∪ An) = 1 -\nY\n(1 - P(Ai)).\ni=1\nG1+ . Alice, Bob, and Caroll play a chess tournament. The first game is played between Alice and\nBob. The player who sits out a given game plays next the winner of that game. The tournament\nends when some player wins two successive games. Let a tournament history be the list of game\nwinners, so for example ACBAA corresponds to the tournament where Alice won games 1, 4,\nand 5, Caroll won game 2, and Bob won game 3.\n(a) Provide a tree-based sequential description of a sample space where the outcomes are the\npossible tournament histories.\n(b) We are told that every possible tournament history that consists of k games has probability\n1/2k, and that a tournament history consisting of an infinite number of games has zero prob\nability. Demonstrate that this assignment of probabilities defines a legitimate probability\nlaw.\n(c) Assuming the probability law from part (b) to be correct, find the probability that the\ntournament lasts no more than 5 games, and the probability for each of Alice, Bob, and\nCaroll winning the tournament.\n+Required for 6.431; optional for 6.041\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn03_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/fe34c807d939cf00462639469cc5af6d_MIT6_041F10_assn03_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 3 Solutions\nDue September 29, 2010\n1. The hats of n persons are thrown into a box. The persons then pick up their hats at random (i.e.,\nso that every assignment of the hats to the persons is equally likely). What is the probability\nthat\n(a) every person gets his or her hat back?\nAnswer: n\n! .\nSolution: consider the sample space of all possible hat assignments. It has n! elements (n\nhat selections for the first person, after that n -1 for the second, etc.), with every single-\nelement event equally likely (hence having probability 1/n!). The question is to calculate\nthe probability of a single-element event, so the answer is 1/n!\n(b) the first m persons who picked hats get their own hats back?\n(n-m)!\nAnswer:\n.\nn!\nSolution: consider the same sample space and probability as in the solution of (a). The\nprobability of an event with (n -m)! elements (this is how many ways there are to disribute\nthe remaining n -m hats after the first m are assigned to their owners) is (n -m)!/n!\n(c) everyone among the first m persons to pick up the hats gets back a hat belonging to one of\nthe last m persons to pick up the hats?\nAnswer: m!(n-m)! =\nn 1 =\nn 1\n. .\nn!\n(m)\n(n-m)\nSolution: there are m! ways to distribute m hats among the first m persons, and (n -m)!\nways to distribute the remaining n -m hats. The probability of an event with m!(n -m)!\nelements is m!(n -m)!/n!.\nNow assume, in addition, that every hat thrown into the box has probability p of getting dirty\n(independently of what happens to the other hats or who has dropped or picked it up). What is\nthe probability that\n(d) the first m persons will pick up clean hats?\nAnswer: (1 -p)m .\nSolution: the probability of a given person picking up a clean hat is 1 - p.\nBy the\nindependence assumption, the probability of m selected persons picking up clean hats is\n(1 -p)m .\n(e) exactly m persons will pick up clean hats?\nAnswer: (1 -p)mpn-m n .\nm\nSolution: every group G of m persons defines the event \"everyone from G picks up a\nclean hat, everyone not from G picks up a dirty hat\". The events are disjoint. Each has\nn-m\nn\nprobability (1 -p)mp\n. Since there are\nm such events, the answer follows.\n2. Since 4 cards are fixed, Bob can only choose 4 more cards out of 48 remaining cards, so total\nnumber of hands Bob can have such that they include Alice's cards is\n4 . The total number\nof ways Bob can choose any 8 cards is\n. So the probability is (4\n(\n)(\n48 )\n8 )\n3.\n(a) The picture below illustrates the double sum needed to prove the statement of this problem:\nPage 1 of 4\n\nX\nX\nX X\nX X\nX\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\ni\ni\ni=k\ninfinity\nk=1\ni=k\nk\nWe first note that\ninf\nP(X ≥ k) =\npX (i)\ni=k\nand proceed as follows:\ninf\ninf\ninf\ninf\ni\ninf\nP(X ≥ k) =\npX (i) =\npX (i) =\ni pX (i) = E[X].\nk=1\nk=1 i=k\ni=1 k=1\ni=1\n(b) We first compute\n\nk ≤ a\n\nb-k+1\nP(Y ≥ k) =\nb-a+1\na + 1 ≤ k ≤ b\n\nk ≥ b + 1\nSo\ninf\na\nb\nX\nX\nX\nb -k + 1\nP(Y ≥ k)\n=\n1 +\nb -a + 1\nk=1\nk=1\nk=a+1\nb-a\n=\na +\nk\nb -a + 1 k=1\n(b -a + 1)(b -a)\n=\na + b -a + 1\nb -a\n=\na +\nb + a\n=\nTherefore E[Y ] = b+\na .\n4.\n(a) For each value of X, we count the number of outcomes which have a difference that equals\nthat value:\n\n1/9\nx = -2, 2\n\n2/9\nx = -1, 1\npX (x) = 3/9\nx = 0\n\notherwise.\nPage 2 of 4\n\nP\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nX\nE[X] =\nxpX (x) = -2\n+ -1\n+ 0\n+ 1\n+ 2\n= 0 .\nx=-2\nWe can also see that E[X] = 0 because the PMF is symmetric around 0.\nTo find the variance of X, we first compute\nE[X2] =\nX\nx 2 pX (x) = 4 1\n+ 1\n+ 0\n+ 1\n+ 4\n=\n3 .\nx=-2\nand\nvar(X) = E[X2] -(E[X])2 =\n3 .\n(b) Let Z = X2 . By matching the possible values of X and their probabilities to the possible\nvalues of Z, we obtain\n\n2/9\nz = 4\n\n4/9\nz = 1\npZ (z) = 3/9\nz = 0\n\notherwise.\n5. Consider k out of n persons forming a club, with one being designated as the leader and another\nas the treasurer. We can first choose the leader (n choices), then the treasurer (n -1 choices),\nand then a subset of the remaining n -2 persons. Thus, there are n(n -1)2n-2 possible clubs.\nAlternatively, for any given k, there are\nn choices for the members of the club. There are\nk\n\nk(k - 1) choices for the leader and treasurer, so that there are k(k - 1) n k-member clubs.\nk\nSumming over all k, we see that there is a total of\nn\nk=2 k(k -1) n\nk possible clubs.\nG1+ . A candy factory has an endless supply of red, orange, yellow, green, blue, black, white, and violet\njelly beans. The factory packages the jelly beans into jars in such a way that each jar has 200\nbeans, equal number of red and orange beans, equal number of yellow and green beans, one more\nblack bean than the number blue beans, and three more violet beans than the number of white\nPage 3 of 4\n\nX\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nbeans. One possible color distribution, for example, is a jar of 50 yellow, 50 green, one black,\n48 white, and 51 violet jelly beans. As a marketing gimmick, the factory guarantees that no\ntwo jars have the same color distribution. What is the maximum number of jars the factory can\nproduce?\nAnswer:\n101 = 166650.\nSolution: Let N1, N2, N3, N4, N5, N6, N7, N8 denote, respectively, the numbers of red, orange,\nyellow, green, blue, black, white, and violet jelly beans in a jar. There is a one-to-one correspon\ndence\nx = (x1, x2, x3, x4) 7→ N = (x1, x1, x2, x2, x3, x3 + 1, x4, x4 + 3)\nbetween the non-negative integer solutions x = (x1, x2, x3, x4) of the equation\nx1 + x2 + x3 + x4 = 98,\nand the sequences N = (N1, N2, N3, N4, N5, N6, N7, N8) of non-negative integers Ni satisfying\nthe conditions\nN2 = N1, N4 = N3, N6 = N5 + 1, N8 = N7 + 3,\nNi = 200\ni=1\n(i.e. possible color arrangements). The number of possible solutions x is\n101 according to the\nsolution of the more general problem given below:\nGiven a non-negative integer n and a positive integer k, consider the equation\nx1 + x2 + . . . + xk = n,\nto be solved with respect to non-negative integer variables x1, x2, . . . , xk. Find the total number\nof solutions (solutions x1 = 1, x2 = 0 and x1 = 0, x2 = 1 to the equation x1 + x2 = 1 are\nconsidered as different).\nn+k-1\nn+k-1\nAnswer:\n=\n.\nk-1\nn\nSolution: there is a one-to-one correspondence between non-negative integer solutions of equa\ntion x1 + . . . + xk = n and sequences of n + k -1 symbols (n \"o\" and k -1 \"|\"), where a solution\nx = (x1, . . . , xk) maps to the sequence in which the i-th \"|\" (where i ∈{1, 2, . . . , k - 1}) is in\nthe x1 + . . . + xi + ith place: in this bijection, the numbers of \"o\" between the consecutive \"|\"\ncorrespond to the values of xi. Hence the total number of solutions equals the number of ways\nof selecting k -1 places for the \"|\" symbols in a sequence of length n + k -1.\n+Required for 6.431; optional for 6.041\nPage 4 of 4\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/83a097196c493a26f8138a4fdebfd8b7_MIT6_041F10_assn03.pdf",
      "content": "X\n!\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 3\nDue September 29, 2010\n1. The hats of n persons are thrown into a box. The persons then pick up their hats at random (i.e.,\nso that every assignment of the hats to the persons is equally likely). What is the probability\nthat\n(a) every person gets his or her hat back?\n(b) the first m persons who picked hats get their own hats back?\n(c) everyone among the first m persons to pick up the hats gets back a hat belonging to one of\nthe last m persons to pick up the hats?\nNow assume, in addition, that every hat thrown into the box has probability p of getting dirty\n(independently of what happens to the other hats or who has dropped or picked it up). What is\nthe probability that\n(d) the first m persons will pick up clean hats?\n(e) exactly m persons will pick up clean hats?\n2. Alice plays with Bob the following game. First Alice randomly chooses 4 cards out of a 52-card\ndeck, memorizes them, and places them back into the deck. Then Bob randomly chooses 8 cards\nout of the same deck. Alice wins if Bob's cards include all cards selected by her. What is the\nprobability of this happening?\n3.\n(a) Let X be a random variable that takes nonnegative integer values. Show that\ninf\nE[X] =\nP(X ≥ k).\nk=1\nHint: Express the right-hand side of the above formula as a double summation then inter\nchange the order of the summations.\n(b) Use the formula in the previous part to find the expectation of a random variable Y whose\nPMF is defined as follows:\npY (y) =\n,\ny = a, a + 1, . . . , b\nb - a + 1\nwhere a and b are nonnegative integers with b > a. Note that for y = a, a + 1, . . . , b, pY (y)\ndoes not depend explicitly on y since it is a uniform PMF.\n4. Two fair three-sided dice are rolled simultaneously. Let X be the difference of the two rolls.\n(a) Calculate the PMF, the expected value, and the variance of X.\n(b) Calculate and plot the PMF of X2 .\n5. Let n ≥ 2 be an integer. Show that\nn\nX\nk(k - 1) n\nk\n= n(n - 1)2n-2 .\nk=2\nHint: As one way of solving the problem, following from Example 1.31 in the text, think of a\ncommittee that includes a chair and a vice-chair.\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nG1+ . A candy factory has an endless supply of red, orange, yellow, green, blue, black, white, and violet\njelly beans. The factory packages the jelly beans into jars in such a way that each jar has 200\nbeans, equal number of red and orange beans, equal number of yellow and green beans, one more\nblack bean than the number blue beans, and three more violet beans than the number of white\nbeans. One possible color distribution, for example, is a jar of 50 yellow, 50 green, one black,\n48 white, and 51 violet jelly beans. As a marketing gimmick, the factory guarantees that no\ntwo jars have the same color distribution. What is the maximum number of jars the factory can\nproduce?\n+Required for 6.431; optional for 6.041\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn04_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/fdea3ff7adc4f8a4e3d9e15e38d52e90_MIT6_041F10_assn04_sol.pdf",
      "content": "X\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 4: Solutions\n1. (a) From the joint PMF, there are six (x, y) coordinate pairs with nonzero probabilities of\noccurring. These pairs are (1, 1), (1, 3), (2, 1), (2, 3), (4, 1), and (4, 3). The probability\nof a pair is proportional to the sum of the squares of the coordinates of the pair, x2 + y2 .\nBecause the probability of the entire sample space must equal 1, we have:\n(1 + 1)c + (1 + 9)c + (4 + 1)c + (4 + 9)c + (16 + 1)c + (16 + 9)c = 1.\nSolving for c, we get c =\n72 .\n(b) There are three sample points for which y < x:\nP(Y < X) = P ({(2, 1)}) + P ({(4, 1)}) + P ({(4, 3)}) = 5 + 17 + 25 =\n47 .\n(c) There are two sample points for which y > x:\nP(Y > X) = P ({(1, 3)}) + P ({(2, 3)}) = 10 + 13 =\n23 .\n(d) There is only one sample point for which y = x:\nP(Y = X) = P ({(1, 1)}) =\n2 .\nNotice that, using the above two parts,\nP(Y < X) + P(Y > X) + P(Y = X) =\n+\n+\n= 1\nas expected.\n(e) There are three sample points for which y = 3:\nP(Y = 3) = P ({(1, 3)}) + P ({(2, 3)}) + P ({(4, 3)}) =\n+\n+\n=\n72 .\n(f) In general, for two discrete random variable X and Y for which a joint PMF is defined, we\nhave\ninf\ninf\npX (x) =\npX,Y (x, y)\nand\npY (y) =\npX,Y (x, y).\ny=-inf\nx=-inf\nIn this problem the ranges of X and Y are quite restricted so we can determine the marginal\nPMFs by enumeration. For example,\npX (2) = P ({(2, 1)}) + P ({(2, 3)}) =\n.\nOverall, we get:\n\n12/72, if x = 1,\n\n24/72, if y = 1,\n18/72, if x = 2,\npX (x) =\nand\npY (y) =\n48/72, if y = 3,\n42/72, if x = 4,\n\n0,\notherwise.\n0,\notherwise\nPage 1 of 7\n\nX\nX X\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(g) In general, the expected value of any discrete random variable X equals\ninf\nE[X] =\nxpX (x).\nx=-inf\nFor this problem,\nE[X] = 1 ·\n+ 2 ·\n+ 4 ·\n= 3\nand\nE[Y ] = 1 ·\n+ 3 ·\n=\n3 .\nTo compute E[XY ], note that pX,Y (x, y) =\npX (x)pY (y). Therefore, X and Y are not inde\npendent and we cannot assume E[XY ] = E[X]E[Y ]. Thus, we have\nE[XY ] =\nxypX,Y (x, y)\nx\ny\n= 1 ·\n+ 2 ·\n+ 4 ·\n+ 3 ·\n+ 6 ·\n+ 12 ·\n=\n.\n(h) The variance of a random variable X can be computed as E[X2]-E[X]2 or as E[(X -E[X])2].\nWe use the second approach here because X and Y take on such limited ranges. We have\nvar(X) = (1 -3)2 12 + (2 -3)2 18 + (4 -3)2 42 =\nand\nvar(Y ) = (1 - 7)2 24 + (3 - 7)2 48\n9 .\n=\nX and Y are not independent, so we cannot assume var(X + Y ) = var(X) + var(Y ). The\nvariance of X +Y will be computed using var(X +Y ) = E[(X +Y )2]-(E[X +Y ])2 . Therefore,\nwe have\nE[(X + Y )2] = 4 · 2 + 9 · 5 + 25 · 17 + 16 · 10 + 25 · 13 + 49 · 25 = 547 .\n(E[X + Y ])2 = (E[X] + E[Y ])2 =\n3 + 72\n=\n.\nTherefore,\nvar(X + Y ) =\n-\n=\n18 .\n(i) There are four (x, y) coordinate pairs in A : (1,1), (2,1), (4,1), and (4,3).\nTherefore,\nP(A) = 72 (2 + 5 + 17 + 25) = 72 . To find E[X | A] and var(X | A), pX|A(x) must be\ncalculated. We have\npX|A(x) =\n\n2/49,\nif x = 1,\n5/49,\nif x = 2,\n42/49, if x = 4,\n0,\notherwise,\nPage 2 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nE[X | A]\n=\n1 ·\n+ 2 ·\n+ 4 ·\n=\n49 ,\nE[X2 | A]\n=\n12 · 2 + 22 · 5 + 42 · 42 = 694 ,\n180 2\nvar(X | A)\n=\nE[X2 | A] -(E[X | A])2 =\n-\n=\n2401 ,\n2. Consider a sequence of six independent rolls of this die, and let Xi be the random variable\ncorresponding to the ith roll.\n(a) What is the probability that exactly three of the rolls have result equal to 3? Each roll Xi\ncan either be a 3 with probability 1/4 or not a 3 with probability 3/4. There are\nways\nof placing the 3's in the sequence of six rolls. After we require that a 3 go in each of these\nspots, which has probability (1/4)3, our only remaining condition is that either a 1 or a 2 go\nin the other three spots, which has probability (3/4)3 . So the probability of exactly three\nrolls of 3 in a sequence of six independent rolls is\n\n(4\n1 )3(4\n3 )3\n.\n(b) What is the probability that the first roll is 1, given that exactly two of the six rolls have\nresult of 1? The probability of obtaining a 1 on a single roll is 1/2, and the probability of\nobtaining a 2 or 3 on a single roll is also 1/2. For the purposes of solving this problem we\ntreat obtaining a 2 or 3 as an equivalent result. We know that there are\nways of rolling\nexactly two 1's. Of these\n\nways, exactly\n\n= 5 ways result in a 1 in the first roll, since\nwe can place the remaining 1 in any of the five remaining rolls. The rest of the rolls must\nbe either 2 or 3. Thus, the probability that the first roll is a 1 given exactly two rolls had\nan outcome of 1 is\n(6\n2)\n.\n(c) We are now told that exactly three of the rolls resulted in 1 and exactly three resulted in 2.\nWhat is the probability of the sequence 121212? We want to find\nP(121212)\nP(121212 | exactly three 1's and three 2's) =\n.\nP(exactly 3 ones and 3 twos)\nAny particular sequence of three 1's and three 2's will have the same probability: (1/2)3(1/4)3 .\nThere are\n\npossible rolls with exactly three 1's and three 2's. Therefore,\nP(121212 | exactly three 1's and three 2's) = (\n3) .\n(d) Conditioned on the event that at least one roll resulted in 3, find the conditional PMF of\nthe number of 3's. Let A be the event that at least one roll results in a 3. Then\nP(A) = 1 -P(no rolls resulted in 3) = 1 -\n.\nNow let K be the random variable representing the number of 3's in the 6 rolls.\nThe\n(unconditional) PMF pK (k) for K is given by\n6 1k 36-k\npK (k) =\n.\nk\nPage 3 of 7\n\n(\n\nX\nX\nX\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nWe find the conditional PMF pk|A(k | A) using the definition of conditional probability:\nP({K = k} ∩A)\npK|A(k | A) =\n.\nP(A)\nThus we obtain\n6 (1\n4 )k(3\n4 )6-k if k = 1, 2, . . . , 6,\npK|A(k | A) =\n1-(3/4)6 k\notherwise.\nNote that pK|A(0 | A) = 0 because the event {K = 0} and the event A are mutually\nexclusive. Thus the probability of their intersection, which appears in the numerator in the\ndefinition of the conditional PMF, is zero.\n3. By the definition of conditional probability,\nP({X = i} ∩{X + Y = n})\nP(X = i | X + Y = n) =\n.\nP(X + Y = n)\nThe event {X = i} ∩{X + Y = n} in the numerator is equivalent to {X = i} ∩{Y = n -i}.\nCombining this with the independence of X and Y ,\nP({X = i} ∩{X + Y = n}) = P({X = i} ∩{Y = n -i}) = P(X = i)P(Y = n -i).\nIn the denominator, P(X +Y = n) can be expanded using the total probability theorem and the\nindependence of X and Y :\nn-1\nP(X + Y = n) =\nP(X = i)P(X + Y = n | X = i)\ni=1\nn-1\n=\nP(X = i)P(i + Y = n | X = i)\ni=1\nn-1\n=\nP(X = i)P(Y = n -i | X = i)\ni=1\nn-1\n=\nP(X = i)P(Y = n -i)\ni=1\nNote that we only get non-zero probability for i = 1, . . . , n - 1 since X and Y are geometric\nrandom variables.\nThe desired result is obtained by combining the computations above and using the geometric\nPage 4 of 7\n\nX\nX\nX\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nPMF explicitly:\nP(X = i)P(Y = n -i)\nP(X = i | X + Y = n) = n-1\nP(X = i)P(Y = n -i)\ni=1\n(1 -p)i-1p(1 -p)n-i-1p\n= n-1\n(1 -p)i-1 p(1 -p)n-i-1 p\ni=1\n(1 -p)n\n= n-1\n(1 -p)n\ni=1\n(1 -p)n\n=\nn-1\n(1 -p)n\ni=1\n=\n,\ni = 1, . . . , n -1.\nn -1\n4. (a) Since P(A) > 0, we can show independence through P(B) = P(B | A):\nP(B ∩A)\np6(1 -p)2p\nP(B | A) =\n=\n= p = P(B).\nP(A)\n\np6(1 -p)2\nTherefore, A and B are independent.\n(b) Let C be the event \"3 heads in the first 4 tosses\" and let D be the event \"2 heads in the last\n3 tosses\". Since there are no overlap in tosses in C and D, they are independent:\nP(C ∩D) = P(C)P(D)\n=\np 3(1 -p) ·\np 2(1 -p)\n= 12p 5(1 -p)2 .\n(c) Let E be the event \"4 heads in the first 7 tosses\" and let F be the event \"2nd head occurred\nduring 4th trial\". We are asked to find P(F | E) = P(F ∩E)/P(E). The event F ∩E occurs\nif there is 1 head in the first 3 trials, 1 head on the 4th trial, and 2 heads in the last 3 trials.\nThus, we have\nP(F ∩E)\np(1 -p)2 · p ·\np2(1 -p)\nP(F | E) =\n=\nP(E)\n\np4(1 -p)3\n· 1 ·\n=\n=\n.\nAlternatively, we can solve this by counting. We are given that 4 heads occurred in the first\n7 tosses. Each sequence of 7 trials with 4 heads is equally probable, the discrete uniform\nPage 5 of 7\n\nX\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nprobability law can be used here. There are\noutcomes in E. For the event E ∩F , there\nare\n\nways to arrange 1 head in the first 3 trials, 1 way to arrange the 2nd head in the 4th\ntrial and\nways to arrange 2 heads in the first 3 trials. Therefore,\n· 1 ·\nP(F | E) =\n1 7\n2 = 35 .\n(d) Let G be the event \"5 heads in the first 8 tosses\" and let H be the event \"3 heads in the last\n5 tosses\". These two events are not independent as there is some overlap in the tosses (the\n6th, 7th, and 8th tosses). To compute the probability of interest, we carefully count all the\ndisjoint, possible outcomes in the set G ∩H by conditioning on the number of heads in the\n6th, 7th, and the 8th tosses. We have\nP(G ∩H) = P(G ∩H | 1 head in tosses 6-8)P(1 head in tosses 6-8)\n+ P(G ∩H | 2 heads in tosses 6-8)P(2 heads in tosses 6-8)\n+ P(G ∩H | 3 heads in tosses 6-8)P(3 heads in tosses 6-8)\n=\np 4(1 -p) · p 2 ·\np(1 -p)2\n+\np 3(1 -p)2 ·\np(1 -p) ·\np 2(1 -p)\n+\np 2(1 -p)3 · (1 -p)2 · p 3 .\n= 15p 7(1 -p)3 + 60p 6(1 -p)4 + 10p 5(1 -p)5 .\n5. Let Ik be the reward paid at time k. We have\nE[Ik] = P(Ik = 1) = P(T at time k and H at time k -1) = p(1 -p).\nComputing E[R] is immediate because\n\" n\n#\nn\nE[R] = E\nIk =\nE [Ik] = np(1 -p).\nk=1\nk=1\nThe variance calculation is not as easy because the Iks are not all independent:\nE[Ik\n2]\n=\np(1 -p)\nE[IkIk+1]\n=\nbecause rewards at times k and k + 1 are inconsistent\nE[IkIk+l]\n=\nE[Ik]E[Ik+l] = p 2(1 -p)2\nfor l ≥ 2\nPage 6 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nn\nn\nn\nn\nE[R2]\n=\nE[(\nX\nIk)(\nX\nIm)] =\nX X\nE[IkIm]\nk=1\nm=1\nk=1 m=1\n=\nnp(1 -p)\n+\n+\n(n 2 -3n + 2)p 2(1 -p)2\n|\n{z\n}\n|{z}\n|\n{z\n}\nn terms with k = m\n2(n -1) terms with |k -m| = 1\nn2 -3n + 2 terms with |k -m| > 1\nvar(R)\n=\nE[R2] -(E[R])2\n=\nnp(1 -p) + (n 2 -3n + 2)p 2(1 -p)2 -n 2 p 2(1 -p)2\n=\nnp(1 -p) -(3n -2)p 2(1 -p)2 .\nG1+ .\n(a) We know that IA is a random variable that maps a 1 to the real number line if ω occurs\nwithin an event A and maps a 0 to the real number line if ω occurs outside of event A. A\nsimilar argument holds for event B. Thus we have,\n1,\nwith probability P(A)\nIA(ω) =\n0,\nwith probability 1 -P(A)\n1,\nwith probability P(B)\nIB (ω) =\n0,\nwith probability 1 -P(B)\nIf the random variables, A and B, are independent, we have P(A ∩B) = P(A)P(B). The\nindicator random variables, IA and IB, are independent if, PIA,IB (x, y) = PIA (x)PIB (y)\nWe know that the intersection of A and B yields.\nPIA,IB (1, 1)\n=\nPIA (1)PIB (1)\n=\nP(A)P(B)\n=\nP(A ∩B)\nWe also have,\nPIA,IB (1, 1)\n=\nP(A ∩B) = P(A)P(B) = PIA (1)PIB (1)\nPIA,IB (0, 1)\n=\nP(Ac ∩B) = P(Ac)P(B) = PIA (0)PIB (1)\nPIA,IB (1, 0)\n=\nP(A ∩Bc) = P(A)P(Bc) = PIA (1)PIB (0)\nPIA,IB (0, 0)\n=\nP(Ac ∩Bc) = P(Ac)P(Bc) = PIA (0)PIB (0)\n(b) If X = IA, we know that\nE[X] = E[IA] = 1 · P(A) + 0 · (1 -P(A)) = P(A)\n+Required for 6.431; optional for 6.041\nPage 7 of 7\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/23d3729be6470f352ec532ad7e3f9981_MIT6_041F10_assn04.pdf",
      "content": "(\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 4\nDue October 6, 2010\n1. Random variables X and Y have the joint PMF\nc(x2 + y2), if x ∈{1, 2, 4} and y ∈{1, 3},\npX,Y (x, y) =\n0, otherwise.\n(a) What is the value of the constant c?\n(b) What is P(Y < X)?\n(c) What is P(Y > X)?\n(d) What is P(Y = X)?\n(e) What is P(Y = 3)?\n(f) Find the marginal PMFs pX (x) and pY (y).\n(g) Find the expectations E[X], E[Y ] and E[XY ].\n(h) Find the variances var(X), var(Y ) and var(X + Y ).\n(i) Let A denote the event X ≥ Y . Find E[X | A] and var(X | A).\n2. The newest invention of the 6.041/6.431 staff is a three-sided die with faces numbered 1, 2, and 3.\nThe PMF for the result of any one roll of this die is\npX (x) =\n⎧\n⎪\n⎨\n⎪\n⎩\n1/2, if x = 1,\n1/4, if x = 2,\n1/4, if x = 3,\n0, otherwise.\nConsider a sequence of six independent rolls of this die, and let Xi be the random variable\ncorresponding to the ith roll.\n(a) What is the probability that exactly three of the rolls have result equal to 3?\n(b) What is the probability that the first roll is 1, given that exactly two of the six rolls have\nresult of 1?\n(c) We are told that exactly three of the rolls resulted in 1 and exactly three resulted in 2.\nGiven this information, what is the probability that the sequence of rolls is 121212?\n(d) Conditioned on the event that at least one roll resulted in 3, find the conditional PMF of\nthe number of 3's.\n3. Suppose that X and Y are independent, identically distributed, geometric random variables with\nparameter p. Show that\nP(X = i | X + Y = n) = n - 1,\nfor i = 1, 2, . . . , n - 1.\nPage 1 of 2\n\n(\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n4. Consider 10 independent tosses of a biased coin with a probability of heads of p.\n(a) Let A be the event that there are 6 heads in the first 8 tosses. Let B be the event that the\n9th toss results in heads. Show that events A and B are independent.\n(b) Find the probability that there are 3 heads in the first 4 tosses and 2 heads in the last 3\ntosses.\n(c) Given that there were 4 heads in the first 7 tosses, find the probability that the 2nd head\noccurred during the 4th trial.\n(d) Find the probability that there are 5 heads in the first 8 tosses and 3 heads in the last 5\ntosses.\n5. Consider a sequence of independent tosses of a biased coin at times t = 0, 1, 2, . . .. On each toss,\nthe probability of a 'head' is p, and the probability of a 'tail' is 1 - p. A reward of one unit is\ngiven each time that a 'tail' follows immediately after a 'head.' Let R be the total reward paid\nin times 1, 2, . . . , n. Find E[R] and var(R).\nG1+. A simple example of a random variable is the indicator of an event A, which is denoted by IA:\nIA(ω) =\n1,\nif ω ∈ A\n0,\notherwise.\n(a) Prove that two events A and B are independent if and only if the associated indicator\nrandom variables, IA and IB are independent.\n(b) Show that if X = IA, then E[X] = P(A).\n+Required for 6.431; optional for 6.041\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn05_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/d7bdb82c344a0ba36de6b6b086689d87_MIT6_041F10_assn05_sol.pdf",
      "content": "Z\nZ\nZ\n\nZ\nZ\nZ\nZ\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 5: Solutions\n1.\n(a) Because of the required normalization property of any joint PDF,\nZ 2\n\n1 =\nax dy\ndx =\nax(2 -x) dx = a\n22 -12 -\n+\n=\na\nx=1\ny=x\nx=1\nso a = 3/2.\n(b) For 1 ≤ y ≤ 2,\ny\na\nfY (y) =\nax dx =\n(y 2 -1) =\n(y 2 -1),\nand fY (y) = 0 otherwise.\n(c) First notice that for 1 ≤ x ≤ 3/2,\nfX,Y (x, 3/2)\n(3/2)x\n8x\nfX|Y (x | 3/2) =\n=\n=\n.\nfY (3/2)\n-12\nTherefore,\nZ 3/2 1 8x\nE[1/X | Y = 3/2]\n=\ndx = 4/5.\nx 5\n2. (a) By definition fX,Y (x, y) = fX(x)fY |X(y | x). fX(x) = ax as shown in the graph. We have\nthat\nZ 40\n1 =\nax dx = 800a.\nSo fX(x) = x/800. From the problem statement fY |X(y | x) = 2\nx for y ∈ [0, 2x]. Therefore,\n1/1600,\nif 0 ≤ x ≤ 4 and 0 < y < 2x,\nfX,Y (x, y) =\n0,\notherwise.\n(b) Paul makes a positive profit if Y > X. This occurs with probability\nZ Z\n2x\nP(Y > X) =\nfX,Y (x, y) dy dx =\ndy dx =\n.\ny>x\nx\nWe could have also arrived at this answer by realizing that for each possible value of X, there\nis a 1/2 probability that Y > X.\n(c) The joint density function satisfies fX,Z(x, z) = fX(x) fZ|X(z|x). Since Z is conditionally\nuniformly distributed given X, fZ|X(z | x) = 2\nx for -x ≤ z ≤ x. Therefore, fX,Z(x, z) =\n1/1600 for 0 ≤ x ≤ 40 and -x ≤ z ≤ x. The marginal density fz(z) is calculated as\n40-|z| ,\nif |z| < 40,\nfZ(z) =\nfX,Z(x, z) dx =\ndx =\nx\nx=|z| 1600\n0,\notherwise.\nPage 1 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n3. (a) In order for X and Y to be independent, any observation of X should not give any information\non Y . If X is observed to be equal to 0, then Y must be 0.\nIn other words, fY |{X=0}(y | 0)\nfY (y). Therefore, X and Y are not independent.\n=\n1.5\n\n0,\notherwise.\n0.5\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\n1.8\n1.6\n1.4\nx/2,\nif 0 ≤ x ≤ 1,\n(b) fX(x) =\n-3x/2 + 3,\nif 1 < x ≤ 2,\nf X |Y (x | 0.5)\nf Y |X (y | 0.5)\nf X (x)\n1.2\nfY |X(y | 0.5) =\n2,\nif 0 ≤y ≤ 1/2,\n0,\notherwise.\n\n1/2,\nif 1/2 ≤ x ≤ 1,\nfX|Y (x | 0.5) =\n3/2,\nif 1 < x ≤ 3/2,\n0,\notherwise.\n0.8\n0.6\n0.4\n0.2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\ny\n1.5\n0.5\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\n(c) The event A leaves us with a right triangle with a constant height. The conditional PDF is\nthen 1/area = 8. The conditional expectation yields:\nE[R | A]\n=\nE[XY | A]\nZ 0.5 Z 0.5\n=\n8xy dx dy\ny\n=\n1/16.\n(d) The CDF of W is FW (w) = P(W ≤ w) = P(Y -X ≤ w) = P(Y ≤ X + w). P(Y ≤ X + w)\ncan be computed by integrating the area below the line Y = X + w for all possible values of\nw. The lines Y = X +w are shown below for w = 0, w = -1/2, w = -1 and w = -3/2. The\nprobabilities of interest can be calculated by taking advantage of the uniform PDF over the\ntwo triangles. Remember to multiply the areas by the appropriate joint density fX,Y (x, y)!\nTake note that there are 4 regions of interest: w < -2, -2 ≤ w ≤-1, -1 < w ≤ 0 and\nw > 0.\nPage 2 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nw = 0\ny = 2 -x\n1.2\nw ∈ (-1, 0)\ny = x\n0.8\ny\nw = -1\n0.6\nw ∈ (-2, -1)\n0.4\n0.2\n1 + w\n2 + w\n-0.2\n1 + w\n2+w\n-w\n-w\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\nThe CDF of W is\n\n0,\nif w < -2,\n\n3/2 · 1/2(2 + w)2/2,\nif -2 ≤w ≤-1,\nFW (w)\n= 1/2 · 1/2(1 + w)2 + 3/2 · (1/2 · 1 · 1 -1/2(-w/2 · -w)),\nif -1 < w ≤ 0,\n\n1,\nif w > 0\n\n0,\nif w < -2,\n\n3/8 · (2 + w)2 ,\nif -2 ≤ w ≤-1,\n= 1/8 · (-w2 + 4w + 8),\nif -1 < w ≤ 0,\n\n1,\nif w > 0.\nAs a sanity check, FW (-inf) = 0 and FW (+inf) = 1. Also, FW (w) is continuous at w = -2\nand at w = -1.\n4. (a) If the transmitter sends the 0 symbol, the received signal is a normal random variable with\na mean of -2 and a variance of 4. In other words, fY |X(y | -2) = N(-2, 4).\nAlso, fY |X(y | 2) = N(2, 4) These conditional pdfs are shown in the graph below.\nPage 3 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n-8\n-6\n-4\n-2\n0.05\n0.1\n0.15\n0.2\n0.25\nfY |X (y | 2)\nfY |X (y | -2)\nP(error | X = 2)\nP(error | X = -2)\nThe probability of error can be found using the total probability theorem.\nP(error)\n=\nP(error | X = -2)P(X = -2) + P(error | X = 2)P(X = 2)\n=\n(P(Y ≥ 0 | X = -2) + P(Y < 0 | X = 2))\n=\n(P(N ≥ 2 | X = -2) + P(N < -2 | X = 2))\n=\n(P(N ≥ 2) + P(N < -2))\nN -0\n2 -0\nN -0\n-2 -0\n=\n(P(\n≥\n) + P(\n<\n))\n=\n((1 -Φ(1)) + (1 -Φ(1)))\n=\n0.1587.\n(b) With 3 components, the probability of error given an obervation of X is the probability of\ndecoding 2 or 3 of the components incorrectly. For each component, the probability of error\nis 0.1587. Therefore,\nP(error | sent 0)\n=\n(0.1587)2 (1 -0.1587) + (0.1587)3\n=\n0.0676.\nBy symmetry, P(error | sent 1) = P(error | sent 0).\nTherefore, P(error) = P(error | sent 0)P(sent 0) + P(error | sent 1)P(sent 1) = 0.0676.\n5.\n(a) There are many ways to show that X and Y are not independent. One of the most intuitive\narguments is that knowing the value of X limits the range of Y , and vice versa. For instance,\nif it is known in a particular trial that X ≥ 1/2, the value of Y in that trial cannot be smaller\nPage 4 of 7\n\nZ\nZ\nZ\nZ\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nthan 1/2. Another way to prove that the two are not independent is to calculate the product\nof their expectations, and show that this is not equal to E[XY ].\n(b) Applying the definition of a marginal PDF,\nfor 0 ≤ x ≤ 1,\nfX(x)\n=\nfX,Y (x, y) dy\ny\nZ x+1\n=\n1 dy\nx\n=\n1;\nfor 0 ≤ y ≤ 1,\nfY (y)\n=\nfX,Y (x, y) dx\nx\ny\n=\n1 dx\n=\ny;\nand for 1 ≤ y ≤ 2,\nfY (y)\n=\nfX,Y (x, y) dx\nx\nZ 1\n=\n1 dx\ny-1\n=\n2 -y.\n0.2\n0.4\n0.6\n0.8\nf Y (y)\n0.8\n0.8\nf X (x)\n0.6\n0.6\n0.4\n0.2\n0.4\n0.2\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\ny\n(c) By linearity of expectation, the expected value of a sum is the sum of the expected values.\nBy inspection, E[X] = 1/2 and E[Y ] = 1.\nThus, E[X + Y ] = E[X] + E[Y ] = 3/2.\nPage 5 of 7\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(d) The variance of X + Y is\nE[(X + Y )2] -E[X + Y ]2 = E[X2] + 2E[XY ] + E[Y 2] -(E[X + Y ])2 .\n(1)\nIn part (c), E[X+Y ] was computed, so only the other three expressions need to be calculated.\nFirst, the expected value of X2:\nZ 1\nZ x+1\nZ 1\nE[X2] =\nx 2\n1 dy dx =\nx 2 dx = 1/3.\nx\nAlso, the expected value of Y 2 is\nZ 1 Z x+1\nZ 1\nE[Y 2] =\ny 2 dy dx =\n(3x 2 + 3x + 1)/3 dx = 7/6.\nx\nFinally, the expected value of XY is\nZ 1 Z x+1\nE[XY ]\n=\nx\ny dy dx\nx\nZ 1\n=\n(2x 2 + x)/2 dx dy = 7/12.\nSubstituting these into (1), we get var(X + Y ) = 1/3 + 7/6 + 7/6 -9/4 = 5/12.\n*Alternative (shortcut) solution to parts (c) and (d)*\nGiven any value of X (in ([0,1]), we observe that Y - X takes values between 0 and 1,\nand is uniformly distributed. Since the conditional distribution of Y -X is the same for\nevery value of X in [0,1], we see that Y -X independent of X. Thus: (a) X is uniform,\nand (b) Y = X + U, where U is also uniform and independent of X.\nIt follows that\nE[X + Y ] = E[2X + U] = 3/2. Furthermore, var(X + Y ) = 4 var(X) + var(U) = 5/12.\n6. (a) Let A be the event that the first coin toss resulted in heads. To calculate the probability\nP(A), we use the continuous version of the total probability theorem:\nZ 1\nZ 1\nP(A) =\nP(A | P = p)fP(p) dp =\np(1 + sin(2πp)) dp,\nwhich after some calculation yields\nπ -1\nP(A) =\n.\n2π\n(b) Using Bayes rule,\nP(A | P = p)fP(p)\nfP |A(p)\n=\nP(A)\n\n2πp(1 + sin(2πp)) ,\nif 0 ≤ p ≤ 1,\n=\nπ -1\n0,\notherwise.\nPage 6 of 7\n\nZ\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(c) Let B be the event that the second toss resulted in heads. We have\nZ 1\nP(B | A)\n=\nP(B | P = p, A)fP |A(p) dp\nZ 1\n=\nP(B | P = p)fP |A(p) dp\n2π\nZ 1\n=\np 2(1 + sin(2πp)) dp.\nπ -1\nAfter some calculation, this yields\n2π\n2π -3\n2π -3\nP(B | A) =\n·\n=\n≈ 0.5110.\nπ -1\n6π\n3π -3\nG1+ . Let a = (cos θ, sin θ) and b = (bx, by). We will show that no point of R lies outside C if and only\nif\n|b| ≤| sin θ|,\nand\n|a| ≤| cos θ|\n(2)\nThe other two vertices of R are (cos θ, by) and (bx, sin θ). If |bx| ≤| cos θ| and |by| ≤| sin θ|, then\neach vertex (x, y) of R satisfies x2 + y2 ≤ cos2 θ + sin2 θ = 1 and no points of R can lie outside of\nC. Conversely if no points of R lie outside C, then applying this to the two vertices other than\na and b, we find\ncos 2 θ + b2 ≤ 1,\nand\na 2 + sin2 θ ≤ 1.\nwhich is equivalent to 2.\nThese conditions imply that (bx, by) lies inside or on C, so for any given θ, the probability that\nthe random point b = (bx, by) satisfies (2) is\n2| cos θ| · 2| sin θ|\n=\n| sin(2θ)|\nπ\nπ\nand the overall probability is\n1 Z 2π 2\nπ/2\n| sin(2θ)|dθ =\nsin(2θ)dθ =\n2π\nπ\nπ2\nπ2\n.\n+Required for 6.431; optional for 6.041\nPage 7 of 7\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_assn05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/1e0cf546aaaa852cbd09eb1fba40bf74_MIT6_041F10_assn05.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nProblem Set 5\nDue October 18, 2010\n1. Random variables X and Y are distributed according to the joint PDF\nax,\nif 1 ≤ x ≤ y ≤ 2,\nfX,Y (x, y) =\n0,\notherwise.\n(a) Evaluate the constant a.\n(b) Determine the marginal PDF fY (y).\n(c) Determine the expected value of\nX , given that Y = 3\n2.\n2. Paul is vacationing in Monte Carlo. The amount X (in dollars) he takes to the casino each\nevening is a random variable with the PDF shown in the figure. At the end of each night, the\namount Y that he has on leaving the casino is uniformly distributed between zero and twice the\namount he took in.\nfX(x )\n(a) Determine the joint PDF fX,Y (x, y). Be sure to indicate what the sample space is.\n(b) What is the probability that on any given night Paul makes a positive profit at the casino?\nJustify your reasoning.\n(c) Find and sketch the probability density function of Paul's profit on any particular night,\nZ = Y -X. What is E[Z]? Please label all axes on your sketch.\nx (dollars)\nPage 1 of 3\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n3. X and Y are continuous random variables. X takes on values between 0 and 2 while Y takes on\nvalues between 0 and 1. Their joint pdf is indicated below.\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\n0.2\n0.4\n0.6\n0.8\ny\nfX,Y (x, y) = 3\nfX,Y (x, y) = 1\nx\n(a) Are X and Y independent? Present a convincing argument for your answer.\n(b) Prepare neat, fully labelled plots for fX (x), fY |X (y | 0.5), and fX|Y (x | 0.5).\n(c) Let R = XY and let A be the event X < 0.5. Evaluate E[R | A].\n(d) Let W = Y -X and determine the cumulative distribution function (CDF) of W.\n4. Signal Classification:\nConsider the communication of binary-valued messages over some\ntransmission medium.\nSpecifically, any message transmitted between locations is one of two\npossible symbols, 0 or 1. Each symbol occurs with equal probability. It is also known that any\nnumerical value sent over this wire is subject to distortion; namely, if the value X is transmitted,\nthe value Y received at the other end is described by Y = X + N where the random variable\nN represents additive noise that is independent of X. The noise N is normally distributed with\nmean μ = 0 and variance σ2 = 4.\n(a) Suppose the transmitter encodes the symbol 0 with the value X = -2 and the symbol 1\nwith the value X = 2. At the other end, the received message is decoded according to the\nfollowing rules:\n- If Y ≥ 0, then conclude the symbol 1 was sent.\n- If Y < 0. then conclude the symbol 0 was sent.\nDetermine the probability of error for this encoding/decoding scheme. Reduce your calcu\nlations to a single numerical value.\n(b) In an effort to reduce the probability of error, the following modifications are made. The\ntransmitter encodes the symbols with a repeated scheme. The symbol 0 is encoded with\nthe vector X = [-2, -2, -2]⊺ and the symbol 1 is encoded with the vector X = [2, 2, 2]⊺ .\nThe vector Y = [Y1, Y2, Y3]⊺ received at the other end is described by Y = X + N. The\nvector N = [N1, N2, N3]⊺ represents the noise vector where each Ni is a random variable\nassumed to be normally distributed with mean μ = 0 and variance σ2 = 4. Assume each\nNi is independent of each other and independent of the Xi's. Each component value of Y\nis decoded with the same rule as in part (a). The receiver then uses a majority rule to\ndetermine which symbol was sent. The receiver's decoding rules are:\n- If 2 or more components of Y are greater than 0, then conclude the symbol 1 was sent.\n- If 2 or more components of Y are less than 0, then conclude the symbol 0 was sent.\nDetermine the probability of error for this modified encoding/decoding scheme. Reduce\nyour calculations to a single numerical value.\nPage 2 of 3\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n5. The random variables X and Y are described by a joint PDF which is constant within the unit\narea quadrilateral with vertices (0, 0), (0, 1), (1, 2), and (1, 1).\ny\nx\n(a) Are X and Y independent?\n(b) Find the marginal PDFs of X and Y .\n(c) Find the expected value of X + Y .\n(d) Find the variance of X + Y .\n6. A defective coin minting machine produces coins whose probability of heads is a random variable\nP with PDF\n1 + sin(2πp),\nif p ∈ [0, 1],\nfP (p) =\n0,\notherwise.\nIn essence, a specific coin produced by this machine will have a fixed probability P = p of giving\nheads, but you do not know initially what that probability is. A coin produced by this machine\nis selected and tossed repeatedly, with successive tosses assumed independent.\n(a) Find the probability that the first coin toss results in heads.\n(b) Given that the first coin toss resulted in heads, find the conditional PDF of P.\n(c) Given that the first coin toss resulted in heads, find the conditional probability of heads on\nthe second toss.\nG1+ . Let C be the circle {(x, y) | x2 +y2 ≤ 1}. A point a is chosen randomly on the boundary of C and\nanother point b is chosen randomly from the interior of C (these points are chosen independently\nand uniformly over their domains). Let R be the rectangle with sides parallel to the x- and\ny-axes with diagonal ab. What is the probability that no point of R lies outside of C?\n+Required for 6.431; optional for 6.041\nPage 3 of 3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probabilistic Systems Analysis and Applied Probability, Complete Lecture Slides",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/60eede37f87fd886d4ba574e1cf26617_MIT6_041F10_lec_slides.pdf",
      "content": "6.041 Probabilistic Systems Analysis\nCoursework\n6.431 Applied Probability\n- Quiz 1 (October 12, 12:05-12:55pm)\n17%\n- Staff:\n- Quiz 2 (November 2, 7:30-9:30pm)\n30%\n- Lecturer: John Tsitsikli\n\n- Final exam (scheduled by registrar)\n40%\n- Weekly homework (best 9 of 10)\n10%\n- Attendance/participation/enthusiasm in\n3%\nrecitations/tutorials\nPick up and\n\n-\n\nread course information handout\n- Turn in recitation and tutorial scheduling form\n\n- Collaboration policy described in course info handout\n(last sheet of course information handout)\nText: Introduction to Probability, 2nd Edition,\n- Pick up copy of slides\n-\nD. P. Bertsekas and J. N. Tsitsiklis, Athena Scientific, 2008\n\nRead the text!\nLECTURE 1\nSample space Ω\n- Readings: Sections 1.1, 1.2\n- \"List\" (set) of possible outcomes\n- List must be:\nLecture outline\n- Mutually exclusive\n- Collectively exhaustive\n- Probability as a mathematical framework\nfor reasoning about uncertainty\n- Art: to be at the \"right\" granularity\n- Probabilistic models\n- sample space\n- probability law\n- Axioms of probability\n- Simple examples\ns\n\nSample space: Discrete example\nSample space: Continuous example\n- Two rolls of a tetrahedral die\nΩ = {(x, y) | 0 ≤ x, y ≤ 1}\n- Sample space vs. sequential description\ny\n1,1\n1,2\n1,3\n1,4\nY\n= Second\nroll\n\nx\nX = First roll\n4,4\nProbability axioms\nProbability law: Example with finite sample space\n- Event: a subset of the sample space\n- Probability is assigned to events\nY\n= Second\nroll\nAxioms:\n1. Nonnegativity: P(A) ≥ 0\n2. Normalization: P(Ω) = 1\nX = First roll\n3. Additivity: If A ∩ B = Ø, then P(A ∪ B) = P(A) + P(B)\n- Let every possible outcome have probability 1/16\n- P((X, Y )\n\nis (1,1) or (1,2)) =\n- P({s1, s2, . . . , sk}) = P({s1}) + · · · + P({sk})\n-\n=\nP( X = 1 ) =\nP(s ) +\n\n{\n\n· · · + P(sk)\n}\n- P(X + Y is odd) =\n- Axiom 3 needs strengthening\n- Do weird sets have probabilities?\n- P(min(X, Y ) = 2) =\n\nDiscrete uniform law\nContinuous uniform law\n- Let all outcomes be equally likely\n- Two \"random\" numbers in [0, 1].\ny\n- Then,\nnumber of elements of A\nP(A) = total number of sample points\n\nx\n- Computing probabilities\n≡ counting\nUniform law: Probability = Area\n- Defines fair coins, fair dice, well-shuffled decks\n-\n- P(X + Y ≤ 1/2) = ?\n- P( (X, Y ) = (0.5, 0.3) )\nProbability law: Ex. w/countably infinite sample space\n- Sample space: {1, 2, . . .}\n\n-n\n-\nWe are given P(n) = 2\n, n = 1, 2, . . .\n- Find P(outcome is even)\nRemember!\np\n1/2\nTurn\n\n-\nin recitation/tutorial scheduling form now\n1/4\n\n1/8\n1/16 .....\n\nTutorials start next week\n\nP({2, 4, 6, . . .})\n= P (2) + P(4) +\n=\n+\n+\n+\n=\n-\n· · ·\n· · ·\n- Countable additivity axiom (needed for this calculation):\nIf A1, A2, . . . are disjoint events, then:\nP(A1 ∪ A2 ∪ · · · ) = P(A1) + P(A2) + · · ·\n\nLECTURE 2\nReview of probability models\n-\nReadings: Sections 1.3-1.4\n-\nSample space Ω\n-\nMutually exclusive\nCollectively exhaustive\nLecture outline\n-\nRight granularity\n-\nReview\n-\nEvent: Subset of the sample space\n-\nConditional probability\nAllocation of probabilities to events\n-\nThree important tools:\n-\n1.\nP(A) ≥0\n-\nMultiplication rule\n2.\nP(Ω) = 1\n3.\nIf\n-\nTotal probability theorem\nA ∩B = Ø,\nthen P(A ∪B) = P(A) + P(B)\n-\nBayes' rule\n3'.\nIf A1, A2, . . . are disjoint events, then:\nP(A1 ∪A2 ∪· · · ) = P(A1) + P(A2) + · · ·\n-\nProblem solving:\n-\nSpecify sample space\n-\nDefine probability law\n-\nIdentify event of interest\n-\nCalculate...\nConditional probability\nDie roll example\nA\nY = Second\nB\nroll\n-\nP(A | B) = probability of A,\nthat B occurred\ngiven\nis our new universe\nX\n-\nB\n= First roll\n-\nDefinition: Assuming P(B) = 0,\n-\nLet B be the event: min(X, Y ) = 2\nP(A\nP(A\nB) =\n∩B)\n|\n-\nLet M = max(X, Y )\nP(B)\nP(A | B) undefined if P(B) = 0\n-\nP(M = 1 | B) =\n- P(M = 2 | B) =\n\nModels based on conditional\nMultiplication rule\nprobabilities\nP(A\nB\nC) = P(A) P(B\nA) P(C\nA\nB)\n-\nEvent A: Airplane is flying above\n∩\n∩\n·\n|\n·\n|\n∩\nEvent B: Something registers on radar\nscreen\nA\nU\nB\nP(C | A\nU\nB)\nP(B | A)=0.99\nA\nU\nB\nU\nC\nP(B | A)\nc\nP(B | A)=0.01\nP(A)=0.05\nA\nc\nP(B | A)\nc\nA\nU\nB\nU\nC\nP(A)\nA\nU\nc\nB\nc\nP(A )=0.95\nc\nA\nU\nc\nB\nU\nC\nc\nP(B | A )=0.10\nc\nc\nP(B | A )=0.90\nc\nP(A )\nc\nA\nP(A ∩B) =\nP(B) =\nP(A | B) =\nTotal probability theorem\nBayes' rule\n-\nDivide and conquer\n-\n\"Prior\" probabilities P(Ai)\n-\ninitial \"beliefs\"\n-\nPartition of sample space into A1, A2, A3\n-\nWe know P(B | Ai) for each i\n-\nHave P(B | Ai), for every i\n-\nWish to compute P(Ai | B)\nA\n-\nrevise \"beliefs\", given that B occurred\nB\nA1\nB\nA\nA\n-\nOne way of computing\nA\nP(\nA\nB):\nP(B) =\nP(A1)P(B | A1)\n+ P(A2)P(B | A2)\nP(\n+\n(\n)\n(\n|\n)\nAi ∩B)\nP A3 P B\nA3\nP(Ai | B)\n=\nP(B)\nP(Ai)P(B\n=\n| Ai)\nP(B)\nP(A )P(B\nA )\n=\n!\ni\n|\ni\nj P(Aj)P(B | Aj)\n\nLECTURE 3\nModels based on conditional\nprobabilities\n-\nReadings: Section 1.5\n-\n3 tosses of a biased coin:\n-\nReview\nP(H) = p, P(T) = 1 -p\n-\nIndependence of two events\np\nHHH\n-\nIndependence of a collection of events\np\nHHT\n1 - p\np\nHTH\nReview\np\n1 - p\n1 - p\nHTT\nP(A ∩B)\n)\np\nP(A | B) =\n,\nassuming P(B\n>\nTHH\nP(B)\n1 - p\np\n1 - p\nTHT\n-\nMultiplication rule:\np\nTTH\n1 - p\nP(A ∩B) = P(B) · P(A | B) = P(A) · P(B | A)\n1 - p\nTTT\n-\nTotal probability theorem:\nP(\n) = P( )P(\n|\n) + P(\nc)P(\n|\nc\nP(THT) =\nB\nA\nB\nA\nA\nB\nA )\n-\nBayes rule:\nP(1 head) =\nP(Ai)P(B\nAi)\nP(Ai | B) =\n|\nP(B)\nP(first toss is H | 1 head) =\nIndependence of two events\nConditioning may affect independence\n-\n\"Defn:\"\nP(B | A) = P(B)\n-\nConditional independence, given C,\nis defined as independence\n-\n\"occurrence of A\nunder probability law P(\nprovides no information\n· | C)\nabout B's occurrence\"\n-\nAssume A and B are independent\n-\nRecall that P(A ∩B) = P(A) · P(B | A)\n-\nDefn:\nP(A ∩B) = P(A) · P(B)\nC\nA\n-\nSymmetric with respect to A and B\nB\n-\napplies even if P(A) = 0\n-\nimplies P(A | B) = P(A)\n-\nIf we are told that C occurred,\nare A and B independent?\n\nConditioning may affect independence\nIndependence of a collection of events\n-\nTwo unfair coins, A and B:\n-\nIntuitive definition:\nP(H | coin A) = 0.9, P(H | coin B) = 0.1\nInformation on some of the events tells\nchoose either coin with equal probability\nus nothing about probabilities related to\n0.9\nthe remaining events\n0.1\n0.9\n-\nE.g.:\nCoin A\nP(\n∩(\nc ∪\n) |\n∩\nc ) = P(\n∩(\nc\n0.9\nA1\nA2\nA3\nA5 A6\nA1\nA2 ∪A3))\n0.5\n0.1\n0.1\n-\nMathematical definition:\n0.1\nEvents A1, A2, . . . , An\n0.5\n0.1\n0.9\nare called independent if:\nCoin B\n0.1\n0.9\nP(Ai∩Aj∩· · ·∩Aq) = P(Ai)P(Aj) · · · P(Aq)\n0.9\nfor any distinct indices i, j, . . . , q,\n(chosen from {1, . . . , n )\n-\nOnce we know it is coin A, are tosses\n}\nindependent?\n-\nIf we do not know which coin it is, are\ntosses independent?\n-\nCompare:\nP(toss 11 = H)\nP(toss 11 = H | first 10 tosses are heads)\nIndependence vs. pairwise\nThe king's sibling\nindependence\n-\nThe king comes from a family of two\n-\nTwo independent fair coin tosses\nchildren.\nWhat is the probability that\n-\nA:\nFirst toss is H\nhis sibling is female?\n-\nB:\nSecond toss is H\n- P(A) = P(B) = 1/2\nHH\nHT\nTH\nTT\n-\nC:\nFirst and second toss give same\nresult\n- P(C) =\n- P(C ∩A) =\n- P(A ∩B ∩C) =\n- P(C | A ∩B) =\n-\nPairwise independence does not\nimply independence\n\nLECTURE 4\nDiscrete uniform law\n-\nReadings: Section 1.6\n-\nLet all sample points be equally likely\n-\nThen,\nLecture outline\nnumber of elements of A\nA\nP(A) =\n= | |\ntotal number of sample points\nΩ\n-\nPrinciples of counting\n|\n|\n-\nJust count. . .\n-\nMany examples\n-\npermutations\n-\nk-permutations\n-\ncombinations\n-\npartitions\n-\nBinomial probabilities\nBasic counting principle\nExample\n-\nr stages\n-\nProbability that six rolls of a six-sided die\n-\nni choices at stage i\nall give different numbers?\n-\nNumber of outcomes that\nmake the event happen:\n-\nNumber of elements\nin the sample space:\n-\nNumber of choices is:\nn1n2 · · · nr\n-\nAnswer:\n-\nNumber of license plates\nwith 3 letters and 4 digits\n=\n-\n. . . if repetition is prohibited\n=\n-\nPermutations: Number of ways\nof ordering n elements is:\n-\nNumber of subsets of {1, . . . , n}\n=\n\nCombinations\nBinomial probabilities\n-\nn\n: number of k-element subsets\nk\n-\nn independent coin tosses\nof a given n-element set\n-\nP(H) = p\n-\nTwo ways of constructing an ordered\nsequence of k distinct items:\nP(HTTHHH) =\n-\nChoose the k items one at a time:\n-\nn!\nn(n-1) · · · (n-k+1) =\nchoices\n(\n-\n)!\n-\n(sequence) =\n# heads(1 -)# tails\nn\nk\nP\np\np\n-\nChoose k items, then order them\n(k! possible orders)\nP(k heads) =\n\nP(seq.)\n-\nHence:\n\nk-\nad seq.\nn\nhe\nn!\n· k! =\nk\n(n -k)!\n= (# of k-head seqs )\nk\np (1 -\nk\np)n\n. ·\n-\nn\nk\n\nn!\n=\n=\nk(1\n)n-k\np\nk!(n -k)!\nn\nk\n\np\n-\nk\n\nn\n=\n=0\nn\nk\n\nCoin tossing problem\nPartitions\n-\nevent B: 3 out of 10 tosses were \"heads\".\n-\n52-card deck, dealt to 4 players\n-\nGiven that B occurred,\n-\nFind P(each gets an ace)\nwhat is the (conditional) probability\n-\nOutcome: a partition of the 52 cards\nthat the first 2 tosses were heads?\n-\nnumber of outcomes:\n-\nAll outcomes in set B are equally likely:\n52!\nprobability\np (1 -p)7\n13! 13! 13! 13!\n-\nConditional probability law is uniform\n-\nCount number of ways of distributing the\nfour aces: 4 3 2\n-\nNumber of outcomes in B:\n·\n·\n-\nCount number of ways of dealing the\n-\nOut of the outcomes in B,\nremaining 48 cards\nhow many start with HH?\n48!\n12! 12! 12! 12!\n-\nAnswer:\n48!\n4 · 3 · 212! 12! 12! 12!\n52!\n13! 13! 13! 13!\n\nLECTURE 5\nRandom variables\n-\nReadings: Sections 2.1-2.3, start 2.4\n-\nAn assignment of a value (number) to\nevery possible outcome\nLecture outline\n-\nMathematically: A function\n-\nRandom variables\nfrom the sample space Ωto the real\nnumbers\n-\nProbability mass function (PMF)\n-\ndiscrete or continuous values\n-\nExpectation\n-\nCan have several random variables\n-\nVariance\ndefined on the same sample space\n-\nNotation:\n-\nrandom variable X\n-\nnumerical value x\nProbability mass function (PMF)\nHow to compute a PMF pX(x)\n-\ncollect all possible outcomes for which\n-\n(\"probability law\",\nX is equal to x\n\"probability distribution\" of X)\n-\nadd their probabilities\n-\nrepeat for all x\n-\nNotation:\n-\nExample: Two independent rools of a\npX(x)\n= P(X = x)\nfair tetrahedral die\n= P({ω ∈Ωs.t. X(ω) = x})\nF: outcome of first throw\n!\nS: outcome of second throw\n-\npX(x) ≥0\nx pX(x) = 1\nX = min(F, S)\n-\nExample: X=number of coin tosses\nuntil first head\n-\nassume independent tosses,\nP(H) = p > 0\nS = Second roll\npX(k)\n= P(X = k)\n= P(TT · · · TH)\n=\n(1 -p)k-1p,\nk = 1, 2, . . .\nF = First roll\n-\ngeometric PMF\npX(2) =\n\nBinomial PMF\nExpectation\n-\nX:\nnumber of heads in n independent\n-\nDefinition:\ncoin tosses\nE[X] =\n$\nxpX(x)\nx\n-\nP(H) = p\n-\nInterpretations:\n-\nLet n = 4\n-\nCenter of gravity of PMF\n-\nAverage in large number of repetitions\npX(2)\n= P(HHTT) + P(HTHT) + P(HTTH)\nof the experiment\n+P(THHT) + P(THTH) + P(TTHH)\n(to be substantiated later in this course)\n=\n6 2(1 -)2\np\np\n-\nExample: Uniform on 0, 1, . . . , n\n=\n\"\n# 2\n-)2\np (1\np\np (x )\nX\nIn general:\n1/(n+1)\nk\n)n-k\npX(k) =\n\"n#\np (1-p\n,\nk = 0, 1, . . . , n\n. . .\nk\nx\nn- 1\nn\nE[X] = 0×\n+1×\n+· · ·+n\n=\nn + 1\nn + 1\n×n + 1\nProperties of expectations\nVariance\n-\nLet X be a r.v. and let Y = g(X)\nRecall:\nE[g(X)] =\n$\ng(x)pX(x)\nx\n-\nHard: E[Y ] =\n$\nypY (y)\ny\n-\nSecond moment: E[\nX ] = !\nx x p\n(\n-\nEasy: E[Y ] =\n$\nX x)\ng(x)pX(x)\nx\n-\nVariance\n-\nCaution: In general, E[g(X)] = g(E[X])\nvar(X)\n= E\n%\n(X -E[X])2&\n=\n$\n(\n[\n])2\nx\nE X\npX(x)\nx\n-\nerties:\nIf α, β are constants, then:\n= E[\nProp\nX ] -(E[X])\n-\nE[α] =\nProperties:\n-\nE[αX] =\n-\nvar(X) ≥0\n- E[αX + β] =\n-\nvar(αX + β) =\nα var(X)\n\nLECTURE 6\nReview\nRandom variable X: function from\n-\nReadings: Sections 2.4-2.6\n-\nsample space to the real numbers\nLecture outline\n-\nPMF (for discrete random variables):\npX(x) = P(X = x)\n-\nReview: PMF, expectation, variance\n-\nExpectation:\n-\nConditional PMF\nE[X] =\n!\nxpX(x)\nx\n-\nGeometric PMF\nE[g(X)] =\ng(x)p\n(x)\n-\nX\nTotal expectation theorem\n!\nx\n-\nJoint PMF of two random variables\nE[αX + β] = αE[X] + β\n-\nE\n\"\nX -E[X]\n#\n=\nvar(X)\n= E\n=\n$\n(\nX -E[X])\n(x\nE[X])2p\n%\n=\n!\nx\n-\nX(x)\nE[\nX ] -(\nE[X])\nStandard deviation:\nσX =\n&\nvar(X)\nRandom speed\nAverage speed vs. average time\n-\nTraverse a 200 mile distance at constant\n-\nTraverse a 200 mile distance at constant\nbut random speed V\nbut random speed V\np (v )\n1/2\n1/2\np (v )\n1/2\n1/2\nV\nV\nv\nv\n-\nd = 200, T = t(V ) = 200/V\n-\ntime in hours = T = t(V ) =\n- E[T] = E[t(V )] = '\nv t(v)pV (v) =\n-\nE[V ] =\n-\nE[TV ] = 200 = E[T] · E[V ]\n-\nvar(V ) =\n-\nE[200/V ] = E[T] = 200/E[V ].\n-\nσV =\n\nConditional PMF and expectation\nGeometric PMF\n-\nX:\nnumber of independent coin tosses\n-\npX|A(x) = P(X = x | A)\nuntil first head\n-\nE[X | A] =\n!\nxp\n(x)\n( ) = (1 -)k-1\nX\np\nx\n|A\npX k\np,\nk = 1, 2, . . .\ninf\ninf\np (x )\nE[X] =\n(1\nk\nkp\n(\nX\n!\nX k) =\np\nk\n!\nk\n-\n=1\n-p)\nk=1\n-\nMemoryless property: Given that X > 2,\n1/4\nthe r.v. X -2 has same geometric PMF\np\np\n(k)\np\n(k)\nX\nX |X>2\np(1-p)\np\nx\n\n...\n...\n-\nLet A = {X ≥2}\nk\nk\np\n(k)\nX- 2|X>2\npX|A(x) =\np\nE[X | A] =\n...\nk\nTotal Expectation theorem\nJoint PMFs\n-\nPartition of sample space\n-\npX,Y (x, y) = P(X = x and Y = y)\ninto disjoint events A1, A2, . . . , An\ny\nA1\n1/20 2/20 2/20\nB\n2/20 4/20\n1/20\n2/20\n1/20\n3/20 1/20\nA\nA\n1/20\nx\nP(B) = P(A1)P(B | A1)+· · ·+P(An)P(B | An)\n-\npX,Y (x, y) =\npX(x) = P(A1)pX A (x)+· · ·+P(An)pX A (x)\nn\n!\nx\n!\ny\n|\n|\nE[X] = P(A1)E[X | A1]+· · ·+P(An)E[X | An]\n-\npX(x) =\n!\npX,Y (x, y)\ny\n-\nGeometric example:\npX,Y (x, y)\npX Y (x\ny) = P(X = x\nY = y) =\nA1 : {X = 1},\nA2 : {X > 1\n-\n}\n|\n|\n|\npY (y)\nE[X]\n=\nP(X = 1)E[X | X = 1]\n-\n!\npX Y (x | y) =\n|\n+P(X > 1)E[X | X > 1]\nx\n-\nSolve to get E[X] = 1/p\n\nLECTURE 7\nReview\n-\nReadings: Finish Chapter 2\npX(x) = P(X = x)\nLecture outline\npX,Y (x, y) = P(X = x, Y = y)\n-\nMultiple random variables\np\n)\n-\nJoint PMF\nX Y (x | y) = P(X = x\nY = y\n|\n|\n-\nConditioning\n-\nIndependence\npX(x) =\n-\nMore on expectations\n!\npX,Y (x, y)\ny\n-\nBinomial distribution revisited\npX,Y (x, y) = pX(x)pY X(y\nx\n|\n|\n)\n-\nA hat problem\nIndependent random variables\nExpectations\npX,Y,Z(x, y, z) = pX(x)pY X(y | x)pZ X,Y (z | x, y)\n[\n|\n|\nE X] =\n!\nxpX(x)\nx\nE[g(X, Y )] =\ng(x, y)pX,Y (x, y)\n-\nRandom variables X, Y , Z are\nx\ny\nindependent if:\n! !\npX,Y,Z(x, y, z) = pX(x) · pY (y) · pZ(z)\n-\nIn general: E[g(X, Y )] = g E[X], E[Y ]\nfor all x, y, z\n\"\n#\ny\n-\nE[αX + β] = αE[X] + β\n1/20 2/20 2/20\n-\nE[X + Y + Z] = E[X] + E[Y ] + E[Z]\n2/20 4/20\n1/20\n2/20\n1/20\n3/20 1/20\n-\nIf X, Y are independent:\n1/20\n- E[XY ] = E[X]E[Y ]\nx\n- E[g(X)h(Y )] = E[g(X)]\n-\nIndependent?\n· E[h(Y )]\n-\nWhat if we condition on X ≤2\nand Y ≥3?\n\nVariances\nBinomial mean and variance\n-\nVar(aX) =\na Var(X)\n-\nX = # of successes in n independent\ntrials\n-\nVar(X + a) = Var(X)\n-\nprobability of success p\nn\nn\n-\nLet Z = X + Y .\nk\nE[X] =\nk\np (1\nk\n-p)n-k\nIf X, Y are independent:\nk\n!\n=0\n\" #\nVar(X + Y ) = Var(X) + Var(Y )\n\n1,\nif success in trial i,\n-\nXi = 0,\notherwise\n-\nExamples:\nE[Xi] =\n-\nIf X = Y , Var(X + Y ) =\n-\n-\nIf X = -Y , Var(X + Y ) =\n-\nE[X] =\n-\nIf X, Y indep., and Z = X -3Y ,\nVar(Z) =\n-\nVar(Xi) =\n-\nVar(X) =\nThe hat problem\nVariance in the hat problem\n-\npeople throw their hats in a box and\n-\nVar(\n) =\n[\nn\nX\nE X ] -(E[X])\n= E[\nX ] -1\nthen pick one at random.\n-\nX: number of people who get their own\nhat\n2 =\n!\nX\nXi +\ni\ni,j\n!\nXiXj\n:i=j\n-\nFind E[X]\n-\nE[\nX ] =\ni\nXi =\n\n1,\nif i selects own hat\n0,\notherwise.\nP(X1X2 = 1) = P(X1 = 1)·P(X2 = 1 | X1 = 1)\n-\nX = X1 + X2 + · · · + Xn\n=\n-\nP(Xi = 1) =\n-\nE[Xi] =\n-\nAre the Xi independent?\n-\nE[\nX ] =\n-\nE[X] =\n-\nVar(X) =\n\nLECTURE 8\nContinuous r.v.'s and pdf's\n-\nA continuous r.v.\nis described by a\n-\nReadings: Sections 3.1-3.3\nprobability density function fX\nLecture outline\nf (x)\nX\nS a m p l e S p a c e\n-\nProbability density functions\n-\nCumulative distribution functions\na\nb\nx\nEvent {a < X < b }\n-\nNormal random variables\nP(a ≤X ≤b) =\n! b\nfX(x) dx\na\n! inf\nfX(x) dx = 1\n-inf\nP(x ≤X ≤x + δ) =\n! x+δ\nfX(s) ds\nx\n≈fX(x) · δ\nP(X ∈B) =\n!\nfX(x) dx,\nfor \"nice\" sets B\nB\nMeans and variances\nCumulative distribution function\n- E[X] =\n! inf\n(CDF)\nxfX(x) dx\n-inf\nx\nE[g(X)]\n! inf\n-\n=\ng(x)fX(x) dx\nFX(x) = P(X ≤x) =\n!\nfX(t) dt\n-inf\n-inf\ninf\n-\nvar(\n) =\n2 =\n!\n(\n[\n])2\nX\nσ\nx\nE X\nfX(x)\nX\n-\ndx\n-inf\nf (x\nCDF\nX\n)\n-\nContinuous Uniform r.v.\nf (x )\nX\na\nb\nx\na\nb\nx\n-\nAlso for discrete r.v.'s:\na\nb\nx\nFX(x) = P(X ≤x) =\n$\npX(k)\nk≤x\n-\nfX(x) =\na ≤x ≤b\n3/6\n2/6\n-\nE[X] =\n1/6\n! b \"\n+\n#2\n( -\n)2\na\nx\n-\na\nb\nb\nσ\n=\nx\na\n-\nx\ndx =\nX\nb -a\n\nMixed distributions\nGaussian (normal) PDF\n-\nSchematic drawing of a combination of\n-\nStandard normal\n(0 1):\n( ) = √\nx /2\nN\n,\nfX x\ne-\na PDF and a PMF\n2π\n1/2\nNormal CDF F (x)\nNormal PDF f (x)\nX\nx\n0.5\n-1\n-1\nx\nx\n\nE[X] =\nvar(X) = 1\n1/2\n-\nx 0\n-\nGeneral normal N(\nμ, σ ):\n-\nThe corresponding CDF:\n-( -)2 2 2\nfX(\n√\nx\nμ\n/ σ\nx) =\ne\nσ\n2π\nFX(x) = P(X ≤x)\nCDF\nout that:\n-\nIt turns\nE[\nX] = μ\nand\nVar(X) = σ .\n3/4\n-\nLet Y = aX + b\n1/4\n-\nThen: E[Y ] =\nVar(Y ) =\n-\nFact: Y\n(\nN aμ +\nb, a σ )\n1/2\nx\n∼\nCalculating normal probabilities\nThe constellation of concepts\n-\nNo closed form available for CDF\n-\nbut there are tables\n(for standard normal)\npX(x)\nfX(x)\nF\n( )\nX\nX x\n-\nIf X ∼N(\nμ\nμ, σ ), then\n-\n∼N(\n)\nE[X], var(X)\nσ\n-\nIf X ∼\np\nN(2, 16):\nX,Y (x, y)\nfX,Y (x, y)\npX Y (x | y)\nf\n|Y (x\nX -2\nX\ny)\nP(X\n-2\n|\n|\n≤3) = P\n\"\n≤\n#\n= CDF(0.25)\n.00\n.01\n.02\n.03\n.04\n.05\n.06\n.07\n.08\n.09\n0.0\n.5000\n.5040\n.5080\n.5120\n.5160\n.5199\n.5239\n.5279\n.5319\n.5359\n0.1\n.5398\n.5438\n.5478\n.5517\n.5557\n.5596\n.5636\n.5675\n.5714\n.5753\n0.2\n.5793\n.5832\n.5871\n.5910\n.5948\n.5987\n.6026\n.6064\n.6103\n.6141\n0.3\n.6179\n.6217\n.6255\n.6293\n.6331\n.6368\n.6406\n.6443\n.6480\n.6517\n0.4\n.6554\n.6591\n.6628\n.6664\n.6700\n.6736\n.6772\n.6808\n.6844\n.6879\n0.5\n.6915\n.6950\n.6985\n.7019\n.7054\n.7088\n.7123\n.7157\n.7190\n.7224\n0.6\n.7257\n.7291\n.7324\n.7357\n.7389\n.7422\n.7454\n.7486\n.7517\n.7549\n0.7\n.7580\n.7611\n.7642\n.7673\n.7704\n.7734\n.7764\n.7794\n.7823\n.7852\n0.8\n.7881\n.7910\n.7939\n.7967\n.7995\n.8023\n.8051\n.8078\n.8106\n.8133\n0.9\n.8159\n.8186\n.8212\n.8238\n.8264\n.8289\n.8315\n.8340\n.8365\n.8389\n1.0\n.8413\n.8438\n.8461\n.8485\n.8508\n.8531\n.8554\n.8577\n.8599\n.8621\n1.1\n.8643\n.8665\n.8686\n.8708\n.8729\n.8749\n.8770\n.8790\n.8810\n.8830\n1.2\n.8849\n.8869\n.8888\n.8907\n.8925\n.8944\n.8962\n.8980\n.8997\n.9015\n1.3\n.9032\n.9049\n.9066\n.9082\n.9099\n.9115\n.9131\n.9147\n.9162\n.9177\n1.4\n.9192\n.9207\n.9222\n.9236\n.9251\n.9265\n.9279\n.9292\n.9306\n.9319\n1.5\n.9332\n.9345\n.9357\n.9370\n.9382\n.9394\n.9406\n.9418\n.9429\n.9441\n1.6\n.9452\n.9463\n.9474\n.9484\n.9495\n.9505\n.9515\n.9525\n.9535\n.9545\n1.7\n.9554\n.9564\n.9573\n.9582\n.9591\n.9599\n.9608\n.9616\n.9625\n.9633\n1.8\n.9641\n.9649\n.9656\n.9664\n.9671\n.9678\n.9686\n.9693\n.9699\n.9706\n1.9\n.9713\n.9719\n.9726\n.9732\n.9738\n.9744\n.9750\n.9756\n.9761\n.9767\n2.0\n.9772\n.9778\n.9783\n.9788\n.9793\n.9798\n.9803\n.9808\n.9812\n.9817\n\nLECTURE 9\nContinuous r.v.'s and pdf's\n-\nReadings: Sections 3.4-3.5\nf (x)\nX\nS a m p l e S p a c e\nOutline\n-\nPDF review\na\nb\nx\nEvent {a < X < b }\n-\nMultiple random variables\n-\nconditioning\nb\n-\nindependence\nP(a ≤X ≤b) =\nf\na\n-\nExamples\n\nX(x) dx\n-\nP(x ≤X ≤x + δ) ≈fX(x) · δ\nSummary of concepts\n-\nE[g(X)] =\ninf\ng(x)fX(x) dx\npX(x)\nfX(x)\n-inf\nF\n(x)\n\nX\nxpX(x)\nE[X]\nx\n\nxfX(x) dx\nvar(X)\npX,Y (x, y)\nfX,Y (x, y)\npX|A(x)\nfX A(x)\n|\npX Y (x\ny\n|\n|\n)\nfX\n(x\n|Y\n| y)\nJoint PDF fX,Y (x, y)\nBuffon's needle\n-\nParallel lines at distance d\nNeedle of length (assume < d)\nFind P(needle intersects one of the lines)\nP((X, Y ) ∈S) =\n\nfX,Y (x, y) dx dy\n-\nS\nq\nx\nl\n-\nd\nInterpretation:\n(\n≤\n≤\n+\n≤\n≤\n+ ) ≈\n(\n)· 2\nP x\nX\nx\nδ, y\nY\ny\nδ\nfX,Y x, y\nδ\n-\nX ∈[0, d/2]: distance of needle midpoint\nto nearest line\n-\nExpectations:\n-\nModel: X, Θ uniform, independent\n(\n) =\n2 0\n[ (\n)] =\ninfinf\n(\nf\nx, θ\nx\nd/ ,\nθ\nπ/2\nE g X, Y\ng x, y)fX,Y (x, y) dx dy\nX,Θ\n≤\n≤\n≤\n≤\n-inf\n-inf\n\n-\nFrom the joint to the marginal:\n-\nIntersect if X ≤\nsin Θ\nfX(x) · δ ≈P(x ≤X ≤x + δ) =\nP\n\nX ≤\nsin Θ\n\n=\n\nfX(x)fΘ(θ) dx dθ\n≤\nx\nsin θ\n4 π/2 (/2) sin θ\n=\ndx dθ\nπd\n-\nX and Y are called independent if\nπ/2\n=\n\nsin θ\nfX,Y (x, y) =\nX( )\ndθ =\nf\nx fY (y),\nfor all x, y\nπd\nπd\n\nConditioning\n-\nRecall\nP(x ≤X ≤x + δ) ≈fX(x) · δ\n-\nBy analogy, would like:\nP(x ≤X ≤x + δ | Y ≈y) ≈fX\n(\n|Y x | y) · δ\n-\nThis leads us to the definition:\nfX,Y (x, y)\nfX Y (x | y) =\nif fY (y) > 0\n|\nfY (y)\n-\nFor given y, conditional PDF is a\n(normalized) \"section\" of the joint PDF\n-\nIf independent, fX,Y = fXfY , we obtain\nfX Y (x|y) = fX(x)\n|\nArea of slice = Height of marginal\ndensity at x\nSlice through\ndensity surface\nfor fixed x\nRenormalizing slices for\nfixed x gives conditional\ndensities for Y given X = x\nJoint, Marginal and Conditional Densities\nImage by MIT OpenCourseWare, adapted from\nProbability, by J. Pittman, 1999.\nStick-breaking example\n-\nBreak a stick of length twice:\nbreak at X: uniform in [0, 1];\nbreak again at Y , uniform in [0, X]\nf\n(y | x)\nf\n(x)\nY |X\nX\n\nx\nL\n\ny\nfX,Y (x, y) = fX(x)fY\n(y\nx) =\n|X\n|\non the set:\ny\nL\nL\nx\nE[Y | X = x] =\n\nyfY\n(y | X\n) dy =\n|X\n= x\nfX,Y (x, y) =\n,\n0 ≤y ≤x\n\nx\n≤\ny\nL\nL\nx\nfY (y)\n=\n\nfX,Y (x, y) dx\n=\n\ndx\ny x\n\n=\nlog\n,\n0 ≤y\n\ny\n≤\n\nE[Y ] =\n\nyfY (y) dy =\n\ny\nlog\ndy =\n\ny\n\nLECTURE 10\nThe Bayes variations\nContinuous Bayes rule;\npX,Y (x, y)\npX(x)pY\ny\nx)\n(x\ny) =\n=\n|X(\np\n|\nDerived distributions\nX|Y\n|\npY (y)\npY (y)\n-\nReadings:\npY (y) =\npX(x)pY X(y | x)\nx\n|\nSection 3.6; start Section 4.1\n\nExample:\nReview\n-\nX = 1, 0: airplane present/not present\n-\nY = 1, 0: something did/did not register\np\n(\non\nada\nX(x)\nf\nr\nr\nX x)\npX,Y (x, y)\nfX,Y (x, y)\npX,Y (x, y)\nfX,Y (x, y)\nContinuous counterpart\npX\n(x\ny) =\nf\n(x\ny) =\n|Y\n|\npY (y)\nX|Y\n|\nfY (y)\n\ninf\nf\n(x, y)\nfX(x)f\np\n(x) =\np\n(x, y)\nf\n(x) =\nf\n(x, y) dy\n|\nX,Y\nY |X(y\n)\nX\nX,Y\nX\nX,\nf\n(x\n=\ny\n-inf\nX|Y\n| x\nY\ny) =\nfY (y)\nfY (y)\nfY (y) =\nfX(x)fY X(y\nx) dx\nx\n|\n|\nFX(x) = P(\n)\n\nX ≤x\nExample: X: some signal; \"prior\" fX(x)\nE[X],\nvar(X)\nY : noisy version of X\nfY X(y | x): model of the noise\n|\nDiscrete X, Continuous Y\nWhat is a derived distribution\n-\nIt is a PMF or PDF of a function of one\npX(x)f\n(y\nx)\np\nY (x |\nY\nX\ny) =\n|X\n|\nor more random variables with known\n|\nfY (y)\nprobability law. E.g.:\n) =\ny\nfY (y\n\npX(x)fY |X(y | x)\nf\n(y,x)=1\nX,Y\nx\nExample:\n-\nX: a discrete signal; \"prior\" pX(x)\n-\nY : noisy version of X\n-\nfY X(y | x): continuous noise model\n|\nx\nContinuous X, Discrete Y\n-\nObtaining the PDF for\nfX(x)pY\nf\nx\nX, Y ) =\nX\nY/X\n|Y (\ny) =\n|X(y | x)\n|\ng(\npY (y)\n\ninvolves deriving a distribution.\npY (y) =\nfX(x)pY X(y | x) dx\nNote: g(X, Y ) is a random variable\nx\n|\nExample:\nWhen not to find them\n-\nX:\na continuous signal; \"prior\" fX(x)\n(e.g., intensity of light beam);\n-\nDon't need PDF for g(X, Y ) if only want\n-\nY : discrete r.v. affected by X\nto compute expected value:\n(e.g., photon count)\nE[g(X, Y )] =\ng(x, y)f\n(x, y) dx dy\n-\ns\nt\nX,\nX(y | x): model of the di cre e r. .\n\nY\npY\nv\n|\n\nHow to find them\nThe continuous case\n-\nDiscrete case\n-\nTwo-step procedure:\n-\nObtain probability mass for each\n-\nGet CDF of Y :\nFY (y) = P(Y\ny)\npossible value of Y = g(X)\n≤\n-\nDifferentiate to get\npY (y)\n= P(g(X) = y)\n=\n\ndF\np\nY\nX(x)\nfY (y) =\n(y)\nx: g(x)=y\ndy\nx\ny\n.\ng(x)\n.\nExample\n.\n.\n-\nX: uniform on [0,2]\n.\n.\n-\nFind PDF of\n=\nY\nX\n.\n.\n-\nSolution:\n.\n.\nFY (y)\n= P(Y ≤y) = P(X\n≤y)\n.\n.\n=\n1 3\n1 1 3\nP(\n/\nX ≤y\n) =\n/\ny\n.\n.\ndF\nfY (\nY\ny) =\n(y) =\ndy\n6 2\ny /3\nExample\nThe pdf of Y=aX+b\n-\nJoan is driving from Boston to New York.\nHer speed is uniformly distributed be-\nY = 2X + 5:\ntween 30 and 60 mph. What is the dis-\ntribution of the duration of the trip?\nfX\nfaX\nfaX+b\n-\nLet T(V ) =\n.\nV\n-\nFind fT(t)\n- 2\n- 1\nf (v )\nv\n1/30\ny\nb\nfY (y) =\nf\n|a| X\n\n-\na\n\nv0\n-\nUse this to check that if X is normal,\nthen Y = aX + b is also normal.\n\nLECTURE 11\nA general formula\nDerived distributions; convolution;\n-\nLet Y = g(X)\ncovariance and correlation\ng strictly monotonic.\nd g\n-\nReadings:\nslope\n(x)\ny\ndx\n\nFinish Section 4.1;\nSection 4.2\ng(x)\n[y, y+?]\nExample\ny\nx\nf\n(y,x)=1\nX,Y\n[x, x+d]\n-\nEvent x ≤X ≤x + δ is the same as\ng(x) ≤Y ≤g(x + δ)\nor (approximately)\nx\ng(x) ≤Y ≤g(x) + δ|(dg/dx)(x)|\nFind the PDF of Z = g(X, Y ) = Y/X\n-\nHence,\ndg\nδfX(x) = δfY (y)\n(z\nz ≤1\n!!!\n(x)\nF\n) =\ndx\nZ\n!\n!!!\nwhere y = g(x)\n!\nFZ(z) =\nz ≥1\nThe distribution of X + Y\nThe continuous case\n-\nW = X + Y ; X, Y independent\n-\nW = X + Y ; X, Y independent\n.\ny\ny\n.\n\n(0,3)\n.(1,2)\nw\n.(2,1)\n.(3,0)\n.\nx\nw\nx\n\npW(w)\n= P(X + Y = w)\n\"\nx + y = w\n=\nP(X = x)P(Y = w -x)\n=\n\"\nx\np\n(x)p (w -x)\n-\nfW X(w | x) = fY (w -x)\nX\nY\nx\n|\n-\nfW,X(w, x) = fX(x)f\n-\nMechanics:\nW|X(w | x)\n= fX(x)fY (w\nx)\n-\nPut the pmf's on top of each other\n-\n-\nFlip the pmf of Y\nf\n(w) =\n# inf\n-\nW\nfX(x)fY (w -x) dx\n-\nShift the flipped pmf by w\n-inf\n(to the right if w > 0)\n-\nCross-multiply and add\n\nTwo independent normal r.v.s\nThe sum of independent normal r.v.'s\n-\nX ∼N(\nμx, σx),\nY ∼N(\nμy, σy),\n-\nX ∼N(0, σx),\nY ∼N(0, σy),\nindependent\nindependent\nfX,Y (x, y)\n=\nfX(x)fY (y)\nLet\n$\nW = X + Y\n(\n-\n)2\n( -\n)2\n-\nx\nμ\n=\nexp\n-\nx\ny\nμy\n-\n%\nfW(w)\n=\n# inf\nfX(x)fY (w -x) dx\n2πσxσy\n2σx\n2σy\n-inf\n-2\nx /2 2\n=\ninf\nσ\n-x)\nx -(w\n/2σ\ne\ne\ny dx\n-\nPDF is constant on the ellipse where\n2πσxσy\n#\n-inf\n(x -\nμx)\n(\n(algebra)\n=\nce\ny\n+\n2σ2\n-μy)2\n-γw\nx\n2σ2y\nConclusion:\nis constant\n-\nW is normal\n-\nmean=0, variance= 2\nσx +\nσy\n-\nEllipse is a circle when σx = σy\n-\nsame argument for nonzero mean case\nCovariance\nCorrelation coefficient\n-\ncov(X, Y ) = E\n&\n(X -E[X]) · (Y -E[Y ])\n'\n-\nDimensionless version of covariance:\n,\n(X\nE[X])\n(Y\nE[Y ])\n-\nZero-mean case:\ncov(X, Y ) = E [XY ]\nρ\n= E\n-\n-\nσX\n·\nσY\n-\nx\n.\ncov(\nx\nX, Y )\n.\n=\n.\n.\n...\nσXσY\n..\n.\n.\n.\n. .\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n. . . .\n.\n.\n. .\n..\n.\n.\n.\n.\n.\n. .\n..\n.\n. .\n.\n.\n.\n.\n.\nρ\n.\n.\n.\n.\n. .\n.\n.\n. . . .\n.\n.\n.\n.\n.\n. .. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n-\n-\n.\n.\n..\n≤\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n≤\n.\n.\n.\n.\n.\n.\n.\n.\n. . .\n.\n.\n.\n.\n.\n.\n.\ny\n.\n.\n.\n..\n.\n.\ny\n.\n.\n...\n..\n.\n.\n-\n|ρ = 1\n(X\nE[X]) = c(Y\nE[Y ])\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n|\n⇔\n-\n-\n..\n.\n..\n.\n.\n(linearly related)\n..\n.\n..\n.\n.\n.\n-\nIndependent ⇒ρ = 0\n-\ncov(X, Y ) = E[XY ] -E[X]E[Y ]\n(converse is not true)\n\nn\n-\nvar\nn\n\nX\n)\ni\n\"\ni\n=1\n=\nvar(Xi) + 2\ncov(Xi, Xj\ni\n\"\n=1\n(i,j\n\"\n):i=j\n-\nindependent ⇒cov(X, Y ) = 0\n(converse is not true)\n\nLECTURE 12\nConditional expectations\n-\nReadings: Section 4.3;\n-\nGiven the value y of a r.v. Y :\nparts of Section 4.5\nE[X | Y = y] =\nxp\nno\n!\nX|Y (x\ny)\n(mean and variance only;\ntransforms)\nx\n|\n(integral in continuous case)\nLecture outline\n-\nStick example: stick of length l\nbreak at uniformly chosen point Y\n-\nConditional expectation\nbreak again at uniformly chosen point X\n-\nLaw of iterated expectations\ny\n- E[X | Y = y] =\n(number)\n-\nLaw of total variance\n-\nSum of a random number\nY\nof independent r.v.'s\nE[X | Y ] =\n(r.v.)\n-\nmean, variance\n-\nLaw of iterated expectations:\nE[E[X | Y ]] =\n!\nE[X | Y = y]pY (y)= E[X]\ny\n-\nIn stick example:\nE[X] = E[E[X | Y ]] = E[Y/2] = l/4\nvar(X | Y ) and its expectation\nSection means and variances\nTwo sections:\n-\nvar(X | Y =\n) = E\n\"\n(\ny\nX -E[X | Y = y])\n| Y = y\n#\ny = 1 (10 students); y = 2 (20 students)\n-\nvar(X | Y ): a r.v.\nwith\n!\nvalue var(X | Y = y) when Y = y\ny = 1 :\nxi = 90\ny = 2 :\nxi = 60\ni=1\ni=11\n-\nLaw of total variance:\n!\nvar(X) = E[var(X | Y )] + var(E[X | Y ])\n+\nE[X] =\n=\n30 i\n!\n90 10\nx\n· 20\ni\n·\n= 70\n=1\nProof:\nE[X | Y = 1] = 90,\nE[X | Y = 2] = 60\n(a) Recall: var(X) = E[\nX ] -(E[X])2\n90,\nw.p. 1/3\nE[X\nY ] =\n|\n\n60,\nw.p. 2/3\n(b) var(X | Y ) = E[X\n| Y ] -(E[X | Y ])\n[ [\n|\n]] = 1 · 90 + 2\nE E X\nY\n· 60 = 70 = E[X]\n(c) E[var(X | Y )] = E[\nX ] -E[ (E[X | Y ])2 ]\n(d) var(E[X | Y ]) =\nE[X | Y ])2\nE[ (\n]-(E[X])2\nvar(E[X | Y ])\n=\n(90 -70)2 +\n(60 -70)2\n=\n= 200\nSum of right-hand sides of (c), (d):\n[\n2] -\n])2\nE X\n(E[X\n= var(X)\n\nSection means and variances (ctd.)\nExample\n!\n!\nvar(X) = E[var(X | Y )] + var(E[X\n])\n-\nY\n(xi\n90)\n= 10\n(xi-60)\n= 20\n|\ni=1\ni=11\nf\n(x)\nX\n2/3\nvar(X | Y = 1) = 10\nvar(X | Y = 2) = 20\n1/3\n\nx\n\nY=1\nY=2\n\n10,\nw.p. 1/3\nvar(X | Y ) = 20,\nw.p. 2/3\nE[X | Y = 1] =\nE[X | Y = 2] =\n[var(X |\n=\n20 = 50\nY )]\n10 + 2\nE\n3 ·\n3 ·\nvar(X | Y = 1) =\nvar(X | Y = 2) =\nvar(X)\n= E[var(X | Y )] + var(E[X | Y ])\nE[X] =\n=\n+ 200\n=\n(average variability within sections)\nvar(E[X | Y ]) =\n+ (variability between sections)\nSum of a random number of\nVariance of sum of a random number\nindependent r.v.'s\nof independent r.v.'s\n-\nN: number of stores visited\n-\nvar(Y ) = E[var(Y | N)] + var(E[Y\n(N is a nonnegative integer r.v.)\n| N])\nE[Y\nN] = N E[X]\n-\nXi: money spent in store i\n-\n|\nvar(E[Y | N]) = (E[X])2 var(N)\n-\nXi assumed i.i.d.\nvar(Y\nN = n) = n var(X)\n-\nindependent of N\n-\n|\nvar(Y | N) = N var(X)\n-\nLet Y = X1 + · · · + X\nE[var(Y | N)] = E[N] var(X)\nN\nE[Y | N = n]\n= E[X1 + X2 + · · · + Xn | N = n]\n= E[X1 + X2 + · · · + Xn]\n= E[X ] + E[X ] + · · · + E[X ]\nvar(Y )\n= E[var(Y | N)] + var(E[Y\n| N])\nn\n=\nn E[X]\n=\n[\n] var(\n) + (\nE N\nX\nE[X]) var(N)\n- E[Y | N] = N E[X]\nE[Y ]\n= E[E[Y | N]]\n= E[N E[X]]\n= E[N] E[X]\n\nLECTURE 13\nThe Bernoulli process\nA sequence of independent\nThe Bernoulli process\n-\nBernoulli trials\n-\nReadings: Section 6.1\n-\nAt each trial, i:\n- P(success) = P(Xi = 1) = p\nLecture outline\n- P(failure) = P(Xi = 0) = 1 -p\n-\nDefinition of Bernoulli process\nExamples:\n-\nRandom processes\n-\n-\nSequence of lottery wins/losses\n-\nBasic properties of Bernoulli process\n-\nSequence of ups and downs of the Dow\n-\nDistribution of interarrival times\nJones\n-\nArrivals (each second) to a bank\n-\nThe time of the kth success\n-\nArrivals (at each time slot) to server\n-\nMerging and splitting\nRandom processes\nNumber of successes S in n time slots\n-\nFirst view:\n-\nP(S = k) =\nsequence of random variables X1, X2, . . .\n- E[Xt] =\n-\nE[S] =\n-\nVar(Xt) =\n-\nVar(S) =\n-\nSecond view:\nwhat is the right sample space?\n- P(Xt = 1 for all t) =\n-\nRandom processes we will study:\n-\nBernoulli process\n(memoryless, discrete time)\n-\nPoisson process\n(memoryless, continuous time)\n-\nMarkov chains\n(with memory/dependence across time)\n\nInterarrival times\nTime of the kth arrival\n-\nT1: number of trials until first success\n-\nGiven that first arrival was at time t\ni.e., T1 = t:\n-\nP(T1 = t) =\nadditional time, T2, until next arrival\n-\nMemoryless property\n-\nhas the same (geometric) distribution\n-\nindependent of T1\n-\nE[T1] =\n-\nVar(T1) =\n-\nYk: number of trials to kth success\n- E[Y\n-\nIf you buy a lottery ticket every day, what\nk] =\nis the distribution of the length of the\nfirst string of losing days?\n-\nVar(Yk) =\n-\nP(Yk = t) =\nSplitting of a Bernoulli Process\nMerging of Indep. Bernoulli Processes\n(using independent coin flips)\nBernoulli (p)\ntime\ntime\nMerged process:\nBernoulli (p q pq\nq\n+ -\n)\ntime\nOriginal\nprocess\ntime\nBernoulli (q)\n1 - q\ntime\ntime\nyields a Bernoulli process\nyields Bernoulli processes\n(collisions are counted as one arrival)\n\nLECTURE 14\nBernoulli review\nDiscrete time; success probability p\nThe Poisson process\n-\nNumber of arrivals in\nm\not\n-\nn ti\ne sl\ns:\nReadings: Start Section 6.2.\n-\nbinomial pmf\n-\nInterarrival times: geometric pmf\nLecture outline\nTime to\nar\n: Pascal p\nf\n-\nk\nrivals\nm\nReview of Bernoulli process\n-\nMemorylessness\n-\nDefinition of Poisson process\n-\n-\nDistribution of number of arrivals\n-\nDistribution of interarrival times\n-\nOther properties of the Poisson process\nDefinition of the Poisson process\nPMF of Number of Arrivals N\nt\nt\nt\nt\nt\nt\n!\n!\nx\nx\nx x\nx\nx\nx x\nx\nx\nx\nx\nx\nx x\nx\nx\nx x\nx\nx\nx\nTime\nTime\n-\nTime homogeneity:\nFinely discretize [0, t]: approximately Bernoulli\nP(k, τ) = Prob. of k arrivals in interval\n-\nof duration τ\nNt (of discrete approximation): binomial\n-\nNumbers of arrivals in disjoint time\n-\nintervals are independent\n-\nTaking δ →0 (or n →inf) gives:\n-\nSmall interval probabilities:\n(λτ)ke-λτ\nFor VERY small δ:\nP(k, τ) =\n,\nk = 0, 1, . . .\nδ\n\nk!\nif\n) ≈\n\n-λδ,\nk = 0;\nP(k,\nλδ,\nif k = 1;\n-\nE[Nt] = λt,\nvar(\n\nN\nt\nif k > 1.\n-\nλ:\nr ival r te\n\nt) = λ\n,\n\"a r\na\n\"\n\nExample\nInterarrival Times\n-\nYou get email according to a Poisson\n-\nYk time of kth arrival\nprocess at a rate of λ = 5 messages per\n-\nErlang distribution:\nhour. You check your email every thirty\nminutes.\nλk\nyk-e-λy\nfY (y) =\n,\ny\nk\n(k -1)!\n≥\n-\nProb(no new messages) =\n-\nProb(one new message) =\n-\nTime of first arrival (k = 1):\nexponential:\nfY (y) = λe-λy,\ny\n≥0\n-\nMemoryless property: The time to the\nnext arrival is independent of the past\nBernoulli/Poisson Relation\nMerging Poisson Processes\nSum of independent Poisson random\n! ! ! ! ! ! ! !\n-\nn = t /!\nvariables is Poisson\nx\nx\nx\nnp =\"t\n=\nTime\np \"!\nArrivals\n-\nMerging of independent Poisson processes\nis Poisson\nRed bulb flashes\n(Poisson)\nPOISSON\nBERNOULLI\nAll flashes\n\"\nTimes of Arrival\nContinuous\nDiscrete\n(Poisson)\nArrival Rate\n/uni\nime\np/per trial\n\"\nλ\nt t\nGreen bulb flashes\nPMF of # of Arrivals\nPoisson\nBinomial\n(Poisson)\nInterarrival Time Distr.\nExponential\nGeometric\n-\nWhat is the probability that the next\nTime to k-th arrival\nErlang\nPascal\narrival comes from the first process?\nGraph of\n\nin\nterarr\nival t\nimes for r = 1, 2, 3.\nImage by MIT OpenCourseWare.\n\nLECTURE 15\nReview\nDefining characteristics\nPoisson process -- II\n-\n-\nTime homogeneity: P(k, τ)\n-\nReadings: Finish Section 6.2.\n-\nIndependence\n-\nSmall interval probabilities (small δ):\n-\nReview of Poisson process\n\n1 -λδ,\nif k = 0,\nP(k, δ) ≈\nλδ,\nif k = 1,\n-\nMerging and splitting\n\n0,\nif k > 1.\n-\nExamples\n-\nNτ is a Poisson\n\nr.v., with parameter λτ:\n(λτ)ke-λτ\n-\nRandom incidence\nP(k, τ) =\n,\nk = 0, 1, . . .\nk!\nE[Nτ] = var(Nτ) = λτ\n-\nInterarrival times (k = 1): exponential:\nfT (t) = λe\n-λt,\nt ≥0,\nE[T1] = 1/λ\n-\nTime Yk to kth arrival: Erlang(k):\nλkyk-1e-λy\nfY (y) =\n,\ny\nk\n(k\n1)!\n≥0\n-\nPoisson fishing\nMerging Poisson Processes (again)\n-\nAssume: Poisson, λ = 0.6/hour.\n-\nMerging of independent Poisson processes\nis Poisson\n-\nFish for two hours.\nRed bulb flashes\n-\nif no catch, continue until first catch.\n(Poisson)\nAll flashes\na)\nP(fish for more than two hours)=\n\"1\n(Poisson)\n\"2\nb)\nP(fish for more than two and less than\nfive hours)=\nGreen bulb flashes\n(Poisson)\nc)\nP(catch at least two fish)=\n-\nWhat is the probability that the next\narrival comes from the first process?\nd)\nE[number of fish]=\ne)\nE[future fishing time | fished for four hours]=\nf)\nE[total fishing time]=\n\nLight bulb example\nSplitting of Poisson processes\n-\nEach light bulb has independent,\n-\nAssume that email traffic through a server\nexponential(λ) lifetime\nis a Poisson process.\nDestinations of different messages are\n-\nInstall three light bulbs.\nindependent.\nFind expected time until last light bulb\ndies out.\nUSA\nEmail Traffic\nleaving MIT\np !\nMIT\nServer\n!\n(1 - p)!\nForeign\n-\nEach output stream is Poisson.\nRandom incidence for Poisson\nRandom incidence in\n\"renewal processes\"\n-\nPoisson process that has been running\nforever\n-\nSeries of successive arrivals\n-\ni.i.d. interarrival times\n-\nShow up at some \"random time\"\n(but not necessarily exponential)\n(really means \"arbitrary time\")\n-\nExample:\nBus interarrival times are equally likely to\nx\nx\nx\nx\nx\nTime\nbe 5 or 10 minutes\nChosen\ntime instant\n-\nIf you arrive at a \"random time\":\n-\nwhat is the probability that you selected\na 5 minute interarrival interval?\n-\nwhat is the expected time\n-\nWhat is the distribution of the length of\nto next arrival?\nthe chosen interarrival interval?\n\nLECTURE 16\nCheckout counter model\n-\nDiscrete time n = 0, 1, . . .\nMarkov Processes - I\nCustomer arrivals: Bernoulli( )\n-\np\nReadings: Sections 7.1-7.2\n-\n-\ngeometric interarrival times\nCustomer service times: geometric(q)\nLecture outline\n-\n-\nCheckout counter example\n-\n\"State\" Xn:\nnumber of customers at\ntime n\n-\nMarkov process definition\n-\nn-step transition probabilities\n-\nClassification of states\n3 ...\nFinite state Markov chains\nn-step transition probabilities\nState occupancy probabilities,\n-\nXn: state after n transitions\n-\ngiven initial state i:\n-\nbelongs to a finite set, e.g., {1, . . . , m}\nr (n) = P(X\n= j\nX\n= i)\n-\nX0 is either given or random\nij\nn\n|\n-\nMarkov property/assumption:\nTime 0\nTime n-1\nTime n\n(given current state, the past does not\nmatter)\nr (n-1)\np\ni1\n1j\n...\npij\n= P(Xn+1 = j | Xn = i)\ni\nk\n= P(Xn+1 = j | X\n(n-1)\nn = i, X\nr\nn-1, . . . , X0)\nik\npkj\nj\n...\nr\n(n-1)\np\nim\nmj\n-\nModel specification:\nm\n-\nidentify the possible states\n-\nKey recursion:\nthe possible transitions\nm\n-\nidentify\nrij(n) =\n!\nrik(n\n-\nidentify the transition probabilities\nk=1\n-1)pkj\n-\nWith random initial state:\nm\nP(Xn = j) =\ni\n!\nP(X0 = i)rij(n)\n=1\n\nExample\nGeneric convergence questions:\n-\nDoes rij(n) converge to something?\n0.5\n0.8\n0.5\n0.5\n0.5\n0.2\nn odd: r22(n)=\nn even: r22(n)=\nn = 0\nn = 1\nn = 2\nn = 100\nn = 101\nDoes the limit depend on initial\nr11(n)\n-\nstate?\nr12(n)\n0.4\nr21(n)\n0.3\n0.3\nr22(n)\nr\n(n)=\nr\n(n)=\nr\n(n)=\nRecurrent and transient states\n-\nState i is recurrent if:\nstarting from i,\nand from wherever you can go,\nthere is a way of returning to i\n-\nIf not recurrent, called transient\n-\ni transient:\nP(Xn = i) →0,\ni visited finite number of times\n-\nRecurrent class:\ncollection of recurrent states that\n\"communicate\" with each other\nand with no other state\n\nLECTURE 17\nReview\nMarkov Processes - II\n-\nDiscrete state, discrete time, time-homogeneous\n-\nReadings: Section 7.3\n-\nTransition probabilities pij\n-\nMarkov property\nLecture outline\n-\nrij(n) = P(Xn = j | X0 = i)\n-\nReview\n-\nKey recursion:\n-\nSteady-State behavior\nrij(n) =\n-\nSteady-state convergence theorem\n!\nrik(n\nk\n-1)pkj\n-\nBalance equations\n-\nBirth-death processes\nWarmup\nPeriodic states\n-\nThe states in a recurrent class are\nperiodic if they can be grouped into\n1 groups so that\nd >\nall transitions from\none group lead to the next group\nP(X1 = 2, X2 = 6, X3 = 7 | X0 = 1) =\nP(X4 = 7 | X0 = 2) =\nRecurrent and transient states\n-\nState i is recurrent if:\nstarting from i,\nand from wherever you can go,\nthere is a way of returning to i\n-\nIf not recurrent, called transient\n-\nRecurrent class:\ncollection of recurrent states that\n\"communicate\" to each other\nand to no other state\n\nSteady-State Probabilities\nVisit frequency interpretation\n-\nDo the rij(n) converge to some πj?\n(independent of the initial state i)\nπj =\nπkpkj\n-\nYes, if:\n!\nk\n-\nrecurrent states are all in a single class,\nand\n-\n(Long run) frequency of being in j:\nπj\n-\nsingle recurrent class is not periodic\n-\nFrequency of transitions k →j:\nπkpkj\n-\nAssuming \"yes,\" start from key recursion\nFrequency of transitions into j:\nrij(n) =\nrik(n -1)pkj\n-\nk\n!\n!\nπkpkj\nk\nπjpjj\n-\ntake the limit as n →inf\nπ1p1j\nπj =\n!\nπkpkj,\nfor all j\nk\nπ2p2j\nj\n-\nAdditional equation:\n. . .\n. . .\n!\nπj = 1\nj\nπmpmj\nm\nExample\nBirth-death processes\n1- p - q\n1- p\n1- q\nm\n0.5\n0.8\np\np\n3 ...\nm\n0.5\nq1\nq\nq\nm\npi\ni+1\n0.2\ni\nπipi = πi+1qi+1\nqi+1\n-\nSpecial case: pi = p and qi = q for all i\nρ = p/q =load factor\np\nπi+1 = πi\n= πiρ\nq\nπi =\ni\nπ0ρ ,\ni = 0, 1, . . . , m\n-\nAssume p < q and m ≈inf\nπ0 = 1 -ρ\nρ\nE[Xn] =\n(in steady-state)\n1 -ρ\n\nLECTURE 18\nReview\nAssume a single class of recurrent\nMarkov Processes - III\n-\nstates,\naperiodic;\nplus transient states. Then,\nlim r (n\nReadings: Section 7.4\nij\n) = πj\nn→inf\nwhere πj does not depend on the initial\nconditions:\nLecture outline\nlim P(Xn = j | X0 = i) = πj\nn→inf\n-\nReview of steady-state behavior\nπ1, . . . , πm can be found as the unique\n-\nProbability of blocked phone calls\n-\nsolution to the balance equations\n-\nCalculating absorption probabilities\nπj =\n-\nCalculating expected time to absorption\n!\nπkpkj,\nj = 1, . . . , m,\nk\ntogether with\n!\nπj = 1\nj\nExample\nThe phone company problem\n0.5\n0.8\n-\nCalls originate as a Poisson process,\nrate λ\n0.5\n-\nEach call duration is exponentially\ndistributed (parameter μ)\n0.2\n-\nB lines available\nπ1 = 2/7, π2 = 5/7\n-\nDiscrete time intervals\nof (small) length δ\n-\nAssume process starts at state 1.\n\"#\n-\nP(X1 = 1, and X100 = 1)=\ni!1\ni\nB-1\nB\niμ#\n-\nP(X100 = 1 and X101 = 2)\n-\nBalance equations:\nλπi-1 = iμπi\nλi\nB\nλi\n-\nπi = π0\nπ0 = 1/\nμii!\ni\n!\nμii!\n=0\n\nCalculating absorption probabilities\nExpected time to absorption\n-\nWhat is the probability ai that:\nprocess eventually settles in state 4,\ngiven that the initial state is i?\n0.5\n0.5\n0.4\n0.6\n0.2\n0.2\n0.3\n0.8\n0.5\n0.4\n-\nFind expected number of transitions μ\n0.6\ni,\n0.2\nuntil reaching the absorbing state,\ngiven that the initial state is i?\n0.8\nFor i = 4, ai =\nFor i = 5, a =\nμi = 0 for i =\ni\n!\nFor all other i: μ\n=\nfor all other\ni = 1 +\npijμj\nai\npijaj,\ni\nj\n!\nj\n-\nunique solution\n-\nunique solution\nMean first passage and recurrence\ntimes\n-\nChain with one recurrent class;\nfix s recurrent\n-\nMean first passage time from i to s:\nti = E[min{n ≥0 such that Xn = s} | X0 = i]\n-\nt1, t2, . . . , tm are the unique solution to\nts\n=\n0,\nti\n=\n1 +\n!\npij tj,\nfor all i = s\nj\n-\nMean recurrence time of s:\nt∗s = E[min{n ≥1 such that Xn = s} | X0 = s]\n-\nt∗s = 1 + \"\nj psj tj\n\nLECTURE 19\nChebyshev's inequality\nLimit theorems - I\n-\nRandom variable X\n-\nReadings: Sections 5.1-5.3;\n(with finite mean μ and variance\nσ )\nstart Section 5.4\nσ\n=\n!\n(\n-\n)2\nx\nμ\nfX(x) dx\nc\n-\nX1, . . . , Xn i.i.d.\n≥\n! -\n(x -μ) fX(x) dx +\n! inf\n(x -μ) fX(x) dx\nc\nX1 + · · · +\n-inf\nXn\nMn =\nn\nWhat happens as\n?\n≥\nc\n· P(|X -μ| ≥c)\nn →inf\n-\nWhy bother?\nσ\nP(|X -μ| ≥c) ≤\nc2\n-\nA tool: Chebyshev's inequality\n-\nConvergence \"in probability\"\nP( X\nσ\nConvergence of\n|\n-μ| ≥k ) ≤k2\n-\nMn\n(weak law of large numbers)\nDeterministic limits\nConvergence \"in probability\"\n-\nSequence an\n-\nSequence of random variables Yn\nNumber a\n-\nconverges in probability to a number a:\n\"(almost all) of the PMF/PDF of Yn ,\n-\nan converges to a\neventually gets concentrated\n(arbitrarily) close to a\"\nlim an = a\nn→inf\n\"an eventually gets and stays\n(arbitrarily) close to a\"\n-\nFor every ε > 0,\nlim P(|Yn -a| ≥ε) = 0\nn→inf\n-\nFor every ε > 0,\nthere exists n0,\nsuch that for every n ≥n0,\nwe have |an -a| ≤ε.\n1 - 1 /n\npmf of Yn\n1 /n\nn\nDoes Yn converge?\n\nConvergence of the sample mean\nThe pollster's problem\n(Weak law of large numbers)\n-\nf: fraction of population that \". . . \"\n-\nX1, X2, . . . i.i.d.\n-\nith (randomly selected) person polled:\nfinite mean μ and variance σ\nX1 + · · · +\nif yes,\nX\n,\nn\ni =\nM\n=\nX\nn\n\n0,\nif no.\nn\n-\nMn = (X\n\n1 + · · · + Xn)/n\n- E[Mn] =\nfraction of \"yes\" in our sample\n-\nGoal: 95% confidence of ≤1% error\n-\nVar(Mn) =\nP(|Mn -f| ≥.01) ≤.05\nUse Chebyshev's\nVar(\n-\ninequality:\nM\nσ\n(|\nn)\nσ\nP Mn -μ| ≥ε) ≤\n=\nP(|Mn -f| ≥\nM\n.01)\nε\nnε\n≤\nn\n(0.01)2\nσ\n-\nMn converges in probability to μ\n=\nx\nn(0.01)2 ≤4n(0.01)2\n-\nIf n = 50, 000,\nthen P(|Mn -f| ≥.01) ≤.05\n(conservative)\nDifferent scalings of Mn\nThe central limit theorem\n-\nX1, . . . , Xn i.i.d.\n\"Standa\nn = X1 +\n-\nrdized\" S\n· · · + Xn:\nfinite variance σ\nSn\nE[Sn]\nSn\nZn =\n-\nnE[X]\n=\n-\nσ\n√n σ\n-\nLook at three variants of their sum:\nSn\n-\nzero mean\n-\nSn =\n+\n+ Xn\nvariance\nX1\n· · ·\nnσ\n-\nunit variance\nS\n-\nn\nLet\nbe a standard normal r.v.\nMn =\nvariance\nσ /n\nn\n-\nZ\n(zero mean, unit variance)\nconverges \"in probability\" to E[X] (WLLN)\nSn\nevery c:\n-\n√\nconstant variance\nσ\n-\nTheorem: For\nn\nP(Zn ≤c) →P(Z ≤c)\n-\nAsymptotic shape?\n- P(Z ≤c) is the standard normal CDF,\nΦ(c), available from the normal tables\n\nLECTURE 20\nUsefulness\nTHE CENTRAL LIMIT THEOREM\n-\nuniversal; only means, variances matter\n-\naccurate computational shortcut\n-\nReadings: Section 5.4\n-\njustification of normal models\n-\n, Xn i.i.d., finite variance\nX1, . . .\nσ\n-\n\"Standardized\" Sn = X1 + · · · + Xn:\nWhat exactly does it say?\nSn\nE[Sn]\nSn\nnE[X]\nZn =\n-\n=\n-√\n-\nCDF of Zn converges to normal CDF\nσSn\nnσ\n-\nnot a statement about convergence of\n- E[Zn] = 0,\nvar(Z ) = 1\nPDFs or PMFs\nn\n-\nLet Z be a standard normal r.v.\n(zero mean, unit variance)\nNormal approximation\n-\n-\nTreat Zn as if normal\nTheorem: For every c:\n-\nalso treat Sn as if normal\nP(Zn ≤c) →P(Z ≤c)\n- P(Z ≤c) is the standard normal CDF,\nΦ(c), available from the normal tables\nCan we use it when n is \"moderate\"?\n-\nYes, but no nice theorems to this effect\n-\nSymmetry helps a lot\n0.14\n0.1\nn =4\n0.12\n0.08\n0.1\n0.06\n0.08\n0.06\n0.04\nThe pollster's problem using the CLT\n0.04\n0.02\n0.02\nf: fraction of population that \" . . .\n-\nith (randomly selected) person polled:\n0.25\n0.035\nn =32\n-\n0.03\n0.2\n0.025\n\n,\nif yes,\nX =\n0.15\ni\n0.02\n0,\nif no.\n0.015\n0.1\n0.01\n0.05\n\n0.005\n-\nMn = (X1 + · · · + Xn)/n\n-\nSuppose we want:\nP( M\nf\n.01) ≤.05\n0.12\n0.0\n|\nn\n-\n| ≥\nn = 16\n0.07\nn = 8\n0.1\n0.06\n-\nEvent of interest:\n|Mn -f| ≥.01\n0.08\n0.05\n0.06\n0.04\n0.03\nX1 + · · · + Xn\n0.04\n-nf\n0.02\n0.02\nn\n≥.\n0.01\n\n0.06\n0.9\nn = 32\n\nX1 +\nXn\n√\n· · · +\n-nf\n\n.01\nn\n0.05\n0.8\n0.7\n√nσ\n≥\nσ\n0.04\n0.6\n0.5\n0.03\n\n0.4\n\n0.02\n0.3\n0.2\n0.01\n0.1\nP(|Mn -f| ≥.01)\n≈P(|Z| ≥.01√n/σ)\n≤P(|Z| ≥.02√n)\n\nApply to binomial\nThe 1/2 correction for binomial\napproximation\n-\nFix p, where 0 < p < 1\n-\nP(Sn ≤21) = P(Sn < 22),\n-\nXi: Bernoulli(p)\nbecause Sn is integer\n-\nSn = X1 + · · · + Xn: Binomial(n, p)\n-\nCompromise: consider P(Sn ≤21.5)\n-\nmean np, variance np(1 -p)\nCDF of Sn -np\n-\nnp(1\np)\n-→standard normal\n-\nExample\n19 20 21\n-\nn = 36, p = 0.5; find P(Sn ≤21)\n-\nExact answer:\n\n21 36136\n= 0.8785\nk\nk=0\nDe Moivre-Laplace CLT (for binomial)\nPoisson vs. normal approximations of\nthe binomial\n-\nWhen the 1/2 correction is used, CLT\ncan also approximate the binomial p.m.f.\n(not just the binomial CDF)\n-\nPoisson arrivals during unit interval equals:\nsum of n (independent) Poisson arrivals\nP(Sn = 19) = P(18.5 ≤Sn ≤19.5)\nduring n intervals of length 1/n\n-\nLet n →inf, apply CLT (??)\n18.5 ≤Sn ≤19.5\n⇐⇒\n-\nPoisson=normal (????)\n18.5 -18\nSn -18\n19.5 -18\n≤\n≤\n⇐⇒\n.17 ≤Z\n-\nBinomial(\n)\nn ≤\nn, p\n0.5\n-\np fixed, n →inf: normal\nP(Sn = 19)\n≈\nP(0.17 ≤Z ≤0.5)\n-\nnp fixed, n →inf, p →0: Poisson\n= P(Z ≤0.5) -P(Z ≤0.17)\n-\np = 1/100, n = 100: Poisson\n=\n0.6915 -0.5675\n-\np = 1/10, n = 500: normal\n=\n0.124\n-\nExact answer:\n= 0.\n\nLECTURE 21\nTypes of Inference models/approaches\nModel building versus inferring unkno\n-\nReadings: Sections 8.1-8.2\n-\nwn\nvariables. E.g., assume X = aS + W\n-\nModel building:\n\"It is the mark of truly educated people\nknow \"signal\" S, observe X, infer a\nto be deeply moved by statistics.\"\n-\nEstimation in the presence of noise:\n(Oscar Wilde)\nknow a, observe X, estimate S.\nModel\n-\nHypothesis testing: unknown takes one of\nReality\n(e.g., customer arrivals)\n(e.g., Poisson)\nfew possible values; aim at small\nprobability of incorrect decision\nData\n-\nEstimation: aim at a small estimation error\n-\nDesign & interpretation of experiments\n-\npolling, medical/pharmaceutical trials. . .\n-\nClassical statistics:\nN\n-\nNetflix competition\n- Finance\nθ\nX\nΘˆ\npX(x; θ)\nEstimator\nobjects\n?\n?\n?\nθ: unknown parameter (not a r.v.)\n1 3\n1 ?\ns\nr\nE.g., θ = mass of electron\no\n? 5\n?\ns\nn\n*\ne\ns\n5 1\n-\nBayesian: Use priors & Bayes rule\n3 ?\n2 ?\n2 ?\nN\n1 5\n5 ?\nΘ\nX\nΘˆ\np\nx\nEstimator\nSignal processing\nX|Θ(\n| θ)\n-\npΘ(θ)\n-\nTracking, detection, speaker identification,. . .\nBayesian inference: Use Bayes rule\nEstimation with discrete data\n-\nHypothesis testing\n-\ndiscrete data\nfΘ(θ) pX Θ(x\nθ)\nfΘ X(θ\n|\npΘ(θ) pX\n|\npΘ X(θ\nx) =\n|Θ(x\n| x) =\n|\n| θ)\npX(x)\n|\n|\npX(x)\npX(x) =\n!\nfΘ(θ)pX Θ(x | θ) dθ\n|\n-\ncontinuous data\npΘ(θ) fX Θ(x\nθ)\np\nExample:\nΘ X(θ\n|\n| x) =\n|\n|\nfX(x)\n-\n-\nCoin with unknown parameter θ\n-\nObserve X heads in n tosses\n-\nEstimation; continuous data\n-\nWhat is the Bayesian approach?\nfΘ(θ) fX Θ(x | θ)\n-\nWant to find fΘ|X(θ | x)\nfΘ|X(θ | x) =\n|\nfX(x)\n-\nAssume a prior on Θ (e.g., uniform)\nZt\n=\nΘ0 + tΘ1 + 2\nt Θ2\nXt\n=\nZt + Wt,\nt = 1, 2, . . . , n\nBayes rule gives:\nfΘ0,Θ1,Θ2|X1,...,X (θ\nn\n0, θ1, θ2 | x1, . . . , xn)\nGraph of S&P 500 index removed\ndue to copyright restrictions.\n\nOutput of Bayesian Inference\nLeast Mean Squares Estimation\n-\nPosterior distribution:\n-\nEstimation in the absence of information\n-\npmf pΘ|X(· | x) or pdf fΘ\n(\nx)\n|X · |\nfΘ(θ)\n1/6\n-\nIf interested in a single answer:\nθ\n-\nMaximum a posteriori probability (MAP):\nfind estimate c, to:\n*\npΘ X(θ∗| x) = maxθ pΘ|X(θ | x)\n-\n|\nminimizes probability of error;\nminimize E\n\"\n(Θ -c)\noften used in hypothesis testing\n#\nOptimal estimate: c = E[Θ]\n*\nfΘ\nx\n|X(θ∗| x) = maxθ fΘ|X(θ\n-\n|\n)\n-\nOptimal mean squared error:\n-\nConditional expectation:\n(Θ\n[Θ])2\nE\nE\n= Var(Θ)\nE[Θ | X = y] =\n!\nθfΘ|X(θ\ndθ\n\"\n-\n| x)\n#\n-\nSingle answers can be misleading!\nLMS Estimation of Θ based on X\nLMS Estimation w. several measurements\n-\nTwo r.v.'s Θ, X\n-\nUnknown r.v. Θ\n-\nwe observe that X = x\n-\nObserve values of r.v.'s X1, . . . , Xn\n-\nnew universe: condition on X = x\n-\nBest estimator: E[Θ\n| X1, . . . , Xn]\n- E\n-\nCan be hard to\nc\n\"\n(Θ -c)\n| X = x\n#\nis minimized by\ncompute/implement\n=\n\"\n-\ninvolves multi-dimensional integrals, etc.\n-\nE (Θ -E[Θ | X = x])2 | X = x\n#\n≤E[(Θ -g(x))2 | X = x]\n*E\n\"\n(Θ -E[Θ |\n-g(\nX])2 | X\n#\n≤E\n\"\n(Θ\nX))\n| X\n#\n*E\n\"\n(Θ -E[Θ |\n])2#\n≤\nX\nE\n\"\n(Θ -g(X))\n#\nE[Θ | X] minimizes E\n\"\n(Θ -(X))2\ng\nover all estimators g(·)\n#\n\nLECTURE 22\nfΘ(θ)\n-\nReadings: pp. 225-226; Sections 8.3-8.4\n1/6\nθ\nTopics\n-\nfX Θ(x | θ)\n(Bayesian) Least means squares (LMS)\n|\nestimation\n1/2\n-\n(Bayesian) Linear LMS estimation\n·\nθ -1\nθ + 1\nΘ\nX\nΘˆ = g(\nEstimator\nX)\nfX Θ(x\nθ)\n|\n|\ng( )\nfΘ(θ)\n·\nθ\nx\n-\nMAP estimate: θˆ\nmaximizes fΘ|X(\nMAP\nθ | x)\n-\nLMS estimation:\n-\nΘˆ = E[Θ | X] minimizes\n(Θ -g(\n))2\nE\nX\nover all estimators g(·)\n\n-\nfor any x, θˆ = E[Θ | X = x]\nminimizes\n(Θ -ˆ)2\nE\nθ\nover a\n\n| X = x\nx\nll estimates θˆ\n\ny\nConditional mean squared error\nSome properties of LMS estimation\n- E[(Θ -E[Θ |\n])2\nX\n| X = x]\n-\nEstimator: Θˆ = E[Θ | X]\n-\nsame as Var(Θ | X = x): variance of the\n-\nEstimation error: Θ = Θˆ -Θ\nconditional distribution of Θ\nθ\nx\n-\nE[Θ ] = 0\nE[Θ\nX = x] = 0\n|\n-\nE[Θ h(X)] = 0, for any function h\n-\ncov(Θ , Θˆ ) = 0\n-\nSince Θ = Θˆ\nx\n-Θ :\ny\nvar(Θ) = var(Θˆ ) + var(Θ )\nx\n|\n(\n|\nVar Θ\nX = x):\nonal distr\n|\nibution of\nx\ny\n\nLinear LMS\nLinear LMS properties\n-\nConsider estimators of Θ,\nCov(X, Θ)\nΘˆ\nˆ\nL = E[Θ] +\n(\nof the form Θ = aX +\nX\nb\nvar(X)\n-E[X])\n-\nMinimize E\n\n(Θ -\n-b)2\naX\n\n[(\n2) 2\nE\nΘˆ L -Θ) ] = (1 -ρ\nσΘ\n-\nBest choice of a,b; best linear estimator:\nLinear LMS with multiple data\nCov(X, Θ)\nΘˆ L = E[Θ] +\n(X -E[X])\nvar(X)\n-\nConsider estimators of the form:\nθ\nΘˆ = a1X1 + · · · + anXn + b\nx\n-\nFind best choices of a1, . . . , an, b\n-\nMinimize:\nE[(\n+ · · · +\n+\n-Θ)2\na1X1\nanXn\nb\n]\n-\nSet derivatives to zero\nlinear system in b and the ai\n-\nOnly means, variances, covariances matter\nx\ny\nThe cleanest linear LMS example\nBig picture\n-\nStandard examples:\nXi = Θ + Wi,\nΘ, W1, . . . , Wn independent\nΘ ∼\n∼0\nμ, σ0\nWi\n, σi\n-\nXi uniform on [0, θ];\nμ/σ\n+\nΘˆ\n=\ni\nL\nn\n\nn\nuniform prior on θ\nXi/σi\n=1\n\n-\nXi Bernoulli(p);\n/σi\nuniform (or Beta) prior on p\ni=0\n(weighted average of μ, X1, . . . , Xn)\n-\nXi normal with mean θ, known variance\nσ ;\nnormal prior on θ;\n-\nIf all normal, Θˆ L = E[Θ | X1, . . . , Xn]\nXi = Θ + Wi\n-\nEstimation methods:\nChoosing Xi in linear LMS\n-\nMAP\n-\nE[Θ | X] is the same as E[Θ |\nX ]\n-\nMSE\n-\nLinear LMS is different:\n-\nLinear MSE\n*\nΘˆ =\n+\nver us Θˆ\naX\nb\ns\n= aX\n+ b\n*\nAlso consider Θˆ =\na1X +\na2X\n+ a3X\n+ b\n\nLECTURE 23\nClassical statistics\n-\nReadings: Section 9.1\nN\n(not responsible for t-based confidence\nθ\nX\nΘˆ\npX(x; θ)\nEstimator\nintervals, in pp. 471-473)\nalso for vectors X and θ:\n-\nOutline\n-\npX ,...,X (x1, . . . , xn;\n, . . . , θ\nn\nθ1\nm)\n-\nClassical statistics\nThese are NOT conditional probabilities;\n-\nMaximum likelihood (ML) estimation\n-\nθ is NOT random\n-\nEstimating a sample mean\n-\nmathematically: many models,\n-\nConfidence intervals (CIs)\none for each possible value of θ\n-\nCIs using an estimated variance\n-\nProblem types:\n-\nHypothesis testing:\nH0 : θ = 1/2 versus H1 : θ = 3/4\n-\nComposite hypotheses:\nH0 : θ = 1/2 versus H1 : θ = 1/2\n-\nEstimation: design an estimator Θˆ ,\nto keep estimation error Θˆ -θ small\nMaximum Likelihood Estimation\nDesirable properties of estimators\n(should hold FOR ALL θ !!!)\n-\nModel, with unknown parameter(s):\nX ∼pX(x; θ)\nUnbiased: E[Θˆ n] = θ\n-\nPick θ that \"makes data most likely\"\n-\n-\nexponential example, with n = 1:\nθˆML = arg max pX(x; θ)\nE[1/X1] = inf= θ\nθ\n(biased)\n-\nCompare to Bayesian MAP estimation:\nConsistent: Θˆ n\nθ (in probability)\nθˆMAP = arg max pΘ\nθ\n|X(θ | x)\n-\n→\n-\nexponential example:\np\n(x|θ)p\n(θ)\n(X1 +\n+ Xn)/n\nE[X] = 1/θ\nX Θ\nΘ\nθˆMAP = arg max\n|\n· · ·\n→\nθ\np\n(x)\n-\ncan use this to show that:\nX\nΘˆ n = n/(X1 +\n+ Xn)\n1/E[X] = θ\n-\nExample: X1, . . . , Xn: i.i.d., exponential(θ)\n· · ·\n→\n\"Small\" mean\nd error (MSE)\nn\nmax\n\nsquare\nθx\n-\nθe-\ni\nθ\n[(Θˆ -θ)2\nE\n]\n=\nvar(Θˆ -θ) + (E[Θˆ -θ])2\ni=1\n=\nvar(Θˆ )\nas)2\nθ\n\n+ (bi\nn\nmax\nn log θ -θ\ni\n\nxi\n=1\n\nn\nn\nθˆ\nΘˆ\nML =\nn =\nx1 + · · · + xn\nX1 + · · · + Xn\n\nEstimate a mean\nConfidence intervals (CIs)\n-\nAn estimate Θˆ\nmay not be informative\nX1, . . . , Xn: i.i.d., mean θ, variance σ\n-\nn\nenough\nXi = θ + Wi\n-\nAn 1 -α confidence interval\nW\nnc\ni: i.i.d., mean, 0, varia\ne σ\n+\nis a (random) interval [Θˆ -\nn , Θˆ n ],\nX\nXn\n= s\nean = Mn =\ns.t.\n+\nΘˆ\n1 +\nn\nample m\n· · · +\nP(Θˆ\nΘˆ\nn-\nn\n≤θ ≤\nn ) ≥1 -α,\n∀θ\n-\noften α = 0.05, or 0.25, or 0.01\nProperties:\n-\ninterpretation is subtle\n- E[Θˆ n] = θ\n(unbiased)\n-\nCI in estimation of the mean\nΘˆ n = (X1 + · · · + X\n-\nWLLN: Θˆ\nn)/n\nn →θ\n(consistency)\n-\nnormal tables: Φ(1.96) = 1\n-\nMSE:\n-0.05/2\nσ /n\nP\n|Θˆ n -θ|\nn\n≤1.\n.\nσ √\n\n≈0 95\n(CLT)\n/\n-\nSample mean often turns out to also be\n1.96 σ\n1.96 σ\nP Θˆ\nθ\nΘˆ +\n0.95\nthe ML estimate.\nn-√\nn\nn\n≤\n≤\n√n\n≈\nE.g., if\nXi ∼N(θ, σ ), i.i.d.\n\nMore generally: let z be s.t. Φ(z) = 1-α/2\n\nzσ\nzσ\nP Θˆ n -√\nθ\nn ≤\n≤Θˆ n + √n\n\n≈1 -α\nThe case of unknown σ\n-\nOption 1: use upper bound on σ\n-\nif Xi Bernoulli: σ ≤1/2\n-\nOption 2: use ad hoc estimate of σ\n-\nif Xi Bernoulli(θ): σˆ =\n\nΘˆ (1 -Θˆ )\n-\nOption 3: Use generic estimate\nof the variance\n-\nStart from\nσ\n= E[(Xi -θ)2]\nσˆ2\nn\nn =\n\n(Xi\nn i=1\n-\nθ)\n→\nσ\n(but do not know θ)\nSˆ2\nn\nn =\n\n(X -ˆ\ni\nΘn)2\nn -1 i=1\n→\nσ\n(unbiased: E[Sˆ2n] =\nσ )\n\nLECTURE 24\nReview\nMaximum likeliho d estimation\n-\nReference: Section 9.3\n-\no\n-\nHave model with unknown parameters:\n-\nCourse Evaluations (until 12/16)\nX ∼pX(x; θ)\nhttp://web.mit.edu/subjectevaluation\n-\nPick θ that \"makes data most likely\"\nmax pX(x; θ)\nθ\n-\nCompare to Bayesian MAP estimation:\nOutline\np\n-\nX Θ(x θ)pΘ(θ)\nReview\nmax pΘ X(θ\nmax\n|\nθ\n|\n| x) or\n|\nθ\npY (y)\n-\nMaximum likelihood estimation\nSample mean estimate of θ = E[X]\n-\nConfidence intervals\n-\nΘˆ n = (X1 +\nLinear regression\n· · · + Xn)/n\n-\n-\n1 -α confidence interval\n-\nBinary hypothesis testing\n+\nP(Θˆ -\nΘˆ\nn ≤θ ≤\nn ) ≥1 -α,\n∀θ\n-\nTypes of error\n-\nconfidence interval for sample mean\n-\nLikelihood ratio test (LRT)\n-\nlet z be s.t. Φ(z) = 1 -α/2\n\nzσ\nzσ\nP Θˆ n -√\nθ ≤Θˆ n +\nn ≤\n√n\n\n≈1 -α\nRegression\nLinear regression\ny\nResidual\n-\nModel y\nθ0 + θ x\n×\n≈\nˆ\nˆ\n(xi, yi)\nx yi -θ0 -θ1xi\n×\nn\n×\nmin\n( i\nθ0\nθ1xi)2\ny\nθ0,θ1 i=1\n-\n-\nx y = θˆ0 + θˆ1x\n\n×\n×\n-\nSolution (set derivatives to zero):\n×\nx1 + · · · + xn\ny1 +\n+ yn\nx =\n,\ny =\n· · ·\n= 0\ny x\nn\nn\n-\nData: (x1, y1), (x2, y2), . . . , (xn, yn)\nn\n(\n)(\nθˆ1 =\ni=1 xi\nn\n-x\nyi\n(x\nx)2\n-y)\n-\nModel: y\nθ0 + θ1x\n\n≈\n\ni=1\ni -\n\nn\nθˆ0 =\nˆ\nmin\n(\nθ\n1 i)\n( )\ny\nx\nyi\nθ0\nθ x\nθ0,θ1 =1\n-\n-\ni\n-\n∗\n-\nInterpretation of the form of the solution\n-\nOne interpretation:\n-\nAssume a model Y = θ0 + θ1X + W\nYi = θ0+\nθ1xi+Wi,\nWi ∼N(0, σ ), i.i.d.\nW independent of X, with zero mean\n-\nLikelihood function fX,Y θ(x, y; θ) is:\n|\n-\nCheck that\n\nn\ncov(\nY\nE[Y ])\nc · exp\n-\nX,\nE\n(y -θ0 -\nY )\n(X\nE[X])(\ni\nθ1xi)2\nθ1 =\n=\n-\n-\n2σ\ni=1\n\nvar(X)\n\nE (X -E[X])\n\n-\nTake logs, same as (*)\n-\nSolution formula for ˆ\n\nθ1 uses natural\n-\nLeast sq. ↔pretend Wi i.i.d. normal\nestimates of the variance and covariance\n\nThe world of linear regression\nThe world of regression (ctd.)\n-\nMultiple linear regression:\n-\nIn practice, one also reports\n-\ndata: (xi, x, x, yi), i = 1, . . . , n\n-\nConfidence intervals for the θ\ni\ni\ni\n-\nmodel: y ≈θ0 + θx + θx+ θx\n-\n\"Standard error\" (estimate of σ)\n-\nformulation:\n-\nR , a measure of \"explanatory power\"\nn\nmin\n\n(\nyi\nθ0\nθxi\nθxi\n\nθx\ni )\nθ,θ,θi=1\n-\n-\n-\n-\n-\nSome common concerns\n-\nHeteroskedasticity\n-\nChoosing the right variables\n-\nMulticollinearity\n-\nmodel y ≈θ0 + θ1h(x)\ne.g.,\ny ≈θ0 + θ1x\n-\nSometimes misused to conclude causal\nrelations\n-\nwork with data points (yi, h(x))\n-\netc.\n-\nformulation:\nn\nmin\n\n(yi -\nθ0 -θ1h1(xi))\nθ\ni=1\nBinary hypothesis testing\nLikelihood ratio test (LRT)\n-\nBinary θ; new terminology:\n-\nBayesian case (MAP rule): choose H1 if:\nP(H1 | X = x) > P(H0 | X = x)\n-\nnull hypothesis H0:\nor\nX ∼pX(x; H0)\n[or fX(x; H0)]\nP(X = x | H1)P(H1)\nP(X = x\nH0)P(H0)\n-\nalternative hypothesis H1:\n>\n|\nP(X = x)\nP(X = x)\nX ∼pX(x; H1)\n[or fX(x; H1)]\nor\nP(X = x | H1)\nP(H0)\n>\n-\nPartition the space of possible data vectors\nP(X = x | H0)\nP(H1)\nRejection region R:\n(likelihood ratio test)\nreject H0 iffdata ∈R\nNonbayesian version: choose H1 if\n-\nTypes of errors:\n-\nP(X = x; H1) > ξ\n(discrete case)\n-\nType I (false rejection, false alarm):\nP(X = x; H0)\nH0 true, but rejected\nfX(x; H1) > ξ\n(continuous case)\nα(R) = P(X ∈R ; H0)\nfX(x; H0)\n-\nType II (false acceptance,\nthreshold ξ trades offthe two types of error\nmissed detection):\n-\n-\nchoose ξ so that P(reject H ; H ) = α\nH0 false, but accepted\n(e.g., α = 0.05)\nβ(R) = P(X ∈R ; H1)\n\nLECTURE 25\nSimple binary hypothesis testing\nOutline\n-\nnull hypothesis H0:\n-\nReference: Section 9.4\nX ∼pX(x; H0)\n[or fX(x; H0)]\n-\nalternative hypothesis H :\n-\nCourse Evaluations (until 12/16)\nX ∼pX(x; H1)\n[or fX(x; H1)]\nhttp://web.mit.edu/subjectevaluation\n-\nChoose a rejection region R;\nreject H0 iffdata\nR\n-\nReview of simple binary hypothesis tests\n∈\nLikelihood ratio test: reject H\nif\n-\nexamples\n-\npX(x; H1)\nfX(x; H1)\n-\n> ξ\nor\n> ξ\nTesting composite hypotheses\npX(x; H0)\nfX(x; H0)\n-\nis my coin fair?\n-\nfix false rejection probability α\n-\nis my die fair?\n(e.g., α = 0.05)\n-\ngoodness of fit tests\n-\nchoose ξ so that P(reject H0; H0) = α\nExample (test on normal mean)\nExample (test on normal variance)\n-\nn data points, i.i.d.\n-\nn data points, i.i.d.\nH0:\nXi ∼N(0, 1)\nH0:\nXi ∼N(0, 1)\nH1:\nXi ∼N(1, 1)\nH1:\nXi ∼N(0, 4)\n-\nLikelihood ratio test; rejection region:\n-\nLikelihood ratio test; rejection region:\n(1\nn\n(\n/\n√\n2π) exp{-\ni Xi -1) /2}\n(1/2\n√\n2π)n exp{\ni\nξ\n-\nX /(2 · 4)}\n(1/\n√\n>\ni\n> ξ\n2π)n exp{\n\n-\ni X /2\ni\n}\n(1/\n√\n2π)n exp{-\n\ni\n-\nalgebra: re\nct\n\nX /2\ni\n}\nje\nH0 if:\n\nXi > ξ\n-\nalgebra: reject\n\nH0 if\n> ξ\ni\n\nXi\ni\n-\nFind ξsuch that\n-\nFind ξsuch that\n\nn\n;\n\n=\n\nn\nP\nXi > ξ\nH0\nα\nP\nXi > ξ; H0\ni=1\ni=1\n\n= α\n-\nuse normal tables\n-\nthe distribution of\ni X\nis known\ni\n(derived distribution\n\nproblem)\n-\n\"chi-square\" distribution;\ntables are available\n\nComposite hypotheses\nIs my die fair?\n-\nGot S = 472 heads in n = 1000 tosses;\n-\nHypothesis H0:\nis the coin fair?\nP(X = i) = pi = 1/6, i = 1, . . . , 6\n-\nH0 : p = 1/2\nversus H1 : p = 1/2\n-\nObserved occurrences of i:\nNi\n-\nPick a \"statistic\" (e.g., S)\n-\nChoose form of rejection region;\nchi-square test:\n-\nPick shape of rejection region\n(e.g., |S -\n)2\nn/2| > ξ)\n(N\nnp\nreject H0 if\nT =\ni -\ni\n> ξ\nnp\ni\ni\n-\nChoose significance level (e.g., α = 0.05)\n\n-\nChoose ξ so that:\n-\nPick critical value ξ so that:\nP(reject H0; H0) = 0.05\nP(reject H0; H0) = α\nUsing the CLT:\nP(T > ξ; H0) = 0.05\nP(|S -500| ≤31; H0) ≈0.95;\nξ = 31\n-\nNeed the distribution of T:\n(CLT + derived distribution problem)\n-\nIn our example: |S -500| = 28 < ξ\n-\nfor large n, T has approximately\nH0 not rejected (at the 5% level)\na chi-square distribution\n-\navailable in tables\nDo I have the correct pdf?\nWhat else is there?\n-\nPartition the range into bins\n-\nSystematic methods for coming up with\n-\nnpi: expected incidence of bin i\nshape of rejection regions\n(from the pdf)\n-\nNi: observed incidence of bin i\n-\nMethods to estimate an unknown PDF\n(e.g., form a histogram and \"smooth\" it\n-\nUse chi-square test (as in die problem)\nout)\n-\nKolmogorov-Smirnov test:\nform empirical CDF, Fˆ , from data\n-\nEfficient and recursive signal processing\nX\n-\nMethods to select between less or more\ncomplex models\n-\n(e.g., identify relevant \"explanatory\nvariables\" in regression models)\n-\nMethods tailored to high-dimensional\nunknown parameter vectors and huge\nnumber of data points (data mining)\n(http://www.itl.nist.gov/div898/handbook/)\n-\netc. etc.. . .\n-\nDn = maxx |FX(x) -FˆX(x)|\n-\nP(√nDn ≥1.36) ≈0.05\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec01_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/68565b5f2f8533ea887831623926304e_MIT6_041F10_rec01_sol.pdf",
      "content": "+\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 1: Solutions\nSeptember 9, 2010\n1. Since the events A ∩ Bc and Ac ∩ B are disjoint, we have, using the additivity axiom,\nP((A ∩ Bc) ∪ (Ac ∩ B)) = P(A ∩ Bc) + P(Ac ∩ B).\nSince A = (A∩B)∪(A∩Bc) is the union of two disjoint sets, we have, again by the additivity\naxiom,\nP(A) = P(A ∩ B) + P(A ∩ Bc),\nso that\nP(A ∩ Bc) = P(A) - P(A ∩ B).\nSimilarly,\nP(B ∩ Ac) = P(B) - P(A ∩ B).\nTherefore,\nP(A ∩ Bc) + P(Ac ∩ B)\n= P(A) - P(A ∩ B) + P(B) - P(A ∩ B)\n= P(A) + P(B) - 2P(A ∩ B).\n2. Let\nA : The event that the randomly selected student is a genius.\nB : The event that the randomly selected student loves chocolate.\nFrom the properties of probability laws proved in lecture, we have\n= P(A ∪ B) + P((A ∪ B)c)\n= P(A) + P(B) - P(A ∩ B) + P(Ac ∩ Bc)\n=\n0.6 + 0.7 - 0.4 + P(Ac ∩ Bc)\n=\n0.9 + P(Ac ∩ Bc).\nTherefore\nP(A randomly selected student is neither a genius nor a chocolate lover)\n= P(Ac ∩ Bc) = 1 - 0.9 = 0.1.\n3. Let c denote the probability of a single odd face. Then the probability of a single even face\nis 2c, and by adding the probabilities of the 3 odd faces and the 3 even faces, we get 9c = 1.\nThus, c = 1/9. The desired probability is\nP({1, 2, 3}) = P({1}) + P({2}) + P({3}) = c + 2c + c = 4c = 4/9.\n4. See the textbook, Example 1.5, page 13.\nG1+. See the textbook, Problem 1.13, page 56.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/f641a82c4c92a8bd5387ab24e5584661_MIT6_041F10_rec01.pdf",
      "content": "+\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 1\nSeptember 9, 2010\n1. Give a mathematical derivation of the formula\nP((A ∩ Bc) ∪ (Ac ∩ B)) = P(A) + P(B) - 2P(A ∩ B).\nYour derivation should be a sequence of steps, with each step justified by appealing to one of\nthe probability axioms.\n2. Problem 1.5, page 54 in the text.\nOut of the students in a class, 60% are geniuses, 70% love chocolate, and 40% fall into both\ncategories. Determine the probability that a randomly selected student is neither a genius\nnor a chocolate lover.\n3. A six-sided die is loaded in a way that each even face is twice as likely as each odd face.\nConstruct a probabilistic model for a single roll of this die, and find the probability that a 1,\n2, or 3 will come up.\n4. Example 1.5, page 13 in the text.\nRomeo and Juliet have a date at a given time, and each will arrive at the meeting place with\na delay between 0 and 1 hour, with all pairs of delays being equally likely. The first to arrive\nwill wait for 15 minutes and will leave if the other has not yet arrived. What is the probability\nthat they will meet?\nG1+. Problem 1.13, page 56 in the text. Continuity property of probabilities.\n(a) Let A1, A2, . . . be an infinite sequence of events that is \"monotonically increasing,\" mean\ning that An ⊂ An+1 for every n. Let A = ∪inf\nn=1An. Show that P(A) = limn→inf P(An).\nHint: Express the event A as a union of countably many disjoint sets.\n(b) Suppose now that the events are \"monotonically decreasing,\" i.e., An+1 ⊂ An for every n.\nLet A =\nn=1An. Show that P(A) = limn→inf P(An). Hint: Apply the result of the\n∩inf\nprevious part to the complements of the events.\n(c) Consider a probabilistic model whose sample space is the real line. Show that\nP([0, inf)) = lim P([0, n])\nand\nlim P([n, inf)) = 0.\nn→inf\nn→inf\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec02_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/70b08609ad338aae678cd2666d5aee74_MIT6_041F10_rec02_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 2: Solutions\nSeptember 14, 2010\n1. Let A be the event that the first toss is a head and let B be the event that the second toss is a\nhead. We must compare the conditional probabilities P(A ∩B|A) and P(A ∩B|A ∪B). We have\nP((A ∩B) ∩A)\nP(A ∩B)\nP(A ∩B|A) =\n=\n,\nP(A)\nP(A)\nand\nP((A ∩B) ∩(A ∪B))\nA ∩B\nP(A ∩B|A ∪B) =\n=\n.\nP(A ∪B)\nA ∪B\nSince P(A ∪B) ≥ P(A), the first conditional probability above is at least as large, so Alice is\nright, regardless of whether the coin is fair or not. In the case where the coin is fair, that is, if\nall four outcomes HH, HT, T H, T T are equally likey, we have\nP(A ∩B)\n1/4\nP(A ∩B)\n1/4\n=\n=\n,\n=\n= 1/3.\nP(A)\n1/2\nP(A ∪B)\n3/4\nA generalization of Alice's reasoning is that if A, B, and C are events such that B ⊂ C and\nA ∩B = A ∩C (for example, if A ⊂ B ⊂ C), then the event A is at least as likely if we know\nthat B has occurred than if we know that C has occurred. Alice's reasoning corresponds to the\nspecial case where C = A ∪B.\n2.\n(a) Each possible outcome has probability 1/36. There are 6 possible outcomes that are doubles,\nso the probability of doubles is 6/36 = 1/6.\n(b) The conditioning event (sum is 4 or less) consists of the 6 outcomes\n{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)},\n2 of which are doubles, so the conditional probability of doubles is 2/6 = 1/3.\n(c) There are 11 possible outcomes with at least one 6, namely, (6, 6), (6, i), and (i, 6), for\ni = 1, 2, . . . , 5. Thus, the probability that at least one die is a 6 is 11/36.\n(d) There are 30 possible outcomes where the dice land on different numbers. Out of these,\nthere are 10 outcomes in which at least one of the rolls is a 6. Thus, the desired conditional\nprobability is 10/30 = 1/3.\n3.\n(a) See the textbook, Example 1.13, page 29.\n(b) See the textbook, Example 1.17, page 33.\n4. See the textbook, Example 1.12 (The Monty Hall Problem), page 27.\nAn alternative solution is given below:\nLet Pi denote the event where the prize is behind door i, Ci denote the event where you initially\nchoose door i, and Oi denote the event where your friend opens door i. The corresponding prob\nability tree is:\nPage 1 of 3\n\n3)\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nO1\nO1\nO1\nO1\nO2\nO2\nO2\nO2\nO3\nO3\nO3\nO3\nC1\nC1\nC1\nC2\nC2\nC2\nC3\nC3\nC3\nP1\nP2\nP3\nP (P1)\nP (P2)\nP (P3)\nP (C1 | P1)\nP (C2 | P1)\nP (C3 | P1)\nP (C1 | P2)\nP (C2 | P2)\nP (C3 | P2)\nP (C1 | P3)\nP (C2 | P3)\nP (C3 | P3)\nP (O2 | P1 ∩C1)\nP (O3 | P1 ∩C1)\nP (O1 | P2 ∩C2)\nP (O3 | P2 ∩C2)\nP (O1 | P3 ∩C3)\nP (O2 | P3 ∩C\nPage 2 of 3\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(a) The probability of winning when not switching from your initial choice is the probability\nthat the prize is behind the door you initially chose:\nP(Win when not switching)\n=\nP(P1 ∩C1) + P(P2 ∩C2) + P(P3 ∩C3)\n=\nP(P1)P(C1|P1) + P(P2)P(C2|P2) + P(P3)P(C3|P3)\n=\nP(P1)P(C1) + P(P2)P(C2) + P(P3)P(C3)\n=\n1/3 · (P(C1) + P(C2) + P(C3))\n=\n1/3\n(b) The probability of winning when switching from your initial choice is the probability that\nthe prize is behind the remaining (unopened) door:\nP(Win when switching)\n=\nP(P1 ∩C2 ∩O3) + P(P1 ∩C3 ∩O2) + P(P2 ∩C1 ∩O3)\n+P(P2 ∩C3 ∩O1) + P(P3 ∩C1 ∩O2) + P(P3 ∩C2 ∩O1)\n=\nP(P1 ∩C2) + P(P1 ∩C3) + P(P2 ∩C1) + P(P2 ∩C3)\n+P(P3 ∩C1) + P(P3 ∩C2)\n=\nP(P1)P(C2) + P(P1)P(C3) + P(P2)P(C1) + P(P2)P(C3)\n+P(P3)P(C1) + P(P3)P(C2)\n=\n2/3 · (P(C1) + P(C2) + P(C3))\n=\n2/3\n(c) Given C1, that you first choose door 1, with the new strategy of switching only if door 3 is\nopened, you win if the prize behind door 1 and door 2 is opened or if the prize is behind\ndoor 2 and door 3 is opened.\nP(Win with new strategy|C1)\n=\nP(P1 ∩O2|C1) + P(P2 ∩O3|C1)\n=\nP(P1|C1)P(O2|P1 ∩C1) + P(P2|C1)P(O3|P2 ∩C1)\n=\nP(P1)P(O2|P1 ∩C1) + P(P2)P(O3|P2 ∩C1)\n=\n1/3 · P(O2|P1 ∩C1) + 1/3 · 1\n=\n1/3 · (P(O2|P1 ∩C1) + 1)\nGiven that your initial choice is door 1, the probability of winning under this new strategy\nis dependent on how your friend decides which of doors 2 or 3 to open if the prize also lies be\nhind door 1. If he always picks door 2, then P(O2|P1∩C1) = 1 and P(Win with new strategy|C1) =\n2/3. If he picks between doors 2 and 3 with equal probability then P(O2|P1 ∩C1) = 1/2\nand P(Win with new strategy|C1) = 1/2.\nPage 3 of 3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/1916e8123da9d6a5f5fa106546316648_MIT6_041F10_rec02.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 2\nSeptember 14, 2010\n1. Problem 1.15, page 56-57 in the text.\nA coin is tossed twice. Alice claims that the event of two heads is at least as likely if we know\nthat the first toss is a head than if we know that at least one of the tosses is a head. Is she right?\nDoes it make a difference if the coin is fair or unfair? How can we generalize Alice's reasoning?\n2. Problem 1.14, page 56 in the text.\nWe roll two fair 6-sided dice. Each one of the 36 possible outcomes is assumed to be equally\nlikely.\n(a) Find the probability that doubles are rolled.\n(b) Given that the roll results in a sum of 4 or less, find the conditional probability that doubles\nare rolled.\n(c) Find the probability that at least one die roll is a 6.\n(d) Given that the two dice land on different numbers, find the conditional probability that at\nleast one die roll is a 6.\n3. Example 1.13, page 29, and Example 1.17, page 33, in the text.\nYou enter a chess tournament where your probability of winning a game is 0.3 against half of the\nplayers (call them type 1), 0.4 against a quarter of the players (call them type 2), and 0.5 against\nthe remaining quarter of the players (call them type 3). You play a game against a randomly\nchosen opponent.\n(a) What is the probability of winning?\n(b) Suppose that you win. What is the probability that you had an opponent of type 1?\n4. Example 1.12, page 27 in the text.\nThe Monty Hall Problem. This is a much discussed puzzle, based on an old American game\nshow. You are told that a prize is equally likely to be found behind any one of three closed doors\nin front of you. You point to one of the doors. A friend opens for you one of the remaining two\ndoors, after making sure that the prize is not behind it. At this point, you can stick to your\ninitial choice, or switch to the other unopened door. You win the prize if it lies behind your final\nchoice of a door. Consider the following strategies:\n(a) Stick to your initial choice.\n(b) Switch to the other unopened door.\n(c) You first point to door 1. If door 2 is opened, you do not switch. If door 3 is opened, you\nswitch.\nWhich is the best strategy?\nf 1\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 o\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec03_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/eeaae1542402636023ff0cc63753b105_MIT6_041F10_rec03_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 3: Solutions\nSeptember 16, 2010\n1. See the textbook, Example 1.20, page 37.\n2.\n(a) In order to wind up in the same place after two steps, the tightrope walker can either step\nforwards, then backwards, or vice versa. Therefore the required probability is:\n2 · p · (1 -p).\n(b) The probability that after three steps he will be one step ahead of his starting point is the\nprobability that out of 3 steps in total, 2 of them are forwards, and one is backwards. This\nequals:\n3 · p 2 · (1 -p).\n(c) Given that out of his three steps only one is backwards, the sample space for the experiment\nis:\n{(F, F, B); (F, B, F ); (B, F, F )}\nwhere F denotes a step forwards, and B a step backwards. Each of these sample points is\nequally likely, therefore the probability that his first step is a step forward is 2\n3 .\n3. See the textbook, Problem 1.31, page 60.\n4.\n(a) A is independent of itself if and only if P(A ∩A) = P(A)P(A). Since A ∩A = A then A\nmust satisfy P(A) = (P(A))2 . Therefore, A is independent of itself if and only if P(A) = 1\nor P(A) = 0.\n(b) See solution to Problem 1.43(a) in text on pages 63-64.\n(c) See solution to Problem 1.44 in text on page 64.\nPage 1 of 1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/150d4f74e52b8820b8727512a414e57f_MIT6_041F10_rec03.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 3: September 16, 2010\n1. Example 1.20, page 37 in the text.\nConsider two independent fair coin tosses, in which all four possible outcomes are equally likely.\nLet\nH1 =\n{1st toss is a head},\nH2 =\n{2nd toss is a head},\nD\n=\n{the two tosses produced different results}.\n(a) Are the events H1 and H2 (unconditionally) independent?\n(b) Given event D has occurred, are the events H1 and H2 (conditionally) independent?\n2. Imagine a drunk tightrope walker, in the middle of a really long tightrope, who manages to keep\nhis balance, but takes a step forward with probability p and takes a step back with probability\n(1 -p).\n(a) What is the probability that after two steps the tightrope walker will be at the same place\non the rope?\n(b) What is the probability that after three steps, the tightrope walker will be one step forward\nfrom where he began?\n(c) Given that after three steps he has managed to move ahead one step, what is the probability\nthat the first step he took was a step forward?\n3. Problem 1.31, page 60 in the text.\nCommunication through a noisy channel. A binary (0 or 1) message transmitted through\na noisy communication channel is received incorrectly with probability o0 and o1, respectively\n(see the figure). Errors in different symbol transmissions are independent. The channel source\ntransmits a 0 with probability p and transmits a 1 with probability 1 -p.\n1-e0\n1-e1\ne1\ne0\nFigure 1: Error probabilities in a binary communication channel.\n(a) What is the probability that a randomly chosen symbol is received correctly?\n(b) Suppose that the string of symbols 1011 is transmitted. What is the probability that all\nthe symbols in the string are received correctly?\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(c) In an effort to improve reliability, each symbol is transmitted three times and the received\nsymbol is decoded by majority rule. In other words, a 0 (or 1) is transmitted as 000 (or\n111, respectively), and it is decoded at the receiver as a 0 (or 1) if and only if the received\nthree-symbol string contains at least two 0s (or 1s, respectively). What is the probability\nthat a transmitted 0 is correctly decoded?\n(d) Suppose that the scheme of part (c) is used. What is the probability that a 0 was transmitted\ngiven that the received string is 101?\n4.\n(a) Can an event A be independent of itself?\n(b) Problem 1.43(a) on page 63 in text.\nLet A and B be independent events. Use the definition of independence to prove that the\nevents A and Bc are independent.\n(c) Problem 1.44 on page 64 in text.\nLet A, B, and C be independent events, with P(C) > 0. Prove that A and B are condi\ntionally independent of C.\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec04_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/f86e38378452fa30e7583b26e7362be0_MIT6_041F10_rec04_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 4 Solutions\nSeptember 21, 2010\n1. The sample space consists of all possible choices for the birthday of each person. Since there\nare n persons, and each has 365 choices for their birthday, the sample space has 365n elements.\nLet us now consider those choices of birthdays for which no two persons have the same birthday.\nAssuming that n ≤ 365, there are 365 choices for the first person, 364 for the second, etc., for a\ntotal of 365 · 364 · · · (365 - n + 1). Thus,\n365 · 364 · · · (365 - n + 1)\nP(no two birthdays coincide) =\n.\n365n\nIt is interesting to note that for n as small as 23, the probability that there are two persons with\nthe same birthday is larger than 1/2.\n2. As we have done before, we will count the number of favorable positions in which we can safely\nplace 8 rooks, and then divide this by the total number of positions for 8 rooks on a 8 × 8\nchessboard. First we count the number of favorable positions for the rooks. We will place the\nPage 1 of 2\nG\nra\nph\ns\nof\n\nth\ne\npr\nob\na\nb\nility\nof\nhavi\nng a\ndis\ntinc\nt bi\nrthd\nay v\ns. n\numb\ner\nof\npeo\nple\npr\nese\nnt.\nImage by MIT OpenCourseWare.\n\n!\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nrooks one by one. For the first rook, there are no constraints, so we have 64 choices. Placing this\nrook, however, eliminates one row and one column. Thus for our second rook, we can imagine\nthat the illegal column and row have been removed, thus leaving us with a 7 × 7 chessboard, and\nthus with 49 choices. Similarly, for the third rook we have 36 choices, for the fourth 25, etc...\nThere are 64 · 63 · · · 57 total ways we can place 8 rooks without any restrictions, and therefore\nthe probability we are after is:\n64 · 49 · 36 · 25 · 16 · 9 · 4 .\n64!\n56!\n3. See textbook, Problem 1.61, page 69.\n4. The group of n slots is divided into segments of length n1, . . . , nr slots. The n items can be\narranged in n! ways, where each arrangement corresponds to a partition into the r segments.\nBut all arrangements within a single segment lead to the same partition, where there are ni!\nways to arrange the items within ith segment. Thus, for each segment we must divide by the\nnumber of ways to arrange the items within that segment. The solution is then:\nWays to arrange n items\nn!\n=\n(Ways to arrange items in segment 1) · · · (Ways to arrange items in segment r)\nn1! · · · nr!\n5. The probability of drawing a particular sequence of balls containing exactly ni of color i balls is\np1\nn1 · · · pr\nnr . The number of possible sequences containing ni of color i balls is the number of ways\nto form a partition of n distinct slots into subsets of cardinality n1, . . . , nr which is\nn\n.\nn1,...,nr\nTherefore, the probability of obtaining exactly ni balls of color i is:\nn\nn1\nnr\np1 · · · pr\nn1, . . . , nr\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/e3b96768f633b03514ae3599b9eb8ccb_MIT6_041F10_rec04.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 4\nSeptember 21, 2010\n1. Problem 1.50, page 67 in the text.\nThe birthday problem. Consider n people who are attending a party. We assume that every\nperson has an equal probability of being born on any day during the year, independently of\neveryone else, and ignore the additional complication presented by leap years (i.e., nobody is\nborn on February 29). What is the probability that each person has a distinct birthday?\n2. Imagine that 8 rooks are randomly placed on a chessboard. Find the probability that all the\nrooks will be safe from one another, i.e. that there is no row or column with more than one rook.\n3. Problem 1.61, page 69 in the text.\nHypergeometric probabilities. An urn contains n balls, out of which exactly m are red. We\nselect k of the balls at random, without replacement (i.e., selected balls are not put back into\nthe urn before the next selection). What is the probability that i of the selected balls are red?\n4. Multinomial coefficient. Derive the multinomial coefficient (the number of partitions of n\ndistinct items into groups of n1, . . . , nr) using a different argument than the one in class. Consider\nn items which can be placed into n slots and divide the group of n slots into segments of length\nn1, . . . , nr slots. Derive the multinomial coefficient by showing how many different ways can the\nn items be arranged into the r segments.\n5. Multinomial probabilities. At each draw, there is a probability pi (i = 1, . . . , r) of getting a\nball of color i. Draw n objects. What is the probability of obtaining exactly ni of each color i?\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 1\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 4: Extra Handout\nSeptember 21, 2010\n1. As part of the solution to problem 1, plotted below are the probabilities of each person having a\ndistinct birthday versus n the number of people present.\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 1\nG\nra\nph\ns\nof\n\nth\ne\npr\nob\na\nb\nility\nof\nhavi\nng a\ndis\ntinc\nt bi\nrthd\nay v\ns. n\numb\ner\nof\npeo\nple\npr\nese\nnt.\nImage by MIT OpenCourseWare.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec05_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/1c3bb2e08f5017ebf07a8b997559a1b2_MIT6_041F10_rec05_sol.pdf",
      "content": "!\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 5 Solutions\nSeptember 23, 2010\n1.\n(a) See derivation in textbook pp. 84-85.\n(b) See derivation in textbook p. 86.\n(c) See derivation in textbook p. 87.\n2.\n(a) X is a Binomial random variable with n = 10, p = 0.2. Therefore,\npX (k) =\n0.2k 0.810-k ,\nfor k = 0, . . . , 10\nk\nand pX (k) = 0 otherwise.\nprobability\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nnumber of hits\n(b) P(No hits) = pX (0) = (0.8)10 = 0.1074\nP10\nP10\n10 0.2k 0.810-k\n(c) P(More hists than misses) =\nk=6 pX (k) =\nk=6\nk\n= 0.0064\n(d) Since X is a Binomial random variable,\nE[X] = 10 · 0.2 = 2\nvar(X) = 10 · 0.2 · 0.8 = 1.6\n(e) Y = 2X - 3, and therefore\nE[Y ] = 2E[X] - 3 = 1\nvar(Y ) = 4var(X) = 6.4\n(f) Z = X2, and therefore\nE[Z] = E[X2] = (E[X])2 + var(X) = 5.6\n3.\n(a) We expect E[X] to be higher than E[Y ] since if we choose the student, we are more likely\nto pick a bus with more students.\n(b) To solve this problem formally, we first compute the PMF of each random variable and then\ncompute their expectations.\n\n40/148\nx = 40\n\n33/148\nx = 33\n\npX (x) =\n25/148\nx = 25\n\n50/148\nx = 50\n\notherwise.\nPage 1 of 2\n\n(\nX\nX\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nand E[X] = 40 40 + 33 33 + 25 25 + 50 50 = 39.28\n1/4\ny = 40, 33, 25, 50\npY (y) =\notherwise.\nand E[Y ] = 40 1\n4 + 33 1\n4 + 25 1\n4 + 50 1\n4 = 37\nClearly, E[X] > E[Y ].\n4. The expected value of the gain for a single game is infinite since if X is your gain, then\ninf\ninf\n2k · 2-k =\n1 = inf\nk=1\nk=1\nThus if you are faced with the choice of playing for given fee f or not playing at all,and your\nobjective is to make the choice that maximizes your expected net gain, you would be willing to\npay any value of f. However, this is in strong disagreement with the behavior of individuals. In\nfact experiments have shown that most people are willing to pay only about $20 to $30 to play\nthe game. The discrepancy is due to a presumption that the amount one is willing to pay is\ndetermined by the expected gain. However, expected gain does not take into account a persons\nattitude towards risk taking.\nBelow are histograms showing the payout results for various numbers of simulations of this game:\n20 simulations, observed average = $19.20\n200 simulations, observed average = $11.16\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_rec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/855330af7b790c271c2aeb2e5b428a15_MIT6_041F10_rec05.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 5\nSeptember 23, 2010\n1.\n(a) Derive the expected value rule for functions of random variables E[g(X)] = P\nx g(x)pX (x).\n(b) Derive the property for the mean and variance of a linear function of a random variable\nY = aX + b.\nE[Y ] = aE[X] + b,\nvar(Y ) = a 2var(X).\n(c) Derive var(X) = E[X2] - (E[X])2\n2. A marksman takes 10 shots at a target and has probability 0.2 of hitting the target with each\nshot, independently of all other shots. Let X be the number of hits.\n(a) Calculate and sketch the PMF of X.\n(b) What is the probability of scoring no hits?\n(c) What is the probability of scoring more hits than misses?\n(d) Find the expectation and the variance of X.\n(e) Suppose the marksman has to pay $3 to enter the shooting range and he gets $2 dollars for\neach hit. Let Y be his profit. Find the expectation and the variance of Y .\n(f) Now let's assume that the marksman enters the shooting range for free and gets the number\nof dollars that is equal to the square of the number of hits. Let Z be his profit. Find the\nexpectation of Z.\n3. 4 buses carrying 148 job-seeking MIT students arrive at a job convention. The buses carry 40,\n33, 25, and 50 students, respectively. One of the students is randomly selected. Let X denote\nthe number of students that were on the bus carrying this randomly selected student. One of\nthe 4 bus drivers is also randomly selected. Let Y denote the number of students on his bus.\n(a) Which of E[X] or E[Y ] do you think is larger? Give your reasoning in words.\n(b) Compute E[X] and E[Y ].\n4. Problem 2.21, page 123 in the text.\nSt. Petersburg paradox. You toss independently a fair coin and you count the number of\ntosses until the first tail appears.\nIf this number is n, you receive 2n dollars.\nWhat is the\nexpected amount that you will receive? How much would you be willing to pay to play this\ngame?\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 1\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nRecitation 5: Extra Handout\nSeptember 23, 2010\n1. To show some relavant computations to Problem 4, the results (plotted as histograms) of simu\nlations of this game have been plotted below for various numbers of simulations.\n20 simulations, observed average = $19.20\n200 simulations, observed average = $11.16\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut01_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/bfadb76b9e97ce2053facf48fdafbee0_MIT6_041F10_tut01_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 1 Solutions\nSeptember 16/17, 2010\n1. If A ⊂ B, then P(B ∩A) = P(A) But we know that in order for A and B to be independent,\nP(B∩A) = P(A)P(B). Therefore, A and B are independent if and only if P(B) = 1 or P(A) = 0.\nThis could happen, for example, if B is the universe or if A is empty.\n2. This problem is similar in nature to Example 1.24, page 40. In order to compute the success\nprobability of individual sub-systems, we make use of the following two properties, derived in\nthat example:\n- If a serial sub-system contains m components with success probabilities p1, p2...pm, then\nthe probability of success of the entire sub-system is given by\nP(whole system succeeds) = p1p2p3...pm\n- If a parallel sub-system contains m components with success probabilities p1, p2...pm, then\nthe probability of success of the entire sub-system is given by\nP(whole system succeeds) = 1 -(1 -p1)(1 -p2)(1 -p3)...(1 -pm)\nLet P(X → Y ) denote the probability of a successful connection between node X and Y. Then,\nP(A → B)\n= P(A → C)P(C → E)P(E → B) (since they are in series)\nP(A → C)\n= p\nP(C → E)\n=\n1 -(1 -p) (1 -P(C → D)P(D → E))\nP(E → B)\n=\n1 -(1 -p)2\nThe probabilities P(C → D), P(D → E) can be similarly computed as\nP(C → D)\n=\n1 -(1 -p)3\nP(D → E)\n= p\nThe probability of success of the entire system can be obtained by substituting the subsystem\nsuccess probabilities:\nP(A → B) = p (1 -(1 -p)(1 -(1 -(1 -p)3)p) (1 -(1 -p)2).\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n3. The Chess Problem.\n(a)\ni. P(2nd Rnd Req) = (0.6)2 + (0.4)2 = 0.52\nii. P(Bo Wins 1st Rnd) = (0.6)2 = 0.36\niii. P(Al Champ) = 1 -P(Bo Champ) -P(Ci Champ)\n= 1 -(0.6)2 ∗ (0.5)2 -(0.4)2 ∗ (0.3)2 = 0.8956\n(b)\ni. P(Bo Challenger|2nd Rnd Req) = (0.6)2 = 0.36 = 0.6923\n0.52\n0.52\nii. P(Al Champ|2nd Rnd Req)\n= P(Al Champ|Bo Challenger, 2nd Rnd Req) × P(Bo Challenger|2nd Rnd Req)\n+ P(Al Champ|Ci Challenger, 2nd Rnd Req) × P(Ci Challenger|2nd Rnd Req)\n= (1 -(0.5)2) × 0.6923 + (1 -(0.3)2) × 0.3077\n= 0.7992\n(0.6)2∗(0.5)\n(c) P((Bo Challenger)|{(2nd Rnd Req) ∩(One Game)}) = (0.6)2∗(0.5)+(0.4)2∗(0.7)\n(0.6)2(0.5)\n=\n= 0.6164\n0.2920\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/ac45ace6e35d1bbc97cdec005bc7010a_MIT6_041F10_tut01.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 1\nSeptember 16/17, 2010\n1. Let A and B be events such that A ⊂ B. Can A and B be independent?\n2. An electrical system consists of identical components that are operational with probability p\nindependently of other components.\nThe components are connected in three subsystems, as\nshown in the figure. The system is operational if there is a path that starts at point A, ends\nat point B, and consists of operational components. This is the same as requiring that all three\nsubsystems are operational. What are the probabilities that the three subsystems, as well as the\nentire system, are operational?\nA\nB\nFigure 1: A system of identical components that consists of the three subsystems 1, 2, and 3. The\nsystem is operational if there is a path that starts at point A, ends at point B, and consists of operational\ncomponents.\n3. The Chess Problem. This year's Belmont chess champion is to be selected by the following\nprocedure. Bo and Ci, the leading challengers, first play a two-game match. If one of them wins\nboth games, he gets to play a two-game second round with Al, the current champion. Al retains\nhis championship unless a second round is required and the challenger beats Al in both games.\nIf Al wins the initial game of the second round, no more games are played.\nFurthermore, we know the following:\n- The probability that Bo will beat Ci in any particular game is 0.6.\n- The probability that Al will beat Bo in any particular game is 0.5.\n- The probability that Al will beat Ci in any particular game is 0.7.\nAssume no tie games are possible and all games are independent.\n(a) Determine the apriori probabilities that\ni. the second round will be required.\nii. Bo will win the first round.\niii. Al will retain his championship this year.\n(b) Given that the second round is required, determine the conditional probabilities that\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\ni. Bo is the surviving challenger.\nii. Al retains his championship.\n(c) Given that the second round was required and that it comprised only one game, what is the\nconditional probability that it was Bo who won the first round?\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut02_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/3319a9ca4cf8a26e58c8d511990ee28b_MIT6_041F10_tut02_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 2 Solutions\nSeptember 23/24, 2010\n1. A player is randomly dealt 13 cards from a standard 52-card deck.\n(a) What is the probability the 13th card dealt is a king?\nAnswer: 4 .\nSolution: Since we are not told anything about the first 12 cards that are dealt, the\nprobability that the 13th card dealt is a King, is the same as the probability that the first\ncard dealt, or in fact any particular card dealt is a King, and this equals:\n4 .\n(b) What is the probability the 13th card dealt is the first king dealt?\n48 52\nAnswer: 13 · 4 12 / 13 .\nSolution: The probability that the 13th card is the first king to be dealt is the probability\nthat out of the first 13 cards to be dealt, exactly one was a king, and that the king was\ndealt last. Now, given that exactly one king was dealt in the first 13 cards, the probability\nthat the king was dealt last is just 1/13, since each \"position\" is equally likely. Thus, it\nremains to calculate the probability that there was exactly one king in the first 13 cards\ndealt. To calculate this probability we count the \"favorable\" outcomes and divide by the\ntotal number of possible outcomes. We first count the favorable outcomes, namely those\nwith exactly one king in the first 13 cards dealt. We can choose a particular king in 4 ways,\nand we can choose the other 12 cards in\n12 ways, therefore there are 4 ·\n12 favorable\noutcomes. There are\n13 total outcomes, so the desired probability is\n4 12\n· .\nFor an alternative solution, we argue as in Example 1.10. The probability that the first card\nis not a king is 48/52. Given that, the probability that the second is not a king is 47/51.\nWe continue similarly until the 12th card. The probability that the 12th card is not a king,\ngiven that none of the preceding 11 was a king, is 37/41. (There are 52 - 11 = 41 cards\nleft, and 48 -11 = 37 of them are not kings.) Finally, the conditional probability that the\n13th card is a king is 4/40. The desired probability is\n48 · 47 · · · 37 · 4 .\n52 · 51 · · · 41 · 40\n2. Consider a random variable X such that\nx\npX (x) =\nfor x ∈{-3, -2, -1, 1, 2, 3}, P(X = x) = 0 for x ∈{-3, -2, -1, 1, 2, 3},\na\nwhere a > 0 is a real parameter.\n(a) Find a.\nPage 1 of 2\n\nX\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nSolution. The sum of the values of the PMF of a random variable over all values that it\ntakes with positive probability must be equal to 1. Hence, we have\n=\npX (x)\nx=-3\n=\n+\n+\n+\n+\n+\na\na\na\na\na\na\n=\n,\na\nwhich implies that a = 28.\n(b) What is the PMF of the random variable Z = X2 ?\nSolution. The following table shows the value of Z for a given value of X and the probability\nof that event.\nx\n-3\n-2\n-1\npX (x)\n9/28\n1/7\n1/28\n1/28\n1/7\n9/28\nZ|X = x\nWe see that Z can take only three possible values with non-zero probability, namely 1,4, and\n9. In addition, for each value, there correspond two values of X. So we have, for example,\npZ (9) = P(Z = 9) = P(X = -3) + P(X = 3) = pX (-3) + pX (3). Hence the PMF of Z is\ngiven by\n\n1/14\nif z = 1,\n\npZ (z) =\n2/7\nif z = 4,\n\n9/14\nif z = 9.\n3. Suppose we label the classes A, B, and C. Now the probability that Joe and Jane will both be\nin class A is the number of possible combinations for class A that involve both Joe and Jane,\ndivided by the total number of combinations for class A. Therefore the probability we are after\nis:\n.\nSince there are three classrooms, the probability that Joe and Jane end up in the same classroom\nis simply three times the answer we found above:\n3 · 28 .\nAnother way of looking at the problem is described as follows,\nAssume one of them pick first, say Joe. He can pick any one of the 90 available places. Then it's\nJane's turn to pick. She has a probability of 29 of picking in the same class as Joe. Therefore,\n(88\n28)\nthe overall probability is 29 , which is the same as 3 · (90 .\n)\n4. Let A = event the 7 cards include exactly 3 aces.\nP (A) = (# ways to choose 3 aces) · (# ways to choose other 4 cards) =\n3 4 .\n# ways to choose 7 cards\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/c014116ab8af3b426139d0c3345b3bca_MIT6_041F10_tut02.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 2\nSeptember 23/24, 2010\n1. A player is randomly dealt 13 cards from a standard 52-card deck.\n(a) What is the probability the 13th card dealt is a king?\n(b) What is the probability the 13th card dealt is the first king dealt?\n2. Consider a random variable X such that\nx\npX (x) =\nfor x ∈{-3, -2, -1, 1, 2, 3}, P(X = x) = 0 for x ∈{-3, -2, -1, 1, 2, 3},\na\nwhere a > 0 is a real parameter.\n(a) Find a.\n(b) What is the PMF of the random variable Z = X2 ?\n3. 90 students, including Joe and Jane, are to be split into three classes of equal size, and this is to\nbe done at random. What is the probability that Joe and Jane end up in the same class?\n4. Draw the top 7 cards from a well-shuffled standard 52-card deck. Find the probability that the\n7 cards include exactly 3 aces.\nPage 1 of 1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut03_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/d200d04fbd04b10c35443eed1a01cac1_MIT6_041F10_tut03_sol.pdf",
      "content": "X X\nX\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 3: Solutions\n1. In general we have that E[aX + bY + c] = aE[X] + bE[Y ] + c. Therefore,\nE[Z] = 2 · E[X] -3 · E[Y ].\nFor the case of independent random variables, we have that if Z = a · X + b · Y , then\nvar(Z) = a 2 · var(X) + b2 · var(Y ).\nTherefore, var(Z) = 4 · var(X) + 9 · var(Y ).\n2. See online solutions.\n3.\n(a) We can find c knowing that the probability of the entire sample space must equal 1.\n=\npX,Y (x, y)\nx=1 y=1\n=\nc + c + 2c + 2c + 4c + 3c + c + 6c\n=\n20c\nTherefore, c = 20 .\nP3\n(b) pY (2) =\nx=1 pX,Y (x, 2) = 2c + 0 + 4c = 6c = 10 .\n(c) Z = Y X2\nE[Z | Y = 2]\n=\nE[Y X2 | Y = 2]\n=\nE[2X2 | Y = 2]\n=\n2E[X2 | Y = 2]\npX|Y (x | 2) = pX,Y (x,2) .\npY (2)\nTherefore,\n\n1/10 = 1\nif x = 1\n\n3/10\npX|Y (x | 2)\n=\n1/5 = 2\nif x = 3\n3/10\n\notherwise\nE[Z | Y = 2]\n=\nx 2 pX|Y (x | 2)\nx=1\n=\n(12) · 1 + (32) · 2\n=\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(d) Yes. Given X = 2, the distribution of X is the same given Y = y.\nP(X = x | Y = y, X = 2) = P(X = x | X = 2).\nFor example,\nP(X = 1 | Y = 1, X = 2) = P(X = 1 | Y = 3, X = 2) = P(X = 1 | X =\n2) = 3\n.\npX,Y (2,y)\n(e) pY |X (y | 2) =\npX (2) .\nP3\npX (2) =\ny=1 pX,Y (2, y) = c + 0 + c = 2c = 10 .\nTherefore,\n\n1/20\n1/10 = 1\nif y = 1\npY |X (y | 2)\n=\n\n1/20\n1/10 = 1\nif y = 3\n\notherwise\nE[Y 2 | X = 2] = P3\ny=1 y 2 pY |X (y | 2) = (12) · 1\n2 + (32) · 1\n2 = 5.\nE[Y | X = 2] = P\ny\n=1 ypY |X (y | 2) = 1 · 2\n1 + 3 · 2\n1 = 2.\nvar(Y | X = 2) = E[Y 2 | X = 2] -E[Y | X = 2]2 = 5 -22 = 1.\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/d012d36ab29c184b3c880f8e6be9b7a2_MIT6_041F10_tut03.pdf",
      "content": "X\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 3\nSeptember 30/October 1, 2010\n1. Let X and Y be independent random variables. Random variable X has mean μX and variance\nσ2 , and random variable Y has mean μY and variance σY\n2 . Let Z = 2X -3Y . Find the mean\nand variance of Z in terms of the means and variances of X and Y .\n2. Problem 2.40, page 133 in the text.\nA particular professor is known for his arbitrary grading policies. Each paper receives a grade\nfrom the set {A, A-, B+, B, B-, C+}, with equal probability, independently of other papers.\nHow many papers do you expect to hand in before you receive each possible grade at least once?\n3. The joint PMF of the random variables X and Y is given by the following table:\ny = 3\nc\nc\n2c\ny = 2\n2c\n4c\ny = 1\n3c\nc\n6c\nx = 1\nx = 2\nx = 3\n(a) Find the value of the constant c.\n(b) Find pY (2).\n(c) Consider the random variable Z = Y X2 . Find E[Z | Y = 2].\n(d) Conditioned on the event that X = 2, are X and Y independent? Give a one-line justifica\ntion.\n(e) Find the conditional variance of Y given that X = 2.\nPage 1 of 1\nTextbook problems are courtesy of Athena Scientific, and are used with permission.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut04_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/95fd5fe6e4607daaac9013c2fb622e8a_MIT6_041F10_tut04_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 4: Solutions\n1.\n(a)\nP(X ≤ 1.5)\n=\nΦ(1.5)\n=\n0.9332.\nP(X ≤-1)\n=\n1 -P(X ≤ 1)\n=\n1 -Φ(1)\n=\n1 -0.8413\n=\n0.1587.\n(b)\nY -1\nE\n=\n2(E[Y ] -1)\n=\n0.\nY -1\nY\nvar\n=\nvar\n=\nvarY\n=\n1.\nThus, the distribution of Y\n-1 is N(0, 1).\n(c)\n-1 -1\nY -1\n1 -1\nP(-1 ≤Y ≤ 1)\n=\nP(\n≤\n≤\n)\n=\nΦ(0) -Φ(-1)\n=\nΦ(0) -(1 -Φ(1))\n=\n0.3413.\n2. Example 3.15, page 169 in text. See solutions in the text.\n3. Problem 3.20, page 191 in text. See online solutions.\nPage 1 of 1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/967053c76fdbd4260b39c94f545f5937_MIT6_041F10_tut04.pdf",
      "content": "Textbook problems are courtesy of Athena Scientific, and are used with permission.\nPage 1 of 1\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 4\nOctober 7/8, 2010\n1. Let X and Y be Gaussian random variables, with X ∼ N(0, 1) and Y ∼ N(1, 4).\n(a) Find P(X ≤ 1.5) and P(X ≤-1).\n(b) What is the distribution of Y\n-1 ?\n(c) Find P(-1 ≤ Y ≤ 1).\n2. Example 3.15, page 169 in text.\nBen throws a dart at a circular target of radius r. We assume that he always hits the target,\nand that all points of impact (x, y) are equally likely. Compute the joint PDF fX,Y (x, y) of the\nrandom variables X and Y and compute the conditional PDF fX|Y (x|y).\n3. Problem 3.20, page 191 in text.\nAn absent-minded professor schedules two student appointments for the same time. The ap\npointment durations are independent and exponentially distributed with mean thirty minutes.\nThe first student arrives on time, but the second student arrives five minutes late. What is the\nexpected value of the time between the arrival of the first student and the departure of the second\nstudent?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut05_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/864ebf4deba343514ed69692f3a8398f_MIT6_041F10_tut05_sol.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 5: Solutions\n1.\n(a) Let A be the event that the machine is functional. Conditioned on the random variable Q\ntaking on a particular value q, P(A|Q = q) = q. Using the continuous form of the total\nprobability theorem, the probability of event A is given by:\nZ 1\nP(A)\n=\nP(A|Q = q)fQ(q)dq\nZ 1\n=\nq dq\n=\n1/2\n(b) Let B be the event that the machine is functional on m out of the last n days. Conditioned\non random variable Q taking on value q (a probability q of being functional) the probability\nof event B is binomial with n trials, m successes, and a probability q of success in each trial.\nAgain using the total probability theorem, the probability of event B is given by:\nZ 1\nP(B)\n=\nP(B|Q = q)fQ(q)dq\nZ 1\n=\nn\nq m(1 -q)n-mfQ(q)dq\nm\nn\nm!(n -m)!\n=\nm\n(n + 1)!\nWe then find the distribution on Q conditioned on event B using Bayes rule:\nP(B|Q = q)fQ(q)\nfQ|B(q)\n=\nP(B)\nqm(1 -q)n-m\n=\nm!(n-m)!\n0 ≤ q ≤ 1,\nn ≥ m.\n(n+1)!\n2. Since Y = |X| you can visualize the PDF for any given y as\nfX (y) + fX (-y), if y ≥ 0,\nfY (y)\n=\n0,\nif y < 0.\nAlso note that since Y = |X|, Y ≥ 0.\n3 , if -2 < x ≤ 1,\n(a) fX (x) =\n0, otherwise.\nSo, fX (x) for -1 ≤ x ≤ 0 gets added to fX (x) for 0 ≤ x ≤ 1:\n\n2/3, if 0 ≤ y ≤ 1,\nfY (y)\n=\n1/3, if 1 < y ≤ 2,\n0,\notherwise.\nPage 1 of 2\n\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\n(b) Here we are told X > 0. So there are no negative values of X that need to be considered.\nThus,\n2e-2y , if y ≥ 0,\nfY (y) = fX (y)\n=\n0,\notherwise.\n(c) As explained in the beginning, fY (y) = fX (y) + fX (-y).\n3. We want to compute the CDF of the ambulance's travel time T , P(T ≤ t) = P(|X -Y | ≤ vt),\nwhere X and Y are the locations of the ambulance and accident (uniform over [0, l]). Since X\nand Y are independent, we know:\n, if 0 ≤ x, y ≤ l\nfX,Y (x, y) =\nl2\n.\n, otherwise\nP(T ≤ t)\n=\nP(|X -Y | ≤vt) = P(-vt ≤ Y -X ≤ vt)\n=\nP(X -vt ≤ Y ≤ X + vt)\nWe can see that P(X -vt ≤ Y ≤ X + vt) corresponds to the integral of the joint density of X\nand Y over the shaded region in the figure below:\ny\nx\nl\nvt\nl\ny = x+vt\nvt\nX,Y\nf\nf (x,y) = 1/l\ny = x-vt\nTherefore, because the joint density is uniform over the entire region, we have:\n, if t < 0\n\nl\nFT (t) = (1/l2) × (Shaded area) =\n2vt - (vt)2\n, if 0 ≤ t <\n.\nl\nl2\nv\n, if t ≥ v\nl\nBy differentiating the CDF, we find the density of T :\n2v - 2v2t\n, if 0 ≤ t ≤ l\nfT (t) =\nl\nl2\nv\n.\n, otherwise\nPage 2 of 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_041F10_tut05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/49db03ccaed812a49db6274c16120c06_MIT6_041F10_tut05.pdf",
      "content": "Z\n(\n(\nMassachusetts Institute of Technology\nDepartment of Electrical Engineering & Computer Science\n6.041/6.431: Probabilistic Systems Analysis\n(Fall 2010)\nTutorial 5\nOctober 14/15, 2010\n1. Let Q be a random variable which is uniformly distributed between 0 and 1. On any given day,\na particular machine is functional with probability Q. Furthermore, given the value of Q, the\nstatus of the machine on different days is independent.\n(a) Find the probability that the machine is functional on a particular day.\n(b) We are told that the machine was functional on m out of the last n days. Find the conditional\nPDF of Q. You may use the identity\nk!(n -k)!\np k(1 -p)n-kdp =\n(n + 1)!\n2. Let X be a random variable with PDF fX . Find the PDF of the random variable Y = |X|\n1/3, if -2 < x ≤ 1,\n(a) when fX (x) =\n0,\notherwise;\n2e-2x , if x > 0,\n(b) when fX (x) =\n0, otherwise;\n(c) for general fX (x).\n3. An ambulance travels back and forth, at a constant specific speed v, along a road of length l.\nWe may model the location of the ambulance at any moment in time to be uniformly distributed\nover the interval (0, l). Also at any moment in time, an accident (not involving the ambulance\nitself) occurs at a point uniformly distributed on the road; that is, the accident's distance from\none of the fixed ends of the road is also uniformly distributed over the interval (0, l). Assume\nthe location of the accident and the location of the ambulance are independent.\nSupposing the ambulance is capable of immediate U-turns, compute the CDF and PDF of the\nambulance's travel time T to the location of the accident.\nPage 1 of 1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability\nFall 2010\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}