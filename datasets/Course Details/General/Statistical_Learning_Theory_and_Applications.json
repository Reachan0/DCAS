{
  "course_name": "Statistical Learning Theory and Applications",
  "course_description": "Focuses on the problem of supervised learning from the perspective of modern statistical learning theory starting with the theory of multivariate function approximation from sparse data. Develops basic tools such as Regularization including Support Vector Machines for regression and classification. Derives generalization bounds using both stability and VC theory. Discusses topics such as boosting and feature selection. Examines applications in several areas: computer vision, computer graphics, text classification and bioinformatics. Final projects and hands-on applications and exercises are planned, paralleling the rapidly increasing practical uses of the techniques described in the subject.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Mathematics",
    "Probability and Statistics",
    "Science",
    "Cognitive Science",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Mathematics",
    "Probability and Statistics",
    "Science",
    "Cognitive Science"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCourse Description\n\nWe introduce and motivate the main theme of the course, setting the problem of learning from examples as the problem of approximating a multivariate function from sparse data. We present an overview of the theoretical part of the course and sketch the connection between classical Regularization Theory and its algorithms - including Support Vector Machines - and Learning Theory, the two cornerstones of the course. We mention theoretical developments during the last few months that provide a new perspective on the foundations of the theory. We briefly describe several different applications ranging from vision to computer graphics, to finance and neuroscience.\n\nPrerequisites\n\n18.02, 9.641, 6.893 or permission of instructor. In practice, a substantial level of mathematical maturity is necessary. Familiarity with probability and functional analysis will be very helpful. We try to keep the mathematical prerequisites to a minimum, but we will introduce complicated material at a fast pace.\n\nGrading\n\nThere will be two problem sets, a MATLAB\n(r)\nassignment, and a final project. To receive credit, you must attend regularly, and put in effort on all problem sets and the project.\n\nProblem Sets\n\nSee the\nassignments\npage for the two problem sets.\n\nProjects\n\nSome of the most promising projects:\n\nHypothesis testing with small sets\n\nConnection between MED and regularization\n\nFeature selection for SVMs theory and experiments\n\nBayes classification rule and SVMs\n\nIOHMMs evaluation of HMMs for classification vs. direct classification\n\nReusing the test set datamining bounds\n\nLarge-scale nonlinear least square regularization\n\nViewbased classification\n\nLocal vs. global classifiers experiments and theory\n\nRKHS invariance to measure historical math\n\nConcentration experiments (dot product vs. square distance)\n\nDecorrelating classifiers: experiments about generalization using a tree of stumps\n\nKernel synthesis and selection\n\nBayesian interpretation of regularization and in particular of SVMs\n\nHistory of induction from Kant to Popper and current state\n\nBayesian Priorhood\n\nResources\n\nThe Center for Biological and Computational Learning\n(CBCL) at MIT was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial, and is the gateway to understanding how the human brain works and to making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning. CBCL is based in the\nDepartment of Brain and Cognitive Sciences\nat MIT and is associated with the\nMcGovern Institute for Brain Research\nand with the\nArtificial Intelligence Laboratory\nat MIT.",
  "files": [
    {
      "category": "Resource",
      "title": "second_set.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/9ba5285ed3557d31aaecaaf2dbb18949_second_set.pdf",
      "content": "l\n\nl\n\nl\nl\nl\nl\n9.520 Problem set 2\nPr. 2.1 Consider the following one dimensional RBF interpolation (ie λ = 0 in the regularization\nnetwork formulation) scheme:\nf (x) =\nciK(x - xi ),\n(1)\ni=1\nwith K(x) = |x| and l = 3.\n1. Compute the coefficients ci, i = 1, 2, 3, in such a way that the following interpolation\nconditions are satisfied:\nf (0) = 1 ,\nf\n\n(3) = 4 ,\nf\n\n(4) = 3\nand draw the corresponding curve.\n2. Show that, on the interval [0, 4], the Radial Basis Functions expansion (1) can be also\nwritten in the form\nf (x) =\nyibi (x)\ni=1\nwhere the yi are the values to be interpolated (y1 = 1, y2 = 4, and y3 = 3). Derive\nand draw the explicit form for the dual kernels bi(x).\nPr. 2.2 Consider the following variational problem:\n1 xi\ninf fˆ(ω)fˆ(ω)\nmin\nf (t)dt - F (xi)\n+ λ\n,\n(2)\nˆ\nf ∈H l i=1\n-inf\n-inf\nK(ω)\nˆ\nˆ\nwhere f (ω) is the Fourier transform of f (x), K(ω) is the Fourier transform of the kernel,\nand F (xi) is the empirical cummulative distribution function (cdf). Assume the kernel to\nbe a Gaussian, K(x, y) = √ 1 e-(x-y)2 /2σ2 .\n2πσ\nShow that the solution of the above problem has\nf (x) =\nK(x, xi).\ni=1\nCompare this solution to the density estimator used in the Parzen's windows algorithm.\nPr. 2.3 Prove that the solution to equation (2) has stability of order 1\nl . Is this always the case\nfor the above variational problem or are further restrictions required to get stability of\norder 1 ?\nPr 2.4 Let Sn be the set of all the hyperplanes in IR\nn .\n\n1. Show that S2 separates any three points not lying on the same line in IR\n2 in all possible\nways;\n2. Show that S2 cannot separate any four points of IR\n2 in all possible ways.\n3. What is the minimal number of points in IR\n100000 that cannot be separated in all\npossible ways by S100000 ?\nPr 2.5 Find the VC dimension of conics for points in the plane. Is there any difference if you\nrestrict the set to one type of conics only (ellipses, parabolae, and hyperbolae)? What\nhappens in the case of degenerate conics?\nPr 2.6 Assuming that the generalization error has the form\nnβ\nE(n, l) ≤\n+\nnα\nl\ndetermine the optimal number of parameters, n, as a function of the number of examples,\nl, and estimate the best possible rate of convergence."
    },
    {
      "category": "Resource",
      "title": "problem_set1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/f690f43067e5398f97183b255f094c0e_problem_set1.pdf",
      "content": "`\nX\n`\nX\nX\nP\nX\nX\nZ\n9.520 Problem set 1\nPr. 1.1 Reproducing Kernel Hilbert Spaces\n1. An RKHS can be defined via a kernel K and has the following reproducing property:\nFt[f ] = hK(t, ·), f (·)iK = f (t).\nGiven two functions\nf (x)\n=\naiK(x, xi)\ni=1\ng(x)\n=\nbiK(x, xi)\ni=1\nwhat is hf, giK , ||f ||2\nK\nK , and ||g||2 ?\n2. Given Mercer's theorem\ninf\nK(s, t) =\nλnφn(s)φn (t),\nn=1\nwhere λn ≥ 0 and the following definition of a RKHS norm for functions f (x) =\nn cnφn(x)\ninf\nX c2\nn\n||f ||2 =\nn=1 λn\nk\nwhere φn(x) are the eigenfunctions of the integral operator defined by the kernel.\nProve that the reproducing property holds.\nProve that one gets the same form for the quantities hf, giK , ||f ||2\nK\nK , and ||g||2 as\nobtained in part 1 where\ninf\nf (·)\n=\ncnφn(·)\nn=1\ninf\ng(·)\n=\ndnφn(·)\nn=1\nand compute the relation between the ai and the cn.\nPr. 1.2 The discrete counterpart in IR\nn of the eigenfunction equation\nK(x, y)v(y)dμ(y) = σv(x),\nwhere K(x, y) is a positive definite function and μ a suitable measure, is\nKP v = σv\n(1)\nwith K a positive definite matrix and P a diagonal positive definite matrix (basically a\nweighting on each data point).\n\nξ\nX\n`\nX\nX\n`\n`\n`\nX\n`\nX\n`\nX\n1. Show that there exists a solution to equation (1) of n linearly independent vectors vi\nand corresponding strictly positive values σi . (Hint: Consider the matrix P 1/2 KP 1/2...)\n2. Let f ∈ IR\nn be represented as\nn\nf =\nai vi\ni=1\nwith\nai = f\n> P vi.\nProve that the norm of f\nn\nX a2\n||f ||2 =\ni=1 σ\ni\ni\nk\nis independent of P . What can you conclude about the dependence on the measure\nP of the norm of f in the RKHS?\nPr 1.3 Given a training set of points not necessarily linearly separable consider the SVM ob\ntained by minimizing\n1 X\nλkwk\n2 +\nξi\n2 ,\n2 i=1\nλ > 0, with respect to w, b, and ξ subject to the constraints\nyi(w · xi + b) ≥ 1 - ξi,\ni = 1, ..., `.\n1. Derive the dual formulation and show that the associated QP is equivalent to the\nQP discussed in class in the linearly separable case but with the Kernel matrix with\nentries defined as\nKij = λyiyj xi · xi + δij .\n2. Set b = 0 and show that if the ` inequalities become equalities the learning scheme\nis Regularized Least-Squares Classification. Derive the associated linear system of\nequations and compare the matrix which has to be inverted with the Kernel matrix\nabove.\nPr 1.4 For the SVM classification problem we have the following optimality conditions\ncj K(xi, xj) -\nyiαj K(xi, xj) = 0\ni = 1, . . . , `\nj=1\nj=1\nαiyi = 0\ni=1\nC - αi - ζi = 0\ni = 1, . . . , `\nyi(\nyj αj K(xi, xj) + b) - 1 + ξi ≥ 0\ni = 1, . . . , `\nj=1\nαi[yi(\nyj αj K(xi, xj) + b) - 1 + ξi] = 0\ni = 1, . . . , `\nj=1\nζiξi = 0\ni = 1, . . . , `\nξi, αi, ζi ≥ 0\ni = 1, . . . , `.\n\n`\nX\nX\nX\nX\nX\nDerive the following \"reduced\" optimality conditions:\nαi = 0 \" ⇐⇒ \" yif (xi) ≥ 1\n0 < αi < C \" ⇐⇒ \" yif (xi) = 1\nαi = C \" ⇐⇒ \" yif (xi) ≤ 1\nand explain why we put quotes around ⇐⇒.\nPr 1.5 We are given the SVM for regression with the -insensitive loss function without a b term\n1 X\nmin\n(|f (x) - y| - )+ + λkf k2\nf ∈H `\nK .\ni=1\n1. Write the primal formulation with slack variables (Hint: You need two slack variables\nfor the error at each point instead of one as was the case in classification).\n2. Derive the Lagrangian (Hint: You need a multiplier for each slack variable).\n3. Derive the dual formulation.\nPr 1.6 Consider any function of one variable that is continuous, symmetric and periodic with\npositive Fourier coefficients. Then K(x) can be expanded in a uniformly convergent Fourier\nseries (all normalization factors set to 1):\ninf\nK(x)\n=\nλncos(nx)\nn=0\ninf\ninf\nK(x - y) = 1 +\nλn sin(nx) sin(ny) +\nλn cos(nx) cos(ny),\nn=1\nn=1\nthe eigenvalues are set to λn = 2-n and the eigenfunctions of K are\n(1, sin(x), cos(x), sin(2x), cos(3x), ..., sin(px), cos(px), ...).\nThe RKHS norm of this function is\ninf\nkf k2\nX hf, cos(nx)i2 + hf, sin(nx)i2\nK ≡\n< inf.\nn=0\nλn\n1. Show that if we write the functions as\ninf\nf (x) =\ncn sin(nx) + dn cos(nx)\nn=1\nthen the coefficients cn and dn are bounded as follows\ncn + d2 < 2-n .\nn\n2. Prove that the space of functions spanned by this RKHS, functions spanned by\ninf\nf (x) =\ncn sin(nx) + dn cos(nx)\nn=1\nwith ||f ||K < inf is compact. (Hint: Look at the proof in Mathcamp 1 of the com\nn + d2\npactness of the Hilbert cube and use the fact that c2\nn < 2-n .).\n\nPr. 1.7 An algorithm is (β, δ) Cross-Validation (CV) stable if for almost all S ∈Z ` (except for\na set of measure δ), the following holds:\n∀i, u ∈Z, |c(fS , u) - c(fSi,u , u)| ≤ β.\nAssuming that our algorithm A has CV-stability β and assuming that the loss function\nc(f, z) (equal to V (f (x), y) ≥ 0) non-negative and bounded above my M , show that\n|IES D[fS ]| ≤ β + Mδ.\nHere D[fS ] is the defect defined in lectures (empirical error - expected error). Note that\nbounding the expectation of the defect was one of two things we had to show for application\nof McDiarmid's inequality."
    },
    {
      "category": "Resource",
      "title": "projects2002.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/f25bf2650fd6825838eb2c904a3b2d26_projects2002.pdf",
      "content": "Ideas\nfo\nr\n\nProjects\n\nHyp\nothesis\nT\nesting\nwith\nSmall\nSets\nDradulov\nGiven\npast\nobservations\nfrom\na\ndistribution\np\nredict\nwhether\na\nnew\nsample\ncomes\nfrom\nthe\ndistribution\nExtension\nKSstatistic\nto\nmultidimensional\ndistributions\n\nConnection\nb\net\nw\neen\nMED\nand\nRegula\nrization\nThe\nfollo\nwing\npap\ner\nMaximum\nentrop\ny\ndiscrimina\ntion\nb\ny\nT\nJaakk\nola\nM\nMeila\nand\nT\nJeba\nra\nintro\nduces\na\nw\na\ny\nof\nusing\ngenerative\nmo\ndels\nin\nclassi\ncation\nand\nregression\np\nroblems\nb\ny\nusing\nentrop\ny\nrather\nthan\na\nRKHS\nno\nrm\nas\nthe\nregula\nrization\nfunctional\n\nF\neature\nSelection\nfo\nr\nSVMs\nTheo\nry\nand\nExp\neriments\n\nOften\nonly\na\nsmall\nnumb\ner\nof\na\nla\nrge\nset\nof\ninput\nfea\ntures\nis\nrelevant\nin\na\nclassi\ncation\ntask\nAlgo\nrithms\nexist\nfo\nr\nselecting\nfeatures\nwhen\nthe\nclassi\ner\nis\nan\nSVM\nP\nossible\np\nrojects\nconsist\nof\ndeveloping\na\nb\netter\ntheo\nretical\nunderstanding\nof\nsome\np\nrop\nerties\nof\nthese\nalgo\nrithms\nCho\nosing\nMultiple\nP\na\nrameters\nfo\nr\nSup\np\no\nrt\nV\necto\nr\nMachines\nb\ny\nO\nChap\nelle\nV\nV\napnik\nO\nBousquet\nand\nS\nMukherjee\n\nBa\ny\nes\nClassi\ncation\nRule\nand\nSVMs\nF\no\nr\na\nt\nw\no\nclass\np\nroblems\none\nif\nthe\nclass\nconditional\np\nrobabilities\na\nre\nkno\nw\nthen\none\ncan\nuse\nthe\nfollo\nwing\nrule\nto\nclassify\ny\n\nsign\n\nln P\nc\n\n|x\nP\nc\n-|x -\nln P\nc\n-\nP\nc\n\n.\n\nWhen\nclasses\nhave\nequal\np\nrobabilities\nthe\nSVM\ncan\nb\ne\nplaced\nin\nsuch\na\nframew\no\nrk\nWhat\nhapp\nens\nwhen\nthe\nclasses\na\nre\nnot\nequip\nrobable\n\nOne\npap\ner\nlo\nok\ned\nat\nwill\nb\ne\nLin\nY\nLee\nY\nand\nW\nahba\nG\n\nSupp\no\nrt\nV\necto\nr\nMachines\nfo\nr\nClassi\n\ncation\nin\nNonstanda\nrd\nSituations\n\nTR\n\nMa\nrch\n\nIOHMMs\nEvaluation\nof\nHMMs\nfo\nr\nclassi\ncation\nvs\ndi\nrect\nclassi\ncation\nHMMs\nand\ngenerative\nmo\ndels\noer\nan\nalternative\nto\nclassi\ners\nlik\ne\nSVM\nA\ncompa\nrison\nof\nIOHMMs\nvs\nSVMs\nThe\nrelevant\npap\ner\nis\nY\nBengio\nand\nP\n\nF\nrasconi\nIn\nputOutput\nHMMs\nfo\nr\nSequence\nPro\ncessing\nIEEE\nT\nransactions\non\nNeural\nNet\nw\no\nrks\nvol\n\nno\n\npp\n\nReusing\nthe\nT\nest\nSet\nDatamininig\nBounds\n\np\nrojects\n\nLittlestone\n\nEvgeniou\nHo\nw\nmuch\ndo\nes\none\npa\ny\nwhen\none\nreuses\ndata\nEv\ngeniou\n\nGet\nV\nC\no\nr\ncon\ndence\nb\nounds\non\ndeviation\nb\net\nw\neen\nempirical\nand\nexp\nected\nwhen\ndata\nis\nreused\nOne\ncan\nget\ngeneralization\nb\nounds\nfrom\nonline\nanal\nysis\nin\na\nvery\ninteresting\nw\na\ny\n\nF\no\nr\nclassi\ncation\nthese\nb\nounds\na\nre\nthe\ntightest\na\nround\nCan\nthis\nb\ne\nextended\nand\nho\nw\ngeneral\nis\nthis\ntechnique\n\nThe\nrelevant\npap\ner\nis\nN\nLittlestone\nF\nrom\nonline\nto\nbatch\nlea\nrning\nIn\nPro\nceedings\nof\nthe\nSecond\nAnnual\nW\no\nrkshop\non\nCom\nputational\nLea\nrning\nTheo\nry\n\npages\n\nSan\nMa\nteo\nCA\n\nLa\nrgeScale\nNonlinea\nr\nLeast\nSqua\nre\nRgula\nrization\nGene\nY\n\nViewbased\nclassi\ncation\n\nLo\ncal\nvs\nGlobal\nclassi\ners\nexp\neriments\nand\ntheo\nry\nconjecture\nunlik\ne\nV\napniks\nstatement\na\nre\nlo\ncal\nand\nglobal\nsubsumed\nunder\nthe\nsame\nfo\nrmulation\nV\napnik\ntalks\nab\nout\nlo\ncal\nand\nglobal\nstructures\nThe\nrel\nevant\npap\ner\nis\nLo\ncal\nLea\nrning\nAlgo\nrithms\n\nLeon\nBottou\nVladimir\nV\napnik\nNeural\nComputation\n\nRKHS\ninva\nriance\nto\nmeasure\nhisto\nrical\nmath\n\nConcentration\nexp\neriments\ndot\np\nro\nduct\nvs\nsqua\nre\ndistance\n\nDeco\nrrelating\nclassi\ners\nNiy\nogi\nexp\neriments\nab\nout\ngeneralization\nusing\na\ntree\nof\nstumps\nThe\npap\ner\nto\nlo\nok\nat\nis\nP\n\nNiy\nogi\nJ\nB\nPierrot\nand\nO\nSiohan\nMultiple\nClassi\ners\nb\ny\nConstrained\nMinimiza\ntion\nto\napp\nea\nr\nPro\nceedings\nof\nInternational\nConfer\nence\non\nAcoustics\nSp\neech\nand\nSignal\nPro\ncessing\nIs\ntanbul\nT\nurk\ney\n\nJune\n\nKernel\nsynthesis\nand\nselection\nThe\npap\ners\nto\nlo\nok\nat\na\nre\nP\noggio\nT\nand\nF\nGirosi\nA\nSpa\nrse\nRep\nresentation\nfo\nr\nF\nunction\nApp\nro\nximation\nNeural\nComputation\nV\nol\n\nNo\n\nand\nOn\nOptimizing\nKernel\nAlignment\nNello\nCristianini\nJaz\nKandola\nAndre\nElissee\nJohn\nSha\nw\neT\na\nylo\nr\n\nBa\ny\nesian\nInterp\nretation\nof\nregula\nrization\nand\nin\npa\nrtic\nula\nr\nof\nSVMs\nlimits\ntheo\nrem\nand\nintegral\napp\nro\nxima\ntions\nand\nGaussian\nPro\ncesses\n\nPhylosophy\np\nroject\nhisto\nry\nof\ninduction\nfrom\nKant\nto\nP\nopp\ner\nand\ncurrent\nstate\n\nReligious\np\nroject\nBa\ny\nesian\nPrio\nrho\no\nd"
    },
    {
      "category": "Resource",
      "title": "svmrules.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/129254d09b9866c96c2fed273bbca01c_svmrules.pdf",
      "content": "SVM Rules of Thumb\nDescription\nWe provide some intuition into how to use SVMs. We give advice about\nchoosing the regularization parameter C, the kernel and its parameters, and\nthe size of the subproblem to solve at each step. Solving machine learning\nprobles remains a mix of science and art, but we hopefully provide some\nuseful advice and tips."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/6bae8a4a5055861e2679c40ff95df414_lecture01.pdf",
      "content": "Lecture 1: The Course at a Glance\nTomaso Poggio\n\nDescription\nWe introduce and motivate the main theme of the course, setting the\nproblem of learning from examples as the problem of approximating a\nmultivariate function from sparse data. We present an overview of the\ntheoretical part of the course and sketch the connection between classical\nRegularization Theory and its algorithms -- including Support Vector\nMachines -- and Learning Theory, the two cornerstones of the course. We\nmention theoretical developments during the last few months that provide\na new perspective on the foundations of the theory. We briefly describe\nseveral different applications ranging from vision to computer graphics, to\nfinance and neuroscience.\n\nSuggested Reading\nT. Poggio and S. Smale. The Mathematics of Learning: Dealing with\nData. Notices of the AMS, 2003"
    },
    {
      "category": "Resource",
      "title": "mathcamp1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/be260f77ee2b9ac1b0808223b8bf64e6_mathcamp1.pdf",
      "content": "Math Camp 1: Functional Analysis\nDescription\nWe give a very brief introduction to key concepts in functional analysis.\nRather than being at all comprehensive, we try to indicate what results will\nbe used in 9.520 and present some intuitions about them.\nSuggested Reading\n-\nKolmogorov and Fomin. Introductory Real Analysis. Dover, 1970."
    },
    {
      "category": "Resource",
      "title": "mathcamp1slides.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/e266bc50309ce337c40d6aa3ac1f0cbd_mathcamp1slides.pdf",
      "content": "Math\nCamp\n\nF\nunctional\nanalysis\nSa\ny\nan\nMukherjee\nAlessandro\nV\nerri\n\nAb\nout\nthe\np\nrimer\nGoal\nT\no\nb\nriey\nreview\nconcepts\nin\nfunctional\nanalysis\nthat\nwill\nb\ne\nused\nthroughout\nthe\ncourse∗\nThe\nfollo\nwing\nconcepts\nwill\nb\ne\ndescrib\ned\n\nF\nunction\nspaces\n\nMetric\nspaces\n\nConvergence\n\nMeasure\n\nDense\nsubsets\n∗\nThe\ndenitions\nand\nconcepts\ncome\np\nrima\nrily\nfrom\nIntro\nducto\nry\nReal\nAnalysis\nb\ny\nKolmogo\nrov\nand\nF\nomin\nhighly\nrecommended\n\nSepa\nrable\nspaces\n\nComplete\nmetric\nspaces\n\nCompact\nmetric\nspaces\n\nLinea\nr\nspaces\n\nLinea\nr\nfunctionals\n\nNo\nrms\nand\nsemino\nrms\nof\nlinea\nr\nspaces\n\nConvergence\nrevisited\n\nEuclidean\nspaces\n\nOrthogonalit\ny\nand\nbases\n\nHilb\nert\nspaces\n\nDelta\nfunctions\n\nF\nourier\ntransfo\nrm\n\nF\nunctional\nderivatives\n\nExp\nectations\n\nLa\nw\nof\nla\nrge\nnumb\ners\n\nF\nunction\nspace\nA\nfunction\nspace\nis\na\nspace\nmade\nof\nfunctions\nEach\nfunction\nin\nthe\nspace\ncan\nb\ne\nthought\nof\nas\na\np\noint\nEx\namples\nC\na, b\nthe\nset\nof\nall\nrealvalued\ncontinuous\nfunctions\nin\nthe\ninterval\na, b\nL\n\na, b\nthe\nset\nof\nall\nrealvalued\nfunctions\nwhose\nab\nsolute\nvalue\nis\nintegrable\nin\nthe\ninterval\na, b\nL\n\na, b\nthe\nset\nof\nall\nrealvalued\nfunctions\nsqua\nre\ninte\ngrable\nin\nthe\ninterval\na, b\nNote\nthat\nthe\nfunctions\nin\n\nand\n\na\nre\nnot\nnecessa\nrily\ncontinuous\n\nMetric\nspace\nBy\na\nmetric\nspace\nis\nmeant\na\npair\nX, ρ\nconsisting\nof\na\nspace X\nand\na\ndistance ρ\na\nsinglevalued\nnonnegative\nreal\nfunction ρx, y\n\ndened\nfo\nr\nall x, y ∈X\nwhich\nhas\nthe\nfollo\nwing\nthree\np\nrop\nerties\nρx, y\n\ni x\ny\n\nρx, y\n\nρy, x\n\nT\nriangle\ninequalit\ny ρx, z\n≤ρx, y\n\nρy, z\n\nExamples\n\nThe\nset\nof\nall\nreal\nnumb\ners\nwith\ndistance\nρx, y\n\n|x -y|\nis\nthe\nmetric\nspace\nI\nR\n\nThe\nset\nof\nall\no\nrdered ntuples\nx\n\nx\n, ..., xn\n\nof\nreal\nnumb\ners\nwith\ndistance\nρx, y\n\nn\n\ni\nxi -yi\n\nis\nthe\nmetric\nspace\nI\nRn\n\nThe\nset\nof\nall\nfunctions\nsatisfying\nthe\ncriteria\n\nf\n\nxdx < inf\nwith\ndistance\nρf\n\nx, f\n\nx\n\nf\n\nx -f\n\nx\ndx\nis\nthe\nmetric\nspace L\n\nI\nR\n\nThe\nset\nof\nall\np\nrobabilit\ny\ndensities\nwith\nKullbackLeibler\ndivergence\nρp\n\nx, p\n\nx\n\nln p\n\nx\np\n\nxp\n\nxdx\nis\nnot\na\nmetric\nspace\nThe\ndivergence\nis\nnot\nsymmetric\nρp\n\nx, p\n\nx ρp\n\nx, p\n\nx.\n\nConvergence\nAn\nop\nenclosed\nsphere\nin\na\nmetric\nspace S\nis\nthe\nset\nof\np\noints x ∈\nI\nR\nfo\nr\nwhich\nρx\n, x < r\nopen\nρx\n, x ≤r\nclosed.\nAn\nop\nen\nsphere\nof\nradius ε\nwith\ncenter x\n\nwill\nb\ne\ncalled\nan\nεneighb\no\nrho\no\nd\nof x\n\ndenoted Oε\nx\n\nA\nsequence\nof\np\noints {xn}\nx\n, x\n, ..., xn, ...\nin\na\nmetric\nspace S\nconverges\nto\na\np\noint x ∈S\nif\nevery\nneighb\no\nrho\no\nd\nOε\nx\nof x\ncontains\nall\np\noints xn\nsta\nrting\nfrom\na\ncertain\ninteger\nGiven\nany ε >\n\nthere\nis\nan\ninteger Nε\nsuch\nthat\nOε\nx\ncontains\nall\np\noints xn\nwith n > Nε\n{xn}\nconverges\nto x\nilim\nn→infρx, xn\n\n.\n\nMeasure\nThroughout\nthe\ncourse\nw\ne\nwill\nsee\nintegrals\nof\nthe\nfo\nrm\n\nV\nf\nx, y\ndν\nx →\n\nV\nf\nx, y\npxdx\nν\nx\nis\nthe\nmeasure\nThe\nconcept\nof\nthe\nmeasure ν\nE\n\nof\na\nset E\nis\na\nnatural\nextension\nof\nthe\nconcept\n\nThe\nlength l\n\nof\na\nline\nsegment\n\nThe\nvolume V\nG\nof\na\nspace G\n\nThe\nintegral\nof\na\nnonnegative\nfunction\nof\na\nregion\nin\nspace\n\nLeb\nesgue\nmeasure\nLet f\nb\ne\na ν\nmeasurable\nfunction\nit\nhas\nnite\nmeasure\ntaking\nno\nmo\nre\nthan\ncountably\nmany\ndistinct\nvalues\ny\n, y\n, ..., yn, ...\nThen\nb\ny\nthe\nLeb\nesgue\nintegral\nof f\nover\nthe\nset A\ndenoted\n\nA f\nx dν,\nw\ne\nmean\nthe\nquantit\ny\n\nn\nynν\nAn\n\nwhere\nAn\n{x\nx ∈A, f\nx\nyn},\np\nrovided\nthe\nseries\nis\nabsolutely\nconvergent\nThe\nmeasure\nν\nis\nthe\nLeb\nesgue\nmeasure\n\nLeb\nesgue\nintegral\nW\ne\ncan\ncompute\nthe\nintegral\n\nf\nxdx\nb\ny\nadding\nup\nthe\na\nrea\nunder\nthe\nred\nrectangles\n-10\n-8\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\nf(x)\n\nRiemann\nintegral\nThe\nmo\nre\ntradition\nfo\nrm\nof\nthe\nintegral\nis\nthe\nRiemann\nintegral\nThe\nintuition\nis\nthat\nof\nlimit\nof\nan\ninnite\nsum\nof\ninnitesimally\nsmall\nrectangles\n\nA f\nxdx\n\nn\nf\nxn\nx.\nIntegrals\nin\nthe\nRiemann\nsense\nrequire\ncontinuous\no\nr\npiece\nwise\ncontinuous\nfunctions\nthe\nLeb\nesgue\nfrom\nsho\nwn\np\nre\nviously\nrelaxes\nthis\nThus\nthe\nintegral\n\nf\nxdx\nwith f\n\n,\n→\nI\nR\ndened\nas\nf\n\nif t\nis\nrational\n\notherwise\ndo\nes\nnot\nexist\nin\nthe\nRiemann\nsense\n\nLeb\nesgueStieltjes\nintegral\nLet F\nb\ne\na\nnondecreasing\nfunction\ndened\non\na\nclosed\ninterval\na, b\nand\nsupp\nose F\nis\ncontinuous\nfrom\nthe\nleft\nat\nevery\np\noint\na, b\nF\nis\ncalled\nthe\ngenerating\nfunction\nof\nthe\nLeb\nesgueStieltjes\nmeasure νF\n\nThe\nLeb\nesgueStieltjes\nintegral\nof\na\nfunction f\nis\ndenoted\nb\ny\nb\na f\nx dF\nx\nwhich\nis\nthe\nLeb\nesgue\nintegral\n\na,b f\nx dνF .\nAn\nexample\nof dνF\nis\na\np\nrobabilit\ny\ndensit\ny pxdx\nThen νF\nw\nould\nco\nrresp\nond\nto\nthe\ncumulative\ndistribution\nfunction\n\nDense\nLet A\nand B\nb\ne\nsubspaces\nof\na\nmetric\nspace\nI\nR\nA\nis\nsaid\nto\nb\ne\ndense\nin B\nif\n\nA ⊂B\n\nA\nis\nthe\nclosure\nof\nthe\nsubset\nA\nIn\npa\nrticula\nr A\nis\nsaid\nto\nb\ne\neverywhere\ndense\nin\nI\nR\nif\n\nA\nR\nA\np\noint x ∈\nI\nR\nis\ncalled\na\ncontact\np\noint\nof\na\nset A ∈\nI\nR\nif\nevery\nneighb\no\nrho\no\nd\nof x\ncontains\nat\nleast\non\np\noint\nof A\nThe\nset\nof\nall\ncontact\np\noints\nof\na\nset A\ndenoted\nb\ny\n\nA\nis\ncalled\nthe\nclosure\nof A\n\nExamples\n\nThe\nset\nof\nall\nrational\np\noints\nis\ndense\nin\nthe\nreal\nline\n\nThe\nset\nof\nall\np\nolynomials\nwith\nrational\nco\necients\nis\ndense\nin C\na, b\n\nLet K\nb\ne\na\np\nositive\ndenite\nRadial\nBasis\nF\nunction\nthen\nthe\nfunctions\nf\nx\n\nn\n\ni\nciK\nx -xi\n\nis\ndense\nin L\n\nNote\nA\nhyp\nothesis\nspace\nthat\nis\ndense\nin L\n\nis\na\ndesired\np\nrop\nert\ny\nof\nany\napp\nro\nximation\nscheme\n\nSepa\nrable\nA\nmetric\nspace\nis\nsaid\nto\nb\ne\nsepa\nrable\nif\nit\nhas\na\ncountable\neverywhere\ndense\nsubset\nExamples\n\nThe\nspaces\nI\nR\n\nI\nRn\nL\n\na, b\nand C\na, b\na\nre\nall\nsepa\nra\nble\n\nThe\nset\nof\nreal\nnumb\ners\nis\nsepa\nrable\nsince\nthe\nset\nof\nrational\nnumb\ners\nis\na\ncountable\nsubset\nof\nthe\nreals\nand\nthe\nset\nof\nrationals\nis\nis\neverywhere\ndense\n\nCompleteness\nA\nsequence\nof\nfunctions fn\nis\nfundamental\nif ∀ε >\n∃Nε\nsuch\nthat\n∀n\nand m > Nε,\nρfn, fm\n< ε.\nA\nmetric\nspace\nis\ncomplete\nif\nall\nfundamental\nsequences\nconverge\nto\na\np\noint\nin\nthe\nspace\nC\nL\n\nand L\n\na\nre\ncomplete\nThat C\n\nis\nnot\ncomplete\ninstead\ncan\nb\ne\nseen\nthrough\na\ncounterexample\n\nIncompleteness\nof C\n\nConsider\nthe\nsequence\nof\nfunctions\nn\n\n,\n, ...\nφn\nt\n\n-\nif -\n≤t < -/n\nnt\nif -\n/n ≤t <\n/n\n\nif\n/n ≤t ≤\n\nand\nassume\nthat φn\nconverges\nto\na\ncontinuous\nfunction φ\nin\nthe\nmetric\nof C\n\nLet\nf\nt\n\n-\nif -\n≤t <\n\nif\n≤t ≤\n\nIncompleteness\nof C\n\ncont\nClea\nrly\n\nf\nt -φt\ndt\n\n/\n≤\n\nf\nt -φn\nt\ndt\n\n/\n\nφn\nt -φt\ndt\n\n/\n.\nNo\nw\nthe\nlhs\nterm\nis\nstrictly\np\nositive\nb\necause f\nt\nis\nnot\ncontinuous\nwhile\nfo\nr n →inf\nw\ne\nhave\n\nf\nt -φn\nt\ndt →\n.\nTherefo\nre\ncontra\nry\nto\nwhat\nassumed φn\ncannot\nconverge\nto φ\nin\nthe\nmetric\nof C\n\nCompletion\nof\na\nmetric\nspace\nGiven\na\nmetric\nspace\nI\nR\nwith\nclosure\n\nI\nR\na\ncomplete\nmetric\nspace\nI\nR∗\nis\ncalled\na\ncompletion\nof\nI\nR\nif\nI\nR ⊂\nI\nR∗\nand\n\nI\nR\n\nI\nR∗\n\nExamples\n\nThe\nspace\nof\nreal\nnumb\ners\nis\nthe\ncompletion\nof\nthe\nspace\nof\nrational\nnumb\ners\n\nLet K\nb\ne\na\np\nositive\ndenite\nRadial\nBasis\nF\nunction\nthen\nL\n\nis\nthe\ncompletion\nthe\nspace\nof\nfunctions\nf\nx\n\nn\n\ni\nciK\nx -xi\n.\n\nCompact\nspaces\nA\nmetric\nspace\nis\ncompact\niit\nis\ntotally\nb\nounded\nand\ncomplete\nLet\nI\nR\nb\ne\na\nmetric\nspace\nand ε\nany\np\nositive\nnumb\ner\nThen\na\nset A ⊂\nI\nR\nis\nsaid\nto\nb\ne\nan εnet\nfo\nr\na\nset M ⊂\nI\nR\nif\nfo\nr\nevery x ∈M\n\nthere\nis\nat\nleast\none\np\noint a ∈A\nsuch\nthat\nρx, a < ε\nGiven\na\nmetric\nspace\nI\nR\nand\na\nsubset M ⊂\nI\nR\nsupp\nose M\nhas\na\nnite εnet\nfo\nr\nevery ε >\n\nThen M\nis\nsaid\nto\nb\ne\ntotally\nb\nounded\nA\ncompact\nspace\nhas\na\nnite εnet\nfo\nr\nall ε >\n\nExamples\n\nIn\nEuclidean\nnspace\nI\nRn\n\ntotal\nb\noundedness\nis\nequiv\nalent\nto\nb\noundedness\nIf M ⊂\nI\nR\nis\nb\nounded\nthen M\nis\ncontained\nin\nsome\nhyp\nercub\ne Q\nW\ne\ncan\npa\nrtition\nthis\nhyp\nercub\ne\ninto\nsmaller\nhyp\nercub\nes\nwith\nsides\nof\nlength ε\nThe\nvertices\nof\nthe\nlittle\ncub\nes\nfrom\na\nnite\n√nε/net\nof Q\n\nThis\nis\nnot\ntrue\nfo\nr\ninnitedimensional\nspaces\nThe\nunit\nsphere\n\nin l\n\nwith\nconstraint\ninf\n\nn\nx\nn\n\n,\nis\nb\nounded\nbut\nnot\ntotally\nb\nounded\nConsider\nthe\np\noints\ne\n\n,\n,\n, ... , e\n\n,\n,\n,\n, ... , ...,\n\nwhere\nthe nth\nco\no\nrdinate\nof en\nis\none\nand\nall\nothers\na\nre\nzero\nThese\np\noints\nlie\non\n\nbut\nthe\ndistance\nb\net\nw\neen\nany\nt\nw\no\nis\n√\n\nSo\n\ncannot\nhave\na\nnite εnet\nwith\nε <\n√\n/\n\nInnitedimensional\nspaces\nma\nyb\ne\ntotally\nb\nounded\nLet\n\nb\ne\nthe\nset\nof\np\noints x\n\nx\n, ..., xn, ..\nin l\n\nsatisfying\nthe\ninequalities\n|x\n| <\n, |x\n| <\n\n, ..., |xn| <\n\nn-, ...\nThe\nset\n\ncalled\nthe\nHilb\nert\ncub\ne\nis\nan\nexample\nof\nan\ninnitedimensional\ntotally\nb\nounded\nset\nGiven\nany\nε >\n\ncho\nose n\nsuch\nthat\n\nn < ε\n,\n\nand\nwith\neach\np\noint\nx\n\nx\n, ..., xn, ..\nis\n\nasso\nciate\nthe\np\noint\nx∗\n\nx\n, ..., xn,\n,\n, ... .\n\nThen\nρx, x∗\n\ninf\n\nk\nn\nx\n\nk <\n\ninf\n\nk\nn\n\nk <\n\nn- < ε\n.\nThe\nset\n∗\nof\nall\np\noints\nin\n\nthat\nsatisfy\n\nis\ntotally\nb\nounded\nsince\nit\nis\na\nb\nounded\nset\nin\nnspace\n\nThe\nRKHS\ninduced\nb\ny\na\nk\nernel K\nwith\nan\ninnite\nnum\nb\ner\nof\np\nositive\neigenvalues\nthat\ndeca\ny\nexp\nonentially\nis\ncompact\nIn\nthis\ncase\nour\nvecto\nr x\n\nx\n, ..., xn, ..\ncan\n\nb\ne\nwritten\nin\nterms\nof\nits\nbasis\nfunctions\nthe\neigenvec\nto\nrs\nof K\n\nNo\nw\nfo\nr\nthe\nRKHS\nno\nrm\nto\nb\ne\nb\nounded\n|x\n| < μ\n, |x\n| < μ\n, ..., |xn| < μn, ...\nand\nw\ne\nkno\nw\nthat μn\nO\nn-α\n\nSo\nw\ne\nhave\nthe\ncase\nanalogous\nto\nthe\nHilb\nert\ncub\ne\nand\nw\ne\ncan\nintro\nduce\na\np\noint\nx∗\n\nx\n, ..., xn,\n,\n, ...\n\nin\na\nb\nounded\nnspace\nwhich\ncan\nb\ne\nmade\na\nrbitra\nrily\nclose\nto x\n\nCompactness\nand\ncontinuit\ny\nA\nfamily\n\nof\nfunctions φ\ndened\non\na\nclosed\ninterval\na, b\nis\nsaid\nto\nb\ne\nunifo\nrmly\nb\nounded\nif\nfo\nr K >\n\n|φ\nx| < K\nfo\nr\nall x ∈\na, b\nand\nall φ ∈\n\nA\nfamily\n\nof\nfunctions φ\nis\nequicontinuous\nof\nfo\nr\nany\ngiven\nε >\n\nthere\nexists δ >\n\nsuch\nthat |x -y| < δ\nimplies\n|φ\nx -φ\ny\n| < ε\nfo\nr\nall x, y ∈\na, b\nand\nall φ ∈\n\nArzelas\ntheo\nrem\nA\nnecessa\nry\nand\nsucient\ncondition\nfo\nr\na\nfamily\n\nof\ncontinuous\nfunctions\ndened\non\na\nclosed\ninterval\na, b\nto\nb\ne\nrelatively\ncompact\nin C\na, b\nis\nthat\n\nis\nunifo\nrmly\nb\nounded\nand\nequicontinuous\n\nLinea\nr\nspace\nA\nset L\nof\nelements x, y, z, ...\nis\na\nlinea\nr\nspace\nif\nthe\nfol\nlo\nwing\nthree\naxioms\na\nre\nsatised\n\nAny\nt\nw\no\nelements x, y ∈L\nuniquely\ndetermine\na\nthird\nelement\nin x\ny ∈L\ncalled\nthe\nsum\nof x\nand y\nsuch\nthat\na x\ny\ny\nx\ncommutativit\ny\nb\nx\ny\n\nz\nx\ny\nz\n\nasso\nciativit\ny\nc\nAn\nelement\n∈L\nexists\nfo\nr\nwhich x\n\nx\nfo\nr\nall\nx ∈L\nd\nF\no\nr\nevery x ∈L\nthere\nexists\nan\nelement -x ∈L\nwith\nthe\np\nrop\nert\ny x\n-x\n\nAny\nnumb\ner α\nand\nany\nelement x ∈L\nuniquely\ndeter\nmine\nan\nelement αx ∈L\ncalled\nthe\np\nro\nduct\nsuch\nthat\na αβx\nβ\nαx\nb\nx\nx\n\nAddition\nand\nmultiplication\nfollo\nw\nt\nw\no\ndistributive\nla\nws\naα\nβ\nx\nαx\nβx\nbαx\ny\n\nαx\nαy\n\nLinea\nr\nfunctional\nA\nfunctional F\n\nis\na\nfunction\nthat\nmaps\nanother\nfunction\nto\na\nrealvalue\nF\nf →\nI\nR.\nA\nlinea\nr\nfunctional\ndened\non\na\nlinea\nr\nspace L\n\nsatises\nthe\nfollo\nwing\nt\nw\no\np\nrop\nerties\n\nAdditive F\nf\ng\n\nF\nf\n\nF\ng\n\nfo\nr\nall f, g ∈L\n\nHomogeneous F\nαf\n\nαF\nf\n\nExamples\n\nLet\nI\nRn\nb\ne\na\nreal\nnspace\nwith\nelements x\n\nx\n, ..., xn\n\nand a\n\na\n, ..., an\n\nb\ne\na\nxed\nelement\nin\nI\nRn\n\nThen\nF\nx\n\nn\n\ni\naixi\nis\na\nlinea\nr\nfunctional\n\nThe\nintegral\nF\nf\nx\n\nb\na f\nxpxdx\nis\na\nlinea\nr\nfunctional\n\nEvaluation\nfunctional\nanother\nlinea\nr\nfunctional\nis\nthe\n\nDirac\ndelta\nfunction\nδt\nf\n·\nf\nt.\nWhich\ncan\nb\ne\nwritten\nδt\nf\n·\n\nb\na f\nxδ\nx -tdx.\n\nEvaluation\nfunctional\na\np\nositive\ndenite\nk\nernel\nin\na\nRKHS\nFt\nf\n·\n\nKt, f\n\nf\nt.\nThis\nis\nsimply\nthe\nrep\nro\nducing\np\nrop\nert\ny\nof\nthe\nRKHS\n\nNo\nrmed\nspace\nA\nno\nrmed\nspace\nis\na\nlinea\nr\nvecto\nr\nspace N\nin\nwhich\na\nno\nrm\nis\ndened\nA\nnonnegative\nfunction ∥· ∥\nis\na\nno\nrm\ni∀f, g ∈N\nand α ∈\nI\nR\n∥f∥≥\n\nand ∥f∥\n\ni f\n\n∥f\ng∥≤∥f∥\n∥g∥\n∥αf∥\n|α| ∥f∥\nNote\nif\nall\nconditions\na\nre\nsatised\nexcept ∥f∥\n\ni f\n\nthen\nthe\nspace\nhas\na\nsemino\nrm\ninstead\nof\na\nno\nrm\n\nMeasuring\ndistances\nin\na\nno\nrmed\nspace\nIn\na\nno\nrmed\nspace N\n\nthe\ndistance ρ\nb\net\nw\neen f\nand g\n\no\nr\na\nmetric\ncan\nb\ne\ndened\nas\nρf, g\n\n∥g -f∥.\nNote\nthat ∀f, g, h ∈N\nρf, g\n\ni f\ng\n\nρf, g\n\nρg, f\n\nρf, h ≤ρf, g\n\nρg, h\n\nExample\ncontinuous\nfunctions\nA\nno\nrm\nin C\na, b\ncan\nb\ne\nestablished\nb\ny\ndening\n∥f∥\n\nmax\na≤t≤b |f\nt|.\nThe\ndistance\nb\net\nw\neen\nt\nw\no\nfunctions\nis\nthen\nmeasured\nas\nρf, g\n\nmax\na≤t≤b |g\nt -f\nt|.\nWith\nthis\nmetric C\na, b\nis\ndenoted\nas C\n\nExamples\ncont\nA\nno\nrm\nin L\n\na, b\ncan\nb\ne\nestablished\nb\ny\ndening\n∥f∥\n\nb\na |f\nt|dt.\nThe\ndistance\nb\net\nw\neen\nt\nw\no\nfunctions\nis\nthen\nmeasured\nas\nρf, g\n\nb\na |g\nt -f\nt|dt.\nWith\nthis\nmetric L\n\na, b\nis\ndenoted\nas L\n\nExamples\ncont\nA\nno\nrm\nin C\n\na, b\nand L\n\na, b\ncan\nb\ne\nestablished\nb\ny\ndening\n∥f∥\n\nb\na f\n\ntdt\n\n/\n.\nThe\ndistance\nb\net\nw\neen\nt\nw\no\nfunctions\nno\nw\nb\necomes\nρf, g\n\nb\na\ng\nt -f\nt\ndt\n\n/\n.\nWith\nthis\nmetric C\n\na, b\nand L\n\na, b\na\nre\ndenoted\nas C\n\nand L\n\nresp\nectively\n\nConvergence\nrevisited\nA\nsequence\nof\nfunctions fn\nconverge\nto\na\nfunction f\nalmost\neverywhere\ni\nlim\nn→inffn\nx\nf\nx\nA\nsequence\nof\nfunctions fn\nconverge\nto\na\nfunction f\nin\nmeasure\ni ∀ε >\n\nlim\nn→infμ{x\n|fn\nx -f\nx| ≥ε}\n\n.\nA\nsequence\nof\nfunctions fn\nconverge\nto\na\nfunction f\nunifo\nrmly\ni\nlim\nn→inf\nsup\nx\nfn\nx -f\nx\n\nRelationship\nb\net\nw\neen\ndierent\nt\nyp\nes\nof\nconvergence\nIn\nthe\ncase\nof\nb\nounded\nintervals\nunifo\nrm\nconvergence\nC\n\nimplies\n-\nconvergence\nin\nthe\nquadratic\nmean\nL\n\nwhich\nimplies\nconvergence\nin\nthe\nmean\nL\n\nwhich\nimplies\nconver\ngence\nin\nmeasure\n-\nalmost\neverywhere\nconvergence\nwhich\nimplies\nconver\ngence\nin\nmeasure\n\nRelationship\nb\net\nw\neen\ndierent\nt\nyp\nes\nof\nconvergence\nThat\nunifo\nrm\nconvergence\nimplies\nall\nother\nt\nyp\ne\nof\ncon\nvergence\nis\nclea\nr\nConsider L\n\nover\na\nb\nounded\ninterval\nof\nwidth A\nKeeping\nin\nmind\nthat\nthe\nfunction g\n\nb\nelongs\nto L\n\nand\nthat\n∥g∥L\n\nA\nconvergence\nin\nthe\nquadratic\nmean\nimplies\ncon\nvergence\nin\nthe\nmean\nb\necause\nfo\nr\nevery\nfunction f ∈L\n\nw\ne\nhave\n∥f∥L\n\nA |f|dx\n\nA |f| ·\ndx ≤∥f∥L\n∥∥L\n\nA∥f∥L\n\nand\nhence\nthat f ∈L\n\nAny\nconvergence\nimplies\nconvergence\nin\nmeasure\nConvergence\nin\nmeasure\nis\nobtained\nb\ny\nconvergence\nin\nthe\nmean\nthrough\nCheb\nyshevs\ninequalit\ny\nF\no\nr\nany\nreal\nrandom\nva\nriable X\nand t >\n\nP\n|X| ≥t ≤E\nX\n/t\n\nThe\np\nro\nof\nthat\nalmost\neverywhere\nconvergence\nimplies\nconvergence\nin\nmeasure\nis\nsomewhat\nmo\nre\ncomplicated\n\nAlmost\neverywhere\nconvergence\ndo\nes\nnot\nimply\nconvergence\nin\nthe\nquadratic\nmean\nOver\nthe\ninterval\n,\n\nlet fn\nb\ne\nfn\n\nn\nx ∈\n,\n/n\n\notherwise\nClea\nrly fn →\n\nfo\nr\nall x ∈\n,\n\nNote\nthat\neach fn\nis\nnot\na\ncontinuous\nfunction\nand\nthat\nthe\nconvergence\nis\nnot\nunifo\nrm\nthe\ncloser\nthe x\nto\n\nthe\nla\nrger n\nmust\nb\ne\nfo\nr\nfn\nx\n\nHo\nw\never\n\n|fn\nx|dx\n\nfo\nr\nall n,\nin\nb\noth\nthe\nRiemann\no\nr\nthe\nLeb\nesgue\nsense\n\nConvergence\nin\nthe\nquadratic\nmean\ndo\nes\nnot\nimply\nconvergence\nat\nall\nOver\nthe\ninterval\n,\n\nfo\nr\nevery n\n\n,\n, ...,\nand i\n\n, ..., n\nlet\nf n\ni\n\ni-\nn\n< x ≤i\nn\n\notherwise\nClea\nrly\nthe\nsequence\nf\n\n, f\n\n, f\n\n, ..., f n\n, f n\n, ...f n\nn-, f n\nn, f n\n\n, ...,\nconverges\nto\n\nb\noth\nin\nmeasure\nand\nin\nthe\nquadratic\nmean\nHo\nw\never\nthe\nsame\nsequence\ndo\nes\nnot\nconverge\nfo\nr\nany x\n\nConvergence\nin\np\nrobabilit\ny\nand\nalmost\nsurely\nAny\nevent\nwith\np\nrobabilit\ny\n\nis\nsaid\nto\nhapp\nen\nalmost\nsurely\nA\nsequence\nof\nreal\nrandom\nva\nriables Yn\nconverges\nalmost\nsurely\nto\na\nrandom\nva\nriable Y\ni P\nYn →Y\n\nA\nsequence Yn\nconverges\nin\np\nrobabilit\ny\nto Y\ni\nfo\nr\nevery\nε >\n\nlimn→infP\n|Yn -Y | > ε\n\n.\nConvergence\nalmost\nsurely\nimplies\nconvergence\nin\np\nroba\nbilit\ny\n\nA\nsequence X\n, ...Xn\nsatises\nthe\nstrong\nla\nw\nof\nla\nrge\nnum\nb\ners\nif\nfo\nr\nsome\nconstant c\n\nn\nn\ni Xi\nconverges\nto c\nalmost\nsurely\n\nThe\nsequence\nsatises\nthe\nw\neak\nla\nw\nof\nla\nrge\nnum\nb\ners\ni\nfo\nr\nsome\nconstant c\n\nn\nn\ni Xi\nconverges\nto c\nin\np\nrobabilit\ny\n\nEuclidean\nspace\nA\nEuclidean\nspace\nis\na\nlinea\nr\nvecto\nr\nspace E\nin\nwhich\na\ndot\np\nro\nduct\nis\ndened\nA\nreal\nvalued\nfunction\n·, ·\nis\na\ndot\np\nro\nduct\ni ∀f, g, h ∈E\nand α ∈\nI\nR\n\nf, g\n\ng, f\n\nf\ng, h\n\nf, h∗\n\ng, h\n\nand\nαf, g\n\nαf, g\n\nf, f\n≥\n\nand\nf, f\n\ni f\n\nA\nEuclidean\nspace\nb\necomes\na\nno\nrmed\nlinea\nr\nspace\nwhen\nequipp\ned\nwith\nthe\nno\nrm\n∥f∥\n\nf, f\n.\n\nOrthogonal\nsystems\nand\nbases\nA\nset\nof\nnonzero\nvecto\nrs {xα}\nin\na\nEuclidean\nspace E\nis\nsaid\nto\nb\ne\nan\no\nrthogonal\nsystem\nif\nxα, xβ\n\nfor α β\nand\nan\no\nrthono\nrmal\nsystem\nif\nxα, xβ\n\nfor α β\nxα, xβ\n\nfor α\nβ.\nAn\no\nrthogonal\nsystem {xα}\nis\ncalled\nan\no\nrthogonal\nbasis\nif\nit\nis\ncomplete\nthe\nsmallest\nclosed\nsubspace\ncontaining\n{xα}\nis\nthe\nwhole\nspace E\n\nA\ncomplete\no\nrthono\nrmal\nsys\ntem\nis\ncalled\nan\no\nrthono\nrmal\nbasis\n\nExamples\n\nI\nRn\nis\na\nreal nspace\nthe\nset\nof ntuples x\n\nx\n, ..., xn\n\ny\n\ny\n, ..., yn\n\nIf\nw\ne\ndene\nthe\ndot\np\nro\nduct\nas\nx, y\n\nn\n\ni\nxiyi\nw\ne\nget\nEuclidean nspace\nThe\nco\nrresp\nonding\nno\nrms\nand\ndistances\nin\nI\nRn\na\nre\n∥x∥\n\nn\n\ni\nx\n\ni\nρx, y\n\n∥x -y∥\n\nn\n\ni\nxi -yi\n\n.\n\nThe\nvecto\nrs\ne\n\n,\n,\n, ....,\n\ne\n\n,\n,\n, ....,\n\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\nen\n\n,\n,\n, ....,\n\nfo\nrm\nan\no\nrthono\nrmal\nbasis\nin\nI\nRn\n\nThe\nspace l\n\nwith\nelements x\n\nx\n, x\n, ..., xn, .... y\n\ny\n, y\n, ..., yn, ....\n\nwhere\ninf\n\ni\nx\n\ni < inf,\ninf\n\ni\ny\n\ni < inf, ..., ...,\nb\necomes\nan\ninnitedimensional\nEuclidean\nspace\nwhen\nequipp\ned\nwith\nthe\ndot\np\nro\nduct\nx, y\n\ninf\n\ni\nxiyi.\n\nThe\nsimplest\no\nrthono\nrmal\nbasis\nin l\n\nconsists\nof\nvecto\nrs\ne\n\n,\n,\n,\n, ...\ne\n\n,\n,\n,\n, ...\ne\n\n,\n,\n,\n, ...\ne\n\n,\n,\n,\n, ...\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\nthere\na\nre\nan\ninnite\nnumb\ner\nof\nthese\nbases\n\nThe\nspace C\n\na, b\nconsisting\nof\nall\ncontinuous\nfunctions\non\na, b\nequipp\ned\nwith\nthe\ndot\np\nro\nduct\nf, g\n\nb\na f\ntg\ntdt\nis\nanother\nexample\nof\nEuclidean\nspace\n\nAn\nimp\no\nrtant\nexample\nof\no\nrthogonal\nbases\nin\nthis\nspace\nis\nthe\nfollo\nwing\nset\nof\nfunctions\n,\ncos\nπnt\nb -a,\nsin\nπnt\nb -a\nn\n\n,\n, ... .\n\nHilb\nert\nspace\nA\nHilb\nert\nspace\nis\na\nEuclidean\nspace\nthat\nis\ncomplete\nsepa\nrable\nand\ngenerally\ninnitedimensional\nA\nHilb\nert\nspace\nis\na\nset H\nof\nelements f, g, ...\nfo\nr\nwhich\nH\nis\na\nEuclidean\nspace\nequipp\ned\nwith\na\nscala\nr\np\nro\nduct\nH\nis\ncomplete\nwith\nresp\nect\nto\nmetric ρf, g\n\n∥f -g∥\nH\nis\nsepa\nrable\ncontains\na\ncountable\neverywhere\ndense\nsubset\n\ngenerally H\nis\ninnitedimensional\nl\n\nand L\n\na\nre\nexamples\nof\nHilb\nert\nspaces\n\nThe δ\nfunction\nW\ne\nno\nw\nconsider\nthe\nfunctional\nwhich\nreturns\nthe\nvalue\nof\nf ∈C\nat\nthe\nlo\ncation t\nan\nevaluation\nfunctional\nf\n\nf\nt.\nNote\nthat\nthis\nfunctional\nis\ndegenerate\nb\necause\nit\ndo\nes\nnot\ndep\nend\non\nthe\nentire\nfunction f\n\nbut\nonly\non\nthe\nvalue\nof\nf\nat\nthe\nsp\necic\nlo\ncation t\nThe δ\nt\nis\nnot\na\nfunctional\nbut\na\ndistribution\n\nThe δ\nfunction\ncont\nThe\nsame\nfunctional\ncan\nb\ne\nwritten\nas\nf\n\nf\nt\n\ninf\n-inff\nsδ\ns -tds.\nNo\no\nrdina\nry\nfunction\nexists\nin L\n\nthat\nb\nehaves\nlik\ne δ\nt\none\ncan\nthink\nof δ\nt\nas\na\nfunction\nthat\nvanishes\nfo\nr t\n\nand\ntak\nes\ninnite\nvalue\nat t\n\nin\nsuch\na\nw\na\ny\nthat\ninf\n-infδ\ntdt\n\n.\n\nThe δ\nfunction\ncont\nThe δ\nfunction\ncan\nb\ne\nseen\nas\nthe\nlimit\nof\na\nsequence\nof\no\nrdina\nry\nfunctions\nF\no\nr\nexample\nif\nrε\nt\n\nε\nU\nt -U\nt -ε\n\nis\na\nrectangula\nr\npulse\nof\nunit\na\nrea\nconsider\nthe\nlimit\nlim\nε→\ninf\n-inff\nsrε\ns -tds.\nBy\ndenition\nof rε\nthis\ngives\nlim\nε→\n\nε\ntε\nt\nf\nsds\nf\nt\nb\necause f\nis\ncontinuous\n\nF\nourier\nT\nransfo\nrm\nThe\nF\nourier\nT\nransfo\nrm\nof\na\nreal\nvalued\nfunction f ∈L\n\nis\nthe\ncomplex\nvalued\nfunction\nf\nω\n\ndened\nas\nF\nf\nx\n\nf\nω\n\ninf\n-inff\nx e-jωxdx.\nThe\nFT\nf\ncan\nb\ne\nthought\nof\nas\na\nrep\nresentation\nof\nthe\ninfo\nrmation\ncontent\nof f\nx\nThe\no\nriginal\nfunction f\ncan\nb\ne\nobtained\nthrough\nthe\ninverse\nF\nourier\nT\nransfo\nrm\nas\nf\nx\n\nπ\n\ninf\n-inf\nf\nω\nejωxdω.\n\nProp\nerties\nf\nat\n⇔\n\n|a|F\nω\na\n\nf∗\nt\n⇔F ∗\nω\n\nF\nt\n⇔\nπf\n-ω\n\nf\nt -t\n\n⇔F\nω\ne-jt\nω\nf\ntejω\nt\n⇔F\nω -ω\n\ndnf\nt\ndtn\n⇔\njω\nnF\nω\n\n-jtnf\nt\n⇔\ndnF\nω\n\ndωn\ninf\n-inff\n\nτ\nf\n\nt -τ\ndτ\n⇔F\n\nω\nF\n\nω\n\ninf\n-inff∗\nτ\nf\nt\nτ\ndτ\n⇔|F\nω\n|\n\nProp\nerties\nThe\nb\no\nx\nand\nthe\nsinc\nf\nt\n\nif -a ≤t ≤a\nand\n\notherwise\nF\nω\n\nsinaω\n\nω\n.\n-10\n-8\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\n1.2\n-10\n-8\n-6\n-4\n-2\n-1\n-0.5\n0.5\n1.5\n2.5\n3.5\n\nProp\nerties\nThe\nGaussian\nf\nt\ne-at\n\nF\nω\n\nπ\nae-ω\n/a.\n-10\n-8\n-6\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-10\n-8\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n\nProp\nerties\nThe\nLaplacian\nand\nCauchy\ndistributions\nf\nt\n\ne-a|t|\nF\nω\n\na\na\n\nω\n.\n-10\n-8\n-6\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-10\n-8\n-6\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\nF\nourier\nT\nransfo\nrm\nin\nthe\ndistribution\nsense\nWith\ndue\nca\nre\nthe\nF\nourier\nT\nransfo\nrm\ncan\nb\ne\ndened\nin\nthe\ndistribution\nsense\nF\no\nr\nexample\nw\ne\nhave\n- δ\nx ⇐⇒\n\n-\ncos\nω\nx ⇐⇒π\nδ\nω -ω\n\nδ\nω\nω\n\n-\nsin\nω\nx ⇐⇒jπ\nδ\nω\nω\n\n-δ\nω -ω\n\n- U\nx ⇐⇒πδ\nω\n-j/ω\n- |x| ⇐⇒-/ω\n\nP\na\nrsevals\nfo\nrmula\nIf f\nis\nalso\nsqua\nre\nintegrable\nthe\nF\nourier\nT\nransfo\nrm\nleaves\nthe\nno\nrm\nof f\nunchanged\nP\na\nrsevals\nfo\nrmula\nstates\nthat\n\ninf\n-inf|f\nx|\ndx\n\nπ\n\ninf\n-inf|\nf\nω\n|\ndω.\n\nF\nourier\nT\nransfo\nrms\nof\nfunctions\nand\ndistributions\nThe\nfollo\nwing\na\nre\nF\nourier\ntransfo\nrms\nof\nsome\nfunctions\nand\ndistributions\n- f\nx\nδ\nx ⇐⇒\nf\nω\n\n- f\nx\n\ncosω\nx ⇐⇒\nf\nω\n\nπ\nδ\nω -ω\n\nδ\nω\nω\n\n- f\nx\n\nsinω\nx ⇐⇒\nf\nω\n\niπ\nδ\nω\nω\n\n-δ\nω -ω\n\n- f\nx\nU\nx ⇐⇒\nf\nω\n\nπδ\nω\n-i/ω\n- f\nx\n|x| ⇐⇒\nf\nω\n\n-/ω\n\nF\nunctional\ndierentiation\nIn\nanalogy\nwith\nstanda\nrd\ncalculus\nthe\nminimum\nof\na\nfunc\ntional\ncan\nb\ne\nobtained\nb\ny\nsetting\nequal\nto\nzero\nthe\nderiva\ntive\nof\nthe\nfunctional\nIf\nthe\nfunctional\ndep\nends\non\nthe\nderivatives\nof\nthe\nunkno\nwn\nfunction\na\nfurther\nstep\nis\nre\nquired\nas\nthe\nunkno\nwn\nfunction\nhas\nto\nb\ne\nfound\nas\nthe\nsolution\nof\na\ndierential\nequation\n\nF\nunctional\ndierentiation\nThe\nderivative\nof\na\nfunctional\nf\n\nis\ndened\nD\nf\n\nDf\ns\n\nlim\nh→\nf\nt\nhδ\nt -s -\nf\nt\nh\n.\nNote\nthat\nthe\nderivative\ndep\nends\non\nthe\nlo\ncation s\nF\no\nr\nexample\nif\nf\n\ninf\n-inff\ntg\ntdt\nD\nf\n\nDf\ns\n\ninf\n-infg\ntδ\nt -sdt\ng\ns.\n\nIntuition\nLet f\n\na, b →\nI\nR a\nx\n\nand b\nxN\n\nThe\nintuition\nb\nehind\nthis\ndenition\nis\nthat\nthe\nfunctional\nf\n\ncan\nb\ne\nthought\nof\nas\nthe\nlimit\nfo\nr N →inf\nof\nthe\nfunction\nof N\nva\nriables\nN\n\nN\nf\n, f\n, ..., fN\n\nwith f\n\nf\nx\n\nf\n\nf\nx\n\nfN\nf\nxN\n\nF\no\nr N →inf\n\ndep\nends\non\nthe\nentire\nfunction f\n\nThe\ndep\nendence\non\nthe\nlo\ncation\nb\nrought\nin\nb\ny\nthe δ\nfunction\nco\nrresp\nonds\nto\nthe\npa\nrtial\nderivative\nwith\nresp\nect\nto\nthe\nva\nriable fk\n\nF\nunctional\ndierentiation\ncont\nIf\nf\n\nf\nt\nthe\nderivative\nis\nsimply\nD\nf\n\nDf\ns\nDf\nt\nDf\ns\nδ\nt -s.\nSimila\nrly\nto\no\nrdina\nry\ncalculus\nthe\nminimum\nof\na\nfunctional\nf\n\nis\nobtained\nas\nthe\nfunction\nsolution\nto\nthe\nequation\nD\nf\n\nDf\ns\n\n.\n\nRandom\nva\nriables\nW\ne\na\nre\ngiven\na\nrandom\nva\nriable ξ ∼F\n\nT\no\ndene\na\nrandom\nva\nriable\ny\nou\nneed\nthree\nthings\n\na\nset\nto\ndra\nw\nthe\nvalues\nfrom\nw\nell\ncall\nthis\n!\n\na σ\nalgeb\nra\nof\nsubsets\nof\n!\nw\nell\ncall\nthis B\n\na\np\nrobabilit\ny\nmeasure F\non B\nwith F\n!\n\nSo\n!, B, F\n\nis\na\np\nrobabilit\ny\nspace\nand\na\nrandom\nva\nriable\nis\na\nmasurable\nfunction X\n\n! →\nI\nR\n\nExp\nectations\nGiven\na\nrandom\nva\nriable ξ ∼F\nthe\nexp\nectation\nis\nI\nEξ ≡\n\nξdF.\nSimila\nrly\nthe\nva\nriance\nof\nthe\nrandom\nva\nriable σ\n\nξ\n\nis\nva\nr\nξ\n≡\nI\nE\nξ -\nI\nEξ\n\n.\n\nLa\nw\nof\nla\nrge\nnumb\ners\nThe\nla\nw\nof\nla\nrge\nnumb\ners\ntells\nus\nlim\nl→inf\n\nl\nl\n\ni\nI\n\nf\nxi\nyi\n→\nI\nEx,yI\n\nf\nxy\n.\nIf lσ →inf\nthe\nCentral\nLimit\nTheo\nrem\nstates\n√\nl\n\nl\nI -\nI\nEI\n\n√\nva\nrI\n→N\n,\n,\nwhich\nimplies\n\nl\n\nI -\nI\nEI\n∼k\n√\nl\n.\nIf lσ →c\nthe\nCentral\nLimit\nTheo\nrem\nimplies\n\nl\n\nI -\nI\nEI\n∼k\nl."
    },
    {
      "category": "Resource",
      "title": "class02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/1b0cd4a68bf1fd1b5345f31adacc6e48_class02.pdf",
      "content": "\"!\n#%$\n&\n\n$\n'\n\n(\n)+*\n,\n\n*\n-\n*\n.\n.\n/\n*\n\n'\n\n/\n/\n\n-\n!\n\n$\n/\n.\n\n&\n\n/\n*\n\n$\n*\n\n/\n,\n\n/\n*\n-\n\n,\n\n/\n$\n/\n\n/\n\n/\n/\n,\n/\n\n/\n*\n-\n\n!\n\n*\n\n!\n!\n\n*\n\n/\n\n!\n\n'\n-\n!\n.\n&\n\n$\n/\n\n/\n*\n-\n\n!\n/\n\n,\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n\n$\n$\n*\n$\n\n!\n/\n\n$\n*\n&\n\n!\n\n!\n\n!\n\n$\n/\n.\n$\n*\n#\n\n!\n,\n\n!\n$\n*\n#\n\n!\n,\n*\n&\n\n/\n*\n\n$\n*\n\n/\n,\n\n/\n*\n$\n*\n,\n\n$\n\n!\n\n!\n!\n\n!\n\n!\n!\n'\n/\n!\n\n*\n\n*\n\n&\n\n/\n*\n\n!\n,\n\n/\n$\n/\n\n!\n$\n$\n*\n$\n\n.\n!\n\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n\n!\n\n!\n/\n\n$\n*\n&\n\n!\n\n!\n\n,\n\n/\n$\n/\n\n/\n\n/\n/\n,\n/\n\n/\n*\n\n$\n*\n\n!\n\n*\n!\n'\n$\n!\n\n&\n/\n$\n!\n,\n!\n\n*\n\n.\n*\n$\n/\n\n,\n\n&\n\n/\n.\n/\n\n!\n\n*\n\n!\n!\n\n*\n\n/\n\n!\n\n'\n\n!\n\n!\n!\n\n$\n/\n#\n!\n\n!\n'\n\n.\n*\n$\n/\n\n,\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n\n/\n\n/\n*\n\n/\n\n!\n\n!\n\n!\n$\n!\n\n&\n/\n$\n!\n,\n!\n\n,\n/\n\n/\n\n$\n/\n\n'\n\n/\n\n#\n\n/\n\n/\n!\n\n/\n$\n*\n#\n\n#\n/\n\n/\n\n'\n\n!\n*\n$\n'\n\n!\n\n&\n,\n!\n\nX\n\nY\n\n$\n!\n\n*\n\n!\n\n*\n$\n\n*\n,\n\n$\n/\n\n#\n\n!\n\n!\n\n$\n!\n.\n/\n\n!\n\nS\n\n*\n\n/\n\n/\n.\nl\n\n,\n\n!\n\n$\n\n/\n\n/\n\n$\n*\n,\n\n!\n$\n*\n#\n\n#\n/\n\n/\n\n'\n/\n\n$\n/\n#\n&\n\n/\n*\nX × Y\n\nx\n\n, y\n\n, . . . ,\n\nxl, yl\n\n!\n\n/\n\n,\n\n!\n$\n!\n\n&\n!\n\n&\n\n!\n*\n\n!\n\n$\n/\n\n!\np\n\ny|x\n\np\n\nx, y\n\np\n\ny|x\n\n· p\n\nx\n\n/\n\n$\n&\n\n/\n\n*\n*\n\n!\n\n!\n\n/\n!\n\np\n\nx, y\n\n#\n&\n\nX\nY\nP(x)\nP(y|x)\n\n)\n\n!\n#\n\n/\n\n.\n*\n\n*\n\n/\n\n*\n&\n\n!\n\n!\n\n$\n\n/\n/\n.\n\n!\n\nS\n\n*\n\n!\n\n$\n\n&\n\n/\n*\nfS\n\n*\n*\n\n!\n\nx\n\n&\n!\nxnew\n\n$\n!\n/\n\n!\n\n*\n\n/\n\n!\n\n&\n!\n*\ny\n\nypred\n\nfS\n\nxnew\n\ny\n/\n\n$\n!\n\n&\n!\n$\n\n*\n,\n\n$\n/\n\n#\n!\n\n!\n\n!\n\ny\n\n!\n\n&\n!\n\n$\n*\n,\n\n&\n*\n$\n!\n$\n!\n\n/\n\n!\n\n!\n\n!\n\n!\n\n*\n\n!\n$\n\n/\n\n/\n*\n\n$\n*\n#\n!\n,\n\n!\n\n/\n.\n*\n!\n\ny\n\n&\n!\n*\n\n!\n*\n\n!\n$\n\ny\n\n&\n!\n*\n-\n\n*\n$\n!\n$\n\n*\n,\n!\n\n&\n$\n!\n.\n*\n*\n!\n\n*\n*\n&\n$\n&\n\n/\n*\n\n!\n!\n!\n\nV\n\n.\n!\n!\n$\n\n!\n\n!\n\nV\n\nf\n\nx\n\n, y∗\n\n!\n*\n\n!\n\n!\n\n$\n/\n\n!\n\n!\n\n'\n\n!\n\n!\n\n!\n!\nx\n\n.\n&\n!\n\n!\n\n*\n\n/\n\n!\ny\n\n&\n!\n/\n\nf\n\nx\n\n!\n/\n\n/\n\n&\n\n'\ny∗\n\n*\n$\n$\n!\n.\n$\n!\n\n/\n*\n\n!\n,\n*\n\n*\n,\n,\n*\n\n*\n\n&\n\n/\n*\n/\n\n&\n\n$\n!\n\n*\n\n*\n$\n\n*\n\nV\n\nf\n\nx\n\n, y\n\nf\n\nx\n\n-y\n\n!\n\n*\n&\n\n*\n&\n\n!\n\n!\n\n#\n\n*\n\n&\n\n!\n\n&\n!\n\n*\n$\n\n*\n\nV\n\nf\n\nx\n\n, y\n\n|f\n\nx\n\n-y|\n\n/\n\n,\n*\n$\n!\n.\n!\n!\n$\n\nε\n\n/\n\n!\n\n/\n\n/\n\n!\n\n*\n\n&\n\n/\n*\n/\n\nV\n\nf\n\nx\n\n, y\n\n|f\n\nx\n\n-y| -ε\n\nV\n\nf\n\nx\n\n, y\n\n-yf\n\nx\n\n!\n\n\"\n\n#\n\n$\n\n#\n\n%\n\nV\n\nf\n\nx\n\n, y\n\n-y · f\n\nx\n\n'&\n\n!\n\nf\n\n!\n\nV\n\n#\n\nP\n\nX\n\nY\n\n!\nf\n\nI\n\nf\n\nZ\nV\n\nf\n\nx\n\n, y\n\ndμ\n\nx, y\n\n#\n\n!\n\ndμ\n\ndP\n\n%\n\n%\n\nI\n\nf\n\n\"\n\n%\n\nP\n\n!\n\nf\n\n!\n\nV\n\n\"\n\nS\n\n\"\n\n!\nl\n\n#\n\n%\n\n!\nf\n\nIS\n\nf\n\nl\nX\nV\n\nf\n\nxi\n\n, yi\n\n)\n\n!\n\nH\n/\n\n!\n\n!\n*\n&\n\n/\n*\n\n!\n\n*\n\n*\n&\n$\n\n.\n*\n$\n/\n\n,\n\n*\n\n!\n\n$\n\n/\n\n*\n\n!\n\n*\n\n!\n\n/\n\n$\n!\n\n!\n\n*\n\n!\n\n,\n*\n&\n\n*\n\n/\n\n#\n!\n\n/\n\n!\n\n$\n\n/\n/\n.\n\n!\n\nS\n\n&\n\n/\n*\n\n!\nH\n\n!\n,\n\n/\n$\n/\n\n$\n/\n\n,\n/\n/\n,\n/\n\n/\n*\n\n/\n\n&\n\n/\n*\nfS\n\n,\n/\n/\n,\n/\n\n!\n\n!\n!\n,\n\n/\n$\n/\n\n$\n/\n\n*\n\n!\n$\n\n&\n\n/\n*\n\nf ∈H\n\nfS\n\n$\n.\n,\n/\nf∈H IS\n\nf\n\n$\n.\n,\n/\nf∈H\n\nl\nl\nX\ni\n\nV\n\nf\n\nxi\n\n, yi\n\n*\n$\n*\n\n!\n\n$\n!\n\n&\n,\n/\n.\n\n!\n!\n\n/\n\n!\n\n!\n*\n\n&\n\n&\n\n/\n*\n\n*\n$\n\n!\n\n*\n\n&\n\n/\n*\n*\n\n*\n#\n!\n&\n\n!\n&\n\n/\n\n!\n\n*\n\n!\n\n*\n\n!\n\n$\n/\n.\n\n!\n\n*\n\n&\n\n/\n*\n,\n&\n\n#\n!\n-\n\n*\n\n/\n\n!\n\n-\n/\n\n*\n,\n&\n\n!\n\n/\n\n#\n!\n&\n/\n\n&\n!\n\n#\n!\n\n#\n!\n\n!\n\n*\n\n!\n!\n\n,\n!\n\n!\n/\n\n!\n$\n!\n\n!\nIS\n\nfS\n\n-I\n\nfS\n\n,\n&\n\n.\n*\n\n*\n\n!\n$\n*\n\n!\n&\n,\n#\n!\n$\n*\n\n$\n\n/\n/\n.\n!\n\n,\n\n!\n\n/\n\n$\n!\n\n!\n\n/\n\nl→inf\n\n*\n\n!\n$\n\n*\n$\n\n!\n\n$\n\n/\n/\n.\n!\n$\n$\n*\n$\n*\n$\n\n!\n\n*\n\n&\n\n/\n*\n,\n&\n\n*\n\n!\n$\n.\n!\n\n*\n\n!\n!\n\n!\n\n!\n!\n$\n$\n*\n$\n\n&\n\n#\n!\n\n$\n*\n\n'\n\n*\n$\n/\n\n!\n$\n\n/\n\n!\n\n!\n\n*\n\n&\n\n/\n*\n\n*\n&\n\n*\n\n#\n!\n\n$\n!\n/\n\n/\n\n!\n\nx\nf(x)\n\nx\nf(x)\n\nx\nf(x)\n\nx\nf(x)\n\n!\n\n/\n\n!\n!\n\n!\n$\n\n$\n*\n\n!\n$\n\n*\n/\n\n!\n*\n\n!\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n!\n\n&\n$\n!\n\n*\n\n/\n\n!\n\n'\n*\n\n)\n\n!\n!\n'\n\n$\n*\n\n!\n$\n\n'\n/\n\n!\n&\n\n$\n*\n\n!\n$\n\n'\n\n!\n\n'\n\n*\n&\n\n/\n*\n\nF\n/\n\n&\n/\n*\n$\n,\n\n/\n\n!\n*\n\n!\n\n/\n\n&\n\n*\n$\n\nε >\n\n*\n$\n\nμ\n\n-\nμ\n\n/\n,\nl→inf\n\n&\n\nf∈F\n|I\n\nf\n\n-IS\n\nf\n\n| > ε\n!\n\n!\n\n/\n\n#\n!\n!\n\n*\n$\n/\n.\n\n/\n\n!\n\n/\n\n/\n*\n\n!\n\n&\n/\n\n!\n\n!\n\n/\n\n/\n*\n\n/\n!\n\n/\n\n/\n\nH\n/\n\n&\n\n/\n\n/\n$\n!\n\n'\n/\n,\n\n/\n!\n\nIS\n\nf\n\n-I\n\nf\n\n*\n$\n!\n\n!\n$\n'\n&\n\n/\n*\nf\n/\nH\n.\n!\n\n,\n\nl→inf\n\n/\n\n/\n.\n!\n!\n$\n\n/\n\n*\n\n!\n$\n*\n#\n!\n,\n\n#\n!\n,\n\n!\n\n!\n\n*\n\n!\n#\n'\n\n$\n*\n$\n/\n\n!\n\n*\n/\n\n!\n*\nH\n\n!\n\n/\n\n!\n!\n\n!\n$\n\n!\n\n*\n\n!\n!\n\n/\n\n,\n\n/\n\n'\n&\n\n!\n\n*\n,\n!\n\n*\n\n!\n\n*\n\n&\n\n/\n*\n\nfS\n!\n\n!\n\n*\n\n/\n&\n*\n&\n\n'\n*\n\n!\n\n$\n\n/\n/\n.\n\n!\n\nS\n\n$\n\n/\n\n&\n\n$\n\n.\n/\n.\n*\n!\n*\n\n!\n\n$\n\n/\n/\n.\n\n*\n/\n\n*\n&\n\n!\n\n!\n\n!\n\n!\n\n*\n\n&\n\n/\n*\n\nl\n.\n*\n!\n\n*\n/\n\n/\n\n'\n\n$\n*\n#\n\n!\n,\n/\n\n/\n/\n\n*\n\n&\n\n/\n*\n\n-\n!\n\n/\n\n-\n/\n\n&\n/\n\n&\n!\n-\n!\n\n!\n\n*\n\n/\n&\n*\n&\n\n'\n*\n\n!\n\n!\n\n.\n\n/\n\n/\n\n$\n*\n#\n\n!\n,\n/\n\n/\n/\n\n/\n\n*\n\n!\n\n*\n\n!\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n)\n\n!\n#\n\n/\n\n/\n!\n\n*\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n/\n\n*\n$\n!\n\n*\n$\n!\n\n!\n\n*\n\n!\n!\n\n*\n\n#\n'\n\n*\n\n$\n\n/\n/\n.\n\n!\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n\n/\n/\n\n$\n!\n\n'\n\n*\n*\n\n*\n/\n\n*\n&\n\n!\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n\n/\n\n/\n\n*\n\n!\n&\n\n/\n*\n/\nH\n\n/\n\n,\n/\n/\n,\n/\n!\n\nl\nl\nX\ni\n\nV\n\nf\n\nxi\n\n, yi\n\n/\n\n/\n.\n!\n!\n$\n\n*\n$\n\n$\n#\n/\n\n$\n\n$\n'\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n\n/\n\n!\n\n!\n,\n/\n/\n,\n/\n!\n*\n\n!\n$\n\n!\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n\n*\n$\n\n!\n\n*\n\n/\n\n/\n\n!\n\n$\n\n,\n!\n\n!\n$\nλ\n\n!\n$\n!\n.\n&\n\n$\n/\n\n!\n&\n\n/\n*\n\nl\nl\nX\ni\n\nV\n\nf\n\nxi\n\n, yi\n\nλ∥f∥\nK,\n\n!\n$\n!\n∥f∥\nK\n/\n\n!\n*\n$\n,\n/\nHK\n\n!\n!\n\n$\n*\n&\n\n/\n.\n\n!\n$\n!\n\n/\n\n#\n!\n$\n\n!\n\n!\n\n!\n#\n'\n\n!\n!\n$\n!\n\nK\n\n!\n\n/\n\n!\n!\n/\n&\n\n&\n$\n!\n\n!\n\n-\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n!\n\n&\n$\n!\n\n!\n\n*\n\n!\n!\n\n!\n.\n!\n\n/\n\n!\n\n!\n\n&\n/\n\n&\n!\n!\n\n!\n\n!\n\n/\n\n'\n\n/\n\n!\n$\n'\n\n$\n*\n.\n*\n$\n,\n\n*\n\n!\n\n*\n\n&\n\n/\n*\n-\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n!\n\n&\n$\n!\n\n*\n\n/\n\n!\n\n'\n-\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n/\n\n*\n\n!\n\n'\n$\n!\n\n!\n\n*\n\n#\n&\n\n/\n\n!\n$\n!\n\n$\n*\n,\n\n*\n\n$\n!\n.\n&\n\n$\n/\n\n/\n*\n\n!\n.\n\n*\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n\n/\n\n/\n\n#\n\n/\n\n-\n\n!\n!\n\n!\n\n/\n\n/\n\n$\n*\n&\n\n!\n\n!\n'\n\n/\n\n#\n!\n\n!\n\n'\n\n*\n\n!\n\n/\n\n!\n\n!\n\n/\n\n*\n$\n\n/\n\n-\n\n!\n\n/\n\n*\n!\n$\n/\n\n!\n\n!\n\n*\n\n&\n\n/\n*\n*\n)\n/\n\n*\n*\n\n$\n!\n.\n&\n\n$\n\n/\n\n/\n*\n\n/\n\n/\n*\n\n*\n\n!\n\n'\n\n*\n\n!\n\n/\n\n!\nH\n\n!\n\n!\n\n!\n\n*\n\n*\n&\n$\n\n.\n*\n$\n/\n\n,\n\n*\n\n!\n\n$\n\n!\n!\n\n!\n\n)\n\n!\n\nT\n/\n\n!\n*\n&\n\n/\n*\n\n*\n\n!\n\n$\n/\n*\n$\n/\n/\n\n'\n.\n/\n\n!\n$\n*\n#\n\n!\n,\n\n/\n\n&\n,\n!\n\n*\n\n*\n\n/\n\n!\n\n$\n&\n!\n\n&\n\n/\n*\n\n,\n/\n/\n,\n/\n!\n\n!\n$\n/\n\n!\n\nT\n/\n\n*\n\n!\n\n*\n#\n!\n\n&\n\n/\n*\n\n/\nL\n\n*\n$\n\n/\n\n!\n$\n!\n\n/\n\n#\n\n!\n&\n\n/\n*\n\n!\n\nfH\n#\n!\n\n!\n&\n\n/\n*\n/\nH\n\n/\n\n!\n\n,\n\n!\n\n$\n&\n!\n$\n/\n\n!\n!\n\n!\n\n!\n\n*\n#\n!\nI\n\nfS\n\n-I\n\nfH\n\n!\n/\n\n!\n$\n!\n\n!\n/\n\n$\n&\n!\n$\n/\n\n#\n!\n\n!\n!\n\n!\n#\n!\n\n&\n\n/\n*\n/\nH\n\n!\n&\n\n/\n*\n/\nH\n\n!\n\n&\n\n'\n\n)\n\n/\n\n/\n\n!\n\n'\n#\n!\n\n&\n\n!\n*\n&\n$\n\n/\n\n!\n\n,\n\n!\n*\n!\n\n*\n\n.\n/\n\n!\n&\n\n!\n*\n&\n.\n\n/\n*\n$\n\n,\n\n/\n*\n\n*\n\n*\n*\n\n!\n\n*\n\n!\n\n#\n!\n\n&\n\n/\n*\n/\nH\n\n!\n\n/\n!\n\n/\n\n*\n#\n!\n\n,\n\n,\n\n/\n\n*\n\n/\n\n*\n\n/\n\n*\n&\n$\n\n!\n/\n\n#\n*\n&\n/\n.\n\n!\n\n,\n\n!\n!\n$\n\n$\n*\n$\n\n!\n\n!\n$\n,\n/\n/\n.\n\n*\n/\n\n/\n*\n\n&\n!\n$\n\n/\n\n!\n\n!\n\nI\n\nfS\n\n-I\n\nfH\n\n/\n\n#\n!\n\n,\n\n/\n\n/\n.\n\n$\n*\n#\n\n#\n/\n\n/\n\n'\n\n/\n,\n\n!\n$\n&\n\n!\n\n!\n!\n\n!\n\n/\nH\n/\n\n!\n\n#\n!\n\n!\n\n!\n\nl\n.\n!\n\n$\n.\n!\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n/\n\n#\n!\n\n*\n,\n!\n\n,\n\n!\n\nf\n\n#\n!\n\n!\n&\n\n/\n*\n/\nT\n\n/\n\n!\n\n,\n\n!\n\n$\n&\n!\n$\n/\n\n!\n!\n\n!\n\n!\n\n*\n#\n!\nI\n\nfH\n\n-I\n\nf\n\n!\n/\n\n!\n$\n!\n\n!\n/\n\n$\n&\n!\n$\n/\n\n#\n!\n\n!\n!\n\n!\n#\n!\n\n&\n\n/\n*\n/\nH\n\n!\n#\n!\n\n&\n\n/\n*\n/\nT\n\n)\n\n/\n\n/\n\n!\n\n'\n#\n!\n\n&\n\n!\nH\n/\n\n,\n\n!\n$\n\nT\n\n!\n\n/\n!\n\n/\n\n!\n$\n$\n*\n$\n\n*\n#\n!\n\n,\n\n*\n*\n\n!\n\n/\n\n*\n\n&\n\n!\n\n*\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n/\n\n#\n&\n\n!\n\n/\n\n!\n\n*\n$\n!\n/\n\n/\n,\n\n!\n$\n&\n\n!\n\n!\n!\n\n!\n\nH\n.\n$\n*\n\n#\n/\n.\n.\n!\n$\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n.\n!\n\n,\n\n!\n$\n\nT ⊆H\n\n/\n\n/\n\n/\n\n&\n\n/\n*\n\n!\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n/\n\n!\n$\n*\n\n!\n!\n\n!\n\n!\n\n*\n#\n!\nI\n\nfS\n\n-I\n\nf\n\n!\n/\n\n!\n$\n!\n\n!\n/\n\n$\n&\n!\n$\n/\n\n#\n!\n\n!\n!\n\n!\n&\n\n/\n*\n\n!\n\n&\n\n'\n\n!\n#\n!\n\n&\n\n/\n*\n/\nT\n\n!\n\n$\n!\n\n'\n\n/\n!\n\n/\n\n*\n#\n!\n\n,\n\n)\n\n!\n.\n!\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n/\n\n!\n\n&\n,\n*\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n\nI\n\nfS\n\n-I\n\nf\n\nI\n\nfS\n\n-I\n\nfH\n\nI\n\nfH\n\n-I\n\nf\n\n!\n\n,\n\n!\n#\n*\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n,\n\n!\n.\n!\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n\n/\n\n#\n!\n\n,\n\n)\n\n!\n$\n!\n/\n\n$\n\n!\n*\n\n#\n!\n\n!\n!\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n*\n&\n\n$\n!\n\n'\n#\n!\n/\n\n&\n/\n\n/\n\n!\n\n'\n\n!\n\n$\n\n,\n\n/\n.\nH\n#\n/\n.\n,\n\n!\n\n!\n\n$\n*\n\n/\n,\n\n/\n*\n!\n$\n$\n*\n$\n\n,\n\n)\n\n/\n\n/\n,\n\n/\n!\n\n!\n\n!\n\n,\n\n!\n\n!\n.\n!\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n\n,\n\n#\n'\n,\n\n/\n.\nH\n#\n/\n.\n\n!\n*\n\n!\n$\n\n!\n\n/\n\n*\n\n,\n\n/\n.\nH\n\n,\n\n/\n\n,\n\n!\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n,\n\n$\n\n/\n\n&\n\n$\n\n/\nH\n/\n\n&\n\n!\n\n,\n\n!\n!\n$\n$\n*\n$\n\n/\n\n.\n*\n\n*\n\n!\n$\n*\n\nl→inf\n\n#\n&\n\n*\n\n&\n/\n\n'\n/\n\n.\n*\n!\n\n*\n\n!\n$\n*\n!\n\n!\n\n/\n$\n!\n\n'\n*\n\n!\n\n/\n\n!\n\n*\nH\n\n)\n\n/\n\n/\n,\n\n/\n!\n\n!\n\n*\n!\n!\n\nH\n\n,\n\n*\n\n/\n#\n!\n\n&\n$\n\n!\n$\n,\n*\n$\n!\n\nT\n/\n\n!\n\n,\n\n'\n*\n$\n,\n\n'\n*\n\n#\n!\n\n&\n\n!\n\n'\n\n!\n\n*\n&\n\n/\n!\n\n*\n\n!\n*\n\n/\n,\n\n$\n\n!\n*\n\n#\n!\n\n!\n!\n\n!\n\n!\n\n*\n\n/\n\n/\n.\n$\n!\n\n&\n/\n$\n!\n,\n!\n\n!\n!\n\n!\n\n!\n.\n!\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n\n*\n#\n!\nI\n\nfS\n\n-I\n\nf\n\n!\n\n/\n\n!\n$\n\n&\n$\n!\n\n!\n\n$\n&\n!\n$\n/\n\n*\n\n!\n&\n\n/\n*\n\n!\n\nI\n\nfS\n\n/\n\n*\n,\n!\n\n/\n,\n!\n\n!\n\n!\n.\n!\n!\n$\n\n/\n\n/\n*\n!\n$\n$\n*\n$\n\n!\n\n!\n\n!\n$\n!\nI\n\nf\n\n!\n\n*\n\n$\n*\n\n!\n\n$\n!\n!\n\n&\n/\n\n!\n\nl1\nl2\nl1 < l2\nl1, l2 = number of data"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/e78149ec3db50b66d09a77399e2b508a_lecture02.pdf",
      "content": "Lecture 2: The Learning Problem In\nPerspective\nTomaso Poggio\n\nDescription\nWe introduce the problem of learning from sparse examples. We introduce\nkey terms and concepts such as loss functions, empirical risk, true risk,\ngeneralization error, hypothesis spaces, approximation error and sample\nerror. We introduce two key requirements on learning algorithms: stability\nand consistency. We then describe Tikhonov regularization -- which in our\ncourse is the algorithm with the magic.\n\nSuggested Reading\n-\nCucker and Smale. On the mathematical foundations of\nlearning. Bulletin of the American Mathematical Society, 2002.\n-\nEvgeniou, Pontil and Poggio. Regularization Networks and\nSupport Vector Machines Advances in Computational\nMathematics, 2000.\n-\nVapnik. The Nature of Statistical Learning Theory. Wiley &\nSons, 1995."
    },
    {
      "category": "Resource",
      "title": "mathcamp2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/70e8bb73cc1289dc41fac4942bbeb2e8_mathcamp2.pdf",
      "content": "Math Camp 2: Convex Optimization\n\nDescription\nWe give a very brief introduction to key concepts in optimization theory\nsuch as convex sets, convex functions, convex optimization, and Lagrange\nmultipliers, and duality. Rather than being at all comprehensive, we try to\nindicate what results will be used in 9.520 and present some intuitions\nabout them.\nSuggested Reading\n-\nFletcher. Practical Methods of Optimization. Wiley & Sons, 1987.\n-\nBazaraa, Sherali, and Shetty. Nonlinear Programming: Theory and\nAlgorithms. Wiley-Interscience, 1993\n-\nMangasarian. Nonlinear Programming. McGraw-Hill, 1969."
    },
    {
      "category": "Resource",
      "title": "class03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/302b891be47423e76d9c5fccb8ae8474_class03.pdf",
      "content": "!#\"\n$&%\n'\n\n%\n(\n\n)+*\n,\n\n*\n-\n*\n.\n.\n/\n*\n\n(\n\n'\n\"\n%\n\"\n\"\n\n)+*\n/\n\n%\n*\n'\n\n\"\n\n%\n\n/\n\n'\n\n%\n\n(\n'\n\n\"\n\n'\n\n,\n/\n\n(\n*\n\n(\n\n*\n\n\"\n\n/\n\n\"\n\n\"\n\n\"\n\n%\n*\n'\n\n/\n.\n\n\"\n%\n\"\n\n/\n\n$\n\"\n%\n\n\"\n\n*\n\"\n%\n/\n\"\n\n\"\n.\n\"\n\"\n%\n\n*\n\n'\n\n/\n*\n*\n\n)\n/\n*\n*\n\n%\n\"\n.\n'\n\n%\n/\n\n/\n*\n/\n\n/\n\n\"\n,\n\n\"\n*\n\n'\n\n/\n*\n\n'\n\n/\n*\n/\n\n\"\n\n\"\n\n$\n\"\n\n*\n'\n.\n\n*\n\n*\n/\n\n,\n\n\"\n\nC\n\na, b\n\n\"\n\n\"\n\n*\n\n%\n\"\n\n'\n\"\n\n'\n\n/\n*\n\n/\n\n\"\n/\n\n\"\n%\n\na, b\n\nL\n\na, b\n\n\"\n\n\"\n\n*\n\n%\n\"\n\n'\n\"\n\n'\n\n/\n*\n\n*\n\n\"\n\n$\n\n*\n\n'\n\n\"\n\n'\n\"\n/\n\n/\n\n\"\n.\n%\n\n$\n\n\"\n/\n\n\"\n/\n\n\"\n%\n\na, b\n\nL\n\na, b\n\n\"\n\n\"\n\n*\n\n%\n\"\n\n'\n\"\n\n'\n\n/\n*\n\n'\n\n%\n\"\n/\n\n\"\n\n.\n%\n\n$\n\"\n/\n\n\"\n/\n\n\"\n%\n\na, b\n\n*\n\n\"\n\n\"\n\n'\n\n/\n*\n\n/\n\n%\n\"\n*\n\n\"\n\n\"\n\n%\n/\n\n(\n\n*\n\n/\n'\n*\n'\n\n\"\n/\n\n/\n\"\n\n%\n\n\"\n\n*\n%\n\n\"\nN\n/\n\n/\n\n*\n%\n,\n/\n\n\"\n\n\"\n\n*\n\"\n.\n\n/\n\n\"\n\n'\n\n/\n*\n∥· ∥\n/\n\n*\n%\n,\n\n∀f, g ∈N\n\nα ∈\n\n∥f∥≥\n\n∥f∥\n\nf\n\n∥f\n\ng∥≤∥f∥\n\n∥g∥\n\n∥αf∥\n\n|α| ∥f∥\n\n*\n\n\"\n\n/\n\n*\n/\n\n/\n*\n\n%\n\"\n\n/\n\n\"\n\"\n\n\"\n\n∥f∥\n\nf\n\n\"\n\n\"\n\n\"\n\n\"\n,\n/\n*\n%\n,\n/\n\n\"\n\n*\n\n*\n%\n,\n\n\"\n/\n\n/\n\"\n\n%\n\n\"\n\n*\n%\n\n\"\nE\n/\n\n/\n\n*\n\n%\n*\n'\n\n/\n\n\"\n\n\"\n\n%\n\"\n\n'\n\"\n\n'\n\n/\n*\n⟨·, ·⟩\n/\n\n*\n\n%\n*\n'\n\n∀f, g, h ∈E\n\nα ∈\n\n⟨f, g⟩\n\n⟨g, f⟩\n\n⟨f\n\ng, h⟩\n\n⟨f, h⟩\n\n⟨g, h⟩\n\n⟨αf, g⟩\n\nα⟨f, g⟩\n\n⟨f, f⟩≥\n\n⟨f, f⟩\n\nf\n\n'\n\n/\n\"\n\n\"\n\n\"\n\n\"\n\n*\n\n\"\n\n$\n\"\n\n\"\n\"\n\n\"\n\n*\n%\n\n\"\n*\n%\n,\n*\n\n\"\n\n*\n%\n/\n\nq\n⟨f, f⟩\n\n\"\n\nA\n\nB\n$\n\"\n\n'\n$\n\n\"\n\n*\n\n*\n,\n\"\n*\n%\n,\n\"\n\n,\n\"\n\n%\n/\n\n\"\nR\n\nA\n/\n\n/\n\n*\n$\n\"\n\n/\nB\n\nA ⊂B\n\nB ⊂\n\nA\n\n,\n\n\"\n\n\"\n\n\"\n\n*\n\n%\n\n/\n*\n\n*\n/\n\n/\n\n\"\n\n\"\n/\n\n\"\n%\n\"\n\n/\n\"\n\n*\n\n\"\n\n(\n\n*\n\n\"\n\n/\n\n\"\n\n/\n\n\"\n\n\"\n/\nL\n\n/\n\n\"\n\n/\n%\n\n$\n\n\"\n\n%\n*\n\n\"\n%\n\n(\n*\n\n,\n\n(\n\n(\n\n%\n*\n\n/\n,\n\n/\n*\n\n\"\n,\n\"\n\n/\n\n'\n\n/\n\"\n\n\"\n\n/\n\n.\n\"\n\"\n%\n\n(\n\n-\n\n*\n'\n\n$\n\"\n$\n\n/\n\nL\n\nn\n\n%\n\"\n\"\n\n,\n\n\"\n\n*\n\n/\n\n$\n\"\n%\n\n\"\n\n*\n%\n,\n\"\n\n\"\n/\n\n/\n\n/\n\n-\n\n/\n\n\"\n/\n\n/\n\n*\n\n(\n$\n*\n'\n\"\n\nε\n\n\"\n\n*\n%\n\n(\nε >\n\n)\n\"\n\n%\n.\n\"\n$\n\n*\n\n/\n\n'\n\n/\n*\n\n/\n\n\"\n\n\"\n\n*\n%\n\"\n\n'\n\n/\n*\n\nfi, fj\n/\n\n,\n\n$\n\n∥fi -fj∥inf< ε\n\n/\n\"\n\n%\n\"\n\n'\n\n/\n*\n\n'\n\n/\n*\n\n/\n\n/\n\"\n\n%\n\n'\n\n/\n*\n\nFt\n\n\"\n\n'\n\n/\n*\n/\n\n\"\n\n\"\n\n\"\n\n*\n/\n\nt\n\n*\n%\nFt\n\nf\n\nf\n\nt\n\nFt\n\nf\n\ng\n\nf\n\nt\n\ng\n\nt\n\n.\n)\n\"\n\n'\n\n/\n*\n\n/\n\n$\n*\n'\n\"\n/\n\n\"\n%\n\"\n\"\n\n/\n\nM\n\n|Ft\n\nf\n\n|\n\n|f\n\nt\n\n| ≤M∥f∥Hil ∀t\n\n*\n%\n\nf\n\n\"\n%\n\"\n∥· ∥Hil\n/\n\n\"\n*\n%\n,\n/\n\n\"\n\n/\n\n$\n\"\n%\n\n\"\n\n)\n\"\n\"\n\n'\n\n/\n*\n\n'\n\n/\n*\n\n/\n\n*\n\n$\n*\n'\n\"\n/\n\n\"\n\n,\n/\n\n/\n\n%\n\n/\n\n$\n\"\n%\n\n\"\nL\n\n,\n\n*\n\n'\n\nM\n\"\n\n/\n\n/\n\n\"\n\n\"\n\n,\n\"\n\n*\n\nL\n\n,\n\n%\n\"\n*\n\n\"\n\n\"\n\"\n\n\"\n\n*\n/\n\n/\n\n\"\n\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\nf(x)\n\n\"\n\n*\n\n*\n\n/\n.\n\n/\n\n'\n%\n\"\n\n\"\n\n*\n\n'\n\n/\n*\n\n\"\n\n\"\n\n,\n\"\n*\n%\n,\n$\n'\n\n\"\n(\n\n%\n\"\n\n\"\n%\n(\n/\n\"\n%\n\"\n\n*\n\n\"\n\n*\n\n\"\n%\n*\n,\n\"\n\n'\n%\n\"\n-10\n-8\n-6\n-4\n-2\n-2\n-1.5\n-1\n-0.5\n0.5\nfunction 1\nx\nf(x)\n-10\n-8\n-6\n-4\n-2\n-1\n-0.5\n0.5\n1.5\n2.5\nx\nf(x)\nfunction 2\n\n%\n\"\n\n/\n\n/\n\n$\n\"\n%\n\n\"\n*\n\n%\n\"\n\n'\n\"\n\n'\n\n/\n*\n\n*\n\n*\n,\n\n*\n,\n\n/\nX\n\n/\n\n\"\n%\n*\n\n\"\n%\n\n(\n\n*\n%\n\"\n\nt\n\n\"\n\"\n\n'\n\n/\n*\n\n'\n\n/\n*\n\nFt\n/\n\n$\n*\n'\n\"\n\n/\n\"\n\n%\n\n'\n\n/\n*\n\n\"\n\nX\n$\n\"\n\n*\n,\n\"\n\n\"\n\n*\n%\n\"\n\n,\n\n\"\n\n'\n$\n\n\"\n\n*\n\nd\n*\n%\n\nd\n/\n\n\"\n\n/\n\n(\n,\n,\n\"\n\n%\n/\n\n'\n\n/\n*\nK\n\nX × X →\n\n.\n\n\"\n%\n\"\n\nK\n\nt, s\n\n/\n\n/\n\nn\nX\ni,j\n\ncicjK\n\nti, tj\n\n≥\n\n*\n%\n\n(\nn ∈\n\n*\n/\n\n\"\n*\n\nt\n\n, ..., tn ∈X\n\nc\n\n, ..., cn ∈\n\n.\n\n\"\n\n'\n/\n\n\"\n\n\"\n\n/\n\n/\n*\n\n*\n'\n\n$\n\"\n.\n/\n\"\n/\n\n\"\n%\n,\n\n*\n\n*\n\n/\n\n/\n\"\n\n\"\n,\n/\n\"\n\n/\n\n\"\n\"\n\n*\n\n\"\n,\n\n%\n/\n\nKij\n\nK\n\nti, tj\n\n.\n\n\"\n%\n\"\n\n/\n\n/\n\n*\n%\n\n(\n/\n\n/\n\n\"\n\n*\n%\n\nt\n\n, ..., tn ∈X\n\n\"\n\n$\n*\n\n\"\n/\n\"\n\n'\n\n/\n\n(\n*\n\n%\n/\n\n(\n\n\"\n\n\"\nci\n\n%\n\"\n*\n\n\"\n%\n*\n\n/\n\n\"\n\n\"\n,\n\n%\n/\n\nKij\n/\n\n*\n\n/\n\n/\n\n\"\n\"\n\n/\n\n\"\n\n*\n\n'\n\n*\n\n/\n\n/\n\"\n\n\"\n,\n/\n\"\n\n/\n\n\"\n\n\"\n\n/\n\n/\n*\n\n/\n\n\"\n\n/\n\n\"\n%\n\n'\n%\n\"\n\n%\n\"\n*\n\n\"\n/\n\n*\n\n/\n\n\"\n\n*\n\n'\n\n/\n.\n\n\"\n\n\"\n\n/\n\n(\n\n\"\n/\n\"\n%\n\"\n\n'\n\n\"\n\n*\n%\n,\n\n%\n/\n\n\"\n\n'\n\n/\n*\n\n)\n\"\n\n*\n\n*\n\n/\n.\n\n\"\n*\n%\n\"\n,\n%\n\"\n\n\"\n\n\"\n%\n\"\n\n!\n*\n%\n\"\n\n\"\n%\n(\n\n\"\n%\n\"\n\"\n\n/\n\n'\n/\n\n'\n\"\n\n*\n\n/\n\n/\n\n\"\n\"\n\n/\n\n\"\n\n'\n\n/\n*\n\n\"\n\n\"\n%\n\"\n%\n*\n'\n\n/\n.\n\"\n%\n\"\n\n%\n\n$\n\n*\n\n\"\n%\n\n\"\n\n(\n\n*\n%\n\"\n\n\"\n%\n(\n\n*\n\n/\n\n/\n\"\n\"\n\n/\n\n\"\n\n'\n\n/\n*\nK\n*\nX × X\n\n\"\n%\n\"\n/\n\n'\n/\n\n'\n\"\n\n*\nX\n\n/\n\nK\n\n/\n\n%\n\nH\n/\n\n\"\n\n*\n%\n\"\n\nx ∈X\n\n\"\n%\n\"\n\"\n\n/\n\n$\n(\n\n\"\n\n/\n\"\n\n%\n\"\n%\n\"\n\n\"\n\n/\n*\n\n\"\n*\n%\n\"\n,\n\n\"\n\n\"\n,\n\"\n\nKx\n*\n\nH\n\n\"\n\n/\n\n\"\n%\n*\n\n\"\n%\n\n(\n\n\"\n$\n(\n\n%\n*\n\n\"\n\n- Fx\n\nf\n\n⟨Kx, f⟩K\n\nf\n\nx\n\n.\n)\n\"\n%\n/\n\nKx\n\nt\n\n)\n\"\n%\nKx\n\nt\n\n$\n\"\n\n*\n'\n.\n\n*\n\n\"\n\"\n\n/\n\"\n\n(\n\n\"\n\n'\n\n/\n*\n\n*\n%\n\n\"\n$\n'\n\n\"\n%\n$\n\"\n\n*\n.\n\n*\n\n\"\n\n/\n\n$\n\"\n%\n\n\"\n\nKx\n\nt\n\n⇔δx\n\nt\n\nf\n\nx\n\n⟨Kx\n\nt\n\n, f\n\nt\n\n⟩K\n⇔f\n\nx\n\n⟨δx\n\nt\n\n, f\n\nt\n\n⟩\nKx\n\nt\n\n∈H\nδx\n\nt\n\n/∈L\n\n.\n\n/\n\n\"\n\n*\n$\n\"\n\n'\n\n\"\n- K\n\nt, x\n\n⟨K\n\nt, ·\n\n, K\n\nx, ·\n\n⟩K.\n\n/\n\n$\n\"\n\n'\n\n\"\nn\nX\ni,j\n\ncicjK\n\nti, tj\n\nn\nX\ni,j\n\ncicj⟨Kti, Ktj⟩K\n\n||\nX\ncjKtj||\n\nK ≥\n\n.\n\n*\n\n\"\n%\n\n\"\n\n(\n\n.\n/\n\"\nK\n*\n\"\n\n*\n\n%\n'\n\n\"\n\nH\n\n\"\n\n*\n,\n\n\"\n\n/\n*\n*\n\n\"\n\n\"\n*\n\n'\n\n/\n*\n\n\"\n$\n(\n\n\"\n\n\"\n\nKx\n\n/\n\nx ∈X\n\n/\n\n/\n\"\n%\n%\n*\n'\n\n\"\n\n\"\n\n*\n\n*\n\n)\n\"\n*\n\n%\n*\n'\n\n*\n\n*\n\n'\n\n/\n*\n\nf\n\ng\n/\nH\nf\n\nx\n\ns\nX\ni\n\nαiKxi\n\nx\n\ng\n\nx\n\ns\nX\ni\n\nβiKxi\n\nx\n\n\"\n%\n\"\ns ∈\n\ns\n/\n\n/\n\n\"\n*\n%\n/\n\n/\n\n\"\n\"\n\n\"\n/\n.\n*\n\n\"\n\"\n%\n\"\n\n$\n\"\n\n%\n/\n\n\"\n\n⟨f, g⟩K\n\n* s\nX\ni\n\nαiK\n\nxi, ·\n\n,\ns\nX\ni\n\nβiK\n\nxi, ·\n\n+\nK\n\ns\nX\ni,j\n\nαiβjK\n\nxi, xj\n\n.\n\n\"\n\n/\n\n,\n\"\n\n'\n%\n\"\n\n\"\n\n*\n,\n\n\"\n\n/\n\n(\n*\n\n(\n\n*\n\n\"\n\n/\n\n\"\n'\n\n/\n.\n\n\"\n\n\"\n\n*\n%\n,\n\n∥f∥K\n\n)\n\"\n\"\n\n\"\n\n,\n\n\"\n/\n\n'\n\n%\n\n\"\n*\n\n$\n*\n'\n/\n.\n\n\"\n\n*\n%\n,\n\n*\n%\n%\n\"\n\n*\n\n*\n\"\n\n*\n%\n\n/\n.\n\n*\n,\n\"\n/\n*\n\n/\n,\n\n/\n\n/\n\n(\n\n*\n%\n\n,\n*\n*\n\n\"\n\n*\n\n\"\n\n'\n\n/\n*\n\nf\n\nx\n\nw x\n\nK\n\nx, xi\n\n≡x xi.\n\nf\n\nx\n\nn\nX\ni\n\nαiK\n\nx, xi\n\nn\nX\ni\n\nαixi x\n\nx\nn\nX\ni\n\nαixi\n\nxw\n\nw\n\nPn\ni\n\nαixi\n\n!\n\"\n\n∥f\n\nx\n\n∥\n#\nK\n\nn\nX\ni,j\n\nαiαjxixj\n\nn\nX\ni\n\nαixi\n!\n\nn\nX\nj\n\nαjxj\n\nw\n#\n\n$\n\n&%\n\n'\n\n(\n\n)\n\n)\n\n)\n\n$\n\n&%\n\n(\n\n)\n\n'\n\n$\n\n$\n\n*\n\n,+\n\n)\n\n'\n\n,+\n\n-\n\n$\n\n-\n\n$\n\n\"\n%\n\"\n\n%\n\"\n\n%\n\"\n\"\n\n\"\n\n/\n\"\n\n%\n\n'\n\n/\n*\n\n*\n'\n\n$\n\"\n'\n\n\"\n\n*\n\n\"\n\n%\n\n\"\n\n\"\n\n\"\n\n*\n\n/\n\n\"\n\n\"\n\n/\n\n/\n\n/\n*\n$\n\"\n\n*\n,\n\"\n\n\"\n%\n\n%\n.\n\"\n%\n\n*\n\n\"\n/\n\n%\n\"\n\n'\n/\n%\n\"\n\n*\n\n\"\n\n%\n\n\"\n\n\"\n\n\"\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\nx\nf(x)\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\nx\nf(X)\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\nx\nf(x)\n\n\"\n%\n\"\n\"\n\n/\n\n/\n\n(\n/\n\n%\n*\n'\n\n\"\n/\n\n\"\n\n%\n/\n.\n\n\"\n*\n%\n(\n$\n(\n/\n%\n*\n\n/\n\n-\n*\n.\n.\n/\n*\n\n/\n%\n*\n\n/\n\n/\n\n%\n*\n'\n\n\"\n)\n/\n*\n*\n\n%\n\"\n.\n'\n\n%\n/\n\n/\n*\n/\n\n\"\n\n%\n/\n.\n\n\"\n*\n%\n(\n\n*\n%\n\"\n\n/\n\n*\n\n(\n/\n,\n\n/\n\n/\n\n(\n\n$\n\"\n\n'\n\n\"\n\n\"\n(\n\"\n\n,\n\n/\n\n(\n\n/\n\n(\n\n*\n\n\"\n\n/\n\n\"\n\n*\n'\n$\n*\n'\n\"\n*\n,\n\n/\n\n/\n\n\"\n\n/\n\n*\n\n/\n\n'\n\n\"\n%\n\"\n\n*\n'\n%\n\n\"\n\n\"\n%\n\"\n'\n\n\"\n,\n'\n\n\"\n\n%\n\n/\n\"\n%\n/\n\n%\n*\n\n/\n,\n\n/\n*\n\n\"\n*\n%\n(\n\n\"\n.\n\n$\n\n*\n,\n\n'\n\n\"\n%\n\n/\n\n/\n*\n\n\"\n.\n\n\"\n%\n\n\"\n%\n*\n\n)\n*\n%\n%\n\"\n\n-\n*\n.\n.\n/\n*\n\n\"\n\n/\n\n\"\n\n\"\n%\n\"\n\n\"\n\n/\n\n,\n/\n/\n,\n/\n\n/\n*\n*\n\n\"\n\n*\n\n*\n\n/\n.\n\n'\n\n/\n*\n\n%\n\n/\n.\n*\n\n\"\n\n%\n\n/\n/\n.\n\"\n%\n%\n*\n%\n\n\"\n\n*\n,\n\n\"\n\n/\n\n(\n*\n\n\"\n(\n\n*\n\n\"\n\n/\n\n,\n\"\n\n'\n%\n\"\n$\n(\n\n\"\n%\n\n/\n'\n\n*\n\n\"\n$\n\n/\n\n\"\n\nfS\n\n%\n.\n,\n/\nf∈H\n\nl\nl\nX\ni\n\nV\n\nf\n\nxi\n\n, yi\n\nλ∥f∥\n\nK\n\n\"\n%\n\"\nH\n/\n\n\"\n\n\"\n\n\"\n$\n(\n\n\"\n\"\n%\n\"\n\nK\n\n·, ·\n\n)\n\"\n\n*\n\n'\n\n/\n*\n\n*\n\n\"\n)\n/\n*\n*\n\n%\n\"\n.\n'\n\n%\n/\n\n/\n*\n\n%\n*\n$\n\"\n,\n,\n/\nf∈H\n\nl\nl\nX\ni\n\nV\n\nyi, f\n\nxi\n\nλ∥f∥\n\nK\n\n$\n\"\n\n%\n/\n\n\"\n/\n\n\"\n\n*\n%\n,\nf\n\nx\n\nl\nX\ni\n\nciK\n\nx, xi\n\n.\n)\n/\n\n\"\n*\n%\n\"\n,\n/\n\n\"\n\n\"\n\"\n/\n.\n\n(\n'\n\n\"\n\n'\n\n/\n\n(\n\n*\n\n*\n\n\"\n\n\"\n)\n/\n*\n*\n\n%\n\"\n.\n'\n\n%\n/\n\n/\n*\n\n%\n*\n$\n\"\n,\n\n\"\n\"\n\"\n*\n\n(\n\n\"\n$\n\"\n\n'\n\n/\n*\n*\n\n\"\n\n*\n%\n,\nf\n\nx\n\nPl\ni\n\nciK\n\nx, xi\n\n-\n'\n\n/\n\"\n%\n\"\n\n(\n\n\"\n\n\"\n\n*\n*\n/\n\n\"\nci\n\n\"\n\n*\n\n/\n\"\n%\n\n\"\n\n'\n\n%\n\"\n\n*\n\n\"\n\n(\n\n\"\n*\n\n\"\n%\n\n*\n%\nR dtf\n\nt\n\n∂\n∂f\n\n/\n\n\"\n/\n\n\"\n.\n%\n\n*\n\n\"\n\n'\n\n/\n*\n\n\"\n%\n/\n\n/\n\"\n\n*\n\nl\nl\nX\ni\n\nf\n\nxi\n\n-yi\n\nλ∥f∥\n\nK\n\n\"\n\n/\n\n\"\n\n'\n\n\"\n%\n*\n\n)\n'\n\nl\nl\nX\ni\n\nf\n\nxi\n\n-yi\n\nf\n\nxi\n\nλ⟨f, f⟩\n\n.\n\n)\n\"\n\"\n\n'\n\n/\n*\n,\n'\n\n$\n\"\n\n/\n\n*\n%\n\n(\nf\n\n%\n\n/\n\n'\n\n%\n\n\"\n\n/\n.\nf\n\nKx\n.\n/\n\"\n\nl\nl\nX\ni\n\nf\n\nxi\n\n-yi\n\nKx\n\nxi\n\nλ⟨f, Kx⟩\n\nλl\nl\nX\ni\n\nyi -f\n\nxi\n\nKx\n\nxi\n\n⟨f, Kx⟩\n\nλl\nl\nX\ni\n\nyi -f\n\nxi\n\nKx\n\nxi\n\nf\n\nx\n\n*\n\n\"\n\n%\n/\n\n\"\nf\n\nx\n\nl\nX\ni\n\nciKxi\n\nx\n\n\"\n%\n\"\nci\n\nyi -f\n\nxi\n\nlλ\n\n/\n\n\"\n⟨f, Kx⟩\n\nf\n\nx\n\n\"\n\"\n\n*\n\n\"\n\n\"\n\n/\n\n'\n(\n)\n/\n*\n*\n\n%\n\"\n.\n'\n\n%\n/\n\n/\n*\n\n/\n\n/\n\"\n%\n\"\n\n*\n\n'\n\n/\n*\n\n*\n%\n$\n*\n\n%\n\"\n.\n%\n\"\n\n/\n*\n\n/\n\n/\n*\n\n\"\n\n/\n\n%\n\n/\n\n\"\n\n'\n\n%\n\"\n\n*\n\n\"\n\n*\n\n/\n\"\n%\n\n*\n\n'\n\n/\n*"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/c60831d52fe513738a763ec58064392d_lecture03.pdf",
      "content": "Lecture 3: Reproducing Kernel Hilbert\nSpaces\nSayan Mukherjee and Tomaso Poggio\n\nDescription\nWe introduce a particularly useful family of hypothesis spaces called\nReproducing Kernel Hilbert Spaces (RKHS) that have a key role in the\ntheory of learning. We first provide the necessary background in\nfunctional analysis and then define RKHS using the reproducing property.\nWe then derive the general solution of Tikhonov regularization in RKHS -\n- the magic algorithm.\n\nSuggested Reading\n-\nAronszajn. Theory of reproducing kernels. Transactions of the\nAmerican Mathematical Society, 686, 337-404, 1950.\n-\nCucker and Smale. On the mathematical foundations of\nlearning. Bulletin of the American Mathematical Society, 2002.\n-\nEvgeniou, Pontil and Poggio. Regularization Networks and\nSupport Vector Machines Advances in Computational\nMathematics, 2000.\n-\nGirosi, F. An Equivalence between Sparse Approximation and\nSupport Vector Machines. Neural Computation, Vol. 10, 1455-\n1480, 1998. (Appendix A)\n-\nWahba, G. Spline Models for Observational Data Series in\nApplied Mathematics, Vol. 59, SIAM, 1990. (Chapter 1)"
    },
    {
      "category": "Resource",
      "title": "class04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-520-statistical-learning-theory-and-applications-spring-2003/a6ec2449cfe17040762f01190d442f32_class04.pdf",
      "content": "\"!\n#\n#\n\n%$\n&\n\n')(\n*+\n,\n!\n+\n-\n\n.\n/\n-\n!\n/\n\n-\n/\n(\n\n+\n(\n#\n#\n,\n#\n\n(\n#\n\n,\n!\n+\n(\n\n#\n#\n-\n\n#\n\n,\n!\n+\n(\n\n#\n#\n+\n(\n\n+\n(\n#\n#\n-\n\n,\n\n!\n\n!\n\n!\n\n+\n\n!\n\n(\n#\n-\n\n!\n\n!\n\n(\n#\n\n!\n\n(\n!\n+\n(\n+\n(\n\n-\n\n\"!\n#\n#\n\n!\n\n!\n#\n+\n(\n\n+\n(\n#\n#\n-\n\n(\n/\n(\n\n!\n\n,\n\"!\n+\n\n(\n\n+\n(\n\n+\n(\n#\n#\n\n$\n\n(\n!\n+\n(\n\n(\n!\n\n+\n!\n\n#\n(\n\nx\n\n, y\n\n, . . . ,\n\nxl, yl\n\n(\n\n\"!\n#\n#\n\n!\n\n$\n\n(\nyi\n!\n+\n(\n\n(\n\n!\n\n#\n\n(\n!\n+\n!\n,\n\nf\n\n+\n(\n\n(\ny\n\n!\n\n,\n(\n#\n!\n#\n#\n\n!\n\n(\n\n(\n\n*\n#\n(\n+\n\n(\n\nx\n\n!\n\n,\n(\n#\n\n(\n\n#\n(\n\n,\n+\n+\n(\n\n+\n(\n#\n#\n\n!\n#\n!\n#\n\n(\n,\n\nf\n\n!\n\n#\n\n(\n#\n!\n\n+\n(\n\n,\n\"!\n+\n!\n\n+\n\n*\n(\n\nf\n\n!\n+\n\nf∈H\n&\nl\nl\nX\ni\n\nV\n\nf\n\nxi\n\n, yi\n\nλ∥f∥\n\nK\n\n,\n\n-\n#\n\n(\n\n-\n\n(\n\n+\n\n*\n(\n\n$\n\n(\n(\n(\n\n#\n(\n!\n\n#\n#\n,\n\nV\n!\n\n!\n(\n+\n(\n\nK\n\n'\n\n+\n+\n(\n\n+\n(\n#\n#\n$\n!\n!\n\n,\n+\n!\n\n(\n\n#\n#\n,\n\n#\n\n(\n#\n\n,\n!\n+\n(\n\n#\n#\nV\n\nf\n\nx\n\n, y\n\nf\n\nx\n\n-y\n\n-3\n-2\n-1\ny-f(x)\nL2 loss\n\n#\n\n(\n#\n\n,\n!\n+\n(\n\n#\n#\n$\n\n,\n+\n\n+\n\n*\n\n(\n\n*\n(\n\n(\n#\nf\n\n!\n+\n\nf∈H\n&\nl\nl\nX\ni\n\nf\n\nxi\n\n-yi\n\nλ∥f∥\n\nK.\n\n(\n#\n\n,\n\n(\n\n+\n(\n\n,\n\"!\n+\n!\n\n+\n\n*\n(\n\nf∈H\n&\nl\nl\nX\ni\n\nV\n\nyi, f\n\nxi\n\nλ∥f∥\n\nK\n\n!\n*\n(\n\n+\n\n(\n\n(\n\n+\n\nf\n\nx\n\nl\nX\ni\n\nciK\n\nx, xi\n\n.\n\n#\n\n(\n\n+\n(\n\n#\n(\n\n(\n(\n\n-\n,\n#\n(\n,\n\n#\n!\n-\n#\n\n!\n\n#\n\n(\n\n(\n\n+\n(\n\n,\n\n!\n+\n!\n\n+\n\n*\n(\n\n$\n\n(\n(\n(\n\n-\n\n(\n*\n(\n#\n\n,\n\n(\n\n+\n\nf\n\nx\n\nPl\ni\n\nciK\n\nx, xi\n\n,\n\n(\n+\n(\n\n-\n$\n!\n\n(\n\n!\n\n(\n\n#\n\n(\nci\n\n(\n\n,\n#\n(\n\n(\n#\n-\n\n*\n\nK\n\n+\n(\n(\n+\n\n(\n\n(\n+\n\n(\n(\n+\n(\n\n,\n\nK\n\n(\nl\n\n*\n-\n\nl\n\n!\n\n+\n\nK\n\n(\n\n(\n\n!\nKij ≡K\n\nxi, xj\n\n#\n\n#\n\n(\n\n$\n\n#\n\n(\n+\n\n(\n\n,\n\n,\n\n,\n+\n,\n\nf\n\nx\n\nl\nX\ni\n\nciK\n\nx, xi\n\n.\n!\n\n(\n\n+\n!\n\nxj\n\nf\n\nxj\n\nl\nX\ni\n\nK\n\nxi, xj\n\nci\n\nKc\n\nj\n\n#\n\n#\n\n!\n\n$\n\n(\n!\n\n-\n\n(\n/\n(\n\n+\n(\n#\n(\n\n(\n+\n\n(\n\n+\n(\n\n,\n+\n\n!\n\n+\n\n*\n(\n\n$\n+\n(\n\n+\n\n,\n\"!\n\n!\n#\n\n#\n\n(\n#\n\n,\n!\n+\n(\n\n#\n#\n$\n\n,\n+\n\n+\n\n*\n\n(\n\n*\n(\n\n(\n#\nf\n\nf∈H\n&\nl\n\nKc -y\n\nλ∥f∥\n\nK.\n\n/\n(\n\n!\n\n!\n\n(\n\n!\n\n(\n!\n,\n\nf\n\nx\n\nl\nX\ni\n\nciK\n\nxi, x\n\n,\n\n(\n\n(\n\n!\n\n(\n||f||\n\nK\n\ncT Kc.\n\n,\n*\n#\n\n,\n\n$\n\n,\n+\n\n!\n\n+\n\n*\n(\n\n#\n\n(\n\n+\n(\n\n-\n!\n\n+\n\n*\n(\n\nc\n\nf\n\n!\n+\n\nc∈\nl\n&\nl\n\nKc -y\n\nλcT Kc.\n\n(\n!\n+\n(\n\n+\n-\n\n(\ng\n\nc\n\n$\n\n(\n+\n(\ng\n\nc\n\n&\nl\n\nKc -y\n\nλcT Kc\n\n#\n#\n!\n\n$\n\n,\n\nc\n$\n#\n\n(\n\n!\n\n(\n\n#\n\n-\n*\n-\n\n!\n\n(\n\n(\n+\n\n!\n\n(\n\n+\n(\n#\n\n(\n\nc\n$\n\n(\n#\n(\n\n#\n\n(\n+\n\n!\n\n(\n\n∂g\n\nc\n\n∂c\n\nlK\n\nKc -y\n\nλKc.\n\n(\n\n(\n\n(\n+\n\n!\n\n(\n\n!\n\n\"!\n-\n\n#\n\n(\n\n!\n\n$\n\nlK\n\nKc -y\n\nλKc\n\n→\nK\n\nKc\n\nλlKc\n\nKy\n→\n\nK\n\nλlI\n\nc\n\ny\n→\nc\n\nK\n\nλlI\n\n-\n\ny\n\n(\n\n!\n\n+\n\nK\n\nλlI\n#\n\n#\n\n(\n\n(\n\n(\n!\n\n*\n(\n\n(\n\n(\n\nλ\n#\n\n#\n\n!\n\n-\n\n(\n\n!\n\n+\n\nK\n\nλlI\n\n#\n\n,\n!\n+\n!\n\n(\n(\n\n*\n(\n\n(\n+\n\n*\n(\nλ >\n\n#\nλ →\n\n$\n\n(\n+\n(\n\n,\n\"!\n+\n(\n\n(\n!\n#\n\n#\n\n,\n!\n+\n(\n#\n#\n\n,\n\n(\n#\n\n(\n#\n\n!\n\n!\n+\n\n!\n,\n#\n#\n!\n\n(\n!\n#\n\n#\n\n,\n!\n+\n(\n#\n#\n\n,\n\n(\n#\n\n(\n(\n\n+\n\n!\n\n#\n#\n\n#\nλ →inf\n$\n\n(\n#\n\n,\n\n(\n#\n\nf\n\nx\n\n-\n\n+\n!\n\n(\n$\n\n(\n\n!\n\n,\n!\n\n-\n\n(\n+\n\nK\n\nλlI\n\n$\n*\n,\n\n#\n\n(\n!\n\n,\n#\n(\n!\n!\n\n+\n\n+\n#\n\n(\n!\n+\n#\n-\n#\n\n(\n\n#\n\n-\n\n+\n\n(\n+\n\n,\n#\n(\n\n#\n!\n\n+\n\n!\n\n$\n\n(\n(\n(\n\n,\n\n(\n!\n\n#\n\n+\n(\n\n(\n(\n\n+\n(\n(\n+\n(\n\n!\n\n+\n\nK\n\n#\n\n!\n(\n#\n\n+\n!\n\n!\n\n+\n,\n#\n(\n\n(\n+\n-\n\"!\n+\n\n(\n\n+\n!\n\n#\n(\n\n#\n\n(\n\n,\n\n!\n\n(\n\n+\n!\n\n(\n\n!\n\n+\n\n#\n!\n\n,\n\n!\n+\n!\n\n+\n\n+\n#\n\n#\n\n(\n\n(\n\n(\n\n(\n!\n+\n#\n-\n#\n\n(\n\n#\n\n'\n\n+\n\n(\n\n,\n+\n\n#\n(\n#\n\n#\n\n\"!\n#\n#\n$\n\n(\n(\n(\n\n!\n\n#\n!\n\n!\n\n+\n\n(\n\n!\n\n+\n\n(\n+\n!\n\n#\n\n,\n\n-\n\n!\n\n(\n\n+\nv\n*\n-\n\n(\n\n!\n\n+\n\nA\n\n(\n\n!\n\n!\n\n+\n\nA\n(\n(\n\n!\n\n!\n-\n#\n*\n(\n#\n,\n\n(\n\n(\n\n-\n$\n\n(\n\n,\n#\n\n(\n(\n\n#\n\n(\n\n!\n-\n\n+\n\n!\n\n+\n\n,\n\nAv\n\n'\n\n+\n\n+\n\n!\n+\n-\n\n#\n\n(\n#\n(\n\n(\n\n(\n#\n-\n#\n\n(\n\n#\n$\n\n*\n(\n\n(\n\n(\n\n+\n(\n\n(\n\n#\n\n!\n*\n(\n\n,\n\n!\n#\n\n(\n+\n\n(\n+\n(\n#\n!\n\n!\n-\n\n,\n\n-\n*\n-\nA\n\n,\n\n-\n. . .\n\n,\n\n#\n(\n\n,\n+\n(\n+\n(\n\nK\n#\n\n(\n!\n+\n\nK\n\nx, y\n\nx · y.\n\n(\n\n,\n+\n#\n\n,\n\nx\n\n!\n*\n(\n\n+\n\n(\n!\n#\nf\n\nx\n\nX\ncixi · x\n\nX\ncixi\n\n· x\n≡\nw · x,\n!\n\n(\n\n!\n!\n\n-\n\n,\n+\n,\n\n(\n\n(\n\n!\n\n(\n#\n\n(\nd\n+\n!\n\n(\n+\n\n!\n\n(\nld\n\n#\n#\n!\n\n(\n(\n+\n!\n\n+\n\n(\n+\n\n-\n\n+\n(\n\n,\n\"!\n+\n\n!\n\n!\n\n(\n!\n+\n(\n+\n(\n\n$\n\n+\n(\n\"!\n\n(\n\n(\n,\n#\n(\n\n(\n#\n\n,\n!\n+\n(\n\n#\n#\n\n(\n\n!\n,\n#\n(\n\n(\n\n!\n\n+\n\n(\n\n!\n\n,\n\n(\n#\n!\n\n#\n\n+\n#\n\n+\n(\n\n,\n\n!\n+\n(\n\n(\n!\n#\n\n#\n\n,\n!\n+\n(\n#\n+\n(\n\n+\n(\n#\n#\n\n!\n\n(\n!\n+\n(\n+\n(\n\nK\n\nx1, x2\n\nx1 · x2\n\n!\n!\n+\n*\n\n+\n!\n+\n-\n(\n+\n(\n\n$\n\n(\n\n,\n#\n\n+\n\n!\n\n+\n\n,\n\nKv\n(\n\n-\n\n(\n\n,\n\n-\n!\n\n(\n\n+\n*\n-\nK\n\n(\n\n(\n!\n+\n(\n+\n(\n\n$\n\n(\n\n(\n\n!\n\nK\n\nAAT\n$\n\n(\n+\n(\nA\n#\n!\n\n!\n\n+\n\n(\n\n!\n\n!\n\n#\n!\n#\n+\n\n(\n\n+\n#\n\n#\n\n#\n\nK\n\nλlI\n\nv\n\nAAT\n\nλlI\n\nv\n\nA\n\nATv\n\nλlIv\n\n,\n\n#\n(\n\n(\n\n!\n\n(\nl\n\n#\nd\n\n(\n#\n#\n\n'\n\n+\n\n(\n(\n+\n(\n\n!\n\n+\n\nK\n(\n\n-\n\n!\n(\n#\nl\n\nd\n\n(\n$\n!\n\n,\n\n-\n\n!\n\n(\n\n+\n*\n-\nK\n\n!\n(\n#\nl\n\n(\n\n(\n,\n#\n(\n\n(\n\n(\n!\n+\n+\n(\n\n+\n(\n#\n(\n\n!\n\n$\n\n(\n\n!\n-\n\n+\n\n(\n(\n+\n(\n\n!\n\n+\n\n$\n!\n\n,\n\n-\n\n!\n\n(\n\n+\n*\n-\nK\n\n!\n(\n#\n\ndl\n\n(\n\nd << l\n$\n\n(\n#\n!\n\n(\n!\n\n+\n\n!\n\n(\n\n-\n!\n!\n\n+\n\nl\n\nd\n\n(\n+\n\n(\n+\n!\n\n(\n\n(\n\n+\n-\n#\n!\n\n#\n!\n+\n(\n(\n\n(\n\n+\n(\n\n+\n\n!\n\n$\n!\n#\n\n(\n\n!\n\n#\n\n+\n(\n\n(\n(\n+\n(\n\n!\n\n+\n\n!\n\n!\n\n+\n\"!\n+\n\n(\n\n+\n!\n\n#\n(\n\n#\n$\n!\n\n(\n+\n(\n\n+\n(\n\n,\n\n(\n\n(\n(\n\n+\n(\n#\n\n(\n(\n+\n(\n\n!\n\n+\n\n!\n#\n(\n(\n\n(\n\n$\n(\n!\n\n(\n+\n!\n\n,\n\n#\n\nl\n\nd\n\n(\n\n#\n\n(\n\n!\n\n(\n\n+\n!\n\n!\n\n!\n!\n+\n(\n#\n\n!\n+\n#\n(\n\n(\n-\n\n#\n#\n\n!\n\"!\n+\n\n(\n,\n\n*\n(\n+\n\n(\n#\n#\n$\n*\n,\n\n(\n\n!\n\n+\n\n-\n\n(\n#\n#\n\n+\n(\n!\n\n!\n+\n(\n\n(\n+\n\n$\n\n(\n\n#\n\n,\n\n-\n\n!\n\n(\n\n+\n*\n-\nK\n\n!\n*\n(\n\n+\n\n(\n!\n#\n\ndl\n$\n\n(\n+\n(\n\nd\n#\n\n(\n\n,\n\n*\n(\n+\n\n(\n+\n\n(\n\n+\n(\n#\n\n(\n+\n\n!\n\n!\n\n#\n#\n\n(\n\n(\n\n!\n#\n(\n\n+\n!\n\n!\n\n#\n+\n(\n\"!\n\n(\n\n$\n\n(\n+\n(\n\n(\n\n(\n#\n#\n\n+\n+\n(\n#\n\n(\n\n+\n\n#\n!\n\n!\n+\n-\n\n(\n+\n(\n\n!\n-\n*\n(\n\n(\n#\n\n,\n#\n!\n\n#\n\n+\n\n#\n$\n*\n,\n\n-\n!\n(\n\n,\n\n+\n(\n\n!\n\n(\n!\n+\n!\n-\n\n(\n\n,\n\n(\n\n(\n+\n(\n#\n\n#\n\n,\n#\n\n+\n,\n#\n\n(\n!\n*\n\n(\n!\n\n+\n\n+\n\n-\n\n#\n\n$\n\n(\n!\n+\n(\n(\n#\n#\n(\n\n!\n\n-\n\n+\n(\n!\n\n,\n+\n\n\"!\n#\n#\n\n!\n\n+\n\n*\n\n(\n\n!\n#\n!\n+\n(\n\n+\n(\n#\n#\n\n+\n\n*\n(\n\ny\n\n!\n\n,\n(\n#\n\n&\n\n+\n\n&\n\n'\n\n+\n!\n\n(\n(\n+\n!\n\n(\n!\n+\n(\n+\n(\n\n$\n\n#\n\n(\n\n(\n\n+\n\n*\n\n(\n\n!\n#\n\n(\n\n(\n\n$\n\n(\n\n,\n#\n\n*\n(\n\n*\n-\n\n,\n\n(\n(\n\n+\n(\nK\n\n!\n\n+\n\n(\n\n#\n(\n(\n\n#\n\n!\n\n#\n\n(\n!\n#\n\n!\n\n(\n\n$\n\n(\n\n#\n\n,\n#\n#\n\n(\n(\n\n\"!\n#\n#\n$\n#\n\n+\n(\n(\n+\n!\n*\n\n(\n\n/\n\n+\n\"!\n+\n\n(\n\n(\n!\n+\n\n+\n\n*\n(\n\n#\n\n#\n\n(\n+\n(\n!\n\n!\n-\n!\n+\n\n,\n\n#\n\n,\n\n#\n(\n\n!\n\n(\n\n+\n\n!\n\n!\n-\n!\n\n*\n,\n\nM\n\n,\n+\n\n!\n\n!\n\n#\n$\n\n(\n+\n(\nM << l\n\n(\n\n(\n\n-\n(\n(\n\n(\nM\n\nd\n\n+\n\n,\n+\n(\n\n$\n#\n\n!\n\n(\n+\n(\n+\n(\n\n!\n\n+\n\n$\n!\n\n(\n\n-\n(\n(\n\n(\nM\n\n+\n(\n!\n\n(\n+\n!\n\n+\n(\n!\n\n$\n#\n\n(\n\n$\n\n(\n\n!\n\n(\n\n,\n\n!\n\n!\n\n*\n(\n\n#\n!\n-\n&\n$\n\n$\n\n#\n.\n\n(\n#\n#\n\n#\n\n+\n\n,\n#\n\n(\n\n(\n(\n+\n!\n\n$\n\n(\n\n#\n(\n!\n\n,\n+\n!\n\n-\n\n,\n\n#\n(\n$\n#\n\n(\n!\n\n+\n\n!\n\n!\n-\n\n!\n\n!\n$\n\n(\n+\n(\n#\n\n+\n\n,\n+\n\n-\n\n(\n#\n#\n#\n\n!\n\n(\n,\n+\n\n(\n+\n\n#\n\n(\n!\n\n!\n\n,\n\n#\n\n(\n\n+\n\nf\n\nx\n\nl\nX\ni\n\nciK\n\nxi, x\n\n,\n\n(\n\n-\n!\n\nf\n\nx\n\nM\nX\ni\n\nciK\n\nxi, x\n\n,\n\n(\n+\n(\nM << l\n\n(\n+\n\n+\n\n#\n$\n\n(\n\n-\n!\n\n(\n\n+\n#\n\nM\n\n#\n\n!\n\n(\n\n(\n+\n\nci\n\n(\n\n(\n+\n$\n\n(\n#\n\n(\n!\n#\n,\n+\n(\n\n(\n\n#\n#\n!\n\n!\n\nl\n\n#\n\n(\n\n(\n\n(\nKMM\n\n*\n(\n\n(\n(\n+\n(\n\n!\n\n+\n\n,\n#\n\n(\nM\n\n#\n\n(\n\n+\n(\n,\n#\n\n+\n(\n\n+\n(\n#\n(\n\n,\n+\n,\n\n$\n!\n\nKML\n\n*\n(\n\n(\n(\n+\n(\n\n+\n\n,\n\n*\n(\n\n(\n(\n\n#\n(\nM\n\n#\n!\n\n(\n+\n(\n\n!\n\nL\n\n#\n$\n\n(\n\n!\n\n(\n+\n\n(\n\n(\n\n+\n\n!\n\n(\n\n!\n\n+\n\n*\n(\n\n*\n(\n\n(\n#\n\nKMLKLM\n\nKMMλl\n\nKMLy,\n\n#\n!\n\n!\n!\nM\n\n*\n-\n\nM\n#\n-\n#\n\n(\n\n!\n+\n,\n#\n!\n,\n\n+\n#\n\n!\n\n(\n+\n(\n\n+\n\n(\n\n+\n(\n#\n,\n\n#\n\n#\n\n+\n\n#\n\n!\n+\n!\n\n+\n\n!\n\n(\n#\n$\n!\n\n,\n\n(\n\n,\n+\n-\n#\n#\n\n,\n\n#\n\n#\n*\n(\n\n\"!\n#\n#\n\n+\n\n("
    }
  ]
}