{
  "course_name": "Program Analysis",
  "course_description": "6.883 is a graduate seminar that investigates a variety of program analysis techniques that address software engineering tasks. Static analysis topics include abstract interpretation (dataflow), type systems, model checking, decision procedures (SAT, BDDs), theorem-proving. Dynamic analysis topics include testing, fault isolation (debugging), model inference, and visualization. While the course focuses on the design and implementation of programming tools, the material will be useful to anyone who wishes to improve his or her programming or understand the state of the art. Students are expected to read classic and current technical papers, actively participate in class discussion, perform small exercises that provide experience with a variety of tools, and complete a team research project.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCourse Description\n\nSoftware is becoming ever more complex and difficult to understand, at the same time as it is used ever more pervasively. It is hopeless to understand how software systems work (or why they do not work!) without automated assistance. Programmers need tool assistance during design, implementation, testing, debugging, and modification (\"maintenance\"). This graduate seminar will investigate a variety of program analysis techniques that address these software engineering tasks.\n\nStatic analysis topics include abstract interpretation (dataflow), type systems, model checking, decision procedures (SAT, BDDs), theorem-proving. Dynamic analysis topics include testing, fault isolation (debugging), model inference, and visualization. While the course focuses on the design and implementation of programming tools, the material will be useful to anyone who wishes to improve his or her programming or understand the state of the art.\n\nStudents are expected to read classic and current technical papers, actively participate in class discussion, perform small exercises that provide experience with a variety of tools, and complete a team research project. Sample projects will be provided, but students are also free to propose their own, particularly ones motivated by their own problems experienced while programming. Examples include proposing and evaluating a fundamental new technique; developing and assessing new algorithms to replace currently-used ones; translating a methodology to a new problem domain; evaluation of proposed techniques (for instance, via a user study); and applying known techniques to new problem domains, such as operating systems, networks, embedded systems, security, biology, aerospace, etc.\n\nPrerequisites\n\n6.170 and 6.042J, or instructor's permission. 6.035 or 6.821 is helpful but not required.\n\nCourse Structure\n\n6.883 is a graduate paper-reading seminar. Each class session will begin with a brief discussion and presentation of material, after (or during) which the floor will be open to rebuttals, discussion of related work, criticism, brainstorming about follow-on research, etc. At this level in your career, you should no longer be a passive listener to lectures but an active participant in the discussion.\n\nTo help you prepare, you will write a one-paragraph commentary on each paper no later than 9 AM on the day\nbefore\nthe class meets to discuss the paper. You will submit your commentary for viewing by the instructor and by other students. The commentary should reflect your understanding and analysis of the issues raised by the paper, and should also help direct (both your and others') preparation for in-class discussion.\n\nYou may write the commentary in whatever style you prefer that meets the goals listed above. One good format for the commentary is to critique the paper, listing the following three points: its biggest contribution (and, briefly, why that result was not already obvious), its biggest mistake (in motivation, methodology, algorithm, data analysis, conclusions, or some other area), and the biggest question that it raises (or the most interesting and important follow-on work that it suggests). Another acceptable format is to summarize the paper, describing its thesis, approach, and conclusions, and stating why it is significant. The commentary should also list questions that you have about the paper, such as about technical points or connections to related work.\n\nIf you read the paper and there are issues you do not understand, then by all means ask questions about them and we will all gain by the discussion. Remember, if you have a question, it is likely that many other people have the same question but are too shy (or vain, or insecure) to ask it; they will appreciate your raising the point. However, do come to class prepared: carefully read the paper and get as much as you can out of it on your own. Doing so will make the class time that much more productive.\n\nEach student will present one paper during the semester. (The presentations will be done by pairs of students if there are enough students registered.) The lecturer will present the other papers. Presenters will meet with the lecturer a week before the class meeting to receive feedback and improve their presentation.\n\nGrading\n\nGrades will be assigned based on the project (this is the largest factor), class participation (during class meeting) and occasional homework. As this is a graduate class, the class is likely to be A-centered, but students are not guaranteed a grade of A (or even a passing grade).",
  "files": [
    {
      "category": "Resource",
      "title": "assign1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/ddf5fdb1f94d903e73654634e91ca763_assign1.pdf",
      "content": "6.883: Program Analysis\nAssignment 1\nDue: Tuesday, September 13 at 11am\nIn 1-2 pages, list several difficulties that you often encounter during software development -- design,\nimplementation, documentation, testing, maintenance, or otherwise. For each such difficulty, discuss the\nunderlying cause. Is there some way that tools could help?\nBelow are some examples (you should come up with your own). Your list need not be nearly this long,\nbut each item should be more substantial. Discuss the problem: motivate why people should care about it,\nexplain why it is not easy to solve or work around (and speculate on why no one has solved it to date), and\nbrainstorm some possible solutions.\nSubmit this assignment to the lecturer electronically in a single file (text, HTML, PDF, or\nGhostscriptviewable PostScript). This will facilitate sharing them with the class.\n- In dynamicallytyped languages, it is easy to apply illegal operations to data, and for such errors to\nbe latent for long periods until some input exposes them. Even if easily reproducible, they might be\nexposed only after other longrunning computations, making them difficult to detect. This sort of error\nis particularly frequent when accessing deeplynested data structures: it is easy to forget one level of\ndereference.\n- Interning values (replacing them by a canonical version) saves space, since only one version needs to be\nstored, and also saves time, since equality testing can be performed via pointer comparisons. Failure\nto properly intern can be extremely hard to track down, however.\n- Reusing code requires an understanding of its behavior. Most code is not documented, however. This\nlack of specifications and documentation makes it difficult to use the code and difficult to know its\nassumptions or operational parameters (the values over which it has been tested and is known to work).\n- Large components are often more worth reusing than small ones. However, they are also more likely\nto make assumptions (such as that they control execution of the program, that there is no need for\nthread safety, or that batch processing is acceptable) that may not be acceptable to an integrater who\nwishes to use these large components.\n- Test suites are crucial to eliminating bugs and improving confidence in programs, but are difficult and\ntedious to create. For example, it may be difficult to create valid inputs to a program, because there\nare implicit constraints on the input. Given an arbitrary input to the program, it can be difficult to\ndetermine whether the program operated correctly.\n- Large, comprehensive test suites tend to take a long time to execute. That discourages developers from\nusing them as frequently as they ought to, or slows them down waiting for tests to complete when they\ncould be moving on to other tasks.\n- Some program analyses produce so much output that it is difficult or timeconsuming to separate the\nuseful information from spurious reports.\n- \"Maintenance\" activities (modifying a program) tend to degrade its structure at both the macro\n(modules) and micro (specific functions) levels.\n- It is hard to build new analyses for existing languages (such as Java); lots of grunge work is required,\neven though building a front end is no longer considered interesting research. A related problem is\nthat real infrastructures are difficult to use, and it is difficult to integrate new tools with them.\nAvoid discussing implementation annoyances unless you can identify an underlying principle. (Example:\n\"Windows (or Unix) lacks this feature that Unix (or Windows) has,\" or \"My favorite tool does not support\nx.\") Avoid mentioning difficulties in performing tasks that don't matter. (Example: \"I can't determine\nthe cyclomatic complexity of my Tcl code.\" No one cares about cyclomatic complexity any more.) Avoid\nproblems that are extremely minor or that can be solved easily. (Example: \"I often fail to balance delimiters\nbefore attempting to compile a file.\" This costs only a few moments of compile time, and a trivial editor\nextensions can detect it even earlier.)\nWhile some people might be able to knock off an answer to this assignment in a few minutes, it will\nreward careful thought about interesting problems and issues. Please introspect deeply and thoughtfully\nabout program development and maintenance. Doing so will help you in this class, and beyond."
    },
    {
      "category": "Resource",
      "title": "elaboration_hard.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/0661ea5a467802fd16dfb36ff4ffc611_elaboration_hard.pdf",
      "content": "Analyzing the Static Elaboration of Parameterized\nHardware Descriptions\nMichael Pellauer\nMassachusetts Institute of Technology\nComputer Science and Artificial Intelligence Lab\nABSTRACT\nHighlevel synthesis languages allow hardware designers\nto describe modules at higher levels of abstraction and\nuse techniques such as polymorphism to increase source\ncode generality and reusability. Abstract descriptions are\nconcretized into specific hardware structures by the com\npiler during a phase known as static elaboration. Experi\nence has shown that as hardware designers created in\ncreasingly generalized designs, static elaboration can be\ncome the most expensive compilation phase. We demon\nstrate that for a generalized PowerPC processor the\nBluespec compiler spends 70% of total compilation time\nin static elaboration.\nOur contribution is to reduce the execution time of the\nelaboration phase by applying techniques which have\nbeen previously developed to analyze and optimize soft\nware programs to the elaboration of hardware. We discuss\nthe similarities between static elaboration and traditional\nsoftware interpretation. We demonstrate that one particu\nlar technique, HigherOrder Abstract Syntax, resulted in\nan average improvement in elaboration time of 45% on\nfive of six designs in our benchmark suite.\n1. INTRODUCTION\nIt is generally accepted that hardware designs have\nincreased in size and complexity by an order of mag\nnitude since 1995, and are expected to increase by an\nother order of magnitude by 2010. To manage this\ncomplexity designers rely on reusing existing blocks\nof intellectual property (IP reuse). A more generalized\nhardware description has greater potential to be re\nused in future situations.\nIn software, developers can use techniques such\nas polymorphism to achieve generalized, reusable de\nscriptions. This generality is resolved at runtime\nthrough established techniques such as dynamic dis\npatch. In hardware design the output of the compiler\nis a concrete structural hardware description which\nmay be used as input for a synthesis tool. Thus all ab\nstract modules must be concretized during compila\ntion, during a compiler phase known as static elabora\ntion.\nCurrent registertransfer level (RTL) languages\nsuch as Verilog allow designers to write modules\nwhich accept simple numeric\nparameters such as\ndata width, latency, or number of read /write ports. In\nthis situation the static elaboration step is straightfor\nward the hardware compiler syntactically duplicates\ncircuit descriptions and alters the names of calls as\nappropriate. However, recent research in highlevel\nsynthesis has pointed to the advantages of a more\npowerful notion of elaboration [1]. Bluespec is a high\nlevel hardware design language which uses the para\nmetric polymorphism and generalized typeclass sys\ntem of the functional programming language Haskell\n[4].\nDuring static elaboration the Bluespec compiler:\n-\nInstantiates modules with specific parameter\nvalues\n-\nApplies (possiblyrecursive) functions\n-\nResolves polymorphism and dispatching\n-\nExecutes and optimizes staticallydetermined\ncontrolflow\n-\nOptimizes don'tcare values as possible\nIn fact, a Bluespec description is not a single hard\nware design, but rather a program that can be ex\necuted to generate designs. Static elaboration in the\nBluespec compiler can thus be viewed as the execu\ntion of this program by a software interpreter on a\nspecific set of input parameters. This allows the de\nsigner to describe more generalized modules, such as\nsorting buffers parameterized by a sort function, or a\nFast Fourrier Transform applicable to both signed and\nunsigned integers.\nAs we will demonstrate, for designs which make\nheavy use of generality the static elaboration phase is\nthe most expensive phase of the compiler, consuming\nmore than 70% of total compilation time. Amdahl's\nLaw thus dictates that we should focus our optimiza\ntion effort on this phase of the compiler.\n\nAs static elaboration becomes closer to traditional\nsoftware execution, it is natural to explore whether\nmethods of program analysis and optimization de\nveloped for software can be applied to optimize the\nelaboration process.\nThe specific technique we have chosen is Higher\nOrder Abstract Syntax (HOAS), developed by Pfen\nning and Elliot [5]. In this technique traditional ab\nstract syntax trees (ASTs) are combined with lambda\ncalculus constructs to improve the correctness and ef\nficiency of evaluation. We will show that this tech\nnique is also applicable to hardware elaboration, as it\nresulted in an average improvement in elaboration\ntime of 45% on five of six benchmark designs.\nHowever for the sixth testcase, HOAS resulted in an\nexponential increase in elaboration time. We analyze\nthis design in detail and suggest possible resolutions\nof this problem.\n1.1 Paper Organization\nIn Section 2 we review the Guarded Atomic Action\nmodel of hardware description and discuss alternat\nive approaches to hardware elaboration. Then in Sec\ntion 3 we describe our experimental methodology in\ncluding our modeling language, reference interpreter,\nand benchmark suite. Section 4 contains details of the\nHOAS transformation and explores experimental res\nults, including analysis problem it created with one of\nthe benchmark designs. We conclude and present fu\nture work in Section 5.\n2. BACKGROUND\nIn this section we discuss static elaboration, and mo\ntivate its benefits using a shifter circuit. We explore\nthe cost this generality can incur on compilation times\nand demonstrate that there is a need to improve the\nelaboration process. Finally, we review alternative ap\nproaches to static elaboration, and discuss the applic\nability of our approach to these projects.\n2.1 Static Elaboration of Parameterized\nHardware\nAs the complexity of hardware designs has increased,\nthe trend in hardware description languages has been\nto increase the level of abstraction, first from schemat\nic capture to RTL languages such as VHDL or Verilog,\nand more recently to highlevel synthesis languages\nand techniques. The Guarded Atomic Action (GAA)\nmodel of hardware synthesis [4] is an abstraction de\nveloped to give the designer a semantic model to\nreason about the concurrent behavior of a complex\nhardware system. Bluespec SystemVerilog is a high\nlevel synthesis language that is based around the\nGAA model. In this paper, rather than working with\nthe full Bluespec language we will work with a re\nstricted subset which we have named FSpec.\nFSpec resembles the intermediate syntax used by\nthe Bluespec compiler, and retains important features\nsuch as modularity and polymorphism. Details of the\nabstract syntax are given in the Appendix. FSpec al\nlows the user to define modules in GAA style with\nrules (internal behavior) and methods (external inter\nfaces). It includes support for functions, recursion,\nhigherorder datatypes such as arrays, and paramet\nerization both in the value domain (via module para\nmeters) and the type domain (parametric polymorph\nism).\nTo motivate the need for parameterized descrip\ntions, consider the case of a Shifter module which can\nleftshift a bit vector v by n, where n=0..7. Such a cir\ncuit can be implemented using log2(8)=3 multiplexors\nand shifters:\nn\n<< 1\n<< 2\n<< 4\nSo if n is 5, or 3'b101, v will be shifted by 1, then\nthis value will be again shifted 4, for a total of 5. Note\nthat the width of v itself is unimportant to this circuit\n(although it will generally not be less than 8 bits).\nThus when we describe our shifter in FSpec we can\nmake it parametrically polymorphic on the width of\nv:\n\nmodule shifter3<v_width> {\nmethod v_width shift(v_width v,\nbit[3] n) {\nv_width v1, v2, v3;\nv2 = (n[0] == 0) ? v : v << 1;\nv3 = (n[1] == 0) ? v2 : v2 << 2;\nv4 = (n[2] == 0) ? v3 : v3 << 4;\nreturn v4;\n}\n}\nDuring static elaboration the user will specify a con\ncrete value for v_width and the elaborator will add\nthe appropriate number of input /output ports and in\nstantiate shifters and mux circuits of that width.\nWe can also conceive of generalizing this circuit to\nany n: add a mux and a shifter for each bit in the rep\nresentation of n. This generalized shifter can be de\nscribed in FSpec using a whileloop:\nmodule shifterN<v_width, n_width> {\nmethod v_width shift(v_width v,\nn_width n) {\nv_width tmpv;\nint k = 0;\nwhile (k < n_width) {\ntmpv = (n[k] == 0) ?\ntmpv\n: tmpv << (2^k);\nk++;\n}\nreturn tmpv;\n}\n}\nWhat is the meaning of the whileloop and the index\nvariable k in the above code? It does not directly cor\nrespond to hardware in that there is no single circuit\nimplementing the loop, as there is for the << operator.\nInstead, the loop is executed by the elaborator. Each\nexecution of the loop will result in the creation of one\nmux /shifter pair.\nThe variable k is entirely statically determined,\nand thus does not survive the elaboration process.\nThis means that the k++ and 2^k expressions are stat\nically calculated - no circuitry is necessary. This is in\ncontrast to the variable v, which is an input port to\nthe module, and thus whose values are unknown at\nelaboration time. If v were hardwired to a certain\nvalue the elaborator could also execute the leftshift.\nevalStmt :: FStmt -> Result ()\n...\nevalStmt(FWhile cond body) =\ndo\nb <- evalExpr cond\ncase b of\nStatic True -> do\nevalStmts body\nevalStmt (FWhile cond body)\nStatic False -> return ()\nDynamic -> error\n...\nevalExpr :: FExpr -> Result FExpr\n...\nevalExpr (FTri b tn el) =\ndo\nb' <- evalExpr b\n--type-checking guarantees a bool\ncase b' of\n(Static True) -> evalExpr tn\n(Static False) -> evalExpr el\nDynamic -> do\ntn' <- evalExpr tn\nel' <- evalExpr el\nreturn (FTri b' tn' el')\nFigure 1: Sample evaluator code in Haskell\nSimilarly if n were hardwired then the control flow\ncould be statically determined and muxes would not\nbe created.\nStatic elaboration thus bears some re\nsemblance to partial evaluation, or to other static op\ntimizations such as loopunrolling.\nFigure 1 shows sample code for the FSpec evalu\nator for whileloops and trinary ? : operators.\n2.2 The Cost of Static Elaboration\nThe amount of compiler time required to perform\nelaboration increases as descriptions become more\ngeneralized. For complex designs, experience has\nshown that elaboration is the most costly compiler\nphase. For example, the UNUM project is a frame\nwork written in Bluespec to model microprocessors\n[3]. To maximize code reuse it consists of a library of\ngeneralized CPU components such as Fetch units,\nALUs, etc.\nTo use UNUM an architect constructs a processor\nvia the \"tinker toy\" approach. For example, the archi\ntect may wish to measure the impact of a new branch\n\nNum\nSlots\nCompile Time (seconds)\nDominating Function (% of execution time)\ntypecheck elaborate bool opt schedule\n...\ntotal\n1st\n2nd\n3rd\n7.95\n105.59\n26.44\n2.54\n...\n318.64\ngetIdQual(8.85%) getIdBase (7.1%) checkUse (6.3%)\n7.93\n214.49\n36.64\n5.15\n...\n607.00\ngetIdQual (9.5%)\ncmpE (8.5%)\ngetIdBase (8.3%)\n7.91\n823.45\n72.34\n9.64\n...\n1557.18\ncmpE (10.8%)\ngetIdQual (8.7%) getIdBase (8.2%)\n8.04\n2410.27\n96.19\n18.80\n...\n3479.86\ncmpE (12.8%)\ngetIdQual (8.8%) getIdBase (7.9%)\n8.01\nDNF\n\n...\n\nFigure 2: Profiling information from the Bluespec compiler synthesizing a complex processor reorder buffer with a given\nnumber of slots. As expected the typechecking phase remains constant, whereas the boolean optimization phase and\nscheduling phases grow linearly. Elaboration, on the other hand quickly dominates the execution. At a low number of\nslots the majority of execution time is spent in the functions getIdQual and getIdBase, which are identifiermanipula\ntion functions used throughout the compiler, including elaboration. As we increase the number of slots the cmpE func\ntion, which the evaluator uses to compare expressions for equality during controlflow optimization, becomes dominant.\nAt six slots elaboration did not finish due to running out of memory. It should be noted that typical modern micropro\ncessors use ROBs with 64 slots or more.\npredictor algorithm. They would construct a new\nmodule to represent their algorithm, and instantiate\nlibrary elements to represent the remainder of the\nprocessor, say a 32bit PowerPC Decoder, a 64ele\nment ReOrder Buffer, a Register File with 3 read\nports and two write ports, etc. This allows the archi\ntect to leverage existing code, but means that 95% or\nmore of the system will be generated via static elabor\nation.\nFigure 2 shows the results of profiling the\nBluespec compiler on a generalized UNUM reorder\nbuffer (ROB). These results were created using the\nstandard profiling features of the Haskell compiler\nGHC 6.4.1 on Bluespec compiler version 3.8.60.\nClearly, the elaboration phase dominates, even bey\nond the boolean optimization phase. Overall, the\nfunction cmpE, which compares expressions for\nequality during controlflow optimization in elabora\ntion, accounts for nearly 13% of the execution time of\nthe entire compiler.\nOur goal is to demonstrate that the application of\ntechniques developed for the analysis of software can\nbe used to improve elaborator performance. In this\nwork we will not directly consider techniques to op\ntimize the hardware which the elaborator produces,\nalthough such techniques could be added in the fu\nture.\n2.3 Other Approaches to Static Elaboration\nIn this section we compare our approach to static\nelaboration to other existing projects. The elaboration\napproach we present is based on the Bluespec hard\nware description language and the Guarded Atomic\nAction semantic model. Rosenband [6] showed how\nstatic elaboration can be used to transform hardware\ndesigns to ensure that they meet userdefined per\nformance specifications. To model these transforma\ntions he developed the languages MRL and FRL.\nThese languages are predecessors to the FSpec lan\nguage we used for this work specifically FSpec can\nbe seen as MRL extended with polymorphism, and\nthus requiring static elaboration.\nAn alternative approach to static elaboration is\nLava [2], which advocates strong elaboration through\na library of reusable circuit patterns. Lava is a Do\nmainSpecific Embedded Language implemented in\nHaskell. As such the user can take advantage of the\nparameterization and polymorphism available in\nHaskell directly. IE in Lava, a module with a paramet\ner is a Haskell function which takes a parameter and\nreturns a module. This is in contrast to our language\nFSpec, which is intended to represent an Abstract\nSyntax Tree after parsing. Thus parameters and func\ntions are modeled in the FSpec syntax tree, rather\nthan in Haskell directly.\n\nDesign\nParameters\nDescription\nUses\nRegFile\ndata and address width, number of registers\nA polymorphic register file\narrays, recursive functions\nFIFO\ndata width, buffer length\nQueue of any datatype, any length\narrays, while loops\nShifter\ndata width, shift range\nbarrelshifter of any width\narrays, recursive functions\nFIR Filter data width, coefficient width, number of taps\nA generalized linear FIR filter\narray of submodules, while loops\nHamming data width, buffer lengths,\nGenerates the first k Hamming\nnumbers (numbers like 2p3q5r)\narrays, FIFO submodules\nCache\ndata,address, and tag widths, cache size\nA blocking, writethrough, direct\nmapped cache\narrays,\nRegFile\nand\nFIFO\nas\nsubmodules\nFigure 3: Overview of Benchmark Designs\nRecent research has developed a novel elabora\ntion technique known as \"shadow wires\" to improve\nLava's handling of irregular circuits [8]. Under this\ntechnique wires are threaded throughout the design\nwhich carry staticallydetermined highlevel know\nledge. This knowledge can be used to make decisions\ne.g. for boolean or lowpower optimization. This fits\nunder our notion of static elaboration as it can be\nviewed as statically determined data which influences\nevaluator decisions but which does not translate dir\nectly into hardware.\nSAFL [7] is a functional language where memory\nis statically allocated, and thus is wellsuited to hard\nware description, or hardwaresoftware codesign. On\nthe surface, SAFL's abstract syntax is quite close to\nthat of FSpec which we present here. The major differ\nence is that SAFL does hardware resource sharing\nthrough source code identifiers. Thus in SAFL if the\nuser wants two copies of a multiplier, the user must\ntextually duplicate and rename the multiply function\nin the source code. Static elaboration is one method\nthat could be used to overcome this limitation, as the\nelaborator would essentially perform this duplication\nfor the user.\nIt should be noted that there have been numerous\napproaches to synthesizing hardware from highlevel\nsoftware languages such as C. Although these trans\nformations are static and performed by a compiler, we\nview this approach as distinct from static elaboration.\nIn these cases the goal is to take a possiblyunsynthes\nizable imperative description and attempt to synthes\nize it while finding an optimal amount of paralleliza\ntion, whereas in static elaboration the goal is to take a\nprogram which describes how to synthesize a hard\nware block over many possible values and perform\nthe synthesis for a set of concrete parameters. We do\nnot expect that analysis techniques to improve static\nelaboration will be applicable to sequential synthesis.\n3. METHODOLOGY\nTo measure the performance of the FSpec evaluator\nwe created a suite of simple benchmark designs, as\ndescribed in Figure 3. These designs can be described\nconcisely in FSpec, yet are generalized along various\nparameters. This allows us to measure the perform\nance of our elaborator across a range of data points by\nvarying parameter values.\nThe results of running these benchmarks through\nthe initial reference elaborator across a range of para\nmeter values are shown in Figure 4. Performance was\nmeasured using the standard code profiler included\nin the GHC Haskell compiler version 6.4.1, which al\nlowed us to obtain results about specific function calls\nwithout the need for instrumentation. The FSpec eval\nuator is dominated by heap references and variable\nlookups, as shown in Figure 5.\nThe results demonstrate that our reference evalu\nator can exhibit hyperlinear growth, similar to the ac\ntual Bluespec compiler. In contrast to the Bluespec\ncompiler however, heap accesses are often the bottle\nneck in our language. For example, in the Hamming\nbenchmark heap accesses consumed 98% of the aver\nage execution time. Of concern is the dominance of\nthe lookupVar function in the RegFile, Shifter and\n\nile\ni\nil\ni\n(\n)\nRegF\nFIFO\nSh fter\nFIR F ter\nHamm ng\nCache\nParameter Value\nElaboration Time seconds\nFigure 4: The results of running our benchmark designs on the reference evaluator. The parameter values of the designs\nwere increased to measure the growth in evaluation time. The parameters were as follows: RegFile (num regs), FIFO (buf\nfer length), Shifter (shift amount), FIR Filter (num taps), Hamming (buffer length), Cache (cache size). These results\ndemonstrate that the elaborator can exhibit hyperlinear growth times. Although in practice there is not much need for a\nregister file with 5000 registers, or a FIR Filter with 5000 taps, we view large values for these parameters as analogous to\nmore complex systems composed of many generalized submodules.\nDesign\nDominating Functions (percent of elaboration time)\nRegFile\ndeHeap (32%)\nlookupVar (31%) addHeap (30%)\nFIFO\ndeHeap (48%)\naddHeap (45%)\nevalStmt (2%)\nShifter\nlookupVar (37%) deHeap (32%)\naddHeap (30%)\nFIR Filter\naddHeap (44%)\ndeHeap (39%)\nupdateHeap (13%)\nHamming\naddHeap (49%)\ndeHeap (47%)\nupdateHeap (1%)\nCache\nlookupVar (34%) deHeap (32%)\naddHeap (32%)\nFigure 5: This table shows the three most dominating functions for the FSpec evaluator on the benchmark designs run at\nn = 5000. deHeap removes a variable from the heap cell after it has left scope. addHeap adds a new variable to the heap.\nupdateHeap updates the value of a heap cell. lookupVar searches the environment for a variable and dereferences it\nfrom the heap. evalStmt is a central evaluation function.\n\nCache (which uses RegFile as a submodule). This in\nformation indicates that any transformation that saves\nus from unnecessarily altering the state of the heap or\nlooking up variables should result in an improvement\nin performance.\nGiven the properties of the reference evaluator we\nnow attempt to use traditional software analysis tech\nniques to improve its performance. The technique we\nhave chosen is Pfenning's HigherOrder Abstract Syn\ntax (HOAS) transformation, which should improve\nperformance by decreasing the number of variable\nlookups and heap operations.\n4. HIGHERORDER ABSTRACT SYNTAX\nFOR FSPEC\nHigherOrder Abstract Syntax refers to the technique\naugmenting a standard abstract syntax tree with\nlambda calculus constructs. Practically, this means us\ning implementation language features such as\nlambdafunctions, to model similar constructs in the\ntarget language. For example, our abstract syntax for\nFSpec expressions contains the following productions\nto represent function definition and application:\ndata FExpr =\n...\n| FELam Id FExpr\n-- \\x -> e\n| FEFuncApp FExpr FExpr -- e1 e2\nThus we will parse the application:\n(\\x -> x + 5) (2 + 3)\ninto the following abstract syntax (simplified for read\nability):\nFEFuncApp (FELam 'x'\n(FEAdd (FEVar 'x') 5)) (FEAdd 2 3)\nNote that when executing the above statement, the\nelaborator must explicitly bind the variable, and re\nname free variables to avoid capture:\nevalExpr (FEFuncApp e1 e2) =\ndo\n-- type checking guarantees this:\n(FELam v f) <- evalExpr e1\ne2' <- evalExpr e2\nnewVar v e2'\nres <- evalExpr f\ndelVar v\nreturn res\nIf we use HOAS the userdefined function will be\nstored in the AST as an actual lambda function:\ndata FExpr =\n...\n| FELam (FExpr -> FExpr) -- \\x -> e\n| FEFuncApp FExpr FExpr -- e1 e2\n(\\x -> x + 5) (2 + 3) ==>\nFEFuncApp (FELam (\\x -> EAdd x 5))\n(EAdd 2 3)\nNote that the variable x has changed from a con\nstructor in the abstract syntax to a direct Haskell vari\nable. Performing the application at elaboration time is\nnow equivalent to performing an actual Haskell ap\nplication:\nevalExpr (FEFuncApp e1 e2) =\ndo\n-- type checking guarantees this:\n(FELam f) <- evalExpr e1\ne2' <- evalExpr e2\nres <- evalExpr (f e2')\nreturn res\nThe variable x must still be defined and bound, but\nthis binding will be handled by the Haskell runtime\nsystem, which we expect to be more efficient than be\ning performed by the elaborator. References to x will\nbe use the heap of the Haskell runtime environment,\ninstead of our userdefined heap structure, which\nshould improve performance.\nThe results of running our benchmark suite after\ntransformation are shown in Figure 6. These results\nshow an improvement on five of the six benchmark\ndesigns. The RegFile shows the largest improvement\npercentage, as it uses recursive functions which\ngreatly benefit from the HOAS. The Cache design\nuses the RegFile as a submodule, and thus benefits\nsimilarly. The FIFO, FIR Filter, Hamming designs use\nwhileloops instead of recursive functions, and thus\ndo not benefit as much. One possible experiment\nwould be to transform the whileloops into recursive\nfunctions to see if the benefit can be increased.\nThe sixth design, the shifter, also uses recursive\nfunctions, even more so than the RegFile. We would\ntherefore expect that it would show the largest im\n\nile\ni\ni\ni\n(\n)\nRegF\nFIFO\nSh fter*\nFIR F lter\nHamm ng\nCache\nParameter Value\nElaboration Time seconds\nFigure 6: Evaluator performance results after implementing HigherOrder Abstract Syntax. Original evaluation times\nfrom Figure 4 are listed in white. The transformation resulted in an average improvement of 45.4% on 5 of the 6 designs,\nincluding improvements of greater than 70% on the RegFile and Cache (which uses both the RegFile and FIFO as sub\nmodules, so we would expect to show the largest improvement). For the shifter design, however, performance signific\nantly degraded to the point where evaluation could only complete with parameter values of less than 20. For analysis onto\nthe cause of this degradation see Section 4.1.\nDesign\nRegFile\nFIFO\nShifter\nFIR Filter\nHamming\nCache\nDominating Functions (percent of elaboration time)\naddHeap (49%)\ndeHeap (48%)\n\naddHeap (43%)\ndeHeap (48%)\naddHeap (48%)\ndeHeap (46%)\naddHeap (47%)\n\ndeHeap (39%)\naddHeap (47%)\ndeHeap (46%)\nevalExpr (2%)\nevalExpr (1%)\n\nupdateHeap (15%)\nevalExpr (1%)\nevalExpr 32%)\nFigure 7: The three most dominating functions for the HOAS evaluator on the benchmark designs for n=5000. In the pre\nvious results (Figure 5) lookupVar was a dominant function for the RegFile, Shifter, and Cache designs. After the\nHOAS transformation this call is eliminated completely. Although total execution time has decreased, heap manipula\ntions for values not affected by the HOAS transformation (such as incoming wire ports to methods) remain the majority\nof the execution time. Future work should focus on improving the representation of the heap data structure to speed\nthese operations.\n\nprovement from the transformation. Contrary to our\nexpectations, the HOAS version of the Shifter design\nhad its performance degraded. Previously the com\npiler could elaborate an 5000bit wide shifter in 147\nseconds. After the HOAS transformation the compiler\ntakes 213 seconds to elaborate a 19bit wide shifter.\nLarger values than 19 cause the elaborator to run out\nof memory after over an hour of execution. We now\nanalyze this case in detail.\n4.1 The Effect of HOAS on the Shifter\nDesign\nThe Shifter benchmark design is similar to the one de\nscribed in Section 2.1, except that it uses recursive\nfunctions rather than whileloops to describe the\nshift /mux pairs. For a shift amount of n, we would\nexpect n function calls or loop iterations. If each itera\ntion contains k program expressions, we would expect\nkn calls to the evalExpr function. Profiling informa\ntion revealed the following behavior:\ntime\nevalExpr\ncalls\nMemory use\nOriginal\n(n=19)\n0.24s\n51MB\nOriginal\n(n=20)\n0.22s\n51MB\nHOAS\n(n=19)\n17.64s\n23,068,544\n3.9GB\nHOAS\n(n=20)\n60.86s\n46,137,211\n7.9GB\nHOAS\n(n=21)\nDNF\n> 1 hour\n\nOut of Memory\nThe original version clearly demonstrates linear beha\nvior, whereas the HOAS version results in an expo\nnential number of calls to evalExpr (23 million is ap\nproximately 43 x 219), and exponential memory use.\nThe other benchmarks in our suite exhibit no such be\nhavior, nor increased memory use.\nInvestigation revealed that the HOAS shifter was\ngenerating unexpected, but functionally correct cir\ncuitry. For n=3 we would expect elaboration to gener\nate the circuit diagrammed in Section 2.1. Instead it\nturned out that the HOAS evaluator generated the\nfollowing structure:\nn\n<< 1\n<< 2\n<< 4\n<< 1\n<< 2\n<< 1\n<< 1\nNote that this circuit is functionally equivalent to\nthe circuit we had intended to describe. Thus the test\nbench for this design identified it as a correctly func\ntioning shifter circuit, and we did not examine the\nhardware structure in detail until we investigated the\nelaboration time. Although functionally equivalent,\nthis circuit has the same critical path as the intended\ncircuit, but exponentially larger area, and therefore is\nstrictly worse and would not be desired by the de\nsigner under any circumstances.\nBut why was this circuit being generated? Con\nsider the original n=3 shifter description:\nmethod v_width shift(v_width v,\nbit[3] n) {\nv_width v1, v2, v3;\nv2 = (n[0] == 0) ? v : v << 1;\nv3 = (n[1] == 0) ? v2 : v2 << 2;\nv4 = (n[2] == 0) ? v3 : v3 << 4;\nreturn v4;\n}\nIf we naively substitute the variable v2 into the defin\nition of v3:\nv3 = (n[1] == 0) ?\n((n[0] == 0) ? v : v << 1)\n: ((n[0] == 0) ? v : v << 1)\n<< 2;\nthen we have duplicated hardware. This is an ex\nample where a transformation that is safe to perform\nin software is undesirable for a hardware compiler.\n\nWe are currently investigating how to reconcile\nthis behavior with Haskell's nonstrict semantics. Giv\nen a function such as:\n\\v -> (b) ? v : (f v)\nwe would expect the argument v to be to be evaluated\nonly once, and the result to be reused when needed.\nTherefore our elaborator should not exhibit exponen\ntial behavior, although it clearly does. We are cur\nrently investigating whether some of the GHC\nHaskell compiler's eagerness optimizations could\ncause a bad interaction in this case. In the meanwhile\nwe are attempting to implement explicit sharing via\nmemoization as a stopgap solution.\n5. CONCLUSIONS\nAs hardware designs become more and more com\nplex, hardware description languages are continually\nadding new features to compensate. We believe that\nhardware description languages should not reinvent\nthe wheel, but rather leverage techniques that have\nbeen developed in the mature field of software pro\ngramming languages. In this paper we demonstrated\nthat the HigherOrder Abstract Syntax technique can\nbe used to speed up the static elaboration phase of a\nhardware compiler. Additionally we demonstrated\nthat HOAS can be used as a framework to transmit in\nformation from static analyses to the elaborator. In the\nfuture we hope to apply this work to the actual\nBluespec compiler, and to expand our framework to\ninclude analyses which perform optimizations of the\ngenerated circuit structures rather than improving\ncompiler performance.\n6. ACKNOWLEDGEMENTS\nWe would like to thank Ravi Nanavati and the rest of\nBluespec, Inc. for their ideas and support.\nREFERENCES\n[1] Arvind, R. S. Nikhil, D. L. Rosenband and N.\nDave, HighLevel Synthesis: An Essential In\ngredient for Designing Complex ASICs. In\nProceedings of ICCAD '04, pages 775782,\n2004.\n[2] P. Bjesse, K. Claessen, M. Sheeran, and S.\nSingh. Lava: Hardware Design in Haskell. In\nProceedings of International Conference on\nFunctional Programming (ICFP) '98, pages\n174184, 1998.\n[3] N.\nDave, M. Pellauer and Arvind. The\nUNUM Microprocessor Framework. Unpub\nlished.\n[4] J.C. Hoe and Arvind. Synthesis of Operation\nCentric Hardware Descriptions. In Proceed\nings of ICCAD '00, pages 511518, 2000.\n[5] F. Pfenning and C. Elliot. HigherOrder Ab\nstract Syntax. In Proceedings of the Conference\non Programming Language Design and Imple\nmentation (PLDI) '88, pages 199208, 1988.\n[6] D. Rosenband. A PerformanceDriven Ap\nproach to Hardware Synthesis of Guarded\nAtomic Actions. PhD Thesis, MIT, 2005.\n[7] R. Sharp and A. Mycroft. A HigherLevel\nLanguage for Hardware Synthesis. Lecture\nNotes on Computer Science, 2144:228240, 2001\n[8] M. Sheeran. Finding Regularity: Describing\nand Analyzing Circuits that are not Quite\nRegular. In Proceedings of the Conference on\nCorrect Hardware Design and Verification Meth\nods (CHARME) '03, pages 418, 2003.\n\n7. APPENDIX\nAbstract Syntax Tree for FSpec in Haskell, and ex\namples of concrete syntax.\ndata FModule = FModule\n{\nmodname :: Id,\nmodtype :: FType,\nparams :: [(FType, Id)],\ndecls :: [FStmt],\nact_methods :: [FActionMethod],\nval_methods :: [FValueMethod],\nrules :: [FRule]\n}\ndata FRule = FRule Id FExpr [FStmt]\ndata FActionMethod = FActionMethod\nFType Id FExpr [Id] [FStmt]\ndata FValueMethod = FValueMethod\nFType Id FExpr [Id] FExpr\ndata FStmt =\nFSRegUpdate FExpr FExpr\n| FSDecl FType Id FExpr\n| FSArrayUpdate Id FExpr FExpr\n| FSVarUpdate Id FExpr\n| FSIf FExpr [FStmt]\n| FSMethodCall FExpr Id [FExpr]\n| FSWhile FExpr [FStmt]\ndata FExpr =\nFERegRead FExpr\n| FEConst Value\n| FEInt Int\n| FEVar Id\n| FELam FType Id FExpr\n| FEFuncApp FExpr FExpr\n| FETri FExpr FExpr FExpr\n| FEArrayRead FExpr FExpr\n| FEArray (Array Int FExpr)\n| FEMethodCall FExpr Id [FExpr]\n-- module nm < Types > { stmts }\n-- rule nm when (p) { stmts }\n-- method Action nm ( params )\nwhen ( expr ) { stmts }\n-- method Type nm ( params )\nwhen ( expr ) expr\n-- r <= expr ;\n-- Type var = expr ;\n-- var[expr] = expr ;\n-- var = expr ;\n-- if ( expr ) { stmts } ;\n-- m.actmeth ( params ) ;\n-- while ( expr ) { stmts };\n-- r\n-- 1'b0, 1'b1, ...\n-- 0, 1, 2, ...\n-- var\n-- \\ x -> expr\n-- f expr\n-- ( expr ) ? expr : expr\n-- var[expr]\n-- { expr, expr,... }\n-- m.valmeth( params )"
    },
    {
      "category": "Resource",
      "title": "javarifier.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/e0cdb8c7dda2b85b3ae377bf45cc1bbb_javarifier.pdf",
      "content": "Javarifier: Inferring Reference Immutability for Java\n∗\n6.883: Program Analysis Course Project\nVineet Sinha\nMatthew Tschantz\nChen Xiao\nABSTRACT\nReference immutability constraints restrict certain references\nin a program from modifying objects.\nThese constraints,\nsuch as those provided by the Javari programming language,\nhave been shown to increase the expressiveness of a language\nand to prevent and detect errors. Unfortunately, annotating\nexisting code, including libraries, with reference immutabil\nity constraints is a tedious and error prone task.\nWe propose an algorithm for inferring the references in a pro\ngram that can be declared as readonly (or romaybe). Addi\ntionally, to gain additional precision, we introduce heuristics\nthat suggest assignable and mutable fields.\nTo evaluate our algorithm, we have implemented a tool,\nJavarifier, to aid programmers in converting Java programs\nto Javari. We also demonstrate the practicality of our algo\nrithm by using Javarifier to annotate an existing program.\n1. INTRODUCTION\nImmutable, or readonly, reference constraints prevent re\nstricted references from being used to modify the objects\n(including transitive state) to which they refer. Using such\nimmutability constraints can have many benefits: program\nmers can write more expressive code; program understand\ning and reasoning is enhanced by providing explicit, machine\nchecked documentation; errors can be detected and pre\nvented; and analyses and transformations depending on com\npiler verified properties can be enabled.\nIn practice, im\nmutability constraints have been shown to be practical and\nto find errors in software [2].\nFor a programmer to gain the benefits of reference immutabil\nity constraints, he must annotate his code with the appropri\nate immutability constraints. For existing code, this can be\na tedious and error prone task. Even worse, a programmer\nwishing to use reference immutability constraints, must first\n∗Authors in alphabetical order\nannotate the libraries he plans to use. Otherwise, a reference\nimmutability type checker would be forced to assume that\nall methods may modify their arguments.\nTherefore, the\nprogrammer would be unable to invoke any of the library's\nmethods on immutable references even if the method, if fact,\ndoes not modify its arguments.\nTo aid programmers with adding reference immutability con\nstraints to Java programs, we have created an algorithm to\nsoundly infer immutable references from Java code. For con\ncreteness, we use the reference immutability constraints of\nJavari [13] (described in section 2).\nGiven a program, our algorithm calculates all the references,\nincluding local variables, method parameters, static and in\nstance fields, that may have Javari's readonly or romaybe\nkeywords added to their declarations.\nreadonly references\ncannot be used to modify the abstract state of the object to\nwhich it refers (see section 2). romaybe references are used\nreduce method duplication and cannot be used to modify\ntheir referents, but can be passed, via a method's return\nvalue, to the client as a mutable reference (see section 2.3).\nUsing this algorithm, programmers can convert Java pro\ngrams, including libraries, to Javari.\nJavari's other annotations, assignable and mutable, exclude\nparts of the concrete representation from the abstract state.\nThus, they cannot be soundly inferred because they require\nknowing the intent of the programmer. However, without\ninferring this annotations, the precision of our algorithm\nwould suffer. This problem can occur when a reference is\nused to modify a part of an object's concrete state that was\nnot intended to be a part of the object's abstract state. Such\na reference would be inferred to be mutable when in reality,\nit should be readonly.\nTherefore, to improve the results\nof our algorithm, we have created heuristics to recommend\nfields that should be declared assignable or mutable.\nWe have implemented our algorithm and heuristics in the\nform of the Javarifier tool. Given a Java program in class\nfile format, Javarifier first recommends fields that should be\ndeclared assignable or mutable. After the user has selected\nthe fields to declare assignable or mutable, Javarifier calcu\nlates and returns to the user the references that may be de\nclared readonly or romaybe. 1 It is important that Javarifier is\n1As future work, Javarifier will be able to automatically in\nsert the correct readonly and romaybe annotations into source\nand class files.\n\n// Java\nclass Event {\nDate date;\nDate getDate() {\nreturn Date;\n}\nvoid setDate(Date d) {\nthis.date = d;\n}\nint hc = 0;\nint hashCode() {\nif (hc == 0) {\nhc = ...;\n}\nreturn hc;\n}\n}\n// Javari\nclass Event {\n/*thismutable*/ Date date;\nromaybe Date getDate() romaybe {\nreturn Date;\n}\nvoid setDate(/*mutable*/ Date d) /*mutable*/ {\nthis.date = d;\n}\nassignable int hc = 0;\nint hashCode() readonly {\nif (hc == 0) {\nhc = ...;\n}\nreturn hc;\n}\n}\nFigure 1: A Java program (above) and the corre\nsponding inferred Javari program (below).\nable to operate on classfiles because programmers may wish\nconvert library code that they only have in classfile format.\nAn example Java program and the corresponding Javari pro\ngram is shown in figure 1.\nNote that hc is heuristically recommended to--and in this\ncase approved by--the user to be declared assignable. Oth\nerwise, hashCode would be inferred, undesirably, to have a\nmutable receiver.\nThe rest of this document is organized as follows. Section 2\nprovides background on the Javari language. Section 3 ex\nplains the algorithm which is used to soundly infer read\nonly references.\nSection 4 discusses how our algorithm is\nsoundly extended to infer romaybe references. Section 5 pro\nvides heuristics that can be used to infer assignable and\nmutable fields.\nSection 6 describes the implementation of\nour algorithm and heuristics within the Javarifier tool. Sec\ntion 7 provides our experience using Javarifier on a variety\nof Java programs. Section 8 discusses related work. Finally,\nsection 10 concludes.\n2. BACKGROUND\nJavari [13] extends Java's type system to allow programmers\nto specify and statically enforce reference immutability con\nstraints.\nFor every Java type T, Javari also has the type\nreadonly T, with T being a subtype of readonly T. A refer\nence declared to have a readonly type cannot be used to\nmutate the object it references.Mutation is any modifica\ntion to an object's transitively reachable state, which is the\nstate of the object and all state reachable from it by fol\nlowing references. 2 References that are not readonly, can be\nused to modify their referent and are said to be mutable. An\nexample of Javari code is shown below\n/*mutable*/ Date date;\nreadonly\nDate rd;\ndate.month = \"June\"; // OK\nrodate.month = \"June\"; // error\nA readonly reference cannot be converted to a mutable ref\nerence through assignment or casting. Mutable references,\nbeing subtypes of readonly references, may be assigned\n(without casting) or casted to readonly references.\nThis\nbehavior is demonstrated below.\n/*mutable*/ Date date;\nreadonly\nDate rodate;\nrodate = date;\n// OK\nrodate = (readonly Date) date; // OK\ndate = rodate;\n// error\ndate = (Date) rodate;\n// error\nIn addition to local variables, the readonly keyword may\nbe applied to fields and method parameters including the\nimplicit this parameter. A readonly this parameter is de\nclared by writing readonly immediately following the pa\nrameter list. For example, an appropriate declaration for\nthe StringBuffer.charAt method in Javari is:\npublic char charAt(int index) readonly { ... }\nSuch a method is called a readonly method. In the context\nof the method, this is readonly. Thus, it is a type error for\na readonly method to change the state of the receiver, and\nit is a type error for a nonreadonly method to be called\nthrough a readonly reference.\nNote that final and readonly are orthogonal notions in vari\nable declaration: final makes the variable not assignable,\nbut the reference may still be used to mutate the object it\nreferences, while readonly disallows the reference from being\nused to mutation its referent, but the variable may still be\nreassigned.\n2.1 Fields\nBy default, a field of an object inherits its assignability and\nthe mutability it is read at from the reference through which\nthe field is reached. This default is called thisassignability\n2Section 2.1.1 discusses overriding this behavior.\n\nand thismutability. If the reference through which the field\nis accessed is readonly, then the field is unassignable (final)\nand read at readonly. If the reference through which the\nfield is accessed is mutable, then the field is assignable and\nread at mutable. These defaults ensure that mutability is\ntransitive by default. A thismutable field is always writ\nten to as a mutable type regardless of the mutability of the\nreference through which it was reached. The requirement\nthat a thismutable field is never written to with a read\nonly time is need to prevent a type loop hole which could\nbe used to convert a readonly reference to a mutable refer\nence [13]. The behavior of thisassignable and thismutable\nfields is illustrated below.\nclass Cell {\n/*thisassignable thismutable*/ Date d;\n}\n/*mutable*/ Cell\nc; // mutable\nreadonly\nCell rc; // readonly\nc.d = new Date();\n// OK: c.d is assignable\nrc.d = new Date();\n// error: rc.d is unassignable (final)\n/*mutable*/ Date d1 =\nc.d; // OK: c.d is read as mutable\n/*mutable*/ Date d2 = rc.d; // error: rc.d is read as readonly\nOnly instance fields may be thisassignable or thismuta\nble. Other references (such as formal parameters and local\nvariables) do not have a this to inherit their assignability\nfrom.\n2.1.1 Assignable and mutable fields\nBy default, fields are thisassignable and thismutable. Un\nder these defaults, all the fields are considered to be a part\nof the object's abstract state and, therefore, can not be\nmodified through a readonly reference. The assignable and\nmutable keywords enable a programmer to exclude specific\nfields from the object's abstract state.\nDeclaring a field assignable specifies that the field may al\nways be reassigned, even through a readonly reference.\nThe assignable keyword specifies that a field may be as\nsigned to even when reached through a readonly reference.\nAssignable fields can be used for caches as shown below.\n/** Assignable Cell. */\nclass ACell {\nassignable /*thismutable*/ Date d;\n}\n/** Converts a readonly Date to a mutable date. */\nstatic /*mutable*/ Date\nconvertReadonlyToMutable(readonly Date roDate) {\n/*mutable*/ ACell mutCell = new ACell();\nreadonly ACell roCell = mutCell;\nroCell.d = roDate;\n// error\n/*mutable*/ Date mutDate = mutCell.d;\nreturn mutDate;\n}\nThe mutable keyword specifies that a field is mutable even\nwhen referenced through a readonly reference. A mutable\nfield's value is not a part of the abstract state of the object\n(but the field's identity may be). For example, in the code\nbelow, log is declared mutable so that it may be mutated\nwithin readonly methods such as hashCode.\nclass Foo {\nfinal mutable List<String> log;\nint hashCode() readonly {\nlog.add(\"entered hashCode()\"); // OK: log is mutable\n...\n}\n}\nNote that assignable and mutable are orthogonal notations\nand may be applied in conjunction.\n2.2 Parametric types and arrays\nIn Java, the client of a generic class controls the type of any\nreference whose declared type is a type parameter. A client\nmay instantiate a type parameter with any type that is equal\nto or a subtype of the declared bound. One can think of the\ntype argument being directly substituted into the param\neterized class wherever the corresponding type parameter\nappears.\nJavari uses the same rules, extended in the natural way to\naccount for the fact that Javari types include a mutability\nspecification. A use of a type parameter within the generic\nclass body has the exact mutability with which the type pa\nrameter was instantiated. Generic classes require no special\nrules for the mutabilities of type arguments, and the defaults\nfor local variables and fields are unchanged.\nAs with any local variable's type, type arguments to the type\nof a local variable may be mutable (by default) or readonly\n(through use of the readonly keyword). Below, four valid\nlocal variable, parameterized type declarations of List are\nshown. Note that the mutability of the parameterized type\nList does not affect the mutability of the type argument.\n/*mutable*/ List</*mutable*/ Date> ld1; // add/rem./mut.\n/*mutable*/ List<readonly\nDate> ld2; // add/remove\nreadonly\nList</*mutable*/ Date> ld3; // mutate\nreadonly\nList<readonly\nDate> ld4; // (neither)\nAs in Java, subtyping is invariant in terms of type argu\nments. Therefore, List</*mutable*/ Date> is not related (a\nsubtype or supertype) to List<readonly Date>.\nWildcard\ntypes can be used to provide a common supertype for List\n</*mutable*/ Date> and List<readonly Date>. The common\nsupertype will have readonly Date as its type argument's\nlower bound and /*mutable*/ Date as its the upper bound.\nUsing Javalike syntax, this type is written as List<? extends\nreadonly Date super /*mutable*/ Date>. Java, however, does\nnot allow the declaration of a lower and upper bound on a\nwildcard.\nTherefore, Javari provides the special keyword\n? readonly, to specify a that a wildcard should be bounded\nabove by the readonly version of a type and below, by\nthe mutable version of the same type. For example, using\n? readonly, the common supertype of List<readonly Date>\nand List</*mutable*/ Date> would be written as List<?\nreadonly Date>. Below, List<? readonly Date>'s relationship\nto the other List types is demonstrated.\nList</*mutable*/ Date> ld1;\nList<readonly\nDate> ld2;\n\nList<? readonly\nDate> ld3;\nld1 = ld2; // error: ld1 and ld2 are unrelated\nld3 = ld1; // OK: ld3 supertype of ld1\nld3 = ld2; // OK: ld3 supertype of ld2\nSince List<readonly Date>s may be assigned to a reference\nof type List<? readonly Date>, Dates taken from a List of\n? readonly elements must be read as being readonly. Oth\nerwise, one would be able to place a readonly Date in a List\nof readonly Date's and take the Date out of a aliasing List of\n? readonly dates. Similarly, because a List of mutable Dates\ncould alias a List of ? readonly Dates, only mutable Dates\nmay be written to a List of ? readonly Dates. Below, these\ntype rules are demonstrated.\nList<? readonly\nDate> ld;\n/*mutable*/ Date\nd;\nreadonly\nDate rd;\nrd = ld.get(0); // OK\nd\n= ld.get(0); // error: elms. read as readonly\nld.add(rd);\n// error: elms. written as mutable\nld.add(d);\n// OK\nAs with any instance field's type, type arguments to the type\nof a field default to thismutable, and this default can be\noverridden by declaring the type argument to be readonly,\nmutable or ?\nreadonly:\nclass DateList {\n// 3 readonly lists whose elements have different mutability\nreadonly List</*thismutable*/ Date> lst;\nreadonly List<readonly\nDate> lst2;\nreadonly List<mutable\nDate> lst3;\nreadonly List<? readonly\nDate> lst4;\n}\nAs in any other case, the mutability of a type with this\nmutability is determined by the mutability of the object in\nwhich it appears (not the mutability of the parameterized\nclass in which it might be a type argument). In the case\nof DateList above, the mutability of lst's elements is deter\nmined by the mutability of the reference to DateList, not by\nthe mutability of lst itself.\nSimilar to nonparametric typed field, when reached through\na mutable reference, the thismutable elements of a parame\nterized class are read as mutable, and when reached through\na readonly reference, the elements are read as mutable.\nHowever, whether reached through a mutable or readonly\nreference, the elements of a thismutable parameterized class\nmust always be written to by mutable references.\nThus,\nthismutable elements of a List when reached through a\nreadonly reference are read as readonly and written to as\nmutable, which is the same behavior as a list of ? readonly\nelements. The following example illustrates this behavior.\nclass EventPlanner {\n/*thismut*/ List</*thismut*/ Event> evts;\n}\n/*mutable*/ EventPlanner\np;\nreadonly\nEventPlanner rp;\n/*mutable*/ List</*mutable*/ Event> e1 =\np.evts; // OK\nreadonly\nList<? readonly Event>\ne2 = rp.evts; // OK\n2.2.1 Arrays\nAs with generic container classes, a programmer may inde\npendently specify the mutability of each level of an array.\nAs with any other local variable's type, each level of an ar\nray is mutable by default and may be declared readonly\nwith the readonly keyword. Additionally, as with paramet\nric types, the type of an array's elements may be declared\nas ? readonly.\nAs with any other field's type, each level\nmay be declared mutable with the mutable keyword, read\nonly with the readonly keyword, or thismutable by default.\nAgain, the type of an array's elements may be declared as\n? readonly. Parentheses may be used to specify to which\nlevel of an array a keyword is to be applied. Below, four\nvalid array local variable declarations are shown.\nDate [] ad1; // add/remove, mutate\n(readonly Date)[] ad2; // add/remove\nreadonly\nDate [] ad3; // mutate\nreadonly (readonly Date)[] ad4; // no add/rem., no mutate\nThe above syntax can be applied to arrays of any dimen\nsionality.\nFor example, the type (readonly Date[])[] is a\ntwodimensional array with a readonly innerarray and that\nis otherwise mutable.\nAs in the case of parameterized list, an array may have this\nmutable elements. When such an array is reached through a\nmutable reference, the elements have mutable type and when\nreached through a readonly reference, the elements have ?\nreadonly type. Below, gives an example of this behavior.\nclass Wheel\n/*thismut*/ (/*thismut*/ Spoke)[] spokes;\n/*mutable*/ Wheel\nw;\nreadonly\nWheel rw;\n/*mutable*/ (/*mutable*/ Spoke)[] spks1 = w.spokes; // OK\nreadonly\n(? readonly\nSpoke)[] spks2 = w.spokes; // OK\nJava's arrays are covariant.\nTo maintain type safety, the\nJVM performs a check when a object is stored to an array.\nTo avoid a runtime representation of immutability, Javari\ndoes not allow covariance across the mutability of array el\nement types. For example,\nDate [] ad1;\n(readonly Date)[] ad2;\nad2 = ad1; // error: arrays are not covariant over mutability\n2.3 romaybe\nWhen the mutability of the return type of a method is de\npendant on the mutability of one (or more) of the method's\nformal parameters, it is often necessary to write two meth\nods that differ on in their signatures. For example, below,\nthe return type of getDate depends on the mutability of the\nmethod's receiver.\nclass Event {\nDate date;\n\n/*mutable*/ Date getDate() /*mutable*/ {\nreturn date;\n}\nreadonly\nDate getDate() readonly {\nreturn date;\n}\n}\nThus, as shown above, one is forced to write two versions of\ngetDate.\nTo reduce this code duplication, Javari provides the romaybe\nkeyword, which is used to template a method over the mu\ntability over the mutability of formal parameters and return\ntypes. If the type modifier romaybe is applied to any formal\nparameter and the return type (including this), then the\ntype checker conceptually duplicates the method, creating\ntwo versions of it. In the first version of the method, all in\nstances of romaybe are replaced by readonly3 . In the second\nversion, all instances of romaybe are removed. For example,\nthe two getDate methods from Event could be replaced by\nthe single method declaration shown below:\nromaybe Date getDate() romaybe {\nreturn date;\n}\n3. INFERRING READONLY REFERENCES\nWe use a flow and context insensitive algorithm to infer\nwhich references may be declared readonly.\nA readonly\nreference may have the readonly keyword added to its Javari\ntype declaration. The algorithm is sound: Javarifier's rec\nommendations will type check under Javari's rules.\nFur\nthermore, our algorithm is precise: declaring any references\nin addition to Javarifier's recommendations as readonly--\nwithout other modifications to the code--will result in the\nprogram not type checking.\nThe readonly inference algorithm does not determine which\nfields should be declared mutable or assignable; however, it\nis capable of inferring in the presence of fields that have been\ndeclared mutable or assignable (section 2.1.1) by an outside\nsource (see section 5). The readonly inference algorithm as\nsumes that all fields that are not readonly are thismutable.\nThis choice is made because declaring fields to be mutable\nis state that the field is not a part of the object's abstract\nstate, an exceptional case.\nFor simplicity, we begin by describing our core algorithm (in\nsection 3.1), i.e., the algorithm in the absence of subtyping,\nuserprovided reference immutability annotations including\nassignable and mutable fields, arrays, and parametric types.\nWe then extend the algorithm to subtyping (in section 3.2),\nuserprovided constraints (in section 3.3), and arrays and\nparametric types (in section 3.4).\n3.1 Core algorithm\nOur algorithm generates then solves a set of mutability con\nstraints for a program. A mutability constraint states when\na given reference must be declared mutable. The algorithm\n3If romaybe appeared as an type argument to a parameter\nized class, then it is replaced by ?\nreadonly.\nC\n::=\nclass {f M}\nM\n::=\nm(x){s;}\ns\n::=\nx = x\nx = x.m(x)\n|\nreturn x\n|\nx = x.f\n|\nx.f = x\n|\nFigure 2: Grammar for core language used during\nconstraint generation.\nuses two types of constraints: unguarded and guarded. Un\nguarded constraints state that a given reference must be\nmutable, e.g.\n\"x is mutable.\"\nGuarded constraints state\nthat a given dependent reference is mutable if the guard ref\nerence is mutable, e.g. \"if y is mutable then x is mutable.\"\nWe use constraint variables to name the references in these\nconstraints--x and y in the previous examples. Unguarded\nconstraints are represented by the constraint variable of the\nreference it is constraining, e.g. \"x.\" Guarded constraints\nare represented as implications with guard reference's con\nstraint variable as the predicate and dependent reference's\nconstraint variable as the consequence, e.g. \"y → x.\nAfter generating the constraints, the algorithm solves the\nconstraints yielding a simplified constraint set.\nThe sim\nplified constraint set is the set of all unguarded constraints\nand guarded constraints that are satisfied directly by an un\nguarded constraint or by indirectly though the consequence\nof a different satisfied guarded constraint.\nThe simplified\nconstraint set contains the set of references that must be de\nclared mutable; all other references may safely be declared\nreadonly.\n3.1.1 Constraint generation\nThe first phase of the algorithm generates constraints for\neach statement in a program.\nUnguarded constraints are\ngenerated when a reference is used to modify an object.\nGuarded constraints are generated when a reference is as\nsigned to another reference or a field is reached through a\nreference.\nWe present constraint generation using a core language. The\ncore language is a simple threeaddress programming lan\nguage.\nThe grammar of the language is shown in figure\n2. We use C and M to refer to class and method definitions,\nrespectively.\nm ranges over method names, f ranges over\nnames fields, and s ranges over allowed statements.\np, x,\ny, and z range over variables (locals and method parame\nters). We use x as the shorthand for the (possibly empty)\nsequence x1...xn We also include the special variable thism,\nwhich refers to the receiver of m. Furthermore, we assume\nany program that attempts to reassign thism is malformed.\nOtherwise, thism is treated as a normal variable. Without\nloss of generality and for ease of presentation, we assume\nthat all references and methods are given globallyunique\nnames.\nControl flow constructs are not modeled because the our al\n4Java source and classfiles can be converted to such a rep\nresentation.\n\ngorithm is flowinsensitive and, therefore, unaffected by such\nconstructs. Java types are not modeled because the core al\ngorithm does not use them. Constructors are modelled as\nregular methods returning a mutable thism, and static mem\nbers are omitted because do not demonstrate any interesting\nproperties.\nEach of the statements from figure 2 has a constraint gener\nation rule, as shown in figure 3. The rules make use of the\nfollowing auxiliary functions. this(m) and params(m) returns\nthe receiver reference (this) and parameters of method m,\nrespectively. retVal(m) returns the constraint variable, retm,\nthat represents the reference to m's return value.\nretm is\ntreated, otherwise, like any other constraint variable.\nThe constraint generation rules are described as follows:\nAssign The assignment of variable y to x causes the guarded\nconstraint x → y to be generated because, if x is a mu\ntable reference, y must also be mutable for the assign\nment to succeed.\nInvk The assignment of the return value of the invocation\nof method m on y with arguments y generates three\nconstraints. The guarded constraint thism → y is gen\nerated because the actual receiver must be mutable if\nm requires a mutable receiver. Similarly, the set of con\nstraints p → y is generated because the ith argument\nmust be mutable if m requires the ith parameter to be\nmutable. Finally, the constraint x\nretm is introduced\n→\nto enforce that m must return a mutable reference if its\nreturn value is assign to a mutable reference.\nThese constraints are extensions of the Assign rule\nwhen method invocation is framed in terms of opera\ntional semantics: the receiver, y, is assigned to the\nthism reference, each actual argument is assigned to\nthe method's formal parameters, and the return value,\nretm, is assigned to x.\nRet The return statement return x adds the constraint retm →\nx because in the case that the return type of the method\nis found to be mutable, all references returned by the\nmethod must be mutable.\nRef The assignment of y.f to x generates two constraints.\nThe first, x → f, is required because, if x is mutable,\nthen the field f cannot be readonly. The second, x →\ny, is needed because, if x is mutable, then y must be\nmutable to yield a mutable reference to field f.\nSet The assignment of y to x.f causes the unguarded con\nstraint x to be generated because x has just been used\nto mutate the object to which it refers. The constraint\nf → y is also added because if f is not readonly, then\na mutable reference must be assigned to it.\nThe constraints for a program is the union of the constraints\ngenerated for each line of the program. Figure 4 shows con\nstraints being generated for a sample program.\nLibrary code:\nOur algorithm as described above makes\nthe closeworld assumption. That is, it assumes that all code\nx = y : {x → y} (Assign)\nthis(m) = thism\nparams(m) = p\nretVal(m) = retm\n(Invk)\nx = y.m(y) : {thism → y, p → y, x → retm}\nretVal(m) = retm\n(Ret)\nreturn x : {retm → x}\nx = y.f : {x → f, x → y} (Ref)\nx.f = y : {x, f → y} (Set)\nFigure 3: Constraint generation rules.\nclass {\nf;\nfoo(p) {\nx = p;\n// {x > p}\nAssign\ny = x.f;\n// {y > f, y > z}\nRef\nz = x.foo(y); // {this_foo > x,\n//\np > x, z > ret_foo} Invk\nthis.f = y;\n// {this_foo, f > y}\nSet\nreturn y;\n// {ret_foo > y}\nRet\n}\nProgram constraints:\n{x → p, thisfoo\nx, p → x, z\n→\n→ retfoo ,\nz, thisfoo , f → y, retfoo\ny → f, y →\n→ y}\nFigure 4:\nExample of constraint generation.\nThe\nconstraints generated and the constraint generation\nrule used for each line of code is shown after the line\nas a comment.\n\nis seen and it, therefore, is safe to change public method re\nturn types and types of public fields.\nIn the case that a\nuser is running the algorithm on the whole program, the\nclosed world assumption allows the results to be more pre\ncise. However, in order to support analyzing library classes\nwhich do not have access to their clients, our algorithm needs\nto be sound even without the presence of all client code.\nThus, all nonprivate 5 fields and the return types of non\nprivate methods must be mutable because an unseen client\nmay rely on a mutability of the field or return value. Our\nalgorithm is easily modified to enforce this restriction: an\nunguarded constraint for every nonprivate field and non\nprivate method return value is added to the constraint set.\nDepending on the needs of the other, the algorithm can be\noptionally conducted either under the closedworld assump\ntion.\n3.1.2 Simplifying constraints\nThe second phase of the algorithm is to simplify the con\nstraint set. Constraint set simplification is done by checking\nif any of the unguarded constraints satisfies, i.e. matches,\nthe guard of a guarded constraint. If so, the guarded con\nstraint is \"fired\" by removing it from the constraint set and\nadding its consequence to the constraint set as an unguarded\nconstraint. The new unguarded constraint can then be used\nto fire other guarded constraints. Once no more constraints\ncan be fired , constraint simplification terminates. The un\nguarded constraints in the simplified constraint set is the\nset of references that are used to mutable their referents. 6 .\nThese references cannot have their declarations annotated\nwith readonly. All other references are safe to have their\ndeclarations annotated with readonly.\nThe constraints generated from the example program in fig\nure 4 will be simplified as shown below. (For clarity, the\nunguarded constraints are listed first.)\nthisfoo, x →p, thisfoo\nx, p →x, z\n{\n→\n→retfoo ,\nf, y →z, f →y, retfoo\ny →\n→y} ⇒\nthisfoo , x, p}\n{\nAt the end of simplification, the unguarded constraints thisfoo ,\nx and p are present in the simplified constraint set and are\nmutable. These references may not have their declarations\nannotated with readonly. All other references, y, z, retfoo ,\nand f, are readonly and may have their declarations anno\ntated with readonly. Figure 5 shows the result of applying\nour algorithm's result to the example program.\n3.2 Subtyping\nJava and Javari allows subtyping polymorphism which en\nables multiple versions of a method to be specified through\noverriding7 . All versions of an overridden method must have\n5In the case that an entire package is being analyzed,\npackageprotected (default access) methods and fields may\nbe processed as under the closed world assumption.\n6They must be declared thismutable for fields, mutable in\nthe case of all other types of references.\n7We use the term overriding to specify when a method im\nplements an interface's method signature, implements an\nabstract method, or overrides a concrete method in a su\nperclass.\nFurthermore, for brevity and to highlight their\nidentical treatment, we will refer to both abstract methods\nand interface method signatures as abstract methods.\nclass {\nreadonly f;\nfoo(p) {\nx = p;\nreadonly y = x.f;\nreadonly z = x.foo(y);\nthis.f = y;\nreturn y;\n}\n}\nFigure 5:\nThe results of applying our algorithm's\nresults the program.\nidentical signatures including the mutability of parameters.8\nTherefore, our algorithm must infer the same mutabilities\nfor the parameters of all versions of the overridden meth\nods. If not, the resulting code would not type check under\nJavari's type rules.\nFor example, in the case of a class's\nmethod overriding an abstract method, the class would no\nlonger implement the interface or abstract class that con\ntained the abstract method. On the other hand, in the case\nof a method overriding a concrete method, the two methods\nwould no longer be in a overriding relationship, but an over\nloading relationship. This is not allowed, however, because\nJavari does not allow two overloading methods to differ only\nin the mutability of parameters.\nTo ensure that our algorithm does not infer different signa\ntures for polymorphic methods, we must place an additional\nset of constraints in the constraint set. For every parame\nter of a overloaded method, we add the constraint that it\nis equal in mutability to every other version of the method.\nEquality is represented by two guarded constraints, one in\ndicated if the first parameter is mutable then the second\nparameter is mutable, the other specifies that if second pa\nrameter is mutable then the first is mutable. For example,\nbelow, the method toString is overloaded.\nclass Event {\nDate d;\n...\nString toString() {\nreturn d.toString();\n}\n}\nclass Birthday extends Event {\n...\nint age;\nString string;\nString toString() {\nif (string == null) {\nstring = \"I'm\" + age + \"on\" + d + \"!\";\n}\nreturn string;\n}\n}\nThus, the mutability of the this parameter of Event's and\nBirthday's toString methods must be the same.\nThis re\nquirement generates the constraints thisEvent.toString →\nthisBirthday.toString and thisBirthday.toString →thisEvent.toString.\n8Java signatures do not include the return type, however,\nsuch methods are additionally restricted to have covariant\nreturn types.\n\nAlthough, to preserve overloading, both methods are de\nclared to have mutable this parameters, only Birthday's\ntoString method actually mutates it receiver. One version\nof the method mutating the receiver while another does not\nis disconcerting because overloaded methods are suppose to\nboth be implementations of the same specification. There\nfore, our algorithm should issue a warning in such cases\nor make both receivers readonly using the technique of\ndeclaring select fields to be mutable or assignable given in\nsection 5.1.\nFor example, the technique would state that\nBirthday's string field should be declared assignable, and\ndoing so would allow Birthday's toString along with Event's\nto be readonly, the expected result.\n3.3 Userprovided annotations\nOur readonly inference algorithm can be extended to in\ncorporate userprovided annotations. This capability is re\nquired because the readonly inference algorithm is not ca\npable of directly inferring the assignable and mutable key\nwords. Additionally, userprovided annotations are needed\nfor native method invocations, on which our algorithm can\nnot operate.\nFinally, a user may, using knowledge of the\nprogram, wish to override the annotations inferred by our\nalgorithm. A user may specify that instance fields are this\nmutable, readonly or mutable and other references (static\nfields, local variables and parameters) are readonly or mu\ntable.\nA user declaring either a field or nonfield reference to be\nreadonly causes the algorithm, upon finishing, to check if\nit is safe to declare the given reference as readonly. If not,\nthe algorithm either issues an error, stating the conflict with\nthe user annotation, or recommend ways to change the code,\ndiscussed in section 5.1, such that reference would be safe\nto declare be readonly.\nIt is expected that this type of declaration will be particu\nlarly useful for overridden methods (see section 3.2) because\ndeclaring a supertype's method to be readonly would propa\ngate to all the subtypes. For example, a user could annotate\nObject's toString, hashCode, and equals methods as having\nreadonly this parameters. The user would then be notified\nif any class's implementation of the above methods are not\nreadonly. Such cases may arise from errors or usage of a\ncache fields that should be declared assignable.\nA user declaring a nonfield reference mutable or a field this\nmutable results in the system adding an unguarded con\nstraint that the reference cannot be readonly. This speci\nfication is useful when a programmer knows that a certain\nreference is not currently used to modify its referent, but\nthat it may be used to do so in the future. This case is ex\npected to be particularly relevant for method return types\nwhen running the algorithm with the closed world assump\ntion (see section 3.1.1). For example, a user may wish to\nannotate an program under the closed world assumption but\nspecify that certain public observer methods return mutable\nreferences.\nFinally, a user may declare fields to be assignable or mutable.\nThese annotations are particularly important because our\nalgorithm cannot soundly infer fields to be assignable or\nmutable as it would require knowing which fields the pro\nassignable(f)\n(SetA)\nx.f = y : {f → y}\n¬assignable(f)\n(SetN)\nx.f = y : {x, f → y}\nmutable f; : {f} (Mutable)\nmutable(f)\n(RefM)\nx = y.f : {}\n¬mutable(f)\n(RefN)\nx = y.f : {x → f, x → y}\nFigure 6: Modified constraint generation rules for\nhandling assignable and mutable fields.\ngrammer intended to be a part of an object's abstract state.\nOur algorithm, however, can be extended to work with fields\nthat are declared, by an outside source, to be assignable or\nmutable. This ability is important because fields may be de\ntermined to be mutable or assignable by hand or through\nheuristics (section 5).\nTo extend our algorithm to handle mutable and assignable\nfields, the constraint generation rules are extended to check\nthe assignability and mutability of fields before adding con\nstraints.\nThe algorithm is given a set of fields that are\ndeclared to be assignable, AS, and the set of fields that\nare declared to be mutable, MS.\nThe auxiliary functions\nassignable(f) and mutable(f) returns true if and only if f is\ncontained in AS or MS, respectively. The changes to the\nconstraint generation rules are shown in figure 6 and are\ndescribed below.\nTo handle assignable fields, the Set rule is divided into two\nrules, SetA and SetN, which depend on the assignability\nof the field.\nIf the field is assignable, then SetA does\nnot add the unguarded constraint that the reference used\nto reach the field must be mutable, because an assignable\nfield may be assigned through either a readonly or mutable\nreference. If the field is not assignable, then SetN proceeds\nas normal.\nTo handle mutable fields, we add the constraint generation\nrule Mutable, which adds an unguarded constraint for each\nmutable field. The Ref rule is again divided into two rules:\nRefM and RefN depending on the mutability of the field.\nIf the field is not mutable, then RefN proceeds as normal.\nIf the field is mutable, then RefM does not add any con\nstraints because, when compared to RefN, (1) the first\nconstraint, that of the field, has already been added to the\nconstraint set via the Mutable rule, and (2) the second con\nstraint, applied to the reference used to reach the field, is\neliminated because mutable field will be mutable even when\nreached through a readonly reference.\n3.4 Arrays and parametric types\nOur algorithm can be extended to handle arrays and para\nmetric types. We begin by discussing the issues involved for\n\n/\n.\nGrammar:\ns0\n::=\ns\nx[x] = x\n|\nx = x[x]\n|\nFigure 7: Core language grammar extended for ar\nrays.\narrays; parametric types are a simple extension.\nWe extend our core language grammar to allow for storing\nand reading from arrays. The extended grammar is shown\nin figure 7.\n3.4.1 Constraint variables\nJavari allows array elements to have twosided bounded types.\nFor example, the array (? readonly Date)[] has elements\nwith upper bound readonly Date and lower bound mutable\nDate. All array element types can be written in the form of\nhaving an upper bound and a lower bound. For example,\n(readonly Date)[] has elements with upper bound readonly\nDate and lower bound readonly Date. Therefore, our algo\nrithm will need infer an upper bound and a lower bound for\nthe types of array elements.\nIn addition to the arrays themselves, we must constraint\nthe upper bound and lower bound of an array's elements.\nThus, we create constraint variables for an array's elements'\nupper and lower bounds in addition to the array's constraint\nvariable. For a reference x that is a single dimension array,\nwe denote the array's mutability by the constraint variable\nx[] and the array's elements' upper bound by x/ and the\nlower bound by x..\n3.4.2 Constraint generation\nThe constraint generation rules are extended to add con\nstraints between array elements during an assignment be\ntween array references.\nFor the assignment, x = y where\nx and y are arrays, the extension must enforce that y is a\nsubtype of x. Simplified subtyping rules for Javari are given\nin figure 8. The simplified rules only check the mutability\nof the type because we assume the program being converted\ntype checks under Java. For parametric types, the subtyp\ning rules check that the element types are in a contains rela\ntion. 9 An array element's type (or a parametric class's type\nargument), TA2, is said to be contained by another array el\nement's type, TA1, written TA2 <= TA1, if the set of types\ndenoted by TA2 is a subset of the types denoted by TA1. Our\nsubtyping rules use ∗ to denote additional, possibly zero,\nlevels of the array.\nWe modify the constraint rules to enforce the subtyping re\nlationship across assignments including the implicit assign\nments that occur during method invocation. The extended\nrules are shown in figure 9.\n3.4.3 Constraint simplification\n9In Java, array are covariant; however, in Javari, array are\ninvariant in respect to mutability, therefore, we use the con\ntains relationship as Java's parametric types do.\ny[]∗[] → x[]∗[]\nx[]∗ <= y[]∗\nx[]∗[] <: y[]∗[]\nx\ny →\nx <: y\n/ <: y[]∗\ny[]∗\nx[]∗\n. <: x[]∗\nx[]∗ <= y[]∗\nFigure 8: Subtyping rules for mutability in Javari.\nx = y : {y<: x} (Assign)\nthis(m) = thism\nparams(m) = p\nretVal(m) = retm\n(Invk)\nx = y.m(y) : {y<: thism, y<: p, retm <: x}\nretVal(m) = retm\n(Ret)\nreturn x : {x<: retm}\nx = y.f : {f<: x, x → y} (Ref)\nx.f = y : {x, y<: f} (Set)\nx = y[z] : {y[] <: x} (ArrayRef)\nx[z] = y : {x, y<: x[]} (ArraySet)\nFigure 9: Constraint generation rules in presence of\narrays.\n\nBefore the constraint set can be simplified as before, sub\ntyping and contains constraints must be reduced to guarded\nconstraints. To do so, each subtyping or contains constraint\nis expanded by adding the rule's predicates, as shown in fig\nure 8, to the constraint set. This step is repeated until only\nguarded and unguarded constraints remain in the constraint\nset. For example, the statement x = y where x and y are one\ndimensional arrays would generate then reduce subtyping\nand contains constraints as follows:\nx = y :\n{y[] <: x[]}\n:\n{x[] → y[], y <= x}\n:\n{x[] → y[], y/ <: x/, x. <: y.}\n:\n{x[] → y[], x/ → y/, y. → x.}\nThe first guarded constraint enforces that y must be a mu\ntable array if x is a mutable array. The second and third con\nstraints constrain the bounds on the arrays' elements types.\nx/ → y/ requires the upper bound of y's elements to be mu\ntable if the upper bound x's elements is mutable. This rule is\ndue to covariant subtyping between upper bounds. y. → x.\nrequires the lower bound of x's elements to be mutable if\nthe lower bound y's elements is mutable. This rule is due to\ncontravariant subtyping between lower bounds.\nAfter eliminating all subtyping or contains constraints, the\nremaining guarded and unguarded constraint set is simpli\nfied as before.\nUnlike the case without arrays, this approach is not guar\nanteed to alway provide the most general method signa\ntures\n, or, therefore, the maximum number of readonly\nannotations. We are unable to always infer the most gen\neral method signature because in some cases one does not\nexist. For example, take the method below:\nvoid addDate(Date[] a, Date d) {\na[0] = d;\n}\nThree typecorrect Javari method signatures can be infer\n(our algorithm as stated above infers the first):\naddDate( (readonly\nDate)[] a, readonly\nDate d)\naddDate( (? readonly\nDate)[] a, /*mutable*/ Date d)\naddDate( (/*mutable*/ Date)[] a, /*mutable*/ Date d)\nIn all cases the array a is mutable. The last method signa\nture is strictly less general than the second, so we will no\nlonger consider it. The first method signature allows a more\ngeneral type for d because could be applied to readonly or\nmutable Dates. The second method signature allows a more\ngeneral type for a because it could applied to arrays with\nreadonly, mutable, or ? readonly elements.\nThus, neither\nthe first or second method signature is the most general and\nthere is no most general method signature that can be in\nferred.\nFurthermore, this problem cannot be solved with method\noverriding, including romaybe (section 2.3), because one is\n10The most general method is the method that accepts the\nmost types of arguments. For example foo(readonly Date)\nhas a more general method signature than foo(mutable*\nDate), because it can take readonly Dates in addition to\nmutable ones.\nnot allowed to overload methods based only on differing mu\ntabilities of the arguments in Javari and romaybe in this case\nwould not expand to the needed types.\nThree approach to deal with this situation are:\n1. Duplicate the method into two versions with different\nnames and then infer which should be method should\nbe used at each call site.\n2. Rewrite the method to be generic using type parame\nters.\n3. Try both signatures, and see which one allows the most\nreferences to be declared readonly.\nThe first and second techniques would guarantee the max\nimum number of readonly annotations, but make a larger\nchange to the program by adding or parameterizing meth\nods in addition to adding immutability annotations. The\nthird technique could infer less readonly annotations in the\ncase that both versions of the method are needed but makes\na smaller change to the code. Unfortunately, the third tech\nnique's runtime would grow exponentially in the number of\nsuch methods, if it tried every possible combination of sig\nnatures.\nEvaluating these techniques in practice remains\nfuture work.\n3.4.4 Parametric types\nParametric types are handled in the same fashion as arrays.\nIn their case, a constraint variable must be made for each\ntype argument to a parametric class.\n4. INFERRING ROMAYBE\nOur core readonly inference algorithm can be extended to\ninfer the romaybe keyword.\nDoing so allows more precise\nimmutability annotations to be inferred.\n4.1 Motivation\nFor our algorithm to be practical, it must also be able to\ninfer romaybe types. romaybe is used to create two versions\nof a method: a readonly version, which takes a readonly\nparameter and returns a readonly type, and a mutable ver\nsion, which takes mutable parameter and returns a mutable\ntype (see section 2). For example, in figure 10, the getSeat\nmethod could be declared to have a romaybe this parame\nter and return type. Declaring getSeat to be romaybe allows\nthe mutable version to be used by lowerSeat, which mutates\nthe returned Seat object, and the readonly version to be\nused by printSeat, which does not mutate the returned Seat\nobject.\nProviding both versions of the method is benefi\ncial, because by using the readonly version of the getSeat\nmethod, printSeat's parameter b may be declared readonly\nas one would expect, since printSeat does not modify its ar\ngument. The results of the readonly inference when getSeat\nis declared to be romaybe are shown in figure 11.\nOn the other hand, if the algorithm is unable to infer romaybe,\nit will be able to only provide the mutable version of the\nmethod as a mutable return type is required for the lowerSeat\nmethod. Unfortunately, inferring only the mutable version\nof getSeat requires contexts that do not need a mutable Seat\nobject to still invoke the mutable version of a method and,\n\nclass Bicycle {\nprivate Seat seat;\nSeat getSeat() {\nreturn seat;\n}\n}\nstatic void lowerSeat(Bicycle b) {\nSeat s = b.getSeat();\nseat.height = 0;\n}\nstatic void printSeat(Bicycle b) {\nSeat s = b.getSeat();\nSystem.out.prinln(s);\n}\nFigure 10:\nJava code containing getSeat method\nwhich should be declared to be romaybe.\nclass Bicycle {\nprivate Seat seat;\nromaybe Seat getSeat() romaybe {\nreturn seat;\n}\n}\nstatic void lowerSeat(/*mutable*/ Bicycle b) {\n/*mutable*/ Seat s = b.getSeat();\nseat.height = 0;\n}\nstatic void printSeat(readonly Bicycle b) {\nreadonly Seat s = b.getSeat();\nSystem.out.prinln(s);\n}\nFigure 11:\nSound and precise Javari code which\ngives getSeat romaybe type.\nclass Bicycle {\nprivate Seat seat;\n/*mutable*/ Seat getSeat() /*mutable*/ {\nreturn seat;\n}\n}\nstatic void lowerSeat(/*mutable*/ Bicycle b) {\n/*mutable*/ Seat s = b.getSeat();\nseat.height = 0;\n}\nstatic void printSeat(/*mutable*/ Bicycle b) {\n/*mutable*/ Seat s = b.getSeat();\nSystem.out.println(s);\n}\nFigure 12: Sound but imprecise Javari code which\nnot using romaybe. getSeat is given the imprecise mu\ntable type instead of the precise romaybe type.\ntherefore, provide a mutable receiver to getSeat. Thus, the\nprintSeat method which does not require a mutable refer\nence to the Seat object returned by getSeat, would still need\nto provide mutable reference as the receiver to the getSeat\nmethod as shown in figure 12.\nThis situation can cause\nmany references, such as printSeat's parameter b, to be de\nclared mutable when they are not used to mutate the object\nto which they refer. While these annotations are sound they\nsuffer from a loss of precision.\n4.2 Approach\nTo extend our readonly inference algorithm, we create two\nversions of every method and invoke one version where a mu\ntable return type is needed and the other when a mutable\nreturn type is not needed. If the type of any of the param\neters of the method is different between the two versions,\nthose parameters and the return type should be declared\nromaybe.\nOtherwise, the two contexts are redundant, and\nonly a single method should be created.\n4.2.1 Method contexts\nWe extend our readonly inference algorithm to infer romaybe\nby recognizing that romaybe methods. In the first context,\nthe return type and romaybe parameters are mutable. In the\nsecond context, the return type and romaybe parameters are\nreadonly. Since we do not know which methods are romaybe\nbefore hand, we create both contexts for every method.\nIn the case of methods that should not have romaybe param\neters, the two contexts are redundant, that is, mutability of\nparameters from each context will be identical. However, in\nthe case of methods should romaybe parameters, the param\neters will be mutable in the mutable context and readonly\nin the readonly context.\nTo create two contexts for a method, we create two con\nstraint variables for every method local reference (param\neters, local variables, and return value).\nTo distinguish\nconstraint variables from each context, we superscript the\nconstraint variables from the readonly context with ro and\nthose from the mutable context with mut. Constraint vari\nables for fields are not duplicated as romaybe may not be\n\nthis(m) = thism\nparams(m) = p\nretVal(m) = retm\nmut\n?\n?\nmut\n?\nx = y.m(y) : {x ? → thism\n→ y , x → p\n→ y , x → retm}\nFigure 13: Constraint generation rule, Invkromaybe\nfor method invocation in the presence of romaybe ref\nerences.\napplied to fields and, thus, only single context exists.\n4.2.2 Constraint generation rules\nWith the exception of Invk, all the constraint generation\nrules are changed to generate identical constraints for both\nthe readonly and the mutable version of constraint vari\nables. Otherwise, the constraints are the same. For exam\nple, x = y generates the constraints:\nro\nro\nmut\nmut\n, x\n{x\n→ y\n→ y\n}\nFor short hand, we write constraints that are identical with\nthe exception of constraint variables' contexts by super\nscripting the constraint variables with ? . For example, the\nresults of the x = y constraint generation rule can be written\nas:\n?\n?\n{x → y }\nThe method invocation rule must be modified to invoke the\nmutable version of a method when a mutable return type\nis needed and to invoke the readonly version, otherwise.\nFor the method invocation, x = y.m(y), a mutable return\ntype is needed when x is mutable. Otherwise, the readonly\nversion of the method is used. When the mutable version\nof a method is used, the actual receiver and arguments to\nthe method must be mutable if the this parameter and for\nmal parameters of the mutable version of the method are\nmutable. When the readonly version of a method is used,\nthe actual receiver and arguments to the method must be\nmutable if the this parameter and formal parameters of the\nreadonly version of the method are mutable. The new Invk\nrule are shown in figure 13.\nThe invocation rule checks whether x is mutable, and in the\ncase that it is, adds constraints that the actual arguments\nmust be mutable of the formal parameters of the mutable\ncontext of the method are mutable. Rule also adds the con\nstraints that the the actual arguments must be mutable of\nthe formal parameters if the readonly context of the method\nare mutable without checking if x is mutable. The check can\nbe skipped because the parameters of the readonly version\nof the method's constraints are guaranteed to be subsumed\nby the mutable version of the method. Thus, adding the\nreadonly version of the invoked method's constraints in ad\ndition to the mutable version's constraints in the case of the\nmutable method being invoked will add nothing new to the\nconstraint set.\nThe constraints are solved as before.\n4.2.3 Interpreting results of solved constraint set\nIf both the mutable and readonly version of a constraint\nvariable is found to be in in the simplified, unguarded con\nstraint set, then the corresponding reference is declared mu\ntable. If both versions of the constraint variable is absent\nfrom the constraint set, then the corresponding reference\nis declared readonly. Finally, if the mutable context's con\nstraint variable is in the constraint set but the readonly\nconstraint variable is not in the constraint set, the corre\nsponding reference is declared romaybe because the mutabil\nity of the reference depends on which version of the method\nis called.11\nIt could be the case that a method contains romaybe refer\nences but no romaybe parameters. For example, below, x and\nthe return value of getNewDate could be romaybe.\nDate getNewDate() {\nDate x = new Date();\nreturn x;\n}\nHowever, romaybe references are only allowed, or useful, if\nthe method has a romaybe parameter.\nThus, if none of a\nmethod's parameters are romaybe, all the method's romaybe\nreferences are converted to be mutable references.\n5. INFERRING MUTABLE AND ASSIGNABLE\nThe core algorithm infers references to objects that are not\nmodified as readonly.\nWhile it is able to leverage user\nprovided assignable and mutable annotations for inferring\nreadonly, it is not able to infer the assignable and mutable\nannotations as these represent overriding of the default tran\nsitive immutability guarantees of the Javari language. We\npresent a technique that infers and provides recommenda\ntions when the default transitive immutability guarantees\nshould be overridden by leveraging references that have been\nmarked readonly but are being modified. Since these refer\nences are annotated readonly but are being modified, the\nreadonly inferencer will not have recommended such an\nnotations and they are likely to be provided by users or via\nother techniques such as via heuristics. We present two such\nheuristic to detect fields used as caches and mark method\nreferences as readonly.\nSince the provided recommendations represent overriding of\nthe immutability guarantees, these techniques are disabled\nby default and need to be explicitly triggered by the user via\na Javarifier input option. The recommendations provided by\nthe sections are presented to the user on as ranked lists of\nfields that should be assignable or mutable.\n5.1 Leveraging readonly annotations\nIn order to detect the overriding of default transitive im\nmutability guarantees, we need to detect references that are\nbeing modified or assigned but have been marked readonly.\nSince these references are annotated readonly but are be\ning modified, the readonly inferencer will not have recom\nmended such annotations and they are likely to be provided\nby users or via other techniques (as described in the next\nsection). Consider the balance method below:\nclass BankAccount {\n11The case that readonly constraint variable is not found\nin the constraint set but the mutable context's constraint\nvariable is not cannot occur.\n\nint balance;\nList<String> securityLog;\nint balance() readonly {\nmethodLog.add(\"balance checked\");\nreturn balance;\n}\n}\nIn this example, the readonly inference algorithm finds the\nmethod balance (the parameter thisbalance ) to be mutable\nbecause it is used to mutate this.securityLog.\nHowever,\nsince the user has specified that thisbalance should be read\nonly, our technique would need to recommend securityLog\nto be declared mutable.\nWhen there are multiple field references leading to an as\nsignment, there may be multiple ways of declaring fields to\nbe mutable or assignable that would resolve the conflict. It\nis always more favorable to modify a field of the class con\ntaining the method in question than modifying a field of\nanother class. For example, consider the code below:\nclass Cell {\nObject val;\n}\nclass Event {\nCell current;\nbeginWeek() readonly {\n((Date) current.val).day = Date.Monday;\n}\n}\nThe technique will need suggest that either current or val\nis declared mutable; or day is declared assignable. However,\nchanging the declarations of val or day is a nonlocal change;\nthus, it is preferred to modify current.\nApproach:\nThis technique needs to find all references be\ning modified, find their cause, and suggest them to be anno\ntated as mutable or assignable. The constraintset provided\nby the readonly inferencer contains most of the information\nneeded by the technique. As can be seen in in figures 3 and\n6, whenever an object can be modified guarded constraints\nare added to the constraintset. Thus, following the guarded\nconstraints backwards from a modified reference annotated\nas readonly allows finding all references that need to be\nconsidered as being annotated mutable.\nThe backwards traversal would end in the actual modifica\ntion, represented as an unguarded constraint. This happens\nwhen the object is being directly modified by assigning a\nfield (the Set rule), unguarded constraints are added to the\nconstraintset. In such cases, we use an objectMutatorFieldSet\nto track the field which is being used to mutate the object,\neach unguarded constraint therefore has its causing field\nadded to this set. Mutator fields found in this set indicates\nthe fields that need to be considered as being annotated\nassignable.\nAlgorithm:\nImplementing support for this technique thus\nhas two requirements. The readonly inferencer is modified\nto add the cause for every unguarded constraint, i.e. the\ndirect mutation done by field assignments should result in\na mapping being added to the the objectMutatorFieldSet.\nThen after the readonly inference is run, and constraints\nare solved, four simple steps are performed:\n1. It checks if a userannotated readonly reference is marked\nas mutable.\n2. Goes backwards through constraintset, from the ref\nerence to collect references which are modified causing\nthe current reference to be mutable.\n3. Maintain the order of collected fields -- a breadthfirst\nbackwards traversal through the constraintset lists\nthe fields from the most recent and local causes to\nindirect causes.\n4. Recommend the collected fields to the user for anno\ntations, either as mutable or as assignable. If a field\nis added as the cause in objectMutatorFieldSet then\nit needs to be annotated as assignable, otherwise the\nfield needs to be annotated as mutable.\nSince userprovided annotations are by default inferred to be\ninherited by subclasses, this technique allows one to anno\ntate a few common methods like Object.hashCode and Object.\ntoString as readonly and the parameters of PrintStream.print\nas readonly, thereby resulting in all subclasses converting\ntheir overriding methods to also include the readonly anno\ntation.\n5.2 Heuristics and Cache Detection\nThe main cases where the mutable and assignable keywords\nare used is in annotating caches. There are two common\ntraits of caches and are provided as heuristics:\n1. Fields which are private and only referenced or as\nsigned in a single method.\n2. Fields using the Java transient keyword (designed to\nmark fields that should not be saved during object\nserialization).\nThe above heuristics use the technique in the previous sec\ntion to determine if making the fields mutable or assignable\nwill result in the corresponding methods becoming readonly.\nOnly field annotations that will result in adding the readonly\non the method are recommended to the user (in order to\nminimize the number of annotations the user has to con\nsider, especially if providing such annotations do not result\nin improved annotations of other parts of the code).\nThe heuristics run on the results of the readonly inferencer.\nA search for all occurrences of cached fields and their re\nspective methods are performed as suggested by the heuris\ntics.\nThe previous technique is then invoked, to traverse\nthe guarded constraints are traversed in a backward man\nner from the searched method. If one of the potential output\nfields is a cache variable, then the field is recommended for\nthe mutable or readonly annotation.\n\nopenness\nchecker\nconstraint generator\ncode-updater\nassignable/\nmutable\ninferencer\nCode\nLegend:\nData Flow\nCauses\nUser-provided\nreadonly\nannotation\nUser-provided\nassignable /\nmutable\nannotations\nCode\nInference\noptions\nUser-provided\nnative method\nannotations\nromaybe\nconstraints set\ndata structure\nJavarifier\nFigure 14: Data flow diagram of Javarifier.\n6. IMPLEMENTATION\ndetect if any fields are to be recommended for being anno\nWe have implemented the readonly and romaybe inference\nalgorithm in the absence of arrays and parametric types. As\nmentioned in this paper, the algorithm can be extended to\nhandle such constructs.\nJavarifier leverages userprovided\nannotations in its inference, and uses readonly annotations\nreferences towards inferring mutable and assignable key\nwords.\nHeuristics for cache detection are expected to be\nsimple extensions but are currently not implemented.\nTo enable the annotation of library classfiles in the absence\nof source code, Javarifier takes classfiles as input. Users can\nspecify a number of commandline options to enable/disable\nvarious components of the inferencer, including running us\ning the closedworld assumption and running the various\nmutable and readonly inferencers. Options also specify files\ncontaining external annotations, as well as files for annota\ntions of library API's and native methods. Javarifier, pro\ncesses these inputs and returns a list consisting of the mu\ntability of all references (parameters, fields and locals). In\nthe future work we discuss extending Javarifier to directly\napply its results to class and source files.\nAs shown in Figure 14, Javarifier processes the various in\nputs and build a constraint set. We use the Soot library [?],\nto convert Java classfiles into a threeaddress programming\nlanguage, for the constraint generator to build the constraints.\nThe openworld mode is then checked and constraints are\nadded to the constraint set. The constraint generator then\nleverages userprovided annotations during its inference, and\nbeyond readonly inference also includes constraint informa\ntion needed for romaybe inference in the constraint set. The\nconstraint generator also also builds a set of fields causing\nthe objects to be mutated, in order to support assignable\nand mutable inferences the constraint generator .\nThe constraints solver processes the constraints from the\nconstraint generator. These solved constraints are then ex\namined, by the assignable/mutable inferencer (if enabled) to\ntated. A checker then ensures that userprovided readonly\nannotations do not conflict with the mutability recommen\ndations as generated by the readonly inferencer, and reports\nconflicts to the user as errors.\n7. EVALUATION\nOur results show that our readonly inference algorithm is\nsound and precise with the exception of immutable classes.\n7.1 Methodology\nWe evaluate Javarifier's readonly inferencing algorithm by\ncomparing its results against handproduced annotations.\nWe have handannotated the key classes in Gizmoball, an\nimplementation of a pinball game done for a MIT software\nengineering class.\nOver approximately 8 hours, we anno\ntated 33 classes in the Gizmoball project, consisting of about\n8000 lines of code. Due to time constraints, we eliminated\nclasses that dealt with the graphical user interface. Those\nclasses are not as interesting because they did not make use\nof abstract state.\nInstead, we chose classes that modeled\nthe state of the pinball game, its elements and the physics\nbehind the pinball game.\nWe annotated all nonprimitive references under the open\nworld assumption, that is, we annotated all public methods\nto return mutable references. This choice led to an interest\ning result for immutable objects. In the case of references\nto immutable objects used as a return type, we labelled the\nreferences as mutable although this label is counterintuitive:\nin the case of references to immutable objects, we know that\nthese objects can never be mutated, either through a muta\nble or readonly reference. However, we cannot label these\nreferences as readonly, to ensure that any other client code\nthat happens to use the method will type check. Further\nmore, the specification of the Javari language [13], does not\nallow the readonly keyword to be applied to references to\nclasses that are declared immutable.\n\nHowever, our implementation does not actually infer im\nmutable classes, and, thus, we can suffer a loss of precision\nwhen a field refers to object of an immutable class. In this\ncase, our analysis would require any reference to the enclos\ning class to be mutable if accessing the field as a mutable\ntype. This situation is imprecise, because the field's type\nshould be treated the same whether reached through a mu\ntable or readonly reference. To work around this impreci\nsion, we declared all such fields mutable.\nAnother possible source of imprecision is method overload\ning, particularly in the case of interfaces.\nAll overloaded\nmethods must have the same method signatures. That means\nif one subtype mutates itself in a method, that mutating\nmethod annotation will be propagated up to the annotation\nof the interface and back down to all subtypes of the inter\nface, even subtypes that do not actually mutate the receiver.\nAgain, we not a loss of precision but not soundness. Addi\ntionally, this behavior is required by the Javari language.\nTo avoid annotating library code through Javarifier, we sim\nulated library annotations by providing external library an\nnotations to the Javarifier. For example, List.clear() has the\nsignature ro List.clear() mut, meaning that the method\nwill mutate its receiver and return a readonly object. For\nsimplicity, we represented void returns and primitives as\nreadonly.\nHowever, these library annotation simulations\nare not sound. Namely, the method annotations were pro\nvided based on our understanding of the library classes and\nwhether we believed the parameters and fields of a library\nclass would be mutated. The more sound approach is to ac\ntually look at the source code for these library classes before\nmaking the annotations.\nThe current implementation of Javarifier can not handle ar\nrays and parametric types. Therefore, we converted all uses\nof arrays to Lists and did not add any parameterized decla\nrations (Gizmoball was written in Java 1.4), including muta\nbilities. Because of this limitation, for a collection class like\nList we can only have annotate for the actual list, but not\nfor the parameterized type of the list. Therefore, the same\nmutability would need to be inferred for all List elements,\na major source of imprecision.\nThis limitation forced us\nto create two copies of the List class and other Collections\nclasses. We created a ListOfMutable and a ListOfReadonly\nclass in order to make the analysis of the Javarifier more\nprecise.\nThe results of our handannotations are summarized in fig\nure 15.\nTo evaluate the readonly inferencing algorithm, we pro\nvided the algorithm with the set of fields handannotated\nto be assignable or mutable. In addition, we provided a set\nof annotations for library method calls. An evaluation of\nJavarifier's performance is given in the next section.\n7.2 Results\nWe ran Javarifier's readonly inference algorithm on Giz\nmoball under threedifferent set of conditions: without hand\nannotated assignable and mutable fields, with handannotated\nfields, and under the closed world assumption with hand\nannotated fields. The results of running Javarifier's read\nonly inference algorithm without providing it the handannotated\nassignable and mutable fields on are given in figure 16. In\naddition, the results of running Javarifier's inference algo\nrithm along with providing a set of assignable and mutable\nfields, shown in figure 17. We also ran Javarifier on the sam\nple project under the closeworld assumption with the same\nset of assignable and mutable fields; these results are shown\nin figure 18.\nAlthough we only handannotated Gizmoball under the sec\nond assumption-- with assignable and mutable fields un\nder the closed world assumption--in the other cases, we in\nspected the changes in Javarifier's output and found it to be\ncorrect.\nFigure 19 shows a comparison of the hand annotations with\nthe Javarifier results under the three conditions mentioned\nabove. Using the hand annotations as the control, under the\nfirst condition (no handannotated assignableand mutablefields),\nthe only difference is that there are more mutableannotations\nin the Javarifier results than the hand annotations because\nall methods that mutate the fields are labelled as mutable\neven if the field is not be part of the abstract state of the\nobject. Under the second condition (user provided annota\ntion of assignable and mutable fields), the hand annotations\nand Javarifier annotations match exactly, as expected. Un\nder condition 3 (closedworld assumption), there are more\nreadonlyannotations in the Javarifier results as expected be\ncause public method are no longer to require return mutable\nreferences in all cases.\n8. RELATED WORK\nFirst we discuss effect analysis which is a related but orthog\nonal analysis to our readonly inference algorithm. Then we\ndiscuss two other techniques, type inference and the Houdini\napproach, that can be used to infer readonly references but\nhave greater complexity.\n8.1 Effect Analysis\nEffect analyses for Java and in general [3, 11, 9, 10, 7, 6] have\nbeen widely studied.\nSimilar to our algorithm's readonly\nparameters, effect analysis can be used to determine the set\nof a method's \"safe\" parameters [12]. A method parameter\nis safe if the method never modifies the object passed to\nthe parameter during method invocation. Unlike readonly\nreferences, a parameter is not safe if the method never uses\nthe parameter but instead mutates the object through a\ndifferent aliasing reference. Additionally, if safe parameter is\nreassigned with a different object, the safe parameter may be\nused to mutate that object. A readonly parameter, on the\nother hand, may not be used to mutate any object. Safety\nand readonlyness are orthogonal: safety is a property over\nobjects while readonlyness is a property over references.\nAdditionally, our algorithm has a much lower complexity\nthan effectanalyses, which must be contextsensitive to achieve\nreasonable precision .\n8.2 Type inference\nMost approaches for type inference, use a constraintbased\napproach. A basic approach detailed in [8], defines a type\ninference algorithm for basic objectoriented languages. The\n\nro\nmutable\nthismut\nromaybe\nassignable\nreferences\nparams\n\nfields (nonprimitives)\n\nstatic fields (nonprimitives)\n\ninstance fields (nonprimitives)\n\nmethod receivers\n\nmethod params\n\nmethod returns\n\nFigure 15: Results of handannotating a software project.\nro\nmutable\nthismut\nromaybe\nassignable\nreferences\nparams\n\nfields (nonprimitives)\n\nstatic fields (nonprimitives)\n\ninstance fields (nonprimitives)\n\nmethod receivers\n\nmethod params\n\nmethod returns\n\nFigure 16: Results of Javarifier's readonly inference algorithm with no assignable and mutable external anno\ntations.\nro\nmutable\nthismut\nromaybe\nassignable\nreferences\nparams\n\nfields (nonprimitives)\n\nstatic fields (nonprimitives)\n\ninstance fields (nonprimitives)\n\nmethod receivers\n\nmethod params\n\nmethod returns\n\nFigure 17: Results of Javarifier's readonly inference algorithm with assignable and mutable external annota\ntions.\nro\nmutable\nthismut\nromaybe\nassignable\nreferences\nparams\n\nfields (nonprimitives)\n\nstatic fields (nonprimitives)\n\ninstance fields (nonprimitives)\n\nmethod receivers\n\nmethod params\n\nmethod returns\n\nFigure 18: Results of Javarifier's readonly inference algorithm under the closed world assumption and with\nassignable and mutable external annotations.\nparams\nro\nparams\nromaybe\nparams\nmutable\nfields\nro\nfields\nthismut\nfields\nmut\nHand annotations\nJavarifier without help\nJavarifier with help\nJavarifier closed world\nFigure 19: Evaluation of Javarifier's readonly inference algorithm.\n\napproach involves defining a set of type constraints on the\nprogram and propagating these constraints to a fixed point.\nIn [1], the introduction of the Cartesian Product Algorithm\nbuilds upon basic type inference algorithm to handle para\nmetric polymorphism. By using the Cartesian product to\ndecompose the analysis of the core algorithm, CPA improves\nthe precision, efficiency, generality and simplicity over the\nother algorithms while addressing the issue of parametric\npolymorphism. The Data Polymorphic CPA algorithm [14]\nexpands on the CPA algorithm to address the issue of data\npolymorphism.\n8.3 Houdini approach\nAn approach that has been used successfully for inferring\nannotations is the Houdini approach [5].\nThis approach\ncould work by declaring all references readonly, then iter\natively running a checker to find which annotations causes\nthe checker to fail, and removing such annotations.\nUs\ning such an approach for providing ESC/Java and rccjava\n(race conditions) [4] annotations have been found useful.\nFor Javarifier, this approach would have poor performance\nas it would need to invoke the typechecker multiple times.\nAdditionally, it is unclear if the approach could be used to\ninfer the romaybe keyword.\n9. FUTURE WORK\nWe are working on implementing inferencing in the presence\nof arrays and parametric types. We further plan on using\nJavarifier on real projects to determine the realworld use\nfulness of Javarifier, and determine any additional heuristics\nneeded for making the tool practical.\nSince many developers currently use the Eclipse IDE and\nthe Ant build systems we plan on integrating Javarifier with\nthese environments. We expect the benefits of the Javari\nlanguage to be apparent even when the annotations are not\nprovided in the source file and are in separate external files,\nat least in the few cases where library APIs indicate re\nturn types to be readonly. Future integration would involve\nadding the Javari keywords as part of the source file as ei\nther escaped comments such as /*=readonly*/ or ideally by\nmodifying the various parsers to provide transparent inte\ngration. We also plan on adding support for Javarifier to\napply its results directly to class, so that separate files do\nnot need to be provided by developers.\n10. CONCLUSION\nIn this paper, we have created an algorithm for inferring\nreadonly and romaybe types. We have implemented a use\nful subset of our algorithm within Javarifier and verified its\neffectiveness. We have also designed an heuristic to suggest\nassignable or mutable fields.\n11. REFERENCES\n[1] Ole Agesen. The cartesian product algorithm: Simple\nand precise type inference of parametric\npolymorphism. In ECOOP '95: Proceedings of the 9th\nEuropean Conference on ObjectOriented\nProgramming, Volume 952 of Lecture notes in\nComputer Science, pages 2-26, London, UK, 1995.\nSpringerVerlag.\n[2] Adrian Birka and Michael D. Ernst. A practical type\nsystem and language for reference immutability. In\nOOPSLA '04: Proceedings of the 19th annual ACM\nSIGPLAN conference on Objectoriented\nprogramming, systems, languages, and applications,\npages 35-49, New York, NY, USA, 2004. ACM Press.\n[3] K. D. Cooper and K. Kennedy. Interprocedural\nsideeffect analysis in linear time. In PLDI '88:\nProceedings of the ACM SIGPLAN 1988 conference on\nProgramming Language design and Implementation,\npages 57-66, New York, NY, USA, 1988. ACM Press.\n[4] Cormac Flanagan and Stephen N. Freund. Detecting\nrace conditions in large programs. In PASTE '01:\nProceedings of the 2001 ACM SIGPLANSIGSOFT\nworkshop on Program analysis for software tools and\nengineering, pages 90-96, New York, NY, USA, 2001.\nACM Press.\n[5] Cormac Flanagan, Rajeev Joshi, and K. Rustan M.\nLeino. Annotation inference for modular checkers.\nInformation Processing Letters, 77(24):97-108, 2001.\n[6] Ana Milanova, Atanas Rountev, and Barbara G.\nRyder. Parameterized object sensitivity for pointsto\nand sideeffect analyses for java. In ISSTA '02:\nProceedings of the 2002 ACM SIGSOFT international\nsymposium on Software testing and analysis, pages\n1-11, New York, NY, USA, 2002. ACM Press.\n[7] Phung Hua Nguyen and Jingling Xue. Interprocedural\nsideeffect analysis and optimisation in the presence of\ndynamic class loading. In CRPIT '38: Proceedings of\nthe Twentyeighth Australasian conference on\nComputer Science, pages 9-18, Darlinghurst,\nAustralia, Australia, 2005. Australian Computer\nSociety, Inc.\n[8] Jens Palsberg and Michael I. Schwartzbach.\nObjectoriented type inference. In OOPSLA '91:\nConference proceedings on Objectoriented\nprogramming systems, languages, and applications,\npages 146-161, New York, NY, USA, October 1991.\nACM Press.\n[9] C. Razafimahefa. A study of sideeffect analyses for\nJava. Master's thesis, McGill University, December\n1999.\n[10] A. Rountev and B. Ryder. Practical pointsto analysis\nfor programs built with libraries. Technical Report\nDCSTR410, Rutgers University, February 2000.\n[11] Barbara G. Ryder, William A. Landi, Philip A.\nStocks, Sean Zhang, and Rita Altucher. A schema for\ninterprocedural modification sideeffect analysis with\npointer aliasing. ACM Trans. Program. Lang. Syst.,\n23(2):105-186, 2001.\n[12] Alexandru Salcianu and Martin Rinard. Purity and\nside effect analysis for java programs. In Proceedings\nof the 6th International Conference on Verification,\nModel Checking and Abstract Interpretation, 2005.\n\n[13] Matthew S. Tschantz and Michael D. Ernst. Javari:\nAdding reference immutability to Java. In\nObjectOriented Programming Systems, Languages,\nand Applications (OOPSLA 2005), San Diego, CA,\nUSA, October 16-20, 2005.\n[14] Tiejun Wang and Scott F. Smith. Precise\nconstraintbased type inference for Java. In ECOOP\n'01: Proceedings of the 15th European Conference on\nObjectOriented Programming, Lecture notes in\nComputer Science 2072, pages 99-117, London, UK,\n2001. SpringerVerlag."
    },
    {
      "category": "Resource",
      "title": "object_control_i.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/58b747da7ffc69edfa1228b02b0f0a8c_object_control_i.pdf",
      "content": "Object Control Invariants\nDerek Rayside and Lucy Mendel\nMIT CSAIL 6.883, Prof Ernst\nAbstract\nSharing mutable data (via aliasing) is a powerful program\nming technique. To facilitate sharing, objectoriented pro\ngramming languages permit the programmer to selectively\nbreak encapsulation boundaries.\nHowever, sharing data\nmakes programs harder to understand and reason about,\nbecause, unlike encapsulated data, shared data cannot be\nreasoned about in a modular fashion. This paper presents\nobject control invariants: a set of program properties to help\nprogrammers understand and reason about shared data.\nIntroduction\nSharing mutable data (via aliasing) is a powerful program\nming technique. For example, the modelviewcontroller de\nsign pattern [19] captures the essential structure of many\ngraphical user interfaces: many controllers and views share\none model object.\nTo facilitate sharing, objectoriented programming lan\nguages permit the programmer to selectively break encap\nsulation boundaries. Visibility keywords such as private\nsuggest that some data should be encapsulated, but do not\nprevent public methods from returning aliases to that (sup\nposedly) internal data.\nHowever, sharing data makes programs harder to under\nstand and reason about, because, unlike encapsulated data,\nshared data cannot be reasoned about in a modular fashion.\nEncapsulating mutable data facilitates modular reasoning\nabout object invariants. For example, consider a linked list\nimplementation with a sentinel at the head and the invariant\nthat the next field of the elements forms a cycle. If we know\nthat elements are only manipulated by the list that owns\nthem, then we need only examine the code of LinkedList and\nLinkedListElement in order to verify the invariant. The more\ndata is encapsulated, the easier it is to reason about the\nprogram.\nPermission to make digital or hard copies of all or part of\nthis work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page. To copy otherwise, to republish,\nto post on servers or to redistribute to lists, requires prior\nspecific permission and/or a fee.\n6.883'05 MIT EECS 6.883 Program Analysis, Prof Ernst\nCopyright held by the authors. December 15, 2005.\nPrevious work has developed various typetheoretic notions\nof object ownership to enable the programmer to specify\nand enforce encapsulation [2, 4, 10, 14, 22, 25, 28, 30]. We\nborrow the notion of ownership from this work, but consider\nit from an analytical rather than a typetheoretic perspec\ntive. Instead of having the programmer specify the data\nthat is encapsulated (ie, not shared), our tool shows the pro\ngrammer which data is shared (ie, not encapsulated). The\nprimary contributions of this work are:\n- An analytic approach to characterizing sharing\nand encapsulation. Most ownership type systems\nhave the programmer annotate the objects that are en\ncapsulated. Our system, on the other hand, visualizes\nthe objects that are shared. This approach has five key\nbenefits:\n- It focuses the programmer's effort on what should\nbe the exceptional case (sharing), rather than\nwhat should be the normal case (encapsulation).\n- It characterizes the sharing that exists.\n- It encourages the programmer to reduce unnec\nessary sharing, eliminate erroneous sharing, and\ndocument essential sharing.\n- It requires less upfront effort from the program\nmer, because the program does not need to be\nannotated with ownership type declarations.\n- It characterizes sharing so that the program may\nbe refactored to use shared objects in a more par\nsimonious and orderly fashion.\n- A lightweight ownership inference algorithm.\nMost previous work on ownership inference (eg, [1, 2,\n10, 21, 34]) has focused on inferring annotations that\nwill typecheck in some type system.\nWe just infer\nthe ownership structure, without inferring local anno\ntations, which is both faster and easier.\nWe previously developed an imprecise static analysis (based\non RTA [5, 6]) for the purpose of inferring ownership and\ncharacterizing shared data [31]. This paper differs from the\nprevious work in two important ways:\n- We define object control invariants to precisely charac\nterize our notion of ownership and encapsulation.\n- We have designed and implemented a brand new dy\nnamic analysis to compute object control invariants.\n\nh\ni\nContents\n1 Introduction\n2 Object Control Invariants\n2.1 Object Ownership Criteria . . . . . . . . . . .\n2.2 Object Control Properties . . . . . . . . . . .\n2.3 Sets of Objects . . . . . . . . . . . . . . . . .\n2.4 Object Group Control Properties . . . . . . .\n2.5 Messages . . . . . . . . . . . . . . . . . . . .\n3 Dynamic Detection of Likely Object Control\nInvariants\n4 Toy Examples\n4.1 Linked List . . . . . . . . . . . . . . . . . . .\n4.2 Observer Design Pattern . . . . . . . . . . . .\n4.3 ArchJava Pipeline (Pipeline.archj) . . . . .\n4.4 ArchJava Repository (Repository.archj) . .\n5 Scalability: TSafe Case Study\n5.1 Reducing Trace Size and Overhead . . . . . .\n5.2 Object Control Dominators . . . . . . . . . .\n5.3 Object Control Properties . . . . . . . . . . .\n5.4 Messages . . . . . . . . . . . . . . . . . . . .\n5.5 Groups . . . . . . . . . . . . . . . . . . . . . .\n6 Related Work\n6.1 Static Specification and Verification . . . . .\n6.2 Inference . . . . . . . . . . . . . . . . . . . . .\n7 Discussion\n7.1 Scalability . . . . . . . . . . . . . . . . . . . .\n7.2 Evaluation . . . . . . . . . . . . . . . . . . . .\n7.3 Future Work . . . . . . . . . . . . . . . . . .\nObject Control Invariants\n2.1\nObject Ownership Criteria\nProgrammers view object x as owning object y if y is part of\nthe abstract state of x. Since it is difficult to mechanically\ndetermine what is part of the abstract state of an object, a\nnumber of different analytical criteria have been proposed\nin the literature:\n- ownerasheapdominator: all paths in the heap to the\nowned object must pass through the owner (eg, [30])\n- ownerasmutator: whoever mutates an object owns it\n(eg, [28])\n- ownerasallocator: whoever allocates (or deallocates)\nan object owns it (eg, [21])\nWe propose a synthesis of these criteria:\n[Defn 1] Ownership. Object x owns object y if x is an im\nmediate dominator of y in the program's write control\ngraph.\nThe following definitions clarify what we mean by this:\n[Defn 2] Control. An object is in control when it is the\nreceiver of a method on the call stack.\n[Defn 3] Control Chain. A control chain is the list of ob\njects in control, starting with the entry point.\n[Defn 4] Write Control Chain. A write control chain is\na control chain that corresponds to writing a field: ie,\nwhere the last object in the chain executes a field store\ninstruction.\n[Defn 5] Control Chain Link. A link in a control chain\nis a tuple x, y where x immediately preceedes y in the\nchain.\n[Defn 6] Controlledby. Object y is controlledby object\nx when x preceeds y in some control chain.\n[Defn 7] Control Graph. A control graph is the union of\na set of control chains. Consider a control chain as a\nset of links; a graph is the union of these link sets from\na collection of chains.\n[Defn 8] Write Control Graph. A write control graph is\na control graph comprised of only write control chains.\n[Defn 9] Dominator. In a directed graph with a distin\nguished root node, node x dominates node y if every\npath from the root to y must pass through x. Each\nnode has a set of dominators.\n[Defn 10] Immediate Dominator. Node x is the imme\ndiate dominator of y if it is the 'closest' dominator.\nMore formally, dom(y) - dom(x) =\nEvery node\n{x}.\n(in a directed graph with a distinguished root element)\nhas exactly one immediate dominator.\nOur definition of ownership is based on object control, rather\nthan on the heap structure. In this view, it doesn't matter\nhow one got an alias to an object, it just matters what one\ndoes with that alias.\n\n2.2\nObject Control Properties\nEach object has one of the following object control proper\nties that characterizes how it is controlled in the program:\n[Defn 11] Unique Controller. Object x uniquely con\ntrols object y if if x immediately preceeds y in every\ncontrol chain.\n[Defn 12] NonLinear Controller. Object x is a non\nlinear controller of y if x controls y both before and\nafter some other object z controls y.\n[Defn 13] Shared Control. Object y is said to be shared\nif it has one or more nonlinear controllers.\n[Defn 14] Linear Controller. Object y is said to be con\ntrolled linearly if it has neither a unique controller nor\na nonlinear controller.\nAll of these definitions can be rephrased in terms of write\ncontrol chains, which is our primary interest in exploring\nour notion of ownership.\n2.3\nSets of Objects\nThus far our discussion has been in terms of individual ob\njects. In order speak about the program more generally, we\nmust speak of groups of objects. We define the following\nways to group objects:\n[Defn 15] Field Group. The field group of field f is all\nobjects stored into f .\n[Defn 16] Class Group. The class group of class c is all\ninstances of class c.\n[Defn 17] Site Group. The site group of instantiation site\ns is all objects instantiated at s.\nThere are a variety of criteria that can be used to identify\nsets of objects. For example, Kuncak et al. [24] identify\nsets of objects via reachability and heap referencing rela\ntionships.\n2.4\nObject Group Control Properties\nThe possible control properties for a set of objects are as\nfollows:\n[Defn 18] Master Unique Controller. A set of objects s\nis said to have a master unique controller if some object\nx is the unique controller of every object in the set.\n[Defn 19] Individual Unique Controllers. A set of ob\njects s is said to have individual unique controllers if\nevery object in the set has a unique controller, but not\nthe same controller.\n[Defn 20] Linear Individual Controller. A set of ob\njects s is said to have a linear individual controllers\nif every object in the set controlled linearly.\n[Defn 21] Shared. A set of objects s is said to be shared\nif every object in the set is shared.\n[Defn 22] Inconsistent. A set of objects s is said to have\ninconsistent control if some objects in the group are\nshared and others are not.\n2.5\nMessages\nWe consider an object to be a message if one of its fields\nis read. Almost all stateful objects will be messages. We\ndefine the following kinds of messages:\nMsg Kind\nReaders\nWriters\nNote\nPersonal\nSimple\nBroadcast\nBlackboard\n> 1\n> 1\n> 1\nreader = writer\nreader 6= writer\n[Defn 23] Personal Message. Personal\nmessages\nare\nfully encapsulated internal state - ie, 'note to self'.\nThese can be reasoned about locally.\n[Defn 24] Simple Message. Simple messages are what\none would expect in a pipe and filter kind of design:\nthe data passes through each component once.\n[Defn 25] Broadcast Message. Broadcast messages have\nmany readers and one writer. Immutable objects will\nlikely be classified as broadcast messages.\n[Defn 26] Blackboard Message. Blackboard\nmessages\nindicate shared data without any known constraints.\nThe programmer should be made aware of these and\nthey should be well documented.\nDynamic Detection of Likely Object Control In\nvariants\nIt is possible to produce object control graphs from a static\nanalysis or a dynamic analysis. In this paper we present a\ndynamic analysis along the lines of Ernst [18]:\n1. Capture trace data during program execution.\n2. Compute properties for individual objects.\n3. Compute properties for sets of objects by looking for\nthe strongest property that holds for every object in\nthe set.\nWe record the read and write control chains observed during\nexecution, as well as the values written into and read out of\nfields. We use the latter pointsto information for identifying\nthe set of objects stored in each field.\nWe implemented our instrumenter with AspectJ. In com\nprises two aspects: one that actually instruments the code,\nand one that controls the initialization, threading, and I/O\nbehaviour of the instrumenter.\nThe latter two steps are implemented with the Crocopat\nrelational calculator by Beyer et al. [8]. As discussed in the\nscalability section below, these scripts are surprisingly not\nscaling to real world examples, and we are reimplementing\nthem in other languages, which take longer to code but are\nfaster to execute.\nOur tool reports results back to the programmer as Eclipse\nbookmarks.\n\nToy Examples\nWe discuss the results of running our analysis on four toy\nexample programs. The observer design pattern example is\nfrom our previous work [31]. The two ArchJava examples\nare from the current ArchJava distribution.\nFor the ArchJava examples we analyzed the Java files pro\nduced by the ArchJava compiler, and we also manually\ntranslated the ArchJava code into equivalent Java code. The\nmanually translated code has fewer mechanical artefacts in\nit, as one would expect.\nThe graphs in this section were mechanically produced by\nour tool. For some of them we renamed the nodes or re\nmoved the distinguished unnamed root node our analysis\nproduces. In the case of cycles we have also manually given\ndot some hints on the layout.\n4.1\nLinked List\nFigure 8 presents a simple program that adds two strings\nfrom a list, iterates over the list, prints the strings, and\nremoves them from the list.\nFigure 3 lists the write control chains collected from the\nexecution of this program. Figure 1 shows the write control\ngraph formed by merging the chains from Figure 3. Note\nthat there is no Main object in the graph because one is\nnever constructed by the program. There is a Thread object\nt that is in control and invokes the main method.\nfooLink\nbarLink\nt\nlist\nit\nFigure 1 Write control graph for linked list example\nFor this example our analysis infers that:\n- all of the objects are owned by the Thread t;\n- the two Link objects are controlled linearly (ie, control\npasses from the list to the iterator)\n4.2\nObserver Design Pattern\nFigure 2 shows the write control graph for the observer ex\nample in our previous paper [31]. In this program a subject\nholds a date that it notifies an observer about changes to.\nOur previous (static) analysis [31] reported that the sub\nject's representation (a Date) was exposed to the observer.\nOur current analysis reports that the date is controlled lin\nearly. The difference is that our previous analysis considered\nthe allocator to be the owner, and so reported mutations by\nnonowners. Our current analysis does not use allocation\nas a special ownership criteria (allocation is a form of muta\ntion); rather, it reports that the date is owned by the thread\nobject.\nThread\nConcreteSubject\nConcreteObserver\nDate\nFigure 2 Observer design pattern write control graph\nFigure 3 Write control chains from linked list example\nTime\nSource Code\nControl Chain\ninstantiating the list\nhead = null;\nt\nlist\nadding 'foo' to the list\ndata = d;\nnext = n;\nt\nt\n→\n→ list → fooLink\nlist\nfooLink\nadding 'bar' to the list\ndata = d;\nnext = n;\nt\nt\n→\n→\n→\nlist → barLink\nlist\nbarLink\ncreating the iterator\nlist = l;\ncurrent = l.head;\nt\nt\n→\n→\n→\n→\nlist → it\nlist\nit\n→\ngetting the first element\ncurrent = current.next;\nremoving the first element\n...\n...\nt\nt\nt\n→\n→\nit\nit → fooLink\nit\nlist\ngetting the second element\ncurrent = current.next;\nremoving the second element\n...\n...\nt\nt\nt\n→\n→\n→\n→\n→\nit\nit → barLink\nit\nlist\n→\n4.3\nArchJava Pipeline (Pipeline.archj)\nPipeline.archj is a simple pipeandfilter style program:\ndata is constructed by a source, read an modified by a filter,\nand finally sent to a sink. Pipeline.archj is annotated to\nstatically guarantee this behaviour. Our analysis discovers\nthis behaviour dynamically: ie, that the data is controlled\nlinearly, and owned by the pipline.\nFigure 4 shows the write control graph for the Java source\ncode produced by the ArchJava compiler. Figure 5 shows\nthe write control graph for the Java source code produced by\nmanually translating Pipline.archj into Java. Our analy\nsis infers the same results for each variant.\n4.4\nArchJava Repository (Repository.archj)\nRepository.archj has a DataStore that is 'shared' by two Mod\nules. Our analysis discovered that the objects stored into\nDataStore.data are owned by the first module. This initially\nsurprised us, and caused us to read the source code more\ncarefully. We had expected that the RepData objects owned\nby each Module object would be stored in that field.\nFigure 6 shows the write control graph for the Java source\ncode produced by the ArchJava compiler. Figure 7 shows\nthe write control graph for the Java source code produced\nby manually translating Repository.archj into Java.\n\nFigure 7 Write control graph for manually generated\nPipeline.java generated\njavalangThread\nstylesPipeline\nstylesFilter\narchjavaruntimeParent\nstylesPipelineCONNECT0\nstylesPipelineCONNECT1\nstylesSink\nstylesSource\nstylesData\nRepository.java\nModule\nDataStore\nRepData\nModule\nRepData\nRepository\nThread\nFigure 4 Write control graph for\nby the ArchJava compiler.\nFigure 8 LinkedList example source code\npublic class Main {\npublic static void main(String[] args) {\nList l = new LinkedList();\nl.add(\"foo\");\nl.add(\"bar\"):\nfor (Iterator i = l.iterator(); i.hasNext(); ) {\nObject x = i.next();\ni.remove();\nSystem.out.println(x);\nFigure\nThread\nPipeline\nFilter\nSource\nSink\nData\n5 Write control graph for manually generated\n}\nPipeline.java\n}\n}\npublic class LinkedList {\nprivate Link head;\npublic LinkedList() {\nhead = null;\n}\npublic add(Object d) {\n...\nnew Link(d);\n...\n}\npublic iterator() {\nreturn new ListIterator(head);\n}\njavalangThread\nstylesRepository\nstylesDataStore\nstylesRepositoryCONNECT0\nstylesModule\narchjavaruntimeParent\nstylesMessage\nstylesRepData\nstylesSpec\nated by the ArchJava compiler.\nprivate static class Link {\nFigure 6 Write control graph for Repository.java gener\nfinal Object data;\nLink next;\nLink(Object d, Link n) {\ndata = d;\nnext = n;\n}\n}\nprivate static class ListIterator implements Iterator {\nprivate List list;\nprivate Link current;\nListIterator(List l) {\nlist = l;\ncurrent = l.head;\n}\npublic boolean hasNext() { ... }\npublic Object next() {\nObject d = current.data;\ncurrent = current.next;\nreturn d;\n}\npublic void remove() { ... mutate a link ... }\n}\n}\n\nScalability: TSafe Case Study\nOnce we were happy that the basic analysis was working, we\ntried to run it on TSafe [16]. TSafe is a program that reads\na feed describing aircraft positions, plots their trajectories,\nand warns of airplanes that are too close to each other. The\nTSafe code comprises 227 classes. Typical inputs to TSafe\nare hundreds of megabytes. We ran TSafe on a small 450kb\ninput file and noticed some scalability issues.\n5.1\nReducing Trace Size and Overhead\nOur original instrumenter produced a 550mb trace from the\n450kb sample input. The second column of the table in\nFigure 9 characterizes this trace. From these figures we see\nthat reads are approximately three times more frequent than\nwrites, and that the largest parts of the trace are the chains.\nFigure 9 Characterizing the trace. Size of relations mea\nsured in tuples.\nRelation\nOriginal Size\nNew Size\nChange\nRoot\n--\nThread\n--\nNew\n137,751\n129,758\n--\nConstructor\n155,344\n149,404\n--\nWrite\n273,812\n259,471\n--\nRead\n805,179\n717,174\n--\nWriteChain\n1,401,755\n443,404\n31.63%\nReadChain\n4,903,825\n1,552,408\n31.65%\nTrace (bytes)\n554,019,011\n252,282,913\n31.66%\n5.2\nObject Control Dominators\nEven with the reduced trace size and refactored analysis\ncode, our Crocopat [8] script still could not compute dom\ninators for the captured object control graph from TSafe.\nOur script implemented a conventional worklist algorithm\nfor computing dominators.\nFigure 10 characterizes the steps of the dominator algorithm\nbefore it runs out of memory. It took two hours of com\nputation on a 2GHz G5, with 1gb of memory allocated to\nthe process, to produce this table. It shouldn't be that\ncomputationally demanding to compute a dominators for\na graph of only 80,000 nodes (Figure 11), so we decided to\nreimplement the dominator computation in Java.\nWe think that the reason the Crocopat script is so inefficient\nat computing dominators is that each iteration requires two\nexistential quantifiers, which are > O(n 2) [7]. The existen\ntial quantifiers are used to compute the intersection of the\ndominator sets for the predecessors of a node.\nFigure 10 Computing object control dominators for TSafe\nSize of\nSize of\nIteration\nDominator\nWorklist\nRelation\n72,269\n12,206\n289,046\n75,491\n324,647\nCaching a hash of the last chain. It dawned on us that it's\nvery common for a method to read or write multiple fields of\nan object. Each of these consecutive reads or writes would\nproduce essentially the same control chain, just at a differ\nent time. Our analysis is only concerned with the temporal\ninterleavings of control chains, not with the times they actu\nally occur at. So we lose no information by dropping control\nchains that are identical to the last control chain recorded.\nWe now only record a chain in the trace if it differs from\nthe last chain recorded for the target object. To accomplish\nthis, we cache an integer hash of the last chain recorded for\neach object. Since we need to keep this information for every\nobject created by the analysand, using the standard Java map\nimplementations would be too much overhead. The GNU\nTrove library provides an int to int hash map that uses open\naddressing for collision resolution.\nThe third column of the table in Figure 9 shows the size\nof the trace collected with the new cache. The size of the\nchains, and the trace, is one third the size of the original.\nInstrumentation\noverhead. Our original instrumenter\nslowed TSafe down from about 1 minute to around 20 min\nutes. With the cache and improved I/O code, we trimmed\nthis time to about 3 minutes.\nSince TSafe is an interactive program it is not possible to\ntake precise timing measurements.\nWe found Figure 10 very surprising, and it caused us to\ninvestigate the topology of the graph (Figure 11).\nFigure 11 characterizes the topology of the object control\ngraph for TSafe running the 450kb sample input. There is\none node, the TSafe ConfigConsole, with a fanout of almost\n70,000. 98% of the nodes are leaf nodes (have no successors;\nfanout is zero).\nFigure 11 Topology characteristics of object control graph\nfor 450kb TSafe example. 81,809 edges. 80,259 nodes.\nFanIn\nCount\nFanOut\nCount\n78,734\n78,777\n1,508\n1 < f ≤ 10\n10 < f ≤ 100\n100 < f < 69, 270\n69,270\nFigure 11 informed the design decisions for our Java re\nimplementation of the dominator computation. The vast\nmajority of nodes have a single predecessor and no succes\nsors. We name these treeleaf nodes. Similarly, we refer\nto nodes with a single predecessor as treenodes. The set of\ntreeleaf nodes is a subset of the treenodes. We name nodes\nwith multiple predecessors joinnodes.\nTreenodes require only a single pointer to represent their\npredecessor relation -- there is no need to allocate a dynamic\ndata structure. Also, treenodes do not need to explicitly\nrepresent their dominator sets: it is simply the dominator\nset of the predecessor plus self. The immediate dominator of\n\na treenode is simply the predecessor. Figure 10 shows that\nthe explicit representation of the global dominator relation\ncan grow quite large; implicitly representing this information\nfor 98% of the nodes should save a substantial amount of\nspace.\nComputing dominators is a well studied problem. Cooper\net al. [15] provide a recent survey, and argue that a care\nfull implementation of an iterative algorithm can be prac\ntically competitive with more sophisticated algorithms that\nhave lower asymptotic bounds (eg, LengauerTarjan). Geor\ngiadis et al. [20] provide a more recent study that shows the\nLengauerTarjan is competitive with the Cooper et al. [15]\nalgorithm for common controlflow graphs.\nCooper et al. [15] note the following problems with na ıve\nimplementations of the iterative algorithm:\n- Bit vectors are space efficient but are too slow to com\npute intersections. (Computing the intersections is es\nsentially where Crocopat is failing.)\n- Sparse sets compute intersections quickly, but take up\ntoo much space.\nThey note that using properly ordered lists to represent the\ndominator sets enables efficient intersection: the intersec\ntion of two lists is their common prefix. This ordering of\nthe dominator sets also allows trivial computation of the\nimmediate dominator: it's the penultimate element in the\nlist.\nHowever, the na ıve representation of a list, like sparse sets,\ntakes too much space. They note that these lists form a tree\n(the dominator tree), and this tree exploits the redundancy\nin the lists.\nWe think we can get adequate space savings by simply not\nexplicitly representing the dominator set for the treenodes,\nwhich are about 98% of the nodes. Our conjecture is that\nexplicitly storing lists of dominators for the remaining 2% of\nthe nodes (the joinnodes) will not consume an overwhelm\ning amount of space. It is easier to simply keep a list for each\nof these joinnodes than to construct the tree that Cooper\net al. [15] suggest to exploit the common structure of the\nlists.\nWith both the Cooper et al. [15] tree representation and our\nimplicit representation for treeleaf nodes, it is much easier\nto iterate dominator sets from the node backwards to the\nroot, rather than forwards from the root to the node. This\nmeans that the intersection computation iterates backwards\nover the two lists being intersected and stops when they are\nthe same. As Cooper et al. [15] describe, this requires a\n'twofinger' approach, and knowing the preorder number of\nthe nodes (or, in our case, the shortest depth from the root).\nThe Cooper et al. [15] iterative algorithm initializes the dom\ninator set of each node to be the entire graph, and then starts\nfrom the leaves and works backwards, computing intersec\ntions that reduce the size of the sets. This only makes sense\nwhen using the tree to exploit sharing. Since we are not us\ning this data structure, we start with empty sets and work\nforwards from the root (as the idea of dominators is often\ndescribed pedagogically).\nCooper et al. [15] compare their algorithm empirically\nagainst the almost linear time LengauerTarjan algorithm,\nand find that their iterative algorithm performs 2.5 times\nbetter for realistic control flow graphs (this figure has been\ndisputed by Georgiadis et al. [20], who claim the approaches\nare equal). They also construct 'unrealistically large graphs'\nof around 30,000 nodes, and note that the iterative algo\nrithm is still competitive at this size. They suspect that\nthe asymptotic lower bound of the LengauerTarjan algo\nrithm will start to come into play for larger graphs. While\nour graphs are larger, if we subtract the 98% of nodes that\nare treenodes, we are left with a tractable graph of a few\nthousand nodes.\nIt is possible that this TSafe object control graph is not\ntopologically representative of the object control graphs that\nwill be generated from other programs. However, we suspect\nthat having many leaf objects with single predecessors will\nbe common.\nOur Java reimplementation computes dominators and im\nmediate dominators for this TSafe object control graph in\nunder 7 seconds.\n5.3\nObject Control Properties\nComputing the control property (eg, unique, linear, shared)\nof each object with our Crocopat implementation was also\nnot scaling to TSafe. We now believe this is because the\ncomputation of nonlinear parents requires three existential\nquantifiers (there exists a time2 between time1 and time3).\nAs discussed above, existential quantifiers cost > O(n 2) in\nCrocopat.\nWe have augmented our dominator computation to also\ncompute the control property of each object as it constructs\nthe graph. Construction follows the trace: edges that occur\nearlier in the trace are added sooner. We store the predeces\nsors of each node in the order that they are added. When a\nnew predecessor is added, we check if it is already in the list\nof predecessors for that node. If it is already in the list, and\nnot the last element of that list, then we know the node is\nshared. Treenodes (ie, only one predecessor) have a unique\ncontroller. Joinnodes (ie, multiple predecessors) are con\ntrolled linearly if we do not detect while constructing the\ngraph that they are shared.\n5.4\nMessages\nThe Crocopat script that computes the message properties\nof each object completed the 450kb TSafe example in 57\nminutes. This is a simple analysis that basically just counts\nthe number of readers and writers for each object: it should\ntake no more than linear time, and could be implemented\nwith gawk.\nAgain, we think it is the (necessary) use of existential quan\ntifiers in the Crocopat formulation that is causing the prob\nlems.\n5.5\nGroups\nThe Crocopat script that generalizes properties of objects to\nproperties of groups of objects cannot complete the 450kb\nTSafe example. The BDD package runs out of memory. This\nscript contains snippets such as the following, that identifies\nall fields that refer to only shared objects:\n\nFieldGroupShared(c) := EX(o, FieldGroup(c,o) & Shared(o)) &\n!EX(o, FieldGroup(c,o) & !Shared(o));\nPRINT [\"FieldGroupShared\"] FieldGroupShared(c) TO \"out.rsf\";\nThis should be a fairly fast and easy computation, but ex\npressing it in Crocopat requires these existential quantifiers.\nWe will reimplement this computation as well.\nRelated Work\nThe problem of nonlocal mutation of shared data making\nprograms difficult to understand and reason about has been\naddressed in many ways. At one extreme, 'pure' functional\nprogrammers disallow it entirely. Most people most of the\ntime find that shared mutable data is useful, and much\nresearch has gone into finding ways to control and reason\nabout it. In the words of Noble et al. [30]:\nAny attempt to address the aliasing problem for\npractical objectoriented programming must be\nevaluated as an engineering compromise: how\nmuch safety does it provide, at what cost, and,\nmost importantly, how usable are the mechanisms\nby typical programmers doing general purpose\nprogramming. The crucial question is how nat\nural (or how contrived) a programming style is\nrequired by the proposed aliasing mode checking.\nThe most common approach is to develop a type sys\ntem where the programmer provides annotations that are\nchecked in a modular fashion. These type systems are gener\nally designed around either a notion of encapsulation, often\ncalled 'ownership', or around alias control.\nAll of these static systems emphasize safety and strive for\nusability. By contrast, our dynamic system emphasizes us\nability (we place no extra restrictions on the programmer),\nand stives for safety. Which produces higher quality software\nis an empirical question that has as of yet to be answered.\n6.1\nStatic Specification and Verification\nUniqueness. A unique object may have at most one in\ncoming reference at a time [11, 22, 23]. Uniqueness type\nsystems achieve encapsulation by ensuring that the whole\nis the only object with a reference to its part. Thus, the\nobject that references a unique type fully encalsulates that\ntype. By preventing data sharing, one can reason modu\nlarly about unique types. However, unique types make data\ncommunication much more difficult.\nAlias transfer occurs via destructive reads, in which a ref\nerence becomes null at the same time it is read [22]. In\naddition to requiring modified assignment statements, these\nsystems require a programming style that trades aliases back\nand forth. Invoking a purely functional method on a unique\nvariable requires returning that parameter alias in addition\nto the normal return value of the method. Furthermore, it\nis unclear how programmers should handle unique objects\nthat have fields that sometimes may be null. In this case,\nreducing complexity with a uniqueness invariant increases\nthe complexity of the representation invariant, even though\nuniqueness can identify an object's representation.\nThe problem with most approaches to unique types is that\nthey are too restrictive. More advanced research has tried\nto make these systems less restrictive while still retaining\nsome control.\n[22, 26] weaken the uniqueness invariant to permit short\nterm borrowing, or dynamic aliases (ie, local variables), that\nare never assigned to fields. Dynamic aliases make it more\ndifficult to reason about sharing. Weakening an impractical\ninvariant is less desirable than finding the right invariant.\nInstead of mandatory destructive reads, alias burying per\nmits nondestructive reads in cases where destruction is \"un\nnecessary\" [11]. A conservative use of unnecessary means\nthat a unique reference may be read if the original reference\nis never used after the read. If the behavior is the same\nas doing a destructive read, then there is no need to ac\ntually do one. Alias burying can therefore capture control\ntransfer (factories and listeners), but not borrowing (itera\ntors, containers, intentionally shared data). The necessarily\nconservative type system results rely on an expensive static\nanalysis.\nOne way in which our work differs from much of the work\njust described is that we focus on the objects that are in\ncontrol rather than on the aliasing in the program. We\nare not concerned with who has a reference or how they\ngot it, only with what happens in the program execution\non account of them having it. So, for example, we do not\ndistinguish between aliases attained through local variables\nor through fields - which is an important distinction for\nmuch of the above work.\nOwnership. Ownership type systems explicitly encapsulate\nowned objects. In the most conservative approach, owner\nas dominator, an object may only be referenced through its\nowner [14]. The owner is said to dominate because all paths\nin the heap go through the owner. Only the owned objects\nare encapsulated; the owner may be aliased by any object.\nBecause no object necessarily has one alias, control transfer\nis nonlocal; thus, an object's owner is typically invariant\nacross execution.\nExternal uniqueness relaxes uniqueness such that internal\nobjects may alias the unique type [13], much like how owned\nobjects may alias their parents. To determine the unique\nobject's internal state, external uniqueness uses an owner as\ndominator type system.\nDominator ownership systems prevent useful program id\nioms, some of which use readonly aliasing, eg, iterators, fac\ntories, containers, observers. Modifier as owner type sys\ntems enforce that only mutating paths in the heap must go\nthrough the owner, which permits modular verification and\nmore implementations than owner as dominator. [17, 28, 29]\nhave a static notion of owner as mutator using the Uni\nverses type system and JML. However, modifier as owner as\na program invariant prevents intentional sharing, eg, itera\ntors that remove elements.\nReadonly annotations. Readonly annotations on pointer\nvariables provide some control over where data is mutated\nfrom. This idea exists in some form in C/C++ in the const\nkeyword. Recent proposals have described adding this func\ntionality to Java [9, 33]. The primary difference between this\napproach and approaches based on uniqueness and owner\nship is that reference immutability is not centred on a notion\nof encapsulation. While reference immutability helps the\nprogrammer manage mutable data, it does not help with\nmodular reasoning.\n\n6.2\nInference\nThere has been relatively little work on ownership inference,\ncompared with the work on static specification and verifica\ntion.\nStatic. The most elaborate static inference system is that\nof Heine and Lam [21]. They define owner as allocator/de\nallocator, and consider that the variables that allocate and\ndeallocate an object own it. They do a static analysis that\nattempts to push the ownership bit through all the assign\nment statements between the allocator and the deallocator.\nIf their system can connect the dots, it proves that the object\nis disposed of exactly once. They use their system to detect\npotential memory leaks, and do not communicate the own\nership information to the programmer, nor do they attempt\nto address the matter of encapsulation.\nBoyapati [10] has an intraprocedural inference system to\naccompany his type system.\nAldrich et al. [3] has an inference system that the author's\nwebpage seems to claim is not quite as satisfactory as one\nmay be led to believe from reading the paper.\nDynamic. Wren [34] provides a theoretical foundation for\ninferring Clarke/Noble [14] ownership types by finding in\nvariant dominators in runtime samples and then relaxing\nowners to contruct consistent ownership type annotations.\nThis is a theoretical framework that was never implemented.\nWren [34] is similar to our work in that it describes an anal\nysis that looks for dominators in graphs that are captured\ndynamically. The difference is in the graphs: Wren [34]\nproposes to capture entire heap graphs, wheres we capture\nmutation chains. Mutation chains have the practical ad\nvantage of being much smaller than heap graphs, and we\nargue that object control invariants get more at the core of\nencapsulation than ownership does.\nDiscussion\n7.1\nScalability\nCrocopat. Much to our surprise, in this project Crocopat\nproved to be very slow and memory intensive. This was\nnot our experience in previous work [31]. Crocopat is based\non BDDs so that it can scale [8]. In this work, all of the\nanalyses that we originally implemented in Crocopat and\nthen reimplemented in another (imperative) language were\nmuch faster in the imperative incarnations. We think this\nis because of the cost of existential quantifiers in Crocopat,\nand their necessity for the properties we are computing.\nDominators. We have implemented an iterative graph\ndominator algorithm customized for the kinds of graphs that\nwe expect to be analyzing. These differ significantly from\ncommon control flow graphs, because they have many tree\nleaf nodes (ie, nodes with one predecessor and no succes\nsors), and because a small minority of nodes have a massive\nnumber of successors.\nTrace compression. We will need to store traces more effi\nciently to do real case studies. We are currently generating\ntraces that are hundreds of megabytes for TSafe, which is\nnot that large of a program.\n7.2\nEvaluation\nOur aspiration is that this technique will help programmers\nunderstand and reason about sharing in their programs. Ide\nally this will reduce unnecessary and erroneous sharing, and\nprovide better ways to think about the essential sharing.\n7.3\nFuture Work\nSimple annotations. Our tool generates a large list of\nEclipse bookmarks, even for toy programs. We currently\nprioritize those bookmarks that reveal sharing. Allowing\nthe user to write simple annotations would give us a good\nprioritization mechanism: highlight those bookmarks where\nthe program's behaviour deviated from the programmer's\nintent.\nStatic analysis. We are interested in developing a static\nanalysis to compute this same information, possibly using\nthe new contextsensitive pointsto analysis of Sridharan and\nBodik [32]. We have a hunch that the static and dynamic\nanalyses will compute similar values -- unlike, for exam\nple, in pointsto analysis where the static and dynamic ap\nproaches tend to produce very different results [27].\nMultithreaded programs. In this paper we have only con\nsidered singlethreaded programs.\nHowever, our instru\nmenter collects information about threads, and we believe\nthe instrumenter is threadsafe. So we just need to think\nabout what to do with this data about threading that we\nare collecting.\nProgram evolution. We think that the evolution of the\nownership structure of a program may reveal design vio\nlations introduced during maintenance activities.\nReferences\n[1] Rahul Agarwal and Scott D. Stoller. Type infer\nence for parameterized racefree Java. In Giorgio Levi\nand Bernhard Steffen, editors, Proc. 5thVMCAI,\nvolume 2937 of LNCS, pages 149-160, Venice, Italy,\nJanuary 2004.\n[2] Jonathan Aldrich. Using Types to Enforce Archi\ntectural Structure. PhD thesis, U. Washington, August\n2003.\n[3] Jonathan Aldrich, Valentin Kostadinov, and\nCraig Chambers. Alias annotations for program un\nderstanding.\nIn Satoshi Matsuoka, editor, Proc.\n17thOOPSLA, pages 311-330, Seattle, WA, October\n2002.\n[4] Paulo Sergio Almeida. Balloon types: Controlling\nsharing of state in data types. In Mehmet Aksit and\nSatoshi Matsuoka, editors, Proc. 11thECOOP, vol\naskyl\nume 1241 of LNCS, Jyv\na, Finland, June 1997.\nISBN 3540630899.\n[5] David F. Bacon. Fast and Effective Optimization of\nStatically Typed ObjectOriented Languages. PhD the\nsis, Berkeley, December 1997. UCB/CSD981017.\n\n[6] David F. Bacon and Peter F. Sweeney.\nFast\nstatic analysis of C++ virtual function calls. In James\nCoplien, editor, Proc. 11thOOPSLA, pages 324 - 341,\nSan Jose, CA, October 1996.\n[7] Dirk Beyer and Andreas Noack.\nCrocopat 2.1\nintroduction and reference manual. Technical Report\nUCB/CSD041338, Berkeley, 2004.\n[8] Dirk Beyer, Andreas Noack, and Claus Lewer\nentz. Efficient relational calculation for software anal\nysis. IEEE Transactions on Software Engineering, 31\n(2):137-149, 2005. URL http://dx.doi.org/10.1109/\nTSE.2005.23.\n[9] Adrian Birka and Michael D. Ernst. A practical\ntype system and language for reference immutability.\nIn Doug Schmidt, editor, Proc. 19thOOPSLA, pages\n35-49, Vancouver, British Columbia, Canada, October\n2004.\n[10] Chandrasekhar Boyapati. SafeJava: A Unified Type\nSystem for Safe Programming. PhD thesis, MIT, 2004.\n[11] John Boyland. Alias burying: unique variables with\nout destructive reads. Software--Practice & Experi\nence, 31(6):533-553, May 2001.\n[12] Luca Cardelli, editor. Proc. 17thECOOP, volume\n2743 of LNCS, Darmstadt, Germany, July 2003. ISBN\n3540405313.\n[13] Dave Clarke and Tobias Wrigstad.\nExternal\nuniqueness is unique enough. In Cardelli [12], pages\n176-200. ISBN 3540405313.\n[14] David G. Clarke, John M. Potter, and James\nNoble. Ownership types for flexible alias protection.\nIn Craig Chambers, editor, Proc. 13thOOPSLA, Van\ncouver, British Columbia, Canada, October 1998.\n[15] Keith D. Cooper, Timothy J. Harvey, and Ken\nKennedy.\nA simple, fast dominance algorithm.\nSoftware--Practice & Experience, (4), 2001.\n[16] Greg Dennis. TSAFE: building a trusted computing\nbase for air traffic control software. Master's thesis,\nMIT, January 2003.\n[17] Werner Dietl and Peter M uller.\nUniverses:\nLightweight ownership for jml. Journal of Object Tech\nnology, 6(8):5-32, 2005.\n[18] Michael D. Ernst. Dynamically Discovering Likely\nProgram Invariants.\nPh.D., University of Washing\nton Department of Computer Science and Engineering,\nSeattle, Washington, August 2000.\n[19] Erich Gamma, Richard Helm, Ralph Johnson,\nand John Vlissides. Design Patterns: Elements of\nReusable ObjectOriented Software. AddisonWesley,\n1995.\n[20] L. Georgiadis, R. F. Werneck, R. E. Tarjan,\nS. Triantafyllis, and D. I. August. Finding dom\ninators in practice. In Proceedings of the 12th Annual\nEuropean Symposium on Algorithms (ESA 2004), vol\nume 3221 of LNCS, pages 677-688, 2004.\n[21] David L. Heine and Monica S. Lam. A Practi\ncal FlowSensitive and ContextSensitive C and C++\nMemory Leak Detector. In Rajiv Gupta, editor, Proc.\nPLDI, June 2003.\n[22] John Hogg. Islands: Aliasing protection in object\noriented languages.\nIn Andreas Paepcke, editor,\nProc. 6thOOPSLA, pages 271-285, Phoenix, October\n1991.\n[23] John Hogg, Doug Lea, Alan Wills, Dennis\ndeChampeaux, and Richard Holt. The geneva con\nvention on the treatment of object aliasing. SIGPLAN\nOOPS Messenger, 3(2):11-16, 1992. ISSN 10556400.\n[24] Viktor Kuncak, Patrick Lam, and Martin Ri\nnard.\nRole analysis.\nIn John Mitchell, editor,\n29thPOPL, Portland, Oregon, January 2002.\n[25] Patrick Lam and Martin Rinard. A Type Sys\ntem and Analysis for the Automatic Extraction and\nEnforcement of Design Information. In Cardelli [12],\npages 275-302. ISBN 3540405313.\n[26] Naftaly H. Minsky. Towards aliasfree pointers. In\nPierre Cointe, editor, Proc. 10thECOOP, volume\n1098 of LNCS, pages 189-209, Linz, Austria, July 1996.\nISBN 3540614397.\n[27] Markus Mock, Manuvir Das, Craig Chambers,\nand Susan J. Eggers.\nDynamic pointsto sets:\nA comparison with static analyses and potential ap\nplications in program understanding and optimiza\ntion.\nIn John Field and Gregor Snelting, ed\nitors, Proc. ACM SIGPLANSIGSOFT Workshop on\nProgram Analysis for Software Tools and Engineering\n(PASTE), Snowbird, UT, June 2001.\n[28] Peter M uller. Modular Specification and Verification\nof ObjectOriented Programs. PhD thesis, FernUniver\nsit at Hagen, 2001.\n[29] ------.\nModular Specification and Verification of\nObjectOriented Programs, volume 2262 of LNCS. 2002.\n[30] James Noble, Jan Vitek, and John Potter. Flex\nible alias protection.\nIn Eric Jul, editor, Proc.\n12thECOOP, volume 1445 of LNCS, Brussels, Belgium,\nJuly 1998. ISBN 3540647376.\n[31] Derek Rayside, Lucy Mendel, Robert Seater,\nand Daniel Jackson. An analysis and visuzliation for\nrevealing object sharing. In MargaretAnne Story\nand LiTe Cheng, editors, Eclipse Technology Ex\nchange (ETX), San Diego, CA, October 2005.\n[32] Manu Sridharan and Ratislav Bodik. Refinement\nBased ContextSensitive PointsTo Analysis for Java.\nTechnical\nReport\nUCB/EECS200513,\nBerkeley,\nNovember 2005.\nURL http://www.eecs.berkeley.\nedu/Pubs/TechRpts/2005/EECS200513.html.\n[33] Matthew S. Tschantz and Michael D. Ernst.\nJavari: Adding reference immutability to Java.\nIn\nRichard P. Gabriel, editor, Proc. 20thOOPSLA,\npages 211-230, San Diego, CA, October 2005. ISBN\n1595930310.\n[34] Alisdair Wren. Ownership type inference. Master's\nthesis, Department of Computing, Imperial College,\n2003. URL http://www.cl.cam.ac.uk/users/aw345/\nwritings/."
    },
    {
      "category": "Resource",
      "title": "test_case_anon.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/90bc3e210a22f16ad95cd83864952878_test_case_anon.pdf",
      "content": "Safe Test Case Reduction\nBrad Howes\nAbstract\nSome objectoriented test cases are inefficient: they per\nform computation that is unnecessary for producing the fi\nnal, tested result. This is especially true of automatically\ngenerated test cases. Reducing the size of a test case can\nimprove test runtime and simplify debugging when an error\nis found. Published techniques for detecting inefficient or\nredundant test cases are unsafe: they rely on assumptions\nabout ways the tested code will not change.\nHowever, developers do intentionally or unintentionally\nbreak these assumptions: they introduce additional data\ndependencies, or make pure methods impure. We present\na safe test case reduction technique that produces statically\nverifiable guards which encode the assumptions introduced\nduring reduction. If these guards are violated, the original\ntest case can be run for a safe result. The guarded tests\nshould combine complete soundness with a faster expected\nruntime and reduced debugging effort.\n1. Introduction\nFor objectoriented systems, the input for a test is a se\nries of method calls, including arguments, against a set of\nobjects, and the oracle is a set of assertions about the ob\njects' behavior in response. For a given implementation of\na system, such a test may be inefficient - it may call meth\nods that have no effect on the objects' tested behavior. The\nsame behavior could be tested by a reduced test, which is\nsues only a subset of the method calls of the inefficient test.\nFor example, one simple reduction technique would be to\nremove all calls to pure methods from a test; by definition,\nthey have no effect on the tested outcome.\nThere are many benefits to running the reduced test in\nstead of the inefficient test. The reduced test may be signifi\ncantly faster. It may be easier to determine that the reduced\ntest is redundant with a test already in the test suite, ob\nviating the need to run the reduced test at all [12]. If the\nreduced test fails, it may be easier for a developer to un\nderstand the failure if unimportant method calls have been\nremoved.\nHowever, given any test and a reduction of that test, it is\nalways possible to find an implementation of the tested sys\ntem on which the two tests produce different results. Thus,\nany reduction of a test implicitly assumes certain ways in\nwhich the tested system will not change. If all pure method\ncalls are removed, this assumes that no pure method will\never be changed, intentionally or not, to be impure. Thus,\nthe reduced test is unsafe; certain faults that would have\nbeen caught by the inefficient test will not be caught by the\nreduced test.\nWe wish, then, to minimize the time required to safely\nguarantee that a new version of a program still passes a\ngiven test. We propose to do so by producing both a reduced\nversion of the original test, and a set of static guards that\nare sufficient to guarantee that the reduction is safe. We call\nthe combination of dynamic test and static guards a guarded\ntest. A traditional test case can be seen as a guarded test\nbut with no guards. Using guarded reduced tests should\nsignificantly reduce test case complexity and test running\ntime and improve developer understanding with no loss in\nfaultfinding capability.\nThe paper continues with a motivating example in the fol\nlowing section, followed by a discussion of where inefficient\ntests come from (Section 3). We then detail our technique\n(Section 4), and give preliminary results (Section 5) and an\nevaluation plan (Section 6). Finally, we present future work\n(Section 7) and conclude (Section 8).\n2. Example\nFor our initial study, we make several simplifying assump\ntions about tests throughout:\n1. Tests can be represented as a singlemethod straight\nline sequence of statements, without loops, branches,\nor meaningful exceptional control flow (except assert\ning that an exception was thrown or not thrown). This\nis true of all automated test generation techniques we\nknow of, and is often true of humangenerated tests\n(eg. JUnit tests [3])\n2. There is no aliasing between variables within the test.\n3. There is only one method call per statement in the\ngenerated test scenario. This can be achieved by intro\nducing fresh temporary variables for any intermediate\nresults.\n4. The last operation in each test is the assertion of a\nsingle equality comparison between an expected value\nsupplied by the test, and a value provided by the ob\nject under test. This is not always the case, but our\ntechniques are easily generalized to multiple or more\ncomplex assertions.\nAs an example of guarded test minimization, consider a\nsimple point class:\nclass Point {\nprivate int _x, _y;\nAnonymous\n\nPoint(int x, int y) {_x = x; _y = y;}\nint getX() { return _x; }\nint getY() { return _y; }\nvoid setX(int x) { _x = x; }\nvoid setY(int y) { _y = y; }\nvoid translate(int x, int y){\n_x += x; _y += y;\n}\npublic String toString() {\nreturn _x + \",\" + _y;\n}\n}\nNext we have a test case which exercises this class.\nPoint p = new Point(3, 5);\np.getX();\np.getY();\np.setX(4);\np.setY(6);\np.setX(0);\np.getY();\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nIf a static analysis uncovers the fact that Point.getX() and\nPoint.getY() are pure, then the test case could reduce to the\nfollowing shortened test code plus two guard checks:\nGuard: Point.getX writes nothing.\nGuard: Point.getY writes nothing.\nPoint p = new Point(3, 5);\np.setX(4);\np.setY(6);\np.setX(0);\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nNow, when the guarded test is run, the first step is to\nevaluate if the guards still hold. If so, this reduced test is\nguaranteed to catch any error caught by the original test. If\nnot, the original test must be run to maintain safety. See\nFigure 1.\nAnalysis of the behavior of the method Point.setX() re\nveals that this method affects (mutates) the state of the\nPoint. x attribute. Since the terminal statement of the test\ncase relies on the value of this attribute, one might assume\nthat all places where this method is invoked must remain.\nHowever, note that in the reduced form above there are two\nlocations where Point.setX() is called, but with no interven\ning assert check. Thus, the first invocation is superfluous,\nand may be removed:\nGuard: getX writes nothing.\nGuard: getY writes nothing.\nGuard: setX reads nothing.\nGuard: setX writes at least this._x.\nGuard: setY reads nothing.\nPoint p = new Point(3, 5);\np.setY(6);\nOriginal test\nGuarded\nreduction\nguards\nhold?\nrun original test\nrun reduced test\nyes\nno\nFigure 1: A schematic of the guarded test reduction proce\ndure. The original test is augmented with a faster reduced\ntest, and a set of guards that determine if the reduced test\nis safe.\np.setX(0);\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nThe final result is a test case that is smaller and therefore\nlikely to take less time to execute. Furthermore, we have\nacquired some facts about the underlying behavior of the\ncode under test, which will be used by the testing framework\nto help determine if and when future code changes invalidate\nthe test case reduction that took place.\n3. Sources of Inefficient Test Cases\nDevelopers may manually create tests that appear ineffi\ncient. These may be tests of the purity of some method, or\nthey may be actual mistakes. However, it is rare for manu\nally created test to have a significant degree of inefficiency.\nAutomaticallygenerated tests are another matter.\nWhen automatically generating objectoriented regression\ntests, the oracle is formed by recording some subset of the\nbehavior of the working system when a method sequence\nis applied. Regressions are found by comparing the behav\nior of a new version of the system against that previously\nrecorded. The behavior of a tested object can include both\nreturn values from method calls and method calls made to\nenvironmental objects; for simplicity, we focus here on re\nturn values. Generation techniques fall into two broad cat\negories, which differ in how input sequences of method calls\nare generated.\nVarious test construction techniques, such as Eclat [8],\nJCrasher [5], or the commercial tool Agitator [2], gener\nate method sequences one call at a time, using a search\nalgorithm that attempts to produce a suite that maximizes\ncoverage of code, or corner cases, or bugs found. In most\ncases, the search algorithm is guided by static features of\nthe classes being tested; for example, tests that inefficiently\ncall a currentlypure method will simply not be generated.\nAn implicit assumption of this guided search is that these\nfeatures are stable.\nA capture/replay technique, such as test factoring [9],\nSCARPE [7], or DejaVu [4], captures each input method\n\nsequence from an execution of the system, whether manu\nally or automatically driven. Existing capture/replay tech\nniques capture every call made to the tested objects during\nan execution, and replay them all. The advantage of this\ncompleteness is that generated tests are guaranteed to be\nsafe - any breaking change in the implementation of any\nmethod in the tested classes will be detected by these tests.\nHowever, these tests can be inefficient, consisting of millions\nof method calls, and failures can be difficult to debug.\nIt is possible to trade safety for efficiency in capture/replay\ntests by eliminating method calls that can be proven to have\nno effect on the final, tested behavior of the tested compo\nnent: statically determined pure methods, or method calls\nthat can be dynamically shown not to change the tested\nstate. This is the approach taken by Xie, Marinov, and\nNotkin in Rostra [12]. However, the safety tradeoff is un\nnecessary - it is possible to have both efficiency and safety,\nusing the technique presented here, which remembers the\nstatic guards required for safe reduction, and verifies them\nincrementally and quickly in new versions.\nOur technique focuses on safe reduction of tests generated\nthrough capture/replay techniques. However, it may be pos\nsible to apply similar techniques to tests generated through\ntest construction, by remembering the static guards that\nguided the initial test search, and recommending a repeat\nsearch if these are violated.\n4. Approach\nIn order to safely reduce a test case, we need to know\nwhat method calls are extraneous to the tested result, and\nwe must be able to recognize when a reduction no longer ap\nplies due to code changes. To satisfy these needs, we propose\na technique that augments a test case with guards, which are\nstatically evaluated prior to running the reduced test case to\nverify that the conditions under which the reduction origi\nnally took place still hold. To produce the reduced test and\nthe guards, we use a simplified slicing algorithm that makes\nexplicit which properties of which methods are necessary to\njustify each statement that is removed.\nWe perform a static analysis of the code under test in\norder to obtain the set of data elements (class fields and\nmethod arguments) that a method reads from and writes to\nwhen it executes. Collectively, these data sets are known\nas method summaries. The elements of the summary sets\nuniquely name the field or argument used or manipulated\nwithin the method or within any call chain started from the\nmethod. Section 4.1 discusses in detail how we generate\nmethod summaries and manage name conflicts.\nOnce we have the method summaries for the code under\ntest, we can then proceed to reduce the test case. Our ap\nproach is to use a backwards static slicing of the test case\nin order to determine whether a method has the potential\nto influence the outcome of the test case. If the method\nsummaries of a method call indicate that it cannot affect\nany of the data elements used by subsequent method calls,\nincluding the terminating test assertion, then the method is\nremoved from the test case. Section 4.2 describes the slicing\nand reduction steps in detail.\nRegardless whether the method call is kept or not, we\nannotate the test case with a set of static guards gener\nated from the call's method summary. For removed method\ncalls, the guards determine when the removal is potentially\nno longer safe, while guards for kept calls identify potential\nadded dependencies on earlier statements. Details of guard\ngeneration are covered in Section 4.3.\n4.1 Generating Method Summaries\nMethod summaries are the result of a static analysis per\nformed on the code under test. The analysis determines\nwhich fields and method arguments the method reads from\nor writes to. This information will be used during the slic\ning of the test case (Section 4.2) and during static guard\ngeneration (Section 4.3).\nOur approach for generating the summaries is to execute\na pointsto analysis over the code under test to precisely\nidentify the set of objects a method method may encounter\nduring its execution. We next execute a intraprocedural\nsideeffect analysis of the method using the pointsto graph,\nand statically evaluate each statement in the method to see\nif it reads from or writes to an object field or method argu\nment. If the statement is a method invocation (callsite), the\nanalysis will proceed to analyze the called method, with any\nattributes found within the callchain for the callsite propa\ngated back up to the calling method. Finally, the individual\nstatement read/write attributes are collected together to be\ncome the method summary.\nFor example, given the Point class show in Section 2,\nthe extracted method summaries for the class methods are\nshown in Table 1.\nTable 1: Point Method Summaries\nKey\nReads\nWrites\nPoint\nx, y\nsetX\nx\ngetX\nx\nsetY\ny\ngetY\ny\nmove\nx, y\nxlate\nx, y\nx, y\ntoString\nx, y\nThe propagation of read/write attributes presents a nam\ning issue. Consider the following class:\nclass PointPair {\nprivate Point _a;\nprivate Point _b;\nPointPair( Point a, Point b ) {\n_a = a, _b = b;\n}\nvoid setX( int x ) {\n_a.setX( x );\n_b.setX( x );\n}\n}\nWithin the method PointPair.setX, there are two invoca\ntions of Point.setX applied to two different objects. The\nanalysis of Point.setX reveals that it writes to Point. x,\nbut when this fact is propagated up to the callsites in\nPointPair.setX, they appear the same. Our solution is to\ntransform, if possible, the names of propagated attributes\nwith the class and field the attribute actually refers to. For\nthe example above, the write attribute Point. x becomes\n\nFigure 2: Datadependency graph for test case from Section\n2. The root of the graph is toStriing().\nPointPair. a. x and PointPair. b. x for the respective call\nsites, resulting in two distinct write attributes in the method\nsummary for PointPair.setX.\n4.2 Test Case Slicing and Reduction\nTo identify which statements may be removed from a test\ncase, we need to understand which statements are required\nto satisfy the data needs of the test's final assertion. Intu\nitively, one needs to build a datadependency graph starting\nwith the method call in the assertion statement of the test\ncase, where the graph nodes are the method calls in the test\ncase, and the presence of a directed edge between two nodes\nindicates that a child writes to one or more fields used by\nthe parent it is connected to. Once the graph is complete,\nany nodes that do not have a path from the root (the asser\ntion statement) by definition have no effect on the behavior\nof the method in the assertion statement, and thus no affect\non the assertion itself.\nFigure 2 shows a datadependency graph for the exam\nple test case in Section 2. Note that the topdown graph\nflow is opposite of the topdown linear flow of the test case.\nThere is no directed path from the root of the graph to the\nroutines Point.getX() and Point.getY(), indicating no data\ndependencies between them.\nBecause we have restricted our approach to linear test\ncases, we can do the datadependency analysis and method\nculling in one iteration over the test case, starting at the\nfinal assertion and working backwards. We manage a set of\nactive reads (A) that reflect the unconnected or unsatisfied\ndata elements forward of the current analysis point. If the\nmethod summary of the method call at the current analysis\npoint does not match any of the elements in A, then the\nmethod will not have any affect on the behavior of succeed\ning methods, and it may be removed from the test case. In\nshort, we perform a backwards static slice of the test case,\nand our abstract state is the contents of A.\nA brief outline of the steps involved in our test case slicing\nand reduction is as follows:\n1. Obtain the method summary of the current method.\n2. Determine whether the method satisfies any data de\npendencies\n3. Update propagated data dependency set A\n4. Install any guards for the method\n5. Move to previous method call, or stop when done\nAgain, the method summaries tell us whether a method\nmay read (R), may write (M), or must write (W) zero or\nmore fields. For the slicing algorithm, we are interested in\nthe contents of R and W, and whether the set M is empty.\nWe start the slicing with the A set containing the may\nread R values from the method call in the assertion state\nment:\nA0 := Rn\n(1)\nwhere n is the number of method calls executed within\nthe test case. For each preceding method call Cn-i | i ∈\n{1,...,n}, we calculate the intersection of the method's W\nattribute set and the A set:\nAi-1 ∩ (Wn-i ∪ Mn-i)\n(2)\nTo determine whether a W attriibute satisfies an entry\nin A, we iterate over the values of A, looking for an exact\nmatch in W, or an element in W that is an effective match:\nthe effect of the write is the same as if the attribute had\ncompletely matched the read attribute in A. For instance,\nin the PointPair example of Section 4.1, a W attribute of\nPointPair. a would satisfy the attribute PointPair. a. x.\nIf the resulting intersection set is empty and the method\nhas an empty M set, then we may safely remove the method\ncall from the test case. Otherwise, we keep the method call,\nand update the A set by first removing from it the result of\nthe intersection calculated above, followed by a union of A\nwith the method's R set:\nAi := Rn-i ∪ (Ai-1 - (Ai-1 ∩ Wn-i))\n(3)\nIn other words, we remove all fields guaranteed to be sat\nisfied by the method call and add in any fields the method\nitself may be dependent on. Note that the contents of M\nhas no affect on A; it only inhibits a method from being\nremoved when it is not empty.\nIf at any time the A set becomes empty, we stop the slic\ning, since there is no possibility that any preceding method\ncalls would affect the test case assertion. This constraint\nshould be satisfied even if we work through the entire test\ncase (i = n), since the test case begins by creating the ob\nject under test, and the constructor for the test object would\nhave initialized the field, even if only to a default value.\n4.3 Guard Generation\nAs our algorithm visits each method call in the test case,\nwe annotate the call with one or more guards. The set of\nguards for the test case is the union of the guards for each\nof the method calls in the unreduced test case. Prior to\nexecuting the test case, the test case's guards are evaluated\nto see if the property they represent still holds. If all guards\npass, then the reduced test case is executed; otherwise, the\noriginal, unreduced, test case runs. In short, guards protect\nthe test case from future code changes, signaling the fact\nthat the conditions under which a test case was previously\nreduced no longer apply.\n\nWe have identified the following set of guards in our ap\nproach:\n1. reads at most the contents of the R set from the\nmethod summary\n2. writes at least the contents of the W set from the\nmethod summary\n3. writes at most the contents of the M + W sets\nThe first guard states that the method does not rely on\nany more data than found in the R set. If a code change\nwere to result in an R0 ⊂ R, then the reduced test case is still\nsafe and should be run. The next time the test is reduced,\nit may now prove possible to reduce it even further.\nThe second and third guards state the minimum and max\nimum set of fields that the method may change. The mini\nmum set contains the fields that the method always writes\nto when it is invoked, whereas the latter is the set of fields\nthat the method may write to. If either or both of these\nbounds change, then the dependency graph generated for\ntest case is no longer valid and the original test case is run.\nIntuitively, the relative sizes of the Mi and Wi sets for\nmethod call Ci may provide a measure of how likely the\ncall would be removed. The more elements there are in Mi ,\nthe greater the likelihood that one of the elements will be\na member of the propagated A set, in which case the call\nmust remain in the test case since the M set only indicates\npotential writes, and not the guaranteed writes of W. Fur\nthermore, for each method Ci kept in a test case, one could\nreasonably expect A to grow as it is joined with the call's\nRi set, thus increasing the potential to keep methods Cj |\nj ∈ {0,...,i}.\n5. Preliminary Results\nWe have created a simple test bed to exercise the indi\nvidual components of our approach. The test scenario uses\nclass Point:\npublic class Point {\nprivate int _x;\nprivate int _y;\npublic Point( int x, int y ) {\n_x = x; _y = y;\n}\npublic int getX() {\nreturn _x;\n}\npublic int getY() {\nreturn _y;\n}\npublic void moveBy( int x, int y ) {\n_x += x;\n_y += y;\n}\npublic void moveHorizontally( int x ) {\n_x += x;\n}\npublic void moveVertically( int y ) {\n_y += y;\n}\npublic String toString() {\nreturn \"(\" + _x + \",\" + _y + \")\";\n}\n}\nThere exist two handcrafted driver programs, one for\neach of the above classes. These driver programs exist to\nsimulate the running of a test scenario, but outside of a\ntesting infrastructure. First, the driver for testing the Point\nclass:\npublic class TestPoint {\nstatic public void\nassertEqual( boolean condition ) {\nSystem.out.println(\n\"test case returned \" + condition );\n}\npublic static void main(String[] args) {\nPoint p =\nnew Point( 10, 20 );\np.moveBy( 4, 4 );\np.getX();\np.getY();\np.moveBy( 3, 8 );\np.getX();\np.getY();\np.setX( 5 );\np.getY();\np.getX();\np.setY( 10 );\np.moveHorizontally( 1 );\np.moveVertically( 2 );\np.getX();\np.getY();\nSystem.out.println( p.toString() );\nassertEqual(\np.toString().equals( \"(6,12)\" ) );\n}\n}\nThe test case simply exercises the Point API, with a final\nassertEqual to check that the previous calls produced the\nexpected final state of the Point object.\nWhen processing is done, the following attributes are re\nvealed:\nPoint: int getY()\n<Read Point:_y>\nPoint: void setX(int)\n<Write Point:_x>\nPoint: int getX()\n<Read Point:_x>\njava.lang.String: boolean equals(java.lang.Object)\n<Read java.lang.String:offset>\n<Read java.lang.String:value>\n<Read java.lang.String:count>\nPoint: void setY(int)\n<Write Point:_y>\nPoint: void moveHorizontally(int)\n<Read Point:_x>\n<Write Point:_x>\nPoint: void moveBy(int,int)\n<Write Point:_y>\n<Read Point:_y>\n<Read Point:_x>\n\n<Write Point:_x>\nPoint: void moveVertically(int)\n<Write Point:_y>\n<Read Point:_y>\n5.1 Performance\nRather discouragingly, our implementation of summariza\ntion takes 2.25 minutes to run on an Apple PowerBook\n1.5GHz with 1GB of RAM (the Java VM is limited to a\nmaximum 400MB heap with the Xmx400m option). En\nabling verbose logging reveals that a majority of the time\nis spent generating Soot Jimple representations for the Java\nruntime and support classes. Using a combination of Soot\noptions, we were able to obtain a set of Jimple files for all\nof the runtime classes reached by the call graph rooted by\nour TestPoint class. We then tried to have Soot use these\nfiles instead of dynamic Jimple generation, we encountered\nerrors in the Jimple processing. Apparently, there are dis\ncrepancies (bugs) between what Soot writes out in its Jimple\nrepresentation and what it reads in.\nWe also submitted the TestPoint class to a Purity Analysis\nKit [10]. Interestingly, we obtained similar timing results.\nWe have briefly investigated generating the call graph it\neratively. Previous work by Souter, Pollock [11] showed\npromising results using enhancements to the Flex Compiler\nInfrastructure [6]. Their call graph generation algorithm is\nbased on the Cartisian Product Algorithm of Agesen [1],\nwith modifications to support reanalysis of only the code\nthat changed.\n6. Evaluation\nOur evaluation is based on monitoring actual developer\nactivity without the benefit of automatically generated and\nreduced tests, and then simulating the impact of introducing\ntest reduction into the development process. This allows us\nto get an idea of the benefits of our approach before a full\nfledged tool has been built. We can also simulate scenarios\nthat real developers would not stand for, such as using an\ninefficient test generation technique without reduction.\nWe have monitored a single developer performing devel\nopment and maintenance on fdanalysis, a Java program of\nabout 9000 lines of code. fdanalysis is a package for an\nalyzing data collected during development and debugging\nsessions. It performs mainly text processing and time calcu\nlation. We have captured 1600 snapshots of the state of the\nprogram during development, which include the introduc\ntion and fixing of 12 regression errors. Most of the running\ntime of the existing test suite operates on the highlevel API\nof the package, sending in text files and making assertions\non text output.\nIn our simulated scenario, the developer would like to gen\nerate unit tests for the edited component of the program by\nusing a capture/replay test generation technique, test fac\ntoring [9], on the current suite of system tests, most of which\noperate on the highlevel API of the package, sending in text\nfiles and making assertions on text output.\nHowever, the\nunit tests generated by test factoring are inefficient, and not\na significant improvement on the current tests. We would\nlike to evaluate whether our safe test reduction technique,\nin combination with test factoring, would produce a safe,\nefficient unit regression test suite.\nIn our simulated scenario, test factoring is run each night\novernight to produce inefficient unit tests, which are then\nreduced by safe test reduction into guarded reduced tests.\nFor each snapshot that we have of development during the\nnext day, we can compare running the guarded reduced tests\nwith the unreduced generated tests, and the original system\ntests. Our hypotheses are:\n- The guarded reduced tests are an order of magnitude\nsmaller (in number of method calls against the tested\ncomponent) and faster than either the unreduced gen\nerated tests.\n- In about 1% of the captured snapshots, a static guard\nis violated. When a static guard is violated, the re\nduced test is useless, and an unreduced test must be\nrun to guarantee the system is correct. If this happens\nvery often, the overhead of evaluating the guards will\neliminate any gains from test reduction. However, if\nit happens very seldom, we may have to conclude that\nthe danger of using unguarded reduced tests is not\nvery great, which reduces the value of the overhead of\ngenerating and evaluating guards.\nUnfortunately, in the time allotted for this course, we have\nnot been able to produce an implementation of our approach\nthat correctly handles all of the generated unit tests, so this\nevaluation remains as future work.\n7. Future Work\nThere are several ways that this work can be expanded\nupon.\n1. Our implementation must be made robust enough to\nbe evaluated against realworld test cases.\n2. The basic slicing algorithm should be enhanced to work\ncorrectly in the presence of aliasing.\n3. A usable implementation of our technique requires an\nincremental mechanism for evaluating guards. We be\nlieve that this is easily achieved, by caching in memory\nor on disk the call graph required for guard genera\ntion, and propogating changes in place. However, our\ncurrent implementation must rerun the entire sum\nmarization, a time requirement that overwhelms the\nruntime benefits of reduction.\n8. Conclusion\nAutomated test generation appears poised to become a\nmuch more common development tool. It can provide a\n\"second opinion\" on code, considering cases that the devel\nopers' own assumptions and biases may cause them to over\nlook. It can also dramatically reduce the effort and improve\nthe effectiveness of testing code.\nWe have introduced a technique for reducing the aver\nage runtime of inefficient test cases, whether automatically\nor manually generated, without making any unsafe assump\ntions about the ways in which the tested code may change.\nThese properties should be an essential guarantee of future\ntest generation techniques.\nReferences\n[1] O. Agesen. The Cartesian Product Algorithm. In\nECOOP'95 Conference Proceedings, 1995.\n\n[2] Agitar software. http://www.agitar.com, 2005.\n[3] M. Albrecht. Testing Java with JUnit.\nhttp://www.ddj.com/documents/s=1679/ddj0302b/, 2003.\n[4] J.D. Choi and H. Srinivasan. Deterministic replay of java\nmultithreaded applications. In SPDT '98: Proceedings of\nthe SIGMETRICS symposium on Parallel and distributed\ntools, pages 48-59, New York, NY, USA, 1998. ACM Press.\n[5] C. Csallner and Y. Smaragdakis. JCrasher: an automatic\nrobustness tester for Java. Softw. Pract. Exper.,\n34(11):1025-1050, 2004.\n[6] Flex compiler infrastructure.\nhttp://www.flexcompiler.lcs.mit.edu/.\n[7] A. Orso and B. Kennedy. Selective Capture and Replay of\nProgram Executions. In Proceedings of the Third\nInternational ICSE Workshop on Dynamic Analysis\n(WODA 2005), pages 29-35, St. Louis, MO, USA, may\n2005.\n[8] C. Pacheco and M. D. Ernst. Eclat: Automatic generation\nand classification of test inputs. In ECOOP 2005 --\nObjectOriented Programming, 19th European Conference,\npages 504-527, Glasgow, Scotland, July 25-29, 2005.\n[9] D. Saff, S. Artzi, J. H. Perkins, and M. D. Ernst.\nAutomatic test factoring for Java. In ASE 2005:\nProceedings of the 21st Annual International Conference\non Automated Software Engineering, Long Beach, CA,\nUSA, November 9-11, 2005.\n[10] A. Salcianu and M. Rinard. Purity and side effect analysis\nfor Java programs. In Proceedings of the 6th International\nConference on Verification, Model Checking and Abstract\nInterpretation (VMCAI'05), 2005.\n[11] A. Souter and L. Pollock. Incremental call graph reanalysis\nfor objectoriented software maintenance. In Proceedings\nInternational Conference on Software Maintenance, pages\n682-691, 2001.\n[12] T. Xie, D. Marinov, and D. Notkin. Rostra: A framework\nfor detecting redundant objectoriented unit tests. In\nProceedings of the 19th IEEE International Conference on\nAutomated Software Engineering (ASE 04), pages 196-205,\nSeptember 2004."
    },
    {
      "category": "Resource",
      "title": "test_case_reduct.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/aa15a1e9bd2385c48931e40c5b0cabb0_test_case_reduct.pdf",
      "content": "Safe Test Case Reduction\nBrad Howes\nDavid Saff\nAbstract\nSome objectoriented test cases are inefficient: they per\nform computation that is unnecessary for producing the fi\nnal, tested result. This is especially true of automatically\ngenerated test cases. Reducing the size of a test case can\nimprove test runtime and simplify debugging when an error\nis found. Published techniques for detecting inefficient or\nredundant test cases are unsafe: they rely on assumptions\nabout ways the tested code will not change.\nHowever, developers do intentionally or unintentionally\nbreak these assumptions: they introduce additional data\ndependencies, or make pure methods impure. We present\na safe test case reduction technique that produces statically\nverifiable guards which encode the assumptions introduced\nduring reduction. If these guards are violated, the original\ntest case can be run for a safe result. The guarded tests\nshould combine complete soundness with a faster expected\nruntime and reduced debugging effort.\n1. Introduction\nFor objectoriented systems, the input for a test is a se\nries of method calls, including arguments, against a set of\nobjects, and the oracle is a set of assertions about the ob\njects' behavior in response. For a given implementation of\na system, such a test may be inefficient - it may call meth\nods that have no effect on the objects' tested behavior. The\nsame behavior could be tested by a reduced test, which is\nsues only a subset of the method calls of the inefficient test.\nFor example, one simple reduction technique would be to\nremove all calls to pure methods from a test; by definition,\nthey have no effect on the tested outcome.\nThere are many benefits to running the reduced test in\nstead of the inefficient test. The reduced test may be signifi\ncantly faster. It may be easier to determine that the reduced\ntest is redundant with a test already in the test suite, ob\nviating the need to run the reduced test at all [12]. If the\nreduced test fails, it may be easier for a developer to un\nderstand the failure if unimportant method calls have been\nremoved.\nHowever, given any test and a reduction of that test, it is\nalways possible to find an implementation of the tested sys\ntem on which the two tests produce different results. Thus,\nany reduction of a test implicitly assumes certain ways in\nwhich the tested system will not change. If all pure method\ncalls are removed, this assumes that no pure method will\never be changed, intentionally or not, to be impure. Thus,\nthe reduced test is unsafe; certain faults that would have\nbeen caught by the inefficient test will not be caught by the\nreduced test.\nWe wish, then, to minimize the time required to safely\nguarantee that a new version of a program still passes a\ngiven test. We propose to do so by producing both a reduced\nversion of the original test, and a set of static guards that\nare sufficient to guarantee that the reduction is safe. We call\nthe combination of dynamic test and static guards a guarded\ntest. A traditional test case can be seen as a guarded test\nbut with no guards. Using guarded reduced tests should\nsignificantly reduce test case complexity and test running\ntime and improve developer understanding with no loss in\nfaultfinding capability.\nThe paper continues with a motivating example in the fol\nlowing section, followed by a discussion of where inefficient\ntests come from (Section 3). We then detail our technique\n(Section 4), and give preliminary results (Section 5) and an\nevaluation plan (Section 6). Finally, we present future work\n(Section 7) and conclude (Section 8).\n2. Example\nFor our initial study, we make several simplifying assump\ntions about tests throughout:\n1. Tests can be represented as a singlemethod straight\nline sequence of statements, without loops, branches,\nor meaningful exceptional control flow (except assert\ning that an exception was thrown or not thrown). This\nis true of all automated test generation techniques we\nknow of, and is often true of humangenerated tests\n(eg. JUnit tests [3])\n2. There is no aliasing between variables within the test.\n3. There is only one method call per statement in the\ngenerated test scenario. This can be achieved by intro\nducing fresh temporary variables for any intermediate\nresults.\n4. The last operation in each test is the assertion of a\nsingle equality comparison between an expected value\nsupplied by the test, and a value provided by the ob\nject under test. This is not always the case, but our\ntechniques are easily generalized to multiple or more\ncomplex assertions.\nAs an example of guarded test minimization, consider a\nsimple point class:\nclass Point {\nprivate int _x, _y;\n\nPoint(int x, int y) {_x = x; _y = y;}\nint getX() { return _x; }\nint getY() { return _y; }\nvoid setX(int x) { _x = x; }\nvoid setY(int y) { _y = y; }\nvoid translate(int x, int y){\n_x += x; _y += y;\n}\npublic String toString() {\nreturn _x + \",\" + _y;\n}\n}\nNext we have a test case which exercises this class.\nPoint p = new Point(3, 5);\np.getX();\np.getY();\np.setX(4);\np.setY(6);\np.setX(0);\np.getY();\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nIf a static analysis uncovers the fact that Point.getX() and\nPoint.getY() are pure, then the test case could reduce to the\nfollowing shortened test code plus two guard checks:\nGuard: Point.getX writes nothing.\nGuard: Point.getY writes nothing.\nPoint p = new Point(3, 5);\np.setX(4);\np.setY(6);\np.setX(0);\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nNow, when the guarded test is run, the first step is to\nevaluate if the guards still hold. If so, this reduced test is\nguaranteed to catch any error caught by the original test. If\nnot, the original test must be run to maintain safety. See\nFigure 1.\nAnalysis of the behavior of the method Point.setX() re\nveals that this method affects (mutates) the state of the\nPoint. x attribute. Since the terminal statement of the test\ncase relies on the value of this attribute, one might assume\nthat all places where this method is invoked must remain.\nHowever, note that in the reduced form above there are two\nlocations where Point.setX() is called, but with no interven\ning assert check. Thus, the first invocation is superfluous,\nand may be removed:\nGuard: getX writes nothing.\nGuard: getY writes nothing.\nGuard: setX reads nothing.\nGuard: setX writes at least this._x.\nGuard: setY reads nothing.\nPoint p = new Point(3, 5);\np.setY(6);\nOriginal test\nGuarded\nreduction\nguards\nhold?\nrun original test\nrun reduced test\nyes\nno\nFigure 1: A schematic of the guarded test reduction proce\ndure. The original test is augmented with a faster reduced\ntest, and a set of guards that determine if the reduced test\nis safe.\np.setX(0);\np.translate(1, 1);\nassertEquals(\"1,7\", p.toString());\nThe final result is a test case that is smaller and therefore\nlikely to take less time to execute. Furthermore, we have\nacquired some facts about the underlying behavior of the\ncode under test, which will be used by the testing framework\nto help determine if and when future code changes invalidate\nthe test case reduction that took place.\n3. Sources of Inefficient Test Cases\nDevelopers may manually create tests that appear ineffi\ncient. These may be tests of the purity of some method, or\nthey may be actual mistakes. However, it is rare for manu\nally created test to have a significant degree of inefficiency.\nAutomaticallygenerated tests are another matter.\nWhen automatically generating objectoriented regression\ntests, the oracle is formed by recording some subset of the\nbehavior of the working system when a method sequence\nis applied. Regressions are found by comparing the behav\nior of a new version of the system against that previously\nrecorded. The behavior of a tested object can include both\nreturn values from method calls and method calls made to\nenvironmental objects; for simplicity, we focus here on re\nturn values. Generation techniques fall into two broad cat\negories, which differ in how input sequences of method calls\nare generated.\nVarious test construction techniques, such as Eclat [8],\nJCrasher [5], or the commercial tool Agitator [2], gener\nate method sequences one call at a time, using a search\nalgorithm that attempts to produce a suite that maximizes\ncoverage of code, or corner cases, or bugs found. In most\ncases, the search algorithm is guided by static features of\nthe classes being tested; for example, tests that inefficiently\ncall a currentlypure method will simply not be generated.\nAn implicit assumption of this guided search is that these\nfeatures are stable.\nA capture/replay technique, such as test factoring [9],\nSCARPE [7], or DejaVu [4], captures each input method\n\nsequence from an execution of the system, whether manu\nally or automatically driven. Existing capture/replay tech\nniques capture every call made to the tested objects during\nan execution, and replay them all. The advantage of this\ncompleteness is that generated tests are guaranteed to be\nsafe - any breaking change in the implementation of any\nmethod in the tested classes will be detected by these tests.\nHowever, these tests can be inefficient, consisting of millions\nof method calls, and failures can be difficult to debug.\nIt is possible to trade safety for efficiency in capture/replay\ntests by eliminating method calls that can be proven to have\nno effect on the final, tested behavior of the tested compo\nnent: statically determined pure methods, or method calls\nthat can be dynamically shown not to change the tested\nstate. This is the approach taken by Xie, Marinov, and\nNotkin in Rostra [12]. However, the safety tradeoff is un\nnecessary - it is possible to have both efficiency and safety,\nusing the technique presented here, which remembers the\nstatic guards required for safe reduction, and verifies them\nincrementally and quickly in new versions.\nOur technique focuses on safe reduction of tests generated\nthrough capture/replay techniques. However, it may be pos\nsible to apply similar techniques to tests generated through\ntest construction, by remembering the static guards that\nguided the initial test search, and recommending a repeat\nsearch if these are violated.\n4. Approach\nIn order to safely reduce a test case, we need to know\nwhat method calls are extraneous to the tested result, and\nwe must be able to recognize when a reduction no longer ap\nplies due to code changes. To satisfy these needs, we propose\na technique that augments a test case with guards, which are\nstatically evaluated prior to running the reduced test case to\nverify that the conditions under which the reduction origi\nnally took place still hold. To produce the reduced test and\nthe guards, we use a simplified slicing algorithm that makes\nexplicit which properties of which methods are necessary to\njustify each statement that is removed.\nWe perform a static analysis of the code under test in\norder to obtain the set of data elements (class fields and\nmethod arguments) that a method reads from and writes to\nwhen it executes. Collectively, these data sets are known\nas method summaries. The elements of the summary sets\nuniquely name the field or argument used or manipulated\nwithin the method or within any call chain started from the\nmethod. Section 4.1 discusses in detail how we generate\nmethod summaries and manage name conflicts.\nOnce we have the method summaries for the code under\ntest, we can then proceed to reduce the test case. Our ap\nproach is to use a backwards static slicing of the test case\nin order to determine whether a method has the potential\nto influence the outcome of the test case. If the method\nsummaries of a method call indicate that it cannot affect\nany of the data elements used by subsequent method calls,\nincluding the terminating test assertion, then the method is\nremoved from the test case. Section 4.2 describes the slicing\nand reduction steps in detail.\nRegardless whether the method call is kept or not, we\nannotate the test case with a set of static guards gener\nated from the call's method summary. For removed method\ncalls, the guards determine when the removal is potentially\nno longer safe, while guards for kept calls identify potential\nadded dependencies on earlier statements. Details of guard\ngeneration are covered in Section 4.3.\n4.1 Generating Method Summaries\nMethod summaries are the result of a static analysis per\nformed on the code under test. The analysis determines\nwhich fields and method arguments the method reads from\nor writes to. This information will be used during the slic\ning of the test case (Section 4.2) and during static guard\ngeneration (Section 4.3).\nOur approach for generating the summaries is to execute\na pointsto analysis over the code under test to precisely\nidentify the set of objects a method method may encounter\nduring its execution. We next execute a intraprocedural\nsideeffect analysis of the method using the pointsto graph,\nand statically evaluate each statement in the method to see\nif it reads from or writes to an object field or method argu\nment. If the statement is a method invocation (callsite), the\nanalysis will proceed to analyze the called method, with any\nattributes found within the callchain for the callsite propa\ngated back up to the calling method. Finally, the individual\nstatement read/write attributes are collected together to be\ncome the method summary.\nFor example, given the Point class show in Section 2,\nthe extracted method summaries for the class methods are\nshown in Table 1.\nTable 1: Point Method Summaries\nKey\nReads\nWrites\nPoint\nx, y\nsetX\nx\ngetX\nx\nsetY\ny\ngetY\ny\nmove\nx, y\nxlate\nx, y\nx, y\ntoString\nx, y\nThe propagation of read/write attributes presents a nam\ning issue. Consider the following class:\nclass PointPair {\nprivate Point _a;\nprivate Point _b;\nPointPair( Point a, Point b ) {\n_a = a, _b = b;\n}\nvoid setX( int x ) {\n_a.setX( x );\n_b.setX( x );\n}\n}\nWithin the method PointPair.setX, there are two invoca\ntions of Point.setX applied to two different objects. The\nanalysis of Point.setX reveals that it writes to Point. x,\nbut when this fact is propagated up to the callsites in\nPointPair.setX, they appear the same. Our solution is to\ntransform, if possible, the names of propagated attributes\nwith the class and field the attribute actually refers to. For\nthe example above, the write attribute Point. x becomes\n\nFigure 2: Datadependency graph for test case from Section\n2. The root of the graph is toStriing().\nPointPair. a. x and PointPair. b. x for the respective call\nsites, resulting in two distinct write attributes in the method\nsummary for PointPair.setX.\n4.2 Test Case Slicing and Reduction\nTo identify which statements may be removed from a test\ncase, we need to understand which statements are required\nto satisfy the data needs of the test's final assertion. Intu\nitively, one needs to build a datadependency graph starting\nwith the method call in the assertion statement of the test\ncase, where the graph nodes are the method calls in the test\ncase, and the presence of a directed edge between two nodes\nindicates that a child writes to one or more fields used by\nthe parent it is connected to. Once the graph is complete,\nany nodes that do not have a path from the root (the asser\ntion statement) by definition have no effect on the behavior\nof the method in the assertion statement, and thus no affect\non the assertion itself.\nFigure 2 shows a datadependency graph for the exam\nple test case in Section 2. Note that the topdown graph\nflow is opposite of the topdown linear flow of the test case.\nThere is no directed path from the root of the graph to the\nroutines Point.getX() and Point.getY(), indicating no data\ndependencies between them.\nBecause we have restricted our approach to linear test\ncases, we can do the datadependency analysis and method\nculling in one iteration over the test case, starting at the\nfinal assertion and working backwards. We manage a set of\nactive reads (A) that reflect the unconnected or unsatisfied\ndata elements forward of the current analysis point. If the\nmethod summary of the method call at the current analysis\npoint does not match any of the elements in A, then the\nmethod will not have any affect on the behavior of succeed\ning methods, and it may be removed from the test case. In\nshort, we perform a backwards static slice of the test case,\nand our abstract state is the contents of A.\nA brief outline of the steps involved in our test case slicing\nand reduction is as follows:\n1. Obtain the method summary of the current method.\n2. Determine whether the method satisfies any data de\npendencies\n3. Update propagated data dependency set A\n4. Install any guards for the method\n5. Move to previous method call, or stop when done\nAgain, the method summaries tell us whether a method\nmay read (R), may write (M), or must write (W) zero or\nmore fields. For the slicing algorithm, we are interested in\nthe contents of R and W, and whether the set M is empty.\nWe start the slicing with the A set containing the may\nread R values from the method call in the assertion state\nment:\nA0 := Rn\n(1)\nwhere n is the number of method calls executed within\nthe test case. For each preceding method call Cn-i | i ∈\n{1,...,n}, we calculate the intersection of the method's W\nattribute set and the A set:\nAi-1 ∩ (Wn-i ∪ Mn-i)\n(2)\nTo determine whether a W attriibute satisfies an entry\nin A, we iterate over the values of A, looking for an exact\nmatch in W, or an element in W that is an effective match:\nthe effect of the write is the same as if the attribute had\ncompletely matched the read attribute in A. For instance,\nin the PointPair example of Section 4.1, a W attribute of\nPointPair. a would satisfy the attribute PointPair. a. x.\nIf the resulting intersection set is empty and the method\nhas an empty M set, then we may safely remove the method\ncall from the test case. Otherwise, we keep the method call,\nand update the A set by first removing from it the result of\nthe intersection calculated above, followed by a union of A\nwith the method's R set:\nAi := Rn-i ∪ (Ai-1 - (Ai-1 ∩ Wn-i))\n(3)\nIn other words, we remove all fields guaranteed to be sat\nisfied by the method call and add in any fields the method\nitself may be dependent on. Note that the contents of M\nhas no affect on A; it only inhibits a method from being\nremoved when it is not empty.\nIf at any time the A set becomes empty, we stop the slic\ning, since there is no possibility that any preceding method\ncalls would affect the test case assertion. This constraint\nshould be satisfied even if we work through the entire test\ncase (i = n), since the test case begins by creating the ob\nject under test, and the constructor for the test object would\nhave initialized the field, even if only to a default value.\n4.3 Guard Generation\nAs our algorithm visits each method call in the test case,\nwe annotate the call with one or more guards. The set of\nguards for the test case is the union of the guards for each\nof the method calls in the unreduced test case. Prior to\nexecuting the test case, the test case's guards are evaluated\nto see if the property they represent still holds. If all guards\npass, then the reduced test case is executed; otherwise, the\noriginal, unreduced, test case runs. In short, guards protect\nthe test case from future code changes, signaling the fact\nthat the conditions under which a test case was previously\nreduced no longer apply.\n\nWe have identified the following set of guards in our ap\nproach:\n1. reads at most the contents of the R set from the\nmethod summary\n2. writes at least the contents of the W set from the\nmethod summary\n3. writes at most the contents of the M + W sets\nThe first guard states that the method does not rely on\nany more data than found in the R set. If a code change\nwere to result in an R0 ⊂ R, then the reduced test case is still\nsafe and should be run. The next time the test is reduced,\nit may now prove possible to reduce it even further.\nThe second and third guards state the minimum and max\nimum set of fields that the method may change. The mini\nmum set contains the fields that the method always writes\nto when it is invoked, whereas the latter is the set of fields\nthat the method may write to. If either or both of these\nbounds change, then the dependency graph generated for\ntest case is no longer valid and the original test case is run.\nIntuitively, the relative sizes of the Mi and Wi sets for\nmethod call Ci may provide a measure of how likely the\ncall would be removed. The more elements there are in Mi ,\nthe greater the likelihood that one of the elements will be\na member of the propagated A set, in which case the call\nmust remain in the test case since the M set only indicates\npotential writes, and not the guaranteed writes of W. Fur\nthermore, for each method Ci kept in a test case, one could\nreasonably expect A to grow as it is joined with the call's\nRi set, thus increasing the potential to keep methods Cj |\nj ∈ {0,...,i}.\n5. Preliminary Results\nWe have created a simple test bed to exercise the indi\nvidual components of our approach. The test scenario uses\nclass Point:\npublic class Point {\nprivate int _x;\nprivate int _y;\npublic Point( int x, int y ) {\n_x = x; _y = y;\n}\npublic int getX() {\nreturn _x;\n}\npublic int getY() {\nreturn _y;\n}\npublic void moveBy( int x, int y ) {\n_x += x;\n_y += y;\n}\npublic void moveHorizontally( int x ) {\n_x += x;\n}\npublic void moveVertically( int y ) {\n_y += y;\n}\npublic String toString() {\nreturn \"(\" + _x + \",\" + _y + \")\";\n}\n}\nThere exist two handcrafted driver programs, one for\neach of the above classes. These driver programs exist to\nsimulate the running of a test scenario, but outside of a\ntesting infrastructure. First, the driver for testing the Point\nclass:\npublic class TestPoint {\nstatic public void\nassertEqual( boolean condition ) {\nSystem.out.println(\n\"test case returned \" + condition );\n}\npublic static void main(String[] args) {\nPoint p =\nnew Point( 10, 20 );\np.moveBy( 4, 4 );\np.getX();\np.getY();\np.moveBy( 3, 8 );\np.getX();\np.getY();\np.setX( 5 );\np.getY();\np.getX();\np.setY( 10 );\np.moveHorizontally( 1 );\np.moveVertically( 2 );\np.getX();\np.getY();\nSystem.out.println( p.toString() );\nassertEqual(\np.toString().equals( \"(6,12)\" ) );\n}\n}\nThe test case simply exercises the Point API, with a final\nassertEqual to check that the previous calls produced the\nexpected final state of the Point object.\nWhen processing is done, the following attributes are re\nvealed:\nPoint: int getY()\n<Read Point:_y>\nPoint: void setX(int)\n<Write Point:_x>\nPoint: int getX()\n<Read Point:_x>\njava.lang.String: boolean equals(java.lang.Object)\n<Read java.lang.String:offset>\n<Read java.lang.String:value>\n<Read java.lang.String:count>\nPoint: void setY(int)\n<Write Point:_y>\nPoint: void moveHorizontally(int)\n<Read Point:_x>\n<Write Point:_x>\nPoint: void moveBy(int,int)\n<Write Point:_y>\n<Read Point:_y>\n<Read Point:_x>\n\n<Write Point:_x>\nPoint: void moveVertically(int)\n<Write Point:_y>\n<Read Point:_y>\n5.1 Performance\nRather discouragingly, our implementation of summariza\ntion takes 2.25 minutes to run on an Apple PowerBook\n1.5GHz with 1GB of RAM (the Java VM is limited to a\nmaximum 400MB heap with the Xmx400m option). En\nabling verbose logging reveals that a majority of the time\nis spent generating Soot Jimple representations for the Java\nruntime and support classes. Using a combination of Soot\noptions, we were able to obtain a set of Jimple files for all\nof the runtime classes reached by the call graph rooted by\nour TestPoint class. We then tried to have Soot use these\nfiles instead of dynamic Jimple generation, we encountered\nerrors in the Jimple processing. Apparently, there are dis\ncrepancies (bugs) between what Soot writes out in its Jimple\nrepresentation and what it reads in.\nWe also submitted the TestPoint class to a Purity Analysis\nKit [10]. Interestingly, we obtained similar timing results.\nWe have briefly investigated generating the call graph it\neratively. Previous work by Souter, Pollock [11] showed\npromising results using enhancements to the Flex Compiler\nInfrastructure [6]. Their call graph generation algorithm is\nbased on the Cartisian Product Algorithm of Agesen [1],\nwith modifications to support reanalysis of only the code\nthat changed.\n6. Evaluation\nOur evaluation is based on monitoring actual developer\nactivity without the benefit of automatically generated and\nreduced tests, and then simulating the impact of introducing\ntest reduction into the development process. This allows us\nto get an idea of the benefits of our approach before a full\nfledged tool has been built. We can also simulate scenarios\nthat real developers would not stand for, such as using an\ninefficient test generation technique without reduction.\nWe have monitored a single developer performing devel\nopment and maintenance on fdanalysis, a Java program of\nabout 9000 lines of code. fdanalysis is a package for an\nalyzing data collected during development and debugging\nsessions. It performs mainly text processing and time calcu\nlation. We have captured 1600 snapshots of the state of the\nprogram during development, which include the introduc\ntion and fixing of 12 regression errors. Most of the running\ntime of the existing test suite operates on the highlevel API\nof the package, sending in text files and making assertions\non text output.\nIn our simulated scenario, the developer would like to gen\nerate unit tests for the edited component of the program by\nusing a capture/replay test generation technique, test fac\ntoring [9], on the current suite of system tests, most of which\noperate on the highlevel API of the package, sending in text\nfiles and making assertions on text output.\nHowever, the\nunit tests generated by test factoring are inefficient, and not\na significant improvement on the current tests. We would\nlike to evaluate whether our safe test reduction technique,\nin combination with test factoring, would produce a safe,\nefficient unit regression test suite.\nIn our simulated scenario, test factoring is run each night\novernight to produce inefficient unit tests, which are then\nreduced by safe test reduction into guarded reduced tests.\nFor each snapshot that we have of development during the\nnext day, we can compare running the guarded reduced tests\nwith the unreduced generated tests, and the original system\ntests. Our hypotheses are:\n- The guarded reduced tests are an order of magnitude\nsmaller (in number of method calls against the tested\ncomponent) and faster than either the unreduced gen\nerated tests.\n- In about 1% of the captured snapshots, a static guard\nis violated. When a static guard is violated, the re\nduced test is useless, and an unreduced test must be\nrun to guarantee the system is correct. If this happens\nvery often, the overhead of evaluating the guards will\neliminate any gains from test reduction. However, if\nit happens very seldom, we may have to conclude that\nthe danger of using unguarded reduced tests is not\nvery great, which reduces the value of the overhead of\ngenerating and evaluating guards.\nUnfortunately, in the time allotted for this course, we have\nnot been able to produce an implementation of our approach\nthat correctly handles all of the generated unit tests, so this\nevaluation remains as future work.\n7. Future Work\nThere are several ways that this work can be expanded\nupon.\n1. Our implementation must be made robust enough to\nbe evaluated against realworld test cases.\n2. The basic slicing algorithm should be enhanced to work\ncorrectly in the presence of aliasing.\n3. A usable implementation of our technique requires an\nincremental mechanism for evaluating guards. We be\nlieve that this is easily achieved, by caching in memory\nor on disk the call graph required for guard genera\ntion, and propogating changes in place. However, our\ncurrent implementation must rerun the entire sum\nmarization, a time requirement that overwhelms the\nruntime benefits of reduction.\n8. Conclusion\nAutomated test generation appears poised to become a\nmuch more common development tool. It can provide a\n\"second opinion\" on code, considering cases that the devel\nopers' own assumptions and biases may cause them to over\nlook. It can also dramatically reduce the effort and improve\nthe effectiveness of testing code.\nWe have introduced a technique for reducing the aver\nage runtime of inefficient test cases, whether automatically\nor manually generated, without making any unsafe assump\ntions about the ways in which the tested code may change.\nThese properties should be an essential guarantee of future\ntest generation techniques.\nReferences\n[1] O. Agesen. The Cartesian Product Algorithm. In\nECOOP'95 Conference Proceedings, 1995.\n\n[2] Agitar software. http://www.agitar.com, 2005.\n[3] M. Albrecht. Testing Java with JUnit.\nhttp://www.ddj.com/documents/s=1679/ddj0302b/, 2003.\n[4] J.D. Choi and H. Srinivasan. Deterministic replay of java\nmultithreaded applications. In SPDT '98: Proceedings of\nthe SIGMETRICS symposium on Parallel and distributed\ntools, pages 48-59, New York, NY, USA, 1998. ACM Press.\n[5] C. Csallner and Y. Smaragdakis. JCrasher: an automatic\nrobustness tester for Java. Softw. Pract. Exper.,\n34(11):1025-1050, 2004.\n[6] Flex compiler infrastructure.\nhttp://www.flexcompiler.lcs.mit.edu/.\n[7] A. Orso and B. Kennedy. Selective Capture and Replay of\nProgram Executions. In Proceedings of the Third\nInternational ICSE Workshop on Dynamic Analysis\n(WODA 2005), pages 29-35, St. Louis, MO, USA, may\n2005.\n[8] C. Pacheco and M. D. Ernst. Eclat: Automatic generation\nand classification of test inputs. In ECOOP 2005 --\nObjectOriented Programming, 19th European Conference,\npages 504-527, Glasgow, Scotland, July 25-29, 2005.\n[9] D. Saff, S. Artzi, J. H. Perkins, and M. D. Ernst.\nAutomatic test factoring for Java. In ASE 2005:\nProceedings of the 21st Annual International Conference\non Automated Software Engineering, Long Beach, CA,\nUSA, November 9-11, 2005.\n[10] A. Salcianu and M. Rinard. Purity and side effect analysis\nfor Java programs. In Proceedings of the 6th International\nConference on Verification, Model Checking and Abstract\nInterpretation (VMCAI'05), 2005.\n[11] A. Souter and L. Pollock. Incremental call graph reanalysis\nfor objectoriented software maintenance. In Proceedings\nInternational Conference on Software Maintenance, pages\n682-691, 2001.\n[12] T. Xie, D. Marinov, and D. Notkin. Rostra: A framework\nfor detecting redundant objectoriented unit tests. In\nProceedings of the 19th IEEE International Conference on\nAutomated Software Engineering (ASE 04), pages 196-205,\nSeptember 2004."
    },
    {
      "category": "Resource",
      "title": "test_generation.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/520791ae3d9771039f3953ac50e5ffd0_test_generation.pdf",
      "content": "Automated Test Data Generation with SAT\nRobert Seater and Gregory Dennis\nABSTRACT\nWe present a novel technique for automatically generating\na suite of test inputs to an objectoriented procedure. The\nsuite is guaranteed to kill all mutant procedures produced\nfrom a given catalog of mutation operators, so long as those\nmutants could be detected by some test within userprovided\nbounds.\nOur test input generator constructs a mutationparameterized\nversion of the procedure, whose behavior may differ from\nthe original by a single mutation. It encodes the original\nand mutationparameterized procedures in a firstorder rela\ntional formula, a solution to which includes both a mutation\nand a test input that detects it. Our tool iteratively invokes\na constraintsolver to find such tests and adds them to the\ntest suite. After each iteration, it amends the formula to\nensure the next mutant found was not killed by a previous\ninput. This process is repeated until the formula becomes\nunsatisfiable, at which point all mutants have been detected.\nWe evaluate an implementation of this technique on a series\nof small benchmarks.\n1. INTRODUCTION\nThe degree to which a test suite explores the behavior of a\nprogram is the suite's coverage. For a well chosen coverage\nmetric, the greater the coverage of a test suite, the more\nlikely the suite is to detect bugs in the program. There are\nseveral metrics for measuring the coverage of a test suite,\nsuch as the percentage of statements the suite executes or\nthe percentage of program paths it exercises. In this paper,\nwe concern ourselves with mutation coverage, a measure of\na test suite's ability to distinguish a piece of code from small\nvariants, or mutants, of that code.\nMutation testing refers to the process of evaluating the mu\ntation coverage of a test suite. A catalog of mutations oper\nators is applied to the program to generate a large collection\nof mutants, which are then executed on inputs from the test\nsuite. If a mutant behaves differently from the original pro\ngram on at least one of the tests in the suite, the suite is\nsaid to kill that mutant. The percentage of mutants killed\nis the suite's rate of mutant killing.\nThe central premise of mutation testing is the presence of\na coupling effect between simple and complex faults in a\nprogram [4]. The effect says that a test suite that detects\nall simple faults in a program (as represented by a single\nmutation) will detect most complex faults. This claim has\nreceived experimental and analytical support [15, 14, 26,\n16].\nMutation testing can be a long and costly process. The en\ntire test suite must be run on every mutant, of which there\ncan be on the order of hundreds to thousands. Furthermore,\nwhen a suite fails to detect a mutant, there are two possi\nbilities: either the suite is inadequate to cover that mutant,\nor the mutant is equivalent to the original program. De\ntermining if a mutant is equivalent usually involves manual\ninspection, although a number of automated heuristics have\nbeen proposed [19, 10, 17, 22]. Such equivalent mutants be\nhave the same as the original program on every input --\nthey cannot be killed by any test -- and therefore, they do\nnot indicate a weakness of the test suite. Equivalent mu\ntants result from irrelevant mutations, such as negating the\nargument to an absolute value function or mutating dead\ncode.\nThe drawbacks of achieving high coverage with testing can\nbe mitigated through the use of automatic test data gen\neration. Constraintbased automated test data generation\n\nis a hybrid staticdynamic approach. It statically gener\nates test inputs guaranteed to desirable property, such as\nhigh statement, branch, or mutation coverage. These in\nputs can then be dynamically executed and either evaluated\nagainst an oracle, or, in the case of regression testing, kept\nas an oracle for future executions. Many such techniques\nare constraint based, meaning they use algebraic or logical\nconstraint solvers to generate the test inputs.\nWe present a constraintbased test data generation tech\nnique for constructing a suite of test inputs that will, by\nconstruction, kill every nonequivalent mutant that is kill\nable by some input within the user defined bounds on heap\nsize and loop unrolling. This is facilitated by the use of a\nSATbased constraint solver. Our test input generator en\ncodes the procedure in a logical formula that allows a single\nmutation of that procedure to be enabled. Which mutation\nis chosen depends on the value of an unconstrained vari\nable in the formula, and additional constraints encode the\nnotion of killing a mutant. Our tool invokes a constraint\nsolver to solve the formula for test inputs that are guaran\nteed to kill all killable mutants drawn from some catalog.\nOur constraint solver is based on the Alloy Analyzer, which\nencodes our formula in SAT and invokes a SATsolver to\nfind its solutions [11].\n1.1 Comparison to Specification Analysis\nSince our approach uses a constraint solver to search for\ntests that kill mutants, one might decide to instead search\ndirectly for an input that causes the procedure to violate its\nspecification. Although doing so would only cost as much\nas a single iteration of our approach, there are two reasons\nwhy it may be undesirable in practice.\nFirst, often no specification is available, either because no\none is capable of or willing to write it or because it is not\nexpressible in the given constraint language. Writing a cor\nrect logical specification is a difficult task that often requires\nskilled abstract thinking. In practice, the average program\nmer may find it easier to manually inspect a series of test\nexecutions for correctness than to write a correct, general\nspecification for all possible executions. Neither our ap\nproach nor mutation testing require existing specifications.\nSecond, even in the presence of a specification, the cost of\nstatically checking the code against the specification may\nbe far greater than running the test suite. If so, it may be\nprofitable to invest substantial time up front to generate a\nregression test suite that kills all mutants and that can be\nquickly run in the future.\n2. APPROACH\nWe now discuss how to construct a relational first order logic\nformula whose solution is a pair - a mutation (drawn from\nsome catalog) and a test input which kills that mutant. A\nmutation m kills an input s if s causes the original program\np and the mutated program p m to produce different out\nput: if p m(s) = p(s). On successive iterations, that formula\nis amended to exclude any mutation which is killed by a\npreviously generated input. The analysis we use requires a\nbounded state space, which corresponds to a bound on the\nsize of the heap and the number of loop unrollings. If, on\nsome iteration, the formula is unsatisfiable, then the pre\nviously generated inputs kill all mutants killable by inputs\nwithin the given bound. The analysis is complete. Every\ntest added is guaranteed to kill at least one additional mu\ntant.\n2.1 Logical Constraints\nA procedure p can be modeled as a function from pre to\npoststates, where p(s) = s0 is true if and only if p relates\ninput state s to output state s0. Applying mutation m1 to\nprocedure p yields the mutant procedure p m1 . An Alloy\nformula can be constructed that is true if and only if there\nexists a mutation m1 and a prestate s1 such that p and p m1\nbehave differently on s1 (s1 kills p m1 ):\n∃m1, s1 . p(s1) = p m1 (s1)\nThe solution witnesses the mutation m1 and the prestate\ns1 that kills p m1 . The input s1 is added to the input suite,\nand the formula is amended to find a new test that kills a\nmutant not killed by the previous test:\n∃m2, s2 . p(s2) = p m2 (s2) ∧p(s1) = p m2 (s1)\nThe prestate s2 found by this analysis kills a new mutant\nm2 that would not have been killed by s1. We then repeat\nthis process. The formula that finds the nth such input is\nas follows:\nn, sn .\np(sn) = p mn (sn)∧\n∃m\np(sn-1\n) = p mn (s\nmn (s1)\nn-1) ∧· · · ∧p(s1 ) = p\nThis process continues until the formula is unsatisfiable, at\nwhich point a suite of test inputs has been assembled, each\nof which kills at least one unique mutant. One could instead\nhalt the process after it generates a certain number of test\ncases or has run for a certain amount of time, but doing so\nrisks dramatically reducing the value of the resulting test\nsuite.\nIf one is generating a test suite to check the correctness of\np, the correct output for each input must be determined\nseparately. If one is generating a regression test suite (and\nassuming the original procedure p to be correct), each out\nput p(si ) witnessed by the solution is considered to be the\ncorrect output.\n2.2 Parameterizing Mutations\nThe procedure p m is the original procedure p parameterized\nby the choice of a single mutation m. Thus, there are as\nmany values m can assume as there are mutants. To con\nstruct the body of p m, we instrument each statement in p\nwith additional conditionals that test the value of m and\nthat either execute the mutation to which m corresponds,\nor that execute the original statement if m does not corre\nspond to a mutation. To illustrate, consider the following\ntwo program statements:\nx = y + z;\nb = x > 10;\nConsider two mutation operators: 1) replacing an addition\nor subtraction sign with the other (the \"ASR\" mutation\n\nOperator\nDescription\nASR\naddition subtraction replacement\nCOR\ncomparison operator replacement\nIAR\ninvocation argument reordering\nNEG\nboolean negation\nOBO\nVIR\noffbyone errors (±1)\nvariable identifier replacement\nTable 1: Mutation Operators\noperation); and 2) swapping one comparison operator with\nanother (COR). If the prior two statements appeared in the\noriginal procedure p, applying ASR to the first statement\nand COR to the second would yield the following statements\nin p m:\nif (m == m1)\nx = y z;\nelse\nx = y + z;\nif (m == m2)\nb = x >= 10;\nelse if (m == m3)\nb = x < 10;\nelse if (m == m4)\nb = x <= 10;\nelse if (m == m5)\nb = x == 10;\nelse if (m == m6)\nb = x != 10;\nelse\nb = x > 10;\nGiven a particular value of m, p m simulates p varied by\na single mutation. Table 1 gives the full list of mutation\noperators our implementation considers.\nASR swaps + and - operators in arithmetic expressions.\nCOR changes one inequality comparison (>, ≥, <, ≤) into\nanother inequality or into an equality comparison (=, =).\nCOR can also toggle = and = operators. IAR reorders ar\nguments in a procedure call if their types are compatible.\nNEG negates a boolean expression. OBO adds or subtracts\n1 from an arithmetic expression. VIR replaces one variable\nidentifier with another inscope variable of the same time.\nOffutt, Rothermel, Untch, and Zapf propose 22 operators for\nC programs, which is often used as a standard [18]. 6 cor\nrespond to our 6 (although not onetoone). 6 more involve\narray references and thus would not affect the programs we\nhave examined so far. 4 are specific to language features\n(e.g. GOTO). Our technique could be extended to handle\nthe remaining 6, which include mutantions such as state\nment deletion and constantforconstant replacement.\n2.3 Constraint Solving\nOur implementation encodes the original and mutation pa\nrameterized procedures and the formulas presented in Sec\ntion 2.1 into a relational first order logic based on Alloy [11].\nIt uses the encoding formalized by Dennis et al. [6], and\nsolves it using a new tool based on the Alloy Analyzer. A\nsolution to the formula indicates a mutant that is not killed\nby any previous test input, and a test input that kills that\nmutant. If the formula is not satisfiable, then no additional\ntest inputs are necessary to kill all killable mutants.\nThe Alloy Analyzer does not symbolically prove the exis\ntence of a solution. Instead, it exhaustively searches the\nentire state space of scenarios within finite, userdefined\nbounds. It is able to analyze millions of scenarios in a mat\nter of seconds. Consequently, failure to produce a solution\ndoes not constitute proof that one does not exist, but ev\nery solution reported is guaranteed to be correct. Failure to\nproduce a solution does, however, guarantee that there is no\nsolution within the chosen bounds.\nThus, the resulting test suite depends on bounds we place on\nthe analysis. Those bounds are limitations on the number of\ninstances of each type, in effect a limitation on the size of the\nheap. If a mutant can only be killed by inputs that exceed\nthose bounds, then it will not be killed by the resulting suite.\n2.4 Jimple Representation and Limitations\nThe programming language accepted by our tool is a sub\nset of Java. The subset we use does not yet include arrays,\niterators, constructors, exceptions, or threads. To render\nJava bytecode amenable to analysis we first convert it to\nthe Jimple intermediate representation using the Soot com\npiler optimization framework [23, 25]. Unlike Java source\ncode, Jimple is a typed 3address representation in which\neach subexpression is assigned to a unique local variable.\nOther differences in Jimple include the presence of gotos in\nplace of loop constructs, the representation of booleans as\nintegers, the desugaring of shortcircuiting boolean opera\ntors into nested ifbranches, and the expanded scope of each\nlocal variable to the entirety of the method body.\nAs a consequence of the Jimple representation, a single mu\ntation to the Jimple does not always correspond to the same\nmutation in the original source code. For example, testing\nwhether a boolean expression is true is represented in Jim\nple as testing whether an integer expression is not equal to\nzero. Thus, using the COR mutation on Jimple to invert\nthis equality test actually subsumes the NEG mutation at\nthe source code level. There are also some simple source\nmutations that cannot be readily accomplished via Jimple,\nsuch as replacing one shortcircuiting boolean operator with\nanother. Also, because Jimple assigns each subexpression\nto a new local variable, applying VIR to Jimple could be\nequivalent to a source mutation that replaces one subex\npression with another. Due to the absence of scopes on\nlocal variables, VIR can occasionally cause mutations that\nwould have prevented the original source code for compiling\nsuccessfully.\n2.5 Preprocessing\nOnce a Java method is converted to Jimple, some additional\npreprocessing is required before it can be encoded as a re\nlational model. First loops are unrolled for a finite number\nof iterations. At the end of the unwinding, an \"assume\"\nstatement that contains the negation of the loop condition\nis inserted. These assume statements are recognized by our\n\nint abs (int x) {\nif(x >= 0)\nreturn x;\nelse\nreturn x;\n}\nFigure 1: Absolute Value Method\nencoding and constrained to be true in the resulting formula.\nThe effect is to exclude all executions of the procedure if it\nwould exceed the fixed number of loop iterations. Thus, if\na mutant could only be killed by a test that exceeded that\nlimit, it would not be killed by this analysis.\nLastly, dynamic dispatch is resolved into a series of tests\non the type of the receiver argument, each test of which is\nfollowed by an invocation of the concrete method provided\nby that type. A simple class hierarchy analysis is currently\nused to determine the potential target methods of an invo\ncation, though more sophisticated analyses could be easily\nplugged in.\n2.6 Small Example\nConsider the absolute value algorithm shown in Figure 1.\nOur mutationparameterization instruments this code with\n10 possible mutations. COR spawns five mutants on line\n2: the ≥ can be replaced with either >, <,\n=, or =.\n≤,\nThe return statement on line 3 spawns two OBO mutants:\nreturn x+1 or return x1. Due to the Jimple intermediate\nrepresentation of the procedure, the single Java statement\nreturn x (line 5) is represented as two statements: int\nx' = x; and then return x';. As a result, there are two\nlocal variables in scope (x and x'), and OBO and VIR create\nthree mutant return values for line 5: return x'+1, return\nx'1, and return x.\nThe test input generator produces 2 test inputs to kill the\n10 possible mutants: 4 and -2. On this tiny example, our\nimplementation runs nearly instantaneously. The tool first\nfinds that the test input \"x = 4\" kills the mutant in which\n≥ is replaced by <. It then amends the constraint problem\nto disallow any mutant killed by that input. Solving the\namended formula produces the input \"x = -2\" to kill the\nmutant where ≥ is replaced by =. Amending the constraint\nproblem a second time produces an unsatisfiable formula,\nindicating that no additional test inputs with scope would\nkill mutants which haven't already been killed.\n3. RESULTS\n3.1 Evaluation On Benchmarks\nWe evaluate our technique on several small examples, record\ning the execution time and number of test inputs generates.\nThe primary factor determining the technique's run time\nappears to be that number of paths in the program, not the\nnumber of lines or eventual size of the test input. Since each\nmutation adds a conditional to the program, the number of\nmutations is a good estimate of the number of paths and\nthus of the time it takes our technique to operate.\nAbsolute Value\nboolean contains(int value) {\nBucket current = this.getHead();\nwhile (current != null) {\nif (value == current.getValue()) {\nreturn true;\n}\ncurrent = current.getNext();\n}\nreturn false;\n10 }\nFigure 2: List Containment\nIn Figure 1 of Section 2.6, we saw a java procedure for com\nputing absolute value.\n6 lines of code\n8 single mutants\nintegers range -10 to +10\n2 inputs generated\n1.5 seconds to compute those inputs\nList Containment\nFigure 2 gives Java code for determining whether or not an\ninteger list contains a given value.\n10 lines of code\n8 single mutants\n3 loop unrollings, 3 element lists, integers range -1 to +2\n3 inputs generated\n1.5 seconds to compute those inputs\nTree Node Insertion\nThe Appendix gives Java code for inserting a node into a\nbinary search tree of integers.\n21 lines of code\n40 single mutants\n3 loop unrollings, 3 node trees, integers range -2 to +2\n5 inputs generated\n30 minutes to compute those inputs\nTree Node Removal\nThe Appendix gives Java code for removing a node from a\nbinary search tree of integers.\n50 lines of code\n242 single mutants\n1 loop unrolling, 3 node trees, integers range -2 to +2\ntimed out after 10 hours, at which point it had generated 8\ninputs\n3.2 Compound Mutant Coverage\nThe coupling effect is the hypothesis that a suite killing most\nsimple mutants will kill most complex ones. We evaluate\nthis hypothesis by evaluating the input suites we generate,\nwhich kill all single mutants, against compound mutants,\nprograms in which several of our mutation operators have\nbeen applied. The 2 inputs generated for absolute value\nkilled all 97 possible compound mutants. The 3 inputs gen\nerated for list containment killed all 136 possible compound\n\nmutants. These are encouraging, but far from definitive,\nresults.\n3.3 Locally Minimal Test Suites\nWhile our technique guarantees that no two inputs will kill\nthe same set of mutants, it makes no guarantee that the\nsuite will be globally minimal, or even locally minimal. It\nis guaranteed only that no two inputs in the suite kill ex\nactly the same set of mutants. However, in the limited cases\nwe have examined so far, the suites generated are locally\nminimal.\nFor the list containment procedure, our technique generated\n3 inputs which killed all 8 possible single mutants. There are\n136 compound mutants - 26 double, 44 triple, 41 quadruple,\n20 quintuple, and 4 sextuple. Only 11 of those compound\nmutants are killed by only one of the three test inputs. 8\nof those cases only killed by input #2, 2 were only killed by\ninput #3, and 1 was only killed by input #3. Thus, of the\n3 possible reduced suites and of the 136 possible compound\nmutants, the reduced suite will fail to catch a mutant which\ncould have been caught by the full suite 136(3) = 2.7% of\nthe time.\nIf one somehow knew to leave out just input #1, the least\nimportant input, the reduced suite would only fall short of\nthe full suite\n= 0.7% of the time. However, if one\n136(1)\neliminated just input #2, the most important input, the\nreduced suite would fall short of the full suite\n= 5.9%\n136(1)\nof the time.\n4. RELATED WORK\nMutation Testing and Coverage\nOur approach is unique in that it is guaranteed to generate\na suite of test inputs with 100% mutation coverage (of mu\ntants killable by small inputs); other techniques rarely go\nabove 90% coverage and are often much lower. With other\ntechniques, generating a suite with a higher rate of muta\ntion coverage means generating a larger suite, and achieving\n100% coverage would require effectively infinite cost (detect\ning equivalent mutants is undecidable, and many algorithms\ninvolve a random component). With our approach, 100% of\nkillable mutants are always killed at finite (but high) cost.\nFuture work includes more thoroughly evaluating the com\npound mutant killing capabilities of suites with 100% muta\ntion coverage and those with high but imperfect coverage.\nMutation coverage is just an estimate of error coverage, and\na suite that kills many mutants could be poor at detecting\nreal errors. It is still an open question as to just how ef\nfective mutation coverage is as a metric for evaluating test\nsuite. While practicioners often reflect positively on muta\ntion testing, there have been only few studies to directly\nsupport its value, and they cannot be considered definitive.\nThe DeMillo [2] published the first analytical and empirical\nevaluation of mutation coverage as a metric, which was later\nexpanded upon by Offutt [21]. Wah [26] and Offutt [16]\nhave worked on directly evaluating the coupling effect, the\nhypothesis that a suite with detects all small errors (single\nmutants) will be effective at detecting most complex bugs.\nWe have added to that work by evaluating the ability of\nsuites that kill all single mutants at killing compound errors\n(multiplemutants).\nFor background on mutation testing, beyond what is neces\nsary for this paper, we recommend Offutt and Untch's 2000\nsurvey paper [22], which provides a clear and thorough dis\ncussion of the the practical obstacles and recent innovations\nof mutation testing.\nConstraintBased Test Data Generation\nDeMillo and Offutt coined the term \"constraintbased auto\nmatic test data generation\" (CBT) [5] Most such techniques\ninvolve constructing a set of algebraic constraints to encode\na both wellformedness and a goal (such as covering a partic\nular statement or path) as an objective function. A minimal\nsolution to those constraints corresponds to finding a well\nformed input that achieves the goal. A much smaller set of\ntechniques instead use logical constraints. We discuss both\ntypes below.\nTest Data Generation with Algebraic Constraints\nDeMillo and Offutt [5, 4, 19, 17] propose a particular CBT\ntechnique that, given a mutant, uses an algebraic constraint\nsolver to attempt to generate a test case to kill that mu\ntant. It tries to generate test input with the following three\nproperties: Reachability condition - the program reaches\nthe mutated statement; Necessary condition - the mutated\nstatement causes the program to enter a different (presum\nably erroneous) state; Sufficient condition - that difference\npropagates to the output of the program. They solve alge\nbraic constraints on the test input to meet the reachability\nand necessary conditions. In contrast, we directly solve the\nsufficient condition, implicitly satisfying both the reachabil\nity and necessary conditions. Unlike our approach, their\ntechnique attempts to kill a mutant even if that mutant\ncould be killed by a prior test case.\nIn followup work, Offutt and Pan [19] present a technique\nfor detecting that two mutants are equivalent. Like DeMillo\nand Offutt's automatic test case generation, it is a heuristic\nbased method for solving algebraic constraints that is un\nsound. Because our technique searches for a test case that\ncauses the mutant to behave differently than the original,\neach mutant found is nonequivalent by construction.\nFerguson and Korel propose chaining, a method to generate\ntests by observing the execution of the program under an ex\nisting test [7]. They bias the algorithm towards generating\ntests that provide coverage of particular lines of code - either\nlines of particular interest or lines that are not covered by the\nexisting tests. To that end, they use data dependence analy\nsis to guide the search process by identifying statements that\naffect the execution of the target statement. They evaluate\nit in terms of the likelihood of finding well formed inputs\nthat exercise the target line.\nTracey, Clark, and Mander introduce a technique which\nsolves algebraic constraints to automatically generate well\nformed test data [24]. They theorize that, while solving\ngeneral constraints is intractable, constraints resulting from\nreal software are a much easier subset. They solve the con\nstraints with the help of simulated annealing, a probabilistic\nalgorithm for global optimization of some objective function\n\ninspired by metallurgy models of cooling. They construct an\nobjective function which represents how close a given solu\ntion is to satisfying the algebraic constraints (e.g. x = 49\nis closer to satisfying X > 50 than is x = 2). Randomiza\ntion in the algorithm causes it to (usually) produce different\nresults on different iterations. They evaluate their work in\nterms of the time taken by their implementation to gener\nate 50 well formed test inputs for small examples. Their\napproach is focused on generating well formed inputs given\na set of algebraic constrains, not inputs with any particular\nfeature (e.g. mutation coverage).\nGupta, Mathur, and Soffa show how iterative relaxation an\nbe used to generate test inputs that cover a specified path,\nas a step in a building a test suite with path coverage. An\ninput is iteratively refined until all branch predicates along\na given path evaluate to the desired outcome. In each it\neration, the current input is executed and used to derive a\nset of linear constraints. Those constraints are solved (us\ning Gausian elimination) to obtain the input for the next\niteration. If the branch conditions on the given path are\nlinear, the the algorithm will terminate with either an in\nput which takes that path, or a guarantee that the path is\ninfeasible. The complexity of the problem grows with the\nnumber and complexity of the branch predicates, not the\nlength of the trace, so the authors are optimistic about the\npractical scalability of the algorithm. [8]\nMichael and McGraw developed the the GADGET system\nwhich uses a form of dynamic test data generation [13]. It\ntreats parts of the program as functions which can be evalu\nated by executing the program, and whose value is minimal\nfor inputs that satisfy some desirable property (in their pa\nper, the use condition coverage). Because of that correspon\ndence between test data generation and function minimiza\ntion, they can instead solve the latter, betterunderstood\nproblem using techniques such as simulated annealing, ge\nnetic search, and gradient descent. A randomized compo\nnent encourages the solutions to be different on different\nruns. The authors evaluate the different methods of solving\nthe algebraic constraints generated by GADGET in terms\nof condition coverage of a 2000 line C program, with re\nspect to the number of tests generated. The curves flatten\nout at about 10,000 test inputs, with the leaders being sim\nulated annealing (91%) and gradient descent (83%). The\ntime taken to generate these tests is not reported.\nLogical Constraint Solving\nMarinov and Khurshid introduced TestEra [12] and Boya\npati, Khurshid, and Marinov introduced Korat [1] - tools\nthat use logical constraint solving to generate inputs to a\nprogram based on a wellformedness predicate on the in\nput. Symmetry breaking constraints added to the constraint\nproblem prevent the generation of isomorphic input. Inputs\nare not filtered according to any particular metric. In con\ntrast, our approach examines the program body to select a\nmuch smaller subset of the inputs that still achieves muta\ntion coverage. Korat, like our tool, uses Alloy technology to\nsolve logical constraints.\n\n5. REFERENCES\n[1] C. Boyapati, S. Khurshid, and D. Marinov. Korat:\nAutomated testing based on Java predicates.\nSubmitted for publication, February 2002.\n[2] T. A. Budd, R. A. DeMillo, R. J. Lipton, and F. G.\nSayward. Theoretical and empirical studies on using\nprogram mutation to test the functional correctness of\nprograms. In POPL '80: Proceedings of the 7th ACM\nSIGPLANSIGACT symposium on Principles of\nprogramming languages, pages 220-233, New York,\nNY, USA, 1980. ACM Press.\n[3] W. H. Deason, D. B. Brown, K. H. Chang, and J. H.\nCross II. A rulebased software test data generator.\nIEEE Transactions on Knowledge and Data\nEngineering, 3(1):108-117, 1991.\n[4] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints\non test data selection: Help for the practical\nprogrammer. 11(4):34-41, April 1978.\n[5] R. A. DeMillo and A. J. Offutt. Constraintbased\nautomatic test data generation. IEEE Trans. Softw.\nEng., 17(9):900-910, September 1991.\n[6] G. Dennis, F. Chang, and D. Jackson. Checking\nrefactorings with SAT. Submitted for publication,\nSeptember 2005.\n[7] R. Ferguson and B. Korel. The chaining approach for\nsoftware test data generation. ACM Trans. Softw.\nEng. Methodol., 5(1):63-86, 1996.\n[8] N. Gupta, A. P. Mathur, and M. L. Soffa. Automated\ntest data generation using an iterative relaxation\nmethod. In SIGSOFT '98/FSE6: Proceedings of the\n6th ACM SIGSOFT international symposium on\nFoundations of software engineering, pages 231-244,\nNew York, NY, USA, 1998. ACM Press.\n[9] R. G. Hamlet. Testing programs with the aid of a\ncompiler. IEEE Transactions on Software\nEngineering, 3(4):279-290, July 1977.\n[10] R. M. Hierons, M. Harman, and S. Danicic. Using\nprogram slicing to assist in the detection of equivalent\nmutants. Software Testing, Verification & Reliability,\n9(4):233-262, 1999.\n[11] D. Jackson. Automating firstorder relational logic. In\nProc. ACM SIGSOFT Conf. Foundations of Software\nEngineering (FSE), November 2000.\n[12] D. Marinov and S. Khurshid. Testera: A novel\nframework for automated testing of Java programs. In\nProc. 16th International Conference on Automated\nSoftware Engineering (ASE), November 2001.\n[13] C. C. Michael and G. McGraw. Automated software\ntest data generation for complex programs. In\nAutomated Software Engineering, pages 136-146, 1998.\n[14] L. J. Morell. A theory of faultbased testing. IEEE\nTrans. Softw. Eng., 16(8):844-857, 1990.\n[15] A. J. Offutt. Investigations of the software testing\ncoupling effect. ACM Trans. Softw. Eng. Methodol.,\n1(1):5-20, 1992.\n[16] A. J. Offutt. Investigations of the software testing\ncoupling effect. ACM Transactions on Software\nEngineering and Methodology, 1(1):5-20, 1992.\n[17] A. J. Offutt and W. M. Craft. Using compiler\noptimization techniques to detect equivalent mutants.\nSoftware Testing, Verification & Reliability,\n4(3):131-154, 1994.\n[18] A. J. Offutt, A. Lee, G. Rothermel, R. H. Untch, and\nC. Zapf. An experimental determination of sufficient\nmutant operators. ACM Transactions on Software\nEngineering and Methodology, 5(2):99-118, 1996.\n[19] A. J. Offutt and J. Pan. Automatically detecting\nequivalent mutants and infeasible paths. Softw. Test.,\nVerif. Reliab., 7(3):165-192, September 1997.\n[20] A. J. Offutt, J. Voas, and J. Payne. Mutation\noperators for Ada, 1996.\n[21] J. Offutt, A. Lee, G. Rothermel, R. Untch, and\nAuthor:. An experimental determination of sufficient\nmutation operators. Technical report, Fairfax, VA,\nUSA, 1994.\n[22] J. Offutt and R. H. Untch. Mutation 2000: Uniting\nthe orthogonal. In Mutation 2000: Mutation Testing\nin the Twentieth and the Twenty First Centuries,\npages 45-55, October 2000.\n[23] V. S. P. L. E. G. Raja Vall eeRai, Laurie Hendren and\nP. Co. Soot a Java optimization framework. In\nProceedings of CASCON 1999, pages 125-135, 1999.\n[24] N. Tracey, J. Clark, and K. Mander. Automated\nprogram flaw finding using simulated annealing. In\nISSTA '98: Proceedings of the 1998 ACM SIGSOFT\ninternational symposium on Software testing and\nanalysis, pages 73-81, New York, NY, USA, 1998.\nACM Press.\n[25] R. ValleeRai. The Jimple framework. 1998.\n[26] K. S. H. T. Wah. An analysis of the coupling effect I:\nsingle test data. Sci. Comput. Program.,\n48(23):119-161, 2003.\n\nAppendix\nparent.right = change;\nJava code for a binary search tree of integers with insert and\n}\nremove methods.\nreturn true;\n}\npackage test;\nstatic Node removeNode(Node current) {\nclass Node {\nNode left = current.left, right = current.right;\nint value;\nNode left, right;\nif (left == null)\nreturn right;\n}\nif (right == null)\nreturn left;\nclass Tree {\nNode root;\nif (left.right == null) {\ncurrent.value = left.value;\ncurrent.left = left.left;\nvoid insert(Node n) {\nNode x = this.root;\n}\nreturn current;\nNode parent = null;\nNode temp = left;\nwhile (x != null) {\nparent = x;\nif (n.value < x.value) {\nwhile (temp.right.right != null) {\ntemp = temp.right;\n}\n}\nx = x.left;\n} else {\nx = x.right;\n}\n}\n}\ncurrent.value = temp.right.value;\ntemp.right = temp.right.left;\nreturn current;\nif (parent == null) {\nthis.root = n;\n} else {\nif (n.value < parent.value) {\nparent.left = n;\n} else {\nparent.right = n;\n}\n}\n}\nboolean remove(int info) {\nNode parent = null;\nNode current = root;\nwhile (current != null) {\nif (info == current.value)\nbreak;\nif (info < current.value) {\nparent = current;\ncurrent = current.left;\n} else /* (info > current.value) */ {\nparent = current;\ncurrent = current.right;\n}\n}\nif (current == null) return false;\nNode change = removeNode(current);\nif (parent == null) {\nroot = change;\n} else if (parent.left == current) {\nparent.left = change;\n} else {"
    },
    {
      "category": "Resource",
      "title": "unit_regression.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/7803dd4de9030c6e9774651041871734_unit_regression.pdf",
      "content": "Automatic Generation of Unit Regression Tests\nShay Artzi\nAdam Kie zun\nCarlos Pacheco\nJeff Perkins\nMIT CSAIL\n32 Vassar Street\nCambridge, MA 02139\nAbstract\nSoftware developers spend a significant amount of time modify\ning existing code. Regression testing (comparing previous behav\nior with current behavior) can be a powerful technique to check\nthat modifications do not introduce unintended changes. This pa\nper introduces a technique to automatically create classspecific\nregression tests from a program run. The technique is implemented\nfor the Java language in Palulu.\nIn order to test a class, it may be necessary to place the class\nand other classes that it depends upon in specific states. To get to\nthe desired state, methods may be need to be called in a particular\norder and with specific parameters. Palulu uses information from\nthe program run to accomplish this. It builds a model for each\nclass and uses the model to explore the possible behaviors of the\nclass and build corresponding regression tests. It uses observer\nmethods (those that do not modify the state) to capture the result.\nIn our experiments, Palulu succeeded in building tests for non\ntrivial programs and performed as well as or better than hand\nwritten tests.\n1. Introduction\nTests for a unit of code require two parts: test inputs (a se\nquence of assignment statements and method calls) to exercise the\ncode and an oracle to determine whether or not the result is cor\nrect. An oracle can be difficult to obtain. Regression testing uses\na previous version of the code as an oracle, which allows a test to\nfind when the behavior of the code has changed. Since much of\nsoftware development is spent modifying code, regression testing\ncan be quite valuable.\nOur goal is to automate the creation of regression tests--both\nthe creation of test inputs, and the creation of regression oracles.\nThe introduction consists of three parts. To motivate the prob\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nCopyright 200X ACM XXXXXXXXX/XX/XX ...$5.00.\nlem, we first describe the challenges that automated input creation\ntechniques face. Next, we describe our approach to creating test\ninputs automatically. Finally, we describe our approach to creating\nregression oracles.\n1.1 Creating Test Inputs is Difficult\nCreating test inputs often requires placing an object in a par\nticular state. We define the state of an object as the value of each\nof its (transitively reachable) fields. As the program progresses,\nmutable objects transition through various states. Each mutator\nmethod (one that changes the value of fields) may change the state\nof the object. The state of the object can thus also be defined by the\nsequence of mutator method calls (including their arguments) that\nhave been made on the object. Consider the example Java classes\nin Figure 1. The sequence of calls\nGraph g1 = new Graph();\ng1.init();\nNode n1 = new Node(\"NYC\");\nn1.setOwner(g1);\ng1.addNode(n1);\ndefines a state for g1 and n1.\nNot all possible sequences of method calls are valid. For exam\nple, Graph.addNode is only valid after Graph.init has been\ncalled. Node.addEdge is only valid after Node.setOwner\nhas been called. Graph.addEdge is only valid when Graph\n.addNode has been called on both of its arguments. A method\ncall is invalid when it violates the (implicit or explicit) invariants\nof the class. In all of the above examples, the invalid call throws\nan exception. Consider, however\nn1.setOwner(g2)\ng1.addNode(n1);\nThe call to g1.addNode is invalid because n1's owner is g2.\nThis does not throw an exception but is clearly not desirable (be\ncause g1 now contains a node whose owner is not g1).\nAdditionally, not all of the possible values for primitive and\nstring arguments will be valid for a method call. For example,\nstrings often need to be in a particular format (such as a date,\nmethod signature, or URL). Numeric arguments may need to be\nin a range or have a particular mathematical relationship to one\nanother.\nA sequence of valid method calls is required to create a test. To\nachieve reasonable coverage, a set of test inputs must be created\n\nthat explores the valid states. It is, however, challenging to create\nvalid test inputs, as we have shown on example before. Specific\ncalls must occur in a specific order with specific arguments. Spe\ncific relationships must be maintained between arguments. The\nsearch space of possible test inputs grows exponentially with the\nnumber of methods, possible arguments, and the length of the\nmethod sequence required. For example, if we presume 5 possible\ngraph objects, 5 possible node objects, and a method sequence 10\nelements in length, there are 23510 different possible sequences.\nAn exhaustive search may be prohibitively expensive. Random\nexploration may not create interesting valid test inputs in a reason\nable amount of time. Specifications may limit the search space,\nbut specifications are often not available. Of course, nontrivial\nprograms contain groups of interrelated classes with many more\npossible test inputs and more complex constraints than the ones\nshown in the example. We discuss a more complex example in\nSection 3.2.\n1.2 Creating Test Inputs by Example\nOur approach builds valid test inputs for classes with complex\nrules and interrelationships by using information collected during\nan example program run to guide the construction of test inputs.\nA run of a program will use the classes of the program in a nor\nmal fashion. Valid sequences of method calls will be made on the\nvarious instances of each class. Arguments to those calls will be\nvalid as well. For example, the main method in Figure 1 makes a\nsequence of valid method calls on Graph and Node.\nUsing information from the program run, the technique builds\nan invocation model for each class of interest. An invocation model\ndescribes the sequences of mutator method calls and arguments\nthat occurred for the class. For example, the model for Node\n(shown in Figure 4) indicates that setOwner must be called be\nfore addEdge. The technique uses the model to build test inputs.\n1.3 Regression Oracles via Observer Methods\nOnce a valid test input is created, the technique creates an ora\ncle using a set of userspecified observer methods. The technique\nexecutes the test input (using the current version of the program)\nand then calls each observer. It creates an assertion using the result\nfrom the observer. Those assertions are combined with the test in\nput to form a complete test. The complete test can be run on later\nversions of the program to ensure that the behavior did not change.\nFigure 2 shows two sample tests generated by an implementa\ntion of our technique. Each executes a valid test input and checks\nthe result using the getDegree observer method. These are not\nthe same executions as in the main method. The invocation model\nguides the construction of test inputs, but does not limit them to\nonly the ones in the original trace.\n1.4 Contributions\nIn summary, we make the following contributions:\n- We present invocation models, which describe valid sequences\nof method calls and their parameters, and a technique that\ndynamically infers them by tracking method calls in a pro\ngram run.\n- We use observer methods to create an oracle for regression\ntesting.\n1 public class Main {\npublic static void main(String[] args) {\nGraph g1 = Graph.generateGraph();\nNode n1 = new Node(\"NYC\");\nn1.setOwner(g1);\nNode n2 = new Node(\"Boston\");\nn2.setOwner(g1);\ng1.addNode(n1);\ng1.addNode(n2);\nn1.addEdge(n2);\nn1.getDegree();\n}\n13 }\n15 public class Graph {\nprivate Map<Node, Set<Node>> adjMap;\npublic static Graph generateGraph() {\nGraph g = new Graph();\ng.init();\nreturn g;\n}\npublic void init() {\nadjMap = new HashMap<Node, Set<Node>>();\n}\npublic void addNode(Node n) {\nadjMap.put(n, new HashSet<Node>());\n}\npublic void addEdge(Node n1, Node n2) {\naddToNode(n1,n2);\naddToNode(n2,n1);\n}\nprivate void addToNode(Node source, Node\nSet<Node> succ = adjMap.get(source);\nsucc.add(target);\n}\npublic int getDegree(Node n) {\nreturn adjMap.get(n).size();\n}\n44 }\n46 public class Node {\nprivate Graph owner;\nprivate String name;\npublic Node(String name) {\nthis.name = name;\n}\npublic void setOwner(Graph a) {\nthis.owner = a;\n}\npublic void addEdge(Node n) {\nowner.addEdge(this, n);\n}\npublic int getDegree() {\nreturn owner.getDegree(this);\n}\n62 }\nFigure 1. Example program. The following implicit rules must be followed\nto create a valid Graph: Graph.init() must be called before any of\nthe other methods of Graph; Node.setOwner() must be called before\nany of the other methods of Node; Graph.addNode() must be called\nfor a node before Graph.addEdge() is called for that node.\ntarget) {\n\npublic void test13() {\nNode var327 = new Node(\"NYC\");\nGraph var328 = new Graph();\nvar327.setOwner(var328);\nvar328.init();\nvar328.addNode(var327);\nvar327.addEdge(var327);\nassertEquals(1, var327.getDegree());\n}\npublic void test22() {\nGraph var494 = Graph.generateGraph();\nNode var496 = new Node(\"Boston\");\nGraph var497 = new Graph();\nvar496.setOwner(var497);\nvar497.init();\nvar494.addNode(var496);\nvar494.addNode(var496);\nvar497.addNode(var496);\nvar494.addEdge(var496, var496);\nassertEquals(0, var496.getDegree());\n}\nFigure 2. Two test cases from the regression suite generated by Palulu for\nthe classes in Figure 1. Each of these follows the implicit rules for Graph\nand Node.\nrun\nmodel\ngenerator\nregression\ntest\ngenerator\ntrace\nregression\nsuite\nmodel\nprogram\npurity information\nFigure 3. Technique overview. The inputs to the technique (left) are (1)\na program under test and (2) a concrete execution of the program. The\noutput (right) is a regression suite.\n- We evaluate the effectiveness of our technique by compar\ning the regression tests it generates with (1) random input\ngeneration that does not use an invocation model, and (2)\nmanuallywritten regression tests.\n- We perform a case of study of our technique's ability to gen\nerate complex test inputs.\nThe remainder of the paper is organized as follows. Section 2\ndescribes our technique. Section 3 contains the description and\nresult of the experiments we performed to evaluate Palulu. Finally,\nSection 5 surveys related work.\n2. Technique\nFigure 3 shows a highlevel view of the technique. Given the\nprogram and an example run, the technique automatically creates\nregression unit tests for selected classes in the program. The tech\nnique has four steps.\n1. Trace method calls. Execute an instrumented version of the\nprogram that traces the method calls (and parameters/return\nvalues to the calls) that occur during the execution. Tracing\nis described in Section 2.1.\n2. Infer a model. Use the trace to generate a model that ab\nstracts the proper uses of the program's components. Model\ninference is described in Section 2.2.\n1 Graph.generateGraph() > g1\nnew Graph() > g1\ng1.init()\nedges = new HashMap()\n5 new Node(\"NYC\":String) > n1\n6 n1.setOwner(Graph)\n7 new Node(\"Boston\":String) > n2\n8 n2.setOwner(Graph)\n9 g1.addNode(n1)\nnew HashSet() > hs1\nedges.put(n1,hs1) >\nnull\n12 g1.addNode(n2)\nnew HashSet() > hs1\nedges.put(n2,hs2) >\nnull\n15 n1.addEdge(n2)\ng1.addEdge(n1,n2)\ng1.addToNode(n1,n2)\nedges.get(n1) >\nhs1\nhs1.add(n2) >\ntrue\ng1.addToNode(n2,n1)\nedges.get(n2) >\nhs2\nhs1.add(n1) >\ntrue\n23 n1.getDegree() > 1\ng1.getDegree(n1) >\nedges.get(n1) >\nhs1\nhs1.size() > 1\nFigure 5. The simplified trace created by running the example's main\nmethod (line 2 in Figure 1) using our tracing tool. The right hand side\nof the arrow refers to the return value.\n3. Generate test inputs. Use the model to generate method\ncall sequences representing test inputs. This step is described\nin Section 2.3.1.\n4. Create a regression oracle. Execute the inputs and create a\nregression test oracle that captures their behavior. We cap\nture the behavior of each method invocation by recording\nits return value, as well as the state of the receiver and pa\nrameters before and after the method is invoked. To capture\nan object's state, we call its observer methods and record\ntheir return values. Regression oracle creation is described\nin Section 2.3.2.\nThe output of the technique is a test suite, in which each test\ncase consists of a method sequence (test input) and assertions about\nthe state of the objects at each point during execution (regression\noracle).\n2.1 Tracing\nTracing collects the history of method invocations. Figure 5\nshows the trace obtained when running the example program from\nFigure 1. Tracing is implemented by dynamically instrumenting\nclass files as they are loaded. Each method call is instrumented\nat the call site, which allows calls to the JDK to be instrumented\nwithout instrumenting the JDK itself. An entry is written imme\ndiately before and after the call so that it can be determined how\ncalls are nested. The receiver, each argument and the return value\nfor the call are logged. Primitives and strings are logged with their\nvalue. Objects are logged with a unique ID.\n2.2 Invocation Model Inference\nThe trace contains many legal sequences of calls. In fact the\ncalls involving each object contain legal call sequences on instances\n\nFigure 4. Invocation models for the Graph (left) and Node (right) from Figure 1.\nof the object's class. We use the term object history to denote the\nsubtrace of calls that involve an object. Object histories follow\nthe implicit rules of the classes. Our technique used those rules to\ncreate test cases. For objects of a given class, many histories may\nexist. However, many of them are likely to share prefixes of calls.\nOur technique summaries object histories into invocation models,\nto use this sharing.\nEach invocation model is a directed graph. One model is cre\nated for each class for which instances are found in the trace1. The\nnodes in each graph are method signatures and an edge between\ntwo nodes indicate that the source method should be called before\nthe target method on an instance of the class.\nFigure 4 shows invocation models for the Graph and Node\nclasses from the example in Figure 1. The models capture many\nof the implicit rules of those classes. For example, we noted in the\nintroduction that Graph.init must be called before a node can\nbe added to the graph by Graph.addNode. This is visible from\nthe model of class Graph on the lefthandside in Figure 4: the\nnode for Graph.init precedes the node for Graph.addNode\nin the graph.\nThe algorithm for inferring invocation models has three steps:\n1. Extract object histories from the trace (Section 2.2.1).\n2. Filter calls from the histories (Section 2.2.2)\n3. Create a model for each class by merging all the histories of\ninstances of this class (Section 2.2.3).\n2.2.1 Extracting Object Histories\nThe first step in the algorithm for inferring invocation models is\nto extract object histories for all the objects observed in the trace.\nAn object's history consists of all invocations in the trace that in\ncludes the object as a receiver, parameter, or return value. An\n1The user can specify a subset of those classes.\nobject's history describes the sequence of observable operations\nperformed on an object. An object can take part in an operation\nwithout being explicitly passed as a parameter; our technique does\nnot currently track this information.\nFigure 6 shows the histories of the Graph and Node instances\nderived from the example trace. The indentation serves to indicate\nmethod call nesting.\n2.2.2 Filtering Calls from Object Histories\nThe second step of the algorithm that infers invocation models\nis applying the following three filters on each object's history:\nNonMutators Filter Since nonmutator method calls do\n-\nnot change the object's state they are less important in test\ncases creation. This filter removes all calls to nonmutator\nmethods as well as any call nested inside them (which should\nalso be a nonmutator). The technique uses argument spe\ncific nonmutator information. A method is nonmutator\nover an argument/receiver if it does not modify any (tran\nsitively reachable) fields in that argument/receiver. It is a\nnonmutator over a return value if the return value is not\ncreated within the method or if it has no nested nonmutator\nmethod calls. Nonmutators can either be supplied by the\nuser or computed via static analysis [12, 10]. Figure 7 con\ntains the list of nonmutators in the example classes from\nFigure 1.\nAn additional reason for removing calls to nonmutator meth\nods is optimization. We found out in one of our experi\nments2 that filtering nonmutators removed 73% of all calls.\nThis filtering helps creating more manageable models.\nReturn Value Filter This filter removes a call from an ob\n-\nject history if the object is only the return value of the call.\n2This observation was made in the experiment described in Sec\ntion 3.1.\n\nHistory for the Graph instances:\n1 new Node(\"NYC\") > n1\n1 Graph.generateGraph() > g1\n2 n1.setOwner(g1)\nnew Graph() > g1\n3 g1.addNode(n1)\ng1.init()\n4 n1.addEdge(n2)\n4 n1.setOwner(g1)\ng1.addEdge(n1,n2)\n5 n2.setOwner(g1)\n6 g1.addNode(n1)\n1 new Node(\"Boston\") > n2\n7 g1.addNode(n2)\n2 n2.setOwner(g1)\ng1.addEdge(n1,n2)\n3 g1.addNode(n2)\ng1.addToNode(n1,n2)\n4 n1.addEdge(n2)\ng1.addToNode(n2,n1)\ng1.addEdge(n1,n2)\n11 g1.getDegree(n1) > 2\nHistory for one of the Node instances:\nFigure 8. Object histories for instances of the Node, after filtering.\n1 new Node(\"NYC\") > n1\n2 n1.setOwner(g1)\nFigure 4 contains the models inferred from the trace of Fig\n3 g1.addNode(n1)\nure 5. The model for each class is a directed graph where nodes\nedges.put(n1, hs1)\nare method calls and edges define permitted order of method calls.\n5 n1.addEdge(n2)\ng1.addEdge(n1,n2)\nNodes also contain the position of the object in the call. The source\ng1.addToNode(n1,n2)\nnodes (nodes with no incoming edges) are methods that are used\nedges.get(n1) > hs1\nto create new instances (constructors and static factories).\ng1.addToNode(n2,n1)\nWhen it adds a history to the model, the algorithm reuses nodes\nhs1.add(n1)\nand edges from an existing prefix of calls (up to variable renam\n11 n1.getDegree() > 1\ning) and creates new nodes and edges if no shared prefix exists.\ng1.getDegree(n1) > 1\nedges.get(n1) > hs1\nWhen searching for an existing prefix, nodes (method calls) and\nobject positions are compared. In addition, whenever a primitive\nor a String parameter is encountered, its value is stored. Thus,\nFigure 6. Simplified object histories for instances of Graph and Node\nprimitive values are directly embedded on a perparameter basis.\nobtained from the trace.\nNode.getDegree() : receiver\nMap.put(Object,Object) : parameters\nMap.get(Object) : receiver, parameters\nSet.add(Object) : parameters\nFigure 7. List of nonmutator methods in the classes in Figure 1. Each\nmethod is followed by a list of arguments that it does not mutate.\nAssuming all field writes are done through setters (method that\ndo not call any other methods), the sequence of method calls nested\nin each method call is an alternative, legal sequence for the same\nobject. The algorithm is concerned with legal call sequences. There\nfore, it adds, as alternative paths to each call, all the paths that are\ncreated from considering the calls nested in it (if any exist). For ex\nample, consider the method Graph.generateGraph (line 18\nin Figure 1). This method calls two other methods: Graph con\nstructor and the Graph.init method. The model for Graph\nThe only exception to this filter is if the object is constructed\n(left in Figure 4 contains two paths from the source nodes to the\ninside the call. Those calls are either constructors or static\nGraph.addNode method. One path is through the method Graph.generateGraph\nfactory method like the Graph.generateGraph() (first\nand the second path is through its nested events.\nline in Figure 6) that can be used to create instances.\n2.3 Generating Test Cases\n- NonPublic Filter This filter removes calls to nonpublic\nThe test case generator takes as input:\nmethods. This filter is applied, because nonpublic methods\ncannot be called from a test.\na. An invocation model for the classes in the tested program P.\nOur technique assumes that methods performing fieldwrites\nb. A list of tested classes: a subset of the classes in P to generate\ndo not call other methods. This can be easily achieved by re\ntest cases for.\nplacing all fieldwrites with set methods.\nc. For each tested class, a list of observer methods: pure methods\nIf a nonpublic set method is nested in a filtered call, then\nthat return a value [12, 10]. As examples, methods Node.getDegree\nfiltering out nonpublic method calls can leave the object is\nand Graph.getDegree (in lines 41 and 59 of Figure 1, re\nan unknown state. In this case, the technique filters all calls\nspectively) are observer methods. The list can be a subset of the\nafter the call to the set method (in the same method).\npurity list provided to the model inferencer. (Our implementa\nFigure 8 shows the filtered object histories for the instances of\ntion restricts observer methods to be parameterless methods\nclass Node.\nthat return a primitive, which avoids the difficulties of provid\ning parameters to the observer methods and of recording a re\n2.2.3 Merging Objects Histories into Models\nturn value which is an object. Addressing these difficulties is\nThe last step of the model inference algorithm combines the\nfuture work.)\nhistories of all instances of a class to create the class's model. For\nd. A time limit.\neach class, the algorithm starts from an empty model and incre\nmentally incorporates the histories of all instances of that class to\nThe test case generator iterates through the tested classes. At\nthe class's model.\neach iteration, it tries to create a test case for the currently tested\n\nclass C. The process stops when the time limit is exhausted. A test\ncase consists of a test input and an oracle; Section 2.3.1 describes\ntest inputs creation, and Section 2.3.2 describes regression oracle\ncreation.\n2.3.1 Generating Test Inputs\nIn the context of this paper, a test input for class C is a se\nquence of method calls that create, and possibly mutate, an object.\nof class C. For each test input, we also keep track of the list of\nobjects contained in the test input, and the current state (from the\ninvocation model) of each object. Figure 9 shows a test input and\nthe state of each of its objects.\nThe test input generator maintains a database of previously\ncreated inputs (i.e., method sequences). The database is initially\nempty. To create a new test input, the generator retrieves from\nthe database uniformly at random an existing input that contains at\nleast one object o of type C (the base case is the empty sequence).\nNext, the generator extends the test input by appending a method\ncall allowed by the model for object o. If there are several possi\nble method calls, the generator creates several new inputs, one per\ncall.\nAs Figure 4 shows, the model specifies what some of the pa\nrameters to the method call are. For the parameters that the model\ndoes not specify, the generator finds parameters to the call using\nthe following algorithm.\n1. If the parameter is a primitive, randomly select a primitive\nvalue from the possible values specified in the model.\n2. If the parameter is of type Ch, an instance of that type must\nbe found. If there is already an instance of Ch in the se\nquence, use that instance. If not, one must be created. If Ch\nappears more than once in the parameter list, do not reuse\nthe same instance.\n3. If there is no applicable reference, select one of two actions\n(at random):\na. recursively try to create an input for type Ch, or\nb. retrieve an existing input from the database that creates\nan object of the desired parameter type. Call this a helper\ninput.\n4. Append the helper input to the end of the sequence of calls;\nthe sequence now contains an object of type Ch. Go to step\n3.\nWhen all parameters are filled, the test input is executed3. If\nexecution leads to an exception, the input is discarded. Our tech\nnique conservatively assumes that exceptional behavior is indica\ntive of an illegal state, and does not attempt to create regression\ntests out of such states (recall that the model is incomplete; this\nincompleteness can lead to creating illegal inputs, even when the\nmodel is followed).\nIf the test input executes normally, a regression oracle is created\nfor it, as described in the next section. In addition, the input is\nadded to the database, where it can later be selected to be extended,\nor used as a helper input.\n3In Palulu, we use Java's reflection mechanism to execute inputs.\n2.3.2 Creating a Regression Oracle\nThe regression oracle creation takes as input a sequence, and\nthe runtime objects resulting from its execution. For each object,\nit calls all observer methods specified for the class of the object,\nand records their return values. Each invocation of an observer\nmethod can be converted into an assertion in a straighforward way.\nThe sequence together with the assertions comprise a unit regres\nsion test. Figure 2 contains examples of test unit regression tests\nfor classes in Figure 1.\n3. Evaluation\nIn this Section, we present the empirical evaluation of:\n- the effectiveness of using observer methods in creating re\ngression oracles (the experiment is described in Section 3.1),\nand\n- the effectiveness of our technique in creating complex test\ninputs (the experiment is described in Section 3.2).\n3.1 Observer Methods Experiment\nIn this experiment, we evaluated how the usage of test oracles\nbased on observer methods affects the errordetection effective\nness of generated test suites. We used RatPoly, an assignment\ngiven in the MIT's software engineering class (6.170). RatPoly\nhas the following salient characteristics:\n- It has many different versions, each implementing the same\ninterface.\n- One of the versions of the program is a reference implemen\ntation, to which we can compare all the other versions. We\nmake the simplifying assumption that the reference imple\nmentation contains no errors. It is justified by our goal of\ncreating regression errors, which reveal inconsistencies be\ntween versions of the program.\nSome of the other versions contain errors. This is known be\n-\ncause a manuallywritten test suite that reveals those errors\nis available.\nThe students were given a set of interfaces defining a one\nvariable rationalnumbered coefficient polynomial library, and a\ntest suite written using those interfaces. Their task was to submit\nan implementation of the interfaces that passed the test suite. (Not\nall students turned in assignments that passed the test suite, despite\nthe fact that they were given the suite as part of the assignment.)\nThe reference implementation (to which the students did not have\naccess), and the manuallywritten test suite were written by the\ncourse staff.\nWe created the suite that we used in our experiments, which\nwe call in this paper the Gold suite, by enhancing the staffwritten\nsuite with tests that were generated by our techniques and that we\nhave manually verified to be faultrevealing. We also present, for\nreference, the results of running the original, staffwritten test suite\nand compare them to tests suites generated automatically by our\ntechniques.\nRecall that our techniques require an example execution to cre\nate test suites. In this experiment, we used a 3line program that\nperforms a sequence of 9 simple operations on polynomials. We\nintentionally used a short example program to see if our technique\n\nline\nsequence\nobjects and states after each line\nNode var1 = new Node(\"NYC\");\nvar1 (state ss6)\nGraph var2 = new Graph();\nvar1 (state ss6), var2 (state ss4)\nvar1.setOwner(var2);\nvar1 (state ss7), var2 (state ss4)\nvar2.init();\nvar1 (state ss7), var2 (state ss5)\nFigure 9. A test input derived from models in Figure 4. The input has 4 lines and creates two objects, a graph and a node. The graph and node go through\ndifferent states of the model. The state names refer to node labels in the graphs in Figure 4.\ncould create useful tests even with a small amount of runtime\ndata. The trace collected was over this small example and the ref\nerence (staffprovided) implementation.\nWe used and compared 6 different test generation techniques.\nThey were a combination of 3 input generation strategies and 2\noracle creation strategies. The input generation strategies were:\na. Unconstrained: strategy that allows invocation of any method\nat any time with any parameter (of a valid type).\nb. Primitive constraints: strategy that allows invocation of any\nmethod at any time but the constrains the arguments to those\nseen in the example execution.\nc. Primitive and method sequencing constraints: strategy based\non a model that uses method sequencing constraints, such as\nthose discussed in Section 2.3.1.\nThe oracle creation strategies were:\na. Exceptions: a test suite fails if executing a test case in it leads\nto an exception being thrown (recall from Section 2.3.1 that we\ndo not use tests inputs that lead to exceptions in the original\nprogram).\nb. Exceptions+observers: a test suite fails if executing a test case\nin it leads to an exception being thrown. Additionally, the suite\nfails if a call to an observer method returns a value that is dif\nferent than in the reference program (when run on the same\ninput).\nWe repeated the following experimental process for each of the\ntest generation techniques:\n1. Execute the example program on the reference implementa\ntion to create the trace (as described in Section 2.1). The Un\nconstrained input generation strategy does not require trac\ning.\n2. Create the model required for test input generation.\n3. Generate the test suite using the model and the reference im\nplementation. We bounded the execution time of this phase\nto 7 seconds.\n4. Run the test suite on each of the 143 student programs for\nRatPoly and record the result. The result is either pass or\nfail. The pass result is given only when each test case in the\ntest suite succeeds.\n5. Compare the result to those of the Gold suite, for each stu\ndent program. There are four possible outcomes (summa\nrized in Figure 10):\na. Both the generated suite and the Gold suite fail. In this\ncase the generated suite correctly identifies an error in\nthe program.\nGold passes\nGold fails\ngenerated passes\nOK Pass\nFalse Negative\ngenerated fails\nFalse Positive\nOK Fail\nFigure 10. Four possible outcomes of running a generated test suite and\nGold test suite on a program.\nb. Both the generated the Gold suite pass. In this case, the\ngenerated suite correctly identifies a nonerroneous pro\ngram.\nc. The generated suite passes, but the Gold suite fails. In\nthis case, the generated suite misses an error (i.e., it is a\nfalse negative).\nd. The generated suite fails, but the Gold suite passes. In\nthis case, the reported a false error (i.e., it is a false posi\ntive).\n3.1.1 Results\nFigures 11 and 12 present the results of the RatPoly experi\nment. The experiment was performed on a Pentium 4 machine,\nrunning Debian Linux, with 3.6 GHz CPU; the heap space was\nlimited to 1 GB. Rows in the tables contain results for the 6 differ\nent generated tests suites. Additionally, we include, for reference,\nresults for the original, staffwritten test suite.\nColumns in Figure 11 indicate, in order: the suite's name, trac\ning time, modelling time, test generation time, number of created\nsequences and test running time (averaged over all students).\nThe first 4 columns in Figure 12 show the results of execut\ning the test suites on student programs. Let T denote the test\nsuite. Then, OKPass(T) is the number of students for which\nboth T and the Gold test suite pass. Similarly, OKFail(T) is\nthe number of student programs for which both test suites fail\nand FalsePositives(T) is the number of programs for which the\nGold suite passes and T fails. Finally, FalseNegatives(T) is the\nnumber of programs for which the Gold suite fails and T passes.\nThe last two columns indicate precision and recall and are cal\nculated as follows:\n- Error Detection Recall measures how often the generated\ntest suite was successful in detecting an error.\nOKFail(T)\nRecall(T) = FalseNegatives(T) + OKFail(T)\nError Detection Precision measures how often errors re\n-\nported by the generated test suite were actual errors.\nOKFail(T)\nPrecision(T) = FalsePositives(T) + OKFail(T)\n\nTest Suite\nTracing time\nModelling time\nTest gen time\n# sequences\nTest run time avg\nUnconstrained\nn/a\nn/a\n7.00\nUnconstrained + observers\nn/a\nn/a\n10.90\nPrimitive constraints\n2.9\n2.6\n5.39\nPrimitive constraints + observers\n2.9\n2.6\n5.95\nMethod seq constraints\n2.9\n2.6\n8.25\nMethod seq constraints + observers\n2.9\n2.6\n9.51\nOriginal Manual\nn/a\nn/a\nn/a\n0.08\nFigure 11. Time characteristics of the RatPoly experiment. Times are in seconds.\nTest Suite\nOKPass\nOKFail\nFalsePositives\nFalseNegatives\nPrecision\nRecall\nUnconstrained\n0.95\n0.31\nUnconstrained + observers\n0.98\n0.60\nPrimitive constraints\n0.78\n0.46\nPrimitive constraints + observers\n0.67\n0.84\nMethod seq constraints\n1.00\n0.47\nMethod seq constraints + observers\n0.98\n0.93\nOriginal Manual\n1.00\n0.21\nFigure 12. Results of the RatPoly experiment. The 4 middle columns refer to the possible outcomes of running a test suite, as described in Figure 10.\n3.1.2 Discussion of the results\nResults presented in Figure 12 show that, in this experiment,\n- Using observer methods to create test oracles was effective.\nError detection recall was almost 2 times higher when ob\nserver methods were used (0.31 vs. 0.6, 0.46 vs. 0.84 and\n0.47 vs. 0.93). At the same time, using observers did not\nresult in a significant loss of precision (0.95 vs. 0.98, 0.78\nvs. 0.67 and 1.00 vs. 0.98).\n- Generated tests suites outperformed the manuallywritten\ntest suite in error detection recall (i.e., revealed more er\nrors). The bestperforming generated test suite had recall\n0.93/0.21 = 4.43 times higher than the manual test suite.\nBy manual inspection, we verified that many of the errors\nthat the staffwritten test suite missed were due to incor\nrect handling of special cases (e.g., the zero polynomial) and\nunexpected mutation of objects (The specification required\nthe classes to be immutable. Some student programs failed\nto meet that requirement--our generated test suites found\nsome of those errors.)\n- Using method sequencing constraints was of only limited\neffectiveness. This is visible by comparing the results for\n'Primitive constraints + observers' and 'Method seq con\nstraints + observers'. Method sequencing constraints were\nnot necessary to create tests for this experiment, because the\nobjects were intended to be immutable.\n- Using primitive values found in the execution increased the\neffectiveness of generated test suites, both in recall and pre\ncision. This is visible by comparing the first 2 rows of the ta\nble (i.e., 'Unconstrained' and 'Unconstrained + observers')\nwith the next 2 rows (i.e., 'Primitive constraints' and 'Prim\nitive constraints + observers'). To understand why this was\neffective, consider the RatPoly.parse(String) method.\nA valid argument for this method must represent a polyno\nmial, e.g., \"1/2*xˆ22*x+1\". Random exploration of\nstrings is unlikely to quickly find many valid inputs for this\nmethod. Using the string values observed in the trace nar\nrows the search space.\nClass\nDescription\nReferences\nVarInfoName\nVariable Name\nVarInfo\nVariable Description\nVarInfoName\nPptTopLevel\nPptSlice2\nTwo variables from a pro\ngram point\nVarInfo\nPptTopLevel\nInvariant\nPptTopLevel\nProgram point\nPptSlice2\nVarInfo\nLinearBinary\nLinear Invariant over two\nscalar variables\nPptSlice2\nBinaryCore\nLinear helper class\nLinearBinary\nFigure 13. Some of the classes needed to create a valid test input for\nDaikon's BinaryCore class\n3.2 Complex Inputs Experiment\nTo evaluate our technique's effectiveness in creating legal and\nstructurally complex inputs, we applied it to the BinaryCore\nclass within Daikon [5], a tool that infers program invariants. Bi\nnaryCore is a helper class that calculates whether or not the points\npassed to it form a line. Daikon maintains a complex data struc\nture involving many classes (see Figure 13) to keep track of the\nvalid invariants at each program point. Some of the constraints in\ncreating a valid BinaryCore instance are:\n- The constructor to a BinaryCore takes an object of type\nInvariant, which has to be of runtime type LinearBinary\nor PairwiseLinearBinary, subclasses of Invariant.\nDaikon contains dozens of classes that extend Invariant,\nso the state space of incorrect but typecompatible possibil\nities is very large.\n- To create a legal LinearBinary, one must first create a\nlegal PptTopLevel and a legal PptSlice2. Both of\nthese classes require an array of VarInfo objects. In addi\ntion, the constructor for PptTopLevel requires a string in\na specific format; in Daikon, this string is read from a line\nin the input file.\n- The constructor to VarInfo takes five objects of different\ntypes. Similar to PptTopLevel, these objects come from\nconstructors that take speciallyformatted strings.\n\n- None of the parameters involved in creating a BinaryCore\nor any of its helper classes can be null.\n4. BinaryCore: Manual vs. Generated\nFigure 14 shows a test input that creates a BinaryCore ob\nject. This test was written by a Daikon developer, who spent about\n30 minutes writing the test input.\nWe used our technique to generate test inputs for BinaryCore.\nWe gave the input generator a time limit of 10 seconds. Dur\ning this time, it generated 3 sequences that create BinaryCore\nobjects, and about 150 helper sequences.\nFigure 14 also shows one of the three inputs that Palulu gen\nerated for BinaryCore. For ease of comparison, we renamed\nvariables (Palulu assigned names like var4722 to all variables),\nreordered method calls when the reordering did not affect the re\nsults, and added some whitespaces. Palulu successfully generated\nall the helper classes involved. It generated some objects in a way\nslightly different from the manual input; for example, to generate\na Slice, it used the return value of a method in PptTopLevel\ninstead of the class's constructor.\nWithout an invocation model, an input generation technique\nwould have little chance of generating this sequence; the specific\nprimitive parameters, the fact that a BinaryCore requires a LinearBinary\nnot just any Invariant, are all crucial pieces of information\nwithout which a search through the space of possibilities would be\ninfeasible. Moreover, the path to a legal BinaryCore is highly\nconstrained: there is not an easier way to obtain a BinaryCore.\n5. Related Work\nThere is a large literature on automatic test generation; the most\nrelevant to our work centers around what we call the method se\nquence generation problem: generating sequences of method calls\nthat create and mutate objects for use as test inputs. Our survey\ncenters around three topics: (1) modeling legal method sequences,\n(2) generating method sequences from the models, and (3) creating\nregression oracles for the sequences.\n5.1 Modeling Legal Method Sequences\nIn this section, we survey techniques that automatically extract\nfinite state machines (FSMs) from a program trace [3, 14, 9]. In\nthese techniques, as in ours, nodes represent method calls, and a\ntransition from node m1 to node m2 means that a call of m1 can\nbe followed by a call of m2.\nCook and Wolf [3] generate FSMs for software processes. They\nconsider only linear traces of atomic, parameterfree events (an\nevent is just a label), which allows them to directly apply and ex\ntend existing grammarinference methods [1]. Their focus is on\nevaluating the effectiveness of different inference methods in lead\ning to a model that a human deems reasonable (for example, one of\ntheir case studies characterizes differences in the models inferred\nfor a correct and an incorrect modification to a software artifact).\nThe grammarinference techniques that they use are not directly\napplicable to objectoriented programs, which produce nested--\nnot linear--traces, and where an event represents a method call,\nwhich takes multiple parameters and affects more than one object.\nOur modeling technique addresses the more complex traces\nthat arise from modeling objectoriented systems. We deal with\nnested calls by creating alternate paths in an FSM and cutting off\nevents for nested private calls. We deal with multipleparameter\nevents by inferring an FSM over a set of singleobject histories,\nwhich consist of calls that affected a single object o, ignoring the\nidentity of all other objects participating in the calls.\nWhaley et al. [14] and Meghani et al. [9] also infer method\nsequence FSMs for objectoriented systems; they also ignore im\nportant modeling issues such as parameters (other than the re\nceiver) and nested calls, which we address. Whaley and Lam [14]\nuse a dynamic analysis of a program run to records observed legal\nmethod sequences, and a static analysis that infers pairs of meth\nods that cannot be called consecutively. Meghani and Ernst [9]\nimprove on Whaley's technique by using a program trace to dy\nnamically infer pre/postconditions on methods, and create a tran\nsition from m1 to m2 when the preconditions of m2 are consistent\nwith the postconditions of m1. The main motivation (and evalua\ntion) in both works is a better understanding of the system being\nmodeled; they do not use their models to generate test inputs. Our\nmain goal in deriving a model is to generate test inputs; this led us\nto augment our models with additional information such as primi\ntive parameters.\n5.2 Generating Test Inputs\nPrevious work on generating inputs from a specification of le\n,\ngal method sequences[2, 4, 8, 13] expect the user to write a model\nby hand. These techniques have been tested largely on toy exam\nples, such as linked lists, stacks, etc. that, in addition to testing\nsmall structures, have few or no methods that take other objects as\nparameters. This makes input generation easier--there are fewer\ndecisions to make, such as how to create an object to pass as a\nparameter.\nSince we use an automaticallygenerated model and apply our\ntechnique to realistic applications, our test input generator must\naccount for any lack of information in the generation model and\nstill be able to generate inputs for complex data structures. Ran\ndomization helps here: whenever the generator faces a decision\n(typically due to underspecification in the generated model), a\nrandom choice is made. As our evaluation shows, the randomized\napproach leads to legal inputs. Of course, this process can also lead\nto illegal structures being created. In future work, we would like\nto investigate techniques that can help us detect illegal structures\nresulting from generating inputs using an underspecified model.\n5.3 Capturing Behavior\nSeveral techniques have been proposed to capture a program's\nintended behavior from an execution trace, ranging from deriv\ning operational abstractions [5, 6], to algebraic specifications [7].\nThese techniques aim to generalize the trace into properties that\nare (with reasonable confidence) invariant across different pro\ngram runs. Daikon [5], for example, infers program invariants\nfrom a program trace.\nOur technique differs in that it does not attempt to derive gen\neralizable properties, but simply records specific observed values\nfor each input, using a predefined set of observer methods (we do\nnot address the problem of finding observer methods, but we could\nuse developed static analyses [12, 10]). The danger in capturing\nobservations specific to one input is that they may reflect imple\nmentation details that are not important for correctness. Looking\nat the observations created for both RatPoly and Daikon, we found\nthem to be close to what a programmer would have written as an\noracle. Augmenting our inputspecific oracles with dynamically\n\ninferred invariants may lead to better oracles, and is a topic of\nfuture work.\n5.4 Creating Unit Tests from System Tests\nAs mentioned in the last section our technique captures exhib\nited behavior at the unit level. Our technique can be used to create\na set of unit tests from a large (potentially longrunning) system\ntest. Another way to think of this process is as a refactoring of the\nsystem test into a set of smaller unit tests.\nSaff et al. propose tests factoring [11] as a different technique\nto create unit tests from a system test. Test factoring captures the\ninteractions between tested and untested components in the sys\ntem, and creates mock objects for the untested portions of the in\nteraction, yielding a unit test where the environment is simulated\nvia mocks. Test factoring accurately reproduces the execution of\nthe entire system test. Our technique, on the other hand, uses the\nsystem test as only as a guide to create new sequences of method\ncalls. Both techniques share the goal of creating focused, fast unit\ntests where there were none.\nTest factoring creates tests that exactly mimic the original exe\ncution, so it never creates illegal method sequences; due to the in\ncompleteness in the model,our technique can create illegal method\nsequences. On the other hand, our technique has several advan\ntages over test factoring:\n- Tests are selfcontained. The end result of our technique\nis a compilable JUnit test suite. Test factoring creates mock\nobjects and requires a special infrastructure to rerun a fac\ntored test.\n- The technique creates a regression oracle; test factoring\nmust be provided one. Test factoring takes a system test\nthat already has an oracle, and makes a unit test case out of\nit, where the oracle is the same as the original. Our tech\nnique creates a regression oracle automatically.\n- The technique explores behavior beyond the system test.\nOur model loses much of the information contained in the\nsystem test, which leads to generating objects that are likely\nto differ from those seen in the system test. Test factoring\nexactly mimics the system test, so it will catch no more (or\nless) than the errors caught by the original test. Our tech\nnique's ability to create tests that explore behaviors not ex\nplored in the system test means that our technique can de\ntect errors that the system test missed. On the flip side, our\ntechnique may also miss errors that the original test caught;\nan experimental evaluation is necessary to determine how\nmany errors we miss that the original test catches, and vice\nversa.\n6. Conclusion\nWe have presented a technique that automatically generates re\ngression test suites at the unit level. To generate test inputs, the\ntechnique creates an invocation model from an example run of\nthe program, and uses the model to generate legal, structurally\ncomplex inputs that would be difficult to create without knowl\nedge of the program's operation. For each test input, it creates a\nregression oracle by capturing the state of objects using observer\nmethods. Our experiments suggest that the resulting regression\ntests are capable of revealing errors, including errors not caught\nby an extensive test suite. Our experiments also demonstrate the\nability of the technique to generates structurallycomplex inputs\nfor real applications.\n7. References\n[1] D. Angluin and C. H. Smith. Inductive inference: Theory\nand methods. ACM Computing Surveys, 15(3):237-269,\nSept. 1983.\n[2] H. Y. Chen, T. H. Tse, and T. Y. Chen. Taccle: a\nmethodology for objectoriented software testing at the class\nand cluster levels. ACM Trans. Softw. Eng. Methodol.,\n10(1):56-109, 2001.\n[3] J. E. Cook and A. L. Wolf. Discovering models of software\nprocesses from eventbased data. ACM Transactions on\nSoftware Engineering and Methodology, 7(3):215-249, July\n1998.\n[4] R.K. Doong and P. G. Frankl. Case studies on testing\nobjectoriented programs. In TAV4: Proceedings of the\nsymposium on Testing, analysis, and verification, pages\n165-177, New York, NY, USA, 1991. ACM Press.\n[5] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin.\nDynamically discovering likely program invariants to\nsupport program evolution. IEEE Transactions on Software\nEngineering, 27(2):99-123, Feb. 2001. A previous version\nappeared in ICSE '99, Proceedings of the 21st International\nConference on Software Engineering, pages 213-224, Los\nAngeles, CA, USA, May 19-21, 1999.\n[6] S. Hangal and M. S. Lam. Tracking down software bugs\nusing automatic anomaly detection. In ICSE'02,\nProceedings of the 24th International Conference on\nSoftware Engineering, pages 291-301, Orlando, Florida,\nMay 22-24, 2002.\n[7] J. Henkel and A. Diwan. Discovering algebraic\nspecifications from java classes. In L. Cardelli, editor,\nECOOP 2003 ObjectOriented Programming, 17th\nEuropean Conference, Darmstadt, July 2003. Springer.\n[8] D. Hoffman and P. Strooper. Classbench: A methodology\nand framework for automated class testing, 1997.\n[9] S. V. Meghani and M. D. Ernst. Determining legal method\ncall sequences in object interfaces, May 2003.\n[10] A. Rountev and B. G. Ryder. Pointsto and sideeffect\nanalyses for programs built with precompiled libraries.\nLecture Notes in Computer Science, 2027:20+, 2001.\n[11] D. Saff, S. Artzi, J. H. Perkins, and M. D. Ernst. Automatic\ntest factoring for Java. In ASE 2005: Proceedings of the 21st\nAnnual International Conference on Automated Software\nEngineering, pages 114-123, Long Beach, CA, USA,\nNov. 9-11, 2005.\n[12] A. S alcianu and M. C. Rinard. Purity and side effect analysis\nfor java programs. In VMCAI, pages 199-215, 2005.\n[13] C. D. Turner and D. J. Robson. The statebased testing of\nobjectoriented programs. In ICSM '93: Proceedings of the\nConference on Software Maintenance, pages 302-310,\nWashington, DC, USA, 1993. IEEE Computer Society.\n[14] J. Whaley, M. Martin, and M. Lam. Automatic extraction of\nobjectoriented component interfaces. In ISSTA 2002,\nProceedings of the 2002 International Symposium on\nSoftware Testing and Analysis, pages 218-228, Rome, Italy,\nJuly 22-24, 2002.\n\nManuallywritten test input (written by an expert)\nVarInfoName namex = VarInfoName.parse (\"x\");\nVarInfoName namey = VarInfoName.parse (\"y\");\nVarInfoName namez = VarInfoName.parse (\"z\");\nProglangType inttype = ProglangType.parse (\"int\");\nProglangType filereptype = ProglangType.repparse (\"int\");\nProglangType reptype = filereptype.fileTypeToRepType();\nVarInfoAux aux = VarInfoAux.parse (\"\");\nVarComparability comp = VarComparability.parse (0, \"22\", inttype);\nVarInfo v1 = new VarInfo (namex, inttype, reptype, comp, aux);\nVarInfo v2 = new VarInfo (namey, inttype, reptype, comp, aux);\nVarInfo v3 = new VarInfo (namez, inttype, reptype, comp, aux);\nVarInfo[] slicevis = new VarInfo[] {v1, v2};\nVarInfo[] pptvis = new VarInfo[] {v1, v2, v3};\nPptTopLevel ppt = new PptTopLevel\n(\"DataStructures.StackAr.StackAr(int):::EXIT33\", pptvis);\nPptSlice2 slice = new PptSlice2 (ppt, slicevis);\nInvariant proto = LinearBinary.getproto();\nInvariant inv = proto.instantiate (slice);\nBinaryCore core = new BinaryCore (inv);\ncore.addmodified (1, 1, 1);\ncore.addmodified (2, 2, 1);\ncore.addmodified (3, 3, 1);\ncore.addmodified (4, 4, 1);\ncore.addmodified (5, 5, 1);\nPalulugenerated test input (formatted for ease of comparison)\nVarInfoName name1 = VarInfoName.parse(\"return\");\nVarInfoName name2 = VarInfoName.parse(\"return\");\nProglangType type1 = ProglangType.parse(\"int\");\nProglangType type2 = ProglangType.parse(\"int\");\nVarInfoAux aux1 = VarInfoAux.parse(\" declaringClassPackageName=DataStructures, \");\nVarInfoAux aux2 = VarInfoAux.parse(\" declaringClassPackageName=DataStructures, \");\nVarComparability comparability1 = VarComparability.parse(0, \"22\", type1);\nVarComparability comparability2 = VarComparability.parse(0, \"22\", type2);\nVarInfo info1 = new VarInfo(name1, type1, type1, comparability1, aux1);\nVarInfo info2 = new VarInfo(name2, type2, type2, comparability2, aux2);\nVarInfo info3 = new VarInfo(info2);\nVarInfo info4 = VarInfo.origVarInfo(info3);\nVarInfo info5 = VarInfo.origVarInfo(info2);\nVarInfo info6 = VarInfo.origVarInfo(info3);\nVarInfo[] infos = new VarInfo[] { info1, info2};\nPptTopLevel ppt1 = new PptTopLevel(\"DataStructures.StackAr.push(java.lang.Object):::EXIT\", infos);\nPptSlice slice1 = ppt1.gettempslice(info1, info2);\nInvariant inv1 = LinearBinary.getproto();\nInvariant inv2 = inv1.instantiate(slice1);\nBinaryCore lbc1 = new BinaryCore(inv2);\nFigure 14. The first code listing is a test input written by a developer of Daikon. It required about 30 minutes to write. The second listing is a test input\ngenerated by our technique. For ease of comparison, we renamed automaticallygenerated variable names and grouped method calls related to each class\n(but we preserved any ordering that affects the results)."
    },
    {
      "category": "Resource",
      "title": "unit_type_infere.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-883-program-analysis-fall-2005/b068226dbde932a842bb87bccf8b7ee3_unit_type_infere.pdf",
      "content": "Annotation-less Unit Type Inference for C\nPhilip Guo and Stephen McCamant\nFinal Project, 6.883: Program Analysis\nDecember 14, 2005\nIntroduction\nTypes in programming languages are crucial for catching errors at compile-time.\nSimilarly, in scientific\napplications, the units system forms a type discipline because a correct equation must necessarily include\nterms with units that are equivalent on both sides. Many scientific and numerically-oriented programs are\nwritten in C, but the language provides no support for attaching units to variables. Thus, programmers\ncannot easily tell whether the units for variables in their programs are consistent. We propose to create an\nanalysis that can infer the unit types of variables in C programs such that the units type discipline holds\nacross all expressions in the program. Unlike previous work which relies on the user to annotate some types\nand then checks the rest for consistency, we propose to infer types with no user annotations at all. This\ncompletely automatic analysis produces a most-general system of units for a program, which include any\nconsistent unit system the programmer may have intended as a special case. This most-general solution can\nthen be specialized interactively by the programmer to give human-readable names to units, a process which\nrequires much less programmer interaction than specifying each variable's units one by one. Our analysis\ncan be used by programmers to find bugs as indicated by the inferred unit types not matching up with their\nintuition and as a starting point for annotating their code with units using an annotation framework.\nThe correct use of physical units and dimensions is an important aspect of correctness for many computer\nprograms. On one hand, many programming errors lead to programs whose results do not have the expected\nunits, analogously to the common experience of students in introductory physics courses. For instance, an\nexperiment performed by Brown [Bro01] showed that checking the units in a short procedure written to\ncalculate a function used in particle physics revealed three separate errors. Conversely, programs with unit\nerrors can harbor subtle bugs. To cite a particularly costly error, an on-ground system used in the navigation\nof the NASA Mars Climate Orbiter spacecraft failed to convert between pound-seconds and newton-seconds\nin calculating the impulse produced by thruster firings, due to a programmer error when updating a program\nused for a previous spacecraft to include the specification of a new model of thruster [EJC01]. This root\ncause, along with inadequate testing, operational failures, and bad luck, eventually led to the loss of the\nspacecraft as it was destroyed in the Mars atmosphere on September 23rd, 1999 [Mar99]. As these examples\nindicate, more careful attention to physical units could improve software quality, and because the rules\ngoverning unit correctness are simple, much of such checking can be performed automatically.\nIn fact, extensions exist for many languages allowing programmers to specify the units of variables and\nconstants so that they can be checked by a compiler. However, though many of the potential benefits of unit\nannotations would be realized in the maintenance of existing systems, adding unit annotations by hand to\nlarge existing programs would be very expensive. Instead of requiring a developer to specify each unit type\nindividually, we believe that a better approach is to automatically infer a general set of unit types based only\non the assumption that a program's use of units is consistent. Such inferred types could be useful for many\nkinds of automatic checking, such as warning a programmer when a change to one function is inconsistent\nwith the units of variables in another one. Such a most-general unit system also provides a more efficient\nway for a developer to add human-readable units to a program: he or she must only specify a few of the\nunit types in a program, and the rest can be automatically assigned to be consistent.\n\nThe Units Inference Problem\nThis section provides some additional details about the problem of units inference as we approach it, and\ndefines terminology that will be used in the rest of the paper.\nA first important distinction is between the concepts of dimension and unit. A dimension is an attribute\nthat can be measured: for instance, length or mass. A unit is a standard amount of some dimension, in\nwhich other measurements of that dimension can be expressed: for instance, inches or kilograms. Every\nphysical quantity has an associated unit and dimension: the dimension is determined by the unit, but there\ncan be multiple units that correspond to the same dimension. For instance, meters per second and miles per\nhour both measure speed. Dimensions and units obey the same algebraic rules: for instance, the product\nof two values must have the product of their dimensions, and also the product of their units. Because of\nthis similarity, the techniques we discuss are generally applicable to checking either the dimensions or the\nunits in a program. For instance, in the statement len1 = 2.54 * len2, one might either assign len1 and\nlen2 the dimension length, so that 2.54 is dimensionless, or one might assign len1 the units of centimeters,\nlen2 the units of inches, and 2.54 the units centimeters per inch. However, because a program with the\ncorrect units will necessarily have the correct dimensions, we will generally speak simply of \"units\". As an\nexception, we refer to quantities with no units as \"dimensionless\" rather than \"unitless\", since the former\nterm is standard.\nUnits inference is the problem of assigning a unit type (or just a \"unit\" for short) to all of the program\nvariables in a piece of code such that the algebraic rules of units are satisfied. Among the program variables\nwe count both true variables and numeric literals. Because unit correctness applies to operations between\nvariables, the units for a single variable cannot be defined in isolation: rather one should consider a unit\nsystem in which all of a program's units are expressed. A unit system is distinguished by its set of basic\nunits, those in which every other unit can be expressed. For instance, in the standard SI (metric system),\nthe basic units include the kilogram, second, meter, and ampere, and the derived unit volt can be expressed\nas kg · m2/(A · s3).\nIn considering different unit systems for a single program, it can be helpful to partially order them\naccording to a relation we call subsumption. A unit system S2 subsumes a unit system S1, written S1 ⊑S2,\nif every basic unit of S1 can be expressed in terms of basic units of S2. Intuitively, S2 is 'more expressive'\nthan S1. As an extreme case, the unit system with no basic units (in which every quantity is dimensionless)\nis subsumed by any unit system. If two unit systems are each subsumed by the other, we say they are\nequivalent.\nAnnotation-less Unit Type Inference\nWe present a technique that first automatically infers unit types from the source code of C programs and\nthen allows a user to annotate program variables with named units. The goal of our annotation-less unit type\ninference technique is to construct a unit system for a program which is general enough to cover whatever\nconsistent system the programmer might have intended. To be precise, the goal is to find a unit system with\nthe minimum number of basic units that subsumes any correct unit system for a program. There may be\nmany equivalent unit systems with this property: the choice between them is arbitrary.\nOur tool builds up constraints over variables and units based on a static source code analysis and solves\nthe constraints to find the minimum number of inferred basic units of a unit system that subsumes the\ncorrect unit system. After constraint solving, each variable's units are expressed in terms of the inferred\nbasic units. The user can then interactively annotate variables with user-defined units (whose components\nare user-defined basic units) to specify a subsumed (more specific) unit system.\nAs a running example, consider the following program that calculates the total kinetic and potential\nenergy of a moving mass:\n\nint main() {\ndouble mass, velocity, height, kinetic, potential;\n/* ... initialize relevant variables ... */\nkinetic = 0.5 * mass * velocity * velocity;\npotential = mass * height * 9.8;\nprintf(\"Total energy is: %g J\\n\", kinetic + potential);\nreturn 0;\n}\nOur tool first generates constraints over the units of program variables based on operations between them\nsuch as addition and multiplication (Section 4.1). For instance, one constraint over the units of the variables\npotential, mass, height, and the constant 9.8 is that the units of potential must be the product of the\nunits of mass, height, and 9.8. We then simplify the constraints (Section 4.2) in various ways, including\nmerging together variables that must have the same units into one set (e.g., kinetic and potential, which\nmust have the same units because they are added together within the printf statement). We then solve\nthe constraints (Section 4.3) to determine the minimum number of basic units (4 in this example) and each\nvariable's units expressed in terms of those inferred basic units. The basic units have no names, so we will\nrefer to them by the numbers 1-4. Here are the results from solving the constraints on our example:\nVariables:\n1: velocity\nUnits: (unit 1)\n2: mass\nUnits: (unit 2)\n3: constant 0.5\nUnits: (unit 3)\n4: constant 9.8\nUnits: (unit 4)\n5: height\nUnits: (unit 1)^2 * (unit 3) * (unit 4)^-1\n6: kinetic, potential\nUnits: (unit 1)^2 * (unit 2) * (unit 3)\nThis solution provides the most general set of units that are consistent with the constraints imposed by\noperations within the program. However, these units will likely be meaningless to the user, whose notion of\nunits includes physics terms such as meter and second, not the unnamed basic units presented above. Our\ntool includes an interactive user interface that allows the user to assign named user-defined units to variables\n(Section 4.4). For this example, the user can annotate velocity with the units meter/second, height with\nthe units meter, and so forth. Our tool combines the named units that the user provides with the inferred\nbasic units from the constraint solver to assist the user in two main ways: First, it can automatically fill in\nthe units of certain variables after the user enters in units for other ones, thus lessening the user's annotation\nburden. Second, such inferred units can alert the user to annotation mistakes or program bugs when the\nunits of a variable do not match expectations.\nVariables:\n1: velocity\nUnits: meter second^-1\n2: mass\nUnits: kilogram\n3: constant 0.5\nUnits: dimensionless\n4: constant 9.8\nUnits: meter second^-2\n5: height\nUnits: meter\n6: kinetic, potential\nUnits: kilogram meter^2 second^-2\n\nThe above snapshot shows a completed session of our tool on this example. The user only needs to\nannotate 4 out of the 6 variables (because there are 4 inferred basic units) and the tool infers the units for\nthe rest of the variables.\nTechnique and Implementation\nHere are the four stages of our inference system:\n1. Constraint generation - Analyzes the program's source code and produces constraints that must\nhold between the units of different variables (Section 4.1).\n2. Constraint simplification - Simplifies the constraints by applying both meaning-preserving trans-\nformations and heuristics which lose information but make sense in practice (Section 4.2).\n3. Constraint solving - Solves constraints using linear algebra techniques and outputs a minimal set of\ninferred basic units (Section 4.3).\n4. User interface for guided annotations - Allows the user to provide user-defined units for variables\ninteractively while being guided by the solved constraints (Section 4.4).\n4.1\nConstraint generation\nThe input to this stage is C source code, and the output is a set of constraints on the unit types of variables.\nThis stage is implemented as OCaml code which is compiled with the CIL analysis framework [NMRW02].\nCIL pre-processes and parses C code to produce an abstract syntax tree; our code then walks through each\nexpression in the program, outputting constraints along the way. A fresh unit type variable is created for\neach variable and numeric literal in the original program, and for each subexpression. (In the rest of this\npaper, we will often use the term \"variable\" as an abbreviation for \"unit type variable\".) Then, certain\nkinds of expressions produce constraints between the type variables. Our tool operates on unit exponents,\nso that unit multiplication is represented by addition and unit exponentiation by scalar multiplication; this\nallows us to use the usual terminology of linear algebra. In the rules that follow, we use xe to represent the\nunit type for an expression e:\n- Addition (+), subtraction (-), assignment (=) and comparison (==, !=, <, >, <=, >=) operators give\nequality constraints between unit variables.\nFor instance, from a = b + c, the tool generates the\nconstraints xb = xc and xa = xb.\n- A multiplication (*) operator gives a sum constraint between unit variables. For instance, from the\ncode a = b * c, the tool generates the constraint xa = xb + xc.\n- A division (/) operator gives a difference constraint between unit variables. For instance, from the\ncode a = b / c, the tool generates the constraint xa = xb -xc.\n- The square-root function sqrt gives a sum constraint according to the multiplicative relationship\n√x√x = x. For instance, from a = sqrt(b), the tool generates the constraint xb = xa + xa. (This\nconstraint is equivalent to the more obvious xa = 1\n2xb, but has the advantage of not requiring extra\nconstraint syntax beyond that required for multiplication above. We have also considered giving a\nspecial treatment to the pow function when its second argument is a constant, but the need did not\narise in our case studies.)\nThis constraint collection can be viewed as a very simple static analysis. It is flow-insensitive, because\nthe desired result of our tool is a single unit type for each program variable. Our analysis is currently also\ncontext-insensitive. For most functions, a context-insensitive analysis is sufficient, but some functions are\nused with arguments of more than one unit type. Such functions are most common in the standard library:\n\nfor instance, generic math functions like pow, memory allocators like malloc, and input-output routines\nlike printf. Since such functions are not defined in the source code available to our tool, it generates no\nconstraints for them. However, if such a unit polymorphic function is defined, the results of our tool suffer:\nusually the only consistent typing makes the arguments to such a function dimensionless. In considering\nexamples, we have found cases where a number of other kinds of sensitivity might improve our tool's results:\nfield sensitivity (giving different units to different fields of a structure), distinguishing different definitions of\na single variable (as by SSA conversion), and giving different unit types to different elements of statically-\nsized arrays. However, it is unclear whether the benefit of such changes would be worth the effort in both\ntool implementation and analysis runtime, and many would also require adding a cast-like mechanism to the\ntarget unit annotation system.\nOur implementation takes a very conservative approach to pointers and aliasing: all of the values accessed\nvia a pointer must have the same units. A natural implementation of this approach would be to create a\nfamily of pointer types for each unit type: for instance, besides a type for \"meters\", there would also be a type\n\"pointer to meters\", another \"pointer to pointer to meters\", and so on. However, because it does not seem\npossible to represent pointer types in linear algebra, our analysis takes a simpler approach of not recording\nthe information about which level of pointer a value contains: the pointer generated by taking a reference\nto a meters value is again simply labelled \"meters\". We believe this is reasonable because the program has\nalready been checked under the C type system, which distinguishes levels of pointers. A confusing result\nmight still be produced if a program, say, casts a pointer to an integer value and then multiplies this integer\nby another dimensioned value, but such situations seem unlikely in practice.\nOur analysis is interprocedural: it simply creates constraints requiring that the arguments to a function\nhave the same unit types as the function's formal parameters, and similarly for the function's return value.\nTo conservatively estimate which functions might be called by code that uses a function pointer, we use a\npointer analysis that is supplied with CIL. Though few of the programs we have considered make extensive\nuse of function pointers, this analysis might also be a source of imprecision for programs that do.\nThe following figure shows the relevant constraints produced in the energy example, to the right of the\nlines of code that produce them. Note the unnamed intermediate variables x11, x12, x13, x17, and x18,\nwhich correspond to non-atomic expressions: for instance, x11 is the unit type of the expression 0.5 * mass.\nint main() {\ndouble mass, velocity, height, kinetic, potential;\n/* ... initialize relevant variables ... */\nkinetic = 0.5 * mass * velocity * velocity;\n// variable velocity\nis x8\n// variable mass\nis x9\n// constant 0.5\nis x10\n// x11 = x10 + x9\n// x12 = x11 + x8\n// x13 = x12 + x8\n// variable kinetic\nis x14\n// x14 = x13\npotential = mass * height * 9.8;\n// constant 9.8\nis x15\n// variable height\nis x16\n// x17 = x9 + x16\n// x18 = x17 + x15\n// variable potential is x19\n// x19 = x18\nprintf(\"Total energy is: %g J\\n\", kinetic + potential);\n// x14 = x19\nreturn 0;\n}\n\n4.2\nConstraint simplification\nIt would be perfectly correct to take the constraints directly as extracted from the program, convert them\ninto matrix form, and solve them as we will describe in Section 4.3. However, the linear algebra constraint\nsolving technique requires time and space that grow more than linearly in the number of unit type variables,\nand are significant in practice even for relatively small programs. Our constraint generation stage usually\nproduces on the order of as many constraints and unit type variables as there are lines of source code, and\nthe matrices used in the constraint solving stage will have at least as many rows and columns as there are\nconstraints and variables, respectively. Thus, it is important in practice to simplify the unit constraints\nbefore solving them.\nTo allow the most freedom in the implementation of the constraint generation stage, constraint simplifi-\ncation is performed by a separate program (a Perl script). The simplification has three parts:\n- First, the constraint equations are simplified algebraically, removing intermediate variables by replacing\nthe variable from the left hand side of a constraint, wherever it appears, with the right-hand side of\nthe constraint, and then removing the constraint. In the example, the constraints x11 = x10 + x9\nand x12 = x11 + x8 can be replaced by a single constraint x12 = x10 + x9 + x8.\n- Second, program variables and constants whose type variables are transitively constrained to be equal\nare merged into a single set; they are then considered together for the rest of the analysis. In the\nexample, the type variables x14, x18, and x19, which correspond to the program variable kinetic, a\nsub-expression, and the program variable potential, can be grouped together in this way.\n- Finally, the type variables are partitioned into sets whose unit types can be inferred independently:\ni.e., if one considers a graph in which type variables are nodes and edges connect variables that appear\ntogether in a constraint, each connected component of the graph is completely independent. Each\nsubset of this partition can then be processed separately for the rest of the analysis. (It is often the\ncase, including in our running example, that all of the variables with meaningful physical units lie in\none connected component.)\nIn addition to the meaning-preserving simplifications described above, it is also helpful to perform some\nheuristic simplifications with respect to the type variables for constants. In theory, these changes might rule\nout some otherwise legal unit typings, but in practice we can be relatively confident that they will not rule\nout any desired ones, and they make later stages of inference easier for both the tool and the programmer\nwho must eventually annotate variables with units. Our technique's default treatment of numeric constants\nis that each occurrence of a constant might have a different unit type, and this is clearly the best behavior\nfor some values; for instance, one occurrence of 0 might represent the net weight of an empty bucket,\nwhile another might be the velocity of a stationary bicycle. On the other hand, floating point constants\nwith a precise but random (in the Chaitin-Kolmogorov sense) value are likely to have only one unit type\nin all the places where they occur. For instance, 745.7 and 2.99792 · 108 are likely always the conversion\nfactor between horsepower and watts, and the speed of light in meters per second, respectively. To avoid\nintroducing superfluous degrees of freedom (leading to additional inferred basic units, and therefore to a need\nfor redundant programmer annotations), our simplification stage gives all occurrences of such a value the\nsame unit type. The only unit typings that would be ruled out by such a modification are those in which the\ncorrect units for two occurrences of, say, 745.7 were different. This merging is particularly significant when\na constant is specified in the original program with #define; because CIL operates after the C preprocessor,\nour tool would not otherwise be able to link the occurrences of the constant. Similarly, certain well-known\nvalues (such as 2.71828) can be reasonably presumed to be dimensionless.\nIn our running example, no merging of constants occurs because no constant appears more than once,\nbut our tool's heuristics would merge additional occurrences of 9.8 if they appeared elsewhere. Though 0.5\nis dimensionless in this case, we were not confident that every occurrence of that value would be, so our\nheuristics do not constrain its type variable. Algebraic simplification reduces the constraints to two:\n\nx19 = 2*x8 + x9\n+ x10\nx19 =\nx9 + x15 + x16\nIn this example, the two constraints produced by simplification are linearly independent, but in general\nthe simplification algorithm is not powerful enough to remove all redundant constraints.\n4.3\nConstraint solving\nThis stage solves a set of constraints produced by the constraint generation and simplification stages to\nfind the minimum number of inferred basic units such that any consistent unit system can be expressed in\nterms of those basic units. It is implemented as a MATLAB script that accepts the constraints in a matrix,\nuses linear algebra to solve the matrix, and outputs a solution matrix that contains the basic units and the\nexponent power of each basic unit assigned to each unit type variable.\nWe will demonstrate the constraint solving algorithm using our energy example. Here are the constraints\nin matrix form after simplification:\nConstraints:\nx19 = 2*x8 + x9\n+ x10\nx19 =\nx9 + x15 + x16\nMatrix:\n#\nx8\nx9\nx10\nx15\nx16\nx19\n[\n-1\n-1 ]\nNames:\nx8: {velocity}\nx9: {mass}\nx10: {constant 0.5}\nx15: {constant 9.8}\nx16: {height}\nx19: {kinetic, potential}\nObserve that the constraints will always be homogeneous (i.e., there will never be any constant terms),\nso they can be represented in a single matrix by putting all the variables in each equation on one side and\nplacing their coefficients into the matrix (the other side of the equation is zero). There is one row for each\nconstraint and one column for each unit type variable. The Names array provides the names of the program\nvariables that are associated with each unit type variable (notice that, due to constraint simplification, two\nprogram variables, kinetic and potential, are associated with the 6th unit type variable, x19).\nWe want to find the maximum number of basic units that can be assigned so that all the constraints are\nsatisfied, to give the most general unit system, subject to the constraint that the basic units are independent:\nnone can be expressed in terms of the others. (Recall that our tool can only find inferred basic units, not the\nbasic units of a real-world units system such as SI. However, we will use the term basic units for brevity.)\nThat number is equal to the number of columns in the matrix minus the rank of the matrix. The number of\ncolumns represents the number of unit type variables, and the rank represents the number of independent\nconstraints.\nTo get some intuition for this formula (num. basic units = num. columns -rank), suppose that there were\nno constraints (rank = 0). Then the number of basic units equals the number of variables, because the most\ngeneral system of units allows each variable to have its own unique basic unit. Adding a constraint allows\nthe units of one variable to be expressed in terms of the units of all the other variables, thus reducing the\nnumber of required basic units by one. Every subsequent independent constraint added reduces the number\nof basic units by one. If the number of independent constraints equals the number of variables, no basic\nunits are required and the only valid solution is to assign all variables the special null unit dimensionless (e.g.,\nin a one-line program consisting of the statement a = a2, the only solution is that a is dimensionless). The\nnumber of independent constraints cannot exceed the number of variables (a theorem from linear algebra).\nIn practice, the rank of the constraint matrix is usually slightly less than the number of rows, because the\nsimplification process of Section 4.2 removes many but not all redundant constraints.\nWe have developed a greedy constraint solving algorithm that attempts to pick basic units from among\nthe unit type variables:\n\n- Place constraints in matrix M\n- j = 1\n- for i = 1 to num. variables:\n1. Save M, then augment it with a new column for the jth basic unit and a new row for the assignment\nof the ith unit type variable to the jth basic unit\n2. Transform the matrix to express all variables in terms of the j basic units and check for linear\ndependence among the basic units (can some be expressed purely in terms of the others?)\n3. If so, undo assignment by restoring M\n4. Else, j++ for successful assignment\n5. If (j == num. basic units), break\nThe algorithm iterates through the list of variables and tries to assign each of them to a new basic unit.\nEach assignment is made by augmenting the matrix by one column to represent the newly-added basic unit\nand one row to represent the newly-added constraint that a particular variable is assigned to that basic unit.\nIf the assignment is successful, then it moves on to assign the next variable to a new basic unit. Otherwise,\nit tries to assign the next variable to the current basic unit. The algorithm terminates when the necessary\nnumber of basic units (as previously computed) have been assigned to variables.\nThe indication of a successful assignment is that no linear dependencies have been introduced among the\nbasic units. A linear dependence occurs when an assignment is made such that a basic unit can be expressed\npurely in terms of the other basic units. For example, let's suppose that we have assigned variable 1 to basic\nunit 1 and variable 2 to basic unit 2. Let's also suppose that there is a constraint stating that variable 3\nhas the units of variable 1 raised to the 4th power. Thus, when we attempt to assign variable 3 to basic\nunit 3, this creates a linear dependence because basic unit 3 can be expressed purely in terms of basic unit\n1: (basic unit 3) = 4 · (basic unit 1). Intuitively, this assignment did not add any extra information, so it is\nmarked as unsuccessful. The algorithm now attempts to assign variable 4 to basic unit 3. We check for linear\ndependence because basic units that were not independent would be redundant, creating extra equivalent\nexpressions for the same unit type without increasing expressiveness.\nThe constraint solving algorithm is implemented as a MATLAB script that roughly follows the pseudo-\ncode presented above.\nIt calls the MATLAB rref function to put the matrix in reduced row-echelon\nform, which makes it efficient to check for linear dependence during each assignment. The check for linear\ndependence is made by using the rank function to find the rank of a slice of the matrix that corresponds to\nthe basic units assigned so far expressed in terms of the variables. If the rank of the slice is less than the\nnumber of basic units assigned so far, then there is a linear dependence among the expressions of the basic\nunits in terms of program variables, so the same dependence holds between those basic units.\nHere are the results of the constraint solver on our energy example:\nVariables:\n1: velocity\nUnits: (unit 1)\n2: mass\nUnits: (unit 2)\n3: constant 0.5\nUnits: (unit 3)\n4: constant 9.8\nUnits: (unit 4)\n5: height\nUnits: (unit 1)^2 * (unit 3) * (unit 4)^-1\n6: kinetic, potential\nUnits: (unit 1)^2 * (unit 2) * (unit 3)\nAll assignments were successful, so the first 4 variables were assigned to the 4 basic units. The solver also\ncalculates the units of the remaining variables in terms of the basic units so that all variables are assigned\nunits.\n\n4.4\nUser interface for guided annotations\nThis final stage of our tool allows the user to interactively assign user-defined units to program variables.\nThe input is a solution matrix from the constraint solving stage and a list of variable names. The user is\nallowed to assign user-defined units for any program variable in any order, except that no assignment can\nviolate the inferred constraints. A user-defined unit consists of 0 or more user-defined basic units (0 only for\nthe special unit dimensionless). For example, the user-defined unit meter · second-1 consists of 2 user-defined\nbasic units, meter and second, which belong to the SI unit system. These basic units differ from the tool's\ninferred basic units in that they have names; they form a unit system that is subsumed by the one formed\nby the inferred basic units.\nEach time the user assigns a user-defined unit to a variable, the tool adds an additional constraint to\nequate the inferred basic units for that variable with the user-defined basic units that comprise that unit\n(adding additional columns for user-defined basic units if necessary). For example, if the user assigns the\nunits of meter · second-1 to velocity, the tool adds a new constraint that equates basic unit 1 with\nmeter · second-1.\nThis is very similar to what happens in the constraint solving stage.\nThe tool then\nsolves the augmented matrix, and presents the user with an updated list of variables and their user-defined\nunits. Solving the matrix after every assignment allows the tool to automatically infer user-defined units for\nvariables that the user did not manually annotate. The user only has to make a number of annotations equal\nto the number of inferred basic units, which is often far less than the total number of variables. For example,\nafter assigning units for velocity, constant 0.5, and height, the tool inferred the units of constant 9.8\nwithout the user having to annotate it:\nVariables:\n1: velocity\nUnits: meter second^-1\n2: mass\n[Units not yet established]\n3: constant 0.5\nUnits: dimensionless\n4: constant 9.8\nUnits: meter second^-2\n5: height\nUnits: meter\n6: kinetic, potential\n[Units not yet established]\nThis feature is useful in two ways: First, it alleviates the burden of making the user annotate additional\nvariables, especially ones that have complex units consisting of many components. Second, it provides the\nuser with confidence that the units that he or she has been assigning thus far are correct. Conversely, it can\nalert a user either to bugs in the program or to errors in the annotation if the inferred units do not match\nexpectations.\nWhen this stage begins executing, all variables are expressed in terms of the inferred basic units provided\nby the constraint solving stage. As the user annotates variables with user-defined units, the tool tries to\nautomatically annotate other variables with user-defined basic units in a way that is consistent with the\ninferred basic units. Often a variable is expressed as a mixture of inferred basic units and user-defined basic\nunits. As soon as a variable can be expressed solely in terms of user-defined basic units, then the inference\nis complete for that variable, and it is reported to the user. This stage completes when the inference is\ncompleted for all variables, so that all can be expressed solely in terms of user-defined basic units. At this\npoint, the transformation from inferred basic units to user-defined basic units is complete, and there is no\nmore need for the inferred basic units.\nExperimental Results\nWe ran our tool on C programs ranging from 50 to 50,000 lines of code and performed quantitative and qual-\nitative evaluations. We found most test programs by performing a web search for programs that performed\ncalculations with physics units (using names for SI units and the word printf in our search string):\n\nProgram\nLOC\nTime\n(sec.)\nNumber of\nProgram\nSimplified\nBasic\ngenerate\nsimplify\nsolve\nconstraints\nvariables\nvar. sets\nunits\nenergia\n0.31\n0.04\n0.13\n76 / 10\nquasi\n0.51\n0.16\n0.43\n579 / 40\nschd\n0.55\n0.27\n5.95\n1041 / 85\nbike-power\n0.39\n0.14\n5.09\n544 / 76\ndemunck\n0.84\n0.36\n8.91\n2099 / 139\nstarexp\n0.87\n1.35\n100.31\n7995 / 550\noggenc\n51.00\n3.28\n168.93\n27864 / 777\nTable 1: Results gathered from running all the stages of our units inference tool except for the interactive\nuser interface. For the number of constraints, the value to the left of the slash is the number before constraint\nsimplification and the value to the right is the number after simplification.\n- energia - This small program computes the kinetic and potential energies of a mass launched from a\nspecified height. It is in fact quite similar to our running energy example, except that it comes from a\nphysics class at the University of Rome (La Sapienza), so all of the identifiers are in Italian.\n- quasi - This program, originally developed for a class assignment and now maintained in the public\ndomain by Eric Weeks, draws a Penrose tiling of a portion of the plane (i.e., a quasicrystal) as a\nPostscript file.\n- schd - This program by Mike Frank of the University of Florida reversibly simulates the evolution of a\nparticle wavefunction according to the Schr odinger equation, and displays the results in an X window.\n- bike-power - This program from Ken Roberts of Columbia University computes a table of the energy\nused when riding a bicycle at various speeds, taking into account a number of other factors including\nthe weight of bicycle and rider, the grade of the slope, and the coefficient of friction between the tires\nand the road.\n- demunck - This program is a simulation of the propagation of electrical impulses through a series of\nconcentric spheres of various conductivities, used to approximate the processes that distort the readings\nof an electroencephalogram (EEG).\n- starexp - This is a relatively large program by James A. Green which computes a number of char-\nacteristics of main sequence stars based on their brightness and spectral type, using models from\nastrophysics.\n- oggenc - This, the largest of our test programs, was not found by a web search as the others were; it is\na common Linux multimedia utility that we have worked with in previous research. It applies a lossy\npsycho-acoustical algorithm to compress audio data into an MP3-like format known as Ogg Vorbis.\nUnlike general purpose compression, this encoding relies on extensive signal processing. Though we\nat first used it just to test our tool's scalability, reading the code revealed a number of physical units,\nincluding ones for sampling rates and frequencies, elapsed times, and sound intensities.\n5.1\nQuantitative Evaluation\nTable 1 demonstrates that our tool scales well to moderate-sized programs. The bulk of the running time is\nin the MATLAB constraint solving script, mostly in two calls to the rref function. We have optimized the\nimplementation so that rref only needs to be called twice regardless of the number of variables or constraints\n(our initial implementation called rref once for every iteration of the loop, which was much slower). The\nsolver's run time is proportional to the number of constraints and the square of the number of variables, or\n\nroughly cubic in the size of the original program. By contrast, the constraint simplification process requires\nonly slightly more than linearly many steps in terms of the number of constraints, so spending time on\nsimplification saves time overall. In fact, the constraint simplifier is crucial for the scalability of our tool\nbecause it greatly reduces the size of the constraint matrix, usually by several orders of magnitude.\nThe number of inferred basic units was always much less than the number of program variables, which\ngreatly lessens the annotation burden on the user during the user interface stage. Without our tool, the\nuser needs to annotate every program variable with units, but using our tool, the user only needs to make\na number of annotations equal to the number of basic units.\nFor schd and bike-power, we removed the definitions of some functions used for memory allocation\nor command-line input which were unit polymorphic (see Section 5.2 for discussion), and for oggenc, we\nused a previously prepared version that had been modified to compile as a single file; the other programs\nwere unmodified. The constraint generation stage for oggenc took an especially long time because of the\npointer analysis that CIL performs to resolve function pointer targets, which here uses the default settings\nfor precision. Most of the time may be saved by supplying options to perform the analysis in a less precise\nmanner; we have not evaluated whether doing so affects the overall results of our tool.\n5.2\nQualitative Evaluation\nTo assess the usability and precision of our tool, we have performed qualitative evaluations on our test\nprograms by using our user interface to annotate program variables with units. When performing these\nevaluations, we simulated the behavior of someone who is familiar with the code and tried to annotate\nvariables with simpler units before ones with more complex units.\nWe performed a fairly detailed annotation session for bike-power. We determined the correct units of\nvariables by inspecting the source code and comments, and we confirmed that the tool's results matched\nthose units. Although there were 29 basic units in this program, we only had to make 12 annotations before\nall of the interesting variables (48 out of 72) were correctly labeled with units. The remaining 17 annotations\nwere required to mark identical copies of a conversion factor constant defined as a C preprocessor #define\n(CIL works on source code after preprocessing). From our experiences with this program and several others\n(e.g., schd, demunck), we believe that it is usually possible for the user to make fewer annotations than\nthe required number (number of basic units) and still have the tool infer units for most of the interesting\nprogram variables.\nWe began our interactive session for bike-power by annotating variables that seemed to have easy-\nto-specify units such as dimensionless, kilogram, pound, and meter · second-1 (velocity). Sometimes after\nwe annotated a variable, our tool was able to infer that several other variables had the exact same units.\nFor example, after we specified that a particular constant represented a velocity, the tool reported that\nseveral other constants were also velocities, which we confirmed to be correct. This helps reduce the number\nof variables that we had to annotate (recall that both constants and variables must be annotated since\nconstants may have units as well). Although this is useful, the tool's inference functionality is more powerful\nwhen it can infer units that are different than the units that we have entered thus far. For example, after\nannotating several variables, the tool reported that the units of a variable P_t was meter2·second-3·kilogram,\nwhich happens to be watts expressed in SI basic units. This saves the user a lot of work because it is difficult\nto remember such complex units and can be error-prone to enter them in manually.\nIn the process of annotating units for bike-power, we discovered the following bug in the comments of\nits source code:\n#define mOs_per_miOhr 0.44704\n/* meters/second per kilometers/hour */\n#define mOs_per_kmOhr (1000.0 / 3600.0)\n/* meters/second per miles/hour */\nThe comments next to these two conversion factors have been mistakenly switched. Because we relied\non the comments to determine the set of correct units for the program, we entered in incorrect units for\nmOs_per_kmOhr. The tool has no way of informing the user of when an annotation is incorrect. However,\n\nwe suspected that there was a bug when we saw the tool inferring units for other variables that did not\nmatch our expectations. We were able to quickly track the source of the problem back to the source code\ncomments.\nThe experience of using the tool is similar for all of the programs, and for the larger programs we did not\nassign units to every variable, so we discuss the remaining programs in less detail. In the energia example,\nwe easily assigned units to each program variable. For the quasi program, we were able to assign the units\nof centimeters to a number of variables that measured length, the most important dimension in the program.\nIn schd and bike-power, our initial results were unsatisfactory in that variables we expected to have a\ndimensioned type were inferred to be dimensionless. This failure was caused by the context-insensitivity\nof our analysis: for instance, in bike-power, a single function was used to parse numeric command line\narguments in various units. Because our analysis can currently assign only one set of units to the result of a\nfunction, the only consistent solution was for the function to return a dimensionless value, which propagated\nthrough much of the rest of the program. To work around this limitation, we removed the definitions of these\nfunctions from the source code, causing our tool to generate no constraints from their use. After this change,\nthe results for both programs were satisfactory: bike-power is discussed above, and for schd we were able\nto assign around a dozen units consistently to various variables. In addition, our tool automatically inferred\nthe correct units for Coulomb's constant of electrostatic attraction, which would have been cumbersome to\ncompute by hand.\nFor the larger programs, unfortunately, our attempts to assign units revealed imprecisions of our im-\nplementation that we could not easily work around. In demunck, a calculation which in the original paper\ndescribing the technique was performed using a matrix was coded with a 2-by-2 multidimensional array;\nunfortunately, the different entries in the matrix had different units, while our analysis assumes that all the\nelements of an array have the same units. In starexp, a single global variable named d is used repeatedly (in\ncode that appears to have been cut and pasted) when reading floating-point values of a number of different\nunits, causing them all to be considered dimensionless. For oggenc, we attempted to assign the units of hertz\n(1/sec) to the numeric literal 44100, which represents the sampling rate of CD-quality audio. Unfortunately,\nbecause it is field insensitive, our tool's result is that this constant must be dimensionless: a single structure\nis used to include a number of parameters to the encoding library, including both the sampling rate and\nsome quantities measured in seconds, but our analysis can assign only one unit to all the elements of the\nstructure.\nRelated work\nScientists and engineers have long recognized that checking whether the results of a calculation have the\nexpected units is an effective practice to recognize and prevent errors, and many domain-specific systems\nincorporate unit information in measurements. It should not be surprising, therefore, that adding unit infor-\nmation to programming languages has been the topic of frequent research. Some languages include sufficiently\npowerful extension mechanisms that systems for unit type information can be added within the language:\nan example is the combination of templates and operator overloading in C++ [Bro01]. However, a more\ncommon approach has been to consider language extensions, which can be suggested for any language, and\nallow the designer freedom in the choice of syntax. It is natural to see unit information as a kind of type, so\nmany approaches have extended the type systems of then-common strongly-typed general purpose languages,\nsuch as Pascal in the 1980s [Bal87] or more recently Java [vD99]. An alternative approach for a language\nlike Java is to treat units as a kind of class [ACL+04]. Perhaps surprisingly, research applied to languages\nused specifically in the scientific community is in the minority; examples include Petty's unit extension for\nFORTRAN [Pet01], and a unit checking tool for the 'language' of Microsoft Excel spreadsheets [ASK+04].\nTo reduce the burden of adding unit annotations, several researchers have suggested embedding unit\ntypes in a type system like that of ML that supports type inference. The earliest work to draw a connection\nbetween unit types and ML-like typechecking was that of Wand and O'Keefe [WO91]; they suggest an\ninference technique using linear algebra much like that originally suggested by Karr and Loveman [KL78].\nThe inference our tool performs is similar, but made more complicated by the fact that our system must\n\ninfer a most general set of basic units; in previous systems, the set of basic units was fixed or user-supplied.\nA variant of the algebraic technique which allows only integral exponents is developed more extensively by\nKennedy [Ken94, Ken96, Ken97]. He also gives theoretical results on the expressiveness of such a system:\nfor instance it is impossible to implement the square-root function so as to have the correct polymorphic\ntype.\nSuch previous inference techniques differ from the problem we consider in that they presume that the set\nof basic units, and the units for values such as constants, are specified by the programmer, just as in ML a\nprogram must declare data-types and their constructors. Assuming the program is correct, this information\nis enough to determine the unit type of each value in a program. By contrast, we consider programs with\nno unit information, for which a large number of valid unit typings exist. Without any annotations, our\ntechnique determines a most-general unit typing (one with the largest number of independent basic units)\nthat is consistent with the program's operations. This most general typing is already suitable for many\nkinds of automated checking: for instance, any modification to a program that would violate a most-general\nunit typing would also violate any more specific typing. Of course, if the unit types of a program are to be\nrelated to standard physical units, the need for some programmer annotation is unavoidable. In previous\nsystems, the programmer provides unit types at all of the syntactic locations required by the language, and\nthe inference system then checks if those annotations are consistent. In our system, a consistent typing is\nautomatically generated, and the programmer then assigns names to a subset of the generated types, from\nwhich the tool infers names of the rest.\nAn approach like ours that performs analysis first has two major advantages: first, if the program is\ncorrect, it allows the programmer to construct a complete unit typing with a minimal number of annotations,\none per basic unit in the most general typing. In previous systems, the annotation burden is proportional\nto static size of the program, which is usually much larger.\nOn the other hand, if the program's unit\ncorrectness is unknown, it is necessary for a programmer to examine each program variable and use their\ndomain understanding to determine what units would be correct. The difference between our approach\nand previous ones is that for many such variables, a programmer using our system would simply verify\nthe correctness of an automatically-produced unit type, when in previous systems they would write such\na type themselves. Though conceptually the programmer's work is the same in these cases, the increased\nautomation of our approach should make the programmer's work easier in most cases.\nBecause previous systems have required a significant amount of program annotation, they have rarely been\nevaluated on large or pre-existing programs. In particular, previous languages that have included unit type\ninference [WO91, Ken97] have been based on languages not commonly used by scientists or engineers, and\nhave not been evaluated in realistic case-studies. Experimental results, like practical uses, have been focused\non systems that require extensive annotation. A very promising, but small and uncontrolled, case study\nwas performed by Brown [Bro01]: he had an independent programmer implement a simple physics function\nusing SIunits, and the unit checking led to the discovery of three separate errors. Antoniu et al. used their\nspreadsheet-checking system [ASK+04] to find previously unknown bugs in three published spreadsheets,\ntwo in which results were labelled incorrectly and one involving an incorrect calculation. Because our system\ndoes not require annotations, it can naturally be applied to pre-existing programs, and we have already\ntested it on some such programs. Improving the efficiency of the tool to operate on large programs, and\ndevising experiments to evaluate the precision of its results without manual comparisons, are directions we\nplan to investigate in the future.\nFuture Work\n- Add context sensitivity/unit polymorphism - Much of our tool's imprecision on larger programs\ncan be attributed to the fact that our static analysis is context-insensitive. For functions that accept\nvalues of different units, we now constrain all values passed into that function to have the same units.\nWe currently have special cases for common functions such as square root (sqrt) but would like to\nadd unit polymorphism to avoid having to rely on special cases.\n\n- Specialize for standard real-world units and constants - Our tool currently infers the most\ngeneral set of basic units without consideration of actual units that a user is likely to annotate (e.g.,\nSI units). Connecting our interactive user interface to a database of units may improve the usability\nof our tool and allow it to provide more accurate results with even fewer user annotations.\n- Incremental constraint solving - The current performance bottleneck of our tool is the MATLAB\nconstraint solving script (especially the calls to rref). Instead of building up a large matrix of con-\nstraints and then simplifying and solving it, we want to find a way to solve the constraints as we\ncollect them. This could greatly improve performance and might even make a dynamic unit inference\nimplementation feasible.\n- Produce compiler-checked annotations in source - We would like to annotate the program's\nsource code with the units that our tool infers and have the compiler be able to perform type checking\nbased on these units. This would make a program more resilient against the introduction of new bugs.\nConclusion\nWe have developed a technique and implemented a tool for inferring unit types from the un-annotated source\ncode of C programs. Our tool allows the user to assign meaningful names to units of program variables with\nminimal effort. It can be applied to find bugs caused by inconsistent use of units, to prevent future bugs, and\nto provide documentation that can aid in code maintenance. We have evaluated our tool both quantitatively\nand qualitatively on a variety of real-world programs of up to 50,000 lines of code and reported its strengths\nand limitations (some of which form the motivation for our future work).\nReferences\n[ACL+04]\nEric Allen, David Chase, Victor Luchangco, Jan-Willem Maessen, and Guy L. Steele Jr. Object-\noriented units of measurement. In Object-Oriented Programming Systems, Languages, and Ap-\nplications (OOPSLA 2004), pages 384-403, Vancouver, BC, Canada, October 26-28, 2004.\n[ASK+04]\nTudor Antoniu, Paul Steckler, Shriram Krishnamurthi, Erich Neuwirth, and Matthias Felleisen.\nValidating the unit correctness of spreadsheet programs. In ICSE'04, Proceedings of the 26th In-\nternational Conference on Software Engineering, pages 439-448, Edinburgh, Scotland, May 26-\n28, 2004.\n[Bal87]\nGeoffBaldwin. Implementation of physical units. ACM SIGPLAN Notices, 22(8):45-50, August\n1987.\n[Bro01]\nWalter E. Brown. Applied template metaprogramming in SIunits: the library of unit-based\ncomputation. In Proceedings of the Second Workshop on C++ Template Programming, Tampa\nBay, FL, USA, October 14, 2001.\n[EJC01]\nEdward A. Euler, Steven D. Jolly, and H.H. 'Lad' Curtis. The failures of the Mars Climate\nOrbiter and Mars Polar Lander: A perspective from the people involved. In 24th Annual AAS\nGuidance and Control Conference, Breckenridge, CO, USA, January 31-February 4 2001.\n[Ken94]\nAndrew Kennedy. Dimension types. In 5th European Symposium on Programming, Edinburgh,\nUK, April 11-13, 1994.\n[Ken96]\nAndrew Kennedy. Programming Languages and Dimensions. PhD thesis, University of Cam-\nbridge, April 1996.\n\n[Ken97]\nAndrew J. Kennedy. Relational parametricity and units of measure. In Proceedings of the 24th\nAnnual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Paris,\nFrance, January 15-17, 1997.\n[KL78]\nMichael Karr and David B. Loveman III. Incorporation of units into programming langauges.\nCommunications of the ACM, 21(5):385-391, May 1978.\n[Mar99]\nMars Climate Orbiter Mishap Investigation Board. Phase I report, November 1999.\n[NMRW02] George C. Necula, Scott McPeak, S.P. Rahul, and Westley Weimer. CIL: Intermediate language\nand tools for analysis and transformation of C programs.\nIn Compiler Construction: 11th\nInternational Conference, CC 2002, Grenoble, France, April 8-12, 2002.\n[Pet01]\nGrant W. Petty. Automated computation and consistency checking of physical dimensions and\nunits in scientific programs. Software: Practice and Experience, 31(11):1067-1076, September\n2001.\n[vD99]\nAndr e van Delft. A Java extension with support for dimensions. Software: Practice and Expe-\nrience, 29(7):605-616, June 1999.\n[WO91]\nMitchell Wand and Patrick O'Keefe. Automatic dimensional inference. In Computational Logic\n- Essays in Honor of Alan Robinson, pages 479-483, 1991."
    }
  ]
}