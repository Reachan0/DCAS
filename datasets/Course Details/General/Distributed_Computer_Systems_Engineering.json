{
  "course_name": "Distributed Computer Systems Engineering",
  "course_description": "This course covers abstractions and implementation techniques for the design of distributed systems. Topics include: server design, network programming, naming, storage systems, security, and fault tolerance. The assigned readings for the course are from current literature. This course is worth 6 Engineering Design Points.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Computer Networks",
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Computer Networks"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nA schedule of topics and important due dates are presented in the\ncalendar\n.\n\nStructure\n\n6.824 is a core graduate subject with lectures, labs, quizzes, and a final project. 6.824 is 12 units. 6 Engineering Design Points.\n\nLectures meet two times a week. Most class meetings will be one half lecture and one half paper discussion. You should read the paper\nbefore\ncoming to class, and be prepared to discuss it. You can find out what paper to read for each meeting given in the\nreadings\nsection.\n\nWe will post a question about each paper 24 hours before we discuss the paper. Please bring your answer to class on a sheet of paper and hand it in. Your answer need only be long enough to demonstrate that you understand the paper; a paragraph or two will usually be enough. We won't hand back the questions, but we will glance at them to make sure your answer makes sense, and they will count for part of the paper discussion grade.\n\n6.824 will have two in-class quizzes.\n\nThere are programming labs due every week or two for the first half of the term.\n\nIn the second half of the term you'll undertake a\nproject\nin small teams. Each team will design and implement a system of its choice. Each team will also write a research paper about its project.\n\nGrading\n\nThe grade for this class will be based on:\n\nActivities\n\nPercentages\n\nLabs (programming assignments)\n\n25%\n\nProject (programming and paper)\n\n40%\n\nQuizzes\n\n25%\n\nPaper discussion, participation, and paper questions\n\n10%\n\nLate Policy\n\nThe late policy for programming assignments is as follows. You can hand assignments in late, but the total amount of lateness summed over all the assignments must not exceed 72 hours. If you hand in an assignment late, and your total late time (include the late time for that assignment) exceeds 72 hours, we will give that assignment a grade of D. Note that a D is better than the grade you'd get if you handed in nothing. You can divide up your 72 hours among the assignments however you like; you don't have to ask or tell us. If you want an exception to this rule, please bring us a letter from a dean.\n\nCollaboration Policy\n\nYou must write all the code you hand in for the programming assignments, except for code that we give you as part of the assignment. You are not allowed to look at anyone else's solution (and you're not allowed to look at solutions from previous years). You may discuss the assignments with other students, but you may not look at or use each other's code.\n\nYou may discuss the questions for each discussion paper with other students, but you may not look at other student's answers. You must write your answer yourself.\n\nUseful Books\n\nThe following books may help provide background for 6.824 or help with lab programming. None of them are required. They are listed in rough order of usefulness.\n\nStevens, W. Richard, Bill Fenner, and Andrew M. Rudoff.\nUNIX Network Programming, Vol. 1: The Sockets Networking API\n. 3rd ed. Reading, MA: Addison-Wesley Professional, 2003. ISBN: 9780131411555.\n\nTanenbaum, Andrew.\nModern Operating Systems\n. 2nd ed. Upper Saddle River, NJ: Prentice Hall, 2001. ISBN: 9780130313584.\n\nTanenbaum, Andrew, and Maarten van Steen.\nDistributed Systems: Principles and Paradigms\n. Upper Saddle River, NJ: Prentice Hall, 2002. ISBN: 9780130888938.\n\nMcKusick, Marshall Kirk, Keith Bostic, Michael J. Karels, and John S. Quarterman.\nThe Design and Implementation of the 4.4 BSD Operating System\n. Reading, MA: Addison-Wesley Professional, 1996. ISBN: 9780201549799.\n\nStroustrup, Bjarne.\nThe C++ Programming Language\n. 3rd ed. Reading, MA: Addison-Wesley Professional, 2000. ISBN: 9780201700732.\n\nStevens, W. Richard, and Stephen Rago.\nAdvanced Programming in the UNIX Environment\n. 2nd ed. Reading, MA: Addison-Wesley Professional, 2005. ISBN: 9780201433074.\n\nCalendar\n\nLEC #\n\nTOPICS\n\nKEY DATES\n\nIntroduction and O/S review\n\nI/O concurrency and event-driven programming\n\nEvent-driven programming (cont.)\n\nLab 1 due\n\nNetwork file system\n\nRPC transparency\n\nLab 2 due\n\nCrash recovery\n\nTeam list due\n\nLogging\n\nLab 3 due\n\nCache consistency and locking\n\nProject proposal due\n\nMemory consistency\n\nLab 4 due\n\nFirst project conferences\n\nMemory consistency (cont.)\n\nVector timestamps and version vectors\n\nLab 5 due\n\nQuiz 1\n\nTwo-phase commit\n\nPaxos\n\nViewstamped replication\n\nHarp\n\nFirst draft of report due\n\nSecond project conferences\n\nFrangipani\n\nScalable lookup\n\nSecond draft of report due\n\nWide-area storage\n\nQuiz 2\n\nHacking day (no class)\n\nProject demonstrations\n\nContent distribution\n\nDistributed computing\n\nProject reports due",
  "files": [
    {
      "category": "Exam",
      "title": "quiz1_05_ans.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/a8bf752dc3de02c7dc40147c55965c73_quiz1_05_ans.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.824 Spring 2005\nMidterm Exam Answers\nNumber of students\nScore in Range [x,x+10)\nThe average is 117. The standard deviation is 19.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nI Part One\n1. [10 points]:\nSuppose you're running Mach 3.0 on a MIPS R3000. You're getting tired of\nhow slow your 15-year-old CPU is. You hear that Anderson et al., authors of the Interaction pa\nper, have started a company that sells a MIPS R3000-compatible CPU called the Mathom2000. The\nMathom2000 runs operating system primitives about 100 times as fast as the R3000, and application\ncode at the same speed as the R3000. If you upgraded to the Mathom2000, approximately how much\nfaster overall would your programs run? Explain your answer based on evidence presented in the\nInteraction paper.\nBased on Table 7, applications spend 5-20% of their running time in OS primitives. On the\n.05x\n.2x\nMathom2000, a program that runs in x seconds on the R3000 will take 100 + .95x to 100 +\n.8x = .9505x to .802x seconds. The corresponding speedup is approximately (1-.95)x 100 to\nx\n(1-.8)x 100 5 - 20%. If my programs follow similar behavior as the ones in Table 7, I can\nx\nexpect an overall speedup of 5-20% when I upgrade to the Mathom2000.\n2. [10 points]:\nLook at Figure 3 of Backtracking Intrusions, by King and Chen. The paper says\nthat GraphGen ignores the event at time 7. Why is that correct? Why couldn't the contents of file 2\nhave been part of the attack?\nThe result of the attack is that file X has the wrong contents, so GraphGen should include all\nevents that can affect the contents of file X. The event at time 6 is the one that directly changes\nthe contents of file X. The event at time 7 does not affect the contents of file X because it happens\nafter the event that directly modified the contents of file X. The contents of file 2 were not read\nat any point in time before the write to file X or the creation of any processes that affect file X.\nFile 2 might have been part of the attack, but its contents did not have any effect on the contents\nof file X.\nII Part Two: Porcupine\n3. [10 points]:\nLook at Figure 6 of the Porcupine paper by Saito et al. When skew is zero, most\nof the policies process about 700 to 800 messages per second. Suppose the CPUs were replaced with\nCPUs ten times as fast. Approximately how many more messages per second would the system be\nable to process? Why?\nEach of the nodes in the Figure 6 experiments has a single slow disk. Therefore, the disk is\nmost likely to be the bottleneck at each node. According to Table 1, the CPU utilization of a\nsingle server running at maximum throughput is only 15% in the no replication case. Hence,\nincreasing the speed of the CPUs will increase the message throughput by less than 15%.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. [10 points]: Look at the S1 and S2 policies in Figure 6 when skew is 1.0. As you can see, the data\npoints are hard to read. Based on the explanations in the rest of the paper, how many messages/second\nwould you expect S1 to process with skew of 1.0? And how much faster than S1 would you expect\nS2 to be? Explain your answers.\nWith skew of 1.0, all messages are handled by one machine, so the result for S1 should be the\nsame as for the maximum throughput for one node with a single disk with no replication, which\naccording to Figure 4 is 23 messages/second. S2 should double the message throughput of S1,\nsince S2 spreads the load over two servers.\n5. [10 points]: Suppose you run a Porcupine experiment like D1 in Figure 6 with skew of 1.0.\nHowever, when the system is first started all the users' mailbox fragments are placed on the same\nserver, node N0. Towards the beginning of the experiment, approximately how many messages/second\nwill the Porcupine cluster be able to process? Towards the end? If there is a change in performance,\nexplain when this change occurs, and what specific steps Porcupine takes as it is running that lead to\nthe change.\nTowards the beginning, the throughput should be similar to the maximum throughput of a single\nnode, 23 messages/second, because all messages are handled by node N0. The D1 policy will only\nappend new mail to a user's existing mailbox fragment. However, after a user deletes all mail,\nwhich deletes the fragment, the next arriving mail message will be placed in a new fragment on\nan unloaded server. This gradual load-balancing causes throughput to increase to the level of\nordinary D1 with skew 1.0.\n6. [10 points]: Figure 6 suggests that the dynamic spread policy is only useful when skew is high.\nThe authors generate the high-skew workloads by using user names that all hash to the same value.\nHow valuable would dynamic spread be in real life? Are there any specific situations, that are likely\nto occur, in which the dynamic spread policy would significantly increase performance?\nDynamic spread would be useful if a small fraction of the users received a large fraction of the\nmail.\nIII Part Three: OK Auctions\nYou've been hired as a consultant by OK Auctions to help make their web site secure. OK Auctions runs an\non-line auction system. At any given time there are thousands of auctions in progress. Users can log into\nthe system, get a list of current auctions, and place bids on one or more auctions. Each auction lasts for a\nfew days; when it ends, the highest bidder wins, and must pay the amount of his or her bid.\nOK Auctions' most pressing security concern is to ensure that users do not learn each others' bids while an\nauction is in progress. OK Auctions wants to prevent users from placing bids that are only slightly higher\nthan the existing high bid, since that would decrease winning bid values and thus profits. They are worried\nthat someone might exploit a bug in their software and steal high bids.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nOK Auctions current web site consists of a single process running on a UNIX server. The process accepts\nHTTP connections, checks usernames and passwords, maintains the list of all auctions, and maintains the\nstate of each auction (including the current high bid). The process keeps all this information in memory (it\ndoesn't use a separate database).\nAfter reading Krohn's OKWS paper, OK Auctions decides to use OKWS and to split their software into\nmultiple service programs. The LOGIN service will display a web page asking for a username and password,\nand check that the results are valid. The LIST service will display a list of all auctions. There will be one\nAUCTION service process per auction. Each AUCTION service will keep track of the current high bid for\nthe auction, will display a page asking for a new bid to any authenticated user, and will update its internal\ncurrent high bid if the new bid is higher. The service processes keep the state they need in memory (they\ndon't use a separate database).\n7. [10 points]:\nWill the use of OKWS with the LOGIN/LIST/AUCTION services make OK\nAuctions' web site more secure? If yes, explain how this arrangement would make it harder for a user\nto steal the high bid of an auction; if no, explain why attacks that work with the single-process design\nare still likely to work in the LOGIN/LIST/AUCTION arrangement.\nYes. Since the LOGIN/LIST/AUCTION services are separate processes, each with its own ad\ndress space, a compromised LOGIN or LIST process would not be able to read bids from an\nAUCTION process. On the other hand, a bug in the AUCTION service could still reveal the\nhigh bid.\n8. [10 points]:\nSuggest a partition of functionality among service processes that would be more\nsecure, and explain why it's more secure. Outline an attack that might have successfully stolen a\ncurrent high bid in the LOGIN/LIST/AUCTION arrangement that would fail with your arrangement.\nSplit the AUCTION service into two processes. The first process should talk to the user, accept\ning the input bid and formatting the response. The second process should hold the current high\nbid, and talk to the first process through an RPC interface that only accepts a \"bid\" RPC. The\nresponse to this RPC should not contain the current high bid. As a result, a bug in the first pro\ncess cannot result in the attacker learning the high bid. It's unlikely that the second process's\nnarrow RPC interface would have an exploitable bug (though not impossible).\nIV Part Four: Lab Four\nLarry Locker is working on adding locks to his file server for Lab 4. Remember that Lab 4 had locking but\nno caching of blocks or locks (no REVOKE, no flush()). The point of the locking is to serialize concurrent\nNFS RPCs that affect the same file or directory, so that the final results are as if the RPCs executed one at a\ntime. Thus, for example, concurrent CREATEs of the same file name would not result in two separate files\nwith the same name.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nLarry is working on SETATTR. He knows that at the end of the SETATTR he must release the lock on the\nfile handle, reply to the RPC, and put() the attributes to the block server. But he's not sure what the right\norder is.\n9. [10 points]:\nPlease write \"yes\" after each of the following orders that would result in proper\nserialization of NFS RPCs, and \"no\" after each order that would not. You can assume that Larry would\nimplement each order without bugs (for example by using callbacks to ensure the correct sequence).\nAny sequence where release follows put is correct.\nrelease, reply, put\nno\nrelease, put, reply\nno\nreply, release, put\nno\nreply, put, release\nyes\nput, release, reply\nyes\nput, reply, release\nyes\n10. [10 points]:\nLarry stores everything about a file (both attributes and contents) in a single\nblock, whose key is the file handle. Larry observes that the READ RPC does not modify the file or its\nattributes, and thinks that means he doesn't have to acquire any locks in READ. Is Larry right? Explain\nwhy or why not, or what extra information you need to decide. If Larry is wrong, describe a specific\nsituation in which lack of locking would lead to an incorrect file system state; include a description\nof the incorrect state and what the correct state should be. Remember that Larry is working on Lab 4,\nnot Lab 5.\nHe is right, assuming that other RPCs (such as WRITE) update the block with a single put. In\nthat case a concurrent READ and WRITE will result in the READ seeing the state either before\nor after the WRITE, but not an intermediate invalid state.\n11. [10 points]:\nLarry also thinks he doesn't need to acquire any locks for WRITE. After all,\nWRITE replaces bytes in the file: it doesn't need to know the old values of the bytes. Is Larry right?\nExplain why or why not, or what extra information you need to decide. If Larry is wrong, describe\na specific situation in which lack of locking would lead to an incorrect file system state; include a\ndescription of the incorrect state and what the correct state should be.\nWrong. For example, there are concurrent writes to the same file (from different ccfs processes).\nOne write updates bytes 0 to 19, and the other write updates bytes 20 through 39. If there is no\nlocking, the resulting file will contain new bytes 0-19 or 20-39, but not both. The correct result\nis to see all 40 new bytes.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n12. [10 points]:\nLarry is considering a different design in which he stores the attributes in one\nblock and the file contents in a different block. The file content block's key would be the file handle\nwith an \"x\" added to the end. Does this design change whether READ needs to lock? How about\nWRITE? Explain your answers.\nREAD now needs to lock, since a process writing to a file might put the attribute block before\nthe file content block. If a READ RPC happens in between the two PUTs, then the reader will\nsee attributes that are inconsistent with the file data.\nWRITE still needs a lock for the same reason as the previous question, and also because the\nWRITE's two puts should be atomic.\nV Part Five: XOM\nThe XOM paper by Lie et al. describes store secure and load secure instructions that use encryption and\nhashing to protect data stored in off-chip RAM (see Sections 2.1 and 3).\n13. [10 points]: XOM's stated goal is to copy-protect programs. Why does XOM need to protect\ndata as well as instructions? Explain an example application in which copy-protected instructions are\nnot useful without protected data.\nSuppose the application checks with a license server to see if this CPU is allowed to run the\napplication. The application is likely to read and write variables in memory as part of the\nlicense decision. If the user could modify these variables, the user might be able to trick the\nprogram into believing that the user was allowed to run the application.\n14. [10 points]:\nSuppose a XOM program in compartment 17 executes store secure, giving an\naddress of 1024 and a 32-bit data value of 55. Assume that a cache block holds just one 32-bit data\nvalue. What information will XOM store in RAM as a result? Write each distinct value stored in\nRAM on a different line below (there may be fewer than four). For example, write \"L0: 1024 L1:\nHash(17, current time, 55)\" if you think XOM stores the value 1024, followed by the value computed\nby a cryptographic hash of the concatenation of the three indicated arguments.\nL0: 55 encrypted with compartment's session key\nL1: Hash(1024, 55, compartment's session key)\nL2:\nL3:\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n15. [10 points]:\nHow does XOM decide whether data it fetches from off-chip RAM is correct?\nAssume that the address given to load secure is Addr, and that the current compartment number is\nComp. Express the steps in terms of the locations from your previous answer. For example, you\nmight answer \"XOM accepts the data in L3 if L0 is equal to the current time and L1 is equal to\nComp.\"\nXOM loads the value at L0 and decrypts it; call the result v. XOM accepts v if the value at L1\nis equal to Hash(Addr, v, session key).\nEnd of Exam\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Exam",
      "title": "quiz2_05_ans.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/d931a3040d61a1173190b1ac59921b46_quiz2_05_ans.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.824 Spring 2005\nSecond Exam\nAnswers\nNumber of students\nScore in Range [x,x+10)\nThe average is 62. The standard deviation is 16.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nI Part One\n1. [10 points]: Why does Frangipani distinguish between read locks and write locks? Why is that\na better approach than having a single kind of lock?\nThe read locks allow multiple Frangipani servers to cache and use files and directories at the\nsame time. This concurrency will increase total performance for read-intensive workloads.\n2. [15 points]: Look at the pseudo-code in Section 3.1 of the Ivy paper (Li and Hudak's Memory\nCoherence in Shared Virtual Memory Systems). Observe this code at the end of the Write Server:\nIF I am manager THEN BEGIN\nlock(info[p].lock);\ninvalidate(p, info[p].copy_set);\nSuppose that the programmer implementing this code accidentally swaps the lock and invalidate, so\nthat the implementation looked like this:\nIF I am manager THEN BEGIN\ninvalidate(p, info[p].copy_set);\nlock(info[p].lock);\nExplain a specific situation in which this erroneous code would cause Ivy to produce incorrect memory\ncontents.\nHost H1 takes a write fault on a page, asks the manager for write access, and the manager\nexecutes the invalidate but has not yet locked. Host H2 takes a read fault on the same page, and\ncompletes the entire read process, which involves the manager adding H2 to copy set. Now the\nmanager gets the lock on behalf of H1's write, clears copy set, and gives ownership of the page\nto H1. Now H1 can write the page, which will result in H2 having a stale copy of the page.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nII Paxos\nThe Reliable Computing Corporation (RCC) has designed a fault-tolerant airline seat reservation service.\nTravel agents send reservation requests to the service over the Internet, and the service responds with either\na seat reservation or an error indication. Obviously it is vital that the service not assign the same seat to two\ndifferent passengers.\nRCC's system achieves fault-tolerance by replicating the reservation state on three servers. The state consists\nof a list of the reservation requests that have been granted, in the order that they were granted. Each entry in\nthe list contains the seat number, the passenger name, and a request ID included by the travel agent in each\nrequest message. The request ID allows the service to recognize retransmitted requests, and reply with the\ninformation from the record that's already in the list, rather than incorrectly allocating a new seat. A server\ncan tell which seats have already been reserved by scanning the reservation list.\nRCC uses a replicated state machine to ensure that the servers have identical reservation lists. The three\nservers use Paxos (see the notes for Lecture 15) to agree on each new reservation to be added to the list.\nThey execute Paxos for each reservation (rather than just using Paxos to agree on a new view and primary\nafter each server failure). Each value that the servers agree on looks like \"reservation list entry number 37\nassigns seat 32 to passenger Robert Morris with request ID 997.\" When a server receives a request from a\ntravel agent, it scans its reservation list to find the next available seat and the next list entry number, and uses\nPaxos to propose the value for that list entry number. The system runs a separate instance of Paxos to agree\non the value for each numbered list entry.\nThe three servers are S1, S2, and S3. S1 receives a request from a travel agent asking for a seat for Aaron\nBurr. S1 picks Paxos proposal number 101 (this is the n in Lecture 15). At about the same time S2 receives\na request asking for a seat for Alexander Hamilton. S2 picks Paxos proposal number 102. Both servers look\nat their reservation lists and see that the next entry number is 38, and that the next seat available is seat 33.\nBoth servers start executing Paxos as a leader for reservation list entry number 38.\nEach of the three sequences below indicates the initial sequence of messages of a possible execution of Paxos\nwhen both S1 and S2 are acting as leader. Each message shown is received successfully. The messages are\nsent and received one by one in the indicated order. No other messages are sent until after the last message\nshown is received. Your job is to answer these questions about the final outcomes that could result from\neach of the initial sequences: Is it possible for the servers to agree on Aaron Burr in seat 33 as entry 38? Is\nit possible for them to agree on Alexander Hamilton in seat 33 as entry 38? For each of these two outcomes,\nhow it could occur or how does Paxos prevent it from occurring?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nS1 -> S1 PREPARE(101)\nS1 -> S1 RESPONSE(nil, nil)\nS1 -> S2 PREPARE(101)\nS2 -> S1 RESPONSE(nil, nil)\nS1 -> S3 PREPARE(101)\nS3 -> S1 RESPONSE(nil, nil)\nS2 -> S1 PREPARE(102)\nS2 -> S2 PREPARE(102)\nS2 -> S3 PREPARE(102)\n... the rest of the Paxos messages.\n3. [5 points]:\nAnswers: Only Alexander Hamilton, because S2's PREPARE will cause all\nservers to ignore S1's subsequent ACCEPT.\nS1 -> S1 PREPARE(101)\nS1 -> S1 RESPONSE(nil, nil)\nS1 -> S2 PREPARE(101)\nS2 -> S1 RESPONSE(nil, nil)\nS1 -> S3 PREPARE(101)\nS3 -> S1 RESPONSE(nil, nil)\nS1 -> S3 ACCEPT(101, ''Aaaron Burr...'')\nS2 -> S1 PREPARE(102)\nS2 -> S2 PREPARE(102)\nS2 -> S3 PREPARE(102)\n... the rest of the Paxos messages.\n4. [5 points]: Answers: Either Aaron Burr or Alexander Hamilton. If S2 hears a RESPONSE\nfrom S3, S2 will propose Aaron Burr. If S2 does not hear a RESPONSE from S3, S2 will propose\nAlexander Hamilton.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nS1 -> S1 PREPARE(101)\nS1 -> S1 RESPONSE(nil, nil)\nS1 -> S2 PREPARE(101)\nS2 -> S1 RESPONSE(nil, nil)\nS1 -> S3 PREPARE(101)\nS3 -> S1 RESPONSE(nil, nil)\nS1 -> S3 ACCEPT(101, ''Aaaron Burr...'')\nS1 -> S1 ACCEPT(101, ''Aaaron Burr...'')\nS2 -> S1 PREPARE(102)\nS2 -> S2 PREPARE(102)\nS2 -> S3 PREPARE(102)\n... the rest of the Paxos messages.\n5. [5 points]:\nAnswers: Only Aaron Burr. Any majority of RESPONSEs that S2 hears in\nPhase 2 will include the value Aaron Burr, so S2 will propose Aaron Burr.\n6. [10 points]: Suppose one of the RCC servers has received an ACCEPT for a particular instance\nof Paxos (i.e. for a particular reservation list entry), but never received any indication about what\nthe final outcome of the Paxos protocol was. Outline what steps the server should take to figure out\nwhether agreement was reached and to find out the agreed-on value. Explain why your procedure is\ncorrect even if there are still active leaders executing this instance of Paxos.\nThe server should send messages to all the servers asking them whether they accepted an AC\nCEPT message. If a majority of them respond with the same value, then agreement has been\nreached. This procedure is safe even if there are still active leaders because any such leader\nmust hear from someone from that majority in Phase 2. (It's actually more complex than this;\na real argument would include an inductive proof that there cannot be a minority value with\nhigher n number.)\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nIII FAB\n7. [15 points]:\nSuppose that a FAB system has three bricks, b1, b2, and b3. There is just one\nseggroup, and it contains all three bricks. FAB is using replication, as detailed in Figure 4 of the FAB\npaper. Block x starts out containing value 100 (all replicas have value 100, and all replicas have ordTs\nequal to valTs). Coordinator c1 starts to execute write(200) for block x, but crashes after it sends the\nWrite message to b1 and before it sends Writes to the other two bricks. b1 receives and processes\nthe Write. This is the only execution of the write() function by any coordinator. Then coordinator c2\nperforms two read()s for block x, the second starting after the first completes, and prints the resulting\npair of values. For each pair below, indicate whether that pair is possible, and explain how it could\narise or how FAB prevents that pair of read values from occurring.\n100 100: Yes. The first read apparently didn't hear from b1, but it would have seen that ordTs and\nvalTs were not equal, repaired b2 and b3, and the repaired valTs would be greater than the valTs on\nb1. Thus the second read would prefer the repaired 100 even if the second read heard from b1.\n100 200: No. In order for the first read to return 100, it must have performed a repair. The repair\nwould have guaranteed that the second read returned 200.\n200 100: No, for reasons similar to the previous question.\n200 200: Yes. The first read must have heard an Order reply from b1 and repaired to 200.\n8. [10 points]: You're the system administrator of a FAB with three replicated bricks. Your users\nuse the FAB at all times to read and write data. You accidentally step on the Ethernet cable of one of\nthe bricks, which permanently damages the cable and causes the brick to lose its network connection.\nIt will take you about fifteen minutes to run out to CompUSA and buy a replacement Ethernet cable.\nBefore you leave for CompUSA, you can either start a reconfiguration (as in Section 5) or not start a\nreconfiguration. Which is the best plan? Explain your reasoning. If you would need more information\nto answer, explain what that information is and how you would use it to decide.\nIf you don't reconfigure, the system will keep serving reads and writes even with the missing\nserver, since only a quorum is required. The two live servers will keep timestamps for each\nwritten block in NVRAM, so that after the Ethernet cable is repaired it will be clear that the\nrepaired server's data is out of date.\nIf you reconfigure, then when you repair the Ethernet cable you'll have to reconfigure again (to\nget back up to three replicas), and the second reconfigure will require a copy of the entire disk\nto the repaired server. This could take a long time. So the only reason to reconfigure is if the two\nlive servers run out of NVRAM in which to store timestamps. Thus the information you need to\nanswer the question is how long it will take for the NVRAM to fill up.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nIV Ruthenian Consistency\nYou are a member of the Ruthenian People's Popular Front (RPPF), whose aim is to plot the overthrow\nof the oppressive government of Ruthenia. The RPPF is organized as a large number of \"cells,\" each cell\nconsisting of three members. Each cell knows the members of a few other cells, but no-one knows all the\nmembers. However, everyone knows how many cells there are, and each cell knows its own unique ID (a\nnumber between 1 and the total number of cells). The total number of cells is constant. The cell structure is\nintended to make it hard for the government to arrest the whole RPPF, even if they have arrested a few of its\nmembers.\nFrom time to time various RPPF cells generate instructions that they want to communicate to every other\ncell. Cells that know each other can exchange messages written on slips of paper. Thus a message will need\nto be forwarded between many cells before every cell has a copy.\nThe RPPF is worried about cells being confused by deceptive orders of arrival of messages. They are\nparticularly anxious to ensure that cells can recognize situations in which two messages are originated\nsimultaneously, by cells that were not aware of the other message. The reason is as follows. Suppose cell\nc0 receives two messages m1 and m2 that are originated by different cells, and that m1 and m2 contain\nconflicting instructions. If the originator of m2 knew about m1 before it generated m2, then c0 should\nignore m1 and pay attention to m2. But if m1 and m2 were generated without knowledge of each other, then\nc0 will know that it should exercise extraordinary care when interpreting the two sets of instructions.\nMore formally, the RPPF wants every cell be able to decide the relative order in which any pair of messages\nwas originated. Suppose cell c0 has a copy of message m1 originated by cell c1, and m2 originated by c2.\nCell c0 should be able to decide whether c2 had a copy of m1 when it originated m2, or c1 had a copy of m2\nwhen it originated m1, or neither had a copy of the other message. Call these situations \"m2 is after m1,\"\n\"m1 is after m2,\" and \"m1 and m2 are concurrent.\"\nYour job is to design an ordering system for the RPPF.\nEach cell is allowed to keep a copy of each message it sees, and can also maintain any state it can derive\nfrom those messages. The only form of communication allowed to a cell is passing written notes to the cells\nit knows about. Your solution should continue to operate even if the Ruthenian government eliminates a few\ncells, so it should not depend on any special cells. You can assume that the government is not aware of your\ncommunication system.\nYou should try to minimize the amount of communication required by your system; it would not be good if\neach message had to include a huge amount of book-keeping information.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n9. [25 points]: Please explain how your ordering system works. You should describe the contents\nof each message, the state that each cell must keep, any communication protocol the cells must adhere\nto, and the algorithm a cell should use to decide whether one messages is after another or whether two\nmessages are concurrent.\nThe hard part is deciding whether two messages are concurrent. A simple scheme is for the orig\ninating cell to include a complete list of all messages it has received in each message it originates.\nThen any cell can decide if two messages are concurrent by noticing that neither message's list\nmentions the other message. This solution requires a lot of communication to carry the lists,\nand the lists will grow in size as time passes.\nIf the messages originated by each cell arrive in order at every other cell, a cell can summarize\nwhat it has seen by just reporting the highest message number it has seen originated from each\nother cell. That is, a cell should include a version vector in each message it originates. This\nscheme requires less communication, but must include a protocol that ensures that each cell\nsees each other cell's messages in order.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nEnd of Exam\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "get_started.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/a5ba851aa5a214ddfa9bb2f1ee378113_get_started.pdf",
      "content": "6.824 - Spring 2006\nGetting started with 6.824 labs\n\nIntroduction\nThe 6.824 labs require you to program C/C++ on FreeBSD machines. This document\nshould help you on your way with logging into the machines, reading manual pages,\ncompiling C/C++ programs, and understanding Makefiles that we will give you.\nLogging in with ssh\nAfter you get your login and password, you can log into class machines using ssh, which\nis installed on Athena. Other Unix versions are available on http://www.openssh.com/.\nWindows versions come with Cygwin. If you just want ssh for Windows without the\nfancy stuff, you can also just install PuTTY. You can download NiftyTelnet SSH for\nMacOS at http://www.lysator.liu.se/~jonasw/freeware/niftyssh/, or just use command line\nssh from the Terminal.app.\n% ssh student@pain.lcs.mit.edu\nwill log you in as student on machine pain.lcs.mit.edu. (Don't type the ``%''!)\nNeedless to say, you need to replace student with the username you receive from us\nwhen you register for lab. If there's a problem with pain.lcs.mit.edu, you can check out\nthe list of class machines.\nIf you are the adventurous type, you can set up your ssh so that you can log in without\nhaving to type a password each time. (The ssh man page may also be useful here.)\nFinding and reading manual pages\nIf you need to find information on Unix commands, system calls, configuration files, etc.,\nthen man is your friend. For example,\n% man socket\ngives you the man page for the socket system call. (man man gives you the manual page\nfor man itself.) Note that it may be necessary to specify which section of the manual you\nwish to read. For example, man read may yield the man page for the wrong read---the\nshell read rather than the read system call. If this is the case, try man 2 read. In this\ncase, 2 specifies the manual section; there are 8 sections. You can find relevant man\npages (and what section they are in) using man -k. For example,\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n% man -k read\nshows a one-line description for each man page that has the word ``read'' in the\ndescription.\nCompiling and linking simple programs\nA compiler takes one or more source files and produces object files that contain machine\ncode. Object files contain machine code and usually have a .o extension. C source files\nusually have a .c extension and g++ files usually have a .C or .cc extension. Object files\nmay have undefined symbols (names of functions or global variables); linking object files\ninto an executable resolves these undefined symbols. Note that one of the object files\nmust have a main() function to create an executable.\nObject files can be linked together to form an executable, provided that one (and only\none) of the object files defines a main() function.\nAn executable is similar to an object file in that it contains machine code, but an\nexecutable contains some additional bits that allow the operating system to start the\nprogram and run it.\nCompiling a source file into the equivalent object file is done using g++ -c. For example:\n% g++ -c hello.C -o hello.o\nThe -c flag indicates pure compilation without linking. Even if hello.C defines main(),\nthe resulting hello.o is not executable. To compile a source file into an executable, omit\nthe -c flag:\n% g++ hello.C -o hello\nThis creates an executable hello. Note that g++ will bark at you if you try to create an\nexecutable without a definition of main().\nNote that all these examples assume you are compiling g++ files. Use gcc instead of g++\nif you are compiling plain old C files.\nMultiple object files can be linked together into one executable. Suppose we have the\nfollowing files:\ncommon.h\n#ifndef __COMMON_H\n#define __COMMON_H\nvoid hello();\nvoid bye();\n#endif // __COMMON_H\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nhello.C\n#include <stdio.h>\n#include \"common.h\"\nvoid hello() {\nprintf(\"hello!\\n\");\n}\nbye.C\n#include <stdio.h>\n#include \"common.h\"\nvoid bye() {\nprintf(\"bye!\\n\");\n}\nmain.C\n#include \"common.h\"\nint main() {\nhello();\nbye();\n}\nTo turn these files into a working executable, do:\n% g++ main.C hello.C bye.C -o hellobye\nThis compiles main.C, bye.C, and hello.C and links them together into an executable\ncalled hellobye. The header file common.h declares---but does not define---hello() and\nbye() so that the compiler can compile main.C, without knowing the details of their\nimplementation.\nLinking is only possible when no two object files try to define C or g++ functions or\nglobal variables with the same name. For example, try declaring a variable int foo in\nboth hello.C and bye.C. Compiling each file separately is no problem, but linking fails\nwith a multiple definition of `foo' error. The same is true for functions.\nDepending on what you want, you can avoid this problem in one of a number of ways:\n-\nIf you want hello.C and bye.C to share variable foo, you can declare one of\nthem as extern int foo.\n-\nIf you want hello.C and bye.C to each have their own independent variable\nnamed foo, then declare one or both of them as static int foo. Keyword\nstatic limits the scope of the declaration to just the current file.\n-\nThe best way to solve this problem is to avoid global variables altogether; use\nfunction arguments, local variables, or g++ class members instead.\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nMakefiles\nWhile doing the 6.824 labs you probably don't need to create your own Makefiles from\nscratch, because we will give them to you. However, you may need to modify them. This\nsection gives a quick overview of what a Makefile is and how it is used.\nA Makefile is used in conjuction with gmake and describes how different source and\nheader files in a project relate and how to compile and link them. A Makefile usually\nbears the meaningful name Makefile and its contents are probably roughly similar to this\nexample:\nMakefile\nhellobye : libhellobye.a main.o\ng++ main.o -L. -lhellobye -o hellobye\n\nlibhellobye.a : hello.o bye.o\nar cru libhellobye.a hello.o bye.o\n# this command puts hello.o and bye.o in a library libhellobye.a\n\nhello.o : hello.C common.h\ng++ -c hello.C -o hello.o\n\nbye.o : bye.C common.h\ng++ -c bye.C -o bye.o\n\nmain.o : main.C common.h\ng++ -c main.C -o main.o\n\nclean :\nrm -f *.a *.o hellobye\nThis Makefile has 6 sections. Each section describes a dependency on the first line and\nsome action on the second. A dependency consists of a target file (before the colon) and\nprerequisite files (after the colon). Lines starting with # are comments.\nWhen you run gmake, it will execute a target's action if any of the prerequisite files have\nbeen modified since the target was last modified. gmake executes the action in much the\nsame way as if you had typed it to the shell. gmake only tries to make sure that the very\nfirst target in the Makefile is up to date. However, if the first target depends on other\ntargets, gmake may end up executing multiple actions.\nThe first section in the Makefile above states that the file hellobye depends on\nlibhellobye.a and main.o. If either libhellobye.a or main.o is newer than\nhellobye, then hellobye is rebuilt. Building the whole project is now as simple as\ntyping\n% gmake\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nIf you write your Makefile correctly, gmake will only compile and link those parts of a\nproject that have changed since the last gmake. If your project is big and the full\ncompile/link process takes a long time, then gmake is your friend. It saves time.\nNotice that the last section in this Makefile is not a real dependency, but allows the user\nto conveniently call\n% gmake clean\nto remove all object files, the library, and the executable.\nMakefiles can get pretty complicated, but this gentle introduction may very well suffice\nfor your 6.824 needs. If you're interested, read the full gmake manual (info make). It's\nfabulous.\nVersion Control\nYou may find it useful to keep track of different versions of your work as you progress. A\nversion control system allows you to checkpoint your work and may help you track down\nregressions in your code. The change log tracked by the VCS can serve to remind you\nwhat your intention was in making a particular change days or weeks afterwards.\nThe lab machines have several popular version control systems available including CVS,\nSubversion, and darcs. If you are familiar with one of these systems, you are welcome to\nuse it. If you are interested in learning one, you ought to be able to get started relatively\nquickly using darcs. There is copious documentation for each of these tools available on\nthe web.\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "lab1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/249a3c224b01f2547ee25a3971ce2a73_lab1.pdf",
      "content": "6.824 - Spring 2006\n6.824 Lab 1: Lock Server\nDue: Lecture 3\n\nIntroduction\nThis is the first in a sequence of labs in which you'll build a multi-server file system in\nthe spirit of Frangipani. In this lab you'll supply the logic for a lock server that you'll use\nin subsequent labs to keep multiple file servers consistent.\nAt the end of all the labs, your file server architecture will look like this:\n---------------- -------------\n| | | |\n| App FS---|-----| block srvr|----- FS on other hosts\n| | | | | | | |\n|--------------| | ------------- |\n| | | | | ------------- |\n| Kernel | | | | |\n| NFS Client | ---| lock srvr |---\n| | | |\n---------------- -------------\nYou'll write a file server process, labeled FS above, using the loop-back NFS toolkit.\nEach client host will run a copy of FS. FS will appear to local applications on the same\nmachine by pretending to be an NFS v3 server. The FS server will store all the file\nsystem data in a block server on the network, instead of on a disk. FS servers on multiple\nclient hosts can share the file system by sharing a single block server.\nThis architecture is appealing because (in principle) it shouldn't slow down very much as\nyou add client hosts. Most of the complexity is in the per-client FS server, so new clients\nmake use of their own CPUs rather than competing with existing clients for the server's\nCPU. The block server is shared, but hopefully it's simple and fast enough to handle a\nlarge number of clients. In contrast, a conventional NFS server is pretty complex (it has a\ncomplete file system implementation) so it's more likely to be a bottleneck when shared\nby many NFS clients. The only fly in the ointment is that the FS servers need a locking\nprotocol to avoid inconsistent updates. In this lab you'll implement the core logic of a\nlock server.\nGetting Started\nWe provide you with a skeleton RPC-based lock server, a lock client interface, a sample\napplication that uses the lock client interface, and a tester. You should fetch this code\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nfrom http://pdos.csail.mit.edu/6.824-2006/labs/lab-1.tgz to one of the lab machines using\nwget. pain% wget -nc http://pdos.csail.mit.edu/6.824-2006/labs/lab-1.tgz\npain% tar xzvf lab-1.tgz\npain% cd lab-1\npain% gmake\nNow start up the lock server, giving it a port number on which to listen to RPC requests.\nYou'll need to choose a UDP port number that other students aren't using. For example:\npain% ./server1 3772\nNow open a second window, log into one of the 6.824 lab hosts, and run lock_demo,\ngiving it the host name and port number on which the server is listening:\nfrustration% cd ~/lab-1\nfrustration% ./lock_demo pain 3772\nAsking for the lock...\nGot the lock. Sleeping...\nReleasing the lock.\nfrustration%\nlock_demo asks the lock server for a lock, waits for the server to grant the lock, pauses\nfor a few seconds, and then releases the lock back to the server.\nThe lock server we've given you always grants lock requests immediately, and ignores\nrelease RPCs. As a result it will give the same lock to multiple clients, which makes for a\npretty poor locking system. You can see this for yourself by starting two lock_demo\nprograms in two different windows at the same time -- you will see that they both say\n\"Got the lock\" at the same time, rather than one waiting for the other to release.\nWe have supplied you with a program lock_tester that tests whether the server grants\neach lock just once at any given time. You run lock_tester with the same arguments as\nlock_demo. A successful run of lock_tester (with a correct lock server) will look like this:\nfrustration% ./lock_tester pain 3772\nAcquire a, release a, acquire a, release a:\nAcquire a, acquire b, release b, release a:\nAcquire a, acquire a, release a, release a:\nAcquire a and b from two clients:\nlock_tester: passed tests\nIf your lock server isn't correct, lock_tester will print an error message. For example, if\nlock_tester complains \"error: server granted lock 61 which we already hold!\", the\nproblem is probably that lock_tester sent two simultaneous requests for the same lock,\nand the server granted the lock twice (once for each request). A correct server would have\nsent one grant, waited for a release, and only then sent a second grant. The 61 is the lock\nname \"a\" in hex.\nRequirements\nYour job is to modify lock_server.C and lock_server.h so that they implement a correct\nlock server. Intuitively, correctness means that the lock server grants a given lock to at\nmost one requester at a given time. More precisely, consider the sequence of grant and\nrelease RPCs that leave and arrive at the server for a particular lock. The grants and\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nreleases must strictly alternate in the sequence: there should never be two grants not\nseparated by a release.\nYour lock server must pass the lock_tester tests. We will test your lock_server with the\nstandard client code, not your copies of the client code. Thus you should not make any\nsignificant changes to files other than lock_server.C and lock_server.h.\nAbout the Lock Server\nThe lock server's RPC messages are defined in lock_proto.x. The protocol has three\nmessages: acquire, release, and grant. The client sends the acquire RPC to the server\nwhen the client wants a lock. The server sends a reply back, but the reply has no\nmeaning. At some point (perhaps immediately if the lock is free) the server will send the\nclient a grant RPC, indicating that the client now possesses the lock. The client replies\nimmediately to the grant RPC, but the reply has no meaning. When the client is done with\nthe lock, it sends a release message to the server.\nThe lock server can manage many distinct locks. Each lock has a name; a name is a string\nof bytes. Names might not be printable ASCII, which is why the lock software prints lock\nnames in hex rather than ASCII. The set of locks is open-ended: if a client asks for a lock\nthat the server has never seen before, the server should create the lock and grant it to the\nclient.\nlock_client.C and lock_client.h implement a client-side interface to the lock server. The\ninterface provides acquire() and release() functions, and takes care of sending and\nreceiving RPCs. See lock_demo.C for an example of how an application uses the\ninterface.\nDifferent modules in a single application might ask for the same lock at roughly the same\ntime. lock_client.C handles this by sending two separate acquire RPCs to the server.\nWhen one of the modules is done with the lock, lock_client.C releases the lock back to\nthe server and waits for a second grant.\nTips\nCoding plan\nYou'll need to modify class LS in lock_server.h to keep track of which locks are currently\ngranted and which clients are waiting for locks. There is just one instance of class LS in\nthe lock server.\nYou'll need to modify the LS::acquire() function in lock_server.C to check the status of a\nlock when an acquire RPC arrives, and only send a grant if the lock isn't currently held.\nacquire() should also update your record of which locks are held and what clients are\nwaiting.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nYou'll need to modify LS::release() in lock_server.C in order to update the lock's status,\nand to potentially send a grant RPC to a waiting client.\nTo give you a feel for how much work should be involved, our solution adds about 40\nlines of code to lock_server.C.\nlibasync\nBoth lock_client.C and lock_server.C are written in an event-driven style using the SFS\nlibasync libraries. You can learn about event-driven programming in Section 4.4 of the\nNFS toolkit paper, or in Using Libasync (you can skip Section 2.2), or by reading the\nLibasync Tutorial. You can find the source for the libraries in /u/6.824/src/sfs1 or at\nthe SFS project page.\nYou may want to use the ihash template class to store state about the set of active locks,\nand the list template class to store the list of clients waiting for a lock. You can see\nexamples of their use in lock_client.h and lock_client.C.\nYou can declare an ihash hash table as follows:\nstruct entry {\nihash_entry<entry> hlink; // ihash uses this\nstr name; // the hash key of this entry\nint a_field; // data you want to store in the entry\nint another_field;\n};\nihash<const str, entry, &entry::name, &entry::hlink> table;\nihash arranges the entries in a linked list per hash bucket, and strings the items of each list\ntogether with the hlink field. To make C++ type checking happy, you have to explicitly\nmention the key type, the entry type, the name of the entry field that contains the entry's\nkey, and the name of the hlink field in the ihash<...> definition.\nAn ihash hash table has three operations defined on it. table[key] yields a pointer to the\nentry with the indicated key, or 0 if there's no such entry. table.remove(entry *) removes\nan entry, given a pointer to the entry. table.insert(entry *) inserts a new entry, given a\npointer to the new entry. You'll need to allocate the entry with the C++ new operator, like\nso:\nentry *e = new entry();\n// initialize e, particularly e->name\ntable.insert(e);\nHere's an example of a definition of a list, called lst:\nstruct element {\nlist_entry<element> llink;\n// things you want to put in each list element...\n};\nlist<element, &element::llink> lst;\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nThe methods implemented by list include insert_head(element*) and remove(element*).\nThe following code iterates through the elements of the list called lst. If you modify the\nlist inside the loop, then you should save a copy of lst.next(i) before modifying the list.\nelement *i;\nfor(i = lst.first; i; i = lst.next(i)){\n...;\n}\nAn alternative to the list class is the vec class which is another way to manage a dynamic\nlist of elements. Compared to list, vec does not require a structure with an internal llink\nobject. Thus, a vec is parameterized with only one argument: the class to be stored in the\nvec. vecs allow random access much like an array, elements can be added to the end of\nthe vec, and then can be removed from either the front or the back. The vec class\ninterface is specified in /u/6.824/include/sfs/vec.h.\nThe str class implements constant, garbage-collected (reference counted) variable-length\nstrings. The constructor str(\"xxxx\") creates a new str object initialized from the C string\n\"xxxx\". The str method cstr() returns a null-terminated C string pointing to the contents\nof the str object. Equality with == is defined on str objects, and returns true iff the two\nstrings contain the same bytes. str objects have an explicit length, so they can contain\nnulls.\nDon't modify a str. Don't modify the result of str.cstr(), since cstr() returns a pointer to the\nstr's internal storage.\nDebugging\nYou may be able to find memory allocation errors with dmalloc by typing this before\nrunning server1:\npain% dmalloc low -l dmalloc.log -i 10\nYou can tweak settings as desired; see the manual for more information (also available\nlocally via running \"info dmalloc\"). Your program must exit cleanly for dmalloc to\nproduce a report (into dmalloc.log).\nYou can see a trace of all RPC requests that your server receives, and its responses, by\nsetting the ASRV_TRACE environment variable. There is a corresponding\nACLNT_TRACE variable that displays RPCs made by a process and their responses.\nSince the lock server must both receive and make RPCs, you may want to run with both:\npain% env ASRV_TRACE=10 ACLNT_TRACE=10 ./server1 ...\nSmaller numeric values will display information less verbosely.\nCollaboration Policy\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nYou must write all the code you hand in for the programming assignments, except for\ncode that we give you as part of the assignment. You are not allowed to look at anyone\nelse's solution (and you're not allowed to look at code from previous years). You may\ndiscuss the assignments with other students, but you may not look at or copy each others'\ncode.\nHandin procedure\nYou should hand in the gzipped tar file lab-1-handin.tgz produced by gmake handin.\nCopy this file to ~/handin/lab-1-handin.tgz. We will use the first copy of the file that\nwe find after the deadline.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "lab2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/67387d705672a2c83be33c8b2f595db4_lab2.pdf",
      "content": "6.824 - Spring 2005\n6.824 Lab 2: Basic File Server\nDue: Lecture 5\n\nIntroduction\nIn this lab you'll start implementing the Frangipani-like file server originally outlined in\nLab 1:\n---------------- -------------\n| | | |\n| App FS---|-----| block srvr|----- FS on other hosts\n| | | | | | | |\n|--------------| | ------------- |\n| | | | | ------------- |\n| Kernel | | | | |\n| NFS Client | ---| lock srvr |---\n| | | |\n---------------- -------------\nWe supply you with a block server, block server client interface, and a skeletal file server\nthat knows little more than how to parse NFS RPC calls. Your job in this lab is to design\nand implement directories. You'll have to choose a representation with which to store\ndirectories in the block server and implement the CREATE, LOOKUP, and READDIR\nNFS RPCs. You will need to consult the NFS v3 specification for information about what\nthe NFS RPCs mean and what arguments/responses are used for each RPC. You can also\nconsult the book \"NFS Illustrated\" by Brent Callaghan.\nYou can learn more about NFS loopback servers and asynchronous programming in the\nloop-back NFS paper. You can find the sources for this software at www.fs.net or in\n/u/6.824/src/sfs1 and /u/6.824/src/classfs-0.1. The NFS protocol is specified in XDR in\n/u/6.824/include/sfs/nfs3_prot.x and the output of the RPC compiler is available as\nnfs3_prot.h.\nThe CCFS Server\nDownload the lab starter files from http://pdos.csail.mit.edu/6.824-2006/labs/lab-2.tgz to\nyour home directory on one of the class machines.\n% wget -nc http://pdos.csail.mit.edu/6.824-2006/labs/lab-2.tgz\n% tar xzvf lab-2.tgz\n% cd lab-2\n% gmake\nNow you should start the block server on one of the class machines. You'll need to\nchoose a UDP port number that other students aren't using. If, for example, you choose to\nrun the block server on host frustration on port 3772, you should type this on frustration:\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nfrustration% cd ~/lab-2\nfrustration% ./blockdbd 3772 &\nAt this point you can start up the file server, called ccfs. By default, the file server\ninitializes a new file system in the block server, chooses a new random file handle for the\nroot directory, and prints out the root handle. You can optionally specify an existing root\nhandle. (The block server keeps its state in memory, so you can't use a root handle after\nyou re-start blockdbd.)\nWhen ccfs starts, it mounts itself as a file server for a sub-directory under /classfs, as\nspecified on the command line. (Each student has their own view of /classfs so you do\nnot need to pick a unique directory name.) You must also tell ccfs the host name and port\nnumber of the block server so that it knows where to get and put data. The following\nexample starts ccfs and mounts a new filesystem on /classfs/dir using the block server\nyou just started on frustration:\npain% ./ccfs dir frustration 3772 &\nroot file handle: 2d1b68f779135270\npain% touch /classfs/dir/file\ntouch: /classfs/dir/file: Input/output error\npain%\nClearly the file server is not very functional. It implements a few NFS RPCs (FSINFO,\nGETATTR and ACCESS) and returns error replies for most other RPCs (e.g. CREATE,\nLOOKUP, and READDIR). When you're done with the lab, you should be able to run\ncommands like this in your file system:\npain% ./ccfs dir frustration 3772 &\nroot file handle: 01a2d816f726da05\npain% cd /classfs/dir\npain% ls\npain% touch a\npain% ls -lt\ntotal 0\n-rw-rw-rw- 0 root wheel 0 Feb 9 11:10 a\npain% touch b\npain% ls -lt\ntotal 0\n-rw-rw-rw- 0 root wheel 0 Feb 9 11:10 b\n-rw-rw-rw- 0 root wheel 0 Feb 9 11:10 a\npain%\nIf all goes well, your file server should also support sharing the file system on other hosts\nvia the block cache. So you should then be able to do this on frustration:\nfrustration% ./ccfs dir frustration 3772 01a2d816f726da05 &\nfrustration% cd /classfs/dir\nfrustration% ls -lt\ntotal 0\n-rw-rw-rw- 0 root wheel 0 Feb 9 11:10 b\n-rw-rw-rw- 0 root wheel 0 Feb 9 11:10 a\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nfrustration%\nThe extra argument to ccfs (01a2d816f726da05) is the root file handle printed by the ccfs\non pain. It tells ccfs on frustration where to look for the file system in the block server.\nYour Job\nYour job is to implement the LOOKUP, CREATE, and READDIR NFS RPCs in fs.C.\nYou must store the file system's contents in the block server, so that in future labs you\ncan share one file system among multiple servers.\nIf your server passes the tester (see below), then you are done. If you have questions\nabout whether you have to implement specific pieces of NFS server functionality, then\nyou should be guided by the tester: if you can pass the tests without implementing\nsomething, then don't bother implementing it. For example, you don't need to implement\nthe exclusive create semantics of the CREATE RPC.\nPlease modify only fs.C, fs.h and fsrep.x. You can make any changes you like to these\nfiles. Please don't modify the block server or its client interface (blockdbc.C and\nblockdbc.h), since we may test your file server against our own copy of the block server.\nTesting\nYou can test your file server using the test-lab-2.pl script, supplying your directory under\n/classfs as the argument. Here's what a successful run of test-lab-2.pl looks like:\npain% ./test-lab-2.pl /classfs/dir\ncreate file-69301-0\ncreate file-69301-1\n...\nPassed all tests!\nThe tester creates lots of files with names like file-XXX-YYY and checks that they\nappear in directory listings.\nIf test-lab-2.pl exits without printing \"Passed all tests!\", then it thinks something is wrong\nwith your file server. For example, if you run test-lab-2.pl on the fs.C we give you, you'll\nprobably see an error message like this:\ntest-lab-2: cannot create /classfs/dir/file-69503-0 : Input/output\nerror\nThis error message is the result of this line at the end of fs::nfs3_lookup(), which you\nshould replace with a working implementation of LOOKUP:\nnc->error(NFS3ERR_IO);\nThe Block Server\nThe goal of the block server is to provide a centralized storage location for all the data\nrepresenting your distributed filesystem, much like a hard disk would. In later labs you\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nwill be serving the same file system contents on multiple hosts, each with its own file\nserver. The only way they can share data is by reading and writing the block server.\nThe block server stores key/value pairs, with writes limited to a maximum of size of 8K\n(8192 bytes); Both keys and values are byte arrays; the block server does not interpret\nthem. The block server supports put(key,value), get(key), and remove(key) RPCs. Your\ncode will use the client interface provided by blockdbc.h and blockdbc.C. The skeleton\nfile server class already contains an instance of the client interface that you can use via\ndb->put() and db->get(). Have a look at fs::get_fh() and fs::put_fh() in fs.C for examples.\nThis block server is somewhat simplistic in that it only stores data in memory; if you\nrestart it, all the data previously stored will be lost.\nFile System Representation\nIn this lab you must choose the format for file and directory meta-data. Meta-data\nincludes per-file information (for example file length) and directory contents. In future\nlabs you'll have to choose a format in which to store each file's contents. The format you\nchoose for your data will need to reflect the 8K block-size limit of the block server.\nNFS requires a file system to store certain generic information for every file and\ndirectory, such as owner, permissions, and modification times. This information\ncorresponds to an i-node in an on-disk UNIX file system. The easiest way for you to store\nthis information is to store an NFS fattr3 structure in the block server, using the file\nhandle as the key; there is already some code provided that works along these lines. Then\nwhen an RPC arrives with the file handle as argument it is easy to fetch the\ncorresponding file or directory's information. The file server we give you uses the file\nhandle as key and an fattr3 structure as the block value, but you are free to change this\nsetup.\nThe other meta-data that you must store in the block server are the contents of each\ndirectory. A directory's content is a list names, each with a file handle. Keeping this\ninformation allows you to handle CREATE, LOOKUP and READDIR RPCs: The\nCREATE RPC must add an entry to the relevant directory's list, the LOOKUP RPC must\nsearch the list, and the READDIR RPC must return each entry from the list.\nSince you're storing this information in the block server, you have to choose a key under\nwhich to store the information, and a format for the information. We have provided you\nwith a suggested representation in fsrep.x and the XDR machinery for translating\nbetween an in-memory representation and one that can be sent to the block server:\nstr data; // Initialized from something like db->get ()\nfs_dirent dirent;\nif (!str2xdr (dirent, data)) {\nfprintf (stderr, \"Unable to unmarshal data\\n\");\nreturn;\n}\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n// Now can use dirent.allocated, dirent.fh and dirent.filename.\n\n// Create a new directory and add a directory entry\nfs_directory dir;\ndir.entries.push_back (dirent);\nstr dirrep = xdr2str (dir);\n// Flush it under key (stored in xxx) to the block server.\ndb->put (XXX, dirrep, wrap (this, ....));\nThus, you can directly declare fs_dirent and fs_directory structures in your C++ code and\nthen use the xdr2str and str2xdr functions to convert between an in-memory and\nserialized version of these datastructures.\nThe design of the NFS RPCs assumes that the server implements a file system similar to\nthat of UNIX. You may want to look at Sections III and IV of The UNIX Time-Sharing\nSystem by Ritchie and Thompson to learn about the UNIX file system. Section IV of the\npaper explains how UNIX represents the file system on a hard disk, which might help\nyou think about how to represent a file system in the block server.\nHints\nImplementing the NFS Protocol\nMany of the fields in the fattr3 structure aren't very important. You can see an example of\ninitializing a new fattr3 structure (which your CREATE RPC will have to do) in\nfs::new_root(). Since you're creating an ordinary file you'll want to set the type field to\nNF3REG, not NF3DIR. Here's some sample code to initialize a fattr3 structure for an\nordinary file in a CREATE RPC:\nint mode = 0;\nif(a->how.obj_attributes->mode.set)\nmode = *a->how.obj_attributes->mode.val;\nnfs_fh3 fh;\nnew_fh(&fh);\nfattr3 fa;\nbzero(&fa, sizeof(fa));\nfa.type = NF3REG;\nfa.mode = mode;\nfa.nlink = 1;\nfa.fileid = fh2fileid(fh);\nfa.atime = fa.mtime = nfstime();\nYour CREATE RPC should check to see if the file already exists. It it does, it should just\nreturn the existing file handle.\nYou will probably need to provide a directory entry for \".\" in the root directory that refers\nto the root directory itself. The ls command sometimes needs to list files from \".\". You\ncould try to fake \".\" on the fly in the LOOKUP RPC implementation, or create a real\ndirectory entry in fs::new_root_cb().\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nLook in fs.C at fs::nfs3_getattr() and fs::nfs3_access() for examples of complete RPC\nhandlers. You can change them if you want to. Look at fs::nfs3_create(),\nfs::nfs3_readdir(), and fs::nfs3_lookup() for sample code that uses the RPC arguments\nand formats an RPC reply. You will need to change these last three functions, and you\nwill also have to add one or more callbacks to each as part of reading and writing blocks.\nGeneral hacking and debugging\nYou can learn more about the asynchronous programming library (wrap, callbacks, str,\nand strbuf) by reading the Libasync Tutorial.\nYou may be able to find memory allocation errors with dmalloc by typing this before\nrunning ccfs:\npain$ dmalloc low -l dmalloc.log -i 10\nYou can see a trace of all RPC requests that your server receives, and its responses, by\nsetting the ASRV_TRACE environment variable, like this:\nenv ASRV_TRACE=10 ./ccfs ...\nYou may find ASRV_TRACE useful in helping you figure out what to work on first:\nyou'll want to implement whatever RPC shows up first in the trace output with an error\nreply (or no reply at all). Chances are the first such RPC will be a LOOKUP.\nYou can find out where a program crashed if you run it under gdb (the GNU debugger).\npain% gdb ccfs\n(gdb) run dir localhost 5566\nThen run the tester, or whatever it is that makes ccfs crash. gdb will print something like\nthis when the program crashes:\nProgram received signal SIGSEGV, Segmentation fault.\n0x806a108 in fs::nfs3_create (this=0x8144600, pnc=0x814d240) at\nfs.C:749\n749 *((char *) 0) = 1;\n(gdb)\nThis means that the program crashed in function nfs3_create(), in file fs.C, at line 749.\nOften the problem is actually in the calling function. You can see the entire call stack\nwith gdb's where command, which shows you function names and line numbers. You can\nmove up and down the call stack with gdb's up and down commands. You can print\narguments and variables of the current point in the call stack with the print command.\nCollaboration policy\nYou must write all the code you hand in for the programming assignments, except for\ncode that we give you as part of the assigment. You are not allowed to look at anyone\nelse's solution (and you're not allowed to look at solutions from previous years). You may\ndiscuss the assignments with other students, but you may not look at or copy each others'\ncode.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nHandin procedure\nYou should hand in the gzipped tar file lab-2-handin.tgz produced by gmake handin.\nCopy this file to ~/handin/lab-2-handin.tgz. We will use the first copy of the file that\nwe find after the deadline.\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "lab3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/66657c51906f8ac18d9b7786004253e4_lab3.pdf",
      "content": "6.824 - Spring 2006\n6.824 Lab 3: Reading, Writing and\nSharing Files\nDue: Lecture 7\n\nIntroduction\nIn this lab, you'll continue implementing the Frangipani-like file server you started in Lab\n2.\nYou'll start with the code you wrote for Lab 2, and add support for reading and writing\nfile contents. You'll have to choose a representation with which to store file contents in\nthe block server and implement the SETATTR, WRITE, and READ RPCs.\nContinuing with the CCFS Server\nThere are no new source files needed for this lab: you will continue to implement\nfunctionality in your existing server. There is a new tester which tests for reading and\nwriting data correctly. Download the lab tester files from\nhttp://pdos.csail.mit.edu/6.824-2006/labs/lab-3.tgz to your home directory on one of the\nclass machines.\npain$ wget -nc http://pdos.csail.mit.edu/6.824-2006/labs/lab-3.tgz\npain$ tar xzvf lab-3.tgz\nThen, copy the source files from your Lab 2 to the Lab 3 directory. For example:\npain$ rsync -av lab-2/ lab-3/\nLab 3 builds upon Lab 2 so make sure you pass the Lab 2 tester before proceeding. When\nyou're done with Lab 3, you should be able to read and write files in your file system. For\nexample, start up the block server on one of the class machines:\nfrustration$ cd ~/lab-3\nfrustration$ ./blockdbd 3772 &\nThen, start up the file server on another:\npain$ ./ccfs dir frustration 3772 &\nroot file handle: 2d1b68f779135270\npain$ cd /classfs/dir\npain$ echo hello > a\npain$ ls -lt\ntotal 0\n-rw-r--r-- 0 root wheel 6 Feb 16 13:52 a\npain$ cat a\nhello\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\npain$\nIf you try the above commands on your Lab 2 file server, you will get an error.\nYour Lab 3 file server must support sharing the file system on other hosts via the block\nserver. So when you're done with Lab 3 you should be able to do this on a third machine\n(e.g. anguish):\nanguish$ ./ccfs dir frustration 3772 2d1b68f779135270 &\n[1] 6608\nroot file handle: 2d1b68f779135270\nanguish$ cd /classfs/dir\nanguish$ ls -lt\ntotal 0\n-rw-r--r-- 0 root wheel 6 Feb 16 13:52 a\nanguish$ cat a\nhello\nanguish$\nReplace the fourth argument to ccfs with the root file handle that ccfs printed out on pain.\nYour Job\nYour job is to implement SETATTR, WRITE, and READ NFS RPCs in fs.C. You must\nstore the file system's contents in the block server, so that you can share one file system\namong multiple servers.\nFor writes, you should ignore the stable argument and always write the file to stable\nstorage (the block server). You should wait for the put of the file's contents and attributes\nto complete before sending a reply to the client. This behavior is equivalent to performing\nwrites with the a->stable argument set to FILE_SYNC.\nIf your server passes the tester (see below), then you are done. If you have questions\nabout whether you have to implement specific pieces of NFS server functionality, then\nyou should be guided by the tester: if you can pass the tests without implementing\nsomething, then don't bother implementing it.\nPlease don't modify the block server or its client interface (blockdbc.C and blockdbc.h),\nsince we may test your file server against our own copy of the block server.\nTesting\nYou should test your file server using the test-lab-3.pl script. The script tests reading,\nwriting, and appending to files, as well as testing whether files written on one server can\nbe read through a second server. To run the tester, first start a block server:\nfrustration$ ~/lab-3/blockdbd 3883 &\nThen start two instances of ccfs, with the same root file handle, on the same machine:\nanguish$ ./ccfs dir1 frustration 3883 &\n[1] 72422\nroot file handle: 2a868a18dad0f84e\nanguish$ ./ccfs dir2 frustration 3883 2a868a18dad0f84e &\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n[3] 72595\nroot file handle: 2a868a18dad0f84e\nanguish$\nThe directory argument for each instance is different, but you should pass the root file\nhandle printed by the first ccfs as an argument to the second ccfs.\nNow you can use test-lab-3.pl to test your file system by passing the two root directories\nthat you just created:\nanguish$ ./test-lab-3.pl /classfs/dir1 /classfs/dir2\nWrite and read one file: OK\nWrite and read a second file: OK\nOverwrite an existing file: OK\nAppend to an existing file: OK\nWrite into the middle of an existing file: OK\nCheck that one cannot open non-existant file: OK\nCheck directory listing: OK\nRead files via second server: OK\nCheck directory listing on second server: OK\nPassed all tests\nIf test-lab-3.pl exits without printing \"Passed all tests!\", then it thinks something is wrong\nwith your file server.\nYou should re-start ccfs before you run the tester. If you run the tester more than once\nwithout re-starting ccfs, the tester is likely to see stale information cached by the NFS\nclient for the second file system. You don't need to solve this problem for Lab 3, but if\nyou're interested, the solution is to implement mtime and ctime for files and directories.\nHints\nYou should continue to consult the NFS v3 specification for details on how NFS should\noperate. You may also want to consult the book \"NFS Illustrated\" by Brent Callaghan.\nSETATTR\nHere's how to extract the argument for the SETATTR RPC:\nsetattr3args *a = nc->Xtmpl getarg<setattr3args> ();\n//nfs_fh3 a->object : the file handle that the client wants to set\nattributes\n//sattr3 a->new_attributes : the new attrbutes\n//sattrguard3 a->guard : ignore this argument\nHere's an example of how to set the size and mtime attributes (atime is similar to mtime):\nfattr3 attr;\nif (a->new_attributes.size.set)\nattr.size = *(a->new_attributes.size.val);\nif (a->new_attributes.mtime.set == SET_TO_CLIENT_TIME)\nattr.mtime = *(a->new_attributes.mtime.time);\nHere's how to generate a reply for the SETATTR RPC:\nwccstat3 *res = nc->Xtmpl getres<wccstat3> ();\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nres->set_status (NFS3_OK);\n*res->wcc = make_wcc (old_attr, new_attr);\nnc->reply (nc->getvoidres ());\nWRITE\nHere's how to extract the argument for the WRITE RPC:\nwrite3args *a = nc->Xtmpl getarg<write3args> ();\n// nfs_fh3 a->file : the file handle where the client wants to write.\n// uint64 a->offset : the offset to start the write\n// uint32 a->count : the length of data to write\n// stable_how a->stable : ignore\n// opaque a->data : handle for the data\nThe last argument is essentially a pointer to the new data. Below is an example of how to\ncopy its contents to a str variable.\nstr newdata (a->data.base (), a->count);\nAfter a WRITE the file length should be MAX(old_length, offset+count). A WRITE\nshould never make a file shorter. If an application wants to overwrite a file, which can\ncause the file to be shorter, the NFS client will first send a SETATTR and then a WRITE.\nThe SETATTR has new_attributes.size.set=true, and new_attributes.size.val=0, which\nshould cause your server to set the file length to zero. The WRITE then writes the\nnumber of bytes specified by the client. The resulting file can have a shorter or longer\nlength than the original file.\nNote that you'll have to update the size of the file (in its fattr3) as a side effect of most\nWRITE RPCs and when SETATTR explicitly asks your server to do so.\nREAD\nHere's how to generate a reply for the WRITE RPC:\nwrite3res *res = nc->Xtmpl getres<write3res> ();\nres->set_status(NFS3_OK);\nres->resok->file_wcc = make_wcc (old_attr, new_attr);\nres->resok->count = /* The number of bytes of data written */;\nres->resok->committed = FILE_SYNC; //Since this is how we implemented\nthe write.\nnc->reply (nc->getvoidres ());\nHere's how to extract the argument for the READ RPC:\nread3args *a = nc->Xtmpl getarg<read3args> ();\n// nfs_fh3 a->file : the file handle where the client wants to read.\n// uint64 a->offset : the offset to start the read.\n// uint32 a->count : the length of data to write.\nHere's how to generate a reply for the READ RPC:\nchar value[bytes_read];\n\nread3res *res = nc->Xtmpl getres<read3res> ();\nres->set_status (NFS3_OK);\nres->resok->eof = /* true, if already end of file; otherwise, false.\n*/;\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nres->resok->file_attributes.set_present (false);\nres->resok->count = bytes_read;\nres->resok->data.set (value, bytes_read);\nnc->reply (nc->getvoidres ());\nCollaboration policy\nYou must write all the code you hand in for the programming assignments, except for\ncode that we give you as part of the assigment. You are not allowed to look at anyone\nelse's solution (and you're not allowed to look at solutions from previous years). You may\ndiscuss the assignments with other students, but you may not look at or copy each others'\ncode.\nHandin procedure\nYou should hand in the gzipped tar file lab-3-handin.tgz produced by running these\ncommands in your ~/lab-3 directory.\n% tar czf lab-3-handin.tgz *.[TCchx] Makefile\n% chmod og= lab-3-handin.tgz\nCopy this file to ~/handin/lab-3-handin.tgz. We will use the first copy of the file that\nwe find after the deadline.\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lec1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/f8c433edc82e161324a038139c333962_lec1.pdf",
      "content": "6.824 2006 Lecture 1: Introduction and O/S Review\n\nOpening\nbuilding distributed systems\nconstruction techniques, robustness, good performance\nlectures on design, paper reading for case studies\nyou'll build real systems\nwhy take the course?\nsynthesize many different areas in order to build working systems\nInternet has made area much more attractive and timely\nhard/unsolved: not many deployed sophisticated distrib systems\n\nExample:\nhow to build HotMail?\nmail arrives from outside world\nstore it until...\nuser's Outlook/Eudora reads/deletes/saves it\n\nSimple Solution:\nOne server w/ disk to store mail-boxes\n[picture: MS, sending \"clients\", reading clients]\nWhat happens as your mail service gets popular?\n\nTopic: Stable performance under high load\nExample: Starbucks.\n5 seconds to write down incoming request. 10 seconds to make it.\n[graph: x=requests, y=output]\nmax thruput at 4 drinks/minute.\nwhat happens at 6 req/min?\nthruput goes to zero at 12 requests/minute.\nEfficiency *decreases* with load -- bad.\nCareful system design to avoid this -- flat line at 4 drinks.\nPeets, for example.\nBetter: build systems whose efficiency *increases* w/ load\nw/ e.g. batching, disk scheduling\n\nTopic: scalable performance\nWhat if more clients than one Hotmail server can handle?\nHow to use more servers to handle more clients?\nIdea: partition users across servers\nbottlenecks: how to ensure incoming mail arrives at the right\nserver?\nscaling: will 10 servers allow us to handle 10x as many users?\nload balance: what if some users get much more mail than others?\nlayout: what if we want to detect spam by looking at all mailboxes?\n\nTopic: high availability\nCan I get at my HotMail mailbox if some servers / networks are down?\nYes: replicate the data.\nProblem: replica consistency. delete mail, re-appears.\nProblem: physical independence vs communication latency\nProblem: partition vs availability. airline reservations.\nTempting problem: can 2 servers yield 2x availability AND 2x\nperformance?\n\nTopic: global scalability\nthis is really an opportunity\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nwe have the entire Internet as a resource\nwhat neat new big systems can we build that take advantage?\nare there any principles to be discovered?\nfinding objects\nstoring objects \"out there\"\nserving same objects to many consumers\nwidely distributed computing (e.g. grid computing)\n\nTopic: security\nold view: secrecy via encryption (msg to Moscow embassy)\nuser authentication via passwords &c\nall parties know each other!\nInternet has changed focus.\nglobal exposure to random attacks from millions of bored students\nand serious hackers, e.g. intrusions for spam bot nets\nyou fetch a new Firefox binary, how do you know it hasn't been\nhacked?\nhow do you know that was Amazon you gave your credit card number to?\nhow does Amazon know it was you?\nno purely technical approach is likely to solve these problem\n\nWe want to understand the individual techniques, and how to assemble\nthem.\n\n--------------\n\nCourse structure\nURL\nmeetings: 1/2 lectures, 1/2 paper discussions\nresearch papers on working systems, starting next week\nmust read papers before class\notherwise boring, and you can't pick it up by listening\nwe will post paper questions 24 hours in advance\nhand in answer on paper in class, one or two paragraphs\ntwo in-class quizzes (no final)\nLabs: build a real cluster file server, cache consistency, locking\nProject. look at the project information page!\ndesign, implement, report\nteams\nproposal\nconferences\ntwo drafts\ndemo\nreport\nEmil is TA, office hours TBA\nLook at the web site:\nsign up for course machine accounts\nlook at the first lab, due in a week\n\n--------------\n\nO/S kernel overview\ncontext in which you build distributed systems\no/s has big impact on design, robustness, performance\nsometimes because of o/s quirks\nmostly because o/s solves some hard problems\nThis should be review for most of you\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nWant to tell what I think is important\nGive you a chance to ask questions\n\nWhat problems does o/s solve?\nsharing hardware resources\nprotection\ncommunication\nhardware independence\n(everyone faces these problems)\n\nApproach to solutions?\no/s designers think like programmers, abstractions + interfaces\n\nUNIX abstractions\n(we'll be programming UNIX in labs, my favorite O/S)\nprocess\naddress space\nthread of control\nuser ID\nfile system\nfile descriptor\non-disk file\npipe\nnetwork connection\ndevice\n\nAll this is implemented by a \"kernel\" with hardware privileges\n\nNote we're partially virtualizing\no/s multiplexes physical resource among multiple processes\nCPU, memory, disk, network\nto share, to control, to provide a simple model to apps\nabstraction helps virtualization: easier to share TCP conns than enet\n\nCan't completely virtualize\nfile system and network stack not the same as physical foundation\nthe differences make sharing possible\n\nabstractions interact, must form a coherent set\nif o/s can start programs, it must know how to read files\n\nSystem call interface to kernel abstractions\nlooks like function call, but special\nfork, exec\nopen, read, creat\n\nStandard picture\napp (show two of them, mark addresses from zero)\nlibraries\n-----\nFS\ndisk driver\n(mention address spaces, protection boundaries)\n(mention h/w runs kernel address space w/ special permissions)\n\nWhy Big Kernels have been successful.\neasy for kernel subsystems to cooperate\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\ndisk buffer shares phys mem with virtual mem system\nall kernel code is 100% privileged\nvery simple security model\neasy to implement sophisticated and efficient services\n\nWhy UNIX abstractions are not perfect\nkernel is big\nkernel has room for lots of bugs; it's all privileged\nkernel limits flexibility\nmultiple threads per process?\nsingle thread crossing into a different address space?\ncontrol disk layout of files for performance?\ndon't like the kernel's TCP implementation?\nwe'll discuss a number of improved abstractions\n\nAlternate set of abstractions: micro-kernel\nMove complex abstractions to server processes\nTalk to FS server, rather than FS module in kernel\nKernel mostly handles IPC\nalso grants h/w access to privileged servers\ne.g. FS server can read/write disk h/w\nLooks like a miniature distributed system!\nMove FS server to a different machine, via network?\nLots of overlap with our concerns in this class.\n\nLet's review some basics which will come up a lot:\nprocess / kernel communication\nhow processes and kernel wait for events (disk and network i/o)\n\nLife-cycle of a simple UNIX system call\n[diagram. process, kernel]\nSee the handout...\n\nInteresting points:\nprotected transfer\nh/w allows process to get kernel permissions\nbut only by jumping to *known* entry point in kernel\nprocess suspended until system call finishes\n\nWhat if the system call needs to wait, e.g. for the disk?\nWe care: this is what busy servers do\nsys_open(path)\nfor each pathname component\nstart read of directory from disk\nsleep waiting for the disk read\nprocess the directory contents\nsleep()\nsave *kernel* registers to PCB1 (including SP)\nfind runnable PCB2\nrestore PCB2 kernel registers (SP...)\nreturn\n\nNote:\neach user process has its own kernel stack [draw in diagram]\nkernel stack contains state of partially executed system call\n\"kernel half\"\ntrap handler must execute on the right stack\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n\"blocking system call\"\n\nWhat happens when disk completion interrupt occurs?\nDevice interrupt routine finds the process waiting for that I/O.\nMarks process as runnable.\nReturns from interrupt.\nSomeday process scheduler will switch to the waiting process.\n\nNow let's look at how services use this kernel structure.\n\nExplain server_1 web server in handout\n\nProblem\n[draw this time-line]\nTime-lines for CPU, disk, network\nServer alternates waiting for each of them\nCPU, disk, network are each idle much of the time\nOK if only one client.\nNot OK if there are clients waiting for service.\nWe may have lots of work AND idle resources. Not good.\ns/w structure forces one-at-time processing\nHow can we use the system's resources more efficiently?\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lec2_concurrency.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/218dc61c38c02d26cb8dcb9193268a4c_lec2_concurrency.pdf",
      "content": "6.824 2006 Lecture 2: I/O Concurrency\n\nRecall timeline\n[draw this time-line]\nTime-lines for CPU, disk, network\nHow can we use the system's resources more efficiently?\n\nWhat we want is *I/O concurrency*\nAbility to overlap I/O wait with other useful work.\nIn web server case, I/O wait mostly for net transfer to client.\nCould be disk I/O: compile 1st part of file while fetching 2nd part.\nCould be user interaction: emacs GC while waiting for you to type.\n\nPerformance benefits of I/O concurrency can be huge\nSuppose we're waiting for disk for client one, 10 milliseconds\nWe can probably server 100 other clients from cache during that time!\n\nTypical ways to get concurrency.\nThis is about s/w structure.\nThere are any number of potential structures.\n[list these quickly]\n0. (One process)\n1. Multiple processes\n2. One process, many threads\n3. Event-driven\nDepends on O/S facilities and type of application.\nDegree of interaction among different sub-tasks.\n\nOne process can be better than you think!\nO/S provides I/O concurrency transparently when it can\nO/S does read-ahead into cache, write-behind from buffer\nworks for disk and network connections\n\nI/O Concurrency with multiple processes\nStart a new UNIX process for each client connection / request\nMaster processes hands out connections.\nNow plenty of work available to keep system busy\nStill simple:\nlook at server_2() in handout.\nfork() after accept()\nPreserves original s/w structure.\nIsolated: bug for one client does not crash the whole server\nMost interaction hidden by O/S. E.g. lock the disk queue.\nIf > 1 CPU, CPU concurrency as a side effect\n\nWe may also want *CPU concurrency*\nMake use of multiple CPUs on shared memory machine.\nOften I/O concurrency tools can be used to get CPU concurrency.\nOf course O/S designer had to work a lot harder...\nCPU concurrency much less important than I/O concurrency: 2x, not\n100x\nIn general, very hard to program to get good scaling.\nUsually easier to buy two separate computers, which we *will* talk\nabout.\n\nMultiple process problems\nCost of starting a new process (fork()) may be high.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nNew address space &c. 300 microseconds *min* on my computer.\nProcesses are fairly isolated by default\nE.g. they do not share memory\nWhat if you want a web cache? Must be shared among processes.\nOr even just keep statistics?\n\nConcurrency with threads\nLooks a bit like multiple processes\nBut thread_fork() leaves address space alone\nSo all threads share memory\nOne stack per thread, inside process\n[picture: thread boxes inside process boxes]\nSeems simple -- still preserves single-process structure.\nPotentially easier to have e.g. shared web cache\nBut programmer needs to know about some kind of locking.\nAlso easier for one thread to corrupt another\n\nThere are some low-level but very important details that are hard to\nget right.\nWhat happens when a thread calls read()? Or some other blocking\nsystem call?\nDoes the whole process block until disk I/O has finished?\nIf you don't get this right, you don't get I/O concurrency.\n\nKernel-supported threads\nO/S kernel knows about each thread\nIt knows a thread was just blocked, e.g. in disk read wait\nCan schedule another thread\n[picture: thread boxes dip down into the kernel]\nWhat does kernel need for this?\nPer-thread kernel stack.\nPer-thread tables (e.g. saved registers).\nSemantics:\nper-process resources: addr space, file descriptors\nper-thread resources: user stack, kernel stack, kernel state\nKernel can schedule one thread per CPU\nThis sounds like just what we want for our server\nBUT kernel threads are usually expensive, just like processes\nKernel has to help create each thread\nKernel has to help with each context switch?\nSo it knows which thread took a fault...\nlock/unlock must go through kernel, but bad for them to be slow\nMany O/S do not provide kernel-supported threads, not portable\n\nUser-level threads\nImplemented purely inside program, kernel does not know\nUser scheduler for threads inside the program\nIn addition to kernel process scheduler\n[picture]\nUser-level scheduler must:\nKnow when a thread is making a blocking system call.\nDon't actually block, but switch to another thread.\nKnow when I/O has completed so it can wake up original thread.\nAnswer:\nthread library has fake read(), write(), accept(), &c system calls\nlibrary knows how to *start* syscall operations without waiting\nlibrary marks threads as waiting, switches to a runnable thread\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nkernel notifies library of I/O completion and other events\nlibrary marks waiting thread runnable\nread(){\ntell kernel to start read;\nmark thread as waiting for read;\nsched();\n}\nsched(){\nask kernel for I/O completion events\nmark threads runnable\nfind a runnable thread;\nrestore registers and return;\n}\nEvents we would like from kernel:\nnew network connection\ndata arrived on socket\ndisk read completed\nclient/socket ready to receive new data\nLike a miniature O/S inside the process\n\nProblem: user-level threads need significant kernel support\n1. non-blocking system calls\n2. uniform event delivery mechanism\n\nTypical O/S provides only partial support for event notification\nyes: new TCP connections, arriving TCP/pipe/tty data\nno: file-system operation completion\n\nSimilarly, not all system calls operations can be started w/o waiting\nyes: connect(), socket read(), write()\nno: open(), stat()\nmaybe: disk read()\n\nWhy are non-blocking system calls hard in general?\nTypical system call implementation, inside the kernel:\n[sys_read.c]\nCan we just return to user program instead of wait_for_disk?\nNo: how will kernel know where to continue?\nie. should it run userspace code or continue in the kernel syscall?\nBig problem: keeping state for multi-step operations.\n\nOptions:\nLive with only partial support for user-level threads\nNew operating system with totally different syscall interface.\nOne system call per non-blocking sub-operation.\nSo kernel doesn't need to keep state across multiple steps.\ne.g. lookup_one_path_component()\nMicrokernel: no system calls, only messages to servers.\nand non-blocking communication\nHelper processes that block for you (Flash paper next week)\n\nThreads are hard to program\nThe point is to share data structures in one address space\nThread *model* involves CPU concurrency even on a single CPU\nso programmer may need to use locks\neven if only goal was to overlap I/O wait\nBut *events* usually occur one at a time\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\ncould do CPU processing sequentially, overlap only the I/O waiting\n\nEvent-driven programming\nSuggested by user threads implementation\nOrganize the s/w around arrival of events\nWrite s/w in state-machine style\nWhen this event occurs, execute this function\nLibrary support to register interest in events\nThe point: this preserves the serial natures of the events\nProgrammer sees events/functions occuring one at a time\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lec2_events.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/a454f39f5cff9f9fa6526235f25e81ca_lec2_events.pdf",
      "content": "a2ps -A fill -o x --line-numbers=1 events.c webclient.c\nwebclient_libasync.c\n\nOffice hours\nDiscussion mailing list\n(Wiki?)\n\n****\nRecall simple web-like server?\nNow we'll look at a client for the server\nShow how to write some asynchronous code\nBuild up to libasync which you will and have been using for labs\n\n[webclient.c]\n\nWhere does this block?\nconnect: makes a tcp connection\nwrite(s): is remote side willing to take data?\nread(s): has data come back from the remote side?\nwrite(1): is terminal ready for output?\n\nHow to program in event style?\nIdentify events and appropriate responses: state machine\nprogrammer has to know when something might block!\nWrite a loop that handles incoming events (I/O events)\n[events.c Example 1]\n\nselect()\nNeed a way to multiplex sockets\nProgram must then interleave operations\n\n[write prototype on the board: nfds, reads, writes, excepts, time]\nCondition is actually \"read() would not block\"; might be EOF.\nselect() blocks (if timeout > 0) to avoid wasteful polling.\nthis is important; you *do* want to block in select().\n\nTranslate low-level system events into application level events\nBuffer net I/O, maintain individual application state\nWriting this event loop for each program is tedious\n[sketch implementation on the board]\nWhat if your program does the one thing in parallel?\nHave to partition up state for each client\nNeed to maintain sets of file descriptors\nWhat if your program does many things?\ne.g. let's add DNS resolution\nHard to be modular if event loop knows about all activities.\nAnd knows how to consult all state.\n\nWe would prefer abstraction...\nUse a library to provide main loop (e.g. libasync)\nProgrammer provides \"callbacks\" to handle events\n[events.c: Example 2]\n\nBreak up code into functions with non-blocking ops\nlet the library handle the boring async stuff\n[prototypes in webclient_libasync.c]\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nIt's unfortunately hard for async programs to maintain state\n[draw logical diagram of select loop and function calls]\nOrdinary programs, and threads, use variables.\nWhich persist across function calls, and blocking operations.\nSince they are stored on the stack.\nAsync programs can't keep state on the stack.\nSince each callback must return immediately.\n\nHow can they maintain state across calls?\nUse global variables\nUse the heap:\nProgrammers package up state in a struct, malloc struct\nEach callback could take a void * (libevent)\n(In C++, can do this somewhat implicitly using an object.)\nThis turns out to be hard to program\nNo type safety\nMust declare structs for every set of state transfer\nUser has to manage memory in potentially tricky cases\nlibasync provides a form of closures\n\ncb = wrap(fn, a, b) generates a closure.\nThat is, a function pointer plus an environment of saved values.\ncb() calls fn(a, b)\nAlso provides something like function currying.\nuseful later on when callbacks do different things based on input\nGiven a function with signature \"R fn (A, B)\":\ncb = wrap (fn) -> callback<R, A, B>::ref\nuse it like this:\ncb (a, b)\nOr:\nwrap (fn, a) -> callback<R, B>::ref\n\nLimited compared to Scheme closures:\nYou must explicitly indicate what variables to keep around.\nCan only pass a certain number of arguments\n\nHow are callbacks implemented?\nSee callback.h: one of the few commented files in libasync.\ntemplates to generate dynamic structs holding values\ntemplates provide type safety:\nR fn (A, B);\ncb = wrap (fn) -> callback<R, A, B>::ref cb (a, b)\ncb = wrap (fn, a) -> callback<R, B>::ref cb (b);\ncb = wrap (fn, a, b) -> callback<R>::ref cb ();\n\ncallbacks are reference counted to simplify mem mgmt\nnormally, arguments in the wrap would have been on stack\nnow, values are stored in closures created by wrap().\nHow do we know when we've used a callback the last time?\nThat's why they're reference counted.\n\nWhat is the result?\n[webclient_libasync.c]\nwhat's the difference between filename and buf?\n\nThis is still somewhat tedious...\nMust handle memory allocation for strings\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nMust manually buffer data to and from client\nHave to translate network read/writes into application level events\n\nlibasync provides some solutions:\nsuio and aios handle raw and line-oriented i/o\nreference counted data (strings and general dynamic structs)\nasynchronous RPC library\nbut you still have to do work like splitting your code up into\nfunctions\nloops can still be a pain\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lec3_events.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/38dc024fe5e0b8fe7115d4e19d2476ba_lec3_events.pdf",
      "content": "Today's plan:\n[Any questions about lock server lab?]\nReviewing event driven programming\nOutline structure of the remaining labs\nCommon libasync/libarpc/nfsloop programming idioms:\nwriting rpc client code\nwriting async functions that call RPCs\nwriting rpc server code\nFlash\n\nEvent driven programming\nAchieve I/O concurrency for communication efficiently\nThreads give cpu *and* i/o concurrency\nNever quote clear when you'll context switch: cpu+i/o concurrency\nState machine style execution\nLots of \"threads\": request handling state machines in parallel\nSingle address space: no context switch overhead ==> efficient\nHave kernel notify us of I/O events that we can handle w/o blocking\nThe point: this preserves the serial natures of the events\nProgrammer sees events/functions occuring one at a time\nSimplifies locking (but when do you still need it?)\n\nlibasync handles most of the busywork\n[draw amain/select on board again]\ne.g. write-ability events are usually boring\nlibarpc translates to events that the programmer might care about:\nrpcs\n\nccfs architecture:\n[draw block diagram on the board:\nOS [app, ccfs] --> blockserver <-- [ccfs, app] OS\n\\-> lockserver <-/\n]\nccfs communicate through RPC: you'll be writing clients and servers\n[include names of RPCs on the little lines]\nreal apps can be structured just like this: okws, chord/dhash\n\nSynchronous RPC:\n[Example 1]\n[Sketch this on the board and use it to show evolution]\n\nMaking RPCs\nAlready saw basic framework in Lab 1\nlibarpc provides an rpc compiler: protocol.x -> .C and .h\nProvides (un)marshalling of structs into strings\nExternal Data Representation, XDR (rfc1832)\n[Example 2]\nlibraries to help:\nhandle the network (axprt: asynchronous transport)\nwrite clients (aclnt),\naclnt handles all bookkeeping/formatting/etc for us:\ne.g. which cb gets called\nwrite servers (asrv/svccb)\n\nAsynchronous RPC: needs a callback!\n[Example 3]\nNote:\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n1. Need to split code into separate functions: need to declare\nprototypes\n2. \"return values\" passed in by aclnt as arguments: e.g. clnt_stat\n3. cb must keep track of where results will be stored.\n4. Actually must split everything that uses an async function!\n\nHow do we translate this into a stub function?\nNeed to provide our own callback....\n[Example 4]\n...translate RPC results/error into something the app can use.\n\nServer side:\nSetup involves listening on a socket, allocating a server with\ndispatch cb\n\n[Example 5]\ndispatch (svccb *sbp):\nswitch to dispatch on sbp->proc ();\ncall sbp->reply (res);\n\nYou must not block when handling proc ()\nyou don't need to reply right away but blocking would be bad\n\nManaging memory with svccb:\nUse getarg<type> to get pointer to argument, svccb managed\nUse getres<type> to get a pointer to a reply struct, svccb managed\nsbp->reply causes the sbp to get deleted.\n\nWriting user-level NFS servers:\nclassfsd code will allow you to mount a local NFS server w/o root\nnfsserv_udp handles tedious work, we register a dispatch function\nSimilar to generic RPC server but use nfscall *, instead of svccb.\nAdds features like nc->error ()\n\nYou'll need to do multiple operations to handle each RPC\n[draw RPC issue timeline os->kernel->ccfs->lockserver/blockserver]\nNot unlike how we might operate:\nget an e-mail from friend: can you make it to my wedding?\ncheck class calendar on web, check research deadlines\nsend IM to wife, research ticket prices, reply\nOr Amazon.com login...\n[Example 6]\n\nAn aside on locking:\nNo locking etc needed usually: e.g. to increment a variable\nWhen do you need locking?\nWhen an operation involving multiple stages\nBe careful about callbacks that are supposed to happen \"later\"\ne.g. delaycb (send_grant);\n\nParallelism and loops\n[Example 7a]: synchronous code\n[Example 7b]: serialized and async\n[Example 7c]: parallelism but yet...\n[Example 7d]: better parallelism?\n\nSummary\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nEvents programming gives programmer a view that is roughly\nconsistent with what happens.\nCan build abstractions to handle app level events\nNeed to break up state and program flow\nbut always know when there's a wait,\nand have good control over parallelism\n\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "prev_projects.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/2d17c2069a82c00f08fadde727802bec_prev_projects.pdf",
      "content": "6.824 Distributed Computer Systems, Spring 2006\nFinal Projects from Previous Years\nFall 2002 Topics\n\n-\nFootloose: A Distributed Data Buffer for Primarily Disconnected Operation (WINNER)\n-\nStarfish: Resilient Distributed Data Structures in Untrusted Environments\n-\nChunk: A Framework for Modular Distributed Shared Memory Systems\n-\nRobust Distributed Storage Using Erasure Codes\n-\nAutomated Build and Test System\n-\nPanache: A Scalable Distributed Index for Keyword Search\n-\nMakhzan\n-\nSpamStrangler: A Chord-Based Distributed Spam Detection Tool\n-\nApplication-Specific File Caching for Unmodified Applications and Kernels\n-\nSyncframe: A Multi-Peer Synchronization Framework\n-\nJava Replicated Objects.\n-\nImplementing Overcast: A High-Level Multicast Protocol.\n-\nCryptoShare: Secure Verifiable Shared Remote File System\n-\nAmorpheus: A Peer-to-Peer Approach to Resilient Overlay Routing\n-\nWashuDCFS: Distributed Consistent File Sharing across Peer Server Systems\n-\nRMI for C++ Rapid Application Development of Distributed Event Driven Software\n-\nA Fault-tolerant Distributed Mail Service\n-\nA Distributed, Semantic Similarity Filter for E-mail\n-\nPractical Durable Writes for Commodity Disks\nFall 2001 Topics\n\n-\nJgss - A Kerberos Security Service Provider for Secure RMI\n-\nChip: A Temporary, Memory Based Network Filesystem\n-\nDINX: A Decentralized Search Engine\n-\nInfranet: Circumventing Web Censorship and Surveillance\n-\nTarzan: A Peer-to-Peer Anonymizing Network\n-\npStore: A Secure Peer-to-Peer Backup System\n-\nsjsFS: A FTP-transparent User-Level File System\n-\nFast Distributed Index Generator\n-\nDude, Where's My Object?\n-\nDistributed Browser Caching\n-\nSFS-AIM\n-\nTechniques for Speeding Web Access in the Developing World\n-\nFDIG - Fast distributed index generator\n-\nConChord: Cooperative SDSI Certificate Storage and Name Resolution\n-\nRequest Queue Scheduling for Graceful Load Management in Asynchronous Event-\nDriven Systems\n-\nBSFS: B-tree Structured File System\n-\nDistributable Functional Objects\n-\nDistributed Command Line Interface\n-\nFlexible Access Control Lists in SFS\n-\nlibasync-mp: A Multiprocessor Aware C++ Asynchronous Programming Library\n-\nAljeda: A self organizing, hierarchical, polymorphic, distributed peer to peer file sharing\nsystem\n-\nSQUNK: Stages, Queues 'N Kontrollers, a high performance network application toolkit\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering, Spring 2006.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "argus.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/9a4259c1b7785210c883f9821e15ec2d_argus.pdf",
      "content": "Guardians and Actions: Linguistic Support for Robust, Distributed\nPrograms\nLiskov and Scheifler, ToPLS 1983\nbig idea:\nlanguage support for distributed programs\nonly useful if it solves significant problems\nand if solutions are general\nRPC/RMI, transactions, sub-actions, locking, persistence, crash\nrecovery\nwant to illustrate:\nguardian == RMI object\nRMI-like transparent guardian references, args, &c\nhas better story than RMI for failure\nRPC always has prepare/commit\nsub-actions: why?\nversions: why?\nimplicit locking\nwalk through send mail to many users example\nwhat if one user doesn't exist?\nbut mail has already been delivered to some other users\nhow to un-do?\nwhy do nested transactions make sense?\nare they just a feature? or neccessary?\ni.e. xactions + RPC => nested transactions?\nrequired by modularity?\nyou don't know who is calling, but you want to be atomic\nwhat if a guardian crashes after RPC return, before final commit?\nor while commiting?\nwhat if concurrently one reader reads his/her mail?\nhow does user not see tentative new mail?\ndoes reading user block? where?\nread_mail also deletes it\nwhat if new mail arrives just before delete?\nwill it be deleted but not returned?\nwhy not? what lock protects? where is the lock acquired? released?\nwhat if a user is on the list twice?\nlocks are held until end of top-level xaction\ndeadlock?\nstable variables are like DB data\nversions for abort, logged (?) to disk\ncrash recovery\nstable variables, per-guardian recovery\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "echo.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/b807cd75edae6e25300a55f1ab462431_echo.pdf",
      "content": "A Coherent Distributed File Cache with Directory Write-Behind\nMann, Birrell, Hisgen, Jerian, Swart\n1993, SRC TR and ToCS\nwhy are we reading this paper?\nit talks about consistency for a distributed system\nit focuses on crash-recovery semantics of use to apps\nit explores an alternate approach to logging\nit provides an efficient way to get those semantics\nbetter than the usual sync writes approach\nbetter than logging, since you don't have to force the whole log\ntokens are related to the locks in Labs 4 and 5\ncontext\nlots of workstations sharing a file server\neach workstation has a cache\nfor performance, performs writes into local cache,\nsend to server disk later\nthis is \"write-behind\"\nmore aggressive than NFS's sync rename()&c and close-to-open\ntrying to support sophisticated distributed apps (e.g. Vesta vcs)\nwhat are the potential problems?\nif cache writes back only in response to server reclaiming tokens?\nvi does create(temp), write(), rename()\nif I crash, might others see an empty file?\nI write foo.c, cc -o foo.o foo.c\nif I crash, might others see new .o but old .c?\nwhat are some simple designs?\n(writes only for token reclaim)\nsync updates (no write caching)\nclose-to-open\nkeep a log, flush prefix on token reclaim\nwhat's their approach?\nclient remembers order of update ops to each object\nsome ops affect more than one object\nwrites all \"prior\" ops back before returning token\nhow does Echo's ordering help solve problem examples?\nwhat does the implementation look like?\ndo they keep pointer among block buffers in the client cache?\ncycle example: write B1, B2, B1\nor two renames between directories, opposite orders\nno: each vnode has list of ops w/ full arguments (?)\neach op refers to other affected files\nmust flush some of those other files' ops when returning a token\nwhat happens when a client workstation crashes?\ndo apps on other workstations need to take action?\ni.e. does the system need to notify apps on other workstations?\nwhat happens when a client loses its network connection?\nand it has holds write tokens & has ops in its write-back cache\ncan server reclaim its tokens if another workstation wants them?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nwhat if the unreachable client then reconnects?\nwill two workstations think they both have tokens?\nwhat is reporting lost write-behind all about?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "fab.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/b37259df4383429ff1c5bf13efff528d_fab.pdf",
      "content": "-----------------\n--------------\nFAB: Federated Array of Bricks\nYasushi Saito, et al. HP Labs\nASPLOS 2004\nWhat are they trying to build?\nA fault-tolerant storage system.\nscalable, flexible, built from cheap commodity disks, CPUs and\nnetwork cards.\n1. Data structure\nDiagram of connected bricks.\nOne brick with global volume-->seggroup mapping\ntimestamp table\n-One logical disk with many logical volumes.\n-brick = physical machine\n-Any brick can be a coordinator for a request.\n-All bricks have the global map of logical volume to seggroup to\nbricks.\nThis mapping is Paxos replicated.\n-Each seggroup is replicated under the same policy at the same places.\nThey use voting-based replication for seggroups.\n-Also erasure codes the block/seggroup?\nTimestamp table --> stores timestamp info for recently modified blocks.\n2. Replication\nThe goal of FAB is to tolerate failures such as power failures to a\nCPU,\nnetwork partitions, and disk failures.\nTo achieve fault-tolerance, FAB replicates each block on several\nbricks.\nDiagram of two bricks A, B want to tolerate the failure of one brick\n-a write returns after finished on one brick.\n-read returns after finished on one brick.\nIs there anything wrong with this scheme?\nWhat technique does FAB uses to guarantee linearizability?\nVoting, Quorums -- each read/write operation has to complete at a\nmajority of the bricks. Read picks the latest\nvalue.\nEach block has 2 persistent timestamps: ordTs and valTs\nWhat is the purpose of valTs?\nIf there is no valTs or ordTs:\nDiagram of 3 bricks: A\nB\nC\nv1 v1 v1\ncoordinator1\nv2 v2\ncoordinator2\nv3 v3 v3\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nv2\nvalT prevents writes of old values from overwriting new values.\nWhat is the purpose of ordTs?\nIf there is only valTs and not ordTs:\nDiagram of 3 bricks: A\nB\nC\nv1 v1 v1\nwrite1 v2\nread1 from A and B --> v2\nread2 from B and C --> v1\nDescribe write and read protocol with ordT.\nordT prevents concurrent reads and writes from seeing value out-of\norder.\nSo, a block has two timestamps and they need to be persistent.\nFor performance, FAB keeps timestamps in NVRAM.\nWhat is the potential problem in keeping timestamps this way?\n-Run out of space. Takes 48GB of timestamps for 1TB of data.\nWhat is FAB's solution to this problem?\n-The timestamp table in NVRAM keeps only timestamps + block\nnumbers of\nrecently modified blocks.\n-The coordinator sends a GC (garbage collect) message to the\nbricks\nafter all bricks acknowledged an update. The bricks then removes the\ncorresponding\ntimestamp entry from the table.\nWhat if want to read or write a block that does not have an entry in\nthe\ntimestamp table? How can we use the pseudocode in figure 4?\nFor garbage collected blocks, assign valTs = ordTs <-- time of latest\nGC\nIn pseudocode in Figure 4, in proc read, in the order phase,\nwhy does it use current timestamp instead of ordTs?\nBecause there maybe incomplete order phases, so want to pick a\ntimestamp\nthat is greater than any incomplete timestamp.\n***********************************************************************\n****\nWrite takes 2 rounds\n1. The coordinator writes the block's ordT on each replica brick,\nwait for a majority to respond, and return.\n2. The coordinator writes the block's value and update valT on each\nbrick.\nIf any of those steps fail, ABORT.\nRead normally takes 1 round\n1. If majority returns with same timestamp, return val.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n------------------\nOtherwise, read takes additional 2 rounds.\n1. Write the latest timestamp ts to ordT on all replica.\n2. If majority succeeds, newval <-- val with highest valT from\nreplica\n3. Write <val, ts> to all replica. Return successfully if majority\nsaid yes;\notherwise ABORT.\nHow does ABORT enable strict linearizability?\n***********************************************************************\n*****\n3. Reconfiguration\nTo tolerate long-term or permanent failures, FAB uses reconfiguration.\nWhy do we need view change?\nDiagram three nodes, one dies\nCan we still operate?\nWhat is value was written only to A?\nYes, we still have a majority. 3/2+1=2.\nCan we stay like this forever?\nNo, eventually NVRAM with fill up because cannot garbage\ncollect the\ntimestamps.\nAny other reasons?\nIf another brick dies soon after the first brick, we might not\nhave\nall the updates on the remaining brick. Therefore, need to\nreconfigure\nso that 1) both nodes synchronize data with one another\n2) prevent NVRAM from filling up.\nNow, have 2 bricks in the configuration. Updates will go to\nboth bricks,\nso if one brick dies, still have all updates on the remaining.\nNeed to reconfigure to increase bricks eventually to maintain\nMTTDL.\nCan we use the same sync pseudocode in figure 9 to synchronize newly\nadded bricks?\nNo, start from empty disk.\nWhat if the newly-added brick used to be in the configuration and\nstill has data from before?\nNo, might have garbage collected timestamps in the current\nconfiguration.\nReconfiguration protocol works similar to Paxos.\nIn this specific case, requires that accept a proposed view only if\nit intersects\na majority of the current view and each of the ambiguous views.\nWhat is the reason for this requirement?\n1) Intersect with current+proposed view so that only one new view\nis chosen\n2) Intersect with current view so that the synchronization works,\nbecause need at least one brick to have the newest value.\nWhat if a dead node that has been reconfigured out comes back online?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nDiagram of 3 nodes. A B C. C dies. Reconfigure to A B.\nThen C comes back online. If C answers queries, can it mess things\nup?\nNo, because A and B are still the majority.\nWhat if reconfigure to add another node D to A B, then network\npartitioned so\nthan A D and B C are in different partitions? Can it mess things up?\nNo, if the protocol includes a configuration number with each reply.\nPaper did not mention that.\n***********************************************************************\n*********\n3.1 View change\nThree rounds. A brick (in the same seggroup?) that detects another\nbrick failure\nbecomes the leader. It computes a candidate view and witnesses = vote\nview.\nRound 1: Leader sends vote view to bricks in vote view.\nBricks received the view and adds it to the ambiguous view.\nWhat do witnesses do?\nRound 2: If leader receives acceptance from majority (including\nwitnesses??),\nit proposes the candidate view to the bricks in the candidate view.\nA brick accepts the view iff the view contains a majority of the\ncurrent view\nand each view in the ambiguous view list.\nRound 3: If leader receives acceptance from ALL bricks in the candidate\nview,\nit sends a message to the view to update its current view to the new\nview and\ndelete the ambiguous view list.\nWhy does the candidate view have to intersect a majority of each\nambiguous view?\n3.2 State synchronization\nRound 1: Sync leader finds all the blocks that are in the timestamp\ntable at\neach brick that replicates a segment.\nRound 2: The leader sends a SyncPoll message for each recently modified\nblock in\nthe oldView. Upon receiving info from an m-quorum (same as majority?)\npick the\nmaxOrdTs, maxValTs and maxVal.\nRound 3: The leader sends this info the bricks in the newView. When\nreceived reply\nfrom m-quorum, declare synchronization done.\n***Optimizations***\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n1. If every quorum in the oldView is a superset of another quorum in\nthe newView,\nthen don't need to sync.\n2. Separate blocks that must be synced before removing the oldView, and\nthe ones\nthat may be synced after oldView is removed. The \"must\" blocks are the\nones whose\nset of bricks that have been successfully written to is not a quorum of\nthe newView.\nThe \"may\" blocks are the ones whose set of bricks that have been\nwritten to is a\nquorum of, but not a superset of, the newView.\n***********************************************************************\n********\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "flash.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/493f120f35de19fefc23db9fbfd77cea_flash.pdf",
      "content": "\"Flash...\", by Pai, Druschel, and Zwaenepoel\nUSENIX 1999\nWhat are we doing here?\ncase studies\nexamples of research papers, learn how to write them\nunderstand content\ndig deeper into details than any one of us could alone\nput work into larger context - does it matter? is it useful?\nDiscussion format\nI'll start out by asking questions\nFeel free to interrupt me or ask your own questions\nOr to answer each others questions, or discuss each others points\nIf you don't understand, ask!\nFeel free to be critical (of me or paper), or to defend\nWhy we're reading this paper:\nto talk about server structure\nto talk about performance evaluation\nWhat are the paper's main claims?\nnew web server architecture, AMPED\nas good as it gets for cached workloads\nas good as it gets for disk-bound workloads\nDoes anyone care about web server efficiency?\nYou do if you have thousands of servers.\nWhat is the basic AMPED idea?\n[draw picture w/ helpers]\nWhat operations does the helper perform?\ndisk read()\nhow about open()? stat()?\nWhy is cached vs disk a big issue?\nWhere might the cache be?\nO/S disk block cache, or\nMaintained by web server[s]\nAlso servers cache results of file lookup, file sizes\nWhat's a reasonable number of helpers?\none or two per disk?\nmany per disk for disk-arm scheduling?\nOther techniques they discuss:\nMP\nMT (user threads? kernel threads?)\nSPED (event-driven, like 2nd lab)\nApache == MP\nZeus == SPED\nWhat performance do we expect?\nDisk-bound: AMPED > MT > MP/Apache >> SPED/Zeus\nCacheable: SPED/AMPED/Zeus > MT > MP/Apache.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nQuestions we'd like performance section to answer:\nDoes AMPED win for both cached and non-cached?\nAre process/thread context switch costs significant?\nAre benefits of single address space significant?\nAre benefits of disk I/O concurrency significant?\nWhat's the test setup?\nReal server and lots of clients. How many clients? Is one enough?\nClients run fake web browsers that issue concurrent requests.\nFigure 6:\nWhy does b/w go up with file size?\nWhat's the limiting factor for small files?\nDisk? Net? RAM? I/O bus? CPU?\nClient's ability to generate requests?\nWhat's the limiting factor for large files?\nWhy does the curve have the shape it does?\nx = file size\na = time to process zero-length request\nb = big-file serving rate, bytes-per-second\ny = bytes/time = x / (a + x/b)\nWhat are a and b?\nFigure 6(b) suggests a is about 1 millisecond.\nFigure 6(a) suggests b is about 100 mbits/second.\nWhy is there no MT line in Figure 7?\nWhy is FreeBSD faster than Solaris?\nSame hardware...\nSolaris is a commercial O/S, you'd expect it to be faster?\nWhy does the paper present Figures 6 and 7?\nIs the workload realistic? no. only one file, no disk...\nWhat have we learned?\nApache is slow.\nWhat would we still like to learn about?\nDisk-bound performance.\n\"Realistic\" performance with typical mix of big/small, cached/disk.\nEffect of various parameters (mem size, # of processes, &c)\nWhy don't they show us a simple disk-only graph like Figure 6?\nHow could we force *all* requests to use the disk?\nWould we want to force all path name lookups to use disk too?\nWhat would we learn from true disk-bound experiment?\nProbably all servers the same, we'd learn # disk reads / request.\nBest we can do is serve enough files that they don't fit in cache.\nThus mixed cache/disk workload.\nWhy is performance only 40 mbits in Figure 8, was ca. 100 in Figure 6?\navg file size apparently 10 kBytes. or 80 kilobits.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nFigure 8, why is SPED < MT for CS, but SPED > MT for Owlnet?\nWhat can we conclude from Figure 8?\nRealistic traces. Flash is a bit faster, but not radically.\nPresumably this is a mix of cached/disk requests.\nBut actual mix is not known, so we don't really know what we're\ntesting.\nHow do figures 9 and 10 shed light on cached/disk performance?\nby varying data set size, control how well data fits in ~100 MB disk\ncache.\nHow do they vary the data set size?\nHow does that affect cache vs disk?\nWhy is there a sudden decrease at around 100 mbytes in Figure 9?\nWhy at 100 mbytes, not 50 mbytes or 200 mbytes?\nWhy is b/w around 50..100 mbits for large data set sizes?\nHow many requests per second? 500 to 1000... assuming 80-kilobit\nfiles.\nIs this workload diskbound?\nWhat would b/w be if diskbound? 8 mbits/second...\nWhat's the cache hit rate?\nHit rate must be around 90%\nThat is, 10 files served per 10 millisecond disk seek (i.e. per\nmiss)\nOr 1000 files per second\nDo they in fact ever evaluate disk-bound behavior?\nFigure 9/10, Flash vs MP.\nWhy does Flash beat MP for small data set?\nWhy does Flash beat MP for large data set?\nFlash vs SPED\nWhy are Flash and SPED close for small data set?\nWhy does Flash beat SPED for large data set?\nFlash vs MT (Figure 10)\nFlash and MT have about the same behavior for all data set sizes.\nWhy?\nWhat does this mean w.r.t. whether Flash is worthwhile?\nAt right of Figure 9, why is MP < SPED?\nwe expect MP to get more I/O concurrency with disk-heavy workload\nso maybe user-level cache is small in MP?\nFigure 12, why do most lines go up w/ # clients?\nWhy does MP line go down a lot, starting at 50?\nContext switch costs? Partitioned 4MB caches?\nWhy does MT line go down a little?\nCynical view:\nShould just use MT, not Flash.\nPractical view:\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nFlash far easier to implement then kernel-supported threads!\nMuch better use of programmer time.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "frangipani.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/b22220c50d950e057daafcadea15d5d5_frangipani.pdf",
      "content": "Frangipani: A Scalable Distributed File System\nThekkath, Mann, Lee\nSOSP 1997\nWhy not primary copy?\nActually they do use primary copy inside Petal\nBut many Petal primary/backup pairs in one FS\nWork out our own design...\nHow to divide data among the network disks.\nWhat does Petal do / guarantee?\nWhat happens if a client e.g. creates a file?\nWhat steps does the server go through?\nacquire lock, append to *local* log, update local meta-data,\nrelease lock locally, reply to client.\nWhat if a client on a different server reads that file?\nS1 gets the REVOKE\nwrites log to Petal, writes meta-data to Petal, RELEASEs lock\nWhy must it write the log entry to Petal before writing the meta-data?\nWhy must it write the meta-data to Petal before releasing the lock?\nWhat if two clients try to create the same file at the same time?\nThe locks are doing two things:\nAtomic multi-write transactions.\nSerializing updates to meta-data (cache consistency).\nWhat if a server dies and it is not holding any locks?\nCan the other servers totally ignore the failure?\nWhat if a server dies while holding locks?\nCan we just ignore it until it comes back up and recovers itself?\nCan we just revoke its locks and continue?\nWhat does Frangipani do to recover?\nWhat's in a log record?\nS1 creates f2, crashes while holding lock\nhow does replay work?\nif S1 crashed before any flush of anything?\nmid-way through flushing log?\nmid-way through flushing data?\njust after all flushing, before releasing lock?\njust after releasing the lock?\nWhat effect will the logging have on ordinary performance?\nSuppose S1 deletes f1, flushes its block+log, releases lock.\nThen S2 acquires lock and creates a new f1.\nThen S1 crashes.\nWill recovery re-play the delete?\nDetails depend on whether S2 has written the block yet.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nDoes the recovery manager have to acquire locks before playing records?\nWhat if some other server currently holds the lock?\nMight the other server have stale data cached? From before replay?\nWhat if two servers crash at about the same time?\nAnd they both modified the same file, then released lock.\nHow do we know what order to replay their logs in?\nI.e. can we replay one, then the other?\nOr must we interleave in the original order?\nWhat if power failure affects all servers?\nSuppose S1 creates f1, creates f2, then crashes.\nWhat combinations of f1 and f2 are allowed after recovery?\nWhat if a server runs out of log space?\nWhat if it hasn't yet flushed corresponding blocks to Petal?\nWhat happens if the network partitions?\nCould a partitioned file server perform updates?\nServe stale data out of its cache?\nWhat if the partition heals just before the lease expires?\nCould file server and lock server disagree about who holds the lock?\nWhy isn't the lock service a performance bottleneck?\nWhat if a lock server crashes?\nWhy does their lock service use Paxos?\nWhy does Frangipani have a disk-like interface to Petal?\nFrangipani was never intended to use a disk, so no compatibility\nreason\nmight some other interface work better?\nTable 2: why are creates relatively slow, but deletes fast?\nWhy is figure 5 flat?\nWhy not more load -> longer run times?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "hagmann.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/a608a4457271be455ffa18a614ba94e7_hagmann.pdf",
      "content": "Reimplementing the Cedar File System Using Logging and Group Commit\nRobert Hagmann\nSOSP 1987\nWhy are we reading this paper?\nRobustness and crash recovery\nLogging will come up many times\nWhat are the main properties the authors want?\nRecoverable if unexpected crash (e.g. while modifying name table).\nIf single- or double-sector disk error, only that file damaged.\nFast crash recovery.\nFast ordinary operation.\nBasic on-disk FSD data structures\nname table, a b-tree, contains file names and block lists\ncontent blocks of files\nWhat data structures should look like after a file create\nnew blocks for file contents\nnew entry in file name table\nWhy unexpected crashes might be a problem\nb-tree update requires multiple disk writes\nWhy does a log help with crash recovery?\nIt makes multi-step operations atomic w.r.t. recovery\nWhat does the on-disk log look like?\nFixed area of disk, not a file, no free list for log blocks.\nProper entry: hdr, blank, hdr copy, data pages..., end, copies of\ndata..., end copy.\nbe clear about memory vs disk\nmemory:\ndisk cache. some blocks might be dirty.\nin-memory log, just most recent, not yet flushed, w/ commit points.\nVAM\ndisk:\nreal homes of data and name table blocks.\non-disk log, in thirds.\nWhat happens when you create a file in FSD?\n1. get two free pages for leader and data from VAM.\n2. update the name table b-tree in memory\n3. write leader+data to disk synchronously\n4. append modified name table pages to the log in memory\nWhen is the in-memory log forced to the disk?\nGroup commit... Every few seconds.\nWhy not log immediately, after every operation?\nWhen are the modified disk cache pages written to the disk?\nIs it OK to write them before the corresponding log records?\nIs it OK to write them after?\nWhat happens during recovery after crash+reboot?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nHow does recovery s/w find the start of the log?\nDisk has pointer to first log of oldest third, updated on entry to\nthird.\nHow does recovery find the last log record?\nhdr/end blocks must have time-stamp or something.\nWhat if the system crashed while writing to the log on disk?\nDo we assume disk wrote pages of log entry in order?\nIf end block exists and matches hdr block, we're OK.\nWhat does recovery actually do with the log records?\nCan it just start re-play at the beginning?\nI.e. what if it crashed during previous replay, is 2nd time OK?\nWhere does recovery stop re-playing?\nDoes recovery work if crash during log write?\nWhat about recovering the free list (VAM)?\nWhat if the free list had incorrect contents?\nWhat can we say about the state of the file system after\ncrash+recovery?\nSome prefix of the operations have been completed.\nI.e. every operation up to a certain point.\nMay have lost the last few operations.\nWhy don't they log data writes?\nWhen do they write the data?\nDoes that break the prefix rule?\nMay have writes from \"the future\", after the recovery point.\nWhat about delete, create, write, crash? W/ recovery point before\ndelete?\nWhat if we run out of log space during normal operation?\nHow can we be sure it's safe to start re-using the log space on disk?\nWhat if crash while writing blocks during switch to a new 1/3?\nWhy is FSD faster than CFS?\nTable 3, why does FSD do 1/6th as many I/Os for small creates?\nWhy 149 I/Os for 100 files?\nDid they evaluate whether they achieved their robustness goals?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "harp.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/99b182decb0a55e75570e81d0d9ac858_harp.pdf",
      "content": "Replication in the Harp File System\nLiskov, Ghemawat, Gruber, Johnson, Shrira, Williams\nSOSP 1991\nKey points\n2b+1 servers, keeps going w/ up to b failures, might recover from > b\nimprovements over viewstamped replication:\nsupports concurrent/overlapped operations, log to maintain order\nsupports huge state, uses log to bring recovered disks up to date\ncan recover after simultaneous power failures (of all 2b+1)\nOutline basic operation.\nClient, primary, backup, witness.\nClient -> Primary\nPrimary -> Backups\nBackups -> Primary, primary waits for all backups\nPrimary replies to Client\nPrimary tells clients to commit\nWhy does Harp use a log?\n1. keep track of multiple concurrent ops\n2. log is the in-memory state recovered by VSR\n3. log maintains order so we recover a prefix after failure\n4. log can bring a separated backup's disk up to date\nWhat is in a typical log record?\nWhy does Harp have so many log pointers?\nFP most recent client request\nCP commit point (real in primary, latest heard in slave)\nAP highest record sent to disk on this node\nLB disk has completed up to here\nGLB all nodes have completed disk up to here?\nWhy the FP-CP gap?\nSo primary doesn't need to wait for ACKs from each backup\nbefore sending next operation to backups\nHigher throughput: can overlap wait for prev op with exec of next\nProbably most useful when there are multiple clients\nWhy the CP-AP gap?\nWhy not apply to disk at CP?\nexactly what happens at AP? how are ops applied?\nWhy the AP-LB gap?\nallows delay between issue of op and when it must complete to disk\nwhy?\nWhat is the LB? How does Harp find out what the current LB is?\nWhy the LB-GLB gap?\nI.e. why not delete log record when disk write completes?\nCan't throw away log records until we know *everyone* has applied\nthem\nBecause we might need to use our log to bring someone up to date\nHow does failure recovery work?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nScenarios\n5 servers, 1-5, 1 is usually primary, 2-3 backups, 4-5 witnesses\nS2's cooling fan fails, so its cpu melts, and it crashes\nnew view\nS4 is promoted (witness -> backup)\nS4 gets copy of log starting at GLB (i.e. all ops not known to be\non disks)\nS4 starts logging all operations, but doesn't apply them\nbut GLB advances, so primary discards log entries\nwhy bother promoting S4?\nS2 gets a new CPU and reboots\nnew view\nS4 sends big log to S2, S2 plays it to get all missing operations\nS2 suffers a disk failure\nneeds to get complete disk image + log from S1 or S3\nwhat if S1 crashes just after replying to a client?\nwhere will new primary's FP and CP be after view change?\ndoes new primary have to do anything special about ops between CP and\nFP?\ndid other backups get the op?\ndoes this do the right thing for ops that the old primary *didn't*\nreply to?\nAll nodes suffer power failure just after S1 replies to a client.\nThen they all re-start.\nCan they continue?\nWhere were the logs stored while the power was out?\nWhat if they had all crashed -- could they continue?\nCrash == lost memory contents (despite UPS).\nHow do they tell the difference?\nWhy does Harp focus so much on UPS and power failure?\nSince it already has a good story for more serious h/w falures?\nS2 and S3 are partitioned (but still alive)\nCan S1+S4+S5 continue to process operations?\nS4 moves to S2/S3 partition\nCan S2+S3+S4 continue?\nS2 and S3 are partitioned (but still alive)\nS4 crashes, loses memory contents, reboots in S2/S3 partition\nCan they continue?\nDepends on what S4's on-disk view # says.\nEverybody suffers a power failure.\nS4 disk and memory are lost, but it does re-start after repair.\nS1 and S5 never recover.\nS2 and S3 save everything on disk, re-start just fine.\nCan S2+S3+S4 continue?\nIn general, how do you know you can form a view?\n1. No other view possible.\n2. Know about most recent view.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\n3. Know all ops from most recent view.\n#1 is true if you have n+1 nodes in new view.\n#2 is true if you have n+1 nodes that did not lose view # since last\nview.\nView # stored on disk, so they just have to know disk is OK.\nOne of them *must* have been in the previous view.\nSo just take the highest view number.\nNow that we know last view number,\nNeed a disk image, and a log, that together reflect all operations\nthrough the end of the previous view.\nPerhaps from different servers, e.g. log from promoted witness,\ndisk from backup that failed multiple views ago.\nIf a node recovers w/ working disk, can you really replay a log into\nit?\nWhat if log contains operations already applied to the disk?\nIf a node recovers but disk needs fsck\nIs it legal to run fsck?\nDoes Harp run fsck?\nCan you avoid fsck and repair by re-doing the log? As in FSD?\nIf a node recovers w/o disk contents, i.e. w/ empty disk\nDoes it work to copy another server's disk?\nWhat if the other server is actively serving Harp/NFS ops?\nCan we avoid pausing for the entire time of disk copy?\nHow does primary generate return values for ops?\nIt replies at CP, before ops have been applied to the file system!\nFor example, how do you know an UNLINK would succeed?\nOr the file handle of a CREATE?\nHow does Harp handle read-only operations?\ne.g. GETATTR?\nWhy doesn't it have to consult the backups?\nWhy is it correct to ignore ops between CP and FP when generating the\nreply?\nWhat if client sends WRITE then READ before WRITE reaches CP?\nDoes Harp have performance benefits?\nYes, due to UPS, no need for sync disk writes.\nBut in general, not 3x performance.\nWhy graph x=load y=response-time?\nWhy does this graph make sense?\nWhy not just graph total time to perform X operations?\nOne reason is that systems sometimes get more/less efficient w/ high\nload.\nAnd we care a lot how they perform w/ overload.\nWhy does response time go up with load?\nWhy first gradual...\nQueuing and random bursts?\nAnd some ops more expensive than others, cause temp delays.\nThen almost straight up?\nProbably has hard limits, like disk I/Os per second.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "hypervisor.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/64da807a63d13ed61c1758b055437d80_hypervisor.pdf",
      "content": "Hypervisor-based Fault-tolerance\nBressoud and Schneider\nSOSP 1995\nWhy are we reading this paper?\nWe're about to look at a bunch of systems that replicate for fault-\ntolerance\nThe hypervisor is at one extreme of the spectrum\nTwo machines run the exact same instructions\nIf one fails, the other just keeps going, no time/data/work is lost\nTotally transparent, runs existing O/S and apps\nOnly a factor of two slower\nSeems like an amazingly general/easy/practical solution to fault-\ntolerance\nWhere they are coming from\nBanks, telephone exchanges, NASA need fault-tolerant computers\n1980s saw lots of *hardware* fault-tolerant machines\nAssumption: CPU most likely part to fail due to complexity\nSo two CPUs connected to bus, single memory, disk, &c system\nAll in the same cabinet. Not distributed over the Internet.\nHypervisor is all s/w! Runs on stock h/w.\nLet's design our own\nStraw man:\ntwo identical machines\nstart with same memory contents (program and data)\njust start them executing!\nif one fails, the other keeps going\nit's *fast* -- just as fast as ordinary single computer!\nare we done?\nQ: Will they perform the same computation?\ntheorem: yes, if they execute same operations,\nin the same order, and each operation has deterministic effect\nADD instruction &c probably deterministic\ninstructions to read time-of-day register, cycle counter, priv level\nnot deterministic\nmemory system?\nlocal devices like hard disk?\nlow level: interrupt timing\nhigh level: whether disk read/write results are deterministic\nerrored blocks...\nexternal input devices like network, keyboard?\nboth CPUs need to get each input\noutput devices like network, display, printer?\nexactly one CPU should produce each output\nLet's figure out how to deal with local devices (e.g. hard disk)\nwhy not replicate all I/O hardware on each machine?\ne.g. let both machines read/write their own disk\nwrites should be identical, thus reads also\nmaybe some h/w replicas don't behave the same.\nfor example, o/s knows how to re-map a failed disk sector,\nbut the two disks won't fail at the same sectors\nso why did they replicate the main-memory system?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\ndoesn't fail? would be too slow to share?\nSo they hook up all devices to both machines: shared I/O bus\nsending commands and data to the device\nonly the \"primary\" writes the device\nhypervisor ignores backup's I/O writes\nreading results back from the device\nonly primary reads\nprimary hypervisor copies to the backup\nbackup hypervisor stalls until correct value arrives from primary\nhandling interrupts\nonly primary CPU sees real h/w interrupts\nhypervisor sends them to the backup\nhow to ensure two machines see interrupts at the same time?\ncannot let them occur at different points in app instruction stream\nthat might cause different behavior\ncould execute instructions one at a time on both prim and backup\nbackup waits until it knows whether interrupt happened after prev\ninstr\nslow...\nepochs!\nCPU h/w can be told to interrupt every N instructions\ni.e. at exactly the same point on both machines\nprimary hypervisor delays all interrupts until end of epoch\nbackup has to wait at the end of its epoch\nmaybe primary hasn't reached epoch end, hasn't seen all interrupts\nat each epoch end, primary and backup deliver all interrupts\nwhat about fail-over?\nif primary fails, backup must start doing I/O\nhow does backup know the primary has failed?\nwhat about I/O writes in the last epoch: might or might not have\nhappened.\nwhat if backup is waiting for an I/O read?\ndid the primary even issue the read request?\ndid the result come back, but not yet sent to backup?\nin the end, the O/S has to re-try the I/O!\ndo all devices follow the IO1/IO2 rules?\nhypervisor has to understand an operation was started by not finished\nso it can generate \"uncertain\" interrupt\ndevice has to support repeated operations correctly\nO/S device drivers have to know to re-try operations\nOK for disk read and disk write\nsome SCSI drivers notice time-outs and re-try\nmaybe OK for network (TCP will sort it out)\nnot so OK for keyboard, line printer, ATM bill vendor\nhow does the hypervisor know about incoming I/O data from devices?\ne.g. disk blocks just DMA's from disk\nneeds to send block data to the secondary\nno problem for programmed I/O (hypervisor can trap on LD)\npuzzle: DMA. how does the hypervisor know?\nis the machine virtualizable?\nwe can always interpret SLOWLY\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nreal question: can I run o/s and apps on the real hardware?\nwill the hardware trap to hypervisor on *every* tricky instruction?\ni.e. any instruction that might be non-deterministic\nOR reveal the existence of the hypervisor?\ntime-of-day register\nmemory-mapped I/O loads and stores\nneed to trap and substitute load values\nHP branch-and-link instruction\nHP TLB replacement\nHP \"space registers\" (page mapping tables?) writable by users (!)\nWhat if ethernet cable breaks?\nprimary still running\nbackup will promote itself\nthat's part of what the fail-stop assumption assumes away...\nWhat if we want to re-start a failed+repaired primary?\nIs this the way we should build all fault-tolerant systems?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "nfsloop.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/9326b4e7325cf7b567021eb4d8661dfb_nfsloop.pdf",
      "content": "\"A toolkit for user-level file systems,\" David Mazieres\nWe're reading this paper to\n1. See horrible details of making remote file systems work\n2. Understand the lab infrastructure\nThe paper describes a toolkit that helps you build user-level file\nsystems\nMost file systems (disk FS, real NFS, &c) live in kernel\nHard to modify, hard to debug, need privs\nThe basic trick: user program looks like network NFS server\nFigure 1\nWhat's going on in Figure 2?\nWhy doesn't the user-level file call mount() itself?\nWhat does mount() do for an NFS server?\nkernel tables. vnode. file descriptor.\nWhy is mount() privileged?\nsetuid programs on FS.\ndisk fs might be corrupted. nfs server hangs lock up client.\nmisbehaving server can wedge client in other ways.\nWhat happens if a mounted NFS server stops responding?\nWhat is the point of the automounter?\nWhat happens when a user-level file system starts up?\ngives fd, dir name to automounter.\nWhat happens when a user process first uses an automounted FS?\nlookup(d) -> automounter\nreply: fh1\nREADLINK(fh1) -> automounter\nreply: \"/sfs/.mnt/0\"\nLOOKUP(\".../0\") -> am\nreply: fh2\nREADLINK(fh) -> am\nno answer for now...\nautomounter asks nfsmounter to mount FS's fd on /sfs/d\nnfsmounter: mount(fd, \"/sfs/d\")\nLOOKUP(d) -> am\nreply: fh3, not the same, real directory\nmount() then marks that vnode as mounted, with fd\nam replies to READLINK: \"/sfs/d\"\nLOOKUP(d) -> am\nreply: fh3\nGETATTR(fh3) now goes to FS server\nWhat does nfsmounter do if, say, the automounter crashes?\nHow does the nfsmounter learn of the crash?\nWhat's the goal of cleanup?\n****\nWhat's the big point?\nWe might want to implement user-level file systems.\nCan do it by pretending to be an NFS server.\nDave will help us get all the details right.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nPicture. app, client NFS, udp socket, server process.\nExamples?\n/proc. crypt fs. client side of network file system.\nautomounter: /net/fs.mit.edu\nWhat problems would we run into if we did this ourselves?\nNeed to parse all those RPCs.\nHow to mount the file system?\nWhat if our server crashes?\nPossibility of deadlock in buffer cache.\nAutomounters have special mount-in-place problem.\nWhat does the mount() system call do?\nFor NFS, takes a socket file descriptor and a directory name.\nLooks up directory name's vnode.\nTags vnode as a mount point, remembers socket.\nSubsequent vnode ops redirected to new type+socket.\nNote that we can mount on a directory in a mounted file system.\nThus /net/fs.mit.edu/... can work.\nWhat are the problems with crashed user-level servers?\nWon't respond, programs will hang forever.\nWant to unmount, to avoid future hanging processes,\nand maybe want to mount something else there.\nUnmount requires naming the mounted-on vnode.\nSpecial trouble if intermediate mounted file system doesn't work.\nHow does the paper help out?\nnfsmounter does all mounts.\nYou give it your user-level server's socket, and mount point name.\nYou give it *both ends* of the socket.\nnfsmounter keeps the socket file descriptor, as well as giving it to\nkernel.\nnfsmounter knows when your program crashes (eof on some other\nsocket?).\nnfsmounter then answers NFS requests from the kernel on the socket.\nReturns errors for access to most files.\nBut returns something reasonable along paths to mount points.\nThus you can un-mount nested mounts.\nWhat is the problem with deadlock?\nUser-level file server needs a page of physical memory.\nTo read from the disk, or to page in.\nKernel chooses some dirty page, wants to clean by writing out.\nSo user-level file server now waiting for write to finish.\nDirty page might be from that user-level server's file system!\nIt cannot respond to the WRITE rpc because it's blocked in the\nkernel.\nHow does the paper help with deadlock?\nAdvice to avoid blocking disk I/O.\nUse Flash-like child processes to do disk I/O.\nLock down all memory, to avoid page faults.\nWhy do you want an automounter?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY].\n\nWithout, you have to have lists of how to mount things.\nI.e. /home/to2 is toil:/disk/to2\nWith, you have generic support for rule-based mounting\nI.e. /home/hostname/fsname is hostname:/disk/fsname\nCritical for large installations or wide-area sharing.\nWhat does automounting in place mean?\nBefore: /net is served by user-level automounter.\nIt sees your LOOKUP for fs.mit.edu.\nAfter: fs.mit.edu NFS server mounted on /net/fs.mit.edu\nwhich is a vnode served by the automounter (tho not used for much).\nuser-level automounter doesn't see any more LOOKUPs for it.\nWhy is automounting in place hard?\nIt all starts with some random process doing a LOOKUP for\n/net/fs.mit.edu\nautomounter (or nfsmounter) wants to say mount(fd, \"/net/fs.mit.edu\")\nKernel mount() will try to find vnode for /net/fs.mit.edu\nTrying to ask automounter (again) to LOOKUP /net/fs.mit.edu\nWorse, original process has locked /net directory vnode.\nHow does the paper help automounting in place?\nMust answer the original LOOKUP right away.\nSo must have a vnode for the answer.\nResponds with a SYMLINK vnode pointing to /net/.mnt/0\nClient NFS code does READLINK, then a LOOKUP for /net/.mnt/0\nautomounter does not answer yet\nThen automounter does whatever to get socket to real NFS server\nAsks nfsmounter to mount the socket on /net/fs.mit.edu\nmount() system call does a lookup for /net/fs.mit.edu\nHow does automounter know not to treat it like a client?\nnfsmounter has some weird group ID, which is in the NFS RPC.\nThis time, automounter returns an ordinary directory vnode.\nmount() succeeds.\nNow automounter answers original client's LOOKUP for /net/.mnt/0\nReturns a SYMLINK pointing to /net/fs.mit.edu\nAnd we're done.\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "porcupine.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-824-distributed-computer-systems-engineering-spring-2006/72875a5da860ba7a618bffcd6a492a00_porcupine.pdf",
      "content": "Manageability, availability and performance in Porcupine:\na highly scalable, cluster-based mail service\nSaito, Bershad, Levy\nSOSP 1999\nfigure 3: is porcupine faster than a sendmail cluster? why?\ndoes porcupine scale better than a sendmail cluster? why?\nwhy 800 msgs/sec with 30 servers?\nhow much slower is replication? why?\nwhat is the main bottleneck?\ncpu? disk? network?\ncould they have got same performance w/ slower cpu? disk? net?\nfigure 6: what is skew and why do they care?\nwhy does performance for some systems go down with increased skew?\nwhy does performance not go down for dynamic lines?\nwhy doesn't their sophisticated load balance beat random?\nwhy doesn't random have terrible performance due to no affinity?\nwhy is spread > 1 helpful for porcupine? for static?\nwhy does performance go down at 300 in Fig 10?\nwhat happens to incoming mail in the middle of Figure 10?\nwhat about deletes of msgs in failed fragment?\nwhat happens when users read mail in the middle of Figure 10?\nhow does porcupine find a user's mail after repair?\nwhy isn't the system slower after 600 than before 300?\nwe'd expect each user's mail to be more spread out...\nwhat happens to incoming mail in the middle of Figure 11?\nwhat about deletes?\nwhy the slow rise after 600 in Figure 11?\nCite as: Robert Morris, course materials for 6.824 Distributed Computer Systems Engineering,\nSpring 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of\nTechnology. Downloaded on [DD Month YYYY]."
    }
  ]
}