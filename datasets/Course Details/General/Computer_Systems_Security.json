{
  "course_name": "Computer Systems Security",
  "course_description": "6.858 Computer Systems Security is a class about the design and implementation of secure computer systems. Lectures cover threat models, attacks that compromise security, and techniques for achieving security, based on recent research papers. Topics include operating system (OS) security, capabilities, information flow control, language security, network protocols, hardware security, and security in web applications.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Software Design and Engineering",
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Software Design and Engineering"
  ],
  "syllabus_content": "This course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\nCourse Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\n6.033 Computer System Engineering\n\nDescription\n\n6.858 Computer Security\nstudies the design and implementation of secure computer systems. Lectures cover threat models, attacks that compromise security, and techniques for achieving security, based on recent research papers. Topics include operating system (OS) security, capabilities, information flow control, language security, network protocols, hardware security, and security in web applications. Assignments include labs that involve implementing and compromising a secure web server and web application, and a group final project.\n\n6.858 is primarily intended for seniors and Masters of Engineering students who want to learn about how to build secure computer systems in detail. Ph.D. students are also welcome. Students can use 6.858 to fulfill the engineering concentration requirements for Computer Systems.\n\nLectures\n\nEach lecture will cover a paper in systems security. Read the paper before lecture, and submit by 10PM the night before:\n\nAn answer to the homework reading question.\n\nYour own question about the paper (will try to answer in lecture).\n\nWe'll discuss the paper in class. Please interrupt, ask questions, and point out mistakes.\n\nQuizzes\n\nThere will be two quizzes during our regular lecture time slot. No \"final exam\" during finals week; second quiz near end-of-term.\n\nAssignments\n\nThere are 6 labs and a final project in this course. Labs will look like real-world systems, in some respects: There are many interacting parts written in different languages. We'll look at / write x86 asm, C, Python, Javascript, etc...\n\nThere will be a final project at the end of the course (groups of 3-4 people), and presentations during the last week of class. Think of projects you'd like to work on as you're reading papers. Either attack or defense-oriented projects are possible. It is ok to combine this project with other class projects or your own research.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\n2 Quizzes\n\n20%\n\nLab Exercises\n\n35%\n\nFinal Project and Presentation\n\n25%\n\nHomework and Class Participation\n\n20%\n\nLab exercises will be graded on the correctness based on both the lab assignment and whether they fulfill the specifications imposed by the grading / checking scripts. Grading will be done with a staff-version of the Makefile and grading scripts, so you should pass all the tests without any modifications to those files.\n\nTurn-In Policy\n\nYou are required to turn in each lab; if you have not turned in all of the labs, you will receive an F. Labs that are turned in but score 0 points will receive a D. You have a total of 72 late hours to use throughout the semester. After you have used up your late hours, each additional day late will incur a full letter grade penalty. Saturday and Sunday both count as days. (Late days are tracked automatically, so you don't need to email before using one.)\n\nCollaboration\n\nYou may not collaborate on quizzes. You are welcome to discuss the labs with other students, but you should complete all assignments on your own, and you should carefully acknowledge all contributions of ideas by others, whether from classmates or from sources you have read. Final projects will be in groups, where you should collaborate.\n\nWarning About Security Work / Research on MITnet (and in General)\n\nYou will learn how to attack systems so that you know how to defend them. Just because something is technically possible, doesn't mean it's legal.",
  "files": [
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Review, Fall 2014",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/1614bcb3be1e439a96404029922f183a_MIT6_858F14_q14_1_review.pdf",
      "content": "Quiz 1: Next Wednesday Walker\nduring regular class time\n- L1: Security Basics\n- L2: Baggy Bounds\n- L3: Hacking Blind\n- L4: OKWS\n- L5: Guest Lec.\n- L6: Capsicum &\nConf. Deputy\n- L7: NaCl\n- L8: OWASP &\nTangled Web\n- L9: Django & CSRF\n- L10: Klee & Symbolic\nExec.\n- L11: Ur/Web\n- L12: TCP/IP\n- L13: Kerberos\n- L14: ForceHTTPs\n- -\n- Lab1: Buffer Overf.\n- Lab2: Priv. Sep.\n- Lab3: Concolic Exec.\n\nHow to study for Quiz 1\n- Do old exams.\n- Review papers and lecture notes together.\n- Youtube videos available for each lecture.\n- Student questions and lecture questions for\neach lecture are available on the\nsubmission page.\n\nL1: Intro\nPolicy\n-\nthe goal you want to achieve\n-\nnegative goals (e.g. \"system should not crash\")\n-\nCIA (confidentiality, integrity, availability)\nThreat model\n-\nattacker's capabilities (access to source? physical access? etc.)\nMechanism\n-\ntechnology to enforce policy (unix permissions, capabilities, etc.)\n\nL12: TCP/IP and Network Security\n1. TCP Handshake\n2. Spoofing connections\n3. DOS attacks (RST, SYN Flooding)\n4. DNS, ARP, DHCP, BGP, ...\n5. Old Quiz Problems\n\nC\n[L12 1/5] TCP Handshake\nSRC: C\nDST: S\nDATA(ISNC)\nS\nSRC: C\nDST: S\nSYN(ISNc)\nSRC: S\nDST: C\nSYN(ISNS)\nACK(ISNC)\nSRC: C\nDST: S\nACK(ISNS)\n\n[L12: 2/5] Connection Spoofing\n???\nC\nS\nA\nSRC: C\nDST: S\nSYN(ISNA)\nSRC: S\nDST: C\nSYN(ISNS)\nACK(ISNA)\nSRC: C\nDST: S\nACK(ISNS)\nSRC: C\nDST: S\nIDATA(ISN A)\nguess\nISN s ?\nSRC: C\nDST: S\nRST\n\n[L12: 2/5] Connection Spoofing\nProblem\n-\nEasy for A to guess ISNs\n-\nISN J (ISN\n+ 250000 * Lt\ns\ns)old\nsec\n-\nA can get (ISN\nby sending a legal SYN(ISNA) packet to S.\ns)old\nSolution\n-\nMake it hard for A to guess ISNS\n-\nISN\n+ F(ipaddr\n, portn\n, ipaddrdst, portndst, key)\ns J ISNoldstyle\nsrc\nsrc\n-\nF a cryptographic hash function (e.g. SHA1)\n\n[L12: 2/5] Connection Spoofing\n- If A knows SNC:\no Reset connection: RST(SNC+ )\no Inject data into existing stream: DATA(SNC)\n\n[L12: 3/5] DOS Attacks\nSYN Floods\nC\nSRC: Co\nDST: S\nSYN(ISNco)\nS\nSRC: C\nDST: S\nSYN(ISNc)\nSRC: C2\nDST: S\nSYN(ISNc2)\nSRC: C3\nDST: S\nSYN(ISNc2) ...\nCONN\nSTATE\n...\nC0\nwaiting\nC1\nwaiting\nC2\nwaiting\n...\n...\n\n!\"\n#\n\n$\n% & ' (\n\n) * +, , , , - ..\n\n) * +, , , , - ..\n\n[L12: 4/5] DNS ARP DHCP B4P\nSee lecture notes and paper for:\n- DNS response spoofing\n- DNS amplification\n- ARP spoofing\n- DHCP spoofing\n- others\n\n!\"\"\n\"\n\"\n\n#\n\n$\"\n\n%&\n\n\"\n'\n\n()' *'\"\"\n\n+,+\n, \"\n' -\n.'\n\n\"\n\n'\"\n\n/\n\nL13: 8erberos\n1. Overview\n2. Practice problems\nFor fun, see:\nhttp://web.mit.edu/kerberos/dialogue.html\n\nC\nL13: 8erberos Oeereiew\nT\n= {s, c, addr, timestamp, life, K\n}\nA = {c, addr, timestamp}\nc,s\nc,s\nc\nKc,s\nK\nTGS\n(c,\nTGS)\n{{T\n}\n,\nc,TGS\nKtgs\nKc,TGS}Kc\n(S,\n{Tc,TGS}Ktgs,\nc,S\n{Ac}Kc,tgs)\n{{T\n} ,\nc,S\nKs\nK\n}Kc,tgs\n({Ac}Kc,s,\n{T\n} )\nc,S\nKs\n{Timestamp}Kc,s\nS\n{ATA}Kc,s\n\nL13: 200: Quiz 2 Problem 2\nBob logs into an Athena workstation, which uses Kerberos to obtain a ticket for\nbobRATHENA.MIT.EDU, and then runs Bob's mail client, which contacts Bob's\npost office server to fetch new messages.\n2. Alice doesn't want Bob to know about an upcoming event, which was\nannounced to Bob via email. To this end, Alice plans to intercept Bob's\ncommunication with his post office server, and to pretend that Bob has no\nnew mail. Alice can observe and modify all network packets sent by Bob.\nHow does Kerberos prevent Alice from impersonating Bob's mail server?\nBe as specific as possibleS explain how Bob can tell between Alice and the\nreal mail server in terms of network packets.\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\nL13: 200: Quiz 2 Problem 3\nNow, Alice wants to read Bob's email, and intercepts all\nnetwork packets ever sent and received by Bob's\nworkstation (which is the only computer that Bob uses).\nHowever, Alice does not know Bob's password to access\nBob's post office server, and Bob's packets to and from the\npost office server are protected by Kerberos.\n3. Suppose that after Bob reads and deletes all of his mail,\nAlice learns what Bob's password was. Describe how\nAlice can obtain Bob's past messages.\n\nL13: 2012 Quiz 1 Problem 7\nBen Bitdiddle is designing a file server that clients connect\nto over the network, and is considering using either\nKerberos (as described in the paper) or SSL/TLS (without\nclient certificates, where users authenticate using\npasswords) for protecting a client's network connection to\nthe file server. For this question, assume users choose\nhard-to-guess passwords.\n6. Would Ben's system remain secure if an adversary\nlearns the server's private key, but that adversary\ncontrols only a single machine (on the adversary's own\nhome network), and does not collude with anyone else?\nDiscuss both for Kerberos and for SSL/TLS.\n\nL13: 2013 Quiz 1 Problem ;\nIn a Unix Kerberos implementation, each user's tickets\n(including the TGT ticket for the TGS service) are stored in\na per-user file in /tmp. The Unix permissions on this file are\nsuch that the user's UID has access to that file, but the\ngroup and others do not.\n7. Ben Bitdiddle wants to send an email to Alyssa, and to\ninclude a copy of the Kerberos paper as an attachment,\nbut because he stayed up late studying for this quiz, he\naccidentally sends his Kerberos ticket file as an\nattachment instead. What can Alyssa do given Ben's\nticket file? Be precise.\n\nL13: 2013 Quiz 1 Problem <\n8. Ben Bitdiddle stores his secret files in his Athena AFS\nhome directory. Someone hands Alyssa P. Hacker a\npiece of paper with the key of the Kerberos principal of\nall-night-tool.mit.edu, which is one of the athena.dialup.\nmit.edu machines.\nCould Alyssa leverage her knowledge of this key to get\naccess to Ben's secret files? Assume Alyssa cannot\nintercept network traffic. Explain either how she could\ndo so (and in what situations this might be possible), or\nwhy it is not possible.\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\n[L13] SSL/HTTPs/=orceHTTPs\n- Covered in lecture next Monday\n- HTTPs handles authentication and encryption for web\n- Still problematic in some cases\no ForceHTTPs fixes some browser quirks\n\n7.<5< Quiz Reeiew\nBlind ROP\n\n7.<5< Quiz Reeiew\nOWASP/Tangled Web\n\nSecurity Risks\n- Open Redirectors\n- Tulnerable Software\n- CSRF/USRF\n- Function Level Access Control\n- Data Exposure\n\nSecurity Risks\n- Misconfigurations\n- Insecure Direct Object Reference\n- USS\n- Broken Session Management\n- Injections\n\nSame Origin Policy\n- Dependent on Protocol and Host\n- Blocks Execution and Access\n- Not foolproof\no Vava\no Flash\n\nSame Origin Policy\n- Dependent on Protocol and Host\n- Blocks Execution and Access\n- Not foolproof\no Vava\no Flash\n\nContent Security Policy\n- Specify Constraints on Loading\n- Prevent \"malicious\" scripts\n- Still Tulnerable\n\nOKWS\n- Separate each service into a distinct process\n- Each service runs as a sepreate UID and GID\n- Each service is chrooted\n- Database proxy is used to enforce query structure\n\nOKWS\n- Okld launches all services, creates sockets, run as root\n- No protection against XXS\n- Pubd stores all templates\n- Oklogd keeps log of all activities, chroot to its own\nsandbox\n\nDB Proxy\n- Each service is given a subset of possible queries it is\nallowed to make\n- Each service passes a token to the db proxy with it's\nquery that proves its identity\n\nDAC\n- Each object has a set of permissions\n- Applications set permissions on objects\n- Privileges are checked when a program access\nan object\n\nDAC Problems\n- Only Root can create new users\n- Sometimes hard to customize permissions\n- Some objects don't have clear configurable access control\nlist\n\nMAC\n- Users can't change the policy\n- Tries to enforce military classified levels\n- Policy is separated from application code\n\nMAC Problems\n- The system controls who can access a newly created\nobject\n- Hard to customize permissions\n\nCapabilities\n- Token based access\n- Can have different tokens for read vs write\n- Each object accessed by the file handler\n- Tokens are passed down to forked processes\n\nCaps Mode\n- Once a process enters it can't leave caps mode\n- Name space is restricted, can't use \"..\"\n- Unix permissions still apply\n- Allowed operations stored in file descriptor\n\nCapabilities\nAdvantages:\n- Any process can create a new sandbox (even a sandbox)\n- Very easy to customize access\nDisadvantages:\n- No global namespace\n- Hard to keep track of who has access to persistent files\n\nUrWeb\n- Only one programming language is used for the entire\napplication\n- All objects passed between different parts of the\napplication are strongly typed\n- Instead of Http requests, clients call typed functions that\nare run on the server atomically\n\nStrongly Typed\nStrongly typed objects are objects that can only be used in\nspecific functions of the application. This prevents XXS\nbecause string input from the user cannot be transformed\ninto HTML or javascript unless the author of the application\nexplicitly allows it.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2009",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/3b2e42fab94f55ea6c83fff3f4b071f1_MIT6_858F14_q09-1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.893 Fall 2009\nQuiz I Solutions\nAll problems are open-ended questions. In order to receive credit you must answer the question\nas precisely as possible. You have 80 minutes to finish this quiz.\nWrite your name on this cover sheet.\nSome questions may be harder than others. Read them all through first and attack them in the\norder that allows you to make the most progress. If you find a question ambiguous, be sure to\nwrite down any assumptions you make. Be neat. If we can't understand your answer, we can't\ngive you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES EXAM.\nPlease do not write in the boxes below.\nI (xx/18)\nII (xx/16)\nIII (xx/18)\nIV (xx/8)\nV (xx/18)\nVI (xx/8)\nVII (xx/8)\nVIII (xx/6)\nTotal (xx/100)\nThe mean score on the quiz was 67, median was 62, and standard deviation 15.\nName:\n\nI Buffer Overflows\nBen Bitdiddle is building a web server that runs the following code sequence, in which process_req()\nis invoked with a user-supplied string of arbitrary length. Assume that process_get() is safe, and for\nthe purposes of this question, simply returns right away.\nvoid process_req(char *input) {\nchar buf[256];\nstrcpy(buf, input);\nif (!strncmp(buf, \"GET \", 4))\nprocess_get(buf);\nreturn;\n}\n1. [6 points]:\nBen Bitdiddle wants to prevent attackers from exploiting bugs in his server, so he\ndecides to make the stack memory non-executable. Explain how an attacker can still exploit a buffer\noverflow in his code to delete files on the server. Draw a stack diagram to show what locations on the\nstack you need to control, what values you propose to write there, and where in the input string these\nvalues need to be located.\nAnswer: An attacker can still take control of Ben's server, and in particular, remove files, by using\na \"return-to-libc\" attack, where the return address is overflowed with the address of the unlink\nfunction in libc. The attacker must also arrange for the stack to contain proper arguments for unlink,\nat the right location on the stack.\n\n2.\n[6 points]:\nSeeing the difficulty of preventing exploits with a non-executable stack, Ben\ninstead decides to make the stack grow up (towards larger addresses), instead of down like on the x86.\nExplain how you could exploit process_req() to execute arbitrary code. Draw a stack diagram\nto illustrate what locations on the stack you plan to corrupt, and where in the input string you would\nneed to place the desired values.\nAnswer: The return address from the strcpy function is on the stack following the buf array. If\nthe attacker provides an input longer than 256 bytes, the subsequent bytes can overwrite the return\naddress from strcpy, vectoring the execution of the program to an arbitrary address when strcpy\nreturns.\n\n3. [6 points]: Consider the StackGuard system from the \"Buffer Overflows\" paper in the context of\nBen's new system where the stack grows up. Explain where on the stack the canary should be placed,\nat what points in the code the canary should be written, and at what points it should be checked, to\nprevent buffer overflow exploits that take control of the return address.\nAnswer: The canary must be placed at an address immediately before each function's return address\non the stack. Because the stack grows up, this space must be reserved by the caller (although it's OK\nif the callee puts the canary value there, before executing any code that might overflow the stack and\ncorrupt the return address). The callee must verify the canary value before returning to the caller.\n\nII XFI\n4. [2 points]: Suppose a program has a traditional buffer overflow vulnerability where the attacker\ncan overwrite the return address on the stack. Explain what attacks, if any, an attacker would be able\nto mount if the same program is run under XFI. Be specific.\nAnswer: Because XFI has two stacks, the attacker will not be able to exploit a traditional buffer\noverflow (corrupting the return address). The attacker may still be able to corrupt other data or pointers\non the allocation stack; see below.\n5. [4 points]:\nSuppose a program has a buffer overflow vulnerability which allows an attacker\nto overwrite a function pointer on the stack (which is invoked shortly after the buffer is overflowed).\nExplain what attacks, if any, an attacker would be able to mount if the same program is run under XFI.\nBe specific.\nAnswer: The attacker can cause the module to start executing the start of any legal function in the\nXFI module, or any legal stub that will in turn execute allowed external functions.\n\n6. [4 points]:\nSuppose a malicious XFI module, which is not allowed to invoke unlink(),\nattempts to remove arbitrary files by directly jumping to the unlink() code in libc. What precise\ninstruction will fail when the attacker tries to do so, if any?\nAnswer: The CFI label check before the jump to unlink will notice that the unlink function does\nnot have the appropriate CFI label, and abort execution.\n7. [6 points]:\nSuppose a malicious XFI module wants to circumvent XFI's inline checks in its\ncode. To do so, the module allocates a large chunk of memory, copies its own executable code to it\n(assume XFI is running with only write-protection enabled, for performance reasons, so the module\nis allowed to read its own code), and replaces all XFI check instructions in the copied code with NOP\ninstructions. The malicious module then calls a function pointer, whose value is the start of the copied\nversion of the function that the module would ordinarily invoke. Does XFI prevent an attacker from\nbypassing XFI's checks in this manner, and if so, what precise instruction would fail?\nAnswer: XFI assumes and relies on the hardware/OS to prevent execution of data memory (e.g. the\nNX flag on recent x86 CPUs).\n\nIII Privilege Separation\n8. [4 points]: OKWS uses database proxies to control what data each service can access, but lab 2\nhas no database proxies. Explain what controls the data that each service can access in lab 2.\nAnswer: Lab 2 relies on file permissions (and data partitioning) to control what service can access\nwhat data.\n9. [8 points]:\nIn lab 2, logging is implemented by a persistent process that runs under a separate\nUID and accepts log messages, so that an attacker that compromises other parts of the application\nwould not be able to corrupt the log. Ben Bitdiddle dislikes long-running processes, but still wants to\nprotect the log from attackers. Suggest an alternative design for Ben that makes sure past log messages\ncannot be tampered with by an attacker, but does not assume the existence of any long-running user\nprocess.\nAnswer: One approach may be to use a setuid binary that will execute the logging service on-demand\nunder the appropriate user ID.\n\n10. [6 points]:\nBen proposes another strawman alternative to OKWS: simply use chroot() to\nrun each service process in a separate directory root. Since each process will only be able to access\nits own files, there is no need to run each process under a separate UID. Explain why Ben's approach\nis faulty, and how an attacker that compromises one service will be able to compromise other services\ntoo.\nAnswer: Processes running under the same UID can still kill or debug each other, even though they\ncannot interact through the file system.\n\nIV Information Flow Control\n11. [8 points]: This problem was buggy; everyone received full credit.\n\nV Java\n12. [4 points]:\nWhen a privileged operation is requested, extended stack introspection walks up\nthe stack looking for a stack frame which called enablePrivilege(), but stops at the first stack\nframe that is not authorized to call enablePrivilege(). Give an example of an attack that could\noccur if stack inspection did not stop at such stack frames.\nAnswer: A luring attack, whereby trusted code that has called enablePrivilege() accidentally\ninvokes untrusted code, which can then perform privileged operations.\n\n13. [8 points]:\nSuppose you wanted to run an applet and allow it to connect over the network to\nweb.mit.edu port 80, but nowhere else. In Java, opening a network connection is done by constructing\na Socket object, passing the host and port as arguments to the constructor. Sketch out how you would\nimplement this security policy with extended stack introspection, assuming that the system library\nimplementing sockets calls checkPrivilege(\"socket\") in the Socket constructor. Explain\nhow the applet must change, if any.\nAnswer: Something like the following code:\npublic class MitSocketFactory {\npublic static Socket getSocket() {\nenablePrivilege(\"socket\");\nreturn new Socket(\"web.mit.edu\", 80);\n}\n}\nThe applet's code would need to invoke MitSocketFactory.getSocket() instead of using\nthe Socket constructor directly.\n\n14. [6 points]: Sketch out how you would implement the same security policy as in part (b), except\nby using name space management. Explain how the applet must change, if any.\nAnswer: Replace the Socket object with MitSocket in the applet's namespace:\npublic class MitSocket extends Socket {\npublic MitSocket(String host, int port) {\nsuper();\nif (!host.equal(\"web.mit.edu\") || port != 80)\nthrow SecurityException(\"only web.mit.edu:80 allowed\");\nconnect(host, port);\n}\n...\n}\nThe applet would not have to change. Note that MitSocket's constructor does not call super(host,\nport). Instead, it invokes super() and calls connect(host, port) later. Invoking the\nsuper(host, port) constructor would have allowed the applet to open connections to arbitrary\nhosts (which would then be immediately closed), by constructing an MitSocket object with the\nright host and port arguments, since the security check comes after the superclass constructor.\n\nVI\nBrowser\n15.\n[8 points]:\nThe paper argues that the child policy (where a frame can navigate its immedi-\nate children) is unnecessarily strict, and that the descendant policy (where a frame can navigate the\nchildren of its children's frames, and so on) is just as good. Give an example of how the descendant\npolicy can lead to security problems that the child policy avoids.\nAnswer: Consider a web site that contains a login frame, where users are expected to input passwords.\nUnder the descedant policy, an attacker can put the web site in a frame, and navigate the login frame\n(a grandchild) to http://attacker.com, which looks similar to the original one, to steal passwords.\n\nVII Resin\n16. [8 points]: Sketch out the Resin filter and policy objects that would be needed to avoid cross-\nsite scripting attacks through user profiles in zoobar. Assume that you have a PHP function to strip\nout JavaScript.\nAnswer: There are several possible solutions. One approach is to define two \"empty\" policies, Un\nsafePolicy and JSSanitizedPolicy, the export check functions of which do nothing. Input strings are\ntagged UnsafePolicy, and the PHP function to strip out JavaScript attaches JSSanitizedPolicy to re\nsulting strings. The standard output filter checks that strings must contain neither or both policies.\n\nVIII 6.893\nWe'd like to hear your opinions about 6.893, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n17. [2 points]: How could we make the ideas in the course easier to understand?\nMore and simpler examples to illustrate the problem;\nMore labs.\n18. [2 points]: What is the best aspect of 6.893?\nLabs;\nRecent papers.\n19. [2 points]: What is the worst aspect of 6.893?\nLong, conceptual papers;\nRepetitive, time-consuming labs.\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2010",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/633732f507738579e7abb86b5acf11e8_MIT6_858F14_q10-1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2010\nQuiz I\nAll problems are open-ended questions. In order to receive credit you must answer the question\nas precisely as possible. You have 80 minutes to finish this quiz.\nWrite your name on this cover sheet.\nSome questions may be harder than others. Read them all through first and attack them in the\norder that allows you to make the most progress. If you find a question ambiguous, be sure to\nwrite down any assumptions you make. Be neat. If we can't understand your answer, we can't\ngive you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES EXAM.\nPlease do not write in the boxes below.\nGrade for quiz1\nMean 49, Median 51, Std. dev. 14\n\nI Baggy bounds checking\nSuppose that you use Baggy bounds checking to run the following C code, where X and Y are constant\nvalues. Assume that slot size is 16 bytes, as in the paper.\nchar *p = malloc(40);\nchar *q = p + X;\nchar *r = q + Y;\n*r = '\\0';\nFor the following values of X and Y , indicate which line number will cause Baggy checking to abort, or\nNONE if the program will finish executing without aborting.\nAnswer: Recall that Baggy bounds checking rounds up the allocation size to the nearest power of 2 (in this\ncase, 64 for pointer p), and can track out-of-bounds pointers that are up slot size/2 out of bounds.\n1. [2 points]: X = 45, Y = 0\nAnswer: NONE: the access is within the 64 bytes limit.\n2. [2 points]: X = 60, Y = 0\nAnswer: NONE: the access is within the 64 bytes limit.\n3. [2 points]: X = 50, Y = -20\nAnswer: NONE: the access is within the 64 bytes limit.\n4. [2 points]: X = 70, Y = -20\nAnswer: NONE: q goes out of bounds, but by less than slot size/2, so r is in bounds again.\n5. [2 points]: X = 80, Y = -20\nAnswer: Line 2: since q goes out of bounds by more than slot size/2, Baggy aborts the pointer\narithmetic. (We also accepted the answer of Line 4, due to some confusion.)\n6. [2 points]: X = -5, Y = 4\nAnswer: Line 4: the reference is 1 byte before the start of the object, i.e. out of bounds.\n7. [2 points]: X = -5, Y = 60\nAnswer: NONE: within the 64-byte object bounds.\n8. [2 points]: X = -10, Y = 20\nAnswer: Line 2: since q goes out of bounds by more than slot size/2, in the negative direction. (We\nalso accepted the answer of Line 4, due to some confusion.)\n\nII Control hijacking\nConsider the following C code:\nstruct foo {\nchar buf[40];\nvoid (*f2) (struct foo *);\n};\nvoid\nf(void)\n{\nvoid (*f1) (struct foo *);\nstruct foo x;\n/* .. initialize f1 and x.f2 in some way .. */\ngets(x.buf);\nif (f1)\nf1(&x);\nif (x.f2) x.f2(&x);\n}\nThere are three possible code pointers that may be overwritten by the buffer overflow vulnerability: f1,\nx.f2, and the function's return address on the stack. Assume that the compiler typically places the return\naddress, f1, and x in that order, from high to low address, on the stack, and that the stack grows down.\n9. [5 points]:\nWhich of the three code pointers can be overwritten by an adversary if the code is\nexecuted as part of an XFI module?\nAnswer: x.f2 can be overwritten, because it lives at a higher address than x.buf on the allocation\nstack. f1 and the return address live on the scoped stack, which cannot be written to via pointers.\n\n10. [5 points]:\nWhat code could the adversary cause to be executed, if any, if the above code is\nexecuted as part of an XFI module?\nAnswer: Any function inside the XFI module that is the target of indirect jumps (i.e., has an XFI\nlabel), and any stubs for allowed external functions (which also have XFI labels). The adversary\ncannot jump to arbitrary functions inside the XFI module that are not the targets of indirect jumps\n(and thus do not have an XFI label).\n11. [5 points]:\nWhat code could the adversary cause to be executed, if any, if the above code is\nexecuted under control-flow enforcement from lab 2 (no XFI)?\nAnswer: Any code that was jumped to during the training run at the calls to f1 or x.f2, or any call\nsites of this function f during the training run.\n\nIII OS protection\nBen Bitdiddle is running a web site using OKWS, with one machine running the OKWS server, and a\nseparate machine running the database and the database proxy.\n12. [12 points]:\nThe database machine is maintained by another administrator, and Ben cannot\nchange the 20-byte authentication tokens that are used to authenticate each service to the database\nproxy. This makes Ben worried that, if an adversary steals a token through a compromised or ma\nlicious service, Ben will not be able to prevent the adversary from accessing the database at a later\ntime.\nPropose a change to the OKWS design that would avoid giving tokens to each service, while providing\nthe same guarantees in terms of what database operations each service can perform, without making\nany changes to the database machine.\nAnswer 1: Implement a second proxy on the OKWS machine that keeps the real database tokens,\naccepts queries from services (authenticating the service using UIDs or another token), and forwards\nthe queries to the real database server / proxy.\nAnswer 2: Establish connections to the database proxies in the launcher, send the 20-byte token from\nthe launcher, and then pass the (now authenticated) file descriptors to the services.\n\n13. [5 points]: Ben is considering running a large number of services under OKWS, and is worried\nhe might run out of UIDs. To this end, Ben considers changing OKWS to use the same UID for\nseveral services, but to isolate them from each other by placing them in separate chroot directories\n(instead of the current OKWS design, which uses different UIDs but the same chroot directory).\nExplain, specifically, how an adversary that compromises one service can gain additional privileges\nunder Ben's design that he or she cannot gain under the original OKWS design.\nAnswer: The compromised service could use kill or ptrace to interfere with or take over other\nservices running under the same UID.\n\nIV Capabilities and C\nBen Bitdiddle is worried that a plugin in his web browser could be compromised, and decides to apply some\nideas from the \"Security Architectures for Java\" paper to sandboxing the plugin's C code using XFI.\nBen decides to use the capability model (§3.2 from the Java paper), and writes a function safe open as\nfollows:\nint\nsafe_open(const char *pathname, int flags, mode_t mode)\n{\nchar buf[1024];\nsnprintf(buf, sizeof(buf), \"/safe-dir/%s\", pathname);\nreturn open(buf, flags, mode);\n}\nwhich is intended to mirror Figure 2 from the Java paper. To allow his plugin's XFI module to access to\nfiles in /safe-dir, Ben allows the XFI module to call the safe open function, as well as the standard\nread, write, and close functions (which directly invoke the corresponding system calls).\n14.\n[10 points]:\nCan a malicious XFI module access files (i.e., read or write) outside of\n/safe-dir? As in the Java paper, let's ignore symbolic links and \"..\" components in the path\nname. Explain how or argue why not.\nAnswer: No. There are two possible attacks. First, a malicious XFI module could guess legitimate\ninteger file descriptor numbers of other open files in the browser process (e.g., cookie files or cache\nfiles), and invoke read or write on them. Second, a malicious XFI module could write arbitrary\ndata D to address A by first writing D to a file in /safe-dir, and then invoking read on that file,\npassing A as the buffer argument to read. This will write D to memory location A (since read is\noutside of XFI), and allow the attacker to gain control of the entire process.\n\nV Browser security\n15. [6 points]:\nIn pages of a site which has enabled ForceHTTPS, <SCRIPT SRC=...> tags\nthat load code from an http://.../ URL are redirected to an https://.../ URL. Explain\nwhat could go wrong if this rewriting was not performed.\nAnswer: An active attacker could replace the Javascript code in the HTTP response with arbitrary\nmalicious code that could modify the containing HTTPS page or steal any of the data in that page, or\nthe cookie for the HTTPS page's origin.\n\nBen Bitdiddle runs a web site that frequently adds and removes files, which leads to customers complaining\nthat old links often return a 404 File not found error. Ben decides to fix this problem by adding a link to\nhis site's search page, and modifies how his web server responds to requests for missing files, as follows (in\nPython syntax):\ndef missing_file(reqpath):\nprint \"HTTP/1.0 200 OK\"\nprint \"Content-Type: text/html\"\nprint \"\"\nprint \"We are sorry, but the server could not locate file\", reqpath\nprint \"Try using the <A HREF=/search>search function</A>.\"\n16. [10 points]:\nExplain how an adversary may be able to exploit Ben's helpful error message to\ncompromise the security of Ben's web application.\nAnswer: An adversary could construct a link such as:\ncontaining arbitrary Javascript code, and trick legitimate users into visiting that link (e.g., by purchas-\ning ads on some popular site). Ben's server would echo the request path back verbatim, including the\nJavascript code, causing the victim's browser to execute the resulting Javascript as part of Ben's page,\ngiving the attacker's Javascript code access to the victim's cookies for Ben's site.\n\nVI 6.858\nWe'd like to hear your opinions about 6.858, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n17. [2 points]: How could we make the ideas in the course easier to understand?\n18. [2 points]: What is the best aspect of 6.858?\n19. [2 points]: What is the worst aspect of 6.858?\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/c2bd015c078fa66a93b98120ca6de70a_MIT6_858F14_q11-1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2011\nQuiz I: Solutions\nPlease do not write in the boxes below.\nI (xx/20)\nII (xx/10)\nIII (xx/16)\nIV (xx/22)\nV (xx/10)\nVI (xx/16)\nVII (xx/6)\nTotal (xx/100)\nGrade for quiz1\n\nI XFI\nConsider the following assembly code, which zeroes out 256 bytes of memory pointed to by the EAX register.\nThis code will execute under XFI. XFI's allocation stack is not used in this code.\nYou will need fill in the verification states for this code, which would be required for the verifier to check the\nsafety of this code, along the lines of the example shown in Figure 4 of the XFI paper. Following the example\nfrom the paper, possible verification state statements include:\nvalid[regname+const, regname+const)\norigSSP = regname+const\nretaddr = Mem[regname]\nwhere regname and const are any register names and constant expressions, respectively. Include all\nverification states necessary to ensure safety of the subsequent instruction, and to ensure that the next\nverification state is legal.\nx86 instructions\nVerification state\nmrguard(EAX, 0, 256)\n(1)\nECX := EAX\n# current pointer\nEDX := EAX+256\n# end of 256-byte array\n8 loop:\n(2)\nMem[ECX] := 0\nECX := ECX+4\n(3)\nif ECX+4 > EDX, jmp out\n(4)\njmp loop\n17 out:\n...\n\n1. [5 points]: What are the verification states needed at location marked (1)?\nAnswer:\n- valid[EAX-0, EAX+256) is the only verification state that can be inferred at this point.\n2. [5 points]: What are the verification states needed at location marked (2)?\nAnswer:\n- valid[EAX-0, EAX+256), from above.\n- valid[ECX-0, ECX+4), to satisfy the subsequent write to 4 bytes at ECX.\n- valid[ECX-0, EDX+0), to represent the loop condition.\n3. [5 points]: What are the verification states needed at location marked (3)?\nAnswer:\n- valid[EAX-0, EAX+256), from above.\n- valid[ECX-4, EDX+0), the loop condition updated with new value of ECX.\n4. [5 points]: What are the verification states needed at location marked (4)?\nAnswer:\n- valid[EAX-0, EAX+256), from above.\n- valid[ECX-4, EDX+0), from above.\n- valid[ECX-0, ECX+4), inferred from the check just before.\nNote that these verification states must imply (i.e., be at least as strong) as the verification states at (2).\n\nII ForceHTTPS\n5. [10 points]: Suppose bank.com uses and enables ForceHTTPS, and has a legitimate SSL\ncertificate signed by Verisign. Which of the following statements are true?\nA. True / False\nForceHTTPS prevents the user from entering their password on a phishing web site\nimpersonating bank.com.\nAnswer: False.\nB. True / False\nForceHTTPS ensures that the developer of the bank.com web site cannot acciden\ntally load Javascript code from another web server using <SCRIPT SRC=...>.\nAnswer: False.\nC. True / False\nForceHTTPS prevents a user from accidentally accepting an SSL certificate for\nbank.com that's not signed by any legitimate CA.\nAnswer: True.\nD. True / False\nForceHTTPS prevents a browser from accepting an SSL certificate for bank.com\nthat's signed by a CA other than Verisign.\nAnswer: False.\n\nIII Zoobar security\nBen Bitdiddle is working on lab 2. For his privilege separation, he decided to create a separate database\nto store each user's zoobar balance (instead of a single database called zoobars that stores everyone's\nbalance). He stores the zoobar balance for user x in the directory /jail/zoobar/db/zoobars.x, and\nensures that usernames cannot contain slashes or null characters. When a user first registers, the login service\nmust be able to create this database for the user, so Ben sets the permissions for /jail/zoobar/db to\n0777.\n6. [4 points]: Explain why this design may be a bad idea. Be specific about what an adversary would\nhave to do to take advantage of a weakness in this design.\nAnswer: Since the directory is world-writable, an adversary could replace the contents of an arbitrary\ndatabase, by first renaming the existing database's subdirectory to some unused name, and then creating\na fresh directory (database) with the desired name of the database. For example, the adversary could\nreplace all passwords with ones that the adversary chooses.\nAnswer: If an attacker can compromise any service, he can rename the zoobars.x file, since the\ndirectory is world-writable and not sticky, and replace it with a new one. (He can also replace the file\nwith a symbolic link to an interesting other file that the zoobar-handling user can write to, and mount\nsomething along the lines of a confused-deputy attack.)\nFull credit was also given for creating a directory before the user gets created; partial credit was given\nfor removing a directory (since you cannot remove a non-empty directory you don't have permissions\non).\n\nBen Bitdiddle is now working on lab 3. He has three user IDs for running server-side code, as suggested in\nlab 2 (ignoring transfer logging):\n- User ID 900 is used to run dynamic python code to handle HTTP requests (via zookfs). The database\ncontaining user profiles is writable only by uid 900.\n- User ID 901 is used to run the authentication service, which provides an interface to obtain a token\ngiven a username and password, and to check if some token for a username is valid. The database\ncontaining user passwords and tokens is stored in a DB that is readable and writable only by uid 901.\n- User ID 902 is used to run the transfer service, which provides an interface to transfer zoobar credits\nfrom user A to user B, as long as a token for user A is provided. The database storing zoobar balances\nis writable only by uid 902. The transfer service invokes the authentication service to check whether a\ntoken is valid.\nRecall that to run Python profile code for user A, Ben must give the profile code access to A's token (the\nprofile code may want to transfer credits to visitors, and will need this token to invoke the transfer service).\nTo support Python profiles, Ben adds a new operation to the authentication service's interface, where the\ncaller supplies an argument username, the authentication service looks up the profile for username, runs\nthe profile's code with a token for username, and returns the output of that code.\n7. [4 points]: Ben discovers that a bug in the HTTP handling code (running as uid 900) can allow an\nadversary to steal zoobars from any user. Explain how an adversary can do this in Ben's design.\nAnswer: An adversary can modify an arbitrary user's profile and inject Python code that will transfer\nall of the user's zoobars to the adversary's account.\n\n8. [8 points]: Propose a design change that prevents attackers from stealing zoobars even if they\ncompromise the HTTP handling code. Do not make any changes to the authentication or transfer\nservices (i.e., code running as uid 901 and 902).\nAnswer: Use a separate service, running as a separate uid, to edit profiles. Make sure the profile\ndatabase is writable only by this new service's uid. Require the user's token to be passed to this service\nwhen editing a user's profile. Have this profile-editing service check the token using the authentication\nservice.\nNote that this only prevents attacking users who never log in, as the HTTP service can get the token of\nany user who does log in. An argument that compromising the HTTP service gets you wide latitude in\ncompromising any user's activity would have been accepted for full credit.\n\nIV Baggy bounds checking\nConsider a system that runs the following code under the Baggy bounds checking system, as described in the\npaper by Akritidis et al, with slot size=16:\n1 struct sa {\nchar buf[32];\nvoid (*f) (void);\n4 };\n6 struct sb {\nvoid (*f) (void);\nchar buf[32];\n9 };\n11 void handle(void) {\nprintf(\"Hello.\\n\");\n13 }\n15 void buggy(char *buf, void (**f) (void)) {\n*f = handle;\ngets(buf);\n(*f) ();\n19 }\n21 void test1(void) {\nstruct sa x;\nbuggy(x.buf, &x.f);\n24 }\n26 void test2(void) {\nstruct sb x;\nbuggy(x.buf, &x.f);\n29 }\n31 void test3(void) {\nstruct sb y;\nstruct sa x;\nbuggy(x.buf, &y.f);\n35 }\n37 void test4(void) {\nstruct sb x[2];\nbuggy(x[0].buf, &x[1].f);\n40 }\nAssume the compiler performs no optimizations and places variables on the stack in the order declared, the\nstack grows down (from high address to low address), that this is a 32-bit system, and that the address of\nhandle contains no zero bytes.\n\n9. [6 points]:\nA. True / False\nIf function test1 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nAnswer: True. (If you overflow x.buf into x.f, you remain within the allocation bounds of x.)\nB. True / False\nIf function test2 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nAnswer: False. (If you overflow x.buf into any higher location, like the return pointer, you exceed the\nallocation bounds of x.)\nC. True / False If function test3 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nAnswer: False. (If you overflow x.buf into any higher location, like y, you exceed the allocation\nbounds of x.)\nD. True / False\nIf function test4 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nAnswer: True. (If you overflow x[0] into x[1], you remain within the allocation bounds of the array x.)\nFor the next four questions, determine what is the minimum number of bytes that an adversary has to provide\nas input to cause this program to likely crash, when running different test functions. Do not count the newline\ncharacter that the adversary has to type in to signal the end of the line to gets. Recall that gets terminates\nits string with a zero byte.\n10. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test1 to crash?\nAnswer: 32 (by overwriting x.f with a NUL byte, and jumping to it). Overwriting 64 bytes would\ncause a baggy bounds exception, but you can crash the program earlier.\n11. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test2 to crash?\nAnswer: 60 (via a baggy bounds exception).\n12. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test3 to crash?\nAnswer: 64 (via a baggy bounds exception).\n\n13. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test4 to crash?\nAnswer: 32 (by overwriting x[1].f with a NUL byte, and jumping to it). Overwriting 124 bytes would\ncause a baggy bounds exception, but you can crash the program earlier.\n\nV Browser security\nThe same origin policy generally does not apply to images or scripts. What this means is that a site may\ninclude images or scripts from any origin.\n14. [3 points]: Explain why including images from other origins may be a bad idea for user privacy.\nAnswer: The other origin's server can track visitors to the page embedding images from that server.\n15. [3 points]: Explain why including scripts from another origin can be a bad idea for security.\nAnswer: The other origin's server must be completely trusted, since the script runs with the privileges\nof the embedding page. For example, the script's code can access and manipulate the DOM of the\nembedding page, or access and send out the cookies from the embedding page.\n16. [4 points]: In general, access to the file system by JavaScript is disallowed as part of JavaScript\ncode sandboxing. Describe a situation where executing JavaScript code will lead to file writes.\nAnswer: Setting a cookie in Javascript typically leads to a file write, since the browser usually stores\ncookies persistently. Loading images can cause the image content to be saved in the cache (in some\nlocal file).\n\nVI Static analysis\nConsider the following snippet of JavaScript code:\n1 var P = false;\n3 function foo() {\nvar t1 = new Object();\nvar t2 = new Object();\nvar t = bar(t1, t2);\nP = true;\n8 }\n10 function bar(x, y) {\nvar r = new Object();\nif (P) {\nr = x;\n} else {\nr = y;\n}\nreturn r;\n19 }\nA flow sensitive pointer analysis means that the analysis takes into account the order of statements in the\nprogram. A flow insensitive pointer analysis does not consider the order of statements.\n17. [4 points]: Assuming no dead code elimination is done, a flow-insensitive pointer analysis (i.e.,\none which does not consider the control flow of a program) will conclude that variable t in function\nfoo may point to objects allocated at the following line numbers:\nA. True / False Line 1\nAnswer: False.\nB. True / False Line 4\nAnswer: True.\nC. True / False Line 5\nAnswer: True.\nD. True / False\nLine 11\nAnswer: True.\n\n18. [4 points]: Assuming no dead code elimination is done, a flow-sensitive pointer analysis (i.e.,\none which considers the control flow of a program) will conclude that variable t in function foo may\npoint to objects allocated at the following line numbers:\nA. True / False Line 1\nAnswer: False.\nB. True / False Line 4\nAnswer: True.\nC. True / False Line 5\nAnswer: True.\nD. True / False\nLine 11\nAnswer: False.\n19. [2 points]: At runtime, variable t in function foo may only be observed pointing to objects\nallocated at the following line numbers:\nA. True / False Line 1\nAnswer: False.\nB. True / False Line 4\nAnswer: True.\nC. True / False Line 5\nAnswer: True.\nD. True / False\nLine 11\nAnswer: False.\n\n20. [2 points]: Do you think a sound analysis that supports the eval construct is going to be precise?\nPlease explain.\nAnswer: No, because it is difficult to statically reason about the code that may be executed at runtime\nwhen eval is invoked, unless the analysis can prove that arbitrary code cannot be passed to eval at\nruntime, and can statically analyze all possible code strings that can be passed to eval.\n21. [4 points]: What is one practical advantage of the bottom-up analysis of the call graph described\nin the PHP paper by Xie and Aiken (discussed in class)?\nAnswer: Performance and scalability, by not analyzing functions that are not invoked by application\ncode, and by summarizing the effects of the function once and reusing that information for inter-\nprocedural analysis.\n\nVII 6.858\nWe'd like to hear your opinions about 6.858, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n22. [2 points]: How could we make the ideas in the course easier to understand?\nAnswer: Any answer received full credit.\n23. [2 points]: What is the best aspect of 6.858 so far?\nAnswer: Any answer received full credit.\n24. [2 points]: What is the worst aspect of 6.858 so far?\nAnswer: Any answer received full credit.\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2012",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/4e610bec14081ed932c7e8b7d180b93b_MIT6_858F14_q12-1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2012\nQuiz I Solutions\nGrade for q1\nHistogram of grade distribution\n\nI Buffer overflows\nConsider the following C program, where an adversary can supply arbitrary input on stdin. Assume no\ncompiler optimizations, and assume a 32-bit system. In this example, fgets() never writes past the end of\nthe 256-byte buf, and always makes it NULL-terminated.\nint main() {\nchar buf[256];\nfgets(buf, 256, stdin);\nfoo(buf);\nprintf(\"Hello world.\\n\");\n}\n1. [12 points]: Suppose the foo function is as follows:\nvoid foo(char *buf) {\nchar tmp[200]; // assume compiler places \"tmp\" on the stack\n// copy from buf to tmp\nint i = 0;\n// assume compiler places \"i\" in a register\nwhile (buf[i] != 0) {\ntmp[i] = buf[i];\ni++;\n}\n}\nWhich of the following are true? Assume there is an unmapped page both above and below the stack in\nthe process's virtual memory.\n(Circle True or False for each choice.)\nA. True / False\nAn adversary can trick the program to delete files on a system where the stack grows\ndown.\nAnswer: True. The adversary can overwrite foo's return address.\nB. True / False\nAn adversary can trick the program to delete files on a system where the stack grows\nup.\nAnswer: False. If the stack grows up, then no other state is placed above tmp on the stack, so even if\nthe adversary overflows tmp, it will not affect the program's execution.\nC. True / False\nAn adversary can trick the program to delete files on a system using Baggy bounds\nchecking with slot_size=16. (Stack grows down.)\nAnswer: False. Baggy will prevent memory writes to tmp from overflowing to the return address or\nany other stack variable.\n\nD. True / False\nAn adversary can prevent the program from printing \"Hello world\" on a system using\nBaggy bounds checking with slot_size=16. (Stack grows down.)\nAnswer: False. The argument buf points to a string that is at most 255 bytes long, since fgets\nNULL-terminates the buffer. Baggy enforces a power-of-2 allocation bound for tmp, which ends up\nbeing 256 bytes.\nE. True / False\nAn adversary can trick the program to delete files on a system using terminator stack\ncanaries for return addresses. (Stack grows down.)\nAnswer: False. A terminator stack canary includes a NULL byte, and if the adversary overwrites\nthe return address on the stack, the canary value will necessarily not contain any NULL bytes (since\notherwise the while loop would have exited).\nF. True / False\nAn adversary can prevent the program from printing \"Hello world\" on a system using\nterminator stack canaries for return addresses. (Stack grows down.)\nAnswer: True. The adversary could simply overwrite the canary value, which will terminate the\nprogram as foo returns.\n\n2. [8 points]: Suppose the foo function is as follows:\nstruct request {\nvoid (*f)(void); // function pointer\nchar path[240];\n};\nvoid foo(char *buf) {\nstruct request r;\nr.f = /* some legitimate function */;\nstrcpy(r.path, buf);\nr.f();\n}\nWhich of the following are true?\n(Circle True or False for each choice.)\nA. True / False\nAn adversary can trick the program to delete files on a system where the stack grows\ndown.\nAnswer: True. The adversary can overwrite foo's return address on the stack.\nB. True / False An adversary can trick the program to delete files on a system where the stack grows\nup.\nAnswer: True. The adversary can overwrite strcpy's return address on the stack.\nC. True / False An adversary can trick the program to delete files on a system using Baggy bounds\nchecking with slot_size=16. Assume strcpy is compiled with Baggy. (Stack grows down.)\nAnswer: False. Baggy bounds checking will prevent strcpy from going past r's allocation bounds,\nand r.f is before r.path in r's memory layout.\nD. True / False\nAn adversary can prevent the program from printing \"Hello world\" on a system using\nBaggy bounds checking with slot_size=16. Assume strcpy is compiled with Baggy. (Stack grows\ndown.)\nAnswer: True. If the adversary supplies 255 bytes of input, then strcpy will write past r's allocation\nbounds of 256 bytes, and Baggy will terminate the program.\n\nII OS sandboxing\nBen Bitdiddle is modifying OKWS to use Capsicum. To start each service, Ben's okld forks, opens the\nservice executable binary, then calls cap_enter() to enter capability mode in that process, and finally\nexecutes the service binary. Each service gets file descriptors only for sockets connected to okd, and for TCP\nconnections to the relevant database proxies.\n3. [6 points]: Which of the following changes are safe now that the services are running under\nCapsicum, assuming the kernel implements Capsicum perfectly and has no other bugs?\n(Circle True or False for each choice.)\nA. True / False It is safe to run all services with the same UID/GID.\nAnswer: True.\nB. True / False It is safe to run services without chroot.\nAnswer: True.\nC. True / False\nIt is safe to also give each service an open file descriptor for a per-service directory\n/cores/servicename.\nAnswer: True.\n\nBen also considers replacing the oklogd component with a single log file, and giving each service a file\ndescriptor to write to the log file.\n4. [5 points]: What should okld do to ensure one service cannot read or overwrite log entries from\nanother service? Be as specific as possible.\nAnswer: okld should call:\nlc_limitfd(logfd, CAP_WRITE);\nTo ensure that the service cannot seek, truncate, or read the log file.\n5. [5 points]: What advantages could an oklogd-based design have over giving each service a file\ndescriptor to the log file?\nAnswer: oklogd can enforce structure on the log file, such as adding a timestamp to each record,\nensuring each record is separated from other records by a newline, ensuring multiple records are not\ninterleaved, etc.\n\nIII Network protocols\nBen Bitdiddle is designing a file server that clients connect to over the network, and is considering using\neither Kerberos (as described in the paper) or SSL/TLS (without client certificates, where users authenticate\nusing passwords) for protecting a client's network connection to the file server. For this question, assume\nusers choose hard-to-guess passwords.\n6. [6 points]: Would Ben's system remain secure if an adversary learns the server's private key, but\nthat adversary controls only a single machine (on the adversary's own home network), and does not\ncollude with anyone else? Discuss both for Kerberos and for SSL/TLS.\nAnswer: With Kerberos, no: the adversary can impersonate any user to this server, by constructing\nany ticket using the server's private key.\nWith SSL, yes: the server can impersonate the server to another client, but no clients will connect to\nthe adversary's fake server.\n7. [6 points]: Suppose an adversary learns the server's private key as above, and the adversary\nalso controls some network routers. Ben learns of this before the adversary has a chance to take any\naction. How can Ben prevent the adversary from mounting attacks that take advantage of the server's\nprivate key (e.g., not a denial-of-service attack), and when will the system be secure? Discuss both for\nKerberos and for SSL/TLS.\nAnswer: With Kerberos, Ben should change the server's private key. The system will be secure from\nthat point forward. If the adversary was recording network traffic from before the attack, Ben should\nalso make sure he changes the server's private key over a secure network link, because kpasswd does\nnot provide forward secrecy. The adversary may be able to decrypt network traffic to the file server\nbefore the key is changed.\nWith SSL, Ben should obtain a new SSL certificate for the server, with a new secret key, but the\nadversary can continue to impersonate Ben's file server until the compromised certificate expires. The\nsystem will only be secure once the certificate expires, or once all clients learn of the certificate being\nrevoked.\n\nIV Static analysis\nWould Yichen Xie's PHP static analysis tool for SQL injection bugs, as described in the paper, flag a potential\nerror/warning in the following short but complete PHP applications?\n8. [10 points]:\nA. True / False\nThe tool would report a potential error/warning in the following code:\nfunction q($s) {\nreturn mysql_query($s);\n}\n$x = $_GET['id'];\nq(\"SELECT .. $x\");\nAnswer: True. The summary for q() indicates that the argument must be sanitized on entry, and the\nmain function does not sanitize the argument.\nB. True / False\nThe tool would report a potential error/warning in the following code:\nfunction my_validate() {\nreturn isnumeric($_GET['id']);\n}\n$x = $_GET['id'];\nif (my_validate()) {\nmysql_query(\"SELECT .. $x\");\n}\nAnswer: False. The summary for my_validate() indicates that $_GET[id] is sanitized if the return\nvalue is true.\n\n9. [10 points]:\nA. True / False\nThe tool would report a potential error/warning in the following code:\nmysql_query(\"SELECT .. $n\");\nAnswer: True. The tool reports warnings when any variable must be sanitized on entry into the main\nfunction, and the variable is not known to be easily controlled by the user, such as $_GET and $_POST.\nB. True / False\nThe tool would report a potential error/warning in the following code:\nfunction check_arg($n) {\n$v = $_GET[$n];\nreturn isnumeric($v);\n}\n$x = $_GET['id'];\nif (check_arg('id')) {\nmysql_query(\"SELECT .. $x\");\n}\nAnswer: True. The summary for check_arg() indicates that $_GET[⊥] is sanitized if the return\nvalue is true, but the call to mysql_query() requires $_GET[id] to be sanitized.\n\nV Runtime instrumentation\n10. [10 points]:\nConsider the following Javascript code:\nfunction foo(x, y) {\nreturn x + y;\n}\nvar a = 2;\neval(\"foo(a, a)\");\nvar p_c = {\nk: 5,\nf: function() { return a + this.k; }\n};\nvar kk = 'k';\np_c[kk] = 6;\np_c.f();\nBased on the description in the paper by Sergio Maffeis et al, and based on lecture 9, what will be the\nFBJS rewritten version of this code, assuming the application-specific prefix is p_?\nAnswer: FBJS adds a p_ prefix to every variable name, and wraps this and variable array indexes in\n$FBJS.ref() and $FBJS.idx() respectively.\nfunction p_foo(p_x, p_y) {\nreturn p_x + p_y;\n}\nvar p_a = 2;\np_eval(\"foo(a, a)\");\nvar p_p_c = {\nk: 5,\nf: function() { return p_a + $FBJS.ref(this).k; }\n};\nvar p_kk = 'k';\np_p_c[$FBJS.idx(p_kk)] = 6;\np_p_c.f();\n\nVI\nBrowser security\nBen Bitdiddle is taking 6.858. Once he's done with his lab at 4:55pm, he rushes to submit it by going to\nhttps://taesoo.scripts.mit.edu/submit/handin.py/student, selecting his labN-handin.tar.gz\nfile, and clicking \"Submit\". The 6.858 submission web site also allows a student to download a copy of their\npast submission.\nFor your reference, when the user logs in, the submission web site stores a cookie in the user's browser to\nkeep track of their user name. To prevent a user from constructing their own cookie, or arbitrarily changing\nthe value of an existing cookie, the server includes a signature/MAC of the cookie's value in the cookie, and\nchecks the signature when a new request comes in. Finally, users can log out by clicking on the \"Logout\"\nlink, https://taesoo.scripts.mit.edu/submit/handin.py/logout, which clears the cookie.\nAlyssa P. Hacker, an enterprising 6.858 student, doesn't want to do lab 5, and wants to get a copy of Ben's\nupcoming lab 5 submission instead. Alyssa has her own web site at https://alyssa.scripts.mit.edu/,\nand can convince Ben to visit that site at any point.\n11. [16 points]: How can Alyssa get a copy of Ben's lab 5 submission?\nAlyssa's attack should rely only on the Same-Origin Policy. Assume there are no bugs in any software,\nBen's (and Taesoo's) password is unguessable, the cookie signature scheme is secure, etc.\nAnswer: Alyssa's web site should force Ben's browser to log out from the 6.858 submission web site,\nby inserting the following tag:\n<IMG SRC=\"https://taesoo.scripts.mit.edu/submit/handin.py/logout\">\nand then set a cookie for domain=scripts.mit.edu containing Alyssa's own cookie. When Ben\nvisits the submission web site to upload his lab 5, he will actually end up uploading it under Alyssa's\nusername, allowing Alyssa to then download it at 4:56pm.\n\nVII 6.858\nWe'd like to hear your opinions about 6.858. Any answer, except no answer, will receive full credit.\n12. [2 points]: This year we started using Piazza for questions and feedback. Did you find it useful,\nand how could it be improved?\nAnswer: Generally good; UI not so great; appreciate anonymity. Would be good if answers show\nup quicker. Bypass email preferences for important announcements. In-person office hours are also\nimportant. Submit paper questions via Piazza. Signal-to-noise ratio too low. More TA/professor\nparticipation other than David. Ask people not to be anonymous. Separate login is annoying. RSS\nfeed.\n13. [2 points]: What aspects of the labs were most time-consuming? How can we make them less\ntedious?\nAnswer: Include more debugging tools, especially for the PyPy sandbox. Explain where errors /\ndebug output goes. Explanation of provided lab code; explain what parts to focus on. Start discussions\nof papers. Avoid asking for the same thing multiple times in the lab; 2nd part of lab 2 was repetitive.\nSpeed up the VM / run Python fewer times. More office hours. Better / more fine-grained / faster make\ncheck. Review/recitation session to provide background knowledge for a lab.\n14. [2 points]: Are there other things you'd like to see improved in the second half of the semester?\nAnswer: Past exploits. More time on labs. More late days. More summaries of papers / what to focus\non / background info. More attacks. More recent papers. Explicit lectures on lab mechanics. More\ndesign freedom in labs / more challenging design choices; less fill-in-the-blank style. Some papers\nare too technical. Fewer ways to turn things in (submit via make; submit via text file; email question;\nPiazza). Balance first and second parts of labs. Novelty lecture on lockpicking. Shorter quiz. Weekly\nreview of lecture (not labs) material - recitation? Relate labs to lectures, teach more hands-on stuff.\nLabs where you solve some problem rather than get the details right? Explain what corner cases matter\nfor labs. Lecture should focus on application of paper's ideas and short review of paper content. More\ninfo on final projects. Scrap the answers.txt stuff, just code.\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/046db345c9137f61843f733a9e7e94aa_MIT6_858F14_q13_1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2013\nQuiz I Solutions\nGrade Distribution\nMean: 59.65. Std. Dev: 15.01\nHistogram of grade distribution\n\nI Lab 1\nThe following is a working exploit for exercise 2 in lab 1:\nreqpath = 0xbfffedf8\nebp\n= 0xbffff608\nretaddr = ebp + 4\ndef build_exploit(shellcode):\nreq = (\"GET ////\" +\nurllib.quote(shellcode) +\n\"x\" * (retaddr - reqpath - (len(shellcode)+8)) +\n\"yyyy\" +\nurllib.quote(struct.pack(\"I\", reqpath+4)) +\n\" HTTP/1.0\\r\\n\\r\\n\")\nreturn req\nThe stack frame that is being attacked is the following:\nstatic void process_client(int fd)\n{\nstatic char env[8192]; /* static variables are not on the stack */\nstatic size_t env_len;\nchar reqpath[2048];\nconst char *errmsg;\nint i;\n/* get the request line */\nif ((errmsg = http_request_line(fd, reqpath, env, &env_len)))\nreturn http_err(fd, 500, \"http_request_line: %s\", errmsg);\n...\nThe function http_request_line overruns reqpath.\n\n1. [11 points]:\nThe following stack diagram corresponds to the state of the vulnerable web server right after http_request_line\nreturns but before process_client returns. Fill in this diagram as follows:\n- Fill in all stack memory contents that you can determine based on the exploit shown. You must fill\nin the return address, saved %ebp, contents of the entire reqpath buffer, and anything in between\nthem. You don't need to write down the exact number of \"x\" bytes.\n- Write down the memory addresses (on the left of the stack diagram) for the reqpath buffer, the\n%ebp register saved by process_client, and the return address that process_client will\nuse.\n- Label the location of the saved %ebp and the return address on the right of the stack diagram, in\nthe way that the reqpath buffer is already labeled.\nreqpath\nVirtual memory address\n0xffffffff\n0x00000000\n\nAnswer:\nreqpath\nVirtual memory address\n0xffffffff\n0x00000000\n0xbffff60c\n0xbffff608\n0xbfffedf8\n/ / / /\nShell code\nx\nx\nx\nx\nx (16 times)\nsaved %ebp\nreturn address\n0xbfffedfc\ny y y y\n\nII Baggy Bounds Checking\nConsider the implementation of Baggy Bounds Checking described in the paper (i.e., the 32-bit version of\nBaggy with slot_size=16) and the following code fragment:\n1. char *p, *q;\n2. char *a, *b, *c, *d, *e, *f;\n3.\n4. p = malloc(48);\n5.\nq = malloc(16);\n6.\n7.\na = p + 46;\n8. b = a + 10;\n9. *b = '\\0';\n10. c = b + 10;\n11. d = c + 10;\n12. e = d - 32;\n13. *e = '\\0';\n14.\n15.\np = q;\n16. f = p + 8;\nAssume that p and q are allocated right after each other, but with the alignment rules that Baggy Bounds\nChecking uses.\n2. [7 points]: Will Baggy Bounds Checking cause an exception at any of the above lines, or will the\nprogram terminate without an error? Explain your answer briefly.\nA. Program terminates without an error.\nB. Program raises an error on line number: ______________\nExplanation:\nAnswer: Error on line 11, because the value of d is 76 bytes beyond p, which is more than half a slot\nsize (8 bytes) over the power-of-2 allocation size for p (64 bytes).\n\nIII Lab 2\n3. [5 points]: The following fragment shows a few lines from chroot-setup.sh to setup the\ntransfer database after implementing privilege separation:\npython /jail/zoobar/zoodb.py init-transfer\nchown -R 61013:61007 /jail/zoobar/db/transfer\nchmod -R g-w /jail/zoobar/db/transfer\n## g stands for group; this maps to clearing 020 in octal\nchmod -R o+rw /jail/zoobar/db/transfer\n## o stands for other; this maps to adding 006 in octal\nUID 61013 corresponds to the bank service, and GID 61007 corresponds to the dynamic zoobar service.\nCan the permissions on the transfer database be set tighter without breaking the functionality of the\nsystem? If so, explain how, and explain the attack that can take place if you don't. If not, explain what\nwould break if it were any tighter.\nAnswer: It should be chmod o-rw /jail/zoobar/db/transfer; the bank is the only service that needs to\nread and write the transfer DB. (Some students also pointed out that if the dynamic service reads the\ntransfer DB via RPC, it need not have read access on the DB file.) Otherwise any program on that\nsystem can modify the transfer DB.\n4. [5 points]: Suppose Alyssa has completed lab 2 and her solution passes all the lab tests. Now\nsuppose an adversary can compromise zookld after the zoobar web site has been running for a while.\nWhat attack can the adversary launch? For example, can the adversary steal zoobars?\nAnswer: Yes, zookld runs as root, so it has full privileges, and can arbitrarily modify all files on the\nsystem, including the zoobars DB.\n\nIV Native Client\nBen Bitdiddle is designing Native Client for a 32-bit ARM processor instead of x86 (the paper in class was\nabout the x86). For the purposes of this question, let us assume that ARM has fixed-sized instructions (4\nbytes long), but does not have the segmentation support (%cs, %ds, etc) that the Native Client on x86 used to\nconstrain loads and stores.\nBen's plan is to insert extra instructions before every computed jump and every computed memory load\nand store. These extra instructions would AND the computed jump, load, or store address with 0x0ffffffc,\nmeaning clearing out the top 4 bits of the address (and also clear the low two bits, to ensure the address is\n4-byte-aligned), and thus constraining the jumps, loads, and stores to the bottom 256 MBytes of the address\nspace. For example, suppose register %r1 contains a memory address. Loading the value stored at that address\ninto register %r2 would result in the following instructions (in a pseudo-x86-like instruction set notation):\nAND %r1, 0x0ffffffc\nMOV (%r1), %r2\nMuch as in the Native Client paper, the attack scenario is that Ben's Native Client system will be used to\nexecute arbitrary code that is received from an unknown source over the network, after it passes Ben's verifier.\n5. [10 points]: Ben is trying to decide which of Native Client's original constraints are still necessary\nin his ARM version (see Table 1 in the Native Client paper). In particular, the x86 version of Native\nClient required all code to be aligned to 32-byte boundaries (see constraint C5 in Table 1 of the Native\nClient paper). Is it necessary for Ben's verifier check this constraint? Explain why or why not.\nAnswer: Ben's verifier does require 32-byte alignment (or something greater than the 4-byte alignment\nprovided by the underlying hardware), in order to ensure that computed jumps do not go to the middle\nof a pseudo-instruction, thereby bypassing the extra AND instructions. In the example code sequence\nshown above, jumping to the second instruction with an arbitrary value in %r1 will result in a memory\nload from an unconstrained address. 32-byte alignment is also required to protect springboard and\ntrampoline code, so that untrusted code cannot jump into the middle of the springboard or trampoline.\n\nV TCP/IP\n6. [7 points]: Ben Bitdiddle tries to fix the Berkeley TCP/IP implementation, described in Steve\nBellovin's paper, by generating initial sequence numbers using this random number generator:\nclass RandomGenerator(object):\ndef __init__(self, seed):\nself.rnd = seed\ndef choose_ISN_s(self):\nisn = self.rnd\nself.rnd = hash(self.rnd)\nreturn isn\nAssume that Ben's server creates a RandomGenerator by passing it a random seed value not known\nto the adversary, that hash() is a well-known hash function that is difficult to invert, and that the server\ncalls choose_ISN_s to determine the ISNs value for a newly established connection.\nHow can an adversary establish a connection to Ben's server from an arbitrary source IP address,\nwithout being able to snoop on all packets being sent to/from the server?\nAnswer: Open a connection to the server, record the received ISNs value as s, and when attempting to\nestablish a spoofed connection from another IP address, guess that ISNs will be hash(s).\n\nVI Kerberos\nIn a Unix Kerberos implementation, each user's tickets (including the TGT ticket for the TGS service) are\nstored in a per-user file in /tmp. The Unix permissions on this file are such that the user's UID has access to\nthat file, but the group and others do not.\n7. [7 points]: Ben Bitdiddle wants to send an email to Alyssa, and to include a copy of the Kerberos\npaper as an attachment, but because he stayed up late studying for this quiz, he accidentally sends his\nKerberos ticket file as an attachment instead. What can Alyssa do given Ben's ticket file? Be precise.\nAnswer: Access all services as Ben, until Ben's ticket expires.\n8. [7 points]: Ben Bitdiddle stores his secret files in his Athena AFS home directory. Someone hands\nAlyssa P. Hacker a piece of paper with the key of the Kerberos principal of all-night-tool.mit.edu,\nwhich is one of the athena.dialup.mit.edu machines. Could Alyssa leverage her knowledge of\nthis key to get access to Ben's secret files? Assume Alyssa cannot intercept network traffic. Explain\neither how she could do so (and in what situations this might be possible), or why it is not possible.\nAnswer: Using the key of all-night-tool.mit.edu, Alyssa should construct a ticket impersonating\nBen to athena.dialup.mit.edu, and use it to log into the dialup as Ben. She should then wait for\nthe real Ben to also log in, at which point Ben's login process will store his tickets into /tmp. Alyssa\ncan then steal his tickets on the dialup and use them to impersonate Ben to any server (including AFS).\nNote that Ben's files are not stored on the dialup server itself, so if Alyssa simply breaks into the dialup\nserver, she cannot get access to Ben's files.\nThis course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\nVII\nWeb security\n9. [7 points]: Ben Bitdiddle sets up a private wiki for his friends, running on scripts.mit.edu, at\nhttp://scripts.mit.edu/~bitdiddl/wiki. Alyssa doesn't have an account on Ben's wiki, but\nwants to know what Ben and his friends are doing on that wiki. She has her own web site running on\nscripts.mit.edu, at http://scripts.mit.edu/~alyssa/.\nHow can Alyssa get a copy of a given page from Ben's wiki (say, http://scripts.mit.edu/\n~bitdiddl/wiki/Secret)?\nAnswer: Alyssa should ask Ben or one of his friends to visit her page. On her page, she should create\nan iframe pointing to the secret page on Ben's wiki, and read the contents of that frame using Javascript\ncode in her own page. The same-origin policy allows this because both Ben's and Alyssa's pages have\nthe same origin (i.e., http://scripts.mit.edu/).\n\nBen Bitdiddle gives up on the wiki, and decides to build a system for buying used books, hosted at http:\n//benbooks.mit.edu/. His code for handling requests to http://benbooks.mit.edu/buy is as follows:\n1.\ndef buy_handler(cookie, param):\n2.\nprint \"Content-type: text/html\\r\\n\\r\\n\",\n3.\n4.\nuser = check_cookie(cookie)\n5.\nif user is None:\n6.\nprint \"Please log in first\"\n7.\nreturn\n8.\n9.\nbook = param['book']\n10.\nif in_stock(book):\n11.\nship_book(book, user)\n12.\nprint \"Order succeeded\"\n13.\nelse:\n14.\nprint \"Book\", book, \"is out of stock\"\nwhere the param argument is a dictionary of the query parameters in the HTTP request (i.e., the part of the\nURL after the question mark). Assume Ben's cookie handling function check_cookie correctly checks the\ncookie and returns the username of the authenticated user.\n\n10.\n[7 points]: Is there a cross-site scripting vulnerability in Ben's code? If so, specify the line\nnumber that is vulnerable, and explain how Ben should fix it.\nAnswer: Yes, line 14 is vulnerable to cross-site scripting. An attacker can supply a value of book\nthat contained something like <script>alert(document.cookie)</script>, and assuming the\nin_stock function returned false for that book ID, the web server would print that script tag to the\nbrowser, and the browser will run the code from the URL.\nTo prevent this vulnerability, wrap book in that line in cgi.escape(book).\n11. [7 points]: Is there a cross-site request forgery vulnerability in Ben's code? If so, specify how an\nadversary could exploit it.\nAnswer:\nYes, an adversary can set up a form that submits a request to buy a book to http://\nbenbooks.mit.edu/buy?book=anyid, and this request will be honored by the server.\nTo solve this problem, include a token with every legitimate request, in the way that Django CSRF\nworks, and check that cookie['csrftoken']==param['csrftoken'].\n12. [7 points]: Ben decides to port his web application to Django, and use Django's stateless CSRF\nprotection. Explain why he should migrate his web application to a separate domain that's not under\nmit.edu.\nAnswer: Django's CSRF protection relies on storing the csrftoken in a cookie. For a site hosted\nunder mit.edu, any other web application under mit.edu can set the csrftoken cookie and break\nDjango's CSRF protection.\n\n13. [7 points]: Ben Bitdiddle moved his book store to https://www.bitdiddlebooks.com/, but\nhe needs to use the popular jQuery Javascript library to make his web page interactive. He adds the\nfollowing line to his web page:\n<SCRIPT SRC=\"http://code.jquery.com/jquery-1.9.1.js\">\nProvide at least two reasons for why this is a bad idea from a security perspective.\nAnswer: First, it allows an adversary with access to the network of a visitor to Ben's web site to inject\narbitrary Javascript code into Ben's HTTPS page, bypassing any cryptographic protection Ben might\nhave wanted. Second, it allows an adversary that compromises code.jquery.com to serve arbitrary\nJavascript code to run in browsers that visit Ben's page, even if that adversary doesn't control the\nvisitor's network.\n\nVIII 6.858\nWe'd like to hear your opinions about 6.858. Any answer, except no answer, will receive full credit.\n14. [2 points]: What aspects of the labs were most time-consuming? How can we make them less\ntedious?\nAnswer: Debugging. Too much existing code to read for each lab. Repetitive exercises.\n15. [2 points]: Are there other things you'd like to see improved in the second half of the semester?\nAnswer: More attack labs. More explanation of background material for papers.\n16. [2 points]: Is there one paper out of the ones we have covered so far in 6.858 that you think we\nshould definitely remove next year? If not, feel free to say that.\nAnswer: The popular answers were Capsicum, KINT, and The Tangled Web (too much reading in\none assignment).\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1 Solution, Fall 2014",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/027a5ae96638b30c82002ee1cfc68554_MIT6_858F14_q14_1_sol.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2014\nQuiz I Solutions\nGrade distribution\nHistogram of grade distribution\nMean 64.4, Stddev 15.5\n\nI Baggy Bounds and Buffer Overflows\n1. [6 points]: At initialization time, a baggy bounds system on a 32-bit machine is supposed to set\nall of the bounds table entries to 31. Suppose that, in a buggy implementation with a slot_size of 32\nbytes, bounds table initialization is improperly performed, such that random entries are incorrectly set\nto 1.\nSuppose that a networked server uses an uninstrumented library to process network messages. Assume\nthat this library has no buffer overflow vulnerabilities (e.g., it never uses unsafe functions like gets()).\nHowever, the server does suffer from the bounds table initialization problem described above, and the\nattacker can send messages to the server which cause the library to dynamically allocate and write an\nattacker-controlled amount of memory using uninstrumented code that looks like this:\n// N is the buffer size that the\n// attacker gets to pick.\nchar *p = malloc(N);\nfor (int i = 0; i < N/4; i++, p += 4) {\n*p = '\\a';\n*(p+1) = '\\b';\n*(p+2) = '\\c';\n*(p+3) = '\\d';\n}\nAssume that the server uses a buddy memory allocator with a maximum allocation size of 216 (i.e.,\nlarger allocations fail). What is the smallest N that the attacker can pick that will definitely crash the\nserver? Why will that N cause a crash?\nAnswer: The uninstrumented library code does not check bounds table entries when it does pointer\narithmetic. Thus, the code snippet above is unaffected by the incorrect initialization of the bounds table.\nHowever, the code snippet does not check the return value of malloc() for NULL; so, the attacker\ncan select an N of 216 + 1, cause malloc() to return NULL, and get the server to crash.\n\n2. [6 points]: Modern CPUs often support NX (\"no execute\") bits for memory pages. If a page has\nits NX bit set to 1, then the CPU will not run code that resides in that page.\nNX bits are currently enforced by the OS and the paging hardware. However, imagine that programs\nexecute on a machine whose OS and paging hardware do not natively support NX. Further imagine\nthat a compiler wishes to implement NX at the software level. The compiler associates a software-\nmanipulated NX bit with each memory page, placing it at the bottom (i.e., the lowest address) of each\n4KB page.\nThe compiler requires that all application-level data structures be at most 4095 bytes large. The\ncompiler allocates each stack frame in a separate page, and requires that a stack frame is never bigger\nthan a page. A stack frame might look like the following:\n...\n|-----------------------|\n|\n|\n+-----------------------+\nentry %esp-->|\nreturn address\n|\n+-----------------------+\nnew %ebp---->|\nsaved %ebp\n|\n+-----------------------+\n|\nbuf[4]\n|\n|\nbuf[3]\n|\n|\nbuf[1]\n|\n|\nbuf[0]\n|\n+-----------------------+\n| ...other stack vars...|\n+-----------------------+\nnew %esp---->|\nNX bit\n|\n+-----------------------+\nsuch that, as shown in the sample code above, an overflow attack in the frame will not overwrite the\nframe's NX bit.\nThe compiler also associates NX bits with each normal code page. The NX bit for a stack frame is set\nto \"non-executable\", and the NX bit for a normal code page is set to \"executable\".\nThe compiler instruments updates to the program counter such that, whenever the PC migrates to a new\npage, the program checks the NX bit for the page. If the bit indicates that the page is non-executable,\nthe program throws an exception.\nDescribe how a buffer overflow attack can still overwrite NX bits.\nAnswer:\n- A buffer overflow in the currently active frame can spill into the frame that is above it in RAM.\nThus, a callee can overwrite its caller's NX bit.\n- If a buffer overflow can corrupt a pointer value, the attacker can make the pointer point to\nthe address of an NX bit. If that pointer is dereferenced and assigned to, the NX bit will be\noverwritten.\n- The attacker could mount a return-to-libc attack to use preexisting code to reset an NX bit.\n\nII Stack Canaries and Return-Oriented Programming\n3. [4 points]: Stack canaries live in an area of memory that programs can read as well as write.\nIn the typical buffer overflow attack (e.g., via the gets() function), what prevents an attacker from\nsimply reading the canary value and then placing that canary value in the overflow payload?\nAnswer: In the typical buffer overflow attack, the attacker cannot execute arbitrary code; instead, the\nattacker can only supply inputs that the program will not bounds-check during a copy operation. Thus,\nthe attacker can only *write* the stack. In other words, vulnerable functions like gets() do not allow\nthe attacker to directly read a value and then insert that value into the attack payload.\nYou get partial credit if you say that the canary might contain terminating characters, such as '\\0', that\nstop gets() from reading beyond the point.\n\n4. [10 points]: In the first part of a BROP attack, the attacker must find gadgets that pop entries from\nthe stack and store them into attacker-selected registers. Suppose that the attacker has already found\nthe address of a stop gadget and a trap value (i.e., a memory value which, if accessed, causes a fault).\nIn the stack diagram below, depict what a buffer overflow should write on the stack to identify pop\ngadgets which pop exactly two things from the stack e.g., pop rdi; pop rsi; ret;. If it doesn't\nmatter what goes in a particular memory location, put \"Doesn't mattter\". To represent the values for\nthe stop gadget and the trap, simply write \"stop\" or \"trap\". To represent the address of a candidate pop\ngadget, write \"probe\".\n...\n|-----------------------|\n|\n|\nValue: ________\n|-----------------------|\n|\n|\nValue: ________\n|-----------------------|\n|\n|\nValue: ________\n|-----------------------|\n|\n|\nValue: ________\n|-----------------------|\n|\n|\nValue: ________\n|-----------------------|\n|\n|\nValue: ________\n+-----------------------+\nentry %esp-->|\nreturn address\n|\nValue: ________\n+-----------------------+\nnew %ebp---->|\nsaved %ebp\n|\nValue: ________\n+-----------------------+\n|\nbuf[3]\n|\nValue: ________\n|\nbuf[2]\n|\nValue: ________\n|\nbuf[1]\n|\nValue: ________\n|\nbuf[0]\n|\nValue: ________\n+-----------------------+\nnew %esp---->| ...other stack vars...|\n+-----------------------+\n\n...\n|-----------------------|\n|\n|\n|-----------------------|\n|\n|\n|-----------------------|\n|\n|\n|-----------------------|\n|\n|\n|-----------------------|\n|\n|\n|-----------------------|\n|\n|\n+-----------------------+\nentry %esp-->|\nreturn address\n|\n+-----------------------+\nnew %ebp---->|\nsaved %ebp\n|\n+-----------------------+\n|\nbuf[3]\n|\n|\nbuf[2]\n|\n|\nbuf[1]\n|\n|\nbuf[0]\n|\n+-----------------------+\nnew %esp---->| ...other stack vars...|\nAnswer:\n+-----------------------+\nValue: trap\nValue: trap\nValue: trap\nValue: stop\nValue: trap\nValue: trap\nValue: probe\nValue: Doesn't matter\nValue: Doesn't matter\nValue: Doesn't matter\nValue: Doesn't matter\nValue: Doesn't matter\n\nIII OKWS and OS Security\nSuppose Unix did not provide a way of passing file descriptors between processes, but still allowed inheriting\nfile descriptors from a parent on fork and exec.\n5. [4 points]:\nWhat aspects of the OKWS design would break without file descriptor passing?\n(Circle True or False for each choice.)\nA. True / False It would be impossible for services to send messages to oklogd.\nAnswer: False.\nB. True / False It would be impossible for services to get a TCP connection to a database proxy.\nAnswer: False.\nC. True / False It would be impossible for services to get a TCP connection to the client web browser.\nAnswer: True.\nD. True / False\nIt would be impossible for okd to run as a non-root user.\nAnswer: False.\n\nConsider the following Python code for a program that might run every night as root on a Unix machine to\nclean up old files in /tmp. The Python function os.walk returns a list of subdirectories and filenames in\nthose subdirectories. It ignores \".\" and \"..\" names. As a reminder, a Unix filename cannot contain / or\nNULL bytes, and os.unlink on a symbolic link removes the symbolic link, not the target of the symbolic\nlink.\ndef cleanup():\n## Construct a list of files under /tmp that are over 2 days old.\nfiles = []\nfor (dirname, _, filenames) in os.walk('/tmp'):\nfor filename in filenames:\nfn = dirname + '/' + filename\nif os.path.getmtime(fn) < time.time() - 2 * 86400:\nfiles.append(fn)\nfor fn in files:\nos.unlink(fn)\n6. [10 points]:\nExplain how an adversary could take advantage of this program to delete /etc/passwd.\nAnswer: The adversary can exploit a race condition. First, create a directory /tmp/foo and a file\n/tmp/foo/passwd in there, and set the modification time of /tmp/foo/passwd to be over 2 days old.\nThen wait for the script to run os.walk and now delete /tmp/foo/passwd and /tmp/foo, and create\na symlink /tmp/foo to /etc. Now the cleanup code will run os.unlink(\"/tmp/foo/passwd\"),\nwhich will remove /etc/passwd.\nWe also gave partial credit to the answer of creating a symlink /tmp/foo pointing at /etc, under the\nassumption that os.walk follows symlinks, even though in reality it does not.\n\nIV Native Client\nAnswer the following questions about how Native Client works on 32-bit x86 systems, according to the paper\n\"Native Client: A Sandbox for Portable, Untrusted x86 Native Code.\"\n7. [6 points]: Which of the following statements are true?\n(Circle True or False for each choice.)\nA. True / False\nThe Native Client compiler is trusted to generate code that follows Native Client's\nconstraints.\nAnswer: False.\nB. True / False The Native Client validator ensures that no instruction spans across a 32-byte boundary.\nAnswer: True.\nC. True / False\nThe Native Client service runtime is checked using the validator to ensure its code\nfollows the constraints.\nAnswer: False.\nD. True / False\nNative Client requires additional instructions before every direct jump.\nAnswer: False.\nE. True / False Native Client requires additional instructions before every indirect jump.\nAnswer: True.\nF. True / False\nNative Client requires additional instructions before every memory access.\nAnswer: False.\n8. [6 points]:\nFor the following x86 code, indicate whether Native Client's validator would allow it (by writing\nALLOW), assuming the parts after ... are valid, or circle the first offending instruction that causes\nthe validator to reject the code.\n10000:\n83 e0 2e\nand\n$0x2e,%eax\n10003:\ninc\n%eax\n10004:\n01 ca\nadd\n%ecx,%edx\n10006:\n4a\ndec\n%edx\n10007:\neb fa\njmp\n0x10003\n10009:\nb9 ef be ad de\nmov\n$0xdeadbeef,%ecx\n1000e:\n8b 39\nmov\n(%ecx),%edi\n10010:\n8b 35 ef be ad de\nmov\n0xdeadbeef,%esi\n\n10016:\n8b 66 64\nmov\n0x64(%esi),%esp\n10019:\n5b\npop\n%ebx\n1001a:\n8b 58 05\nmov\n0x5(%eax),%ebx\n1001d:\n83 e0 e0\nand\n$0xffffffe0,%eax\n10020:\nff e0\njmp\n*%eax\n10022:\nf4\nhlt\n...\nAnswer: The validator will complain about the jmp at 0x10020 with error \"Bad indirect control\ntransfer\"; see Figure 3 in the NaCl paper.\n\nV Symbolic execution\nConsider the following Python program running under the concolic execution system from lab 3, where x is a\nconcolic integer that gets the value 0 on the first iteration through the loop:\ndef foo(x):\ny = x + 7\nif y > 10:\nreturn 0\nif y * y == 256:\nreturn 1\nif y == 7:\nreturn 2\nreturn 3\n9. [6 points]:\nAfter running foo with an initial value of x=0, what constraint would the concolic execution system\nsend to Z3 for the second if statement?\nAnswer: (x+7)*(x+7)=256 AND NOT (x+7)>10\nMore precisely, in Z3's s-expression:\n(and (= (> (+ x 7) 10) false) (not (= (= (* (+ x 7) (+ x 7)) 256) false))).\n\nVI Web security\n10. [8 points]: Suppose that a user visits a mashup web page that simultaneously displays a user's\nfavorite email site, ecommerce site, and banking site. Assume that:\n- The email, ecommerce, and banking sites allow themselves to be placed in iframes (e.g., they\ndon't prevent this using X-Frame-Options headers).\n- Each of those three sites is loaded in a separate iframe that is created by the parent mashup frame.\n- Each site (email, ecommerce, banking, and mashup parent) come from a different origin with\nrespect to the same origin policy. Thus, frames cannot directly tamper with each other's state.\nDescribe an attack that the mashup frame can launch to steal sensitive user inputs from the email,\necommerce, or banking site.\nAnswer: The parent mashup frame can place a invisible iframe atop (say) the banking site. Using this\ninvisible frame, the mashup can steal the user's keypresses as she tries to enter her login name and\npassword.\nAdditional attacks are possible. For example, if the user allows the mashup frame to do screensharing,\nthe mashup frame can take a snapshot of child frame content and send that snapshot to an attacker-\ncontrolled server; this allows the attacker to (for example) see emails that the user is currently\ncomposing. The mashup frame can also exploit a child frame that does improper postMessage()\nvalidation and responds to requests from arbitrary initiators.\n\n11. [8 points]: Each external object in a web page has a type. That type is mentioned in the object's\nHTML tag (e.g., an image should have an <img> tag like <img src=\"http://x.com/x.gif\">).\nAn object's type is also described as a MIME type in its HTTP response (e.g., Content-type:\n\"image/gif\").\nThese two kinds of type specifications can mismatch due to programmer error, misconfiguration, or\nmalice. For example, for the tag <img src=\"http://x.com/x.gif\">, the server might return the\nMIME type \"text/css\".\nSuppose that, in the case of a type mismatch, the browser uses the MIME type in the HTTP response\nto determine how to interpret an object. For example, if X's frame tries to load the MIME-type-less tag\n<img src=\"http://Y/file\">, and Y's server returns a MIME type of \"text/css\", the browser will\ninterpret the fetched object as CSS in X's frame, even though the object is embedded in X's frame as\nan <img> tag.\nWhy is this a bad security policy?\nAnswer: The security policy is bad because origin X can include what it believes to be passive content\n(e.g., an image), but origin Y can convince the browser to interpret that content as Javascript code!\nThat JavaScript code will be supplied by Y, but it will run with the authority of X.\n\n12. [6 points]: In a SQL injection attack, attacker-controlled input is evaluated in the context of\na SQL query, resulting in malicious SQL statements executing over sensitive data. Ur/Web allows\nweb applications to directly embed SQL queries in a page; furthermore, those queries may contain\ninformation that originates from the user or an untrusted source. Why is this safe in Ur/Web?\nAnswer: Ur/Web is a strongly-typed system which does not allow external strings to be directly (and\nmaybe accidentally!) interpreted as executable code, SQL queries, etc. This contrasts with the standard\nweb world, in which it is not obvious whether it is safe to (say) assign an externally-supplied string to\nthe innerHTML property of a DOM node.\n\nVII Network security and Kerberos\nBen Bitdiddle is concerned about the sequence number guessing attack that Steve Bellovin described in\nsection 2 of his paper, where an adversary can spoof a TCP connection to a server from an arbitrary source IP\naddress, and send data on that connection.\nBen implements the following strategy that his server will use for choosing the initial sequence number ISNs\nof an incoming TCP connection:\nISNs = ISNoriginal ⊕ IPsrc ⊕ IPdst ⊕ (Portsrc||Portdst)\n(1)\nwhere ⊕ refers to the XOR operation and || refers to concatenation; the IP fields being XORed refer to the\n32-bit IP addresses of the source and destination of the TCP connection; and the Port fields refer to the 16-bit\nsource and destination ports. Assume ISNoriginal increments by 64 for each new incoming connection, and\ninitially starts at some random value.\n13. [8 points]:\nExplain how an adversary could still launch a sequence-number-guessing attack against Ben's server\nwith a small number of tries.\nAnswer: Send two connection requests (SYN packets) to Ben's server, back-to-back: one from the\nadversary's own IP address, and one from the spoofed source IP address. Let's send the connection\nrequest from the adversary's own request first. Then, when the SYN-ACK arrives to the adversary,\nrecover the corresponding ISNoriginal by XORing with the source and destination IP and port numbers.\nThen reconstruct the ISNoriginal that the spoofed connection would get (+64), and XOR with the spoofed\nsource and destination IP and port. Use that result when sending the ACK for the spoofed connection.\n\nSuppose the KDC server at MIT developed a subtle hardware problem, where the random number generator\nbecame highly predictable (e.g., it would often produce the same result when asked for a \"random\" number).\n14. [6 points]:\nHow could an adversary leverage this weakness to access some user's data on a file server that uses\nKerberos for authentication? Describe the minimal amount of additional access the adversary might\nneed to mount such an attack. Assume the file server ignores IP addresses in Kerberos tickets, and that\nthe keys of all principals were generated before the server developed this hardware problem.\nAnswer: Observe at least one message from victim to file server, and extract the user's ticket from that\npacket. Use the knowledge of the RNG predictability to guess the corresponding Kc,s. Now use the\nticket and the guessed Kc,s to issue arbitrary requests to the file server.\n\nVIII 6.858\nWe'd like to hear your opinions about 6.858. Any answer, except no answer, will receive full credit.\n15. [2 points]: We introduced a new lab on symbolic execution this semester (lab 3). How would\nyou suggest improving this lab in future semesters?\nAnswer: 18x iffy instructions, missing/vague specs, more comments in the code; 15x make it more\nexploratory + open-ended, less pre-defined, have students implement more code; 8x it was possible to\ndo the lab without understanding things; 7x better test cases (e.g., exercise 3); 6x improve instructions\nfor exercise 3 (copy Jon's piazza post); 3x recitation about the lab; 2x find more interesting bugs; 2x\nmore background / docs on Z3; 2x faster test cycle; give fewer hints; implement parts of the SMT\nsolver; examples of how concolic execution would work on some piece of code; better explanation of\nconcolic vs symbolic; more exercises like signed avg; shorter explanations needed for lab; use real\nweb app instead of zoobar; better debugging support; maybe ask students to implement parts of the\nAST structure?; actually create constraints for Z3; expand exercises 6 and 7 (more invariant checks);\nbetter visualizations (borrow Austin's grapher from Commuter); unclear what's in concolic variables;\ndiagrams in lab writeup; add concolic_int.__rsub__.\n16. [2 points]: Are there other things you'd like to see improved in the second half of the semester?\nAnswer: 11x more attacks; 5x give hints about hard-to-understand points / background from papers /\nwhere to focus, before reading; 4x office hours on weekends / friday; 3x less tedious papers; 3x more\nfeedback on labs; 3x slower-paced lectures/class; 3x less discussion of papers, more discussion of\nnew material; 2x allow submitting paper questions after 10pm; 2x more quiz review sessions; 2x more\ndiagrams / examples; recitations for stuff not covered in lecture; CTFs; more extensive lab test cases;\nmore web security; newer versions of papers/ideas/systems; post lecture notes before lecture; address\nmore questions from paper questions; more OS-level security; do lecture before paper; more late days;\nstay longer on each given topic; break in the middle of lecture; more analyzing other students' lab\ncode, peer review; more interactive discussions; hands-on exercises for ideas from class; program\nverification; don't sweep details under the rug; more conceptual readings; in-class demos; more help\nfrom TAs on Piazza; fewer labs to give more project time; talk more about papers in lecture; upload\nlecture videos quicker; dislike ASCII diagrams; coffee in lecture.\n17. [2 points]: Is there one paper out of the ones we have covered so far in 6.858 that you think we\nshould definitely remove next year? If not, feel free to say that.\nAnswer: 16x tangled web (long, many didn't read the whole thing!); 8x ur/web; 7.5x capsicum; 5x\ndjango (more context); 4.5x nacl; 3x BROP; 3x forcehttps; 2x kerberos (update it!); 2x klee; confused\ndeputy; TCP.\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1, Fall 2009",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/069aa63e523fa65aa0a905b7cd743914_MIT6_858F14_q09_1.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.893 Fall 2009\nQuiz I\nAll problems are open-ended questions. In order to receive credit you must answer the question\nas precisely as possible. You have 80 minutes to finish this quiz.\nWrite your name on this cover sheet.\nSome questions may be harder than others. Read them all through first and attack them in the\norder that allows you to make the most progress. If you find a question ambiguous, be sure to\nwrite down any assumptions you make. Be neat. If we can't understand your answer, we can't\ngive you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES EXAM.\nPlease do not write in the boxes below.\nI (xx/18)\nII (xx/16)\nIII (xx/18)\nIV (xx/8)\nV (xx/18)\nVI (xx/8)\nVII (xx/8)\nVIII (xx/6)\nTotal (xx/100)\nName:\n\nI Buffer Overflows\nBen Bitdiddle is building a web server that runs the following code sequence, in which process_req()\nis invoked with a user-supplied string of arbitrary length. Assume that process_get() is safe, and for\nthe purposes of this question, simply returns right away.\nvoid process_req(char *input) {\nchar buf[256];\nstrcpy(buf, input);\nif (!strncmp(buf, \"GET \", 4))\nprocess_get(buf);\nreturn;\n}\n1. [6 points]:\nBen Bitdiddle wants to prevent attackers from exploiting bugs in his server, so he\ndecides to make the stack memory non-executable. Explain how an attacker can still exploit a buffer\noverflow in his code to delete files on the server. Draw a stack diagram to show what locations on the\nstack you need to control, what values you propose to write there, and where in the input string these\nvalues need to be located.\n\n2.\n[6 points]:\nSeeing the difficulty of preventing exploits with a non-executable stack, Ben\ninstead decides to make the stack grow up (towards larger addresses), instead of down like on the x86.\nExplain how you could exploit process_req() to execute arbitrary code. Draw a stack diagram\nto illustrate what locations on the stack you plan to corrupt, and where in the input string you would\nneed to place the desired values.\n\n3. [6 points]: Consider the StackGuard system from the \"Buffer Overflows\" paper in the context of\nBen's new system where the stack grows up. Explain where on the stack the canary should be placed,\nat what points in the code the canary should be written, and at what points it should be checked, to\nprevent buffer overflow exploits that take control of the return address.\n\nII XFI\n4. [2 points]: Suppose a program has a traditional buffer overflow vulnerability where the attacker\ncan overwrite the return address on the stack. Explain what attacks, if any, an attacker would be able\nto mount if the same program is run under XFI. Be specific.\n5. [4 points]:\nSuppose a program has a buffer overflow vulnerability which allows an attacker\nto overwrite a function pointer on the stack (which is invoked shortly after the buffer is overflowed).\nExplain what attacks, if any, an attacker would be able to mount if the same program is run under XFI.\nBe specific.\n\n6. [4 points]:\nSuppose a malicious XFI module, which is not allowed to invoke unlink(),\nattempts to remove arbitrary files by directly jumping to the unlink() code in libc. What precise\ninstruction will fail when the attacker tries to do so, if any?\n7. [6 points]:\nSuppose a malicious XFI module wants to circumvent XFI's inline checks in its\ncode. To do so, the module allocates a large chunk of memory, copies its own executable code to it\n(assume XFI is running with only write-protection enabled, for performance reasons, so the module\nis allowed to read its own code), and replaces all XFI check instructions in the copied code with NOP\ninstructions. The malicious module then calls a function pointer, whose value is the start of the copied\nversion of the function that the module would ordinarily invoke. Does XFI prevent an attacker from\nbypassing XFI's checks in this manner, and if so, what precise instruction would fail?\n\nIII Privilege Separation\n8. [4 points]: OKWS uses database proxies to control what data each service can access, but lab 2\nhas no database proxies. Explain what controls the data that each service can access in lab 2.\n9. [8 points]:\nIn lab 2, logging is implemented by a persistent process that runs under a separate\nUID and accepts log messages, so that an attacker that compromises other parts of the application\nwould not be able to corrupt the log. Ben Bitdiddle dislikes long-running processes, but still wants to\nprotect the log from attackers. Suggest an alternative design for Ben that makes sure past log messages\ncannot be tampered with by an attacker, but does not assume the existence of any long-running user\nprocess.\n\n10. [6 points]:\nBen proposes another strawman alternative to OKWS: simply use chroot() to\nrun each service process in a separate directory root. Since each process will only be able to access\nits own files, there is no need to run each process under a separate UID. Explain why Ben's approach\nis faulty, and how an attacker that compromises one service will be able to compromise other services\ntoo.\n\nIV Information Flow Control\n11. [8 points]:\nThis problem was buggy; everyone received full credit.\n\nV Java\n12. [4 points]:\nWhen a privileged operation is requested, extended stack introspection walks up\nthe stack looking for a stack frame which called enablePrivilege(), but stops at the first stack\nframe that is not authorized to call enablePrivilege(). Give an example of an attack that could\noccur if stack inspection did not stop at such stack frames.\n\n13. [8 points]:\nSuppose you wanted to run an applet and allow it to connect over the network to\nweb.mit.edu port 80, but nowhere else. In Java, opening a network connection is done by constructing\na Socket object, passing the host and port as arguments to the constructor. Sketch out how you would\nimplement this security policy with extended stack introspection, assuming that the system library\nimplementing sockets calls checkPrivilege(\"socket\") in the Socket constructor. Explain\nhow the applet must change, if any.\n\n14. [6 points]: Sketch out how you would implement the same security policy as in part (b), except\nby using name space management. Explain how the applet must change, if any.\n\nVI Browser\n15. [8 points]:\nThe paper argues that the child policy (where a frame can navigate its immedi\nate children) is unnecessarily strict, and that the descendant policy (where a frame can navigate the\nchildren of its children's frames, and so on) is just as good. Give an example of how the descendant\npolicy can lead to security problems that the child policy avoids.\n\nVII Resin\n16. [8 points]: Sketch out the Resin filter and policy objects that would be needed to avoid cross-\nsite scripting attacks through user profiles in zoobar. Assume that you have a PHP function to strip\nout JavaScript.\n\nVIII 6.893\nWe'd like to hear your opinions about 6.893, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n17. [2 points]: How could we make the ideas in the course easier to understand?\n18. [2 points]: What is the best aspect of 6.893?\n19. [2 points]: What is the worst aspect of 6.893?\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1, Fall 2010",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/304853fdff3e62e8a3a3d7389bfdc89b_MIT6_858F14_q10_1.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2010\nQuiz I\nAll problems are open-ended questions. In order to receive credit you must answer the question\nas precisely as possible. You have 80 minutes to finish this quiz.\nWrite your name on this cover sheet.\nSome questions may be harder than others. Read them all through first and attack them in the\norder that allows you to make the most progress. If you find a question ambiguous, be sure to\nwrite down any assumptions you make. Be neat. If we can't understand your answer, we can't\ngive you credit!\nTHIS IS AN OPEN BOOK, OPEN NOTES EXAM.\nPlease do not write in the boxes below.\nI (xx/16)\nII (xx/15)\nIII (xx/17)\nIV (xx/10)\nV (xx/16)\nVI (xx/6)\nTotal (xx/80)\nName:\n\nI Baggy bounds checking\nSuppose that you use Baggy bounds checking to run the following C code, where X and Y are constant\nvalues. Assume that slot size is 16 bytes, as in the paper.\nchar *p = malloc(40);\nchar *q = p + X;\nchar *r = q + Y;\n*r = '\\0';\nFor the following values of X and Y , indicate which line number will cause Baggy checking to abort, or\nNONE if the program will finish executing without aborting.\n1. [2 points]: X = 45, Y = 0\n2. [2 points]: X = 60, Y = 0\n3. [2 points]: X = 50, Y = -20\n4. [2 points]: X = 70, Y = -20\n5. [2 points]: X = 80, Y = -20\n6. [2 points]: X = -5, Y = 4\n7. [2 points]: X = -5, Y = 60\n8. [2 points]: X = -10, Y = 20\n\nII Control hijacking\nConsider the following C code:\nstruct foo {\nchar buf[40];\nvoid (*f2) (struct foo *);\n};\nvoid\nf(void)\n{\nvoid (*f1) (struct foo *);\nstruct foo x;\n/* .. initialize f1 and x.f2 in some way .. */\ngets(x.buf);\nif (f1)\nf1(&x);\nif (x.f2) x.f2(&x);\n}\nThere are three possible code pointers that may be overwritten by the buffer overflow vulnerability: f1,\nx.f2, and the function's return address on the stack. Assume that the compiler typically places the return\naddress, f1, and x in that order, from high to low address, on the stack, and that the stack grows down.\n9. [5 points]:\nWhich of the three code pointers can be overwritten by an adversary if the code is\nexecuted as part of an XFI module?\n\n10. [5 points]:\nWhat code could the adversary cause to be executed, if any, if the above code is\nexecuted as part of an XFI module?\n11. [5 points]:\nWhat code could the adversary cause to be executed, if any, if the above code is\nexecuted under control-flow enforcement from lab 2 (no XFI)?\n\nIII OS protection\nBen Bitdiddle is running a web site using OKWS, with one machine running the OKWS server, and a\nseparate machine running the database and the database proxy.\n12. [12 points]:\nThe database machine is maintained by another administrator, and Ben cannot\nchange the 20-byte authentication tokens that are used to authenticate each service to the database\nproxy. This makes Ben worried that, if an adversary steals a token through a compromised or ma\nlicious service, Ben will not be able to prevent the adversary from accessing the database at a later\ntime.\nPropose a change to the OKWS design that would avoid giving tokens to each service, while providing\nthe same guarantees in terms of what database operations each service can perform, without making\nany changes to the database machine.\n\n13. [5 points]: Ben is considering running a large number of services under OKWS, and is worried\nhe might run out of UIDs. To this end, Ben considers changing OKWS to use the same UID for\nseveral services, but to isolate them from each other by placing them in separate chroot directories\n(instead of the current OKWS design, which uses different UIDs but the same chroot directory).\nExplain, specifically, how an adversary that compromises one service can gain additional privileges\nunder Ben's design that he or she cannot gain under the original OKWS design.\n\nIV Capabilities and C\nBen Bitdiddle is worried that a plugin in his web browser could be compromised, and decides to apply some\nideas from the \"Security Architectures for Java\" paper to sandboxing the plugin's C code using XFI.\nBen decides to use the capability model (§3.2 from the Java paper), and writes a function safe open as\nfollows:\nint\nsafe_open(const char *pathname, int flags, mode_t mode)\n{\nchar buf[1024];\nsnprintf(buf, sizeof(buf), \"/safe-dir/%s\", pathname);\nreturn open(buf, flags, mode);\n}\nwhich is intended to mirror Figure 2 from the Java paper. To allow his plugin's XFI module to access to\nfiles in /safe-dir, Ben allows the XFI module to call the safe open function, as well as the standard\nread, write, and close functions (which directly invoke the corresponding system calls).\n14.\n[10 points]:\nCan a malicious XFI module access files (i.e., read or write) outside of\n/safe-dir? As in the Java paper, let's ignore symbolic links and \"..\" components in the path\nname. Explain how or argue why not.\n\nV Browser security\n15. [6 points]:\nIn pages of a site which has enabled ForceHTTPS, <SCRIPT SRC=...> tags\nthat load code from an http://.../ URL are redirected to an https://.../ URL. Explain\nwhat could go wrong if this rewriting was not performed.\n\nBen Bitdiddle runs a web site that frequently adds and removes files, which leads to customers complaining\nthat old links often return a 404 File not found error. Ben decides to fix this problem by adding a link to\nhis site's search page, and modifies how his web server responds to requests for missing files, as follows (in\nPython syntax):\ndef missing_file(reqpath):\nprint \"HTTP/1.0 200 OK\"\nprint \"Content-Type: text/html\"\nprint \"\"\nprint \"We are sorry, but the server could not locate file\", reqpath\nprint \"Try using the <A HREF=/search>search function</A>.\"\n16. [10 points]:\nExplain how an adversary may be able to exploit Ben's helpful error message to\ncompromise the security of Ben's web application.\n\nVI 6.858\nWe'd like to hear your opinions about 6.858, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n17. [2 points]: How could we make the ideas in the course easier to understand?\n18. [2 points]: What is the best aspect of 6.858?\n19. [2 points]: What is the worst aspect of 6.858?\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Computer Systems Security, Exam 1, Fall 2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/b3307e64f56cf5634085870d8af0a7e2_MIT6_858F14_q11_1.pdf",
      "content": "Department of Electrical Engineering and Computer Science\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.858 Fall 2011\nQuiz I\nYou have 80 minutes to answer the questions in this quiz. In order to receive credit you must answer\nthe question as precisely as possible.\nSome questions are harder than others, and some questions earn more points than others. You may\nwant to skim them all through first, and attack them in the order that allows you to make the most\nprogress.\nIf you find a question ambiguous, be sure to write down any assumptions you make. Be neat and\nlegible. If we can't understand your answer, we can't give you credit!\nWrite your name on this cover sheet.\nTHIS IS AN OPEN BOOK, OPEN NOTES EXAM.\nPlease do not write in the boxes below.\nI (xx/20)\nII (xx/10)\nIII (xx/16)\nIV (xx/22)\nV (xx/10)\nVI (xx/16)\nVII (xx/6)\nTotal (xx/100)\nName:\nUsername from handin site:\n\nI XFI\nConsider the following assembly code, which zeroes out 256 bytes of memory pointed to by the EAX register.\nThis code will execute under XFI. XFI's allocation stack is not used in this code.\nYou will need fill in the verification states for this code, which would be required for the verifier to check the\nsafety of this code, along the lines of the example shown in Figure 4 of the XFI paper. Following the example\nfrom the paper, possible verification state statements include:\nvalid[regname+const, regname+const)\norigSSP = regname+const\nretaddr = Mem[regname]\nwhere regname and const are any register names and constant expressions, respectively. Include all\nverification states necessary to ensure safety of the subsequent instruction, and to ensure that the next\nverification state is legal.\nx86 instructions\nVerification state\nmrguard(EAX, 0, 256)\n(1)\nECX := EAX\n# current pointer\nEDX := EAX+256\n# end of 256-byte array\n8 loop:\n(2)\nMem[ECX] := 0\nECX := ECX+4\n(3)\nif ECX+4 > EDX, jmp out\n(4)\njmp loop\n17 out:\n...\n\n1. [5 points]: What are the verification states needed at location marked (1)?\n2. [5 points]: What are the verification states needed at location marked (2)?\n3. [5 points]: What are the verification states needed at location marked (3)?\n4. [5 points]: What are the verification states needed at location marked (4)?\n\nII ForceHTTPS\n5. [10 points]: Suppose bank.com uses and enables ForceHTTPS, and has a legitimate SSL\ncertificate signed by Verisign. Which of the following statements are true?\nA. True / False ForceHTTPS prevents the user from entering their password on a phishing web site\nimpersonating bank.com.\nB. True / False ForceHTTPS ensures that the developer of the bank.com web site cannot acciden\ntally load Javascript code from another web server using <SCRIPT SRC=...>.\nC. True / False ForceHTTPS prevents a user from accidentally accepting an SSL certificate for\nbank.com that's not signed by any legitimate CA.\nD. True / False\nForceHTTPS prevents a browser from accepting an SSL certificate for bank.com\nthat's signed by a CA other than Verisign.\n\nIII Zoobar security\nBen Bitdiddle is working on lab 2. For his privilege separation, he decided to create a separate database\nto store each user's zoobar balance (instead of a single database called zoobars that stores everyone's\nbalance). He stores the zoobar balance for user x in the directory /jail/zoobar/db/zoobars.x, and\nensures that usernames cannot contain slashes or null characters. When a user first registers, the login service\nmust be able to create this database for the user, so Ben sets the permissions for /jail/zoobar/db to\n0777.\n6. [4 points]: Explain why this design may be a bad idea. Be specific about what an adversary would\nhave to do to take advantage of a weakness in this design.\n\nBen Bitdiddle is now working on lab 3. He has three user IDs for running server-side code, as suggested in\nlab 2 (ignoring transfer logging):\n- User ID 900 is used to run dynamic python code to handle HTTP requests (via zookfs). The database\ncontaining user profiles is writable only by uid 900.\n- User ID 901 is used to run the authentication service, which provides an interface to obtain a token\ngiven a username and password, and to check if some token for a username is valid. The database\ncontaining user passwords and tokens is stored in a DB that is readable and writable only by uid 901.\n- User ID 902 is used to run the transfer service, which provides an interface to transfer zoobar credits\nfrom user A to user B, as long as a token for user A is provided. The database storing zoobar balances\nis writable only by uid 902. The transfer service invokes the authentication service to check whether a\ntoken is valid.\nRecall that to run Python profile code for user A, Ben must give the profile code access to A's token (the\nprofile code may want to transfer credits to visitors, and will need this token to invoke the transfer service).\nTo support Python profiles, Ben adds a new operation to the authentication service's interface, where the\ncaller supplies an argument username, the authentication service looks up the profile for username, runs\nthe profile's code with a token for username, and returns the output of that code.\n7. [4 points]: Ben discovers that a bug in the HTTP handling code (running as uid 900) can allow an\nadversary to steal zoobars from any user. Explain how an adversary can do this in Ben's design.\n\n8. [8 points]: Propose a design change that prevents attackers from stealing zoobars even if they\ncompromise the HTTP handling code. Do not make any changes to the authentication or transfer\nservices (i.e., code running as uid 901 and 902).\n\nIV Baggy bounds checking\nConsider a system that runs the following code under the Baggy bounds checking system, as described in the\npaper by Akritidis et al, with slot size=16:\n1 struct sa {\nchar buf[32];\nvoid (*f) (void);\n4 };\n6 struct sb {\nvoid (*f) (void);\nchar buf[32];\n9 };\n11 void handle(void) {\nprintf(\"Hello.\\n\");\n13 }\n15 void buggy(char *buf, void (**f) (void)) {\n*f = handle;\ngets(buf);\n(*f) ();\n19 }\n21 void test1(void) {\nstruct sa x;\nbuggy(x.buf, &x.f);\n24 }\n26 void test2(void) {\nstruct sb x;\nbuggy(x.buf, &x.f);\n29 }\n31 void test3(void) {\nstruct sb y;\nstruct sa x;\nbuggy(x.buf, &y.f);\n35 }\n37 void test4(void) {\nstruct sb x[2];\nbuggy(x[0].buf, &x[1].f);\n40 }\nAssume the compiler performs no optimizations and places variables on the stack in the order declared, the\nstack grows down (from high address to low address), that this is a 32-bit system, and that the address of\nhandle contains no zero bytes.\n\n9. [6 points]:\nA. True / False If function test1 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nB. True / False If function test2 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nC. True / False If function test3 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nD. True / False\nIf function test4 is called, an adversary can construct an input that will cause the\nprogram to jump to an arbitrary address.\nFor the next four questions, determine what is the minimum number of bytes that an adversary has to provide\nas input to cause this program to likely crash, when running different test functions. Do not count the newline\ncharacter that the adversary has to type in to signal the end of the line to gets. Recall that gets terminates\nits string with a zero byte.\n10. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test1 to crash?\n11. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test2 to crash?\n12. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test3 to crash?\n13. [4 points]: What is the minimum number of bytes that an adversary has to provide as input to\nlikely cause a program running test4 to crash?\n\nV Browser security\nThe same origin policy generally does not apply to images or scripts. What this means is that a site may\ninclude images or scripts from any origin.\n14. [3 points]: Explain why including images from other origins may be a bad idea for user privacy.\n15. [3 points]: Explain why including scripts from another origin can be a bad idea for security.\n16. [4 points]: In general, access to the file system by JavaScript is disallowed as part of JavaScript\ncode sandboxing. Describe a situation where executing JavaScript code will lead to file writes.\n\nVI Static analysis\nConsider the following snippet of JavaScript code:\n1 var P = false;\n3 function foo() {\nvar t1 = new Object();\nvar t2 = new Object();\nvar t = bar(t1, t2);\nP = true;\n8 }\n10 function bar(x, y) {\nvar r = new Object();\nif (P) {\nr = x;\n} else {\nr = y;\n}\nreturn r;\n19 }\nA flow sensitive pointer analysis means that the analysis takes into account the order of statements in the\nprogram. A flow insensitive pointer analysis does not consider the order of statements.\n17. [4 points]: Assuming no dead code elimination is done, a flow-insensitive pointer analysis (i.e.,\none which does not consider the control flow of a program) will conclude that variable t in function\nfoo may point to objects allocated at the following line numbers:\nA. True / False\nLine 1\nB. True / False\nLine 4\nC. True / False\nLine 5\nD. True / False\nLine 11\n\n18. [4 points]: Assuming no dead code elimination is done, a flow-sensitive pointer analysis (i.e.,\none which considers the control flow of a program) will conclude that variable t in function foo may\npoint to objects allocated at the following line numbers:\nA. True / False\nLine 1\nB. True / False\nLine 4\nC. True / False\nLine 5\nD. True / False\nLine 11\n19. [2 points]: At runtime, variable t in function foo may only be observed pointing to objects\nallocated at the following line numbers:\nA. True / False\nLine 1\nB. True / False\nLine 4\nC. True / False\nLine 5\nD. True / False\nLine 11\n\n20. [2 points]: Do you think a sound analysis that supports the eval construct is going to be precise?\nPlease explain.\n21. [4 points]: What is one practical advantage of the bottom-up analysis of the call graph described\nin the PHP paper by Xie and Aiken (discussed in class)?\n\nVII 6.858\nWe'd like to hear your opinions about 6.858, so please answer the following questions. (Any answer, except\nno answer, will receive full credit.)\n22. [2 points]: How could we make the ideas in the course easier to understand?\n23. [2 points]: What is the best aspect of 6.858 so far?\n24. [2 points]: What is the worst aspect of 6.858 so far?\nEnd of Quiz\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Lab 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/af3398eef4ec504934b989b26bbc920a_MIT6_858F14_lab1.pdf",
      "content": "6.858 Fall 2014 Lab 1: Buffer overflows\nHanded out:\nLecture 1\nParts 1 and 2 due: Two days after Lecture 3 (5:00pm)\nAll parts due:\nTwo days after Lecture 5 (5:00pm)\nIntroduction\nYou will do a sequence of labs in 6.858. These sequence of labs will give you practical experience with common attacks and counter\nmeasures. To make the issues concrete, you will explore the attacks and counter meatures in the context of the zoobar web\napplication in the following ways:\nLab 1: you will explore the base structure of the zoobar web application, and use buffer overrun attacks to break its security\nproperties.\nLab 2: you will improve zoobar web application by using privilege separation, so that if one components is compromised, the\nadversary doesn't have control over the whole web application.\nLab 3: you will build a program analysis tool (likely based on symbolic execution) to find bugs in Python code such as the\nzoobar web application.\nLab 4: you will perform a security audit of two other student's solutions. You probably want to do a good job in lab 2 and lab\n3.\nLab 5: you will improve the zoobar application against browser attacks.\nLab 6: you will extend the zoobar application to support javascript user profiles in a secure manner.\nLab 1 will introduce you to buffer overflow vulnerabilities, in the context of a web server called zookws. The zookws web server is\nrunning a simple python web application, zoobar, where users transfer \"zoobars\" (credits) between each other. You will find buffer\noverflows in the zookws web server code, write exploits for the buffer overflows to inject code into the server, figure out how to\nbypass non-executable stack protection, and finally look for other potential problems in the web server implementation. Later labs\nlook at other security aspects of the zoobar and zookws infrastructure.\nEach lab requires you to learn a new programming language or some other piece of infrastructure. For example, in this lab you must\nbecome intimate familiar with certain aspects of the C language, x86 assembly language, gdb, etc. The labs do so because that allows\nyou to understand attacks and defenses in realistic situations. Often you need to understand certain parts of this new infrastructure in\ndetail; security weaknesses often show up in corner cases, and so you need to understand the details to craft exploits and design\ndefenses for those corner cases. These two factors (new infrastructure and details) can make the labs time consuming. You should\nstart early on the labs and work on them daily for some limited time (each lab has several exercises), instead of trying to do all\nexercises in a single shot before the deadline. You should also try to understand the necessary details, instead of muddling your way\nthrough the exercises. If you don't, the labs will take a lot of time. If you get stuck on a detail, post a question on Piazza.\nSeveral labs, including this lab, ask you to design exploits. These exploits are realistic enough that you might be able to use them for\na real attack, but you should not do so. The point of the designing exploits is to teach you how to defend against them, not how to use\nthem---attacking computer systems is illegal and can get you into serious trouble. Don't do it.\nLab infrastructure\nExploiting buffer overflows requires precise control over the execution environment. A small change in the compiler, environment\nvariables, or the way the program is executed can result in slightly different memory layout and code structure, thus requiring a\ndifferent exploit. For this reason, this lab uses a VMware virtual machine to run the vulnerable web server code.\nTo start working on this lab assignment, you should download the VMware Player, which can run virtual machines on Linux and\nWindows systems. For Mac users, MIT has a site license for VMware Fusion. You can download VMware Fusion from this web\nsite.\nOnce you have VMware installed on your machine, you should download the course VM image, and unpack it on your computer.\nThis virtual machine contains an installation of Ubuntu 14.04.1 Linux, and the following accounts have been created inside the VM.\nUsername Password Description\nYou can use the\nroot\nroot account to install new software packages into the VM, if you find\nsomething missing, using apt-get install pkgname.\nThe\nhttpd\nhttpd account is used to execute the web server, and contains the source code you will\nneed for this lab assignment, in /home/httpd/lab.\nFor Linux users, we've also tested running the course VM on KVM, which is built into the Linux kernel and should be much easier\n\nto get working than VMware. KVM should be available through your distribution, and is preinstalled on Athena cluster computers;\non Debian or Ubuntu, try apt-get install qemu-kvm. Once installed, you should be able to run a command like\nkvm -m 512 -net nic -net user,hostfwd=tcp:127.0.0.1:2222-:22,hostfwd=tcp:127.0.0.1:8080-:8080 vm-6858.vmdk to\nrun the VM and forward the relevant ports.\nYou can either log into the virtual machine using its console, or you can use ssh to log into the virtual machine over the (virtual)\nnetwork. To determine the virtual machine's IP address, log in as root on the console and run /sbin/ifconfig eth0. (If using KVM\nwith the command above, then ssh -p 2222 httpd@localhost should work.)\nThe files you will need for this and subsequent lab assignments in this course is distributed using the Git version control system. You\ncan also use Git to keep track of any changes you make to the initial source code. Here's an overview of Git and the Git user's\nmanual\nyou may find useful.\nrepository is available at git://g.csail.mit.edu/6.858-lab-2014. Alternately, download lab1.zip from the Labs\non the MIT OpenCourseWare site. To begin with, log into the VM using the httpd account and clone the source\nfollows.\nhttpd@vm-6858:~$ git clone git://g.csail.mit.edu/6.858-lab-2014 lab\nInitialized empty Git repository in /home/httpd/lab/.git/\nhttpd@vm-6858:~$ cd lab\nhttpd@vm-6858:~/lab$\nproceed with this lab assignment, make sure you can compile the zookws web server:\nhttpd@vm-6858:~/lab$ make\nzookld.c -c -o zookld.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE -fno-stack-protector\nhttp.c -c -o http.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE -fno-stack-protector\nzookld.o http.o -lcrypto -o zookld\nzookfs.c -c -o zookfs.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE -fno-stack-protector\nzookfs.o http.o -lcrypto -o zookfs\nzookfs zookfs-nxstack\nzookd.c -c -o zookd.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE -fno-stack-protector\nzookd.o http.o -lcrypto -o zookd\nzookd-nxstack\nzookfs zookfs-exstack\nexecstack -s zookfs-exstack\nzookd-exstack\nexecstack -s zookd-exstack\nzookfs.c -c -o zookfs-withssp.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE\nhttp.c -c -o http-withssp.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE\nzookfs-withssp.o http-withssp.o -lcrypto -o zookfs-withssp\nzookd.c -c -o zookd-withssp.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE\nzookd-withssp.o http-withssp.o -lcrypto -o zookd-withssp\n-c -o shellcode.o shellcode.S\n-S -O binary -j .text shellcode.o shellcode.bin\nrun-shellcode.c -c -o run-shellcode.o -m32 -g -std=c99 -Wall -Werror -D_GNU_SOURCE -fno-stack-protector\nrun-shellcode.o -lcrypto -o run-shellcode\nshellcode.o\nhttpd@vm-6858:~/lab$\nserver consists of the following components.\nzookld, a launcher daemon that launches services configured in the file zook.conf.\nzookd, a dispatcher that routes HTTP requests to corresponding services.\nzookfs and other services that may serve static files or execute dynamic scripts.\nAfter zookld launches configured services, zookd listens on a port (8080 by default) for incoming HTTP requests and reads the first\nline of each request for dispatching. In this lab, zookd is configured to dispatch every request to the zookfs service, which reads the\nrest of the request and generates a response from the requested file. Most HTTP-related code is in http.c. Here is a tutorial of the\nHTTP protocol.\nThere are two versions of the web server you will be using:\nzookld, zookd-exstack, zookfs-exstack, as configured in the file zook-exstack.conf;\nzookld, zookd-nxstack, zookfs-nxstack, as configured in the file zook-nxstack.conf.\nIn the first one, the *-exstack binaries have an executable stack, which makes it easier to inject executable code given a stack buffer\noverflow vulnerability. The *-nxstack binaries in the second version have a non-executable stack, and you will write exploits that\nbypass non-executable stacks later in this lab assignment.\nIn order to run the web server in a predictable fashion---so that its stack and memory layout is the same every time---you will use the\nclean-env.sh script. This is the same way in which we will run the web server during grading, so make sure all of your exploits\nwork on this configuration!\n\nThe reference binaries of zookws are provided in bin.tar.gz, which we will use for grading. Make sure your exploits work on those\nbinaries.\nNow, make sure you can run the zookws web server and access the zoobar web application from a browser running on your\nmachine, as follows:\nhttpd@vm-6858:~/lab$ /sbin/ifconfig eth0\neth0 Link encap:Ethernet HWaddr 00:0c:29:57:90:a1\ninet addr:172.16.91.143 Bcast:172.16.91.255 Mask:255.255.255.0\ninet6 addr: fe80::20c:29ff:fe57:90a1/64 Scope:Link\nUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\nRX packets:149 errors:0 dropped:0 overruns:0 frame:0\nTX packets:94 errors:0 dropped:0 overruns:0 carrier:0\ncollisions:0 txqueuelen:1000\nRX bytes:15235 (15.2 KB) TX bytes:12801 (12.8 KB)\nInterrupt:19 Base address:0x2000\nhttpd@vm-6858:~/lab$ ./clean-env.sh ./zookld zook-exstack.conf\nThe /sbin/ifconfig command will give you the virtual machine's IP address. In this particular example, you would want to open\nyour browser and go to the URL http://172.16.91.143:8080/. (If you're using KVM with the command above, just access\nhttp://localhost:8080/ on your host.) If something doesn't seem to be working, try to figure out what went wrong, or contact the\ncourse staff, before proceeding further.\nPart 1: Finding buffer overflows\nIn the first part of this lab assignment, you will find buffer overflows in the provided web server. Read Aleph One's article, Smashing\nthe Stack for Fun and Profit, as well as this paper, to figure out how buffer overflows work.\nExercise 1. Study the web server's code, and find examples of code vulnerable to memory corruption through a buffer\noverflow. Write down a description of each vulnerability in the file /home/httpd/lab/bugs.txt; use the format\ndescribed in that file. For each vulnerability, describe the buffer which may overflow, how you would structure the\ninput to the web server (i.e., the HTTP request) to overflow the buffer, and whether the vulnerability can be prevented\nusing stack canaries. Locate at least 5 different vulnerabilities.\nYou can use the command make check-bugs to check if your bugs.txt file matches the required format, although the\ncommand will not check whether the bugs you listed are actual bugs or whether your analysis of them is correct.\nNow, you will start developing exploits to take advantage of the buffer overflows you have found above. We have provided template\nPython code for an exploit in /home/httpd/lab/exploit-template.py, which issues an HTTP request. The exploit template takes\ntwo arguments, the server name and port number, so you might run it as follows to issue a request to zookws running on localhost:\nhttpd@vm-6858:~/lab$ ./clean-env.sh ./zookld zook-exstack.conf &\n[1] 2676\nhttpd@vm-6858:~/lab$ ./exploit-template.py localhost 8080\nHTTP request:\nGET / HTTP/1.0\n...\nhttpd@vm-6858:~/lab$\nYou are free to use this template, or write your own exploit code from scratch. Note, however, that if you choose to write your own\nexploit, the exploit must run correctly inside the provided virtual machine.\nYou will find gdb useful in building your exploits. As zookws forks off many processes, it can be difficult to debug the correct one.\nThe easiest way to do this is to run the web server ahead of time with clean-env.sh and then attaching gdb to an already-running\nprocess with the -p flag. To help find the right process for debugging, zookld prints out the process IDs of the child processes that it\nspawns. You can also find the PID of a process by using pgrep; for example, to attach to zookd-exstack, start the server and, in\nanother shell, run\nhttpd@vm-6858:~/lab$ gdb -p $(pgrep zookd-exstack)\n...\n0x4001d422 in __kernel_vsyscall ()\n(gdb) break your-breakpoint\nBreakpoint 1 at 0x1234567: file zookd.c, line 999.\n(gdb) continue\nContinuing.\n\nKeep in mind that a process being debugged by gdb will not get killed even if you terminate the parent zookld process using ^C. If\nyou are having trouble restarting the web server, check for leftover processes from the previous run, or be sure to exit gdb before\nrestarting zookld.\nWhen a process being debugged by gdb forks, by default gdb continues to debug the parent process and does not attach to the child.\nSince zookfs forks a child process to service each request, you may find it helpful to have gdb attach to the child on fork, using the\ncommand set follow-fork-mode child. We have added that command to /home/httpd/lab/.gdbinit, which will take effect if\nyou start gdb in that directory.\nFor this and subsequent exercises, you may need to encode your attack payload in different ways, depending on which vulnerability\nyou are exploiting. In some cases, you may need to make sure that your attack payload is URL-encoded; that is, use + instead of\nspace and %2b instead of +. Here is a URL encoding reference and a handy conversion tool. You can also use quoting functions in the\npython urllib module to URL encode strings. In other cases, you may need to include binary values into your payload. The Python\nstruct module can help you do that. For example, struct.pack(\"<I\", x) will produce a 4-byte (32-bit) binary encoding of the\ninteger x.\nExercise 2. Pick two buffer overflows out of what you have found for later exercises (although you can change your\nmind later, if you find your choices are particularly difficult to exploit). The first must overwrite a return address on the\nstack, and the second must overwrite some other data structure that you will use to take over the control flow of the\nprogram.\nWrite exploits that trigger them. You do not need to inject code or do anything other than corrupt memory past the end\nof the buffer, at this point. Verify that your exploit actually corrupts memory, by either checking the last few lines of\ndmesg | tail, using gdb, or observing that the web server crashes.\nProvide the code for the exploits in files called exploit-2a.py and exploit-2b.py, and indicate in answers.txt\nwhich buffer overflow each exploit triggers. If you believe some of the vulnerabilities you have identified in Exercise 1\ncannot be exploited, choose a different vulnerability.\nYou can check whether your exploits crash the server as follows:\nhttpd@vm-6858:~/lab$ make check-crash\nPart 2: Code injection\nIn this part, you will use your buffer overflow exploits to inject code into the web server. The goal of the injected code will be to\nunlink (remove) a sensitive file on the server, namely /home/httpd/grades.txt. Use the *-exstack binaries, since they have an\nexecutable stack that makes it easier to inject code. The zookws web server should be started as follows.\nhttpd@vm-6858:~/lab$ ./clean-env.sh ./zookld zook-exstack.conf\nWe have provided Aleph One's shell code for you to use in /home/httpd/lab/shellcode.S, along with Makefile rules that produce\n/home/httpd/lab/shellcode.bin, a compiled version of the shell code, when you run make. Aleph One's exploit is intended to\nexploit setuid-root binaries, and thus it runs a shell. You will need to modify this shell code to instead unlink\n/home/httpd/grades.txt.\nTo help you develop your shell code for this next exercise, we have provided a program called run-shellcode that will run your\nbinary shell code, as if you correctly jumped to its starting point. For example, running it on Aleph One's shell code will cause the\nprogram to execve(\"/bin/sh\"), thereby giving you another shell prompt:\nhttpd@vm-6858:~/lab$ ./run-shellcode shellcode.bin\n$\nWhen developing an exploit, you will have to think about what values are on the stack, so that you can modify them accordingly. For\nyour reference, here is what the stack frame of some function foo looks like; here, foo has a local variable char buf[256]:\n+------------------+\n| ... |\n| stack frame of |\n| foo's caller |\n| ... |\n+------------------+\n| return address | (4 bytes)\n| to foo's caller |\n+------------------+\n\n%ebp ------> | saved %ebp | (4 bytes)\n+------------------+\n| ... |\n+------------------+\n| buf[255] |\n| ... |\nbuf ------> | buf[0] |\n+------------------+\nNote that the stack grows down in this figure, and memory addresses are increasing up.\nWhen you're constructing an exploit, you will often need to know the addresses of specific stack locations, or specific functions, in a\nparticular program. The easiest way to do this is to use gdb. For example, suppose you want to know the stack address of the pn[]\narray in the http_serve function in zookfs-exstack, and the address of its saved %ebp register on the stack. You can obtain them\nusing gdb as follows:\nhttpd@vm-6858:~/lab$ gdb -p $(pgrep zookfs-exstack)\n...\n0x40022416 in __kernel_vsyscall ()\n(gdb) break http_serve\nBreakpoint 1 at 0x8049415: file http.c, line 248.\n(gdb) continue\nContinuing.\nBe sure to run gdb from the ~/lab directory, so that it picks up the set follow-fork-mode child command from ~/lab/.gdbinit.\nNow you can issue an HTTP request to the web server, so that it triggers the breakpoint, and so that you can examine the stack of\nhttp_serve:\n[New process 1339]\n[Switching to process 1339]\nBreakpoint 1, http_serve (fd=3, name=0x8051014 \"/\") at http.c:248\n248 void (*handler)(int, const char *) = http_serve_none;\n(gdb) print &pn\n$1 = (char (*)[1024]) 0xbfffd10c\n(gdb) info registers\neax 0x3 3\necx 0x400bdec0 1074519744\nedx 0x6c6d74 7105908\nebx 0x804a38e 134521742\nesp 0xbfffd0a0 0xbfffd0a0\nebp 0xbfffd518 0xbfffd518\nesi 0x0 0\nedi 0x0 0\neip 0x8049415 0x8049415 <http_serve+9>\neflags 0x200286 [ PF SF IF ID ]\ncs 0x73 115\nss 0x7b 123\nds 0x7b 123\nes 0x7b 123\nfs 0x0 0\ngs 0x33 51\n(gdb)\nFrom this, you can tell that, at least for this invocation of http_serve, the pn[] buffer on the stack lives at address 0xbfffd10c, and\nthe value of %ebp (which points at the saved %ebp on the stack) is 0xbfffd518.\nNow it's your turn to develop an exploit.\nExercise 3. Starting from one of your exploits from Exercise 2, construct an exploit that hijacks control flow of the web\nserver and unlinks /home/httpd/grades.txt. Save this exploit in a file called exploit-3.py.\nExplain in answers.txt whether or not the other buffer overflow vulnerabilities you found in Exercise 1 can be\nexploited in this manner.\nVerify that your exploit works; you will need to re-create /home/httpd/grades.txt after each successful exploit run.\nSuggestion: first focus on obtaining control of the program counter. Sketch out the stack layout that you expect the\nprogram to have at the point when you overflow the buffer, and use gdb to verify that your overflow data ends up where\nyou expect it to. Step through the execution of the function to the return instruction to make sure you can control what\naddress the program returns to. The next, stepi, info reg, and disassemble commands in gdb should prove helpful.\nOnce you can reliably hijack the control flow of the program, find a suitable address that will contain the code you\nwant to execute, and focus on placing the correct code at that address---e.g. a derivative of Aleph One's shell code.\n\nNote: SYS_unlink, the number of the unlink syscall, is 10 or '\\n' (newline). Why does this complicate matters? How\ncan you get around it?\nYou can check whether your exploit works as follows:\nhttpd@vm-6858:~/lab$ make check-exstack\nThe test either prints \"PASS\" or fails. We will grade your exploits in this way. If you use another name for the exploit script, change\nMakefile accordingly.\nThe standard C compiler used on Linux, gcc, implements a version of stack canaries (called SSP). You can explore whether GCC's\nversion of stack canaries would or would not prevent a given vulnerability by using the SSP-enabled versions of the web server\nbinaries (zookd-withssp and zookfs-withssp), by using the zook-withssp.conf config file when starting zookld.\nSubmit your answers to the first two parts of this lab assignment by running make submit-a. Alternatively, run\nmake prepare-submit-a. The resulting lab1a-handin.tar.gz file will be graded.\nPart 3: Return-to-libc attacks\nMany modern operating systems mark the stack non-executable in an attempt to make it more difficult to exploit buffer overflows. In\nthis part, you will explore how this protection mechanism can be circumvented. Run the web server configured with binaries that\nhave a non-executable stack, as follows.\nhttpd@vm-6858:~/lab$ ./clean-env.sh ./zookld zook-nxstack.conf\nThe key observation to exploiting buffer overflows with a non-executable stack is that you still control the program counter, after a\nRET instruction jumps to an address that you placed on the stack. Even though you cannot jump to the address of the overflowed\nbuffer (it will not be executable), there's usually enough code in the vulnerable server's address space to perform the operation you\nwant.\nThus, to bypass a non-executable stack, you need to first find the code you want to execute. This is often a function in the standard\nlibrary, called libc, such as execl, system, or unlink. Then, you need to arrange for the stack to look like a call to that function with\nthe desired arguments, such as system(\"/bin/sh\"). Finally, you need to arrange for the RET instruction to jump to the function you\nfound in the first step. This attack is often called a return-to-libc attack. This article contains a more detailed description of this style\nof attack.\nIn the next exercise, you will need to understand the calling convention for C functions. For your reference, consider the following\nsimple C program:\nvoid\nfoo(int x, char *msg, int y)\n{\n/* ... */\n}\nvoid\nbar(void)\n{\nint a = 3;\nfoo(5, \"Hello, world!\", 7);\n}\nThe stack layout when bar invokes foo, just after the program counter has switched to the beginning of foo, looks like this:\n+------------------+\n%ebp ------> | saved %ebp | (4 bytes)\n+------------------+\n| ... |\n+------------------+\nbar's a ------> | 3 | (4 bytes)\n+------------------+\n| ... |\n+------------------+\n| 7 | (4 bytes)\n+------------------+\n| pointer to | ------> \"Hello, world!\", somewhere in memory\n| string | (4 bytes)\n+------------------+\n| 5 | (4 bytes)\n+------------------+\n\n| return address | (4 bytes)\n%esp ------> | into bar |\n+------------------+\n| |\nWhen foo starts running, the first thing it will do is save the %ebp register on the stack, and set the %ebp register to point at this saved\nvalue on the stack, so the stack frame will look like the one shown just above Exercise 3.\nExercise 4. Starting from your two exploits in Exercise 2, construct two exploits that take advantage of those\nvulnerabilities to unlink /home/httpd/grades.txt when run on the binaries that have a non-executable stack. Name\nthese new exploits exploit-4a.py and exploit-4b.py.\nAlthough in principle you could use shellcode that's not located on the stack, for this exercise you should not inject any\nshellcode into the vulnerable process. You should use a return-to-libc (or at least a call-to-libc) attack where you vector\ncontrol flow directly into code that existed before your attack.\nIn answers.txt, explain whether or not the other buffer overflow vulnerabilities you found in Exercise 1 can be\nexploited in this same manner.\nYou can test your exploits as follows:\nhttpd@vm-6858:~/lab$ make check-libc\nThe test either prints two \"PASS\" messages or fails. We will grade your exploits in this way. If you use other names for the exploit\nscripts, change Makefile accordingly.\nPart 4: Fixing buffer overflows and other bugs\nNow that you have figured out how to exploit buffer overflows, you will try to find other kinds of vulnerabilities in the same code.\nAs with many real-world applications, the \"security\" of our web server is not well-defined. Thus, you will need to use your\nimagination to think of a plausible threat model and policy for the web server.\nExercise 5. Look through the source code and try to find more vulnerabilities that can allow an attacker to compromise\nthe security of the web server. Describe the attacks you have found in answers.txt, along with an explanation of the\nlimitations of the attack, what an attacker can accomplish, why it works, and how you might go about fixing or\npreventing it. You should ignore bugs in zoobar's code. They will be addressed in future labs.\nOne approach for finding vulnerabilities is to trace the flow of inputs controlled by the attacker through the server code.\nAt each point that the attacker's input is used, consider all the possible values the attacker might have provided at that\npoint, and what the attacker can achieve in that manner.\nYou should find at least two vulnerabilities for this exercise.\nFinally, you will explore fixing some of the vulnerabilities you have found in this lab assignment.\nExercise 6. For each buffer overflow vulnerability you have found in Exercise 1, fix the web server's code to prevent\nthe vulnerability in the first place. Do not rely on compile-time or runtime mechanisms such as stack canaries,\nremoving -fno-stack-protector, baggy bounds checking, etc.\nYou are done! Submit your answers to the lab assignment by running make submit. Alternatively, run make prepare-submit. The\nresulting lab1-handin.tar.gz file will be graded.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Lab 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/aba3bf6028f30e9d40d0df6b742e8baa_MIT6_858F14_lab2.pdf",
      "content": "6.858 Fall 2014 Lab 2: Privilege separation and server-\nside sandboxing\nHanded out:\nLecture 5\nPart 1 due:\nTwo days after Lecture 7 (5:00pm)\nParts 2 and 3 due: Two days after Lecture 9 (5:00pm)\nAll parts due:\nTwo days after Lecture 10 (5:00pm)\nIntroduction\nThis lab will introduce you to privilege separation and server-side sandboxing, in the context of a simple python web\napplication called zoobar, where users transfer \"zoobars\" (credits) between each other. The main goal of privilege\nseparation is to ensure that if an adversary compromises one part of an application, the adversary doesn't compromise the\nother parts too. To help you privilege-separate this application, the zookws web server used in the previous lab is a clone\nof the OKWS web server, discussed in lecture. In this lab, you will set up a privilege-separated web server, examine\npossible vulnerabilities, and break up the application code into less-privileged components to minimize the effects of any\nsingle vulnerability.\nYou will also extend the Zoobar web application to support executable profiles, which allow users to use Python code as\ntheir profiles. To make a profile, a user saves a Python program in their profile on their Zoobar home page. (To indicate\nthat the profile contains Python code, the first line must be #!python.) Whenever another user views the user's Python\nprofile, the server will execute the Python code in that user's profile to generate the resulting profile output. This will\nallow users to implement a variety of features in their profiles, such as:\nA profile that greets visitors by their user name.\nA profile that keeps track of the last several visitors to that profile.\nA profile that gives a zoobar to every visitor (limit 1 per minute).\nSupporting this safely requires sandboxing the profile code on the server, so that it cannot perform arbitrary operations\nor access arbitrary files. On the other hand, this code may need to keep track of persistent data in some files, or to access\nexisting zoobar databases, to function properly. You will use the RPC library and some shim code that we provide to\nsecurely sandbox executable profiles.\nTo fetch the new source code, use Git to commit your Lab 1 solutions, and merge them into our lab2 branch. For those\noperating with the provided .zip files, please download lab2.zip from the Labs section on MIT OpenCourseWare.\nhttpd@vm-6858:~$ cd lab\nhttpd@vm-6858:~/lab$ git status\n...\nhttpd@vm-6858:~/lab$ git add bugs.txt exploit-*.py [and any other new files...]\nhttpd@vm-6858:~/lab$ git commit -am 'my solution to lab1'\n[lab1 c54dd4d] my solution to lab1\n1 files changed, 1 insertions(+), 0 deletions(-)\nhttpd@vm-6858:~/lab$ git pull\n...\nhttpd@vm-6858:~/lab$ git checkout -b lab2 origin/lab2\nBranch lab2 set up to track remote branch lab2 from origin.\nSwitched to a new branch 'lab2'\nhttpd@vm-6858:~/lab$ git merge lab1\nMerge made by recursive.\n...\nhttpd@vm-6858:~/lab$\nIn some cases, Git may not be able to figure out how to merge your changes with the new lab assignment (e.g. if you\nmodified some of the code that is changed in the second lab assignment). In that case, the git merge command will tell\nyou which files are conflicted, and you should first resolve the conflict (by editing the relevant files) and then commit the\nresulting files with git commit -a.\nYou'll then need to patch flask in order to get it to work with the lab:\n\nhttpd@vm-6858:~/lab$ sudo make fix-flask\n[sudo] password for httpd: 6858\n./fix-flask.sh\npatching file /usr/lib/python2.7/dist-packages/werkzeug/routing.py\nDone\nnce your source code is in place, make sure that you can compile and install the web server and the zoobar application:\nhttpd@vm-6858:~/lab$ make\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookld.o zookld.c\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o http.o http.c\ncc -m32 zookld.o http.o -lcrypto -o zookld\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookfs.o zookfs.c\ncc -m32 zookfs.o http.o -lcrypto -o zookfs\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookd.o zookd.c\ncc -m32 zookd.o http.o -lcrypto -o zookd\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zooksvc.o zooksvc.c\ncc -m32 zooksvc.o -lcrypto -o zooksvc\nhttpd@vm-6858:~/lab$ sudo make setup\n[sudo] password for httpd: 6858\n./chroot-setup.sh\n+ grep -qv uid=0\n+ id\n...\n+ python /jail/zoobar/zoodb.py init-person\n+ python /jail/zoobar/zoodb.py init-transfer\nhttpd@vm-6858:~/lab$\nrelude: What's a zoobar?\no understand the zoobar application itself, we will first examine the zoobar web application code.\nne of the key features of the zoobar application is the ability to transfer credits between users. This feature is\nmplemented by the script transfer.py.\no get a sense what transfer does, start the zoobar Web site:\nhttpd@vm-6858:~/lab$ sudo make setup\n[sudo] password for httpd: 6858\n./chroot-setup.sh\n+ grep -qv uid=0\n+ id\n...\n+ python /jail/zoobar/zoodb.py init-person\n+ python /jail/zoobar/zoodb.py init-transfer\nhttpd@vm-6858:~/lab$ sudo ./zookld zook.conf\nzookld: Listening on port 8080\nzookld: Launching zookd\n...\now, make sure you can run the web server, and access the web site from your browser, as follows:\nhttpd@vm-6858:~/lab$ /sbin/ifconfig eth0\neth0 Link encap:Ethernet HWaddr 00:0c:29:57:90:a1\n\ninet addr:172.16.91.143 Bcast:172.16.91.255 Mask:255.255.255.0\n\ninet6 addr: fe80::20c:29ff:fe57:90a1/64 Scope:Link\n\nUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\n\nRX packets:149 errors:0 dropped:0 overruns:0 frame:0\n\nTX packets:94 errors:0 dropped:0 overruns:0 carrier:0\n\ncollisions:0 txqueuelen:1000\n\nRX bytes:15235 (15.2 KB) TX bytes:12801 (12.8 KB)\n\nInterrupt:19 Base address:0x2000\nn this particular example, you would want to open your browser and go to\nttp://172.16.91.143:8080/zoobar/index.cgi/, or, if you are using KVM, to\nttp://localhost:8080/zoobar/index.cgi/. You should see the zoobar web site.\nO\nP\nT\nO\ni\nT\nN\nI\nh\nh\n\nExercise 1. In your browser, connect to the zoobar Web site, and create two user accounts. Login in as one\nof the users, and transfer zoobars from one user to another by clicking on the transfer link and filling out the\nform. Play around with the other features too to get a feel for what it allows users to do. In short, a registered\nuser can update his/her profile, transfer \"zoobars\" (credits) to another user, and look up the zoobar balance,\nprofile, and transactions of other users in the system.\nRead through the code of zoobar and see how transfer.py gets invoked when a user sends a transfer on\nthe transfer page. A good place to start for this part of the lab is templates/transfer.html, __init__.py,\ntransfer.py, and bank.py\nNote: You don't need to turn in anything for this exercise, but make sure that you understand the structure of\nthe zoobar application--it will save you time in the future!\nPrivilege separation\nHaving surveyed the zoobar application code, it is worth starting to think about how to apply privilege separation to the\nzookws and zoobar infrastructure so that bugs in the infrastructure don't allow an adversary, for example, to transfer\nzoobars to the adversary account.\nThe web server for this lab uses the /jail directory to setup chroot jails for different parts of the web server, much as in\nthe OKWS paper. The make command compiles the web server, and make setup installs it with all the necessary\npermissions in the /jail directory.\nAs part of this lab, you will need to change how the files and directories are installed, such as changing their owner or\npermissions. To do this, you should not change the permissions directly. Instead, you should edit the chroot-setup.sh\nscript in the lab directory, and re-run sudo make setup. If you change the permissions in a different script, your server\nmight not work.\nTwo aspects make privilege separation challenging in the real world and in this lab. First, privilege separation requires\nthat you take apart the application and split it up in separate pieces. Although we have tried to structure the application\nwell so that it is easy to split, there are places where you must redesign certain parts to make privilege separation\npossible. Second, you must ensure that each piece runs with minimal privileges, which requires setting permissions\nprecisely and configuring the pieces correctly. Hopefully, by the end of this lab, you'll have a better understanding of\nwhy many applications have security vulnerabilities related to failure to properly separate privileges: proper privilege\nseparation is hard!\nOne problem that you might run into is that it's tricky to debug a complex application that's composed of many pieces.\nTo help you, we have provided a simple debug library in debug.py, which is imported by every Python script we give\nyou. The debug library provides a single function, log(msg), which prints the message msg to stderr (which should go to\nthe terminal where you ran zookld), along with a stack trace of where the log function was called from.\nIf something doesn't seem to be working, try to figure out what went wrong before proceeding further.\nPart 1: Privilege-separate the web server setup using Unix principals and\npermissions\nAs introduced in Lab 1, the zookws web server is modeled after OKWS from lecture 4. Similar to OKWS, zookws\nconsists of a launcher daemon zookld that launches services configured in the file zook.conf, a dispatcher zookd that\nroutes requests to corresponding services, as well as several services. For simplicity zookws does not implement helper\nor logger daemon as OKWS does.\nThe file zook.conf is the configuration file that specifies how each service should run. For example, the zookd entry:\n[zookd]\ncmd = zookd\nuid = 0\ngid = 0\ndir = /jail\n\nspecifies that the command to run zookd is zookd, that it runs with user and group ID 0 (which is the superuser root), in\nthe jail directory /jail.\nThe zook.conf file configures only one HTTP service, zookfs_svc, that serves static files and executes dynamic scripts.\nThe zookfs_svc does so by invoking the executable zookfs, which should be jailed in the directory /jail by chroot.\nYou can look into /jail; it contains executables (except for zookld), supporting libraries, and the zoobar web site. See\nzook.conf and zookfs.c for details.\nThe launcher daemon zookld, which reads zook.conf and sets up all services is running under root and can bind to a\nprivileged port like 80. Note that in the default configuration, zookd and vulnerable services are inappropriately running\nunder root and that zookld doesn't jail processes yet; an attacker can exploit buffer overflows and cause damages to the\nserver, e.g., unlink a specific file as you have done in Lab 1.\nTo fix the problem, you should run these services under unprivileged users rather than root. You will modify zookld.c\nand zook.conf to set up user IDs, group IDs, and chroot for each service. This will proceed in a few steps: first you will\nmodify zookld.c to support chroot, second you will modify zookld.c to support user and group IDs other than root,\nand finally you will modify zook.conf to use this support.\nExercise 2. Modify the function launch_svc in zookld.c so that it jails the process being created.\nlaunch_svc creates a new process for each entry in zook.conf, and then configures that process as\nspecified in zook.conf. Your job is to insert the call to chroot in the specified place. You want to run man\n2 chroot to read the manual page about chroot. If you do this correctly, services won't be able to read files\noutside of the directory specified. For example, zookd shouldn't be able to read the real /etc/passwd\nanymore.\nRun sudo make check to verify that your modified configuration passes our basic tests in check_lab2.py,\nbut keep in mind that our tests are not exhaustive. You probably want to read over the cases before you start\nimplementing.\nExercise 3. Modify the function launch_svc in zookld.c so that it sets the user and group IDs and the\nsupplementary group list specified in zook.conf. For example, you want to set in zook.conf the uid in\nzookd's entry to, say, 61011, and have zookld.c change the user ID to 61011. You should do the same for\ngroup IDs. You will need to use the system calls setresuid, setresgid, and setgroups.\nThink carefully about when your code can set the user ID. For example, can it go before setegid or\nchroot?\nThis will also require you to modify chroot-setup.sh to ensure that the files on disk, such as the database,\ncan be read only by the processes that should be able to read them. You can either use the built-in chmod and\nchown commands or our provided set_perms function, which you can invoke like so:\nset_perms 1234:5678 755 /path/to/file\nwhich will set the owner of the file to 1234, the group to 5678, and the permissions to 755 (i.e., user\nread/write/execute, group read/execute, other read/execute).\nRun sudo make check to verify that your modified configuration passes our basic tests.\nNow that none of the services are running as root, we will try to further privilege-separate the zookfs_svc service that\nhandles both static files and dynamic scripts. Although it runs under an unprivileged user, some Python scripts could\neasily have security holes; a vulnerable Python script could be tricked into deleting important static files that the server is\nserving. Conversely, if the static service is compromised, it might read the databases used by the Python scripts, such as\nperson.db and transfer.db. A better organization is to split zookfs_svc into two services, one for static files and the\nother for Python scripts, running under different users.\nExercise 4. Create two new HTTP services (replacing zookfs_svc), along the lines of the existing\nzookfs_svc service, such that one will execute dynamic content, and one will serve static files. Modify the\n\nconfiguration file zook.conf to split the zookfs_svc service into two services running under different users:\nthe static_svc service that only serves static files, and the dynamic_svc service that only executes the\nintended Python scripts (i.e., /zoobar/index.cgi).\nSet file and directory permissions (using chroot-setup.sh) to ensure that the static service cannot read the\ndatabase files from the dynamic service, that the dynamic service cannot modify static files, and that the\ndynamic service cannot be tricked into executing other scripts, such as the various .py programs under\nzoobar/.\nThis separation requires zookd to determine which service should handle a particular request. You may use\nzookws's URL filtering to do this, without modifying the application or the URLs that it uses. The URL\nfilters are specified in zook.conf, and support regular expressions. For example, url = .* matches all\nrequests, while url = /zoobar/(abc|def)\\.html matches requests to /zoobar/abc.html and\n/zoobar/def.html.\nDo not rely on URL filters for security; it is exceedingly difficult to do so correctly. For example, even if\nyou configure the filter to pass only URLs matching .cgi to the dynamic service, an adversary can still\ninvoke a hypothetical buggy /zoobar/foo.py script by issuing a request for /zoobar/foo.py/xx.cgi.\nThere are many executable files on the filesystem, and we can not mark all of them non-executable for the\nzookfs service. For example, the zookfs service needs to execute /jail/usr/bin/python to run the zoobar\nwebsite. We have added a feature to zookfs to only run trusted executables marked by a particular\ncombination of owner user and group. To use this function, add an args = UID GID line to the service's\nconfiguration. For example, the following zook.conf entry:\n[safe_svc]\ncmd = zookfs\nuid = 0\ngid = 0\ndir = /jail\nargs = 123 456\nspecifies that safe_svc will only execute files owned by user ID 123 and group ID 456.\nYou should only be modifying configurations and permissions for this exercise in order to enforce privilege\nseparation.\nRun sudo make check to verify that your modified configuration passes our tests.\nSubmit your answers to the first part of this lab assignment by running make submit-a. Alternatively, run\nmake prepare-submit-a. The resulting lab2a-handin.tar.gz file will be graded.\nInterlude: RPC library\nIn this part, you will privilege-separate the zoobar application itself in several processes. We would like to make sure we\ncan deal with any future such bugs that come up. That is, if one piece of the zoobar application has an exploitable bug,\nthen an attacker cannot use that bug to exploit other parts of the zoobar application. A challenge in spitting the zoobar\napplication in several processes running with their own privileges is that the different processes must interact and have a\nway to communicate. You will first study a Remote Procedure Call (RPC) library that allows processes to communicate\nover a Unix socket. Then, you will use that library to separate the zoobar in several processes that communicate using\nRPC.\nThe RPC library itself shouldn't have any exploitable bugs, but historically parsing of messages has been a problem. We\nprovide you with a functional RPC library, but think carefully about ways that an attacker could leverage the (typically\ncomplex) parsing code latent in most RPC libraries.\nTo illustrate how our RPC library might be used, we have implemented a simple \"echo\" service for you, in\nzoobar/echo-server.py. This service is invoked by zookld; look for the echo_svc section of zook.conf to see how it\nis started.\necho-server.py is implemented by defining an RPC class EchoRpcServer that inherits from RpcServer, which in turn\ncomes from zoobar/rpclib.py. The EchoRpcServer RPC class defines the methods that the server supports, and\n\nrpclib invokes those methods when a client sends a request. The server defines a simple method that echos the request\nfrom a client.\necho-server.py starts the server by calling run_sockpath_fork(sockpath). This function listens on a UNIX-domain\nsocket. The socket name comes from the argument, which in this case is /echosvc/sock (specified in zook.conf). When\na client connects to this socket, the function forks the current process. One copy of the process receives messages and\nresponds on the just-opened connection, while the other process listens for other clients that might open the socket.\nWe have also included a simple client of this echo service as part of the Zoobar web application. In particular, if you go\nto the URL /zoobar/index.cgi/echo?s=hello, the request is routed to zoobar/echo.py. That code uses the RPC\nclient (implemented by rpclib) to connect to the echo service at /echosvc/sock and invoke the echo operation. Once it\nreceives the response from the echo service, it returns a web page containing the echoed response.\nThe RPC client-side code in rpclib is implemented by the call method of the RpcClient class. This methods formats\nthe arguments into a string, writes the string on the connection to the server, and waits for a response (a string). On\nreceiving the response, call parses the string, and returns the results to the caller.\nPart 2: Privilege-separating the login service in Zoobar\nWe will now use the RPC library to improve the security of the user passwords stored in the Zoobar web application.\nRight now, an adversary that exploits a vulnerability in any part of the Zoobar application can obtain all user passwords\nfrom the person database.\nThe first step towards protecting passwords will be to create a service that deals with user passwords and cookies, so that\nonly that service can access them directly, and the rest of the Zoobar application cannot. In particular, we want to\nseparate the code that deals with user authentication (i.e., passwords and tokens) from the rest of the application code.\nThe current zoobar application stores everything about the user (their profile, their zoobar balance, and authentication\ninfo) in the Person table (see zoodb.py). We want to move the authentication info out of the Person table into a separate\nCred table (Cred stands for Credentials), and move the code that accesses this authentication information (i.e., auth.py)\ninto a separate service.\nThe reason for splitting the tables is that the tables are stored in the file system in zoobar/db/, and are accessible to all\nPython code in Zoobar. This means that an attacker might be able to access and modify any of these tables, and we might\nnever find out about the attack. However, once the authentication data is split out into its own database, we can set Unix\nfile and directory permissions such that only the authentication service---and not the rest of Zoobar---can access that\ninformation.\nSpecifically, your job will be as follows:\nDecide what interface your authentication service should provide (i.e., what functions it will run for clients). Look\nat the code in login.py and auth.py, and decide what needs to run in the authentication service, and what can run\nin the client (i.e., be part of the rest of the zoobar code). Keep in mind that your goal is to protect both passwords\nand tokens. We have provided initial RPC stubs for the client in the file zoobar/auth_client.py.\nCreate a new auth_svc service for user authentication, along the lines of echo-server.py. We have provided an\ninitial file for you, zoobar/auth-server.py, which you should modify for this purpose. The implementation of\nthis service should use the existing functions in auth.py.\nModify zook.conf to start the auth-server appropriately (under a different UID).\nSplit the user credentials (i.e., passwords and tokens) from the Person database into a separate Cred database,\nstored in /zoobar/db/cred. Don't keep any passwords or tokens in the old Person database.\nModify chroot-setup.sh to set permissions on the cred database appropriately, and to create the socket for the\nauth service.\nModify the login code in login.py to invoke your auth service instead of calling auth.py directly.\nExercise 5. Implement privilege separation for user authentication, as described above.\nDon't forget to create a regular Person database entry for newly registered users.\nRun sudo make check to verify that your privilege-separated authentication service passes our tests.\nNow, we will further improve the security of passwords, by using hashing and salting. The current authentication code\n\nstores an exact copy of the user's password in the database. Thus, if an adversary somehow gains access to the cred.db\nfile, all of the user passwords will be immediately compromised. Worse yet, if users have the same password on multiple\nsites, the adversary will be able to compromise users' accounts there too!\nHashing protects against this attack, by storing a hash of the user's password (i.e., the result of applying a hash function\nto the password), instead of the password itself. If the hash function is difficult to invert (i.e., is a cryptographically\nsecure hashes), an adversary will not be able to directly obtain the user's password. However, a server can still check if a\nuser supplied the correct password during login: it will just hash the user's password, and check if the resulting hash\nvalue is the same as was previously stored.\nOne weakness with hashing is that an adversary can build up a giant table (called a \"rainbow table\"), containing the\nhashes of all possible passwords. Then, if an adversary obtains someone's hashed password, the adversary can just look it\nup in its giant table, and obtain the original password.\nTo defeat the rainbow table attack, most systems use salting. With salting, instead of storing a hash of the password, the\nserver stores a hash of the password concatenated with a randomly-generated string (called a salt). To check if the\npassword is correct, the server concatenates the user-supplied password with the salt, and checks if the result matches the\nstored hash. Note that, to make this work, the server must store the salt value used to originally compute the salted hash!\nHowever, because of the salt, the adversary would now have to generate a separate rainbow table for every possible salt\nvalue. This greatly increases the amount of work the adversary has to perform in order to guess user passwords based on\nthe hashes.\nA final consideration is the choice of hash function. Most hash functions, such as MD5 and SHA1, are designed to be\nfast. This means that an adversary can try lots of passwords in a short period of time, which is not what we want!\nInstead, you should use a special hash-like function that is explicitly designed to be slow. A good example of such a hash\nfunction is PBKDF2, which stands for Password-Based Key Derivation Function (version 2).\nExercise 6. Implement password hashing and salting in your authentication service. In particular, you will\nneed to extend your Cred table to include a salt column; modify the registration code to choose a random\nsalt, and to store a hash of the password together with the salt, instead of the password itself; and modify the\nlogin code to hash the supplied password together with the stored salt, and compare it with the stored hash.\nYou can store the hashed password in the existing password column you have in the Cred table.\nTo implement PBKDF2 hashing, you can use the Python PBKDF2 module. Roughly, you should import\npbkdf2, and then hash a password using pbkdf2.PBKDF2(password, salt).hexread(32). We have\nprovided a copy of pbkdf2.py in the zoobar directory. Do not use the random.random function to generate a\nsalt as the documentation of the random module states that it is not cryptographically secure. A secure\nalternative is the function os.urandom.\nRun sudo make check to verify that your hashing and salting code passes our tests. Keep in mind that our\ntests are not exhaustive.\nA surprising side-effect of using a very computationally expensive hash function like PBKDF2 is that an adversary can\nnow use this to launch denial-of-service (DoS) attacks on the server's CPU. For example, the popular Django web\nframework recently posted a security advisory about this, pointing out that if an adversary tries to log in to some account\nby supplying a very large password (1MB in size), the server would spend an entire minute trying to compute PBKDF2\non that password. Django's solution is to limit supplied passwords to at most 4KB in size. For this lab, we do not require\nyou to handle such DoS attacks.\nChallenge 1! (optional) For extra credit, implement the honeywords proposal from Ari Juels and Ron Rivest\nin your authentication service. Consider implementing the honeychecker as a separate service running with\nits own user ID. If you decide to complete this challenge, please include a file named honeywords.txt in the\ntop level of your lab submission that gives a brief overview of your approach and solution.\nPart 3: Privilege-separating the bank in Zoobar\nFinally, we want to protect the zoobar balance of each user from adversaries that might exploit some bug in the Zoobar\n\napplication. Currently, if an adversary exploits a bug in the main Zoobar application, they can steal anyone else's\nzoobars, and this would not even show up in the Transfer database if we wanted to audit things later.\nTo improve the security of zoobar balances, our plan is similar to what you did above in the authentication service: split\nthe zoobar balance information into a separate Bank database, and set up a bank_svc service, whose job it is to perform\noperations on the new Bank database and the existing Transfer database. As long as only the bank_svc service can\nmodify the Bank and Transfer databases, bugs in the rest of the Zoobar application should not give an adversary the\nability to modify zoobar balances, and will ensure that all transfers are correctly logged for future audits.\nExercise 7. Privilege-separate the bank logic into a separate bank_svc service, along the lines of the\nauthentication service. Your service should implement the transfer and balance functions, which are\ncurrently implemented by bank.py and called from several places in the rest of the application code.\nYou will need to split the zoobar balance information into a separate Bank database (in zoodb.py);\nimplement the bank server by modifying bank-server.py; add the bank service to zook.conf; modify\nchroot-setup.sh to create the new Bank database and the socket for the bank service, and to set\npermissions on both the new Bank and the existing Transfer databases accordingly; create client RPC stubs\nfor invoking the bank service; and modify the rest of the application code to invoke the RPC stubs instead of\ncalling bank.py's functions directly.\nDon't forget to handle the case of account creation, when the new user needs to get an initial 10 zoobars.\nThis may require you to change the interface of the bank service.\nRun sudo make check to verify that your privilege-separated bank service passes our tests.\nFinally, we need to fix one more problem with the bank service. In particular, an adversary that can access the transfer\nservice (i.e., can send it RPC requests) can perform transfers from anyone's account to their own. For example, it can\nsteal 1 zoobar from any victim simply by issuing a transfer(victim, adversary, 1) RPC request. The problem is\nthat the bank service has no idea who is invoking the transfer operation. Some RPC libraries provide authentication,\nbut our RPC library is quite simple, so we have to add it explicitly.\nTo authenticate the caller of the transfer operation, we will require the caller to supply an extra token argument, which\nshould be a valid token for the sender. The bank service should reject transfers if the token is invalid.\nExercise 8. Add authentication to the transfer RPC in the bank service. The current user's token is\naccessible as g.user.token. How should the bank validate the supplied token?\nAlthough make check does not include an explicit test for this exercise, you should be able to check whether\nthis feature is working or not by manually connecting to your transfer service and verifying that it is not\npossible to perform a transfer without supplying a valid token.\nSubmit your answers to parts 2 and 3 of this lab assignment by running make submit-b. Alternatively, run\nmake prepare-submit-b. The resulting lab2b-handin.tar.gz file will be graded.\nPart 4: Server-side sandboxing for executable profiles\nYou should familiarize yourself with the following new components of the lab source:\nFirst, the profiles/ directory contains several executable profiles, which you will use as examples throughout\nthis lab:\nprofiles/hello-user.py is a simple profile that prints back the name of the visitor when the profile code\nis executed, along with the current time.\nprofiles/visit-tracker.py keeps track of the last time that each visitor looked at the profile, and prints\nout the last visit time (if any).\nprofiles/last-visits.py records the last three visitors to the profile, and prints them out.\nprofiles/xfer-tracker.py prints out the last zoobar transfer between the profile owner and the visitor.\nprofiles/granter.py gives the visitor one zoobar. To make sure visitors can't quickly steal all zoobars\n\nfrom a user, this profile grants a zoobar only if the profile owner has some zoobars left, the visitor has less\nthan 20 zoobars, and it has been at least a minute since the last time that visitor got a zoobar from this\nprofile.\nSecond, zoobar/sandboxlib.py is a Python module that implements sandboxing for untrusted Python profile\ncode; see the Sandbox class, and the run() method which executes a specified function in the sandbox. The run\nmethod works by forking off a separate process and calling setresuid in the child process before executing the\nuntrusted code, so that the untrusted code does not have any privileges. The parent process reads the output from\nthe child process (i.e., the untrusted code) and returns this output to the caller of run(). If the child doesn't exit\nafter a short timeout (5 seconds by default), the parent process kills the child.\nSandbox.run() also uses chroot to restrict the untrusted code to a specific directory, passed as an argument to the\nSandbox constructor. This allows the untrusted profile code to perform some limited file system access, but the\ncreator of Sandbox gets to decide what directory is accessible to the profile code.\nSandbox uses just one user ID for running untrusted profiles. This means that it's important that at most one profile\nbe executing in the sandbox at a time. Otherwise, one sandboxed process could tamper with another sandboxed\nprocess, since they both have the same user ID! To enforce this guarantee, Sandbox uses a lockfile; whenever it\ntries to run a sandbox, it first locks the lockfile, and releases it only after the sandboxed process has exited. If two\nprocesses try to run some sandboxed code at the same time, only one will get the lockfile at a time. It's important\nthat all users of Sandbox specify the same lockfile name if they use the same UID.\nHow does Sandbox know that some sandboxed code has fully exited and it's safe to reuse the user ID to run a\ndifferent user's profile? After all, the untrusted code could have forked off another process, and is waiting for some\nother profile to start running with the same user ID. To prevent this, Sandbox uses Unix's resource limits: it uses\nsetrlimit to limit the number of processes with a given user ID, so that the sandboxed code simply cannot fork.\nThis means that, after the parent process kills the child process (or notices that it has exited), it can safely conclude\nthere are no remaining processes with that user ID.\nThe final piece of code is zoobar/profile-server.py: an RPC server that accepts requests to run some user's\nprofile code, and returns the output from executing that code.\nThis server uses sandboxlib.py to create a Sandbox and execute the profile code in it (via the run_profile\nfunction). profile-server.py also sets up an RPC server that allows the profile code to get access to things\noutside of the sandbox, such as the zoobar balances of different users. The ProfileAPIServer implements this\ninterface; profile-server.py forks off a separate process to run the ProfileAPIServer, and also passes an RPC\nclient connected to this server to the sandboxed profile code.\nBecause profile-server.py uses sandboxlib.py, which it turn needs to call setresuid to sandbox some\nprocess, the main profile-server.py process needs to run as root. As an aside, this is a somewhat ironic\nlimitation of Unix mechanisms: if you want to improve your security by running untrusted code with a different\nuser ID, you are forced to run some part of your code as root, which is a dangerous thing to do from a security\nperspective.\nPython profiles with privilege separation\nTo get started, you will need to add profile-server.py to your zook.conf and modify chroot-setup.sh to create a\ndirectory for its socket, /jail/profilesvc. Remember that profile-server.py needs to run as root, so put 0 for the\nuid in its zook.conf entry.\nExercise 9. Add profile-server.py to your web server. Change the uid value in\nProfileServer.rpc_run() from 0 to some other value compatible with your design from lab 2.\nMake sure that your Zoobar site can support all of the five profiles. Depending on how you implemented\nprivilege separation in lab 2, you may need to adjust how ProfileAPIServer implements rpc_get_xfers or\nrpc_xfer.\nRun sudo make check to verify that your modified configuration passes our tests. The test case (see\ncheck_lab2_part4.py) creates some user accounts, stores one of the Python profiles in the profile of one\nuser, has another user view that profile, and checks that the other user sees the right output.\nIf you run into problems from the make check tests, you can always check /tmp/html.out for the output\n\nhtml of the profiles. Similarly, you can also check the output of the server in /tmp/zookld.out. If there is\nan error in the server, they will usually display there.\nThe next problem we need to solve is that some of the user profiles store data in files; for example, see last-visits.py\nand visit-tracker.py. However, all of the user profiles currently run with access to the same files, because\nProfileServer.rpc_run() sets userdir to /tmp and passes that as the directory to Sandbox (which it turn chroots the\nprofile code to that directory). As a result, one user's profile can corrupt the files stored by another user's profile.\nExercise 10. Modify rpc_run in profile-server.py so that each user's profile has access to its own files,\nand cannot tamper with the files of other user profiles.\nRemember to consider the possibility of usernames with special characters. Also be sure to protect all of\nthese files from other services on the same machine (such as the zookfs that serves static files).\nRun make check to see whether your implementation passes our test cases.\nFinally, recall that all of profile-server.py currently runs as root because it needs to create a sandbox. This is\ndangerous, and we would like to reduce the amount of code in profile-server.py that runs as root. In particular, the\nProfileAPIServer that runs as part of profile-server.py does not strictly need to run as root (it does not invoke the\nsandbox), and in fact, it might be the most vulnerable part of the code to attacks, because it accepts RPC commands from\nthe untrusted profile code!\nExercise 11. Change ProfileAPIServer in profile-server.py to avoid running as root. Recall that\nprofile-server.py forks off a separate child process to run ProfileAPIServer, so you can switch to a\ndifferent user ID (and group ID, if necessary) in ProfileAPIServer.__init__.\nYou will need to make sure that rpc_xfer can still perform transfers from the profile owner's account. It\nmay be helpful to obtain the correct token before giving up root privileges.\nAs before, use make check to ensure your code passes our tests.\nYou are now done with the basic sandbox.\nChallenge 2! (optional) Think of some interesting features that you could implement using Python server-\nside profiles, possibly in combination with extending the sandboxing infrastructure (e.g., providing an API\nfor sending messages between users, or for sharing files between users). For example, can you build profile\ncode that analyzes the social graph of who visited whose profile, or an equivalent to a Facebook wall, all\nusing untrusted profile code?\nWrite a profile that demonstrates this functionality in profiles/my-profile.py. Describe what your profile\nis implementing in a comment at the top of the profile source code. Make any changes to your\nProfileAPIServer necessary to support your feature.\nChallenge 3! (optional) Now that profiles contain Python code, and can give away the user's zoobars, it's\nimportant that the user's profile code is not modified by an attacker, and only the correct profile code is\nexecuted by profile-server.py.\nCreate an RPC server that is in charge of modifying user profiles, and which requires a valid user token in\norder to modify a user's profile. Change the rest of the Zoobar application code to modify user profiles via\nthis RPC server. Set permissions on the profile database so that the rest of the Zoobar application cannot\nmodify profiles directly. Change profile-server.py to read profile code directly from the profile database,\ninstead of accepting it as input to the run RPC call.\n\nmake check only does a cursory inspection of the person db, so it may be that your solution is correct but\nthe test fails, or that the test succeeds but your solution is wrong. Therefore, if you've completed the\nchallenge and want us to grade it, add an empty file named challenge3.txt to the lab directory so we know\nto take a look at your solution.\nYou are done! Submit your answers to the lab assignment by running make submit. Alternatively, run\nmake prepare-submit. The resulting lab2-handin.tar.gz file will be graded.\nAcknowledgments\nThanks to Stanford's CS155 course staff for the initial zoobar web application code, which we extended in this lab\nassignment.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Lab 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/d5123ae79495b489dc7255376c9842ae_MIT6_858F14_lab3.pdf",
      "content": "6.858 Fall 2014 Lab 3: Symbolic execution\nHanded out:\nLecture 10\nPart 1 due:\nTwo days after Lecture 11 (5:00pm)\nAll parts due: Two days after Lecture 13 (5:00pm)\nIntroduction\nThis lab will introduce you to a powerful technique for finding bugs in software: symbolic execution. This can be a good way to audit your application for security\nvulnerabilities so that you can then fix them. By the end of this lab, you will have a symbolic execution system that can take the Zoobar web application and mechanically\nfind inputs that trigger different kinds of bugs in Zoobar that can lead to security vulnerabilities. (More precisely, this lab will be building a concolic execution system; we\nwill explain what this means later on.)\nThe KLEE paper describes a symbolic execution system for C programs. For simplicity, this lab will focus on building a symbolic/concolic execution system for Python\nprograms, by modifying Python objects and overloading specific methods. Much like KLEE, we will be using an SMT solver to check for satisfiable constraints and come\nup with example inputs to the program we are testing. (As an aside, SMT stands for Satisfiability Modulo Theories, which means the solver is able check constraints that\ninvolve both traditional boolean satisfiability expressions as well as constraints that refer to other \"theories\" like integers, bit vectors, strings, and so on.)\nIn this lab, you will first familiarize yourself with the use of Z3, a popular SMT solver, by using it to find a correct way to compute the unsigned (and signed) average of\ntwo 32-bit values. You will then create wrappers for integer operations in Python (much like KLEE provides replacements for operations on symbolic values), and\nimplement the core logic of invoking Z3 to explore different possible execution paths. Finally, you will explore how to apply this approach to web applications, which\ntend to handle strings rather than integer values. You will wrap operations on Python strings, implement symbolic-friendly wrappers around the SQLalchemy database\ninterface, and use the resulting system to find security vulnerabilities in Zoobar.\nGetting started\nTo fetch the new source code, first use Git to commit your solutions from lab 2, then run git pull to fetch the new code, and then check out the lab3 branch. If you are\nusing the provided .zip files, please download lab3.zip from the Labs section on the MIT OpenCourseWare site.\nhttpd@vm-6858:~$ cd lab\nhttpd@vm-6858:~/lab$ git commit -am 'my solution to lab2'\n[lab1 f54fd4d] my solution to lab2\n1 files changed, 1 insertions(+), 0 deletions(-)\nhttpd@vm-6858:~/lab$ git pull\n...\nhttpd@vm-6858:~/lab$ git checkout -b lab3 origin/lab3\nBranch lab3 set up to track remote branch lab3 from origin.\nSwitched to a new branch 'lab3'\nhttpd@vm-6858:~/lab$\nWARNING: do not merge your lab 2 solutions into the lab 3 source code! Our basic symbolic execution system cannot track symbolic constraints across RPC between\nmultiple processes, so we will focus on using symbolic execution on a non-privilege-separated Zoobar site. The symbolic execution system will also help us find bugs that\nprivilege separation does not fully mitigate.\nNext, make sure that the lab 3 source code is running correctly in your VM, by running make check. As shown below, this command should report that all checks fail, but\nif you get other error messages, stop and figure out what went wrong (perhaps by contacting the course staff or asking on Piazza).\nhttpd@vm-6858:~/lab$ make check\n./check_lab3.py\nFAIL Exercise 1: unsigned average\nFAIL Challenge 1: signed average\nFAIL Exercise 2: concolic multiply\nFAIL Exercise 2: concolic divide\nFAIL Exercise 2: concolic divide+multiply+add\nFAIL Exercise 3: concolic execution for integers\nFAIL Exercise 4: concolic length\nFAIL Exercise 4: concolic contains\nFAIL Exercise 4: concolic execution for strings\nFAIL Exercise 5: concolic database lookup (str)\nFAIL Exercise 5: concolic database lookup (int)\nFAIL Exercise 5: eval injection not found\nFAIL Exercise 6: balance mismatch not found\nFAIL Exercise 6: zoobar theft not found\nPASS Exercise 7: eval injection not found\nPASS Exercise 7: balance mismatch not found\nPASS Exercise 7: zoobar theft not found\nhttpd@vm-6858:~/lab$\nOne common failure is that you have a leftover zoobar/db directory from lab 2, owned by a user other than httpd. If you are getting an exception when running make\ncheck, try removing the zoobar/db directory as root, by running sudo rm -rf zoobar/db.\nNote that parts of the concolic execution system are fairly CPU-intensive. If you are running QEMU without KVM support, you might observe that the last check (for\nexercise 6) can take a very long time (over 5 minutes). Consider enabling KVM or using some other low-overhead VMM (like VMware).\nUsing an SMT solver\nA key piece of machinery used by symbolic execution is an SMT solver. For this lab, you will be using the Z3 solver from Microsoft Research. Since our goal is to look\nfor bugs in Zoobar, which performs a lot of string processing, we will use the Z3-str extension of Z3 which adds support for reasoning about constraints on strings. We\nwill invoke Z3 using its Python-based API; you may find it useful to consult the documentation for Z3's Python API. The lab comes with a pre-built binary of Z3-str; it\nwas built from our modified version of Z3-str.\nAs a first step to learn about Z3, we will use it to help us implement a seemingly simple but error-prone piece of code: computing the average of two 32-bit integers. This\nis surprisingly subtle to do correctly. One naive approach to compute the average of x and y might be to use (x+y)/2. However, if both x and y are large, their sum x+y\nmight overflow and wrap around modulo 232, so (x+y)/2 will not be the correct average value. Infact, integer overflow errors are a significant source of security problems\nfor systems code (check out the KINT paper if you are curious to learn more about that), so it is important to know how to write this code correctly.\n\nZ3 can help us get a correct implementation of the averaging function by checking whether a particular implementation we have in mind is correct. In particular, given a\nboolean expression, Z3 can tell us whether it's possible to make that boolean expression true (i.e., satisfy it). Moreover, if it's possible to make the expression true, and the\nexpression contains some variables, Z3 will give us an example assignment of values to these variables which makes the expression true.\nTo see how we can use Z3, take a look at the code we provided for you in int-avg.py. The first few lines construct two 32-bit variables called a and b. The next line tries\nto compute the unsigned average of a and b (that is, treating both a and b as unsigned integers) and stores it in u_avg. Note that this code does not actually perform the\naddition and division. Instead, it constructs a symbolic expression representing these operations, and Z3 will reason about the possible values of this expression later on.\nYou can observe this just by printing out the u_avg variable:\nhttpd@vm-6858:~/lab$ python\nPython 2.7.6 (default, Mar 22 2014, 22:59:38)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import symex.z3str\n>>> import z3\n>>> a = z3.BitVec('a', 32)\n>>> b = z3.BitVec('b', 32)\n>>> u_avg = z3.UDiv(a + b, 2)\n>>> print u_avg\nUDiv(a + b, 2)\n>>> s_avg = (a + b) / 2\n>>> print s_avg\n(a + b)/2\n>>>\nAs the comment in the source code says, you should beware of the difference between signed and unsigned integer operations. In Z3, the default Python operators for\ndivision (/) and right-shifting (>>) treat bit vectors as signed values. When you want to perform unsigned operations, you should use the z3.UDiv and z3.LShR functions\ninstead.\nThe initial code computes u_avg in the naive way that we discussed above, and is not always correct. Let's now see how Z3 can help us spot this mistake. In the rest of\nint-avg.py, we compute a reference value representing the expected, correct average of a and b. To compute this reference value, we \"cheat\": we actually turn both a and\nb into 33-bit integers (one bit more than before) using z3.ZeroExt. We then compute their average using the naive method, but since we are using 33-bit arithmetic, there is\nno overflow, and the naive method works correctly. We finally truncate the 33-bit value back down to 32 bits, which is also safe to do (because the average will always fit\nin 32 bits), and store the resulting symbolic expression in real_u_avg.\nNow, to check whether u_avg computed the average correctly, we just ask Z3 whether it's possible to satisfy the expression u_avg != real_u_avg, meaning our u_avg\nvalue is not correct. In this case, since our naive 32-bit averaging is broken, the expression is satisfiable, and Z3 tells us so (for now, you can ignore the second part about\nthe signed average):\nhttpd@vm-6858:~/lab$ ./int-avg.py\nChecking unsigned avg using Z3 expression:\nUDiv(a + b, 2) !=\nExtract(31, 0, UDiv(ZeroExt(1, a) + ZeroExt(1, b), 2))\nAnswer for unsigned avg: sat\nExample solution: [b = 2147483616, a = 4261412832]\nChecking signed avg using Z3 expression:\n(a + b)/2 !=\nExtract(31, 0, (SignExt(1, a) + SignExt(1, b))/2)\nAnswer for signed avg: sat\nExample solution: [b = 402641344, a = 1744842304]\nhttpd@vm-6858:~/lab$\nAs you can see, Z3 tells us that the expression is satisfiable, and gives us an example assignment of values to both a and b for which the expression is true. Indeed, these\nexample values look quite large, and their sum clearly exceeds 232, breaking our naive averaging method.\nExercise 1. Implement a correct function to compute the unsigned average of a and b, by modifying the u_avg = ... line in int-avg.py.\nFor the purposes of this exercise, you are not allowed to change the bit-widths of your operands. This is meant to represent the real world, where you\ncannot just add one more bit to your CPU's register width.\nYou may find it helpful to search online for correct ways to perform fixed-width integer arithmetic. The book Hacker's Delight by Henry S. Warren is a\nparticularly good source of such tricks.\nCheck your averaging function by re-running ./int-avg.py or make check. If your implementation is correct, int-avg.py should produce the message\nAnswer for unsigned avg: unsat.\nChallenge! (optional) For extra credit, figure out how to compute the average of two 32-bit signed values. Modify the s_avg = ... line in int-avg.py,\nand run ./int-avg.py or make check to check your answer. Keep in mind the direction of rounding: 3/2=1 and -3/2=-1, so so the average of 1 and 2\nshould be 1, and the average of -2 and -1 should be -1.\nInterlude: what are symbolic and concolic execution?\nAs you probably recall from the KLEE paper, symbolic execution is an approach for testing a program by observing how the program behaves on different possible inputs.\nTypically, the goal of symbolic execution is to achieve high code coverage or path coverage on the program. In the context of security, this is useful because it helps\nexplore rare code paths that might contain vulnerabilities but that aren't being triggered in typical executions of the code. At a high level, if we are building a symbolic\nexecution system, we have to address several points:\n1. As the program constructs intermediate values based on the input (e.g., taking two input integer values, computing their average, and storing that in some variable),\nwe need to remember the relation between the input and these intermediate values. Typically this is done by allowing variables or memory locations to have either\nconcrete or symbolic values. A concrete value is what an ordinary program would store in a variable or memory location: some specific value, such as the integer\n42. A symbolic value is not a specific value but rather a symbolic expression describing what the value would be as a function of the inputs, such as (a+b)/2. This is\nsimilar to the symbolic Z3 expression you constructed for u_avg in the first exercise above.\n2. We need to determine what control flow decisions (branches) the application makes based on the input. This boils down to constructing a symbolic constraint every\ntime the program branches, describing the boolean condition (in terms of the program's original input) under which the program takes some particular branch (or\ndoes not). Since we are keeping track of how all intermediate values are related to the program's original input, using symbolic values, we typically do not need to\nlook back at the original input to make these constraints. These constraints are similar to the constraint you used above to look for bugs in the integer average\n\nfunction. Determining these control flow constraints is important because if the program initially goes one particular way at some branch, we would like to figure\nout how to get it to go down the other way, to see if there are interesting bugs in that other code. In KLEE's case, they build an interpreter for LLVM bytecode, and\nthis interpreter knows the behavior of all branching instructions.\n3. For each of the above branches, we need to decide if there's a possible input that will cause the program to execute the other way at a branch. (More generally, we\noften think of entire control flow paths, rather than individual branches in isolation.) This helps us find control flow decisions in a program that we can affect by\ntweaking the input (as opposed to control flow decisions that will always go a certain way in a program, regardless of the input we are considering). All symbolic\nexecution systems rely on some kind of SMT solver to do this.\n4. We need to specify what we are looking for in our testing. Typically this is best thought of in terms of some invariant that you care about ensuring in your program,\nand symbolic execution looks for inputs that violate this invariant. One thing we could look for is crashes (i.e., the invariant is that our program should never crash).\nLooking for crashes makes a lot of sense in the context of C programs, where crashes often indicate memory corruption which is almost certainly a bug and often\ncould be exploited. In higher-level languages like Python, memory corruption bugs are not a problem by design, but we could still look for other kinds of issues,\nsuch as Python-level code injection attacks (some part of the input gets passed into eval(), for example), or application-specific invariants that matter for security.\n5. Finally, given all of the control flow paths though the program that are possible to execute, we need to decide which path to actually try. This is important because\nthere can be exponentially many different paths as the size of the program gets larger, and it quickly becomes infeasible to try all of them. Thus, symbolic execution\nsystems typically include some kind of scheduler or search strategy that decides which path is the most promising in terms of finding violations of our invariant. A\nsimple example of a search strategy is trying branches that we haven't tried before, in hopes that it will execute new code that we haven't run yet; this will lead to\nhigher code coverage, and perhaps this new code contains a bug we haven't run into yet.\nAn alternative to symbolic execution is fuzzing (also called fuzz-testing). Fuzzing takes a randomized approach: instead of trying to carefully reason about what inputs\nwill trigger different code paths in the application, fuzzing involves constructing concrete random inputs to the program and checking how the program behaves. This has\nthe advantage of being relatively easy, but on the other hand, it can be difficult to construct precise inputs that hit some specific corner case in the application code.\nOne challenge in building a symbolic execution system, such as KLEE, is that your system has to know how to execute all possible operations on symbolic values (steps 1\nand 2 above). In the case of KLEE, which works at the level of LLVM bytecode, this means that KLEE has to understand how every LLVM opcode works. In this lab, we\nare going to interpose at the level of Python objects (in particular, integers and strings). This is challenging for symbolic execution because there are a very large number\nof operations that one can do on these Python objects, so building a complete symbolic execution system for such a high-level interface would be a tedious process.\nLuckily, there is an easier option, called concolic execution, which you can think of as somewhere in the middle between completely random fuzzing and full symbolic\nexecution. The idea is that, instead of keeping track of purely symbolic values (like in KLEE), we can store both a concrete and a symbolic value for variables that are\nderived from the input. (The name concolic is a combination of concrete and symbolic.) Now that we have both a concrete and a symbolic value, we can almost get the\nbest of both worlds:\nIf the application performs some operation that our concolic system knows about, we will run pretty much like symbolic execution (except that we will also\npropagate the concrete part of every value). For instance, suppose we have two concolic integer variables aa and bb, whose concrete values are 5 and 6, and whose\nsymbolic expressions are a and b. If the application stores aa+bb into variable cc, variable cc will now have concrete value 11 and symbolic expression a+b.\nSimilarly, if the application branches on cc==12, the program can execute as if the branch condition was false (since 11 != 12) and record the corresponding\nsymbolic branch condition (a+b != 12).\nIf, on the other hand, the application performs some operation that our concolic system does not know about, the application will just get the concrete value. For\nexample, if the application writes the variable cc to a file, or perhaps passes it to some external library that we don't instrument, the code can still execute, using the\nconcrete value 11 as if the application was just running normally.\nThe benefit of concolic execution, for the purposes of this lab, is that we do not need to be complete in terms of supporting operations on symbolic values. As long as we\nsupport enough operations to find interesting bugs that we care about in the application, the system will be good enough (and in practice, most bug finding systems are\napproximate anyway, since it's usually infeasible to find all bugs). The trade-off is of course that, if the application performs some operations we do not support, we will\nlose track of the symbolic part, and will not be able to do symbolic-execution-style exploration of those paths.\nConcolic execution for integers\nTo start with, you will implement a concolic execution system for integer values. The skeleton code that we provide you for concolic execution is in symex/fuzzy.py in\nyour lab directory. There are several important layers of abstraction that are implemented in fuzzy.py:\nThe AST. Instead of using Z3 expressions to represent symbolic values, as you did in the int-avg.py exercise above, we build our own abstract syntax tree (AST)\nto represent symbolic expressions. An AST node could be a simple variable (represented by a sym_str or sym_int object), a constant value (represented by a\nconst_int, const_str, or const_bool object), or some function or operator that takes other AST nodes as arguments (e.g., sym_eq(a, b) to represent the boolean\nexpression a==b where a and b are AST nodes, or sym_plus(a, b) to represent the integer expression a+b).\nEvery AST node n can be converted into a Z3 expression by calling z3expr(n). This works by calling n._z3expr(), and every AST node implements the _z3expr\nmethod that returns the corresponding Z3 representation.\nThe reason we introduce our own AST layer, instead of using Z3's symbolic representation, is that we need to perform certain manipulations on the AST that are\ndifficult to do with Z3's representation. Furthermore, we need to fork off a separate process to invoke Z3's solver, so that in case the Z3 solver takes a really long\ntime, we can time out, kill that process, and assume the constraint is just unsolvable. (In this case, we might miss those paths, but at least we will make progress\nexploring other paths.) Having our own AST allows us to cleanly isolate Z3 state to just the forked process.\nThe concolic wrappers. To intercept Python-level operations and perform concolic execution, we replace regular Python int and str objects with concolic\nsubclasses: concolic_int inherits from int and concolic_str inherits from str. Each of these concolic wrappers stores a concrete value (in self.__v) and a\nsymbolic expression (an AST node, in self.__sym). When the application computes some expression derived from a concolic value (e.g., a+1 where a is a\nconcolic_int), we need to intercept the operation and return another concolic value containing both the concrete result value and a symbolic expression for how\nthe result was computed.\nTo perform this interception, we overload various methods on the concolic_int and concolic_str classes. For example, concolic_int.__add__ is invoked when\nthe application computes a+1 in the above example, and this method returns a new concolic value representing the result.\nIn principle, we should have a concolic_bool that is a subclass of bool as well. Unfortunately, bool cannot be subclassed in Python (see here and here). So, we\nmake concolic_bool a function that logically pretends that, once you construct a concolic boolean value, the program immediately branches on its value, so\nconcolic_bool also adds a constraint to the current path condition. (The constraint is that the symbolic expression of the boolean value is equal to the concrete\nvalue.) The concolic_bool function then returns a concrete boolean value.\nThe concrete inputs. The input to the function being tested under concolic execution is represented by a Python dictionary, which maps input variable names\n(regular Python strings) to the value of that variable. The value is a regular Python integer (for integer variables) or a regular Python string (for string variables). The\ncurrent input is always stored in the concrete_values dictionary.\nThe reason this is a global variable is that applications create concolic values by invoking fuzzy.mk_str(name) or fuzzy.mk_int(name) to construct a concolic\nstring or integer, respectively. This returns a new concolic value, whose symbolic part is a fresh AST node corresponding to a variable named name, but whose\nconcrete value is looked up in the concrete_values dictionary. If there is no specific value assigned to that variable in concrete_values, the system defaults to\nsome initial value (0 for integers and the empty string for strings).\n\nThe concolic execution framework maintains a queue of different inputs to try, in an InputQueue object (also defined in symex/fuzzy.py). The concolic execution\nframework first adds an initial input (the empty dictionary {}), and then runs the code. If the application makes any branches, the concolic execution system will\ninvoke Z3 to come up with new inputs to test other paths in the code, add those inputs to the input queue, and keep iterating until there are no more inputs to try.\nThe SMT solver. The fork_and_check(c) function checks whether constraint c (an AST) is a satisfiable expression, and returns a pair of values: the satisfiability\nstatus ok and the example model (assignment of values to variables) if the constraint is satisfiable. The ok variable is z3.sat if the constraint is satisfiable, and\nz3.unsat or z3.unknown otherwise. Internally, this function forks off a separate process, tries to run the Z3 solver, but if it takes longer than a few seconds\n(controlled by z3_timeout), it kills the process and returns z3.unknown.\nThe current path condition. When the application executes and makes control flow decisions based on the value of a concolic value (see discussion of\nconcolic_bool above), the constraint representing that branch is appended to the cur_path_constr list. To help with debugging and search heuristics, information\nabout the line of code that triggered this branch is added to the cur_path_constr_callers list.\nNow, your job will be to finish the implementation of concolic_int, and then implement the core of the concolic execution loop. We provide two test programs for you,\ncheck-concolic-int.py and check-symex-int.py. Take a look at these programs to get a sense of how we are using concolic execution, and what code these test cases\nare invoking.\nExercise 2. Finish the implementation of concolic_int by adding support for integer multiply and divide operations. You will need to overload\nadditional methods in the concolic_int class, add AST nodes for multiply and divide operations, and implement _z3expr appropriately for those AST\nnodes.\nLook for the comments Exercise 2: your code here in symex/fuzzy.py to find places where we think you might need to write code to solve this\nexercise.\nRun ./check-concolic-int.py or make check to check that your changes to concolic_int work correctly.\nExercise 3. Implement the core concolic execution logic in concolic_test() in symex/fuzzy.py to get concolic execution working. Look for the\ncomment Exercise 3: your code here and read the comment below that for a proposed plan of attack for implementing that loop.\nRun ./check-symex-int.py or make check to check that your changes to concolic_test() work correctly.\nBeware that our check for this exercise is not complete. You may well find that later on something does not work, and you will have to revisit your code\nfor this exercise.\nThis completes part 1 of this lab.\nSubmit your answers to the first part of this lab assignment by running make submit-a. Alternatively, run make prepare-submit-a. The resulting lab3a-handin.tar.g\nfile will be graded.\nConcolic execution for strings and Zoobar\nBefore we can run the entire Zoobar application through our concolic system, we have to first add support for strings, in addition to support for integers we added above.\nThis is now your job.\nExercise 4. Finish the implementation of concolic execution for strings in symex/fuzzy.py. We left out support for two operations on concolic_str\nobjects. The first is computing the length of a string, and the second is checking whether a particular string a appears in string b (i.e., a is contained in b).\nLook for the comment Exercise 4: your code here to find where you should implement the code for this exercise. We have already implemented the\nsym_contains and sym_length AST nodes for you, which should come in handy for this exercise.\nRun ./check-concolic-str.py and ./check-symex-str.py (or just run make check) to check that your answer to this exercise works correctly.\nIn addition to performing string operations, Zoobar's application code makes database queries (e.g., looking up a user's Profile object from the profile database). Our pla\nis to supply a concolic HTTP request to Zoobar, so the username being looked up is going to be a concolic value as well (coming through the HTTP cookie). But how can\nwe perform SQL queries on a concolic value? We would like to allow the concolic execution system to somehow explore all possible records that it could fetch from the\ndatabase, but the SQL query goes into SQLite's code which is written in C, not Python, so we cannot interpose on that code.\nExercise 5. Figure out how to handle the SQL database so that the concolic engine can create constraints against the data returned by the database. To\nhelp you do this, we've written an empty wrapper around the sqlalchemy get method, in symex/symsql.py. Implement this wrapper so that concolic\nexecution can try all possible records in a database. Examine ./check-symex-sql.py to see how we are thinking of performing database lookups on\nconcolic values.\nYou will likely need to consult the reference for the SQLalchemy query object to understand what the behavior of get should be and what your\nreplacement implementation should be doing.\nRun ./check-symex-sql.py (or just run make check) to check that your answer to this exercise works correctly.\nNow that we can perform concolic execution for strings and even database queries, we can finally try running Zoobar under concolic execution. Take a look at the check\nsymex-zoobar.py program to see how we can invoke Zoobar on symbolic inputs. To make sure that concolic execution of Zoobar finishes relatively quickly, we over-\nconstrain the initial inputs to Zoobar. In particular, we specify that the HTTP method (in environ['REQUEST_METHOD']) is always GET, and that the URL requested (in\nenviron['PATH_INFO']) always starts with trans. This greatly cuts down the number of possible paths that concolic execution must explore; making these inputs\narbitrary leads to about 2000 paths that must be explored, which takes about 10 minutes.\nTry running check-symex-zoobar.py to make sure that all of the code you've implemented so far in this lab works properly. We suggest running it inside of script so\nthat the output is saved in the typescript file for later inspection:\nhttpd@vm-6858:~/lab$ script -c ./check-symex-zoobar.py\nScript started, file is typescript\nTrying concrete values: {}\nstartresp 404 NOT FOUND [('Content-Type', 'text/html'), ('Content-Length', '233'), ('X-XSS-Protection', '0')]\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>404 Not Found</title>\nz\nn\n-\n\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\nTrying concrete values: {'referrer': '', 'cookie': '', 'path': 'fer'}\n/home/httpd/lab/zoobar/debug.py:23 :: __try : caught exception in function transfer:\nTraceback (most recent call last):\nFile \"/home/httpd/lab/zoobar/debug.py\", line 20, in __try\nreturn f(*args, **kwargs)\nFile \"/home/httpd/lab/zoobar/login.py\", line 59, in loginhelper\nif not logged_in():\nFile \"/home/httpd/lab/zoobar/login.py\", line 50, in logged_in\ng.user.checkCookie(request.cookies.get(\"PyZoobarLogin\"))\nFile \"/usr/lib/python2.7/dist-packages/werkzeug/local.py\", line 338, in __getattr__\nreturn getattr(self._get_current_object(), name)\nFile \"/usr/lib/python2.7/dist-packages/werkzeug/utils.py\", line 71, in __get__\nvalue = self.func(obj)\nFile \"/home/httpd/lab/symex/symflask.py\", line 44, in cookies\nfuzzy.require(hdr == name + '=' + val)\nFile \"/home/httpd/lab/symex/fuzzy.py\", line 362, in require\nraise RequireMismatch()\nRequireMismatch\nstartresp 500 INTERNAL SERVER ERROR [('Content-Type', 'text/html'), ('Content-Length', '291')]\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is\n</p>\nTrying concrete values: {'referrer': '', 'path': 'fer', 'cookie_name': 'v', 'cookie': 'v=aC', 'cookie_val': 'aC'}\n...\nStopping after 139 iterations\nScript done, file is typescript\nhttpd@vm-6858:~/lab$\no what bugs did we find in these 139 different paths? Since Zoobar is written in Python, there are no memory corruption or crash bugs, so it's not immediately clear h\ne could tell that there's a problem. As a result, we have to write explicit invariants to catch bad situations indicative of a security problem.\nne such invariant we have supplied for you is a check for eval injection; that is, arbitrary input being passed to the eval() function in Python. Examine\nymex/symeval.py to see how we check for eval injection. We make an approximation that seems reasonable in practice: if the string passed to eval() can ever conta\nbadstuff(); somewhere in the string, it's a good bet that we have an eval injection vulnerability. Since the eval implementation is written in Python, the check\n;badstuff();' in expr invokes your overloaded method in concolic_str, and this tells the concolic execution system to try to construct an input that contains tha\nubstring.\nou can see whether Zoobar contains any such bugs by looking for the string printed by that check in symex/symeval.py in the output from check-symex-zoobar.p\nhttpd@vm-6858:~/lab$ grep \"Exception: eval injection\" typescript\nException: eval injection\nException: eval injection\nhttpd@vm-6858:~/lab$\nt looks like the concolic execution system found two different inputs that lead to our \"eval injection\" check. Now you could look at the lines just before that message t\nee what concrete input triggers eval injection. This can greatly help a developer in practice to find and fix such a bug.\now, your job will be to implement two additional invariant checks to see whether Zoobar balances can ever be corrupted. In particular, we want to enforce two\nuarantees:\nIf no new users are registered, the sum of Zoobar balances in all accounts should remain the same before and after every request. (That is, zoobars should never\ncreated out of thin air.)\nIf a user u does not issue any requests to Zoobar, u's Zoobar balance should not shrink. (That is, it should be impossible for requests by one user to steal another\nuser's zoobars.)\nExercise 6. Add invariant checks to check-symex-zoobar.py to implement the above two rules (total balance preservation and no zoobar theft). Look for\nthe comment Exercise 6: your code here to see where you should write this code. When you detect a zoobar balance mismatch, call the\nreport_balance_mismatch() function. When you detect zoobar theft, call report_zoobar_theft().\nRecall that our check for exercise 3, where you implemented the core of the concolic execution system, was not complete. If you are having trouble with\nthis exercise, it may be that you did not implement exercise 3 correctly, so you may need to go back and fix it.\nTo check whether your solution works correctly, you need to re-run ./check-symex-zoobar.py and see whether the output contains the messages\nWARNING: Balance mismatch detected and WARNING: Zoobar theft detected. Alternatively, you can run make check, which will do this for you\n(run check-symex-zoobar.py and look for these magic strings).\ninally, your job is to fix these two bugs (zoobar balance mismatch and zoobar theft), and make sure that your symbolic execution system properly reports that these b\nre no longer reachable. To fix the bugs, we ask that you do not modify the original Zoobar source code in the zoobar directory, so that make check can continue to w\nnstead, please make a copy of any .py file you want to fix from the zoobar directory into zoobar-fixed. Then use the check-symex-zoobar-fixed.sh script to see i\nour fixes work properly.\ne already fixed the eval injection bug for you, in zoobar-fixed/transfer.py. You should also make sure that your concolic execution system does not report eval\nnjection bugs anymore.\nExercise 7. Fix the two bugs you found in Exercise 6, by copying whatever .py files you need to modify from the zoobar directory to zoobar-fixed and\nchanging them there.\nRecall that our check for exercise 3, where you implemented the core of the concolic execution system, was not complete. If you are having trouble with\nthis exercise, it may be that you did not implement exercise 3 correctly, so you may need to go back and fix it.\nTo check whether your solution works correctly, you need to run ./check-symex-zoobar-fixed.sh and see whether the output still contains the\nmessages Exception: eval injection, WARNING: Balance mismatch detected and WARNING: Zoobar theft detected. Alternatively, you can run\nmake check, which will do this for you.\nou are done! Submit your answers to the lab assignment by running make submit. Alternatively, run make prepare-submit. The resulting lab3-handin.tar.gz fil\nan er\nS\now\nw\nO\ns\nin\n;\n'\nt\ns\nY\ny:\nI\no\ns\nN\ng\nbe\nF\nugs\na\nork.\nI\nf\ny\nW\ni\nY\ne\n\nwill be graded.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Lab 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/ae2fa5a6fe6ceef121522a6b45ecec87_MIT6_858F14_lab4.pdf",
      "content": "6.858 Fall 2014 Lab 4: Attacking server isolation\nHanded out:\nLecture 13\nAll parts due: Two days after Quiz 1 (5:00pm)\nIntroduction\n(Note: this lab was given out during this course's initial run, and the instructions will not apply for users accessing this\npage for the first time. This is meant as a snapshot of the class as it originally progressed.)\nIn this lab, you will be attacking zoobar's privilege isolation and sandboxing. You will download the source code for the\nlab 2 submissions of two other students, which include privilege separation and sandboxing for Python profiles, and\nexamine their code for possible vulnerabilities.\nFor each submission, you will deliver a text file, lab4-code{0,1}.txt respectively, that should contain your analysis of\nthe zoobar server's security, including any possible weaknesses, and potential exploits (in Python) for vulnerabilities you\nhave uncovered.\nTo get started, download the source code for each of the two zoobar sites you will be attacking. Copy the lab4.tar.gz\nfile into your virtual machine (e.g., using scp, not shown below), and extract it in the home directory:\nhttpd@vm-6858:~$ mkdir lab4\nhttpd@vm-6858:~$ cd lab4\nhttpd@vm-6858:~/lab4$ tar -zxvf ~/lab4.tar.gz\n...\nNow, build and run this zoobar site, as shown below.\nhttpd@vm-6858:~/$ cd lab4/code1\nhttpd@vm-6858:~/lab4/code1$ make\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookld.o zookld.c\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o http.o http.c\ncc -m32 zookld.o http.o -lcrypto -o zookld\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookfs.o zookfs.c\ncc -m32 zookfs.o http.o -lcrypto -o zookfs\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zookd.o zookd.c\ncc -m32 zookd.o http.o -lcrypto -o zookd\ncc -m32 -g -std=c99 -fno-stack-protector -Wall -Werror -D_GNU_SOURCE -c -o zooksvc.o zooksvc.c\ncc -m32 zooksvc.o -lcrypto -o zooksvc\nhttpd@vm-6858:~/lab4/code1$ sudo rm -Rf /jail\nhttpd@vm-6858:~/lab4/code1$ sudo make setup\n[sudo] password for httpd: 6858\n./chroot-setup.sh\n+ grep -qv uid=0\n+ id\n...\nhttpd@vm-6858:~/lab4/code1$ sudo ./zookld\nzookld: Listening on port 8080\n...\nNow that you have the zoobar code you need to review up and running, look at the code and understand how it works\nbefore proceeding to the next part.\nPart 1: Code review\nFor the code review process, you do not need to comment on (or exploit) any buffer overflow vulnerabilities in the\nzookws web server from lab 1, or any attacks that involve a victim's web browser (such as exploiting a cross-site\nscripting vulnerability). The latter will be the focus of subsequent labs. Also remember to review both your code0 and\ncode1 submissions. For each one, do the following exercises:\n\nExercise 1: Attack privilege isolation. Evaluate the security of the privilege isolation design in the zoobar\ncode you are reviewing. Look for possible ways to violate the guarantees that privilege separation was\nsupposed to provide. You may want to look back at the lab 2 description to review what the privilege\nseparation was trying to achieve.\nOne possible approach may be to examine the RPC interfaces exposed by each service; are there ways to\ntrick the RPC interface into performing an unintended operation? Another approach may be to examine the\npermissions on files in /jail.\nWrite down your review in lab4-code{0,1}.txt. Comment on any particularly good or bad aspects of the\ndesign. How did your design differ: was it any better or worse? For possible weaknesses, explain why they\nmay be a bad design, even if you cannot immediately exploit them. For extra credit, develop working\nexploits that take advantage of any vulnerabilities you may have discovered. Include the Python code for\nany exploits you developed in lab4-code{0,1}.txt.\nExercise 2: Attack credentials. Evaluate the security of the auth and bank services. In particular, check if\nhashing and salting is properly done, so passwords are reasonably secure, even if the database is stolen by an\nadversary. Also check whether tokens are properly verified by the bank service before a transfer happens.\nComment on the design in much the same way as for the above exercise. Write down your code review in\nlab4-code{0,1}.txt. For extra credit, include Python code for any working exploits you may have\nconstructed in lab4-code{0,1}.txt.\nExercise 3: Attack the Python sandbox. Evaluate the security of the sandbox used to execute Python\nprofile code. Try to look for ways in which code running in one Python sandbox may be able to interfere\nwith the rest of the system, or with code from another user's Python profile.\nComment on the design as in the previous exercises. Write down your code review in lab4-code{0,1}.txt.\nFor extra credit, include Python code for any working exploits you may have constructed in lab4-\ncode{0,1}.txt.\nChallenge! (optional) Use your concolic execution system to analyze the other students' code for possible\nvulnerabilities. For example, you may be able to hook up your concolic execution system to the RPC\ninterface, to look for malicious inputs that might trigger bugs in an RPC server, or malicious responses that\nmight come back from an RPC server and trigger issues in the client.\nIf you do this challenge exercise, append the results to lab4-code{0,1}.txt and write the word CHALLENGE\nsomewhere in the file so that we know to look for it.\nSubmit the resulting code review by running cd ~/lab4 && make submit. The resulting lab4-handin.tar.gz will be\ngraded.\nYou are now done with lab 4.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Lab 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/ba8aea686fc8e4a96ccc2febca2fa2d1_MIT6_858F14_lab5.pdf",
      "content": "6.858 Fall 2014 Lab 5: Browser security\nHanded out: Lecture 14\nDue:\nTwo days after Lecture 17 (5:00pm)\nIntroduction\nThis lab will introduce you to browser-based attacks, as well as to how one might go about preventing\nthem.\nWhen working on the exercises, you may find the following hints and tools useful:\nFirefox's JavaScript console and DOM inspector, both accessible from the Tools menu. The\nJavaScript console lets you see which exceptions are being thrown and why. The DOM Inspector\nlets you peek at the structure of the page and the properties and methods of each node it contains.\n(If the DOM Inspector isn't installed, make sure it's selected when you install Mozilla Firefox.) You\nmight also want to try Firebug. In Firefox 16, these tools are accessible from the Web Developer\nmenu item under the Tools menu.\nYou may need to use CSS to make your attacks invisible to the user. You should know what basic\nsyntax like <style>.warning{display:none}</style> means, and you should feel free to use\nstealthy attributes like style=\"display: none; visibility: hidden; height: 0; width: 0;\nposition: absolute\" in the HTML of your attacks. Beware that frames and images may behave\nstrangely with display: none, so you might want to use visibility: hidden instead. For\ninstance, to create a hidden iframe, try <iframe style=\"visibility: hidden\" ...>.\nYou can use the LiveHTTPHeaders browser extension to see the cookies and form data that a\nbrowser sends.\nIf you need to encode certain special characters in URL parameters, such as newlines, percents, or\nampersands, take a look at encodeURIComponent and decodeURIComponent.\nBeware of Race Conditions: Depending on how you write your code, all four of these attacks could\npotentially have race conditions. Attacks that fail on the grader's browser during grading will\nreceive less than full credit. To ensure that you receive full credit, you should wait after making an\noutbound network request rather than assuming that the request will be sent immediately. You may\nfind the load event on the iframe element helpful.\nNetwork setup\nFor this lab, you will be crafting attacks in your web browser that exploit vulnerabilities in the zoobar\nweb application. To ensure that your exploits work on our machines when we grade your lab, we need to\nagree on the URL that refers to the zoobar web site. For the purposes of this lab, your zoobar web site\nmust be running on http://localhost:8080/. If you have been using your VM's IP address, such as\nhttp://192.168.177.128:8080/, it will not work in this lab.\nIf you are using KVM or VirtualBox, the instructions we provided in lab 1 already ensure that port 8080\non localhost is forwarded to port 8080 in the virtual machine. If you are using VMware, we will use\nssh's port forwarding feature to expose your VM's port 8080 as http://localhost:8080/. First find\nyour VM IP address. You can do this by going to your VM and typing ifconfig. (This is the same IP\naddress you have been using for past labs.) Then configure SSH port forwarding as follows (which\ndepends on your SSH client):\nFor Mac and Linux users: open a terminal on your machine (not in your VM) and run\n\n$ ssh -L localhost:8080:localhost:8080 httpd@VM-IP-ADDRESS\nhttpd@VM-IP-ADDRESS's password: 6858\nFor Windows users, this should be an option in your SSH client. In PuTTY, follow these\ninstructions. Use 8080 for the source port and localhost:8080 for the remote port.\nThe forward will remain in affect as long as the SSH connection is open.\nSetting up the web server\nBefore you begin working on these exercises, please use Git to commit your Lab 3 solutions, fetch the\nlatest version of the course repository, and then create a local branch called lab5 based on our lab5\nbranch, origin/lab5. Do not merge your lab 2 and 3 solutions into lab 5. Here are the shell commands:\nhttpd@vm-6858:~$ cd lab\nhttpd@vm-6858:~/lab$ git commit -am 'my solution to lab3'\n[lab3 c54dd4d] my solution to lab3\n1 files changed, 1 insertions(+), 0 deletions(-)\nhttpd@vm-6858:~/lab$ git pull\nAlready up-to-date.\nhttpd@vm-6858:~/lab$ git checkout -b lab5 origin/lab5\nBranch lab5 set up to track remote branch lab5 from origin.\nSwitched to a new branch 'lab5'\nhttpd@vm-6858:~/lab$ make\n...\nFor those using the provided .zip files, please download lab5.zip from the MIT OpenCourseWare site.\nNote that lab 5's source code is based on the initial web server from lab 1. It does not include privilege\nseparation or Python profiles.\nNow you can start the zookws web server, as follows.\nhttpd@vm-6858:~$ ./zookld\nOpen your browser and go to the URL http://localhost:8080/. You should see the zoobar web\napplication. If you don't, go back and double-check your steps. If you cannot get the web server to work,\nget in touch with course staff before proceeding further.\nCrafting attacks\nYou will craft a series of attacks against the zoobar web site you have been working on in previous labs.\nThese attacks exploit vulnerabilities in the web application's design and implementation. Each attack\npresents a distinct scenario with unique goals and constraints, although in some cases you may be able to\nre-use parts of your code.\nWe will run your attacks after wiping clean the database of registered users (except the user named\n\"attacker\"), so do not assume the presence of any other users in your submitted attacks.\nYou can run our tests with make check; this will execute your attacks against your server, and tell you\nwhether your exploits seem to be working correctly or not. As in previous labs, keep in mind that the\nchecks performed by make check are not exhaustive, especially with respect to race conditions.\n\nExercises 1, 3, and 4, as well as the challenge exercise, require that the displayed site look a certain way.\nThe make check script is not quite smart enough to compare how the site looks like with and without\nyour attack, so you will need to do that comparison yourself (and so will we, during grading). When make\ncheck runs, it generates reference images for what the attack page is supposed to look like (answer-\nXX.ref.png) and what your attack page actually shows (answer-XX.png), and places them in the lab5-\ntests/ directory. Make sure that your answer-XX.png screenshots look like the reference images in\nanswer-XX.ref.png.\nTo view these images from lab5-tests/, either copy them to your local machine, or run python -m\nSimpleHTTPServer 8080 and view the images by visiting http://localhost:8080/lab5-tests/. Note\nthat SimpleHTTPServer caches responses, so you should kill and restart it after a make check run.\nWe will grade your attacks with default settings using the current version of Mozilla Firefox on Ubuntu\n12.04 (as installed on, e.g., the Athena workstations) browser at the time the project is due. We chose this\nbrowser for grading because it is widely available and can run on a variety of operating systems. There\nare subtle quirks in the way HTML and JavaScript are handled by different browsers, and some attacks\nthat work or do not work in Internet Explorer or Chrome (for example) may not work in Firefox. In\nparticular, you should use the Mozilla way of adding listeners to events. We recommend that you test\nyour code on Firefox before you submit, to ensure that you will receive credit for your work.\nFor exercises 1 and 3, you will need a server-side script to automatically email information captured by\nyour client-side JavaScript code to the TAs for grading. We have provided this script for you. Please\nreview the instructions at http://css.csail.mit.edu/6.858/2014/labs/sendmail.php and use that URL in your\nattack scripts to send emails. You may send as many emails as you like while working on the project, but\nplease do not attack or abuse the email script.\nExercise 1: Cookie Theft. Construct an attack that will steal a victim's cookie for the zoobar\nsite when the victim's browser opens a URL of your choosing. (You do not need to do\nanything with the victim's cookie after stealing it, for the purposes of this exercise, although in\npractice an attacker could use the cookie to impersonate the victim, and issue requests as if\nthey came from the victim.)\nYour solution is a URL starting with\nhttp://localhost:8080/zoobar/index.cgi/users?\nThe grader will already be logged in to the zoobar site before loading your URL.\nYour goal is to steal the document cookie and email it to yourself using the email script.\nExcept for the browser address bar (which can be different), the grader should see a\npage that looks exactly as it normally does when the grader visits\nhttp://localhost:8080/zoobar/index.cgi/users. No changes to the site\nappearance or extraneous text should be visible. Avoiding the red warning text is an\nimportant part of this attack. (It's ok if the page looks weird briefly before correcting\nitself.)\nHint: You will need to find a cross-site scripting vulnerability in the\n/zoobar/index.cgi/users page, and then use it to inject Javascript code into the\nbrowser. What input parameters from the HTTP request does the resulting\n/zoobar/index.cgi/users page include? Which of them are not properly escaped?\nHint: To steal the cookie, read about how cookies are accessed from Javascript.\nPlease write your attack URL in a file named answer-1.txt. Your URL should be the only\nthing on the first line of the file.\nFor exercise 1, you will want the server to reflect back certain character strings to the victim's browser.\nHowever, the HTTP server performs URL decoding on your request before passing it on to the zoobar\ncode. Thus, you'll need to make sure that your attack code is URL-encoded. For example, use + instead of\n\nspace and %2b instead of +. Here is a URL encoding reference and a handy conversion tool. You can also\nuse quoting functions in the python urllib module or the JavaScript encodeURIComponent function to\nURL encode strings.\nExercise 2: Cross-Site Request Forgery. Construct an attack that transfers zoobars from a\nvictim to the attacker, when the victim's browser opens an HTML document that you\nconstruct. Do not exploit cross-site scripting vulnerabilities (where the server reflects back\nattack code), such as the one involved in exercise 1 above, or logic bugs in transfer.py that\nyou fixed in lab 3.\nYour solution is a short HTML document named answer-2.html that the grader will\nopen using the web browser.\nBe sure that you do not load the answer-2.html file from\nhttp://localhost:8080/..., because that would place it in the same origin as the site\nbeing attacked, and therefore defeat the point of this exercise.\nThe grader (victim) will already be logged in to the zoobar site before loading your\npage.\nYour goal is to transfer 10 zoobars from the grader's account to the \"attacker\" account.\nThe browser should be redirected to http://css.csail.mit.edu/6.858/2014/ as soon\nas the transfer is complete (so fast the user might not notice).\nThe location bar of the browser should not contain the zoobar server's name or address\nat any point. This requirement is important, and makes the attack more challenging.\nHint: One way to construct the attack is to develop answer-2.html in small steps that\nincrementally meet all the requirements.\nHint: You might find the target attribute of the HTML form element useful in making\nyour attack contained in a single page.\nFor exercise 2, you should test if your attack works by opening your answer-2.html file in your browser,\nand seeing if you achieve the desired result while meeting the requirements for the attack.\nFor exercise 2, you will need to synthesize an HTTP POST request from your HTML page. To do so,\nconsider creating an HTML form whose action attribute points to .../index.cgi/transfer, and which\ncontains <input> fields with the necessary names and values. Look at the source of the HTML that's\ngenerated by index.cgi/transfer to get an idea of what this form should look like. You can submit a\nform by using JavaScript to invoke the click method on the submit button, or the submit method on the\nform itself.\nExercise 3: Side Channels and Phishing. Construct an attack that will steal a victim's\nzoobars, if the user is already logged in (using the attack from exercise 2), or will ask the\nvictim for their username and password, if they are not logged in. The attack scenario is that\nthe victim opens an HTML document that you constructed.\nYour solution is an HTML document named answer-3.html that the grader will open\nusing the web browser.\nAs with the previous exercise, be sure that you do not load the answer-3.html file from\nhttp://localhost:8080/.\nThe grader will run the code once while logged in to the zoobar site before loading your\npage.\nThe grader will run the code a second time while not logged in to the zoobar site before\nloading your page.\nWhen the browser loads your document, the document should sniff out whether the user\n\nis logged into the zoobar site:\nIf the user is not logged in, present an HTML document visibly identical to the\nzoobar login page, by copying the HTML from the real zoobar login page (this\nshould be self-contained in the HTML file you turn in):\nWhen the \"Log in\" button is pressed, send the username and password\n(separated by a comma) using the email script.\nOnce the email is sent, log the user into the real zoobar website (the\nhostname should change to localhost:8080).\nThe behavior for the Register button is left unspecified.\nIf the user is logged in, then forward to the attack from exercise 2.\nHint: The same-origin policy generally does not allow your attack page to access the\ncontents of pages from another domain. What types of files can be loaded by your attack\npage from another domain? Does the zoobar web application have any files of that type?\nHow can you infer whether the user is logged in or not, based on this?\nHint: develop your attack in steps, incrementally addressing all of the above\nrequirements.\nExercise 4: Profile Worm. Create a worm that will transfer 1 zoobar from the victim to the\nattacker, and spread to the victim's profile, when the victim views the profile of another\ninfected user. The scenario is that the first victim views the attacker's profile, and the worm\nspreads onward from there.\nYour solution is a profile that, when viewed, transfers 1 zoobar from the current user to\na user called \"attacker\" (that's the actual username) and replaces the profile of the\ncurrent user with itself (i.e., the attack profile code).\nYour malicious profile should display the message Scanning for viruses... when\nviewed, as if that was the entirety of the viewed profile.\nTo grade your attack, we will cut and paste the submitted profile code into the profile of\nthe \"attacker\" user, and view that profile using the grader's account. We will then view\nthe grader's profile with more accounts, checking for both the zoobar transfer and the\nreplication of profile code.\nThe transfer and replication should be reasonably fast (under 15 seconds). During that\ntime, the grader will not click anywhere.\nDuring the transfer and replication process, the browser's location bar should remain at\nhttp://localhost:8080/zoobar/index.cgi/users?user=username, where username\nis the user whose profile is being viewed. The visitor should not see any extra graphical\nuser interface elements (e.g., frames), and the user whose profile is being viewed should\nappear to have 10 zoobars, and no transfer log entries. These requirements make the\nattack harder to spot for a user, and thus more realistic, but they make the attack also\nharder to pull off.\nYou will not be graded on the corner case where the user viewing the profile has no\nzoobars to send.\nHint: Start by writing a simple HTML profile and uploading it, just to familiarize\nyourself with how an HTML profile works in zoobar. Next, develop the solution profile\nin small steps (e.g., first arrange that the malicious profile code transfers 1 zoobar to the\nattacker, and then make it spread to the visitor's profile).\nHint: This MySpace vulnerability may provide some inspiration.\nPlease write your profile in a file named answer-4.txt.\nFor exercise 4, you may need to create an iframe and access data inside of it. You can use the DOM\n\nmethods document.createElement and document.body.appendChild to do so. Getting access to form\nfields in an iframe differs by browser, and only works for frames from the domain (according to the\nsame-origin policy). In Firefox, you can do iframe.contentDocument.forms[0].zoobars.value = 1;.\nAnother approach may be to use XMLHttpRequest instead of an iframe.\nChallenge: Password Theft. Create an attack that will steal the victim's username and\npassword, even if the victim is diligent about entering their password only when the URL\naddress bar shows http://localhost:8080/zoobar/index.cgi/login.\nYour solution is a short HTML document named answer-chal.html that the grader will\nopen using the web browser.\nThe grader will not be logged in to the zoobar web site before loading your page.\nUpon loading your document, the browser should immediately be redirected to\nhttp://localhost:8080/zoobar/index.cgi/login. The grader will enter a username\nand password, and press the \"Log in\" button.\nWhen the \"Log in\" button is pressed, send the username and password (separated by a\ncomma) using the email script.\nThe login form should appear perfectly normal to the user. No extraneous text (e.g.,\nwarnings) should be visible, and assuming the username and password are correct, the\nlogin should proceed the same way it always does.\nFor this final attack, you may find that using alert() to test for script injection does not work; Firefox\nblocks it when it's causing an infinite loop of dialog boxes. Try other ways to probe whether your code is\nrunning, such as document.loginform.login_username.value=42.\nDeliverables\nMake sure you have the following files: answer-1.txt, answer-2.html, answer-3.html, answer-4.txt,\nand if you are doing the challenge, answer-chal.html, containing each of your attacks. Feel free to\ninclude any comments about your solutions in the answers.txt file (we would appreciate any feedback\nyou may have on this assignment).\nRun make submit. The resulting lab5-handin.tar.gz will be graded. You're done!\nAcknowledgments\nThanks to Stanford's CS155 course staff for the original version of this assignment.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/f98bcda36fdba59627cb14dbb8800bc9_MIT6_858F14_lec1.pdf",
      "content": "6.858 Lecture 1\nAdmin\n\nLab 1\nistrivia:\nhow to get\nout today: buffer overflows. Start early. Next week, there will be tutorials on\nInt\n\nWh\n\nr\nat i\nod\ns\nu\ns\nct\n-\ne\ni\nst\non:\nart\n\ned with lab 1, during office hours.\nAchieving some goal in the presence of an\ncurity?\n\nadversary.\nMany systems are connected to the Internet, which has adversaries. Thus, design of\nmany systems might need to address security, i.e. will the system work when there's\nan adversary?\nHig\n\nh\n-\n-\n-‐le\n\nvel plan for thinki\n-\nC\nPol\nommon goals: confidentiality, integrity, availability.\nicy: the goal you want to achieve. e.g. only Alice should read file F.\nng about security:\n\nThreat model: assumptions about what the attacker could do. e.g. can guess\n-\npas\nassuming attacker can do something.\nswords, cannot physically grab file s\n\nerver. Better to err on the side of\n-\nMechanism: knobs that your system provides to help uphold policy. e.g. user\naccounts, passwords, file permissions, encryption.\nResulting goal: no way for adversary within threat model to violate policy.\n\nNote that goal has nothing to say about mechanism.\nWhy\n-\n-\nis s\nNeed to guarantee policy, assuming the threat model.\necurity hard? Negative goal.\n-\nDifficult to think of all possible ways that attacker might break in.\n\n-\nRealistic threat models are open\nContrast: easy to check whether a positive goal is upheld, e.g. Alice can\n-‐ended (almost negative models).\n\n-\n-\nWeakest link matters.\nactually read file F.\nIterative process: design, update threat model as necessary, etc.\n\nWh\nboundary of each system to see when it breaks.\nat'\n-\ns the point if we can't achieve perfect secur\n\nity? In this class, we'll push the\n-\n-\nEach system will likely have some breaking point leading to compromise.\nDoesn't necessarily mean the system is not useful: depends on context.\n\nImportant to understand what a system can do, and what a system cannot.\nIn reality, must manage security risk vs. benefit.\n- More secure systems means less risk (or consequence) of some\n\ncompromises.\n\nBetter security often makes new functionality practical and safe. Suppose you want\n\n-\n-\nInsecure system may require manual auditing to check for attacks, etc.\n\nHigher cost of attack means more adversaries will be deterred.\nto run some application on your system. Large companies sometimes prohibit users\nfrom installing software that hasn't been approved\nsecurity. Javascript in the browser is isolated, making it ok (for the most part) to run\non their desktops, partly due to\nnew code/applications without manual inspection/approval (or virtual machines, or\nNative C\nto mitigate risk of allowing employees to connect to a corporate network from\nlient, or better OS isolation mechanisms). Similarly, VPNs make it practical\nanywhere\nWhat goes wrong #1: problems with the policy.\n\non the Internet.\n\nExample: Sarah Palin's email account.\nhttp://en.w\n-\n-\nikipedia.org/wiki/Sara\n-\n-\nIf\nYahoo email accounts have a username, password, and security questions.\nUser can log in by supplying username and password.\nh_Palin_email_hack\n\n-\nSecurity questions can sometimes be easier to guess\nuser forgets password, can reset by answering secur\nth\nity Qs\nan pas\n.\n-\nSome adversary guessed Sarah Palin's high school, birthday, etc.\nsword.\nPolicy amounts to: can log in with either password or security Qs.\nenforce \"Only if user forgets password, then ...\")\n(no way to\nExample: Mat H\n\nhttp://www.wired.com/gadgetlab\nonan's accounts at Amazon, Apple, Google, etc.\nhack\n-\ning/all/\n/2012/08/apple-‐amazon-‐mat-‐honan-‐\nGmail password reset: send a verification link to a backup email address.\no\no\nGoogle helpfully prints part of the backup email address.\n\n- Apple password reset: need billing address, last 4 digits of credit card.\no\nMat Honan's backup address was his Apple @me.com account.\n-\nAddress can be easy, but how to get 4 digits of user's credit card\nAmazon: can add a credit card to an account, no password required.\no\nnumber?\no\nAmazon pass\nnumbers.\nword reset: provide any one of user's credit card\n\nAmazon: will not print credit card numbers. But will print last 4 digits!\nExample: Twitter's @N account hijacking.\nhttps://medium.com/p/24eb09e026dd\n- Can be hard for legitimate user to prove they o\n\nwn an account!\nHow\n-\n-\nto\nThink hard about implications of policy statements.\nsolve?\nSome policy checking tools can help, but need a way to specify what's bad.\n\nDifficult in distributed systems: don't know what everyone is doing.\n\nWhat goes wrong #2: problems with threat model / assumptions.\n-\n\nExample: human factors not accounted for, ex.\n-\n-\nUser gets email asking to renew email account, transfer money, or ...\nPhishing attacks.\n-\nTech support gets call from convincing-‐\n\"Rubberhose cryptanalysis\".\nsounding user to reset password.\n\nExample: computational assumptions change over time.\n-\n-\n-\nMIT's Kerberos system used 56-‐bit DES keys, since mid-‐1980's.\n\n-\nAt the time, seemed fine to assume adversary can't check all 2^56 keys.\n-\nNo\nhttps:/\nlonger\n/www.clou\nreasona\ndcracker.com/dictionaries.html\nble: now costs about $100.\nSeveral years ago, 6.858 final project showed you can get\n\nany key in a day.\nExample: all SSL certificate C\n- To connect\no\nto an SSL-‐ena\nAs are fully trusted.\nbled web site, web browser verifies certificate.\n-\nCert\nke\nificate is a combination of server's host name and cryptographic\n-\nLong list (hundreds) of certificate authorities trusted by most browsers.\ny, signed by some trusted certificate authority (CA).\n-\nIf any C\n\"fake\" certificate for any server host name.\nA is compromised, adversary can intercept SSL connections with a\nIn 2011, two C\n(googl\no\ne, ya\nAs were compromised, issued fake certs for many domains\no\n-\nht\nht\nt\nt\np://e\np://en.w\nhoo, t\nn.wikipedia.or\nor, ...), apparent\ng/wiki/D\nly used in Ira\nigiNotar\nn (?\n\n).\nIn 2012, a C\no\nA inadvertently issued a root certificate valid for any domain.\nikipedia.org/wiki/Comodo_Group\nht\nman-‐\ntp://w\n\nin-‐th\nw\ne\nw\n-‐m\n.h\niddl\n-‐online.com/security/news/item/Trustwave-‐\ne-‐certificate-‐1429982.html\nissued-‐a-‐\nExample: assuming your hardware is trustworthy.\n- If NSA is your adversary, turns out to not be a good assumption.\no https://www.schneier.com/blog/archives/2013/12/more_about_the.\n\nhtml\nExample: assuming good randomness for cryptography.\n- Need\no\nhigh\no\n-\nProblem: embedded devices, virtual machines may not have much\nrandomness.\n-‐quality randomness to generate the keys that can't be guessed.\nAs a result, many keys are similar or susceptible to\n\nhttps://factorable.net/weakkeys12.extended.pdf\nguessing attacks.\nExample: subverting milita\n- In the 80's, military encouraged research into secure OS'es.\nry OS security.\n\n-\n\n-\n\n-\n-\n\n!\n\n\"\no\n\n#\n$\n\n!!%&&'())*+,-./00)1/2&+3456+7/80-\n\n:;<=\n>?\n\n@\n\n?\n-\n\nB\n\n:C\n\nA\n?\n- \"\n\n!\n-\n\n- C\n\nD\n\no\n\nB\n\n#\no\n\nE\nFG\n\nBB\n\nC\n\n=!>\n\n@\n\nB\n\n#B\n\nDD\nD ! D\n\n- H\n!!I\nD\n\n=JKB\nJ\">\n- \"\n#\nB\n\n- # H@\n- H@=\nLMHL >\n\nB\n\n- !\n\n- H\n\n!\nNN\"D\n\n\"\n!#\n\nDD\nDOPJJDPQDJRD\nDJR\n\n- #\n\no <!\no @K\n\n- S:;<\n\no S\n\n=\n\n>\n\no E\n\n!\n\n-\n\n- H\n\n?\no\n\no\n-\nSystem not secure if adversary synthesizes new URLs on their own.\n\nHard to say if developers had wrong threat model, or buggy mechanism.\nExample: Android's Java SecureRandom weakness leads to Bitcoin theft.\nhttps://b\n-\n-\nBitco\nitcoin.o\nins can be\nrg/en\ns\n/\npe\nalert\nnt by anyo\n/2013-‐08-‐11-‐\noid\n\nne that kno\nandr\n-\nMany Bitcoin wallet apps on Android used Java's SecureRandom API.\nws the owner's private key.\n-\n-\nTurns out the system was sometimes forgetting to seed the PRNG!\nAs a result, some Bitcoin keys turned out to be easy to guess.\n\nAdversaries searched for guessable keys, spent any corresponding bitcoins.\nExample: bugs in sandbox (NaC\n- Allows adversary to escape isolation, do operations they weren't supposed\nl, Javascript, Java runtime).\n\nto.\nExample: Moxie's SSL certificate name checking bug\n\n- Null byte vs. length-‐encoding.\nExample: buffer overflows (see below).\n\nCase study:\nw\nb\nsecu\nConsider\nrity. E.g., checking which URL\na eb\nuffer\nserver.\noverfl\nOften times, the web server's code is responsible for\nows.\nThus, bugs in the server's code can lead to security compromises.\ns can be accessed, checking SSL client certs, etc.\n\nWhat's the threat model, policy?\n-\n-\n-\nAssume that adversary can connect to web server, supply any inputs.\nPolicy is a bit fuzzy: only perform operations intended by programmer?\n\nE.g., don't want adversary to steal data, bypass checks, install backdoors.\nC\n\nint read_req(void) {\nonsider the following simplified example code from, say, a web server:\n\nchar buf[128];\nint i;\ngets(buf);\ni = atoi(buf);\nreturn i;\n\n}\n\nDemo to go along with the discussion below:\n\n% make\n% ./readreq\n% ./readreq\n% ./readreq\nAAAAAAAAAAAA....AAAA\n% gdb ./readreq\nb read_req\nr\ndisas $eip\ninfo reg\nprint &buf[0]\nx $ebp\nx $ebp+4\ndisas 0x08048e5f\nnext\n\nAAAAAAA...AAA\nprint &buf[0]\nx $ebp\nx $ebp+4\nnext\n\nprint &buf[0] ## why just 128 bytes now?\nx $ebp\nx $ebp+4\n\ndisas $eip\nnexti\nnexti\ndisas $eip\ninfo reg\nx $esp\nstepi\nstepi\n\n..\ndisas main\nset {int}$esp = 0x.. ##from main\nc\n\nWhat does the compiler generate in terms of memory layout?\nx86\n-\ns\n\ntack:\n\n-\n-\nStack gr\n%esp points\nows down.\nto the las\n\n%ebp points to the cal\nt (bottom\nler's %esp\n-‐most) valid thing on the stack.\n+------------------\nvalue.\n+\nentry %ebp ----> | .. prev frame .. |\n| |\n| |\n+------------------+\nentry %esp ----> | return address |\n+------------------+\nnew %ebp ------> | saved %ebp |\n+------------------+\n| i |\n+------------------+\n| buf[127] |\n| ... |\n| buf[0] |\n+------------------+\nnew %esp ------> | ... |\n\nCaller's code (say, main):\n+------------------+\n\nrea\ncall read_req\nd_req's code:\n\npush %ebp\nmov %esp -> %ebp\nsub 168, %esp # stack vars, etc\n...\nmov %ebp -> %esp\npop %ebp\n\nHow does\nret\n-\n-\nSupply long input, over\nthe\n\nadversary take advantage of this code?\n- C\nInter\nan set return address to the buffer itself, include some code in there.\nesting bit of data: r\nwr\netur\nite data on stack past buffer\nn address, gets used by 'ret'.\n.\n\nHow\n-\n-\ndoes\nWhat if one machine has twice as much memory?\nthe adversary know the address of the buffer?\n\n-\nLu\nFor a given OS and program, addresses will often be the same.\nckily for adversary, virtual memory makes things more deterministic.\n\nWhat h\n- Look at the stack frame for gets.\nappens if stack grows up, instead of down?\n\nWhat c\n-\n-\nan th\nUse any priv\ne advers\nil\nar\neges of t\ny do onc\nhe process.\ne they are\n\nexecuting code?\nOften leverage overflow to gain easier access into system.\no\n-\n-\nIf the process is running as root or Administrator, can do anything.\nOriginally on Unix, run shell /bin/sh (thus, \"shell code\").\n- C\nEven if not, can still send spam, read\nan attack other machines behind a firewall.\nfiles (web server, database), etc.\n\nWhy would programmers write such code?\n-\n-\n-\nLeg\nProgrammers were not thinking about security.\nacy code, wasn't exposed to the internet.\n-\nMany\nEven s\ns\naf\ntand\ne ver\nar\ns\nd\nions\nfunc\nhave gotchas\ntions used to\n(s\nbe\ntr\nuns\nncpy d\nafe (\noes\nstr\nnot null\ncpy, gets, spr\n-‐terminate).\nintf).\n\nMore generally, any memory errors can translate into a vulnerability.\n- Using memory after it has been deallocated (use-‐\no\no\n-\nIf reading, might call a corrupted function pointer.\nIf writing, overwrite new data structure, e.g. f\nafte\nunction ptr\nr-‐free).\n.\nFreeing the same memory twice (double-‐\nDecrementing the stack ptr past the end\no\n-\nMight cause malloc to later return the same memory twice.\nfree).\no http://www.invisiblethingslab.com/resources/misc-‐\nof stack, into some other memory.\n-\nlarge-‐memory-‐attacks.pdf\n2010/xorg-‐\n-\nA one-‐byte stray write can lead to compromise.\nMigh\no\no\nt no\nhttp://www.openwall.com/lists/oss-‐security/2014/08/26/2\no\nCa\nt e\nn su\nve\nffice\nn nee\nt\nd\no\nto\nrea\no\nd\nve\nsen\nrwr\nsit\nite\nive\na\nda\nret\nt\nu\na\nrn\nlik\na\ne\nddress\nan encryp\nor fu\n\nC\nt\nnction poin\n\nion key.\nter.\nan suffice to change some bits (e.g. int isLoggedIn, int isRoot).\nHow to avoid mechanism problems?\n- Reduce the amount of security-‐\no\no\nDon't rely on the entire\ncritical\ncati\ncode.\non to\n\n-\nLab 2.\nappli\nenforce security.\nAvoid bugs in security-‐\no\no\nE.g., don't use gets(), use fgets() which can limit buffer length.\ncritical code.\no\nUse common, well-‐tested security mechanisms (\"Economy of\no\nmechanism\").\no\nAudit these common security mechanisms (lots of incentive to do so).\nAvoid developing new, one-‐off mechanisms that may have bugs.\n-\nGood mechanism supports many uses, policies (more incentive to\naudit).\nExamples of common mechanisms:\n\no\no\no\nOS\nnet\n-‐\nw\nle\nork\nvel ac\nfirew\ncess\na\nc\ncryptography, crypt\nll\no\ns (b\nntr\nut\nol (\n, coul\nbut, c\nographic prot\nd oft\nould\nen b\nof\ne b\nten be\netter)\nbe\n\ntter)\nocols.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/76e4b3b4cd463e82c7c58361ed93fd29_MIT6_858F14_lec2.pdf",
      "content": "6.858 Lecture 2\nREVIEW OF BUFFER OVERFLOW ATTACKS\nLast lecture, we looked at the basics of performing a buffer overflow attack. That\nattack leveraged several observations:\n- Systems software is often written in C (operating systems, file systems,\ndatabases, compilers, network servers, command shells and console utilities)\n- C is essentially high-‐level assembly, so . . .\no Exposes raw pointers to memory\no Does not perform bounds-‐checking on arrays (b/c the hardware\ndoesn't do this, and C wants to get you as close to the hardware as\npossible)\n- Attack also leveraged architectural knowledge about how x86 code works:\no The direction that the stack grows\no Layout of stack variables (esp. arrays and return addresses for\nfunctions)\nvoid read_req() {\nchar buf[128];\nint i;\ngets(buf);\n//. . . do stuff w/buf . . .\n}\nWhat does the compiler generate in terms of memory layout?\nx86 stack looks like this:\n- %esp points to the last (bottom-‐most) valid thing on the stack.\n- %ebp points to the caller's %esp value.\n+------------------+\nentry %ebp ----> | .. prev frame .. |\n|\n| |\n|\n| | stack grows down\n+------------------+ |\nentry %esp ----> | return address | v\n+------------------+\nnew %ebp ------> |\nsaved %ebp\n|\n+------------------+\n|\nbuf[127]\n|\n|\n...\n|\n|\nbuf[0]\n|\n+------------------+\nnew %esp ------> |\ni\n|\n+------------------+\n\nHow does the adversary take advantage of this code?\n- Supply long input, overwrite data on stack past buffer.\n- Key observation 1: attacker can overwrite the return address, make the\nprogram jump to a place of the attacker's choosing!\n- Key observation 2: attacker can set return address to the buffer itself, include\nsome x86 code in there!\nWhat can the attackers do once they are executing code?\n- Use any privileges of the process! If the process is running as root or\nAdministrator, it can do whatever it wants on the system. Even if the process\nis not running as root, it can send spam, read files, and interestingly, attack or\nsubvert other machines behind the firewall.\n- Hmmm, but why didn't the OS notice that the buffer has been overrun?\no As far as the OS is aware, nothing strange has happened! Remember\nthat, to a first approximation, the OS only gets invoked by the web\nserver when the server does IO or IPC. Other than that, the OS\nbasically sits back and lets the program execute, relying on hardware\npage tables to prevent processes from tampering with each other's\nmemory. However, page table protections don't prevent buffer\noverruns launched by a process \"against itself,\" since the overflowed\nbuffer and the return address and all of that stuff are inside the\nprocess's valid address space.\no Later in this lecture, we'll talk about things that the OS *can* do to\nmake buffer overflows more difficult.\nFIXING BUFFER OVERFLOWS\nApproach #1: Avoid bugs in C code.\nProgrammer should carefully check sizes of buffers, strings, arrays, etc. In\nparticular, the programmer should use standard library functions that take buffer\nsizes into account (strncpy() instead of strcpy(), fgets() instead of\ngets(), etc.).\nModern versions of gcc and Visual Studio warn you when a program uses unsafe\nfunctions like gets(). In general, YOU SHOULD NOT IGNORE COMPILER WARNINGS.\nTreat warnings like errors!\nGood: Avoid problems in the first place!\nBad: It's hard to ensure that code is bug-‐free, particularly if the code base is large.\nAlso, the application itself may define buffer manipulation functions which do not\nuse fgets() or strcpy() as primitives.\nApproach #2: Build tools to help programmers find bugs.\n\nFor example, we can use static analysis to find problems in source code before it's\ncompiled. Imagine that you had a function like this:\nvoid foo(int *p){\nint offset;\nint *z = p + offset;\nif(offset > 7){\nbar(offset);\n}\n}\nBy statically analyzing the control flow, we can tell that offset is used without being\ninitialized. The if-‐statement also puts bounds on offset that we may be able to\npropogate to bar. We'll talk about static analysis more in later lectures.\n\"Fuzzers\" that supply random inputs can be effective for finding bugs. Note that\nfuzzing can be combined with static analysis to maximize code coverage!\nBad: Difficult to prove the complete absence of bugs, esp. for unsafe code like C.\nGood: Even partial analysis is useful, since programs should become strictly less\nbuggy. For example, baggy bounds checking cannot catch all memory errors, but it\ncan detect many important kinds.\nApproach #3: Use a memory-‐safe language (JavaScript, C#, Python).\nGood: Prevents memory corruption errors by not exposing raw memory addresses\nto the programmer, and by automatically handling garbage collection.\nBad: Low-‐level runtime code DOES use raw memory addresses. So, that runtime\ncore still needs to be correct. For example, heap spray attacks:\n-\nhttps://www.usenix.org/legacy/event/sec09/tech/full_papers/ratanaworab\nhan.pdf\n-\nhttps://www.corelan.be/index.php/2011/12/31/exploit-writing-tutorial-\npart-11-heap-spraying-demystified/\nBad: Still have a lot of legacy code in unsafe languages (FORTRAN and COBOL oh\nnoes).\nBad: Maybe you need access to low-‐level hardware features b/c, e.g., you're writing\na device driver.\nBad: Perf is worse than a fine-‐tuned C application?\n\n-\nUsed to be a bigger problem, but hardware and high-‐level languages are\ngetting better.\no\nJIT compilation FTW!\no\nasm.js is within 2x of native C++ perf! [http://asmjs.org/faq.html]\n-\nUse careful coding to avoid garbage collection jitter in critical path.\n-\nMaybe you're a bad person/language chauvinist who doesn't know how to\npick the right tool for the job. If your task is I/O-‐bound, raw compute speed is\nmuch less important. Also, don't be the chump who writes text manipulation\nprograms in C.\nAll 3 above approaches are effective and widely used, but buffer overflows are still a\nproblem in practice.\n-\nLarge/complicated legacy code written in C is very prevalent.\n-\nEven newly written code in C/C++ can have memory errors.\nHow can we mitigate buffer overflows despite buggy code?\n-\nTwo things going on in a \"traditional\" buffer overflow:\no\nAdversary gains control over execution (program counter).\no\nAdversary executes some malicious code.\n-\nWhat are the difficulties to these two steps?\no\nRequires overwriting a code pointer (which is later invoked).\nCommon target is a return address using a buffer on the stack. Any\nmemory error could potentially work, in practice. Function pointers,\nC++ vtables, exception handlers, etc.\no\nRequires some interesting code in process's memory. This is often\neasier than #1, because:\n§*\nit's easy to put code in a buffer, and\n§*\nthe process already contains a lot of code that might be\nexploitable.\no\nHowever, the attacker needs to put this code in a predictable location,\nso that the attacker can set the code pointer to point to the evil code!\nMitigation approach 1: canaries (e.g., StackGuard, gcc's SSP)\nIdea: OK to overwrite code ptr, as long as we catch it before invocation.\nOne of the earlier systems: StackGuard\n-\nPlace a canary on the stack upon entry, check canary value before return.\n-\nUsually requires source code; compiler inserts canary checks.\n-\nQ: Where is the canary on the stack diagram?\no\nA: Canary must go \"in front of\" return address on the stack, so that\nany overflow which rewrites return address will also rewrite canary.\n\n|\n|\n+------------------+\nentry %esp ----> | return address |\n^\n+------------------+\n|\nnew %ebp ------> |\nsaved %ebp\n|\n|\n+------------------+\n|\n|\nCANARY\n|\n| Overflow goes\n+------------------+\n| this way.\n|\nbuf[127]\n|\n|\n|\n...\n|\n|\n|\nbuf[0]\n|\n|\n+------------------+\n|\n|\nQ: Suppose that the compiler always made the canary 4 bytes of the 'a' character.\nWhat's wrong with this?\n- A: Adversary can include the appropriate canary value in the buffer overflow!\nSo, the canary must be either hard to guess, or it can be easy to guess but still\nresilient against buffer overflows. Here are examples of these approaches.\n- \"Terminator canary\": four bytes (0, CR, LF, -‐1)\no Idea: Many C functions treat these characters as terminators(e.g.,\ngets(), sprintf()). As a result, if the canary matches one of these\nterminators, then further writes won't happen.\n- Random canary generated at program init time: Much more common today\n(but, you need good randomness!).\nWhat kinds of vulnerabilities will a stack canary not catch?\n- Overwrites of function pointer variables before the canary.\n- Attacker can overwrite a data pointer, then leverage it to do arbitrary mem\nwrites.\nint *ptr = ...;\nchar buf[128];\ngets(buf); //Buffer is overflowed, and overwrites ptr.\n*ptr = 5;\n//Writes to an attacker-controlled address!\n//Canaries can't stop this kind of thing.\n- Heap object overflows (function pointers, C++ vtables).\n- malloc/free overflows\nint main(int argc, char **argv) {\nchar *p, *q;\np = malloc(1024);\nq = malloc(1024);\n\nif(argc >= 2)\nstrcpy(p, argv[1]);\nfree(q);\nfree(p);\nreturn 0;\n}\nAssume that the two blocks of memory belonging to p and q are adjacent/nearby in\nmemory.\nAssume that malloc and free represent memory blocks like this:\n+----------------+\n|\n|\n|\nApp data\n|\n|\n|\nAllocated memory block\n+----------------+\n|\nsize\n|\n+----------------+\n+----------------+\n|\nsize\n|\n+----------------+\n| ...empty...\n|\n+----------------+\n|\nbkwd ptr\n|\n+----------------+\n|\nfwd ptr\n|\nFree memory block\n+----------------+\n|\nsize\n|\n+----------------+\nSo, the buffer overrun in p will overwrite the size value in q's memory block! Why is\nthis a problem?\nWhen free() merges two adjacent free blocks, it needs to manipulate bkwd and fwd\npointers, and the pointer calculation uses size to determine where the free memory\nblock structure lives!\np = get_free_block_struct(size);\nbck = p->bk;\nfwd = p->fd;\nfwd->bk = bck; //Writes memory!\nbck->fd = fwd; //Writes memory!\nThe free memory block is represented as a C struct; by corrupting the size value, the\nattacker can force free() to operate on a fake struct that resides in attacker\n\ncontrolled memory and has attacker-‐controlled values for the forward and\nbackwards pointers.\nIf the attacker knows how free() updates the pointers, he can use that update code\nto write an arbitrary value to an arbitrary place. For example, the attacker can\noverwrite a return address.\nActual details are a bit more complicated; if you're interested in gory details, go\nhere: http://www.win.tue.nl/~aeb/linux/hh/hh-11.html\nThe high-‐level point is that stack canaries won't prevent this attack, because the\nattacker is \"skipping over\" the canary and writing directly to the return address!\nSo, stack canaries are one approach for mitigating buffer overflows in buggy code.\nMitigation approach 2: bounds checking.\nOverall goal: prevent pointer misuse by checking if pointers are in range.\nChallenge: In C, it can be hard to differentiate between a valid pointer and an invalid\npointer. For example, suppose that a program allocates an array of characters ...\nchar x[1024];\n... as well as a pointer to some place in that array, e.g.,\nchar *y = &x[107];\nIs it OK to increment y to access subsequent elements?\n-\nIf x represents a string buffer, maybe yes.\n-\nIf x represents a network message, maybe no.\nLife is even more complicated if the program uses unions.\nunion u{\nint i;\nstruct s{\nint j;\nint k;\n};\n};\nint *ptr = &(u.s.k); //Does this point to valid data?\nThe problem is that, in C, a pointer does not encode information about the intended\nusage semantics for that pointer. So, a lot of tools don't try to guess those semantics.\nInstead, the tools have a less lofty goal than \"totally correct\" pointer semantics: the\n\ntools just enforce the memory bounds on heap objects and stack objects. At a high\nlevel, here's the goal: For a pointer p' that's derived from p, p' should only be\ndereferenced to access the valid memory region that belongs to p.\nEnforcing memory bounds is a weaker goal than enforcing \"totally correct\" pointer\nsemantics. Programs can still shoot themselves in the foot by trampling on their\nmemory in nasty ways (e.g., in the union example, the application may write to ptr\neven though it's not defined).\nHowever, bounds checking is still useful because it prevents *arbitrary* memory\noverwrites. The program can only trample its memory if that memory is actually\nallocated! THIS IS CONSIDERED PROGRESS IN THE WORLD OF C.\nA drawback of bounds checking is that it typically requires changes to the compiler,\nand programs must be recompiled with the new compiler. This is a problem if you\nonly have access to binaries.\nWhat are some approaches for implementing bounds checking?\nBounds checking approach #1: Electric fences\n-\nThis is an old approach that had the virtue of being simple.\n-\nIdea: Align each heap object with a guard page, and use page tables to ensure\nthat accesses to the guard page cause a fault.\n+---------+\n| Guard\n|\n|\n| ^\n+---------+ | Overflows cause a page exception\n| Heap\n| |\n| obj\n| |\n+---------+\n-\nThis is a convenient debugging technique, since a heap overflow will\nimmediately cause a crash, as opposed to silently corrupting the heap and\ncausing a failure at some indeterminate time in the future.\n-\nBig advantage: Works without source code-‐-‐-‐don't need to change compilers\nor recompile programs! [You *do* need to relink them so that they use a new\nversion of malloc which implements electric fences.]\n-\nBig disadvantage: Huge overhead! There's only one object per page, and you\nhave the overhead of a dummy page which isn't used for \"real\" data.\n-\nSummary: Electric fences can be useful as debugging technique, and they can\nprevent some buffer overflows for heap objects. However, electric fences\ncan't protect the stack, and the memory overhead is too high to use in\nproduction systems.\nBounds checking approach #2: Fat pointer\n\nIdea: Modify the pointer representation to include bounds information. Now, a\npointer includes a memory address and bounds information about an object that\nlives in that memory region.\nEx:\nRegular 32-bit pointer\n+-----------------+\n| 4-byte address |\n+-----------------+\nFat pointer (96 bits)\n+-----------------+----------------+---------------------+\n| 4-byte obj_base | 4-byte obj_end | 4-byte curr_address |\n+-----------------+----------------+---------------------+\nYou need to modify the compiler and recompile the programs to use the fat pointers.\nThe compiler generates code to abort the program if it dereferences a pointer whose\naddress is outside of its own base...end range.\nint *ptr = malloc(sizeof(int) * 2);\nwhile(1){\n*ptr = 42;\n<----------|\nptr++;\n|\n}\n|\n__________________________|\n|\nThis line checks the current address of the pointer and ensures that it's in-‐bounds.\nThus, this line will fail during the third iteration of the loop.\nProblem #1: It can be expensive to check all pointer dereferences. The C community\nhates things that are expensive, because C is all about SPEED SPEED SPEED.\nProblem #2: Fat pointers are incompatible with a lot of existing software.\n- You can't pass a fat pointer to an unmodified library.\n- You can't use fat pointers in fixed-‐size data structures. For example,\nsizeof(that_struct) will change!\n- Updates to fat pointers are not atomic, because they span multiple words.\nSome programs assume that pointer writes are atomic.\nBounds checking approach #3: Use shadow data structures to keep track of\nbounds information (Jones and Kelly, Baggy).\nBasic idea: For each allocated object, store how big the object is. For example:\nRecord the value passed to malloc:\nchar *p = malloc(mem_size);\nFor static variables, the values are determined by the compiler:\n\nchar p[256];\nFor each pointer, we need to interpose on two operations:\n- pointer arithmetic: char *q = p + 256;\n- pointer dereferencing: char ch = *q;\nQ: Why do we need to interpose on dereference? Can't we do just arithmetic?\n- A: An invalid pointer isn't always a bug! For example, a pointer to one\nelement past the last item of an array might be used as a stopping test in a\nloop. Applications can also do goofy stuff like:\no Simulating 1-‐indexed arrays\no Computing p+(a-‐b) as (p+a)-‐b\no Generating OOB pointers that are later checked for validity\nSo, the mere creation of invalid pointer shouldn't cause program to fail.\nQ: Why do we need to interpose on arithmetic? Can't we do just dereference?\n- A: Interposing on arithmetic is what allows us to track the provenance of\npointers and set the OOB bit. Without the OOB, we won't be able to tell when\na derived pointer goes outside of the bounds of its base object.\nChallenge 1: How do we find the bounds information for a regular pointer, i.e., a\npointer that's in-‐bounds?\nNaive: Use a hash table or interval tree to map addresses to bounds.\nGood: Space efficient (only store info for in-‐use pointers, not all possible addresses).\nBad: Slow lookup (multiple memory accesses per look-‐up).\nNaive: Use an array to store bounds info for *every* memory address.\nGood: Fast!\nBad: Really high memory overhead.\nChallenge 2: How do we force out-‐of-‐bounds pointer dereferences to fail?\nNaive: Instrument every pointer dereference.\nGood: Uh, it works.\nBad: Expensive-‐-‐-‐we have to execute extra code for every dereference!\nThe baggy bounds approach: 5 tricks\n- Round up each allocation to a power of 2, and align the start of the allocation\nto that power of 2.\n- Express each range limit as log_2(alloc_size). For 32-‐bit pointers, only need 5\nbits to express the possible ranges.\n- Store limit info in a linear array: fast lookup with one byte per entry. Also, we\ncan use virtual memory to allocate the array on-‐demand!\n- Allocate memory at slot granularity (e.g., 16 bytes): fewer array entries.\n\nEx:\nslot_size = 16\np = malloc(16);\ntable[p/slot_size] = 4;\np = malloc(32);\ntable[p/slot_size] = 5;\ntable[(p/slot_size) + 1] = 5;\nNow, given a known good pointer p, and a derived pointer p', we can test whether p'\nis valid by checking whether both pointers have the same prefix in their address\nbits, and they only differ in their e least significant bits, where e is equal to the\nlogarithm of the allocation size.\nC code\n-‐-‐-‐-‐-‐-‐\np' = p + i;\nBounds check\n-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐\nsize = 1 << table[p >> log_of_slot_size];\nbase = p & ~(size - 1);\n(p' >= base) && ((p' - base) < size)\nOptimized bounds check\n-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐-‐\n(p^p') >> table[p >> log_of_slot_size] == 0\n-\nUse virtual memory system to prevent out-‐of-‐bound derefs: set most\nsignificant bit in an OOB pointer, and then mark pages in the upper half of\nthe address space as inaccessible. So, we don't have to instrument pointer\ndereferences to prevent bad memory accesses!\nExample code (assume that slot_size=16):\nchar *p = malloc(44);\n//Note that the nearest power of 2 (i.e.,\n//64 bytes) are allocated. So, there are\n//64/(slot_size) = 4 bounds table entries\n//that are set to log_2(64) = 6.\nchar *q = p + 60;\n//This access is ok: It's past p's object\n//size of 44, but still within the baggy\n//bounds of 64.\nchar *r = q + 16;\n//r is now at an offset of 60+16=76 from\n\n//p. This means that r is (76-64)=12 bytes\n//beyond the end of p. This is more than\n//half a slot away, so baggy bounds will\n//raise an error.\nchar *s = q + 8;\n//s is now at an offset of 60+8=68 from p.\n//So, s is only 4 bytes beyond the baggy\n//bounds, which is les than half a slot\n//away. No error is raised, but the OOB\n//high-order bit is set in s, so that s\n//cannot be dereferenced.\nchar *t = s - 32;\n//t is now back inside the bounds, so\n//the OOB bit is cleared.\nFor OOB pointers, the high bit is set (if OOB within half a slot).\n-\nTypically, OS kernel lives in upper half, protects itself via paging\nhardware.\n-\nQ: Why half a slot for out-‐of-‐bounds?\nSo what's the answer to the homework problem?\nchar *p = malloc(256);\nchar *q = p + 256;\nchar ch = *q; //Does this raise an exception?\n//Hint: How big is the baggy bound for p?\nADDITIONAL/SUPPLEMENTAL INFO\n===============================\nSome bugs in the baggy bounds paper:\nFigure 3, explicit bounds check should generate the size like this:\nsize = 1 << table[p >> log_of_slot_size]\nFigure 3, optimized bounds check should be\n(p^p') >> table[p >> log_of_slot_size] == 0\nFigures 5 and 18, pointer arithmetic code should be\nchar *p = &buf[i];\nor\nchar *p = buf + i;\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/8cee7fe9db0bf87fccc332570b7d11ef_MIT6_858F14_lec3.pdf",
      "content": "6.858 Lecture 3\nBaggy bounds continued:\nExample code (assume that slot_size=16)\nchar *p = malloc(44);\n//Note that the nearest power of 2 (i.e.,\n//64 bytes) are allocated. So, there are\n//64/(slot_size) = 4 bounds table entries\n//that are set to log_2(64) = 6.\nchar *q = p + 60;\n//This access is ok: It's past p's object\n//size of 44, but still within the baggy\n//bounds of 64.\nchar *r = q + 16;\n//ERROR: r is now at an offset of 60+16=76\n//from p. This means that r is (76-64)=12\n//beyond the end of p. This is more than\n//half a slot away, so baggy bounds will\n//raise an error.\nchar *s = q + 8;\n//s is now at an offset of 60+8=68 from p.\n//So, s is only 4 bytes beyond the baggy\n//bounds, which is less than half a slot\n//away. No error is raised, but the OOB\n//high-order bit is set in s, so that s\n//cannot be derefernced.\nchar *t = s - 32;\n//t is now back inside the bounds, so\n//the OOB bit is cleared.\nFor OOB pointers, the high bit is set (if OOB within half a slot).\n- Typically, OS kernel lives in upper half, protects itself via paging hardware.\n- Q: Why half a slot for out-‐of-‐bounds?\nSo what's the answer to the homework problem\nchar *p = malloc(256);\nchar *q = p + 256;\nchar ch = *q; //Does this raise an exception?\n//Hint: How big is the baggy bound for p?\nDoes baggy bounds checking have to instrument *every* memory address\ncomputation and access? No: static analysis can prove that some addresses are\nalways safe to use. However, some address calculations are \"unsafe\" in the sense\n\nthat there's no way to statically determine bounds on their values. Such unsafe\nvariables need checks.\nHandling function call arguments is a bit tricky, because the x86 calling convention\nis fixed, i.e., the hardware expects certain things to be in certain places on the stack.\nHowever, we can copy unsafe arguments to a separate area, and make sure that the\ncopied arguments are aligned and protected.\nQ: Do we have to overwrite the original arguments with the copies values upon\nfunction return?\n- A: No, because everything is pass-‐by-‐value in C!\nHow does baggy bounds checking ensure binary compatibility with existing\nlibraries? In particular, how does baggy bounds code interact with pointers to\nmemory that was allocated by uninstrumented code?\nSolution: Each entry in the bounds table is initialized to the value 31, meaning that\nthe corresponding pointer has a memory bound of 2^31 (which is all of the\naddressable memory). On memory allocation in *instrumented* code, bounds\nentries are set as previously discussed, and reset to 31 when the memory is\ndeallocated. Memory allocated to uninstrumented code will never change bounds\ntable entries from their default values of 31; so, when instrumented code interacts\nwith those pointers, bound errors will never happen.\nExample:\nContiguous range of memory used for the heap\n+-------------------+\n|\n|\n|\n|\n| Heap allocated by |\n|\nuninstrumented |---+\n|\ncode\n|\n\\\nBounds table\n|\n|\n\\\n+-------------------+ \\\n+-----------+\n|\n|\n+->|\n|\n|\n|\n| Always 31 |\n| Heap allocated by |\n|\n|\n| instrumented code |\n+-----------+\n|\n|\n| Set using |\n|\n|--------->| baggy bnds|\n+-------------------+\n+-----------+\nWhat does this all mean?\n\n- Can't detect out-‐of-‐bounds pointers generated in uninstrumented code.\n- Can't detect when OOB pointer passed into library goes in-‐bounds again.\no Q: Why?\no A: Because there is no pointer inspection in the uninstrumented code\nwhich could clear the high-‐order OOB bit!\no Q: Why do they instrument strcpy() and memcpy()?\no A: Because otherwise, those functions are uninstrumented code, and\nsuffer from the same problems that we just discussed. For example\noff-‐the-‐shelf strcpy() does not ensure that dest has enough space to\nstore src!\nHow can baggy bits leverage 64-‐bit address spaces?\n- Can get rid of the table storing bounds information, and put it in the pointer.\nRegular pointer\n+---------------+-------+------------------------+\n|\nzero\n| size |\nsupported addr space |\n+---------------+-------+------------------------+\nOOB pointer\n+--------+------+-------+------------------------+\n| offset | size | zero |\nsupported addr space |\n+--------+------+-------+------------------------+\nThis is similar to a fat pointer, but has the advantages that:\n1) tagged pointers are the same size as regular pointers\n2) writes to them are atomic\nso programmer expectations are not broken, and data layouts stay the same.\nAlso note that, using tagged pointers, we can now keep track of OOB pointers that go\nmuch further out-‐of-‐bounds. This is because now we can tag pointers with an offset\nindicating how far they are from their base pointer. In the 32-‐bit world, we couldn't\ntrack OOB offsets without having an additional data structure!\nCan you still launch a buffer overflow attack in a baggy bounds system? Yes, because\nthe world is filled with sadness.\n- Could exploit a vulnerability in uninstrumented libraries.\n- Could exploit temporal vulnerabilities (use-‐after-‐free).\n- Mixed buffers and code pointers:\nstruct {\nvoid (*f) (void);\nchar buf[256];\n} my_type;\n\nNote that *f is not an allocated type, so there are no bounds checks associated with\nits dereference during invocation. Thus, if s.buf is overflowed (e.g., by a bug in an\nuninstrumented library) and s.f is corrupted, the invocation of f will not cause a\nbounds error!\nWould re-‐ordering f and buf help?\n- Might break applications that depend on struct layout.\n- Might not help if this is an array of (struct my_type)'s\nIn general, what are the costs of bounds checking?\n- Space overhead for bounds information (fat pointer or baggy bounds table).\n- Baggy bounds also has space overhead for extra padding memory used by buddy\nallocator (although some amount of overhead is intrinsic to all popular\nalgorithms for dynamic memory allocation).\n- CPU overheads for pointer arithmetic, dereferencing.\n- False alarms!\no Unused out-‐of-‐bounds pointers.\no Temporary out-‐of-‐bounds pointers by more than slot_size/2.\no Conversion from pointer to integers and back.\no Passing out-‐of-‐bounds pointer into unchecked code (the high address bit\nis set, so if the unchecked code does arithmetic using that pointer,\ninsanity may ensue).\n- Requires a significant amount of compiler support\nSo, baggy bounds checking is an approach for mitigating buffer overflows in buggy\ncode.\nMitigation approach 3: non-‐executable memory (AMD's NX bit, Windows\nDEP, W^X, ...)\n- Modern hardware allows specifying read, write, and execute perms for memory\n(R, W permissions were there a long time ago; execute is recent.)\n- Can mark the stack non-‐executable, so that adversary cannot run their code.\n- More generally, some systems enforce \"W^X\", meaning all memory is either\nwritable, or executable, but not both. (Of course, it's OK to be neither.)\no Advantage: Potentially works without any application changes.\no Advantage: The hardware is watching you all of the time, unlike the OS.\no Disadvantage: Harder to dynamically generate code (esp. with W^X).\n§*\nJITs like Java runtimes, Javascript engines, generate x86 on the fly.\n§*\nCan work around it, by first writing, then changing to executable.\nMitigation approach 4: randomized memory addresses (ASLR, stack\nrandomization, ...\n\nObservation: Many attacks use hardcoded addresses in shellcode! [The attacker\ngrabs a binary and uses gdb to figure out where stuff lives.]\n- So, we can make it difficult for the attacker to guess a valid code pointer.\no Stack randomization: Move stack to random locations, and/or place\npadding between stack variables. This makes it more difficult for\nattackers to determine:\n§*\nWhere the return address for the current frame is located\n§*\nWhere the attacker's shellcode buffer will be located\no Randomize entire address space (Address Space Layout Randomization):\nrandomize the stack, the heap, location of DLLs, etc.\n§*\nRely on the fact that a lot of code is relocatable.\n§*\nDynamic loader can choose random address for each library,\nprogram.\n§*\nAdversary doesn't know address of system(), etc.\no Can this still be exploited?\n§*\nAdversary might guess randomness. Especially on 32-‐bit\nmachines, there aren't many random bits (e.g., 1 bit belongs to\nkernel/user mode divide, 12 bits can't be randomized because\nmemory-‐mapped pages need to be aligned with page boundaries,\netc.).\n§*\nFor example, attacker could buffer overflow and try to overwrite\nthe return address with the address of usleep(16), and then seeing\nif the connection hangs for 16 seconds, or if it crashes (in which\ncase the server forks a new ASLR process with the same ASLR\noffsets). usleep() could be in one of 2^16 or 2^28 places. [Mor\ndetails: https://cseweb.ucsd.edu/~hovav/dist/asrandom.pdf]\no ASLR is more practical on 64-‐bit machines (easily 32 bits of randomness).\n- -‐Adversary might extract randomness.\no Programs might generate a stack trace or error message which contains a\npointer.\no If adversaries can run some code, they might be able to extract real\naddresses (JIT'd code?).\no Cute address leak in Flash's Dictionary (hash table):\n1) Get victim to visit your Flash-‐enabled page (e.g., buy an ad).\n2) Hash table internally computes hash value of keys.\n3) Hash value of integers is the integer.\n4) Hash value of object is its memory address.\n5) Iterating over a hash table is done from lowest hash key to highest\nhash key.\n6) So, the attacker creates a Dictionary, inserts a string object which\nhas shellcode, and then inserts a bunch of numbers into the\nDictionary.\n\n7) By iterating through the Dictionary, the attacker can determine\nwhere the string object lives by seeing which integers the object\nreference falls between!\n8) Now, overwrite a code pointer with the shellcode address and\nbypass ASLR!\n- Adversary might not care exactly where to jump.\no Ex: \"Heap spraying\": fill memory w/ shellcode so that a random jump is\nOK!\n- Adversary might exploit some code that's not randomized (if such code exists).\n- Some other interesting uses of randomization:\no System call randomization (each process has its own system call\nnumbers).\no Instruction set randomization so that attacker cannot easily determine\nwhat \"shellcode\" looks like for a particular program instantiation.\no *Ex: Imagine that the processor had a special register to hold a \"decoding\nkey.\" Each installation of a particular application is associated with a\nrandom key. Each machine instruction in the application is XOR'ed with\nthis key. When the OS launches the process, it sets the decoding ke\nregister, and the processor uses this key to decode instructions before\nexecuting them.\nWhich buffer overflow defenses are used in practice?\n- gcc and MSVC enable stack canaries by default.\n- Linux and Windows include ASLR and NX by default.\n- Bounds checking is not as common, due to:\n1) Performance overheads\n2) Need to recompile program\n3) False alarms: Common theme in security tools: false alarms preven\nadoption of tools! Often, zero false alarms with some misses better than\nzero misses but false alarms.\nRETURN-‐ORIENTED PROGRAMMING (ROP)\nASLR and DEP are very powerful defensive techniques.\n- DEP prevents the attacker from executing stack code of his or her choosing\n- ASLR prevents the attacker from determining where shellcode or return\naddresses are located.\n- However, what if the attacker could find PREEXISTING CODE with KNOWN\nFUNCTIONALITY that was located at a KNOWN LOCATION? Then, the attacker\ncould invoke that code to do evil.\no Of course, the preexisting code isn't *intentionally* evil, since it is a\nnormal part of the application.\no However, the attacker can pass that code unexpected arguments, or jum\nto the middle of the code and only execute a desired piece of that code.\n\nThese kinds of attacks are called return-‐oriented programming, or ROP. To\nunderstand how ROP works, let's examine a simple C program that has a securit\nvulnerability.\nvoid run_shell(){\nsystem(\"/bin/bash\");\n}\nvoid process_msg(){\nchar buf[128];\ngets(buf);\n}\nLet's imagine that the system does not use ASLR or stack canaries, but it does use\nDEP. process_msg() has an obvious buffer overflow, but the attacker can't use this\noverflow to execute shellcode in buf, since DEP makes the stack non-‐executable.\nHowever, that run_shell() function looks tempting . . . how can the attacker execute\nit?\n1) Attacker disassembles the program and figures out where the starting address of\nrun_shell().\n2) The attacker launches th buffer overflow, and overwrites the return address of\nprocess_msg() with the address of run_shell(). Boom! The attacker now has\naccess to a shell which runs with the privileges of the application.\n+------------------+\nentry %ebp ----> | .. prev frame .. |\n|\n|\n|\n|\n+------------------+\nentry %esp ----> | return address | ^ <--Gets overwritten\n+------------------+ |\nwith address of\nnew %ebp ------> |\nsaved %ebp\n| |\nrun_shell()\n+------------------+ |\n|\nbuf[127]\n| |\n|\n...\n| |\n|\nbuf[0]\n| |\nnew %esp ------> +------------------+\nThat's a straightforward extension of the buffer overflows that we've already looked\nat. But how can we pass arguments to the function that we're jumping to?\nchar *bash_path = \"/bin/bash\";\nvoid run_cmd(){\n\nsystem(\"/something/boring\");\n}\nvoid process_msg(){\nchar buf[128];\ngets(buf);\n}\nIn this case, the argument that we want to pass to is already located in the progra\ncode. There's also a preexisting call to system(), but that call isn't passing the\nargument that we want.\nWe know that system() must be getting linked to our program. So, using our trust\nfriend gdb, we can find where the system() function is located, and where bash_path\nis located.\nTo call system() with the bash_path argument, we have to set up the stack in the\nway that system() expects when we jump to it. Right after we jump to system()\nsystem() expects this to be on the stack:\n|\n...\n|\n+------------------+\n|\nargument\n| The system() argument.\n+------------------+\n%esp ---->\n|\nreturn addr\n| Where system() should\n+------------------+ ret after it has\nfinished.\nSo, the buffer overflow needs to set up a stack that\nlooks like this:\n+------------------+\nentry %ebp ----> | .. prev frame .. |\n|\n|\n|\n|\n| - - - - - - - - | ^\n|\n| |Address of bash_path\n+ - - - - - - - - | |\n|\n| |Junk return addr for\n+------------------+ | system()\nentry %esp ----> | return address | |Address of system()\n+------------------+ |\nnew %ebp ------> |\nsaved %ebp\n| |Junk\n+------------------+ |\n|\nbuf[127]\n| |\n|\n...\n| |Junk\n|\nbuf[0]\n| |\nnew %esp ------> +------------------+ |\n\nIn essence, what we've done is set up a fake calling frame for the system() call! In\nother words, we've simulated what the compiler would do if it actually wanted to\nsetup a call to system().\nWhat if the string \"/bin/bash\" was not in the program\nWe could include that string in the buffer overflow, and then have the argument to\nsystem() point to the string.\n|\nh\\0\n| ^\n| - - - - - - - - | |\n|\n/bas\n| |\n| - - - - - - - - | |\n|\n/bin\n| | <-------------------+\n| - - - - - - - - | |\n|\n|\n| | Address of bash_path-+\n+ - - - - - - - - | |\n|\n| | Junk return addr from\n+------------------+ | system()\nentry %esp -> | return address | | Address of system()\n+------------------+ |\nnew %ebp ---> |\nsaved %ebp\n| | Junk\n+------------------+ |\n|\nbuf[127]\n| |\n|\n...\n| | Junk\n|\nbuf[0]\n| |\nnew %esp ---> +------------------+ |\nNote that, in these examples, I've been assuming that the attacker used a junk return\naddress from system(). However, the attacker could set it to something useful. In\nfact, by setting it to something useful, the attacker can chain calls together!\nGOAL: We want to call system(\"/bin/bash\") multiple times. Assume that we've\nfound three addresses:\n1) The address of system()\n2) The address of the string \"/bin/bash\"\n3) The address of these x86 opcodes:\n-\npop %eax //Pops the top-‐of-‐stack and puts it in %eax\n-\nret\n//Pops the top-‐of-‐stack and puts it in %eip\nThese opcodes are an example of a \"gadget.\" Gadgets are preexisting instruction\nsequences that can be strung together to create an exploit. Note that there are user-\nfriendly tools to help you extract gadgets from preexisting binaries (e.g. msfelfscan).\n\n|\n| ^\n+ - - - - - - + |\n|\n| | Address of bash_path -+ Fake calling\n+ - - - - - - + |\n| frame for\n(4)\n|\n| | Address of pop/ret\n-+ system()\n+ - - - - - - + |\n(3)\n|\n| | Address of system()\n+ - - - - - - + |\n(2)\n|\n| | Address of bash_path -+ Fake calling\n+ - - - - - - + |\n| frame for\n(1)\n|\n| | Address of pop/ret\n-+ system()\n+--------------+ |\nentry %esp-> |return address| | Address of system()\n+--------------+ |\nnew %ebp --> | saved %ebp | | Junk\n+--------------+ |\n|\nbuf[127]\n| |\n|\n...\n| | Junk\nnew %esp --> |\nbuf[0]\n| |\n+--------------+ |\nSo, how does this work? Remember that the return instruction pops the top of the\nstack and puts it into %eip.\n1) The overflowed function terminates by issuing ret. Ret pops off the top-‐of\nthe-‐stack (the address of system()) and sets %eip to it. system() starts\nexecuting, and %esp is now at (1), and points to the pop/ret gadget.\n2) system() finishes execution and calls ret. %esp goes from (1)-‐-‐>(2) as the ret\ninstruction pops the top of the stack and assigns it to %eip. %eip is now the\nstart of the pop/ret gadget.\n3) The pop instruction in the pop/ret gadget discards the bash_path variable\nfrom the stack. %esp is now at (3). We are still in the pop/ret gadget!\n4) The ret instruction in the pop/ret gadget pops the top-‐of-‐the-‐stack and puts\nit into %eip. Now we're in system() again, and %esp is (4).\nAnd so on and so forth. Basically, we've created a new type of machine that is driven\nby the stack pointer instead of the regular instruction pointer! As the stack pointe\nmoves down the stack, it executes gadgets whose code comes from preexisting\nprogram code, and whose data comes from stack data created by the buffer\noverflow. This attack evades DEP protections-‐-‐we're not generating any new code,\njust invoking preexisting code!\nStack reading: defeating canaries\nAssumptions\n1) The remote server has a buffer overflow vulnerability.\n2) Server crashes and restarts if a canary value is set to an incorrect value.\n3) When the server respawns, the canary is NOT re-‐randomized, and the ASLR\nis NOT re-‐randomized, e.g., because the server uses Linux's PIE mechanism,\nand fork() is used to make new workers and not execve().\nSo, to determine an 8-‐byte canary value:\n\nchar canary[8];\nfor(int i = 1; i <= 8; i++){ //For each canary byte...\nfor(char c = 0; c < 256; c++){ //...guess the value.\ncanary[i-1] = c;\nserver_crashed = try_i_byte_overflow(i, canary);\nif(!server_crashed){\n//We've discovered i-th byte of the\n//the canary!\nbreak;\n}\n}\n}\n//At this point we have the canary, but remember that the\n//attack assumes that the server uses the same canary after\n//a crash.\nGuessing the correct value for a byte takes 128 guesses on average, so on a 32-‐bit\nsystem, we only need 4*128=512 guesses to determine the canary (on a 64-‐bit\nsystem, we need 8*128=1024).\n- Much faster than brute force attacks on the canary (2^15 or 2^27 expected\nguesses on 32/64 bit systems with 16/28 bits of ASLR randomness).\n- Brute force attacks can use the usleep(16) probe that we discussed earlier.\nCanary reading can be extended to reading arbitrary values that the buffer overflow\ncan overwrite!\nSo, we've discussed how we can defeat randomized canaries if canaries are not\nchanged when a server regenerates. We've also shown how to use gdb and gadgets\nto execute preexisting functions in the program using arguments that the attacker\ncontrols. But what if the server DOES use ASLR? This prevents you from usin\noffline analysis to find where the preexisting functions are?\nThis is what the paper for today's lecture discussed. That paper assumed that we're\nusing a 64-‐bit machine, so that's what we'll assume in this lecture from now on. For\nthe purposes of this discussion, the main change is that function arguments are now\npassed in registers instead of on the stack.\nBlind Return-‐oriented Programming\nSTEP 1: Find a stop gadget\nA stop gadget is a return address that points to code that will hang the program, but\nnot crash it. Once the attacker can defeat canaries, he can overwrite the overflown\nfunction's return address and start guessing locations for a stop gadget. If the client\nnetwork connection suddenly closes, the guessed address was not a stop gadget. If\nthe connection stays open, the gadget is a stop gadget.\nSTEP 2: Find gadgets that pop stack entries.\n\nOnce you have a stop gadget, you can use it to find other gadgets that pop entries off\nof the stack and into registers. There are three building blocks to locate stack\npopping gadgets:\n- probe: Address of a potential stack popping gadget\n- stop: Address of a stop gadget\n- crash: Address of non-‐executable code (0x0)\nExample: Find a gadget that pops one thing off the stack.\nsleep(10)\n^\n^\n+--- pop rax\n/\n\\\n|\nret\n/\n\\\n|\n\\--->[stop] 0x5....\n0x5....\n|\n[trap] 0x0\n0x0\n<-----------------+\n+----------[probe] 0x4...8\n0x4...c -->xor rax, rax | Crash!\nret\n|\n\\__________|\nAfter you do this a bunch of times, you'll have a collection of gadgets that pop one\nthing from the stack and then return. However, you won't know which *register*\nthose gadgets store the popped value in. You need to know which registers are used\nto store data so that you can issue a system call. Each system call expects its\narguments to be in a specific set of registers.\nNote that we also don't know the location of the syscall() library function.\nSTEP 3: Find syscall() and determine which registers the pop gadgets use\npause() is a system call that takes no arguments (and thus ignores everything in the\nregisters). To find pause(), the attacker chains all of the \"pop x; ret\" gadgets on the\nstack, pushing the system call number for pause() as the \"argument\" for each\ngadget. At the bottom of the chain, the attacker places the guessed address for\nsyscall().\n|\n| ^\n+ - - - - - - - - + |\n|\n| | Guessed addr of syscall()\n+ - - - - - - - - + |\n|\n| | ...\n+ - - - - - - - - + |\n|\n| | Sys call # for pause\n+ - - - - - - - - + |\n|\n| | Address of pop rsi; ret //Gadget 2\n+ - - - - - - - - + |\n|\n| | Sys call # for pause\n+------------------+ |\nentry %esp ----> | return address | | Address of pop rdi; ret //Gadget 1\n+------------------+ |\nnew %ebp ------> |\nsaved %ebp\n| | Junk\n+------------------+ |\n|\nbuf[127]\n| |\n\n|\n...\n| | Junk\nnew %esp ------> |\nbuf[0]\n| |\n+------------------+ |\nSo, at the end of this chain, the pop gadgets have placed the syscall number for\npause() in a bunch of registers, hopefully including rax, which is the one that\nsyscall() looks in to find the the syscall number.\nOnce this mega-‐gadget induces a pause, we know that we've determined the\nlocation of syscall(). Now we need to determine which gadget pops the top-‐of-‐the\nstack into rax. The attacker can figure this out by process-‐of-‐elimination: iterativel\ntry just one gadget and see if you can invoke pause().\nTo identify arbitrary \"pop x; ret\" gadgets, you can use tricks with other system calls\nthat use the x register that you're trying to find.\nSo, the outcome of this phase is knowledge of \"pop x; ret\" gadgets, location of\nsyscall().\nSTEP 4: Invoke write()\nNow we want to invoke the write call on the network socket that the server has with\nthe attacker's client. We need the following gadgets:\npop rdi; ret (socket)\npop rsi; ret (buffer)\npop rdx; ret (length)\npop rax; ret (write syscall number)\nsyscall\nWe have to guess the socket value, but that's fairly easy to do, since Linux restricts\nprocesses to 1024 simultaneously open file descriptors, and new file descriptors\nhave to be the lowest one available (so guessing a small file descriptor works well in\npractice).\nTo test whether we've guessed the correct file descriptor, simply try the write and\nsee if we receive anything!\nOnce we have the socket number, we issue a write, and for the data to send . . . we\nsend a pointer to the program's .text segment! This allows the attacker to read the\nprogram's code (which was randomized but now totally known to the attacker!)\nNow the attacker can find more powerful gadgets directly, and leverage those\ngadgets to open a shell.\nDefenses against BROP\n- Re-‐randomize the canaries and the address space after each crash!\no Use exec() instead of fork() to create processes, since fork() copies the\naddress space of the parent to the child.\n\no Interesting, Windows is vulnerable to BROP because Windows has no\nfork() equivalent.\n- Sleep-‐on-‐crash?\no Now a BROP attack is a denial-‐of-‐service!\n- Bounds-‐checking?\no Up to 2x performance overhead . . .\nMore info on ROP and x86 calling conventions:\n- http://www.slideshare.net/saumilshah/dive-into-rop-a-quick-introduction-to\nreturn-oriented-programming\n- https://cseweb.ucsd.edu/~hovav/dist/rop.pdf\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/f6502464629d8fc5f03c157bd3c03fbd_MIT6_858F14_lec4.pdf",
      "content": "6.858 Lecture 4\nOKWS\nAdministrivia:\nLab 1 due this Friday.\nToday's lecture: How to build a secure web server on Unix. The design of our lab\nweb server, zookws, is inspired by OKWS.\nPrivilege separation\n- Big security idea\n- Split system into modules, each with their own privilege\no Idea: if one module is compromised, then other modules won't be\n- Use often:\no Virtual machines (e.g., run web site in its own virtual machine)\no SSH (seperates sshd, agent)\n- Challenges:\no Modules need to share\no Need OS support\no Need to use OS carefully to set things up correctly\no Performance\nOKWS\n- Interesting case study of privilege separation\no Lots of sharing between services\n§*\nstrict partitioning doesn't work\no Lots of code\n- Not widely used outside of OKcupid\no Many web sites have their privilege separation plan\no But no papers describing their plans\nBackground: security and protection in Unix\nTypical principals: user IDs, group IDs (32-‐bit integers).\n- Each process has a user ID (uid), and a list of group IDs (gid + grouplist).\n- For mostly-‐historical reasons, a process has a gid + extra grouplist.\n- Superuser principal (root) represented by uid=0, bypasses most checks.\nWhat are the objects + ops in Unix, and how does the OS do access control?\n1. Files, directories.\n- File operations: read, write, execute, change perms, ..\n- Directory operations: lookup, create, remove, rename, change perms, ..\n- Each inode has an owner user and group.\n- Each inode has read, write, execute perms for user, group, others.\n- Typically represented as a bit vector written base 8 (octal); octal works well\nbecause each digit is 3 bits (read, write, exec).\n\n- Who can change permissions on files? Only user owner (process UID).\n- Hard link to file: need write permission to file.\no Possible rationale: quotas.\no Possible rationale: prevent hard-‐linking /etc/passwd to /var/mail/root,\nwith a world-‐writable /var/mail.\n- Execute for directory means being able to lookup names (but not ls).\n- Checks for process opening file /etc/passwd:\no Must be able to look up 'etc' in /, 'passwd' in /etc.\no Must be able to open /etc/passwd (read or read-‐write).\n- Suppose you want file readable to intersection of group1 and group2.\no Is it possible to implement this in Unix?\n2. File descriptors.\n- File access control checks performed at file open.\n- Once process has an open file descriptor, can continue accessing.\n- Processes can pass file descriptors (via Unix domain sockets).\n3. Processes.\n- What can you do to a process?\no debug (ptrace), send signal, wait for exit & get status, ..\n- Debugging, sending signals: must have same UID (almost).\no Various exceptions, this gets tricky in practice.\n- Waiting / getting exit status: must be parent of that process.\n4. Memory.\n- One process cannot generally name memory in another process.\n- Exception: debug mechanisms.\n- Exception: memory-‐mapped files.\n5. Networking.\n- Operations:\no bind to a port\no connect to some address\no read/write a connection\no send/receive raw packets\n- Rules:\no only root (UID 0) can bind to ports below 1024; (e.g., arbitrary user\ncannot run a web server on port 80.)\no only root can send/receive raw packets.\no any process can connect to any address.\no can only read/write data on connection that a process has an fd for.\n- Additionally, firewall imposes its own checks, unrelated to processes.\nHow does the principal of a process get set?\n- System calls: setuid(), setgid(), setgroups().\n- Only root (UID 0) can call these system calls (to first approximation).\nWhere does the user ID, group ID list come from?\n- On a typical Unix system, login program runs as root (UID 0)\n- Checks supplied user password against /etc/shadow.\n\n- Finds user's UID based on /etc/passwd.\n- Finds user's groups based on /etc/group.\n- Calls setuid(), setgid(), setgroups() before running user's shell\nHow do you regain privileges after switching to a non-‐root user?\n- Could use file descriptor passing (but have to write specialized code)\n- Kernel mechanism: setuid/setgid binaries.\no When the binary is executed, set process UID or GID to binary owner.\no Specified with a special bit in the file's permissions.\no For example, su / sudo binaries are typically setuid root.\no Even if your shell is not root, can run \"su otheruser\"\no su process will check passwd, run shell as otheruser if OK.\no Many such programs on Unix, since root privileges often needed.\n- Why might setuid-‐binaries be a bad idea, security-‐wise?\no Many ways for adversary (caller of binary) to manipulate process.\no In Unix, exec'ed process inherits environment vars, file descriptors, ..\no Libraries that a setuid program might use not sufficiently paranoid\no Historically, many vulnerabilities (e.g. pass $LD_PRELOAD, ..)\nHow to prevent a malicious program from exploiting setuid-‐root binaries?\n- Kernel mechanism: chroot\no Changes what '/' means when opening files by path name.\no Cannot name files (e.g. setuid binaries) outside chroot tree.\n- For example, OKWS uses chroot to restrict programs to /var/okws/run, ..\n- Kernel also ensures that '/../' does not allow escape from chroot.\n- Why chroot only allowed for root?\no setuid binaries (like su) can get confused about what's /etc/passwd.\no many kernel implementations (inadvertently?) allow recursive calls to\nchroot() to escape from chroot jail, so chroot is not an effective security\nmechanism for a process running as root.\n- Why hasn't chroot been fixed to confine a root process in that dir?\no Root can write kern mem, load kern modules, access disk sectors, ..\nBackground: traditional web server architecture (Apache).\n- Apache runs N identical processes, handling HTTP requests.\n- All processes run as user 'www'.\n- Application code (e.g. PHP) typically runs inside each of N apache processes.\n- Any accesses to OS state (files, processes, ...) performed by www's UID.\n- Storage: SQL database, typically one connection with full access to DB.\no Database principal is the entire application.\n- Problem: if any component is compromised, adversary gets all the data.\n- What kind of attacks might occur in a web application?\no Unintended data disclosure (getting page source code, hidden files, ..)\no Remote code execution (e.g., buffer overflow in Apache)\no Buggy application code (hard to write secure PHP code), e.g. SQL inj.\no Attacks on web browsers (cross-‐site scripting attacks)\n\nBack to OKWS: what's their application / motivation?\n-\nDating web site: worried about data secrecy.\n-\nNot so worried about adversary breaking in and sending spam.\n-\nLots of server-‐side code execution: matching, profile updates, ...\n-\nMust have sharing between users (e.g. matching) -‐-‐ cannot just partition.\n-\nGood summary of overall plan: \"aspects most vulnerable to attack are least\nuseful to attackers\"\nWhy is this hard?\n-\nUnix makes it tricky to reduce privileges (chroot, UIDs, ..)\n-\nApplications need to share state in complicated ways.\n-\nUnix and SQL databases don't have fine-‐grained sharing control mechanisms.\nHow does OKWS partition the web server? (Figure 1 in paper)\n-\nHow does a request flow in this web server?\nokd -> oklogd\n-> pubd\n-> svc -> dbproxy\n-> oklogd\n-\nHow does this design map onto physical machines?\no\nProbably many front-‐end machines (okld, okd, pubd, oklogd, svc)\no\nSeveral DB machines (dbproxy, DB)\nHow do these components interact?\n-\nokld sets up socketpairs (bidirectional pipes) for each service.\no\nOne socketpair for control RPC requests (e.g., \"get a new log socketpair\").\no\nOne socketpair for logging (okld has to get it from oklogd first via RPC).\no\nFor HTTP services: one socketpair for forwarding HTTP connections.\no\nFor okd: the server-‐side FDs for HTTP services' socketpairs (HTTP+RPC).\n-\nokd listens on a separate socket for control requests (repub, relaunch).\no\nSeems to be port 11277 in Figure 1, but a Unix domain socket in OKWS\ncode.\no\nFor repub, okd talks to pubd to generate new templates, then sends\ngenerated templates to each service via RPC control channel.\n-\nServices talk to DB proxy over TCP (connect by port number).\nHow does OKWS enforce isolation between components in Figure 1?\n-\nEach service runs as a separate UID and GID.\n-\nchroot used to confine each process to a separate directory (almost).\n-\nComponents communicate via pipes (or rather, Unix domain socket pairs).\n-\nFile descriptor passing used to pass around HTTP connections.\n-\nWhat's the point of okld?\n-\nWhy isn't okld the same as okd?\n\n- Why does okld need to run as root? (Port 80, chroot/setuid.)\n- What does it take for okld to launch a service?\no Create socket pairs\no Get new socket to oklogd\no fork, setuid/setgid, exec the service\no Pass control sockets to okd\n- What's the point of oklogd?\n- What's the point of pubd?\n- Why do we need a database proxy?\no Ensure that each service cannot fetch other data, if it is compromised.\n§*\nDB proxy protocol defined by app developer, depending on what\napp requires.\n§*\nOne likely-‐common kind of proxy is a templatized SQL query.\n§*\nProxy enforces overall query structure (select, update), but allows\nclient to fill in query parameters.\no Where does the 20-‐byte token come from? Passed as arguments to\nservice.\no Who checks the token? DB proxy has list of tokens (& allowed queries?)\no Who generates token? Not clear; manual by system administrator?\no What if token disclosed? Compromised component could issue queries.\n- Table 1: why are all services and okld in the same chroot? Is it a problem?\no How would we decide? What are the readable, writable files there?\no Readable: shared libraries containing service code.\no Writable: each service can write to its own /cores/<uid>.\no Where's the config file? /etc/okws_config, kept in memory by okld.\no oklogd & pubd have separate chroots because they have important state:\noklogd's chroot contains the log file, want to ensure it's not modified.\npubd's chroot contains the templates, want to avoid disclosing them (?).\n- Why does OKWS need a separate GID for every service?\no Need to execute binary, but file ownership allows chmod.\no Solution: binaries owned by root, service is group owner, mode 0410.\no Why 0410 (user read, group execute), and not 0510 (user read & exec)?\n- Why not process per user? Is per user strictly better? user X service?\no Per-‐service isolation probably made sense for okcupid given their apps.\n(i.e. perhaps they need a lot of sharing between users anyway?)\no Per-‐user isolation requires allocating UIDs per user, complicating okld,\nand reducing performance (though may still be OK for some use cases).\nDoes OKWS achieve its goal?\n- What attacks from the list of typical web attacks does OKWS solve, and how?\no Most things other than XSS are addressed.\no XSS sort-‐of addressed through using specialized template routines.\n- What's the effect of each component being compromised, and \"attack surface\"?\no okld: root access to web server machine, but maybe not to DB.\n§*\nattack surface: small (no user input other than svc exit).\n\no okd: intercept/modify all user HTTP reqs/responses, steal passwords.\n§*\nattack surface: parsing the first line of HTTP request; control\nrequests.\no pubd: corrupt templates, leverage to maybe exploit bug in some service?\n§*\nattack surface: requests to fetch templates from okd.\no oklogd: corrupt/ignore/remove/falsify log entries\n§*\nattack surface: log messages from okd, okld, svcs\no service: send garbage to user, access data for svc (modulo dbproxy)\n§*\nattack surface: HTTP requests from users (+ control msgs from\nokd)\no dbproxy: access/change all user data in the database it's talking to\n§*\nattack surface: requests from authorized services, requests from\nunauthorized services (easy to drop)\n- OS kernel is part of the attack surface once a single service is compromised.\no Linux kernel vulnerabilities rare, but still show up several times a year.\n- OKWS assumes developer does the right thing at design level (maybe not impl):\no Split web application into separate services (not clump all into one).\no Define precise protocols for DB proxy (otherwise any service gets any\ndata).\n- Performance?\no Seems better than most alternatives.\no Better performance under load (so, resists DoS attacks to some extent)\n- How does OKWS compare to Apache?\no Overall, better design.\no okld runs as root, vs. nothing in Apache, but probably minor.\no Neither has a great solution to client-‐side vulnerabilities (XSS, ..)\n- How might an adversary try to compromise a system like OKWS?\no Exploit buffer overflows or other vulnerabilities in C++ code.\no Find a SQL injection attack in some dbproxy.\no Find logic bugs in service code.\no Find cross-‐site scripting vulnerabilities.\nHow successful is OKWS?\n- Problems described in the paper are still pretty common.\n- okcupid.com still runs OKWS, but doesn't seem to be used by other sites.\n- C++ might not be a great choice for writing web applications.\no For many web applications, getting C++ performance might not be\ncritical.\no Design should be applicable to other languages too (Python, etc).\no In fact, zookws for labs in 6.858 is inspired by OKWS, runs Python code.\n- DB proxy idea hasn't taken off, for typical web applications.\no But DB proxy is critical to restrict what data a service can access in\nOKWS.\no Why? Requires developers to define these APIs: extra work, gets in the\nway.\n\no Can be hard to precisely define the allowed DB queries ahead of time.\n(Although if it's hard, might be a flag that security policy is fuzzy.)\n- Some work on privilege separation for Apache (though still hard to use).\no Unix makes it hard for non-‐root users to manipulate user IDs.\no Performance is a concern (running a separate process for each request).\n- scripts.mit.edu has a similar design, running scripts under different UIDs.\no Mostly worried about isolating users from one another.\no Paranoid web app developer can create separate locker for each\ncomponent.\n- Sensitive systems do partitioning at a coarser granularity.\no Credit card processing companies split credit card data vs. everything\nelse.\no Use virtual machines or physical machine isolation to split apps, DBs, ..\nHow could you integrate modern Web application frameworks with OKWS?\n- Need to help okd figure out how to route requests to services.\n- Need to implement DB proxies, or some variant thereof, to protect data.\no Depends on how amenable the app code is to static analysis.\no Or need to ask programmer to annotate services w/ queries they can run.\n- Need to ensure app code can run in separate processes (probably OK).\nReferences:\n- http://css.csail.mit.edu/6.858/2014/readings/setuid.pdf\n- http://httpd.apache.org/docs/trunk/suexec.html\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/0f397e7b504aa9b146763705619d49ba_MIT6_858F14_lec6.pdf",
      "content": "6.858 Lecture 6\nCapabilities and other Protection Mechanisms\nWhat's the problem the authors of \"confused deputy\" encountered?\n- Their system had a Fortran compiler, /sysx/fort (in Unix filename syntax)\n- They wanted the Fortran compiler to record usage statistics, but where?\no Created a special statistics file, /sysx/stat.\no Gave /sysx/fort \"home files license\" (kind-‐of like setuid w.r.t. /sysx)\n- What goes wrong?\no User can invoke the compiler asking it to write output to /sysx/stat.\n§*\ne.g. /sysx/fort /my/code.f -‐o /sysx/stat\no Compiler opens supplied path name, and succeeds, because of its license.\no User alone couldn't have written to that /sysx/stat file.\n- Why isn't the /sysx/fort thing just a bug in the compiler?\no Could, in principle, solve this by adding checks all over the place.\no Problem: need to add checks virtually everywhere files are opened.\no Perfectly correct code becomes buggy once it's part of a setuid binary.\n- So what's the \"confused deputy\"?\no The compiler is running on behalf of two principals:\n§*\nthe user principal (to open user's files)\n§*\nthe compiler principal (to open compiler's files)\no Not clear what principal's privileges should be used at any given time.\nCan we solve this confused deputy problem in Unix?\n- Suppose gcc wants to keep statistics in /etc/gcc.stats\n- Could have a special setuid program that only writes to that file\no Not so convenient: can't just open the file like any other.\n- What if we make gcc setuid to some non-‐root user (owner of stats file)?\no Hard to access user's original files.\n- What if gcc is setuid-‐root? (Bad idea, but let's figure out why..)\no Lots of potential for buffer overflows leading to root access.\no Need to instrument every place where gcc might open a file.\n- What check should we perform when gcc is opening a file?\no If it's an \"internal\" file (e.g. /etc/gcc.stats), maybe no check.\no If it's a user-‐supplied file, need to make sure user can access it.\no Can look at the permissions for the file in question.\no Need to also check permissions on directories leading up to this file.\n- Potential problem: race conditions.\no What if the file changes between the time we check it and use it?\no Common vulnerability: attacker replaces legit file with symlink\no Symlink could point to, say, /etc/gcc.stats, or /etc/passwd, or ...\no Known as \"time-‐of-‐check to time-‐of-‐use\" bugs (TOCTTOU).\nSeveral possible ways of thinking of this problem:\n\n1. Ambient authority: privileges that are automatically used by process are the\nproblem here. No privileges should ever be used automatically. Name of an\nobject should be also the privileges for accessing it.\n2. Complex permission checks: hard for privileged app to replicate. With simpler\nchecks, privileged apps might be able to correctly check if another user should\nhave access to some object.\nWhat are examples of ambient authority?\n- Unix UIDs, GIDs.\n- Firewalls (IP address vs. privileges for accessing it)\n- HTTP cookies (e.g. going to a URL like http://gmail.com)\nHow does naming an object through a capability help?\n- Pass file descriptor instead of passing a file name.\n- No way to pass a valid FD unless caller was authorized to open that file.\nCould we use file descriptors to solve our problem with a setuid gcc?\n- Sort-‐of: could make the compiler only accept files via FD passing.\n- Or, could create a setuid helper that opens the /etc/gcc.stats file, passes an open\nfile descriptor back to our compiler process.\n- Then, can continue using this open file much like any other file.\n- How to ensure only gcc can run this helper?\no Make gcc setgid to some special group.\no Make the helper only executable to that special group.\no Make sure that group has no other privileges given to it.\nWhat problem are the Capsicum authors trying to solve with capabilities?\n- Reducing privileges of untrustworthy code in various applications.\n- Overall plan:\no Break up an application into smaller components.\no Reduce privileges of components that are most vulnerable to attack.\no Carefully design interfaces so one component can't compromise another.\n- Why is this difficult?\no Hard to reduce privileges of code (\"sandbox\") in traditional Unix system.\no Hard to give sandboxed code some limited access (to files, network, etc).\nWhat sorts of applications might use sandboxing?\n- OKWS.\n- Programs that deal with network input:\no Put input handling code into sandbox.\n- Programs that manipulate data in complex ways:\no (gzip, Chromium, media codecs, browser plugins, ...)\no Put complex (& likely buggy) part into sandbox.\n- How about arbitrary programs downloaded from the Internet?\no Slightly different problem: need to isolate unmodified application code.\n\no\nOne option: programmer writes their application to run inside sandbox.\n§*\nWorks in some cases: Javascript, Java, Native Client, ...\n§*\nNeed to standardize on an environment for sandboxed code.\no\nAnother option: impose new security policy on existing code.\n§*\nProbably need to preserve all APIs that programmer was using.\n§*\nNeed to impose checks on existing APIs, in that case.\n§*\nUnclear what the policy should be for accessing files, network, etc.\n-\nApplications that want to avoid being tricked into misusing privileges?\no\nSuppose two Unix users, Alice and Bob, are working on some project.\no\nBoth are in some group G, and project dir allows access by that group.\no\nLet's say Alice emails someone a file from the project directory.\no\nRisk: Bob could replace the file with a symlink to Alice's private file.\no\nAlice's process will implicitly use Alice's ambient privileges to open.\no\nCan think of this as sandboxing an individual file operation.\nWhat sandboxing plans (mechanisms) are out there (advantages, limitations)?\n-\nOS typically provides some kind of security mechanism (\"primitive\").\no\nE.g., user/group IDs in Unix, as we saw in the previous lecture.\no\nFor today, we will look at OS-‐level security primitives/mechanisms.\no\nOften a good match when you care about protecting resources the OS\nmanages.\no\nE.g., files, processes, coarse-‐grained memory, network interfaces, etc.\n-\nMany OS-‐level sandboxing mechanisms work at the level of processes.\no\nWorks well for an entire process that can be isolated as a unit.\no\nCan require re-‐architecting application to create processes for isolation.\n-\nOther techniques can provide finer-‐grained isolation (e.g., threads in proc).\no\nLanguage-‐level isolation (e.g., Javascript).\no\nBinary instrumentation (e.g., Native Client).\no\nWhy would we need these other sandboxing techniques?\n§*\nEasier to control access to non-‐OS / finer-‐grained objects.\n§*\nOr perhaps can sandbox in an OS-‐independent way.\no\nOS-‐level isolation often used in conjunction with finer-‐grained isolation.\n§*\nFiner-‐grained isolation is often hard to get right (Javascript, NaCl).\n§*\nE.g., Native Client uses both a fine-‐grained sandbox + OS-‐level\nsandbox.\no\nWill look at these in more detail in later lectures.\nPlan 0: Virtualize everything (e.g., VMs).\n-\nRun untrustworthy code inside of a virtualized environment.\n-\nMany examples: x86 qemu, FreeBSD jails, Linux LXC, ..\n-\nAlmost a different category of mechanism: strict isolation.\n-\nAdvantage: sandboxed code inside VM has almost no interactions with outside.\n-\nAdvantage: can sandbox unmodified code that's not expecting to be isolated.\n-\nAdvantage: some VMs can be started by arbitrary users (e.g., qemu).\n-\nAdvantage: usually composable with other isolation techniques, extra layer.\n\n-\nDisadvantage: hard to allow some sharing: no shared processes, pipes, files.\n-\nDisadvantage: virtualizing everything often makes VMs relatively heavyweight.\no\nNon-‐trivial CPU/memory overheads for each sandbox.\nPlan 1: Discretionary Access Control (DAC).\n-\nEach object has a set of permissions (an access control list).\no\nE.g., Unix files, Windows objects.\no\n\"Discretionary\" means applications set permissions on objects (e.g.,\nchmod).\n-\nEach program runs with privileges of some principals.\no\nE.g., Unix user/group IDs, Windows SIDs.\n-\nWhen program accesses an object, check the program's privileges to decide.\no\n\"Ambient privilege\": privileges used implicitly for each access.\nName\nProcess privileges\n|\n|\nV\nV\nObject -> Permissions -> Allow?\nHow would you sandbox a program on a DAC system (e.g., Unix)?\n-\nMust allocate a new principal (user ID):\no\nOtherwise, existing principal's privileges will be used implicitly!\n-\nPrevent process from reading/writing other files:\no\nChange permissions on every file system-‐wide?\n§*\nCumbersome, impractical, requires root.\no\nEven then, new program can create important world-‐writable file.\no\nAlternative: chroot (again, have to be root).\n-\nAllow process to read/write a certain file:\no\nSet permissions on that file appropriately, if possible.\no\nLink/move file into the chroot directory for the sandbox?\n-\nPrevent process from accessing the network:\no\nNo real answer for this in Unix.\no\nMaybe configure firewall? But not really process-‐specific.\n-\nAllow process to access particular network connection:\no\nSee above, no great plan for this in Unix.\n-\nControl what processes a sandbox can kill / debug / etc:\no\nCan run under the same UID, but that may be too many privileges.\no\nThat UID might also have other privileges...\nProblem: only root can create new principals, on most DAC systems.\n-\nE.g., Unix, Windows.\nProblem: some objects might not have a clear configurable access control list.\n-\nUnix: processes, network...\nProblem: permissions on files might not map to policy you want for sandbox.\n-\nCan sort-‐of work around using chroot for files, but awkward.\n\nRelated problem: performing some operations with a subset of privileges.\n-\nRecall example with Alice emailing a file out of shared group directory.\n-\n\"Confused deputy problem\": program is a \"deputy\" for multiple principals.\n-\nOne solution: check if group permissions allow access (manual, error-‐prone).\no\nAlternative solution: explicitly specify privileges for each operation.\n§*\nCapabilities can help: capability (e.g., fd) combines object +§*\nprivileges.\n§* Some Unix features incompat. w/ pure capability design (symlinks\nby name).\nPlan 2: Mandatory Access Control (MAC).\n-\nIn DAC, security policy is set by applications themselves (chmod, etc).\n-\nMAC tries to help users / administrators specify policies for applications.\no\n\"Mandatory\" in the sense that applications can't change this policy.\no\nTraditional MAC systems try to enforce military classified levels.\no\nE.g., ensure top-‐secret programs can't reveal classified information.\nName\nOperation + caller process\n|\n|\nV\nV\nObject --------> Allow?\n^\n|\nPolicy ------------+\n-\nNote: many systems have aspects of both DAC + MAC in them.\no\nE.g., Unix user IDs are \"DAC\", but one can argue firewalls are \"MAC\".\no\nDoesn't really matter -‐-‐ good to know the extreme points in design space\nWindows Mandatory Integrity Control (MIC) / LOMAC in FreeBSD.\n-\nKeeps track of an \"integrity level\" for each process.\n-\nFiles have a minimum integrity level associated with them.\n-\nProcess cannot write to files above its integrity level.\n-\nIE in Windows Vista runs as low integrity, cannot overwrite system files.\n-\nFreeBSD LOMAC also tracks data read by processes.\no\n(Similar to many information-‐flow-‐based systems.)\no\nWhen process reads low-‐integrity data, it becomes low integrity too.\no\nTransitive, prevents adversary from indirectly tampering with files.\n-\nNot immediately useful for sandboxing: only a fixed number of levels.\nSElinux.\n-\nIdea: system administrator specifies a system-‐wide security policy.\n-\nPolicy file specifies whether each operation should be allowed or denied.\n-\nTo help decide whether to allow/deny, files labeled with \"types\".\n\no\n(Yet another integer value, stored in inode along w/ uid, gid, ..)\nMac OS X sandbox (\"Seatbelt\") and Linux seccomp_filter.\n-\nApplication specifies policy for whether to allow/deny each syscall.\no\n(Written in LISP for MacOSX's mechanism, or in BPF for Linux's.)\n-\nCan be difficult to determine security impact of syscall based on args.\no\nWhat does a pathname refer to? Symlinks, hard links, race\nconditions... (Although MacOSX's sandbox provides a bit more\ninformation.)\n-\nAdvantage: any user can sandbox an arbitrary piece of code, finally!\n-\nLimitation: programmer must separately write the policy + application code.\n-\nLimitation: some operations can only be filtered at coarse granularity.\no\nE.g., POSIX shm in MacOSX's filter language, according to Capsicum\npaper.\n-\nLimitation: policy language might be awkware to use, stateless, etc.\no\nE.g., what if app should have exactly one connection to some server?\n-\nNote: seccomp_filter is quite different from regular/old seccomp, and the\nCapsicum paper talks about the regular/old seccomp. ]\nIs it a good idea to separate policy from application code?\n-\nDepends on overall goal.\n-\nPotentially good if user/admin wants to look at or change policy.\n-\nProblematic if app developer needs to maintain both code and policy.\n-\nFor app developers, might help clarify policy.\n-\nLess-centralized \"MAC\" systems (Seatbelt, seccomp) provide a compromise.\nPlan 3: Capabilities (Capsicum).\nDifferent plan for access control: capabilities.\n-\nIf process has a handle for some object (\"capability\"), can access it.\nCapability --> Object\n-\nNo separate question of privileges, access control lists, policies, etc.\n-\nE.g.: file descriptors on Unix are a capability for a file.\no\nProgram can't make up a file descriptor it didn't legitimately get. (Why\nnot?)\no\nOnce file is open, can access it; checks happened at open time.\no\nCan pass open files to other processes.\no\nFDs also help solve \"time-‐of-‐check to time-‐of-‐use\" (TOCTTOU) bugs.\n-\nCapabilities are usually ephemeral: not part of on-‐disk inode.\no\nWhatever starts the program needs to re-‐create capabilities each time.\nGlobal namespaces.\n-\nWhy are these guys so fascinated with eliminating global namespaces?\n-\nGlobal namespaces require some access control story (e.g., ambient privs).\n\n-\nHard to control sandbox's access to objects in global namespaces.\nKernel changes.\n-\nJust to double-‐check: why do we need kernel changes?\no\nCan we implement everything in a library (and LD_PRELOAD it)?\n-\nRepresent more things as file descriptors: processes (pdfork).\no\nGood idea in general.\n-\nCapability mode: once process enters cap mode, cannot leave (+all children).\n-\nIn capability mode, can only use file descriptors -‐-‐ no global namespaces.\no\nCannot open files by full path name: no need for chroot as in OKWS.\no\nCan still open files by relative path name, given fd for dir (openat).\n-\nCannot use \"..\" in path names or in symlinks: why not?\no\nIn principle, \"..\" might be fine, as long as \"..\" doesn't go too far.\no\nHard to enforce correctly.\no\nHypothetical design:\n§*\nProhibit looking up \"..\" at the root capability.\n§*\nNo more \"..\" than non-‐\"..\" components in path name, ignoring \".\".\n-\nAssume a process has capability C1 for /foo.\n-\nRace condition, in a single process with 2 threads:\nT1: mkdir(C1, \"a/b/c\")\nT1: C2 = openat(C1, \"a\")\nT1: C3 = openat(C2, \"b/c/../..\")\n## should return a cap\nfor /foo/a\nLet openat() run until it's about to look up the first \"..\"\nT2: renameat(C1, \"a/b/c\", C1, \"d\")\nT1: Look up the first \"..\", which goes to \"/foo\"\nLook up the second \"..\", which goes to \"/\"\n-\nDo Unix permissions still apply?\no\nYes --can't access all files in dir just because you have a cap for dir.\no\nBut intent is that sandbox shouldn't rely on Unix permissions.\n-\nFor file descriptors, add a wrapper object that stores allowed operations.\n-\nWhere does the kernel check capabilities?\no\nOne function in kernel looks up fd numbers -‐-‐ modified it to check caps.\no\nAlso modified namei function, which looks up path names.\no\nGood practice: look for narrow interfaces, otherwise easy to miss checks.\nlibcapsicum.\n-\nWhy do application developers need this library?\n-\nBiggest functionality: starting a new process in a sandbox.\nfd lists.\n-\nMostly a convenient way to pass lots of file descriptors to child process.\n-\nName file descriptors by string instead of hard-‐coding an fd number.\ncap_enter() vs lch_start().\n-\nWhat are the advantages of sandboxing using exec instead of cap_enter?\n-\nLeftover data in memory: e.g., private keys in OpenSSL/OpenSSH.\n\n- Leftover file descriptors that application forgot to close.\n- Figure 7 in paper: tcpdump had privileges on stdin, stdout, stderr.\n- Figure 10 in paper: dhclient had a raw socket, syslogd pipe, lease file.\nAdvantages: any process can create a new sandbox.\n- (Even a sandbox can create a sandbox.)\nAdvantages: fine-‐grained control of access to resources (if they map to FDs).\n- Files, network sockets, processes.\nDisadvantage: weak story for keeping track of access to persistent files.\nDisadvantage: prohibits global namespaces, requires writing code differently.\nAlternative capability designs: pure capability-‐based OS (KeyKOS, etc).\n- Kernel only provides a message-‐passing service.\n- Message-‐passing channels (very much like file descriptors) are capabilities.\n- Every application has to be written in a capability style.\n- Capsicum claims to be more pragmatic: some applications need not be changed.\nLinux capabilities: solving a different problem.\n- Trying to partition root's privileges into finer-‐grained privileges.\n- Represented by various capabilities: CAP_KILL, CAP_SETUID,\nCAP_SYS_CHROOT...\n- Process can run with a specific capability instead of all of root's privs.\n- Ref: capabilities(7), http://linux.die.net/man/7/capabilities\nUsing Capsicum in applications.\n- Plan: ensure sandboxed process doesn't use path names or other global NSes.\no For every directory it might need access to, open FD ahead of time.\no To open files, use openat() starting from one of these directory FDs.\no .. programs that open lots of files all over the place may be cumbersome.\n- tcpdump.\no 2-‐line version: just cap_enter() after opening all FDs.\no Used procstat to look at resulting capabilities.\no 8-‐line version: also restrict stdin/stdout/stderr.\no Why? E.g., avoid reading stderr log, changing terminal settings...\n- dhclient.\no Already privilege-‐separated, using Capsicum to reinforce sandbox (2\nlines).\n- gzip.\no Fork/exec sandboxed child process, feed it data using RPC over pipes.\no Non-‐trivial changes, mostly to marshal/unmarshal data for RPC: 409 LoC.\no Interesting bug: forgot to propagate compression level at first.\n- Chromium.\no Already privilege-‐separated on other platforms (but not on FreeBSD).\no ~100 LoC to wrap file descriptors for sandboxed processes.\n- OKWS.\n\no What are the various answers to the homework question?\nDoes Capsicum achieve its goals?\n- How hard/easy is it to use?\no Using Capsicum in an application almost always requires app changes.\n§*\n(Many applications tend to open files by pathname, etc.)\n§*\nOne exception: Unix pipeline apps (filters) that just operate on\nFDs.\no Easier for streaming applications that process data via FDs.\no Other sandboxing requires similar changes (e.g., dhclient, Chromium).\no For existing applications, lazy initialization seems to be a problem.\n§*\nNo general-‐purpose solution -‐-‐ either change code or initialize\nearly.\no Suggested plan: sandbox and see what breaks.\n§*\nMight be subtle: gzip compression level bug.\n- What are the security guarantees it provides?\no Guarantees provided to app developers: sandbox can operate only on\nopen FDs.\no Implications depend on how app developer partitions application, FDs.\no User/admin doesn't get any direct guarantees from Capsicum.\no Guarantees assume no bugs in FreeBSD kernel (lots of code), and that the\nCapsicum developers caught all ways to access a resource not via FDs.\n- What are the performance overheads? (CPU, memory)\no Minor overheads for accessing a file descriptor.\no Setting up a sandbox using fork/exec takes O(1msec), non-‐trivial.\no Privilege separation can require RPC / message-‐passing, perhaps\nnoticeable.\n- Adoption?\no In FreeBSD's kernel now, enabled by default (as of FreeBSD 10).\no A handful of applications have been modified to use Capsicum: dhclient,\ntcpdump, and a few more since the paper was written (Ref:\nhttp://www.cl.cam.ac.uk/research/security/capsicum/freebsd.html)\no Casper daemon to help applications perform non-‐capability operations.\n§*\nE.g., DNS lookups, look up entries in /etc/passwd, etc.\n§*\nhttp://people.freebsd.org/~pjd/pubs/Capsicum_and_Casper.pdf\no There's a port of Capsicum to Linux (but not in upstream kernel repo).\nWhat applications wouldn't be a good fit for Capsicum?\n- Apps that need to control access to non-‐kernel-‐managed objects.\no E.g.: X server state, DBus, HTTP origins in a web browser, etc.\no E.g.: a database server that needs to ensure DB file is in correct format.\no Capsicum treats pipe to a user-‐level server (e.g., X server) as one cap.\n- Apps that need to connect to specific TCP/UDP addresses/ports from sandbox.\no Capsicum works by only allowing operations on existing open FDs.\no Need some other mechanism to control what FDs can be opened.\n\no\nPossible solution: helper program can run outside of capability mode,\nopen TCP/UDP sockets for sandboxed programs based on policy.\nReferences:\n-\nhttp://reverse.put.as/wp-‐content/uploads/2011/09/Apple-‐Sandbox-‐Guide\nv1.0.pdf\n-\nhttp://git.kernel.org/?p=linux/kernel/git/torvalds/linux\n2.6.git;a=blob;f=Documentation/prctl/seccomp_filter.txt;hb=HEAD\n-\nhttp://en.wikipedia.org/wiki/Mandatory_Integrity_Control\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/fae7e5f49a2aac3030eb1c8fab2731da_MIT6_858F14_lec7.pdf",
      "content": "6.858 Lecture 7\nNative Client\nWhat's the goal of this paper?\n-\nAt the time, browsers allowed any web page to run only JS (+Flash) code.\n-\nWant to allow web apps to run native (e.g., x86) code on user's machine.\no\nDon't want to run complex code on server.\no\nRequires lots of server resources, incurs high latency for users.\n-\nWhy is this useful?\no\nPerformance.\no\nLanguages other than JS.\no\nLegacy apps.\n-\nActually being used in the real world.\no\nShips as part of Google Chrome: the NaCl runtime is a browser extension.\no\nWeb page can run a NaCl program much like a Flash program.\no\nJavascript can interact with the NaCl program by passing messages.\no\nNaCl also provides strong sandboxing for some other use cases.\n-\nCore problem: sandboxing x86 code.\nUsing native client:\n-\nhttps://developers.google.com/native-client/\n-\nInstall browser plug in\n-\nUse Nacl tool change to compile C or C++ program\no\nThere are restrictions on what system calls you can use\no\nExample app: games (don't need much systems support)\no\nSpecial interface to talk to browser (in release called Pepper)\n-\nMake a web page that includes Nacl module:\n<embed name=\"nacl_module\"\nid=\"hello_world\"\nwidth=0 height=0\nsrc=\"hello_world.nmf\"\ntype=\"application/x-nacl\" />\n-\nModule is \"controled\" x86 code.\nQuick demo:\n% urxvt -fn xft:Monospace-20\n% export NACL_SDK_ROOT=/home/nickolai/tmp/nacl_sdk/pepper_35\n% cd ~/6.858/git/fall14/web/lec/nacl-demo\n## this is from NaCl's tutorial part1\n% vi hello.cc\n% vi index.html\n% make\n% make serve\n\n## copy-paste and add --no-dir-check as the error message asks\n## visit http://localhost:5103/\n## change hello.cc to \"memset(buf, 'A', 1024);\"\n% make\n% !python\n## visit http://localhost:5103/\n## ctrl-shift-J, view console\nWhat are some options for safely running x86 code?\nApproach 0: trust the code developer.\n-\nActiveX, browser plug-‐ins, Java, etc.\n-\nDeveloper signs code with private key.\n-\nAsks user to decide whether to trust code from some developer.\n-\nUsers are bad at making such decisions (e.g., with ActiveX code).\no\nWorks for known developers (e.g., Windows Update code, signed by MS).\no\nUnclear how to answer for unknown web applications (other than \"no\").\n-\nNative Client's goal is to enforce safety, avoid asking the user.\nApproach 1: hardware protection / OS sandboxing.\n-\nSimilar plan to some ideas we've already read: OKWS, Capsicum, VMs, ..\n-\nRun untrusted code as a regular user-‐space program or a separate VM.\n-\nNeed to control what system calls the untrusted code can invoke.\no\nLinux: seccomp.\no\nFreeBSD: Capsicum.\no\nMacOSX: Seatbelt.\no\nWindows: unclear what options exist.\n-\nNative client uses these techniques, but only as a backup plan.\n-\nWhy not rely on OS sandboxing directly?\no\nEach OS may impose different, sometimes incompatible requirements.\n§*\nSystem calls to allocate memory, create threads, etc.\n§*\nVirtual memory layout (fixed-‐address shared libraries in\nWindows?).\no\nOS kernel vulnerabilities are reasonably common.\n§*\nAllows untrusted code to escape sandbox.\no\nNot every OS might have a sufficient sandboxing mechanism.\n§*\nE.g., unclear what to do on Windows, without a special kernel\nmodule.\n§*\nSome sandboxing mechanisms require root: don't want to run\nChrome as root.\no\nHardware might have vulnerabilities (!).\n§*\nAuthors claim some instructions happen to hang the hardware.\n§*\nWould be unfortunate if visiting a web site could hang your\ncomputer.\n\nApproach 2: software fault isolation (Native Client's primary sandboxing\nplan).\n-\nGiven an x86 binary to run in Native Client, verify that it's safe.\no\nVerification involves checking each instruction in the binary.\no\nSome instructions might be always safe: allow.\no\nSome instructions might be sometimes safe.\n§*\nSoftware fault isolation's approach is to require a check before\nthese.\n-\nMust ensure the check is present at verification time.\n§*\nAnother option: insert the check through binary rewriting.\n-\nHard to do with x86, but might be more doable with higher\nlevel lang.\no\nSome instructions might be not worth making safe: prohibit.\n-\nAfter verifying, can safely run it in same process as other trusted code.\n-\nAllow the sandbox to call into trusted \"service runtime\" code. (Figure 2 from\npaper)\nWhat does safety mean for a Native Client module?\n-\nGoal #1: does not execute any disallowed instructions (e.g., syscall, int).\no\nEnsures module does not perform any system calls.\n-\nGoal #2: does not access memory or execute code outside of module boundary.\no\nEnsures module does not corrupt service runtime data structures.\no\nEnsures module does not jump into service runtime code, ala return-‐to\nlibc.\no\nAs described in paper, module code+data live within [0..256MB) virt\naddrs.\n§*\nNeed not populate entire 256MB of virtual address space.\no\nEverything else should be protected from access by the NaCl module.\nHow to check if the module can execute a disallowed instruction?\n-\nStrawman: scan the executable, look for \"int\" or \"syscall\" opcodes.\no\nIf check passes, can start running code.\no\nOf course, need to also mark all code as read-‐only.\no\nAnd all writable memory as non-‐executable.\n-\nComplication: x86 has variable-‐length instructions.\no\n\"int\" and \"syscall\" instructions are 2 bytes long.\no\nOther instructions could be anywhere from 1 to 15 bytes.\n-\nSuppose program's code contains the following bytes:\n25 CD 80 00 00\n-\nIf interpreted as an instruction starting from 25, it is a 5-‐byte instr:\nAND %eax, $0x000080cd\n\n-\nBut if interpreted starting from CD, it's a 2-‐byte instr:\nINT $0x80\n# Linux syscall\n-\nCould try looking for disallowed instructions at every offset..\no\nLikely will generate too many false alarms.\no\nReal instructions may accidentally have some \"disallowed\" bytes.\nReliable disassembly.\n-\nPlan: ensure code executes only instructions that verifier knows about.\n-\nHow can we guarantee this? Table 1 and Figure 3 in paper.\n-\nScan forward through all instructions, starting at the beginning.\n-\nIf we see a jump instruction, make sure it's jumping to address we saw.\n-\nEasy to ensure for static jumps (constant addr).\n-\nCannot ensure statically for computed jumps (jump to addr from register)\nComputed jumps.\n-\nIdea is to rely on runtime instrumentation: added checks before the jump.\n-\nFor computed jump to %eax, NaCl requires the following code:\nAND $0xffffffe0, %eax\nJMP *%eax\n-\nThis will ensure jumps go to multiples of 32 bytes.\n-\nNaCl also requires that no instructions span a 32-‐byte boundary.\n-\nCompiler's job is to ensure both of these rules.\no\nReplace every computed jump with the two-‐instruction sequence above.\no\nAdd NOP instructions if some other instruction might span 32-‐byte\nboundary.\no\nAdd NOPs to pad to 32-byte multiple if next instr is a computed jump\ntarget.\no\nAlways possible because NOP instruction is just one byte.\n-\nVerifier's job is to check these rules.\no\nDuring disassembly, make sure no instruction spans a 32-‐byte boundary.\no\nFor computed jumps, ensure it's in a two-‐instruction sequence as above.\n-\nWhat will this guarantee?\no\nVerifier checked all instructions starting at 32-‐byte-‐multiple addresses.\no\nComputed jumps can only go to 32-‐byte-‐multiple addresses.\n-\nWhat prevents the module from jumping past the AND, directly to the JMP?\no\nPseudo-‐instruction.\n-\nHow does NaCl deal with RET instructions?\no\nProhibited -‐-‐ effectively a computed jump, with address stored on stack.\no\nInstead, compiler must generate explicit POP + computed jump code.\nWhy are the rules from Table 1 necessary?\n\n-\nC1: executable code in memory is not writable.\n-\nC2: binary is statically linked at zero, code starts at 64K.\n-\nC3: all computed jumps use the two-‐instruction sequence above.\n-\nC4: binary is padded to a page boundary with one or more HLT instruction.\n-\nC5: no instructions, or our special two-‐instruction pair, can span 32 bytes.\n-\nC6/C7: all jump targets reachable by fall-‐through disassembly from start.\nHomework Q: what happens if verifier gets some instruction length wrong?\nHow to prevent NaCl module from jumping to 32-‐byte multiple outside its code?\n-\nCould use additional checks in the computed-‐jump sequence.\n-\nE.g.:\nAND $0x0fffffe0, %eax\nJMP *%eax\nWhy don't they use this approach?\n-\nLonger instruction sequence for computed jumps.\n-\nTheir sequence is 3+2=5 bytes, above sequence is 5+2=7 bytes.\n-\nAn alternative solution is pretty easy: segmentation\nSegmentation.\n-\nx86 hardware provides \"segments\".\n-\nEach memory access is with respect to some \"segment\".\no\nSegment specifies base + size.\n-\nSegments are specified by a segment selector: ptr into a segment table.\n%cs, %ds, %ss, %es, %fs, %gs\no\nEach instruction can specify what segment to use for accessing memory.\no\nCode always fetched using the %cs segment.\n-\nTranslation: (segment selector, addr) -‐> (segbase + addr % segsize).\n-\nTypically, all segments have base=0, size=max, so segmentation is a no-‐op.\n-\nCan change segments: in Linux, modify_ldt() system call.\n-\nCan change segment selectors: just \"MOV %ds\", etc.\nLimiting code/data to module's size.\n-\nAdd a new segment with offset=0, size=256MB.\n-\nSet all segment selectors to that segment.\n-\nModify verifier to reject any instructions that change segment selectors.\n-\nEnsures all code and data accesses will be within [0..256MB).\n-\n(NaCl actually seems to limit the code segment to the text section size.)\nWhat would be required to run Native Client on a system without segmentation?\n-\nFor example, AMD/Intel decided to drop segment limits in their 64-‐bit CPUs.\n\n- One practical possibility: run in 32-‐bit mode.\no AMD/Intel CPUs still support segment limits in 32-‐bit mode.\no Can run in 32-‐bit mode even on a 64-‐bit OS.\n- Would have to change the computed-‐jump code to limit target to 256MB.\n- Would have to add runtime instrumentation to each memory read/write.\n- See the paper in additional references below for more details.\nWhy doesn't Native Client support exceptions for modules?\n- What if module triggers hardware exception: null ptr, divide-‐by-‐zero, etc.\n- OS kernel needs to deliver exception (as a signal) to process.\n- But Native Client runs with an unusual stack pointer/segment selector.\n- Some OS kernels refuse to deliver signals in this situation.\n- NaCl's solution is to prohibit hardware exceptions altogether.\n- Language-‐level exceptions (e.g., C++) do not involve hardware: no problem\nWhat would happen if the NaCl module had a buffer overflow?\n- Any computed call (function pointer, return address) has to use 2-‐instr jump.\n- As a result, can only jump to validated code in the module's region.\n- Buffer overflows might allow attacker to take over module.\n- However, can't escape NaCl's sandbox.\nLimitations of the original NaCl design?\n- Static code: no JIT, no shared libraries.\n- Dynamic code supported in recent versions (see additional refs at the end).\nInvoking trusted code from sandbox.\n- Short code sequences that transition to/from sandbox located in [4KB..64KB).\n- Trampoline undoes the sandbox, enters trusted code.\no Starts at a 32-‐byte multiple boundary.\no Loads unlimited segment into %cs, %ds segment selectors.\no Jumps to trusted code that lives above 256MB.\no Slightly tricky: must ensure trampoline fits in 32 bytes.\no (Otherwise, module could jump into middle of trampoline code..)\no Trusted code first switches to a different stack: why?\no Subsequently, trusted code has to re-‐load other segment selectors.\n- Springboard (re-‐)enters the sandbox on return or initial start.\no Re-‐set segment selectors, jump to a particular address in NaCl module.\no Springboard slots (32-‐byte multiples) start with HLT.\no Prevents computed jumps into springboard by module code.\nWhat's provided by the service runtime? NaCl's \"system call\" equivalent.\n- Memory allocation: sbrk/mmap.\n- Thread operations: create, etc.\n- IPC: initially with Javascript code on page that started this NaCl program.\n\n-\nBrowser interface via NPAPI: DOM access, open URLs, user input, ..\n-\nNo networking: can use Javascript to access network according to SOP.\nHow secure is Native Client?\n-\nList of attack surfaces: start of section 2.3.\n-\nInner sandbox: validator has to be correct (had some tricky bugs!).\n-\nOuter sandbox: OS-‐dependent plan.\no\nOn Linux, probably seccomp.\no\nOn FreeBSD (if NaCl supported it), Capsicum would make sense.\n-\nWhy the outer sandbox?\no\nPossible bugs in the inner sandbox.\n-\nWhat could an adversary do if they compromise the inner sandbox?\no\nExploit CPU bugs.\no\nExploit OS kernel bugs.\no\nExploit bugs in other processes communicating with the sandbox proc.\n-\nService runtime: initial loader, runtime trampoline interfaces.\n-\nIMC interface + NPAPI: complex code, can (and did) have bugs.\nHow well does it perform?\n-\nCPU overhead seems to be dominated by NaCl's code alignment requirements.\no\nLarger instruction cache footprint.\no\nBut for some applications, NaCl's alignment works better than gcc's.\n-\nMinimal overhead for added checks on computed jumps.\n-\nCall-‐into-‐service-‐runtime performance seems comparable to Linux syscalls.\nHow hard is it to port code to NaCl?\n-\nFor computational things, seems straightforward: 20 LoC change for H.264.\n-\nFor code that interacts with system (syscalls, etc), need to change them.\no\nE.g., Bullet physics simulator (section 4.4).\nAdditional references.\n-\nNative Client for 64-‐bit x86 and for ARM.\no\nhttp://static.usenix.org/events/sec10/tech/full_papers/Sehr.pdf\n-\nNative Client for runtime-‐generated code (JIT).\no\nhttp://research.google.com/pubs/archive/37204.pdf\n-\nNative Client without hardware dependence.\no\nhttp://css.csail.mit.edu/6.858/2012/readings/pnacl.pdf\n-\nOther software fault isolation systems w/ fine-‐grained memory access control.\no\nhttp://css.csail.mit.edu/6.858/2012/readings/xfi.pdf\no\nhttp://research.microsoft.com/pubs/101332/bgii-sosp.pdf\n-\nFormally verifying the validator.\no\nhttp://www.cse.lehigh.edu/~gtan/paper/rocksalt.pdf\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/e59ea1c3e94bd1fb73e362ceabb8d7f9_MIT6_858F14_lec8.pdf",
      "content": "6.858 Lecture 8\nWeb Security\nWhat is the web? In the old days, it was a simple client/server architecture (client\nwas your web browser, server was a machine on the network that could deliver\nstatic text and images to your browser).\n-\nIn the old days, the server-‐side was much more complex than the client-‐side:\nbrowsers didn't support rich interactivity, but the server might interface with\ndatabases,other servers, etc.\n-\nBecause the server was so much more complicated, \"web security\" focused on\nthe server-‐side. Up to this point, this class has largely focused on the server-‐side\nas well (e.g., buffer overflows on web servers, privilege separation in the OKWS\nserver).\nThe web has changed: now the browser is very complicated.\n-\nJavaScript: Allows a page to execute client-‐side code.\n-\nDOM model Provides a JavaScript interface to the page's HTML, allowing the\npage to add/remove tags, change their styling, etc.\n-\nXMLHttpRequests (AJAX): Asynchronous HTTP requests.\n-\nWeb sockets: Full-‐duplex client-‐server communication over TCP.\n-\nWeb workers: Multi-‐threading support.\n-\nMultimedia support: <video>, web cams, screen-‐sharing.\n-\nGeolocation: Browser can determine your location by examining GPS units.\nFirefox can also locate you by passing your WiFi information to the Google\nLocation Service.\n-\n<canvas> and WebGL: Bitmap manipulation and interactive 2D/3D graphics.\n-\nNacl: Allows browsers to run native code!\nThe web is now a complex platform for distributed computation! But what does this\nmean for security?\n-\nThe threat surface is huge!\n-\nA single web application now spans multiple programming languages, OSes,\nhardware platforms. I might be running Firefox on Windows interacting with a\nLinux server running Apache and interfacing with memcached and MySQL).\n-\nAll of this composition makes it difficult to verify end-‐to-‐end correctness, or even\nunderstand what the system is doing. Ex: Parsing contexts and content\nsanitization.\n<script> var x = 'UNTRUSTED'; </script>\n//Single quote breaks out of JS string\n//context into JS context\n//\n//\"</script>\" breaks out of JS context\n//into HTML context\n\n-\nThe web specs are incredibly long, very complex, occasionally contradictory, and\nconstantly evolving.\no\nSo, browser vendors do something that roughly resembles the specs and\nthen laugh about it with their friends.\no\nIf you want to understand the horror, go to quirksmode.org.\nIn this lecture, we're going to focus on the client-side of a web application. In\nparticular, we're going to focus on how to isolate content from different providers\nthat has to reside within the same browser.\n-\nBig difference between a web application and a traditional desktop application:\nthe bits in a desktop application typically come from a single vendor (e.g.,\nMicrosoft or Apple or TurboTax), but a single web application contains content\nfrom a bunch of different principals!\n+--------------------------------------------+\n| +--------------------------------------+ |\n| |\nad.gif from ads.com\n| |\n| +--------------------------------------+ |\n| +-----------------+ +------------------+ |\n| | Analytics .js\n| | jQuery.js from\n| |\n| | from google.com | | from cdn.foo.com | |\n| +-----------------+ +------------------+ |\n|\n|\n|\nHTML (text inputs, buttons)\n|\n|\n|\n| +--------------------------------------+ |\n| | Inline .js from foo.com (defines\n| |\n| | event handlers for HTML GUI inputs) | |\n| +--------------------------------------+ |\n|+------------------------------------------+|\n|| frame: https://facebook.com/likeThis.html||\n||\n||\n|| +----------------------+ +--------------+||\n|| | Inline .js from\n| | f.jpg from https://\n|| | https://facebook.com | | facebook.com |||\n|| +----------------------+ +--------------+||\n||\n||\n|+------------------------------------------+|\n|\n|\nQuestion: Which pieces of JavaScript code can access which pieces of state? For\nexample...\n\n-\n\n-\n\n!\n\n\"\n# $ $\n-\n\n%\"&' ()#!\n* !\n-\n! +*\n,\n+* -..\n-..\n\" /\n\n-\n1 -\"\n#\n-\n*\n\no\n3!\n- 4 4 !\n! !\n\no\n3!\n- ,! 0\n\n!\n\n02(- 5 6 &\n\n02(- 5!\n\n02(- 7 +* 8 *8 9\no\n%\n- 4 ! : !\n! ; !\n-\n<\n-\" !\n! !\n\n-\n, - = =\n-\n+ ( -\no\n-...(\n7 >? 9\no\n-...(\n7 @@A 9\no\n-..->B>B.( 7>B>B9\n-\n\n-\n+ -\nB 2 0 7 * ,3&\n! ,3& !\n*9\n\n5 /! C4, C(\nD 2 CE' 5 /!\nC(\nA\n(\n%\"&' #\n\" $$\n( F C(\n- E\n#\n\n#\n\n4. Passive content (e.g., images and CSS) can't execute code, so this content\nis given zero authority.\n-\nReturning to our example:\no\nThe Google analytics script and the jQuery script can access all the\nresources belonging to foo.com (e.g., they can read and write cookies,\nattach event handlers to buttons, manipulate the DOM tree, access\nJavaScript variables, etc.).\no\nJavaScript code in the Facebook frame has no access to resources in the\nfoo.com frame, because the two frames have different origins. The two\nframes can only talk via postMessage(), a JavaScript API that allows\ndomains to exchange immutable strings.\n§*\nIf the two frames *were* in the same origin, they could use\nwindow.parent and window.frames[] to directly interact with each\nother's JavaScript state!\no\nJavaScript code in the Facebook frame cannot issue an XMLHttpRequest\nto foo.com's server [the network is a resource with an origin!] . . .\no\nHowever, the Facebook frame *can* import scripts, CSS, or images from\nfoo.com (although that content can only update the Facebook frame, since\nthe content inherits the authority of the Facebook origin, not foo.com\norigin).\no\nThe browser checks the type of ad.gif, determines that ad.gif is a image,\nand concludes that the image should receive no authority at all.\nWhat happens if the browser mistakenly identifies the MIME type of an object?\n-\nOld versions of IE used to do MIME sniffing.\no\nGoal: Detect when a web server has given an incorrect file extension to an\nobject (e.g., foo.jpg should actually be foo.html).\no\nMechanism: IE looks at the first 256 bytesof the file and looks for magic\nvalues which indicate a file type. If there's a disagreement between the\nmagic values and the file extension, IE trusts the file extension.\no\nProblem: Suppose that a page includes some passive content (e.g., an\nimage) from an attacker-‐controlled domain. The victim page thinks that\nit's safe to import passive content, but the attacker can intentionally put\nHTML+JavaScript in the image and execute code in the victim page!\n-\nMoral: Browsers are complex-‐-‐-‐adding a well-‐intentioned feature may cause\nsubtle and unexpected security bugs.\nLet's take a deeper look at how the browser secures various resources.\nFrame/window objects\n-\nNote: A frame object is a DOM node of type HTMLIFrameElement, whereas the\nwindow object is the alias for the global JavaScript namespace. Both objects have\nreferences to each other.\n-\nGet the origin of their frame's URLs\n-‐OR-‐\n\n-\nGet the origin of the adjusted document.domain\no\nA frame's document.domain is originally derived from the URL in the\nnormal way.\no\nA frame can set document.domain to be a suffix of the full domain. Ex:\n§*\nx.y.z.com\n//Original value\n§*\ny.z.com\n//Allowable new value\n§*\nz.com\n//Allowable new value\n§*\na.y.z.com\n//Disallowed\n§*\n.com\n//Disallowed\no\nBrowsers distinguish between a document.domain that has been written,\nand one that has not, even if both have the same value! Two frames can\naccess each other if:\no\nThey have both set document.domain to the same\nvalue, or\no\nNeither has changed document.domain (and those\nvalues are equal in both frames)\no\nThese rules help protect a site from being attacked by a buggy/malicious\nsubdomain, e.g., x.y.z.com trying to attack y.z.com by shortening its\ndocument.domain.\nDOM nodes\n-\nGet the origin of their surrounding frame\nCookies\n-\nA cookie has a domain AND a path. Ex: *.mit.edu/6.858/\no\nDomain can only be a (possibly full) suffix of a page's current domain.\no\nPath can be \"/\" to indicate that all paths in the domain should have access\nto the cookie.\n-\nWhoever sets cookie gets to specify the domain and path.\no\nCan be set by the server using a header, or by JavaScript code that writes\nto document.cookie.\no\nThere's also a \"secure\" flag to indicate HTTPS-‐only cookies.\n-\nBrowser keeps cookies on client-‐side disk (modulo cookie expiration, ephemeral\ncookies, etc.).\n-\nWhen generating an HTTP request, the browser sends all matching cookies in\nthe request.\no\nSecure cookies only sent for HTTPS requests.\n-\nJavaScript code can access any cookie that match the code's origin, but note that\nthe cookie's path and the origin's port are ignored!\no\nThe protocol matters, because HTTP JavaScript cannot access HTTPS\ncookies (although HTTPS JavaScript can access both kinds of cookies).\n-\nQ: Why is it important to protect cookies from arbitrary overwriting?\n\n-\nA: If an attacker controls a cookie, the attacker can force the user to use an\naccount that's controlled by an attacker!\no\nEx: By controlling a Gmail cookie, an attacker can redirect a user to an\nattacker controlled account and read any emails that are sent from that\naccount.\n-\nQ: Is it valid for foo.co.uk to set a cookie's domain to co.uk?\n-\nA: This is valid according to the rules that we've discussed so far, but in practice,\nwe should disallow such a thing, since \".co.uk\" is semantically a single, \"atomic\"\ndomain like \".com\". Mozilla maintains a public list which allows browsers to\ndetermine the appropriate suffix rules for top-level domains.\n[https://publicsuffix.org]\nHTTP responses: Many exceptions and half-‐exceptions to same-‐origin policy.\n-\nXMLHttpRequests: By default, JavaScript can only send XMLHttpRequests to its\norigin server... unless the remote server has enabled Cross-‐origin Resource\nSharing (CORS). The scheme defines some new HTTP response headers:\no\nAccess-‐Control-‐Allow-‐Origin specifies which origins can see HTTP\nresponse.\no\nAccess-‐Control-‐Allow-‐Credentials specifies if browser should accept\ncookies in HTTP request from the foreign origin.\n-\nImages: A frame can load an image from any origin... but it can't look at the\nimage pixels... but it can determine the image's size.\n-\nCSS: Similar story to images-‐-‐a frame can't directly read the content of external\nCSS files, but can infer some of its properties.\n-\nJavaScript: A frame can load JavaScript from any origin . . . but it can't directly\nexamine the source code in a <script> tag/XMLHttpRequest response body . . .\nbut all JavaScript functions have a public toString() method which reveals source\ncode... and a page's home server can always fetch the source code directly and\nthen pass it to the page!\no\nTo prevent reverse-‐engineering, many sites minify and obfuscate their\nJavaScript.\n-\nPlugins: A frame can run a plugin from any origin.\no\n<embed src=...> // Requires plugin-‐specific elaborations.\nRemember that, when the browser generates an HTTP request, it automatically\nincludes the relevant cookies.\n-\nWhat happens if the browser creates a frame with a URL like this?\no\nhttp://bank.com/xfer?amount=500&to=attacker\n-\nThis attack is called a cross-‐site request forgery (CSRF).\n-\nSolution: Include some random data in URLs that is difficult for the attacker\nto guess. Ex:\n<form action=\"/transfer.cgi\" ...>\n<input type=\"hidden\"\nname=\"csrfToken\"\n\nvalue=\"a6dbe323...\"/>\n- Each time a user requests the page, the server generates HTML with new\nrandom tokens. When the user submits a request, the server validates the\ntoken before actually processing the request.\n- Drawback: If each URL to the same object is unique, it's difficult to cache that\nobject!\nNetwork addresses almost have an origin.\n- A frame can send HTTP *and* HTTPS requests to a host+port that match its\norigin.\n- Note that the security of the same-‐origin policy depends on the integrity of the\nDNS infrastructure!\n- DNS rebinding attack\no Goal: Attacker wants to run attacker-‐controlled JavaScript code with the\nauthority of an origin that he does not control (victim.com).\no Approach:\n1) Attacker registers a domain name (e.g., attacker.com) and creates\na DNS server to respond to the relevant queries.\n2) User visits the attacker.com website, e.g., by clicking on an\nadvertisement.\n3) The attacker website wants to downloads a single object, but first,\nthe browser must issue a DNS request for attacker.com. The\nattacker's DNS server responds with a DNS record to the attacker's\nIP address. However, the record has a short time-‐to-‐live.\n4) The attacker rebinds attacker.com to the IP address of victim.com.\n5) A bit later, the attacker website creates an XMLHttpRequest that\nconnects to attacker.com. That request will actually be sent to the\nIP address of victim.com! The browser won't complain because it\nwill revalidate the DNS record and see the new binding.\n6) Attacker page can now exfiltrate data, e.g., using CORS\nXMLHttpRequest to the attacker domain.\no Solutions:\n§*\nModify DNS resolvers so that external hostnames can never\nresolve to internal IP addreses.\n§*\nBrowsers can pin DNS bindings, regardless of their TTL settings.\nHowever, this may break web applications that use dynamic DNS\n(e.g., for load-‐balancing).\nWhat about the pixels on a screen?\n- They don't have an origin! A frame can draw anywhere within its bounding box.\n- Problem: A parent frame can overlay content atop the pixels of its child frames.\no Ex: At attacker creates a page which has an enticing button like \"Click\nhere for a free iPad!\" Atop that button, the page creates a child frame that\ncontains the Facebook \"Like\" button. The attacker places that button atop\n\nthe \"free iPad\" button, but makes it transparent! So, if the user clicks on\nthe \"free iPad\" button, he'll actually \"Like\" the attackers page on\nFacebook.\n-\nSolutions\n1) Frame-‐busting code: Include JavaScript that prevents your page from\nbeing included as a frame. Ex: if(top != self)\n2) Have your web server send the X-‐Frame-‐Options HTTP response header.\nThis will instruct the browser not to put your content in a child frame.\nWhat about frame URLs that don't have an origin?\nEx:\nfile://foo.txt\nabout:blank\njavascript:document.cookie=\"x\"\n-\nSometimes the frame is only accessible to other frames with that protocol (e.g.,\nfile://). [This can be irritating if you're debugging a site and you want to mix\nfile:// and http:// content].\n-\nSometimes the frame is just inaccessible to all other origins (e.g., \"about:\").\n-\nSometimes the origin is inherited from whoever created the URL (e.g.,\n\"javascript:\"). This prevents attacks in which a attacker.com creates a frame\nbelonging to victim.com, and then navigates the victim frame to a javascript:\nURL-‐-‐we don't want the JavaScript to execute in the context of victim.com!\nNames can be used as an attack vector!\n-\nIDN: internationalized domain names (non-‐latin letters).\n-\nSupporting more languages is good, but now, it can be difficult for users to\ndistinguish two domain names from each other.\n- *Ex: The Cyrillic \"C\" character looks like the Latin \"C\" character! So, an attacker\ncan buy a domain like \"cats.com\" (with a Cyrillic \"C\") and trick users who\nthought that they were going to \"cats.com\" (Latin \"C\").\n-\nGood example of how new features can undermine security assumptions.\n-\nBrowser vendors thought registrars will prohibit ambiguous names.\n-\nRegistrars thought browser vendors will change browser to do something\nPlugins often have subtly-‐different security policies\n-\nJava: Sort of uses the same-‐origin policy, but Java code can set HTTP headers\n(bad! see \"Content-‐Length\" discussion), and in some cases, different hostnames\nwith the same IP address are considered to share the same origin.\n-\nFlash: Developers place a file called crossdomain.xml on their web servers. That\nfile specifies which origins can talk to the server via Flash.\nHTML5 introduces a new screen-‐sharing API: Once the user gives permission, a site\ncan capture the entire visible screen area and transmit it back\nto the site's origin.\n\n-\nSo, if an attacker page can convince the user to grant screen-‐sharing permission,\nthe attacker page can open an iframe to a sensitive site (e.g., banking, Facebook,\nemail), and capture the screen contents!\n-\nThe iframe will send cookies, so the user will automatically be logged in,\nallowing the attacker to see \"real\" information, not boring login page stuff.\n-\nAttacker can make the iframe flash only briefly to prevent the user from noticing\nthe mischief.\n-\nPossible defenses:\no\nAllow users to only screen-‐share part of the DOM tree? It seems like this\nwill be tedious and error-‐prone.\no\nOnly allow an origin to screen-‐capture content from its own origin?\nSeems like a more reasonable approach, although it prevents\n\"The Tangled Web,\" there have been various modifications and additions to\nthe aggregate web stack.\n-\nIn general, things have gotten more complicated, which is typically bad for\nsecurity.\n-\nFor reference, here are some of the new features:\no\nhttp://en.wikipedia.org/wiki/Content_Security_Policy\no\nhttp://en.wikipedia.org/wiki/Strict_Transport_Security\no\nhttp://en.wikipedia.org/wiki/Cross origin_resource_sharing\no\nHTML5 iframe sandbox attribute [http://msdn.microsoft.com/enn\nus/hh563496.aspx]\nThe browser security model is obviously a mess. It's very complex and contains a lot\nof subtleties and inconsistencies.\n-\nQ: Why not rewrite the security model from scratch?\n-\nA1: Backwards compatibility! There's a huge amount of preexisting web\ninfrastructure that people rely on.\n-\nA2: How do we know that a new security model would be expressive\nenough? Users typically do not accept a reduction of features in exchange for\nan increase in security.\n-\nA3: Any security model may be intrinsically doomed-‐-‐-‐perhaps all popular\nsystems are destined to accumulate a ton of features as time progresses. [Ex:\nWord processing programs, smartphones.]\n-\nWhat might a better design look like?\no\nStrict isolation Embassies-‐-‐-‐everything is a network message, even\nlocally\n§*\nhttps://www.usenix.org/system/files/conference/nsdi13/nsd\ni13-final85.pdf\no\nDon't make policy extraction and enforcement dependent on complex\nparsing rules (remember our sanitization example)\nSince\n\no\nand the need for guessing.\nOnly add features in small, clearlyn defined quanta with minimal room\nfor implementation error or interpretation mistakes---remove ambiguity\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/fa42d95747fd5e97ebd57c247f0c6d2a_MIT6_858F14_lec9.pdf",
      "content": "6.858 Lecture 9\nWEB SECURITY: Part II\nLast lecture, we looked at a core security mechanism for the web: the same-‐origin\npolicy. In this lecture, we'll continue to look at how we can build secure web\napplications.\nThe recent \"Shell Shock\" bug is a good example of how difficult it is to design web\nservices that compose multiple technologies.\n-\nA web client can include extra headers in its HTTP requests, and determine\nwhich query parameters are in a request. Ex:\no\nGET /query.cgi?searchTerm=cats HTTP 1.1\no\nHost: www.example.com\no\nCustom-‐header: Custom-‐value\n-\nCGI servers map the various components of the HTTP request to Unix\nenvironment variables.\n-\nVulnerability: Bash has a parsing bug in the way that it handles the setting of\nenvironment variables! If a string begins with a certain set of malformed bytes,\nbash will continue to parse the rest of the string and execute any commands that\nit finds! For example, if you set an environment variable to a value like this...\n() { :;}; /bin/id\n- ...will confuse the bash parser, and cause it to execute the /bin/id command\n(which displays the UID and GID information for the current user).\n-\nLive demo\no\nStep 1: Run the CGI server.\n§*\n./victimwebserver.py 8082\no\nStep 2: Run the exploit script.\n§*\n./shellshockclient.py localhost:8082 index.html\n-\nMore information: http://seclists.org/oss-sec/2014/q3/650\nShell Shock is a particular instance of security bugs which arise from improper\ncontent sanitzation. Another type of content sanitzation failure occurs during cross-‐\nsite scripting attacks (XSS).\nExample: Suppose that a CGI script embeds a query string parameter in the HTML\nthat it generates.\nDemo:\n-\nStep 1: Run the CGI server.\no\n./cgiServer.py\n-\nStep 2: In browser, load these URLs:\nhttp://127.0.0.1:8282/cgi-bin/uploadRecv.py?msg=hello\nhttp://127.0.0.1:8282/cgi-bin/uploadRecv.py?msg=<b>hello</b>\n\nhttp://127.0.0.1:8282/cgi\nbin/uploadRecv.py?msg=<script>alert(\"XSS\");</script>\n//The XSS attack doesn't work for this one . . .\n//we'll see why later in the lecture.\nhttp://127.0.0.1:8282/cgi-bin/uploadRecv.py?msg=<IMG\n\"\"\"><SCRIPT>alert(\"XSS\")</SCRIPT>\">\n//This works! [At least on Chrome 37.0.2062.124.]\n//Even though the browser caught the\n//straightforward XSS injection, it\n//incorrectly parsed our intentionally\n//malformed HTML.\nFor more examples of XSS exploits via malformed code, go here:\nhttps://www.owasp.org/index.php/XSS_Filter_Evasion_Cheat_Sheet\nWhy is cross-‐site scripting so prevalent?\n- Dynamic web sites incorporate user content in HTML pages (e.g., comments\nsections).\n- Web sites host uploaded user documents.\no HTML documents can contain arbitrary Javascript code!\no Non-‐HTML documents may be content-‐sniffed as HTML by browsers.\n- Insecure Javascript programs may directly execute code that comes from\nexternal parties (e.g., eval(), setTimeout(), etc.).\nXSS defenses\n- Chrome and IE have a built-‐in feature which uses heuristics to detect potential\ncross-‐site scripting attacks.\no Ex: Is a script which is about to execute included in the request that\nfetched the enclosing page?\n§*\nhttp://foo.com?q=<script src=\"evil.com/cookieSteal.js\"/>\no If so, this is strong evidence that something suspicious is about to\nhappen! The attack above is called a \"reflected XSS attack,\" because the\nserver \"reflects\" or \"returns\" the attacker-‐supplied code to the user's\nbrowser, executing it in the context of the victim page.\n§*\nThis is why our first XSS attack in the CGI example didn't work--\nthe browser detected reflected JavaScript in the URL, and removed\nthe trailing </script> before it even reached the CGI server.\n§*\nHowever . . .\no Filters don't have 100% coverage, because there are a huge number of\nways to encode an XSS attack!\nhttps://www.owasp.org/index.php/XSS_Filter_Evasion_Cheat_Sheet\n§*\nThis is why our second XSS attack succeeded-‐-‐-‐the browser got\nconfused by our intentionally malformed HTML.\no Problem: Filters can't catch persistent XSS attacks in which the server\nsaves attacker-‐provided data, which is then permanently distributed to\nclients.\n§*\nClassic example: A \"comments\" section which allows users to post\nHTML messages.\n\n§*\nAnother example: Suppose that a dating site allows users to\ninclude HTML in their profiles. An attacker can add HTML that will\nrun in a *different* user's browser when that user looks at the\nattacker's profile! Attacker could steal the user's cookie.\n-\nAnother XSS defense: \"httponly\" cookies.\no\nA server can tell a browser that client-‐side JavaScript should not be able\nto access a cookie. [The server does this by adding the \"Httponly\" token to\na \"Set-‐cookie\" HTTP response value.]\no\nThis is only a partial defense, since the attacker can still issue requests\nthat contain a user's cookies (CSRF).\n-\nPrivilege separation: Use a separate domain for untrusted content.\no\nFor example, Google stores untrusted content in googleusercontent.com\n(e.g., cached copies of pages, Gmail attachments).\no\nEven if XSS is possible in the untrusted content, the attacker code will run\nin a different origin.\no\nThere may still be problems if the content in googleusercontent.com\npoints to URLs in google.com.\n-\nContent sanitization: Take untrusted content and encode it in a way that\nconstrains how it can be interpreted.\no\nEx: Django templates: Define an output page as a bunch of HTML that has some\n\"holes\" where external content can be inserted.\n[https://docs.djangoproject.com/en/dev/topics/templates/#automatico\nhtmlo escaping]\no\nA template might contain code like this...\n§*\n<b>Hello {{ name }} </b>\no ... where \"name\" is a variable that is resolved when the page is processed\nby the Django template engine. That engine will take the value of \"name\" (e.g.,\nfrom a usero supplied HTTP query string), and then automatically escape\ndangerous characters. For example:\n§*\nangle brackets < and > -‐-‐> &lt; and &gt;\n§*\ndouble quotes \"\n-‐-‐> &quot;\no\nThis prevents untrusted content from injecting HTML into the rendered\npage.\no\nTemplates cannot defend against all attacks! For example . . .\n§*\n<div class={{ var }}>...</div>\no ...if var equals...\n§*\n'class1 onmouseover=javascript:func()'\no ...then there may be an XSS attack, depending on how the browser parses\nthe malformed HTML.\no\nSo, content sanitization kind-‐of works, but it's extremely difficult to parse\nHTML in an unambigous way.\no\nPossibly better approach: Completely disallow externally-‐provided\nHTML, and force external content to be expressed in a smaller language\n(e.g., Markdown: http://daringfireball.net/projects/markdown/syntax).\nValidated Markdown can then be translated into HTML.\n\n-\nContent Security Policy (CSP): Allows a web server to tell the browser which\nkinds of resources can be loaded, and the allowable origins for those resources.\no\nServer specifies one or more headers of the type \"Content-‐Security\nPolicy\".\no\nExample:\n§*\nContent-‐Security-‐Policy: default-‐src 'self' *.mydomain.com\n-\nOnly allow content from the page's domain and its\nsubdomains.\no\nYou can specify separate policies for where images can come from, where\nscripts can come from, frames, plugins, etc.\no\nCSP also prevents inline JavaScript, and JavaScript interfaces like eval()\nwhich allow for dynamic JavaScript generation.\n-\nSome browsers allow servers to disable content-‐type sniffing (X-‐Content-‐Type\nOptions: nosniff).\nSQL injection attacks.\n-\nSuppose that the application needs to issue SQL query based on user input:\no\nquery = \"SELECT * FROM table WHERE userid=\" + userid\n-\nProblem: adversary can supply userid that changes SQL query structure\no e.g.,\"0; DELETE FROM table;\"\n-\nWhat if we add quoting around userid?\no\nquery = \"SELECT * FROM table WHERE userid='\" + userid + \"'\"\n-\nThe vulnerability still exists! The attacker can just add another quote as first\nbyte of userid.\n-\nReal solution: unambiguously encode data.\n-\nEx: replace ' with \\', etc.\no\nSQL libraries provide escaping functions.\n-\nDjango defines a query abstraction layer which sits atop SQL and allows\napplications to avoid writing raw SQL (although they can do it if they really want\nto).\n-\n(Possibly fake) German license plate which says \";DROP TABLE\" to avoid\nspeeding cameras which use OCR+SQL to extract license plate number.\nYou can also run into problems if untrusted entities can supply filenames.\n-\nEx: Suppose that a web server reads files based on user-‐supplied parameters.\no\nopen(\"/www/images/\" + filename)\n-\nProblem: filename might look like this:\no\n../../../../../etc/passwd\n-\nAs with SQL injection, the server must sanitize the user input: the server must\nreject file names with slashes, or encode the slashes in some way.\nWhat is Django?\n-\nModerately popular web framework, used by some large sites like Instagram,\nMozilla, and Pinterest.\n\no\nA \"web framework\" is a software system that provides infrastructure for\ntasks like database accesses, session management, and the creation of\ntemplated content that can be used throughout a site.\no\nOther frameworks are more popular: PHP, Ruby on Rails.\no\nIn the enterprise world, Java servlets and ASP are also widely used.\n-\nDjango developers have put some amount of thought into security.\no\nSo, Django is a good case study to see how people implement web\nsecurity in practice.\n-\nDjango is probably better in terms of security than some of the alternatives like\nPHP or Ruby on Rails, but the devil is in the details.\no\nAs we'll discuss two lectures from now, researchers have invented some\nframeworks that offer provably better security.\n§*\n[Ur/Web: http://www.impredicative.com/ur/]\nSession management: cookies.\n(http://pdos.csail.mit.edu/papers/webauth:sec10.pdf\nZoobar, Django, and many web frameworks put a random session ID in the cookie.\n-\nThe Session ID refers to an entry in some session table on the web server. The\nentry stores a bunch of per-‐user information.\n-\nSession cookies are sensitive: adversary can use them to impersonate a user!\n-\nAs we discussed last lecture, the same-‐origin policy helps to protect cookies\n...but you shouldn't share a domain with sites that you don't trust! Otherwise,\nthose sites can launch a session fixation attack:\n1) Attacker sets the session ID in the shared cookie.\n2) User navigates to the victim site; the attacker-‐choosen session ID is sent\nto the server and used to identify the user's session entry.\n3) Later, the attacker can navigate to the victim site using the attacker\nchosen session id, and access the user's state!\n-\nHmmm, but what if we don't want to have server-‐side state for every logged in\nuser?\nStateless cookies\n-\nIf you don't have the notion of a session, then you need to authenticate every\nrequest!\no\nIdea: Authenticate the cookie using cryptography.\no\nPrimitive: Message authentication codes (MACs)\n§*\nThink of it like a keyed hash, e.g., HMAC-‐SHA1: H(k, m)\n§*\n-‐Client and server share a key; client uses key to produce the\nmessage, and the server uses the key to verify the message.\no\nAWS S3 REST Services use this kind of cookie\n[http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthenticatio\nn.html].\n§* Amazon gives each developer an AWS Access Key ID, and an AWS\nsecret key. Each request looks like this:\n\nGET /photos/cat.jpg HTTP/1.1\nHost: johndoe.s3.amazonaws.com\nDate: Mon, 26 Mar 2007 19:37:58 +0000\nAuthorization: AWS\nAKIAIOSFODNN7EXAMPLE:frJIUN8DYpKDtOLCwoyllqDzg=\n|___________________| |________________________|\nAccess key ID\nMAC signature\n§*\nHere's what is signed (this is slightly simplified, see the link above\nfor the full story):\nStringToSign = HTTP-Verb + \"\\n\" +\nContent-MD5 + \"\\n\" +\nContent-Type + \"\\n\" +\nDate + \"\\n\" +\nResourceName\no\nNote that this kind of cookie doesn't expire in the traditional sense\n(although the server will reject the request if Amazon has revoked the\nuser's key).\n§*\nYou can embed an \"expiration\" field in a *particular* request, and\nthen hand that URL to a third-‐party, such that, if the third-‐party\nwaits too long, AWS will reject the request as expired.\nAWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&Expires=1141889120&Sign\nature=vjbyPxybd...\n|__________________|\nIncluded in the string\nthat's covered by the\nsignature!\no\nNote that the format for the string-‐to-‐hash should provide unambiguous\nparsing!§*\n§* Ex: No component should be allowed to embed the escape\ncharacter, otherwise the server-‐side parser may get confused.\n-\nQ: How do you log out with this kind of cookie design?\n-\nA: Impossible, if the server is stateless (closing a session would require a server\nside table of revoked cookies).\n-\nIf server can be stateful, session IDs make this much simpler.\n-\nThere's a fundamental trade-‐off between reducing server-‐side memory state and\nincreasing server-‐side computation overhead for cryptography.\nAlternatives to cookies for session management.\n-\nUse HTML5 local storage, and implement your own authentication in Javascript.\no\nSome web frameworks like Meteor do this.\n\no\nBenefit: The cookie is not sent over the network to the server.\no\nBenefit: Your authentication scheme is not subject to complex same\norigin policy for cookies (e.g., DOM storage is bound to a single origin,\nunlike a cookie, which can be bound to multiple subdomains).\n-\nClient-‐side X.509 certificates.\no\nBenefit: Web applications can't steal or explicitly manipulate each other's\ncertificates.\no\nDrawback: Have weak story for revocation (we'll talk about this more in\nfuture lectures).\no\nDrawback: Poor usability-‐-‐-‐users don't want to manage a certificate for\neach site that they visit!\no\nBenefit/drawback: There isn't a notion of a session, since the certificate is\n\"always on.\" For important operations, the application will have to\nprompt for a password.\nThe web stack has some protocol ambiguities that can lead to security holes.\n-\nHTTP header injection from XMLHttpRequests\no\nJavascript can ask browser to add extra headers in the request. So, what\nhappens if we do this?\nvar x = new XMLHttpRequest();\nx.open(\"GET\", \"http://foo.com\");\nx.setRequestHeader(\"Content-Length\", \"7\");\n//Overrides the browser-computed field!\nx.send(\"Gotcha!\\r\\n\" +\n\"GET /something.html HTTP/1.1\\r\\n\" +\n\"Host: bar.com\");\no\nThe server at foo.com may interpret this as two separate requests! Later,\nwhen the browser receives the second request, it may overwrite a cache\nentry belonging to bar.com with content from foo.com!\no\nSolution: Prevent XMLHttpRequests from setting sensitive fields like\n\"Host:\" or \"Content-‐Length\".\no\nTakehome point: Unambiguous encoding is critical! Build reliable\nescaping/encoding!\n-\nURL parsing (\"The Tangled Web\" page 154)\no\nFlash had a slightly different URL parser than the browser.\no\nSuppose the URL was http://example.com:80@foo.com/\n§*\nFlash would compute the origin as \"example.com\".\n§*\nBrowser would compute the origin as \"foo.com\".\no\nBad idea: complex parsing rules just to determine the principal.\no\nBad idea: re-‐implementing complex parsing code.\n-\nHere's a hilarious/terrifying way to launch attacks using Java applets that are\nstored in the .jar format.\no\nIn 2007, Lifehacker.com posted an article which described how you could\nhide .zip files inside of .gif files.\n\no\nLeverage the fact that image renderers process a file top-‐down, whereas\ndecompressors for .zip files typically start from the end and go upwards.\no\nAttackers realized that .jar files are based on the .zip format!\no\nTHUS THE GIFAR WAS BORN: half-‐gif, half-‐jar, all-‐evil.\n§*\nReally simple to make a GIFAR: Just use \"cat\" on Linux or \"cp\" on\nWindows.\n§*\nSuppose that target.com only allows external parties to upload\nimages objects. The attacker can upload a GIFAR, and the GIFAR\nwill pass target.com's image validation tests!\n§*\nThen, if the attacker can launch a XSS attack, the attacker can inject\nHTML which refers to the \".gif\" as an applet.\n<applet code=\"attacker.class\"\narchive=\"attacker.gif\"\n..>\n§*\nThe browser will load that applet and give it the authority of\ntarget.com!\nWeb applications are also vulnerable to covert channel attacks.\n-\nA covert channel is a mechanism which allows two applications to exchange\ninformation, even though the security model prohibits those applications from\ncommunicating.\no\nThe channel is \"covert\" because it doesn't use official mechanisms for\ncross-‐app communication.\n-\nExample #1: CSS-‐based sniffing attacks\no\nAttacker has a website that he can convince the user to visit.\no\nAttacker goal: Figure out the other websites that the user has visited (e.g.,\nto determine the user's political views, medical history, etc.).\no\nExploit vector: A web browser uses different colors to display visited\nversus unvisited links! So, attacker page can generate a big list of\ncandidate URLs, and then inspect the colors to see if the user has visited\nany of them.\n§*\nCan check thousands of URLs a second!\n§*\nCan go breadth-‐first, find hits for top-‐level domains, then go depth\nfirst for each hit.\no\nFix: Force getComputedStyle() and related JavaScript interfaces to always\nsay that a link is unvisited.§*\n§* https://blog.mozilla.org/security/2010/03/31/plugging-the-css-\nhistory-leak/\n-\nExample #2: Cache-‐based attacks\no *Attacker setup and goal are the same as before.\no *Exploit vector: It's much faster for a browser to access data that's cached\ninstead of fetching it over the network. So, attacker page can generate a\nlist of candidate images, try to load them, and see which ones load\nquickly!\n\no\nThis attack can reveal your location if the candidate images come from\ngeographically specific images, e.g., Google Map tiles.\n§*\nhttp://w2spconf.com/2014/papers/geo_inference.pdf\no\nFix: No good ones. A page could never cache objects, but this will hurt\nperformance. But suppose that a site doesn't cache anything. Is it safe\nfrom history sniffing? No!\n-\nExample #3: DNS-‐based attacks\no\nAttacker setup and goal are the same as before.\no\nExploit vector: Attacker page generates references to objects in various\ndomains. If the user has already accessed objects from that domain, the\nhostnames will already reside in the DNS cache, making subsequent\nobject accesses faster!\n§*\nhttp://sip.cs.princeton.edu/pub/webtiming.pdf\no\nFix: No good ones. Could use raw IP addresses for links, but this breaks a\nlot of things (e.g. DNS-‐based load balancing). However, suppose that a\nsite doesn't cache anything and uses raw IP addresses for hostnames. Is it\nsafe from history sniffing? No!\n-\nExample #4: Rendering attacks.\no\nAttacker setup and goal are the same as before.\no\nExploit vector: Attacker page loads a candidate URL in an iframe. Before\nthe browser has fetched the content, the attacker page can access...\nwindow.frames[1].location.href\no ...and read the value that the attacker set. However, once the browser has\nfetched the content, accessing that reference will return \"undefined\" due\nto the same-‐origin policy. So, the attacker can poll the value and see how\nlong it takes to turn \"undefined\". If it takes a long time, the page must not\nhave been cached!§*\n§* http://lcamtuf.coredump.cx/cachetime/firefox.html\no\nFix: Stop using computers.\nA web page also needs to use postMessage() securely.\n-\nTwo frames from different origins can use postMessage() to asynchronously\nexchange immutable strings.\no\nSender gets a reference to a window object, and does this:\n§*\nwindow.postMessage(msg, origin);\no\nReceiver defines an event handler for the special \"message\" event. The\nevent handler receives the msg and the origin.\n-\nQ: Why does the receiver have to check the origin of received message?\n-\nA: To perform access control on senders! If the receiver implements sensitive\nfunctionality, it shouldn't respond to requests from arbitary\n-\norigins.\no\nCommon mistake: The receiver uses regular expressions to check the\nsender's origin.\n\no\nEven if origin matches /.foo.com/, doesn't mean it's from foo.com! Could\nbe \"xfoo.com\", or \"www.foo.com.bar.com\".\no\nMore details:\nhttps://www.cs.utexas.edu/~shmat/shmat_ndss13postman.pdf\n-\nQ: Why does the sender have to specify the intended origin of the receiver?\n-\nA: postMessage() is applied to a window, not an origin.\no\nRemember that an attacker may be able to navigate a window to a\ndifferent location.\no\nIf the attacker navigates the window, another origin may receive\nmessage!\no\nIf the sender explictly specifies a target origin, the browser checks\nrecipient origin before delivering the msg.\no\nMore details: http://css.csail.mit.edu/6.858/2013/readings/post-\nmessage.pdf\nThere are many other aspects to building a secure web application.\n-\nEx: ensure proper access control for server-‐side operations.\no\nDjango provides Python decorators to check access control rules.\n-\nEx: Maintain logs for auditing, prevent an attacker from modifying the log.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 12",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/b3c3ad23c29b4381e41b619e081cce13_MIT6_858F14_lec12.pdf",
      "content": "6.858 Lecture 12\nTCP/IP security\nThreat model for network security:\n- Adversary can intercept / modify network traffic.\n- Adversary can send packets.\n- Adversary has full control of their own machines.\n- Adversary can participate in protocols (usually).\no Often not feasible to keep bad guys out of a large systems.\nEavesdropping on packets.\n- Important to keep in mind, but relatively well understood.\n- Any data sent over the network can be observed by an adversary.\nSending / spoofing packets.\n- IP allows sender to construct an arbitrary packet.\n- In particular, sender can fill in any source address.\n- Can pretend that a packet is coming from any address.\n- What can an adversary do with this?\nEasy target: trigger bugs in some implementation.\n- Author isn't so interested in this class of problems.\n- Instead, want to look at \"protocol-‐level problems\".\n- What is a protocol-‐level problem?\no A problem inherent in the design.\no A correct implementation will have this problem.\n- Why is it so important?\no Can fix implementation bugs.\no To fix protocol-‐level bugs, might need to change protocol!\no Might be incompatible with existing systems.\no As we will see, sometimes possible to come up with compatible fixes.\nTCP sequence number attack.\nStandard handshake (figure on the right side of page 2):\nC: SRC=C, DST=S, SYN(SNc)\nS: SRC=S, DST=C, SYN(SNs), ACK(SNc)\nC: SRC=C, DST=S, ACK(SNs)\nC: SRC=C, DST=S, data(SNc), ACK(SNs)\nHow does the adversary know the data is coming from the client?\n- Only the client should have been able to receive the second message.\n- Thus, only the client should know SNs.\n- Third message is rejected, unless it has the right SNs value.\nSuppose adversary A wants to simulate a connection to S from C. (Assume A knows\nC's IP address -‐-‐ usually not a big deal in practice.)\n\nA: SRC=C, DST=S, SYN(SNc)\nS: SRC=S, DST=C, SYN(SNs), ACK(SNc)\nA: SRC=C, DST=S, ACK(SNs)\n-- but how to guess SNs?\nA: SRC=C, DST=S, data(SNc)\nWhere does the adversary get SNs?\n- TCP specification suggested a specific way to choose them.\n- In particular, increment at a ~constant rate: ~250,000 per second.\n- Why so specific?\no Subtle interactions with reused connections (src/dst port numbers).\no Want to avoid old packets (from past conns) interfering with new conn.\no [ Ref: RFC 1185 appendix ]\n- If adversary knows a recent sequence number, can guess the next one.\no Impl would actually bump ISN every second, making it easy to guess.\nWhat happens to the real packet that S sends to C (second pkt)?\n- C would assume the packet is from an old conn, send RST in response.\n- Even if that RST was sent, adversary could try to race before RST arrives.\n- Luckily, there was another curious bug; will get to it later.\nBut why do sequence number attacks turn into a security problem?\n1. Spoof connections to applications that rely on IP addresses.\n- E.g., Berkeley remote access tools: rlogin, rsh, rcp.\n- Allowed login without a password, if connection came from a \"trusted\" system.\no Required connection to come from a trusted source port (512-‐1023).\n§*\nWhy this requirement?\no Trusted rlogin/rsh/rcp program sent the client's username.\no If username was the same as the account on the server, no password\nneeded.\no E.g.: \"rsh athena.dialup.mit.edu ls\".\n- Made a bad assumption about what the TCP layer provided.\no Assumed TCP conn from an IP address meant it really came from that\nhost.\n- If adversary can guess SNs, then can simulate connection from trusted host.\no Issue any command using rsh.\no Could change the user's .rhosts file to allow login from attacker's host.\no Then connect directly without having to simulate a connection.\n- Host-‐based authentication seems like a bad plan.\no Especially relying on \"trusted\" vs \"untrusted\" ports on a machine.\no Still in some use today: e.g., SMTP for outgoing mail.\n- Actually rlogin authentication was even worse: they authenticated by hostname.\no Where does hostname come from? Reverse DNS lookup.\no E.g., 18.26.4.9: find the PTR record of 9.4.26.18.in-‐addr.arpa.\no Owner of that domain can set PTR record to any hostname!\no (Can make a slight improvement: check if host resolves to same addr.)\no Similar problems show up in log files: log resolved (untrusted) hostname.\n\n2. Denial of service attack: connection reset.\n- Once we know SNc, can send a RST packet.\n- Worse yet: server will accept a RST packet for any SNc value within window.\n- With a large window (~32K=2^15), only need 2^32/2^15 = 2^17 guesses.\nHow bad is a connection reset?\n- One target of such attacks were the TCP connections between BGP routers.\n- Causes routers to assume link failure, could affect traffic for minutes.\n- Solutions:\no TTL hack (255).\no MD5 header authentication (very specialized for router-‐to-‐router links).\n3. Hijack existing connections.\n- In similar vein, can also inject data into an existing connection.\n- All adversary needs to know is the current SNc.\nHow to mitigate this problem?\n- Baseline: don't rely on IP addresses for authentication.\no Use encryption / authentication at a higher level.\no Next lecture: Kerberos.\no But still, want to fix the situation we're in, for TCP.\n- ISPs can filter packets sent by their customers.\no Often done today for small customers, but not consistently.\no Not straightforward for customers with complex networks,\nmultihoming...\nHow to patch up TCP?\n- Can't choose ISN's in a completely random way, without violating TCP spec.\no Might break connection (port) reuse guarantees.\n- Random increments?\no Should preserve increment rate (~250k/second).\no Not a huge amount of randomness (say, low 8 bits per increment).\n- Aside: must be careful about how we generate random numbers!\no Common PRNG: linear congruential generator: R_k = A*R_{k-‐1}+B mod N.\no Not secure: given one pseudo-‐random value, can guess the next one!\no Lots of better cryptographically secure PRNGs are available.\n§*\nIdeally, use your kernel's built-‐in PRNG (/dev/random\n/dev/urandom)\n§*\nRef: http://en.wikipedia.org/wiki/Fortuna_(PRNG), or any stream\ncipher like http://en.wikipedia.org/wiki/RC4\n- However, SN values for different src/dst pairs never interact!\n- So, can choose the ISN using a random offset for each src/dst pair.\no Nice trick: ISN = ISN_oldstyle + F(srcip, srcport, dstip, dstport, secret)\no F is some pseudo-‐random function; roughly, think SHA1.\n\no\nRequires no extra state to keep track of per-‐connection ISNs.\nAre sequence number attacks still relevant?\n-\nMost operating systems implement the per-‐connection ISN workaround above.\no\nRef: Linux secure_tcp_sequence_number in net/core/secure_seq.c\n-\nBut other protocols suffer from almost identical problems -‐-‐ e.g., DNS.\no\nDNS runs over UDP, no seq numbers, just ports, and dst port fixed (53).\no\nIf adversary knows client is making a query, can fake a response.\n§*\nJust need to guess src port, often predictable.\no\nProblem gained popularity in 2008, though well-‐understood by djb\nbefore.§* §*\n§* Ref: http://cr.yp.to/djbdns/forgery.html\n§* Ref: http://unixwiz.net/techtips/iguide-kminsky-dns-vuln.html\no\nSolution: carefully take advantage of all possible randomness!\n§*\nDNS queries contain 16-‐bit query ID, and can randomize ~16 bit\nsrc port.\no\nSolution: deploy DNSSEC (signed DNS records, including missing\nrecords).\no\nOne problem: key distribution (who is allowed to sign each domain?)\no\nAnother problem: name enumeration (to sign \"no such name\" responses).\n§*\nPartially mitigated by NSEC3: http://tools.ietf.org/html/rfc5155\no\nSlow adoption, not much incentive to upgrade, non-‐trivial costs.\no\nCosts include both performance and administrative (key/cert\nmanagement).\nSYN flooding.\n-\nNote that server must store some state when it receives a SYN packet.\no\nCalled a half-‐open connection: replied with SYN-‐ACK, waiting for the ACK.\n-\nWhat if it receives SYN messages from many sources?\no\nMany implementations try to keep state for all half-‐open connections.\no\nBut eventually run out of memory, must reject connections!\n-\nAnnoying problem: we don't even know who we're keeping state for!\no\nAdversary could have a single host, and generate SYNs from many src IPs.\n-\nDenial-‐of-‐service attack: big asymmetry between client + server resources.\no\nClient spoofs a single packet (less than 1 millisecond).\no\nServer wastes memory until connection times out (minutes).\nDefense for SYN flooding: SYN cookies.\n-\nIdea: make the server stateless, until it receives that third packet (ACK).\n-\nWhy is this tricky?\no\nNeed to ensure an adversary can't make up a conn from any src address.\no\nPreviously, this was done by storing ISNs, and expecting it in the ACK.\n-\nUse a bit of cryptography to achieve similar goal.\n-\nEncode server-‐side state into sequence number.\no\nISNs = MAC_k(src/dst addr+port, timestamp) || timestamp\n\no\nTimestamp is coarse-‐grained (e.g., minutes).\no\nServer stores secret key k, not shared with anyone else.\no\nDetailed ref: http://cr.yp.to/syncookies.html\n-\nServer computes seq as above when sending SYN-‐ACK response.\n-\nServer can verify state is intact by verifying hash (MAC) on ACK's seq.\no\nNot quite ideal: need to think about replay attacks within timestamp.\n-\nAnother problem: if third packet lost, noone retransmits.\no\nMaybe not a big deal in case of a DoS attack.\no\nOnly a problem for protocols where server speaks first.\nAnother DoS attack vector: bandwidth amplification.\n-\nSend ICMP echo request (ping) packets to the broadcast address of a network.\no\nE.g., 18.26.7.255.\no\nUsed to be that you'd get an ICMP echo reply from all machines on\nnetwork.\no\nWhat if you fake a packet from victim's address? Victim gets all replies.\no\nFind a subnet with 100 machines on a fast network: 100x amplification!\no\nRef: http://en.wikipedia.org/wiki/Smurf_attack\n-\nCan we fix this?\no\nRouters now block \"directed broadcast\" (packets sent to broadcast\naddress).\n-\nModern-‐day variant: DNS amplification.\no\nDNS is also a request-‐response service.\no\nWith a small query, server might send back a large response.\no\nWith DNSSEC, responses contain lots of signatures, so they're even larger!\no\nSince DNS runs over UDP, source address is completely unverified.\no\nRef: http://blog.cloudflare.com/deep-inside-a-dns-amplification-ddos-\nattack\n-\nCan we fix the DNS attack?\no\nActually quite hard! Root name servers must answer to queries from\nanyone.\n-\nWhat if we had a chance to re-‐design DNS from scratch?\no\nOne possible plan: query must be as big as response (require padding).\no\nGeneral technique: force client to expend at least as much work.\nTCP congestion control.\n-\nReceiver can get the sender to speed up, by ACKing unreceived segments. Or\nsend more ACKs (e.g., send ACK for each byte instead of every packet).\nRouting protocols: overly-‐trusting of participants.\n-\nARP: within a single Ethernet network.\no\nTo send IP packet, need the Ethernet MAC address of router / next hop.\no\nAddress Resolution Protocol (ARP): broadcast a request for target's MAC.\no\nAnyone can listen to broadcast, send a reply; no authentication.\n\no\nAdversary can impersonate router, intercept packets, even on switched\nnet.\no\nPotential solution: make the switch in charge of ARP.\n§*\nNot widely deployed: would require managing MAC/IP addresses\ncarefully.\n-\nDHCP: again, within a single Ethernet network.\no\nClient asks for IP address by sending a broadcast request.\no\nServer responds, no authentication (some specs exist but not widely\nused).\n§*\nIf you just plugged into a network, might not know what to expect.\no\nLots of fields: IP address, router address, DNS server, DNS domain list, ..\no\nAdversary can impersonate DHCP server to new clients on the network.\n§*\nCan choose their DNS servers, DNS domains, router, etc.\no\nAlso, DoS attack on server: ask for lots of leases, from many MAC addrs.\no\nSolution: make the switch in charge of DHCP (forward reqs to real\nserver).\n§*\nNot widely deployed: would require careful switch configuration.\n§*\nEven more complicated on a wireless network.\n-\nBGP: Internet-‐wide (similar to RIP attacks described in paper).\no\nAny BGP participant router can announce route to a prefix.\no\nWhat if adversary has a router? Can announce any prefix or route.\no\nIs this problem still relevant?\n§*\nSpammers often exploit this: announce an unused address, and\nsend spam.\n§*\nGets around IP-‐level blacklisting of spam senders: choose almost\nany IP!\no\nHow to fix?\n§*\nSBGP: cryptographic signing of route announcements.\n§*\nMust know who is allowed to announce every particular IP prefix.\n§*\nRequires someone to distribute keys / certificates for every IP\nprefix.\n§*\nBootstrapping problem is tricky; some performance overheads§*\ntoo.\n§* Getting some traction but still not widely deployed.\nMany other problems too.\n-\nICMP messages like redirect: no authentication, basically unused now.\n-\nExposing too much information (netstat, SNMP, finger): mostly fixed.\n-\nidentd (\"Authentication Service\"): bad design, no real authentication.\n-\nEmail: real problem but no practical solutions yet.\no\nAuthentication vs authorization.\no\nE.g., PGP would not solve the spam problem.\n-\nPasswords in protocols: supporting ONLY passwords isn't so great.\n\no\nWe'll talk about alternatives in a few weeks.\n-\nFTP data transfer protocol.\no\nServer connects back to client to send a file to the client.\no\nClient tells the server what IP address and port number to use.\no\nCould be used for port-‐scanning from server's IP.\no\nCould be used to send any traffic (embedded in file) from server's IP.\n§*\nE.g., back to IP authentication problems: rlogin, spam, etc.\nHow do adversaries know what software / protocol you are running?\n-\nProbing:\no\nCheck if a system is listening on a well-‐known port.\no\nProtocols / systems often send an initial banner message.\n-\nnmap can guess OS by measuring various impl-‐specific details.\no\nRef: http://nmap.org/book/man-os-detection.html\n-\nUse DNS to look up the hostname for an IP address; may give hints.\n-\nGuessing: assume system is vulnerable, try to exploit bug.\nHow do adversaries know the IP address of the system to attack?\n-\ntraceroute to find routers along the way, for BGP attacks.\n-\nCan also just scan the entire Internet: only 2^32 addresses.\no\n1 Gbps (100 MB/s) network link, 64 byte minimum packets.\no\n~1.5M packets per second.\no\n2^32=4B packets in ~2500 seconds, or 45 minutes.\no\nzmap: implementation of this [ Ref: https://zmap.io/ ]\nWhy are things so insecure at the TCP/IP level?\n-\nHistorically, designers did not worry as much about security.\no\nEven Bellovin says: \"The Internet in 1989 was a much friendlier place\".\no\nOriginal Internet had a small number of relatively trustworthy users.\no\nDesign requirements changed over time.\n-\nEnd-‐to-‐end argument in action.\no\nMust provide security at the application level anyway.\no\nThings are \"good enough\" at the transport level to let application work.\n-\nSome fixes do get added, but only for the worst problems / easier solutions.\nHow to improve security?\n-\nProtocol-‐compatible fixes to TCP implementations.\n-\nFirewalls.\no\nPartial fix, but widely used.\no\nIssue: adversary may be within firewalled network.\no\nIssue: hard to determine if packet is \"malicious\" or not.\no\nIssue: even for fields that are present (src/dst), hard to authenticate.\no\nTCP/IP's design not a good match for firewall-‐like filtering techniques.\no\nE.g., IP packet fragmentation: TCP ports in one packet, payload in another.\n-\nImplement security on top of TCP/IP: SSL/TLS, Kerberos, SSH, etc.\n\no Beware: this paper isn't clear on encryption vs. authentication.\no Will talk about this more in next lecture on Kerberos.\n- Use cryptography (encryption, signing, MACs, etc).\no Quite a hard problem: protocol design, key distribution, trust, etc.\n- Some kinds of security hard to provide on top: DoS-‐resistance, routing.\n- Deployment of replacement protocols: SBGP, DNSSEC.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Computer Systems Security, Lecture 13",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/1812c0fe15b67c5b73b983475243e19d_MIT6_858F14_lec13.pdf",
      "content": "6.858 Lecture 13\nKerberos\nAdministrivia\nQuiz review today (Actual quiz next Wednesday.)\nPost your final project idea by tomorrow.\nKerberos setting:\n-\nDistributed architecture, evolved from a single time-‐sharing system.\n-\nMany servers providing services: remote login, mail, printing, file server.\n-\nMany workstations, some are public, some are private.\n-\nEach user logs into their own workstation, has root access.\n-\nAdversary may have his/her own workstation too.\n-\nAlternatives at the time: rlogin, rsh.\n-\nGoal: allow users to access services, by authenticating to servers.\n-\nOther user information distributed via Hesiod, LDAP, or some other directory.\n-\nWidely used: Microsoft Active Directory uses the Kerberos (v5) protocol\nWhat's the trust model?\n-\nAll users, clients, servers trust the Kerberos server.\n-\nNo apriori trust between any other pairs of machines.\n-\nNetwork is not trusted.\n-\nUser trusts the local machine.\nKerberos architecture:\n-\nCentral Kerberos server, trusted by all parties (or at least all at MIT).\n-\nUsers, servers have a private key shared between them and Kerberos.\n-\nKerberos server keeps track of everyone's private key.\n-\nKerberos uses keys to achieve mutual *authentication* between client, server.\no\nTerminology: user, client, server.\no\nClient and server know each other's names.\no\nClient is convinced it's talking to server and vice-‐versa.\n-\nKerberos does not provide authorization (can user access some resource).\no\nIt's the application's job to decide this.\nWhy do we need this trusted Kerberos server?\n-\nUsers don't need to set up accounts, passwords, etc on each server.\n\nOverall architecture diagram\n+-----------------------+\nc, tgs\n|\n|\n[ User: Kc ] <--------> [ Kerberos ]\n|\n^\n\\\n|\nDatabase:\n|\n|\n\\\n|\nc: Kc\n|\nV\n\\\ns\n|\ns: Ks\n|\n[ Server: Ks ] \\--------> [ TGS ]\n|\n|\nKDC\n|\n+-----------------------+\nBasic Kerberos constructs from the paper:\nTicket, T_{c,s} = { s, c, addr, timestamp, life, K_{c,s} }\n[ usually encrypted w/ K_s ]\nAuthenticator, A_c = { c, addr, timestamp }\n[ usually encrypted w/ K_{c,s} ]\nKerberos protocol mechanics.\n- Two interfaces to the Kerberos database: \"Kerberos\" and \"TGS\" protocols.\n- Quite similar; few differences:\no In Kerberos protocol, can specify any c, s; client must know K_c.\no In TGS protocol, client's name is implicit (from ticket).\no Client just needs to know K_{c,tgs} to decrypt response (not K_c).\n- Where does the client machine get K_c in the first place?\no For users, derived from a password using, effectively, a hash function.\n- Why do we need these two protocols? Why not just use \"Kerberos\" protocol?\no Client machine can forget user password after it gets TGS ticket.\no Can we just store K_c and forget the user password? Password\nequivalent.\nNaming.\n- Critical to Kerberos: mapping between keys and principal names.\n- Each principal name consists of ( name, instance, realm )\no Typically written name.instance@realm\n- What entities have principals?\no Users: name is username, instance for special privileges (by convention).\no Servers: name is service name, instance is server's hostname.\no TGS: name is 'krbtgt', instance is realm name.\n- Where are these names used / where do the names matter?\no Users remember their user name.\no Servers perform access control based on principal name.\no Clients choose a principal they expect to be talking to.\n§*\nSimilar to browsers expecting specific certificate name for HTTPS\n- When can a name be reused?\no For user names: ensure no ACL contains that name, difficult.\n\no\nFor servers (assuming not on any ACL): ensure users forget server name.\no\nMust change the key, to ensure old tickets not valid for new server.\nGetting the initial ticket.\n-\n\"Kerberos\" protocol:\no\nClient sends pair of principal names (c, s), where s is typically tgs.\no\nServer responds with { K_{c,s}, { T_{c,s} }_{K_s} }_{K_c}\n-\nHow does the Kerberos server authenticate the client?\no\nDoesn't need to -‐-‐ willing to respond to any request.\n-\nHow does the client authenticate the Kerberos server?\no\nDecrypt the response and check if the ticket looks valid.\no\nOnly the Kerberos server would know K_c.\n-\nIn what ways is this better/worse than sending password to server?\no\nPassword doesn't get sent over network, but easier to brute-‐force.\n-\nWhy is the key included twice in the response from Kerberos/TGS server?\no\nK_{c,s} in response gives the client access to this shared key.\no\nK_{c,s} in the ticket should convince server the key is legitimate.\nGeneral weakness: Kerberos 4 assumed encryption provides message integrity.\n-\nThere were some attacks where adversary can tamper with ciphertext.\n-\nNo explicit MAC means that no well-‐defined way to detect tampering.\n-\nOne-‐off solutions: kprop protocol included checksum, hard to match.\n-\nThe weakness made it relatively easy for adversary to \"mint\" tickets.\n-\nRef: http://web.mit.edu/kerberos/advisories/MITKRB5-SA-2003-004-krb4.txt\nGeneral weakness: adversary can mount offline password-‐guessing attacks.\n-\nTypical passwords don't have a lot of entropy.\n-\nAnyone can ask KDC for a ticket encrypted with user's password.\n-\nThen try to brute-‐force the user's password offline: easy to parallelize.\n-\nBetter design: require client to interact with server for each login attempt.\nGeneral weakness: DES hard-‐coded into the design, packet format.\n-\nDifficult to switch to another cryptosystem when DES became too weak.\n-\nDES key space is too small: keys are only 56 bits, 2^56 is not that big.\n-\nCheap to break DES these days ($20--$200 via https://www.cloudcracker.com/).\n-\nHow could an adversary break Kerberos give this weakness?\nAuthenticating to a server.\n-\n\"TGS\" protocol:\no\nClient sends ( s, {T_{c,tgs}}_{K_tgs}, {A_c}_{K_{c,tgs}} )\no\nServer replies with { K_{c,s}, { T_{c,s} }_{K_s} }_{K_{c,tgs}}\n-\nHow does a server authenticate a client based on the ticket?\no\nDecrypt ticket using server's key.\no\nDecrypt authenticator using K_{c,s}.\no\nOnly Kerberos server could have generated ticket (knew K_s).\n\no\nOnly client could have generated authenticator (knew K_{c,s}).\n-\nWhy does the ticket include c? s? addr? life?\no\nServer can extract client's principal name from ticket.\no\nAddr tries to prevent stolen ticket from being used on another machine.\no\nLifetime similarly tries to limit damage from stolen ticket.\n-\nHow does a network protocol use Kerberos?\no\nEncrypt/authenticate all messages with K_{c,s}\no\nMail server commands, documents sent to printer, shell I/O, ..\no\nE.g., \"DELETE 5\" in a mail server protocol.\n-\nWho generates the authenticator?\no\nClient, for each new connection.\n-\nWhy does a client need to send an authenticator, in addition to the ticket?\no\nProve to the server that an adversary is not replaying an old message.\no\nServer must keep last few authenticators in memory, to detect replays.\n-\nHow does Kerberos use time? What happens if the clock is wrong?\no\nPrevent stolen tickets from being used forever.\no\nBound size of replay cache.\no\nIf clock is wrong, adversary can use old tickets or replay messages.\n-\nHow does client authenticate server? Why would it matter?\no\nConnecting to file server: want to know you're getting legitimate files.\no\nSolution: send back { timestamp + 1 }_{K_{c,s}}.\nGeneral weakness: same key, K_{c,s}, used for many things\n-\nAdversary can substitute any msg encrypted with K_{c,s} for any other.\n-\nExample: messages across multiple sessions.\no\nAuthenticator does not attest to K_{c,s} being fresh!\no\nAdversary can splice fresh authenticator with old message\no\nKerberos v5 uses fresh session key each time, sent in authenticator\n-\nExample: messages in different directions\no\nKerberos v4 included a direction flag in packets (c-‐>s or s-‐>c)\no\nKerberos v5 used separate keys: K_{c-‐>s}, K_{s-‐>c}\nWhat if users connect to wrong server (analogue of MITM / phishing attack)?\n-\nIf server is intercepting packets, learns what service user connects to.\n-\nWhat if user accidentally types ssh malicious.server?\no\nServer learns user's principal name.\no\nServer does not get user's TGS ticket or K_c.\no\nCannot impersonate user to others.\nWhat happens if the KDC is down?\n-\nCannot log in.\n-\nCannot obtain new tickets.\n-\nCan keep using existing tickets.\nAuthenticating to a Unix system.\n\n- No Kerberos protocol involved when accessing local files, processes.\n- If logging in using Kerberos, user must have presented legitimate ticket.\n- What if user logs in using username/password (locally or via SSH using pw)?\no User knows whether the password he/she supplied is legitimate.\no Server has no idea.\n- Potential attack on a server:\no User connects via SSH, types in username, password.\no Create legitimate-‐looking Kerberos response, encrypted with password.\no Server has no way to tell if this response is really legitimate.\n- Solution (if server keeps state): server needs its own principal, key.\no First obtain user's TGS, using the user's username and password.\no Then use TGS to obtain a ticket for server's principal.\no If user faked the Kerberos server, the second ticket will not match.\nUsing Kerberos in an application.\n- Paper suggests using special functions to seal messages, 3 security levels.\n- Requires moderate changes to an application.\no Good for flexibility, performance.\no Bad for ease of adoption.\no Hard for developers to understand subtle security guarantees.\n- Perhaps a better abstraction: secure channel (SSL/TLS).\nPassword-‐changing service (administrative interface).\n- How does the Kerberos protocol ensure that client knows password? Why?\no Special flag in ticket indicates which interface was used to obtain it.\no Password-‐changing service only accepts tickets obtained by using K_c.\no Ensure that client knows old password, doesn't just have the ticket.\n- How does the client change the user's password?\no Connect to password-‐changing service, send new password to server.\nReplication.\n- One master server (supports password changes), zero or more slaves.\n- All servers can issue tickets, only master can change keys.\n- Why this split?\no Only one master ensures consistency: cannot have conflicting changes.\n- Master periodically updates the slaves (when paper was written, ~once/hour).\no More recent impls have incremental propagation: lower latency (but not\n0).\n- How scalable is this?\no Symmetric crypto (DES, AES) is fast -‐-‐ O(100MB/sec) on current\nhardware.\no Tickets are small, O(100 bytes), so can support 1M tickets/second.\no Easy to scale by adding slaves.\n- Potential problem: password changes take a while to propagate.\n- Adversary can still use a stolen password for a while after user changes it.\n\n-\nTo learn more about how to do replication right, take 6.824.\nSecurity of the Kerberos database.\n-\nMaster and slave servers are highly sensitive in this design.\n-\nCompromised master/slave server means all passwords/keys have to change.\n-\nMust be physically secure, no bugs in Kerberos server software,\no\nno bugs in any other network service on server machines, etc.\n-\nCan we do better? SSL CA infrastructure slightly better, but not much.\no\nWill look at it in more detail when we talk about browser security /\nHTTPS.\n-\nMost centralized authentication systems suffer from such problems.\no\nglobally-‐unique freeform names require some trusted mapping authority.\nWhy didn't Kerberos use public key crypto?\n-\nToo slow at the time: VAX systems, 10MHz clocks.\n-\nGovernment export restrictions.\n-\nPatents.\nNetwork attacks.\n-\nOffline password guessing attacks on Kerberos server.\no\nKerberos v5 prevents clients from requesting ticket for any principal.\no\nMust include { timestamp }_{K_c} along with request, proves know K_c.\no\nStill vulnerable to password guessing by network sniffer at that time.\no\nBetter alternatives are available: SRP, PAKE.\n-\nWhat can adversary do with a stolen ticket?\n-\nWhat can adversary do with a stolen K_c?\n-\nWhat can adversary do with a stolen K_s?\no\nRemember: two parties share each key (and rely on it) in Kerberos!\n-\nWhat happens after a password change if K_c is compromised?\no\nCan decrypt all subsequent exchanges, starting with initial ticket\no\nCan even decrypt password change requests, getting the new password!\n-\nWhat if adversary figures out your old password sometime later?\no\nIf the adversary saved old packets, can decrypt everything.\no\nCan similarly obtain current password.\nForward secrecy (avoiding the password-‐change problem).\n-\nAbstract problem: establish a shared secret between two parties.\n-\nKerberos approach: someone picks the secret, encrypts it, and sends it.\n-\nWeakness: if the encryption key is stolen, can get the secret later.\n-\nDiffie-‐Hellman key exchange protocol:\no\nTwo parties pick their own parts of a secret.\no\nSend messages to each other.\no\nMessages do not have to be secret, just authenticated (no tampering).\no\nTwo parties use each other's messages to reconstruct shared key.\no\nAdversary cannot reconstruct key by watching network messages.\n\n-\nDiffie-‐Hellman details:\no\nPrime p, generator g mod p.\no\nAlice and Bob each pick a random, secret exponent (a and b).\no\nAlice and Bob send (g^a mod p) and (g^b mod p) to each other.\no\nEach party computes (g^(ab) mod p) = (g^a^b mod p) = (g^b^a mod p).\no\nUse (g^(ab) mod p) as secret key.\no\nAssume discrete log (recovering a from (g^a mod p)) is hard.\nCross-‐realm in Kerberos.\n-\nShared keys between realms.\n-\nKerberos v4 only supported pairwise cross-‐realm (no transiting).\nWhat doesn't Kerberos address?\n-\nClient, server, or KDC machine can be compromised.\n-\nAccess control or groups (up to service to implement that).\n-\nMicrosoft \"extended\" Kerberos to support groups.\no\nEffectively the user's list of groups was included in ticket.\n-\nProxy problem: still no great solution in Kerberos, but ssh-‐agent is nice.\n-\nWorkstation security (can trojan login, and did happen in practice).\no\nSmartcard-‐based approach hasn't taken off.\no\nTwo-‐step authentication (time-‐based OTP) used by Google Authenticator.\no\nShared desktop systems not so prevalent: everyone has own phone,\nlaptop, ..\nFollow-‐ons.\n-\nKerberos v5 fixes many problems in v4 (some mentioned), used widely (MS AD).\n-\nOpenID is a similar-‐looking protocol for authentication in web applications.\no\nSimilar messages are passed around via HTTP requests.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/f7b9665c1307cae9fde0ec5e8edba70e_MIT6_858F14_Reading2.pdf",
      "content": "P\n\nFo\na\nr\np\ne\ne\na\nr\nc\nR\nh\ne\np\na\na\nd\np\ni\ne\nn\nr\ng\n, y\nQ\n-\n-\nweb s\nmi\no\nu\nu\ne\nr\nst\nas\nio\nsi\nn\ng\ns\nSub\ni\nt\nte\ny\n\no\nin\nu\na\nr\n\na\nfi\nn\nle\ns\n\nw\nna\ne\nnment i\nme\nr fo\nd\nr\n\nlec\neac\ns\nh\n\nn\nt\n\nw\nl\n.txt\ne\no\nct\n-‐\nu\nfol\nre\nd.\n, and\n's\nB\np\ny\nap\n10PM\ner qu\nt\ne\nhe\nstio\nevening\n\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit your own question about the pap\nnamed sq\n\nn\nabo\n.txt\nut t\n. You\nhe pa\ncannot\nper or t\nuse t\nhe pa\nhe quest\nper's\ne\ng\nr\ne\n\nn\n(e\ne\n.\nr\ng\na\n.,\nl\nw\nco\nh\nn\na\nt\nt\ne\ny\nx\no\nt/\nu\np\nf\nr\nind most\nion below\noblem) i\n\nn a file\nLectur\n\ne 2\nduring\n\nlecture we will try to answer questions submit\n. To t\nted t\nhe ex\nhe ev\nt\ne\nent\nnin\npossib\ng befor\nl\ne.\ne,\n\nSuppose\n\nchar *p = malloc(256);\nslot_size is set to 16 bytes. Consider t\n\nhe following code snippet:\nchar *q = p + 256;\nExplain whether\nchar ch = *q;\ndereference\n\nof q.\nor\n\nnot baggy bounds checking will raise an exception at the\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/b3cc1a5fb05454a31b3ef6dcc2ef5800_MIT6_858F14_Reading3.pdf",
      "content": "Paper Readin\n\nFor each paper, your assignment is two\ng Questions\n-\n-‐fold. By 10PM the evening before lecture:\n-\nSubmit your answer for each lecture's paper question via the\n\nSubmit your own question about the\nweb site in a file named lecn.txt, and\nsubmission\nconfusing\nnamed\nabo\n\nduring\nsq\nut the paper or the paper's general context/problem) in a file\npaper (e.g., what you find most\nLecture 3\nlect\nn\nu\n.txt\nre we\n. You\nwill t\ncannot\nry to answer questions submitted the evening befor\nuse the question below. To the extent possibl\ne.\ne,\n\nFor a BROP attack to succeed, the server must not rerandomize canaries after\ncrashing. Suppose that, after a server crashes, it creates a new canary by SH\nhashing t\nvalue. Is this new scheme secure?\nA1\n\nhe current gettimeofday()\n-‐\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/fd5b1b51d610793b86f6e0d7df74f919_MIT6_858F14_Reading4.pdf",
      "content": "Paper Readin\n\nFor each paper, your assignment is two\ng Questions\n- Submit your answer for each lecture's paper question via the\n-‐fold. By 10PM the evening before lecture:\n-\nweb site in a file named\n, and\nsubmission\nconfusing\nSubmit your own question about the\nlec\n\nnamed sq\n\ndu\nn\nabout the paper or the paper's general context/problem) in a file\nn.txt\n.txt\npaper (e.g., what you find most\nring lecture we\n. You\nwill t\ncannot\nry to answer questions submitted the evening befor\nuse the question below. To the extent possibl\ne.\ne,\n\nLectur\nWh\ns\ne\n\ndat\nat'\nabase\nth\np\ne wo\nroxy\nrs\na\nt th\nuthent\nat co\nica\nuld\ntion\nhappe\ntoken?\nn if o\n\nne service in OKWS were to leak its 20-‐byte\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/df196c626d6a8c3a1c94d3ffb85d2246_MIT6_858F14_Reading6.pdf",
      "content": "P\n\nFo\na\nr\np\n\ne\na\nr\nc\nR\nh\ne\np\na\na\nd\np\nin\n-\ne\n\ner\ng\n, y\nQ\no\nu\nu\ne\nr\nst\nas\nio\nsi\nn\ng\ns\nn\n\nmen\nlec\nt is\nn\ntw\n.txt\no-‐fold. By 10PM\n-\nS\nwe\nub\nb s\nmi\ni\nt\nte\ny\n\no\nin\nu\na\nr\n\na\nfi\nn\nle\ns\n\nw\nna\ne\nme\nr fo\nd\nr\n\neach lecture\n, and\n's pa\n\nper qu\nt\ne\nhe\nstio\nevening\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit\nsq\nyo\n\nu\nn\nabo\nr\n.txt\no\nut\nwn\nt\n\nh\nq\ne\nu\np\nes\na\nt\np\nio\ner\nn\n\no\na\nr\nb\nt\no\nh\nu\ne\nt\n\np\nth\na\ne\np\n\ne\np\nr\na\n'\np\ns\ne\ng\nr\ne\n\nn\n(e\ne\n.g\name\na\n.,\nl\nw\nn\nc\nha\nd\n. You\nr\nont\nt\ne\ny\nx\no\nt/\nu\np\nf\nr\nin\no\nd\nb\n\nl\nmo\nem)\nst\ni\n\nn a file\n\nLecture\ndu\nring\n\nlecture we will t\ncannot\nry to a\nuse t\nnswe\nhe quest\nr questio\nion b\nns su\nel\nb\now\nmit\n. To t\nted t\nhe ex\nhe ev\nt\ne\nent\nnin\npossib\ng befor\nl\ne.\ne,\n\nT\nCa\nh\np\ne\ns\na\nic\nu\nu\nth\nm\nor\nin\ns\n\no\nse\nf\nv\nth\ne\ne\nra\nC\nl\na\na\np\np\ns\np\ni\nl\nc\ni\nu\nca\nm\nti\np\non\nap\ns\ne\n(\nr\nS\n\ne\nd\nc\ne\nt\ns\nio\ncr\nn\ni\n\nb\ne\n)\n\n.\ns\nH\nev\no\ne\nw\nr\n\na\nw\nl\no\nst\nu\nr\nl\na\nd\nt\n\ne\ny\ng\no\ni\nu\ne\n\ns\nr\n\ne\nfo\ns c\nc\nr how to u\nCap i u\nommend u\ns\ns\ne\ni\n\nng\nCapsicu\nm\nm\nin\na\nt\n\nth\n\nt\nh\nw\ne d\no\ni\nu\nff\nld\ner\nh\ne\na\nn\nv\nt\ne\nc\n\no\nma\nmp\nd\no\ne\nn\nit\ne\ne\nn\na\nts\ns\n\ni\no\ne\nf\nr\n\nO\nto\nK\nb\nW\nu\nS\nil\n?\nd\nAr\nOK\ne\nW\nth\nS\ne\n?\nr\n\ne features missing from\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/bfcff0720f0da10d1499cbd22d21c182_MIT6_858F14_Reading7.pdf",
      "content": "P\n\nFo\na\nr\np\ne\ne\na\nr\nc\nR\nh\ne\np\na\na\nd\n-\nb\np\ni\ne\nng Questions\nSu\nr, yo\n-\nweb s\nmi\ni\nt\nte\ny\n\no\nin\nu\na\nr\nu\n\nr\na\nass\nfi\nn\nle\ns\n\nw\nig\ne\nn\nr\nme\nfor\nn\ne\nt\na\ni\nc\ns\nh\n\nn\nt\nnamed lec\n\nw\nl\n.txt\ne\no\nct\n-‐\nu\nfol\nre\nd.\n's\nB\np\ny\nap\n10PM\n, and\ner qu\nt\ne\nhe\nstio\nevening\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit your own question about the paper (e.g., what you find most\ndu\nname\nring\nd sq\n\nn\nabo\n.txt\nut t\n. You\nhe pa\ncannot\nper or t\nuse t\nhe pa\nhe quest\nper's ge\nion b\nneral\nel\nc\now\nont\n. To t\next/p\nhe ex\nroblem) in a file\nLectur\n\ne 7\nlecture we will try to answer questions submitted the ev\nt\ne\nent\nnin\npossib\ng befor\nl\ne.\ne,\n\nSuppose an adversary discover\nex\nde\npl\nte\noit\nrmi\nt\nn\nhis t\nes t\no esca\nhe len\npe t\ngth\nhe inner sa\nof a parti\ns a bug in NaC\ncular x86\nl where the checker\nndbox?\ninstruction. How could a\nincor\nn adv\nr\ne\nectly\nrsary\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/37c7be06bc03d6908e2743097b1920d6_MIT6_858F14_Reading8.pdf",
      "content": "P\n\nFo\na\nr\np\ne\ner Reading Question\n-\nac\nS\nh\nu\np\nb\na\nmi\nper\ns\ny\n,\no\ny\n\nt\nu\no\nr\nu\n\nr\na\n\nn\nas\ns\ns\nw\nig\ne\nn\nr fo\nment is tw\nr\nlec\neach\nn\nl\n.txt\ne\no-‐\nctu\nfol\nre\nd.\n's\nB\np\ny\nap\n10PM\ner qu\nthe evening before lecture:\n-\nwe\nSubmit\nb site\nyo\nin\nu\na file na\n\nabo\nr\nmed\n, and\nestion via the submission\nconfusing\nsq\n\nn.txt\no\nut\nwn\nnamed\nt\n. You\nh\nq\ne\nu\np\nes\na\nt\np\nio\ner\nn\n\nabout t\nor the p\nh\na\ne\np\n\ne\np\nr\na\n'\np\ncannot\ns\ne\ng\nr\ne\n\nn\n(e.\nion b\nr\ng\nuse the quest\ne a\n.,\nl\nw\nco\nh\nn\na\nt\nt\ne\nyou\nxt/p\nfind most\nduring\nelow. To the ex\nroble\ntent\nm) i\npossib\nn a file\nle,\n\nLectur\n\nSuppose you ar\ne 8\nlecture we will try to answer questions submitted the evening before.\nhttp://bitdiddle\ne helping the deve\n.com/ to evaluate\nl\nt\no\nh\np\ne\ne\ni\nr\nc\no a\nr\ns\ns\no\ne\nf a omplex web site at\ncook\nt\nu\nie fr\nthen\no\nt\nm\nica\no\nt\nn\ne\ne\nu\no\nse\nf\nr\nth\ns.\ne\nT\nv\nh\ni\ne\nsi\ns\nto\nit\nr\ne\ns\n\nd\nto\nev\nt\ne\nh\nl\ne\no\n\np\nsi\ne\nt\nr\ne\ns\n,\n\ncurity. Th\na\na\nn\nre\nd\n\nw\nus\nor\nis w\ne\nr\nth\nie\na\nd\nt\na\neb site uses an HTTP cookie\ncookie t\nn adver\no\ns\n\na\nimp\nry mi\ners\ng\no\nh\nn\nt\na\ns\nt\nt\ne\ne\n\na\nth\nl\ne\nth\n\ne\n\nvictim visitor.\nWhat shou\nstolen by a\nl\nn\nd\na\nt\nd\nh\nv\ne\ne\ndevelop\nrsary? I\ne\nn\nr\no\ns\nt\nl\nh\no\ne\no\nr\nk\nw\na\no\nt\nr\nin\nd\n\ns\no\n,\nr\nw\nd\nh\ner\na\n\nt\nt\n\no determi\nkinds\nne if a user's cookie can be\ns\na\nt\nllow the adversary to obtain the cookie, and how might th\neal the cookie of one of the visitors to http://bitdid\no\nd\nf a\nle\nd\n.c\nv\no\ne\nm/\nrsa\nmi\ne d\n,\nr\ne\nw\nie\nh\ns\nvel\na\no\nt\np\ng\nght be able to\n\ne\no\nrs\ne\n\ns\np\n\"\nr\nw\nev\nr\ne\no\nn\nn\nt\ng\ni\n\"\nt\n\n?\nt\n\no\nNot\nsubst\ne:\nanti\nan e\nally d\nxhau\ni\ns\nf\nt\nf\ni\ne\nv\nr\ne\ne\n\nnt i\nans\ns\nw\nsue\ner\ns\nmi\nth\ng\nat th\nht be\ne\n\nd\nqu\ne\ni\nve\nte\nlo\nlo\npe\nng\nr\n,\ns\ns\nh\no\nave\nyou\nto\nc\nco\nan\nns\nsto\nid\np\ne\na\nr\nf\n.\nter about 5\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/41cc3912b8472e2a8c6f0d75b5abe056_MIT6_858F14_Reading9.pdf",
      "content": "P\n\nFo\na\nr\np\n\ner Reading\n-\ne\n\nac\nS\nh\nu\np\nb\naper, y\nQ\no\nu\nu\ne\nr\nst\nas\nio\nsi\nn\ng\ns\nn\n\nment is two-‐fold. By 10PM the evening\n-\nweb s\nmi\ni\nt\nte\ny\n\no\nin\nu\na\nr\n\na\nfi\nn\nle\ns\n\nw\nna\ne\nme\nr fo\nd\nr\n\nlec\neach\nn\nl\n.txt\necture\n, and\n's pa\n\nper question via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit\nsq\nyo\n\nu\nn\nabo\nr\n.txt\no\nut\nwn\nn\nth\nq\ne\nu\np\nes\na\nt\np\nio\nr\nn\n\no\na\named\n. You cannot\ne\nr\nb\nt\no\nh\nu\ne\nt\n\np\nth\na\ne\np\n\ne\np\nr\na\n'\np\ns\ne\ng\nr\nuse t\ne\n\nhe question b\nn\n(e\ne\n.\nr\ng\na\n.,\nl\nw\nco\nh\nn\na\nt\nt\ne\ny\nx\no\nt/\nu\np\nfind most\nduring lecture we will try to answer questions su\nel\nb\now\nmit\n. To t\nted t\nhe ex\nroble\ntent\nm) in a file\nLectur\n\ne 9\nhe evenin\npossib\ng befor\nl\ne.\ne,\n\nAf\nTa\nt\nngl\ner y\ned W\nou h\neb\nav\n\"\ne\n.\n\nW\nrea\nh\nd\nat\na\ns\nb\ne\no\nc\nu\nu\nt\nr\n\ni\nD\nty\nja\np\nn\ni\ng\nt\no\nfa\n's\nll\n\ns\ns\n\ne\ns\nc\nt\nu\nil\nr\nl\ni\nr\nt\ne\ny\nma\nme\nin\nch\nf\na\no\nn\nr\ni\nd\ns\ne\nms\nve\n,\nl\nt\no\nh\np\ni\ne\nn\nr\nk\ns\n\nb\nu\na\ns\nc\nin\nk to \"The\nDj\nyo\na\nu\nn\ne\ng\nx\no's\nten\nex\nd\nist\nDja\nin\nn\ng\ng\n\no\np\n\nrot\nto h\nect\nelp\nion\nde\ns?\nv\n\nelopers avoid those pitfalls, in a style s\ng\nimi\nDj\nl\na\na\nn\nr\ng\nt\no\no\n?\n\nCould\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/22d2fcbec7fe6fbb1df4590e8c93d999_MIT6_858F14_Reading10.pdf",
      "content": "P\nF\n\no\na\nr\np\n\ner Re\n-\ne\n\nac\nS\nh\nu\np\nadin\nb\na\nmi\npe\nt\nr\ng\ny\n, y\nQu st\nns\nou\no\ne\nio\n\nr\nu\n\nr\na\n\nn\nas\ns\ns\nw\nig\ne\nn\nr\nme\nfor\nn\n\nlec\ne\nt\na\ni\nc\ns\nh\n\nn\nt\n\nw\nl\n.txt\ne\no\nct\n-‐\nu\nfol\nwe\n\nr\nd. By\n-\nb site in a file named\ne\n, and\n's pa\n\np\n10PM\ner qu\nt\ne\nhe\nstio\nevening\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit\nsq\nyo\n\nu\nn\nabo\nr\n.txt\no\nut\nwn\nt\n\nh\nq\ne\nu\np\nes\na\nt\np\nio\ner\nn\n\no\na\nr\nb\nt\no\nh\nu\ne\nt\n\np\nth\na\ne\np\n\ne\np\nr\na\n'\np\ns\ne\ng\nr\ne\n\nn\n(e\ne\n.\nr\ng\na\n.,\nl\nwhat you find most\ndu\nname\nring\nd\nlecture we\n. You\nwill t\ncannot\nry to a\nuse t\nnswe\nhe quest\nr questio\nion b\nns su\nel\nc\now\nont\n. To t\next/p\nhe ex\nroble\ntent\nm) in a file\nLectur\n\ne 10\nbmitted the evenin\npossib\ng befor\nl\ne.\ne,\n\nK\nw\nL\no\nE\nu\nE\nld\nu\ng\ns\no\ne\n\ns\nw\na\nr\n\no\nsa\nn\nt\ng\nis\ni\nf\nf\ni\n\na\nK\nb\nL\ni\nE\nli\nE\nty\nd\n(\ni\nS\nd\nAT\nno\n/\nt\nS\nu\nM\nse\nT\n\n)\na\n\ns\nS\no\nAT\nlve\n/\nr\nS\nt\nM\no\nT\nimplement symbolic execution. What\nb\nsy\nr\nmb\nanc\no\nh\nl\ne\nic\ns\n\n?\nv\n\nalue\nWha\nco\nt w\nuld\nou\nbe\nld g\n?\no\n\nwrong if KLEE just gu\n\ne\nso\nss\nlv\ne\ne\nd\nr\nr\n,\na\na\nn\nn\nd\nd\no\ni\nml\nnst\ny\ne\na\na\nb\nd\no\nt\nu\nri\nt\ne\nw\nd\nh\na\na\nll\nt\n\na\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 11",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/5b3f834f355107985670c5bab3ae8c7d_MIT6_858F14_Reading11.pdf",
      "content": "P\nF\n\no\na\nr\np\n\ner Re\n-\ne\n\nac\nS\nh\nu\np\nadin\nb\na\nmi\npe\nt\nr\ng\ny\n, y\nQu st\nns\nou\no\ne\nio\n\nr\nu\n\nr\na\n\nn\nas\ns\ns\nw\nig\ne\nn\nr\nme\nfor\nn\n\nlec\ne\nt\na\ni\nc\ns\nh\n\nn\nt\n\nw\nl\n.txt\ne\no\nct\n-‐\nu\nfol\nwe\nname\nr\nd. B\n-\nb site in a file\nd\ne\n, and\n's p\ny\na\n10PM\n\nper qu\nt\ne\nhe\nstio\nevening\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nS\nconfusing\nubmit\nsq\nyo\n\nu\nn\nabo\nr\n.txt\no\nut\nwn\nt\n\nh\nq\ne\nu\np\nes\na\nt\np\nio\ner\nn\n\no\na\nr\nb\nt\no\nh\nu\ne\nt\n\np\nth\na\ne\np\n\ne\np\nr\na\n'\np\ns\ne\ng\nr\ne\n\nn\n(e\ne\n.\nr\ng\na\n.,\nl\nw\nco\nh\nn\na\nt\nt you find most\nnamed\n. You cannot use the question below. To t\next/p\nhe ex\nroble\ntent\nm)\npossib\nin a file\nLectur\n\ne\ndu\nring\n\nlecture we will try to answer questions submitted the evening befor\nl\ne.\ne,\n\nWh\nOne\nat ki\napp\nnd\nro\ns\na\no\nch\nf s\nmi\nec\ng\nur\nht\ni\n\nty\nbe\nv\nt\nulne\no ke\nr\ne\nabi\np th\nli\ne\nti\n\ne\nO\ns are still possible in an Ur/Web application?\ncl\nth\nasses of b\ne Ur/Web\nugs, or w\npaper, and consider whe\nW\nth\nAS\ner\nP\nU\n\nr\nT\n/\no\nW\np-‐\ne\nb\n's\nl\nmi\n's st\n\ni\nf\ns\ne\nt\na\ni\nt\nn\nu\n\nre\nn\nhether it\nill possible to have vulnerab\ns\nil\nc\nd\na\n\nit\nn\na\n\ns\ne\ny\nli\nou are readin\nies.\nminate certai\ng\nn\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Computer Systems Security, Reading Question 12",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-858-computer-systems-security-fall-2014/900bfe0631529bd0bc6dc29d6e38cf76_MIT6_858F14_Reading12.pdf",
      "content": "P\n\nFo\na\nr\np\ne\ne\na\nr\nc\nR\nh\ne\np\na\na\nd\np\ni\ne\nn\nr\ng\n, y\nQ\n-\no\nu\nu\nestions\n\n-\nwe\nSub\nb s\nmi\ni\nt your\nr\na\n\nn\nas\ns\ns\nw\nig\ne\nn\nr\nme\nfor\nn\ne\nt\na\ni\nc\ns\nh\nt\n\nw\nle\no\nct\n-‐\nu\nfol\nre\nd.\n's\nB\np\ny\nap\n10PM\ner qu\nt\ne\nhe\nstio\nevening\nn via th\nb\ne\nefore\nsubmi\nlect\nssi\nu\non\nre:\n\nconfusing\nSubmit\nte\n\nsq\ny\n\no\nin\n\nu\na\nr\n\no\nfi\nw\nle\nn\nn\nq\na\nu\nmed\nn\nabo\n.txt\nut the p\nes\na\nti\n\nn\n, and\nnamed\np\no\ncannot\ne\nlec\nr\nn\no\na\nr\nb\n\n. You\n\nt\no\n.txt\nh\nu\ne\nt\n\np\nth\na\ne\np\n\ne\np\nr\na\n'\np\ns\ne\ng\nr\ne\n\nn\n(e\ne\n.\nr\ng\na\n.,\nl\nwh\nn\na\nuse t\nco t\nt\ne\ny\nx\no\nhe question below. To t\nt/\nu\np\nf\nr\nin\no\nd\nb\n\nl\nmo\ne\nst\nduring lecture we will try to answer questions submitted t\nhe ex\nhe ev\nt\ne\nent\nm)\npossib\nin a file\n\nLecture 12\nning befor\nl\ne.\ne,\n\nS\n(\nteve B\ns\na\ne\nn\ncu\nd\nr\nt\ni\nh\nty\ne\ne\n\nl\np\np\nlo in'\nr\na\nv\npe\ns \"A Look Back\"\nobl\nr\ne\n\nms\nitse\ni\nl\nn\nf\n\ni\nt\ns\nh\na\ne\n\nr\nT\ne\nC\ntr\nP\no\n/\ns\nI\np\np\ne\na\nc\np\nt\ne\niv\nr\ne\nw\no\na\nn\ns\n\npublished in 2004,\nP protocol\nh\nsu\nis\ni\n\nt\ne\ne\na\nd\nrl\ne\ni\ns\ne\nc\nr\nr\np\nib\na\ne\np\nd\ne\n\nr\nin\nfr\nS\no\na\nm\nlmo\ns\nt\n)\n.\n\nW\nye\nh\na\ni\nr\nc\ns ago\nare still relevant today?\nteve Bellovin's paper\nh of th\n\ne\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.858 Computer Systems Security\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}