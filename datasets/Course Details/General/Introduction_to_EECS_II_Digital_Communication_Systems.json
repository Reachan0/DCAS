{
  "course_name": "Introduction to EECS II: Digital Communication Systems",
  "course_description": "An introduction to several fundamental ideas in electrical engineering and computer science, using digital communication systems as the vehicle. The three parts of the course—bits, signals, and packets—cover three corresponding layers of abstraction that form the basis of communication systems like the Internet.\nThe course teaches ideas that are useful in other parts of EECS: abstraction, probabilistic analysis, superposition, time and frequency-domain representations, system design principles and trade-offs, and centralized and distributed algorithms. The course emphasizes connections between theoretical concepts and practice using programming tasks and some experiments with real-world communication channels.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Computer Networks",
    "Electrical Engineering",
    "Digital Systems",
    "Signal Processing",
    "Telecommunications",
    "Engineering",
    "Computer Science",
    "Computer Networks",
    "Electrical Engineering",
    "Digital Systems",
    "Signal Processing",
    "Telecommunications"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1 hour / session\n\nRecitations: 2 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.03\n,\n6.01\n. The problem sets involve programming; they require knowledge of Python at the level of\n6.00\nor\n6.01\n.\n\nCourse Objectives\n\nAn introduction to several fundamental ideas in electrical engineering and computer science, using digital communication systems as the vehicle. The three parts of the course--\nbits, signals, and packets\n--cover three corresponding layers of abstraction relevant to the system:\n\nbinary representation, compression (source coding), and error correction (channel coding) for messages transmitted across a noisy link;\n\nsignal representation of binary messages for transmission across a shared physical channel subject to distortion and noise;\n\nefficient, reliable communication across networks made up of multiple links.\n\nTopics investigated in depth include:\n\nBits\n: Information and entropy, Huffman coding and LZW compression, error correction with linear block codes and convolutional codes (Viterbi decoding).\n\nSignals\n: Additive Gaussian noise and the relationship between noise variance and bit errors, linear-time invariant channel models, frequency-domain (Fourier) analysis, spectral content of signals and filtering, modulation and demodulation.\n\nPackets\n: Media access protocols (TDMA, Aloha, and carrier sense), packet-switched networks, queues, and Little's law, network routing (distance/path vector & link-state protocols), and reliable data transport (adaptive timers, stop-and-wait, sliding windows, round-trip time and bandwidth-delay product concepts).\n\nThese topics form the basis of communication systems like the Internet.\n\nThe course teaches ideas that are useful in other parts of EECS: abstraction, probabilistic analysis, superposition, time- and frequency-domain representations, system design principles and trade-offs, and centralized and distributed algorithms. The course emphasizes connections between theoretical concepts and practice using programming tasks and some experiments with real-world communication channels.\n\nAt the end of the course, a successful student will understand these topics and be able to apply them to the design and analysis of communication systems and networks. In particular, they will appreciate how to build reliable and efficient communication systems: cleverly applying redundancy for reliability and sharing via multiplexing channels, links, and paths for efficiency.\n\nSoftware\n\nInstructions for setting up the 6.02 environment on your computer\n\nProblem Sets\n\nThere are nine problem sets, assigned more-or-less weekly most Wednesdays. Problem sets are usually due at midnight the following Wednesday (we'll let that deadline slide to 6am the following Thursday morning in keeping with MIT tradition).\n\nEach problem set includes problems that involve writing Python code, so be sure to start early and leave enough time to debug your implementation before the due date. There will be six checkoff interviews during the semester, lasting 15-20 minutes each on average, which you must complete with your assigned TA on or before the dates specified on the problem set. Your TA will contact you to schedule these interviews.\n\nCompleting the interviews is a pre-requisite for passing the course. A missing interview will result in a failing grade; we will not grant \"incomplete\" for missing interviews.\n\nPlease note that working through the problem sets (and other practice problems we provide) is the best way to test your understanding of what we teach and to prepare for the quizzes.\n\nLate policy\n: You may use up to five extension days (in total) over the course of the semester for the nine problem set, apportioned in any way. For any other late problem sets, your score will be multiplied by 0.5; moreover, you must submit it within 7 days of the original due date to get any credit.\n\nCollaboration policy\n: The problem sets must be done individually. You may get help from the course staff and other students on the underlying material in the problem sets, but the work you hand in must be your own. In particular, you must not copy another person's solution, code, or other work. Someone telling you the solution to a problem is also not acceptable. Copying another person's work or allowing your work to be copied by others is a serious academic offense and will be treated as such. We will spot-check your submissions using a software utility, as well as manually, for cheating, so please don't tempt fate by submitting someone else's work as your own; it will save us all a lot of grief.\n\nParticipation\n\nWe expect you to attend all lectures and recitations, unless there are pressing or unforeseen conflicts. Conflicts that are persistent (e.g., registering for another class at the same time and \"splitting\" attendance between them) are not excused. Attending recitations is not merely optional. Things we teach in lecture and recitation are fair game on quizzes and problem sets.\n\nTo assess and encourage participation, recitations (and perhaps some lectures) will include simple \"spot questions\" that we will ask from time to time. Over the duration of the term, between lectures and recitations, we anticipate many dozens of such questions; if you pay a little attention, answering them will be trivial. At the end of the term we will take your responses to all these questions into consideration to assess a participation score, which will count toward a small portion (3%) of the overall grade.\n\nIf you miss a few lectures and recitations, it shouldn't materially affect this score. If you miss more, it probably will, and may affect your grade if you end up at the border between two letter grades.\n\nGrading\n\nYour final grade will be determined as follows:\n\nACTIVITIES\n\nPERCENTAGES\n\nQuiz 1\n\n17%\n\nQuiz 2\n\n18%\n\nQuiz 3\n\n17%\n\nProblem sets\n\n5% each, 45% total\n\nParticipation\n\n3%",
  "files": [
    {
      "category": "Assignment",
      "title": "6.02 Introduction to EECS II, Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/ade7157c7f80238dececc3788d97c147_MIT6_02F12_ps1.pdf",
      "content": "Your answers will be graded by actual human beings (at least that's what we believe!), so\ndon't limit your answers to machine-gradable responses. Some of the questions specifically\nask for explanations; regardless, it's always a good idea to provide a short explanation for\nyour answer. Before attempting the problems below, we encourage you to review Chapters 1\nand 3, and solve the online practice problems on this material.\nProblem 1: Information (2 points)\nX is an unknown 8-bit binary number picked uniformly at random from the set of all possible\n8-bit numbers. You are told the value of another 8-bit binary number, Y, and also told that X\nand Y differ in exactly 4 bits. How many bits of information about X do you now know?\nExplain your answer.\n(points: 2)\nProblem 2. Entropy when combining symbols. (1 point)\nSuppose you are given a collection of N ≥ 1 symbols, the i symbol occuring with probability\npi, for 1 ≤ i ≤ N, with p1 + p2 + ... + pN = 1. This distribution has entropy H. Now suppose we\ncombine any two symbols, i and j, to a combined symbol with probability of occurrence pi +\npj. All the other symbol probabilities remain the same. Let the entropy of this new\ndistribution by H'. In general, is H' smaller than H, bigger than H, or the same as H, or is it not\npossible to tell for sure? Prove your answer.\nth\nProblem Set 1\n\n(points: 1)\nProblem 3. Green Eggs and Hamming. (8 points)\nBy writing Green Eggs and Ham, Dr. Seuss won a $50 bet with his publisher because he used\nonly 50 distinct English words in the entire book of 778 words. The probabilities of\noccurrence of the most common words in the book are given in the table below, in decreasing\norder:\nRank\nWord\nProbability of occurrence of word in book\nnot\n10.7%\nI\n9.1%\nthem\n7.8%\na\n7.6%\nlike\n5.7%\nin\n5.1%\ndo\n4.6%\nyou\n4.4%\n9--50 all other words 45.0%\nI pick a secret word from the book.\nA.\nThe Bofa tells you that the secret word is one of the 8 most common words in the\nbook.\nYertle tells you it is not the word not.\nThe Zlock tells you it is three letters long.\nHow many bits of information about the secret word have you learned from:\n1. The Bofa alone?\n2. Yertle alone?\n3. The Bofa and the Zlock together?\n4. All of them together?\nExpress your answers in log2(100 / x) form, for suitable values of x in each case.\n\n(points: 2)\nB. The Lorax decides to compress Green Eggs and Ham using Huffman coding, treating\neach word as a distinct symbol, ignoring spaces and punctuation marks. He finds that\nthe expected code length of the Huffman code is 4.92 bits. The average length of a\nword in this book is 3.14 English letters. Assume that in uncompressed form, each\nEnglish letter requires 8 bits (ASCII encoding). Recall that the book has 778 total\nwords (and 50 distinct ones).\n1. What is the uncompressed (ASCII-encoded) length of the book? Show your\ncalculations.\n(points: 0.5)\n2. What is the expected length of the Huffman-coded version of the book? Show\nyour calculations.\n(points: 0.5)\n3. The words \"if\" and \"they\" are the two least popular words in the book. In the\nHuffman-coded format of the book, what is the Hamming distance between their\ncodewords?\n(points: 0.5)\nC. The Lorax now applies Huffman coding to all of Dr. Seuss's works. He treats each word\nas a distinct symbol. There are n distinct words in all. Curiously, he finds that the most\npopular word (symbol) is represented by the codeword 0 in the Huffman\nencoding. Symbol i occurs with probability pi; p1 ≥ p2 ≥ p3 ... ≥ pn. Its length in the\n\nHuffman code tree is Li.\n1. Given the conditions above, is it True or False that p1 ≥ 1/3? Explain.\n(points: 1)\n2. The Grinch removes the most-popular symbol (whose probability is p1) and\nimplements Huffman coding over the remaining symbols, retaining the same\nprobabilities proportionally; i.e., the probability of symbol i (where i > 1) is now\npi / (1 - p1). What is the expected code length of the Grinch's code tree, in terms\nof L, the expected code length of the original code tree, as well as p1? Explain.\n(points: 1)\nD. The Cat in the Hat compresses Green Eggs and Ham with the LZW compression\nmethod described in 6.02 (codewords from 0 to 255 are initialized to the corresponding\nASCII characters, which includes all the letters of the alphabet and the space\ncharacter). The book begins with these lines:\nI_am_Sam\nI_am_Sam\nSam_I_am\nWe have replaced each space with an underscore (_) for clarity, and eliminated\npunctuation marks.\n1. What are the strings corresponding to codewords 256, 257, and 258 in the string\ntable?\n(points: 0.5)\n2. When compressed, the sequence of codewords starts with the codeword 73,\nwhich is the ASCII value of the character \"I\". The initial few codewords in this\nsequence will all be < 255, and then one codeword > 255 will appear. What\nstring does that codeword correspond to?\n\n(points: 0.5)\n3. Cat finds that codeword 700 corresponds to the string \"I_do_not_l\". This string\ncomes from the sentence \"I_do_not_like_them_with_a_mouse\" in the book.\nWhat are the first two letters of the codeword numbered 701 in the string table?\n(points: 1)\n4. Thanks to a stuck keyboard (or because Cat is an ABBA fan), the phrase\n\"IdoIdoIdoIdoIdo\" shows up at the input to the LZW compressor. The\ndecompressor gets a codeword, already in its string table, and finds that it\ncorresponds to the string \"Ido\". This codeword is followed immediately by a new\ncodeword not in its string table. What string should the decompressor return for\nthis new codeword?\n(points: 0.5)\nProblem 4. LZW compression. (2 points)\nA. Suppose the sender adds two strings with corresponding codewords c1 and c2 in that\norder to its string table. Then, it may transmit c2 for the first time before it transmits c1.\nExplain whether this statement is True or False.\n(points: 1)\nB. Consider the LZW compression and decompression algorithms as described in 6.02.\nAssume that the scheme has an initial table with code words 0 through 255\ncorresponding to the 8-bit ASCII characters; character ``a'' is 97 and ``b'' is 98. The\n\nreceiver gets the following sequence of code words, each of which is 10 bits long:\n97 97 98 98 257 256=\nWhat was the original message sent by the sender?\n(points: 1)\nZip archive of all required files for the programming tasks on this PS. Extract using unzip.\nYou'll need to install numpy and matplotlib (they should be available on the lab machines).\nClick here for instructions.\nPython Task 1: Creating Huffman codes (3 points)\nUseful download links:\nPS1_tests.py -- test jigs for this assignment\nPS1_1.py -- template file for this task\nThe process of creating a variable-length code starts with a list of message symbols and their\nprobabilities of occurrence. As described in the lecture notes, our goal is to encode more\nprobable symbols with shorter binary sequences, and less probable symbols with longer\nbinary sequences. The Huffman algorithm builds the binary tree representing the variable-\nlength code from the bottom up, starting with the least probable symbols.\nPlease complete the implementation of a Python function to build a Huffman code from a list\nof probabilities and symbols:\nencoding_dictionary = huffman(plist)=\nGiven plist, a sequence of tuples (prob,symbol), use the Huffman algorithm to\nconstruct and return a dictionary that maps symbols to their corresponding Huffman\ncodes, which should be represented as lists of binary digits.\nYou will find it necessary to select the minimum element from pList. During the process, if\nyou need to select the minimum element from a pList, you can use built-in Python functions\nsuch as sort() and sorted(). Python's heapq module is another alternative.\nPS1_1.py is the template file for this problem.\nThe testing code in the template runs your code through several test cases. You should see\nsomething like the following print-out (your encodings may be slightly different, although the\nlength of the encoding for each of the symbols should match that shown below):\n\nHuffman encoding:=\nB = 00=\nD = 01=\nA = 10=\nC = 11=\nExpected length of encoding a choice = 2.00 bits\nInformation content in a choice = 2.00 bits\nHuffman encoding:\nA = 00=\nD = 010=\nC = 011=\nB = 1=\nExpected length of encoding a choice = 1.66 bits\nInformation content in a choice = 1.61 bits\nHuffman encoding:\nII = 000\nI = 0010\nIII = 0011\nX = 010\nXVI = 011\nVI = 1\nExpected length of encoding a choice = 2.38 bits\nInformation content in a choice = 2.30 bits\nHuffman encoding:\nHHH = 0=\nHHT = 100=\nHTH = 101=\nTHH = 110=\nHTT = 11100=\nTHT = 11101=\nTTH = 11110=\nTTT = 11111=\nExpected length of encoding a choice = 1.60 bits\nInformation content in a choice = 1.41 bits=\nWhen you're ready, please submit the file with your code using the field below.\nFile to upload for Task 1:\n(points: 3)\nPython Task 2: Decoding Huffman-encoded messages (5 points)\nUseful download links:\nPS1_2.py -- template file for this task\nEncoding a message is a one-liner using the encoding dictionary returned by the huffman=\nroutine -- just use the dictionary to map each symbol in the message to its binary encoding\nand then concatenate the individual encodings to get the encoded message:\ndef encode(encoding_dict,message):\nreturn numpy.concatenate([encoding_dict[obj]=\n\nfor obj in message])=\nDecoding, however, is a bit more work. Write a Python function to decode an encoded\nmessage using the supplied encoding dictionary:\ndecoded_message = decode(encoding_dict,encoded_message)=\nencoded_message is a numpy array of binary values, as returned by the encode=\nfunction shown above (this array can be indexed just like a list). encoding_dict is a\ndictionary mapping objects to lists of binary characters, as with the output of your\nhuffman function from task 1.\nYour function should return (as a list) the sequence of symbols representing the\ndecoded message.\nPS1_2.py is the template file for this problem:\nWhen you're ready, please submit the file with your code using the field below.\nFile to upload for Task 2:\n(points: 5)\nPython Task 3: Huffman codes in practice -- fax transmissions (5 points)\nUseful download links:\nPS1_3.py -- Python file for this task\nPS1_fax_image.png -- fax image\nA fax machine scans the page to be transmitted, producing row after row of pixels. Here's\nwhat our test text image looks like:\n\nInstead of sending 1 bit per pixel, we can do a lot better if we think about transmitting the\nimage in chunks, observing that in each chunk we have alternating runs of white and black\npixels. What's your sense of the distribution of run lengths, for example when we arrange the\npixels in one long linear array? Does it differ between white and black runs?\nPerhaps we can compress the image by using run-length encoding, where we send the\nlengths of the alternating white and black runs, instead of sending the pixel pattern directly.\nFor example, consider the following representation of a 4x7 bit image (1=white, 0=black):\n1 1 0 0 1 1 1=\n1 1 1 0 0 1 1=\n1 1 1 1 0 0 1=\n1 1 1 1 1 1 1=\nThis bit image can be represented as a sequence of run lengths: [2,2,6,2,6,2,8]. If the receiver\nknows that runs alternate between white and black (with the first run being white) and that\nthe width of the image is 7, it can easily reconstruct the original bit pattern.\nIt's not clear that it would take fewer bits to transmit the run lengths than to transmit the\noriginal image pixel-by-pixel -- that'll depend on how clever we are when we encode the\nlengths! If all run lengths are equally probable then a fixed-length encoding for the lengths\n(e.g., using 8 bits to transmit lengths between 0 and 255) is the best we can do. But if some\nrun length values are more probable than others, we can use a variable-length Huffman code\n\nto send the sequence of run lengths using fewer bits than can be achieved with a fixed-length\ncode.\nPS1_3.py runs several encoding experiments, trying different approaches to using Huffman\nencoding to get the greatest amount of compression. As is often the case with developing a\ncompression scheme, one needs to experiment in order to gain the necessary insights about\nthe most compressible representation of the message (in this case the text image).\nPlease run PS1_3.py, look at the output it generates, and then tackle the questions below.\nHere are the alternative encodings we'll explore:\nBaseline 0 -- Transmit the b/w pixels as individual bits\nThe raw image contains 250,000 black/white pixels (0 = black, 1 = white). We could\nobviously transmit the image using 250,000 bits, so this is the baseline against which\nwe can measure the performance of all other encodings.\nBaseline 1 -- Encode run lengths with fixed-length code\nTo explore run-length encoding, we've represented the image as a sequence of\nalternating white and black runs, with a maximum run size of 255. If a particular run is\nlonger than 255, the conversion process outputs a run of length 255, followed by a run\nof length 0 of the opposite color, and then works on encoding the remainder of the run.\nSince each run length can be encoded in 8 bits, the total size of the fixed-length\nencoding is 8 times the number of runs.\nBaseline 2 -- Lempel-Ziv compressed PNG file\nThe original image is stored in a PNG-format file. PNG offers lossless compression\nbased on the Lempel-Ziv algorithm for adaptive variable-length encoding described in\nChapter 3. We'd expect this baseline to be very good since adaptive variable-length\ncoding is one of the most widely-used compression techniques.\nExperiment 1 -- Huffman-encoding runs\nAs a first compression experiment, try using encoding run lengths using a Huffman\ncode based on the probability of each possible run length. The experiment prints the 10\nmost-probable run lengths and their probabilities.\nExperiment 2 -- Huffman-encoding runs by color\nIn this experiment, we try using separate Huffman codes for white runs and black runs.\nThe experiment prints the 10 most-probable run lengths of each color.\nExperiment 3 -- Huffman-encoding run pairs\nCompression is always improved if you can take advantage of patterns in the message.\nIn our run-length encoded image, the simplest pattern is a white run of some length (the\nspace between characters) followed by a short black run (the black pixels of one row\nof the character).\nExperiment 4 -- Huffman-encoding 4x4 image blocks\nIn this experiment, the image is split into 4x4 pixel blocks and the sixteen pixels in each\nblock are taken to be a 16-bit binary number (i.e., a number in the range 0x0000 to\n\n0xFFFF). A Huffman code is used to encode the sequence of 16-bit values. This\nencoding considers the two-dimensional nature of the image, rather than thinking of all\nthe pixels as a linear array.\nThe questions below will ask you analyze the results. In each of the experiments, look closely\nat the top 10 symbols and their probabilities. When you see a small number of symbols that\naccount for most of the message (i.e., their probabilities are high), that's when you'd expect to\nget good compression from a Huffman code.\nThe questions below include the results of running PS1_3.py using a particular\nimplementation of huffman. Your results should be similar.\nA. Baseline 1:'=\ntotal number of runs: 37712'\nbits to encode runs with fixed-length code: 301696'=\nSince 301696 > 250000, using an 8-bit fixed-length code to encode the run lengths uses\nmore bits than encoding the image pixel-by-pixel. What does this tell you about the\ndistribution of run length values? Hint: what happens when runs are longer than 8 bits?\n(points: 1)\nB. Experiment 1:\nbits when Huffman-encoding runs: 111656\nTop 10 run lengths [probability]:\n1 [0.39]\n2 [0.19]\n3 [0.13]\n4 [0.12]\n5 [0.05]\n7 [0.03]\n6 [0.02]\n8 [0.01]\n0 [0.01]\n255 [0.01]=\nHow much compression did Huffman encoding achieve, expressed as the ratio of\nunencoded size to encoded size (aka the compression ratio)? Briefly explain why the\nHuffman code was able to achieve such good compression.\n(points: .5)\n\nBriefly explain why the probability of zero-length runs is roughly equal to the\nprobability of runs of length 255.\n(points: .5)\nC. Experiment 2:\nbits when Huffman-encoding runs by color: 95357\nTop 10 white run lengths [probability]:\n2 [0.25]\n4 [0.20]\n3 [0.19]\n5 [0.08]\n6 [0.05]\n7 [0.05]\n1 [0.04]\n8 [0.02]\n255 [0.02]\n10 [0.02]\nTop 10 black run lengths [probability]:\n1 [0.73]\n2 [0.13]\n3 [0.07]\n4 [0.03]\n0 [0.02]\n5 [0.01]\n7 [0.01]\n8 [0.00]\n9 [0.00]\n10 [0.00]=\nBriefly explain why the compression ratio is better in Experiment 2 than in Experiment\n1.\n(points: 1)\nD. Experiment 3:\nbits when Huffman-encoding run pairs: 87310\nTop 10 run-length pairs [probability]:\n(2, 1) [0.20]\n(4, 1) [0.15]\n(3, 1) [0.12]\n(5, 1) [0.07]\n(3, 2) [0.04]\n(7, 1) [0.03]\n(2, 2) [0.03]=\n\n(4, 2) [0.03]=\n(1, 1) [0.03]\n(6, 1) [0.03]=\nBriefly explain why the compression ratio is better in Experiment 3 than in\nExperiments 1 and 2.\n(points: 1)\nE. Experiment 4:\nbits when Huffman-encoding 4x4 image blocks: 71628\nTop 10 4x4 blocks [probability]:\n0xffff [0.55]\n0xbbbb [0.02]\n0xdddd [0.02]\n0xeeee [0.01]\n0x7777 [0.01]\n0x7fff [0.01]\n0xefff [0.01]\n0xfff7 [0.01]\n0xfffe [0.01]\n0x6666 [0.01]=\nUsing a Huffman code to encode 4x4 pixel blocks results in a better compression ratio\nthan achieved even by PNG encoding. Briefly explain why. [Note that the number of\nbits reported for the Huffman-encoded 4x4 blocks does not include the cost of\ntransmitting the custom Huffman code to the receiver, so the comparison is not really\napples-to-apples. But ignore this for now -- one can still make a compelling argument\nas to why block-based encoding works better than sequential pixel encoding in the case\nof text images.]\n(points: 1)\nPython Task 4: LZW compression (7 points)\nUseful download links:\nPS1_lzw.py -- template file for this task\nIn this task, you will implement the compression and decompression methods using the LZW\nalgorithm, as presented in class and in Chapter 3. (We won't repeat the description here.) Our\n\ngoal is to compress an input file to a binary file of codewords, and uncompress binary files\n(produced using compress) to retrieve the original file.\nWe will use a table size of 216 entries, i.e., each codeword is 16 bits long. The first 256 of\nthese, starting from 0000000000000000 to 0000000011111111 should be initialized to the\ncorresponding ASCII character; i.e., entry i should be initialized to chr(i) in Python for 0 ≤\ni=≤ 255.\nYour task is to write two functions: compress and uncompress:\ncompress(filename)\nTakes a filename as input and creates an LZW-compressed file named filename.zl (i.e.,\nappend the string \".zl\" to the input file name). The basic task is to correctly handle all\nfiles that require no more than 216 codewords. If a file has more than this number, you\nmay take one of two actions:\nterminate the program after printing the string \"This file needs more than 2**16\nentries\" OR\nOver-writing previously used table entries in compress() and uncompress=\n(consistently), coming up with a design of your own. This part is optional and\nwill not be graded, so please don't feel compelled to work on it. But if you have\nthe time and inclination and want to design this part (so you can handle large\nfiles, for instance), you are welcome to do it and demonstrate it during your\ncheckoff interview.\nuncompress(filename)\nTakes a filename as input and creates an uncompressed file named filename.u (i.e.,\nappend the string \"u\" to the input file name). For example, if the input file is test.zl,\nyour output file should be test.zl.u. If the file cannot be uncompressed successfully\nbecause it contains an invalid codeword, then terminate the program (don't worry about\nperfect error handling in this case; it's OK if the program terminates abruptly).\nYou can run the program as follows:\nTo compress: python PS1_lzw.py -f <filename>'\nTo uncompress: python PS1_lzw.py -f <filename> -u'=\nYou may find the following notes helpful for implementing these two functions in Python:\nThe main thing to keep in mind that the compressed file is a binary file; you can't just\nwrite out strings corresponding to the codewords into the output file and expect that it\nwill reduce space. The 16-bit number 0001010101011010, for example, should be\nstored in the file packed into two bytes, and not as a string of 16 characters (the latter\nwould take up 8 times as much space). So remember to open the output file as a binary\nwriteable file in compress() and remember to open the input file as a binary readable\nfile in uncompress().\nIf you aren't familiar with how to open, read, and write files, check out the Python\ndocumentation on file I/O. For this task, code like the following should suffice:\noutname = 'myoutputfilename' # note: should be filename + '.zl'=\n\nf = open(filename, 'rb') # open file in read binary mode=\ndata = f.read() # reads data from file f into a string\noutfile = open(outname, 'wb') # open file for binary writing =\nTo read and write binary data to/from files, you may find the array module useful. You\nmay also find the struct module useful, depending on how you write your code.\n16-bit codewords means that you can think of each codeword as an unsigned short\ninteger in your Python code; the typecode format \"H\" for such data will be convenient\nto use.\nThis is probably the simplest way of reading data into an array from a file :\nWhen you are reading two bytes at a time ie when decompressing a file (a\ncompressed file is the input)\n=\nf = open(filename, 'rb')\ncompressed = array.array(\"H\", f.read())=\n=\nWhen reading one byte at a time ie when compressing a file\nf = open(filename, 'rb')\ncompressed = array.array(\"B\", f.read())=\n=\nYour task is to implement both the compress and uncompress functions according to\nthe method described in lecture and Chapter 3. To increase the likelihood that your\nsoftware meets the intended specification according to Chapter 3, we have provided\nsome test inputs and outputs, which you can download here (these only test the cases\nwhen the number of codewords does not exceed 216). Each test file (a and rj) has a\ncorresponding .zl (a.zl and rj.zl) version. You may use these test files in debugging\nyour implementation (but note that working correctly on these tests does not\nnecessarily imply that your implementation is perfect). (We will use additional inputs to\ntest your code, but again not exceeding 216 codewords.)\nAs an added consistency check, please verify for some of your own test files that\nrunning compress followed by uncompress produces the original file exactly (you can\nuse diff on Linux or Mac machines to see if two files differ; you can use diff on\nWindows too if you install cygwin).\nWhen you're ready, please submit the file with your code using the field below.\nFile to upload for LZW task:\n(points: 7)\nQuestions for LZW task:\nA. Download this zip archive and run unzip to extract three compressed (.zl) files, g.zl,\ns.zl, and w.zl. For these files, run your program and enter the following information:\nThe ratio of the compressed file size to the original uncompressed version.\n(Optionally (and only optionally), compare the effectiveness of your\n\ncompression to the UNIX compress and gzip utilities, if you want to see how\nclose you get to these programs. Both programs are available on athena.)\nThe number of entries are in the string table (maintained by both compress and\nuncompress). You can print the length of the table in uncompress to obtain this\ninformation.\n(points: .5)\nB. Using your compress implementation, try compressing s.zl (from the zip archive)\neven though it is already compressed. Does it further reduce the file size? Why or why\nnot?\n(points: .5)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "6.02 Introduction to EECS II, Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/e152b00717ecd56108896ad71c310c29_MIT6_02F12_ps2.pdf",
      "content": "Your answers will be graded by actual human beings (at least that's what we believe!), so\ndon't limit your answers to machine-gradable responses. Some of the questions specifically\nask for explanations; regardless, it's always a good idea to provide a short explanation for\nyour answer.\nBefore doing this PSet, please read Chapters 5 and 6 of the readings. Also attempt the\npractice problems on this material; these practice problems include both linear block codes\nand convolutional codes; you may ignore the latter for now.\nPlease also note that solving the two Python programming tasks in this PSet will be useful in\nthe next PSet, which will require you to compare these block codes with convolutional codes,\nwhich you will learn next week.\nDue dates are as mentioned above. Checkoff interviews for PS2 and PS3 will be held\ntogether and will happen between October 4 and 8.\nProblem 1: Properties of linear block codes\nFor the nine statements given below, consider an (n, k, d) linear block code in systematic\nform. Fill in the blanks by writing the missing parts for each statement in the space provided\nbelow.\na. If the code can correct all single-bit errors, then the number of parity bits must be at\nleast\n.\nb. Suppose we transmit codewords over a binary symmetric channel with non-zero error\nprobability. Then, the total number of possible invalid received words (i.e., those not\nequal to a valid codeword) is\n.\nc. The weight of a codeword is the number of ones in it. The weight of each non-zero\ncodeword is at least\n.\nd. If the code can correct all single-bit errors, but not all 2-bit errors, then the possible\nvalue(s) of d is/are\n.\ne. If d is odd, then this code can be transformed into a (n+1, k, d+1) code by\n.\nf. Suppose two codewords c1 and c2 from this code have a Hamming distance of 4. If an\neven parity bit (i.e., a bit equal to the sum of the other bits) is appended to each\ncodeword to produce two new codewords, c1' and c2', the Hamming distance between\nc1' and c2' is\n.\nProblem Set 2\n\ng. The generator matrix for the code has rows and\ncolumns.\nh. The parity check matrix for the code has rows and\ncolumns.\ni. For a single-error correction code with parity check matrix H, each of the n syndromes\ncorresponding to an error case is obtained by calculating H vT. Here, v is a\nx\nmatrix with zeroes and\nones.\nj. With maximum-likelihood decoding, the syndrome 00...0 (all zeroes) corresponds to\nthe case when there are errors in the codeword.\n(points: 5)\nProblem 2: Decoding error probability\nA binary symmetric channel has bit-flip probability e. Suppose we take a stream of S bits in\nwhich zeroes and ones occur with equal probability, divide it into blocks of k bits each, and\napply an (n,k,3) code to each block. To correct bit errors, we decode each block of n bits at\nthe receiver using maximum-likelihood decoding. Your goal in this problem is to obtain a\nlower bound on the probability that the stream of S bits is decoded correctly. To this end, you\nmay assume that any n-bit block with two or more errors will not be decoded correctly. You\nmay also assume that S is an integral multiple of k. Show your work.\n(points: 2)\nProblem 3. Parity Inc.'s Code\nParity Inc. has developed a simple linear block code with three message bits, D1, D2, and D3,\nand three parity bits, P1, P2, and P3. The parity equations are:\nP1 = D1 + D2\nP2 = D1\n+ D3\nP3 = D1 + D2 + D3\n\nAll additions are modulo 2, as usual. Parity Inc. has implemented a maximum-likelihood\ndecoder that is able to correct a certain number of bit errors with this code. Assume that the\ndecoder returns the most-likely message sequence if the number of bit errors is no larger than\nthis correctable number, and returns \"uncorrectable errors\" otherwise. The sender transmits\ncoded bits in the order D1 D2 D3 P1 P2 P3.\nA. What is the minimum Hamming distance of Parity Inc.'s code? Explain your answer.\n(points: 1)\nB. If the receiver gets the word 111110, what will the decoder return?\n(points: 1)\nC. If the receiver gets the word 110100, what will the decoder return?\n(points: 1)\nProblem 4. Angry Coders (1 points)\nAngry Coders, Inc., is hiring smart hackers to join their team. You apply for a position. Your\ninterviewer asserts that their product uses a (19, 12, 5) linear block code that corrects all\ncombinations of 0-, 1-, and 2-bit errors. He asks you to construct such a code. How would\nyou respond to his question?\n\n(points: 1)\nProblem 5: Rectangular parity\nConsider the following variant of the rectangular parity code with r rows and c columns. Each\nrow has a row parity bit. Each column has a column parity bit. In addition, each codeword\nhas an overall parity bit, to ensure that the number of ones in each codeword is even.\nA. What is the rate of the code?\n(points: 0.5)\nB. Ben Bitdiddle takes each codeword of this code and removes the first row parity bit\nfrom each codeword. Note that the overall parity bit is calculated by including the row\nparity bit he eliminated. Ben then transmits these codewords over a noisy channel.\nWhat is the largest number of bit errors in a codeword that this new code can always\ncorrect? Explain.\n(points: 1)\nProblem 6: The Matrix Reloaded\nNeo receives a 7-bit string, D1 D2 D3 D4 P1 P2 P3 from Morpheus, sent using a code, C,\nwith parity equations\nP1 = D1 + D2 + D3\nP2 = D1 + D2 + D4\nP3 = D1 + D3 + D4\nA. Write down the generator matrix, G, for C.\n\n(points: 0.5)\nB. Write down the parity check matrix, H, for C.\n(points: 0.5)\nC. If Neo receives 1 0 0 0 0 1 0 and does maximum-likelihood decoding on it, what would\nhis estimate of the data transmission D1 D2 D3 D4 from Morpheus be? For your\nconvenience, the syndrome si corresponding to data bit Di being wrong are given\nbelow, for i=1, 2, 3, 4:\ns1 = (1 1 1)T, s2 = (1 1 0)T, s3 = (1 0 1)T, s4 = (0 1 1)T.\n(points: 0.5)\nOn Trinity's advice, Morpheus decides to augment each codeword in C with an overall\nparity bit, so that each codeword has an even number of ones. Call the resulting code\nC+ .\nD. Write down the generator matrix, G+, of code C+. Express your answer as a\nconcatenation (or stacking) of G (the generator for code C) and another matrix (which\nyou should specify). Explain your answer.\n(points: 1)\nE. Morpheus would like to use a code that corrects all patterns of 2 or fewer bit errors in\neach codeword, by adding an appropriate number of parity bits to the data bits D1 D2\nD3 D4. He comes up with a code, C++, which adds 5 parity bits to the data bits to\n\nproduce the required codewords. Explain whether or not C++ will meet Neo's error\ncorrection goal.\n(points: 1)\nPython programming tasks\nZip archive of all required files for the programming tasks on this PS. Extract using unzip.\nPython Task #1: Rectangular parity SEC code (6 points)\nUseful download link:\nPS2_rectparity.py -- template file for this task\nThe intent in this task is to develop a decoder for the rectangular parity single error correction\n(SEC) code using the structure of the rectangular parity code to \"triangulate\" the location of\nthe error, if any. Your job is to take a received codeword which consists of a data block\norganized into nrows rows and ncols columns, along with even parity bits for each row and\ncolumn. The codeword is represented as a binary sequence (i.e., a list of 0's and 1's) in the\nfollowing order:\n[D(0,0), D(0,1), ..., D(0,ncols-1),\n# data bits, row 0\nD(1,0), D(1,1), ...,\n# data bits, row 1\n...\nD(nrows-1,0),\n..., D(nrows-1,ncols-1), # data bits, last row\nR(0), ..., R(nrows-1),\n# row parity bits\nC(0), ..., C(ncols-1)]\n# column parity bits\nin other words, all the data bits in row 0 (column 0 first), followed all the data bits in row 1,\n..., followed by the row parity bits, followed by the column parity bits. The parity bits are\nchosen so that all the bits in any row or column (data and parity bits) will have an even\nnumber of 1's.\nDefine a Python function rect_parity(codeword, nrows, ncols) as follows:\nmessage_sequence = rect_parity(codeword,nrows,ncols)\ncodeword is a binary sequence of length nrows*ncols + nrows + ncols whose\nelements are in the order described above.\nThe returned value message_sequence should have nrows*ncols binary elements\nconsisting of the corrected data bits D(0,0), ..., D(nrows-1,ncols-1). If no correction is\nnecessary, or if an uncorrectable error is detected, then return the raw data bits as they\n\nappeared in the codeword.\nDo not use syndrome decoding in this task. (That will be the next programming task.) This\ngoal of this task is to use the structure of the parity computations to pinpoint error locations.\n(This method is much faster than syndrome decoding, but is far less general and varies from\ncode to code.)\nPS2_tests.even_parity(seq) is a function that takes a binary sequence seq and returns\nTrue if the sequence contains an even number of 1's, otherwise it returns False. This parity\ncheck will be useful when performing the parity computations necessary to do error\ncorrection. PS2_rectparity.py is a template that you will extend by writing the function\nrect_parity.\nThe PS2_tests.test_correct_errors function will try a variety of test codewords and\ncheck for the correct results. If it finds an error, it'll tell you which codeword failed; if your\ncode is working, it'll print out\nTesting all 2**n = 256 valid codewords\n...passed\nTesting all possible single-bit errors\n...passed\n(8,4) rectangular parity code successfully passed all 0,1 and 2 bit error tests\nWhen you're ready, please submit the file with your code using the field below.\nTrue\n(points: 6)\nPython Task #2: Syndrome Decoding of Linear Block Codes\nUseful download link:\nPS2_syndecode.py -- template file for this task\nWrite syndrome_decode, a function that takes the four arguments given below and returns an\narray of bits corresponding to the most likely transmitted message:\nsyndrome_decode(codeword, n, k, G)\ncodeword is a numpy array of bits received at the decoder. The array has size n, which we\nexplicitly specify as an argument to syndrome_decode for clarity. The original message is k\nbits long. G is the generator matrix of the code. See Chapter 6 for what the generator matrix\nmeans and the syndrome decoding procedure.\nYou may assume that G is a code that can correct all combinations of single bit errors. So your\nsyndrome decoder needs to handle only that case. Of course, you may feel free to be\nambitious and handle up to t bit errors, but our testing won't exercise those cases.\n\nThe following implementation notes may be useful:\n1. Syndrome decoding uses matrix operations. You will very likely find it convenient to\nuse numpy's matrix module, which is described fairly succinctly in this tutorial. The\ntable on this page titled \"Linear Algebra Equivalents\" is a useful resource.\n2. All arithmetic should be modulo 2, in F_2. When you multiply two matrices whose\nelements are all 0 or 1, you may get numbers different from 0 or 1. Of course, one\nneeds to replace each such number with its modulo-2 value. We have given you a\nfunction, mod2(A), which does that for an integer matrix, A.\n3. We have also given you a simple function, equal(a, b), which returns True iff two\nmatrices a and b are equal element-by-element.\n4. In this task, we are not expecting you to pre-compute the syndromes for the code, but\nto compute the syndromes in syndrome_decode just before decoding a codeword. It\nwould be more efficient to pre-compute syndromes, but it would also mean that we\nneed to specify the interface for a function like compute_syndromes, and for your\nsoftware to adhere to that interface. In the interest of simplifying the interfaces and\nspecifying only the behavior of syndrome_decode, we will take the small performance\nhit and just have you compute the syndromes every time you decode a codeword.\nTesting: We will test your code with a few different generator matrices. These tests will tell\nyou whether your code works on that input; if not, it will print out the input on which the\ndecoder failed (printing out what it produced and what was expected) and exit. We test all\nsingle-bit error patterns and the no-error case over the n-bit codeword.\nA successful test prints out lines similar to these:\nTesting all 2**k = 16 valid codewords\n...passed\nTesting all n*2**k = 112 single-bit error codewords\n...passed\nAll 0 and 1 error tests passed for (7,4,3) code with generator matrix G =\n[[1 0 0 0 1 1 0]\n[0 1 0 0 1 0 1]\n[0 0 1 0 0 1 1]\n[0 0 0 1 1 1 1]]\nA failed test might print out lines similar to these:\nTesting all 2**k = 16 valid codewords\n...passed\nTesting all n*2**k = 112 single-bit error codewords\nOOPS: Error decoding [1 0 0 0 0 0 0] ...expected [0 0 0 0] got [1 0 0 0]\nWhen you're ready, please submit the file with your code using the field below.\nTrue\n(points: 9)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "6.02 Introduction to EECS II, Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/a77a0ca2a20366c9a750ff57a25dc6e6_MIT6_02F12_ps3.pdf",
      "content": "'\n'\nual human beings (at least that s what we believe!), so\ndon' t limit your answers to machine-gradable responses. Some of the questions specifically\nask for explanations; regardless, it s always a good idea to provide a short explanation for\nyour answer.\nBefore doing this PSet, please read Chapters 7 and 8 of the readings. Also attempt the\npractice problems on this material.\nDue dates are as mentioned above. Checkoff interviews for PS2 and PS3 will be held\ntogether and will happen between October 4 and 8.\nProblem 1.\nConsider a convolutional code with three generators, and with state transition diagram shown\nbelow:\nA. Deduce the equations defining the three parity bits.\n(points: 1.5)\nProblem Set 3\nYour answers will be graded by act\n\nB. What are the constraint length and code rate for this code?\n(points: 0.5)\nC. What sequence of bits would be transmitted for the message \"1011\"? Assume the\ntransmitter starts in the state labeled \"0\".\n(points: 0.5)\nThe Viterbi algorithm is used to decode a sequence of received parity bits, as shown in the\nfollowing trellis diagram:\nD. Determine the path metrics for the last two columns of the trellis.\n(points: 0.5)\n\n'\nE. What is the most likely state of the transmitter after time 5?\n(points: 0.5)\nF. What is the most likely message sequence, as decoded using the trellis above? And how\nmany bit errors have been detected for this most likely message?\n(points: 0.5)\nG. What is the free distance of this code?\n(points: 0.5)\nProblem 2.\nLet conv_encode(x) be the resulting bit-stream after encoding bit-string x with a\nconvolutional code, C. Similarly, let conv_decode(y) be the result of decoding y to produce\nthe maximum-likelihood estimate of the encoded message. Suppose we send a message M\nusing code C over some channel. Let P = conv_encode(M) and let R be the result of sending\nP over the channel and digitizing the received samples at the receiver (i.e., R is another\nbit-stream). Suppose we use Viterbi decoding on R, knowing C, and find that the maximum-\nlikelihood estimate of M is M . During the decoding, we find that the minimum path metric\namong all the states in the final stage of the trellis is D. Then, D is the Hamming distance\nbetween two quantities. What are the two quantities? Consider both the cases when the\ndecoder corrects all the errors and when it does not. Explain your answer. (This question\nrequires some careful thinking.)\n(points: 2)\n\n=\n=\n'\nProblem 3.\nA technique called puncturing can be used to obtain a convolutional code with a rate higher\nthan 1/r, when r generators are used. The idea is to produce the r parity streams as before and\nthen carefully omit sending some of the parity bits that are produced on one or more of the\nparity streams. One can express which bits are sent and which are not on each stream as a\nvector of 1' s and 0' s: for example, on a given stream, if the normal convolutional encoder\nproduces 4 bits on a given parity stream, the vector (1 1 0 1) says that the first, second, and\nfourth of these bits are sent, but not the third, in the punctured code.\nA. Suppose we start with a rate-1/2 convolutional code without puncturing. The encoder\nthen uses the vector (1 0) on the first parity stream and the vector (1 1) on the second\none: that is, it sends the first parity bit but not the second on the first parity stream and\nevery parity bit produced on the second one. What is the rate of the resulting\nconvolutional code?\n(points: 0.5)\nB. When using a punctured code, missing parity bits don' t participate in the calculation of\nbranch metrics. Consider the following trellis from a transmission using K 3, r 1/2\nconvolutional code that experiences a single bit error:\nHere s the trellis for the same transmission using the punctured code from part (A) that\nexperiences the same error:\n\nFill in the path metrics, remembering that with the punctured code from part (A) the\nbranch metrics are calculated as follows:\nBM(-0,00)=0 BM(-0,01)=1 BM(-0,10)=0 BM(-0,11)=1\nBM(-1,00)=1 BM(-1,01)=0 BM(-1,10)=1 BM(-1,11)=0\nwhere the missing incoming parity bit has been marked with a \"-\". Use a text\nrepresentation of the matrix (a 2-D array of numbers) when typing in the answer.\n(points: 1)\nC. Did using the punctured code result in a different decoding of the message?\n(points: 0.5)\nProblem 4.\nBen enters the 6.02 lecture hall just as the professor is erasing the convolutional coding with\nViterbi decoding example he has worked out, and finds the trellis shown below with several\nof the numbers and bit strings erased (yeah, the professor is strangely obsessive about how he\nerases items!).\n\nYour job is to help Ben piece together the missing information by answering the questions\nbelow. Provide a brief explanation.\na. The constraint length of this convolutional code is .\n(points: .25)\nb. The rate of this convolutional code is .\n(points: .25)\nc. Assuming hard-decision Viterbi decoding, write the missing path metric numbers inside\nthe boxes in the picture above. (Work carefully and check your work to avoid a\ncascade of errors!)\n(points: 1)\n\nd. What is the received parity stream, which the professor has erased? Write your\nanswer in order from left to right in the space below.\n(points: 1)\ne. What is the maximum-likelihood message returned by the Viterbi decoder, if we\nstopped the decoding process at the last stage in the picture shown? Explain your\nanswer below.\n(points: 1)\nProblem 5.\nDona Ferentes is debugging a Viterbi decoder for her client, The TD Company, which is\nbuilding a wireless network to send gifts from mobile phones. She picks a rate-1/2 code with\nconstraint length 4, no puncturing. Parity stream p0 has the generator g0 = 1110. Parity\nstream p1 has the generator g1 = 1xyz, but she needs your help determining x,y,z, as well as\nsome other things about the code. In these questions, each state is labeled with the\nmost-recent bit on the left and the least-recent bit on the right.\nThese questions are about the state transitions and generators.\na. From state 010, the possible next states are ___ and ___.\n(points: 0.25)\nb. From state 010, the possible predecessor states are ___ and ___.\n\n(points: 0.25)\nc. Given the following facts, find g1, the generator for parity stream p1. g1 has the form\n1xyz, with the standard convention that the left-most bit of the generator multiplies the\nmost-recent input bit.\nStarting at state 011, receiving a 0 produces p1 = 0.\nStarting at state 110, receiving a 0 produces p1 = 1.\nStarting at state 111, receiving a 1 produces p1 = 1.\n(points: 1)\nd. Dona has just completed the forward pass through the trellis and has figured out the\npath metrics for all the end states. Suppose the state with smallest path metric is 110.\nThe traceback from this state looks as follows:\n000 ← 100 ← 010 ← 001 ← 100 ← 110\nWhat is the most likely transmitted message? Explain your answer, and if there is not\nenough information to produce a unique answer, say why.\n(points: 0.5)\nDuring the decoding process, Dona observes the voltage pair (0.9, 0.2) volts for the\nparity bits p0, p1, where the sender transmits 1.0 volts for a \"1\" and 0.0 volts for a \"0\".\nThe threshold voltage at the decoder is 0.5 volts. In the portion of the trellis shown\nbelow, each edge shows the expected parity bits p0, p1. The number in each circle is\nthe path metric of that state.\ne. With hard-decision decoding, give the branch metric near each edge and the path\nmetric inside the circle.\n\n(points: 0.5)\nf. Timmy Dan (founder of TD Corp.) suggests that Dona use soft-decision decoding using\nthe squared Euclidean distance metric. Give the branch metric near each edge and the\npath metric inside the circle.\n(points: 0.5)\nPython Task 1: Viterbi decoder for convolutional codes using hard-decision decoding\nUseful download links:\nPS3_tests.py -- test jigs for this assignment\nPS3_viterbi.py -- template file for this task\nAs explained in lecture and Chapter 7, a convolutional encoder is characterized by two\nparameters: a constraint length K and a set of r generator functions {G0, G1, ...Gr}. The\nencoder processes the message one bit at a time, generating a set of r parity bits {p0, p1,\n...} by applying the generator functions to the current message bit, x[n], and K-1 of the\nprevious message bits, x[n-1], x[n-2], ..., x[n-(K-1)]. The r parity bits are then\ntransmitted and the encoder moves on to the next message bit.\nThe operation of the encoder may be described as a state machine, as explained in Chapter 7.\nThe generator functions corresponding to the parity equations can be described compactly by\nsimply listing its K coefficients as a K-bit binary sequence, or even more compactly (for a\nhuman reader) if we construe the K-bit sequence as an integer, e.g., G0: 7, G1: 5, etc. Here,\n\"7\" stands for the generator \"111\" and \"5\" for the generator \"101\", which in turn work out to\n\n'\n'\n'\nx[n] + x[n-1] + x[n-2], and x[n] + x[n-2], respectively. We will use this integer\nrepresentation in this task.\nAs explained in Chapter 8, the Viterbi decoder works by determining a path metric PM[s,n]\nfor each state s and bit time n. Consider all possible encoder state sequences that cause the\nencoder to be in state s at time n. In hard-decision decoding, the most-likely state sequence is\nthe one that produced the parity bit sequence nearest in Hamming distance to the sequence of\nreceived parity bits. Each increment in Hamming distance corresponds to a bit error. In this\ntask, PM[s,n] records this smallest Hamming distance for each state at the specified time.\nThe Viterbi algorithm computes PM[...,n] incrementally. Initially\nPM[s,0] = 0 if s == starting_state else inf\nThe decoder uses the first set of r parity bits to compute PM[...,1] from PM[...,0]. Then, it\nuses the next set of r parity bits to compute PM[...,2] from PM[...,1]. It continues in this\nfashion until it has processed all the received parity bits.\nThe ViterbiDecoder class contains a method decode, which does the following (inspect its\nsource code for details):\n1. Initialize PM[...,0] as described above.\n2. Use the Viterbi algorithm to compute PM[...,n] from PM[...,n-1] and the next r parity\nbits; repeat until all received parity bits have been consumed. Call the last time point N.\n3. Identify the most-likely ending state of the encoder by finding the state s that has the\nmimimum value of PM[s,N].\n4. Trace back along the most likely path from start state to s to build a list of'\ncorresponding message bits, and return that list.\nIn this task you will write the code for three methods of the ViterbiDecoder class, which\nare crucial in the steps described above. Once these methods are implemented, one can make\nan instance of the class, supplying K and the parity generator functions, and then use the\ninstance to decode messages transmitted by the matching encoder.\nThe decoder will operate on a sequence of received voltage samples; the choice of which\nsample to digitize to determine the message bit has already been made, so there s one voltage\nsample for each bit. The transmitter has sent a 0 Volt sample for a \"0\" and a 1 Volt sample for\na \"1\", but those nominal voltages have been corrupted by additive Gaussian noise zero\nmean and non-zero variance. Assume that 0 s and 1 s appear with equal probability in the\ntransmitted message.\nPS3_viterbi.py is the template file for this task.\nYou need to write the following functions:\nnumber = branch_metric(self,expected,received)\nexpected is an r-element list of the expected parity bits (or you can also think of them\n\n'\nas voltages given how we send bits down the channel). received is an r-element list of\nactual sampled voltages for the incoming parity bits (floats in the range [0,1]). In this\ntask, we will do hard-decision decoding; that is, we will digitize the received voltages\nusing a threshold of 0.5 volts to get bits, and then compute the Hamming distance\nbetween the expected sequence and the received sequences. That Hamming Distance\nwill be our branch metric for hard-decision decoding.\nYou may use PS3_tests.hamming(seq1,seq2), which computes the Hamming\ndistance between two binary sequences.\nviterbi_step(self,n,received_voltages)\nupdate self.PM[...,n] using the batch of r parity bits and self.PM[...,n-1] computed\non the previous iteration. In addition to making an entry for self.PM[s,n] for each\nstate s, keep track of the most-likely predecessor for each state in the\nself.Predecessor array. You' ll find the following instance variables and functions\nuseful (please read the code we have provided to understand how to use them --\nlearning to read someone else s code is a useful life skill :-)\nself.predecessor_states\nself.expected_parity\nself.branch_metric()\ns = most_likely_state(self,n)\nIdentify the most-likely state of the encoder at time n by finding the state s that has the\nmimimum value of PM[s,n] If there are several states with the same minimum value,\nchoose one arbitrarily. Return the state s.\nmessage = traceback(self,s,n)\nStarting at state s at time n, use the self.Predecessor array to trace back through the\ntrellis along the most-likely path, determining the corresponding message bit at each\ntime step. Note that you' re tracing the path backwards through the trellis, so that you' ll\nbe collecting the message bits in reverse order -- don' t forget to put them in the right\norder before returning the final decoded message at the result of this procedure. You\ncan determine the message bit at each state along the most likely path from the state\nitself -- think about how the current message bit is incorporated into the state when the\ntransmitter makes a transition in the state transition diagram. Be sure that your code\nwill work for different values of self.K. Also, note that we are storing these states as\nintegers (e.g. state ' 101' is stored as 5); you may find the >> (right shift) operator, or the\nbin function useful in extracting specific bits from integers.\nThe testing code at the end of the template tests your implementation on a few messages and\nreports whether the decoding was successful or not. Note that passing these tests does not\nguarantee that your code is correct, and your code may be graded on different test\ncases; you should make fairly certain that your code will work for arbitrary tests (i.e.,\nnot just the ones in the file) before submitting.\nFile to upload for Task 1:\n\n'\n\n(points: 8)\nHere s the debugging printout generated when Alyssa P. Hacker ran her correctly written\ncode for the (3, (7,6)) convolutional code:\nFinal PM table :\ninf inf 1\ninf 0\ninf inf 1\nFinal Predecssor table :\nEach column shows the new column added to the PM and Predecessor matrices after each\ncall to viterbi_step. The last column indicates that the most-likely final state is 0 and that\nthere were two bit errors on the most-likely path leading to that state.\nA. Consider the most-likely path back through the trellis using the Predecessor matrix. At\nwhich processing steps did the decoder detect a transmission error? Call the transition\nfrom column 1 of the debugging printout to column 2 Step 1, from Column 2 to 3 Step\n2 and so on.\n(points: 0.5)\nBen Bitdiddle ran Task #1 and found the following path metrics at the end for the four states:\n[ 6200.\n6197.\n6199.\n6199.]\nB. What is the most likely ending state? And what can you say about the number of errors\nthe decoder detected? Were they all corrected?\n(points: 0.5)\n\n'\n'\n=\nHere s the debugging output from running decoder tests with a 500000-bit message a K = 3'\ncode with generators 7 (111) and 6 (110).\n**** ViterbiDecoder: K=3, rate=1/2 ****\nBER: without coding = 0.006200, with coding = 0.000218\nTrellis state at end of decoding:\n[ 6200.\n6197.\n6199.\n6199.]\nPlease answer the following questions based on the above output for the K = 3 code:\nC. What was the most-likely final state of the transmitter? And what were the most-likely\nlast two bits of the message?\n(points: 0.5)\nD. The test message used was 500,000 bits long. Since we re using a rate 1/2\nconvolutional code, there were 1,000,000 parity bits transmitted over the channel. The\nBER \"without coding\" was calculated by comparing the transmitted bit stream that\nwent into the noisy channel with the bit stream that came out of the noisy channel.\nHow many transmission errors occurred?\n(points: 0.5)\nE. After passing the received parity bits through the decoder and calculating the\nmost-likely transmitted message, the BER \"with coding\" was calculated by comparing\nthe original message with the decoded message. In this example, the density of bit\nerrors was high enough that the decoder was unable to find and correct all the\ntransmission errors. How many uncorrected errors remained after the decoder did the\nbest it could at correcting errors?\n(points: 0.5)\n\n'\n=\n=\nHere s the debugging output from running decoder tests with a 500000-bit message a K = 4'\ncode with generators 0xD (1101) and 0xE (1110).\n**** ViterbiDecoder: K=4, rate=1/2 ****\nBER: without coding = 0.006200, with coding = 0.000022\nTrellis state at end of decoding:\n[ 6203.\n6204.\n6205.\n6200.\n6203.\n6202.\n6203.\n6202.]\nF. What was the most-likely final state of the transmitter? And what were the most-likely\nfinal three bits of the message?\n(points: 0.5)\nG. How many uncorrected errors remained after the decoder did the best it could at\ncorrecting errors?\n(points: 0.5)\nH. Consider a given message bit. How many transmitted (coded) bits are influenced by\nthat message bit in both the K = 3 and K = 4 cases?\n(points: 0.5)\nPython Task 2: Soft-decision Viterbi decoding\nUseful download link:\nPS3_softviterbi.py -- template file for this task\n\nLet 's change the branch metric to use soft decision decoding, as described in lecture and\nChapter 8. The soft metric we 'll use is the square of the Euclidean distance between the\nreceived vector of dimension r, the number of parity bits produced per message bit, of\nvoltage samples and the expected r-bit vector of parities. Just treat them like a pair of\nr-dimensional vectors and compute the squared distance.\nPS3_softviterbi.py is the template file for this task.\nComplete the branch_metric method for the SoftViterbiDecoder class. Note that other\nthan changing the branch metric calculation, the hard-decision and soft-decision decoders are\nidentical. The code we have provided runs a simple test of your implementation. It also tests\nwhether the branch metrics are as expected.\nFile to upload for Task 2:\n(points: 2)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "6.02 Introduction to EECS II, Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/7c428cc7bf4a0704076fc14efca423d5_MIT6_02F12_ps4.pdf",
      "content": "Your answers will be graded by actual human beings (at least that's what we believe!), so\ndon't limit your answers to machine-gradable responses. Some of the questions specifically\nask for explanations; regardless, it's always a good idea to provide a short explanation for\nyour answer.\nBefore doing this PSet, please read Chapters 9 and 10 of the readings. Also attempt the noise\nand LTI practice problems on this material.\nProblem 1. What's the SNR?\nBen Bitdiddle is running a communication link between a sender and receiver, transmitting a\nbit stream of equally likely \"1\"s and \"0\"s. His scheme uses a voltage of +sqrt(Es) to send\nbinary digit \"1\", and a voltage of either 0 or -sqrt(Es) to send binary digit \"0\", depending on\nwhether he uses on-off or bipolar signaling, respectively. Assume these voltage levels are\npreserved at the receiver in the absence of noise. The receiver takes a single sample in each\nbit slot, and uses a threshold halfway between the two nominal voltage levels for deciding\nwhether a received sample is \"1\" or \"0\". Assume the samples are perturbed by additive white\nGaussian noise (AWGN), with N0 denoting twice the noise variance.\nIn this problem you will help Ben evaluate the two schemes.\nProblem Set 4\n\nYou may use the picture shown below, which plots the function 0.5*erfc(sqrt(x)) as a\nfunction of x on the dB scale. You ought to be able to eyeball the result from the picture (a\nvery useful skill for an engineer; it's the fastest way to answer this question), and then\nconfirm that your answer is correct by typing in numerical formulas into WolframAlpha.\n(Pic from http://www.tlc.polito.it/~gaudino/foundations/didactical_material/erfc_curves/)\nWe may not have been as clear as we should have been about the definition of SNR in\nthis problem. It turns out that there are many reasonable interpretations/definitions of\nSNR in different settings.\nOn p.118 of the notes, in connection with bipolar communication, we refer to E s/N 0\nas the SNR. The square root of this ratio is what appears in the erfc expression in Eq.\n(9.9) for the bit error rate, BER.\nWhat we intended in this problem was for you to figure out how the BER expression\nchanged in going from bipolar to on-off signaling. You should find that BER = 0.5\nerfc(sqrt{something}) for on-off signaling. That \"something\" is what we were thinking\nof as the effective SNR for on-off signaling.\nA. What happens to the SNR when Ben goes from on-off to bipolar signaling?\n(points: 1)\nCourtesy of Roberto Gaudino. Used with permission.\n\nB. Suppose we were operating at 2dB SNR with an on-off scheme. Read the graph to get\nan approximate BER.\n(points: 0.5)\nC. Use the graph again to figure out what the BER is for bipolar signaling under the same\nnoise conditions as in part (B).\n(points: 0.5)\nD. Suppose in the on-off scheme considered in (B) we were to average 4 samples in each\nbit slot before comparing to a threshold. The AWGN assumption guarantees that noise\nsamples are independent on each sample. What is the BER now?\n(points: 2)\nProblem 2. Attenuation\nThe cable television signal in your home is poor. The receiver in your home is connected to\nthe distribution point outside your home using two coaxial cables in series, as shown in the\npicture below. The power of the cable signal at the distribution point is P. The power of the\nsignal at the receiver is R.\n\nThe first cable attenuates (i.e., reduces) the signal power by 7 dB. The second cable\nattenuates the signal power by an additional 13 dB. Calculate P/R as a numeric ratio.\n(points: 1)\nProblem 3. Mininmum-error threshold detection for 4-level signaling\nBen Bitdiddle studies the bipolar signaling scheme from 6.02 and decides to extend it to a\n4-level signaling scheme, which he calls Ben's Aggressive Signaling Scheme, or BASSTM. In\nBASS, the transmitter can send four possible signal levels, or voltages: (-3A, -A, +A, +3A)\nwhere A is some positive value. To transmit bits, the sender's mapper maps consecutive pairs\nof bits to a fixed voltage level that is held for some fixed interval of time, creating a symbol.\nFor example, we might map bits \"00\" to -3A, \"01\" to -A, \"10\" to +A, and \"11\" to +3A. Each\ndistinct pair of bits corresponds to a unique symbol. Call these symbols s_minus3, s_minus1,\ns_plus1, and s_plus3. Each symbol has the same prior probability of being transmitted.\nThe symbols are transmitted over a channel that has no distortion but does have additive\nnoise, and are sampled at the receiver in the usual way. Assume the samples at the receiver\nare perturbed from their ideal noise-free values by a zero-mean additive white Gaussian noise\n(AWGN) process with noise intensity N0 = 2σ2, where 2σ2 the variance of the Gaussian noise\non each sample. In the time slot associated with each symbol, the BASS receiver digitizes a\nselected voltage sample, r, and returns an estimate, s, of the transmitted symbol in that slot,\nusing the following intuitive digitizing rule (written in Python syntax):\ndef digitize(r):\nif r < -2*A: s = s_minus3\nelif r < 0: s = s_minus1\nelif r < 2*A: s = s_plus1\nelse: s = s_plus3\nreturn s\nThe figure below illustrates the detection strategy:\n\nConditional noise distributions P(s | r)\nBen wants to calculate the symbol error rate for BASS, i.e., the probability that the symbol\nchosen by the receiver was different from the symbol transmitted. Note: we are not interested\nin the bit error rate here. Help Ben calculate the symbol error rate by answering the following\nquestions.\nA. Suppose the sender transmits symbol s_plus3. What is the conditional symbol error\nrate given this information; i.e., what is P(symbol error | s_plus3 sent)? Express your\nanswer in terms of A, N0, and the erfc function defined as\n(points: 1.5)\nB. Now suppose the sender transmits symbol s_plus1. What is the conditional symbol\nerror rate given this information, in terms of A, N0, and the erfc function? The\nconditional symbol error rates for the other two symbols don't need to be calculated\nseparately.\n(points: 1.5)\nC. The symbol error rate when the sender transmits symbol s_minus3 is the same as the\nsymbol error rate of which of these symbols?\ni. s_minus1\nii. s_plus1\niii. s_plus3\nerfc(z) =\ndx.\nπ\n√\n∫\ninf\nz\ne-x2\n\n(points: 0.5)\nD. The symbol error rate when the sender transmits symbol s_minus1 is the same as the\nsymbol error rate of which of these symbols?\ni. s_minus3\nii. s_plus1\niii. s_plus3\n(points: 0.5)\nE. Combining your answers to the previous parts, what is the symbol error rate in terms of\nA, N0, and the erfc function? Recall that all symbols are equally likely to be\ntransmitted.\n(points: 2)\nProblem 4. Averaging filter\nIn this problem set we will build a communication system using your computer's speakers and\nmicrophone as transmitter and receiver, respectively. We will multiply the message sequence\nwith a high-frequency sine wave sequence (the carrier) at the transmitter in order to match\nthe transmitted wavelength with the characteristic length of your antenna (your speakers) and\nthereby radiate power efficiently. The receiver will recover the message sequence by running\nthe received sequence through an averaging filter represented by the following difference\nequation:\nA. Show that the system is linear and time-invariant.\ny[n] =\n(x[n -k] + x[n -k -1] + ⋯+ x[n -k -m])\nm + 1\n\n(points: 1)\nB. Find the unit-sample response of this system for k=2, m=5.\n(points: 0.5)\nC. Compute the output y[n] when k=2, m=5 and the input sample sequence is\nu[n] is the unit-step function and it's defined as\n(points: 0.5)\nD. Compute the output y[n] when k=2, m=5 and the input sequence is\n(points: 1)\nProgramming Tasks\nPSets 4-6 will use the Audiocom communication system, which uses your computer's\nspeakers to transmit signals, and your computer's microphone to receive signals. In this lab,\nwe will be exploring the effect of noise in communication channels, and working with a\nx[n] = 2u[n -3] -2u[n -7].\nu[n] = {1,\n0,\nn ≥0\nn < 0\nx[n] = 12 + sin(\n).\nπn\n\nparticular demodulation scheme known as envelope detection.\nTask 1: Audiocom\nThe first task is simply to familiarize yourself with the Audiocom system, and to use it to send\na text file from your speakers to your microphone.\nDownload Audiocom and unzip it. For PS4, there are two .pyc files: preamble.pyc and\ndemodulate_audiocom.pyc that you need to set to the appropriate pyc file depending on\nwhether you have Python 2.6 or 2.7. For example, if you have Python 2.7, symbolically link\npreamble_27.pyc to preamble.pyc (or rename the former to the latter). Similarly for\ndemodulate_audiocom.pyc\nThe documentation for Audiocom is available at http://audiocom602.blogspot.com. This\ndocument should be your starting point; it contains some useful tips for getting started and\nsimple debugging.\nOnce you have successfully sent some bits using the process described in section 3.1 of the\ndocumentation, it might be fun to send a text file. We have included several test files\n(conveniently located in the testfiles directory). Replacing the -S 1 and -n 100 options\nwith -f testfiles/A, for example, will attempt to send the text \"Mens et manus\" across\nyour audio channel. (Note that if you're using IDLE, command-line options won't work and\nyou have to specify the program's run-time parameters in config.py as explained in the\ndocumentation.) If all goes well, you should see that text appear shortly after transmission.\nTask 2: Bypass Channel\nWe will first implement part of our Bypass Channel, which we will expand in PSets 5 and 6.\nThis synthetic model of the communication channel will prove to be useful and informative\nmoving forward, primarily as a means of debugging various demodulation schemes. At the\nvery least, it will save us from having to listen to lots of long beeps during our initial\ndebugging stages.\nAs mentioned in lecture, we will model environmental noise as a zero-mean Gaussian random\nvariable. In the next section, we will try to convince ourselves that this is a decent model, but\nfor now, we will simply implement it.\nThe file bypass_channel.py provides a template class BypassChannel. Notice that its\n__init__ method takes three arguments. For this lab, we will only need to consider the first\nargument, noise; we will get to lag and h in future labs.\nWe will implement our synthetic channel by filling in the method xmit_and_recv, which\ntakes a list of samples we wish to transmit, and returns a list of samples modeling those we\nwould expect to receive, were we actually transmitting over the audio channel. For now, it\nwill suffice to add a sample from a zero-mean Gaussian distribution to each element in\ntx_samples. The variable self.noise contains the desired variance of the noise.\nTake each input sample in the numpy array, tx_samples and return a numpy array in which\neach sample is replaced with the original value plus a Gaussian random variable with zero\nmean and variance given by self.noise. You may find numpy.random useful (especially the\n\nnormal function in that module).\nUpload your bypass_channel.py:\n(points: 4)\nTask 3: Measuring Noise\nIn this section, we will look at the effect of noise on our communcation channel, and will also\ntry to convince ourselves that the Gaussian model of environmental noise is, in fact,\nreasonable.\nThroughout this task, you will be making use of our envelope demodulator (which you will\nimplement in the next task). This demodulator begins by \"rectifying\" the signal; that is, by\ntaking the absolute value of the received samples so that every value is positive. Then, we\nreplace each sample by a value representing the mean of the next few samples (specifically,\nthe mean over the next half-period of the carrier waveform). This averaging filter was\ndiscussed in lecture. Given a carrier frequency and sampling rate, you can figure out easily\nhow many samples are in one period of the waveform, and divide that number by 2 (more on\nthis averaging in the next task).\nWe will start by taking a few measurements. First, let's try sending a bunch of 1's across the\nchannel, and recording the voltages we receive back. Let's start by running sendrecv.py\nwith the options -S 1 (send only 1's), -H (send a header), -g (display graphs), -n 500 (send\n500 bits), -c 1000 (use a 1000 Hz carrier), and -o 1.0 (use a voltage of 1.0 to represent a\n1). Some other options (such as -q, -i and -p) may be necessary for some machines. You\nshould have found acceptable values for these options in task 1; make sure to use those\nvalues again here!\nThe bottom plot displayed by the -g option is a histogram of the demodulated samples.\nAssuming that environmental noise is Gaussian, we should expect to see a roughly Gaussian-\nshaped distribution over received samples.\nSave and upload a graph that results from running this test:\n1's plot\n(points: 0)\nWhat does your plot look like? In some cases, it might not look Gaussian, but look like two or\nmore Gaussians with different means added together. Why does that happen (hint: think\nabout what might happen if too few samples are being averaged together).\n\n(points: 1.5)\nWe also made an assumption that environmental noise is not only Gaussian, but is additive as\nwell; that is, the effect noise has on a signal does not depend on the value being sent. Let's\ntest this by sending 1's at a lower voltage. Try all of the same options as above, but change -o\n1.0 to -o 0.7. (You may also achieve this goal by reducing the transmission volume.)\nSave and upload a graph that results from running this test:\n1's plot: 0.7v\n(points: 0)\nTry running this a few times. Compare and contrast the plots you get from this test, versus the\nplots you generated with -o 1.0:\n(points: 1)\nLet's now try sending 0's instead of 1's. Change -S 1 to -S 0 and run the test again.\nSave and upload a graph that results from running this test:\n0's plot\n(points: 0)\nTry running this a few times. Likely, you find that this graph does not look quite like a\nGaussian centered at 0, but rather looks more like a \"half-Gaussian\" whose left edge runs up\nagainst 0. Explain why this is happening; what is different about the all-0's case, versus the\nall-1's case? (Hint: think about what the rectification step in the demodulation does.)\n\n(points: 1.5)\nTask 4: Demodulation\nIn this task, we will implement the envelope detector demodulation scheme described above.\nChanges for this section should be made in the template file demodulate.py. To run your\ndemodulator instead of the one we provided, change line 13 in receiver.py to:\nimport demodulate First, define a method avgfilter(samples_in, window). This\nmethod should return a new numpy array x, each of whose elements x[i] is the average of\nall values samples_in[i] to samples_in[i+window-1], inclusive. It makes sense to\nseparate this averaging step (which is a form of \"low pass\" filtering) out of our envelope\ndemodulator, because we will use it in other demodulators as well (in later labs).\nx should have the same length as samples_in. Note that this means that values near the end\nof $x$ will not be the average of window samples (we would run off the end of the list if we\ntried to do that); in this case, x[i] should be the average of all values from $i$ to the end of\nsamples_in.\nTest your avgfilter function on multiple inputs. Here is the output of one test:\n>>> avgfilter([1,1,2,3,5,8,13,21], 3)\narray([ 1.33333333,\n2.\n,\n3.33333333,\n5.33333333,\n8.66666667, 14.\n, 17.\n, 21.\n])\nOnce your avgfilter method is working, move on to implementing the actual demodulator,\nwhich should now be a straightforward application of absolute value and the averaging filter\nyou just wrote.\nOne question remains, though: what window size should we use during demodulation? Try\nplaying around with a few different values, and see how this affects the shape of the\n\"demodulated samples\" plot.\nFor \"low-enough\" carrier frequencies, a good value to use for the window size is the number\nof samples in one-half of the carrier waveform's period. Change your demodulator to use\nthis window size. You can calculate this value easily from the sampling rate and the carrier\nfrequency of the waveform.\nSubmit your code for the demodulator below.\nUpload your demodulate.py:\n(points: 4)\nAs an extra challenge, try to think for yourself why this window size might begin to fail as\nwe move to higher and higher carrier frequencies, and how we might go about improving its\nperformance at these higher frequencies.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "6.02 Introduction to EECS II, Problem Set 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/e9972131f3b60c02c3b5874dda78d1ad_MIT6_02F12_ps5.pdf",
      "content": "Your answers will be graded by actual human beings (at least that's what we believe!), so\ndon't limit your answers to machine-gradable responses. Some of the questions specifically\nask for explanations; regardless, it's always a good idea to provide a short explanation for\nyour answer.\nBefore doing this PSet, please read Chapters 11 and 12 of the readings. Also attempt the\nConvolution and Frequency Response practice problems on this material.\nDue dates are as mentioned above. Checkoff interviews for PS4 and PS5 will happen\ntogether between October 24 and 28, 2012.\nProblem 1. Eye Diagrams\nConsider the following three eye diagrams generated by applying a random sequence of 200\nbits, with 10 samples/bit, to three different causal LTI systems:\nProblem Set 5\n\nEye Diagram A\nEye Diagram B\nEye Diagram C\nThe unit sample responses for each of the three causal LTI systems are given below, in some\norder:\n\nUnit-sample Response 1\nUnit-sample Response 2\nUnit-sample Response 3\nS. For each of the channels 1, 2, and 3, determine the value of the step response at time n\n= 4\n(points: 1)\n\nA. Which unit sample response goes with eye diagram A?\n(points: 1)\nB. Which unit sample response goes with eye diagram B?\n(points: 1)\nC. Which unit-sample response goes with eye diagram C?\n(points: 1)\nProblem 2.\nSuppose the only nonzero values of the input x[n] to a causal LTI system are:\nx[0] = 3\nx[1] = 2\nx[2] = 1\nx[3] = -2\nx[4] = -1\nIf the unit sample response of the system is h[n] = (1/3)n for n ≥ 0 , what is the maximum\nvalue of the output y[n] , and for what value of n does y[n] achieve its maximum? (Hint for\nhow to do this easily: Sketch x[·] and h[·] and think about the \"flip-slide-dot product\"\nimplementation of convolution.)\n(points: 2)\nProblem 3.\nSuppose the unit sample response of a causal LTI channel is given by h[k] = (1/2)k for k ≥\n\n0 . The input to the channel is x[n] , and the output is y[n] .\nA. Find the unit sample response g[·] of a causal LTI filter that the receiver could use,\nsuch that applying the channel output y[n] to the input of this filter results in a filter\noutput z[n] that equals the channel input x[n] .\n(points: 2)\nB. Write down the frequency responses H(Ω) and G(Ω).\n(points: 2)\nProblem 4.\nConsider a causal LTI channel with the unit sample response h[k] = (1/3)k for k ≥ 0.\nAssume the input is x[n] = cos(Ω0 n + π/4) for all n.\nA. Determine the frequency response H(Ω) of the channel.\n(points: 1)\nB. Find an expression for y[n] when Ω0 = 0, and write down the value of y[3].\n(points: 1)\nC. Find an expression for y[n] when Ω0 = π, and write down the value of y[3].\n\n(points: 1)\nD. Find an expression for y[n] when Ω0 = π/2, and write down the value of y[3].\n(points: 2)\nProgramming Tasks\nAs in PSet 4, we will use the Audiocom communication system, which uses your computer's\nspeakers to transmit signals, and your computer's microphone to receive signals. In this lab,\nwe will explore the unit-step and unit-sample response of the \"baseband-equivalent\" channel.\nReminder: The documentation for audiocom is available at http://audiocom602.blogspot.com.\nTask 1: Bypass Channel\nIt will help us debug issues over the audio channel if we implement a bypass channel that\nsimulates an LTI channel with delay, lag, and (additive Gaussian) noise.\nGo to bypass_channel.py, which provides a template class, BypassChannel. Notice that its\n__init__ method takes three arguments. You already dealt with the first argument, noise, in\nPSet 4. Here, your task is, in addition, to use the specified lag and h to simulate their effects.\nWrite xmit_and_recv: given the channel's unit-sample response, self.h, the number of\nsamples of delay between transmitter and receiver, self.lag, and Gaussian noise,\nself.noise, return a numpy array in which each the input numpy array, tx_samples, is\nconvolved with h, followed by the prepending by self.lag samples of value 0 volts, and\nfinally a Gaussian random variable with zero-mean and variance self.noise added. Return\nthe resulting numpy array.\nUpload your bypass_channel.py:\n(points: 2)\nTask 2: Eye diagrams\nIn this task, we will be looking at the effects of noise and channel h on eye diagrams, as well\nas their effect on our ability to successfully decode messages sent across a channel.\nBefore actually making any plots, we will make some changes to make sure we always see an\neye diagram, regardless of whether we successfully decode the message. In receiver.py,\nadd the following lines before the call to plot_hist:\np.figure(1)\n\nplot_eye(demod_samples,spb,'eye diagram')\np.figure(2)\nAlso uncomment lines 80 and 81 in graphs.py.\nAll parts of this problem except 2.5 should be completed using the bypass channel.\nTask 2.1: Noise and Eyes\nWith that out of the way, let's consider the effect of noise on our eye diagrams. Try sending\n500 random bits across your bypass channel with a noise variance of 0.1:\npython sendrecv.py -b -g -n 200 -z 0.1\nNow repeat the experiment with all parameters the same except noise variance; adjust it to be\n0.3, and then 0.5. What effect does the noise variance have on the \"openness\" of the eye\ndiagrams in this experiment? Why?\n(points: 1)\nTask 2.2: Noise and SPB\nYou may have noticed in the previous lab that higher values of samples per bit generally\nallowed us to more accurately decode our received messages. Let's try keeping noise variance\nconstant at 0.3, and adjusting samples per bit (specified by parameter -s). Try with spb\nvalues 16, 64, 256, and 1024. What effect does the number of samples per bit have on the\n\"openness\" of the eye diagrams in this experiment? Why?\n(points: 2)\nTask 2.3: h and Eyes\nIn the previous two questions, we have looked at the effect of noise on our eye diagrams.\nNow we will consider the effect of h, the unit sample response of the channel. In this\nexperiment, we will hold constant both the noise variance (at 0.1) and the samples per bit (at\n64). First, let's try sending 200 random bits with the following channel h: -u \"0.25 0.25\n0.25 0.25\"\nHow long does it take the unit step response of a system with unit sample response\n[0.25,0.25,0.25,0.25] to converge to 1?\n\n(points: 1)\nNow let's repeat the experiment with the following channel USR's (command line arguments\nhave been reproduced to save you some typing):\n-u \"0.5 0.5\"\n-u \"0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\"\n-u \"0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n0.05 0.05 0.05 0.05 0.05 0.05 0.05\"\nWhat effect does the channel's USR have on the \"openness\" of the eye diagrams in this\nexperiment? Why?\n(points: 1)\nTask 2.4: h and SPB\nIn this section, we will hold noise variance and h constant, and change the number of samples\nper bit. Try running with the last USR from the previous step (0.05 0.05...), with samples per\nbit values of 8, 16, 32, 64, and 256.\nWhat effect does the number of samples per bit have on the \"openness\" of the eye diagrams\nin this experiment? Why?\n(points: 2)\nTask 2.5: The Real World\nNow try sending 200 random bits across the actual audio channel. Try this with a number of\ndifferent values of samples per bit.\nWhat effect does the number of samples per bit have on the \"openness\" of the eye diagrams\nin this experiment? Why?\n\n(points: 2)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "6.02 Introduction to EECS II, Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/0b549abc33190c170fabb3709410231e_MIT6_02F12_quiz1.pdf",
      "content": "6.02 Fall 2012, Quiz 2\nPage 1 of 12\nName:\nDEPARTMENT OF EECS\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.02: Digital Communication Systems, Fall 2012\nQuiz I\nOctober 11, 2012\n\"×\" your section\nSection\nTime\nRecitation Instructor\nTA\nD\n10-11\nVictor Zue\nRuben Madrigal\nD\n11-12\nVictor Zue\nCassandra Xia\nD\n12-1\nJacob White\nKyu Seob Kim\nD\n1-2\nYury Polyanskiy\nShao-Lun Huang\nD\n2-3\nYury Polyanskiy\nRui Hu\nD\n3-4\nJacob White\nEduardo Sverdlin Lisker\nPlease read and follow these instructions:\n0. Please write your name in the space above and × your section.\n1. One two-sided \"crib sheet\" and a calculator are allowed. No other aids.\n2. There are ** questions in ** sections, and 12 pages in this quiz booklet.\n3. Your total allotted time is 120 minutes, but we have designed the test to be considerably\nshorter, to allow you time to work carefully and check your answers.\n4. Please write legibly. Explain your answers, especially when we ask you to. If you find a\nquestion ambiguous, write down your assumptions. Show your work for partial credit.\n5. Use the empty sides of this booklet if you need scratch space. If you use the blank sides for\nanswers, make sure to say so!\nPLEASE NOTE: SOME STUDENTS WILL TAKE THE MAKE-UP QUIZ TOMORROW\nAND MONDAY AT 9 AM. PLEASE DON'T DISCUSS THIS QUIZ WITH ANYONE IN\nTHE CLASS, UNLESS YOU'RE SURE THEY HAVE TAKEN IT WITH YOU TODAY.\nDo not write in the boxes below\n1-4 (*/15)\n5-7 (*/18)\n8 (*/20)\n9-15 (*/26)\n16-18 (*/21)\nTotal (*/100)\n\nI\n6.02 Fall 2012, Quiz 2\nPage 2 of 12\nWho Said What?\nAli and Bob are communicating on a two-way channel. At every transmission slot, Ali and Bob each\nindependently sends to the other a randomly chosen symbol from a specified set. Ali transmits one of two\ndistinct symbols, a1 or a2, with respective probabilities α and 1 - α, while Bob transmits a fixed symbol b1\n(i.e., transmits b1 with probability 1).\n1. [2+2=4 points]: What are the respective entropies, HAli and HBob, of Ali's and Bob's transmis\nsions at any slot, in bits? (For HAli, your answer will be an expression in terms of α, while for HBob\nyour answer will be a number.)\nHAli =\nHBob =\nSuppose Cat is listening in on the channel through a flaky switch that, at each transmission slot, connects her\nto Ali's transmission with probability p, and to Bob's transmission otherwise, i.e., with probability (1 - p).\nThe switch has lights to indicate whether Cat is connected to Ali or to Bob. The fact that Cat is listening has\nno effect on Ali's and Bob's transmissions.\n2. [4 points]: With Ali and Bob transmitting as in Problem 1, Cat can announce one of three possible\nmessages: \"Ali transmitted a1\", \"Ali transmitted a2\", or \"Bob transmitted b1\". What is the entropy,\nHCat, of this set of messages?\nHCat =\n\n6.02 Fall 2012, Quiz 2\nPage 3 of 12\n3. [3 points]: Your friend believes that the general expression relating the entropy HCat of Cat's\npossible messages to the entropies HAli and HBob of Ali's and Bob's transmissions is\nHCat = pHAli + (1 - p)HBob ,\nas long as Ali's and Bob's transmissions are independent. It turns out that your friend in not quite\nright. If you've answered Problem 2 correctly, you know that he's missing one or more additional\nterms on the right that depend only on p. Specify what's missing in your friend's expression. Be sure\nto simplify your expression to make clear that it only depends on p.\nMissing term(s) on right side of preceding equation:\n4. [2+2=4 points]: Suppose Cat's switch lights up to tell her she's listening to Ali.\nBefore seeing what symbol Ali sent, how much information, in bits, has Cat obtained by recognizing\nthat the transmission comes from Ali?\nKnowing now that the transmission comes from Ali, how much uncertainty, in bits, does she still have\nabout the actual intercepted symbol?\n\n6.02 Fall 2012, Quiz 2\nPage 4 of 12\nII Shaking The Tree\nIn this problem E denotes a small positive number, 0 < E « 1.\n5. [8 points]: Find a Huffman code and its expected code length L for a source whose symbols A,\nB, C, D have respective probabilities\n3 - E 2 + E 2\n(\n,\n,\n,\n) .\nBe sure to draw the code tree!\nThe binary codewords for A, B, C, D are respectively:\nThe expected code length L =\n\n6.02 Fall 2012, Quiz 2\nPage 5 of 12\n6. [8+1=9 points]: Repeat the preceding Huffman coding problem for the case where the symbol\nprobabilities are respectively\n4 3 + E 2 - E 2\n(\n,\n,\n,\n) .\nBe sure to draw the code tree!\nThe binary codewords for A, B, C, D are now respectively:\nThe expected code length now is L =\nHow much shorter is this expected length than what would have been obtained if the code from the\nprevious problem had been used instead?\n7. [1 points]: What conclusions are suggested (though of course note proved!) by the preceding\ncalculations, regarding the possible sensitivity of the Huffman code tree and of the expected code\nlength to small perturbations in the symbol probabilities?\n\n6.02 Fall 2012, Quiz 2\nPage 6 of 12\nIII I'm a Webster, You're a Webster\nA particular source uses the Lempel-Ziv-Welch algorithm to communicate with a receiver. The message that\nthe source wishes to communicate is made up of just three symbols: a, b, c. These symbols are respectively\nstored in positions 1, 2 and 3 in the dictionary at both the source and the receiver. The subsequent dictionary\nentries, as they are built by the LZW algorithm, are assigned to positions numbered 4, 5, 6, ....\n8. [5+15=20 points]: Suppose the transmitted sequence is\n2, 3, 3, 1, 3, 4, 5, 10, 11, 6, 10, 1\nDecode the sequence, and write down the receiver's entire dictionary at the end of the transmission.\n\n6.02 Fall 2012, Quiz 2\nPage 7 of 12\nIV Secret Letter\nMembers of the Hesperian Order communicate with each other in a binary block code of length 7, with\nthe individual codeword bits sent on consecutive days of the week by messenger, and the color of the\nmessenger's horse -- white or black -- signaling the bit. The Order makes allowance for the fact that on\noccasion one of the messengers in a block (and never more than one) is intercepted and induced to swap a\nwhite horse for black, or vice versa. The Order's \"H-code\" is derived by arranging the codeword bits in the\nform of the letter H:\nx1\nx2\nx3 x7 x4\nx5\nx6\nThe set of codewords comprises exactly those words (x1, ..., x7) whose GF(2) sum in each of the two vertical\ncolumns and in the single horizontal row is 0, i.e.,\nx1 + x3 + x5 = 0 ,\nx2 + x4 + x6 = 0 ,\nx3 + x7 + x4 = 0 .\nTo get full credit, you need to provide careful explanations in addition to correct answers!\n9. [5 points]: A vector c = (x1, ..., x7) is a valid codeword if and only if it satisfies the equation\nHcT = 0 for some matrix H. Write down an appropriate matrix H:\nH=\n10. [2 points]: Is the code linear?\nExplanation:\n\n6.02 Fall 2012, Quiz 2\nPage 8 of 12\n11. [4 points]: What is the rate of the code?\nExplanation:\n12. [2+1+2=5 points]: What is the minimum weight of a nonzero codeword?\nExplanation (include an example of a codeword with this weight):\nWhat is the minimum Hamming distance dmin of the code?\nExplanation:\n\n6.02 Fall 2012, Quiz 2\nPage 9 of 12\n13. [2+2=4 points]: How many bit errors per block can this code be guaranteed to detect?\nExplanation:\nHow many bit errors per block can this code be guaranteed to correct?\nExplanation:\n14.\n[3 points]: Name a specific block code of the same length and rate, but with better error\ncorrecting properties, and specify in what respect the error correcting properties are better.\n15. [2+1=3 points]: The Order consults you about the possibility of adding one more bit, x8, to\ntheir codewords, and one more constraint to the existing three constraints that define their H-code.\nThey wish to thereby improve the error correction properties of their code, even if it means getting a\nsomewhat lower rate. The fact that they have two column parity relations and one row parity relation\nin their existing code reminds you of a code you studied in 6.02. Suggest what additional parity\nrelation they should add, and state what minimum Hamming distance you expect the resulting code\nwill have (no proof needed).\n\n6.02 Fall 2012, Quiz 2\nPage 10 of 12\nV Amazing Ariadne\nFigure 1: Ariadne's sketch of the maze that Theseus escaped from.\nYour archaeologist friend has stumbled on some Cretan figures and inscriptions. She copies the two most\nintriguing figures, translating their inscriptions, and tells you what she thinks they represent. Figure 1 shows\nAriadne's sketch of a 4 × 4 maze that Theseus managed to escape from, going from location A1 to location\nD4 in 8 steps (possibly taking steps back and returning to cells he had previously been to), before exiting\nthe maze. Ariadne seems to have asked Theseus to recall the sequence of directions -- North or East or\nSouth or West -- that he took to make his escape. His response is the following, though perhaps not entirely\nreliable in view of the fact that he was being stalked by the Minotaur:\nN E N W N E E E\n16. [1 points]: Use a dashed line to draw Theseus's stated path on the 4x4 maze above, and verify\nthat his memory cannot be completely correct.\n\n6.02 Fall 2012, Quiz 2\nPage 11 of 12\nIn Figure 2 on the next page you'll see Ariadne's incomplete sketch of the relevant part of a trellis that\ndescribes possible 8-step paths through the maze. (A copy of the maze is overlaid there for your conve\nnience.) Your first task -- described more specifically in the problem below -- is to complete her sketch\nwith whatever states and edges you think are necessary; avoid drawing states and edges you don't need,\nto keep your trellis from looking like a hopeless tangle! You will then run a Viterbi algorithm to search\nthrough all possible paths of 8 steps from A1 to D4, finding the one that matches Theseus's recollected set\nof directions in as many positions as possible.\n17. [8 points]: Mark in the relevant missing states and edges in the trellis in Figure 2 for Theseus's\npossible 2nd, 7th and 8th steps. Also put in any necessary labels. You need NOT mark in those states\nor edges that you are sure will not be traversed.\n18. [10+1+1=12 points]: Run the Viterbi algorithm, filling in the path metric in all the relevant\nstates (i.e., boxes) in the trellis diagram in Figure 2. You need not compute the metric for any boxes\nthat are clearly not going to be needed. Keep track of which edges were used on the optimal path to\neach relevant state. Then mark in the optimum reconstructed path on the trellis AND in the maze.\nAlso summarize your answer by completing the statements below.\nThe sequence of directions on the optimum reconstructed path is as follows:\nThe number of positions in which the directions on the optimum reconstructed path differ from the\ndirections on Theseus's recollected path:\n\n6.02 Fall 2012, Quiz 2\nPage 12 of 12\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "6.02 Introduction to EECS II, Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/5e20fd86bf643a4ff77377cbebac7238_MIT6_02F12_quiz2.pdf",
      "content": "6.02 Fall 2012, Quiz 2\nPage 1 of 17\nName:\nDEPARTMENT OF EECS\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.02: Digital Communication Systems, Fall 2012\nQuiz II\nNovember 13, 2012\n\"×\" your section\nSection\nTime\nRecitation Instructor\nTA\nD\n10-11\nVictor Zue\nRuben Madrigal\nD\n11-12\nVictor Zue\nCassandra Xia\nD\n12-1\nJacob White\nKyu Seob Kim\nD\n1-2\nYury Polyanskiy\nShao-Lun Huang\nD\n2-3\nYury Polyanskiy\nRui Hu\nD\n3-4\nJacob White\nEduardo Sverdlin Lisker\nPlease read and follow these instructions:\n0. Please write your name in the space above and × your section.\n1. Two two-sided \"crib sheets\" and a calculator are allowed. No other aids.\n2. There are 27 questions (mostly short!) in VI sections, and 17 pages in this quiz booklet.\n3. Your total allotted time is 120 minutes.\n4. Please write legibly. Explain your answers, not just when we explicitly ask you to! If\nyou find a question ambiguous, write down your assumptions. Show your work for partial\ncredit.\n5. Use the empty sides of this booklet if you need scratch space. If you use the blank sides for\nanswers, make sure to say so!\nPLEASE NOTE: SOME STUDENTS WILL TAKE A MAKE-UP QUIZ LATER THAN\nYOU. PLEASE DON'T DISCUSS THIS QUIZ WITH ANYONE IN THE CLASS,\nUNLESS YOU'RE SURE THEY HAVE TAKEN IT WITH YOU TODAY.\nDo not write in the boxes below\n1-6(*/19)\n7-10 (*/19)\n11-17 (*/25)\n18-21 (*/13)\n22-24 (*/12)\n25-27 (*/12)\nTotal (*/100)\n\nI\n6.02 Fall 2012, Quiz 2\nPage 2 of 17\nBring in The Noise\nA particular digital communication scheme sends signals over a channel that behaves essentially as a linear,\ntime-invariant (LTI) system at baseband. The binary source at the transmitter generates the symbols \"0\" and\n\"1\" with equal probability. The characteristics of the channel and the timing at the receiver are such that the\nreceiver is able to obtain M ≥ 1 good samples in each bit slot. The good samples in a bit slot corresponding\nto a \"0\" are all of the form\ny[n] = V0 + w[n]\nwhile in a bit slot corresponding to a \"1\" they are all of the form\ny[n] = V1 + w[n] .\nHere w[n] denotes a noise term that is a Gaussian random variable of mean 0 and variance σ2, and is\nindependent across samples, i.e., the samples at the receiver are perturbed by additive white Gaussian noise.\nFor on-off signaling, V0 = 0 and V1 = V . The linearity of the channel then ensures that for bipolar signaling\nV0 = -V and V1 = V .\nThe receiver decides on whether a \"0\" or \"1\" was sent by comparing the average value of the M samples in\nany bit slot with a threshold voltage Vth. If the average is below Vth, the receiver decides a \"0\" was sent; if\nthe average is above Vth, the receiver decides a \"1\" was sent.\nSuppose we are using bipolar signaling and M = 1, i.e., only a single sample is taken in each bit slot.\nThen you've seen that the probability of the receiver making an error in deciding whether a \"0\" or \"1\" was\nsent in any particular bit slot, i.e., the bit error rate (BER), is minimized by choosing Vth = 0, with the\ncorresponding BER being given by\n\nV\nBERbipol = 0.5 erfc\n√\n.\nσ 2\n1.\n[2 points]: If V is doubled from its original value, by what maximum factor can the noise\nvariance σ2 be increased without exceeding the original BER?\nNoise variance can be increased by a factor of :\n2. [4 points]: If V is kept the same as originally, but M is increased to 4, by what maximum factor\ncan the noise variance σ2 be increased without increasing the original BER? Explain your answer!\nNoise variance can be increased by a factor of :\n\n6.02 Fall 2012, Quiz 2\nPage 3 of 17\nSuppose the transmitter now converts to on-off signaling, perhaps to save on the average power transmitted,\nand thereby extend its battery life. Some indication of the average power at the transmitter can be obtained\nby computing the average value that (y[n])2 would have across all the received good samples in the absence\nof noise. By this measure it follows that, for a given V , on-off signaling uses half the average power at the\ntransmitter as bipolar signaling, when \"0\" and \"1\" are equally likely.\nYou can assume the receiver is properly converted for optimal (i.e, minimum BER) detection of on-off\nsignaling by now using a threshold Vth = V/2.\n3. [4 points]: For M = 1, what is the corresponding BER now? (You need not derive your answer\nin detail; it will suffice to explain what modifications you need to make to the earlier BERbipol\nexpression, and why.)\nBERonoff =\n4. [4 points]: For a given M and noise variance, by what factor does V in on-off signaling have to\nbe increased in order to get a BER that matches the case of bipolar signaling under the original V ?\nV must be increased by a factor of :\n5. [2 points]: If V for on-off signaling is increased by the factor you determined in the preceding\nproblem, by what factor does the average power at the transmitter increase?\nAverage power at the transmitter increases by :\n\n6.02 Fall 2012, Quiz 2\nPage 4 of 17\n6. [3 points]: Suppose (for either bipolar or on-off signaling) the probability of a \"1\" being sent\nbecomes greater than the probability of a \"0\" being sent. Will Vth have to be increased, decreased,\nor left unchanged from its previous value for optimum performance, i.e., minimum BER? Circle the\ncorrect answer, and explain!\nVth will have to be Increased / Decreased / Unchanged\n\n6.02 Fall 2012, Quiz 2\nPage 5 of 17\nII Signals and Systems\nConsider the signal x[n] given by\nx[n] = cos(2π n\n3 )\nfor all n. A segment of the signal, for n in the interval [-3, 3], is shown below. (Note that cos( 2π ) = -0.5.)\nFigure 1: Input signal x[n].\n7. [3 points]: What is the angular frequency Ω0, in radians/sample, of this sinusoidal signal x[n]?\nAnd what is its period?\nΩ0 =\nPeriod =\nSuppose the above sinusoidal signal x[n] is used as the input to an LTI system whose unit sample response\nis\nh[n] = δ[n] - δ[n - 1] + δ[n - 2] ,\nwhere δ[n] as usual denotes the unit sample function. A plot of h[n] for n in [-3, 3] is shown on the next\npage.\n\n6.02 Fall 2012, Quiz 2\nPage 6 of 17\nFigure 2: Unit sample response h[n] of LTI system.\n8. [6 points]: Determine the output y[n] of the system for n = 0, 1, 2, showing your computations\nexplicitly.\ny[0] =\n,\ny[1] =\n,\ny[2] =\nNow suppose instead that we use a sinusoidal input that has half the frequency of the preceding case. This\ninput, which we'll denote by x/[n], is given for all n by the expression\nn\nx/[n] = cos(π )\nand is plotted on the next page for n in [-3, 3]. (Note that cos( π ) = 0.5.)\n\n6.02 Fall 2012, Quiz 2\nPage 7 of 17\nFigure 3: New input signal x/[n].\nThis input is applied to the same system as before, namely the one with unit sample response\nh[n] = δ[n] - δ[n - 1] + δ[n - 2] .\n9. [6 points]: Determine the output y/[n] of the system for this new input, for n = 0, 1, 2, 3, 4, 5.\nYou only need show your computations for the first three of these values.\ny/[0] =\n,\ny/[1] =\n,\ny/[2] =\ny/[3] =\n,\ny/[4] =\n,\ny/[5] =\n\n6.02 Fall 2012, Quiz 2\nPage 8 of 17\nStaying with the LTI system we have been considering, whose unit sample response is\nh[·] = δ[n] - δ[n - 1] + δ[n - 2] ,\nsuppose some input signal xc[n] to this system produces an output of 0 for all time. In other words,\n(h ∗ xc)[n] = 0\nfor all n. Consider now the composite system shown below, where the first subsystem is a stable LTI\nsystem with some unknown unit sample response h1[·], and the second subsystem is the one we have been\nconsidering.\nh1[.]\nh[.]\nxc[n]\nyc[n]\nFigure 4: Composite system.\n10. [4 points]: What is the output yc[·] of the composite system? It is essential that you explain\nyour reasoning here.\n\n6.02 Fall 2012, Quiz 2\nPage 9 of 17\nIII Frequently\nWe stick with the LTI system introduced in the previous section, namely the system with unit sample re\nsponse h[n] = δ[n] - δ[n - 1] + δ[n - 2] .\n11. [3 points]: Write down an explicit expression for the frequency response\ninf\n-jΩm\nH(Ω) =\nh[m]e\nm=-inf\nof the particular LTI system specified above.\nH(Ω) =\n12. [6 points]: Rewrite your answer from the previous part in the form\njα(Ω)\nH(Ω) = A(Ω)e\n,\nwhere α(Ω) and A(Ω) are real functions of Ω. [We don't insist that A(Ω) be nonnegative, so A(Ω)\nneed not be the magnitude of H(Ω).]\nA(Ω) =\n,\nα(Ω) =\n\n6.02 Fall 2012, Quiz 2\nPage 10 of 17\n13. [4 points]: Evaluate A(Ω) and α(Ω) explicitly for Ω = π and for Ω = 2π\n.\n2π\nα( 3\nA(π\nα(π\n2π\nA(\n14. [4 points]: Provide a careful and fully labeled sketch of A(Ω) below, for Ω in the interval\n[-π, π].\n3 ) =\n) =\n,\n) =\n) =\n,\n\n6.02 Fall 2012, Quiz 2\nPage 11 of 17\n15. [3 points]: Suppose the input to the system is x//[n] = cos(2π n + θ0) . Write down an explicit\n//[n].\nexpression for the corresponding output y\ny//[n] =\n///[n]\n16. [3 points]: Suppose the input to the system is now x\n= cos(π n + θ0) . Write down an\n///[n].\nexplicit expression for the corresponding output y\ny///[n] =\n17. [2 points]: For the special case of θ0 = 0 in the preceding two problems, show that you recover\nthe results you obtained in the last section (Problems 8 and 9). If you don't, then you have some\nchecking (or explaining!) to do.\n\n6.02 Fall 2012, Quiz 2\nPage 12 of 17\nIV Spectre in the Mirror\nGiven a real signal x[·] with DTFT\ninf\nX(Ω) =\nx[m]e\nm=-inf\n-jΩm\nj∠X(Ω)\n= |X(Ω)|e\n,\nlet v[·] denote the signal obtained by time-reversing x[·], so v[n] = x[-n].\n18. [4 points]: Let V (Ω) denote the DTFT of v[·]. Which of the following equations correctly shows\nhow to obtain V (Ω) from X(Ω)? Note, incidentally, that X(-Ω) = X∗(Ω), where the latter quantity\nis the complex conjugate of X(Ω). (Circle the correct answer, and explain your reasoning.)\n- V (Ω) = X(Ω)\n- V (Ω) = 1/X(Ω)\n- V (Ω) = 1/X(-Ω)\n- V (Ω) = X(-Ω)\n- V (Ω) = -X(Ω)\nDefine the signal r[·] as the convolution of x[·] and v[·], so r[n] = (x ∗ v)[n]. More explicitly,\ninf\ninf\nr[n] =\nx[m]v[n - m] =\nx[m]x[m - n] .\n(1)\nm=-inf\nm=-inf\nThe signal r[·] is called the autocorrelation function of x[·], and r[n] is called the autocorrelation at lag n.\n19. [4 points]: Denote the DTFT of r[·] by R(Ω). Write down an expression that shows how to\nobtain R(Ω) from X(Ω); the result of the previous problem is likely to be helpful. If you've done\nthings correctly, you should find that R(Ω) is entirely determined by the magnitude of X(Ω). Please\nwrite your answer in terms of |X(Ω)|.\nIn terms of |X(Ω)| we can write R(Ω) =\n\n6.02 Fall 2012, Quiz 2\nPage 13 of 17\n20. [3 points]: State whether each of the following statements is True or False, with a very short\nexplanation in each case.\n- R(Ω) is real. True / False\n- R(Ω) is an even function of Ω. True / False\n- R(Ω) is nonnegative at all Ω. True / False\nWe know from the inverse DTFT that\nr[n] = 1 π\nR(Ω)ejΩndΩ ,\n2π\n-π\nfrom which\nπ\nr[0] =\nR(Ω) dΩ .\n2π\n-π\n21. [2 points]: In the latter equation, express r[0] in terms of x[·] using Eq. (1), and express R(Ω)\nin terms of |X(Ω)| using the result of Problem 19. Write down the resulting equation; this equality is\nknown as Parseval's theorem for a discrete-time signal.\nParseval's theorem:\n(From what you've proved above, the following chain of reasoning is justified:\n\nπ\n\n|r[n]| =\n\nR(Ω)ejΩndΩ\n2π\n-π\nπ\n≤\n|R(Ω)ejΩn|dΩ\n2π\n-π\nπ\n=\n|R(Ω)| dΩ\n2π\n-π\nπ\n=\nR(Ω) dΩ\n2π\n-π\n= r[0] .\nThis establishes that the autocorrelation function has its maximum magnitude when the lag is 0. You've\nused similar ideas for preamble detection.)\n\n6.02 Fall 2012, Quiz 2\nPage 14 of 17\nV Q to the Rescue\nJames Bond at the casino in Macau wants urgently to send to MI6 in London a signal x[n] that is 0 for\nn < 0 and whose values for n ≥ 0 contain vital information. To prevent the casino from snooping, Mr.\nBond decides to instead transmit the DTFT X(Ω) of this signal (actually samples of the DTFT at points in\nthe interval [-π, π], computed using his Furiously Fast Transformer, but we shall assume these samples are\nso close together that he's effectively transmitting the DTFT itself). Unfortunately, what gets transmitted\nis just the real part of X(Ω). We'll lead you through how the Q Branch wizards at MI6 (some of whom\nare reported to have studied at MIT on the Cambridge-MIT Exchange) reconstructed x[n] from just this\ninformation, and using the knowledge that x[n] = 0 for n < 0.\nAny signal x[n] can be written as the sum of two terms:\nx[n] + x[-n]\nx[n] - x[-n]\nx[n] =\n,\nJ\n+\n,\nJ\n.\nx1[n]\nx2[n]\nAs indicated above, we shall call the first term x1[n] and the second term x2[n].\n22. [4 points]: Either x1[n] or x2[n] is an even function of time, and the other is an odd function of\ntime. Which is which? Circle the correct answer below and show your reasoning. (Recall that f[n] is\ncalled an even function if f[-n] = f[n]; it is called an odd function if f[-n] = -f[n].)\nx1[n] is Odd / Even\nx2[n] is Odd / Even\nThe above decomposition is unique (we don't ask you to show that, though it's not hard) -- i.e., there is no\nother decomposition of an arbitrary signal into the sum of an even part and an odd part.\n23. [4 points]: Denote the DTFTs of the above signals x1[n] and x2[n] by X1(Ω) and X2(Ω)\nrespectively. One of these DTFTs is purely real and the other one is purely imaginary. Which is\nwhich? Circle the correct answer and show your reasoning.\nX1(Ω) is Real / Imaginary\nX2(Ω) is Real / Imaginary\n\n6.02 Fall 2012, Quiz 2\nPage 15 of 17\nIt follows from the answers to the previous two problems that the real part of X(Ω) equals either X1(Ω) or\nX2(Ω) -- you will know which from your answers above. Thus the Q Branch wizards can inverse transform\nwhat the casino sent them, to obtain either x1[n] or x2[n] -- you will know which from your earlier answers.\n24. [4 points]: How can you recover x[n] from either x1[n] alone or x2[n] alone, using the fact that\nx[n] is 0 for n < 0?\nThe general lesson here is that a one-sided signal (i.e., a signal that is identically zero for all n less than\nsome n0, or for all n greater than some n0) is uniquely determined by just the real part (or equivalently, by\njust the imaginary part) of its DTFT.\n\n6.02 Fall 2012, Quiz 2\nPage 16 of 17\nVI Bring Me the Messenger\n[Although this problem is phrased in continuous-time, it should be quite straightforward for you to do on\nthe basis of the fundamentals you've learned in the setting of DT modulation/demodulation. If it helps you,\nyou could think instead in terms of DT samples of the signals here, taken at some sampling rate fs kHz,\nand then map each frequency f given here in kHz to a frequency of Ω = 2πf/fs radians/sample. However,\nthat's much more complication than is needed for this fairly simple and straightforward problem!]\nRF filter and\namplifier\nIF band-‐pass filter\nand amplifier for\nͶͷͷ േͷ kHz\nAudio detector and\namplifier\nʹߨݐ\n(adjustable)\nAntenna\nSpeaker\nFigure 5: Structure of a typical radio receiver.\nPractical radio receivers typically use several stages in their demodulation process, in order to manage\nvarious design tradeoffs. The baseband signal in AM is allowed to carry frequencies in the range of -5\nkHz to 5 kHz, which is adequate for music and speech. To tune in an AM broadcast station operating at\na carrier frequency of fc kHz, we would like to select from the \"radio frequency\" (RF) signal picked up\nby the antenna only those frequencies in the ranges fc ± 5 kHz and -fc ± 5 kHz, as no other parts of the\nreceived signal are relevant. This would require building a very good bandpass filter for the antenna signal\nto go through, but the pass band of the filter would have to be shifted around in frequency as we tuned from\none station to another -- and this turns out to be tricky to do in hardware for a good bandpass filter.\nInstead, what's done is to build an excellent bandpass filter whose passband is centered on a fixed pair of\nfrequencies ±fI , referred to as the intermediate frequency or IF. For AM radio, the standard IF frequency\nis fI = 455 kHz. This filter passes frequencies in the range fI ± 5 kHz and -fI ± 5 kHz, and effectively\nblock out other frequencies. It can also include the amplification necessary to raise the signal level from the\nmicrovolts at the antenna to the higher levels needed to drive downstream electronics.\nTo move the desired radio station's signal into the passband of the IF filter/amplifier, we use the hetero\ndyning idea that we've seen several times, i.e., multiply the signal at the antenna by a sinusoidal signal of\nappropriate frequency, fo kHz. (In AM radio jargon, this multiplication is called mixing.) To tune in differ\nent stations, we just change the frequency of this sinusoidal signal, which is easy to do using a controllable\noscillator. The common practice is to pick fo > fc; this is referred to as super-heterodyning.\n25. [4 points]: Multiplying a carrier signal at fc kHz with a receiver oscillator signal at fo kHz\n\n6.02 Fall 2012, Quiz 2\nPage 17 of 17\nproduces a signal that is a sum of sinusoids. What are the frequencies of these sinusoids?\nThe sinusoids are at the following frequencies (in kHz):\n26. [4 points]: What frequency fo (in kHz, and with fo > fc) should the receiver's oscillator be set\nat in order to translate the signal received from an AM station operating at a carrier frequency of 580\nkHz (which happens to be WTAG Worcester, in this area) into the passband of the IF filter/amplifier\nat 455 kHz? Explain!\n27.\n[4 points]: With fo set as in the preceding problem, there is actually another AM station,\nat a carrier frequency f/ > fo, that will also find its signal translated into the passband of the IF\nc\nfilter/amplifier. What is f/?\nc\nf/ =\nc\n(This f/ is referred to as the image frequency of fc, and the way to prevent it interfering with the desired\nc\nsignal centered on fc is to do some filtering at the antenna itself, in the RF stage. Also, the FCC is careful\nnot to assign the carrier frequencies fc and f/ to nearby stations.)\nc\nThe output from the IF filter/amplifier can now be demodulated in the standard ways, for instance using an\naveraging filter if the modulating signal is always nonnegative, otherwise using a further stage of mixing in\nwhich the signal is multiplied by a sinusoid at the IF frequency. But that's a story for another day.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "6.02 Introduction to EECS II, Quiz 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/d1e0576e4ae7c48e3e1042ef49172658_MIT6_02F12_quiz3.pdf",
      "content": "6.02 Fall 2012, Quiz 3\nPage 1 of 13\nName:\nDEPARTMENT OF EECS\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\n6.02 Fall 2012\nQuiz III\nDecember 18, 2012\n\"×\" your section\nSection\nTime\nRecitation Instructor\nTA\nD\n10-11\nVictor Zue\nRuben Madrigal\nD\n11-12\nVictor Zue\nCassandra Xia\nD\n12-1\nJacob White\nKyu Seob Kim\nD\n1-2\nYury Polyanskiy\nShao-Lun Huang\nD\n2-3\nYury Polyanskiy\nRui Hu\nD\n3-4\nJacob White\nEduardo Sverdlin-Lisker\nPlease read and follow these instructions:\n0. Please write your name in the space above and × your section.\n1. There are a lucky 13 questions (some with multiple parts) and a lucky 13 pages in this quiz\nbooklet.\n2. Answer each question according to the instructions given, within 120 minutes.\n3. Please answer legibly. Explain your answers. If you find a question ambiguous, write\ndown your assumptions. Show your work for partial credit.\n4. Use the empty sides of this booklet if you need scratch space. If you use the blank sides for\nanswers, make sure to say so!\nTwo two-sided \"crib sheets\" and a calculator allowed. No other aids.\nDo not write in the boxes below\n1-2 [15]\n3-4 [20]\n5-6 [13]\n7 [8]\n8-9 [14]\n10 [14]\n11-12 [16]\nTotal [100]\n\nI\n6.02 Fall 2012, Quiz 3\nPage 2 of 13\nPacket-Switching Basics\n1. [5 points]: Which of the following actions does a switch S in a best-effort packet-switched\nnetwork (as studied in 6.02) perform when it receives a packet? Circle True or False for each\nchoice.\nA. True / False S sends an ACK to the sender of the packet.\nB. True / False S looks up the destination address of the packet in its routing table.\nC. True / False S sends a routing advertisement to its neighbors.\nD. True / False S decrements the hop-limit field in the packet header.\nE. True / False S guarantees that packets are forwarded toward the destination in the order in\nwhich they arrive.\n2. [10 points]: A sender is connected to a switch using a link whose transmission rate is 2 Gigabits/second.\nThe switch is connected to the receiver using a link whose transmission rate is 1 Gigabit/second. The\npacket size is 10 kbits. Each link is 100 meters long, and the speed of signal propagation over each\nlink is 2 × 108 meters/second. The switch receives all the bits of a packet from the first link, and then\ntakes 10 microseconds to process the packet, after which it enqueues the packet for transmission on\nthe 1 Gigabit/s link. On average, there are 4 packets in the queue at the switch. There are no queues\nanywhere else, nor any other delays.\nCalculate the average delay, in microseconds (accurate to one microsecond), from the initiation of a\npacket transmission at the sender to the completion of its reception at the receiver. Show your work.\n\n6.02 Fall 2012, Quiz 3\nPage 3 of 13\nII MAC\n3. [4+6=10 points]: Answer the following questions about MAC protocols as studied in 6.02.\nA. In a certain shared medium network with N total nodes, we know that at any time, only k < N\nof the nodes are actually backlogged. Each packet is 1 time slot in length. Express the condition\nunder which the maximum possible utilization of slotted Aloha with a fixed probability of\ntransmission exceeds that of time-division multiple access (TDMA) in this network.\nB. Circle True or False for each statement below.\n(a) True / False Using contention windows in stabilized slotted Aloha guarantees that a back\nlogged node will attempt to transmit a packet within a finite number of time slots.\n(b) True / False Using contention windows in stabilized slotted Aloha guarantees that a back\nlogged node's packet transmission will successfully be received within a finite number of\ntime slots.\n(c) True / False The Carrier Sense Multiple Access (CSMA) protocol with stabilization,\nwhere the packet size is larger than the length of one time slot, never suffers from collisions.\n\n6.02 Fall 2012, Quiz 3\nPage 4 of 13\n4. [4+6=10 points]: Annette Werker has set up a wireless network with an access point (AP) and\nN clients. Each packet is sent either from the AP to a client, or from a client to the AP. The AP\nand clients use slotted Aloha to share the wireless channel. Annette configures the AP to transmit a\npacket with probability p when it is backlogged, and configures each client to transmit a packet with\nprobability q when it is backlogged. Assume that all clients and the AP are always backlogged and\nthat each packet is one time slot long.\nA. Derive an expression for the utilization of the network. Show your work.\nB. If Annette's goal is for the throughput of the AP's transmissions to be equal to the aggregate\nthroughput summed over all the client transmissions, derive an expression for p in terms of the\nother specified parameters. Show your work.\n\n6.02 Fall 2012, Quiz 3\nPage 5 of 13\nIII Routing\n5. [2+3+3=8 points]: The 6.02 link-state protocol runs in the network below (the numbers are link\ncosts).\n\nA. Node E receives HELLO protocol messages from nodes\n(list all that apply).\nB. Suppose node E has a bug in its implementation, which prevents it from correctly receiving any\nLSAs originating from nodes A and C. Circle True or False for each choice below.\n(a) True / False E will compute a route to every node in the network.\n(b) True / False E will correctly compute a minimum-cost route to every node in the network.\nC. Suppose node E has a bug in its implementation, which prevents it from correctly receiving any\nLSAs originating from nodes B and C. Circle True or False for each choice below.\n(a) True / False E will compute a route to every node in the network.\n(b) True / False E will correctly compute a minimum-cost route to every node in the network.\n\n6.02 Fall 2012, Quiz 3\nPage 6 of 13\n6. [5 points]: In a network running a link-state routing protocol, exactly one node is buggy. Instead\nof Dijkstra's algorithm to minimize path costs, the buggy node runs a different algorithm, as follows:\n1. Sort the links in the network in non-decreasing order of their link costs.\n2. Create a tree rooted at the switch by adding links to the tree from the sorted list, proceeding\nin link-cost order. Add a link only if it does not create a cycle with the links already added,\ncontinuing until there is a path in the tree from the node to every other node in the network.\n3. For each destination, select as the route the link in the computed tree that connects the node to\nthe destination.\nThe other nodes correctly implement Dijkstra's algorithm.\nGive an example of a four-node network topology, with link costs and an example path, in which this\nmethod produces a routing loop. Clearly indicate the buggy node and specify the routing loop.\n(Aside: The algorithm at the buggy node computes a \"minimum spanning tree\", and is called Kruskal's\nalgorithm.)\n\n6.02 Fall 2012, Quiz 3\nPage 7 of 13\n7. [8 points]: Alice has an unreliable network in which she uses a distance-vector approach to\ncompute most reliable paths between nodes. The most reliable path from node S to a destination D\nis a path from S to D along which packets are least likely to be lost.\nThe link metric in this network is the packet loss probability for each link: each packet sent on link i\nis lost independently with probability £i, where 0 ≤ £i < 1. Each node is able to compute this value\nfor each of its links, and you may assume that this probability is the same in each direction of a link.\nIf the routing protocol were a distance-vector protocol whose link metrics are these values, write the\nintegrate step in Python.\ndef integrate(self, link, adv):\n# Your code here: explain what any new variable you introduce does.\n# adv is a vector of (dest,lossprob) tuples coming from the\n# neighbor at the other end of the link. The lossprob in the\n# advertisement is the cumulative loss probability from the node\n# making the advertisement for destination \"dest\".\n# Assume that the variable link.loss stores the loss rate of \"link\".\n# Your code should correctly compute self.routes[dest] for each dest\n# as well as self.lossprob[dest], the path metric to the destination,\n# which should be equal to the loss probability of the path to dest.\n\n6.02 Fall 2012, Quiz 3\nPage 8 of 13\n8.\n[10 points]: Ben Bitdiddle operates a network whose routing protocol computes minimum-\ncost paths between nodes. All link costs are positive integers. Initially the network has five nodes,\nS, T, U, V, and W , with link costs as shown in the picture below. After the routing protocol has\nconverged to minimum-cost routes at each node, Ben adds a new node, R, to the network, with the\nlinks and link costs as shown. When the routing protocol converges, he finds that:\n1. S's route to destination V has changed,\n2. T 's route to destination U has changed, and\n3. U's route to destination V has not changed.\nT\n\nIn the routing protocol, an existing route changes only if the new route has a lower cost; if there is a\ntie, the old route persists.\nTo satisfy all of the three observations above, some constraints on the positive-integer link costs\n(w1, w2, and w3) must necessarily hold. Specify these constraints, with complete explanations.\n\n6.02 Fall 2012, Quiz 3\nPage 9 of 13\n9. [4 points]: The picture below shows the ARPANET from September 1971. At that time, the\nnetwork ran a distance-vector protocol. Assume that each link has a cost of 1.\nSuppose CARNEGIE believes it is MIT, sending out erroneous distance-vector advertisements that\nmake it look like CARNEGIE is actually MIT: these route advertisements are of the form [(MIT 0),\n(CASE 1), (MITRE 1), ...]. The rest of the protocol runs correctly at all the nodes. There are no\npacket losses or other failures. Note that the nodes BBN and BBN-T (the \"T\" marked with a circle)\nare distinct nodes.\nImage in the public domain, from the ARPANET Completion Report, January 1978.\nPackets sent from which nodes to MIT will definitely not reach the true MIT now? Explain.\n\n6.02 Fall 2012, Quiz 3\nPage 10 of 13\nIV C-DNS: Concurrent Domain Name System Queries\nThe Domain Name System (DNS) is a critical component of the Internet infrastructure. It provides a way\nto convert from human-readable hostnames to IP addresses. It achieves this task using a simple request-\nresponse protocol similar to the 6.02 stop-and-wait protocol:\n1. The client seeking to resolve a hostname sends a request to a DNS server.\n2. The server responds with the answer.\n3. If the client does not get a response within a timeout period, it retransmits the request to the server.\nAlyssa P. Hacker is interested in improving the response time for DNS requests. She modifies the system to\nset up two independent DNS servers, 1 and 2. In the modified system, the client sends each request to the\ntwo servers concurrently. If it does not get a response from either of the two servers within τ seconds of the\ntransmission of the original request, the client retransmits the request to both servers. The client continues\nto do that until it obtains a response.\nThe RTT between the client and server i is Ri (each Ri is a fixed value). The probability that any given\nrequest sent to server i will get a response is pi > 0. The paths between each of the two servers and the\nclient are independent of each other. Assume that R1 ≤ R2 and that the timeout value τ > R2 is a constant.\n10. [4 points]: Derive an expression for Ti, the expected amount of time for the client to receive a\nresponse from server i. Explain your answer.\n\n6.02 Fall 2012, Quiz 3\nPage 11 of 13\n11. [10 points]: Derive an expression for the expected amount of time before the client obtains the\nfirst response from one of the servers. (Remember that R1 ≤ R2.) Explain your answer.\n\n6.02 Fall 2012, Quiz 3\nPage 12 of 13\nV Sliding Windows\n12. [10 points]: Running the 6.02 sliding window protocol on a network path, Eager B. Eaver\nobserves the following experimental results during the steady state of the connection. There is no\nother traffic along the network path, and there are no data packet losses or ACK packet losses.\nFill in the blanks in the table below. Show your calculations in the space below the table.\nWindow size (packets)\nRTT (ms)\nThroughput (packets/s)\nAverage queue length (packets)\n\n6.02 Fall 2012, Quiz 3\nPage 13 of 13\n13. [6 points]: Louis Reasoner decides to optimize the 6.02 sliding window protocol for a best-effort\npacket network. When the sender gets an acknowledgment (ACK) for packet with sequence number\ni, if it has not already received an ACK for any packet with sequence number j < i, it immediately\nretransmits each such packet with sequence number j. The rest of the protocol is the same as we\nstudied in 6.02.\nIn Louis's network, ACK packets are never lost, and timeouts are large enough to not cause spurious\nretransmissions. Recall that a spurious retransmission is a situation when the sender retransmits a\npacket that has not actually been lost.\nCircle True or False for each choice below, with an explanation for your answer in each case.\nA. True / False Suppose the network reorders the packets sent between the sender and receiver, but\nnot between the receiver and sender. Then, Louis's protocol may cause spurious retransmissions.\nB. True / False Suppose the network never reorders packets on any path. Then, Louis's protocol\nwill never cause spurious retransmissions.\nFIN\nHave a great winter!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 1: Overview: information and entropy",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/21637440fda8bd0e28d06a9eac518b03_MIT6_02F12_lec01.pdf",
      "content": "6.02 Fall 2012\nLecture 1, Slide #1\n6.02 Fall 2012\nLecture #1\n- Digital vs. analog communication\n- The birth of modern digital communication\n- Information and entropy\n- Codes, Huffman coding\n\n6.02 Fall 2012\nLecture 1, Slide #3\n\n-\n\n! \"\"\n#!$%\"&#!\n-\n\n- &\"%\n- ''(\n\n\"\n\n$\n%)\n\n)\n- *\"\"\n\n$\n%)\n+\n\"\"\n)%\"\n,\"-\n- *\n\n%\n\n- &\"%\n\n- .\n\"\"\n\n)\"/\n\n6.02 Fall 2012\nLecture 1, Slide #4\n6.02 Syllabus\nPoint-to-point communication channels (transmitterreceiver):\n-\nEncoding information BITS\n-\nModels of communication channels SIGNALS\n-\nNoise, bit errors, error correction\n-\nSharing a channel\nMulti-hop networks:\n-\nPacket switching, efficient routing PACKETS\n-\nReliable delivery on top of a best-efforts network\n\n6.02 Fall 2012\nLecture 1, Slide #7\nSamuel&0#\n\n-\n' \"-123\"\n4-567-16,!\n\n%\"8%\n\n8\"\n%\n/\"#\n\"9\n-\n.8\"\n\n%\n)\n%\n-\n*\n\n%\n$8%\"\"\"\"%\n\"%\"\n- (\n):\n-1;7-5\n\n\"<1\"\n=\n>?\n\"0 9!-1;1-15;@%\n\n-1551\"\nA\n!\n- (\nBC-15-D %\"\" ?%EF\n\n!\n- (\n?@-<,3\n-\n( %\n\"\n-,\"%\n\n)%\n\n)% !\"\n\n\"G\n\"\nEE %HI%J I\n\"?/!\n\n6.02 Fall 2012\nLecture 1, Slide #13\n&\n\"-,,%\n\n- >\n- ( K' ( %L4\n-766;50-175!\n- .\n\n%#-<,-!\n- #\"&\n\n\"-<,5!\n- &#\"\n-<22!\n- (\n)\"\n)% 00-<25!\n- /\n- 0)\nF%\n\n- %$\n0\"I%/\n\n6.02 Fall 2012\nLecture 1, Slide #14\nClaude E. Shannon, 1916-2001\n-<27#\n\nEE#'(\n\n'\"\"0\n)\n\"\n\n>%M\"\"\n\nK#\n#\n\n%L\n\n-<6,? # #'(\n\n(%N \"%\n#\"\n\nPhotograph (c) source unknown. All rights reserved.This\ncontent is excluded from our Creative Commons license.\nFor more information, see http://ocw.mit.edu/fairuse.\n\nMIT faculty\nO\"0)\n-<6,\n1956-1978\n\nK %% %L-<6;A-<6<\n\n6.02 Fall 2012\nLecture 1, Slide #15\nA probabilistic theory requires at least a one-slide\nchecklist on Probabilistic Models!\n-\nB\n%\n\n-\n3/\n/\"%\nF \"\n-\nE\n0/\n)\n\n.\n%\n\"\nF\n\n-\nE\nK)L\n\n:P0!\n\n:0!\n\n:!\nC\"\n\n-\n?))\n\"@\"\n\n,Q?!Q-?!R-\"\n?P0!R?!P?0!\"0%F\n0R#\n%?P0!R?!P?0!?0!\n-\nE\n0E\n\")%!\"\"G\n))% %)\n\n\"\n\" \"))\n\n?0E!R?!?0!?!?!?E!?0!R?!?\n0!?!?!?E!R?!?!?E!\n-\n\"))%? 0\n\"!R?S0!R?0!A?0!\n-\nEF\" \" )\n))% \"\n\n6.02 Fall 2012\nLecture 1, Slide #16\nMeasuring Information\nShannon's (and Hartley's) definition of the information obtained on\nsi\nbeing told the outcome of a pr\nS\nobabilistic experiment :\n⎛\n⎞\nI(S = si) = log2 ⎜\n⎟\n⎝pS(si)⎠\nwher\npS(si)\ne is the pr\nS = si\nobability of the event .\nThe unit of measurement (when the log is base-2) is the bit\n(binary information unit --- not the same as binary digit!).\n1 bit of information corresponds to\npS(si) = 0.5\n. So, for example, when the\noutcome of a fair coin toss is revealed to\nus, we have received 1 bit of information.\n\"Information is the\nresolution of uncertainty\"\nShannon A hand flipping a coin.\nImage by MIT OpenCourseWare.A hand flipping a coin.\n\n6.02 Fall 2012\nLecture 1, Slide #17\nExamples\nWe're drawing cards at random from a standard N=52-card\ndeck. Elementary outcome: card that's drawn, probability 1/52,\ninformation log2(52/1) = 5.7 bits.\nFor an event comprising M such (mutually exclusive) outcomes,\nthe probability is M/52.\nQ. If I tell you the card is a spade , how many bits of\ninformation have you received?\nA. Out of N=52 equally probable cards, M=13 are spades , so\nprobability of drawing a spade is 13/52, and the amount of\ninformation received is log2(52/13) = 2 bits.\nThis makes sense, we can encode one of the 4 (equally probable)\nsuits using 2 binary digits, e.g., 00=, 01=, 10=, 11=.\nQ. If instead I tell you the card is a seven, how much info?\nA. N=52, M=4, so info = log2(52/4) = log2(13) = 3.7 bits\n\n6.02 Fall 2012\nLecture 1, Slide #18\n?\n'\"@\n- ))%%\"\n\n- % \"\n\n%\n)%\n\n+\n\n)\n\n\" 8\n\n\"\n\n!\n- *\n\"T( \"\"\n\n\"\" T \"\"\n\nU\n\"\" %\n\n%)\n?0!R?\n!?0!?! 0\"\"\n\"\"$\n\nG))\n\n!\n\nExpected Information as\nUncertainty or Entropy\nConsider a discr\nS\nete random variable , which may represent\nthe set of possible symbols to be transmitted at a particular\ns1, s2,..., sN\ntime, taking possible values , with respective\npr\npS(s1), pS(s2),..., pS(sN )\nobabilities .\nThe entropy H(S)\nS\nof is the expected (or mean or average)\nvalue of the information obtained by learning the outcome\nS\nof :\nN\nN\n⎛\n⎞\nH(S) =∑pS(si)I(S = si) =∑pS(si)log2 ⎜\n⎟\ni=1\ni=1\n⎝pS(si)⎠\npS(si)\nWhen all the are equal\n1/N\n(with value ), then\nH(S) = log2 N\nor\nN = 2H (S)\n6.02 Fall 2012\nThis is the maximum attainable value!\nLecture 1, Slide #19\n\n6.02 Fall 2012\nLecture 1, Slide #20\ne.g., Binary entropy function h(p)\nHeads (or C=1) with\npr\np\nobability\nTails (or C=0) with\nprobability 1-p\nh(p)\np\n1.0\n0.5\n1.0\n0.5\n1.0\nH(C)= -plog2 p-(1-p)log2(1-p)= h(p)\n\n6.02 Fall 2012\nLecture 1, Slide #21\nConnection to (Binary) Coding\n-\nSuppose p=1/1024, i.e., very small probability of getting a\nHead, typically one Head in 1024 trials. Then\nh(p) = (1/1024)log2(1024 /1)+(1023/1024)log2(1024 /1023)\n=.0112 bits of uncertainty or information per trial\non average\n-\nSo using 1024 binary digits (C=0 or 1) to code the results of\n1024 tosses of this particular coin seems inordinately\nwasteful, i.e., 1 binary digit per trial. Can we get closer to an\naverage of .0112 binary digits/trial?\n-\nYes!\nConfusingly, a binary digit\nis also referred to as a bit!\n-\nBinary coding: Mapping source symbols to binary digits\n\n6.02 Fall 2012\nLecture 1, Slide #22\nSignificance of Entropy\nEntropy (in bits) tells us the average amount of information (in\nbits) that must be delivered in order to resolve the uncertainty\nabout the outcome of a trial. This is a lower bound on the\nnumber of binary digits that must, on the average, be used to\nencode our messages.\nIf we send fewer binary digits on average, the receiver will have\nsome uncertainty about the outcome described by the message.\nIf we send more binary digits on average, we're wasting the\ncapacity of the communications channel by sending binary\ndigits we don't have to.\nAchieving the entropy lower bound is the \"gold standard\" for an\nencoding (at least from the viewpoint of information\ncompression).\n\n6.02 Fall 2012\nLecture 1, Slide #23\nFixed-length Encodings\nAn obvious choice for encoding equally probable outcomes\nis to choose a fixed-length code that has enough sequences\nto encode the necessary information\n\n-\n96 printing characters 7-\"bit\" ASCII\n-\nUnicode characters UTF-16\n-\n10 decimal digits 4-\"bit\" BCD (binary coded decimal)\nFixed-length codes have some advantages:\n\n-\nThey are \"random access\" in the sense that to decode\nthe nth message symbol one can decode the nth fixed-\nlength sequence without decoding sequence 1 through\nn-1.\n-\nTable lookup suffices for encoding and decoding\n\nchoicei\npi\nlog2(1/pi)\nA\n1/3\n1.58 bits\nB\n1/2\n1 bit\nC\n1/12\n3.58 bits\nD\n1/12\n3.58 bits\nThe expected information content in a choice is given by the\nentropy:\n= (.333)(1.58) + (.5)(1) + (2)(.083)(3.58) = 1.626 bits\nCan we find an encoding where transmitting 1000 choices\nrequires 1626 binary digits on the average?\nThe natural fixed-length encoding uses two binary digits for\neach choice, so transmitting the results of 1000 choices requires\n2000 binary digits.\n6.02 Fall 2012\nLecture 1, Slide #24\nNow consider:\n\n6.02 Fall 2012\nLecture 1, Slide #25\nVariable-length encodings\n(David Huffman, in term paper for MIT graduate class, 1951)\nUse shorter bit sequences for high probability choices,\nlonger sequences for less probable choices\nchoicei\npi\nencoding\nA\n1/3\nB\n1/2\nC\n1/12\nD\n1/12\n\nExpected length\n=(.333)(2)+(.5)(1)+(2)(.083)(3)\n= 1.666 bits\n\nB 0\nTransmitting 1000\nA\nchoices takes an\naverage of 1666 bits...\nC\nD better but not optimal\nHuffman Decoding Tree\nBC A BA D\nNote: The symbols are at the leaves of the tree;\nnecessary and sufficient for instantaneously decodability.\n\n6.02 Fall 2012\nLecture 1, Slide #26\nHuffman's Coding Algorithm\n-\nBegin with the set S of symbols to be encoded as binary strings,\ntogether with the probability p(s) for each symbol s in S.\n-\nRepeat the following steps until there is only 1 symbol left in S:\n-\nChoose the two members of S having lowest probabilities.\nChoose arbitrarily to resolve ties.\n-\nRemove the selected symbols from S, and create a new node of\nthe decoding tree whose children (sub-nodes) are the symbols\nyou've removed. Label the left branch with a \"0\", and the right\nbranch with a \"1\".\n-\nAdd to S a new symbol that represents this new node. Assign\nthis new symbol a probability equal to the sum of the\nprobabilities of the two nodes it replaces.\n\n6.02 Fall 2012\nLecture 1, Slide #27\nHuffman Coding Example\n-\nInitially S = { (A, 1/3) (B, 1/2) (C, 1/12) (D, 1/12) }\n-\nCD\nFirst iteration\n- Symbols in S with lowest probabilities: C and D\nC\nD\n- Create new node\n- Add new symbol to S = { (A, 1/3) (B, 1/2) (CD, 1/6) }\nACD\n-\nSecond iteration\n-\nA\nSymbols in S with lowest probabilities: A and CD\n-\nC\n\nD\nCreate new node\n-\nAdd new symbol to S = { (B, 1/2) (ACD, 1/2) }\n-\nThird iteration\nB\n-\nSymbols in S with lowest probabilities: B and ACD\nA\n- Create new node\nC\nD\n- Add new symbol to S = { (BACD, 1) }\n-\nDone\n\n6.02 Fall 2012\nLecture 1, Slide #28\nAnother Variable-length Code (not!)\nHere's an alternative variable-length for the example on the\nprevious page:\n\n,\n-\n\n,,\n\n,-\nWhy isn't this a workable code?\n\nThe expected length of an encoded message is\n\n(.333+.5)(1) + (.083 + .083)(2) = 1.22 bits\n\nwhich even beats the entropy bound ☺\n\n6.02 Fall 2012\nLecture 1, Slide #29\nHuffman Codes - the final word?\n-\nGiven static symbol probabilities, the Huffman algorithm creates\nan optimal encoding when each symbol is encoded separately.\n(optimal no other encoding will have a shorter expected\nmessage length). It can be proved that\nexpected length L satisfies H ≤ L ≤ H+1\n-\nHuffman codes have the biggest impact on average message\nlength when some symbols are substantially more likely than\nother symbols.\n-\nYou can improve the results by adding encodings for symbol\npairs, triples, quads, etc. From example code:\n-\n\nPairs: 1.646 bits/sym, Triples: 1.637, Quads 1.633, ...\n\n-\nBut the number of possible encodings quickly becomes\nintractable.\n-\nSymbol probabilities change message-to-message, or even within\na single message. Can we do adaptive variable-length encoding?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 10: Linear time-invariant (LTI) systems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/49242956b17f47b9c85bd6784aa5d6b8_MIT6_02F12_lec10.pdf",
      "content": "6.02 Fall 2012\nLecture 10, Slide #1\n6.02 Fall 2012\nLecture #10\n- Linear time-invariant (LTI) models\n- Convolution\n\n6.02 Fall 2012\nLecture 10, Slide #2\nModeling Channel Behavior\ncodeword\nbits in\ncodeword\nbits out\nDAC\nADC\nNOISY & DISTORTING ANALOG CHANNEL\nmodulate\ndemodulate\n& filter\ngenerate\ndigitized\nsymbols\nsample &\nthreshold\nx[n]\ny[n]\n\nThe Baseband**\n\nChannel\n6.02 Fall 2012\nLecture 10, Slide #3\nS\nx[n]\ny[n]\ninput\nresponse\nA discrete-time signal such as x[n] or y[n] is described by\nan infinite sequence of values, i.e., the time index n takes\nvalues in inf to +inf. The above picture is a snapshot at a\nparticular time n.\n\nIn the diagram above, the sequence of output values y[.] is\nthe response of system S to the input sequence x[.]\n\nThe system is causal if y[k] depends only on x[j] for j≤k\n\n**From before the modulator till after the demodulator &\nfilter\n\nTime Invariant Systems\nLet y[n] be the response of S to input x[n].\n\nIf for all possible sequences x[n] and integers N\n\nthen system S is said to be time invariant (TI). A time\nshift in the input sequence to S results in an identical\ntime shift of the output sequence.\n\nIn particular, for a TI system, a shifted unit sample\nfunction at the input generates an identically\nshifted unit sample response at the output.\n6.02 Fall 2012\nLecture 10, Slide #4\nS\nx[n-N]\ny[n-N]\nδ[n -N]\nh[n -N]\n\nLinear Systems\nLet y1[n] be the response of S to an arbitrary input x1[n]\nand y2[n] be the response to an arbitrary x2[n].\n\nIf, for arbitrary scalar coefficients a and b, we have:\n\nthen system S is said to be linear. If the input is the\nweighted sum of several signals, the response is the\nsuperposition (i.e., same weighted sum) of the response\nto those signals.\n\nOne key consequence: If the input is identically 0 for a\nlinear system, the output must also be identically 0.\n6.02 Fall 2012\nLecture 10, Slide #5\nS\nax1[n]+ bx2[n]\nay1[n]+ by2[n]\n\nUnit Sample and Unit Step Responses\n6.02 Fall 2012\nLecture 10, Slide #6\nS\nUnit sample\nδ[n]\nh[n]\nUnit sample response\nThe unit sample response of a system S is the response of\nthe system to the unit sample input. We will always\ndenote the unit sample response as h[n].\nS\nu[n]\ns[n]\nUnit step\nUnit step response\nSimilarly, the unit step response s[n]:\n\n6.02 Fall 2012\nLecture 10, Slide #7\nδ[n]= u[n]-u[n -1]\nh[n]= s[n]-s[n -1]\nRelating h[n] and s[n] of an LTI System\nS\nu[n]\ns[n]\nUnit step signal\nUnit step response\nS\nδ[n]\nh[n]\nUnit sample signal\nUnit sample response\nfrom which it follows that\n\n(assuming , e.g., a causal LTI system; more\ngenerally, a \"right-sided\" unit sample response)\ns[n]=\nh[k]\nk=-inf\nn\n∑\ns[-inf]= 0\n\n6.02 Fall 2012\nLecture 10, Slide #8\nh[n] s[n]\n\n6.02 Fall 2012\nLecture 10, Slide #9\nh[n] s[n]\n\n6.02 Fall 2012\nLecture 10, Slide #10\nh[n] s[n]\n\n6.02 Fall 2012\nLecture 10, Slide #11\n\nUnit Step Decomposition\n\"Rectangular-wave\" digital\nsignaling waveforms, of the sort\nwe have been considering, are\neasily decomposed into time-\nshifted, scaled unit steps --- each\ntransition corresponds to another\nshifted, scaled unit step.\n\ne.g., if x[n] is the transmission of\n1001110 using 4 samples/bit:\nx[n]\n= u[n]\n-u[n -4]\n+u[n -12]\n-u[n -24]\n6.02 Fall 2012\n\"\ns\nw\ne\ns\nt\ns\ne\n\n6.02 Fall 2012\nLecture 10, Slide #12\n... so the corresponding response is\ny[n]\n= s[n]\n-s[n -4]\n+ s[n -12]\n-s[n -24]\nx[n]\n= u[n]\n-u[n -4]\n+u[n -12]\n-u[n -24]\nNote how we have invoked linearity and time invariance!\n\nExample\n6.02 Fall 2012\nLecture 10, Slide #13\n\nTr\n\nansmission Over a Channel\n6.02 Fall 2012\nLecture 10, Slide #14\nIgnore this\nnotation for\nnow, will\nexplain\nshortly\n\nReceiv\n\ning the Response\n6.02 Fall 2012\nLecture 10, Slide #15\nDigitization threshold = 0.5V\n\nFaster Tr\n\nansmission\n6.02 Fall 2012\nLecture 10, Slide #16\nFall 2012\nLecture 10, Slid\nNoise margin? 0.5 y[28]\nNoise m\n\nUnit Sample\nDecomposition\n6.02 Fall 2012\nLecture 10, Slide #17\n6.02 Fall\nA discrete-time signal can be decomposed\ninto a sum of time-shifted, scaled unit\nsamples.\n\nExample: in the figure, x[n] is the sum of\nx[-2]δ[n+2] + x[-1]δ[n+1] + ... + x[2]δ[n-2].\n\nIn general:\ninf\nx[n]= ∑x[k]δ[n -k]\nk=-inf\nFor any particular index, only\none term of this sum is non-zero\nl\ni\n\n6.02 Fall 2012\nLecture 10, Slide #18\nIf system S is both linear and time-invariant (LTI), then we can\nuse the unit sample response to predict the response to any\ninput waveform x[n]:\n\nIndeed, the unit sample response h[n] completely characterizes\nthe LTI system S, so you often see\nS\nx[n]=\nx[k]δ[n -k]\nk=-inf\ninf\n∑\ny[n]=\nx[k]h[n -k]\nk=-inf\ninf\n∑\nSum of shifted, scaled unit samples\nSum of shifted, scaled responses\nModeling LT\n\nI Systems\nh[.]\nCONVOLUTION SUM\nx[n]\ny[n]\n\n6.02 Fall 2012\nLecture 10, Slide #19\n\nConvolution\nEvaluating the convolution sum\n\ninf\n\ny[n]= ∑x[k]h[n -k]\n\nk=-inf\nfor all n defines the output signal y in terms of the input x and\nunit-sample response h. Some constraints are needed to ensure\nthis infinite sum is well behaved, i.e., doesn't \"blow up\" --- we'll\ndiscuss this later.\n\nWe use to denote convolution, and write y=x h. We can then\n∗\n∗\nwrite the value of y at time n, which is given by the above sum,\ny[n]= (x∗h)[n]\nas . We could perh\ny[n]= x∗h[n]\naps even write\n\n6.02 Fall 2012\nLecture 10, Slide #20\n\nConvolution\nEvaluating the convolution sum\n\ninf\n\ny[n]= ∑x[k]h[n -k]\n\nk=-inf\nfor all n defines the output signal y in terms of the input x and\nunit-sample response h. Some constraints are needed to ensure\nthis infinite sum is well behaved, i.e., doesn't \"blow up\" --- we'll\ndiscuss this later.\n\nWe use to denote convolution, and write y=x h. We can thus\n∗\n∗\nwrite the value of y at time n, which is given by the above sum,\nas y[n]= (x∗h)[n]\n\nInstead you'll find people writing , where the\npoor index n is doing double or triple duty. This is awful\nnotation, but a super-majority of engineering professors\n(including at MIT) will inflict it on their students.\nDon't stand for it!\n\ny[n]= x[n]∗h[n]\n\nProperties of Convolution\ninf\ninf\n(x∗h)[n]≡∑x[k]h[n -k]= ∑h[m]x[n -m]\nk=-inf\nm=-inf\nThe second equality above establishes that convolution is\ncommutative:\n\nx ∗h = h∗x\n\nConvolution is associative:\n\nx ∗(h1 ∗h2) = x ∗h1 ∗h2\n(\n)\n\nConvolution is distributive:\nx ∗h1 + h2 = (x ∗\n(\n)\nh1)+(x ∗h2)\n6.02 Fall 2012\nLecture 10, Slide #21\n\nSeries Interconnection of LT\n\nI Systems\n6.02 Fall 2012\nLecture 10, Slide #22\nh1[.]\nx[n]\nh2[.]\ny[n]\ny = h2 ∗w = h2 ∗h1 ∗x\n(\n) = h2 ∗h1\n(\n)∗x\n(h2h1)[.]\nx[n]\ny[n]\nw[n]\n(h1h2)[.]\nx[n]\ny[n]\nh2[.]\nx[n]\nh1[.]\ny[n]\n\n6.02 Fall 2012\nLecture 10, Slide #23\nSpot Quiz\n0.5\n0 1 2 3 4 5 ... n\nUnit step response: s[n]\n0.5\n0 1 2 3 4 5 6 7 8 9 n\nx[n]\nS\nx[n]\ny[n]\ninput\nresponse\nFind y[n]:\n\n1. Write x[n] as a function of\nunit steps\n\n2. Write y[n] as a function of\nunit step responses\n\n3. Draw y[n]\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 11: LTI channel and intersymbol interference",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/8989d3b33e177b9a2e373223c62c5b13_MIT6_02F12_lec11.pdf",
      "content": "6.02 Fall 2012\nLecture #11\n- Eye diagrams\n- Alternative ways to look at convolution\n6.02 Fall 2012\nLecture 11, Slide #1\n\nEye Diagrams\n000 100 010 110 001 101 011 111\nThese are overlaid\nEye diagrams make it easy to find\ntwo-bit-slot segments\nthe worst-case signaling conditions\nof step responses, plotted\nat the receiving end.\nwithout the 'stems' of\nthe stem plot on the left\n6.02 Fall 2012\nLecture 11, Slide #2\n\n\"Width\" of Eye\nWorst-case \"1\"\nWorst-case \"0\"\n\"width\" of eye\n(as in \"eye wide open\")\nTo maximize noise margins:\nPick the best sample point widest point in the eye\nPick the best digitization threshold half-way across width\n6.02 Fall 2012\nLecture 11, Slide #3\n\nChoosing Samples/Bit\nOops, no eye!\nye!\nGiven h[n], you can use the eye diagram to pick the\nnumber of samples transmitted for each bit (N):\nReduce N until you reach the noise margin you feel\nis the minimum acceptable value.\n6.02 Fall 2012\nLecture 11, Slide #4\n\nExample: \"ringing\" channel\n6.02 Fall 2012\nLecture 11, Slide #5\n\nConstructing the Eye Diagram\n(no need to wade through all this unless you\nreally want to!)\n1. Generate an input bit sequence pattern that contains all possible\ncombinations of B bits (e.g., B=3 or 4), so a sequence of 2BB bits.\n(Otherwise, a random sequence of comparable length is fine.)\n2. Transmit the corresponding x[n] over the channel (2BBN samples, if\nthere are N samples/bit)\n3. Instead of one long plot of y[n], plot the response as an eye diagram:\na. break the plot up into short segments, each containing\nKN samples, starting at sample 0, KN, 2KN, 3KN, ... (e.g., K=2 or\n3)\nb. plot all the short segments on top of each other\n6.02 Fall 2012\nLecture 11, Slide #6\n\nBack To Convolution\nFrom last lecture: If system S is both linear and time-invariant\n(LTI), then we can use the unit sample response h[n] to predict\nthe response to any input waveform x[n]:\nSum of shifted, scaled unit sample\nSum of shifted, scaled unit sample functions responses, with the same scale factors\ninf\nS\ninf\nx[n] = ∑ x[k]δ[n - k]\ny[n] = ∑ x[k]h[n - k]\nk=-inf\nk=-inf\nCONVOLUTION SUM\nIndeed, the unit sample response h[n] completely characterizes\nthe LTI system S, so you often see\nh[.]\nx[n]\ny[n]\n6.02 Fall 2012\nLecture 11, Slide #7\n\nS\nx[n]\nUnit Sample Response of a\nScale-&-Delay System\ny[n]=Ax[n-D]\nIf S is a system that scales the input by A and delays it by D\ntime steps (negative 'delay' D = advance), is the system\ntime-invariant?\nYes!\nlinear?\n\nYes!\nUnit sample response is h[n]=Aδ[n-D]\nGeneral unit sample response\nh[n]=... + h[-1] δ[n+1] + h[0]δ[n] + h[1]δ[n1]+...\nfor an LTI system can be thought of as resulting from\n6.02 Fall 2012\nmany scale-&-delays in parallel\nLecture 11, Slide #8\n\nA Complementary View of Convolution\nSo instead of the picture:\ninf\ninf\nx[n] = ∑ x[k]δ[n - k]\ny[n] = ∑ x[k]h[n - k]\nk=-inf\nk=-inf\nwe can consider the picture:\nh[.]\nh[.]=...+h[-1]δ[n+1]+h[0]δ[n]+h[1]δ[n-1]+...\nx[n]\ny[n]\ninf\nfrom which we get y[n] = ∑ h[m]x[n - m]\nm=-inf\n(To those who have an eye for these things, my apologies\n6.02 Fall 2012 for the varied math font --- too hard to keep uniform!)\nLecture 11, Slide #9\n\n(side by side)\ny[n] =\ninf\ninf\n(x ∗ h)[n] = ∑ x[k]h[n - k]\n=\n∑ h[m]x[n - m] = (h ∗ x)[n ]\nk=-inf\nm=-inf\nInput term x[0] at\nUnit sample response\ntime 0 launches\nterm h[0] at time 0\nscaled unit sample\ncontributes scaled input\nresponse x[0]h[n] at\nh[0]x[n] to output\noutput\nInput term x[k] at\nUnit sample response\ntime k launches\nterm h[m] at time m\nscaled shifted unit\ncontributes scaled shifted\nsample response\ninput h[m]x[n-m]\nx[k]h[n-k] at output\nto output\n6.02 Fall 2012\nLecture 11, Slide #10\n\nTo Convolve (but not to \"Convolute\"!)\ninf\ninf\n∑ x[k]h[n - k] = ∑ h[m]x[n - m]\nk=-inf\nm=-inf\nA simple graphical implementation:\nPlot x[.] and h[.] as a function of the dummy index\n(k or m above)\nFlip (i.e., reverse) one signal in time,\nslide it right by n (slide left if n is -ve), take the\ndot.product with the other.\nThis yields the value of the convolution at\nthe single time n.\n'flip one & slide by n .... dot.product with the other'\n6.02 Fall 2012\nLecture 11, Slide #11\n\nExample\n- From the unit sample response h[n] to the unit step response\ns[n] = (h *u)[n]\n- Flip u[k] to get u[-k]\n- Slide u[-k] n steps to right (i.e., delay u[-k]) to get u[n-k]),\nplace over h[k]\n- Dot product of h[k] and u[n-k] wrt k:\nn\ns[n] = ∑ h[k]\nk=-inf\n6.02 Fall 2012\nLecture 11, Slide #12\n\nChannels as LTI Systems\nMany transmission channels can be effectively modeled as\nLTI systems. When modeling transmissions, there are few\nsimplifications we can make:\n- We'll call the time transmissions start t=0; the signal before\nthe start is 0. So x[m] = 0 for m < 0.\n- Real-word channels are causal: the output at any time\ndepends on values of the input at only the present and\npast times. So h[m] = 0 for m < 0.\nThese two observations allow us to rework the convolution\nsum when it's used to describe transmission channels:\ninf\ninf\nn\nn\ny[n] = ∑ x[k]h[n - k] =∑x[k]h[n - k\n∑\n] =\nx[k]h[n - k] =∑x[n - j]h[ j]\nk=-inf\nk=0\nk=0\nj=0\n6.02 Fall 2012\nstart at t=0\ncausal\nj=n-k Lecture 11, Slide #13\n\nProperties of Convolution\ninf\ninf\n(x ∗ h)[n] ≡∑ x[k]h[n - k] = ∑ h[m]x[n - m]\nk=-inf\nm=-inf\nThe second equality above establishes that convolution is\ncommutative:\nx ∗ h = h ∗ x\nConvolution is associative:\nx ∗(h1 ∗ h2) = (x ∗ h1 )∗ h2\nConvolution is distributive:\nx ∗(h1 + h2 ) = (x ∗ h1) + (x ∗ h2)\n6.02 Fall 2012\nLecture 11, Slide #14\n\nSeries Interconnection of LTI Systems\nx[n]\nh1[.]\nh2[.]\nw[n]\ny[n]\ny = h2 ∗ w = h2 ∗(h1 ∗ x) = (h2 ∗ h1 )∗ x\n(h2 *h1)[.]\nx[n]\ny[n]\n(h1 *h2)[.]\nx[n]\ny[n]\nh2[.]\nx[n]\nh1[.]\ny[n]\n6.02 Fall 2012\nLecture 11, Slide #15\n\n\"Deconvolving\" Output of\nEcho Channel\nChannel,\nh1[.]\nReceiver\nfilter, h2[.]\nx[n]\ny[n]\nz[n]\nSuppose channel is LTI with\nh 1[n]=δ[n]+0.8δ[n-1]\nFind h2[n] such that z[n]=x[n]\n(h2*h1)[n]=δ[n]\nGood exercise in applying\nFlip/Slide/Dot.Product\n6.02 Fall 2012\nLecture 11, Slide #16\n\n\"Deconvolving\" Output of\nChannel with Echo\nChannel,\nh1[.]\nReceiver\nfilter, h2[.]\nx[n]\ny[n]\n+\nz[n]+v[n]\nw[n]\nEven if channel was well modeled as LTI and h1[n]\nwas known, noise on the channel can greatly degrade\nthe result, so this is usually not practical.\n6.02 Fall 2012\nLecture 11, Slide #17\n\nParallel Interconnection of LTI Systems\nh1[.]\nx[n]\ny1[n]\nh2[.]\n+\ny2[n]\ny[n]\ny = y1 + y2 = (h1 ∗ x) + (h2 ∗ x) = (h1 + h2 )∗ x\n(h1+h2)[.]\nx[n]\ny[n]\n6.02 Fall 2012\nLecture 11, Slide #18\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 12: Filters and composition",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/c609f44097dd5b8ebc46ba1e6ca6d754_MIT6_02F12_lec12.pdf",
      "content": "6.02 Fall 2012\nLecture #12\n- Bounded-input, bounded-output stability\n- Frequency response\n6.02 Fall 2012\nLecture 12, Slide #1\n\nBounded-Input Bounded-Output (BIBO)\nStability\nWhat ensures that the infinite sum\n\"\ny[n] = # h[m]x[n ! m]\nis well-behaved?\nm=!\"\nOne important case: If the unit sample response is absolutely\n\"\nsummable, i.e.,\n# | h[m]|!<!\"\nm=!\"\nand the input is bounded, i.e., | x[k]|!! M < \"\nUnder these conditions, the convolution sum is well-behaved,\nand the output is guaranteed to be bounded.\nThe absolute summability of h[n] is necessary and sufficient\nfor this bounded-input bounded-output (BIBO) stability.\n6.02 Fall 2012\nLecture 12, Slide #2\n\nTime now for a Frequency-Domain Story\nin which\nconvolution\nis transformed to\nmultiplication,\nand other\ngood things\nhappen\n6.02 Fall 2012\nLecture 12, Slide #3\n\nA First Step\nDo periodic inputs to an LTI system, i.e., x[n] such that\nx[n+P] = x[n] for all n, some fixed P\n(with P usually picked to be the smallest positive integer\nfor which this is true) yield periodic outputs? If so, of\nperiod P?\nYes! --- use Flip/Slide/Dot.Product to see\nthis easily: sliding by P gives the same picture\nback again, hence the same output value.\nAlternate argument: Since the system is TI, using\ninput x delayed by P should yield y delayed by P. But\nx delayed by P is x again, so y delayed by P must be y.\n6.02 Fall 2012\nLecture 12, Slide #4\n\nBut much more is true for\nSinusoidal Inputs to LTI Systems\nSinusoidal inputs, i.e.,\nx[n] = cos(Ωn + θ)\nyield sinusoidal outputs at the same 'frequency' Ω rads/sample.\nAnd observe that such inputs are not even periodic\nin general!\nPeriodic if and only if 2π/Ω is rational, =P/Q for some\nintegers P(>0), Q. The smallest such P is the period.\nNevertheless, we often refer to 2π/Ω as the 'period' of this\nsinusoid, whether or not it is a periodic discrete-time\nsequence. This is the period of an underlying\ncontinuous-time signal.\n6.02 Fall 2012\nLecture 12, Slide #5\n\nExamples\ncos(3πn/4) has frequency 3π/4 rad/sample, and\nperiod 8; shifting by integer multiples of 8 yields the same\nsequence back again, and no integer smaller than\n8 accomplishes this.\ncos(3n/4) has frequency 3⁄4 rad/sample, and is not periodic as\na DT sequence because 8π/3 is irrational, but we could\nstill refer to 8π/3 as its 'period', because we can\nthink of the sequence as arising from sampling the\nperiodic continuous-time signal cos(3t/4) at integer t.\n6.02 Fall 2012\nLecture 12, Slide #6\n\nSinusoidal Inputs and LTI Systems\nh[n]\nA very important property of LTI systems or channels:\nIf the input x[n] is a sinusoid of a given amplitude,\nfrequency and phase, the response will be a sinusoid at the\nsame frequency, although the amplitude and phase may be\naltered. The change in amplitude and phase will, in\ngeneral, depend on the frequency of the input.\nLet's prove this to be true ... but use complex exponentials\ninstead, for clean derivations that take care of sines and\ncosines (or sinusoids of arbitrary phase) simultaneously.\n6.02 Fall 2012\nLecture 12, Slide #7\n\nA related simple case:\nreal discrete-time (DT) exponential\ninputs also produce exponential outputs\nof the same type\n- Suppose x[n] = rn for some real number r\n\"\n-\ny[n] = # h[m]x[n ! m]\nm=!\"\n= #\n\"\nh[m]rn!m\nm=!\"\n$\n'\n= & #\n\"\nh[m]r!m )rn\n%m=!\"\n(\n- i.e., just a scaled version of the exponential input\n6.02 Fall 2012\nLecture 12, Slide #8\n\nComplex Exponentials\nA complex exponential is a complex-valued function of a\nsingle argument - an angle measured in radians. Euler's\nformula shows the relation between complex exponentials\nand our usual trig functions:\ne j! = cos(!) + j sin(!)\n1 e! j!\ne j! !\ncos(!) = 2 e j! + 2 e! j!\nsin(!) = 2 j\n2 j\nIn the complex plane, e j! = cos(!) + j sin(!) is a\npoint on the unit circle, at an angle of φ with respect\nto the positive real axis. cos and sin are projections on\nreal and imaginary axes, respectively.\nIncreasing φ by 2π brings you back to the same point!\ne j!\nSo any function of\nonly needs to be studied for φ in [-π, π] .\n6.02 Fall 2012\nLecture 12, Slide #9\n\nUseful Properties of ejφ\nWhen φ = 0:\ne j 0 =1\nWhen φ = ±π:\ne j! = e! j! = !1\ne j!n = e! j!n = (!1)\nn\n(More properties later)\n6.02 Fall 2012\nLecture 12, Slide #10\n\nFrequency Response\ny[n]\nh[.]\nA(cosΩn + jsinΩn)=AejΩn\nUsing the convolution sum we can compute the system's\nresponse to a complex exponential (of frequency Ω) as input:\ny[n] = \"h[m]x[n ! m]\nm\n= \"h[m]Ae j#(n!m)\nm\n$\n'\n= &\"h[m]e! j#m ) Ae j#n\n(\n% m\n= H(#)* x[n]\nwhere we've defined the frequency response of the system as\nH(!) \"$h[m]e# j!m\nm\n6.02 Fall 2012\nLecture 12, Slide #11\n\nBack to Sinusoidal Inputs\nInvoking the result for complex exponential inputs, it is\neasy to deduce what an LTI system does to sinusoidal inputs:\n|H(Ω0)|cos(Ω0n + <H(Ω0))\ncos(Ω0n)\nH(Ω)\nThis is IMPORTANT\n6.02 Fall 2012\nLecture 12, Slide #12\n\nFrom Complex Exponentials to\nSinusoids\ncos(Ωn)=(ejΩn+e-jΩn))/2\nSo response to this cosine input is\n(H(Ω)ejΩn+H(-Ω)e-jΩn))/2 = Real part of H(Ω)ejΩn\n= Real part of |H(Ω)|ej(Ωn+<H(Ω))\ncos(Ω0n)\n|H(Ω0)|cos(Ω0n + <H(Ω0))\nH(Ω)\n6.02 Fall 2012\nLecture 12, Slide #13\n\nSometimes\nwritten\nExample h[n] and H(Ω) as H(ejΩn)\n6.02 Fall 2012\nLecture 12, Slide #14\n\nFrequency Response of \"Moving Average\"\nFilters\n6.02 Fall 2012\nLecture 12, Slide #15\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 13: Frequency response of LTI systems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/e755e873f0d98d5f9b9cb31d6c5ca6ec_MIT6_02F12_lec13.pdf",
      "content": "6.02 Fall 2012\nLecture #13\n- Frequency response\n- Filters\n- Spectral content\n6.02 Fall 2012\nLecture 13 Slide #1\n\nSinusoidal Inputs and LTI Systems\nh[n]\nA very important property of LTI systems or channels:\nIf the input x[n] is a sinusoid of a given amplitude,\nfrequency and phase, the response will be a sinusoid at the\nsame frequency, although the amplitude and phase may be\naltered. The change in amplitude and phase will, in\ngeneral, depend on the frequency of the input.\n6.02 Fall 2012\nLecture 13 Slide #2\n\nComplex Exponentials as\n\"Eigenfunctions\" of LTI System\nx[n]=ejΩn\nh[.]\ny[n]=H(Ω)ejΩn\nEigenfunction: Undergoes only scaling -- by the frequency\nresponse H(Ω) in this case:\n- jΩm\nH(Ω) ≡∑h[m]e\nm\n=\nh[m]cos(Ωm) - j\nh[m]sin(Ωm)\n∑\n∑\nm\nm\nThis is an infinite sum in general, but is well behaved if\nh[.] is absolutely summable, i.e., if the system is stable.\nWe also call H(Ω) the discrete-time Fourier transform (DTFT)\nof the time-domain function h[.] --- more on the DTFT later.\n6.02 Fall 2012\nLecture 13 Slide #3\n\nFrom Complex Exponentials to Sinusoids\ncos(Ωn)=(ejΩn+e-jΩn))/2\nSo response to a cosine input is:\nAcos(Ω0n+Ø0)\n|H(Ω0)|Acos(Ω0n+Ø0+<H(Ω0))\nH(Ω)\n(Recall that we only need vary Ω in the interval [-π,π].)\nThis gives rise to an easy experimental way to determine\nthe frequency response of an LTI system.\n6.02 Fall 2012\nLecture 13 Slide #4\n\nLoudspeaker Frequency Response\nGra\nph\no\nf\nSP\nL\n(d\nec\nib\nel\ns)\nv\ns.\nfrequ\nency i\nn blue,\nwith a green\nhorizontal lin\ne at 85\ndB, and red li\nnes at 82 and 88 dB.\nImage by MIT OpenCourseWare.\n\nSpectral Content of Various Sounds\n6.02 Fall 2012\nLecture 13 Slide #6\nChart showi\nng the frequ\nency range\ns of the\nhuman\nvoice and s\neveral musi\ncal i\nnstrument\ns.\nImage by MIT OpenCourseWare.\n\nConnection between CT and DT\nThe continuous-time (CT) signal\nsampled every T seconds, i.e., at a sampling\nfrequency of fs = 1/T, gives rise to the discrete-time\n(DT) signal\nSo\nΩ = ωΤ\nand Ω = π corresponds to ω = π/T or f = 1/(2T) = fs/2\n6.02 Fall 2012\nLecture 13 Slide #7\nx(t) = cos(ωt) = cos(2πft)\nx[n] = x(nT) = cos(ωnT) = cos(Ωn)\n\nProperties of H(Ω)\nRepeats periodically on the frequency (Ω) axis, with period 2π,\nbecause the input ejΩn is the same for Ω that differ by\ninteger multiples of 2π. So only the interval Ω in [-π,π] is of interest!\n6.02 Fall 2012\nLecture 13 Slide #8\n\nProperties of H(Ω)\nRepeats periodically on the frequency (Ω) axis, with period 2π,\nbecause the input ejΩn is the same for Ω that differ by\ninteger multiples of 2π. So only the interval Ω in [-π,π] is of interest!\nΩ = 0, i.e., ejΩn = 1, corresponds to a constant (or \"DC\", which\nstands for \"direct current\", but now just means constant) input,\nso H(0) is the \"DC gain\" of the system, i.e., gain for constant inputs.\nH(0) = ∑ h[m]\n--- show this from the definition!\n6.02 Fall 2012\nLecture 13 Slide #9\n\nProperties of H(Ω)\nRepeats periodically on the frequency (Ω) axis, with period 2π,\nbecause the input ejΩn is the same for Ω that differ by\ninteger multiples of 2π. So only the interval Ω in [-π,π] is of interest!\nΩ = 0, i.e., ejΩn = 1, corresponds to a constant (or \"DC\", which\nstands for \"direct current\", but now just means constant) input,\nso H(0) is the \"DC gain\" of the system, i.e., gain for constant inputs.\nH(0) = ∑ h[m]\n--- show this from the definition!\nΩ = π or ‒π, i.e., AejΩn=(-1)nA, corresponds to the\nhighest-frequency variation possible for a discrete-time\nsignal, so H(π)=H(-π) is the high-frequency gain of the system.\nH(π) = ∑ (-1)m h[m] --- show from definition!\n6.02 Fall 2012\nLecture 13 Slide #10\n\nSymmetry Properties of H(Ω)\n- jΩm\nH(Ω) ≡∑h[m]e\nm\n=\nh[m]cos(Ωm) - j\nh[m]sin(Ωm)\n∑\n∑\nm\nm\n= C(Ω) - jS(Ω)\nFor real h[n]:\nReal part of H(Ω) & magnitude are EVEN functions of Ω.\nImaginary part & phase are ODD functions of Ω.\nFor real and even h[n] = h[-n],\nH(Ω) is purely real.\nFor real and odd h[n] = -h[-n], H(Ω) is purely imaginary.\n6.02 Fall 2012\nLecture 13 Slide #11\n\nConvolution in Time <--->\nMultiplication in Frequency\nx[n]\nh1[.]\nh2[.]\ny[n]\nx[n]\ny[n]\n(h2*h1)[.]\nIn the frequency domain (i.e., thinking about input-to-output\nfrequency response):\nx[n]\nH1(Ω)\nH2(Ω)\ny[n]\ni.e., convolution in time\nhas become multiplication\nH(Ω)=H2(Ω)H1(Ω) in frequency!\n6.02 Fall 2012\nLecture 13 Slide #12\n\nExample: \"Deconvolving\" Output of\nChannel with Echo\nChannel,\nh1[.]\nReceiver\nfilter, h2[.]\nx[n]\ny[n]\nz[n]\nSuppose channel is LTI with\n1[n]=δ[n]+0.8δ[n-1]\n- jΩm\nH1(Ω) = ?? = ∑h1[m]e\nm\n= 1+ 0.8e-jΩ = 1 + 0.8cos(Ω) - j0.8sin(Ω)\nSo:\n1(Ω)| = [1.64 + 1.6cos(Ω)]1/2\nEVEN function of Ω;\n1(Ω) = arctan [-(0.8sin(Ω)/[1 + 0.8cos(Ω)]\nODD .\n6.02 Fall 2012\nLecture 13 Slide #13\n|H\n<H\n\nA Frequency-Domain view of Deconvolution\nChannel,\nH1(Ω)\nReceiver\nfilter, H2(Ω)\nx[n]\ny[n]\nz[n]\nNoise w[n]\nGiven H1(Ω), what should H2(Ω) be, to get z[n]=x[n]?\nH2(Ω)=1/H1(Ω)\n\"Inverse filter\"\n= (1/|H1(Ω)|). exp{-j<H1(Ω)}\nInverse filter at receiver does very badly in the presence of noise\nthat adds to y[n]:\nfilter has high gain for noise precisely at frequencies where\nchannel gain|H1(Ω)| is low (and channel output is weak)!\n6.02 Fall 2012\nLecture 13 Slide #14\n\nA 10-cent Low-pass Filter\nSuppose we wanted a low-pass filter with a cutoff frequency of π/4?\nH/4(Ω)\nx[n]\nH/2(Ω)\nH3/4(Ω)\nH(Ω)\ny[n]\n6.02 Fall 2012\nLecture 13 Slide #15\n\nTo Get a Filter Section with a\nSpecified Zero-Pair in H(Ω)\n- Let h[0] = h[2] = 1,\nh[1] = μ,\nall other h[n] = 0\n- Then H() = 1 + μe-j + e-j2 = e-j (μ + 2cos())\n- So |H()| = |μ + 2cos()|, with zeros at\n± arccos(-μ/2)\n6.02 Fall 2012\nLecture 13 Slide #16\n\nThe $4.99 version of a Low-pass Filter,\nh[n] and H(Ω)\n6.02 Fall 2012\nLecture 13 Slide #17\n\nDetermining h[n] from H(Ω)\nH(Ω) =∑h[m]e-jΩm\nm\njΩn\nMultiply both sides by e\nand integrate over a\n(contiguous) 2 interval. Only one term survives!\njΩn\n-jΩ(m-n)\n∫ H (Ω)e\ndΩ = ∫∑h[m]e\ndΩ\n<2π>\n<2π> m\n= 2π ⋅h[n]\njΩn\nh[n] =\n∫ H (Ω)e\ndΩ\n2π <2π>\n6.02 Fall 2012\nLecture 13 Slide #18\n\nDesign ideal lowpass filter with cutoff\nfrequency ΩC and H(Ω)=1 in passband\nh[n] = 1 ∫ H(Ω)e jΩndΩ\n2π <2π>\nΩC\njΩn\n=\n∫ 1⋅e\ndΩ\n2π -ΩC\nsin(ΩCn)\n=\n, n = 0\nπn\n(extends to ±inf in time,\n= ΩC / π\n, n = 0\nfalls off only as 1/n))\nDT \"sinc\" function\n6.02 Fall 2012\nLecture 13 Slide #19\n\nExercise: Frequency response of h[n-D]\nGiven an LTI system with unit sample response h[n]\nand associated frequency response H(Ω),\ndetermine the frequency response HD(Ω) of an LTI\nsystem whose unit sample response is\nhD[n] = h[n-D].\nAnswer:\nHD(Ω) = exp{-jΩD}.Η(Ω)\nso :\n|HD(Ω)| = |Η(Ω)| ,\ni.e., magnitude unchanged\nD(Ω) = -ΩD + <Η(Ω) , i.e., linear phase term added\n6.02 Fall 2012\nLecture 13 Slide #20\n<H\n\ne.g.: Approximating an ideal lowpass filter\n-300\nh[n]\nH[Ω]\n-\n\nNot\ncausal\nΩ\nIdea: shift h[n] right to get\ncausal LTI system.\nWill the result still be a\n6.02 Fall 2012 lowpass filter?\nLecture 13 Slide #21\n\nCausal approximation to ideal lowpass filter\nDetermine <HC(Ω)\n6.02 Fall 2012\nLecture 13 Slide #22\n0 300 600\nn\nhC[n]= h[n-300]\n|HC[Ω]|\n-π 0 π\nΩ\n\nDT Fourier Transform (DTFT) for\nSpectral Representation of General x[n]\nIf we can write\njΩn\n- jΩn\nh[n] =\n∫ H (Ω)e\ndΩ\nwhere\nH(Ω) =∑h[n]e\n2π <2π>\nn\nAny contiguous\ninterval of length\nthen we can write\njΩn\n- jΩn\nx[n] =\n∫ X(Ω)e\ndΩ\nwhere\nX(Ω) =∑x[n]e\n2π <2π>\nn\nThis Fourier representation expresses x[n] as\na weighted combination of\nfor all Ω in [-,].\ne jΩn\nX(Ωο)dΩ is the spectral content of x[n]\n6.02 Fall 2012\nin the frequency interval [Ωο, Ωο+ dΩ ]\nLecture 13 Slide #23\n\nUseful Filters\n6.02 Fall 2012\nLecture 13 Slide #24\n\nFrequency Response of Channels\n6.02 Fall 2012\nLecture 13 Slide #25\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 14: Spectral representation of signals",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/3d71e96b6e7942d74c15fb2f99244985_MIT6_02F12_lec14.pdf",
      "content": "6.02 Fall 2012\nLecture #14\n- Spectral content via the DTFT\n6.02 Fall 2012\nLecture 14 Slide #1\n\nDemo: \"Deconvolving\" Output of\nChannel with Echo\nChannel,\nh1[.]\nReceiver\nfilter, h2[.]\nx[n]\ny[n]\nz[n]\nSuppose channel is LTI with\nh1[n]=δ[n]+0.8δ[n-1]\n- jΩm\nH1(Ω) = ?? = ∑h1[m]e\nm\n= 1+ 0.8e-jΩ = 1 + 0.8cos(Ω) - j0.8sin(Ω)\nSo:\n1(Ω)| = [1.64 + 1.6cos(Ω)]1/2\nEVEN function of Ω;\n1(Ω) = arctan [-(0.8sin(Ω)/[1 + 0.8cos(Ω)]\nODD .\n6.02 Fall 2012\nLecture 14 Slide #2\n|H\n<H\n\nA Frequency-Domain view of Deconvolution\nChannel,\nH1(Ω)\nReceiver\nfilter, H2(Ω)\nx[n]\ny[n]\nz[n]\nNoise w[n]\nGiven H1(Ω), what should H2(Ω) be, to get z[n]=x[n]?\nH2(Ω)=1/H1(Ω)\n\"Inverse filter\"\n= (1/|H1(Ω)|). exp{-j<H1(Ω)}\nInverse filter at receiver does very badly in the presence of noise\nthat adds to y[n]:\nfilter has high gain for noise precisely at frequencies where\nchannel gain|H1(Ω)| is low (and channel output is weak)!\n6.02 Fall 2012\nLecture 14 Slide #3\n\nDT Fourier Transform (DTFT) for\nSpectral Representation of General x[n]\nIf we can write\njΩn\n- jΩm\nh[n] =\n∫ H (Ω)e\ndΩ\nwhere\nH(Ω) =∑h[m]e\n2π <2π>\nm\nAny contiguous\ninterval of length\nthen we can write\njΩn\n- jΩm\nx[n] =\n∫ X(Ω)e\ndΩ\nwhere\nX(Ω) =∑x[m]e\n2π <2π>\nm\nThis Fourier representation expresses x[n] as\na weighted combination of\nfor all Ω in [-,].\ne jΩn\nX(Ωο)dΩ is the spectral content of x[n]\n6.02 Fall 2012\nin the frequency interval [Ωο, Ωο+ dΩ ]\nLecture 14 Slide #4\n\nThe spectrum of the exponential signal (0.5)nu[n] is shown over the\nfrequency range Ω = 2πf in [-4π,4π], The angle has units of degrees.\nhttp://cnx.org/content/m0524/latest/\n6.02 Fall 2012\nLecture 14 Slide #5\nCourtesy of Don Johnson. Used with permission; available under a CC-BY license.\n\nx[n] and X(Ω)\n6.02 Fall 2012\nLecture 14 Slide #6\n\nInput/Output Behavior of\nLTI System in Frequency Domain\njΩn\nx[n] =\n∫ X(Ω)e\ndΩ\n2π <2π>\nH(Ω)\ny[n] = 1\n2π\nH(Ω)X(Ω)e jΩn\n<2π> ∫\ndΩ\njΩn\ny[n] =\n∫ Y (Ω)e\ndΩ\n2π <2π>\nY (Ω) = H (Ω)X(Ω)\nCompare with y[n]=(h*x)[n]\nAgain, convolution in time\nhas mapped to\nmultiplication in frequency\n6.02 Fall 2012\nLecture 14 Slide #7\n\nMagnitude and Angle\nY (Ω) = H(Ω)X(Ω)\n|Y (Ω)|= |H(Ω)|. | X(Ω)|\nand\n<Y (Ω) = < H(Ω)+ < X(Ω)\n6.02 Fall 2012\nLecture 14 Slide #8\n\nCore of the Story\n1. A huge class of DT and CT signals\ncan be written --- using Fourier transforms --- as a\nweighted sums of sinusoids (ranging from very slow to very fast)\nor (equivalently, but more compactly) complex exponentials.\nThe sums can be discrete ∑ or continuous ∫ (or both).\n2. LTI systems act very simply on sums of sinusoids:\nsuperposition of responses to each sinusoid, with the\nfrequency response determining the frequency-dependent\nscaling of magnitude, shifting in phase.\n6.02 Fall 2012\nLecture 14 Slide #9\n\nLoudspeaker Bandpass Frequency Response\n6.02 Fall 2012\nF ll 2012\nLecture 14 Slide #10\nGra\nph\no\nf\nSP\nL\n(d\nec\nib\nel\ns)\nv\ns.\nfrequ\nency i\nn blue,\nwith a green\nhorizontal lin\ne at 85\ndB, and red li\nnes at 82 and 88 dB.\nImage by MIT OpenCourseWare.\n\n6.02 Fall 2012\nLecture 14 Slide #11\n(c) PC Magazine. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nSpectral Content of Various Sounds\n6.02 Fall 2012\nLecture 14 Slide #12\nChart showi\nng the frequ\nency range\ns of the\nhuman\nvoice and s\neveral musi\ncal i\nnstrument\ns.\nImage by MIT OpenCourseWare.\n\nConnection between CT and DT\nThe continuous-time (CT) signal\nx(t) = cos( ωt) = cos(2πft)\nsampled every T seconds, i.e., at a sampling\nfrequency of fs = 1/T, gives rise to the discrete-time\n(DT) signal\nx[n] = x(nT) = cos(ωnT) = cos(Ωn)\nSo\nΩ = ωΤ\nand Ω = π corresponds to ω = π/T or f = 1/(2T) = fs/2\n6.02 Fall 2012\nLecture 14 Slide #13\n\nSignal x[n] that has its frequency content\nuniformly distributed in [-Ωc , Ωc]\njΩn\nx[n] =\n∫ X(Ω)e\ndΩ\n2π <2π>\nΩC\njΩn\n=\n∫ 1⋅ e\ndΩ\n2π -ΩC\nsin(ΩCn)\n=\n, n = 0\nπn\nDT \"sinc\" function\n(extends to ±inf in time,\n= ΩC / π\n, n = 0\nfalls off only as 1/n)\n6.02 Fall 2012\nLecture 14 Slide #14\n\nx[n] and X(Ω)\n6.02 Fall 2012\nLecture 14 Slide #15\n\nX(Ω) and x[n]\n6.02 Fall 2012\nLecture 14 Slide #16\n\nFast Fourier Transform (FFT) to compute\nsamples of the DTFT for\nsignals of finite duration\nP-1\n(P/2)-1\n- jΩkm\njΩkn\nX( Ωk ) =∑x[m]e\n,\nx[n]= 1 ∑ X(Ωk )e\nP\nm=0\nk=-P/2\nwhere Ωk = k(2π/P), P is some integer (preferably a power of 2)\nsuch that P is longer than the time interval [0,L-1] over which\nx[n] is nonzero, and k ranges from -P/2 to (P/2)-1 (for even P).\nComputing these series involves O(P2) operations - when P gets\nlarge, the computations get very s l o w....\nHappily, in 1965 Cooley and Tukey published a fast method for\ncomputing the Fourier transform (aka FFT, IFFT), rediscovering\na technique known to Gauss. This method takes O(P log P)\noperations.\nP = 1024, P2 = 1,048,576, P logP ≈ 10,240\n6.02 Fall 2012\nLecture 14 Slide #17\n\nWhere do the Ωk live?\ne.g., for P=6 (even)\nΩ0\nΩ1\nΩ2\nΩ3\nΩ3\nΩ2\nΩ1\n-\n\nexp(jΩ0)\nexp(jΩ1)\nexp(jΩ2)\nexp(jΩ3)\n= exp(jΩ3)\nexp(jΩ1)\nexp(jΩ2)\n.\n-1\nj\n-j\n6.02 Fall 2012\nLecture 14 Slide #18\n\n6.02 Fall 2012\nLecture 14 Slide #19\n\nSpectrum of Digital Transmissions\n6.02 Fall 2012\nLecture 14 Slide #19\n(scaled version of DTFT samples)\n\n6.02 Fall 2012\nLecture 14 Slide #20\n\nSpectrum of Digital Transmissions\n6.02 Fall 2012\nLecture 14 Slide #20\n\nObservations on previous figure\n- The waveform x[n] cannot vary faster than the step change every 7\nsamples, so we expect the highest frequency components in the\nwaveform to have a period around 14 samples. (The is rough and\nqualitative, as x[n] is not sinusoidal.)\n- A period of 14 corresponds to a frequency of 2/14 = /7, which\nis 1/7 of the way from 0 to the positive end of the frequency axis\nat (so k approximately 100/7 or 14 in the figure). And that\nindeed is the neighborhood of where the Fourier coefficients drop\noff significantly in magnitude.\n- There are also lower-frequency components corresponding to the\nfact that the 1 or 0 level may be held for several bit slots.\n- And there are higher-frequency components that result from the\ntransitions between voltage levels being sudden, not gradual.\n6.02 Fall 2012\nLecture 14 Slide #21\n\nEffect of Low-Pass Channel\n6.02 Fall 2012\n6 02 Fall 2012\nLecture 14 Slide #22\n\nHow Low Can We Go?\n7 samples/bit 14 samples/period k=(N/14)=(196/14)=14\n6.02 Fall 2012\nLecture 14 Slide #23\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 15: Modulation/demodulation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/7c65224293acae2b1ec17d2563845bff_MIT6_02F12_lec15.pdf",
      "content": "6.02 Fall 2012\nLecture 15 Slide #1\n6.02 Fall 2012\nLecture #15\n-\nModulation\n- to match the transmitted signal to\nthe physical medium\n-\nDemodulation\n\n6.02 Fall 2012\nLecture 15 Slide #3\n6.02 Fall 2012\nLecture 15 Slide #3\n\nSingle Link Communication Model\nDigitize\n(if needed)\nOriginal source\nSource coding\nSource binary digits\n(\"message bits\")\nBit stream\nRender/display,\netc.\nReceiving app/user\nSource decoding\nBit stream\nChannel\nCoding\n(bit error\ncorrection)\nRecv\nsamples\n+\nDemapper\nMapper\n+\nXmit\nsamples\nBits\nSignals\n(Voltages)\nover\nphysical link\nSignals\n(Voltages)\nChannel\nDecoding\n(reducing or\nremoving\nbit errors)\nRender/display,\netc.\nReceiving app/user\ng a\nSource decoding\netc\nde\nBit stream\nB\nDigitize\n(if needed)\nOriginal source\niti\nSource coding\nSource binary digits\nS\n(\"message bits\")\nBit stream\nB\nEnd-host\ncomputers\n(\nBits\n\nDT Fourier Tr\n\nansform (DTFT) for\nSpectral Representation of Genera\n\nl x[n]\n6.02 Fall 2012\nLecture 15 Slide #4\nx[n]= 1\n2π\nX(Ω)e jΩn\n<2π>∫\ndΩ\nX(Ω) =\nx[m]e-jΩm\nm∑\nwhere\nThis Fourier representation expresses x[n] as\ne jΩn\na weighted combination of for all Ω in [-,].\nX(Ωο)dΩ is the spectral content of x[n]\nin the frequency interval [Ωο, Ωο+ dΩ ]\n\nInput/Output Behav\n\nior of\nLT\n\nI System in Frequency Domain\n6.02 Fall 2012\nLecture 15 Slide #5\nH(Ω)\nx[n]= 1\n2π\nX(Ω)e jΩn\n<2π>∫\ndΩ\ny[n]= 1\n2π\nH(Ω)X(Ω)e jΩn\n<2π>∫\ndΩ\nY(Ω) = H(Ω)X(Ω)\ny[n]= 1\n2π\nY(Ω)e jΩn\n<2π>∫\ndΩ\nSpectral content\nof output\nSpectral content\nof input\nFrequency response\nof system\n\n6.02 Fall 2012\nLecture 15 Slide #6\n(c) PC Magazine. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nPhase\n\nof the frequency response\n\nis important too!\n- Maybe not if we are only interested in audio,\nbecause the ear is not so sensitive to phase\ndistortions\n- But it's certainly important if we are using an\naudio channel to transmit non-audio signals such\nas digital signals representing 1's and 0's, not\nintended for the ear\n6.02 Fall 2012\nLecture 15 Slide #7\n\n6.02 Fall 2012\nLecture 15 Slide #8\nTo gauge how it will fare on\nlowpass and bandpass channels,\nlet's look at the spectral content\nof a rectangular pulse,\n\nx[n]=u[n]-u[n-256],\n\nof the kind we've been using\nin on-off signaling in our\nAudiocom lab.\n\nAny guesses as to spectral shape?\n\nDerivation of DTFT\n\nfor rectangular pulse\n\nx[m]=u[m]-u[m-N]\n6.02 Fall 2012\nLecture 15 Slide #9\nX(Ω) =\nx[m]e-jΩm\nm=0\nN-1\n∑\n=1+e-jΩ +e-j2Ω +...+e-jΩ(N-1)\n= (1-e-jΩN ) / (1-e-jΩ)\n= e-jΩ(N-1)/2 sin(ΩN / 2)\nsin(Ω / 2)\nHeight N at the origin,\nfirst zero-crossing at\n2/N\nShifting in time only changes the phase term in front.\nIf the rectangular pulse is centered at 0, this term is 1.\n\n6.02 Fall 2012\nLecture 15 Slide #10\nSimpler case: DTFT of x[n] = u[n+5] - u[n-6]\n(centered rectangular pulse of length 11)\nA periodic sinc\n(or \"Dirichlet kernel\")\n- not the sinc we've\nseen before!\nhttps://ccrma.stanford.edu/~jos/sasp/Rectangular_Window.html\nN\n2/N\nCourtesy of Julius O. Smith. Used with permission.\n\n6.02 Fall 2012\nLecture 15 Slide #11\nMagnitude of preceding DTFT\nhttps://ccrma.stanford.edu/~jos/sasp/Rectangular_Window.html\nCourtesy of Julius O. Smith. Used with permission.\n\n6.02 Fall 2012\nLecture 15 Slide #12\nDTFT of x[n]= u[n] - u[n-10],\nrectangular pulse of length 10\nstarting at time 0\nhttp://cnx.org/content/m0524/latest/\nCourtesy of Don Johnson. Used with permission; available under a CC-BY license.\n\nBack to our\n\nAudiocom lab example\n6.02 Fall 2012\nLecture 15 Slide #13\n\n6.02 Fall 2012\nLecture 15 Slide #14\n\nx[n]=u[n]-u[n-256]\n\n6.02 Fall 2012\nLecture 15 Slide #15\n|DTFT| of x[n]=u[n]-u[n-256],\nrectangular pulse of length 256:\n48000 samples of\n|DTFT| spread\nevenly between\n[-π , π], computed\nusing FFT (around\n3000 times faster\nthan direct\ncomputation in this\ncase!)\n-\nΩ =\nf = fs /2\n0 rads/sample\n0 Hz\nIf sampling rate is\n48 kHz, then this\nis 24,000 Hz\n\n6.02 Fall 2012\nLecture 15 Slide #16\nZooming in:\n187.5 Hz (corresponds to 2/N\nwhen fs = 48 kHz)\n256 = N\nToo much of the\nsignal's energy misses\nthe loudspeaker's\npassband!\n\n6.02 Fall 2012\nLecture 15 Slide #17\nWhat if we sent this pulse through an\n\nideal lowpass channel?\n|DTFT| of lowpass\nfiltered version of\nx[n]=u[n]-u[n-256],\ncutoff 400 Hz\n-\nΩ =\nf = fs /2\n\n6.02 Fall 2012\nLecture 15 Slide #18\nZooming in:\n400 Hz\n256 = N\n\n6.02 Fall 2012\nLecture 15 Slide #19\nNo longer confined to\nits 256-sample slot, so\ncauses \"intersymbol\ninterference\" (ISI).\nCorresponding pulse in\ntime, i.e., lowpass filtered\nversion of rectangular\npulse\n\nEffect of Low-P\n\nass Channel\n6.02 Fall 2012\nLecture 15 Slide #20\n6 02 Fall 2012\n\nHow Low Can We\n\nGo?\n6.02 Fall 2012\nLecture 15 Slide #21\n\nComplementary/dual behav\n\nior in\n\ntime and frequency domains\n- Wider in time, narrower in frequency; and vice\nversa.\n- This is actually the basis of the uncertainty principle\nin physics!\n- Smoother in time, sharper in frequency; and vice\nversa\n- Rectangular pulse in time is a (periodic) sinc in\nfrequency, while rectangular pulse in frequency is\na sinc in time; etc.\n\n6.02 Fall 2012\nLecture 15 Slide #22\n\n6.02 Fall 2012\nLecture 15 Slide #23\nSlightly round the transitions\nfrom 0 to 1, and from 1 to 0,\nby making them sinusoidal,\njust 30 samples on each end.\nA shaped pulse versus a rectangular pulse:\n\n6.02 Fall 2012\nLecture 15 Slide #24\n|DTFT| of\nrectangular pulse\nNegative|DTFT| of\nshaped pulse\nFrequency content\nof shaped pulse\nonly extends to here,\naround 1500 Hz\nIn the spectral domain:\n\n6.02 Fall 2012\nLecture 15 Slide #25\nThe lowpass filtered\nshaped pulse conforms\nmore tightly to the\n256-sample slot,\nand settles a little quicker\nAfter passing the two pulses through a 400 Hz cutoff lowpass filter:\n\nBut loudspeakers are bandpass,\n\nnot lowpass\n6.02 Fall 2012\nLecture 15 Slide #26\n\n6.02 Fall 2012\nLecture 15 Slide #27\n(c) PC Magazine. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n6.02 Fall 2012\nLecture 15 Slide #28\nSpectrum of\nrectangular pulse\nafter ideal\nbandpass filtering,\n100 Hz\nto 10,000 Hz\n10,000 Hz\n\n6.02 Fall 2012\nLecture 15 Slide #29\nZooming in:\n100 Hz\n10,000 Hz\n\n6.02 Fall 2012\nLecture 15 Slide #30\nCorresponding pulse in\ntime, i.e., bandpass\nWon't do\nfiltered version of\nat all!!\nrectangular pulse\n\nThe Solution: Modulation\n- Shift the spectrum of the signal x[n] into the\nloudspeaker's passband by modulation!\n6.02 Fall 2012\nLecture 15 Slide #31\nx[n]cos(Ωcn) = 0.5x[n](e jΩcn +e-jΩcn)\n= 0.5\n2π [\nX(Ω')e j(Ω'+Ωc )n\n<2π>∫\ndΩ'+\nX(Ω\")e j(Ω\"-Ωc )n\n<2π>∫\ndΩ\"]\n= 0.5\n2π [\nX(Ω-Ωc)e jΩn\n<2π>∫\ndΩ+\nX(Ω+Ωc)e jΩn\n<2π>∫\ndΩ]\nSpectrum of modulated signal comprises half-height\nreplications of X(Ω) centered as ±Ωc (i.e., plus and minus\nthe carrier frequency). So choose carrier frequency comfortably\nin the passband, leaving room around it for the spectrum of x[n].\n\nIs Modulation Linear? Ti\n\nme-Invariant? ...\n6.02 Fall 2012\nLecture 15 Slide #32\n×\nx[n]\ncos(Ωcn)\nt[n]\n... as a system that takes input x[n] and produces\noutput t[n] for transmission?\nYes, linear!\n\nNo, not time-invariant!\n\n6.02 Fall 2012\nLecture 15 Slide #33\nSo for our rectangular pulse example:\nTime domain:\nPulse modulated onto\n1000 Hz carrier\n\n6.02 Fall 2012\nLecture 15 Slide #34\nCorresponding\nspectrum of\nsignal modulated\nonto carrier\n\n6.02 Fall 2012\nLecture 15 Slide #35\n0 Hz\n100 Hz, lower cutoff\nof bandpass filter\nZooming in:\n1000 Hz\n128, i.e.\nhalf height\nof original\n10,000 Hz, upper\ncutoff of bandpass\nfilter\n-1000 Hz\n\n6.02 Fall 2012\nLecture 15 Slide #36\nPulse modulated\nonto 1000 Hz\ncarrier makes it\nthrough the bandpass\nchannel with very little\ndistortion\n\nAt the Receiver: Demodulation\n- In principle, this is (as easy as) modulation again:\nIf the received signal is\nr[n] = x[n]cos(Ωcn),\nthen simply compute\nd[n] = r[n]cos(Ωcn)\n= x[n]cos2(Ωcn)\n= 0.5 {x[n] + x[n]cos(2Ωcn)}\n\n- What does the spectrum of d[n] look like?\n- What constraint on the bandwidth of x[n] is needed\nfor perfect recovery of x[n]?\n6.02 Fall 2012\nLecture 15 Slide #38\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 16: More on modulation/demodulation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/48f208ee650147f6c3d6f7bcc602cc66_MIT6_02F12_lec16.pdf",
      "content": "6.02 Fall 2012\nLecture 16 Slide #1\n6.02 Fall 2012\nLecture #16\n-\nDTFT vs DTFS\n-\nModulation/Demodulation\n-\nFrequency Division Multiplexing\n(FDM)\n\nFast Fourier Transform (FFT) to compute\nsamples of the DTFT\n\nfor\nsignals of finite dura\n\ntion\nP-1\nX(Ωk) =∑x[m]e-jΩkm,\nx[n]=\nm=0\nP\n6.02 Fall 2012\nLecture 16 Slide #2\nFor an x[n] that is zero outside of the interval [0,L-1], choose\nP ≥ L (with P preferably a power of 2; we'll assume that it's at\nleast a multiple of 2, i.e., even):\n(P/2)-1\n∑X(Ωk)e jΩkn\nk=-P/2\nwhere Ωk = k(2π/P), and k ranges from -P/2 to (P/2)-1, or over\nany P successive integers. Simpler notation: X(Ωk) = Xk\n\nWhere do the Ω\n6.02 Fall 2012\nLecture 16 Slide #3\nΩk\n\nlive?\ne.g., for P=6 (even )\n-\n\nΩ0\nΩ1\nΩ2\nΩ3\nΩ3\nΩ2\nΩ1\nexp(jΩ0)\nexp(jΩ1)\nexp(jΩ2)\nexp(jΩ3)\n= exp(jΩ3)\nexp(jΩ1)\nexp(jΩ2)\n.\n-1\nj\n-j\n\nFast Fourier Transform (FFT) to compute\nsamples of the DTFT\n\nfor\nsignals of finite dura\n\ntion\nP-1\nX(Ωk) =∑x[m]e-jΩkm,\nx[n]=\nm=0\nP\n6.02 Fall 2012\nLecture 16 Slide #4\nFor an x[n] that is zero outside of the interval [0,L-1], choose\nP ≥ L (with P preferably a power of 2; we'll assume that it's at\nleast a multiple of 2, i.e., even):\n(P/2)-1\n∑X(Ωk)e jΩkn\nk=-P/2\nwhere Ωk = k(2π/P), and k ranges from -P/2 to (P/2)-1, or over\nany P successive integers. Simpler notation: X(Ωk) = Xk\n\nNote that X(Ω\n) = X() = X(-\nP/2\n) =X(Ω-P/2).\n\nThe above formulas have essentially the same structure, and are\nboth efficiently computed, with Plog(P) computations, by the FFT.\n\nSome further details, assuming real\n\nx[n]\nP-1\nX(Ω ) =∑x[m]e-jΩkm\nk\n\nm=0\n-\nX(0) = sum of x[m] = real\n-\nX() = X(-) = alternating sum of x[m] = real\n-\nIn general P-2 other complex values, but X(-Ωk) = X*(Ωk)\n-\nSo: total of P numbers to be determined, given P values of x[m]\nx[n]= P\n6.02 Fall 2012\nLecture 16 Slide #5\n(P/2)-1\n∑X(Ωk)e jΩkn\nk=-P/2\n-\nEvaluating this eqn. for n in [0,P-1] recovers the original x[n] in this\ninterval\n-\nEvaluating it for n outside this interval results in periodic\nreplication of the values in [0,P-1], producing a periodic signal x[n]\n-\nSo this eqn. is also called a DT Fourier Series (DTFS) for the\nperiodic signal x[n]. Notation: Ak=X(Ωk)/P=Xk/P, Fourier coefficient.\n\nWhy the periodicity of x\n6.02 Fall 2012\nLecture 16 Slide #6\nx[n] is irrelevant in\n\nmany applications\nh[.]\nx[n]\ny[n]\n\nSuppose x[n] is nonzero only over the time interval [0 , nx],\nand h[n] is nonzero only over the time interval [0 , nh] .\n\nIn what time interval can the non-zero values of y[n] be\nguaranteed to lie? The interval [0 , nx + nh] .\nSince all the action we are interested in is confined to this\ninterval, choose P - 1 ≥ nx + nh .\n\nIt's now irrelevant what happens outside [0,P-1]. So we can use\nthe FFT to go back and forth between samples of X(Ω), Η(Ω), Y(Ω)\nin the frequency domain and time-domain behavior in [0,P-1].\n\nBack to Modulation/Demodulation\n-\nYou have: a signal x[n] at baseband (i.e., centered around 0\nfrequency)\n-\nYou want: the same signal, but centered around some specific\nfrequency Ωc\n-\nModulation: convert from baseband up to Ωc , to get t[n]\n-\nDemodulation: convert from Ωc down to baseband\n6.02 Fall 2012\nLecture 16 Slide #7\nRe(Xk)\nIm(Xk)\n+Ωm\nΩm\nRe(Tk)\nIm(Tk)\n+Ωc\nΩc\nmodulation\ndemodulation\nSignal centered at 0\nSignal centered at Ωc\n\nModulation by Heterodyning or\nAmplitude Modulation (AM)\n6.02 Fall 2012\nLecture 16 Slide #8\n×\nx[n]\ncos(Ωcn)\nt[n]\nIm(Tk)\n+Ωc\nΩc\nB/2\nk\nA/2\nRe(T )\ni.e., just replicate baseband\nsignal at ±Ωc, and scale\nby 1⁄2.\nk\nIm(Xk)\n+Ωm\nΩm\nA\nB\nRe(X )\nTo get this nice picture, the\nbaseband signal needs to be\nband-limited to some range of\nfrequencies [-Ωm,Ωm], where\nΩm ≤ Ωc\n\n6.02 Fall 2012\nLecture 16 Slide #9\n0 Hz\n1000 Hz\n-1000 Hz\nNot great band-limiting, but maybe we can get away with it!\n\nAt the Receiver: Demodulation\n-\nIn principle, this is (as easy as) modulation again:\nIf the received signal is\nr[n] = x[n]cos(Ωcn) = t[n],\n(no distortion or noise) then simply compute\nd[n] = r[n]cos(Ωcn)\n= x[n]cos2(Ωcn)\n= 0.5 {x[n] + x[n]cos(2Ωcn)}\nIf there is distortion (i.e., r[n] = t[n]), then write y[n] instead of\nx[n] (and hope that in the noise-free case y[.] is related to x[.] by\nan approximately LTI relationship!)\n\n-\nWhat does the spectrum of d[n], i.e., D(Ω), look like?\n-\nWhat constraint on the bandwidth of x[n] is needed for perfect\nrecovery of x[n]?\n6.02 Fall 2012\nLecture 16 Slide #10\n\nDemodulation Frequency Diagra\nm\n6.02 Fall 2012\nLecture 16 Slide #11\nIm(Tk)\n+Ωc\nΩc\nA/2\nB/2\nk\nRe(T )\nIm(Dk)\n2Ωc\nA/2\nB/2\n+2Ωc\nΩm\nΩm\nRe(Dk)\n\nR(Ω)=T(Ω)\nD(Ω)\nWhat we want\nNote combining of signals around 0\nresults in doubling of amplitude\n\nDemodulation + LPF\n6.02 Fall 2012\nLecture 16 Slide #12\n×\nr[n]\ncos(Ωc n)\nd[n]\nLPF\nx[n]\nCutoff @ ±Ωc\nGain = 2\n\nPhase Error In Demodulation\nWhen the receiver oscillator is out of phase with the transmitter:\nd[n]= r[n]⋅cos(Ωcn -φ) = x[n]⋅cos(Ωcn)⋅cos(Ωcn -φ)\n6.02 Fall 2012\nLecture 16 Slide #13\ny[n]= x[n].cos(φ)\nSo a phase error of φ results in amplitude scaling by cos(φ).\n\nNote: in the extreme case where φ=π/2, we are demodulating\nby a sine rather than a cosine, and we get y[n]=0 .\nBut\ncos(Ωcn).cos(Ωcn -φ) = 0.5{cos(φ)+cos(2Ωcn -φ)}\nIt follows that the demodulated output, after the LPF of\ngain 2, is\n\nDemodulation with sin(Ω\n6.02 Fall 2012\nLecture 16 Slide #14\nΩc\nn)\n+\n+2Ωc\n-2Ωc\nD(Ω)\nIm(Tk)\n+Ωc\nΩc\nA/2\nB/2\nRe(Tk)\nR(Ω)\n\n... produces\n6.02 Fall 2012\nLecture 16 Slide #15\nNote combining of signals around 0\nresults in cancellation!\n\nChannel Delay\n6.02 Fall 2012\nLecture 16 Slide #16\n×\nt[n]\nd[n]\nLPF\ny[n]\nCutoff @ ±kin\nGain = 2\n×\ncos(Ωcn)\nx[n]\nD\nTime delay of D samples\nd[n]= t[n -D]⋅cos(Ωcn)\n= x[n -D]⋅cos[Ωc(n -D)]⋅cos(Ωcn)\nPassing this through the LPF:\nLooks like a phase error\nof ΩcD\ny[n]= x[n -D]⋅cos(ΩcD)\nD)\ncos(Ωcn)\nVery similar math to the previous \"phase error\" case:\nIf ΩcD is an odd multiple of /2, then y[n]=0 !!\n\nFixing Phase Problems in the Receiver\nSo phase errors and channel delay both result in a scaling of the\noutput amplitude, where the magnitude of the scaling can't\nnecessarily be determined at system design time:\n- channel delay varies on mobile devices\n- phase difference between transmitter and receiver is arbitrary\n\nOne solution: quadrature demodulation\n6.02 Fall 2012\nLecture 16 Slide #17\n×\ncos(Ωcn)\nLPF\nI[n] = x[n-D]·cos(θ)\n×\nLPF\nQ[n] = x[n-D]·sin(θ)\nFrom\nchannel\nθ = φ - ΩcD\nsin(Ωcn)\n\nQuadrature Demodulation\nIf we let\nw[n]= I[n]+ jQ[n]\nthen\nw[n]\n6.02 Fall 2012\nLecture 16 Slide #18\n=\nI[n]2 +Q[n]2\n=| x[n -D]|\ncos2θ +sin2θ\n=| x[n -D]|\nx[n-D]cos(θ)\nx[n-D]sin(θ)\nI\nθ\njQ\nConstellation diagrams:\ntransmitter\nreceiver\nI\nQ\nI\nQ\nx[n] = { 0, 1 }\nOK for recovering x[n] if it\nnever goes negative, as in\non-off keying\n\nDealing With Phase\n\nAmbiguity\n\nin Bipolar Modulation\nIn bipolar modulation (x[n]=±1), also called Binary\nPhase Shift Keying (BPSK) since the modulated\ncarrier changes phase by /2 when x[n] switches\nlevels, the received constellation will be rotated\nwith respect to the transmitter's constellation.\nWhich phase corresponds to which bit?\n6.02 Fall 2012\nLecture 16 Slide #19\nQ\nI\nDifferent fixes:\n\n1. Send an agreed-on sign-definite preamble\n2. Transmit differentially encoded bits, e.g., transmit a \"1\" by\nstepping the phase by , transmit a \"0\" by not changing the\nphase\n\nQPSK Modulation\n6.02 Fall 2012\nLecture 16 Slide #20\n×\ncos(Ωcn)\n×\nsin(Ωcn)\n+\nt[n]\nI[n]\nQ[n]\nI\nQ\n(-1,1)\n(1,1)\n(-1,-1)\n(1,-1)\nWe can use the quadrature scheme at the transmitter too:\nSamples from first bit stream\nSamples from second bit stream\n\nhttp://en.wikipedia.org/wiki/Phase-shift_keying\n6.02 Fall 2012\nLecture 16 Slide #21\nPhase Shift Keying underlies many\n\nfamiliar modulation schemes\nThe wireless LAN standard, IEEE 802.11b-1999, uses a variety of different PSKs depending on the data-\nrate required. At the basic-rate of 1 Mbit/s, it uses DBPSK (differential BPSK). To provide the extended-rate\nof 2 Mbit/s, DQPSK is used. In reaching 5.5 Mbit/s and the full-rate of 11 Mbit/s, QPSK is employed, but has\nto be coupled with complementary code keying. The higher-speed wireless LAN standard,\nIEEE 802.11g-2003 has eight data rates: 6, 9, 12, 18, 24, 36, 48 and 54 Mbit/s. The 6 and 9 Mbit/s modes\nuse OFDM modulation where each sub-carrier is BPSK modulated. The 12 and 18 Mbit/s modes use\nOFDM with QPSK. The fastest four modes use OFDM with forms of quadrature amplitude modulation.\n\nBecause of its simplicity BPSK is appropriate for low-cost passive transmitters, and is used\nin RFID standards such as ISO/IEC 14443 which has been adopted for biometric passports, credit cards\nsuch as American Express's ExpressPay, and many other applications.\n\nBluetooth 2 will use (p/4)-DQPSK at its lower rate (2 Mbit/s) and 8-DPSK at its higher rate (3 Mbit/s) when\nthe link between the two devices is sufficiently robust. Bluetooth 1 modulates with\nGaussian minimum-shift keying, a binary scheme, so either modulation choice in version 2 will yield a higher\ndata-rate. A similar technology, IEEE 802.15.4 (the wireless standard used by ZigBee) also relies on PSK.\nIEEE 802.15.4 allows the use of two frequency bands: 868-915 MHz using BPSK and at 2.4 GHz using\nOQPSK.\n\nMultiple Transmitters:\n\nFrequency Division Multiplexing (FDM)\n6.02 Fall 2012\n×\nxB[n]\n×\nxR[n]\n×\nxG[n]\ncos(ΩBn)\ncos(ΩRn)\ncos(ΩGn)\nLecture 16 Slide #22\n+\nChannel \"performs addition\" by\nsuperposing signals (\"voltages\") from\ndifferent frequency bands.\nChoose bandwidths and Ωc's so as to\navoid overlap! Once signals combine at\na given frequency, can't be undone...\nLPF cutoff needs to be half the minimum\nseparation between carriers\n-ΩG -ΩR -ΩB\nΩB\nΩR\nΩG\n\nAM Radio\n\nAM radio stations are on 520 - 1610 kHz ('medium wave\")\nin the US, with carrier frequencies of different stations\nspaced 10 kHz apart.\n\nPhysical effects very much affect operation. e.g., EM signals\nat these frequencies propagate much further at night (by\n\"skywave\" through the ionosphere) than during the day (100's\nof miles by \"groundwave\" diffracting around the earth's\nsurface), so transmit power may have to be lowered at night!\n\n6.02 Fall 2012\nLecture 16 Slide #23\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 17: Packet switching",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/8f332043eaaa2d41028991a442dd4fb6_MIT6_02F12_lec17.pdf",
      "content": "11/7/12\n6.02 Fall 2012\nLecture #17\n- Communication networks (intro)\n- Packet switching\n- Delays, queues, and Little's Law\n6.02 Fall 2012\nLecture 17, Slide #1\nFrom Links to Networks\nWe've studied channel coding,\nand modulation: we know how\nto build a communication link\n-\nHave: digital point-to-point\n-\nWant: many interconnected points\n6.02 Fall 2012\nLecture 17, Slide #2\n6.02 Fall 2012\nLecture 17, Slide #3\nMulti-hop Networks\nEnd point\nLink\nSwitch\nNetwork topology (modeled as a graph)\n6.02 Fall 2012\nLecture 17, Slide #4\n6.02\n6.0\n6.022\n6.02\n6.022\n6.02\n6.02\n6 0\n6.002\n6 022\n6.02\n.02\n.02\n6 02\n6.02\n6 0\n6.\n6 0\nFall\nFall\nFal\nFall\nall\nFal\nFal\nFall\na\nFa\nFa\nFa\nFa\nFa\nFa\nF\nLecture 17, Slide #4\nMIT\n\nNetwork\nYour Network Here!\nYour Network Here!\n,PDJHE\\0,72SHQ&RXUVH:DUH\n\n11/7/12\nSharing the Network\nWe have many applicationlevel communications,\nwhich we'll call \"connections\", that need to mapped\nonto a smaller number of links\nHow should we shar e the links between all the\nconnections?\nTwo approaches possible:\nCircuit switching (isochronous)\nPacket switching (asynchronous)\n6.02 Fall 2012\nLecture 17, Slide #5\nMultiplexing/Demultiplexing\nSwitch\n0 1 2 3 4 5\nSlots =\nFrames\n0 1 2 3 4 5\nOne sharing technique: timedivision multiplexing (TDM)\n- Time divided into frames and frames divided into slots\n- Number of slots = number of concurrent conversations\n- Relative slot position inside a frame deter mines which\nconversation the data belongs to\n- E.g., slot 0 belongs to the r ed conversation\n- Mapping established during setup, removed at tear down\n- Forwarding step at switch: consult table\n6.02 Fall 2012\nLecture 17, Slide #7\nCircuit Switching\n-\nFirst establish a\nCallee\nCaller\ncircuit between end\npoints\n- E.g., done when you\ndial a phone number\n- Message propagates\n(1)\nDATA\nEstablish\nfrom caller toward\ncallee, establishing\nsome state in each\nswitch\n(2)\n-\nThen, ends send\nCommunicate\ndata (\"talk\") to each\nother\n(3)\nTear down\n-\nAfter call, tear down\n(close) circuit\n- Remove state\n6.02 Fall 2012\nLecture 17, Slide #6\nTDM Shares Link Equally, But Has Limitations\nSwitch\nTimeslots\nframes =0 1 2 3 4 5 0 1 2 3 4 5\n- Suppose link capacity is C bits/sec\n- Each communication requires R bits/sec\n- #frames in one \"epoch\" (one frame per communication)\n= C/R\n- Maximum number of concurrent communications is C/R\n- What happens if we have more than C/R communications?\n- What happens if the communication sends less/mor e than R\nbits/sec?\nDesign is unsuitable when traf fic arrives in bursts\n6.02 Fall 2012\nLecture 17, Slide #8\n\n11/7/12\nCircuitSwitching Example: Telephone Network\n6RXUFHXQNQRZQ$OOULJKWVUHVHUYHG7KLVFRQWHQWLVH[FOXGHG\nIURPRXU&UHDWLYH&RPPRQVOLFHQVH)RUPRUHLQIRUPDWLRQ\nVHHKWWSRFZPLWHGXIDLUXVH\n6.02 Fall 2012\nLecture 17, Slide #9\nPacket Switching\n- Used in the Inter net\n- Data is sent in packets\nHost 1\nHost 2\n(header contains control info,\ne.g., source and destination\naddresses)\nNode 1\nNode 2\nHeader\nData\npropagation\nPacket 1\nPacket 2\nPacket 3\nPacket 1\nPacket 2\nPacket 3\nPacket 1\nPacket 2\nPacket 3\ndelay between\ntransmission\nHost 1 &\nprocessing\ntime of Packet 1\nNode 2\ndelay of\n- Per packet forwarding\nat Host 1\nPacket 1 at\n- At each node the entir e\nNode 2\npacket is received, stored,\nand then forwar ded (store\nandforward networks)\n- No capacity is allocated\n6.02 Fall 2012\nLecture 17, Slide #11\nPacket-Switched Networks\n5$1'&RUSRUDWLRQ2Q'LVWULEXWHG&RPPXQLFDWLRQV,QWURGXFWLRQ\nWR'LVWULEXWHG&RPPXQLFDWLRQV1HWZRUNV5035\n5HSULQWHGZLWKSHUPLVVLRQ\nPaul Baran in the late 1950s envisioned a communications network that would\nsurvive a major enemy attack. The sketch shows three dif ferent network topologies\ndescribed in his RAND Memorandum,\n\"On Distributed Communications: 1. Intr oduction to Distributed Communications\nNetwork\" (August 1964). The distributed network structure was judged to of fer the\nbest survivability.\n6.02 Fall 2012\nLecture 17, Slide #10\nSimple header example\n6.02 Fall 2012\nLecture 17, Slide #12\nHop Limit\nDestnaton Address\nSource Address\nLength\n\n11/7/12\nTraffic\nVersion\nFlow Label\nClass\nLength\nNext Header\nHop Limit\nIP Version 6 header\nDestination Address\nSource Address\n6.02 Fall 2012\n6 02 Fall 2012\nLecture 17 Slide #13\n6 02 Fall 2012\nLecture 17 Slide #13\n6 02 Fall 2012\nLecture 17 Slide #13\n6 02 Fall 2012\nLecture 17 Slide #13\n6.02 Fall 2012\nLecture 17, Slide #13\nLectu\nLecture\nre 17\n17,\nFall\n, Sli\nSlide #1\nde #13\n6.02\n6.02 Fall 2012\nLecture 17, Slide #14\nRouter\nPacket Switching: Multiplexing/Demultiplexing\n- Router has a routing table that contains information about\nwhich link to use to reach a destination\n- For each link, packets are maintained in a queue\n- If queue is full, packets will be dropped\n- Demultiplex using information in packet header\n- Header has destination\nQueue\n6.02 Fall 2012\nLecture 17, Slide #15\nWhy Packet Switching Works:\nStatistical Multiplexing\nAggregatng mu tp e conersatons smooths usage\n6.02 Fall 2012\nLecture 17, Slide #16\n10 ms windows\n100 ms windows\ntes\ntme\ntes\ntme\nrac in ire ess LA during a ecture\nNotice how bursts\nbecome smoother\n(but don't completely\ndisappear)\n\nQueues\n\n11/7/12\n6.02 Fall 2012\nLecture 17, Slide #17\n1 second windows\n100 ms windows\ntes\ntme\ntes\ntme\nNotice how bursts\nbecome smoother\n(but don't completely\ndisappear)\nQueues are Essential in a PacketSwitched Network\n- Queues manage packets between arrival and departure\n- They are a \"necessary evil\"\n- Needed to absorb bursts\n- But they add delay by making packets wait until link is\navailable\n- So they shouldn't be too big\n6.02 Fall 2012\nLecture 17, Slide #19\n1 second windows\n6.02 Fall 2012\nLecture 17, Slide #18\n10 second windows\nBest Effort Delivery Model\nNo Guarantees!\n- No guarantee of delivery at all!\n- Packets get dropped (due to corruption or congestion)\n- Use Acknowledgement/Retransmission protocol to recover\n- How to deter mine when to retransmit? T imeout?\n- Each packet is individually routed\n- May arrive at final destination reordered from the transmit or der\n- No latency guarantee for delivery\n- Delays through the network vary packettopacket\n- If packet is r etransmitted too soon duplicate\n6.02 Fall 2012\nLecture 17, Slide #20\n\n6.02 Fall 2012\n\n11/7/12\nFour Sources of Delay (Latency) in Networks\n- Propagation delay\n- Speedofsignal (light) delay: T ime to send 1 (first) bit\n- Processing delay\n- Time spent by the hosts and switches to process\npacket (lookup header, compute checksums, etc.)\n- Transmission delay\n- Time spent sending packet of size S bits over link(s)\n- On a given link of rate R bits/s, transmission delay =\nS/R sec\n- Queueing delay\n- Time spent waiting in queue\n- Variable\n- Whose mean can be calculated from Little's law\n6.02 Fall 2012\nLecture 17, Slide #21\nCircuit v. Packet Switching\nLittle's Law\nn(t) = # pkts at time t in queue\nH\nG\nH\nD\nF\nG\nH\nC D\nE\nF\nG H\nB\nC\nD\nE\nF G H\nA\nB\nC\nD\nE F G H\nA\nB\nC\nD E F GH\nT\nt\n-\nP packets are forwarded in time T (assume T large)\n-\nRate = A = P/T\n-\nLet A = area under the n(t) curve from 0 to T\n-\nMean number of packets in queue = N = A/T\n- A is aggregate delay weighted by each packet's time in queue.\nSo, mean delay D per packet = A/P\n-\nTherefore, N = AD < Little's Law\n-\nFor a given link rate, incr easing queue size increases delay\n6.02 Fall 2012\nLecture 17, Slide #22\nPlan for Rest of 6.02\nCircuit switching\nPacket Switching\nGuaranteed rate\nNo guarantees (best ef fort)\nLink capacity wasted if data\nMore efficient\nis bursty\nBefore sending data\nSend data immediately\nestablishes a path\nAll data in a single flow\nDifferent packets might\nfollow one path\nfollow dif ferent paths\nNo reordering; constant\nPackets may be reordered,\ndelay; no dropped packets\ndelayed, or dropped\nSharing a common medium (MAC protocols)\nHow to find paths between any two end points? (Routing)\nHow to communicate infor mation reliably? (T ransport)\n6.02 Fall 2012\nLecture 17, Slide #23\n&RXUWHV\\RI,QWHUQHW1HWZRUN12&8VHGZLWKSHUPLVVLRQ\nLecture 17, Slide #24\n\n0,72SHQ&RXUVH:DUH\nKWWSRFZPLWHGX\nIntroduction to EECS II: Digital Communication Systems\n)DOO\n)RULQIRUPDWLRQDERXWFLWLQJWKHVHPDWHULDOVRURXU7HUPVRI8VHYLVLWKWWSRFZPLWHGXWHUPV"
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 18: MAC protocols",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/a352bb61a321060b1a821e145f394196_MIT6_02F12_lec18.pdf",
      "content": "11/14/12\n6.02 Fall 2012\nLecture 18, Slide #1\n6.02 Fall 2012\nLecture #18\n- Shared medium Media access (MAC) protocol\n- Time division multiplexing\n- Contention protocols: Aloha\n6.02 Fall 2012\nLecture 18, Slide #2\nShared Media Networks\n\n6 02 F ll 20122\n\nShared Communications Channels\n- Basic idea in its simplest form: avoid collisions between\ntransmitters - collision occurs if transmissions are concurrent\n- Wanted: a communications protocol (\"rules of engagement\")\nthat ensures \"good performance\"\n- Nodes may all hear each other perfectly, or not at all, or\npartially\n6.02 Fall 2012\nLecture 18, Slide #3\nShared channel, e.g., wireless or cable\nchannel\ninterface\npacket\nqueues\n6.02 Fall 2012\nLecture 18, Slide #4\n\"Good Performance\": What are the Metrics?\n- High utilization\n- Channel capacity is a limited resource use it efficiently\n- Ideal: use 100% of channel capacity in transmitting packets\n- Waste: idle periods, collisions, protocol overhead\n- Fairness\n- Divide capacity equally among requesters\n- But not every node is requesting all the time...\n- Bounded wait\n- An upper bound on the wait before successful transmission\n- Important for isochronous communications (e.g., voice/video)\n- Dynamism and scalability\n- Accommodate changing number of nodes, ideally without\nchanging implementation of any given node\n- Not all protocols do well on all these metrics\nIndividual images (c) source unknown. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\n11/14/12\n\nUtilization\n-\nUtilization measures the throughput of a channel:\n\ntotal throughput over all nodes\nUchannel =\n\nmaximum data rate of channel\n\n-\nExample: 10 Mbps channel, four nodes get throughputs of 1,\n2, 2 and 3 Mbps. So utilization is (1+2+2+3)/10 = 0.8.\n-\n0 ≤ U ≤ 1. Utilization can be less than 1 if\n- The nodes have packets to transmit (nodes with packets in their\ntransmit queues are termed backlogged), but the protocol is\ninefficient.\n- There is insufficient offered load, i.e., there aren't enough\npackets to transmit to use the full capacity of the channel.\n-\nWith backlogged nodes, perfect utilization is easy: just let one\nnode transmit all the time! But that wouldn't be fair...\n6.02 Fall 2012\nLecture 18, Slide #5\nFairness\n-\nMany plausible definitions. A standard recipe:\n- Measure throughput of nodes = xi, over a given time interval\n- Say that a distribution with lower standard deviation is \"fairer\"\nthan a distribution with higher standard deviation.\n- Given number of nodes, N, fairness F is defined as\n\nx\n\n∑\nN\ni\n\nF =\ni=1\n(\n)\n\n∑\nN\ni\nN\nx\ni=1\n\n-\n1/N ≤ F ≤ 1, where F=1/N implies single node gets all the\nthroughput and F=1 implies perfect fairness.\n-\nWe'll see that there is often a tradeoff between fairness and\nutilization, i.e., fairness mechanisms often impose some\noverhead, reducing utilization\n6.02 Fall 2012\nLecture 18, Slide #6\n\nChannel Sharing Protocols\n-\nProtocol \"rules of engagement\" for good performance\n- Known as media access control (MAC) or multiple access control\n-\nTime division\n- Share time \"slots\" between requesters\n- Prearranged: time division multiple access (TDMA)\n- Not prearranged: contention protocols (e.g., Alohanet).\n-\nFrequency division\n- Give each transmitter its own frequency, receivers choose\n\"station\"\n- Cf. lab for PS 6 - use different carrier frequencies & recv filters\n-\nCode division\n- Uses unique orthogonal pseudorandom code for each transmitter\n- Channel adds transmissions to create combined signal\n- Receiver listens to one \"dimension\" of combined signal using dot\nproduct of code with combined signal\n- Not covered in 6.02\n6.02 Fall 2012\nLecture 18, Slide #7\n6.0\nAbstraction for Shared Medium\nTime is divided into slots of equal length\nEach node can start transmission of a packet only at the\nbeginning of a time slot\nAll packets are of the same size and hence take the same\namount of time to transmit, equal to some integral multiple of\ntime slots.\nIf the transmissions of two or more nodes overlap, they are\nsaid to collide and none of the packets are received correctly.\nNote that even if the collision involves only part of the packet,\nthe entire packet is assumed to be lost.\nTransmitting nodes can detect collisions, which usually\nmeans they'll retransmit that packet at some later time.\nEach node has a queue of packets awaiting transmission. A\nnode with a non-empty queue is said to be backlogged.\nDepending on context, nodes may hear each other perfectly (eg, Ethernet), or\nnot at all (e.g., satellite ground stations), or partially (e.g., WiFi devices or cell\nphones). For now, assume all nodes want to send packets to a fixed\n\"master\" (eg, base station)\n2 Fall 2012\nLecture 18, Slide #8\n-\n-\n-\n-\n-\n-\n-\n\n11/14/12\nTime Division Multiple\n\nAccess (TDMA)\n-\nSuppose that there is a centralized resource allocator and a\nway to ensure time synchronization between the nodes - for\nexample, a cellular base station.\n\n-\nFor N nodes, give each node a unique index in the range\n[0,N-1]. Assume each slot is numbered starting at 0.\n\n-\nNode i gets to transmit in time slot t if, and only if, t mod N =\ni. So a particular node transmits once every N time slots.\n\n-\nNo packet collisions! But unused time slots are \"wasted\",\nlowering utilization. Poor when nodes send data in bursts or\nhave different offered loads.\n6.02 Fall 2012\nLecture 18, Slide #9\n6.02 Fall 2012\nLecture 18, Slide #10\nTDMA\n\nfor GSM Phones\nFirst slot is used in cell\nphones to contact tower\nfor slot assignment.\nTower can determine\nappropriate timing\nadvance for each user\n(accounts for varying\ndistance from tower) so\nthat transmissions wont\noverlap at the tower.\nThe Aloha Protocol\n- Context: Norm Abramson, Hawaii\n- Developed scheme to connect islands via\nsatellite network\n6.02 Fall 2012\nLecture 18, Slide #11\nContention Procotols:\n\nAloha (Simplest Example)\nTo improve performance when there are burst data patterns or\nskewed loads, use a contention protocol where allocation is not\npre-determined.\nAlohanet, designed by Norm\nAbramson et al. (Hawaii), was\na satellite-based data network\nconnecting computers on the\nHawaiian islands. One\nfrequency was used to send\ndata to the satellite, which\nrebroadcast it on a different\nfrequency to be received by all\nstations.\n\nStations could only \"hear\" the\nsatellite, so had to decide\nindependently when it was\ntheir turn to transmit.\n6.02 Fall 2012\nLecture 18, Slide #12\n(c) Mozzerati (top), source unknown (bottom). CC BY-SA. This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/fairuse .\nSatellite clip art is in\nthe public domain.\nSatellite clip art is in\nthe public domain.\nMap (c) Europa Technologies, TerraMetrics, Google, and NASA. All rights reserved. This content is\nexcluded from our Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\nMap (c) Europa Technologies, TerraMetrics, Google, and NASA. All\nrights reserved. This content is excludedfrom our Creative Comm-\nons license. For more information, see http://ocw.mit.edu/fairuse\n.\n\n11/14/12\n6.02 Fall 2012\nLecture 18, Slide #13\ntime slot\nCollision\nChannel idle\n\nSuccess, Idleness, Collisions\n- Throughput = Uncollided packets per time interval\n- Utilization of the above pic = Throughput / Channel Rate\n= 13/20 = .65\n\nSlotted Aloha\n-\nAloha protocol - each node independently implements:\n\nIf a node is backlogged, it sends a packet\nin the next time slot with probability p.\n\n-\nAssume (for now) each packet takes exactly one time slot to\ntransmit (slotted Aloha)\n-\nUtilization when N nodes are backlogged? The probability\nthat exactly one node sends a packet.\n- prob(send a packet) = p\n- prob(don't send a packet) = 1-p\n- prob(only one sender) = p(1-p)N-1\n- There are (N choose 1) = N ways to choose the one sender\nU\n= Np(1-p)N-1\nslotted Aloha\n6.02 Fall 2012\nLecture 18, Slide #14\ng\n\nMaximizing Utilization\nTo determine maximum:\nset dU/dp = 0, solve for p.\n\nResult: p = 1/N, so\nUmax = (1-\n)N-1\nN\nAs N →inf, Umax →\n≈37%\ne\n⎛\n⎜\n-1⎞\nln (1-\n)N ⎟= (N -1)ln(1-\n)\n⎝\nN\n⎠\nN\n= (N -1)(-\n-2N 2 -3N 3 -...)\nN\n= -1+\n+\n+\nN\n6N 2 +\n...\n12N 3\n= -1 as N →inf\n6.02 Fall 2012\nLecture 18, Slide #15\n6.02 Fall 2012\nLecture 18, Slide #16\nSimulation of Slotted Aloha (N=10)\nUtilization = .38, Fairness = .98\nTop: success\nBottom: failure\n\n11/14/12\n\nStabilization: Selecting the Right p\n-\nSetting p = 1/N maximizes utilization, where N is the number\nof backlogged nodes.\n-\nWith bursty traffic or nodes with unequal offered loads (aka\nskewed loads), the number of backlogged is constantly\nvarying.\n-\nIssue: how to dynamically adjust p to achieve maximum\nutilization?\n- Detect collisions by listening, or by missing acknowledgement\n- Each node maintains its own estimate of p\n- If collision detected, too much traffic, so decrease local p\n- If success, maybe more traffic possible, so increase local p\n-\n\"Stabilization\" is, in general, the process of ensuring that a\nsystem is operating at, or near, a desired operating point.\n- Stabilizing Aloha: finding a p that maximizes utilization as\nloading changes.\n6.02 Fall 2012\nLecture 18, Slide #17\n6.02 Fall 2012\nLecture 18, Slide #18\n\nBinary Exponential Backoff\n\nDecreasing p on collision\n- Estimate of N (# of backlogged nodes) too low, p too high\n- To quickly find correct value use multiplicative decrease:\np p/2\n- k collisions in a row: p decreased by factor of 2k\n- Binary: 2, exponential: k, back-off: smaller p more time\nbetween tries\n\nIncreasing p on success\n- While we were waiting to send, other nodes may have emptied\ntheir queues, reducing their offered load.\n- If increase is too small, slots may go idle\n- Try multiplicative increase: p min(2*p,1)\n- Or maybe just: p 1 to ensure no slots go idle\n-\n-\n6.02 Fall 2012\nLecture 18, Slide #19\nSimulation of Stabilized\n\nAloha\nSome nodes did well\nOthers didn't\nUtilization = .33, Fairness = .47\npython PS7_stabaloha.py -r -n 10 -t 1000\n6.02 Fall 20\n\nNode probabilities over time\n(different run from prev\n\nious page)\nY-axis is per-node transmission probability\n12 Bottom panel: per-node throughput Lecture 18, Slide #20\n\n11/14/12\nWhat Went Wrong?\n\nny successive failures p very small no xmit atte\n-\nStarvation\n- Too ma\nmpts\n- Result: significant long-term unfairness\n- Try a reduction rule with a lower bound: p max(pmin, p/2)\n- Choosing pmin << 1/max(N) seems to work best\n-\nBut there's another problem: capture effect\nSome node \"captures\" the\nnetwork for a period, starving\nthe others...\n\nSignificant short-term\nunfairness!\nUtilization = .71, Fairness = .99\npython PS7_stabaloha.py -r -n 6 -t 1000 --pmin=.05\n6.02 Fall 2012\nLecture 18, Slide #21\n\nLimiting the Capture Effect\n-\nCapture effect\n- A successful node maintains a high p (avg. near 1)\n- Starves out other nodes for short periods\n- Try an incr\ny\npp\nease rule with an upper bound: p min(p\n(pmax\nmax,\np)\n,2*p)\nUtilization = .41, Fairness = .99\npython PS7_stabaloha.py -r -n 10 -t 10000 --pmin=.05 --pmax=0.8\n6.02 Fall 2012\nLecture 18, Slide #22\nx\n\nNode Probabilities: pmax=.25, pmin=.01\n(different run from prev\n\nious page)\nY-axis is per-node transmission probability\n6.02 Fall 2012 Bottom panel: per-node throughput Lecture 18, Slide #23\n6.02 Fall 2012\nLecture 18, Slide #24\nAloha in Pictures: Collisions\nCollisions! Packet is 5 slots long in this picture\n- A collision occurs when multiple\ntransmissions overlap in time (even partially)\n- Throughput = Uncollided packets per second\n- Utilization = Throughput / Channel Rate\n\n11/14/12\nDear Professor Balakrishnan,\n\nI just wanted to let you know how my IAP has been going because\nI've been using a few 6.02 concepts. I'm interning at Quizlet.\n\n... I also worked on making a testing tool. I set up a node server to\nmanage many headless browsers (using phantomjs). I realized that\nphantomjs wasn't built to manage the number of connections I\nrequired. Then I noticed that this was similar to the problem of\nconnecting multiple clients to the same router. So I set up a syste\nsimilar to one of the 6.02 labs in which everyone was assigned a\nrandom time to start their connection, over a period of time. When\nbrowser started initialising, I put on a lock that made other\nbrowsers attempting to connect wait another random time period.\nOnce the \"messy\" part of the initialisation was done, I unset the loc\nin order to allow other clients to connect.\n\nI had to implement various other 6.02ish features like exponential\nback off, and I've also noticed that node.js is (of course) a very 6.02\ntype of project. For example, it has a concept of heartbeats.\n\nBest, Chase (Fall 2011 student that sat in the front row and whom\nyou approached at the gym)\n6.02 Fall 2012\nLecture 18, Slide #25\nm\na\nk\n\nSpot Quiz\n1. In Aloha, each node maintains a variable, p. What does p\nrepresent? [P(x) is \"probability of event x\"]\nA.\nP(node being backlogged)\nB.\nP(backlogged node sends in a timeslot)\nC.\nP(packet transmission is received correctly)\nD. P(time slot is kept idle)\n2. In stabilized Aloha, the value of p never goes below pmin.\nWhy should pmin not be too small?\nA.\nTo increase the utilization\nB.\nTo avoid extreme unfairness\nC.\nTo reduce the problems caused by the \"capture effect\"\nD. To reduce the number of collisions\n3. Slotted Aloha, packet size 1 slot, N backlogged nodes, each\nnode has a fixed p. Calculate: P(collision in a timeslot)\n\n6.02 Fall 2012\nLecture 18, Slide #26\n\nSpot Quiz\n1. In Aloha, each node maintains a variable, p. What does p\nrepresent? [P(x) is \"probability of event x\"]\nA.\nP(node being backlogged)\nB.\nP(backlogged node sends in a timeslot)\nC.\nP(packet transmission is received correctly)\nD. P(time slot is kept idle)\n2. In stabilized Aloha, the value of p never goes below pmin.\nWhy should pmin not be too small?\nA.\nTo increase the utilization\nB.\nTo avoid extreme unfairness\nC.\nTo reduce the problems caused by the \"capture effect\"\nD. To reduce the number of collisions\n3. Slotted Aloha, packet size 1 slot, N backlogged nodes, each\nnode has a fixed p. Calculate: P(collision in a timeslot)\nSoln: P(collision) = P(≥2 xmits) = 1 - P(no xmit) - P(1 xmit) =\n\nP(no xmit) = (1-p)N; P(1 xmit) = Np(1-p)(N-1)\nTherefore, P(collision) = 1 - (1-p)N - Np(1-p)(N-1).\n6.02 Fall 2012\nLecture 18, Slide #27\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.02 Lecture 9: Transmitting on a physical channel",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/56d958fded205681d34fab3eac4b7ab5_MIT6_02F12_chap09.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nFall 2011 (Last update: September 30, 2012)\nCHAPTER 9\nNoise\nLiars, d---d liars, and experts.\n--possibly Judge George Bramwell (quoted in 1885), expressing his opinion of\nwitnesses\nThere are three kinds of lies: lies, damned lies, and statistics.\n--definitely Mark Twain (writing in 1904), in a likely misattribution to Ben\njamin Disraeli\nGod does not play dice with the universe.\n--Albert Einstein, with probability near 1\nIn general, many independent factors affect a signal received over a channel. Those\nthat have a repeatable, deterministic effect from one transmission to another are generally\nreferred to as distortion. We shall examine a very important class of distortions--those\ninduced by linear, time-invariant channels--in later chapters. Other factors have effects that\nare better modeled as random, and we collectively refer to them as noise. Communication\nsystems are no exception to the general rule that any system in the physical world must\ncontend with noise. In fact, noise is a fundamental aspect of all communication systems.\nIn the simplest binary signaling scheme--which we will invoke for most of our pur\nposes in this course--a communication system transmits one of two voltages, mapping a\n\"0\" to the voltage V0 and mapping a \"1\" to V1. The appropriate voltage is held steady over\na fixed-duration time slot that is reserved for transmission of this bit, then moved to the\nappropriate voltage for the bit associated with the next time slot, and so on. We assume\nin this chapter that any distortion has been compensated for at the receiver, so that in an\nideal noise-free case the receiver ends up measuring V0 in any time slot corresponding to\na \"0\", and V1 in any slot corresponding to a \"1\".\nIn this chapter we focus on the case where V1 = Vp > 0 and V0 = -Vp, where Vp is\nsome fixed positive voltage, typically the peak voltage magnitude that the transmitter is\ncapable of imposing on the communication channel. This scheme is sometimes referred\nto as bipolar signaling or bipolar keying. Other choices of voltage levels are possible, of\ncourse.\n\nCHAPTER 9. NOISE\nIn the presence of noise, the receiver measures a sequence of voltage samples y[k] that\nis unlikely to be exactly V0 or V1. To deal with this variation, we described in the previous\nchapter a simple and intuitively reasonable decision rule, for the receiver to infer whether\nthe bit transmitted in a particular time slot was a \"0\" or a \"1\". The receiver first chooses a\nsingle voltage sample from the sequence of received samples within the appropriate time\nslot, and then compares this sample to a threshold voltage Vt. Provided \"0\" and \"1\" are\nequally likely to occur in the sender's binary stream, it seems reasonable that we should\npick as our threshold the voltage that \"splits the difference\", i.e., use Vt = (V0 + V1)/2.\nThen, assuming V0 < V1, return \"0\" as the decision if the received voltage sample is smaller\nthan Vt, otherwise return \"1\".\nThe receiver could also do more complicated things; for example, it could form an av\nerage or a weighted average of all the voltage samples in the appropriate time slot, and\nthen compare this average with the threshold voltage Vt. Though such averaging leads in\ngeneral to improved performance, we focus on the simpler scheme, where a single well-\nselected sample in the time slot is compared with Vt. In this chapter we will analyze the\nperformance of this decision rule, in terms of the probability of an incorrect decision at the\nreceiver, an event that would manifest itself as a bit error at the receiver.\nThe key points of this chapter are as follows:\n1. A simple model--and often a good model--for the net effect at the receiver of noise\nin the communication system is to assume additive, Gaussian noise. In this model,\neach received signal sample is the sum of two components. The first component is\nthe deterministic function of the transmitted signal that would be obtained in the ab\nsence of noise. (Throughout this chapter, we will assume no distortion in the chan\nnel, so the deterministic function referred to here will actually produce at the receiver\nexactly the same sample value transmitted by the sender, under the assumption of\nno noise.) The second component is the noise term, and is a quantity drawn from\na Gaussian probability distribution with mean 0 and some variance, independent of\nthe transmitted signal. The Gaussian distribution is described in more detail in this\nchapter.\nIf this Gaussian noise variable is also independent from one sample to another, we\ndescribe the underlying noise process as white Gaussian noise, and refer to the noise\nas additive white Gaussian noise (AWGN); this is the case we will consider. The origin\nof the term \"white\" will become clearer when we examine signals in the frequency\ndomain, later in this course. The variance of the zero-mean Gaussian noise variable\nat any sample time for this AWGN case reflects the power or intensity of the un\nderlying white-noise process. (By analogy with what is done with electrical circuits\nor mechanical systems, the term \"power\" is generally used for the square of a signal\nmagnitude. In the case of a random signal, the term generally denotes the expected or\nmean value of the squared magnitude.)\n2. If the sender transmitted a signal corresponding to some bit, b, and the receiver mea\nsured its voltage as being on the correct side of the threshold voltage Vt, then the bit\nwould be received correctly. Otherwise, the result is a bit error. The probability of\na bit error is an important quantity, which we will analyze. This probability, typi\ncally called the bit error rate (BER), is related to the probability that a Gaussian ran\n\nSECTION 9.1. ORIGINS OF NOISE\ndom variable exceeds some level; we will calculate it using the probability density\nfunction (PDF) and cumulative distribution function (CDF) of a Gaussian random\nvariable. We will find that, for the bipolar keying scheme described above, when\nused with the simple threshold decision rule that was also specified above, the BER\nis determined by the ratio of two quantities: (i) the power or squared magnitude, V 2 ,\np\nof the received sample voltage in the noise-free case; and (ii) the power of the noise\nprocess. This ratio is an instance of a signal-to-noise ratio (SNR), and such ratios are\nof fundamental importance in understanding the performance of a communication\nsystem.\n3. At the signal abstraction, additive white Gaussian noise is often a good noise model.\nAt the bit abstraction, this model is inconvenient because we would have to keep\ngoing to the signal level to figure out exactly how it affects every bit. Fortunately, the\nBER allows us to think about the impact of noise in terms of how it affects bits. In\nparticular, a simple, but powerful, model at the bit level is that of a binary symmetric\nchannel (BSC). Here, a transmitted bit b (0 or 1) is interpreted by the receiver as\n1 - b with probability pe and interpreted as b with probability 1 - pe, where pe is the\nprobability of a bit error (i.e., the bit error rate). In this model, each bit is corrupted\nindependently of the others, and the probability of corruption is the same for all bits\n(so the noise process is an example of an \"iid\" random process: \"independent and\nidentically distributed\").\n- 9.1\nOrigins of noise\nA common source of noise in radio and acoustic communications arises from interfer\ners who might individually or collectively make it harder to pick out the communication\nthat the receiver is primarily interested in. For example, the quality of WiFi communi\ncation is affected by other WiFi communications in the same frequency band (later in the\ncourse we will develop methods to mitigate such interference), an example of intereference\nfrom other users or nodes in the same network. In addition, interference could be caused\nby sources external to the network of interest; WiFi, for example, if affected by cordless\nphones, microwave ovens, Bluetooth devices, and so on that operate at similar radio fre\nquencies. Microwave ovens are doubly troublesome if you're streaming music over WiFi,\nwhich in the most common mode runs in the 2.4 GHz frequency band today--not only\ndo microwave ovens create audible disturbances that affect your ability to listen to music,\nbut they also radiate power in the 2.4 GHz frequency band. This absorption is good for\nheating food, but leakage from ovens interferes with WiFi receptions! In addition, wireless\ncommunication networks like WiFi, long-range cellular networks, short-range Bluetooth\nradio links, and cordless phones all suffer from fading, because users often move around\nand signals undergo a variety of reflections that interfere with each other (a phenomenon\nknown as \"multipath fading\"). All these factors cause the received signal to be different\nfrom what was sent.\nIf the communication channel is a wire on an integrated circuit, the primary source of\nnoise is capacitive coupling between signals on neighboring wires. If the channel is a wire\non a printed circuit board, signal coupling is still the primary source of noise, but coupling\nbetween wires is largely inductive or carried by unintended electromagnetic radiation.\n\nCHAPTER 9. NOISE\nIn both these cases, one might argue that the noise is not truly random, as the signals\ngenerating the noise are under the designer's control. However, a signal on a wire in an\nintegrated circuit or on a printed circuit board will frequently be affected by signals on\nthousands of other wires, so approximating the interference using a random noise model\nturns out to work very well.\nNoise may also arise from truly random physical phenomena. For example, electric\ncurrent in an integrated circuit is generated by electrons moving through wires and across\ntransistors. The electrons must navigate a sea of obstacles (atomic nuclei), and behave\nmuch like marbles traveling through a Pachinko machine. They collide randomly with\nnuclei and have transit times that vary randomly. The result is that electric currents have\nrandom noise. In practice, however, the amplitude of the noise is typically several orders\nof magnitude smaller than the nominal current. Even in the interior of an integrated cir\ncuit, where digital information is transported on micron-wide wires, the impact of electron\ntransit time fluctuations is negligible. By contrast, in optical communication channels, fluc\ntuations in electron transit times in circuits used to convert between optical and electronic\nsignals at the ends of the fiber are the dominant source of noise.\nTo summarize: there is a wide variety of mechanisms that can be the source of noise;\nas a result, the bottom line is that it is physically impossible to construct a noise-free channel.\nBy understanding noise and analyzing its effects (bit errors), we can develop approaches\nto reducing the probability of errors caused by noise and to combat the errors that will\ninevitably occur despite our best efforts. We will also learn in a later chapter about a cele\nbrated and important result of Shannon: provided the information transmission rate over\na channel is kept below a limit referred to as the channel capacity (determined solely by the\ndistortion and noise characteristics of the channel), we can transmit in a way that makes\nthe probability of error in decoding the sender's message vanish asymptotically as the\nmessage size goes to inf. This asymptotic performance is attained at the cost of increas\ning computational burden and increasing delay in deducing the sender's message at the\nreceiver. Much research and commercial development has gone into designing practical\nmethods to come close to this \"gold standard\".\n- 9.2\nAdditive White Gaussian Noise: A Simple but Powerful\nModel\nWe will posit a simple model for how noise affects the reception of a signal sent over a\nchannel and processed by the receiver. In this model, noise is:\n1. Additive: Given a received sample value y[k] at the kth sample time, the receiver\ninterprets it as the sum of two components: the first is the noise-free component y0[k],\ni.e., the sample value that would have been received at the kth sample time in the\nabsence of noise, as a result of the input waveform being passed through the channel\nwith only distortion present; and the second is the noise component w[k], assumed\nindependent of the input waveform. We can thus write\ny[k] = y0[k] + w[k] .\n(9.1)\n\nSECTION 9.2. ADDITIVE WHITE GAUSSIAN NOISE: A SIMPLE BUT POWERFUL MODEL\nIn the absence of distortion, which is what we are assuming here, y0[k] will be either\nV0 or V1.\n2. Gaussian: The noise component w[k] is random, but we assume it is drawn at each\nsample time from a fixed Gaussian distribution; for concreteness, we take this to be\nthe distribution of a Gaussian random variable W, so that each w[k] is distributed\nexactly as W is. The reason why a Gaussian makes sense is because noise is often\nthe result of summing a large number of different and independent factors, which\nallows us to apply an important result from probability and statistics, called the cen\ntral limit theorem. This states that the sum of independent random variables is well\napproximated (under rather mild conditions) by a Gaussian random variable, with\nthe approximation improving as more variables are summed in.\nThe Gaussian distribution is beautiful from several viewpoints, not least because it is\ncharacterized by just two numbers: its mean μ, and its variance σ2 or standard deviation\nσ. In our noise model, we will assume that the mean of the noise distribution is 0.\nThis assumption is not a huge concession: any consistent non-zero perturbation is\neasy to compensate for. For zero-mean Gaussian noise, the variance, or equivalently\nthe standard deviation, completely characterizes the noise. The standard deviation σ\nmay be thought of as a measure of the expected \"amplitude\" of the noise; its square\ncaptures the expected power.\nFor noise not to corrupt the digitization of a bit detection sample, the distance be\ntween the noise-free value of the sample and the digitizing threshold should be suf\nficiently larger than the expected amplitude--or standard deviation--of the noise.\n3. White: This property concerns the temporal variation in the individual noise sam\nples that affect the signal. If these Gaussian noise samples are independent from\none sample to another, the underlying noise process is referred to as white Gaussian\nnoise. \"White\" refers to the frequency decomposition of the sequence of noise sam\nples, and essentially says that the noise signal contains components of equal expected\npower at all frequencies. This statement will become clearer later in the course when\nwe talk about the frequency content of signals.\nThis noise model is generally given the term AWGN, for additive white Gaussian noise.\nWe will use this term.\n-\n9.2.1\nEstimating the Noise Parameters\nIt is often of interest to estimate the noise parameters from measurements; in our Gaussian\nmodel, these are the parameters μ and σ2 . If we simply transmit a sequence of \"0\" bits,\ni.e., hold the voltage V0 at the transmitter, and observe the received samples y[k] for k =\n0,1,...,K - 1, we can process these samples to obtain the statistics of the noise process\nfor additive noise. Under the assumption of no distortion, and constant (or \"stationary\")\nnoise statistics, and noise samples w[k] = y[k] - V0 that are independent from one sampling\ninstant to another, we can use the sample mean m to estimate μ, where\nK-1\n\nm =\nw[k] .\n(9.2)\nK k=0\n\nCHAPTER 9. NOISE\nThe law of large numbers from probability and statistics ensures that as K tends to inf, the\nsample mean m converges to μ, which we have assumed is 0.\nWith μ = 0, the quantity that is more indicative of the power of the noise is the variance\nσ2, which can be estimated by the sample variance s2, given by\nK-1\ns =\n(w[k] - m) .\n(9.3)\nK k=0\nAgain, this converges to σ2 as K tends to inf.\n-\n9.2.2\nThe Gaussian Distribution\nLet us now understand the Gaussian distribution in the context of our physical commu\nnication channel and signaling process. In our context, the receiver and sender both deal\nwith voltage samples. The sample y[k] at the receiver has a noise term, w[k], contributing\nto it additively, where w[k] is obtained from the following probability density function (PDF),\nwhich specifies a Gaussian distribution:\n(w-μ)2\n-\nfW (w) = √\ne\n2σ2\n.\n(9.4)\n2πσ2\nFor zero-mean noise, μ = 0.\nThe PDF fW (w), which is assumed to govern the distribution of all the noise samples\nw[k], specifies the probability that W , or equivalently w[k], takes values in the vicinity of\nw. Specifically,\nP(w ≤ w[k] ≤ w + dw) ≈ fW (w) dw .\nMore generally, the probability that w[k] is between two values w1 and w2 is given by\nw2\nP(w1 < w[k] ≤ w2) =\nfW (w) dw .\nw1\nThe reason we use the PDF rather than a discrete histogram is that our noise model is\ninherently \"analog\", taking on any real value in (-inf, inf). For a noise sample that can take\non any value in a continuous range, the natural mathematical tool to use is a continuous-\ndomain random variable, described via its PDF, or via the integral of the PDF, which is\ncalled the cumulative distribution function (CDF).\nIt will be helpful to review the basic definitions and properties of continuous-domain\nrandom variables, especially if you aren't comfortable with these tools. We have provided\na brief recap and tutorial in the appendix near the end of this chapter (§9.6).\n- 9.3\nBit Errors\nNoise disrupts the quality of communication between sender and receiver because the re\nceived noisy voltage samples can cause the receiver to incorrectly identify the transmitted\nbit, thereby generating a bit error. If we transmit a long stream of known bits and count\nthe fraction of received bits that are in error, we obtain a quantity that--by the law of large\nnumbers--asymptotically approaches the bit error rate (BER), which is the probability that\nX\n\nSECTION 9.3. BIT ERRORS\nany given bit is in error, P(error). This is the probability that noise causes a transmitted \"1\"\nto be reported as \"0\" or vice versa.\nCommunication links exhibit a wide range of bit error rates. At one end, high-speed\n(multiple gigabits per second) fiber-optic links implement various mechanisms that reduce\nthe bit error rates to be as low as 1 in 1012 . This error rate looks exceptionally low, but a\nlink that can send data at 10 gigabits per second with such an error rate will encounter a bit\nerror every 100 seconds of continuous activity, so it does need ways of masking errors that\noccur. Wireless communication links usually have errors anywhere between 1 in 103 for\nrelatively noisy environments, down to to 1 in 107, and in fact allow the communication to\noccur at different bit rates; higher bit rates are usually also associated with higher bit error\nrates. In some applications, very noisy links can still be useful even if they have bit error\nrates as high as 1 in 103 or 102 .\nWe now analyze the BER of the simple binary signaling scheme. Recall the receiver\nthresholding rule, assuming that the sender sends V0 volts for \"0\" and V1 > V0 volts for\n\"1\" and that there is no channel distortion (so in the absence of noise, the receiver would\nsee exactly what the sender transmits):\nIf the received voltage sample y < Vt = (V0 + V1)/2 then the received bit is\nreported as \"0\"; otherwise, it is reported as \"1\".\nFor simplicity, we will assume that the prior probability of a transmitted bit being a \"0\"\nis the same as it being a \"1\", i.e., both probabilities are 0.5. We will find later that when\nthese two prior probabilities are equal, the choice of threshold Vt specified above is the one\nthat minimizes the overall probability of bit error for the decision rule that the receiver is\nusing. When the two priors are unequal, one can either stick to the same threshold rule\nand calculate the bit error probability, or one could calculate the threshold that minimizes\nthe error probability and then calculate the resulting bit error probability. We will deal\nwith that case in the next section.\nThe noise resilience of the binary scheme turns out to depend only on the difference\nV1 - V0, because the noise is additive. It follows that if the transmitter is constrained to\na peak voltage magnitude of Vp, then the best choice of voltage levels is V1 = Vp > 0 and\nV0 = -Vp, which corresponds to binary keying. The associated threshold is Vt = 0. This is\nthe case that we analyze now.\nAs noted earlier, it is conventional to refer to the square of a magnitude as the power, so\nV 2 is the power associated with each voltage sample at the receiver, under the assumption\np\nof no distortion, and in the ideal case of no noise. Summing the power of these samples\nover all T samples in the time slot associated with a particular bit sent over the link yields\nthe energy per transmitted bit , T · V 2 . It is thus reasonable to also think of V 2 as the\np\np\nsample energy, which we shall denote by Es. With this notation, the voltage levels in\n√\n√\nbipolar keying can be written as V1 = + Es and V0 = - Es.\nNow consider in what cases a bit is incorrectly decided at the receiver. There are two\nmutually exclusive possibilities:\n√\n1. The sender sends b = 0 at voltage - Es and the value received is > 0; or\n√\n2. The sender sends b = 1 at voltage + Es and the value received is < 0.\nFor a source that is equally likely to send 0's and 1's, and given the symmetry of a zero-\nmean Gaussian about the value 0, the two events mentioned above have exactly the same proba\n\nCHAPTER 9. NOISE\nbilities. Each one of the events has a probability that is half the probability of a zero-mean\n√\nGaussian noise variable W taking values larger than\nEs (the \"half\" is because the prob\nability of b = 0 is 0.5, and similarly for b = 1). Hence the probability of one or the other\nof these mutually exclusive events occurring, i.e., the probability of a bit error, is simply\nthe sum of these two probabilities, i.e., the BER is given by the probability of a zero-mean\n√\nGaussian noise variable W taking values larger than\nEs. The BER is therefore\ninf\n-w\nBER = P(error) = √\ne\n2/(2σ2) dw .\n(9.5)\n√\n2πσ2\nEs\nWe will denote 2σ2 by N0. It has already been mentioned that σ2 is a measure of the\nexpected power in the underlying AWGN process. However, the quantity N0 is also often\nreferred to as the noise power, and we shall use this term for N0 too.1\nAfter a bit of algebra, Equation (9.5) simplifies to\ninf\n-v\nBER = P(error) = √ · √\ne\ndv .\n(9.6)\nπ\nEs/N0\nThis equation specifies the tail probability of a Gaussian distribution, which turns out to\nbe important in many scientific and engineering applications. It's important enough to be\ntabulated using two special functions called the error function and the complementary error\nfunction, denoted erf(z) and erfc(z) = 1 - erf(z) respectively, and defined thus:\nz\n-v\nerf(z) = √\n·\ne\ndv ,\n(9.7)\nπ\nand\ninf\n-v\nerfc(z) = 1 - erf(z) = √\n·\ne\ndv .\n(9.8)\nπ\nz\nOne can now easily write the following important (and pleasingly simple) equation for\nthe BER of our simple binary signaling scheme over an AWGN channel:\nf\nEs\nBER = P(error) = erfc(\n).\n(9.9)\nN0\nEquation (9.9) is worth appreciating and perhaps even committing to memory (at least\nfor the duration of the course!). But it is more important to understand how we arrived\nat it and why it makes sense. The BER for our bipolar keying scheme with the specified\ndecision rule at the receiver is determined entirely by the ratio Es . The numerator of this\nN0\nratio is the power of the signal used to send a bit, or equivalently the power or energy\nEs of the voltage sample selected from the corresponding time slot at the receiver in the\nnoise-free case, assuming no distortion (as we are doing throughout this chapter). The\ndenominator of the ratio is the noise power N0 encountered during the reception of the\nsignal. This ratio is also commonly referred to as the signal-to-noise ratio (SNR) of the\n1The factor of 2 between the two uses of the term arises from the fact that under one notational convention\nthe distribution of expected noise power over frequency is examined over both negative and positive frequen\ncies, while under the other convention it is examined over just positive frequencies--but this difference is\nimmaterial for us.\nZ\nZ\nZ\nZ\n\nSECTION 9.4. BER: THE CASE OF UNEQUAL PRIORS\nSource: http://www.dsplog.com/2007/08/05/bit-error-probability-for-bpsk-modulation/.\nCourtesy of Krishna Sankar Madhavan Pillai. Used with permission.\nFigure 9-1: The BER of the simple binary signaling scheme in terms of the erfc function. The chart shows\nthe theoretical prediction and simulation results.\n\ncommunication scheme.\nThe greater the SNR, the lower the BER, and vice versa. Equation (9.9) tells us how the\ntwo quantities relate to one another for our case, and is plotted in Figure 9-1. The shape of\nthis curve is characteristic of the BER v. SNR curves for many signaling and channel coding\nschemes, as we will see in the next few chapters. More complicated signaling schemes will\nhave different BER-SNR relationships, but the BER will almost always be a function of the\nSNR.\n- 9.4\nBER: The Case of Unequal Priors\nWhen the prior probability of the sender transmitting a \"0\" is the same as a \"1\", the optimal\n√\ndigitizing threhold is indeed 0 volts, by symmetry, if a \"0\" is sent at - Es and a \"1\" at\n√\n+ Es volts. But what happens when a \"0\" is more likely than a \"1\", or vice versa?\nIf the threshold remains at 0 volts, then the probability of a bit error is the same as\nEquation (9.9). To see why, suppose the prior probability of a \"0\" is p0 and a \"1\" is p1 =\n1 - p0. Then, the probability of bit error can be simplified using a calculation similar to the\nprevious section to give us\np0\nJ\np1\nJ\nJ\nP(error) =\nerfc(\nEs/N0) +\nerfc(\nEs/N0) = erfc(\nEs/N0).\n(9.10)\nThis equation is the same as Equation (9.9). It should make intuitive sense: when the\nthreshold is 0 volts, the channel has the property that the probability of a \"0\" becoming a\n\"1\" is the same as the opposite flip. The probability of a \"0\" flipping depends only on the\nthreshold used and the signal-to-noise ratio, and not on p0 in this case.\n\nCHAPTER 9. NOISE\nNote, however, that when p0\n\n= p1 = 1/2, the optimal digitizing threshold is not 0 (or,\nin general, not half-way between the voltage levels used for a \"0\" and a \"1\"). Intuitively,\nif zeroes are more likely than ones, the threshold should actually be greater than 0 volts,\nbecause the odds of any given bit being 0 are higher, so one might want to \"guess\" that a\nbit is a \"0\" even if the voltage level were a little larger than 0 volts. Similarly, if the prior\nprobability of a \"1\" were larger, then the optimal threshold will be lower than 0 volts.\nSo what is the optimal digitizing threshold, assuming the receiver uses a single thresh\nold to decide on a bit? Let's assume it is Vt, then write an equation for the error probability\n(BER) in terms of Vt, differentiate it with respect to Vt, set the derivative to 0, and determine\nthe optimal value. One can then also verify the sign of the second derivative to establish\nthat the optimal value is indeed a minimum.\nFortunately, this calculation is not that difficult or cumbersome, because Vt will show\nup in the limit of the integration, so differentiation is straightforward. We will use the\nproperty that\ninf\nd\nd 2\n-v\n-z\nerfc(z) =\n√\ne\ndv = -√ e\n.\n(9.11)\ndz\ndz\nπ\nπ\nz\nThe equation for the BER is a direct extension of what we wrote earlier in Equation\n(9.10) to the case where we use a threshold Vt instead of 0 volts:\n√\n√\n(Vt +\n)\n(\n)\np0\nEs\np1\nEs - Vt\nP(error) =\nerfc\n√\n+\nerfc\n√\n.\n(9.12)\nN0\nN0\nUsing Equation (9.11) to differentiate the RHS of Equation (9.12) and setting it to 0, we\nget the following equation for Vt:\n√\n√\n-(Vt+ Es)2/N0 + p1e -(Vt- Es)2/N0\n-p0e\n= 0.\n(9.13)\nSolving Equation (9.13) gives us\nN0\np0\nVt = √\n· log\n.\n(9.14)\ne\n4 Es\np1\nIt is straightforward to verify by taking the second derivative that this value of Vt does\nindeed minimize the BER.\nOne can sanity check a few cases of Equation (9.14). When p0 = p1, we know the answer\nis 0, and we get that from this equation. When p0 increases, we know that the threshold\nshould shift toward the positive side, and vice versa, both of which follow from the equa\ntion. Also, when the noise power N0 increases, we expect the receiver to pay less atten\ntion to the received measurement and more attention to the prior (because there is more\nuncertainty in any received sample), and the expression for the threshold does indeed ac\ncomplish that, by moving the threshold further away from the origin and towards the side\nassociated with the less likely bit.\nNote that Equation (9.14) is for the case when a \"0\" and \"1\" are sent at voltages sym\nmetric about 0. If one had a system where different voltages were used, say V0 and V1,\nthen the threshold calculation would have to be done in analogous fashion. In this case,\nthe optimal value would be offset from the mid-point, (V0 + V1)/2.\nZ\n\nSECTION 9.5. UNDERSTANDING SNR\n10 10 α\nα\n-10\n0.1\n-20\n0.01\n-30\n0.001\n-40\n0.0001\n-50\n0.000001\n-60\n0.0000001\n-70\n0.00000001\n-80\n0.000000001\n-90\n0.0000000001\n-100\n0.00000000001\nFigure 9-2: The dB scale is a convenient log scale; α is the absolute ratio between two energy or power\nquantities in this table.\n- 9.5\nUnderstanding SNR\nThe SNR of a communication link is important because it determines the bit error rate;\nlater, we will find that an appropriate SNR also determines the capacity of the channel\n(the maximum possible rate at which communication can occur reliably). Because of the\nwide range of energy and power values observed over any communication channel (and\nalso in other domains), it is convenient to represent such quantities on a log scale. When\nmeasured as the ratio of two energy or power quantities, the SNR is defined on a decibel\nscale according to the following formula.\nLet α denote the ratio of two energy or power quantities, such as the energy per sample,\nEs, and the noise power, N0 = 2σ2. Then, we say that the decibel separation corresponding\nto this ratio α is\nSNRdb = 10 · log10 α.\n(9.15)\nFigure 9-2 shows some values on the dB scale. A convenient rule to remember is that 3\ndB is equivalent to a ratio of about 2, because log10 2 = 0.301.\nThe decibel (or dB) scale is widely used in many applications. It goes between -inf and\ninf, and succinctly captures ratios that may be multiple powers of ten apart. The online\n\nCHAPTER 9.\nNOISE\nFigure 9-3: Histograms become smoother and more continuous when they are made from an increasing\nnumber of samples. In the limit when the number of samples is infinite, the resulting curve is a probability\ndensity function.\nproblem set has some simple calculations to help you get comfortable with the dB scale.\n■\n9.6\nAppendix: A Brief Recap of Continuous Random Vari-\nables\nTo understand what a PDF is, let us imagine that we generate 100 or 1000 independent\nnoise samples and plot each one on a histogram. We might see pictures that look like the\nones shown in Figure 9-3 (the top two pictures), where the horizontal axis is the value of the\nnoise sample (binned) and the vertical axis is the frequency with which values showed up\nin each noise bin. As we increase the number of noise samples, we might see pictures as in\nthe middle and bottom of Figure 9-3. The histogram is increasingly well approximated by\na continuous curve. Considering the asymptotic behavior as the number of noise samples\nbecomes very large leads to the notion of a probability density function (PDF).\nFormally, let X be the random variable of interest, and suppose X can take on any\nvalue in (-inf,inf). Then, if the PDF of the underlying random variable is the non-negative\nfunction fX(x) ≥0, it means that the probability the random variable X takes on a value\nbetween x and x + dx, where dx is a small increment around x, is fX(x) dx. More generally,\nthe probability that a random variable X lies in the range (x1,x2] is given by\nx2\nP(x1 < X ≤x2) =\nfX(x) dx .\n(9.16)\nx1\n\nSECTION 9.6.\nAPPENDIX: A BRIEF RECAP OF CONTINUOUS RANDOM VARIABLES\nAn example of a PDF fX(x) is shown in Figure 9-5.\nThe PDF is by itself not a probability; the area under any portion of it is a probability.\nThough fX(x) itself may exceed 1, the area under any part of it is a probability, and can\nnever exceed 1. Also, the PDF is normalized to reflect the fact that the probability X takes\nsome value is always 1, so\ninf\nfX(x) dx = 1 .\n-inf\nMean\nThe mean μX of a random variable X can be computed from its PDF as follows:\nμX =\ninf\nxfX(x) dx.\n(9.17)\n-inf\nIf you think of the PDF as representing a \"probability mass\" distribution on the real axis,\nthen the mean is the location of its center of mass; pivoting the real axis at this point will\nallow the mass distribution to remain in balance, without tipping to one side or the other.\nThe law of large numbers states that if x[k] is an iid random process with the underlying\nPDF at each time being fX(x), then the sample mean converges to the the mean μX as the\nnumber of samples approaches inf:\nlim\nK→inf\nK-1\n\nx[k] = μX .\n(9.18)\nK k=0\nVariance\nThe variance is a measure of spread around the mean, and is defined by\ninf\nσ2\nX =\n\n(x -μX)2fX(x) dx .\n(9.19)\n-inf\n(To continue the mass analogy, the variance is analogous to the moment of inertia of the\nprobability mass. Probability mass that is further away from the center of mass on either\nside, i.e., further away from the mean, contributes significantly more to the variance than\nmass close to the mean.) Again, under appropriate conditions, the sample variance for an\niid process x[k] converges to the variance. The standard deviation is defined as the square\nroot of the variance, namely σX.\nCumulative distribution function\nThe integral of the PDF from -infto x,\nx\nFX(x) =\n\nfX(α) dα ,\n-inf\nis called the cumulative distribution function (CDF), because it represents the cumulative\nprobability that the random variable takes on a value ≤x. The CDF increases monotoni-\ncally (or, more precisely, is monotonically non-decreasing) from 0 when x = -infto 1 when\nx = inf.\nExample: Uniform distribution\nThis simple example may help illustrate the idea of a\nPDF better, especially for those who haven't see this notion before. Suppose that a random\n\nCHAPTER 9.\nNOISE\nFigure 9-4: PDF of a uniform distribution.\nFigure 9-5: PDF of a Gaussian distribution, aka a \"bell curve\".\nvariable X can take on any value between 0 and 2 with equal probability, and always lies\nin that range. What is the corresponding PDF?\nBecause the probability of X being in the range (x,x + dx) is independent of x as long\nas x is in [0,2], it must be the case that the PDF fX(x) is some constant, h, for x ∈[0,2].\nMoreover, it must be 0 for any x outside this range. We need to determine h. To do so,\nobserve that the PDF must be normalized, so\ninf\nfX(x) dx =\nh dx = 1,\n(9.20)\n-inf\nwhich implies that h = 0.5. Hence, fX(x) = 0.5 when 0 ≤x ≤2 and 0 otherwise. Figure 9-4\nshows this uniform PDF.\nOne can easily calculate the probability that an x chosen from this distribution lies in\nthe range (0.3,0.7). It is equal to\n0.7(0.5) dx = 0.2\n0.3\n.\nA uniform PDF also provides a simple example that shows how the PDF, fX(x), could\neasily exceed 1. A uniform distribution whose values are always between 0 and δ, for some\nδ < 1, has fX(x) = 1/δ, which is always larger than 1. To reiterate a point made before: the\nPDF fX(x) is not a probability, it is a probability density, and as such, could take on any non-\nnegative value. The only constraint on it is that the total area under its curve (the integral\nover the possible values it can take) is 1.\nAs an exercise, you might try to determine the PDF, mean and variance of a random\nvariable that is uniformly distributed in the arbitrary (but finite-length) interval [a,b].\n\nSECTION 9.6.\nAPPENDIX: A BRIEF RECAP OF CONTINUOUS RANDOM VARIABLES\nFigure 9-6: Changing the mean of a Gaussian distribution merely shifts the center of mass of the distribu-\ntion because it just shifts the location of the peak. Changing the variance widens the curve.\nExample: Gaussian distribution\nThe PDF for a Gaussian random variable X is given by\nfW (w) =\ne-(x-μX) /(2σ2 )\nX .\n(9.21)\n2πσ2\nX\nThis equation is plotted in Figure 9-5, which makes evident why a Gaussian distribution\nis colloquially referred to as a \"bell curve\". The curve tapers off to 0 rapidly because of\nthe e-x2 dependence. The form of the expression makes clear that the PDF is symmetric\nabout the value μX, which suffices to deduce that this parameter is indeed the mean of\nthe distribution. It is an exercise in calculus (which we leave you to carry out, if you are\nsufficiently interested in the details) to verify that the area under the PDF is indeed 1 (as it\nhas to be, for any PDF), and that the variance is in fact the parameter labeled as σ2\nX in the\nabove expression. Thus the Gaussian PDF is completely characterized by the mean and\nthe variance of the distribution.\nChanging the mean simply shifts the distribution to the left or right on the horizontal\naxis, as shown in the pictures on the left of Figure 9-6. Increasing the variance is more\ninteresting from a physical standpoint; it widens (or fattens) the distribution and makes it\nmore likely for values further from the mean to be selected, compared to a Gaussian with\na smaller variance. A Gaussian random variable with a wider distribution (i.e., a larger\nvariance) has more \"power\" compared to a narrower one.\n■\nAcknowledgments\nThanks to Bethany LaPenta and Kerry Xing for spotting various errors.\n\nCHAPTER 9.\nNOISE\n■\nProblems and Exercises\n1. The cable television signal in your home is poor. The receiver in your home is con-\nnected to the distribution point outside your home using two coaxial cables in series,\nas shown in the picture below. The power of the cable signal at the distribution point\nis P. The power of the signal at the receiver is R.\nP\n1st cable\nSignal attenuation = 7 dB\nR\n2nd cable\nSignal attenuation = 13 dB\nDistribution\npoint\nReceiver\nThe first cable attenuates (i.e., reduces) the signal power by 7 dB. The second cable\nattenuates the signal power by an additional 13 dB. Calculate P\nR as a numeric ratio.\n2. Ben Bitdiddle studies the bipolar signaling scheme from 6.02 and decides to extend\nit to a 4-level signaling scheme, which he calls Ben's Aggressive Signaling Scheme,\nor BASS. In BASS, the transmitter can send four possible signal levels, or voltages:\n(-3A,-A,+A,+3A), where A is some positive value. To transmit bits, the sender's\nmapper maps consecutive pairs of bits to a fixed voltage level that is held for some\nfixed interval of time, creating a symbol. For example, we might map bits \"00\" to\n-3A, \"01\" to -A, \"10\" to +A, and \"11\" to +3A. Each distinct pair of bits corresponds\nto a unique symbol. Call these symbols s minus3, s minus1, s plus1, and s plus3.\nEach symbol has the same prior probability of being transmitted.\nThe symbols are transmitted over a channel that has no distortion but does have ad-\nditive noise, and are sampled at the receiver in the usual way. Assume the samples at\nthe receiver are perturbed from their ideal noise-free values by a zero-mean additive\nwhite Gaussian noise (AWGN) process with noise intensity N0 = 2σ2, where σ2 is\nthe variance of the Gaussian noise on each sample. In the time slot associated with\neach symbol, the BASS receiver digitizes a selected voltage sample, r, and returns\nan estimate, s, of the transmitted symbol in that slot, using the following intuitive\ndigitizing rule (written in Python syntax):\ndef digitize(r):\nif r < -2A: s = s_minus3\nelif r < 0: s = s_minus1\nelif r < 2A: s = s_plus1\nelse: s = s_plus3\nreturn s\nBen wants to calculate the symbol error rate for BASS, i.e., the probability that the\nsymbol chosen by the receiver was different from the symbol transmitted. Note: we\nare not interested in the bit error rate here. Help Ben calculate the symbol error rate\nby answering the following questions.\n(a) Suppose the sender transmits symbol s plus3. What is the conditional sym-\nbol error rate given this information; i.e., what is P(symbol error | s plus3\n\nSECTION 9.6.\nAPPENDIX: A BRIEF RECAP OF CONTINUOUS RANDOM VARIABLES\nsent)? Express your answer in terms of A, N0, and the erfc function, defined\nas erfc(z) =\n√\ninf\nπ\n\ne\nz\n-x2 dx.\n(b) Now suppose the sender transmits symbol s plus1. What is the conditional\nsymbol error rate given this information, in terms of A, N0, and the erfc func-\ntion?\n(c) The conditional symbol error rates for the other two symbols don't need to be\ncalculated separately.\ni. The symbol error rate when the sender transmits symbol s minus3 is the\nsame as the symbol error rate of which of these symbols?\nA. s minus1.\nB. s plus1.\nC. s plus3.\nii. The symbol error rate when the sender transmits symbol s minus1 is the\nsame as the symbol error rate of which of these symbols?\nA. s minus3.\nB. s plus1.\nC. s plus3.\n(d) Combining your answers to the previous parts, what is the symbol error rate in\nterms of A, N0, and the erfc function? Recall that all symbols are equally likely\nto be transmitted.\n3. Bit samples are transmitted with amplitude ATX = ±1 (i.e.bipolar signaling). The\nchannel attenuation is 20 dB, so the power of any transmitted signal is reduced by\nthis factor when it arrives at the receiver.\n(a) What receiver noise standard deviation value (σ) corresponds to a signal-to-\nnoise ratio (SNR) of 20 dB at the receiver? (Note that the SNR at the receiver is\ndefined as the ratio of the received signal power to σ2.)\n(b) Express the bit error rate at the receiver in terms of the erfc() function when the\nSNR at the receiver is 20 dB.\n\nCHAPTER 9.\nNOISE\n(c) Under the conditions of the previous parts of this question, suppose an ampli-\nfier with gain of 10 dB is added to the receiver after the signal has been corrupted\nwith noise. Explain how this amplification affects the bit error rate.\n4. Due to inter-symbol interference (ISI), which we will study in the next chapter, the\nreceived signal distribution (probability mass function) without noise looks like in\nthe diagram below.\n(a) Determine the value p marked on the graph above.\n(b) Determine the optimal decision threshold Vthreshold, assuming that the prior\nprobabilities of sending a \"0\" and a \"1\", and the noise standard deviations on\nsending a \"0\" and a \"1\" are also equal (σ0 = σ1).\n(c) Derive the expression for the bit error rate in terms of the erfc() function if σ =\n0.025.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 1: Introduction",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/21992dcef7a08aa8b376a366d2979770_MIT6_02F12_chap01.pdf",
      "content": "Bits, Signals, and Packets\nAn Introduction to Digital Communications & Networks\nM.I.T. 6.02 Lecture Notes\nHari Balakrishnan\nChristopher J. Terman\nGeorge C. Verghese\nM.I.T. Department of EECS\nLast update: November 2012\n\nMIT 6.02 DRAFT Lecture Notes\nLast update: September 13, 2012\nCHAPTER 1\nIntroduction\nThe ability to deliver and exchange information over the world's communication networks\nhas revolutionized the way in which people work, play, and live. At the turn of the century,\nthe U.S. National Academy of Engineering produced a list of 20 technologies that made the\nmost impact on society in the 20th century.1 This list included life-changing innovations\nsuch as electrification, the automobile, and the airplane; joining them were four technolog\nical achievements in the area of commununication--radio and television, the telephone, the\nInternet, and computers--whose technological underpinnings we will be most concerned\nwith in this book.\nSomewhat surprisingly, the Internet came in only at #13, but the reason given by the\ncommittee was that it was developed toward the latter part of the century and that they\nbelieved the most dramatic and significant impacts of the Internet would occur in the 21st\ncentury. Looking at the first decade of this century, that sentiment sounds right--the ubiq\nuitous spread of wireless networks and mobile devices, the advent of social networks, and\nthe ability to communicate any time and from anywhere are not just changing the face of\ncommerce and our ability to keep in touch with friends, but are instrumental in massive\nsocietal and political changes.\nCommunication is fundamental to our modern existence. It is hard to imagine life with\nout the Internet and its applications and without some form of networked mobile device.\nIn early 2011, over 5 billion mobile phones were active worldwide, over a billion of which\nhad \"broadband\" network connectivity. To put this number in perspective, it is larger\nthan the number of people in the world who in 2011 had electricity, shoes, toothbrushes,\nor toilets!2\n⌅ 1.1\nObjectives\nWhat makes our communication networks work? This book is a start at understanding the\nanswers to this question. This question is worth studying for two reasons. First, to under-\n1\"The Vertiginous March of Technology\", obtained from nae.edu. Document at http://bit.ly/owMoO6\n2It is in fact distressing that according to a recent survey conducted by TeleNav--and we can't tell if this\nis a joke--40% of iPhone users say they'd rather give up their toothbrushes for a week than their iPhones!\nhttp://www.telenav.com/about/pr-summer-travel/report-20110803.html\n\nCHAPTER 1. INTRODUCTION\nstand the key design principles and basic techniques of analysis used in communication\nsystems. Second, because the technical ideas involved also arise in several other fields of\ncomputer science (CS) and electrical engineering (EE), the study of communication sys\ntems provides an excellent context to introduce concepts that are more widely applicable.\nBefore we dive in and describe the technical topics, we to share a bit of the philosophy\nbehind the material and approach used in this book. The material is well-suited for a\none-semester course on the topic; at MIT, such a course is taken (mostly) by sophomores\nwhose background includes some basic programming (for the accompanying labs) and\nsome exposure to probability and the Fourier series.\nTraditionally, in both education and in research, much of \"low-level communication\" has\nbeen considered an EE topic, covering primarily the issues governing how information\nmoves across a single communication link. In a similar vein, much of \"networking\" has\nbeen considered a CS topic, covering primarily the issues of how to build communication\nnetworks composed of multiple links. In particular, many traditional courses on digi\ntal communication rarely concern themselves with how networks are built and how they\nwork, while most courses on computer networks treat the intricacies of communication\nover physical links as a black box. As a result, a sizable number of people have a deep\nunderstanding of one or the other topic, but few people are expert in every aspect of the\nproblem. This division is one way of conquering the immense complexity of the topic. Our\ngoal in this book is to understand the important details of both the CS and EE aspects of\ndigital communications, and also to understand how various abstractions allow different\nparts of the system to be designed and modified without paying close attention (or even\nfully understanding) what goes on elsewhere in the system.\nOne drawback of preserving strong boundaries between different components of a com\nmunication system is that the details of how things work in another component may re\nmain a mystery, even to practising engineers. In the context of communication systems,\nthis mystery usually manifests itself as things that are \"above my layer\" or \"below my\nlayer\". And so although we will appreciate the benefits of abstraction boundaries in this\nbook, an important goal for us is to study the most important principles and ideas that go\ninto the complete design of a communication system. Our goal is to convey to you both\nthe breadth of the field as well as its depth.\nWe cover communication systems all the way from the source, which has some infor\nmation it wishes to transmit, to packets, which messages are broken into for transmission\nover a network, to bits, each of which is a \"0\" or a \"1\", to signals, which are analog wave\nforms sent over physical communication links (such as wires, fiber-optic cables, radio, or\nacoustic waves). We study a range of communication networks, from the simplest dedi\ncated point-to-point link, to shared media comprising a set of communicating nodes sharing\na common physical communication medium, to larger multi-hop networks that themselves\nare connected to other networks to form even bigger networks.\n⌅ 1.2\nThemes\nThree fundamental challenges lie at the heart of all digital communication systems and\nnetworks: reliability, sharing, and scalability. We will spend a considerable amount of time\non the first two issues in this introductory course, but much less time on the third.\n\nSECTION 1.2. THEMES\n⌅\n1.2.1\nReliability\nA large number of factors conspire to make communication unreliable, and we will study\nnumerous techniques to improve reliability. A common theme across these different tech\nniques is that they all use redundancy in creative and efficient ways to provide reliability us\ning unreliable individual components, using the property of independent (or perhaps weakly\ndependent) failures of these unreliable components to achieve reliability.\nThe primary challenge is to overcome a wide range of faults and disturbances that one\nencounters in practice, including Gaussian noise and interference that distort or corrupt sig\nnals, leading to possible bit errors that corrupt bits on a link, to packet losses caused by\nuncorrectable bit errors, queue overflows, or link and software failures in the network. All\nthese problems degrade communication quality.\nIn practice, we are interested not only in reliability, but also in speed. Most techniques to\nimprove communication reliability involve some form of redundancy, which reduces the\nspeed of communication. The essence of many communication systems is how reliability\nand speed tradeoff against one another.\nCommunication speeds have increased rapidly with time. In the early 1980s, people\nwould connect to the Internet over telephone links at speeds of barely a few kilobits per\nsecond, while today 100 Megabits per second over wireless links on laptops and 1-10 Gi\ngabits per second with wired links are commonplace.\nWe will develop good tools to understand why communication is unreliable and how\nto overcome the problems that arise. The techniques involve error-correcting codes, han\ndling distortions caused by \"inter-symbol interference\" using a linear time-invariant chan\nnel model, retransmission protocols to recover from packet losses that occur for various rea\nsons, and developing fault-tolerant routing protocols to find alternate paths in networks to\novercome link or node failures.\n⌅\n1.2.2\nEfcient Sharing\n\"An engineer can do for a dime what any fool can do for a dollar,\" according to folklore. A\ncommunication network in which every pair of nodes is connected with a dedicated link\nwould be impossibly expensive to build for even moderately sized networks. Sharing is\ntherefore inevitable in communication networks because the resources used to communi\ncate aren't cheap. We will study how to share a point-to-point link, a shared medium, and\nan entire multi-hop network among multiple communications.\nWe will develop methods to share a common communication medium among nodes, a\nproblem common to wired media such as broadcast Ethernet, wireless technologies such\nas wireless local-area networks (e.g., 802.11 or WiFi), cellular data networks (e.g., \"3G\"),\nand satellite networks (see Figure 1-1).\nWe will study modulation and demodulation, which allow us to transmit signals over\ndifferent carrier frequencies. In the process, we can ensure that multiple conversations\nshare a communication medium by operating at different frequencies.\nWe will study medium access control (MAC) protocols, which are rules that determine\nhow nodes must behave and react in the network--emulate either time sharing or frequency\nsharing. In time sharing, each node gets some duration of time to transmit data, with no\nother node being active. In frequency sharing, we divide the communication bandwidth\n\nCHAPTER 1. INTRODUCTION\nFigure 1-1: Examples of shared media.\nIndividual images (c) source unknown. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n(i.e., frequency range) amongst the nodes in a way that ensures a dedicated frequency\nsub-range for different communications, and the different communications can then occur\nconcurrently without interference. Each scheme has its sweet spot and uses.\nWe will then turn to multi-hop networks. In these networks, multiple concurrent com\nmunications between disparate nodes occur by sharing over the same links. That is, one\nmight have communication between many different entities all happening over the same\nphysical links. This sharing is orchestrated by special computers called switches, which im\nplement certain operations and protocols. Multi-hop networks are generally controlled in\ndistributed fashion, without any centralized control that determines what each node does.\nThe questions we will address include:\n1. How do multiple communications between different nodes share the network?\n2. How do messages go from one place to another in the network?\n3. How can we communicate information reliably across a multi-hop network (as op\nposed to over just a single link or shared medium)?\nThe techniques used to share the network and achieve reliability ultimately determine\nthe efficiency of the communication network. In general, one can frame the efficiency ques\ntion in several ways. One approach is to minimize the capital expenditure (hardware\nequipment, software, link costs) and operational expenses (people, rental costs) to build\nand run a network capable of meeting a set of requirements (such as number of connected\ndevices, level of performance and reliability, etc.). Another approach is to maximize the\nbang for the buck for a given network by maximizing the amount of \"useful work\" that can\nbe done over the network. One might measure the \"useful work\" by calculating the aggre\ngate throughput (in \"bits per second\", or at higher speeds, the more convenient \"megabits\nper second\") achieved by the different communications, the variation of that throughput\namong the set of nodes, and the average delay (often called the latency, measured usu\nally in milliseconds) achieved by the data transfers. We will primarily be concerned with\nthroughput and latency in this course, and not spend much time on the broader (but no\nless important) questions of cost.\n\nSECTION 1.3. OUTLINE AND PLAN\nOf late, another aspect of efficiency that has become important in many communica\ntion systems is energy consumption. This issue is important both in the context of massive\nsystems such as large data centers and for mobile computing devices such as laptops and\nmobile phones. Improving the energy efficiency of these systems is an important problem.\n⌅\n1.2.3\nScalability\nIn addition to reliability and efficient sharing, scalability (i.e., designing networks that scale\nto large sizes) is an important design consideration for communication networks. We will\nonly touch on this issue, leaving most of it to later courses (6.033, 6.829).\n⌅ 1.3\nOutline and Plan\nWe have divided the course into four parts: the source, and the three important abstrac\ntions (bits, signals, and packets). For pedagogic reasons, we will study them in the order\ngiven below.\n1. The source. Ultimately, all communication is about a source wishing to send some\ninformation in the form of messages to a receiver (or to multiple receivers). Hence,\nit makes sense to understand the mathematical basis for information, to understand\nhow to encode the material to be sent, and for reasons of efficiency, to understand how\nbest to compress our messages so that we can send as little data as possible but yet\nallow the receiver to decode our messages correctly. Chapters 2 and 3 describe the\nkey ideas behind information, entropy (expectation of information), and source coding,\nwhich enables data compression. We will study Huffman codes and the Lempel-Ziv-\nWelch algorithm, two widely used methods.\n2. Bits. The main issue we will deal with here is overcoming bit errors using error-\ncorrecting codes, specifically linear block codes (Chapters 5 and 6) and convolutional\ncodes (Chapters 7 and 8). These codes use interesting and sophisticated algorithms\nthat cleverly apply redundancy to reduce or eliminate bit errors.\n3. Signals. The main issues we will deal with here are how to modulate bits over signals\nand demodulate signals to recover bits, as well as understanding how distortions of\nsignals by communication channels can be modeled using a linear time-invariant (LTI)\nabstraction. Topics include going between time-domain and frequency-domain rep\nresentations of signals, the frequency content of signals, and the frequency response\nof channels and filters. Chapters 9 through 14 describe these topics.\n4. Packets. The main issues we will study are how to share a medium using a MAC pro\ntocol, routing in multi-hop networks, and reliable data transport protocols. Chapters\n15 through 19 describe these topics.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 10: Models for Physical Communication Channels",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/6fb1d88aaf5f8d2e9e7655158d1c4408_MIT6_02F12_chap10.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\n(Last update: September 30, 2012)\nCHAPTER 10\nModels for Physical Communication\nChannels\nTo preview what this chapter is about, it will be helpful first to look back briefly at the\nterritory we have covered. The previous chapters have made the case for a digital (versus\nanalog) communication paradigm, and have exposed us to communication at the level of\nbits or, more generally, at the level of the discrete symbols that encode messages.\nWe showed, in Chapters 2 and 3, how to obtain compressed or non-redundant repre-\nsentations of a discrete set of messages through source coding, which produced codewords\nthat reflected the inherent information content or entropy of the source. In Chapter 4 we\nexamined how the source transmitter might map a bit sequence to clocked signals that are\nsuited for transmission on a physical channel (for example, as voltage levels).\nChapter 5 introduced the binary symmetric channel (BSC) abstraction for bit errors on\na channel, with some associated probability of corrupting an individual bit on passage\nthrough the channel, independently of what happens to every other bit in the sequence.\nThat chapter, together with Chapters 6, 7, and 8, showed how to re-introduce redundancy,\nbut in a controlled way using parity bits. This resulted in error-correction codes, or channel\ncodes, that provide some level of protection against the bit-errors introduced by the BSC.\nChapter 9 considered the challenges of \"demapping\" back from the received noise-\ncorrupted signal to the underlying bit stream, assuming that the channel introduced no\ndeterministic distortion, only additive white noise on the discrete-time samples of the re-\nceived signal. A key idea from Chapter 9 was showing how Gaussian noise experienced\nby analog signals led to the BSC bit-flip probability for the discrete version of the channel.\nThe present chapter begins the process--continued through several subsequent\nchapters--of representing, modeling, analyzing, and exploiting the characteristics of the\nphysical transmission channel. This is the channel seen between the signal transmitted\nfrom the source and the signal captured at the receiver. Referring back to the \"single-link\nview\" in the upper half of Figure 4-8 in Chapter 4, our intent is to study in more detail the\nportion of the communication channel represented by the connection between \"Mapper +\nXmit samples\" at the source side, and \"Recv samples + Demapper\" at the receiver side.\n\nCHAPTER 10.\nMODELS FOR PHYSICAL COMMUNICATION CHANNELS\ncodeword\nbits in\ncodeword\nbits out\nDAC\nADC\nNOISY & DISTORTING ANALOG CHANNEL\nmodulate\ndemodulate\n& filter\ngenerate\ndigitized\nsamples\nsample &\nthreshold\nFigure 10-1: Elements of a communication channel between the channel coding step at the transmitter and\nchannel decoding at the receiver.\n1 0 0 1 1 1 0 1 0 1\nDistorted noise-free signal y[n] at receiver\nSample number n\n(discrete-time index)\nSample\nvalue\nFigure 10-2: Digitized samples of the baseband signal.\n■\n10.1\nGetting the Message Across\n■\n10.1.1\nThe Baseband Signal\nIn Figure 10-1 we see an expanded version of what might come between the channel cod-\ning operation at the transmitter and the channel decoding operation at the receiver (as\ndescribed in the upper portion of Figure 4-8). At the source, the first stage is to convert\nthe input bit stream to a digitized and discrete-time (DT) signal, represented by samples pro-\nduced at a certain sample rate fs samples/s. We denote this signal by x[n], where n is the\ninteger-valued discrete-time index, ranging in the most general case from -infto +inf.\nIn the simplest case, which we will continue to use for illustration, each bit is repre-\nsented by a signal level held for a certain number of samples, for instance a voltage level\nof V0 = 0 held for 8 samples to indicate a 0 bit, and a voltage level of V1 = 1 held for 8\nsamples to indicate a 1 bit, as in Figure 10-2. The sample clock in this example operates\n\nSECTION 10.1.\nGETTING THE MESSAGE ACROSS\nat 8 times the rate of the bit clock, so the bi\nreferred to as the baseband signal.\nt rate is fs/8 bits/s. Such a signal is usually\n■\n10.1.2\nModulation\nThe DT baseband signal shown in Figure 10-2 is typically not ready to be transmitted on\nthe physical transmission channel. For one thing, physical channels typically operate in\ncontinuous-time (CT) analog fashion, so at the very least one needs a digital-to-analog\nconverter (DAC) to produce a continuous-time signal that can be applied to the channel.\nThe DAC is usually a simple zero-order hold, which maintains or holds the most recent\nsample value for a time interval of 1/fs. With such a DAC conversion, the DT \"rectangular-\nwave\" in Figure 10-2 becomes a CT rectangular wave, each bit now corresponding to a\nsignal value that is held for 8/fs seconds.\nConversion to an analog CT signal will not suffice in general, because the physical\nchannel is usually not well suited to the transmission of rectangular waves of this sort.\nFor instance, a speech signal from a microphone may, after appropriate coding for digital\ntransmission, result in 64 kilobits of data per second (a consequence of sampling the micro-\nphone waveform at 8 kHz and 8-bit resolution), but a rectangular wave switching between\ntwo levels at this rate is not adapted to direct radio transmission. The reasons include the\nfact that efficient projection of wave energy requires antennas of dimension comparable\nwith the wavelength of the signal, typically a quarter wavelength in the case of a tower an-\ntenna. At 32 kHz, corresponding to the waveform associated with alternating 1's and 0's in\nthe coded microphone output, and with the electromagnetic waves propagating at 3 × 108\nmeters/s (the speed of light), a quarter-wavelength antenna would be a rather unwieldy\n3 × 108/(4 × 32 × 103) = 2344 meters long!\nEven if we could arrange for such direct transmission of the baseband signal (after\ndigital-to-analog conversion), there would be issues related to the required transmitter\npower, the attenuation caused by the atmosphere at this frequency, interference between\nthis transmission and everyone else's, and so on. Regulatory organizations such as the\nU.S. Federal Communications Commission (FCC), and equivalent bodies in other coun-\ntries, impose constraints on transmissions, which further restrict what sort of signal can be\napplied to a physical channel.\nIn order to match the baseband signal to the physical and regulatory specifications of a\ntransmission channel, one typically has to go through a modulation process. This process\nconverts the digitized samples to a form better suited for transmission on the available\nchannel. Consider, for example, the case of direct transmission of digital information on\nan acoustic channel, from the speaker on your computer to the microphone on your com-\nputer (or another computer within \"hearing\" distance). The speaker does not respond\neffectively to the piecewise-constant voltages that arise from our baseband signal. It is in-\nstead designed to respond to oscillatory voltages at frequencies in the appropriate range,\nproducing and projecting a wave of oscillatory acoustic pressure. Excitation by a sinu-\nsoidal wave produces a pure acoustic tone. With a speaker aperture dimension of about 5\ncm (0.05 meters), and a sound speed of around 340 meters/s, we anticipate effective pro-\njection of tones with frequencies in the low kilohertz range, which is indeed in (the high\nend of) the audible range.\nA simple way to accomplish the desired modulation in the acoustic wave exam-\n\nCHAPTER 10.\nMODELS FOR PHYSICAL COMMUNICATION CHANNELS\nple above is to apply--at the output of the digital-to-analog converter, which feeds the\nloudspeaker--a voltage V0 cos(2πfct) for some duration of time to signal a 0 bit, and a\nvoltage of the form V1 cos(2πfct) for the same duration of time to signal a 1 bit.1 Here\ncos(2πfct) is referred to as the carrier signal and fc is the carrier frequency, chosen to be ap-\npropriately matched to the channel characteristics. This particular way of imprinting the\nbaseband signal on a carrier by varying its amplitude is referred to as amplitude modulation\n(AM), which we will study in more detail in Chapter 14. The choice V0 = 0 and V1 = 1 is\nalso referred to as on-off keying, with a burst of pure tone (\"on\") signaling a 1 bit, and an\ninterval of silence (\"off\") signaling a 0.\nOne could also choose V0 = -1 and V1 = +1, which would result in a sinusoidal voltage\nthat switches phase by π/2 each time the bit stream goes from 0 to 1 or from 1 to 0. This\napproach may be referred to as polar keying (particularly when it is thought of as an instance\nof amplitude modulation), but is more commonly termed binary phase-shift keying (BPSK).\nYet another modulation possibility for this acoustic example is frequency modulation (FM),\nwhere a tone burst of a particular frequency in the neighborhood of fc is used to signal a 0\nbit, and a tone burst at another frequency to signal a 1 bit. All these schemes are applicable\nto radio frequency (RF) transmissions as well, not just acoustic transmissions, and are in\nfact commonly used in practice for RF communication.\n■\n10.1.3\nDemodulation\nWe shall have more to say about demodulation later, so for now it suffices to think of it\nas a process that is inverse to modulation, aimed at extracting the baseband signal from\nthe received signal. While part of this process could act directly on the received CT analog\nsignal, the block diagram in Figure 10-1 shows it all happening in DT, following conversion\nof the received signal using an analog-to-digital converter (ADC). The block diagram also\nindicates that a filtering step may be involved, to separate the channel noise as much as\npossible from the signal component of the received signal, as well as to compensate for\ndeterministic distortion introduced by the channel. These ideas will be explored further in\nlater chapters.\n■\n10.1.4\nThe Baseband Channel\nThe result of the demodulation step and any associated filtering is a DT signal y[n], com-\nprising samples arriving at the rate fs used for transmission at the source. We assume\nissues of clock synchronization are taken care of separately. We also neglect the effects of\nany signal attenuation, as this can be simply compensated for at the receiver by choosing\nan appropriate amplifier gain.\nIn the ideal case of no distortion, no noise on the channel, and insignificant propagation\ndelay, y[n] would exactly equal the modulating baseband signal x[n] used at the source, for\nall n. If there is a fixed and known propagation delay on the channel, it can be convenient\nto simply set the clock at the receiver that much later than the clock at the sender. If this\nis done, then again we would find that in the absence of distortion and random noise, we\nget y[n] = x[n] for all n.\n1A zero-order-hold DAC will produce only an approximation of a pure sinusoid, but if the sample rate fs\nis sufficiently high, the speaker may not sense the difference.\n\nSECTION 10.2.\nLINEAR TIME-INVARIANT (LTI) MODELS\nSignal x[n] from digitized samples at transmitter\nExample of distorted noise-free signal y[n] at receiver\nFigure 10-3: Channel distortion example. The distortion is deterministic.\nMore realistically, the channel does distort the baseband signal, so the output DT signal\nmay look (in the noise-free case) as the lower waveform in Figure 10-3. Our objective in\nwhat follows is to develop and analyze an important class of models, namely linear and\ntime-invariant (LTI) models, that are quite effective in accounting for such distortion, in a\nvast variety of settings. The models would be used to represent the end-to-end behavior\nof what might be called the baseband channel, whose input is x[n] and output is y[n], as in\nFigure 10-3.\n■\n10.2\nLinear Time-Invariant (LTI) Models\n■\n10.2.1\nBaseband Channel Model\nOur baseband channel model, as represented in the block diagram in Figure 10-4 takes the\nDT sequence or signal x[.] as input and produces the sequence or signal y[.] as output. We\nwill often use the notation x[.]--or even simply just x--to indicate the entire DT signal\nor function. Another way to point to the entire signal, though more cumbersome, is by\nreferring to \"x[n] for -inf< n < inf\"; this often gets abbreviated to just \"the signal x[n]\", at\nthe risk of being misinterpreted as referring to just the value at a single time n.\nFigure 10-4 shows x[n] at the input and y[n] at the output, but that is only to indicate\nthat this is a snapshot of the system at time n, so indeed we see x[n] at the input and y[n]\n\nCHAPTER 10.\nMODELS FOR PHYSICAL COMMUNICATION CHANNELS\nS\nx[n]\ny[n]\ninput\nresponse\nFigure 10-4: Input and output of baseband channel.\nFigure 10-5: A unit step. In the picture on the left the unit step is unshifted, switching from 0 to 1 at index\n(time) 0. On the right, the unit step is shifted forward in time by 3 units (shifting forward in time means\nthat we use the -sign in the argument because we want the switch to occur with n -3 = 0).\nat the output of the system. What the diagram should not be interpreted as indicating is\nthat the value of the output signal y[.] at time n depends exclusively on the value of the\ninput signal at that same time n. In general, the value of the output y[.] at time n, namely\ny[n], could depend on the values of the input x[.] at all times. We are most often interested\nin causal models, however, and those are characterized by y[n] only depending on past and\npresent values of x[.], i.e., x[k] for k ≤n.\n■\n10.2.2\nUnit Sample Response h[n] and Unit Step Response s[n]\nThere are two particular signals that will be very useful in our description and study of\nLTI channel models. The unit step signal or function u[n] is defined as\nu[n]\n=\n1 if n ≥0\nu[n]\n=\n0 otherwise\n(10.1)\nIt takes the value 0 for negative values of its argument, and 1 everywhere else, as shown\nin Figure 10-5. Thus u[1 -n], for example, is 0 for n > 1 and 1 elsewhere.\nThe unit sample signal or function δ[n], also called the unit impulse function, is defined\nas\nδ[n]\n=\n1 if n = 0\nδ[n]\n=\n0 otherwise.\n(10.2)\nIt takes the value 1 when its argument is 0, and 0 everywhere else, as shown in Figure 10-6.\n\nSECTION 10.2.\nLINEAR TIME-INVARIANT (LTI) MODELS\nFigure 10-6: A unit sample. In the picture on the left the unit sample is unshifted, with the spike occurring\nat index (time) 0. On the right, the unit sample is shifted backward in time by 5 units (shifting backward\nin time means that we use the + sign in the argument because we want the switch to occur with n + 5 = 0).\nFigure 10-7: Time-invariance: if for all possible sequences x[.] and integers D, the relationship between\ninput and output is as shown above, then S is said to be \"time-invariant\" (TI).\nThus δ[n -3] is 1 where n = 3 and 0 everywhere else. One can also deduce easily that\nδ[n] = u[n] -u[n -1] ,\n(10.3)\nwhere addition and subtraction of signals such as u[n] and u[n -1] are defined \"point-\nwise\", i.e., by adding or subtracting the values at each time instant. Similarly, the multipli-\ncation of a signal by a scalar constant is defined as pointwise multiplication by this scalar,\nso for instance 2u[n -3] has the value 0 for n < 3, and the value 2 everywhere else.\nThe response y[n] of the system in Figure 10-4 when its input x[n] is the unit sample\nsignal δ[n] is referred to as the unit sample response, or sometimes the unit impulse re-\nsponse. We denote the output in this special case by h[n]. Similarly, the response to the\nunit step signal u[n] is referred to as the unit step response, and denoted by s[n].\nA particularly valuable use of the unit step function, as we shall see, is in representing\na rectangular-wave signal as an alternating sum of delayed (and possibly scaled) unit step\nfunctions. An example is shown in Figure 10-9. We shall return to this decomposition later.\n■\n10.2.3\nTime-Invariance\nConsider a DT system with input signal x[.] and output signal y[.], so x[n] and y[n] are the\nvalues seen at the input and output at time n. The system is said to be time-invariant if\nshifting the input signal x[.] in time by an arbitrary positive or negative integer D to get\na new input signal xD[n] = x[n -D] produces a corresponding output signal yD[n] that is\njust y[n -D], i.e., is the result of simply applying the same shift D to the response y[.] that\nwas obtained with the original unshifted input signal. The shift corresponds to delaying\nthe signal by D if D > 0, and advancing it by |D| if D < 0. In particular, for a TI system,\na shifted unit sample function at the input generates an identically shifted unit sample\nresponse at the output. Figure 10-7 illustrates time-invariance.\nS\ny[n-D]\nx[n-D]\n\nCHAPTER 10.\nMODELS FOR PHYSICAL COMMUNICATION CHANNELS\na1x1[n]+ a2x2[n]\nS\na1y1[n]+ a2y2[n]\nFigure 10-8: Linearity: if the input is the weighted sum of several signals, the response is the corresponding\nsuperposition (i.e., weighted sum) of the response to those signals.\nThe key to recognizing time-invariance in a given system description is to ask whether\nthe rules or equations by which the input values x[.] are combined, to create the output\ny[n], involve knowing the value of n itself (or something equivalent to knowing n), or just\ntime differences from the time n. If only the time differences from n are needed, the system is\ntime-invariant. In this case, the same behavior occurs whether the system is run yesterday\nor today, in the following sense: if yesterday's inputs are applied today instead, then the\noutput today is what we would have obtained yesterday, just occurring a day later.\nAnother operational way to recognize time-invariance is to ask whether shifting the\npair of signals x[.] and y[.] by the arbitrary but identical amount D results in new signals\nxD[.] and yD[.] that still satisfy the equations defining the system. More generally, a set\nof signals that jointly satisfies the equations defining a system, such as x[.] and y[.] in our\ninput-output example, is referred to as a behavior of the system. And what time-invariance\nrequires is that time-shifting any behavior of the system by an arbitrary amount D results\nin a set of signals that is still a behavior of the system.\nConsider a few examples. A system defined by the relation\ny[n] = 0.5y[n -1] + 3x[n] + x[n -1]\nfor all n\n(10.4)\nis time-invariant, because to construct y[.] at any time instant n, we only need values of y[.]\nand x[.] at the same time step and one time step back, no matter what n is -- so we don't\nneed to know n itself. To see this more concretely, note that the above relation holds for all\nn, so we can write\ny[n -D] = 0.5y[(n -D) -1] + 3x[n -D] + x[(n -D) -1]\nfor all n\nor\nyD[n] = 0.5yD[n -1] + 3xD[n] + xD[n -1]\nfor all n .\nIn other words, the time-shifted input and output signals, xD[.] and yD[.] respectively, also\nsatisfy the equation that defines the system.\nThe system defined by\ny[n] = n3x[n]\nfor all n\n(10.5)\nis not time-invariant, because the value of n is crucial to defining the output at time n. A\nlittle more subtle is the system defined by\ny[n] = x[0] + x[n]\nfor all n .\n(10.6)\nThis again is not time-invariant, because the signal value at the absolute time 0 is needed,\nrather than a signal value that is offset from n by an amount that doesn't depend on n. We\n\nSECTION 10.2.\nLINEAR TIME-INVARIANT (LTI) MODELS\nFigure 10-9: A rectangular-wave signal can be represented as an alternating sum of delayed (and possibly\nscaled) unit step functions. In this example, x[n] = u[n] -u[n -4] + u[n -12] -u[n -24].\nhave yD[n] = x[0] + xD[n] rather than what would be needed for time-invariance, namely\nyD[n] = xD[0] + xD[n].\n■\n10.2.4\nLinearity\nBefore defining the concept of linearity, it is useful to recall two operations on signals\nor time-functions that were defined in connection with Equation (10.3) above, namely (i)\naddition of signals, and (ii) scalar multiplication of a signal by a constant. These operations\nwere defined as pointwise (i.e., occurring at each time-step), and were natural definitions.\n(They are completely analogous to vector addition and scalar multiplication of vectors,\nthe only difference being that instead of the finite array of numbers that we think of for\na vector, we have an infinite array, corresponding to a signal that can take values for all\ninteger n.)\nWith these operations in hand, one can talk of weighted linear combinations of signals.\nThus, if x1[.] and x2[.] are two possible input signals to a system, for instance the signals\nassociated with experiments numbered 1 and 2, then we can consider an experiment 3 in\nwhich the input x3[.] a weighted linear combination of the inputs in the other two experi-\nments:\nx3[n] = a1x1[n] + a2x2[n]\nfor all n ,\n\nCHAPTER 10.\nMODELS FOR PHYSICAL COMMUNICATION CHANNELS\nwhere a1 and a2 are scalar constants.\nThe system is termed linear if the response to this weighted linear combination of the\ntwo signals is the same weighted combination of the responses to the two signals, for all possible\nchoices of x1[.], x2[.] a1 and a2, i.e., if\ny3[n] = a1y1[n] + a2y2[n]\nfor all n ,\nwhere yi[.] denotes the response of the system to input xi[.] for i = 1,2,3.\nThis relationship is shown in Figure 10-8. If this property holds, we say that the results\nof any two experiments can be superposed to yield the results of other experiments; a linear\nsystem is said to have the superposition property. (In terms of the notion of behaviors\ndefined earlier, what linearity requires is that weighted linear combinations, or superposi-\ntions, of behaviors are again behaviors of the system.)\nWe can revisit the examples introduced in Equations (10.4), (10.5), (10.6) to apply this\ndefinition, and recognize that all three systems are linear. The following are examples of\nsystems that are not linear:\ny[n] = x[n] + 3 ;\ny[n] = x[n] + x2[n -1] ;\ny[n] = cos\n\nx2[n]\n.\nx2[n] + 1\n\nAll three examples here are time-invariant.\n■\n10.2.5\nLinear, Time-Invariant (LTI) Models\nModels that are both linear and time-invariant, or LTI models, are hugely important in en-\ngineering and other domains. We will mention some of the reasons in the next chapter. We\nwill develop insights into their behavior and tools for their analysis, and then return to ap-\nply what we have learned, to better understand signal transmission on physical channels.\nIn the context of audio communication using a computer's speaker and microphone,\ntransmissions are done using bursts at the loudspeaker of a computer, and receptions by\ndetecting the response at a microphone. The input x[n] in this case is a baseband signal\nof the form in Figure 10-3, but alternating regularly between high and low values. This\nwas converted through a modulation process into the tone bursts that you heard. The\nsignal received at the microphone is then demodulated to reconstruct an estimate y[n] of\nthe baseband input.\nWith the microphone in a fixed position, responses have some consistency from one\ntransition to the next (between tone and no-tone), despite the presence of random fluctua-\ntions riding on top of things. The deterministic or repeatable part of the response y[n] does\nshow distortion, i.e., deviation from x[n], though more \"real-world\" than what is shown\nin the synthetic example in Figure 10-3. However, when the microphone is very close to\nthe speaker, the distortion is low.\nThere were features of the system response in this communication system to suggest\nthat it may not be unreasonable to model the baseband acoustic channel as LTI. Time-\ninvariance (at least over the time-horizon of the demo!) is suggested by the repeatability\nof the transient responses to the various transitions. Linearity is suggested by the fact that\n\nSECTION 10.2.\nLINEAR TIME-INVARIANT (LTI) MODELS\nthe downward transients caused by negative (i.e., downward) steps at the input look like\nreflections of the upward transients caused by positive (i.e., upward) steps of the same\nmagnitude at the input, and is also suggested by the appropriate scaling of the response\nwhen the input is scaled.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 11: LTI Models and Convolution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/c99c8b2514977bc7f008aa8f69dfe9ad_MIT6_02F12_chap11.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 4, 2012\nCHAPTER 11\nLTI Models and Convolution\nThis chapter will help us understand what else besides noise (which we studied in Chapter\n9) perturbs or distorts a signal transmitted over a communication channel, such as a volt-\nage waveform on a wire, a radio wave through the atmosphere, or a pressure wave in an\nacoustic medium. The most significant feature of the distortion introduced by a channel is\nthat the output of the channel typically does not instantaneously respond to or follow the\ninput. The physical reason is ultimately some sort of inertia effect in the channel, requiring\nthe input to supply energy in order to generate a response, and therefore requiring some\ntime to respond, because the input power is limited. Thus, a step change in the signal at\nthe input of the channel typically causes a channel output that rises more gradually to its\nfinal value, and perhaps with a transient that oscillates or \"rings\" around its final value\nbefore settling. A succession of alternating steps at the input, as would be produced by\non-off signaling at the transmitter, may therefore produce an output that displays inter-\nsymbol interference (ISI): the response to the portion of the input signal associated with a\nparticular bit slot spills over into other bit slots at the output, making the output waveform\nonly a feeble representation of the input, and thereby complicating the determination of\nwhat the input message was.\nTo understand channel response and ISI better, we will use linear, time-invariant (LTI)\nmodels of the channel, which we introduced in the previous chapter. Such models provide\nvery good approximations of channel behavior in a range of applications (they are also\nwidely used in other areas of engineering and science). In an LTI model, the response\n(i.e., output) of the channel to any input depends only on one function, h[·], called the unit\nsample response function. Given any input signal sequence, x[·], the output y[·] of an\nLTI channel can be computed by combining h[·] and x[·] through an operation known as\nconvolution.\nKnowledge of h[·] will give us guidance on choosing the number of samples to associate\nwith a bit slot in order to overcome ISI, and will thereby determine the maximum bit\nrate associated with the channel. In simple on-off signaling, the number of samples that\nneed to be allotted to a bit slot in order to mitigate the effects of ISI will be approximately\nthe number of samples required for the unit sample response h[·] to essentially settle to\n0. In this connection, we will also introduce a tool called an eye diagram, which allows\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nFigure 11-1: On-off signaling at the channel input produces a channel output that takes a non-zero time to\nrise or fall, and to settle at 1 or 0.\na communication engineer to determine whether the number of samples per bit is large\nenough to permit reasonable communication quality in the face of ISI.\n■\n11.1\nDistortions on a Channel\nEven though communication technologies come in enormous variety, they generally all\nexhibit similar types of distorting behavior in response to inputs. To gain some intution\non the basic nature of the problem, we first look at some simple examples. Consider a\ntransmitter that does on-off signaling, sending voltage samples that are either set to V0 = 0\nvolts or to V1 = 1 volt for all the samples in a bit period. Let us assume an LTI channel, so\nthat\n1. the superposition property applies, and\n2. if the response to a unit step u[n] at the input is unit-step response s[n] at the output,\nthen the response to a shifted unit step u[n -D] at the input is the identically shifted\nunit-step response s[n -D], for any (integer) D.\nLet us also assume that the channel and receiver gain are such that the unit step response\ns[n] eventually settles to 1.\nIn this setting, the output waveform will typically have two notable deviations from the\ninput waveform:\n\nSECTION 11.1.\nDISTORTIONS ON A CHANNEL\nFigure 11-2: A channel showing \"ringing\".\n1. A slower rise and fall. Ideally, the voltage samples at the receiver should be iden-\ntical to the voltage samples at the transmitter. Instead, as shown in Figure 11-1, one\nusually finds that the nearly instantaneous transition from V0 volts to V1 volts at the\ntransmitter results in an output voltage at the receiver that takes longer to rise from\nV0 volts to V1 volts. Similarly, when there is a nearly instantaneous transition from\nV1 volts to V0 volts at the transmitter, the voltage at the receiver takes longer to fall. It\nis important to note that if the time between transitions at the transmitter is shorter\nthan the rise and fall times at the receiver, the receiver will struggle (and/or fail!) to\ncorrectly infer the value of the transmitted bits using the voltage samples from the\noutput.\n2. Oscillatory settling, or \"ringing\". In some cases, voltage samples at the receiver\nwill oscillate before settling to a steady value. In cables, for example, this effect can\nbe due to a \"sloshing\" back and forth of the energy stored in electric and magnetic\nfields, or it can be the result of signal reflections at discontinuities. Over radio and\nacoustic channels, this behavior arises usually from signal reflections. We will not\ntry to determine the physical source of ringing on a channel, but will instead observe\nthat it happens and deal with it. Figure 11-2 shows an example of ringing.\nFigure 11-3 shows an example of non-ideal channel distortions. In the example, the\ntransmitter converted the bit sequence ...0101110... to voltage samples using ten 1 volt\nsamples to represent a \"1\" and ten 0 volt samples to represent a \"0\" (with sample values\nof 0 before and after). In the example, the settling time at the receiver is longer than the\nreciprocal of the bit period, and therefore bit sequences with frequent transitions, like 010,\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nSending 0101110\n\nFigure 11-3: The effects of rise/fall time and ringing on the received signal.\nare not received faithfully. In Figure 11-3, at sample number 21, the output voltage is still\nringing in response to the rising input transition at sample number 10, and is also respond-\ning to the input falling transition at sample number 20. The result is that the receiver may\nmisidentify the value of one of the transmitted bits. Note also that the the receiver will\ncertainly correctly determine that the the fifth and sixth bits have the value '1', as there is\nno transition between the fourth and fifth, or fifth and sixth, bit.\nAs this example demonstrates, the slow settling of the channel output implies that the\nreceiver is more likely to wrongly identify a bit that differs in value from its immediate\npredecessors. This example should also provide the intuition that if the number of samples\nper bit is large enough, then it becomes easier for the receiver to correctly identify bits\nbecause each sequence of samples has enough time to settle to the correct value (in the\nabsence of noise, which is of course a random phenomenon that can still confound the\nreceiver).\nThere is a formal name given to the impact of rise/fall times and settling times that\nare long compared to a bit slot: we say that the channel output displays inter-symbol\ninterference, or ISI. ISI is a fancy way of saying that the received samples corresponding to the\ncurrent bit depend on the values of samples corresponding to preceding bits. Figure 11-4 shows\nfour examples: two for channels with a fast rise/fall compared to the duration of the bit\n\nSECTION 11.2.\nCONVOLUTION FOR LTI MODELS\nLong Bit Period (slow rate)\nShort Bit Period (Fast Rate)\nFigure 11-4: Examples of ISI.\nslot, and two for channels with a slower rise/fall.\nWe now turn to a more detailed study of LTI models, which will allow us to understand\nchannel distortion and ISI more fundamentally. The analysis tools we develop, in this\nchapter and the next two, will also be very valuable in the context of signal processing,\nfiltering, modulation and demodulation, topics that are addressed in the next few chapters.\n■\n11.2\nConvolution for LTI Models\nConsider a discrete-time (DT) linear and time-invariant (LTI) system or channel model\nthat maps an input signal x[.] to an output signal y[.] (Figure 10-4), which shows what the\ninput and output values are at some arbitrary integer time instant n. We will also use other\nnotation on occasion to denote an entire time signal such as x[.], either simply writing x,\nor sometimes writing x[n] (but with the typically unstated convention that n ranges over\nall integers!).\nA discrete-time (DT) LTI system is completely characterized by its response to a unit\nsample function (or unit pulse function, or unit \"impulse\" function) δ[n] at the input. Recall\nthat δ[n] takes the value 1 where its argument n = 0, and the value 0 for all other values of\nthe argument. An alternative notation for this signal that is sometimes useful for clarity is\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nδ0[.], where the subscript indicates the time instant for which the function takes the value\n1; thus δ[n -k], when described as a function of n, could also be written as the signal δk[.].\nFigure 11-5: A discrete-time signal can be decomposed into a sum of time-shifted, scaled unit-sample func-\ntions: x[n] = inf\nk=\nx[k]δ[n\n-inf\n-k].\nThe unit sample response h[n], with n taking all integer values, is simply the sequence\nof values that y[n] takes when we set x[n] = δ[n], i.e., x[0] = 1 and x[k] = 0 for k = 0. The\nresponse h[n] to the elementary input δ[n] can be used to characterize the response of an\nLTI system to any input, for the following two reasons:\n- An arbitrary signal x[.] can be written as a sum of scaled (or weighted) and shifted\nunit sample functions, as shown in Figure 11-5. This is expressed in two ways below:\nx[.]\n=\n··· + x[-1]δ\n1[.] + x[0]δ0[.] + ··· + x[k]δ\n-\nk[.] + ···\nx[n]\n=\n··· + x[-1]δ[n + 1] + x[0]δ[n] + ··· + x[k]δ[n -k] + ···\n(11.1)\n- The response of an LTI system to an input that is the scaled and shifted combina-\ntion of other inputs is the same scaled combination--or superposition--of the corre-\nspondingly shifted responses to these other inputs, as shown in Figure 11-6.\nSince the response at time n to the input signal δ[n] is h[n], it follows from the two obser-\n\nSECTION 11.2.\nCONVOLUTION FOR LTI MODELS\n\nx[n]=\nx[k]δ[n -k]\nk=-inf\ninf\n∑\ny[n]=\nx[k]h[n -k]\nk=-inf\ninf\n∑\n\nx[n]\ny[n]\nCONVOLUTION SUM\nFigure 11-6: Illustrating superposition: If S is an LTI system, then we can use the unit sample response h\nto predict the response to any waveform x by writing x as a sum of shifted, scaled unit sample functions,\nand writing the output as a sum of shifted, scaled, unit sample responses, with the same scale factors.\nvations above that the response at time n to the input x[.] is\ny[n]\n=\n··· + x[-1]h[n + 1] + x[0]h[n] + ··· + x[k]h[n -k] + ···\ninf\n=\n\nx[k]h[n -k] .\n(11.2)\nk=-inf\nThis operation on the time functions or signals x[.] and h[.] to generate a signal y[.] is called\nconvolution. The standard symbol for the operation of convolution is ∗, and we use it\nto write the prescription in Equation (11.2) as y[n] = (x ∗h)[n]. We will also simply write\n∗\nwhen that suffices. 1\ny = x\nh\nA simple change of variables in Equation (11.2), setting n -k = m, shows that we can\nalso write\ninf\ny[n] =\n\nh[m]x[n\nm=\n-m] = (h ∗x)[n] .\n(11.3)\n-inf\nThe preceding calculation establishes that convolution is commutative, i.e.,\nx ∗h = h ∗x .\nWe will mention other properties of convolution later, in connection with series and paral-\nlel combinations (or compositions) of LTI systems.\nExample 1\nSuppose h[n] = (0.5)nu[n], where u[n] denotes the unit step function defined\npreviously (taking the value 1 where its argument n is non-negative, and the value 0 when\n1A common (but illogical, confusing and misleading!) notation for convolution in much or most of the\nengineering literature is to write y[n] = x[n] ∗h[n]. The index n here is doing triple duty: in y[n] it marks\nthe time instant at which the result of the convolution is desired; in x[n] and h[n] it is supposed to denote\nthe entire signals x[.] and h[.] respectively; and finally its use in x[n] and h[n] is supposed to convey the time\ninstant at which the result of the convolution is desired. The defect of this notation is made apparent if one\nsubstitutes a number for n, so for example y[0] = x[0] ∗h[0]--where does one go next with the right hand\nside? The notation y[0] = (x ∗h)[0] has no such difficulty. Similarly, the defective notation might encourage\none to \"deduce\" from y[n] = x[n] ∗h[n] that, for instance, y[n -3] = x[n -3] ∗h[n -3], but there is no natural\ninterpretation of the right hand side that can covert this into a correct statement regarding convolution.\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nthe argument is strictly negative). If x[n] = 3δ[n] -δ[n -1], then\ny[n] = 3(0.5)nu[n] -(0.5)n-1u[n -1] .\nFrom this we deduce, for instance, that y[n] = 0 for n < 0, and y[0] = 3, y[1] = 0.5, y[2] =\n(0.5)2, and in fact y[n] = (0.5)n for all n > 0.\nThe above example illustrates that if h[n] = 0 for n < 0, then the system output cannot\ntake nonzero values before the input takes nonzero values. Conversely, if the output never\ntakes nonzero values before the input does, then it must be the case that h[n] = 0 for n < 0.\nIn other words, this condition is necessary and sufficient for causality of the system.\nThe summation in Equation (11.2) that defines convolution involves an infinite number\nof terms in general, and therefore requires some conditions in order to be well-defined.\nOne case in which there is no problem defining convolution is when the system is causal\nand the input is zero for all times less than some finite start time sx, i.e., when the input is\nright-sided. In that case, the infinite sum\ninf\n\nx[k]h[n -k]\nk=-inf\nreduces to the finite sum\nn\n\nx[k]h[n -k] ,\nk=sx\nbecause x[k] = 0 for k < sx and h[n -k] = 0 for k > n.\nThe same reduction to a finite sum occurs if h[n] is just right-sided rather than causal,\ni.e., is 0 for all times less than some finite start time sh, where sh can be negative (if it isn't,\nthen we're back to the case of a causal system). In that case the preceding sum will run\nfrom sx to n -\nsh . Yet another case in this vein involves an input signal or unit sample\nresponse that is nonzero over only a finite interval of time, in which case it almost doesn't\nmatter what the characteristics of the other function are, because the convolution yet again\nreduces to running over the terms in a finite time-window.\nWhen there actually are an infinite number of nonzero terms in the convolution sum,\nthe situation is more subtle. You may recall from discussion of infinite series in your calcu-\nlus course that such a sum is well-defined--independently of the order in which the terms\nare added--precisely when the sum of absolute values (or magnitudes) of the terms in the\ninfinite series is finite. In this case we say that the series is absolutely summable. In the case\nof the convolution sum, what this requires is the following condition:\ninf\n\n|h[m]|.|x[n -m]\nm=\n| < inf\n(11.4)\n-inf\nAn important set of conditions under which this constraint is satisfied is when\n1. the magnitude or absolute value of the input at each instant is bounded for all time\n2The infinite sum also reduces to a finite sum when both x[.] and h[.] are left-sided, i.e., are each zero for\ntimes greater than some finite time; this case is not of much interest in our context.\n\nSECTION 11.2.\nCONVOLUTION FOR LTI MODELS\nby some fixed (finite) number, i.e.,\n|x[n]| ≤μ < inf\nfor all n ,\nand\n2. the unit sample response h[n] is absolutely summable:\ninf\n\n|h[n]| = α <\n.\nn=\ninf\n(11.5)\n-inf\nWith this, it follows that\ninf\n\nn\nm=\n|h[m]|.|x[ -m]| ≤μα < inf,\n-inf\nso it's clear that the convolution sum is well-defined in this case.\nFurthermore, taking the absolute value of the output y[n] in Equation (11.3) shows that\ninf\ninf\n|y[n]|\n=\n\nh[m]x[n -m]\n\nm=\n≤μ\n\nh[m]\n\n-inf\n\nm=\n\n-inf\n\ninf\n≤\nμ\n\nm=\n|h[m]| = μα .\n(11.6)\n-inf\nThus absolute summability of the unit sample response suffices to ensure that, with a\nbounded input, we not only have a well-defined convolution sum but that the output\nis bounded too.\nIt turns out the converse is true also: absolute summability of the unit sample response\nis necessary to ensure that a bounded input yields a bounded output. One way to see this is\nto pick x[n] = sgn{h[-n]} for |n| ≤N and x[n] = 0 otherwise, where the function sgn{·} is\ntakes the sign of its argument, i.e., is +1 or -1 when its argument is respectively positive\nor negative. With this choice, the convolution sum shows that\nN\ny[0] =\n\n|h[n]|\nn=-N\nIf h[·] is not absolutely summable, then y[0] is unbounded as N →inf.\nThe above facts motivate the name that's given to an LTI system with absolutely\nsummable unit sample response h[n], i.e., satisfying Equation (11.5): the system is termed\nbounded-input bounded-output (BIBO) stable. As an illustration, the system in Example\n1 above is evidently BIBO stable, because\nn |h[n]| = 1/(1 -0.5) = 2.\nNote that because convolution is commutative, the roles of x and h can be interchanged.\nIt follows that convolution is well-defined if the input x[.] is absolutely summable and the\nunit sample response h[.] is bounded, rather than the other way around; and again, the\nresult of this convolution is bounded.\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nw[n]\nx[n]\n\ny[n]\ny = h2 ∗w = h2 ∗(\n)\nh1 ∗x = (\n)\nh2 ∗h1 ∗x\nx[n]\ny[n]\n\nx[n]\ny[n]\n\nx[n]\n\ny[n]\nFigure 11-7: LTI systems in series.\n■\n11.2.1\nSeries and Parallel Composition of LTI Systems\nWe have already noted that convolution is commutative, i.e., x ∗h = h ∗x. It turns out that\nit is also associative, i.e.,\n(h2 ∗h1) ∗x = h2 ∗(h1 ∗x) ,\nprovided each of the involved convolutions is well-behaved. Thus the convolutions--\neach of which involves two functions--can be done in either sequence. The direct proof is\nessentially by tedious expansion of each side of the above equation, and we omit it.\nThese two algebraic properties have immediate implications for the analysis of systems\ncomposed of series or cascade interconnections of LTI subsystems, as in Figure 11-7. The fig-\nure shows three LTI systems that are equivalent, in terms of their input-output properties,\nto the system represented at the top. The proof of equivalence simply involves invoking\nassociativity and commutativity.\nA third property of convolution, which is very easy to prove from the definition of\nconvolution, is that it is distributive over addition, i.e.,\n(h1 + h2) ∗x = (h1 ∗x) + (h2 ∗x) ,\nprovided each of the individual convolutions on the right of the equation is well-defined.\nRecall that the addition of two time-functions, as with h1 + h2 in the preceding equation,\nis done pointwise, component by component. Once more, there is an immediate applica-\ntion to an interconnection of LTI subsystems, in this case a parallel interconnection, as in\nFigure 11-8.\n\nSECTION 11.2.\nCONVOLUTION FOR LTI MODELS\ny1[n]\n\nx[n]\n\ny[n]\n\ny2[n]\ny = y1 + y2 = (h1 ∗x)+(h2 ∗x) = (\n)\nh1 + h2 ∗x\nx[n]\ny[n]\n\nFigure 11-8: LTI systems in parallel.\nx[n]\ny[n]=Ax[n-D]\nS\nFigure 11-9: Scale-and-delay LTI system.\nExample 2 (Scale-&-Delay System)\nConsider the system S in Figure 11-9 that scales its\nDT input by A and delays it by D > 0 units of time (or, if D is negative, advances it by\n|D|). This system is linear and time-invariant (as is seen quite directly by applying the\ndefinitions from Chapter 10). It is therefore characterized by its unit sample response,\nwhich is\nh[n] = Aδ[n -D] .\nWe already know from the definition of the system that if the input at time n is x[n], the\noutput is y[n] = Ax[n -D], but let us check that the general expression in Equation (11.2)\ngives us the same answer:\ninf\ninf\ny[n] =\n\nx[k]h[n -k] =\n\nx[k]Aδ[n -k -D] .\nk=-inf\nk=-inf\nAs the summation runs over k, we look for the unique value of k where the argument of\nthe unit sample function goes to zero, because this is the only value of k for which the unit\nsample function is nonzero (and in fact equal to 1). Thus k = n -D, so y[n] = Ax[n -D],\nas expected.\nA general unit sample response h[.] can be represented as a sum--or equivalently, a\nparallel combination--of scale-&-delay systems:\nh[n] = ··· + h[-1]δ[n + 1] + h[0]δ[n] + ··· + h[k]δ[n -k] + ··· .\n(11.7)\nAn input signal x[n] to this system gets scaled and delayed by each of these terms, with the\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nresults added to form the output. This way of looking at the LTI system response yields\nthe expression\ny[n]\n=\n··· + h[-1]x[n + 1] + h[0]x[n] + ··· + h[m]x[n -m] + ···\ninf\n=\n\nh[m]x[n\nm=\n-m] .\n-inf\nThis is the alternate form of convolution sum we obtained in Equation (11.3).\n■\n11.2.2\nFlip-Slide-Dotting Away: Implementing Convolution\nThe above descriptions of convolution explain why we end up with the expressions in\nEquations (11.2) and (11.3) to describe the output of an LTI system in terms of its input\nand unit sample response. We will now describe a graphical construction that helps to\nvisualize and implement these computations, and that is often the simplest way to think\nabout the effects of convolution.\nLet's examine the expression in Equation (11.2), but the same kind of reasoning works\nfor Equation (11.3). Our task is to implement the computation in the summation below:\ninf\ny[n0] =\n\nx[k]h[n0 -k] .\n(11.8)\nk=-inf\nWe've written n0 rather than the n we used before just to emphasize that this computation\ninvolves summing over the dummy index k, with the other number being just a parameter,\nfixed throughout the computation.\nWe first plot the time functions x[k] and h[k] on the k axis (with k increasing to the right,\nas usual). How do we get h[n0 -k] from this? First note that h[-k] is obtained by reversing\nh[k] in time, i.e., a flip of the function across the time origin. To get h[n0 -k], we now slide\nthis reversed time function, h[-k], to the right by n0 steps if n0 ≥0, or to the left by |n0|\nsteps if n0 < 0. To confirm that this prescription is correct, note that h[n0 -k] should take\nthe value h[0] at k = n0.\nWith these two steps done, all that remains is to compute the sum in Equation (11.8).\nThis sum takes the same form as the familiar dot product of two vectors, one of which has\nx[k]\nk\nh[n\nk]\nk\nas its\nth component, and the other of which has\n0 -\nas its\nth component. The\nonly twist here is that the vectors could be infinitely long. So what this steps boils down\nto is taking an instant-by-instant product of the time function x[k] and the time function\nh[n0 -k] that your preparatory \"flip and slide\" step has produced, then summing all the\nproducts.\nAt the end of all this (and it perhaps sounds more elaborate than it is, till you get a\nlittle practice), what you have computed is the value of the convolution for the single value\nn0. To compute the convolution for another value of the argument, say n1, you repeat the\nprocess, but sliding by n1 instead of n0.\nTo implement the computation in Equation (11.3), you do the same thing, except that\nnow it's h[m] that stays as it is, while x[m] gets flipped and slid by n to produce x[n -m],\nafter which you take the dot product. Either way, the result is evidently the same.\n\nSECTION 11.2.\nCONVOLUTION FOR LTI MODELS\nExample 1 revisited\nSuppose again that h[m] = (0.5)mu[m] and x[m] = 3δ[m] -δ[m -1].\nThen\nx[-m] = -δ[-m -1] + 3δ[-m] ,\nwhich is nonzero only at m = -1 and m = 0. (Sketch this!) As a consequence, sliding x[-m]\nto the left, to get x[n -m] when n < 0, will mean that the nonzero values of x[n -m] have\nno overlap with the nonzero values of h[m], so the dot product will yield 0. This establishes\nthat y[n] = (x ∗h)[n] = 0 for n < 0, in this example.\nFor n = 0, the only overlap of nonzero values in h[m] and x[n -m] is at m = 0, and we\nget the dot product to be (0.5)0 × 3 = 3, so y[0] = 3.\nFor n > 0, the only overlap of nonzero values in h[m] and x[n -m] is at m = n -1 and\nm = n, and the dot product evaluates to\ny[n] = -(0.5)n-1 + 3(0.5)n = (0.5)n-1(-1 + 1.5) = (0.5)n .\nSo we have completely recovered the answer we obtained in Example 1. For this example,\nour earlier approach--which involved directly thinking about superposition of scaled and\nshifted unit sample responses--was at least as easy as the graphical approach here, but in\nother situations the graphical construction can yield more rapid or direct insights.\n■\n11.2.3\nDeconvolution\nWe've seen in the previous chapter how having an LTI model for a channel allows us to\npredict or analyze the distorted output y[n] of the channel, in response to a superposition\nof alternating positive and negative steps at the input x[n], corresponding to a rectangular-\nwave baseband signal. That analysis was carried out in terms of the unit step response,\ns[n], of the channel.\nWe now briefly explore one plausible approach to undoing the distortion of the channel,\nassuming we have a good LTI model of the channel. This discussion is most naturally\nphrased in terms of the unit sample response of the channel rather than the unit step re-\nsponse. The idea is to process the received baseband signal y[n] through an LTI system, or\nLTI filter, that is designed to cancel the effect of the channel.\nConsider a simple channel that we model as LTI with unit sample function\nh1[n] = δ[n] + 0.8δ[n -1] .\nThis is evidently a causal model, and we might think of the channel as one that transmits\nperfectly and instantaneously along some direct path, and also with a one-step delay and\nsome attenuation along some echo path.\nSuppose our receiver filter is to be designed as a causal LTI system with unit sample\nresponse\nh2[n] = h2[0]δ[n] + h2[1]δ[n -1] + ··· + h2[k]δ[n -k] + ··· .\n(11.9)\nIts input is y[n], and let us label its output as z[n]. What conditions must h2[n] satisfy\nif we are to ensure that z[n] = x[n] for all inputs x[n], i.e., if we are to undo the channel\ndistortion?\nAn obvious place to start is with the case where x[n] = δ[n]. If x[n] is the unit sample\nfunction, then y[n] is the unit sample response of the channel, namely h1[n], and z[n] will\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nthen be given by z[n] = (h2 ∗h1)[n]. In order to have this be the input that went in, namely\nx[n] = δ[n], we need\n(h2 ∗h1)[n] = δ[n] .\n(11.10)\nAnd if we satisfy this condition, then we will actually have z[n] = x[n] for arbitrary x[n],\nbecause\nz = h2 ∗(h1 ∗x) = (h2 ∗h1) ∗x = δ0 ∗x = x ,\nwhere δ0[.] is our alternative notation for the unit sample function δ[n]. The last equality\nabove is a consequence of the fact that convolving any signal with the unit sample function\nyields that signal back again; this is in fact what Equation (11.1) expresses.\nThe condition in Equation (11.10) ensures that the convolution carried out by the chan-\nnel is inverted or undone, in some sense, by the filter. We might say that the filter de-\nconvolves the output of the system to get the input (but keep in mind that it does this by\na further convolution!). In view of Equation (11.10), the function h2[.] is also termed the\nconvolutional inverse of h1[.], and vice versa.\nSo how do we find h2[n] to satisfy Equation (11.10)? It's not by a simple division of any\nkind (though when we get to doing our analysis in the frequency domain shortly, it will\nindeed be as simple as division). However, applying the \"flip-slide-dot product\" mantra\nfor computing a convolution, we find the following equations for the unknown coefficients\nh2[k]:\n1 · h2[0] = 1\n0.8 · h2[0] + 1 · h2[1] = 0\n0.8 · h2[1] + 1 · h2[2] = 0\n...\n0.8 · h2[k -1] + 1 · h2[k] = 0\n... ,\nfrom which we get h2[0] = 1, h2[1] = -0.8, h2[2] = -0.8h2[1] = (-0.8)2, and in general\nh2[k] = (-0.8)ku[k].\nDeconvolution as above would work fine if our channel model was accurate, and if\nthere were no noise in the channel. Even assuming the model is sufficiently accurate, note\nthat any noise process w[.] that adds in at the output of the channel will end up adding\nv[n] = (h2 ∗w)[n] to the noise-free output, which is z[n] = x[n]. This added noise can com-\npletely overwhelm the solution. For instance, if both x[n] and w[n] are unit samples, then\nthe output of the receiver's deconvolution filter has a noise-free component of δ[n] and\nan additive noise component of (-0.8)nu[n] that dwarfs the noise-free part. After we've\nunderstood how to think about LTI systems in the frequency domain, it will become much\nclearer why such deconvolution can be so sensitive to noise.\n\nSECTION 11.3.\nRELATING THE UNIT STEP RESPONSE TO THE UNIT SAMPLE RESPONSE\n■\n11.3\nRelating the Unit Step Response to the Unit Sample\nResponse\nSince\nδ[n] = u[n] -u[n -1]\nit follows that for an LTI system the unit sample response h[n] and the unit step response\ns[n] are simply related:\nh[n] = s[n] -s[n -1] .\nThis relation is a consequence of applying superposition and invoking time-invariance.\nHence, for a causal system that has h[k] = 0 and s[k] = 0 for k < 0, we can invert this\nrelationship to write\nn\ns[n] =\n\nh[k] .\n(11.11)\nWe can therefore very simply determine the unit step response from the unit sample re-\nsponse. It is also follows from Equation (11.11) that the time it takes for the unit step\nresponse s[n] to settle to its final value is precisely the time it takes for the unit sample\nresponse h[n] to settle back down to 0 and stay there.\nWhen the input to an LTI system is a sum of scaled and delayed unit steps, there is no\nneed to invoke the full machinery of convolution to determine the system output. Instead,\nknowing the unit step response s[n], we can again simply apply superposition and invoke\ntime-invariance.\nWe describe next a tool for examining the channel response under this ISI, and for set-\nting parameters at the transmitter and receiver.\n■\n11.4\nEye Diagrams\nOn the face of it, ISI is a complicated effect because the magnitude of bit interference and\nthe number of interfering bits depend both on the channel properties and on how bits are\nrepresented on the channel. Figure 11-11 shows an example of what the receiver sees (bot-\ntom) in response to what the transmitter sent (top) over a channel with ISI but no noise.\nEye diagrams (or \"eye patterns\") are a useful graphical tool in the toolkit of a communi-\ncations system designer or engineer to understand ISI. We will use this tool to determine\nwhether the number of samples per bit is large enough to enable the receiver to determine\n\"0\"s and \"1\"s reliably from the demodulated (and filtered) sequence of received voltage\nsamples.\nTo produce an eye diagram, one begins with the channel output that results from a long\nstretch of on-off signaling, as in the bottom part of Figure 11-11, then essentially slices this\nup into smaller segments, say 3 bit-slots long, and overlays all the resulting segments. The\nresult spans the range of waveform variations one is likely to see over any 3 bit-slots at the\noutput. A more detailed prescription follows.\nTake all the received samples and put them in an array of lists, where the number of\nlists in the array is equal to the number of samples in k bit periods. In practice, we want k\nto be a small positive integer like 3. If there are s samples per bit, the array is of size k · s.\nEach element of this array is a list, and element i of the array is a list of the received\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\n\n!\"# $ %\n\n\"\n\"\n\n&\"# $ %\n\n'\n\n'\n\nFigure 11-10: Eye diagrams for a channel with a slow rise/fall for 33 (top) and 20 (bottom) samples per bit.\nNotice how the eye is wider when the number of samples per bit is large, because each step response has\ntime to settle before the response to the next step appears.\nsamples y[i],y[i + ks],y[i + 2ks],.... Now suppose there were no ISI at all (and no noise).\nThen all the samples in the ith list corresponding to a transmitted \"0\" bit would have\nthe same voltage value, and all the samples in the ith list corresponding to a transmitted\n\"1\" would have the same value. Consider the simple case of just a little ISI, where the\nprevious bit interferes with the current bit, and there's no further impact from the past.\nThen the samples in the ith list corresponding to a transmitted \"0\" bit would have two\ndistinct possible values, one value associated with the transmission of a \"10\" bit sequence,\nand one value associated with a \"00\" bit sequence. A similar story applies to the samples\nin the ith list corresponding to a transmitted \"1\" bit, for a total of four distinct values for\nthe samples in the ith list. If there is more ISI, there will be more distinct values in the ith\nlist of samples. For example, if two previous bits interfere, then there will be eight distinct\nvalues for the samples in the ith list. If three bits interfere, then the ith list will have 16\n\nSECTION 11.4.\nEYE DIAGRAMS\n\nFigure 11-11: Received signals in the presence of ISI. Is the number of samples per bit \"just right\"? And\nwhat threshold should be used to determine the transmitted bit? It's hard to answer these question from\nthis picture. An eye diagram sheds better light.\ndistinct values, and so on.\nWithout knowing the number of interfering bits, to capture all the possible interactions,\nwe must produce the above array of lists for every possible combination of bit sequences\nthat can ever be observed. If we were to plot this array on a graph, we will see a picture\nlike the one shown in Figure 11-10. This picture is an eye diagram.\nIn practice, we can't produce every possible combination of bits, but what we can do is\nuse a long random sequence of bits. We can take the random bit sequence, convert it in to\na long sequence of voltage samples, transmit the samples through the channel, collect the\nreceived samples, pack the received samples in to the array of lists described above, and\nthen plot the result. If the sequence is long enough, and the number of interfering bits is\nsmall, we should get an accurate approximation of the eye diagram.\nBut what is \"long enough\"?\nWe can answer this question and develop a less ad hoc procedure by using the proper-\nties of the unit sample response, h[n]. The idea is that the sequence h[0],h[1],...,h[n],...\ncaptures the complete noise-free response of the channel. If h[k] ≈0 for k > l, then we\ndon't have to worry about samples more than lin the past. Now, if the number of samples\nper bit is s, then the number of bits in the past that can affect the present bit is no larger\nthan l/s, where lis the length of the non-zero part of h[·]. Hence, it is enough to generate\nall bit patterns of length B = l/s, and send them through the channel to produce the eye\ndiagram. In practice, because noise can never be eliminated, one might be a little conser-\nvative and pick B = (l/n) + 2, slightly bigger than what a noise-free calculation would\nindicate. Because this approach requires 2B bit patterns to be sent, it might be unreason-\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\nable for large values of B; in those cases, it is likely that s is too small, and one can find\nwhether that is so by sending a random subset of the 2B possible bit patterns through the\nchannel.\nFigure 11-10 shows the width of the eye, the place where the diagram has the largest dis-\ntinction between voltage samples associated with the transmission of a '0' bit and those\nassociated with the transmission of a '1' bit. Another point to note about the diagrams\nis the \"zero crossing\", the place where the upward rising and downward falling curves\ncross. Typically, as the degree of ISI increases (i.e., the number of samples per bit is re-\nduced), there is a greater degree of \"fuzziness\" and ambiguity about the location of this\nzero crossing.\nThe eye diagram is an important tool, useful for verifying some key design and opera-\ntional decisions:\n1. Is the number of samples per bit large enough? If it is large enough, then at the center\nof the eye, the voltage samples associated with transmission of a '1' are clearly above\nthe digitization threshold and the voltage samples associated with the transmission\nof a '0' are clearly below. In addition, the eye must be \"open\" enough that small\namounts of noise will not lead to errors in converting bit detection samples to bits.\nAs will become clear later, it is impossible to guarantee that noise will never cause\nerrors, but we can reduce the likelihood of error.\n2. Has the value of the digitization threshold been set correctly? The digitization thresh-\nold should be set to the voltage value that evenly divides the upper and lower halves\nof the eye, if 0's and 1's are equally likely. We didn't study this use of eye diagrams,\nbut mention it because it is used in practice for this purpose as well.\n3. Is the sampling instant at the receiver within each bit slot appropriately picked? This\nsampling instant should line up with where the eye is most open, for robust detection\nof the received bits.\n■\nProblems and Questions\n1. Each of the following equations describes the relationship that holds between the\ninput signal x[.] and output signal y[.] of an associated discrete-time system, for all\nintegers n. In each case, explain whether or not the system is (i) causal , (ii) linear,\n(iii) time-invariant.\n(a)\ny[n] = 0.5x[n] + 0.5x[n -1].\n(b)\ny[n] = x[n + 1] + 7.\n(c)\ny[n] = cos(3n)x[n -2].\n(d)\ny[n] = x[n]x[n -1].\n(e)\nn\ny[n] =\nk=13\n(f)\ny[n] = x[-n].\nx[k]\nfor n\n13, otherwise y[n] = 0.\n\n≥\n\nSECTION 11.4.\nEYE DIAGRAMS\n2. Suppose the unit step response s[n] of a particular linear, time-invariant (LTI) commu-\nnication channel is given by\ns[n] =\n\n1 -\nn\nu[n] ,\nwhere u[n] denotes the unit step function: u[n] = 1 for n ≥0, and u[n] = 0 for n < 0.\n(a) Draw a labeled sketch of the above unit step response s[n] for 0 ≤n ≤4.\n(b) Suppose the input x[n] to this channel is given by\nx[n]\n=\nfor n = 0,1,2,\n=\nfor all other n.\nDraw a labeled sketch of this x[n] for n in the range -1 to 5.\n(c) With the x[.] from Part (b), determine the value of the output y[n] at times n = 1\nand n = 4. Explain your reasoning.\n(d) Determine the unit sample response h[n] of the channel, explaining how you\narrived at it.\nAlso draw a labeled sketch of h[n] for 0 ≤n ≤4.\n(e) Is this channel bounded-input bounded-output stable? Explain your answer.\n3. (By Vladimir Stojanovic) A line-of-sight channel can be represented as an LTI system\nwith a unit sample response:\nh[n] = aδ[n -M] ,\nwhere M is the channel delay, and M > 0.\n(a) Is this channel causal? Explain.\n(b) Write an expression for the unit step response s[n] of this system.\n4. (By Vladimir Stojanovic) A channel with echo can be represented as an LTI system with\na unit sample response:\nh[n] = aδ[n -M] + bδ[n -N] ,\n(11.12)\nwhere M is the channel delay, N is the echo delay, and N > M.\n(a) Derive the unit step response s[n] of this channel with echo.\n(b) Two such channels, with unit sample responses\nh1[n] = δ[n] + 0.1δ[n -2]\nand\nh2[n] = δ[n -1] + 0.2δ[n -2],\nare cascaded in series.\ni. Derive the unit sample response h12[n] of the cascaded system.\nii. Derive the unit step response s12[n] of the cascaded system.\n\nCHAPTER 11.\nLTI MODELS AND CONVOLUTION\n(c) The transmitter maps each bit to Nb samples using bipolar signaling (bit 0 maps\nto Nb samples of value -1, and bit 1 maps to Nb samples of value +1). The\nmapped samples are sent over the channel with echo, with unit sample response\ngiven by Equation (11.12), with a > b > 0, Nb = 4, N = Nb, and M = 0.\nSketch the output of the channel for the input bit sequence 01. The initial condi-\ntion before the 01 bit sequence is that the input to the channel was a long stream\nof zeroes. Clearly mark the signal levels on the y-axis, as well as sample indices\non the x-axis.\n5. (By Yury Polyanskiy.) Explain whether each of the following statements is true or\nfalse.\n(a) Let S be the LTI system that delays signal by D. Then h ∗S(x) = S(h) ∗x for\nany signals h and x.\n(b) Adding a delay by D after LTI system h[n] is equivalent to replacing h[n] with\nh[n -D].\n(c) if h ∗x[n] = 0 for all n then necessarily one of signals h[·] or x[·] is zero.\n(d) LTI system is causal if and only if h[n] = 0 for n < 0.\n(e) LTI system is causal if and only if u[n] = 0 for n < 0.\n(f) For causal LTI h[n] is zero for all large enough n if and only if u[n] becomes\nconstant for all large enough n.\n(g) s[n] is zero for all n ≤n0 and then monotonically grows for n > n0 if and only if\nh[n] is zero for all n ≤n0 and then non-negative for n > n0.\n6. (By Yury Polyanskiy.) If h[n] is non-zero only inside interval [-10,10] and x[n] is\nnon-zero only on [20,35], which samples of y[] may be non zero if y[n] = (h ∗x)[n]?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 12: Frequency Response of LTI Systems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/43ab3e7240ebab8bbdcde3a8601abdac_MIT6_02F12_chap12.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 3, 2012\nCHAPTER 12\nFrequency Response of LTI Systems\nSinusoids--and their close relatives, the complex exponentials--play a distinguished role\nin the study of LTI systems. The reason is that, for an LTI system, a sinusoidal input gives\nrise to a sinusoidal output again, and at the same frequency as the input. This property is not\nobvious from anything we have said so far about LTI systems. Only the amplitude and\nphase of the sinusoid might be, and generally are, modified from input to output, in a way\nthat is captured by the frequency response of the system, which we introduce in this chapter.\n- 12.1\nSinusoidal Inputs\nBefore focusing on sinusoidal inputs, consider an input that is periodic but not necessarily\nsinusoidal. A signal x[n] is periodic if\nx[n + P] = x[n] for all n ,\nwhere P is some fixed positive integer. The smallest positive integer P for which this\ncondition holds is referred to as the period of the signal (though the term is also used at\ntimes for positive integer multiples of P), and the signal is called P-periodic.\nWhile it may not be obvious that sinusoidal inputs to LTI systems give rise to sinusoidal\noutputs, it's not hard to see that periodic inputs to LTI systems give rise to periodic outputs\nof the same period (or an integral fraction of the input period). The reason is that if the P-\nperiodic input x[.] produces the output y[.], then time-invariance of the system means that\nshifting the input by P will shift the output by P. But shifting the input by P leaves the\ninput unchanged, because it is P-periodic, and therefore must leave the output unchanged,\nwhich means the output must be P-periodic. (This argument actually leaves open the\npossibility that the period of the output is P/K for some integer K, rather than actually\nP-periodic, but in any case we will have y[n + P] = y[n] for all n.)\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\n-\n12.1.1\nDiscrete-Time Sinusoids\nA discrete-time (DT) sinusoid takes the form\nx[n] = cos(Ω0n + θ0) ,\n(12.1)\nWe refer to Ω0 as the angular frequency of the sinusoid, measured in radians/sample; Ω0 is\nthe number of radians by which the argument of the cosine increases when n increases by\n1. (It should be clear that we can replace the cos with a sin in Equation (12.1), because cos\nand sin are essentially equivalent except for a pi/2 phase shift.)\nNote that the lowest rate of variation possible for a DT signal is when it is constant,\nand this corresponds, in the case of a sinusoidal signal, to setting the frequency Ω0 to\n0. At the other extreme, the highest rate of variation possible for a DT signal is when it\nalternates signs at each time step, as in (-1)n . A sinusoid with this property is obtained\nby taking Ω0 = ±π, because cos(±πn) = (-1)n . Thus all the action of interest with DT\nsinusoids happens in the frequency range [-π,π]. Outside of this interval, everything\nrepeats periodically in Ω0, precisely because adding any integer multiple of 2π to Ω0 does\nnot change the value of the cosine in Equation (12.1).\nIt can be helpful to consider this DT sinusoid as derived from an underlying continuous-\ntime (CT) sinusoid cos(ω0t + θ0) of period 2π/ω0, by sampling it at times t = nT that are\ninteger multiples of some sampling interval T. Writing\ncos(Ω0n + θ0) = cos(ω0nT + θ0)\nthen yields the relation Ω0 = ω0T (with the constraint |ω0| ≤π/T, to reflect |Ω0| ≤π). It is\nnow natural to think of 2π/(ω0T) = 2π/Ω0 as the period of the DT sinusoid, measured in\nsamples. However, 2π/Ω0 may not be an integer!\nNevertheless, if 2π/Ω0 = P/Q for some integers P and Q, i.e., if 2π/Ω0 is rational,\nthen indeed x[n + P] = x[n] for the signal in Equation (12.1), as you can verify quite eas\nily. On the other hand, if 2π/Ω0 is irrational, the DT sequence in Equation (12.1) will\nnot actually be periodic: there will be no integer P such that x[n + P] = x[n] for all n.\nFor example, cos(3πn/4) has frequency 3π/4 radians/sample and a period of 8, because\n2π/3π/4 = 8/3 = P/Q, so the period, P, is 8. On the other hand, cos(3n/4) has frequency\n3/4 radians/sample, and is not periodic as a discrete-time sequence because 2π/3/4 = 8π/3\nis irrational. We could still refer to 8π/3 as its \"period\", because we can think of the se\nquence as arising from sampling the periodic continuous-time signal cos(3t/4) at integral\nvalues of t.\nWith all that said, it turns out that the response of an LTI system to a sinusoid of the\nform in Equation (12.1) is a sinusoid of the same (angular) frequency Ω0, whether or not\nthe sinusoid is actually DT periodic. The easiest way to demonstrate this fact is to rewrite\nsinusoids in terms of complex exponentials.\n-\n12.1.2\nComplex Exponentials\nThe relation between complex exponentials and sinusoids is captured by Euler's famous\nidentity:\nejφ = cos φ + j sin φ .\n(12.2)\n\nSECTION 12.2. FREQUENCY RESPONSE\n√\nwhere j = -1. ejφ represents a complex number (or a point in the complex plane) that has\na real component of cos φ and an imaginary component of sin φ. It therefore has magnitude\n1 (because cos2 φ + sin2 φ = 1), and makes an angle of φ with the positive real axis. In other\nwords, ejφ is the point on the unit circle in the complex plane (i.e., at radius 1 from the\norigin) and at an angle of φ relative to the positive real axis.\nA short refresher on complex numbers may be worthwhile.\nThe complex number c = a + jb can be thought of as the point (a, b) in the plane,\n√\nand accordingly has magnitude |c| =\na2 + b2 and angle with the positive real axis of\n∠c = arctan(b/a). Note that a = |c| cos(∠c) and b = |c| sin(∠c). Hence, in view of Euler's\nidentity, we can also write the complex number in so-called polar form, c = |c|.ej∠c; this\nrepresents a point at distance |c| from the origin, at an angle of ∠c.\nThe extra thing you can do with complex numbers, which you cannot do with just\npoints in the plane, is multiply them. And the polar representation shows that the product\nof two complex numbers c1 and c2 is\nj∠c1\nj∠c2\nj(∠c1+∠c2)\nc1.c2 = |c1|.e\n.|c2|.e\n= |c1|.|c2|.e\n,\ni.e., the magnitude of the product is the product of the individual magnitudes, and the\nangle of the product is the sum of the individual angles. It also follows that the inverse of a\ncomplex number c has magnitude 1/|c| and angle -∠c.\nSeveral other identities follow from Euler's identity above. Most importantly,\n(\n)\n(\n)\n(\n)\nj\njφ\n-jφ\njφ - e -jφ\n-jφ - ejφ\ncos φ =\ne\n+ e\nsin φ =\ne\n=\ne\n.\n(12.3)\n2j\nAlso, writing\njA jB\nj(A+B)\ne\ne\n= e\n,\nand then using Euler's identity to rewrite all three of these complex exponentials, and\nfinally multiplying out the left hand side, generates various useful identities, of which we\nonly list two:\n(\n)\ncos(A) cos(B) =\ncos(A + B) + cos(A - B)\n;\ncos(A ∓ B) = cos(A) cos(B) ± sin(A) sin(B) .\n(12.4)\n- 12.2\nFrequency Response\nWe are now in a position to determine what an LTI system does to a sinusoidal input.\nThe streamlined approach to this analysis involves considering a complex input of the form\nx[n] = ej(Ω0n+θ0) rather than x[n] = cos(Ω0n + θ0). The reasoning and mathematical calcu\nlations associated with convolution work as well for complex signals as they do for real\nsignals, but the complex exponential turns out to be somewhat easier to work with (once\nyou are comfortable working with complex numbers)--and the results for the real sinu\nsoidal signals we are interested in can then be extracted using identities such as those in\nEquation (12.3).\nIt may be helpful, however, to first just plough in and do the computations directly,\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nsubstituting the real sinusoidal x[n] from Equation (12.1) into the convolution expression\nfrom the previous chapter, and making use of Equation (12.4). The purpose of doing this is\nto (i) convince you that it can be done entirely with calculations involving real signals; and\n(ii) help you appreciate the efficiency of the calculations with complex exponentials when\nwe get to them.\nThe direct approach mentioned above yields\ninf\nf\ny[n]\n=\n\nh[m]x[n - m]\nm=-inf\ninf\nf\n(\n)\n=\nh[m]cos Ω0(n - m) + θ0\nm=-inf\n(\ninf\n)\nf\n=\nh[m]cos(Ω0m) cos(Ω0n + θ0)\nm=-inf\n(\ninf\n)\nf\n+\nh[m] sin(Ω0m) sin(Ω0n + θ0)\nm=-inf\n= C(Ω0)cos(Ω0n + θ0) + S(Ω0) sin(Ω0n + θ0) ,\n(12.5)\nwhere we have introduced the notation\ninf\ninf\nf\nf\nC(Ω) =\nh[m]cos(Ωm) ,\nS(Ω) =\nh[m] sin(Ωm) .\n(12.6)\nm=-inf\nm=-inf\nNow define the complex quantity\nH(Ω) = C(Ω) - jS(Ω) = |H(Ω)|.exp{j∠H(Ω)} ,\n(12.7)\nwhich we will call the frequency response of the system, for a reason that will emerge\nimmediately below. Then the result in Equation (12.5) can be rewritten, using the second\nidentity in Equation (12.4), as\n[\n]\ny[n]\n=\n|H(Ω0)|. cos ∠H(Ω0).cos(Ω0n + θ0) - sin ∠H(Ω0) sin(Ω0n + θ0)\n(\n)\n= |H(Ω0)|.cos Ω0n + θ0 + ∠H(Ω0)\n.\n(12.8)\nThe result in Equation (12.8) is fundamental and important! It states that the entire effect\nof an LTI system on a sinusoidal input at frequency Ω0 can be deduced from the (com\nplex) frequency response evaluated at the frequency Ω0. The amplitude or magnitude of\nthe sinusoidal input gets scaled by the magnitude of the frequency response at the input\nfrequency, and the phase gets augmented by the angle or phase of the frequency response\nat this frequency.\nNow consider the same calculation as earlier, but this time with complex exponentials.\nSuppose\nj(Ω0n+θ0)\nx[n] = A0e\nfor all n .\n(12.9)\n\nSECTION 12.2. FREQUENCY RESPONSE\nConvolution then yields\ninf\nf\ny[n]\n=\n\nh[m]x[n - m]\nm=-inf\n=\ninf\nf\nh[m]A0e\nj\n(\nΩ0(n-m)+θ0\n)\nm=-inf\ninf\n=\n( f\nm=-inf\nh[m]e -jΩ0m)\nA0ej(Ω0n+θ0) .\n(12.10)\nThus the output of the system, when the input is the (everlasting) exponential in Equation\n(12.9), is the same exponential, except multiplied by the following quantity evaluated at\nΩ = Ω0:\ninf\nf\nh[m]e -jΩm = C(Ω) - jS(Ω) = H(Ω) .\n(12.11)\nm=-inf\nThe first equality above comes from using Euler's equality to write e-jΩm = cos(Ωm) -\nj sin(Ωm), and then using the definitions in Equation (12.6). The second equality is simply\nthe result of recognizing the frequency response from the definition in Equation (12.7).\nTo now determine was happens to a sinusoidal input of the form in Equation (12.1), use\nEquation (12.3) to rewrite it as\n(\n)\nA0\nj(Ω0n+θ0)\n-j(Ω0n+θ0)\nA0 cos(Ω0n + θ0) =\ne\n+ e\n,\nand then superpose the responses to the individual exponentials (we can do that because\nof linearity), using the result in Equation (12.10). The result (after algebraic simplification)\nwill again be the expression in Equation (12.8), except scaled now by an additional A0,\nbecause we scaled our input by this additional factor in the current derivation.\nTo succinctly summarize the frequency response result explained above:\nIf the input to an LTI system is a complex exponential, ejΩn, then the output is\nH(Ω)ejΩn, where H(Ω) is the frequency response of the LTI system.\nExample 1 (Moving-Average Filter) Consider an LTI system with unit sample response\nh[n] = h[0]δ[n] + h[1]δ[n - 1] + h[2]δ[n - 2] .\nBy convolving this h[·] with the input signal x[·], we see that\ny[n] = (h ∗ x)[n] = h[0]x[n] + h[1]x[n - 1] + h[2]x[n - 2] .\n(12.12)\nThe system therefore produces an output signal that is the \"3-point weighted moving\naverage\" of the input. The example in Figure 12-1 is of this form, with equal weights of\nh[0] = h[1] = h[2] = 1/3, producing the actual (moving) average.\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nFigure 12-1: Three-point weighted moving average: h and the frequency response, H.\nThe frequency response of the system, from the definition in Equation (12.11), is thus\n-jΩ\n-j2Ω\nH(Ω) = h[0] + h[1]e\n+ h[2]e\n.\nConsidering the case where h[0] = h[1] = h[2] = 1/3, the frequency response can be rewrit\nten as\n(\n)\n-jΩ\n-jΩ\nH(Ω)\n= 1 e\nejΩ + 1 + e\n=\n1 e -jΩ(1 + 2cosΩ) .\n(12.13)\nNoting that |e-jΩ| = 1, it follows from the preceding equation that the magnitude of H(Ω)\nis\n|H(Ω)| = |1 + 2cosΩ| ,\nwhich is consistent with the plot on the right in Figure 12-1: it takes the value 1 at Ω = 0,\nthe value 0 at Ω = arccos(-1 ) = 2π , and the value 1 at Ω = ±π. The frequencies at which\n|H(Ω)| = 0 are referred to as the zeros of the frequency response; in this moving-average\nexample, they are at Ω = ± arccos(-1 ) = ± 2π .\nFrom Equation (12.13), we see that the angle of H(Ω) is -Ω for those values of Ω where\n1 + 2cosΩ > 0; this is the angle contributed by the term e-jΩ . For frequencies where 1 +\n2 cos Ω < 0, we need to add or subtract (it doesn't matter which) π radians to -Ω, because\n-1 = e±jπ. Thus\n{\n-Ω for |Ω| < 2π/3\n∠H(Ω) =\n-Ω ± π for (2π/3) < |Ω| < π\nExample 2 (The Effect of a Time Shift) What does shifting h[n] in time do to the fre\nquency response H(Ω)? Specifically, suppose\nhD[n] = h[n - D] ,\nso hD[n] is a time-shifted version of h[n]. How does the associated frequency response\nHD(Ω) relate to H(Ω)?\nFrom the definition of frequency response in Equation (12.11), we have\ninf\ninf\ninf\nf\nf\nf\n-jΩm\n-jΩm\n-jΩD\n-jΩn\nHD(Ω) =\nhD[m]e\n=\nh[m - D]e\n= e\nh[n]e\n,\nm=-inf\nm=-inf\nn=-inf\n\nSECTION 12.2. FREQUENCY RESPONSE\nwhere the last equality is simply the result of the change of variables m -D = n, so m =\nn + D. It follows that\nHD(Ω) = e -jΩDH(Ω) .\nEquivalently,\n|HD(Ω)| = |H(Ω)|\nand\n∠HD(Ω) = -ΩD + ∠H(Ω) ,\nso the frequency response magnitude is unchanged, and the phase is modified by an addi\ntive term that is linear in Ω, with slope -D.\nAlthough we have introduced the notion of a frequency response in the context of what\nan LTI system does to a single sinusoidal input, superposition will now allow us to use\nthe frequency response to describe what an LTI system does to any input made up of a\nlinear combination of sinusoids at different frequencies. You compute the (sinusoidal) response\nto each sinusoid in the input, using the frequency response at the frequency of that sinusoid.\nThe system output will then be the same linear combination of the individual sinusoidal\nresponses.\nAs we shall see in the next chapter, when we use Fourier analysis to introduce the\nnotion of the spectral content or frequency content of a signal, the class of signals that can be\nrepresented as a linear combination of sinusoids at assorted frequencies is very large. So\nthis superposition idea ends up being extremely powerful.\nExample 3 (Response to Weighted Sum of Two Sinusoids) Consider an LTI system with\nfrequency response H(Ω), and assume its input is the signal\nπ\nπ\nx[n] = 5 sin( n + 0.2) + 11cos( n -0.4) .\nThe system output is then\n(π\n)\n(π\n)\nπ\nπ\nπ\nπ\ny[n] = |H(\n)|.5sin\nn + 0.2 + ∠H(\n) + |H(\n)|.11 cos\nn -0.4 + ∠H(\n)\n.\n-\n12.2.1\nProperties of the Frequency Response\nExistence The definition of the frequency response in terms of h[m] and sines and cosines\nin Equation (12.7), or equivalently in terms of h[m] and complex exponentials in Equation\n(12.11), generally involves summing an infinite number of terms, so again (just as with\nconvolution) one needs conditions to guarantee that the sum is well-behaved. One case,\nof course, is where h[m] is nonzero at only a finite number of time instants, in which case\nthere is no problem with the sum. Another case is when the function h[·] is absolutely\nsummable,\ninf\nf\n|h[n]| ≤μ < inf,\nn=-inf\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nas this ensures that the sum defining the frequency response is itself absolutely summable.\nThe absolute summability of h[·] is the condition for bounded-input bounded-output\n(BIBO) stability of an LTI system that we obtained in the previous chapter. It turns out\nthat under this condition the frequency response is actually a continuous function of Ω.\nVarious other important properties of the frequency response follow quickly from the\ndefinition.\nPeriodicity in Ω Note first that H(Ω) repeats periodically on the frequency (Ω) axis, with\nperiod 2π, because a sinusoidal or complex exponential input of the form in Equation (12.1)\nor (12.9) is unchanged when its frequency is increased by any integer multiple of 2π. This\ncan also be seen from Equation (12.11), the defining equation for the frequency response.\nIt follows that only the interval |Ω| ≤ π is of interest.\nLowest Frequency An input at the frequency Ω = 0 corresponds to a constant (or \"DC\",\nwhich stands for direct current, but in this context just means \"constant\") input, so\ninf\nf\nH(0) =\nh[n]\n(12.14)\nn=-inf\nis the DC gain of the system, i.e., the gain for constant inputs.\nHighest Frequency At the other extreme, a frequency of Ω = ±π corresponds to an input\nof the form (-1)n, which is the highest-frequency variation possible for a discrete-time\nsignal, so\ninf\nf\nH(π) = H(-π) =\n(-1)nh[n]\n(12.15)\nn=-inf\nis the high-frequency gain of the system.\nSymmetry Properties for Real h[n] We will only be interested in the case where the unit\nsample response h[·] is a real (rather than complex) function. Under this condition, the\ndefinition of the frequency response in Equations (12.7), (12.6) shows that the real part of\nthe frequency response, namely C(Ω), is an even function of frequency, i.e., has the same value\nwhen Ω is replaced by -Ω. This is because each cosine term in the sum that defines C(Ω)\nis an even function of Ω.\nSimilarly, for real h[n], the imaginary part of the frequency response, namely -S(Ω), is an\nodd function of frequency, i.e., gets multiplied by -1 when Ω is replaced by -Ω. This is\nbecause each sine term in the sum that defines S(Ω) is an odd function of Ω.\nIn this discussion, we have used the property that h[·] is real, so C and S are also both\nreal, and correspond to the real and imaginary parts of the frequency response, respec\ntively.\nIt follows from the above facts that for a real h[n] the magnitude |H(Ω)| of the frequency\nresponse is an even function of Ω, and the angle ∠H(Ω) is an odd function of Ω.\nYou should verify that the claimed symmetry properties indeed hold for the h[·] in Ex\nample 1 above.\n\nSECTION 12.2. FREQUENCY RESPONSE\nReal and Even h[n] Equations (12.7) and (12.6) also directly show that if the real unit\nsample response h[n] is an even function of time, i.e., if h[- n] = h[n], then the associated\nfrequency response must be purely real. The reason is that the summation defining S (Ω),\nwhich yields the imaginary part of H(Ω), involves the product of the even function h[m]\nwith the odd function sin(Ωm), which is thus an odd function of m, and hence sums to 0.\nReal and Odd h[n] Similarly if the real unit sample response h[n] is an odd function of time,\ni.e., if h[- n] = - h[n], then the associated frequency response must be purely imaginary.\nFrequency Response of LTI Systems in Series We have already seen that a cascade or\nseries combination of two LTI systems, the first with unit sample response h1[· ] and the\nsecond with unit sample response h2[· ], results in an overall system that is LTI, with unit\nsample response (h2 ∗ h1)[· ] = (h1 ∗ h2)[· ].\nTo determine the overall frequency response of the system, imagine applying an (ever\nlasting) exponential input of the form x[n] = AejΩn to the first subsystem. Its output will\nthen be w[n] = H1(Ω) · AejΩn, which is again an exponential of the same form, just scaled\nby the frequency response of the first system. Now with w[n] as the input to the second\nsystem, the output of the second system will be y[n] = H2(Ω) · H1(Ω) · AejΩn . It follows\nthat the overall frequency response H(Ω) is given by\nH(Ω) = H2(Ω)H1(Ω) = H1(Ω)H2(Ω) .\nThis is the first hint of a more general result, namely that convolution in time corresponds\nto multiplication in frequency:\nh[n] = (h1 ∗ h2)[n] ←→ H(Ω) = H1(Ω)H2(Ω) .\n(12.16)\nThis result makes frequency-domain methods compelling in the analysis of LTI systems--\nsimple multiplication, frequency by frequency, replaces the more complicated convolution\nof two complete signals in the time-domain. We will see this in more detail in the next\nchapter, after we introduce Fourier analysis methods to describe the spectral content of\nsignals.\nFrequency Response of LTI Systems in Parallel Using the same sort of argument as\nin the previous paragraph, the frequency response of the system obtained by placing the\ntwo LTI systems above in parallel rather than in series results in an overall system with\nfrequency response H(Ω) = H1(Ω) + H2(Ω), so\nh[n] = (h1 + h2)[n] ←→ H(Ω) = H1(Ω) + H2(Ω) .\n(12.17)\nGetting h[n] From H(Ω) As a final point, we examine how h[n] can be determined from\nH(Ω). The relationship we obtain here is crucial to designing filters with a desired or\nspecified frequency response. It also points the way to the results we develop in the next\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nchapter, showing how time-domain signals -- in this case h[·] -- can be represented as\nweighted combinations of exponentials, the key idea in Fourier analysis.\nBegin with Equation (12.11), which defines the frequency response H(Ω) in terms of the\nsignal h[·]:\ninf\nf\n-jΩm\nH(Ω) =\nh[m]e\n.\nm=-inf\nMultiply both sides of this equation by ejΩn, and integrate the result over Ω from -π to π:\n\ninf\n\nπ\nf\n(\nπ\n)\n-jΩ(m-n) dΩ\nH(Ω)ejΩn dΩ =\nh[m]\ne\n-π\n-π\nm=-inf\nwhere we have assumed h[·] is sufficiently well-behaved to allow interchange of the sum\nmation and integration operations.\nThe integrals above can be reduced to ordinary real integrals by rewriting each complex\nexponential ejkΩ as cos(kΩ) + j sin(kΩ), which shows that the result of each integration will\nin general be a complex number that has a real and imaginary part. However, for all k = 0,\nthe integral of cos(kΩ) or sin(kΩ) from -π to π will yield 0, because it is the integral over\nan integer number of periods. For k = 0, the integral of cos(kΩ) from -π to π yields 2π,\nwhile the integral of sin(kΩ) from -π to π yields 0. Thus every term for which m = n on the\nright side of the preceding equation will evaluate to 0. The only term that survives is the\none for which n = m, so the right side simplifies to just 2πh[n]. Rearranging the resulting\nequation, we get\nπ\nh[n] =\nH(Ω)ejΩn dΩ .\n(12.18)\n2π\n-π\nSince the integrand on the right is periodic with period 2π, we can actually compute the\nintegral over any contiguous interval of length 2π, which we indicate by writing\n\nh[n] = 1\nH(Ω)ejΩn dΩ .\n(12.19)\n2π\n<2π>\nNote that this equation can be interpreted as representing the signal h[n] as a weighted\ncombination of a continuum of exponentials of the form ejΩn, with frequencies Ω in a 2π\nrange, and associated weights H(Ω) dΩ.\n-\n12.2.2\nIllustrative Examples\nExample 4 (More Moving-Average Filters) The unit sample responses in Figure 12-2 all\ncorrespond to causal moving-average LTI filters, and have the form\n(\n)\nhL[n] =\nδ[n] + δ[n -1] + ··· + δ[n -(L -1)]\n.\nL\nThe corresponding frequency response, directly from the definition in Equation (12.11), is\ngiven by\n(\n)\n-jΩ\n-j(L-1)Ω\nHL(Ω) =\n1 + e\n+ ··· + e\n.\nL\n\nSECTION 12.2. FREQUENCY RESPONSE\nFigure 12-2: Unit sample response and frequency response of different moving average filters.\nTo examine the magnitude and phase of HL(Ω) as we did in the special case of L = 3 in\nExample 1, it is helpful to rewrite the preceding expression. In the case of odd L, we can\nwrite\n(\n)\n1 -j(L-1)Ω/2\nj(L-1)Ω/2\nj(L-3)Ω/2\n-j(L-1)Ω/2\nHL(Ω) =\ne\ne\n+ e\n+ ··· + e\nL\n(1\n)\n2 -j(L-1)Ω/2\n=\ne\n+ cos(Ω) + cos(2Ω) + ··· + cos((L -1)Ω/2)\n.\nL\nFor even L, we get a similar expression:\n(\n)\n1 -j(L-1)Ω/2\nj(L-1)Ω/2\nj(L-3)Ω/2\n-j(L-1)Ω/2\nHL(Ω) =\ne\ne\n+ e\n+ ··· + e\nL\n(\n)\n2 -j(L-1)Ω/2\n=\ne\ncos(Ω/2) + cos(3Ω/2) + ··· + cos((L -1)Ω/2)\n.\nL\nFor both even and odd L, the single complex exponential in front of the parentheses con\ntributes -(L -1)Ω/2 to the phase, but its magnitude is 1 for all Ω. For both even and odd\ncases, the sum of cosines in parentheses is purely real, and is either positive or negative\nat any specific Ω, hence contributing only 0 or ±π to the phase. So the magnitude of the\nfrequency response, which is plotted on Slide 13.12 for these various examples, is simply\nthe magnitude of the sum of cosines given in the above expressions.\nExample 5 (Cascaded Filter Sections) We saw in Example 1 that a 3-point moving aver\nage filter ended up having frequency-response zeros at Ω = arccos(-1 ) = ±2π/3. Review\ning the derivation there, you might notice that a simple way to adjust the location of the\nzeros is to allow h[1] to be different from h[0] = h[2]. Take, for instance, h[0] = h[2] = 1 and\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nH/4(Ω)\nx[n]\nH/2(Ω)\nH3/4(Ω)\nH(Ω)\ny[n]\nFigure 12-3: A \"10-cent\" low-pass filter obtained by cascading a few single-zero-pair filters.\nh[1] = α. Then\n(\n)\n-j2Ω\n-jΩ\nH(Ω) = 1 + αe-jΩ + e\n= e\nα + 2 cos(Ω)\n.\nIt follows that\n|H(Ω)| = |α + 2 cos(Ω)| ,\nand the zeros of this occur at Ω = ±arccos(-α/2). In order to have the zeros at the pair of\nfrequencies Ω = ±φo, we would pick h[1] = α = -2 cos(φo).\nIf we now cascade several such single-zero-pair filter sections, as in the top part of\nFigure 12-3, the overall frequency response is the product of the individual ones, as noted\nin Equation (12.16). Thus, the overall frequency response will have zero pairs at those\nfrequencies where any of the individual sections has a zero pair, and therefore will have all\nthe zero-pairs of the constituent sections. This is evident in curves on Figure 12-3, where\nthe zeros have been selected to produce a filter that passes low frequencies (approximately\nin the range |Ω| ≤π/8) preferentially to higher frequencies.\nExample 6 (Nearly Ideal Low-Pass Filter) Figure 12-4 shows the unit sample response\nand frequency response of an LTI filter that is much closer to being an ideal low-pass filter.\nSuch a filter would have H(Ω) = 1 in the band |Ω| < Ωc, and H(Ω) = 0 for Ωc < |Ω| ≤π;\nhere Ωc is referred to as the cut-off (or cutoff) frequency. Equation (12.18) shows that the\ncorresponding h[n] must then be given by\nh[n]\n=\n2⇡\nZ ⌦c\n-⌦c\nej⌦n d⌦\n=\n>\n<\n>\n:\nsin(⌦cn)\n⇡n\nfor n = 0\n⌦c\n⇡\nfor n = 0\n\nSECTION 12.2. FREQUENCY RESPONSE\nFigure 12-4: A more sophisticated low-pass filter that passes low frequencies ≤ π/8 and blocks higher\nfrequencies.\nThis unit sample response is plotted on the left curve in Figure 12-4, for n ranging from\n-300 to 300. The fact that H(Ω) is real should have prepared us for the fact that h[n] is\nan even function of h[n], i.e., h[-n] = h[n]. The slow decay of this unit sample response,\nfalling off as 1/n, is evident in the plot. In fact, it turns out that the ideal lowpass filter is not\nbounded-input bounded-output stable, because its unit sample response is not absolutely\nsummable.\nThe frequency response plot on the right in Figure 12-4 actually shows two different\nfrequency responses: one is the ideal lowpass characteristic that we used in determining\nh[n], and the other is the frequency response corresponding to the truncated h[n], i.e., the\none given by using Equation (12.20) for |n| ≤300, and setting h[n] = 0 for |n| > 300. To\ncompute the latter frequency response, we simply substitute the truncated unit sample\nresponse in the expression that defines the frequency response, namely Equation (12.11);\nthe resulting frequency response is again purely real. The plots of frequency response\nshow that truncation still yields a frequency response characteristic that is close to ideal.\nOne problem with the truncated h[n] above is that it corresponds to a noncausal system.\nTo obtain a causal system, we can simply shift h[n] forward by 300 steps. We have already\nseen in Example 2 that such shifting does not affect the magnitude of the frequency re\nsponse. The shifting does change the phase from being 0 at all frequencies to being linear\nin Ω, taking the value -300Ω.\nWe see in Figure 12-5 the frequency response magnitudes and unit sample responses\nof some other near-ideal filters. A good starting point for the design of the unit sample\nresponses of these filters is again Equation (12.18) to generate the ideal versions of the fil\nters. Subsequent truncation and time-shifting of the corresponding unit sample responses\nyields causal LTI systems that are good approximations to the desired frequency responses.\nExample 7 (Autoregressive Filters) Figure 12-6 shows the unit sample responses and\nfrequency response magnitudes of some other LTI filters. These can all be obtained as the\nNot\ncausal\nh[n]\nH[Ω]\n-300 0 300\nn\n- 0\n\nΩ\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nFigure 12-5: The frequency response and h[·] for some useful near-ideal filters.\ninput-output behavior of causal systems whose output at time n depends on some previ\nous values of y[·], along with the input value x[n] at time n; these are termed autoregressive\nsystems. The simplest example is a causal system whose output and input are related by\ny[n] = λy[n - 1] + βx[n]\n(12.20)\nfor some constant parameters λ and β. This is termed a first-order autoregressive model,\nbecause y[n] depends on the value of y[·] just one time step earlier. The unit sample re\nsponse associated with this system is\nn\nh[n] = βλ u[n] ,\n(12.21)\nwhere u[n] is the unit step function. To deduce this result, set x[n] = δ[n] with y[k] = 0 for\nk < 0 since the system is causal (and therefore cannot tell the difference between an all-\nzero input and the unit sample input till it gets to time k = 0), then iteratively use Equation\n(12.20) to compute y[n] for n ≥ 0. This y[n] will be the unit sample response, h[n].\nFor a system with the above unit sample response to be bounded-input bounded-output\n(BIBO) stable, i.e., for h[n] to be absolutely summable, we require |λ| < 1. If 0 < λ < 1, the\nunit sample has the form shown in the top left plot in Figure 12-6. The associated frequency\nresponse in the BIBO-stable case, from the definition in Equation (12.11), is\ninf\nf\nβ\nm -jΩm\nH(Ω) = β\nλ e\n=\n-jΩ .\n(12.22)\n1 - λe\nm=0\nThe magnitude of this is what is shown in the top right plot in Figure 12-6, for the case\n0 < λ < 1.\n\nSECTION 12.2. FREQUENCY RESPONSE\nFigure 12-6: h[·] and the frequency response for some other useful ideal auto-regressive filters.\nAnother way to derive the unit sample response and frequency response is to start with\nthe frequency domain. Suppose that the system in Equation (12.20) gets the input x[n] =\njΩn\njΩn\ne\n. Then, by the definition of the frequency response, the output is y[n] = H(Ω)e\n.\nSubstituting ejΩn for x[n] and H(Ω)ejΩn for y[n] in Equation (12.20), we get\njΩn\njΩ(n-1)\nH(Ω)e\n= λH(Ω)e\n+ βejΩn .\nMoving the H(Ω) terms to one side and canceling out the ejΩn factor on both sides (we can\ndo that because ejΩn is on the unit circle in the complex plane and cannot be equal to 0),\nwe get\nβ\nH(Ω) =\n.\n1 - λe-jΩ\nThis is the same answer as in Equation (12.22).\nβ\nTo obtain h, one can then expand\nas a power series, using the property that\n1-λe-jΩ\n-jΩ\n-j2Ω\n-j3Ω\n= 1 + z + z + .... The expansion has terms of the form e\n,e\n,e\n,..., and\n1-z\ntheir coefficients form the unit sample response sequence.\nWhether one starts with the time-domain, setting x[n] = δ[n], or the frequency-domain,\nsetting x[n] = ejΩn, depends on one's preference and the problem at hand. Both methods\nare generally equivalent, though in some cases one approach may be mathematically less\ncumbersome than the other.\nThe other two systems in Figure 12-6 correspond to second-order autoregressive models,\nfor which the defining difference equation is\ny[n] = -a1y[n - 1] - a2y[n - 2] + bx[n]\n(12.23)\nfor some constants a1, a2 and b.\nTo take one concrete example, consider the system whose output and input are related\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\naccording to\ny[n] = 6y[n - 1] - 8y[n - 2] + x[n]\n(12.24)\nWe want to determine h[·] and H(Ω). We can approach this task either by first setting\nx[n] = δ[n], finding h[·], and then applying Equation (12.11) to find H(Ω), or by first calcu\nlating H(Ω). Let us consider the latter approach here.\nSetting x[n] = ejΩn in Equation (12.24), we get\nejΩnH(Ω) = 6ejΩ(n-1)H(Ω) - 8ejΩ(n-2)H(Ω) + ejΩn .\nSolving this equation for H(Ω) yields\nH(Ω) =\n.\n(1 - 2e-jΩ)(1 - 4e-jΩ)\nOne can now work out h by expanding H as a power series of terms involving various\npowers of e-jΩ, and also derive conditions on BIBO-stability and conditions under which\nH(Ω) is well-defined.\nComing back to the general second-order auto-regressive model, it can be shown (fol\nlowing a development analogous to what you may be familiar with from the analysis of\nLTI differential equations) that in this case the unit sample response takes the form\nh[n] = (β1λn\n1 + β2λn\n2 )u[n] ,\nwhere λ1 and λ2 are the roots of the characteristic polynomial associated with this system:\na(λ) = λ2 + a1λ + a2 ,\nand β1,β2 are some constants. The second row of plots of Figure 12-6 corresponds to the\ncase where both λ1 and λ2 are real, positive, and less than 1 in magnitude. The third row\n∗\ncorresponds to the case where these roots form a complex conjugate pair, λ2 = λ (and\ncorrespondingly β2 = β∗), and have magnitude less than 1, i.e., lie within the unit circle in\nthe complex plane.\nExample 8 (Deconvolution Revisited) Consider the LTI system with unit sample re\nsponse\nh1[n] = δ[n] + 0.8δ[n - 1]\nfrom the previous chapter. As noted there, you might think of this channel as being ideal,\nwhich would imply a unit sample response of δ[n], apart from a one-step-delayed echo,\nwhich accounts for the additional 0.8δ[n - 1]. The corresponding frequency response is\n-jΩ\nH1(Ω) = 1 + 0.8e\n,\nimmediately from the definition of frequency response, Equation (12.11).\nWe introduced deconvolution in the last chapter as aimed at undoing--at the receiver--\nthe convolution carried out on the input signal x[·] by the channel. Thus, from the channel\noutput y[·], we wish to reconstruct the input x[·] using an LTI deconvolution filter with\n\nSECTION 12.2. FREQUENCY RESPONSE\nChannel,\nH1(Ω)\nReceiver\nfilter, H2(Ω)\nx[n]\ny[n]\nz[n]\nNoise w[n]\nFigure 12-7: Noise at the channel output.\nunit sample response h2[n] and associated frequency response H2(Ω). We want the output\nz[n] of the deconvolution filter at each time n to equal the channel input x[n] at that time.1\nTherefore, the overall unit sample response of the channel followed by the deconvolution\nfilter must be δ[n], so\n(h2 ∗ h1)[n] = δ[n] .\nWe saw in the last chapter how to use this relationship to determine h2[n] for all n, given\nh1[·]. Here h2[·] serves as the \"convolutional inverse\" to h1[·].\nIn the frequency domain, the analysis is much simpler. We require the frequency re\nsponse of the cascade combination of channel and deconvolution filter, H2(Ω)H1(Ω) to be\n1. This condition immediately yields the frequency response of the deconvolution filter as\nH2(Ω) = 1/H1(Ω) ,\n(12.25)\nso in the frequency domain deconvolution is simple multiplicative inversion, frequency\nby frequency. We thus refer to the deconvolution filter as the inverse system for the channel.\nFor our example, therefore,\nH2(Ω) = 1/(1 + 0.8e -jΩ) .\nThis is identical to the form seen in Equation (12.22) in Example 7, from which we find that\nh2[n] = (-0.8)n u[n] ,\nin agreement with our time-domain analysis in the previous chapter.\nThe frequency-domain treatment of deconvolution brings out an important point that\nis much more hidden in the time-domain analysis. From Equation (12.25), we note that\n|H2(Ω)| = 1/|H1(Ω)| so the deconvolution filter has high frequency response magnitude\nin precisely those frequency ranges where the channel has low frequency response magni\ntude. In the presence of the inevitable noise at the channel output (Figure 12-7), we would\nnormally and reasonably want to discount these frequency ranges, as the channel input x[n]\nproduces little effect at the output in these frequency ranges, relative to the noise power\nat the output in these frequency ranges. However, the deconvolution filter does the ex\nact opposite of what is reasonable here: it emphasizes and amplifies the channel output in\nthese frequency ranges. Deconvolution is therefore not a good approach to determine the\nchannel input in the presence of noise.\n1We might also be content to have z[n] = x[n- D] for some integer D > 0, but this does not change anything\nessential in the following development.\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\n- Acknowledgments\nWe thank Anirudh Sivaraman for several useful comments and Patricia Saylor for a bug\nfix.\n- Problems and Questions\n1. Ben Bitdiddle designs a simple causal LTI system characterized by the following unit\nsample response:\nh[0]\n=\n\nh[1]\n=\n\nh[2]\n=\n\nh[n]\n=\n0 ∀n > 2\n(a) What is the frequency response, H(Ω)?\n(b) What is the magnitude of H at Ω = 0,π/2,π?\n(c) If this LTI system is used as a filter, what is the set of frequencies that are re\nmoved?\n2. Suppose a causal linear time invariant (LTI) system with frequency response H is\ndescribed by the following difference equation relating input x[·] to output y[·]:\ny[n] = x[n] + αx[n - 1] + βx[n - 2] + γx[n - 3].\n(12.26)\nHere, α,β, and γ are constants independent of Ω.\n(a) Determine the values of α, β and γ so that the frequency response of system H\nis H(Ω) = 1 - 0.5e-j2Ω cosΩ.\n(b) Suppose that y[·], the output of the LTI system with frequency response H, is\nused as the input to a second causal LTI system with frequency response G,\nproducing W, as shown below.\n(c) If H(ejΩ) = 1 - 0.5e-j2Ω cosΩ, what should the frequency response, G(ejΩ), be\nso that w[n] = x[n] for all n?\n(d) Suppose α = 1 and γ = 1 in the above equation for an H with a different fre\nquency response than the one you obtained in Part (a) above. For this different\nH, you are told that y[n] = A(-1)n when x[n] = 1.0 + 0.5(-1)n for all n. Using\nthis information, determine the value of β in Eq. (12.26) and the value of A in\nthe formula for y[n].\n\nSECTION 12.2. FREQUENCY RESPONSE\n3. Consider an LTI filter with input signal x[n], output signal y[n], and unit sample\nresponse\nh[n] = aδ[n] + bδ[n - 1] + bδ[n - 2] + aδ[n - 3] ,\nwhere a and b are positive parameters, with b > a > 0. Thus h[0] = h[3] = a and\nh[1] = h[2] = b, while h[n] at all other times is 0. Your answers in this problem should\nbe in terms of a and b.\n(a) Determine the frequency response H(Ω) of the filter.\n(b) Suppose x[n] = (-1)n for all integers n from -inf to inf. Use your expression for\nH(Ω) in Part (a) above to determine y[n] at all times n.\n(c) As a time-domain check on your answer from Part (a), use convolution to de\ntermine the values of y[5] and y[6] when x[n] = (-1)n for all integers n from -inf\nto inf.\n(d) The frequency response H(Ω) = |H(Ω)|ej∠H(Ω) that you found in Part (a) for\nthis filter can be written in the form\n-j3Ω/2\nH(Ω) = G(Ω)e\n,\nwhere G(Ω) is a real function of Ω that can be positive or negative, depending\non the values of a, b, and Ω. Determine G(Ω), writing it in a form that makes\nclear it is a real function of Ω.\n(e) Suppose the input to the filter is x[n] = (-1)n + cos( π n+ θ0) for all n from -inf to\ninf, where θ0 is some constant. Use the frequency response H(Ω) to determine\nthe output y[n] of the filter (writing it in terms of a, b, and θ0).\nDepending on how you solve the problem, it may help you to recall that\n√\n√\ncos(π/4) = 1/ 2 and cos(3π/4) = -1/ 2. Also keep in mind our assumption\nthat b > a > 0.\n4. Consider the following three plots of the magnitude of three frequency responses,\n|HI(ejΩ)|, |HII(ejΩ)|, and |HIII(ejΩ)|, shown in Figure 12-8.\nSuppose a linear time-invariant system has a frequency response HA(ejΩ) given by\nthe formula\nHA(ejΩ) = (\n)(\n)\n-j(Ω- π )\n-j(Ω+ π )\n1 - 0.95e\n1 - 0.95e\n(a) Which frequency response plot (I, II, or III) best corresponds to HA(ejΩ) above?\nWhat is the numerical value of M in the plot you selected?\n(b) For what values of a1 and a2 will the system described by the difference equa\ntion\ny[n] + a1y[n - 1] + a2y[n - 2] = x[n]\nhave a frequency response given by HA(ejΩ) above?\n5. Suppose the input to a linear time invariant system is the sequence\n5π\nπ\nx[n] = 2 + cos\nn + cos n + 3(-1)n\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\nM-\nM-\nM-\nFigure 12-8: Channel frequency response curves for Problems 4 through 6.\n(a) What is the maximum value of the sequence x, and what is the smallest positive\nvalue of n for which x achieves its maximum?\n(b) Suppose the above sequence x is the input to a linear time invariant system\ndescribed by one of the three frequency response plots in Figure 12-8 (I, II, or\nIII). If y is the resulting output and is given by\ny[n] = 8 + 12(-1)n ,\nwhich frequency response plot best describes the system? What is the value of\nM in the plot you selected?\n6. Suppose the unit sample response of an LTI system has only three nonzero real val\nues, h[0],h[1], and h[2]. In addition, suppose these three real values satisfy these three\nequations:\nh[0] + h[1] + h[2]\n=\n\n-jπ/2\n-j2π/2\nh[0] + h[1]e\n+ h[2]e\n=\n\njπ/2\nj2π/2\nh[0] + h[1]e\n+ h[2]e\n=\n\n(a) Without doing any algebra, simply by inspection, you should be able to write\ndown the frequency response H(Ω) for some frequencies. Which frequencies\nare these? And what is the value of H at each of these frequencies?\n\nSECTION 12.2. FREQUENCY RESPONSE\n(b) Which of the above plots in Figure 12-8 (I, II, or III) is a plot of the magnitude\nof the frequency response of this system, and what is the value of M in the plot\nyou selected? Be sure to justify your selection and your computation of M.\n(c) Suppose the input to this LTI system is\njπ/6n\nx[n] = e\n.\nWhat is the value of y[n]/x[n]?\n7. A channel with echo can be represented as an LTI system with a unit sample response:\nh[n] = aδ[n - M] + bδ[n - M - kNb] ,\nwhere a is a positive real constant, M is the channel delay, k is an integer greater than\n0, Nb is the number of samples per bit, and M + kNb is the echo delay.\n(a) Derive the expression for the frequency response of this channel, H(Ω), as a\nfunction of a, b, M, k, and Nb.\n(b) If b = -a, k = 1, and Nb = 4, find the values of Ω ∈ [-π,+π] for which H(Ω) = 0.\n(c) For b = -a, M = 4, and kNb = 12, derive the expression for the output of the\nchannel (time sequence y[n]) for the input x[n] when\ni. x[n] = 1, for all n.\nii. x[n] = 2cos( π n), for all n.\nπ\niii. For x[n] = 3sin( π n +\n), for all n, derive y[n].\n8. A wireline channel has unit sample response h1[n] = e-an for n ≥ 0, and 0 otherwise,\nwhere a > 0 is a real number. (As an aside, a = Ts/τ, where Ts is the sampling\nrate and τ is the wire time constant. The wire resistance and capacitance prevent\nfast changes at the end of the wire regardless of how fast the input is changing. We\ncapture this decay in time with exponential unit sample response e-an).\nBen Bitdiddle, an MIT student who recently got a job at WireSpeed Inc., is trying\nto convince his manager that he can significantly improve the signaling speed (and\nhence transfer the bits faster) over this wireline channel, by placing a filter with unit\nsample response\nh2[n] = Aδ[n] + Bδ[n - D],\nat the receiver, so that\n(h1 ∗ h2)[n] = δ[n].\n(a) Derive the values of A, B and D that satisfy Ben's goal.\n(b) Sketch the frequency response of H2(Ω) and mark the values at 0 and ±π.\n(c) Suppose a = 0.1. Then, does H2(Ω) behave like a (1) low-pass filter, (2) high-\npass filter, (3) all-pass filter? Explain your answer.\n\nCHAPTER 12. FREQUENCY RESPONSE OF LTI SYSTEMS\n(d) Under what noise conditions will Ben's idea work reasonably well? Give a brief,\nqualitative explantion for your answer; there's no need to calculate anything\nhere.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 13: Fourier Analysis and Spectral Representation of Signals",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/06d7972059805d20e6a7db3a52ec23f2_MIT6_02F12_chap13.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 3, 2012\nCHAPTER 13\nFourier Analysis and Spectral\nRepresentation of Signals\nWe have seen in the previous chapter that the action of an LTI system on a sinusoidal or\ncomplex exponential input signal can be represented effectively by the frequency response\nH(Ω) of the system. By superposition, it then becomes easy--again using the frequency\nresponse--to determine the action of an LTI system on a weighted linear combination of si\nnusoids or complex exponentials (as illustrated in Example 3 of the preceding chapter). The\nnatural question now is how large a class of signals can be represented in this manner. The\nshort answer to this question: most signals you are likely to be interested in!\nThe tool for exposing the decomposition of a signal into a weighted sum of sinusoids\nor complex exponentials is Fourier analysis. We first discuss the Discrete-Time Fourier\nTransform (DTFT), which we have actually seen hints of already and which applies to the\nmost general classes of signals. We then move to the Discrete-Time Fourier Series (DTFS),\nwhich constructs a similar representation for the special case of periodic signals, or for sig\nnals of finite duration. The DTFT development provides some useful background, context\nand intuition for the more special DTFS development, but may be skimmed over on an\ninitial reading (i.e., understand the logical flow of the development, but don't struggle too\nmuch with the mathematical details).\n- 13.1\nThe Discrete-Time Fourier Transform\nWe have in fact already derived an expression in the previous chapter that has the flavor\nof what we are looking for. Recall that we obtained the following representation for the\nunit sample response h[n] of an LTI system:\nh[n] = 1\n2π\n\n<2π>\nH(Ω)ejΩn dΩ ,\n(13.1)\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nwhere the frequency response, H(Ω), was defined by\ninf\n\nH(Ω) =\nh[m]e -jΩm .\n(13.2)\nm=-inf\nEquation (13.1) can be interpreted as representing the signal h[n] by a weighted combina\ntion of a continuum of exponentials, of the form ejΩn, with frequencies Ω in a 2π-range,\nand associated weights H(Ω) dΩ.\nAs far as these expressions are concerned, the signal h[n] is fairly arbitrary; the fact that\nwe were considering it as the unit sample response of a system was quite incidental. We\nonly required it to be a signal for which the infinite sum on the right of Equation (13.2)\nwas well-defined. We shall accordingly rewrite the preceding equations in a more neutral\nnotation, using x[n] instead of h[n]:\nx[n] = 1\n2π\n<2π>\nX(Ω)ejΩn dΩ ,\n(13.3)\nwhere X(Ω) is defined by\ninf\n\nX(Ω) =\nx[m]e -jΩm .\n(13.4)\nm=-inf\nFor a general signal x[·], we refer to the 2π-periodic quantity X(Ω) as the discrete-time\nFourier transform (DTFT) of x[·]; it would no longer make sense to call it a frequency\nresponse. Even when the signal is real, the DTFT will in general be complex at each Ω.\nThe DTFT synthesis equation, Equation (13.3), shows how to synthesize x[n] as a\nweighted combination of a continuum of exponentials, of the form ejΩn, with frequen\ncies Ω in a 2π-range, and associated weights X(Ω) dΩ. From now on, unless mentioned\notherwise, we shall take Ω to lie in the range [-π,π].\nThe DTFT analysis equation, Equation (13.4), shows how the weights are determined.\nWe also refer to X(Ω) as the spectrum or spectral distribution or spectral content of x[·].\nExample 1 (Spectrum of Unit Sample Function) Consider the signal x[n] = δ[n], the unit\nsample function. From the definition in Equation (13.4), the spectral distribution is given\nby X(Ω) = 1, because x[n] = 0 for all n = 0, and x[0] = 1. The spectral distribution is thus\nconstant at the value 1 in the entire frequency range [-π,π]. What this means is that it takes\nthe addition of equal amounts of complex exponentials at all frequencies in a 2π-range to\nsynthesize a unit sample function, a perhaps surprising result. What's happening here is\nthat all the complex exponentials reinforce each other at time n = 0, but effectively cancel\neach other out at every other time instant.\nExample 2 (Phase Matters) What if X(Ω) has the same magnitude as in the previous\nexample, so |X(Ω)| = 1, but has a nonzero phase characteristic, ∠X(Ω) = -αΩ for some\nα = 0? This phase characteristic is linear in Ω. With this,\n-jαΩ\n-jαΩ\nX(Ω) = 1.e\n= e\n.\nZ\n\nSECTION 13.1. THE DISCRETE-TIME FOURIER TRANSFORM\nTo find the corresponding time signal, we simply carry out the integration in Equation\n(13.3). If α is an integer, the integral\nyields the value 0 for all n = α. To see this, note that\n(\n)\n(\n)\nj(n-α)Ω\ne\n= cos (n -α)Ω + j sin (n -α)Ω\n,\nand the integral of this expression over any 2π-interval is 0, when n -α is a nonzero inte\nger. However, if n -α = 0, i.e., if n = α, the cosine evaluates to 1, the sine evaluates to 0,\nand the integral above evaluates to 1. We therefore conclude that when α is an integer,\nx[n] = δ[n -α] .\nThe signal is just a shifted unit sample (delayed by α if α > 0, and advanced by |α| other\nwise). The effect of adding the phase characteristic to the case in Example 1 has been to\njust shift the unit sample in time.\nFor non-integer α, the answer is a little more intricate:\nThis time-function is referred to as a \"sinc\" function. We encountered this function when\ndetermining the unit sample response of an ideal lowpass filter in the previous chapter.\nExample 3 (A Bandlimited Signal) Consider now a signal whose spectrum is flat but\nband-limited:\n{\nfor\n|Ω| < Ωc\nX(Ω) =\nfor\nΩc ≤|Ω| ≤π\nThe corresponding signal is again found directly from Equation (13.3). For n = 0, we get\nx[n] = 1\n2⇡\nZ\n<2⇡>\ne-j↵⌦ej⌦n d⌦= 1\n2⇡\nZ\n<2⇡>\nej(n-↵)⌦d⌦\nx[n]\n=\n2⇡\nZ ⇡\n-⇡\ne-j↵⌦ej⌦n d⌦\n=\n2⇡\nej(n-↵)⌦\nj(n -↵)\n%%%\n⇡\n-⇡\n=\nsin\n⇣\n⇡(n -↵)\n⌘\n⇡(n -↵)\nx[n]\n=\n2⇡\nZ\n-⌦c\nej⌦n d⌦\n=\n2⇡\nej⌦\njn\n%%%\n⌦c\n-⌦c\n=\nsin(⌦cn)\n⇡n\n,\n(13.5)\n⌦c\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nwhich is again a sinc function. For n = 0, Equation (13.3) yields\n(This is exactly what we would get from Equation (13.5) if n was treated as a continuous\nvariable, and the limit of the sinc function as n →0 was evaluated by L'Hˆopital's rule--a\nuseful mnemonic, but not a derivation!)\nFrom our study of the analogous equations for h[·] in the previous chapter, we know\nthat the DTFT of x[·] is well-defined when this signal is absolutely summable,\nfor some μ. However, the DTFT is in fact well-defined for signals that satisfy less demand\ning constraints, for instance square summable signals,\nThe sinc function in the examples above is actually not absolutely summable because it\nfollows off too slowly--only as 1/n--as |n| →inf. However, it is square summable.\nA digression: One can also define the DTFT for signals x[n] that do not converge to 0 as\n|n| →inf, provided they grow no faster than polynomially in n as |n| →inf. An example\nof such a signal of slow growth would be x[n] = ejΩ0n for all n, whose spectrum must be\nconcentrated at Ω = Ω0. However, the corresponding X(Ω) turns out to no longer be an\nordinary function, but is a (scaled) Dirac impulse in frequency, located at Ω = Ω0:\nX(Ω) = 2πδ(Ω -Ω0) .\nYou may have encountered the Dirac impulse in other settings. The unit impulse at Ω = Ω0\ncan be thought of as a \"function\" that has the value 0 at all points except at Ω = Ω0, and has\nunit area. This is an instance of a broader result, namely that signals of slow growth possess\ntransforms that are generalized functions (e.g., impulses), which have to be interpreted in\nterms of what they do under an integral sign, rather than as ordinary functions. It is\npartly in order to avoid having to deal with impulses and generalized functions in treating\nsinusoidal and periodic signals that we shall turn to the Discrete-Time Fourier Series rather\nthan the DTFT. End of digression!\nWe make one final observation before moving to the DTFS. As shown in the previous\nchapter, if the input x[n] to an LTI system with frequency response H(Ω) is the (everlasting)\nexponential signal ejΩn, then the output is y[n] = H(Ω)ejΩn. By superposition, if the input\nis instead the weighted linear combination of such exponentials that is given in Equation\n(13.3), then the corresponding output must be the same weighted combination of responses,\nso\nx[n] = 1\n2⇡\nZ ⌦c\n-⌦\n1d⌦= ⌦c\n⇡.\nX\nm=-1\n%%%x[m]\n%%% μ < 1\nX\nm=-1\n%%%x[m]\n%%%\nμ < 1 .\ny[n] = 1\n2⇡\nZ\n<2⇡>\nH(⌦)X(⌦)ej⌦n d⌦.\n(13.6)\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\nHowever, we also know that the term H(Ω)X(Ω) multiplying the complex exponential in\nthis expression must be the DTFT of y[·], so\nY (Ω) = H(Ω)X(Ω) .\n(13.7)\nThus, the time-domain convolution relation y[n] = (h∗ x)[n] has been converted to a simple\nmultiplication in the frequency domain. This is a result we saw in the previous chapter\ntoo, when discussing the frequency response of a series or cascade combination of two LTI\nsystems: the relation h[n] = (h1 ∗ h2)[n] in the time domain mapped to an overall frequency\nresponse of H(Ω) = H1(Ω)H2(Ω) that was simply the product of the individual frequency\nresponses. This is a major reason for the power of frequency-domain analysis; the more\ninvolved operation of convolution in time is replaced by multiplication in frequency.\n- 13.2\nThe Discrete-Time Fourier Series\nThe DTFT synthesis expression in Equation (13.3) expressed x[n] as a weighted sum of a\ncontinuum of complex exponentials, involving all frequencies Ω in [-π,π]. Suppose now\nthat x[n] is a periodic signal of (integer) period P, so\nx[n + P] = x[n]\nfor all n. This signal is completely specified by the P values it takes in a single period, for\ninstance the values x[0],x[1],...,x[P - 1]. It would seem in this case as though we should\nbe able to get away with using a smaller number of complex exponentials to construct x[n]\non the interval [0,P - 1] and thereby for all n. The discrete-time Fourier series (DTFS)\nshows that this is indeed the case.\nBefore we write down the DTFS, a few words of reassurance are warranted. The expres\nsions below may seem somewhat bewildering at first, with a profusion of symbols and\nsubscripts, but once we get comfortable with what the expressions are saying, interpret\nthem in different ways, and do some examples, they end up being quite straightforward.\nSo don't worry if you don't get it all during the first pass through this material--allow\nyourself some time, and a few visits, to get comfortable!\n-\n13.2.1\nThe Synthesis Equation\nThe essence of the DTFS is the following statement:\nAny P-periodic signal x[n] can be represented (or synthesized) as a weighted lin\near combination of P complex exponentials (or spectral components), where the fre\nquencies of the exponentials are located evenly in the interval [-π,π], starting in the\nmiddle at the frequency Ω0 = 0 and increasing outwards in both directions in steps of\nΩ1 = 2π/P.\nkn\nMore concretely, the claim is that any P-periodic DT signal x[n] can be represented in the\nform\nx[n] =\nX\nk=hPi\nAkej⌦kn ,\n(13.8)\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nΩ_1\nΩ0\nΩ1\n-l\nl\nΩ0\nΩ1\nΩ2\nΩ3\nΩ_3\nΩ_2\nΩ_1\nexp(jΩ0)\nexp(jΩ_1)\nexp(jΩ2)\nexp(jΩ3)\n= exp(jΩ_3)\nexp(jΩ1)\nexp(jΩ_2)\n.\n-1\nj\n-j\n-l\nl\nexp(jΩ1)\n.\n-1\nj\n-j\nexp(jΩ0)\nexp(jΩ_1)\nFigure 13-1: When P is even, the end frequencies are at ±π and the Ωk values are as shown in the pictures\non the left for P = 6. When P is odd, the end frequencies are at ±(π - Ω\n1 ), as shown on the right for P = 3.\nwhere we write k = (P) to indicate that k runs over any set of P consecutive integers. The\nFourier series coefficients or spectral weights Ak in this expression are complex numbers in\ngeneral, and the spectral frequencies Ωk are defined by\n2π\nΩk = kΩ1 ,\nwhere Ω1 =\n.\n(13.9)\nP\nWe refer to Ω1 as the fundamental frequency of the periodic signal, and to Ωk as the k-th\nharmonic. Note that Ω0 = 0.\nNote that the expression on the right side of Equation (13.8) does indeed repeat period\nically every P time steps, because each of the constituent exponentials\n2π\n2π\njΩkn\njkΩ1n\ne\n= e\n= ejkn(2π/P) = cos(k\nn) + j sin(k\nn)\n(13.10)\nP\nP\nrepeats when n changes by an integer multiple of P.\nIt also follows from Equation (13.10) that changing the frequency index k by P -- or more\ngenerally by any positive or negative integer multiple of P -- brings the exponential in that\nequation back to the same point on the unit circle, because the corresponding frequency\nΩk has then changed by an integer multiple of 2π. This is why it suffices to choose k = (P)\nin the DTFS representation.\nPutting all this together, it follows that the frequencies of the complex exponentials\nused to synthesize a P-periodic signal x[n] via the DTFS are located evenly in the interval\n[-π,π], starting in the middle at the frequency Ω0 = 0 and increasing outwards in both\ndirections in steps of Ω1 = 2π/P. In the case of an even value of P, such as the case P = 6 in\nFigure 13-1 (left), the end frequencies will be at ±π (we need only one of these frequencies,\nnot both, as they translate to the same point on the unit circle when we write ejΩkn). In\nthe case of an odd value of P, such as the case P = 3 shown in Figure 13-1 (right), the end\npoints are ±(π - Ω1 ).\nThe weights {Ak} collectively constitute the spectrum of the periodic signal, and we\ntypically plot them as a function of the frequency index k, as in Figure 13-2 The spectral\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\nFigure 13-2: The spectrum of two periodic signals, plotted as a function of the frequency index, k, showing\nthe real and imaginary parts for each case. P = 11 (odd).\nweights in these simple sinusoidal examples have been determined by inspection, through\ndirect application of Euler's identity. We turn next to a more general and systematic way\nof determining the spectrum for an arbitrary real P -periodic signal.\n-\n13.2.2\nThe Analysis Equation\nWe now address the task of computing the spectrum of a P -periodic x[n], i.e., determining\nthe Fourier coefficients Ak. Note first that the {Ak} comprise P coefficients that in general\ncan be complex numbers, so in principle we have 2P real numbers that we can choose\nto match the P real values that a P -periodic real signal x[n] takes in a period. It would\ntherefore seem that we have more than enough degrees of freedom to choose the Fourier\ncoefficients to match a P -periodic real signal. (If the signal x[n] was an arbitrary complex\nP -periodic signal, hence specified by 2P real numbers, we would have exactly the right\nnumber of degrees of freedom.)\nIt turns out--and we shall prove this shortly--that for a real signal x[n] the Fourier\ncoefficients satisfy certain symmetry properties, which end up reducing our degrees of\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nfreedom to precisely P rather than 2P . Specifically, we can show that\nAk = A ∗\n(13.11)\n-k ,\nso the real part of Ak is an even function of k, while the imaginary part of Ak is an odd\nfunction of k. This also implies that A0 is purely real, and also that in the case of even\nP , the values AP/2 = A-P/2 are purely real. (These properties should remind you of the\nsymmetry properties we exposed in connection with frequency responses in the previous\nchapter -- but that's no surprise, because the DTFS is a similar kind of object.)\nMaking a careful count now of the actual degrees of freedom, we find that it takes\nprecisely P real parameters to specify the spectrum {Ak} for a real P -periodic signal. So\ngiven the P real values that x[n] takes over a single period, we expect that Equation (13.8)\nwill give us precisely P equations in P unknowns. (For the case of a complex signal, we\nwill get 2P equations in 2P unknowns.)\nTo determine the mth Fourier coefficient Am in the expression in Equation (13.8), where\nm is one of the values that k can take, we first multiply both sides of Equation (13.8) by\ne-jΩmn and sum over P consecutive values of n. This results in the equality\nThe summation over n in the last equality involves summing P consecutive terms of a\ngeometric series. Using the fact that for r = 1\nP\n1 -r\nP -1\n1 + r + r + ··· + r\n=\n,\n1 -r\nit is not hard to show that the above summation over n ends up evaluating to 0 for k = m.\nThe only value of k for which the summation over n survives is the case k = m, for which\neach term in the summation reduces to 1, and the sum ends up equal to P . We therefore\narrive at\nor, rearranging and going back to writing k instead of m,\nThis DTFS analysis equation -- which holds whether x[n] is real or complex -- looks very\nsimilar to the DTFS synthesis equation, Equation (13.8), apart from e-jΩkn replacing ejΩkn ,\nand the scaling by P .\nx\ne\nn\nX\n[n] -j⌦mn\n=\n=hPi\nn\nX\n=hPi k\nX\nAkej(⌦k-⌦m)n\n=hPi\n=\nX\nA\nX\nej⌦1(k\nk\n-m)n\nk=hPi\nn=hPi\n=\nX\nA\nX\nej2⇡(k\nm\nk\n-\n)n/P .\nk=hPi\nn=hPi\nAk =\nx[n]e-j⌦kn .\nP\nX\n(13.12)\nX\nx[n]e\nm\n= AmP\nn=hPi\n-j⌦\nn\nn=hPi\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\nTwo particular observations that follow directly from the analysis formula:\nA0 =\nx[n] ,\n(13.13)\nP n=(P)\nand, for the case of even P, where ΩP/2 = π,\nAP/2 = A-P/2 = 1\n(-1)n x[n] .\n(13.14)\nP n=(P)\nThe symmetry properties of Ak that we stated earlier in the case of a real signal follow\ndirectly from this analysis equation, as we leave you to verify. Also, since A-k = A∗ for a\nk\nreal signal, we can combine the terms\n-jΩkn\njΩkn\nAke\n+ Ake\ninto the single term\n2|Ak|cos(Ωkn + ∠Ak) .\nThus, for even P,\nP/2\nx[n] = A0 +\n2|Ak|cos(Ωkn + ∠Ak) ,\nk=1\nwhile for odd P the only change is that the upper limit becomes (P -1)/2.\n-\n13.2.3\nThe Aperiodic Limit, P →inf\nThere is a slightly modified form in which the DTFS is sometimes written:\njΩk n\nx[n] =\nXke\n,\n(13.15)\nP k=(P)\nwhich just corresponds to working with a scaled version of the Ak that we have used so\nfar, namely\n-jΩkn\nXk = PAk =\nx[n]e\n.\n(13.16)\nn=(P)\nThis form of the DTFS is useful when one considers the limiting case of aperiodic signals\nby letting P →inf, (2π/P) →dΩ, and Ωk →Ω. In this limiting case, it is easy to deduce\nfrom Equation (13.12) that Xk →X(Ω), precisely the DTFT of the aperiodic signal that we\ndefined in Equation (13.4). Correspondingly, the DTFS synthesis equation, Equation (13.8),\nin this limiting case becomes precisely the expression in Equation (13.3).\n-\n13.2.4\nAction of an LTI System on a Periodic Input\nSuppose the input x[·] to an LTI system with frequency response H(Ω) is P-periodic. This\nsignal can be represented as a weighted sum of exponentials, by the DTFS in Equation\nX\nX\nX\nX\nX\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nFigure 13-3: Effect of band-limiting a transmission, showing what happens when a periodic signal goes\nthrough a lowpass filter.\n(13.8). It follows immediately that the output of the system is given by\njΩkn\njΩkn\ny[n] =\nH(Ωk)Ake\n=\nH(Ωk)Xke\n.\nP\nk=(P )\nk=(P )\nThis immediately shows that the output y[·] is again P-periodic, with (scaled) spectral\ncoefficients given by\nYk = H(Ωk)Xk .\n(13.17)\nSo knowledge of the input spectrum and of the system's frequency response suffices to\ndetermine the output spectrum. This is precisely the DTFS version of the DTFT result in\nEquation (13.7).\nAs an illustration of the application of this result, Figure 13-3 shows what happens\nwhen a periodic signal goes through an ideal lowpass filter, for which H(Ω) = 1 only for\n|Ω| < Ωc < π, with H(Ω) = 0 everywhere else in [-π,π]. The result is that all spectral\ncomponents of the input at frequencies above the cutoff frequency Ωc are no longer present\nin the output. The corresponding output signal is thus more slowly varying--a \"blurred\"\nversion of the input--because it does not have the higher-frequency components that allow\nit to vary more rapidly.\nX\nX\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\n-\n13.2.5\nApplication of the DTFS to Finite-Duration Signals\nThe DTFS turns out to be useful in settings that do not involve periodic signals, but rather\nsignals of finite duration. Suppose a signal x[n] takes nonzero values only on some finite\ninterval, say [0,P - 1] for example. We are not forbidding x[n] from taking the value 0 for\nn within this interval, but are saying that x[n] = 0 for all n outside this interval. If we now\ncompute the DT Fourier transform of this signal, according to the definition in Equation\n(13.4), we get\nP -1\nX(Ω) =\nx[n]e -jΩn .\n(13.18)\nn=0\nThe corresponding representation of x[n] by a weighted combination of complex exponen\ntials would then be the expression in Equation (13.3), involving a continuum of frequencies.\nHowever, it is possible to get a more economical representation of x[n] by using the DT\nFourier series.\nIn order to do this, consider the new signal xP [·] obtained by taking the portion of x[·]\nthat lies in the interval [0,P - 1] and replicating it periodically outside this interval, with\nperiod P. This results in xP [n + P] = xP [n] for all n, with xP [n] = x[n] for n in the interval\n[0,P - 1]. We can represent this periodic signal by its DTFS:\nxP [n] = 1\nXkejΩkn ,\n(13.19)\nP k=<P >\nwhere\nP -1\n-jΩkn\n-jΩkn\nXk =\nxP [n]e\n=\nx[n]e\n.\n(13.20)\nn=<P>\nn=0\n(For consistency, we should perhaps have used the notation XPk instead of Xk, but we are\ntrying to keep our notation uncluttered.)\nWe can now represent x[n] by the expression in Equation (13.19), in terms of just P\ncomplex exponentials at the frequencies Ωk defined earlier (in our development of the\nDTFS), rather than complex exponentials at a continuum of frequencies. However, this\nrepresentation only captures x[n] in the interval [0,P - 1]. Outside of this interval, we have\nto ignore the expression, instead invoking our knowledge that x[n] is actually 0 outside.\nAnother observation worth making from Equations (13.18) and (13.20) is that the\n(scaled) DTFS coefficients Xk are actually simply related to the DTFT X(Ω) of the finite-\nduration signal x[n]:\nXk = X(Ωk) ,\n(13.21)\nso the (scaled) DTFS coefficients Xk are just P samples of the DTFT X(Ω). Thus any method\nfor computing the DTFS for (the periodic extension of) a finite-duration signal will yield\nsamples of the DTFT of this finite-duration signal (keep track of our use of DTFS versus\nDTFT here!). And if one wants to evaluate the DTFT of this finite-duration signal at a\nlarger number of sample points, all that needs to be done is to consider x[n] to be of finite-\nduration on a larger interval, of length P i > P, where of course the additional signal values\nin the larger interval will all be 0; this is referred to a zero-padding. Then computing the\nDTFS of (the periodic extension of) x[n] for this longer interval will yield P i samples of the\nX\nX\nX\nX\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nunderlying DTFT of the signal.\nAs an application of the above results on finite-duration signals, consider the case of\nan LTI system whose unit sample response h[n] is known to be 0 for all n outside of some\ninterval [0,nh], and whose input x[n] is known to be 0 for all n outside some interval [0,nx].\nIt follows that the earliest time instant at which a nonzero output value can appear is\nn = 0, while the latest such time instant is n = nx + nh. In other words, the response\ny[n] = (h ∗ x)[n] is guaranteed to be 0 for all n outside of the interval [0,nx + nh]. All the\ninteresting input/output action of the system therefore takes place for n in this interval.\nOutside of this interval we know that x[·] and y[·] are both 0. We can therefore take all the\nsignals of interest to have finite duration, being 0 outside of the interval [0,P - 1], where\nP = nx + nh + 1. A DTFS representation of x[·] and y[·] on this interval, with this choice of\nP, can then be used to carry out a frequency-domain analysis of the system. In particular,\nthe kth (scaled) Fourier coefficients of the input and output will be related as in Equation\n(13.17).\n-\n13.2.6\nThe FFT\nImplementing either the DTFS synthesis computation or the DTFS analysis computation,\nas defined earlier, would seem to require on the order of P 2 multiply/add operations:\nwe have to do P multiply/adds for each of P frequencies. This can quickly lead to pro\nhibitively expensive computations in large problems.\nHappily, in 1965 Cooley and Tukey published a fast method for computing these DTFS\nexpressions (rediscovering a technique known to Gauss!). Their algorithm is termed the\nFast Fourier Transform or FFT, and takes on the order of P log P operations, which is a big\nsaving. (Note that the FFT is not a new kind of transform, despite its name! -- it's a fast\nalgorithm for computing a familiar transform, namely the DTFS.)\nThe essence of the idea is to recursively split the computation into a DTFS computation\ninvolving the signal values at the even time instants and another DTFS computation in\nvolving the signal values at the odd time instants. One then cleverly uses the nice algebraic\nproperties of the P complex exponentials involved in these computations to stitch things\nback together and obtain the desired DTFS.\nThe FFT has become a (or maybe the) workhorse of practical numerical computation. Its\nmost common application is to computing samples of the DTFT of finite-duration signals,\nas described in the previous subsection. It can also be applied, of course, to computing the\nDTFS of a periodic signal.\n- Acknowledgments\nThanks to Patricia Saylor, Kaiying Liao, and Michael Sanders for bug fixes.\n- Problems and Questions\n1. Let x[·] be a signal that is periodic with period P = 12. For each of the following x[.],\ngive the corresponding spectral coefficients Ak for the discrete-time Fourier series\nfor x[·], for k in the range -6 ≤ k ≤ 6. (Hint: In most of the following cases, all you\nneed to do is express the signal as the sum of appropriate complex exponentials, by\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\ninspection--this is much easier than cranking through the formal definition of the\nspectral coefficient.)\n(a) Determine Ak when x[0 : 11] = [0,0,1,0,0,0,0,0,0,0,0,0].\n(b) Determine Ak when x[n] = 1 for all n.\n(c) Determine Ak when x[n] = sin(r(2π/12)n) for the following two choices of r:\ni. r = 3; and\nii. r = 8.\n(d) Determine Ak when x[n] = sin(3(2π/12)n + φ) where φ is some specified phase\noffset.\n2. Consider a lowpass LTI communication channel with input x[n], output y[n], and\nfrequency response H(Ω) given by\n-j3Ω\nH(Ω) = e\nfor 0 ≤|Ω| < Ωm ,\n=\n\nfor Ωm ≤|Ω| ≤π .\nHere Ωm denotes the cutoff frequency of the channel; the output y[n] will contain no\nfrequency components in the range Ωm ≤|Ω| ≤π. The different parts of this problem\ninvolve different choices for Ωm.\n(a) Picking Ωm = π/4, provide separate and properly labeled sketches of the mag\nnitude |H(Ω)| and phase ∠H(Ω) of the frequency response, for Ω in the interval\n0 ≤|Ω| ≤π. (Sketch the phase only in the frequency ranges where |H(Ω)| > 0.)\n(b) Suppose Ωm = π, so H(Ω) = e-j3Ω for all Ω in [-π,π], i.e., all frequency com\nponents make it through the channel. For this case, y[n] can be expressed quite\nsimply in terms of x[.]; find the relevant expression.\n(c) Suppose the input x[n] to this channel is a periodic \"rectangular-wave\" signal\nwith period 12. Specifically:\nx[-1] = x[0] = x[1] = 1\nand these values repeat every 12 steps, so\nx[11] = x[12] = x[13] = 1\nand more generally\nx[12r -1] = x[12r] = x[12r + 1]\nfor all integers r from -infto inf. At all other times n, we have x[n] = 0. (You\nmight find it helpful to sketch this signal for yourself, e.g., for n ranging from\n-2 to 13.)\n\nCHAPTER 13. FOURIER ANALYSIS AND SPECTRAL REPRESENTATION OF SIGNALS\nFind explicit values for the Fourier coefficients in the discrete-time Fourier series\n(DTFS) for this input x[n], i.e., the numbers Ak in the representation\njΩkn\nx[n] =\nAke\n,\nk=-6\nwhere Ωk = k(2π/12). Recall that\n-jΩk n\nAk =\nx[n]e\n,\n12 (n)\nwhere the summation is over any 12 consecutive values of n (as indicated by\nwriting (n)), so all you need to do is evaluate this expression for the particular\nx[n] that we have.\nSince x[n] is an even function of n, all the Ak should be purely real, so be sure your\nexpression for Ak makes clear that it is real. (Depending on how you proceed,\n-j11(2π/12)\nj(2π/12).)\nyou may or may not find it helpful to note that e\n= e\nCheck that your values for A0 and A-6 = A6 are correct, and be explicit about\nhow you are checking.\n(d) Suppose the cutoff frequency of the channel is Ωm = π/4 (which is the case you\nsketched in part (a), and the input is the x[n] specified in part (c). Compute the\nvalues of all the nonzero Fourier coefficients of the channel output y[n], i.e., find\nthe values of the nonzero numbers Bk in the representation\njΩkn\ny[n] =\nBke\n,\nk=-6\nwhere Ωk = k(2π/12). Don't forget that H(Ω) = e-j3Ω in the passband of the\nfilter, 0 ≤|Ω| < Ωm.\n(e) Express the y[n] in part (d) as an explicit and real function of time n. (If you were\nto sketch y[n], you would discover that it is a low-frequency approximation to\nthe y[n] that would have been obtained if Ωm = π.)\n3. The figure below shows the real and imaginary parts of all non-zero Fourier se\nries coefficients X[k] of a real periodic discrete-time signal x[n], for frequencies\nΩk ∈[0,π]. Here Ωk = k(2π/N) for some fixed even integer N, as in all our anal\nysis of the discrete-time Fourier series (DTFS), but the plots below only show the\nrange 0 ≤k ≤N/2.\n(a) Find all non-zero Fourier series coefficients of x[n] at Ωk in the interval [-π,0),\ni.e., for -(N/2) ≤k < 0. Give your answer in terms of careful and fully labeled\nplots of the real and imaginary parts of X[k] (following the style of the figure\nabove).\n(b) Find the period of x[n], i.e., the smallest integer T for which x[n + T] = x[n], for\nall n.\nX\nX\nX\n\nSECTION 13.2. THE DISCRETE-TIME FOURIER SERIES\nRe(X[k])\n+\n\nΩk\n\nπ/3\n2π/3\nπ\n\n0.5\nIm(X[k])\n+\n\nΩk\n\nπ/2\nπ\n(c) For the frequencies Ωk ∈ [0,π], find all non-zero Fourier series coefficients of the\nsignal x[n - 6] obtained by delaying x[n] by 6 samples.\n4. Consider an audio channel with a sampling rate of 8000 samples/second.\n(a) What is the angular frequency of the piano note A (in radians/sample), given\nthat its continuous time frequency is 880 Hz?\n(b) What is the smallest number of samples, P, needed to represent the note A as a\n2π\nspectral component at Ωk =\nk, for integer k? And what is the value of k?\nP\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 14: Modulation and Demodulation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/2ab9b67e8fccafa520dac8033003b289_MIT6_02F12_chap14.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 3, 2012\nCHAPTER 14\nModulation and Demodulation\nThis chapter describes the essential principles behind modulation and demodulation, which\nwe introduced briefly in Chapter 10. Recall that our goal is to transmit data over a commu\nnication link, which we achieve by mapping the bit stream we wish to transmit onto analog\nsignals because most communication links, at the lowest layer, are able to transmit ana\nlog signals, not binary digits. The signals that most simply and directly represent the bit\nstream are called the baseband signals. We discussed in Chapter 10 why it is generally un\ntenable to directly transmit baseband signals over communication links. We reiterate and\nelaborate on those reasons in Section 14.1, and discuss the motivations for modulation of\na baseband signal. In Section 14.2, we describe a basic principle used in many modulation\nschemes, called the heterodyne principle. This principle is at the heart of amplitude modulation\n(AM), the scheme we study in detail. Sections 14.3 and 14.4 describe the \"inverse\" process\nof demodulation, to recover the original baseband signal from the received version. Fi\nnally, Section 14.5 provides a brief overview of more sophisticated modulation schemes.\n- 14.1\nWhy Modulation?\nThere are two principal motivating reasons for modulation. We described the first in Chap\nter 10: matching the transmission characteristics of the medium, and considerations of\npower and antenna size, which impact portability. The second is the desire to multiplex, or\nshare, a communication medium among many concurrently active users.\n-\n14.1.1\nPortability\nMobile phones and other wireless devices send information across free space using electro\nmagnetic waves. To send these electromagnetic waves across long distances in free space,\nthe frequency of the transmitted signal must be quite high compared to the frequency of\nthe information signal. For example, the signal in a cell phone is a voice signal with a\nbandwidth of about 4 kHz. The typical frequency of the transmitted and received signal is\nseveral hundreds of megahertz to a few gigahertz (for example, the popular WiFi standard\nis in the 2.4 GHz or 5+ GHz range).\n\nCHAPTER 14. MODULATION AND DEMODULATION\nWi-Fi\nFigure 14-1: Top: Spectrum allocation in the United States (3 kHz to 300 GHz). Bottom: a portion of the to\ntal allocation, highlighting the 2.4 GHz ISM (Industrial, Scientific, and Medical) band, which is unlicensed\nspectrum that can be used for a variety of purposes, including 802.11b/g (WiFi), various cordless tele\nphones, baby monitors, etc.\nOne important reason why high-frequency transmission is attractive is that the size of\nthe antenna required for efficient transmission is roughly one-quarter the wavelength of\nthe propagating wave, as discussed in Chapter 10. Since the wavelength of the (electro\nmagnetic) wave is inversely proportional to the frequency, the higher the frequency, the\nsmaller the antenna. For example, the wavelength of a 1 GHz electromagnetic wave in free\nspace is 30 cm, whereas a 1 kHz electromagnetic wave is one million times larger, 300 km,\nwhich would make for an impractically huge antenna and transmitter power to transmit\nsignals of that frequency!\n-\n14.1.2\nSharing using Frequency-Division\nFigure 14-1 shows the electromagnetic spectrum from 3 kHz to 300 GHz; it depicts how\nportions of spectrum have been allocated by the U.S. Federal Communications Commis\nThis image was created by the US Department of Commerce, and is in the public domain.\n\nSECTION 14.1. WHY MODULATION?\nFigure 14-2: An analog waveform corresponding to someone saying \"Hello\". Picture from http://\nelectronics.howstuffworks.com/analog-digital2.htm. The frequency content and spectrum of\nthis waveform is inherently band-limited to a few kilohertz.\nsion (FCC), which is the government agency that allocates this \"public good\" (spectrum).\nWhat does \"allocation\" mean? It means that the FCC has divided up frequency ranges\nand assigned them for different uses and to different entities, doing so because one can be\nassured that concurrent transmissions in different frequency ranges will not interfere with\neach other.\nThe reason why this approach works is that when a sinusoid of some frequency is sent\nthrough a linear, time-invariant (LTI) channel, the output is a sinusoid of the same frequency,\nas we discovered in Chapter 12. Hence, if two different users send pure sinusoids at dif\nferent frequencies, their intended receivers can extract the transmitted sinusoid by simply\napplying the appropriate filter, using the principles explained in Chapter 12.\nOf course, in practice one wants to communicate a baseband signal rather than a sinu\nsoid over the channel. The baseband signal will often have been produced from a digital\nsource. One can, as explained in Chapters 9 and 10, map each \"1\" to a voltage V1 held\nfor some interval of time, and each \"0\" to a voltage V0 held for the same duration (let's\nassume for convenience that both V1 and V0 are non-negative). The result is some wave\nform that might look like the picture shown in Figure 10-2.1 Alternatively, the baseband\nsignal may come from an analog source, such as a microphone in an analog telephone,\nwhose waveform might look like the picture shown in Figure 14-2; this signal is inherently\n\"band-limited\" to a few kilohertz, since it is produced from human voice. Regardless of\nthe provenance of the input baseband signal, the process of modulation involves preparing\nthe signal for transmission over a channel.\nIf multiple users concurrently transmitted their baseband signals over a shared\n1We will see in the next section that we will typically remove its higher frequencies by lowpass filtering, to\nobtain a \"band-limited\" baseband signal.\n(c) HowStuffWorks, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nCHAPTER 14. MODULATION AND DEMODULATION\nmedium, it would be difficult for their intended receivers to extract the signals reliably\nbecause of interference. One approach to reduce this interference, known as frequency-\ndivision multiplexing, allocates different carrier frequencies to different users (or for dif\nferent uses, e.g., one might separate out the frequencies at which police radios or emer\ngency responders communicate from the frequencies at which you make calls on your\nmobile phone). In fact, the US spectrum allocation map shown in Figure 14-1 is the result\nof such a frequency-division strategy. It enables users (or uses) that may end up with sim\nilar looking baseband signals (those that will interfere with each other) to be transmitted\non different carrier frequencies, eliminating interference.\nThere are two reasons why frequency-division multiplexing works:\n1. Any baseband signal can be broken up into a weighted sum of sinusoids using\nFourier decomposition (Chapter 13). If the baseband signal is band-limited, then\nthere is a finite maximum frequency of the corresponding sinusoids. One can take\nthis sum and modulate it on a carrier signal of some other frequency in a simple\nway: by just multiplying the baseband and carrier signal (also called \"mixing\"). The\nresult of modulating a band-limited baseband signal on to a carrier is a signal that is\nband-limited around the carrier, i.e., limited to some maximum frequency deviation from\nthe carrier frequency.\n2. When transmitted over a linear, time-invariant (LTI) channel, and if noise is negli\ngible, each sinusoid shows up at the receiver as a sinusoid of the same frequency, as\nwe saw in Chapter 12. The reason is that an LTI system preserves the sinusoids. If we\nwere to send a baseband signal composed of a sum of sinusoids over the channel,\nthe output will be the sum of sinuoids of the same frequencies. Each receiver can\nthen apply a suitable filter to extract the baseband signal of interest to it. This insight\nis useful because the noise-free behavior of real-world communication channels is\noften well-characterized as an LTI system.\n- 14.2\nAmplitude Modulation with the Heterodyne Principle\nThe heterodyne principle is the basic idea governing several different modulation\nschemes. The idea is simple, though the notion that it can be used to modulate signals\nfor transmission was hardly obvious before its discovery!\nHeterodyne principle: The multiplication of two sinusoidal waveforms may\nbe written as the sum of two sinusoidal waveforms, whose frequencies are\ngiven by the sum and the difference of the frequencies of the sinusoids being\nmultiplied.\nThis result may be seen from standard high-school trigonometric identities, or by (per\nhaps more readily) writing the sinusoids as complex exponentials and performing the mul\ntiplication. For example, using trigonometry,\n\ncos(Ωsn) · cos(Ωcn) =\ncos(Ωs + Ωc)n + cos(Ωs - Ωc)n .\n(14.1)\n\nSECTION 14.2. AMPLITUDE MODULATION WITH THE HETERODYNE PRINCIPLE\nWe apply the heterodyne principle by treating the baseband signal --think of it as periodic\nwith period 2π for now--as the sum of different sinusoids of frequencies Ωs1 = k1Ω1,Ωs2 =\nΩ1\nk2Ω1,Ωs3 = k3Ω1 ... and treating the carrier as a sinusoid of frequency Ωc = kcΩ1. Here, Ω1\nis the fundamental frequency of the baseband signal.\n×\nx[n]\nt[n]\ncos(kcΩ1n)\nFigure 14-3: Modulation involved \"mixing\", or multiplying, the input signal x[n] with a carrier signal\n(cos(Ωcn) = cos(kcΩ1n) here) to produce t[n], the transmitted signal.\nThe application of the heterodyne principle to modulation is shown schematically in\nFigure 14-3. Mathematically, we will find it convenient to use complex exponentials; with\nthat notation, the process of modulation involves two important steps:\n1. Shape the input to band-limit it. Take the input baseband signal and apply a low-\npass filter to band-limit it. There are multiple good reasons for this input filter, but\nthe main one is that we are interested in frequency division multiplexing and wish\nto make sure that there is no interference between concurrent transmissions. Hence,\nif we limit the discrete-time Fourier series (DTFS) coefficients to some range, call it\n[-kx,-kx], then we can divide the frequency spectrum into non-overlapping ranges\nof size 2kx to ensure that no two transmissions interfere. Without such a filter, the\nbaseband could have arbitrarily high frequencies, making it hard to limit interference\nin general. Denote the result of shaping the original input by x[n]; in effect, that is\nthe baseband signal we wish to transmit. An example of the original baseband signal\nand its shaped version is shown in Figure 14-4.\nWe may express x[n] in terms of its discrete-time Fourier series (DTFS) representation\nas follows, using what we learned in Chapter 13:\nkx\nL\njkΩ1n\nx[n] =\nAke\n.\n(14.2)\nk=-kx\nNotice how applying the input filter ensures that high-frequency components are\nzero; the frequency range of the baseband is now [-kxΩ1,kxΩ1] radians/sample.\n2. Mixing step. Multiply x[n] (called the baseband modulating signal) by a carrier,\ncos(kcΩ1n), to produce the signal ready for transmission, t[n]. Using the DTFS form,\n\nCHAPTER 14. MODULATION AND DEMODULATION\nBaseband input x[n]: shaped pulses to band-limit signal\nCarrier signal\nTransmitted signal t[n]: \"mix\" (multiply x[n] and carrier)\nFigure 14-4: The two modulation steps, input filtering (shaping) and mixing, on an example signal.\nwe get\nEquation (14.3) makes it apparent (see the underlined terms) that the process of mix\ning produces, for each DTFS component, two frequencies of interest: one at the sum\nand the other at the difference of the mixed (multiplied) frequencies, each scaled to be\none-half in amplitude compared to the original.\nWe transmit t[n] over the channel. The heterodyne mixing step may be explained math\nematically using Equation (14.3), but you will rarely need to work out the math from\nscratch in any given problem: all you need to know and appreciate is that the (shaped)\nbaseband signal is simply replicated in the frequency domain at two different frequencies,\n±kc, which are the nonzero DTFS coefficients of the carrier sinusoidal signal, and scaled by\n1/2. We show this outcome schematically in Figure 14-5.\nThe time-domain representation shown in Figure 14-4 is not as instructive as the\nfrequency-domain picture to gain intuition about what modulation does and why frequency-\ndivision multiplexing avoids interference. Figure 14-6 shows the same information as Fig\nure 14-4, but in the frequency domain. The caption under that figure explains the key\nt[n]\n=\n⇣\nkx\nX\nk=-kx\nAkejk⌦1n⌘⇣1\n2(ejkc⌦1n + e-jkc⌦1n)\n⌘\n=\nkx\nk=-kx\nAkej(k+kc)⌦1n + 1\nkx\nk=-kx\nAkej(k-kc)⌦1n.\nX\nX\n(14.3)\n\nSECTION 14.3. DEMODULATION: THE SIMPLE NO-DELAY CASE\nFor band-limited signal\nI.e., just replicate baseband\nAk are nonzero only for\nsignal at ±kc, and scale\nsmall range of ±k\nby 1⁄2.\nA/2\nt[n] =\nA/2\n=\nAke jkΩ1n\nk=-kx\nkx ∑\n⎡\n⎣\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n2 e jkcΩ1n + 1\n2 e- jkcΩ1n\n⎡\n⎣⎢\n⎤\n⎦⎥\nAke\nj k+kc\n(\n)Ω1n\nk=-kx\nkx ∑\nj k-k\nRe(ak)\nk\n+\nAke\nc\n(\n)Ω1n\nk=-kx\nkx ∑\nIm(ak)\n\n+kc\nc\nFigure 14-5: Illustrating the heterodyne principle.\ninsights.\nThis completes our discussion of the modulation process, at least for now (we'll revisit\nit in Section 14.5), bringing us to the question of how to extract the (shaped) baseband\nsignal at the receiver. We turn to this question next.\n- 14.3\nDemodulation: The Simple No-Delay Case\nAssume for simplicity that the receiver captures the transmitted signal, t[n], with no distor\ntion, noise, or delay; that's about as perfect as things can get. Let's see how to demodulate\nthe received signal, r[n] = t[n], to extract x[n], the shaped baseband signal.\nThe trick is to apply the heterodyne principle once again: multiply the received signal\nby a local sinusoidal signal that is identical to the carrier! An elegant way to see what would\nhappen is to start with Figure 14-6, rather than the time-domain representation. We now\ncan pretend that we have a \"baseband\" signal whose frequency components are as shown\nin Figure 14-6, and what we're doing now is to \"mix\" (i.e., multiply) that with the carrier.\nWe can accordingly take each of the two (i.e., real and imaginary) pieces in the right-most\ncolumn of Figure 14-6 and treat each in turn.\nThe result is shown in Figure 14-7. The left column shows the frequency components\nof the original (shaped) baseband signal, x[n]. The middle column shows the frequency\ncomponents of the modulated signal, t[n], which is the same as the right-most column of\nFigure 14-6. The carrier (cos(35Ω1n), so the DTFS coefficients of t[n] are centered around\nk = -35 and k = 35 in the middle column. Now, when we mix that with a local signal\nidentical to the carrier, we will shift each of these two groups of coefficients by ±35 once\nagain, to see a cluster of coefficients at -70 and 0 (from the -35 group) and at 0 and +70\n(from the +35 group). Each piece will be scaled by a further factor of 1/2, so the left and\nright clusters on the right-most column in Figure 14-7 will be 1/4 as large as the original\nbaseband components, while the middle cluster centered at 0, with the same spectrum as the\noriginal baseband signal, will be scaled by 1/2.\nWhat we are interested in recovering is precisely this middle portion, centered at 0, be\n\nCHAPTER 14. MODULATION AND DEMODULATION\nBand-limited x[n]\ncos(35Ω1n)\nt[n]\nFigure 14-6: Frequency-domain representation of Figure 14-4, showing how the DTFS components (real\nand imaginary) of the real-valued band-limited signal x[n] after input filtering to produce shaped pulses\n(left), the purely cosine sinusoidal carrier signal (middle), and the heterodyned (mixed) baseband and\ncarrier at two frequency ranges whose widths are the same as the baseband signal, but that have been\nshifted ±kc in frequency, and scaled by 1/2 each (right). We can avoid interference with another signal\nwhose baseband overlaps in frequency, by using a carrier for the other signal sufficiently far away in\nfrequency from kc.\ncause in the absence of any distortion, it is exactly the same as the original (shaped) baseband,\nexcept that is scaled by 1/2.\nHow would we recover this middle piece alone and ignore the left and right clusters,\nwhich are centered at frequencies that are at twice the carrier frequency in the positive and\nnegative directions? We have already studied a technique in Chapter 12: a low-pass filter.\nBy applying a low-pass filter whose cut-off frequency lies between kx and 2kc - kx, we can\nrecover the original signal faithfully.\nWe can reach the same conclusions by doing a more painstaking calculation, similar to\nthe calculations we did for the modulation, leading to Equation (14.3). Let z[n] be the sig\nnal obtained by multiplying (mixing) the local replica of the carrier cos(kcΩ1n) and the re\nceived signal, r[n] = t[n], which is of course equal to x[n] cos(kcΩ1n). Using Equation 14.3,\nwe can express z[n] in terms of its DTFS coefficients as follows:\n\nSECTION 14.3. DEMODULATION: THE SIMPLE NO-DELAY CASE\nx[n]\nt[n]\nz[n]\nFigure 14-7: Applying the heterodyne principle in demodulation: frequency-domain explanation. The left\ncolumn is the (shaped) baseband signal spectrum, and the middle column is the spectrum of the modu\nlated signal that is transmitted and received. The portion shown in the vertical rectangle in the right-most\ncolumn has the DTFS coefficients of the (shaped) baseband signal, x[n], scaled by a factor of 1/2, and may\nbe recovered faithfully using a low-pass filter. This picture shows the simplified ideal case when there is\nno channel distortion or delay between the sender and receiver.\nThe middle term, underlined, is what we want to extract. The first term is at twice the\ncarrier frequency above the baseband, while the third term is at twice the carrier frequency\nbelow the baseband; both of those need to be filtered out by the demodulator.\n-\n14.3.1\nHandling Channel Distortions\nThus far, we have considered the ideal case of no channel distortions or delays. We relax\nthis idealization and consider channel distortions now. If the channel is LTI (which is very\nz[n]\n=\nt[n]\n⇣1\n2ejkc⌦1n + 1\n2e-jkc⌦1n⌘\n=\n⇣1\nkx\nX\nk=-kx\nAkej(k+kc)⌦1n + 1\nkx\nX\nk=-kx\nAkej(k-kc)⌦1n⌘⇣1\n2ejkc⌦1n + 1\n2e-jkc⌦1n⌘\n=\nkx\nX\nk=-kx\nAkej(k+2kc)⌦1n + 1\nkx\nX\nk=-kx\nAkejk⌦1n + 1\nkx\nX\nk=-kx\nAkej(k-2kc)⌦1n\n(14.4)\n\nCHAPTER 14. MODULATION AND DEMODULATION\n\ny[n]\nz[n]\nt[n]\nChannel\nH(Ω)\ncos(kcΩ1n)\nFigure 14-8: Demodulation in the presence of channel distortion characterized by the frequency response\nof the channel.\noften the case), then one can extend the approach described above. The difference is that\neach of the Ak terms in Equation (14.4), as well as Figure 14-7, will be multiplied by the\nfrequency response of the channel, H(Ω), evaluated at a frequency of kΩ1. So each DTFS\ncoefficient will be scaled further by the value of this frequency response at the relevant\nfrequency.\nFigure 14-8 shows the model of the system now. The modulated input, t[n], traverses\nthe channel en route to the demodulator at the receiver. The result, z[n], may be written as\nfollows:\nOf these three terms in the RHS of Equation (14.5), the first term contains the baseband\nsignal that we want to extract. We can do that as before by applying a lowpass filter to get\nrid of the ±2kc components. To then recover each Ak, we need to pass the output of the\nlowpass filter to another LTI filter that undoes the distortion by multiplying the kth Fourier\ncoefficient by the inverse of H((k + kc)Ω1) + H((k - kc)Ω1). Doing so, however, will also\namplify any noise at frequencies where the channel attenuated the input signal t[n], so a\nbetter solution is obtained by omitting the inversion at such frequencies.\nFor this procedure to work, the channel must be relatively low-noise, and the receiver\nneeds to know the frequency response, H(Ω), at all the frequencies of interest in Equation\n(14.5); i.e., in the range [-kc - kx,-kc + kx] and [kc - kx,kc + kx]. To estimate H(Ω), a com\nmon approach is to send a known preamble at the beginning of each packet (or frame)\nz[n]\n=\ny[n]cos(kc⌦1n)\n=\ny[n]\n⇣1\n2ejkc⌦1n + 1\n2e-jkc⌦1n⌘\n=\n⇣1\nkx\nX\nk=-kx\nH((k + kc)⌦1)Akej(k+kc)⌦1n + 1\nkx\nX\nk=-kx\nH((k -kc)⌦1)Akej(k-kc)⌦1n⌘\n·\n⇣1\n2ejkc⌦1n + 1\n2e-jkc⌦1n⌘\n=\nkx\nX\nk=-kx\nAkejk⌦1n⇣\nH((k + kc)⌦1) + H((k -kc)⌦1)\n⌘\n+\nkx\nX\nk=-kx\nAkej(k+2kc)⌦1n⇣\nH((k + kc)⌦1) + H((k -kc)⌦1)\n⌘\n+\nkx\nk=\nkx\nAkej(k-2kc)⌦1n H((k + kc)⌦1) + H((k -kc)⌦1)\n(14.5)\nX\n-\n⇣\n⌘\n\nc\nSECTION 14.4. HANDLING CHANNEL DELAY: QUADRATURE DEMODULATION\n×\nt[n]\nz[n]\nLPF\ny[n]\nCutoff @ ±kx\nFilter gain depends on H values\ncos(k Ω1n)\nFigure 14-9: Demodulation steps: the no-delay case (top). LPF is a lowpass filter. The graphs show the\ntime-domain representations before and after the LPF.\nof transmission. The receiver looks for this known preamble to synchronize the start of\nreception, and because the transmitted signal pattern is known, the receiver can deduce\nchannel's the unit sample response, h[·], from it, using an approach similar to the one out\nlined in Chapter 11. One can then apply the frequency response equation from Chapter\n12, Equation (2.2), to estimate H(Ω) and use it to approximately undo the distortion intro\nduced by the channel.\nUltimately, however, our interest is not in accurately recovering x[n], but rather the\nunderlying bit stream. For this task, what is required is typically not an inverse filtering\noperation. We instead require a filtering that produces a signal whose samples, obtained at\nthe bit rate, allow reliable decisions regarding the corresponding bits, despite the presence\nof noise. The optimal filter for this task is called the matched filter. We leave the discussion\nof the matched filter to more advanced courses in communication.\n- 14.4\nHandling Channel Delay: Quadrature Demodulation\nWe now turn to the case of channel delays between the sender and receiver. This delay\nmatters in demodulation because we have thus far assumed that the sender and receiver\nhave no phase difference with respect to each other. That assumption is, of course, not\ntrue, and one needs to somehow account for the phase delays.\nLet us first consider the illustrative case when there is a phase error between the sender\n\nCHAPTER 14. MODULATION AND DEMODULATION\nTime delay of D samples\nX\nt[n]\nz[n]\nLPF\ny[n]\nCutoff @ ±kx\nX\nx[n]\nD\ntD[n]\nGain depends on H\ncos(kcΩ1n)\ncos(kcΩ1n)\nFigure 14-10: Model of channel with a delay of D samples.\nand receiver. We will then show that a non-zero delay on the channel may be modeled\nexactly like a phase error. By \"phase error\", we mean that the demodulator, instead of\nmultiplying (heterodyning) by cos(kcΩ1n), multiplies instead by cos(kcΩ1n - φ), where φ\nis some constant value. Let us understand what happens to the demodulated output in\nthis case.\nWorking out the algebra, we can write\nz[n]\n=\nt[n] cos(kcΩ1n - φ)\n= x[n] cos(kcΩ1n) cos(kcΩ1n - φ)\n(14.6)\nBut noting that\nit follows that the demodulated output, after the LPF step with the suitable gains, is\ny[n] = x[n]cos φ.\nHence, a phase error of φ radians results in the demodulated amplitude being scaled\nby cos φ. This scaling is problematic: if we were unlucky enough to have the error close\nto π/2, then we would see almost no output at all! And if x[n] could take on both positive\nand negative values, then cos φ going negative would cause further confusion.\nA channel delay between sender and receiver manifests itself as a phase error using the\ndemodulation strategy we presented in Section 14.3. To see why, consider Figure 14-10,\nwhere we have inserted a delay of D samples between sender and receiver. The algebra is\nvery similar to the phase error case: with a sample delay of D samples, we find that\ny[n] = t[n - D] cos(kcΩ1n) = x[n - D] cos(kcΩ1(n - D)) cos(kcΩ1n).\nThe first cos factor in effect looks like it has a phase error of kcΩ1D, so the output is attenu\nated by cos(kcΩ1D).\nSo how do we combat phase errors? One approach is to observe that in situations\nwhere cos φ is 0, sin φ is close to 1. So, in those cases, multiplying (heterodyning) at the\ndemodulator by sin(kcΩ1n) = cos( π - kcΩ1n) corrects for the phase difference. Notice,\nhowever, that if the phase error were non-existent, then multiplying by sin(kcΩ1n) would\ncos(kc⌦1n)cos(kc⌦1n -') = 1\n⇣\ncos(2kc⌦1n -') + cos'\n⌘\n,\n\nSECTION 14.4. HANDLING CHANNEL DELAY: QUADRATURE DEMODULATION\n×\nLPF\nI[n] = x[n-D]·cos(θ)\ntD[n]=t[n-D]\nCutoff @ ±kin\nFrom\nGain = 2\nθ = ΩcD - φ\ncos(Ωcn-φ)\nchannel\n×\nLPF\nQ[n] = x[n-D]·sin(θ)\nCutoff @ ±kin\nGain = 2\nsin(Ωcn-φ)\nFigure 14-11: Quadrature demodulation to handle D-sample channel delay.\nlead to no baseband signal--you should verify this fact by writing\nand expanding t[n] using its DTFS. Hence, multiplying by the sin when the carrier is a cos\nwill not always work; it will work only when the phase error is a fortunate value (≈ π/2).\nThis observation leads us to a solution to this problem, called quadrature demodula\ntion, depicted in Figure 14-11 for the case of channel delay but no channel distortion (so\nwe can apply a gain of 2 on the LPFs rather than factors dependent on H(Ω)). The idea is\nto multiply the received signal by both cos(Ωcn) (where Ωc = kcΩ1 is the carrier frequency),\nand sin(Ωcn). This method is a way of \"hedging\" our bet: we cannot be sure which term,\ncos or sin would work, but we can be sure that they will not be 0 at the same time! We can\nuse this fact to recover the signal reliably always, as explained below.\nFor simplicity (and convenience), suppose that x[n] ≥ 0 always (at the input). Then,\nusing the notation from Figure 14-11, define w[n] = I[n] + jQ[n] (the I term is generally\ncalled the in-phase term and the Q term is generally called the quadrature term). Then,\nJ\n|w[n]| =\n(I[n])2 + (Q[n])2\n\n= |x[n - D]|\n(cos2θ + sin2 θ)\n= |x[n - D]|\n(14.7)\n= x[n - D] becausex[·] ≥ 0\n(14.8)\nHence, the quadrature demodulator performs the following step, in addition to the\nones for the no-delay case explained before: compute I[n] and Q[n], and calculate |w[n]|\nusing Equation (14.8). Return this value, thresholding (to combat noise) at the mid-point\nbetween the voltage levels corresponding to a \"0\" and a \"1\". With quadrature demodula\ntion, suppose the sender sends 0 volts for a \"0\" and 1 volt for a \"1\", the receiver would,\nin general, demodulate a rotated version in the complex plane, as shown in Figure 14-12.\nHowever, the magnitude will still be 1, and quadrature demodulation can successfully\nrecover the input.\nFigure 14-13 summarizes the various steps of the quadrature demodulator that we de\nscribed in this section.\nThis concludes our discussion of the basics of demodulation. We turn next to briefly\nz[n] = t[n]sin(kc⌦1n) = t[n]\n⇣-j ejkc⌦1n + j e-jkc⌦1n⌘\n,\n\nCHAPTER 14. MODULATION AND DEMODULATION\njQ\nx[n-D]sin(θ)\nθ\nConstellation diagrams:\nx[n-D] = { 0, 1 }\nI\nx[n-D]cos(θ)\nI\nQ\nI\nQ\ntransmitter\nreceiver\n\nFigure 14-12: Quadrature demodulation. The term \"constellation diagram\" refers to the values that the\nsender can send, in this case just 0 and 1 volts. The receiver's steps are shown in the picture.\nsurvey more sophisticated modulation/demodulation schemes.\n- 14.5\nMore Sophisticated (De)Modulation Schemes\nWe conclude this chapter by briefly outlining three more sophisticated (de)modulation\nschemes.\n-\n14.5.1\nBinary Phase Shift Keying (BPSK)\nIn BPSK, as shown in Figure 14-14, the transmitter selects one of two phases for the carrier,\ne.g. -π/2 for \"0\" and π/2 for \"1\". The transmitter does the same mixing with a sinusoid\nas explained earlier. The receiver computes the I and Q components from its received\nwaveform, as before. This approach \"almost\" works, but in the presence of channel delays\nor phase errors, the previous strategy to recover the input does not work because we had\nassumed that x[n] ≥ 0. With BPSK, x[n] is either +1 or -1, and the two levels we wish to\ndistinguish have the same magnitude on the complex plane after quadrature demodula\ntion!\nThe solution is to think of the phase encoding as a differential, not absolute: a change in\nphase corresponds to a change in bit value. Assume that every message starts with a \"0\"\nbit. Then, the first phase change represents a 0 → 1 transition, the second phase change a\n1 → 0 transition, and so on. One can then recover all the bits correctly in the demodulator\nusing this idea, assuming no intermediate glithces (we will not worry about such glitches\nhere, which do occur in practice and must be dealt with).\n-\n14.5.2\nQuadrature Phase Shift Keying (QPSK)\nQuadrature Phase Shift Keying is a clever idea to add a \"degree of freedom\" to the system\n(and thereby extracting higher performance). This method, shown in Figure 14-15, uses a\nquadrature scheme at both the transmitter and the receiver. When mapping bits to voltage\nvalues in QPSK, we would choose the values so that the amplitude of t[n] is constant.\nMoreover, because the constellation now involves four symbols, we map two bits to each\n\nSECTION 14.5. MORE SOPHISTICATED (DE)MODULATION SCHEMES\nFigure 14-13: Quadrature demodulation: overall system view. The \"alternative representation\" shown\nimplements the quadrature demodulator using a single complex exponential multiplication, which is a\nmore compact representation and description.\nsymbol. So 00 might map to (A, A), 01 to (-A, A), 11 to (-A, -A), and 10 to (A, -A)\n√\n(the amplitude is therefore\n2A). There is some flexibility in this mapping, but it is not\ncompletely arbitrary; for example, we were careful here to not map 11 to (A, -A) and 00 to\n(A, A). The reason is that any noise is more likely to cause (A, A) to be confused fo (A, -A),\ncompared to (-A, -A), so we would like a symbol error to corrupt as few bits as possible.\n-\n14.5.3\nQuadrature Amplitude Modulation (QAM)\nQAM may be viewed as a generalization of QPSK (in fact, QPSK is sometimes called QAM\n4). One picks additional points in the constellation, varying both the amplitude and the\nphase. In QAM-16 (Figure 14-16), we map four bits per symbol. Denser QAM constella\ntions are also possible; practical systems today use QAM-4 (QPSK), QAM-16, and QAM\n64. Quadrature demodulation with the adjustment for phase is the demodulation scheme\nused at the receiver with QAM.\nFor a given transmitter power, the signal levels corresponding to different bits at the\ninput get squeezed closer together in amplitude as one goes to constellations with more\npoints. The resilience to noise reduces because of this reduced separation, but sophisti\ncated coding and signal processing techniques may be brought to bear to deal with the\neffects of noise to achieve higher communication bit rates. In many real-world commu\nnication systems, the physical layer provides multiple possible constellations and choice\nof codes; for any given set of channel conditions (e.g., the noise variance, if the channel\nis well-described using the AWGN model), there is some combination of constellation,\ncoding scheme, and code rate, which maximizes the rate at which bits can be received\n\ncos(Ωcn-φ)\nLPF\nI[n] = x[n-D]·cos(θ)\nCutoff @ kin\n\nsin(Ωcn-φ)\nLPF\nQ[n] = x[n-D]·sin(θ)\nCutoff @ kin\nθ = ΩcD - φ\n| |2\n| |2\n+\nsqrt()\ny[n]=|x[n-D]|\nQ[n]2\nI[n]2\ny[n] = sqrt(I[n]2+Q[n]2)\nt[n]\n\ncos(Ωcn)\nx[n]\nD\ntD[n]\nTransmitter\nChannel\nReceiver\nDecimate &\nslice\nReceived\nbits\ny[n]\nI[n]\nQ[n]\nDelay\nBits to\nsamples\nTransmit\nbits\ntD[n] = t[n-D]\nAlternative\nrepresentation\n\nLPF\nCutoff @ kin\n| |\n|y[n]|=|x[n-D]|\ntD[n]\nReceiver\nDecimate &\nslice\nReceived\nbits\nQuadrature demodulator\n)\n(\nφ\n-\nΩ n\nj\nc\ne\n)\n(\n]\n[\n]\n[\nφ\n-\nΩ\n-\n=\nD\nj\nc\ne\nD\nn\nx\nn\ny\nQuadrature demodulator\n\ni\n6.02 Spring 2012\n\nCHAPTER 14. MODULATION AND DEMODULATION\nIn binary phase-shift keying (BPSK), the message bit selects one of\nI\ntwo phases for the carrier, e.g., T/2 for 0 and -T/2 for 1.\nQ\n×\nsin(Ωcn)\n(-1,1)\nx[n]\n×\ncos(Ωcn)\n×\nsin(Ωcn)\nLPF\n\nLPF\nphase[n]\n6.02 Spriiriiririri\npriririririing\nng\nng\nng\nn\nI[n]\nQ[n]\nFigure 14-14: Binary Phase Shift Keying (BPSK).\nand decoded reliably. Higher-layer \"bit rate selection\" protocols use information about\nthe channel quality (signal-to-noise ratio, packet loss rate, or bit error rate) to make this\ndecision.\n- Acknowledgments\nThanks to Mike Perrot, Charlie Sodini, Vladimir Stojanovic, and Jacob White for lectures\nthat helped shape the topics discussed in this chapter, and to Ruben Madrigal and Patricia\nSaylor for bug fixes.\n\nSECTION 14.5. MORE SOPHISTICATED (DE)MODULATION SCHEMES\nStill need band limiting at transmitter\n(-A,A)\nLPF\nQ\nmsg[0::2]\n×\ncos(Ωcn)\n×\nsin(Ωcn)\n+\nI[n]\nQ[n]\n(-A,A)\n(A,A)\nEven bits\nI\nt[n]\n(-A,A)\nLPF\n(-A,-A)\n(A,-A)\nmsg[1::2]\nOdd bits\nMap bit into voltage value\nFigure 14-15: Quadrature Phase Shift Keying (QPSK).\nStill need band-limiting at transmitter\nmsg[0::4]\nEven pairs of bits\n(-3A,-A,\nA, 3A)\n×\ncos(Ωcn)\n×\nsin(Ωcn)\nLPF\n+\nI[n]\nQ[n]\nLPF\n(-3A,-A,\nA, 3A)\nt[n]\nQ\nMsg[2::4]\nOdd pairs of bits Map bits into voltage value\nI\nSymbol/bits mapping table\n00 -3A\n01 -A\n11 A\n00 01\n10 11\n10 3A\nGray Code (noise movement into another constellation point\nonly causes single bit errors)\nFigure 14-16: Quadrature Amplitude Modulation (QAM).\n-\n\nProblems\n\nand\n\nQuestions\n\n1. The Boston sports radio station WEEI AM (\"amplitude modulation\") broadcasts on a\ncarrier frequency of 850 kHz, so its continuous-time (CT) carrier signal can be taken\nto be cos(2π × 850 × 103t), where t is measured in seconds. Denote the CT audio\nsignal that's modulated onto this carrier by x(t), so that the CT signal transmitted by\nthe radio station is\ny(t) = x(t)cos(2π × 850 × 103t) ,\n(14.9)\nas indicated schematically on the left side of the figure below.\n\nCHAPTER 14. MODULATION AND DEMODULATION\nWe use the symbols y[n] and x[n] to denote the discrete-time (DT) signals that would\nhave been obtained by respectively sampling y(t) and x(t) in Equation (14.9) at fs\nsamples/sec; more specifically, the signals are sampled at the discrete time instants\nt = n(1/fs). Thus\ny[n] = x[n]cos(Ωcn)\n(14.10)\nfor an appropriately chosen value of the angular frequency Ωc. Assume that x[n] is\nperiodic with some period N, and that fs = 2 × 106 samples/sec.\nAnswer the following questions, explaining your answers in the space provided.\n(a) Determine the value of Ωc in Equation (14.10), restricting your answer to a value\nin the range [-π,π]. (You can assume in what follows that the period N of x[n] is\nsuch that Ωc = 2kcπ/N for some integer kc; this is a detail, and needn't concern\nyou unduly.)\n(b) Suppose the Fourier series coefficients X[k] of the DT signal x[n] in Equation\n(14.10) are purely real, and are as shown in the figure below, plotted as a function\nof Ωk = 2kπ/N. (Note that the figure is not drawn to scale. Also, the different\nvalues of Ωk are so close to each other that we have just interpolated adjacent\nvalues of X[k] with a straight line, rather than showing you a discrete \"stem\"\nplot.) Observe that the Fourier series coefficients are non-zero for frequencies\nΩk in the interval [-.005π,.005π], and 0 at all other Ωk in the interval [-π,π].\nDraw a carefully labeled sketch below (though not necessarily to scale) to show\nthe Fourier series coefficients of the DT modulated signal y[n]. However, rather\nthan labeling your horizontal axis with the Ωk, as we have done above, you\nshould label the axis with the appropriate frequency fk in Hz.\nAssume now that the receiver detects the CT signal w(t) = 10-3y(t - t0), where t0 =\n3 × 10-6 sec, and that it samples this signal at fs samples/sec, thereby obtaining the\n\nSECTION 14.5. MORE SOPHISTICATED (DE)MODULATION SCHEMES\nfor an appropriately chosen integer M.\nC. Determine the value of M in Equation (14.11).\nD. Noting your answer from part B, determine for precisely which intervals of the\nfrequency axis the Fourier series coefficients of the signal y[n - M] in Equation\n(14.11) are non-zero. You need not find the actual coefficients, only the fre\nquency range over which these coefficients will be non-zero. Also state whether\nor not the Fourier coefficients will be real. Explain your answer.\nE. The demodulation step to obtain the DT signal x[n - M] from the received signal\nw[n] now involves multiplying w[n] by a DT carrier-frequency signal, followed\nby appropriate low-pass filtering (with the gain of the low-pass filter in its pass-\nband being chosen to scale the signal to whatever amplitude is desired). Which\none of the following six DT carrier-frequency signals would you choose to mul\ntiply the received signal by? Circle your choice and give a brief explanation.\ni. cos ⌦cn .\nii. cos\n⇣\n⌦c(n -M)\n⌘\n.\niii. cos\n⇣\n⌦c(n + M)\n⌘\n.\niv. sin\n⇣\n⌦cn\n⌘\n.\nv. sin\n⇣\n⌦c(n -M)\n⌘\n.\nvi. sin ⌦c(n + M) .\n⇣\n⌘\n⇣\n⌘\nDT signal\nw[n] = 10-3y[n\nM] = 10-3x[n\nM]cos ⌦c(n\nM)\n(14.11)\n-\n-\n⇣\n-\n⌘\n.\n\nCHAPTER 14. MODULATION AND DEMODULATION\nM-sample\ndelay\nx1[n]\nx2[n]\ny[n]\nw[n]\nv[n]\n-1000\n-500\n-250\nFigure 14-17: System for problem 2.\n2. All parts of this question pertain to the following modulation-demodulation system\nshown in Figure 14-17, where all signals are periodic with period P = 10000. Please\nalso assume that the sample rate associated with this system is 10000 samples per\nsecond, so that k is both a coefficient index and a frequency. In the diagram, the\nmodulation frequency, km, is 500.\n(a) Suppose the DFT coefficients for the signal y[n] in the modula\ntion/demodulation diagram are as plotted in Figure 14-17.\nAssuming that M = 0 for the M-sample delay (no delay), plot the coefficients\nfor the signals w and v in the modulation/demodulation diagram. Be sure to\nlabel key features such as values and coefficient indices for peaks.\n(b) Assuming the coefficients for the signal y[n] are the same as in part (a), please\nplot the DTFS coefficients for the signal x1 in the modulation/demodulation\ndiagram. Be sure to label key features such as values and coefficient indices for\npeaks.\n\nSECTION 14.5. MORE SOPHISTICATED (DE)MODULATION SCHEMES\n(c) If the M-sample delay in the modulation/demodulation diagram has the right\nnumber of samples of delay, then it will be possible to nearly perfectly recover\nx2[n] by low-pass filtering w[n]. Determine the smallest positive number of sam\nples of delay that are needed and the cut-off frequency for the low-pass filter.\nExplain your answer, using pictures if appropriate.\n3. Figure 14-18 shows a standard modulation/demodulation scheme where N = 100.\nFigure 14-18: System for problem 3.\n(a) Figure 14-19 shows a plot of the input, x[n]. Please draw the approximate time-\ndomain waveform for y[n], the signal that is the input to the low-pass filter in\nthe demodulator. Don't bother drawing dots for each sample, just use a line\nplot to indicate the important timing characteristics of the waveform.\nFigure 14-19: Plot for problem 3(a).\n(b) Building on the scheme shown in Part (a), suppose there are multiple modu\nlators and demodulators all connected to a single shared channel, with each\nmodulator given a different modulation frequency. If the low-pass filter in each\n\nCHAPTER 14. MODULATION AND DEMODULATION\nmodulator is eliminated, briefly describe what the effect will be on signal z[n],\nthe output of a demodulator tuned to the frequency of a particular transmitter.\n4. The plot on the left of Figure 14-20 shows ak, the DTFS coefficients of the signal at\nthe output of a transmitter with N = 36. If the channel introduces a 3-sample delay,\nplease plot the Fourier series coefficients of the signal entering the receiver.\nFigure 14-20: System for problem 4.\n\nSECTION 14.5. MORE SOPHISTICATED (DE)MODULATION SCHEMES\n5. Figure 14-21 shows an image rejection mixer. The frequency responses of the two\nfilter components (the 90-degree phase shift and the low-pass filter) are as shown.\nThe spectral plot to the left in figure above shows the spectrum of the input sig\nnal, x[n]. Using the same icon representation of a spectrum, draw the spectrum for\nsignals p[n],q[n],r[n], and s[n] below, taking care to label the center frequency and\nmagnitude of each spectral component. If two different icons overlap, simply draw\nthem on top of one another. If identical icons overlap, perform the indicated addi\ntion/subtraction, showing the net result with a bold line.\nFigure 14-21: Problem 5: image rejection mixer.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 15: Sharing a Channel: Media Access (MAC) Protocols",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/308c589e958e3d073568bde00a63ac76_MIT6_02F12_chap15.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 3, 2012\nCHAPTER 15\nSharing a Channel:\nMedia Access (MAC) Protocols\nThere are many communication channels, including radio and acoustic channels, and\ncertain kinds of wired links (coaxial cables), where multiple nodes can all be connected\nand hear each other's transmissions (either perfectly or with some non-zero probability).\nThis chapter addresses the fundamental question of how such a common communication\nchannel--also called a shared medium--can be shared between the different nodes.\nThere are two fundamental ways of sharing such channels (or media): time sharing and\nfrequency sharing.1 The idea in time sharing is to have the nodes coordinate with each other\nto divide up the access to the medium one at a time, in some fashion. The idea in frequency\nsharing is to divide up the frequency range available between the different transmitting\nnodes in a way that there is little or no interference between concurrently transmitting\nnodes. The methods used here are the same as in frequency division multiplexing, which\nwe described in the previous chapter.\nThis chapter focuses on time sharing. We will investigate two common ways: time\ndivision multiple access, or TDMA, and contention protocols. Both approaches are used in\nnetworks today.\nThese schemes for time and frequency sharing are usually implemented as communica\ntion protocols. The term protocol refers to the rules that govern what each node is allowed\nto do and how it should operate. Protocols capture the \"rules of engagement\" that nodes\nmust follow, so that they can collectively obtain good performance. Because these sharing\nschemes define how multiple nodes should control their access to a shared medium, they\nare termed media access (MAC) protocols or multiple access protocols.\nOf particular interest to us are contention protocols, so called because the nodes contend\nwith each other for the medium without pre-arranging a schedule that determines who\nshould transmit when, or a frequency reservation that guarantees little or no interference.\nThese protocols operate in laissez faire fashion: nodes get to send according to their own\n1There are other ways too, involving codes that allow multiple concurrent transmissions in the same fre\nquency band, with mechanisms to decode the individual communications. We won't study these more ad\nvanced ideas here. These ideas are sometimes used in practice, but all real-world systems use a combination\nof time and frequency sharing.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nFigure 15-1: The locations of some of the Alohanet's original ground stations are shown in light blue\nmarkers.\nvolition without any external agent telling them what to do. These contention protocols\nare well-suited for data networks, which are characterized by nodes transmitting data in\nbursts and at variable rates (we will describe the properties of data networks in more detail\nin a later chapter on packet switching).\nIn this chapter and the subsequent ones, we will assume that any message is broken up\ninto a set of one or more packets, and a node attempts to send each packet separately over\nthe shared medium.\n- 15.1\nExamples of Shared Media\nSatellite communications. Perhaps the first example of a shared-medium network de\nployed for data communication was a satellite network: the Alohanet in Hawaii. The Alo\nhanet was designed by a team led by Norm Abramson in the 1960s at the University of\nHawaii as a way to connect computers in the different islands together (Figure 15-1). A\ncomputer on the satellite functioned as a switch to provide connectivity between the nodes\non the islands; any packet between the islands had to be first sent over the uplink to the\nswitch,2 and from there over the downlink to the desired destination. Both directions used\nradio communication and the medium was shared. Eventually, this satellite network was\nconnected to the ARPANET (the precursor to today's Internet).\nSuch satellite networks continue to be used today in various parts of the world, and\nthey are perhaps the most common (though expensive) way to obtain connectivity in the\nhigh seas and other remote regions of the world. Figure 15-2 shows the schematic of such\na network connecting islands in the Pacific Ocean and used for teleconferencing.\nIn these satellite networks, the downlink usually runs over a different frequency band\nfrom the uplinks, which all share the same frequency band. The different uplinks, however,\nneed to be shared by different concurrent communications from the ground stations to the\nsatellite.\n2We will study switches in more detail in later lectures.\n(c) Europa Technologies, TerraMetrics, Google, and NASA. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nSECTION 15.1. EXAMPLES OF SHARED MEDIA\nFigure 15-2: A satellite network. The \"uplinks\" from the ground stations to the satellite form a shared\nmedium.\nWireless networks. The most common example of a shared communication medium to\nday, and one that is only increasing in popularity, uses radio. Examples include cellular\nwireless networks (including standards like EDGE, 3G, and 4G), wireless LANs (such as\n802.11, the WiFi standard), and various other forms of radio-based communication. An\nother example of a communication medium with similar properties is the acoustic channel\nexplored in the 6.02 labs. Broadcast is an inherent property of radio and acoustic communi\ncation, especially with so-called omni-directional antennas, which radiate energy in all (or\nmany) different directions. However, radio and acoustic broadcasts are not perfect because\nof interference and the presence of obstacles on certain paths, so different nodes may cor\nrectly receive different parts of any given transmission. This reception is probabilistic and\nthe underlying random processes that generate bit errors are hard to model.\nShared bus networks. An example of a wired shared medium is Ethernet, which when\nit was first developed (and for many years after) used a shared cable to which multiple\nnodes could be connected. Any packet sent over the Ethernet could be heard by all stations\nconnected physically to the network, forming a perfect shared broadcast medium. If two\nor more nodes send packets that overlap in time, both packets ended up being garbled and\nreceived in error.\nOver-the-air radio and television. Even before data communication, many countries in\nthe world had (and still have) radio and television broadcast stations. Here, a relatively\nsmall number of transmitters share a frequency range to deliver radio or television content.\nBecause each station was assumed to be active most of the time, the natural approach to\nsharing is to divide up the frequency range into smaller sub-ranges and allocate each sub-\nrange to a station (frequency division multiplexing).\nGiven the practical significance of these examples, and the sea change in network access\nbrought about by wireless technologies, developing methods to share a common medium\n(c) International Telecommunication Union. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nis an important problem.\n- 15.2\nModel and Goals\nBefore diving into the protocols, let's first develop a simple abstraction for the shared\nmedium and more rigorously model the problem we're trying to solve. This abstraction is\na reasonable first-order approximation of reality.\nWe are given a set of N nodes sharing a communication medium. We will assume N\nis fixed, but the protocols we develop will either continue to work when N varies, or can\nbe made to work with some more effort. Depending on the context, the N nodes may\nor may not be able to hear each other; in some cases, they may not be able to at all, in\nsome cases, they may, with some probability, and in some cases, they will always hear\neach other. Each node has some source of data that produces packets. Each packet may\nbe destined for some other node in the network. For now, we will assume that every node\nhas packets destined to one given \"master\" node in the network. Of course, the master\nmust be capable of hearing every other node, and receiving packets from those nodes. We\nwill assume that the master perfectly receives packets from each node as long as there are\nno \"collisions\" (we explain what a \"collision\" is below).\nThe model we consider has the following rules:\n1. Time is divided into slots of equal length, τ.\n2. Each node can send a packet only at the beginning of a slot.\n3. All packets are of the same size, and equal to an integral multiple of the slot length. In\npractice, packets will of course be of varying lengths, but this assumption simplifies\nour analysis and does not affect the correctness of any of the protocols we study.\n4. Packets arrive for transmission according to some random process; the protocol\nshould work correctly regardless of the process governing packet arrivals. If two\nor more nodes send a packet in the same time slot, they are said to collide, and none\nof the packets are received successfully. Note that even if only part of a packet en\ncounters a collision, the entire packet is assumed to be lost. This \"perfect collision\"\nassumption is an accurate model for wired shared media like Ethernet, but is only a\ncrude approximation of wireless (radio) communication. The reason is that it might\nbe possible for multiple nodes to concurrently transmit data over radio, and depend\ning on the positions of the receivers and the techniques used to decode packets, for\nthe concurrent transmissions to be received successfully.\n5. The sending node can discover that a packet transmission collided and may choose\nto retransmit such a packet.\n6. Each node has a queue; any packets waiting to be sent are in the queue. A node with\na non-empty queue is said to be backlogged.\nPerformance goals. An important goal is to provide high throughput, i.e., to deliver\npackets successfully at as high a rate as possible, as measured in bits per second. A mea\n\nSECTION 15.2. MODEL AND GOALS\nsure of throughput that is independent of the rate of the channel is the utilization, which\nis defined as follows:\nDefinition. The utilization that a protocol achieves is defined as the ratio of the total\nthroughput to the maximum data rate of the channel.\nFor example, if there are 4 nodes sharing a channel whose maximum bit rate is 10\nMegabits/s,3 and they get throughputs of 1, 2, 2, and 3 Megabits/s, then the utilization\nis (1 + 2 + 2 + 3)/10 = 0.8. Obviously, the utilization is always between 0 and 1. Note that\nthe utilization may be smaller than 1 either because the nodes have enough offered load\nand the protocol is inefficient, or because there isn't enough offered load. By offered load,\nwe mean the load presented to the network by a node, or the aggregate load presented to\nthe network by all the nodes. It is measured in bits per second as well.\nBut utilization alone isn't sufficient: we need to worry about fairness as well. If we\nweren't concerned about fairness, the problem would be quite easy because we could ar\nrange for a particular backlogged node to always send data. If all nodes have enough load\nto offer to the network, this approach would get high utilization. But it isn't too useful in\npractice because it would also starve one or more other nodes.\nA number of notions of fairness have been developed in the literature, and it's a topic\nthat continues to generate activity and interest. For our purposes, we will use a simple,\nstandard definition of fairness: we will measure the throughput achieved by each node\nover some time period, T , and say that an allocation with lower standard deviation is\n\"fairer\" than one with higher standard deviation. Of course, we want the notion to work\nproperly when the number of nodes varies, so some normalization is needed. We will use\nthe following simplified fairness index:\nN\n(\ni=1 xi)2\nF =\n\n,\n(15.1)\nN\nxi\nwhere xi is the throughput achieved by node i and there are N backlogged nodes in all.\nClearly, 1/N ≤ F ≤ 1; F = 1/N implies that a single node gets all the throughput, while\nF = 1 implies perfect fairness. We will consider fairness over both the long-term (many\nthousands of \"time slots\") and over the short term (tens of slots). It will turn out that in\nthe schemes we study, some schemes will achieve high utilization but poor fairness, and\nthat as we improve fairness, the overall utilization will drop.\nThe next section discusses Time Division Multiple Access, or TDMA, a scheme that\nachieves high fairness, but whose utilization may be low when the offered load is non\nuniform between the nodes, and is not easy to implement in a fully distributed way with\nout a central coordinator when nodes join and leave dynamically. However, there are\npractical situations when TDMA works well, and such protocols are used in some cellular\nwireless networks. Then, we will discuss a variant of the Aloha protocol, the first con\ntention MAC protocol that was invented. Aloha forms the basis for many widely used\ncontention protocols, including the ones used in the IEEE 802.11 (WiFi) standard.\n3In this course, and in most, if not all, of the networking and communications world, \"kilo\" = 103, \"mega\"\n= 106 and \"giga\" = 109, when talking about network rates, speeds, or throughput. When referring to storage\nunits, however, one needs to be more careful because \"kilo\", \"mega\" and \"giga\" often (but not always) refer\nto 210 , 220, and 230, respectively.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\n- 15.3\nTime Division Multiple Access (TDMA)\nIf one had a centralized resource allocator, such as a base station in a cellular network, and\na way to ensure some sort of time synchronization between nodes, then a \"time division\"\nis not hard to develop. As the name suggests, the goal is to divide time evenly between\nthe N nodes. One way to achieve this goal is to divide time into slots starting from 0\nand incrementing by 1, and for each node to be given a unique identifier (ID) in the range\n[0,N - 1].\nA simple TDMA protocol uses the following rule:\nIf the current time slot is t, then the node with ID i transmits if, and only if, it is\nbacklogged and tmodN = i.\nIf the node whose turn it is to transmit in time slot t is not backlogged, then that time\nslot is \"wasted\".\nThis TDMA scheme has some good properties. First, it is fair: each node gets the same\nnumber of transmission attempts because the protocol provides access to the medium in\nround-robin fashion among the nodes. The protocol also incurs no packet collisions (as\nsuming it is correctly implemented!): exactly one node is allowed to transmit in any time\nslot. And if the number of nodes is static, and there is a central coordinator (e.g., a master\nnodes), this TDMA protocol is simple to implement.\nThis TDMA protocol does have some drawbacks. First and foremost, if the nodes send\ndata in bursts, alternating between periods when they are backlogged and when they are\nnot, or if the amount of data sent by each node is different, then TDMA under-utilizes the\nmedium. The degree of under-utilization depends on how skewed the traffic pattern; the\nmore the imbalance, the lower the utilization. An \"ideal\" TDMA scheme would provide\nequal access to the medium only among currently backlogged nodes, but even in a sys\ntem with a central master, knowing which nodes are currently backlogged is somewhat\nchallenging. Second, if each node sends packets that are of different sizes (as is the case in\npractice, though the model we specified above did not have this wrinkle), making TDMA\nwork correctly is more involved. It can still be made to work, but it takes more effort. An\nimportant special case is when each node sends packets of the same size, but the size is\nbigger than a single time slot. This case is not hard to handle, though it requires a little\nmore thinking, and is left as an exercise for the reader.) Third, making TDMA work in a\nfully distributed way in a system without a central master, and in cases when the number\nof nodes changes dynamically, is tricky. It can be done, but the protocol quickly becomes\nmore complex than the simple rule stated above.\nContention protocols like Aloha and CSMA don't suffer from these problems, but un\nlike TDMA, they encounter packet collisions. In general, burst data and skewed work\nloads favor contention protocols over TDMA. The intuition in these protocols is that we\nsomehow would like to allocate access to the medium fairly, but only among the back\nlogged nodes. Unfortunately, only each node knows with certainty if it is backlogged or\nnot. Our solution is to use randomization, a simple but extremely powerful idea; if each\nbacklogged node transmits data with some probability, perhaps we can arrange for the\nnodes to pick their transmission probabilities to engineer an outcome that has reasonable\nutilization (throughput) and fairness!\nThe rest of this chapter describes such randomized contention protocols, starting with\n\nSECTION 15.4. ALOHA\nthe ancestor of them all, Aloha.\n- 15.4\nAloha\nThe basic variant of the Aloha protocol that we're going to start with is simple, and as\nfollows:\nIf a node is backlogged, it sends a packet from its queue with probability p.\nFrom here, until Section 15.6, we will assume that each packet is exactly one slot in length. Such\na system is also called slotted Aloha.\nWe have not specified what p is; we will figure that out later, once we analyze the\nprotocol as a function of p. Suppose there are N backlogged nodes and each node uses\nthe same value of p. We can then calculate the utilization of the shared medium as a\nfunction of N and p by simply counting the number of slots in which exactly one node sends\na packet. By definition, a slot with 0 or greater than 1 transmissions does not correspond to\na successfully delivered packet, and therefore does not contribute toward the utilization.\nIf each node sends with probability p, then the probability that exactly one node sends in\nany given slot is Np(1 -p)N-1. The reason is that the probability that a specific node sends\nin the time slot is p, and for its transmission to be successful, all the other nodes should\nnot send. That combined probability is p(1 -p)N-1 . Now, we can pick the successfully\ntransmitting node in N ways, so the probability of exactly one node sending in a slot is\nNp(1 -p)N-1 .\nThis quantity is the utilization achieved by the protocol because it is the fraction of slots\nthat count toward useful throughput. Hence,\n.\n(15.2)\nUSlotted Aloha(p) = Np(1 -p)N -1\nFigure 15-3 shows Eq.(15.2) for N = 8 as a function of p. The maximum value of U\noccurs when p = 1/N , and is equal to (1 - 1 )N-1. As N →inf,U →1/e ≈37%.\nThis\nN\nresult is an important one: the maximum utilization of slotted Aloha for a large number of\nbacklogged nodes is roughly 1/e.\n37% might seem like a small value (after all, the majority of the slots are being wasted),\nbut notice that the protocol is extremely simple and has the virtue that it is hard to botch\nits implementation! It is fully distributed and requires no coordination or other specific\ncommunication between the nodes. That simplicity in system design is worth a lot--\noftentimes, it's a very good idea to trade simplicity off for high performance, and worry\nabout optimization only when a specific part of the system is likely to become (or already\nhas become) a bottleneck.\nThat said, the protocol as described thus far requires a way to set p. Ideally, if each node\nknew the value of N, setting p = 1/N achieves the maximum. Unfortunately, this isn't\nas simple as it sounds because N here is the number of backlogged nodes that currently\nhave data in their queues. The question then is: how can the nodes pick the best p? We\n4Here, we use the fact that limN →inf(1 - 1/N)N = 1/e. To see why this limit holds, expand the log of the\nleft hand side using a Taylor series: log(1 - x) = -x - x\n3 - .... for |x| < 1.\n- x\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nFigure 15-3: The utilization of slotted Aloha as a function of p for N = 10. The maximum occurs at p =\n1/N and the maximum utilization is U = (\nN )N -1. As N →inf\ne ≈37%. N doesn't have to\n1 - 1\n, U → 1\nbe particularly large for the 1/e approximation to be close--for instance, when N = 10, the maximum\nutilization is 0.387.\nturn to this important question next, because without such a mechanism, the protocol is\nimpractical.\n- 15.5\nStabilizing Aloha: Binary Exponential Backoff\nWe use a special term for the process of picking a good \"p\" in Aloha: stabilization. In\ngeneral, in distributed protocols and algorithms, \"stabilization\" refers to the process by\nwhich the method operates around or at a desired operating point. In our case, the desired\noperating point is around p = 1/N , where N is the number of backlogged nodes.\nStabilizing a protocol like Aloha is a difficult problem because the nodes may not be able\nto directly communicate with each other (or even if they could, the overhead involved in\ndoing so would be significant). Moreover, each node has bursty demands for the medium,\nand the set of backlogged nodes could change quite rapidly with time. What we need is a\n\"search procedure\" by which each node converges toward the best \"p\".\nFortunately, this search for the right p can be guided by feedback: whether a given\npacket transmission has been successful or not is invaluable information. In practice, this\nfeedback may be obtained either using an acknowledgment for each received packet from\nthe receiver (as in most wireless networks) or using the ability to directly detect a collision\nby listening on one's own transmission (as in wired Ethernet). In either case, the feed\nback has the same form: \"yes\" or \"no\", depending on whether the packet was received\nsuccessfully or not.\nGiven this feedback, our stabilization strategy at each node is conceptually simple:\n1. Maintain the current estimate of p, pest, initialized to some value. (We will talk about\ninitialization later.)\n\nSECTION 15.5. STABILIZING ALOHA: BINARY EXPONENTIAL BACKOFF\n2. If \"no\", then consider decreasing p.\n3. If \"yes\", then consider increasing p.\nThis simple-looking structure is at the core of a wide range of distributed network pro\ntocols that seek to operate around some desired or optimum value. The devil, of course,\nis in the details, in that the way in which the increase and decrease rules work depend on\nthe problem and dynamics at hand.\nLet's first talk about the decrease rule for our protocol. The intuition here is that because\nthere was a collision, it's likely that the node's current estimate of the best p is too high\n(equivalently, its view of the number of backlogged nodes is too small). Since the actual\nnumber of nodes could be quite a bit larger, a good strategy that quickly gets to the true\nvalue is multiplicative decrease: reduce p by a factor of 2. Akin to binary search, this method\ncan reach the true probability within a logarithmic number of steps from the current value;\nabsent any other information, it is also the most efficient way to do so.\nThus, the decrease rule is:\np ← p/2\n(15.3)\nThis multiplicative decrease scheme has a special name: binary exponential backoff. The\nreason for this name is that if a packet has been unsuccessful k times, the probability with\nwhich it is sent decays proportional to 2-k . The \"2\" is the \"binary\" part, the k in the\nexponent is the \"exponential\" part, and the \"backoff\" is what the sender is doing in the\nface of these failures.\nTo develop an increase rule upon a successful transmission, observe that two factors\nmust be considered: first, the estimate of the number of other backlogged nodes whose\nqueues might have emptied during the time it took us to send our packet successfully, and\nsecond, the potential waste of slots that might occur if the increased value of p is too small.\nIn general, if n backlogged nodes contended with a given node x, and x eventually sent\nits packet, we can expect that some fraction of the n nodes also got their packets through.\nHence, the increase in p should at least be multiplicative. pmax is a parameter picked by\nthe protocol designer, and must not exceed 1 (obviously).\nThus, one possible increase rule is:\np ← min(2p, pmax).\n(15.4)\nAnother possible rule is even simpler:\np ← pmax.\n(15.5)\nThe second rule above isn't unreasonable; in fact, under burst traffic arrivals, it is quite\npossible for a much smaller number of other nodes to continue to remain backlogged, and\nin that case resetting to a fixed maximum probability would be a good idea.\nFor now, let's assume that pmax = 1 and use (15.4) to explore the performance of the\nprotocol; one can obtain similar results with (15.5) as well.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\n-\n15.5.1\nPerformance\nLet's look at how this protocol works in simulation using WSim, a shared medium simula\ntor that you will use in the lab. Running a randomized simulation with N = 6 nodes, each\ngenerating traffic in a random fashion in such a way that in most slots many of the nodes\nare backlogged, we see the following result:\nNode 0 attempts 335 success 196 coll 139\nNode 1 attempts 1691 success 1323 coll 367\nNode 2 attempts 1678 success 1294 coll 384\nNode 3 attempts 114 success 55 coll 59\nNode 4 attempts 866 success 603 coll 263\nNode 5 attempts 1670 success 1181 coll 489\nTime 10000 attempts 6354 success 4652 util 0.47\nInter-node fairness: 0.69\nEach line starting with \"Node\" above says what the total number of transmission\nattempts from the specified node was, how many of them were successes, and how\nmany of them were collisions. The line starting with \"Time\" says what the total number\nof simulated time slots was, and the total number of packet attempts, successful packets\n(i.e., those without collisions), and the utilization. The last line lists the fairness.\nA fairness of 0.69 with six nodes is actually quite poor (in fact, even a value of 0.8 would\nbe considered poor for N = 6). Figure 15-4 shows two rows of dots for each node; the top\nrow corresponds to successful transmissions while the bottom one corresponds to colli\nsions. The bar graph in the bottom panel is each node's throughput. Observe how nodes\n3 and 0 get very low throughput compared to the other nodes, a sign of significant long-\nterm unfairness. In addition, for each node there are long periods of time when both nodes\nsend no packets, because each collision causes their transmission probability to reduce by\ntwo, and pretty soon both nodes are made to starve, unable to extricate themselves from\nthis situation. Such \"bad luck\" tends to happen often because a node that has backed off\nheavily is competing against a successful backlogged node whose p is a lot higher; hence,\nthe \"rich get richer\".\nHow can we overcome this fairness problem? One approach is to set a lower bound on\np, something that's a lot smaller than the reciprocal of the largest number of backlogged\nnodes we expect in the network. In most networks, one can assume such a quantity; for\nexample, we might set the lower bound to 1/128 or 1/1024.\nSetting such a bound greatly reduces the long-term unfairness (Figure 15-5) and the\ncorresponding simulation output is as follows:\nNode 0 attempts 1516 success 1214 coll 302\nNode 1 attempts 1237 success 964 coll 273\nNode 2 attempts 1433 success 1218 coll 215\nNode 3 attempts 1496 success 1207 coll 289\nNode 4 attempts 1616 success 1368 coll 248\nNode 5 attempts 1370 success 1115 coll 254\nTime 10000 attempts 8668 success 7086 util 0.71\nInter-node fairness: 0.99\n\nSECTION 15.5. STABILIZING ALOHA: BINARY EXPONENTIAL BACKOFF\nFigure 15-4: For each node, the top row (blue) shows the times at which the node successfully sent a packet,\nwhile the bottom row (red) shows collisions. Observe how nodes 3 and 0 are both clobbered getting almost\nno throughput compared to the other nodes. The reason is that both nodes end up with repeated collisions,\nand on each collision the probability of transmitting a packet reduces by 2, so pretty soon both nodes are\ncompletely shut out. The bottom panel is a bar graph of each node's throughput.\nThe careful reader will notice something fishy about the simulation output shown\nabove (and also in the output from the simulation where we didn't set a lower bound\non p): the reported utilization is 0.71, considerably higher than the \"theoretical maximum\"\nof (1 - 1/N )N-1 = 0.4 when N = 6. What's going on here is more apparent from Fig\nure 15-5, which shows that there are long periods of time where any given node, though\nbacklogged, does not get to transmit. Over time, every node in the experiment encounters\ntimes when it is starving, though over time the nodes all get the same share of the medium\n(fairness is 0.99). If pmax is 1 (or close to 1), then a backlogged node that has just succeeded\nin transmitting its packet will continue to send, while other nodes with smaller values of\np end up backing off. This phenomenon is also sometimes called the capture effect, man\nifested by unfairness over time-scales on the order several packets. This behavior is not\ndesirable.\nSetting pmax to a more reasonable value (less than 1) yields the following:5\nNode 0 attempts 941 success 534 coll 407\nNode 1 attempts 1153 success 637 coll 516\nNode 2 attempts 1076 success 576 coll 500\nNode 3 attempts 1471 success 862 coll 609\n5We have intentionally left the value unspecified because you will investigate how to set it in the lab.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nFigure 15-5: Node transmissions and collisions when backlogged v. slot index and each node's throughput\n(bottom row) when we set a lower bound on each backlogged node's transmission probability. Note the\n\"capture effect\" when some nodes hog the medium for extended periods of time, starving others. Over\ntime, however, every node gets the same throughput (fairness is 0.99), but the long periods of inactivity\nwhile backlogged is undesirable.\nNode 4 attempts 1348 success 780 coll 568\nNode 5 attempts 1166 success 683 coll 483\nTime 10000 attempts 7155 success 4072 util 0.41\nInter-node fairness: 0.97\nFigure 15-6 shows the corresponding plot, which has reasonable per-node fairness over\nboth long and short time-scales. The utilization is also close to the value we calculated\nanalytically of (1 - 1/N )N -1. Even though the utilization is now lower, the overall result\nis better because all backlogged nodes get equal share of the medium even over short time\nscales.\nThese experiments show the trade-off between achieving both good utilization and en\nsuring fairness. If our goal were only the former, the problem would be trivial: starve\nall but one of the backlogged nodes. Achieving a good balance between various notions\nof fairness and network utilization (throughput) is at the core of many network protocol\ndesigns.\n\nSECTION 15.6. GENERALIZING TO BIGGER PACKETS, AND \"UNSLOTTED\" ALOHA\nFigure 15-6: Node transmissions and collisions when we set both lower and upper bounds on each back\nlogged node's transmission probability. Notice that the capture effect is no longer present. The bottom\npanel is each node's throughput.\n- 15.6\nGeneralizing to Bigger Packets, and \"Unslotted\" Aloha\nSo far, we have looked at perfectly slotted Aloha, which assumes that each packet fits ex\nactly into a slot. But what happens when packets are bigger than a single slot? In fact, one\nmight even ask why we need slotting. What happens when nodes just transmit without\nregard to slot boundaries? In this section, we analyze these issues, starting with packets\nthat span multiple slot lengths. Then, by making a slot length much smaller than a single\npacket size, we can calculate the utilization of the Aloha protocol where nodes can send\nwithout concern for slot boundaries--that variant is also called unslotted Aloha.\nNote that the pure unslotted Aloha model is one where there are no slots at all, and each\nnode can send a packet any time it wants. However, this model may be approximated by\na model where a node sends a packet only at the beginning of a time slot, but each packet\nis many slots long. When we make the size of a packet large compared to the length of\na single slot, we get the unslotted case. We will abuse terminology slightly and use the\nterm unslotted Aloha to refer to the case when there are slots, but the packet size is large\ncompared to the slot time.\nSuppose each node sends a packet of size T slots. One can then work out the probability\nof a successful transmission in a network with N backlogged nodes, each attempting to\nsend its packet with probability p whenever it is not already sending a packet. The key\ninsight here is that any packet whose transmission starts in 2T - 1 slots that have any overlap\nwith the current packet can collide. Figure 15-7 illustrates this point, which we discuss in\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nFigure 15-7: Each packet is T slots long. Packet transmissions begin at a slot boundary. In this picture,\nevery packet except U and W collide with V. Given packet V, any other packet sent in any one of 2T - 1\nslots--the T slots of V as well as the T - 1 slots immediately preceding V's transmission--collide with V.\nmore detail next.\nSuppose that some node sends a packet in some slot. What is the probability that this\ntransmission has no collisions? From Figure 15-7, for this packet to not collide, no other\nnode should start its transmission in 2T - 1 slots. Because p is the probability of a back\nlogged node sending a packet in a slot, and there are N - 1 nodes, this probability is equal\nto (1 - p)(2T -1)(N-1). (There is a bit of an inaccuracy in this expression, which doesn't\nmake a significant material difference to our conclusions below, but which is worth point\ning out. This expression assumes that a node sends packet independently in each time slot\nwith probability p. Of course, in practice a node will not be able to send a packet in a time\nslot if it is sending a packet in the previous time slot, unless the packet being sent in the\nprevious slot has completed. But our assumption in writing this formula is that such \"self\ninteference\" is permissible, which can't occur in reality. But it doesn't matter much for our\nconclusion because we are interested in the utilization when N is large, which means that\np would be quite small. Moreover, this formula does represent an accurate lower bound on\nthe throughput.)\nNow, the transmitting node can be chosen in N ways, and the node has a probability p\nof sending a packet. Hence, the utilization, U, is equal to\nU = Throughput/Maximum rate\n= Np(1 - p)(2T -1)(N-1)/(1/T)\n= TNp(1 - p)(2T -1)(N-1).\n(15.6)\nFor what value of p is U maximized, and what is the maximum value? By differentiating\nU wrt p and crunching through some algebra, we find that the maximum value, for large\nT\nN, is\n.\n(2T -1)e\nNow, we can look at what happens in the pure unslotted case, when nodes send without\nregard to slot boundaries. As explained above, the utilization of this scheme is identical to\nthe case when we make the packet size T much larger than 1; i.e., if each packet is large\ncompared to a time slot, then the fact that the model assumes that packets are sent along\nslot boundaries is irrelevant as far as throughput (utilization) is concerned. The maximum\nutilization in this case when N is large is therefore equal to\n≈ 0.18. Note that this value\n2e\nis one-half of the maximum utilization of pure slotted Aloha where each packet is one\n\nSECTION 15.7. CARRIER SENSE MULTIPLE ACCESS (CSMA)\nslot long. (We're making this statement for the case when N is large, but it doesn't take N\nto become all that large for the statement to be roughly true, as we'll see in the lab.)\nThis result may be surprising at first glance, but it is intuitively quite pleasing. Slotting\nmakes it so two packets destined to collide do so fully. Because partial collisions are just\nas bad as full ones in our model of the shared medium, forcing a full collision improves\nutilization. Unslotted Aloha has \"twice the window of vulnerability\" as slotted Aloha, and\nin the limit when the number of nodes is large, achieves only one-half the utilization.\n- 15.7\nCarrier Sense Multiple Access (CSMA)\nSo far, we have assumed that no two nodes using the shared medium can hear each other.\nThis assumption is true in some networks, notably the satellite network example men\ntioned here. Over a wired Ethernet, it is decidedly not true, while over wireless networks,\nthe assumption is sometimes true and sometimes not (if there are three nodes A, B, and C,\nsuch that A and C can't usually hear each other, but B can usually hear both A and C, then\nA and C are said to be hidden terminals).\nThe ability to first listen on the medium before attempting a transmission can be used\nto reduce the number of collisions and improve utilization. The technical term given for\nthis capability is called carrier sense: a node, before it attempts a transmission, can listen\nto the medium to see if the analog voltage or signal level is higher than if the medium\nwere unused, or even attempt to detect if a packet transmission is in progress by process\ning (\"demodulating\", a concept we will see in later lectures) a set of samples. Then, if it\ndetermines that another packet transmission is in progress, it considers the medium to be\nbusy, and defers its own transmission attempt until the node considers the medium to be\nidle. The idea is for a node to send only when it believes the medium to be idle.\nOne can modify the stabilized version of Aloha described above to use CSMA. One\nadvantage of CSMA is that it no longer requires each packet to be one time slot long to\nachieve good utilization; packets can be larger than a slot duration, and can also vary in\nlength.\nNote, however, that in any practical implementation, it will takes some time for a node\nto detect that the medium is idle after the previous transmission ends, because it takes time\nto integrate the signal or sample information received and determine that the medium is\nindeed idle. This duration is called the detection time for the protocol. Moreover, multiple\nbacklogged nodes might discover an \"idle\" medium at the same time; if they both send\ndata, a collision ensues. For both these reasons, CSMA does not achieve 100% utilization,\nand needs a backoff scheme, though it usually achives higher utilization than stabilized\nslotted Aloha over a single shared medium. You will investigate this protocol in the lab.\n- 15.8\nA Note on Implementation: Contention Windows\nIn the protocols described so far, each backlogged node sends a packet with probability p,\nand the job of the protocol is to adapt p in the best possible way. With CSMA, the idea is to\nsend with this probability but only when the medium is idle. In practice, many contention\nprotocols such as the IEEE 802.3 (Ethernet) and 802.11 (WiFi) standards do something a\nlittle different: rather than each node transmitting with a probability in each time slot,\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nthey use the concept of a contention window.\nA contention window scheme works as follows. Each node maintains its own current\nvalue of the window, which we call CW. CW can vary between CWmin and CWmax;\nCWmin may be 1 and CWmax may be a number like 1024. When a node decides to trans\nmit, it does so by picking a random number r uniformly in [1, CW] and sends in time slot\nC + r, where C is the current time slot. If a collision occurs, the node doubles CW; on\na successful transmission, a node halves CW (or, as is often the case in practice, directly\nresets it to CWmin).\nYou should note that this scheme is similar to the one we studied and analyzed above.\nThe doubling of CW is analogous to halving the transmission probability, and the halving\nof CW is analogous to doubling the probability (CW has a lower bound; the transmission\nprobability has an upper bound). But there are two crucial differences:\n1. Transmissions with a contention window are done according to a uniform probabil\nity distribution and not a geometrically distributed one. In the previous case, the a\npriori probability that the first transmission occurs t slots from now is geometrically\ndistributed; it is p(1 - p)t-1, while with a contention window, it is equal to 1/CW\nfor t ∈ [1, CW] and 0 otherwise. This means that each node is guaranteed to attempt\na transmission within CW slots, while that is not the case in the previous scheme,\nwhere there is always a chance, though exponentially decreasing, that a node may\nnot transmit within any fixed number of slots.\n2. The second difference is more minor: each node can avoid generating a random\nnumber in each slot; instead, it can generate a random number once per packet trans\nmission attempt.\nIn the lab, you will implement the key parts of the contention window protocol and\nexperiment with it in conjunction with CSMA. There is one important subtlety to keep in\nmind while doing this implementation. The issue has to do with how to count the slots\nbefore a node decides to transmit. Suppose a node decides that it will transmit x slots from\nnow as long as the medium is idle after x slots; if x includes the busy slots when another\nnode transmits, then multiple nodes may end up trying to transmit in the same time slot\nafter the ending of a long packet transmission from another node, leading to excessive\ncollisions. So it is important to only count down the idle slots; i.e., x should be the number\nof idle slots before the node attempts to transmit its packet (and of course, a node should\ntry to send a packet in a slot only if it believes the medium to be idle in that slot).\n- 15.9\nSummary\nThis lecture discussed the issues involved in sharing a communication medium amongst\nmultiple nodes. We focused on contention protocols, developing ways to make them pro\nvide reasonable utilization and fairness. This is what we learned:\n1. Good MAC protocols optimize utilization (throughput) and fairness, but must be\nable to solve the problem in a distributed way. In most cases, the overhead of a\ncentral controller node knowing which nodes have packets to send is too high. These\nprotocols must also provide good utilization and fairness under dynamic load.\n\nSECTION 15.9. SUMMARY\n2. TDMA provides high throughput when all (or most of) the nodes are backlogged\nand the offered loads is evenly distributed amongst the nodes. When per-node loads\nare bursty or when different nodes send different amounts of data, TDMA is a poor\nchoice.\n3. Slotted Aloha has surprisingly high utilization for such a simple protocol, if one can\npick the transmission probability correctly. The probability that maximizes through\nput is 1/N , where N is the number of backlogged nodes, the resulting utilization\ntends toward 1/e ≈ 37%, and the fairness is close to 1 if all nodes present the same\nload. The utilization does remains high even when the nodes present different loads,\nin contrast to TDMA.\nIt is also worth calculating (and noting) how many slots are left idle and how many\nslots have more than one node transmitting at the same time in slotted Aloha with\np = 1/N . When N is large, these numbers are 1/e and 1 - 2/e ≈ 26%, respectively.\nIt is interesting that the number of idle slots is the same as the utilization: if we\nincrease p to reduce the number of idle slots, we don't increase the utilization but\nactually increase the collision rate.\n4. Stabilization is crucial to making Aloha practical. We studied a scheme that adjusts\nthe transmission probability, reducing it multiplicatively when a collision occurs and\nincreasing it (either multiplicatively or to a fixed maximum value) when a successful\ntransmission occurs. The idea is to try to converge to the optimum value.\n5. A non-zero lower bound on the transmission probability is important if we want\nto improve fairness, in particular to prevent some nodes from being starved. An\nupper bound smaller than 1 improves fairness over shorter time scales by alleviating\nthe capture effect, a situation where one or a small number of nodes capture all the\ntransmission attempts for many time slots in succession.\n6. Slotted Aloha has double the utilization of unslotted Aloha when the number of\nbacklogged nodes grows. The intuitive reason is that if two packets are destined to\ncollide, the \"window of vulnerability\" is larger in the unslotted case by a factor of\ntwo.\n7. A broadcast network that uses packets that are multiple slots in length (i.e., mim\nicking the unslotted case) can use carrier sense if the medium is a true broadcast\nmedium (or approximately so). In a true broadcast medium, all nodes can hear each\nother reliably, so they can sense the carrier before transmitting their own packets. By\n\"listening before transmitting\" and setting the transmission probability using stabi\nlization, they can reduce the number of collisions and increase utilization, but it is\nhard (if not impossible) to eliminate all collisions. Fairness still requires bounds on\nthe transmission probability as before.\n8. With a contention window, one can make the transmissions from backlogged nodes\noccur according to a uniform distribution, instead of the geometric distribution im\nposed by the \"send with probability p\" schemes. A uniform distribution in a finite\nwindow guarantees that each node will attempt a transmission within some fixed\nnumber of slots, which is not true of the geometric distribution.\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\n- Acknowledgments\nMythili Vutukuru provided several useful comments that improved the explanations pre\nsented here. Thanks also to Sari Canelake, Katrina LaCurts, and Lavanya Sharan for sug\ngesting helpful improvements, and to Kerry Xing for bug fixes.\n- Problems and Questions\n1. We studied TDMA, (stabilized) Aloha, and CSMA protocols in this chapter. In each\nstatement below, assume that the protocols are implemented correctly. Which of\nthese statements is true (more than might be).\n(a) TDMA may have collisions when the size of a packet exceeds one time slot.\n(b) There exists some offered load for which TDMA has lower throughput than\nslotted Aloha.\n(c) In stabilized Aloha, two nodes have a certain probability of colliding in a time\nslot. If they actually collide in that slot, then they will experience a lower prob\nability of colliding with each other when they each retry.\n(d) There is no workload for which stabilized Aloha achieves a utilization greater\nthat (1 - 1/N )N-1 (≈ 1/e for large N ) when run for a long period of time.\n(e) In slotted Aloha with stabilization, each node's transmission probability con\nverges to 1/N , where N is the number of backlogged nodes.\n(f) In a network in which all nodes can hear each other, CSMA will have no colli\nsions when the packet size is larger than one time slot.\n2. In the Aloha stabilization protocols we studied, when a node experiences a collision,\nit decreases its transmission probability, but sets a lower bound, pmin. When it trans\nmits successfully, it increases its transmission probability, but sets an upper bound,\npmax.\n(a) Why would we set a lower bound on pmin that is not too close to 0?\n(b) Why would we set pmax to be significantly smaller than 1?\n(c) Let N be the average number of backlogged nodes. What happens if we set\npmin >> 1/N ?\n3. Alyssa and Ben are all on a shared medium wireless network running a variant of\nslotted Aloha (all packets are the same size and each packet fits in one slot). Their\ncomputers are configured such that Alyssa is 1.5 times as likely to send a packet as\nBen. Assume that both computers are backlogged.\n(a) For Alyssa and Ben, what is their probability of transmission such that the uti\nlization of their network is maximized?\n(b) What is the maximum utilization?\n\nSECTION 15.9. SUMMARY\n4. You have two computers, A and B, sharing a wireless network in your room. The\nnetwork runs the slotted Aloha protocol with equal-sized packets. You want B to\nget twice the throughput over the wireless network as A whenever both nodes are\nbacklogged. You configure A to send packets with probability p. What should you\nset the transmission probability of B to, in order to achieve your throughput goal?\n5. Which of the following statements are always true for networks with N > 1 nodes\nusing correctly implemented versions of unslotted Aloha, slotted Aloha, Time Divi\nsion Multiple Access (TDMA) and Carrier Sense Multiple Access (CSMA)? Unless\notherwise stated, assume that the slotted and unslotted versions of Aloha are stabi\nlized and use the same stabilization method and parameters. Explain your answer\nfor each statement.\n(a) There exists some offered load pattern for which TDMA has lower throughput\nthan slotted Aloha.\n(b) Suppose nodes I, II and III use a fixed probability of p = 1/3 when transmitting\non a 3-node slotted Aloha network (i.e., N = 3). If all the nodes are backlogged\nthen over time the utilization averages out to 1/e.\n(c) When the number of nodes, N, is large in a stabilized slotted Aloha network,\nsetting pmax = pmin = 1/N will achieve the same utilization as a TDMA network\nif all the nodes are backlogged.\n(d) Using contention windows with a CSMA implementation guarantees that a\npacket will be transmitted successfully within some bounded time.\n6. Suppose that there are three nodes, A, B, and C, seeking access to a shared medium\nusing slotted Aloha, each using some fixed probability of transmission, where each\npacket takes one slot to transmit. Assume that the nodes are always backlogged, and\nthat node A has half the probability of transmission as the other two, i.e., pA = p and\npB = pC = 2p.\n(a) If pA = 0.3, compute the average utilization of the network.\n(b) What value of pA maximizes the average utilization of the network and what is\nthe corresponding maximum utilization?\n7. Ben Bitdiddle sets up a shared medium wireless network with one access point and\nN client nodes. Assume that the N client nodes are backlogged, each with packets\ndestined for the access point. The access point is also backlogged, with each of its\npackets destined for some client. The network uses slotted Aloha with each packet\nfitting exactly in one slot. Recall that each backlogged node in Aloha sends a packet\nwith some probability p. Two or more distinct nodes (whether client or access point)\nsending in the same slot causes a collision. Ben sets the transmission probability, p,\nof each client node to 1/N and sets the transmission probability of the access point\nto a value pa.\n(a) What is the utilization of the network in terms of N and pa?\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\n(b) Suppose N is large. What value of pa ensures that the aggregate throughput of\npackets received successfully by the N clients is the same as the throughput of\nthe packets received successfully by the access point?\n8. Consider the same setup as the previous problem, but only the client nodes are\nbacklogged--the access point has no packets to send. Each client node sends with proba\nbility p (don't assume it is 1/N).\nBen Bitdiddle comes up with a cool improvement to the receiver at the access point. If\nexactly one node transmits, then the receiver works as usual and is able to correctly\ndecode the packet. If exactly two nodes transmit, he uses a method to cancel the\ninterference caused by each packet on the other, and is (quite remarkably) able to\ndecode both packets correctly.\n(a) What is the probability, P2, of exactly two of the N nodes transmitting in a slot?\nNote that we want the probability of any two nodes sending in a given slot.\n(b) What is the utilization of slotted Aloha with Ben's receiver modification? Write\nyour answer in terms of N, p, and P2, where P2 is defined in the problem above.\n9. Imagine a shared medium wireless network with N nodes. Unlike a perfect broad\ncast network in which all nodes can reliably hear any other node's transmission at\ntempt, nodes in our network hear each other probabilistically. That is, between any\ntwo nodes i and j, i can hear j's transmission attempt with some probability pij ,\nwhere 0 ≤ pij ≤ 1. Assume that all packets are of the same size and that the time slot\nused in the MAC protocol is much smaller than the packet size.\n(a) Show a configuration of nodes where the throughput achieved when the nodes\nall use carrier sense is higher than if they didn't.\n(b) Show a configuration of nodes where the throughput achieved when slotted\nAloha without carrier sense is higher than with carrier sense.\n10. Token-passing is a variant of a TDMA MAC protocol. Here, the N nodes sharing\nthe medium are numbered 0,1,...N - 1. The token starts at node 0. A node can\nsend a packet if, and only if, it has the token. When node i with the token has a\npacket to send, it sends the packet and then passes the token to node (i + 1) mod N.\nIf node i with the token does not have a packet to send, it passes the token to node\n(i + 1) mod N. To pass the token, a node broadcasts a token packet on the medium\nand all other nodes hear it correctly.\nA data packet occupies the medium for time Td. A token packet occupies the medium\nfor time Tk. If s of the N nodes in the network have data to send when they get the\ntoken, what is the utilization of the medium? Note that the bandwidth used to send\ntokens is pure overhead; the throughput we want corresponds to the rate at which\ndata packets are sent.\n11. Alyssa P. Hacker is designing a MAC protocol for a network used by people who:\nlive on a large island, never sleep, never have guests, and are always on-line. Sup\npose the island's network has N nodes, and the island dwellers always keep exactly\n\nSECTION 15.9. SUMMARY\nsome four of these nodes backlogged. The nodes communicate with each other by\nbeaming their data to a satellite in the sky, which in turn broadcasts the data down. If\ntwo or more nodes transmit in the same slot, their transmissions collide (the satellite\nuplink doesn't interfere with the downlink). The nodes on the ground cannot hear\neach other, and each node's packet transmission probability is non-zero. Alyssa uses\na slotted protocol with all packets equal to one slot in length.\n(a) For the slotted Aloha protocol with a fixed per-node transmission probability,\nwhat is the maximum utilization of this network? (Note that there are N nodes\nin all, of which some four are constantly backlogged.)\n(b) Suppose the protocol is the slotted Aloha protocol, and the each island dweller\ngreedily doubles his node transmission probability on each packet collision (but\nnot exceeding 1). What do you expect the network utilization to be?\n(c) In this network, as mentioned above, four of the N nodes are constantly back\nlogged, but the set of backlogged nodes is not constant. Suppose Alyssa must\ndecide between slotted Aloha with a transmission probability of 1/5 or time\ndivision multiple access (TDMA) among the N nodes. For what N does the\nexpected utilization of this slotted Aloha protocol exceed that of TDMA?\n(d) Alyssa implements a stabilization protocol to adapt the node transmission prob\nabilities on collisions and on successful transmissions. She runs an experiment\nand finds that the measured utilization is 0.5. Ben Bitdiddle asserts that this uti\nlization is too high and that she must have erred in her measurements. Explain\nwhether or not it is possible for Alyssa's implementation of stabilization to be\nconsistent with her measured result.\n12. Tim D. Vider thinks Time Division Multiple Access (TDMA) is the best thing since\nsliced bread (\"if equal slices are good for bread, then equal slices of time must be\ngood for the MAC too\", he says). Each packet is one time slot long.\nHowever, in Tim's network with N nodes, the offered load is not uniform across the\ndifferent nodes. The rate at which node i generates new packets to transmit is ri = 2i\npackets per time slot (1 ≤ i ≤ N). That is, in each time slot, the application on node\ni produces a packet to send over the network with probability ri.\n(a) Tim runs an experiment with TDMA for a large number of time slots. At the end\nof the experiment, how many nodes (as a function of N) will have a substantial\nbacklog of packets (i.e., queues that are growing with time)?\n(b) Let N = 20. Calculate the utilization of this non-uniform workload running\nover TDMA.\n13. Recall the MAC protocol with contention windows from §15.8. Here, each node\nmaintains a contention window, W, and sends a packet t idle time slots after the cur\nrent slot, where t is an integer picked uniformly in [1,W]. Assume that each packet\nis 1 slot long.\nSuppose there are two backlogged nodes in the network with contention windows\nW1 and W2, respectively (W1 ≥ W2). Suppose that both nodes pick their random\n\nCHAPTER 15. SHARING A CHANNEL:\nMEDIA ACCESS (MAC) PROTOCOLS\nvalue of t at the same time. What is the probability that the two nodes will collide\nthe next time they each transmit?\n14. Eager B. Eaver gets a new computer with two radios. There are N other devices on\nthe shared medium network to which he connects, but each of the other devices has\nonly one radio. The MAC protocol is slotted Aloha with a packet size equal to 1 time\nslot. Each device uses a fixed transmission probability, and only one packet can be\nsent successfully in any time slot. All devices are backlogged.\nEager persuades you that because he has paid for two radios, his computer has a\nmoral right to get twice the throughput of any other device in the network. You\nbegrudgingly agree.\nEager develops two protocols:\nProtocol A: Each radio on Eager's computer runs its MAC protocol independently.\nThat is, each radio sends a packet with fixed probability p. Each other device on the\nnetwork sends a packet with probability p as well.\nProtocol B: Eager's computer runs a single MAC protocol across its two radios, send\ning packets with probability 2p, and alternating transmissions between the two ra\ndios. Each other device on the network sends a packet with probability p.\n(a) With which protocol, A or B, will Eager achieve higher throughput?\n(b) Which of the two protocols would you allow Eager to use on the network so\nthat his expected throughput is double any other device's?\n15. Carl Coder implements a simple slotted Aloha-style MAC for his room's wireless\nnetwork. His room has only two backlogged nodes, A and B. Carl picks a transmis\nsion probability of 2p for node A and p for node B. Each packet is one time slot long\nand all transmissions occur at the beginning of a time slot.\n(a) What is the utilization of Carl's network in terms of p?\n(b) What value of p maximizes the utilization of this network, and what is the max\nimum utilization?\n(c) Instead of maximizing the utilization, suppose Carl chooses p so that the\nthroughput achieved by A is three times the throughput achieved by B. What\nis the utilization of his network now?\n16. Carl Coder replaces the \"send with fixed probability\" MAC of the previous problem\nwith one that uses a contention window at each node. He configures node A to use a\nfixed contention window of W and node B to use a fixed contention window of 2W .\nBefore a transmission, each node independently picks a random integer t uniformly\nbetween 1 and its contention window value, and transmits a packet t time slots from\nnow. Each packet is one time slot long and all transmissions occur at the beginning\nof a time slot.\n(a) Which node, A or B, has a higher probability of being the next to transmit a\npacket successfully? (Use intuition, don't calculate!)\n\nSECTION 15.9. SUMMARY\n(b) What is the probability that A and B will collide the next time they each trans\nmit?\n(c) Suppose A and B each pick a contention window value at some point in time.\nWhat is the probability that A transmits before B successfully on its next trans\nmission attempt? Note that this probability is equal to the probability that the\nvalue picked by A value is strictly smaller than the value picked by B. (It may\nn\nbe useful to apply the formula\ni = n(n + 1)/2.)\ni=1\n(d) Suppose there is no collision at the next packet transmission. Calculate the prob\nability that A will transmit before B? Explain why this answer is different from\nthe answer to the previous part. You should be able to obtain the solve this\nproblem using the previous two parts.\n(e) None of the previous parts directly answer the question, \"What is the proba\nbility that A will be the first node to successfully transmit a packet before B?\"\nExplain why.\n17. Ben Bitdiddle runs the slotted Aloha protocol with stabilization. Each packet is one\ntime slot long. At the beginning of time slot T , node i has a probability of trans\nmission equal to pi, 1 ≤ i ≤ N, where N is the number of backlogged nodes. The\nincrease/decrease rules for pi are doubling/halving, with pmin ≤ pi ≤ pmax, as de\nscribed in this chapter.\nBen notes that exactly two nodes, j and k, transmit in time slot T . After thinking\nabout what happens to these two packets, derive an expression for the probability\nthat exactly one node (out of the N backlogged nodes) will transmit successfully in\ntime slot T + 1.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 16: Communication Networks: Sharing and Switches",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/950e349a9779e491000426ccb218746e_MIT6_02F12_chap16.pdf",
      "content": "MIT 6.02 Lecture Notes\nLast update: November 3, 2012\nCHAPTER 16\nCommunication Networks:\nSharing and Switches\nThus far we have studied techniques to engineer a point-to-point communication link to\nsend messages between two directly connected devices. These techniques give us a com\nmunication link between two devices that, in general, has a certain error rate and a corre\nsponding message loss rate. Message losses occur when the error correction mechanism is\nunable to correct all the errors that occur due to noise or interference from other concurrent\ntransmissions in a contention MAC protocol.\nWe now turn to the study of multi-hop communication networks--systems that connect\nthree or more devices together.1 The key idea that we will use to engineer communication\nnetworks is composition: we will build small networks by composing links together, and\nbuild larger networks by composing smaller networks together.\nThe fundamental challenges in the design of a communication network are the same\nas those that face the designer of a communication link: sharing for efficiency and relia\nbility. The big difference is that the sharing problem has different challenges because the\nsystem is now distributed, spread across a geographic span that is much larger than even\nthe biggest shared medium we can practically build. Moreover, as we will see, many more\nthings can go wrong in a network in addition to just bit errors on the point-to-point links,\nmaking communication more unreliable than a single link's unreliability.2. The next few\nchapters will discuss these two challenges and the key principles to overcome them.\nIn addition to sharing and reliability, an important and difficult problem that many\ncommunication networks (such as the Internet) face is scalability: how to engineer a very\nlarge, global system. We won't say very much about scalability in this book, leaving this\nimportant topic for more advanced courses.\nThis chapter focuses on the sharing problem and discusses the following concepts:\n1. Switches and how they enable multiplexing of different communications on individ\nual links and over the network. Two forms of switching: circuit switching and packet\n1By device, we mean things like computer, phones, embedded sensors, and the like--pretty much anything\nwith some computation and communication capability that can be part of a network.\n2As one wag put it: \"Networking, just one letter away from not working.\"\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\n\nFigure 16-1: A communication network with a link between every pair of devices has a quadratic number\nof links. Such topologies are generally too expensive, and are especially untenable when the devices are\nfar from each other.\nswitching.\n2. Understanding the role of queues to absorb bursts of traffic in packet-switched net-\nworks.\n3. Understanding the factors that contribute to delays in networks: three largely fixed\ndelays (propagation, processing, and transmission delays), and one significant vari-\nable source of delays (queueing delays).\n4. Little's law, relating the average delay to the average rate of arrivals and the average\nqueue size.\n■\n16.1\nSharing with Switches\nThe collection of techniques used to design a communication link, including modulation\nand error-correcting channel coding, is usually implemented in a module called the phys-\nical layer (or \"PHY\" for short). The sending PHY takes a stream of bits and arranges to\nsend it across the link to the receiver; the receiving PHY provides its best estimate of the\nstream of bits sent from the other end. On the face of it, once we know how to develop\na communication link, connecting a collection of N devices together is ostensibly quite\nstraightforward: one could simply connect each pair of devices with a wire and use the\nphysical layer running over the wire to communicate between the two devices. This pic-\nture for a small 5-node network is shown in Figure 16-1.\nThis simple strawman using dedicated pairwise links has two severe problems. First,\nit is extremely expensive. The reason is that the number of distinct communication links\nthat one needs to build scales quadratically with N--there are\nN\n\nN(N\n=\n-1)\nbi-directional\nlinks in this design (a bi-directional link is one that can transmit data in both directions,\n\nSECTION 16.1.\nSHARING WITH SWITCHES\nEnd point\nLink\nSwitch\nFigure 16-2: A simple network topology showing communicating end points, links, and switches.\nas opposed to a uni-directional link). The cost of operating such a network would be pro-\nhibitively expensive, and each additional node added to the network would incur a cost\nproportional to the size of the network! Second, some of these links would have to span\nan enormous distance; imagine how the devices in Cambridge, MA, would be connected\nto those in Cambridge, UK, or (to go further) to those in India or China. Such \"long-haul\"\nlinks are difficult to engineer, so one can't assume that they will be available in abundance.\nClearly we need a better design, one that can \"do for a dime what any fool can do for a\ndollar\".3 The key to a practical design of a communication network is a special computing\ndevice called a switch. A switch has multiple \"interfaces\" (often also called \"ports\") on it; a\nlink (wire or radio) can be connected to each interface. The switch allows multiple different\ncommunications between different pairs of devices to run over each individual link--that\nis, it arranges for the network's links to be shared by different communications. In addition\nto the links, the switches themselves have some resources (memory and computation) that\nwill be shared by all the communicating devices.\nFigure 16-2 shows the general idea. A switch receives bits that are encapsulated in data\nframes arriving over its links, processes them (in a way that we will make precise later),\nand forwards them (again, in a way that we will make precise later) over one or more other\nlinks. In the most common kind of network, these frames are called packets, as explained\nbelow.\nWe will use the term end points to refer to the communicating devices, and call the\nswitches and links over which they communicate the network infrastructure. The resulting\nstructure is termed the network topology, and consists of nodes (the switches and end points)\nand links. A simple network topology is shown in Figure 16-2. We will model the network\ntopology as a graph, consisting of a set of nodes and a set of links (edges) connecting vari-\nous nodes together, to solve various problems.\nFigure 16-3 show a few switches of relatively current vintage (ca. 2006).\n■\n16.1.1\nThree Problems That Switches Solve\nThe fundamental functions performed by switches are to multiplex and demultiplex data\nframes belonging to different device-to-device information transfer sessions, and to deter-\nmine the link(s) along which to forward any given data frame. This task is essential be-\n3That's what an engineer does, according to an old saying.\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\np\nAlcatel 7670 RSP\nJuniper TX8/T640\nTX8\nAvici TSR\nCisco GSR 12416\n6ft x 2ft x 1.5ft\n4.2 kW power\n160 Gb/s cap.\nLucent 5ESS\ntelephone\nswitch\n802.11\naccess point\np\naccess\nFigure 16-3: A few modern switches.\ncause a given physical link will usually be shared by several concurrent sessions between\ndifferent devices. We break these functions into three problems:\n1. Forwarding: When a data frame arrives at a switch, the switch needs to process it,\ndetermine the correct outgoing link, and decide when to send the frame on that link.\n2. Routing: Each switch somehow needs to determine the topology of the network,\nso that it can correctly construct the data structures required for proper forwarding.\nThe process by which the switches in a network collaboratively compute the network\ntopology, adapting to various kinds of failures, is called routing. It does not happen\non each data frame, but occurs in the \"background\". The next two chapters will\ndiscuss forwarding and routing in more detail.\n3. Resource allocation: Switches allocate their resources--access to the link and local\nmemory--to the different communications that are in progress.\nOver time, two radically different methods have been developed for solving these\nproblems. These techniques differ in the way the switches forward data and allocate re-\nsources (there are also some differences in routing, but they are less significant). The first\nmethod, used by networks like the telephone network, is called circuit switching. The sec-\nond method, used by networks like the Internet, is called packet switching.\nThere are two crucial differences between the two methods, one philosophical and the\nother mechanistic. The mechanistic difference is the easier one to understand, so we'll talk\nabout it first. In a circuit-switched network, the frames do not (need to) carry any special\ninformation that tells the switches how to forward information, while in packet-switched\nIndividual images (c) source unknown. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nSECTION 16.2.\nCIRCUIT SWITCHING\nDATA\n(1)\n(2)\n(3)\nCaller\nCallee\nEstablish\nCommunicate\nTear down\nFigure 16-4: Circuit switching requires setup and teardown phases.\nnetworks, they do. The philosophical difference is more substantive: a circuit-switched\nnetwork provides the abstraction of a dedicated link of some bit rate to the communicating\nentities, whereas a packet switched network does not.4 Of course, this dedicated link tra-\nverses multiple physical links and at least one switch, so the end points and switches must\ndo some additional work to provide the illusion of a dedicated link. A packet-switched\nnetwork, in contrast, provides no such illusion; once again, the end points and switches\nmust do some work to provide reliable and efficient communication service to the appli-\ncations running on the end points.\n■\n16.2\nCircuit Switching\nThe transmission of information in circuit-switched networks usually occurs in three\nphases (see Figure 16-4):\n1. The setup phase, in which some state is configured at each switch along a path from\nsource to destination,\n2. The data transfer phase when the communication of interest occurs, and\n3. The teardown phase that cleans up the state in the switches after the data transfer ends.\n4One can try to layer such an abstraction atop a packet-switched network, but we're talking about the\ninherent abstraction provided by the network here.\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\nSwitch\n0 1 2 3 4 5 0 1 2 3 4 5\nframes =\nTime-slots\nFigure 16-5: Circuit switching with Time Division Multiplexing (TDM). Each color is a different conver-\nsation and there are a maximum of N = 6 concurrent communications on the link in this picture. Each\ncommunication (color) is sent in a fixed time-slot, modulo N.\nBecause the frames themselves contain no information about where they should go,\nthe setup phase needs to take care of this task, and also configure (reserve) any resources\nneeded for the communication so that the illusion of a dedicated link is provided. The\nteardown phase is needed to release any reserved resources.\n■\n16.2.1\nExample: Time-Division Multiplexing (TDM)\nA common (but not the only) way to implement circuit switching is using time-division\nmultiplexing (TDM), also known as isochronous transmission. Here, the physical capacity,\nor bit rate,5 of a link connected to a switch, C (in bits/s), is conceptually divided into N\n\"virtual links\", each virtual link being allocated C/N bits/s and associated with a data\ntransfer session. Call this quantity R, the rate of each independent data transfer session.\nNow, if we constrain each frame to be of some fixed size, s bits, then the switch can perform\ntime multiplexing by allocating the link's capacity in time-slots of length s/C units each,\nand by associating the ith time-slice to the ith transfer (modulo N), as shown in Figure 16-5.\nIt is easy to see that this approach provides each session with the required rate of R bits/s,\nbecause each session gets to send s bits over a time period of Ns/C seconds, and the ratio\nof the two is equal to C/N = R bits/s.\nEach data frame is therefore forwarded by simply using the time slot in which it arrives\nat the switch to decide which port it should be sent on. Thus, the state set up during the\nfirst phase has to associate one of these channels with the corresponding soon-to-follow\ndata transfer by allocating the ith time-slice to the ith transfer. The end points transmitting\ndata send frames only at the specific time-slots that they have been told to do so by the\nsetup phase.\nOther ways of doing circuit switching include wavelength division multiplexing (WDM),\nfrequency division multiplexing (FDM), and code division multiplexing (CDM); the latter two\n(as well as TDM) are used in some wireless networks, while WDM is used in some high-\n5This number is sometimes referred to as the \"bandwidth\" of the link. Technically, bandwidth is a quantity\nmeasured in Hertz and refers to the width of the frequency over which the transmission is being done. To\navoid confusion, we will use the term \"bit rate\" to refer to the number of bits per second that a link is currently\noperating at, but the reader should realize that the literature often uses \"bandwidth\" to refer to this term. The\nreader should also be warned that some people (curmudgeons?) become apoplectic when they hear someone\nusing \"bandwidth\" for the bit rate of a link. A more reasonable position is to realize that when the context is\nclear, there's not much harm in using \"bandwidth\". The reader should also realize that in practice most wired\nlinks usually operate at a single bit rate (or perhaps pick one from a fixed set when the link is configured),\nbut that wireless links using radio communication can operate at a range of bit rates, adaptively selecting the\nmodulation and coding being used to cope with the time-varying channel conditions caused by interference\nand movement.\n\nSECTION 16.3.\nPACKET SWITCHING\nspeed wired optical networks.\n■\n16.2.2\nPros and Cons\nCircuit switching makes sense for a network where the workload is relatively uniform,\nwith all information transfers using the same capacity, and where each transfer uses a con-\nstant bit rate (or near-constant bit rate). The most compelling example of such a workload is\ntelephony, where each digitized voice call might operate at 64 kbits/s. Switching was first\ninvented for the telephone network, well before devices were on the scene, so this design\nchoice makes a great deal of sense. The classical telephone network as well as the cellular\ntelephone network in most countries still operate in this way, though telephony over the\nInternet is becoming increasingly popular and some of the network infrastructure of the\nclassical telephone networks is moving toward packet switching.\nHowever, circuit-switching tends to waste link capacity if the workload has a variable bit\nrate, or if the frames arrive in bursts at a switch. Because a large number of computer appli-\ncations induce burst data patterns, we should consider a different link sharing strategy for\nst\ncomputer networks. Another drawback of circuit switching shows up when the (N + 1)\ncommunication arrives at a switch whose relevant link already has the maximum number\n(N) of communications going over it. This communication must be denied access (or ad-\nmission) to the system, because there is no capacity left for it. For applications that require\na certain minimum bit rate, this approach might make sense, but even in that case a \"busy\ntone\" is the result. However, there are many applications that don't have a minimum bit\nrate requirement (file delivery is a prominent example); for this reason as well, a different\nsharing strategy is worth considering.\nPacket switching doesn't have these drawbacks.\n■\n16.3\nPacket Switching\nAn attractive way to overcome the inefficiencies of circuit switching is to permit any sender\nto transmit data at any time, but yet allow the link to be shared. Packet switching is a way\nto accomplish this task, and uses a tantalizingly simple idea: add to each frame of data a\nlittle bit of information that tells the switch how to forward the frame. This information\nis usually added inside a header immediately before the payload of the frame, and the\nresulting frame is called a packet.6 In the most common form of packet switching, the\nheader of each packet contains the address of the destination, which uniquely identifies the\ndestination of data. The switches use this information to process and forward each packet.\nPackets usually also include the sender's address to help the receiver send messages back\nto the sender. A simple example of a packet header is shown in Figure 16-6. In addition to\nthe destination and source addresses, this header shows a checksum that can be used for\nerror detection at the receiver.\nThe figure also shows the packet header used by IPv6 (the Internet Protocol version 6),\nwhich is increasingly used on the Internet today. The Internet is the most prominent and\nsuccessful example of a packet-switched network.\nThe job of the switch is to use the destination address as a key and perform a lookup on\n6Sometimes, the term datagram is used instead of (or in addition to) the term \"packet\".\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\n\n!\n\nFigure 16-6: LEFT: A simple and basic example of a packet header for a packet-switched network. The\ndestination address is used by switches in the forwarding process. The hop limit field will be explained\nin the chapter on network routing; it is used to discard packets that have been forwarded in the network\nfor more than a certain number of hops, because it's likely that those packets are simply stuck in a loop.\nFollowing the header is the payload (or data) associated with the packet, which we haven't shown in this\npicture. RIGHT: For comparison, the format of the IPv6 (\"IP version 6\") packet header is shown. Four\nof the eight fields are similar to our simple header format. The additional fields are the version number,\nwhich specifies the version of IP, such as \"6\" or \"4\" (the current version that version 6 seeks to replace) and\nfields that specify, or hint at, how switches must prioritize or provide other traffic management features for\nthe packet.\na data structure called a routing table. This lookup returns an outgoing link to forward the\npacket on its way toward the intended destination. There are many ways to implement\nthe lookup opertion on a routing table, but for our purposes we can consider the routing\ntable to be a dictionary mapping each destination to one of the links on the switch.\nWhile forwarding is a relatively simple7 lookup in a data structure, the trickier question\nthat we will spend time on is determining how the entries in the routing table are obtained.\nThe plan is to use a background process called a routing protocol, which is typically imple-\nmented in a distributed manner by the switches. There are two common classes of routing\nprotocols, which we will study in later chapters. For now, it is enough to understand that\nif the routing protocol works as expected, each switch obtains a route to every destination.\nEach switch participates in the routing protocol, dynamically constructing and updating\nits routing table in response to information received from its neighbors, and providing\ninformation to each neighbor to help them construct their own routing tables.\nSwitches in packet-switched networks that implement the functions described in this\nsection are also known as routers, and we will use the terms \"switch\" and \"router\" inter-\nchangeably when talking about packet-switched networks.\n■\n16.3.1\nWhy Packet Switching Works: Statistical Multiplexing\nPacket switching does not provide the illusion of a dedicated link to any pair of commu-\nnicating end points, but it has a few things going for it:\n7At low speeds. At high speeds, forwarding is a challenging problem.\n\nSECTION 16.3.\nPACKET SWITCHING\n6.02 Fall 2011\nLecture 19, Slide #17\nFigure 16-7: Packet switching works because of statistical multiplexing. This picture shows a simulation\nof N senders, each connected at a fixed bit rate of 1 megabit/s to a switch, sharing a single outgoing link.\nThe y-axis shows the aggregate bit rate (in megabits/s) as a function of time (in milliseconds). In this\nsimulation, each sender is in either the \"on\" (sending) state or the \"off\" (idle) state; the durations of each\nstate are drawn from a Pareto distribution (which has a \"heavy tail\").\n1. It doesn't waste the capacity of any link because each switch can send any packet\navailable to it that needs to use that link.\n2. It does not require any setup or teardown phases and so can be used even for small\ntransfers without any overhead.\n3. It can provide variable data rates to different communications essentially on an \"as\nneeded\" basis.\nAt the same time, because there is no reservation of resources, packets could arrive\nfaster than can be sent over a link, and the switch must be able to handle such situations.\nSwitches deal with transient bursts of traffic that arrive faster than a link's bit rate using\nqueues. We will spend some time understanding what a queue does and how it absorbs\nbursts, but for now, let's assume that a switch has large queues and understand why packet\nswitching actually works.\nPacket switching supports end points sending data at variable rates. If a large number\nof end points conspired to send data in a synchronized way to exercise a link at the same\ntime, then one would end up having to provision a link to handle the peak synchronized\nrate to provide reasonable service to all the concurrent communications.\nFortunately, at least in a network with benign, or even greedy (but non-malicious) send-\ning nodes, it is highly unlikely that all the senders will be perfectly synchronized. Even\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\n5-minute traffic averages:\nTraffic is bursty and rates\nare variable\nFigure 16-8: Network traffic variability.\nursts of traffic, as long as they alternate bet\nwhen senders send long b\nween \"on\" and \"off\"\nstates and move between these states at random (the probability distributions for these\ncould be complicated and involve \"heavy tails\" and high variances), the aggregate traffic\nof multiple senders tends to smooth out a bit.8\nAn example is shown in Figure 16-7. The x-axis is time in milliseconds and the y-axis\nshows the bit rate of the set of senders. Each sender has a link with a fixed bit rate connect-\ning it to the switch. The picture shows how the aggregate bit rate over this short time-scale\n(4 seconds), though variable, becomes smoother as more senders share the link. This kind\nof multiplexing relies on the randomness inherent in the concurrent communications, and\nis called statistical multiplexing.\nReal-world traffic has bigger bursts than shown in this picture and the data rate usu-\nally varies by a large amount depending on time of day. Figure 16-8 shows the bit rates\nobserved at an MIT lab for different network applications. Each point on the y-axis is\na 5-minute average, so it doesn't show the variations over smaller time-scales as in the\nprevious figure. However, it shows how much variation there is with time-of-day.\nSo far, we have discussed how the aggregation of multiple sources sending data tends\nto smooth out traffic a bit, enabling the network designer to avoid provisioning a link for\nthe sum of the peak offered loads of the sources. In addition, for the packet switching idea\nto really work, one needs to appreciate the time-scales over which bursts of traffic occur in\nreal life.\n8It's worth noting that many large-scale distributed denial-of-service attacks try to take out web sites by sat-\nurating its link with a huge number of synchronized requests or garbage packets, each of which individually\ntakes up only a tiny fraction of the link.\n5-minute traffic averages:\nTraffic is bursty and rates\nare variable\n\nSECTION 16.3.\nPACKET SWITCHING\n1 second windows\n100 ms windows\n\n10 ms windows\nFigure 16-9: Traffic bursts at different time-scales, showing some smoothing. Bursts still persist, though.\nWhat better example to use than traffic generated over the duration of a 6.02 lecture on\nthe 802.11 wireless LAN in 34-101 to illustrate the point?! We captured all the traffic that\ntraversed this shared wireless network on a few days during lecture in Fall 2010. On a\ntypical day, we measured about 1 Gigabyte of traffic traversing the wireless network via\nthe access point our monitoring laptop was connected to, with numerous applications in\nthe mix. Most of the observed traffic was from Bittorrent, Web browsing, email, with the\noccasional IM sessions thrown in the mix. Domain name system (DNS) lookups, which\nare used by most Internet applications, also generate a sizable number of packets (but not\nbytes).\nFigure 16-9 shows the aggregate amount of data, in bytes, as a function of time, over\ndifferent time durations. The top picture shows the data over 10 millisecond windows--\nhere, each y-axis point is the total number of bytes observed over the wireless network\ncorresponding to a non-overlapping 10-millisecond time window. We show the data here\nfor a randomly chosen time period that lasts 17 seconds. The most noteworthy aspect of\nthis picture is the bursts that are evident: the maximum (not shown) is as high as 50,000\nbytes over this duration, but also note how successive time windows could change be-\ntween close to 20,000 bytes and 0 bytes. From time to time, larger bursts occur where the\nnetwork is essentially continuously in use (for example, starting at 14:12:38.55).\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\nQueue\nFigure 16-10: Packet switching uses queues to buffer bursts of packets that have arrived at a rate faster than\nthe bit rate of the link.\nThe middle picture shows what happens when we look at windows that are 100 mil-\nliseconds long. Clearly, bursts persist, but one can see from the picture that the variance\nhas reduced. When we move to longer windows of 1 second each, we see the same effect\npersisting, though again it's worth noting that the bursts don't actually disappear.\nThese data sets exemplify the traffic dynamics that a network designer has to plan\nfor while designing a network. One could pick a data rate that is higher than the peak\nexpected over a short time-scale, but that would be several times larger than picking a\nsmaller value and using a queue to absorb the bursts and send out packets over a link of\na smaller rate. In practice, this problem is complicated because network sources are not\n\"open loop\", but actually react to how the network responds to previously sent traffic.\nUnderstanding how this feedback system works is beyond the scope of 6.02; here, we will\nlook at how queues work.\n■\n16.3.2\nAbsorbing bursts with queues\nQueues are a crucial component in any packet-switched network. The queues in a switch\nabsorb bursts of data (see Figure 16-10): when packets arrives for an outgoing link faster\nthan the speed of that link, the queue for that link stores the arriving packets. If a packet\narrives and the queue is full, then that packet is simply dropped (if the packet is really\nimportant, then the original sender can always infer that the packet was lost because it\nnever got an acknowledgment for it from the receiver, and might decide to re-send it).\nOne might be tempted to provision large amounts of memory for packet queues be-\ncause packet losses sound like a bad thing. In fact, queues are like seasoning in a meal--\nthey need to be \"just right\" in quantity (size). Too small, and too many packets may be\nlost, but too large, and packets may be excessively delayed, causing it to take longer for the\nsenders to know that packets are only getting stuck in a queue and not being delivered.\nSo how big must queues be? The answer is not that easy: one way to think of it is to ask\nwhat we might want the maximum packet delay to be, and use that to size the queue. A\nmore nuanced answer is to analyze the dynamics of how senders react to packet losses and\nuse that to size the queue. Answering this question is beyond the scope of this course, but\nis an important issue in network design. (The short answer is that we typically want a few\ntens to ≈100 milliseconds of a queue size--that is, we want the queueing delay of a packet\nto not exceed this quantity, so the buffer size in bytes should be this quantity multiplied\n\nSECTION 16.4.\nNETWORK PERFORMANCE METRICS\nby the rate of the link concerned.)\nThus, queues can prevent packet losses, but they cause packets to get delayed. These\ndelays are therefore a \"necessary evil\". Moreover, queueing delays are variable--different\npackets experience different delays, in general. As a result, analyzing the performance of\na network is not a straightforward task. We will discuss performance measures next.\n■\n16.4\nNetwork Performance Metrics\nSuppose you are asked to evaluate whether a network is working well or not. To do your\njob, it's clear you need to define some metrics that you can measure. As a user, if you're\ntrying to deliver or download some data, a natural measure to use is the time it takes to\nfinish delivering the data. If the data has a size of S bytes, and it takes T seconds to deliver\nthe data, the throughput of the data transfer is S\nT bytes/second. The greater the throughput,\nthe happier you will be with the network.\nThe throughput of a data transfer is clearly upper-bounded by the rate of the slow-\nest link on the path between sender and receiver (assuming the network uses only one\npath to deliver data). When we discuss reliable data delivery, we will develop protocols\nthat attempt to optimize the throughput of a large data transfer. Our ability to optimize\nthroughput depends more fundamentally on two factors: the first factor is the per-packet\ndelay, sometimes called the per-packet latency and the second factor is the packet loss rate.\nThe packet loss rate is easier to understand: it is simply equal to the number of packets\ndropped by the network along the path from sender to receiver divided by the total num-\nber of packets transmitted by the sender. So, if the sender sent St packets and the receiver\ngot Sr packets, then the packet loss rate is equal to 1 -Sr = St\nSr\nSt\nS\n-\nt\n. One can equivalently\nthink of this quantity in terms of the sending and receiving rates too: for simplicity, sup-\npose there is one queue that drops packets between a sender and receiver. If the arrival\nrate of packets into the queue from the sender is A packets per second and the departure\nrate from the queue is D packets per second, then the packet loss rate is equal to 1 -D\nA.\nThe delay experienced by packets is actually the sum of four distinct sources: propaga-\ntion, transmission, processing, and queueing, as explained below:\n1. Propagation delay. This source of delay is due to the fundamental limit on the time\nit takes to send any signal over the medium. For a wire, it's the speed of light over\nthat material (for typical fiber links, it's about two-thirds the speed of light in vac-\nuum). For radio communication, it's the speed of light in vacuum (air), about 3 × 108\nmeters/second.\nThe best way to think about the propagation delay for a link is that it is equal to\nthe time for the first bit of any transmission to reach the intended destination. For a path\ncomprising multiple links, just add up the individual propagation delays to get the\npropagation delay of the path.\n2. Processing delay. Whenever a packet (or data frame) enters a switch, it needs to be\nprocessed before it is sent over the outgoing link. In a packet-switched network, this\nprocessing involves, at the very least, looking up the header of the packet in a table\nto determine the outgoing link. It may also involve modifications to the header of\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\nthe packet. The total time taken for all such operations is called the processing delay\nof the switch.\n3. Transmission delay. The transmission delay of a link is the time it takes for a packet\nof size S bits to traverse the link. If the bit rate of the link is R bits/second, then the\ntransmission delay is S/R seconds.\nWe should note that the processing delay adds to the other sources of delay in a\nnetwork with store-and-forward switches, the most common kind of network switch\ntoday. In such a switch, each data frame (packet) is stored before any processing\n(such as a lookup) is done and the packet then sent. In contrast, some extremely low\nlatency switch designs are cut-through: as a packet arrives, the destination field in the\nheader is used for a table lookup, and the packet is sent on the outgoing link without\nany storage step. In this design, the switch pipelines the transmission of a packet on\none link with the reception on another, and the processing at one switch is pipelined\nwith the reception on a link, so the end-to-end per-packet delay is smaller than the\nsum of the individual sources of delay.\nUnless mentioned explicitly, we will deal only with store-and-forward switches in\nthis course.\n4. Queueing delay. Queues are a fundamental data structure used in packet-switched\nnetworks to absorb bursts of data arriving for an outgoing link at speeds that are\n(transiently) faster than the link's bit rate. The time spent by a packet waiting in the\nqueue is its queueing delay.\nUnlike the other components mentioned above, the queueing delay is usually vari-\nable. In many networks, it might also be the dominant source of delay, accounting for\nabout 50% (or more) of the delay experienced by packets when the network is con-\ngested. In some networks, such as those with satellite links, the propagation delay\ncould be the dominant source of delay.\n■\n16.4.1\nLittle's Law\nA common method used by engineers to analyze network performance, particularly delay\nand throughput (the rate at which packets are delivered), is queueing theory. In this course,\nwe will use an important, widely applicable result from queueing theory, called Little's law\n(or Little's theorem).9 It's used widely in the performance evaluation of systems ranging\nfrom communication networks to factory floors to manufacturing systems.\nFor any stable (i.e., where the queues aren't growing without bound) queueing system,\nLittle's law relates the average arrival rate of items (e.g., packets), λ, the average delay\nexperienced by an item in the queue, D, and the average number of items in the queue, N.\nThe formula is simple and intuitive:\nN = λ × D\n(16.1)\nNote that if the queue is stable, then the departure rate is equal to the arrival rate.\n9This \"queueing formula\" was first proved in a general setting by John D.C. Little, who is now an Institute\nProfessor at MIT (he also received his PhD from MIT in 1955). In addition to the result that bears his name, he\nis a pioneer in marketing science.\n\nSECTION 16.4.\nNETWORK PERFORMANCE METRICS\nn(t) = # pkts at time t in queue\nt\nT\nA B C D\nE F G H\nG\nA B\nB\nC\nC\nD D\nC\nD E F G H H\nG\nF\nE\nD\nH\nG\nF\nE\nH\nG\nF\nH\nH\nFigure 16-11: Packet arrivals into a queue, illustrating Little's law.\nExample. Suppose packets arrive at an average rate of 1000 packets per second into\na switch, and the rate of the outgoing link is larger than this number. (If the outgoing\nrate is smaller, then the queue will grow unbounded.) It doesn't matter how inter-packet\narrivals are distributed; packets could arrive in weird bursts according to complicated\ndistributions. Now, suppose there are 50 packets in the queue on average. That is, if we\nsample the queue size at random points in time and take the average, the number is 50\npackets.\nThen, from Little's law, we can conclude that the average queueing delay experienced\nby a packet is 50/1000 seconds = 50 milliseconds.\nLittle's law is quite remarkable because it is independent of how items (packets) arrive\nor are serviced by the queue. Packets could arrive according to any distribution. They\ncan be serviced in any order, not just first-in-first-out (FIFO). They can be of any size. In\nfact, about the only practical requirement is that the queueing system be stable. It's a\nuseful result that can be used profitably in back-of-the-envelope calculations to assess the\nperformance of real systems.\nWhy does this result hold? Proving the result in its full generality is beyond the scope\nof this course, but we can show it quite easily with a few simplifying assumptions using\nan essentially pictorial argument. The argument is instructive and sheds some light into\nthe dynamics of packets in a queue.\nFigure 16-11 shows n(t), the number of packets in a queue, as a function of time t.\nEach time a packet enters the queue, n(t) increases by 1. Each time the packet leaves, n(t)\ndecreases by 1. The result is the step-wise curve like the one shown in the picture.\nFor simplicity, we will assume that the queue size is 0 at time 0 and that there is some\ntime T >> 0 at which the queue empties to 0. We will also assume that the queue services\njobs in FIFO order (note that the formula holds whether these assumptions are true or not).\nLet P be the total number of packets forwarded by the switch in time T (obviously, in\nour special case when the queue fully empties, this number is the same as the number that\nentered the system).\nNow, we need to define N, λ, and D. One can think of N as the time average of the\nnumber of packets in the queue; i.e.,\nT\nN =\n\nn(t)/T.\nt=0\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\nThe rate λ is simply equal to P/T, for the system processed P packets in time T.\nD, the average delay, can be calculated with a little trick. Imagine taking the total area\nunder the n(t) curve and assigning it to packets as shown in Figure 16-11. That is, packets\nA, B, C, ... each are assigned the different rectangles shown. The height of each rectangle\nis 1 (i.e., one packet) and the length is the time until some packet leaves the system. Each\npacket's rectangle(s) last until the packet itself leaves the system.\nNow, it should be clear that the time spent by any given packet is just the sum of the\nareas of the rectangles labeled by that packet.\nTherefore, the average delay experienced by a packet, D, is simply the area under the\nn(t) curve divided by the number of packets. That's because the total area under the curve,\nwhich is n(t), is the total delay experienced by all the packets.\nHence,\nT\nD =\n\nn(t)/P.\nt=0\nFrom the above expressions, Little's law follows: N = λ × D.\nLittle's law is useful in the analysis of networked systems because, depending on the\ncontext, one usually knows some two of the three quantities in Eq. (16.1), and is interested\nin the third. It is a statement about averages, and is remarkable in how little it assumes\nabout the way in which packets arrive and are processed.\n■\nAcknowledgments\nMany thanks to Sari Canelake, Lavanya Sharan, Patricia Saylor, Anirudh Sivaraman, and\nKerry Xing for their careful reading and helpful comments.\n■\nProblems and Questions\n1. Under what conditions would circuit switching be a better network design than\npacket switching?\n2. Which of these statements are correct?\n(a) Switches in a circuit-switched network process connection establishment and\ntear-down messages, whereas switches in a packet-switched network do not.\n(b) Under some circumstances, a circuit-switched network may prevent some\nsenders from starting new conversations.\n(c) Once a connection is correctly established, a switch in a circuit-switched net-\nwork can forward data correctly without requiring data frames to include a\ndestination address.\n(d) Unlike in packet switching, switches in circuit-switched networks do not need\nany information about the network topology to function correctly.\n3. Consider a switch that uses time division multiplexing (rather than statistical multi-\nplexing) to share a link between four concurrent connections (A, B, C, and D) whose\n\nSECTION 16.4.\nNETWORK PERFORMANCE METRICS\npackets arrive in bursts. The link's data rate is 1 packet per time slot. Assume that\nthe switch runs for a very long time.\n(a) The average packet arrival rates of the four connections (A through D), in pack-\nets per time slot, are 0.2, 0.2, 0.1, and 0.1 respectively. The average delays ob-\nserved at the switch (in time slots) are 10, 10, 5, and 5. What are the average\nqueue lengths of the four queues (A through D) at the switch?\n(b) Connection A's packet arrival rate now changes to 0.4 packets per time slot.\nAll the other connections have the same arrival rates and the switch runs un-\nchanged. What are the average queue lengths of the four queues (A through D)\nnow?\n4. Annette Werker has developed a new switch. In this switch, 10% of the packets are\nprocessed on the \"slow path\", which incurs an average delay of 1 millisecond. All\nthe other packets are processed on the \"fast path\", incurring an average delay of 0.1\nmilliseconds. Annette observes the switch over a period of time and finds that the\naverage number of packets in it is 19. What is the average rate, in packets per second,\nat which the switch processes packets?\n5. Alyssa P. Hacker has set up eight-node shared medium network running the Carrier\nSense Multiple Access (CSMA) MAC protocol. The maximum data rate of the net-\nwork is 10 Megabits/s. Including retries, each node sends traffic according to some\nunknown random process at an average rate of 1 Megabit/s per node. Alyssa mea-\nsures the network's utilization and finds that it is 0.75. No packets get dropped in\nthe network except due to collisions, and each node's average queue size is 5 packets.\nEach packet is 10000 bits long.\n(a) What fraction of packets sent by the nodes (including retries) experience a col-\nlision?\n(b) What is the average queueing delay, in milliseconds, experienced by a packet\nbefore it is sent over the medium?\n6. Over many months, you and your friends have painstakingly collected 1,000 Giga-\nbytes (aka 1 Terabyte) worth of movies on computers in your dorm (we won't ask\nwhere the movies came from). To avoid losing it, you'd like to back the data up on\nto a computer belonging to one of your friends in New York.\nYou have two options:\nA. Send the data over the Internet to the computer in New York. The data rate for\ntransmitting information across the Internet from your dorm to New York is 1\nMegabyte per second.\nB. Copy the data over to a set of disks, which you can do at 100 Megabytes per\nsecond (thank you, firewire!). Then rely on the US Postal Service to send the\ndisks by mail, which takes 7 days.\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\nWhich of these two options (A or B) is faster? And by how much?\nNote on units:\n1 kilobyte = 103 bytes\n1 megabyte = 1000 kilobytes = 106 bytes\n1 gigabyte = 1000 megabytes = 109 bytes\n1 terabyte = 1000 gigbytes = 1012 bytes\n7. Little's law can be applied to a variety of problems in other fields. Here are some\nsimple examples for you to work out.\n(a) F freshmen enter MIT every year on average. Some leave after their SB degrees\n(four years), the rest leave after their MEng (five years). No one drops out (yes,\nreally). The total number of SB and MEng students at MIT is N.\nWhat fraction of students do an MEng?\n(b) A hardware vendor manufactures $300 million worth of equipment per year.\nOn average, the company has $45 million in accounts receivable. How much\ntime elapses between invoicing and payment?\n(c) While reading a newspaper, you come across a sentence claiming that \"less than\n1% of the people in the world die every year\". Using Little's law (and some common\nsense!), explain whether you would agree or disagree with this claim. Assume\nthat the number of people in the world does not decrease during the year (this\nassumption holds).\n(d) (This problem is actually almost related to networks.) Your friendly 6.02 pro-\nfessor receives 200 non-spam emails every day on average. He estimates that of\nthese, 50 need a reply. Over a period of time, he finds that the average number\nof unanswered emails in his inbox that still need a reply is 100.\ni. On average, how much time does it take for the professor to send a reply to\nan email that needs a response?\nii. On average, 6.02 constitutes 25% of his emails that require a reply. He re-\nsponds to each 6.02 email in 60 minutes, on average. How much time on\naverage does it take him to send a reply to any non-6.02 email?\n8. You send a stream of packets of size 1000 bytes each across a network path from\nCambridge to Berkeley, at a mean rate of 1 Megabit/s. The receiver gets these packets\nwithout any loss. You find that the one-way delay is 50 ms in the absence of any\nqueueing in the network. You find that each packet in your stream experiences a\nmean delay of 75 ms.\n(a) What is the mean number of packets in the queue at the bottleneck link along\nthe path?\nYou now increase the transmission rate to 1.25 Megabits/s. You find that the receiver\ngets packets at a rate of 1 Megabit/s, so packets are being dropped because there\nisn't enough space in the queue at the bottleneck link. Assume that the queue is full\nduring your data transfer. You measure that the one-way delay for each packet in\nyour packet stream is 125 milliseconds.\n\nSECTION 16.4.\nNETWORK PERFORMANCE METRICS\n(b) What is the packet loss rate for your stream at the bottleneck link?\n(c) Calculate the number of bytes that the queue can store.\n9. Consider the network topology shown below. Assume that the processing delay at\nall the nodes is negligible.\n(a) The sender sends two 1000-byte data packets back-to-back with a negligible\ninter-packet delay. The queue has no other packets. What is the time delay\nbetween the arrival of the first bit of the second packet and the first bit of the\nfirst packet at the receiver?\n(b) The receiver acknowledges each 1000-byte data packet to the sender, and each\nacknowledgment has a size A = 100 bytes. What is the minimum possible round\ntrip time between the sender and receiver? The round trip time is defined as the\nduration between the transmission of a packet and the receipt of an acknowl-\nedgment for it.\n10. The wireless network provider at a hotel wants to make sure that anyone trying to\naccess the network is properly authorized and their credit card charged before being\nallowed. This billing system has the following property: if the average number of\nrequests currently being processed is N, then the average delay for the request is\na + bN2 seconds, where a and b are constants. What is the maximum rate (in requests\nper second) at which the billing server can serve requests?\n11. \"It may be Little, but it's the law!\" Carrie Coder has set up an email server for a large\nemail provider. The email server has two modules that process messages: the spam\nfilter and the virus scanner. As soon as a message arrives, the spam filter processes\nthe message. After this processing, if the message is spam, the filter throws out the\nmessage. The system sends all non-spam messages immediately to the virus scanner.\nIf the scanner determines that the email has a virus, it throws out the message. The\nsystem then stores all non-spam, non-virus messages in the inboxes of users.\nCarrie runs her system for a few days and makes the following observations:\n1. On average, λ = 10000 messages arrive per second.\n2. On average, the spam filter has a queue size of Ns = 5000 messages.\n3. s = 90% of all email is found to be spam; spam is discarded.\n4. On average, the virus scanner has a queue size of Nv = 300 messages.\n5. v = 20% of all non-spam email is found to have a virus; these messages are\ndiscarded.\n\n!\"\n\n!\"\n\nCHAPTER 16.\nCOMMUNICATION NETWORKS:\nSHARING AND SWITCHES\n(a) On average, in 10 seconds, how many messages are placed in the inboxes?\n(b) What is the average delay between the arrival of an email message to the email\nserver and when it is ready to be placed in the inboxes? All transfer and pro-\ncessing delays are negligible compared to the queueing delays. Make sure to\ndraw a picture of the system in explaining your answer. Derive your answer\nin terms of the symbols given, plugging in all the numbers only in the final\nstep.\n12. \"Hunting in (packet) pairs:\" A sender S and receiver R are connected using a link with\nan unknown bit rate of C bits per second and an unknown propagation delay of D\nseconds. At time t = 0, S schedules the transmission of a pair of packets over the\nlink. The bits of the two packets reach R in succession, spaced by a time determined\nby C. Each packet has the same known size, L bits.\nThe last bit of the first packet reaches R at a known time t = T1 seconds. The last bit\nof the second packet reaches R at a known time t = T2 seconds. As you will find, this\npacket pair method allows us to estimate the unknown parameters, C and D, of the\npath.\n(a) Write an expression for T1 in terms of L, C, and D.\n(b) At what time does the first bit of the second packet reach R? Express your an-\nswer in terms of T1 and one or more of the other parameters given (C, D, L).\n(c) What is T2, the time at which the last bit of the second packet reaches R? Express\nyour answer in terms of T1 and one or more of the other parameters given (C,\nD, L).\n(d) Using the previous parts, or by other means, derive expressions for the bit rate\nC and propagation delay D, in terms of the known parameters (T1,T2,L).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Notes, Chapter 17: Network Routing I: Without Any Failures",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/5f691ab218df35a3850df29a6b9a94b3_MIT6_02F12_chap17.pdf",
      "content": "MIT 6.02 DRAFT Lecture Notes\nLast update: November 3, 2012\nCHAPTER 17\nNetwork Routing - I\nWithout Any Failures\nThis chapter and the next one discuss the key technical ideas in network routing. We start\nby describing the problem, and break it down into a set of sub-problems and solve them.\nThe key ideas that you should understand by the end are:\n1. Addressing and forwarding.\n2. Distributed routing protocols: distance-vector and link-state protocols.\n3. How routing protocols handle failures and find usable paths.\n- 17.1\nThe Problem\nAs explained in earlier chapters, sharing is fundamental to all practical network designs.\nWe construct networks by interconnecting nodes (switches and end points) using point-to\npoint links and shared media. An example of a network topology is shown in Figure 17-1;\nthe picture shows the \"backbone\" of the Internet2 network, which connects a large number\nof academic institutions in the U.S., as of early 2010. The problem we're going to discuss\nat length is this: what should the switches (and end points) in a packet-switched network\ndo to ensure that a packet sent from some sender, S, in the network reaches its intended\ndestination, D?\nThe word \"ensure\" is a strong one, as it implies some sort of guarantee. Given that\npackets could get lost for all sorts of reasons (queue overflows at switches, repeated colli\nsions over shared media, and the like), we aren't going to worry about guaranteed delivery\njust yet.1 Here, we are going to consider so-called best-effort delivery: i.e., the switches will\n\"do their best\" to try to find a way to get packets from S to D, but there are no guaran\ntees. Indeed, we will see that in the face of a wide range of failures that we will encounter,\nproviding even reasonable best-effort delivery will be hard enough.\n1Subsequent chapters will address how to improve delivery reliability.\n\nCHAPTER 17. NETWORK ROUTING - I\nWITHOUT ANY FAILURES\nFigure 17-1: Topology of the Internet2 research and education network in the United States as of early 2010.\nTo solve this problem, we will model the network topology as a graph, a structure with\nnodes (vertices) connected by links (edges), as shown at the top of Figure 17-2. The nodes\ncorrespond to either switches or end points. The problem of finding paths in the network\nis challenging for the following reasons:\n1. Distributed information: Each node only knows about its local connectivity, i.e., its\nimmediate neighbors in the topology (and even determining that reliably needs a\nlittle bit of work, as we'll see). The network has to come up with a way to provide\nnetwork-wide connectivity starting from this distributed information.\n2. Efficiency: The paths found by the network should be reasonably \"good\"; they\nshouldn't be inordinately long in length, for that will increase the latency (delay) ex\nperienced by packets. For concreteness, we will assume that links have costs (these\ncosts could model link latency, for example), and that we are interested in finding a\npath between any source and destination that minimizes the total cost. We will as\nsume that all link costs are non-negative. Another aspect of efficiency that we must\npay attention to is the extra network bandwidth consumed by the network in finding\ngood paths.\n3. Failures: Links and nodes may fail and recover arbitrarily. The network should be\nable to find a path if one exists, without having packets get \"stuck\" in the network\nforever because of glitches. To cope with the churn caused by the failure and recovery\nof links and switches, as well as by new nodes and links being set up or removed,\nCourtesy of Internet2 Network NOC. Used with permission.\n\nSECTION 17.2. ADDRESSING AND FORWARDING\nany solution to this problem must be dynamic and continually adapt to changing\nconditions.\nIn this description of the problem, we have used the term \"network\" several times\nwhile referring to the entity that solves the problem. The most common solution is for the\nnetwork's switches to collectively solve the problem of finding paths that the end points'\npackets take. Although network designs where end points take a more active role in deter\nmining the paths for their packets have been proposed and are sometimes used, even those\ndesigns require the switches to do the hard work of finding a usable set of paths. Hence,\nwe will focus on how switches can solve this problem. Clearly, because the information\nrequired for solving the problem is spread across different switches, the solution involves\nthe switches cooperating with each other. Such methods are examples of distributed com\nputation.\nOur solution will be in three parts: first, we need a way to name the different nodes\nin the network. This task is called addressing. Second, given a packet with the name\nof a destination in its header we need a way for a switch to send the packet on the correct\noutgoing link. This task is called forwarding. Finally, we need a way by which the switches\ncan determine how to send a packet to any destination, should one arrive. This task is done\nin the background, and continuously, building and updating the data structures required\nfor forwarding to work properly. This background task, which will occupy most of our\ntime, is called routing.\n- 17.2\nAddressing and Forwarding\nClearly, to send packets to some end point, we need a way to uniquely identify the end\npoint. Such identifiers are examples of names, a concept commonly used in computer sys\ntems: names provide a handle that can be used to refer to various objects. In our context,\nwe want to name end points and switches. We will use the term address to refer to the\nname of a switch or an end point. For our purposes, the only requirement is that addresses\nrefer to end points and switches uniquely. In large networks, we will want to constrain\nhow addresses are assigned, and distinguish between the unique identifier of a node and\nits addresses. The distinction will allow us to use an address to refer to each distinct net\nwork link (aka \"interface\") available on a node; because a node may have multiple links\nconnected to it, the unique name for a node is distinct from the addresses of its interfaces\n(if you have a computer with multiple active network interfaces, say a wireless link and an\nEthernet, then that computer will have multiple addresses, one for each active interface).\nIn a packet-switched network, each packet sent by a sender contains the address of the\ndestination. It also usually contains the address of the sender, which allows applications\nand other protocols running at the destination to send packets back. All this information\nis in the packet's header, which also may include some other useful fields. When a switch\ngets a packet, it consults a table keyed by the destination address to determine which link\nto send the packet on in order to reach the destination. This process is a table lookup, and the\ntable in question is called the routing table.2 The selected link is called the outgoing link.\n2In practice, in high-speed networks, the routing table is distinct from the forwarding table. The former\ncontains both the route to use for any destination and other properties of the route, such as the cost. The latter\n\nCHAPTER 17. NETWORK ROUTING - I\nWITHOUT ANY FAILURES\nTable @ node B\nB\nC\nD\nE\nA\nL2\nL1\nL0\ni\nL\nDestination\nLink (next-hop)\nCost\nA\nL1\nB\n'Self'\nC\nL1\nD\nL2\nE\nL1\nROUTE\n2 S\nFigure 17-2: A simple network topology showing the routing table at node B. The route for a destination is\nmarked with an oval. The three links at node B are L0, L1, and L2; these names aren't visible at the other\nnodes but are internal to node B.\nThe combination of the destination address and outgoing link is called the route used by\nthe switch for the destination. Note that the route is different from the path between source\nand destination in the topology; the sequence of routes at individual switches produces a\nsequence of links, which in turn leads to a path (assuming that the routing and forwarding\nprocedures are working correctly). Figure 17-2 shows a routing table and routes at a node\nin a simple network.\nBecause data may be corrupted when sent over a link (uncorrected bit errors) or because\nof bugs in switch implementations, it is customary to include a checksum that covers the\npacket's header, and possibly also the data being sent.\nThese steps for forwarding work as long as there are no failures in the network. In the\nnext chapter, we will expand these steps to combat problems caused by failures, packet\nlosses, and other changes in the network that might cause packets to loop around in the\nnetwork forever. We will use a \"hop limit\" field in the packet header to detect and discard\npackets that are being repeatedly forwarded by the nodes without finding their way to the\nintended destination.\nis a table that contains only the route, and is usually placed in faster memory because it has to be consulted\non every packet.\n\nSECTION 17.3. OVERVIEW OF ROUTING\n- 17.3\nOverview of Routing\nIf you don't know where you are going, any road will take you there.\n--Lewis Carroll\nRouting is the process by which the switches construct their routing tables. At a high\nlevel, most routing protocols have three components:\n1. Determining neighbors: For each node, which directly linked nodes are currently\nboth reachable and running? We call such nodes neighbors of the node in the topology.\nA node may not be able to reach a directly linked node either because the link has\nfailed or because the node itself has failed for some reason. A link may fail to deliver\nall packets (e.g., because a backhoe cuts cables), or may exhibit a high packet loss rate\nthat prevents all or most of its packets from being delivered. For now, we will assume\nthat each node knows who its neighbors are. In the next chapter, we will discuss a\ncommon approach, called the HELLO protocol, by which each node determines who\nits current neighbors are. The basic idea if for each node to send periodic \"HELLO\"\nmessages on all its live links; any node receiving a HELLO knows that the sender of\nthe message is currently alive and a valid neighbor.\n2. Sending advertisements: Each node sends routing advertisements to its neighbors.\nThese advertisements summarize useful information about the network topology.\nEach node sends these advertisements periodically, for two reasons. First, in vec\ntor protocols, periodic advertisements ensure that over time the nodes all have all\nthe information necessary to compute correct routes. Second, in both vector and\nlink-state protocols, periodic advertisements are the fundamental mechanism used\nto overcome the effects of link and node failures (as well as packet losses).\n3. Integrating advertisements: In this step, a node processes all the advertisements it\nhas recently heard and uses that information to produce its version of the routing\ntable.\nBecause the network topology can change and because new information can become\navailable, these three steps must run continuously, discovering the current set of neigh\nbors, disseminating advertisements to neighbors, and adjusting the routing tables. This\ncontinual operation implies that the state maintained by the network switches is soft: that\nis, it refreshes periodically as updates arrive, and adapts to changes that are represented\nin these updates. This soft state means that the path used to reach some destination could\nchange at any time, potentially causing a stream of packets from a source to destination to\narrive reordered; on the positive side, however, the ability to refresh the route means that\nthe system can adapt by \"routing around\" link and node failures. We will study how the\nrouting protocol adapts to failures in the next chapter.\nA variety of routing protocols have been developed in the literature and several differ\nent ones are used in practice. Broadly speaking, protocols fall into one of two categories\ndepending on what they send in the advertisements and how they integrate advertise\nments to compute the routing table. Protocols in the first category are called vector pro\ntocols because each node, n, advertises to its neighbors a vector, with one component per\ndestination, of information that tells the neighbors about n's route to the corresponding\n\nCHAPTER 17. NETWORK ROUTING - I\nWITHOUT ANY FAILURES\ndestination. For example, in the simplest form of a vector protocol, n advertises its cost to\nreach each destination as a vector of destination:cost tuples. In the integration step, each\nrecipient of the advertisement can use the advertised cost from each neighbor, together\nwith some other information (the cost of the link from the node to the neighbor) known to\nthe recipient, to calculate its own cost to the destination. A vector protocol that advertises\nsuch costs is also called a distance-vector protocol.3\nRouting protocols in the second category are called link-state protocols. Here, each\nnode advertises information about the link to its current neighbors on all its links, and\neach recipient re-sends this information on all of its links, flooding the information about\nthe links through the network. Eventually, all nodes know about all the links and nodes\nin the topology. Then, in the integration step, each node uses an algorithm to compute the\nminimum-cost path to every destination in the network.\nWe will compare and contrast distance-vector and link-state routing protocols at the\nend of the next chapter, after we study how they work in detail. For now, keep in mind the\nfollowing key distinction: in a distance-vector protocol (in fact, in any vector protocol), the\nroute computation is itself distributed, while in a link-state protocol, the route computation\nprocess is done independently at each node and the dissemination of the topology of the\nnetwork is done using distributed flooding.\nThe next two sections discuss the essential details of distance-vector and link-state pro\ntocols. In this chapter, we will assume that there are no failures of nodes or links in the network;\nwe will assume that the only changes that can occur in the network are additions of either\nnodes or links. We will relax this assumption in the next chapter.\nWe will assume that all links in the network are bi-directional and that the costs in each\ndirection are symmetric (i.e., the cost of a link from A to B is the same as the cost of the\nlink from B to A, for any two directly connected nodes A and B).\n- 17.4\nA Simple Distance-Vector Protocol\nThe best way to understand any routing protocol is in terms of how the two distinctive\nsteps--sending advertisements and integrating advertisements--work. In this section, we\nexplain these two steps for a simple distance-vector protocol that achieves minimum-cost\nrouting.\n-\n17.4.1\nDistance-vector Protocol Advertisements\nThe advertisement in a distance-vector protocol is simple, consisting of a set of tuples as\nshown below:\n[(dest1, cost1), (dest2, cost2), (dest3, cost3), ...]\nHere, each \"dest\" is the address of a destination known to the node, and the corre\nsponding \"cost\" is the cost of the current best path known to the node. Figure 17-3 shows\nan example of a network topology with the distance-vector advertisements sent by each\nnode in steady state, after all the nodes have computed their routing tables. During the\n3The actual costs may have nothing to do with physical distance, and the costs need not satisfy the triangle\ninequality. The reason for using the term \"distance-vector\" rather than \"cost-vector\" is historic.\n\nSECTION 17.4. A SIMPLE DISTANCE-VECTOR PROTOCOL\n[(A,o), (B,6), (e,8), (0:13),(E:9)]\n[(A,9), (B,3), (e,1), (0:4),(E:o)]\nA\nE\nB\ne\n[(A,13),\n(B,7),\n(e,5),\n(0,o),\n(E,4)]\n,13),\n)\n7)\n4)]\n[(A,6), (B,o), (e,2),\n[(A:8), (B:2), (e:o), (0:5), (E:1)]\n(0,7), (E,3)]\nFigure 17-3: In steady state, each node in the the topology in this picture sends out the distance-vector\nadvertisements shown near the node,along each link at the node.\nprocess of computing the tables, each node advertises its current routing table (i.e., the des\ntination and cost fields from the table), allowing the neighbors to make changes to their\ntables and advertise updated information.\nWhat does a node do with these advertised costs? The answer lies in how the adver\ntisements from all the neighbors are integrated by a node to produce its routing table.\n-\n17.4.2\nDistance-Vector Protocol: Integration Step\nThe key idea in the integration step uses an old observation about finding shortest-cost\npaths in graphs, originally due to Bellman and Ford. Consider a node n in the network\nand some destination d. Suppose that n hears from each of its neighbors, i, what its cost,\nci, to reach d is. Then, if n were to use the link n-i as its route to reach d, the corresponding\ncost would be ci + li, where li is the cost of the n-i link. Hence, from n's perspective, it\nshould choose the neighbor (link) for which the advertised cost plus the cost of the link\nfrom n to that neighbor is smallest. More formally, the lowest-cost path to use would be\nvia the neighbor j, where\nj = arg min(ci + li).\n(17.1)\ni\nThe beautiful thing about this calculation is that it does not require the advertisements\nfrom the different neighbors to arrive synchronously. They can arrive at arbitrary times,\nand in any order; moreover, the integration step can run each time an advertisement ar\nrives. The algorithm will eventually end up computing the right cost and finding the\ncorrect route (i.e., it will converge).\nSome care must be taken while implementing this algorithm, as outlined below:\n\nCHAPTER 17. NETWORK ROUTING - I\nWITHOUT ANY FAILURES\nFigure 17-4: Periodic integration and advertisement steps at each node.\n1. A node should update its cost and route if the new cost is smaller than the current\nestimate, or if the cost of the route currently being used changes. One question you\nmight have is what the initial value of the cost should be before the node hears any\nadvertisements for a destination. clearly, it should be large, a number we'll call \"in\nfinity\". Later on, when we discuss failures, we will find that \"infinity\" for our simple\ndistance-vector protocol can't actually be all that large. Notice that \"infinity\" does\nneed to be larger than the cost of the longest minimum-cost path in the network for\nrouting between any pair of nodes to work correctly, because a path cost of \"infinity\"\nbetween some two nodes means that there is no path between those two nodes.\n2. In the advertisement step, each node should make sure to advertise the current best\n(lowest) cost along all its links.\nThe implementor must take further care in these steps to correctly handle packet losses,\nas well as link and node failures, so we will refine this step in the next chapter.\nConceptually, we can imagine the advertisement and integration processes running pe\nriodically, for example as shown in Figure 17-4. On each advertisement, a node sends the\ndestination:cost tuples from its current routing table. In the integration step that follows,\nthe node processes all the information received in the most recent advertisement from each\nneighbor to produce an updated routing table, and the subsequent advertisement step uses\nthis updated information. Eventually, assuming no packet losses or failures or additions,\nthe system reaches a steady state and the advertisements don't change.\n-\n17.4.3\nCorrectness and Performance\nThese two steps are enough to ensure correctness in the absence of failures. To see why,\nfirst consider a network where each node has information about only itself and about no\nother nodes. At this time, the only information in each node's routing table is its own, with\na cost of 0. In the advertisement step, a node sends that information to each of its neighbors\n(whose liveness is determined using the HELLO protocol). Now, the integration step runs,\nand each node's routing table has a set of new entries, one per neighbor, with the route set\nto the link along which the advertisement arrived and a path cost equal to the cost of the\nlink.\nThe next advertisement sent by each node includes the node-cost pairs for each routing\ntable entry, and the information is integrated into the routing table at a node if, and only\nif, the cost of the current path to a destination is larger than (or larger than or equal to) the\nadvertised cost plus the cost of the link on which the advertisement arrived.\nOne can show the correctness of this method by induction on the length of the path. It\nis easy to see that if the minimum-cost path has length 1 (i.e., 1 hop), then the algorithm\nfinds it correctly. Now suppose that the algorithm correctly computes the minimum-cost\n\nSECTION 17.5. A SIMPLE LINK-STATE ROUTING PROTOCOL\npath from a node s to any destination for which the minimum-cost path is ≤ C hops. Now\nconsider a destination, d, whose minimum-cost path is of length C + 1. It is clear that this\npath may be written as s,t,...,d, where t is a neighbor of s and the sub-path from t to d\nhas length C. By the inductive assumption, the sub-path from t to d is a path of length C and\ntherefore the algorithm must have correctly found it. The Bellman-Ford integration step at\ns processes all the advertisements from s's neighbors and picks the route whose link cost\nplus the advertised path cost is smallest. Because of this step, and the assumption that the\nminimum-cost path has length C + 1, the path s,t,...,d must be a minimum-cost route that\nis correctly computed by the algorithm. This completes the proof of correctness.\nHow well does this protocol work? In the absence of failures, and for small networks,\nit's quite a good protocol. It does not consume too much network bandwidth, though the\nsize of the advertisements grows linearly with the size of the network. How long does it\ntake for the protocol to converge, assuming no packet losses or other failures occur? The\nnext chapter will discuss what it means for a protocol to \"converge\"; briefly, what we're\nasking here is the time it takes for each of the nodes to have the correct routes to every other\ndestination. To answer this question, observe that after every integration step, assuming\nthat advertisements and integration steps occur at the same frequency, every node obtains\ninformation about potential minimum-cost paths that are one hop longer compared to the\nprevious integration step. This property implies that after H steps, each node will have\ncorrect minimum-cost paths to all destinations for which the minimum-cost paths are ≤ H\nhops. Hence, the convergence time in the absence of packet losses is equal to the length\n(i.e., number of hops) of the longest minimum-cost path in the network.\nIn the next chapter, when we augment the protocol to handle failures, we will calculate\nthe bandwidth consumed by the protocol and discuss some of its shortcomings. In partic\nular, we will discover that when link or node failures occur, this protocol behaves poorly.\nUnfortunately, it will turn out that many of the solutions to this problem are a two-edged\nsword: they will solve the problem, but do so in a way that does not work well as the size\nof the network grows. As a result, a distance vector protocol is limited to small networks.\nFor these networks (tens of nodes), it is a good choice because of its relative simplicity.\nIn practice, some examples of distance-vector protocols include RIP (Routing Information\nProtocol), the first distributed routing protocol ever developed for packet-switched net\nworks; EIGRP, a proprietary protocol developed by Cisco; and a slew of wireless mesh\nnetwork protocols (which are variants of the concepts described above) including some\nthat are deployed in various places around the world.\n- 17.5\nA Simple Link-State Routing Protocol\nA link-state protocol may be viewed as a counter-point to distance-vector: whereas a node\nadvertised only the best cost to each destination in the latter, in a link state protocol, a\nnode advertises information about all its neighbors and the link costs to them in the ad\nvertisement step (note again: a node does not advertise information about its routes to\nvarious destinations). Moreover, upon receiving the advertisement, a node re-broadcasts\nthe advertisement along all its links.4 This process is termed flooding.\nAs a result of this flooding process, each node has a map of the entire network; this map\n4We'll assume that the information is re-broadcast even along the link on which it came, for simplicity.\n\nCHAPTER 17. NETWORK ROUTING - I\nWITHOUT ANY FAILURES\nconsists of the nodes and currently working links (as evidenced by the HELLO protocol at\nthe nodes). Armed with the complete map of the network, each node can independently\nrun a centralized computation to find the shortest routes to each destination in the network.\nAs long as all the nodes optimize the same metric for each destination, the resulting routes\nat the different nodes will correspond to a valid path to use. In contrast, in a distance-\nvector protocol, the actual computation of the routes is distributed, with no node having\nany significant knowledge about the topology of the network. A link-state protocol dis\ntributes information about the state of each link (hence the name) and node in the topology\nto all the nodes, and as long as the nodes have a consistent view of the topology and optimize the\nsame metric, routing will work as desired.\n-\n17.5.1\nFlooding link-state advertisements\nEach node uses the HELLO protocol (mentioned earlier, and which we will discuss in the\nnext chapter in more detail) to maintain a list of current neighbors. Periodically, every\nADVERT INTERVAL, the node constructs a link-state advertisement (LSA) and sends it along\nall its links. The LSA has the following format:\n[origin addr, seq, (nbhr1, linkcost1), (nbhr2, linkcost2), (nbhr3, linkcost3), ...]\nHere, \"origin addr\" is the address of the node constructing the LSA, each \"nbhr\" refers\nto a currently active neighbor (the next chapter will describe more precisely what \"cur\nrently active\" means), and the \"linkcost\" refers to the cost of the corresponding link. An\nexample is shown in Figure 17-5.\nIn addition, the LSA has a sequence number, \"seq\", that starts at 0 when the node turns\non, and increments by 1 each time the node sends an LSA. This information is used by the\nflooding process, as follows. When a node receives an LSA that originated at another node,\ns, it first checks the sequence number of the last LSA from s. It uses the \"origin addr\" field\nof the LSA to determine who originated the LSA. If the current sequence number is greater\nthan the saved value for that originator, then the node re-broadcasts the LSA on all its links,\nand updates the saved value. Otherwise, it silently discards the LSA, because that same\nor later LSA must have been re-broadcast before by the node. There are various ways to\nimprove the performance of this flooding procedure, but we will stick to this simple (and\ncorrect) process.\nFor now, let us assume that a node sends out an LSA every time it discovers a new\nneighbor or a new link gets added to the network. The next chapter will refine this step to\nsend advertisements periodically, in order to handle failures and packet losses, as well as\nchanges to the link costs.\n-\n17.5.2\nIntegration step: Dijkstra's shortest path algorithm\nThe competent programmer is fully aware of the limited size of his own skull. He\ntherefore approaches his task with full humility, and avoids clever tricks like the plague.\n--Edsger W. Dijkstra, in The Humble Programmer, CACM 1972\nYou probably know that arrogance, in computer science, is measured in nanodijkstras.\n--Alan Kay, 1997\n\nSECTION 17.5. A SIMPLE LINK-STATE ROUTING PROTOCOL\nLSA Flooding\n[F, seq, (G, 8), (C, 2)]\nA\nB\nC\nD\nE\nF\nG\n- LSA travels each link in each direction\n- Don't bother with figuring out which link LSA came from\n- Termination: each node rebroadcasts LSA exactly once\n- All reachable nodes eventually hear every LSA\n- Time required: number of links to cross network\n6.02 Spring 2011\nLecture 20, Slide #10\nFigure 17-5: Link-state advertisement from node F in a network. The arrows show the same advertisement\nbeing re-broadcast (at different points in time) as part of the flooding process once per node, along all of\nthe links connected to the node. The link state is shown in this example for one node; in practice, there is\none of these originating from each node in the network, and re-broadcast by the other nodes.\nThe final step in the link-state routing protocol is to compute the minimum-cost paths\nfrom each node to every destination in the network. Each node independently performs\nthis computation on its version of the network topology (map). As such, this step is quite\nstraightforward because it is a centralized algorithm that doesn't require any inter-node\ncoordination (the coordination occurred during the flooding of the advertisements).\nOver the past few decades, a number of algorithms for computing various proper\nties over graphs have been developed. In particular, there are many ways to compute\nminimum-cost path between any two nodes. For instance, one might use the Bellman-\nFord method developed in Section 17.4. That algorithm is well-suited to a distributed im\nplementation because it iteratively converges to the right answer as new updates arrive,\nbut applying the algorithm on a complete graph is slower than some alternatives.\nOne of these alternatives was developed a few decades ago, a few years after the\nBellman-Ford method, by a computer scientist named Edsger Dijkstra. Most link-state\nprotocol implementations use Dijkstra's shortest-paths algorithm (and numerous exten\nsions to it) in their integration step. One crucial assumption for this algorithm, which is\nfortunately true in most networks, is that the link costs must be non-negative.\nDijkstra's algorithm uses the following property of shortest paths: if a shortest path from\nnode X to node Y goes through node Z, then the sub-path from X to Z must also be a shortest path.\nIt is easy to see why this property must hold. If the sub-path from X to Z is not a shortest\npath, then one could find a shorter path from X to Y that uses a different, and shorter,\nsub-path from X to Z instead of the original sub-path, and then continue from Z to Y . By\n\nCHAPTER 17.\nNETWORK ROUTING - I\nWITHOUT ANY FAILURES\nIntegration Step: Dijkstras Algorithm\n6.02 Fall 2011\nLecture 20, Slide #22\nA\nB\nC\nD\nE\nF\nG\n(6)\n(6)\n(8)\n(10)\n(13)\n(16)\n(Example)\nSuppose we want to find paths from A to other nodes\nC\n(12)\nB\nF\nA\n(0)\n(11)\n(10)\nE\nD\nG\nFigure 17-6: Dijkstra's shortest paths algorithm in operation, finding paths from A to all the other nodes.\nInitially, the set S of nodes to which the algorithm knows the shortest path is empty. Nodes are added to\nit in non-decreasing order of shortest path costs, with ties broken arbitrarily. In this example, nodes are\nadded in the order (A, C, B, F, E, D, G). The numbers in parentheses near a node show the current value of\nspcost of the node as the algorithm progresses, with old values crossed out.\nthe same logic, the sub-path from Z to Y must also be a shortest path in the network. As\na result, shortest paths can be concatenated together to form a shortest path between the\nnodes at the ends of the sub-paths.\nThis property suggests an iterative approach toward finding paths from a node, n, to all\nthe other destinations in the network. The algorithm maintains two disjoint sets of nodes,\nS and X = V -S, where V is the set of nodes in the network. Initially S is empty. In\neach step, we will add one more node to S, and correspondingly remove that node from\nX. The node, v, we will add satisfies the following property: it is the node in X that has\nthe shortest path from n. Thus, the algorithm adds nodes to S in non-decreasing order of\nshortest-path costs. The first node we will add to S is n itself, since the cost of the path\nfrom n to itself is 0 (and not larger than the path to any other node, since the links all have\nnon-negative weights). Figure 17-6 shows an example of the algorithm in operation.\nFortunately, there is an efficient way to determine the next node to add to S from the set\nX. As the algorithm proceeds, it maintains the current shortest-path costs, spcost(v), for\neach node v. Initially, spcost(v) = inf(some big number in practice) for all nodes, except\nfor n, whose spcost is 0. Whenever a node u is added to S, the algorithm checks each\nof u's neighbors, w, to see if the current value of spcost(w) is larger than spcost(u) +\nlinkcost(uw). If it is, then update spcost(w). Clearly, we don't need to check if the\nspcost of any other node that isn't a neighbor of u has changed because u was added to\nS--it couldn't have. Having done this step, we check the set X to find the next node to\nIntegration Step: Dijkstra s Algorithm\n\nSECTION 17.5.\nA SIMPLE LINK-STATE ROUTING PROTOCOL\nadd to S; as mentioned before, the node with the smallest spcost is selected (we break\nties arbitrarily).\nThe last part is to remember that what the algorithm needs to produce is a route for each\ndestination, which means that we need to maintain the outgoing link for each destination.\nTo compute the route, observe that what Dijkstra's algorithm produces is a shortest path\ntree rooted at the source, n, traversing all the destination nodes in the network. (A tree is a\ngraph that has no cycles and is connected, i.e., there is exactly one path between any two\nnodes, and in particular between n and every other node.) There are three kinds of nodes\nin the shortest path tree:\n1. n itself: the route from n to n is not a link, and we will call it \"Self\".\n2. A node v directly connected to n in the tree, whose parent is n. For such nodes, the\nroute is the link connecting n to v.\n3. All other nodes, w, which are not directly connected to n in the shortest path tree.\nFor such nodes, the route to w is the same as the route to w's parent, which is the\nnode one step closer to n along the (reverse) path in the tree from w to n. Clearly, this\nroute will be one of n's links, but we can just set it equal to the route to w's parent\nand rely on the second step above to determine the link.\nWe should also note that just because a node w is directly connected to n, it doesn't\nimply that the route from n is the direct link between them. If the cost of that link\nis larger than the path through another link, then we would want to use the route\n(outgoing link) corresponding to that better path.\n■\nAcknowledgments\nThanks to Sari Canelake, Sophie Diehl, Katrina LaCurts, and Anirudh Sivaraman for many\nhelpful comments, and to Karl Berggren, Fred Chen, and Eduardo Lisker for bug fixes.\n■\nProblems and Questions\n1. Consider the network shown in Figure 17-7. The number near each link is its cost.\nWe're interested in finding the shortest paths (taking costs into account) from S to\nevery other node in the network.\nWhat is the result of running Dijkstra's shortest path algorithm on this network? To\nanswer this question, near each node, list a pair of numbers: The first element of the\npair should be the order, or the iteration of the algorithm in which the node is picked.\nThe second element of each pair should be the shortest path cost from S to that node.\n2. Alice and Bob are responsible for implementing Dijkstra's algorithm at the nodes in a\nnetwork running a link-state protocol. On her nodes, Alice implements a minimum-\ncost algorithm. On his nodes, Bob implements a \"shortest number of hops\" algo-\nrithm. Give an example of a network topology with 4 or more nodes in which a\nrouting loop occurs with Alice and Bob's implementations running simultaneously\nin the same network. Assume that there are no failures.\n(Note: A routing loop occurs when a group of k ≥1 distinct nodes, n0,n1,n2,...,nk-1\nhave routes such that ni's next-hop (route) to a destination is ni+1mod k.)\n\nCHAPTER 17.\nNETWORK ROUTING - I\nWITHOUT ANY FAILURES\n\nFigure 17-7: Topology for problem 1.\n3. Consider any two graphs(networks) G and G′ that are identical except for the costs\nof the links.\n(a) The cost of link l in graph G is cl > 0, and the cost of the same link l in Graph G′\nis kcl, where k > 0 is a constant. Are the shortest paths between any two nodes\nin the two graphs identical? Justify your answer.\n(b) Now suppose that the cost of a link l in G′ is kcl + h, where k > 0 and h > 0\nare constants. Are the shortest paths between any two nodes in the two graphs\nidentical? Justify your answer.\n4. Eager B. Eaver implements distance vector routing in his network in which the links\nall have arbitrary positive costs. In addition, there are at least two paths between\nany two nodes in the network. One node, u, has an erroneous implementation of\nthe integration step: it takes the advertised costs from each neighbor and picks the\nroute corresponding to the minimum advertised cost to each destination as its route\nto that destination, without adding the link cost to the neighbor. It breaks any ties\narbitrarily. All the other nodes are implemented correctly.\nLet's use the term \"correct route\" to mean the route that corresponds to the\nminimum-cost path. Which of the following statements are true of Eager's network?\n(a) Only u may have incorrect routes to any other node.\n(b) Only u and u's neighbors may have incorrect routes to any other node.\n(c) In some topologies, all nodes may have correct routes.\n\nSECTION 17.5.\nA SIMPLE LINK-STATE ROUTING PROTOCOL\n(d) Even if no HELLO or advertisements packets are lost and no link or node fail-\nures occur, a routing loop may occur.\n5. Alyssa P. Hacker is trying to reverse engineer the trees produced by running Dijk-\nstra's shortest paths algorithm at the nodes in the network shown in Figure 19-9 on\nthe left. She doesn't know the link costs, but knows that they are all positive. All\nlink costs are symmetric (the same in both directions). She also knows that there is\nexactly one minimum-cost path between any pair of nodes in this network.\n\nFigure 17-8: Topology for problem 5.\nShe discovers that the routing tree computed by Dijkstra's algorithm at node A looks\nlike the picture in Figure 19-9 on the right. Note that the exact order in which the\nnodes get added in Dijkstra's algorithm is not obvious from this picture.\n(a) Which of A's links has the highest cost? If there could be more than one, tell us\nwhat they are.\n(b) Which of A's links has the lowest cost? If there could be more than one, tell us\nwhat they are.\nAlyssa now inspects node C, and finds that it looks like Figure 17-9. She is sure that\nthe bold (not dashed) links belong to the shortest path tree from node C, but is not\nsure of the dashed links.\n(c) List all the dashed links in Figure 17-9 that are guaranteed to be on the routing\ntree at node C.\n(d) List all the dashed links in Figure 17-9 that are guaranteed not to be (i.e., surely\nnot) on the routing tree at node C.\n6. Consider a network implementing minimum-cost routing using the distance-vector\nprotocol. A node, S, has k neighbors, numbered 1 through k, with link cost ci to\nneighbor i (all links have symmetric costs). Initially, S has no route for destination\nD. Then, S hears advertisements for D from each neighbor, with neighbor i adver-\ntising a cost of pi. The node integrates these k advertisements. What is the cost for\ndestination D in S's routing table after the integration?\n\nCHAPTER 17.\nNETWORK ROUTING - I\nWITHOUT ANY FAILURES\n\nFigure 17-9: Picture for problems 5(c) and 5(d).\nS1\nC\nA\nD\nS2\nB\nw0\nw0\nw1\nw2\nw3\nw4\nFigure 17-10: Fishnet topology for problem 6.\n7. Ben Bitdiddle is responsible for routing in FishNet, shown in Figure 17-10. He gets\nto pick the costs for the different links (the w's shown near the links). All the costs\nare non-negative.\nGoal: To ensure that the links connecting C to A and C to B, shown as darker lines,\ncarry equal traffic load. All the traffic is generated by S1 and S2, in some unknown\nproportion. The rate (offered load) at which S1 and S2 together generate traffic for\ndestinations A, B, and D are rA, rB, and rD, respectively. Each network link has a\nbandwidth higher than rA + rB + rD. There are no failures.\nProtocol: FishNet uses link-state routing; each node runs Dijkstra's algorithm to pick\nminimum-cost routes.\n(a) If rA + rD = rB, then what constraints (equations or inequalities) must the link\ncosts satisfy for the goal to be met? Explain your answer. If it's impossible to\nmeet the goal, say why.\n\nSECTION 17.5.\nA SIMPLE LINK-STATE ROUTING PROTOCOL\n(b) If rA = rB = 0 and rD > 0, what constraints must the link costs satisfy for the\ngoal to be met? Explain your answer. If it's impossible to meet the goal, say\nwhy.\n8. Consider the network shown in Figure 17-11. Each node implements Dijkstra's short-\nest paths algorithm using the link costs shown in the picture.\nD\nB\nE\nA\nC\nF\nFigure 17-11: Topology for Problem 8.\n(a) Initially, node B's routing table contains only one entry, for itself. When B runs\nDijkstra's algorithm, in what order are nodes added to the routing table? List\nall possible answers.\n(b) Now suppose the link cost for one of the links changes but all costs remain\nnon-negative. For each change in link cost listed below, state whether it is\npossible for the route at node B (i.e., the link used by B) for any destination to\nchange, and if so, name the destination(s) whose routes may change.\ni. The cost of link(A, C) increases:\nii. The cost of link(A, C) decreases:\niii. The cost of link(B, C) increases:\niv. The cost of link(B, C) decreases:\n9. Eager B. Eaver implements the distance-vector protocol studied in this chapter, but\non some of the nodes, his code sets the cost and route to each advertised destination\nD differently:\nCost to D = min(advertised cost) heard from each neighbor.\nRoute to D = link to a neighbor that advertises the minimum cost to D.\nEvery node in the network periodically advertises its vector of costs to the destina-\ntions it knows about to all its neighbors. All link costs are positive.\nAt each node, a route for destination D is valid if packets using that route will even-\ntually reach D.\nAt each node, a route for destination D is correct if packets using that route will\neventually reach D along some minimum-cost path.\n\nCHAPTER 17.\nNETWORK ROUTING - I\nWITHOUT ANY FAILURES\nAssume that there are no failures and that the routing protocol has converged to\nproduce some route to each destination at all the nodes.\nExplain whether each of these statements is True or False. Assume a network in\nwhich at least two of the nodes (and possibly all of the nodes) run Eager's modified\nversion of the code, while the remaining nodes run the method discussed in this\nchapter.\n(a) There exist networks in which some nodes will have invalid routes.\n(b) There exist networks in which some nodes will not have correct routes.\n(c) There exist networks in which all nodes will have correct routes.\n10. The hypercube is an interesting network topology. An n-dimensional hypercube has\n2n nodes, each with a unique n-bit address. Two nodes in the hypercube are con-\nnected with a link if, and only if, their addresses have a Hamming distance of 1. The\npicture below shows hypercubes for n = 3 and 4. The solid and dashed lines are the\nlinks. We are interested in link-state routing over hypercube topologies.\n\n(a) Suppose n = 4. Each node sends a link-state advertisement (LSA) periodically,\nstarting with sequence number 0. All link costs are equal to 5. Node 1000 dis-\ncovers that its link to 1001 has failed. There are no other failures. What are the\ncontents of the fourth LSA originating from node 1000?\n(b) Suppose n = 4. Three of the links at node 1000, including the link to node 1001,\nfail. No other failures or packet losses occur.\ni. How many distinct copies of any given LSA originating from node 1000\ndoes node 1001 receive?\nii. How many distinct copies of any given LSA originating from node 1001\ndoes node 1000 receive?\n(c) Suppose n = 3 and there are no failures. Each link has a distinct, positive,\nintegral cost. Node 000 runs Dijkstra's algorithm (breaking ties arbitrarily) and\nfinds that the minimum-cost path to 010 has 5 links on it. What can you say\nabout the cost of the direct link between 000 and 010?\n\nSECTION 17.5.\nA SIMPLE LINK-STATE ROUTING PROTOCOL\n11. Alyssa P. Hacker runs the link-state routing protocol in the network shown below.\nEach node runs Dijkstra's algorithm to compute minimum-cost routes to all other\ndestinations, breaking ties arbitrarily.\nB\nC\nA\nD\nE\nFigure 17-12: Alyssa's link-state routing problem.\nAnswer the following questions, explaining each answer.\n(a) In what order does C add destinations to its routing table in its execution of\nDijkstra's algorithm? Give all possible answers.\n(b) Suppose the cost of link ⟨CB⟩increases. What is the largest value it can increase\nto, before forcing a change to any of the routes in the network? (On a tie, the\nold route remains.)\n(c) Assume that no link-state advertisement (LSA) packets are lost on any link.\nWhen C generates a new LSA, how many copies of that LSA end up getting\nflooded in total over all the links of this network, using the link-state flooding\nprotocol described in 6.02?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 09 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/1524661f19778fc7025a41b6ae83663e_MIT6_02F12_tutor09_sol.pdf",
      "content": "6.02 Practice Problems: MAC protocols\nPlease read Chapter 15 before trying to solve these problems. Please also solve the problems at the end\nof Chapter 15.\nProblem 1.\nWhich of these statements are true for correctly implemented versions of stabilized unslotted Aloha,\nstabilized slotted Aloha, and Time Division Multiple Access (TDMA)? Assume that the slotted and unslotted\nversions of Aloha use the same stabilization method and parameters.\nA. When the number of nodes is large, unslotted Aloha has a lower maximum throughput than slotted\nAloha.\nTrue. By a factor of 2: 1/(2e) instead of 1/e.\nB. When the number of nodes is large and nodes transmit data according to a Poisson process, there exists\nsome offered load for which the throughput of unslotted Aloha is higher than the throughput of slotted\nAloha.\nFalse.\nC. TDMA has no packet collisions.\nTrue. TDMA eliminates collisions by explicitly allotting time slots.\nD. There exists some offered load pattern for which TDMA has lower throughput than slotted Aloha.\nTrue. For example, a skewed workload in which some nodes have much more traffic to send than\nothers.\nProblem 2.\nBinary exponential backoff is a mechanism used in some MAC protocols. Which of the following statements\nis correct?\nA. It ensures that two nodes that experience a collision in a time slot will never collide with each other when\nthey each retry that packet.\nB. It ensures that two or more nodes that experience a collision in a time slot will experience a lower\nprobability of colliding with each ther when they each retry that packet.\n1 of 9\n\nC. It can be used with slotted Aloha but not with carrier sense multiple access.\nD. Over short time scales, it improves the fairness of the throughput achieved by different nodes compared to\nnot using the mechanism.\nB is true. The others are false.\nProblem 3.\nIn the Aloha stabilization protocols we studied, when a node experiences a collision, it decreases its\ntransmission probability, but sets a lower bound, p_min. When it transmits successfully, it increases its\ntransmission probability, but sets an upper bound, p_max.\nA. Why would we set a lower bound on p_min that is not too close to 0?\nTo avoid starvation where some nodes are denied access to the medium for long periods of time.\nB. Why would we set p_max to be significantly smaller than 1?\nTo avoid the capture effect, in which a successful node hogs the medium for multiple time slots even\nwhen other nodes are backlogged.\nC. Let N be the average number of backlogged nodes. What happens if we set p_min >> 1/N?\nThe rate of collisions will be high and the utilization close to 0.\nProblem 4.\nConsider a shared medium with N backlogged nodes running the slotted Aloha MAC protocol without any\nbackoffs. An idle slot is one in which no node sends data. We will refer to the fraction of time during which\nno node uses the medium as the \"idletime\" of the protocol.\nA. If each node has a sending probability of p, what is the idletime? What are the smallest and largest\npossible values of the idletime?\n(1-p)N. 0 and 1.\nB. Assume N is large. If the Aloha sending probability, p, for each node is picked so as to maximize the\nutilization, what is the corresponding idletime?\n2 of 9\n\n1/e --> same as the utilization!\nProblem 5.\nTrue or false?\nAssume that the shared medium has N nodes and they are always backlogged.\nA. In a slotted Aloha MAC protocol using binary exponential backoff, the probability of transmission will\nalways eventually converge to some value p, and all nodes will eventually transmit with probability p.\nFalse - In a binary exponential backoff, the probability of transmission constantly changes. If the a\ntransmission succeeds, then the probability goes up. If the transmission fails, the probability goes down.\nB. Using carrier sense multiple access (CSMA), suppose that a node \"hears\" that the channel is busy at\ntime slot t. To maximize utilization, the node should not transmit in slot t and instead transmit the\npacket in the next time slot with probability 1.\nFalse - The node should not transmit at time t+1 with 100% probability. Other nodes may have also\n\"heard\" that the channel is busy and would want to send a packet at time t+1. So the node should send\nwith a probability less than 100% to reduce the possibility of a collision.\nC. There is some workload for which an unslotted Aloha with perfect CSMA will not achieve 100%\nutilization.\nTrue - Multiple nodes may think that the channel is idle and send a packet at the same time, resulting in\na collision. Thus, the utilization can never reach 100%.\nProblem 6.\nEight Cell Processor cores are connected together with a shared bus. To simplify bus arbitration, Ben\nBittdidle, young IBM engineer, suggested time-domain multiplexing (TDM) as an arbitration mechanism.\nUsing TDM each of the processors is allocated a equal-sized time slots on the common bus in a round-robin\nfashion. He's been asked to evaluate the proposed scheme on two types of applications: 1) core-to-core\nstreaming, 2) random loads.\n1) Core-to-core streaming setup: Assume each core has the same stream bandwidth requirement.\n2) Random loads setup: core 1 load = 20%, core 2 load = 30%, core 3 load = 10%, core 4 = 5%, core 5\n= 1%, core 6 = 3%, core 7 = 1%, core 8 load = 30%\nHelp Ben out by evaluating the effectiveness (bus utilization) of TDM under these two traffic scenarios.\n1) All cores have same bandwidth requirements during streaming, so, bus utilization is 100%\n3 of 9\n\n2) Each core is given 12.5% of bus bandwidth, but not all cores can use it, and some need more than that. So\nbus utilization is: 12.5% + 12.5% + 10% + 5% + 1% + 3% + 1% + 12.5% = 57.5%\nProblem 7.\nRandomized exponential backoff is a mechanism used to stabilize contention MAC protocols. Which of the\nfollowing statements is correct?\nA. It ensures that two nodes that experience a collision in a time-slot will never collide with each other\nwhen they each retry that packet.\nFalse. They might get unlucky and back-off the same amount.\nB. It ensures that two or more nodes that experience a collision n a time-slot will experience a lower\nprobability of colliding with ach other when they each retry that packet.\nTrue. The probability of a repeat collision is a smaller in each subsequent backoff.\nC. It can be used with slotted Aloha but not with CSMA.\nFalse. It can be used in any contention protocol.\nProblem 8.\nThree users X, Y and Z use a shared link to connect to the Internet. Only one of X, Y or Z can use the link at\na given time. The link has a capacity of 1 Megabit/s. There are two possible strategies for accessing the shared\nlink:\nTDMA: equal slots of 0.1 seconds.\n\"Taking turns\": adds a latency of 0.05 seconds before taking the turn. The user can then use the link for\nas long as it has data to send. A user requests the link only when it has data to send.\nIn each of the following two cases, which strategy would you pick and why?\nA. X, Y and Z send a 40 Kbytes file every 1sec.\nTDMA. Why: Each of the users generate a load of 40KB/s = 0.32 Megabits/s, which can be fully\ntransmitted given the share of 0.33 Megabits/s available per user when partitioning the channel with\nTDMA. Taking turns on the other hand does not offer enough capacity for all the files to be transmitted:\n3*0.32 + 3*0.05 = 1.11s > 1s, and would incur extra overhead.\nB. X sends 80 Kbytes files every 1sec, while Y and Z send 10 Kbytes files every 1sec.\n4 of 9\n\nTaking Turns Why: First, by using TDMA, X does not have enough capacity to transmit, 80 Kbytes/s =\n0.640 Megabits/s > 0.33 Megabigs/s. Second, with TDMA, Y and Z waste 3 out of 4 slots. On the other\nhand, when taking turns, there is enough capacity to transmit all the data:\n0.64+0.05+0.08+0.05+0.08+0.05=0.95s.\nProblem 9.\nAlyssa P. Hacker is setting up an 8-node broadcast network in her apartment building in which all nodes can\nhear each other. Nodes send packets of the same size. If packet collisions occur, both packets are corrupted\nand lost; no other packet losses occur. All nodes generate equal load on average.\nAlyssa observes a utilization of 0.5. Which of the following are consistent with the observed utilization?\nA. True/False: Four nodes are backlogged on average, and the network is using Slotted Aloha with\nstabilization, and the fairness is close to 1.\nFalse. With four backlogged nodes and fairness close to 1, the probability of of transmission is 1/4. That\nwould put the utilization at 4*(1/4)*(1-1/4)3 = 0.42 < 0.5.\nB. True/False: Four nodes are backlogged on average, and the network is using TDMA, and the fairness is\nclose to 1.\nTrue. TDMA gives each node an equal share of the network, but 4 nodes have nothing to send, giving a\nutilization of 0.5.\nNow suppose Alyssa's 8-node network runs the Carrier Sense Multiple Access (CSMA) MAC protocol. The\nmaximum data rate of the network is 10 Megabits/s. Including retries, each node sends traffic according to\nsome unknown random process at an average rate of 1 Megabit/s per node. Alyssa measures the network's\nutilization and finds that it is 0.75. No packets get dropped in the network except due to collisions.\nC. What fraction of packets sent by the nodes (including retries) experience a collision?\nThe offered load presented to the network is 8 Megabits/s in aggregate. The throughput of the protocol\nis 0.75*10 = 7.5 Megabits/s. The packet collision rate is therefore equal to\n1-7.5/8 = 1/16 = 6.25%.\nProblem 10.\nNote: this problem is useful to review how to set up and solve problems related to Aloha-like access\nprotocols, but the calculations shown in the answer are more complex than we would ask on a quiz.\nConsider a network with four nodes, where each node has a dedicated channel to each other node. The\n5 of 9\n\nprobability that any node transmits is p. Each node can only send OR receive one packet at a time. What is\nthe utilization of the network? Assume each packet takes 1 time slot. Assume each node has 3 queues -- one\nfor each other node -- and that each is backlogged.\nUtilization is the expected number of packets that get through in each slot divided by the maximum (2, i.e.\nwhen A always transmits to B and C always transmits to D).\nP[4 nodes transmit] = p4\nP[3 nodes transmit] = 4p3(1-p)\nP[2 nodes transmit] = 6p2(1-p)2\nP[1 node transmits] = 4p(1-p)3\np[0 nodes transmit] = (1-p)4\nFor each case, we compute the expected number of packets that get through, then remove conditioning using\nthe total probability theorem.\nExpected packets through if 4 nodes transmit = 0\nNo one is free to receive a packet.\nExpected packets through if 3 nodes transmit = 1*(3/27) = 1/9\nWithout loss of generality, assume that A,B and C transmit. There are 27 combinations of destinations that\nthey can transmit to (each can choose one of 3). Of these, only 3 result in the successful transmission of one\npacket (e.g. A->D,B->C,C->B) or (B->D,A<->C) or (C->D,A<->B)\nExpected packets through if 2 nodes transmit = 2*(2/9) = 4/9\nSimilar to above, assume that A and B transmit. There are 9 combinations of destinations. Of these, only 2\nresult in the successful transmission of 2 packets (A->D,B->C) or (A->C,B->D).\nExpected packets through if 1 node transmits = 1\nNo matter who the node transmits to, it will get through.\nExpected packets through if 0 nodes transmit = 0\nNo packets sent.\nThus, the expected number of packets through is\n(1/9)*4p3(1-p) + (4/9)*6p2(1-p)2 + (1)*4p(1-p)3 = 4/9*p3(1-p) + (4/3)p2(1-p)2 + 4p(1-p)3\nThe utilization is half of that.\nProblem 11.\nSuppose that there are three nodes seeking access to a shared medium using slotted Aloha, where each packet\ntakes one slot to transmit. Assume that the nodes are always backlogged, and that each has probability p_i of\nsending a packet in each slot, where i = 1, 2 and 3 indexes the node. Suppose that we assign more the sending\nprobabilities so that\np_1 = 2(p_2) and p_2 = p_3\n6 of 9\n\nA. What is the utilization of the shared medium?\nU = (p_1)(1 - p_2)(1 - p_3) + (1 - p_1)(p_2)(1 - p_3) + (1 - p_1)(1 - p_2)(p3)\nIf p = p3\nU = 2p(1-p)2 + 2(1 - 2p)p(1 - p) = 4p - 10p2 + 6p3\nB. What are the probabilities that maximize the utilization and the corresponding utilization?\nDifferentiating U and sett the result equal to zero we obtain\ndU/dp = 4 - 20p + 18p2 = 0\nhas two roots at p=0.2616 and 0.8495. However, only the root at 0.2616 is feasible since the other\nleads to a value of p_1 that is greater than 1. Thus,\np_1 = 0.5232, p_2 = 0.2616 and p_3 = 0.2616.\nThe corresponding utilization is 0.4695.\nProblem 12.\nSuppose that two nodes are seeking access to a shared medium using slotted Aloha with binary exponential\nbackoff subject to maximum and minimum limits of the probability pmax = 0.8 and pmin = 0.1. Suppose that\nboth nodes are backlogged, and at slot n, the probabilities the two nodes transmit packets are p_1 = 0.5 and\np_2 = 0.3.\nA. What are the possible values of p_1 at slot n+1? What are the probabilities assocated with each\npossible value?\np_1 increases to 0.8 if node 1 transmits a packet and node 2 does not transmit a packet. Probability of\nthis event:\n(p_1)(1 - p_2) = 0.5*0.7 = 0.35\np_1 decreases to 0.25 if node 1 transmits a packeet and node 2 also transmits a packet. Probability of\nthis event:\n(p_1)(p_2) = 0.15\nOtherwise p_1 stays the same with probability 1 - 0.35 - 0.15 = 0.5.\nB. What are the possible values of p_2 at slot n+1? What are the probabilities associated with each\npossible value?\n7 of 9\n\np_2 increases to 0.6 if node 2 transmits a packet and node 1 does not transmit a packet. Probability of\nthis event:\n(p_2)(1 - p_1) = 0.3*0.5 = 0.15\np_2 decreases to 0.15 if node 2 transmits a packeet and node 1 also transmits a packet. Probability of\nthis event:\n(p_1)(p_2) = 0.15\nOtherwise p_2 stays the same with probability 1 - 0.15 - 0.15 = 0.7.\nProblem 13. Bluetooth is a wireless technology found on many mobile devices, including laptops, mobile\nphones, GPS navigation devices, headsets, and so on. It uses a MAC protocol called Time Division Duplex\n(TDD). In TDD, the shared medium network has 1 master and N slaves. You may assume that the network\nhas already been configured with one device as the master and the others as slaves. Each slave has a unique\nidentifier (ID) that serves as its address, an integer between 1 and N . Assume that no devices ever turn off\nduring the operation of the protocol. Unless otherwise mentioned, assume that no packets are lost.\nThe MAC protocol works as follows. Time is slotted and each packet is one time slot long.\nIn every odd time slot (1, 3, 5, ..., 2t-1, ...), the master sends a packet addressed to some slave for which it\nhas packets backlogged, in round-robin order (i.e., cycling through the slaves in numeric order).\nIn every even time slot (2, 4, 6, ..., 2t, ...), the slave that received a packet from the master in the\nimmediately preceding time slot gets to send a packet to the master, if it has a packet to send. If it has no\npacket to send, then that time slot is left unused, and the slot is wasted.\nA. Alyssa P. Hacker finds a problem with the TDD protocol described above, and implements the\nfollowing rule in addition:\nFrom time to time, in an odd time slot, the master sends a \"dummy\" packet addressed to a slave\neven if it has no other data packets to send to the slave (and even if it has packets for other\nslaves).\nWhy does Alyssa's rule improve the TDD protocol?\nBecause it will prevent a slave from being starved; without it, a slave that has packets to send will\nnever send data packets if the master never has packets to send to it.\nHenceforth, the term \"TDD\" will refer to the protocol described above, augmented with Alyssa's rule.\nMoreover, whenever a \"dummy\" packet is sent, that time slot will be considered a wasted slot.\nB. Alyssa's goal is to emulate a round-robin TDMA scheme amongst the N slaves. Propose a way to\nachieve this goal by specifying the ID of the slave that the master should send a data or dummy packet\nto, in time slot 2t-1 (note that 1 ≤ t ≤inf).\nSend to the slave whose ID is (t mod N)+1. This is almost exactly the same problem as in PSet #6, Task\n8 of 9\n\n1 (TDMA). The only difference is that the nodes begin with ID 1 here, not 0.\nHenceforth, assume that the TDD scheme implements round-robin TDMA amongst the slaves. Suppose the\nmaster always has data packets to send only to an arbitrary (but fixed) subset of the N slaves. In addition, a\n(possibly different) subset of the slaves always has packets to send to the master. Each subset is of size r, a\nfixed value. Answer the questions below (you may find it helpful to think about different subsets of slaves).\nC. What is the maximum possible utilization of such a configuration?\nThe protocol is TDMA, so the master sends useful data packets in r of the 2N slots in an epoch, and\nreceives useful packets in the r of the 2N slots. So the utilization is r/N. If one N seeks to maximize that\nover r, it's clear that the maximum happens when r = N, giving us a maximum of 1.\nD. What is the minimum possible utilization (for a given value of r) of such a configuration? Assume that r\n> N/2. Note that if the master does not have a data packet to send to a slave in a round, it sends a\n\"dummy\" packet to that slave instead. A dummy packet does not count toward the utilization of the\nmedium.\nr/N. When minimizing over all r, the smallest value becomes 1/2 + 1/N when N is even and 1/2 + 1/2N\nwhen N is odd.\n9 of 9\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/2299b396f5d5a0d5a18f2ca24ee1f43f_MIT6_02F12_tutor01.pdf",
      "content": "6.02 Practice Problems: Information, Entropy, & Source Coding\nProblem 1. Huffman coding is used to compactly encode the species of fish tagged by a game warden. If\n50% of the fish are bass and the rest are evenly divided among 15 other species, how many bits would be\nused to encode the species when a bass is tagged?\nProblem 2. Several people at a party are trying to guess a 3-bit binary number. Alice is told that the number is\nodd; Bob is told that it is not a multiple of 3 (i.e., not 0, 3, or 6); Charlie is told that the number contains\nexactly two 1's; and Deb is given all three of these clues. How much information (in bits) did each player get\nabout the number?\nProblem 3. X is an unknown 8-bit binary number. You are given another 8-bit binary number, Y, and told that\nY differs from X in exactly one bit position. How many bits of information about X have you been given?\nProblem 4. In Blackjack the dealer starts by dealing 2 cards each to himself and his opponent: one face\ndown, one face up. After you look at your face-down card, you know a total of three cards. Assuming this\nwas the first hand played from a new deck, how many bits of information do you now have about the dealer's\nface down card?\nProblem 5. The following table shows the current undergraduate and MEng enrollments for the School of\nEngineering (SOE).\nCourse (Department) # of students Prob.\nI (Civil & Env.)\n.07\nII (Mech. Eng.)\n.23\nIII (Mat. Sci.)\n.07\nVI (EECS)\n.38\nX (Chem. Eng.)\n.13\nXVI (Aero & Astro)\n.12\nTotal\n1.0\nA. When you learn a randomly chosen SOE student's department you get some number of bits of\ninformation. For which student department do you get the least amount of information?\n1 of 9\n\nB. Design a variable length Huffman code that minimizes the average number of bits in messages encoding\nthe departments of randomly chosen groups of students. Show your Huffman tree and give the code for\neach course.\nC. If your code is used to send messages containing only the encodings of the departments for each\nstudent in groups of 100 randomly chosen students, what's the average length of such messages? Write\nan expression if you wish.\nProblem 6. You're playing an on-line card game that uses a deck of 100 cards containing 3 Aces, 7 Kings, 25\nQueens, 31 Jacks and 34 Tens. In each round of the game the cards are shuffled, you make a bet about what\ntype of card will be drawn, then a single card is drawn and the winners are paid off. The drawn card is\nreinserted into the deck before the next round begins.\nA. How much information do you receive when told that a Queen has been drawn during the current\nround?\nB. Give a numeric expression for the average information content received when learning about the\noutcome of a round (aka the entropy).\nC. Construct a variable-length Huffman encoding that minimizes the length of messages that report the\noutcome of a sequence of rounds. The outcome of a single round is encoded as A (ace), K (king), Q\n(queen), J (jack) or X (ten). Specify your encoding for each of A, K, Q, J and X.\nD. Using your code from part (C) what is the expected length of a message reporting the outcome of 1000\nrounds (i.e., a message that contains 1000 symbols)?\nE. The Nevada Gaming Commission regularly receives messages in which the outcome for each round is\nencoded using the symbols A, K, Q, J and X. They discover that a large number of messages describing\nthe outcome of 1000 rounds (i.e. messages with 1000 symbols) can be compressed by the LZW\nalgorithm into files each containing 43 bytes in total. They immediately issue an indictment for running\na crooked game. Briefly explain their reasoning.\nProblem 7.\nConsider messages made up entirely of vowels (A, E, I, O, U). Here's a table of probabilities for each of the\nvowels:\n2 of 9\n\nl\np(l) log2(1/p(l)) p(l)*log2(1/p(l))\nA\n0.22\n2.18\n0.48\nE\n0.34\n1.55\n0.53\nI\n0.17\n2.57\n0.43\nO\n0.19\n2.40\n0.46\nU\n0.08\n3.64\n0.29\nTotals 1.00\n12.34\n2.19\nA. Give an expression for the number of bits of information you receive when learning that a particular\nvowel is either I or U.\nB. Using Huffman's algorithm, construct a variable-length code assuming that each vowel is encoded\nindividually. Please draw a diagram of the Huffman tree and give the encoding for each of the vowels.\nC. Using your code from above, give an expression for the expected length in bits of an encoded message\ntransmitting 100 vowels.\nD. Ben Bitdiddle spends all night working on a more complicated encoding algorithm and sends you email\nclaiming that using his code the expected length in bits of an encoded message transmitting 100 vowels\nis 197 bits. Would you pay good money for his implementation?\nProblem 8. Describe the contents of the string table created when encoding a very long string of all a's using\nthe simple version of the LZW encoder shown below. In this example, if the decoder has received E encoded\nsymbols (i.e., string table indices) from the encoder, how many a's has it been able to decode?\ninitialize TABLE[0 to 255] = code for individual bytes\nSTRING = get input symbol\nwhile there are still input symbols:\nSYMBOL = get input symbol\nif STRING + SYMBOL is in TABLE:\nSTRING = STRING + SYMBOL\nelse:\noutput the code for STRING\nadd STRING + SYMBOL to TABLE\nSTRING = SYMBOL\noutput the code for STRING\nProblem 9. Consider the pseudo-code for the LZW decoder given below:\ninitialize TABLE[0 to 255] = code for individual bytes\nCODE = read next code from encoder\nSTRING = TABLE[CODE]\n3 of 9\n\noutput STRING\nwhile there are still codes to receive:\nCODE = read next code from encoder\nif TABLE[CODE] is not defined:\nENTRY = STRING + STRING[0]\nelse:\nENTRY = TABLE[CODE]\noutput ENTRY\nadd STRING+ENTRY[0] to TABLE\nSTRING = ENTRY\nSuppose that this decoder has received the following five codes from the LZW encoder (these are the first\nfive codes from a longer compression run):\n97 -- index of 'a' in the translation table\n98 -- index of 'b' in the translation table\n257 -- index of second addition to the translation table\n256 -- index of first addition to the translation table\n258 -- index of third addition to the translation table\nAfter it has finished processing the fifth code, what are the entries in TABLE and what is the cumulative\noutput of the decoder?\nProblem 10. Huffman and other coding schemes tend to devote more bits to the coding of\n(A) symbols carrying the most information\n(B) symbols carrying the least information\n(C) symbols that are likely to be repeated consecutively\n(D) symbols containing redundant information\nProblem 11. Consider the following two Huffman decoding tress for a variable-length code involving 5\nsymbols: A, B, C, D and E.\nA. Using Tree #1, decode the following encoded message: \"01000111101\".\n4 of 9\n\nB. Suppose we were encoding messages with the following probabilities for each of the 5 symbols: p(A) =\n0.5, p(B) = p(C) = p(D) = p(E) = 0.125. Which of the two encodings above (Tree #1 or Tree #2) would\nyield the shortest encoded messages averaged over many messages?\nC. Using the probabilities of part (B), if you learn that the first symbol in a message is \"B\", how many bits\nof information have you received?\nD. Using the probabilities of part (B), If Tree #2 is used to encode messages what is the average length of\n100-symbol messages, averaged over many messages?\nProblem 12. Ben Bitdiddle has been hired by the Registrar to help redesign the grades database for WebSIS.\nBen is told that the database only needs to store one of five possible grades (A, B, C, D, F). A survey of the\ncurrent WebSIS repository reveals the following probabilities for each grade:\nGrade\nA\nB\nC\nD\nF\nProbability of occurrence\np(A) = 18%\np(B) = 27%\np(C) = 25%\np(D) = 15%\np(E) = 15%\nA. Given the probabilities above, if you are told that a particular grade is \"C\", give an expression for the\nnumber of bits of information you have received.\nB. Ben is interested in storing a sequence of grades using as few bits as possible. Help him out by creating\na variable-length encoding that minimizes the average number of bits stored for a sequence of grades.\nUse the table above to determine how often a particular grade appears in the sequence.\nProblem 13. Consider a sigma-delta modulator used to convert a particular analog waveform into a sequence\nof 2-bit values. Building a histogram from the 2-bit values we get the following information:\nModulator value\n# of occurrences\n5 of 9\n\nA. Using probabilities computed from the histogram, construct a variable-length Huffman code for\nencoding these four values.\nB. If we transmit the 2-bit values as is, it takes 2 bits to send each value (doh!). If we use the Huffman\ncode from part (A) what is the average number of bits used to send a value? What compression ratio do\nwe achieve by using the Huffman code?\nC. Using Shannon's entropy formula, what is the average information content associated with each of the\n2-bit values output by the modulator? How does this compare to the answers for part (B)?\nProblem 14. In honor of Daisuke Matsuzaka's first game pitching for the Redsox, the Boston-based members\nof the Search for Extraterrestrial Intelligence (SETI) have decided to broadcast a 1,000,000 character\nmessage made up of the letters \"R\", \"E\", \"D\", \"S\", \"O\", \"X\". The characters are chosen at random according\nthe probabilities given in the table below:\nLetter p(Letter)\nR\n.21\nE\n.31\nD\n.11\n\nS\n.16\nO\n.19\nX\n.02\nA. If you learn that one of the randomly chosen letters is a vowel (i.e., \"E\" or \"O\") how many bits of\ninformation have you received?\nB. Nervous about the electric bill for the transmitter at Arecibo, the organizers have asked you to design a\nvariable length code that will minimize the number of bits needed to send the message of 1,000,000\nrandomly chosen characters. Please draw a Huffman decoding tree for your code, clearly labeling each\nleaf with the appropriate letter and each arc with either a \"0\" or a \"1\".\nC. Using your code, what bit string would they send for \"REDSOX\"?\nProblem 15. \"Information, please\"\nA. You're given a standard deck of 52 playing cards that you start to turn face up, card by card. So far as\n6 of 9\n\nyou know, they're in completely random order. How many new bits of information do you get when the\nfirst car is flipped over? The fifth card? The last card?\nB. Suppose there three alternatives (\"A\", \"B\" and \"C\") with the following probabilities of being chosen:\np(\"A\") = 0.8\np(\"B\") = 0.1\np(\"C\") = 0.1\nWe might encode the of \"A\" with the bit string \"0\", the choice of \"B\" with the bit string \"10\" and the\nchoice of \"C\" with the bit string \"11\".\nIf we record the results of making a sequence of choices by concatenating in left-to-right order the bit\nstrings that encode each choice, what sequence of choices is represented by the bit string\n\"00101001100000\"?\nC. Using the encoding of the previous part, what is the expected length of the bit string that encodes the\nresults of making 1000 choices? What is the length in the worst case? How do these numbers compare\nwith 1000*log2(3/1), which is the information content of 1000 equally-probable choices?\nD. Consider the sum of two six-sided dice. Even when the dice are \"fair\" the amount information\nconveyed by a single sum depends on what the sum is since some sums are more likely than others, as\nshown in the following figure:\nWhat is the average number of bits of information provided by the sum of 2 dice? Suppose we want to\ntransmit the sums resulting from rolling the dice 1000 times. How many bits should we expect that\ntransmission to take?\n7 of 9\nE. Suppose we want to transmit the sums resulting from rolling the dice 1000 times. If we use 4 bits to\nencode each sum, we'll need 4000 bits to transmit the result of 1000 rolls. If we use a variable-length\nbinary code which uses shorter sequences to encode more likely sums then the expected number of bits\nneed to encode 1000 sums should be less than 4000. Construct a variable-length encoding for the sum\nof two dice whose expected number of bits per sum is less than 3.5. (Hint: It's possible to find an\n\nencoding for the sum of two dice with an expected number of bits = 3.306.)\nF. Can we make an encoding for transmitting 1000 sums that has an expected length smaller than that\nachieved by the previous part?\nProblem 16.\nConsider messages comprised of four symbols -- A, B, C, D -- each with an associated probability of\noccurrence: p(A), p(B), p(C), p(D). Suppose p(A) ≥ p(B) ≥ p(C) ≥ p(D). Write down a single condition\n(equation or inequality) that is both necessary and sufficient to guarantee that the Huffman algorithm will\ngenerate a two-bit encoding for each symbol, i.e., the corresponding Huffman code will actually be a fixed-\nlength encoding using 2 bits for each symbol.\nProblem 17. Consider a Huffman code over four symbols: A, B, C, and D. For each of the following\nencodings indicate if it is a valid Huffman code, i.e., a code that would have been produced by the Huffman\nalgorithm given some set of probabilities p(A), ..., p(D).\nA. A:0, B:11, C:101, D:100\nB. A: 1, B:01, C:00, D:010\nC. A:00, B:01, C:110, D:111\nProblem 18.\nAfter careful data collection, Alyssa P. Hacker observes that the probability of HIGH or LOW traffic on\nStorrow Drive is given by the following table:\nCondition\np(HIGH traffic) p(LOW traffic)\nIf the Redsox are playing\n0.999\n.001\nIf the Redsox are not playing 0.25\n0.75\nA. If it is known that the Red Sox are playing, then how many bits of information are conveyed by the\nstatement that the traffic level is LOW.\nB. Suppose it is known that the Red Sox are not playing. What is the entropy of the corresponding\n8 of 9\n\nprobability distribution of traffic?\nProblem 19.\nConsider Huffman coding over four symbols (A, B, C and D) with probabilities p(A)=1/3, p(B)=1/2,\np(C)=1/12 and p(D)=1/12.\nThe entropy of the discrete random variable with this probability distribution was calculated to be 1.62581\nbits.\nWe then used the Huffman algorithm to build the following variable length code:\nA: 10\nB: 0\nC: 110\nD: 111\nwhich gave an expected encoding length of 1.666 bits/symbol, slightly higher than the entropy bound.\nA. Suppose we made up a new symbol alphabet consisting of all possible pairs of the original four symbols.\nEnumerate the new symbols and give the probabilities associated with each new symbol.\nB. What is the entropy associated with the discrete random variable that has the probability distribution\nyou gave in part (A). Is is the same, bigger, or smaller than the entropy of 1.626 calculated above?\nExplain.\nC. Derive the Huffman encoding for the new symbols and compute the expected encoding length\nexpressed in bits/symbol. How does it compare with the 1.666 bits/symbol for the original alphabet?\nNote: this is tedious to do by hand -- try using the Huffman program you wrote!\n9 of 9\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 1 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/ce7e6222a226cb69ddb950cde03a5341_MIT6_02F12_tutor01_sol.pdf",
      "content": "6.02 Practice Problems: Information, Entropy, & Source Coding\nProblem 1. Huffman coding is used to compactly encode the species of fish tagged by a game warden. If\n50% of the fish are bass and the rest are evenly divided among 15 other species, how many bits would be\nused to encode the species when a bass is tagged?\nIf a symbol has a probability of ≥ .5, it will be incorporated into the Huffman tree on the final step of the\nalgorithm, and will become a child of the final root of the decoding tree. This means it will have a 1-bit\nencoding.\nProblem 2. Several people at a party are trying to guess a 3-bit binary number. Alice is told that the number is\nodd; Bob is told that it is not a multiple of 3 (i.e., not 0, 3, or 6); Charlie is told that the number contains\nexactly two 1's; and Deb is given all three of these clues. How much information (in bits) did each player get\nabout the number?\nN = 8 choices for a 3-bit number\nAlice: odd = {001, 011, 101, 111} M=4, info = log2(8/4) = 1 bit\nBob: not a multiple of 3 = {001, 010, 100, 101, 111} M=5, info = log2(8/5) = .6781 bits\nCharlie: two 1's = {011, 101, 110} M=3, info = log2(8/3) = 1.4150\nDeb: odd, not a multiple of 3, two 1's = {101}, M=1, info = log(8/1) = 3 bits\nProblem 3. X is an unknown 8-bit binary number. You are given another 8-bit binary number, Y, and told that\nY differs from X in exactly one bit position. How many bits of information about X have you been given?\nKnowing Y, we've narrowed down the value of X to one of 8 choices. Bits of information = log2(256/8) = 5\nbits.\nProblem 4. In Blackjack the dealer starts by dealing 2 cards each to himself and his opponent: one face\ndown, one face up. After you look at your face-down card, you know a total of three cards. Assuming this\nwas the first hand played from a new deck, how many bits of information do you now have about the dealer's\nface down card?\nWe've narrowed down the choices for the dealer's face-down card from 52 (any card in the deck) to one of 49\ncards (since we know it can't be one of three visible cards. bits of information = log2(52/49).\nProblem 5. The following table shows the current undergraduate and MEng enrollments for the School of\nEngineering (SOE).\n1 of 15\n\nCourse (Department) # of students Prob.\nI (Civil & Env.)\n.07\nII (Mech. Eng.)\n.23\nIII (Mat. Sci.)\n.07\nVI (EECS)\n.38\nX (Chem. Eng.)\n.13\nXVI (Aero & Astro)\n.12\nTotal\n1.0\nA. When you learn a randomly chosen SOE student's department you get some number of bits of\ninformation. For which student department do you get the least amount of information?\nWe'll get the smallest value for log2(1/p) by choosing the choice with the largest p => EECS.\nB. Design a variable length Huffman code that minimizes the average number of bits in messages encoding\nthe departments of randomly chosen groups of students. Show your Huffman tree and give the code for\neach course.\nstep 1 = {(I,.07) (II,.23) (III,.07) (VI,.38) (X,.13) (XVI,.12)}\nstep 2 = {([I,III],.14) (II,.23) (VI,.38) (X,.13) (XVI,.12)}\nstep 3 = {([I,III],.14) (II,.23) (VI,.38) ([X,XVI],.25)}\nstep 4 = {([II,[I,III]],.37) (VI,.38) ([X,XVI],.25)}\nstep 5 = {([[X,XVI],[II,[I,III]]]),.62) (VI,.38)}\nstep 6 = {([[[X,XVI,[II,[I,III]],VI],1.0)}\ncode for course I: 0 1 1 0\ncode for course II: 0 1 0\ncode for course III: 0 1 1 1\ncode for course VI: 1\ncode for course X: 0 0 0\ncode for course XVI: 0 0 1\nThere are of course many equivalent codes derived by swapping the \"0\" and \"1\" labels for the children\nof any interior node of the decoding tree. So any code that meets the following constraints would be\nfine:\ncode for VI = length 1\ncode for II, X, XVI = length 3\ncode for I, III = length 4\nC. If your code is used to send messages containing only the encodings of the departments for each\nstudent in groups of 100 randomly chosen students, what's the average length of such messages? Write\nan expression if you wish.\n2 of 15\n\n100*[(.38)(1) + (.23 + .13 + .12)*(3) + (.07 + .07)*(4)] = 238 bits\nProblem 6. You're playing an on-line card game that uses a deck of 100 cards containing 3 Aces, 7 Kings, 25\nQueens, 31 Jacks and 34 Tens. In each round of the game the cards are shuffled, you make a bet about what\ntype of card will be drawn, then a single card is drawn and the winners are paid off. The drawn card is\nreinserted into the deck before the next round begins.\nA. How much information do you receive when told that a Queen has been drawn during the current\nround?\ninformation received = log2(1/p(Queen)) = log2(1/.25) = 2 bits\nB. Give a numeric expression for the average information content received when learning about the\noutcome of a round (aka the entropy).\n(.03)log2(1/.03) + (.07)log2(1/.07) + (.25)log2(1/.25) + (.31)log2(1/.31) + (.34)log2(1/.34) = 1.97 bits\nC. Construct a variable-length Huffman encoding that minimizes the length of messages that report the\noutcome of a sequence of rounds. The outcome of a single round is encoded as A (ace), K (king), Q\n(queen), J (jack) or X (ten). Specify your encoding for each of A, K, Q, J and X.\nEncoding for A: 001\nEncoding for K: 000\nEncoding for Q: 01\nEncoding for J: 11\nEncoding for X: 10\nD. Using your code from part (C) what is the expected length of a message reporting the outcome of 1000\nrounds (i.e., a message that contains 1000 symbols)?\n(.07+.03)(3 bits) + (.25+.31+.34)(2 bits) = 2.1 bits average symbol length using Huffman code. So\nexpected length of 1000-symbol message is 2100 bits.\nE. The Nevada Gaming Commission regularly receives messages in which the outcome for each round is\nencoded using the symbols A, K, Q, J and X. They discover that a large number of messages describing\nthe outcome of 1000 rounds (i.e. messages with 1000 symbols) can be compressed by the LZW\n3 of 15\n\nalgorithm into files each containing 43 bytes in total. They immediately issue an indictment for running\na crooked game. Briefly explain their reasoning.\n43 bytes is 344 bits, way below the entropy of 1970 bits for encoding 1000 symbols. This indicates the\nactual symbol probabilities must be dramatically different than the stated probabilities that were used to\ncalculate the entropy in part (B). The experimentally determined value of the actual entropy (344/1000\n= .344 bits) indicates that one or more of the symbols must have a much higher probability of occuring\nthan stated, which suggests a rigged game.\nProblem 7.\nConsider messages made up entirely of vowels (A, E, I, O, U). Here's a table of probabilities for each of the\nvowels:\nl\np(l) log2(1/p(l)) p(l)*log2(1/p(l))\nA\n0.22\n2.18\n0.48\nE\n0.34\n1.55\n0.53\nI\n0.17\n2.57\n0.43\nO\n0.19\n2.40\n0.46\nU\n0.08\n3.64\n0.29\nTotals 1.00\n12.34\n2.19\nA. Give an expression for the number of bits of information you receive when learning that a particular\nvowel is either I or U.\nThe probability that a letter is either I or U is p(I) + p(U) = 0.25. So the number of bits of information\nyou receive is log2(1/.25) = 2 bits.\nB. Using Huffman's algorithm, construct a variable-length code assuming that each vowel is encoded\nindividually. Please draw a diagram of the Huffman tree and give the encoding for each of the vowels.\nRunning Huffman's algorithm:\n/\\\n/ \\\n/\n\\\n/\\\n/\\\nA O E \\\n/\\\nI U\nSo assuming we label left branches with \"0\" and right branches with \"1\", the encodings are\nA = 00\nO = 01\nE = 10\nI = 110\n4 of 15\n\nU = 111\nC. Using your code from above, give an expression for the expected length in bits of an encoded message\ntransmitting 100 vowels.\nExpected length for one vowel = Σ p(l)*len(encoding(l)):\n0.22*2 + 0.19*2 + 0.34*2 + 0.17*3 + 0.08*3 = 2.25 bits\nSo the expected length for 100 vowels is 225 bits.\nD. Ben Bitdiddle spends all night working on a more complicated encoding algorithm and sends you email\nclaiming that using his code the expected length in bits of an encoded message transmitting 100 vowels\nis 197 bits. Would you pay good money for his implementation?\nNo! The entropy is 2.19 bits which means that 219 bits is a lower bound on the number of bits needed\non the average to encode 100 vowels. Ben's encoding uses less bits, which is impossible, so his code or\nhis derviation of the expected lenth must be bogus in some way.\nProblem 8. Describe the contents of the string table created when encoding a very long string of all a's using\nthe simple version of the LZW encoder shown below. In this example, if the decoder has received E encoded\nsymbols (i.e., string table indices) from the encoder, how many a's has it been able to decode?\ninitialize TABLE[0 to 255] = code for individual bytes\nSTRING = get input symbol\nwhile there are still input symbols:\nSYMBOL = get input symbol\nif STRING + SYMBOL is in TABLE:\nSTRING = STRING + SYMBOL\nelse:\noutput the code for STRING\nadd STRING + SYMBOL to TABLE\nSTRING = SYMBOL\noutput the code for STRING\nThe string table is filled with sequences of a's, each one longer than the last. So\ntable[256] = aa\ntable[257] = aaa\ntable[258] = aaaa\n...\nHere's what the decoder receives, and the decoding it does (it's instructive to figure out how the decoder is\nbuilding its copy of the string table based on what it's receiving):\nindex of a => decode a\n256 => decode aa\n257 => decode aaa\n5 of 15\n\n258 => decode aaaa\n...\nSo if E symbols have arrived, the decoder has produced 1 + 2 + 3 + ... + E a's, which sums to E(E+1)/2.\nProblem 9. Consider the pseudo-code for the LZW decoder given below:\ninitialize TABLE[0 to 255] = code for individual bytes\nCODE = read next code from encoder\nSTRING = TABLE[CODE]\noutput STRING\nwhile there are still codes to receive:\nCODE = read next code from encoder\nif TABLE[CODE] is not defined:\nENTRY = STRING + STRING[0]\nelse:\nENTRY = TABLE[CODE]\noutput ENTRY\nadd STRING+ENTRY[0] to TABLE\nSTRING = ENTRY\nSuppose that this decoder has received the following five codes from the LZW encoder (these are the first\nfive codes from a longer compression run):\n97 -- index of 'a' in the translation table\n98 -- index of 'b' in the translation table\n257 -- index of second addition to the translation table\n256 -- index of first addition to the translation table\n258 -- index of third addition to the translation table\nAfter it has finished processing the fifth code, what are the entries in TABLE and what is the cumulative\noutput of the decoder?\nHere's the step-by-step operation of the decoder:\nCODE=97, STRING='a', output 'a'\nCODE=98, ENTRY='b', output 'b', TABLE[256]='ab', STRING='b'\nCODE=257, ENTRY='bb', output 'bb', TABLE[257]='bb', STRING='bb'\nCODE=256, ENTRY='ab', output 'ab', TABLE[258]='bba', STRING='ab'\nCODE=258, ENTRY='bba', output 'bba', TABLE[259]='abb', STRING='bba'\nSo the cummulative output of the decoder is 'abbbabbba'.\nProblem 10. Huffman and other coding schemes tend to devote more bits to the coding of\n(A) symbols carrying the most information\n(B) symbols carrying the least information\n(C) symbols that are likely to be repeated consecutively\n(D) symbols containing redundant information\nAnswer is (A). Symbols carrying the most information, i.e., the symbols that are less likely to occur. This\n6 of 15\n\nmakes sense: to keep messages as short as possible, frequently occurring symbols should be encoded with\nfewer bits and infrequent symbols with more bits.\nProblem 11. Consider the following two Huffman decoding tress for a variable-length code involving 5\nsymbols: A, B, C, D and E.\nUsing Tree #1, decode the following encoded message: \"01000111101\".\nA.\nStarting at the root of the decoding tree, use the bits of the message to traverse the tree until a leaf node\nis reached; output that symbol. Repeat until all the bits in the message are consumed.\n0 = A\n100 = B\n0 = A\n111 = E\n101 = C\nB. Suppose we were encoding messages with the following probabilities for each of the 5 symbols: p(A) =\n0.5, p(B) = p(C) = p(D) = p(E) = 0.125. Which of the two encodings above (Tree #1 or Tree #2) would\nyield the shortest encoded messages averaged over many messages?\nUsing Tree #1, the expected length of the encoding for one symbol is:\n1*p(A) + 3*p(B) + 3*p(C) + 3*p(D) + 3*p(E) = 2.0\nUsing Tree #2, the expected length of the encoding for one symbol is:\n2*p(A) + 2*p(B) + 2*p(C) + 3*p(D) + 3*p(E) = 2.25\nSo using the encoding represented by Tree #1 would yield shorter messages on the average.\nC. Using the probabilities of part (B), if you learn that the first symbol in a message is \"B\", how many bits\nof information have you received?\n7 of 15\n\nbits of info received = log2(1/p) = log2(1/.125) = 3\nD. Using the probabilities of part (B), If Tree #2 is used to encode messages what is the average length of\n100-symbol messages, averaged over many messages?\nIn part (B) we calculated that the expected length of the encoding for one symbol using Tree #2 was\n2.25 bits, so for 100-symbol messages the expected length is 225 bits.\nProblem 12. Ben Bitdiddle has been hired by the Registrar to help redesign the grades database for WebSIS.\nBen is told that the database only needs to store one of five possible grades (A, B, C, D, F). A survey of the\ncurrent WebSIS repository reveals the following probabilities for each grade:\nGrade\nA\nB\nC\nD\nF\nProbability of occurrence\np(A) = 18%\np(B) = 27%\np(C) = 25%\np(D) = 15%\np(E) = 15%\nA. Given the probabilities above, if you are told that a particular grade is \"C\", give an expression for the\nnumber of bits of information you have received.\n# of bits for \"C\" = log(1/pC) = log(1/.25) = log(4) = 2\nB. Ben is interested in storing a sequence of grades using as few bits as possible. Help him out by creating\na variable-length encoding that minimizes the average number of bits stored for a sequence of grades.\nUse the table above to determine how often a particular grade appears in the sequence.\nusing Huffman algorithm:\n- since D,F are least probable, make a subtree of them, p(D/\\F) = 30%\n- now A,C are least probable, make a subtree of them, P(A/\\C) = 43%\n- now B,DF are least probable, make a subtree of them P(B/\\(D/\\F)) = 55%\n- just AC,BDF are left, make a subtree of them (A/\\C)/\\(B/\\(D/\\F))\n- so A = 00, B = 10, C = 01, D = 110, F = 111\nProblem 13. Consider a sigma-delta modulator used to convert a particular analog waveform into a sequence\nof 2-bit values. Building a histogram from the 2-bit values we get the following information:\nModulator value # of occurrences\n8 of 15\n\nA. Using probabilities computed from the histogram, construct a variable-length Huffman code for\nencoding these four values.\np(00)=.0668 p(01)=.4334 p(10)=.4327 p(11)=.0671\nHuffman tree = 01 /\\ (10 /\\ (00 /\\ 11))\nCode: 01=\"0\" 10=\"10\" 00=\"110\" 11=\"111\"\nB. If we transmit the 2-bit values as is, it takes 2 bits to send each value (doh!). If we use the Huffman\ncode from part (A) what is the average number of bits used to send a value? What compression ratio do\nwe achieve by using the Huffman code?\nsum(pi*len(code for pi)) = 1.7005, which is 85% of the original 2-bit/symbol encoding.\nC. Using Shannon's entropy formula, what is the average information content associated with each of the\n2-bit values output by the modulator? How does this compare to the answers for part (B)?\nsum(pi*log2(1/pi)) = 1.568, so the code of part(B) isn't quite as efficient as we can achieve by using an\nencoding that codes multiple pairs as in one code symbol.\nProblem 14. In honor of Daisuke Matsuzaka's first game pitching for the Redsox, the Boston-based members\nof the Search for Extraterrestrial Intelligence (SETI) have decided to broadcast a 1,000,000 character\nmessage made up of the letters \"R\", \"E\", \"D\", \"S\", \"O\", \"X\". The characters are chosen at random according\nthe probabilities given in the table below:\nLetter p(Letter)\nR\n.21\nE\n.31\nD\n.11\n\nS\n.16\nO\n.19\nX\n.02\nA. If you learn that one of the randomly chosen letters is a vowel (i.e., \"E\" or \"O\") how many bits of\ninformation have you received?\np(E or O) = .31 + .19 = .5, log2(1/pi) = 1 bit\n9 of 15\n\nB. Nervous about the electric bill for the transmitter at Arecibo, the organizers have asked you to design a\nvariable length code that will minimize the number of bits needed to send the message of 1,000,000\nrandomly chosen characters. Please draw a Huffman decoding tree for your code, clearly labeling each\nleaf with the appropriate letter and each arc with either a \"0\" or a \"1\".\nHuffman algorithm:\nchoose X,D: X/\\D, p = .13\nchoose S,XD: S/\\(X/\\D), p = .29\nchoose R,0: R/\\O, p = .40\nchoose E,SXD: E/\\(S/\\(X/\\D)), p = .6\nchoose RO,ESXD: (R/\\O) /\\ (E/\\(S/\\(X/\\D)))\ncode: R=00 O=01 E=10 S=110 X=1110 D=1111\nC. Using your code, what bit string would they send for \"REDSOX\"?\n00 10 1111 110 01 1110\nProblem 15. \"Information, please\"\nA. You're given a standard deck of 52 playing cards that you start to turn face up, card by card. So far as\nyou know, they're in completely random order. How many new bits of information do you get when the\nfirst car is flipped over? The fifth card? The last card?\nFirst card: log2(52/1)\nFifth card: log2(48/1)\nLast card: log2(1/1) = 0 (no information since, knowing the other 51 cards, one can predict with\ncertainty the last card)\nB. Suppose there three alternatives (\"A\", \"B\" and \"C\") with the following probabilities of being chosen:\np(\"A\") = 0.8\np(\"B\") = 0.1\np(\"C\") = 0.1\nWe might encode the of \"A\" with the bit string \"0\", the choice of \"B\" with the bit string \"10\" and the\nchoice of \"C\" with the bit string \"11\".\nIf we record the results of making a sequence of choices by concatenating in left-to-right order the bit\nstrings that encode each choice, what sequence of choices is represented by the bit string\n\"00101001100000\"?\nAABBACAAAAA\n10 of 15\n\nC. Using the encoding of the previous part, what is the expected length of the bit string that encodes the\nresults of making 1000 choices? What is the length in the worst case? How do these numbers compare\nwith 1000*log2(3/1), which is the information content of 1000 equally-probable choices?\nExpected length = 1000[(0.8)(1) + (0.1)(2) + (0.1)(2)] = 1200 bits\nIn the worst case, each choice would take 2 bits to encode = 2000 bits.\n1000*log2(3/1) = 1585. In general, a variable-length encoding of sequences of N choices with differing\nprobabilities gets better compression than can be achieved if the choices are equally probable.\nD. Consider the sum of two six-sided dice. Even when the dice are \"fair\" the amount information\nconveyed by a single sum depends on what the sum is since some sums are more likely than others, as\nshown in the following figure:\nWhat is the average number of bits of information provided by the sum of 2 dice? Suppose we want to\ntransmit the sums resulting from rolling the dice 1000 times. How many bits should we expect that\ntransmission to take?\nAverage number of bits = sum (p_i)log2(1/p_i) for i = 2 through 12. Using the probabilities given in the\nfigure above the average number of bits of information provided by the sum of two dice is 3.2744.\nSo if we had the perfect encoding, the expected length of the transmission would be 3274.4 bits. If we\nencode each sum separately we can't quite achieve this lower bound -- see the next question for details.\nE. Suppose we want to transmit the sums resulting from rolling the dice 1000 times. If we use 4 bits to\nencode each sum, we'll need 4000 bits to transmit the result of 1000 rolls. If we use a variable-length\nbinary code which uses shorter sequences to encode more likely sums then the expected number of bits\nneed to encode 1000 sums should be less than 4000. Construct a variable-length encoding for the sum\nof two dice whose expected number of bits per sum is less than 3.5. (Hint: It's possible to find an\nencoding for the sum of two dice with an expected number of bits = 3.306.)\nUsing Huffman's algorithm, we arrive at the following encoding which has 3.3056 as the expected\nnumber of bits for each sum.\n11 of 15\n\nF. Can we make an encoding for transmitting 1000 sums that has an expected length smaller than that\nachieved by the previous part?\nYes, but we have to look at encoding more than one sum at a time, e.g., by applying the construction\nalgorithm to pairs of sums, or ultimately to all 1000 sums at once. Many of the more sophisticated\ncompression algorithms consider sequences of symbols when constructing the appropriate encoding\nscheme.\nProblem 16.\nConsider messages comprised of four symbols -- A, B, C, D -- each with an associated probability of\noccurrence: p(A), p(B), p(C), p(D). Suppose p(A) ≥ p(B) ≥ p(C) ≥ p(D). Write down a single condition\n(equation or inequality) that is both necessary and sufficient to guarantee that the Huffman algorithm will\ngenerate a two-bit encoding for each symbol, i.e., the corresponding Huffman code will actually be a fixed-\nlength encoding using 2 bits for each symbol.\np(C) + p(D) > p(A). The reason is that there are only two possible Huffman trees with 4 leaves. For the tree\nwhere every symbol is encoded with two bits, we need to have p(C) + p(D) + p(B) > p(B) + p(A) to ensure\nthat the algorithm will combine B with A instead of with C/D. We need a strict inequality because otherwise\nthere is a possiblity of a tie and the unbalanced tree may be chosen since it has the same expected number of\nbits.\nProblem 17. Consider a Huffman code over four symbols: A, B, C, and D. For each of the following\nencodings indicate if it is a valid Huffman code, i.e., a code that would have been produced by the Huffman\nalgorithm given some set of probabilities p(A), ..., p(D).\nA. A:0, B:11, C:101, D:100\nValid. This is one of the two possible Huffman trees with 4 leaves.\n12 of 15\n\nB. A: 1, B:01, C:00, D:010\nNot valid. This code can't be unambigously decoded by the receiver since the encoding for B is a prefix\nfor the encoding of D. For example, the message 0101 might be decoded as \"BB\" or \"DA\" -- the\nreceiver doesn't have sufficient information to tell which.\nC. A:00, B:01, C:110, D:111\nNot valid. This a legal variable length code, but it's not optimal. A shorter code would have C and D\nencoded in 2 bits, as 10 and 11 (or vice versa), and that would be a Huffman code for the same symbol\nprobabilities, not the one given.\nProblem 18.\nAfter careful data collection, Alyssa P. Hacker observes that the probability of HIGH or LOW traffic on\nStorrow Drive is given by the following table:\nCondition\np(HIGH traffic) p(LOW traffic)\nIf the Redsox are playing\n0.999\n.001\nIf the Redsox are not playing 0.25\n0.75\nA. If it is known that the Red Sox are playing, then how many bits of information are conveyed by the\nstatement that the traffic level is LOW.\nlog2(1/0.001) = 9.966 bits.\nB. Suppose it is known that the Red Sox are not playing. What is the entropy of the corresponding\nprobability distribution of traffic?\n0.25*log2(1/0.25) + 0.75*log2(1/0.75) = 0.811 bits.\nProblem 19.\nConsider Huffman coding over four symbols (A, B, C and D) with probabilities p(A)=1/3, p(B)=1/2,\np(C)=1/12 and p(D)=1/12.\nThe entropy of the discrete random variable with this probability distribution was calculated to be 1.62581\nbits.\nWe then used the Huffman algorithm to build the following variable length code:\nA: 10\nB: 0\n13 of 15\n\nC: 110\nD: 111\nwhich gave an expected encoding length of 1.666 bits/symbol, slightly higher than the entropy bound.\nA. Suppose we made up a new symbol alphabet consisting of all possible pairs of the original four symbols.\nEnumerate the new symbols and give the probabilities associated with each new symbol.\nHere are the 16 new symbols and their associated probabilities:\nAA p(AA)=1/9\nAB p(AB)=1/6\nAC p(AC)=1/36\nAD p(AD)=1/36\nBA p(BA)=1/6\nBB p(BB)=1/4\nBC p(BC)=1/24\nBD p(BD)=1/24\nCA p(CA)=1/36\nCB p(CB)=1/24\nCC p(CC)=1/144\nCD p(CD)=1/144\nDA p(DA)=1/36\nDB p(DB)=1/24\nDC p(DC)=1/144\nDD p(DD)=1/144\nB. What is the entropy associated with the discrete random variable that has the probability distribution\nyou gave in part (A). Is is the same, bigger, or smaller than the entropy of 1.626 calculated above?\nExplain.\nThe entropy for the new double-symbol alphabet is 3.25163 bits/double-symbol, exactly twice that of\nthe entropy of the original alphabet. This means that we haven't changed the expected information\ncontent by choosing to encode pairs of symbols instead of one symbol at a time.\nC. Derive the Huffman encoding for the new symbols and compute the expected encoding length\nexpressed in bits/symbol. How does it compare with the 1.666 bits/symbol for the original alphabet?\nNote: this is tedious to do by hand -- try using the Huffman program you wrote!\nAB = 000\nBA = 001\nBB = 01\nDB = 10000\nDD = 1000100\nCC = 1000101\nCD = 1000110\nDC = 1000111\nAD = 10010\nCA = 10011\nAA = 101\nDA = 11000\nAC = 11001\nBC = 1101\n14 of 15\n\nBD = 1110\nCB = 1111\nThe expected length of encoding a choice is 3.29167 bits/double-symbol, which is slightly less than\ntwice 1.6666 bits/symbols. In other words, the expected encoded length of a message will be slightly\nsmaller if we encode pairs of symbols using the encoding above than if we used the original\none-symbol-at-a-time encoding.\n15 of 15\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/88ca2f5581b4f3d52151cfc052342ac4_MIT6_02F12_tutor10.pdf",
      "content": "6.02 Practice Problems: Routing\nIMPORTANT: IN ADDITION TO THESE PROBLEMS, PLEASE SOLVE THE PROBLEMS AT THE END OF\nCHAPTERS 17 AND 18.\nProblem 1. Consider the following networks: network I (containing nodes A, B, C) and network II (containing nodes D, E,\nF).\nsteps, all links are fully functional and there is no delay in the links. Nodes take zero time to process advertisements\nonce they receive them. The HELLO protocol runs in the background every time step in a way that any changes in\nlink connectivity are reflected in the next DV advertisement. We count time steps from t=0 time steps.\nPlease fill in the following table:\nThe Distance Vector Protocol described in class is used in both networks. Assume advertisements are sent every 5 time\nA.\nEvent\nNumber of time steps\nA's routing table has an entry for B\nA's routing table has an entry for C\nD's routing table has an entry for E\nF's routing table has an entry for D\nB. Now assume the link B-C fails at t = 51 and link D-E fails at t = 71 time steps. Please fill in this table:\nEvent\nNumber of time steps\nB's advertisements reflect that C is unreachable\nA's routing table reflects C is unreachable\nD's routing table reflects a new route for E\nProblem 2. Alyssa P. Hacker manages MIT's internal network that runs link-state routing. She wants to experiment with a\nfew possible routing strategies. Of all possible paths available to a particular destination at a node, a routing strategy specifies\nthe path that must be picked to create a routing table entry. Below is the name Alyssa has for each strategy and a brief\ndescription of how it works.\nMinCost: Every node picks the path that has the smallest sum of link costs along the path. (This is the minimum cost\nrouting you implemented in the lab).\n1 of 9\n\nMinHop: Every node picks the path with the smallest number of hops (irrespective of what the cost on the links is).\nSecondMinCost: Every node picks the path with the second lowest sum of link costs. That is, every node picks the\nsecond best path with respect to path costs.\nMinCostSquared: Every node picks the path that has the smallest sum of squares of link costs along the path.\nAssume that sufficient information (e.g., costs, delays, bandwidths, and loss probabilities of the various links) is exchanged in\nthe link state advertisements, so that every node has complete information about the entire network and can correctly\nimplement the strategies above. You can also assume that a link's properties don't change, e.g., it doesn't fail.\nA. Help Alyssa figure out which of these strategies will work correctly, and which will result in routing with loops. In case\nof strategies that do result in routing loops, come up with an example network topology with a routing loop to\nconvince Alyssa.\nB. How would you implement MinCostSquared in a distance-vector protocol?\nProblem 3. Which of the following tasks does a router R in a packet-switched network perform when it gets a packet with\ndestination address D? Indicate True or False for each choice.\nA. R looks up D in its routing table to determine the outgoing link.\nB. R sends out a HELLO packet or a routing protocol advertisement to its neighbors.\nC. R calculates the minimum-cost route to destination D.\nD. R may discard the packet.\nProblem 4. Alice and Bob are responsible for implementing Dijkstra's algorithm at the nodes in a network running a\nlink-state protocol. On her nodes, Alice implements a minimum-cost algorithm. On his nodes, Bob implements a \"shortest\nnumber of hops\" algorithm. Give an example of a network topology with 4 or more nodes in which a routing loop occurs\nwith Alice and Bob's implementations running simultaneously in the same network. Assume that there are no failures.\n(Note: A routing loop occurs when a group of k ≥ 1 distinct nodes, n_0 , n_1 , n_2 , ..., n_(k-1) have routes such that n_i's\nnext-hop (route) to a destination is n_(i+1 mod k).\nProblem 5. Consider the network shown below. The number near each link is its cost.\n2 of 9\n\nWe're interested in finding the shortest paths (taking costs into account) from S to every other node in the network. What is\nthe result of running Dijkstra's shortest path algorithm on this network? To answer this question, near each node, list a pair of\nnumbers: The first element of the pair should be the order, or the iteration of the algorithm in which the node is picked. The\nsecond element of each pair should be the shortest path cost from S to that node.\nTo help you get started, we've labeled the first couple of nodes: S has a label (Order: 0, Cost: 0) and A has the label (Order:\n1, Cost: 2).\nProblem 6. Consider any two graphs(networks) G and G' that are identical except for the costs of the links. Please answer\nthese questions.\nA. The cost of link l in graph G is c_l > 0, and the cost of the same link l in Graph G' is k*c_l, where k > 0 is a constant\nand the same scaling relationship holds for all the links. Are the shortest paths between any two nodes in the two\ngraphs identical? Justify your answer.\nB. Now suppose that the cost of a link l in G' is k*c_l + h, where k > 0 and h > 0 are constants. Are the shortest paths\nbetween any two nodes in the two graphs identical? Justify your answer.\nProblem 7. Dijkstra's algorithm\nA. For the following network\n3 of 9\n\nan empty routing tree generated by Dijkstra's algorithm for node A (to every other node) is shown below. Fill in the\nmissing nodes and indicate the order that each node was added and its associated cost. For reference, node C's\ncompleted routing tree is shown as well.\nB. Now assume that node F has been added to the network along with links L1, L2 and L3.\nWhat are the constraints on L1, L2 and L3 such that node A's routing tree must match the topology shown below\n(regardless of how ties are broken in the algorithm), and it is known that node F is not the last node added when using\nDijkstra's algorithm? All costs are positive integers.\n4 of 9\n\nProblem 8. Under some conditions, a distance vector protocol finding minimum cost paths suffers from the \"count\nto-infinity\" problem. Indicate True or False for each choice.\nA. The count-to-infinity problem may arise in a distance vector protocol when the network gets disconnected.\nB. The count-to-infinity problem may arise in a distance vector protocol even when the network never gets disconnected.\nC. The \"path vector\" enhancement to a distance vector protocol always enables the protocol to converge without\ncounting to infinity.\nProblem 9. Ben Bitdiddle has set up a multi-hop wireless network in which he would like to find paths with high probability\nof packet delivery between any two nodes. His network runs a distance vector protocol similar to what you developed in the\npset. In Ben's distance vector (BDV) protocol, each node maintains a metric to every destination that it knows about in the\nnetwork. The metric is the nodea€TMs estimate of the packet success probability along the path between the node and the\ndestination.\nThe packet success proba bility along a link or path is defined as 1 minus the packet loss probability along the corresponding\nlink or path. Each node uses the periodic HELLO messages sent by each of its neighbors to estimate the packet loss\nprobability of the link from each neighbor. You may assume that the link loss probabilities are symmetric; i.e., the loss\nprobability of the link from node A to node B is the same as from B to A. Each link L maintains its loss probability in the\nvariable L.lossprob and 0 < L.lossprob < 1.\nA. The key pieces of the Python code for each node's integrate() function in BDV is given below. It has three missing\nblanks. Please fill them in so that the protocol will eventually converge without routing loops to the correct metric at\neach node. The variables are the same as in the pset: self.routes is the dictionary of routing entries (mapping\ndestinations to links), self.getlink(fromnode) returns the link connecting the node self to the node fromnode, and\nthe integrate procedure runs whenever the node receives an advertisement (adv) from node fromnode. As in the\npset, adv is a list of (destination, metric) tuples. In the code below, self.metric is a dictionary storing the nodea€TMs\ncurrent estimate of the routing metric (i.e., the packet success probability) for each known destination. Please fill in\nthe missing code.\n# Process an advertisement from a neighboring node in BDV\n5 of 9\n\ndef integrate(self, fromnode, adv):\nL = self.getlink(fromnode)\nfor (dest, metric) in adv:\nmy_metric = __________________________________\nif (dest not in self.routes\nor self.metric[dest] _____ my_metric\nor ___________________________________________):\nself.routes[dest] = L\nself.metric[dest] = my_metric\n# rest of integrate() not shown\nBen wants to try out a link-state protocol now. During the flooding step, each node sends out a link-state advertisement\ncomprising its address, an incrementing sequence number, and a list of tuples of the form (neighbor,lossprob), where the\nlossprob is the estimated loss probability to the neighbor.\nB. Why does the link-state advertisement include a sequence number?\nBen would like to reuse, without modification, his implementation of Dijkstra's shortest paths algorithm from the pset, which\ntakes a map in which the links have non-negative costs and produces a path that minimizes the sum of the costs of the links\non the path to each destination.\nC. Ben has to transform the lossprob information from the LSA to produce link costs so that he can use his Dijkstra\nimplementation without any changes. Which of these transformations will accomplish this goal? Choose the BEST\nanswer.\na. Use lossprob as the link cost.\nb. Use -1/log(1-lossprob) as the link cost.\nc. Use log(1/(1-lossprob)) as the link cost.\nd. Use log(1 - lossprob) as the link cost.\nProblem 10. We studied a few principles for designing networks in 6.02.\nA. State one significant difference between a circuit-switched and a packet-switched network.\nB. Why does topological addressing enable large networks to be built?\nC. Give one difference between what a switch does in a packet-switched network and a circuit-switched network.\nProblem 11. Eager B. Eaver implements distance vector routing in his network in which the links all have arbitrary positive\ncosts. In addition, there are at least two paths between any two nodes in the network. One node, u, has an erroneous\nimplementation of the integration step: it takes the advertised costs from each neighbor and picks the route corresponding to\nthe minimum advertised cost to each destination as its route to that destination, without adding the link cost to the neighbor.\nIt breaks any ties arbitrarily. All the other nodes are implemented correctly.\nLet's use the term \"correct route\" to mean the route that corresponds to the minimum-cost path. Which of the following\nstatements are true of Eager's network?\n6 of 9\n\na. Only u may have incorrect routes to any other node.\nb. Only u and u's neighbors may have incorrect routes to any other node.\nc. In some topologies, all nodes may have correct routes.\nd. Even if no HELLO or advertisements packets are lost and no link or node failures occur, a routing loop may occur.\nProblem 12. Consider a network running the link-state routing protocol as described in lecture and on the pset. How many\ncopies of any given LSA are received by a given node in the network?\nProblem 13. In implementing Dijkstra's algorithm in the link-state routing protocol at node u, Louis Reasoner first sets the\nroute for each directly connected node v to be the link connecting u to v. Louis then implements the rest of the algorithm\ncorrectly, aiming to produce minimum-cost routes, but does not change the routes to the directly connected nodes. In this\nnetwork, u has at least two directly connected nodes, and there is more than one path between any two nodes. Assume that\nall link costs are non-negative. Which of the following statements is true of u's routing table?\nA. There are topologies and link costs where the majority of the routes to other nodes will be incorrect.\nB. There are topologies and link costs where no routing table entry (other than from u to itself) will be correct.\nC. There are topologies and link costs where all routing table entry (other than from u to itself) will be correct.\nProblem 14. A network with N nodes and N bidirectional links is connected in a ring, and N is an even number.\nThe network runs a distance-vector protocol in which the advertisement step at each node runs when the local time is T*i\nseconds and the integration step runs when the local time is T*i + T/2 seconds, (i=1,2,...). Each advertisement takes time δ\nto reach a neighbor. Each node has a separate clock and time is not synchronized between the different nodes.\nSuppose that at some time t after the routing has converged, node N + 1 is inserted into the ring, as shown in the figure\nabove. Assume that there are no other changes in the network topology and no packet losses. Also assume that nodes 1 and\nN update their routing tables at time t to include node N + 1, and then rely on their next scheduled advertisements to\npropagate this new information.\nA. What is the minimum time before every node in the network has a route to node N + 1?\n7 of 9\n\nB. What is the maximum time before every node in the network has a route to node N + 1?\nProblem 15. Louis Reasoner implements the link-state routing protocol discussed in 6.02 on a best-effort network with a\nnon-zero packet loss rate. In an attempt to save bandwidth, instead of sending link-state advertisements periodically, each\nnode sends an advertisement only if one of its links fails or when the cost of one of its links changes. The rest of the protocol\nremains unchanged. Will Louis' implementation always converge to produce correct routing tables on all the nodes?\nProblem 16. Consider a network implementing minimum-cost routing using the distance-vector protocol. A node, S, has k\nneighbors, numbered 1 through k, with link cost c_i to neighbor i (all links have symmetric costs). Initially, S has no route for\ndestination D. Then, S hears advertisements for D from each neighbor, with neighbor i advertising a cost of p_i. The node\nintegrates these k advertisements. What is the cost for destination D in S's routing table after the integration?\nProblem 17. Consider the network shown in the picture below. Each node implements Dijkstra's shortest path algorithm\nusing the link costs shown in the picture.\nA. Initially, node B's routing table contains only one entry, for itself. When B runs Dijkstra's algorithm, in what order are\nnodes added to the routing table? List all possible answers.\nB. Now suppose the link cost for one of the links changes but all costs remain non-negative. For each change in link cost\nlisted below, state whether it is possible for the route at node B (i.e., the link used by B) for any destination to change,\nand if so, name the destination(s) whose routes may change.\na. The cost of link(A, C) increases.\nb. The cost of link(A, C) decreases.\nc. The cost of link(B, C) increases.\nd. The cost of link(B, C) decreases.\n8 of 9\n\nProblem 18. Alyssa P. Hacker implements the 6.02 distance-vector protocol on the network shown below. Each node has its\nown local clock, which may not be synchronized with any other node's clock. Each node sends its distance-vector\nadvertisement every 100 seconds. When a node receives an advertisement, it immediately integrates it. The time to send a\nmessage on a link and to integrate advertisements is negligible. No advertisements are lost. There is no HELLO protocol in\nthis network.\nA. At time 0, all the nodes except D are up and running. At time 10 seconds, node D turns on and immediately sends a\nroute advertisement for itself to all its neighbors. What is the minimum time at which each of the other nodes is\nguaranteed to have a correct routing table entry corresponding to a minimum-cost path to reach D? Justify your\nanswers.\nB. If every node sends packets to destination D, and to no other destination, which link would carry the most traffic?\nAlyssa is unhappy that one of the links in the network carries a large amount of traffic when all the nodes are sending\npackets to D. She decides to overcome this limitation with Alyssa's Vector Protocol (AVP). In AVP, S lies, advertising a \"path\ncost\" for destination D that is different from the sum of the link costs along the path used to reach D. All the other nodes\nimplement the standard distance-vector protocol, not AVP.\nC. What is the smallest numerical value of the cost that S should advertise for D along each of its links, to guarantee that\nonly its own traffic for D uses its direct link to D? Assume that all advertised costs are integers; if two path costs are\nequal, one can't be sure which path will be taken.\n9 of 9\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 10 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/8d5d612c8960c954dae89f007db5eae4_MIT6_02F12_tutor10_sol.pdf",
      "content": "6.02 Practice Problems: Routing\nIMPORTANT: IN ADDITION TO THESE PROBLEMS, PLEASE SOLVE THE PROBLEMS AT THE END\nOF CHAPTERS 17 AND 18.\nProblem 1. Consider the following networks: network I (containing nodes A, B, C) and network II (containing\nnodes D, E, F).\nevery 5 time steps, all links are fully functional and there is no delay in the links. Nodes take zero time to\nprocess advertisements once they receive them. The HELLO protocol runs in the background every time step\nin a way that any changes in link connectivity are reflected in the next DV advertisement. We count time\nsteps from t=0 time steps.\nPlease fill in the following table:\nThe Distance Vector Protocol described in class is used in both networks. Assume advertisements are sent\nA.\nEvent\nNumber of time steps\nA's routing table has an entry for B\nA's routing table has an entry for C\nD's routing table has an entry for E\nF's routing table has an entry for D\nA's routing table has an entry for B: 5 time steps\nA's routing table has an entry for C: 10 time steps\nD's routing table has an entry for E: 5 time steps\nF's routing table has an entry for D: 5 time steps\nB. Now assume the link B-C fails at t = 51 and link D-E fails at t = 71 time steps. Please fill in this table:\nEvent\nNumber of time steps\nB's advertisements reflect that C is unreachable\nA's routing table reflects C is unreachable\nD's routing table reflects a new route for E\n1 of 15\n\nB's advertisements reflect that C is unreachable: 55 time steps\nA's routing table reflects C is unreachable: 55 time steps\nD's routing table reflects a new route for E: 75 time steps\nProblem 2. Alyssa P. Hacker manages MIT's internal network that runs link-state routing. She wants to experiment\nwith a few possible routing strategies. Of all possible paths available to a particular destination at a node, a routing\nstrategy specifies the path that must be picked to create a routing table entry. Below is the name Alyssa has for each\nstrategy and a brief description of how it works.\nMinCost: Every node picks the path that has the smallest sum of link costs along the path. (This is the\nminimum cost routing you implemented in the lab).\nMinHop: Every node picks the path with the smallest number of hops (irrespective of what the cost on the\nlinks is).\nSecondMinCost: Every node picks the path with the second lowest sum of link costs. That is, every node\npicks the second best path with respect to path costs.\nMinCostSquared: Every node picks the path that has the smallest sum of squares of link costs along the path.\nAssume that sufficient information (e.g., costs, delays, bandwidths, and loss probabilities of the various links) is\nexchanged in the link state advertisements, so that every node has complete information about the entire network\nand can correctly implement the strategies above. You can also assume that a link's properties don't change, e.g., it\ndoesn't fail.\nA. Help Alyssa figure out which of these strategies will work correctly, and which will result in routing with\nloops. In case of strategies that do result in routing loops, come up with an example network topology with a\nrouting loop to convince Alyssa.\nAnswer: All except SecondMinCost will work fine.\nTo see why SecondMinCost will not work: consider the triangle topology with 3 nodes A, B, D, and equal\ncost on all the links. The second route at A to D is via B, and the second best route at B to D is via A,\nresulting in a routing loop.\nB. How would you implement MinCostSquared in a distance-vector protocol?\nTo implement MinCostSquared, your integrate_announcement routine would add the square of the link cost\n(instead of just the link cost) to any route costs advertised over that link.\nProblem 3. Which of the following tasks does a router R in a packet-switched network perform when it gets a\npacket with destination address D? Indicate True or False for each choice.\nA. R looks up D in its routing table to determine the outgoing link.\nTrue.\n2 of 15\n\nB. R sends out a HELLO packet or a routing protocol advertisement to its neighbors.\nFalse.\nC. R calculates the minimum-cost route to destination D.\nFalse.\nD. R may discard the packet.\nTrue.\nProblem 4. Alice and Bob are responsible for implementing Dijkstra's algorithm at the nodes in a network running a\nlink-state protocol. On her nodes, Alice implements a minimum-cost algorithm. On his nodes, Bob implements a\n\"shortest number of hops\" algorithm. Give an example of a network topology with 4 or more nodes in which a\nrouting loop occurs with Alice and Bob's implementations running simultaneously in the same network. Assume that\nthere are no failures.\n(Note: A routing loop occurs when a group of k ≥ 1 distinct nodes, n_0 , n_1 , n_2 , ..., n_(k-1) have routes such\nthat n_i's next-hop (route) to a destination is n_(i+1 mod k).\nIn the picture below, the grey nodes (A in particular) run Bob's algorithm (shortest number of hops), while the white\nnodes (B in particular) run Alice's (minimum-cost).\nSuppose the destination is E. A will pick B as its next hop because ABE is the shortest path. B will pick A as its\nnext hop because BACDE is the minimum-cost path (cost of 4, compared to 11 for the ABE path). The result is a\nrouting loop ABABAB...\nProblem 5. Consider the network shown below. The number near each link is its cost.\n3 of 15\n\nWe're interested in finding the shortest paths (taking costs into account) from S to every other node in the network.\nWhat is the result of running Dijkstra's shortest path algorithm on this network? To answer this question, near each\nnode, list a pair of numbers: The first element of the pair should be the order, or the iteration of the algorithm in\nwhich the node is picked. The second element of each pair should be the shortest path cost from S to that node.\nTo help you get started, we've labeled the first couple of nodes: S has a label (Order: 0, Cost: 0) and A has the label\n(Order: 1, Cost: 2).\nA: order = 1, cost = 2\nE: order = 2, cost = 3\nB: order = 3, cost = 4\nC: order = 4, cost = 5\nD: order = 5, cost = 6\nProblem 6. Consider any two graphs(networks) G and G' that are identical except for the costs of the links. Please\nanswer these questions.\nA. The cost of link l in graph G is c_l > 0, and the cost of the same link l in Graph G' is k*c_l, where k > 0 is a\nconstant and the same scaling relationship holds for all the links. Are the shortest paths between any two\nnodes in the two graphs identical? Justify your answer.\nYes, they're identical. Scaling all the costs by a constant factor doesn't change their relative size.\nB. Now suppose that the cost of a link l in G' is k*c_l + h, where k > 0 and h > 0 are constants. Are the\nshortest paths between any two nodes in the two graphs identical? Justify your answer.\nNo, they're not necessarily identical. Consider two paths between nodes A and B in graph G. One path takes\n3 hops, each of cost 1, for a total cost of 3. The other path takes 1 hop, with a cost of 4. In this case, the\nshortest path between nodes A and B is the first one.\nConsider k=1 and h=1 and compute the costs and shortest paths in G'. Now the 3-hop path has cost 6 and the\n1-hop path has cost 5. In G' the shortest path is the second path.\n4 of 15\n\nProblem 7. Dijkstra's algorithm\nA. For the following network\nan empty routing tree generated by Dijkstra's algorithm for node A (to every other node) is shown below. Fill\nin the missing nodes and indicate the order that each node was added and its associated cost. For reference,\nnode C's completed routing tree is shown as well.\n5 of 15\n\nB. Now assume that node F has been added to the network along with links L1, L2 and L3.\nWhat are the constraints on L1, L2 and L3 such that node A's routing tree must match the topology shown\nbelow (regardless of how ties are broken in the algorithm), and it is known that node F is not the last node\nadded when using Dijkstra's algorithm? All costs are positive integers.\n6 of 15\n\nFirst note that there are several possibilities for the tree, but only one really works as we will show.\nCase 1: The left could be ACDF and the right could be ABE.\nThis case is impossible because ABE is not a min-cost path; ACDE has cost 9, which is less than the cost of\nABE (10).\nCase 2: The left is ACDE and the right is ABF.\nThis is not possible because F cannot be the last node added, and the last node added is on the bottom right.\nCase 3 is the only remaining possibility: The left is ABFE and the right is ACD. We need to analyze this case\nto come up with the required constraints.\n1) Node E is added before D, so cost(ABFE) < cost (ACD), so 3 + L1 + L2 < 1 + 6 ==> L1 + L2 < 4.\n2) Node D is not added via F, so cost(ABFD) > cost(ACD), so 3 + L1 + L3 > 1 + 6 ==> L1 + L3 > 4\n3) Node D is not added via E, so cost(ABFED) > cost(ACD), so 3 + L1 + L2 + 2 > 1 + 6 ==> L1 + L2\n> 2.\n1) and 3) imply L1 + L2 = 3. So the two constraints are: L1 + L2 = 3 AND L1 + L3 > 4.\n7 of 15\n\nProblem 8. Under some conditions, a distance vector protocol finding minimum cost paths suffers from the \"count\nto-infinity\" problem. Indicate True or False for each choice.\nA. The count-to-infinity problem may arise in a distance vector protocol when the network gets disconnected.\nTrue.\nB. The count-to-infinity problem may arise in a distance vector protocol even when the network never gets\ndisconnected.\nFalse.\nC. The \"path vector\" enhancement to a distance vector protocol always enables the protocol to converge\nwithout counting to infinity.\nTrue.\nProblem 9. Ben Bitdiddle has set up a multi-hop wireless network in which he would like to find paths with high\nprobability of packet delivery between any two nodes. His network runs a distance vector protocol similar to what\nyou developed in the pset. In Ben's distance vector (BDV) protocol, each node maintains a metric to every\ndestination that it knows about in the network. The metric is the nodea€TMs estimate of the packet success\nprobability along the path between the node and the destination.\nThe packet success proba bility along a link or path is defined as 1 minus the packet loss probability along the\ncorresponding link or path. Each node uses the periodic HELLO messages sent by each of its neighbors to estimate\nthe packet loss probability of the link from each neighbor. You may assume that the link loss probabilities are\nsymmetric; i.e., the loss probability of the link from node A to node B is the same as from B to A. Each link L\nmaintains its loss probability in the variable L.lossprob and 0 < L.lossprob < 1.\n8 of 15\n\nA. The key pieces of the Python code for each node's integrate() function in BDV is given below. It has three\nmissing blanks. Please fill them in so that the protocol will eventually converge without routing loops to the\ncorrect metric at each node. The variables are the same as in the pset: self.routes is the dictionary of\nrouting entries (mapping destinations to links), self.getlink(fromnode) returns the link connecting the\nnode self to the node fromnode, and the integrate procedure runs whenever the node receives an\nadvertisement (adv) from node fromnode. As in the pset, adv is a list of (destination, metric) tuples. In the\ncode below, self.metric is a dictionary storing the nodea€TMs current estimate of the routing metric (i.e.,\nthe packet success probability) for each known destination. Please fill in the missing code.\n# Process an advertisement from a neighboring node in BDV\ndef integrate(self, fromnode, adv):\nL = self.getlink(fromnode)\nfor (dest, metric) in adv:\nmy_metric = __________________________________\nif (dest not in self.routes\nor self.metric[dest] _____ my_metric\nor ___________________________________________):\nself.routes[dest] = L\nself.metric[dest] = my_metric\n# rest of integrate() not shown\nFirst blank: (1 - L.lossprob)*metric\nSecond blank: < (<= is also fine since we said that lossprob is strictly > 0)\nThird blank: self.routes[dest] == L\nBen wants to try out a link-state protocol now. During the flooding step, each node sends out a link-state\nadvertisement comprising its address, an incrementing sequence number, and a list of tuples of the form\n(neighbor,lossprob), where the lossprob is the estimated loss probability to the neighbor.\nB. Why does the link-state advertisement include a sequence number?\nTo enable a node to determine whether the advertisement is new or not; only new information should be\nintegrated into the routing table. (This information is also used to decide whether to rebroadcast the\nadvertisement, since we want to rebroadcast an advertisement only once per link.)\nBen would like to reuse, without modification, his implementation of Dijkstra's shortest paths algorithm from the\npset, which takes a map in which the links have non-negative costs and produces a path that minimizes the sum of\nthe costs of the links on the path to each destination.\nC. Ben has to transform the lossprob information from the LSA to produce link costs so that he can use his\nDijkstra implementation without any changes. Which of these transformations will accomplish this goal?\nChoose the BEST answer.\na. Use lossprob as the link cost.\nb. Use -1/log(1-lossprob) as the link cost.\nc. Use log(1/(1-lossprob)) as the link cost.\nd. Use log(1 - lossprob) as the link cost.\nThe correct choice is C. The reason is that maximizing the product of link success probabilities is the same as\n9 of 15\n\nmaximizing the sum of the logs of these probabilities, and that is the same as minimizing the sum of the logs\nof the reciprocals of these probabilities. A is correct only when lossprob << 1, which isn't always the case. D\nis plausible because of the log term, but is negative, so Dijkstra's doesn't work on a network with negative\ncosts with negative-cost loops. B is plausible for the same reason, but is a decreasing function of lossprob,\nand so can't be right.\nProblem 10. We studied a few principles for designing networks in 6.02.\nA. State one significant difference between a circuit-switched and a packet-switched network.\nIn a packet-switched network, packets carry information in the header that tells the switches about the\ndestination. Circuit-switched networks dona€TMt carry any destination information in the data frames.\nThe abstraction provided by a circuit-switched network is that of a dedicated link of a fixed rate; a packet-\nswitched network provides no such guarantee to the communicating end points.\nB. Why does topological addressing enable large networks to be built?\nIt reduces the size of the routing tables and the amount of information that must be exchanged in the routing\nprotocol.\nC. Give one difference between what a switch does in a packet-switched network and a circuit-switched\nnetwork.\nSwitches in a circuit-switched network participate in a connection set-up/teardown protocol, but not in a\npacket-switched network.\nSwitches in a packet-switched network look-up the destination address of a packet during forwarding, but not\nin a circuit-switched network.\nProblem 11. Eager B. Eaver implements distance vector routing in his network in which the links all have arbitrary\npositive costs. In addition, there are at least two paths between any two nodes in the network. One node, u, has an\nerroneous implementation of the integration step: it takes the advertised costs from each neighbor and picks the\nroute corresponding to the minimum advertised cost to each destination as its route to that destination, without\nadding the link cost to the neighbor. It breaks any ties arbitrarily. All the other nodes are implemented correctly.\nLet's use the term \"correct route\" to mean the route that corresponds to the minimum-cost path. Which of the\nfollowing statements are true of Eager's network?\na. Only u may have incorrect routes to any other node.\nb. Only u and u's neighbors may have incorrect routes to any other node.\nc. In some topologies, all nodes may have correct routes.\nd. Even if no HELLO or advertisements packets are lost and no link or node failures occur, a routing loop may\noccur.\nA and B are false, C is true, and D is false.\n10 of 15\n\nA is false because u could propagate an incorrect cost to its neighbors causing the neighbor to have an incorrect\nroute. In fact, u's neighbors could do the same.\nC is correct; a simple example is where the network is a tree, where there is exactly one path between any two\nnodes.\nD is false; no routing loops can occur under the stated condition. We can reason by contradiction. Consider the\nshortest path from any node s to any other node t running the flawed routing protocol. If the path does not traverse\nu, no node on that path can have a loop because distance vector routing without any packet loss or failures is\nloop-free. Now consider the nodes for which the computed paths go through u; all these nodes are correctly\nimplemented except for u, which means the paths between u and each of them is loop-free. Moreover, the path to u\nis itself loop-free because u picks one of its neighbors with smaller cost, and there is no possibility of a loop.\nProblem 12. Consider a network running the link-state routing protocol as described in lecture and on the pset.\nHow many copies of any given LSA are received by a given node in the network?\nWhen there are no packet losses, it is equal to the number of neighbors of the node.\nProblem 13. In implementing Dijkstra's algorithm in the link-state routing protocol at node u, Louis Reasoner first\nsets the route for each directly connected node v to be the link connecting u to v. Louis then implements the rest of\nthe algorithm correctly, aiming to produce minimum-cost routes, but does not change the routes to the directly\nconnected nodes. In this network, u has at least two directly connected nodes, and there is more than one path\nbetween any two nodes. Assume that all link costs are non-negative. Which of the following statements is true of u's\nrouting table?\nA. There are topologies and link costs where the majority of the routes to other nodes will be incorrect.\nTrue. For example, all the neighbors but one could have very high cost, and all the other links have low cost,\nso all the routes could in fact be just one link.\nB. There are topologies and link costs where no routing table entry (other than from u to itself) will be correct.\nFalse. The lowest-cost neighbor's route will be the direct link, of course!\nC. There are topologies and link costs where all routing table entry (other than from u to itself) will be correct.\nTrue. A trivial example is when all the links have equal cost.\nProblem 14. A network with N nodes and N bidirectional links is connected in a ring, and N is an even number.\n11 of 15\n\nThe network runs a distance-vector protocol in which the advertisement step at each node runs when the local time\nis T*i seconds and the integration step runs when the local time is T*i + T/2 seconds, (i=1,2,...). Each\nadvertisement takes time δ to reach a neighbor. Each node has a separate clock and time is not synchronized\nbetween the different nodes.\nSuppose that at some time t after the routing has converged, node N + 1 is inserted into the ring, as shown in the\nfigure above. Assume that there are no other changes in the network topology and no packet losses. Also assume\nthat nodes 1 and N update their routing tables at time t to include node N + 1, and then rely on their next scheduled\nadvertisements to propagate this new information.\nA. What is the minimum time before every node in the network has a route to node N + 1?\nThe key insight to observe is that each introduces a delay of at least T/2 because it takes that long between\nthe integration and advertisement steps. Given this fact, the answer would be\n(N/2 - 1)*(T/2 + δ)\nHowever, there is a small \"fence-post error\" in this argument. As stated in the problem, the nodes labeled 1\nand N update their routing tables at time t to include node N + 1. In the best case, these two nodes could both\nimmediately send out advertisements, and nodes 2 and N-1 could run their integration steps immediately after\nreceiving these advertisements. Because of that, the delay of T/2 only starts applying to the other nodes in the\nnetwork. Hence, the answer is\n(N/2 - 2)*T/2 + (N/2 - 1)*δ\nB. What is the maximum time before every node in the network has a route to node N + 1?\nThe key insight is that it takes in the worst case T + T/2 seconds per hop because each node may get the\ninformation about the new node just after it completes the previous integration step. So it has to wait T for\nthe next integration, and then another T/2 to advertise. The correct answer is\n(N/2 -1)*(3T/2 + δ)\nProblem 15. Louis Reasoner implements the link-state routing protocol discussed in 6.02 on a best-effort network\nwith a non-zero packet loss rate. In an attempt to save bandwidth, instead of sending link-state advertisements\nperiodically, each node sends an advertisement only if one of its links fails or when the cost of one of its links\nchanges. The rest of the protocol remains unchanged. Will Louis' implementation always converge to produce\ncorrect routing tables on all the nodes?\n12 of 15\n\nNo, because the LSA could be lost on all of the links connected to some one node (or more than one node), causing\nthat node to not necessarily have correct routes. In fact this protocol doesn't converge even if packets are not lost.\nConsider a network where the failure of one link disconnects the network into two connected components, each\nwith multiple nodes and links. Suppose the cost of one or more of the links in some component changes. When the\nnetwork heals because the failed link recovers, the previously discon- nected component will not have the correct\nlink costs for one or more links in the other component. However, its routes will still be correct because all paths to\nthe other component go via the failed link. However, if we were to add another link between the two components at\nthis time, the routing would never converge correctly.\nProblem 16. Consider a network implementing minimum-cost routing using the distance-vector protocol. A node,\nS, has k neighbors, numbered 1 through k, with link cost c_i to neighbor i (all links have symmetric costs). Initially,\nS has no route for destination D. Then, S hears advertisements for D from each neighbor, with neighbor i advertising\na cost of p_i. The node integrates these k advertisements. What is the cost for destination D in S's routing table after\nthe integration?\nThis question asks for the update rule in the Bellman-Ford integration step. The cost in S's routing table for D should\nbe set to\nmini{c_i + p_i}\nProblem 17. Consider the network shown in the picture below. Each node implements Dijkstra's shortest path\nalgorithm using the link costs shown in the picture.\nA. Initially, node B's routing table contains only one entry, for itself. When B runs Dijkstra's algorithm, in what\norder are nodes added to the routing table? List all possible answers.\nB,C,E,A,F,D and B,C,E,F,A,D.\nB. Now suppose the link cost for one of the links changes but all costs remain non-negative. For each change in\nlink cost listed below, state whether it is possible for the route at node B (i.e., the link used by B) for any\ndestination to change, and if so, name the destination(s) whose routes may change.\na. The cost of link(A, C) increases.\nNo effect. The edge AC is not in any shortest path.\n13 of 15\n\nb. The cost of link(A, C) decreases.\nCan affect route to A. If cost_AC ≤ 3, then we can start using this edge to go to A instead of the edge\nBA.\nc. The cost of link(B, C) increases.\nCan affect route to C,F,E. If cost_BC ≥ 7, then we can use BE-EC to go to C instead of BC. If cost BC\n> 5, then we can use BE-EF to go to F. If cost_BC ≥ 3, can use BE to go to B.\nd. The cost of link(B, C) decreases.\nCan affect route to D. If cost_BC ≤ 1, then we can use BC-CE-ED to go to D instead of BD.\nProblem 18. Alyssa P. Hacker implements the 6.02 distance-vector protocol on the network shown below. Each\nnode has its own local clock, which may not be synchronized with any other node's clock. Each node sends its\ndistance-vector advertisement every 100 seconds. When a node receives an advertisement, it immediately\nintegrates it. The time to send a message on a link and to integrate advertisements is negligible. No advertisements\nare lost. There is no HELLO protocol in this network.\nA. At time 0, all the nodes except D are up and running. At time 10 seconds, node D turns on and immediately\nsends a route advertisement for itself to all its neighbors. What is the minimum time at which each of the\nother nodes is guaranteed to have a correct routing table entry corresponding to a minimum-cost path to\nreach D? Justify your answers.\nNode S: 10 seconds.\nNode A: 110 seconds.\nNode B: 110 seconds.\nNode C: 210 seconds.\nAt time t = 10, D advertises to S, A, and C. They integrate this advertisement into their routing tables, so that\ncost(S,D) = 2, cost(A,D) = 2, cost(C,D) = 7. Note that only Sss route is correct. In the worst case, we wait\n100s for the next round of advertisements. So at time t = 110, S, A, and C all advertise about D, and everyone\nintegrates. Now cost(A, D) = 4 (via S), cost(B, D) = 4 (via S), and cost(C, D) = 7 still. A and B's routes are\ncorrect; C's is not. Finally, after 100 more seconds, another round of advertisements is sent. In particular, C\nhears about B's route to D, and updates cost(C, D) = 5 (via B).\n14 of 15\n\nB. If every node sends packets to destination D, and to no other destination, which link would carry the most\ntraffic?\nS→D. Every node's best route to D is via S.\nAlyssa is unhappy that one of the links in the network carries a large amount of traffic when all the nodes are\nsending packets to D. She decides to overcome this limitation with Alyssa's Vector Protocol (AVP). In AVP, S lies,\nadvertising a \"path cost\" for destination D that is different from the sum of the link costs along the path used to\nreach D. All the other nodes implement the standard distance-vector protocol, not AVP.\nC. What is the smallest numerical value of the cost that S should advertise for D along each of its links, to\nguarantee that only its own traffic for D uses its direct link to D? Assume that all advertised costs are\nintegers; if two path costs are equal, one can't be sure which path will be taken.\n7. S needs to advertise a high enough cost such that everyone's path to D via S will no longer be the best path.\nIn particular, since B's cost to D without going through S is the highest (8), S must advertise a cost so that\nlinkcost(B, S) + advertisedcost(S, D) > 8. Hence, S advertises a cost of 7.\n15 of 15\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 11",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/3a2fbf5386b57011e2410c8ec6b3683e_MIT6_02F12_tutor11.pdf",
      "content": "6.02 Practice Problems: Reliable Data Transport Protocols\nProblem 1. Consider the following chain topology:\nA ---- B ----- C ---- D ---- E\nA is sending packets to E using a reliable transport protocol. Each link above can transmit one packet per\nsecond. There are no queues or other sources of delays at the nodes (except the transmission delay of course).\nA. What is the RTT between A and E?\nB. What is the throughput of a stop-and-wait protocol at A in the absence of any losses at the nodes?\nC. If A decides to run a sliding window protocol, what is the optimum window size it must use? What is\nthe throughput achieved when using this optimum window size?\nD. Suppose A is running a sliding window protocol with a window size of four. In the absence of any\nlosses, what is the throughput at E? What is the utilization of link B-C?\nE. Consider a sliding window protocol running at the optimum window size found in part 3 above.\nSuppose nodes in the network get infected by a virus that causes them to drop packets when odd\nsequence numbers. The sliding window protocol starts numbering packets from sequence number 1.\nAssume that the sender uses a timeout of 40 seconds. The receiver buffers out-of-order packets until it\ncan deliver them in order to the application. What is the number of packets in this buffer 35 seconds\nafter the sender starts sending the first packet?\nProblem 2. Ben Bitdiddle implements a reliable data transport protocol intended to provide \"exactly once\"\nsemantics. Each packet has an 8-bit incrementing sequence number, starting at 0. As the connection\nprogresses, the sender \"wraps around\" the sequence number once it reaches 255, going back to 0 and\nincrementing it for successive packets. Each packet size is S = 1000 bytes long (including all packet headers).\nSuppose the link capacity between sender and receiver is C = 1 Mbyte per second and the round-trip time is\nR = 100 milliseconds.\nA. What is the highest throughput achievable if Ben's implementation is stop-and-wait?\nB. To improve performance, Ben implements a sliding window protocol. Assuming no packet losses, what\n1 of 7\n\nshould Ben set the window size to in order to saturate the link capacity?\nC. Ben runs his protocol on increasingly higher-speed bottleneck links. At a certain link speed, he finds\nthat his implementation stops working properly. Can you explain what might be happening? What\nthreshold link speed causes this protocol to stop functioning properly?\nProblem 3. A sender S and receiver R are connected over a network that has k links that can each lose\npackets. Link i has a packet loss rate of p_i in one direction (on the path from S to R) and q_i in the other (on\nthe path from R to S). Assume that each packet on a link is received or lost independent of other packets, and\nthat each packet's loss probability is the same as any other's (i.e., the random process causing packet losses is\nindepedent and identically distributed).\nA. Suppose that the probability that a data packet does not reach R when sent by S is p and the probability\nthat an ACK packet sent by R does not reach S is q. Write expressions for p and q in terms of the p_i's\nand q_i's.\nB. If all p's are equal to some value α << 1 (much smaller than 1), then what is p (defined above)\napproximately equal to?\nC. Suppose S and R use a stop-and-wait protocol to communicate. What is the expected number of\ntransmissions of a packet before S can send the next packet in sequence? Write your answer in terms of\np and q (both defined above).\nProblem 4. Consider a 40 kbit/s network link connecting the earth to the moon. The moon is about 1.5 light-\nseconds from earth.\nA. 1 Kbyte packets are sent over this link using a stop-and-wait protocol for reliable delivery, what data\ntransfer rate can be achieved? What is the utilization of the link?\nB. If a sliding-window protocol is used instead, what is the smallest window size that achieves the\nmaximum data rate? Assume that error are infrequent. Assume that the window size is set to achieve\nthe maximum data transfer rate.\nC. Consider a sliding-window protocol for this link with a window size of 10 packets. If the receiver has a\nbuffer for only 30 undelivered packets (the receiver discards packets it has no room for, and sends no\nACK for discarded packets), how bits of sequence number are needed?\n2 of 7\n\nProblem 5. Consider a best-effort network with variable delays and losses. Here, Louis Reasoner suggests\nthat the receiver does not need to send the sequence number in the ACK in a correctly implemented\nstop-and-wait protocol, where the sender sends packet k+1 only after the ACK for packet k is received.\nExplain whether he is correct or not.\nProblem 6. Consider a sliding window protocol between a sender and a receiver. The receiver should deliver\npackets reliably and in order to its application.\nThe sender correctly maintains the following state variables:\nunacked_pkts -- the buffer of unacknowledged packets\nfirst_unacked -- the lowest unacked sequence number (undefined if all packets have been acked)\nlast_unacked -- the highest unacked sequence number (undefined if all packets have been acked)\nlast_sent -- the highest sequence number sent so far (whether acknowledged or not)\nIf the receiver gets a packet that is strictly larger than the next one in sequence, it adds the packet to a buffer\nif not already present. We want to ensure that the size of this buffer of packets awaiting delivery never\nexceeds a value W ≥ 0. Write down the check(s) that the sender should perform before sending a new packet\nin terms of the variables mentioned above that ensure the desired property.\nProblem 7. Ben decides to use the sliding window transport protocol we studied in 6.02 and implemented in\nthe pset on the network below. The receiver sends end-to-end ACKs to the sender. The switch in the middle\nsimply forwards packets in best-effort fashion.\nA. The sender's window size is 10 packets. Selecting the best answer from the choices below, at what\napproximate rate (in packets per second) will the protocol deliver a multi-gigabyte file from the sender\n3 of 7\n\nto the receiver? Assume that there is no other traffic in the network and packets can only be lost\nbecause the queues overflow.\na. Between 900 and 1000.\nb. Between 450 and 500.\nc. Between 225 and 250.\nd. Depends on the timeout value used.\nB. You would like to double the throughput of this sliding window transport protocol running on the\nnetwork shown on the previous page. To do so, you can apply one of the following techniques alone:\na. Double the window size.\nb. Halve the propagation time of the links.\nc. Double the speed of the link between the Switch and Receiver.\nFor each of the following sender window sizes, list which of the above techniques, if any, can\napproximately double the throughput. If no technique does the job, answer \"None\". There might be\nmore than one answer for each window size, in which case you should list them all. Note that each\ntechnique works in isolation. Explain your answers.\n1. W = 10.\n2. W = 50.\n3. W = 30.\nProblem 8. Consider the sliding window protocol described in lecture and implemented in the pset. The\nreceiver sends \"ACK k\" when it receives a packet with sequence number k. Denote the window size by W.\nThe sender's packets start with sequence number 1. Which of the following is true of a correct\nimplementation of this protocol over a best-effort network?\nA. Any new (i.e., previously unsent) packet with sequence number greater than W is sent by the sender if,\nand only if, a new (i.e., previously unseen) ACK arrives.\nB. The sender will never send more than one packet between the receipt of one ACK and the next.\nC. The receiver can discard any new, out-of-order packet it receives after sending an ACK for it.\nD. Suppose that no packets or ACKs are lost and no packets are ever retransmitted. Then ACKs will arrive\nat the sender in non-decreasing order.\n4 of 7\n\nE. The sender should retransmit any packet for which it receives a duplicate ACK (i.e., an ACK it has\nreceived earlier).\nProblem 9. In his haste in writing the code for the exponential weighted moving average (EWMA) to\nestimate the smoothed round-trip time, srtt, Ben Bitdiddle writes\nsrtt = alpha * r + alpha * srtt\nwhere r is the round-trip time (RTT) sample, and 0 < alpha < 1.\nFor what values of alpha does this buggy EWMA over-estimate the intended srtt? You may answer this\nquestion assuming any convenient non-zero sequence of RTT samples, r.\nProblem 10. A sender S and receiver R communicate reliably over a series of links using a sliding window\nprotocol with some window size, W packets. The path between S and R has one bottleneck link (i.e., one link\nwhose rate bounds the throughput that can be achieved), whose data rate is C packets/second. When the\nwindow size is W, the queue at the bottleneck link is always full, with Q data packets in it. The round trip time\n(RTT) of the connection between S and R during this data transfer with window size W is T seconds. There\nare no packet or ACK losses in this case, and there are no other connections sharing this path.\nA. Write an expression for W in terms of the other parameters specified above.\nB. We would like to reduce the window size from W and still achieve high utilization. What is the\nminimum window size, Wmin, which will achieve 100% utilization of the bottleneck link? Express your\nanswer as a function of C, T, and Q.\nC. Now suppose the sender starts with a window size set to Wmin. If all these packets get ac- knowledged\nand no packet losses occur in the window, the sender increases the window size by 1. The sender keeps\nincreasing the window size in this fashion until it reaches a window size that causes a packet loss to\noccur. What is the smallest window size at which the sender observes a packet loss caused by the\nbottleneck queue overflowing? Assume that no ACKs are lost.\nProblem 11. A sender A and a receiver B communicate using the stop-and-wait protocol studied in 6.02.\nThere are n links on the path between A and B, each with a data rate of R bits per second. The size of a data\npacket is S bits and the size of an ACK is K bits. Each link has a physical distance of D meters and the speed\nof signal propagation over each link is c meters per second. The total processing time experienced by a data\npacket and its ACK is T_p seconds. ACKs traverse the same links as data packets, except in the opposite\ndirection on each link (the propagation time and data rate are the same in both directions of a link). There is\n5 of 7\n\nno queueing delay in this network. Each link has a packet loss probability of p, with packets being lost\nindependently.\nWhat are the following four quantities in terms of the given parameters?\nA. Transmission time for a data packet on one link between A and B.\nB. Propagation time for a data packet across n links between A and B.\nC. Round-trip time (RTT) between A and B?. (The RTT is defined as the elapsed time between the start of\ntransmission of a data packet and the completion of receipt of the ACK sent in response to the data\npacketa€TMs reception by the receiver.)\nD. Probability that a data packet sent by A will reach B.\nProblem 12.\nOpt E. Miser implements the 6.02 stop-and-wait reliable transport protocol with one modification: being\nstingy, he replaces the sequence number field with a 1-bit field, deciding to reuse sequence numbers across\ndata packets. The first data packet has sequence number 1, the second has number 0, the third has number 1,\nthe fourth has number 0, and so on. Whenever the receiver gets a packet with sequence number s (= 0 or 1),\nit sends an ACK to the sender echoing s. The receiver delivers a data packet to the application if, and only if,\nits sequence number is different from the last one delivered, and upon delivery, updates the last sequence\nnumber delivered.\nD. He runs this protocol over a best-effort network that can lose packets (with probability less than 1) or\nreorder them, and whose delays may be variable. Does the modified protocol always provide correct\nreliable, in-order delivery of a stream of packets?\nProblem 13. Consider a reliable transport connection using the 6.02 sliding window protocol on a network\npath whose RTT in the absence of queueing is RTTmin = 0.1 seconds. The connection's bottleneck link has a\nrate of C = 100 packets per second, and the queue in front of the bottleneck link has space for Q = 20\npackets.\nAssume that the sender uses a sliding window protocol with fixed window size. There is no other connection\non the path.\nA. If the size of the window is 8 packets, then what is the throughput of the connection?\nB. If the size of the window is 16 packets, then what is the throughput of the connection?\n6 of 7\n\nC. What is the smallest window size for which the connection's RTT exceeds RTTmin?\nProblem 14. TCP, the standard reliable transport protocol used on the Internet, uses a sliding window. Unlike\nthe protocol studied in 6.02, however, the size of the TCP window is variable. The sender changes the size of\nthe window as ACKs arrive from the receiver; it does not know the best window size to use a priori.\nTCP uses a scheme called slow start at the beginning of a new connection. Slow start has three rules, R1, R2,\nand R3, listed below (TCP uses some other rules too, which we will ignore).\nIn the following rules for slow start, the sender's current window size is W and the last in-order ACK received\nby the sender is A. The first packet sent has sequence number 1.\nR1. Initially, set W ← 1 and A ← 0.\nR2. If an ACK arrives for packet A+1, then set W←W+1, and set A←A+1.\nR3. When the sender retransmits a packet after a timeout, then set W←1.\nAssume that all the other mechanisms are the same as the 6.02 sliding window protocol. Data packets may be\nlost because packet queues overflow, but assume that packets are not reordered by the network.\nWe run slow start on a network with RTTmin = 0.1 seconds, bottleneck link rate = 100 packets per second,\nand bottleneck queue = 20 packets.\nA. What is the smallest value of W at which the bottleneck queue overflows?\nB. Sketch W as a function of time for the first 5 RTTs of a connection. The X-axis marks time in terms of\nmultiples of the connection's RTT. (Hint: Non-linear!)\n7 of 7\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 11 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/5a967cba3213d84dba9b0fde2e7246b8_MIT6_02F12_tutor11_sol.pdf",
      "content": "6.02 Practice Problems: Reliable Data Transport Protocols\nProblem 1. Consider the following chain topology:\nA ---- B ----- C ---- D ---- E\nA is sending packets to E using a reliable transport protocol. Each link above can transmit one packet per\nsecond. There are no queues or other sources of delays at the nodes (except the transmission delay of course).\nA. What is the RTT between A and E?\n8 seconds\nB. What is the throughput of a stop-and-wait protocol at A in the absence of any losses at the nodes?\n1/8 pkts/s\nC. If A decides to run a sliding window protocol, what is the optimum window size it must use? What is\nthe throughput achieved when using this optimum window size?\noptimum window = 8 pkts, optimum throughput = 1 pkt/s\nD. Suppose A is running a sliding window protocol with a window size of four. In the absence of any\nlosses, what is the throughput at E? What is the utilization of link B-C?\nthroughput=0.5 pkts/s, utilization=0.5\nE. Consider a sliding window protocol running at the optimum window size found in part 3 above.\nSuppose nodes in the network get infected by a virus that causes them to drop packets when odd\nsequence numbers. The sliding window protocol starts numbering packets from sequence number 1.\nAssume that the sender uses a timeout of 40 seconds. The receiver buffers out-of-order packets until it\ncan deliver them in order to the application. What is the number of packets in this buffer 35 seconds\nafter the sender starts sending the first packet?\nAnswer: 7.\nWith a window size of 8, the sender sends out packets 1--8 in the first 8 seconds. But it gets back only\n4 ACKs because packets 1,3,5,7 are dropped. Therefore, the sender transmits 4 more packets (9--12) in\nthe next 8 seconds, 2 packets (13--14) in the next 8 seconds, and 1 (sequence number 15) packet in the\nnext 8 seconds. Note that 32 seconds have elapsed so far. Now the sender gets no more ACKs because\n1 of 12\n\npacket 15 is dropped, and it stalls till the first packet times out at time step 40. Therefore, at time 35,\nthe sender would have transmitted 15 packets, 7 of which would have reached the receiver. But\nbecause all of these packets are out of order, the receiver's buffer would have 7 packets.\nProblem 2. Ben Bitdiddle implements a reliable data transport protocol intended to provide \"exactly once\"\nsemantics. Each packet has an 8-bit incrementing sequence number, starting at 0. As the connection\nprogresses, the sender \"wraps around\" the sequence number once it reaches 255, going back to 0 and\nincrementing it for successive packets. Each packet size is S = 1000 bytes long (including all packet headers).\nSuppose the link capacity between sender and receiver is C = 1 Mbyte per second and the round-trip time is\nR = 100 milliseconds.\nA. What is the highest throughput achievable if Ben's implementation is stop-and-wait?\nThe highest throughput for stop-and-wait is (1000 bytes)/(100ms) = 10 Kbytes/s.\nB. To improve performance, Ben implements a sliding window protocol. Assuming no packet losses, what\nshould Ben set the window size to in order to saturate the link capacity?\nSet the window size to the bandwidth-delay product of the link, 1 Mbyte/s * 0.1 s = 100 Kbytes.\nC. Ben runs his protocol on increasingly higher-speed bottleneck links. At a certain link speed, he finds\nthat his implementation stops working properly. Can you explain what might be happening? What\nthreshold link speed causes this protocol to stop functioning properly?\nSequence number wraparound causes his protocol to stop functioning properly. When this happens, two\npackets with different content but the same sequence number are in-flight at once, and so an ack for\nthe firstt spuriously acknowledges the second as well, possibly causing data loss in Ben's \"reliable\"\nprotocol. This happens when\n(255 packets)(1000 bytes/1 packet) = (C bytes/s)(0.1 s)\nTherefore C = 2.55 Mbytes/s.\nProblem 3. A sender S and receiver R are connected over a network that has k links that can each lose\npackets. Link i has a packet loss rate of p_i in one direction (on the path from S to R) and q_i in the other (on\nthe path from R to S). Assume that each packet on a link is received or lost independent of other packets, and\nthat each packet's loss probability is the same as any other's (i.e., the random process causing packet losses is\nindepedent and identically distributed).\nA. Suppose that the probability that a data packet does not reach R when sent by S is p and the probability\nthat an ACK packet sent by R does not reach S is q. Write expressions for p and q in terms of the p_i's\nand q_i's.\n2 of 12\n\np = 1 - (1 - p_1)(1 - p_2)...(1 - p_k) and\nq = 1 - (1 - q_1)(1 - q_2)...(1 - q_k)\nB. If all p's are equal to some value α << 1 (much smaller than 1), then what is p (defined above)\napproximately equal to?\np = 1 - (1 - &alpha)k ≈1 - (1 - kα) = kα\nC. Suppose S and R use a stop-and-wait protocol to communicate. What is the expected number of\ntransmissions of a packet before S can send the next packet in sequence? Write your answer in terms of\np and q (both defined above).\nThe probability of a packet reception from S to R is 1-p and the probability of an ACK reaching S given\nthat R sent an ACK is 1-q. The sender moves from sequence number k to k + 1 if the packet reaches\nand the ACK arrives. That happens with probability (1-p)(1-q). The expected number of transmissions\nfor such an event is therefore equal to 1/((1-p)(1-q)).\nProblem 4. Consider a 40 kbit/s network link connecting the earth to the moon. The moon is about 1.5 light-\nseconds from earth.\nA. 1 Kbyte packets are sent over this link using a stop-and-wait protocol for reliable delivery, what data\ntransfer rate can be achieved? What is the utilization of the link?\nStop-and-wait sends 1 packet per round-trip-time so the data transfer rate is 1 Kbyte/3 seconds = 333\nbytes/s = 2.6 Kbit/s. The utilization is 2.6/40 = 6.5%.\nThe estimate above omits the transmission time of the packet. If we include the transmission time (8\nkbit/(40 kbit/s) = 0.2 s), the result is 1 kbyte/3.2 seconds = 312 bytes/s = 2.5 Kbits/s.\nB. If a sliding-window protocol is used instead, what is the smallest window size that achieves the\nmaximum data rate? Assume that error are infrequent. Assume that the window size is set to achieve\nthe maximum data transfer rate.\nAchieving full rate requires a send window of at least\nbandwidth-delay product = 5 packets/s * 3 s = 15 packets.\nC. Consider a sliding-window protocol for this link with a window size of 10 packets. If the receiver has a\nbuffer for only 30 undelivered packets (the receiver discards packets it has no room for, and sends no\nACK for discarded packets), how bits of sequence number are needed?\nThe window size determines the number of unacknowledged packets the transmitter will send before\n3 of 12\n\nstalling, but there can be arbitrarily many acknowledged but undelivered (because of one lost packet)\npackets at the receiver. But if only 30 packets are held at the receiver, after which it stops\nacknowledging packets except the one it's waiting for, the total number of packets in transit or sitting in\nthe receivers buffer is at most 40.\nSo a 6-bit sequence number will be sufficent to ensure that all unack'ed and undelivered packets have a\nunique sequence number (avoiding the sequence number wrap-around problem).\nProblem 5. Consider a best-effort network with variable delays and losses. Here, Louis Reasoner suggests\nthat the receiver does not need to send the sequence number in the ACK in a correctly implemented\nstop-and-wait protocol, where the sender sends packet k+1 only after the ACK for packet k is received.\nExplain whether he is correct or not.\n(Not surprisingly,) Louis is wrong. Imagine that the sender sends packet k and then retransmits k. However,\nthe original transmission and the retransmission get through to the receiver. The receiver sends an ACK for k\nwhen it gets the original transmission, and in response the sender sends packet k+1. Now, when the sender\ngets an ACK, it cannot tell whether the ACK was for packet k (the retransmission), or for packet k+1!\nProblem 6. Consider a sliding window protocol between a sender and a receiver. The receiver should deliver\npackets reliably and in order to its application.\nThe sender correctly maintains the following state variables:\nunacked_pkts -- the buffer of unacknowledged packets\nfirst_unacked -- the lowest unacked sequence number (undefined if all packets have been acked)\nlast_unacked -- the highest unacked sequence number (undefined if all packets have been acked)\nlast_sent -- the highest sequence number sent so far (whether acknowledged or not)\nIf the receiver gets a packet that is strictly larger than the next one in sequence, it adds the packet to a buffer\nif not already present. We want to ensure that the size of this buffer of packets awaiting delivery never\nexceeds a value W ≥ 0. Write down the check(s) that the sender should perform before sending a new packet\nin terms of the variables mentioned above that ensure the desired property.\nThe largest sequence number that a receiver could have possibly received is last_sent. The size of the\nreceiver buffer can become as large as last_sent - first_unacked. One might think that we need to add 1\nto this quantity, but observe that the only reason any packets get added to the buffer is when some packet is\nlost (i.e., at least one of the packets in the sender's unacked buffer must have been lost).\nWe also need to handle the case when all the packets sent by the sender have been acknowledged -- clearly,\nin this case, the sender should be able to send data.\nHence, if the sender sends a new packet only if\nif len(unacked_packets) == 0 or last_sent - first_unacked < W\n4 of 12\n\nthe desired requirement is satisfied.\nProblem 7. Ben decides to use the sliding window transport protocol we studied in 6.02 and implemented in\nthe pset on the network below. The receiver sends end-to-end ACKs to the sender. The switch in the middle\nsimply forwards packets in best-effort fashion.\nA. The sender's window size is 10 packets. Selecting the best answer from the choices below, at what\napproximate rate (in packets per second) will the protocol deliver a multi-gigabyte file from the sender\nto the receiver? Assume that there is no other traffic in the network and packets can only be lost\nbecause the queues overflow.\na. Between 900 and 1000.\nb. Between 450 and 500.\nc. Between 225 and 250.\nd. Depends on the timeout value used.\nChoice b. The RTT, which is the time taken for a single packet to reach the receiver and the ACK to\nreturn, is about 20 milliseconds plus the transmission time, which is about 1 millisecond (1000 bytes at\na rate of 1 Megabyte/s). Hence, the throughput is 10 packets / 21 milliseconds = 476 packets per\nsecond. If one ignored the transmission time, which is perfectly fine given the set of choices, one would\nestimate the throughput to be about 500 packets per second.\nB. You would like to double the throughput of this sliding window transport protocol running on the\nnetwork shown on the previous page. To do so, you can apply one of the following techniques alone:\na. Double the window size.\nb. Halve the propagation time of the links.\nc. Double the speed of the link between the Switch and Receiver.\nFor each of the following sender window sizes, list which of the above techniques, if any, can\napproximately double the throughput. If no technique does the job, answer \"None\". There might be\nmore than one answer for each window size, in which case you should list them all. Note that each\ntechnique works in isolation. Explain your answers.\n1. W = 10.\n5 of 12\n\nA and B.\nWhen W = 10, the throughput is about 476 packets/s. If we double the window size, throughput\nwould double to 952 packet/s. If we reduce the propagation time of the links, throughput would\nroughly double as well. The new throughput would still be smaller than the bottleneck capacity of\n1000 packets/s.\n2. W = 50.\nC.\nWhen W = 50, throughput is already 1000 packets/s. At this stage, doubling the window or\nhalving the RTT does not increase the throughput. If we double the speed of the link between the\nSwitch and Receiver, the bottleneck becomes 2000 packets/s. A window size of 50 packets over\nan RTT of 20 or 21 milliseconds has a throughput of more than 2000 packet/s. Hence, doubling\nthe bottleneck link speed will double the throughput when W = 50 packets. With a queue size of\n30 packets and a window size of 50, the initial window of packets sent back-to-back would\nindeed cause the queue to overflow. However, that doesn't cause the throughput to drop in the\nsteady state, so for a long data transfer, the throughput will be as described above.\n3. W = 30.\nNone.\nWhen W = 30, throughput is already 1000 packets/s. Now, if we double the window or halve the\nRTT, the throughput won't change. An interesting situation occurs when we double the link\nspeed, because the bottleneck link would now be capable of delivering 2000 packets/s. But our\nwindow size is 30 and RTT about 20 milliseconds, giving a throughput of about 1500 packets/s\n(or if we use 21 milliseconds, we get 1428 packet/s). That's an improvement of about 50%, far\nfrom the doubling we wanted. None of the techniques work.\nProblem 8. Consider the sliding window protocol described in lecture and implemented in the pset. The\nreceiver sends \"ACK k\" when it receives a packet with sequence number k. Denote the window size by W.\nThe sender's packets start with sequence number 1. Which of the following is true of a correct\nimplementation of this protocol over a best-effort network?\nA. Any new (i.e., previously unsent) packet with sequence number greater than W is sent by the sender if,\nand only if, a new (i.e., previously unseen) ACK arrives.\nTrue.\nB. The sender will never send more than one packet between the receipt of one ACK and the next.\nFalse. The sender could time-out and retransmit.\n6 of 12\n\nC. The receiver can discard any new, out-of-order packet it receives after sending an ACK for it.\nFalse. The sender thinks the receiver has delivered this packet to the application.\nD. Suppose that no packets or ACKs are lost and no packets are ever retransmitted. Then ACKs will arrive\nat the sender in non-decreasing order.\nFalse. Packets or ACKs could get reordered in the network.\nE. The sender should retransmit any packet for which it receives a duplicate ACK (i.e., an ACK it has\nreceived earlier).\nFalse. Duplicate ACKs can be ignored by the sender.\nProblem 9. In his haste in writing the code for the exponential weighted moving average (EWMA) to\nestimate the smoothed round-trip time, srtt, Ben Bitdiddle writes\nsrtt = alpha * r + alpha * srtt\nwhere r is the round-trip time (RTT) sample, and 0 < alpha < 1.\nFor what values of alpha does this buggy EWMA over-estimate the intended srtt? You may answer this\nquestion assuming any convenient non-zero sequence of RTT samples, r.\nThis buggy EWMA over-estimates the true srtt when alpha > 0.5. The true srtt is equal to\nalpha * r + (1-alpha) * srtt\nSo\nalpha * r + alpha * srtt > alpha * r + (1-alpha) * srtt\nimplies alpha > 0.5.\nProblem 10. A sender S and receiver R communicate reliably over a series of links using a sliding window\nprotocol with some window size, W packets. The path between S and R has one bottleneck link (i.e., one link\nwhose rate bounds the throughput that can be achieved), whose data rate is C packets/second. When the\nwindow size is W, the queue at the bottleneck link is always full, with Q data packets in it. The round trip time\n(RTT) of the connection between S and R during this data transfer with window size W is T seconds. There\nare no packet or ACK losses in this case, and there are no other connections sharing this path.\nA. Write an expression for W in terms of the other parameters specified above.\nW = C*T. Note that some students may interpret T as the RTT without any queueing. That's wrong, but\n7 of 12\n\nwe ought to still give them credit as long as they have consistently made this mistake in all the parts.\nWith this interpretation, W = CT + Q.\nB. We would like to reduce the window size from W and still achieve high utilization. What is the\nminimum window size, Wmin, which will achieve 100% utilization of the bottleneck link? Express your\nanswer as a function of C, T, and Q.\nClearly, W = Wmin + Q, where Wmin is the smallest window size that gives 100% utilization. A\nsmaller window than that would keep the network idle some fraction of the time. Hence, Wmin = C*T\nQ.\nWith the flawed interpretation of T, Wmin = C*T.\nC. Now suppose the sender starts with a window size set to Wmin. If all these packets get ac- knowledged\nand no packet losses occur in the window, the sender increases the window size by 1. The sender keeps\nincreasing the window size in this fashion until it reaches a window size that causes a packet loss to\noccur. What is the smallest window size at which the sender observes a packet loss caused by the\nbottleneck queue overflowing? Assume that no ACKs are lost.\nPackets start getting dropped when the window size is W+1, i.e., when it is equal to C*T+1.\nWith the flawed interpretation of T, the window size at which packets start being dropped is W+1 =\nC*T + Q + 1.\nProblem 11. A sender A and a receiver B communicate using the stop-and-wait protocol studied in 6.02.\nThere are n links on the path between A and B, each with a data rate of R bits per second. The size of a data\npacket is S bits and the size of an ACK is K bits. Each link has a physical distance of D meters and the speed\nof signal propagation over each link is c meters per second. The total processing time experienced by a data\npacket and its ACK is T_p seconds. ACKs traverse the same links as data packets, except in the opposite\ndirection on each link (the propagation time and data rate are the same in both directions of a link). There is\nno queueing delay in this network. Each link has a packet loss probability of p, with packets being lost\nindependently.\nWhat are the following four quantities in terms of the given parameters?\nA. Transmission time for a data packet on one link between A and B.\nS/R. Each data packet has size S bits, and the speed of the link is R bits per second.\nB. Propagation time for a data packet across n links between A and B.\nnD/C . Total distance to be travelled is nD since each link has length D meters, and there are n such\nlinks. The propagation speed is C meters/second.\n8 of 12\n\nC. Round-trip time (RTT) between A and B?. (The RTT is defined as the elapsed time between the start of\ntransmission of a data packet and the completion of receipt of the ACK sent in response to the data\npacketa€TMs reception by the receiver.)\nnS/R + nK/r + 2nD/C + T_p\nWe need to consider the following times:\nTransmit data across n links: nS/R using result from part A.\nTransmit ACK across n links: nK/R also using result from part A.\nPropagate data across n links and ACKS across n links: 2nD/C\nTotal time to process the data and the ACK: T_p\nD. Probability that a data packet sent by A will reach B.\n(1- p)n. Probability of loss in a link is p, so probability of no loss in a link is 1-p. Since link losses are\nindependent, probability of no loss in n links is (1-p)n . No loss in n links means the data gets from A to\nB successfuly.\nProblem 12.\nOpt E. Miser implements the 6.02 stop-and-wait reliable transport protocol with one modification: being\nstingy, he replaces the sequence number field with a 1-bit field, deciding to reuse sequence numbers across\ndata packets. The first data packet has sequence number 1, the second has number 0, the third has number 1,\nthe fourth has number 0, and so on. Whenever the receiver gets a packet with sequence number s (= 0 or 1),\nit sends an ACK to the sender echoing s. The receiver delivers a data packet to the application if, and only if,\nits sequence number is different from the last one delivered, and upon delivery, updates the last sequence\nnumber delivered.\nD. He runs this protocol over a best-effort network that can lose packets (with probability less than 1) or\nreorder them, and whose delays may be variable. Does the modified protocol always provide correct\nreliable, in-order delivery of a stream of packets?\nNo. For example, see the picture below.\n9 of 12\n\nProblem 13. Consider a reliable transport connection using the 6.02 sliding window protocol on a network\npath whose RTT in the absence of queueing is RTTmin = 0.1 seconds. The connection's bottleneck link has a\nrate of C = 100 packets per second, and the queue in front of the bottleneck link has space for Q = 20\npackets.\nAssume that the sender uses a sliding window protocol with fixed window size. There is no other connection\non the path.\nA. If the size of the window is 8 packets, then what is the throughput of the connection?\nThe bandwidth-delay product of the connection is 10 packets (bottleneck rate times the minimum\nRTT). With a window size of 8, queues will not yet have built up, so the throughput is 80\npackets/second.\nB. If the size of the window is 16 packets, then what is the throughput of the connection?\nThe bandwidth-delay product of the network is 10 packets, so if W >= 10, there will be 10 packets in\nflight. With W=16, 6 of these packets will be in the queue. The queuing delay will be 6/100 = 0.06\nseconds. Then RTT = RTTmin + queuing delay = .1 + .06 = 0.16 and the throughput is W/RTT = 16/.16\n= 100 pkts/s.\nC. What is the smallest window size for which the connection's RTT exceeds RTTmin?\n10 of 12\n\n11 packets. The bandwidth-delay product is 10 packets. It's probably reasonable to accept an answer of\n10 packets too.\nProblem 14. TCP, the standard reliable transport protocol used on the Internet, uses a sliding window. Unlike\nthe protocol studied in 6.02, however, the size of the TCP window is variable. The sender changes the size of\nthe window as ACKs arrive from the receiver; it does not know the best window size to use a priori.\nTCP uses a scheme called slow start at the beginning of a new connection. Slow start has three rules, R1, R2,\nand R3, listed below (TCP uses some other rules too, which we will ignore).\nIn the following rules for slow start, the sender's current window size is W and the last in-order ACK received\nby the sender is A. The first packet sent has sequence number 1.\nR1. Initially, set W ← 1 and A ← 0.\nR2. If an ACK arrives for packet A+1, then set W←W+1, and set A←A+1.\nR3. When the sender retransmits a packet after a timeout, then set W←1.\nAssume that all the other mechanisms are the same as the 6.02 sliding window protocol. Data packets may be\nlost because packet queues overflow, but assume that packets are not reordered by the network.\nWe run slow start on a network with RTTmin = 0.1 seconds, bottleneck link rate = 100 packets per second,\nand bottleneck queue = 20 packets.\nA. What is the smallest value of W at which the bottleneck queue overflows?\nThe smallest W for which the queue overflows is 10 + 20 + 1 = 31 packets. The 10 is because that's the\nbandwidth-delay product; the 20 is the maximum size of the queue. And we need one more packet to\ncause an overflow.\nB. Sketch W as a function of time for the first 5 RTTs of a connection. The X-axis marks time in terms of\nmultiples of the connection's RTT. (Hint: Non-linear!)\n11 of 12\n\n12 of 12\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/44ee077a1513bb24f3c026fa4986ea03_MIT6_02F12_tutor02.pdf",
      "content": "6.02 Practice Problems: Error Correcting Codes\nProblem 1. For each of the following sets of codewords, please give the appropriate (n,k,d) designation\nwhere n is number of bits in each codeword, k is the number of message bits transmitted by each code word\nand d is the minimum Hamming distance between codewords. Also give the code rate.\nA. {111, 100, 001, 010}\nB. {00000, 01111, 10100, 11011}\nC. {00000}\nProblem 2. Suppose management has decided to use 20-bit data blocks in the company's new (n,20,3) error\ncorrecting code. What's the minimum value of n that will permit the code to be used for single bit error\ncorrection?\nProblem 3. The Registrar has asked for an encoding of class year (\"Freshman\", \"Sophomore\", \"Junior\",\n\"Senior\") that will allow single error correction. Please give an appropriate 5-bit binary encoding for each of\nthe four years.\nProblem 4. For any block code with minimum Hamming distance at least 2t + 1 between code words, show\nthat:\nProblem 5. Pairwise Communications has developed a block code with three data (D1, D2, D3) and three\nparity bits (P1, P2, P3):\nP1 = D1 + D2\nP2 = D2 + D3\nP3 = D3 + D1\n1 of 8\n\n-------------------------\nA. What is the (n,k,d) designation for this code.\nB. The receiver computes three syndrome bits from the (possibly corrupted) received data and parity bits:\nE1 = D1 + D2 + P1\nE2 = D2 + D3 + P2\nE3 = D3 + D1 + P3.\nThe receiver performs maximum likelihood decoding using the syndrome bits. For the combinations of\nsyndrome bits listed below, state what the maximum-likelihood decoder believes has occured: no errors,\na single error in a specii¬c bit (state which one), or multiple errors.\nE1 E2 E3 = 000\nE1 E2 E3 = 010\nE1 E2 E3 = 101\nE1 E2 E3 = 111\nProblem 6. Dos Equis Encodings, Inc. specializes in codes that use 20-bit transmit blocks. They are trying to\ndesign a (20, 16) linear block code for single error correction. Explain whether they are likely to succeed or\nnot.\nProblem 7. Consider the following (n,k,d) block code:\nD0 D1 D2 D3 D4\n| P0\nD5 D6 D7 D8 D9\n| P1\nD10 D11 D12 D13 D14 | P2\nP3 P4 P5 P6 P7\n|\nwhere D0-D14 are data bits, P0-P2 are row parity bits and P3-P7 are column parity bits. The transmitted code\nword will be:\nD0 D1 D2 ... D13 D14 P0 P1 ... P6 P7\nA. Please give the values for n, k, d for the code above.\nB. If D0 D1 D2 ... D13 D14 = 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, please compute P0 through P7.\nC. Now we receive the four following code words:\nM1: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 0 0 0 1 1 0 1 0\nM2: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 0 0 1 1 1 0 1 0\nM3: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 1 1 0 1 1 0 1 0\nM4: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 1 0 0 1 1 0 1 0\nFor each of received code words, indicate the number of errors. If there are errors, indicate if they are\n2 of 8\n\ncorrectable, and if they are, what the correction should be.\nProblem 8. The following matrix shows a rectangular single error correcting code consisting of 9 data bits, 3\nrow parity bits and 3 column parity bits. For each of the examples that follow, please indicate the correction\nthe receiver must perform: give the position of the bit that needs correcting (e.g., D7, R1), or \"no\" if there are\nno errors, or \"M\" if there is a multi-bit uncorrectable error.\nProblem 9. Consider two convolutional coding schemes - I and II. The generator polynomials for the two\nschemes are\nScheme I: G0 = 1101, G1 = 1110\nScheme II: G0 = 110101, G1 = 111011\nNotation is follows: if the generator polynomial is, say, 1101, then the corresponding parity bit for message bit\nn is\n(x[n] + x[n-1] + x[n-3]) mod 2\nwhere x[n] is the message sequence.\nA. Indicate TRUE or FALSE\na. Code rate of Scheme I is 1/4.\nb. Constraint length of Scheme II is 4.\nc. Code rate of Scheme II is equal to code rate of Scheme I.\nd. Constraint length of Scheme I is 4.\nB. How many states will there be in the state diagram for Scheme I? For Scheme II?\nC. Which code will lead to a lower bit error rate? Why?\nD. Alyssa P. Hacker suggests a modification to Scheme I which involves adding a third generator\n3 of 8\n\npolynomial G2 = 1001. What is the code rate r of Alyssa's coding scheme? What about constraint\nlength k? Alyssa claims that her scheme is stronger than Scheme I. Based on your computations for r\nand k, is her statement true?\nProblem 10. Consider a convolution code that uses two generator polynomials: G0 = 111 and G1 = 110. You\nare given a particular snapshot of the decoding trellis used to determine the most likely sequence of states\nvisited by the transmitter while transmitting a particular message:\nA. Complete the Viterbi step, i.e., fill in the question marks in the matrix, assuming a hard branch metric\nbased on the Hamming distance between expected an received parity where the received voltages are\ndigitized using a 0.5V threshold.\nB. Complete the Viterbi step, i.e., fill in the question marks in the matrix, assuming a soft branch metric\nbased on the square of the Euclidean distance between expected an received parity voltages. Note that\nyour branch and path metrics will not necessarily be integers.\nC. Does the soft metric give a different answer than the hard metric? Base your response in terms of the\nrelative ordering of the states in the second column and the survivor paths.\nD. If the transmitted message starts with the bits \"01011\", what is the sequence of bits produced by the\nconvolutional encoder?\nThe receiver determines the most-likely transmitted message by using the Viterbi algorithm to process the\n(possibly corrupted) received parity bits. The path metric trellis generated from a particular set of received\nparity bits is shown below. The boxes in the trellis contain the minimum path metric as computed by the\n4 of 8\n\nViterbi algorithm.\nE. Referring to the trellis above, what is the receiver's estimate of the most-likely transmitter state after\nprocessing the bits received at time step 6?\nF. Referring to the trellis above, show the most-likely path through the trellis by placing a circle around\nthe appropriate state box at each time step and darkening the appropriate arcs. What is the receiver's\nestimate of the most-likely transmitted message?\nG. Referring to the trellis above, and given the receiver's estimate of the most-likely transmitted message,\nat what time step(s) were errors detected by the receiver? Briefly explain your reason- ing.\nH. Now consider the path metric trellis generated from a different set of received parity bits.\n5 of 8\n\nReferring to the trellis above, determine which pair(s) of parity bits could have been been received at\ntime steps 1, 2 and 3. Briefly explain your reasoning.\nProblem 11. Consider a binary convolutional code specified by the generators (1011, 1101, 1111).\nA. What are the values of\na. constraint length of the code\nb. rate of the code\nc. number of states at each time step of the trellis\nd. number of branches transitioning into each state\ne. number of branches transitioning out of each state\nf. number of expected parity bits on each branch\nA 10000-bit message is encoded with the above code and transmitted over a noisy channel. During Viterbi\ndecoding at the receiver, the state 010 had the lowest path metric (a value of 621) in the final time step, and\nthe survivor path from that state was traced back to recover the original message.\nB. What is the likely number of bit errors that are corrected by the decoder? How many errors are likely\nleft uncorrected in the decoded message?\n6 of 8\n\nC. If you are told that the decoded message had no uncorrected errors, can you guess the approximate\nnumber of bit errors that would have occured had the 10000 bit message been transmitted without any\ncoding on the same channel?\nD. From knowing the final state of the trellis (010, as given above), can you infer what the last bit of the\noriginal message was? What about the last-but-one bit? The last 4 bits?\nConsider a transition branch between two states on the trellis that has 000 as the expected set of parity bits.\nAssume that 0V and 1V are used as the signaling voltages to transmit a 0 and 1 respectively, and 0.5V is used\nas the digitization threshold.\nE. Assuming hard decision decoding, which of the two set of received voltages will be considered more\nlikely to correspond to the expected parity bits on the transition: (0V, 0.501V, 0.501V) or (0V, 0V,\n0.9V)? What if one is using soft decision decoding?\nProblem 12. Indicate whether each of the statements below is true or false, and a brief reason why you think\nso.\nA. If the number states in the trellis of a convolutional code is S, then the number of survivor paths at any\npoint of time is S. Remember that if there is \"tie\" between to incoming branches (i.e., they both result in\nthe same path metric), we arbitrarilly choose only one as the predecessor.\nThe path metric of a state s1 in the trellis indicates the number of residual uncorrected errors left along\nthe trellis path from the start state to s1.\nB. Among the survivor paths left at any point during the decoding, no two can be leaving the same state at\nany stage of the trellis.\nC. Among the survivor paths left at any point during the decoding, no two can be entering the same state\nat any stage of the trellis. Remember that if there is \"tie\" between to incoming branches (i.e., they both\nresult in the same path metric), we arbitrarilly choose only one as the predecessor.\nD. For a given state machine of a convolutional code, a particular input message bit stream always\nproduces the same output parity bits.\n7 of 8\n\nl\nProblem 13. Consider a convolution code with two generator polynomials: G0=101 and G1=110.\nA. What is code rate r and constraint length k for this code?\nB. Draw the state transition diagram for a transmitter that uses this convolutional code. The states should\nbe labeled with the binary string xn-1...xn-k+1 and the arcs labeled with xn/p0p1 where x[n] is the next\nmessage bit and p0 and p1 are the two parity bits computed from G0 and G1 respectively.\nThe figure below is a snapshot of the decoding trellis showing a particular state of a maximum likelihood\ndecoder implemented using the Viterbi algorithm. The labels in the boxes show the path metrics computed for\neach state after receiving the incoming parity bits at time t. The labels on the arcs show the expected parity\nbits for each transition; the actual received bits at each time are shown above the trellis.\nC. Fill in the path metrics in the empty boxes in the diagram above (corresponding to the Viterbi\ncalculations for times 6 and 7).\nD. Based on the updated trellis, what is the most-likely final state of the transmitter? How many errors\nwere detected along the most-likely path to the most-likely final state?\nE. What's the most-likely path through the trellis (i.e., what's the most-likely sequence of states for the\ntransmitter)? What's the decoded message?\nF. Based on your choice of the most-likely path through the trellis, at what times did the errors occur?\n8 of 8\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 2 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/a42553855f6ae34e4fbfab2389d0f322_MIT6_02F12_tutor02_sol.pdf",
      "content": "6.02 Practice Problems: Error Correcting Codes\nProblem 1. For each of the following sets of codewords, please give the appropriate (n,k,d) designation where n is\nnumber of bits in each codeword, k is the number of message bits transmitted by each code word and d is the\nminimum Hamming distance between codewords. Also give the code rate.\nA. {111, 100, 001, 010}\nn=3, k=2 (there are 4 codewords), d = 2. The code rate is 2/3.\nB. {00000, 01111, 10100, 11011}\nn=5, k=2 (there are 4 codewords), d = 2. The code rate is 2/5.\nC. {00000}\nA bit of a trick question: n=5, k=0, d = undefined. The code rate is 0 -- since there's only one codeword the\nreceiver can already predict what it will receive, so no useful information is transferred.\nProblem 2. Suppose management has decided to use 20-bit data blocks in the company's new (n,20,3) error\ncorrecting code. What's the minimum value of n that will permit the code to be used for single bit error correction?\nn and k=20 must satisfy the constraint that n + 1 ≤ 2n-k. A little trial-and-error search finds n=25.\nProblem 3. The Registrar has asked for an encoding of class year (\"Freshman\", \"Sophomore\", \"Junior\", \"Senior\")\nthat will allow single error correction. Please give an appropriate 5-bit binary encoding for each of the four years.\nWe want a (5,2,3) block code. For such a code, 00000 is a codeword by definition. Every other codeword must\nhave weight at least 3, and 00111 is an obvious choice (or any permutation thereof).\nWe now need only two more codewords and each must have at least three ones, and must also have a Hamming\ndistance of 3 from the second codeword above. A little bit of trial and error shows that 11011 and 11100 work.\nSo: 00000, 00111, 11011, 11100 should satisfy the Registrar\nProblem 4. For any block code with minimum Hamming distance at least 2t + 1 between code words, show that:\n1 of 14\n\nAn (n, k) block code can represent in its parity bits at most 2n-k patterns, and these must cover all the error cases\nwe wish to correct, as well as the one case with no errors. When the minimum Hamming distance is 2t + 1, the\ncode can correct up to t errors. The number of ways in which the transmission can experience 0,1,2,...,t errors is\nequal to 1 + (n choose 1) + (n choose 2) + ... + (n choose t), and clearly this number must not exceed 2n-k .\nProblem 5. Pairwise Communications has developed a block code with three data (D1, D2, D3) and three parity\nbits (P1, P2, P3):\nP1 = D1 + D2\nP2 = D2 + D3\nP3 = D3 + D1\nA. What is the (n,k,d) designation for this code.\nn = 6, k = 3. To determine d, list the 8 possible codewords (we'll use the order D1, D2, D3, P1, P2, P3):\nBy inspection, the minimum Hamming distance d = 3.\nB. The receiver computes three syndrome bits from the (possibly corrupted) received data and parity bits:\nE1 = D1 + D2 + P1\nE2 = D2 + D3 + P2\nE3 = D3 + D1 + P3.\nThe receiver performs maximum likelihood decoding using the syndrome bits. For the combinations of\nsyndrome bits listed below, state what the maximum-likelihood decoder believes has occured: no errors, a\nsingle error in a specii¬c bit (state which one), or multiple errors.\nE1 E2 E3 = 000\nE1 E2 E3 = 010\nE1 E2 E3 = 101\nE1 E2 E3 = 111\nE1 E2 E3 = 000. No errors.\nE1 E2 E3 = 010. Error in P2.\nE1 E2 E3 = 101. Error in D1.\nE1 E2 E3 = 111. Multiple errors.\n2 of 14\n\n-------------------------\n\n---------------\n\n---------------\n\nProblem 6. Dos Equis Encodings, Inc. specializes in codes that use 20-bit transmit blocks. They are trying to\ndesign a (20, 16) linear block code for single error correction. Explain whether they are likely to succeed or not.\nA single error correcting code must be able to uniquely identify n + 1 patterns (n error patterns and one without\nany errors). So 2n-k must be ≥ n+1. That is not true when n=20 and k=16.\nProblem 7. Consider the following (n,k,d) block code:\nD0 D1 D2 D3 D4 | P0\nD5 D6 D7 D8 D9 | P1\nD10 D11 D12 D13 D14 | P2\nP3 P4 P5 P6 P7 |\nwhere D0-D14 are data bits, P0-P2 are row parity bits and P3-P7 are column parity bits. The transmitted code\nword will be:\nD0 D1 D2 ... D13 D14 P0 P1 ... P6 P7\nA. Please give the values for n, k, d for the code above.\nn=23, k=15, d=3.\nFor a discussion of why d=3, see section 6.4.1 in the notes.\nB. If D0 D1 D2 ... D13 D14 = 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, please compute P0 through P7.\nP0 through P7 = 0 0 0 1 0 0 1 0\nC. Now we receive the four following code words:\nM1: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 0 0 0 1 1 0 1 0\nM2: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 0 0 1 1 1 0 1 0\nM3: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 1 1 0 1 1 0 1 0\nM4: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1, 1 0 0 1 1 0 1 0\nFor each of received code words, indicate the number of errors. If there are errors, indicate if they are\ncorrectable, and if they are, what the correction should be.\nM1:\n0 1 0 1 0 | 0 | 0\n0 1 0 0 1 | 0 | 0\n1 0 0 0 1 | 0 | 0\n1 1 0 1 0 |\n0 1 0 0 0\nThe syndrome bits are shown in red. Since there is only one error -- the parity bit P4 -- it must be that parity\nbit itself that had the error. So the correct data bits are:\n3 of 14\n\n----------------\n\n----------------\n\n----------------\n----------------\n\n----------------\n\n----------------\n\nM1: 0 1 0 1 0, 0 1 0 0 1, 1 0 0 0 1\nM2:\n0 1 0 1 0 | 0 | 0\n0 1 0 0 1 | 0 | 0\n1 0 0 0 1 | 1 | 1\n1 1 0 1 0 |\n0 1 0 0 0 0\nThere are parity errors detected for the third row and second column, so D11 must be in error. So the correct\ndata bits are:\nM2: 0 1 0 1 0, 0 1 0 0 1, 1 1 0 0 1\nM3:\n0 1 0 1 0 | 1 | 1\n0 1 0 0 1 | 1 | 1\n1 0 0 0 1 | 0 | 0\n1 1 0 1 0 | 0\n0 1 0 0 0 0\nThere are more than two row/column parity errors, indicating an uncorrectable multi-bit error.\nM4:\n0 1 0 1 0 | 1 | 1\n0 1 0 0 1 | 0 | 0\n1 0 0 0 1 | 0 | 0\n1 1 0 1 0 |\n0 1 0 0 0\nThe row and column parity bit indicate the data bit which got flipped (D1). Flipping it gives us the correct\ndata bits:\nM4: 0 0 0 1 0, 0 1 0 0 1, 1 0 0 0 1\nProblem 8. The following matrix shows a rectangular single error correcting code consisting of 9 data bits, 3 row\nparity bits and 3 column parity bits. For each of the examples that follow, please indicate the correction the\nreceiver must perform: give the position of the bit that needs correcting (e.g., D7, R1), or \"no\" if there are no\nerrors, or \"M\" if there is a multi-bit uncorrectable error.\n4 of 14\n\n1) Error in column 3 parity bit.\n2) M.\n3) Error in D8.\n4) No errors.\n5) M.\nProblem 9. Consider two convolutional coding schemes - I and II. The generator polynomials for the two schemes\nare\nScheme I: G0 = 1101, G1 = 1110\nScheme II: G0 = 110101, G1 = 111011\nNotation is follows: if the generator polynomial is, say, 1101, then the corresponding parity bit for message bit n is\n(x[n] + x[n-1] + x[n-3]) mod 2\nwhere x[n] is the message sequence.\nA. Indicate TRUE or FALSE\na. Code rate of Scheme I is 1/4.\nb. Constraint length of Scheme II is 4.\nc. Code rate of Scheme II is equal to code rate of Scheme I.\nd. Constraint length of Scheme I is 4.\nThe code rate (r) and constraint length (k) for the two schemes are\nI: r = 1/2, k = 4\nII: r = 1/2, k = 6\nSo\na. false\nb. false\nc. true\nd. true\nB. How many states will there be in the state diagram for Scheme I? For Scheme II?\nNumber of states is given by 2k-1 where k = constraint length. Following the convention of state machines as\noutlined in lecture, number of states in Scheme I is 8 and in Scheme II, 32.\nC. Which code will lead to a lower bit error rate? Why?\nScheme II is likely to lead to a lower bit error rate. Both codes have the same code rate but different\nconstraint lengths. So Scheme II encodes more history and since it is less likely that 6 trailing bits will be in\nerror vs. 4 trailing bits, II is stronger.\n5 of 14\n\nD. Alyssa P. Hacker suggests a modification to Scheme I which involves adding a third generator polynomial G2\n= 1001. What is the code rate r of Alyssa's coding scheme? What about constraint length k? Alyssa claims\nthat her scheme is stronger than Scheme I. Based on your computations for r and k, is her statement true?\nFor Alyssa's scheme r = 1/3, k = 4. Alyssa's code has a lower code rate (more redundancy), and given then\nshe's sending additional information, the modified scheme I is stronger in the sense that more information\nleads to better error detection and correction.\nProblem 10. Consider a convolution code that uses two generator polynomials: G0 = 111 and G1 = 110. You are\ngiven a particular snapshot of the decoding trellis used to determine the most likely sequence of states visited by\nthe transmitter while transmitting a particular message:\nA. Complete the Viterbi step, i.e., fill in the question marks in the matrix, assuming a hard branch metric based\non the Hamming distance between expected an received parity where the received voltages are digitized\nusing a 0.5V threshold.\nThe digitized received parity bits are 1 and 0.\nFor state 0:\nPM[0,n] = min(PM[0,n-1]+BM(00,10), PM[1,n-1]+BM(10,10)) = min(1+1,0+0) = 0\nPredecessor[0,n] = 1\nFor state 1:\nPM[1,n] = min(PM[2,n-1]+BM(11,10), PM[3,n-1]+BM(01,10)) = min(2+1,3+2) = 3\nPredecessor[1,n] = 2\nFor state 2:\nPM[2,n] = min(PM[0,n-1]+BM(11,10), PM[1,n-1]+BM(01,10)) = min(1+1,0+2) = 2\nPredecessor[2,n] = 0 or 1\n6 of 14\n\nB.\nC.\nD.\nFor state 3:\nPM[1,n] = min(PM[2,n-1]+BM(00,10), PM[3,n-1]+BM(10,10)) = min(2+1,3+0) = 3\nPredecessor[1,n] = 2 or 3\nComplete the Viterbi step, i.e., fill in the question marks in the matrix, assuming a soft branch metric based\non the square of the Euclidean distance between expected an received parity voltages. Note that your branch\nand path metrics will not necessarily be integers.\nFor state 0:\nPM[0,n] = min(PM[0,n-1]+BM([0,0],[0.6,0.4]), PM[1,n-1]+BM([1,0],[0.6,0.4])) = min(1+0.52,0+.32)\n= .32\nPredecessor[0,n] = 1\nFor state 1:\nPM[1,n] = min(PM[2,n-1]+BM([1,1],[0.6,0.4]), PM[3,n-1]+BM([0,1],[0.6,0.4])) =\nmin(2+0.52,3+0.72) = 2.52\nPredecessor[1,n] = 2\nFor state 2:\nPM[2,n] = min(PM[0,n-1]+BM([1,1],[0.6,0.4]), PM[1,n-1]+BM([0,1],[0.6,0.4])) =\nmin(1+0.52,0+0.72) = 0.72\nPredecessor[2,n] = 1\nFor state 3:\nPM[1,n] = min(PM[2,n-1]+BM([0,0],[0.6,0.4]), PM[3,n-1]+BM([1,0],[0.6,0.4])) = min(2+0.52,3+.32)\n= 2.52\nPredecessor[1,n] = 2\nDoes the soft metric give a different answer than the hard metric? Base your response in terms of the relative\nordering of the states in the second column and the survivor paths.\nThe soft metric certainly gives different path metrics, but the relative ordering of the likelihood of each state\nremains unchanged. Using the soft metric, the choice of survivor path leading to states 2 and 3 has firmed up\n(with the hard metric either of the survivor paths for each of states 2 and 3 could have been chosen).\nIf the transmitted message starts with the bits \"01011\", what is the sequence of bits produced by the\nconvolutional encoder?\nsequence produced by encoder: 00 11 11 01 00.\nThe receiver determines the most-likely transmitted message by using the Viterbi algorithm to process the (possibly\ncorrupted) received parity bits. The path metric trellis generated from a particular set of received parity bits is\nshown below. The boxes in the trellis contain the minimum path metric as computed by the Viterbi algorithm.\n7 of 14\n\nE. Referring to the trellis above, what is the receiver's estimate of the most-likely transmitter state after\nprocessing the bits received at time step 6?\nMost-likely transmitter state = state with smallest path metric = state 01.\nF. Referring to the trellis above, show the most-likely path through the trellis by placing a circle around the\nappropriate state box at each time step and darkening the appropriate arcs. What is the receiver's estimate of\nthe most-likely transmitted message?\nTracing backwards through the trellis:\nafter state 6: state 01 (most-likely final state)\nafter state 5: state 10\nafter state 4: state 01\nafter state 3: state 11\nafter state 2: state 10\nafter state 1: state 00\nThe message can be read off from the high-order bit of the transmitter state (now moving foward through the\ntrellis): 011010.\nG. Referring to the trellis above, and given the receiver's estimate of the most-likely transmitted message, at\nwhat time step(s) were errors detected by the receiver? Briefly explain your reason- ing.\nAt time steps 1 and 2, where the path metric increments along the most-likely path.\nH. Now consider the path metric trellis generated from a different set of received parity bits.\n8 of 14\n\nReferring to the trellis above, determine which pair(s) of parity bits could have been been received at time\nsteps 1, 2 and 3. Briefly explain your reasoning.\nAt time 1, the transition from state 00 to 10 has a branch metric BM(??,11)=0, so the parity bits must have\nbeen 11.\nAt time 2, looking at the transition from state 10 to states 01 and 11, we see that BM(??,11)=1 and\nBM(??,00)=1, so the possible pairs of parity bits are 01 and 10.\nAt time 3, looking at the transition from state 01 to state 10, we see that BM(??,01)=0, so the parity bits are\n01.\nProblem 11. Consider a binary convolutional code specified by the generators (1011, 1101, 1111).\nA. What are the values of\na. constraint length of the code\nb. rate of the code\nc. number of states at each time step of the trellis\nd. number of branches transitioning into each state\ne. number of branches transitioning out of each state\nf. number of expected parity bits on each branch\na. 4\nb. 1/3\n9 of 14\n\nc. 24-1 = 8\nd. 2\ne. 2\nf. 3\nA 10000-bit message is encoded with the above code and transmitted over a noisy channel. During Viterbi\ndecoding at the receiver, the state 010 had the lowest path metric (a value of 621) in the final time step, and the\nsurvivor path from that state was traced back to recover the original message.\nB. What is the likely number of bit errors that are corrected by the decoder? How many errors are likely left\nuncorrected in the decoded message?\n621 errors were likely corrected by the decoder to produce the final decoded message. We cannot infer the\nnumber of uncorrected errors still left in the message absent more information (like, say, the original\ntransmitted bits).\nC. If you are told that the decoded message had no uncorrected errors, can you guess the approximate number\nof bit errors that would have occured had the 10000 bit message been transmitted without any coding on the\nsame channel?\n3*10000 bits were transmitted over the channel and the received message had 621 bit errors (all corrected by\nthe convolutional code). Therefore, if the 10000-bit message would have been transmitted without coding, it\nwould have had approximately 621/3 = 207 errors.\nD. From knowing the final state of the trellis (010, as given above), can you infer what the last bit of the original\nmessage was? What about the last-but-one bit? The last 4 bits?\nThe state gives the last 3 three bits of the original message. In general, for a convolutional code with a\nconstraint length k, the state indicates the final k-1 bits of the original message. To determine more bits we\nwould need to know the states along the most-likely path as we trace back through the trellis.\nConsider a transition branch between two states on the trellis that has 000 as the expected set of parity bits.\nAssume that 0V and 1V are used as the signaling voltages to transmit a 0 and 1 respectively, and 0.5V is used as the\ndigitization threshold.\nE. Assuming hard decision decoding, which of the two set of received voltages will be considered more likely to\ncorrespond to the expected parity bits on the transition: (0V, 0.501V, 0.501V) or (0V, 0V, 0.9V)? What if one\nis using soft decision decoding?\nWith hard decision decoding: (0V, 0.501V, 0.501V) -> 011 -> hamming distance of 2 from expected parity\nbits. (0V, 0V, 0.9V) -> 001 -> hamming distance of 1. Therefore, (0V, 0V, 0.9V) is considered more likely.\nWith soft decision decoding, (0V, 0.501V, 0.501V) will have a branch metric of approximately 0.5. (0V, 0V,\n0.9V) will have a metric of approximmately 0.8. Therefore, (0V, 0.501V, 0.501V) will be considered more\nlikely.\n10 of 14\n\nProblem 12. Indicate whether each of the statements below is true or false, and a brief reason why you think so.\nA. If the number states in the trellis of a convolutional code is S, then the number of survivor paths at any point\nof time is S. Remember that if there is \"tie\" between to incoming branches (i.e., they both result in the same\npath metric), we arbitrarilly choose only one as the predecessor.\nTrue. There is one survivor per state.\nThe path metric of a state s1 in the trellis indicates the number of residual uncorrected errors left along the\ntrellis path from the start state to s1.\nFalse. It indicates the number of likely corrected errors.\nB. Among the survivor paths left at any point during the decoding, no two can be leaving the same state at any\nstage of the trellis.\nFalse. In fact, the survivor paths will likely merge at a certain stage in the past, at which point all of then will\nemerge from the same state.\nC. Among the survivor paths left at any point during the decoding, no two can be entering the same state at any\nstage of the trellis. Remember that if there is \"tie\" between to incoming branches (i.e., they both result in the\nsame path metric), we arbitrarilly choose only one as the predecessor.\nTrue. When two paths merge at any state, only one of them will ever be chosen as a survivor path.\nD. For a given state machine of a convolutional code, a particular input message bit stream always produces the\nsame output parity bits.\nFalse. The same input stream with different start states will produce different output parity bits.\nProblem 13. Consider a convolution code with two generator polynomials: G0=101 and G1=110.\nA. What is code rate r and constraint length k for this code?\nWe send two parity bits for each message bit, so the code rate r is 1/2. Three message bits are involved in the\ncomputation of the parity bits, so the constraint length k is 3.\nB. Draw the state transition diagram for a transmitter that uses this convolutional code. The states should be\nlabeled with the binary string xn-1...xn-k+1 and the arcs labeled with xn/p0p1 where x[n] is the next\nmessage bit and p0 and p1 are the two parity bits computed from G0 and G1 respectively.\n11 of 14\n\nThe figure below is a snapshot of the decoding trellis showing a particular state of a maximum likelihood decoder\nimplemented using the Viterbi algorithm. The labels in the boxes show the path metrics computed for each state\nafter receiving the incoming parity bits at time t. The labels on the arcs show the expected parity bits for each\ntransition; the actual received bits at each time are shown above the trellis.\nC. Fill in the path metrics in the empty boxes in the diagram above (corresponding to the Viterbi calculations for\ntimes 6 and 7).\n12 of 14\n\nD. Based on the updated trellis, what is the most-likely final state of the transmitter? How many errors were\ndetected along the most-likely path to the most-likely final state?\nThe most-likely final state is 01, the state with the smallest path metric. The path metric tells us the total\nnumber of errors along the most-likely path leading to the state. In this example there were 3 errors\naltogether.\nE. What's the most-likely path through the trellis (i.e., what's the most-likely sequence of states for the\ntransmitter)? What's the decoded message?\nThe most-likely path has been highlighted in red below. The decoded message can be read off from the state\ntransitions along the most-likely path: 1000110.\nF. Based on your choice of the most-likely path through the trellis, at what times did the errors occur?\n13 of 14\n\nThe path metric is incremented for each error along the path. Looking at the most-likely path we can see that\nthere were single-bit errors at times 1, 3 and 5.\n14 of 14\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.02 Tutorial 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/62e9fbe90eae7381417162f67df32e8e_MIT6_02F12_tutor03.pdf",
      "content": "6.02 Practice Problems: Noise & Bit Errors\nProblem 1.\nSuppose the bit detection sample at the receiver is V + noise volts when the sample corresponds to a\ntransmitted '1', and 0.0 + noise volts when the sample corresponds to a transmitted '0', where noise is a\nzero-mean Normal(Gaussian) random variable with standard deviation σNOISE.\nA. If the transmitter is equally likely to send '0''s or '1''s, and V/2 volts is used as the threshold for deciding\nwhether the received bit is a '0' or a '1', give an expression for the bit-error rate (BER) in terms of the\nerfc function and σNOISE.\nB. Suppose the transmitter is equally likely to send zeros or ones and uses zero volt samples to represent a\n'0' and one volt samples to represent a '1'. If the receiver uses 0.5 volts as the threshold for deciding bit\nvalue, for what value of σNOISE is the probability of a bit error approximately equal to 1/5?\nC. Will your answer for σNOISE in part (B) change if the threshold used by the receiver is shifted to 0.6\nvolts? Do not try to determine σNOISE, but justify your answer.\nD. Will your answer for σNOISE in part (B) change if the transmitter is twice as likely to send ones as\nzeros, but the receiver still uses a threshold of 0.5 volts? Do not try to determine σNOISE, but justify\nyour answer.\nProblem 2.\nMessages are transmitted along a noisy channel using the following protocol: a \"0\" bit is transmitted as -0.5\nVolt and a \"1\" bit as 0.5 Volt. The PDF of the total noise added by the channel, H, is shown below. It is not a\nGaussian.\n1 of 4\n\na. Compute H(0), the maximum value of H.\nb. It is known that a \"0\" bits 3 times as likely to be transmitted as a \"1\" bit. The PDF of the message\nsignal, M, is shown below. Fill in the values P and Q.\nc. If the digitization threshold voltage is 0V, what is the bit error rate?\nd. What digitization threshold voltage would minimize the bit error rate?\n2 of 4\n\nProblem 3.\nBen Bitdiddle studies the bipolar signaling scheme from 6.02 and decides to extend it to a 4-level signaling\nscheme, which he calls Ben's Aggressive Signaling Scheme, or BASS. In BASS, the transmitter can send four\npossible signal levels, or voltages: (-3A, - A, + A, + 3A), where A is some positive value. To transmit bits, the\nsender's mapper maps consecutive pairs of bits to a fixed voltage level that is held for some fixed interval of\ntime, creating a symbol. For example, we might map bits \"00\" to -3A, \"01\" to -A, \"10\" to +A, and \"11\" to\n+3A. Each distinct pair of bits corresponds to a unique symbol. Call these symbols s_minus3, s_minus1,\ns_plus1, and s_plus3. Each symbol has the same prior probability of being transmitted.\nThe symbols are transmitted over a channel that has no distortion but does have additive noise, and are\nsampled at the receiver in the usual way. Assume the samples at the receiver are perturbed from their ideal\nnoise-free values by a zero-mean additive white Gaussian noise (AWGN) process with noise intensity\nN0 = 2σ2, where σ2 is the variance of the Gaussian noise on each sample. In the time slot associated with\neach symbol, the BASS receiver digitizes a selected voltage sample, r, and returns an estimate, s, of the\ntransmitted symbol in that slot, using the following intuitive digitizing rule (written in Python syntax):\ndef digitize(r):\nif r < -2A:\ns = sminus3\nelif r < 0:\ns = sminus1\nelif r < 2A:\ns = splus1\nelse: s = splus3\nreturn s\n1. The power of a symbol transmission is defined as the square of the voltage level at which the symbol is\ntransmitted. What is the average power level, P, of a symbol transmission in BASS (i.e., the average\npower dissipated at the transmitter)?\nBen wants to calculate the symbol error rate for BASS, i.e., the probability that the symbol chosen by\nthe receiver was different from the symbol transmitted. Note: we are not interested in the bit error rate\nhere. Help Ben calculate the symbol error rate by answering the following questions below from 2\nthrough 8\n2. Suppose the sender transmits symbol s_plus3. What is the conditional symbol error rate given this\ninformation; i.e., what is P(symbolError| s_plus3 sent) ? Express your answer in terms of A, N0, and\ninf\n-x\n√π∫\n2 Քx.\nthe erfc function, defined as erfc z=\ne\nz\n3 of 4\n\n3. Now suppose the sender transmits symbol s_plus1. What is the conditional symbol error rate given this\ninformation, in terms of A, N0, and the erfc function? The conditional symbol error rates for the other\ntwo symbols don't need to be calculated separately.\n4. The symbol error rate when the sender transmits symbol s_minus3 is the same as the symbol error rate\nof which of these symbols?\n1. s_minus1.\n2. s_plus1.\n3. s_plus3.\n5. The symbol error rate when the sender transmits symbol s_minus1 is the same as the symbol error rate\nof which of these symbols?\n1. s_minus3.\n2. s_plus1.\n3. s_plus3.\n6. Now suppose the sender transmits symbol s_minus1. What is the conditional symbol error rate given\nthis information, in terms of A, N0, and the erfc function? Do not recalculate; use symmetry and your\nprevious answer(s).\n7. Combining your answers to the questions above, what is the symbol error rate in terms of A, N0, and\nthe erfc function? Recall that all symbols are equally likely to be transmitted.\n8. Express the answer to the above question in terms of the signal-to-noise ratio. The average signal power\nis in the answer to one of the questions above.\n4 of 4\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.02 Introduction to EECS II: Digital Communication Systems\nFall 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}