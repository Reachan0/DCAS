{
  "course_name": "Principles of Discrete Applied Mathematics",
  "course_description": "No description found.",
  "topics": [
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Communication",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Communication"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nRecitations: 1 session / week, 1 hour / session\n\nPrerequisites\n\n18.02 Multivariable Calculus\n\nDescription\n\nThis course is an introduction to discrete applied mathematics. The topics presented are generally grouped into units covering between one and two weeks. These units include probability, counting, linear programming, number-theoretic algorithms, sorting, data compression, and error-correcting codes.\n\nTextbook\n\nThere is no textbook; a full set of\nlecture notes\nis provided.\n\nRequirements\n\nThis is a CI-M (\nCommunication Intensive in the Major\n) course, and thus teaches students to write mathematics. This component will take the form of writing assignments in homework problems; one of them will be a relatively long (several pages) term paper whose writing will take several iterations spread between mid October and the end of the Fall term.\n\nThe homework assignments (the problem sets and writing assignments) must all be submitted electronically (the only exceptions are the pre-recitation assignments, see the recitation section below). Carefully read the instructions on each problem set. Homework will generally be due one week after it is assignment.\n\nWriting assignments submitted electronically must be word processed (see the section below on LaTeX and other word processing programs), and should be submitted in PDF format. Hand-drawn figures are permitted (for some problems, figures may be quite useful). You may draw your figures on the printed output and scan it in, or scan in your figures and include them in your document. If you do the rest of your problem sets by hand, you must write them legibly and scan them in with enough resolution for them to be easily legible. If they are barely legible, we may ask you to resubmit them.\n\nThere will also be 3 quizzes throughout the term.\n\nCollaboration Policy\n\nCollaboration on homework is permitted, but you first need to think about the problems on your own and you must write the solutions yourself; no copying is permitted. For the writing assignments, you may seek feedback from classmates and others, but the writing must be your own. You must list the names of your collaborators on your submitted homework, or list you had none. We ask that you not refer to solutions from previous incarnations of the course, or from solution banks.\n\nRecitations\n\nThere is a 1-hour weekly recitation in which we will both discuss communication and further explain the course material, especially as it relates to communication. Attendance in this recitation is mandatory. You may miss one recitation unexcused without penalty. A second unexcused absence will result in a reduction in your grade and a warning; if you miss any recitations without an excuse after the warning, you will fail the class.\n\nFor some of the recitations, there will be a pre-recitation assignment. These will be due in recitation and should not be submitted electronically.\n\nLaTeX / Word Processing\n\nLaTeX\nis a document preparation system which is very good at handling mathematical equations, and which has become a standard in several fields, including mathematics, physics, and computer science. If you are planning to go into one of these fields, we encourage you to learn it, and we will provide resources to help you do so.\n\nThere are other scientific fields for which LaTeX is not the standard, and if you are in one of these majors, there is not really any need for you to learn it. In this case, feel free to use Microsoft\n(r)\nWord or another word processing system for your writing assignments and term paper. We do encourage you to figure out how to do equations properly in Word. Without any extra add-on's, the way Microsoft\n(r)\nWord handles equations is really quite cumbersome, but there is software available that fixes this. Unfortunately, we don't have any recommendations because we use LaTeX. But, remember, in all cases, you must save the file as a PDF before submitting it electronically.\n\nGrading\n\nThe grade will be made up of problem sets and writing assignments (55%), and three quizzes (15% each). The homework will include writing assignments, some of which will have to be revised after we give comments on them. Depending on their length, the writing assignments in the homework may be weighted more heavily than the rest of the problems.\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets and writing assignments\n\n3 Quizzes @ 15% each",
  "files": [
    {
      "category": "Resource",
      "title": "18.310 Homework 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/abf00d007c2ed257001d191ed3c1f98d_MIT18_310F13_Homework1.pdf",
      "content": "Fall 2013\n18.310 Homework 1\nDue September 11th at 6PM\nInstructions: Remember to submit a separate PDF for each question .Do not forget to include\na list of your collaborators or to state that you worked on your own.\n1. Writing Assignment Following the discussion in recitation, write up a clear and complete\nproof of the following theorem. Your proof should be easily understandable by another 18.310\nstudent. Your solution must be word processed, using either LATEX, Word, or some other\nword processor, and submitted as a pdf file (no Word files allowed). Half of the marks for this\nquestion will be allocated to the writing as opposed to simply mathematical correctness. Have\na look at the resources linked from the course website for some examples and suggestions.\nSee in particular the notes on \"guiding text\".\nTheorem 1. A box contains 7 black balls and 4 white balls. Suppose that we repeatedly draw\na ball at random from the box, observe its color and then discard it. We do this 4 times. For\ni ∈{1, 2, 3, 4}, let Xi be the random variable representing the color of the ith ball drawn from\nthe box. Then,\nP ({X1, X2} = {white, black})\n= P ({X3, X4} = {white, black}) .\n2. Let X be a uniformly random subset of {1, 2, · · · , n} (there are 2n possible subsets, and each\nis chosen with probability 1/2n). Let Y be another independently chosen random subset.\n(a) For i ∈{1, 2, · · · , n}, let Ai be the event that i ∈ X, and similarly let Bi be the event\nthat i ∈ Y . Argue that the 2n events A1, A2, . . . , An, B1, B2, . . . , Bn are all independent.\nAlso determine:\n(b) the probability that X ∩ Y = ∅,\n(c) P(X ∩ Y = ∅ and X has k elements), where k is an integer between 1 and n.\nRemark: finding (b) using (c) might not be the simplest solution.\n3. A coin is tossed 10 times. This will give a sequence of H's and T's, such as HTTHHTHTTT.\n(a) What is the expected number of times TTH appears consecutively in this sequence.\n(b) Use the inclusion-exclusion formula to compute the probability that TTH appears (con\nsecutively) at least once in the sequence.\n4. Suppose N people enter an elevator in the basement of a building with K floors above it.\nAssume each person gets off at a floor with probability 1/K and independent of any other\nperson. What is the expected number of stops the elevator will make in unloading everyone?\nHW1-1\n\nThe following exercises should not be handed in, but we nevertheless encourage you to do them.\n1. (a) We have a box with two black balls and one white ball. We play a game in which we\nwin if we draw the white ball. We draw a ball at random from the box. Before we\ncan see which color it is, the ball is taken away from us and we draw another ball at\nrandom from the box. Suppose that we see this second ball and it happens to be black.\nAt this point, we may choose either the first ball we drew or the ball remaining in the\nbox. Which should we choose? Hint: Prove that both strategies have the same success\nprobability.\n(b) Consider the same setup, but now the second ball is not drawn at random, but selected\nto be black (i.e. someone opens the box, finds a black ball and removes it). Do both\nstrategies still have the same success probability? Prove or disprove. Hint: What is the\nrelevant subset of the sample space?\n2. You roll two dice, each having numbers 1 to 6 on its faces. Consider the following events:\n(a) A: The number on the first die is even.\n(b) B: The number on the second die is 1.\nFind:\n(a) an event C so that A, B and C are 3 independent events.\n(b) an event D so that A, B and D are 3 events that are pairwise independent but not\nindependent.\nHW1-2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 10: Peer reviews",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/653e2005b8c74e3db10916a8226c0f04_MIT18_310F13_Homework10.pdf",
      "content": "Fall 2013\n18.310 Homework 10: Peer reviews\nDue Wednesday November 20th at 6PM\nLogistics\nIn this assignment, you will write peer reviews on other students' complete articles (see homework\n6). Each student is assigned to a group of 3 or 4 students (see the groups later in this document),\nand will write a peer review on the articles submitted by two (irrespective of whether the group\nhas size 3 or 4) students in their group, namely the students listed (in cyclic order) before and\nafter them in the group. In a group of size 4, the 1st student listed will thus comment on the\npapers of the 2nd and 4th students in the group.\nYour first task is to email as soon as possible your complete article (exactly the same\nversion as what you have uploaded on\nfor HW9, part 1) to the two students who will write\nreviews on it (the ones before and after you inyourgroup). This needs to be done by Friday\nNovember 15th at 11:59PM at the latest. If you fail to email your paper by then, we will\nimpose a penalty for the final grade of your article. If you haven't received the articles you are\nsupposed to review by midnight Friday, email go one of the instructors and we will send them to\nyou.\nOnce your two peer reviews are prepared, please send them to us, and also email them to these\ntwo students. And on Thursday November 21st, bring a printout of your reviews to your assigned\nrecitation, and the reviews will be discussed then.\nPeer Review Guidelines\nThe purpose of writing a peer review is for you to help your classmate to improve his/her paper.\nTo write a review that is helpful, think about what sorts of comments would be most helpful to you\nas you revise your paper; write similarly helpful comments for your two peers. You should devote\nat least an hour to reading and commenting on each of your peer's paper. Consider the following\nquestions:\n1. Is the paper clear? The assigned audience are math majors, so the author should write\nclearly enough for the paper to make sense to you. Point out anything that you find to be\nconfusing or unclear. Try to point out precisely what is causing the confusion so the author\ncan determine how best to clarify the text.\n2. Is the paper consistent and correct? If you aren't comfortable stating that something is\nincorrect, you could word your comment as a question (\"Why is...?\") or as a confusion (\"I\ndon't understand why...\").\n3. Are new topics and ideas introduced with sufficient explanation? New topics and ideas should\nbe connected to ideas that are familiar to the audience. Also, the reason for introducing the\nnew topic or idea should be clear.\nHW10-1\nthe site\n\n4. Are topics presented in a logical order? This question applies to all levels of the paper. For\nexample, sections should be presented in a logical order, and the steps within a proof should\nbe presented in a logical order.\n5. Does the paper achieve an appropriate balance of conciseness and explanation? Point out\nplaces where the text is too wordy or too concise.\n6. Is the paper proofread for grammar, spelling, etc? Be sure to give some honest comments\nabout what is done well in the paper as well suggestions for improving the paper.\nGrading\nYou will not assign a grade as part of your peer review. However, your peer reviews will be graded\non a scale of 10 as follows:\n- 10: A thorough review that points out confusing parts of the paper and includes helpful\nsuggestions (e.g., suggesting restructuring, how to explain more clearly, helpful figures...)\nand probing questions (e.g., Is this lemma really necessary? Could you prove this theorem\nmore elegantly by...? ). Rationales for comments are clearly explained.\n- 7: A less thorough review with some helpful comments.\n- 4: Few helpful comments.\n- 0: Failed to submit a peer review.\nHW10-2\n\nGroups\nStudent names removed for privacy reasons.\nHW10-3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 11",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/b9863df46de028a8d9578920205376fb_MIT18_310F13_Homework11.pdf",
      "content": "Fall 2013\n18.310 Homework 11\nDue Tuesday November 26th at 6PM\n1. Determine the Discrete Fourier transform (over the complex numbers) for the sequence\ny0, y1, y2, y3 where y0 = 0, y1 = 1, y2 = 2 and y3 = 3.\nNow take the inverse Fourier transform for the sequence of complex numbers c0, c1, c2, c3 you\njust obtained. Show your calculations.\n2. Suppose we want to multiply two binary numbers u and v using Discrete Fourier Transforms\nperformed over Zp for an appropriate prime p. For simplicity, let's assume that u and v\nhave only 4 bits (for just 4 bits, it will be much more cumbersome than doing the usual long\nmultiplication, but you probably don't want to have a homework problem in which you need\nto multiply two 106-bit integers....). It will be easier for you if you use excel for the various\ncalculations in this exercise. We will need to compute the Discrete Fourier Transforms of u\nand v, multiply the corresponding coefficients, and take the inverse Fourier transform, and\nthen perform the carryover to get the product of u and v in binary. Since the product of u and\nv can have 8 bits, we will be performing Fourier transforms on sequences of n = 8 numbers.\n(Thus, if we are multiplying u = 1010 (ten in binary) by v = 0111 (seven in binary), we\nwould see these numbers as 00001010 and 00000111, and hope to get seventy in binary as the\nproduct.)\n(a) Explain why we can use p = 17 in this specific case of multiplying two 4-bit numbers.\nCan we use any smaller p (remember p has to be a prime)? Explain. What would be\nthe smallest prime p you would use if we were multiplying two 8-bit numbers?\n(b) What are all the primitive 8th-root of unity over Z17 (read the lecture notes or use\nexcel...)?\n(c) Suppose we use z = 2 as a primitive 8th-root of unity. What is z-1 (mod 17)?\n(d) Using Z17 and z = 2 as primitive 8th-root of unity, what is the Discrete Fourier trans\nform for u = 00001010 (i.e., for the sequence with ui = 1 for i ∈ 1, 3 and 0 for\ni ∈{0, 2, 4, 5, 6, 7})? Call it a. And what is b, the DFT for v = 00000111? Remember\nthat, here, the DFT of (y0, y1, · · · , yn-1) is given by\nn-1\nn\n\njk\nck ≡\nyj z -1\n(mod 17),\nj=0\nfor k = 0, · · · , n - 1.\n(e) Multiply the corresponding coefficients (over Z17) and compute the inverse DFT (re\nmember that in the DFT you will be using z = 2 rather than z-1, and that there will\nbe an additional factor n-1 (mod 17). Is this what you expected? How much is uv in\nbinary?\nHW11-1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 11 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/b384fa3a6b0db4b2e34d59bba9de7917_MIT18_310F13_Homework11Sol.pdf",
      "content": "Fall 2013\n18.310 Homework 11\nDue Tuesday November 26th at 6PM\n1. Determine the Discrete Fourier transform (over the complex numbers) for the sequence\ny0, y1, y2, y3 where y0 = 0, y1 = 1, y2 = 2 and y3 = 3.\nSolution: We have n = 4 points. So using the definition of discrete fourier transform:\nn-1\n\n-2πijk/n\nck =\nyj e\n,\nj=0\nwe get\nc0 = 1 + 2 + 3 = 6,\n-3π/2i\nc1 = e -π/2i + 2e -πi + 3e\n= -i - 2 + 3i = 2i - 2,\n-3π\nc2 = e -πi + 2e -2πi + 3e\n= -1 + 2 - 3 = -2,\nand\n-9π/2i\nc3 = e -3π/2i + 2e -3πi + 3e\n= i - 2 - 3i = -2i - 2.\nNow take the inverse Fourier transform for the sequence of complex numbers c0, c1, c2, c3 you\njust obtained. Show your calculations.\nIf we take the inverse given by\nn-1\n\n2πijk/n\nyj =\ncke\nn k=0\nwe get\ny0 = (6 + 2i - 2 - 2 - 2i - 2) = 0,\nπ/2i - 2e\ny1 = 1(6 + (2i - 2)e\nπi - (2i + 2)e 3π/2i) = 1(6 + (-2 - 2i) + 2 - (2 - 2i)) = 1,\nπi - 2e\ny2 = 1(6 + (2i - 2)e\n2πi - (2i + 2)e 3πi) = 1(6 + (-2i + 2) - 2 + (2i + 2)) = 2,\n3π/2i - 2e\ny3 = 1(6 + (2i - 2)e\n3πi - (2i + 2)e 9π/2i) = 1(6 + (2 + 2i) + 2 - (-2 + 2i)) = 3\njustifying its name.\n2. Suppose we want to multiply two binary numbers u and v using Discrete Fourier Transforms\nperformed over Zp for an appropriate prime p. For simplicity, let's assume that u and v\nhave only 4 bits (for just 4 bits, it will be much more cumbersome than doing the usual long\nmultiplication, but you probably don't want to have a homework problem in which you need\nto multiply two 106-bit integers....). It will be easier for you if you use excel for the various\nHW11-1\nSolutions\n\ncalculations in this exercise. We will need to compute the Discrete Fourier Transforms of u\nand v, multiply the corresponding coefficients, and take the inverse Fourier transform, and\nthen perform the carryover to get the product of u and v in binary. Since the product of u and\nv can have 8 bits, we will be performing Fourier transforms on sequences of n = 8 numbers.\n(Thus, if we are multiplying u = 1010 (ten in binary) by v = 0111 (seven in binary), we\nwould see these numbers as 00001010 and 00000111, and hope to get seventy in binary as the\nproduct.)\n(a) Explain why we can use p = 17 in this specific case of multiplying two 4-bit numbers.\nCan we use any smaller p (remember p has to be a prime)? Explain. What would be\nthe smallest prime p you would use if we were multiplying two 8-bit numbers?\nSolution: There are two conditions that p need to satisfy. The first one is that it needs\nto be large enough so that we can recover the coefficients of the convolution from their\nvalues modulo p. The coefficients of the convolution will be between 0 and 4 · 12 = 4\nand so for this purpose we need to take p ≥ 5. (The 22b bound in the lecture notes is\nan upper bound to the real bound which is (2b - 1)2, the largest product possible with b\nbits. In our case b = 1 so the real bound is 4(2 - 1)2 = 4.) The second condition is that\nour prime p needs to have an n-th root of unity (here n = 8); for this, we need that p\nsatisfies the equation p = mn + 1 with m integer. The first one to satisfy it is p = 17,\nand that's why we use it.\nIn the case of multiplying two 8 bit integers, we get that the maximum coefficient of the\nconvolution is 8, so using p > 8 are candidates. Since we are looking at products of size\nat most 16 bits, we need to find a prime with a 16th root of unity, i.e. p = 16m + 1. So\n17 also works in this case.\n(b) What are all the primitive 8th-root of unity over Z17 (read the lecture notes or use\nexcel...)?\nSolution: From the lecture notes: The 8th roots of unity are 2, 8, 9 ad 15. For example\n28 = 256 = 17 × 15 + 1.\n(c) Suppose we use z = 2 as a primitive 8th-root of unity. What is z-1 (mod 17)?\nSolution: Since 2 is an 8th root of unity 2-1 = 27 = 128 = 9 (mod 17).\n(d) Using Z17 and z = 2 as primitive 8th-root of unity, what is the Discrete Fourier trans\nform for u = 00001010 (i.e., for the sequence with ui = 1 for i ∈ 1, 3 and 0 for\ni ∈{0, 2, 4, 5, 6, 7})? Call it a. And what is b, the DFT for v = 00000111? Remember\nthat, here, the DFT of (y0, y1, · · · , yn-1) is given by\nn-1\nck ≡\nyj\n\nz -1 jk\n(mod 17),\nj=0\nfor k = 0, · · · , n - 1.\nSolution: I will drop the symbol (mod 17) in the next calculations, to make it easier\nto read. First it will be useful to list the powers of 2-1 = 9. They are\n90 = 0, 91 = 9, 92 = 13, 93 = 15, 94 = 16, 95 = 8, 96 = 4, 97 = 2.\nUsing the formula we get that\na0 = 1 + 1 = 2,\nHW11-2\nX\n\na1 = 9 + 93 = 7,\na2 = 92 + 96 = 0,\na3 = 93 + 9 = 7,\na4 = 94 + 94 = 15,\na5 = 95 + 97 = 10,\na6 = 96 + 92 = 0,\na7 = 97 + 95 = 10.\nSo a = (2, 7, 0, 7, 15, 10, 0, 10). In a very similar fashion we obtain\nb = (3, 6, 13, 3, 1, 5, 4, 7).\n(e) Multiply the corresponding coefficients (over Z17) and compute the inverse DFT (re\nmember that in the DFT you will be using z = 2 rather than z-1, and that there will\nbe an additional factor n-1 (mod 17). Is this what you expected? How much is uv in\nbinary?\nSolution: Since we are regarding the numbers as polynomials, multiplying this poly\nnomials corresponds to a convolution of their coefficients, which in transform domain\ncorresponds to usual multiplication, so the transform of the coefficients of the multiplied\npolynomial corresponds to\nab = (6, 8, 0, 4, 15, 16, 0, 2).\nFirst notice that 8-1 = 2-3 = 25 = 32 = 15, and the powers of 2 are of course the\ninverse table to the powers of 9:\n20 = 1, 21 = 2, 22 = 4, 23 = 8, 24 = 16, 25 = 15, 26 = 13, 27 = 9.\nCall the coefficients of the multiplied polynomials dk for k = 0, . . . , 7, then we may\ncalculate them from ab using the inverse transform:\nd0 = 15(6 + 8 + 4 + 15 + 16 + 2) = 0,\nd1 = 15(6, 8, 0, 4, 15, 16, 0, 2)·(1, 2, 4, 8, 16, 15, 13, 9) = 15(6+16+32+240+240+18) = 1,\nd2 = 15(6, 8, 0, 4, 15, 16, 0, 2) · (1, 4, 16, 13, 1, 4, 16, 13) = 1,\nand similarly\nd3 = 2, d4 = 1, d5 = 1, d6 = 0, d7 = 0.\nSo d = (0, 1, 1, 2, 1, 1, 0, 0). If we remember we are looking at coefficients of a polynomial,\nwe get that the product is then 2 + 4 + 16 + 16 + 32 = 70, which is correct since our\ninitial numbers were 10 and 7. The product in binary can be seen by carrying over the\nelements in d, so we get 01000110(remember elements in d are in inverse order per our\ndefinition of u and v.\nHW11-3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 12",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/f5d55b27153af018c981da7f7ac82717_MIT18_310F13_Homework12.pdf",
      "content": "Fall 2013\n18.310 Homework 12\nDue Wednesday December 4th at 6PM\n1. Suppose you are encoding a source that emits one of three letters: a with probability 1\n2, b\nwith probability 1\n3 and c with probability 1\n6.\n(a) What is the Shannon bound on the best encoding of n letters from this source.\n(b) Use the Huffman algorithm to find an optimal prefix code for encoding this source. What\nis the number of bits used per letter?\n2. Now, consider the same source as in problem (1), but the new 9-letter \"alphabet\" consisting\nof all pairs of letters, so aa would have probability 1\n4, ab would have probability 1\n6, etc.\n(a) What is the Shannon bound on the best encoding of n \"letters\" from this source.\n(b) Use the Huffman algorithm to find an optimal prefix code for encoding this source. What\nis the number of bits used per \"letter'? Per letter of the original source?\n3. Level-Ziv encoding will be covered on Monday.\n(a) Suppose you encode n digits from the sequence\n12345678910111213141516171819202122 · · ·\nobtained by concatenating all natural numbers. Approximately how many bits will this\ntake to encode using Lempel-Ziv? By approximately, we mean that we care only about\nthe asymptotic growth as n gets large.\n(b) Suppose you encode n bits from the sequence\n01010101010101010101 · · ·\nobtained by alternating 0's and 1's. Approximately how many bits will this take to\nencode using Lempel-Ziv?\nHW12-1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 12 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/864d13f1289dc2a54177a33e8779fe79_MIT18_310F13_Homework12Sol.pdf",
      "content": "Fall 2013\n18.310 Homework 12\nDue Wednesday December 4th at 6PM\n1. Suppose you are encoding a source that emits one of three letters: a with probability 2 , b\nwith probability 1 and c with probability 1\n6 .\n(a) What is the Shannon bound on the best encoding of n letters from this source.\nSolution. The expected length per letter is at least\nH(p) = - log2\n-\nlog2\n-\nlog2\n= 1.4591 · · ·\n(b) Use the Huffman algorithm to find an optimal prefix code for encoding this source. What\nis the number of bits used per letter?\nSolution. To get the optimal prefix encoding, we take the two least frequent letters, b\nand c, and replace them by a letter α with pα = 0.5. The optimum for this new alphabet\nis (for example) a → 0 and α → 1, therefore we get as optimum for the original alphabet\na → 0b → 10c → 11\nThe expected length per letter transmitted is\n1 ·\n+ 2 ·\n+ 2 ·\n= 1.5.\n2. Now, consider the same source as in problem (1), but the new 9-letter \"alphabet\" consisting\nof all pairs of letters, so aa would have probability 1\n4 , ab would have probability 1 , etc.\n(a) What is the Shannon bound on the best encoding of n \"letters\" from this source.\n'\nSolution. The entropy of this new probability distribution p on A' = A × A (where\nA = {a, b, c}) is\n\nH(p')\n= -\n(i, j) ∈ A × Apipj log2(pipj )\n\n= -\npi log2(pi) -\npj log2(pj )\ni∈A\nj∈A\n=\n2H(p) = 2.9182 · · · .\nThis was expected by Shannon's theorem as generating independently n/2 'letters' from\nA' (with the associated probabilities) gives the same probabilistic source as generating\nn letters from A. Shannon's bound for n letters is thus 2.9182n + o(n).\nHW12-1\nSolutions\n\n(b) Use the Huffman algorithm to find an optimal prefix code for encoding this source. What\nis the number of bits used per \"letter'? Per letter of the original source?\nSolution. Our new alphabet A ' and associated probabilities (in non-increasing order)\nare:\naa\n1/4 = 9/36\nab\n1/6 = 6/36\nba\n1/6 = 6/36\nbb\n1/9 = 4/36\nac 1/12 = 3/36\nca 1/12 = 3/36\nbc 1/18 = 2/36\ncb 1/18 = 2/36\ncc 1/36 = 1/36\nThe best prefix code is given below (drawn upside down, as it was built by repeatedly\ncombining the two least likely letters).\nHW12-2\n\nThe Huffman code (with probabilities shown as well) is (there are several optimal ones):\naa 9/36 11\nab 6/36 01\nba 6/36 101\nbb 4/36 000\nac 3/36 1000\nca 3/36 1001\nbc 2/36 0011\ncb 2/36 00101\ncc 1/36 00100\nThis gives an encoding with an expected number of bits per 'letter' of A ' of\n(9 · 2 + 6 · 2 + 6 · 3 + 4 · 3 + 3 · 4 + 3 · 4 + 2 · 4 + 2 · 5 + 1 · 5) =\n= 2.9722 · · · ,\nor 1.4861 per letter of A.\n3. Level-Ziv encoding will be covered on Monday.\n(a) Suppose you encode n digits from the sequence\n12345678910111213141516171819202122 · · ·\nobtained by concatenating all natural numbers. Approximately how many bits will this\ntake to encode using Lempel-Ziv? By approximately, we mean that we care only about\nthe asymptotic growth as n gets large.\nSolution. Each new number will become a new dictionary phrase. So, the ith phrase\nwill require 4 + 1log2 il bits (since a decimal digit requires 4 bits) in its encoding. Now,\nn\nin the first n digits, there are approximately O(\n) phrases. So, the total number of\nlog10 n\nbits required is approximately\nn\nlog10 n\n4 + 1log2 il\ni=1\nn\nlog10 n\n≤\n5 + log2 i\ni=1\n\n5n\nn\n≤\n+ log2\n!\nlog10 n\nlog10 n\nUsing big-O notation and Sterling's approximation, the last line comes out to be about\nn\nn\nO(\nlog2\n), which is equivalent to O(n) (roughly a constant times n), so this\nlog10 n\nlog10 n\nstring does not compress very well using Lempel-Ziv.\n(b) Suppose you encode n bits from the sequence\n01010101010101010101 · · ·\nHW12-3\nX\nX\n\nobtained by alternating 0's and 1's. Approximately how many bits will this take to\nencode using Lempel-Ziv?\nSolution. Breaking up the first few phrases, we see the pattern:\n10101.\ni\nSo, the ith phrase takes approximately\ndigits (this could be proved formally by induc\n√\ntion). Therefore, the first n digits contains about 2 n phrases. Hence, the total number\nof bits required is about\n√\n2 n\n1 + 1log2 il\ni=1\n√\n√\nUsing the same analysis as above, we see that this is bounded above by 4 n+log2(2 n)!\n√\n√\n√\nand below by 2 n + log2(2 n)!, so in big-O notation, it is O( n log n), i.e., roughly a\n√\nconstant times\nn log n.\nHW12-4\nX\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/994dd0e3fb85d106f6d2a9e8c25e38cc_MIT18_310F13_Homework2.pdf",
      "content": "Fall 2013\n18.310 Homework 2\nDue September 18th at 6PM\nInstructions: Remember to submit a separate PDF for each question Do not forget to include\n.\na list of your collaborators or to state that you worked on your own.\n1. The following theorem and its proof are mathematically incorrect.\nLet n be any positive integer and recall that Zn = {0, 1, . . . , n - 1}. For a set\nX ⊆ Zn, let 2X = {2x (mod n) : x ∈ X}.\nTheorem 1. Suppose that X and Y are drawn independently and uniformly at\nrandom among all 2n subsets of Zn. Then P(2X ∩ Y = ∅) = 3n\n4n .\nProof. We know that a uniformly random set X can be generated by independently\ndeciding to include i in X with probability , for each i ∈ Zn. Thus, we obtain\nthat\n\nn-1\nn\nn\nP(2X ∩ Y = ∅) =\nP(if i ∈ X then (2i (mod n)) ∈/ Y ) =\n.\ni=0\n- To be handed in in recitation on 9/12/2013. Show that the theorem is false by\nexplicitly calculating P(2X ∩ Y = ∅) for n = 2. Does your counterexample generalize to\nn = 3? (Also, think about what step of the current proof is incorrect prior to recitation.)\n- Writing assignment. (To be submitted with the rest of the problem set on\n9/18/2013.) Correct the statement of the theorem above so that it is true for every n.\nProvide a well-written proof. Pay attention to notation, and also to the issues that\nmade the \"proof\" above wrong.\n2. The classroom that we are in has six blackboard frames. In some of the lectures, the instructor\nenjoys showing his (lack of) drawing skills and draws a pigeon on one or several board frames.\nShow that over the course of a semester with 36 lectures, there exist two lectures and three\nboard frames such that these three frames either all had no pigeons drawn on them in both\nlectures, or all had at least one pigeon drawn on them in both lectures.\n3. A random variable Y : Ω → Z is distributed according to the Poisson distribution with\nparameter λ ≥ 0 if for all i ≥ 0 :\nP(Y = i) = e-λ λi\n.\ni!\ninf\n- Verify that\ni=0 P(Y = i) = 1.\nHW2-1\n\n- Show that E[Y ] = Var(Y ) = λ.\n- Suppose that each random variable X1, X2, . . . , Xn follows the Poisson distribution with\nn\nparameter λi. Assume that all Xi are independent and let X =\ni=1 Xi. Show that, for\nμ ≥ E[X] and for all δ > 0 :\nμ\nδ\ne\nP(X > (1 + δ)μ) ≤\n.\n(1 + δ)(1+δ)\nHint: Compare this theorem with part (i) of Theorem 1 in the lecture notes on Chernoff\nBound. Try to follow the proof of Theorem 1 closely.\n4. You may have heard recently some story about former MIT students (and other groups)\nwinning a fair amount of money at the Massachusetts lottery game Cash WinFall (if not, just\ngoogle 'Cash WinFall MIT students'). Let's analyze the game (or some simplification of it).\nIn Cash WinFall, a customer can buy a ticket for $2 which let him/her choose 6 numbers\nbetween 1 and 46 hoping to match the 6 (distinct) numbers being randomly selected at the\nnext drawing. If the 6 numbers on the ticket match the 6 numbers that are drawn, he/she\nwins the jackpot, which is at least $500,000. The customer also wins prize money if 5, 4 or\n3 of the numbers are matched, see the second column in the table below for the prize money\nin each case.\nMatch\nPrize money\nExample (from 2/8/2010) of\nprize money when rolldown\n6 out of 6\n5 out of 6\n4 out of 6\n3 out of 6\njackpot\n$4,000\n$150\n$5\n-\n$22,096\n$807\n$26\nThe MIT students and the other groups exploited the fact that if the jackpot reaches $2,000,000\nand the jackpot is not won then part of the jackpot money is used to considerably increase\nthe prize money for matching 5, 4 or 3 of the numbers; see the third column in the table\nabove. Notice that the increase is more than 5-fold. Such a drawing is known as a rolldown\ndrawing. The precise increase for a rolldown drawing is based on formulas that are not (quite)\nrevealed to the public (and depends on the amount of the jackpot, etc.), but the increase is\nalways very significant and of the order of magnitude shown in the 3rd column above.\n(a) For i = 6, 5, 4, 3, what is the probability pi that one ticket matches precisely i of the 6\nnumbers that are randomly drawn? Give a formula and also numerically compute these\nprobabilities.\n(b) Let A be the event that one wins any amount of prize money when buying a single ticket.\nWhat is P(A)?\n(c) Let the random variable X be the prize money for a single ticket, assuming (i) that the\njackpot amount is $1,900,000 and (ii) that the drawing is not a rolldown drawing. What\nis E(X)? Compute its numerical value. (Should you play?)\nHW2-2\nP\n\n(d) Assume that we have a rolldown drawing (i.e. no one wins the jackpot which happens\nto be over $2,000,000). Suppose furthermore that the prize money for matching 5, 4 or\n3 numbers are as in the 3rd column in the table. Let Y be the prize money for a single\nticket under these assumptions.\nWhat is E(Y ) and Var(Y )? Compute their values.\n(e) If you purchase only one ticket, you have a large probability of not recovering your bet.\nNow suppose you purchase 1,000,000 tickets1, each randomly drawn. Let Z be the total\nprize money received.\nWhat is E(Z)? What is Var(Z)? Use Chebyshev's inequality to compute an upper\nbound on the probability that Z < 2, 000, 000 (i.e. that you are losing money).\n(f) Now use the Chernoff-Hoeffding bound to compute a better upper bound on the proba\nbility that Z < 2, 000, 000. How much better is your result?\n(g) If the jackpot goes over $2,000,000, a rolldown might not happen since some ticket might\nwin the jackpot. Suppose that, for a given drawing, the total number of (distinct) tickets\nsold2 is 1, 000, 000. Let B be the event that someone wins the jackpot. What is P(B)?\n-x\nTo evaluate this numerically, it is convenient to use the approximation3 1 - x ∼ e\n.\nThe following exercises should not be handed in, but we nevertheless encourage you to do them.\n1. The union bound states that, for any set of n events, the probability that at least one of the\nevents happens is no greater than the sum of the probabilities of the individual events, i.e.\n\nn_\nn\nX\nP\nAi\n≤\nP (Ai) ,\ni=1\ni=1\nn\nwhere\nAi = A1 ∨ A2 ∨· · · ∨ An. Prove the union bound using linearity of expectation.\ni=1\nRecall that the indicator variable for an event A, lA is a random variable that's 1 when A\nholds and 0 otherwise.\n2. Informally, this question asks you to show that any real number (even irrational) can be\napproximated by a fraction with small denominator without incurring too much error. More\nformally, prove that for any x ∈ R and for any n ∈ N, there exist integers p and q with\n1 ≤ q ≤ n satisfying\n\np\nx -\n<\n.\nq\nqn\nHint: Prove that there exist p and q with 1 ≤ q ≤ n such that |qx - p| ≤ 1 .\nn\n1The MIT students purchased up to 700,000 tickets for one drawing...\n2In 2004-2005, the number of tickets sold in anticipation of a rolldown drawing was never more than 950,000 and\ntypically less than 600,000 while in 2007, the number of tickets sold in a rolldown drawing was typically between\n1,200,000 and 1,400,000.\n-x\n-x\nOne has 1 - x ≤ e\nfor all x, and the approximation 1 - x ∼ e\nis very good for x close to 0.\nHW2-3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 2 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/4543b59437e4d6cbbd81daf7cc7b6471_MIT18_310F13_Homework2Sol.pdf",
      "content": "Fall 2013\n18.310 Homework 2 Solutions\n.\nInstructions: Remember to submit a separate PDF for each question Do not forget to include\na list of your collaborators or to state that you worked on your own.\n1. The following theorem and its proof are mathematically incorrect.\nLet n be any positive integer and recall that Zn = {0, 1, . . . , n - 1}. For a set\nX ⊆ Zn, let 2X = {2x (mod n) : x ∈ X}.\nTheorem 1. Suppose that X and Y are drawn independently and uniformly at\nrandom among all 2n subsets of Zn. Then P(2X ∩ Y = ∅) = 3n\n4n .\nProof. We know that a uniformly random set X can be generated by independently\ndeciding to include i in X with probability 1\n2, for each i ∈ Zn. Thus, we obtain\nthat\n\nn-1\nn\nn\nP(2X ∩ Y = ∅) =\nP(if i ∈ X then (2i (mod n)) ∈/ Y ) =\n.\ni=0\n- To be handed in in recitation on 9/12/2013. Show that the theorem is false by\nexplicitly calculating P(2X ∩ Y = ∅) for n = 2. Does your counterexample generalize to\nn = 3? (Also, think about what step of the current proof is incorrect prior to recitation.)\n- Writing assignment. (To be submitted with the rest of the problem set on\n9/18/2013.) Correct the statement of the theorem above so that it is true for every\nn. Provide a well-written proof. Pay attention to notation, and also to the issues that\nmade the \"proof\" above wrong.\nSolution. The correct statement is the following.\nTheorem 2. Suppose that X and Y are drawn independently and uniformly at random among\nall 2n subsets of Zn. Then\n\nP(2X ∩ Y = ∅) =\n\nrn\nif n is odd\nrn/2\nif n is even.\nProof. We know that a uniformly random set X can be generated by independently deciding\nto include i in X with probability 1\n2, for each i ∈ Zn. Similarly for Y . Thus we can see the\nHW2sols-1\n\nprocess of generating X and Y as coming from 2n independent coin tosses, n for X and n for\nY .\nNow define event Ai for i ∈ Zn to be the event that if i is in X then 2i mod n is not in Y .\nThe probability we are looking, P(2X ∩ Y = ∅), can thus be expressed as\n\nn-1\n\nP\nAi\n.\ni=0\nIt is easy to see that P(Ai) = 3 for any i.\nIn the case in which n is odd, we have that all n events (Ai)i=0,··· ,n-1 are independent since\neach depends on the result of two coin tosses (one for whether i is in X, the other whether\n2i is in Y ) and overall these correspond to distinct coins since 2i mod n is never equal to\n2j mod n unless i = j. Thus, we have that\n\nn\n-1\nn-1\nn\nn\nP\nAi\n=\nP(Ai) =\n.\ni=0\ni=0\nHowever, when n is even, the events Ai and Ai+n/2 are not independent as they both involve\nwhether 2i mod n are not in Y . Instead, we define the event Bi to be Ai ∧Ai+n/2 for 0 ≤ i < n\n2 .\nObserve that Bi is the event that if either i or i + n (or both) is in X then 2i mod n is not in\nY ; therefore P(Bi) = 1 - P(2i mod n is in Y )P(either i or i + n/2 is not in X) = 1 - 1\n= 8 .\n2 4\nWe can now write\n⎛\n⎞\n\n-1\n\nn\nn-1\n\n⎝\n⎠\nP\nAi\n= P\nBi\n.\ni=0\ni=0\nn\nNow all our events Bi's for i = 0, · · · ,\n- 1 are independent and thus we obtain:\n⎛\n⎞\n\nn\nn\n-1\n\n-1\nn-1\n\nn/2\nn\nP\nAi\n= P ⎝\nBi⎠ =\nP(Bi) =\n.\ni=0\ni=0\ni=0\n2. The classroom that we are in has six blackboard frames. In some of the lectures, the instructor\nenjoys showing his (lack of) drawing skills and draws a pigeon on one or several board frames.\nShow that over the course of a semester with 36 lectures, there exist two lectures and three\nboard frames such that these three frames either all had no pigeons drawn on them in both\nlectures, or all had at least one pigeon drawn on them in both lectures.\nSolution 1. In a lecture, there are 25 ways in which the instructor can draw or not draw a\npigeon in the first 5 frames. Since 25 = 32 < 36, there are two lectures in which the professor\nleaves the first 5 blackboards the same way. Suppose this way involves leaving 3 of those\nframes empty. In this case these three frames all had no pigeons drawn in both lectures. If\nthere are no 3 empty frames, there must be 3 frames that were drawn on. In this case these\n3 frames had pigeons in both lectures.\nHW2sols-2\n\nSolution 2. In any given lecture, there are at least two groups of three boards that either\nall have pigeons, or none of them do. Indeed, we have either (i) 3 boards with pigeons and 3\nboards without (and these are the two groups), or (ii) at least 4 boards all with pigeons or all\nwithout pigeons and in this latter case, we can choose any two subsets of size 3 of these (at\nleast) 4 boards. So in 36 lectures, we have at least 72 of these groups of three. These groups\nr\nof three can be any of 2 6 = 40 in a given lecture. Since 40 < 72 there are two among these\n72 that occupy the same boards, and they correspond to two such lectures.\n3. A random variable Y : Ω → Z is distributed according to the Poisson distribution with\nparameter λ ≥ 0 if for all i ≥ 0 :\nP(Y = i) = e-λ λi\n.\ni!\noinf\n- Verify that\ni=0 P(Y = i) = 1.\n- Show that E[Y ] = Var(Y ) = λ.\n- Suppose that each random variable X1, X2, . . . , Xn follows the Poisson distribution with\no n\nparameter λi. Assume that all Xi are independent and let X =\ni=1 Xi. Show that, for\nμ ≥ E[X] and for all δ > 0 :\nμ\nδ\ne\nP(X > (1 + δ)μ) ≤\n.\n(1 + δ)(1+δ)\nHint: Compare this theorem with part (i) of Theorem 1 in the lecture notes on Chernoff\nBound. Try to follow the proof of Theorem 1 closely.\nSolution:\n(a) This is just stating that\nλi\nλ\n= e ,\ni!\ni≥0\nwhich is true by Taylor's theorem.\n(b) This is just a calculation:\nλi\nλi\n-λ\nE[Y ] = e\ni\n= λe-λ\n= λ,\ni!\ni!\ni≥1\ni≥0\nand\n\ni2 λi\nλi\nλi\n-λ\n= λ2\nE[X2] = e\n= λe-λ\ni\n+ λe-λ\n+ λ,\ni!\ni!\ni!\ni≥1\ni≥1\ni≥0\nand since Var[Y ] = E[X2] - E[X]2, the result follows.\n(c) Let r > 0 be arbitrary. We start with the inequality\nrX\ne r(1+δ)μl{X>(1+δ)μ} ≤ e\n.\nBy the monotonicity of expectation, we get\nrX ).\ne r(1+δ)μP(X > (1 + δ)μ) ≤ E(e\nHW2sols-3\n\nUsing the independence of the variables Xi we can calculate\nn\nn\nnλ(er -1)\nE(e rX ) =\nE(e rXi ) = e\n.\ni=1\nwhere we have used that\nriλi\n-λ\ne\nλ(er-1)\nE(e rXi ) = e\n= e\n.\ni!\ni≥0\nPlugging this calculation into our inequality, and using that μ ≥ E(X) = nλ, we get\nP(X > (1 + δ)μ) ≤ exp[nλ(e r - 1) - r(1 + δ)μ] ≤ exp[μ(e r - 1 - r(1 + δ))].\nWe optimize the value of er - 1 - r(1 + δ) to obtain r = log(1 + δ), which in turn gives\nus\nμ\nδ\ne\nP(X > (1 + δ)μ) ≤\n.\n(1 + δ)(1+δ)\n4. You may have heard recently some story about former MIT students (and other groups)\nwinning a fair amount of money at the Massachusetts lottery game Cash WinFall (if not, just\ngoogle 'Cash WinFall MIT students'). Let's analyze the game (or some simplification of it).\nIn Cash WinFall, a customer can buy a ticket for $2 which let him/her choose 6 numbers\nbetween 1 and 46 hoping to match the 6 (distinct) numbers being randomly selected at the\nnext drawing. If the 6 numbers on the ticket match the 6 numbers that are drawn, he/she\nwins the jackpot, which is at least $500,000. The customer also wins prize money if 5, 4 or\n3 of the numbers are matched, see the second column in the table below for the prize money\nin each case.\nMatch\nPrize money\nExample (from 2/8/2010) of\nprize money when rolldown\n6 out of 6\n5 out of 6\n4 out of 6\n3 out of 6\njackpot\n$4,000\n$150\n$5\n-\n$22,096\n$807\n$26\nThe MIT students and the other groups exploited the fact that if the jackpot reaches $2,000,000\nand the jackpot is not won then part of the jackpot money is used to considerably increase\nthe prize money for matching 5, 4 or 3 of the numbers; see the third column in the table\nabove. Notice that the increase is more than 5-fold. Such a drawing is known as a rolldown\ndrawing. The precise increase for a rolldown drawing is based on formulas that are not (quite)\nrevealed to the public (and depends on the amount of the jackpot, etc.), but the increase is\nalways very significant and of the order of magnitude shown in the 3rd column above.\n(a) For i = 6, 5, 4, 3, what is the probability pi that one ticket matches precisely i of the 6\nnumbers that are randomly drawn? Give a formula and also numerically compute these\nprobabilities.\nHW2sols-4\nX\n\nSolution. We have that pi is\nr\nr\ni\n6-i\npi =\nr ,\nas we need to choose i numbers among the 6 winning ones, and 6 - i numbers among\nthe remaining 40. The values are:\np6 =\n= 0.000000106 · · ·\np5 =\n= 0.000025622 · · ·\np4 =\n= 0.001249090 · · ·\np3 =\n= 0.021095742 · · ·\n(b) Let A be the event that one wins any amount of prize money when buying a single ticket.\nWhat is P(A)?\nSolution. It is the probability that we win some prize, i.e.\np6 + p5 + p4 + p3 =\n= 0.022370561 · · ·\n(c) Let the random variable X be the prize money for a single ticket, assuming (i) that the\njackpot amount is $1,900,000 and (ii) that the drawing is not a rolldown drawing. What\nis E(X)? Compute its numerical value. (Should you play?)\nSolution. E(X) = 1900000p6 + 4000p5 + 150p4 + 5p3 = 0.59817 · · · . (As this is less\nthan the price of the winning, you shouldn't be playing if you are rational...)\n(d) Assume that we have a rolldown drawing (i.e. no one wins the jackpot which happens\nto be over $2,000,000). Suppose furthermore that the prize money for matching 5, 4 or\n3 numbers are as in the 3rd column in the table. Let Y be the prize money for a single\nticket under these assumptions.\nWhat is E(Y ) and Var(Y )? Compute their values.\nSolution. We have\nE(Y ) = 22096p5 + 807p4 + 26p3 = 2.12265658 · · ·\nAlso,\nE(Y 2) = 220962 p5 + 8072 p4 + 262 p3 = 13337.416 · · ·\nand thus\nVar(Y ) = E(Y 2) - E(Y )2 = 13332.910451 · · ·\n\nThe standard deviation is\nVar(Y ) = 115.46 (much larger than the expected earnings).\nHW2sols-5\n\n(e) If you purchase only one ticket, you have a large probability of not recovering your bet.\nNow suppose you purchase 1,000,000 tickets1, each randomly drawn. Let Z be the total\nprize money received.\nWhat is E(Z)? What is Var(Z)? Use Chebyshev's inequality to compute an upper\nbound on the probability that Z < 2, 000, 000 (i.e. that you are losing money).\nSolution. We have E(Z) = 1000000E(Y ) = 2122656.58, while Var(Z) = 1000000Var(Y ) =\n13332910451. (Not asked: Here the standard deviation is 115468, much more comparable\nto the expected profit of E(Z) - 2000000.)\nChebyshev's inequality says that\nVar(Z)\nP(|Z - E(Z)| ≥ 2122656.58 - 2000000) ≤\n= 0.886 · · · .\n122656.582\nThis implies that the probability that Z < 2000000 is less than 0.886.\n(f) Now use the Chernoff-Hoeffding bound to compute a better upper bound on the proba\nbility that Z < 2, 000, 000. How much better is your result?\n(g) If the jackpot goes over $2,000,000, a rolldown might not happen since some ticket might\nwin the jackpot. Suppose that, for a given drawing, the total number of (distinct) tickets\nsold2 is 1, 000, 000. Let B be the event that someone wins the jackpot. What is P(B)?\n-x\nTo evaluate this numerically, it is convenient to use the approximation3 1 - x ∼ e\n.\nSolution. We have\n-1000000/(46\n-0.10675983 = 0.1012585.\nP(B) = 1-P(¬B) = 1- 1 -\nr\n∼ 1-e\n6 ) = 1-e\n1The MIT students purchased up to 700,000 tickets for one drawing...\n2In 2004-2005, the number of tickets sold in anticipation of a rolldown drawing was never more than 950,000 and\ntypically less than 600,000 while in 2007, the number of tickets sold in a rolldown drawing was typically between\n1,200,000 and 1,400,000.\n-x\n-x\nOne has 1 - x ≤ e\nfor all x, and the approximation 1 - x ∼ e\nis very good for x close to 0.\nHW2sols-6\n\n!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/050ae617c3342f30ae0b8770eb41ca87_MIT18_310F13_Homework3.pdf",
      "content": "Fall 2013\n18.310 Homework 3\nDue September 27th at 6PM\nInstructions: Remember to submit a separate PDF for each question .Do not forget to include\na list of your collaborators or to state that you worked on your own.\n1. Writing Assignment: Following the discussion in recitation, revise Section 3 (and Section\n3 only) of the lecture notes on Chernoff Bound. You will find a LATEXversion of the notes in\nthe webpage for Homework 3 under the name \"chernoff.tex\".\nYou have complete editorial freedom and should feel free to add, delete and move text within\nSection 3, following the instructions given at the top of \"chernoff.tex\". You should pay\nparticular attention to instances of missing guiding or explanatory text, such as those that\nwere pointed out in recitation.\nAfter revising the notes, write a short supporting text (1-2 paragraphs) explaining your most\nsignificant revisions to the notes and the rationale behind them.\nPlease submit online the following documents: i-ii) pdf and latex of your revised lecture notes,\niii) pdf of your supporting text (as a separate document).\n2. A tournament on n vertices is an orientation of a complete graph on n vertices, i.e. for\nany two vertices u and v, exactly one of the directed edges (u, v) or (v, u) is present. A\nHamiltonian path in a tournament is a directed path passing through all vertices exactly\nonce. For example, in the example below, 1 - 2 - 3 - 4 is a Hamiltonian path since all edges\n(1, 2), (2, 3) and (3, 4) are directed in the direction of traversal and it visits every vertex once,\nwhile for example 1 - 2 - 4 - 3 is not a directed path since the second and third edges are\ndirected in the wrong direction.\nn!\nShow that, for any n, there exists a tournament with at least 2n-1 Hamiltonian paths.\nFor example for n = 4, this shows that there exists a tournament on 4 vertices with at least\n3 Hamiltonian paths. Here is such a tournament with Hamiltonian paths 1 - 2 - 3 - 4,\n2 - 3 - 4 - 1, 3 - 4 - 1 - 2, 4 - 1 - 2 - 3, 4 - 2 - 3 - 1.\n(Hint: think probabilistically...)\nHW3-1\n\n3. The examples of Chernoff bounds discussed in class bound the probability of deviating from\nthe expectation by a certain amount. We can also ask the reverse question of finding intervals\nwhere we have high confidence that the random variable will fall into.\nConsider flipping a fair coin 100 times. Using the Chernoff bounds shown in class, find x such\nthat with at least 95% probability we obtain at least x heads.\n4. Recall the sequential choice problem discussed in class: n objects are shuffled in random\norder, and are presented to you one at a time. For each object, you know how it compares\nagainst all previous ones that you've seen, However, we're now considering the setting in\nwhich, rather than your choice being final, you can alter it at any time. Of course, now the\nstrategy becomes obvious: you will switch to the current object if it is better than everything\nyou have seen so far (or equivalently better than the object you last selected). Find the\nexpected number of times you will select a new object.\nFormally, given a random permutation of [n], find the expected number of entries smaller\nthan all entries before it, and express the answer in terms of n.\n5. Consider 2n points on the plane labelled 1, 2, · · · , 2n, all spaced equally on a circle. A matching\nof these points is a collection of n straight line segments, with every point being the endpoint\nof precisely one of the line segments. A matching is noncrossing if no two of its line segments\ncross. Here is an example of a noncrossing matching on 8 points (so n = 4).\nDetermine (with proof) the number of noncrossing matchings of 2n points, as a function of\nn. (You might want to look for an appropriate bijection.) Check your result for n = 1, 2 and\n3.\nHW3-2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Homework 3 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/1240eaccf33fcaee613260cdd6b55a3f_MIT18_310F13_Homework3Sol.pdf",
      "content": "Fall 2013\n18.310 Homework 3 Solutions\nInstructions: Remember to submit a separate PDF for each question Do not forget to include\n.\na list of your collaborators or to state that you worked on your own.\n1. Writing Assignment: Following the discussion in recitation, revise Section 3 (and Section\n3 only) of the lecture notes on Chernoff Bound. You will find a LATEXversion of the notes in\nthe webpage for Homework 3 under the name \"chernoff.tex\".\nYou have complete editorial freedom and should feel free to add, delete and move text within\nSection 3, following the instructions given at the top of \"chernoff.tex\". You should pay\nparticular attention to instances of missing guiding or explanatory text, such as those that\nwere pointed out in recitation.\nAfter revising the notes, write a short supporting text (1-2 paragraphs) explaining your most\nsignificant revisions to the notes and the rationale behind them.\nPlease submit online the following documents: i-ii) pdf and latex of your revised lecture notes,\niii) pdf of your supporting text (as a separate document).\n2. A tournament on n vertices is an orientation of a complete graph on n vertices, i.e. for\nany two vertices u and v, exactly one of the directed edges (u, v) or (v, u) is present. A\nHamiltonian path in a tournament is a directed path passing through all vertices exactly\nonce. For example, in the example below, 1 - 2 - 3 - 4 is a Hamiltonian path since all edges\n(1, 2), (2, 3) and (3, 4) are directed in the direction of traversal and it visits every vertex once,\nwhile for example 1 - 2 - 4 - 3 is not a directed path since the second and third edges are\ndirected in the wrong direction.\nn!\nShow that, for any n, there exists a tournament with at least 2n-1 Hamiltonian paths.\nFor example for n = 4, this shows that there exists a tournament on 4 vertices with at least\n3 Hamiltonian paths. Here is such a tournament with Hamiltonian paths 1 - 2 - 3 - 4,\n2 - 3 - 4 - 1, 3 - 4 - 1 - 2, 4 - 1 - 2 - 3, 4 - 2 - 3 - 1.\n(Hint: think probabilistically...)\nSolution:\nFor every undirected edge {u, v} choose with probability 1/2 to orient it (u, v),\nand with probability 1/2 to orient it (v, u). Do this independently for every edge in the\nHW3sols-1\n\ncomplete graph Kn. Clearly the resulting graph G is a tournament, since for every pair of\nvertices there is in the end exactly one directed edge. Now given a permutation σ of [n], the\nprobability that this permutation corresponds to a Hamiltonian path in G is\nn-1\nn\nP(∀i = 1, . . . , n - 1\n(σi, σi+1) ∈ E(G)) =\nP((σi, σi+1) ∈ E(G)) =\n,\n2n-1\ni=1\nbecause every edge is chosen independently and has a 1/2 chance of appearing. Now define\nX to be the number of Hamiltonian paths in G. Clearly X is a random variable and\n\nX =\nXσ,\nσ∈Sn\nwhere Xσ is the random variable that takes the value 1 when σ is a Hamiltonian path and 0\notherwise, and Sn is the set of permutations of [n]. Then by linearity of expectation\n\nn!\nE(X) =\nP(Xσ = 1) =\n,\n2n-1\nσ∈Sn\nsince there are n! permutations of [n]. But this implies that there is a choice of directed edges\nthat achieves at least this number of Hamiltonian paths, otherwise\nn!\n2n\n-1 -1\n\nn!\nn!\nE(X) =\nkP(X = k) =\nkP(X = k) ≤\n- 1\nP(X = k) <\n.\n2n-1\n2n-1\nk≥0\nk=0\nk≥0\n3. The examples of Chernoff bounds discussed in class bound the probability of deviating from\nthe expectation by a certain amount. We can also ask the reverse question of finding intervals\nwhere we have high confidence that the random variable will fall into.\nConsider flipping a fair coin 100 times. Using the Chernoff bounds shown in class, find x such\nthat with at least 95% probability we obtain at least x heads.\nSolution: We will use the lower tail Chernoff bound\n-μδ2/2\nP(X < (1 - δ)μ) ≤ e\n,\nfor all 0 < δ < 1, valid when X is a sum of independent, identically distributed Bernoulli\nrandom variables, and μ = E(X). Let X be the number of heads. Obviously it satisfies the\nhypotheses of the Chernoff bound. Note that,\nE(X) = E(100 - X) = 50,\nso applying the Chernoff bound we obtain\n-25δ2\nP(X < (1 - δ)50) ≤ e\nor equivalently\n-25δ2\nP(X ≥ (1 - δ)50) ≥ 1 - e\n.\nHW3sols-2\n\n-25δ2\nNow 1 - e\n= 0.95 if and only if\n\nln 20\nδ =\n∈ (0, 1).\nWe then conclude that if\nx = (1 - δ)50 > 32\nthen\nP(X ≥ 32) ≥ P(X ≥ x) ≥ 19/20.\n4. Recall the sequential choice problem discussed in class: n objects are shuffled in random\norder, and are presented to you one at a time. For each object, you know how it compares\nagainst all previous ones that you've seen, However, we're now considering the setting in\nwhich, rather than your choice being final, you can alter it at any time. Of course, now the\nstrategy becomes obvious: you will switch to the current object if it is better than everything\nyou have seen so far (or equivalently better than the object you last selected). Find the\nexpected number of times you will select a new object.\nFormally, given a random permutation of [n], find the expected number of entries smaller\nthan all entries before it, and express the answer in terms of n.\nSolution: Take σ to be a random permutation of [n]. Given a position i ∈ [n] the probability\nthat all entries before position i were smaller than σi is equal to 1/i, since each position\nbetween 1 and i is equally likely to be the maximum of these i values. Now define the random\nvariable X to be the number of entries that are bigger that its predecessors. Then\nn\nn 1\nE(X) =\nP(σi > σj : ∀j < i) =\n= Hn,\ni\ni=1\ni=1\nwhere Hn is the n-th Harmonic number.\n5. Consider 2n points on the plane labelled 1, 2, · · · , 2n, all spaced equally on a circle. A matching\nof these points is a collection of n straight line segments, with every point being the endpoint\nof precisely one of the line segments. A matching is noncrossing if no two of its line segments\ncross. Here is an example of a noncrossing matching on 8 points (so n = 4).\nHW3sols-3\nX\nX\n\nDetermine (with proof) the number of noncrossing matchings of 2n points, as a function of\nn. (You might want to look for an appropriate bijection.) Check your result for n = 1, 2 and\n3.\nSolution: We will show that the number of noncrossing matchings of 2n points is the Catalan\n1 s2n\nnumber Cn =\n, by showing a bijection between these drawings and Dyck paths of\nn+1 n\nlength 2n. Define a Dyck path from a drawing by setting the step in the i-th position to be\nan up step if i is connected to j where j > i and a down step otherwise. First we need to prove\nthat the result is a Dyck path. First each edge has one end that is going to correspond to an\nup step, and one that will correspond to a down step, so the path ends in zero. Furthermore,\ntake j to be a down step position in the path. Take i < j to be the corresponding up step\nposition(i.e. the vertex connected to j in the matching). Since there are no edges crossing ij,\nall steps given by edges between the numbers {i + 1, . . . , j - 1} sum to zero. And then the\nsame is true for the steps given by edges between the numbers {i, . . . , j}. If at position j the\nsum of up steps and down steps resulted in a negative position, the last argument shows that\nthere is a position before i that also has this property. And since this cannot go on forever,\nand at the beginning we are in zero, there is no such j. To see that this is a bijection, given a\nDyck path and an up step in position i connect i to j where j is the position of the first down\nstep at the same height as the step in i. Clearly there is such a step, since the Dyck path goes\nback to zero, and also this step occurs before any other up step of the same height, because\na Dyck path is continuous, so for every i up step there is a unique j. Now paths don't cross\nsince if i1 < i2 < j1 the height of i2 must be greater than that of i1 otherwise j1 would have\noccurred before. This implies j2 < j1 by the continuity of the Dyck path. So this drawing is\na noncrossing matching, whose image is clearly the original Dyck path, so we have found a\nbijection.\nHW3sols-4\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "18.310 Exam 1 practice questions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/613c051a8fcc487c36dd83e85d32f699_MIT18_310F13_PracExam1.pdf",
      "content": "18.310 Exam 1 practice questions\nCollection of problems from past quizzes and other sources. It does not necessarily reflect\nwhat will be on the exam on Friday.\n1. Suppose one has two coins C1 and C2. Coin C1 gives head with probability 1/2, while\ncoin C2 gives head with probability 1/3. We pick one of the coins uniformly at random\nand toss it twice. We get twice the same result. Compute the probability the it was coin\nC1 being used.\n2. You have a biased coin that comes up heads with probability 1/3. Show that the prob\nability of obtaining 80 heads or more from 90 throws is not more than 0.16.\n3. In a permutation of n elements, a pair (j, i) is called an inversion if and only if i < j and i\ncomes after j. For example, the permutation 31542 in the case n = 5 has five inversions:\n(3,1), (3,2), (5,4), (5,2), and (4,2). What is the expected number of inversions in a\nuniform random permutation of the numbers 1, 2, · · · , n?\n4. Prove that if C is any subset of {100, 101, . . . , 199} with |C| = 51, then C contains two\nconsecutive integers.\n5. You have an n × 3 strip of unit squares. Let an be the number of ways you can tile it\na\nwith 1 × 1, and 3 × 3 squares. Find the generating function A(x) =\nn≥0 anxn .\n6. Consider the generating function\ninf\nf(x) =\nanx n .\nn=0\nSuppose\n2 + 2x\nf(x) = 1 - 2x - x2 .\nGive an expression for an. Can you give a recursion for an and initial conditions that\nwould give rise to this generating function?\n7. Explain why the following are not valid bijections from [0 . . . n - 1] to [0 . . . n - 1]\n1. f(x) = x + 1\n2. f(x) = 2x mod n\n3. f(x) = n - 1 - x\n4. f(x) = x2\n\n8. Consider a game with three gates, one of which (chosen uniformly) has a prize hidden\nbehind. The player picks one gate, after which one of the other two gates that does\nnot contain the prize is revealed. The player can then choose to change or stay with\ntheir choice (of course, it does not make sense to pick the opened gate). Compute the\nprobability of winning if:\n- The player never switches.\n- The player always switches.\n9. A chess board is an eight-by-eight square grid, and a rook is a piece that can attack\nanything in the same row or column. Compute the number of ways of placing six rooks\non a eight-by-eight chess board such that no two rooks attack each other.\n10. Give a (short) expression for the number of balanced bracket sequences of length 2n that\nstarts with ((( for all n ≥ 3.\nPage 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "18.310 Exam 2 practice questions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/b9969506844bc7b9d48c86d41b078958_MIT18_310F13_PracExam2.pdf",
      "content": "18.310 Exam 2 practice questions\nCollection of problems from past quizzes and other sources. It does not necessarily reflect\nwhat will be on the exam on Friday.\n1. (From quiz, Fall 2012; was one out of 5 questions.) Consider a 2-player zero-sum game\nwith payoff matrix\n\nA =\n.\n1. Let y1, y2 denote a mixed strategy for player 1. Write a linear program that gives\nthe mixed strategy for player 1 that maximizes his expected payoff.\n2. Write the dual of the above linear program.\n3. Is the strategy y1 = y2 = 1\n2 optimal for player 1? Explain your reasoning. What is\nthe expected payoff for player 1 if he plays y1 = y2 = 2\n1 ?\n4. What is an optimum strategy for player 2?\n2. (Practice problem 2012.) Determine the dual of the following LP:\nmin 6x - 3y - z\ns.t. 4x - 2y + z = 4\nx + 3y - z ≥ 2\nx, y, z ≥ 0.\n3. (From quiz, Fall 2010; was one of 4 questions.)\nSuppose that we are doing heapsort. At some point, the numbers in our tree are ar\nranged as follows:\n23 28\n(a) (5 points): How would this heap be stored in an array?\n(b) (5 points). Which numbers have already been processed, and should now be con\nsidered inactive?\n(c) (10 points). What will the tree look like after the next step, when it has been\nturned into a heap again?\n\n4. (From Quiz, Fall 2010; was one out of 4 questions.) For each question below, asnwer\nTrue or False and give a one-line justification.\n(a) A heap on n elements can be built with O(n) comparisons, i.e. with a number of\ncomparisons bounded by a constant times n.\nTRUE or FALSE\n(b) The pigeonhole principle implies that the number of comparisons required for merg\nr\nc\ning two sorted arrays of size n/2 is at least log2 n/\nn\n2 .\nTRUE or FALSE\n(c) From the construction of Batcher's network described in lecture, one can obtain a\nnon-adaptive algorithm to merge two sorted arrays of size n/2 with a linear number\nof comparisons.\nTRUE or FALSE\n(d) A heap with k levels can store n keys where 2k-1 ≤ n < 2k - 1.\nTRUE or FALSE\n5. (From a 2010 problem set.) Prove the following: To show that a sorting network on n\ninputs correctly sorts any input, one only needs to consider all inputs with 0's and 1's\n(there are 2n of them). (This is much less than trying all permutations, which would be\nn!.)\n6. (Old problem set question, modified. ) Suppose you have a nonnegative (i.e. all entries\nare nonnegative) m × n matrix A such that all row sums\nn\nm\nri :=\naij\nj=1\nfor i = 1, · · · , m and all column sums\nm\nm\ncj :=\naij\ni=1\nfor j = 1, · · · , n are all integers. Then show that there exists a matrix B with\n1. bij = 0 if aij = 0, and\n2. the same row sums and column sums, and\n3. with all entries being integers.\n7. (From a pset.) Derive an upper bound on the number of comparisons needed to find the\nmedian based on partitioning into subarrays of 7 elements. You may use the fact that 7\nelements can be sorted with 13 comparisons.\nPage 2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "18.310 Exam 2 practice questions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/e44b77a088f3978b0b3fd2bf50f7a34a_MIT18_310F13_PracExam3.pdf",
      "content": "18.310 Exam 3 practice questions\nCollection of problems from past quizzes and other sources. It does not necessarily reflect\nwhat will be on the exam on Friday.\n1. What is the gcd of 161 and 119. Show your calculations.\nFor each of the following numbers, write it as a linear combination of 161 and 119, if\npossible; if it is not possible, explain why not.\n- 21\n- 15\n2. (From final, spring 2013.)\n1. State Lagrange's theorem.\n2. Let p be a prime number, and suppose that (G, ·) is a group of order p2 . Let x ∈ G,\nwith x = e (e being the identity element of the group).\nLet H = {xk : k ∈ Z}. Prove that either H = G, or else |H| = p.\n3. (From quiz Spring 2012)\nAlice chooses primes p = 5 and q = 11. The public key she generates is the pair (55, 7)\nand the private key is (55, 23). Bob wants to encrypt the message m = 53 and send it\nto Alice. What is the encrypted message?\n4. (From quiz, Fall 2010.)\nSuppose a message m is encoded using RSA with the public key N = 77 and z = 43,\ni.e.\ns ≡ m z\n(mod 77),\nand the encoded message is s = 2. Compute (numerically) m. Explain your steps.\n5. (From quiz Spring 2012)\n1. How many elements of {0, 1, 2, . . . , 134} are divisible both by 3 and by 5? (Note\nthat 135 = 33 · 5).\n2. Determine the cardinality (number of elements) of the multiplicative group (Z∗\n135, ⊗).\n6. (From quiz spring 2012)\nLet a0, a1, . . . , an-1 be real numbers. Let c0, c1, . . . , cn-1 be the coefficients associated\nto the sequence a0, a1, . . . , an-1 by the Discrete Fourier Transform. Let ck = ck for all\nk = {0, . . . , n - 1} except c0 = c1 + 1. Let a0\n0, a0\n1, . . . , a0\nbe the number obtained\nn-1\nfrom c0, c1, . . . , c\nby the inverse Discrete Fourier Transform. Express a0, a1, . . . , a\nn-1\nn-1\nin terms of a0, a1, . . . , an-1.\n\n7. (From quiz, spring 2012)\nLet z be a primitive 32nd root of unity modulo p.\n1. Give two possible values for p.\n2. Write the formula for the discrete Fourier transform a0, a1, · · · , a31 of a sequence\ny0, y1, · · · , y31 modulo p and using z as 32nd root of unity.\n3. Let yi = 1 for i = 0, · · · , 31. What is its discrete Fourier transform?\n8. (From quiz, Fall 2012; was one out of 5 questions.)\nSuppose you have a random source which outputs independent letters from an alphabet\nof size k, and each letter is equally likely (its probability is thus 1/k).\n1. In expectation, how many bits does the best code use to compress a string of n\nletters from this source?\n2. Give a value of k for which the best code is not a prefix code. Explain.\n3. Give values of k for which this best code can be chosen to be a prefix code. Justify\nyour answer.\n9. (From quiz, Fall 2010; was one of 4 questions.)\nConsider the following probabilities for the alphabet A = {A, B, C, D, E, F }.\nA 0.15\nB 0.13\nC\n0.2\nD\n0.1\nE 0.34\nF 0.08\n(a) Derive the optimum Huffman code for A with the probabilities above.\n(b) How would you encode CAFE?\n(c) What is the expected number L of bits per encoded letter?\n10. (From quiz Fall 2012; was one out of 5 questions.)\nSuppose we have two sequences of length 32, fk and ak, related by\n-2πijk/32\nfk =\naj e\nj=0\nfor k = 0, 1 · · · , 31.\n1. Give a formula for aj in terms of the fk's.\nPage 2\n\n2. Suppose that\n-20πik/32\nfk = e\n,\nfor k = 0, 1 · · · , 32. Give an explicit expression for the numbers aj . This answer\nshould have no summation.\n11. (From quiz Spring 2012)\nConsider a source which outputs independent random letters from the alphabet A =\n{a, b, c, d, e} with probabilities pa = 1/4, pb = 1/4, pc = 1/6, pd = 1/6 and pe = 1/6.\n1. Give a Huffman code for this source.\n2. Let Ln be the random length of the Huffman code for a random sequence of n\nletters from that source. Compute the expectation E(Ln).\n3. Let Ln be the (random) length of the Lempel-Ziv code for a random sequence of\nE(Ln)\nE(L0 )\nn\nn letters from that source. Say if limn→inf\nn\nand limn→inf\nn\nare equal, and if\nnot which one is larger.\n12. Consider the message\naababcabcdabcde.\nDescribe the decomposition into phrases that will be used by Lempel-Ziv, and give the\nencoded string obtained using Lempel-Ziv. When encoding a letter, use the mapping\na → 000,\nb → 001,\nc → 010,\nd → 011,\ne → 100.\nPage 3\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Analyzing Randomized Median Finding and Quicksort Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/7db820f51ad2de2340639a7ea14b7795_MIT18_310F13_Ch12.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nAnalyzing Randomized Median Finding and Quicksort\nLecturer: Michel Goemans\nWe now know enough probability theory that we can analyze the expected number of com-\nparisons taken by two randomized algorithms, one for median finding (or any rank m element)\nand the other for sorting. Th latter, known as Quicksort is one of the most widely used sorting\nalgorithms.\nRandomized Median Finding\nIn a previous lecture, we discussed the problem of finding the median of a list of m elements, or\nmore generally the element of rank m. The approach we followed was to select a pivot element p,\nto compute its rank, and partition the list into 2 sublists, one containing the elements smaller than\np, the other containing those larger than p. By comparing m, the desired rank, to the size of these\nlists, we can restrict our search to one of the lists, and proceed recursively. If we select an arbitrary\nelement for p, we might be unlucky and be able to discard very few elements; in the worst-case,\nthis may lead to cn2 comparisons for some c. By carefully selecting a good candidate as pivot\nelement, we were able to devise a rank m selection algorithm that makes at most cn comparisons\nin the worst-case.\nBut this was a pretty complicated algorithm and analysis, and instead, we\npropose now a simple probabilistic approach. Just select as pivot p an element selected uniformly\nat random among all elements in the list. This is very simple to implement. In the worst-case, we\nmay be unlucky at every step and perform roughly cn2 comparisons, but that is an extremely rare\nsituation (an event with extremely tiny probability). In fact, on average, i.e. in expectation, this\nsimple randomized algorithm performs at most cn comparisons for some appropriate c. We'll show\nthis now.\nLet fn be the number of comparisons this algorithm performs on an input of n elements. This\nis a random variable as this depends on our random choice of pivot at every level of the recursion.\nWe want to compute, or at least upper bound, E(fn). We claim that E(fn) ≤4n.\nDescribing the sample space precisely and assigning/computing the probability of each sample\npoint does not seem to be trivial. Rather, to get a bound on the expected value of fn, we will\ntry to isolate the first level of the recursion from subsequent levels by exploiting independence and\nmaking use of indicator random variables.\nBefore we proceed, let us observe that when we partition our list into two sublists, which one\nwe keep depends on the rank m we need to select. To make the analysis valid for any value of m,\nlet's assume we always keep the larger of the two sublists. This is as if an adversary would tell us\nwhich sublist to keep and which to discard. If the sublist with elements smaller than p has size l\nand the other sublist has size n -1 -l(the pivot element p is in neither list), the list we keep has\nsize max(l, n -1 -l). Observe that lis a uniform random variable taking values 0, 1, · · · , n -1,\nthus\nP(l= i) =\n,\nn\nQuicksort-1\n\nfor i = 0, 1, · · · , n -1. Let k be a random variable denoting the length of the list we keep after the\nfirst pivot. Since both l= 0 and l= n -1 would result in keeping a sublist of length k = n -1,\nthe probability that the list we keep has length n -1 is P(k = n -1) = 2 . More generally, we have\nn\nthat\nP(k = i) = n\nfor i = n, n + 1,\n, n\n2, n\n1. This is not quite correct if n is odd (e.g. if n = 5, we have that\nP\n· · ·\n-\n-\n(l= 2) = 1/5, P(l= 3) = 2/5, P(l= 4) = 2/5), but this can be fixed easily. For simplicity, we'll\ndo the calculations as if we only deal with even sized lists.\nTo compute E(fn), let us introduce the event Ak that the sublist we keep after the first level\nhas size k. Observe that for different values of k, these events are disjoint, that is Ak1 ∧Ak2 = ∅\nfor k1 = k2, while they partition S, that is An/2 ∨An/2+1 ∨· · · ∨An-1 = S. Thus,\nfn = fn\n\nIAn/2 + IAn/2+1 + · · · + IAn-1\n\n.\nWhat multiplies fn in the expression above is (a random variable) always equal to 1, no matter\nwhat happens. Therefore,\nE(fn) = E\n\nfn\n\nIAn/2 + IAn/2+1 + · · · + IAn-1\n\n,\nand by linearity of expectations, this is equal to:\nE(fn) = E(fnIAn/2) + E(fnIAn/2+1) + · · · + E(fnIAn-1).\nLet us consider any such term, say E(fnIAk) for some fixed value k. Here k is not a random variable\nbut just a given value, say imagine that k = 857. This is the expectation of a product, and recall\nthat this is not necessarily the product of the expectations, unless the two random variables are\nindependent. In order to get independence, we write fn as the sum of the number of comparisons\nin the first level (which is equal to n-1 since we have to compare the pivot to every other element)\nand the number of comparisons in subsequent levels. But observe that fnIAk is the same random\nvariable as (n -1 + fk)IAk where fk denotes the number of comparisons we perform on a list of k\nelements, since both random variables take value 0 on any sample point corresponding to a different\nvalue of k. But IAk depends only on the the choice of our first pivot, while fk depends only on our\nchoices of subsequent pivots, and thus we have independence and can write:\nE(fnIAk)\n=\nE((n -1 + fk)IAk) = E((n -1)IAK) + E(fkIAk)\n=\n(n -1)E(IAk) + E(fk)E(IAk)\n=\n((n -1) + E(fk)) E(IAk).\nUsing E(IAk) = P(Ak) and summing over k, we get:\nn-1\nE(fn)\n=\n\n(n -1 + E(f )) P\nk\n(Ak)\nk=n/2\n⎛\nn-1\n=\n(n -1)\n\nP\n⎞\nn-1\n(A\n)\n⎝\nE\nk)\n+\n\n(f\nk=n/2\n⎠\nP\nk\n(Ak)\nk=n/2\nn\n-1\n=\n(n -1) + n\n\nE(fk).\nk=n/2\nQuicksort-2\n\nWe have thus obtained a recurrence relation for E(fn) and we simply need to solve it. Let's show\nby induction that E(fn) ≤cn for some value of n. We check the base case and also assume as\ninductive hypothesis that E(fm) ≤cm for all m < n. Thus we get that\nn\n-1\nE(fn)\n=\n(n -1) + n\n\nE(fk)\nk=n/2\nn\n-1\n≤\n(n -1) +\nc\nn\n\nk\nk=n/2\n<\n(n -1) + c n\n3c\n<\n(1 +\n)n\n≤\ncn,\nprovided that we choose c ≥4. This proves by induction that E(fn) ≤4n. This randomized rank\nm selection algorithm therefore makes an expected number of comparisons linear in n (even though\nin the worst-case it is quadratic).\nQuicksort\nWe now analyze Quicksort, one of the most widely used sorting algorithms. Quicksort is an\nalgorithm that can be implemented in place; that is, we do not need more storage than is required\nto store the number of keys we are sorting.\nHow does Quicksort work? The first step is to choose a random key. We call this key the\npivot. We then divide all the other keys into ones larger and smaller than the pivot. Now, we put\nthe keys less than the pivot before it, and the keys greater than the pivot after it. This gives us\ntwo lists which we still need to sort: the list of keys smaller than the pivot and those larger than\nthe pivot. Let's give an example. Assume that we have the following array to sort:\n10,\nSuppose we choose 7 as our random key. Putting those keys smaller than 7 before it and those\nlarger than 7 after it, we get the following array:\n10.\nNow, we have two new lists to sort: the first consisting of six numbers (6 3 1 2 4 5), and the\nsecond of three. We sort these lists recursively, applying Quicksort to each list. Although in\nthis example, we have kept the relative order within the two lists, there is in general no reason an\nimplementation of Quicksort needs to do this (and it does make it harder to program). This first\nstep takes n -1 comparisons, as we have to compare the pivot to every other key.\nHow many comparisons does Quicksort take? First, let's look at the two extremes, that is,\nthe worst-case and the bast-case performance. Take k to be the number of keys that are less than\nthe pivot. Then n-k-1 are the number of keys that are larger than the pivot, and we use recursion\non lists of size k and size n -k -1.\nQuicksort-3\n\nIf we let f(j) be the number of comparison Quicksort takes to sort j items, we thus get the\nrecursion\nf(n) = n -1 + f(k) + f(n -k -1)\nThe best-case running time for Quicksort occurs when the pivot is always in the exact middle\nof the list. If we were very lucky, and the pivot always divides the list into two nearly equally sized\nlists at each step, we get the recursion equation\nf(n) ≤2f(n/2) + n -1.\n(Actually, this is slightly different depending on whether n is even or odd, but this only makes a\nvery small difference to the analysis.) This recursion is very similar to an equation we have seen\nbefore, and solves to f(n) ≈n log2 n.\nIf, on the other hand, we are really unlucky in the choice of the pivot, and always chose a pivot\nthat was either the smallest or largest key in the list, then we get the equation\nf(n) = n -1 + f(n -1)\nwhich gives\nf(n) = (n -1) + (n -2) + (n -3) + . . . + 1,\nso f(n) = n(n -1)/2 ≈n2/2. This case happens if you always choose the first key in your list to\nbe sorted, and you start with a list that is already sorted. Since in practice, many lists which need\nto be sorted are already nearly sorted, choosing the first key as the pivot is not a good idea.\nIf we have some deterministic algorithm for picking the pivot, then we can arrange the input\nso the pivot is always the first element of the sublist, and Quicksort will take around n2/2\ncomparisons on this input. How can we do better? What we can do is always choose a random\nkey in the list. If we do this, we can show that the expected number of comparisons taken by\nQuicksort is cn log n.\nIf we let f be the random variable which gives the amount of time taken by the algorithm on\nan input of size n, then we have by linearity of expectation,\nE(f(n)) = n -1 + E(f(k)) + E(f(n -k -1))\nwhere k is a random variable which is uniformly distributed between 0 and n -1. One way to\nanalyze Quicksort is to solve this equation.\nHowever, there's another very clever way which\nillustrates the use of indicator variables and linearity of expectations, and we will explain this now.\nWe will compute the probability that the rank j and rank k keys are compared in Quicksort.\nTo get some intuition into this, let's look at the extreme cases first. If j = i + 1, then Quicksort\n(and in fact, any sorting algorithm) must compare these keys, as otherwise there would be no way\nto tell which was larger. In the Quicksort algorithm, what happens is that they remain in the\nsame sublist until one of them is chosen as a pivot, at which point they are compared. If i = 1 and\nj = n, then the first key chosen as a pivot will separate them into two sublists, except in the case\nwhen one of them is chosen as the first pivot; in this case they will be compared on the first step.\nThus, the probability that these keys are compared is 2/n.\nLet us consider the probability the the i'th rank key and the j'th rank key get compared for\narbitrary i and j. There are j -i -1 keys strictly between these two keys, and j -i + 1 keys in\nbetween if we also include i and j. What happens when we run Quicksort? As long as these\nQuicksort-4\n\ntwo keys remain in the same sublist, the possibility exists that these keys will be compared later\nin the algorithm. When we process this sublist, if we do not pick either of these two keys, or any\nof the j -i + 1 keys between them as a pivot, they will remain in the same sublist. If we pick one\nof these two keys as a pivot, then they will be compared. If we pick one of the keys between them,\nthen the i'th and j'th rank keys will be placed in different sublists, and so will never be compared.\nThus, there is exactly one critical step that determines whether the i'th and j'th rank keys will\nbe compared. This is the first pivot that picks either one of these keys or one of the keys between\nthem (and there must exist a critical step, otherwise this would not be a correct sorting algorithm).\nGiven that this pivot step picks one of these j -i + 1 keys, the conditional probability that it picks\nany particular one of these keys is 1/(j -i + 1). Thus, the probability that key i and key j are\ncompared is the probability that one of these is picked on this step, which is 2/(j -i + 1).\nNow, let Ii,j be the indicator variable which is 1 if keys i and j are compared by our algorithm\nand 0 otherwise. The number of comparisons needed by Quicksort is\nC =\n\nIi,j\ni<j\nso by linearity of expectation, we can take the expectation on both sides and get\nE(C) =\n\nE(Ii,j).\n1≤i<j≤n\nWhat is E(Ii,j)? It is 1 if we compare them, and 0 if we don't, so the expectation of Ii,j is exactly\nthe probability that we compare the rank i and rank j keys, or 2/(j -i + 1). Thus, we have\nE(C) =\n\n.\nj\n1≤i<j≤n\n-i + 1\nWe can count that there are n -k pairs of keys whose difference is exactly k, so\nn-1\nn\nE(C) =\n\n(n -k)\n≤2n\nk + 1\n\n.\nh\nk=1\nh=2\nn\nThe harmonic series\nh=1\nis approximately ln n (you can show this by bounding the sum from\nh\nabove and below by an integral that you can compute exactly). Thus, we have an upper bound\nof 2n ln n for E(C). Looking at the sum above more carefully, it is not hard to check that to first\norder this bound is correct, and E(C) ≈2n ln n.\nThere is one last thing to do: we claimed that Quicksort was an algorithm that sorts \"in\nplace,\" that is, without using any extra workspace, and we haven't showed how to do this. The\nonly hard part of doing this is the first step: namely, rearranging the list to put the keys smaller\nthan the pivot first and the keys larger than the pivot last? There are several ways of doing this,\nand the one presented in class is different from the one in these notes. Let's first put the pivot at\nthe end where it will be out of the way. Now let us first assume that we know where the pivot goes.\nWe'll put an imaginary dividing line there. Doing this with our example,\nQuicksort-5\n\nwith 7 as the pivot, gives\n|\n|\n7.\nNow, the correct number of keys are on each side of this dividing line. This means that the number\nof out-of-place keys on the right is the same as the number of out-of-place keys on the left. [This\nisn't hard to prove, but we'll leave it as an exercise for you.]\nNow, we can go through the list on the right of the dividing line and the list on the left, one\nkey at a time, and when we find two out-of-place keys we can swap them. Once we've swapped all\nthe out-of-place keys, the correct number will be on each side of the line, since there are an equal\nnumber of out-of-place keys on either side. Let's look for the misplaced keys by working from the\noutside in, and comparing each key to the pivot. The first misplaced keys we find are 9 (on the\nleft) and 5 (on the right). Swapping them gives\n|\n|\n7.\nThe next two are 10 and 4. Swapping them gives\n|\n|\n7,\nand now all the keys are on the correct size of the line. Now, we can swap the pivot and the first\nkey to the right of the dividing line, to get\n|\n|\n8.\nThis places the keys smaller and larger than the pivot on the correct side of the pivot, and we're\nready for the next step of Quicksort.\nBut how do we know where to put the dividing line? In fact, we don't need to know where the\ndividing line is to run the algorithm. Suppose we don't know where the dividing line is. We can\nstill work our way in from the two ends of the array, and whenever we find two out-of-place keys,\nwe swap them. The place where we meet in the middle is the dividing line.\nQuicksort-6\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Batcher's Algorithm Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/ebe195a69f1f7dbe71ff20bd90089355_MIT18_310F13_Ch13.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nBatcher's Algorithm\nLecturer: Michel Goemans\nPerhaps the most restrictive version of the sorting problem requires not only no motion of the\nkeys beyond compare-and-switches, but also that the plan of comparison-and-switches be fixed in\nadvance. In each of the methods mentioned so far, the comparison to be made at any time often\ndepends upon the result of previous comparisons. For example, in HeapSort, it appears at first\nglance that we are making only compare-and-switches between pairs of keys, but the comparisons\nwe perform are not fixed in advance. Indeed when fixing a headless heap, we move either to the\nleft child or to the right child depending on which child had the largest element; this is not fixed\nin advance. A sorting network is a fixed collection of comparison-switches, so that all comparisons\nand switches are between keys at locations that have been specified from the beginning. These\ncomparisons are not dependent on what has happened before. The corresponding sorting algorithm\nis said to be non-adaptive.\nWe will describe a simple recursive non-adaptive sorting procedure, named Batcher's Algorithm\nafter its discoverer. It is simple and elegant but has the disadvantage that it requires on the order\nof n(log n)2 comparisons. which is larger by a factor of the order of log n than the theoretical\nlower bound for comparison sorting. For a long time (ten years is a long time in this subject!)\nnobody knew if one could find a sorting network better than this one. Then Ajtai, Koml os, and\nSzemer edi proved the existence of very complex networks that can sort with cn log n comparisons\nfor a very large constant c.\nTheir procedure is extremely complicated, and for most practical\npurposes Batcher's algorithm is more useful.\nThe idea behind Batcher's algorithm is the following claim (which at first glance looks incred-\nible): If you sort the first half of a list, and sort the second half separately, and then sort the\nodd-indexed entries (first, third, fifth, ...) and the even-indexed entries (second, fourth, sixth, ...)\nseparately, then you need make only one more comparison-switch per key to completely sort the\nlist. We will prove this below.\nFor the remainder of this section, we will assume that the length of our list, n, is a power of 2.\nLet's see what happens for a particular list of length 8. Suppose we are given the following list of\nnumbers:\nWe wish to sort it from least to greatest. If we sort the first and second halves separately we obtain:\nSorting the odd-indexed keys (2, 6, 1, 8) and then the even-indexed keys (3, 7, 4, 9) while leaving\nthem in odd and even places respectively yields:\nThis list is now almost sorted: doing a comparison switch between the keys in positions (2 and 3),\n(4 and 5) and (6 and 7) will in fact finish the sort.\nBatcher-1\n\nThis is no accident: given any list of length 8, if we sort the entries in the first half, then sort\nthe entries in the second half, then sort the entries in odd positions, then sort the entries in even\npositions and lastly perform this same round of exchanges (second with third, fourth with fifth,\nand sixth with seventh), the list will end up sorted. Furthermore, and even more incredibly, the\nsame fact holds for any list whose length is a multiple of 4 (as we shall see below); in that case, in\nthe final step, we sort the 2l-th element with the (2l+ 1)-st for l= 1, 2, · · · , (n/2) -1.\nTheorem 1. For any list of length n, where n is a multiple of 4, first sorting separately the first\nand second halves, then sorting separately the odd-indexed keys and the even-indexed keys, and\nfinally comparing-and-switching the keys indexed 2land 2l+ 1 for l= 1, 2, · · · , (n/2) -1 results in\na sorted list.\nProof. After the two halves of the list have been sorted separately, it is obvious that for all i\nbetween 1 and n except for 1 and n + 1, the (i\n1)-st element of the list is less than the i-th.\n-\nCall the (i -1)-st key the predecessor of the i-th. Note that 1 and n/2 + 1 are both odd. Every\neven-indexed key has as its predecessor a number smaller than itself (since any even-indexed key\nand its predecessor are in the same half), so the l-th smallest even-indexed key must be larger\nthan at least lodd-indexed keys (look at its predecessor as well as the predecessors of the l-1\neven-indexed keys that are smaller than it). Similarly, every odd-indexed key (with the possible\nexception of two keys, namely the 1st and (n + 1)-st) has as its predecessor a number smaller than\nitself, so the l+ 1-st smallest odd-indexed key must be larger than at least l-1 even-indexed keys.\nIf we denote by ki the i-th indexed key after sorting the first and second halves and the even\nand odd halves, we have just argued that\nk2l\nk\n-1 ≤\n2l\nand\nk2l-2 ≤k2l+1,\nfor any appropriate l. Since we have sorted the even indexed keys and the odd indexed keys, we also\nknow that k2l\n2 ≤k2land that k2l\n1 ≤k2l+1. Thus, if we group the elements in pairs (k2l, k\n)\n-\n-\n2l+1\nfor each appropriate l, we see that both elements of a pair are greater or equal to both elements of\nthe previous pair. To finish the sort after sorting the odds and evens, it is therefore only necessary\nto compare the l+ 1-st smallest odd-indexed key (k2l+1) to the l-th smallest even-indexed key\n(k2l) for each appropriate lto determine its rank completely; this is precisely what the final step\ndoes.\nIf we start with a list whose size is a power of two, we can use this idea to create longer and\nlonger sorted lists by repeatedly sorting the first and second halves and then the odd and even\nentries followed by one additional round of comparison-switches. This task is slightly easier than it\nlooks, as a result of a second observation: If you first sort the first and second halves, then the first\nand second halves of the odds and evens are already sorted. As a result, when sorting the odds\nand evens here, we only need to merge the two halves of the odd indexed keys and the two halves\nof the even indexed keys. We have shown how to merge 2 sorted lists of length p while performing\nonly 2p -1 comparisons, but this requires an adaptive algorithm. Instead, Batcher's procedure\nproceeds recursively and has two different components: a sorting algorithm that sorts completely\ndisordered lists and a merging algorithm that sorts lists whose first and second halves have already\nBatcher-2\n\nbeen sorted. We will call these algorithms Sort and Merge. Let us emphasize that Merge only\nsorts if the first half and second half are already sorted. If not, it is unclear what Merge does. Our\nplan when executing Batcher's procedure is to successively merge smaller lists to get larger ones,\nbut we use Batcher's merging algorithm rather than the simple merge, to make it non-adaptive.\nIt follows that Batcher's Algorithm can be written in the following recursive form (provided n\nis a power of 2 larger than 2):\n- Sort(x1, . . . , xn) calls:\nSort(x1, . . . , xn/2), then Sort(xn/2+1, . . . , xn), and then Merge(x1, . . . , xn).\n- Merge(x1, . . . , xn) calls:\nMerge(xi, for i odd), then Merge(xi for i even), and then Comp(x2, x3), Comp(x4, x5), · · ·\nComp(xn-2, xn-1).\n- Comp(xi, xj) means:\ncompare the key in the position i with the one in position j and put the larger one in position\nj, the smaller one in position i.\nIn the definition of Merge, notice that if the first and second halves of (x1, . . . , xn) are already\nsorted then the first and second halves of both (xi, for i odd) and (xi for i even) are already sorted\nas well, and therefore will become sorted after their Merge calls. For the base case, we should say\nthat\nSort(x1, x2) = Merge(x1, x2) = Comp(x1, x2).\nWe now turn to the question: how many steps does this algorithm take? Let S(n) denote the\nnumber of comparisons needed to sort n items, and let M(n) denote the number of comparisons\nneeded to merge two sorted lists of n/2 items each. Then we have\nS(n) = 2S(n/2) + M(n),\nM(n) = 2M(n/2) + n/2 -1\nwith the initial conditions S(2) = M(2) = 1. If we ignore the \"-1\" term (which is certainly fine if\nwe're aiming for an upper bound on S(n)), the second recurrence relation has the solution\nn\nM′(n) =\nlog n.\nYou can verify this by induction.\nTherefore M(n) ≤\nn log n.\nWe can use this to show that\nS(n) ≤n\n(log n) . We will see another way to count the number of comparisons below.\nWe now examine the Batcher procedure from the bottom up rather from the top down as we\nhave done so far. We have seen that the procedure for sorting n items reduces to two applications\nof the procedure for sorting n/2 items followed by two applications of the procedure for merging\nn/2 items followed by a single round of comparison-switches. The procedures for sorting-merging\nn/2 items similarly involve procedures for sorting-merging n/4 items, which involve procedure for\nsorting n/8 items, and so on, until we get down to sorting single pairs. From the opposite point\nof view, we start from an unsorted list, sort into pairs, then apply several rounds of these sorts to\ncreate sorted groups of size four, then sorted groups of size eight, and so on until we have sorted all\nBatcher-3\n\nn keys. We can use this description of Batcher's Algorithm to deduce what comparison switches\nare involved in sorting 2j keys in this way.\nWe will describe rounds of comparisons; a round will be a set of comparison-switches which use\neach key at most once and hence can be performed simultaneously; this is for example useful for a\nparallel implementation of a sorting algorithm.\nThe first step is to arrange odd-even adjacent pairs of keys into ordered pairs. This can be done\nin one round.\n(1, 2), (3, 4), (5, 6), . . .\nFor brevity, we write (1, 2) for Comp(x1, x2).\nTo sort into groups of four we then want to sort the odd and even-indexed keys into pairs by\napplying a similar round to the odd and even-indexed keys separately and in parallel; we then\nperform the final step for sorting into groups of four, by doing the comparisons (a, a + 1) for a ≡2\n(mod 4).\n(1.3), (2, 4), (5, 7), (6, 8), . . .\n(2, 3), (6, 7), . . .\nProducing sorted groups of size eight from what we now have then involves repeating these last\ntwo rounds on odds and evens, and then doing the final round for sorting into groups of eight (see\nupcoming figure).\n(1, 5), (2, 6), (3, 7), (4, 8), (9, 13) . . .\n(3, 5), (4, 6), (11, 13), (12, 14), . . .\n(2, 3), (4, 5), (6, 7), (10, 11), . . .\nEach merging step here involves one more round of comparisons than the previous one, because\nwe must repeat the previous round on odds and evens and add a final round. Producing sorted 8's\nfrom sorted 4's takes 3 rounds of comparisons (see upcoming figure); in general, getting sorted 2k's\nfrom sorted 2k-1's takes k rounds of comparisons.\nTo actually sort n = 2k keys in this way involves making them into sorted 2's, then sorted 4's,\nthen sorted 8's, etc,. until one has sorted 2k's. This takes 1+2+3+. . .+k rounds of comparisons,\nwhich is k(k+1)/2 rounds. Since each round takes no more than n/2 comparisons and k = log n, we\nneed approximately n(log n)2/4 comparisons in this algorithm. In fact, when n is sufficiently large,\nit can be shown that the number of comparisons required becomes strictly less than n(log n)2/4.\nWe schematically represent Batcher's algorithm as a sorting network in the following way. We\ndraw an horizontal line (wire) for each key to be sorted and the convention is that processing goes\nfrom left to right. A compare-and-switch is represented by a vertical line segment between the\ncorresponding wires; the larger key is output on the top wire (to the right of the comparator, as\nprocessing goes from left to right) and the smaller key on the bottom wire. Inputs enter the sorting\nnetwork on the left (with the ith key on the ith wire from the top) and outputs can be read on the\nright side. The comparisons made in each round of Batcher's algorithm are drawn closely to each\nother. Here is such a representation for Batcher's algorithm on 8 inputs; the color coding refers to\nsorting in groups of size 8 from groups of size 4.\nBatcher-4\n\n2. Sort on second half.\n3. Merge on odd keys.\n4. Merge on even keys.\n5. Final compare and switch of adjacent keys.\nKey:\n1. Sort on first half.\nBatcher's algorithm is not difficult to implement. If one can write recursive code, the initial\ndescription we gave can be coded directly. Otherwise, one can explicitly construct the rounds of\ncomparisons that are to be made as described above.\nBatcher-5\n1.\n2.\n4.\n3,\n5.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Chernoff bounds, and some applications Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/d43d47ff4e18566a7a64577fba543edc_MIT18_310F13_Ch4.pdf",
      "content": "18.310 lecture notes\nJune 3, 2014\nChernoff bounds, and some applications\nLecturer: Michel Goemans, Lorenzo Orecchia\nPreliminaries\nBefore we venture into Chernoff bound, let us recall two simple bounds on the probability that a random\nvariable deviates from the mean by a certain amount: Markov's inequality and Chebyshev's inequality.\nMarkov's inequality only applies to non-negative random variables and gives us a bound depending on\nthe expectation of the random variable.\nTheorem 1 (Markov's Inequality). Let X : S → R be a non-negative random variable. Then, for any a > 0,\nE(X)\nP(X ≥ a) ≤\n.\na\nProof. Let A denote the event {X ≥ a}. Then:\n\nE(X) =\nP(ω)X(ω) =\nP(ω)X(ω) +\nP(ω)X(ω).\nω∈S\nω∈A\nω∈A\n\nAs X is non-negative, we have\nP(ω)X(ω) ≥ 0. Hence:\nω∈A\n\nE(X) ≥\nP(ω)X(ω) ≥ a\nP(ω) = a · P(A).\nω∈A\nω∈A\nChebyshev's inequality requires the variance of the random variable and is normally stronger.\nP(|X - E(X)| ≥ a) ≤\nTheorem 2 (Chebyshev's Inequality). Let X : S → R be a random variable with expectation E(X) and\nvariance Var(X). Then, for any a ∈ R:\nVar(X) .\na2\nProof. Apply Markov's Inequality to the non-negative random variable (X - E(X))2 . Notice that\n\nE (X - E(X))2 = Var(X).\nEven though Markov's and Chebyshev's Inequality only use information about the expectation and the\nvariance of the random variable under consideration, they are essentially tight for a general random variable.\nExercise Verify this by constructing non-trivial (i.e. non-constant) random variables for which Theo\nrem 1 and Theorem 2 are tight, i.e. hold with equality.\nChernoff-1\n\n^\nY\nDeviation of a sum on independent random variables\nAs we are not able to improve Markov's Inequality and Chebyshev's Inequality in general, it is worth to\nconsider whether we can say something stronger for a more restricted, yet interesting, class of random\nvariables. This idea brings us to consider the case of a random variable that is the sum of a number of\nindependent random variables.\nThis scenario is particularly important and ubiquitous in statistical applications. Examples of such\nrandom variables are the number of heads in a sequence of coin tosses, or the average support obtained by\na political candidate in a poll.\nCan Markov's and Chebyshev's Inequality be improved for this particular kind of random variable?\nBefore confronting this question, let us check what Chebyshev's Inequality (the stronger of the two) gives us\nfor a sum of independent random variables.\nTheorem 3. Let X1, X2, . . . , Xn be independent random variables with E(Xi) = μi and Var(Xi) = σi\n2 .\nThen, for any a > 0:\nn\nn\nn\nσ2\ni=1\ni\nP(|\nXi -\nμi| ≥ a) ≤\na2\ni=1\ni=1\nn\nn\nProof. This follows from Chebyshev's Inequality applied to\nXi and the fact that Var(\nXi) =\ni=1\ni=1\nn\nVar(Xi) for independent variables.\ni=1\nIn particular, for identically distributed random variables with expectation μ and variance σ2, we obtain\nP\nn\ni=1 Xi - μ\nσ2\nn\n≥ b ≤ nb2\nfor any b > 0. We covered this derivation in the lecture on the Weak Law of Large Numbers.\nCan this result be improved or is it tight? At a first glance, you may suspect that this is tight, as we\nhave made use of all our assumptions. In particular, we exploited the independence of the variables {Xi}\nn\nn\nto get Var(\nXi) =\nVar(Xi). Notice, however, that this last step actually only uses the pairwise\ni=1\ni=1\nindependence of the variables {Xi}, i.e. the fact that, for all couples i =\nj ∈ [n] and all x, y ∈ R:\nP(Xi = x ∧ Xj = y) = P(Xi = x) · P(Xj = y).\n(1)\nIndeed, it is possible to show that Theorem 3 is tight when all the variables {Xi} are just guaranteed to be\npairwise independent.\n\nHard Exercise Let X1, . . . , Xd be independent random variables that take value 1 or -1, each with\nprobability 1/2. For each S ⊆ [d], define the random variable YS\nare pairwise independent. ii) Let Z =\nS⊆D\n=\nXi. i) Show that the variables {YS }\ni∈S\nYS . Show that Chebyshev's Inequality is asymptotically tight\nfor Z.\nWe are now ready to tackle the case of a sum of independent random variables. Recall that we are now\nusing the following strong version of independence (also known as joint or mutual independence), which\nguarantees the same property of Equation 1 for any subset S ⊆ [n] of random variables:\n∀S ⊆ [n], P(\nXi = xi) =\nP(Xi = xi).\ni∈S\ni∈S\nIn this case, the proof of Theorem 3 is too weak as it does not rely on the joint independence. In the next\nsection, we will see that we can indeed obtain stronger bounds under this stronger assumpiton. These bounds\nare known as Chernoff bounds, after Herman Chernoff, Emeritus Professor of Applied Mathematics here at\nMIT!\nChernoff-2\nX\nX\nP\nP\nP\nP\nP\nP\n\nP\nP\n\nY\nChernoff Bound\nThere are many different forms of Chernoff bounds, each tuned to slightly different assumptions. We will\nstart with the statement of the bound for the simple case of a sum of independent Bernoulli trials, i.e. the\ncase in which each random variable only takes the values 0 or 1. For example, this corresponds to the case\nof tossing unfair coins, each with its own probability of heads, and counting the total number of heads.\nn\nTheorem 4 (Chernoff Bounds). Let X =\nXi, where Xi = 1 with probability pi and Xi = 0 with\ni=1\nn\nprobability 1 - pi, and all Xi are independent. Let μ = E(X) =\ni=1 pi. Then\nδ2\n2+δ\n(i) Upper Tail: P(X ≥ (1 + δ)μ) ≤ e -\nμ for all δ > 0;\n-μδ2/2\n(ii) Lower Tail: P(X ≤ (1 - δ)μ) ≤ e\nfor all 0 < δ < 1;\nNotice that the lower and upper tail take slightly different forms. Curiously, this is necessary and boils\ndown to the use of different approximation of the logarithmic function. There exist more general versions of\nthis bound, where this asymmetry is not present, but they are more complicated, as the involve the entropy\nof the distribution at the exponent.\n3.1\nProof idea and moment generating function\nLet X be any random variable, and a ∈ R. We will make use of the same idea which we used to prove\nChebyshev's inequality from Markov's inequality. For any s > 0,\nsX ≥ e sa)\nP(X ≥ a) = P(e\nsX )\nE(e\n≤\nby Markov's inequality.\n(2)\nesa\n(Recall that to obtain Chebyshev, we squared both sides in the first step, here we exponentiate.) So we have\nsome upper bound on P(X > a) in terms of E(esX ). Similarly, for any s > 0, we have\n-sX ≥ e -sa)\nP(X ≤ a) = P(e\n-sX )\nE(e\n≤\n-sa\ne\nThe key player in this reasoning is the moment generating function MX of the random variable X, which\nis a function from R to R defined by\n\nsX\nMX (s) = E e\n.\nThe reason for the name is related to the Taylor expansion of esX ; assuming it converges, we have\ninf\n\n2X2\n3X3\nMX (s) = E 1 + sX + s\n+\ns\n+ · · ·\n=\n1 s iE(Xi).\n3!\ni!\ni=0\nThe terms E(Xi) are called \"moments\" and encode important information about the distribution; notice that\nthe first moment (i = 1) is just the expectation, and the second moment is closely related to the variance.\nSo the moment generating function encodes information of all of these moments in some way.\nMoment generating functions behave wonderfully with respect to addition of independent random vari\nables:\nn\nLemma 1. If X =\nXi where X1, X2, . . . , Xn are independent random variables, then\ni=1\nn\nMX (s) =\nMXi (s).\ni=1\nChernoff-3\nP\nP\nX\nY\nP\n\nY\nY\nY\nY\n\n!\nProof.\n\ns\ni=1 Xi\nMX (s) = E(e sX ) = E e\nn\nn\nsXi\n= E\ne\ni=1\nn\nsXi )\n=\nE(e\nby independence\ni=1\nn\n=\nMXi (s).\ni=1\nThis lemma allows us to prove a Chernoff bound by bounding the moment generating function of each\nXi individually.\n3.2\nProof of Theorem 4\nBefore proceeding to prove the theorem, we compute the form of the moment generating function for a single\nBernoulli trial. Our goal is to then combine this expression with Lemma 1 in the proof of Theorem 4.\nLemma 2. Let Y be a random variable that takes value 1 with probability p and value 0 with probability\n1 - p. Then, for all s ∈ R:\nsY ) ≤ ep(e s-1)\nMY (s) = E(e\n.\nProof. We have:\nsY )\nMY (s) = E(e\n= p · e s + (1 - p) · 1\nby definition of expectation\n= 1 + p(e s - 1)\np(e s-1)\ny\n≤ e\nusing 1 + y ≤ e\nwith y = p(e s - 1).\nWe are now ready to prove Theorem 4 by combining Lemma 1 and 2.\nProof of Theorem 4. Applying Lemma 1 and Lemma 2, we obtain\nn\n\npi(e s-1)\n(e s -1)\npi\n(e s -1)μ\ni=1\nMX (s) ≤\ne\n= e\nn\n≤ e\n,\n(3)\ni=1\nn\nusing that\n= E(X) = μ.\ni=1 pi\nFor the proof of the upper tail, we can now apply the strategy described in Equation 2, with a = (1+ δ)μ\nand s = ln(1 + δ).\n-s(1+δ)μ (e s-1)μ)\nP(X ≥ (1 + δ)μ) ≤ e\ne\nμ\nδ\ne\n=\n.\n(1 + δ)1+δ\nChernoff-4\nY\nY\nY\nY\nP\n\nOur choice of s is motivated as follows: we are trying to make our upper bound for the tail probability to\nbe as small as possible. To do this, we can minimize our expression for the upper bound as a function of s.\nTaking the derivative of the exponent shows that this minimum is achieved exactly at s = log(1 + δ).\nTaking the natural logarithm of the right-hand side yields\nμ(δ - (1 + δ) ln(1 + δ)).\nUsing the following inequality for x > 0(left as an exercise):\nx\nln(1 + x) ≥\n,\n1 + x/2\nwe obtain\nδ2\nμ(δ - (1 + δ) ln(1 + δ)) ≤-\nμ.\n2 + δ\nHence, we have the desired bound for the upper tail:\nμ\nδ\nδ2\ne\n2+δ\nP(X ≥ (1 + δ)μ) ≤\n≤ e -\nμ .\n(1 + δ)1+δ\nThe proof of the lower tail is entirely analogous. It proceeds by taking s = ln(1 - δ) and applies the\nfollowing inequality for the logarithm of (1 - δ) in the range 0 < δ < 1 :\nδ2\nln(1 - δ) ≥-δ +\n.\nDetails are left as an exercise.\nOther versions of Chernoff Bound\nFor δ ∈ (0, 1), we can combine the lower and upper tails in Theorem 4 to obtain the following simple and\nuseful bound:\nCorollary 5. With X and X1, . . . , Xn as before, and μ = E(X),\n-μδ2/3\nP(|X - μ| ≥ δμ) ≤ 2e\nfor all 0 < δ < 1.\nChernoff bound can be applied to more general settings than that of Bernoulli variables. In particular,\nthe following version of the bound applies to bounded random variables, regardless of their distribution!\nn\nTheorem 6. Let X1, X2, . . . , Xn be random variables such that a ≤ Xi ≤ b for all i. Let X =\ni=1 Xi and\nset μ = E(X). Then, for all δ > 0 :\n2δ2 μ\n- n(b-a)2\n(i) Upper Tail: P(X ≥ (1 + δ)μ) ≤ e\n;\nδ2 μ 2\n- n(b-a)2\n(ii) Lower Tail: P(X ≤ (1 - δ)μ) ≤ e\n.\nExample application: coin tossing\nSuppose we have a fair coin. Repeatedly toss the coin, and let Sn be the number of heads from the first n\ntosses. Then the weak law of large numbers tells us that P(|Sn/n - 1/2| ≥ E) → 0 as n →inf. But what can\nwe say about this probability for some fixed n? If we go back to the proof of the weak law that we gave in\nterms of Chebyshev's inequality, we find that it tells us that\nP(|Sn/n - 1/2| ≥ E) ≤\n.\n4nE2\nChernoff-5\n\nP\n\nSo for example, P(|Sn/n - 1/2| ≥ 1/4) ≤ 4 .\nn\nBut we can apply Chernoff instead of Chebyshev; what do we get then? From Corollary 5, using\nE(Sn) = n/2,\n-nδ2/6\nP(|Sn - n/2| ≥ δ(n/2)) ≤ 2e\n.\n-n/24\nTaking δ = 1/2 we obtain P(|Sn/n - 1/2| ≥ 1/4) ≤ 2e\n. This is a massive improvement over the\n\nChebyshev bound! Let's try this now with a much smaller δ: let δ =\n6 ln n/n. Then we obtain\n\n- ln n\nP(|Sn/n - 1/2| ≥ 1\n6 ln n/n) ≤ 2e\n= 2 .\nn\n\nIf instead we take δ just twice as large, δ = 2\n6 ln n/n,\n\n-4 ln n\nP(|Sn/n - 1/2| ≥\n6 ln n/n) ≤ 2e\n= 2\n.\nn4\nChernoff-6\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Counting, Coding, Sampling Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/28ab492c0aecd8ff4b37d9fd1f460042_MIT18_310F13_Ch6.pdf",
      "content": "18.310 lecture notes\nSeptember 25, 2013\nCounting, Coding, Sampling\nLecturer: Michel Goemans\nIn these notes we discuss techniques for counting, coding and sampling some classes of objects.\nWe start by presenting several classes of objects counted by the Catalan sequence Cn =\n2n .\nn+1\nn\nThis is an occasion to present several bijective techniques for counting, and simply beautiful math-\n\nematics. We then discuss some algorithmic application of (bijective) counting: some coding and\nrandom sampling algorithms.\nSome Catalan families\nWe start by defining three classes of objects, and then discuss the relation between them.\nA plane tree (a.k.a. ordered tree) is a rooted tree in which the order of the children matters.\nLet Tn be set of plane trees with n edges. The set T3 is represented in Figure 1. A binary tree is a\nplane tree in which vertices have either 0 or 2 children. Vertices with 2 children are called nodes,\nwhile vertices with 0 children are called leaves. Let Bn be the set of binary trees with n nodes. The\nset B3 is represented in Figure 2. A Dyck path is a lattice path (sequence of steps) made of steps\n+1 (up steps) and steps -1 (down steps) starting and ending at level 0 and remaining non-negative.\nSince the final level of a Dyck path is 0 the number of up steps and down steps are the same, and\nits length is even. Let Dn be the set of Dyck paths with 2n steps. The set D3 is represented in\nFigure 3.\nFigure 1: The set T3 of plane trees.\nFigure 2: The set B3 of binary trees.\nFigure 3: The set D3 of Dyck paths.\nCount-1\n\nObserve that there is the same number of elements in T3, B3 and D3. This is no coincidence, as\nwe will now prove that for all n, the sets Tn, Bn, Dn have the same number of elements. We now\nuse the notation |S| to denote the cardinality of a set S. We will now prove that\n|Tn| = |Bn| = |Dn| = n + 1\n2n\nn\n\n.\nThe number\n1 2n\nis the so-called nth Catalan number.\nn+1\nn\n1.1\nCounting Dyck paths\n(0)\nWe first compute the number of Dyck paths. Let Pn\nbe the set of paths of length 2n made of steps\n+1 steps and -1 steps starting and ending at level 0. Ending at level 0 is the same as having the\nsame number of up steps and down steps, and any choice of order of such steps is allowed. Hence\nP(0)\nn\n=\n2n\nn\n\n.\nNow D\n(0)\nn is a subset of Pn . It seems hard to find |Dn| because of the non-negativity constraint,\nbut actually a trick will now allow us to compute the cardinality of the complement subset\nDn ≡P(0)\nn\n\\ Dn.\n2n\n(\nIndeed we claim that |Dn|\n2)\n=\n. To prove this claim we consider the set\nn\nPn\n-\nof paths\nof length 2n made of steps +1 s\n-\nteps and -1 steps starting at level 0 and ending at level -2.\nThese paths have n -1 up steps and n + 1 down steps, and any order of steps is possible, hence\n(\n2)\n2n\n|Pn\n-|\n(\n2)\n=\n\n. So it suffices to give a bijection f between\nn -1\nDn and Pn\n-. This bijection is\ndefined as follows: take a path D in Dn consider the first time t it reaches level -1. The path\nf(D) is obtained from D by flipping all the steps after time t with respect to the line y = -1. An\n(\n2)\nexample is shown in Figure 4. We let the reader check that f is a bijection between Dn and Pn\n-.\n(\n2)\nSince f is a bijection we have |Dn| = |Pn\n-| =\n2n .\nn-1\n\nD\nf(D)\nFigure 4: The bijection f: the path D ∈D\n(\n2)\nin red, the path f(D) ∈Pn\n-\nin black.\nBy the preceding, we have\n(0)\n2n\n|Dn| = |Pn | -|Dn| =\nn\n\n-\n2n\n(2n)!\n(2n)!\n=\n.\nn\n\n-\nn!n! -(n + 1)!(n -1)!\nCount-2\n\nAnd by reducing to the same denominator we find\n(2n)!\n2n\n|Dn| =\n=\nn + 1!n!\nn + 1\n\nn\n\n,\nas wanted.\n1.2\nBijection between plane trees, binary trees and Dyck paths\nWe now present bijections between the sets Tn, Bn and Dn.\nWe first present a bijection Φ between plane trees and Dyck paths as follows: given any tree\nT in Tn, perform a depth-first search of the tree T (as illustrated in Figure 5) and define Φ(T) as\nthe sequence of up and down steps performed during the search. A Dyke path is obtained from\nT because Φ(T) has n up steps and n down steps (one step in each direction for each edge of T),\nstarts and end at level 0 and remains non-negative. Because Φ is a bijection between Tn and Dn,\nwe conclude\n|Tn| = |Dn| = n + 1\nn\nn\n\n.\nT\nΦ(T)\nFigure 5: A plane tree T and the associated Dyck path Φ(T). The depth-first search of the tree T\nis represented graphically by a tour around the tree (drawn in orange).\nWe now present a bijection Ψ between binary trees and Dyck paths. Let B be a binary tree in\nBn. The tree B has n nodes. It can be shown that it has n + 1 leaves (do it!). We can perform a\ndepth-first search of the tree B and make a up step the first time we encounter each node and a\ndown step each time we encounter a leaf. This makes a path with n up steps and n+1 down steps.\nThe last step is a down step and we ignore it. We denote by Ψ(B) the sequence of n up steps and\nn down steps obtained in this way. An example is represented in Figure 6. It is actually true that\nΨ(B) is always a Dyck path and that Ψ is a bijection between Bn and Dn. We omit the proof of\nthese facts. Since the sets Bn and Dn are in bijection we conclude\n2n\n|Bn| = |Dn| = n + 1\n\nn\n\n.\nCoding\nLet S be a finite set of objects. A coding function for the set S is a function which associate a\ndistinct binary sequence f(s) to each element s in S. The binary sequence f(S) is called code of\nS. Here are lower bounds for the length of codes.\nCount-3\n\n-\n-\n-\n+\n-\n+\nB\n-\n+\n-\n-\n-\n-\n+\nΨ(B)\n+\n-\n+\n+\n-\n+\n+\n+\nFigure 6: A binary tree B and the associated Dyck path Ψ(B). The depth-first search of the tree\nB is represented graphically by a tour around the tree (drawn in orange).\nLemma 1. If S contains N elements then at least one of the codes has length greater or equal to\n⌊log2(N)⌋. If one consider the uniform distribution for elements in S then the codes have length at\nleast log2(N) -2 in average.\nExercise: Prove Lemma 1 for N = 2k -1.\nExample 1: coding permutations. Let Sn be the set of permutations of {1, 2, . . . , n}. We\nnow discuss a possible coding function f for the set Sn. Recall that for any integer i, the binary\nrepresentation of i is ⌈log2(i + 1)⌉. Thus each number i ∈{1, 2, . . . , n} can be represented uniquely\nby binary sequences of length exactly ⌈log2(n + 1)⌉: it suffice to take their binary representations\nand add a few 0 in front if necessary to get this length. Let π ∈Sn be a permutation seen as\na sequence of distinct numbers π = π1π2 . . . πn. One can define f(π) as the concatenation of the\nbinary sequences (of length ⌈log2(n)⌉) corresponding to each number π1π2 . . . πn. Then the length\nof the code f(π) is n⌈log2(n + 1)⌉∼n log2(n). We can recover the permutation from the code:\nif one has the code, it can cut it in subsequences of length log2(n + 1) each and then recover the\nnumbers π1π2 . . . πn making the permutation. Is it an efficient coding? Well according to Lemma\n1 we cannot achieve codes shorter than log2(n!) -2 in average. Moreover, log2(n!) ∼n log2(n).\nTherefore our coding function f has length as short as possible asymptotically.\nExample 2: coding Dyck paths. Consider the set Dn of Dyck path of length 2n. There is an\neasy way of coding a Dyck path D ∈Dn by a binary sequence of length 2n. Simply encode down\nsteps by \"0\" and up steps by \"1\" this give a binary sequence f(D) of length 2n. Could we hope\nfor shorter codes? Certainly it would be possible to get a code of length 2n -2 because the first\nstep is an up step and the last step is a down step, so these could be ignored. But could we do\nbetter than 2n + o(n) (where the \"little o\" notation means that the expression divided by n goes\nto zero as n goes to infinity)? We have seen that the set Dn has cardinality N =\n2n!\n. Using\nn!(n+1)!\nthe Stirling formula\nn\nn\nn!\n√\n∼\n2nπ\n\ne\n\none gets log2(n!) = n log2(n) -n log2(e) + o(n). Hence one can compute\nlog2(N) = log2(2n!) -log2(n!) -log2((n + 1)!) = 2n + o(n).\nTherefore, by Lemma 1 one cannot encode Dyck paths by codes of length less than 2n + o(n) on\naverage. So our naive coding is asymptotically optimal. Observe that this also gives a way of\ncoding plane trees or binary trees optimally.\nCount-4\n\nRandom sampling\nLet S be a finite set of objects. A (uniformly random) sampling algorithm for the set S is an al-\ngorithm which outputs an element in S uniformly at random from S. Here we suppose we dispose\nof a perfect random generator for integers. More precisely, let us suppose that one can generate a\nuniformly random integer in {1, 2, . . . , n} for any integer n.\nExample 1: sampling permutations. How to sample a permutation in Sn? Here is a solution\nwritten in pseudo-code.\nInput an integer n.\n- Initialize an array V of size n with value i at position i for i = 1 . . . n.\n- For i = 1 to n do\nChoose a integer r uniformly at random in {i, i + 1, . . . , n}.\nSwap the values at position i and r in V .\nOutput the array V .\nThe output of the above algorithm is an array of number which corresponds to a uniformly\nrandom permutation. Indeed, the first number of the array is chosen uniformly in {1, 2, . . . , n}, the\nsecond number in the array is chosen uniformly randomly from the remaining numbers etc. Thus\nthe above algorithm is indeed a sampling algorithm for the set Sn.\nExample 2: sampling Dyck paths. Sampling Dyck paths is a bit more difficult. We will need\nto first define an algorithm for sampling paths from another set. Let P(-1)\nn\nbe the set of paths\nof length 2n + 1 with steps +1 and -1 starting at level 0 end ending at level -1. Hence a path\nP ∈P(-1)\nn\nhas steps \"+1\" and n + 1 steps \"-1\" in any order. Here is a sampling algorithm for the\nset P(-1)\nn\n.\nInput an integer n.\n- Initialize an array V of length 2n + 1 with value 1 in the first n entries and\nvalue -1 in the remaining n + 1 entries.\n- For i = 1 to 2n + 1 do\nChoose an integer r uniformly at random in {i, i + 1, . . . , 2n + 1}.\nSwap the values at position i and r in V .\nOutput the array V .\nBecause the algorithm randomly permutes the steps +1 and -1, it indeed outputs a uniformly\nrandom path in P(-1)\nn\n.\n(\n1)\nNow we will show how to obtain an Dyck path D ∈Dn from a path P ∈Pn\n-. The trick we\n(\n1)\nwill use is known as the cycle lemma. Let P ∈Pn\n-. Let l≤0 be the lowest level of the path P,\nand let t be the first time the level lis reached. This decomposes P as P1P2 where P1 is the path\nbefore time t and P2 is the path after time t. Now consider the path P2P1. This path is ending\nwith a -1 step. Then we define g(P) as the path obtained from P2P1 by ignoring the last step. The\nmapping g is illustrated in Figure 7. The path g(P) has n up steps and n down steps so it ends at\nlevel 0. In fact we claim that it is a Dyck path. Here is an even stronger claim.\nLemma 2. For any path P is P ∈P(-1)\n(\n1)\nn\n, the path g(P) is a Dyck path. So g maps the set Pn\n-\nto the set D\n(\n1)\nn. Moreover, any Dyck path in Dn is the image of exactly 2n + 1 paths in Pn\n-.\nCount-5\n\nD = g(P)\nP\nt\ng\nP1\nP2\nFigure 7: A path P ∈P(-1)\nn\nand the resulting g(P).\nWe will not prove this Lemma. However we argue that this gives a way of sampling Dyck paths.\n(\n1)\nIndeed, by the above algorithm, one can sample a path P in\n(\n1)\nPn\n-, and then apply the mapping g\nto obtain a Dyck path g(P). Since every path in Pn\n-\nhas the same probability of being sampled\nand every Dyck path in Dn has the same number of preimages, every Dyck path in Dn has the same\nprobability of being sampled. We have thus found a sampling algorithm for Dyck paths. Observe\nthat this also gives a way of sampling plane trees or binary trees.\nCount-6\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Cryptography Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/9e96bf48fb5d12f1300365d05e227be1_MIT18_310F13_Ch15.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nCryptography\nLecturer: Michel Goemans\nPublic Key Cryptosystems\nIn these notes, we will be concerned with constructing secret codes. A sender would like to encrypt\nhis message to protect its content while the message is in transit. The receiver would like to easily\ndecode the received message. Our coding scheme should contain the following properties:\n1. encoding is easy to perform;\n2. decoding is extremely difficult (for protection against eavesdroppers);\n3. decoding is easy if you are in possession of some secret \"key\".\nThe traditional way of creating secret codes (used in various degrees of sophistication for cen-\nturies) is that both the sender and the receiver share a secret, called a key. The sender scrambles\nthe message in a complicated way that depends on the key. The receiver then uses the key to\nunscramble the message. If these codes are constructed properly (something which is surprisingly\nhard to do), it seems virtually impossible for somebody without the key to decode the message,\neven if they have many examples of pairs of messages and their encodings to work from in trying\nto deduce the key.\nThe drawback of this method is ensuring that every pair of people who need to communicate\nsecretly have a shared secret key. Distributing and managing these keys is surprisingly difficult even\nwhen these keys are used for espionage. This would be extremely difficult for secret communication\nover the internet, when you may wish to send your credit card securely to a store you have never\nbefore heard of. Luckily, there is another way to proceed.\nDiffie and Hellman in 1976 came up with a scheme for handling such communications, called a\npublic key cryptosystem. It is based on the assumption that there is a wide class of functions that\nare relatively easy to compute but extraordinarily difficult to invert unless you possess a secret.\nAccording to this scheme, each communicator or recipient, say Bob or B, publishes in a well\ndefined place (a kind of telephone directory) a description of his function, fB from this class; this\nis Bob's public key. Bob knows also the inverse of fB, this is his private key. The assumption is\nthat this inverse is extremely difficult to compute if one does not know some private information.\nSuppose now that someone, say Alice or A, would like to send a message m to B. She looks\nup Bob's public key and sends m′ = fB(m). Since Bob knows his own private key, he can recover\nm = f-1\nB (m′). The problem here is that Bob has no guarantee that Alice sent the message. Maybe\nsomeone else claiming to be Alice sent it.\nSo, instead, suppose that Alice sends the message\nm′ = f\nB(fA\n-(m)) to Bob. Notice that Alice needs to know Bob's public key (which she can find\nin the diretory) and also her known private key, which is known only to her. Having received this\nmessage, Bob can look up Alice's public key and recover m by computing m = fA(f-1\nB (m′)); again,\nfor this purpose, knowledge of Bob's private key and Alice's public key is sufficient. Anyone else\nRSA-1\n\nwould have to solve the said-to-be-extraordinarily-difficult task of inverting the action of one or\nanother of these functions on some message in order to read the message or alter it in any way at\nall. This is the basic setup for a public-key cryptosystem. One can also use it for digital signatures.\nIf Alice wants to show to anyone that she wrote message m, she can publish or send f-1\nA (m), and\nanyone can test it came from Alice by computing fA(m′).\nWe will discuss one of the classes of problems that have been suggested and deployed for\ncommunications of this public key type. It was actually developed here at M.I.T., a number of\nyears ago and is known as the RSA public key cryptosystem. It was invented by three MIT people,\nRon Rivest, Adi Shamir and Len Adleman in 1977. Their scheme is based on the fact that it is easy\nto multiply two large numbers together, but it appears to be hard to factor a large number. The\nrecord so far for factoring has been to factor a 768-bit number (i.e., 232 digits) given in an RSA\nchallenge, and this took the equivalent of 20,000 years of computing on a single-core machine...\nThe task of factoring a 1024-bit number appears to be 1,000 harder with the current algorithms.\nThe RSA code\nFor this code, choose two very large prime numbers (say with several hundred digits), p and q,\nand form the product N = pq.\nChoose a number z < N such that z is relatively prime to\n(p -1)(q -1) = N -p -q + 1. Knowing p and q (or (p -1)(q -1)) we can find the multiplicative\ninverse y to z modulo (p-1)(q-1) by the extended Euclidean algorithm. The pair (N, z) constitutes\nthe public key, and (N, y) constitutes the private key.\nIf we want to encode a message, we first view it as a number in base N.\nEvery digit is a\nnumber between 0 and N -1 and we will encode each digit 0 ≤m < N separately. The sender\ncomputes s = mz mod N and transmits s. Upon receiving s, the receiver, who knows the private\nkey, computes sy mod N. The claim is that this is precisely m, i.e. m = sy mod N.\nIf one could factor N into N = pq then one can easily compute y from z (by the extended\nEuclid algorithm) and therefore break this public-key cryptosystem. However, as we said previously,\nfactoring large numbers appears to be very challenging.\nWhy does this scheme work? Let x = sy; we want to show that m = x mod N. We have that\nx = symyz\n(mod N),\nand since z and y are multiplicative inverses modulo (p -1)(q -1), we get that\nx = m1+k(p-1)(q-1) = mmk(p-1)(q-1)\n(mod N).\nWe want to prove that this is equal to m mod N. We will first show that x ≡m (mod p) and\nx ≡m (mod q) and then deduce from the Chinese remainder theorem that x ≡m (mod pq). To\nshow that x ≡m (mod p), we need to consider two cases. First, if m is a multiple of p, then x\nis also a multiple of p, and so x ≡m ≡0 (mod p). Otherwise, if m is not a multiple of p then it\nmust be relatively prime to p (since p is prime): gcd(m, p) = 1. Thus we can apply Fermat's Little\nTheorem, which tells us that mp-1 ≡1 (mod p), and thus\nmk(p-1)(q-1) ≡1k(q-1) ≡1\n(mod p).\nMultiplying by m, we indeed obtain x ≡m (mod p).\nRSA-2\n\nWe can apply the same argument to q, to obtain that x ≡m (mod q) also. Thus the Chinese\nremainder theorem tells us that x ≡m (mod N), and we have proved the correctness of the RSA\nscheme.\nIn order to use RSA, we need to show how to generate large primes, and also how to efficiently\ncompute mz mod N (or sy mod N) when z is very large.\nRaising a Number to a High Power\nIn this section, we show how to raise a number to a high power modulo N efficiently, say mz mod N.\nThe technique is known as repeated squaring and the idea is very simple.\nIf z is a power of 2, say 2k, we can compute mz mod N by starting with m, and repeatedly\n(k times) squaring it and taking it modulo N. This requires only k multiplications. For example,\nto compute 332 mod 83, we first compute 32 = 9 (mod 83) then 34 = 92 = 81 (mod 83), then\n38 = 812 = (-2)2 = 4 (mod 83) then 316 = 42 = 16 (mod 83), and finally 332 = 162 = 256 = 7\n(mod 83). Even though 332 is more than 1015, the fact that we did every operation modulo 83\nmeant that we never had to deal with large numbers.\nIn general, when z is not necessarily a power of 2, we write z's binary representation. Suppose\nthat z requires d bits, and let zk denote the k leading bits of z.\nObserve that zk = 2zk\n1 or\nz = 2z\n+1 depending on the kth bit of z. We compute mz\n-\nk\n-\nk\nk\nmod N for k = 1, · · · , d. For k = 1,\nthis is simply z2 mod N. Once we have computed ak\n1 = mzk-1 mod N, it is easy to compute a\n-\nk.\nSquare ak\n1, multiply it by m if the kth leading bit is a 1, and do these operations modulo N. We\n-\nthen get ak and we can repeat.\nExample.\nSuppose z = 201. Then z can be respresented by 11001001 in binary. There are 8\nsteps, in which (i) we square and multiply by m in steps 1, 2, 5, 8 and (ii) we only square in steps\n3, 4, 6, and 7.\nPrimality Testing\nTo be able to use RSA, we need to be able to generate large primes. The prime number theorem\nstates that among integers near n, when n is large, approximately one in ln n is a prime.\nTo\ngenerate a large prime, we can then generate a random number with the appropriate number of\ndigits or bits, and check if it is prime. If it is, we are done, else we increment it, and try again until\nwe find a prime number.\nWe therefore need to be able to efficiently check if a number is prime. This is known as primality\ntesting. We could try whether it is divisible by any of the small primes, say all primes up to 30.\nThis would detect a good fraction of the composite numbers, but clearly not all of them. Checking\nall possible factors up to the square root of the number n is extremely slow if n is large, and we\nare interested in numbers with hundreds of digits.\nOne approach is based on Fermat's little theorem, and so is called the Fermat primality test.\n4.1\nFermat primality test\nSuppose we are given a large number n, and we want to determine if it's prime.\nRSA-3\n\nLet a be any positive number less than n; then by Fermat's Little Theorem, if n is indeed prime,\nthen since gcd(a, n) = 1 we have that\nan-1 ≡1\n(mod n).\nIf n is not prime, on the other hand, this doesn't have to be true (though it might happen, depending\non the specific a and n we choose). So here is a test: choose a large number of randomly chosen\nvalues a1, a\nn\n2, . . . , aN, all positive and less than n, and calculate a -1\ni\n(mod n) for each. This we\ncan do very quickly by repeated squaring, as we saw already. If we obtain a value other than 1 for\nany a, then n is not prime; that a acts a a certificate (\"proof\") that n is not prime. We call such\nan a a Fermat witness for n; if an-1 ≡1 (mod n) (and n is composite) then we instead call a a\n\"Fermat liar\" for n.\nAs an example, let's apply this test to n = 1591, using a = 2. By repeated squaring, we get\n- 21 ≡2 (mod 1591),\n- 23 ≡22 · 2 ≡8 (mod 1591)\n- 26 ≡82 ≡64 (mod 1591)\n- 212 ≡642 ≡914 (mod 1591)\n- 224 ≡9142 ≡121 (mod 1591)\n- 249 ≡1212 · 2 ≡644 (mod 1591)\n- 299 ≡6442 · 2 ≡561 (mod 1591)\n- 2198 ≡5612 ≡1294 (mod 1591)\n- 2397 ≡12942 · 2 ≡1408 (mod 1591)\n- 2795 ≡14082 · 2 ≡156 (mod 1591)\n- 21590 ≡1562 ≡471 (mod 1591),\nand this proves that 1591 is not a prime. Indeed, 1591 = 37 · 43.\nHowever, we might obtain that 2n-1 ≡1 mod n even though n is not prime; this happens for\n22 values of n below 10 000. But we can apply the test for multiple values of a, chosen randomly.\nLet's define precisely our primality test as follows:\nFermat primality test for an integer n\n1. Pick a ∈{1, 2, 3, . . . , n -1} uniformly at random.\n2. Calculate (efficiently via repeated squaring) the value an-1 mod n. If this is not 1, output\n\"not prime\"; otherwise output \"maybe prime\".\nWe can repeat this test many times; if it outputs \"not prime\" at least once, we can be sure that\nit is indeed not prime; if it returns \"maybe prime\" each time, then perhaps we can conclude that\nn is very likely to be prime?\nThis turns out to be almost, but not quite, true. There are certain special composite numbers,\ncalled Carmichael numbers, that do a very good job of fooling this test.\nRSA-4\n\nDefinition 1. A positive integer n is a Carmichael number if it is composite and an-1 ≡1 (mod n)\nfor all a relatively prime to n.\nIf we apply Fermat's test to a Carmichael number, then the only way it can spot that it's not\nprime is if the a chosen happens to share a common factor with n. If the factors of n are all large,\nthen there are few such numbers (compared to n) and we're very unlikely to pick one of them.\nIt's not obvious that these numbers exist, but they do, and there are infinitely many of them;\nthe smallest is 561. They are very rare however (much much rarer than primes), and if you pick a\nlarge random n, you'd have to be very unlucky to pick a Carmichael number. What we will prove\nnow is that the Fermat test does work on all numbers except for Carmichael numbers.\nTheorem 1. If n > 2 is composite and not a Carmichael number, then the probability that the\nFermat test works is at least 1/2.\nProof. Let\nL = {b ∈\n∗\nn : bn\nZ\n-≡1 mod n},\ni.e., the set of Fermat liars for n. Let W = Z∗\nn \\ L, the set of Fermat witnesses that are relatively\nprime to n. Since n is not a Carmichael number, W is nonempty. (Note that all elements of Zn \\Z∗\nn\nare also Fermat witnesses.)\nWe will show that |L| ≤|W|, from which the theorem follows. Since then the probability we\npick a ∈L is not more than 1/2; and so the probability of choosing a Fermat witness is at least\n1/2.\nThe plan is to find a 1 -1 map φ from L to W, which immediately gives us that |L| ≤|W|.\nThe map is very simple: pick an arbitrary a ∈W (recall it's nonempty), and define\nφ(b) = ab mod n\n∀b ∈L.\nFirst, let's see that it is a map from L to W, and not merely a map from L to Zn. For any b ∈L,\nφ(b)n-1 = an-1bn-1 ≡an-1 ≡1\n(mod n).\nSo indeed φ(b) ∈W. It's also clearly 1 -1: if φ(b) = φ(b′), then multiplying by a-1 gives b = b′.\nThis concludes the proof.\nWe can conclude that the probability that if we run the Fermat test K times on a number\nn that is composite and not a Carmichael number, the probability that the Fermat test outputs\n\"maybe prime\" on all tries is at most 2-K; by choosing K large enough, we can make the error\nprobability tiny.\nThe defect of the Fermat primality test, that it does not work on Carmichael numbers, can be\nrectified by more advanced tests, in particular the Miller-Rabin test. This is based on a different\ncertificate of compositeness: if you can find a number a ∈Zn so that a2 ≡1 (mod n), but a ≡±1\n(mod n), then n is not prime.\nExercise. Prove this fact.\nThere is also a (much more complicated) deterministic primality test, due to Agrawal, Kayal\nand Saxena.\nRSA-5\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Factoring Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/48dedaa5cc66d0afc8666f101975762a_MIT18_310F13_Ch16.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nFactoring\nLecturer: Michel Goemans\nWe've seen that it's possible to efficiently check whether an integer n is prime or not. What\nabout factoring a number? If this could be done efficiently (for example, in say D4 operations,\nwhere D ∼log n is the number of digits of n), then the RSA encryption scheme could be broken,\njust by factoring N = pq in the public key.\nProbably the first algorithm that comes to mind is the brute-force approach: for each positive\ninteger d greater than 1 and not larger than √n, check if d divides n. The reason we go up to √n\nis simply that if n is composite, it must have a divisor no larger than √n. This can be done very\nefficiently for any particular choice of d, using the Euclidean algorithm: compute gcd(d, n), and if\nit's not 1, the result is a nontrivial1 factor of n. Unfortunately, this method is extremely slow, just\nbecause of the number of potential divisors to check: √n.\nWe will see two faster algorithms. The first, Pollard's rho algorithm will require roughly n1/4\ngcd operations (rather than n1/2 as above). The second, the quadratic sieve, will run roughly in\n√\ntime e\nlog n log log n. This looks a bit complicated, but notice that\n(log n)C = eC log log n\nand\nnε = eε log n.\n√\nSo e\nlog n log log n lies inbetween these two, in terms of its speed of growth as n gets large.\nPollard's rho algorithm\nSuppose n is the number to be factored, and n = pq, where p is a prime factor not larger than √n\n(q need not be prime here, if n has more than 2 prime factors). Note that we do not know p; once\nwe figure out p, we're done!\nConsider the following thought experiment.\nPick some numbers x1, x2, . . . , xluniformly at\nrandom in Zn (the choice of lwill be decided later). Assume that all the numbers are distinct (if\nn is large compared to l, this is very unlikely anyway). Now suppose that\nthere exists some 1 ≤i < j ≤lsuch that xi ≡xj\n(mod p).\n(1)\nThen p | xi -xj, and since p | n also, we have that\np | gcd(xi -xj, n).\nMoreover, since -n < xi -xj < n and xi = xk, gcd(xi -xj, n) < n. Thus gcd(xi -xj, n) provides\na nontrivial factor of n, and our job is done.\nAgain, keep in mind that we do not know p.\nBut, the above tells us that if we computed\ngcd(xi′ -xj′, n) for every pair 1 ≤i′ < j′ ≤l, we would find the nontrivial factor. But how big\ndoes lneed to be such that the chance that (1) holds is reasonable?\n1\"Nontrivial\" here just means that the factor is neither 1, nor n itself.\n-1\n\nThe answer is roughly √p. This is essentially a reformulation of the \"Birthday paradox\": the\nsomewhat surprising fact that in a group of only 30 people, the probability that two people in the\ngroup have the same birthday is already quite large--more than 60%. Let's calculate the probability\nthat (1) does not hold, i.e., that the residue classes (\"birthdays\") of x1, x2, . . . , xlare all different.\nThis is\nPr(all different) =\n\n1 -n\n\n1 -n\n\n· · ·\n\nl\n-1\n-\nn\n\n≤e-1/n · e-2/n · · · e-(l-1)/n\nusing 1\nl\n-y ≤e-y\n-\ne-(l\n1)\n=\n2n\n∼e-l2/(2n).\nSo if l= √n, the probability is roughly e-1/2 ≤2/3, and so there is a substantial probability that\n(1) occurs.\nSince p\n√\n≤\nn, we only need to choose l= n1/4 (remember we don't know p!). But--then the\nnumber of pairs i, j is\nl\nwhich is about 1√n. Checking all these pairs takes again about\n√n gcd\ncomputations--this is no better than brute search! What whas the point?\nThe clever trick here is to not pick the xi's randomly, but instead in a way that \"looks\" random.\nLet f : Zn →Zn be defined by\nf(x) = x2 + 1 mod n.\nFix any x0 ∈Zn. Now consider the sequence\nx1 = f(x0),\nx2 = f(x1),\n. . .\nxi = f(xi-1), . . . .\nEmpirical \"fact\": The sequence x0, x1, . . . \"looks random\".\nThis is not a precise statement, and indeed there is no formal proof that the algorithm that\nwe will describe now actually works efficiently. But it is well established empirically that it does,\nin that for most choices of x0, there will be a pair i < j < C√p (for some small constant C)\nwith xi ≡xj (mod p). The plan now is to look for such a pair i, j in this sequence (rather than a\nrandomly generated one). But we still can't check every pair i′, j′, so how does this help?\nThe observation is that we're actually looking for cycles in the sequence\nx0 mod p,\nx1 mod p,\nx2 mod p,\n. . . .\nFor suppose xi ≡xj (mod p), with i < j and i chosen as small as possible so that this holds.\nThen f(xi) ≡f(xj) (mod p), i.e., xi+1 ≡xj+1 (mod p) (you should check this!). Figure 1 shows\nthe situation schematically: the points in the picture represent the values Zp.\nEventually the\nsequence x0 mod p, x1 mod p, . . . runs into itself, and from then one goes around a cycle. (The\nname \"Pollard's rho algorithm\" comes from this picture...). It's worth recalling again at this point\nthat we don't know p, so we cannot directly see the cycle. Nevertheless, we can find it very efficiently\nwith the following algorithm.\nTortoise and hare algorithm:\n1. Let y0 = x0.\n-2\n\nxi+1 mod p\nxi mod p\nx1 mod p\nx0 mod p\nFigure 1: A cycle formed in the sequence x0 mod p, x1 mod p, . . ..\n2. For i = 1, 2, . . .\n(a) xi = f(xi-1).\n(b) yi = f(f(yi-1))\n(c) If gcd(xi -yi, n) = 1, return this discovered factor.\nInformally, we start a tortoise (the sequence (xi)) and a hare (the sequence (yi)) at x0, and the\ntortoise moves one step each turn, but the hare moves two. Once both the tortoise and the hare are\non the cycle, it's just a matter of time until the hare runs into the tortoise. Our \"empirical fact\"\nimplies that this algorithm will generally finish within C√p ≤Cn1/4 steps for some small constant\nC; if we run for much longer than this without finding a cycle, we can give up and start again with\na different choice of x0.\nThe quadratic sieve\nThis algorithm is closely related to the currently fastest known method for factoring. As already\n√\nmentioned, it takes time roughly e\nlog n log log n.\nThe approach is based on the following idea. If we could find two numbers a, b such that a ≡b\n(mod n), a ≡-b (mod n), but a2 ≡b2 (mod n), then we can easily get a factor. For we have that\nn | (a -b)(a + b), but n | a -b and n | a + b. Thus gcd(a + b, n) will be a nontrivial factor (as will\ngcd(a -b, n), but we only need one).\nLet g :\n√Z →\nZ\nn\nbe the function defined by g(z) = z mod n. Now if we could find some integer\nz, with\nn ≤z\n√\n≤\nn + K (K is something big, but much smaller than √n) and where g(z) is a\nperfect square, then we are done. (Note: by perfect square, we really mean a perfect square as an\ninteger, not as an element of Zn. So we mean g(z) = y2 for some integer y, not g(z) ≡y2 (mod n).)\nFor let g(z) = y2. Then y2 ≡z2 (mod n). But z ≡y (mod n), since y < √n ≤z < n. Moreover, if\nK is not too large, it can also be shown that z ≡-y (mod n). So then we can obtain a nontrivial\nfactor from gcd(y -z, n) as already discussed.\nExample. Suppose n = 1817. Then √\n⌈\nn⌉= 43. We get lucky: g(51) = 784 = 282. So 512 ≡282\n(mod 1817), hence 23 · 79 ≡0 (mod 1817), and we have a factor.\nUnfortunately, starting from\n√\n⌈\nn⌉and working our way up one at a time looking for a z s.t.\ng(z) is a perfect square does not work well; the special values of z are rare, and so the number of\n-3\n\ntries needed is again huge. But there is another hope. Suppose we find distinct z , z , . . . , z ∈\n,\nwith all zi\n√\nZ\nl\nn\n≥\nn, and where\nl\ng(zi)\nis a perfect square.\n(2)\ni=1\n(Again, note that we multiply the numbers g(zi) as normal integers, not modulo n; so this product\nl\nl\ncan be larger than n). Then this is also (probably) good for us: let z2 =\ni=1 zi and y =\ni=1 g(zi).\nThen y2 ≡z2 (mod n), and so as long as y ≡±z (mod n), we again find our nontrivial factor from\ngcd(y + z, n). It is (vaguely speaking) unlikely that y ≡±z (mod n), though we won't deal with\nthis formally; if we get unlucky, we have to try for another collection of zi's.\nIs it any easier to find a collection of zi's satisfying (2)? Notice that given an integer m with\nprime factorization m = pα1pα2\n2 · · · pαt\nt , then m is a perfect square if and only iffαj is even for each\n1 ≤j ≤t. The idea will be to work with zi's so that g(zi) contain only small primes in their prime\nfactorization, which will allow us to exploit this characterization of being a perfect square.\nDefinition 1. For B ∈N, an integer is called B-smooth if all its prime factors are at most B.\nFor example, 15 and 75 are 5-smooth, but 14 is not 5-smooth.\nIf B is not too large, we can i) check if an integer m is B-smooth, and ii) if it is, find its prime\nfactorization, reasonably efficiently. Simply compute gcd(pi, m) for each prime pi ≤B in turn, and\nif this is larger than 1, divide m by pi (keeping track of the number of factors of pi found). If m\neventually reduces down to 1, we have a prime factorization of m in terms of primes not larger than\nB; otherwise, m is not B-smooth.\nThe plan is: pick some B (which will grow with n unfortunately, but much more slowly than n;\n1 √log n log log n\nroughly, B = e 2\n), and find B + 1 numbers z1, z\nZ\n2, zB+1 ∈\nn, zi\n√\n≥\nn, so that g(zi) is\nB-smooth for each i. It turns out that these numbers are not so rare (depending on the choice of\nB), so we could do this just by trying each value z starting from √\n⌈\nn⌉, checking if g(z) is B-smooth,\nas described above.\nOnce we have these numbers z1, . . . , zB+1, we will in fact be able to find a subset of these\nnumbers which satisfy (2)! This is a consequence of some linear algebra, and the reason we wanted\nt\nα\nprecisely B+1 numbers. Let p1, . . . , pt be the list of primes of size at most B. Let g(zi) =\ni,j\nj=1 pj\nbe the prime factorization of g(zi) (of course, some αi,j's can be zero). Then for a giv\n\nen subset\nI ⊆{1, 2, . . . , B + 1},\nt\n\n(\n\ng(zi) =\np\ni∈I αi,j)\nj\n.\ni∈I\nj\n\n=1\nThus, applying our condition for a number to be a perfect square,\n\ng(zi)\nis a perfect square\n⇔\n\nαi,j is even ∀j ∈{1, 2, . . . , t}.\n(3)\ni∈I\ni∈I\nNow the linear algebra connection. (Warning: some knowledge of linear algebra will be assumed\nhere). Consider the (B + 1)\nZ\n× t matrix A, with entries Aij = αi,j mod 2. We think of the entries of\nA as being in\n2. Let Ai denote the i'th row of A (so this is a vector). Then what we are looking\nfor is a nonempty collection of rows I ⊆{1, 2, . . . , B + 1} so that\ni I Ai = 0. The existence of\n∈\nsuch a subset I follows from a key concept in linear algebra, linear dependence. Since A has only\n-4\n\nB columns, the rank of A is at most B; since there are more than B rows, this means there must\nbe a linear dependence amongst the rows, i.e., wi ∈Z2 so that\nB+1\n\nwiAi = 0\ni=1\n(and not all the wi are zero.) But that's precisely what we want; just take I = {i | wi = 1}. There\nis also an efficient algorithm to find this linear dependence.\n-5\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Fast Fourier Transform Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/b4a37cf6c0e515fb863e139c2a6a51ac_MIT18_310F13_Ch17.pdf",
      "content": "18.310 lecture notes\nNovember 18, 2013\nFast Fourier Transform\nLecturer: Michel Goemans\nIn these notes we define the Discrete Fourier Transform, and give a method for computing it\nfast: the Fast Fourier Transform. We then use this technology to get an algorithms for multiplying\nbig integers fast. Before going into the core of the material we review some motivation coming from\nthe classical theory of Fourier series.\nMotivation: Fourier Series\nIn this section we discuss the theory of Fourier Series for functions of a real variable. In the next\nsections we will study an analogue which is the \"discrete\" Fourier Transform.\nEarly in the Nineteenth century, Fourier studied sound and oscillatory motion and conceived of\nthe idea of representing periodic functions by their coefficients in an expansion as a sum of sines\nand cosines rather than their values. He noticed, for example, that you can represent the shape of\na vibrating string of length L, fixed at its ends, as\ninf\ny(x) =\n\nak sin(πkx/L).\nk=1\n(Observe that indeed y(0) = y(L) = 0.) The coefficients, ak, contain important and useful infor-\nmation about the quality of the sound that the string produces, that is not easily accessible from\nthe ordinary y = f(x) description of the shape of the string.\nThis kind of representation is called a Fourier Series, and there is a tremendous amount of\nmathematical lore about properties of such series and for what classes of functions they can be\nshown to exist. One particularly useful fact about them is the orthogonality property of sines:\nL\nL\nsin(πkx/L) sin(πjx/L)dx = δj,k\n,\nx=0\nfor nonnegative integers j and k. Here δj,k is the Kronecker delta function, which is 0 if j = k and\n1 if j = k. The integral above, then, is 0 unless j = k, in which case it is L/2. To see this, you can\nwrite\nsin(πkx/L) sin(πjx/L) =\ncos(π(k\nj)x/L)\n-\n-\ncos(π(k + j)x/L),\nand realize that unless j = ±k, each of these cosines integrates to 0 over this range.\nBy multiplying the expression for y(x) above by sin(πjx/L), and integrating the result from 0\nto L, by the orthogonality property everything cancels except the sin(πjx/L) term, and we get the\nexpression\naj = L\nL\nf(x) sin(πjx/L)dx.\nx=0\nNow, the above sum of sines is a very useful way to represent a function which is 0 at both\nendpoints. If we are trying to represent a function on the real line which is periodic with period L,\nFFT-1\n\nit is not quite as useful. This is because the sum of sines above is not periodic with period L but\nonly periodic with period 2L. For periodic functions, a better Fourier expansion is\ninf\ninf\ny(x) = a0 +\n\naj cos(2πjx/L) +\nj=1\n\nbk sin(2πkx/L).\nk=1\nIt is fairly easy to rewrite this as a sum of exponentials (over the complex numbers), using the\nidentity eix = cos(x) + i sin(x) which implies\neix + e-ix\ncos x\n=\neix\nsin x\n=\n-e-ix\n.\n2i\nThis results in the expression (with a different set of coefficients cj)\ninf\ny(x) =\n\nc e2πijx/L\nj\n,\n(1)\nL j=-inf\nwhere i is the standard imaginary unit with i2 = 1. The scaling factor 1 is introduced here for\nL\nsimplicity; we will see why shortly. The orthogonality relations are now\nL\ne2πijx/Le2πikx/Ldx = δ\nj,kL,\nx=0\n-\nand thus, after dividing by L, we get that the integral is 0 or 1. This means that we now can\nrecover the cj coefficient from y by calculating the integral\nc =\nL\ny(x)e-2πijx/L\nj\ndx.\n(2)\nx=0\n(2) is referred to as the Fourier transform and (1) to as the inverse Fourier transform. If we hadn't\nintroduced the factor 1/L in (1), we would have to include it in (2), but the convention is to put\nit in (1).\nThe Discrete Fourier Transform\nSuppose that we have a function from some real-life application which we want to find the Fourier\nseries of. In practice, we're not going to know the value of the function on every point between\n0 and L, but just on some finite number of points. Let's assume that we have the function at n\nequally spaced points, and do the best that we can. This gives us the finite Fourier transform, also\nknown as the Discrete Fourier Transform (DFT).\nWe have the function y(x) on points j, for j = 0, 1, . . . , n-1; let us denote these values by yj for\nj = 0, 1, · · · , n -1. We define the discrete Fourier transform of y0, . . . , yn\n1 to be the coefficients\n-\nc0, . . . , cn-1, where\nn-1\nck =\n\nyje-2πijk/n,\n(3)\nj=0\nFFT-2\n\nfor k = 0, · · · , n -1.\nObserve that it would not make sense to define (these complex Fourier coefficients) ck for more\nvalues of k since the above expression is unchanged when we add n to k (since e2πi = 1). This\nmakes sense -- if we start with n complex numbers yj's, we end up with n complex numbers ck's,\nso we keep the same number of degrees of freedom.\nCan we recover the yj's, given the ck's? Yes, this is known as the inverse Fourier transform,\nand is stated below.\nTheorem 1. If c0, c1, . . . , cn\n1 is the discrete Fourier transform of y\n-\n0, . . . , yn-1. Then\nn\n-\nyj = n\n\nc\nke πijk/n,\n(4)\nk=0\nfor j = 0, · · · , n -1.\nThe equation (4) is known as the inverse discrete Fourier transform. Observe that this is similar\nto (1), except that the scaling factor 1 which replaces 1 is not at the same place.1\nn\nL\nThe proof of Theorem 1 will be based on the following lemma.\nLemma 1. If z is a complex number satisfying zn = 1 and z, z2, . . . , zn-1 = 1 then we have the\nfollowing \"orthogonality relation\": for all j, k ∈{0, 1, . . . , n -1},\nn-1\n\nzjlz-kl = nδj,k,\nl=0\nwhere δj,k is the Kronecker delta function, which is 0 if j = k and 1 if j = k.\nObserve that z = e-2πi/n satisfies the condition of the Lemma 1 so the orthogonality relations\nturn into the sum\nn-1\n\ne-2πijl/ne2πikl/n = nδj,k.\nl=0\nProof of Lemma 1.\nn-1\nn-1\nn-1\n\nzjlz-kl =\nl=0\n\n(zj-k)l =\nl=0\n\nwl\nl=0\nwhere w = zj-k.\nIf k = l then w = 1 and the above sum equals n.\nOn the other hand, if\nk = l then our assumption on zi = 1 for i ∈{1, 2, · · · , n -1} means that w = 1. Thus, we have\nn-1\nl=0 wl = (wn -1)/(w -1) = 0, since wn = 1.\nWe are now ready to prove the Theorem.\n1This is just a matter of convention. Actually, to avoid the confusion that this\n1 factor may create, sometimes\n√\nn\nthis factor of 1/n is distributed equally, with a 1/\nn on both the forward and the inverse Fourier transforms; we will\nnot use this.\nFFT-3\n\nProof of Theorem 1. The definition of ck can be written as\nn-1\nc =\n\ny zkj\nk\nj\n,\nj=0\nwhere z = e-2πi/n.\nNow we can compute\nn\n-1\nn-1\n\nc\n2πijl/n\njl\nle\n=\nn\nn\nl=0\nn\n\nclz-\nk=0\n-1 n-1\n=\nn\n\n(\nykzkl)z-jl\nl=0\nn\n\nk=0\n-1\nn-\n=\n\nyk\n\nzklz-jl\nn\nk=0\n\nl=0\n\n=\nyj.\nwhere the last equality comes from applying Lemma 1 to z (which shows that all but one of the\ninner sums are 0).\nComputing the discrete Fourier transform\nIt's easy to compute the finite Fourier transform or its inverse if you don't mind using O(n2)\ncomputational steps. The formulas (4) and (3) above both involve a sum of n terms for each of n\ncoefficients. However, there is a beautiful way of computing the finite Fourier transform (and its\ninverse) in only O(n log n) steps.\nOne way to understand this algorithm is to realize that computing a finite Fourier transform is\nequivalent to plugging into a degree n -1 polynomial at all the n n-th roots of unity, e2πik/n, for\n0 ≤k ≤n -1. (Recall that an n-th root of unity is any (complex) number such that zn = 1; for\nexample, the 4th root of unity are 1, eiπ/2 = i, eiπ = -1 and ei3π/2 = -i.) The Fourier transform\nand its inverse are essentially the same for this part, the only difference being which n-th root of\nunity you use, and that one of them has to get divided by n. So, let's do the forward discrete\nFourier transform (3).\nSuppose we know the values of yj and we want to compute the ck using the Fourier transform,\n(3). Let the polynomial p(x) be\nn-1\np(x) =\n\nyjxj.\nj=0\nNow, let z = e-2πi/n. Then, it is easy to check that we have\nck = p(zk).\nThis shows we can express the problem of computing the Fourier transform as evaluating the\npolynomial p (of degree n -1) at the n-th roots of unity. (If we were computing the inverse one\nFFT-4\n\n(i.e. exchange the role of y and c ), we would use the root z = e2πi/n\nj\nk\nand divide the overall result\nby 1/n.)\nWhat we will show is that if n is even, say n = 2s, it will be possible to find two degree s -1\npolynomials (thus of degrees roughly half the degree of p(x)), peven and podd, such that we get all n\nof the values ck for 0 ≤k ≤n -1 by plugging in the s-th roots of unity (rather than the n-th roots\nof unity) into peven and podd. The evaluation of p at even powers of z will appear when evaluating\npeven, and the odd powers of z will appear in podd. If n is a multiple of 4, we can then repeat this\nstep for each of peven and podd, so we now have our n values of ck appearing as the values of four\npolynomials of degree n/4 -1, when we plug the n-th units of unity, i.e., the powers of z4, into all\nof them. If n is a power of 2, we can continue in the same way, and eventually reduce the problem\nto evaluating n polynomials of degree 0. But it's really easy to evaluate a polynomial of degree 0:\nthe evaluation is the polynomial itself, which only has a constant term. So at this point we will be\ndone.\nThe next question we address is: how do we find these two polynomials peven and podd? We will\ndo the case of peven first. Let us consider an even power of z, say z2k, at which we want to evaluate\np(·). We look at the j-th term and the (j + s)-th term. These are\nyjz2kj\nand\nyj+sz2kj+2ks.\nBut since z2s = zn = 1, we have\nz2kj+2ks = z2kj.\nThus, we can combine these terms into a new term in the polynomial peven, with coefficients\nbj = yj + yj+s.\nIf we let\ns-1\npeven(x) =\n\nbjxj\nj=0\nwe find that\np(z2k) = peven(z2k).\nObserve furthermore that since zk is an n-th root of unity, z2k is an s-th root of unity (since n = 2s).\nNow, let us do the case of the odd powers. Suppose we are evaluating p at an odd power of z,\nsay z2k+1. Again, let's consider the contribution from the j-th and the (j + s)-th terms together.\nThis contribution is\nyjz(2k+1)j + y\n+\nj\nsz(2k+1)(j\ns)\n+\n.\nHere we find that z(2k+1)s = e(2πi)(2k+1)s/n = e(πi)(2k+1) = -1. We now have\ny z(2k+1)j + y\nz(2k+1)(j+s)\n=\n(y zj)z2kj\nj\nj\n+ (\nj+s\nj\nyj+sz )z kj(-1)\n=\n(yj -y\nj\n2kj\nj+s)z z\n.\nSetting the j-th coefficient of podd to\nbj = (yj -yj+s)zj\nFFT-5\n\nand letting\ns-1\npodd(x) =\nbjxj\nj=0\nwe see that\np(z2k+1) = podd(z2k).\nWhat we just did was reduce the problem of evaluating one degree n -1 polynomial, p, at the\nn-th roots of unity to that of evaluating two degree n\n2 -1 polynomials, podd and peven at the n-th\nroots of unity. That is, we have taken a problem of size n and reduced it to solving two problems\nof size n. We've seen this type of recursion before in sorting, and you should recognize that it will\ngive you an O(n log n) algorithm for finding the finite Fourier transform.\nSo now, we can show how the Fast Fourier transform is done. Let's take n = 2t. Now, consider\nan n×t table, as we might make in a spreadsheet. Let's put in our top row the numbers y0 through\nyn\n1. In the next row, we can, in the first n places, put in the coefficients of p\ni\n-\neven, and then n\nthe next n places, put in the coefficients of podd. In the next row, we repeat the process, to get\nfour polynomials, each of degree n -1. After we have evaluated the second row, we treat each of\np\nn\neven and podd separately, so that nothing in the first\ncolumns subsequently affects anything in\nthe last n columns. In the third row, we will have in the first n places the coefficients of peven,even,\nwhich give us the value of p(z4k) when we evaluate p\n(z4k). Then in the next n\neven,even\nplaces, we\nput in the coefficients of peven,odd. This polynomial will give the value of p(z4k+2) when we evaluate\npeven,odd(z4k). The third n places will contain the coefficients of podd,even, which gives us the values\nof p(z4k+1). The last n places will be occupied by the coefficients of p\nodd,odd, which gives the values\nof p(z4k+3). From now on, we treat each of these four blocks of n columns separately. And so on.\nThere are two remaining steps we must remember to carry out. The first step arises from the\nfact that is that the values of p(zk) come out in the last row in a funny order. We have to reshuffle\nthem so that they are in the right order. I will do the example of n = 8. Recall that in the second\nrow, the polynomial po, giving odd powers of z, followed pe, giving even powers of z. In the third\nrow, first we get the polynomial giving z4k, then z4k+2, then z4k+1, then z4k+3. So in the fourth\nrow (which is the last row for n = 8), we get the values of p(zk) in the order indicated below.\ncoefficients of p\np (z2k) = p(z2k)\np (z2k\ne\no\n) = p(z k+1)\np\n(z4k) = p(z4k)\np\n(z4k) = p(z4k+2\ne,e\ne,o\n)\npo,e(z4k) = p(z4k+1)\npo,o(z4k) = p(z4k+3)\np(z0)\np(z4)\np(z2)\np(z6)\np(z1)\np(z5)\np(z3)\np(z7)\nYou can figure out where each entry is supposed to go is by looking at the numbers in binary, and\nturning the bits around. For example, the entry in column 6 (the 7th column as we start labeling\nwith 0) is p(z3). You can figure this out by expressing 6 in binary: 110. You then read this binary\nnumber from right to left, to get 011, which is 3. Thus, the entry in the 6 column is p(z3). The\nreason this works is that in the procedure we used, putting in the even powers of z first, and then\nthe odd powers of z, we were essentially sorting the powers of z by the 1's bit. The next row ends\nup sorting them by the 2's bit, and the next row the 4's bit, and so forth. If we had sorted starting\nwith the leftmost bit rather than the rightmost, this would have put the powers in numerical order.\nSo, by numbering the columns in binary, and reversing the bits of these binary numbers, we get\nthe right order of the transformed sequence.\nFFT-6\n\nThe other thing we have to do is to remember to divide by n if it is necessary. We only need\ndo this for the inverse Fourier transform, and not the forward Fourier transform.\nComputing convolutions of sequences using Fast Fourier Trans-\nform\nSuppose you have two sequences f0, f1, . . . , fn\n1 and g0, g1, . . . , gn\nt\n-1 and wan\nto compute the\n-\nsequence h0, h1, . . . , hn\ndefined by\n-1\nn-1\nhk =\n\nfjgk-j\nj=0\nwhere the index k-j is taken modulo n. Clearly it is possible to compute the numbers h0, . . . , hn-1\nin n2 arithmetic operations. We will now explain how to do it faster.\nLet ak and bk be the discrete Fourier transform of fk and gk and their finite Fourier, that is,\nak\n=\n\nfje-2πijk/n\nj\nbk\n=\n\ngje-2πijk/n.\nj\nNow let's compute the inverse Fourier transform of the sequence akbk. For all l = 0, · · · , n -1 we\nget :\n′\ne2πilk/na\n2πilk/n\n2πijk/n\n2πij k/n\nkbk\n=\ne\nfje-\nk\n\nj\n\ngj′e-\nn\nn\n\nk\nj′\n=\n\nf g\n\ne2πik(l\nj j′\n-j-j′)/n\nn\nj\nj′\nk\nn-1\n=\n\nfjgl-j\nj=0\n=\nhl\nwhere the second last equality holds because the sum over k is 0 unless l ≡j + j′ (mod n).\nWe have just found a way of computing the sequence h0, h1, . . . , hn-1 by first applying Fourrier\ntransform to the sequences f0, f1, . . . , fn\n1 and g0, g1, . . . , gn\n1 and then taking the inverse Fourier\n-\n-\ntransform of the sequence akbk. Since the Fourier transforms and inverse Fourrier transform can be\ncomputed in O(n log(n)) operations, the sequence h0, h1, . . . , hn-1 can be computed in O(n log(n))\ninstead of O(n2) operations.\nWe can now use this method in order to multiply polynomials efficiently. Suppose we have two\ndegree d polynomials, and we want to multiply them. This corresponds to convolution of the two\nseries that make up the coefficients of the polynomials. If we do this the obvious way, it takes\nO(d2) steps. However, if we use the Fourier transform, multiply them pointwise, and transform\nback, we use O(d log d) steps for the Fourier transforms and O(d) steps for the multiplication. This\ngives O(d log d) total, a great savings. We must choose the n for the Fourier series carefully. If we\nmultiply two degree d polynomials, the resulting polynomial has degree 2d, or 2d + 1 terms. We\nFFT-7\n\nmust choose n ≥2d + 1, because we need to have room in our sequence f0, f1, . . . fn-1 for all the\ncoefficients of the polynomial; if we choose n too small, the convolution will \"wrap around\" and\nwe'll end up adding the last terms of our polynomial to earlier terms.\nFourier transforms modulo p and fast integer multiplication\nSo far, we've been doing finite Fourier transforms over the complex numbers. We can actually\nwork over any field with a primitive n-th root of unity, that is, a number z such that zn = 1 and\nz, z2, . . . , zn-1 = 1. Indeed if such a z exists, we can define the Fourier transform of some number\ny0, . . . , yn-1 as\nn-1\nck =\n\nyjz-jk.\nj=0\nIn this case we can prove similarly as in Section 2 that the inverse Fourier transform is\nn-1\nyj = n-1\nc zjk\nk\n.\nk=0\nThe factor n-1 is the multiplicative inverse of n over this field, and comes from the fact that\nn\nk=0\n-z = n.\nIf we take a prime p, then the field of integers mod p has a primitive n-th root of unity if\np = mn + 1 for some integer m. In this case, we can take the Fourier transform over the integers\nmod p. Thus, 17 has a primitive 16-th root of unity, one of which can be seen to be 3. (By Fermat's\nlittle theorem, any a = 0 satisfies a16 ≡1 (mod 17), but for many a's, a smaller power than 16\nwill give 1. For example, modulo 17, 1 is a primitive 1st root of unity, 16 is a primitive 2nd root\nof unity, 4 and 13 are primitive 4-th root of unity, 2, 8, 9 and 15 are primitive 8th roots of unity\nand 3, 5, 6, 7, 10, 11, 12 and 14 are primitive 16-th root of unity.) So if we use z = 3 in our fast\nFourier transform algorithm, and take all arithmetic modulo 17, we get a finite Fourier transform.\nAnd we have seen how to compute n-1 modulo a prime p by the Euclidean gcd.\nWe can use this for multiplying polynomials. Suppose we have two degree d polynomials, each\nof which has integer coefficients of size less than B. The largest possible coefficient in the product is\n(B -1)2(d + 1). If we want to distinguish between positive and negative coefficients of this size, we\nneed to make sure that p > 2(B-1)2(d+1). We also need to choose n ≥2d+1, so as to have at least\nas many terms as there are coefficients in the product. We can then use the Fast Fourier transform\n(mod p) to multiply these polynomials, with only O(d log d) operations (additions, multiplications,\ntaking remainders modulus p), where we would have needed d2 originally.\nNow, suppose you want to multiply two very large integers. Our regular representation of these\nintegers is as\nk dk10k, where d\nk\nk are the digits. We can replace this by\nk dkx\nto turn it into a\npolynomial, then multiply the two polynomials using the fast Fourier transform.\nHow many steps does this take? To make things easier, let's assume that our large integers are\ngiven in binary, and that we use a base B which is a power of 2. Let's assume the large integers\nhave N bits each and that we use a base B (e.g., 10 in the decimal system, 2 in binary) that has\nb bits. We then have our number broken up into N/b \"digits\" of b bits each. How large does our\nprime have to be? It has to be larger than the largest possible coefficient in the product of our two\nFFT-8\n\npolynomials. This coefficient comes from the sum of at most N/b terms, each of which has size at\nmost (2b -1)2 < 22b. This means that we are safe if we take p at least\nN\n(\n)22b\nb\nor taking logs, p must have around 2b + log\nN\nbits.\nb\nRather than optimizing this perfectly, let's just set the two terms in this formula to be approx-\nimately equal by letting b = log2 N; this is much simpler and will give us the right asymptotic\ngrowth rate. We thus get that p has around 3 log2 N bits. We then set n to be a power of 2 larger\nthan 2 N , so that our finite Fourier transform involves O(n log n) = O(N) operations, each of which\nb\nmay be an operation on a (3 log2 N)-bit number. If we use longhand multiplication and division\n(taking O(b2) time) to do these operations, we get an O(N log2 N)-time algorithm.\nThere's no reason that we need to stop there. We could always use recursion and perform these\noperations on the 3b-bit numbers using fast integer multiplication as well. If we use two levels of\nrecursion, we get an O(N log N(log log N)2) time algorithm. If we use three levels of recursion, we\nget an O(N log N(log log N)(log log log N)2 time algorithm, and so forth.\nIt turns out, although we won't go into the details, that you can get a O(N log N log log N)\ntime algorithm. The main difference from what we've done is that you choose the number you use\nk\nto do the FFT not of size around log N, but of a number of the form 22 + 1 of size around\n√\nN\n(it actually doesn't have to be prime). You then carefully compute the time taken by applying\nk\nthis algorithm recursively, making sure that you use the fact that mod 22 + 1, multiplication by\nsmall powers 2 can be accomplished fairly easily by just shifting bits. Details can be found in\nAho, Hopcroft and Ullman's book \"Design and Analysis of Computer Algorithms.\" In fact, very\nrecently, still using the finite Fourier transform, Furer\n\nfound a way to speed up multiplication even\nfurther so that the running time is only a tiny bit more than O(N log N).\nFFT-9\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Finding the Median Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/25c0eb2a94ecef291b061eebda640139_MIT18_310F13_Ch11.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nFinding the Median\nLecturer: Michel Goemans\nThe problem of finding the median\nSuppose we have a list of n keys that are completely unsorted. If we want to find the largest or the\nsmallest key, it is very easy to do so with n -1 comparisons. If we want to find the mth largest\nkey then you can use a heap structure to do it in cn + cm log(n) comparisons for some constant c.\nIndeed, it takes linear time to build the heap and then one can extract the maximum key m times\nin order to get the desired key (at a cost of log(n) comparisons per extraction). If m is larger than\nn/2 we can do slightly better by reversing the ordering (that is, building the heap based on minus\nthe keys). In any case, if we are interested in the mth smallest key (the key of rank m) and m is\nclose to n/2 (or linear in n), we are not saving much time comparing to sorting all keys. In these\nnotes, we show how to find the median (key of rank (n + 1)/2 if n is odd, or key of rank n/2 or\nn/2 + 1 if n is even) with only a linear number (cn) comparisons in the worst-case.\nA good strategy ?\nThe first approach that comes to mind when trying to find the rank m key is to take an arbitrary\nkey, say p, and partition the n keys into two piles, one containing the keys smaller than p and\nthe other containing the keys greater than p. By comparing m to the size of the piles, we can\neither find out that we got extremely lucky and the mth key is p or we can discard one of the piles\nand recursively search the appropriate key in the other pile. More precisely, if the first pile has at\nleast m elements, we can recursively find the rank m key in the first pile; if not, we can replace m\nappropriately and search in the second pile. The problem of this approach is that, in the worst-case,\none of the piles will (repeatedly) be empty, which would result in a number of comparisons equal\nto (n -1) + (n -2) + · · · , which grows quadratically with n.\nWe will now describe a technique for finding a key p for which the two piles will be reasonably\nbalanced, and this will result in an algorithm for finding the median or any rank m key that takes a\nlinear number of comparisons. This will not be a practical algorithm, as there are algorithms that\nrun much faster most of the time (although in the worst-case will take more than a linear number\nof comparisons). We show you this because it is a really neat algorithm, and because you will learn\nsome useful techniques and facts about recursion from it.\nThe big question is how to find a \"good\" candidate for the key p. Ideally, we would like p to\nbe as close as possible to the median1. The following is a simple plan for doing just that.\nStep 1. We arbitrarily split the keys into groups of size 5, assuming n is divisible by 5; we ignore\nsmall differences otherwise. (Why did we choose 5? Actually, any small odd number larger\nthan 3 works, but it turns out that 5 gives the fewest number of comparisons.)\n1You might wonder why do we try to find a good candidate for the median while our goal is to find the key of\nrank m? Again, we would like the two piles to be of roughly the same size so that we make progress faster.\nMedian-1\n\nHere is a an example of a set of 35 keys; the seven groups of 5 correspond to the columns.\nWe then sort each group of 5 keys, noting that each one can be sorted using a maximum of\n7 comparisons (this is a slightly tricky exercise). After performing 7n/5 comparisons, this\nleaves us with n/5 groups of 5 sorted keys. Here is the example in which the columns (groups\nof 5) are sorted from top to bottom).\nStep 2. Now, we take each of the keys of rank 3 (one per group), the median in its group of 5, and\nfind the median of this group of n/5 median keys. This \"median of medians\" is our good\ncandidate p. If f(m) is the number of comparisons it takes to find the median of m keys, this\nstep takes f(m/5) comparisons.\nIn our example, we find the median of:\nwhich happens to be p = 40.\nStep 3. Now we partition the keys in two piles, those smaller than p are placed in L< and those larger\nthan p are placed in L>. If we find other keys equal to p, we can place them either in L< or\nin L>.\nIn order to make this partition, we first compare the rank 3 key of each group of five to p.\nThis takes n/5 comparisons. If such a rank 3 key is larger than p then not only do we know\nthat it can be placed in L> but we also know that the rank 4 and rank 5 keys of that group\ncan also be placed in L>. Similarly, if the rank 3 key happens to be smaller then we can\nautomatically place the rank 1 and 2 keys of that group in L<. For example, we first compare\n75 (the rank 3 key of the first group) to 40 and can immediately tell that the keys 82 and 91\nare greater than 40 without doing additional comparisons. By comparing the rank 3 elements\nin the example to p, we can immediately decide in which pile to place the following keys:\nMedian-2\n\nThis has two implications. First, to construct both piles, we only need to do (at most2) 3\ncomparisons for each group of 5, for a total of 3n/5 comparisons.\nSecondly, since half of the rank 3 keys will be smaller or equal to p, at least 3 1 n = 3n keys\n2 5\nwill be in L> and similarly for L<. This means that both\n3n\n7n\n10 ≤|L<| ≤\n,\nand\n3n\n7n\nL\n10 ≤|\n>| ≤\n.\nBy simply counting the number of keys we place in L< and in L>, we can find out the exact\nrank of our candidate p.\nStep 4. Knowing the rank of p and comparing it to m, we can keep only one of the two piles thereby\neliminating at least 3n/10 keys. If m is smaller than the rank of p we can throw away all keys\nin L>, while if m is larger than the rank of p, we can discard L< (and decrease our value of\nm by the number |L<| of keys we have just discarded).\nIn any case, we have at most 7n/10 keys left, and need to find the rank m′ key in them. This\ntakes at most f(7n/10) comparisons in the worst-case.\nIt is important to realize that our procedure does not involve circular reasoning, even though\nour procedure uses as a subroutine a procedure for finding the rank m′ key. What we are doing\nis using the technique of recursion. To find the rank m key out of n keys, as intermediate steps\nwe find the median of n/5 keys and the the rank m′ key out of 7n/10 keys. During each of these\nintermediate steps, we again run the procedure for a smaller number of keys. Our algorithm does\nnot run forever because we will terminate this recursion whenever the number of keys is small\n(which might be when we reach fewer than five keys); in this case we must use a different procedure\nfor finding the rank m key (we could sort them, for example -- this is more efficient for small n).\nShowing the Procedure is Linear\nIf f(n) is the number of comparisons to find the rank m key out of n keys (irrespective of what m\nis), we have the formula\n7n\nf(n) ≤\n+ f\nn\n3n\n+\n+ f\n7n\n\n,\nwhere the 7n/5 is the cost of sorting the groups of 5, the f(n/5) is the number of comparisons\nrequired to find the median of medians, p, the 3n/5 is the cost of comparing the median of medians\nto all the keys, and f(7n/10) is the number of comparisons used to find the rank m element out of\n7n/10 elements.\nHow do we know that the number of comparisons f(n) is linear in the number of keys? That\nis, how do we know that f(n) ≤cn for some constant c? We will show this by using induction.\n2if the rank 3 key happens to be equal to p, we don't need any additional comparisons since we can place the rank\n1 and 2 keys in L< and the rank 4 and 5 keys in L>.\nMedian-3\n\nSuppose that we have shown that the equation f(m) ≤cm holds for all m < n (we'll figure out the\nvalue of c later). Remember we have\nn\nf(n) ≤f\n\n+ f\n7n\n+ 2n.\nBut we can substitute f(m) ≤cm for both of these values of f on the right hand side of the\nequation, because for both of these we have m ≤n. This gives:\ncn\nc7n\nf(n) ≤\n+\n+ 2n.\nNow, we'd like to choose c so that this inequality is true for all n. To get the best value for c,\nwe can impose that the right-hand side of this equation is equal to cn. This is a linear equation\ncn = cn + c 7n + 2n where the n's cancel and the solution is c = 20. We also need to verify that\nwhen n is small enough and we decide to simply sort the keys that the number of comparisons is\nindeed at most 20n.\nImproving the procedure\nThere are lots of clever tricks we can use to improve the constant in this procedure. All use the\nfact that we have been discarding lots of precious information that we have gathered in previous\nsteps, and that we could instead recycle. We will describe one extremely simple improvement.\nIn step 2, when finding the median p of the n/5 rank 3 keys, we end up knowing for each of\nthese n/5 keys whether they are greater or smaller than p. There is no need therefore to compare\nthem again to p in step 3. By combining step 2 and part of step 3, we can therefore decrease the\nnumber of comparisons in step 3 from 3n/5 to 2n/5. The recurrence we thus get says that\n7n\nn\nf(n) ≤\n+ f\nn\n7n\n+\n+ f\n\n,\nand this implies that f(n) ≤18n.\nMany further improvements can be made, but we won't discuss them here.\nMedian-4\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, Generating Functions Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/1d3495b2adb2dc35cb8dd1933398a647_MIT18_310F13_Ch7.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nGenerating Functions\nLecturer: Michel Goemans\nWe are going to discuss enumeration problems, and how to solve them using a powerful tool:\ngenerating functions. What is an enumeration problem? That's trying to determine the number of\nobjects of size n satisfying a certain definition. For instance, what is the number of permutations of\n{1, 2, . . . , n}? (answer: n!), or what is the number of binary sequences of length n? (answer: 2n).\nOk, now let us introduce some tools to answer more difficult enumerative questions.\nWhat is a generating function?\nA generating function is just a different way of writing a sequence of numbers. Here we will be\ndealing mainly with sequences of numbers (an) which represent the number of objects of size n\nfor an enumeration problem. The interest of this notation is that certain natural operations on\ngenerating functions lead to powerful methods for dealing with recurrences on an.\nDefinition 1. Let (an)n\n0 be a sequence of numbers. The generating function associated to this\n≥\nsequence is the series\nA(x) =\nn\n\nanxn.\n≥0\nAlso if we consider a class A of objects to be enumerated, we call generating function of this class\nthe generating function\nA(x) =\n\nanxn,\nn≥0\nwhere an is the number of objects of size n in the class.\nNote that the variable x in generating functions doesn't stand for anything but serves as a\nplaceholder for keeping track of the coefficients of xn.\nExample 1. The generating function associated to the class of binary sequences (where the size\nof a sequence is its length) is A(x) =\nn≥0 2nxn since there are a\nn\nn = 2\nbinary sequences of size\nn.\nExample 2.\nLet p be a positive integer.\nThe generating function associated to the sequence\nan =\nk\nn\n\nfor n ≤k and an = 0 for n > k is actually a polynomial:\nA(x) =\nn\n\nx\n≥\nk\n+\nn\n\nxn = (1\n)k.\nHere the second equality uses the binomial theorem. Thus A(x) = (1 + x)k is the generating func-\ntion of the subsets of {1, 2, . . . , k} (where the size of a subset is its number of elements).\nGenFun-1\n\nWe see on this second example that the generating function has a very simple form. In fact,\nmore simple than the sequence itself. This is the first magic of generating functions: in many\nnatural instances the generating function turns out to be very simple.\nLet us now modify this example in connection with probabilities. Suppose we have a coin having\nprobability p of landing on heads and a probability q = 1-p of landing on tails. We toss it k times\nand denote by an the probability of getting exactly n heads. What is the generating function of\nthe sequence (an)? Well, it can be seen that an =\nk\nn\nk\nk\nn n\n\nqk-npn thus the generating function is\nA(x) =\n\nq -p xn = (q + px)k,\nn\nn≥0\nusing the binomial theorem again.\nNow, observe that the generating function is\n(q + px)(q + px)(q + px) · · · (q + px),\nwhich is just multiplying k times the generating function (q+px) corresponding to a single toss of the\ncoin1. This is the second magic of generating functions: the generating function for complicated\nthings can be obtained from the generating function for simple things. We will explain this in\ndetails, but first we consider an example.\nOperations on classes and generating functions\nWe start with an easy observation.\nSuppose that A and B are disjoint classes of objects, and\nC = A ⊎B is their union (the symbol ⊎denotes disjoint union). For instance A could be the set of\npermutations and B could be the set of binary sequences. Can we express the generating function\nof C(x) of C in terms of the generating function A(x) =\nn≥0 anxn of A and B(x) =\nn≥0 bnxn\nof B? Well yes it is simply\n\nC(x) =\n\n(an + bn)xn =\nanxn +\nbnxn = A(x) + B(x)\nn≥0\nn\n\n≥0\nn\n\n≥0\nsince the number cn of objects of size n in C is an + bn.\nWe have just seen addition of generating functions, and we will now look at multiplication of\ngenerating functions. Consider the following problem. We have a die with six faces (numbered 1\nto 6) and a die with eight faces (numbered 1 to 8). We roll the dice and we consider the sum of\nthe dice. We want to know the number of ways cn of getting each number n.\nWe claim that that the generating function C(x) =\nn\nn≥0 cnx\nis given by\nC(x) = (x + x2 + x3 + x4 + x5 + x6) × (x + x2\n\n+ x3 + x4 + x5 + x6 + x7 + x8).\n1This is not a coincidence: if we were to expand out the product into a sum, it would be a sum of 2k terms, each of\nwhich takes either a q or a px from each of the k terms in the product. Hence each terms can be seen as a particular\nsequence of tails (represented by q) and head tosses (represented by px). In this calculation, the x's were a device\nfor keeping track of the number of heads.\nGenFun-2\n\nIndeed the first part accounts for the possible outcomes of the first die and the second part accounts\nfor the possible outcome of the second die. For instance getting the sum 5 by getting 2 from the\nfirst die and 3 from the second die is accounted by the multiplication of the monomial x2 from the\nfirst parenthesis with monomial x3 from the second parenthesis x3, etc. Multiplying this out, we\nget\nC(x) = 1+2 x+3 x2 +4 x3 +5 x4 +6 x5 +7 x6 +7 x7 +7 x8 +6 x9 +5 x10 +4 x11 +3 x12 +2 x13 +x14.\nIn the above problem we see that multiplying generating function is meaningful. Let us now\ntry to generalize the above reasoning. Given two sets, A and B the Cartesian product A × B is\ndefined as the set of pairs (a, b) with a ∈A and b ∈B. So if A and B are finite the cardinality of\nthese sets are related by |A × B| = |A| × |B|. We also suppose that the size of a pair (a, b) is the\nsize of a plus the size of b.\nFor instance, in the example above the class A represents the possible numbers of the first die,\nso that A = {1, 2, 3, 4, 5, 6} and the class B represents the possible number of the second die, so\nthat B = {1, 2, 3, 4, 5, 6, 7, 8}. Now C = A × B represents the possible numbers of the two dice. The\nsize of a number on the first die is just that number, so the generating function for\nis A(x) =\nx+x2+x3+x4+x5+x6\nA\nwhile the generating function for B is B(x) = x+x +x3+x4+x5+x6+x7+x8.\nNow the size of a pair of number (a, b) ∈C is the sum of the numbers of the two dice. So we want to\ndetermine cn which is the number of pairs (a, b). We have claimed above that C(x) = A(x)×B(x).\nWe now prove a generalization of the above relation between generating functions.\nTheorem 1. Let A and B be classes of objects and let A(x) and B(x) be their generating functions.\nThen the class C = A × B has generating function C(x) = A(x)B(x).\nProof. Let cn be the number of objects of size n in the Cartesian product C = A×B. These objects\nc = (a, b) are obtained by picking an object a ∈A of size k ≤n (ak choices) and an object b ∈B\nof size n -k (bn-k choices). Thus\nn\ncn =\n\nakbn-k.\nk=0\nNow let us consider product of generating functions\nA(x)B(x) =\n⎛\n⎝\nakxk\nk≥0\n⎞\n⎠×\n⎛\n⎝\nbkxk\nk≥0\n⎞\n⎠.\nIn order to get a monomial xn in this product, one must multiply a monomial akxk for k ≤n from\nthe first sum with a monomial bn-kxn-k from the second sum. Thus one has\nA(x)B(x) =\nn≥0\nn\n\nakbn-k\nk=0\n\nxn.\nThis completes the proof.\nWe denote by Ak = A × A × · · · × A the set of k-tuples of elements in A. Using Theorem 1 we\nsee that this class has generating function A(x)k = A(x) × A(x) × · · · × A(x) (where A(x) is the\nGenFun-3\n\ngenerating function of A). For instance, the generating function for the sum of numbers obtained\nby rolling 4 dice with 6 faces is\nC(x) = (x + x2 + x3 + x4 + x5 + x6)4.\nLastly we define\nSeq(A) = ∪\nk\nk\n0A ,\n≥\nas the set of finite sequences of elements in A. For instance if A = {0, 1} then\nA3 = {000, 001, 010, 011, 100, 101, 110, 111}\nand Seq(A) is the set of binary sequences. Because of Theorem 1 we see that the generating function\nof the class C = Seq(A) is\nC(x) =\n\nA(x)k\nk≥0\nwhere A(x) is the generating function of A. Observe also that\n\nA(x)k\n=\nsince\n)\n≥0\n-A(x\nk\n(1 -A(x)) ×\n\nA(x)k = (1 -A(x)) × (1 + A(x) + A(x)2 + A(x)3 + . . .) = 1.\nk≥0\nFor instance for the binary sequences, A = {0, 1} has generating function A(x) = 2x (A contains\n2 binary sequences of length 1 and nothing else) so the class of binary sequences C = Seq(A) has\ngenerating function\nC(x) =\n\nA(x)k\n=\nk≥0\n\n(2x)k =\n.\nk≥0\n-2x\nWe will know use these results to treat various problems.\nNumber of ways of giving change\nLet us look at the following simple question. Suppose we have 6 pennies, 1 nickel, and 2 dimes.\nFor what prices can we give exact change? and in how many different ways? Let gn be the number\nof ways we can give the exact changes for n cents (gn = 0 if we cannot make the change), and let\nG(x) =\nn\n0 gnxn be the generating function for this problem.\n≥\nWe claim that\nG(x) = (1 + x + x2 + x3 + x4 + x5 + x6) × (1 + x5) × (1 + x10 + x20)\nIndeed here a way of giving change is determined by a triple (a, b, c) where a is the number of\npennies, b is the number of nickels, c is the number of dimes. Moreover the \"size\" is the total\nnumber of cents it represents. So by Theorem 1 the generating function G(x) is A(x)B(x)C(x)\nwhere A(x) = (1 + x + x2 + x3 + x4 + x5 + x6) is the generating function of the change which can\nbe made in pennies, B(x) = (1 + x5) is the generating function of the change which can be made\nin nickels, and B(x) = (1 + x10 + x20) is the generating function of the change which can be made\nin dimes.\nGenFun-4\n\nWhat happens when we have an arbitrary number of dimes, nickels and pennies?\nWell in\nthis case the generating function for the change which can be made in pennies becomes A(x) =\n\nk\nxk =\n1 , generating function for the change in nickels becomes B(x) =\nx5k =\n,\n≥0\n1-x\nk≥0\n1-x5\nand the generating function for the change in dimes becomes C(x) =\nk≥0 x10k\ngenerating function for the number of ways of giving the change if one has infinitely\n\nnic\n\n=\n1-x10 . So the\nmany pennies,\nkels, and dimes is\n.\n(1 -x)(1 -x5)(1 -x10)\nDots and Dashes\nNow, let's think about another problem. Suppose that you are sending information using a sequence\nof two symbols, say dots and dashes, and suppose that sending a dash takes 2 units of times, while\nsending a dot take 1 unit of time. Here the size of a message will be defined as the number of units\ntimes it takes. So we ask the question: how many different messages can you send in n time units?\nLet's call this number fn. We'll figure out for the first few fn. We have\nn\nfn\nmessages\n.\n..\n...\n....\n.....\n.\n..\n...\n.\n. .\n. ..\n..\n.. .\n...\n.\n.\n.\nYou may already recognize the pattern: these are the Fibonacci numbers. But let's see what we\ncan learn about Fibonacci numbers by using generating functions. The recursion for the Fibonacci\nnumbers is\nfn = fn-1 + fn-2\nIt's not difficult to see why this works. The first symbol must either be a dot or a dash. If the\nfirst symbol is a dash, removing it leaves a sequence two units short, and if the first symbol is a\ndot, removing it leaves a sequence one unit shorter. Adding up these two possibilities gives us the\nabove recursion relation.\nNow, how does this connect to generating functions? Let us define\ninf\nF(x) =\n\nfjxj.\nj=0\nWhat is f0? It has to be 1, in order to have f2 = f1 + f0. This makes sense intuitively: there is one\nmessage, the empty message, using zero units of time. What does this recurrence say about F(x)?\nLet's look at the following equations\nF(x)\n=\n+\nx\n+\n2x2\n+\n3x3\n+\n5x4\n+\n8x5\n. . .\nxF(x)\n=\nx\n+\nx2\n+\n2x3\n+\n3x4\n+\n5x5\n. . .\nx2F(x)\n=\nx2\n+\nx3\n+\n2x4\n+\n3x5\n. . .\nGenFun-5\n\nWe can see that by multiplying by x and x2 we have shifted the terms, so that instead of fkxk we\nget fk\n1xk and f\nxk\nk\n. We thus get that the equation fk = f\n+\n-\n-\nk-1\nfk-2 nearly corresponds to the\nequation F(x) = xF(x) + x2F(x). This isn't quite right. All the terms xk for k > 0 do cancel, but\nthe constant term doesn't. To make the constant term correct, we need to add 1 to the right side,\nobtaining the correct equation\nF(x) = xF(x) + x2F(x) + 1.\nNow, this can be rewritten as\nF(x) = 1 -x -x2\nAt this point the perceptive reader will have observed that there was a much faster way to\nobtain the above result. Indeed one can observe that F(x) is the generating function of the set\nof sequences Seq(A) of the class A = {dot, dash}. Moreover the class A has generating function\nA(x) = x + x2 since it has one element of size 1 and one element of size 2. Therefore by the\ndiscussion in the previous section on gets F(x) =\n=\n.\n1 -A(x)\nx\nx2\nNow we want to use the expression of F(x) in order to obtain\n-\nsome\n-\ninformation on the coef-\nficients fn. When we have a polynomial in the denominator of a fraction like this, we can factor\nthe polynomial and express it as the sum of two simpler fractions. That is, we first factor the\ndenominator\n1 -x -x2 = (1 -φ+x)(1 -φ x)\n-\nwhere φ+ = 1+\n√\n5 and φ\n= 1\n√\n-\n5. (Note that φ\nand φ\nare not the roots of 1\nx\nx2, but the\n-\n+\n-\n-\n-\ninverses of the roots.)\nWe now use the method of partial fractions to rewrite this as\na\nb\nF(x) =\n+\n1 -φ+x\n1 -φ x\n-\nfor some a and b. Elementary algebra gives\n1 +\na = √\n\n√\n\n,\nb = -√\n\n√\n-\n.\n\nNow, we need to remember the Taylor series for 1/(1 -αx). This is\n= 1 + αx + α2x2 + α3x3 + . . .\n1 -αx\nEven if you don't remember Taylor expansions, you should recognize this as the formula for summing\na geometric series.\nGenFun-6\n\nWe thus have that, expanding each of the fractions in the expression for F(x) above in a Taylor\nseries,\n⎛\n\n1 +\n√\nF(x\n1 +\n. . .\n\nx +\n\n1 +\n√\n)\n=\n\nx2 +\n\n1 +\n√\na\n\nx3 +\n⎞\n⎝\n⎠\n+b\n⎛\n1 +\n\n√\n-\n\nx +\n\n√\n-\n\n⎝\nx2 +\n\n√\n-\nx3 + . . .\n⎞\n⎠\nSubstituting in the values we know for a and b, we get\nF(x\n√\n⎛\n1 +\n√\n)\n=\n\n1 +\n+\n\n√\n\nx +\n\n1 +\n√3\n⎝\nx2 +\n\n1 +\n√\nx3 + . . .\n⎞\n⎠\n⎛\n\n√\n-\n\n√\n-\n\n√\n1 -\n\n√\n-\n-√\n⎝\n+\nx +\nx +\n\nx3 + . . .\n⎞\n⎠\nNow, this gives us a nice expression for fn, the nth Fibonacci number. We equate the coefficients\nof xn on the left- and right-hand sides of this equation. Since the nth Fibonacci number fn is the\ncoefficient on xn in F(x), we get\n⎛\n1 +\n√\nn+1\n\n√\n-\nfn = √\n-\nn+1⎞\n⎝\n⎠.\nSince\n√\n-\n\n< 1, we can see that the second term goes to 0 as n gets large, and fn grows as\n\nC\n\n1 +\n√\nn\nfor some C. This means that we have found the asymptotic growth rate for the number of messages\nthat can be encoded by our dots and dashes, and that the number of bits sent per time unit is\n1 +\n√\nlog2\n.\nGeneralized Recurrence Equations\nWe've seen an example of a linear recurrence equation in the last section. I'm going to now give\na general method for solving linear recurrence equations (also called linear difference equations).\nIf you've taken 18.03, you'll notice that this method looks a lot like the method for solving linear\ndifferential equations.\nSuppose we have a recurrence equation\nfn = αfn-1 + βfn-2 + γfn-3\nGenFun-7\n\nI'm only writing this equation down with three terms, but the generalization to k terms is obvious,\nand works exactly like you'd expect. How do we solve this? What we do is to write down the\ngenerating function\ninf\nF(x) =\n\nfjxj.\nj=0\nThen, using the same reasoning as before, we get an equation for F(x) of the following form:\nF(x) = αxF(x) + βx2F(x) + γx3F(x) + p(x)\nwhere p(x) is a low-degree polynomial that makes this equation work for the first few elements of\nthe sequence, where the recurrence equation doesn't necessarily work (because we don't have an\nf\n1 term). For the Fibonacci number example above, we have p(x) = 1. Note that if we don't have\n-\na p(x) term, we get the solution F(x) = 0 which, while its coefficients (all 0's) satisfy the linear\nrecurrence equation, doesn't tell us anything useful. The maximum degree on p(x), if the recurrence\nequation has three terms, is quadratic (and if the recurrence equation has k terms, is k -1). You\ncan see this by noticing that for the x3 component and later, the recurrence is guaranteed to work.\nI'll let you check this fact.\nAs before, we next obtain\np(x)\nF(x) = 1 -αx -βx2 -γx3\nLet's suppose we can factor the denominator as follows:\n1 -αx -βx2 -γx3 = (1 -r1x)(1 -r2x)(1 -r3x).\nI'll leave the question of what happens if you have a double or triple root for a homework problem.\nWe then use the method of partial fractions (which you may remember from Calculus) to get\na\nb\nc\nF(x) =\n+\n+\n1 -r1x\n1 -r2x\n1 -r3x\nwhere a, b, and c are constants.\nWe can then see, by taking a series expansion for this generating function, that a generic term\nof our sequence will be\nf = arn + brn + crn\nn\n3 .\nHow did we get the roots r1, r2 and r3? They are the zeroes of the polynomial\ny3 -αy2 -βy -γ = 0.\nWe can see this by taking y = 1, so\nx\n1 -αx -βx2 -γx3\n=\nx3(y3 -αy2\n-βy -γ)\n=\nx (y -r1)(y -r2)(y -r3)\n=\n(1 -r1x)(1 -r2x)(1 -r3x).\nGenFun-8\n\nChord diagrams\nLet's count something harder now. Let's count how many ways there are of putting chords into a\nk-gon to divide it into triangles. We'll call this number Ck\n2. The sequence starts as follows:\n-\nk\nj = k -2\nCj\nas you can see from the following figure.\nX5\nX6\nX6\nX2\nX14\nX14\nX7\nX7\nHere, I've illustrated one of each essentially different way of dividing a k-gon into triangles, along\nwith the number of times it must be counted (because of symmetry) for k ≤7.\nHow can we find a recurrence for this number? Well, for a k-gon, let's look at the triangle\nthrough the edge (k, 1), one of the specific sides of the polygon. There must be a third point in\nthis triangle. Call it j. Clearly, we must have 2 ≤j ≤k -1. If we remove this triangle, we now\nhave two smaller polygons, a j-sided one, and a (k -j + 1)-sided one. We can now divide these\npolygons up into triangles independently. We thus get that the number of ways of triangulating a\nk-gon, given that we have a triangle with vertices 1, j, k, is Cj\nC\n-2\nk-j-1.\nOne thing we notice is that for this to be true for j = 2 or j = k -1, we have to set C0 = 1.\nThis takes care of the case where j is 2 or k -1, and one of the two smaller polygons is just an\nedge.\nNow, we can get a recurrence. Summing over all the j between 2 and k -1 gives\nk-1\nCk-2 =\n\nCj-2Ck-j-1\nj=2\nThis formula can use some rethinking of the limits. Let's let k′ = k -2 and j′ = j -2. We get\nk′-1\nCk′ =\nj\n\nCj′Ck′-j′-1\n′=0\nGenFun-9\n\nwhich is a nicer looking recurrence relation.\nThe next question is how we evaluate it using generating functions. Let's look at the generating\nfunction for counting these triangulations. That is,\ninf\nG(x) =\n\nC\nx3\nixi = 1 + x + 2x2 + 5\n+ 14x4 + 42x5 + . . .\ni=0\nWhat happens when we square G(x). We get\nG(x)2\n=\n1 + (1 + 1)x + (1 · 2 + 1 · 1 + 2 · 1)x2 + (1 · 5 + 1 · 2 + 2 · 1 + 5 · 1)x3 + . . .\ninf\nk\n=\n\nxk\nj\nk=0\n\nCjCk-\nj=0\nYou can see that the xk expression on the right is the right-hand-side of the recurrence relation we\nfound about for Ck+1 (with k′ = k + 1), so we get\ninf\nG(x)2 =\n\nCk+1xk.\nk=0\nMultiplying by x gives a sum with the xj coefficient equal to Cjxj. We now have an expression\nrelating xG(x2) and G(x). We need to make sure we get the smallest terms right. We can check\nthe constant term is the only one that is wrong, and we can fix that by adding 1 to the right hand\nside, to get the equation\nG(x) = 1 + xG(x)2.\nAt this point the perceptive reader will have observed that there was a faster way to obtain the\nabove equation. Indeed one can observe that a chord diagram is either empty (it has no chord),\nor has one chord dividing 2 chord diagrams. Thus the non empty chord diagrams are made of a\nCartesian product of a chord (generating function x), a left chord diagram (generating function\nG(x)), and a right chord diagram (generating function G(x)). Using Theorem 1 we get that the\nnon empty chord diagrams have generating function x × G(x) × G(x) (while empty chord diagram\nhave generating function 1). This gives\nG(x) = 1 + xG(x)2.\nas above.\nWe now solve this equation. This is a quadratic equation in G(x), so we can use the quadratic\nformula to solve for G(x), obtaining\n√1\n4x\nG(x) =\n±\n-\n.\n2x\nWe now have a choice. Which of the two roots of this equation should we use. We can figure this\nout by looking at the first term. We should have G(0) = 1. Depending on which root we choose,\nwhen we plug in 0 we either get G(0) = 2/0, or G(0) = 0/0. Clearly, the first option gives the\nwrong answer. Using l'Hopital's rule, we can figure out that in the second case, we indeed have\nG(0) = 1, so we get\n√\nG(x\n-\n) =\n-x.\n2x\nGenFun-10\n\nThis was the fun part; we have an algebraic expression for G(x). We now need to expand it in a\npower series to find the xk term. In this case, unfortunately, this happens to be somewhat tedious.\nWe will go through the steps carefully.\nThe first step is expanding (1 -y)1/2 in a power series. We use the binomial formula\n(1 -y)1/2 = 1 -\n/2\n\ny +\n1/2\n\ny2 -\n1/2\n\ny3 +\n1/2\n\ny4 -. . . .\nThis might look odd if you haven't seen it before, but one can define\nα\neven when α is not a\nk\npositive integer: the formula is\n\nα\nα(α\nα\n=\n-1) · · · (\n-k + 1).\nk\nk!\nSimplifying,\n(1 -y)1/2 = 1 -1y + 1(-1 y2\n)\n-1\ny\n(-)(-3 y\n)\n+ 1(-1)(-3)(-5)\n+\n2 2!\n2 3!\n2 4!\n· · ·\nNow, we need to substitute y = 4x, and plug the resulting expression into the formula we got for\nG(x). We obtain\n√\n(x) =\n-\n1 -4x\ninf\n(2\n7) · (2\nG\n=\n1 · 3 · 5 · . . . ·\nk -\nk\nk\n-5) · (2k -3) (4x)k\n2x\n2x\nk!\nk=1\nwhere we have the product of all odd numbers between 1 and 2k -3 in the numerator, and k! in\nthe denominator. All the -signs cancel out, as they should: since we're counting things, we have\nto get a positive integer.\nHow do we simplify this expression? Recall G(x) = Ckxk, so equating coefficients, we get\n1 · 3 · 5 · . . . · (2k -5) · (2k -3) (2\nCk =\n·\nk -1)\n(4)k+1\n2 ·\n2k+1\n· (k + 1)!\nwhere we have had to replace k by k + 1 in the above formula to make up for the 1 in front of it.\nx\nWe can cancel out the powers of 2 in the numerator and denominator to get\nCk =\n· 3 · 5 · . . . · (2k -5) · (2k -3) · (2k -1)2k.\n(k + 1)!\nNow, let's multiply the top and bottom of the above expression by k!, and write k!2k = 2·4·6 · · · (2k).\nWe get\nCk =\n· 2 · 3 · · · (2k -1) · (2k).\n(k + 1)(k!)2\nThus, we have\n(2k)!\n2k\nCk =\n=\n,\nk + 1 (k!)2\nk + 1\n\nk\n\nwhich is the definition of the k'th Catalan number.\nThe Catalan numbers turn up in quite a few places (as we've already seen). Prof. Richard Stan-\nley has a section on his webpage (which is also in his book) giving 66 combinatorial interpretations\nof the Catalan numbers.\nExercise. Give a bijective proof that the number of chord diagrams is given by the Catalan\nnumbers.\nGenFun-11\n\nDiagonals in Pascal's triangle\nIn this section we use generating function of more than one variable in order to solve a neat problem.\nRecall that Pascal's triangle is formed by binomial coefficients:\nWe will compute the sums of the diagonal elements in Pascal's triangle. For example, the sum of\nthe boldface elements above is\n34 = 1 + 10 + 15 + 7 + 1 =\n+\n+\n\n+\n\n+\n\n.\nHow do we do this with generating functions? Let's use a generating function of two variables.\nConsider the two variable function\ninf\ninf\na + b\ng(x, y) =\n\na=0\n\nb=0\n\na\n\nxayb.\nNote that the coefficient of xayb of g is precisely the value in row a, column b of Pascal's triangle\n(both indexed starting from 0). We already saw the sum of the terms in the n'th row of Pascal's\ntriangle\nn\n(x + y)n =\n\na=0\nn\na\n\nxayn-a\nNow, let's sum over all rows.\ninf\ninf\na + b\nG(x, y) =\ny\na\na=0 =0\n\ninf\nxa b =\nb\nn\n\n(x + y)n\n= 1\n=0\n-x -y\nwhere the last equality comes from the sum of a geometric series.\nWhat are we looking for? We're looking for the sum\nm\n\nj=0\nm + j\n2j\n\nfor all m. The generating function for that would be\ninf\nm\nH(z) :=\nm\n\n=0\n\nj=0\nm + j\n2j\n\nzm.\nGenFun-12\n\nWe would like to relate H to G somehow (since we have an expression for G). So we'd like to\nsomehow turn\na+b\ninto\nm+j . Can we do this? Looking at this more closely, it involves setting\na\n2j\na = 2j and b\n\n= m\n\n-j.\n\nThis\n\nwould involve turning xayb into x2jym-j.\nSo we'd like to get\nx2jym-j = zm. Note we can do this if we let x = z1/2 and y = z. Dealing with square roots is not\nso nice, so lets square everything and let z = x2, y = x2. Our hope will be that H(x2) is related to\nG(x, x2).\nSo consider G(x, x2):\ninf\ninf\nG(x, x2) =\n\na=0\n\nb=0\na + b\na\n\nxa+2b.\nNow let's compute the coefficient of x2m in G(x, x2); in other words, the sum of all terms where\na + 2b = 2m, or a = 2(m -b). We obtain\nm\ncoeff. of x2m in G(x, x2\nb\n) =\n\n) +\nb=0\n2(m -b\n2(m -b)\n\nm\n=\nm + j\nwith the substution j = m\nj\n-b.\n2j\n=0\nSuccess! This is exactly hm (or in other words, the coefficient of x2m in H(x2)). Note that we don't\nhave H(x2) = G(x, x2); rather, we have\nG(x, x2) = H(x2) + Q(x),\nwhere Q(x) consists of only odd powers of x.\nAt any rate, we can now determine hn, since we know\nG(x, x2\ninf\n) =\n=\n\nfrxr,\n1 -x -x2\nr=0\nwhere fm is the m'th Fibonacci number. So hm = f2m.\nIt turns out that once you know that you know the answer, it's easy to prove it by induction.\nGenerating functions have the advantage that we didn't have to guess the answer first for the\ntechnique to work.\nExponential generating functions\nThe generating functions we have seen so far are technically known as ordinary generating functions.\nThere are other kinds of generating functions.\nA particularly important kind are exponential\ngenerating functions.\nDefinition 2. If a0, a1, a2, . . . is an infinite sequence of integers, the exponential generating function\n(EGF) of (an)n∈N is the function\ninf\nˆA(x) =\nan xn.\nn!\nn=0\nGenFun-13\n\n(The notation here of putting a hat on exponential generating functions is not standard, but\nwe will just use it as a reminder that we're not working with the normal generating function.)\nExample. The EGF of the sequence 1, 1, 1, . . . is\ninf\nn\nxn\n= ex.\nn!\n=0\nThe choice of whether to use ordinary or exponential generating functions depends on the\nsituation. Here, we'll see a setting where exponential generating functions are particularly nice.\nLet pn denote the number of permutations of a set of size n. We will define p0 = 1 (this just\nturns out to be the most convenient definition for us). You should recall that pn = n!. So we can\ncompute the EGF:\ninfn!\ninf\nˆP(x) =\nxn =\nxn\n=\n.\nn!\nx\nn\n\n=0\nn\n\n=0\n-\nPretty simple!\nA cycle is a special kind of permutation.\nLet π be a permutation of the set {1, 2, . . . , n}.\nConsider the sequence\nr1 = 1,\nr2 = π(r1),\nr3 = π(r2),\n. . . rn = π(rn-1).\nThen π is a cycle if and only if (r1, r2, . . . , rn) is a reordering of (1, 2, . . . , n).\nHow many cycles on a set of size n are there? It's not too hard to see that this number (call it\ncn) is just (n -1)!: we have n -1 choices for r2 = π(1) (we can't pick 1), then n -2 choices for\nπ(r2) (we can't pick 1 or r2), and so on. We also define c0 = 0; again this is a matter of convenience\nfor us.\nSo the generating function for the number of cycles is\ninf(n\n1)!\ninfxn\nˆC(x) =\n\n-\nxn =\nn!\n\n= log\n.\nn\nn\n\nx\n=1\nn=1\n-\n\nHow did we get that last step? One way is to integrate both sides of the identity\ninf\n=\nxn.\n1 -x\nn\n\n=0\nˆ\nˆ\nˆ\nˆ\nNow consider P(x) and C(x); you might notice that P(x) = eC(x). Is this a coincidence? In\nfact, no! It is a very special case of a much more general (and very useful) fact. The key reason for\nthe relation is that any permutation can be descibed in terms of a disjoint collection of cycles:\nExample. Consider the set S = {1, 2, 3, 4, 5, 6}, and the permutation π on S defined by\nπ(1) = 2, π(2) = 5, π(3) = 6, π(4) = 4, π(5) = 1, π(6) = 3.\nThis can be described by the cycle 1 →2 →5 →1 · · · on the set {1, 2, 5}, the cycle 3 →6 →3 · · ·\non the set {3, 6}, and the cycle 4 →4 →4 · · · on the set {4}.\nGenFun-14\n\nRather than dealing with the abstraction of the general setting, we will prove the connection\nˆ\nˆ\nbetween P(x) and C(x) in a way that is clearly quite general, and then see some other examples\nof the same connection.\nˆ\nˆ\nTheorem 2. Let P be the EGF for the number of permutations (pn)n N, and C be the EGF for\n∈\nˆ\nˆ\nthe number of cycles (c )\n. Then P(x) = eC(x)\nn n∈N\n.\nProof. As noted already, any permutation of a set S of size n can be split up into a collection of\ndisjoint cycles. So in order to enumerate all the permutations of S, we can consider all possible\npartitions of S into nonempty pieces, and then choose any cycle we like on each piece of the partition.\nTo understand the number of ways of doing this, let's first count the number of permutations with\nexactly k cycles.\nLemma 1. The number of permutations of a set of size n with exactly k cycles has generating\nfunction\nk!\nProof. We'll first do the case k = 2, which is easier\n\nˆC(x)\nk\n.\nto understand.\nWe'll begin by counting a slightly different thing. Let's count the number of ways of partitioning\na set S of size n into 2 cycles, one colored red and the other blue (so we'll count as different two\npermutations which are exactly the same, but whose cycles are colored differently). Let qn be this\nˆ\nnumber, and Q(x) be the EGF for (qn)n N. What is qn? Well, we must decide which elements of\n∈\nS will be in the red cycle (the rest will be in the blue), and then we get to pick any cycle on the\nred part and any cycle on the blue part. Hence\nqn =\nT\n\nc\nc\n|T| ·\nn-|T .|\n⊆S\n(You might ask why we are allowing T = ∅or T = S, since we really want exactly 2 cycles. But\nrecall that c0 = 0, and so it makes no difference if we include these terms in the sum or not.) We\ncan rewrite this by splitting the sum up by the size of T: there are\nn\nchoices of T with\n,\nt\n|T| = t\nand so\nn\n\nqn =\n\nt\n=0\nn\nt\n\nc cn-t.\nt\nˆ\nˆ\nSo now let's write down Q(x): Now consider Q(x):\ninf\nˆQ(x) =\nn\nqn xn\nn!\n=0\ninf\n=\nn\n!\n-\nn=0\nn\nn!\nc\nt\nn\nt!(n\nt\n-\ntc\nt)!\n=0\n\nxn\ninf\ninf\n=\n\nt=0\nc\nn\ntxt\ncn\ntx -t\n-\nt!\nt!\nn=t\n·\ninf\n=\nc\nt\ntx\ninf\ns =\nt!\n·\ncsxs\nsubstituting\nn\nt\ns!\nt=0\ns=0\n-\nˆ\n= C(x) · ˆC(x).\nGenFun-15\n\nNow to finish the lemma for k = 2, we note that we've overcounted every permutation with 2\ncycles twice, since there are 2 ways of coloring the cycles. So the EGF for what we're interested in\nˆ\nis 1 C(x)2, as required.\n2!\nNow let's do the general case. Again, suppose we have k colors; let's count the number of ways\nof partitioning a set S of size n into k cycles, one of each color (so we'll count as different two\npermutations which are exactly the same, but whose cycles are colored differently). Let hn be this\nˆ\nnumber, and H(x) be the EGF for (hn)n N. We wish to show that hn/n! is the coefficient of xn in\n∈\nˆC(x)k. So let's consider this coefficient of\nˆC(x) · ˆ\nˆ\nC(x) · · · · C(x).\nIt is a sum of various contributions, where each contribution consists of choosing nonnegative\nintegers n1, n2, . . . , nk that sum to n, and multiplying the xn1 term from the first term in this\nproduct with the xn2 term of the second, the xn3 term of the third, etc. So we obtain\ncoeff. of xn\nc\nin\n(x) =\n\ncn\nn\ncn\nH\nk\nn1! · n2! · · · nk!\nn1,...,nk, ni=n\n=\n\nn\n\ncn1cn2 · · · cn\nn!\nn1\nn2\nk,\nn\n· · ·\nn\nk.\nk\nn1,...,n\nni=\nHere,\n\nn\n\n:=\nn!\nis the multinomial coefficient, and it counts the number of ways of\nn1 n2 ··· nk\nn1!n2!···nk!\npartitioning a set of size n into a piece of size n1, a piece of size n2, etc. (If you haven't seen this\nbefore, verify this! It's a generalization of the usual binomial coefficients.) So this sum is precisely\ndescribing all the ways of placing k disjoint colored cycles, and hence is exactly hn, as required.\nAgain, to finish the lemma, we note that we've overcounted every permutation with k cycles\nprecisely k! times, since there are k! ways of coloring the cycles.\nSo the EGF for what we're\ninterested in is 1 ˆC(x)k, as required.\nk!\nThe rest of the proof is easy now. We just need to consider all possible partitions with any\nnumber of parts between 1 and n, so we should sum up the EGF's for each:\nn\nˆ\n\nˆ\nˆ\nP(x) =\n(C(x))k = eC(x);\nk!\nk=1\nˆ\nthe last step follows by considering the Taylor series of eC(x).\nThe thing to notice about this proof is that we never used anything specific about the values\ncn--all that we used was that any permutation can be uniquely described by a partition into cycles.\nThe theorem in fact holds much more generally. Let's see another example.\nA derangement on a set S is a permutation π of S such that π(i) = i for all i ∈S; no element\nis kept fixed by the permutation. Let dn denote the number of derangements on a set of size n,\ndefining d0 = 1.\nIt should be clear that a derangement can be partitioned into cycles of length at least 2. What\nis the generating function for cycles of length at least 2? Let c′\nn be the number of cycles of length\nGenFun-16\n\nat least 2 on a set of size n. Then clearly c′\n0 = c′\n1 = 0, and c′\nn = cn for n ≥2. Since c0 = 0 but\nˆ\nc1 = 1, we deduce that the EGF for (cn\n′ ) is C′(x) = C(x) -x = -log(1 -x) -x.\nBut now we can deduce that the EGF for (dn)n∈N) is simply\ne-x\nˆD(x) = exp(C′(x)) =\n.\n1 -x\nFrom this, a formula for dn can be determined; we leave this as an exercise.\nThis relation is certainly not restricted to permutations. For example, any graph on n nodes\nˆ\ncan be described as a collection of connected graphs. If G(x) is the generating function for the\nˆ\nˆ\nˆ\nnumber of graphs, and H(x) the generating function for connected graphs, then G(x) = eH(x).\nGenFun-17\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Principles of Discrete Applied Mathematics, How to sort? Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/4fb2bd51c26ba780d5376fe65e5d1fdb_MIT18_310F13_Ch10.pdf",
      "content": "18.310 lecture notes\nSeptember 2, 2013\nHow to sort?\nLecturer: Michel Goemans\nThe task of sorting\n1.1\nSetup\nSuppose we have n objects that we need to sort according to some ordering. These could be integers\nor real numbers we want to sort numerically, words we want to sort alphabetically, or diamonds\nwe want to sort according to their size. The basic operation we can perform is to take two of these\nobjects and compare them; we will then know which one is bigger and which is smaller. In fact,\nwe typically do not compare the objects themselves, but rather an attribute or key of them. This\ncould be their weight, value, size, etc. Comparing two keys does not necessarily mean comparing\nthem numerically, but we have a way to tell which is bigger. For example, if we want to sort objects\naccording to their weight, we only need to have at our disposal one of these old-fashioned scales,\nand not a modern one that displays the weight of an object. In this discussion, we will stick to\nbinary comparisons, that is, comparisons of one key with another. With an old-fashioned scale, for\nexample, one could compare several objects at once, but we will not allow this. We also assume\nthat no two keys are equal. In other words, when comparing two keys, we assume that only 2\noutcomes (rather than 3) are possible since one of them will be strictly smaller than the other.\nOur n keys are given to us in an arbitrary order. We want to rearrange them in increasing\norder, according to our sense of order. If we are given the following input of numerical values as\nkeys\n6 2 8 12 5 1 10\nthen the output would be\n1 2 5 6 8 10 12.\n1.2\nAlgorithms and their performance\nAn algorithm is a method for computing a task, which always terminate with a correct output,\nirrespective of what input it is given. For instance a sorting algorithm should always work: it\nshould always output a sorted list, whatever initial list it is given as an input. Beside that, we\nSorting-1\n\nwould like our algorithm to perform our sorting task with the least amount of \"effort\" (measured\nin terms of number operations it requires). But how will we evaluate the performance of a sorting\nalgorithm? Well, clearly it will take more operations to sort 1, 000, 000 keys rather than just a\nmere 1, 000 keys. So we certainly want to evaluate its behavior as a function of n, the number of\nkeys to sort. The number of operations will also depend on the input itself; n already sorted keys\nseem much easier to sort than n keys given in an arbitrary order. We will usually, but not always,\nadopt a worst-case perspective. This means we judge a sorting algorithm based on the number of\noperations it performs on the worst possible input of n keys. The worst input, of course, depends\non the algorithm itself. One can view this worst-case analysis as a game with a malicious adversary.\nOnce he (or she) knows the precise algorithm we will be using, (s)he will feed an input that makes\nour task as hard as possible. In practice, this worst-case type of analysis might be very conservative\nas the behavior on most inputs might be far from the worst-case behavior, but sitting right now on\na flight over the Atlantic, I very much hope that a worst-case analysis of all the algorithms used in\nthe control of the plane was performed...\n1.3\nLower bound on the number of comparisons required to sort a list\nThe first question we ask is, given n keys, can we find useful upper and lower bounds on the number\nof (binary) comparisons we need to perform in order to sort them completely? We can find a lower\nbound by using the pigeonhole principle: the number of possible outcomes should be at least as\nlarge as the number of possible answers to our sorting problem. If we never perform more than\nk comparisons then the number of possible outcomes of the algorithm is at most 2k (remember,\nwe are not allowing equal keys). On the other hand, given n keys, any one of them can be the\nsmallest, anyone of the remaining n -1 keys can come next, and so on, resulting in n! possible\nanswers. Therefore, the pigeonhole principle tells us that we have n! ≤2k; otherwise there would\nbe two different orderings of the input that give the same list of answers to the comparisons (hence\nthe algorithm would fail to return a correctly sorted list in one of the two cases). This means that,\nfor any correct sorting algorithm, we have k ≥log2(n!) where log2 denotes the logarithm taken in\nbase 2. Since k must be an integer, we must therefore have that\nk ≥⌈log2(n!)⌉\nwhere ⌈x⌉denotes the smallest integer greater or equal to x.\nTo get an estimate of log2(n!), we can use the easy fact that\nn\nn/\n\n≤n! ≤nn,\nor even better the more precise Stirling's formula which gives\nn!\n√\n∼\n2πn\nn\nn\n.\ne\nThus, we see that for large values of n, log2(n!) behaves\n\nlike n log2(n) (the other terms grow more\nslowly than n log2 n).\nSorting-2\n\nThe table below shows the lower bound k ≥⌈log2(n!)⌉for a few values of n:\nn\nk\n≥1\n≥3\n≥5\n≥7\n≥10\n≥13\n...\n...\n≥525\n...\n...\n≥8530\n...\n...\nExercise.\nShow that for n = 5, it is possible to sort using 7 comparisons (tricky).\nSome sorting algorithms\nThere are many different approaches for sorting a list of numbers. We can divide them into several\ngeneral approaches.\n1. Sorting by inserting keys one at a time in a growing sorted list of keys.\nThe\nprinciple is to maintain a sorted list of keys. Initially we start with just one key. Once we\nhave a sorted list of k keys, we can insert a new key into it in order to obtain a sorted list of\nk + 1 keys, and repeat the process. This leads to Insertion Sort and we will discuss later\nhow to do the insertion.\n2. Sorting by merging. The idea here is that if we have two sorted lists, it is rather easy to\ntake their union and create a merged sorted list. This means that one can obtain larger and\nlarger sorted lists. This leads to Merge Sort, and we will describe it in more details as well.\n3. Sorting by pulling offthe largest (or smallest) element. If we have an efficient way to\nextract the largest element then we can repeat the process and obtain a sorted list of elements.\nThis leads to several sorting algorithms; we will describe Heap Sort, a rather beautiful and\nefficient way for sorting.\n4. Sorting by inserting keys one at a time in their correct final position. The idea\nhere is that we take the first key x of the list to be sorted and determine what will be its\nposition in the sorted list. Then we place x at its correct position, we place all keys smaller\nthan x before x and all keys larger than x after x. Now we can simply reiterate on the two\nsublists that come before and after x. This leads to QuickSort.\nSorting-3\n\n2.1\nInsertion Sort\nTo describe Insertion Sort, we only need to say how we will go about inserting a new key, call\nit x, into a sorted list of size k.\n11 4\nsorted\nunsorted\n11 4\nsorted\nunsorted\nFigure 1: Inserting a new key in the Insertion Sort algorithm.\nLet the sorted keys be denoted as key1, key2, · · · , keyk. One possibility would be to compare x\nwith each key, from smallest to largest, and insert it just before the first one that is larger than x\n(or after the last one, if no such key exists). This would not be very efficient since we would make\nn(n\n1)\nk comparisons in the worst-case for one insertion, and thus about 1 + 2 + · · · + (n -1) =\n-\ncomparisons over all insertions in the worst-case.\nAnother possibility is to compare x to a middle key, say the one in position m = ⌊(k + 1)/2⌋.\nIf x is smaller we want to move all the keys from this middle and beyond over by 1 to make room\nfor x, and insert x into the list key1, key2, · · · , keym-1.\nIf x is bigger, we don't move anything but now insert x into the right hand half of the list\nkeym+1, ., keyk. This can be shown to require a minimum number of comparisons, but it requires\nlots of key movement, which can be bad if key movement has a cost as it happens when implementing\nsuch a sorting algorithm in a computer. It is a fine way when sorting a hand of cards (because\nthere is no cost in \"key movements\" in that situation).\n2.2\nMerge Sort\nThe Merge Sort approach involves dividing the n keys into n/2 pairs, then sorting each pair, then\ncombining the sorted pairs into sorted groups of size 4, then combining these into sorted groups of\nsize 8, etc. For example, consider the following 8 keys to sort:\n6 2 8 12 5 1 10 11.\nWe can represent the groupings and the mergings as follows:\nAt each stage, you take two sets of keys of the same size, each of which is already sorted, and\ncombine them together. Notice that the only candidates for the front element of the merged lists\nare the front elements of both lists. Thus, with one comparison, we can find out the front of the\nmerged list and delete this element from the corresponding list. We again have two sorted lists\nleft, and can again pull offthe smallest of the two fronts. If we keep on doing this until one list\nis completely depleted, we will have sorted the union of the two sorted lists into one. And if both\nlists have size k, we will have done at most 2k -1 comparisons. Over each level, we do less than\nn comparisons, and since we have roughly log2(n) levels, we get that we perform roughly n log2(n)\ncomparisons.\nThe trouble with this approach is that it requires extra space to put the keys that are pulled\nofffrom the front. Since you do not know which list will be depleted when you merge them, you\ncan't use the partially depleted list space very efficiently.\nSorting-4\n\n[1 2 5 6 8 10 11 12]\n[2 6 8 12]\n[2 6]\n[6]\n[2]\n[8 12]\n[8]\n[12]\n[1 5 10 11]\n[1 5]\n[5]\n[1]\n[10 11]\n[10]\n[11]\nFigure 2: Groupings and mergings in the Merge Sort algorithm.\n2.3\nHeap Sort\nThis is a pull offat the top method that has the virtue that it can sort everything in place,\nand performs essentially (up to a small factor) the optimum number of comparisons.\nTo describe it, we first need to define a heap. A heap is a tree whose nodes contains the keys to\nbe sorted and which satisfies the conditions that\n1. every node has 0, 1 or 2 children,\n2. every level from the root down is completely full, except the last level which is filled from the\nleft,\n3. the key stored in any node is greater than the keys of (both of) its children (the heap property).\nFirst, let's focus on conditions 1. and 2. A heap on 9 elements must therefore have the following\nstructure:\nThe first three levels are filled (by 1, 2 and 4 nodes respectively) while the 2 nodes in the last level\nare as much to the left as possible. A heap with k full levels will have 1+2+4+· · ·+2k-1 = 2k -1\nnodes.\nBelow are two (out of many more) valid ways of putting the keys 1 up to 9 into a heap structure\nwith 9 nodes.\nWe should emphasize that the left and right children of a node serve the same\npurpose; both are supposed to have a key smaller than their parent. There is an easy way to store\nSorting-5\n\nFigure 3: Two heap structures with 9 nodes\nthe keys of a heap without drawing the tree. Indeed, if we label the nodes of the heap from top to\nbottom, and from left to right in each level (see below), then notice that the children of the node\nlabelled x will be labelled 2x and 2x + 1 (if they exist).\nFigure 4: Labels of the nodes of a heap.\nThis property allows us to store a heap conveniently as an array of the same length as the\nnumber of keys. For example, Figure 5 shows a heap and its array representation. And knowing\nthe array, one can easily rebuild the heap.\nFigure 5: A heap and its array representation.\nFor now, let us postpone the discussion of how we initially arrange the given keys so that they\nSorting-6\n\nsatisfy the heap property. Once we have a heap, the root node contains a key which is larger than\nits children, which is larger than its own children, etc, so this means that the root contains the\nlargest key. So this key (since it is the largest) should be placed at the end of the array. To do\nthis, we can simply exchange the root key with the last key in the heap, and make this new last\nkey inactive.\nThe problem is that the remaining active keys do not quite form a heap; indeed the heap\nproperty may be violated at the root. Let us call this a headless heap. Consider for example the\nfollowing heap to the left, and suppose we exchange the root element and the last element to get\nthe headless heap to the right.\nThe heap property is indeed violated at the root. Key 4 is smaller than its children. This can be\nfixed rather easily. We can exchange key 4 with the key of its largest child, in this case, this is its\nleft child with key 10 (but it could have been the right child as well). This gives:\nBy replacing the key 10 by the key 4, unfortunately, we are now violating the heap property at\nthe node containing this key 4. In other words, the structure rooted at the node with key 4 is not a\nquite a heap but a headless heap. But we know how to fix this. In general, to fix a headless heap,\nwe compare its root with its two children (by making 2 comparisons). If the root is larger than\nboth its children, there is nothing to do and we are done. Otherwise, we exchange the root with\nthe largest child, and proceed to fix the headless heap now rooted at what used to be this largest\nchild. The headless heap problem has now trickled down one level. In our example, exchanging the\nkey 4 with 8, we get the following heap, and we have recovered a (real) heap. In general, though,\nthe headless problem may not end until we have reached a leaf.\nSorting-7\n\nThe array that corresponds to this heap, and the inactive 9th element is\nIn general, the height of a heap is roughly equal to log2(n), and fixing a headless heap therefore\ntakes at most 2 log2(n) comparisons (2 comparisons at each node on the path from the root to a\nleaf).\nNow that we have another heap (with one fewer key), we can proceed and extract the maximum\nof the remaining active keys, which must sit at the root of the heap. Again, we can switch it with\nthe last active element to get a headless heap:\nAnd fix this headless heap problem by trickling it down the tree until we get a real heap:\nThe array that corresponds to this structure is now:\nSorting-8\n\nand we have made progress since the last 2 elements are correctly positioned.\nEvery time we extract an element, we need to perform at most 2 log2(n) comparisons, and\ntherefore we can extract all elements by doing at most 2n log2(n) elements.\nBut we have not discussed yet how to create the initial heap. There are two obvious ways to try\nit. One is a top-down approach. In this approach, we build up the heap one key at a time, starting\nby trying to place that key at the root. Another is a bottom-up approach. In this approach, we\nstart with a binary tree with the keys placed arbitrarily at the nodes, and fix the levels one at a\ntime, starting with the nodes closest to the leaves. The bottom-up approach takes roughly c1n\ntime, and the top-down approach takes roughly c2n log n time, for some constants c1 and c2.\nHere is a sketch of the bottom-up approach: If we look at the leaves alone, they are heaps since\nthere are no descendants to be bigger than them. If we go up one level then we may have headless\nheaps with two levels. And we know how to handle headless heaps! So we make them into heaps by\ncuring their heads as done above. Now we can go up to the third level from the bottom: including\nthese keys we now have headless heaps again and we know how to cure them! Each time we extend\nour heap property one level up the tree.\nHow do we cure a headless heap on the kth level of the tree? We've already shown how to do\nthis, but let's go over it again. If we do two comparisons, we've fixed the keys on the kth level, but\nwe might have created one headless heap on the next level closer to the bottom. We then might\nhave to apply two compares to this one, and so on. You can figure out how many steps building\nthe original heap takes in the worst case from this description. It's only cn comparisons, though.\nExercise.\nShow that the number of comparisons necessary to create a heap in the first place is\na constant times n.\nSorting-9\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Improving Information Order and Connectivity",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/17a9ee994d31de7fc7ba1eb2b8a0c4ec_MIT18_310F13_Worksheet2.pdf",
      "content": "18.310 IMPROVING INFORMATION ORDER AND CONNECTIVITY\n1. Revise the sentence to better ft each context.\nFamiliar\nImportant new\ninformation\ninformation\nEnumeration of binary trees was achieved by defning a bijection between binary trees and Dyck paths.\nImportant new\nFamiliar\ninformation\ninformation\nEnumeration of binary trees was achieved by using a bijection between binary trees and Dyck paths.\n2. Revise to improve information order and connectivity. This text is from a section about\ncounting. Catalan numbers are a familiar concept. The new concept here is binary trees.\nA binary tree is a plane tree in which vertices have either 0 or 2 children. Binary trees can be counted by the\nCatalan sequence.\n3. Read the theorem and proof below. Starting with the second sentence of the proof, identify\nthe familiar information and the important new information in each sentence. Then revise to\nimprove the information order and connectivity, without revising the theorem statement.\nTheorem. For each n, the set Tn of plane trees with n edges is the same size as the set Dn of Dyke paths with\n2n steps:\n|Tn| = |Dn| for all n.\nProof. We defne a bijection Φ between plane trees and Dyck paths as follows: given any tree T ∈Tn, perform\na depth-frst search of the tree T (as illustrated in Figure 0.1) and defne Φ(T ) as the sequence of up and down\nsteps performed during the search. A Dyke path D ∈Dn is obtained from T because Φ(T ) has n up steps and n\ndown steps (one step in each direction for each edge of T ), starts and ends at level 0, and remains non-negative.\nBecause Φ is a bijection between Tn and Dn, we conclude |Tn| = |Dn|.\nD\nΦ(T)\nT\nFIGURE 0.1. A plane tree T and the associated Dyke path Φ(T ). The depth-frst search of\nthe tree is represented graphically by a tour around the tree (drawn in orange): frst visit the\nleftmost subtree entirely, then the next subtree, etc.\nFor more information about connectivity, see \"The Science of Scientifc Writing,\" by Gopen & Swan. Published in American Scientist 78(6)\n550-558, Nov-Dec 1990 and available online.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Sample Proofs 1 and 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/77bd67abf4635223b59008fb686ada70_MIT18_310F13sample3.pdf",
      "content": "Given two sets, A and B, the Cartesian product A × B is defined as the\nset of pairs (a, b) with a ∈A and b ∈B. The size of a pair (a, b) is defined\nto be the size of a plus the size of b.\nTheorem 1. Let A and B be classes of objects and let A(x) and B(x) be\ntheir generating functions. Then the class C = A × B has generating func\ntion C(x) = A(x)B(x).\n\nProof. Let A(x) =\nj , B(x) =\nbkxk, and C(x) =\nn .\nj≥0 aj x\nk≥0\nn≥0 cnx\nTo show that C(x) and A(x)B(x) are equal, we show that the coefficients\nof their series expansions are equal.\nAn ordered pair (a, b) of size n in C = A × B can be obtained by choosing\nan object a ∈A of size i ≤ n (ai choices) and an object b ∈B of size n - i\n(bn-i choices). So\nn\n\ncn =\naibn-i\n(1)\ni=0\nis the total number of ways to obtain such an ordered pair.\nBut the coefficients of the series expansion of A(x)B(x) turn out to be\nthe cn from (1). Consider\n⎛\n⎞\n⎛\n⎞\n\nj\nk\nA(x)B(x) = ⎝\n⎠ × ⎝\nbkx ⎠\naj x\n.\nj≥0\nk≥0\nn\nIn order to get the coefficient for x\nin this product, we must multiply each\nmonomial aixi for i ≤ n from the first sum with the corresponding monomial\nbn-ixn-i from the second sum. Thus we have\n\nn\n\nn\nA(x)B(x) =\naibn-i\nx = C(x).\nn≥0\ni=0\nWe have seen that A(x)B(x) = C(x) because the coefficients of their\nseries expansions are equal.\nD\n\nTheorem 2. Let A and B be classes of objects and let A(x) and B(x) be\ntheir generating functions. Then the class C = A × B has generating func\ntion C(x) = A(x)B(x).\nProof. To show that C(x) and A(x)B(x) are equal, we show that the coef\nficients of their series expansions are equal.\nn\nFirst let C(x) =\nn≥0 cnx . Because C = A × B, the coefficient cn is the\ntotal number of ways to obtain an ordered pair (a, b) ∈ C of size n. Each\npair is obtained by choosing an object a ∈A of size i ≤ n (ai choices) and\nan object b ∈B of size n - i (bn-i choices). Thus the total ways to obtain\nsuch a pair is\nn\ncn =\naibn-i.\n(2)\ni=0\nBut these cn turn out to also be the coefficients of the series expansion of\nA(x)B(x). If we let A(x) =\nj and B(x) =\nbkxk, then\nj≥0 aj x\nk≥0\n⎛\n⎞\n⎛\n⎞\nA(x)B(x) = ⎝\naj xj ⎠ × ⎝\nbkx k⎠ .\nj≥0\nk≥0\nn\nIn order to get the coefficient for x\nin this product, we must multiply each\nmonomial aixi for i ≤ n from the first sum with the corresponding monomial\nbn-ixn-i from the second sum. Thus we have\nn\nn\nA(x)B(x) =\naibn-i\nx .\nn≥0\ni=0\nBecause the coefficients of this series expansion are just the cn from (2),\nn\nwe have that A(x)B(x) =\nn≥0 cnx = C(x).\nD\nP\nX\nX\nX\nP\nX\nX\n\n!\nP\nP\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310 Sketch of the Logic",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/31e060e87aebc3b2bb66cb3020a6c020_MIT18_310F13_proofsketch.pdf",
      "content": "This is a sketch of the proof of the following fact:\nIf class C is the Cartesian product of classes A and B, with\ngenerating functions C(x), A(x), and B(x), respectively, then C(x) =\nA(x)B(x).\nSketching the logic of a proof can help you to understand the\nproof and may also help you decide how to write the proof.\nThis sketch also illustrates why writing mathematics is often\nchallenging: logic that requires two (or more) dimensions to display\nmust be condensed to one dimension to fit within a single\ncontinuous line of text.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "18.310C Pre-recitation Assignment 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/03af54d57c96712e9a5597b8700b56f7_MIT18_310F13_prerec1.pdf",
      "content": "Fall 2013\n18.310C Pre-recitation Assignment 1\nDue: Thu, 9/3/2013\nReading: Sections 1, 2 and 5 of the lecture notes on Probability. You will need these readings\nto complete the assignment.\nInstructions: Hand in written solutions to the following questions at the begininning of\nrecitation.\n1. A box contains 7 black balls and 4 white balls. Two balls are randomly drawn from the box.\nLet X1 be the random variable denoting the color of the first ball and X2 denote the color\nof the second ball. Define an appropriate sample space for this random process. Using your\ndefinition, compute the probabilities of the following events by describing the subsets of the\nsample space to which they correspond:\n- (X1, X2) = (white, black).\n- {X1, X2} = {white, black}.\nA note on notation: The notation (a1, . . . , an) indicates an ordered n-tuple, while the\nnotation {a1, . . . , an} indicates a set of n elements. In particular, notice that (a, b) = (b, a)\nand {a, b} = {b, a}.\n2. Consider now the same setting. Initially the box contains again 7 black balls and 4 white\nballs. Suppose that we repeatedly draw a ball at random from the box, observe its color\nand then discard it. We do this 4 times. For i ∈{1, 2, 3, 4}, let Xi be the random variable\nrepresenting the color of the ith ball drawn from the box.\nQuestion: Is it true that\nP ({X1, X2} = {white, black})\n= P ({X3, X4} = {white, black})?\nWrite your best guess for the answer to this question and a few short sentences explaining\nhow you came to this conclusion.\nRC1-1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "18.310C Pre-recitation Assignment 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/c157ab1ee4dcd9baa0bdbc14ac5dc2d4_MIT18_310F13_prerec3.pdf",
      "content": "Fall 2013\n18.310C Pre-recitation Assignment 3\nDue: Thu, 9/19/2013\nReading: Download from\nthe latest lecture notes on Chernoff bounds and go over them,\nfocusing on Section 3. You will need this reading to complete the questions below and the writing\nassignment in homework 3.\nInstructions: Hand in brief answers to the following questions at the beginning of recita\ntion.\n1. Draw a small diagram of the proof of Theorem 4, highlighting key steps and using arrows to\nshow which statements follow directly from which other statements. Which parts of Section\n3 aren't necessary for the proof? Why are they included?\n2. In the equation preceding Equation (2), why has X been put in the exponent? How does this\nmake the rest of the proof easier?\n3. What parts of Section 3 were less clearly explained? What steps did you find confusing or\npoorly motivated?\n4. What parts of Section 3 were most helpful for understanding the proof?\n5. How could you improve Section 3 to make the proof of Theorem 4 easier to follow?\nRC3-1\nthe site\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.310C Recitation 7: Writing for your Audience",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-310-principles-of-discrete-applied-mathematics-fall-2013/858ec3eaa5071156d8fb60d35fa923a6_MIT18_310F13_prerec7.pdf",
      "content": "Fall 2013\n18.310C Recitation 7: Writing for your Audience\nDue in recitation\nBefore recitation, read the following two newsarticle from MIT News:\n\"First Improvement of Fundamental Algorithm in 10 Years\" 1\n\"Explained: Graphs.\" 2\nAlso read the description of the purpose of MIT News at\nhttp://web.mit.edu/newsoffice/about.html\nAfter completing the reading, consider and answer the following questions:\n- Who do you think is/are the expected audience(s) of the news articles?\n- How much do they know/not know? What is the audience's background?\n- Why are they reading the articles? What is the audience's purpose?\n- Why was the article written? What is the author's purpose?\n- The article provided few details-what would be the advantages/disadvantages of providing\nmore detail? If you were to include more detail, what would you include?\nNotice how the article \"First Improvement of Fundamental Algorithm in 10 Years\" communi\ncates the \"essence\" of the maxflow problem. Keeping in mind the questions above, write a brief\nand rough draft of how you would communicate the \"essence\" of the following topic in an MIT\nNews article:\n- Generating functions (if you have been assigned to Problem 1 for your term paper),\n- Union bound (Problem 2),\n- The maxflow-mincut theorem (Problem 3)\n1http://web.mit.edu/newsoffice/2012/explained-graphs-computer-science-1217.html\n2http://web.mit.edu/newsoffice/2010/max-flow-speedup-0927.html\nRC7-1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.310 Principles of Discrete Applied Mathematics\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}