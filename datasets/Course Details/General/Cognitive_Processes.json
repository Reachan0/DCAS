{
  "course_name": "Cognitive Processes",
  "course_description": "This undergraduate course is designed to introduce students to cognitive processes. The broad range of topics covers each of the areas in the field of cognition, and presents the current thinking in this discipline. As an introduction to human information processing and learning, the topics include the nature of mental representation and processing, the architecture of memory, pattern recognition, attention, imagery and mental codes, concepts and prototypes, reasoning and problem solving.",
  "topics": [
    "Science",
    "Cognitive Science",
    "Science",
    "Cognitive Science"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nOverview\n\nWe cover human experimental cognitive psychology, the study of human intelligent processing. Includes high-level vision (e.g., object recognition), attention, immediate and longer-term memory, learning, nature of concepts, imagery, language processing, mental codes, reasoning, and problem-solving. Emphasis on experimental methods and evidence.\n\nIT IS ESSENTIAL TO COME TO LABS AND THE QUIZZES. Note the dates on which the lab reports are due. There will be handouts with an outline at the beginning of most lectures, but they will not provide all the information you need; there will be space on the handout to add your own notes during the lecture.\n\nPrerequisites\n\n9.00, Introduction to Psychology.\n\nTextbook\n\nReisberg, Daniel.\nCognition: Exploring the Science of The Mind\n. 2nd ed. New York: Norton, 2001. ISBN: 039397622X.\n\nLabs\n\nParticipation in three in-class labs are required. These labs are designed to train students in analyzing and understanding the cognitive processes involved in memory and vision.\n\nGrading\n\nActivities\n\npercentages\n\nParticipation in 3 In-class Labs, with Lab Reports\n\n24%\n\nTwo In-class Quizzes\n\n48%\n\nFinal Exam (2 hours)\n\n28%",
  "files": [
    {
      "category": "Resource",
      "title": "mar1h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/9dd4218a41472436acb6ea0bb379002e_mar1h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 March 1, 2004 MEMORY III: Explicit and implicit memory and the brain\nHANDOUT.\nNote: Chapter 6 begins with several topics that won't be discussed in\ntoday's lecture, but you should read about, including:\n-state-dependent learning\n-encoding specificity\n-source memory versus familiarity as the basis for remembering\n-how familiarity without source memory can generate some phenomena such as \"how to become\nfamous overnight\" (p.178), false belief in the truth of statements, systematic mistakes in\nidentification of a criminal.\n-the hypothesis that a feeling of familiarity results not only from\nactual previous experience (which makes a repetition easy to process) but\nsometimes simply because something is unexpectedly easy to process\n(processing fluency) one judges that the word or stimulus must be\nfamiliar (other examples in text).\nOUTLINE for lecture:\n1. Implicit versus explicit memory\n2. Amnesia and the neuropsychology of memory\n-The brain and memory: H.M.\n-Preserved implicit memory\n1. Implicit and Explicit Memory:\nExplicit (declarative) memory:\nThe term \"declarative memory\" has been used for this kind of memory, because you can talk\nabout what you remember: you can declare it.\na. Episodic memory: memory of your personal history, of when and where you encountered\nsomeone, learned that there is no Santa Claus, etc.\nb. Semantic memory: your general knowledge, including word meanings, knowledge of\nscience, history, facts about yourself and others that are not specific memories of a particular\nexperience.\nImplicit memory:\nImplicit or \"procedural\" memory: no conscious retrieval,little or no ability to describe or\nbecome conscious of what you know. There are various forms of nondeclarative memory, not\njust a single \"implicit memory system\". For example:\n\n-Skills\nRiding a bicycle or ironing a shirt, or speaking a language: normally, such learned skills take lots\nof practice. In using these skills, you are not aware of having remembered anything: there is no\nconscious retrieval process. The implicit memory just seems to be triggered when the\nappropriate cues are present.\n-Priming as implicit memory\nIn priming, a recent experience activates nodes or neural structures that make the same or a\nrelated process easier the next time. Semantic priming : seeing \"doctor\" primes \"nurse.\"\nRepetition priming: seeing a particular stimulus like a word, and then seeing it again: the first\nencounter can be shown to speed up the second encounter, and make it more accurate.\nJacoby & Dallas (1981): compared implicit and explicit memories for words, in a two-part\nexperiment:\nPart 1: They had subjects look at a list of 60 words, answering questions such as,\nDoes it contain the letter A? Does it rhyme with \"bad\"? Is it an animal?\nPart 2: Subjects were divided randomly into 2 groups. One group was given a list of 120 words\n(the 60 they had looked at in Part 1 and 60 new ones) and told to say whether they recognized\nthem from Part 1: that is,they had to decide whether each word was old or new.\nThe other group was given a perceptual test, in which each word was presented for 35 ms,\nfollowed by a row of ampersands &&&&&&& that served as a visual mask, making it hard to see\nthe word. Subjects had to say what the word was. New words that hadn't been in Part 1 were\nmixed with words that had been in Part 1. The big question was what the effect of the 3 tasks in\nPart 1 would be, on each of the two Part 2 tests.\nResults:\nPart 1 task\nPhys. Rhyme Semantic\nNew words\n--------------------------------------\nRecognition | .50\n.63\n.87\nFalse yes, .15 |\n|--------------------------------------|\nPerceptual\n| .80\n.81\n.82\n.65\n|\nthreshold\n--------------------------------------\nThe questions in Part 1 were graded in the depth of processing they invoked. When the task was\nto decide whether or not the word had been seen in Part 1, the best task was the semantic\ndecision: is that an animal? (for example).\n\nFor explicit recognition, the \"orienting task\" made a large difference. But for implicit, perceptual\n\"memory\", although the benefit from having been presented in Part 1 was considerable\n(improvement of ~ 15%), it was notinfluenced by the type of TASK in Part 1.\nImplicit memories are not just memories too weak to become explicit (conscious): they are\nqualitatively different from explicit, declarative memories.\nPerceptual tests don't show any benefit unless the original item is shown again in the same\nformat--a spoken word doesn't help perception of that word in written form. Note that Reisberg\npoints out that having completed_L_P_A_T won't help you later complete E_E_H_N_.\nHowever, a spoken word can be RECOGNIZED again in written form. Also, the perceptual effect\nseems to decay less rapidly than the ability to recognize an item as seen before in the\nexperiment.\nThere is often little correlation between the words a given subject can perceive (on the\nperceptual test) and the ones that s/he can recognize, on a recognition test.\nHowever, the repetition effect isn't restricted to perceptual tasks. Jacoby & Witherspoon\n(1982):\nPart 1: (Spoken questions by the experimenter:) \"Name a musical instrument that employs a\nreed.\" (Etc.)\nPart 2: write spoken words to dictation: read/reed. Results: more likely to write r-e-e-d than a\ncontrol group.\nBut did subjects simply remember that they'd heard that word-meaning before? No: because the\nexperimenters found the same results for a group of amnesic patients--who could not explicitly\nremember the words from Part 1.\n2. Amnesia and the neuropsychology of memory:\nThe amnesic syndrome is produced by lesions in the medial temporal and diencephalic parts of\nthe brain, including the hippocampus. General intellectual functions, including language, are\nnormal--and so, more or less, is memory of events before the lesion. But there is impairment,\nsometimes almost total,in the ability to retain new information--that is, when the new\ninformation is explicit, declarative information, like \"Who is the vice president?\"\nAnterograde amnesia: inability to remember new information. Versus retrograde amnesia:\nforgetting of information you knew before the lesion or damage to the brain.\nDamage from strokes or injuries...or alcohol, without proper vitamins, produces individuals with\nKorsakoff's syndrome: used to be a lot of the latter, before people knew how to prevent the\nsyndrome.\nBut it turns out that such amnesics don't lose ALL capacity to learn new material, as used to be\nthought: such patients may have a relatively intact ability to acquire new skills and new\ninformation, if that memory is tapped by IMPLICIT memory tests.\n-Jacoby and Witherspoon's REED finding (above). The amnesic subjects didn't remember Part 1\nexplicitly, but showed as much biasing effect as did the normal subjects.\n\n-------- --------\n-Word-completion tasks like:\nTAB__ (tabby, table). Or:\n_SS_SS__ (assassin, assessed).\nPrior exposure to a possible completion (or, the only possible one) facilitated later completion, or\nbiased toward a certain completion when many were available.\n-Similarly, a prior exposure to particular words makes them more likely to be produced in tasks\nsuch as producing exemplars of a category, or as many words as possible beginning with C...\n-HM: (hippocampus removed bilaterally)\nHas good working memory, has old LT memories but can't fix or consolidate new information.\nBUT can pick up some new tasks! Mirror tracing, rotary pursuit, recognizing mirror-reversed\nwords (mirror-reading): all are pretty normal. This much was known about HM in late 60's, and\nthought to be limited to sensory-motor learning.\nBut then it was discovered that he had preserved repetition priming, word-stem completions,\netc. And perhaps he was able to learn a complex cognitive task, the Tower of Hanoi.\nFurthermore, recent studies have shown that he can pick out the famous name from a pair of\nnames--including names of people who became famous after his surgery--and he can even give\nsome correct reasons for what they are famous for. (He watches a lot of TV.)\nAlzheimers Disease patients (who have problems both in the hippocampus and in their temporal-\nparietal lobes) learn new skills as amnesics do, but (unlike amnesics) they have impaired\npriming; whereas Huntington's disease patients and Parkinson's disease patients, with damage\nto striatum, are impaired on skill learning but normal on priming.\nSo: fact-learning, skill-learning, and \"priming\" (reactivation of existing structures) all seem to be distinct forms\nof memory.\nMEMORY\nDECLARATIVE\nNONDECLARATIVE\n(Explicit)\n(Implicit)\nSemantic\nEpisodic\nfacts\nevents\nMedial temporal lobe,\nDiencephalon\n\nSkills\nPriming\nSimple\nNonassociative\n& Habits\n& Percept.\nClassical\nLearning\nLearning\nConditioning\nEmotional Skeletal\nResponses Responses\nstriatum\nneocortex\namgdala cerebellum\nreflex\npathways\n(From Squire & Zola, 1996.)"
    },
    {
      "category": "Resource",
      "title": "feb4h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/b36588b2745e1fbc5cf075fc9511ec42_feb4h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 February 4, 2004 Introduction; How we read HANDOUT\nI. Introduction to 9.65 Cognitive Processes.\nII. Visual acuity, eye movements, and reading\na. Visual acuity\nb. Eye movements and fixations\nc. Eye movements in reading\nd. Immediacy of processing?\ne. Are eye movements a waste of time? RSVP\nf. \"Speech recoding\" and reading\nI. Introduction to 9.65\nWe cover human experimental cognitive psychology, the study of human intelligent processing.\nIncludes high-level vision (e.g., object recognition), attention, immediate and longer-term memory,\nlearning, nature of concepts, imagery, language processing, mental codes, reasoning, and problem-\nsolving. Emphasis on experimental methods and evidence.\nIT'S ESSENTIAL TO COME TO LABS AND THE QUIZZES. Note the dates on which the lab reports\nare due. There will be handouts with an outline at the beginning of most lectures, but they will not\nprovide all the information you need: there will be space on the handout to add your own notes during\nthe lecture. (This particular handout is more complete than most and includes some information not in\nthe lecture.)\nII. Visual acuity and eye movements\na. Visual acuity:\nWe see in fine detail only about the width of our thumb held at arm's length: acuity (grain) falls off\nvery rapidly in all directions, so that we only getgross information outside a 10 degree circle.\nIn the visual system, the changing grain of acuity corresponds to the density of photoreceptors (cones\nand rods) and retinal ganglion cells in the retina (the back of the eyeball). (Cones, which are sensitive\nover a wide range ofbrightness except for very low levels of illumination, are highly concentrated in\nthe fovea, whereas rods, which are sensitive to very low levels of light but saturate with high levels,\nare more evenly distributed over the retina.) Likewise, cells in the visual cortex (in the brain) are more\nnumerous in thepart that represents the fovea (close to the center of fixation) and thinout in parts that\nrepresent the periphery (away from the fovea). This change in density of representation is called the\ncortical magnification factor.\nb. Eye movements and fixations:\n\nHow do we manage to deal with the 360 degree world, when we have such a tiny area of sharp\nvision? We move our eyes around, to fix briefly on regions that we want more detailed information\nabout: peoples' faces, things that are moving, bright things, parts of the visual field that have lots of\ncontours (such as corners), and words as we read.\nMain characteristics of eye movements:\n2 kinds of eye movements:\n(1) saccade (jump, ballistic movement): jump from one spot to another, and make a FIXATION on\neach spot. This is the main way we pick up information. The eyes are attracted to points in a scene\nwith more information--more \"complexity.\"\n-the average eye fixation lasts for 200-300 ms, and there is a 20-40 ms saccade or movement to the\nnext spot, so THREE OR FOUR FIXATIONS PER SECOND.\n-can't see much while eye is moving (saccadic suppression)\n(2) pursuit movement: fix eye on a moving object and follow it.\nc. Eye movements and visual acuity in reading:\nMethods for studying:\n(1) Eye tracking devices can determine when and to where the eyes go--and how long they stay on\neach spot. Our basic knowledge about reading comes from these studies.\n(2) Moving window (how small a window can be tolerated without affecting reading?)\n(3) Mangled text at some distance from fixation: fixation is indicated by the *.\nxwekicnelkivn you are read*ing this text wlkedimgsmc\n-can see specific letter information for only 7-8 characters (letters or spaces) to the right of fixation,\nless to the left, though can get a little information, including presence of a space, up to about 15\ncharacter spaces to the right of fixation.\n-Our eye movement pattern while reading is determined by number of characters, regardless of size\nof print or distance from the page or CRT (unless extremely small or large).\n-We ordinarily fixate each longish word at least once\n-If we don't directly look at a word, it is probably short and fairly predictable, like some function words\nsuch as and, or, of. (About 2/3 of such words may not be fixated.)\n-regressions (looking back) to an earlier point on 10-15% of saccades.\n-Reading rate: So, for most college students, their net reading rate is something like 5 or 6 words per\nsecond, or around 300 to 350 words per minute (although 250 wpm is common).\nd. Immediacy of processing?\n\nHow immediate is processing? Do we process each word fully while we are fixating it, or do we store\nup several words and then process them?\n\"The cotton cloth is made of grows in the south.\"\nA reader may be momentarily garden-pathed by that sentence, because when he or she arrives at\nthe word \"cloth,\" it seems to be a noun phrase, \"the cotton cloth\" instead of \"The cotton (that) cloth is\nmade of...\"\nThis and other evidence suggests that we do immediately process each word as we fixate it, as far as\nthe sentence and text to that point allows. How do we know this? By studying the durations of\nindividual fixations on a given word, researchers have found that the duration of a fixation reflects\nword frequency, appropriateness to context, syntactic difficulty at that point in the sentence, etc.\nSome of the difficulty created by a given word may \"spill over\" into longer times spent looking at the\nnext word or two, but most problems show up immediately.\nBecause the motor planning for the next eye movement reaches a \"point of no return\" (can't stop the\nplanned movement) about 140 ms before the actual saccade takes place, that leaves only about 160\nms of an average 300 ms fixation for processing the word one is looking at and \"deciding\" whether to\nextend the duration of the fixation or move on. That is, for word frequency to have an effect on how\nlong you look at the word, you have to have recognized the word (or know that you've failed to\nrecognize it) within the initial160 ms of processing.\nThe time for processing in the brain is actually even shorter than that, because it takes about 40 ms\nto get information from the eye to the brain: thus, the central processing time for the average word\nmust be less than 120 ms--including not only looking up the word mentally but also understanding its\nfit to the syntax and meaning of the sentence.\ne. Are eye movements (plus acuity limitations) a waste of time, in reading?\nWould we read faster and more effectively if we could read without eye movements? With computers\nit is possible to present each word, one at a time, in the center of the screen so that you get perfect\nclarity without having to move your eyes. This method, called \"RSVP\" for rapid serial visual\npresentation , has been used to study just how fast people CAN read if they don't have to move their\neyes.\nRSVP reading at 10 or 12 words/s is fairly comparable to reading NORMALLY: provided that you give\nonly one sentence in isolation. People can understand the sentence, make a quick judgment as to\nwhether or not it's plausible, and repeat it accurately.\nBUT: If you read a paragraph at this rate: you seem to understand each sentence and get the general\ndrift, but the amount you remember is small. We'll return to this issue a little later in the course.\nCONCLUSION: RSVP allows fast skimming but only poor memory: so eye movements are not a\nwaste of time. We read the way we do because we need the time for memory consolidation, and\nindeed we adjust the time spent on each word according to the word's intrinsic difficulty, and\naccording to how well it fits the sentence and the discourse.\nTherefore: Speed reading? It's not possible and probably not desirable anyway. Skimming, however,\nmay be sufficient for some purposes, and is worth learning how to do.\nf. Speech recoding and reading:\n\nThere is a lot of evidence in the experimental literature that we DO recode to speech (create the\nsounds of the words in our head) when we read: for example, Van Orden showed that people can be\nmisled in a task such as this. You are shown a word or phrase, and then a second word, and you\nhave to decide as fast as possible whether the second word is consistent with the first word . For\nexample:\n\"Animal\" POODLE [Yes]\n\"Part of a building\" SELLER\n\"Flower\" ROWS\n[Participants may mistakenly say Yes to homophones like seller and rows, when they sound the same\nas cellar, rose (to which you'd say yes)]\n-Is speech recoding a bad feature of reading? Probably not, because a phonological representation\nmay be needed for understanding, for example to hold info. in memory briefly while we process a\nsentence.\n-Does phonological recoding--\"hearing\" the sounds of words as we read--slow us down? Probably\nNOT, because we apparently can recode even when reading at 12 words/s (in RSVP).\nPetrick experiment:\nThe boy's pockets were heavy with change which jingled as he walked.\nCHAINS? (acoustic) COINS? (semantic) ROPES (unrelated, but related to the acoustic distractor)\nPositive probe CHANGE\nResults: people were slower and made more errors to both chains and coins, after reading at 83\nms/word (12 words/sec).\nScrambled sentences: more phonological confusions, fewer semantic.\nIs recoding to \"sound\" only true for alphabetic writing systems? No! it happens in reading Chinese\nand other logographic systems.\nCONCLUSIONS: -acoustic/phonological recoding as one reads is universal,and happens FAST, so\nprobably doesn't slow you down in reading. You just become more aware of the sound if reading is\ndifficult or you are distracted and not concentrating on the meaning."
    },
    {
      "category": "Resource",
      "title": "apr5h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/93974c7d2e4c3cefb51af4c9726df25f_apr5h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 April 5, 2004 Visual Knowledge HANDOUT\nNote: Quiz 2, covering everything since the last quiz, is on Wednesday.\nI. What is a mental code?\nII. Imagery as a mental code\nBrief history\nA. What is imagery for?\nB. Relation of imagery and perception\nC. Imagery and the brain\nD. Why have more than one mental code?\nIII. Memory for pictures: Boundary extension\nI. What is a \"mental code\"?\n-a mental code is a hypothetical code used by the mind-brain to represent, store,\nand transform information: transforming the information includes the processes we\ncall \"thinking.\"\n-Note that a mental code has two roles:\n1) to make some information easily available\n2) to serve as an index to other information: e.g., holding words in an acoustic\ncode in STM\nII. IMAGERY as a candidate for a major code\nHistory of ideas about imagery, in brief:\nGreeks\nThoughts are faint mental images\nBishop Berkeley\nTriangles\nJerry Fodor\nImages are ambiguous, thoughts aren't\n\nZenon Pylyshyn\nThe Homunculus problem: who looks at the images?\nStructural descriptions, not dense arrays\nA. What is imagery for ?\nTry answering these questions:\nWhich is larger, a golf ball or a tangerine?\nA tiger or a Great Dane dog?\nIs the hot water tap on the L or R?\nDoes a tractor have two big wheels in front?\nWhich is darker (on the outside), a cucumber or a watermelon?\n-Did you feel that you used imagery to answer any of these?\nSo: One thing that imagery seems to be used for:\n-Retrieving subtle spatial or perceptual information from memory:\n(i) that has not been stored as such and (ii) that can't be deduced easily from other\ninformation.\nOther possible uses for imagery:\n-planning movements\n-understanding descriptions (e.g., form a mental model)\n-maintaining an image of the immediate environment?\n-solving certain kinds of problems\n-planning layouts (e.g., artist)\n-getting around in the dark\n\n-route-planning\n-MENTAL ABACUS: highly skilled abacus users are faster in doing arithmetic than\nsomeone with a hand calculator: AND can use a \"mental abacus.\" Stigler (1984,\nCognitive Psychology, p. 145 ff.)\nEvidence for mental abacus: recognizing intermediate abacus layouts\nB. Relationship of visual imagery and perception\n-Selective interference: The Brooks experiment\n-Spatial or visual interference? Baddeley's experiment: Mental matrix task with two\ntypes of interfering tasks:\n-blindfolded subject and pendulum (spatial, not visual)\n-detect change in brightness of a stationary light (visual, not spatial)\nConstraints on imagery: Like perception, it has some limitations:\n(1) Visual angle of mind's eye (Kosslyn, 1978)\nINTERPRETATION: K: there's a specialized imagery ability that has constraints\nsomewhat like those of vision itself, in this case with a limited \"angle\" of view. (Lab\n2)\n\n(2) Imagery \"acuity\"\n-rabbit beside an elephant versus beside a fly:\n-Image size can be changed: zooming.\n(3) Imagery \"scanning\" [see Reisberg chapter]\n-memorize map with places on it\n-scan from designated start point to a place named by the experimenter\n-critics: \"demand characteristics\" of experiment\n(4) Mental rotation (Shepard, etc.: See Reisberg. Understand what a linear RT\nfunction tells us.)\n(5) Inspecting one's image to read off information, like perception:\nWeber and Castleman exercise in Lab 2.\n(6) Limited capacity of imaging\n(In class:) Try task in which you mentally create an image in an imaginary grid of\nlarge squares. When I say North, draw a mental line upward on the grid, one unit,\nwhen I say Northeast, draw one diagonally from that point to the next square up and\nover to the right, etc. Okay start: North, northeast,west, south, west, west, south,\n\nwest, northeast, west, southwest, east. Now draw it [back of handout].\nC. Imagery and the brain\nImagery and perception overlap in the brain:\n(1) Kosslyn and his colleagues: imaging activates the primary visual areas in the\ncortex. Larger, smaller images generate larger, smaller areas of activation.\n(2) O'Craven & Kanwisher (2001):\n-Background: FFA (fusiform face area) and PPA (parahippocampal place area)\n-task of viewing/imaging faces, or viewing/imaging places\n(3) Lesions in the brain\n-unilateral neglect patient (damage to R hemisphere, Bisiach and colleagues)\nD. Why might it be advantageous to have more than one code?\n\n1. Different time courses (short-term; longer-term)\n2. Different information is available on the surface--so suitable to different tasks.\n-analog versus digital clocks\n3. Can take advantage of reduction in interference when performing dual tasks\n-using fingers to tally \"targets\" when listening to a list\nSo: just how many mental codes do we have?\nWe don't know for sure.\nCONCLUSION: Imagery is a well-established code distinct from at least one other\ncode--although there continue to be skeptics.\nIII. Memory for pictures: Boundary extension\nEven our immediate memory for pictures is schematic rather than literal (Intraub &\nRichardson, 1989).\nChange blindness: It's difficult to spot a change in a picture, unlessthe change is\ncentral to your interpretation of the picture.\n\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN\nY\nN"
    },
    {
      "category": "Resource",
      "title": "may5h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/e70ef1998035676745c20bcd7dc053b1_may5h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 May 5, 2004 Solving Problems Handout\nOutline:\nApproaches to understanding problem solving:\n1. Different styles of problem-solving\n2. How and why people get stuck\n3. Prescriptive approaches\n4. Simulations of problem-solving behavior\nIntroduction: Try this problem (from W. Kohler). [See overhead.]\nA circle is divided into 4 sections. One radius is labelled \"y\". A\nrectangle is drawn in another section, in which a right triangle\nis embedded: d is the length of one side of the right angle, x the\nlength of the other side of the right angle, and L is the length\nof the third side. Given d, x, and y, how long is L?\nScientists and mathematicians have sometimes classified themselves\nas symbol-pushers versus visualizers. Which are you?\nNow, classify yourself into one of 4 categories:\nVisualizer\nSymbol-pusher\nInsight solution to\ncircle problem\nFormula (or no) solu-\ntion to the circle..\n\n1. Different styles of problem-solving:\nThe idea of classifying people as visualizers versus symbol-\npushers is an example of this approach.\nWertheimer, Productive thinking, 1923:\nThe Altar window problem (lab)\nWertheimer proposed that there are 3 styles of problem-solving:\na) Use deductive logic: standard algorithms; math; logical\nsyllogisms.\nb) Retrieve the answer from memory: learn the steps in a proof.\nImprove performance by repetition, habit, frequency, recency,\ntrial and error.\nc) Use productive thinking: analogies, application of knowledge\nfrom another field, and especially \"structural analysis.\"\nAnother example that distinguishes among these styles or\napproaches:\nArea of a parallelogram:\n\nThese are examples in which a visual) representation may help one\nto pick out the appropriate transformations to make the problem\ninto a more familiar one: \"insight\" (or, in Reisberg,\n\"illumination\").\nHowever, visualization can sometimes be misleading: as in folding\na sheet of paper 1/100 inches thick over, repeatedly: how thick\nwill it be after 50 folds?\nWhat is \"insight\"? A one-step recognition that a new problem is\nequivalent to a familiar problem? If so, insight will depend on\nthe knowledge you already have.\nThe 13 problem of Duncker: Why are all 6-place numbers of the form\n267,267 or 591,591, etc., all divisible by 13? (8% of subjects in\nDuncker's experiment solved with no further information; hints led\nto a higher percentage of solutions)\nCan't have a feeling of \"insight\" unless the underlying principle\nis fully familiar and intuitively correct, once it comes to mind.\n2. How people get stuck.\n(a) Set or Einstellung: Luchin's water jars. [Lab 3, and Reisberg\np.471-473]\n(b) Duncker: \"Functional fixedness.\" (See Reisberg. 469-471.)\nProblems with real objects; tacks in a box made it less likely the\nsubject would think of using the box as part of the solution.\nOther problems in which an object with a standard use is not seen\nas useable in another role (like pliers as a weight for a\npendulum).\n\nMore abstract problems can be regarded as cases of \"functional\nfixedness\":\n\"A surgeon comes across an automobile accident, takes a look at\nthe victim lying in the road, and exclaims...\"\nRelated issue: transfer of training: recognition that another\nproblem is analogous to one you already know the answer to.\nIt is often difficult to realize that two problems have the same\nunderlying structure, so that if you've solved one, you can easily\nsolve the other. See the discussion in Reisberg, 459-463.\n3. Prescriptive approaches\n-make inferences & associations\n-look for analogies\n-figure out subgoals\n-hill-climb\n-if stuck, look for new routes\n-if necessary, leave the problem and return later\nQuestion of incubation: Does your brain continue to work on the\nproblem while you're not consciously thinking about it? See\nReisberg, pp. 480-83 for a possible answer.\n\n4. Simulations of problem-solving:\nNewell & Simon (1972): The General Problem Solver (GPS) [Reisberg,\npp. 448-453] They proposed the following.\nA. There are just a few general characteristics of human problem\nsolving:\n-serial processing\n-certain elementary processes: information transfer, elementary\noperations: they take of the order of 50-300 ms each\n-Inputs and outputs are held in a small-capacity STM, measured in\nchunks that depend on experience\n-storing in LTM takes about 5 s per chunk\n--retrieval from LTM is fairly fast: e.g., 200 ms.\nB. A task environment is represented as a problem space in which\nproblem solving take place: a space of possible situations to be\nsearched.\nHow do you set up the problem space? Task instructions and\ndisplay; memory for similar or analogous tasks; metaprograms for\nsetting up space-general task programs.\nC. The task structures the space.\nD. The structure determines the possible problem-solving program.\n\nYou start with an initial state, and you work toward a goal state,\nusing operators. In setting up the problem space, you may have\nsome path constraints. (R., p. 448.)\nAt any one time you are at a given node or knowledge state in the\nproblem space; the knowledge state is what you have in STM. You\ncan either choose an operator to apply to take you to a new node--\nor can retreat to a different node (recover a different set of STM\ninfo--or look at one's external memory).\nHeuristics include: hill-climbing (which doesn't always work, as\nyou may have to go away from the goal at some point in the\nsolution), means-end analysis, and working backward from the goal\n[R., pp. 450-453].\nNewell & Simon's computer simulation is consistent with protocol\nanalyses of subjects solving problems like the DONALD + GERALD =\nROBERT problem: Each letter is a different digit: e.g., all D's\nare one digit: in this case, D = 5. The simulation predicts\n(approximately) the time for each step in solution, and the order\nof steps, the harder places, etc.\nD O N A L D\n+ G E R A L D\n_______________\nR O B E R T"
    },
    {
      "category": "Resource",
      "title": "feb9h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/b386947247759265613e9fe8435a1d51_feb9h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 February 9, 2004 Object recognition HANDOUT\nI. Why is object recognition difficult?\nII. Basics of visual perception\nA. Gestalt principles, constancy\nB. Computational approaches: e.g., perception of\nsurfaces\nIII. Higher-level vision: Object recognition\nIV. Word perception\nToday: Visual perception, leading to higher-level vision: object recognition, word\nperception.\nI. Why is object recognition difficult?\nFor example, what's this?\nThis example illustrates the ambiguity of perception, and its interpretive nature.We\ncombine information from our senses with our knowledge of what's likely, especially\nwhat's likely in context.\nThe early visual system extracts information that is likely to be correlated with actual\nobjects in the environment--under these generally true assumptions about objects:\n-continuous boundaries\n-surfaces that change texture and color and brightness relatively smoothly, if the surface\nis continuous, but change sharply when the surface is discontinuous.\n-move as a unit\n-rigid (or they deform in a regular way).\nWhy simple solutions don't work in object recognition\nSEPARATING OBJECTS IN A SCENE: difficult\nTEMPLATE-MATCHING (once a single object has been segregated)--why this often won't\nwork\n\n-need to recognize in [almost] any position and rotational orientation\n-any size on the retina\n-any brightness\n-any illuminant color\n-with various distortions or variations\nLearn by experience with each specific object? To some extent, but we generalize well to\nshifts of size, orientation.\nSo: we must have some way of abstracting certain \"invariants\" from an object.\nBiederman's RBC theory, in your text (Chap. 3).\nII. Basics of visual perception:\nA. Gestalt principles, perceptual constancies, the Weber-Fechner Law :\nGestalt psychologists: Wertheimer, Koffka, Ko\"hler, and others.\nThe first principle: figure-ground organization:\nfigure \"captures\" the common contour, figure has more definite localization, more solid\n(less filmy) color\nOther principles:\nproximity\nsimilarity\ngood continuation\nclosure: dotted lines joined\ncommon fate (moving together)\nAll represent \"good form\" [Law of Pra\"gnanz]: simplicity, inclusiveness, symmetry,\n\nregularity.\nThe perceptual constancies:\nThe Gestalt psychologists and their followers also emphasized the RELATIONAL character\nof perception: we see relative (not absolute) brightness, so that a white piece of paper\nstill seems white in a dim room, when the light intensity reflected from it may be lower\nthan that of a black piece of paper in a bright light.\nThis is perceptual CONSTANCY--perception \"correcting\" for the brightness or color of\nillumination, correcting for the angle at which we view something, the object's size on\nthe retina as a function of distance, etc.\nPerceptual constancy is not perfect, however: we do notice changes. Just how much\nchange does there have to be, before we notice it?\nThe Weber-Fechner Law: Fechner's Law: S = k log I, where S is the psychological\nsensation, I is the physical measure, e.g., intensity of light, and k is a constant that\ndiffers for different physical dimensions. This law is a good descriptive generalization,\nbut it tends not to hold at the extremes of any dimension.\nThe perceived \"strength\" of a sensation such as the brightness of a light grows as a\nRATIO of the light's physical intensity to that of the light you are comparing it with.\nSubstituting a 100 watt bulb for a 50 watt bulb makes a very noticable difference,\nwhereas going from 200 to 250 watts is much less noticeable.\nThis ratio law reflects a compromise between perfect constancy (which would leave us\nunable to detect a shadow, for example) and a perfect correlation between light energy\nand perceived brightness (which would make the same object look too extremely\ndifferent as lighting changed).\nTo sum up, the Gestalt psychologists focused on the abstract, inferential character of\nperception: we don't \"see\" what our retina sees, but instead perception is the\nconsequence of a complex set of processes.\nB. Computational approaches to object perception:\nA more unified, computational account of the many phenomena described by Gestaltists\nand other visual scientists.\nDavid Marr\nAn important goal is to discover CONSTRAINTS or assumptions made by the visual\nsystem: to resolve AMBIGUITY.\nOne example: perception of surfaces.\n\nRecommended: Nakayama, He, & Shimojo (1995), chapter in Kosslyn & Osherson\n(Eds.), Visual cognition: Vol. 2, An invitation to cognitive science 2nd Edition.\nOne illustration of this approach: the perception of figure and ground.\nPrinciple One: only one surface can \"own\" a boundary between it and another surface.\nPrinciple Two: we are built to see probable events, not unlikely ones.\nGeneric (typical) view\nAccidental (coincidental) view.\nHard-wired and MODULAR?\nIII. Higher-level vision:\nRole of TOP-DOWN CONTEXT\nDemo:\nIV. Word perception:\na. Words in isolation:\nThe Wheeler-Reicher WORD SUPERIORITY EFFECT (WSE) (see text, pp. 68-69): It's\neasier to detect a letter in the context of a word than in isolation.\nExperiment:\n\nK\n_ _ _ K --> # # # # --> _ _ _ _\nD\n(which did you see: K or D?)\nOR:\nK\nW O R K --> # # # # --> _ _ _ _\nD\n-word shape doesn't matter: WORK, work, even WoRk\n-the effect requires a very clear glimpse, followed by patterned masking--not dim or\nindistinct, followed by a blank field\n-you still get some benefit from pronounceable \"pseudowords\": REET, MAVE as\ncompared with random, nonpronounceable strings: RTEE, AEVM.\nExplanations of these effects require that, at the brief moment of perception, possible\nwords CONSTRAIN what letters are seen: tentative letter assignments constrain each\nother.\nC\nC\nT E\nT E\nTHE vs. CAT:\nT\nT\nH OR A? (Fig. 3.6 in text)\nIn Chapter 3 several examples of simple network models are described to give you an\nidea about how the visual system might place its bets effectively, even when reading\nrapidly or in imperfect visual conditions. The McCLELLAND-RUMELHART Interactive\nActivation Model (an influential early model) is also described.\nb. Word perception in context\nNormally we don't read words in isolation, but in sentence or paragraph CONTEXT.\n\nPROOF-READING\nContext experiment: Tulving, Mandler, & Baumal (1964) gave participants 0, 2, 4, or 8\nwords of sentence context (9 was full sentence),and then a very brief word for 0 to 140\nms. Ascending method: first saw word at 0, then increasing durations until recognized.\nE.g., An apple/from the/ tree hit/ my bare/ HEAD.\nResults:\nViewers required a longer exposure duration to recognize final word, the shorter the\npreceding context.\nProblems:\nSubjects had multiple chances to guess the word, as the duration increased --unlike\nnormal reading. And they could think about the context as long as they wished, before\nguessing the word.\nOTHER EXPERIMENTS on context: Immediate measures:\nPotter, Moryadas, Abrams, & Noel, 1993, used RSVP presentation of sentences, so that\nsubjects didn't have time to puzzle out what each word was, in relation to the context.\nThey showed that the context was still able to influence recognition of a critical word\nthat might or might not belong in the context: e.g., horse/house/honse:\na. They looked at the horse from their car. Neutral: horse or house usually read\ncorrectly\nb. The boy rode the house around the pasture. Biased against word: house often\nmisread as horse.\nc. The lawn in front of the honse was overgrown. Nonword honse was often misread as\nhouse, rarely as horse.\nA McClelland-Rumelhart type of model could readily explain these results, but only if\nexpanded to include high-level knowledge as a source of top-down input once several\nalternatives are proposed by visual analysis.\nCONCLUSION ABOUT CONTEXT AND WORD PERCEPTION:\nInitial access on basis of perception only? Context selects or confirms."
    },
    {
      "category": "Resource",
      "title": "mar10h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/32d1d27795e37e5b245193783ee7e0f2_mar10h04.pdf",
      "content": "________________\n9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 March 10, 2004 Memory V: AssociativeMemory HANDOUT\nNOTE: that LAB 2 IS ON WEDNESDAY NEXT WEEK. Pick up a\ncopy of the Anderson reading for next Monday.\nAssigned: Chap. 8.\nOutline:\n1. Meaning and Long-term Memory\n2. Network Models of Semantic Memory\na. Quillian's network model of semantic memory\nb. Spreading activation\nc. Anderson's ACT\n-the fan effect and the paradox of the expert\n3. Nodes, Links, and Connectionism\n1. Meaning and Long-term Memory\nTo review: Much of memory seems to be structured associatively. In the next lecture on\nlearning, you'll read about the conditions under which associative links are formed.\nToday, we focus on the way associations are STRUCTURED. It turns out that, in general,\nmultiple links to already well-structured information gives the greatest chance of\nretaining the information. How do we knowthat?\nEarlier in the course we discussed Craik and Lockhart's theory that Depth of Processing\ndetermines LTM. Another experiment, by Craik and Tulving (1975) on the effect of\ndifferent tasks to be performed on a series of words.\n-decide whether the test word would fit in a sentence, such as \"He met a\n______________ on the street\": FRIEND (versus TABLE)\n-is this word in capital letters? friend\n-does this word rhyme with WEIGHT? FRIEND\nDifferential later memory for the words. INTENTION to remember the words doesn't\nmatter much.\nSUMMING UP THE ROLE OF MEANINGFULNESS IN LT MEMORY: What gets linked up\nduring encoding determines what gets remembered. And coding in terms of meaning\nprovides the most distinctive encoding, the largest set of connections to already-known\ninformation, and therefore the most RETRIEVAL ROUTES.\n\nContent-addressing:\nSome kinds of information CANNOT BE USED EFFECTIVELY as a retrieval address, even\nthough the information is in memory. E.g., word for a family relationship (for example,\ngrandmother, cousin) that ends with w?\n2. Network models of semantic memory\nThe idea that memory consists of associations between mental entities underlies\nassociative models of human memory. In such models the contents of memory\nconsist of mental representations of items and their associative connections.\n\na. Quillian's network model of semantic memory\nUsing the terminology of graph theory, items are commonly called nodes and\nconnections between them are called links.\nbreathes\nanimal----has skin\nreproduces\nfeathers\nswims\nwings----bird\nfish----scales\nflies\ngills\npredator\neats worms\npink\neagle\nrobin\nsalmon\nAmer.symbol\nred breast\nedible\nFrom Quillian (1966) and Collins & Quillian (1969).\nhas (or hasa) links (features or properties), isa links (superordinate, subordinate)\nb. Spreading activation\nExperiments by Collins and Quillian (1969)\nHierarchical structure\n\nEconomy of storage\nProblems:\nc. Anderson's ACT:\nPropositions, not just single concepts.\n>From Anderson, 2000:\nJohn bought some candy because he was hungry.\nThe fan effect\nParadox of the expert\nNetworks connecting concepts? E.g., Anderson, certain labelled links:\n-isa\n-has\n-relation: verbs/actions as relations between an agent and an\nobject.\n\n-time and location make a proposition an episode, rather than a semantic \"fact\"\nProcessing assumptions\nNeeded: more complex accounts of the structure of information.\n3. Nodes, Links, and Connectionism\nBackground: Perceptrons:\nNo way to represent \"exclusive OR\" (XOR):\nNeed hidden units\nConnectionism or Parallel Distributed Processing (PDP)\nUnits and associative links (connections)\nDistributed information\nPDP framework:\n1. Set of processing units\n2. State of activation over these units\n3. Pattern of connectivity among units\n\n4. An activation function for each unit--either a threshold (fires or doesn't), or a smooth,\ncontinuous function such as a sigmoid.\n5. Learning rule: modification by experience, by changes in weights that connect\nprocessing units, as a result of FEEDBACK (BACKPROP).\n6. Environment of the system (e.g., where do the inputs come from, what sorts of\noutputs).\nMany differences between the real CNS and the PDP models\nThe two most important advances of the PDP models are:\n(a) the demonstration that one can get \"intelligent\" performance out of simple units in a\nnetwork just by adjusting the strength (weight) of the connection of one to another.\n(b) the very same units can participate in representing many different pieces of\ninformation.\nVital to this kind of model are HIDDEN UNITS.\nFrequency"
    },
    {
      "category": "Resource",
      "title": "feb11h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/a39f58e79423b07fa1174d2fdc9ad717_feb11h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 February 11, 2004 Attention I. HANDOUT\n1. Attention: What is it? Iconic memory and attentional selection\n2. Theories of attention:\na) Filter theories (early selection)\nb) Late selection theories\nc) Flexible selection\n3. Automatic vs. Controlled attention: Stroop demo.\n1. Attention: What is it?\nTwo senses:\n(1) alertness or arousal, awakeness\n(2) selective attention: what we'll talk about here\nIconic Memory: The partial report technique\nThe method for studying this form of visual memory illustrates the effects of selective\nattention.\nThe experiment: Sperling, 1960:\nTwo possibilities:\n(1) when you try to report all, you get confused as you report (output\ninterference)\n(2) you \"know\" about all 12 letters (or, at least 9 of them), but the memory\nholding all that information decays rapidly--nothing to do with \"output interference\"\nHow to distinguish? Delay the tone signalling the row: if (1), still get 3 in the row; if\n(2), you'll lose information with delay.\nDelay of tone: Results:\n\nVisual factors (dark versus bright) are involved in the very brief memory:\nThese and other results suggested that there is a form of visual \"persistence\"-- iconic\nmemory: a very short form of memory that can be processed selectively, showing\nthat attention can be deployed rapidly.\nAttentional selection is a gateway to further processing and memory. But what\ndetermines what you attend to?\nInvoluntary selective attention:\nVoluntary selective attention: DEMO\nWHY do we need to attend selectively?\n2. Theories of attention\nHOW do we attend selectively? Cherry (1953: MIT in RLE: Research Lab. of\nElectronics): \"the cocktail party problem.\" Presented 2 messages on headphones and\ninstructed subjects to shadow one of the messages [that is, say the words as they\nwere heard]. That is, listeners were to pay attention to one of the messages. Binaural\n(both messages to both heads) harder than dichotic (one to each ear).\n-Easier to select if there's a PHYSICAL (SENSORY) difference between two channels\n-SPATIAL LOCATION\n-Little information about NONATTENDED stimuli seems to be processed.\nThese findings led to the first theory of selective attention:\n\na. Filter theory of selective attention\nBroadbent's (1958) filter theory:\n-Initial sensory representation: the S-store or sensory store.\n-Attention imposes a \"physical\" filter on selection from S-store information\n-The filtered information from the S-store is passed to the P-store (perceptual store,\nwhich is more stable)...information decays rapidly from the S-store. Key experiment:\nProblems with the Broadbent filter model:\nGray & Wedderburn '60 also used 2 sets of 3 words or syllables, e.g.:\nand Ss were able to follow \"sense,\" as shown by the order in which they wrote down\nthe words or syllables.\nSimilarly, Treisman (1960) (dichotic) found that subjects would sometimes\nspontaneously follow meaningful text to the other ear. (The task was to SHADOW\n[speak aloud right after hearing it] the ear that started out with the meaningful\nmessage). Both results conflict with filter theory.\nMORAY and your name.\nTriesman's attenuation theory: Attention attenuates ignored channel.\nb. Late selection theories of attention\nAll (or a lot) of the information gets processed to a fairly high level, before selection\ntakes place.\n\nHow complete is the processing of an unattended message? Semantic\nprocessing of unattended material? More evidence\nLackner & Garrett (1972): Ambiguous sentence attended in one ear.\nCorteen & Wood (1972): Experiment in which subjects were first given mild shocks\nwhenever certain words--city names--were presented in a long ist. This set up a\nconditioned autonomic response, the galvanic skin response or GSR, which measures\nchanges in the resistance of the skin with sweating. After training, just seeing the city\nname increased GSR.\nThen Ss SHADOWED prose in one ear and heard a list of words in the other. The list\nincluded the city names plus new city names, and neutral words. Measured GSR.\nSo: Some unattended information seems to be processed at a rather high level.\nc. Flexible selection:\nThe earlier the stage of processing at which selection is possible, the faster and more\nefficient the response to the attended channel, and the less is processed on the\nunattended channel.\n3. Automatic versus Controlled Attention\nStroop Effect: Name the color of ink that the words are written in. DEMO.\nControl conditions:\nname colored words, name uncolored words, name colored squares\nWhy the effect?\n\nShiffrin & Schneider (1977): the effect of practice on selective attention. (See Reisberg\nchapter.)\nThe target set was 1-4 letters, and it was either consistent--same throughout the\nexperiment--or varied on each trial.\nAutomatic versus controlled attention."
    },
    {
      "category": "Resource",
      "title": "apr12_14h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/7708018a91b3cf3b5ff68f2b5fa108e2_apr12_14h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 April 12-14, 2004 HANDOUT\nMental Codes: Pictures, Words, and Thoughts\nOutline:\nI. Theories about codes:\nA. dual coding theory\nB. conceptual coding\nII. A test: naming and categorizing pictures and words\nIII. Some further issues and experiments\nIV. Conclusions\nI. Theories about codes.\nWhat are the roles of different mental codes in thought, and how many\nsuch codes are there? Earlier, we talked about concepts and how they\nare represented: images, definitions (necessary and sufficient\nconditions), prototypes, exemplars. Last week, I talked about mental\nimagery as a possible mental code, and some findings that link vision\nand visual imagery in the brain. Today and Wednesday, two theories\nabout the nature and architecture of mental codes, and how\nexperimental evidence can distinguish between them.\nA. Dual coding theory. PAIVIO: two codes in thinking: IMAGERY\n(visualization), and LANGUAGE (the VERBAL CODE). (Note discussion\nof Paivio's ideas in Reisberg, pp. 362-365, which you've already read.)\n______________\n_______________\n|\n|\n|\n|\n| VERBAL\n|--------------->| IMAGE\n|\n|\n|\n|\n|\n| SYSTEM\n|<--------------| SYSTEM\n|\n|______________|\n|______________|\nVerbal code:\n1. based on associations formed between words via reading,\ntalking, listening. So, is especially effective for representing temporal\nor spatial sequences: serial information.\n2. can represent abstractions\n\nImagery code:\n1. Sensory experiences\n2. Concrete, IMAGEABLE objects and events\n-Dual coding for concrete objects\n-associative link between image and name\nHow to test?\n-written words and pictures of objects: hypothesized that a word\nenters the verbal system directly, a picture enters the imagery system\ndirectly and the verbal system only indirectly, by the name-association\nlink.\nEvidence:\n(1) Takes longer to NAME a picture than to name (aloud) a written\nword (old finding)\n(2) In long-term memory, pictures are recalled better than words.\n(TWO CODES). IMAGING helps recall of words--but only for concrete\nwords that HAVE images. So, concrete words are remembered better\nthan abstract words.\n(3) Model is consistent with known hemispheric specialization: the L\nhemisphere is specialized for language, the R hemisphere for\nperceiving visual patterns such as faces.\n\nProblems:\n-simple associations don't have the power to account for our ability to\nuse language\n-is a more abstract code needed?\n-something for a natural language to be mapped onto. Why do we\nneed that?\na. Natural language is ambiguous, but thoughts are NOT\n(although they may be vague)\nb. Natural language permits synonymy: but thoughts don't\nc. Second language\nd. Children and other animals\nThere seems to need to be an abstract representational system:\n(a) natural language has to map onto something, to account for the\nAMBIGUITY of language but the unambiguity of thought--to account\nfor SYNONYMY--and to allow DIFFERENT LANGUAGES to map onto one\nthought system--and to have a medium in which to form the thoughts\nwhich you then express (although this isn't by itself compelling,\nbecause there has to be SOME initial rep. system, or you'd have an\ninfinite regress).\n(b) CHILDREN and ANIMALS can evidently think without knowing\nlanguage.\nThese are logical arguments, not empirical demonstrations, and many\nconsider that there may be some clever way to get word-nodes to do\nthe necessary mental work, as in Paivio's dual-coding theory.\n\nB. Conceptual coding theory : Potter & Faulconer, 1975; Potter,\n1979.\n______________\n|\n|\n|\n|\n| LEXICON\n|\n|\n|\n|_____________|\n_______________\n|\n|\n|\nIMAGE\n|\n|\n|\n|\nSTORE\n|\n|______________|\n_____________\n|\n|\n| CONCEPT\n|\n|\n|\n| SYSTEM\n|\n|____________ |\nThe third code represents IDEAS (CONCEPTS) without necessarily\nclothing them in WORDS or IMAGES: the Language of Thought (Fodor)\nor mentalese (Fodor, Bever, & Garrett): it represents concepts or\nideas.\nExplaining the data:\n(1) NAMING pictures and words\n(2) Memory advantage for pictures/imaging/concrete (not abstract)\nwords\n(3) Hemispheric specialization\n\n________\n________\nBOTH MODELS CAN EXPLAIN THE RESULTS\nDUAL CODING MODEL\nfork\nfork\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n________\n________\n| Verbal |------>| Image |\n| FORK |------>| Image |\n| System |<------| System |\n| UTENSIL|<------| System |\n|________|\n|________|\n|________|\n|________|\n|\n|\n|\nCategory\n|\nMatch\n\"fork\"\n|\n\"yes\" or \"no\"\nCONCEPTUAL CODING MODEL\nfork\n|\n|\n|\n|\n|\n________\n________\n|\n|\n| Image |\n| Lexicon |\n| Store |\n|________|\n|________|\n| \\ \\\n/ /\n|\n\\__\\\n__/__/\n|\n| Concept |\n|\n| System |\n|\n|________|\n|\n|\n|\nCategory\n|\nMatch\n|\n|\n|\n\"yes\" or \"no\"\n\"fork\"\nfork\n|\n|\n|\n|\n|\n________\n________\n|\n|\n| Image |\n|Lexicon |\n| Store\n|\n|_______| |________|\n| \\ \\\n/ /\n| \\__\\___/__/\n| | Concept |\n| | System |\n| |________|\n|\n|\n|\nCategory\n|\nMatch\n|\n|\n| \"yes\" or \"no\"\n\"fork\"\n\nII. A test: Naming and categorizing pictures and words.\nPredictions from the two models:\nDEMONSTRATION: Naming versus categorizing pictures and words.\nRESULTS\nDEMO: Name the category of a picture, word.\nPossible problems with these experiments and the argument:\nIII. Further issues and experiments.\na. How to get an advantage for words rather than pictures:\nSentence plus picture or word probe: Potter, Valian, & Faulconer\n(1987).\n\nb. Probing a scene with line drawings versus words: Potter & Elliot.\nc. Size, value judgments:\nd. Bilinguals and the mental lexicon: Potter, So, Von Eckardt, &\nFeldman (1984).\ne. Brain imaging study:\nVandenberghe, R. Price, C., Wise, R., Josephs, O., & Frackowiak, R.S.\nJ. (1996). Functional anatomy of a common semantic system for\nwords and pictures. Nature, 383, 254-256. PLUS Commentary by A.\nCaramazza, \"Pictures, words and the brain\" in the same issue, pp.\n216-217. (NOTE: you don't have to read this article!)\nIV. Conclusions: Mental codes\n1. Most of our general knowledge is represented in an abstract\n(conceptual) form, not tied to a particular sense or a particular\n(natural) language. We use this conceptual system in looking around,\nthinking, conversing.\n\n2. Special-purpose memories of surface form: E.g., imagery is like\nperception.\n3. The surface representations are what we are introspectively aware\nof. \"Blackboard\" or \"playback\" device in thought?"
    },
    {
      "category": "Resource",
      "title": "mar15h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/47f507768ed9fe03417c8e100d8d1fe2_mar15h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 March 15, 2004 LEARNING: Handout\nAssigned: One chapter (2) and part of Chapter 3 from J. R. Anderson (2000).\nLearning and memory: 2nd Edition. New York: Wiley. Note: you may omit pp. 45-\n48; read pp. 49-57 to pick up the general ideas about conditioning; and study the\nrest of the chapter (including the beginning sections, 39-44).\nOutline:\nElementary principles of learning:\nA. Contiguity\nB. Frequency\nC. Contingencies and blocking\nIntroduction: Learning vs. memory\nPavlov's dogs now link up with computational modelling and even with Bayesian\nreasoning.\nA. CONTIGUITY\nThe principle of association by contiguity in time [and space]:\nE.g., flashbulb memories\nBasics of classical conditioning:\nUS: Unconditioned stimulus\nUR: Unconditioned response\nCS: Conditioned stimulus\nCR: Conditioned response\n[Also: CER: Conditioned emotional response]\n\nStandard procedure in classical conditioning:\nWhy form an association only when objects or events are in close temporal [and\nspatial] proximity?\nConstraints on learning:\nIs simple contiguity sufficient for learning?\nPreparedness: Seligman (1970) (also called associative bias )\nFor example, a rat can readily associate a sound or light with a shock (and learn\nwhat to do to avoid it), or can learn to avoid a food or liquid with a certain taste to\navoid becoming nauseated, but has great difficulty associating the taste with shock\nor the sound or light with nausea (Garcia, Hawkins, & Rusiniak, 1974)\n\nContinguity: Necessary for an association?\nIn rats and other animals, the association between a foodstuff and getting sick may\nbe made even though the sickness does not begin until hours after the food\nhas been eaten (the Garcia effect).\nThis \"long-distance\" association is more readily made if the food the animal has\neaten is different from the animal's usual diet.\nHumans: (in one study, root-beer Lifesavers), that food becomes a scapegoat: the\npatient is less likely to develop aversions to ordinary foods eaten at the same pre-\ntreatment meal. (Bernstein, Webster, & Bernstein, 1982)\nB. Second fundamental principle of learning is FREQUENCY\nThis supplementary principle increases the likelihood that valid associations (such\nas causal relationships) will be strengthened at the expense of chance associations\nthat do not reflect regularities in the world.\nOther principles of learning and memory are also relevant: attention, elaborative\nprocessing\nE.g., chein-dog\n\nC. CONTINGENCY LEARNING AND BLOCKING\nContiguity and frequency are not sufficient for learning: you need contingency\nbetween the two events you are associating:\nLearning reflects not only positive pairing, but also failures of pairing: that is, if you\nare exposed to A+B, the likelihood that you will learn A-->B depends not only on\nthe frequency of A+B, but also the frequency of 0+Band A+0. In effect, A has to\npredict B more often than not-A does.\nPartial reinforcement\nBlocking: If you already \"know\" that it is a light that predicts shock, adding in a\ntone that is also correlated with the shock will not lead to learningthat tone--\n>shock.\n\nThe Rescorla-Wagner Rule\ndelta V = alpha (lambda - V)\nCompound stimuli and competitive learning\nApplication to blocking\nConditioned inhibition\n\nModifications of Rescorla-Wagner\nDelta Rule in neural-net learning\nRelation to Bayes' Theorem\nSumming up:"
    },
    {
      "category": "Resource",
      "title": "feb17h04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-65-cognitive-processes-spring-2004/789b5c946acccb556e8244fa7471b14a_feb17h04.pdf",
      "content": "9.65 - Cognitive Processes - Spring 2004\nMIT Department of Brain and Cognitive Sciences\nCourse Instructor: Professor Mary C. Potter\n9.65 Feb. 17, 2004 Attention II. HANDOUT\nAssigned reading: Luck, S. J., Vogel, E. K., & Shapiro, K. L. (1996,\nOctober 17). Word meanings can be accessed but not reported\nduring the attentional blink. Nature, 383, 616-618.\nREMINDER: Lab I here on Wednesday.\n1. Visual attention: Treisman & Gelade's theory\na. Feature integration theory\nb. Visual search\nc. Illusory conjunctions\n2. Lateral neglect: Involuntary selective attention\n3. The Psychological Refractory Period (PRP)\n4. The Attentional Blink (AB)\n5. The Theory of Signal Detection\n1. Visual attention: Treisman & Gelade's theory\na. Feature integration theory:\nb. Visual search: simple features versus conjunctions of features:\n-Feature maps\nFeature integration in the brain: The binding problem\nc. Illusory conjunctions\nTreisman & Schmidt (1982).\n\n2. Lateral neglect: Involuntary selective attention\nWhen there is damage to the right parietal lobe, a person tends to ignore stimuli\nappearing in the left visual field, and also ignores the left half of objects or words.\n3. The Psychological Refractory Period (PRP)\na. Basic phenomenon\nb. Three main theories\nc. Pashler's PRP paradigm for studying dual task interference\nd. Why is there a bottleneck?\n\n4. The Attentional Blink (AB)\nSearching for two targets in rapid serial visual presentation (RSVP): e.g., two letters\namong a string of digits, presented at 100 ms/item.\nModel: Chun & Potter (1995): Two stages of processing:\nLuck, Vogel, & Shapiro study:\n5. The Theory of Signal Detection:\nOne important landmark in the understanding of the inferential, bet-placing character\nof perception was the Theory of Signal Detection, sometimes known as TSD. This\ntheory emerged from psychophysical studies of the ability to detect a faint tone or\nlight.\nAbsolute threshold?\nFalse alarms\nThe theory of signal detection [TSD]\nCriterion\n\nSo 2 factors: sensitivity (distance between distributions, d' ), and bias, or criterion\n(beta)."
    }
  ]
}