{
  "course_name": "Mobile Autonomous Systems Laboratory",
  "course_description": "MASLab (Mobile Autonomous System Laboratory), also known as 6.186, is a robotics contest. The contest takes place during MIT’s Independent Activities Period and participants earn 6 units of P/F credit and 6 Engineering Design Points. Teams of three to four students have less than a month to build and program sophisticated robots which must explore an unknown playing field and perform a series of tasks.\nMASLab provides a significantly more difficult robotics problem than many other university-level robotics contests. Although students know the general size, shape, and color of the floors and walls, the students do not know the exact layout of the playing field. In addition, MASLab robots are completely autonomous, or in other words, the robots operate, calculate, and plan without human intervention. Finally, MASLab is one of the few robotics contests in the country to use a vision based robotics problem.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Mechanical Engineering",
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Mechanical Engineering"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 15 sessions for 4 weeks, 2 hours / session\n\nCourse Policies\n\nCourse Credit\n\nMASLab is a for-credit course with 6 units of P/F and 6 engineering design points. Only passing teams with passing members will be allowed to keep their equipment (except for the computer). There are a number of requirements to receive a passing grade, including:\n\nAdequate effort and time invested in MASLab\n\nAttend mandatory meetings/events (see\ncalendar\n)\n\nMajority of work in lab\n\nCompletion of \"\ncheckpoints\n\"\n\nMake\ndaily lab entries\n(few sentences minimum)\n\nSubmit final report (5-10 pages per team)\n\nHelp tidy workshop on\nyour team's turn\n\nHelp final cleanup on lab cleanup day\n\nThese requirements are not intended to be an undue burden, but rather to ensure a quality educational experience and that your work is preserved for use by future students.\n\nDamaged Equipment/Tools\n\nMASLab uses a lot of expensive hardware, and cannot absorb the cost of replacing equipment that is broken. To encourage students to exercise care in their use of the equipment, including both general-use tools and kit contents, students will be expected to pay for replacements. The staff knows that accidents happen, and consequently MASLab will, at the staff's discretion, split the cost of a first replacement part with the team.\n\nPlease note that you are responsible for keeping your valuables safe at all times. MASLab parts are expensive. The staff does not take responsibility for stolen parts, and teams will be fined sensor points if a staff member discovers parts unattended.\n\nCollaboration\n\nCollaboration, with citation when appropriate, is highly encouraged.\n\nMentors And Checkoffs\n\nOver the course, we'll be looking for the following milestones from you:\n\nActivity: orcpad hello world\n\nActivity: sensor edge or bump detection\n\nActivity: feature detection\n\nCheckpoint One: find red ball and indicate on the bot client\n\nDesign Review\n\nCheckpoint Two: score a point\n\nParticipation in mock contest one\n\nParticipation in mock contest two\n\nParticipation in final contest\n\nDaily wiki entries\n\nFinal paper\n\nLab clean-up\n\nAny staff member can check off/witness you for these milestones, and we're all here to help. However, each team also has a staff member as their mentor. A mentor's job is to keep up with their teams' progress, help answer questions, and make sure teams don't fall behind.\n\nDaily Wiki Guidelines\n\nWrite a few sentences about what your team did - problems encountered, tasks accomplished, lessons learned - and/or plans for the next day(s). You don't need to write a lot, but take some time to reflect as a team on your day. If you have specific problems or suggestions for the staff, you may include them in your journal, but please also tell/email the staff as well so we can address the issue.\n\nLab Cleanup Schedule\n\nTeams will take turns to clean up lab at the end of the day (putting tools away, throwing away trash). Everyone should keep their lab area clean, so this should be a quick job. Teams may switch clean up days (just let the staff know). Not helping to clean up on your team's day may result in loss of sensor points and/or failure from the course.\n\nHere's the general list of tasks for clean up (talk to staff for more specifics):\n\nLook around lab and return tools to the workshop\n\nPut tools around the workshop in their proper places (especially return drill bits to holders)\n\nPut hand drills, all drill bits and containers of nuts and bolts on the cart in the back of lab\n\nDust off / vacuum workshop tables\n\nSweep floor\n\nUnplug scroll saw and drill press and put the orange boxes around the cord plugs, with locks",
  "files": [
    {
      "category": "Resource",
      "title": "basicvision.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/e15ab92bc9f5a90e80fcec56730f32f6_basicvision.pdf",
      "content": "Basic Vision\nJanuary 5, 2005\n\nAgenda\nColor and colorspaces\nNumbers and Java\nFeature detection\n\nWhat are Colors?\nFrequencies are one dimensional...\nBut human\nperception of color\nis not!\nhttp://www2.ncsu.edu/scivis/lessons/colormodels/color_models2.html\n\nHumans and Vision\nWe use cones to detect red, green, and blue\nSo computer monitors use the same, with\none byte per channel (RGB).\nimage 640x480 = 900 KB !\ncomputers can cheat...\n...like our cameras:\ninterpolate pixel values\n\nColorspaces\nRGB good for light\nCYMK good for pigment\nwikipedia\n... but both mix color, tint, and brightness\n\nMaslab Colorspace: HSV\nSaturation (amount\n(c) wikipedia\nValue (amount of light\nHue (color):\n\nof color)\n\nand dark)\n\nUsing the colorspace\nWe provide the code to convert to HSV\nFor hue: 360 degrees mapped to 0 to 255\nRed is both 0 and 255!\nWhite is low saturation, but can have any\nhue.\nBlack is high value, but can have any hue.\n\nTips on Differentiating Colors\nGlobally define thresholds\nSelf-calibrate for different lights\nUse the gimp/bot client on real images\n\nHow HSV values are stored\nUses Hexadecimal (base 16)\n1,2,3,4,5,6,7,8,9,A,B,C,D,E,F,10,11,12...\n0x12 = 18\nA color is four bytes = 8 hexadecimal\nnumbers\nAlpha\nHue\nSaturation\nValue\n\nManipulating HSV values\nUse masks to pick out parts:\n0x12345678 & 0x00FF0000 = 0x00340000\nShift to move parts around:\n0x12345678 >> 8 = 0x00123456\nExample: hue = (X >> 16) & 0xFF\n\nA note on java...\nAll java types are signed\nA byte ranges from -128 to 127\nCoded in two's complement: to change sign,\nflip every bit and add one\nDon't forget higher order bits\n(int) 0x0000FF00 = (int) 0xFF00\n(int) ((byte) 0xFF) = (int) 0xFFFFFFFF\nWatch out for shifts\n0xFD000000 >> 8 = 0xFFFD0000\n\nExample\nHow about\nint v = image.getPixel(25,25); // v = 0x8AC12390\nbyte hue = (v >> 16) & 0xFF //hue = 0xC1\nif (hue > 200)\nfoundRedBall();\n\nSolution\nUse\nint v = image.getPixel(25,25); // v = 0x8AC12390\nint hue = (v >> 16) & 0xFF //hue = 0xC1\nif (hue > 200)\nfoundRedBall();\n\nPerformance...\nGetting an image performs a copy\nInt[] = bufferedImage.getRGB(...)\nGetting a pixel performs a multiplication\nint v = bufferedImage.RGB(x,y)\noffset = y*width + x\nMemory in rows, not columns...so go\nacross rows and then down columns\n\nFeature Detection...\nand other Concepts\n\nMaslab Features\nRed balls\nYellow Goals\nBlue line\nBlue ticks\nBar codes\n\nBlue line ideas\nSearch for 'n' wall-blue pixels in a column\nMake sure there's wall-white below?\nCandidate voting\nin each column, list places where you think\nline might be\nfind shortest left to\nright path through\ncandidates\n\nBar code ideas\nLook for green and black\nIs there not-white under the blue line?\nCheck along a column to determine\ncolors\nRANdom SAmple Consensus\n(RANSAC)\nPick random pixels within bar code\nAre they black or green?\n\nFinding a single color object\nMatched filter: convolve the image with a\nmatched filter\ncompuatationally expensive\ndon't know the scale\n\nOther things to try\nLook for a red patch\nSet center to current coordinates\nLoop:\nFind the new center based on\npixels within d of the old center\nEnlarge d and recompute\nStop when increasing d doesn't\nadd enough red pixels\n\nOr try fitting a rectangle\nScan image for a yellow patch\nIn each direction, loop:\nMake rectangle bigger\nIf it doesn't add enough new\nyellow pixels, then stop\n\nEstimating distance\nCloser objects are bigger\nCloser objects are lower\n\nFeature-based processing\nImage processing for navigation\nIn each frame, list 'corners' - such as the\nblue tick marks\nMatch corners from one image to the next\nEstimate the rigid 3D transformations to\nthat best map the corners\n\nReminders\nBasics to get you started\n(cool advanced stuff on Monday)\nTry out your own algorithms! Have fun!\nMust prune out silly solutions:\nNoise\nOcclusion\nAcute viewing angles\nOverly large thresholds\n\nUpdates on Rules\nRobot must fit in tub\nThere will be yellow field goal posts over the\ngoals (above the yellow line)\nUsing outside parts: cost = how much it would\ncost another team to have similar functionality\nAlso, don't forget to refresh wiki periodically\nduring the day and check for updates\n\nYour job for today\nFinish yesterday's activity\nRead a barcode\nWork on Friday's check point"
    },
    {
      "category": "Resource",
      "title": "control.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/e9a9513ff1f5ba25866c372a721ff6a4_control.pdf",
      "content": "Control for Mobile Robots\nChristopher Batten\nMaslab IAP Robotics Course\nJanuary 7, 2005\n\nBuilding a control system for\na mobile robot can be very challenging\nMobile robots are very complex and involve\nmany interacting components\nMechanical\nElectrical\nSoftware\nYour control system must integrate these components\nso that your robot can achieve the desired goal\n\nBuilding a control system for\na mobile robot can be very challenging\nJust as you must carefully design your\nrobot chassis you must carefully design\nyour robot control system\n- How will you debug and test your robot?\n- What are the performance requirements?\n- Can you easily improve aspects of your robot?\n- Can you easily integrate new functionality?\n\nBasic primitive\nof a control system is a behavior\nBehaviors should be well-defined,\nself-contained, and independently testable\nTurn right 90°\nGo forward until reach obstacle\nCapture a ball\nExplore playing field\n\nKey objective is to compose behaviors\nso as to achieve the desired goal\n\nOutline\n- High-level control system paradigms\n- Model-Plan-Act Approach\n- Behavioral Approach\n- Finite State Machine Approach\n- Low-level control loops\n- PID controller for motor velocity\n- PID controller for robot drive system\n\nModel-Plan-Act Approach\n1. Use sensor data to create model of the world\n2. Use model to form a sequence of behaviors\nwhich will achieve the desired goal\n3. Execute the plan\nModel\nActuators\nSensors\nPlan\nAct\nEnvironment\n\nExploring the playing field\nusing model-plan-act approach\nRed dot is the mobile robot\nwhile the blue line is the mousehole\n\nExploring the playing field\nusing model-plan-act approach\nRobot uses sensors to create local map of the\nworld and identify unexplored areas\n\nExploring the playing field\nusing model-plan-act approach\nRobot moves to midpoint of\nunexplored boundary\n\nExploring the playing field\nusing model-plan-act approach\nRobot performs a second sensor scan and\nmust align the new data with the global map\n\nExploring the playing field\nusing model-plan-act approach\nRobot continues to explore\nthe playing field\n\nExploring the playing field\nusing model-plan-act approach\nRobot must recognize when it starts to\nsee areas which it has already explored\n\nFinding a mousehole\nusing model-plan-act approach\nGiven the global map,\nthe goal is to find the mousehole\n\nFinding a mousehole\nusing model-plan-act approach\nTransform world into configuration space\nby convolving robot with all obstacles\n\nFinding a mousehole\nusing model-plan-act approach\nDecompose world into convex cells\nTrajectory within any cell is free of obstacles\n\nFinding a mousehole\nusing model-plan-act approach\nConnect cell edge midpoints and centroids to\nget graph of all possible paths\n\nFinding a mousehole\nusing model-plan-act approach\nUse an algorithm (such as the A*\nalgorithm) to find shortest path to goal\n\nFinding a mousehole\nusing model-plan-act approach\nThe choice of cell decomposition can\ngreatly influence results\n\nAdvantages and disadvantages\nof the model-plan-act approach\n- Advantages\n- Global knowledge in the model enables optimization\n- Can make provable guarantees about the plan\n- Disadvantages\n- Must implement all functional units before any testing\n- Computationally intensive\n- Requires very good sensor data for accurate models\n- Models are inherently an approximation\n- Works poorly in dynamic environments\n\nBehavioral Approach\nSensors\nActuators\nAs in simple biological systems,\nbehaviors directly couple sensors and actuators\nHigher level behaviors are layered\non top of lower level behaviors\nEnvironment\nBehavior C\nBehavior B\nBehavior A\n\nTo illustrate the behavioral approach\nwe will consider a simple mobile robot\nBall Gate\nBump Switches\nInfrared Rangefinders\nBall Detector Switch\nCamera\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nMotors\nCruise behavior simply moves robot forward\nCruise\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nInfrared\nS\nMotors\nLeft motor speed inversely proportional to left IR range\nRight motor speed inversely proportional to right IR range\nIf both IR < threshold stop and turn right 120 degrees\nSubsumption\nCruise\nAvoid\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nEscape\nInfrared\nBump\nS\nS\nMotors\nEscape behavior stops motors,\nbacks up a few inches, and turns right 90 degrees\nCruise\nAvoid\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nEscape\nll\nInfrared\nBump\nCamera\nS\nS\nS\nCruise\nAvoid\nTrack Ba\nMotors\nThe track ball behavior adjusts the\nmotor differential to steer the robot towards the ball\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nEscape\nll\nll\nInfrared\nBump\nCamera\nBall\nS\nS\nS\nBall\nGate\nCruise\nAvoid\nTrack Ba\nHold Ba\nSwitch\nMotors\nHold ball behavior simply closes ball gate\nwhen ball switch is depressed\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nEscape\nll\nll\nTrack Goal\nInfrared\nBump\nCamera\nBall\nS\nS\nS\nS\nS\nBall\nGate\nCruise\nAvoid\nTrack Ba\nHold Ba\nSwitch\nMotors\nThe track goal behavior opens the ball gate and\nadjusts the motor differential to steer the robot\ntowards the goal\n\nLayering simple behaviors can create\nmuch more complex emergent behavior\nEscape\nll\nll\nTrack Goal\nInfrared\nBump\nCamera\nBall\nS\nS\nS\nS\nS\nBall\nGate\nCruise\nAvoid\nTrack Ba\nHold Ba\nSwitch\nMotors\nAll behaviors are always running in parallel and an\narbiter is responsible for picking which behavior can\naccess the actuators\n\nAdvantages and disadvantages\nof the behavioral approach\n- Advantages\n- Incremental development is very natural\n- Modularity makes experimentation easier\n- Cleanly handles dynamic environments\n- Disadvantages\n- Difficult to judge what robot will actually do\n- No performance or completeness guarantees\n- Debugging can be very difficult\n\nModel-plan-act fuses sensor data,\nwhile behavioral fuses behaviors\nModel\nPlan\nAct\nEnvironment\nModel-Plan-Act\n(Fixed Plan of Behaviors)\nBehavioral\n(Layered Behaviors)\nEnvironment\nBehavior C\nBehavior B\nBehavior A\n\nModel-plan-act fuses sensor data,\nwhile behavioral fuses behaviors\nEnvironment\nModel\nPlan\nAct\nEnvironment\nModel-Plan-Act\n(Sensor Fusion)\n(Behavior Fusion)\nBehavior C\nBehavior B\nBehavior A\nBehavioral\n\nFinite State Machines offer another\nalternative for combining behaviors\nFwd behavior moves robot\nstraight forward a given distance\nFwd\n(dist)\nTurnR\n(deg)\nTurnR behavior turns robot to the\nright a given number of degrees\n\nFinite State Machines offer another\nTurnR\n(90°)\nalternative for combining behaviors\nFwd\n(2ft)\nFwd\n(2ft)\ncan easily link them together to create\nEach state is just a behavior and we\nan open loop control system\n\nFinite State Machines offer another\nTurnR\n(90°)\nalternative for combining behaviors\nFwd\n(2ft)\nFwd\n(2ft)\ncan easily link them together to create\nEach state is just a behavior and we\nan open loop control system\n\nFinite State Machines offer another\nTurnR\n(90°)\nalternative for combining behaviors\nFwd\n(2ft)\nFwd\n(2ft)\ncan easily link them together to create\nEach state is just a behavior and we\nan open loop control system\n\nFinite State Machines offer another\nTurnR\n(90°)\nalternative for combining behaviors\nFwd\n(2ft)\nFwd\n(2ft)\nEach state is just a behavior and we\ncan easily link them together to create\nan open loop control system\n\nFinite State Machines offer another\nTurnR\n(90°)\nalternative for combining behaviors\nFwd\n(2ft)\nFwd\n(2ft)\nSince the Maslab playing field is\nunknown, open loop control systems\nhave no hope of success!\n\nFinite State Machines offer another\nTurnR\n(45°)\nalternative for combining behaviors\nFwd\n(1ft)\nClosed loop finite state machines use\nsensor data as feedback to make\nstate transitions\nNo Obstacle\nObstacle\nWithin 2ft\nNo\nObstacle\nObstacle\nWithin 2ft\n\nFinite State Machines offer another\nTurnR\n(45°)\nalternative for combining behaviors\nFwd\n(1ft)\nNo Obstacle\nObstacle\nWithin 2ft\nNo\nObstacle\nObstacle\nWithin 2ft\nClosed loop finite state machines use\nsensor data as feedback to make\nstate transitions\n\nFinite State Machines offer another\nTurnR\n(45°)\nalternative for combining behaviors\nFwd\n(1ft)\nNo Obstacle\nObstacle\nWithin 2ft\nNo\nObstacle\nObstacle\nWithin 2ft\nClosed loop finite state machines use\nsensor data as feedback to make\nstate transitions\n\nFinite State Machines offer another\nTurnR\n(45°)\nalternative for combining behaviors\nFwd\n(1ft)\nNo Obstacle\nObstacle\nWithin 2ft\nNo\nObstacle\nObstacle\nWithin 2ft\nClosed loop finite state machines use\nsensor data as feedback to make\nstate transitions\n\nFinite State Machines offer another\nTurnR\n(45°)\nalternative for combining behaviors\nFwd\n(1ft)\nNo Obstacle\nObstacle\nWithin 2ft\nNo\nObstacle\nObstacle\nWithin 2ft\nClosed loop finite state machines use\nsensor data as feedback to make\nstate transitions\n\nImplementing a\nFSM in Java\nTurnR\n(45°)\nFwd\n(1ft)\nNo Obstacle\nObstacle\nWithin 2ft\n// State transitions\nswitch ( state ) {\ncase States.Fwd_1 :\nif ( distanceToObstacle() < 2 )\nstate = TurnR_45;\nbreak;\ncase States.TurnR_45 :\nif ( distanceToObstacle() >= 2 )\nstate = Fwd_1;\nbreak;\n}\n// State outputs\nswitch ( state ) {\ncase States.Fwd_1 :\nmoveFoward(1); break;\ncase States.TurnR_45 :\nturnRight(45); break;\n}\nObstacle\nWithin 2ft\n\n-\nas parameterized\nfunctions\n-\nstatement handles\nstate transitions\n-\nstatement executes\nbehaviors associated\nwith each state\n-\nvariables\nImplementing a\nFSM in Java\n// State transitions\nswitch ( state ) {\ncase States.Fwd_1 :\nif ( distanceToObstacle() < 2 )\nstate = TurnR_45;\nbreak;\ncase States.TurnR_45 :\nif ( distanceToObstacle() >= 2 )\nstate = Fwd_1;\nbreak;\n}\n// State outputs\nswitch ( state ) {\ncase States.Fwd_1 :\nmoveFoward(1); break;\ncase States.TurnR_45 :\nturnRight(45); break;\n}\nImplement behaviors\nFirst switch\nSecond switch\nUse enums for state\n\nFinite State Machines offer another\nTurn\nTo\nOpen\nalternative for combining behaviors\nFwd\nUntil\nObs\nCan also fold closed loop feedback\ninto the behaviors themselves\n\nSimple finite state machine\nto locate red balls\nScan\nWander\n(20sec)\nFwd\n(1ft)\nAlign\nBall\nTurnR\nStop\nNo Balls\nFound\nBall\nLost\nBall\nBall\n< 1ft\nBall\n> 1ft\n\nSimple finite state machine\nto locate red balls\nScan\nWander\n(20sec)\nFwd\n(1ft)\nAlign\nBall\nTurnR\nStop\nNo Balls\nFound\nBall\nLost\nBall\nBall\n< 1ft\nBall\n> 1ft\nObstacle < 2ft\n\nObstacle < 2ft\nTo debug a FSM control system\nverify behaviors and state transitions\nScan\nWander\n(20sec)\nFwd\n(1ft)\nAlign\nBall\nTurnR\nStop\nNo Balls\nFound\nBall\nLost\nBall\nBall\n< 1ft\nBall\n> 1ft\nWhat if robot\nhas trouble\ncorrectly\napproaching\nthe ball?\n\nObstacle < 2ft\nTo debug a FSM control system\nverify behaviors and state transitions\nScan\nWander\n(20sec)\nFwd\n(1ft)\nAlign\nBall\nTurnR\nStop\nNo Balls\nFound\nBall\nLost\nBall\nBall\n< 1ft\nBall\n> 1ft\nIndependently\nverify Align\nBall and Fwd\nbehaviors\n\nObstacle < 2ft\nImprove FSM control system by replacing\na state with a better implementation\nScan\nWander\n(20sec)\nFwd\n(1ft)\nAlign\nBall\nTurnR\nStop\nNo Balls\nFound\nBall\nLost\nBall\nBall\n< 1ft\nBall\n> 1ft\nCould replace\nrandom wander with\none which is biased\ntowards unexplored\nregions\n\nImprove FSM control system by replacing\na state with a better implementation\nWhat about integrating camera code into wander\nbehavior so robot is always looking for red balls?\n- Image processing is\ntime consuming so\nmight not check for\nobstacles until too late\n- Not checking camera\nwhen rotating\n- Wander behavior\nbegins to become\nmonolithic\nball = false\nturn both motors on\nwhile ( !timeout and !ball )\ncapture and process image\nif ( red ball ) ball = true\nread IR sensor\nif ( IR < thresh )\nstop motors\nrotate 90 degrees\nturn both motors on\nendif\nendwhile\n\nMulti-threaded\nObstacle\nSensors\nThread\nImage\nCompute\nThread\nController\nFSM\nfinite state machine control systems\nCamera\nShort IR\n+ Bump\nDrive Motors\n\nMulti-threaded\nObstacle\nSensors\nThread\nImage\nCompute\nThread\nController\nFSM\nfinite state machine control systems\nCamera\nShort IR\n+ Bump\nDrive Motors\n\nMulti-threaded\nSensor\nStalk\nThread\nObstacle\nSensors\nThread\nImage\nCompute\nThread\nController\nFSM\nfinite state machine control systems\nDrive Motors\nCamera\nShort IR\n+ Bump\nStalk\nServo\nStalk\nSensors\n\nMapping\nThread\nSensor\nStalk\nThread\nObstacle\nSensors\nThread\nImage\nCompute\nThread\nController\nFSM\nMulti-threaded\nfinite state machine control systems\nDrive Motors\nCamera\nShort IR\n+ Bump\nStalk\nServo\nStalk\nSensors\n\nFSMs in Maslab\nfor your Maslab robotic control system\nFinite state machines can combine the\nmodel-plan-act and behavioral\napproaches and are a good starting point\n\nOutline\n- High-level control system paradigms\n- Model-Plan-Act Approach\n- Behavioral Approach\n- Finite State Machine Approach\n- Low-level control loops\n- PID controller for motor velocity\n- PID controller for robot drive system\n\nProblem: How do we set\na motor to a given velocity?\nOpen Loop Controller\n-\nsome kind of relationship\nbetween velocity and voltage\n-\ndrive surface could result in\nincorrect velocity\nMotor\nVelocity\nTo Volts\nDesired\nVelocity\nActual\nVelocity\nUse trial and error to create\nChanging supply voltage or\n\nProblem: How do we set\nController\na motor to a given velocity?\nClosed Loop Controller\n-\nvoltage sent to the motor so\nthat the actual velocity equals\nthe desired velocity\n-\nmeasure actual velocity\nMotor\nDesired\nVelocity\nActual\nVelocity\nAdjusted\nVoltage\nFeedback is used to adjust the\nCan use an optical encoder to\n\nStep response\nwith no controller\nTime (sec)\nVelocity\n-\n-\nseveral differential\nequations\n-\n-\nMotor\nVelocity\nTo Volts\nDesired\nVelocity\nActual\nVelocity\nNaive velocity to volts\nModel motor with\nSlow rise time\nStead-state offset\n\nStep response\nwith proportional controller\nTime (sec)\nVelocity\n(\n)\nact\ndes\nP\ndes\nV\nV\nK\nV\nX\n-\n⋅\n+\n=\n-\n-\n-\n-\nController\nMotor\nDesired\nVelocity\n(Vdes)\nActual\nVelocity\n(Vact)\nAdjusted\nVoltage (X)\nBig error big = big adj\nFaster rise time\nOvershoot\nStead-state offset\n\nStep response\nwith proportional-derivative controller\nTime (sec)\nVelocity\ndt\nK\nV\nX\nD\nP\ndes\n)\n(\n)\n(\n-\n+\n=\n-\ncounteracts proportional\nterm slowing adjustment\n-\n-\nController\nMotor\nDesired\nVelocity\n(Vdes)\nActual\nVelocity\n(Vact)\nAdjusted\nVoltage (X)\nt\nde\nt\ne\nK\nWhen approaching desired\nvelocity quickly, de/dt term\nFaster rise time\nReduces overshoot\n\nStep response\nwith proportional-integral controller\nTime (sec)\nVelocity\n∫\n-\n+\n=\ndt\nK\nK\nV\nX\nI\nP\ndes\n)\n(\n)\n(\n-\naccumulated error\n-\nController\nMotor\nDesired\nVelocity\n(Vdes)\nActual\nVelocity\n(Vact)\nAdjusted\nVoltage (X)\nt\ne\nt\ne\nIntegral term eliminates\nIncreases overshoot\n\nStep response\nwith PID controller\nTime (sec)\nVelocity\ndt\nK\nK\nV\nX\nD\nI\nP\ndes\n)\n(\n)\n(\n)\n(\n-\n+\n+\n=\n∫\nController\nMotor\nDesired\nVelocity\n(Vdes)\nActual\nVelocity\n(Vact)\nAdjusted\nVoltage (X)\nt\nde\ndt\nt\ne\nt\ne\nK\n\nChoosing and tuning\na controller\nController\nMotor\nDesired\nVelocity\n(Vdes)\nActual\nVelocity\n(Vact)\nAdjusted\nVoltage (X)\n- Use the simplest controller which\nachieves the desired result\n- Tuning PID constants is very tricky,\nespecially for integral constants\n- Consult the literature for more\ncontroller tips and techniques\n\nProblem: How do we make\nour robots go in a nice straight line?\nTrajectory\nMotor Velocities vs Time\nModel differential drive with slight motor mismatch\nWith an open loop controller, setting motors to same velocity\nresults in a less than straight trajectory\n\nProblem: How do we make\nour robots go in a nice straight line?\nTrajectory\nMotor Velocities vs Time\nWith an independent PID controller for each motor,\nsetting motors to same velocity results in a straight trajectory\nbut not necessarily straight ahead!\n\nProblem: How do we make\nour robots go in a nice straight line?\n- Need to couple drive motors\n- Use low-level PID controllers\nto set motor velocity and a\nhigh-level PID controller to\ncouple the motors\n- Use one high-level PID\ncontroller which uses\nodometry or even image\nprocessing to estimate error\n\nProblem: How do we make\nour robots go in a nice straight line?\nNeed to couple drive motors\n- Use low-level PID controllers to\nset motor velocity and a high-\nlevel PID controller to couple\nthe motors\nerror\nangle\n- Use one high-level PID\ncontroller which uses odometry\nor even image processing to\nestimate error\n\nTake Away Points\n- Integrating feedback into your control system\n\"closes the loop\" and is essential for creating\nrobust robots\n- Simple finite state machines make a solid\nstarting point for your Maslab control systems\n- Spend time this weekend designing behaviors\nand deciding how you will integrate these\nbehaviors to create your control system"
    },
    {
      "category": "Resource",
      "title": "mapping.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/d2642bd410197d7bcf03d69a420ff07a_mapping.pdf",
      "content": "Mapping and Navigation\nJanuary 6th, 2005\nEdwin Olson\n\nWhy build a map?\n- Time!\n- Playing field is big, robot is slow\n- Driving around perimeter takes a minute!\n- Scoring takes time... often ~20 seconds\nto \"line up\" to a mouse hole.\n- Exploration round gives advantage to\nrobots that can map\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nWhat is a feature?\n- An object/structure in\nthe environment that\nwe will represent in\nour map\n- Something that we\ncan observe multiple\ntimes, from different\nlocations\nBunker Hill Monument\n(Image courtesy of H. Oestreich and stock.xchng)\n\nWhat is an Observation?\nWhere do we get\nobservations from?\nCamera\nRange/bearing to ticks and\nlandmarks\nCorners detected from\ncamera, range finders\nFor now, let's assume we\nget these observations plus\nsome noise estimate.\nrobot\nuncertainty ellipse\nwmaximum likelihood\nwestimate\nan observation\n\nData Association\nThe problem of recognizing that an object\nyou see now is the same one you saw\nbefore\nHard for simple features (points, lines)\nEasy for \"high-fidelity\" features (barcodes,\nbunker hill monuments)\nWith perfect data association, most\nmapping problems become \"easy\"\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nMetrical Maps\nTry to estimate actual\nlocations of features and\nrobot\n\"The robot is at (5,3) and\nfeature 1 is at (2,2)\"\nBoth \"occupancy grid\" and\ndiscrete feature approaches.\nRelatively hard to build\nMuch more complete\nrepresentation of the world\nDen\nBath\nFoyer\nBedroom\nKitchen\nDining\n\nMetrical Maps\nState Estimation\nEstimate discrete quantities: \"If we fit a line to the\nwall, what are its parameters y=mx+b?\"\nOften use probabilistic machinery, Kalman filters\nOccupancy Grid\nDiscretize the world. \"I don't know what a wall is, but\ngrids 43, 44, and 45 are impassable.\"\n\nBayesian Estimation\n- Represent unknowns with\nprobability densities\n- Often, we assume the\ndensities are Gaussian\n- Or we represent arbitrary\ndensities with particles\n- We won't cover this today\n-5\n-4\n-3\n-2\n-1\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nNormal Distribution\n/\n)\n(\n)\n(\nσ\nμ\nπσ\n-\n-\n=\nx\ne\nx\nP\n-2.5\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0.2\n0.4\n0.6\n0.8\n\nBayesian Data Fusion\n- Example: Estimating where\nJill is standing:\n- Alice says: x=2\n- We think σ2 =2; she wears\nthick glasses\n- Bob says: x=0\n- We think σ2 =1; he's pretty\nreliable\n- How do we combine these\nmeasurements?\n-5\n-4\n-3\n-2\n-1\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nA\nB\n\nSimple Kalman Filter\n-5\n-4\n-3\n-2\n-1\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nA\nB\nboth\nB\nA\nσ\nσ\nσ\n+\n=\nB\nA\nA\nB\nB\nA\nσ\nσ\nσ\nμ\nσ\nμ\nμ\n+\n+\n=\n- Answer: algebra (and a\nlittle calculus)!\n- Compute mean by\nfinding maxima of the log\nprobability of the product\nPAPB.\n- Variance is messy;\nconsider case when\nPA=PB=N(0,1)\n- Try deriving these\nequations at home!\n\nKalman Filter Example\n- We now think Jill is\nat:\n- x=0.66\n- σ2 =0.66\n-5\n-4\n-3\n-2\n-1\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nA\nB\nboth\n\nKalman Filter: Properties\nYou incorporate sensor observations one at a\ntime.\nEach successive observation is the same\namount of work (in terms of CPU).\nYet, the final estimate is the global optimal\nsolution.\nThe Kalman Filter is an optimal,\nrecursive estimator.\n\nKalman Filter: Properties\nObservations always reduce the\nuncertainty.\n\nKalman Filter\n- Now Jill steps forward\none step\n- We think one of Jill's\nsteps is about 1 meter,\nσ2 =0.5\n- We estimate her position:\n- x=xbefore+xchange\n- σ2 = σbefore\n2 + σchange\n- Uncertainty increases\n-5\n-4\n-3\n-2\n-1\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nBefore\nAfter\n\nState Vector\nWe're going to estimate robot location and\norientation (xr, xy, xt), and feature locations (fnx,\nfny).\nWe could try to estimate each of these variables\nindependently\nBut they're correlated!\nx = [ xr xy xt f1x f1y f2x f2y ... fnx fny ]T\n\nState Correlation/Covariance\nWe observe features relative to the robot's\ncurrent position\nTherefore, feature location estimates covary (or\ncorrelate) with robot pose.\nWhy do we care?\nWe need to track covariance so we can correctly\npropagate new information:\nRe-observing one feature gives us information about\nrobot position, and therefore also all other features.\n\nCorrelation/Covariance\nIn multidimensional Gaussian\nproblems, equal-probability\ncontours are ellipsoids.\nShoe size doesn't affect\ngrades:\nP(grade,shoesize)=P(grade)P(shoesize)\nStudying helps grades:\nP(grade,studytime)!=P(grade)P(studytime)\nWe must consider P(x,y) jointly,\nrespecting the correlation!\nIf I tell you the grade, you learn\nsomething about study time.\nExam score\nTime spent studying\nShoe Size\n\nKalman Filters and Multi-Gaussians\nWe use a Kalman filter to estimate the\nwhole state vector jointly.\nState vector has N elements.\nWe don't have a scalar variance σ2, we\nhave NxN covariance matrix Σ.\nElement (i,j) tells us how the uncertainties in\nfeature i and j are related.\nx = [ xr xy xt f1x f1y f2x f2y ... fnx fny ]T\n\nKalman Filters and Multi-Gaussians\nKalman equations tell us how to\nincorporate observations\nPropagating effects due to correlation\nKalman equations tell us how to add new\nuncertainty due to robot moving\nWe choose a Gaussian noise model for this\ntoo.\n\nSystem Equations (EKF)\n- Consider range/bearing measurements,\ndifferentially driven robot\n- Let xk=f(xk-1,uk-1, wk-1) u=control inputs, w=noise\n- Let zk=h(xk,vk)\nv=noise\n⎟\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎜\n⎝\n⎛\n+\n+\n=\n+\n+\n+\n=\n+\n+\n+\n=\n=\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nw\nu\nw\nw\nu\ny\ny\nw\nw\nu\nx\nx\nf\nd\nd\nd\nd\n'\n)\nsin(\n)\n(\n'\n)\ncos(\n)\n(\n'\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎝\n⎛\n+\n-\n-\n-\n=\n+\n-\n+\n-\n=\n=\nθ\nθ\nθ\nv\nx\nx\nx\ny\ny\nz\nv\ny\ny\nx\nx\nz\nh\nr\nf\nr\nf\nd\nr\nf\nr\nf\nd\n)\n,\n(\narctan\n]\n)\n(\n)\n[(\n/\n\nEKF Update Equations\n-\nTime update:\n-\nx'=f(x,u,0)\n-\nP=APAT+WQWT\n-\nObservation\n-\nK=PHT(HPHT + VRVT)-1\n-\nx'=x+K(z-h(x,0))\n-\nP=(I-KH)P\n-\nP is your covariance matrix\n-\nThey look scary, but once you compute your\nJacobians, it just works!\n-\nA=df/dx\nW=df/dw\nH=dh/dx\nV=dh/dv\n-\nStaff can help... (It's easy except for the atan!)\n⎟\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎜\n⎝\n⎛\n+\n+\n=\n+\n+\n+\n=\n+\n+\n+\n=\n=\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nw\nu\nw\nw\nu\ny\ny\nw\nw\nu\nx\nx\nf\nd\nd\nd\nd\n'\n)\nsin(\n)\n(\n'\n)\ncos(\n)\n(\n'\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n+\n-\n-\n-\n=\n+\n-\n+\n-\n=\n=\nθ\nθ\nθ\nv\nx\nx\nx\ny\ny\nz\nv\ny\ny\nx\nx\nz\nh\nr\nf\nr\nf\nd\nr\nf\nr\nf\nd\n)\n,\n(\narctan\n]\n)\n(\n)\n[(\n/\n\nEKF Jacobians\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎛\n=\n=\n+\n+\n=\n+\n+\n+\n=\n+\n+\n+\n=\n=\n'\n'\n'\n)\nsin(\n)\n(\n'\n)\ncos(\n)\n(\n'\ny\ny\nx\nx\nw\nu\nw\nw\nu\ny\ny\nw\nw\nu\nx\nx\nf\nd\nd\nd\nd\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nθ\nr\nf\ny\nr\nf\nx\nr\nf\nr\nf\ny\ny\nd\nx\nx\nd\ny\ny\nx\nx\nd\n-\n=\n-\n=\n-\n+\n-\n=\n/\n]\n)\n(\n)\n[(\n)\ncos(\n)\nsin(\nθ\nθ\nd\nd\nu\nu\nA\n-\n=\n)\ncos(\n)\nsin(\n)\nsin(\n)\ncos(\nθ\nθ\nθ\nθ\nd\nd\nu\nu\nW\n-\n=\nθ\nσ\nσ\nw\nwd\nQ =\n\nEKF Jacobians\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎝\n⎛\n+\n-\n-\n-\n=\n+\n-\n+\n-\n=\n=\nθ\nθ\nθ\nv\nx\nx\nx\ny\ny\nz\nv\ny\ny\nx\nx\nz\nh\nr\nf\nr\nf\nd\nr\nf\nr\nf\nd\n)\n,\n(\narctan\n]\n)\n(\n)\n[(\n/\n2)\n/\n(\n/(\nx\ny d\nd\n+\n=\nλ\nθ\nσ\nσ\nv\nv\nT\nd\nVRV\n=\nx\nx\ny\nx\nx\ny\ny\nx\ny\nx\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nH\n/\n/\n/\n/\n/\n/\n/\n/\nλ\nλ\nλ\nλ\n-\n-\n-\n-\n-\n=\n\nKalman Filter: Properties\nIn the limit, features become highly correlated\nBecause observing one feature gives information\nabout other features\nKalman filter computes the posterior pose, but\nnot the posterior trajectory.\nIf you want to know the path that the robot traveled,\nyou have to make an extra \"backwards\" pass.\n\nKalman Filter: a movie\n\nKalman Filters' Nemesis\n- With N features, update\ntime is O(N2)!\n- For Maslab, N is small.\nWho cares?\n- In the \"real world\", N can\nbe 106.\n- Current research: lower-\ncost mapping methods\n\nNon-Bayesian Map Building\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nOccupancy Grids\n- Another way of\nmapping:\n- Divide the world into a\ngrid\n- Each grid records\nwhether there's\nsomething there or not\n- Use current robot\nposition estimate to fill\nin squares according\nto sensor observations\n\nOccupancy Grids\n- Easy to generate, hard to maintain accuracy\n- Basically impossible to \"undo\" mistakes\n- Occupancy grid resolution limited by the\nrobot's position uncertainty\n- Keep dead-reckoning error as small as possible\n- When too much error has accumulated, save the\nmap and start over. Use older maps for reference?\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nTopological Maps\nTry to estimate how locations are\nrelated\n\"There's an easy (straight) path\nbetween feature 1 and 2\"\nEasy to build, easy to plan paths\nOnly a partial representation of\nthe world\nResulting paths are suboptimal\nKitchen\nDen\nDining\nFoyer\nBedroom\nBath\n\nTopological Maps\nMuch easier than this metrical map stuff.\nDon't even try to keep track of where\nfeatures are. Only worry about\nconnectivity.\n\nTopological Map Example\n2 G\nG\n2 G\nG\n1.6\n0.8\n1.0\n2.3\n0.7\n1.7\n1.9\n1.0\nNote that the way we draw (where we draw the\nnodes) does not contain information.\n\nTopological Map-Building Algorithm\nUntil exploration round ends:\nExplore until we find a previously unseen\nbarcode\nTravel to the barcode\nPerform a 360 degree scan, noting the\nbarcodes, balls, and goals which are visible.\nBuild a tree\nNodes = barcode features\nEdges connect features which are \"adjacent\"\nEdge weight is distance\n\nTopological Maps: Planning\nGraph is easy to do\nprocess!\nIf we're lost, go to\nnearest landmark.\nNodes form a \"highway\"\nCan find \"nearest\" goal,\nfind areas of high ball\ndensity\nA* Search\n2 G\nG\n2 G\nG\n1.6\n0.8\n1.0\n2.3\n0.7\n1.7\n1.9\n1.0\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nData Association\nIf we can't tell when we're reobserving a\nfeature, we don't learn anything!\nWe need to observe the same feature twice to\ngenerate a constraint\n\nData Association: Bar Codes\nTrivial!\nThe Bar Codes have unique IDs;\nread the ID.\n\nData Association: Tick Marks\nThe blue tick marks can be\nused as features too.\nYou only need to reobserve\nthe same feature twice to\nbenefit!\nIf you can track them over\nshort intervals, you can use\nthem to improve your dead-\nreckoning.\n\nData Association: Tick Marks\nIdeal situation:\nLots of tick marks, randomly arranged\nGood position estimates on all tick marks\nThen we search for a rigid-body-\ntransformation that best aligns the points.\n\nData Association: Tick Marks\nFind a rotation that aligns the most tick marks...\nGives you data association for matched ticks\nGives you rigid body transform for the robot!\nRotation+Translation\n\nAttack Plan\n- Motivation: why build a map?\n- Terminology, basic concepts\n- Mapping approaches\n- Metrical\n- State Estimation\n- Occupancy Grids\n- Topological\n- Data Association\n- Hints and Tips\n\nUsing the exploration round\n\nContest day:\n1.\nDuring exploration round, build a map.\n2.\nWrite map to a file.\n3.\nDuring scoring round, reload the map.\n4.\nScore lots of points.\n\nUse two separate applications for explore/score\nrounds.\n\nSaving state to a file will ease testing:\n-\nYou can test your scoring code without having to re-\nexplore\n-\nYou can hand-tweak the state file to create new test\nconditions or troubleshoot.\n\nDebugging map-building algorithms\nYou can't debug what you can't see.\nProduce a visualization of the map!\nMetrical map: easy to draw\nTopological map: draw the graph (using graphviz/dot?)\nDisplay the graph via BotClient\nWrite movement/sensor observations to a file to test\nmapping independently (and off-line)\n\nCourse Announcements\nGyros:\nForgot to mention that your first gyro costs\nZERO sensor points.\nGyro mounting issues: axis of rotation\nLab checkoffs\nOnly a couple checkoffs yesterday\n\nToday's Lab Activities\n\nNo structured activities today\n\nWork towards tomorrow's check-off:\n1.\nRobot placed in playfield\n2.\nFind and approach a red ball.\n3.\nStop.\n\nKeep it simple!\n\nRandom walks are fine!\n\nStatus messages must be displayed on OrcPad or\nBotClient"
    },
    {
      "category": "Resource",
      "title": "mechanical.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/3d97f0b560e89204cc83709e49835a4e_mechanical.pdf",
      "content": "Mechanical Issues\nJanuary 4th, 2005\nAaron Sokoloski\n\nAgenda\nThe Maslab Workshop\nRaw Materials\nOther Materials\nFasteners\nTools\nSafety & Maintenance\nMechanical issues\nMotors\nTechniques\nDesign Principles\nOther resources\n\nThe Maslab Workshop\nGoal: Be able to build a\nsimple robot with the tools\nand materials provided in\nthe Maslab Workshop\n\nRaw Materials\nPegboard\nHardboard\nBaltic Birch Plywood\nSheet Aluminum\nPolycarbonate\nPrototyping Foam\n\nRaw Materials\nPegboard (1/4\" thick)\nGreat for initial testing -\nalready has 1⁄4\" holes on 1\ninch spacing\nUseful in some specific\napplications, generally limited\nCan be cut with anything\nsharper than a butter knife\n\nRaw Materials\nHardboard (1/4\" thick)\nPegboard, without the holes\nBetter for intermediate designs (cheap!)\nHardboard used during development can be\nreplaced with better quality plywood for final\nversion\n\nRaw Materials\nBaltic Birch Plywood (1/4\", 3/8\")\nThe good stuff - strong, looks nice\nA bit slower to cut\nPre-drill holes for wood screws to avoid\ncracking\n\nRaw Materials\nSheet Aluminum (1/16\")\nGreat for smaller structural\nmembers like L-brackets\nBending can increase strength\nEasy holes with hand punch\nQuick cuts on shear\n\nRaw Materials\nPolycarbonate (1/8\", 1/4\")\nLooks really cool\nNot too hard to machine, unless it gets hot\nand softens\n1/8\" can be sheared and hand punched\n1/4\" can be cut using scroll saw and drilled\nGood for mounting gears\n\nRaw Materials\nPrototyping foam (2\" blue foam)\nLarge sheets available\nGood for bulky parts\nCuts easily with hot knife\nAlso can be sculpted with hot knife for\ninteresting / irregular shapes\n\nOther materials\nWooden dowels\nHollow metal tubing\nSprings\nPVC pipe\nFoam pipe insulation\nGears\nOthers...\n\nFasteners\nBolts and machine screws\nsizes from 1⁄4\" down\nWood screws\nGlue (hot glue, superglue, wood glue)\nTape\n\nFasteners\nUse the bolts! We have plenty\nat top, bottom)\nWashers protect softer materials like wood (one each\nMany 1⁄4\"-20 bolts, but also from #10-#2\nTry to pick most appropriate size. Sometimes longer\nbolts can eliminate need for additional pieces\nFor loose but permanent connection, tighten 2 nuts\nagainst each other\n\nFasteners\nBolts continued\nBolts are great for\ntemporary fasteners, as\nwell as permanent ones\nUse lock washers to\nprevent loosening from\nvibrations - teeth bite into\nsurface of material and nut\n\nFasteners\nWood glue - best with wood screws for\npermanent joints.\nMake a solid piece out of multiple pieces\nWhen glue dries, stronger than the wood\naround it. Dry time is long, though\nSuperglue - quick and dirty, or use with\nother fasteners for permanence\n\nFasteners\nIf you're not sure how well a joint will work,\nuse scrap and test it\nTesting mechanical parts is a good idea in\ngeneral, just like software\nDesign for assembly and re-assembly\n\nTools\nScroll Saw\nThin (1/4\") wood and polycarbonate only\nMakes curved cuts\nDon't force the blade in any direction, medium\npressure will cut\nNo metal allowed!\n\nTools\nHacksaws, wood saw\nCut wood, PVC, cardboard\nPipe cutter (small red gadget)\nCuts brass tubing - turn and tighten gradually\nRotary cutting tool\nQuick, but inaccurate\n\nTools\nMitre saw\nMore accurate wood cuts, any angle\nUse clamps for best result\nDrill press\nWood, plastic, metal (carefully)\nClamp small or light pieces\nPunch is preferable for sheet metal - if you have to\ndrill, make sure the piece will not cut you if it binds\nMake sure to use harder drill bits for metal\n\nTools\nShear / Brake\nCuts thin materials only (1/16 sheet aluminum\nand polycarbonate)\nUse stop (in back) for repeated cuts\nMakes right-angle bends in metal\nUse to make L-brackets\n\nTools\nPunch\nUse the centerpunch\n(pointy thing) and hammer\nto make dents where you\nwant holes\nPunch tip will be easier to\nposition\n\nSafety\nWear goggles when in shop area\nYou may not be using a dangerous tool, but\nsomeone else might\nIf you're unsure about a tool's use, ask!\nUse fan when soldering\nBe nice to the benches\n\nMaintenance\nBe nice to your labmates\nBring tools back as soon as you are done\nPut bolts into correct bin, or the mix bin to be\nsorted later. Just not into the wrong bin\nDrill bits have nice racks. Use them!\nAgain, be nice to the benches! Take care\nwhen soldering, use scrap under workpiece\nwhen drilling\n\nMotors\n- Be careful of side loading,\naxial loading\n- Use appropriate motors -\nservos have a limited range\nof motion, and cannot bear\nthe load of metal motors\n- Extra high speed and extra\nhigh torque motors available\n- Servos can be modified for\nlarger range of motion\n\nTechniques\n- Many possibilities with wood and bolts\n\nSimple Rotating Gripper\n\nTechniques: Mounting IR and Servos\nIR range finder\nServomotor\n\nTechniques: Metal bending\nTo bend without the brake, make guide cuts using\nsnips\n(and holes along bend line for wide pieces)\nThis makes it bend where you want it to\n\nDesign Principles\n- Rule of 3-5\n(Saint Venant's principle)\n- Applies to shafts (rotary and\nlinear motion) wheel hubs,\nothers\n- Anytime something should\nmove and it gets stuck, or\nshould be stuck and moves,\ncheck this rule\n\nDesign Principles\nSometimes a mechanical solution can\nsave software design time\nCompensate for lack of precision\nmechanically\n\nOther Mechanical Engineering Resources\nCentral machine shop\nBasement of Building 38\nAll kinds of metal and plastic stock\nEdgerton Shop\nAcross Vassar Street\nTraining required, safety lecture\n\nParts Resources\nMcmaster.com\nRaw materials, fasteners, and almost infinitely more\nSdp-si.com\nGears, shafts, bearings, pulleys, chains\nAllelectronics.com\nSurplus - limited selection, but cheap\nBrowse and order interesting parts ahead of time,\neven if you're not sure you'll use them"
    },
    {
      "category": "Resource",
      "title": "morevision.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/ede1ed75f3cfc660590248f02abf23ee_morevision.pdf",
      "content": "\"Advanced\" Vision\nJanuary 10, 2005\n\nAgenda\n? Hodge Podge of Vision Stuff\n? Stereo Vision\n? Rigid body motion\n? Edge Detection\n? Optical Flow\n? EM Algorithm to locate objects\n? May not be directly applicable, but we've tried to\nmake it relevant.\n\nStereo Vision\n? We can judge distance based on the how\nmuch the object's position changes.\nLeft Image\nLeft Eye\nRight Eye\nRight Image\n\nStereo Vision\n? Use the image to find the angle to the\nobject, then apply some trig:\nLeft Image\nangle-side-angle gives\nRight Image\nyou a unique triangle\n\nStereo Vision\n? What's the angle?\nX\n? Perspective projection\nequation tells us\nZ\nx\nx/f = X/Z\n? f is focal length, x is\nf\npixel location\ncenter of projection\n? tan(f ) = X/Z = x/f\n\nStereo Vision\n? But in a complex image, objects may be\nhard to identify...\n? Try to match regions instead (block\ncorrelation)\n\nStereo Vision\n? Difference\nmetric = Sum\nof (Li - Ri)^2\n? Search\nhorizontally for\nbest match\n(least\ndifference)\n1 6\n6 5\n5 5\n5 7\n6 5\n5 5\n\nStereo Vision\n? Still have a problem: unless the object is\nreally close, the change might be small...\nLeft Image\nLeft Eye\nRight Eye\nRight Image\n\nStereo Vision\n? And many regions\nwill be the same in\nboth pictures, even if\nthe object has\nLeft Image\nmoved.\n? We need to apply\nstereo only to\n\"interesting\" regions.\nRight Image\n\nStereo Vision\n? Uniform regions are not\ninteresting\n? Patterned regions are\ninteresting\n? Let the \"interest\" operator\nbe the lowest eigenvalue\nof a matrix passed over\nthe region.\n5 4\n5 5\n5 5\nlowest eigenvalue = 0\n5 4\n1 5\n5 2\nlowest eigenvalue = 2.5\n\nStereo Vision\n\nStereo Vision\n? For Maslab, the problem is simpler... can\neasily identify objects and compute\nhorizontal disparity.\n? To convert disparity to distance, calibrate\nthe trig.\n? Use two cameras... or mount a camera on\na movable platform... or move your robot\n\nRigid Body Motion\n? Going from data association to motion\n? Given\n? a starting x1,y1,?1\n? a set of objects visible in both images\n? What is x2, y2, and ?2?\nposition one\nposition two\n\nRigid Body Motion\n? If we only know angles, the problem is\nquite hard:\n? Assume distances to objects are known.\n\nRigid Body Motion\n? If angles and distances are known, we can\nconstruct triangles:\ndistance between\nobjects should be\nthe same from\nboth positions\n\nRigid Body Motion\n? Apply the math for a rotation:\nx1i = cos(?)*x2i + sin(?)*y2i + x0\ny1i = cos(?)*y2i - sin(?)*x2i + y0\n? Solve for x0, y0, and ? with least squares:\nS (x1i - cos(?)*x2i - sin(?)*y2i - x0)^2 +\n(y1i - cos(?) *y2i + sin(?)*x2i - y0 )^2\n? Need at least two objects to solve\n\nRigid Body Motion\n? Advantages\n? Relies on the world, not on odometry\n? Can use many or few associations\n? Disadvantage\n? Can take time to compute\n\nEdge Detection\n? Edges are places of large change\n? Scan the image with little computational\nmolecules or a 'kernel'\n-1\n-1\n\nEdge Detection\n\nEdge Detection\n? More sophisticated filters work better\n(Laplacian of Gaussian, for example)\n\nEdge Detection\n? Need to choose a good value for threshold\n? Too small--gets lots of noise, fat edges\n? Too big--lose sections of edge\n? What do you do with an edge?\n? Extract lines for a map?\n? Use to separate regions?\n\nOptical Flow\n? Look at changes between successive\nimages\n? identify moving objects\n? identify robot motion (flow will radiate out\nfrom direction of motion)\n? For each point on image, set total\nderivative of brightness change to zero:\n? 0 = u*Ex + v*Ey + Et\n\nOptical flow\n\nOptical Flow\n? Computationally expensive and requires\nvery fast frame rates... or very slow robots\n? Idea from optical flow: looking at change\nbetween frames can help segment an\nimage (only edges will move).\n\nEM Algorithm\n? Given an image with k objects\n? How can we find their locations?\n\nEM Algorithm\n? Assume there are k red objects\n? Randomly choose object locations xk, yk\n? Loop:\n? Assign each pixel to nearest xk, yk\n? Recenter xk, yk at center of all pixels\nassociated with it\n\nEM Algorithm\n? Key question: what is k?\n? Need to know how many objects\n? Convergence criteria for random values?\n? Pick good guesses for centers\n\nPerformance Note\n? Faster access:\n? bufferedImage =\nImageUtil.convertImage(bufferedImage,\nBufferedImage.INT_RGB);\n? DataBufferInt intBuffer = (DataBufferInt)\nbufferedImage.getRaster().getDataBuffer();\n? int[] b = dataBufferInt.getData();\n? Need to keep track of where pixels are:\n? offset = (y*width + x)\n? (b[offset] >> 16) & 0xFF = red or hue\n? (b[offset] >> 8) & 0xFF = green or saturation\n? b[offset] & 0xFF = blue or value\n\nReminders\n? No lecture tomorrow\n? Design Review Wednesday\n? Check Point Two: Friday\n? If you haven't completed check point one,\nyou finish it today!"
    },
    {
      "category": "Resource",
      "title": "overview.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/d139fe13987372b9d936d19093fe6f19_overview.pdf",
      "content": "Welcome to MASLab!\nFirst Lecture\nJanuary 3, 2005\n\nAgenda\nIntroducing the Staff\nLogistics--website, wiki, rooms, calendar\nCourse Policies and Philosophy\nContest Preview\nThe Game\nThe Kit\nToday's Objectives\nORC API preview\nAssembling the ORC Pad\nPegbot\n\nThe MASLab Staff\nUndergrads and grads like you!\nProgram\nFinale\nTechnical\nYuran\nSoftware\nTim\nMechanical\nAaron\nHardware\nDany\nORC\nEd\n\nMentors and Check-offs\nEveryone on the staff is here to help\nEveryone can witness a check-off\nMentors keep a closer watch:\nTeams 1-4 -- Finale\nTeams 5-8 -- Yuran\nTeams 9-12 -- Aaron\nTeams 13-15 -- Tim\n\nSponsors\nMIT Course 6\nCypress\nAdvance Circuits\nGlobtek\niRobot\nAnalog Devices\nHankscraft Motors\nAcroname\nDigikey\n\nLogistics\nStorage Options\nTake it with you--tubs provided\nLocker or storage closet (we're working on it)\nUnattended valuables = loss of sensor points\n\nLab and Lecture Schedule\nLectures Jan. 3-7, Jan. 10 and Jan. 12\nEnrichment Lectures to be scheduled\nThis week: lab after lecture until 6:30 pm\nStarting next week:\nlab from 12 - 8+ pm weekdays,\n12 - 5 pm weekends\nJava Tutorial: tomorrow night!!\n7-9pm\n\nKey Dates\nCheckpoint One\nJan. 7\nDesign Review\nJan. 12\nCheckpoint Two\nJan. 14\nMock Contest One\nJan. 20\nMock Contest Two\nJan. 25\nImpounding\nJan. 27\nFinal Contest\nJan. 28\nClean-up day\nJan. 29\n\nCourse Philosophy\nMaslab should be fun!\nYou will learn a lot!\nWhy all the rules?\nKeep you on track.\nRespect your volunteer staff.\n\nCourse Policies\n6 Units Pass/Fail\n6 EDPs\nPassing Grade\nKeep kit (except computer)\nMeet course requirements\n\nCourse Policy: Requirements\nAdequate effort and time invested in MASLab\nAttend mandatory meetings/events\nMajority of work in lab\nCompletion of \"checkpoints\"\nMake daily lab entries (few sentences minimum)\nSubmit final report (5-10 pages per team)\nHelp tidy workshop on your team's turn\nHelp final cleanup on lab cleanup day\n\nCourse Policies: Disasters\nYou are responsible for the working\ncondition of your hardware\nIf hardware breaks:\nYou're responsible for a replacement.\nIn most cases of accidental damage, MASLab\nwill split the cost of a replacement.\nCosts: Eden $250, Orc $150, OrcPad $40\nLet's avoid this situation! Be careful!\n\nContest Preview: The Basics\nYou'll build and program a robot\nRobots use vision, range finders, other\nsensors to locate and transport \"target\"\nobjects.\nThe playing field is unknown\nWhere are obstacles?\nWhere are targets?\nWhat is the shape of the playing field?\nThe robot functions autonomously\n\nContest Preview: The Rules\n3 minute scoring round\nOptional 3 minute exploring pre-round\nTargets are red wooden balls\nScore by:\n5 pts - field goal over mouse hole\n3 pts - through mouse hole\n1 pt - porch in front of mouse hole\n1 pt - possession\n\nContest Preview: The Field\nBlue line on top of white walls with\npseudo-randomly spaced tick marks\nYellow border around mouse holes\n4-bit vertical green and black bar codes on\nwalls\nRed balls\n\nContest Preview: Prizes!\nMore of an exhibition than a competition\nIt's a hard problem. Work together!\nYou'll do better if you work with other teams.\nAwards\n1st place\nMASLab Engineering Award and other staff\npicks for cool ideas or clever implementations\nAt least one award for cosmetics\n\nThe Kit\nWe supply basic parts\nEnough to build a complete robot.\nMotors, wheels, computer, sensors...\nYou supply \"extras\"\nBetter motors, custom-made widgets,\nunique/unusual sensors\nSubject to spending limit ($100 per team) and\nnon-passive components to staff thumbs-up\n\nSensor Budget\n~30 pts, subject to staff approval and availability\nSolenoid\nGyro\nServo\nValue\nPhotodiodes, etc.\nMoment buttons\nWhisker switch\nIR (short range)\nOptical Encoder\nIR (long range)\nUltrasound\nExtra drive motor\nItem\n\nBuilding Tips\nMechanical: machine shop access is very\nuseful!\nMASLab tools limited, imprecise\nSoftware\nMany conceptual parts\nOutputs hard to observe without care, so...\n\nBuilding Tips\nWrite modular code\nFocus on behaviors (go straight, turn, etc.)\nDesign for test:\niterate between coding, compiling, and tests\nautomate tasks (calibration)\ntest on static images\nuse the debug clients\n\nUpdates, bugs, advice\nmaslab.jar updates at boot\nfirmware updates as needed\nProblems? Suggestions? Let us know!\nDon't stew bitterly\nYour advice is very welcome\n\nKit details: Hardware Overview\nOrc Board (the larger board)\nprovides hardware resources--interface\nbetween compute and sensors, motors\nOrc Pad (the smaller board)\njoystick and lcd\ndraw images on to it\nlog text messages\nstart robot without wireless\n\nKit details: batteries\nOne 12 V lead-acid battery\nmay trade or borrow a different size (and\ndifferent amp-hours: 2, 5, or 7)\nALWAYS fused\n13.8 V DC regulator\nif both battery and regulator are plugged in,\nthe battery is recharged\n\nKit details: Software\nJava documentation at Sun's website\nMaslab goodies on maslab website\n\nKit details: orcd\nPersistent service on the eden that\nimplements low-level usb port handling\nArbitrates between client applications\nProvides shell capability (Eden's IP address,\nrun/execute arbitrary programs)\nWe provide the binary. You never need to\ncompile/write anything\nExcept maybe /etc/orcd.conf\n\nKit Details: Maslab APIs\nMaslab.camera--get frames from camera\nMaslab.orc--implements Orc API\nanalog digital\norcpad\nlcd\nmotor\nlcd console\nservo\nsoar\nMaslab.telemetry--data logging,\nvisualization, debugging\nMaslab.util--helper classes\n\nExample\nimport maslab.orc.*;\nimport maslab.util.*;\nimport java.io.*;\npublic class hello\n{\npublic static void main(String[] args)\n{\nOrc o;\ntry {\no=new Orc();\n} catch (IOException ex) {\nSystem.out.println(\"Could not create orc!\");\nreturn;\n}\no.lcdConsoleHome();\no.lcdConsoleWrite(\"Hello, world\\n\\n\");\no.lcdConsoleWrite(\"Press a key...\");\no.padButtonsGet();\n}\n}\n\nToday's Objectives: OrcPad\nAssemble OrcPad\nStep-by-step instructions included\nAfter soldering kit, check with a staffer:\nMake sure it's right--get LCD, chip to finish\nGet suggestions on your soldering technique\n(this is a class, after all :)\n\nToday's Objectives: Soldering\nSoldering is non-trivial, especially surface\nmount components\nGoals: good physical connection; electrical\nand thermal connectivity\nTechnique: heat both parts of joint first.\n(Don't paint with solder!) Avoid oxidation -\nthe joint should be shiny.\n\nDiagrams\nhttp://www.epemag.wimborne.co.uk/solderpix.htm\nTo see this image, go to:\nImage removed due to copyright considerations.\n\nMore on Soldering:\nUse the lowest heat that will work (about\n650 F)\nKeep iron tip clean and shiny. Store with\nsolder on it. Never \"sharpen\" tip.\nMinimize heating time (avoids oxidation,\ndamaging sensitive components)\nContact shouldn't be more than 2-3 seconds\nLet components cool for a few seconds\n\nTips for Surface Mount\nPut a dab of solder on one of the pads\nSlide the device right next to the solder.\nRemelt the solder and slide the\ncomponent in place.\nSolder second pad.\nUse all left pads/right pads for the dabs\nwhen components are next to each other.\n\nSurface Mount\nhttp://www.geocities.com/vk3em/smtguide/websmt.html\nTo see this image, go to:\nImage removed due to copyright considerations.\n\nToday's Objectives\nGryo sensor - solder the board\nOrcboard - add 3x2 header for gyro\nSoftware\nWrite a hello world for your Eden\nPrint a hello world to the orc pad\nPegbot\nSlap it together!\nGet something moving!\n\nMore objectives\nStaff and equipment are limited, so please\nbe patient! Everyone will get a turn.\nOther things to do:\nmake your battery cable\ninspect orc board for missing/poor joints\nplay with the playing field\ntake pictures of the playing field\nextend tutorial code\nbrainstorm contest strategies"
    },
    {
      "category": "Resource",
      "title": "sensors.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/9775875a049c67236b1d465744ca15c7_sensors.pdf",
      "content": "Sensors and Cables\nKen Barr, Christopher Batten,\nAlana Lafferty, Edwin Olson\nMaslab 2005\n\nMaslab Sensor Types\nCommon types:\nCamera\nInfra-Red (IR) range finders/reflectance\nAuto-flush toilets\nUltrasound\nCameras\nPhysical contact\nRoomba\nGyroscopes: Angular Rate Sensor\nAutomotive, GPS-assist\nMotor current sense\nOptical encoders\nTimer?\nOther types:\nPhotodiodes from 6.270\nDigital Compass\nReed switch\nMercury switch\nBe creative!\n\nInfrared\n750 nm to 1,000,000 nm\nWe typically use near-infrared, ~900nm. Near-infrared used on many\ncamcorders for \"night vision\"\nFar-infrared is used for body heat detection\nCheapest: excited silicon emits IR\nDoes not penetrate walls\nTransmitters (LEDs or thermal)\nIn our case, almost always LEDs\nDetectors (photo diodes, photo transistors)\nSensors use notch filter to pass only IR\nInfrared\nVisible\n\nSimple IR sensors\nBreak-beam\nShine a light directly onto a detector. You can detect if something\nbreaks the beam of light.\nReflection\nShine a light and detect its reflection off a nearby object\nTriangulation\nShine a light at an angle, have an array of detectors\n\nFar =\nMaslab Infrared Range Detectors\nSensor includes:\nInfrared light emitting diode\n(IR LED)\nPosition sensing device (PSD)\nuses small lens to focus\nreflected pulse onto a linear\nCCD array (or magic, differential\nFET)\nTo detect an object:\nIR pulse is emitted by the IR\nLED\nPulse hopefully reflects off object\nand returns to the PSD\nPSD measures the angle at\nwhich the pulse returns\nWider angle = greater distance\nFigure: Acroname.com\n\nAnal\nut ut Volt\nV)\nLies, damn lies, and datasheets? Characterize your\nsensors. Understand the default profiles.\nGP2D12: Theoretical Range:\nGP2D12: Measured Range:\n4in (10cm) to 31in (80cm)\n~4in (10cm) to ~ 18in (45cm)\nog O p\nage (\n2.5\n1.5\n0.5\nDistance to reflected object (cm)\n\nNon-linear response presents small problems\nUltra short readings can look \"far-away\"\nMount to accommodate this\nLarger error in steep part of curve\nOrc library use inverse of curve\nand fits a line\nVoltage = 1/(distance + Xd) * Xm + Xb\ndistance = (Xm/(Voltage-Xb)) - Xd\n\nLong range IR sensor uses different lens; increases\nboth min and max limits\nGP2Y0A02YK\n\nIR Ranger Properties\nSmall, eraser-sized point beam\nEasy to resolve details; easy to miss small objects if\nyou're not looking right at them.\nSet up a perimeter\n\nIR Rangefinders\nCan use signal strength\nSort of.\nCan use time-of-flight, c=299,792,458 m/s\nHow fast can you count?\nNot fast enough!\nSick industrial laser scanner: $5000\nProvides ~5cm accuracy, 1⁄4 degree resolution, 30m range\n(collective \"ooooh!\")\n\nUltrasound Rangers\nSend an ultrasonic pulse, listen for an echo\nTime of flight. Speed of sound only ~347 m/s\nLimited supply?\n\nUltrasound Ranger Properties\n\nDistance to reflected object (in)\n\nDistance to reflected object (ft)\nE c h o P u ls e W id th (m s)\nE c h o P u ls e W id th (m s )\n\nUltrasound Ranger Properties\nBroad beam width \"blurs\"\nSound can \"scatter\"\ndetail... but less likely to\n(shortest path) or \"reflect\"\n\"miss\" something\nCan dramatically overstate range.\nSmall detail hard to resolve\nMultipath can fool you!\n\nOptical encoders are another use for IR emitter and\ndetector\nAttach a disk to the motor shaft and attach a break-beam\nsensor across the teeth.\nOutput of Encoder\ntime\nOr, use a reflectivity sensor and a disk with black & white\ncolored wedges.\nWhat if wheel stops halfway between slats?\nAre we going forwards or backwards?\n\nQuadrature Phase Encoders allow us to distinguish\ndirection\nUse TWO single encoders, 90 degrees out of phase.\nClose-up of teeth\ntime\ntime\nForward\nBackward\nForward and backward are now distinguishable!\nIllegal state transitions cancel out (for each spurious\nforward tick, there's a spurious backward tick)\n\nUsing Quad Phase\nQuad phase can allow us to:\nDo relative positioning- i.e., rotate 10 clicks from our\npresent position (remember that gyro can help with this)\nDo velocity control.\n\"driving\" but not ticking? Probably stuck. Current spike may\nreveal this, too.\nIt's hard to drive in a straight line. PID.\nCompute the robot's path using odometry.\n\nDigital Inputs\nBump sensors\nNES, anyone?\nUses an internal pullup\nresistor.\n5V\nGND\nN/C\nSIG\nOrcBoard\n\nMEMS Gyroscope\nOutputs a voltage corresponding to degrees/sec\nNote that OrcBoard integrates for you\nThanks, Ed!\nBut, what is effect of noise\nSmall voltages could mean the gyro thinks it's turning.\nLots of \"slow turns\" + Integration = Drift\nStudy odometry tutorial\nUses\nAccurate turns, straight lines\nCombine with other sensor data (camera, encoders, etc) for dead reckoning\n\"Columbus Style\"\n\nMEMS Gyroscope takes advantage of coriolis effect\nSensors Online Magazine (sensormag.com)\nImages by\nDavid Krakauer, Analog Devices Inc.\nImage removed due to copyright considerations.\n\nTwo sensors allow differential sensing to eliminate\ncommon-mode error (shock, vibration)\nImages by\nSensors Online Magazine (sensormag.com)\nDavid Krakauer, Analog Devices Inc.\nImage removed due to copyright considerations.\n\nMaslab bloopers\nBe aware of the size of your robot\nYou clock is a sort of sensor, timeout!\n\nOrc board features\nConfigurable low-pass filter on analog\ninputs removes noise\nLess need for a capacitor on the IR sensor\nBuilt-in current sense:\nApproximate, but useful\nAll drive motors, servos 0 and 1\nCurrent is proportional to torque\nA known Rsense and measured voltage\n(Vx) yield current: V=IR\nOptional optical encoders\nWe'll demo, distribute today\nQ = motor.encoder()\nQ = motor.encoderAbsolute();\nMotor\nDriver\nRsense\nORC\nBoard\nVx\n\nSome additional soldering points\nFor MASLab-style soldering, a cheap iron probably will do.\nStill, if you're in the \"biz\", an investment makes sense\nSome tools available for purchase through 6.270 store\nCheap soldering irons, helping hands, wire strippers\nSo cheap, who cares if it's crappy?\nTell them you're with MASLab.\n\nSoldering Mistakes\nUse a wet sponge to keep your iron tip clean\nIf you don't have a sponge, get one\nKeep it quite damp. Don't want sponge to burn onto tip\nMake sure you apply heat to both surfaces to be joined and that solder\n\"wets\" both.\nGood. Solder\nSolder\nNot wet to\nNot wet to\nboard.\nhas \"wet\" the\nhasn't \"wet\"\npin, not\npin and\nthe pin\nenough\nProbably no\nboard.\nsolder\nconnection.\n\nSoldering Mistakes\nWatch out for \"ears\"\nIndicates a bit of oxidation,\noften aggravated by too much\nsolder.\nIf the solder feels \"thick\", then\nit's oxidized some.\nConnection is probably okay,\nbut something to work on!\nOn cables, can poke through\ninsulation and heatshrinking!\n\nCable making: General Tips\nUse Stranded Wire only, strip only 1⁄4\", twist strands\ntogether\nPre-tin all wire leads and header\nUse heatshrink on connections\nHeader is plastic and will melt easily\nUse a dab of hot glue to reinforce (optional)\nColor code! Make absolutely sure pin 1 is indicated!\n(Use sharpie to indicate a pin if it's not otherwise\nobvious to you and any random person.\n\nCable making, step-by-step\nStep 1\nStep 2\nStep 3\nstranded wire.\nPre-tin the connector.\nPre-tin (add some solder) the\nAdd heat shrink tubing and\nsolder the pins together.\nSolder the wire to the header (not shown)...\n\nCable making, step-by-step (cont)\nStep 4\nStep 5\nThis cable is now ready for\nshrinking.\nShrink the heatshrink tubing.\n\nCable Making: Pinouts\nGP2Y0A02YK\nGP2D12\nSee Orc Manual for connector pinouts\n\nReminder\nJava for the clueless\" - tonight, 7-9PM\nToday:\nMake sensor cables; start with short range IR\nCharacterize sensors\nHandy worksheets\nBuild your intuition and start making [mental] selections\nPegBot: IR proximity with OrcPad feedback. Choose\nbump/nobump or edge finder."
    },
    {
      "category": "Resource",
      "title": "software.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/222e989bfe21b02a813a2c10e4a646d7_software.pdf",
      "content": "Maslab Software\nEngineering\nJanuary 5th, 2005\nYuran Lu\n\nAgenda\nGetting Started\nOn the Server\nUsing the Documentation\nDesign Sequence\nTools\nThe Maslab API\nDesign Principles\nThreading in Java\n\nOn the Server\nPut these lines in your .environment:\nadd 6.186\nadd -f java_v1.5.0\nsetenv JAVA_HOME /mit/java_v1.5.0\nsetenv CLASSPATH /mit/6.186/2005/maslab.jar:.\nIf you're Serverphobic, just ask for help.\nYou'll learn fast, and you'll be glad you did.\n\nUsing the Documentation\nMaslab API\nJava 1.5.0 API\nSun's Java Tutorial\nEd Faulkner's Java Reference\nAll linked from SoftwareInfo page on wiki\n\nDesign Sequence\nOpen a text editor to edit a source code file:\nemacs MyExample.java\nWrite class declaration, and declarations for each of your\nmethods, and annotate with comments\nFill in source code\nCompile:\njavac MyExample.java\nThis produces MyExample.class if successful\nFix compile errors and repeat compilation until successful\nRun:\njava MyExample\nThis searches the CLASSPATH for MyExample.class and executes it.\n\nTools\nCVS, subversion\nmake, ant\nOrcSpy, BotClient\nInstructions all on SoftwareInfo page of wiki\n\nThe Maslab API\nmaslab.orc\nmaslab.camera\nmaslab.telemetry.channel\n\nAgenda\nGetting Started\nDesign Principles\nMotivation\nModularity and the Design Process\nWriting Good Specifications\nTesting\nGood Design Practices\nThreading in Java\n\nDesign Principles - Motivation\nCoding a Maslab Robot is a formidable, multi-\nperson project\nMaking debugging easier\nMaking sure different team member's code all\nwork together\nMaking sure one team member's changes\ndoesn't break another team member's code\n\nModularity and the Design Process\nModular Design\nProvides abstraction\nGives up fine-control abilities, but makes code much\nmore manageable\nThe Design Process\nTop-down vs. Bottom-up\nWrite out specifications for each module\nWrite code for modules\nTest each module separately as it is being written\nTest overall system for functionality\n\nModularity - An Example\nStart with most basic behaviors:\nDriveTowardBall\nWallFollow\nDriveToWallAndStop\nBuild up more complicated behaviors:\nHuntRedBalls\nGoToLastRememberedBall\nAlignAndDepositBall\nWanderToGetUnstuck\nBuild highest-level behaviors:\nWinMaslab\n\nWriting Good Specifications\nThis that should go into the specification:\nSynopsis of classes and methods\nHow methods are called\nRestrictions on argument values\nThe return value and effect of calling the method\nWhat shouldn't go into the specification:\nHow code is implemented\nLong paragraphs of text\n\nTesting\nTest each module separately\nTest overall system\nTest special cases\nCome up with test cases before coding, or\nhave a different team member do testing\nUsing the main() method\nUnit testing\n\nGood Design Practices\nEXTREMELY IMPORTANT!\nThou shalt Test Constantly\nStart small, build up\nModularity\nAvoid over-abstraction\nBack up code\nKeep multiple versions backed up\nKeep separate backups off of the robot computer\n\nAgenda\nGetting Started\nDesign Principles\nThreading in Java\nMotivation\nUsing Threading\nSynchronization\n\nMotivation for Threading\nAbility to perform tasks in parallel\nIf used properly, can make your robot run faster\nDifferent threads for:\nImage capture and processing\nKeeping a current map\nControlling the current motion behavior (Wandering,\nBall-seeking, Obstacle Avoidance, etc.)\nHigher-level strategic control\n\nUsing Threading\nLook in Sun's Java tutorial, or Ed\nFaulkner's Java reference\nLook at the Java API:\nThread, Runnable, wait(), notify(), sleep(),\nyield()\nMust take care to avoid deadlock\n\nSynchronization in Threading\nAllows blocks of code to be mutually\nexclusive\nWriting to the same object from two\nthreads at the same time will cause your\nprogram to break"
    },
    {
      "category": "Resource",
      "title": "teameightpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/6ae4749d42e1bddcbae71375f8bee4c7_teameightpaper.pdf",
      "content": "Team Eight Paper\nSoftware Strategy\nCONTROL\nOur robot was controlled by a Finite State Machine (FSM) that had four main states: Find Ball, Chase\nBall, Find Goal, and Chase Goal. Both \"Find\" states used the same lower-level functions to implement\nthe wandering algorithm, and both \"Chase\" states used the same helper functions to chase down an\nobjective that was in sight.\nThe wander functions used a random walk to navigate the playing field. The random walk used its own\nFSM, the basic structure of which is as follows: the robot would move, forward or backward, for a\nrandom time or until it hit a wall. If it timed out, it would proceed to the turn phase. Otherwise, if it\nbumped, it would reverse direction for a short time before turning. The turn phase was similar: turn, left\nor right, for a random time or until hitting a wall. If it timed out, it would reverse the turn direction and\ngo back to the move phase. If it bumped, it would reverse the turn direction before moving again.\nThis algorithm proved to be quite robust and resistant to getting stuck. The integrated use of the bump\nsensors allowed us to quickly determine, fairly reliably, whether we were stuck. And if our bump\nsensors failed, then we would time out and would eventually, due to the random nature of the\nexploration, get ourselves out of trouble. This ability was extremely important because of our bot's large\nsize: our robot was big, had its center of rotation in the rear, and had square corners. All in all, these\nfactors combined to make it very easy for our bot to get stuck, so we focused our efforts in the code on\nminimizing the chance of a fatal error.\nAs for the chase code, this was fairly simple: if the desired object is in sight and to the right, turn right. If\nit is in sight and to the left, turn left. If it is in sight and in the middle, drive straight ahead. If it is out of\nsight, give up and search again. If, while driving straight, a forward bump sensor is triggered, check\nwhich side the sensor was on. Then, back up, move a little to the opposite side, and chase the target\nagain.\nThis behavior proved to be very robust and efficient in chasing down balls and mouse holes. If it could\never see a ball, it would move quickly towards it; the avoidance code that monitored the bump sensors\nallowed it to quickly maneuver around obstacles in its pursuit of the target. All in all, these functions\nwere pretty successful, at least during practice. In our best run, we managed to collect eight balls into\nour storage bin, wandering from ball to ball, bouncing off walls and avoiding obstacles.\n\nOur vision algorithm was well suited for our needs. As we wandered around the playing field, we\ncontinuously took images with the Logitech web cam. We processed these images and from them\ndeduced the location of visible balls and field goals.\nImage processing involved 2 steps. First we converted the images we took with the web cam from the\nRGB color space to the HSV color space. This allowed us to more easily distinguish the colors we were\nlooking for. Next we searched for the blue strip on the top of the walls by scanning across down from\nthe top in columns. Once a pixel was determined to be blue, we colored everything above that pixel\nwhite. This allows us to ignore everything that is not inside the playing field and prevents the rest of the\nvision algorithm from being confused with colors outside the playing field.\nAfter we processed the images, we were ready to search for red balls and yellow field goals. We scanned\nacross the image, keeping track of the width of each ball or field goal we came across. We then return\nthe position of the object with the largest width. For red balls, we returned the angle from the center of\nthe closest ball to the center of the entire image. 0 was the center of the image and the right half of the\nimage was set to positive radians and the left set to negative radians. For field goals we returned the\nangle from the center of the image of both edges of field goal along with the slope of the top of the field\ngoal. The slope was used in our docking code to determine which angle of approach we should take.\nThe biggest problem with the vision algorithm that we encountered was which thresholds to set for the\nblue strip, red balls, and yellow field goals. The lighting in the lab differed greatly from the lighting in\nroom. Our vision worked extremely well in the lab during our tests. During the competition however,\nour thresholds were not calibrated correctly, leaving our robot to wander blind, either not being able to\nsee objects of interest or seeing objects that weren&#8217;t actually present.\nMECHANICAL STRATEGY\nOur main mechanical strategy was ultimately to scoop up balls and store them somewhere until the robot\nreached a goal. Our method for collecting balls was to use a rotating combine which would come down\nfrom behind the ball and push it up and over a short ramp onto a sloped section where it would sit with\nseveral other balls. However, our final design was not what we had originally planned on fabricating.\nCombine\nThe greatest time sink in our fabrication was undoubtedly the gear box we made. The main motivation\nbehind the gearbox was that we needed a way to needed a way to rotate the combine by a motor which\nwas 3 inches off axis. Since the two ball transfers were in line with the combine we realized we could\nnot place a motor directly next to the combine. What we failed to realize was that a servo could be used\nin place of a motor and that it would also fit due to its smaller size. Consequently, we wasted four days\nmaking a fancy two-stage reduction gearbox. Upon completion of the gearbox we soon realized that it\ndid not work because one of the cantilevered shafts deflected too much when it was loaded and would\nnot rotate past a certain angle.\nVISION\n\nThe original combine design we had also did not fully work. We started with a four-finned design with\nmetal continuous from the center to the outer radius. This design had a tendency to trap the ball from\nabove, which was unacceptable. We modified the combine by making a two-finned design where there\nwas metal only at the outer radius and no central shaft. We also switched from aluminum fins to steel\nfins since the new thinner design required a greater material stiffness.\nStorage and Release\nOur storage and release mechanisms for the balls did not have any problems with them. The storage\nmechanism was essentially a bent sheet of aluminum, suspended on an angle from the main platform.\nThe release mechanism was simply a small piece of aluminum attached to a servo, mounted to the main\nplatform, which when rotated allowed the stored balls to roll out from the side of the robot.\nBefore settling on this idea, we had wanted to score five-pointers. To do this, we had come up with the\nidea of an \"Archimedes Screw\" to lift the balls from the carrying trough up to a ball hopper about 13\ninches off of the floor. That way, we would be able to swing a gate out of the way using a servo and\ndrop all of the balls into the bin.\nThe device was a tube with a flat bottom, a screw that rotated inside of it, and a motor to turn the screw.\nThere were holes in the top and bottom so that that balls could fall into and out of the device.\nThe tube was made of plastic: white 3.5\" PVC with lexan for the bottom and ABS for the motor mount.\nThe screw was made of .2\" solid copper grounding wire, which we found as scrap in the lab.\nWe bent the copper into the helical shape using a lathe and a 1.25\" round aluminum bar. After drilling a\nhole in the bar to fit the wire, we chucked the bar up in a lathe and turned the machine on to the slowest\npossible speed, about 30 RPM. Holding the wire firmly, we were able to get the proper shape, although\nthe whole process ended up being repeated three times, first because it was not big enough, then because\nthe helix was right-handed and needed to be left-handed. It was disappointing to put in all this work, but\nthen not end up using the part; our time could have been better spent making the other things work better."
    },
    {
      "category": "Resource",
      "title": "teamelevenpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/9cc98c47f265614ea62593c950a8fcf7_teamelevenpaper.pdf",
      "content": "Team Eleven Paper\nFinal Report\nThe purpose of the following Final Report for Maslab 2005 is to highlight our successes and failures\nwhile providing some insight into our design decisions.\nMechanical Design\nWoody (our robot), wasn't actually Woody until his second revision. Before his death, and subsequent\nreincarnation as a wooden robot - Woody was created from Polycarbonate, sheet metal, and steel bolts.\nRevision 1 The initial chassis was basically a working prototype modified as the team went along. The\nteam decided that having a working robot at as long as possible was important for writing code that\nworked - so always having at least a drivable chassis was paramount throughout Maslab.\nThe ball pickup mechanism was something that was debated at length, and the team finally decided to\nhave a a belt driven system consisting of one aluminum dowel connect to a drive motor (both the Maslab\nprovided \"high speed\", and \"high torque\" motors were tested, but the \"high torque\" motor was less\ntorque than a drive motor, and the high speed wasn't really that much faster).\nThe underbody of the robot was build around the ball pickup mechanism. it was intented to be a\ndetachable piece from the ball pickup mechanism - so that the two could be tested independently. This\ngoal was achieved - but possibly could have been avoided if we had chosen a less error prone ball\npickup design that required less debugging from the start.\nRevision 2 (Woody)\nRev 2 took much less time than Rev 1, and although it was originally intended to fix all of the problems\nwith Rev 1, because of time constraints it mostly improved upon the speed and manuverability of\nWoody.\nThe second revision was also made out of wood and sheet metal, rather than the polycarbonate of the\nfirst revision. This allowed for a faster construction time (the lab was much better for working with\nwood than polycarbonate) but was less \"cool\" looking.\n\nThe overall control system structure for our robot was of particular importance to our team. Our initial\naims were to minimize repitition of code, allow for complex behavior, and to have the simplest\ninterfaces possible. In our design all field features of concern to the robot are represented abstractly as\nthe same Java type: the Feature. All Features have an integer-based (power of 2) type (e.g.\nNEAREST_BALL=1, NEAREST_GOAL=2, BARCODE=4...), an angle relative to the robot at their\nmoment of discovery, and a distance. This type of generalization made navigation very simple --\nwhether it was a ball, or a goal, or a barcode, our robot could use the same feedback loop to \"drive for\nFeature\". An equally great benefit is afforded by the use of integers to specify type. This allows\nBehaviors to specify which types of features they care about, simplifying their design and minimizing\nthe work involved with extracting the Features for further analysis.\nFeatures are created by FeatureProviders (each of which runs in its own thread) that sample and\nanalyze data as fast and often as possible. The three FeatureProviders we used were\nImageFeatureProvider, IRFeatureProvider, and CurrentSpikeFeatureProvider.\n● ImageFeatureProvider creates Features that corresponded to field elements within the view of\nthe camera (balls, goals, and barcodes).\n● IRFeatureProvider creates Features corresponding to distances measured by the 4 80cm range\nfinders\n● CurrentSpikeFeatureProvider creates a unique BERSERK feature, which suggested that the\nrobot's motors were stuck, and berserk behavior was appropriate.\nThe Brain, as we so aptly named it, requests an updated Feature list from each registered\nFeatureProvider and places them in a hashmap of lists. This allows different feature providers to both\nprovide the same types of features with no effect on later computation. The Brain then creates another\nhashmap of lists for the registered Behaviors, and populates it with the types of Features each Behavior\nincludes in their integer feature-mask. Once this map is created, each Behavior then does some degree of\ncomputation on the Features (or lack thereof) and returns a suggested Action. These Actions each have a\npriority and an execution method associated with them. Once all the suggested Actions are gathered and\nsorted by priority, the highest ranking one is executed. Actions also provide some degree of state to their\nexecution methods, allowing for emergent characteristics that would otherwise be difficult to accomplish\nwith this type of constant re-election. It is this maintenance of state that allows the Action returned by\nthe WanderBehavior to select a random direction and stick with it until losing an election to some other\nAction of higher priority.\nWhile this abstraction took some time to get used to, its architecture made it very easy to build a very\nrobust and complex system from a minimal set of rules. For example, when attempting to score a field\ngoal we find that aligning and releasing the balls are two distinct yet highly interrelated tasks. We don't\nwant to release them if we aren't aligned, but on the other hand we don't want to force a realignment\nwhen it isn't necessary. We therefore place all scoring-related Actions within the same Behavior, and use\nthe maintenance of state to guarantee only valid tranisitions can occur. Now, the robot will only release\nthe balls if it has just finished chasing or realigning with a goal. In this respect our system is analogous\nControl System\n\nto a collection of finite-state-machines. Well actually, a collection of flexibly-ranked, mutating finite-\nstate machines, any of whom have the potential to jump to the foreground, and all of whom can safely\nignore the existence and quantity of the others.\nImage Processing\nOne of the areas that that we especially concentrated efforts at the beginning was in testing and\noptimising image processing techniques. It was important to the goals of MASlab that our algorithms\nwere both reliable at locating and identifying objects as well as fast, to allow for faster computation and\nthus faster movement of the robot itself.\nMASlab assigns a unique colour to each component of the field in the tournament. The carpet is light\nblue, the walls are white with a blue strip at the top, goals are bordered on the left/top/bottom with\nyellow and all balls are red. This eliminates the need to use advanced processing techniques such as\nshape recognition and high-quality edge detection.\nOne possible algorithm is to convert every pixel value from the raw RGB values provided by the camera\ninto HSV (hue, saturation, value) coordinates, and filter each pixel by its hue and apply algorithms to\nfind large filled areas. The advantage to this approach is the ability to locate every possible instance of a\nparticular object and decide upon the most likely candidate. For example, one could find all red distinct\nregions, mark them as balls, and then remove any that are too small (noise) or too large (possible\ntemporary camera white balancing issues). While an implementation of this type of \"complete\"\nalgorithm. in C/C++ or assembly would be fast enough to be feasible at the camera resolution of\n160x120, and could be interfaced with Java using JNI, we took a less computationally intensive\napproach that did not require us to use faster languages.\nA native function was already provided as part of the MASlab API to convert from RGB to HSV\ncoordinates. Assuming this is fast, our Java approach to image processing involved subsampling the\nimage from the bottom up, at different levels of subsampling at each row. Given the camera's field of\nview and the size of a ball, it is possible to determine the diameter of a ball on the screen, n, in pixels.\nWhen searching for a ball for the first time, the algorithm then subsamples by checking every n/2 pixels\nat every other row from the bottom up. Thus it is guaranteed to have at least 1 sampled pixel in the ball\nfor each row sampled. Once a row returns a hit (i.e. a red pixel is found), the algorithm then proceeds to\ncheck if at least 4 of the neigboring pixels are close enough to red to regard the pixel as a legitamate find.\nThe algorithm then called a bounding function to find the smallest rectangle that enclose all pixels,\nstarting from the found red pixel, that are close to red. This was done by creating an initial rectangle of\n3x3 pixels surrounding the initial pixel, and then expanding the box upward, one pixel at a time, until\nclose to no red pixels exist in the topmost row. This was then repeated for the left, right, and bottom\nedges. While a fill function could have been written, much as most graphics editors have as a tool for the\nuser, this is much slower than simply finding a bounding box using the previously described method.\nThe bounding box algorithm described is not generally perfect, although it should work with extremely\n\nlow error if the object is a simple shape (such as a ball) and the image quality is good.\nThe advantages to such an algorithm are mainly in computational speed. Our algorithm was able to\nachieve almost 20 frames per second on the 733 MHz Eden board and Logitech Quickcam 3000\nprovided by MASlab to locate the nearest ball, goal, and identify one or two on the screen; the speed\nlimitation actually mainly came from the camera capture speed! A second advantage is that one can\nalways locate the closest ball or closest goal to the robot by starting at the bottom of the image using this\nmethod. Thus, the robot can pursue the ball without having to scan any other part of the image. We also\nexperimented with tracking techniques by searching for the same ball in subsequent images by starting\nby the ball coordinates in the previous frame and spiraling outward in the search for red pixels; this was\nextremely successful at first but became problematic if the ball ever disappeared from view due to\ncollisions or otherwise.\nThis algorithm also eliminated the need to do an exhaustive top-line filtering; by searching bottom-up\none can simply terminate the search after enough top-wall-coloured pixels are seen (about 10 to 15\nshould be sufficient). Thus, the entire top wall is not searched, and with our camera mount there was\nnever a way for any ball to be visible above the height of a top wall blue pixel. This also improved the\nalgorithm speed.\nGoals were located using a very similar procedure; however, due to the complex shape it was necessary\nto begin at the bottom as usual, then search left and right to bound the edges of the vertical portion\nreliably. Then it would search updward, left and right, and then downward, corresponding to the typical\ngeometrical description of a goal if one were to follow the yellow pixels from the bottom of one verical\nsegment.\nBarcodes were located by searching for green pixels, first bounding left/right with green, and then up/\ndown with black or green. Barcode reading was fairly straightforward from this point; it would also\nreturn -1 as the barcode's value if it was not confident of the pixels read or a red pixel was found nearby\n(corresponding to a ball that may be blocking part of the barcode). We also intended to ensure that there\nwas a top wall above the barcode and floor below the barcode, but since our final code did not utilise\nbarcodes, we did not finish this section. Our barcode-reading algorithm was nevertheless fairly reliable.\nSummary and Final Competition\nGood:\n● Scored 7 possession points\n● Arguably had the greatest coverage of the playing field (Explored the entire field).\n● Won the Maslab Engineering Award\n\n● Ball pickup became stuck on the last two balls (could have scored more possesion points but 2\nballs at once were caught in the belt drive).\n● Robot did not find a goal and attempt to score until the last 7 seconds\nOn the contest day, Woody lived up to most of his expectations with the only dissapointment being in\nnot finding a goal to score into until the last few seconds left in the game. Possible solutions could be\n● After 2 minutes - do nothing but try to score. Go into ball ignoring mode (make the ball pickup\npriority very low, and wandering / wall-following priority very high)\n● Have a simple laser sensor that keeps track of how many balls are in the pickup - and goes into\nthe same \"score-or-else mode\" after a certain number of balls (E.G. 3 balls).\nSomething worth noting is that Woody seemed like a crowd favorite. With the Linux Festival Program\nbased Speech Server working flawlessly, all of Woody's debugging output was translated into spoken\nwords picked from a library generated by the team. This made Woody's actions both humorous to\nobserve - and more understandable to the crowd. Throughout the software development, audio\ndebugging output helped greatly while testing code - and did the same on competition day.\nBad:"
    },
    {
      "category": "Resource",
      "title": "teamfifteenpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/9e914d31b7427c342bbd5c03890b87c0_teamfifteenpaper.pdf",
      "content": "Team Fifteen Paper\n\nOverall Strategy\nTeam fifteen took a cue from last year's winners and decided that -whatever comes up, our robot\nneeds to be constantly moving. In order to do this we implemented fast vision and control algorithms\nand a threading structure that allowed for simultaneous processing. We chose a simple behavioral\napproach that can accept goals from steady state machine layouts. This allowed the possibility to use\nmapping if it was deemed applicable, as it was after more planning.\nThe mapping round offered a chance to make a very accurate map of the arena (assuming everything\nwas calibrated correctly and all error was attempted to be accounted for). Using this map is was then\npossible to set waypoints in the contest arena. These waypoints were goals for the robot. It would\nattempt to reach these waypoints, then search for balls in a wall following/object avoidance mode.\nWe decided that our bot would not have any real \"scan for balls\" behavior because it could be\nimplemented to can as the bot was naturally moving and/or avoiding walls. By setting these\nwaypoints, it is possible to guarantee that the bot cover the entire contest arena during the scoring\nphase of competition, and thus not miss any balls.\nAs far as scoring was concerned, team fifteen chose to place balls through the \"field goal,\" or over\nthe wall over the goal. This method of scoring landed our team the most number of points per ball,\nand it also separated the collection and scoring areas of the robot. This strategy combined with an\naccurate map would wreak devastation on other teams.\n\nMechanical Design and Sensors\nA very distinct characteristic of our robot is its laser-cut chassis. As soon as the team agreed on a\ngeneral strategy and plan of attack, we began to design our ideal robot using CAD software. Over the\ncourse of a few days, the robot was completely designed in Rhino 3d. Rhino is an inexpensive CAD\npackage similar to Solidworks. Several design cycles were completed until the final model emerged.\nHaving a design on a computer screen is a far cry from having a working robot sitting in front of you.\nWe virtually disassembled the Rhino model into flat plates that could be cut with the laser. Then, the\nsilhouettes of these shapes were projected onto a plane and exported in a standard .dwg format for\n\nprocessing. We placed beautiful white acrylic plastic on the laser-cutter's bed and clicked print. A\nfew hours later a real robot was sitting on the ground, ready to be programmed. The design cycle we\nchose not only allowed us to have a completely built robot only a few days into the contest, but also\ngave us the ability to focus more on software while other teams were troubled with mechanical\nissues.\nThe robot is completely round, with no part of it extending beyond the circular shape. This ensures\nthat the robot can spin in place and not worry about bits of robot getting caught up on walls and goal\nposts. The bumper design was based on the bumper from the Roomba Robotic Vacuum. iRobot spent\nyears developing bumper geometry with a wide range of motion and maximum impact absorption.\nBy replicating that design, we were able to use the benefits of years of commercial research.\nNeedless to say our robot never got stuck on a wall.\nThe ball lifting mechanism is quite simple. It consists of a rotating drum and a vertical conveyor belt\npowered by a small DC gear motor. The drum is made of foam pipe insulation and is positioned so it\nwill have positive interference with the balls so they are 'sucked' into the robot. The vertical\nconveyor belt is made of carpet liner and rides on wooden dowel rods covered in foam pipe\ninsulation. This creates a compressible surface that is ideal for lifting wooden balls.\nThe balls are stored on the top of the robot until it's time to score. A solenoid holds a spring loaded\ngate arm down while the robot drives around. When the robot is ready to score, power is simply cut\noff from the solenoid, releasing the spring loaded gate and allowing the balls to roll down the slanted\ntop and into the scoring bin.\n\nSoftware Design\nThe software for our robot was modeled after standard robot sensor input software architecture. A\nconstant thread runs in the background pulling updating data from the Orc board at 50 Hz while\ntime stamping each data input so that upper level software threads know not only what data was\nread, but when it was read as well.\nFrom here, the software architecture has two other threads running under the main behavior. A thread\ncontrolling dead reckoning navigation updated the robot's position at 5 Hz. Each loop through, the\nthread would read the raw gyro data, then using calculated quadratic regressions, adjust for gyro drift\nto within 2 degrees a minute. Odometry data was then integrated with gyro data, and from the two,\nan extremely accuracy X, Y position could be derived. The robot, in running a course over three\n\nminutes, would only lose about 6 inches in accuracy.\nThe other loop running behind the main behavior control was the sensor thread. The sensor thread\ncontrolled all low level threshold triggers. It directly controlled private data Booleans such as\nLeftBumper and RightBumper, and converted analog values from the IR sensors to distances in feet.\nFrom the data calculations, higher level behaviors could access its senses without having to worry\nabout overburden the orcboard. The low level sensor thread also includes some low level behaviors\nsuch as obstacle avoidance. If bumped from the left side, the robot would involuntarily stop, then\nbackup three inches, and right a few degrees. Similarly, the robot turns left when bumped right.\nBehavior-wise, the robot was simple, it would map the course by wall following, then, in the second\nround, it would follow walls by tracing its map, and stray to balls that it sees. The robot's mechanical\nfeatures allowed ball following to be quick and imprecise; the robot needed only to roll over the ball\nand didn't need to stop and calibrate based upon the ball's location. In ball chasing mode, the wheel\nspeeds are proportional to the offset of the ball from the camera center. Overall, the philosophy\nbehind the robot behavior was keep it simple, keep it working.\n\nOverall performance\nOn a whole our robot performed well. Although we did not score a very large number of points, three\ncompared to the winning team's 25, most of the components of \"The White Castle\" functioned as\nintended. Those problems we did have were simple mechanical issues that could have been worked\nout given a bit more time.\nOne of the parts of our robot which worked extremely well was our mapping program. During the\nexploratory phase, our robot wall followed very closely, making an accurate map of the playing field.\nIt completed this map within a short period of time, about one minute, meaning that even in a plying\nfield with a greater perimeter, our robot would have finished mapping well within the three minute\nexploratory period. Although we didn't have enough time to make real use of this map, other than to\nshow a pretty picture at the competition, with a bit more work, such an accurate map could be very\nuseful. In this case, the credit goes to Peter Lai, the mastermind behind the map. Another aspect of\nour robot which worked out very well was the laser cut Plexiglas body. The precision cut pieces\nensured a robot which was sturdy and well proportioned, eliminating most of the human error which\ncould have occurred if pieces were hand cut. This also created a very sleek look which, combined\nwith the golden crown adorning the top of our robot, won us the Best Dressed award.\n\nThe ball collection mechanism was one area that we should have worked on more. During the first\nrun of our robot, we forgot to turn on the roll bar that pushes balls into the robot, leading a ball to\nbecome lodged underneath our robot, preventing it from moving. This also meant that none of the\nballs collected were lifted to the upper level. Still, this was less of a problem with or robots design\nand more with our own carelessness. Thus we were given a second chance, during which our robot\nsuccessfully collected balls, pushed them back to the elevator, and raised them to the upper level.\nHowever, as the balls passed though the friction drive, the base of the drive was gradually pushed\ndownward, causing it to catch in the carpet and bring our robot to a disappointing halt.\nOne more crazy awesome feature of our robot was our ball following code. During the practice\nround, the software guided the robot in such a way that it collected nearly all the balls in the arena, a\nperformance that put it above the other bots at that time. Had our robot not gotten stuck during the\nactual competition, we can only assume that it would have performed equally well.\n\nConclusions / Suggestions\nWithout threading the robot will be to slow to successfully complete the tasks needed within the\ncompetition time limit. Threading is absolutely necessary. Plan your threads with thought though,\nand remember that even though something is threaded, it is still sharing CPU power with the other\nthreads in the program. A CPU intensive thread will slow down the rest of your threads.\nThere is certain logic in the scoring / software of the robot. If you can score very well -- that is,\ncreate an accurate map and actually implement this map for scoring purposes -- you may not need to\nscore the most number of points, as this requires some sort of lifting mechanism to get the balls to\nthe top of the robot. Time is tight with the competition, and a good map takes time to get, so scoring\nthree points with every ball may be better than 5 with only 3 or 4.\nThere should be a checkpoint where the bots must score a three or five points. This will force teams\nto get their goal code working earlier, which tended to hurt many teams, as the code did not tend to\nfunction very well. By forcing the teams to find the goal, at least the bots will be able to score and\nmake the competition more fun to watch."
    },
    {
      "category": "Resource",
      "title": "teamfivepaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/c8325f59629e9ab265fb7e55ef450023_teamfivepaper.pdf",
      "content": "Team Five Paper\nFrom Self Flagellation to Michael Craig: A Robot's Journey\nThe Bot\nWe named our robot Michael Craig to replace our lost teammate. Unfortunately, Michael\ndid not perform up to our lofty expectations from the start of the course. Perhaps a\nstronger effort in the first two weeks of the course would have allowed us to finish the\nmechanical systems earlier and to make more progress on the software. Then again,\nperhaps not. In the end, Michael wandered in what appeared to be an aimless manner, but\nmanaged to score three points. It was lucky that we did score, because we chose to tempt\nfate and left the completion of checkpoint two until contest day. But we're gambling men,\nand such a situation was only to be expected.\nOverall Strategy - Keepin' it Simple\nThere were a couple of things that we knew, from the start, that we didn't want to do in the\ncontest. We didn't want to do tasks that would spend time and processing power\n\ngenerating extra data. This meant metric mapping was out. The thought was that most of\nthe details of the data we generated would simply be thrown away. Instead, we thought\nthat the more abstract topological mapping was a better choice.\nSecond, we didn't want to design our robot in such a way as to require accurate movement\nand positioning. This meant that an arm was out. We wanted our robot to thrive in the\nerror-filled environment that was sure to ensue. A large mouth that would allow us to\ndrive in the general direction of balls appealed to us. A precision instrument to grab and\nlift balls did not (though the two arms that we did see were really cool).\nMechanical Systems\nFrame\nWe decided to build the robot almost entirely out of sheet metal. The main reason for this\nwas that it would look cool. Almost all of the sheet metal that we used was purchased\nfrom CMS, as MASLab did not have enough for it to be used for structural purposes. John\nacquired high-bond double-sided tape from McMaster to hold the different pieces\ntogether. It worked well in long strips, but was not so great for small bonds.\nPaddle Wheel\n\nOur robot divided the task of delivering a ball from the playing field to a field goal\nbetween three main mechanical systems. The first was a paddle wheel which was slightly\nrecessed in the frame of the robot. We recessed the paddle wheel in order to keep our\ncamera as close to the ground as possible. To clear the top of the wheel, the camera would\nhave had to be about 5-6 inches high. By recessing the wheel we were able to get it to\nonly about 3 inches off the ground. We decided to use a paddle wheel rather than simply\ndriving over the balls, because we decided that it would make it much easier to feed balls\ninto a lifting mechanism if they were on an incline. In order to accomplish this we needed\nto push them up a small ramp.\nWe constructed the paddle wheel from small strips of sheet metal rotating around a\nsection of brass tubing. We drove it with a high torque motor, which in retrospect was\noverkill. One problem that we foresaw was that at certain angles the wheel would push\nballs straight into the ground. We spent a lot of time worrying about this, but it turned out\nnot to be a problem at all. Since we were always driving forward over balls, they never\nonce got stuck during testing or the contest.\nScrew\nThe second mechanical system was an archimedean screw. We used a single screw which\ntrapped balls against two bars. The shaft was aluminum with 1 inch brass supports. The\n\nhelix was constructed using surgical tubing from Home Depot. We drilled holes in the\ntubing and superglued it to the brass supports. The screw was also driven by a high-torque\nmotor. It was able to elevate four balls at once.\nAqueduct and Trapdoor\nOnce again, aesthetics and bad-assness played a major role in our design decisions. Rather\nthan make a simple tilted container for the balls, we decided that it would look much\nbetter to have a long, winding aqueduct on top of the robot. The initial idea was to use\nPVC piping, but John decided that bending yet more sheet metal to his will would be even\nbetter.\n\nThe trapdoor was the result of running out of sensor points. Ideally we would have used a\nservo to open and close the door. Instead, we had a mechanical system that was\nimprovised on the last day before impounding. It was a one-shot affair, with a spring\nholding the door closed and a bar acting as a switch. When the bar hit a field goal upright\non either side, it pulled the trap door open.\n\nWe didn't use many sensors, because we had used most of our points on the two high\ntorque motors for our capture and elevation systems. We had two short range IR sensors\non the sides of our robot. These pointed forward and were mainly used for obstacle\navoidance. We also used optical encoders to help with straight driving.\nSoftware Design\nRobot Control\nWe used optical encoders to dynamically adjust the speed of each wheel when moving\nforward and backward. We decided that a PID controller was not necessary since we\nultimately could drive straight without it. For turning, we simply used the gyro. This was\nfairly accurate for angles between 30 and 90 degrees. It was not accurate for small angles\nexcept on very low speeds. This was because we could only access a gyro reading 20\ntimes a second. For medium to high speeds, this made the error on angles under 20\ndegrees prohibitive.\nRobot control turned out to be one of the biggest surprises of MASLab. We realized it\nwould not be simple, but it turned out to take up an enormous amount of our time. Simply\ndriving straight and turning a requested angle were not simple tasks. The motors never\nbehaved how we thought they would. We inititially did not want to use encoders, but soon\nSensors\n\nrealized that if we did not, we would not be able to rely on the robot going where we\nwanted it to. A pleasant surprise was that they worked really well, and we never needed to\nfiddle with them.\nWe decided fairly late in the course that we would use current sense to detect bumps. This\nintroduced a whole new set of problems. We had a set of constants calibrated for each\nspeed, which indicated when the current was a result of normal driving and when it meant\nthat the robot had hit something. Every time we made a major change to the structure\n(which was all the time during the last week) we had to fix these constants. In short,\ndriving was a major headache and seriously impeded our progress in other areas.\nImage Processing\nThe image processor on the robot ran at a peak of 12 frames per second. The final speed\nwas lower than this as we started scrambling during the last week and never got around to\noptimizing. Barcode recognition proved to be the only non-trivial task in this area (in fact,\nit was very far from trivial). We ended up using the low-resolution image to define the\narea of a barcode, and then taking a high-resolution picture and analyzing only that area.\nA problem we ran into was that if we were moving, the high-res picture would be slightly\ndifferent than the low-res. The only way to fix this was to stop when looking at a barcode.\nWe didn't want to do this, but we had to, as correct barcode recognition was absolutely\nessential to our mapping system.\nAnother feature of our image processing was that we were able to estimate the distance\nand angle to an object based on its size and position in the picture. The estimates were\naccurate enough to get balls and drive near barcodes.\nOne thing our image processing never managed to do was to determine what the\norienation of a goal was. This didn't turn out to be a problem as we never got close to\nscoring a field goal.\nMapping\nWe decided against metric mapping. In fact, we never had any desire to do anything close.\nOur biggest thought on metric mapping was that we would have to pay an enormous\namount of time and processing power to generate and upkeep a set of specific data. The\nproblem was that in the end, this data didn't seem all that useful. Instead, we sought to\ncreate a topological map of the playing field. We would note the different barcodes and\nconnect them using breadth-first search. We had plans to note the ball density around each\nbarcode, but this went on the backburner.\nWe ended up with accurate mapping that correctly determined the barcodes present and a\n\ngood portion of the links. However, due to lack of testing time, we never used our maps.\nWe were concerned about our ability to reliably travel through more than one link.\nBecause we didn't test, we also weren't sure it would save time over random wandering.\nScoring\nOur initial plan for scoring was to use our map to ensure that we travelled to all of the\nrooms on the field. We would weight the different barcodes on the map based on their:\n● ball density\n● proximity to robot\n● proximity to a goal\nThe third value would be weighted higher as the round progressed. We would use this to\ndetermine which barcode we should travel to next (and the current room would be\nincluded in this computation). At a certain point, we would drop everything, go to a goal,\nand try to score.\nThis all went out the window when we realized we were out of time. We ended up\nwandering randomly. With another 1-2 days, we could have figured out some basic uses\nfor our topological map. With another 3-4, we could have generated this complex system\nto compute the value of each of our options at any given point.\nSuggestions for Future Teams\nDriving Good is Hard\nDo not underestimate the problems you will have in this area. Get encoders, even if you\nare not doing metric mapping and don't care where on the field your robot is at any given\ntime. You will care about driving straight at some point, we guarantee it. That is all we\nused our encoders for, and they were invaluable.\nBe prepared to spend tons of time on this.\nGet Your Mechanical Stuff Done\nOne of our biggest problems was that we couldn't do testing. Our pegbot had issues that\nwe resolved with the final design, such as wheels sticking way out of the frame and\ngetting stuck on stuff. As a result, we couldn't do useful testing of our software for much\nof the time we had. We finished our mechanical systems about an hour before\nimpounding. We tried to test as much as we could before then, but it wasn't nearly as\neffective as it could have been.\n\nHaving a near final robot by the end of the second week would have been real nice.\nSmall and Simple\nOur robot, though it performed all of its intended functions well, was too cumbersome and\ntoo slow. If given another chance, we would have ignored field goals and gone for a short,\nlight, fast robot. To succeed in this contest, one doesn't need to do it all. Just make sure\nyou do what you do well. Here's an example of a robot that didn't do anything well. Our\nprototype, lovingly dubbed \"Sh*tBot v_1.0\". Notice the distinct lack of front wheels, and\nthe excess amount of masking tape, often serving structural function.\n\nTake Advantage of the Staff\nYes, this is exactly what it sounds like. And in addition, the staff is amazingly helpful and\nfriendly. Don't hesitate to ask them for help when you get stuck."
    },
    {
      "category": "Resource",
      "title": "teamfourpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/adac9eceff0814ef113e3ece01e62a5b_teamfourpaper.pdf",
      "content": "Team Four Paper\nOverall Strategy\nCoilette is a bulldozer robot, with a shaped shovel in the front designed to gather balls and\nram them into the mouseholes. Designed for turning in place and high speed forward\nmovement, the robot was controlled by a series of behaviours to gather balls and deposit\nthem in the goals.\nMechanical Design and Sensors\nWe chose to construct Coilette out of foam PVC due to its strength and ease of\nmanufacturing our design. In particular, using a heat gun we were able to create arbitrary\ncurves in the material, which we used to sculpt an outer circle, a raised front array of\nsensors, a strong angle bracket for the compute, and a complex curved surface for the\nshovel. According to usplastic.com, \"this PVC sheet has excellent corrosion resistance and\nweather resistance. The working temp is 33 deg F to 160 deg F. and the forming\ntemperatures of 245 deg F. It is good electrical and thermal insulator and has a self-\nextinguishing per UL Test 94. PVC applications are almost unlimited. It's the most widely\nused member of the vinyl family. It is excellent when used for corrosion-resistant tanks,\nducts, fume hoods, and pipe. Ideal for self-supporting tanks, fabricated parts, tank linings,\nand spacers. It is not UV stabilized and has a tolerance of +or 10%. Not FDA approved\nmaterials.\" The only reservations we have about this material are the following:\n● It produced itchy white fine dust wherever we machined it. This dust was\nparticularly annoying when it got into our eyes.\n● A TA at maslab, told us we will probably get cancer from it.\nBesides these problems, it was an awesome material.\nOur design had no motorized parts except the initial pair of wheels, so we used our sensor\npoints to make a large array of IR sensors in front to keep it moving without colliding\nmuch with the walls. We wanted to go fast and be cool like sonic. Sonic has no motors!\nWe chose to build a cylindrical robot because it enforces better configuration space\nmodularity and ensures turning in place is safe. There were some implementation details\nthat caused this to only be partially true, but for these cases it was able to escape with a\nsimple left and right turning timeout. This shape did not guarantee our robot would not get\nstuck, but it certainly helperd us out a little!\n\nThe sculpted front shovel was designed to match our desired mobility. The back side of\nthe shovel had a depression so that the lowest potential for a given ball is at the center of\nthe shovel. That way Coilette can expel balls into the goal by driving straight and stopping\nrapidly. Turning moves the balls to a particular side of the shovel, so wings were added to\nthe front sides to prevent balls from being lost when turning. It worked well for our\npurposes, but testing showed sustained turning wasn't able to control more than 2 or 3\nballs.\nThe front had an array of IR sensors to facilitate forward movement. 6 IR sensors: 45, 20,\n0, 0, -20, -45 degree angles respectively. The two straight sensors are at different heights\nto find goals without need for vision (one is above 6\" and the other is below -- so they will\nreturn significantly different values when pointed at a goal). Due to time constraints, they\nwere merely used for redundancy in our wandering algorithm, as we only used vision for\ngoal detection.\nSoftware Design\nOur software was written as a single threaded driving loop that queries its input devices,\nIR sensors and processed images, and then acts on the first behaviour that subsumes the\nrobot at that time. For this project our behaviours were simply layered Boredum, Score\nGoals, Chase Balls, Wander with the first behaviour that has an action controlling the\nrobot. In heinsight, swapping the order of the goal scoring and ball gathering may have led\nto better performance, since we would attempt to have all known balls before going after\nthe goal.\nAI Behaviours\n1. Boredum -- Boredum's purpose was a catch-all errors kind of thing. It was added to improve our\nperformance with blind-spots and turned out to interact well with scoring goals and gathering\nballs near walls. Boredum used the difference between every few processed camera image to\ndetermine whether the robot was not moving. This was chosen after current sensing proved\nunreliable without excessive modeling of the robot's current state (desired PWM to motors,\ncurrent voltage of the batter, averaging over time, etc). A quick and dirty implementation of\nimage difference turned out to be more than enough because it had effectively no false negatives\n(it wouldn't subsume the behaviour layers in the middle of gathering a ball, for example), and the\nfalse positives were recoverable (if the image was changing enough, even though it was stuck\nagainst a wall, it generally meant that while the robot was dragging one side against a wall, it was\nstill moving forward). However, as I noticed during the exhibition rounds, it turned out not to be\ngeneralizable to other enviornments. In particular, it would sometimes observe other robots\nmovement and believe it had made progress. I believe with an improved image differential\nalgorithm, it would be able to overcome this.\n2. Score Goals -- If the camera sees the top bar of a goal (best fit by a 7:1 ratio yellow bounding\n\nbox) it accelerates towards the center of the goal -- and the first frame it drops out of vision off\nthe top, it tops out its speed and all non-boredum behaviours sleep for half a second. This worked\nbecause of the design of the shovel in the front -- the balls would gather in the middle of the\nbackside because of the forward momentum and some would roll out upon collision. Limitations\nof the motors speeds prevented the proportional controller from ever reaching a point where we\nneeded to make it a differential controller, since the motors' top speed was our coefficient. It did\nappear to hit off-center on the goal fairly regularly, so perhaps it did need an integral controller.\nBut what we had was simple and worked for our purposes.\n3. Chase Balls -- If the camera sees a red ball (best fit by a 1:1 ratio red bounding box) (selecting\nlargest if multiple observations occur) it drives proportionately to the x-coordinate of the center\nof the region. The actual output to the motors was very similar for ball chasing as for goal\nchasing, but since balls remained within its field of view until they were inside the shovel, it\ndidn't have any problems not being centered like the goal scoring behaviour did.\n4. Wander -- If any of its sensors sense a nearby wall it turns away from the wall, otherwise it\nmoves towards the largest measured distance. While we didn't use PID controllers for any of our\nlayers, for this behaviour it is important not to use one because the camera is our primary means\nof observing balls and goals it is important to change what it is looking at when nothing\ninteresting shows up. This also results in a more thorough observation of the field than a wall-\nfollowing or center-following algorithm.\nAction Selection\nOne behaviour drives the robot at any time -- the first behaviour that returns a desired\naction in the order listed above; however, if either desired motor speed is non-negative\nand lower than its current motor speed, both motors speeds are weighted with the current\nmotor speed. The purpose of this is to prevent sudden stopping as a result of AI decisions.\nCamera Input\nImage Processing\nWe performed a decent amount of precomputation on our image and reduce its data down\nto a managable size for the behaviours to interact with. First, it does horizontal edge\ndetection in the HSV domain by looking for significant differences in the any of the three\n(hue was the most discriminating, as individual objects are all monochromatic, value was\nsecond most to distinguish between two balls that are overlapping in our vision). Then we\nhave a large number of line segments, the color for each was averaged to reduce the\namount of noise in the color system and enhance the differences between objects. Finally,\na color lookup table converts each color to an enumerated list of colors to be used for data\nassociation (Red, Green, Black, Blue, White, Yellow).\nData Association\n\nImage processing returns a list of line segments in colored buckets and a count of how\nmany line segments have which x-coordinate as their center. Data association attempts to\nmatch up multiple line segments of the same color that are centered roughly at the same\nheight. For the actual competition, we only extracted 3 types of data, balls (1:1 red), goal\nsides (1:3 yellow) and goal tops (7:1 yellow). But in the past it extracted barcodes as well\nby the same system.\nOverall Performance\nCoilette placed 3rd in the final competition, but more importantly she performed in a\nmanner we felt was more interesting than other robots. She may not have gotten the same\nkind of reaction from the audience (due to the continuous nature of Coilette, no particular\ninstance was particular amazing or hindering), she was the only robot to thoroughly\nexplore the map, the only one to score a ball through the mouse hole, and managed to\ncompete against robots that were much larger than she was. Because she is so small, and\nnot hindered by mechanical mechanisms, she will be easy to mutate into an awesome pet\nrobot.\nConclusions/Suggestions for future teams\nOne of the biggest downfalls of coilette was the small opening for gathering balls. While\nchosen specifically for this robot to improve ball control with no motors, it really couldn't\ngather balls as quickly as the ones that had a foot wide entrance. But it sure was cool to\nhave an awesomely small robot! Also, it is unfortunate that we will get cancer, according\nto TA."
    },
    {
      "category": "Resource",
      "title": "teamfourteenpapr.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/68795e23c0736bc2a264c85760623f60_teamfourteenpapr.pdf",
      "content": "Team Fourteen Paper\nOverview\nThe experience in Maslab of my team was excellent, occupying and stimulating; the\nproject in general greatly exceeded all our collective expectations, both in terms of\ncommitment and reward. Inevitably, we made many mistakes and concurrently acquired\nenormous amounts of information on everything from Java to managing team dynamics\nand from mechanical structure to computational objectives and implementation. We ended\nup spending approximately 40 - 50 hours per week per person on our task, and spending\nadditional time would have been beneficial. Our team had minimal experience both in the\nmechanical implementation of the rigor of a robotics course and with computational\nrequirements of robotics in particular. This course is certainly feasible for groups and\nindividuals of no experience, but in such instances, we strongly recommend a certain\ningenuity and willingness to explore alternative possibilities and paradigm shifts.\nOverall Strategy\nWe reasoned that the winning strategy would implement field goals, since their score\nyield (5 per ball) was greater than traditional goals (3), and we hoped to win rather than\nmerely excel. To that end, we fashioned our robot around the requirements of detecting,\ncapturing, lifting, and scoring balls, within the constraints of fitting our robot into the\nstorage box and within the various time restrictions and material limitations.\nMechanical Design\nStructurally, our bot was divided into three horizontal tiers; their precise configuration\nemerged as a consequence of our design needs. In general, our chassis began with a\nrectangular platform twenty centimeters wide and thirty centimeters long with two motors\nmounted on the back corners and two free wheels mounted on the front corners.\nWe divided the mechanical problem into three parts: collecting the balls, lifting them, and\ndepositing them in the goal. To collect the balls, we found inspiration from a Wisconsin\ngirl in our dorm and mounted a 'bailer' on the front of our robot akin to the mechanism\nused to collect wheat and form it into bales; the mechanism consisted of a servo rotating a\nhorizontal bar about twenty centimeters wide approximately five centimeters from the\naxis. We installed a ramp slightly behind the bailer, which lifted the balls up\napproximately four centimeters onto the bottom level of the bot. We had to spend\nconsiderable time calibrating the spacing and inclination of the ramp with respect to the\n\nbailer so as to achieve a path that a ball would always follow regardless of approach and\ninitial velocity, but we succeeded; surprisingly, the angle of the ramp was very steep, but\nin general, any ball in the path of the bailer would be collected. The bottom tier was\ninclined slightly toward the front, so after the balls were collected by the bailer and ramp,\nthey would roll to the back of the bot. The second mechanism had to lift the balls from the\nbottom tier to the top tier, which was at its peak approximately thirty-five centimeters\nabove the ground; we resolved to use an arm powered by a high-torque motor mounted\ndirectly on the robot superstructure. Positioning the arm was difficult; it had to be\nsufficiently compact to fit in the box, long enough to raise the ball to the appropriate\nheight and positioned correctly to avoid crashing into any other parts of the robot -\nclearance was a factor. The 'ball end' of the arm had a short metal cylinder we referred to\nas the 'cup;' the ball would firmly stay in place due to minor shaping adjustments we\nimplemented. The arm only rotated through 180 degrees; it would hit 'bumpers'\npositioned at the desired limits of motion. The motor was slightly off-center with respect\nto the back of the bot along the horizontal axis. The third mechanism required the fewest\nmoving parts; the balls were at this point thirty-five centimeters off the ground and had to\npass over a thirty-centimeter tall wall; we had a gentle, constant incline that took the shape\nof a wedged siphon at the back of the bot which gave way to a slender, single pathway\ncomprised of a number of dowels. A servo powered barricade restrained the balls in the\nsiphon until such a point as we desired them to move. The final ramp extended over the\nbailer at the front of the robot, so the robot did not have to drive all the way up to the wall\nfor the balls to have clearance. Given the three mechanisms, then, the chassis and\nremaining hardware structure of the bot became little more than a mounting platform for\nthe ball transporting devices, the computational hardware and the sensors. We constructed\nit using three vertical columns between the bottom tier and the top tier; a middle level was\ninstalled to mount the Eden, battery and Orcpad, leaving the bottom level completely free\nfor the balls to roll to the back of the bot, where they would be lifted by the arm. Because\nof small but annoying misalignments in the construction of the bot, different parts of the\nbot were slightly crooked, but in general the construction was sound. Wood was our\nprimary material, with metal sufficing for parts which were required to be especially\nsturdy, especially precise, or especially light.\nSensors\nOur primary navigational sensors were a pair of short-range infrared sensors mounted on\ntwo structural columns near the front of the bot. The distance along the length of the robot\nfrom the front to the infrared sensors was approximately ten centimeters, which we\nbelieved was sufficient to avoid the non-linear behavior of the sensors as the range\napproaches zero. The sensors were mounted high on the columns - sufficiently high to\navoid being inadvertently triggered by either our spinning bailer or by gaps at the goal.\nThe camera was mounted between the IR sensors with slight declination, and with an\nelevation of about twelve centimeters with respect to the ground. This caused some\n\nproblems; as the robot closed on a ball, the ball would drop out of visual range. This made\nmodifications in our algorithms necessary. Also critical for navigation in the original\nconfiguration were two encoders mounted on our drive wheels and the gyroscope\nsupplied. Additional motor output came from the servo on the bailer, the servo governing\nthe barricade on the release tier, and the high-torque motor on the back of the bot.\nSoftware Design\nBecause we had two primary programmers, we relegated the division of code into\nmodules based on either sensory input, processing and interpretation or motor and\nhardware response. We considered threading, but decided that the vast bulk of the\nprocessing would be in a single thread, with minor secondary threads to monitor such\nsmall tasks as the clock, rotating the back arm, and Our primary sensor input besides the\ninfrared sensor navigation was the camera; processing was executed in HSV format; we\nimplemented an HSV function which was more efficient than the provided procedure.\nRather than completely converting every pixel in the image, it sampled regions in a search\nfor red pixels (or other colors, depending upon our needs); we incorporated blue-line\nfiltering to negate the wall. While the input code was initially more complicated to digest\nand accurately plot the positions of objects on the field, once odometry was abandoned,\nour image needs vastly simplified. Our initial intent was to construct a powerful map\nbased on the gyroscope, odometry, and landmark recognition, loosely founded on\nKallman odometry. We devised a method to drive straight by providing feedback to the\nmotors' power settings based on their odometry rates; turning hinged upon our gyroscope,\nwhich was fantastically temperamental. Retrospectively, calibrating the gyroscope with a\nregression function would have been fantastic, but we did not believe the massive\ngyroscope drift was entirely deterministic. To proceed once our bot had constructed the\ndetailed map during the exploration round, we devised a series of algorithms to plot\nwaypoints for the robot to follow in an effort to maximize our search efficiency and to\noptimize our red ball and goal finding processes. A more sophisticated wander algorithm\nwas implemented to avoid the shortcomings of wall-following and of random wandering;\nthe algorithm divided regions into cells based on the positioning of the walls, and then\nthoroughly evaluated each cell to determine whether a ball was present by turning slightly\nmore than the angle required to dodge walls and turning 360 degrees at calculated\nintervals. Unfortunately, during the testing of this powerful series of algorithms,\nseemingly overwhelming problems emerged; the behavior was nothing like designed.\nAfter nearly a day of debugging and little progress, on the Wednesday before competition,\nthe algorithms were abandoned in favor of simpler structures which we believed were\nmore reliable, largely due to their simplicity. Unfortunately, because of time constraints\nand hardware malfunctions with the encoders, the odometry was completely abandoned.\nOur basic structure was a looped finite state machine; a wander algorithm would run,\ntaking pictures intermittently until a red ball was located. At that point, a getRedBall\nfunction would retrieve the ball, returning to the wander algorithm. After two minutes had\n\nelapsed, red ball operations would terminate and the bot would seek the goal again using\nthe wander algorithm. This was a vastly simplified structure to our original, more\nambitious configuration. The new wander algorithm was vastly simpler, merely\nencountering and turning away from walls, with a periodic 360 degree turn; it lacked the\nhigher order functions that were possible with accurate odometry and was effectively a\nrandom algorithm based on the positioning of the walls it encountered. Also, we\ndiscovered shortly before the competition that one of our two infrared sensors was\ncritically miscalibrated, such that its behavior no longer linearly varied with distance. It is\nlikely this error contributed to the failure of our original algorithms; there were no\navailable replacement sensors and time quickly ran out before we were able to counter this\nproblem. Unfortunately, we still had not determined a viable solution to the gyroscope\nproblem (a problem we had hoped odometry would have been able to assist), so our\nnavigation had critical errors going into the final competition and turning was erratic.\nOverall Performance\nWhile our theoretical design was well-thought and extremely robust, our execution\nencountered often insurmountable problems. Our hardware design malfunctioned because\nour the gears of our high-torque motor stripped away, making any hopes of a complete\n180 degree rotation impossible. Because we failed to solve that problem in the last few\ndays, as well as the gyroscope and infrared sensor bugs, our robot had very little chance of\nactually scoring during the final round. Moreover, in an effort to get the simple algorithms\nup and running for the competition, the time stream was neglected while attention was\ndiverted to the wander algorithms, so our robot would not universally stop after three\nminutes. As it turns out, the only way our robot would have come to a stop would have\nbeen had it successfully located a goal in its field of view between t = 170s and 180s. That\ndid not happen during the competition, and we were penalized. The pragmatic success of\nour robot was severely lacking, but we generated many theoretical and interesting\napproaches to the obstacles which faced us. Obviously the general failure of our bot to\ncomplete its objectives was extremely disheartening, but the practical experience which\nwe obtained in addition to mentally attacking the problem was stimulating and highly\nrewarding.\nSuggestions for future teams:\n● When you design an algorithm or mechanical solution, it doesn't have to be the optimal or\nperfect solution, it merely has to work. Once you have a working implementation, you can\nimprove it enormously.\n● Make sure you have something working very early on; if you attempt the impossible and find\nyourself incapable of completing your task before the deadline, your substitute will lack many\nimprovements and refinements you solved very early on.\n● Test ridiculously frequently and refine everything as many times as you can reasonably afford.\n\n● Delusions of grandeur and computing excellence have no place in the pragmatic world; a solution\nto the problem must be implemented before idealism can hope to survive.\n● Organize early on and outline clear objectives for everyone on the team. Address conceptual and\ndesign problems together, then implement them solitarily.\n● Try to work on designs long before you need them; start building the chassis as early as you have\nclear objectives and designs, e.g.\n● Invest as much time as you can humanly afford from the beginning.\n● Always have a backup plan; Murphy was right.\n● When in doubt, use a regression equation."
    },
    {
      "category": "Resource",
      "title": "teamninepaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/28c0558275f4a0e7a880597a9d246104_teamninepaper.pdf",
      "content": "Team Nine Paper\nIntroduction\nMASLab contest over IAP presents the participants a difficult problem to solve--\nbuilding an autonomous robot with vision to score points with balls. Even though the\nessence of the contest can be summarized into a one-liner, the complexity of the problem\nis on the other extreme. Much of the difficulty of the problem comes from the time and\nresource constraint. You only have four weeks to solve the problem at hand. Attending all\nthe lectures and mock contests, participants realistically have only three weeks. Time is a\nbig issue for this contest. This report will present solution that the team came up with\nafter putting in roughly one thousand hours during IAP. Even though the result was not as\nexpected, the Legend of Drunken Panda will continue.\nOverall Strategy\nThe overall strategy of our robot dealt with essentially three different problems; how to\nallow our robot to navigate the unknown playing field and identify objects successfully,\nhow to capture the red balls, and how to score in the goal area. The problem dealing with\nthe navigation of the unknown playing field and identifying objects was by far the most\ncomplex and dealt with the integration of software, sensors, and structural design. The\ncapture and scoring problems were ones which were mostly mechanical situations. At the\nbeginning of the design process our team sat down and discussed a game plan to solve\neach of these problems.\nIt was inherent from day one that we would be using our camera to deal with identifying\nobjects. The obvious color differences between balls, walls and goals quickly gave us a\nmethod of identifying one object from another. However, there was still the issue of\ndepth perception and navigation. We decided that a combination of IR sensors could be\nused to give our robot a strong sense of how far away it was from a wall. On top of this\nwe decided that we would have our robot scan the field with it's IR sensors to make a\nmap. To navigate about the field we decided that we would use a combination of a gyro\nand quadraphase encoders to enable us to determine the changes in direction and\ndistances traveled by the robot. To bring all of this information together in an efficient\nmanner it was decided that our software structure should make use of a multi-threaded\nbehavioral model control system.\nTo solve the issue of capturing the red balls it was decided that a mechanism that could\npick up the balls continuously. The hope was that this would cut down on the time spent\ncapturing the ball. To accomplish this it was decided that a belt system would be used to\nraise the balls up a series of ramps and into a storage level. This belt would continuously\nrun so that the robot simply had to run over the balls.\nTo maximize our score, the balls were to be put through the field goal. To do this a ball\nshooter was designed. However, if the ball shooter mechanism proved to be incapable of\n\nproviding the forces desired, then a chute would be used to allow the balls to roll through\nthe mouse holes.\nMechanical Design\nFor our robot, we planned on integrating various mechanical modules to create an\ninteresting robot. The idea behind creating mechanical modules is just like the idea of\ncreating modules in our programming. Each could be independently modified, and the\nperformance on a particular module could be tested before all the rest were un and\nrunning.\nOur first most interesting idea was to try and create a ball shooter like in a tennis ball\nlauncher or like the spinners found in a Matchbox car set. To create this module, we first\nrealized that we would need a high speed motor in order to spin the wheels fast enough to\naccelerate a ball between them. The difficulty in creating this module is that it was hard\nto prototype. With our other modules, we could make a quick inefficient models of what\nwe wanted by using the drill press and modifying some pieces of hardboard, but with the\nshooter, inefficiencies would quickly stall the high speed motor we needed to shoot a ball.\nOnce we acquired a motor and some wheels to use for the shooter, we carefully designed\nall the gear spacing and the structure that we would need to act as bearings for the shafts.\nThe whole design went through two iterations before we had shooter wheels that had\nalmost frictionless turning and a properly coupled motor. Once the module was\ncompleted we gauged it at being able to shoot at ball a little over 10 ft/s. Sadly, since we\nfinished this module only four days before contest, we didn't really have the time to\nmount it properly to our robot and use it in the contest.\nOur second most critical module was a roller assembly that used a belt to pull balls up to\nthe top of our robot. This module, unlike the shooter, was easy to prototype. Once we had\nour basic design, we made rollers on a lathe to dramatically better our rolling, and then\nwe made a housing out of sheet metal by drilling some holes our using a mill. The roller\nassembly encountered the classic problems that come with all belt assemblies: drift and a\nlack of belt tension. Due to a lack of materials, we couldn't make the belt out of a nice\nrubbery material, so instead we used some backward duct tape stuck to itself. The choice\nof material was not ideal because the tape had some viscoelastic effects, but as a tradeoff,\nthe stickiness proved useful in gabbing balls and carrying them up the belt. On\ncompetition day the belt itself ran into a completely new set of difficulties and ended up\nsticking to itself in an unexpected way.\nSoftware Architecture\nOur original plan for the software was to implement a multithreaded behavioral model\ncontrol system, where sensor data would be fed into a set of behavior modules, which\nwould then output information and instructions to each other and to actuator modules. As\nthe code evolved, however, the structure of the modules came to resemble the classical\nsense-plan-act model, as illustrated below.\n\nEach module was a self-contained Java thread responsible for a single behavior. To\ncommunicate with other modules, we designed a thread-safe Edge class for storing\narbitrary Data Packet objects; each module had a list of incoming and outgoing edges,\nfrom which they could read or post information. For example, the Sensors module had no\nincoming edges, but had outgoing edges to nearly all of the other modules. Every 100ms,\nthe Sensors module would poll the robots sensors and write a new SensorDataPacket to\neach of its outgoing edges. Other modules could then check if their incoming sensor edge\nhad changed, and could read the information and process it, likely resulting in the posting\nof some new information.\nThis software model had the advantage of being very easy to test and to extend. It was\neasy to test because each module could be individually tested using JUnit software by\nsimply supplying false inputs to the module and then reading its outputs. To extend the\nsoftware, you just needed to wire the new module into the system as if adding a\ncomponent into an electrical circuit. An example of the flexibility of this system was our\nimplementation of our Exploration and Play rounds. In the Exploration round, we simply\nremoved the GetBall and Score modules (and their corresponding edges) from the\nsystem; this way, our robot would still Avoid, Explore, and Map, but it would ignore\nballs and goals. Then, during the Play round, we added these modules back in.\nThe only real difficulty with this software model was the heavy threading it required. Our\nteam had limited experience with Java threading, causing us to write code which was\n\nlikely not completely thread-safe. As a result, we sometimes found we could \"fix\nproblems\" by polling the sensors less often (to reduce the likelihood of race conditions),\nand our log file often had statements out of order, making debugging more difficult.\nThe other notable feature of our software was our exploration and mapping code. To\nexplore the map efficiently, we would have our robot execute \"exploration circles,\"\nwhere the robot would spin 360 degrees while recording data from the front-facing IR\nsensors and camera. This data could then be filtered to produce a local occupancy grid\naround the robot. Using a heuristic, we could then determine which direction the robot\nshould travel in next, favoring directions which seemed to be more open and which were\nfarther away from previously visited points. We also stored this local occupancy grid into\na global grid, allowing our robot to build a map of the environment. This map was\ncombined with feature information from the camera, enabling the robot to request the\nshortest path (determined using A*) to the nearest goal or ball.\nVision Software\nSimplicity and speed is the design motto for the vision portion of the Panda. The main\nclass files are: PandaVisionModule and various Feature classes (Red ball, barcode, and\ngoal). PVM is responsible for capturing images from the camera, locating the\nfeatures, and estimating distance and bearing. All these information will be posted to\nother interested modules such as Score and GetBall in the form of VisionDataPacket.\nFirst step in improving speed is using the underlying data buffer of the images. Calling\nimage.getRaster().getDataBuffer().getData() returns an integer array that is compact and\nfast to access. Traversing the array is really fast as compared to using the getRGB method\nof BuffereImage. The final version of the PandaVisionModule can process an image of\n160x120 pixels in 30 milliseconds. With this kind of speed, the software can either run\nimage analysis frequently or save computation time for other parts such as threading.\nFeature identification relies on color identification. Various thresholds are determined to\ndistinguish red, green, black, and so forth. Then, it is just a matter of traversing through\nthe array to determine the region of interest. PandaVisionModule uses the expanding\nrectangle approach as resented in vision lecture to identify interested color patches. That\nenables the Panda to see multiple balls, barcodes, etc. Estimation of bearing and distance\nis solved by fitting exponential graphs to data collected from the camera. Blue line\nfiltering was also implemented using the same tricks in feature identification.\nThe final version of the PandaVisionModule excelled in redball identification and blue\nline filtering. However, goal and barcode identification weren't solved due to time\nconstraint. Though they could be properly identified in the analysis all the time,\nextracting useful information from them wasn't perfect. Those information were right or\nleft goal post and reading from an incomplete barcode.\n\nTime worked against the team throughout MASLab. It proved impossible to do design\niterations as taught by the institute. The team did all it could to finish the robot, but much\nof it was still untested on contest day. All in all it was a good character-building\nexperience. The hours were long and there was little support from the staff besides\nreplacing defected parts. If anyone is looking for a good hacking activity for IAP,\nMASLab is it.\nSuggestions\nFor the future participants--you can forget about those good design iteration principles\nyou learned from engineering classes because you don't have time. This contest isn't\nabout building a perfect robot by the end of IAP. It is about presenting something that\nworks. It might not be perfect, but at least it is functional. That goes for both hardware\nand software. Hardware needs to be done a week before the contest so the software can\nbe tested to work out the kinks. You should build only two robots, the PegBot and the\nfinal one. And of course, if you are a genius and hardcore hacker who doesn't sleep,\nplease ignore all the above suggestions.\nFor the staff--please be more organized. Putting stuff on WIKI doesn't mean you can run\nthe course remotely. It takes personal contacts to announce key information. Also, move\nthe guest lectures into the first week since most people need the time at the end to finish\nthe robot. The mock contests are waste of time since you only get to run your robot once\nin three hours. What kind of productivity is that for testing? Also, just make the lectures\nonline since they are pretty self-explanatory. That's my two cents.\n\nConclusion"
    },
    {
      "category": "Resource",
      "title": "teamonepaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/fc304099439de486a65f75bcedbbff83_teamonepaper.pdf",
      "content": "Team One Paper\nFinal Paper\nOverall Strategy\nOur Robot is controlled by a finite state machine structure. Each state has exit conditions which are\nchecked periodically. When an exit condition is satisfied, the robot will change state to that specified by\nthe satisfied exit condition. The process repeats in the next state until three minutes are up. As long as we\nhave no balls we will always be wandering until a ball is found, when we will pick it up. If we have 4\nballs within the first two minutes we will transition to searching but now for a goal in which to score. It\nwill also transition to scoring if we have a ball, and we have a minute left to try to score the balls we have.\nIf we have no ball with a minute left we will still look for a ball, pick it up and try to score before time\nruns out.\nAs the robot initiates, it ensures the arm is in its protected position (up) and enters the wandering state\nmachine where it will drive forward for 5 seconds unless an exit condition is fulfilled. One exit condition\nis if one of the IR sensors reaches a threshold value where it then will turn slightly away from the obstacle\nuntil the IR's move away from their thresholds. If both IR sensors trip we turn more substantially to\ncompletely avoid the danger. Also inside the wandering state is a spin random state where we try to\nchange our heading a bit to avoid going to the same places over and over again.\nThe wandering state machine will transition to the ball pickup state machine if a red ball shows up on our\ncamera. We then use the balls horizontal position on the image and center the ball on the screen. Then\nmove toward the ball until we are a set distance away based on the height of the camera off the ground\nand on screen position. As we do this we are still running a thread looking at the IR sensor reading and\nwill avoid danger is the IR sensors trip by shimmying left or right. A shimmy is a series of movements\nthat end up moving a robot horizontally left or right of its previous position. After a shimmy we again\ncenter the ball on screen and go in to pick it up. Once the ball pickup state has picked up a ball we then go\nback to wander to look for more balls or score depending on how many balls we have and time left in the\nround.\nThe other large state consists of goal scoring. When a goal has been spotted, enough balls have been\ncollected and the time is right the robot will go after the goal. Since our arm is sensitive our only approach\nto goal is from an angle with the ball hopper leading forward. The robot will shimmy until the angle\nbetween the camera and the center of the goal has reached a threshold that will score and protect the arm.\nAt which point, we ram the goal and then back up one wheel to rotate the robot and try to unload all balls.\nAfter this action the robot will return to wandering and set our ball count to zero.\n\nMechanical Design\nThe mechanical design of the robot was divided into 3 main divisions: The chassis, the gripper arm, and\nthe ball delivery bin.\nThe mechanical design proved to be somewhat insufficient. The positioning of the arm and the scoring\nend of the ball delivery bin proved to be such that the robot could only score if it approached the goal at a\nspecific angle. Since it was not a simple task, much more intense code had to be written in order that the\nrobot would be able to score in difficult situations. As it turns out, the code proved to be too difficult and\nthe robot was unable to score when it mattered.\nThere were three IR sensors, two facing forward on either side and one facebackward. They were placed\non the extremes of the robot (left and right) so that no walls would be able to catch parts of the robot\nwithout the IR sensors detecting them. The rear facing IR sensor was in place to make sure that the robot\ndid not back up into any walls.\nThere was a touch sensor on the claw to indicate that it had fully opened since the servo used to open the\nMechanical Design and Sensors\n\nclaw had been modified. The servo was modified so that it would have enough range of motion to fully\nopen or close the claw.\nTwo servos were used for the arm. One to close the claw and one to move the arm up and down.\nChassis\nThe chassis consisted of the lower baseplate, upper baseplate, the motor mounts, and the side panels.\nThe chassis was designed almost completely around this piece of scrap plastic we found (which was some\nstudent's attempt at a perfect octagon). We ended up deciding not to use the piece but kept the design.\nInstead of using an imperfect octagon, however, we decided to make it a perfect octagon. The lower\nbaseplate was originally designed to hold both the computer and the battery. It was later found that it was\nin the best interests of space for the computer to go on the top. The orcboard was then mounted on the\nlower baseplate. The sidepanels were also each slightly different, with an arrangement of holes so that the\norcboard and orcpad could be mounted in an accessible and safe fashion. In the final design they were not\nmounted in the same orientation because they did not allow for enough clearance between the wheels and\nthe chassis. The upper baseplate had horizontal holes in it so that the ball delivery bin could be easily\n\nmounted. Since the ball delivery bin was laserjetted, it could only be a two dimensional shape which\nproved to make mounting more difficult. The motor mounts were precision ground to an outrageous\ntolerance because it we originally planned to mount potentiometers and to have metal gears connect the\nshafts of the motor and potentiometers. (Metal gears are more durable, but require more precision)\nThroughout the evolution of the chassis, the original design proved to be less and less space efficient. If\nthere were time, the chassis would be completely changed in order to improve the ball delivery\ncapabilities.\nGripper Arm\nThe Gripper Arm was designed to minimize the amount of precision and energy needed to pick up a ball.\nThe end effector of gripper arm has the general shape of a plow (when open) with two stable positions\nfrom an approaching ball. It has span of 8 inches, so if a ball was even remotely centered in front of the\nrobot, when the robot drives forward, it would be able to center the ball and initiate the closing\nmechanism. As the claw closed the geometry of the device insured that any ball with its workspace would\nbe corralled with perfect reliability. The sole link of the gripper arm was designed and constructed out of\nacrylic to not reduce both weight and a truss-like pattern was implemented to reduce stress concentrations.\nActuation is achieved through cable driven pulleys to minimize the arms weight resulting in less energy\nusage. Due to the grippers actuation geometry compliance was employed to maintain cable tension. Light\nsprings where used on the cable mechanism for opening opposed by heavy springs on the opening cable\nchain. After grabbing a ball the arm will rotate 180 degrees to deposit the ball in the ball delivery bin.\nBall Delivery Bin\nThe ball delivery bin was designed as a completely passive device.\n\nThe ball delivery bin was designed to hold 14 balls. It had three different sloped sections so that a ball\ncould be both dropped in the front and later be ready to be delivered again from the front. The bin had a\nfront flap and a spring system so that if the robot were to drive into a goal, it would be able to score\nwithout the need of an extra servo or drive motor.\nSoftware Design\nWhen the contest began, we had no idea what our robot would look like, or what capabilities. Therefore,\none of the requirements of our software design was that it should be very general and extensible. This\nmay have been a bad decision, as getting the framework down for such architecture took time away from\nthe development of algorithms to control our robot.\nThe original code design was broken up into four modules: a Navigator, which would encapsulate all\nrobot movement commands and keep track of odometry data, a Scanner, which would create a constant\nstream of information about visible objects (using the camera), a Mapper, which would incorporate both\nscanner and navigator information in order to estimate the location of the robot and other features of the\nenvironment (like the barcodes), and a Controller, which would use the other modules to make our robot\nact intelligently.\n\nIn the end the code design stayed relatively the same, except that some of the modules were lacking in\ncontent. For example, we never got to write any Mapper code, though the mechanism for incorporating\nthose capabilities was there.\nThe Navigator\nThe Navigator contains the robot's motors, IR sensors, and a gyro. The original design called for two\npotentiometers affixed to the wheels which would allow us to tell the exact angle of the wheels, but all our\npotentiometers broke, leaving us with nothing but a gyro and the IR sensors for non-camera navigation.\nWithin the Navigator, an Odometer runs a task which was to update the position using the sensor data\nevery 20ms, passing the information to the Mapper, where it would be corrected using available camera\ndata. This task would create OdometryData objects which would be stored in a sorted tree by time stamp,\nallowing fast access of data for specific blocks of time. While all of this code was written, it had to be\nabandoned when the potentiometers broke. Because our Navigator did not completely depend on this\nOdometryData we were able to hook up the gyro at the last minute so that we could at least turn a given\nnumber of degrees.\nThe Scanner\nThe Scanner contains the camera and allows access to the latest CameraData objects. Each of these is a\nseries of arrays containing the information about the balls, goals, and barcodes found in the latest captured\npicture, along with the odometry data for the time the picture was taken.\nThe vision algorithms are based largely on the fact that our interesting objects are connected blobs of\ncolor in some range. We implemented HSVColor and HSVColorRange classes to encapsulate a single\nHSV color and a range of colors between two such colors. Another class, HSVMultipleColorRange, was\nadded to create ranges which contain many disconnected ranges. This was used for the barcode, which\nincludes both black and green. A general ImageScanner class is initialized with HSVColorRanges which\nit is to scan for. It can then be used to scan parts of a picture for areas of interest containing these colors,\nusing a fill algorithm to mark related pixels once a single interesting pixel is found, and never scanning\npixels which have already been marked. Each interesting area is created by initializing an AreaOfInterest\nobject. This object performs the fill algorithm and collects a lot of data about the area, such as the\nleftmost, rightmost, lowest and highest point, and information necessary to compute the centroid (this was\nextremely useful, as sometimes the area would leak a bit under the barcodes, shifting the midpoint away\nfrom the actual barcode). Depending on what we are scanning for, these areas of interest can be analyzed\nto classify them as goals, balls, barcodes, etc.\nOur implementation includes a BallScanner, GoalScanner , and BarcodeScanner , which each use this\nmethod to detect the GameObjects. Because our camera was mounted 5.3 inches above the ground, it\nwas only necessary to scan only the bottom half of the picture when scanning for balls, and amazingly\nonly the middle few rows of pixels to detect goals and barcodes (the fill algorithm takes care of\neverything after you detect the first pixel in the range). This allowed us to run our camera at a good 10\n\nFPS using full 160x120 images, including some processing to produce the output debug images.\nGameObjects use details about the dimensions of the camera to estimate their angle and distance from\nthe robot. The Camera Data was updated every 100ms using a new picture and stored and accessed using\nthread-safe methods.\nThe Mapper\nThe mapper was supposed to create a map. Unfortunately our lack of odometry data and time left this\ntotally empty. Ideally, this would have used a Kalman filter to keep estimates of the robot and barcode\npositions, updating these every time odometry or camera data was refreshed, thus giving a good\napproximation of their positions.\nThe Controller\nThe Controller is what Popular Science might call the \"brain\" of our robot. The final implementation\nconsisted of a finite state machine which continuously used the latest camera and sensor data to decide\nwhen to transition between states.\nThe actual StateMachine class was coded to be entirely general-purpose. Action objects comprise the\nstates, and ExitCondition objects define the transition conditions between states. The underlying\nstructure is a DirectedGraph with Actions as node labels and ExitConditions as edge labels. When run,\nactions can be queried for their state, with values such as PREPARING, PERFORMING, FINISHED,\nERROR, etc. ExitConditions specify their requirement for activation (which can use the state of the any\naction being performed) as well as the frequency with which they are checked, the default being 100ms.\nWhen a StateMachine runs, it creates a java.awt.Timer object which checks the exit conditions from the\ncurrent action at their frequency, then runs the Action in another thread. When an ExitCondition is\nactivated, it interrupts the Action thread, kills the timer, and causes the state machine to transition to the\nnext state and repeat the process. Because Actions are very general, it was possible to create an action\nwhich itself ran a given state machine, allowing us to have many levels of state machine with their exit\nconditions at the various levels checked simultaneously. Because our objects were also fairly general, we\nwere able to write a single state machine which could be adapted to seek ball, goals, barcodes and\npossibly other objects with a simple change in input at initialization. This allowed for improvements in\ngoal-seeking, for example, to also improve ball-seeking actions, improving our efficiency when\ndeveloping the state machines. The same collision avoidance states never had to be copied into different\nstate machines.\n\nOverall Performance\nOur contest round began (or rather didn't begin) with our orc pad freezing, a reoccurring problem that\ncould only be fixed with a system reboot. But after coming back online and jumping back into the contest\ncue, Johnny initiated fine and began to transition beautifully through the FSM. The robust wandering code\nperformed as expected, as Johnny bounced around the course. Unfortunately, most of our wander testing\nwas done while tethered to a power supply, so the overall speed and -- therefore -- the area that our robot\ncovered was less than ideal. Johnny failed to collect the first ball he encoutered due to the fact that the\narm was not brought down in time, simply an error in calibration. The second ball Johnny encountered\nwas centered with a relatively large error, seriously testing the limits of the gripper's mechanical\nrobustness. The gripper did manage to grasp the ball, although the far reach of the right claw was only\nbarely past the center of mass of the ball. During the round, we collected three balls and successfully\ndeposited them into our bin. At this point, Johnny began to look for a goal. The goal scoring state found a\ngoal within a short time and attempted to center and shimmy to the proper orientation. This unfortunately\nwas the weakest part of our implementation, since it was only tested a few times on three goal\nconfigurations. As we had feared at the start of the contest, the alignment was slightly off and our passive\ntrigger hit the field goal upright. Overall, we were extremely pleased with Johnny's performance and\n\neverything performed (or didn't perform) as it was expected to, which is a miracle in engineering\nimplementation.\nConclusions/Suggestions for future teams\nSuggestions\n● organize formal team design reviews, the key to your robots implementation is in the group\ndynamics\n● Be modular and build something that works before going into super complicated mapping and\nodometry\n● Make sure you are making progress towards your goal each day, don't waste time on stuff you cant\nuse\n● Do not underestimate the difficulty of things that may seem simple on the surface.\n● Buy encoders you cheap bastard\n● Make sure to test everything as you go along... make sure each function you write works before\nyou move on.\n● Plan out your software structure before you start coding randomly\n● When designing your mechanical structure make decisions based on how long it will take to make\nbecause you need a working machine early that you can test.\n● Make sure those making software talk to those doing mechanical things because something that\nmay make sense mechanical may make the software 10x harder to write.\nConclusion\nJohnny 5 made a good showing and impressed a number of people as they walked by but ultimately could\nhave been much more than it was. The mechanical portion of our robot took over as things began to take\nlonger and longer to come in and built. The arm ended up being a beautiful thing but it did not take the\nsingle week we expected to have it in working condition. Our software, although functional, could have\nbeen better if we had gone all out at the beginning of the month. We were unsure as to what kind of\nodometry we would be able to utilize and spent much time planning a mapping round that never came\nabout. Potentiometers that would have been to used to measure our distance and be used to keep track of\nour location ended up failing as the pots broke after only a few days of use. In the end we scraped together\nan impressive machine using only a camera, gyro and IR sensors to navigate through space. I believe we\ndid extremely well under the pressures, setbacks and inevitable frustrations that came up along the way.\nWorking up to the last minute, we worked as a team utilizing the strengths of each individual to produce a\nrobot that did perform well. I think we will forgive Johnny 5 for becoming stuck on the left goal post on\nits scoring run and call him a success. We all got a great deal out of this experience and for some of us it\nis the beginning of many more endeavors into robotics.\n● Team ONE (Nu Delta)"
    },
    {
      "category": "Resource",
      "title": "teamsevenpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/d8c8f5bde01e5c40d549b4ca9f3e6a63_teamsevenpaper.pdf",
      "content": "Team Seven Paper\n\nOverall Strategy\nThe robot design centered around capturing red balls, elevating them to a height above\nthe walls, and then opening a gate to release the balls into a field goal, in order to score\nthe maximum number of points.\nMechanical Design and Sensors\nOverall Frame and Powertrain\nRoller\nOur initial design for a ball collecting mechanism was some sort of paddle wheel,\ninspired by several of the teams' designs from last year. After getting a complete concept\nsketch down on paper for our entire ball collecting/lifting/dumping system, the team\ncame to the conclusion that, in order for our ball lifting mechanism to work properly, our\ncollection device would need to impart enough energy to the balls to reliably move them\nto the rear of our bot, where the entrance for the ball lifting device was to be positioned\n(in order to ease camera and sensor placement). We then decided that a paddle wheel or\ncombine-type mechanism wouldn't be able to be spun fast enough to impart the necessary\nenergy to the balls. It was then decided that a foam or rubber roller would be more\nfeasable, as it allowed us to spin it much faster, without worrying about stalling the\ncollector on a ball positioned in an inopportune spot (such as directly under the outer\nradius of a sweeping paddle). In looking for materials in lab, our first plan was to use a\n3/4\" dowel, with a section of gray foam pipe insulation glued onto the dowel in order to\nprovide a higher-friction surface to grasp the ball. After searching through Mike's bag of\nmotors, though, we were unable to come up with a viable solution for easily driving a\n3/4\" shaft, as would be required with the pipe insulation-and-dowel idea. On a trip to\nHome Depot, however, when looking for glue, we came across a foam paint roller which\nlooked like it would fit our needs quite well, as it had a high friction surface, was the\nproper diameter, and fit on a 1/4\" shaft, something we could easily drive. The final roller\nwas made with this paint roller supported by a 1/4\" aluminum shaft riding in two wood\nblocks attached to the support blocks for the front casters. A power seat motor from a\nPlymouth was used to drive the roller via a 1:1 chain drive. The motor was run at 6V.\nElevator\nThe design for the ball lifting mechanism was relatively unchanged from our original\ndesign. The idea was to use a rubber belt driven by two pulleys, attached to which would\nbe wire or plastic paddles to push the balls up to a hopper through an aluminum channel.\nThe only changes made to the concept were the following: First, the drive motor was\n\nmoved to the top pulley, in order to free up chassis space. Second, a separate support\nstructure was made to support the pulley shafts, instead of having the upper pulley shaft\nsupported by the hopper deck, which would then be, in turn, supported by support struts.\nThis was done due to the relatively high forces placed on the upper pulley shaft by the\nbelt tension. Third, our initial plan was to have a toothed timing belt, or a notched V-belt\nas the paddle mounting surface, but we switched to a section of 1/2\" rubber fishtank air\nhose after discovering that the friction in the system caused by a notched V-belt was\nmuch, much too high for our motor to turn the shaft against. As was initially planned, the\npaddles were made out of steel wire, and the guide channel was fashioned out of thin\nsheet aluminum. The assembly included a threaded-rod-type belt tensioner, and was\ndriven by a geared motor found in Mike's bag-O-motors.\nBall Gate\nThe ball release mechanism in the hopper was the simplist of our mechanical systems. It\nconsisted of a wood flap zip-tied and hot glued to the output arm of a servo. Tape was\nused to bridge the gap between the hopper deck and door when the door was open. Cut up\nShaw's cards were taped to the sides and front of the door to provide a guide for the balls\nas they rolled into the field goal, as well as provide an extension to the ramp, in order to\nallow our bot to dump the balls at a distance far enough away from the wall so that the IR\nsensor on the bot's front end would still get a valid reading. Bolts were taped to the inside\nof the ramp to agitate the balls, so that no two would get stuck against each other in\ntrying to exit the hopper, thus preventing more points from being scored.\nIR Scanning Head\nOur two primary sensors were two short-range IR sensors mounted at 45 degrees to each\nother on a rotating head which was actuated by a servo mounted to the underside of the\nhopper deck. By havign two separate sensors, we were able to constantly be getting data\nfrom two side of the robot simultaneously, regardless of what angle the sensor head was\nat. The alleviated a previous problem, in which the robot was blind on one side when wall\nfollowing, thus limiting the algorithm's robustness.\nWheel Encoders\nThe standard wheel encoders were used, with the 18-delineation encoder disks. Due to\nour inverted positioning of the stock motor mounts, however, the encoder circuit boards\nhad to be moved 90 degrees around the motor output shaft, so that the encoders were now\non the \"front\" side of the motor mounts, as opposed to the \"bottom\" (as they would be in\nour configuration), or the \"top\" (as they would be in the stock configuration).\n\nMike began developing vision software during winter break. During that time, he wrote a\nV4LCap class which captured images from any Video4Linux device, using the\nJava Media Framework to speak to the webcam. (Ultimately, the V4LCap class was used\nonly for initial testing and not during IAP.) He also found and decided to use ImageJ,\na public-domain image processing library, for making it easier to handle different image\nsources. This allowed us to take advantage of many library routines already written and\noptimized in Java. Our final design has several classes that dealt with computer vision\nand image processing.\n-\nThe CaptureThread class is responsible for grabbing frames from the webcam as\nfast as possible. It contains two image buffers and captures to one while the other\nis being read and processed. It was important to have the call to\nmaslab.camera.Camera.capture() be inside a thread because it was a blocking call\nand would otherwise prevent the rest of the program from doing anything\ninteresting.\n-\nThe VisionThread class polls the CaptureThread and then performs all\nnecessary processing of the image data.\n-\nThe MachineVisionProcessorIJ class is a holder for an ImageJ\nColorProcessor and contains lots of our image processing functions. It also holds\nthree byte arrays, byte h], s[v[] which contain hue/saturation/value data\nfor every pixel in the image.\n-\nThe HSVColor class represents a color in HSV space. There were several pre-\ndefined colors inside this class: white, yellow, green, black, red, blue, grey, violet,\ncyan. This allowed for an easy HSVColor.red.equals() mechanism to test for the\npresence of a certain color. There was also an HSVColor.toColor() function that\nreturned a standard Java AWT Color object representing this color.\n-\nThe Contour class dealt with int[] arrays that contained one value for each\ncolumn of the image. These contours were initialized to -1 in all elements, and\nthen functions would operate on an image and identify the top of the blue line (not\ntoo useful), the bottom of the blue line (much more useful), and the break between\nfloor and wall.\nAs noted above, the VisionThread controls the per-frame processing of images. When a\nnew frame is taken, its order of operations is as follows:\n-\nDownsample to 32x24 (from capture resolution of 160x120)\n-\nRun MachineVisionProcessorIJ.basicColorFilter1(), which classifies each\npixel as one of the basic colors. Having this code working over break was very\nuseful. It uses a series of if statements to determine the color of the pixel. First,\nwhite is detected by a low saturation (less than 70 of 255) and high value (above\nwhiteMinValue, which is auto-calibrated at run-time to better distinguish between\nwall and carpet). Black is detected by low value (< 80). Of pixels that remain,\nthose that are considered not brightly colored enough (s<64 or v<90) will be\nSoftware Design\nVision\n\nturned grey. Any remaining pixels are sorted by hue. There are then two extra\ncomparisons that make extra demands on pixels deemed red or yellow,\nparticularly requiring a higher value or saturation. One extra comparison is then\nmade that turns any whiteish-blue (carpet) into grey.\n-\nIdentify the Contour of the bottom of the blue line. This searches for blue -> other\ntransitions down a column.\n-\nDo blue-line filtering, filling in the image with blue, for any points above the\ncontour just determined.\n-\nFind a floor-wall contour, hopefully because the carpet will be mostly grey and\nthe wall will be mostly white.\n-\nRuns MachineVisionProcessorIJ.findRedBall1(), which determines\nnavigation toward a ball using the lowest pixel of red found and calculates a turn\ndirection.\n-\nRuns MachineVisionProcessorIJ.findGoal1(), which determines if a goal is\nvisible and which way to steer toward it.\n-\nPublishes every 10th frame to the BotClient.\nOdometry\nThe robot keeps track of its current location from its start, where X,Y, and Theta are all 0.\nThe bot knows how far it has travelled by averaging the changes in the readings from the\nleft and right encoders. The angle of the bot, theta, is determined by calculating the\nquotient of the difference between the change in the right encoder and the change in the\nleft encoder and the distance between each encoder. It then uses to trigonometry to get\nthe absolute location, with the starting point as reference.\nMapping\nThe robot draws sensor data onto two maps of the Mapper class to store information\nabout its surroundings.\n-\nFirst, there is a global map, where the robot position is moved around the map\nwith its absolute X,Y, and theta. In the global map, both vision data to calculate\nthe distance to a wall by its apparent height, knowing its actual height, and IR\ndistance data are used to determine the location of \"wall\" points. The global map\nhas 10 cm resolution.\n-\nA local map stores data only from the IR sensors and is always centered around\nthe bot. As the bot moves, local map data that would be off the map is discarded.\nThe rational behind this is that we shouldn't use data from large distances away\nbecause odometry errors have accumulated since the bot moved from the distant\nlocation. The local map has 2 cm resolution.\nCollision Avoidance\nTo avoid colliding into walls, the robot has a CollisionAvoidance class, which\nimplements a series of \"virtual sensors.\" These sensors will look around different\n\ndirections for \"walls\" in the local map to determine if an obstacle is in the robot's way\nbefore moving in that direction.\nMechanical Control\nA few classes were written just to ease control of various robot actuators:\n-\nThe BallGate class controls the servo on the hopper. BallGate.open() opens\nthe gate so that balls can flow, while =BallGate.close() closes it.\n-\nThe Roller class controls the OrcBoard's pin 3 as a digital out. This pin is\nconnected to a 2N7000 which in turn drives the coil of a relay. When pin 3 is\nset high and the relay connection is closed, the roller motor is activated. We found\nthat the OrcBoard would sometimes \"lose\" the pin mode. To deal with this, the\nRoller class actually contained a thread that frequently overwrote both the pin\nmode and the value of the corresponding pin.\n-\nThe Elevator class was very similar to the Roller. It uses the OrcBoard's pins 4\nand 5 as digital outs to control Mike's magical motor controllers. Pin 5 controls\nwhether or not the elevator is on, while pin 4 controls the direction. The Elevator\nclass implements a thread that forces the elevator to go forward for 9 seconds and\nreverse for one, in order to reduce the likelihood of the elevator getting stuck. As\ndoes the Roller , the Elevator frequently re-writes its OrcBoard pin values.\nOverall Control\nThe robot's central control class creates threads such as the IR/Servo SensorStalk, the\nElevator controller, the CollisionAvoidance scanner, the Mapper, and the Odometry\nupdater. These threads run as fast as possible so that their variables are as consistant with\nactual conditions as possible. Whenever the central control class switches into a state that\nrequires movement, it will poll some combination of the threads' variables to see if it is\npossible to turn a certain direction, if the robot is getting too close to a wall, or if a goal is\nin sight.\nOverall Performance\nThe robot performed its scanning and ball-capture abilities admirably, catching a total of\n9 balls. Unfortunately, the computer crashed within the last fifteen seconds of the round\nand all powered sources stayed in their \"on positions.\" The robot got stuck to a wall, as\nthe function to determine stalls and to back up was disabled when the computer turned\noff. The robot was unable to find a goal within the alloted time to deposit the balls that it\nhad captured.\nConclusions/Suggestions for future teams\n-\nGoal docking is quite difficult. For a greater degree of success, start thinking of an\nimplementation that allows for good goal searching/docking before you design\n\nyour bot and optimize your mechanical design for goal docking, not just for ball\nfinding.\n-\nThink twice before doing MASLab during IAP AND the following activities:\nTaking 2.670, participating in a rocket team engine creation contest, running IAP\nworkweek, and teaching a class for SIPB in PHP. It does require a lot of time.\nRunning out of term to implement exactly what you want, with machined parts\nand a well-tested finite state machine is quite distressing."
    },
    {
      "category": "Resource",
      "title": "teamsixpaper.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/b34e09056e9e602d3679f41fe5036067_teamsixpaper.pdf",
      "content": "Team Six Paper\nIntroduction\nThe MASLAB competition this year was a stimulating challenge which our team decided\nto take on as soon as we heard we had been admitted into the class. As in previous years,\nthe competition consisted of collecting red wooden balls and placing them in goals.\nAdditionally, this year teams were also allowed to place balls over goals for a significant\nincrease in points. Our team immediately set out to make a robot that not only efficiently\ncaptured red balls but also would be capable of raising them to sufficient heights to make\nfield goals. We faced many challenges, including software issues, mechanical obstacles,\nand especially the minimal time frame in which we had to complete our robot. In the end,\nhowever, we were able to construct a sound robot which fulfilled all of our expectations\nand performed solidly in the final competition.\nMechanical Design\nDesign Considerations:\nThe task to pick up a round object is not as trivial as it might sound, and our design\nevolved over the duration of the competition to maximize efficiency. Not only did we\nbuild 3 prototypes, we also drew upon our past experience in the Lego Robotics class\n6.270 to decide our design for picking up balls. We initially decided that there are 3\nprimary methods for collecting a ball with a robot, including using the robot itself to drive\nover and chorale balls, using a claw-like mechanism to pick up balls, and using rollers to\nfeed balls into a holding mechanism.\nWe ruled out using the robot to chorale balls because we wished to lift the ball and score a field goal. In\norder to lift the ball, we would need to have the ball in the robots possession, not simply stuck\nunderneath it. Although using a claw to pickup, and potentially lift the ball to field goal height is\npossible, we ruled it out because our experience in 6.270 proved that this design has major technical\ndifficulties. This left only the roller mechanism for our design, which appeared to have several\nmechanical advantages over the other two methods, including simplicity, large tolerances for ball\npositioning relative to the robot, and the ability to funnel balls to a lifting mechanism. Implementation\nBall Retrieval\nOnce we had decided on implementing the roller design to pick up balls with our robot,\nwe designed a simple lifting mechanism which coupled with our rollers and lifted balls to\nour goal height. We originally had implemented rollers side by side; however we decided\n\nto change our design to a single horizontal roller due to the increased \"sweet spot\" for\ncollecting balls and the simpler drive train to power the single roller. We also decided to\nplace the roller lower and on hinges rather then fixing it at the ball height, allowing the\nroller to maintain contact with the ball for an increased period of time. By fixing the roller\non hinges, it allowed the robot to collect balls without needing to drive as closely to them\nand also to shoot them more effectively into our lifting mechanism.\nBall Lifting\nWe decided to use a motor to mechanically lift the ball up 13 inches which is high enough\nto roll balls over the goals for field goal points. The roller forced the ball into a V shaped\nfunnel which then placed the ball at the bottom of the lifting mechanism. The lift\nconsisted of two gears, one at ground height and one 13 inches higher, with a bike chain\nconnecting the two. A small high speed motor powered the chain, spinning it around\nwithin a custom built metal box. One of the pins holding the chain together was replaced\nwith a 3 inch long pin which would grab the ball from underneath and drive the ball up\nthrough the metal box onto the top of the robot.\nBall Storage\nWhen the ball had been lifted by our chain drive, we originally planned to have a servo\ndump the ball over the goal; however we ran out of sensor points and did not have enough\ntime to make a mechanical gate. Because of our lack of time, we instead built a storage\nbox on the side of the robot, which would hold up to 20 balls in an effort to maximize ball\npossession points. Balls lifted would run through the top tray if it was empty, and fall\nthrough a hole into the storage box, until eventually the box filled and the platform above\nwould then also be filled.\nMovement\nAll of the above mechanical design relates to the ball retrieval mechanism, but a brief\nmention of the drive train and structural frame upon which this was mounted is necessary.\nWe drove the robot with two motors mounted in the center and allowed efficient turning\nabout the center by pulsing motors forward or reverse. The motors were attached to the\nbottom of a three tier structure. The bottom tier stored the batter and motors, the second\ntier stored the computer, and the third tier was the ball storage level.\nSoftware\nConsiderations:\nIn any autonomous system, stability and robustness must be paramount; if a situation\n\narises for which no software is written, the robot will likely fail. There are no chances to\ncommunicate with the system to allow it to deal with the new problem, so all such\nprovisions must be considered ahead of time.\nSecondly, we wanted software that was conducive to being worked on by\nboth team members at different times. Importable or obscure code would\nmake it practically impossible to write as a team without undoing each\nother's work.\nLastly, it was important that the software be versatile since the mechanical\ndesign of the robot, as well as its strategy, were undergoing constant\nrevision. Code tied too closely to a particular physical piece or action would\nbe rendered useless every time the robot was changed. Conversely, by\ncreating enough low-level functions such that all higher-level behaviors\ncould be built completely without references to robot-specifics, behaviors\nwould be simple to create as well as to modify.\nImplementation:\nTo create stable code, some care was taken to handle exceptions thrown by methods.\nAdditionally, all variables were initialized to safe values. Thus, problems would not\ncascade through our software; if a method failed and could not return an expected\nvariable, the code would continue to operate based on our initialized values. While this\nsituation would still probably cause the robot to behave poorly for a time, the system as a\nwhole would continue to operate. Additionally, all processes were broken into basic\nthreads, such that if a single thread failed, the others would persist. Ideally, a thread could\nbe dedicated to monitoring other threads for such crashes, and restarting them as\nnecessary. In the interests of time, this functionality was never fully implemented, but it\nwould have been relatively simple to do so. Threading did allow us, however, to forgo\nadding timeouts to behaviors to ensure robust software design. Because one thread's job\nwas to monitor the robot for signs that it was stuck (an unchanging gyroscope, high\ncurrent draw from motors, etc.), we were able to ensure that even if other systems failed\nand left the robot in an unexpected state where it was unable to move, our software would\nfree see this and take appropriate action.\nIn the interest of portable code, we tried to take advantage of the object-oriented features\nof Java. Wherever possible, we made methods private and avoided global variables. Also,\nwe commented our code prolifically. Additionally, we ensured standard formatting of our\ncode by sticking to Eclipse's built-in function, which made our code more readable.\nObject-oriented programming also served to make our code versatile. We created a\nseparate class for the robot itself, which included everything necessary to determine its\n\nstate and access its functions. In this way, whenever the physical design was modified,\nfields could be added or removed from the robot's class and all methods acting on the\nrobot were automatically given the right information. Moreover, changes to low-level\nmethods would not adversely affect higher-level behaviors, so long as methods still did\nwhat they were originally designed to do.\nOur image processing scheme is a good example of versatile software. Rather than\nassigning pixels individual colors representing the most likely choice, we represented\npixels by \"color distributions.\" The visible spectrum was divided into 12 colors and for\nevery pixel, a finite probability was assigned to each color using the HSV space. Pixels\nwere dealt with in this state for as long as possible--only when a final color determination\nhad to be made was the candidate with the highest probability selected as the pixels \"true\ncolor.\" This approach gave us many useful features. Methods could add or subtract from a\npixel's color distribution to reflect any factors beyond of the captured image's color. For\ninstance, if a group of pixels were part of a horizontal line, the constraints of the\ncompetition made this group of pixels more likely to be blue, regardless of their\nappearance to the webcam.\nAnalysis:\nOverall, our software performed extremely well. Everything was fast and consistent\nenough to function without adjusting our strategy, and no systems failed permanently\nduring the competition (the robot did get stuck for 10 or 15 seconds for unknown reasons,\nbut it automatically freed itself). In particular, our image processing code behaved\neffectively. While other teams struggled with lighting changes and calibrations, we never\nhad to adjust any code to achieve flawless ball-recognition.\nAlthough at first we adhered strongly to the considerations above, a lack of time forced us to make some\nconcessions as the competition approached. For instance, our final code contains an entire class of\npublicly-visible variables that act as globals throughout the code.\nConclusion\nAlthough we began Maslab worried about our two person team, with neither a computer\nscientist nor a mechanical engineer, we finished very satisfied with our robot and its\nsoftware. Because we recognized that fabrication and code would not be our strengths, we\nworked hard to find an inherently robust design; unlike a claw, our roller could\nconsistently grab any balls along the entire width of the robot, even if they were not seen.\nHad we had time to include a way to dump balls at a goal, we believe we could have been\na serious threat to the winners. Of course, if my grandmother had wheels, she would be a\ntrolley-car.\n\nJohn Aquadro and Elvio Sadun\nTeam Bob Hagopian"
    },
    {
      "category": "Resource",
      "title": "ant.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/ce68d3563892fd55e1c6ed5892b20f98_ant.pdf",
      "content": "Ant\nAnt\nAnt is a tool for automatically building your code. Put this file in the directory with your source:\nbuild.xml. Now to build your code just type ant. Ant will try to be intelligent about which files\nneed to be recompiled based on the changes you have made. Sometimes you need to rebuild the whole\ntree, however. In that case, type ant clean to delete all your classfiles, and then type ant to rebuild.\nYou'll notice Ant creates a directory named depcache; this is a dependency cache that helps it build\nfaster. You can safely delete it, but your next build will be slightly slower as it is recreated.\nAnt can also automatically upload your files to the geode. Open build.xml and set the appropriate IP\naddress (robotIP) and destination directory (destDir). Then typing ant upload will build all\nyour software and then upload it to the geode. You should set up passwordless login (see SSH) to avoid\ntyping your password every time.\nAnt is in the sipb locker (add sipb).\n\nbuild.xml\n<project name=\"maslab\" default=\"build\" basedir=\".\">\n<!-- CHANGE THESE THREE VALUES FOR AUTOMATIC UPLOAD -->\n<property name=\"robotIP\" value=\"18.251.0.140\"/>\n<property name=\"destDir\" value=\"/home/edward\"/>\n<property name=\"username\" value=\"root\"/>\n<target name=\"build\">\n<!-- This does deep dependency checking on class files -->\n<depend srcdir=\".\" cache=\"depcache\" closure=\"true\"/>\n<!-- This compiles all the java -->\n<javac srcdir=\".\" includes=\"**/*.java\" debug=\"true\"\nclasspath=\"/mit/6.186/maslab.jar:.\"/>\n</target>\n<!-- Clean everything -->\n<target name=\"clean\">\n<delete>\n<fileset dir=\".\" includes=\"**/*.class\"/>\n<fileset dir=\".\" includes=\"**/*~\" defaultexcludes=\"no\"/>\n</delete>\n</target>\n<!-- Upload files to robot -->\n<target name=\"upload\" depends=\"build\">\n<exec executable=\"rsync\">\n<arg line=\"-e ssh -avr . ${username}@${robotIP}:${destDir}\"/>\n</exec>\n</target>\n</project>"
    },
    {
      "category": "Resource",
      "title": "botclient.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/3dffb5667cd2550ca7d7aefc25bd7e88_botclient.pdf",
      "content": "Bot Client\nBotClient\nTo run the BotClient on the server, just run /mit/6.186/2005/botclient. If this doesn't work, do\njava -cp /mit/6.186/2005/maslab.jar maslab.telemetry.botclient.\nBotClient.\nIn the \"control panel\" window, enter your robot's IP address. Channels published by the robot will\nappear in the control panel. Double-click a channel to display it.\nChannel Types\nSee maslab.telemetry.channel in the MASLab API for details.\nImageChannel\nAs the mouse pointer hovers over a pixel, the RGB and HSV values are reported. Click to see\nwhere the color falls on an HSV colorwheel. You can save an image (the captured image is saved\nin the directory where BotClient was invoked; the filename is captureNN.png), as well as\npause/resume a video feed.\nTextChannel\nYup, text.\nScopeChannel\nFor plotting values graphically over time.\nVectorChannel\nChannel for sending vector graphics to the BotClient. Useful for anotating an image or plotting\nsensor data in space\nYou can also have your robot listen for commands from the BotClient using the \"command\" field in the\ncontrol panel.\nIf I add any new features to BotClient, I'll describe them here."
    },
    {
      "category": "Resource",
      "title": "build_xml.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/0fa500849012d9434d56e725417a3d29_build_xml.pdf",
      "content": "build.xml\n<project name=\"maslab\" default=\"build\" basedir=\".\">\n<!-- CHANGE THESE THREE VALUES FOR AUTOMATIC UPLOAD -->\n<property name=\"robotIP\" value=\"18.251.0.140\"/>\n<property name=\"destDir\" value=\"/home/edward\"/>\n<property name=\"username\" value=\"root\"/>\n<target name=\"build\">\n<!-- This does deep dependency checking on class files -->\n<depend srcdir=\".\" cache=\"depcache\" closure=\"true\"/>\n<!-- This compiles all the java -->\n<javac srcdir=\".\" includes=\"**/*.java\" debug=\"true\" classpath=\"/\nmit/6.186/maslab.jar:.\"/>\n</target>\n<!-- Clean everything -->\n<target name=\"clean\">\n<delete>\n<fileset dir=\".\" includes=\"**/*.class\"/>\n<fileset dir=\".\" includes=\"**/*~\" defaultexcludes=\"no\"/>\n</delete>\n</target>\n<!-- Upload files to robot -->\n<target name=\"upload\" depends=\"build\">\n<exec executable=\"rsync\">\n<arg line=\"-e ssh -avr . ${username}@${robotIP}:${destDir}\"/>\n</exec>\n</target>\n</project>"
    },
    {
      "category": "Resource",
      "title": "cvs.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/bf317326d575e7781653b4b3de58ea49_cvs.pdf",
      "content": "CVS\nCVS\nSetting up a CVS respository\nTo set up a CVS repository for your team in your Server locker:\n● mkdir maslabcvs (you can name it whatever you want)\n● fs sa -dir maslabcvs -acl teammate1 all teammate2 all teammate3\nall (where teammate1, teammate2, teammate3 are the usernames of your teammates)\n● setenv CVSROOT $HOME/maslabcvs\n● cvs init\n● Change to the directory that contains your code. Make sure it contains only files/directories that\nyou want to put in the repository.\n● cvs import modulename init head (where modulename is a name you pick)\nIf one of your teammates has set up a repository for your team, do this:\n● attach teammate (where teammate is the username of the person who made the\nrepository)\n● setenv CVSROOT /mit/teammate/maslabcvs\nIf you put the previous two lines in your .environment file, you won't ever need to type them again.\nYou might also want to set CVS_EDITOR to your favorite text editor. It will be used for recording log\nmessages when you commit.\nUsing CVS\n● To checkout a copy of the code: cvs co modulename.\n● To update your copy to the latest version: cvs up.\n● To commit your changes: cvs commit (you must update before you can commit)."
    },
    {
      "category": "Resource",
      "title": "emacs.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/5d90e104d0bbfe179e0ee84ee1ff1b9f_emacs.pdf",
      "content": "Emacs\nEmacs\nTo turn on syntax highlighting, parens matching, and region highlighting, add these lines to your .\nemacs file:\n(global-font-lock-mode 1)\n(show-paren-mode 1)\n(transient-mark-mode 1)\nThis makes writing Java in Emacs more convenient.\nIf you have any other Emacs tips, add them here."
    },
    {
      "category": "Resource",
      "title": "softwareinfo.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/689c1ab973e6fafb97ab8720d1cf612d_softwareinfo.pdf",
      "content": "Software Info\nGeneral API Information\n●\nMASLab API: documentation on the MASLab custom software: Orc, Camera, Channel,\netc.\n●\nJava Standard API: documentation on the standard Java classes.\n●\nSun's Java Tutorials: online tutorials in many Java topics.\n●\nEd Faulkner's Java Quick Reference: examples of all the important language features.\nApplication Information\n● Ant: A tool that can automatically build and upload your code. Use it, it's nice.\n● ServerSettings: Things you should set in your server locker to make development easier.\n● BotClient: The tool for viewing remote channel data.\n● CVS: A tool for managing your source code amongst your teammates.\n● Subversion: A tool similar to CVS.\n● Emacs: Some notes on getting the most out of the GNU Emacs text editor.\n● GCJ : Compile java to native code. No more slow starts or warming up. Yay!\n● SSH: Tool for logging into other machines remotely. Also has info on scp, which copies files to\nand from other machines.\n● orcd : Notes for starting your robot via the orcpad.\nMASLab Software Distribution\nWe're running Debian Linux with the 2.6.9 kernel.\nThe freshest version of the software will generally be in /mit/6.186/2005/maslab.jar."
    },
    {
      "category": "Resource",
      "title": "ssh.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/7e9dae2b222081e62f09d84161eaedeb_ssh.pdf",
      "content": "SSH\nSSH\nWhat does it mean when we say \"SSH into your robot\"? Say your Orc Pad reports an address of\n18.111.0.300. Just type the following at an Server prompt:\nssh root@18.111.0.300\nYou'll be prompted for the root password. To set up passwordless login (which you'll need unless you\nwant to type your password every time you do ant upload), do this:\n● On the Server, do ssh-keygen -t rsa. When you're prompted for a file name, accept the\ndefault. Leave a blank password.\n● Copy your public key to the geode like this (use your robots correct IP address): scp ~/.ssh/\nid_rsa.pub root@18.111.0.300:\n● SSH to the geode: ssh root@18.111.0.300\n● On the geode, make a .ssh directory if it doesn't already exist: mkdir .ssh\n● Set restrictive permission on it (SSH requires this): chmod 700 .ssh\n● Add your public key to the authorized keys file: cat id_rsa.pub >> .ssh/\nauthorized_keys\n● Set restrictive permission on this file, too: chmod 700 .ssh/authorized_keys\nNow when you type ssh root@18.111.0.300 on the Server, you should be logged in automatically\nwith no password prompt.\nIf this doesn't work: on the robot open /etc/ssh/sshd_config and change the line\n#StrictModes yes\nto\nStrictModes no\nThen type /etc/init.d/sshd restart. Now it should work.\nTo copy files to your robot, do scp filename root@18.111.0.300:/destination/\ndirectory. If passwordless login is configured, scp will also use it."
    },
    {
      "category": "Resource",
      "title": "subversion.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-186-mobile-autonomous-systems-laboratory-january-iap-2005/08b826627b7fd220f8186507d5ffd4e8_subversion.pdf",
      "content": "Subversion\nSubversion\nSubversion is like CVS, only without a decade of cruft. Its main advantage is the fact that it tracks\nchanges to your source code base as a whole together, not to individual files.\nSetting up a Subversion respository\nTo set up a Subversion repository for your team in your server locker:\n● add svn (Try to get this in your dotfiles)\n● mkdir ~/maslab-repository (you can name it whatever you want)\n● fs sa ~/maslab-repository teammate1 all teammate2 all teammate3\nall (where teammate1, teammate2, teammate3 are the usernames of your teammates)\n● svnadmin create ~/maslab-repository (make the repository)\n● Make, somewhere unrelated in your home directory, a directory called maslab-project (or\nsomething); inside it place three directories called trunk, branches, and tags. Copy all of\nyour code so far into the trunk folder.\n● Change to the directory that contains the maslab-project directory. Type svn import\nmaslab-project file:///mit/yourservername/maslab-repository\n● Follow the instructions below to check out your code somewhere.\n● The maslab-project directory was only to be used for initially importing the code into the\nrepository; feel free to delete it once you're sure you can check out the code from it. (Don't delete\nmaslab-repository!)\nIf one of your teammates has set up a repository for your team, do this:\n● If you are logged into your bot (as opposed to the Server), kinit serverusername, type in\nyour password, and then aklog.\n● svn checkout file:///afs/server/user/t/e/teammate/maslab-\nrepository/trunk maslab-checked-out where t and e are the first two letters of\nyour teammate's name. This will plop your project into the folder maslab-checked-out inside\nyour current folder.\n\nTo use Subversion from your bot\n● Unfortunately, in order to use a Subversion repository on server you need to use the \"FSFS\"\ndatabase, which was not in Subversion until version 1.1.0, which is newer than the version on\nyour bot. You'll need to upgrade the one on your bot if you want to use svn to update code to\nyour bot. (You can also just only use svn on your other computers and use rsync to sync from\nthe other computer to the bot.) Upgrading is pretty simple:\n● (as root) apt-get remove subversion (since there's probably an older version already\nthere)\n● wget http://subversion.tigris.org/tarballs/subversion-1.1.2.tar.\ngz\n● tar xzvf subversion-1.1.2.tar.gz\n● cd subversion-1.1.2\n● ./configure\n● make\n● (as root) make install\nUsing Subversion\n● To update your (already checked out) archive: type svn update. This updates to the latest\nversion in the repository, letting you know if you've made any incompatible changes locally.\n● To commit any changes you've made (in all files): svn commit.\n● To see what files have been changed: svn status (M means modified)\n● To add a new file to be version controlled: make the file like usual, then do svn add file\n● To copy or rename a file: svn cp file1 file2 or svn mv file1 file2. (If you don't\nuse svn here, it won't know that the file has changed!)\n● To get help: svn help or svn subcommand help\nFull documentation:\nThe Subversion Book"
    }
  ]
}