{
  "course_name": "Probability and Random Variables",
  "course_description": "This course introduces students to probability and random variables. Topics include distribution functions, binomial, geometric, hypergeometric, and Poisson distributions. The other topics covered are uniform, exponential, normal, gamma and beta distributions; conditional probability; Bayes theorem; joint distributions; Chebyshev inequality; law of large numbers; and central limit theorem.",
  "topics": [
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.02SC Multivariable Calculus\n\nCourse Description\n\nThis course introduces students to probability and random variables. Topics include distribution functions, binomial, geometric, hypergeometric, and Poisson distributions. The other topics covered are uniform, exponential, normal, gamma and beta distributions; conditional probability; Bayes theorem; joint distributions; Chebyshev inequality; law of large numbers; and central limit theorem.\n\nTextbook\n\nRequired\n\nRoss, Sheldon.\nA First Course in Probability\n. 8th ed. Pearson Prentice Hall, 2009. ISBN: 9780136033134.\n\nA Free and Fun-to-Read Book\n\nIntroduction to Probability (PDF - 3.1MB)\nby Charles Grinstead and J. Laurie Snell.\n\nProblem Sets\n\nThere will be ten problem sets assigned throughout the semester, but there will be no problem sets in the weeks that have exams.\n\nExams\n\nThere will be two midterm exams, as well as a final exam for the course.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem Sets\n\n20%\n\nMidterm Exams\n\n40%\n\nFinal Exam\n\n40%",
  "files": [
    {
      "category": "Resource",
      "title": "Doob’s Optional Stopping Theorem",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/8e03ea694a66e069cdc8e59255a3ba84_MIT18_440S14_mrtingalenote.pdf",
      "content": "Doob's Optional Stopping Theorem\nThe Doob's optional stopping time theorem is contained in many basic\ntexts on probability and Martingales. (See, for example, Theorem 10.10 of\nProbability with Martingales, by David Williams, 1991.) The essential\ncontent of the theorem is that you can't make money (in expectation) by\nbuying and selling an asset whose price is a martingale. Precisely, the\ntheorem states that if you buy the asset at some time and adopt any\nstrategy at all for deciding when to sell it, then the expected price at the\ntime you sell is the price you originally paid. Thus--if market price is a\nmartingale--you cannot make money in expectation by \"timing the\nmarket.\"\nLet Ω be the probability space. Let T be a map from Ω to the set of\npositive integers. We think of T (ω) as giving the time at which the asset\nwill be sold if the price sequence is S(0), S(1), S(2), . . . (where each S(i) is\na random variable, i.e., a real-valued function of ω). We say that T is a\nstopping time if the event that T (ω) = n depends only on the values S(i)\nfor i ≤ n. In other words, the decision of whether to sell the stock at time\nn depends only on the history of the stock up until time n, and not on the\nfuture values of the stock (which the investor hasn't seen yet).\nDoob's Optional Stopping Theorem: If the sequence\nS(0), S(1), S(2), . . . is a bounded martingale, and T is a stopping time,\nthen the expected value of S(T ) is S(0).\nMost real world asset prices are not martingales, even in theory. So why is\nthis theorem relevant to finance?\n1. Many asset prices behave approximately like martingales in the short\nterm.\n2. According to the fundamental theorem of asset pricing, as\npresented in Zastawniak and Capi nski (see the text for the precise\nS(n)\nconditions of the theorem), the discounted price\n, where A is a\nA(n)\nrisk-free asset, is a martingale with respected to the risk neutral\nprobability.\n3. Sequences of conditional expectations of a quantity--involving\nconditioning on increasing amounts of information--are martingales.\nFor example, let C be the amount of oil available for drilling under a\nparticular piece of land. Suppose that ten geological tests are done\n\nthat will ultimately determine the value of C. Let Cn be the\nconditional expectation of C given the outcome of the first n of\nthese tests. Then the sequence C0, C1, C2, . . . , C10 = C is a\nmartingale.\nSOME MARTINGALE PROBLEMS:\n1. Suppose Harriet has 7 dollars. Her plan is to make one dollar bets on\nfair coin tosses until her wealth reaches either 0 or 50, and then to go\nhome. What is the expected amount of money that Harriet will have\nwhen she goes home? What is the probability that she will have 50\nwhen she goes home?\n2. Consider a contract that at time N will be worth either 100 or 0. Let\nS(n) be its price at time 0 ≤ n ≤ N. If S(n) is a martingale, and\nS(0) = 47, then what is the probability that the contract will be\nworth 100 at time N?\n3. Pedro plans to buy the contract in the previous problem at time 0\nand sell it the first time T at which the price goes above 55 or below\n15. What is the expected value of S(T )?\n4. Suppose S(N) is with probability one either 100 or 0 and that\nS(0) = 50. Suppose further there is at least a sixy percent\nprobability that the price will at some point dip to below 40 and\nthen subsequently rise to above 60 before time N. Prove that S(n)\ncannot be a martingale.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/03d1ebefb25e0891eb397f6e02b93f0f_MIT18_440S14_ProblemSet1.pdf",
      "content": "18.440 PROBLEM SET ONE, DUE FEBRUARY 14\nA. FROM ROSS 8th EDITION CHAPTER ONE:\n1. Problem 10: In how many ways can 8 people be seated in a row if\n(a) there are no restrictions on the seating arrangement?\n(b) persons A and B must sit next to each other?\n(c) there are 4 women and 4 men and no 2 men or 2 women can sit\nnext to each other?\n(d) there are 5 men and they must sit next to each other?\n(e) there are 4 married couples and each couple must sit together?\n2. Problem 26: Expand (x1 + 2x2 + 3x3)4 .\n3. Problem 32: An elevator starts at the basement with 8 people (not\nincluding the elevator operator) and discharges them all by the time\nit reaches the top floor, number 6. In how many ways could the\noperator have perceived the people leaving the elevator if all people\nlook alike to him? What if the 8 people consisted of 5 men and 3\nwomen and the operator could tell a man from a woman?\n4. Theoretical Exercise 8: Prove that\nT\n\nT T\nT T\nn+m\nn\nm\nn\nm\nT T\n=\n+\n-\n+ . . . +\nn\nm . Hint: Consider\na group of\nr\nr\nr\nr\nn men and m women. How many groups of size r are possible?\n5. Theoretical Exercise 11: The following identity is known as\nFermat's combinatorial identity:\n\nn\n\nn\nt\ni - 1\n=\nn ≥ k.\nk\nk - 1\ni=k\nGive a combinatorial argument (no computations are needed) to\nestablish this identity. Hint: Consider the set of numbers 1 through\nn. How many subsets of size k have i as their highest-numbered\nmember?\n6. Self-Test Problem/Exercise 17: Give an analytic verification of\n\nn\nk\nn - k\n=\n+ k(n - k) +\n, 1 ≤ k ≤ n\nNow give a combinatorial argument for this identity.\n\nB. Suppose you have 12 (distinguishable) hats and 4 (distinguishable)\npeople. How many ways are there to divide the 12 hats among the 4\npeople with each person getting exactly three hats?\nC. Consider permutations σ : {1, 2, . . . , n} →{1, 2, . . . , n}.\n1. How many such σ have only one cycle, i.e., have the property that\nσ(1), σ *σ(1), σ *σ *σ(1), . . . cycles through all elements of\n{1, 2, . . . , n}?\n2. How many σ are fixed-point-free involutions, i.e., have the property\nthat for each j, σ(j) = j but σ *σ(j) = j?\n=\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/0f8be749d7ed502fb04d340fe85bc3c1_MIT18_440S14_ProblemSet2.pdf",
      "content": "18.440 PROBLEM SET TWO, FEBRUARY 24\nA. FROM ROSS 8TH EDITION CHAPTER TWO:\n1. Problem 25: A pair of dice is rolled until a sum of either 5 or 7\nappears. Find the probability that a 5 occurs first. Hint. Let En\ndenote the event that a 5 occurs on the nth roll and no 5 or 7 occurs\non the first (n - 1) rolls. Compute P (En) and argue that\noinf P (En) is the desired probability.\ni=1\n2. Problem 48: Given 20 people, what is the probability that, among\nthe 12 months in the year, there are 4 months containing exactly 2\nbirthdays and 4 containing exactly 3 birthdays?\n3. Problem 49: A group of 6 men and 6 women is randomly divided\ninto 2 groups of size 6 each. What is the probability that both\ngroups will have the same number of men?\n4. Theoretical Exercise 10: Prove that P (E ∪ F ∪ G) =\nP (E)+P (F )+P (G)-P (EcFG)-P (EF cG)-P (EFGc)-2P (EF G).\n5. Theoretical Exercise 15: An urn contains M white and N black\nballs. If a random sample of size r is chosen, what is the probability\nthat it contains exactly k white balls?\n6. Theoretical Exercise 20: Consider an experiment whose sample\nspace consists of a countably infinite number of points. Show that\nnot all points can be equally likely. Can all points have a positive\nprobability of occurring?\nB. A deck of cards contains 30 cards with labels 1, 2, . . . , 30. Suppose that\nsomebody is randomly dealt a set of 7 cards of these cards (numbered with\nseven distinct numbers).\n1. Find the probability that 3 of the cards contain odd numbers and 4\ncontain even numbers.\n2. Find the probability each of the numbers on the seven cards ends\nwith a different digit. (For example, the cards could be 3, 5, 14, 16,\n22, 29, 30.)\nC. (Just for fun - not to hand in.) The following is a popular and rather\ninstructive puzzle. A standard deck of 52 cards (26 red and 26 black) is\n\nshuffled so that all orderings are equally likely. We then play the following\ngame: I begin turning the cards over one at a time so that you can see\nthem. At some point (before I have turned over all 52 cards) you say \"I'm\nready!\" At this point I turn over the next card and if the card is red, you\nreceive one dollar; otherwise you receive nothing. You would like to design\na strategy to maximize the probability that you will receive the dollar.\nHow should you decide when to say \"I'm ready\"?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/1d56744cf27367eac19f027e672bf2a2_MIT18_440S14_ProblemSet3.pdf",
      "content": "18.440 PROBLEM SET THREE, DUE FEBRUARY 28\nA. FROM TEXTBOOK CHAPTER THREE:\n1. Problem 26: Suppose that 5 percent of men and .25 percent of\nwomen are color blind. A color-blind person is chosen at random.\nWhat is the probability of this person being male? Assume that\nthere are an equal number of males and females. What if the\npopulation consisted of twice as many males as females?\n2. Problem 43: There are 3 coins in a box. One is a two-headed coin,\nanother is a fair coin, and the third is a biased coin that comes up\nheads 75 percent of the time. When one of the 3 coins is selected at\nrandom and flipped, it shows heads. What is the probability that it\nwas the two-headed coin?\n3. Problem 47: An urn contains 5 white and 10 black balls. A fair die is\nrolled and that number of balls is randomly chosen from the urn.\nWhat is the probability that all of the balls selected are white? What\nis the conditional probability that the die landed on 3 if all the balls\nselected are white?\n4. Suppose that E and F are mutually exclusive events of an\nexperiment. Show that if independent trials of this experiment are\nperformed, then E will occur before F with probability\nP (E)/[P (E) + P (F )].\n5. Theroetical Exercise 1. Show that if P (A) > 0, then\nP (AB|A) ≥ P (AB|A ∪ B).\n6. Theoretical Exercise 24: A round-robin tournament of n contestants\nn\nis a tournament in which each of the\npairs of contestants play\neach other exactly once, with the outcome of any play being that one\nof the contestants wins and the other loses. For a fixed integer k,\nk < n, a question of interest is whether it is possible that the\ntournament outcome is such that, for every set of k players, there is a\nplayer who beat each member of that set. Show that if\nn\ni1 kn-k\n1 -\n< 1\nk\nthen such an outcome is possible. Hint: Suppose that the results of\nthe games are independent and that each game is equally likely to be\n\nn\nwon by either contestant. Number the\nsets of k contestants, and\nk\nlet Bi denote the event that no contestant beat all of the k players in\nthe ith set. Then use Boole's inequality to bound P ∪iBi\nB. Suppose that a fair coin is tossed infinitely many times, independently.\nLet Xi denote the outcome of the ith coin toss (an element of {H, T }).\nCompute the probability that:\n1. Xi = H for all positive integers i.\n2. The pattern HHTTHHTT occurs at some point in the sequence\nX1, X2, X3, . . ..\nC. Two unfair dice are tossed. Let pi,j , for i and j in {1, 2, 3, 4, 5, 6},\ndenote the probability that the first die comes up i and the second j.\nSuppose that for any i and j in {1, 2, 3, 4, 5, 6} the event that the first die\ncomes up i is independent of the event that the second die comes up j.\nShow that this independence implies that, as a 6 by 6 matrix, pi,j has rank\none (i.e., show that there is some column of the matrix such that each of\nthe other five column vectors is a constant multiple of that one).\n\n.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/cff6665c188369d15980e2f197c61e7c_MIT18_440S14_ProblemSet4.pdf",
      "content": "18.440 PROBLEM SET FOUR, DUE MARCH 7\nA. FROM TEXTBOOK CHAPTER FOUR:\n1. Problem 23: You have $1000, and a certain commodity presently\nsells $2 per ounce. Suppose that after one week the commodity will\nsell for either $1 or $4 an ounce, with these two possibilities being\nequally likely.\n(a) If your objective is to maximize the expected amount of money\nthat you possess at the end of the week, what strategy should\nyou employ?\n(b) If your objective is to maximize the expected amount of the\ncommodity that you possess at the end of the week, what\nstrategy should you employ?\n2. Problem 35: A box contains 5 red and 5 blue marbles. Two marbles\nare withdrawn randomly. If they are the same color, then you win\n$1.10; if they are different colors, then you win -$1.00. (That is, you\nlose $1.00.) Calculate\n(a) the expected value of the amount you win;\n(b) the variance of the amount you win.\n3. Problem 50: Suppose that a biased coin that lands on heads with\nprobability p is flipped 10 times. Given that a total of 6 heads\nresults, find the conditional probability that the first 3 outcomes are\n(a) h, t, t (meaning that the first flip results in heads, the second in\ntails, and the third in tails);\n(b) t, h, t.\n4. Problem 57: The probability of being dealt a full house in a hand of\npoker is approximately .0014. Find an approximation for the\nprobability that, in 1000 hands of poker, you will be dealt at least 2\nfull houses.\n5. Theoretical Exercise 13: Let X be a binomial random variable with\nparameters (n, p). What value of p maximizes\nP {X = k}, k = 0, 1, 2, . . . , n? This is an example of a statistical\nmethod used to estimate p when a binomial (n, p) random variable is\nobserved to equal k. If we assume that n is known, then we estimate\n\nP\np by choosing that value of p which maximizes P {X = k}. This is\nknown as the method of maximum likelihood estimation.\n6. Theoretical Exercise 19: Show that if X is a Poisson random variable\nwith parameter λ, then\nE[Xn] = λE[(X + 1)n-1].\nNow use this result to compute E[X3].\nB. Define the covariance Cov(X, Y ) = E[XY ] - E[X]E[Y ].\n1. Check that Cov(X, X) = Var(X), that Cov(X, Y ) = Cov(Y, X), and\nthat Cov(·, ·) is a bilinear function of its arguments. That is, if one\nfixes one argument then it is a linear function of the other. For\nexample, if we fix the second argument then for real constants a and\nb we have Cov(aX + bY, Z) = aCov(X, Z) + bCov(Y, Z).\n2. If Cov(Xi, Xj ) = ij, find Cov(X1 - X2, X3 - 2X4).\n3. If Cov(Xi, Xj ) = ij, find Var(X1 + 2X2 + 3X3).\nC. Instead of maximizing her expected wealth E[W ], Jill maximizes\nE[U(W )] where U(x) = -(x - x0)2 and x0 is a large positive number.\nThat is, Jill has a quadratic utility function. (It may seem odd that Jill's\nutility declines with wealth once wealth exceeds x0. Let us assume x0 is\nlarge enough so that this is unlikely.) Jill currently has W0 dollars. You\npropose to sample a random variable X (with mean μ and variance σ2)\nand to give her X dollars (she will lose money if X is negative) so that her\nnew wealth becomes W = W0 + X.\n1. Show that E[U(W )] depends on μ and σ2 (but not on any other\ninformation about the probability distribution of X) and compute\nE[U(W )] as a function of x0, W0, μ, σ2 .\n2. Show that given μ, Jill would prefer for σ2 to be as small as possible.\n(One sometimes refers to σ as risk and says that Jill is risk averse.)\nn\n3. Suppose that X =\ni=1 aiXi where ai are fixed constants and the Xi\nare random variables with E[Xi] = μi and Cov[Xi, Xj ] = σij . Show\nthat in this case E[U(W )] depends on the μi and the σij (but not on\nany other information about the joint probability distributions of the\nXi) and compute E[U(W )]. Hint: first compute the mean and\nvariance of X.\n\n4. Read the Wikipedia article on \"Modern Portfolio Theory\".\nSummarize what you learned in two or three sentences.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/889885184ca98ed6ca2eef9052071b3d_MIT18_440S14_ProblemSet5.pdf",
      "content": "18.440 PROBLEM SET FIVE, DUE MARCH 21\nA. FROM TEXTBOOK CHAPTER FOUR:\n1. Problem 70: At time 0 a coin that comes up heads with probability p\nis flipped and falls to the ground. Suppose it lands on heads. At\ntimes chosen according to a Poisson process with rate λ, the coin is\npicked up and flipped. (Between these times the coin remains on the\nground.) What is the probability that the coin is on its head side at\ntime t? Hint: What would be the conditional probability if there\nwere no additional flips by time t, and what would it be if there were\nadditional flips by time t?\n2. Problem 84: Suppose that 10 balls are put into 5 boxes, with each\nball independently being put in box i with probability pi,\na5\ni=1 pi = 1.\n(a) Find the expected number of boxes that do not have any balls.\n(b) Find the expected number of boxes that have exactly 1 ball.\n3. Theoretical Exercise 16: Let X be a Poisson random variable with\nparameter λ. Show that P {X = i} increases monotonically and then\ndecreases monotonically as i increases, reaching its maximum when i\nis the largest integer not exceeding λ. Hint: Consider\nP {X = i}/P {X = i - 1}.\n4. Theoretical Exercise 25: Suppose that the number of events that\noccur in a specified time is a Poisson random variable with\nparameter λ. If each event is \"counted\" with probability p,\nindependently of every other event, show that the number of events\nthat are counted is a Poisson random variable with parameter λp.\nAlso, give an intuitive argument as to why this should be so. As an\napplication of the preceding result, suppose taht the number of\ndistinct uranium deposits in a given area is a Poisson random\nvariable with parameter λ = 10. If, in a fixed period of time, each\ndeposit is discovered independently with probability\n, find the\nprobability that (a) exactly 1, (b) at least 1, and (c) at most 1\ndeposit is discovered during that time.\n\nB. FROM TEXTBOOK CHAPTER FIVE:\n1. Problem 8: The lifetime in hours of an electronic tube is a random\nvariable having a probability density function given by\n-x\nf(x) = xe\nx ≥ 0\nCompute the expected lifetime of such a tube.\n2. Problem 11: A point is chosen at random on a line segment of length\nL. Interpret this statement, and find the probability that the ratio of\nthe shorter to the longer segment is less than 1/4.\nC. ANSWER THE FOLLOWING:\n1. Compute the expectation of Xn where n is a positive integer and X\nis a uniform random variable on the interval [0, 1].\n2. How does the answer change if the random variable is instead taken\nto be uniform on [0, L] for some constant L?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/d2d0e00badbbf98b6184dbf7948499ae_MIT18_440S14_ProblemSet6.pdf",
      "content": "18.440 PROBLEM SET SIX DUE APRIL 4\nA. FROM TEXTBOOK CHAPTER FIVE:\n1. Problem 23: One thousand independent rolls of a fair die will be\nmade. Compute an approximation to the probability that the\nnumber 6 will appear between 150 and 200 times inclusively. If the\nnumber 6 appears exactly 200 times, find the probability that the\nnumber 5 will appear less than 150 times.\n2. Problem 27: In 10,000 independent tosses of a coin, the coin lands on\nheads 5800 times. Is it reasonable to assume that the coin is not fair?\nExplain.\n3. Problem 32: The time (in hours) required to repair a machine is an\nexponentially distributed random variable with parameter λ = 1/2.\nWhat is\n(a) the probability that a repair time exceeds 2 hours?\n(b) the conditional probability that a repair takes at least 10 hours,\ngiven that its duration exceeds 9 hours?\n4. Theoretical Exercise 9: If X is an exponential random variable with\nparameter λ, and c > 0, show that cX is exponential with parameter\nλ/c.\n√\n5. Theoretical Exercise 21: Show that Γ(1/2) =\nπ. Hint:\na inf\n√\n-x\nΓ(1/2) =\ne\nx-1/2dx. Make the change of variables y =\n2x and\nthen relate the resulting expression to the normal distribution.\n6. Theoretical Exercise 29: Let X be a continuous random variable\nhaving cumulative distribution function F . Define the random\nvariable Y by Y = F (X). Show that Y is uniformly distributed over\n(0, 1).\n7. Theoretical Exercise 30: Let X have probability density fX . Find\nthe probability density function of the random variable Y defined by\nY = aX + b.\nB. At time zero, a single bacterium in a dish divides into two bacteria.\nThis species of bacteria has the following property: after a bacterium B\ndivides into two new bacteria B1 and B2, the subsequent length of time\nuntil each Bi divides is an exponential random variable of rate λ = 1,\nindependently of everything else happening in the dish.\n\n1. Compute the expectation of the time Tn at which the number of\nbacteria reaches n.\n2. Compute the variance of Tn.\n3. Are both of the answers above unbounded, as functions of n? Give a\nrough numerical estimate of the values when n = 1050 .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/9ccc5dabfc106d3d0ccdc4df57d6089c_MIT18_440S14_ProblemSet7.pdf",
      "content": "18.440 PROBLEM SET SEVEN, DUE APRIL 11\nA. FROM TEXTBOOK CHAPTER FIVE:\n1. Theoretical Exercise 26: If X is a beta random variable with\nparameters a and b show that\na\nE[X] =\n,\na + b\nVar(X) =\nab\n.\n(a + b)2(a + b + 1)\n2. Theoretical Exercise 28: Consider the beta distribution with\nparameters (a, b). Show that\n(a) when a > 1 and b > 1, the density is unimodal (that is, it has a\nunique mode) with mode equal to (a - 1)/(a + b - 2);\n(b) when a ≤ 1, b ≤ 1, and a + b < 2, the density is either unimodal\nwith mode at 0 or 1 or U-shaped with modes at both 0 and 1;\n(c) when a = 1 = b, all points in [0, 1] are modes.\nB. FROM TEXTBOOK CHAPTER SIX:\n1. Problem 30: Jill's bowling scores are approximately normally\ndistributed with mean 170 and standard deviation 20, while Jack's\nscores are approximately normally distributed with mean 160 and\nstandard deviation 15. If Jack and Jill each bowl one game, then\nassuming that their scores are independent random variables,\napproximate the probability that\n(a) Jack's score is higher;\n(b) the total of their scores is above 350.\n2. Theoretical Exercise 12: Show that the jointly continuous (discrete)\nrandom variables X1, . . . Xn are independent if and only if their joint\nprobability density (mass) function f(x1, . . . , xn) can be written as\nn\nn\nf(x1, . . . , xn) =\ngi(xi),\ni=1\nfor nonnegative functions gi(x), i = 1, . . . , n.\n\nC. FROM TEXTBOOK CHAPTER SEVEN:\n1. Problems 34: If 10 wife-husband couples are randomly seated at a\nround table, compute (a) the expected number and (b) the variance\nof the number of wives who are seated next to their husbands.\n2. Theoretical Exercise 19: Show that if X and Y are identically\ndistributed (but not necessarily independent) then\nCov(X + Y, X - Y ) = 0.\nD. The following is one formulation of a famous \"two envelope\" paradox.\nJill is a money-loving individual who, given two options, invariably chooses\nthe one that gives her the most money in expectation. One day Harry, a\ntrusted (and capable of delivering) individual, offers her the following deal\nas a gift. He will secretely toss a fair coin until the first time that it comes\nup tails. If there are n heads before the first tails, he will place 10n dollars\nin one envelope and 10n+1 dollars in the second envelope. (Thus, the\nprobability that one envelope has 10n dollars and the other has 10n+1\ndollars is 2-n-1 for n ≥ 0.) Harry will then hand Jill the pair of envelopes\n(randomly ordered, indistinguishable from the outside) and invite her to\nchoose one. After Jill chooses an envelope she will be allowed to open it.\nOnce she does, she will be allowed to either keep the money in the first\nenvelope or switch to the second envelope and keep whatever amount of\nmoney is in the second envelope. However, if she decides to switch\nenvelopes, she has to pay a one dollar \"switching fee.\"\n1. If Jill finds 100 dollars in the first envelope she opens, what is the\nconditional probability that the other envelope contains 1000 dollars?\nWhat is the conditional probability that the other envelope contains\n10 dollars?\n2. If Jill finds 100 dollars in the first envelope she opens, how much\nmoney does Jill expect to win from the game if she does not switch\nenvelopes? (Answer: 100 dollars.) How much does she expect to win\n(net, after the switching fee) if she does switch envelopes?\n3. Generalize the answers above to the case that the first envelope\ncontains 10n dollars (for n ≥ 0) instead of 100.\n4. Jill concludes from the above that, no matter what she finds in the\nfirst envelope, she will expect to earn more money if she switches\nenvelopes and pays the one dollar switching fee. This strikes Jill as a\n\nbit odd. If she knows she will always switch envelopes, why doesn't\nshe just take the second envelope first and avoid the envelope\nswitching fee? How can she be maximizing her expected wealth if she\nspends an unnecessary \"switching fee\" dollar no matter what? How\ndoes one resolve this apparent paradox?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/c820f2196fa2e0c5fb83c2837ecf9d03_MIT18_440S14_ProblemSet8.pdf",
      "content": "18.440 PROBLEM SET 8: DUE APRIL 25\nA. FROM TEXTBOOK CHAPTER SEVEN:\n1. Problem 51: The joint density of X and Y is given by f(x, y) = e-y ,\ny\n0 < x < y, 0 < y < inf. Compute E[X3|Y = y].\n2. Problem 67: Consider a gambler who, at each gamble, either wins or\nloses her bet with respective probabilities p and 1 - p. A popular\ngambling system knkown as the Kelly strategy is to always bet the\nfraction 2p - 1 of your current fortune when p > 1/2. Compute the\nexpected fortune after n gambles of a gambler who starts with x\nunits and employs the Kelly strategy.\n3. Problem 76: Let X be the value of the first die and Y the sum of the\nvalues when two standard (six-sided) dice are rolled. Compute the\njoint moment generating function of X and Y .\n4. Theoretical Exercise 29: Let X1, . . . , Xn be independent and\nidentically distriuted random variables. Find\nE[X1|X1 + . . . + Xn = x].\n5. Theoretical Exercise 36: One ball at a time is randomly selected from\nan urn containing a white and b black balls until all of the remaining\nballs are of the same color. Let Ma,b denote the expected number of\nballs left in the urn when the experiment ends. Compute a recursive\nformula for Ma,b and solve when a = 3 and b = 5.\n6. Theoretical Exercise 48: If Y = aX + b, where a and b are constants,\nexpress the moment generating function of Y in terms of the moment\ngenerating function of X.\n7. Theoretical Exercise 52: Show how to compute Cov(X, Y ) from the\njoint moment generating function of X and Y .\n8. Theoretical Exercise 54: If Z is a standard normal random variable,\nwhat is Cov(Z, Z2)?\nB. (Just for fun -- not to hand in) Let V = (V1, V2, . . . , Vn) be a random\nvector whose components Vi are independent, identically distributed\nnormal random variables of mean zero, variance one. Note that the density\nfunction for V may be written as f(v) = (2π)-n/2e-|v|2/2 where\nv = (v1, v2, . . . , vn) and |v|2 = v + v + . . . + v .\nn\n\n1. Let M be an n × n matrix. Write W = MV and compute the mean\nand covariance of Wi for each 1 ≤ i ≤ n.\n2. Write the probability density function for W .\n3. Classify the set of matrices M for which MV has the same\nprobability density function as V .\n4. Is every n-dimensional mean zero multivariate normal distribution\n(as defined in Section 7.8 of the textbook) the distribution of MV for\nsome choice of M? If so, to what extent does the distribution\nuniquely determine M?\nC.(Just for fun -- not to hand in) Try to formulate and prove a version of\nthe central limit theorem that shows that sums of independent heavy-tailed\nrandom variables (divided by appropriate constants) converge in law to a\nstable random variable (instead of a normal random variable). See for\nexample http://eom.springer.de/A/a013920.htm or the wikipedia\narticles on stable distributions for definitions and hints. You will need to\nuse characteristic functions instead of the moment generating function.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Probability and Random Variables, Problem Set 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/0662f3125596a911820426fb87a88276_MIT18_440S14_ProblemSet9.pdf",
      "content": "18.440 PROBLEM SET 9: DUE MAY 2\nA. FROM TEXTBOOK CHAPTER EIGHT:\n1. Problem 7: A person has 100 light bulbs whose lifetimes are\nindependent exponentials with mean 5 hours. If the bulbs are used\none at a time, with a failed bulb being replaced immediately by a\nnew one, approximate the probability that there is still a working\nbulb after 525 hours.\n2. Problem 15: An insurance company has 10,000 automobile\npolicyholders. The expected yearly claim per policy-holder is $240,\nwith a standard deviation of $800. Approximate the probability that\nthe total yearly claim exceeds $2.7 million.\n3. Theoretical Exercise 8: Explain why a gamma random variable with\nparameters (t, λ) has an approximately normal distribution when t is\nlarge.\n4. Problem/Theoretical Exercises 9: Suppose a fair coin is tossed 1000\ntimes. If the first 100 tosses all result in heads, what proportion of\nheads would you expect on the final 900 tosses? Comment on the\nstatement \"The strong law of large numbers swamps, but does not\ncompensate.\"\n5. Problem/Theoretical Exercises 13: Show that if E[X] < 0 and θ = 0\nis such that E[eθX ] = 1, then θ > 0.\nB. FROM TEXTBOOK CHAPTER NINE:\n1. Problem/Theoretical Exercises 7: A transition matrix is said to be\nM\ndoubly stochastic if\nPij = 1 for all states j = 0, 1, . . . , M. Show\ni=0\nthat if such a Markov chain is ergodic, then\n\nj = 1/(M + 1), j = 0, 1, . . . , M.\n2. Problem/Theoretical Exercises 9: Suppose that whether it rains\ntomorrow depends on past weather conditions only through the last\n2 days. Specifically, suppose that if it has rained yesterday and\ntoday, then it will rain tomorrow with probability .8; if it rained\nyesterday but not today, then it will rain tomorrow with probability\n.3; if it rained today but not yesterday, then it will rain tomorrow\nwith probability .4; and if it has not rained either yesterday or today,\nthen it will rain tomorrow with probability .2. Over the long term,\nwhat proportion of days does it rain?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "18.440 Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/43e2c6c316ec8fead65c1307f0ada51e_MIT18_440S14_final_2011.pdf",
      "content": "18.440 Final Exam: 100 points\nCarefully and clearly show your work on each problem (without\nwriting anything that is technically not true) and put a box\naround each of your final computations.\n\n1. (10 points) Let X be the number on a standard die roll (i.e., each of\n{1, 2, 3, 4, 5, 6} is equally likely) and Y the number on an independent\nstandard die roll. Write Z = X + Y .\n1. Compute the condition probability P[X = 4|Z = 6].\n2. Compute the conditional expectation E[Z|Y ] as a function of Y .\n\n2. (10 points) Janet is standing outside at time zero when it starts to\ndrizzle. The times at which raindrops hit her are a Poisson point process\nwith parameter λ = 2. In expectation, she is hit by 2 raindrops in each\ngiven second.\n(a) What is the expected amount of time until she is first hit by a\nraindrop?\n(b) What is the probability that she is hit by exactly 4 raindrops during\nthe first 2 seconds of time?\n\n3. (10 points) Let X be a random variable with density function f,\ncumulative distribution function F, variance V and mean M.\n(a) Compute the mean and variance of 3X + 3 in terms of V and M.\n(b) If X1, . . . , Xn are independent copies of X. Compute (in terms of F)\nthe cumulative distribution function for the largest of the Xi.\n\n4. (10 points) Suppose that Xi are i.i.d. random variables, each P\nuniform\nn\non [0, 1]. Compute the moment generating function for the sum\ni=1 Xi.\n\n5. (10 points) Suppose that X and Y are outcomes of independent\nstandard die rolls (each equal to {1, 2, 3, 4, 5, 6} with equal probability).\nWrite Z = X + Y .\n(a) Compute the entropies H(X) and H(Y ).\n(b) Compute H(X, Z).\n(c) Compute H(10X + Y ).\n(d) Compute H(Z) + HZ(Y ). (Hint: you shouldn't need to do any more\ncalculations.)\n\n6. (10 points) Elaine's not-so-trusty old car has three states: broken (in\nElaine's possession), working (in Elaine's possession), and in the shop.\nDenote these states B, W, and S.\n(i) Each morning the car starts out B, it has a .5 chance of staying B\nand a .5 chance of switching to S by the next morning.\n(ii) Each morning the car starts out W, it has .5 chance of staying W,\nand a .5 chance of switching to B by the next morning.\n(iii) Each morning the car starts out S, it has a .5 chance of staying S and\na .5 chance of switching to W by the next morning.\nAnswer the following\n(a) Write the three-by-three Markov transition matrix for this problem.\n(b) If the car starts out B on one morning, what is the probability that\nit will start out B two days later?\n(c) Over the long term, what fraction of mornings does the car start out\nin each of the three states, B, S, and W?\n\n7. Suppose that X1, X2, X3, . . . is an infinite sequence of independent\nrandom variables which are each equal to 2Qwith probability 1/3 and .5\nn\nwith probability 2/3. Let Y0 = 1 and Yn =\ni=1 Xi for n ≥1.\n(a) What is the the probability that Yn reaches 8 before the first time\nthat it reaches 1?\n(b) Find the mean and variance of log Y10000.\n(c) Use the central limit theorem to approximate the probability that\nlog Y10000 (and hence Y10000) is greater than its median value.\n\n8. (10 points) Eight people toss their hats into a bin and the hats are\nredistributed, with all of the 8! hat permutations being equally likely. Let\nN be the number of people who get their own hat. Compute the following:\n(a) E[N]\n(b) Var[N]\n\n9. (10 points) Let X be a normal random variable with mean μ and\nvariance σ2.\n(a) EeX.\n(b) Find μ, assuming that σ2 = 3 and E[eX] = 1.\n\n10. (10 points)\n1. Let X1, X2, . . . be independent random variables, each equal to 1\nwith probability 1/2 and -1 with probability 1/2. In which of the\ncases below is the sequence Yn a martingale? (Just circle the\ncorresponding letters.)\n(a) Yn = Xn\n(b) Yn = 1 + Xn\n(c) Yn = 7\nPn\n(d) Yn =\ni=1 iX\nQ\ni\nn\n(e) Yn =\ni=1(1 + Xi)\nPn\n2. Let Yn =\ni=1 Xi. Which of the following is necessarily a stopping\ntime for Yn?\n(a) The smallest n for which |Yn| = 5.\n(b) The largest n for which Yn = 12 and n < 100.\n(c) The smallest value n for which n > 100 and Yn = 12.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "18.440 Final Exam Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/04a943be98f224cf8df50c0a40962d78_MIT18_440S14_final2011_sol.pdf",
      "content": "18.440 Final Exam: 100 points\nCarefully and clearly show your work on each problem (without\nwriting anything that is technically not true) and put a box\naround each of your final computations.\n1. (10 points) Let X be the number on a standard die roll (i.e., each of\n{1, 2, 3, 4, 5, 6} is equally likely) and Y the number on an independent\nstandard die roll. Write Z = X + Y .\n1. Compute the condition probability P [X = 4|Z = 6]. ANSWER:\n1/5\n2. Compute the conditional expectation E[Z|Y ] as a function of Y .\nANSWER: Y + 7/2.\n2. (10 points) Janet is standing outside at time zero when it starts to\ndrizzle. The times at which raindrops hit her are a Poisson point process\nwith parameter λ = 2. In expectation, she is hit by 2 raindrops in each\ngiven second.\n(a) What is the expected amount of time until she is first hit by a\nraindrop? ANSWER: 1/2 second\n(b) What is the probability that she is hit by exactly 4 raindrops during\nthe first 2 seconds of time? ANSWER: e-2λ(2λ)k/k! = e-444/4!.\n3. (10 points) Let X be a random variable with density function f,\ncumulative distribution function F , variance V and mean M.\n(a) Compute the mean and variance of 3X + 3 in terms of V and M.\nANSWER: Mean 3M + 3, variance 9V .\n(b) If X1, . . . , Xn are independent copies of X. Compute (in terms of F )\nthe cumulative distribution function for the largest of the Xi.\nANSWER: F (a)n . This is the probability that all n values are less\nthan a.\n4. (10 points) Suppose that Xi are i.i.d. random variables, each uniform\nu n\non [0, 1]. Compute the moment generating function for the sum\ni=1 Xi.\n= EaX1\nANSWER: MaX1\n=\neaxdx = (ea - 1)/a. Moment generating\nfunction for sum is (ea - 1)n/an .\n5. (10 points) Suppose that X and Y are outcomes of independent\nstandard die rolls (each equal to {1, 2, 3, 4, 5, 6} with equal probability).\nWrite Z = X + Y .\n\n(a) Compute the entropies H(X) and H(Y ). ANSWER: log 6 and log 6\n(b) Compute H(X, Z). ANSWER: log 36 = 2 log 6.\n(c) Compute H(10X + Y ). ANSWER: log 36 = 2 log 6 (since 36 sums\nall distinct).\n(d) Compute H(Z) + HZ (Y ). (Hint: you shouldn't need to do any more\ncalculations.) ANSWER: log 36\n6. (10 points) Elaine's not-so-trusty old car has three states: broken (in\nElaine's possession), working (in Elaine's possession), and in the shop.\nDenote these states B, W, and S.\n(i) Each morning the car starts out B, it has a .5 chance of staying B\nand a .5 chance of switching to S by the next morning.\n(ii) Each morning the car starts out W, it has .5 chance of staying W,\nand a .5 chance of switching to B by the next morning.\n(iii) Each morning the car starts out S, it has a .5 chance of staying S and\na .5 chance of switching to W by the next morning.\nAnswer the following\n(a) Write the three-by-three Markov transition matrix for this problem.\nANSWER: Markov chain matrix is\n⎛\n⎞\n.5\n.5\nM = ⎝.5 .5\n0 ⎠\n.5 .5\n(b) If the car starts out B on one morning, what is the probability that\nit will start out B two days later? ANSWER: 1/4\n(c) Over the long term, what fraction of mornings does the car start out\nin each of the three states, B, S, and W ? ANSWER: Row vector π\nsuch that πM = π (with components of π summing to one) is\ns\n\n3 .\n7. Suppose that X1, X2, X3, . . . is an infinite sequence of independent\nrandom variables which are each equal to 2 with probability 1/3 and .5\nn\nwith probability 2/3. Let Y0 = 1 and Yn =\nXi for n ≥ 1.\ni=1\n\n(a) What is the the probability that Yn reaches 8 before the first time\nthat it reaches 1 ? ANSWER: sequences is martingale, so\n1 = EYT = 8p + (1/8)(1 - p). Solving gives 1 - 8p = (1 - p)/8, so\n8 - 64p = 1 - p and 63p = 7. Answer is p = 1/9.\n(b) Find the mean and variance of log Y10000. ANSWER: Compute for\nlog Y1, multiply by 10000.\n(c) Use the central limit theorem to approximate the probability that\nlog Y10000 (and hence Y10000) is greater than its median value.\nANSWER: About .5.\n8. (10 points) Eight people toss their hats into a bin and the hats are\nredistributed, with all of the 8! hat permutations being equally likely. Let\nN be the number of people who get their own hat. Compute the following:\n(a) E[N] ANSWER: 1\n(b) Var[N] ANSWER: 1\n9. (10 points) Let X be a normal random variable with mean μ and\nvariance σ2 .\nX\nμ+σ2/2\n(a) Ee . ANSWER: e\n.\n(b) Find μ, assuming that σ2 = 3 and E[eX ] = 1. ANSWER:\nμ + σ2/2 = 0 so μ = -9/2.\n10. (10 points)\n1. Let X1, X2, . . . be independent random variables, each equal to 1\nwith probability 1/2 and -1 with probability 1/2. In which of the\ncases below is the sequence Yn a martingale? (Just circle the\ncorresponding letters.)\n(a) Yn = Xn NO\n(b) Yn = 1 + Xn NO\n(c) Yn = 7 YES\nu n\n(d) Yn =\ni=1 iXi YES\nn\n(e) Yn =\ni=1(1 + Xi) YES\nu n\n2. Let Yn =\ni=1 Xi. Which of the following is necessarily a stopping\ntime for Yn?\nQ\n\n(a) The smallest n for which |Yn| = 5. YES\n(b) The largest n for which Yn = 12 and n < 100. NO\n(c) The smallest value n for which n > 100 and Yn = 12. YES\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 1 Solutions, Spring 2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/aeb87aac43fc0bff4f43d677e5c31bd4_MIT18_440S14_mid1_2011_sol.pdf",
      "content": "18.440 Midterm 1, Spring 2011: 50 minutes, 100 points.\nSOLUTIONS\n1. (20 points) Consider an infinite sequence of independent tosses of a coin\nthat comes up heads with probability p.\n(a) Let X be such that the first heads appears on the Xth toss. In other\nwords, X is the number of tosses required to obtain a heads.\nCompute (in terms of p) the expectation E[X]. ANSWER:\ngeometric random variable with parameter p has\nexpectation 1/p.\n(b) Compute (in terms of p) the probability that exactly 5 of the first 10\nR N\ntosses are heads. ANSWER: binomial probability\np5(1 - p)5\n(c) Compute (in terms of p) the probability that the 5th head appears\non the 10th toss. ANSWER: negative binomial. Need 4 heads\nR N\nin first 9 tosses, 10th toss heads. Probability\np4(1 - p)5p.\n2. (20 points) Jill sends her resume to 1000 companies she finds on\nmonster.com. Each company responds with probability 3/1000\n(independently of what all the other companies do). Let R be the number\nof companies that respond.\n(a) Compute E[R]. ANSWER: binomial random variable with\nn = 1000 and p = 3/1000. E[R] = np = 3.\n(b) Compute Var[R]. ANSWER: binomial random variable with\nn = 1000 and p = 3/1000. Var[R] = np(1 - p) = 3(1 - 3/1000).\n(c) Use a Poisson random variable approximation to estimate the\nprobability P {R = 3}. ANSWER: R is approximately Poisson\nwith λ = 3. So P {R = 3} ≈ e-λλk/k! = e-333/3! = 9e-3/2.\n3. (10 points) How many four-tuples (x1, x2, x3, x4) of non-negative\nintegers satisfy x1 + x2 + x3 + x4 = 10? ANSWER: represent partition\nR N\nwith stars and bars ∗ ∗ | ∗ ∗|| ∗ ∗ ∗ ∗ ∗ ∗. Have\nways to do this.\n4. (10 points) Suppose you buy a lottery ticket that gives you a one in a\nmillion chance to win a million dollars. Let X be the amount you win.\nCompute the following:\n(a) E[X]. ANSWER:\n6 106 = 1.\n(b) Var[X]. ANSWER: E[X2] - E[X]2 = 106 (106)2 - 12 = 106 - 1.\n\n5. (20 points) Suppose that X is continuous random variable with\n2x\nx ∈ [0, 1]\nprobability density function fX (x) =\n. Compute the\nx ∈ [0, 1]\nfollowing:\ni\n(a) The expectation E[X]. ANSWER:\ninf\n-inf\nW\nW\nW\n2 3 1\n1 2x2dx = x |\nfX (x)xdx =\nfX (x)xdx =\n= 2/3.\n(b) The variance Var[X]. ANSWER:\n2dx =\nfX (x)x2dx =\nW\nSo variance is 1/2 - (2/3)2 = 1/2 - 4/9 = 1/18.\nW\nW 1 2x\ninf\nE[X2] =\n3dx =\n4 1 = 1/2.\nx |0\nfX (x)x\n-inf\n(c) The cumulative distribution function FX . ANSWER:\na < 0\n⎧\n⎪\n⎨\nW a\nFX (a) =\nfX (x)dx =\na\na ∈ [0, 1] .\na > 1\n-inf\n⎪\n⎩\n6. (20 points) A standard deck of 52 cards contains 4 aces. Suppose we\nchoose a random ordering (all 52! permutations being equally likely).\nCompute the following:\n(a) The probability that all of the top 4 cards in the deck are aces.\nANSWER: 4! ways to order aces, 48! ways to order\nremainder. Probability 4!48!/52!\n(b) The probability that none of the top 4 cards in the deck is an\nace.ANSWER: choose cards one at a time starting at the\ntop and multiply number of available choices at each stage\nto get total number. Probability is 48 · 47 · 46 · 45 · 48!/52!.\n(c) The expected number of aces among the top 4 cards in the deck.\n(There is a simple form for the solution.) ANSWER: have\nprobability 4/52 = 1/13 that top card is an ace. Similarly,\nprobability 1/13 that jth card is an ace for each\nj ∈{1, 2, 3, 4}. Additivity of expectation gives answer: 4/13.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 1 Solutions, Spring 2014",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/44d951803afa1389031c49933115b24e_MIT18_440S14_mid1_2014_sol.pdf",
      "content": "18.440 Midterm 1, Spring 2014: 50 minutes, 100 points\n1. (10 points) How many quintuples (a1, a2, a3, a4, a5) of non-negative\nintegers satisfy a1 + a2 + a3 + a4 + a5 = 100? ANSWER: This the number\n\ns\n104!\nof ways to make a list of \"100 stars and 4 bars\", which is\n=\n4!100! .\n2. (20 points) Thirty people are invited to a party. Each person accepts\nthe invitation, independently of all others, with probability 1/3. Let X be\nthe number of accepted invitations. Compute the following:\n(a) E[X] ANSWER: X is binomial with n = 30 and p = 1/3, so the\nexpectation is np = 10.\n(b) Var[X] ANSWER: X is binomial with n = 30 and p = 1/3, so the\nvariance is npq = np(1 - p) = 20/3.\n(c) E[X2] ANSWER: Var(X) = E[X2] - E[X]2 . Using previous two\nparts and solving gives E[X2] = 20/3 + 100 = 320/3.\n(d) E[X2 - 4X + 5] ANSWER: By linearity of expectation, this is\nE[X2] - 4E[X] + 5 = 320/3 - 40 + 5 = 215/3.\n3. (20 points) Bob has noticed that during every given minute, there is a\n1/720 chance that the Facebook page for his dry cleaning business will get\na \"like\", independently of what happens during any other minute. Let L\nbe the total number of likes that Bob receives during a 24 hour period.\n(a) Compute E[L] and Var[L]. (Give exact answers, not approximate\nones.) ANSWER: This is binomial with n = 60 × 24 and\np = 1/720. So E[L] = np = 2 and Var[L] = np(1 - p) = 2719\n720 .\n(b) Compute the probability that L = 0. (Give an exact answer, not an\n719 s1440\napproximate answer.) ANSWER: (1 - p)n =\n(c) Bob is really hoping to get at least 2 more likes during the next 24\nhours (because this would boost his cumulative total to triple digits).\nUse a Poisson random variable calculation to approximate the\nprobability that L ≥ 2. ANSWER: Note that L is approximately\nbinomial with parameter λ = E[L] = 2. Thus P {L ≥ 2} = 1 - P {L =\n1} - P {L = 0} ≈ 1 - e-λλ0/0! - e-λλ1/1! = 1 - 3e-2 = 1 - 3/e2\n4. (10 points) Consider an infinite sequence of independent tosses of a coin\nthat comes up heads with probability p. Compute (in terms of p) the\n\nprobability that the fifth head occurs on the tenth toss. ANSWER: This\nis the probability that exactly four of the first nine tosses are heads, and\ns\nthen the tenth toss is also heads. This comes to\np5(1 - p)5 .\n5. (20 points) Let X be the number on a standard die roll (assuming\nvalues in {1, 2, 3, 4, 5, 6} with equal probability). Let Y be the number on\nan independent roll of the same die. Compute the following:\n(a) The expectation E[X2]. ANSWER:\n(1 + 4 + 9 + 16 + 25 + 36)/6 = 91/6.\n(b) The expectation E[XY ]. ANSWER: By independence of X and Y ,\nwe have E[XY ] = E[X]E[Y ] = (7/2)2 = 49/4.\n(c) The covariance Cov(X, Y ) = E[XY ] - E[X]E[Y ]. ANSWER:\nBecause of independence, Cov(X, Y ) = 0.\n6. (20 points) Three hats fall out of their assigned bins and are randomly\nplaced back in bins, one hat per bin (with all 3! reassignments being\nequally likely). Compute the following:\n(a) The expected number of hats that end up in their own bins.\nANSWER: Let Xi be 1 if ith hat ends up in own bin, zero\notherwise. Then X = X1 + X2 + X3 is total number of hats to end\nup in their own bins, and E[X] = E[X1] + E[X2] + E[X3] = 31 = 1.\n(b) The probability that the third hat ends up in its own bin.\nANSWER: 1/3\n(b) The conditional probability that the third hat ends up in its own bin\ngiven that the first hat does not end up in its own bin. ANSWER:\nLet A be event third hat gets own bin, B event that first hat does\nnot end up in its own bin. Then P (A) = 1/3 and P (B) = 2/3. There\nis only one permutation that assigns the third hat to its own bin and\ndoes not assign first hat to its own bin, so P (AB) = 1/6. Thus\nP (A|B) = P (AB)/P (B) = (1/6)/(2/3) = 1/4.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 1, Fall 2009",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/62a760cfe1ef2dc1df185ce2b59d0253_MIT18_440S14_mid1_2009.pdf",
      "content": "18.440 Midterm 1, Fall 2009: 50 minutes, 100 points\n1. Carefully and clearly show your work on each problem (with\nout writing anything that is technically not true).\n2. No calculators, books, or notes may be used.\n1. (20 points) Evaluate the following explicitly:\n10!\n(a)\n9i\n= (1 + 9)10 = 1010 by the Binomial theorem.\ni=0\ni!(10-i)!\n2-9\n9!\n(b)\n=\n( 1 + 1 )9 =\nby the Binomial theorem and\ni=5\ni!(9-i)!\ni\n\ni\n=\n.\ni\n9-i\n2. (20 points) Six people, labeled {1, 2, 3, 4, 5, 6}, each own one hat. They\nthrow their hats into a box, and each person removes and holds onto one\nof the six hats (with all of the 6! hat orderings being equally likely). Let M\nbe the number of people who get their own hat. A pair of people is called\na swapped pair if each one has the other person's hat. For example, if\nperson 1 has person 4's hat and person 4 has person 1's hat, then 1 and 4\nconstitute a single swapped pair. (There can be at most three swapped\npairs.) Let S be the number of swapped pairs. Compute the following:\n(a) E[M] Let Xi be 1 if ith person gets own hat, 0 otherwise. Then\nE[M] = E[\nXi] =\nEXi = 6\n= 1\ni=1\ni=1\n.\n(b) E[S] Let Xi,j be 1 if i and j are a swapped pair, zero otherwise. Then\n\n1 1\nE[S] = E[\nXi] =\nEX1,2 =\n= .\n6 5\n1≤i<j≤6\n(c)\n6 6\nVar[M] =\nCov[Xi, Xj ] = 6Cov[X1, X1]+30Cov[X1, X2] = 6\n+30\n= 1.\ni=1 j=1\nThe first equality is a basic property of variance. The second equality\ncomes from expanding the sum and collapsing symmetrically\nequivalent terms (e.g., Cov(X1, X1) = Cov(X2, X2)). We have\nCov(X1, X1) = Var(X1) =\n. Also,\nCov(X1, X2) = E[X1X2] - E[X1]E[X2] =\n-\n=\n.\n\n3. (20 points) Let D1 and D2 be the outcomes (in {1, 2, 3, 4, 5, 6}) of two\nindependent fair die rolls. Let Yi be the random variable which is equal to\n1 if D1 = i and 0 otherwise. Compute the following:\n] = (91)2\n(a) E[D1\n2D2] = E[D2]E[D2\n=\nby independence and\n1+4+9+16+25+36\nED2 =\n=\n.\ni\n(b) Var[D1 - D2] = Var[D1] + Var[D2] - 2Cov[D1, D2] = 2Var[D1] = 35 .\n(c) Cov(Y1 + Y2 + Y3, Y5 + Y6) = 6Cov[Y1, Y5] = -1 by bilinearity of\ncovariance.\n(d) Var[\nYi]. The sum is constant, so the variance is zero.\ni=1\n4. (20 points) Let X1, X2, and X3 be independent Poissonian random\nvariables with parameters λ1 = 1, λ2 = 2, λ3 = 3, respectively. Compute\nthe probabilities of the following events:\n(a) The largest of X1, X2, and X3 is at least 1. One minus the\n-λ1 e-λ2 e-λ3\n-6\nprobability they are all zero is 1 - e\n= 1 - e\n.\n(b) The largest of X1, X2, and X3 is exactly 1. The probability that each\nXi is 1 or 0 is\n-λi\n-λ1λ2λ3\n-6\n(e\n+ λie -λi ) = e\n(1 + λi) = 24e\n.\ni=1\ni=1\n-6\nSubtracting the probability that the Xi are all zero yields 23e\n.\n5. (20 points) There are ten children: five attend school A, three attend\nschool B, and two attend school C. Suppose that a pair of two children is\nchosen uniformly at random from the set of all possible pairs of children.\nLet a be the number of students in the random pair that attend school A\nand let b be the number in the pair that attend school B. (So both a and b\ntake values in the set {0, 1, 2}.)\n(a) Compute E[ab]. Product will be non-zero (and equal to 1) only if\na = b = 1. The expectation is the probability of this: (10 = .\n2 )\n(b) Given that the two children in this pair attend the same school, what\nis the conditional probability that they both attend school A?\ni\ni\ni\ni =\n= .\n+\n+\nP\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 1, Spring 2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/ee62c138daa6fd21f732e5123a7cc91c_MIT18_440S14_mid1_2011.pdf",
      "content": "18.440 Midterm 1, Spring 2011: 50 minutes, 100 points\n1. Carefully and clearly show your work on each problem (with\nout writing anything that is technically not true). In partic\nular, if you use any known facts (or facts proved in lecture)\nyou should state clearly what fact you are using and why it\napplies.\n2. No calculators, books, or notes may be used.\n3. Simplify your answers as much as possible (but answers may\ninclude factorials -- no need to multiply them out).\n\n1. (20 points) Consider an infinite sequence of independent tosses of a coin\nthat comes up heads with probability p.\n(a) Let X be such that the first heads appears on the Xth toss. In other\nwords, X is the number of tosses required to obtain a heads.\nCompute (in terms of p) the expectation E[X].\n(b) Compute (in terms of p) the probability that exactly 5 of the first 10\ntosses are heads.\n(c) Compute (in terms of p) the probability that the 5th head appears\non the 10th toss.\n\n2. (20 points) Jill sends her resume to 1000 companies she finds on\nmonster.com. Each company responds with probability 3/1000\n(independently of what all the other companies do). Let R be the number\nof companies that respond.\n(a) Compute E[R]. (Give an exact answer, not an approximate one.)\n(b) Compute Var[R]. (Give an exact answer, not an approximate one.)\n(c) Use a Poisson random variable approximation to estimate the\nprobability P {R = 3}.\n\n3. (10 points) How many four-tuples (x1, x2, x3, x4) of non-negative\nintegers satisfy x1 + x2 + x3 + x4 = 10?\n\n4. (10 points) Suppose you buy a lottery ticket that gives you a one in a\nmillion chance to win a million dollars. Let X be the amount you win.\nCompute the following:\n(a) E[X].\n(b) Var[X].\n\n(\n5. (20 points) Suppose that X is continuous random variable with\nprobability density function fX (x) =\n2x\nx ∈ [0, 1] . Compute the\nx 6∈ [0, 1]\nfollowing:\n(a) The expectation E[X].\n(b) The variance Var[X].\n(c) The cumulative distribution function FX .\n\n6. (20 points) A standard deck of 52 cards contains 4 aces. Suppose we\nchoose a random ordering (all 52! permutations being equally likely).\nCompute the following:\n(a) The probability that all of the top 4 cards in the deck are aces.\n(b) The probability that none of the top 4 cards in the deck is an ace.\n(c) The expected number of aces among the top 4 cards in the deck.\n(There is a simple form for the solution.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 1, Spring 2014",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/e99b68c15e006a2ef7a4ef084a1ac801_MIT18_440S14_mid1_2014.pdf",
      "content": "18.440 Midterm 1, Spring 2014: 50 minutes, 100 points\n1. Carefully and clearly show your work on each problem (with\nout writing anything that is technically not true). In partic\nular, if you use any known facts (or facts proved in lecture)\nyou should state clearly what fact you are using and why it\napplies.\n2. No calculators, books, or notes may be used.\n3. Simplify your answers as much as possible (but answers may\ninclude factorials -- no need to multiply them out).\n\n1. (10 points) How many quintuples (a1, a2, a3, a4, a5) of non-negative\nintegers satisfy a1 + a2 + a3 + a4 + a5 = 100?\n\n2. (20 points) Thirty people are invited to a party. Each person accepts\nthe invitation, independently of all others, with probability 1/3. Let X be\nthe number of accepted invitations. Compute the following:\n(a) E[X]\n(b) Var[X]\n(c) E[X2]\n(d) E[X2 - 4X + 5]\n\n3. (20 points) Bob has noticed that during every given minute, there is a\n1/720 chance that the Facebook page for his dry cleaning business will get\na \"like\", independently of what happens during any other minute. Let L\nbe the total number of likes that Bob receives during a 24 hour period.\n(a) Compute E[L] and Var[L]. (Give exact answers, not approximate\nones.)\n(b) Compute the probability that L = 0. (Give an exact answer, not an\napproximate answer.)\n(c) Bob is really hoping to get at least 2 more likes during the next 24\nhours (because this would boost his cumulative total to triple digits).\nUse a Poisson random variable calculation to approximate the\nprobability that L ≥ 2.\n\n4. (10 points) Consider an infinite sequence of independent tosses of a coin\nthat comes up heads with probability p. Compute (in terms of p) the\nprobability that the fifth head occurs on the tenth toss.\n\n5. (20 points) Let X be the number on a standard die roll (assuming\nvalues in {1, 2, 3, 4, 5, 6} with equal probability). Let Y be the number on\nan independent roll of the same die. Compute the following:\n(a) The expectation E[X2].\n(b) The expectation E[XY ].\n(c) The covariance Cov(X, Y ) = E[XY ] - E[X]E[Y ].\n\n6. (20 points) Three hats fall out of their assigned bins and are randomly\nplaced back in bins, one hat per bin (with all 3! reassignments being\nequally likely). Compute the following:\n(a) The expected number of hats that end up in their own bins.\n(b) The probability that the third hat ends up in its own bin.\n(b) The conditional probability that the third hat ends up in its own bin\ngiven that the first hat does not end up in its own bin.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 2 Solutions, Fall 2009",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/29c7b6768729b8c8f1e409ccf87484d4_MIT18_440S14_mid2_2009_sol.pdf",
      "content": "18.440 Midterm 2 Solutions, Fall 2009\n1.\n(a) P {X < a} = a2 for a ∈ (0, 1) and thus FX (a) = a . Differentiating\ngives\n\n2a a ∈ [0, 1]\nfX (a) =\notherwise\n(b) E[X] =\nfX (x)xdx =\n2x2dx = 2/3.\n(c) The pair (X, Y ) is uniformly distributed on the triangle\n{(x, y) : x ∈ [0, 1], y ∈ [0, 1], x > y}. Thus, conditioned on X, Y is\nuniform on [0, X], so E[Y |X] = X/2.\n(d) Cov(X, Y ) = E[XY ] - E[X]E[Y ] = E[A1A2] - E[X](E[1 - X]) =\n1/4 - 2/9 = 1/36.\n2.\n(a) As derived in lecture and on problem sets, the collection of two\nPoisson processes (the earthquake and the flood processes) may be\nconstructed equivalently by first taking a Poisson point process of\nrate 2 + 3 = 5 and then independently declaring each point in the\nprocess to be an earthquake with probability 3/5 (and a flood\notherwise). Thus, the number N of earthquakes before the first flood\nsatisfies P {N = k} = (3/5)k(2/5).\n(b) One may recall that the sum of independent rate one exponentials is\na gamma distribution with α = 2 and λ = 1: the density is thus\n\n-λt\n-t\ne\ntα-1λα/Γ[α] = e\nt\nt ≥ 0\nf(t) =\n.\notherwise\nThis can also be derived directly by letting X and Y denote the two\nexponential random variables and writing\nZ t\nZ t\n-x\n-t\nfX+Y (t) =\nfX (x)fY (t - x)dx =\ne\ne -(t-x)dx = te\n.\nx=0\nx=0\n\n(c)\nCov(min{E, F, M}, M) = Cov(min{E, F, M}, min{E, F, M})\n+Cov(min{E, F, M}, M - min{E, F, M})\nThe memoryless property of exponentials implies that min{E, F, M}\nand M - min{E, F, M} are independent, so this becomes\nVar(min{E, F, M}). This is the variance of an exponential of rate 6,\nwhich is 1/36.\n3.\n(a) E[X2Y 2] - E[XY ]2 = E[X2]E[Y 2] - E[X]2E[Y ]2 = E[X2]E[Y 2] = 1\n3 .\ntX\ntX ]E[e\nt2/2 et-1\n(b) E[e\netY ] = E[e\ntY ] = e\n.\nt\n4.\n(a) Cov(X, Y ) = Cov(\nXi,\nXj ) =\nCov(Xi, Xi) = 20.\ni=1\nj=41\ni=41\n√\nSince Var(X) = Var(Y ) = 60, we have ρ(X, Y ) = 20/ 60 · 60 = 1/3.\n(b) Let Z =\nThen X and Z are independent with joint\ni=61 Xi.\n-a2/120\n-b2/80\ndensity f(a, b) = √\n√ e\n√\n√ e\n. Conditioning on\n2π 60\n2π 40\nX + Z = x restricts us to the line a + b = x, so the conditional\ndensity for X will be (for some constant C)\n2/120 -(x-a)2/80\nf(a) = Cf(a, x - a) = Ce-a\ne\n,\ninf\nwhere C is chosen so that\nf(a)da = 1.\n-inf\n5.\n(a) If Xi is the multiplicative factor during the ith year, then\nu\nE[Xi] = 2 · .4 + .5 · .6 = 1.1 and E\nXi = 1.1100 ∼ 13780.6.\n(b) We need to get at least 50 \"up\" steps. Mean number is 40, variance is\n√\n100(.6 · .4) = 24. So, very roughly, the chance is 1 - Φ(10/ 24) ∼ .96.\nDespite high expectation, investment usually loses money.\nR\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 2 Solutions, Fall 2011",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/00f0f71bbb6467a037f653de8115944d_MIT18_440S14_mid2_f2011.pdf",
      "content": "18.440 Midterm 2 Solutions, Fall 2011: 50 minutes, 100 points\n1. (20 points) Suppose that a fair die is rolled 72000 times. Each roll turns\nup a uniformly random member of the set {1, 2, 3, 4, 5, 6} and the rolls are\nindependent of each other. For each j ∈{1, 2, 3, 4, 5, 6} let Xj be the\nnumber of times that the die comes up j.\n(a) Compute E[X3] and Var[X3]. ANSWER: Take n = 72000, p = 1/6.\nThen E[X3] = np = 12000 and Var[X3] = np(1 - p) = 10000.\n(b) Compute Var[X1 + X2]. ANSWER: This counts the number of\ntimes that either a one or a two comes up. Each die roll has a 1/3\nchance of being a 1 or 2. So Var[X1 + X2] = n(1/3)(2/3) = 16000.\n(c) Use a normal random variable approximation to estimate the\nprobability that X3 > 12100. You may use the function\na\n√1\n-x\nΦ(a) =\n-inf e\n2/2dx in your answer. ANSWER: 12100 is one\n2π\nstandard deviation above the mean. Approximate probability is\n1 - Φ(1).\n2. (20 points) Suppose that a fair die is rolled just once. Let Y be 1 if the\ndie comes up 3 and zero otherwise. Let Z be 1 if the die comes up 2 and\nzero otherwise.\n(a) Compute the covariance Cov(Y, Z) and the variances Var(Y ) and\nVar(Z). ANSWER:\nCov(Y, Z) = E[Y Z] - E[Y ]E[Z] = 0 - 1/36 = -1/36 and\nVar(Y ) = Var(Z) = (1/6)(5/6) = 5/36.\n(b) Compute the covariance of 3Y + Z and Y - 3Z. ANSWER:\nCov(3Y + Z, Y - 3Z) = 3Var(Y ) - 8Cov(Y, Z) - 3Var(Z) =\n-8Cov(Y, Z) = 2/9.\n(c) What is the conditional expectation of Y given that Z = 0?\nANSWER: 1/5.\n3. (20 points) At a certain track competition, ten athletes take turns\nthrowing javelins. Let Xi be the distance that the ith athlete throws the\njavelin. Suppose that each Xi is an exponential random variable with an\nexpectation of 50 meters and that the Xi are independent of each other.\n(a) What is the probability density function for X1? What is the\nparameter λ of this exponential random variable? ANSWER:\nλ = 1/50 and f(x) = λe-λx if x > 0, and 0 otherwise.\n\n(b) Compute the probability that the first athlete throws the javelin\n- 50λ\n- 1\nmore than 50 meters. ANSWER: e\n= e\n.\n(c) Compute the probability that at least one athlete throws the javelin\n- 1)10\nmore than 50 meters. ANSWER: 1 - (1 - e\n.\n(d) Compute E[min{X1, X2, . . . , X10}], i.e., the expectation of the\ndistance that the last place athlete throws the javelin. ANSWER:\nMinimum of ten independent exponentials of rate λ = 1/50 is\nexponential of rate 10λ = 1/5. Expectation is 1/(10λ) = 5 meters.\n4. (20 points) Let X be a uniformly random variable on [0, 5].\n(a) Write the probability density function fX and the cumulative\ne\n1/5\n0 ≤ x ≤ 5\ndistribution function FX . ANSWER: fX (x) =\n.\notherwise\ne5t- 1\n(b) What is the moment generating function MX (t)? ANSWER:\n5t .\n(c) Suppose that Y is a random variable for which MY (0) = 1 and\nM (0) = 1 and M (0) = 2. What are E[Y ], E[Y 2] are Var[Y ]?\nY\nY\nANSWER: E[Y ] = 1, E[Y 2] = 2, and Var[Y ] = 2 - 12 = 1.\n5. (20 points) Suppose that on a certain road, the times at which red cars\ngo by a given spot are given by a Poisson point process with rate\nλ = 2/hour. Suppose that the times at which green cars go by are also\ngiven by a Poisson point process of rate λ = 2/hour. Similarly, the times\nat which blue cars go by are given by a Poisson point process of rate\nλ = 2/hour. Suppose that these three Poisson point processes are\nindependent of each other.\n(a) Write down the probability density function for the amount of time\nuntil the first red car goes by. ANSWER: Write λ = 2. Answer is\n- 2x\nλe- λx = 2e\nif x > 0, and 0 otherwise.\n(c) Compute the expected amount of time until the first car of any of\nthe three colors goes by. ANSWER: 1/6 hour, or 10 minutes.\n(c) Compute the probability that exactly three red cars go by during the\nfirst hour. ANSWER: e- λλ3/3! = 4/(3e2).\n(d) Compute the expected amount of time until at least one car of each\nof the three colors has gone by. (Hint: does this remind you of the\nradioactive decay problem?) ANSWER: (1 +\n+\n) = 11 hours, or\n55 minutes.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "18.440 Midterm 2 Solutions, Fall 2012",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/e7b7762f0965a48e0077597687e106c6_MIT18_440S14_mid2_2012_sol.pdf",
      "content": "18.440 Midterm 2, Fall 2012: 50 minutes, 100 points\n1. (10 points) Suppose that a fair die is rolled 18000 times. Each roll turns\nup a uniformly random member of the set {1, 2, 3, 4, 5, 6} and the rolls are\nindependent of each other. Let X be the total number of times the die\ncomes up 1.\n(a) Compute Var(X). ANSWER: npq = 18000(5/6)(1/6) = 2500\n(b) Use a normal random variable approximation to estimate the\nprobability that X < 2900. You may use the function\na\n√1\n-x\nΦ(a) =\ne\n2/2dx in your answer. ANSWER: Standard\n-inf\n2π √\nderivation is\n2500 = 50. Probability X more than 2 standard\ndeviations below mean is approximately Φ(-2).\n2. (20 points) Let X1, X2, and X3 be independent uniform random\nvariables on [0, 1]. Write Y = X1 + X2 and Z = X2 + X3.\n(a) Compute E[X1X2X3]. ANSWER: Independence implies\nE[X1X2X3] = E[X1]E[X2]E[X3] = (1/2)3 = 1/8.\n(b) Compute Var(X1). ANSWER: E(X1\n2) =\nx2dx = 1/3, so\nVar(X1) = E(X1\n2) - E(X1)2 = 1/3 - 1/4 = 1/12.\n(c) Compute the covariance Cov(Y, Z) and the correlation coefficient\nρ(Y, Z). ANSWER: By bilinearity of covariance,\nCov(Y, Z) = Cov(X1 + X2, X2 + X3)\n= Cov(X1, X2) + Cov(X1, X3) + Cov(X2, X2) + Cov(X2, X3).\nAll terms are zero by independence except\nCov(X2, X2) = Var(X2) = 1/12. Then ρ(Y, Z) = √\n1/12\n= 1/2.\n(2/12)(2/12)\n(d) Compute and draw a graph of the density function fY . ANSWER:\ninf\nfY (a) =\n-inf fX (a - y)fX (y)dy where\n\n1 x ∈ (0, 1)\nfX (x) =\n.\n0 x ∈ (0, 1)\nThen\n\n1 a - y ∈ (0, 1), y ∈ (0, 1)\nfX (a - y)fX (y) =\n.\n0 otherwise\n\nNow a - y ∈ (0, 1) is equivalent to -y ∈ (-a, 1 - a) or equivalently\ny ∈ (a - 1, a). Thus fY (a) is equal to the length of the intersection of\nthe intervals (0, 1) and (a - 1, a). This becomes\nfY (a) =\n⎧\n⎪\n⎪\n⎨\n⎪\n⎪\n⎩\na < 0\na\n0 ≤ a < 1\n2 - a\n1 ≤ a < 2\n0a ≥ 2\na\nfY (a)\n3. (20 points) Suppose that X1, X2, . . . , Xn are independent uniform\nrandom variables on [0, 1].\n(a) Write Y = min{X1, X2, . . . , Xn}. Compute the cumulative\ndistribution function FY (a) and the density function fY (a) for\na ∈ [0, 1]. ANSWER: By independence, P (min{X1, X2, . . . , Xn} >\na) = P (X1 > a)P (X2 > a) · · · P (Xn > a) = (1 - a)n So\nFY (a) = 1 - (1 - a)n, and fY (a) = F 0 (a) = n(1 - a)n-1\nY\n\n(b) Compute P (X1 < .3) and P max{X1, X2, . . . , Xn} < .3.\nANSWER: .3 and .3n .\n(c) Compute the expectation E[X1 + X2 + . . . + Xn]. ANSWER: By\nadditivity of expectation, this is nE[X1] = n/2.\n4. (20 points) Aspiring writer Rachel decides to lock herself in her room to\nthink of screenplay ideas. When Rachel is thinking, the moments at which\ngood new ideas occur to her form a Poisson process with parameter\nλG = .5/hour. The times when bad new ideas occur to her are a Poisson\npoint process with parameter λB = 1.5 per hour.\n(a) Let T be the amount of time until Rachel has her first idea (good or\nbad). Write down the probability density function for T .\n\nANSWER: T is exponential with parameter λ = λG + λB = 2, so\nfT (x) = 2e-2x .\n(b) Compute the probability that Rachel has exactly 3 bad ideas total\nduring her first hour of thinking. ANSWER: Number N of bad\n-1.5\n(1.5)3e\nideas is Poisson with rate 1 · λB = 1.5. So P (N = 3) =\n3!\n.\n(c) Let S be the amount of time elapsed before the fifth good idea\noccurs. Compute Var(S). ANSWER: Variance of time till one good\nidea is 1/λ2\nMemoryless property and additivity of variance of\nG.\nindependent sums gives Var(S) = 5/λ2 = 20.\nG\n(d) What is the probability that Rachel has no ideas at all during her\nfirst three hours of thinking? ANSWER: Time till first idea is\nexponential with λ = 2. Probability this time exceeds 3 is\n-2·3\n-6\ne\n= e\n.\n5. (20 points) Suppose that X and Y have a joint density function f given\nby\n1/π x2 + y2 < 1\nf(x, y) =\n.\n2 ≥ 1\nx + y\n(a) Compute the probability density function fX for X. ANSWER:\ninf\n1 √\n1 - x\n-1 ≤ x ≤-1\nfX (x) =\nf(x, y)dy =\nπ\n-inf\notherwise\n(b) Compute the conditional expectation E[X|Y = .5]. ANSWER:\nProbability density for X given Y = .5 is uniform on\n√\n√\n(- 1 - .52 , 1 - .52). So E[X|Y = .5] = 0.\n(c) Express E[X3Y 3] as a double integral. (You don't have to explicitly\nevaluate the integral.) ANSWER:\n1 √\n1-x\n1 3\nx y 3dydx.\n√\n2 π\n-1\n- 1-x\n6. (10 points) Let X and Y be independent normal random variables, each\nwith mean 1 and variance 9.\n(\n(\n\n(a) Let f be the joint probability density function for the pair (X, Y ).\nWrite an explicit formula for f. ANSWER:\n(x-1)2-(y-1)2\n-(x-1)2/18\n-(y-1)2/18\n-\nf(x, y) = √\ne\n√\ne\n=\ne\n.\n2π\n2π\n18π\n(b) Compute E[X2] and E[X2Y 2]. ANSWER:\nVar(X) = E[X2] - E[X]2 = E[X2] - 1 = 9, so E[X2] = 10. By\nindependence E[X2Y 2] = E[X2]E[Y 2] = 100.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/1e65bc1b283de747eac7a2e0ca9bbfe7_MIT18_440S14_Lecture1.pdf",
      "content": "18.440: Lecture 1\nPermutations and combinations, Pascal's\ntriangle, learning to count\nScott Sheffield\nMIT\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nPolitics\nI Suppose that betting markets place the probability that your\nfavorite presidential candidates will be elected at 58 percent.\nPrice of a contact that pays 100 dollars if your candidate wins\nis 58 dollars.\nI Market seems to say that your candidate will probably win, if\n\"probably\" means with probability greater than .5.\nI The price of such a contract may fluctuate in time.\nI Let X (t) denote the price at time t.\nI Suppose X (t) is known to vary continuously in time. What is\nthe probability it will reach 59 before reaching 57?\nI \"Efficient market hypothesis\" suggests about .5.\nI Reasonable model: use sequence of fair coin tosses to decide\nthe order in which X (t) passes through different integers.\n18.440 Lecture 1\n\nI\nI\nI\nI\nI\nI\nI\nI\nI\nWhich of these statements is \"probably\" true?\n1. X (t) will go below 50 at some future point.\n2. X (t) will get all the way below 20 at some point\n3. X (t) will reach both 70 and 30, at different future times.\n4. X (t) will reach both 65 and 35 at different future times.\n5. X (t) will hit 65, then 50, then 60, then 55.\nAnswers: 1, 2, 4.\nFull explanations coming at the end of the course.\nPoint for now is that probability is everywhere: politics,\nmilitary, finance and economics, all kinds of science and\nengineering, philosophy, religion, making cool new cell phone\nfeatures work, social networking, dating websites, etc.\nAll of the math in this course has a lot of applications.\n18.440 Lecture 1\nI\nI\nI\nI\nI\nI\nI\nI\nI\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nI\nI\nI\nI\nI\nI\nI\nPermutations\nHow many ways to order 52 cards?\nAnswer: 52 · 51 · 50 · . . . · 1 = 52! =\n80658175170943878571660636856403766975289505440883277824×\nn hats, n people, how many ways to assign each person a hat?\nAnswer: n!\nn hats, k < n people, how many ways to assign each person a\nhat?\nn · (n - 1) · (n - 2) . . . (n - k + 1) = n!/(n - k)!\nA permutation is a map from {1, 2, . . . , n} to {1, 2, . . . , n}.\nThere are n! permutations of n elements.\n18.440 Lecture 1\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nPermutation notation\nA permutation is a function from {1, 2, . . . , n} to\n{1, 2, . . . , n} whose range is the whole set {1, 2, . . . , n}. If σ\nis a permutation then for each j between 1 and n, the the\nvalue σ(j) is the number that j gets mapped to.\nFor example, if n = 3, then σ could be a function such that\nσ(1) = 3, σ(2) = 2, and σ(3) = 1.\nIf you have n cards with labels 1 through n and you shuffle\nthem, then you can let σ(j) denote the label of the card in the\njth position. Thus orderings of n cards are in one-to-one\ncorrespondence with permutations of n elements.\nOne way to represent σ is to list the values\nσ(1), σ(2), . . . , σ(n) in order. The σ above is represented as\n{3, 2, 1}.\nIf σ and ρ are both permutations, write σ * ρ for their\ncomposition. That is, σ * ρ(j) = σ(ρ(j)).\n18.440 Lecture 1\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nCycle decomposition\nAnother way to write a permutation is to describe its cycles:\nFor example, taking n = 7, we write (2, 3, 5), (1, 7), (4, 6) for\nthe permutation σ such that σ(2) = 3, σ(3) = 5, σ(5) = 2 and\nσ(1) = 7, σ(7) = 1, and σ(4) = 6, σ(6) = 4.\nIf you pick some j and repeatedly apply σ to it, it will \"cycle\nthrough\" the numbers in its cycle.\nGenerally, a function is called an involution if f (f (x)) = x for\nall x.\nA permutation is an involution if all cycles have length one or\ntwo.\nA permutation is \"fixed point free\" if there are no cycles of\nlength one.\n18.440 Lecture 1\n▶\n▶\n▶\n▶\n▶\n▶\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nI\nI\nI\nFundamental counting trick\nn ways to assign hat for the first person. No matter what\nchoice I make, there will remain n - 1 was to assign hat to the\nsecond person. No matter what choice I make there, there will\nremain n - 2 ways to assign a hat to the third person, etc.\nThis is a useful trick: break counting problem into a sequence\nof stages so that one always has the same number of choices\nto make at each stage. Then the total count becomes a\nproduct of number of choices available at each stage.\nEasy to make mistakes. For example, maybe in your problem,\nthe number of choices at one stage actually does depend on\nchoices made during earlier stages.\n18.440 Lecture 1\n▶\n▶\n▶\n\nI\nI\nAnother trick: overcount by a fixed factor\nIf you have 5 indistinguishable black cards, 2 indistinguishable\nred cards, and three indistinguishable green cards, how many\ndistinct shuffle patterns of the ten cards are there?\nAnswer: if the cards were distinguishable, we'd have 10!. But\nwe're overcounting by a factor of 5!2!3!, so the answer is\n10!/(5!2!3!).\n18.440 Lecture 1\n▶\n▶\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nI\nI\nI\nI\nI\nI\nI\nI\nn\nk\n\nnotation\nHow many ways to choose an ordered sequence of k elements\nfrom a list of n elements, with repeats allowed?\nk\nAnswer: n\nHow many ways to choose an ordered sequence of k elements\nfrom a list of n elements, with repeats forbidden?\nAnswer: n!/(n - k)!\nHow many way to choose (unordered) k elements from a list\nof n without repeats?\n\nn\nn!\nAnswer:\n:=\nk\nk!(n-k)!\nWhat is the coefficient in front of xk in the expansion of\n(x + 1)n?\nn\nAnswer:\n.\nk\n18.440 Lecture 1\nn\nnotation\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\n\nI\nI\n\nI\n\nI\n\nI\nPascal's triangle\nArnold principle.\nn\nn-1\nn-1\nA simple recursion:\n=\n+\n.\nk\nk-1\nk\nWhat is the coefficient in front of xk in the expansion of\n(x + 1)n?\nn\nAnswer:\n.\nk\nn-1 +\nn\nn\nn\nn\nn\nn\n(x + 1)n =\n· 1 +\nx1 +\nx2 + . . . +\nx\nx .\nn-1\nn\nn\nn\nQuestion: what is\nk=0\n?\nk\nAnswer: (1 + 1)n = 2n .\n18.440 Lecture 1\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nPascal's triangle\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nOutline\nRemark, just for fun\nPermutations\nCounting tricks\nBinomial coefficients\nProblems\n18.440 Lecture 1\n\nI\nI\n\nI\nI\n\nI\nI\nMore problems\nHow many full house hands in poker?\n13 4\n3 · 12 4\nHow many \"2 pair\" hands?\n13 4\n2 · 12 4\n2 · 11 4\n1 /2\nHow many royal flush hands?\n18.440 Lecture 1\nMore problems\n▶\n▶\n\n▶\n▶\n\n▶\n▶\n\nI\nI\n\nI\nI\nI\nI\nI\nI\nMore problems\nHow many hands that have four cards of the same suit, one\ncard of another suit?\n4 13 · 3 13\nHow many 10 digit numbers with no consecutive digits that\nagree?\nIf initial digit can be zero, have 10 · 99 ten-digit sequences. If\ninitial digit required to be non-zero, have 910 .\nHow many 10 digit numbers (allowing initial digit to be zero)\nin which only 5 of the 10 possible digits are represented?\nThis is one is tricky, can be solved with inclusion-exclusion (to\ncome later in the course).\nHow many ways to assign a birthday to each of 23 distinct\npeople? What if no birthday can be repeated?\n36623 if repeats allowed. 366!/343! if repeats not allowed.\n18.440 Lecture 1\n▶\n▶\n\n▶\n▶\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/d65bdc5634ead31fec4185c1bdf1390c_MIT18_440S14_Lecture2.pdf",
      "content": "18.440: Lecture 2\nMultinomial coefficients and more counting\nproblems\nScott Sheffield\nMIT\n18.440 Lecture 2\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nPartition problems\nI You have eight distinct pieces of food. You want to choose\nthree for breakfast, two for lunch, and three for dinner. How\nmany ways to do that?\nI Answer: 8!/(3!2!3!)\nI One way to think of this: given any permutation of eight\nelements (e.g., 12435876 or 87625431) declare first three as\nbreakfast, second two as lunch, last three as dinner. This\nmaps set of 8! permutations on to the set of food-meal\ndivisions in a many-to-one way: each food-meal division\ncomes from 3!2!3! permutations.\nI How many 8-letter sequences with 3 A's, 2 B's, and 3 C 's?\nI Answer: 8!/(3!2!3!). Same as other problem. Imagine 8\n\"slots\" for the letters. Choose 3 to be A's, 2 to be B's, and 3\nto be C 's.\n18.440 Lecture 2\n\nI\nI\nPartition problems\nIn general, if you have n elements you wish to divide into r\ndistinct piles of sizes n1, n2 . . . nr , how many ways to do that?\n\nn\nn!\nAnswer\n:=\nn1,n2,...,nr\nn1!n2!...nr ! .\n18.440 Lecture 2\n▶\n▶\n\nI\nI\nI\nI\nI\n\nOne way to understand the binomial theorem\nExpand the product (A1 + B1)(A2 + B2)(A3 + B3)(A4 + B4).\n16 terms correspond to 16 length-4 sequences of A's and B's.\nA1A2A3A4 + A1A2A3B4 + A1A2B3A4 + A1A2B3B4+\nA1B2A3A4 + A1B2A3B4 + A1B2B3A4 + A1B2B3B4+\nB1A2A3A4 + B1A2A3B4 + B1A2B3A4 + B1A2B3B4+\nB1B2A3A4 + B1B2A3B4 + B1B2B3A4 + B1B2B3B4\nWhat happens to this sum if we erase subscripts?\n(A + B)4 = B4 + 4AB3 + 6A2B2 + 4A3B + A4 . Coefficient of\nA2B2 is 6 because 6 length-4 sequences have 2 A's and 2 B's.\nn\nn\nGenerally, (A + B)n =\nAk Bn-k , because there are\nk=0 k\nn sequences with k A's and (n - k) B's.\nk\n18.440 Lecture 2\n▶\n▶\n▶\n▶\n▶\n\na\n\nI\nI\nI\nI\nI\nHow about trinomials?\nExpand\n(A1 + B1 + C1)(A2 + B2 + C2)(A3 + B3 + C3)(A4 + B4 + C4).\nHow many terms?\nAnswer: 81, one for each length-4 sequence of A's and B's\nand C 's.\nWe can also compute (A + B + C )4 =\nA4 +4A3B +6A2B2 +4AB3 +B4+4A3C +12A2BC +12AB2C +\n4B3C + 6A2C 2 + 12ABC 2 + 6B2C 2 + 4AC 3 + 4BC 3 + C 4\nWhat is the sum of the coefficients in this expansion? What is\nthe combinatorial interpretation of coefficient of, say, ABC 2?\nAnswer 81 = (1 + 1 + 1)4 . ABC 2 has coefficient 12 because\nthere are 12 length-4 words have one A, one B, two C 's.\n18.440 Lecture 2\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\n\nMultinomial coefficients\nIs there a higher dimensional analog of binomial theorem?\nAnswer: yes.\nThen what is it?\nX\nn\nn1 n2\nnr\n(x1+x2+. . .+xr )n =\nx\nx . . . x\nr\nn1, . . . , nr\nn1,...,nr :n1+...+nr =n\nThe sum on the right is taken over all collections\n(n1, n2, . . . , nr ) of r non-negative integers that add up to n.\nPascal's triangle gives coefficients in binomial expansions. Is\nthere something like a \"Pascal's pyramid\" for trinomial\nexpansions?\n18.440 Lecture 2\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nI\nBy the way...\nIf n! is the product of all integers in the interval with\nendpoints 1 and n, then 0! = 0.\nActually, we say 0! = 1.\nBecause there is one map from the empty set to itself.\n3! of these:\n{1, 2, 3}, {1, 3, 2}, {2, 1, 3}, {2, 3, 1}, {3, 1, 2}, {3, 2, 1}, 2! of\nthese: {1, 2}, {2, 1}, 1! of these: {1} and 0! of these {}.\nBecause this is the convention that makes the binomial and\nmultinomial theorems true.\nBecause we want the recursion n(n - 1)! = n! to hold for\nn = 1. (We won't define factorials of negative integers.)\ninf tz-1\nBecause we can write Γ(z) :=\ne-t dt and define\ninf\nn! := Γ(n + 1) =\ntne-t dt.\nBecause there is a consensus among MIT faculty that 0!\nshould be 1.\n18.440 Lecture 2\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nI\nI\n\nInteger partitions\nHow many sequences a1, . . . , ak of non-negative integers\nsatisfy a1 + a2 + . . . + ak = n?\nn+k-1\nAnswer:\n. Represent partition by k - 1 bars and n\nn\nstars, e.g., as ∗ ∗ | ∗ ∗|| ∗ ∗ ∗ ∗|∗.\n18.440 Lecture 2\n▶\n▶\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nOutline\nMultinomial coefficients\nInteger partitions\nMore problems\n18.440 Lecture 2\n\nI\nI\nI\nI\n\nI\nI\n\nMore counting problems\nIn 18.821, a class of 27 students needs to be divided into 9\nteams of three students each? How many ways are there to\ndo that?\n27!\n(3!)99!\nYou teach a class with 90 students. In a rather severe effort to\ncombat grade inflation, your department chair insists that you\nassign the students exactly 10 A's, 20 B's, 30 C's, 20 D's, and\n10 F's. How many ways to do this?\n90!\n=\n10,20,30,20,10\n10!20!30!20!10!\nYou have 90 (indistinguishable) pieces of pizza to divide\namong the 90 (distinguishable) students. How many ways to\ndo that (giving each student a non-negative integer number of\nslices)?\n=\n18.440 Lecture 2\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\n\nI\nI\nI\nI\n\nMore counting problems\nHow many 13-card bridge hands have 4 of one suit, 3 of one\nsuit, 5 of one suit, 1 of one suit?\n4! 13\nHow many bridge hands have at most two suits represented?\n26 - 8\nHow many hands have either 3 or 4 cards in each suit?\nNeed three 3-card suits, one 4-card suit, to make 13 cards\n3 13\ntotal. Answer is 4 13\n18.440 Lecture 2\n▶\n▶\n▶\n▶▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/8c5daeab46031beef7f01c3bfef48f7e_MIT18_440S14_Lecture3.pdf",
      "content": "Formalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440: Lecture 3\nSample spaces, events, probability\nScott Sheffield\nMIT\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nWhat does \"I'd say there's a thirty percent chance it will\nrain tomorrow\" mean?\nI Neurological: When I think \"it will rain tomorrow\" the\n\"truth-sensing\" part of my brain exhibits 30 percent of its\nmaximum electrical activity.\nI Frequentist: Of the last 1000 days that meteorological\nmeasurements looked this way, rain occurred on the\nsubsequent day 300 times.\nI Market preference (\"risk neutral probability\"): The\nmarket price of a contract that pays 100 if it rains tomorrow\nagrees with the price of a contract that pays 30 tomorrow no\nmatter what.\nI Personal belief: If you offered me a choice of these contracts,\nI'd be indifferent. (What if need for money is different in two\nscenarios. Replace dollars with \"units of utility\"?)\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nEven more fundamental question: defining a set of possible\noutcomes\nRoll a die n times. Define a sample space to be\n{1, 2, 3, 4, 5, 6}n, i.e., the set of a1, . . . , an with each\naj ∈{1, 2, 3, 4, 5, 6}.\nShuffle a standard deck of cards. Sample space is the set of\n52! permutations.\nWill it rain tomorrow? Sample space is {R, N}, which stand\nfor \"rain\" and \"no rain.\"\nRandomly throw a dart at a board. Sample space is the set of\npoints on the board.\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nEvent: subset of the sample space\nIf a set A is comprised of some (but not all) of the elements\nof B, say A is a subset of B and write A ⊂ B.\nSimilarly, B ⊃ A means A is a subset of B (or B is a superset\nof A).\nIf S is a finite sample space with n elements, then there are 2n\nsubsets of S.\nDenote by ∅ the set with no elements.\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nIntersections, unions, complements\nA ∪ B means the union of A and B, the set of elements\ncontained in at least one of A and B.\nA ∩ B means the intersection of A and B, the set of elements\ncontained on both A and B.\nAc means complement of A, set of points in whole sample\nspace S but not in A.\nA \\ B means \"A minus B\" which means the set of points in A\nbut not in B. In symbols, A \\ B = A ∩ (Bc ).\n∪ is associative. So (A ∪ B) ∪ C = A ∪ (B ∪ C ) and can be\nwritten A ∪ B ∪ C .\n∩ is also associative. So (A ∩ B) ∩ C = A ∩ (B ∩ C ) and can\nbe written A ∩ B ∩ C .\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n▶\n▶\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nVenn diagrams\nA\nB\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nVenn diagrams\nA\nB\nAc ∩Bc\nAc ∩B\nA ∩B\nA ∩Bc\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nI\nI\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nDeMorgan's laws\n\"It will not snow or rain\" means \"It will not snow and it will\nnot rain.\"\nIf S is event that it snows, R is event that it rains, then\n(S ∪ R)c = Sc ∩ Rc\nMore generally: (∪n Ei )c = ∩n (Ei )c\ni=1\ni=1\n\"It will not both snow and rain\" means \"Either it will not\nsnow or it will not rain.\"\n(S ∩ R)c = Sc ∪ Rc\n(∩n Ei )c = ∪n (Ei )c\ni=1\ni=1\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n▶\n▶\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nOutline\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\n18.440 Lecture 3\n\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nAxioms of probability\nP(A) ∈ [0, 1] for all A ⊂ S.\nP(S) = 1.\nFinite additivity: P(A ∪ B) = P(A) + P(B) if A ∩ B = ∅.\ninf\nCountable additivity: P(∪inf Ei ) =\nP(Ei ) if Ei ∩ Ej = ∅\ni=1\ni=1\nfor each pair i and j.\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nFormalizing probability\nSample space\nDeMorgan's laws\nAxioms of probability\nNeurological: When I think \"it will rain tomorrow\" the\n\"truth-sensing\" part of my brain exhibits 30 percent of its\nmaximum electrical activity. Should have P(A) ∈ [0, 1] and\nP(S) = 1 but not necessarily P(A ∪ B) = P(A) + P(B) when\nA ∩ B = ∅.\nFrequentist: P(A) is the fraction of times A occurred during\nthe previous (large number of) times we ran the experiment.\nSeems to satisfy axioms...\nMarket preference (\"risk neutral probability\"): P(A) is\nprice of contract paying dollar if A occurs divided by price of\ncontract paying dollar regardless. Seems to satisfy axioms,\nassuming no arbitrage, no bid-ask spread, complete market...\nPersonal belief: P(A) is amount such that I'd be indifferent\nbetween contract paying 1 if A occurs and contract paying\nP(A) no matter what. Seems to satisfy axioms with some\nnotion of utility units, strong assumption of \"rationality\"...\n18.440 Lecture 3\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/b57aff9567d7b564967adcdb69c3d4eb_MIT18_440S14_Lecture4.pdf",
      "content": "18.440: Lecture 4\nAxioms of probability and\ninclusion-exclusion\nScott Sheffield\nMIT\n18.440 Lecture 4\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nAxioms of probability\nI P(A) ∈ [0, 1] for all A ⊂ S.\nI P(S) = 1.\nI Finite additivity: P(A ∪ B) = P(A) + P(B) if A ∩ B = ∅.\ninf\nI Countable additivity: P(∪inf Ei ) =\nP(Ei ) if Ei ∩ Ej = ∅\ni=1\ni=1\nfor each pair i and j.\n18.440 Lecture 4\n\nI\nI\nI\nI\nNeurological: When I think \"it will rain tomorrow\" the\n\"truth-sensing\" part of my brain exhibits 30 percent of its\nmaximum electrical activity.\nFrequentist: P(A) is the fraction of times A occurred during\nthe previous (large number of) times we ran the experiment.\nMarket preference (\"risk neutral probability\"): P(A) is\nprice of contract paying dollar if A occurs divided by price of\ncontract paying dollar regardless.\nPersonal belief: P(A) is amount such that I'd be indifferent\nbetween contract paying 1 if A occurs and contract paying\nP(A) no matter what.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nAxiom breakdown\nWhat if personal belief function doesn't satisfy axioms?\nConsider an A-contract (pays 10 if candidate A wins election)\na B-contract (pays 10 dollars if candidate B wins) and an\nA-or-B contract (pays 10 if either A or B wins).\nFriend: \"I'd say A-contract is worth 1 dollar, B-contract is\nworth 1 dollar, A-or-B contract is worth 7 dollars.\"\nAmateur response: \"Dude, that is, like, so messed up.\nHaven't you heard of the axioms of probability?\"\nProfessional response: \"I fully understand and respect your\nopinions. In fact, let's do some business. You sell me an A\ncontract and a B contract for 1.50 each, and I sell you an\nA-or-B contract for 6.50.\"\nFriend: \"Wow... you've beat by suggested price by 50 cents\non each deal. Yes, sure! You're a great friend!\"\nAxioms breakdowns are money-making opportunities.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nNeurological: When I think \"it will rain tomorrow\" the\n\"truth-sensing\" part of my brain exhibits 30 percent of its\nmaximum electrical activity. Should have P(A) ∈ [0, 1],\nmaybe P(S) = 1, not necessarily P(A ∪ B) = P(A) + P(B)\nwhen A ∩ B = ∅.\nFrequentist: P(A) is the fraction of times A occurred during\nthe previous (large number of) times we ran the experiment.\nSeems to satisfy axioms...\nMarket preference (\"risk neutral probability\"): P(A) is\nprice of contract paying dollar if A occurs divided by price of\ncontract paying dollar regardless. Seems to satisfy axioms,\nassuming no arbitrage, no bid-ask spread, complete market...\nPersonal belief: P(A) is amount such that I'd be indifferent\nbetween contract paying 1 if A occurs and contract paying\nP(A) no matter what. Seems to satisfy axioms with some\nnotion of utility units, strong assumption of \"rationality\"...\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nI\nIntersection notation\nWe will sometimes write AB to denote the event A ∩ B.\n18.440 Lecture 4\n▶\n\nI\nI\nI\nI\nI\nI\nConsequences of axioms\nCan we show from the axioms that P(Ac ) = 1 - P(A)?\nCan we show from the axioms that if A ⊂ B then\nP(A) ≤ P(B)?\nCan we show from the axioms that\nP(A ∪ B) = P(A) + P(B) - P(AB)?\nCan we show from the axioms that P(AB) ≤ P(A)?\nCan we show from the axioms that if S contains finitely many\nelements x1, . . . , xk , then the values\ne\nl\nP({x1}), P({x2}), . . . , P({xk }) determine the value of P(A)\nfor any A ⊂ S?\nWhat k-tuples of values are consistent with the axioms?\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nFamous 1982 Tversky-Kahneman study (see wikipedia)\nPeople are told \"Linda is 31 years old, single, outspoken, and\nvery bright. She majored in philosophy. As a student, she was\ndeeply concerned with issues of discrimination and social\njustice, and also participated in anti-nuclear demonstrations.\"\nThey are asked: Which is more probable?\nI Linda is a bank teller.\nI Linda is a bank teller and is active in the feminist movement.\n85 percent chose the second option.\nCould be correct using neurological/emotional definition. Or a\n\"which story would you believe\" interpretation (if witnesses\noffering more details are considered more credible).\nBut axioms of probability imply that second option cannot be\nmore likely than first.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nOutline\nAxioms of probability\nConsequences of axioms\nInclusion exclusion\n18.440 Lecture 4\n\nI\nI\nI\nI\nI\nInclusion-exclusion identity\nImagine we have n events, E1, E2, . . . , En.\nHow do we go about computing something like\nP(E1 ∪ E2 ∪ . . . ∪ En)?\nIt may be quite difficult, depending on the application.\nThere are some situations in which computing\nP(E1 ∪ E2 ∪ . . . ∪ En) is a priori difficult, but it is relatively\neasy to compute probabilities of intersections of any collection\nof Ei . That is, we can easily compute quantities like\nP(E1E3E7) or P(E2E3E6E7E8).\nIn these situations, the inclusion-exclusion rule helps us\ncompute unions. It gives us a way to express\nP(E1 ∪ E2 ∪ . . . ∪ En) in terms of these intersection\nprobabilities.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\n\nInclusion-exclusion identity\nCan we show from the axioms that\nP(A ∪ B) = P(A) + P(B) - P(AB)?\nHow about P(E ∪ F ∪ G ) =\nP(E ) + P(F ) + P(G ) - P(EF ) - P(EG ) - P(FG) + P(EFG )?\nMore generally,\nn\n\nP(∪n\ni=1Ei ) =\nP(Ei ) -\nP(Ei1 Ei2 ) + . . .\ni=1\ni1<i2\n\n+ (-1)(r+1)\nP(Ei1 Ei2 . . . Eir )\ni1<i2<...<ir\n+ . . . + (-1)n+1P(E1E2 . . . En).\ne l\nn\nThe notation\nmeans a sum over all of the\ni1<i2<...<ir\nr\nsubsets of size r of the set {1, 2, . . . , n}.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\nP\n\nI\nI\nI\nI\nI\nI\nInclusion-exclusion proof idea\nConsider a region of the Venn diagram contained in exactly\nm > 0 subsets. For example, if m = 3 and n = 8 we could\n3 E4 E5E6 E7 E\nc\nc\nc\nc\nc\nconsider the region E1E2E\n8 .\nThis region is contained in three single intersections (E1, E2,\nand E5). It's contained in 3 double-intersections (E1E2, E1E5,\nand E2E5). It's contained in only 1 triple-intersection\n(E1E2E5).\ne l\ne l\ne l\ne l\nm\nm\nm + . . . ± m\nm times in the\nIt is counted\n-\n+\ninclusion exclusion sum.\nHow many is that?\nAnswer: 1. (Follows from binomial expansion of (1 - 1)m.)\nThus each region in E1 ∪ . . . ∪ En is counted exactly once in\nthe inclusion exclusion sum, which implies the identity.\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nI\nFamous hat problem\nn people toss hats into a bin, randomly shuffle, return one hat\nto each person. Find probability nobody gets own hat.\nInclusion-exclusion. Let Ei be the event that ith person gets\nown hat.\nWhat is P(Ei1 Ei2 . . . Eir )?\n(n-r)!\nAnswer:\n.\nn!\ne l\nn\nThere are\nterms like that in the inclusion exclusion sum.\nr\ne l\nn (n-r )!\nWhat is\n?\nr\nn!\nAnswer: r! .\n1 - 1\nP(∪n Ei ) = 1 - 1 +\n+ . . . ±\ni=1\n2!\n3!\n4!\nn!\n- 1\n1 - P(∪n Ei ) = 1 - 1+ 1\n+\n- . . . ± 1 ≈ 1/e ≈ .36788\ni=1\n2!\n3!\n4!\nn!\n18.440 Lecture 4\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/6c2c7390c2166f6f1941423e282255ac_MIT18_440S14_Lecture5.pdf",
      "content": "18.440: Lecture 5\nProblems with all outcomes equally like,\nincluding a famous hat problem\nScott Sheffield\nMIT\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nEqual likelihood\nI If a sample space S has n elements, and all of them are\nequally likely, the each one has to have probability 1/n\nI What is P(A) for a general set A ⊂ S?\nI Answer: |A|/|S|, where |A| is the number of elements in A.\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nI\nI\nI\nI\nI\nProblems\nRoll two dice. What is the probability that their sum is three?\nToss eight coins. What is the probability that exactly five of\nthem are heads?\nIn a class of 100 people with cell phone numbers, what is the\nprobability that nobody has a number ending in 37?\nRoll ten dice. What is the probability that a 6 appears on\nexactly five of the dice?\nIn a room of 23 people, what is the probability that two of\nthem have a birthday in common?\n18.440 Lecture 5\n▶\n▶\n▶\n▶\n▶\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nI\nI\nRecall the inclusion-exclusion identity\nn\nn\nn\nP(∪n\ni=1Ei ) =\nP(Ei ) -\nP(Ei1 Ei2 ) + . . .\ni=1\ni1<i2\nn\n+ (-1)(r+1)\nP(Ei1 Ei2 . . . Eir )\ni1<i2<...<ir\n= + . . . + (-1)n+1P(E1E2 . . . En).\n\nn\nThe notation\nmeans a sum over all of the\ni1<i2<ir\nr\nsubsets of size r of the set {1, 2, . . . , n}.\n18.440 Lecture 5\n▶\n▶\n\nI\nI\nI\nI\nI\n\nI\nI\nI\nFamous hat problem\nn people toss hats into a bin, randomly shuffle, return one hat\nto each person. Find probability nobody gets own hat.\nInclusion-exclusion. Let Ei be the event that ith person gets\nown hat.\nWhat is P(Ei1 Ei2 . . . Eir )?\n(n-r)!\nAnswer:\n.\nn!\nn\nThere are\nterms like that in the inclusion exclusion sum.\nr\nn (n-r )!\nWhat is\n?\nr\nn!\nAnswer: r! .\n1 - 1\nP(∪n Ei ) = 1 - 1 +\n+ . . . ±\ni=1\n2!\n3!\n4!\nn!\n- 1\n1 - P(∪n Ei ) = 1 - 1+ 1\n+\n- . . . ± 1 ≈ 1/e ≈ .36788\ni=1\n2!\n3!\n4!\nn!\n18.440 Lecture 5\n▶\n▶\n▶\n▶\n▶\n\n▶\n▶\n▶\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nOutline\nEqual likelihood\nA few problems\nHat problem\nA few more problems\n18.440 Lecture 5\n\nI\nI\nI\nI\nI\n\nI\nI\nProblems\nWhat's the probability of a full house in poker (i.e., in a five\ncard hand, 2 have one value and three have another)?\nAnswer 1:\n# ordered distinct-five-card sequences giving full house\n# ordered distinct-five-card sequences\nThat's\n2 ∗13∗12∗(4∗3∗2)∗(4∗3)/(52∗51∗50∗49∗48) = 6/4165.\nAnswer 2:\n# unordered distinct-five-card sets giving full house\n# unordered distinct-five-card sets\n/ 52\nWhat is the probability of a two-pair hand in poker?\nWhat is the probability of a bridge hand with 3 of one suit, 3\nof one suit, 2 of one suit, 5 of another suit?\n18.440 Lecture 5\nThat's 13 ∗ 12 ∗ 4\n3 ∗ 4\n= 6/4165.\n▶\n▶\n▶\n▶\n▶\n\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/6e27fc8d402dbc59ab102a1ed11ec90d_MIT18_440S14_Lecture6.pdf",
      "content": "18.440: Lecture 6\nConditional probability\nScott Sheffield\nMIT\n18.440 Lecture 6\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nConditional probability\nI Suppose I have a sample space S with n equally likely\nelements, representing possible outcomes of an experiment.\nI Experiment is performed, but I don't know outcome. For\nsome F ⊂ S, I ask, \"Was the outcome in F ?\" and receive\nanswer yes.\nI I think of F as a \"new sample space\" with all elements\nequally likely.\nI Definition: P(E |F ) = P(EF )/P(F ).\nI Call P(E |F ) the \"conditional probability of E given F \" or\n\"probability of E conditioned on F \".\nI Definition makes sense even without \"equally likely\"\nassumption.\n18.440 Lecture 6\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nI\nI\nI\nI\nI\nI\nI\nMore examples\nProbability have rare disease given positive result to test with\n90 percent accuracy.\nSay probability to have disease is p.\nS = {disease, no disease} × {positive, negative}.\nP(positive) = .9p + .1(1 - p) and P(disease, positive) = .9p.\n.9p\nP(disease|positive) =\n. If p is tiny, this is about 9p.\n.9p+.1(1-p)\nProbability suspect guilty of murder given a particular\nsuspicious behavior.\nProbability plane will come eventually, given plane not here\nyet.\n18.440 Lecture 6\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nAnother famous Tversky/Kahneman study (Wikipedia)\nImagine you are a member of a jury judging a hit-and-run\ndriving case. A taxi hit a pedestrian one night and fled the\nscene. The entire case against the taxi company rests on the\nevidence of one witness, an elderly man who saw the accident\nfrom his window some distance away. He says that he saw the\npedestrian struck by a blue taxi. In trying to establish her\ncase, the lawyer for the injured pedestrian establishes the\nfollowing facts:\nI There are only two taxi companies in town, \"Blue Cabs\" and\n\"Green Cabs.\" On the night in question, 85 percent of all taxis\non the road were green and 15 percent were blue.\nI The witness has undergone an extensive vision test under\nconditions similar to those on the night in question, and has\ndemonstrated that he can successfully distinguish a blue taxi\nfrom a green taxi 80 percent of the time.\nStudy participants believe blue taxi at fault, say witness\ncorrect with 80 percent probability.\n18.440 Lecture 6\n▶\n▶\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nOutline\nDefinition: probability of A given B\nExamples\nMultiplication rule\n18.440 Lecture 6\n\nI\nI\nI\nI\nI\nMultiplication rule\nP(E1E2E3 . . . En) =\nP(E1)P(E2|E1)P(E3|E1E2) . . . P(En|E1 . . . En-1)\nUseful when we think about multi-step experiments.\nFor example, let Ei be event ith person gets own hat in the\nn-hat shuffle problem.\nAnother example: roll die and let Ei be event that the roll\ndoes not lie in {1, 2, . . . , i}. Then P(Ei ) = (6 - i)/6 for\ni ∈{1, 2, . . . , 6}.\nWhat is P(E4|E1E2E3) in this case?\n18.440 Lecture 6\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nMonty Hall problem\nPrize behind one of three doors, all equally likely.\nYou point to door one. Host opens either door two or three\nand shows you that it doesn't have a prize. (If neither door\ntwo nor door three has a prize, host tosses coin to decide\nwhich to open.)\nYou then get to open a door and claim what's behind it.\nShould you stick with door one or choose other door?\nSample space is {1, 2, 3} × {2, 3} (door containing prize, door\nhost points to).\n\nWe have P (1, 2) = P (1, 3) = 1/6 and\n\nP (2, 3) = P (3, 2) = 1/3. Given host points to door 2,\nprobability prize behind 3 is 2/3.\n18.440 Lecture 6\n▶\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/036326e36a495d9c74607f28064af905_MIT18_440S14_Lecture7.pdf",
      "content": "18.440: Lecture 7\nBayes' formula and independence\nScott Sheffield\nMIT\n18.440 Lecture 7\n\nOutline\nBayes' formula\nIndependence\n18.440 Lecture 7\n\nOutline\nBayes' formula\nIndependence\n18.440 Lecture 7\n\nRecall definition: conditional probability\nI Definition: P(E |F ) = P(EF )/P(F ).\nI Equivalent statement: P(EF ) = P(F )P(E |F ).\nI Call P(E |F ) the \"conditional probability of E given F \" or\n\"probability of E conditioned on F \".\n18.440 Lecture 7\n\nI\nI\nI\nI\nI\nDividing probability into two cases\nP(E ) = P(EF ) + P(EF c )\n= P(E |F )P(F ) + P(E |F c )P(F c )\nIn words: want to know the probability of E . There are two\nscenarios F and F c . If I know the probabilities of the two\nscenarios and the probability of E conditioned on each\nscenario, I can work out the probability of E .\nExample: D = \"have disease\", T = \"positive test.\"\nIf P(D) = p, P(T |D) = .9, and P(T |Dc ) = .1, then\nP(T ) = .9p + .1(1 - p).\nWhat is P(D|T )?\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nBayes' theorem\nBayes' theorem/law/rule states the following:\nP(B|A)P(A)\nP(A|B) =\n.\nP(B)\nFollows from definition of conditional probability:\nP(AB) = P(B)P(A|B) = P(A)P(B|A).\nTells how to update estimate of probability of A when new\nevidence restricts your sample space to B.\nP(B|A)\nSo P(A|B) is\ntimes P(A).\nP(B)\nP(B|A)\nRatio\ndetermines \"how compelling new evidence is\".\nP(B)\nWhat does it mean if ratio is zero?\nWhat if ratio is 1/P(A)?\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nBayes' theorem\nP(B|A)P(A)\nBayes' formula P(A|B) =\nis often invoked as tool\nP(B)\nto guide intuition.\nExample: A is event that suspect stole the 10, 000 under my\nmattress, B is event that suspect deposited several thousand\ndollars in cash in bank last week.\nBegin with subjective estimates of P(A), P(B|A), and\nP(B|Ac ). Compute P(B). Check whether B occurred.\nUpdate estimate.\nRepeat procedure as new evidence emerges.\nCaution required. My idea to check whether B occurred, or is\na lawyer selecting the provable events B1, B2, B3, . . . that\nmaximize P(A|B1B2B3 . . .)? Where did my probability\nestimates come from? What is my state space? What\nassumptions am I making?\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\n\"Bayesian\" sometimes used to describe philosophical view\nPhilosophical idea: we assign subjective probabilities to\nquestions we can't answer. Will Obama win election? Will\nRed Sox win world series? Will stock prices go up this year?\nBayes essentially described probability of event as\nvalue of right to get some thing if event occurs .\nvalue of thing\nPhilosophical questions: do we have subjective\nprobabilities/hunches for questions we can't base enforceable\ncontracts on? Do there exist other universes? Are there other\nintelligent beings? Are there beings smart enough to simulate\nuniverses like ours? Are we part of such a simulation?...\nDo we use Bayes subconsciously to update hunches?\nShould we think of Bayesian priors and updates as part of the\nepistemological foundation of science and statistics?\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nUpdated \"odds\"\nDefine \"odds\" of A to be P(A)/P(Ac ).\nDefine \"conditional odds\" of A given B to be\nP(A|B)/P(Ac |B).\nIs there nice way to describe ratio between odds and\nconditional odds?\nP(A|B)/P(Ac |B) =?\nP(A)/P(Ac )\nBy Bayes P(A|B)/P(A) = P(B|A)/P(B).\nP(A|B)/P(Ac |B)\nAfter some algebra,\n= P(B|A)/P(B|Ac )\nP(A)/P(Ac )\nSay I think A is 5 times as likely as Ac , and\nP(B|A) = 3P(B|Ac ). Given B, I think A is 15 times as likely\nas Ac .\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nP(·|F ) is a probability measure\nWe can check the probability axioms: 0 ≤ P(E|F ) ≤ 1,\n\nP(S|F ) = 1, and P(∪Ei |F ) =\nP(Ei |F ), if i ranges over a\ncountable set and the Ei are disjoint.\nThe probability measure P(·|F ) is related to P(·).\nTo get former from latter, we set probabilities of elements\noutside of F to zero and multiply probabilities of events inside\nof F by 1/P(F ).\nIt P(·) is the prior probability measure and P(·|F ) is the\nposterior measure (revised after discovering that F occurs).\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n\nOutline\nBayes' formula\nIndependence\n18.440 Lecture 7\n\nOutline\nBayes' formula\nIndependence\n18.440 Lecture 7\n\nI\nI\nI\nI\nI\nI\nI\nI\nIndependence\nSay E and F are independent if P(EF ) = P(E )P(F ).\nEquivalent statement: P(E |F ) = P(E ). Also equivalent:\nP(F |E ) = P(F ).\nExample: toss two coins. Sample space contains four equally\nlikely elements (H, H), (H, T ), (T , H), (T , T ).\nIs event that first coin is heads independent of event that\nsecond coin heads.\nYes: probability of each event is 1/2 and probability of both is\n1/4.\nIs event that first coin is heads independent of event that\nnumber of heads is odd?\nYes: probability of each event is 1/2 and probability of both is\n1/4...\ndespite intuitive sense that oddness of the number of heads\n\"depends\" on the first coin.\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nIndependence of multiple events\nSay E1 . . . En are independent if for each\n{i1, i2, . . . , ik } ⊂{1, 2, . . . n} we have\nP(Ei1 Ei2 . . . Eik ) = P(Ei1 )P(Ei2 ) . . . P(Eik ).\nIn other words, the product rule works.\nIndependence implies P(E1E2E3|E4E5E6) =\nP(E1)P(E2)P(E3)P(E4)P(E5)P(E6) = P(E1E2E3), and other similar\nP(E4)P(E5)P(E6)\nstatements.\nDoes pairwise independence imply independence?\nNo. Consider these three events: first coin heads, second coin\nheads, odd number heads. Pairwise independent, not\nindependent.\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nIndependence: another example\nShuffle 4 cards with labels 1 through 4. Let Ej,k be event that\ncard j comes before card k. Is E1,2 independent of E3,4?\nIs E1,2 independent of E1,3?\nNo. In fact, what is P(E1,2|E1,3)?\nGeneralize to n > 7 cards. What is\nP(E1,7|E1,2E1,3E1,4E1,5E1,6)?\n18.440 Lecture 7\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/77f1687d8bbcb56abe049dbca2375326_MIT18_440S14_Lecture8.pdf",
      "content": "18.440: Lecture 8\nDiscrete random variables\nScott Sheffield\nMIT\n18.440 Lecture 8\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nRandom variables\nI A random variable X is a function from the state space to the\nreal numbers.\nI Can interpret X as a quantity whose value depends on the\noutcome of an experiment.\nI Example: toss n coins (so state space consists of the set of all\n2n possible coin sequences) and let X be number of heads.\nI Question: What is P{X = k} in this case?\n\nn\nI Answer:\n/2n, if k ∈{0, 1, 2, . . . , n}.\nk\n18.440 Lecture 8\n\nI\nI\nI\nI\nI\nI\nIndependence of multiple events\nIn n coin toss example, knowing the values of some coin\ntosses tells us nothing about the others.\nSay E1 . . . En are independent if for each\n{i1, i2, . . . , ik } ⊂{1, 2, . . . n} we have\nP(Ei1 Ei2 . . . Eik ) = P(Ei1 )P(Ei2 ) . . . P(Eik ).\nIn other words, the product rule works.\nIndependence implies P(E1E2E3|E4E5E6) =\nP(E1)P(E2)P(E3)P(E4)P(E5)P(E6) = P(E1E2E3), and other similar\nP(E4)P(E5)P(E6)\nstatements.\nDoes pairwise independence imply independence?\nNo. Consider these three events: first coin heads, second coin\nheads, odd number heads. Pairwise independent, not\nindependent.\n18.440 Lecture 8\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nExamples\nShuffle n cards, and let X be the position of the jth card.\nState space consists of all n! possible orderings. X takes\nvalues in {1, 2, . . . , n} depending on the ordering.\nQuestion: What is P{X = k} in this case?\nAnswer: 1/n, if k ∈{1, 2, . . . , n}.\nNow say we roll three dice and let Y be sum of the values on\nthe dice. What is P{Y = 5}?\n18.440 Lecture 8\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nIndicators\nGiven any event E , can define an indicator random variable,\ni.e., let X be random variable equal to 1 on the event E and 0\notherwise. Write this as X = 1E .\nThe value of 1E (either 1 or 0) indicates whether the event\nhas occurred.\nk\nIf E1, E2, . . . Ek are events then X =\nis the number\ni=1 1Ei\nof these events that occur.\nExample: in n-hat shuffle problem, let Ei be the event ith\nperson gets own hat.\nn\nThen\nis total number of people who get own hats.\ni=1 1Ei\nWriting random variable as sum of indicators: frequently\nuseful, sometimes confusing.\n18.440 Lecture 8\n▶\n▶\n▶\n▶\n▶\n▶\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nI\nI\nProbability mass function\nSay X is a discrete random variable if (with probability one)\nit takes one of a countable set of values.\nFor each a in this countable set, write p(a) := P{X = a}.\nCall p the probability mass function.\n18.440 Lecture 8\n▶\n▶\n\nI\n\nI\nI\nI\nCumulative distribution function\nWrite F (a) = P{X ≤ a} =\np(x).\nx≤a\nExample: Let T1, T2, T3, . . . be sequence of independent fair\ncoin tosses (each taking values in {H, T }) and let X be the\nsmallest j for which Tj = H.\nWhat is p(k) = P{X = k} (for k ∈ Z) in this case?\nWhat is F ?\n18.440 Lecture 8\n▶\nP\n▶\n▶\n▶\n\nI\nI\n\nI\nI\nI\n\nI\nI\nAnother example\nAnother example: let X be non-negative integer such that\np(k) = P{X = k} = e-λλk /k!.\nRecall Taylor expansion\ninf λk /k! = eλ .\nk=0\nIn this example, X is called a Poisson random variable with\nintensity λ.\nQuestion: what is the state space in this example?\nAnswer: Didn't specify. One possibility would be to define\nstate space as S = {0, 1, 2, . . .} and define X (as a function\non S) by X (j) = j. The probability function would be\ndetermined by P(S) =\nk∈S e-λλk /k!.\nAre there other choices of S and P -- and other functions X\nfrom S to P -- for which the values of P{X = k} are the\nsame?\nYes. \"X is a Poisson random variable with intensity λ\" is\nstatement only about the probability mass function of X .\n18.440 Lecture 8\n▶\n▶\nP\n▶\n▶\n▶\nP\n▶\n▶\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nOutline\nDefining random variables\nProbability mass function and distribution function\nRecursions\n18.440 Lecture 8\n\nI\nI\nI\nI\nI\n\nI\nUsing Bayes' rule to set up recursions\nGambler one has integer m dollars, gambler two has integer n\ndollars. Take turns making one dollar bets until one runs out\nof money. What is probability first gambler runs out of money\nfirst?\nGambler's ruin: what if gambler one has an unlimited\namount of money?\nProblem of points: in sequence of independent fair coin\ntosses, what is probability Pn,m to see n heads before seeing m\ntails?\nObserve: Pn,m is equivalent to the probability of having n or\nmore heads in first m + n - 1 trials.\nm+n-1\nProbability of exactly n heads in m + n - 1 trials is\n.\nn\nFamous correspondence by Fermat and Pascal. Led Pascal to\nwrite Le Triangle Arithm etique.\n18.440 Lecture 8\n▶\n▶\n▶\n▶\n▶\n\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/7bd7eddd391935e5a54e8438e436853c_MIT18_440S14_Lecture9.pdf",
      "content": "18.440: Lecture 9\nExpectations of discrete random variables\nScott Sheffield\nMIT\n18.440 Lecture 9\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nExpectation of a discrete random variable\nI Recall: a random variable X is a function from the state space\nto the real numbers.\nI Can interpret X as a quantity whose value depends on the\noutcome of an experiment.\nI Say X is a discrete random variable if (with probability one)\nit takes one of a countable set of values.\nI For each a in this countable set, write p(a) := P{X = a}.\nCall p the probability mass function.\nI The expectation of X , written E [X ], is defined by\n\nE [X ] =\nxp(x).\nx:p(x)>0\nI Represents weighted average of possible values X can take,\neach value being weighted by its probability.\n18.440 Lecture 9\n\nI\nI\nI\nI\nI\nI\nI\nSimple examples\nSuppose that a random variable X satisfies P{X = 1} = .5,\nP{X = 2} = .25 and P{X = 3} = .25.\nWhat is E [X ]?\nAnswer: .5 × 1 + .25 × 2 + .25 × 3 = 1.75.\nSuppose P{X = 1} = p and P{X = 0} = 1 - p. Then what\nis E [X ]?\nAnswer: p.\nRoll a standard six-sided die. What is the expectation of\nnumber that comes up?\n6 = 21\nAnswer:\n1 + 2 + 3 + 4 + 5 +\n= 3.5.\n18.440 Lecture 9\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\n\nI\n\nI\nI\nExpectation when state space is countable\nIf the state space S is countable, we can give SUM OVER\nSTATE SPACE definition of expectation:\nE [X ] =\nP{s}X (s).\ns∈S\nCompare this to the SUM OVER POSSIBLE X VALUES\ndefinition we gave earlier:\nE [X ] =\nxp(x).\nx:p(x)>0\nExample: toss two coins. If X is the number of heads, what is\nE [X ]?\nState space is {(H, H), (H, T ), (T , H), (T , T )} and summing\nover state space gives E [X ] = 1 2 + 1 1 + 1 1 + 1 0 = 1.\n18.440 Lecture 9\n▶\n▶\n▶\n▶\nX\nX\n\nI\nI\nA technical point\nIf the state space S is countable, is it possible that the sum\nn\nE [X ] =\nP({s})X (s) somehow depends on the order in\ns∈S\nwhich s ∈ S are enumerated?\nIn principle, yes... We only say expectation is defined when\nn\nP({x})|X (s)| < inf, in which case it turns out that the\ns∈S\nsum does not depend on the order.\n18.440 Lecture 9\n▶\n▶\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nI\nI\nI\n\nI\nI\nI\nI\nExpectation of a function of a random variable\nIf X is a random variable and g is a function from the real\nnumbers to the real numbers then g(X ) is also a random\nvariable.\nHow can we compute E [g(X )]?\nAnswer:\nE[g(X )] =\ng(x)p(x).\nx:p(x)>0\nSuppose that constants a, b, μ are given and that E [X ] = μ.\nWhat is E [X + b]?\nHow about E [aX ]?\nGenerally, E [aX + b] = aE [X ] + b = aμ + b.\n18.440 Lecture 9\n▶\n▶\n▶\nX\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nMore examples\nLet X be the number that comes up when you roll a standard\nsix-sided die. What is E[X 2]?\nLet Xj be 1 if the jth coin toss is heads and 0 otherwise.\nnn\nWhat is the expectation of X =\nXj ?\ni=1\nnn\nCan compute this directly as\nP{X = k}k.\nk=0\nAlternatively, use symmetry. Expected number of heads\nshould be same as expected number of tails.\nThis implies E [X ] = E [n - X ]. Applying\nE [aX + b] = aE [X ] + b formula (with a = -1 and b = n), we\nobtain E [X ] = n - E [X ] and conclude that E[X ] = n/2.\n18.440 Lecture 9\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nAdditivity of expectation\nIf X and Y are distinct random variables, then can one say\nthat E [X + Y ] = E [X ] + E [Y ]?\nYes. In fact, for real constants a and b, we have\nE [aX + bY ] = aE [X ] + bE [Y ].\nThis is called the linearity of expectation.\nAnother way to state this fact: given sample space S and\nprobability measure P, the expectation E[·] is a linear\nreal-valued function on the space of random variables.\nCan extend to more variables\nE [X1 + X2 + . . . + Xn] = E [X1] + E [X2] + . . . + E [Xn].\n18.440 Lecture 9\n▶\n▶\n▶\n▶\n▶\n\nMore examples\nI Now can we compute expected number of people who get\nown hats in n hat shuffle problem?\nI Let Xi be 1 if ith person gets own hat and zero otherwise.\nI What is E [Xi ], for i ∈{1, 2, . . . , n}?\nI Answer: 1/n.\nI Can write total number with own hat as\nX = X1 + X2 + . . . + Xn.\nI Linearity of expectation gives\nE [X ] = E [X1] + E [X2] + . . . + E [Xn] = n × 1/n = 1.\n18.440 Lecture 9\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nOutline\nDefining expectation\nFunctions of random variables\nMotivation\n18.440 Lecture 9\n\nI\nI\nI\nI\nWhy should we care about expectation?\nLaws of large numbers: choose lots of independent random\nvariables same probability distribution as X -- their average\ntends to be close to E [X ].\nExample: roll N = 106 dice, let Y be the sum of the numbers\nthat come up. Then Y /N is probably close to 3.5.\nEconomic theory of decision making: Under \"rationality\"\nassumptions, each of us has utility function and tries to\noptimize its expectation.\nFinancial contract pricing: under \"no arbitrage/interest\"\nassumption, price of derivative equals its expected value in\nso-called risk neutral probability.\n18.440 Lecture 9\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nExpected utility when outcome only depends on wealth\nContract one: I'll toss 10 coins, and if they all come up heads\n(probability about one in a thousand), I'll give you 20 billion\ndollars.\nContract two: I'll just give you ten million dollars.\nWhat are expectations of the two contracts? Which would\nyou prefer?\nCan you find a function u(x) such that given two random\nwealth variables W1 and W2, you prefer W1 whenever\nE [u(W1)] < E [u(W2)]?\nLet's assume u(0) = 0 and u(1) = 1. Then u(x) = y means\nthat you are indifferent between getting 1 dollar no matter\nwhat and getting x dollars with probability 1/y.\n18.440 Lecture 9\n▶\n▶\n▶\n▶\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Probability and Random Variables, Lecture 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/2937276069c34885c7da6c1e66cf9e0d_MIT18_440S14_Lecture10.pdf",
      "content": "18.440: Lecture 10\nVariance and standard deviation\nScott Sheffield\nMIT\n18.440 Lecture 10\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nRecall definitions for expectation\nI Recall: a random variable X is a function from the state space\nto the real numbers.\nI Can interpret X as a quantity whose value depends on the\noutcome of an experiment.\nI Say X is a discrete random variable if (with probability one)\nit takes one of a countable set of values.\nI For each a in this countable set, write p(a) := P{X = a}.\nCall p the probability mass function.\nI The expectation of X , written E [X ], is defined by\n\nE [X ] =\nxp(x).\nx:p(x)>0\nI Also,\n\nE[g(X )] =\ng(x)p(x).\nx:p(x)>0\n18.440 Lecture 10\n\nI\nI\nI\n\nI\nDefining variance\nLet X be a random variable with mean μ.\nThe variance of X , denoted Var(X ), is defined by\nVar(X ) = E [(X - μ)2].\nTaking g(x) = (x - μ)2, and recalling that\n)\nE [g(X )] =\nx:p(x)>0 g(x)p(x), we find that\nVar[X ] =\n(x - μ)2 p(x).\nx:p(x)>0\nVariance is one way to measure the amount a random variable\n\"varies\" from its mean over successive trials.\n18.440 Lecture 10\n▶\n▶\n▶\nX\n▶\n\nI\nI\nI\nI\nI\nI\nVery important alternate formula\nLet X be a random variable with mean μ.\nWe introduced above the formula Var(X ) = E [(X - μ)2].\nThis can be written Var[X ] = E [X 2 - 2X μ + μ2].\nBy additivity of expectation, this is the same as\nE [X 2] - 2μE[X ] + μ = E[X 2] - μ .\nThis gives us our very important alternate formula:\nVar[X ] = E [X 2] - (E [X ])2 .\nOriginal formula gives intuitive idea of what variance is\n(expected square of difference from mean). But we will often\nuse this alternate formula when we have to actually compute\nthe variance.\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n▶\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nI\nI\nI\nI\nI\nVariance examples\nIf X is number on a standard die roll, what is Var[X ]?\nVar[X ] = E [X 2] - E [X ]2 =\n91 - 49\n12 +\n22 +\n32 +\n42 +\n52 +\n62 - (7/2)2 =\n=\n12 .\nLet Y be number of heads in two fair coin tosses. What is\nVar[Y ]?\nRecall P{Y = 0} = 1/4 and P{Y = 1} = 1/2 and\nP{Y = 2} = 1/4.\nThen Var[Y ] = E[Y 2] - E [Y ]2 =\n02 +\n12 +\n22 - 12 = 2 .\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nMore variance examples\nYou buy a lottery ticket that gives you a one in a million\nchance to win a million dollars.\nLet X be the amount you win. What's the expectation of X ?\nHow about the variance?\nVariance is more sensitive than expectation to rare \"outlier\"\nevents.\nAt a particular party, there are four five-foot-tall people, five\nsix-foot-tall people, and one seven-foot-tall person. You pick\none of these people uniformly at random. What is the\nexpected height of the person you pick?\nVariance?\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n▶\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nI\nI\nI\nI\nIdentity\nIf Y = X + b, where b is constant, then does it follow that\nVar[Y ] = Var[X ]?\nYes.\nWe showed earlier that E [aX ] = aE [X ]. We claim that\nVar[aX ] = a2Var[X ].\nProof: Var[aX ] = E [a2X 2] - E [aX ]2 = a2E [X 2] - a2E [X ]2 =\na2Var[X ].\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nStandard deviation\n\nWrite SD[X ] =\nVar[X ].\nSatisfies identity SD[aX ] = aSD[X ].\nUses the same units as X itself.\nIf we switch from feet to inches in our \"height of randomly\nchosen person\" example, then X , E [X ], and SD[X ] each get\nmultiplied by 12, but Var[X ] gets multiplied by 144.\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nOutline\nDefining variance\nExamples\nProperties\nDecomposition trick\n18.440 Lecture 10\n\nI\nI\nI\nI\nI\nI\nI\nI\nI\nNumber of aces\nChoose five cards from a standard deck of 52 cards. Let A be\nthe number of aces you see.\nCompute E[A] and Var[A].\nHow many five card hands total?\n\nAnswer:\n.\nHow many such hands have k aces?\n\nAnswer:\n.\nk\n5-k\n(4)( 48 )\nSo P{A = k} =\nk\n(52\n5-k .\n)\n)4\nSo E[A] =\nkP{A = k},\nk=0\n)4\nand Var[A] =\nk2P{A = k} - E [A]2 .\nk=0\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\nI\nNumber of aces revisited\nChoose five cards from a standard deck of 52 cards. Let A be\nthe number of aces you see.\nChoose five cards in order, and let Ai be 1 if the ith card\nchosen is an ace and zero otherwise.\n)5\n)5\nThen A =\nAi . And E [A] =\nE [Ai ] = 5/13.\ni=1\ni=1\nNow A2 = (A1 + A2 + . . . + A5)2 can be expanded into 25\n)5\n)5\nterms: A2 =\nAi Aj .\ni=1\nj=1\n)5\n)5\nSo E[A2] =\nE [Ai Aj ].\ni=1\nj=1\nFive terms of form E [Ai Aj ] with i = j five with i = j. First\nfive contribute 1/13 each. How about other twenty?\nE [Ai Aj ] = (1/13)(3/51) = (1/13)(1/17). So\nE [A2] =\n+\n=\n13×17\n13×17 .\nVar[A] = E [A2] - E [A]2 =\n-\n13×17\n13×13 .\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n▶\n\n▶\n▶\n\nI\nI\nI\nI\nI\nI\nI\n\nI\nHat problem variance\nIn the n-hat shuffle problem, let X be the number of people\nwho get their own hat. What is Var[X ]?\nWe showed earlier that E [X ] = 1. So Var[X ] = E [X 2] - 1.\nBut how do we compute E [X 2]?\nDecomposition trick: write variable as sum of simple variables.\nLet Xi be one if ith person gets own hat and zero otherwise.\n)n\nThen X = X1 + X2 + . . . + Xn =\ni=1 Xi .\nWe want to compute E [(X1 + X2 + . . . + Xn)2].\nExpand this out and using linearity of expectation:\nn\nn\nn\nn\nE [\nXi\nXj ] =\nE [Xi Xj ] = n· +n(n-1)\n= 2.\nn\nn(n - 1)\ni=1\nj=1\ni=1 j=1\nSo Var[X ] = E [X 2] - (E [X ])2 = 2 - 1 = 1.\n18.440 Lecture 10\n▶\n▶\n▶\n▶\n▶\n▶\n▶\nX\nX\nX X\n▶\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.440 Probability and Random Variables\nSpring 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}