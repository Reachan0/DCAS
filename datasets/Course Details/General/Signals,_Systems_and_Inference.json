{
  "course_name": "Signals, Systems and Inference",
  "course_description": "This course covers signals, systems and inference in communication, control and signal processing. Topics include input-output and state-space models of linear systems driven by deterministic and random signals; time- and transform-domain representations in discrete and continuous time; and group delay. State feedback and observers. Probabilistic models; stochastic processes, correlation functions, power spectra, spectral factorization. Least-mean square error estimation; Wiener filtering. Hypothesis testing; detection; matched filters.",
  "topics": [
    "Engineering",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Signal Processing",
    "Telecommunications",
    "Engineering",
    "Electrical Engineering",
    "Robotics and Control Systems",
    "Signal Processing",
    "Telecommunications"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1 hour / session\n\nRecitations: 2 sessions / week, 1 hour / session\n\nPrerequisites\n\nThe essential prerequisites for this course are\n6.003 Signals and Systems\nand\n6.041A Intro to Probability I\n(or equivalents), and the\n18.03 Differential Equations\nmaterial related to solving linear, time-invariant sytems of first-order differential equations using eigenvalues and eigenvectors.\n\nNo matter how fresh and comfortable you are with the prerequisite material, it can take some time to connect it to what's going on in a new class, so make some allowance for that fact--we on the staff certainly will. If we say \"xyz should be familiar from 6.003/6.041A\" and you don't recall ever having seen \"xyz\" in your life, ask questions, do some reading, discuss things with your fellow students or the staff, until you are able to connect the dots.\n\nAbout This Course\n\nThe lectures, recitations, and homework in 6.011 will expand on signals, systems and probabilistic models. We will also explore prototype problems and applications from communication, control and signal processing. The topics will involve aspects of analysis, synthesis and optimization, for both continuous-time (CT) and discrete-time (DT) systems. The ideas, approaches and methods you learn here will significantly expand the range of engineering applications that you will be able to understand and work with at some level.\n\nWhat will be new relative to 6.003 and 6.041A? The list includes most of the following:\n\nnew kinds of signals (e.g., random processes)\n\nnew signal characterizations (e.g., autocorrelation functions, energy/power spectral densities)\n\nnew kinds of system descriptions (e.g., state-space models for causal systems)\n\nnew system properties (e.g., reachability/observability)\n\nnew signal processing tasks (e.g., optimal estimation)\n\nnew communication tasks (e.g., optimal detection)\n\nnew control tasks (e.g., state estimation, observer-based controller design)\n\nmore intimate mixing of DT and CT in several applications\n\nThese topics do not fall in a linear path away from 6.003/6.041A. Rather, we will be expanding out in a spiral, sampling the many routes that lead out from 6.003/6.041A, coming back to some of them one or two times. At the end you may be surprised to find how much territory you have covered without moving all that far away from the basics in 6.003 and 6.041A.\n\nTextbooks\n\nRequired:\n\nThere are two versions of the book for purchase. Problem numbers for both the hardcover and softcover versions will be given in assignments.\n\nHardcover\n: Oppenheim, Alan, and George Verghese.\nSignals, Systems and Inference\n. Pearson, 2015. ISBN: 9180133943283\n\nSoftcover\n: Oppenheim, Alan, and George Verghese.\nSignals, Systems and Inference\n. Pearson, 2017. ISBN: 9781292156200\n\nRecommended:\n\nOppenheim, Alan, and Alan Willsky, with S. Hamid Nawab.\nSignals and Systems\n. 2nd ed. Prentice-Hall, 1996.\n\nBertsekas, Dimitri, and John Tsitsiklis.\nIntroduction to Probability\n. 2nd ed. Athena Scientific, 2008.\n\nLearning Objectives\n\nWe'll aim to (un)cover the following topics:\n\nBrief review of linear, time-invariant (LTI) system models in continuous and discrete time (CT and DT), and in the frequency domain. Deterministic autocorrelation.\n\nState-space models (mainly LTI).\n\nBrief review of random variables.\n\nEstimation.\n\nStationary random processes in time and frequency domains.\n\nSignal estimation.\n\nHypothesis testing.\n\nSome intimations of machine learning: training and applying quadratic discriminators in feature space.\n\nSignal detection.\n\nGrading\n\nactivities\n\nPercentages\n\nParticipation\n\n15%\n\nWeekly Assignments\n\n25%\n\nQuiz 1\n\n20%\n\nQuiz 2\n\n15%\n\nFinal Exam\n\n25%\n\nAttendance and participation in lectures and recitations are expected and necessary, in order for you to engage with and learn the subject well. We recognize \"participation\" as including\n\nengagement with the course material\n\nkeeping up week-to-week\n\ncontributions to class discussions\n\nresponsiveness to questions and questionnaires\n\nWe encourage you to view each homework as an occasion for learning, on your own or/and in a study group, and through discussions with the staff. The problem sets are not intended as weekly tests, and will be graded liberally on a 0, 1, 2, 3 scale. If you attend, participate, and do your homework, you will have earned at least a C in the class.\n\nThe tests will all be closed-book, but you will be allowed two two-sided sheets of notes for Quiz 1, three sheets for Quiz 2, and four sheets for the final exam.\n\nLower-Friction Option\n\nIf your particular optimization of your route through MIT causes you to prefer a \"lower-friction\" version of the course, please inform Prof. Verghese and he will switch your grading to eliminate the 15% allotted to attendance and participation. (Keep in mind, however, that anything covered in lecture and recitation is fair game on quizzes and the final exam.) If you elect this option, it will be irreversible. Homework will still count for 25% of your grade, but the two quizzes and final will count for 25%, 20%, and 30% of the course grade respectively. Also, if you select this option, the sign-up tutorials with a teaching assistant (TA) will not be available to you (as we cannot have the TAs providing private make-ups for missed lectures and recitations), but you are welcome to all other components of the class. Finally, if you elect this option, you do not need to email your instructor to let them know in advance of any lecture or recitation you will miss.\n\nProject Option\n\nIf you are interested in doing a project related to the class material, and provided you are not in the lower-friction version of the class (see above), please talk to Prof. Verghese. If your project proposal is accepted, you will have until the last day to add a class to commit to doing it, submitting a one-page written proposal. In this case, the relative weightings of the course components in determining your final grade will be: attendance & participation 15%; homework 25%; Quiz 1 for 15%; Quiz 2 for 15%; Final 20%, and project 10%. You will not be allowed to return to the non-project option after committing, so don't commit unless you are sure you will follow through all the way. A written (roughly 10-page) report on the project will be due at the end of the term.\n\nAssignments\n\nSome part of the homework may involve computer exercises. We assume that you already have some familiarity with using Matlab or Python or Mathematica or some such environment to tackle these exercises.\n\nEach problem set will be given a score of 0-3, with the 0 indicating little evidence of any original thought or work (e.g., if a solution is quite clearly lifted from solutions distributed in an earlier term--if this happens more than once, the consequences will be worse than just a 0 on the homework), and with the 3 indicating a good homework submission, demonstrating understanding and careful work. If you're following the course well and doing the problem sets attentively, you should expect to be getting 2's and 3's.\n\nDon't leave the comparison and study of our solutions and yours until the night before a test! Make sure this is a weekly effort, and feel free to see the staff for further discussion on the homework solutions. To facilitate this effort, you may want to make a copy of your homework solutions for yourself before you turn them in! This will allow you to begin comparing your solutions with ours as soon as you receive ours, rather than waiting until your solutions are returned to you one week later (by which time we will be into other topics).\n\nIt should be evident from our grading policy that we do not intend the homeworks as tests, but as vehicles for learning. Therefore, we will not hesitate to use problems from previous terms. Relying upon \"bibles\" to get you through the homeworks--rather than on your own thinking and understanding--will undoubtedly cause you difficulties on the tests. You may expect that each test will include some problems of the same flavor and difficulty as those encountered on the homework, but sufficiently modified to assess your thinking and understanding, rather than your ability to \"pattern match.\" The tests will also include problems that require you to integrate and reason with the course material.\n\nSolutions to problems labeled as \"Optional\" on the homework do not need to be turned in. However, we select these problems with the same care as the assigned problems. They will give you valuable practice and/or bring up important issues, so you are likely to find them helpful to do as time permits, either along with the other problems, or in reviewing for tests.\n\nWe expect each of you to put in enough time alone to understand the specific difficulties and issues raised by each problem. Moderate collaboration on homework problems with one or two of your classmates may be useful for some of you. Discussions with the staff are encouraged, especially since this is the best way for the staff to get to know you. There is no harm in seeking minor assistance from others who are knowledgeable but not involved in the class, although we would much prefer that your discussion be with those in the class. We expect you to independently write up the actual solutions that you turn in, and to note on your solutions the name of anyone you have collaborated with or obtained help from.\n\nTutorials\n\nThe tutorial sessions run by the teaching assistants (TAs) are optional. The sessions are an hour each and will be strictly limited to five students at a time. These will start from the second week of term. We will be arranging for on-line sign-up. Please sign up for only one slot at a time each week. If you do sign up for a tutorial slot, you will be expected to attend. If, after signing up, you find you will be unable to attend for whatever reason, please remove yourself from the schedule so that the slot can be cleared for someone else. If all posted slots fill up, we will try to schedule additional ones. The tutorials will, in effect, function as the TAs' primary office hours, so bring your questions to them. While the TAs may have prepared questions and problems for you to work through interactively at the board, the burden will be largely on you to articulate what you are having trouble with, and to give examples of the kinds of things you are getting stuck on. You should expect to go to the board and help work things out alongside your TA or fellow students. The tutorials are not intended to be mini-lectures or mini-recitations, and are emphatically not intended to help you make up for missed lectures or recitations or prerequisite subjects! At the end of each term, several students invariably suggest that tutorials should have been required rather than optional, either because they benefited greatly from them, or because they recognized (too late!) that they would have benefited from them.",
  "files": [
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 1 Signals & Systems in Time & Frequency",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/5ede01cbe5c8fa60740a60ef60c70b1a_MIT6_011S18ps1.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 1\nSignals & Systems in Time & Frequency\nReading:\nRead Sections 1.1-1.3 of Signals, Systems & Inference (SSI from now on, if we need to be\nexplicit, otherwise understood). Most of this should be familiar to you from 6.003. You can\ndefer reading Section 1.4 till later in the term. The first problem below relates to the material\nin Section 1.5, though you can/should try tackling it before reading the section.\nIn the next two weeks we will be drawing on significant portions of what you learned in\n6.003, and adding some more. You should work on re-familiarizing yourself with the material in\nChapters 1, 2, 4, 5, 6, 7, 9 and 10 of the Oppenheim and Willsky text (Signals & Systems, 2nd\nEdition), or equivalent material from whatever other text you are more familiar with. (Also\ntake a glance at Chapter 7 of SSI, so you can gauge the level of comfort we'll need with the\nprobability material, when we get to it around spring break.)\nTry to get an early start on the homework. Keep in mind that the homework is intended\nto guide your learning, rather than to be a test or exhaustive checklist of things you have to\nknow. Put in a good, honest effort, but talk to others (students in the class, or staff) if you're\nstuck, to see if they can help you get unstuck before you put in an inordinate amount of time.\nProblem 1.1\nRead the posted note1 on \"What you need to know about bandlimited signals for 6.011\".\nRead it partly as a way to review some of your 6.003, because we invoke material from there\nat every step of the development in the note. Then tackle Problem 1.17, part (a) only (which\nis Problem 1.21(a) in the softcover edition).\nYou might find it helpful to read Section 1.5 for background on this problem, or after you've\ndone this problem. However, we're less interested this term in \"DT Processing of CT Signals\"\nas a topic in itself than as a vehicle for reviewing 6.003 material.\n1Since this note was written today, it may still have a bug or two, so please read attentively and let George\nknow if something doesn't seem quite right!\n\nProblem 1.2\nProblem 1.56 (which is 1.49 in the softcover edition of SSI), but instead of the Barker code\nof length 13 in part (f), use the following sequence of length 14:\n-1 1 -1 -1 1 -1 1 1 1 -1 -1 1 1 1\n(This was generated as a pseudorandom signed binary sequence using ltePRBS in Matlab, but\nthat doesn't matter for this problem.)\nProblem 1.3 (Optional)\nThe following problems from Chapter 1 of SSI should be good practice on the basics of\nlinearity, time invariance, convolution, and Fourier transforms, if you're looking for practice.\nProblems 1.2, 1.8, 1.9, 1.10, 1.11, 1.12, 1.27, 1.30, 1.36 in the hardcover version of SSI\n(which are Problems 1.4, 1.12, 1.10, 1.11, 1.9, 1.8, 1.26, 1.32 in the softcover version).\nProblem 1.4 (Optional)\nYou might find Problem 1.55 (which is Problem 1.48 in the softcover edition) interesting.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 10 Hypothesis Testing",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/bd4758fd4a44082575b59ca3bf4029e8_MIT6_011S18ps10.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 10\nHypothesis Testing\nReading: Chapter 9. To make a start on this problem set in advance of Monday's lecture,\nread and make sense of Section 9.2 till just before Example 9.1, and also tackle Section 9.3.1.\nThere will be one more homework set: Problem Set 11 will go out next week, and will be\ndevoted to the material in Chapter 13. Although that problem set is not to be turned in, it\nwill not be optional, in the sense that the material on it will absolutely be covered on the\nfinal exam. Around half of the final exam will likely be devoted to material covered after Quiz\n2, and much that to hypothesis testing for random variables and for signals, i.e., the material\non this problem set and the next. So do pay attention to these two problem sets!\nProblem 10.1\nProblem 9.6 (in the hardcover \"North American\" edition of SSI, which is also Problem 9.1\nin the softcover \"Global Edition\" of SSI)\nProblem 10.2\nProblem 9.10 (which is Problem 9.15 in softcover).\nProblem 10.3\nProblem 9.17 (which is Problem 9.18 in softcover)\nProblem 10.4\nProblem 9.22 (which is Problem 9.19 in softcover).\nProblem 10.5 (Optional)\nFor additional practice, try Problems 9.1, 9.2, 9.3, 9.4, 9.9, 9.11, 9.12, 9.13, 9.14, 9.16,\n9.20, 9.23 (which are Problems 9.3, 9.5, 9.2, 9.7, 9.10, 9.13, 9.9, 9.14, 9.11, 9.12, 9.23, 9.22 in\nsoftcover).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 11 Signal Detection",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/e8148aada6cb05f9147c310dc38e938f_MIT6_011S18ps11.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 11\nSignal Detection\nReading: Chapter 13.\nWhat's not optional: Although this problem set is not to be turned in or graded, it is not\noptional, in the sense that the material on it is very relevant to what will be examined in a\nportion of the final. At least half of the final exam will be devoted to material covered after\nQuiz 2, and perhaps a quarter of the exam to hypothesis testing for random variables and for\nsignals, i.e., the material on the previous problem set and this one. So do pay attention to these\ntwo problem sets! Some of the problems here, or closely related material, may be discussed in\nrecitations and tutorials next week.\nFinal exam: The exam will be comprehensive, but as noted above, at least half will be\ndevoted to material covered after Quiz 2. You can bring in 4 sheets of notes (8 sides), but\nno other aids will be needed or allowed.\nProblem 11.1\nProblem 13.5 (in the hardcover \"North American\" edition of SSI, which is also Problem\n13.2 in the softcover \"Global Edition\" of SSI)\nProblem 11.2\nProblem 13.10 (which is Problem 13.7 in the softcover \"Global Edition\" of SSI), but add\nthe following part (f):\n(f) The ideal lowpass filter at the receiver is not the optimal filter for minimizing the proba\nbility of error. Specify in detail what the optimal frequency response would be (keeping\nin mind the given scaling of its impulse response).\nProblem 11.3\nProblem 13.3 (which is Problem 13.5 in the softcover \"Global Edition\" of SSI)\nProblem 11.4\n\nProblem 13.12 (which is Problem 13.8 in the softcover \"Global Edition\" of SSI).\nProblem 11.5 (Optional)\nIf you are doing additional problems, please note that Problem 13.4 (in the hardcover\nedition) was incorrectly classified as Basic; it should have been an Advanced problem (it's now\nProblem 13.12 in the softcover version)!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 2 State-Space Models",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/d029f63226c753cda8cc80da896013ca_MIT6_011S18ps2.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 2\nState-Space Models\nReading: Section 1.5.1 and 1.5.2; then move on to Chapter 4, also Sections 5.1-5.2.\nThe first two problems below still relate to the 6.003 review material we have been occupied\nwith so far, and you should try to get them done before next week's classes. We will be moving\non to state-space models, Chapters 4-6, for the next couple of weeks.\nWe haven't listed any optional problems, because all the unassigned ones in Chapter 4 could\nusefully serve for additional practice.\n\nProblem 2.1\nConsider a discrete-time LTI filter whose frequency response is\njΩ) = 2 exp{j(Ω + Ω3)} ,\nH(e\n|Ω| < π .\njΩ)\n6 H(ejΩ), verifying that they\n(a) Determine the associated magnitude H(e\nand phase\nhave the required symmetry properties (magnitude even in Ω, phase odd in Ω).\n(b) If the input to the system is x[n] = 4 cos(2n + 1), what is the output y[n]?\n(c) Determine the following quantities associated with the unit sample response h[n] of the\nabove system (none of these requires computing h[n] itself):\nP\n-\nh[n] ;\nn\nP\n-\nn\nh[n]\n\n2 ;\nPinf\n- the deterministic autocorrelation function Rhh[m] =\n.\nn=-inf h[n + m]h[n]\nProblem 2.2\nThe figure below shows a standard configuration for DT processing of a CT signal xc(t) to\nproduce a CT signal yc(t).\n\n-\n-\n-\n-\nC/D\nD/C\nHd(ejΩ)\nxc(t)\nxd[n]\nyd[n]\nyc(t)\nT\nT\nThe C/D converter samples its input at integer multiples of T :\nxd[n] = xc(nT ) .\nThe DT processing here involves filtering by an LTI system with frequency response Hd(ejΩ).\nWe assume an ideal D/C converter that performs bandlimited sinc interpolation of its input\nsamples with reconstruction interval T , so\nX\ninf\nsin(π(t - nT )/T )\nyc(t) =\nyd[n]\n.\nn=-inf\nπ(t - nT )/T\n(a) Assume that xc(t) is bandlimited to the frequency range |ω| < π/T , and that Hd(ejΩ) is\nas given in the figure below, with a constant gain of 2 in its passband and 0 elsewhere in\nthe region |Ω| < π.\n-\n6Hd(ejΩ)\nΩ\n-π\nπ\n-π/2\nπ/2\nR\nP\n(i) If the energy\n|xc(t)|2 dt of the CT signal xc(t) is E, what is the energy\n|xd[n]|2\nn\nof the DT signal xd[n]? Be sure to explain how you arrive at your answer, and state\nwhether the answer is approximate or exact.\n(ii) Is it possible for the energy of yd[n] to exceed that of xd[n] for this particular system?\nAgain, explain your answer.\n(b) Suppose still that xc(t) is bandlimited to the frequency range |ω| < π/T .\n(i) Fully and carefully specify a frequency response Hd(ejΩ) that will result in yd[n] =\nT\nxc(nT + 2 ) . (This yd[n] comprises samples of xc(t) at odd integer multiples of T/2,\nwhereas xd[n] comprised samples at even integer multiples of T/2.) Be sure to explain\nyour reasoning.\n(ii) Find the unit sample response hd[n] corresponding to the filter you found in part\n(b)(i).\n\nProblem 2.3\nPart (a) of Problem 4.4, followed by all of Problem 4.5. Note that the state-space models\nhere will be nonlinear, so of the form given in Eq. (4.43) rather than in the matrix form given\nin Eq. (4.41). (These are Problems 4.1(a) and 4.2 in softcover SSI.)\nProblem 2.4\nThe following continuous-time state-space model has been used to represent the time-\nevolution of glucose concentration y(t) in blood plasma, in response to plasma insulin con\ncentration x(t):\ndq1(t)\ndt\n= -k1\n\nq1(t) - 90\n\n- q1(t)q2(t)\ndq2(t)\ndt\n= -k2q2(t) + k3\n\nx(t) - 11\n\ny(t)\n= q1(t) .\nHere q1(t) = y(t) is the plasma concentration of glucose, while q2(t) represents the effective\ninsulin activity, and the parameters k1, k2 and k3 are all positive constants (assume k1 is\ndifferent from k2).\n(a) Determine the equilibrium state of this model when x(t) is fixed at the value x = 11, i.e.,\ndetermine the equilibrium values q1 and q2 of the two state variables.\n(b) Write down the LTI linearized state-space model that governs small perturbations qe1(t),\nqe2(t), xe(t) and ye(t) of the state variables, input and output away from their equilibrium\nvalues.\n(c) With xe(t) set identically to 0 (i.e., with x(t) fixed at 11), is it possible for qe1(t) to vary\nwithout qe2(t) varying? And is it possible for qe2(t) to vary without qe1(t) varying?\nProblem 2.5\nProblem 4.17 (a) and (b) (in the hardcover \"North American\" edition of SSI, which is\nProblem 4.15 (a) and (b) in the softcover \"Global Edition\" of SSI).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 3 Modal Solutions of LTI Systems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/bf151c81159a09d60f38e6363d4a1b13_MIT6_011S18ps3.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 3\nModal Solutions of LTI Systems\nReading:\nLinear Algebra Notes (see Readings).\nAlso at least Sections 5.1-5.3 of the text, but work on finishing Chapter 5.\nStart on this problem set early!\nProblem 3.1\nProblem 5.22 in the hardcover edition of SSI, but ignore the last part of (c) that asks\nwhether the linearized model is reachable/observable. (This is Problem 5.20 in the softcover\nedition.)\nProblem 3.2\nProblem 5.6 (which is Problem 5.4 in softcover)\nProblem 3.3\nProblem 5.16 (which is Problem 5.5 in softcover)\nProblem 3.4\nProblem 5.26 (which is still Problem 5.26 in softcover)\nProblem 3.5\nProblem 5.10 (which is Problem 5.16 in softcover)\nProblem 3.6 (Optional)\nFor additional practice, try Problems 5.2, 5.3, 5.4, 5.7, 5.11, 5.12, 5.13, 5.17, 5.18 (which\nare Problems 5.1, 5.13, 5.3, 5.6, 5.17, 5.10, 5.11, 5.7, 5.8 in softcover SSI).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 4 Reachability and Observability, Transfer Functions, Hidden Modes, Observers",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/d454aa1ae5733af4b91b2e15afec0c6d_MIT6_011S18ps4.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 4\nReachability and Observability, Transfer Functions, Hidden Modes, Observers\nReading: Finish Chapter 5, and read Sections 6.1-6.2.\nNOTE: Quiz 1 portions will run through the material on this problem set and everything\ndone in lectures and recitations this coming week. You can bring 2 sheets (4 sides) of notes\nto the quiz.\nAs usual, start on this problem set early!\nProblem 4.1\nProblem 5.3 (which is Problem 5.13 in softcover), but instead of the forward Euler approx\nimation given in the problem, use the backward Euler approximation:\nh\ni\nq (nT ) ≈\nq(nT ) - q(nT - T ) .\nT\nAlso, assume the input x(t) is identically 0, or equivalently that b = 0.\n[For part (c), the forward Euler approximation causes T to be limited to an upper value\ndetermined by the eigenvalues. Is there such a limit in the case of the backward Euler approxi\nmation? Of course, the smaller the value of T , the better the approximation to the CT solution\nin either case, but part (c) is only asking about asymptotic stability of the DT system.]\nProblem 4.2\nProblem 5.7 (which is Problem 5.6 in softcover), and also determine the value of the product\nβ1ξ1 as well as the product β2ξ2, where these symbols are as defined in the chapter.\nProblem 4.3\nProblem 5.23 (which is Problem 5.24 in softcover), but in (b) and what follows, use\ns + 2\nH2(s) =\n,\ns - 2\n\nand in (c)(ii) also determine for what values of γ the system is BIBO stable.\nProblem 4.4\nConsider an undriven 2nd-order LTI state-space system of the form\nq[n + 1] = Aq[n] and y[n] = c T q[n] + μ[n] ,\nwhere μ[n] denotes an additive noise on the measured output y[n]. Suppose A has distinct\neigenvalues λ1 and λ2, with associated eigenvectors v1 and v2, and denote cT v1, cT v2 by ξ1,\nξ2 respectively. Assume the system is observable, i.e., ξ1 and ξ2 are both nonzero.\n(a) We know the initial condition can be written in the form\nq[0] = α1v1 + α2v2\nfor some weights α1 and α2. Express q[1] in the same form, as a linear combination of v1\nand v2, expressing the weights in terms of α1, α2, λ1, λ2.\n(b) Suppose we don't know the initial condition q[0]. Let's see how well we can infer this\ninitial state -- or equivalently infer α1 and α2 -- from the two measurements y[0] and\ny[1]. Begin by expressing each of these output values in terms of α1, α2, λ1, λ2, ξ1, ξ2,\nμ[0], μ[1], and arranging your results in the form\n\"\n#\n\"\n#\n\"\n#\ny[0]\nα1\nμ[0]\n= M\n+\n.\ny[1]\nα2\nμ[1]\nWrite down M.\n(c) Assuming no measurement noise, obtain explicit expressions for α1 and α2 in terms of\ny[0] and y[1].\n(d) If there is measurement noise, then the actual values of α1 and α2 will differ from the\nvalues you computed in (c) under the assumption of no measurement noise. Use your\nanalysis above to explain how this discrepancy between the actual and estimated value\nbehaves in the following two cases:\n(i) ξ1 or ξ2 becomes very small;\n(ii) λ1 - λ2 becomes very small.\nProblem 4.5\nYou can turn in your solution to this in recitation on Tuesday March 13.\n\nA model of a rotating machine driven by a piecewise-constant torque takes the state-space\nform\n\"\n#\n\"\n# \"\n#\n\"\n#\nq1[k + 1]\n1 T\nq1[k]\nT 2/2\nq[k + 1]\n=\n=\n+\nx[k]\nq2[k + 1]\nq2[k]\nT\n= Aq[k] + bx[k]\nwhere the state vector q[k] comprises the position q1[k] and velocity q2[k] of the rotor, sampled\nat time t = kT ; x[k] is the constant value of the torque in the interval kT ≤ t < kT + T .\nAssume for this problem that T = 0.5.\nNow suppose that we have a noisy measurement of the position, so the available quantity is\n\"\n#\nq1[k]\ny[k] = [1 0]\n+ ζ[k] = c T q[k] + ζ[k] ,\nq2[k]\nwhere ζ[k] denotes the unknown noise. One way to estimate the actual position and velocity is\nby using an observer, which has the form\n\"\n#\n\n` 1\nT b\nqb[k + 1] = Aqb[k] + bx[k] -\ny[k] - c q[k] .\n` 2\nHere qb[k] is our estimate of q[k]. Let the observer error be denoted by qe[k] = q[k] - qb[k].\n(a) Determine the state-space equation that qe[k] satisfies; we'll refer to this as the error\nequation.\n(b) Show that by proper choice of the observer gains ` 1 and ` 2 we can obtain arbitrary self-\nconjugate natural frequencies for the error equation. What choice of ` 1 and ` 2 will place\nthe natural frequencies of the error equation at 0 and 0.8 ?\n(c) For your choice of observer gains in (b), and assuming q1[0] = 4, q2[0] = 1, with zero\ninput for all time (i.e., x[k] ≡ 0) and zero measurement noise (i.e., ζ[k] ≡ 0), set qb1[0] = 0\nand qb2[0] = 0, then compare plots of qb1[k] and qb2[k] with plots of the underlying state\nvariables q1[k] and q2[k], for 0 ≤ k ≤ 20.\nAlso show plots of the estimation error qe1[k] and qe2[k] over this same time window for the\ncase of zero-mean measurement noise ζ[k] that takes the values +0.2 or -0.2 with equal\nprobability at each instant, independently of the values taken at other instants.\nProblem 4.6 (Optional)\nFor additional practice, try Problems 5.2, 5.4, 5.11, 5.12, 5.13, 5.17, 5.18 (which are Problems\n5.1, 5.3, 5.17, 5.10, 5.11, 5.7, 5.8 in softcover SSI).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 5 Probabilistic Models, Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/676b47a24b5a3db42cda6afa7cd378fc_MIT6_011S18ps5.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 5\nProbabilistic Models, Random Variables\nReading: Chapters 7 and 8\nProblem 5.1\nProblem 7.1, but add on as parts (c) and (d) the two parts of Problem 7.7. (These problems\nhave the same numbers in the softcover edition.)\nProblem 5.2\nProblem 7.6, part (a) only (this is Problem 7.4(a) in the softcover edition).\nProblem 5.3\nProblem 8.26. In part (f), the \"MMSE estimate\" referred to is\nE[R | X1 = x1, X2 = x2, . . . , Xn = xn] .\n(This is Problem 8.31 in the softcover edition.)\nProblem 5.4 (Optional)\nProblems 7.3, 7.4 and 7.12 (which are Problems 7.2, 7.5 and 7.8 in the softcover edition).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 6 MMSE and LMMSE Estimation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/a7cc8385953a72f1cd941f7fc42903e3_MIT6_011S18ps6.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 6\nMMSE and LMMSE Estimation\nReading: Chapter 8. To tackle the early problems here, you'll need to read through to\nSection 8.3.1, which is a little ahead of where we left off in lecture, but covers material already\nexplored in Recitation 11.\nProblem 6.1\nProblem 8.11 (in the hardcover \"North American\" edition of SSI, which is Problem 8.6 in\nthe softcover \"Global Edition\" of SSI), but in each part also find the LMMSE estimator, and\ndetermine the minimum mean square error of each of your estimators.\nProblem 6.2\nProblem 8.17 (which is Problem 8.19 in softcover SSI).\nProblem 6.3\nProblem 8.25 (which is Problem 8.27 in softcover SSI).\nProblem 6.4\nProblem 8.27, but part (b) only (this is Problem 8.26(b) in softcover SSI), but change\nthe specification of the random variables to: Q distributed uniformly in the interval [0, 6]\nand W distributed uniformly in the interval [-1, 1], the two being independent of each other.\nProblem 6.5\nProblem 8.31 (which is Problem 8.30 in softcover SSI).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 7 WSS Processes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/1d5577e3da962909efea071969acac74_MIT6_011S18ps7.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 7\nWSS Processes\nReading: Chapter 10\nSome preliminaries: We've introduced the mean function μX (t) = E[X(t)] and autocorre\nlation function RXX (t1, t2) = E[X(t1)X(t2)] of a random process X(t) in last Wednesday's\nlecture, and the notion of wide-sense (or weak-sense) stationarity, WSS, in last Thursday's\nrecitation. For a WSS process, the mean function does not depend on time, so μX (t) = μX ,\nand the autocorrelation function depends only on the lag τ = t2 - t1 rather than on t1 and\nt2 individually, so RXX (t + τ, t) = RXX (τ, 0) for all t. We can therefore streamline the nota\ntion for the autocorrelation function of a WSS process X(t) to RXX (τ). Note also that the\nautocovariance function CXX (t1, t2) can be computed as RXX (t1, t2)-μX (t1)μX (t2); for a WSS\nprocess this becomes CXX (τ ) = RXX (τ) - μX .\nProblem 7.1\nA waveform used in a digital communication system is modeled as a random signal (or\nprocess) X(t) with the following properties:\n- The signal is piecewise constant over intervals (or \"slots\") of length T , and has the value\nAk in the kth interval, where the values {Ak} form an i.i.d. sequence with mean 0 and\nvariance σ2\nA for each k.\n- The start time of the first interval following the time origin t = 0 is equally likely to\nbe any value between 0 and T . Thus X(t) = A1 for D < t < D + T , where D is\nuniformly distributed in the interval 0 < D < T . (This is to model the situation where\nthe transmitter and receiver clocks are not synchronized, so the actual start of the first\nslot is not known at the receiver.)\n(a) Draw a labeled sketch of a typical realization of this process, i.e., a typical signal, to help\nyou visualize what's going on.\n(b) What is μX (t) = E[X(t)]?\n\n(c) What is RXX (t1, t2) = RXX (t2, t1) when |t1 - t2| > T ?\n(d) Choose t1 and t2 such that 0 < t1 < t2 < T , and determine RXX (t1, t2). (Your analysis\nwill probably be helped by separately considering two cases: first where 0 < D < t1 or\nt2 < D < T , and second where t1 < D < t2.)\n\n(e) Write RXX (t1 + `T, t2 + `T ) in terms of RXX (t1, t2) for an arbitrary integer `.\n(f) Putting together all the above parts, you can conclude that the process is WSS. What is\nRXX (τ), where τ = t1 - t2?\n(g) Did you need the (zero-mean) Ak and Ai for k 6\ni to be independent for your analysis\n=\nabove to work, or would it have sufficed to have them be uncorrelated?\nProblem 7.2\nConsider a DT random process X[n] defined as follows: the values at distinct times are\nchosen independently; for n even, X[n] is +1 or -1 with equal probability; for n odd, X[n] = 1\nwith probability 10 and X[n] = -3 with probability\nThe process is clearly not strict-sense\n10 .\nstationary. Determine the mean and autocorrelation functions of the process, and thus decide\nwhether the process is wide-sense stationary.\nProblem 7.3\nProblem 10.34 (which is Problem 10.38 in the softcover version).\nProblem 7.4\nThis problem develops the condition for a DT WSS process to be ergodic in mean\nvalue (see definition below). The CT case is developed in Problem 10.43 of SSI, which is\nProblem 10.36 in the softcover version, if you wish to look there for connections or inspiration.\nConsider the random variable Y obtained by time-averaging a WSS process X[·] over the\ninterval [-L, L]:\nL\nX\nY =\nX[n] .\n2L + 1 n=-L\nThis quantity is the (local) time-average of the process X[·] (where \"local\" refers to the fact\nthat the average is taken in the vicinity of time 0 -- but our eventual interest will be in the\ncase L % inf). For any particular realization x[·] of the process, the random variable Y takes\nthe value\nL\nX\ny =\nx[n] .\n2L + 1 n=-L\n(We will usually be less fussy about distinguishing notationally between a random variable and\na particular realized value of it, but it is worth doing in the present context.) The values y\nthat Y takes in different experiments (or realizations) will be centered around the mean value\nE[Y ] = μY of Y , with a spread that is indicated by the standard deviation σY of Y .\n\n(a) Show that E[Y ] = μY = μX , so the expected value of the time-average of the random\nprocess X[·] is the ensemble-average of the process.\n(b) Find an explicit expression for the variance σ2 of Y in terms of the autocovariance function\nY\nCXX [m] of X[·]. (To keep your calculations clean, your first step should be to relate the\ne\ne\ndeviation Y = Y - μY to the relevant deviations X[n] = X[n] - μX .)\nCheck/hint: You should be able to write your expression for σ2 in the form\nY\nK\n\nX\n|m|\n1 -\nCXX [m]\n(K + 1)\nK + 1\nm=-K\nfor an appropriately chosen K - what K?\nA WSS process X[·] for which σ2 & 0 as L % inf has the property that time-averages\nY\ncomputed in different experiments cluster more and more closely (as L % inf) around the\nensemble mean μY = μX , because the variance of this time-average tends to 0. We say in\nsuch a case, where the time-average of a WSS process X[·] tends to its ensemble mean,\nthat the process is ergodic in mean value. This is a significant generalization of the Weak\nLaw of Large Numbers beyond the case of processes that are uncorrelated across time,\nto the more general case of WSS processes. So it is of interest to know what conditions\non the original process X[·] will guarantee that σ2 & 0 as L % inf. This motivates the\nY\nfollowing questions.\n(c) For each of the following processes in (i)-(iii), use the expression you derived in (b) to\ndetermine whether σ2 & 0 as L % inf. Note that it is not necessary to actually evaluate\nY\nyour expression in (b) exactly for the following cases; it could suffice, depending on what\nyou are trying to establish, to determine an upper bound on σ2 that tends to 0 as L % inf,\nY\nor determine a lower bound on σ2 that tends to a positive value as L % inf.\nY\n(i) CXX [m] nonzero over only a finite range of m, say |m| ≤ M, and 0 everywhere else.\nA particular case of this is an iid process, where CXX [m] = σ2 δ[m]. (Note that iid\nX\nimplies the autocovariance is nonzero only at m = 0, but the converse is not true:\nhaving an autocovariance of this form doesn't mean that samples at different times\nare independent, only that they are uncorrelated.)\n(ii) CXX [m] = 3α|m| for some positive or negative α satisfying |α| < 1.\n(iii) CXX [m] = (0.8)|m| + 2.\nSome subtler cases that we don't ask you to solve here, but for which it turns out the σ2\nY\nyou computed in (b) tends to 0 as L % inf, are the following:\n(iv) CXX [m] & 0 as |m| % inf ,\nand\n(v) CXX [m] = cos(Ω0m) .\n\nNote that (i) and (ii) above are special cases of (iv).\nThe sufficient condition (iv) is a good one to remember: Any WSS process X[·] whose\nautocovariance function CXX [m] tends to 0 as |m| tends to inf is ergodic in mean value.\nThis condition, though sufficient, is not necessary -- as case (v) shows; in case (v) the\nautocovariance function does not tend to any limit as |m| % inf but the process is still\nergodic in mean value (because σ2 from (b) does in fact tend to 0 as L % inf).\nY\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "6.011 Signals, Systems and Inference, Assignment 8 LTI Filtering of WSS Processes, Power Spectral Density",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/8826ca643a8e90f6cff6ce455203894b_MIT6_011S18ps8.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nSpring 2018\nProblem Set 8\nLTI Filtering of WSS Processes, Power Spectral Density\nReading: Chapter 11\nQuiz 2: The portions for the quiz will be everything covered since Quiz 1, except that we\nwill omit all state-space material that wasn't covered on Quiz 1 (we'll save that material -\nobservers, etc. - for the final exam). So the portions for Quiz 2 will essentially be Chapters 7, 8, 10\nand whatever part of Chapter 11 we cover next week. You can bring 3 sheets of notes (6 sides) to\nthe quiz.\nProblem 8.1\nProblem 10.43 (which is Problem 10.36 in the softcover edition). This is the CT version of\nthe DT problem you did on ergodicity of the mean in PSet 7. Do only only parts (a) and\n(b), but read part (c) and see if you agree that the processes in parts (i), (iii) and (iv) are\nergodic in mean value, whereas the process in (ii) is not.\nThe result you obtain in (b) can be used to show that a sufficient condition for ergodicity\nof the mean is that Cxx(τ) → 0 as τ →inf (this covers cases (i) and (iii) in this problem, but\nnot case (iv), which is ergodic in mean value even though the autocovariance does not go to 0\nat infinite lag). Similarly, the result you obtained on PSet 7 regarding ergodicity of the mean\ncan be used to show that a sufficient condition for such ergodicity in the case of a DT WSS\nprocess is that Cxx[m] → 0 as m →inf, a fact that you can invoke in the next problem here.\nProblem 8.2\nProblem 10.27 (which is Problem 10.22 in softcover).\nProblem 8.3\nProblem 10.42 (which is Problem 10.34 in softcover). In part (a), feel free to use the\nmnemonic mentioned in Lec 17; you don't need to derive things from scratch.\nContinued ...\n\nProblem 8.4\nProblem 11.22 (which is Problem 11.18 in softcover)\nProblem 8.5 (Optional)\nProblems 10.30, 10.41, 11.1, 11.6, 11.17, 11.23 (which are Problems 10.31, 10.42, 11.2, 11.11,\n11.16, 11.19 in softcover).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.011 Signals, Systems and Inference, Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/e3112fd332b9fba612657572480574f4_MIT6_011S18final.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nFINAL EXAM\nQUESTION & ANSWER BOOKLET\nYour Full Name:\nThe exam is closed book, but 4 sheets of notes (8 sides) are allowed.\nCalculators and other aids will not be necessary and are not allowed.\nCheck that this QUESTION & ANSWER BOOKLET has pages num\nbered up to 20. The booklet contains spaces for all relevant work and rea\nsoning, and we'll hand out scratch paper as needed for your rough work.\nThe prompt given above each answer space is an abbreviated version of the\nfull question, so read the full question, not just the prompt!\nNeat work and clear explanations count; show all relevant work and\nreasoning in the indicated spaces, because those spaces are all that we will\nbe looking at in grading.\nThere are 5 problems, for a total of 75 points. This roughly translates\nto your being able to spend about 2 minutes per point, on average, so don't\nget too bogged down on a problem that is giving you inordinate trouble.\nProblem\nYour Score\n1 (10 points)\n2 (19 points)\n3 (17 points)\n4 (10 points)\n5 (19 points)\nTotal (75 points)\n\nProblem 1 (10 points)\nSuppose we are given a real and finite-energy (but otherwise arbitrary)\nDT signal w[n], with associated DTFT W (ejΩ). We want to approximate\nw[n] by another real, finite-energy DT signal y[n] that is bandlimited to\nthe frequency range |Ω| < π/4 (within the usual [-π, π] interval for Ω); so\nY (ejΩ) is zero for |Ω| ≥ π/4. Apart from this constraint on its bandwidth,\nwe are free to choose y[n] as needed to get the best approximation.\nSuppose we measure the quality of approximation by the following sum-\nof-squared-errors criterion:\ninf\nX\nE =\n(w[n] - y[n])2 .\nn=-inf\nOur problem is then to minimize E by appropriate choice of the bandlimited\ny[n], given the signal w[n]. This problem leads you through to the solution.\n(a) (3 points) Express E in terms of a frequency-domain integral on the\ninterval |Ω| ≤ π that involves W (ejΩ) - Y (ejΩ).\n(b) (5 points) Write your integral from (a) as a sum of integrals, one over\neach of the following ranges: -π ≤ Ω ≤-π/4, -π/4 < Ω < π/4, and\nπ/4 ≤ Ω ≤ π. Use this to deduce how Y (ejΩ) needs to be picked in\norder to minimize E, and what the resulting minimum value of E is.\n(Hint: Resist the temptation in this case to expand out |a - b|2, for\ncomplex a and b, as |a|2 + |b|2 - ab∗ - a ∗b.)\n(c) (2 points) Using your result in (b), write down an explicit formula for\nthe y[n] that minimizes E, expressing this y[n] as a suitable integral\ninvolving W (ejΩ).\nPinf\n1(a) (3 points) With E =\n(w[n] - y[n])2, express E in terms of\nn=-inf\na frequency-domain integral on the interval |Ω| ≤ π that involves\nW (ejΩ) - Y (ejΩ).\nE =\n\n1(b) (5 points) Write your integral from 1(a) as a sum of integrals, one\nover each of the following ranges: -π ≤ Ω < -π/4, -π/4 ≤ Ω ≤ π/4, and\nπ/4 < Ω ≤ π:\nUse this to deduce how Y (ejΩ) needs to be picked in order to minimize\nE (resist the temptation in this problem to expand out |a - b|2, for complex\na and b, as |a|2 + |b|2 - ab∗ - a ∗b):\nY (ejΩ) =\nResulting minimum value of E =\n1(c) (2 points) Using your result in 1(b), write down an explicit formula\nfor the y[n] that minimizes E, expressing this y[n] as a suitable integral\ninvolving W (ejΩ):\ny[n] =\n\nProblem 2 (19 points)\nA continuous-time state-space model for the spread of fashion through\na community takes the form\nd q1(t)\n= -γ q1(t)q2(t) + x(t)\ndt\nd q2(t)\n= γ q1(t)q2(t) - ε q2(t)\ndt\ny(t)\n= η q2(t) .\nThe state variable q1(t) denotes the number of people (approximated as a\nreal number) susceptible to adopting the fashion at time t, and q2(t) denotes\nthe number (again a real number) that have adopted the fashion at this time.\nIn this model, the rate at which the fashion is adopted is proportional (with\nproportionality constant γ) to the product of the susceptible and fashionable\npopulations. The constant ε reflects the rate at which fashionable people\noutgrow/abandon the fashion. The quantity x(t) represents a control input\nthat denotes the rate at which susceptibles are replenished (perhaps through\nadvertising efforts) and y(t) is a measured output, proportional (with pro\nportionality constant η) to the number of fashionables.\n(a) (2 points) Suppose x(t) is held constant at the positive value x >\n0. Show that there is precisely one equilibrium point, and determine\nthe corresponding equilibrium values q1 and q2 of the state variables,\nexpressed in terms of the problem parameters.\n(b) (8 points) Write down the linearized model at this equilibrium point.\nThis model approximately governs the deviations qe1(t) = q1(t) - q1,\nqe2(t) = q2(t) - q2, xe(t) = x(t) - x and ye(t) = y(t) - y when these are\nsufficiently small.\nAs a check, for some choice of parameters in the original nonlinear\nmodel, your linearized model should take the form\n\nd\ndt\neq1(t)\neq2(t)\n=\n-3\n-1\neq1(t)\neq2(t)\n+\nex(t)\ney(t) =\n\n0 0.1\neq1(t)\neq2(t)\n\n.\n\nWhat choice of γx, ε and η yields the above numerical values? And\nwhich - if any - of the three 0 entries that are shown in this linearized\nmodel occur only for a particular combination of parameters?\nUse the linearized model with these specific numerical entries\nfor the rest of this problem.\n(c) (4 points) Determine if the linearized system in (b) is: (i) reachable; (ii)\nobservable. You will get 2 points extra credit if you do this without\nexplicitly computing eigenvectors. Show all relevant reasoning and\ncomputations.\n(d) (5 points) Suppose we implement an output feedback of the form\nxe(t) = g ye(t). Write down the resulting (linearized) closed-loop system\nin the form\nd qe(t) = Ac qe(t) ,\ndt\nand determine what choice of output feedback gain g, if any, will\nresult in the eigenvalues of Ac being -1 and -2.\nd\ndtq1(t) = -γ q1(t)q2(t) + x(t)\nd\ndtq2(t) = γ q1(t)q2(t) - ε q2(t)\ny(t) = η q2(t) .\n2(a) (2 points) Suppose x(t) = x > 0. Show that there is precisely one\nequilibrium point, and determine the equilibrium values q1 and q2 of the\nstate variables:\nq1 =\n, q2 =\n\n2(b) (8 points) Write down the linearized model at this equilibrium\npoint. This model approximately governs the deviations qe1(t) = q1(t) - q1,\nqe2(t) = q2(t) - q2, xe(t) = x(t) - x and ye(t) = y(t) - y when these are\nsufficiently small.\nAs a check, for some choice of parameters in the original nonlinear model,\nyour linearized model should take the form\n\nd\nqe1(t)\n-3 -1\nqe1(t)\n=\n+\nxe(t)\ndt\nqe2(t)\nqe2(t)\n\nqe1(t)\nye(t)\n=\n0.1\n.\nqe2(t)\nWhat choice of γx, ε, and η yields the above numerical values?\nγx =\n, ε =\nand η =\nAnd which - if any - of the three 0 entries that are shown in this linearized\nmodel occur only for a particular combination of parameters?\n\n2(c) (4 points) Determine if the linearized system in 2(b) is: (i) reach\nable; (ii) observable. You will get 2 points extra credit if you do this\nwithout explicitly computing eigenvectors. Show all relevant reasoning\nand computations.\n(i) Reachability:\n(ii) Observability:\n\n2(d) (5 points) Suppose we implement an output feedback of the form\nxe(t) = g ye(t). Write down the resulting (linearized) closed-loop system in\nthe form\nd qe(t) = Ac qe(t) ,\ndt\nand determine what choice of output feedback gain g, if any, will result in\nthe eigenvalues of Ac being -1 and -2.\nAc =\ng =\n\nH(z), LMMSE predictor\nx[n], measured\n\nx[n + 1], prediction\n+ 2 ],\nM(z), minimum phase\nw[n], white\nx[n], measured\nProblem 3 (17 points)\nWe have measurements of a WSS random process x[n] that is modeled as\nthe output of a minimum-phase LTI system whose input is a white process\nw[n], with E{w2[n]} = 1. (Recall that a minimum-phase DT system is\ndefined as stable, causal, and with a stable, causal inverse.) The situation is\nshown in the upper figure.\nSuppose the transfer function of the system above is\nγ\nz - (λ - γ\nd )\nM(z) =\n+ d = d\n,\nz - λ\nz - λ\nwhere γ 6\n= 0 and d = 0. You may also find it helpful to note that\n\n+ λ2 -2\nM(z) = d + γz-1 1 + λz-1\nz\n+ · · · .\n(a) (2 points) What is the expected value of x[n]? (No points unless you\nexplain your reasoning fully!)\n(b) (2 points) What is the fluctuation spectral density Dxx(ejΩ) of x[·],\nexpressed in terms of the given quantities γ, λ, d? (It suffices to have\na correct expression; you need not simplify your expression.)\n\n(c) (3 points) Suppose y[·] is some other process that is jointly wide-sense\nstationary with x[·] (and hence with w[·] too, though we don't ask\nyou to explain why). Express Dyw(z) in terms of Dyx(z) and M(z)\n(and/or closely related quantities, if you think these are needed).\nWe would like to pass the process x[n] through a stable LTI filter with\nsystem function H(z) that is chosen to make this filter the LMMSE estimator\nof x[n+2], i.e., the LMMSE two-step predictor, as shown in the lower figure\non the preceding page. Denote the resulting estimate by xb[n + 2].\n(d) (3 points) Suppose there are no constraints on the LTI filter H(z) be\nyond stability. Determine the optimum H(z) and draw a fully labeled\nsketch of the associated unit sample response h[n]. Also determine the\nassociated MMSE,\nE{(x[n + 2] - xb[n + 2])2} .\n(e) (6 points) Suppose now that we constrain the filter H(z) to not only\nbe stable but also causal. Again determine the optimum filter and\nthe associated mean square error (explaining your reasoning!). Your\nanswers will be expressed in terms of the given parameters, namely γ,\nλ, and d.\n(f) (1 point) Returning to the unit sample response of the optimal un\nconstrained filter in (d), suppose you were to set all the negative-time\nvalues of that h[n] to 0, would you get the unit sample response of the\noptimal causal filter in (e)?\n3(a) (2 points) What is the expected value of x[n]? (No points unless\nyou explain your reasoning fully!)\nE[x[n]] =\n\n3(b) (2 points) What is the fluctuation spectral density Dxx(ejΩ) of\nx[·], expressed in terms of the given quantities γ, λ, d? (It suffices to have a\ncorrect expression; you need not simplify your expression.)\nDxx(ejΩ) =\n3(c) (3 points) Suppose y[·] is some other process that is jointly wide-\nsense stationary with x[·] (and hence with w[·] too, though we don't ask\nyou to explain why). Express Dyw(z) in terms of Dyx(z) and M(z) (and/or\nclosely related quantities, if you think these are needed).\nDyw(z) =\n\n3(d) (3 points) Suppose there are no constraints on the LTI filter H(z)\nbeyond stability. Determine the optimum H(z) and draw a fully labeled\nsketch of the associated unit sample response h[n]. Also determine the\nassociated MMSE, E{(x[n + 2] - xb[n + 2])2} .\nH(z) =\nAssociated MMSE =\nPlot of h[n]:\n3(e) Suppose now that we constrain the filter H(z) to not only be sta\nble but also causal. Again determine the optimum filter and the associated\nmean square error (explaining your reasoning!). Your answers will be ex\npressed in terms of the given parameters, namely γ, λ, and d. (Start work\nhere, continue on next page.)\n\n3(e) continued (6 points)\nH(z) =\nAssociated MMSE =\n3(f) (1 point) Returning to the unit sample response of the optimal\nunconstrained filter in 3(d), suppose you were to set all the negative-time\nvalues of that h[n] to 0, would you get the unit sample response of the\noptimal causal filter in 3(e)?\n\nProblem 4 (10 points)\nSuppose that\nX = S + W\nwhere S and W are independent Gaussian random variables with respective\nS, σ\nmeans μS , μW and respective variances σ\nW .\n(a) (3 points) Is X guaranteed to be a Gaussian random variable? (Be\nsure to state the reasoning behind your answer, otherwise you will lose\npoints.) Also write down the mean and variance of X.\n(b) (6 points) Let sb(X) denote the LMMSE estimator of S from mea\nsurement of X. Obtain an expression for this estimator, and for its\nassociated mean square error, expressed in terms of the given param\neters.\n(c) (1 point) Can the MMSE estimator do better in this case? (To get the\npoint for this part, you will need to explain your answer.)\n4(a) (3 points) Is X guaranteed to be a Gaussian random variable?\n(Be sure to state the reasoning behind your answer, otherwise you will lose\npoints.) Also write down the mean and variance of X.\nIs X guaranteed to be Gaussian?\nμX =\n,\nσ2\nX =\n\n4(b) (6 points) Let sb(X) denote the LMMSE estimator of S from mea\nsurement of X. Obtain an expression for this estimator, and for its associ\nated mean square error, expressed in terms of the given parameters.\nsb(X) =\nMMSE =\n4(c) (1 point) Can the MMSE estimator do better in this case? (To get\nthe point for this part, you will need to explain your answer.)\n\nProblem 5 (19 points)\nA signal X[n] that we will be measuring for n = 1, 2, . . . , L is known to\nbe generated according to one of the following two hypotheses:\nH0 :\nX[n]\n= W [n] holds with a priori probability P (H0) = p0 ,\nH1 :\nX[n]\n= V [n] holds with a priori probability P (H1) = p1 = 1 - p0 .\nHere W [n] is a zero-mean i.i.d. Gaussian process with known constant\nvariance σ2 at each time instant, i.e., the value at each time instant is\nW\ngoverned by the probability density function\nn\n2 o\nw\nfW (w) =\n√\nexp - 2σ2\nσW\n2π\nW\nand the values at different times are independent of each other. Similarly,\nV [n] is a zero-mean Gaussian process, taking values that are independent\nat distinct times, but with a variance that changes in a known manner over\ntime, so the variance at time n is known to be σ2 . We will find it notationally\nn\nhelpful in working through this problem to use the definition\n\nξ[n] =\n-\n.\nσ2\nσ2\nW\nn\nNote that ξ[n] may be positive for some n but negative or zero for others,\ncorresponding to having σW < σn, σW > σn or σW = σn respectively.\n(a) (5 points) Suppose we only have a measurement at n = 1, with X[1] =\nx[1]. Show that the decision rule for choosing between H0 and H1 with\nminimum probability of error, given this measurement, takes the form\n\n2 'H1 '\n>\nξ[1] x[1]\n<\nγ\n'H0 '\nfor some appropriately chosen threshold γ. Specify this γ in terms of\nthe problem parameters.\n(b) (5 points) With your result from (a), but now assuming ξ[1] > 0, sketch\nand label the two conditional densities--namely fX[1]|H (x|H0) and\nfX[1]|H (x|H1)--that govern X[1] under the two respective hypotheses.\n\nAssuming that the two hypotheses are equally likely so p0 = p1,\np\nmark in the points ±\nγ/ξ[1] on the horizontal (i.e., x) axis, then shade\nin the region or regions whose total area yields the conditional proba\nbility P ('H1 '|H0), and express this conditional probability in terms of\nthe standard Q function,\nZ inf\nQ(α) = √\ne -ν2/2 dν .\n2π\nα\n(c) (3 points) With the same situation as in (b), but with the hypotheses\nno longer restricted to be equally likely a priori, specify the range of\nvalues for p0 in which the optimal decision will always be 'H1', no\nmatter what the measured value x[1].\n(d) (6 points) Now suppose we have measurements at n = 1, 2, . . . , L, i.e.,\nwe know X[1] = x[1], X[2] = x[2], . . . , X[L] = x[L]. Determine the\ndecision rule for minimum probability of error, writing it in a form\nthat generalizes your result from (a).\n5(a) Suppose we only have a measurement at n = 1, with X[1] = x[1].\nShow that the decision rule for choosing between H0 and H1 with minimum\nprobability of error, given this measurement, takes the form\n\n2 'H1 '\n>\nξ[1] x[1]\n<\nγ ,\nξ[1] =\n-\n.\nσ2\nσ2\n'H0 '\nW\nfor some appropriately chosen threshold γ. Specify this γ in terms of the\nproblem parameters. (Start work here and continue on next page.)\n\n5(a) continued (5 points)\nγ =\n5(b) (5 points) Assuming ξ[1] > 0, sketch and label fX[1]|H (x|H0) and\np\nfX[1]|H (x|H1). Assuming p0 = p1, carefully mark in the points ±\nγ/ξ[1] on\nthe horizontal (i.e., x) axis, then shade in the region or regions whose total\narea yields the conditional probability P ('H1 '|H0), and express this condi-\nR inf\ntional probability in terms of the standard Q function, Q(α) = √1\ne-ν2/2 dν .\nα\n2π\nP ('H1 '|H0) =\n\n5(c) (3 points) With the same situation as in 5(b), but with the hy\npotheses no longer restricted to be equally likely a priori, specify the range\nof values for p0 in which the optimal decision will always be 'H1', no matter\nwhat the measured value x[1].\nRange of p0 for which we will always pick 'H1':\n5(d) Now suppose we have measurements at n = 1, 2, . . . , L, i.e., we\nknow X[1] = x[1], X[2] = x[2], . . . , X[L] = x[L]. Determine the decision\nrule for minimum probability of error, writing it in a form that generalizes\nyour result from 5(a). (Start your work here, continue on next page.)\n\n5(d) continued (6 points)\nDecision rule:\n[Optional reading: For the special case in which V [n] = S[n] + W [n] ,\nwhere S[n] is a zero-mean i.i.d. Gaussian process that is independent of\nW [·] and has variance σ2 [n], the decision rule from 5(d) can be written as a\nS\ncomparison of the quantity\nL\nX\nx[n] sbn(x[n])\n(1)\nn=1\nwith a fixed threshold, where sbn(X[n]) denotes the LMMSE estimator of\nS[n] from measurement of X[n] under hypothesis H1; this is the estimator\nyou derived in Problem 4(b). This form of the decision rule is similar to\nwhat we obtained in the case of a deterministic signal.]\nThanks for taking 6.011. Enjoy the summer, and if you're gradu\nating and leaving, all good wishes for your life beyond MIT!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.011 Signals, Systems and Inference, Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/0be8ee6b0d04a3ca94662e0059d1b0d1_MIT6_011S18quiz1.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems & Inference\nQuiz 1\nQUESTION & ANSWER BOOKLET\nYour Full Name:\nEmail :\nPlease read this page carefully; it has important information!\nThis exam is closed book, but 2 sheets of notes (4 sides) are allowed. Cal\nculators and other electronic aids will not be necessary and are not allowed.\nThere are 3 problems with various subparts, weighted as shown, and for\na total of 60 points. The points indicated on the following pages for the\nvarious subparts of the problems are our best guess for now, but may be\nmodified slightly when we get to grading.\nCheck that this booklet has pages numbered up to 14. The booklet contains\nspaces for all relevant work and reasoning BUT don't forget to read the\nactual questions! - reading only the short prompts in the answer\nspaces can lead you astray! The work you want us to look at should be\nonly in the indicated spaces. You can use scratch paper for rough work. Neat\nwork and clear explanations count; show all relevant work and reasoning!\n***If you get stuck on something, try and move on!***\nProblem\nYour Score\n1 (18 points)\n2 (23 points)\n3 (19 points)\nTotal (60 points)\n\nProblem 1 (18 points)\nThe figure below shows our standard configuration for DT processing of\na CT signal xc(t) to produce a CT signal yc(t). The C/D box samples the\nCT waveform; the D/C box performs ideal bandlimited sinc interpolation.\n[To minimize the reading you have to do before getting to the questions on\nthis, we've put a more detailed reminder of the specification of this system\nat the end of this problem, if you feel you need it.]\n-\n-\n-\n-\nC/D\nD/C\nH(ejΩ)\nxc(t)\nxd[n]\nyd[n]\nyc(t)\nT\nT\nAssume throughout this problem that xc(t) is bandlimited to the\nfrequency range |ω| < π/T , and that the frequency response of the DT LTI\nfilter is\nΩ\nH(ejΩ) = j\n,\n|Ω| < π .\nT\nAs always, this frequency response repeats periodically with period 2π (so\nit is discontinuous at values of Ω that are odd multiples of ±π, e.g., it is\nπ/T just below Ω = π and it is -π/T just above Ω = π).\n(a) (9 points) Denote the (real-valued) unit sample response of the DT\nfilter by h[n]. Without computing h[n] itself, answer the following and\nexplain your reasoning (but don't get bogged down here -- if you can't\nfigure out some part, move on to the next!):\nP\n(i)\nh[n] = ?\nn\nP\n(ii)\n|h[n]|2 = ?\nn\n(iii) What does H(ejΩ) being purely imaginary tell you about h[n]?\n(iv) h[0] = ?\nP\n(v) Is h[n] absolutely summable, i.e., is\n|h[n]| finite?\nn\n(b) (9 points) Let Xc(jω) and Yc(jω) denote the transforms of xc(t) and\nyc(t) respectively.\n\n(i) Determine the ratio Yc(jω)/Xc(jω) for |ω| < π/T . (The following\ntwo subparts depend on your getting this right, so pay extra\nattention!)\n(ii) Suppose the energy spectral density of xc(t) is Sxx(jω) = 3 across\nthe frequency range |ω| < π/T (and 0 outside, of course). Find\nan expression for the energy spectral density Syy(jω) of yc(t) as\na function of ω, for -inf < ω < inf.\n(iii) For the given system, yc(t) can be obtained from xc(t) by a simple\nmathematical operation. What operation?\nHere's a more detailed description of the components of the system con\nsidered in this problem, if you need a reminder:\nThe C/D converter samples its input at integer multiples of T :\nxd[n] = xc(nT ) .\nThe DT processing here involves filtering by an LTI system with frequency\nresponse H(ejΩ). We assume an ideal D/C converter that performs band-\nlimited sinc interpolation of its input samples with reconstruction interval\nT , so\nX\ninf\nsin(π(t - nT )/T )\nyc(t) =\nyd[n]\n.\nπ(t - nT )/T\nn=-inf\nSpace for rough work below. Begin your answers on next page.\n\n1(a) (9 points) With H(ejΩ) = j Ω ,\n|Ω| < π , determine the follow-\nT\ning:\nP\n(i)\nh[n] =\nn\nExplanation:\nP\n(ii)\n|h[n]|2 =\nn\nExplanation:\n(iii) What does H(ejΩ) being purely imaginary tell you about h[n]?\nAnswer:\n(iv) h[0] =\nExplanation:\nP\n(v) Is h[n] absolutely summable, i.e., is\n|h[n]| finite?\nn\nAnswer and explanation:\n\n1(b) (9 points)\n(i) For |ω| < π/T ,\nYc(jω) =\nXc(jω)\nExplanation:\n(ii) Suppose Sxx(jω) = 3 for |ω| < π/T , and is 0 outside this range.\nFor -inf < ω < inf,\nSyy(jω) =\nExplanation:\n\n1(b)(iii) What simple operation on xc(t) yields yc(t)?\nAnswer and explanation:\nSpace below for additional rough work:\n\nProblem 2 (23 points)\nPart (e) of this problem, designing an observer, can be done indepen\ndently of the earlier parts of this problem.\nA particular mechanical system involves a single mass whose position\nw(t) is governed by the differential equation\n\nd2\nd\nw(t) + w 2(t) - 3\nw(t) + 2w(t) = x 2(t) ,\ndt2\ndt\nwhere x(t) denotes the input to the system (but notice that this input is\nsquared in the above equation) and t is time in seconds. Suppose the output\nof interest is\nd\ny(t) =\nw(t) .\ndt\n(a) (5 points) Obtain a continuous-time 2nd-order state-space description\nof the above system, of the form\n\nd q(t) = f q(t), x(t) ,\ny(t) = g q(t), x(t)\ndt\nfor an appropriately chosen state vector q(t). (Do this carefully, as\nparts (b)-(d) build on this!)\n(b) (3 points) Suppose x(t) = x, a constant, for all t. Determine the\ncorresponding equilibrium values q and y of respectively the state and\noutput of your model in (a) in terms of x.\n(c) (6 points) Obtain a linearized state-space model of the form\nd\nT ˇ\nqˇ(t) = Aqˇ(t) + bxˇ(t) ,\nyˇ(t) = c q(t) + dxˇ(t)\ndt\nthat approximately describes small perturbations from the equilibrium\nvalues you computed in (b), where we have used the notation qˇ(t) =\nq(t)-q, and similarly for ˇx(t) and ˇy(t). Specify the coefficient matrices\nA, b, cT , d completely.\n(d) (3 points) If x = 0 in (b), what are the eigenvalues governing your\nlinearized model in (c), and is the system asymptotically stable?\n(e) (6 points) Depending on how you picked q(t) in (a), you may or may\nnot find that the corresponding linearized model in (c) for a particular\nx has the following coefficient matrices:\n\nA =\n,\nb =\n,\nc T =\n,\nd = 0 .\n-2 -61\n\n[If there is no choice of x that will result in the above matrices for\nyour model in (c), it may just be that you've picked q(t) differently\nthan we have, which is fine -- but it's still worth doing a quick check\nof your work. In any case, use the particular matrices above for this\npart.]\nDesign an observer for the linearized model, using the measured ˇx(t)\nand ˇy(t) to produce an estimate qb(t) of the state vector qˇ(t) of the\nlinearized model. Specifically, determine what choice of observer gains\nwill result in the error dynamics being governed by eigenvalues at -10\nand -20. If we consider a decaying exponential to have essentially\nsettled after 3 time constants (it actually is just under 5% of its initial\nvalue then), around how long does it take for the observer error to\nsettle?\nBegin answers here:\n\nd2\nd\n2(a) (5 points) Given dt2 w(t) + w2(t) - 3\nw(t) + 2w(t) = x2(t)\ndt\n(notice that the input x(t) is squared!) and y(t) = d w(t), a possible choice\ndt\nstate variables is:\nq1(t) =\nq2(t) =\nResulting state evolution equation and output equation\n\nd\ndt q(t) = f q(t), x(t) ,\ny(t) = g q(t), x(t) are:\n\n2(b) (3 points) The equilibrium values q and y corresponding to\nx(t) = x:\nq1 =\nq2 =\ny =\nCalculations:\n2(c)(6 points) Specify the coefficient matrices A, b, cT , d of the lin\nearized model obtained at the equilibrium you computed in (b):\nA =\n,\nb =\n, cT =\n, d =\n\n2(d) (3 points) If x = 0 in (b), the eigenvalues of the linearized model\nin (c) are\nλ1 =\n, λ2 =\nand the system IS\n/\nISN'T asymptotically stable.\nCalculations and explanation:\n2(e) (6 points) Design an observer for a CT LTI system defined by the\nfollowing coefficient matrices:\n\nA =\n,\nb =\n,\nc T =\n,\nd = 0 ,\n-2 -61\nplacing the error decay eigenvalues at -10 and -20. Also determine the\napproximate settling time of the error.\nObserver gains are ` 1 =\n,\n` 2 =\nApproximate settling time is\n\nProblem 3 (19 points)\nParts (a), (b) and (c) are completely independent of each other.\n(a) (6 points) Suppose we are given a 3rd-order reachable LTI CT system\nthat is initially in the zero state, i.e., q(0) = 0. Is it possible to select\na nonzero input x(t) such that the motion of the state away from the\norigin 0 is confined entirely to motion along the direction of the first\neigenvector, i.e., is it possible to get q(t) = r1(t)v1 for all t and some\nr1(t)? If so, explain how; otherwise explain why not. (Don't try and\nget by with just intuition on this! -- start with the equations that\ngovern the motion of the state.)\n(b) (7 points) Consider a reachable and observable L-th order DT LTI\nstate-space system of the form\nq[n + 1] = Aq[n] + bx[n] ,\ny[n] = c T q[n] ,\nwhere A has distinct eigenvalues {λi}L and associated eigenvectors\n{vi}L\n1 . We know the transfer function of such a system has the form\np1\np2\npL\nH(z) =\n+\n+ · · · +\nz - λ1\nz - λ2\nz - λL\nfor some constants {pi}L\n1 . The corresponding unit sample response\nwith zero initial conditions, i.e., the output y[n] when x[n] = δ[n] and\nq[0] = 0, is\n\nh[n] = p1λn-1 + p2λn-1 + · · · + pLλn-1 u[n - 1] ,\nL\nwhere u[n] is the unit step function.\n(i) What does the reachability and observability of the system tell\nyou about the coefficients pi?\n(ii) Suppose the input is again the unit sample function, i.e., x[n] =\nδ[n], but that the initial state is picked to be some nonzero value\nq[0]. The output at time 0 then becomes y[0] = cT q[0]. What\nchoice of q[0] will ensure that the output for n ≥ 1 is\ny[n] = p2λn-1 + · · · + pLλn-1 ,\nL\ni.e., the same as the zero-state unit impulse response, but with\nthe first mode absent? Express and explain your answer in terms\nof the symbols introduced above.\n\n(c) (6 points) Suppose\n\ne At =\ncos t\n- sin t\nsin t\ncos t\n.\nDetermine\nd At\ne\nand\nA .\ndt\n(As a check, A will be quite simple, with each entry being 0, 1, or -1.)\n3(a) (6 points) 3rd-order reachable LTI CT system with q(0) = 0. Is\nit possible to get q(t) = r1(t)v1 for all t and some r1(t)? If so, explain how;\notherwise explain why not. (Don't try and get by with just intuition on this!\n-- start with the equations that govern the motion of the state.)\n\n3(b)(7 points) Reachable and observable LTI DT state-space system\n\nwith unit sample response h[n] = p1λn-1 +p2λn-1 +· · ·+pLλn-1 u[n-1] .\nL\n(i) What does the reachability and observability of the system tell you\nabout the coefficients pi?\nAnswer and explanation:\n(ii) Suppose x[n] = δ[n] again, but that the initial state is picked to be\nsome nonzero value q[0]. What choice of q[0] will ensure that the\noutput for n ≥ 1 is y[n] = p2λn-1 + · · · + pLλn-1 ?\nL\nCalculation and answer:\nq[0] =\n\ncos t\nsin t\nAt\nd At\n3(c) (6 points) Suppose e\n=\n. Determine dt e\n- sin t cos t\nand A . (As a check, A will be quite simple, with each entry being 0, 1, or\n-1.)\nd At\ne\n=\nA =\ndt\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.011 Signals, Systems and Inference, Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/cc8afbdedd3ab5d54e554ec7eba19e1d_MIT6_011S18quiz2.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Electrical Engineering and Computer Science\n6.011: Signals, Systems and Inference\nQUIZ 2\nQUESTION & ANSWER BOOKLET\nYour Full Name:\nRecitation Time :\no'clock\nThis quiz is closed book, but three sheets of notes (both sides) are\nallowed. Calculators and other electronic aids will not be necessary and are\nnot allowed.\nCheck that this QUESTION & ANSWER BOOKLET has pages num\nbered up to 8. The booklet contains spaces for all relevant work and rea\nsoning.\nNeat work and clear explanations count; show all relevant work\nand reasoning! You may want to first work things through on scratch\npaper and then neatly transfer to this booklet the work you would like us to\nlook at. Let us know if you need additional scratch paper. Only this booklet\nwill be considered in the grading; no additional answer or solution\nwritten elsewhere will be considered. Absolutely no exceptions!\nThere are 3 problems, each carrying the indicated number of\npoints, for a total of 30 points. (The points assigned to subparts may be\nchanged slightly when we get to grading.)\nProblem\nYour Score\n1 (6 points)\n2 (6 points)\n3 (18 points)\nTotal (30 points)\n\nSome of the questions on the following pages ask whether a\ngiven statement is True or False. In each such case, circle\nyour answer and give a sufficiently detailed and convincing\nexplanation of this answer. (For a False statement, a clear\ncounter-example might suffice.)\nYou will get no points at all for simply marking True\nor False, with no supporting reasoning!\n\nProblem 1 (6 points)\nThe random variable X has mean μX and variance σ2\nThe figure shows\nX .\na vector representation of this random variable and a vector representation\nof the \"random variable\" 1. Also shown are the components A and B of X,\nrespectively along 1 and orthogonal to 1. What are the lengths of X, A and\nB, expressed in terms of μX and σX ?\nLength of X =\nLength of A =\nLength of B =\n\nProblem 2 (6 points)\n(a) Claim: If X and Y are uncorrelated, then E[X2Y ] = E[X2]E[Y ].\nTRUE or\nFALSE ?\nExplanation:\n(b) Claim: If X and Y are uncorrelated, then E[Y |X] = E[Y ].\nTRUE or\nFALSE ?\nExplanation:\n\nProblem 3 (18 points)\nAll parts of this question refer to a WSS random process x[n] with mean\nvalue μx = 3 and autocovariance function\nCxx[m] = -0.6 δ[m + 2] + 1.2 δ[m] - 0.6 δ[m - 2] .\nAs usual, we will denote the fluctuation from the mean by xe[n] = x[n] - μx.\n3(a) (2 points) Make a fully labeled sketch of Cxx[m] as a function of m:\n3(b) (1 point) Claim: The (weak) law of large numbers would tell us that\nPL\nlimL→inf\nn=-L x[n] = μx if the values of x[·] at different times\n2L+1\nwere uncorrelated, but this identity holds true in this case as well, even\nthough the uncorrelatedness condition does not hold.\nTRUE or\nFALSE ?\nExplanation:\n\n3(c) (4 points) Write down the LMMSE estimator xbL[9] of x[9] that uses a\nmeasurement of x[7], and determine its mean square error (MSE).\nxbL[9] =\nMSE =\n3(d) (4 points) The fluctuation spectral density of the process x[·], i.e., the\nPSD of xe[·], can be written in the form\nDxx(ejΩ) = a + b cos(c Ω) .\nDetermine the constants a, b, c and (on the next page) sketch Dxx(ejΩ)\nfor |Ω| ≤ π. (Recall that cos(θ) = 1 ejθ + 1 e-jθ.)\na =\nb =\nc =\n\nSketch of Dxx(ejΩ):\n3(e) (1 point) Use your sketch in 3(d) to determine if the expected instan\ntaneous power of the fluctuations from the mean is concentrated at\nLOW or\nINTERMEDIATE or\nHIGH frequencies?\n(No explanation needed.)\n3(f) We know from class that the LMMSE estimator of x[n + 2] using\nmeasurements of x[n], x[n - 1] and x[n - 2] has the form\nxbL[n + 2] = μx + d xe[n] + e xe[n - 1] + f xe[n - 2]\nfor some constants d, e, f. The corresponding estimation error is\nxe[n + 2] - d xe[n] - e xe[n - 1] - f xe[n - 2] .\n(i) (2 points) Claim: e = 0, i.e., this LMMSE estimator of x[n + 2]\ndoes not actually make use of the measurement of x[n - 1].\nTRUE or\nFALSE ?\n(Hint: Check orthogonality of the error to xe[n - 1].)\nExplanation:\n(Continued on other side =⇒)\n\n(ii) (4 points) Claim: f = 0, i.e., this LMMSE estimator of x[n + 2]\ndoes not actually make use of the measurement of x[n - 2].\nTRUE or\nFALSE ?\n(Hint: Don't solve by intuition, do some calculation!)\nExplanation:\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 1 Introduction",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/b00929a765946777aef3a9bf7f160d0f_MIT6_011S18lec1.pdf",
      "content": "6.011: Signals, Systems &\nInference\nMIT, Spring 2018\n\nWeather prediction\nCourtesy of NOAA. This image is in the public domain.\n\nThe measurements\nrespiratory rate, tidal volume\nbedside\nend-tidal carbon dioxide\nmonitor\nelectrocardiogram\nintracranial\npressure\ncentral venous, right\nventricular, pulmonary\nartery pressures\nfluid drainage\ncerebrospinal\nfluid drainage\nsystemic arterial\npressure\nblood\noxygenation\ntemperature\nurine output\nbed with weight\nImaging studies, lab results, clinical assessment measuring capability\n(c) source unknown. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nThe interventions\nmedications\nnutrition\nfluids\n(infusion pumps)\nventilator\ncontrols\nhead-of-bed\nangle\n(c) source unknown. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nBlood pressure regulation\n(Guyton 1972)\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nBaroreflex\n\nBlood pressure regulation +\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nBlood pressure regulation +++\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nTime-based capnography!\nExhaled CO2 partial\npressure (PeCO2)\nvs. time\nNon-invasive\nEffort-independent\nPortable (point-of-\ncare)\nOridion. Microstream Bedside Capnography\nMonitoring - CS08653 Data Sheet, 2012.\n(c) Medtronic. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nMechanistic model for capnography\n\n... and the governing equations\nV (t)\nLV (t) + RV (t) +\n= ∆P\nC\n-pD(t) + pA\n\np D(t) =\nV (t) ,\nV (t) > 0\nVD\npD(t)\n\np D(t) =\nV (t) ,\nV (t) < 0\nVD\n\nWhat we'll (un)cover\n- Brief review of linear, time-invariant (LTI) system models in continuous\nand discrete time (CT and DT), and in the frequency domain. Determin\nistic autocorrelation. (Sections 1.1-1.3)\n- State-space models (mainly LTI). (Chapters 4, 5 and 6)\n- Brief review of random variables. (Chapter 7)\n- Estimation. (Chapter 8)\n- Stationary random processes in time and frequency domains. (Chapters\n10 and 11)\n\nWhat we'll (un)cover\n- Signal estimation. (Chapter 12)\n- Hypothesis testing. (Chapter 9)\n- Some intimations of machine learning: training and applying quadratic\ndiscriminators in feature space. (Based on Chapter 9)\n- Signal detection. (Chapter 13)\n- Hidden Markov models (briefly, as a counterpoint to LTI state-space mod\nels).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 10 Observers, State Feedback",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/374940065e1339ea37dd2aa3cf8c54bd_MIT6_011S18lec10.pdf",
      "content": "Observers, state feedback\n6.011, Spring 2018\nLec 10\n\nObservers\n\nSystem (\"plant\")\nw[n]\nx[n]\ny[n]\n1[n]\nq[n]\nA, b, cT, d\n+\n\ny[n]\n1[n]\nw[n]\n[n\n\n+\nA good model\nx[n] q\nbq[n]\nA, b, cT, d\nby[n]\n\nObserver configuration\nw[n]\nx[n]\nq[n]\nA, b, cT\ny[n]\n+\nZ[n]\nPlant\ny[n]\nq[n]\nA, b, cT\n[n]\nq\ny[n]\n-\n+\nObserver\nB\n\nObserver performance (with\nmeasurement noise)\n-6\n-5\n-4\n-3\n-2\n-1\nActual\nEstimate\nq1\nPendulum angle (q 1 )\nq1\n0.0\n0.5\n1.0\n1.5\n2.0\nTime (s)\n\n\"\nσ\n\"\nσ\n\"\nObserver for ship heading error\nDesired\nheading\nActual\nheading\nq1[n]\nx[n]\nRudder\nangle\n= Aq[n] + bx[n] .\nq[n + 1] =\nq1[n + 1]\nq2[n + 1]\n\"\n=\nσ\n↵\n\" q1[n]\nq2[n]\n\"\n+\n⇢\nσ\n\"\nx[n]\n\nState feedback\nx[n]\np[n]\ngT\n+\nA, b, cT\nq[n]\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 11 State Feedback, Observer-Based Feedback",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/aecb8bb1edba3f3d958bd9cbfe911889_MIT6_011S18lec11.pdf",
      "content": "State feedback, observer-based\nfeedback\n6.011, Spring 2018\nLec 11\n\nSystem (\"plant\")\nw[n]\nx[n]\ny[n]\n1[n]\nq[n]\nA, b, cT, d\n+\n\ny[n]\n1[n]\nw[n]\n[n\n\n+\nA good model\nx[n] q\nbq[n]\nA, b, cT, d\nby[n]\n\nObserver configuration\nw[n]\nx[n]\nq[n]\nA, b, cT\ny[n]\n+\nZ[n]\nPlant\ny[n]\nq[n]\nA, b, cT\n[n]\nq\ny[n]\n-\n+\nObserver\nB\n\nState feedback\nx[n]\np[n]\ngT\n+\nA, b, cT\nq[n]\n\nObserver-based controller\nw[n]\nq[n]\nA, b, cT\n+\ny[n]\np[n]\nx[n]\nζ[n]\nPlant\n+\ny[n]\ngT\nq[n]\nA, b, cT\n[n]\nq\ny[n]\n-\nObserver\n+\nB\n\nControl of inverted Pendulum\nObserver-based controller:\nState feedback control:\n0.1\n0.05\nTime (sec)\np(t) = 0, v(t) = 0, Z(t) = 0\nx(t) generated by\ndirect state feedback\nPendulum\nangle (q1)\n-0.05\n0.15\n0.5\nTime (sec)\nController\ninput (x)\n-0.5\n1.5\n0.5\nTime (sec)\nPendulum\nangle (q1)\n-0.5\n1.5\n0.5\nTime (sec)\nController\ninput (x)\n-0.5\n1.5\np(t) = 0, v(t) = 0, Z(t) = 0\nx(t) generated by\nobserver-based feedback\n/1 = -7, /2 = -18\n/1 = 14, /2 = 5\nActual\nEstimate\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 12 Probabilistic Models, Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/3e0a5c25cd7b5e1e0b1a328e13c82025_MIT6_011S18lec12.pdf",
      "content": "Probabilistic models,\nrandom variables\n6.011, Spring 2018\nLec 12\n\nSample space and events\nSample space °\nCollection of\noutcomes (event)\nA specific\noutcome c\n\nRandom variable\nReal line\n°\nX(c)\nc\n\nJoint pdf\n(c) The MathWorks, Inc. All rights reserved. This content is excluded from our Creative Commons license. For more information,\nsee https://ocw.mit.edu/help/faq-fair-use/\nMultivariateNormalGaussian-Matlab\n\nMarginal pdfs\nThis image is in the public domain. Source: Wikimedia Commons\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 13 Vector Picture for First- and Second-Order Statistics, MMSE and LMMSE Estimation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/bf865d4453d74062b775581570a30f43_MIT6_011S18lec13.pdf",
      "content": "Vector picture for first- and\nsecond-order statistics;\nMMSE and LMMSE estimation\n6.011, Spring 2018\nLec 13\n\nCovariance and correlation\nCovariance: eX,Y = E[(X - μX)(Y - μY )]\n=\nE[XY ]\n-μXμY\nCorrelation\n| {z }\nrX,Y\nShorthand notation: eXY , rXY\n\nβ\nβ\nσ\nγ\nδ\nσ\nσ\nσ\nσ σ\nCorrelation coefficient\nE↵ect of shifting and scaling: If V = ↵(X - β)\nthen μV = ↵(μX -β) , V = ↵σ X\nFor a shift- and scale-invariant measure:\nIf\nW = γ(Y -δ) then\nσV W = ↵γ σXY\nσXσY\nσXY\n⇢XY =\n= ⇢V W\n\nA geometric picture\nThink of X and Y as vectors, with inner product E[XY ]\nμX = E[X.1] : inner product of X and \"random variable\" 1\nE[X2] : squared length of X\nX = X - μX : vector di↵erence between X and \"random variable\" μX\ne\nOX : length of Xe\n\nGeometric interpretation of\ncorrelation coefficient\nY - mY\nsY\nu = cos -1r\nX - mX\nsX\n\nOrthogonality\nE[XY ] = 0\nCorrelation is 0, but not uncorrelated!\nUncorrelated = zero covariance, i.e., E[XY ] = E[X]E[Y ]\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 14 LMMSE Estimation, Orthogonality",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/e13347b4a1297aaacc9633efe1d573c7_MIT6_011S18lec14.pdf",
      "content": "LMMSE estimation, orthogonality\n6.011, Spring 2018\nLec 14\n\n-\n-\n-\n-\n-\nLMMSE estimator:\nfirst step (obtaining unbiasedness)\nLinear estimator: Y` = aX + b , with a and b picked to\nb\nminimize E[(Y - Yb`)2] over joint density of X and Y\nb\nFirst m\nin E[(Z - b)2]\n)\nb = μZ = μY - aμX\nThis yields an unbiased estimator: E[ Yb` ] = E[Y ] = μY\n)\nmin\na,b E[(|Y -{zaX}\nZ\n-b)2]\n\n-\n-\n-\n-\n-\n-\nσ\nσ\nσ\nσ\nLMMSE estimator:\nsecond step (solve reduced problem)\n(can be shown in di↵erent ways, e.g., by vector picture)\nNow min\na\nE[(Y -aX -b)2] = E[({|Y -{zμY}\nYe\ne\nX\n} -a{|X -{zμX}})2]\ni.e.\nmin\na\nE[(Ye -aXe)2]\n)\na = σY X\nσ2\nX\n= ⇢Y X\nσY\nσX\n\n-\n-\nσ\nσ\nσ\nσ\nLMMSE estimator as projection\n'\nY\n'\n'\nY - aX = Y - Yˆ/\ncos -1 (r\n)\nYX\naX\n'\n'\nX\nFor the optimum a,\n(Ye -aXe) ?\ne\nX\ni.\nX\ne.,\nE[(\nσ\nYe -aXe) e\nσ\n] = 0\n)\na = σ2\nX\n= ⇢Y X σX\n\nσ\nσ\nσ\nσ\nσ\nPutting it all together\nY` = y`(X) = μY + ⇢\n(X - μX )\nb\nb\nY` - μY\nor equivalently\nb\n= ⇢\nσY\nσX\nσY\nX -μX\nσX\nAlso, the resulting MMSE is\nY\nσ (1 -⇢2)\n\n-\n-\n-\n-\nOrthogonality relations\nUnbiasedness condition can be written as Y - Yb` ? 1\n(or ? to any constant)\nConversely, first + last above yield equations for a, b\nWe also know\n(Ye -aXe) ?\ne\nX\ne\nX\nor equivalently\nY -Yb` ?\nor equivalently\nY -Yb` ? X\n\n-\n-\n-\nExtension to multivariate case\nmin E[(Y\n{a0 + ⌃L\nj=1 aj Xj })2]\na0,...,aL\n|\n{z\n}\nYb`\nThis ensures unbiasedness of the estimator.\nFirst min\na0\nL\n)\na0 = μY -⌃j=1 ajμXj\nNow\nmin\na1,...,aL\nL\nX\nE[(Ye -⌃j=1 aj ej)2]\n\nApplying orthogonality gives the\n\"normal equations\"\nE\nh⇣\nYe - ⌃L\nj=1 aj Xej\n⌘\nXei\ni\n= 0\n2 CX1X1\nCX1X2\n· · ·\nCX1XL\n3 2 a1\n2 CX1Y\nCX2X1\nCX2X2\n· · ·\nCX2XL\na2\nCX2Y\n7 6\n7 6\n7 6\n5 4\n75 = 64\n...\n.\n.\n.\n...\n...\n...\n...\nCXLX1\nCXLX2\n· · ·\nCXLXL\naL\nCXLY\n(CXX) a = cXY\nMMSE: C2\nY - cY X(CXX)-1 cXY = C2\nY - cY X.a\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 15 Normal Equations, Random Processes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/0df9fff5e1e92239e0f755a893117b91_MIT6_011S18lec15.pdf",
      "content": "Normal equations\nRandom processes\n6.011, Spring 2018\nLec 15\n\nZillow (founded 2006)\n(c) Zillow. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\nZestimates\n(c) Zillow. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/\n\n-\n-\n-\nLMMSE for multivariate case\na\nFirst m\n0in\n) a0 = μY - ⌃j\nL\n=1 aj μXj\nThis ensures unbiasedness of the estimator.\nmin\na0,...,aL\nL\nE[(Y -{\n|\na0 + ⌃j=1 ajXj\n{z\n}\nYb`\n})2]\nNow\nmin\na1,...,aL\nL\nX\nE[(Ye -⌃j=1 aj ej)2]\n\nGeometric picture\n\nApplying orthogonality gives the\n\"normal equations\"\nE\nh⇣\nYe - ⌃L\nj=1 aj Xej\n⌘\nXei\ni\n= 0\n2 CX1X1\nCX1X2\n· · ·\nCX1XL\n3 2 a1\n2 CX1Y\nCX2X1\nCX2X2\n· · ·\nCX2XL\na2\nCX2Y\n7 6\n7 6\n7 6\n5 4\n75 = 64\n...\n.\n.\n.\n...\n...\n...\n...\nCXLX1\nCXLX2\n· · ·\nCXLXL\naL\nCXLY\n(CXX) a = cXY\nMMSE: C2\nY - cY X(CXX)-1 cXY = C2\nY - cY X.a\n\nEstimating mean vector and\ncovariance matrix from data\nGiven N independent measurements: Xi ,\ni = 1, · · · , N\nN\nEstimate of mean: μX = 1 X\nXi\nb\nN\nN\nX\nEstimate of covariance: Cb XX =\n(Xi ! μbX)(Xi ! μbX)T\nN ! 1 1\n\nRandom variable\nReal line\n°\nX(c)\nc\n\nRandom process\n°\nX(t; c)\nAmplitude\nc\nt1\nt\n\nSignal ensemble for outcomes a,b,c,d;\n& determination of RXX(t1,t2)\nt\nt\nt\nt\nX(t) = Xa(t)\nX(t) = Xb(t)\nX(t) = Xc(t)\nX(t) = Xd(t)\nt1\nt2\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 16 Wide-sense Stationary Processes, LTI Filtering of WSS Processes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/b6554551f64bd5f75ff3f4ff4f0a23f6_MIT6_011S18lec16.pdf",
      "content": "Wide-sense stationary processes;\nLTI filtering of WSS processes\n6.011, Spring 2018\nLec 16\n\nRandom process\n°\nX(t; c)\nAmplitude\nc\nt1\nt\n\nXa(t)\nXb(t)\nXc(t)\nXd(t)\nSignal ensemble for outcomes a,b,c,d;\n& determination of RXX(t1,t2)\nt\nt\nt\nt\nX(t) = xa(t)\nX(t) = xb(t)\nX(t) = xc(t)\nX(t) = xd(t)\nt1\nt2\n\nCourtesy of Alex Albright. Used with permission.\nWeather plot was generated with code adapted from Bradley Boehmke.\n\niid signal x[n], uniform in [-0.5,+0.5]\n\ny=h*x, with h[n] = δ[n] + δ[n-1]\n\ny=h*x, with h[n] = δ[n] - δ[n-1]\n\ny=h*x, with h[n] = (0.5)n u[n]\n\n|H| when h[n]=(0.5)n u[n]\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 17 LTI Filtering of WSS Processes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/aff4e7915b3de35e2481d62b444d71ee_MIT6_011S18lec17.pdf",
      "content": "LTI filtering of WSS processes\n6.011, Spring 2018\nLec 17\n\niid signal x[n], uniform in [-0.5,+0.5]\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n\nDTFT magnitude |X(ejΩ)|, 0 to 2π\n\n0.2\n0.4\n0.6\n0.8\nM 4, T 50\n0.2\n0.4\n0.6\n0.8\nM 4, T 200\nM\n16, T\nM\n16, T\nv/(2p)\nv/(2p)\nTransform magnitudes for 4\nrealizations of a ±1 Bernoulli process\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\n0.5\n0.5\n0.5\n0.5\nv/(2p)\nv/(2p)\nv/(2p)\nv/(2p)\n=\n=\n=\n=\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "6.011 Signals, Systems and Inference, Lecture 18 Power Spectral Density",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/e6d7ee6c7901199ddec42d9e75ffbe1b_MIT6_011S18lec18.pdf",
      "content": "Power Spectral Density (PSD)\n6.011, Spring 2018\nLec 18\n\niid signal x[n], uniform in [-0.5,+0.5]\n\ny[.] obtained by passing x[.] through\nresonant 2nd-order filter H(z),\npoles at ±0.95e^{jπ/3}\n\nExtracting the portion of x(t) in a\nspecified frequency band\nx(t)\nH(jv)\ny(t)\nH(jv)\n¢\n¢\n-v0\nv0\n\nδ\nσ\nQuestions (warm-up for Quiz 2!)\nWSS process x[·] with\nWhat is the largest magnitude ⇢ can have?\nWSS process x(·) with mean μx and PSD Sxx(j!).\nWhat is its FSD?\nZero-mean WSS process x(·) with\nSxx(j!) = 1 + !2\nWhat are μy and Syy(j!)?\nCxx[m] = ⇢δ[m -1] + δ[m] + ⇢δ[m + 1] .\nand let y(t) = Z +x(t), where Z has zero mean, variance σ2, and is uncorrelated\nwith x(·).\n\n0.2\n0.4\n0.6\n0.8\nM = 4, T = 50\n0.2\n0.4\n0.6\n0.8\nM = 4, T = 200\nM = 16, T = 50\nM = 16, T = 200\nv/(2p)\nv/(2p)\n-\n-\nPeriodograms\n(e.g., a unit-intensity \"white\" process)\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\n0.5\n0.5\n0.5\n0.5\nv/(2p)\nv/(2p)\nv/(2p)\nv/(2p)\n|XT (j!)|2\nPeriodogram =\n2T\n|XN (ej⌦)|2\nPeriodogram =\n2N + 1\nCT case:\nXT (j!) $ x(t) windowed to [-T, T]\nDT case:\nXN(ej⌦) $ x[n] windowed to [-N, N]\n\nM\n16, T\nM\n16, T\nv/(2p)\nv/(2p)\nPeriodogram averaging (illustrating\nthe Einstein-Wiener-Khinchin theorem)\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\n0.5\n0.5\n0.5\n0.5\nv/(2p)\nv/(2p)\nv/(2p)\nv/(2p)\nM = 4, T = 50\nM = 4, T = 200\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n\nPeriodogram averaging (illustrating\nthe Einstein-Wiener-Khinchin theorem)\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\nM = 1, T = 50\n0.5\n0.5\n0.5\n0.5\nv/(2p)\nv/(2p)\nv/(2p)\nv/(2p)\nM = 4, T = 50\nM = 4, T = 200\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\nv/(2p)\nM = 16, T = 50\nv/(2p)\nv/(2p)\nM = 16, T = 200\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\nv/(2p)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "6.011 Signals, Systems and Inference, Bandlimited Signals Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/604ae31ec42d8163bc62283748f330a4_MIT6_011S18band-sign.pdf",
      "content": "What you need to know about bandlimited signals for 6.011\n1A continuous-time (CT) signal xc(t) is termed bandlimited to ωc rad/s or fc = ωc/(2π)\nHz if its (CT) Fourier transform Xc(jω), defined by\nZ inf\n(1)\nXc(jω) =\nxc(t)e -jωtdt ,\n-inf\nis 0 for |ω| ≥ ωc(< inf) (we normally quote the smallest ωc for which this is true). In\nother words, the signal has no frequency content at frequencies fc Hz or higher, so it varies\n\"smoothly\" (or we could be more explicit and say \"fc-smoothly\").\nFor such a signal we can write the inverse CTFT as:\n1 Z ωc\n(2)\nxc(t) =\nXc(jω)ejωtdω .\n2π\n-ωc\nNote the limits on the integral!\nNow consider the discrete-time (DT) signal xd[n] obtained by sampling xc(t) at intervals\nof T seconds. We label this sampling operation as continuous-to-discrete (C/D) trans\nformation (it's the most common kind of C/D transformation, and the only kind we'll\nconsider). We can now write\n1 Z ωc\n(3)\nxd[n] = xc(nT ) =\nXc(jω)ejωnT dω .\n2π\n-ωc\nAll that's happened here is we've written t = nT in the exponent in Eq. (2). Making the\nchange of variable Ω = ωT rad, we can rewrite Eq. (3) as\nZ Ωc\n(4)\nxd[n] = 1\n2π\n-Ωc\nXc(jω)\nT\nejΩndΩ ,\nwhere Ωc = ωcT .\nCompare the expression in Eq. (4) with the inverse DTFT formula that expresses the\nDT signal xd[n] in terms of its DTFT Xd(ejΩ):\nZ π\n(5)\nxd[n] = 1\nXd(ejΩ)ejΩndΩ ,\n2π\n-π\nwhere\ninf\nX\n(6)\nXd(ejΩ) =\nxd[n]e -jΩn .\nn=-inf\nWe see from comparing Eqs. (4) and (5) that if Ωc ≤ π, i.e., if ωcT ≤ π, i.e., if the\nsampling frequency fs = 1/T satisfies\nωc\n(7)\nfs =\n≥\n≥ 2fc ,\nT\nπ\n1George Verghese, 9 Feb 2018, MIT\n\nthen (because the Fourier transform of a signal is unique)\nXc(jω)\n(8)\nXd(ejΩ) =\nT\nfor angular frequencies Ω in the interval [-π, π], which we might refer to as the canonical\nor principal interval (and repeats periodically with period 2π outside that, as any DTFT\nmust). This is Eq. (1.80) in the text.\nThus, if the sampling frequency is greater than twice the highest frequency present in\nthe signal xc(t), we can reconstruct the CTFT of this CT signal, and hence the CT signal\nitself, from the DTFT of the sampled DT signal, and hence from the sampled signal itself.\nThis is Nyquist's sampling theorem, and the relationship in Eq. (8) is key. This\nequation shows that for sampling at or above the Nyquist rate, we get the DTFT of the\nsampled sequence by simply scaling the CTFT amplitude by the factor 1/T and scaling\nthe frequency axis by the factor T (to go from ω to Ω = ωT ). And to go in the other\ndirection, we simply perform the opposite scaling of amplitude and frequency axes.\nAlthough many of the problems in Chapter 1 deal with cases where there is aliasing,\ni.e., where the sampling frequency does not satisfy the condition in Eq. (7), we shall not\nworry this semester about such refinements.\nA small exercise (for you to do before reading further): Show that why Eq. (2) can\nbe rewritten as\nZ ωc\n\ninf\n\nX\n-jΩn\n(9)\nxc(t) = 1\nT\nxd[n]e\nejωtdω .\n2π\n-ωc\nn=-inf\nAssume now that T is set to the largest possible value that avoids aliasing, so T = π/ωc,\ni.e., we are sampling at the Nyquist rate. Interchanging summation and integration in the\nabove expression, evaluating the integral on each term, and verifying that\n\nπ\nZ ωc\nsin\n(t - nT )\n2π\nT\njω(t-nT )dω =\n(10)\ne\nπ\n,\nT\n(t - nT )\n-ωc\nT\nwe see that Eq. (9) yields a formula expressing the bandlimited signal xc(t) directly in\nterms of its samples:\n\nπ\ninf\nX\nsin\n(t - nT )\nT\n(11)\nxc(t) =\nxd[n]\nπ\n.\n(t - nT )\nT\nTo tease apart this expression, first look at the n = 0 term, which is xd[0] sin(πt/T )/(πt/T ).\nThe unit-height sinc function sin(πt/T )/(πt/T ) takes the value 1 at t = 0 and the value 0\nat all other sampling instants, i.e., at all nonzero integer multiples of T , varying smoothly\nin between these points. Note also that the transform of this sinc function is constant at\nthe value T for ω in (-ωc, ωc), and is zero outside this; it has no frequencies higher than fc\nHz (and this is the sense in which it varies \"smoothly\"). Multiplying the unit-height sinc\nn=-inf\n\nby xd[0] scales it to a sinc function that takes the value xd[0] at t = 0 but is still 0 at all\nother sampling instants, and varies smoothly in between.\nNow we do the same thing for the samples at all the other sampling instants: for a general\nsampling instant nT , we center a sinc function on that sampling time, which causes the\ntime argument of the sinc to change from t to t - nT , and then scale this shifted sinc\nby xd[n]. The combination of this infinite set of scaled and shifted sinc functions, each\nbandlimited to (-ωc, ωc), is what creates the expression in Eq. (11), which we refer to\nas the ideal bandlimited interpolation of the samples xd[n], to create or reconstruct the\nbandlimited signal xc(t). The operation of generating a CT signal from a DT one in this\nfashion is referred to as ideal discrete-to-continuous (D/C) conversion.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.011 Signals, Systems and Inference, Linear Algebra Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/f30ecb1d1fb90db356a095e00764b319_MIT6_011S18lin-alg.pdf",
      "content": "Notes on Linear Algebra for a Tutorial\nP. L. Hagelstein\nMar. 7, 2017\nI. Introduction\nWe make use of some linear algebra concepts in 6.011 with the thought that most students\nhave been exposed to relevant concepts previously, perhaps in high school, and perhaps in 18.03\nor equivalent. However, it may be that not everyone has seen linear algebra before, and it may be\nthat some students may be rusty. So, this provides motivation to develop some notes that cover\nsome of the basic notions, hopefully in a simple way. In what follows we discuss some of the ideas,\nnotation, and results that may be useful.\nII. Linear equations in terms of a matrix and vectors\nPerhaps the place to start is with a set of linear equations of the form\n2x1 + 3x2 = 5\nx1 -x2 = 0\n(1)\nProbably we can solve these equations pretty quickly for x1 and for x2. However, given that linear\nequations come up often in mathematics, engineering and science, it would be good to have a\nsystematic way of thinking about them. For example, we might consider these two equations as a\nparticular example of a more general set of coupled equations of the form\nA11x1 + A12x2 = b1\nA21x1 + A22x2 = b2\n(2)\nThere may be more complicated problems of this kind that involve more unknowns, such as\nA11x1 + A12x2 + A13x3 = b1\nA21x1 + A22x2 + A23x3 = b2\nA31x1 + A32x2 + A33x3 = b3\n(3)\nProbably matrices were developed (long ago) originally to help with dealing with this kind of\nproblem. For two linear equations we can make use of matrices and vectors to write\n\"\n# \"\n#\n\"\n#\n\"\n#\nA11\nA12\nx1\nA11x1 + A12x2\nb1\n=\n=\n(4)\nA21\nA22\nx2\nA21x1 + A22x2\nb2\n\nwhere matrix multiplication of a vector is indicated in the intermediate step. A nice thing about\nthis approach is that it allows us to write lots of coupled linear equations simply making use of the\nsubscripts as\nX\nAijxj = bj\n(5)\nk\nIn terms of matrix and vector notation this is written as\nAx = b\n(6)\nIn the case of three unknowns we can write\n3 2\nA11\nA12\nA13\nx1\nA11x1 + A12x2 + A13x3\nb1\n7 6\n4 A21\nA22\nA23 5 4 x2 5 =\n4 A21x1 + A22x2 + A23x3 5 =\n4 b2 5\n(7)\nA31\nA32\nA33\nx3\nA31x1 + A32x2 + A33x3\nb3\nThis can be generalized to as many unknowns as we like. If we have the same number of equations\nas unknowns, then the matrix A will be a square matrix.\nThere are written records that this idea was known in antiquity; where a mathematical text\nwritten in the second century BCE in China discusses linear equations, solution by Gaussian elim\nination, and a treatment equivalent to the use of matrices and vectors as above.\nIII. Matrix multiplication\nOnce we have matrices, a natural question concerns what are the associated properties. This\nopens up the door to a potentially vast field of study, of which in these notes we will only touch\non a few simple ones that we need for class. To proceed, we will need to know how to multiply\nmatrices. We consider the multiplication of two matrices to make a third\nAB = C\n(8)\nin the case of square matrices. In the case of 2 × 2 matrices this can be written out as\n\"\n# \"\n#\n\"\n#\n\"\n#\nA11\nA12\nB11\nB12\nA11B11 + A12B21\nA11B12 + A12B22\nC11\nC12\n=\n=\n(9)\nA21\nA22\nB21\nB22\nA21B11 + A22B21\nA21B12 + A22B22\nC21\nC22\nWe can also make use of the subscripts to write\nX\nCij =\nAikBkj\n(10)\nk\nAt this point it seems useful to consider the notion of an identity matrix, which in the 2 × 2\ncase can be written as\n\"\n#\nI =\n(11)\n\nWe can evaluate the product\n\"\n# \"\n#\n\"\n#\nA11\nA12\nA11\nA12\nIA =\n=\n= A\n(12)\nA21\nA22\nA21\nA22\nMultiplication of a matrix by the identity matrix simply gives the matrix as a result. We can\nconstruct bigger (square) identity matrices similarly with 1 for diagonal elements and 0 for o\ndiagonal elements, which in general which satisfy\nIA = A\n(13)\nThe identity matrix multiplied by a vector gives the same vector back as well\nIx = x\n(14)\nIV. Eigenvectors and characteristic equations for eigenvalues\nIt is possible to find vectors which are able to produce scaled versions of themselves when\nmultiplied by a matrix. For example, notice that\n\"\n# \"\n#\n\"\n#\n\"\n#\n=\n= 3\n(15)\n-2\nThis is a special case of\nAv = λv\n(16)\nwhere λ is a scalar and v is a vector.\nWe are interested in figuring out how to find the eigenvalues and eigenvectors. We might start\nwith finding and equation for the eigenvalue λ in the simple case of a 2 × 2 matrix, in which case\nwe can write\n\"\n# \"\n#\n\"\n#\nA11\nA12\nv1\nv1\n= λ\n(17)\nA21\nA22\nv2\nv2\nThis is equivalent to the linear equations\nA11v1 + A12v2 = λv1\nA21v1 + A22v2 = λv2\n(18)\nHere the unknowns include v1, v2 and λ. It is possible to eliminate v1 and v2 to get an equation\nfor λ. To do this we can first write\n(A11 -λ)v1 + A12v2 = 0\nA21v1 + (A22 -λ)v2 = 0\n(19)\n\nand then eliminate v1 by forming\nA21(A11 -λ)v1 + A21A12v2 = 0\n(A11 -λ)A21v1 + (A11 -λ)(A22 -λ)v2 = 0\n(20)\nand then subtracting to obtain\n\n(A11 -λ)(A22 -λ) -A21A12 v2 = 0\n(21)\nIn the general case v2 might not be zero, so we end up with a constraint on λ\n(A11 -λ)(A22 -λ) -A21A12 = 0\n(22)\nThis is the characteristic equation in the case of a 2 × 2 matrix that the eigenvalues must satisfy\nin order for the eigenvalue relation to be consistent.\nWe can repeat this kind of calculation in the case of a general 3 × 3 matrix, and get a more\ncomplicated characteristic equation. The result can be written as\nλ3 -(A11 + A22 + A33)λ2 + (A12A21 + A13A31 + A23A32 -A11A22 -A11A33 -A22A33)λ\n+A11A22A33 + A13A23A31 + A13A32A21 -A11A23A32 -A22A13A31 -A33A12A21 = 0\n(23)\nThere is no diyculty continuing this kind of calculation for bigger square matrices; however, it is\nclear that we will generate large numbers of terms. Instead of repeating this kind of elimination by\nhand each time, we would like to automate the calculation.\nV. Characteristic equation in terms of a determinant\nToday we recognize the development of the characteristic equation as deriving from the deter\nminantal equation\ndet(λI -A) = 0\n(24)\nThe notion of the determinant originated in connection with the solution of sets of linear equations,\nwhere a solution to Ax = b can be obtained if det(A) 6\n0. In 1683 Seki in Japan, and independently\n=\nLiebnitz in Europe, evaluated the determinant for the lowest square matrices. There were earlier\ncalculations for the 2×2 and 3×3 matrices that can retrospectively be interpreted as involving the\nevaluation of the determinant. It is likely that the Chinese mathematicians understood the need\nfor the determinant to be nonzero to obtain a solution to a linear system of equations in antiquity.\nWe can think of the determinant as being defined as the result of the calculation above that\neliminates the Aij matrix elements in the eigenvalue calculation. This calculation has been well\nstudied over the years, with the result that specific formulas were derived long ago for \"small\"\nmatrices. You might have seen a more systematic approach that makes use of cofactors and minors\n(due to Laplace). Expansion formulas have been developed that describe the general case as well;\none such formula can be written as\n\nX\nX\ndet(A) =\n· · ·\nεi1···in A1,i1A2,i2 · · · An,in\n(25)\ni1\nin\nwhere εi1···in is a Levi-Civita symbol which has a value of 0, +1 or -1 according to a test of the\npermutation of the indices.\nFor class this term we will be able to survive for the most part with determinantal formulas for\nthe 2 × 2 case\n\"\n#\nA11\nA12\ndet\n= A11A22 -A12A21\n(26)\nA21\nA22\nand for the 3 × 3 case\nA11\nA12\nA13\ndet 4 A21\nA22\nA23 5 =\nA31\nA32\nA33\nA11A22A33 + A12A23A31 + A13A21A32 -A11A23A32 -A12A21A33 -A13A22A31\n(27)\nVI. Matrix inverse\nIf we have a single linear equation of the form\nAx = b\n(28)\nthen if A is not zero we can solve it by writing\nA-1\nx =\nb\n(29)\nIf we have a matrix and vector equation written as\nAx = b\n(30)\nthen it seems to be reasonable that we might be able to write\nA-1\nx =\nb\n(31)\nFor this to work we need to be able to compute the inverse of a matrix, and it would be good to\nknow for what matrices this can work. For example, we know that in the scalar case above that if\nA is zero, then we are not going to be able to use the inverse. The same idea applies in the case of\na matrix.\nIf we consider the case of a 2 × 2 matrix, we might start with\n\"\n# \"\n#\n\"\n#\nA11\nA12\nx1\nb1\n=\n(32)\nA21\nA22\nx2\nb2\nWe know that we can write these as linear equations of the form\n\nA11x1 + A12x2 = b1\nA21x1 + A22x2 = b2\n(33)\nWe can solve these by multiplying by a factor and subtracting; for example, to eliminate x1 we\nmultiply by factors to get\nA21A11x1 + A21A12x2 = A21b1\nA11A21x1 + A11A22x2 = A11b2\n(34)\nand then subtract to obtain\n(A11A22 -A12A21)x2 = A11b2 -A21b1\n(35)\nIf A11A22 -A12A21 is not zero, then we can divide to obtain\nA11b2 -A21b1\nx2 =\n(36)\nA11A22 -A12A21\nFor the other case we can write\nA22A11x1 + A22A12x2 = A22b1\nA12A21x1 + A12A22x2 = A12b2\n(37)\nand subtract to obtain\n(A22A11 -A12A21)x1 = A22b1 -A12b2\n(38)\nOnce again if A22A11 -A12A21 is not zero, then we can solve to obtain\nA22b1 -A12b2\nx1 =\n(39)\nA22A11 -A12A21\nThese solutions can be combined to write\n\"\n#\n\"\n# \"\n#\nx1\nx2\n=\nA22A11 -A12A21\nA22\n-A21\n-A12\nA11\nb1\nb2\n(40)\nWe recognize that for this approach to work we require\nA22A11 -A12A21 6= 0\n(41)\nBut we recognize that this can be written using the definition of the determinant for the 2 × 2 case\nas\ndet(A) 6\n=\n(42)\nWe will not be able to obtain a unique solution for a linear system if the determinant is zero, which\nis consistent with not being able to construct an inverse for A if the determinant is zero.\nFrom the calculation above we conclude that the inverse for a 2 × 2 matrix is\n\n\"\n#\nA-1\nA22\n-A12\n=\n(43)\ndet(A)\n-A21\nA11\nAlthough we will not need it this term, for the inverse of a 3 ×3 matrix, as long as the determinant\nis not zero, we can write\nA22A33 -A23A32\nA13A32 -A12A33\nA12A23 -A13A22\nA-1 =\n4 A23A31 -A21A33\nA11A33 -A13A31\nA13A21 -A11A23 5\n(44)\ndet(A)\nA21A32 -A22A31\nA12A31 -A11A32\nA11A22 -A12A21\nFor bigger square matrices it is possible to develop explicit formulas for the inverse as long as the\ndeterminant is not zero.\nVII. Matrix of the eigenvectors\nIt is possible to construct a matrix for a 2 × 2 matrix A from the eigenvectors v1 and v2\naccording to\nV = [ v1 v2 ]\n(45)\nIf we write the eigenvectors in terms of their elements\n\"\n#\n\"\n#\nv1\nv1\nv1 =\nv2 =\n(46)\nv2\nv2\nthen the V matrix might be written as\n\" \"\n#\n\"\n#\n#\nv1\nv1\nV =\n(47)\nv2\nv2\nTo proceed we might adopt for this a notation of the form\n\"\n#\n(v1)1\n(v1)2\nV =\n(48)\n(v2)1\n(v2)2\nwhere (vi)k indicates the ith element of the vector of the kth eigenfunction.\nSince this matrix is made up of the eigenvectors of A, and since each of the eigenvectors satisfies\nthe eigenvalue equation\nAvk = λkvk\n(49)\nthen we can write\n\"\n#\nλ1(v1)1\nλ2(v1)2\nAV = A[ v1 v2 ] = [ λ1v1 λ2v2 ] =\n(50)\nλ1(v2)1\nλ2(v2)2\nFor the argument that follows, we will need to define a diagonal matrix made up of the eigenvalues\naccording to\n\nV\n\"\n#\nλ1\nΛ =\n(51)\nλ2\nIf we multiply V and Λ, we get the same resulting matrix as we got when we multiplied A times\n\"\n# \"\n#\n\"\n#\nVΛ =\n(v1)1\n(v2)1\n(v1)2\n(v2)2\nλ1\nλ2\n=\nλ1(v1)1\nλ1(v2)1\nλ2(v1)2\nλ2(v2)2\n(52)\nConsequently, we can write\nAV = VΛ\n(53)\nAlthough we found this to be true for the 2 × 2 case, it works for the other cases as well.\nOne reason that this is interesting is that it allows us to (formally) diagonalize the A matrix\nby writing\nV-1\nΛ =\nAV\n(54)\nwhich works as long as the inverse matrix exists. In 6.011 we focus almost exclusively on matrices\nfor which the eigenvalues are distinct, in which case the eigenvectors are independent, which is the\nconditions that the inverse matrix exists.\nVIII. Application: Diagonalization of a state space model\nOne application of the matrix of eigenvectors is for the diagonalization of a state space model.\nSuppose that we start with a state space model of the form\nd q(t) = Aq(t) + bx(t)\ndt\nT\ny(t) = c q(t) + dx(t)\n(55)\nWe would like to expand the state vector in terms of the eigenvectors according to\nq(t) = v1r1(t) + v2r2(t)\n(56)\nBased on the discussion above, we know that we can write this in terms of the matrix of eigenvectors\naccording to\nq(t) = Vr(t)\n(57)\nIf we plug this into the evolution equation for the state vector q(t), we obtain\nd Vr(t) = AVr(t) + bx(t)\n(58)\ndt\nSince the matrix of eigenvectors does not depend on time, this can be written as\n\nd\nV\nr(t) = AVr(t) + bx(t)\n(59)\ndt\nIf the eigenvalues are independent, then the V matrix has an inverse, which leads to\nV-1\nd r(t) =\nAVr(t) + V-1bx(t)\n(60)\ndt\nWe recall that\nV-1AV = Λ\n(61)\nIf the b vector is written in terms of the eigenvectors\nb = v1 1 + v2 2 = V\n(62)\nthen we can express as\nV-1\n=\nb\n(63)\nWe end up with the state evolution equation written in terms of the mode amplitudes as\nd r(t) = Λr(t) + x(t)\n(64)\ndt\nThis is equivalent to\nd r1(t) = λ1r1(t) + 1x(t)\ndt\nd r2(t) = λ2r2(t) + 2x(t)\n(65)\ndt\nThere is no coupling between the di erent modes now, which makes it much easier to solve.\nFor the state space output equation we can write\nT\ny(t) = c Vr(t) + dx(t)\n(66)\nWe can evaluate the product\nT\nT\nc T V = c T [ v1 v2 ] = [ c v1 c v2 ] = [ 1 2 ] = T\n(67)\nWe end up with\nT\ny(t) =\nr(t) + dx(t)\n(68)\nwhich is equivalent to\ny(t) = 1r1(t) + 2r2(t) + dx(t)\n(69)\nWe now have the output in terms of the mode amplitudes.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.011 Signals, Systems and Inference\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    }
  ]
}