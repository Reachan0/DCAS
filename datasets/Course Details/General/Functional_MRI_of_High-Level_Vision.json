{
  "course_name": "Functional MRI of High-Level Vision",
  "course_description": "We are now at an unprecedented point in the field of neuroscience: We can watch the human brain in action as it sees, thinks, decides, reads, and remembers. Functional magnetic resonance imaging (fMRI) is the only method that enables us to monitor local neural activity in the normal human brain in a noninvasive fashion and with good spatial resolution. A large number of far-reaching and fundamental questions about the human mind and brain can now be answered using straightforward applications of this technology. This is particularly true in the area of high-level vision, the study of how we interpret and use visual information including object recognition, mental imagery, visual attention, perceptual awareness, visually guided action, and visual memory.\nThe goals of this course are to help students become savvy and critical readers of the current neuroimaging literature, to understand the strengths and weaknesses of the technique, and to design their own cutting-edge, theoretically motivated studies. Students will read, present to the class, and critique recently published neuroimaging articles, as well as write detailed proposals for experiments of their own. Lectures will cover the theoretical background on some of the major areas in high-level vision, as well as an overview of what fMRI has taught us and can in future teach us about each of these topics. Lectures and discussions will also cover fMRI methods and experimental design. A prior course in statistics and at least one course in perception or cognition are required.",
  "topics": [
    "Health and Medicine",
    "Biomedical Instrumentation",
    "Biomedical Signal and Image Processing",
    "Medical Imaging",
    "Science",
    "Biology",
    "Neuroscience",
    "Health and Medicine",
    "Biomedical Instrumentation",
    "Biomedical Signal and Image Processing",
    "Medical Imaging",
    "Science",
    "Biology",
    "Neuroscience"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1 session / week, 3 hours / session\n\nOverview\n\nWe are now at an unprecedented point in the field of neuroscience: We can watch the human brain in action as it sees, thinks, decides, reads, and remembers. Functional magnetic resonance imaging (fMRI) is the only method that enables us to monitor local neural activity in the normal human brain in a noninvasive fashion and with good spatial resolution. A large number of far-reaching and fundamental questions about the human mind and brain can now be answered using straightforward applications of this technology. This is particularly true in the area of high-level vision, the study of how we interpret and use visual information including object recognition, mental imagery, visual attention, perceptual awareness, visually guided action, and visual memory.\n\nThe goals of this course are to help students become savvy and critical readers of the current neuroimaging literature, to understand the strengths and weaknesses of the technique, and to design their own cutting-edge, theoretically motivated studies. Students will read, present to the class, and critique recently published neuroimaging articles, as well as write detailed proposals for experiments of their own. Lectures will cover the theoretical background on some of the major areas in high-level vision, as well as an overview of what fMRI has taught us and can in future teach us about each of these topics. Lectures and discussions will also cover fMRI methods and experimental design. A prior course in statistics and at least one course in perception or cognition are required.\n\nRequired Reading\n\nHuettel, Scott A., Allen W. Song, and Gregory McCarthy.\nFunctional Magnetic Resonance Imaging\n. 1st ed. Sunderland, MA: Sinauer Associates, 2004. ISBN: 9780878932887.\n\nOther Resources\n\nFarah, Martha J.\nVisual Agnosia\n. 2nd ed. Cambridge, MA: MIT Press, 2004. ISBN: 9780262062381.\n\n------.\nThe Cognitive Neuroscience of Vision\n. Malden, MA: Blackwell Publishers, 2000. ISBN: 9780631214038.\n\nInformation About fMRI Methods, fMRI Physics, and Neuroanatomy\n\nUseful Slides on fMRI Physics and Methods\n\nAssignments and Exams\n\nAssignments include a written essay, a paper critique, two class presentations, a term paper and class participation. All written assignments are due at the beginning of class on the day they are due. There is a midterm.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nWritten essay\n\n10%\n\nPaper critique\n\n10%\n\nMidterm\n\n10%\n\nTwo class presentations (15% each)\n\n30%\n\nTerm paper\n\n30%\n\nClass participation\n\n10%\n\nCalendar\n\nWEEK #\n\nTOPICS\n\nKEY DATES\n\nIntroduction to fMRI and high-level vision\n\nFunctional organization of the ventral visual pathway\n\nControversies concerning this organization\n\nFMRI design/methods\n\nHow to critique an fMRI paper\n\n2 - 4 page essay due at the beginning of class\n\n(Essay details: Read Talbot, Margaret. \"\nDuped\n.\"\nNew Yorker\n, July 2007 and then address whether and how you could test if fMRI can be used as a lie detector in the real world, what conditions would you need to test, could such an experiment actually be run, and how might you do it?)\n\nVisual recognition, object shape, and the lateral occipital complex (LOC)\n\nHow to do a presentation\n\nBasic neuroanatomy of the visual system\n\nWritten critique of an fMRI paper due at the beginning of class\n\nScene perception and the PPA\n\nClass presentations\n\nFace processing and the FFA\n\nClass presentations\n\nVisual attention\n\nClass presentations\n\nVisual awareness\n\nClass presentations\n\nThe dorsal/parietal pathway: visual attention, visually guided action and number including visually guided action, number, attention, response selection, etc.\n\nTerm paper outline due in class\n\nIn second half of class (if we don't get to this topic earlier): classification methods, brain reading\n\nMidterm\n\nDevelopment and effects of experience on visual and extrastriate cortex\n\nStudent presentations\n\nProject presentations and discussion\n\nStudent presentations (cont.)\n\nProject presentations and discussion\n\nIn class we will have one or more guest lectures, e.g.:\n\nEvelina Fedorenko, on Neuroimaging of Language\n\nRebecca Saxe, on Neuroimaging of Theory of Mind\n\nBasic MR Physics (if you are interested)\n\nFinal term papers due",
  "files": [
    {
      "category": "Resource",
      "title": "Differential Activation by Convex and Concave Stimuli in Human Parahippocampal Cortex and Lateral Occipital Cortex",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/907618c496ac1e0fcd598a42e9fd16c6_difrntalactivton.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nDifferential Activation\nby Convex and Concave Stimuli\nin Human Parahippocampal Cortex\nand Lateral Occipital Cortex\n\nMIT Student\n9.71 Experiment Proposal\nDecember 9, 2004\n\nAbstract\nPrevious research suggests the involvement of several ventral temporal brain areas in the\nprocessing of specific categories of stimuli. One such region is the Parahippocampal\nPlace Area (PPA), an area within the posterior parahippocampal cortex that responds\nselectively to visual stimuli conveying information about the layout of local space.\nAnother region, the lateral occipital complex (LOC), responds preferentially to objects\nand is believed to play a critical role in the processing of object shape. In particular, the\nLOC seems to be involved in the processing 3-dimensional aspects of objects, rather than\ntheir 2-dimensional contours. These results beg the question: if the PPA responds\nselectively to the 3D layout of scenes and the LOC responds selectively to the 3D layout\nof objects, what is the difference between scenes and objects such that objects do not\nrecruit preferential activation in the PPA and scenes do not produce preferential\nactivation in the LOC? The current experiment investigates the possibility that the PPA\nis selectively activated by the concavity of scenes, while the LOC is selectively activated\nby the convexity of objects. Using stereo to convey the depth information of convex and\nconcave dot arrays, this study tests the hypothesis that the PPA responds more strongly to\nconcave than convex stimuli, and that the LOC responds more strongly to convex than\nconcave stimuli.\n\nBackground and Motivation\nThe PPA\n\nThe parahippocampal place area (PPA) is a region within the parahippocampal\ncortex that has been shown in functional magnetic resonance imaging (fMRI) studies to\nrespond selectively to photographs of scenes such as rooms, landscapes, and city streets\ncompared to photographs of objects, faces, houses, or other kinds of visual stimuli\n(Epstein & Kanwisher, 1998). The critical factor for this activation appears to be the\npresence in the stimulus of information about the layout of local space. Evidence in\nsupport of this claim includes the finding that the response in the PPA to scenes with\nspatial layout but no discrete objects was found to be as strong as the response to scenes\n\ncontaining multiple objects. The response to objectless spatial layouts is also more than\ntwice as strong as the response to arrays of multiple objects without three-dimensional\nspatial context.\n\nThe PPA also responds much more strongly to \"scenes\" made out of Lego blocks\n(whose geometric structure is similar to a room or street scene, yet is not a real place)\nthan it does to objects made out of Lego materials (Epstein, Harris, Stanley, &\nKanwisher, 1999). We can conclude from this that the PPA is responsive to a specific\nkind of geometric organization in the stimulus--even if the stimulus does not depict a real\nplace in the world. In particular, it responds strongly to depictions of surfaces that in\nsome sense \"enclose\" the observer and define a space within which one can act. One\nfeature that all such surfaces have in common is concavity. These results reinforce\nbehavioral data from studies on rats (Cheng, 1986; Gallistel, 1990; Margules & Gallistel,\n1988) and human infants (Hermer & Spelke, 1994, 1996; Hermer-Vazquez,\nSpelke,&Katsnelson, 1999).\n\nSeveral subsequent PPA studies have implicated the PPA in processes involved in\nnavigation, especially the encoding of new perceptual information about the appearance\nand layout of scenes. An experiment by Epstein & Kanwisher in 2000 found that PPA\nactivity is not affected by the subjects' familiarity with the place depicted (e.g. a familiar\ncollege campus produces as much activation as an unfamiliar college campus). However,\nwithin a scan, novel scenes do produce greater activation than repeated scenes. This\nresult cannot be explained solely on the basis of novelty, however, because novel faces\ndid not generate a greater response than repeated faces.\n\nOther evidence in support of the PPA as a topographical information encoding\nregion includes the results of lesions to this area of the brain. Often, damage to the PPA\nand surrounding regions results in an inability to use salient topographical features such\nas landscapes and buildings to orient oneself (\"landmark agnosia,\" e.g., Landis et al.,\n1986; Pallis, 1955) or in a more general inability to learn new topographical information\n(\"anterograde disorientation,\" e.g., Habib & Sirigu, 1987). Damage to the\nparahippocampal cortex may also produce deficits for visual materials that conveyed\ninformation about the shape of surrounding space. Since this deficit was demonstrated\nonly during a memory encoding test, these data suggest that the PPA may be a learning\nmechanism specifically dedicated to encoding topographical materials into memory\n(Epstein et al., 2001).\n\nThe LOC\n\nThe LOC is defined functionally as the region of occipital and temporal cortex\nthat responds more strongly to objects than to scrambled objects or textures (Malach et al.\n1995). The LOC has been shown to respond quite strongly and similarly to all kinds of\nobjects tested so far, from cars to novel three-dimensional (3D) objects that subjects have\nnever encountered before (Grill-Spector et al., 2001).\n\nThis study by Grill-Spector et al. found no significant difference in magnitude of\nthe response in this region between common objects and uncommon objects that had\nclear three-dimensional shape interpretations. A similar result was found by Kanwisher et\nal. In 1996. Using line drawings, stronger responses were obtained to 3D objects depicted\nin line drawings, whether familiar or novel, compared to scrambled line drawings, in\n\nwhich no clear shape interpretation was possible. Several more recent studies (Grill-\nSpector et al., 1998; Grill-Spector, Kushnir, Edelman, Itzchak & Malach, 1998; Murtha,\nChertkow, Beauregard & Evans, 1999; Kourtzi & Kanwisher, 2000a; Doniger, Foxe,\nMurray, Higgins, Snodgrass & Schroeder, 2000) suggest that the entire region, from the\nlateral occipital cortex to the posterior temporal region, stimuli whose forms depict a\ndistinct shape elicit more activation than control stimuli that do not depict clear shapes.\n\nResults of an adaptation experiment by Kourtzi & Kanwisher in 2001 suggest that\nneural populations in the LOC represent object shape rather than the low-level features\ndefining the shape. Although this result was important, the question of whether the LOC\nprocesses information about visual contours or information about the shape itself was yet\nto be answered. These two hypotheses at one time seemed difficult to distinguish because\ncontours are always present in images of objects. However, Kourtzi and Kanwisher\n(2000b) devised an experiment to investigate whether adaptation in the LOC would be\nobserved for objects that have the same shape but different contours and objects that have\ndifferent shape but share the same contours. They found that when the perceived shape\nwas the same for the two stimuli but the contours were different, adaptation was as strong\nas for a pair of identical stimuli. In contrast, when the perceived shapes were different but\nthe contours were identical, no adaptation was found. These data suggest that the LOC\nprocesses shape information, not just information about the contours that define that\nshape.\n\nThe LOC has been characterized as the main neural locus of visual shape\nprocessing. But what about objects that convey depth? After all, many objects in the real\nworld have three dimensions. A recent study by Kourtzi et al. asked whether neural\n\npopulations in the LOC encode the perceived 3-D shape of objects or simply their 2-D\ncontours. In particular, the experiment tested first for adaptation in the LOC between 2-D\nsilhouettes and 3-D shaded images of objects and then for adaptation in the LOC across\nchanges in the 3-D shape structure of objects (convex versus concave stimuli) compared\nto changes in their 2-D contours but not the 3-D shape. No adaptation was observed in\nthe LOC for objects that had the same 2-D contours but different perceived 3-D shape\nstructure (convex versus concave objects). In contrast, adaptation was observed in the\nLOC for images of objects that had the same perceived 3-D shape even when they have\ndifferent 2-D contours (Kourtzi et al., 2003). They claimed that the sameness of the\nperceived 3-D shape of objects is necessary and sufficient for adaptation in the LOC.\n\nA recent study by Moore and Engel in 2001 also found increased neural activity\nin the LOC in response to images of objects perceived as 3D volumes rather than as 2D\nshapes. These findings are consistent with other recent human fMRI studies that have\nimplicated the LOC in the analysis not only of 2-D but also of 3-D images of objects\n(Kourtzi and Kanwisher, 2000; Gilaie-Dotan et al., 2001; Moore and Engel, 2001;\nKourtzi et al., 2002) independent of the cues that define the object shape (Sary et al.,\n1993; Mendola et al., 1999; Kourtzi and Kanwisher, 2000). These data suggest that the\nLOC is involved in processing the three-dimensional features of objects, rather than their\ntwo-dimensional contours.\nMotivation for Study\n\nMuch of the precise mechanisms behind the sensory processing by the PPA and\nthe LOC still remain a mystery. However, data from numerous studies has converged to\nsuggest that the PPA is involved in the processing of the layout of scenes but not objects,\n\nwhile the LOC preferentially processes three-dimensional information about objects, but\nnot scenes. If this is the case, there must be a critical difference between the features of\nscenes and the features of objects responsible for the fact that of each region\ndiscriminates between the two classes of stimuli.\n\nWhat is this critical difference between scenes and objects? The current study\ninvestigates the hypothesis that the concavity of scenes and convexity of objects is\nresponsible for the selective activation of the PPA and the LOC.\n\nExperimental Design\n\nLogic\n\nThis experiment will compare the BOLD response produced by concave stimuli\nto the BOLD response produced by convex stimuli in each region of interest (ROI) to\ndetermine whether or not either ROI responds preferentially to one type of stimulus. In\neach experimental condition, concavity or convexity will be conveyed in depth using\nstereo. While it is possible to use 2D images to convey depth, stereo more closely\nresembles the 3D world that humans experience.\n\nSubjects\nTen right-handed students from the Massachusetts Institute of Technology (MIT) will be\nrecruited to participate in this experiment.\n\nDesign\nThis experiment will include two classes of experimental stimuli: convex dot arrays and\nconcave dot arrays. The stimuli in each class will vary in degree of depth and in spacing\nof the dots.\n\nScanning sessions will consist of two localizer scans followed by one experimental scan.\n\nThe experimental scan will use an event-related design. The scan will consist of one\nepoch of experimental trials and two 8 s fixation epochs, one at the beginning and one at\nthe end of the scan. Each scan will consist of 90 experimental trials of convex versus\nconcave dot arrays and 18 fixation trials. Each image will be presented for 300 ms with a\nblank interval 400 ms between images. The stimuli will be counterbalanced for order to\nensure that both images appear an equal number of times and each precedes the other\nequally often.\n\nData Analysis\n\nFor each subject, the BOLD response to concave stimuli will be compared to the BOLD\nresponse to convex stimuli in each region of interest.\n\nTwelve coronal-like slices will be used, (as in Kourtzi et al., 2003).\n\nANOVAs across subjects will be run on the average per cent signal change from fixation\nproduced by each type of stimulus. Data will be analyzed within independently defined\nROIs.\n\nIndependent localizer scans will be used to define each region of interest (ROI).\nLocalization of the PPA will involve a scenes-versus-objects discrimination task.\nLocalization of the LOC will involve an objects-versus-scrambled line drawings\ndiscrimination task. Although the PPA and the LOC are the primary regions of interest, a\nwhole-brain analysis will also be conducted for each subject.\n\nFor the PPA localizer scans, photographs of urban scenes, tabletop scenes, and\nlandscapes will be compared to photographs of intact objects. The localizer scan will\nfeature a blocked design with sixteen 16 s stimulus epochs and interleaved fixation\nperiods as described in previous studies (e.g. Kourtzi and Kanwisher, 2000, 2001).\nTwenty different images of the same type will be presented in each epoch. Each image\nwill be presented for 250 ms with a blank interval of 550 ms between images. Each of the\nfour stimulus types (intact objects and three types of scenes) will be presented in different\nepochs within each scan, in a design that balances for the order of conditions. The\nsubjects will perform a one-back matching task; i.e. they will be instructed to press a\nbutton whenever they see two identical pictures in a row. Two or more consecutive\nrepetitions will in each epoch. This one-back matching task is intended to engage\nobservers' attention on all the stimulus conditions used.\n\nFor the LOC localizer scan, grayscale images and line drawings of novel and familiar\nobjects, as well as scrambled versions of each set will be used (as in Kourtzi, 2003). The\nscrambled images will be created by dividing the intact images in a 20 ◊ 20 square grid\nand by scrambling the positions of each of the resulting squares. The grid lines will be\npresent in both the intact and the scrambled images.\n\nFor the LOC localizer scans a blocked design with sixteen 16-s stimulus epochs and\ninterleaved fixation periods, similar to the PPA localizer scan, will be sued. Twenty\ndifferent images of the same type will be presented in each epoch. Each image will be\npresented for 250 ms with a blank interval of 550 ms between images. Each of the four\nstimulus types (grayscale images and line drawings of objects, as well as scrambled\nversions of each set) will be presented in different epochs within each scan, in a design\nthat balances for the order of conditions. As in the PPA localizer scan, the subjects will\nperform a one-back task.\n\nIn the PPA localizer scan, the PPA will be defined as the region activated by the scene\n\nstimuli. In the LOC localizer scan, the LOC will be defined as the region activated by the\nnon-scrambled objects.\n\nPredictions and Implications\nThere are several possible outcomes of this experiment.\n\nPossible Result #1\n\n1.\nNo significant difference in BOLD response between convex stimuli and\n\nconcave stimuli is observed in the PPA. No significant difference in BOLD\n\nresponse between convex stimuli and concave stimuli is observed in the LOC.\n\nPPA\n\nLOC\n\nThis result does not support the hypothesis. One possible explanation is that neither the\nPPA nor the LOC is preferentially activated by one class of stimuli over the other.\nAnother possible explanation of this result is that a Type II statistical error occurred (the\nPPA and the LOC do in fact each respond more strongly to one class of stimuli, but our\nexperiment failed to detect this difference). If this is believed to be the case, one might\ninvestigate the possibility that the two classes of stimuli were not distinct enough to\nrecruit selective activation from either area. To test this possibility, the stimuli could be\naltered by exaggerating the concavity and convexity of the stimuli and the experiment\ncould be rerun.\n\nPossible Result #2\n\nNo significant difference in BOLD response between convex stimuli and\n\nconcave stimuli is observed in the PPA. Convex stimuli produce a greater bold\nresponse than concave stimuli in the LOC.\n\nPPA\n\nLOC\n\nThis result would suggest that the PPA is not selectively activated by concave stimuli.\nUnlike the previous possible result, this outcome can not be easily explained in terms of\ninadequately distinct stimuli, since the LOC result demonstrates that the stimuli were\ndistinct enough to recruit selective activation in that region. This outcome also suggests\nthat the LOC is preferentially activated by convex stimuli. This may provide insight into\nthe features of objects responsible for the historical selective activation of the LOC by\nobjects.\n\nPossible Result #3\n\nNo significant difference in BOLD response between convex stimuli and\n\nconcave stimuli is observed in the PPA. Concave stimuli produce a greater\n\nbold response than convex stimuli in the LOC.\n\nPPA\n\nLOC\n\nThis outcome would be difficult to explain. It suggests that the PPA is not selectively\nactivated by either class of stimuli and that the LOC responds more strongly to concave\nstimuli than convex stimuli. This is counterintuitive, given that objects are convex and\nthe LOC historically has shown a higher BOLD response to objects than to any other\nclass of stimuli. The selective activation in the LOC by concave stimuli can not be\nattributed to an attentional confound. If it were the result of an attentional confound, one\nwould expect the PPA to be more activated by concave stimuli as well.\nUsing a region of interest approach and an event-related design already gives this\nexperiment more statistical power and fewer possible confounds than a non-ROI\napproach and a blocked design. However, if this result were obtained, it is quite possible\nthat some error occurred, either in the experimental trials or in the data analysis. It would\nbe advisable to rerun the experiment. At the very least, one could test additional subjects\nin order to increase statistical power and possibly overcome some of the 'noise' in the\ndata.\n\nPossible Result #4\n\nNo significant difference in BOLD response between convex stimuli and\n\nconcave stimuli is observed in the LOC. Convex stimuli produce a greater bold\nresponse than concave stimuli in the PPA.\n\nPPA\n\nLOC\n\nAs in the case of the previous result, this outcome would be difficult to explain. It is\nfeasible that the LOC would not respond more strongly to one class of stimuli than\nanother. This simply suggests that some other feature of objects is responsible for\nrecruiting selective activation in the LOC. However, it would be difficult to explain a\nresult in which the PPA is more activated by convex stimuli than concave stimuli, given\nthat the PPA responds preferentially to scenes and scenes are concave. As in the last\npossible result, one should consider the possibility that an error occurred in the\nexperimental run or in the analysis of the data.\n\nPossible Result #5\n\nNo significant difference in BOLD response between convex stimuli and\n\nconcave stimuli is observed in the LOC. Concave stimuli produce a greater\n\nbold response than convex stimuli in the PPA.\n\nPPA\n\nLOC\n\nThis outcome supports part of the hypothesis: that concave stimuli produce a greater PPA\nresponse than do convex stimuli. This may give insight into the features of a scene that\nare responsible for selectively activating the PPA. An attentional confound is unlikely, as\nsuch a confound would likely produce a greater activation by concave stimuli in both\nregions, not just the PPA. Also, an attentional confound is also unlikely given that the\nstimuli share all features in common except the feature that we are testing (concavity\nversus convexity). This result does not support the claim that the LOC is more strongly\nactivated by convex stimuli than concave stimuli. Given the PPA result, it is unlikely that\nthis result was due to a confound.\n\nPossible Result #6\n\nConvex stimuli produce a greater bold response than concave stimuli in the\n\nPPA. Convex stimuli produce a greater bold response than concave stimuli\n\nin the LOC.\n\nThis result is similar to the last one in that it supports part of the hypothesis and fails to\nsupport the rest. It suggests that convex stimuli produce a greater LOC response than do\nconcave stimuli. This may give insight into the features of an object that are responsible\nfor selectively activating the LOC. This result does not support the claim that the PPA is\nmore strongly activated by concave stimuli than convex stimuli. In fact, it produces a\nresponse that is quite different from that which is expected and intuitive. Under some\ncircumstances, it is reasonable to hypothesize that a greater response in both regions to\none class of stimuli is due to the fact that those stimuli were more salient or complex (a\nconfound). However, given the similarity of the stimuli in this experiment, such an\nexplanation is not likely to be true.\n\nPossible Result #7\n\nConvex stimuli produce a greater bold response than concave stimuli in the\n\nPPA. Concave stimuli produce a greater bold response than convex stimuli\n\nin the LOC.\n\nPPA\n\nLOC\n\nThis unexpected outcome is doubly difficult to interpret. It contradicts both parts of the\nhypothesis and suggests that either the hypothesis is completely wrong, or the experiment\nand/or analysis of the data was flawed.\n\nPossible Result #8\n\nConcave stimuli produce a greater bold response than convex stimuli in the\n\nPPA. Concave stimuli produce a greater bold response than convex stimuli\n\nin the LOC.\n\nPPA\n\nLOC\n\nThis result would support the first portion of the hypothesis: that PPA response is higher\nto concave stimuli than to convex stimuli. However, it challenges the experimenter to\n\nexplain why the same result was found in the LOC. As previously mentioned, the features\nof these stimuli are so similar that it is unlikely that the concave stimuli could be\nconsidered more salient or more complex than the convex stimuli. Given the PPA results,\nit would be reasonable to hypothesize that some feature other than convexity is\nresponsible for selective LOC activation by objects.\n\nPossible Result #9\n\nConcave stimuli produce a greater bold response than convex stimuli in the\n\nPPA. Convex stimuli produce a greater bold response than concave stimuli\n\nin the LOC.\n\nPPA\n\nLOC\n\nThis is the optimal result, and the only one that supports the hypothesis in its entirety.\nThe fact that concave stimuli produced more activation than convex stimuli in the PPA\nand less activation than convex stimuli in the LOC (and vice versa for convex stimuli)\nsuggests that there was no attentional confound.\n\nConclusion\nThe current study investigates the mechanisms responsible for the selective activation of\nthe PPA and the LOC. As such, it is at the intersection of two important lines of research,\neach of which has broad and far-reaching implications. The PPA is believed to be\ncritically involved in processes necessary for human navigation, including the encoding\nof spatial information into memory. Insight into the functions of the LOC is equally\nvaluable. Understanding the mechanisms by which the LOC processes object shape\ninformation is critical to the development of a full theory of object recognition. This\nalone has important implications, especially to the future of artificial intelligence.\nPrevious research has addressed the question of what kinds of stimuli activate each of\n\nthese important regions. The current study investigates 'why.'\n\nReferences\n\nAltmann, C.F., Bulthoff, H.H., & Kourtzi, Z. (2003). Perceptual organization of\nlocal elements into global shapes in the human visual cortex. Current Biology,\n13(4): 342-9.\n\nAvidan, G., Harel, M., Hendler, T., Ben-Bashat, D., Zohary, E., & Malach, R. (2002a).\nContrast sensitivity in human visual areas and its relationship to object\nrecognition. Journal of Neurophysiology, 87: 3102-3116.\n\nBar, M., Tootell, R.B.H., Schacte,r D.L., Greve, D.N., Fischl, B., Mendola, J.D., Rosen,\nB.R., & Dale, A.M. (2001). Cortical mechanisms specific to explicit visual object\nrecognition. Neuron, 29: 529-535.\n\nBohbot V.D., Kalina M., Stepankova, K., Spackova, N., Petrides, M., & Nadel, L.\n(1998). Spatial memory deficits in patients with lesions to the right\nhippocampus and to the right parahippocampal cortex. Neuropsychologia.\n36(11): 1217-38.\n\nCheng, K. (1986). Apurely geometric module in the rat's spatial representation.\nCognition, 23: 149-178.\n\nDoniger, G. M., Foxe, J. J., Murray, M. M., Higgins, B. A., Snodgrass, J. G., & Schroeder, C. E.\n(2000). Activation timecourse of ventral visual stream object-recognition areas: high\ndensity electrical mapping of perceptual closure processes. Journal of Cognitive\nNeuroscience, 12: 615-621.\n\nEpstein, R., DeYoe, E.A., Press, D.Z., Rosen, A.C. & Kanwisher, N. (2001).\nNeuropsychological evidence for a topographical learning mechanism in\nparahippocampal cortex. Cognitive Neuropsychology, 18: 481-508.\n\nEpstein, R., Graham, K.S., & Downing, P.E. (2003). Viewpoint-specific scene\nrepresentations in human parahippocampal cortex. Neuron, 37(5): 865-76.\n\nEpstein, R., & Kanwisher, N. (1998). A cortical representation of the local visual\nenvironment. Nature, 392(6676): 598-601.\n\nEpstein, R., Stanley, D., Harris, A., & Kanwisher, N. (2000). The Parahippocampal\nPlace Area: Perception, Encoding, or Memory Retrieval?. Neuron, 23: 115-125.\n\nGallistel, C.R. (1990). The organization of learning. Cambridge, MA: MIT Press.\n\nGrill-Spector, K., Kourtzi, Z., & Kanwisher, N. (2001). The Lateral Occipital Complex and\nits Role in Object Recognition. Vision Research, 41: 1409-1422.\n\nGrill-Spector, K., Kushnir, T., Edelman, S., Itzchak, Y., & Malach, R. (1998a). Cue-invariant\nactivation in object-related areas of the human occipital lobe. Neuron, 21: 191-202.\n\nGrill-Spector, K., Kushnir, T., Hendler, T., Edelman, S., Itzchak, Y., & Malach, R. (1998b). A\nsequence of object processing stages revealed by fMRI in the human occipital lobe.\nHuman Brain Mapping, 6: 316-328.\n\nGrill-Spector, K., Kushnir, T., Hendler, T., & Malach, R. (2000). The dynamics object\nselective activation correlate with recognition performance humans. Nature\nNeuroscience, 3: 837-843.\n\nHabib, M., & Sirigu, A. (1987). Pure topographical disorientation: a definition and\nanatomical basis. Cortex, 23: 73-85.\n\nHaxby, J.V., Gobbini, M.I., Furey, M.L., Ishai, A., Schouten, J.L., & Pietrini, P. (2001).\nDistributed and overlapping representations of faces and objects in ventral temporal\ncortex. Science, 293: 2425-2330.\n\nHermer, L., & Spelke, E.S. (1994).A geometric process for spatial reorientation in young\nchildren. Nature, 370: 57-59.\n\nHermer, L., & Spelke, E.S. (1996). Modularity and development: the case of spatial\nreorientation. Cognition, 61: 195-232.\n\nHermer-Vasquez, L., Spelke, E.S., & Katsnelson, A.S. (1999). Sources of flexibility in\nhuman cognition: Dual-task studies of space and language. Cognitive Psychology,\n39: 3-36.\n\nKourtzi, Z., Erb, M., Grodd, W., & Bulthoff, H.H. (2003). Representation of the\nperceived 3-D object shape in the human lateral occipital complex. Cerebral Cortex,\n13(9):911-20\n\nKourtzi, Z, & Kanwisher, N. (2001). Representation of Perceived Object Shape by the\nHuman Lateral Occipital Complex. Science, 293: 1506-1509.\n\nKourtzi, Z., & Kanwisher, N. G. (2000a). Cortical regions involved in perceiving object\nshape. Journal of Neuroscience, 20: 3310-3318.\n\nKourtzi, Z., & Kanwisher, N. G. (2000b). Processing of object structure versus contours in\nthe human lateral occipital cortex. Society for Neuroscience Abstracts, 30: 22.\n\nLandis, T., Cummings, J.L., Benson, D.F., & Palmer, E.P. (1986). Loss of topographic\nfamiliarity: an environmental agnosia. Archives of Neurology, 43: 132-136.\n\nMalach, R., Grill-Spector, K., Kushnir, T., Edelman, S., & Itzchak, Y. (1998). Rapid\nshape adaptation reveals position and size invariance. Neuroimage, 7: S43.\n\nMalach, R., Reppas, J.B., Benson, R.B., Kwong, K.K., Jiang, H., Kennedy, W.A., Ledden,\nP.J., Brady, T.J., Rosen, B.R., & Tootell, R.B.H. (1995). Object-related activity\nrevealed by functional magnetic resonance imaging in human occipital cortex.\nProceedings of the National Academy of Sciences USA, 92: 8135-8138.\n\nMargules, J., & Gallistel, C.R. (1988). Heading in the rat: determination by\n\nenvironmental shape. Animal Learning and Behavior, 16: 404-410.\n\nMoore, C., & Engel, S. A. (2001). Neural response to the perception of volume in\nthe lateral occipital complex. Neuron, 29: 277-286.\n\nMurtha, S., Chertkow, H., Beauregard, M., & Evans, A. (1999). The neural substrate\nof picture naming. Journal of Cognitive Neuroscience, 11: 399-423.\n\nPallis, C.A. (1955). Impaired identification of faces and places with agnosia for\ncolours. Journal of Neurology, Neurosurgery and Psychiatry, 18: 218-224."
    },
    {
      "category": "Resource",
      "title": "Final Projects",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/90f34906f26ffb4027d184f0901aef22_finalprojects.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFinal Projects 9.71 Fall 2007\nI. New schedule:\n1. This outline is due one day before SES #10 and will form part of your grade for the\nproject. All of you have received extensive feedback - you should now take this feedback\ninto account and use it to turn in a much-improved outline.\n2. On SES #10, we will not have presentations - instead, I will lecture about visual\nexperience (scheduled on SES #12 in the syllabus).\n3. On SES #11 and SES #12, we will have student presentations.\nII. Presentations\nThere will be a strict time limit of 10 minutes of presentation and 10 minutes of class\ndiscussion for each.\nA. Presentations on Experiment Proposals\nThese presentations should be very similar to the paper presentations you have\nalready given earlier in the course - see my notes on presentations from the earlier\npresentations. The main difference is that you will not have data to present. Instead, you\nwill present several alternative predictions of how the data might come out. You will then\ndiscuss how each of these possible outcomes will answer the question you posed or will\nbear on the hypothesis. After that you can discuss the implications of these possible\noutcomes, and any further control experiments you may need to do to address remaining\nopen questions.\nB. Presentations Reviewing the Literature relevant to a specific research question\nThese presentations should be very similar to the paper presentations you have\nalready given earlier in the course - see my notes on presentations from the earlier\npresentations. The main difference is that you will start by explicitly stating and\nmotivating a larger-scope question, and instead of talking about one experiment in detail,\nyou will describe several studies more briefly - preferably each of the experiments you\ndescribe will address a different facet of the larger question. Obviously you won't have\ntime to go into the methods of each of these studies n detail, so you will have to figure\nout what the essential elements are of each of the most relevant studies. See my lecture\nslides from the course for examples of how to do this. A critical aspect of these\npresentations will be to identify any emerging consensus in the literature that answers\nsome part of the larger question, but also crucially to identify unanswered questions that\neither has not been addressed in the literature, or that have been addressed with\nconflicting results.\nIII. Written term Papers (Due at the beginning on SES #13)\nSee my previous handouts for guidelines on written term papers. Those guidelines cover\nthe format of term papers proposing experiments. I will soon hand out guidelines for term\npapers that review the existing literature on a specific question, but these will be\nessentially an expanded written version of the presentations described in Part II above."
    },
    {
      "category": "Resource",
      "title": "How To Give a Paper Presentation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/64ee4ab51307fed3e595219cd9dab48c_presentingpaper.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nfMRI of High-level Vision\nMIT Course 9.71\n\nDept. of Brain and Cognitive Sciences\nN Kanwisher\nSeptember 2007\n\nHOW TO GIVE A PAPER PRESENTATION\n(based in part on notes from DiCarlo)\n\nScientific oral presentations are not simply readings of scientific manuscripts. Nevertheless, at\nan overall level, your scientific talk should be organized into sections that parallel the sections in\nthe scientific paper. As in the scientific paper, the key sections are:\n\n1) INTRODUCTION: The rationale for the experiment (why did you do it?)\n2) METHODS: The methods that were used (how did you do it?)\n3) RESULTS: The results obtained (what did you find?)\n4) DISCUSSION: An interpretation of those results (what does this mean?)\n\nAlthough not part of a standard oral presentation, you should end your talk with:\n5) CRITIQUE AND GROUP DISCUSSION: Your job as presenter is to not only present the\npaper, but also lead class discussion of its strength & weaknesses and how this work\nmight be extended in the future. To help focus the class discussion, end your\npresentation with a list of approximately three major questions/issues worthy of further\ndiscussion (see below).\n\n1. INTRODUCTION\nThe first 1 or 2 slides should introduce your subject to the audience. Very briefly (you only have\nabout 20 minutes total, including discussion) give a concise background. Explicitly state the\nquestion addressed in the paper. Start with the \"big picture\" and then immediately drive to how\nyour study fits in the big picture (one or two sentences.) One option that often works well in a\ntalk is to state your major conclusion(s) up front. That is, in a few sentences, tell the audience\nwhere you will lead them in this presentation. (e.g. \"Although previous studies have found that\nthe fusiform face area responds selectively to faces, in this talk I will show that, under certain\nconditions, the FFA responds almost as strongly to bodies as to faces.\") Whether or not you do\nthis, all presentations must include a slide that comes up near the beginning of the talk\n(within the first 3 slides) with a title that says \"The Question\", in which you state as\nsimply as possible the main question addressed in the paper. (Sometimes there will be two\nor max three questions, but do not include more - if the paper addresses more than three\nquestions it is your job to select only the most important ones.)\n\n2. METHODS\nThere should be 1 or 2 methods slides that allow the audience to understand the overall \"logic\"\nand design of the experiment.. Critical here for imaging experiments is an explanation of the\n\nstimulus and the task: what variables were manipulated, what was compared to what, and\nwhat mental processes are the researchers trying to isolate with this comparison? Also\nimportant here is a discussion of whether particular brain regions will be targeted in a given\nexperiment: is this a whole-brain exploratory experiment, or an experiment designed to test a\nparticular hypothesis about a particular region or the brain, and if so what region? Will functional\nlocalizers be used? Is this an event-related or a blocked design? Often experimental design is\nbetter explanation with diagrams than words. Also, your job as presenter is to select only the\nmost important aspects of the experiment to explain, not to distract the listener with a barrage of\nless-relevant information. Do not put in details that might be appropriate in a paper (people can\nask about them at the end if they are interested). For example, \"7 subjects were functionally\nscanned at 3T while... ...\" NOT: \"The a Siemens 3T TRIO scanner (Siemens, Erlangen,\nGermany) at the MGH/MIT/HMS Athinoula A. Martinos Center for Biomedical Imaging in Charlestown,\nMA; A Gradient Echo pulse sequence was used with a TR = 1.5 s and TE = 30 ms ...\"\n\n3. RESULTS\nThe next slides should show the major results. If appropriate, it is nice to start with a slide\nshowing the basic phenomenon: activation blobs, and/or time courses, with graph axes clearly\nlabelled. Even though your axes are labeled, start the discussion of any data by first telling the\naudience what your axes are. It is fine, and usually a good thing, to reminding the audience of\nanything they may already know but that is relevant to understanding the data, just to help orient\nthem (For example, \"here is the fMRI response as a function of time, showing the usual peak\naround six seconds after stimulus onset, tailing off around twelve to fifteen seconds after the\nbeginning of the trial.\"). Next, show figures that clearly illustrate the main results. All figures\nshould be clearly labeled. When showing figures, be sure to explain the figure axes before you\ntalk about the data (e.g., \"the X axis shows time. The Y axis shows the fMRI response.\").\n\nTo make these slides it is often easiest to export the images from the original paper and import\nthem into powerpoint.\n\n4. DISCUSSION (Conclusions)\nList the conclusions in clear, easy-to-understand language. You can read them to the audience\n(provided there are not too many words). However, don't have a lot of text on the screen if you\nwill be simultaneously saying something else; people cannot pay attention to both. For this\ncourse, I virtually insist that you return to your \"The Question\" slide, reminding your\nlisteners what the question was, then adding in the answer just below (perhaps under a\nheading \"The Answer\", and perhaps as a pop-up).. Also give one or two sentences about\nwhat this likely means (your interpretation) in the big picture (i.e. come full circle back to your\nintroduction) and perhaps some future directions.\n\n5. CRITIQUE\nPlease end your presentation with at least two or three major things that should be discussed.\nThese should consist of things like: was there a major confound in the study, or an alternative\naccount of the data that the author did not discuss? Here is the place to mention anything that\nmight be improved in the study, any additional experiments that you think might be appropriate\n(better?), and general theoretical issues about the topic investigated (i.e. put the study in the\n\"big picture\" of object recognition, visual attention, or whatever the topic is). Discussion from the\n\naudience should be especially encouraged at this point, but you should be prepared to foster\nthis by raising these issues (e.g. one slide with a list of issues).\n\nOften there is an awkward moment at the end of a talk when you trail off, people don't know if\nyou are finished, and no one whows whether to clap yet. You can avoid this awkward moment\nby pausing briefly, looking at the audience, and saying, definitively and with closure: \"Thank\nyou\". That makes a nice, crisp, socially comfortable ending.\n\nOverall tips:\n\n- Control of time. For most conference presentations, interruptions are not allowed and\nquestions are held until the end. Ffr this class we will take only questions of clarification during\nthe presentations, holding discussion till the end.\n\n- As a rule of thumb, you should have no more slides than the minutes allowed for the\npresentation (e.g. 15 slides for a 15 minute presentation), but less is better.\n\n- Data reduction. One of your most important tasks as a presenter is to figure out what\nNOT to present. You could of course give people the paper and they could read it themselves.\nThe point of a talk is to get the critical information across in much less time than it takes to read\na paper. Thus, in any good talk, there will be a lot of information in the paper that is not in the\ntalk. A big part of your job in preparing the talk is to decide what is important to understand the\nstudy. In general, less information means more clarity (up to a point).\n\n- Establish eye contact with your audience. If they are alert and looking at you, even nodding,\nyou know you are on track. If they are frowning then you are losing them. If you are unsure, at\nleast for a small group, ask them: \"is that clear?\" (people will rarely say no, so if you don't get a\nlot of encouraging nods, go over it again, trying to be simpler.) In a good talk (though maybe not\na conference presentation to a large audience) you should feel like you are having a\nconversation with your audience: even though they may not be saying anything, you are\nwatching them all the time to see if they are with you. If you have just gone through a\ncomplicated explanation, and you are not sure your audience has tracked you, synopsize the\nslide with a simplified statement by saying for example \"So in other words, the logic of this\nexperiment is to compare X to Y to find the brain locus of Z.\"\n\n- show enthusiasm for your topic. Both enthusiasm and boredom are contagious.\n\n- As you put up each slide, have in mind the key points you want to make with that slide (one or\ntwo points). When you are preparing and designing the presentation, think: \"Why is this slide in\nhere?\" If you cannot think of an important reason, the slide probably should not be in the\npresentation.\n\n- Speak in short sentences and use easy to understand language. Avoid jargon. If you are a\ncognitive neuroscientist, think about how you would like a molecular biologist (i.e. a scientist\nwho knows something about neuroscience, but is not an expert in your particular field) to\npresent to you (and vice-versa).\n\n- Practice, practice, practice! No one, no matter how experienced, can give a good short\npresentation like this without practicing it straight through at least two or three times. Once you\nget your talk in decent shape, practice it on your friends. If there is anything they don't\nunderstand (even if they are not course 9 majors) that is probably something you need to fix.\n\n- With enough practice, you will find that once you are into a presentation, it will flow smoothly\n(you have already put the work in to create the proper slides and have thoroughly practiced\npresenting the key points of each slide before you even walked into the room). However,\nbecause you may be facing a large audience, you may feel more nervous than during your\npractice sessions (often practiced alone). Thus, the goal is simply to \"get started\" with the\npresentation. Once you are started, the nervousness will disappear as you are now \"in your\nfamiliar zone\" (well practiced). A very helpful tip here is to memorize your first few sentences.\nWhen you get up to speak, these will almost reflexively come out of your mouth and launch you\ninto your presentation.\n\n- If there is a possible concern about the study that you don't have time to address in the talk\n(but you think you might get asked about), have a slide (or a few) ready to address that point\nthat you can put up if/when asked. It always looks good to show that you have anticipated a\nquestion.\n\n- 95% of the work is done before you even show up to give the presentation:\n\n- Don't have too much information (particularly, too many WORDS) on each slide. Use the\npop-up feature of powerpoint to avoid snowing your listener with too much information when a\nfirst slide appears. The goal should be clear slides, with large fonts and minimal clutter. Avoid\ncomplex and distracting backgrounds.\n\n- Slides are organized along the primary topics discussed above.- You have practiced the entire\npresentation and worked out any \"rough spots\". You know what slide is coming next before it\npops up, so you can make a nice smooth segue from the current slide to the next one. If you\nhave to look for a moment at a slide to remember what you want to say about it, that is bad.\n\n- Because you have practiced so much, you are confident that the presentation is at the\nexpected time limit, so there is no reason to rush or to become concerned with the clock. You\ncan relax and enjoy showing off your grasp of the study and your well-polished presentation."
    },
    {
      "category": "Resource",
      "title": "Some Tips on How to Critically Evaluate fMRI Studies",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/8e6dd36465750d786f0426cb175f59ad_crtqufmripapr.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nSome Tips on How to Critically Evaluate fMRI Studies\nNancy Kanwisher\n9.71 Sept, 2007\n\n1. First, figure out what question the researcher is asking and what answer they are\ngiving to that question.\nAsk yourself: Is this an interesting question? Does it have clear theoretical\nimplications and if so what are they? Do you care about the result? Should anyone?\nWhy? Are you surprised by the result? Situate the question in a broader theoretical\ncontext. If there is no such broader context, be worried.\n\n2. The most critical aspect of the design of the experiment is: what is getting compared to\nwhat?\nMake a list of all the mental functions that you think go on during the critical test\ncondition. Then make a list of all the mental functions that are going on in the control\ncondition, then see how many go on only (or more) in the test condition than the control\ncondition.\n\nAre the test and control conditions \"minimal pairs\"?\n\nBe wary of very low baselines such as fixation.\nBeware of comparisons of a very difficult task versus a very easy task. Lots of\nbrain areas get activated by virtually any difficult task; this activation may be very\nnonspecific and may be difficult to interpret (without appropriate controls).\nWatch out for attention confounds: is one condition much more\ninteresting/engaging/attention capturing than another?\nWatch out for eye movement confounds between conditions.\n\nImagine doing the task (or code up a version of it) and introspect while you do\nthis mental simulation. What does it feel like and what do you think you would really do\nin this task?\n\n3. Classic problems in analyses/inferences/conclusions to be wary of:\n\nA. \"Brain area X was activated by task Y.\"\n\ni. Ask: task Y compared to what? Everything is a comparison, and many\ncomparisons are uninformative/trivial.\n\nii. What else activates brain area X? the specific activation seen in task Y\n(compared to whatever) may not be so specific if prior studies have already been\nimplicated the same area in dozens of other tasks/processes. Two classic cases in point:\nthe anterior cingulate cortex (ACC), and the intraparietal sulcus (IPS).\n\niii. How strongly activated was that region? Not all 'activations\" are the same -\nEffect sizes matter! If one condition produces a massive response compared to a given\nbaseline, and another condition produces a very small but significant activation, the two\n\"activations\" are not the same. Brain imaging is the only field I can think of where people\nseem to think it is OK to report p levels without means; this is completely bizarre and can\nbe highly misleading. (Note: many fMRI researchers appear to disagree with me on this\none.)\n\nB. \"Because Region X responded significantly more strongly in Task A than control, but\ndidn't respond significantly more strongly in Task B than control, it is\nselectively activated by Task A.\"\n\nA difference in significances is not necessarily a significant difference. If you\nwant to claim that the region responds more to A than B, then compare A to B. Statistics\nare not transitive.\n\nC. Claims of this form: \"We found activation in the medial prefrontal cortex for tasks\ninvolving reasoning about other minds, consistent with numerous prior studies.\"\n\nBrains are as different across individuals as faces are, so what counts as the \"same\nplace\" in the brain is not well defined across different brains. (Is the freckle on Joe's nose\nin the same place as the freckle on Bob's nose? This means something, but it is highly\nimprecise. ) The prior activations for reasoning about other minds in the medial prefrontal\ncortex range up to 5 centimeters apart from each other. This is not the \"same place\" in\nany meaningful sense. The 'same place\" in the brain is only clearly defined within an\nindividual subject. When pooling data across subjects, better options are to define regions\nof interest (functionally or anatomically) within each individual subject before pooling\nacross subjects. Note that this is my opinion, and though many in the field agree with me,\nsome disagree; see Friston et al (2006) and Saxe et al (2006) for a lively debate on this\ntopic.\n\nD. \"The results of the present study demonstrate that Task A is carried out in a distributed\nnetwork of cortical areas.\"\n\nWhat has been learned here?\n\n4. Some of the many ways to cheat:\nA. Showing data from the \"best voxel\".\n\nWith tens of thousands of voxels to chose from in an overall nosiy data set, some\nof them will look pretty good.\n\nB. Showing \"fitted data\". If you have a real effect, show your real data.\n\nC. Showing activation maps that \"look similar\" or \"look different\". There are many ways\nto chose particular slices, thresholds, etc to make activations look similar or different. If\nthe claim is that they are similar or different, this should be tested statistically on the\nexact same voxels. Just showing similar-looking activations (especially in group data or\nacross subjects) without statistically testing whether the same voxels are activated, is very\nweak. Beware of sneaky choice of slices; look at the anatomical images to see if it really\nis the same slices.\n\n5. Some signs of a well done study:\nA. The researchers show some raw data, e.g. nonfitted time courses or at least percent\nsignal increases from fixation (or \"beta weights\") in independently-defined regions of\ninterest.\nB. The critical result is replicated at least once.\nC. More than one control condition is used, or the control condition is a \"minimal pair\".\n\n6. Some important general caveats about fMRI research:\nA. Typical imaging parametrs include about several hundred thousand neurons per\nvoxel! Most studies smooth their data and average across subjects which increases this\nnumber dramatically. It is a great miracle that we see anything at all with this method.\nB. Temporal resolution of fMRI is lousy - at best a few 100 ms. Most of cognition\nhappens in tens of milliseconds, not hundreds. So component steps cant usually be\nresolved.\nC. fMRI activations do not imply necessity!"
    },
    {
      "category": "Resource",
      "title": "Term Paper Options",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/49bcf9657c6e1ef6b2eaa6320928d3a4_trmpaproptions.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nTerm Paper Options 9.71 Fall 2007\nYou have two options to chose from: I) writing a review article, and ii) proposing a novel\nexperiment. The expectations and requirements of each are outlined below.\nOption I: Write a Review Article\nChoose a relatively focused question about visual cognition that has been investigated\nextensively with fMRI, and write a term paper reviewing the relevant literature on this\nquestion. Conclude with a discussion of how the question has been answered by the\nrelevant literature, and what aspects of the question have not been answered. Finally,\ndiscuss any important questions for future fMRI research that arise from the literature\nyou have reviewed.\nYou may either chose your own topic question, or chose from the list below (which we\nwill add to as the course proceeds). If you chose from the list below email Nancy with\nyour choice - these topics are on a first-come first-served basis, I don't want two people\nchosing the same topic.\nWhat does the PPA do?\nWhat does the EBA do?\nWhat does LO do?\nWhat does MT do?\nWhere is position invariance achieved in the cortical visual pathway?\n(How) is cortical processing of visual information different in people with autism versus\ntypically-developing people?\n(How) is cortical processing of visual information different in people with dyslexia\nversus typically-developing people?\n(How) is the visual system of blind people different from sighted people?\nDo visual imagery and visual perception share the same neural substrates?\nA topic question and article outline (including at least ten of the main articles you will\nreview) is due at the beginning of class October 25, first draft is due at the beginning of\nclass Wed Nov 21, and final draft is due at the beginning of class December 6. In\naddition you will do an oral presentation of this material on Nov 8 or 23. More details\nabout the format of each of these will be handed out as the time gets closer.\nNote that often when considering a deep scientific question and reviewing the existing\nliterature on that question, you will see what questions have not yet been answered in the\nliterature, and you will have ideas for how to tackle these questions empirically. If this\nhappens early enough in the process, discuss with Nancy or Talia the possibility of doing\na hybrid project combining Options I and II, or switching to Option II altogether.\nOption II: Propose a Novel Experiment\n\nHere you will propose a novel experiment testing a theoretically-motivated\nhypothesis that has not been resolved in the prior literature. This assignment requires\nmore independent thought and creativity than Option I, but may also emerge naturally\nfrom Option I. If you elect Option II you should email Nancy or Talia a very short\nsynopsis of your idea (one paragraph) as early as possible. Your choice of topics is\nsubject to my approval of the outline you hand in on October 25. The requirements for\nthe three phases of the project are described below.\n1. Experiment Proposal Outline Guidelines (due Oct 25)\nStart with a single statement of the question you are asking in the experiment, if\npossible in the form of a hypothesis that your experiment will test. You *might* have two\nhypotheses, but don't have more than two without discussing it with me.\nThen say why this is an interesting question. For example, you can mention prior\nfindings that set the context for this question.\nBe sure to answer and explain the following:\n1. What will the subject see and do in the experiment? What are the stimuli? What is the\ntask? Consider using a figure or diagram if your stimuli are not standard.\n2. What is the design of your experiment (what are the factors you will vary and the\nconditions within each)? A diagram would be helpful here.\n3. How will you analyze the data? Will you look everywhere in the brain, or will you\nhave have ROIs? If ROIs, which ones and how will you define them? What conditions\nwill you compare to what?\n4. Predictions: Sketch or describe two or three of the main possible outcomes that you\nmay find in your experiment, and what each of these outcomes say about the hypothesis\nyou are testing. (If the outcomes are not relevant to the hypothesis, then you need to\nredesign the experiment or come up with a different hypothesis.)\n5. If you like, make an appointment to meet with me or Talia to discuss your experiment.\nBut I expect you to have thought hard about it before you come meet with me.\n2. Presentations on Experiment Proposals (Nov 8 and 15)\nThere will be a strict time limit of 10 minutes of presentation and 10 minutes of\nclass discussion for each.\nThese presentations should be very similar to the paper presentations you will\nhave already been giving earlier in the course - see my notes on presentations from the\nearlier presentations. The main difference is that you will not have data to present.\nInstead, you will present several alternative predictions of how the data might come out.\nYou will then discuss how each of these possible outcomes will answer the question you\nposed or will bear on the hypothesis. After that you can discuss the implications of these\npossible outcomes, and any further control experiments you may need to do to address\nremaining open questions.\n3. Guidelines for Term Papers Proposing Experiments (due Dec 6)\nYour term paper proposing an experiment should in most respects be a more in-\ndepth version of your class presentation. It should contain:\n\n1. Title: short and clear (cute is ok but not at the expense of unclarity concerning your\ntopic).\n2. Abstract. Roughly a third to half a page. This should include an explicit statement of\nthe question your research addresses, as well as the logic and design of your experiment.\n3. Background & Significance (a few pages).\nSet up the background and motivation to the question you are asking, and review\nthe relevant prior findings. This part should be like the introduction to a scientific paper\n(say, in Neuron or Nature Neuroscience). Explain why the question is important (what\nbig theoretical issues are at stake?), and what prior work is relevant. Say what is known\nfrom prior work, what is not known, and how your proposed experiment fits in.\nOften it is useful to first argue (say, in one paragraph) why you might get one\nanswer to the question (based on prior work), then (perhaps in the next paragraph) rally a\ndifferent set of evidence and arguments to say why you might get a different result. This\nis useful in making the case that the answer to the question is not obvious in advance, and\nto show how your question links to other work.\n3. Logic, Design & Methods, and Predictions\nLogic: briefly explain the overall logic of your experiment. (e.g. ?\"This\nexperiment will test whether representations in the FFA are invariant with respect to\nchanges in lighting, by asking whether the adaptation found in the FFA when identical\nfaces are repeated is still found when two faces appear that are of the same individual but\ndiffer in lighting.\"). This part can appear at the end of the Background & Significance\nsection if you prefer.\nDesign: What will you measure (and in which brain areas)? For your\nexperimental design: list all your conditions, and say how they fit; e.g., is this a 2x2\ndesign? Is it event related? Describe your stimuli and task. How long is each stimulus on\nin each trial, how long is each interval. If stimuli are distinctive and not generic, provide\nexamples (in the appendix if you have lots of them). How long will each scan be? How\nmany scans will you run? What will the order of conditions be? What will you\ncounterbalance for?\nScanning Details. You don't need to go through all of this in detail, but be sure to\nsay at least what parts of the brain you will scan, and mention any unusual scanning\nprocedures your experiment may require. A useful thing to do here is to find another\nstudy that uses similar scanning methods and say that your study will follow a similar\ndesign except as indicated. (But don't leave this as a reason not to mention explicitly any\nkey aspects of your experiment; your reader should not have to consult another paper to\nget the main point.)\n4. Analysis.\nHow exactly will you analyze your data? Is this an ROI design? If so how will\nyou define the ROI? IF not will you run your analyses on each voxel of the brain, or\nfocus on some general regions?\nSAY EXACTLY WHAT YOU WILL COMPARE TO WHAT. You may have\nseveral comparisons; if so list each of the critical ones.\n5. Predictions.\nMake tables or charts, or if the data is very simple just describe it verbally, for\neach of the possible outcomes you may get in your experiment. (Hand sketched charts are\nfine if they are clear, don't waste time with fancy visuals if you don't want to.)\n\nFor each possible outcome say how this pattern of data would answer the question\nyou started with. You can also here discuss other possible implications of each pattern of\ndata.\n6. Conclusions\nHere you go back to the top level to briefly discuss the possible implications and\nthe bigger picture.\nYou can also discuss what questions your study would leave unanswered (e.g., if\nthere is an alternative account of one of your possible findings, what follow-up\nexperiment might you run) and how you might address those in future experiments.\n7. References.\nInclude a complete bibliography of all cited articles. I don't care about the precise\nformat of the citations, so long as the authors and then the year are listed first, you have\nsufficient information to find the article in a good library or on Pubmed, and you are\nconsistent."
    },
    {
      "category": "Resource",
      "title": "Effect of Experience on Extrastriate Cortex",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/8b14ff753ba0567d4e08b74d2662aedf_lec10_exper.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 10:\nEffect of experience on\nextrastriate cortex\n9.71 Fall 2007\nI.\nBrief comment from midterm\nII.\nCool stuff from SFN\nIII. Main lecture on effects of experience on cortex\n\nFrom midterm:\n- The PPA is not in the dorsal pathway! (neither is hippocampus\nwith its \"place cells\").\nPPA : perception of the shape of space\nand/or recognition of that place\nHippocampal place cells: where you are in the world.\n- Dorsal pathway: where objects are in the visual field, not where\nyou the organism are in the world\n\nCool stuff from SFN\n- Pitcher et al\n-Loc info in ventral pathway\n-About 5 papers!\n-Moeller, Friedrich, & Tsao (microstim of face patches)\n-Freiwald & Tsao (characterization of face patches)\n\n- Is all this structure fixed, or does it depend on experience? Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nWhy might/should experience change visual cortex?\nCan't build in everything innately\n(e.g., what toasters look like)\nNeed to be able to learn:\nnew objects,\nstatistical regularities in the world\nLearning implies some kind of change in the brain.\n\n1. Does experience affect fMRI responses to learned stimuli?\nunder what conditions?\neven in adulthood?\neven in primary visual areas?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each?\nWhat is spatial distribution of experience effects?\nclusters? new areas? distributed changes?\nQuestions\n\nTwo related but importantly different questions:\nOrigins: How does a functionally distinctive region of cortex arise\nduring development?\nWhat role does experience play in wiring up this region?\ne.g., how does the FFA get there in the cortex? (and why\nTHERE?).\nAdult Plasticity: Once a region exists (in adulthood), to what extent\ncan experience alter its response profile?\nFine tuning, or radically different response?\nAre some areas more shaped by adult experience than\nothers? (e.g. FFA? \"higher\" areas more than \"primary\")?\nHow much experience is necessary?\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli?\nunder what conditions?\neven in adulthood?\neven in primary visual areas?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each?\nWhat is spatial distribution of experience effects?\nclusters? new areas? distributed changes?\n\nA\nB\nC\nD\nPre-experience\nStronger responses\nSharper tuning\nStronger+sharper\nSingle unit response\nPopulation response\nA\nB\nC\nD\n\nA\nB\nC\nD\nPre-experience\nStronger responses\nSharper tuning\nStronger+sharper\nSingle unit response\nPopulation response\nA\nB\nC\nD\n\nWith experience, neural responses might change...\ni) selectivities of individual neurons\nWhat changes will\nfMRI\"see\"after\n?\nexperience with\n?\nii) the total # of neurons w/ a certain selectivity\niii) the spatial clustering of selective neurons\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli?\nunder what conditions?\neven in adulthood?\neven in primary visual areas?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each?\nWhat is spatial distribution of experience effects?\nclusters? new areas? distributed changes?\nNext: 5 case studies that address these questions.\n\nCase 1: The Other-Race Effect\nImage removed due to copyright restrictions.\nA pair of portrait photos: African-American and European-American males\nGolby, Gabrieli et al. (2001). Nature Neuroscience, 4, 845-850.\n\nBehavioral Data\nPostscan memory for own-race faces is better than for other-race faces.\nPresumably this is not genetic, but learned.\nDoes this learning affect FFA responses to faces?\nG\nrap\nh s\nhow\ns s\na\nme-race discrimin\nation is\nhigher than other-rac\ne, with a far gre\nater dro\np in other-race discrim\nination for European\nAmerican subjects than for African American subjects.\nFigure by MIT OpenCourseWare. After Golby et al., Nat Neurosci 2001.\n\nfMRI Data\nLocalizer: Faces versus radios\nImage removed due to copyright restrictions.\nMRI activation maps, Figure 2 in Golby et al.,\nNat Neurosci 4, 845-850. doi:10.1038/90565.\nSo: FFA response to a face depends on experience with similar faces.\ni.e., FFA is tuned by experience\nother accounts?\nDo these data indicate that the FFA is not innate?\nGra\np\nh s\nhow\ns 0\n.8%\n\nsignal ch\nange for s\name race, and 0.6% signal\nchange for other race.\nFigure by MIT OpenCourseWare. After Golby et al., Nat Neurosci 2001.\n\nTwo related but importantly different questions:\nOrigins: How does a functionally distinctive region of cortex arise\nduring development?\nWhat role does experience play in wiring up this region?\ne.g., how does the FFA get there in the cortex? (and why\nTHERE?).\nAdult Plasticity: Once a region exists (in adulthood), to what extent\ncan experience alter its response profile?\nFine tuning, or radically different response?\nAre some areas more shaped by adult experience than\nothers? (e.g. FFA? \"higher\" areas more than \"primary\")?\nHow much experience is necessary?\n\nThis increase in response with familiarity could reflect an increase in\ni) selectivities of individual neurons\nChanges fMRI\nwill \"see\"after\n?\nexperience with:\nii) the total # of neurons w/ a certain selectivity\niii) the spatial clustering of selective neurons\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli?\nunder what conditions?\nNot clear when this\neven in adulthood?\nexperience happened.\nCould find out.\neven in primary visual areas? How?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each? up\nWhat is spatial distribution of experience effects?\nclusters? new areas? distributed changes?\nNext: three lab training studies in adults....\n\nCase 2: Training Object Recognition\nEffect of training in backward masking\n(Grill-Spector et al., 2000, Nature Neuroscience)\nImage removed due to copyright restrictions.\nFigure 1 (Sample object photos used in experiment: object epochs vs. scrambled epochs.)\nGrill-Spector, K., et al. \"The dynamics of object-selective activation correlate with\nrecognition performance in humans.\" Nature Neuroscience 3 no. 8 (2000): 837-43.\ndoi:10.1038/77754.\nTask: covert naming.\n\nImage removed due to copyright restrictions.\nFigure 3 graph: sensitivity of recognition and fMRI activation to object duration.\nGrill-Spector, K., et al. \"The dynamics of object-selective activation correlate with\nrecognition performance in humans.\" Nature Neuroscience 3 no. 8 (2000): 837-43.\ndoi:10.1038/77754.\nUse \"percent of max\" to put both behavior and fMRI on the same scale.\nResponse in LO and related regions correlated w/ recognition performance!\n\nImage removed due to copyright restrictions.\nFigure 4 graph: Training effect - psychophysics.\nGrill-Spector, K., et al. \"The dynamics of object-selective activation correlate with\nrecognition performance in humans.\" Nature Neuroscience 3 no. 8 (2000): 837-43.\ndoi:10.1038/77754.\n5-7 days of training on naming these 60 images presented for 40 ms.\nPerformance improved for all subjects.\n\na. Increased response to 40 ms stimulus after vs before training in 2 Ss:\nImage removed due to copyright restrictions.\nFigure 5 set of graphs: Training effect - brain activation.\nGrill-Spector, K., et al. \"The dynamics of object-selective activation correlate with\nrecognition performance in humans.\" Nature Neuroscience 3 no. 8 (2000): 837-43.\ndoi:10.1038/77754.\nb. Training effect across all Ss and ROIs:\nc. Training effect is (partly) specific to the trained stimulus: greater for\ntrained than novel stimuli.\n\nCase 3: How does the response to novel objects (\"Greebles\") change\nw/ training?\n(Gauthier et al., 1999, Nature Neuroscience)\nSequential\nImage removed due to copyright restrictions.\nmatching\nGreebles and sample trials from the sequential-matching task.\nFigure 1 in Gauthier, I., et al. (1999). \"Activation of the middle\ntask used in\nfusiform \"face area\" increases with expertise in recognizing novel\nobjects.\" Nature Neuroscience 2(6): 568-573. doi:10.1038/9224.\nscanner\nExpertise hypothesis: changes should be largely in the rFFA.\n\nMethod:\n1. Define rFFA as an 8x8 voxel square that\nresponds more to faces > objects.\n2. Collect the sum of the t-values for each\nvoxel in this ROI from a comparison of\nupright - inverted faces, and for upright\nminus inverted greebles, longitudinally over\nthe course of training.\n3. Show that this number increases in the\n\"rFFA\" ROI over training w/ greebles.\nWhat is wrong with this picture?\nWhite = Faces\nGrey = Greebles\nImage removed due to copyright\nrestrictions.\nFigure 2d (graph) in Gauthier, I.,\net al. (1999). \"Activation of the\nmiddle fusiform \"face area\"\nincreases with expertise in\nrecognizing novel objects.\"\nNature Neuroscience 2(6): 568\n573. doi:10.1038/9224.\n\"rFFA\"\n\nFaces>objects Greebles>objects\nFaces>objects Greebles>objects\nImage removed due to copyright restrictions.\nSet of fMRI activation maps.\nFigure 4 in Gauthier, I., et al. (1999). \"Activation of\nthe middle fusiform \"face area\" increases with\nexpertise in recognizing novel objects.\" Nature\nNeuroscience 2(6): 568-573. doi:10.1038/9224.\n3 novices\n3 experts\nWhy didn't they show us the same subjects?\n\"images are\nthresholded at the\narbitrary threshold\nof 0.75\".\n\nHow does the response to novel objects (\"Greebles\") change w/ training?\n(Gauthier et al., 1999, Nature Neuroscience)\nImage removed due to copyright restrictions.\nGreebles and sample trials from the sequential-matching task.\nFigure 1 in Gauthier, I., et al. (1999). \"Activation of the middle\nfusiform \"face area\" increases with expertise in recognizing novel\nobjects.\" Nature Neuroscience 2(6): 568-573. doi:10.1038/9224.\nExpertise hypothesis: changes should be largely in the rFFA.\nHave they shown this? How could you do better?\n\nCase 4: Effects of Object Discrim. Training in\nVentral Pathway\nOp de Beeck, H., Baker, C., DiCarlo, J., Kanwisher, N. \"Discrimination Train\ng Alters Object Representa ions in Human Extrastriate Cortex.\"\nJournal of Neuroscience 26 (2006): 13025-36\nOp de Beeck, Baker, DiCarlo & Kanwisher, 2006\nAre object representations fixed, or are they shaped continually by\nexperience?\nAre training effects (if any) found:\nin LOC?\nin rFFA?\nin new discrete region?\nscattered & discontiguous?\nDoes training produce:\nincreases in response?\ndecreases in response?\nincreases in selectivity?\nin\nt\n\nEffects of Object Discrim. Training in Ventral\nPathway\nOp de Beeck, Baker, DiCarlo & Kanwisher, 2006\nShape\ndiscrimination\ntask:\nSmoothies\nSpikies\nCubies\nDesign:\nScan Ss\nbefore & after\n10 hours\nof training on\ndiscrimination\nof one of three\nobject classes.\nReference\nMatch or not ?\nMatch or not ?\nMatch or not ?\nMatch or not ?\n1 s\n1 s\n1 s\n1 s\n300 ms\n300 ms\n300 ms\n300 ms\n300 ms\nOp de Beeck, H., Baker, C., DiCarlo, J., Kanwisher, N. \"Discrimination Training Alters Object Representations in Human Extrastriate Cortex.\"\nJournal of Neuroscience 26 (2006): 13025-36.\nCourtesy of the Society for Neuroscience. Used with permission.\n\nOp de Beeck, H., Baker, C., DiCarlo, J., Kanwisher, N. \"Discrimination Train\ng Alters Object Representa ions in Human Extrastriate Cortex.\"\nJournal of Neuroscience 26 (2006): 13025-36.\nD' as a function of training session\nfMRI Training Effect:\nPSC: (Tr - Untr) after - (Tr - Untr) before\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\nSo: responses to trained objects increase in LOC, not FFA.\nEffects of Object Discrim. Training in Ventral\nPathway\nin\nt\nCourtesy of the Society for Neuroscience. Used with permission.\n\nBut only some of those effects are in LOC - for example:\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\nSpikies\nR\nL\nR\nL\nTrained > untrained\nObjects > scrambled Overlap\nHow do we find replicable and quantifiable training effects when they are\nscattered?\nSig trained >untrained effect in 366 voxels after training, vs 82 before (p< .02).\nEffects of Object Discrim. Training in Ventral\nPathway\n\nEffects of Object Discrim. Training in Ventral\nPathway\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\n>>Replicable two-fold selectivity for trained objects after training (not before).\n3. look at same voxels in the\npre-training session\n2. look in same voxels at other half\nof the data from the\npost-training session\n1. find \"training effect voxels\" in\nhalf of the data from the\npost-training session\nTrained\nUntrained\nHow do we find replicable and quantifiable training effects when they are\nscattered?\n\nEffects of Object Discrim. Training in Ventral\nPathway\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\nAre object representations fixed, or are they shaped continually by\nexperience?\nDoes training produce:\nincreases in response?\ndecreases in response?\nincreases in selectivity?\nAre training effects (if any) found:\nin LOC?\nin rFFA?\nin new discrete region?\nscattered & discontiguous\n\nEffects of Object Discrim. Training in Ventral\nPathway\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\nAre object representations fixed, or are they shaped continually by\nexperience?\nDoes training produce:\nincreases in response?\ndecreases in response?\nincreases in selectivity?\nAre training effects (if any) found:\nin LOC?\nin rFFA?\nin new discrete region?\nscattered & discontiguous: a change in profile of response across\ncortex?\n√\n√\nx\n√\nx\nx\n√\n\nEffects of Object Discrim. Training in Ventral\nPathway\nOp de Beeck, Baker, DiCarlo & Kanwisher, 1996\nAre object representations fixed, or are they shaped continually by\nexperience?\nDoes training produce:\nincreases in response?\ndecreases in response?\nincreases in selectivity?\nAre training effects (if any) found:\nin LOC?\nin rFFA?\nin new discrete region?\nscattered & discontiguous: a change in profile of response across\ncortex?\n√\n√\nx\n√\nx\nx\n√\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli? Yes!\nunder what conditions? A few hours of lab training\neven in adulthood? Yes!\neven in primary visual areas?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each? Up only.\nWhat is spatial distribution of experience effects?\nclusters?\nNo major \"new blobs\".\nnew areas?\nBut maybe a few hours is not enough.\ndistributed changes?\n\nCase 4: Words\nCan a special-purpose bit of brain hardware be\nconstructed from experience,\nwithout any specific genetic blueprint?\nChris Baker, Jia Liu et al (2007), PNAS\nPerfect test case: visually presented WORDS! (Polk & Farah)\n- Experience is on a par with experience with faces.\n- People have only been reading for a few thousand\nyears, not enough for natural selection to create a specialized\nvisual word recognition system.\n- Finding a region selective for visual words would be\nan existence proof that extensive experience can be sufficient.\n- Ongoing debate about \"visual word form area\";\nspecificity and role of experience is unclear.\nRefer to Baker, C., J. Liu, L. Wald, K. Kwong, T. Benner, and N. Kanwisher. \"Visual word processing and experiential origins of functional\nselectivity in human extrastriate cortex.\" PNAS 104, no. 21 (2007): 9087-9092. doi:10.1073/pnas.0703300104.\n\nInitial criterion\nwords\ncoat >\ncalf\nduck\nline drawings\nDidn't see much (groups, individuals, passive, 1-back...)\nWhat about higher resolution?\n\nA small region in the left hemisphere\nshows higher activation for words than\nline drawings\nin > 80% of subjects\nR\nL\np < 10-4\n\nn = 6\nHow selective is this region?\nN=9\n\nn = 6\nHow selective is this region?\nN=9\n\nn = 6\nHow selective is this region?\nQuite!\nN=9\n\nn = 6\nHow selective is this region?\nBut it's\nnot exactly\na VWFA\nIs it\nshaped by\nexperience?\nNeed to\nvary\nexperience\nto know.\nN=9\n\nn = 6\nn = 3\nExperience modulates responsiveness of\nthis region\nNon-Hebrew readers\nHebrew readers\nIs this \"just attention\"?\n\n1-back task, blocked\nNon-Hebrew readers\nHebrew readers\nNot \"just attention\"; experience shapes selectivity.\n\nCase 4: Words\nCan a special-purpose bit of brain hardware be\nconstructed from experience,\nwithout any specific genetic blueprint?\nChris Baker\nYes!\nJia Lia\nUnclear if this is the origin of FFA, PPA, EBA\nPerfect test case: visually presented WORDS! (Polk & Farah)\n- Experience is on a par with experience with faces.\n- People have only been reading for a few thousand\nyears, not enough for natural selection to create a specialized\nvisual word recognition system.\n- Finding a region selective for visual words would be\nan existence proof that extensive experience can be sufficient.\n- Ongoing debate about \"visual word form area\";\nspecificity and role of experience is unclear.\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli? Yes!\nunder what conditions? A few hours of lab training\neven in adulthood? Yes!\neven in primary visual areas?\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each? Up only.\nWhat is spatial distribution of experience effects?\nclusters?\nYes, can get a \"new blob, if\nnew areas?\nexperience is strong enough.\ndistributed changes?\n\nTest Case: Macular Degeneration\nwith Chris Baker\nand Eli Peli\n- Affects 10s of millions of people\nworldwide\n- Retinal disease: destroys the fovea\n- Causes total loss of central vision\nImage removed due to copyright restrictions.\nSee Fig 7 in: Dougherty, R. F., et al.\nJournal of Vision 3, no. 10\n(2003): 586-598.\nto\n(http://www.journalofvision.org/3/10/1/article.aspx).\n- Central 2 degrees of vision maps\nabout twenty cm2 of cortex\nWhat happens to this cortical region in MD when it is deprived of input?\n\nFov.\nconf.\nTest case: Macular Degeneration - Loss of foveal retina in adulthood\n\"Foveal confluence\" in\nnormal subject: 20 cm2\nQ: What happens to this region in\nSubjects with MD and no foveal vision?\nFirst: Activation from peripheral\nStimuli in normal subject\nActivation from peripheral\nstimuli in MD subject\nMajor functional reorganization\nof retinotopic cortex in adulthood!\nBaker, Peli, Knouf, & Kanwisher (2005)\n\nQuestions\n1. Does experience affect fMRI responses to learned stimuli? Yes!\nunder what conditions? A few hours of lab training\neven in adulthood? Yes!\neven in primary visual areas? Yes!\n2. How does experience affect fMRI responses?\nWhat kind of fMRI changes are predicted from different\nkinds of neural changes?\nDoes the fMRI response go up? Down? Some of each? Up only.\nWhat is spatial distribution of experience effects?\nclusters?\nYes, can get a \"new blob, if\nnew areas?\nexperience is strong enough.\ndistributed changes?\n\n- Is all this structure fixed, or does it depend on experience?\n- Dynamic system: new areas arise & functional props can change. Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nMany different time scales of experiential effects:\nSome very fast:\nDolan RJ, Fink GR, Rolls E, Booth M, Holmes A, Frackowiak RS, Friston KJ. (1997). How the brain learns to see\nobjects and faces in an impoverished context. Nature. 1997 Oct 9;389(6651):596-9.\nDemo....\n\nSequence of three slides deleted due to copyright restrictions.\nFaces: pre-learning, learning, and post-learning\nFig. 1 in Dolan RJ et al. (1997). \"How the brain learns to see objects and\nfaces in an impoverished context.\" Nature 389 no. 6651(1997): 596-9.\ndoi:10.1038/39309.\n\nMany different time scales of experiential effects:\nSome very fast:\nDolan RJ, Fink GR, Rolls E, Booth M, Holmes A, Frackowiak RS, Friston KJ. (1997). How the brain learns to see\nobjects and faces in an impoverished context. Nature. 1997 Oct 9;389(6651):596-9.\nFound: higher responses in ventral temporal regions associated\nwith face processing after disambiguation than before.\nCan you think of another very rapid experiential effect in fMRI?\n\n1. Does experience affect fMRI responses to learned stimuli?\nYes it can; there are many cases of this.\n2. Does the response go up? Down? Some of each?\nOften responses go up with training.\nBut in adaptation they go down.\n3. What is spatial distribution of experience effects?\nclusters? new areas? distributed changes\nLab training (hours) neither uniformly distributed nor clustered.\nWords: some clustering. (other kinds of expertise?)\n4. Time scale of changes?\nCan be short or fast. Probably different mechanisms.\n\nTwo related but importantly different questions:\nOrigins: How does a functionally distinctive region of cortex arise\nduring development?\nNo one knows!\nWhat role does experience play in wiring up this region?\ne.g., how does the FFA get there in the cortex? (and why\nTHERE?).\nNo one knows! But specialization for words\nsuggests that a specific genetic predisposition for a\nparticular stimulus class may not be necessary.\nAdult Plasticity: Once a region exists (in adulthood), to what extent\ncan experience alter its response profile?\nFine tuning, or radically different response? Some of each.\nAre some areas more shaped by adult experience than\nothers? (e.g. FFA? \"higher\" areas more than \"primary\")?\nNot only higher areas.\nHow much experience is necessary?\nEven a small amount is enough to change responses...\n\nTraining studies:\nIncreases in population response as a result of experience.\nExamples: - Greeble training (Gauthier et al., 1999)\n- Backward masking (Grill-Spector et al., 2000)\n\nPriming\nRepetition leads to:\n1) Faster RTs\n2) Increased accuracy\ne.g. symmetry judgment, size judgment\n\nRepetition Suppression in Neurons\nLi, Miller and Desimone (1993). Journal of Neurophysiology,\n69, 1918-1930.\nCourtesy of the American Physiological Association. Used with permission.\n\nLi, Miller and Desimone (1993). Journal of Neurophysiology,\n69, 1918-1930.\nCourtesy of the American Physiological Association. Used with permission.\n\nPriming and fMRI\nThree images removed due to copyright restrictions.\nFigures 1, 2, and 3 from Henson, R., et al. (2000). « Neuroimaging Evidence\nfor Dissociable Forms of Repetition Priming.\" Science, 287, 1269-1272.\ndoi: 10.1126/science.287.5456.1269\nHenson et al. (2000). Science, 287, 1269-1272\n\nConclusions\n- Many different time-scales over which\nexperience modifies both behavior and\nneural processing\n- Likely to be multiple mechanisms\n\nA\nB\nC\nD\nPre-experience\nStronger responses\nSharper tuning\nStronger+sharper\nSingle unit response\nPopulation response\nA\nB\nC\nD\n\nMany different time scales:\n- Experience throughout development\nexample: face recognition ?\nMaybe not a good example !\n(evolutionary arguments)\n\nMany different time scales:\n- Expertise acquired in adulthood\nExamples: car and bird experts\n\nMany different time scales:\n- Adaptation\nVery short timescale\nExample: McCullough effect\n\nWith experience, neural responses might change...\ni) selectivities of individual neurons\nWhat changes will\nfMRI\"see\"after\nexperience with\nii) the total # of neurons w/ a certain selectivity\niii) the spatial clustering of selective neurons\n\n\"Visual Word Form Area\"\nImage removed due to copyright restrictions.\nFigure 1 in Cohen, L. et al. \"Language-specific tuning of visual cortex? Functional\nproperties of the Visual Word Form Area.\" Brain 125, no. 5 (2002): 1054-1069.\nhttp://brain.oxfordjournals.org/cgi/content/abstract/125/5/1054\nCohen et al. (2002). Brain, 125, 1054-1069.\nAlso: Cohen and Dehaene (2004). NeuroImage, 22, 466-476\n\nMany different time scales:\n- Perceptual learning throughout multiple training sessions\nMany studies in low- and high-level vision\nExample: greeble training\nTwo greeble mages removed due to copyright restrictions.\n\nExperience may affect responses over many different time scales:\n- Experience throughout development (faces, letters)\n- Expertise acquired in adulthood (e.g., car experts)\n- Perceptual learning throughout multiple training sessions\n- Priming\n- Adaptation\nUnresolved question:\nTo what extent distinct mechanisms at different time scales ?\n\n\"Visual Word Form Area\"\nImage removed due to copyright restrictions.\nFigure 2 in Cohen, L. et al. \"Language-specific tuning of visual cortex? Functional\nproperties of the Visual Word Form Area.\" Brain 125, no. 5 (2002): 1054-1069.\nhttp://brain.oxfordjournals.org/cgi/content/abstract/125/5/1054\nCohen et al. (2002). Brain, 125, 1054-1069.\nAlso: Cohen and Dehaene (2004). NeuroImage, 22, 466-476\n\nImage removed due to copyright restrictions.\nFigure 4 in Cohen, L. et al. \"Language-specific tuning of visual cortex? Functional\nproperties of the Visual Word Form Area.\" Brain 125, no. 5 (2002): 1054-1069.\nhttp://brain.oxfordjournals.org/cgi/content/abstract/125/5/1054\nAttention?"
    },
    {
      "category": "Resource",
      "title": "Face perception and the FFA",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/d785ff626e49a47e3fcd52e20fc8f467_lec5b_faces_ip.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n9.71 Class 5 October 4\nOutline for Today\n5A: Bare Basics of Visual Neuroanatomy\nThe Visual Pathway\nNeuranatomical Landmarks\n5B: Face Perception and the FFA\n<break>\nStudent Presentations\nGrace presents Avidan et al (2005)\nRavi presents Gauthier et al (2000)\nPeter presents Rotshtein et al (2005)\n\nFace Perception\nFaces are particularly important stimuli because\n- they convey many kinds of critical information:\nidentity, age, sex, mood, and direction of attention\n- faces are among the stimuli we look at most frequently in daily life\n- the ability to perceive faces was probably critical\nto the survival of our primate ancestors\nEvidence that special mechanisms may be used in face perception from:\nlesions, neurophysiology, behavior, ERPs, MEG and fMRI\n\n1. Prosopagnosia\n- many reported cases (several dozen ?)\n- lesion in inferior temporal cortex\n- Impairs face discrimination & recognition, not face detection\n- In rare cases where lesion is small, deficit can be very specific,\nleaving object recognition intact.\ne.g. the patient of Wada & Yamamoto, 2001\nImage removed due to copyright restrictions.\na double dissociation of face and object recognition.\nFigure 1 in Wada, Y. and T. Yamamoto. \"Selective Impairment of Facial Recognition\ndue to a Haematoma Restricted to the Right Fusiform and Lateral Occipital Region.\" J Neurol\nNeurosurg Psychiatry 71 (2001): 254-257.\n\n2. Face-Selective Neurons in Macaque IT\nBruce, Perrett, Desimone, Gross, Tanaka, and many others\nSource of Slide: Jody Culham\nImage removed due to copyright restrictions. Diagram of responses to different faces.\n\n3. Face-Selective electromagnetic responses\na. Subdural ERPs\n2 A\nB\nC\n\nABC\n\nLMT\nRMT 4\n\nLMT\nRMT 3\n\nLMT 10\nRMT 2\n\nFaces\nFace\ns\nFlowers\nCars\nNouns\nScr\nFaces\nScr Faces\nNumbers\nTarget\nButte\nrflies\n\nLMT 9\nRMT 1\n\n-500\n\n-100\n0 1\n0 2\n0 5\nCourtesy of Gregory McCarthy. Used with permission.\nSource: Greg McCarthy\n\n3. Face-Selective electromagnetic responses\nb. The MEG \"M170\"\nImage removed due to copyright restrictions.\nModification of Fig. 1b in Liu, J., Harris, A. and Kanwisher, N.\n\"Stages of processing in face perception: an MEG study.\" in\nNature Neuroscience 5, no. 9 (2002): 910-916.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/LiuHarrisKanwisherNN02.pdf\n\n4. Brain Regions Involved in Face Perception\nAmygdala:\nFFA: Perceptual\nOFA\nSTS:\nEmotional\nexpression,\nGaze\ndirection....\nRecognizing\nexpressions of fear/anger.\nanalysis of faces\nFigure by MIT OpenCourseWare.\n\nCollateral Sulcus (red) and Fusiform Gyrus (pink)\nImage removed due to copyright restrictions. See Jody Culham's slides on Cortical Sulci\np. 6 in http://psychology.uwo.ca/fmri4newbies/Tutorials/9_Louvain_Cortical%20Sulci.ppt\n\n4. Fusiform Face Area\nKanwisher, Tong, McDermott, Chun, Nakayama, Moscovitch, Weinrib, Stanley, Harris, Liu\nCartoon\nFace photos modified by OCW\nfor privacy considerations.\nSpecificity\nGenerality\nFront-View\n19-2.3\nInv. Grey\n1.6\n1.3\nEyes Only\nHand\n0.7\nProfile-View\n1.8\n\"Mooney\"\n2.0\n1.7\nNo Eyes\nImage removed due to\ncopyright restrictions.\nCat Face\n1.6\nHuman Head\n1.7\nBack of Head\n0.9\nInv. Mooney\n1.3\n1.0\nWhole Animal\nBuildings\n0.6\nAnimal Head\n1.3\n1.7\nInv. Cartoon\nImage removed due to\ncopyright restrictions.\n1.4\nSchematic\n0.9\nHuman Body\n1.0\nObject\n0.6-1.1\nAnimal Body\n0.8\nFace photos modified by OCW\nfor privacy considerations.\nCourtesy of Society for Neuroscience. Used with permission.\n\n5. Behavioral Signatures of Face Perception\n- A. The \"face inversion effect\": a greater decrease in\nperformance for upside-down compared to upright\nstimuli for faces than other stimuli (Yin, 1969).\n5.0\n(Yin, 1969)\n4.0\nUpright\nInverted\n3.0\n2.0\n1.0\n0.0\nFaces\nHouses\nStick Figures\nMean Errors\n\nWhole-part effect\n(Tanaka and Farah, 1993)\nSubjects are better\nComposite face effect\n(Young et al., 1987)\n5. Behavioral Signatures of Face Perception\n- B. Holistic Processing: Mandatory processing of the\nwhole face, and interactive processing of face parts.\nable to discriminate\nSubjects are slower\nparts in the context\nto identify half the\nof the whole face\nface when it is\nthan when\naligned than\npresented alone.\nmisaligned (cannot\nImage removed due to copyright restrictions.\nignore whole). Courtesy of Andy Young. Used with permission.\nNeither effect is found at all for inverted faces!\n\n5. Behavioral Signatures of Face Perception\n- C. Newborn Infants Preference for Faces Suggest\nInnateness.\n- Johnson & Morton (1991): look longer at schematic faces than\ninverted schematic faces. >> may lead to greater experience\nwith faces and hence cortical development of \"special\" face\nmechanisms.\n- Simion et al (2002): They look more at any topheavy stimulus,\ne.g.:\nSimion argues this is therefore not really a face mechanism,\nbut in practice it may serve to pick out faces for infants.\n\nSo: lesions, neurophysiology, behavior, ERPs, MEG and fMRI\nall suggest that special mechanisms may be used in face perception\nProbably multiple face-processing mechanisms,\nunclear which methods are measuring the same thing\ne.g. does the M170 come from the FFA?\nFocus for today: the FFA.\n\nQuestions about the FFA:\n1. Does the FFA process faces only (the \"Face Specificity Hypothesis\")?\nOr is it also engaged in processing nonface objects:\n- via overlapping \"distributed codes\" (Haxby)\n- individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n\nWho Cares?\nLocalization of Function in the Brain\nThe brain is not a homogeneous and undifferentiated\nmush in which all the bits are \"equipotential\". Rather, at least\nsome mental functions are physically segregated (to at least\nsome degree) in the brain.\nLoF is uncontroversial for primary sensory & motor cortex,\nbut a debate has long raged concerning the degree to which it\nis also true of high-level cognition....\nDomain Specificity of Cognition\nIs the mind composed of special mechanisms for specific\ndomains of cognition - e.g. faces, language, number, etc.?\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n- via overlapping \"distributed codes\" (Haxby)\n- individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- Gaze discrimination? Expression?\n- processing face parts?\n- face configurations?\n- does it reflect physical or psychological similarity btwn faces?\n\nControversies and Questions about\nCategory-selective Regions of Cortex\nAlternative view I: The brain is not organized around content domains\n(e.g., faces or places), but instead around processes (e.g. fine-grained\ndiscrimination) that can be conducted on any stimulus type.\nAlternate view II: faces, places, and objects are represented not by focal\nregions of cortex, but by distributed patterns of activation spanning\ncentimeters of cortex.\nIs face information spread far beyond the FFA?\nDoes the FFA contain information about nonfaces?\n\nNonpreferred Responses in the FFA\nF\nF\nF\nO\nO\nO\nFaces > Objects\n% signal change\n?!\nTime (seconds)\nCourtesy of Society for Neuroscience. Used with permission.\n- Do \"nonpreferred\" responses carry information about nonpreferred stimuli?\n- A potential challenge to the domain specificity of the FFA.\nUsing Haxby's method to ask whether the FFA contains info about nonfaces....\n\nCorrelation-based Classification Analysis (Haxby et al., 2001)\n1. Scan each subject while they view multiple stimulus categories.\n2. Split the data in 1/2; generate activation maps for each category.\n3. Compute correlation across activation maps.\nWithin\ncategory\nbetween\ncategories\nIf r(Within) > r(Between)\nWhat do we find for\nnonfaces in the FFA?\nthe region contains category information\n\nDoes the Pattern of Response Across the FFA\ncontain information that discriminates between nonfaces?\nHaxby et al (2001): yes\n\"Regions such as the .... 'FFA' are not dedicated to representing only .... human\nfaces,.. but, rather, are part of a more extended representation for all objects.\"\nSpiridon & Kanwisher (2002): no\nTsao et al (2003), in face patches in monkey brains: no\nO'Toole, Haxby et al. (2005): no (sort of):\n\"preferred regions for faces and houses are not well suited to object classifications\nthat do not involve faces and houses, respectively.\"\nReddy & Kanwisher (submitted): yes (sort of).\nBUT: maybe these tests are unfair, in two ways:\ni) Spatial resolution limits of fMRI necessarily entail some\ninfluence of neural populations outside the region in question.\nii) The presence of discriminative information does not mean it\nplays an important role in perception!\n\nThe Ultimate High Resolution: Single-Unit\nNeurophysiology\nTsao et al (2003, NN) fMRI in monkeys:\nImage removed due to copyright restrictions.\nDiagram of macaque brain surface, highlighting middle face patch and body response regions.\nIn Kanwisher, N. \"What's in a Face?\" Science 311 no. 5761 (2006): 617-618.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/Kanwisher.science2006.perspec.pdf\n\nResponse of all 320 visually-responsive neurons\nin the faces patches of two monkeys\nto 96 different stimuli\nMonkey 1\nMonkey 2\nImage removed due to copyright restrictions.\nFig.2B in Tsao, Doris. \"A Cortical Region Consisting\nEntirely of Face-Selective Cells.\"\nScience 311 no. 5761 (2006): 670-674.\ndoi:10.1126/science.1119983.\nThe cells in this patch respond selectivity, indeed virtually exclusively to faces.\nTsao et al (2006), Science\n\nA Basic fMRI Experiment\nKanwisher et al (1997)\nF\nF\nF\nO\nO\nO\nFaces > Objects\n% signal change\n?\nTime (seconds)\nCourtesy of Society for Neuroscience. Used with permission.\n- Selectivity looks pretty strong, but....\n- Recall that we typically have hundreds of thousands of neurons per voxel.\n- How strong is selectivity at the level of single units?\nVERY! (at least in macaques)\nNot much room for \"overlapping codes\"\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\nx\n- via overlapping \"distributed codes\" (Haxby)\n- individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- Gaze discrimination? Expression?\n- processing face parts?\n- face configurations?\n- does it reflect physical or psychological similarity btwn faces?\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\nx\n- via overlapping \"distributed codes\" (Haxby)\n- individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- Gaze discrimination? Expression?\n- processing face parts?\n- face configurations?\n- does it reflect physical or psychological similarity btwn faces?\n\nCorrelating fMRI signals with behavioral outcomes\nGrill-Spector, Knouf, & Kanwisher (2004)\nOverall Strategy:\nHave Ss perform perceptual task in scanner;\nMake task difficult so subjects make some mistakes;\nbin fMRI data by behavioral response;\nLook for correlations btwn behavioral responses and fMRI signal.\n\n4000ms\nimage exposure\n2000 ms\nFace photos modified by OCW\nfor privacy considerations.\nTask: is this\ni) Harrison Ford\nii) some other guy\niii) nothing\ntime\n2000ms\n33ms or 50ms:\nNear threshold\nCourtesy of K. Grill-Spector. Used with permission.\nStimulus images not repeated\n\nGrill-Spector, Knouf, & Kanwisher (2004)\nFigures from:\nArea Subserves Face Perception, not Generic Within-Category Identification.\"\nNat Neurosci 7, no. 5 (May 2004): 555-62.\n\nGrill\n-\nSpector K, N. Knouf, and N. Kanwisher. \"The Fusiform Face\n\nRight FFA Response to Target Faces (e.g., Harrison)\nAs a Function of Performance, N=5\nIdentification\nCategori-\nzation/\nDetection\n0.6 -\nPercent Signal Increase\n|\n6 sec\nTime\nIdentified\n(\"Harrison\")\ndetected but\nnot identified\n(\"someone else\")\nnot detected\n(\"nothing\")\nFFA Involved in:\nDetec & Ident of faces\nWhat about other\nkinds of objects?\nCourtesy of K. Grill-Spector. Used with permission.\nGrill-Spector, Knouf, & Kanwisher (2004)\nFigures from: Grill Spector K, N. Knouf, and N. Kanwisher. \"The Fusiform Face\nArea Subserves Face Perception, not Generic Within-Category Identification.\"\nNat Neurosci 7, no. 5 (May 2004): 555-62.\n\n-\n\n4000ms\n2000 ms\nCourtesy of K. Grill-Spector. Used with permission.\ntime\n2000ms\nTask: is this\ni) electric guitar\nii) other guitar\niii) nothing\nimage exposure\n33ms or 50ms\nStimulus images not repeated\nGrill-Spector, Knouf, & Kanwisher (2004)\nFigures from:\nArea Subserves Face Perception, not Generic Within-Category Identification.\"\nNat Neurosci 7, no. 5 (May 2004): 555-62.\n\nGrill\n-\nSpector K, N. Knouf, and N. Kanwisher. \"The Fusiform Face\n\nRight FFA Response to Target Faces or Guitars\nAs a Function of Performance, N=5\nfaces\nguitars\n0.6 -\nPercent Signal Incr\n|\n6 sec\nTime\n0.6 -\nPercent Signal Incr\n|\n6 sec\nTime\nIdentified\ndetected but\nnot identified\nnot detected\nFFA involved in:\nDetec & Ident of faces\nNeither detect nor ident\nOf guitars\nCourtesy of K. Grill-Spector. Used with permission.\nGrill-Spector, Knouf, & Kanwisher (2004)\nNat Neurosci 7, no. 5 (May 2004): 555-62.\n\nFigures from: Grill-Spector K, N. Knouf, and N. Kanwisher.\n\"The Fusiform Face Area Subserves Face Perception, not\nGeneric Within-Category Identification.\"\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n√ - face detection?\n√ - Identification?\n- Gaze discrimination? Expression?\n- processing face parts?\n- face configurations?\n- does it reflect physical or psychological similarity btwn faces?\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\n- individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n√ - face detection?\n√ - Identification?\n- Gaze discrimination? Expression?\n- processing face parts?\n- face configurations?\n- does it reflect physical or psychological similarity btwn faces?\n\nExpertise Hypothesis\n(Gauthier & Tarr; Carey & Diamond)\nFace-selective neural mechanisms are not specialized for pro\ncessing faces per se, but rather for processing any class of visual stimuli\ni) for which the subject has gained substantial visual expertise and\nii) upon which the subject is making fine-grained \"subordinate\nlevel\" discriminations between exemplars that share the same basic\nconfiguration.\nLots of evidence against this idea - e.g.:\nDouble dissociation betwn face recognition and expertise on nonfaces.\nStrongest evidence for it: Gauthier et al, (2000) & Xu (2005)\nRavi will present Gauthier et al (2000)\nBUT....\n\nignal\nFFA Response in 5 Car Experts\n(c) FFA response for cars as a\nfunction of expertise\n% s\nsubject\nsubject\n(a) Faces\n(b) Cars\ntime[s]\ntime[s]\nidentification\ndetection\ndetection\nhit\nhit\nmiss\nNo evidence that FFA response is correlated trial-by-trial\nwith successful identification of objects of expertise.\nCourtesy of K. Grill-Spector. Used with permission.\n0.2\n0.1\n0.0\n-0.1\n-0.2\n-0.3\nR2 = 0.0526\nd'\nNo evidence that FFA response\nduring car identification\nis correlated across subjects\nwith car expertise.\nSo: FFA apparently involved in\ndetection and identification of\nfaces but not detection or\nidentification of other objects,\nincluding objects of expertise.\nFigures from Grill-Spector K, N. Knouf, and N. Kanwisher. \"The Fusiform Face\nArea Subserves Face Perception, not Generic Within-Category Identification.\"\nNat Neurosci 7, no. 5 (May 2004): 555-62.\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n√ - face detection?\n√ - Identification?\n- processing face parts?\n- face configurations?\n- Gaze discrimination? Expression?\n- does it reflect physical or psychological similarity btwn faces?\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\n- \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n√ - face detection?\n√ - Identification?\n- processing face parts?\n- face configurations?\n- Gaze discrimination? Expression?\n- does it reflect physical or psychological similarity btwn faces?\n\nYovel & Kanwisher (2004)\n1. Does the FFA primarily process information about face\nconfigurations (rather than face parts)?\n2. Is the FFA truly face-specific, or can it be engaged on nonface stimuli\nif we force subjects to process those stimuli like faces?\nSuppose we get subjects to process a nonface stimulus\nin the same way they process a face, then will the FFA become engaged?\n\nPart versus Configuration Discrimination Tasks\nFace part\nFace\nOn Faces\nConfiguration\nNew Text\nCourtesy Elsevier, Inc., http://www.sciencedirect.\ncom. Used with permission.\nHouse part\nHouse\nOn Houses\nConfiguration\n\nrFFA Response\nYovel & Kanwisher (2004), Neuron\n0.3\n0.5\n0.7\n0.9\n1.1\n1.3\nFaces\nHouses\nFFA response\nConfiguration\nPart\nCourtesy Elsevier, Inc., http://www.sciencedirect.\ncom. Used with permission.\nIs this a case of shallow neural domain specificity\n- FFA is equally\nengaged in part\nand config.\ndiscriminations\non faces.\n- FFA is NOT\nstrongly\nengaged when\nsubjects do very\nsimilar\ndiscriminations\non nonface\nstimuli.\n- FFA is\nstimulus-\nspecific, not\nprocess-\nspecific.\nwithout a corresponding functional domain specificity? Test with...\n\nFace inversion effects\n1.1\nUpright\nInverted\n- fMRI inversion\neffects for faces\n0.9\n(both tasks),\n0.8\nmirroring\nrFFA\n0.7\nbehavioral data.\nresponse\n0.6\n0.5\n0.4\n0.3\nAre these\nConfiguration\nPart\ninversion\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission. effects linked?\n0.9\nUpright\nInverted\n0.8\nBehavioral\n0.7\n- Behavioral\ninversion effects\nperformance\n0.6\nfound for faces\nin the scanner\n0.5\n(both tasks), not\nfor houses.\n0.4\n0.3\nConfiguration\nPart\nYovel & Kanwisher (2004), Neuron\nAccuracy\nPct Signal Change\n\nIf behavioral inversion effects originate in the FFA,\nwe should find a correlation across subjects between\nfMRI inversion effects and behavioral inversion effects.\nFFA inversion effect\n. 6\n. 5\n. 4\n. 3\n. 2\n. 1\n0 .0\n- . 1\n- . 2\n- . 3\n- . 4\n. 3\n. 2\n. 1\n0 .0\n- .1\nr = .63\nBe h a vior (a c c u r a c y)\nBehavioral inversion effect\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nThis correlation implicates FFA in the face inversion effect.\nYovel & Kanwisher (2004), Neuron\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\nx - \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- processing face parts?\n- face configurations?\n- does it reflect psychological (vs. physical) similarity btwn faces?\n- Gaze discrimination? Expression?\n√\n√\n√\n√\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\nx - \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- processing face parts?\n- face configurations?\nPeter will present a paper on this....\n- does it reflect psychological (versus physical) similarity btwn faces?\n- Gaze discrimination? Expression?\n√\n√\n√\n√\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\nx - \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- processing face parts?\n- face configurations?\n- does it reflect psychological (vs. physical) similarity btwn faces?\n- Gaze discrimination? Expression?\n√\n√\n√\n√\n√\n\nIdentity, Expression, & Gaze in\nthe FFA and STS\nHoffman & Haxby (2000):\nmanipulated selective attention to gaze vs identity on same stimuli\nLine\ndrawi\nng o\nf side human brain.\n- FFA: identity > gaze\n- STS (prob pSTS): gaze > identity (weakly)\nWinston et al (2004): used event-related fMRI adaptation\n- FFA: identity, not expression\n- pSTS: identity and expression\n- mSTS: expression, not identity\nCourtesy of the American Physiological Association. Used with permission.\nSource: Winston, J. S. et al. J Neurophysiol 92 (2004): 1830-1839. doi:10.1152\nDeveloping consensus:\n/jn.00155.2004.\n- \"STS\" codes changeable properties of face (expression & gaze)\n- FFA codes face identity.\n\nQuestions about the FFA:\n1. Is the FFA \"domain-specific\" for faces (\"Face Specificity Hypothesis\")?\nOr is it engaged in \"domain-general\" processing :\n+ - - via overlapping \"distributed codes\" (Haxby)\nx - individuation of exemplars within any category\nx - individuating exemplars of objects of expertise\nx - \"configural\" processing\n- \"holistic\" processing\n2. What does the FFA do with faces?\n- face detection?\n- Identification?\n- processing face parts?\n- face configurations?\n- does it reflect psychological (vs. physical) similarity btwn faces?\nX (?) - Gaze discrimination? Expression?\n√\n√\n√\n√\n√\n\nImportant Open Questions\n- What is the nature of the representations in the FFA?\ninvariances to viewpoint, expression, etc?\n(how) do they differ from representations of objects?\n- How are these representations extracted?\n- Does FFA hold any information about nonfaces? Is this information used?\n- Division of labor between FFA and other areas?\nConnections and interactions between these areas?\n- Where and how are face percepts matched to memories (recognition)?\n- How do FFAs arise in development?\nWhy in same place?\nWhy cant this area \"move over\" in prosopagnosia?\nWhy do we have FFA and not FCA (fusiform chair area)?\n\nQuestion 2: Is the FFA necessary for face recognition??\nMaybe!\nBarton, Press,\nKeenan, & O'Connor\nTested four patients with\nlesions in the region of the\nFusiform face area.\nAll were severely\nimpaired in discriminating\nFaces that differed in terms\nof changes in the spatial\npositions of features.\n\nTh\nis di\nagra\nm sho\nws\nthe r\nes\npo\nnse\ns i\nn Right FF\nA\nfo\nr F\nace\n, Car and\nObjects.\nFigure by MIT OpenCourseWare.\n- Why this small response to cars in car experts, if it isnt correlated\nwith car identification?\n- Attentional modulation, rather than perceptual identification of cars?\n\nIf this region is truly engaged in perceptual identification of cars\n(rather than simply getting modulated by attention), we should see expertise\neffects at early latencies when perceptual identification occurs, i.e. the M170.\nTest: Is M170 response higher to cars than control objects in car experts?\nSensor of Interest Approach: Find face-selective sensors in each subject,\nthen measure the magnitude of response in these sensors while subjects\nview cars, faces, and objects.\n\n0.2\n0.4\n0.6\n0.8\n1.2\nExperts\nControls\nNormalized Response Amplitude\nFaces\nCars\nShoe\nCourtesy Elsevier, Inc.,\nhttp://www.sciencedirect.com.\nUsed with permission.\nSource: Xu Y., Liu, J., and Kanwisher,\nN. (2005) \"The M170 is selective for\nfaces, not for expertise.\" Neuropsychologia\n43: 588-97\n\n0.2\n0.4\n0.6\n0.8\n1.2\nExperts\nControls\nNormalized Response Amplitude\nFaces\nCars\nShoe\nCourtesy Elsevier, Inc.,\nhttp://www.sciencedirect.com.\nUsed with permission.\nSource: Xu Y., Liu, J., and Kanwisher, N.\n(2005) \"The M170 is selective for faces, not\nfor expertise.\" Neuropsychologia 43: 588-97\nNo expertise effect on the M170. (And no interaction).\nThis mechanism is selective for identification of faces, not any expert category.\nIs M170 correlated trial-by-trial with car identification in car experts?\n\n1.5\n0.5\nFace Identification\nFace Identification\nCar Identification\nFailure\nSuccess\nCar Identification\nFace Selective M 170 Amplitude in Car Experts\n(Error bars\nshow the SD of\nthe differences\nbetween Success\nand Failure)\np < 0.0001\np > 0.20\nSignificant interaction of faces versus cars and identification success (p < 0.02).\nM170 does not show expertise effect. Suggests the small FFA expertise effect in some\nstudies may reflect attentional modulation and/or postidentification processing.\n\nGeneral Questions Addressed Today\nWhat does the FFA do with faces?\n\n√\n√\n√\n√\n√\n√\n- process individual face parts (eyes, nose, mouth)?\n- extract the configuration of the face (relative position of parts)?\n- face detection?\n- face identification?\nSpecificity: Does the FFA process faces only? Or is it also engaged in:\nX - configural processing of any stimulus type\nX X - individuation of any stimulus for which the subject is expert\n- \"holistic\" processing of any stimulus type\n\nStrongest car expert shows no correlation with success at\ncar identification in rFFA\nred= correct identification\nblue= detection w/out identification\nblack = not detected\nLeft = lFFA\nRight = rFFA\n\nIs the FFA really Specialized for Face Perception?\nSome Hypotheses concerning FFA function:\n- perception of bodies (e.g., Peelen & Downing, 2005)?\n- within-cat. discrim. of objects of expertise (Gauthier, Tarr)\n- within-cat. discrim. of other categories (e.g., chairs)\n- face recognition (discriminating one individual from another)\nPartial voluming of two adjacent selectivities?\nLet's try scanning at higher resolution\n8 channel coil\n1.4x1.4x2mm voxels\nSpiridon, Fischl, & Kanwisher (in press), HBM.\nGraph removed due to copyright restrictions.\nSee Fig. 4 (left) in Spiridon, M., B. Fischl,\nand N. Kanwisher. \"Location and Spatial\nProfile of Category-Specific Regions in Human\nExtrastriate Cortex.\" Human Brain Mapping 27\n(2006): 77-89.\n\nfaces > objects\nbodies > objects\noverlap\nSee Schwarzlose, Moore and Kanwisher\nJ Neurosci 25 no. 47 (2005): 11055-11059.\nFusiform Responses to Faces & Bodies\nFace-Only Region\nBody-Only Region\nN=9\nN=9\nAt high resolution, face selectivity\ncan be dissociated from body\nselectivity in the fusiform.\n\nCategorical Perception of Faces\n\"within\"\nRotshtein et al (2004)\nCourtesy of Pia Rotshtein. Used with permission.\nCP: Do subjects detect\ndifferences better when\nthey straddle a category\n\"between\"\nboundary than when\nthey do not?\nDoes FFA represent physical\nor psychological distance?\nYes!\n\nCategorical Perception of Faces\nRotshtein et al (2004)\nfMRI:\nEvent-related\nFFA\nadaptation\nOFA\nexperiment.\nFFA represents perceived\n(categorical) differences\nbetween faces.\nOFA represents physical\ndifferences btwn faces.\nCourtesy of Pia Rotshtein. Used with permission.\n\nSAME\nDIFFERENT\nUsing fMRI-adaptation to ask\nwhat stimulus differences the FFA is sensitive to.\nYovel & Kanwisher, submitted\n250ms\n500ms 250ms\nFFA Response\n-0.2\n-0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\nDifferent\nSame\nSo: FFA can discriminate btwn faces.\nCounterbalanced across stimuli;\nIs this specific to faces?? What about...\nDisplaced slightly; matching task.\n\nSAME\nDIFFERENT\n200ms\n500ms 200ms\n\nInverted Face\nUpright Chair\nDifferent\nSame\nFFA is sensitive\n0.7\n0.6\nto diffs betwn\n0.5\nupright faces,\n0.4\nnot chairs.\n0.3\nFFA\n0.2\n0.1\nApples & oranges!\nUpright Face\n% signal change\nDoes salience\naccount for\nface specificity?\nNo.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n% signal change\nDifferent\nSame\nLOC\nUpright Face\nInverted Face\nUpright Chair\nArea(FFA/LOC) x Stimulus (Face/InvFace/Chair) x Adaptation (Diff/Same) is p < .02"
    },
    {
      "category": "Resource",
      "title": "fMRI of High-level Vision",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/484d80b0eddc696e1d6386342cd9df0e_lec1_intro.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n9.71: fMRI of High-level Vision\nNancy Kanwisher\nFall 2007\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. Localization of Function\nIV. What is High-Level Vision?\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. Localization of Function\nIV. What is High-Level Vision?\n\nWhat is fMRI?\n(functional Magnetic Resonance Imaging)\nImage: NIH\n- FAST:\n10+ images/ sec\n- NEURAL\nACTIVITY\n\"BOLD\" (blood oxygenation level dependent) signal:\nIncreased neural activity >\nIncreased local blood flow more than compensates for O2 use >\ndecrease in deO2Hb concentration>\nincrease in MR signal intensity (deO2Hb is paramagnetic)\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. What is High-Level Vision?\nIV. Localization of Function\n\nQuestion:\nAre there any parts of the brain that are specialized\nfor perceptually processing faces, more than other kinds\nof visual stimuli?\nWhere are they?\n\nStimulus Sequence\n\"blocked\" d\nesign\n45 faces\nfix\nfix\nfix\nfix\nfix\nfix\nfix\n45 objs\n45 faces\n45 objs\n45 faces\n45 objs\n30s\n30s\n30s\n30s\n30s\n30s\nFace photos modified by OCW for privacy considerations.\n- A single scan = 5.5 minutes long\n- Presentation rate = 1.5\npictures/second\n\nfMRI Data\nChose slice number,\nEach functional image\nposition, thickness,\nof one slice is at least\nand orientation\n64 x 64 voxels\nAn image is made of each slice e.g. every 2 seconds (TR=2)\n\nThis makes a \"movie\" of each slice, in which\nthe MRI signal intensity at each position\nand time is represented as the\nbrightness of the corresponding\nvoxel:\nTR\ntime\n\nfMRI Data\nEach voxel has a \"time course\" like this:\n1 sample per \"TR\"\nMR\nSignal\nOf one\nvoxel\ntime\nthan during the object epochs.....\nWe statistically test each voxel to see if it produced\na stronger response, e.g. during the face epochs\n\nStatistical analysis on each voxel\nTime\nCourse\nFrom\nOne\nvoxel\nWhat\nThe\nSubject\nSaw:\nprivacy considerations.\n45 faces\nfix\nfix\nfix\nfix\nfix\nfix\nfix\n45 objs\n45 faces\n45 objs\n45 faces\n45 objs\n30s\n30s\n30s\n30s\n30s\n30s\nthan for\nFaces\nObjects\nIs the signal\nHigher for\n?\nFace photos modified by OCW for\n\nStronger Response to Faces Than Objects\n\nTime (seconds)\nFaces > Objects\nF\nF\nF\nO\nO\nO\n% signal change\nKanwisher, N., et al. \"The Fusiform Face Area: A Module in\nHuman Extrastriate Cortex Specialized for Face Perception.\"\nThe Journal for Neuroscience 17, no. 11 (1997): 4302-4311.\nCourtesy of Society for Neuroscience. Used with permission.\n\nQuestion:\nAre there any parts of the brain that are specialized\nfor perceptually processing faces, more than other kinds\nof visual stimuli?\nWhere are they?\nWe have a beginning of an answer to ths question, but\nhave not yet nailed it.\nBe thinking about why it isn't nailed yet.\nWe will come back to this........\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. What is High-Level Vision?\nIV. Localization of Function\n\nThe Brain Before fMRI (1957)\nPolyak, in Savoy, 2001, Acta Psychologica\nCourtesy of University of Chicago Press. Used with permission.\n\nBecause fMRI was the first method for noninvasive\nfunctional mapping of the normal human brain, it\ntook off......\n\nNumber of papers (PubMed)\n2 papers (1990)\nOver 5,000 papers in 2006\nTHE RISE OF fMRI\nYear of Publication\nSource: Mel Goodale\n\nThe effect of fMRI on vision research is particularly\nstriking.\nIn the early 1990s...\n\nMacaque Visual Cortex, early 90s\nBrain diagram removed due\nto copyright restrictions.\nImage removed due to copyright restrictions.\nFig. 4, \"Hierarchy of visual areas,\" showing organization\nof 32 visual cortical areas.\nIn Felleman, Daniel J., and David C. Van Essen.\n\"Distributed Hierarchical Processing in the Primate\nCerebral Cortex.\" Cerebral Cortex 1, no. 1 (1991): 1-47. Line drawing of human brain.\nWhat about Human Visual Cortex?\nalmost nothing was known.\nand then.....\nSource: Felleman & Van Essen, 1991\nFigure by MIT OpenCourseWare.\n\nIn 1994, only two or three\nareas had been identified\nin human visual cortex.\nTwo years later, Tootell\net al published this\nmap, containing ten\nvisual areas.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nfMRI research continues to identify new cortical areas regularly.\n(My lab has identified 3 new regions.)\nMyriad discoveries have been made with fMRI......\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. What is High-Level Vision?\nIV. Localization of Function\n\n1. Question:\nSometimes you read stuff and remember it later......\nSometimes you read it and it goes poof.\nWhat is the difference in these two events?\nDoes something different happen in the brain\nwhile you are actually reading it in the first place?\n\n1. Predicting Verbal Explicit Memory\nfMRI Scanning during\nWord Learning\nPost-Scan Memory Test\nPEACE\nANVIL\nABSTRACT or CONCRETE?\n+\n2 s\nCHAIR\nBOOK\nSTUDIED?\nPEACE\n\nPredicting Verbal Explicit Memory:\nLeft Ventrolateral PFC\nWagner et al (1998)\np < 10 -6\np < .01\nAnterior LIPC\nPosterior LIPC\nA\nB\n-1\nSignal\nChange\n\nTime (s)\n\nForgotten\nRemembered\nB\nA\nTime (s)\nMRI diagrams\nremoved due to\ncopyright restrictions.\nMRI diagrams\nremoved due to\ncopyright restrictions.\n\n2. We are highly social organisms. We spend a lot of\ntime thinking about what other people are\nthinking.\nQuestion:\nDo we have special purpose brain machinery\nspecialized just for thinking about what other people\nare thinking?\n\n2. Understanding Others' Beliefs\nThe TPJ\nRebecca Saxe Line drawing of human brain.\nReasoning about hidden causes.\nReasoning about or perceiving physical attributes of people .\nReasoning about a person's cultural background.\nReasoning about or other people's bodily sensations (e.g., thirst, hunger).\nBeliefs\nReasoning about\nanother person's\nbeliefs.\nNOT:\nLogically identical problems about nonmental representation.\nFigure by MIT OpenCourseWare.\n\n3. We are also highly visual organisms. We spend a lot\nof time looking at faces, bodies, scenes, words......\nQuestion:\nDo we have special purpose brain machinery just for\nperceiving faces? bodies? scenes?....\n\n3. People, Places, & Things\nOf course, I think this is pretty fundamental, but... Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nNot Everyone Agrees:\n\"The face-selective region--dubbed the fusiform face area or FFA-\nhas attained a level of notoriety that is arguably far out of proportion\nrelative to its potential to inform us about the nature of object\nrecognition\".\nPeissig & Tarr (2006) Ann Rev Psych chapter\n\n4. Might fMRI be able to tell us whether a person in a\npersistent vegetative state is actually \"in there\"\ndespite their inability to speak or move?\n\nOwen et al (2006), Science, 313, p. 1402.\n23-year old woman\nTraffic accident >\nVegetative state.\nPreserved sleep-wake\ncycles, but\nunresponsive.\n\"imagine playing\ntennis\" and\n\"imagine walking\naround your house\"\nBut.....\nImage removed due to copyright restrictions.\nfMRI images of supplementary motor area in\ntwo imagery scenarios: playing tennis and\nwalking around the house.\nSee Figure 1 in Owen, A. M., et al. \"Detecting\nawareness in the vegetative state.\" Science\n313 (2006): 1402.\n\n5. What happens when we close our eyes and simply\nimagine e.g. a face or a place?\nno visual input\nbut it feels visual (to some people).\nis visual machinery in the brain recruited?\n\n5. Mental Imagery Experiment\n(eyes closed)\nO'Craven & Kanwisher (2000)\nSubjects heard the name of a famous person or\nfamiliar place once every 12 seconds, in random\norder, and were instructed to image the face or\nplace.\n\"Woody Allen\"\n\"MIT Great Court\"\n\"Bill Clinton\"\n\"Cary Grant\"\n\"Media Lab\"\n\nImaging Single Mental Events\ntime\nActivity in the Fusiform Face Area\nActivity in the Parahippocampal Place Area\n1.0\n1.5\n2.0\n0.5\n-0.5\n% Signal Change\nStimulus\ntype\nimagined\n\"Mindreading?!\"\nO'Craven & Kanwisher (2000)\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. What is High-Level Vision?\nIV. Localization of Function\n\nE = mc2\n???\nThe Principle behind fMRI\nAngelo Mosso\nItalian physiologist\n(1846-1910)\n\"[In Mosso's experiments] the subject to be observed lay on a delicately balanced table which could\ntip downward either at the head or at the foot if the weight of either end were increased. The moment\nemotional or intellectual activity began in the subject, down went the balance at the head-end, in\nconsequence of the redistribution of blood in his system.\"\n-- William James, Principles of Psychology (1890)\nCourtesy of Jody Culham. Used with permission.\nAdapted from fMRI for Dummies\n\nMRI vs. fMRI\nlow resolution\n(1 mm)\n(~3 mm but can be better)\nMRI\nfMRI\nhigh resolution\none image\nfMRI\n...\nBlood Oxygenation Level Dependent (BOLD) signal\nmany images\nindirect measure of neural activity\n(e.g., every 2 sec for 5 mins)\nCourtesy of Jody Culham. Used with permission.\nAdapted from Prof. Jody Culham's fMRI for Newbies\n\nFunctional Magnetic Resonance Imaging (fMRI)\n\"BOLD\" (blood oxygenation level dependent) signal:\nIncreased neural activity >\nIncreased local blood flow more than compensates for O2 use >\ndecrease in deO2Hb concentration>\nincrease in MR signal intensity (deO2Hb is paramagnetic)\nSo: fMRI reveals local brain function!\n[deO2Hb blood has higher magnetic susceptibility than O2Hb blood,\nhence causes more spin dephasing, hence faster \"relaxation\".]\n\nRecipe for MRI\n1) Put graduate student in a strong magnetic field.\nSource: Robert Cox's web slides\n\nEarly Human MR Scanner: \"The Indomitable\"\nDamadian sticks his postdoc\nLarry Minkoff in as the first subject &\nobtains the first MR image\nof the human body, below.\nImages removed due to copyright restrictions. See http://fonar.com/fonar_timeline.htm.\n\nRecipe for MRI\n1) Put graduate student in a strong magnetic field.\nA VERY strong magnetic field.\nSource: Robert Cox's web slides\n\nTypical Static Magnetic Field Strength of an MRI Scanner\n105 x earth's magnetic field\n1.5 or 3 or 4 Tesla for humans\nIMPORTANT SAFETY ISSUE:\nA small metal object (e.g. a key in your pocket) can become a bullet.\nBefore you walk into a scanner room first stop everything you are doing\n(talking, planning an experiment, etc), think of yourself as a potential lethal\nweapon, and carefully check everything: pockets, hands, jewelry, etc.\nNEVER relax this constant vigilance.\nPhoto removed due to copyright restrictions.\nFloor polisher pulled into the chamber of an MRI scanner.\nSee http://simplyphysics.com/flying_objects.html.\nEven very strong static magnetic fields have no known long-term effects on\nbiological tissue. However,\n\nVery Serious Risk\nImage removed due to copyright restrictions.\nOpening paragraphs from newspaper story: Klein, M., and O. Prichard.\n\"Boy, 6, killed in MRI accident.\" The Journal News, July 31, 2001.\nThe boy was smashed in the head by a metal oxygen canister accidentally\nbrought near the MRI scanner.\nWestchester NY, 2001\nSee also New York Times, August 19, 2005,\n\" M.R.I. Scanners' Strong Magnets Are Cited in a Rash of Accidents\"\nCourtesy of Jody Culham. Used with permission.\nfMRI for Newbies\n\nThe Big Magnet\nVery strong\n1 Tesla (T) = 10,000 Gauss\nEarth's magnetic field at the surface = 0.5 Gauss\n3 Tesla = 3 x 10,000 ÷ 0.5 = 60,000X Earth's magnetic field\nContinuously on\nMain field = B0\nRobarts Research Institute 4T\nx 60,000 =\nB0\nCourtesy of Prof. Gary A. Glatzmaier. Used with permission.\nCourtesy of Robarts Research Institute. Used with permission.\nCourtesy of Jody Culham. Used with permission.\nfMRI for Newbies\n\nRecipe for MRI\n1) Put subject in big magnetic field (leave him there)\n2) Transmit radio waves into subject\n[about 3 ms]\n3) Turn off radio wave transmitter\n4) Receive radio waves re-transmitted by subject\n- Manipulate re-transmission with magnetic fields during this readout interval\n- [10-100 ms: MRI is not a snapshot]\n5) Store measured radio wave data vs. time\n- Now go back to 2) to get some more data\n6) Process raw data to reconstruct images\n7) Take subject out of scanner\nCourtesy of Robert Cox, NIMH\nhttp://afni.nimh.nih.gov/pub/dist/edu/latest/afni01_intro/afni01_intro.ppt\n\n-\nfMRI Activation\nFlickering Checkerboard\nOFF (60 s) ON (60 s) -OFF (60 s) - ON (60 s) - OFF (60 s)\nBrain\nActivity\nKwong et al., 1992\nCourtesy of Jody Culham.\nTime -\nUsed with permission.\nfMRI for Newbies\nCourtesy of National Academy of Sciences, U. S. A. Used with permission.\nSource: Kwong, K. K., et al. \"Dynamic Magnetic Resonance Imaging of Human\nBrain Activity During Primary Sensory Stimulation.\" PNAS 89 (1992): 5675-5679.\nCopyright 1992 National Academy of Sciences, U.S.A.\n\nMukamel et al, Science, 2005\n- meaured neural activity in auditory cortex w/ depth\nelectrodes in epileptic patients (black) during movie\n- measured fMRI response in normal subjects\nlistening to same stimuli (orange)\nGraph image removed due to copyright restrictions.\nFig. 3, \"Correlation of the spike predictor with measured fMRI activation.\"\nMukamel, R., et al. \"Coupling Between Neuronal Firing, Field Potentials, and\nfMRI in Human Auditory Cortex.\" Science 309 (2005): 951.\n- similar time course indicates that fMRI signal\nfollows neural activity (at least somewhat)\n\nTemporal Properties of BOLD Response:\nThe hemodynamic response function (HRF)\nVisual stimulus on\nNeurons fire\nBOLD response\n>>>> BOLD response is SLOW, usually peaking around 5-6\nseconds after stimulus onset. Bad news for temporal resolution.\n\nImportant aspects of BOLD signal:\n- Because the BOLD signal is based on blood flow, the\nspatial & temporal resolution is limited by the\nprecision blood flow regulation:\n~ 1 mm; > a few 100 milliseconds\n- Cannot measure absolute amounts of activity/metabolism,\nonly differences between two conditions.\n- Physiological basis of the BOLD signal is unknown\n(Action potentials? Synaptic activity? Inhibition?)\n\nFunctional Magnetic Resonance Imaging (fMRI)\nvs. Other Methods\n- Advantages:\nThe best spatial resolution available for studies on normal subjects.\nNoninvasive.\nCheaper than PET (only $539/hour!).\n- Disadvantages:\nTemporal resolution not on a par with visual information processing.\nSpatial resolution about one mm (hard/impossible to see\ncortical columns), can never get better than the vasculature.\n\"Susceptibility artifact\" due to magnetic inhomogeneities near ear\ncanals and sinuses.\nLoud banging noise.\nUncertainty about the basis of the BOLD signal (spikes vs synaptic\nactivity).\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. What is High-Level Vision?\nIV. Localization of Function\n\nChosing a Question/Hypothesis\nStep 1. Get clear about what exact question you are asking in your\nexperiment. State it explicitly in a single sentence/question.\nNow ask yourself:\nIs this really an interesting and worthwhile question?\nIs the answer obvious (in which case why do the expt)?\nDoes this question bear on an important theory?\nSuppose you get the answer you want. Do you care?\nWill anyone else care? Will you have learned anything\nimportant?\nNote: Being able to answer these questions requires knowing a lot about\ncurrent theories on this topic, & about relevant work using other methods.\n\nWhat will you compare to what?\nStep 2. Recall that fMRI can only show differences in brain\nactivity between conditions, not absolute amounts.\nIn any imaging experiment, you will need to turn at least\none mental function on and off (or on more vs less strongly).\nFor studies of high-level vision, There are two main ways to do this:\ni) Change the stimulus\nii) Change the task\nHow do you chose which to do?\nConsider a task manipulation......\n\nSuppose we want to find out where object recognition\nhappens in the brain. In our critical test condition we want subjects\nto be recognizing objects. So for example we might tell them to do this:\nlook at the pictures that flash up next, and recognize them as they go by.\nReady? Let's try it.....\n\nPhoto of rose on\nwhite background\nremoved due to\ncopyright\nrestrictions.\n\nPhoto of tiger on\nwhite background\nremoved due to\ncopyright\nrestrictions.\n\nLook at the pictures that flash up next, & recognize them as they go by.\nDo you think you recognized them?\nOK, now let's try a possible task manipulation control condition\nto turn object recognition off:\nLook at the pictures that flash up next, and\nOnly perceive their shape, don't determine what kind of object they are.\nHere we go....\n\nPhoto of\npepperoni pizza\non white\nbackground\nremoved due to\ncopyright\nrestrictions.\n\nPhoto of car on\nwhite background\nremoved due to\ncopyright\nrestrictions.\n\nFor example,\nlook at the pictures that flash up next, and\nrecognize them as they go by.\nDo you think you recognized them?\nOK, now let's try a possible task manipulation control condition:\nlook at the pictures that flash up next, and\nOnly perceive their shape, don't determine what kind of object they are.\nHere we go....\nDid you follow the instructions this time? Why not?\nThe Moral:\nTask manipulations don't work well with automatic processes.\n\nStimulus manipulations can work better for automatic processes.\nFor example, if you look at pictures like this:\nYou will automatically recognize them as they go by.\nBut if you look at pictures like this:\nYou wont. So we might be able to \"isolate\" visual recognition\nby comparing brain activity when people look at pictures of familiar\nobjects versus novel unfamiliar objects, i.e. a stimulus manipulation.\n\nHowever, stimulus manipulations don't work so well for\nnonautomatic processes. For example, mental arithmetic.\nFor example, try just looking at the next few stimuli......\n\n96 ÷ 17 = ?\n\n14 x 23 = ?\n\nHowever, stimulus manipulations don't work so well for\nnonautomatic processes like mental arithmetic.\nFor example, try just looking at the next few stimuli......\nDid everyone get the answers?\nWhy not?\nThe Moral:\nStimulus manipulations don't work well with nonautomatic processes.\n(But a task manipulation could be very effective here.)\n\nManipulating stimulus versus task\nConclusion:\nTo control the mental operations that subjects carry out\nin the scanner you can either:\n- Manipulate the stimulus\nworks best for automatic mental processes,\ne.g. visual recognition.\n- Manipulate the task\nworks best for controlled mental processes,\ne.g. mental arithmetic\nDON'T DO BOTH AT ONCE!!!\n\nTips on deciding what to manipulate\n1. In a well-designed functional imaging study, two paired conditions\nshould differ by the inclusion/exclusion of a single mental process.\nThe single most common problem with imaging experiments is\nthat the conditions compared differ in many respects, not just one.\nThat is, the two conditions are not \"minimal pairs\". The other\nDifferences between conditions are confounds in the experiment.\nFor example......\n\nExample: Faces versus Objects\n45 faces\nfix\nfix\nfix\nfix\nfix\nfix\nfix\n45 objs\n45 faces\n45 objs\n45 faces\n45 objs\n30s\n30s\n30s\n30s\n30s\n30s\nFace photos modified by OCW\nfor privacy considerations.\nIs this a minimal pair that isolates just face recognition?\n\nDoes the face activation reflect:\n- Visual attention?\n- Processing any human body parts?\n- Processing only front views of faces?\n- Fine-grained within-category discrimination?\n- Luminance or other low-level confounds?\n- Face-specific visual processing?\n- Et cetera.....\n\nTips on deciding what to manipulate\n1. In a well-designed functional imaging study, two paired conditions\nshould differ by the inclusion/exclusion of a single mental process.\nThe single most common problem with imaging experiments is\nthat the conditions compared differ in many respects, not just one.\nThat is, the two conditions are not \"minimal pairs\". The other\nDifferences between conditions are confounds in the experiment.\nA. Be careful of \"low\" baselines such as \"rest\" or \"fixation\".\nLet's try it.\nTry fixating on the cross and don't think about anything.....\n\n+\n\nWas your mind blank?\n\nTips on deciding what to manipulate\n1. Two paired conditions should differ by the inclusion/exclusion of\na single mental process, I.e. they should be \"minimal pairs\".\nA. Be careful of \"low\" baselines such as \"rest\" or \"fixation\".\nThese can be useful, but you cant turn your mind off,\nso don't know what your subject is doing. Plus:\nB. Be careful of comparing difficult versus easy conditions:\nlots of brain areas get activated by virtually any difficult task,\nso you will get a lot of activation but you wont know why.\nC. Watch out for attention confounds: is one condition much more\ninteresting/engaging/attention capturing than another?\nD. Watch out for eye movement confounds between conditions.\nE. Try your own task and introspect, this can be very informative.\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. Localization of Function\nIV. What is High-Level Vision?\n\nThe Concept of Localization of Function\nThe brain is not a homogeneous and undifferentiated\nmush in which all the bits are \"equipotential\". Rather, at least\nsome mental functions are physically segregated (to at least\nsome degree) in the brain.\nWhile this idea is widely (but not universally) accepted\nfor primary sensory cortex and motor cortex, a debate has\nlong raged concerning the degree to which it is also true\nof high-level cognition....\n\nHistory of Debate on Functional Specificity in the Cortex\n1800s\n- Gall & Spurzheim\n- Flourens: \"all sensory and volitional\nfaculties exist in the cerebral hemispheres and\nmust be regarded as occupying concurrently\nthe same seat in these structures\"\n- Broca announces at the Societe\nd'Anthropologie in 1861 that left frontal lobe is\nthe seat of speech.\n- Gratiolet delivers scathing\ncounterargument immediately thereafter.\nImage courtesy of Curious Expeditions.\n\nHistory of Debate on Functional Specificity in the Cortex\nSometimes the pendulum swings within the same person...\nLashley's principle of \"mass action\"/\"equipotentiality\"\n1930s- Lashley: \"in the field of neurophysiology, no fact is more\nfirmly established than the functional differentiation of various parts\nof the cerebral cortex\"\n\nHistory of Debate on Functional Specificity in the Cortex\nSome more recent views.....\nSchiller, 1994 \"each extrastriate visual area, rather than performing\na unique, one-function analysis, is engaged, as are most neurons in\nthe visual system, in may different tasks.\"\nHuettel et al (2004): \"unlike the phrenologists, who believed that\nvery complex traits were associated with discrete brain regions,\nmodern researchers recognize that many functions rely upon\ndistributed networks and that a single brain region may participate in\nmore than one function\".\n\nMethodological Implications of L. of F.\nBecause the brain does appear to have at least some localization\nof function, we can start with one of the oldest tricks in science:\nDivide and conquer!\nThat is, understand a complex system by:\n1. Break it into parts to see how the system is organized.\n2. Try to understand how each part works.\n3. Then try to understand how the parts interact and work together.\nHow do we apply this to high-level vision?\nWhat is high-level vision?\n\nOutline for Today\nLecture 1: Introduction to fMRI & High-level Vision\nI. What is fMRI?\nA. A very simple fMRI experiment\nB. Impact of fMRI on cognitive neuroscience\nC. Some Examples of cool findings from fMRI\nD. The fMRI \"BOLD\" signal - absolute basics\nII. Basic Experimental Design\nIII. Localization of Function\nIV. What is High-Level Vision?\n\nWhat rabbit?\nWhat is High-level Vision (Visual Cognition)?\nVisual\nWorld/\nEye/\nVision/\nExperience/\nVisual field\nRetinal image\n?\nVisual Cog.\nInformation\nPhoto courtesy\nof Nick Devenish.\nRabbit\nWho cares?\nHow would\nI catch it?\nObject Recognition: How do we figure out what we are looking at?\nwhat kind of computations are carried out on the image?\nwhat kinds of representations are extracted?\nVisual Attention: How do we process visual information selectively?\nAwareness: Can we perceive information without being aware of it?\nVisually-Guided Action: How do we pick up a coffee cup, shoot a\nbasketball into a hoop, or catch a rabbit?\n\nDid I see\na Rabbit?\nHow would\nI get there?\nVisual\nWorld/\nEye/\nVisual Cognition\nExperience/\nVisual field\nRetinal image\n?\nInformation\nPhoto courtesy\nof Nick Devenish.\n..........\nObject Recognition\n- A lot of complex processing involved\nVisual Attention\nAwareness\n- Vision happens in the brain, not the eye\nVisually-Guided Action\n- Machine vision cant touch us; Humans rule!\nVisual Memory\nSpatial Navigation\nEtc.\n\nWhy Visual Cognition?\nVisual Cognition can tell us a lot about cognition in general because:\n- We are highly visual animals; Close to\nhalf the cortex in humans is involved in\nsome kind of visual processing.\n- Visual parts of the brain are used not\nonly in seeing, but also in thinking.\n- Visual cognition is one of the most\nsuccessful areas in cognitive science and\ncognitive neuroscience. In part because\n- visual cortex exhibits a\nhigh degree of\nlocalization of function\nLatera\nl view\nbrain with \"what\" and \"where\" location cartoons.\nFigure by MIT OpenCourseWare."
    },
    {
      "category": "Resource",
      "title": "Introduction to the Ventral Visual Pathway",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/06b4c65fd7f29cd421c978570dcb6035_lec2_vvp_ip.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n9.71: fMRI of High-level Vision\nNancy Kanwisher\nFall 2007\nLecture 2A: Introduction to the Ventral Visual Pathway\nLecture 2B: Experimental Design & Data Analysis\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n[Lecture 2C: Critiquing fMRI Experiments: Some Tips\nDiscussion of Lie Detection]\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n\nTwo Visual Pathways\nThe Ventral Visual Pathway:\nObject Recognition\nHow is it organized?\nSlide adapted from Jody Culham\nCourtesy of http://psychology.uwo.ca/culhamlab/\nVentral pathway that\ntr\nac\nks\no\nbj\nect\nre\ncognitio\nn; div\nided in\nto \"wh\nere\" path (V2, V3, V5 to parietal cortex) and \"what\" path (V2, V4, inferotemporal cortex).\nFigure by MIT OpenCourseWare.\nImage removed due to copyright restrictions.\nFig. 4 in Felleman, Daniel J. and David C. Van Essen.\n\"Distributed Hierarchical Processing in the Primate Cerebral Cortex.\"\nCerebral Cortex 1, no. 1 (1991): 1-47.\n\nAre different parts of the ventral visual pathway active when we look at\ndifferent kinds of objects?\nTime (seconds)\nFaces > Objects\nF O F\nF\nO\nO\n% signal change\nCourtesy of Society for Neuroscience. Used with permission.\nHow systematic is this across subjects?\n\nAreas Responding More to Faces than Objects in 12 Ss\nCourtesy of Society for Neuroscience. Used with permission.\n\nDoes the face activation reflect:\n- Visual attention?\n- Processing any human body parts?\n- Processing only front views of faces?\n- Fine-grained within-category discrimination?\n- Luminance or other low-level confounds?\n- Face-specific visual processing?\n- Et cetera.....\n\nRegion of Interest Approach:\nDoes the face activation reflect\n-\nGreater attention to faces than other stimuli?\n1. Localize the face area individually in each subj:\nthe fusiform region in which faces>objects\n2. Measure the response in this area in new scan:\n\"1-back\" task on:\nvs.\nCourtesy of Society for Neuroscience. Used with permission.\n\n-\nResults\n4a. Faces > Objects\nTime (seconds)\nF\nF\nF\nO\nO\nO\n% signal change\nH F\nF\nF\nH\nH\nTime (seconds)\n4b. 3/4 F > H (1-back)\n% signal change\nCourtesy of Society for Neuroscience. Used with permission.\n\nDoes the face activation reflect:\n- Visual attention?\nno\n- Processing any human body parts?\nno\n- Processing only front views of faces?\nno\n- Fine-grained within-category discrimination? no\n- Luminance or other low-level confounds?\n?\n- Face-specific visual processing?\n- Et cetera.....\n\nCat Face\n1.6\n0.6-1.1\nFusiform Face Area\nKanwisher, Tong, McDermott, Chun, Nakayama, Moscovitch, Weinrib, Stanley, Harris, Liu\nCartoon\nProfile-View\n1.8\n\"Mooney\"\n2.0\nFront-View\n1.9-2.3\nInv. Grey\n1.6\n1.7\nNo Eyes\nHuman Head\n1.7\nAnimal Head\nBack of Head\n0.9\n1.0\nWhole Animal\n0.7\nCat Face\n1.6\n1.3\n1.3\nInv. Mooney\nEyes Only\n1.3\nHuman Body\n1.0\nHand\nAnimal Body\n0.8\nBuildings\n0.6\nImage removed due to\ncopyright restrictions.\n1.7\nInv. Cartoon\nImage removed due to\ncopyright restrictions.\n1.4\nExternal Ftrs\n1.1\nObject\nCourtesy of Society for Neuroscience. Used with permission.\nFace photos modified by OCW\nfor privacy considerations.\n\nAre some brain regions selectively activated by specific categories of\nstimuli?\nYes, apparently, at least for faces.\nAny others?\n\nScenes > Objects in 9/9 Subjects\nImages removed due to copyright restriction.\nFig. 2a. in Epstein, Russell and Kanwisher, Nancy. \"A\ncortical representation of the local visual\nenvironment.\" NATURE 392 (9 APRIL 1998): 598-601.\nUsed in lecture slides by Prof. Kanwisher.\nEpstein & Kanwisher 1998\n\nParahippocampal Place Area\nEpstein & Kanwisher (1998)\nImages removed due to copyright restriction.\nFig. 2a. in Epstein, Russell and Kanwisher, Nancy.\n\"A cortical representation of the local visual\nenvironment.\" NATURE 392 (9 APRIL 1998):\n598-601.\nUsed in lecture slides by Prof. Kanwisher.\nLandscape\n1.2\nLego Objects\n0.6\nLego Scene\n1.1\nDispl. Rooms\n0.8\nIndoor Unfurn\n1.2\nFrac. Rooms\n1.2\nVehicles\n0.4\nFaces\n0.0\nText. Gradients\n0.5\nHouse\n1.0\nF. Landmark\n1.5\nU. Landmarks\n1.1\nOutdoor Fam\n1.9\nOutdoor Unfam\n1.8\nIndoor Furn\n1.3\nFurniture\n0.5\nDrop Shadow\n0.5\nScr. Scenes\n0.5\nMaps\n0.3\nObjects\n0.4\nFace photos modified by OCW\nfor privacy considerations.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nCategory-Specific Regions in Human Extrastriate Cortex Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nHow many of these category-specific\nregions are in there, anyway?\nPaul Downing and I tried to find out,\nby testing every category that seemed plausible.\n\nOther Category-Specific Regions in Visual Cortex?\nDowning & Kanwisher\nImages removed due to copyright restriction.\nThis slide and the next slide show that Faces and Places\nare really special in the Visual Cortex region.\nCars\n\nOther Category-Specific Regions in Visual Cortex?\nDowning & Kanwisher\nImages removed due to copyright restriction.\nThis slide and the next slide show that Faces and Places\nare really special in the Visual Cortex region.\nFaces & Places really are special!\n\nBut there was one new category that did selectively activate\na region of cortex.....\nDowning & Kanwisher\n\nHuman bodies and body parts:\n\n0.9\nObject\nObject\nObject\nObject\n1.0\nExtrastriate Body Area\nDowning, Jiang, Shuman, & Kanwisher (2001)\nObjects\nHands\nPeople Line\nArtic Objs\nFish\nStick Figures\nSleepy People\nCurvy Cars\nFaces\nBody Parts\nPeople\nObj Parts\nHeadless Bods\n1.6\n1.4\n1.4\n1.6\n1.6\n1.4\n0.5\n0.6\n0.7\n0.6\n1.0\nSilhouettes\n0.8\n1.0\n0.6\nFace photos modified by OCW\nfor privacy considerations.\nImages removed due to copyright restriction.\nFig. 1 in Downing, Paul E. et. Al. \"A Cortical Area\nSelective for Visual Processing of the Human Body.\"\nScience 293 (28 SEPTEMBER 2001): 2470-2473.\n(http://web.mit.edu/bcs/nklab/media/pdfs/DowningJiang\nShumanKanwisherScience02.pdf)\n1.7\n1.8\nScr.Stick Figs.\nScr.Silhouette\n\nFaces, Places, Bodies\nWhat about chairs (Ishai et al)?\nLine drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nFaces\nR\nChairs\nHouses\nFace photos modified by OCW\nfor privacy considerations.\n% Signal Change\n0.00\n1.00\n2.00\nH\nF\nCh\n\nH\nF\nCh\n\nH\nF\nCh\n\nScans (TR = 3 sec)\nCourtesy of Alumit Ishai. Used with permission.\nAlumit Ishai, LBC/NIMH\n\nChairs > (Faces + Scenes), 9 subjects group data\nDowning & Kanwisher\n0.6\n0.5\n0.4\nPSC\n0.3\n0.2\n0.1\nA \"chair\" area?\nchairs scenes faces\n\nChairs > (Faces + Scenes), 9 subjects group data\nDowning & Kanwisher\n0.6\n0.5\n0.4\nPSC\n0.3\n0.2\n0.1\nA \"chair\" area?\n.....no!\nchairs scenes faces cells food\ncars flowers animals\n\nHow do we recognize everything else?\n\nThe Lateral Occipital Complex (LOC):\nCortical Regions Involved in Processing Object Shape\nI Malach et al (1995), \"LO\"\nand\n:\n>\nCourtesy of National Academy of Sciences, U. S. A. Used with permission.\nSource: Malach, R. et. al. \"Object-related activity revealed by functional magnetic\nresonance imaging in human occipital cortex.\" Proc. Natl. Acad. Sci. 92 (1995): 8135-8139.\nCopyright (c) 1995, National Academy of Sciences, U.S.A.\nII Kanwisher et al (1996) - a similar region\nand\n>\n: Brain regions processing object shapes.\nFigure by MIT OpenCourseWare.\n\nLine drawing of human brain, viewed from\nunderside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n\nControversies and Questions about\nCategory-selective Regions of Cortex\nAlternative view I: The brain is not organized around content domains\n(e.g., faces or places), but instead around processes (e.g. fine-grained\ndiscrimination) that can be conducted on any stimulus type.\nwe'll cover some of these arguments in later lectures*\nAlternate view II: faces, places, and objects are represented not by focal\nregions of cortex, but by distributed patterns of activation spanning\ncentimeters of cortex.\nIs face information spread far beyond the FFA?\nDoes the FFA contain information about nonfaces?\n*Pernet C, Schyns PG, Demonet JF. Specific, selective or preferential: comments on\ncategory specificity in neuroimaging. Neuroimage. 2007 Apr 15;35(3):991-7.\n\nHaxby et al (2001)\nMain Idea:\nInformation about object categories is spread over a large\nswath of cortex, not restricted to small specialized regions.\nMethods:\n1. Scan each subject on 8 stimulus categories\n2. Split the data in half.\n3. Generate \"known\" activation patterns from each half of data:\nFaces\nBottles Shoes\nChairs Houses Scissors Cats\nScrambled\n(fake data)\n\nHaxby et al (2001)\nIs the pattern of response across cortex more similar (i.e. more\ncorrelated) for the same category than for different categories?\nImages removed due to copyright restrictions.\nFig. 3A and 3B in Haxby et. al. in \"Distributed\nand Overlapping Representations of Faces and\nObjects in Ventral Temporal Cortex.\" Science\n293, no. 5539 (28 Sep 2001): 2425-2430.\nFace photos modified by OCW for privacy considerations.\nYes:\nYes:\nFace1 - face2 is more similar\nChairs1-chairs2 is more similar\nThan face1 - house 2\nThan chairs1 - shoes 2\nSo if you look at the response across cortex you \"can tell\" which object was seen.\n\nControversies and Questions about\nCategory-selective Regions of Cortex\nAlternative view I: The brain is not organized around content domains\n(e.g., faces or places), but instead around processes (e.g. fine-grained\ndiscrimination) that can be conducted on any stimulus type.\nwe'll cover some of these arguments in later lectures*\nAlternate view II: faces, places, and objects are represented not by focal\nregions of cortex, but by distributed patterns of activation spanning\ncentimeters of cortex.\nIs face information spread far beyond the FFA?\nDoes the FFA contain information about nonfaces?\n\nNonpreferred Responses in the FFA\nFaces > Objects\nignal change\nT\nF\nF\nF\nO\nO\nO\nime (seconds)\n?!\n% s\nCourtesy of Society for Neuroscience. Used with permission.\n- Do \"nonpreferred\" responses carry information about nonpreferred stimuli?\n- A potential challenge to the domain specificity of the FFA.\nUsing Haxby's method to ask whether the FFA contains info about nonfaces....\n\nCorrelation-based Classification Analysis (Haxby et al., 2001)\n1. Scan each subject while they view multiple stimulus categories.\n2. Split the data in 1/2; generate activation maps for each category.\n3. Compute correlation across activation maps.\nWithin\ncategory\nbetween\ncategories\nIf r(Within) > r(Between)\nWhat do we find for\nthe region contains category information nonfaces in the FFA?\n\nDoes the Pattern of Response Across the FFA\ncontain information that discriminates between nonfaces?\nHaxby et al (2001): yes\n\"Regions such as the .... 'FFA' are not dedicated to representing only .... human\nfaces,.. but, rather, are part of a more extended representation for all objects.\"\nSpiridon & Kanwisher (2002): no\nTsao et al (2003), in face patches in monkey brains: no\nO'Toole, Haxby et al. (2005): no (sort of):\n\"preferred regions for faces and houses are not well suited to object classifications\nthat do not involve faces and houses, respectively.\"\nReddy & Kanwisher (submitted): yes (sort of).\nBUT: maybe these tests are unfair, in two ways:\ni) Spatial resolution limits of fMRI necessarily entail some\ninfluence of neural populations outside the region in question.\nii) The presence of discriminative information does not mean it\nplays an important role in perception!\n\nPerhaps at a finer grain one could detect discriminative information\nin the nonpreferred responses. But even if so, is this information used?\nImage removed due to copyright restrictions.\nFigure 1 in Wada, Y. and T. Yamamoto. \"Selective Impairment of Facial Recognition\ndue to a Haematoma Restricted to the Right Fusiform and Lateral Occipital Region.\" J Neurol\nNeurosurg Psychiatry 71 (2001): 254-257.\nSuggests: Information in the FFA is critical for face discriminations but not for\nobject discriminations. (We will return to this topic in a couple weeks.)\n\nSome Currently Hot Unanswered Questions\n1. Do truly category-selective regions of cortex exist, or have the\nFFA, PPA, & EBA been mischaracterized, and they really do something\nmuch more general?\n2. Do these regions work in fundamentally different ways from each\nother, or are they in some sense all performing variations of the 'same\"\ncomputations?\n3. (hard!) How do these regions arise in development? What role\ndoes experience play in shaping the selectivity of these regions?\n4. To what extent can these regions \"move over\" after brain\ndamage, and to what extent must each of them live only in its standard\nlocation?\n5. Why do we have selectivities for these categories and\n(apparently) not others?\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n\nStandard Designs\n- Manipulate one factor with two levels, e.g.:\npassive viewing of faces versus objects\npassive viewing of moving versus stationary rings/dots\n- Manipulate 1 factor over several levels: \"parametric design\", e.g.:\nvary the # of attentively tracked balls (Culham et al 2001)....\n\nA Parametric Study of Attentive Tracking\nCulham et al (2001)\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nAttentive Tracking Demo\n\nA Parametric Study of Attentive Tracking\nCulham et al (2001)\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nRegion FEF is more\nsimply task-\ndependent:\nWhen you are doing\nthe task, this region\nis active.\nRegion SFS is more\nmonotonic:\nActivity in this region\nincreases with\nattentional load.\nSuggests different\nfunctional roles\nof these two\nregions.\n\nStandard Designs\n- Manipulate one factor with two levels, e.g.:\npassive viewing of faces versus objects\ncomparing the number versus color of two dot arrays\n- Manipulate 1 factor over several levels: \"parametric design\", e.g.:\nvary the contrast of gratings or the speed of moving dots\nvary the # of attentively tracked balls (Culham et al 2001)....\n- Manipulate 2 factors orthogonally, e.g........\n\n\"Factorial Designs\"\nEnables us to\nfaces\nobjects\nask:\n(How) Is\nAtten-\nMonitor for\nselectivity for\nded\na\nFace/obj\na\nrepetitions\nby attention?\nUn\nfaces affected\nMonitor for\nAtten\na\na\nletter\nded\nrepetitions\n\n\"Factorial Designs\"\nEnables us to\nask:\n(How) Is\nselectivity for\nfaces affected\nby attention?\nSelectivity found\nonly when\nattended!\nfaces\nobjects\nAtten\nded\nUn-\nAtten\nded\n(Fake data)\nThis is an\n\"interaction\": the\neffect of one\nfactor (face/obj)\ndepends on\nwhat level we\nare at on the\nother factor\n(att/unatt).\n\nStandard Designs\n- Manipulate one factor with two levels, e.g.:\npassive viewing of faces versus objects\ncomparing the number versus color of two dot arrays\n- Manipulate 1 factor over several levels: \"parametric design\", e.g.:\nvary the contrast of gratings or the speed of moving dots\nvary the # of attentively tracked balls (Culham et al 2001)....\n- Manipulate 2 factors orthogonally, e.g.:\nfaces vs objects x attended versus unnattended (on same stimuli)\nenables you to ask (with the interaction term in an ANOVA) if the\nincrease in activation for faces in a given region is affected by\nattention.\n- Manipulate nothing; bin by behavior, e.g....\n\nWagner et al (1998)\nPredicting Verbal Explicit Memory\nfMRI Scanning during\nWord Learning\nPost-Scan Memory Test\nPEACE\nANVIL\nABSTRACT or CONCRETE?\n+\n2 s\nCHAIR\nBOOK\nSTUDIED?\nPEACE\n\nTemporal Arrangement\nHow should these various conditions be distributed temporally\nwithin and across scans?\nSome Tips:\n- Try to include all conditions within a subject and within a scan\n- Avoid order confounds by counterbalancing within and across\nscans - subjects are more alert at beginning of scan.\n- Tradeoffs concerning the length of each epoch:\ndifficulty of task switching\nnoise is generally low frequency so rapid alternation\nbetween conditions moves signal away from noise in\nfreq space\nimportance of unpredictability\nextreme ends of the spectrum: \"blocked\" vs \"event-related\"\n\nBlocked vs. Event-related\nSource: Buckner 1998\nImages removed due to copyright restrictions. Fig. 1A in Buckner, R. L. \"Event-Related fMRI and the Hemodynamic Response.\"\nHuman Brain Mapping 6, no. 5-6 (1998): 373-377.\nn B\n\nRecall the BOLD\nhemodynamic response function (HRF)\n>>>> BOLD response is SLOW.\nHow do we analyze trials that occur in rapid succession?\nVisual stimulus on\nNeurons fire\nBOLD response\n\nObserved: the sum of all of these:\nNOW: how do we recover the response to houses, and\nthe response to faces?\nThe simplest way: just average...\nmodified by OCW\nfor privacy considerations.\nCourtesy of Society for Neuroscience. Used with permission.\n\nEvent-related Design Logic\nCollect all the face responses, align them, and average.\nThen collect all the house responses, align them, and average.\nFace photos modified by OCW\nfor privacy considerations.\nFFA:\nNew Text\nCourtesy of Paul Downing. Used with permission.\nSlide adapted from Paul Downing\nCourtesy of Society for Neuroscience. Used with permission.\n\nAnalysis of Single Trials w/ Counterbalanced Order\nEvent-related average\nRaw data\nEvent-related average\nwith control period factored out\nA signal change = (A - F)/F\n...\nB signal change = (B - F)/F\nA\nB\nF\nsync to trial onset\nNote that this will only work if MRI signals from different trias sum linearly. Do they?\nAdapted from Jody Culham's fMRI for Dummies web site\nhttp://psychology.uwo.ca/fmri4newbies/\n\nDale & Buckner, 1997 Linearity of BOLD response\nLinearity:\n\"Do things add up?\"\nred = 2 - 1\ngreen = 3 - 2\nSync each trial response to\nstart of trial\nNot quite linear but good enough\nSoon et al (2003): things are less linear in\nmore anterior regions.\nCopyright (c) 1997 Wiley-Liss, Inc., a subsidiary of John Wiley & Sons, Inc.\nReprinted with permission of John Wiley & Sons., Inc. Source: Dale, A., and\nR. Buckner. \"Selective averaging of rapidly presented individual trials\nSlide from Jody Culham\nusing fMRI.\" Human Brain Mapping 5 no. 5 (1997): 329 - 340.\nCourtesy of http://psychology.uwo.ca/culhamlab/\n\nAdvantages of Event-Related\nFlexibility and randomization\n-\neliminate predictability of block designs\n-\nreduce practice effects\n-\nreduce attentional confounds\nPost hoc sorting\n- (e.g., correct vs. incorrect, aware vs. unaware, remembered vs. forgotten\nitems, fast vs. slow RTs)\nRare or unpredictable events can be measured\n-e.g., P300\nCan look at different phases of the response within a trial (if it is long enough to\nresolve these)\n-Sample versus delay in a working memory tasks\n-attentional cue versus response in an attention task\nSource: Buckner & Braver, 1999\nvia Culham\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n\nExample of Raw Data & The \"Eyeball Test\"\nTime (seconds)\nFaces > Objects\nF\nF\nF\nO\nO\nO\n% signal change\nCourtesy of Society for Neuroscience. Used with permission.\nDo we need stats here? Why?\n\nWhy do we need stats?\n- Eyeballing raw time courses isn't a viable option.\nWe'd have to do it 49,152 times and it would require\na lot of subjective decisions about whether\nactivation was real. Plus, somewhere in there we\nare bound to find a nice result (so beware of \"voxel\nsniffing\").\n- This is why we need statistics\n- Statistics:\n» tell us where to look for activation that is related to our\nparadigm\n» help us decide how likely it is that activation is \"real\"\nSource: Jody Culham's fMRI for Dummies web site\nhttp://psychology.uwo.ca/fmri4newbies/\n\nFormal Statistics\n-\nFormal statistics are just doing what your eyeball test of significance did\n» Estimate how likely it is that the signal is real given how noisy the data is\n-\nconfidence: how likely is it that the results could occur purely due to chance?\n-\n\"p value\" = probability value\n» If \"p = .03\", that means there is a 3% chance that these results could be found even if the data were noise.\n-\nBy convention, if the probability that a result could be due to chance is less than 5% (p < .05), we\nsay that result is statistically significant\n-\nSignificance depends on\n» signal (differences between conditions)\n» noise (other variability)\n» sample size (more time points are more convincing)\n-\nSource: Jody Culham's fMRI for Dummies web site\n-http://psychology.uwo.ca/fmri4newbies/\n\nA Big Challenge in\nNeuroImaging\nSuppose you run your statistics on each of the 49,152 voxels you scanned\nYou find 200 voxels that reach the p<.05 significance level.\nShould you be impressed?\nLots of fancy math has been proposed for how you \"correct for multiple\ncomparisons\".\nYou can avoid this whole problem if your hypothesis refers to a specific place in\nthe brain that you specify in advance (though not all intersting hypotheses are of\nthis form).\n\nA Common Statistical Error\nCommon flawed logic:\nRun1: A - baseline\nRun2: B - baseline\n\"A - 0 was significant, B - 0 was not, ∴ Area X is activated by A more than B\"\nIf you do this, you can get a situation where A is\nsignificantly > 0 but B is not, yet the difference between A\nand B is not significant\nBottom line: If you want to compare A vs. B, compare A vs. B!\nFaces\nPlaces\nError bars = 95% confidence limits\nYou can find this error in some fancy journals....\n\nOwen et al (2006), Science, 313, p. 1402.\nTennis > rest\nAnd\nNavigation > rest\nImage removed due to copyright restrictions.\nAre these patterns of\nNew Text\nFig. 1 in \"Detecting Awareness in the Vegetative\nState.\" Adrian M. Owen, Martin R. Coleman, Melanie\nactivation different\nBoly, Matthew H. Davis, Steven Laureys, John D.\nPickard. Science, 8 SEPTEMBER 2006, VOL 313.\nfrom each other?\n1. These statistics don't tell us!\n(What would we have to do?)\nWhat else is fishy here?\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Challenges with fMRI Experiments\n\nProblem 1: What counts as the \"same place\" in the brain?\nI. Individual Subject versus Group Analyses\nWe can ask whether the \"same place\" in the brain is activated by two\ndifferent tasks if we look within individual subjects. Here same place means\nexact same voxel/s in the exact same subject. But then how to we\ngeneralize to other subjects?\n- We want to be able to make a general claim about all (or most)\npeople, not just a claim about Joe Shmo's brain.\n- But people's brains are as different in shape from one person to the\nnext as their faces are.\n- So: What is the \"same place\" in two different brains?\n(Is the freckle on Joe's nose in the same place as the freckle on\nBob's face?)\n- Approach 1: use gyri/sulci to indicate brain locations....\n\nFor example is this face > object activation\nin the \"same place\" in these 12 subjects?\nCourtesy of Society for Neuroscience. Used with permission.\n\nFusiform Gyrus\nConsider this (published) argument:\n1. Kanwisher says there is a face-\nselective region in the fusiform gyrus.\n2. But we found a region that\nresponds strongly to non-face stimuli in the\nfusiform gyrus. The role of fusiform gyrus.\n3. Kanwisher is wrong about that\nface-selective region: it isnt face-selective.\nCourtesy of wikipedia.\nhttp://en.wikipedia.org/wiki/File:Gray727.svg\nIs this a good argument? Why/why not? The role of fusiform gyrus.\n\nWhat counts as the \"same place\" in different brains?\nApproach 2: Register all subjects to a \"common space\"\nFor example: Talairach space: an alignment method with several\ndegrees of freedom - linear transformations (stretch, twist) for best fit.\nThen run statistics across subjects on voxels in this common space.\nProblems:\n-Even \"hard\" anatomical loci (e.g. major sulci) do not coregister to the same\nplace across subjects.....\n\nVariability in Sulcal Locations Across Individuals in\nTalairach (1967) Space\nSource: R. Woods, Correlation of Brain Structure and Function. Chapter in Brain Mapping\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nWhat counts as the \"same place\" in different brains?\nApproach 2: Register all subjects to a \"common space\"\nFor example: Talairach space: an alignment method with several\ndegrees of freedom - linear transformations (stretch, twist) for best fit.\nThen run statistics across subjects on voxels in this common space.\nProblems:\n-Even \"hard\" anatomical loci (e.g. major sulci) do not coregister to the same\nplace across subjects.\n-Even with respect to \"hard\" anatomical landmarks, some functionally-defined regions\nmay vary anatomically across subjects.\n-To get around this data are typically blurred (\"smoothed\") within each subject before\nanalysis.\n- The smoothing and the imperfect registration drastically lowers resolution.\n\nInterpreting Group-Averaged Data\nApproach 2: Register all subjects to a \"common space\"\nThen run statistics across subjects on voxels in this common space.\nInferences:\n- If an activation is significant across subjects in the group data that implies that\nthe region is consistent enough (or large enough) that it lands in an overlapping\nlocation across many of the subjects.\n- BUT: failing to find an activation in the group data could just mean the region in\nquestion is anatomically variable and does not get well aligned across subjects.\n-Similar or overlapping group activations for two different comparisons do not\nnecessarily imply that the same voxels are activated in each individual. Why?\n\nCommon Use\nof Whole Brain Group Stats\n1. You don't necessarily need a priori hypotheses (though sometimes you\ncan use less conservative stats if you have them)\n2. Average all of your data together in Talairach space\n3. Compare two (or more) conditions using precise statistical procedures and\nassumptions. Anything that passes at a carefully determined threshold is\nconsidered real.\n4. Make a \"laundry list\" of these areas and publish it.\nWhen will this\naproach be\nuseful/interesting?\nAlternative:\nROI approach....\nAdapted from Jody Culham's fMRI for Dummies web site\n-http://psychology.uwo.ca/fmri4newbies/\n\nWhat counts as the \"same place\" in different brains?\nApproach 3: use individually-defined \"regions of interest\" (ROI).\n1. Localize ROI individually in each subject anatomically\n(e.g., hippocampus; calcarine sulcus) or w/ functional\n\"localizer\" scan, e.g. face area = faces > objects.\n2. Run new scans in the same subject and session.\nQuantify the response of previously-defined region to\nnew conditions.\n- deals with anatomical variability across Ss\n- removes requirement to correct for multiple comparisons\nThough widely used, this method is considered controversial by some....\n\nSee reply by\nSaxe, Brett, &\nKanwisher (2006)\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nComparing the two approaches\nRegion of Interest (ROI) Analyses\n-\nIs useful to the extent that the ROI is a real \"thing\", that we are \"carving nature at\nits joints\".\n-\nGives you more statistical power because you do not have to correct for the\nnumber of comparisons\n-\nHypothesis-driven\n-\nROI is not smeared due to intersubject averaging\n-\nEasy to analyze and interpret\n-\nNeglects other areas which may play a fundamental role (though can use multiple\nROIs)\n-\nPopular in North America\nWhole Brain Analysis\n-\nRequires no prior hypotheses about areas involved\n-\nIncludes entire brain\n-\nOften neglects individual differences\n-\nCan lose spatial resolution with intersubject averaging\n-\nCan produce meaningless \"laundry lists of areas\" that are difficult to interpret\n-\nYou have to be fairly stats-savvy\n-\nPopular in Europe\nNOTE: Though different experimenters tend to prefer one method over the other, they are NOT mutually exclusive.\nYou can check ROIs you predicted and then check the data for other areas.\nAdapted from: Jody Culham's web site\nCourtesy of Jody Culham\n\nProblem 2: Infering Function at the Right\nLevel of Generality/Specificity\nHypothesis:\nRegion X is involved in process Y.\nEvidence:\nRegion X is activated when subjects do an instance of process Y.\nProblem:\nWithout running several further conditions, we can't tell whether\nregion X might instead be involved in something either more\nspecific or more general than process Y.\nExample:\nHYPOTHESIS SPACE - FUNCTION OF REGION X\nAnything Animate\nAny Human Body Part\nHands\nFeet\nEyes\nFaces\nw/o hair\nGreyscale\nPhotos\nProfile\nFaces\nTwo-Tone\nFaces\nFaces\n\nProblem 3: Attentional Confounds\nA given region might respond more strongly in condition A than\ncondition B simply because A is more interesting/attention-capturing than B.\nSolutions:\nI.\nDouble dissociations\nII.\nTest conditions with opposite\nattentional predictions\nA\nB\nPredictions from Attention alone:\npassive viewing: A > B\n1-back task:\nB > A\nCourtesy of Society for Neuroscience. Used with permission.\nIf A > B in both, then result is probably\nnot due to an attentional confound.\n\nProblem 4: Statistical Significance vs.\nTheoretical Significance\nP levels alone are not sufficient.\nFor example, the FFA may respond significantly more to pineapples than\nwatermelons, but the response to pineapples might nonetheless be much lower\nthan the response to faces.\nSolutions:\n- Quantify effect size, e.g. with percent signal change.\n- Provide \"benchmark\" conditions within the same scan to give these\nmagnitudes meaning.\nNO BIG DEAL\nTROUBLE\nObjects\nWatermel. Pineapp.\nFaces\n0.6\nO.7\n0.9\n2.0\n0.6\nO.7\n1.8\n2.0\n\nProblem 5: Activity vs. Necessity\nJust because a given region is active during a given process\ndoesn't mean that region is necessary for that process.\nfMRI has no way to test necessity, though we can get a little\ncloser to a causal connection if we find a correlation between fMRI signal\nand performance.\nSolutions (?): Use other methods!\n-\nTMS\n-\nPatient Studies\n-\nAnimal Lesion or Microstimulation Studies\n\nProblem 6: Time Course\nVisual recognition happens within about 200 ms, which means\nthat its component processing steps take tends of milliseconds. Yet the\ntemporal resolution of fMRI is much lower than this.\nSolutions (?): Use other methods for studying temporal information\n-\nERPs & MEG\n-\nSingle unit recordings\n\nOutline for Today\nLecture 2A: Introduction to the Ventral Visual Pathway\nI. Basic Organization of the VVP\nincluding FFA, PPA, EBA, LOC\nII. Controversies about the VVP & Unanswered Questions\nLecture 2B: Experimental Design & Data Analysis\nI. Basic Kinds of Experimental Designs\nII. Basic Data Analysis Methods\nIII. Five Common Problems with fMRI Experiments\n[Lecture 2C: Critiquing fMRI Experiments: Some Tips\nDiscussion of Lie Detection]\n\nTips on How to Critically Evaluate fMRI Studies\n1. First, figure out what question the researcher is asking and what\nanswer they are giving to that question.\nAsk yourself: Is this an interesting question? Does it have clear\ntheoretical implications and if so what are they? Do you care about\nthe result? Should anyone? Why? Are you surprised by the result?\nSituate the question in a broader theoretical context. If there is no\nsuch broader context, be worried.\n\nTips on How to Critically Evaluate fMRI Studies\n2. The most critical aspect of the design of the experiment is: what is\ngetting compared to what?\nMake a list of all the mental functions that you think go on during the\ncritical test condition. Then make a list of all the mental\nfunctions that are going on in the control condition, then see how\nmany go on only (or more) in the test condition than the control\ncondition.\nAre the test and control conditions \"minimal pairs\"?\n\nTips on How to Critically Evaluate fMRI Studies\n3. Classic problems in analyses/inferences/conclusions to be wary of:\nA. \"Brain area X was activated by task Y.\"\ni. Ask: task Y compared to what? Everything is a comparison, and\nmany comparisons are uninformative/trivial.\nii. What else activates brain area X?\niii. How strongly activated was that region? Not all 'activations\" are\nthe same - Effect sizes matter! If one condition produces a massive\nresponse compared to a given baseline, and another condition\nproduces a very small but significant activation, the two\n\"activations\" are not the same.\n\nTips on How to Critically Evaluate fMRI Studies\nB. \"Because Region X responded significantly more strongly in Task A\nthan control, but didn't respond significantly more strongly in Task B\nthan control, it is selectively activated by Task A.\"\nA difference in significances is not necessarily a significant\ndifference.\nIf you want to claim that the region responds more to A than B, then\ncompare A to B. Statistics are not transitive.\n\nTips on How to Critically Evaluate fMRI Studies\nC. Claims of this form: \"We found activation in the medial prefrontal\ncortex for tasks involving reasoning about other minds, consistent\nwith numerous prior studies.\"\nBrains are as different across individuals as faces are, so what counts\nas the \"same place\" in the brain is not well defined across different\nbrains.\n\nTips on How to Critically Evaluate fMRI Studies\nD. \"The results of the present study demonstrate that Task A is carried\nout in a distributed network of cortical areas.\"\nWhat has been learned here?\n\nTips on How to Critically Evaluate fMRI Studies\n4. Some of the many ways to cheat:\nA. Showing data from the \"best voxel\".\nWith tens of thousands of voxels to chose from in an overall nosiy\ndata set, some of them will look pretty good.\nB. Showing activation maps that \"look similar\" or \"look different\".\nThere are many ways to chose particular slices, thresholds, etc to\nmake activations look similar or different. If the claim is that they are\nsimilar or different, this should be tested statistically on the exact\nsame voxels. Just showing similar-looking activations (especially in\ngroup data or across subjects) without statistically testing whether\nthe same voxels are activated, is very weak. Beware of sneaky\nchoice of slices; look at the anatomical images to see if it really is the\nsame slices.\n\nTips on How to Critically Evaluate fMRI Studies\n5. Some signs of a well done study:\nA. The researchers show some raw data, e.g. nonfitted time courses or at\nleast percent signal increases from fixation (or \"beta weights\") in\nindependently-defined regions of interest.\nB. The critical result is replicated at least once.\nC. More than one control condition is used, or the control condition is a\n\"minimal pair\".\n\nTips on How to Critically Evaluate fMRI Studies\n6. Some important general caveats about fMRI research:\nA. Typical imaging parametrs include about several hundred thousand\nneurons per voxel! Most studies smooth their data and average\nacross subjects which increases this number dramatically. It is a great\nmiracle that we see anything at all with this method.\nB. Temporal resolution of fMRI is lousy - at best a few 100 ms. Most of\ncognition happens in tens of milliseconds, not hundreds. So\ncomponent steps cant usually be resolved.\nC. fMRI activations do not imply necessity!"
    },
    {
      "category": "Resource",
      "title": "Neural Correlates of Scene Perception",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/8c84326e5582274037dabb541f18653f_lec4_scene_ip.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nNeural Correlates of Scene Perception\n27 September 2007\n9.71\n\nWhat's a scene?\nPhoto courtesy\nof independentman.\nPhoto courtesy\nof mikefats.\nPhoto courtesy\nPhoto courtesy\nof Kitty Cats.\nof wizwow.\nPhoto courtesy\nof antonychammond.\nPhoto courtesy\nof John-Morgan.\nPhoto courtesy\nof equusignis.\n- anything that's not an object?\n- anything that extends\nbeyond the scope of your\nview\n- anything with a spatial\nlayout\n- Oliva: space is a 3d object\nwith size and contents\n\nWhat's a place?\n- a semantically coherent (and\noften nameable) view\n- of a real world environment\nPhoto courtesy\nof Rita Crane Photography.\n- with background elements and\ndiscrete objects\nPhoto courtesy\nof folica.com.\nPhoto courtesy\nof exfordy.\n(Henderson & Hollingworth 1999)\n\"act on objects\"\n\"act in scenes\"\n(Epstein 2005)\n\n+\nWe're very good at recognizing scenes\nPhoto courtesy\nof Gaetan Lee.\nThis is termed the \"gist\" of the scene.\n\nWhat processes and representations mediates this rapid\nscene recognition?\nObject\nScene\nCourtesy of Nick Devenish.\nCourtesy of Per Ola Wieberg.\n\"bunny\"\n\"field\"\n\nWhat processes and representations mediates this rapid\nscene recognition?\nPossibility 1\nObject\nScene\nWorld/\nEye/\nVisual field\nRetinal image\nObject Recognition\nMatching\nPhoto courtesy\nCourtesy of Nick Devenish.\nCourtesy of Per Ola Wieberg.\nof Nick Devenish.\n\"bunny\"\n\"field\"\nSo... knowing the scene helped you recognize the object!\nScene\ninput\nObject\n\nAnother Example - What do you think are the hidden objects?\nCourtesy of Wonderlane.\n\nAnother Example - What do you think are the hidden objects?\nCourtesy of Wonderlane.\nAnswering this question does not require knowing what the objects look like. It is all about context.\n\nWhat processes and representations mediates this rapid\nscene recognition?\nPossibility 1\nObject\nScene\nWorld/\nEye/\nVisual field\nRetinal image\nObject Recognition\nMatching\nPhoto courtesy\nCourtesy of Nick Devenish.\nCourtesy of Per Ola Wieberg.\nof Nick Devenish.\n\"bunny\"\n\"field\"\nPossibility 2\n(Oliva and Torralba, 2006)\nCourtesy of Aude Oliva. Used with permission.\n\nQuestions\n1) Are scenes processed differently from objects in the brain?\n2) Is there evidence that scenes and objects are processed in different parallel\npathways in the brain?\n\nAre there brain regions that respond\nselectively to scenes?\n?\nScan subjects while they look at these\nthree kinds of stimuli\nCourtesy of Jason Gulledge.\nCourtesy of wrestlingentropy.\nFace photos modified by OCW\nfor privacy considerations.\n\nScenes > Faces & Objects\nin 1 subject\n\"Parahippocampal Place Area\" (PPA)\n\nPPA in all 9 subjects\nImage removed due to copyright restrictions.\nFig. 2a in Epstein, Russell and Kanwisher, Nancy.\n\"A cortical representation of the local visual\nenvironment.\" Nature 392 (1998): 598 - 601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nPPA\n(scenes>objects at t>4)\n1 subject\n\nRegion of Interest Analysis\n- Using a separate set of localizer scans, define PPA.\n- Then look at response to stimuli of interest within PPA during\ntest scans:\nImages removed due to copyright restrictions.\nFig. 1b and part of Fig. 2b (left) in Epstein, Russell\nand Kanwisher, Nancy. \"A cortical representation\nof the local visual environment.\" Nature 392\n(1998): 598 - 601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nEpstein & Kanwisher, 1998\nImage removed due to copyright restrictions.\nFig. 1a in Epstein, Russell and Kanwisher, Nancy.\n\"A cortical representation of the local visual\nenvironment.\" Nature 392 (1998): 598 - 601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nEach Scan:\nEach Epoch:\n(20 pictures in an epoch)\nIntact Houses\nIntact Objects\nInt act Scenes\nInt act Faces\nScram. Houses\nScram. Objects\nScram. Scenes\nScram. Faces\nIntact Houses\nIntact Objects\nInt act Scenes\nInt act Faces\nScram. Houses\nScram. Objects\nScram. Scenes\nScram. Faces\n5:36\n:16s :32s :48s\n300 msec\n500 msec\n300 msec\nProcedure\nTasks: Passive Viewing or 1-Back Repetition Detection\n\nResults\naverage % signal change for each condition (N=9)\nImage removed due to copyright restrictions.\nFig. 1a in Epstein, Russell and Kanwisher,\nNancy. \"A cortical representation of the\nlocal visual environment.\" Nature 392\n(1998): 598 - 601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nWhy does the PPA respond to scenes?\n- high-level visual/semantic complexity\n- multiplicity/relative position of objects\n- spatial layout\n?\nFace photos modified by OCW\nfor privacy considerations.\nCourtesy of wrestlingentropy.\nCourtesy of Jason Gulledge.\nCourtesy of greenbroke.\n\nScene\nFurniture\nArrays\nCourtesy of Jason Gulledge.\nCourtesy of greenbroke.\nEmpty\nRooms\n- visual/semantic complexity\n- multiplicity of objects\nPredictions:\n} Furniture ?? Empty Rooms\nFurniture ?? Empty Rooms\nCourtesy of ZapTheDingbat.\nCourtesy of Baltimike.\n- spatial layout\n\nScene\nFurniture\nArrays\nEmpty\nRooms\n- visual/semantic complexity\n- multiplicity of objects\nPredictions:\n} Furniture > Empty Rooms\n1.3\n0.5\n1.2\nPSC\n(N=6)\n[p<0.01]\nCourtesy of Jason Gulledge.\nCourtesy of greenbroke.\nCourtesy of ZapTheDingbat.\nCourtesy of Baltimike.\nFurniture < Empty Rooms\n- spatial layout\n\nExperiment 3\nIf the PPA responds to spatial layout, then its\nresponse to surfaces that do not define a space\nshould be low.\nImage removed due to copyright restrictions.\nFig. 4 in Epstein, Russell and Kanwisher,\nNancy. \"A cortical representation of the\nlocal visual environment.\" Nature 392\n(1998): 598 - 601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nExperiment 3 Results\naverage % signal change in PPA for each condition (N=5)\nImage removed due to copyright restrictions.\nFig. 4 in Epstein, Russell and\nKanwisher, Nancy. \"A cortical\nrepresentation of the local visual\nenvironment.\" Nature 392 (1998): 598 -\n601.\nhttp://web.mit.edu/bcs/nklab/media/pdfs/EpsteinKanwisher98.pdf\n\nIssue: Is it the layout (physical structure) or the placeness\n(meaning) of the scene that drives the PPA response?\nTo Test: Examine PPA response to layouts that are not real\nplaces in the world.\nLayout, Real Place in World\nLayout, Not Real Place\nvs.\nExperiment 4\nCourtesy of greenbroke.\n\nExperiment 4\nQuestion: Does the PPA respond strongly to spatial layouts\nthat are not real places?\nObjects\nLego Objects\nLego Layouts\nLayout+Anim. Empty Rooms\nFurn. Rooms\nCourtesy of greenbroke.\nCourtesy of wrestlingentropy.\nCourtesy of Baltimike.\n\nExperiment 4 Results\nAvg. % signal change in PPA (N=6):\n0.6\n0.6\n1.0\n1.2\n1.6\n1.6\nObjects\nLego Objects\nLego Layouts\nLayout+Anim. Empty Rooms\nFurn. Rooms\np<0.01\nCourtesy of wrestlingentropy.\nCourtesy of greenbroke.\nYes: The PPA is strongly activated by spatial layouts that do not\nrepresent real places in the world.\n\nExperiment 4 Results\nAvg. % signal change in PPA (N=6):\n0.6\n0.6\n1.0\n1.2\n1.6\n1.6\nObjects\nLego Objects\nLego Layouts\nLayout+Anim. Empty Rooms\nFurn. Rooms\np<0.01\np<0.01\nCourtesy of wrestlingentropy.\nCourtesy of greenbroke.\nYes: The PPA is strongly activated by spatial layouts that do not\nrepresent real places in the world.\nHowever, PPA response is even greater to real scenes.\n\nQuestion: Is the PPA involved in the recognition of a scene, or\nin processes specific to familiar scenes?\nTo Test: Examine PPA response to MIT versus Tufts scenes\nin MIT versus Tufts students - can thus counterbalance for\nspecific stimuli.\nConclusion: The PPA does not conduct semantic or\nother postrecognition processing on scenes.\nResult:\n1.9 PSC familiar vs. 1.8 PSC unfamiliar, n.s.\nExperiment 5\n\nQuestion: Is the PPA involved in planning navigation?\nTo Test: Examine the PPA response to places you cant\nnavigate in: \"desktop scenes\".\nExperiment 6\n\nExperiment 6: The PPA responds as\nstrongly to tabletops as to \"full\" scenes.\nPPA 2 (N = 5)\n0.00%\n0.20%\n0.40%\n0.60%\n0.80%\n1.00%\n1.20%\n1.40%\nreal scenes\ndesktops\nfull-view objects\nCourtesy of Per Ola Wieberg.\nCourtesy of independentman.\nCourtesy of Living Juicy.\n\nQuestion: Is the PPA involved in planning navigation?\nTo Test: Examine the PPA response to places you cant\nnavigate in: \"desktop scenes\".\nExperiment 6\nResult:\nResponse is just as high for tabletop as \"real\" scenes.\nConclusions:\nThe PPA is not specific to navigational planning..\n\nSummary of Exps. 1-6\nExp. 1 There is region of parahippocampal cortex that responds\nselectively and automatically to scenes.\nExp. 2 When all the objects are removed from the scenes, the\nresponse is unchanged.\nExp. 3\nWhen the surfaces of the scenes are rearranged so that\nthey no longer define a coherent space, the response is\nsignificantly reduced.\nExp. 4\nResponse to layouts is strong even if they do not represent\nreal places in the world.\nExp. 5&6\nThe PPA does not respond differentially to familiar &\nunfamiliar scenes, or to navigable versus non-navigable scenes.\nThe PPA analyzes the shape of the local environment.\n\nWhat exactly does the PPA do with scene information?\nwhat information does it represent?\nwhat tasks is it engaged in?\njust seeing spatial layout?\nscene recognition (specific versus general?)?\nnavigation? (construed broadly? web\n\"navigation\"?)\nother?\nwhat functions would you lose if you did not have a\nPPA?\nWhat about those other scene-selective regions?\nare they functionally different from the PPA? How?\n[How] do these regions interact with each other and the rest of\nthe brain?\nUnanswered Questions\n\nBeyond the PPA\n-\nThere are three contiguous brain regions that you find which have greater\nactivity for scenes than objects:\nRSC\nParahippocampal Place Area\nTransverse Occipital Sulcus\nRetrosplenial Cortex\nCourtesy of Russell Epstein. Used with permission.\n\nPresentations\n1) Are scenes processed differently from objects in the brain?\nYes! There is an area called the PPA that selectively responds to scenes and\nnot objects.\nWhat about the other scene selective areas you mentioned?\nEpstein et al, 2007. Presented by Christina.\n2) Is there evidence that scenes and objects are processed in different parallel\npathways in the brain?\nSteves et al. 2004. Presented by Jess.\n3) What is the role of the PPA in during navigation?\nJanzen & van Turennout 2004. Presented by Steve."
    },
    {
      "category": "Resource",
      "title": "Number: A Candidate \"Special\" Domain of Cognition",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/aa92f4935274fdef95cf86232d387f99_lec8b_num.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 8B\nNumber: A Candidate \"Special\" Domain\nof Cognition\nI. Understanding approximate number\nAdults\nInfants\nAnimals\nII. Brain basis of number:\nneuropsychological patients\nfMRI nderstanding approximate number\n\nUnderstanding Number\n- \"animals, young infants, and adult humans possess a\nbiologically determined, domain-specific representation of\nnumber\"\n- \"a specific neural substrate, located in the left and right\nintraparietal area, is associated with knowledge of numbers\nand their relations ('number sense'). The number domain\nis a prime example where strong evidence points to an\nevolutionary endowment of abstract domain-specific\nknowledge in the brain because there are parallels between\nnumber processing in animals and humans.\"\n-Dehaene, Dehaene-Lambertz & Cohen, TINS, 1998\n\nWhat does \"number sense\" mean?\n- Adults can represent large numerical magnitudes without verbal\ncounting.\n- The representations are approximate; discriminability of two\nnumerosities depends on their ratio.\n- The representations are abstract.\n- The representations enter into arithmetic computations\n(addition).\nFor example......\n\nWhich has more dots?\n\nWhich has more dots?\n\nWhich has more dots?\n\nWhich has more dots?\n\nWhich has more dots?\n\nHow did you do this?\nDid you count verbally?\nWhich ones were harder?\n\nHow accurate are adults' large number representations?\n(Barth, Kanwisher & Spelke, 2003)\n\"Is\nfewer or\nmore than\n?\"\n\nNumerosity discrimination by adults (Barth)\nChance (50%)\naccuracy (% correct)\n16 vs 32 16 vs 24 16 vs 20 16 vs 18 16 vs 17 8 vs 16\n8 vs 12\n8 vs 10\n8 vs 9\nWeber's Law:\nlarge (40-80)\nmedium (20-40)\nThe discriminability\nsmall (10-20)\nof two numerosities\ndepends on their ratio\n1.5\n1.25 1.15 1.1\n(not absolute diff).\nset size ratio\n\nWhat does \"number sense\" mean?\n- Adults can represent large numerical magnitudes without verbal\ncounting.\n- The representations are approximate; discriminability of two\nnumerosities depends on their ratio.\n- The representations are not based on continuous quantities like\narea, but rather on discrete number.\n? - The representations are abstract.\n? - The representations enter into arithmetic computations\n(addition).\n\nHow abstract are adults' large number representations?\n\"Is\nfewer or\nmore than\n?\"\n\n% Correct\nAccuracy\nVisual\nCrossmodal\nComparison\nComparison\nCross-modal comparisons are almost as accurate as comparisons\nwithin the visual modality alone.\n\nWhat does \"number sense\" mean?\n- Adults can represent large numerical magnitudes without verbal\ncounting.\n- The representations are approximate; discriminability of two\nnumerosities depends on their ratio.\n- The representations are not based on continuous quantities like\narea, but rather on discrete number.\n- The representations are abstract.\n? - The representations enter into arithmetic computations\n(addition).\n\nWhat can adults do with these large number representations?\nAddition of visual arrays\n\"Is the sum of\nand\nfewer or\nmore than\n?\"\n\"add\"\nFor example...\n\nadd\n\nWhat can adults do with these large number representations?\nCross-modal addition\n\"Is the sum of\nand\nfewer or\nmore than\n?\"\n\"add\"\n\nNonsymbolic Comparison and Addition\nAccuracy\nVisual\nCrossmodal\nVisual\nCrossmodal\nComparison\nComparison\nAddition\nAddition\nBarth (2001)\n\nWhat does \"number sense\" mean?\n- Adults can represent large numerical magnitudes without verbal\ncounting.\n- The representations are approximate; discriminability of two\nnumerosities depends on their ratio.\n- The representations are not based on continuous quantities like\narea, but rather on discrete number.\n- The representations are abstract.\n- The representations enter into arithmetic computations\n(addition).\nBut: the people in these studies have spent years learning and using formal\narithmetic. Do these abilities exist in infants? Animals?\n\nLarge number representations in infants\nHabituation\nOld number\nNew number\n(...)\n(...)\nTest\nXu & Spelke (2000)\n\nDiscriminating 8 vs. 16 dots at 6 months\nLooking time (sec)\nhabituation\nnew number\nold number\n\nHabituation Trials Test trials\nInfants discriminate between large numerosities in dot arrays.\n\nDiscriminating 8 vs. 12 dots at 6 months\nLooking time (sec)\nhabituation\nnew number\nold number\n\nHabituation Trials Test trials\nInfants' number representations are imprecise.\n\nLarge number representations in non-human animals\nThe case of Clever Hans\nSource: Wikipedia (public domain photo).\nArguments against number representations in animals: what\ngood is number?\n- foraging? (continuous amount, not number)\n- keeping track of offspring? (individual recognition, not\nnumber)\n\nLarge number representations in non-human animals:\nEvidence from rats (Mechner expts.)\nCourtesy of The Society for the Experimental Analysis of Behavior.\nUsed with permission. (c) 1958 The Society for the Experimental\nAnalysis of Behavior.\nRats represent the approximate number of presses.\nTheir representation of number accords with Weber's Law.\nMechner, F. \"Probability relations within response sequences under ratio reinforcement.\" Journal of the Experimental Analysis of Behavior 16 (1958): 109-121.\n\nChurch & Meck:\nabstract number in rats\nTraining phase:\nIf 2 lights or 2 sounds\npress \"2\" lever\nIf 4 lights\nor 4 sounds press \"4\" lever\n\nChurch & Meck:\nabstract number in rats\nTesting phase:\nPresent 2 lights AND 2 sounds\nRats press the \"4\" lever: spontaneous\nabstraction across modalities!\n\nWhat does \"number sense\" mean?\n- Adults can represent large numerical magnitudes without verbal\ncounting.\n- The representations are approximate; discriminability of two\nnumerosities depends on their ratio.\n- The representations are not based on continuous quantities like\narea, but rather on discrete number.\n- The representations are abstract.\n- The representations enter into arithmetic computations\n(addition).\nBut: the people in these studies have spent years learning and using formal\narithmetic. Do these abilities exist in infants? Animals?\nYES!\nThey are part of our basic cognitive machinery.\n\nI. Understanding approximate number\nAdults\nInfants\nAnimals\nII. Brain basis of number:\nneuropsychological patients\nfMRI: understanding approximate number\n\nHUMAN PARIETAL CORTEX\nNeuroanatomy\nIntraparietal sulcus\n(IPS)\ndivides superior (SPL)\nand inferior (IPL)\nparietal lobules\nRear\nview\ndra\nwin\ng of human brain, highlighting the parietal cortex.\nFigure by MIT OpenCourseWare.\n\nNeuropsychological Studies\n- Lemer, Dehaene, Spelke, Cohen (2003):\n- One \"acalculic\" patient :\n- Left parietal lobe damage\n- Bad at approximation\n- More impaired on subtraction than multiplication\n- Another \"acalculic\" patient :\n- left temporal\n- Intact approximation\n- More imparied at multiplication than subtraction\n-Taken together, these two patients are a........???\n\nfMRI: Comparing \"Approximate Vs.\nExact\" calculation (Dehaene et al, 1999)\n- Addition of Arabic Numerals\n-\nTwo versions:\n\nApproximate\n8 + 9\n12 16\n9 + 5\nA + H\n14 16\nA D\n- One control task:\nExact\nLetters\n\nImage removed due to copyright restrictions.\nTwo MRI images from Fig. 3 in Dehane, S., et al. \"Sources of Mathematical Thinking: Behavioral and\nBrain-Imaging Evidence.\" Science 284, no. 5416 (1999): 970-974.\nDehaene-Spelke Results\n-\nApproximate > exact activations (in\nyellow) in the intaparietal sulci,\nextending anteriorily to the depth of\nthe postcentral sulcus and laterally\ninto the inferior parietal lobule\n\nSimon, Mangin, Cohen, Le Bihan & Dehaene, Neuron, 2002\nClaim: this parietal\nregion is the\napproximate number\n\"HIPS\"\nsystem that has been\ncharacterized\nbehaviorally.\nBut only tested with\nsymbolic number.\nSimon, O., et al. \"Topographical Layout of Hand, Eye, Calculation, and Language-Related Areas in the Human Parietal Lobe.\"\nNeuron 33 no. 3 (2002): 475-487. Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nShuman & Kanwisher (2004)\n- Are these parietal regions engaged in\nprocessing abstract numerical magnitude?\n- Test using non-symbolic number:\n- Are these parietal regions selectively engaged\nin processing numerical magnitude?\n\nDesign\n- fMRI using 2 methods:\n- Task Manipulation\n- Adaptation\n- ROI method: Look in \"HIPS\"\n-Mean Coordinates from Meta-Analysis\n-Individual, Functionally Defined ROIs\nusing Dehaene's letter approximation task as a localizer\nSimon, O., et al. \"Topographical Layout of Hand, Eye, Calculation, and Language-Related Areas in the Human Parietal Lobe.\"\nNeuron 33 no. 3 (2002): 475-487. Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nLocalizer task: Dehaene et al, 1999, Science\n8 + 9 12 16\nA D\nA + H\nApproximate Addition\nLetter Comparison\n\nExperiment 1: Task Manipulation\n50 ms\n50-150 ms\n4000 ms Sequence + Fixation\n400 ms Array\nColor:\nSame or different?\nNumber:\nMore Flashes or Dots?\nOR\nShuman, M., and N. Kanwisher. Neuron 44 (2004): 1-20. Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nExperiment 1: Task Manipulation\n50 ms\n50-150 ms\n4000 ms Sequence + Fixation\n400 ms Array\n2 x 2 Design:\nDifficulty (Hard / Easy) x Task (Number / Color)\nShuman, M., and N. Kanwisher. Neuron 44 (2004): 1-20. Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nPredictions for a \"number\" region:\nand\n\nPredictions for a \"number\" region: :\nResults in HIPS:\nDoesn't look like a number region.\n\nExperiment 2: fMRI Adaptation\n\nExperiment 2: fMRI Adaptation\n. . .\nSequence of 16 different shape arrays\nBlocked; task = Passive viewing or color '1-back'\nShape Const\nShape Varies\nNumber Const\nNumb. Varies\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nExperiment 2: fMRI Adaptation\n. . .\nSequence of 16 different shape arrays\nShape Const\nShape Varies\nNumber Const\nNumb. Varies\nPassive viewing or color '1-back'\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nPredictions:\nNumber\nNumber\nShape\n\nPredictions:\nNumber\nShape\nResults in HIPS:\nNumber\nShape\n\nShape Adaptation in LO\nCourtesy of Susan Whitfield-Gabrieli. Used with permission.\n\nShuman & Kanwisher (2004)\n- Are these parietal regions engaged in\nprocessing abstract numerical magnitude?\n- Test using non-symbolic number:\n- Are these parietal regions selectively engaged\nin processing numerical magnitude?\nNo evidence in our data for parietal regions selectively engaged in\nprocessing number. But this debate goes on.....\n\nUnderstanding Number:\nWhat is the evidence for these Claims?\n- \"animals, young infants, and adult humans possess a biologically\ndetermined, domain-specific representation of number\"\n- \"a specific neural substrate, located in the left and right intraparietal\narea, is associated with knowledge of numbers and their relations\n('number sense'). The number domain is a prime example where\nstrong evidence points to an evolutionary endowment of abstract\ndomain-specific knowledge in the brain because there are parallels\nbetween number processing in animals and humans.\"\n-Dehaene, Dehaene-Lambertz & Cohen, TINS, 1998\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission."
    },
    {
      "category": "Resource",
      "title": "Spatial Patterns of fMRI Response",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/b0b6396004e3e8e363ddd6695360660d_lec9_pattern.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 9 9.71 Fall 2007\nI.\nMidterm\nII. Spatial Patterns of fMRI response: What information do\nthey contain and what can they tell us about object\nrepresentations?\nFeaturing work from my lab by:\nBecca Schwarzlose, Hans Op de Beeck, Leila Reddy, Mark\nWilliams.\n\nHow spatially distributed are object representations in the cortex?\nImagine a profile of response across the cortex for two object classes A & B:\nCaveat: voxels can\ntell us only about\nspatially clustered\ninformation.\ndistance across the cortex in neurons (microns) or voxels (mms)\nAre objects coded neurally by a relatively small number of peak responses?\nOr by the distributed profile of response across a larger number of\nneurons/voxels, most of which respond weakly to that object (Haxby)?\nIf the weak responses are fairly consistent within an object class,\nyet consistently different between object classes, then they contain information.\nThis question applies to micro level (neurons) and to macro level (voxels).\n\nFour Questions\nA = faces\nB = chairs\nC = shoes\n- What info is contained in the spatial profile of fMRI response across VVP? ?\n- Does SPfR in FFA contain information about nonfaces? B vs. C ?\n- Can SPfRs represent multiple objects presented simultaneously?\n- Which of the information contained in SPfRs is used/read out in\nbehavioral tasks?\n\nCorrelation-based Classification Analysis (Haxby et al., 2001)\n1. Scan each subject while they view multiple stimulus categories.\n2. Split the data in 1/2; generate activation maps for each category.\n3. Compute correlation across activation maps.\nWithin\ncategory\nbetween\ncategories\nMuch fancier\nIf r(Within) > r(Between)\nPerformance: for\nmethods exist\nwhat % of pairs is\n(e.g., SVMs).\nthe region contains category information\nr(w/in)>r(btwn)?\n\nHaxby et al (2001):\nDistributed Representations of Objects\n-Findings:\n1. Overall performance is quite good, 96% of pairwise comparisons\ndetermined correctly.\n2. So: can \"read out\" the code of what category of object the\nsubject was viewing.\nWill it generalize at all to new images of these categories?\n\n\"\nSpiridon & Kanwisher (2002)\nWe used a very similar technique to Haxby et al, but\nBut now looked for classification performance across changes in:\nImage format\nViewpoint change.\nExemplar change.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nPerformance was very good for all three, suggesting that low-level\nvisual properties are not driving performance.\nSpiridon, M. & Kanwisher, N. (2002) \"How distributed is visual category information in human occipital-temporal cortex? An fMRI study. Neuron 35 (6) 1157-1165\n\nWhat does this Distributed Object Representation Reflect?\nDoes this object information reflect\nmeaning?\nfamiliarity?\nshape?\nWe can't tell, these are all confounded.\nVVP, not just meaning.\nIf the same thing could be shown with novel shapes,\nthat would suggest that shape is represented in the\nImages: Williams, M. A,\nN. Kanwisher et al. \"Feedback\nof Visual Object Information to\nFoveal Retinotopic Cortex.\"\nNature Neuroscience 11\n(2008): 1439-1445.\nHans Op de Beeck, Jim DiCarlo, et al.....\nTwo monkeys scanned repeatedly while viewing these objects\n\n-\nConclusions: A Large-Scale Shape Map in Monkey IT Cortex\nOp de Beeck, Dicarlo, et al. (2008)\nMonkey J\nMonkey M\nNovel objects\nNovel objects\nSpikies\nSmoothies\nCubies\n- A robust topography of shape selectivity across IT,\nreplicable across time, training, task, and stim. position.\n- Systematically related to (but not reducible to) the face map.\n- Shape map includes both global and local shape features.\nSee Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676 94.\n\nObject Categorization Performance from SPfRs in VVP\nDoes this object information reflect\nNot clear yet if\nmeaning?\nhuman \"object\nshape?\nfamiliarity?\ntopography\" is\nbased on shape\nversus meaning.\nWe can't tell, these are all confounded.\nVVP, not just meaning.\nIf the same thing could be shown with novel shapes,\nthat would suggest that shape is represented in the\nImages: Williams, M. A,\nN. Kanwisher et al. \"Feedback\nof visual object information to\nfoveal retinotopic cortex.\"\nNature Neuroscience 11\n(2008): 1439-1445.\nHans Op de Beeck, Jim DiCarlo, et al.....\n\nBecca Schwarzlose\nQuestion: Is there location\ninformation in object- and\ncategory-selective regions?\n-\nMonkey electrophysiology: more sensitivity to\nlocation in IT neurons than previously thought\n» (Op de Beeck & Vogels, 2000;DiCarlo & Maunsell, 2003)\n-\nVs.\nRetinotopic mapping with fMRI in humans\nshows retinotopic organization extending\npartway down the temporal lobe\n» (Brewer, 2005; Heeger, 2006; Swisher, 2007)\n- Findings of eccentricity biases in some\ncategory-selective regions\n-(Levy et al, 2001;Hasson et al, 2003)\nLateral v\niew draw\ning of\nhuman brain s\nhowing\npaths\nfrom pr\nimary visual cortex to posterior parietal cortex and inferotemporal cortex.\nFigure by MIT OpenCourseWare.\nImage removed due to copyright restrictions. See Fig. 2 in:\nSpiridon, M., B. Fischl, and N. Kanwisher. \"Location and Spatial\nProfile of Category-Specific Regions in Human Extrastriate Cortex.\"\nHuman Brain Mapping 27 (2006): 77-89.\n\nExperimental Design\nLOCATION\nCATEGORY\nX\nCourtesy of Becca Schwarzlose. Used with permission.\nBecca Schwarzlose\n\nRegions-of-Interest (ROIs) included:\nFaces\nScenes\nBodies\nObjects\nFFA\nPPA\nFBA\npFs\nOFA\nTOS\nEBA\nLO\nCourtesy of Becca Schwarzlose.\nUsed with permission.\nBecca Schwarzlose\nImage removed due to copyright restrictions. See Fig. 2 in:\nSpiridon, M., B. Fischl, and N. Kanwisher. \"Location and Spatial\nProfile of Category-Specific Regions in Human Extrastriate Cortex.\"\nHuman Brain Mapping 27 (2006): 77-89.\n\n\"\nPattern Analysis Results: 2x2 ANOVA\n-All ROIs contain category information except earlyV\n-All ROIs contain location information except FBA\nBecca Schwarzlose\nThe Distribution of Category and Location Information in Ventral Visual Cortex.\" MIT Ph.D. Thesis, 2008.\n\nBecca Schwarzlose\nQuestion: Is there location\ninformation in object- and\ncategory-selective regions?\nYES!\n-\nMonkey electrophysiology: more sensitivity to\nlocation in IT neurons than previously thought\n» (Op de Beeck & Vogels, 2000;DiCarlo & Maunsell, 2003)\nVs. -\nRetinotopic mapping with fMRI in humans\nshows retinotopic organization extending\npartway down the temporal lobe\n» (Brewer, 2005; Heeger, 2006; Swisher, 2007)\n- Findings of eccentricity biases in some\ncategory-selective regions\n-(Levy et al, 2001;Hasson et al, 2003)\nLateral v\niew draw\ning of\nhuman brain s\nhowing\npaths\nfrom pr\nimary visual cortex to posterior parietal cortex and inferotemporal cortex.\nFigure by MIT OpenCourseWare.\nImage removed due to copyright restrictions. See Fig. 2 in:\nSpiridon, M., B. Fischl, and N. Kanwisher. \"Location and Spatial\nProfile of Category-Specific Regions in Human Extrastriate Cortex.\"\n\nHuman Brain Mapping 27 (2006): 77-89.\n\nSummary: Object Information contained in SPfRs\n- Some ability to 'read out\" what the subject was viewing from the pattern\nof response across their cortex.\nfrom 8 categories of objects (Haxby et al, 2001)\neven across image changes (Spiridon & Kanwisher 2002)\neven for novel shapes (Op de Beeck et al, in press)\nsome exemplar information (Tong/Kamitani VSS 2005)\nsome location information in VVP (Schwarzlose et al., sub)\nnot Joe versus Bob in the FFA (Kriegeskorte in press)\nbut maybe temporal pole?\n\nFour Questions\n- What info is contained in the spatial profile of fMRI response across VVP? ?\nCrude object category information is present in the SPfR across VVP\n- Does SPfR in FFA contain information about nonfaces? ?\n- Can SPfRs represent multiple objects presented simultaneously?\n- Which of the information contained in SPfRs is used/read out in\nbehavioral tasks?\n\nNonpreferred Responses in the FFA\nF\nF\nF\nO\nO\nO\nFaces > Objects\n% signal change\n?!\nTime (seconds)\n- Do \"nonpreferred\" responses carry information about nonpreferred stimuli?\n- A potential challenge to the domain specificity of the FFA.\nCourtesy of Society for Neuroscience. Used with permission.\n\nDoes the Pattern of Response Across the FFA\ncontain information that discriminates between nonfaces?\nHaxby et al (2001): yes\n\"Regions such as the .... 'FFA' are not dedicated to representing only .... human\nfaces,.. but, rather, are part of a more extended representation for all objects.\"\nSpiridon & Kanwisher (2002): no\nTsao et al (2003), in face patches in monkey brains: no\nO'Toole, Haxby et al. (2005): no (sort of):\n\"preferred regions for faces and houses are not well suited to object classifications\nthat do not involve faces and houses, respectively.\"\nReddy & Kanwisher (in prep): yes (sort of) - stay tuned.\nBUT: maybe these tests are unfair, in two ways:\ni) Spatial resolution limits of fMRI necessarily entail some\ninfluence of neural populations outside the region in question.\nii) The presence of discriminative information does not mean it\nplays an important role in perception!\n\nThe Ultimate High Resolution: Single-Unit\nNeurophysiology\nTsao et al (2003) found three face-selective patches in\nmonkeys using fMRI:\nDoris Tsao\nImage removed due to copyright restrictions.\nDiagram of macaque brain surface, highlighting middle face patch and body response regions.\nIn Kanwisher, N. \"What's in a Face?\" Science 311 no. 5761 (2006): 617-618.\n(http://web.mit.edu/bcs/nklab/media/pdfs/Kanwisher.science2006.perspec.pdf)\nTsao et al (2006) directed electrodes smack into the middle face patch\nand recorded from neurons that comprise it.\n\nResponse of all 320 visually-responsive neurons\nin the faces patches of two monkeys\nto 96 different stimuli\nImage removed due to copyright restrictions.\nFig.2B in Tsao, Doris. \"A Cortical Region Consisting Entirely of Face-Selective Cells.\"\nScience 311 no. 5761 (2006): 670-674. doi:10.1126/science.1119983.\nThe cells in this patch respond selectively, indeed virtually exclusively to faces.\nTsao et al (2006), Science\n\nAn Important Challenge\nHaxby and others\nF\nF\nF\nO\nO\nO\nFaces > Objects\n% signal change\n?!\nTime (seconds)\n- At higher resolution, can you find information that discriminates\nbetween nonfaces? Face patches have very little discriminative\ninformation about nonfaces, at least in monkeys.\n- Even if there were some information about nonfaces in the FFA,\nis this information necessary for perceiving those objects?\nCourtesy of Society for Neuroscience. Used with permission.\n\nWada & Yamamoto (2001)\nIs profoundly prosopagnosic but apparently normal at object recognition.\nSuggests: Information in the FFA is critical for face recognition\nbut not for object recognition.\nImage removed due to copyright restrictions.\nFigure 1 in Wada, Y. and T. Yamamoto. \"Selective Impairment of Facial Recognition\ndue to a Haematoma Restricted to the Right Fusiform and Lateral Occipital Region.\" J Neurol\nNeurosurg Psychiatry 71 (2001): 254-257.\n\nFour Questions\n- What info is contained in the spatial profile of fMRI response across VVP?\nCrude object category information is present in the SPfR across VVP\n- Does SPfR in FFA contain information about nonfaces? ?\nProbably not (or not much).\n- Can SPfRs represent multiple objects presented simultaneously? ?\nThe problem of crosstalk in overlapping distributed representations\n- Which of the information contained in SPfRs is used/read out in\nbehavioral tasks?\n\nThe Problem:\nIn prior classification experiments, subjects have viewed stimuli like this:\nBut natural images look more like this:\nmultiple objects\nbackground texture\n\"clutter\"\nGeneral Question:\nAre SPfR codes any use in natural viewing conditions, i.e. in clutter?\n\nReddy & Kanwisher (2007)\nSpecific Questions:\n1.) Can information about object category be read out\nfrom the distributed profile of fMRI response even when two\nobjects are present in the stimulus at once (\"clutter\")?\n2) How is the ability to read out the code for an object\naffected by attention?\n3) Does the answer to these questions depend on the\ncategory of the stimulus?\n\nReddy & Kanwisher (2007)\nCurrent Biology 17 no. 23 (2007): 2067-2072. doi:10.1016/j.cub.2007.10.043.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nSubjects perform 1-back on stimuli of one category in a block.\n- Isolated/Attended/Unattended\n- Faces & houses versus Shoes & cars\n- Look at classification performance in FFA/PPA/\"OR\"\n\nResults 1: Is there a \"Clutter Cost\"?\n1. Performance is\nhigher for isolated\nthan attended,\n= \"clutter cost\".\n[2. Performance is\nhigher for faces and\nhouses than shoes\nand cars.]\nWhat re FFA/PPA?\n3. There is no\n\"clutter cost\" for\nfaces in the FFA\nor for houses\nin the PPA.\n[4. Above-chance performance for shoes & cars in FFA (even when face & houses are excluded).]\nIsolated vs attended\nFaces Houses Shoes Cars\nWhat happens when attention is diverted?\nReddy & Kanwisher (2007)\n\nResults 2: Is there an Effect of Attention?\n1. Performance is\nnot above chance\nfor any category in\nOR when\nunattended.\nBut what re FFA/PPA?\n2. Some \"sparing\"\nfrom costs of diverted\nattention for faces in\nthe FFA and houses\nin the PPA.\nFaces Houses Shoes Cars\nAttended vs Unattended\nBUT:\nIs this sparing for faces and houses in clutter and diverted attention just due to the fact\nthat performance is higher for these categories to begin with?\nReddy & Kanwisher (2007)\n\nPerformance in all conditions when performance in the\nisolated condition is equalized by throwing out voxels\nCurrent Biology 17 no. 23 (2007): supplemental resources. doi:10.1016/j.cub.2007.10.043.\n- Qualitatively same pattern of results obtained when isolated performance is equalized.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nReddy & Kanwisher (2007)\n\nReddy & Kanwisher (2007)\nSpecific Questions:\n1.) Can information about object category be read out from the distributed\nprofile of fMRI response even when two objects are present in the stimulus at\nonce (\"clutter\")?\nA substantial clutter cost, challenging utility of SPfRs as object representations\n2) How is the ability to read out the code for an object affected by attention?\nA substantial cost of diverting attention, with very little object info present\nfor unattended objects\n3) Does the answer to these questions depend on the category of the\nstimulus?\nYes! Selective sparing of faces & houses in clutter and (somewhat) in\ndiverted attention\n\nFour Questions\n- What info is contained in the spatial profile of fMRI response across VVP?\nCrude object category information is present in the SPfR across VVP\n- Does SPfR in FFA contain information about nonfaces?\nProbably not (or not much).\n- Can SPfRs represent multiple objects presented simultaneously? ?\nLimited utility of distributed cortical fMRI codes in \"clutter\"\nBUT: category selectivity confers robustness to clutter & diverted attention\n- Which of the information contained in SPfRs is used/read out in\n\nbehavioral tasks? ?\n\nAre SPfRs used/read out?\nHaxby argues that the whole spatial profile of response across centimeters\nof ventral visual pathway constitutes the representation of an object.\nAnd indeed there is some object info spread across here.\nBut:\nJust because some information is present in the SPfR\nin a given ROI does not mean that that SPfR is\npart of the representation.\nWhat we need to know:\nIs that SPfR used/read out behaviorally?\nHow could we tell? Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\n\"\nWilliams et al (submitted)\nOverall Logic\nBrief masked stimuli, shape categorization task\nsome errors, but above chance\nbin fMRI data by behavioral performance\nKey prediction:\nAny pattern information that is used in task performance\nshould be stronger on correct than incorrect trials, i.e.:\n[r(w/in) - r(betwn)] on correct trials > [r(w/in) - r(betwn)] on incorrect trials\nImages: Williams, M. A, N. Kanwisher et al. Feedback of visual object information to foveal retinotopic cortex.\" Nature Neuroscience 11 (2008): 1439-1445.\n\n\"\nLateral Occipital Complex (LOC)\nWilliams et al, Results\nRetinotopic Cortex\nWithin-category\nBetween-category\nInformation is present in\nretinotopic cortex, but not\nrelated to behavior.\nInformation is present in\nLOC on correct trials, not\non incorrect trials>>>>\nThis information is\napparently read out in\nperformance. Other areas?\nWilliams, M. A, S. Dang, and N. Kanwisher. Only some\nspatial patterns of fMRI response are read out in task\nperformance.\" Nature Neuroscience 10 (2007): 685-686.\n\nRetinotopic Cortex\nWilliams et al, Results\nLateral Occipital Complex (LOC)\nPosterior Fusiform\nFFA\nNo info re nonface objects!\nNo info re nonface objects!\n\nCould weaker category information in incorrect trials be due to the\nsmaller number of trials?\nNo\nEqualised trial numbers\nLateral Occipital\nLateral Occipital\n\nIs the greater category information in LO than FFA due to\na larger number of voxels?No\nEqualised voxel numbers.\nLateral Occipital\nLateral Occipital\nFFA\n\nFour Questions\n- What information is contained in the spatial pattern of fMRI response?\nCrude object category information is present in the SPfR across VVP\n- Does SPfR in FFA contain information about nonfaces?\nProbably not (or not much).\n- Can SPfR across cortex represent multiple objects presented simultaneously?\nlimits on utility of distributed cortical fMRI codes in \"clutter\"\ncategory selectivity confers robustness to clutter & diverted attention\n- Which of the information contained in SPfRs is used/read out?\nNot all information present in SPfRs is part of the representation,\ni.e. is read out in task performance.\nNow we have a way to tell which SPfRs are read out in which tasks.\nDoes readout change with expertise? Task?\n\nHow Distributed are Object Codes in the Cortex?\nHaxby argues that the whole spatial profile of response across centimeters\nof ventral visual pathway constitutes the representation of an object.\nOur data suggest that:\n- information about some object categories is\nindeed somewhat spread out in the cortex, BUT\n- some categories (e.g., faces) are represented\nin much more focal regions of cortex (why?!)\n- distributed object representations may be of limited utility because\nthey are vulnerable to clutter\n- and only part of the distributed representation is \"used\" Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nCaveats/Open Questions/Challenges\nfMRI Classification methods are exciting because they give us a way to look\nat information in the brain. But:\n- Voxels are not natural kinds. No one knows how readout of neural\ncodes works, but it is a good bet that what is read out is spikes not blood flow\nchanges.\n- We are looking at the cortex at a very coarse scale. Failure to detect\npattern information with fMRI does not mean it is not there!\n- So far ability to see finer-grained discriminative information is\ndisappointing.\n- On the other hand, SPfR analyses do afford a much wider field of\nview than available in physiology studies....\n\nDistributed Representations to Classify\nVisual Objects\n- Haxby et al. (2001): Activity patterns in ventral temporal\ncortex can reliably predict if subject is viewing images of\nfaces, houses, cats, bottles, scissors, shoes, chairs,\nscrambled stimuli\nPattern of\nActivity on\nRuns 1-4\nPattern of\nActivity on\nRuns 5-8\nR≈0\nR≈.25-.75\nR≈.25-.75\n\nSummary of Orientation Experiments\n- Ensemble activity patterns in early human visual areas\ncontain highly reliable information about stimulus orientation\n- Brain states, evoked by unambiguous stimuli, can be used to\nreliably predict mental states\n- Multi-voxel pattern analysis may allow for extraction of\nfeature information that varies at much more fine-scale\nresolutions (i.e. cortical columns)\n- This approach may be useful for cortical feature tuning, mind\nreading, possibilities for human neuroprosthesis, and the\nneural underpinnings of mental states\n\n\"\nSpiridon & Kanwisher - Design\nScan each of 6 subjects on the same 8 categories used by Haxby et al;\n- Twelve different kinds of blocks for each category created by\northogonally crossing exemplar x viewpoint x format:\nPhotographs I\nPhotographs II\nLine Drawings\nExemplars A Exemplars B\nExemplars A Exemplars B\nExemplars A Exemplars B\nView 1\nView 2\n8 runs\n8 runs\n8 runs\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nSpiridon, M. & Kanwisher, N. (2002) \"How distributed is visual category information in human occipital-temporal cortex? An fMRI study. Neuron 35 (6) 1157-1165\n\nSpiridon & Kanwisher - Design\nPhotographs I\nPhotographs II\nLine Drawings\nExemplars A\nExemplars B\nExemplars B\nExemplars A\nView 1\nView 2\nExemplars A Exemplars B\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nTo test replicability of Haxby et al result (across identical photographs),\nUse activation from yellow blocks as original basis images to be used\nas the \"key\" in categorizing the activation from red \"test\" data.\n\nSpiridon & Kanwisher - Design\nScan each of 6 subjects on the same 8 categories used by Haxby et al;\n- Eight different kinds of blocks for each category created by\northogonally crossing exemplar x viewpoint x format:\nPhotographs I\nPhotographs II\nLine Drawings\nExemplars A Exemplars B\nExemplars A Exemplars B\nExemplars A Exemplars B\nView 1\nView 2\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nTo test generalization across format (photo > line drawing),\nUse activation from yellow blocks as original basis images to be used\nas the \"key\" in categorizing the activation from red \"test\" data.\n\nSpiridon & Kanwisher - Design\nScan each of 6 subjects on the same 8 categories used by Haxby et al;\n- Eight different kinds of blocks for each category created by\northogonally crossing exemplar x viewpoint x format:\nPhotographs I\nPhotographs II\nLine Drawings\nExemplars A Exemplars B Exemplars A Exemplars B\nExemplars A Exemplars B\nView 1\nView 2\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nTo test generalization across viewpoint,\nUse activation from yellow blocks as original basis images to be used\nas the \"key\" in categorizing the activation from red \"test\" data.\n\nSpiridon & Kanwisher - Design\nScan each of 6 subjects on the same 8 categories used by Haxby et al;\n- Eight different kinds of blocks for each category created by\northogonally crossing exemplar x viewpoint x format:\nExemplars A\nExemplars B\nExemplars B\nExemplars A\nPhotographs I\nLine Drawings\nView 1\nView 2\nExemplars A\nExemplars B\nPhotographs II\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\nTo test generalization across exemplars,\nUse activation from yellow blocks as original basis images to be used\nas the \"key\" in categorizing the activation from red \"test\" data.\n\nHypothesis: IT cortex contains a large-scale map of shape.\nPredictions:\ni) Continuous and spatially varying shape selectivity will be found\nspanning much of IT cortex, at a large grain.\nii) This \"shape map\" will be stable across\ntime, training, task, and stimulus position\nBackground:\nTanaka/Tanifuji/others - columnar organization of IT in macaques\nscale: several mms >> what about larger scale?\nHuman fMRI - category-selective regions, and patterns, but\nvirtually all of this work is on familiar, meaningful objects\nto unconfound shape from meaning and memory>>\nuse novel objects!\n\nTesting for Shape Maps with Novel Objects\nOp de Beeck, Vanduffel, Deutsch, Kanwisher, & DiCarlo\nTwo Monkeys scanned while viewing 3 novel object classes:\nSmoothies\nSpikies\nCubies\nStimuli: 5x5 degrees\nImage removed due to copyright restrictions.\nSee Fig. 2 in Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar\nTR = 3 sec\nShape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676-94.\nBlock length: 45 sec\n[http://web.mit.edu/bcs/nklab/media/pdfs/OpdeBeeck_etal_2007.pdf]\nBlocked design; monkeys perform orthogonal color task.\n1.25 mm isotropic functional resolution; MION\nTo look for \"shape maps\", compute\n3 pair-wise t-tests on each visually responsive voxel in IT, &\ndisplay it like this.....\n\n-\nShape Maps in IT Cortex?\n>>>Are they stable across time, training, task, and stimulus position?\nMonkey J\nMonkey M\nSpikies\nSmoothies\nCubies> Spikies p<10-5 and\nCubies\nCubies> Spikies p<10-2 and\nCubies> Smoothies p<10-5\nSmoothies> Spikies p<10-2\nHigher saturation indicates higher selectivity.\nSee Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676 94.\n\nScanning Sequence\nMonkeys were scanned while viewing novel objects in four phases\n(several scanning sessions each):\nPhase 1 Scans. Color task - detect rare color changes between successive stimuli.\n>>Three months of training on one object category on successive same-different\nshape judgement. 130,000 stimulus presentations.\nPhase 2 Scans. Color task: transfer over time & training (& meaning change)?\nPhase 3 Scans. Shape discrimination task: transfor over task?\nPhase 4 Scans. Position change: transfer over stimulus position?\nMonkey J - stimuli @ 8.4 degrees eccentricity, nonoverlapping (fixation task).\nMonkey M - stimulus-class-specific jitter to remove differences in retinotopic\nenvelope; color task.\n\nGet\nMonkey J\n(trained on Spikies)\nMonkey M\n(trained on Smoothies)\nImage removed due to copyright restrictions.\nSee Fig. 6 in Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography\nof Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal\nCortex.\" Cerebral Cortex 18 no. 7 (2008): 1676-94.\n\n(http://web.mit.edu/bcs/nklab/media/pdfs/OpdeBeeck_etal_2007.pdf)\nShape maps look\nsimilar across time,\n& training.\nTo quantify the\nreplicability of these\nmaps within an animal,\nwe calculated the\ncorrelation across the\ncolor maps between\nexperiments.\nShape maps are about\nas replicable across\nexperiments as they can\nbe given the variability\nin the data!\nCorrelations across position are zero in V1.\n\nHypothesis: IT cortex contains a large-scale map of shape.\nPredictions:\n√ i) Continuous and spatially varying shape selectivity will be found\nspanning much of IT cortex.\nii) This \"shape map\" will be stable across\ntime, training, task, and position\n√\n√\n√\n√\nMany more questions.....\n1. Is the \"shape map\" systematically related to the face patches?\n2. Is it reducible to the face map?\n3. Does the \"shape map\" reflect selectivity for global or local features?\n\n-\nComparison of Novel Object Map and Face Map\nMonkey J\nMonkey M\nNovel objects\nNovel objects\nSpikies\nSmoothies\nCubies\nNatural objects\nNatural objects\nObjects\nFaces\nScrambled\nimages\n- Novel object map bears a clear relationship to face map.\n- But is not reducible to it: correlations in object selectivity maps (r=0.52, p<.001)\nare almost as strong when all face-selective voxels are omitted (r=.47) .\nSee Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676 94.\n\nV A R 0 0 0 2 9\n- 1\n- 2\n- 3\n1 . 5\n1 . 0\n. 5\n0 . 0\n- . 5\n- 1 . 0\n- 1 . 5\nV A R 0 0 0 1 0\ns p d\ns p\ns m d\ns m\nc u d\nc u\n\"Shape Map\": Global or Local Features?\nTest on new stimulus set where local and global feature are exchanged:\nSpiky smoothies\nCuby spikies\nSmoothy cubies\nPerceived similarity between six classes\nCorrelation btwn selectivity maps for\nDimension 1\nDimension 2\n0.6\n0.3\nCorrelation\nGlobal Local\nGlobal+Local\nbased on same-different confusion matrix.\nobjects sharing glob, loc, or both.\nGlobal\nLocal\nMonkey J\nMonkey M\nSee Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676-94.\n\nHypothesis: IT cortex contains a large-scale map of shape.\nPredictions:\n√ i) Spatially varying selectivity will be for novel objects spanning\nmuch of IT cortex.\nii) This \"shape map\" will be stable across\ntime, training, task, and position\n√\n√\n√\n√\nMany More Questions:\nYES 1. Is it systematically related to the face patches?\nNO\n2. Is it reducible to the face map?\nBOTH 3. Does the \"shape map\" reflect selectivity for global or local features?\n\n-\nConclusions: A Large-Scale Shape Map in Monkey IT Cortex\nMonkey J\nMonkey M\nNovel objects\nNovel objects\nSpikies\nSmoothies\nCubies\n- A robust topography of shape selectivity across IT,\nreplicable across time, training, task, and stim. position.\n- Systematically related to (but not reducible to) the face map.\n- Shape map includes both global and local shape features.\nCourtesy of the American Physiological Association. Used with permission.\nSee Op de Beeck, H. P., N. Kanwisher et al. \"A Stable Topography of Selectivity for Unfamiliar Shape Classes in Monkey Inferior Temporal Cortex.\" Cerebral Cortex 18 no. 7 (2008): 1676 94.\n\nQuestions\n- What is this map a map of?\n- What is the dimensionality of this map?\nso far: 3-dim projection of presumably much bigger space\n- How consistent is the shape map across individuals? Species?\n- How does the shape map arise in development?\nFace patches first?\nShape map first?\nDo both arise together and somehow constrain each other?\n- How are the properties of this shape map related to perception?\n- Why do things cluster in the cortex in the first place?\n- Which parts of the map are used/ read out during task performance?\nAn even smaller speck of data on this last point.....\nCourtesy of the American Physiological Association. Used with permission.\n\nCorrelations across all object categories\n\nTalk Outline\n1) Is there location information in object- and\ncategory-selective regions?\n- Means: Yes\n- Pattern: Yes\n2) Are category and location information\nindependent or do they interact?\n\nTalk Outline\n1) Is there location information in object- and\ncategory-selective regions?\n2) Are category and location information\nindependent or do they interact?\n4 Categories x 3 Locations x 8 ROIs\nTwo methods:\n- Mean response magnitude\n- Pattern classification analyses\n\n\"\nCategory Biases Across Locations\nSchwarzlose, R. A. The Distribution of Category and Location Information in Ventral Visual Cortex.\" MIT Ph.D. Thesis, 2008\n\nBut what about location biases?\n\n\"\nLocation Biases Across Categories\nSchwarzlose, R. A. The Distribution of Category and Location Information in Ventral Visual Cortex.\" MIT Ph.D. Thesis, 2008\n\nTalk Outline\n1) Is there location information in object- and\ncategory-selective regions?\n- Means: Yes\n2) Are category and location information\nindependent or do they interact?\n\nAn Orthogonal Measure of Location\nInformation:\nPattern Classification Analyses\n(voxel-based \"population code\")\n\nOrthogonal Measures of Information\nDifferent Mean,\nDifferent Pattern,\nSame Pattern\nSame Mean\n\nCorrelation-based classification analysis\n(Haxby et al., 2001)\n1.\nScan each subject while they view multiple stimulus conditions.\n2.\nSplit the data in half; generate activation maps for each condition.\n3.\nCompute correlation across activation maps.\nWithin\ncategory\nBetween\ncategories\nIf r(Within) > r(Between), the region contains information\n\nFormat of Pattern Classification Results\nCATEGORY\nSame\nDifferent"
    },
    {
      "category": "Resource",
      "title": "The Neural basis of Perceptual Awareness or Perceptual Representation vs. Perceptual Awareness: What’s the Difference?",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/8cff1bfa5154a98df747ae9bb22f2440_lec7_awareness.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n9.71 Fall 2007\nLecture 7: The Neural basis of Perceptual Awareness\nor\nPerceptual Representation vs. Perceptual Awareness:\nWhat's the Difference?\nOutline of Class today:\nI. Reminder: Term Paper outlines due in class next week!\nII. Lecture: Perceptual Awareness\n(break in the middle somewhere)\nIII. Presentation: Nune, on Pasley et al (2004)\n\nTerm Papers\nOption I: Write a Review Article\nChoose a relatively focused question about visual cognition that has been investigated\nextensively with fMRI, and write a term paper reviewing the relevant literature on this\nquestion. Conclude with a discussion of how the question has been answered by the\nrelevant literature, and what aspects of the question have not been answered. Finally,\ndiscuss any important questions for future fMRI research that arise from the literature\nyou have reviewed.\nA topic question and article outline (including REFERENCES TO at least ten of the main\narticles you will review) is due at the beginning of class October 25\nOption II: Propose a Novel Experiment\nHere you will propose a novel experiment testing a theoretically-motivated hypothesis\nthat has not been resolved in the prior literature. This assignment requires more\nindependent thought and creativity than Option I, but may also emerge naturally from\nOption I. If you elect Option II you should email Nancy or Talia a very short synopsis\nof your idea (one paragraph) as early as possible. Your choice of topics is subject to my\napproval of the outline you hand in on October 25. The requirements for the three\nphases of the project are described below.\nCarefully read \"Experiment Proposal Outline Guidelines\" from handout for what is due Oct\n25..\n\nConscious States versus Conscious Percepts\nA distinction:\nImage removed due to copyright restrictions.\n- Conscious states: normal waking /\nfMRI images of supplementary motor area in\ntwo imagery scenarios: playing tennis and\nsleep/ meditation/ anesthesia/ PVS/\nwalking around the house.\nSee Figure 1 in Owen, A. M., et al.\ncoma\n\"Detecting awareness in the vegetative\nstate.\" Science 313 (2006): 1402.\n- Consciousness of something (= perceptual awareness).\nthis is what we'll try to grapple with today\n\nWhat is to be explained?\nConsider two cases:\na stimulus lands on your retina, and you become aware of it\nthe same stimulus lands on your retina, and you fail to become aware of it\nA complete understanding of perceptual awareness would have to specify:\n- What happens differently in the mind and brain in the two cases?\nbrain: the \"neural correlates of consciousness\" (NCC)\n- Why does it happen differently?\ni.e., what are the causal mechanisms that lead to one outcome vs. another?\n- What are the consequences of this difference?\n\nConsiderable hoopla (from Crick,\nKoch, others) about the \"neural\ncorrelates of consciousness\" (NCC)\nCourtesy of Christopher Koch. Used with permission.\nLet's instead use NCA since it is awareness that we are focusing on\nDo they involve some special class of neurons?\nDo they all live in some \"theater of awareness\" localized somewhere in\nthe brain?\n\nBeware Assuming a \"Cartesian Theater\"\nWhere the contents of consciousness are presented.\n\"So a function of consciousness is to present the\nresults of underlying computations- but to whom? The\nQueen? This kind of hypothesizing merely begs the\nquestion: \"And then what happens?\" and avoids the\nhard questions of how to explain \"the tricky path from\n(presumed) consciousness to behavior, including,\nespecially, introspective reports.\"\nPhoto of Prof. Daniel\nC. Dennett removed\ndue to copyright\nrestrictions.\nDan Dennett\n\nUncoupling Mental/Neural Representation and Awareness\nSame stimulus > different awareness\n- Identical stimuli that are either perceived or not (on diff trials)\n- Also: attention (same stim, diff experience)\n- Rivalry\nLet's consider some data from fMRI that speak to this.\nWe can use the FFA and PPA as \"markers\" for representations of faces\nand houses respectively.\nAnd we can ask what happens in e.g. the FFA when a face is presented\nand the subject is versus is not conscious of the stimulus.\nI.e. conscious of different things: it's presence, it's category, it's\nspecific identity\n\nCorrelating fMRI signals with behavioral outcomes\nOverall Strategy:\n- Have Ss perform perceptual task in scanner;\n- Make task difficult so subjects make some mistakes;\nbin fMRI data by behavioral response;\n- Look for correlations btwn behavioral responses and BOLD\nresponse in FFA (identified individually in each subject in a\nprior \"localizer\" scan).\nKalanit Grill-Spector\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\ntime\n2000ms\nTask: is this\ni) Harrison Ford\nii) some other guy\niii) nothing\nimage exposure\n33ms or 50ms:\nNear threshold\nPhoto of Harrison Ford\nremoved due to\ncopyright restrictions.\nPhoto of Richard Gere\nremoved due to\ncopyright restrictions.\n4000ms\n2000 ms\nStimulus images not repeated\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\nRight FFA Response to Target Faces (e.g., Harrison)\nAs a Function of Performance, N=5\nIdentification\nCategori-\nzation/\nDetection\n0.6 -\nPercent Signal Increas\n|\n6 sec\nTime\nIdentified\n(\"Harrison\")\ndetected but\nnot identified\n(\"someone else\")\nnot detected\n(\"nothing\")\nFFA Involved in:\nDetec & Ident of faces\nWhat about other\nkinds of objects?\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\ntime\n2000ms\nimage exposure\n4000ms\n2000 ms\nTask: is this\ni) electric guitar\nii) other guitar\niii) nothing\n33ms or 50ms\nStimulus images not repeated\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\nRight FFA Response to Target Faces or Guitars\nIdentified\ndetected but\nnot identified\nnot detected\nFFA involved in:\nDetec & Ident of faces\nNeither detect nor ident\nof guitars.\nAs a Function of Performance, N=5\nfaces\n0.6 -\nPercent Signal Incr\n|\n6 sec\nTime\nguitars\n0.6 -\nPercent Signal Incr\n|\n6 sec\nTime\n\nUncoupling Mental/Neural Representation and Awareness\nSame stimulus > different awareness\n- Identical stimuli that are either perceived or not (on diff trials)\n- Also: attention (same stim, diff experience)\n- Rivalry\nQuestion 1: What happens in e.g. the FFA when a face is presented and the\nsubject is versus is not conscious of the stimulus (or its category/identity).\nAnswer 1: Activity in the FFA is higher when the S is aware of\nthe presence/category of stimulus, higher yet for specific identity.\nBUT: retinal stimulation not identical...\nQuestion 2: What happens in the FFA and the PPA when awareness switches\nbetween face and house, even though the retinal stimulus has not changed?\n\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nYoked\nStimulus\nTime (s)\nNonrivalry\nBinocular Rivalry and Visual Awareness\nTong, Nakayama, Vaughan & Kanwisher (1998)\nRivalry\nStimulus\nPercept\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\n.\nTong et al 1998\n% MR Signal\nH\nH\nH\nH\nH\nF\nF\nF\nF\nF\n-2\n-1\nPPA\nFFA\n+\n+\nTime (s)\n\n.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n% MR Signal\n% MR Signal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-8 -4\nFFA\nPPA\nHouse\nFace\nS1\n-8 -4\nFFA\nPPA\nTime from reported perceptual switch (s)\nRivalry\nFace\nHouse\nTong , Nakayama, Vaughan, & Kanwisher,1998\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nBinocular Rivalry and Visual Awareness\nTong, Nakayama, Vaughan & Kanwisher (1998)\nYoked\nStimulus\nTime (s)\nNonrivalry\nRivalry\nStimulus\nPercept\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nTong et al 1998\n.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-8\n-4\nFFA\nPPA\nS1\nFFA\nPPA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-8\n-4\nRivalry\nNon-Rivalry\nTime (s)\nTime (s)\nHouse\nFace\nHouse\nFace\n% MR Signal\n% MR Signal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-8\n-4\nFFA\nPPA\nFFA\nPPA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-8\n-4\nTime (s)\nTime (s)\nRivalry\nNon-Rivalry\nS1\nFace\nHouse\nFace\nHouse\n% MR Signal\n% MR Signal\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nTong, Nakayama, Vaughan & Kanwisher (1998)\n-1\n-0.5\n0.5\n-1\n-0.5\n0.5\nS1\nS4\nS2\nS3\nFFA PPA Subject\nNonrivalry\nChange in\n% MR Signal\nRivalry\n1:1\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nUncoupling Mental/Neural Representation and Awareness\nSame stimulus > different awareness\n- Identical stimuli that are either perceived or not (on diff trials)\n- Also: attention (same stim, diff experience)\n- Rivalry\nQuestion 1: What happens in e.g. the FFA when a face is presented and the\nsubject is versus is not conscious of the stimulus (or its category/identity).\nAnswer 1: Activity in the FFA is correlated with subject's awareness.\nQuestion 2: What happens in the FFA and the PPA when awareness switches\nbetween face and house, even though the retinal stimulus has not changed?\nAnswer 2: Activity in the FFA and PPA is 100% driven by what the\nsubject is aware of, completely unconfounded from the stimulus!\n\nUncoupling Mental/Neural Representation and Awareness\nSame stimulus > different awareness\n- Identical stimuli that are either perceived or not (on diff trials)\n- Also: attention (same stim, diff experience)\n- Rivalry\nWhat happens differently in the brain when you are vs. are not aware of the\nstimulus?\n>>> Can find \"NCC\", unconfounded from the stimulus. Lots of these .\nThey arent all in the same single \"awareness area\" in the brain, instead:\nThe neural correlates of awareness of a given stimulus attribute are found in\nthe neural structure that analyzes that stimulus attribute.\nWhat are we to do with these NCCs now that we have them?\nReally we want to understand not just correlation, but causal connection\n\nUncoupling Perception and Awareness\nNCCs:\ncorrelations\nStronger tests of\ncausal connection:\nKanwisher, N. \"Neural\nevents and perceptual\nawareness.\" Cognition\nAre these NCCs\n79, no. 1-2 (April 2001):\n89-113.\nnecessary for awareness?\nCourtesy Elsevier, Inc.,\nhttp://www.sciencedirect.com.\nUsed with permission.\nLook for cases here.\nAre they sufficient? Look for cases here.\ncases of representation/neural activity without awareness?\na topic in American psychology since James, Sidis (1898)\nwhat does this mean?....\n\nUncoupling Representation and Awareness\nRepresentation without awareness\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nHow do you measure these?\nIndirect Tests:\nBehavioral evidence, e.g.:\npriming\npsychophys. adaptation\nNeural evidence, e.g.:\ndifferential responses\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nExplicit tests:\nForced choice (2AFC)\ndetection of stimulus.\n- this stimulus not detected\n- overall at chance\nDiscrimination (e.g., 2AFC)\nSome examples.....\nfMRI adapt'n; classific'n\n\nRepresentation without Awareness: Example 1\n1. He & MacLeod (2001): found orientation-selective tilt after-effect from\n\"invisible\" gratings.\n- adapt to gratings so high in spatial frequency they are perceptually\nindistinguishable from a uniform field,\n- observers nonetheless experience tilt aftereffect from these gratings\n\nReprsentation without Awareness: Example 1\n1. He & MacLeod (2001): found tilt after-effect from \"invisible\" gratings.\n\nRepresentation without Awareness: Example 1\n1. He & MacLeod (2001): found orientation-selective tilt after-effect from\n\"invisible\" gratings.\n- adapt to gratings so high in spatial frequency they are perceptually\nindistinguishable from a uniform field,\n- observers nonetheless experience tilt aftereffect from these gratings\nConclusion:\n\"Because these after-effects are due to changes in orientation-sensitive\nmechanisms in visual cortex, our observations imply that extremely fine\ndetails, even those too fine to be seen, can penetrate the visual system as far as\nthe cortex, where they are represented neurally without conscious awareness.\"\n\nRepresentation without Awareness: Example 2\nBut that is just orientations.\nWhat about conjunctions of orientation & color?\nTest with the McCoullough Effect:\nEd Vul and David MacLeod, Nat Neurosci 9:7 (2006): 873.\ndemo.....\nCourtesy Edward Vul. Used with permission.\n\nCourtesy Edward Vul. Used with permission.\n\nCourtesy Edward Vul. Used with permission.\n\nCourtesy Edward Vul. Used with permission.\n\nRepresentation without Awareness\nExample 1: He & MacLeod (2001): found orientation-selective tilt after-effect\nfrom \"invisible\" gratings.\n- adapt to gratings so high in spatial frequency they are perceptually\nindistinguishable from a uniform field,\n- observers nonetheless experience tilt aftereffect from these gratings\nConclusion:\n\"Because these after-effects are due to changes in orientation-sensitive\nmechanisms in visual cortex, our observations imply that extremely fine\ndetails, even those too fine to be seen, can penetrate the visual system as far as\nthe cortex, where they are represented neurally without conscious awareness.\"\nExample 2: Vul & MacLeod (2006): found color-contingent aftereffects from\ngratings that were \"invisible\" because they were flickering so fast.\nSo: not just orientations, but combinations of color & orientation, can be\ncoded outside of awareness.\n\nUncoupling Perception and Awareness\nSo: we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus.\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)?\n- \"Activation strength hypothesis\"?\n- Kind of neural event? (Feedback to V1? Synchrony?)\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain...\n\nRepresentation without Awareness: Example 2\nMcGlinchey-Berroth et al (1993) semantic priming from unseen stimuli, neglect\n+\nWhich\nwas\nseen\nbefore?\nA. Neglect\nSs at chance\nat this task:\nPhoto of\napple\nAbstract\npainted\ndots\nPhoto of\napple\nPhoto of\nbed\nImages removed due to copyright restrictions.\n+\nWord\nor not?\nB. But show\nsemantic\npriming here:\nTREE\nPhoto of\napple\nAbstract\npainted\ndots\nImages removed due to copyright restrictions.\nSo even meanings can be represented outside of awareness.\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)?\n- \"Activation strength hypothesis\"?\n- Kind of neural event? (Feedback to V1? Synchrony?)\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain...\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\n- Special brain regions (e.g., cortex)?\nHe & MacLeod; Vul & MacLeod: info in V1 w/out awareness\nPasley et al: info in amygdala but not VVP w/out awareness\nWhat about the dorsal visual pathway?\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n\nTwo visual pathways\nThe two visual processing streams for different visual percepts:\n\"What\" (ventral/occipitotemporal stream)- object recognition\n\"Where\" (dorsal/occipitoparietal stream) - spatial perception\nBased on Mishkin & Ungerleider experiments, 1982\nCourtesy of Jody Culham. Used with permission.\nLateral view brain wi\nth\n\"\nwh\nat\n\"\nand\n\"w\nhere\" lo\ncation\ncartoo\nns.\nFigure by MIT OpenCourseWare.\n\n\"What\" vs. \"How\"\nGoodale and Milner, 1991\n- dichotomy should be \"what\" (ventral stream)\nvs. \"how\" (dorsal stream)\n- dorsal system has strong input to motor\nsystems and is essential for using visual\ninformation to guide actions\n- Information in dorsal system is not\nconsciously accessible (think of EMs)\n- Evidence for this view?\nCourtesy of Jody Culham. Used with permission.\n\nPatient DF: no visual form perception\nImages removed due to copyright restrictions.\nSee Figure 10.3 (p.320) in Goodale, M. A., and G. K. Humphrey, \"Separate Visual Systems for\nAction and Perception.\" Blackwell Handbook of Perception. Edited by E. Bruce Goldstein. New\nYork, NY: Wiley-Blackwell, 2001. [Preview this content in Google Books.]\nPatient DF has a \"ventral stream\" lesion\nObject agnosia\n- Cannot identify line drawings of common objects\n- Cannot copy line drawings\n- Can draw from memory as long as she doesn't lift hand from paper\nCourtesy of Jody Culham. Used with permission.\n\nPatient DF: acting without\nperceiving\nDF\nControl\nPosting task\nImages removed due to copyright restrictions.\nSee Figure 10.4 (p.321) in Goodale, M. A., and G. K. Humphrey, \"Separate Visual Systems for\nAction and Perception.\" Blackwell Handbook of Perception. Edited by E. Bruce Goldstein. New\nYork, NY: Wiley-Blackwell, 2001. [Preview this content in Google Books.]\nPerceptual matching task: performs very badly. But:\nPosting task: performs well, begins to rotate card in the\ncorrect direction when movement begins.\nSo dorsal pathway sufficient for action but not awareness?\nLet's look at more data....\nCourtesy of Jody Culham. Used with permission.\n\nFang & He (2005)\nQuestions:\nCan object-selective responses in the ventral pathway\nregister without awareness?\nCan object-selective responses in the dorsal pathway\nregister without awareness?\nLocalisers:\nIntact> scrambled objects\nDorsal\nVentral\nCourtesy of Sheng He. Used with permission.\nFang Fang & Sheng He. \"Cortical responses to invisible objects in the human dorsal and ventral pathways.\" Nature Neuroscience 8, 1380 - 1385 (2005).\n\nFang & He (2005) Experiment 1\nInvisible stimuli\nVisible stimuli\nSo: dorsal pathway \"sees\" the\ninvisible stimulus, ventral\ndoes not!\nHow much does dorsal\npathway \"know\" about the\nstimulus?\nCourtesy of Sheng He. Used with permission.\nFang Fang & Sheng He. \"Cortical responses to invisible objects in the human dorsal and ventral pathways.\" Nature Neuroscience 8, 1380 - 1385 (2005).\n\nFang & He (2005) Experiment 2\nDoes the dorsal pathway have info about specific unseen objects?\ne.g. tools versus faces\nYes!\nBut ventral pathway does not.\nMystery:\nSo why then have\nothers apparently showed\nselective activation of the\nFFA for unseen faces in\nCourtesy of Sheng He. Used with permission.\nneglect patients ?\nFang Fang & Sheng He. \"Cortical responses to invisible objects in the human dorsal and ventral pathways.\" Nature Neuroscience 8, 1380 - 1385 (2005).\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)?\nHe & MacLeod; Vul & MacLeod: info in V1 w/out awareness\nSo: no evidence\nfor brain regions\nPasley et al: info in amygdala but not VVP w/out awareness\nwhere specific\nFang & He: dorsal pathway registers w/out awareness\nactivation is\nbut ventral pathway does not\nsufficient for\nBut see Vuilleumier et al (2001)on unconscious FFA activation\nawareness.\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)? No evidence yet....\n- \"Activation strength hypothesis\"?\n- Kind of neural event? (Feedback to V1? Synchrony?)\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain...\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)? maybe\n- \"Activation strength hypothesis\"?\n- Kind of neural event? (Feedback to V1? Synchrony?)\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain...\n\nMarois et al (2004) Attentional Blink\nDemo:\nA very rapid sequence of digits will flash on. Two\nletters will be included in the sequence. Your task is to\nreport the two letters.....\n\nQuickTimeTM and a\ndecompressor\nare needed to see this picture.\n\nMarois et al (2004) Attentional Blink\nThat was X then H\nReady for the next one?\n\nQuickTimeTM and a\ndecompressor\nare needed to see this picture.\n\nMarois et al (2004) Attentional Blink\n1. That was X then H\nlong lag, 4 intervening letters\n2. That was A then P\nshort lag, one intervening letter\nPeople detect the second target more often at long than short\nlags: the \"attentional blink\"\nCritical question: when you fail to see the second letter, what\nhappens to it?\n\n\"\nMarois et al (2004) Attentional Blink\nTask:\ndetect face (T1), then\nscene (T2)\nMarois, R., et al. \"The Neural Fate of Consciously\nPerceived and Missed Events in the Attentional\nBlink. Neuron 41, no. 3 (2004): 465-472.\nCourtesy Elsevier, Inc.,\nhttp://www.sciencedirect.com.\nUsed with permission.\n\n\"\nMarois et al (2004) Results\nBehavioral Data:\nfMRI Data - Response of PPA:\nRepresentation without awareness\nMarois, R., et al. \"The Neural Fate of Consciously\nPerceived and Missed Events in the Attentional\nBlink. Neuron 41, no. 3 (2004): 465-472.\nCourtesy Elsevier, Inc.,\nhttp://www.sciencedirect.com.\nUsed with permission.\nActivation Strength hypothesis\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)? maybe\n- \"Activation strength hypothesis\"?\nmaybe\n- Kind of neural event? (Feedback to V1? Synchrony?)\nmaybe\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain...\n\nDehaene et al\n(2001)\nWider activation\nthroughout the brain\nfor unmasked words\n(perceived\nconsciously) than\nmasked words (not\nperceived\nconsciously).\nAre you convinced that\nwidespread access to the\ninformation is the crux of\nawareness?\nUnmasked Words\nMasked Words\nImage removed due to copyright restrictions.\nFigure 2 in Dehaene, S., et al. \"Cerebral mechanisms of word\nmasking and unconscious repetition priming.\" Nature\nNeuroscience 4 (2001): 752-758. doi:10.1038/89551 .\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)? maybe\n- \"Activation strength hypothesis\"?\nmaybe\n- Kind of neural event? (Feedback to V1? Synchrony?) maybe\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain... maybe\n\nUncoupling Perception and Awareness\nOther questions:\n1. Is awareness/access all or none? (Dehaene PNAS)\n2. Is awareness necessarily tied to space & time?\n3. Is awareness composed of discrete time points (Koch/Crick)?\n4. Is there (or what is the nature of the) limit on the capacity/bandwidth of\nawareness?\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n- Special brain regions (e.g., cortex)? maybe\n- \"Activation strength hypothesis\"?\nmaybe\n- Kind of neural event? (Feedback to V1? Synchrony?) maybe\n- Information access hypothesis: awareness of a particular element of\nstimulus information entails not just a strong neural representation, but also\naccess to that information by most of the rest of the mind/brain... maybe\n\nCoda: Mark Williams' work on\nreadout of distributed cortical\npatterns.\n\nWhich Spatial Patterns of fMRI Response are accessible\nto awareness?\nHaxby argues that the whole spatial profile of response across centimeters\nof ventral visual pathway constitutes the representation of an object.\nAnd indeed there is some object info spread across here.\nBut:\nJust because some information is present in the SPfR\nin a given ROI does not mean that that SPfR is\naccessible to awareness and hence used in task\nperformance. Line drawing of human brain, viewed from underside.\nWhat we need to know:\nWhich pattern information is accessible to awareness?\nHow could we tell?\nFirst, a reminder about pattern analysis.... Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nCorrelation-based Classification Analysis (Haxby et al., 2001)\n1. Scan each subject while they view multiple stimulus categories.\n2. Split the data in 1/2; generate activation maps for each category.\n3. Compute correlation across activation maps.\nWithin\ncategory\nbetween\ncategories\nIf r(Within) > r(Between)\nthe region contains category information\n\n-\nWilliams et al (2007)\nOverall Logic\nThe Question:\nWhich pattern information is accessible to awareness?\nDesign: Brief masked stimuli, shape categorization task\nsome errors, but above chance\nbin fMRI data by behavioral performance\nKey prediction:\nAny pattern information that is accessible to awareness should\nbe stronger on correct than incorrect trials, i.e.:\n[r(w/in) - r(betwn)] on correct trials > [r(w/in) - r(betwn)] on incorrect trials\nWilliams, M. A, N. Kanwisher et al. \"Feedback of visual object information to foveal retinotopic cortex.\" Nature Neuroscience 11 (2008): 1439 1445.\n\n-\nLateral Occipital Complex (LOC)\nWilliams et al, Results\nRetinotopic Cortex\nWithin-category\nBetween-category\nInformation is present in\nretinotopic cortex, but not\naccessible to awareness!\nInformation is present in\nLOC on correct trials, not\non incorrect trials>>>>\nThis information is\naccessible to awareness.\nOther areas?\nWilliams, M. A, S. Dang, and N. Kanwisher. \"Only some spatial\npatterns of fMRI response are read out in task performance.\"\nNature Neuroscience 10 (2007): 685 686\n\nRetinotopic Cortex\nWilliams et al, Results\nLateral Occipital Complex (LOC)\nPosterior Fusiform\nFFA\nNo info re nonface objects!\n\nCould weaker category information in incorrect trials be due to the\nsmaller number of trials?\nNo\nEqualised trial numbers\nLateral Occipital\n\nIs the greater category information in LO than FFA due to\na larger number of voxels?No\nEqualised voxel numbers.\nFFA\nLateral Occipital\n\nWhich Spatial Patterns of fMRI Response are accessible\nto awareness?\nHaxby argues that the whole spatial profile of response across centimeters\nof ventral visual pathway constitutes the representation of an object.\nAnd indeed there is some object info spread across here.\nBut:\nJust because some information is present in the SPfR\nin a given ROI does not mean that that SPfR is\naccessible to awareness and hence used in task\nperformance.\nWhat we need to know:\nWhich pattern information is accessible to awareness?\nPattern information in LOC is accessible to awareness, but information in\nretinotopic cortex is not! Line drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nExtra slides\n\nA Sampler of Historical Perspectives on Consciousness\nLeibniz, 1704:\n\"there are hundreds of indications leading us to conclude that at every moment there\nis in us an infinity of perceptions, unaccompanied by awareness or reflection; that\nis, of alterations in the soul itself, of which we are unaware because the impressions\nare either too minute or too numerous, or else too unvarying, so that they are not\nsufficiently distinctive on their own. \"\nHelmholtz, 1866:\n\"These inductive conclusions leading to the formation of our sense-perceptions\ncertainly do lack the purifying and scrutinizing work of conscious thinking.\nnevertheless, in my opinion, by their peculiar nature they may be classified as\nconclusions, inductive conclusions unconsciously formed.\"\nNietzsche, about 1882\n\"The absurd overvaluation of consciousness ...consciousness only touches the\nsurface...The basic activity is unconscious...The real continuous process takes place\nbelow our consciousness; the series and sequence of feelings, thoughts, and so on,\nare symptoms of this underlying process.\"\n\nA Sampler of Historical Perspectives on Consciousness (continued)\nJames, 1890\n\"however numerous the things [we may attend to], they can only be known in a single pulse\nof consciousness for which they form one complex 'object', so that properly speaking there\nis before the mind at no time a plurality of ideas, properly so called.\"\nSidis, 1898:\nConcluded that his experiments demonstrated \"the presence within us of a secondary\nsubwaking self that perceives things which the primary waking self is unable to get at.\"\nWatson, 1930\n\"Behaviorism claims that consciousness is neither a definite nor a usable concept. The\nBehaviorist, who has been trained always as an experimentalist, holds further that belief in\nthe existence of consciousness goes back to the ancient days of superstition and magic.\"\nCrick & Koch, 1992\n\"the time is now ripe for an attack on the neural basis of consciousness.\"\nJulesz, 1994\n\"Psychology without consciousness is like math without infinity\" - possible but not very\ninteresting.\n\nA Cool Result Reported in Science this week\nOwen et al (2006), Science, 313, p. 1402.\nTennis > rest\nAnd\nNavigation > rest\nImage removed due to copyright restrictions.\nfMRI images of supplementary motor area in\ntwo imagery scenarios: playing tennis and\nwalking around the house.\nAre these patterns of\nSee Figure 1 in Owen, A. M., et al.\n\"Detecting awareness in the vegetative\nactivation different\nstate.\" Science 313 (2006): 1402.\nfrom each other?\nThese statistics don't\ntell us!\n(What would we have\nto do?)\n\nSome of the ways that people have thought about\nconsciousness throughout history:\nas a fact that poses fundamental questions about the nature of\nreality,\nas the natural focus for scientific psychology,\nas a topic psychology must avoid at any cost,\nas a nonexistent or \"epiphenomenal\" by-prouct of brain\nfunctioning, and\nas an important unsolved problem for psychology and\nneuroscience.\n(from \"A Cognitive Theory of Consciousness\", by Bernie Baars)\n\nA Cool Result Reported in Science this week\nOwen et al (2006), Science, 313, p. 1402.\n23-year old woman\nTraffic accident >\nVegetative state.\nPreserved sleep-wake\nImage removed due to copyright restrictions.\nfMRI images of supplementary motor area in\ntwo imagery scenarios: playing tennis and\nwalking around the house.\nunresponsive.\ncycles, but\nSee Figure 1 in Owen, A. M., et al.\n\"Detecting awareness in the vegetative\nstate.\" Science 313 (2006): 1402.\nMight she be \"in there\"\ncognitively despite her\ninability to respond?\nBut.......\n\nConsider these Events\n- 1957 New jersey company inserted subliminal messages (\"drink\ncoke\"/\"eat popcorn\") into movies and claimed to increase sales\n- self-help audio tapes claim to raise self-esteem or improve\nmemory by presentation of subliminal messages\n- 2 teenagers commit suicide, and their families try to sue a rock\nband for having placed subliminal messages in their music.\nAre these things possible?\n- Swets & Bjork, 1990 showed that students don't learn a thing when\nthey are asleep\n- A careful study of self-help audio tapes done in 1991 showed no\nimprovement after months of use.\n- The judge in the suicide case ruled that there isn't \"credible\nscientific evidence\" that a subliminal message can influence\nbehavior.\nWhat can science tell us?\n\nUncoupling Mental/Neural Representation and Awareness\n1. Same stimulus > different awareness\n- Identical stimuli that are either perceived or not (on diff trials)\n- Also: attention (same stim, diff experience)\n- Rivalry\n>>> Can find \"NCC\", or NCA, unconfounded from the stimulus\nLots of these how now been reported.\nThey arent all in the same single \"awareness area\" in the brain, but rather it\nseems like:\nThe neural correlates of awareness of a given stimulus attribute are found in\nthe neural structure that analyzes that stimulus attribute.\nWhat are we to do with these NCAs now that we have them?\nReally we want to understand not just correlation, but causal connection\n\nri ht\nleft\nCos\nCos\nFus\nFus\nAre Other Regions Correlated with Face Identification? Object Identification?\nflowers\nbirds\nfaces\ng right\nleft\nhouses\nguitars\ncars\nCos\nFus\nIdentification > detection\nFace selective regions\nP<10-3\nhit\nhit\nfrom localizer scan\nP<10-2\n- Face Identification Primarily Involves the FFA\n- Identification of other Categories primarily Involves Other Regions\n- What about experts?\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\n% signal\nFFA Response in 5 Car Experts\n(c) FFA response for cars as a\nfunction of expertise\ntime[s]\ntime[s]\n0.2\n(a) Faces\n(b) Cars\n0.1\n0.0\n-0.1\n-0.2\n-0.3\nsubject\nsubject\nidentification\ndetection\ndetection\nhit\nhit\nmiss\nR2 = 0.0526\nd'\nNo evidence that FFA is involved in car identification in \"car experts\".\nGrill-Spector, Knouf, & Kanwisher (2004), Nature Neuroscience.\n\nConsiderable hoopla (from Crick, Koch,\nothers) about the \"neural correlates of\nconsciousness\" (NCC).\nWe have just described several NCCs,\nunconfounded from the stimulus.\nIn fact, they are a dime a dozen.\nWhat are we supposed to do with\nthem, now that we have them?\nThey arent all in the same single \"awareness area\" in the brain, instead:\nThe neural correlates of awareness of a given stimulus attribute are apparently\nfound in the neural structure that analyzes that stimulus attribute.\nReally we want to understand not just correlation, but causal connection\nCourtesy of Christopher Koch. Used with permission.\n\nUncoupling Representation and Awareness\nRepresentation without awareness\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\nOne maybe-example seen before:\n- Janzen et al (2004):\nPPA: memory w/out awareness\n- More.....\nImages removed due to copyright restrictions.\nToy at decision point and non-decision point.\n\nSee Fig. 1 in: Janzen, G., and M. van Turennout.\n\"Selective Neural Representation of Objects Relevant\nfor Navigation.\" Nature Neuroscience 7 (2004): 673-677.\n\nHow can this be?\nStandard Ventral object recognition pathway\nInferior Temporal (IT)\nCortex\nLGN V1\nV2\nRetina\nAmygdala\nRetina\nSuperior Colliculus (SC) Pulvinar (Thal)\nAlternate Subcortical pathway\n\nUncoupling Perception and Awareness\n1. If we can have Representation without Awareness:\nMental/neural\nrepresentations\nextracted from a\nstimulus (whether\nreportable or not).\nAwareness:\nThe ability to explicitly\nreport about the presence or\nproperties of the stimulus.\n- Special brain regions (e.g., cortex)?\nHe & MacLeod; Vul & MacLeod: info in V1 w/out awareness\nSo V1 representation is apparently not sufficient\nWhat about other areas? (Nune's report on Pasley - amygdala)\nWhat determines which perceptual information reaches awareness??\n- Kind of information (only low-level stuff? Or semantic/Motor?).\n\nPasley et al(2004)\nBinocular Rivalry\nQuestion: do invisible stimuli get represented in higher-level cortex? Amygdala?\nPasley, B. N., L. C. Mayes, and R. T. Schultz. \"Subcortical\nDiscrimination of Unperceived Objects during Binocular Rivalry.\"\nNeuron 42, no. 1 (2004): 163-172.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com.\nUsed with permission.\nRight (suppressed) eye sees: moving house, fearful face, or chair\n\nIT cortex shows\nno specificity for\nsuppressed\nimage type\nPasley, B. N., L. C. Mayes, and R. T. Schultz. \"Subcortical Discrimination of\nUnperceived Objects during Binocular Rivalry.\" Neuron 42, no. 1 (2004): 163-172.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nBut Amygdala does!\nPasley, B. N., L. C. Mayes, and R. T. Schultz. \"Subcortical Discrimination of\nUnperceived Objects during Binocular Rivalry.\" Neuron 42, no. 1 (2004): 163-172.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission."
    },
    {
      "category": "Resource",
      "title": "The Problem of Object Recognition and The Lateral Occipital Complex (LOC)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/7b4f608c82a5bef9b02a77ae69f7a8d6_lec3_recogn_ip.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n9.71 Lecture 3 Sept 20\nThe Problem of Object Recognition\nand The Lateral Occipital Complex (LOC)\nOutline for Today\nI. Demo of an fMRI scan\nII. Lecture: The Problem of Object Recognition:\n1. Why study it, what is entailed computationally\n2. The Lateral Occipital Complex (LOC)\nWhat does LOC do?\nWhat does LOC representation?\nIII. A few tips on doing presentations.\nIV. If time: Discussion of B&Z]\n\nWhy Study Object Recognition?\nOR is an important problem:\nCritical for survival\nWe are very good at it\nA distinct domain of cognition\n\nObject Recognition: A Distinct Domain of Cognition\nVisual Agnosia: specific deficit in visual object recognition\nwithout impaired visual acuity\nwithout impaired object recognition by touch, sound, smell\nThe fact that visual OR can be selectively lost in brain damage implies\nthat it is a distinct domain of cognition,\nwith its own special neural hardware,\ndistinct from low-level visual processing,\nand from knowledge of the meanings and names of objects.\n(different kinds of agnosias can give us further dissociations...)\nAn example....\n\nWhat does object recognition entail exactly, and what is to be\nexplained?\n\nObject Recognition as Matching to Memory\nVisual LTM:\nWorld/\nEye/\nthousands of\nVisual field\nRetinal image\nObject Recognition.\nStored shapes\nPhoto courtesy\nof Nick Devenish.\nMatching\n-\n- -\n\nA Theory of Object Recognition\nWould have to Specify:\nWorld/\nEye/\nVisual field\nRetinal image\nVisual Recognition\nPhoto courtesy\nof Nick Devenish.\na. the nature of the stored visual representations in LTM\nb. the nature of the intermediate representations\nc. a computational account of how each intermediate\nrepresentation can be derived from the previous one\nd. a determination of whether the answers to a-d are different\nfor different kinds of objects\nVisual LTM:\nthousands of\nStored shapes\n-\n- -\n\nKinds of Cues Available for Visual Object Recognition\na. Characteristic motion (e.g. a fly).\nb. Color/texture (e.g., lawn, ocean, beach)\nc. Stored knowledge plus minimal cues (e.g. I left newspaper on\ndining table, that's what that blob must be).\nd. The most important cue: SHAPE!\n(which is the primary focus of most theories of object recognition)\n\nWhat Makes Object Recognition (by Shape) Hard?\nWorld/\nEye/\nVisual field\nRetinal image\nPhoto courtesy\nInverse optics problem\nof Nick Devenish.\n1. A single\nimage can\nbe cast by\nmany\ndifferent\n3D\nobjects\nInverse\noptics problem cau\nsed by retinal image projection.\nFigure by MIT OpenCourseWare.\n\nWhat Makes Object Recognition Hard?\nWorld/\nVisual field\nPhoto courtesy\nof Nick Devenish.\n2. A single\nObject can\ncast many\nDifferent\nretinal\nimages that\ndiffer in....\nEye/\nRetinal image\nViewpoint\n1. A single\nimage can\nbe cast by\nDistance/size\nmany\ndifferent\nOcclusion\n3D\nobjects ....\nConfiguration\nLighting, etc....\n\nThe Problem of Object Recognition\nGiven a\nRetinal\nWhat is it?\nImage such\nAs this:\n\nTwo main Challenges:\nSpecificity:\nAppreciating the distinction\nInvariance/Tolerance:\nbetween different categories.\nGeneralizing across changes\nin size, orientation, lighting,\netc. to realize these images\nare all of the same thing:\nPhoto courtesy\nof Nick Devenish.\n=\n\nHow do we solve this problem?\nOptions:\n1. Inverse Optics:\nExtract an abstract\nrepresentation of 3D\nshape \"invariant\" to\nthese image changes.\n2. Association:\nStore each possible\nVersion of an\nobject. Brute force.\nRabbit\n- an \"ill-posed\" problem.\n- That's a lot to store!\n- What about novel\nviews?\n- Alignment\n3. Other intermediate descriptors\ne.g. image fragments, parts....\nPhoto courtesy\nof Nick Devenish.\n\nA Theory of Object Recognition\nWould have to Specify:\nWorld/\nEye/\nVisual field\nRetinal image\nVisual Recognition\nPhoto courtesy\nof Nick Devenish.\nHow can brain imaging help?\na. the nature of the stored visual representations in LTM\nb. the nature of the intermediate representations\nc. a computational account of how each intermediate\nrepresentation can be derived from the previous one\nd. a determination of whether the answers to a-d are different\nfor different kinds of objects\nVisual LTM:\nthousands of\nStored shapes\n-\n- -\n\nA Theory of Object Recognition\nWould have to Specify:\nWorld/\nEye/\nVisual field\nRetinal image\nVisual Recognition\nPhoto courtesy\nof Nick Devenish.\nHow can brain imaging help?\na. the nature of the stored visual representations in LTM\nb. the nature of the intermediate representations\nc. a computational account of how each intermediate\nrepresentation can be derived from the previous one\nd. a determination of whether the answers to a-d are different\nfor different kinds of objects\nVisual LTM:\nthousands of\nStored shapes\n-\n- -\n\nBrain Regions Involved in Visual Cognition\nLine drawing of human brain, viewed from underside.\nFigure by MIT OpenCourseWare. After Allison, 1994.\n\nThe Lateral Occipital Complex (LOC):\nCortical Regions Involved in Processing Object Shape\nI Malach et al (1995), \"LO\" Brain regions processing object shapes.\nand\n:\n>\nCourtesy of National Academy of Sciences, U. S. A. Used with permission.\nSource: Malach, R. et. al. \"Object-related activity revealed by functional\nmagnetic resonance imaging in human occipital cortex.\" Proc. Natl. Acad.\nSci. 92 (1995): 8135-8139. Copyright (c) 1995, National Academy of Sciences, U.S.A.\nII Kanwisher et al (1996) - a similar region\nand\n>\n: Brain regions processing object shapes.Brain regions processing object shapes.\nFigure by MIT OpenCourseWare.\n\n10-4\n10-4\n10-10\nisphere\niew\nleft hemisphere\nObject-Selective Regions in the Human Brain:\nLOC in one Subject\nsubject: NT\n10-4\n10-10\nright hem\nlateral v\nventral view\nKalanit Grill-Spector\nWhat does this region do?\n>\nPhoto courtesy\nof Enid Yu.\nPhoto courtesy\nof caspermoller.\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nComponents of Object Recognition\nfrom Common Sence (plus a speck of data)\nmeaning\naction\nEye/\nRetinal image\nLOC?\nEarly\nsensory\nprocessing\n~shape\ncoding\n\"recognition\"/\nmatching to\nstored reps.\nword/\nname\nLOC: familiarity/meaning/name apparently not important\nnot processing very low-level information\n(Is this what is messed up in the \"lock guy\"?)\nIs this region necessary for perceiving shape?\nfMRI can't tell you this, but......\n\nPatient DF: no form visual perception\nPatient DF has a \"ventral stream\" lesion\nObject agnosia (a diff. Kind from the \"lock guy\")\n- Cannot identify line drawings of common objects\n- Cannot copy line drawings\n- Can draw from memory as long as she doesn't lift hand from paper\nImage removed due to copyright restrictions.\nSee Figure 10.3 (p.320) in Goodale, M. A., and G. K. Humphrey, \"Separate Visual Systems for\nAction and Perception.\" Blackwell Handbook of Perception. Edited by E. Bruce Goldstein. New\nYork, NY: Wiley-Blackwell, 2001. [Preview this content in Google Books.]\nSlide adapted from Jody Culham\n\nLOC in Normals and Lesion site in DF\nImage removed due to copyright restrictions.\nFig 4b in James et al. Brain 126, no. 11 (2003): 2463-2475.\nView this figure at http://brain.oxfordjournals.org/cgi/content/full/126/11/2463.\nApparently, LOC is necessary\nfor object recognition.\nJames, Culham, Humphrey, Milner, & Goodale (2003)\n\nCharacterizing Representations\nand Processes in the LOC\nAre shape representations in LOC\nindependent of how shape is represented, i.e. independent of\nform-cues (motion, luminance, texture)?\ncontours?\nIndependent of changes in the size, position, viewpoint, etc?\nCool method: fMRI adaptation\n\nAre common regions involved in processing object structure\nindependent of the cues defining the object's shape (e.g. line\ncontours, surface shading)?\nGrayscale\nGrayscale\nLine Drawings\nLine Drawings\nIntact\nScrambled\nIntact\nScrambled\nKourtzi & Kanwisher, 2000\nCourtesy of Society for Neuroscience. Used with permission.\n\nProcedure\nEach Scan:\n5:36\n:16s :32s :48s\nLine Scrambled\nLine Intact\nGray Scrambled\nGray Intact\nGray Intact\nGray Intact\nGray Scrambled\nGray Scrambled\nGray Scrambled\nLine Intact\nLine Intact\nLine Scrambled\nLine Scrambled\nLine Scrambled\nLine Intact\nGray Intact\nEach Epoch:\n(20 pictures per epoch)\n250 msec\nTasks: Passive Viewin\nKourtzi & Kanwisher, 2000\n550 msec\n250 msec\nCourtesy of Society for Neuroscience. Used with permission.\ng\n\nActivations in one subject for:\na. Intact versus Scrambled Grayscale images\nb. Intact versus Scrambled Line Drawings\nKourtzi & Kanwisher, 2000\nCourtesy of Society for Neuroscience. Used with permission.\n\nExperiment 1: Results\nActivation Map for Intact-Scrambled Images averaged across subjects\nA big chunk of cortex is more active for intact than scrambled shapes.\n% Signal Change in LO\n0.4\n0.8\nGrayscale\nIntact\nLine Drawings\nIntact\nCourtesy of Society for Neuroscience. Used with permission.\n\nAre common regions involved in processing object structure\nindependent of the cues defining the object's shape (e.g. line\ncontours, surface shading)?\nYES!\nAre the same neurons responsive to photos and drawings?\nDoes it respond to shapes defined in other ways?\nGrayscale\nIntact\nGrayscale\nScrambled\nLine Drawings\nIntact\nLine Drawings\nScrambled\nKourtzi & Kanwisher, 2000\nCourtesy of Society for Neuroscience. Used with permission.\n\nObjects from Motion Experiment\n15 different images per block\npresented rate: 0.5Hz\nGrill-Spector et al. , Neuron 1998\nAre object-selective regions preferentially activated by objects from\nLuminance? Motion? Texture?\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nObjects from Motion Experiment\nDefine object selective regions: OFL > GFL\nOFM\nG-MTN\nOFM\n% signal\ntime (s)\nn=9\nGFL\nOFL\nleft hemi\nlateral\nventral\nOFL > GFL\nGrill-Spector et al. , Neuron 1998\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nObjects from Motion Experiment\nDefine object selective regions: OFL > GFL\nOFM\nGFL\nOFL\nMTN\nOFM\n% signal\ntime (s)\nn=9\nleft hemi\nlateral\nventral\nOFL > GFL\nTime course from object-selective regions: LOC\nGrill-Spector et al. , Neuron 1998\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nObjects from Motion Experiment\nDefine object selective regions: OFL > GFL\nOFM\nG-MTN\nOFM\n% signal\ntime (s)\nn=9\nGFL\nOFL\nleft hemi\nlateral\nventral\nOFL > GFL\nGrill-Spector et al. , Neuron 1998\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nConclusion: Cue-independent\nRepresentations of Object Shape\nObjects from motion\nLeft hemisphere\neral\nObjects from greyscale\nphotos\nObjects from line drawings\nGrill-Spector et al. , Neuron 1998\nCourtesy of National Academy of Sciences,\nU. S. A. Used with permission. Source:\nMalach, R. et. al. \"Object-related activity\nrevealed by functional magnetic resonance\nimaging in human occipital cortex.\" Proc.\nNatl. Acad. Sci. 92 (1995): 8135-8139.\nCopyright (c) 1995, National Academy of\nSciences, U.S.A.\nlat\nCourtesy Elsevier, Inc., http://www.sciencedirect.com.\nUsed with permission.\nObjects from texture\nCourtesy Elsevier, Inc., http://www.sciencedirect.com.\nUsed with permission.\nObjects from luminance\n\nCharacterizing Representations\nand Processes in the LOC\nAre shape representations in LOC\nindependent of how shape is represented, i.e. independent of\nform-cues (motion, luminance, texture)? probably\ncontours?\nIndependent of changes in the size, position, viewpoint, etc?\nCool method: fMRI adaptation\nBUT: Have I shown you evidence that the very same neurons\nrespond to form independent of how that form is defined?\n\nEvent-Related fMRI Adaptation\nBasic idea: Any measure that is sensitive to the sameness vs. difference\nbetween 2 stimuli can reveal what the system takes to be the same and diff.\nExample: If brain region X discriminate between two similar stimuli, say....\nThen if we measure fMRI response in that region to same vs. different trials:\nDIFFERENT\nWe see this:\nThen region X\ncan discriminate\nthese 2 stimuli.\nPhoto courtesy\nof Trpster.\nPhoto courtesy\nof floridapfe.\n250ms\n500ms 250ms\nPhoto courtesy\nPhoto courtesy\nof floridapfe.\nof floridapfe.\n\nSAME\nDIFFERENT\nTEST\nEvent-Related fMRI Adaptation\nBasic idea: Any measure that is sensitive to the sameness vs. difference\nbetween 2 stimuli can reveal what the system takes to be the same.\nDoes region X\n\"think\" these\nimages are the\nWhat is the\nsame?\nPhoto courtesy\nanswer if we\nThen region X\nof floridapfe.\nsee this:\ncan discriminate\nPhoto courtesy\nof floridapfe.\nthese 2 stimuli.\nPhoto courtesy\nNow we can\nof Trpster.\n250ms\n500ms 250ms\nalso ask what\nimages region\nX \"thinks\" are\nthe same,\ne.g....\nPhoto courtesy\nPhoto courtesy\nof floridapfe\nof floridapfe\nPhoto courtesy\nof floridapfe.\n\nCharacterizing Representations\nand Processes in the LOC\nAre shape representations in LOC\nindependent of how shape is represented, i.e. independent of\nform-cues (motion, luminance, texture)? probably\ncontours?\nIndependent of changes in the size, position, viewpoint, etc?\nCool method: fMRI adaptation\n\nIs LOC \"Contour Invariant\"?\nIf the LOC represents object shape, independent\nof the contours defining that shape, then\nif the two stimuli have........\n1. Diff. Contours\nAdaptation\nBut Same Shape\n2. Same Contours\nNo Adaptation\nBut Different Shape\nKourtzi & Kanwisher (2001)\n\nF\nImage removed due to copyright restrictions.\nFig. 2 in \"Representation of Perceived Object Shape by the Human\nLateral Occipital Complex.\" Science, 24 AUGUST 2001 VOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n1. Diff. Contours But Same Shape\nIs there neural adaptation in the LOC for objects\nthat have different contours but the same\nperceived shape?\nImage removed due to copyright restrictions.\nFig. 2 in Kourtzi, Zoe, and Nancy Kanwisher. \"Representation of Perceived Object Shape by the Human\nLateral Occipital Complex.\" Science, 24 AUGUST 2001 VOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n300 ms\n300 ms\n400 ms\n2000 ms\n\nExperiment 1: Results\n- Define the LOC for intact versus scrambled images in each subject (n=10).\n- Average time course of activation in the LOC.\nImage removed due to copyright restrictions.\nFig. 3 in Kourtzi, Zoe, and Nancy Kanwisher. \"Representation of Perceived Object Shape by the\nHuman Lateral Occipital Complex.\" Science, 24 AUGUST 2001\nVOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n- Significant adaptation for identical shapes (p<0.05).\nKourtzi & Kanwisher\n\nExperiment 1: Results\n- Define the LOC for intact versus scrambled images in each subject (n=10).\n- Average time course of activation in the LOC.\nImage removed due to copyright restrictions.\nFig. 3 in Kourtzi, Zoe, and Nancy Kanwisher. \"Representation of Perceived Object Shape by the\nHuman Lateral Occipital Complex.\" Science, 24 AUGUST 2001\nVOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\nKourtzi & Kanwisher\n- Significant adaptation for displays with the same shape but different contours (p<0.05).\n\nIf the LOC represents object shape, independent\nof the contours defining that shape, then\nif the two stimuli have........\n1. Diff. Contours\nAdaptation √\nImage removed due to copyright restrictions.\nFig. 2 in\n\"Representation of Perceived Object Shape by the\n\nSame Shape\nHuman Lateral Occipital Complex.\" Science, 24 AUGUST 2001\nVOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n2. Same Contours\nNo Adaptation ?\nDifferent Shape\nKourtzi & Kanwisher\nKourtzi, Zoe, and Nancy Kanwisher.\n\n2. Same Contours But Different Shape\nIs there neural adaptation for stereoscopically\ndefined shapes that share the same contours\nbut have different shape?\nF\nImage removed due to copyright restrictions.\nFig. 2B in Kourtzi, Zoe and Kanwisher, Nancy. \"Representation of Perceived\nObject Shape by the Human Lateral Occipital Complex.\" Science, 24\nAUGUST 2001 VOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\nKourtzi & Kanwisher\n\nExperiment 2: Results\n- Define the LOC for intact versus scrambled images in each subject (n=10).\n- Average time course of activation in the LOC.\nImage removed due to copyright restrictions.\nFig. 4 in Kourtzi, Zoe and Kanwisher, Nancy. \"Representation of Perceived Object Shape by the\nHuman Lateral Occipital Complex.\" Science, 24 AUGUST 2001\nVOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n- Significant adaptation for identical shapes (p<0.01).\n\nExperiment 2: Results\n- Define the LOC for intact versus scrambled images in each subject (n=10).\n- Average time course of activation in the LOC.\nImage removed due to copyright restrictions.\nFig. 4 in Kourtzi, Zoe and Kanwisher, Nancy. \"Representation of Perceived Object Shape by the\nHuman Lateral Occipital Complex.\" Science, 24 AUGUST 2001\nVOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\n- No significant adaptation for displays with the same contours but different shape.\n\nConclusions\n1. Diff. Contours\nSame Shape\n2. Same Contours\nDifferent Shape\nAdaptation\nImage removed due to copyright restrictions.\nFig. 2 in Kourtzi, Zoe, and Nancy Kanwisher. \"Representation\nof Perceived Object Shape by the Human Lateral Occipital\nComplex.\" Science, 24 AUGUST 2001 VOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\nImage removed due to copyright restrictions. Fig. 2B in\nKourtzi, Zoe, and Nancy Kanwisher. \"Representation of\nPerceived Object Shape by the Human Lateral Occipital No Adaptation\nComplex.\" Science, 24 AUGUST 2001 VOL 293.\n(http://web.mit.edu/bcs/nklab/media/pdfs/KourtziKanwisherScience01.pdf)\nThe adaptation effects in the LOC suggest\nthat these neural populations represent object shape\nindependent of the contours defining the shape.\nKourtzi & Kanwisher\n\nCharacterizing Representations\nand Processes in the LOC\nAre shape representations in LOC\nindependent of how shape is represented, i.e. independent of\nform-cues (motion, luminance, texture)? probably\ncontours? Yes!\nIndependent of changes in the size, position, viewpoint, etc?\nUh, why does this matter again?\nCool method: fMRI adaptation\n\nHow do we Recognize Objects\ndespite Variations in the Image of Each Object?\nExtract one common\nPhoto courtesy\nof Nick Devenish.\nrepresentation from\neach of these that is\n\"invariant\" to changes\nin size, position,\nviewpoint, etc.\nExtract a different\nrepresentation for\neach, then map all\nof these to \"rabbit\".\nRabbit\n\nChanges in Viewpoint\nFront\nProfile\nCheek\nBack\nFace photos modified by OCW for privacy considerations.\n- Are responses to faces tuned to specific\nviewpoints of faces?\nTong, Kanwisher, & Nakayama, 2000\n\n1.8\n1.8\n1.3\nDoes this\nFront\nProfile\nCheek\nBack\nmean that\nFace photos modified by OCW\nfor privacy considerations.\nPSC in\n0.9\nFFA\n(n=5)\nthe same\nneurons\nresponse\nto front &\nprofile\nviews of\nfaces?\n% MR\nSignal\nChange\nIn FFA\n-1\n+\nB\nC\nP\nF\n+\nP\nB\nF\nC\n+\nC\nF\nB\nP\n\nF\nP\nC\nB\n\nTime\nTong, Kanwisher, & Nakayama, 2000\n\nUsing Adaptation to Test for Invariances\nExpect lower\nresponses for blocks\n+\n+\n+\n+\n+\nof identical images\nthan blocks of\ndifferent faces/cars.\nThen use that effect to\ntest for invariances\nacross changes in\nposition, etc....\n(Grill-Spector et al. 1999)\ntexture\n+\n+\nidentical\n+\n+\n+\n+\ndifferent\n+\n+\n+\n+\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n\nUsing Adaptation to Test for Invariances\nposition\nviewpoint\ndifferent\nidentical\ntexture\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nDo images that vary only in position or viewpoint count as the \"same\" and\nhence get adapted, or do they count as \"different\" and not get adapted?\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n(Grill-Spector et al. 1999)\n\nDifferential Invariance in Anterior-Ventral\nObject-Selective Areas: LOa /pFs\nventral\n0.40\n0.60\n0.80\n1.00\nadaptation ratio\n*\nIdentical\nPosition\nSize\nIllumination Viewpoint\nLOa/pFs\n0.48\n0.69\n0.68\n0.87\n0.88\nratio =\n% signal condition\nratio = 1.0\nthere is no adaptation\n% signal different\nratio < 0.7 * significant adaptation (p<0.01)\n(Grill-Spector et al. 1999)\n\nDifferential Invariance in Anterior-Ventral\nObject-Selective Areas: LOa /pFs\n0.40\n0.60\n0.80\n1.00\nadaptation ratio\nLOa/pFs\n0.48\n0.69\n0.68\n0.87\n0.88\nIdentical\nPosition\nSize\nIllumination Viewpoint\n*\n*\n*\nventral\nratio =\n% signal condition\nratio = 1.0\nthere is no adaptation\n% signal different\nratio < 0.7 * significant adaptation (p<0.01)\n(Grill-Spector et al. 1999)\n\nadaptation ratio\nventral\nlateral\nDifferential Invariance in\nSubdivisions of LOC\n0.40\n0.60\n0.80\n1.00\nLO\n0.64\n0.92\n0.88\n0.85\n1.01\nLOa/pFs\n0.48\n0.69\n0.68\n0.87\n0.88\nIdentical\nPosition\nSize\nIllumination Viewpoint\n*\n*\n*\n*\nratio =\n% signal condition\nratio = 1.0\nthere is no adaptation\n% signal different\nratio < 0.7 * significant adaptation (p<0.01)\n(Grill-Spector et al. 1999)\n\nUsing Adaptation to Test for Invariances\nposition\nviewpoint\ndifferent\nidentical\ntexture\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nDo images that vary only in position or viewpoint count as the \"same\" and\nhence get adapted, or do they count as \"different\" and not get adapted?\nCourtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.\n(Grill-Spector et al. 1999)\n\nCharacterizing Representations\nand Processes in the LOC\nAre shape representations in LOC\nindependent of how shape is represented, i.e. independent of\nform-cues (motion, luminance, texture)? probably\ncontours? Yes!\nIndependent of changes in the size, position, viewpoint, etc?\nPartly. More to size & position than viewpoint.\nCool method: fMRI adaptation\n\nFace Scrambling Experiment\nGrill-Spector, et al (1998)\ncar\nX 3\nX 4\n2 vertical\nwhole\nX 2\n2 horizontal\nX 2\nX 4\nX 4\nX 4\n414sec\nX 4\n\nWhole vs. Parts\nR. Malach\nWeizmann. Inst.\nGrill-Spector, Malach, others (1998)\n\nComments on Papers\n1. Writing matters in life; learn to do it well now. You cannot be a\nsuccessful scientist unless you write well. Probably true for most other\nprofessions as well.\n2. One strong argument (or maybe tops 3) is much more effective\nthan 12 weak ones. \"Kitchen sink\" papers are ineffective.\n3. Start the paper with a statement of what was found & claimed.\n4. A paper (or talk) is not a note to me >> it should pass the\n\"roomate test\".\n5. Don't just say X is a problem; say WHY!\n6. Paragraph structure.\n7. Write it, print it, get away from it, come back, and read it.\nREAD IT ALOUD.\n8. If you have a paragraph with 3-5 separate ideas that related to\nthes same point, it helps to indicate in advance, and enumerate them.\nE.g., \"There are four problems with this design...\"\n9. Distinguish between design problems that matter versus those\nthat don't.\n\nVariability Across Individual Subjects\nContrasts\nmoving>\nstationary\nbody parts>\nobjects\nfaces>\nobjects\nscenes>\nobjects\n- some discontiguities\nare apparent in\nindividual regions.\ntions are\noverlapping but\nnot identical\nacross subjects\nCoregister data across subjects using \"spherical coordinates\", then ask which regions\nshow a significant response in a given contrast in the same location in at least 30% of Ss.\nImage removed due to copyright restrictions. See Fig. 3 in Spiridon, M., B. Fischl, and N. Kanwisher.\n\"Location and Spatial Profile of Category-Specific Regions in Human\n\nExtrastriate Cortex.\" Human Brain Mapping 27 (2006): 77-89.\n- loca\n\nPopulation overlap Maps on Cortical Surface, Spherical Coords.\nWith Bruce Fischl & Mona Spiridon\nImage removed due to copyright restrictions. See Fig. 2 in Spiridon, M., B. Fischl, and N. Kanwisher. \"Location and\nSpatial Profile of Category-Specific Regions in Human Extrastriate Cortex.\" Human Brain Mapping 27 (2006): 77-89.\n\nPotter (1971)\nPresented a random sequence of complex scenes to subjects\nat a rate of around 7/second. Found that subjects could get\nthe gist of pretty much each one. (e.g., detect a \"picnic\").\nImplies:\ni) don't need \"top-down\" prediction to recognize objects\nii) object/scene recognition is FAST!"
    },
    {
      "category": "Resource",
      "title": "Visual Attention",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-71-functional-mri-of-high-level-vision-fall-2007/9cb46fdeb928ff3bc7bbfe44bd45608c_lec6_attn.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n9.71 Functional MRI of High-Level Vision\nFall 2007\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 6: Visual Attention 9.71\nOctober 11, 2007\nReminder: Term paper Outline due October 25 & Midterm Nov 1\nOutline of Today's Visual Attention Lecture\nA.\nIntroduction to attention\nlimited capacity and selectivity\nB. Three questions about attention\n1. How \"early\" is attentional selection?\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural responses?\nC. Presentations\nTess, on Corbetta et al (2005)\nLeah, on Muller & Kleinschmidt (2003)\nLisa, on McMains & Somers (2004)\n\nTerm Paper Topics/Outlines due Nov 1!\nMust be an fMRI Study of High-level Vision\nNo idea? Here are some strategies to try for coming up with an\nexperiment:\n1. List 3 of your favorite topics from this class.\n2. Remind yourself what the key questions are in this area (see\nlectures notes and syllabus).\n3. Read the assigned articles on those topics, in order to help answer\n#2 and #4. Also type the relevant keywords into Pubmed and\nbrowse.\n4. Come up with either a new way to answer one of these questions,\nor a related question that is not already answered.\nOr:\nThink of an experiment that used a particularly cool method. What\nother questions might you be able to answer by applying this\nmethod to another question?\n\nSome Questions\n- what method would you use to find out if a representation (say of\nchairs) is invariant to changes in position?\n- what visual area lies in the calcarine sulcus?\n- can fMRI tell you if a brain region is necessary for carrying out a\ntask?\n- what is a counterbalanced design?\n- how could you determine if neurons in the FFA can discriminate\nbetween different coffee cups?\n\nQuestion to Consider\nHow do you feel about people driving while talking on their\ncell phones. Is this a good idea?\nWhy/why not?\nThe notion of capacity, or resources.\nThe \"toaster model\": when you plug in the toaster the lights dim.\nWhich mental processes are \"on the same circuit\"?\nCan you listen to music and read at the same time?\nRecognize faces and scenes at the same time?\n\nIdentify (to yourself) the blue letters\nin the following display\n\nIdentify (to yourself) the blue letters\nin the following display\n\nProbability of reporting \"N\" is higher for 2nd display than the 1st\nDisplay 1\n\nLimited capacity - Only a small amount of information\non the retina can be fully processed and used for behavior\nSelectivity - We have the ability to filter out unwanted\ninformation (e.g., the red letters)\n\nProbability of reporting \"N\" is independent of the number of red\nletters.\nDisplay 1\n\nWhy is our capacity to process visual information\nlimited?\n1. Full analysis of everything in the visual field is impossible.\nBUT: Given the massively parallel structure of the human\nvisual system, why cant we process everything at once?\n2. We can only direct action to one object or portion of the visual\nfield at a time.\nA pikefish put in a tank with 10 sticklebacks will take much\nlonger to catch the first stickleback than a pikefish put in a\ntank with 1 stickleback. But why, exactly?\n\nAttention\nAttention as a filter that lets attended/selected information in\nbut filters out unattended information.\nTwo key properties of attention that go hand in hand:\n- capacity limits - we cant efficiently process everything at\nonce.\n- selectivity - so we select a subset of the available\ninformation for detailed analysis\nAn important distinction realized long ago by Helmholtz.....\n\nHelmholtz: Attention is Different from Fixation\n\" ...our attention is quite independent of the position and\naccommodation of the eyes, and of any known alteration in these\norgans, and free to direct itself by a conscious and voluntary effort\nupon any selected portion of a dark and undifferentiated field of\nview. This is one of the most important observations for a future\ntheory of attention.\"\nPhysiological Optics, circa 1860, quoted in James Principles, pg. 414\n\nOvert versus Covert Visual Attention\n- \"overt attention\" - eye movements - change in retinal input\na very powerful selection mechanism because of the fovea\n- \"covert attention\" - no eye movements, only changes in the\nway the same retinal image is processed\n\nDistinctions re Visual Attention\n- overt versus covert attention\n- automatic/stimulus-driven attention\ne.g. web pop-ups\nversus controlled/voluntary attention\ne.g. sneaking a peak on your neighbors computer screen\npartly distinct mechanisms\n- the source of attentional control, versus its site and effect\ne.g. \"who\" is turning the knobs versus what brain\nregions are affected and how exactly are they affected?\nfor today, we'll focus on the latter.\n\nOutline: Three Classic Questions about Attention\n1. How \"early\" is attentional selection?\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural responses?\n\nOutline: Three Classic Questions about Attention\n1. How \"early\" is attentional selection?\n- Anatomically\n- Temporally\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural responses?\n\nHow \"early\" does attention select?\nAnatomy:\nPrerequisite for attentional control:\nBidirection information flow.\nFirst structure in visual pathway that\nReceives feedback from higher centers:\nThe LGN.\nWhere along this pathway can we\nfirst see attentional effects?\nVentral pathway that\ntr\nac\nks\no\nbj\nect\nre\ncognitio\nn; div\nided in\nto \"wh\nere\" path (V2, V3, V5 to parietal cortex) and \"what\" path (V2, V4, inferotemporal cortex).\nFigure by MIT OpenCourseWare.\nCourtesy of George Eade. Used with permission.\n(c) 2009 Eade Creative Services Inc. 704-643-7335.\n\nAttentional effects in V1 w/ Physiology\nEarly physiological studies found little evidence for attentional effects\nin V1 but large effects at later stages.\nBut three studies that did report some effects in V1:\nMotter et al (1993)\nVidyasagar (1998)\nRoelfsema et al (1998)\nNone of this really convinced the field.\nThen people started testing this question with fMRI....\n\nAttentional Effects in V1 w/ fMRI\nIn 1998-9, six papers were published showing often large effects\nof attention on V1:\nWatanabe et al, 98\nSomers et al, 99\nBrefczynski & DeYoe, 99\nKastner et al, 99\nMartinez et al, 99\nGhandhi et al, 99\nI'll describe one of the most elegant studies in detail:\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000\n\nResponses increase with stimulus contrast in V1\nstimulus contrast\nlow\nhigh\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000 Courtesy of David J. Heeger. Used with permission.\n\nPattern detection protocol\nCourtesy of David J. Heeger. Used with permission.\n\nStrong response when stimulus present\nIndividual trial time\nseries\nWhat do you expect to see when the stimulus is absent?\n0.5\n0.4\n0.3\n0.2\n0.1\nmean, std. error\n-0.1\n-0.2\naverage of 296 trials\nsubject: DBR\nfMRI response\n(% BOLD signal)\n\nTime (s)\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000\nCourtesy of David J. Heeger. Used with permission.\n\nLarge response even when stimulus absent!\n- Base response when stimulus absent -- attention?\n- Small increment when stimulus present -- sensory signal?\nfMRI response\n(% BOLD signal)\n0.5\n0.4\n0.3\n0.2\n0.1\n-0.1\n-0.2\nStimulus present\nStimulus\nabsent\nsubject: DBR\nIncrement\nBase response\n\nTime (s)\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000\nCourtesy of David J. Heeger. Used with permission.\n\nBase response is spatially selective\nAnalyzed data in each of 3 subregions of V1:\nfMRI response\n(% BOLD signal)\n0.6\n0.4\n0.2\n-0.2\ncenter\nannulus\nperiphery\nsubject: DJH\n\nTime (s)\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000\nCourtesy of David J. Heeger. Used with permission.\n\nBase response predicts performance\nr = 0.92\np < 0.001\n1.3\n1.2\n1.1\n0.9\n0.8\n0.7\n-0.5\n0.5\n1.5\n2.5\nfMRI response amplitude\nRess, Backus, & Heeger, Nat Neurosci 3:940-, 2000\nCourtesy of David J. Heeger. Used with permission.\n\nHow \"early\" does attention select?\nAnatomy:\nSo: clear attentional effects in V1\nWhat about the LGN?\nMassive direct cnxns from V1 to LGN\n& via the thalamic reticular complex (TRN).\nCrick (1984) and Koch & Ullman (1985)\nHypothesized a major role for these structures\nin attention.\nBut few studies. and little evidence for this until:\nVanduffel et al (2000) deoxyglucose\nO'Connor et al (2002)....\nVentral pathway that\ntr\nac\nks\no\nbj\nect\nre\ncognitio\nn; div\nided in\nto \"wh\nere\" path (V2, V3, V5 to parietal cortex) and \"what\" path (V2, V4, inferotemporal cortex).\nFigure by MIT OpenCourseWare.\nCourtesy of George Eade. Used with permission.\n(c) 2009 Eade Creative Services Inc. 704-643-7335.\n\nAttentional response modulation in the human\nLGN\nO'Connor, Fukui, Pinsk, & Kastner\n(Nature Neuroscience, November 2002)\n\nActivating the human LGN: Checkerboards\nImages removed due to copyright restrictions.\nSee Fig. 1 in O'Connor, D., M. M. Fukui, M. A. Pinsk, and S. Kastner.\n\"Attention modulates responses in the human lateral geniculate nucleus.\"\nNature Neuroscience 5 (2002): 1203-1209.\n\nExperimental Design\nUnattended (task: count letters Attended (task: report\namong digits in RSVP)\nluminance changes in\ncheckerboard).\n+\na\na\n+\nCourtesy of Sabine Kastner. Used with permission.\n\nLGN: Contrast Effects\nUnattended\na\na\n% Signal Change\n+\n+\n\nN=4\nTime (sec)\nCourtesy of Sabine Kastner. Used with permission.\n\nExperimental Design\nUnattended (task: count letters Attended (task: report\namong digits in RSVP)\nluminance changes in\ncheckerboard).\n+\na\na\n+\nCourtesy of Sabine Kastner. Used with permission.\n\nHow anatomically \"early\" does\nattention select?\nCan already find attentional modulation\nin the LGN, the first structure in\nthe visual pathway that receives feedback\nfrom higher centers. That's an\nanatomically early stage.\nBUT: are these modulations of the\nfirst pass of visual information up\nthe visual system, or do they occur\nonly at later latencies?\nWith fMRI we cannot tell.\nBut ERPs can be very informative...\nVentral pathway that\ntr\nac\nks\no\nbj\nect\nre\ncognitio\nn; div\nided in\nto \"wh\nere\" path (V2, V3, V5 to parietal cortex) and \"what\" path (V2, V4, inferotemporal cortex).\nFigure by MIT OpenCourseWare.\nCourtesy of George Eade. Used with permission.\n(c) 2009 Eade Creative Services Inc. 704-643-7335.\n\nSpatial Attention: 40-200 ms\nN1\n-1μV\n+1μV\nP1\nC1\nA t t end ed\nIgno red\nP2\n\"C1\" wave comes from\nV1; Does not show\n-100\nattentional modulation.\nTim e (m s)\nCourtesy of Steven J. Luck. Used with permission.\n\nOutline\n1. How \"early\" is attentional selection?\n- Anatomically: very early (LGN)\n- Temporally: after the first feedforward pass thru V1.\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural representations?\n\nOutline\n1. How \"early\" is attentional selection?\n- Anatomically: very early (LGN)\n- Temporally: after the first feedforward pass thru V1.\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural representations?\n\nTest: when you try to select a particular feature\nat a particular location,\nDoes attention select: what else \"comes along for the ride\"?\nlocations?\n>>>other features at the same location\nobjects?\n>>>other features in the same object\nfeatures?\n>>> other stimuli sharing the same features\nNeural responses a good way to answer this kind of question.\nNote: these are not mutually exclusive.\n\n1. Does attention select locations?\nDowning & Kanwisher\nPREDICTION OF LOCATION-BASED ATTENTION:\nAll the visual information at the attended location will be enhanced,\nwhether task-relevant or not.\nFor example, in this stimulus:\nattention to red oval >> enhancement of representation of face\nattention to green oval >> enhancement of representation of house\nFace photo modified by OCW for privacy considerations.\n\nOverall Logic\nFusiform face area\nParahippocampal place area\n(FFA)\n(PPA)\nFaces > Houses\nHouses > Faces\nfMRI signal from FFA:\nfMRI signal from PPA:\na measure of the strength of\na measure of the strength of\nthe neural response to a face\nthe neural response to a house\nstimulus\nstimulus\nNote: this use of fMRI signals as markers of particular processes is very\nuseful in attention research, since we can measure the response to a\nstimulus without asking subjects about it & hence making it attended.\n\nDoes attention select locations?\nDowning & Kanwisher\n-stimuli like this appear for 160 ms, once every 2 sec in a random order\n-Ss fixate the center dot; attend to the red oval or green oval (cued by block)\n- Ss report orientation of attended oval\n- faces and houses are never task relevant\n- all stimulus conditions randomly interleaved\nFace photo modified by OCW for privacy considerations.\n\nDoes Attention Select Locations?\nDowning & Kanwisher\nPredictions of Location-Based Selection\nFFA: MR signal higher when\nthe face is at attended\nlocation than when the\nhouse is\nPPA: MR signal higher when\nthe house is at attended\nlocation than when the\nface is\nFace photo modified by OCW for privacy considerations.\n\nDoes Attention Select Locations?\nYES!\nDowning & Kanwisher\nPredictions of Location-Based Selection\nFFA: MR signal higher when\nthe face is at attended\nlocation than when the\nhouse is\nPPA: MR signal higher when\nthe house is at attended\nlocation than when the\nface is\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\nAtt Loc H\nAtt Loc F\n-0.2\n-0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n\nATTLoc H\nAtt Loc F\nFFA\nPPA\n\nWhat does attention select?\nlocations?\nenhancement of\nirrelevant information\nat the same location\nSimilar single-unit results by Connor et al 96\n\"objects\"?\nenhancement of\nirrelevant attributes of the\nattended object\nfeatures?\nenhancement of\nirrelevant objects sharing\nfeatures w/ the target object.\nFace photos modified by OCW for privacy considerations.\n\nDoes attention select \"objects\"?\nO'Craven, Downing & Kanwisher (1999)\nPREDICTION OF OBJECT-BASED ATTENTION:\nAll the visual attributes of the attended object will be enhanced,\nwhether task-relevant or not.\nFor example, in this stimulus:\nfaces moves back and forth\nhouse remains stationary\nand is slightly off center\nattention to motion >> enhancement of representation of face\nattention to position >> enhancement of representation of house\nFace photo modified by OCW for privacy considerations.\n\nDoes Attention Select Objects?\nO'Craven, Downing & Kanwisher (1999)\n-stimuli like this appear for 200 ms, once every 2 sec\n-either face moves,or house moves in a random order\n-Ss fixate the center dot; attend to motion or position (cued by block)\n- faces and houses never task relevant\n- all visual features are superimposed in the same location\nFace photos modified by OCW for privacy considerations.\n\nDoes Attention Select Objects?\nO'Craven, Downing & Kanwisher (1999)\nPredictions of Object-based Selection\nFFA: MR signal higher when\nthe face is the irrelevant\nproperty of attended\nobject than when the house\nis\nPPA: MR signal higher when\nthe house is the irrelevant\nproperty of attended\nobject than when the\nface is\nFace photo modified by OCW for privacy considerations.\n\n1b. Does Attention Select Objects? YES!\nO'Craven, Downing & Kanwisher (1999)\nResults\nPredictions of Object-based Selection\nFFA: MR signal higher when\nthe face is the irrelevant\nproperty of attended\nobject than when the house\nis\n-0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n-0.2\nFFA\nface\nhouse\n\nPPA: MR signal higher when\nthe house is the irrelevant\nproperty of attended\nobject than when the\nface is\n- loc-based selection cannot easily explain\nFace photo modified by OCW for privacy considerations.\n-0.2\n-0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\nface\nhouse\nPPA\n\nWhat does attention select?\nlocations?\nenhancement of\nirrelevant information\nat the same location\n\"objects\"?\nenhancement of\nirrelevant attributes of the\nattended object\nfeatures?\nenhancement of\nirrelevant objects sharing\nfeatures w/ the target object.\nFace photos modified by OCW for privacy considerations.\n\nLogic:\nIf attention selects visual features, then when a subject tries\nto attend to a given feature in a given location, other instances\nof that same feature elsewhere in the visual field will get\nenhanced along with the attended item.\nSaenz et al (2002) tested this hypothesis for color and for motion,\nfollowing the logic of a similar paper by Treue & Trujillo (1999).\n\nExperiment 1: Influence of unattended motion features\non visual cortex\ntarget\nignored\nAttend one direction of\nmotion in the target area (cued\nto shift attention from up to\nImage removed due to copyright restrictions.\ndown every 20s)\nSee Fig. 1a in Saenz, M., G. Buracas and G.\nBoynton. \"Global effects of feature-based\nattention in human visual cortex.\" Nature\nTwo stimuli per trial.\nNeuroscience 5, no. 7 (2002): 631-632.\nTask: which stimulus has\nfaster motion?\n\n-Recorded BOLD signal in V1, V2, V3, V3A, MT+ in hemisphere\ncontralateral to ignored stimulus\n-baseline: just fixation point\nBOLD response over time for ignored stimulus for MT+\n(same v. different direction as attended direction)\nImage removed due to copyright restrictions.\nSee Fig. 1b in Saenz, M., G. Buracas and G.\nBoynton. \"Global effects of feature-based\nattention in human visual cortex.\" Nature\nNeuroscience 5, no. 7 (2002): 631-632.\n\nAll visual areas responded more strongly to\nignored stimulus when it moved in the same\ndirection as the attended motion.\nResponse amplitudes to ignored motion stimulus (same vs diff\ndirec of motion as target direction)\nImage removed due to copyright restrictions.\nSee Fig. 1c in Saenz, M., G. Buracas and G.\nBoynton. \"Global effects of feature-based\nattention in human visual cortex.\" Nature\nNeuroscience 5, no. 7 (2002): 631-632.\nExp 2: Find same\nthing with color.\nSaenz et al (2002)\n\n-\n-\nWhat does attention select?\nlocations?\nenhancement of irrelevant\ninfo at the same location\n\"objects\"?\nenhancement of irrelevant\nattributes of target object\nfeatures?\nenhancement of irrelevant objects sharing\nfeatures w/ the target object.\nImage removed due to copyright restrictions.\nSee Fig. 1a in Saenz, M., G. Buracas and G.\nBoynton. \"Global effects of feature based\nattention in human visual cortex.\" Nature\nNeuroscience 5, no. 7 (2002): 631 632.\nDuncan & Desimone's \"biased competition\" model: features with common\nobject, location, or feature \"stick together\" in the competition.\nFace photos modified by OCW for privacy considerations.\n\nBiased Competition Model of Attention (Desimone & Duncan, 95)\n1. Objects or locations\ncompete for representation at\nupper levels of the visual\nsystem\n2. Attention results when\na certain object or location\nwins the competition.\n3. The competition can be\nbiased toward certain items\nby bottom-up or top-down\nsignals that add more\nexcitation to to-be-attended\nitems\nD\ni\na\ng\nram depiciting biased competition attention process between images of dog and fire hydrant.\nFigure by MIT OpenCourseWare.\n\nMystery: How does the visual system bias competition,\ne.g. in this case, how does it \"know\" whether\nit is the face or the house that is moving when\nthe two are superimposed?\n- Duncan & Desimone's Biased Competition model presumes\nthat the binding problem has already been solved, but does not\nsay how.\n- It is unlikely that the necessary information is available in MT,\nFFA, & PPA; earlier stages of the visual system are probably\ninvolved.\nFace photo modified by OCW for privacy considerations.\n\nOutline\n1. How \"early\" is attentional selection?\n- Anatomically\n- Temporally\n2. What are the units of attentional selection?\n3. How exactly does attention affect neural representations?\n\nTwo ways attention might\nincrease postsynaptic\nimpact:\nA.) Increase firing rate\nB.) Increase synchronization\n(invisible to fMRI).\nFries et al (2001):\nattention in RF not only\nincreases firing rate, but\nalso increases synchron\nization of spikes with LFP,\nspecifically in 35-50 Hz\nrange.\nCourtesy of MIT Press. Used with permission. Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights\nfrom Brain Imaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd edition. Edited by Michael S.Gazzaniga.\nCambridge, MA: MIT Press, 2004.\n\nTwo ways attention might\nincrease postsynaptic\nimpact:\nA.) Change firing rate\nB.) Increase synchronization\n(invisible to fMRI).\nCourtesy of MIT Press. Used with permission. Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights\nfrom Brain Imaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd edition. Edited by Michael S.Gazzaniga.\nCambridge, MA: MIT Press, 2004.\n\nFiring Rate Effects\n1. Possible effects of Attention on Neural Response:\nBlue = unattended; Red = attended.\n1a. McAdams & Maunsell (1999): (multiplicative) gain modulation in V4.\nCourtesy of MIT Press. Used with permission. Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights\nfrom Brain Imaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd edition. Edited by Michael S.Gazzaniga.\nCambridge, MA: MIT Press, 2004.\n\nFiring Rate Effects\n1. Possible effects of Attention on neural Response:\nBlue = unattended; Red = attended.\nCourtesy of MIT Press. Used with permission. Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights\nfrom Brain Imaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd edition. Edited by Michael S.Gazzaniga.\nCambridge, MA: MIT Press, 2004.\n\n1. Possible effects of Attention Neural Response:\nb. Offset Modulation (= Baseline Shift)\nRess, Backus & Heeger (2000):\nstim-independent activity in V1.\nImage removed due to copyright restrictions.\nFigure 1 in Ress, D., B. T. Backus, & D. J. Heeger.\n\"Activity in primary visual cortex predicts\nperformance in a visual detection task.\"\nNature Neuroscience 3, no. 9 (2000):940-945.\nWhy can't this be gain modulation or\nchange of tuning width?\n\nFiring Rate Effects\n1. Possible effects of Attention on Neural Response:\nBlue = unattended; Red = attended.\nNeurophysiologists can study this by directly measuring the tuning\nfunction of individual neurons. What fMRI method would you use?\nCourtesy of MIT Press. Used with permission.Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights from Brain\nImaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd ed. Edited by Michael S.Gazzaniga. Cambridge, MA: MIT Press, 2004.\n\nFiring Rate Effects\n1. Observed effects of Attention on Neural Response:\nCourtesy of MIT Press. Used with permission.Source: Friewald, W. and N. Kanwisher. \"Visual Selective Attention: Insights from Brain\nImaging and Neurophysiology.\" In The Cognitive Neurosciences. 3rd ed. Edited by Michael S.Gazzaniga. Cambridge, MA: MIT Press, 2004.\n\nWhat Have we Learned about Attention?\n1. How \"early\" is attentional selection?\n- Anatomically very early: LGN\n- Temporally not necessarily so early - may occur only after\ninitial feedforward pass up the system.\nBut: baseline shifts = very early....\n2. What are the units of attentional selection?\nlocations, features, and objects\n3. How exactly does attention affect neural responses?\nsynchrony of firing, gain modulation, & baseline shifts\nsharpening of tuning"
    }
  ]
}