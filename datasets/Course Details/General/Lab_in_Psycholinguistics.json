{
  "course_name": "Lab in Psycholinguistics",
  "course_description": "Hands-on experience designing, conducting, analyzing, and presenting experiments on the structure and processing of human language. Focuses on constructing, conducting, analyzing, and presenting an original and independent experimental project of publishable quality. Develops skills in reading and writing scientific research reports in cognitive science, including evaluating the methods section of a published paper, reading and understanding graphical displays and statistical claims about data, and evaluating theoretical claims based on experimental data. Instruction and practice in oral and written communication provided.",
  "topics": [
    "Humanities",
    "Linguistics",
    "Semantics",
    "Syntax",
    "Science",
    "Cognitive Science",
    "Humanities",
    "Linguistics",
    "Semantics",
    "Syntax",
    "Science",
    "Cognitive Science"
  ],
  "syllabus_content": "Course Meeting Times\n\nLecture: 2 sessions / week, 1.5 hours / session\n\nLab: 1 session / week, 3 hours / session\n\nPrerequisites\n\n9.00 Introduction to Psychology\nor\n24.900 Introduction to Linguistics\n; students will be expected to learn some R and Mechanical Turk.\n\nSummary\n\nIn this class, students learn to design, conduct, analyze and present experiments on\nthe structure and processing of human language\n, through hands-on experience. The main focus of the class is on constructing, conducting, analyzing, and presenting an original and independent experimental project of publishable quality. This will include developing skills in using Amazon.com's Mechanical Turk and the statistical programming language R. We will also develop skills in reading and writing scientific research reports in human language research, including evaluating the methods section of a published paper, reading and understanding graphical displays and statistical claims about data, and evaluating theoretical claims based on experimental data.\n\nThe topics of research will all involve the structure and processing of human language. We will focus on issues regarding language as communication (including the field of pragmatics), the domain specificity / domain generality of language, and the relationship between language and thought.\n\nResearch Projects\n\nStudents choose a topic on human language research from a list provided by the instructors. Students design and conduct a replication of the published experiment. Then, with guidance from the instructor and the TA, students design, implement, and analyze results of an original experiment, which extends the first project in a novel direction. Students report their findings from the replication and extension experiments in papers and presentations. Students are encouraged to construct and conduct the experiments during lab time, when the TA or instructor will be available.\n\nAt the same time that students are choosing their topic for research, there are letures and discussion classes presenting background on experimental research in language. For most of these classes, students write a brief report about one of the papers to be discussed. The reports will then be used by the instructor in guiding the discussion in class. Writing these discussion notes will help students to read and evaluate current research in the cognitive psychology of language.\n\nMechanical Turk\n\nMost students will run their experiments on Mechanical Turk, a crowd-sourcing website available through Amazon.com, where one can get experimental participants efficiently.\n\nGrading\n\nMost of the grade in this class comes from lab-based writeups or oral presentations of students' experiments. Details are provided below. Project topics will be handed out in the first three weeks of class.\n\nActivity\n\npercentage\n\n4 problem sets on R, statistics\n\n15%\n\n16 discussion notes\n\n15%\n\nPaper 1: Evaluation of a related experimental topic in the literature (40% for draft 1; 60% for draft 2)\n\n10%\n\nPaper 2: Project proposal, plus bibliography for the project\n\n5%\n\nOral presentation of proposed project\n\n5%\n\nPaper 3: Replication writeup\n\n10%\n\nOral presentation of final project\n\n10%\n\nPaper 4: Final paper\n\n20%\n\nClass and lab participation (including obligatory attendance)\n\n10%\n\nLate policy: 10% off each day late, down to 50% off. Then you can hand it in later for 50% credit.",
  "files": [
    {
      "category": "Assignment",
      "title": "pset 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/d3a4c74276283ce1ecd059287a7e7271_MIT9_59S17_pset1.pdf",
      "content": "9.59 Lab in Psycholinguistics: Problem Set #1\nThe goal of this problem set is to get you comfortable manipulating and analyzing data in R. Here is what\nyou should turn in:\n1. A document (preferably a PDF but Word or txt is okay) that contains JUST the answers to the\nquestions.\n2. Your R code with comments.\nOR\n1. An R Markdown document (pdf or html) where you write your responses and show your code.\nYou will explore a data set provided in the languageR package from the Baayen book. It contains a set of\nEnglish words with lexical decision time data and other measures like written and spoken frequency. Lexical\ndecision time is one of the most important behavioral measures used in psycholinguistics. In a lexical decision\ntask, a subject is presented with a word (or non-word like gnuppet) and asked to judge as quickly as possible\nwhether or not it is a word. How fast they can make a decision reflects something about the psychological\nresponse of the subject to the word in question. In this problem set, you will find out what sorts of things are\npredictive of lexical decision time.\n- Download rts.csv and put it in the same folder as your script. Read in the data as a data frame.\n- RTlexdec is the lexical decision time for each data point in the data frame. Other relevant columns will\nbe defined below.\n1. What is the overall mean and the overall standard deviation for RTlexdec? Use the functions mean()\nand sd().\n2. How is lexical decision time a ected by how long the word is? Calculate the means and standard\ndeviations for RTlexdec by LengthInLetters. Look at the mean and standard deviation for each of the\npossible lengths. Using ggplot, make a scatterplot with LengthInLetters on the x axis and RTlexdec\non the y-axis. What can you conclude?\n3. Make a new data frame containing only words for which VerbFrequency and NounFrequency are greater\nthan 0. How many data points were there originally in the dataset and how many does this eliminate\nfrom the original data frame? How is the mean RT a ected and why?\n4. The WrittenSpokenFrequencyRatio compares the frequency of a word in writing to its spoken frequency.\nSo big numbers mean it's more likely to be written. What's the relationship between word length and\nWrittenSpokenFrequency? (You can answer in a number of di erent ways.)\n5. How does the mean RTlexdec for words that start with \"p\" compare to the overall mean RTlexdec?\nWhat about words that start with \"q\"? Useful functions might be mean(), str_sub(), mutate(),\nfilter(). Using facet_wrap(), show a histogram of RTs for words starting with \"p\" juxtaposed with\na histogram of RTs for words starting with \"q\". What does this tell you about words that start with \"p\"\nand words that start with \"q\"?\n6. What has a lower mean RTlexdec: nouns or verbs? What are the means? Find the log ratio log(A/B) of\nthe NounFrequency and VerbFrequency and add it as a column called NounVerbFreqRatio. Sometimes\nthis gives you an answer that's not a number. In those cells, replace the non-number with NA. Using\nthe function is.na(), how many NA's are produced?\n7. Now we want to know how far the lexical decision time for some given words are from the means for\nthe group. To do this, you can use z-transformed values. Add a column to the original data frame\ncontaining z-transformed values for RTlexdec. The formula for the z-score is (x - μ)/s, where x is the\nparticular value, μ is the mean value, and s is the standard deviation. Estimate z-scores separately for\neach of the two possible AgeSubject categories. That is, for each word, calculate one z-score using the\nmean and standard deviation for YOUNG people and one using the mean and standard deviation for\n\nOLD people. Give z-scores for the words GULP and DOE. There should be two z-scores for each word\n(one for YOUNG and one for OLD). How do response times to \"doe\" and \"gulp\" compare to the mean\nin each group?\n8. Consider and plot the correlations between each pair of 2 among the following: Familiarity, WrittenFre\nquency, FamilySize, and RTlexdec. Which factors are positively correlated with reaction time? Are you\nsurprised by the results? Why or why not?\n9. Using the z scores calculated before for young and old separately, find the words that have the biggest\ndi erence between young z score and old z score. List the 3 for which young is biggest relative to old\nand the 3 for which old is biggest relative to young. Are you surprised by the results? (For added fun,\ntype these words into the Google Ngram viewer!)\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Pset 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/d441e42ef37bec359c7acf2d2b5c07cf_MIT9_59S17_pset2.pdf",
      "content": "9.59 Lab in Psycholinguistics: Problem Set #2\nReading and processing Turk data\nIn this problem, you will get some slightly preprocessed data from Amazon's Mechanical Turk from an\nexperiment run in the lab in 2012 (now published in Mahowald, Fedorenko, Piantadosi, & Gibson, 2013). (It\nwill be a good idea to save this code to re-use for your own Turk project.)\nIn this project, Mahowald et al. looked at word pairs like chimp/chimpanzee and math/mathematics. A\nprediction from information theory is that words that are predictable in context should be shorter. If you\nalready know what the next word will be (as in \"To be or not to. . . \"), there is no reason to spend a lot of\ntime and e ort saying a long word. On the other hand, if the word is entirely unpredictable (\"The next word\nI am going to say is . . .\"), the word should be longer and thus more robust to noise. That is, if one syllable\ngets garbled or misunderstood, the meaning can still be recovered. To test whether English is eycient in\nthis regard, they gave subjects on Turk sentences like \"Susan loves the apes at the zoo, and she even has a\nfavorite . . . \" (supportive context) or \"During a game of charades, Susan was too embarrassed to act like\na . . . \" (neutral context) in which they were asked to complete the sentence with either the word chimp or\nchimpanzee.\nHere is a sample trial from the supportive condition:\nSusan loves the apes at the zoo, and she even has a favorite. . .\n1. chimp 2. chimpanzee\nThere were 40 longshort word pairs, and they were presented to the subjects in random order such that each\nsubject saw a di erent order of sentences. They also varied which form appeared first (the short one or the\nlong one). They predicted that the shorter form would be more common in the predictable construction than\nin the unpredictable one for a whole list of such word pairs. We want you to look and see if this prediction is\ncorrect!\n- Download the csv files long_short_turk.csv and decode_long_short.csv and put them in the same\nfolder as the R script you are creating. This data set is from a behavioral experiment run on Turk in\nthe Gibson lab.\n- Read in the results data from the csv file so that you have a data frame containing everything from\nlong_short_turk.csv. This is your main results file. HINT: Make sure you have the tidyverse package\nloaded using library()\n- You now have a data frame with many columns. As is typical of Turk output, there is a row for each\nsubject. This is not ideal for analysis in R. Rearrange the data using gather() so that there is one row\nfor each individual trial. That is, because there are 80 items on the survey, each Subject will have 80\nAnswer Choice rows.\n- You should now have the following columns:\n- WorkerId: unique Turk ID for each subject\n- Input.list: Each subject receives her own random order (identified by the list number).\n- Answer.country: Country of subject\n- Answer.English: Whether the subject's native language is English\n- variable: This is output by the gather() function. See below.\n- value: The value for the variable output by gather().\nThe column variable consists of two parts separated by an underscore. The first part indicates whether that\nrow's value specifies the input, a question, or the answer choice. For our purposes, you are interested in\nwhen the value is 'AnswerChoice'. The part after the underscore specifies where on the subject's list that\nindividual item appeared. So when a row has 'AnswerChoice_1' in the variable column, that means that the\n\nvalue column will say whether that subject picked Choice 1 or Choice 2 for their first question on the survey.\nNote that the first question on the survey is likely to be di erent for every subject.\n- To get them into their own columns, use separate() to split the 'variable' column at the underscore\ninto 2 columns: 'Type' and 'PresentationOrder'\n- When Type is either InputTrial or InputQuestion, that means that the row contains either the prompt\ngiven to the subject or a question asked to see if the subject was paying attention. You can ignore these\nfor now by filtering out everything in the dataframe except the rows where Type is AnswerChoice.\n- Read in the file decode_long_short.csv into a separate data frame. This new data frame contains the\ninformation you need to actually match up the items and words with the Turk output. Merge the decode\ndataframe with your main data frame, using left_join(), based on the columns PresentationOrder\nand Input.list. The Item column is consistent across subjects. That is, Item 1 refers uniquely to the\npair math/mathematics.\n- For some trials, the subject may have failed to answer the question. In this case, an NA will appear in\nthe dataframe. Remove all NA rows from the data frame using na.omit() on the data frame.\n- If you have column names like \"value\" or \"variable\", you should change them since those words are\nused by R and you might get errors. Change \"value\" to AnswerChoice using rename().\n- You should also remove the items that fall into the Filler condition. These were used in the experiment\nto prevent the subjects from noticing that the task was about long and short forms of the same word.\nFilter the subjects so that we have only English speakers and only people from the US.\n- To get the data in a meaningful form, you should add a column called PickedShort that is 1 if the\nsubject picked the short form and is 0 otherwise. You can do this by using AnswerChoice and Condition\nto determine what the subject picked for each trial. You might want to use separate() on Condition\n(similar to what we did a few steps above). This will give you a Cond column that is either \"neut\" for\nneutral or \"supp\" for supportive, and a First column that is either \"longfirst\" or \"shortfirst\" (depending\non whether the long form or short form was shown first).\n- AnswerChoice tells you 1 or 2 for which option was chosen. If the long form was first (i.e. First ==\n\"longfirst\") and the AnswerChoice is 1, that means that the person picked the long form. If the short\nform was first and the AnswerChoice is 2, that means that the person chose the long form. Use this\nknowledge to set the PickedShort column to 1 if the subject picked the short form or 0 if they picked\nthe long form. HINT: you can use mutate() and if_else().\n- Now that the data are \"tidy\", you can start analyzing and looking for patterns. The item of interest is\nwhether the person picked the long or short form and what the condition was. Give a short quantitative\ndiscussion of the results. Make sure to answer the following questions. There is some flexibility in how\nyou answer the question, but be sure to report numbers (no inferential statistics necessary) and be clear\nabout which numbers you are reporting.\n- How many subjects remain in the experiment after exclusion?\n- Averaging across all trials, on what proportion of trials did someone pick the short form?\n- How does the proportion who picked short di er in the neutral condition vs. the supportive\ncondition?\n- Which word pair had the highest overall PickedShort percentage?\n- Which was least often used in the short form?\n- Which word had the biggest di erence between the supportive and neutral condition?\n- Do subjects seem to choose the item that appears first more often than the one that appears\nsecond?\n\n- Does it look like there is a shift in preference for long vs. short words over the course of the\nexperiment?\n- What else do you notice in this data set that is worth pointing out?\n- What sorts of things still need to be controlled for in this analysis?\n- What conclusions do you draw from this experiment?\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Pset 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/1522b43a998bbe25ad31e469f9ff0730_MIT9_59S17_pset3.pdf",
      "content": "9.59 Lab in Psycholinguistics: Problem Set #3\nThe goal of this problem set is to get you comfortable doing some exploratory data analysis and significance\ntesting in R.\nIn the first pset, you used the reaction times data set. Look at pset 1 to figure out how to load the RT's\ninto R. You may want to re-use some of your code from that problem set to answer the questions below. As\nbefore, RT refers to RTlexdec.\n1. Plot a histogram to look at the distribution of RT's in the data set. Make sure the number of bins is\nsuch that you can clearly see the distribution. Do RT's appear to be normally distributed? How many\n'peaks' are there in the distribution?\n2. Now use facetting to make separate histograms for young and old subjects. What does this reveal about\nthe first histogram? Do the young person data look normally distributed?\n3. IMPORTANT: For all remaining questions, use only the young subject data. That is, throw out the old\nsubjects. Now (as in pset 1) calculate z-scores for each of the data points. Remember that the z-score\nis (x - μ)/sd.\na. If the data were normally distributed, what percentage of the data would you expect to have a\nz-score greater than 1.96? Less than -1.96?\nb. What percentage of the data actually has a z-score above 1.96? What percentage is actually below\n-1.96? If one of these things is very di erent from what you expect (hint, hint), why might that be\nthe case?\nc. What percentage of words, if the data were normally distributed, would have a z-score higher than\n3? Look at the words that DO have a z-score higher than 3. Why do you think they do?\n4. Compare the median and mean of young people RT's. How close are they? Now compare the median\nand mean of the column NounFrequency. What's going on here? (i.e., why the big di erence in one\ncase?). It may be helpful to plot the distribution.\n5. We previously looked at the mean RT for words that start with 'p' vs all other words. Do a two-sided\n(i.e., default) t-test using t.test() on the p-word RT's vs the other RT's to see if the di erence is\nsignificant. Report the t-value and the p value, and say whether it is significant at 95%. What can we\nconclude based on this?\n6. Using ggplot2, create a boxplot comparing noun RT's to verb RT's. Do the outliers generally appear\nabove or below? Why? Run the function fivenum() on the data and compare to the boxplot values.\n7. Create a bar plot with error bars showing 95% confidence intervals for the mean noun RT vs. the mean\nverb RT.\n8. Make a boxplot (one box for each letter) showing the mean RT for each initial letter of the word. I.e.,\none box for a, one for b, one for c, etc.\n9. Compare RTs for words that start with 2 consonants to the RTs of all other words (i.e. any word\nthat starts with something other than 2 consonants). Using an appropriate test of your choice, give a\nreasonable discussion of whether that di erence is significant.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Pset 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/b1803dc52a8f8848f4aadcdba3ab7179_MIT9_59S17_pset4.pdf",
      "content": "9.59 Lab in Psycholinguistics: Problem Set #4\nIn this assignment, you will analyze data from an experiment that was run on Amazon Mechanical Turk. In\nthis experiment we tested how and whether the placement of a verb particle (i.e., the position of a particle\npreposition relative to a verb, for verbs like \"put up\", \"throw out\", . . . ) depends on the length (in number of\nwords) of the direct object of the verb. We show participants four kinds of sentences, in which the particle\neither comes early or late, and the direct object is either long or short. This is called a 2x2 design. The\nindependent variables are: Particle position (early or late), and object length (long or short).\nExample Sentences:\n- Joe threw the documents out. (late-short)\n- Joe threw the very important documents that he brought home out. (late-long)\n- Joe threw out the documents. (early-short)\n- Joe threw out the very important documents that he brought home. (early-long)\nYour task is to produce a full analysis of this experiment and write a paragraph fit for publication in the\nresults section of a cognitive science journal paper, with plots. You should use simple linear regression to\nanalyze the data.\n1. Load the pset4_particle_shift_data.csv file into R. Exclude all participants from the analysis i) whose\nhome country is not USA (Answer.country), whose native language is not English (Answer.English), iii)\nand who did not answer at least 90% of the item comprehension questions correctly (Correct). Also,\nthrow out any individual data points (not the whole participants!) that have NA for Answer.Rating.\nReport the number of remaining rows in the data frame after excluding these participants and data\npoints.\n2. The column Condition simultaneously encodes the two independent variables. It would be better to\nhave one independent variable per column. Use separate() to split the column Condition into two\ncolumns based on the position of the character.\n3. Transform the grammaticality ratings (Answer.Rating) into z-scores with means and standard deviations\nestimated within subjects.\n4. Make a bar plot with the means for each condition and their 95% confidence intervals (plot raw means,\nnot z-scores). Map one independent variable to the x axis and a fill color, and split the data by the\nother independent variable using facet_wrap or facet_grid.\n5. Define two dummy-coded predictors based on the independent variables (early vs late, long vs not long).\nWhat will their coeycients fit? What will the coeycient of the intercept fit?\n6. Fit a least squares regression (using lm() or glm()) to the data predicting z-scored judgments from the\ndummy coded predictors based on the independent variables and their interaction. Use the summary\nfunction applied to the output of the lm function to get the model output. Briefly describe the result.\n7. Use the coeycients the model outputs to calculate the predicted group means for the four cells of the\ndesign. How far o are they from the actual group means?\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/9cae9fb783fa388f0c8d54043b891afb_MIT9_59jS17_lec1.pdf",
      "content": "9.59J / 24.905 Laboratory in\nPsycholinguistics (CI-M)\nInstructor: Ted Gibson\npre-requisite: 9.00\nOther useful classes: 24.900;\nProbability and Statistics\n\nGoals\nStudents will learn to:\n1. read the primary literature in language\nresearch and design informative\nexperiments;\n2. present research orally;\n3. write a research paper;\n4. gather data on the crowd-sourcing website\nMechanical Turk (run by Amazon.com);\n5. analyze their experiments using the R\nprogramming language\n\nClass 1: Overview\n-\nMechanical Turk demonstration in class\n-\nSyllabus / requirements information\n-\nThe usefulness of quantitative methods in linguistics\n-\nLanguage information sources and constraints\n- Lexicon; syntax; world knowledge; pragmatics; prosody;\nworking memory; context\n\nMechanical Turk in class\n1. Set up the materials in turkolizer format (Gibson, Fedorenko & Piantadosi\n(2011)\nParticle-shift and length: Locality of syntactic dependencies\nVerb-particle shift (-, +) x Length (short, long)\nShort NP object:\nLocal Particle: Joe threw out the documents.\nNon-local Particle: Joe threw the documents out.\nLong NP object:\nLocal Particle: Joe threw out the very important documents that he brought home. *\nNon-local Particle: Joe threw the very important documents that he brought home out. *\nPredictions of dependency locality:\nNo difference for short NP object; Big difference for long NP object\n\nMechanical Turk in class\nParticle-shift and length: Dependency-locality\n# particle-length 1 loc-particle-short\nJoe threw out the documents.\n? Did Joe throw out the documents? Yes\n# particle-length 1 nonloc-particle-short\nJoe threw the documents out.\n? Did Joe throw out the documents? Yes\n# particle-length 1 loc-particle-long\nJoe threw out the very important documents that he brought home.\n? Did Joe throw out the documents? Yes\n# particle-length 1 nonloc-particle-long\nJoe threw the very important documents that he brought home out.\n? Did Joe throw out the documents? Yes\n\nMechanical Turk in class\n2. Run the turkolizer program on the formatted items\nproduces a .turk.csv file:\n- turk materials: look at this file\n- Includes information for decoding later:\na.\nlinking the presentation order to the appropriate conditions\nb.\ncorrect answers to comprehension questions\n\nMechanical Turk in class\n3. Set up the Turk template in Mechanical Turk \"Create\"\n-Instructions\n-Format for ratings (1 - 5 or 7)\n-Rating then question(s)\n\nMechanical Turk in class\n4. Post the survey\n-Go to M Turk: go to Create\n-New Batch with an existing project\n5. Wait 45 - 60 mins\n6. Get the data from \"Manage\":\n-Download data\n\nTopics and Texts\nConstraints on language processing:\nLexicon, syntax, world knowledge,\npragmatics\nLanguage as communication: words,\nsyntax\nThe domain specificity / generality of\nlanguage\nPragmatics of language use\nBehavioral methods and issues in\nexperimental design\nThe R programming language\nDescriptive statistics & plotting\nSignificance testing & confidence\nintervals\nRegression and mixed effects\nregression\nAmazon.com's Mechanical Turk\nReadings: Analyzing Linguistic Data, Harald Baayen textbook; Wickham &\nGrol (2016). R for Data Science; 2-3 journal papers every class; 10-15 for\nfinal paper. (Approximately: 50 total)\n\nAssignments and grading\n15%\n15%\n10%\n5%\n5%\n10%\n10%\n20%\n4 programming assignments in R:\n16 discussion notes:\nPaper 1: Evaluation of an experiment in the\nliterature (40% for draft 1; 60% for draft 2):\nPaper 2: Project proposal, plus bibliography:\nOral presentation of proposed project:\nPaper 3: Replication writeup:\nOral presentation of final project:\nPaper 4: Final paper:\nClass/lab participation (obligatory attendance):\n10%\nLate policy: 10% off each day late, down to 50% off. Then you can hand it in later for\n50% credit. (If you contact us ahead of time, with reasons for needing to be late, then\nwe may be able to avoid the deductions entirely)\n\nPsycholinguistics Lab topic areas\n- Language above the word level\n- Lexicon, syntax, semantics, pragmatics, discourse, world\nknowledge, working memory constraints\n- Language as communication: Language as rational\ninference\n- The domain specificity / generality of language\n\nClass 1: Overview\n-\nMechanical Turk demonstration in class\n-\nSyllabus / requirements information\n-\nThe usefulness of quantitative methods in linguistics\n-\nLanguage information sources and constraints\n- Lexicon; syntax; world knowledge; pragmatics; prosody;\nworking memory; context\n\nPreliminaries:\nProperties of Human Language\n(1) Discreteness / hierarchical structure\n- Language is made up of little units that combine\nto make bigger units\nUnit\nNumber\nMeaning\nphonemes\n20-40\nnone\nmorphemes\n10,000+\nsingle chunk\nwords\n50,000+\nsimple combinations\nsentences\ninfinite\ncomplete thought\n\nPreliminaries:\nProperties of Human Language\n(2) Productivity - Language is not just a\nmemorized set of sentences\nColorless green ideas sleep furiously.\n*Furiously sleep ideas green colorless.\n-\nThere are rules that govern sentence structure\n\nPreliminaries:\nProperties of Human Language\nWhat kind of rules?\nNot prescriptive rules.\nPrescriptive rules: What an old-fashioned English teacher might tell you\n1.\"Don't say 'ain't' \"\n2.\"Don't end sentences in prepositions\"\nCounterexamples:\n-\nThat is something which I cannot put up with.\n-\n*That is something up with which I cannot put.\n3.\"Don't split infinitives\"\nCounterexamples:\n-\nTo boldly go where no man has gone before.\n-\n? To go boldly where no man has gone before.\n-\n? Boldly to go where no man has gone before.\n\nPreliminaries:\nProperties of Human Language - - -\nDescriptive Rules: Rules obeyed implicitly.\nSentence formation rules:\nS a NP VP\n\"A sentence (S) consists of a noun phrase (NP) and a verb phrase (VP)\"\nVP a V NP\nVP a V that S\nThe Red Sox beat the Yankees.\nMario said that the Red Sox beat the Yankees.\nJill thought that Mario said that the Red Sox beat the Yankees. ...\n(Note: this is a recursive rule: The category S expands to another S\nfurther along)\n\nDifferences between 24.900 (intro to\nlinguistics) and 9.59/24.905 (this class)\n- Methods / evidence:\n- 24.900: within some domains (syntax & semantics): acceptability\njudgments on a few individuals\n- 9.59/24.905: experiments using many items and participants; many\ndifferent dependent measures:\n- Acceptability judgments (how good does this sound?);\n- Accuracy on comprehension questions;\n- Sentence completions;\n- Reaction times:\no Reading times;\no Looking times to visual scenes, given auditory input;\no Lexical decision times\n- Brain imaging:\no Event-related potentials (EEG)\no Magneto-encephalography (MEG)\no Functional MRI\n\nWhen do we need an experiment?-\n- Acceptability ratings (\"How natural does this utterance sound?\")\nØ E.g., compare:\n- \"the cat\"\n- \"cat the\"\n- Do we need an experiment to decide that determiners / articles come\nbefore the noun?\n- How could we convince ourselves without an experiment that this was\nthe case?\n- What if you didn't speak the language? Wouldn't you want some\nquantitative data to provide evidence?\n- In addition, most current theoretical questions depend on more complex\nexamples, where the judgments aren't so clear\n\nBehavioral measures *-\n- Ratings (\"How natural / normal does this utterance sound?\")\nØ E.g., compare:\n-\n\"Mary wondered who bought what.\"\n-\n\"Mary wondered what who bought.\"\n-\n\"Mary wondered what who bought when.\"\n- Careful about the notion of \"grammatical\" vs. \"ungrammatical\": is there\na binary choice? Or is it continuous?- - -\n- Reading times / reaction time\n- Response accuracy to questions about the content of a sentence\nØ All of these measures are noisy, probably because there are many factors\nthat contribute to them, and we are generally only investigating one or two\nØ That's why we do statistical analyses of the behavioral results: If there are\nreliable differences in the measure across materials and participants, then\nthe factor in question may affect the dependent measure\nØ But of course there can always be other confounding factors that we didn't\nconsider: it's difficult to design good experiments\n\nSyntax & Semantics\n- Standard method in the field of syntax:\nØAcceptability judgment method\n- Single-subject / single-item\nØWeaknesses (Schutze, 1996; Cowart, 1997; Wasow & Arnold,\n2005; Ferreira, 2005; Featherston, 2007; Myers, 2009; Gibson &\nFedorenko, 2010, 2011; Gibson, Fedorenko & Piantadosi, 2013)\n- small number of experimental participants (typically 1);\n- small number of experimental stimuli (typically 1);\n- cognitive biases on the part of the researcher and\nparticipants\n\nSyntax & Semantics: Quantitative methods\n- The advantages of quantitative methods\n(controlled experiments or corpus analyses)\nØenable the use of inferential statistics to evaluate the\nlikelihood of particular hypotheses;\nØexperimental participants are naive with respect to the\nhypotheses;\nØexperimental materials are presented in such a way so\nas to avoid context effects (i.e. in a random order,\nvarying orders across participants).\n\nMechanical Turk in class\n6. 45 mins later: download the data from \"Manage\"\n7. Analyze data: edit an R analysis file\n-Check quality of participant work (and if they have done\nprevious surveys with the same name): look at variable\ndata_summ\n-Possibly reject bad participants (to ensure the quality of the\nparticipant pool)\n8. Plot the results from each experiment: look at means /\nvariance / individual data\n9. run statistics (if needed)\n\n9.59J/24.905J Lab in\nPsycholinguistics Spring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/422c034d26f5eac35901b2b680490928_MIT9_59jS17_lec2.pdf",
      "content": "Quantitative methods in\nsyntax & semantics research\nTed Gibson\n9.59 / 24.905\n\nAcceptability judgments\nThe standard method in syntax / semantics research since at least Chomsky (1957):\nThe researcher's own intuitions about the acceptability of different sentences.\nOK: the cat\n* cat the\nExamples from Mahowald et al. (2016), Linguistic Inquiry 2001-2010:\n37.2.Sigurdsson: OK:They would have elected me.\n* There would have been me elected.\n32.1.Martin:\nOK: Pam likes soccer, and Rebecca does too.\n* I consider Pam to like soccer, and I believe Rebecca to as well.\n35.3.Hazout\nOK:There seem to have appeared some new candidates in the course of the campaign.\n* There seems to have appeared some new candidates in the course of the campaign.\n32.3.Culicover\nOK:There was a promise to Susan from John to take care of himself.\n* There was a promise to Susan from John to take care of herself.\n\nAcceptability judgments\nBut what about:\nOK: John was sleeping.\n*? Mary told Bill that Fred said that Arianna believed that John was sleeping.\nOK:The girl ate the pizza.\n*? The pizza ate the girl.\nOK:The girl ate the pizza.\n?* The girl ate the dugong.\nPre-theoretically we think that there is a lexicon, syntax (word order/\ncomposition rules), and the meaning associated with each. Consider making a\ncomparison of sentence a vs sentence b. If we want to argue that this effect shows\nan effect of syntax (word order/composition rules), we need to control for other\nfactors. That is, we need to make sure that the meaning is controlled across the two,\nand the words are the mostly the same, and have the same frequency (familiarity).\n\nNon-quantitative syntax / semantics:\nThe single-subject/single-item method\nThe standard method in syntax / semantics research c. 2010 and before:\nThe researcher's own intuitions about the acceptability of different\nsentences.\nThis worked ok when the field was developing:\ne.g., the big cat vs. *cat big the\nBut as the field progressed, the materials became more complex, and judgments are\nmore subtle\ne.g., What do you wonder who saw? vs. I wonder what who saw.\nFurthermore, a researcher often doesn't natively speak the language that is being\ndocumented: How to evaluate those judgments?\n\nNon-quantitative syntax / semantics:\nThe single-subject/single-item method\nWeaknesses of the single-subject/single-item method\n(e.g., Schutze, 1996; Cowart, 1997;Wasow & Arnold, 2005; Ferreira, 2005; Featherston,\n2007; Myers, 2009; Gibson & Fedorenko, 2010, 2012):\n- Cognitive biases on the part of the researcher and participants\n-The non-quantitative method presupposes that there is some categorical difference\nbetween \"grammatical\" and \"ungrammatical\".What if the difference is continuous, from\ncompletely unacceptable to very acceptable? Using the non-quantitative method, we can't\nfind probabilistic effects or relative effect sizes, or interactions among factors\n- Perhaps the biggest problem: without quantitative methods, if researchers make\nany judgment errors, other researchers can never know which comparisons are ok,\nand which are not.\n- (Note: problems with the experimental design -- confound with lexicon / context\netc -- are problems for all methods)\n\nNon-quantitative syntax / semantics:\nThe single-subject/single-item method\nAdvantages of quantitative methods (controlled\nexperiments or corpus analyses): (from class responses)\n1. The current acceptable error rate in linguistics studies is too high\n2. Informal linguistic experiments (judgments) make it difficult for researchers who either are from\nother fields or do not speak the target language in the materials\n3. Cognitive biases: experiments are conducted using the experimenters' judgement to determine\nwhat is deemed correct or preferred or more grammatical\n4. Only formal experiment can give detailed information on the size of effects in an objective\nmanner\n5. It is not always obvious that a contrast is obvious\n6. All the reasons for adopting quantitative methods in linguistics research presented in this paper\nare convincing: it's hard to choose the strongest one...\n\nNon-quantitative syntax / semantics:\nThe single-subject/single-item method\nAdvantages of quantitative methods (controlled\nexperiments or corpus analyses):\n- allow the use of inferential statistics to evaluate the relative likelihoods of\nalternative hypotheses\n- experimental participants are naive with respect to the hypotheses\n- experimenter has control over the presentation of the experimental materials\n(e.g., can control for context effects by randomizing the order within and across\nparticipants)\n- Language is *not* binary / thresholded. There is a continuum of difficulty.\nPresupposing a binary judgment is a weakness\n- Biggest advantage (?): other researchers have quantitative information\nabout the quality of data: quantitative details enable an understanding of which\ncomparisons support a theory, and which do not / might not\n\nResponse 1 to a plea for quantitative\nmethods in syntax/semantics (G&F 2010)\nUsing quantitative methods would slow down research a great deal:\n\"It would cripple linguistic investigation if it were required that all judgments of\nambiguity and grammaticality be subject to statistically rigorous experiments on\nnaive subjects.\" (Culicover & Jackendoff, 2010, p. 234).\n\nAnswer to Response 1\nNo: Crowd-sourcing (e.g., Amazon.com's Mechanical Turk) makes it easy to\nconduct experiments these days\n- cheap, reliable, fast labor\n- Free software (e.g., Turkolizer, Gibson, Piantadosi & Fedorenko, 2011)\n- An experiment can be completed in a couple of hours\nE.g., Sprouse, Schutze & Almeida (2013) tested 148 pairs of examples\nfrom Linguistic Inquiry (2001-2010), using 3 different methods, in a few\nmonths; and S&A tested every example from Adger's (2003) textbook\nMahowald, Graff, Hartman & Gibson (2016, Language) tested 101 further\nexamples from Linguistic Inquiry (2001-2010), 2 methods, with 12\nexemplars of each, in a few weeks\n\nResponse 2 to a plea for quantitative\nmethods in syntax/semantics (G&F 2010)\nPhillips (2008)\n\"In order for there to be a crisis, however, it would need to be the case that\nintuitive judgments have led to generalizations that are widely accepted yet\nbogus... Carefully controlled judgment studies would solve these problems.\"\nPhillips' claim: There are few enough incorrect judgments in the\nliterature such that adopting quantitative standards wouldn't solve an existing\nproblem.\n(cf.The biggest problem of non-quantitative methods: If researchers\nmake any judgment errors, other researchers can never know which\ncomparisons are ok, and which are not.)\n\nAnswer to Phillips (2008):\nJudgment errors really do occur\nExamples of the kind that Phillips claims do not exist\n(Gibson & Fedorenko, 2010, LCP; Gibson & Fedorenko, 2012, LCP):\n(a) What do you wonder who saw?\n(b) *I wonder what who saw.\nChomsky (1986, p. 48):\n(a) is more acceptable than sentences that violate the Superiority\ncondition like (b), due to a process of \"vacuous movement\"\nBUT: (a) is actually judged as less acceptable than (b) (Gibson &\nFedorenko, 2012).\n\nAnswer to Phillips (2008):\nJudgment errors really do occur\nFurthermore, Sprouse, Schutze & Almeida (2013) find that approximately\n5% of 146 Linguistic Inquiry contrasts from 2001-2010 were not ratified,\nwith 1-2% reliable in the opposite direction\nThus judgment errors really do occur.\n\nFollowing up Sprouse et al.:\nMahowald, Graff, Hartman & Gibson (2016)\nMahowald et al. (2016): 100 randomly sampled comparisons from SSA's Linguistic\nInquiry set, ones that SSA did not run. (12 items / comparison; 60 participants /\nexpt; 2 methods: forced choice; ratings)\nResults: 11% of judgments do not show a sig. result in at least one of the two\nmethods; 5% do not show a sig. expected result in both methods. In the forced\nchoice experiment, 2 judgments are sig. in the opposite direction.\nExamples:\nFox (2002):\n*I read something yesterday John recommended. vs. I read something yesterday John did.\nHazout (2004):\nThere seem / *seems to have appeared [some new candidates] in the course of the presidential campaign.\nLasnik (2003):\n?The detective asserted two students to have been at the demonstration during each other's hearings. vs.\n?*The detective asserted that two students were at the demonstration during each other's hearings.\nNunes (2001):\nWe proved Smith to the authorities to be the thief. vs. *We proved to the authorities Smith to be the thief.\n\nFollowing up Sprouse et al.:\nMahowald, Graff, Hartman & Gibson (2016)\nMethod: acceptability\njudgement 1-7, z-scored\nwithin individuals. obtain\nmean z-score for each item in\neach contrast, and averaged\nthese to give an overall z-\nscore for the 'acceptable'\nsentence and for the\n'unacceptable' sentence in\neach contrast.\nThe effect size is the\ndifference between these two\nz-scores.\n(Effect size: Cohen's d is a\nmeasure of effect size that is\nequal to the difference in means\nbetween the two conditions, in z-\nscores.)\nz-score:\n(value\nmean)/\nsd\n(c) Linguistic Society\nof America. All rights\nreserved. This\ncontent is excluded\nfrom our Creative\nCommons license.\nFor more\ninformation, see\nhttp://ocw.mit.edu/\nhelp/faq-fair-use/\nSource: Mahowald,\nKyle, Peter Graff,\nJeremy Hartman, and\nEdward Gibson.\n\"SNAP judgments: A\nsmall N acceptability\nparadigm (SNAP) for\nlinguistic\nacceptability\njudgments.\"\nLanguage 92, no. 3\n(2016): 619-635.\n\nResponse 3 to a plea for quantitative\nmethods in syntax/semantics (G&F 2010)\nSprouse, Schutze & Almeida (2013): Judgment errors are too rare to\nmatter.\nA 5% error rate is the acceptable standard in cognitive psychology\nexperiments. Therefore, this should also be acceptable in linguistics\njudgments.\n\nAnswer to Sprouse & Almeida (2012):\n5% errors is too many errors\n(1) 5% is actually no longer standardly acceptable in psychology\nexperiments: many failures to replicate (e.g., Nosek et al., the Open\nScience Collaboration, 2015)\nAbandon quantitative methods? No!\nQuantitative objective replication is critical. The error rate can then be\nmade arbitrarily small, with more data (e.g., p < .00001 or smaller, for real\neffects).\n\nAnswer to Sprouse & Almeida (2012):\n5% errors is too many errors\n(2) A p < 0.05 false-positive threshold for null hypothesis significance testing\n(NHST) in behavioral experiments is not comparable to a 5% false-positive rate\nin published acceptability judgments.\nThe NHST paradigm assumes that one has performed statistical significance testing for each effect;\nthe p < 0.05 threshold is an easy way to classify the results, but it does not substitute for the\nquantitative information.\nFurthermore a 5% error rate in linguistic acceptability judgments suggests that 5% of all judgments\nwould diverge from the results of a formal experiment. But there is no sampling being done; the\nmethod provides no quantitative information about any individual effect.\nIf the average linguistics paper has thirty-three examples, divergences are uniformly distributed, and if\nthe divergence rate is 5%, then every paper is likely to contain ~1-2 questionable judgments.\nSo perhaps the biggest problem with non-quantitative methods: if researchers make any judgment\nerrors, other researchers have no information about which comparisons are ok, and which are\nnot.\n\nJudgments in other languages:\nLinzen & Oseki (2015)\nMost readers speak / read English, so the judgment rate is likely to be better than\nfor languages for which most readers don't speak.\nEvaluation: Linzen & Oseki (2015): Hebrew & Japanese\nSelected 4 \"obvious\" control comparisons and 14 comparisons which they were\nless sure of in each language.\nResults:\n5 of 14 were reliably different in predicted direction in Hebrew;\n7 of 14 were reliably different in predicted direction in Japanese\nOverall, only 12 of 28 (~40%) were ratified\n\nJudgments in other languages:\nLinzen & Oseki (2015)\nHebrew\nJapanese\nCourtesy of Tal Linzen and Yohei\nOseki. Used with permission.\nSource: Linzen, Tal, and Yohei\nOseki. \"The reliability of\nacceptability judgments across\nlanguages.\" New York: New York\nUniversity, ms (2015).\n\nSummary:\nReasons to do quantitative research\n- In non-quantitative work, because there are some judgment errors,\nother researchers can never know which comparisons are ok, and which\nare not. Even if we don't require arbitrarily low error rates, the details of\na quantitative experiment provide some evidence about how strongly to\nbelieve the effect. Objectivity.\n- Intuitions are not reliable for interactions among factors.\n- Difficult / impossible to maintain consistency of judgments across\nmany pairs of judgments.\n- Can learn about effect sizes, which can often be used to determine if\nsome factor is theoretically important.\n\nPossible project\n-There is evidence that the lexical decision task is affected by the context\nin important ways: instructions etc.\n-For sentences, how does the context affect the judgments?\n-If the distractor materials vary, does this affect the judgments in an\ninteresting way?\nAlready known: no major differences among Likert scale judgments vs.\nmagnitude estimation judgments\n(or even simple forced choice: effectively a 2-point scale)\nas long at there are lots of items\n\nNon-quantitative syntax / semantics:\nThe single-subject/single-item method\nDisadvantages of quantitative methods (controlled\nexperiments or corpus analyses): (from class responses)\n1. Quantitative methods/experiments require either access to mechanical turk or funding.This can\nhinder researchers outside of the US, or researchers anywhere without financial backing.\n2. What matters is the effect size, not just statistical significance\n3. Unnecessary a lot of the time: the experimenters' intuition towards which tendencies will be\npreferred are correct.\n4. More difficult to do an experiment: creating materials + money\n5. Data from quantitative methods would not account for a researcher's failure to address\nexceptions to syntactical generalizations that stem from situational or wording-related factors\n\nFurther qs\n1. The tested sentences in Mahowald et al were originally designed to show the same contrast as\nthe original paired-data from Linguistic Inquiry.. How were the pairs of sentences constructed by\nstudents in a class?\n2. Under many circumstances, linguists provide more than one pairs of sentences with different\ncontrasts (independent evidence of distinct types) to support one step of their reasoning. In such\ncases, one pair of unreliable language data does not necessarily impair their reasoning.To consider\nthis factor, it might be useful to categorize the linguistic data extracted from Linguistic Inquiry into\nseveral subcategories. Several pairs of language data which are contributed to the same argument\nmight be marked as 'parallel data' which are separated from other language data which is the sole\nevidence for a certain argument.\n3. Bayesian statistics? Confidence intervals for the SNAP judgments? Not for now...\n4. With the SNAP judgements, I was a little bit unsure whether each decision of the 5 would have\nthe same two tendencies and the test was if 5 people chose the same tendency over the other\nfive times out of five.\n5. potential differences between the magnitude-estimation and the rating study\n6. What qualitative-research approaches are commonly employed in linguistics research?\n7. How does this relate to other forms of linguistics research? Is the SNAP judgment paradigm only\napplicable to experiments investigating sentence acceptability?\n\n9.59J/24.905J Lab in\nPsycholinguistics Spring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/b49e9bb43484e5b5664ac49d9f7dc7fb_MIT9_59jS17_lec3.pdf",
      "content": "Language as communication\nTed Gibson\n9.59J/24.905J\n\nOverview\n- Language information sources and constraints\nLexicon; syntax; world knowledge; working memory;\ncontext; pragmatics; prosody\n- Language as communication\n- Ambiguity?\n- Words\n- Sentences\n- Communication-based models of language\nevolution and processing\n\nLanguage information sources and\nconstraints\n- Lexicon\n- Syntax\n- World knowledge\n- Context\n- Working memory\n- Pragmatics\n- Prosody\n\nLanguage: \nInformation sources and constraints\n-\nLexical (Word) information, e.g., frequency\nUnambiguous words: more frequent, faster access:\"class\" vs.\"caste\"\n-\nAmbiguity: more frequent usages are preferred\n# The old man the boats.\n-\nSyntactic argument structure frequencies\n# I put the candy on the table into my mouth.\nThe verb \"put\" prefers to have a locative goal prepositional phrase\n(like \"on ...\")\nThe noun \"candy\" has no bias to have a locative prepositional phrase\n\nThe existence of \"garden-path\" effects \nprovides evidence:\n- That the relevant information factor(s) play a role in human\nlanguage processing (e.g., lexical frequency, syntactic phrase\nstructure frequency, etc.)\nAnd more generally:\nThat language is processed on-line, as it is heard or read\nThat the human parser is not unlimited parallel. Rather, it must\nbe ranked parallel or serial.\n\nLanguage: \nInformation sources and constraints\nSyntax / word order / sentence structure: giving rise to the literal\npredicate-argument meaning of a phrase / sentence\nThe cat is watching the mouse.\n?? mouse cat the is the watching.\nCompositional rules: meaning of the larger phrase is formed from the\nmeaning of the parts: NP → Det Noun; S → NP VP; VP → Verb NP\nThe syntax of a language makes some interpretations available:\nThe dog bit the boy. vs.\nThe boy bit the dog.\nAmbiguity: multiple syntactic interpretations\nThe boy saw the man with the telescope.\n\nLanguage: \nInformation sources and constraints\nSyntax / word order / sentence structure, giving rise to the literal predicate-\nargument meaning of a phrase / sentence\nThe rules corresponding to assigning the meaning of a phrase like \"The dog with\nthe white fur\" are context-independent: (so-called \"context-free\" rules)\nSubject position of sentence (the noun phrase to the left of verb):\nThe dog with the white fur chased the black squirrel into the home of the grey cat.\nDirect object position of sentence (first noun phrase to the right of verb):\nThe grey cat chased the dog with the white fur into the home of the black squirrel.\nDirect object position of a preposition (first noun phrase to the right of a\npreposition):\nThe grey cat chased the black squirrel into the home of the dog with the white fur.\n\nLanguage: \nInformation sources and constraints\nMore frequent phrase rules, easier processing (Jurafsky, 1996; Hale, 2001; Levy,\n2008):\nAmbiguity\nThe defendant examined ...\nS → NP VP\nvs.\nNP → NP RC\n√The defendant examined ... the evidence.\n?? The defendant examined ... by the lawyer turned out to be unreliable.\nUnambiguous syntax\nJohn was smoking.\n? That John was smoking bothered me.\n?? John's face needs washed.\n\nLanguage: \nInformation sources and constraints\nWorld knowledge\nUnambiguous examples:\nThe dog bit the boy. vs. The boy bit the dog.\nAmbiguity: (Trueswell,Tanenhaus & Garnsey, 1994)\nThe defendant examined by the lawyer turned out to be unreliable.\nThe evidence examined by the lawyer turned out to be unreliable.\nMethods: (1) Eye-tracking during reading; (2) Self-paced reading\n\nReading time studies\n-\nCompare target to its control:\nTemporary ambiguity:\nThe defendant examined by the lawyer turned out to be unreliable.\nUnambiguous control:\nThe defendant that was examined by the lawyer turned out to be unreliable.\nTarget regions:\"examined\",\"by the lawyer\"\n\nInformation sources and constraints:\nModularity / Information-\nTwo kinds of questions:\nWHAT are the information sources that people are sensitive to? (And how are\nthey organized in the brain: we don't know this well yet)\nWHEN are information constraints applied?\nFodor (1983) proposed \"modularity\" / \"information-encapsulation\" of words and\nsyntax\nOne concrete idea: people compute the literal meanings of compositional\nlanguage first, and then make inferences about what might have been meant\nNon-literal language: inferences about the intended meaning: PRAGMATICS\nSome of the students passed the test. → Not all the students passed the test.\nJOHN went to the store. → Only John went to the store.\nCan you please pass the salt? → Pass the salt.\nI am cold. (next to an open window): → Close the window.\n\nInformation sources and constraints:\nModularity / Information-\nWHEN are information constraints applied?\nFodor (1983) proposed \"modularity\" / \"information-encapsulation\" of words and\nsyntax\nAnother idea: people use syntactic disambiguation rules to decide\namong choices, independent of their meaning: choose simplest syntactic\nchoice, independent of meaning. E.g., most frequent syntax\nThus the choice between Main-Verb or Relative Clause structure of \"the\ndefendant / evidence examined\" would not depend on the meanings\nThus people should favor the simpler structure, independent of meaning.\nThis is what Ferreira & Clifton (1986) found for \"the evidence examined\" case.\nBut there were serious confounds in their materials, which undermined their\ninterpretation\n\nLanguage: \nInformation sources and constraints\nCurrent Context (Crain & Steedman, 1985;Altmann & Steedman, 1988;\nTanenhaus et al., 1995): visual or linguistic\nAmbiguity:\nThere were two defendants, one of whom the lawyer ignored entirely, and the\nother of whom the lawyer interrogated for two hours.\nThe defendant examined by the lawyer turned out to be unreliable.\n\nMonitoring visual eye-movements while listening to spoken instructions\n(Tanenhaus et al., 1995;Trueswell et al., 1999) \n1-referent context:\"Put the hippo on the towel in the basket.\" \nMany looks to the incorrect target\n\nMonitoring visual eye-movements while listening to spoken instructions\n(Tanenhaus et al., 1995;Trueswell et al., 1999) \n2-referent context:\"Put the bear on the plate into the box.\" \nNo looks to the incorrect target\n\nLanguage: \nInformation sources and constraints\nWorking memory: Longer distance dependencies are harder to process than more local ones\nDependencies between a verb and its post-verbal objects:\nShort NP object:\nLocal Particle: Joe threw out the documents.\nNon-local Particle: Joe threw the documents out.\nLong NP object:\nLocal Particle: Joe threw out the very important documents that he brought home.\nNon-local Particle: Joe threw the very important documents that he brought home out.\n\nInformation processing: Working memory\nWorking memory: Local connections are easier to make than long-distance\nones (Gibson, 1998, 2000; Grodner & Gibson, 2005;Warren & Gibson, 2002;\nLewis & Vashishth, 2005; Hawkins, 1994)\nAmbiguous attachments:\nThe bartender told the detective that the suspect left the country yesterday.\nyesterday is preferred as modifying left rather than told\n(Frazier & Rayner, 1982; Gibson et al., 1996;Altmann et al., 1998; Pearlmutter & Gibson, 2001)\nUnambiguous connections:\nThe reporter wrote an article.\nThe reporter from the newspaper wrote an article.\nThe reporter who was from the newspaper wrote an article.\n\nRetrieval / Integration-based theories\nIntegration: connecting the current word into the structure built\nthus far: Local integrations are easier than longer-distance integrations\n- The Dependency Locality Theory (DLT) (Gibson, 1998; 2000):\nintervening discourse referents cause retrieval difficulty (also in\nproduction)\n- Activation-based memory theory: similarity-based interference\n(Lewis & Vasishth, 2005;Vasishth & Lewis, 2006; Lewis,Vasishth\n& Van Dyke, 2006): intervening similar elements cause retrieval\ndifficulty\n- Production: Hawkins (1994; 2004): word-based distance metric.\n\nDependency Length Minimization\nFutrell, Mahowald & Gibson, 2015, PNAS\n- Corpora from 37 languages parsed into dependencies, from\nNLP sources: the HamleDT and UDT; cf.WALS (Dryer\n2013)\n- Family / Region\nIndo-European (IE)/West-Germanic; IE/North-Germanic; IE/\nRomance; IE/Greek; IE/West Slavic; IE/South Slavic; IE/East\nSlavic; IE/Iranian; IE/Indic; Finno-Ugric/Finnic; Finno-Ugric/Ugric;\nTurkic; West Semitic; Dravidian; Austronesian; East Asian\nIsolate (2); Other Isolate (1)\n- Result:All languages minimize dependency distances (c.f.\nHawkins, 1994; Gibson, 1998)\n\nthe girl kicks the ball\nthe girl the ball kicks\nthe ball the girl kicks\ngirl the kicks the ball\nball the girl the kicks\nFutrell, Mahowald, & Gibson, 2015, PNAS\n\nDependency Length Minimization\nFutrell, Mahowald & Gibson, 2015, PNAS\nCourtesy of National\nAcademy of Sciences, U. S.\nA. Used with permission.\nSource: Futrell, Richard,\nKyle Mahowald, and\nEdward Gibson. \"Large-\nscale evidence of\ndependency length\nminimization in 37\nlanguages.\" Proceedings of\nthe National Academy of\nSciences 112, no. 33\n(2015): 10336-10341.\nCopyright (c) 2015 National\nAcademy of Sciences,\nU.S.A.\n\nPotential project\nResult to replicate: Subject-extractions in Relative clauses (RCs) are easier to process than object-\nextractions:\nSubj-RC: The reporter who attacked the senator admitted the error.\nObj-RC: The reporter who the senator attacked admitted the error.\nRTs faster at \"attacked\" in SRC than in ORC\nTwo explanations: ORCs are rare, and longer-distance\nExtension: evaluation other kinds of extraction in English:\nDative extractions: infrequent, long-distance\nThe boy who the girl gave the book to admitted the error.\nThe boy to whom the girl gave the book admitted the error.\nGenitive extractions: infrequent, short-distance \nThe girl whose friend invited the kids to the party was kind.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/c50ae829cf6d02ad14c4a50b05dd467d_MIT9_59jS17_lec4.pdf",
      "content": "Language as communication 2\nTed Gibson\n9.59\n\nLanguage as communication\n- Information theory\n- Words\n- Sentences\n- Communication-based models of language\nevolution and processing\n\nOptimally designing a language\nWhat features of a language might make it\noptimal?\nWhat do we mean by optimal?\n- optimal for use?\n- optimal for comprehension?\n- optimal for production?\n- optimal for acquisition?\n\nWords: Optimized for Communication?\nDesigning a language: Ithkuil\n(thanks to Kyle Mahowald)\nFoer, from the NewYorker, 2012:\nLanguages are something of a mess.They evolve over centuries\nthrough an unplanned, democratic process that leaves them teeming\nwith irregularities, quirks, and words like \"knight.\" No one who set out\nto design a form of communication would ever end up with anything\nlike English, Mandarin, or any of the more than six thousand\nPortrait of John Quijada removed due\nlanguages spoken today.\nto copyright restrictions.\nHence: Ithkuil, developed by John Quijada, a 53-year-old\nformer employee of the California State Department of Motor\nJohn Quijada\nVehicles\nGoals of Ithkuil:\n- no ambiguity\n- concision of expression\n- broad coverage of ideas\n\nWords: Optimized for Communication?\nDesigning a language: Ithkuil\n(thanks to Kyle Mahowald)\nfrom Wikipedia\nIthkuil words can be divided into just two parts of speech, formatives and adjuncts.\nFormatives can function both as nouns and as verbs, depending on the morpho\nsemantic context.[8] Both nominal and verbal formatives are inflected to one of the\npossible 3 stems, 3 patterns, 2 designations (formal or informal), 9 configurations, 4\naffiliations, 4 perspectives, 6 extensions, 4 contexts, 2 essences, and 96 cases; formatives\nalso can take on some of the 153 affixes, which are further qualified into one of 9\ndegrees.Verbal formatives are additionally inflected for 7 illocutions and 7 conflations.\nVerbal adjuncts work in conjunction with adjacent formatives to provide additional\ngrammatical information.[9]Verbal adjuncts are inflected to indicate 14 valencies, 6\nversions, 8 formats, 37 derivations, 30 modalities, 4 levels, 14 validations, 9 phases, 9\nsanctions, 32 aspects, 8 moods, and 24 biases.\n\nIthkuil would not be mistaken for a\nnatural language\nFoer: Ideas that could be expressed\nonly as a clunky circumlocution in\nEnglish can be collapsed into a single\nword in Ithkuil.A sentence like \"On the\nPortrait of John Quijada removed due\ncontrary, I think it may turn out that\nto copyright restrictions.\nthis rugged mountain range trails off\nat some point\" becomes simply \"Tram\nmloi hhasmarptuktox.\"\nJohn Quijada\n\nWords: Optimized for Communication?\nDesigning a language: Ithkuil\n(thanks to Kyle Mahowald)\n- Ithkuil is not like a human language. (how so?)\n- What is the baseline? Should we expect a\nlanguage to look like Ithkuil?\n- Important idea from the NewYorker article:\nHow should a language be designed for optimal\ncommunication?\n\nInformation theory\n- Claude Shannon:\nA Mathematical Theory of\nCommunication (1948)\nInformation theory / communication:\n(1) Minimize code length;\n(2) Noisy channel, so we need extra bits of\ninformation for robustness, especially for low\nfrequency events \nPortrait of Claude Shannon removed\ndue to copyright restrictions.\n\nDiverging paths\nPortrait of Claude Shannon removed\nPortrait of Noam Chomsky removed\ndue to copyright restrictions.\ndue to copyright restrictions.\nThe fundamental problem of communication\nis that of reproducing at one point either\nexactly or approximately a message selected\nat another point. ...The significant aspect is\nthat the actual message is one selected from a\nset of possible messages.The system must be\ndesigned to operate for each possible\nselection, not just the one which will actually\nbe chosen since this is unknown at the time\nof design.\nBut it must be recognized that the notion of\n\"probability of a sentence\" is an entirely useless\none, under any known interpretation of this\nterm.\n\nInformation\n- The information of an event relates to the probability that the\nevent occurs\n- The more surprised you are by the event, the greater its\nsurprisal: the more information in it\n- An event with 0 information is already known (P = 1)\n- An event that is infinitely unknowable should be infinitely\ninformative (P = 0)\n- Units of information: bits = coin flips = -log2(P(event)) =\nsurprisal of event\n\nGuess a word\n- Suppose that there are 10,000 words in the lexicon\n- -log2(10,000) = 13.3\n- 13.3 bits of information\n- The optimal number of Yes-no questions that you might\nneed to guess this word.\n- \"Is it pizza?\" is not a good question: how many bits in\nthe answer to that q?\n- -log2(10,000) - -log2(9999) = .0001 bits\nGoal: 13.3 bits\n\nIt's a noun\n- 5,000 nouns\n- How much information did we gain?\n- 1 bit\nCurrent: 1 bit\nGoal: 13.3 bits\n\nIt's an animal\n- There are 200 animals. How much\ninformation did we gain?\n- log2(5000/200) = log2(25) = 4.64 bits\n- Total: 5.64 bits\nCurrent: 5.64 bit Goal: 13.3 bits\n\nWhat if it were a planet?\n- There are 8 planets. How much\ninformation would we have gained?\n- log2(5000/8) = 9.3 bits\n- Or: 9.1 bits (Pluto)\nCurrent: 10.3 bit Goal: 13.3 bits\n\nBut it's really an animal\n- Total information thus far: 5.64 bits\n- Needed: 13.3\nCurrent: 5.64 bit Goal: 13.3 bits\n\nBut it's really an animal\n- 200 animals\n- It starts with a 'b'\n- 20 out of 200 start with b\n(c) Source Unknown. All rights reserved. This\ncontent is excluded from our Creative\nCommons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/\n- 3.3 bits from \"starts with b\"\n- Total bits: 8.94\nCurrent: 8.94 bit Goal: 13.3 bits\n\nWhat if it starts with z?\n- 200 animals\n- It starts with a 'z'\n- 1 out of 200 start with z\n- 7.64 bits from \"starts with z\"\n- Total bits: 13.3: uniquely identified\nGoal: 13.3 bits\n\nMackay 2003\nBrainteaser\n- You are given 12 balls and a scale. Of the\n12 balls, 11 are identical and 1 weighs\neither slightly more or slightly less. How\ndo you find the special ball (and whether\nit is heavier or lighter) using the scale\nonly three times?\n- The scale can only tell you which side is\nheavier.\n\nMackay 2003 Brainteaser\n- How many bits of information do you need to get?\n- 12 balls, each with 2 possibilities (normal, special) = 24\npossibilities, giving -log2(24) = 4.58 bits\n- How much information can you get (at most) from each\nweighing?\n- You get three possible answers: left heavier, right heavier,\nsame = -log2(3) = 1.58 bits\n- If you can divide the groups of balls into smaller groups of 3\nwith each weighing, you might be able to get the needed\ninformation after 3 weighings\n\nMackay 2003\n1 bit\n1.58 bits\n\nMackay 2003\n(c) Cambridge University Press. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/ Source: figure\n4.2 from MacKay, David JC. Information theory, inference and learning algorithms. Cambridge university press, 2003.\n\nCoding\nSuppose I want to transmit information: communicate\nI need a code\nSimplified code: just parts of speech (POS):\n00 - NOUN\n01 -VERB\n10 - ADJECTIVE\n11 - OTHER\nThe ugly man ran quickly to the rhinoceros\n01 11\n11 11 00\n[16 bits]\n\nCoding\nThe ugly man ran quickly to the rhinoceros\n01 11\n11 11 00\n[16 bits]\n00 - NOUN (2 times)\n01 -VERB (1 time)\n10 - ADJECTIVE (1 time)\n11 - OTHER (4 times)\nDifferent POS tags occur with different frequencies / probabilities.\nWe can therefore use this to build a more efficient code: to\nminimize the expected code length (optimize efficiency)\n\nCoding\nThe ugly man ran quickly to the rhinoceros\n001 01\n000 1\n[14 bits]\n1 - OTHER (4/8)\n01 - NOUN (2/8)\n000 -VERB (1/8)\n001 - ADJECTIVE (1/8)\n\nInformation content\nSuppose we have a distribution P on events (words, part of\nspeech tags, weather conditions, notes in a song, etc.)\n●The amount of information it takes to specify which event\noccurred is the average number of bits the best code must send\nThe ugly man ran quickly to the rhinoceros\n001 01\n000 1\n[14 bits]\n\nResult from Information theory (Shannon, 1948)\nThe best code will assign an event of probability p a code word of length\n-log(p) (roughly, in the limit): the surprisal of that event\nLikely events have low surprisal: few bits of information\nUnlikely events have high surprisal: many bits of information:\nthe depth of the binary tree is the negative log probability\n\nEntropy\n- -Log probability (surprisal) - measures of the amount of information\nit takes to specify that a specific event occurred (measured on events)\n- Entropy - measures the average number of bits it takes to specify which\nevent will occur (measured on distributions)\n\nUniform distribution\n-\nA uniform distribution maximizes entropy\ncalc.entropy <- function(x) {return (sum(-x*log2(x)))}\n-\n[.97, .01, .01, .01] : Entropy =\n-(.03 * log2(.01) + .97 * log2(.97)) = 0.24 bits\n-\n[.25, .25, .25, .25] : Entropy =\n-4 * (1/4 * log2(.25)) = 2 bits\n\nDistributions\n- ABABABABCABABACABABA\n- ABABCACBACCBABCBCBCA\n- eiqtyp2q3450761Q[WR8Y[82Qdsiew92\n\nConditional surprisal\nWe are typically are in situations where events are not\nindependent:\n●The ...\n●The silly...\n●The silly grasshopper...\n●The silly grasshopper wanted to find his friend the ...\n\nConditional surprisal\n\nEfficient communication\n- Hypothesis: Natural language\nis a largely efficient code\n- Concise while still being\nrobust to noise\n- Longer words are more\nrobust to noise\nredundant code\n\nLanguage / Communication:Words\nPiantadosi,Tily & Gibson (2011)\nZipf (1949): more frequent words are shorter:\n-\n\"Principle of least effort\"\nExtension: more predictable words should be shorter.\n-\ne.g., to maintain Uniform Information Density\n(Aylett & Turk, 2004; Jaeger, 2006; Levy & Jaeger,\n2007)\n-\nEstimate of predictability: n-grams (3-grams) over\nlarge corpora\n\nphrase\ncount\nfreq\ninformation\nin last word\n(bits)\n\"to be or\nnot to be\"\n86/87\n0.99\n-log2(86/87)\n= 0.01\n\"to be or\nnot to bop\"\n1/87\n0.01\n-log2(1/87) =\n6.44\nfrom corpus of\ncontemporary American\nEnglish (COCA)\n\nAverage information\n- average information\nof a word w over all\ncontexts in which it\nappears\nif nothing else in\nvocabulary:\nphrase\nword\naverage\nsurprisal\n\"to be or not to\nbe\"\nbe\n0.01\n\"to be or not to\nbop\"\nbop\n6.44\n\nLanguage for communication:Words\nPiantadosi,Tily & Gibson (2011)\nCourtesy of National Academy of Sciences, U. S. A. Used with permission.\nSource: Piantadosi, Steven T., Harry Tily, and Edward Gibson. \"Word lengths\nare optimized for efficient communication.\" Proceedings of the National\nMore predictable words are shorter!\nAcademy of Sciences 108, no. 9 (2011): 3526-3529.\nCopyright (c) 2011 National Academy of Sciences, U.S.A.\n\nHow does the effect arise?\n- Is it just differences among broad classes of words\nlike content vs. function words? Or within class too?\n- How does the effect come about in the lexicon?\nLong-term evolution?\n- look at long/short pairs (chimpanzee\nchimp),which\ndiffer in length but are controlled for meaning\n\nInfo/Information theory\nUsing Google trigrams, we\nlooked at average surprisal\nfor long forms vs. short\nforms.\nMean surprisal for long\nforms (9.21) is significantly\nhigher than mean surprisal\nfor short forms (6.90) (P = .\n004 by Wilcoxon signed rank\ntest)\nLinear regression shows\nsignificant effect of log\nfrequency on surprisal (t =\n2.76, P = .01) even when\ncontrolling for frequency.\nCorpus Results\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n-10\n-5\nMean difference in surprisal (long form - short form)\nbike\ncarbs\nchemo\nchimp\nroach\ndorm\nexam\nfrat\nhippo\nkilo\nlimo\nmath\nmemo\nphoto\nporn\nref\nfridge\nrhino\nsax\nphone\ntv\nundergrad\nLog combined corpus count (short + long)\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with permission. Source: Mahowald, Kyle, Evelina\nFedorenko, Steven T. Piantadosi, and Edward Gibson. \"Info/information theory: Speakers choose shorter words in\npredictive contexts.\" Cognition 126, no. 2 (2013): 313-318.\nMahowald, Fedorenko, Piantadosi and Gibson (Cognition 2013)\n\nForced-choice sentence completion\nin supportive and neutral contexts:\nsupportive-context: Bob was\nvery bad at algebra, so he hated...\n1. math\n2. mathematics\nneutral-context: Bob introduced\nhimself to me as someone who\nloved...\n1. math\n2. mathematics\nShort form is chosen 67% of the time\nin supportive-context sentences vs.\njust 56% of the time in neutral-\ncontext sentences.\nSignificant by maximal mixed effect\nlogistic regression with both item and\nparticipant slopes and intercepts (β =\n.67, z = 2.59, P < .01).\nBehavioral Results\nDifference in proportion choosing long\n-1.0\n-0.5\n0.0\n0.5\n1.0\nvet\n●\n●\nmic\nporn\nsax\n●\n●\n●\nfridge\n●\nLab\n●\nmayo\nphone\nquads\n●\nA/C\n●\n●\n●\n●\n●\n●\nburger\nchemo\nchimp\nCoke\ndorm\nER\n●\ngas\n●\nlab\n●\n●\nmath\nmemo\n●\n●\n●\n●\nphoto\nquad\n●\n●\n●\n●\n●\nref\nrhino\nroach\nshake\nTV\n●\nundergrad\n●\nad\n●\n●\nbike\ncarbs\n●\nexam\n● frat\n●\n●\nhippo\nID\n● kilo\n●\nlimo\n●\nUK\n●\nUN\n●\nUS\nWords\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with permission. Source: Mahowald, Kyle, Evelina\nFedorenko, Steven T. Piantadosi, and Edward Gibson. \"Info/information theory: Speakers choose shorter words in\npredictive contexts.\" Cognition 126, no. 2 (2013): 313-318.\nMahowald, Fedorenko, Piantadosi and Gibson (Cognition 2013)\n\nExamples from Mahowald et al. behavioral study\nsupportive: For commuting to work, John got a 10-speed...\nneutral: Last week John finally bought himself a new...\nbicycle / bike\nsupportive: Henry stayed up all night studying for his...\nneutral: Henry was stressed because he had a major...\nexamination / exam\nsupportive: Jason moved off campus because he was tired of living in a...\nneutral:After leaving Dan's office, Jason did not want to go to the...\ndormitory / dorm\n\nAudience design?\nClark (1996):Yes, for word choices\nAsking for directions: Speakers use words\nthat are appropriate to listeners background\nknowledge\n\nAudience design?\nFerreira & Dell (2000): Exploring syntactic optionality in sentence\nproduction\nMethod: produce memorized sentences, either for a listener or not.\nMaterials contained optional \"that\"\nNo ambiguity:\nMatch:\nI knew (that) I had ...\nNo match: You knew (that) I had ...\nAmbiguity:\nNo match: I knew (that) you had ...\nMatch:\nYou knew (that) you had ...\n\nAudience design?\nCourtesy of Elsevier, Inc., http://\nwww.sciencedirect.com. Used with\npermission. Source: Ferreira, Victor\nS., and Gary S. Dell. \"Effect of\nambiguity and lexical availability on\nsyntactic and lexical production.\"\nCognitive psychology 40, no. 4\n(2000): 296-340.\nThe left bar in each pair is \"I\" in the main clause; the right is \"you\".\ne.g.,\"I / you knew that I / you had missed practice\"\n\nWhat do you want to know more about?\nI'm curious about the algebra adopted in the corpus study to calculate surprisal of each word.\nI would like to know more concretely how the surprisal of each word was estimated in the corpus study. I would appreciate it very\nmuch if you could give a couple of actual examples in the class.\nI would like elaboration on the details of the information theory involved in this and especially in other similar experiments.\nI am curious to learn more on what Shannon information content/theory.\nDoes this pattern of information-theoretical optimization hold true for other languages as well as English? Is there any data on how\nlanguage evolved through time to become more efficient at communicating information?\nI want to learn more about the robustness of the negative log-probability surprisal measure and about how non-lexical, non-syntactic,\ncontext-based information interacts with this word-length, information correlation.\nCan we talk more about the statistics behind how you perform a corpus study? How exactly does an n-gram model allow you to\ngenerate probabilities for not just word predictions, but also for how contextual a sentence is?\nI would be interested in seeing other examples of questions from the behavioral study.\nI'm interested in the difference between supportive contexts and neutral contexts; more broadly, I'd love a continued exploration of\ndifferent areas of syntax.\n1. what's the relationship of linguistic theories and information theory?\n2. As is mentioned in the last part of the paper, this research might help account for the language change where words become\nshorter. I was wondering how we understand/explain the condition where the surprisal and word length increases.\nThe results of this paper are convincing, but I'm also curious about the selection of synonyms of different lengths in neutral context\nand very unsupportive context. Following the line of this research, we might predict that words of long length would be preferred in\nvery unsupportive context.\nI'm interested in learning more about other factors that cause people to chose longer words\nI would like to discuss more about the difference between written and verbal communications because there may also be differences\nin word length between these two methods of communication. Additionally, I want to know what kinds of follow up studies would be/\nhave been conducted as a result of this study; what has it led to?\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/dc0093a74067131c01dcc6bda30db2a4_MIT9_59jS17_lec5.pdf",
      "content": "Class 5:\nLanguage processing over a\nnoisy channel\nTed Gibson\n9.59J/24.905J\n\nReview from last time: Mahowald et al. 2013\nWords with a long/ short form (e.g., math, mathematics) are preferred as short in a\nsupportive context.\nQuestion: is this because of audience design: the desire to help out our\nconversation participants?\nAnswer: it's impossible to tell based purely on corpus data.\nEvidence for audience design:\nClark (1996): Asking for directions: Speakers use words that are appropriate to\nlisteners background knowledge. But maybe this difference reflects different lexical\nknowledge?\nWilkes-Gibbs & Clark (1992): participants coordinating over names for objects of\n\"tangrams\", moving them around. Directors who talk to one listener first and then\nhave to describe the same tangrams to a naive listener use more words with the\nnaive person. (Probably not an effect of lexicon.)\n\nLanguage for communication?\n-\nMore controversial than some might think...\n\"The natural approach has always been: Is it well designed\nfor use, understood typically as use for communication? I\nthink that's the wrong question. The use of language for\ncommunication might turn out to be a kind of\nepiphenomenon. ... If you want to make sure that we never\nmisunderstand one another, for that purpose language is\nnot well designed, because you have such properties as\nambiguity. If we want to have the property that the things\nthat we usually would like to say come out short and simple,\nwell, it probably doesn't have that property.\" (Chomsky,\n2002, p. 107)\nPortrait of Noam Chomsky\nremoved for copyright restrictions.\n\nAmbiguity\nSyntax: Frank shot the hunter with the shotgun.\nLexicon: run (polysemy); two/to/too (homophony)\nReferential: He said that we should give it to them.\n\nAmbiguity:\nA communicative benefit\n-\nAmbiguity is only a problem in theory\n-\nAmbiguity is not a problem in normal language use, because context disambiguates\n(Wasow & Arnold, 2003;Wasow et al., 2005; Jaeger, 2006; Roland, Elman, & Ferreira,\n2006; Ferreira, 2008; Jaeger, 2010).\n-\nPiantadosi,Tily & Gibson (2012):\n-\nAn information-theoretic proof that efficient communication systems\nwill necessarily be globally ambiguous when context is informative\nabout meaning\n-\n\"ambiguity\" potentially allows for re-use of easy linguistic elements:\n-\nJohn wanted to run.\n-\nJohn went to school.\n-\nJohn wanted two dollars.\n-\nSam wanted some money too.\n\nAmbiguity:\nA communicative benefit\nBecause context disambiguates, we don't have to say so much:\nWhen language is constructed to be as unambiguous as possible, with no other possible\ninterpretations, what you get is legalese:\nEXCEPT FOR THE LIMITED WARRANTY ON MEDIA SET FORTH ABOVE AND TO THE\nMAXIMUM EXTENT PERMITTED BY APPLICABLE LAW,THE APPLE SOFTWARE AND\nSERVICES ARE PROVIDED \"AS IS\" AND \"AS AVAILABLE\",WITH ALL FAULTS AND\nWITHOUT WARRANTY OF ANY KIND,AND APPLE AND APPLE'S LICENSORS\n(COLLECTIVELY REFERRED TO AS \"APPLE\" FOR THE PURPOSES OF SECTIONS 8 and 9)\nHEREBY DISCLAIM ALL WARRANTIES AND CONDITIONS WITH RESPECT TO THE APPLE\nSOFTWARE AND SERVICES, EITHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT\nNOT LIMITED TO,THE IMPLIED WARRANTIES AND/OR CONDITIONS OF\nMERCHANTABILITY, OF SATISFACTORY QUALITY, OF FITNESS FOR A PARTICULAR\nPURPOSE, OF ACCURACY, OF QUIET ENJOYMENT,AND NON-INFRINGEMENT OF\nTHIRD PARTY RIGHTS.\n\nLanguage as efficient communication:\nShorter words are more ambiguous\nPiantadosi,Tily & Gibson (2012)\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com.\nUsed with permission. Source: Piantadosi, Steven T.,\nHarry Tily, and Edward Gibson. \"The communicative\nfunction of ambiguity in language.\" Cognition 122, no. 3\n(2012): 280-291.\n-\nNumber of additional meanings each phonological form has, as a function of length.\n-\nShorter phonological forms having more homophones / meanings.\n\nAmbiguity:\na communicative benefit\nThe existence of ambiguity out of context in human language (which is\ndisambiguated by context) is explained by information theory.\nIn other approaches, the existence of ambiguity out of context is an\nunexplained accident.\n\nNoisy-channel models of comprehension\n(c) The Morning Bulletin. All rights reserved. This content is\nexcluded from our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/\n\n(c) Source Unknown. All rights reserved. This content\nis excluded from our Creative Commons license. For\nmore information, see http://ocw.mit.edu/help/faq-\nfair-use/\nThirty sows and pigs\nin a river\nThirty thousand pigs in a river\n\nNoisy-channel models of language\ncomprehension: Mondegreens\nWikipedia: American writer SylviaWright coined the term in her essay\n\"The Death of Lady Mondegreen\", published in Harper's Magazine in\nNovember 1954.\n17th-century ballad \"The Bonnie Earl o' Moray\":\nYe Highlands and ye Lowlands,\nOh, where hae ye been?\nThey hae slain the Earl o' Moray,\nAnd laid him on the green.\nWright misheard the last line as \"And Lady Mondegreen\"\nIn unsupportive contexts, more frequent words and phrases are\nsometimes perceived instead\n\nMondegreens in songs\nCreedence Clearwater Revival,\"Bad Moon Rising\"\n\"There's a bathroom on the right\"\n\nMondegreens in songs\nCreedence Clearwater Revival,\"Bad Moon Rising\"\n\"There's a bathroom on the right\"\nManfred Mann,\"Blinded by the light\"\n\"wrapped up like a douche\"\n\nMondegreens in songs\nCreedence Clearwater Revival,\"Bad Moon Rising\"\n\"There's a bathroom on the right\"\nManfred Mann,\"Blinded by the light\"\n\"wrapped up like a douche\"\nJimi Hendrix,\"Purple Haze\"\n\"Excuse me while I kiss this guy\"\n\nMondegreens in songs\nCreedence Clearwater Revival,\"Bad Moon Rising\"\n\"There's a bathroom on the right\"\nManfred Mann,\"Blinded by the light\"\n\"wrapped up like a douche\"\nJimi Hendrix,\"Purple Haze\"\n\"Excuse me while I kiss this guy\"\nRush,\"Limelight\"\n\"living in a fish island\"\n\nRational inference in language:\nNoisy-channel models of language\n\"thirty sows and pigs\"\n\"thirty thousand pigs\"\nLanguage for communication: The rational integration of noise\nand prior lexical, syntactic and semantic expectation:\nMaximize P(si | sp) by maximizing P(si) * P(si → sp)\nAll linguistic measures (e.g., reading times, acceptability ratings) reflect:\n-\nthe prior expectation of what might be produced\n-\nthe likelihood of noise changing si into sp\n\nNoisy-channel models of comprehension\n- Classic assumption in sentence processing:\ninput to the parser is an error-free sequence of words\n(e.g., Frazier & Fodor, 1978; Gibson, 1991, 1998; Jurafsky, 1996; Hale,\n2001; Levy, 2008a).\n- This assumption is problematic (e.g., Levy, 2008b).\nMany sources of noise:\n(a) perception errors (mis-hearing/mis-reading); the environment\ncan be noisy\n(b) production errors (mis-speaking/mis-typing)\n- Classic issue in signal processing (e.g., Shannon, 1948)\n- Previous work: Speech (Jelinek, 1975; Clayards,Tanenhaus,Aslin &\nJacobs, 2008); Memory (Botvinick, 2005); Reading (Levy et al., 2009)\n\nNoisy-channel models of comprehension\nGeneral prediction for sentence interpretation:\nThe ultimate interpretation of a sentence should depend on the proximity\nof plausible alternatives under the noise model.\nA plausible noise model (cf. Levenshtein distance):\nsome cost for deletions, insertions (maybe swaps?)\n(Gibson, Bergen & Piantadosi, 2013,PNAS)\n\nNoisy-channel models of comprehension\nTesting the predictions: syntactic alternations:\nMore changes leads to lower likelihood of inferring the\nalternative (cf. MacWhinney & Bates, 1989; Ferreira, 2003)\n\"Minor\" change alternations:\nPO-goal - DO-goal (1 deletion):\nThe mother gave the candle to the daughter. - The mother gave the candle the daughter.\nDO-goal - PO-goal (1 insertion):\nThe mother gave the daughter the candle. - The mother gave the daughter to the candle.\n\"Major\" change alternations:\nPassive - Active (2 deletions):\nThe ball was kicked by the girl. - The ball kicked the girl.\nActive - Passive (2 insertions):\nThe girl kicked the ball. - The girl was kicked by the ball.\n\nNoisy-channel models of comprehension\nDesign:\n- manipulate plausibility (using role reversals)\n- examine interpretation\nInterpretation was assessed with comprehension questions.\nExamples:\na. Sentence: The ball kicked the girl.\nQuestion: Did the ball kick something/someone?\nb. Sentence: The mother gave the candle the daughter.\nQuestion: Did the daughter receive something/someone?\nE.g., in (a) a \"yes\" answer indicates that the reader relied on syntax (surface\nform) to interpret the sentence; a \"no\" answer indicates that the reader relied\non semantics. The reverse holds for (b).\n(Gibson, Bergen & Piantadosi, 2013)\n\nResults\ndeletion\nThe ball was/∅ kicked by/∅ the girl.\n2 deletions\nThe girl ∅/was kicked ∅/by the ball.\n2 insertions\n∅/Onto The cat jumped onto/∅ a table.\n1 insertion, 1\n1a. Passive -- Active:\n1b. Active -- Passive:\n2a. Subj-loc -- Obj-loc:\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0.9!\n1!\n0.6!\n0.7!\n0.8!\nOnto/∅ the table jumped ∅/onto a cat.\n1 deletion, 1\n2b. Obj-loc -- Subj-loc:\ninsertion\n0.3!\n0.4!\n0!\n0.1!\n0.2!\n0.5!\nThe tax law benefited ∅/from the businessman. 1 insertion\n3a. Intrans ->Trans:\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\nThe businessman benefited from/∅ the tax law. 1 deletion\n3b. Trans -- Intrans:\n0!\n0.1!\n0.2!\nCourtesy of Proceedings\nof the National Academy\nof Sciences. Used with\n0.7!\n0.8!\n0.6!\n0.9!\n1!\npermission. Source:\nThe mother gave the daughter ∅/to the candle. 1 insertion\n4a. DO -- PO-goal:\n0.4!\n0.5!\n0.3!\nBergen, and Steve\nGibson, Edward, Leon\nn T.\nThe mother gave the candle to/∅ the daughter. 1 deletion\n4b. PO -- DO-goal:\n0!\n0.1!\n0.2!\n1!\nPiantadosi. \"Ratio\nintegration of noisy\nnal\nevidence and prior\nsemantic expectations in\nsentence interpretation.\"\n1 insertion\n5a. DO -- PO-benef: The cook baked Lucy ∅/for a cake.\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\nProceedings of the\nNational Academy of\n1 deletion\n5b. PO -- DO-benef: The cook baked a cake for/∅ Lucy.\n0!\n0.1!\n0.2!\n0.3!\nSciences 110, no.\n(2013): 8051-8056.\nMore changes lead to a greater reliance on syntax:\nmajor changes (93.4%) vs. minor changes: (56.1%)\nDeletions are perceived to be more likely than insertions, leading to lower likelihood of literal\nmeaning for deletions:\nsingle insertions (66.1%) vs. single deletions (46.0%)\n\nResults\nCourtesy of Proceedings\nof the National Academy\nof Sciences. Used with\npermission. Source:\nGibson, Edward, Leon\nBergen, and Steven T.\nPiantadosi. \"Rational\nintegration of noisy\nevidence and prior\nsemantic expectations in\nsentence interpretation.\"\nProceedings of the\nNational Academy of\nSciences 110, no. 20\n(2013): 8051-8056.\n1a. Passive -> Active: The ball was/∅ kicked by/∅ the girl.\n2 deletions\n1b. Active -> Passive: The girl ∅/was kicked ∅/by the ball.\n2 insertions\n2a. Subj-loc -> Obj-loc:\n∅/Onto The cat jumped onto/∅ a table.\n1 insertion, 1\ndeletion\n2b. Obj-loc -> Subj-loc:\nOnto/∅ the table jumped ∅/onto a cat.\n1 deletion, 1\ninsertion\n3a. Intrans ->Trans: The tax law benefited ∅/from the businessman. 1 insertion\n3b. Trans -> Intrans: The businessman benefited from/∅ the tax law. 1 deletion\n4a. DO -> PO-goal: The mother gave the daughter ∅/to the candle. 1 insertion\n4b. PO -> DO-goal: The mother gave the candle to/∅ the daughter. 1 deletion\n5a. DO -> PO-benef: The cook baked Lucy ∅/for a cake.\n1 insertion\n5b. PO -> DO-benef: The cook baked a cake for/∅ Lucy.\n1 deletion\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\nPrediction: more noise should lead to greater reliance on likely meaning\nManipulation:\nadd noise to 30 of the 60 fillers\n10 - extra function word; 10 - missing function word; 10 - local transpositions\n\nCourtesy of Proceedings of the National Academy of Sciences. Used\nwith permission. Source: Gibson, Edward, Leon Bergen, and Steven T.\nPiantadosi. \"Rational integration of noisy evidence and prior semantic\nResults\nexpectations in sentence interpretation.\" Proceedings of the National\nAcademy of Sciences 110, no. 20 (2013): 8051-8056.\n1a. Passive -- Active: The ball was/∅ kicked by/∅ the girl.\n2 deletions\n1b. Active -- Passive: The girl ∅/was kicked ∅/by the ball.\n2 insertions\n2a. Subj-loc -- Obj-loc:\n∅/Onto The cat jumped onto/∅ a table.\n1 insertion, 1\ndeletion\n2b. Obj-loc -- Subj-loc:\nOnto/∅ the table jumped ∅/onto a cat.\n1 deletion, 1\ninsertion\n3a. Intrans ->Trans: The tax law benefited ∅/from the businessman. 1 insertion\n3b. Trans -- Intrans: The businessman benefited from/∅ the tax law. 1 deletion\n4a. DO -- PO-goal: The mother gave the daughter ∅/to the candle. 1 insertion\n4b. PO -- DO-goal: The mother gave the candle to/∅ the daughter. 1 deletion\n5a. DO -- PO-benef: The cook baked Lucy ∅/for a cake.\n1 insertion\n5b. PO -- DO-benef: The cook baked a cake for/∅ Lucy.\n1 deletion\n1!\n0.9!\n0.8!\n0.7!\n0.6!\n0.5!\n0.4!\n0.3!\n0.2!\n0.1!\n0!\n1!\n0.9!\n0.8!\n0.7!\n0.6!\n0.5!\n0.4!\n0.3!\n0.2!\n0.1!\n0!\n1!\n0.9!\n0.8!\n0.7!\n0.6!\n0.5!\n0.4!\n0.3!\n0.2!\n0.1!\n0!\n1!\n0.9!\n0.8!\n0.7!\n0.6!\n0.5!\n0.4!\n0.3!\n0.2!\n0.1!\n0!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\n0!\n0.1!\n0.2!\n0.3!\n0.4!\n0.5!\n0.6!\n0.7!\n0.8!\n0.9!\n1!\nMore syntactic errors decreased the reliance on syntax:\n56.1% vs. 42.7 for the minor-change alternations\n\nSteve\nLeon\n\nNoisy-channel models of comprehension\nManipulations of semantic / plausibility prior:\nPlausibility prior: how likely it is that an implausible utterance will\nbe generated\nExpt 1a - 1e:\nEach was run with 60 plausible fillers.\nImplausible ratio = 1/8 (10 implaus + 70 plaus)\nExpt 3a - 3e:\nEach was run with 60 plausible fillers plus the materials in the other\nexperiments.\nImplausible ratio = 5/16 (50 implaus + 110 plaus)\n\nCourtesy of Proceedings of the National Academy of Sciences.\nUsed with permission. Source: Gibson, Edward, Leon Bergen, and\nSteven T. Piantadosi. \"Rational integration of noisy evidence and\nResults\nprior semantic expectations in sentence interpretation.\" Proceedings\nof the National Academy of Sciences 110, no. 20 (2013): 8051-8056.\n1a. Passive -- Active: The ball was/∅ kicked by/∅ the girl.\n2 deletions\n1b. Active -- Passive: The girl ∅/was kicked ∅/by the ball.\n2 insertions\n2a. Subj-loc -- Obj-loc:\n∅/Onto The cat jumped onto/∅ a table.\n1 insertion, 1\ndeletion\n2b. Obj-loc -- Subj-loc:\nOnto/∅ the table jumped ∅/onto a cat.\n1 deletion, 1\ninsertion\n3a. Intrans ->Trans: The tax law benefited ∅/from the businessman. 1 insertion\n3b. Trans -- Intrans: The businessman benefited from/∅ the tax law. 1 deletion\n4a. DO -- PO-goal: The mother gave the daughter ∅/to the candle. 1 insertion\n4b. PO -- DO-goal: The mother gave the candle to/∅ the daughter. 1 deletion\n5a. DO -- PO-benef: The cook baked Lucy ∅/for a cake.\n1 insertion\n5b. PO -- DO-benef: The cook baked a cake for/∅ Lucy.\n1 deletion\n1!\n1!\n0.9!\n0.9!\n0.8!\n0.8!\n0.7!\n0.7!\n0.6!\n0.6!\n0.5!\n0.5!\n0.4!\n0.4!\n0.3!\n0.3!\n0.2!\n0.2!\n0.1!\n0.1!\n0!\n0!\n1!\n1!\n0.9!\n0.9!\n0.8!\n0.8!\n0.7!\n0.7!\n0.6!\n0.6!\n0.5!\n0.5!\n0.4!\n0.4!\n0.3!\n0.3!\n0.2!\n0.2!\n0.1!\n0.1!\n0!\n0!\n1!\n1!\n0.9!\n0.9!\n0.8!\n0.8!\n0.7!\n0.7!\n0.6!\n0.6!\n0.5!\n0.5!\n0.4!\n0.4!\n0.3!\n0.3!\n0.2!\n0.2!\n0.1!\n0.1!\n0!\n0!\n1!\n1!\n0.9!\n0.9!\n0.8!\n0.8!\n0.7!\n0.7!\n0.6!\n0.6!\n0.5!\n0.5!\n0.4!\n0.4!\n0.3!\n0.3!\n0.2!\n0.2!\n0.1!\n0.1!\n0!\n0!\n1!\n1!\n0.9!\n0.9!\n0.8!\n0.8!\n0.7!\n0.7!\n0.6!\n0.6!\n0.5!\n0.5!\n0.4!\n0.4!\n0.3!\n0.3!\n0.2!\n0.2!\n0.1!\n0.1!\n0!\n0!\nMore implausible materials increased the reliance on syntax:\n56.1% vs. 72.6 for the minor-change alternations\n\nSteve\nLeon\n\nNoisy-channel models of comprehension\nSummary:\nEvidence for a noise model:\n1. People are more likely to infer the plausible alternative if it\ninvolves inferring fewer errors.\n2. People are more likely to infer the plausible alternative if it is\none deletion away compared to one insertion.\n3. Increasing the noise increases the reliance on plausibility.\nEvidence for priors:\n1. Plausibility Prior: Increasing the likelihood of implausible events\ndecreases the reliance on semantics.\n(Gibson, Bergen & Piantadosi, 2013,PNAS)\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nA classic finding in the sentence production literature (Bock & Miller, 1991)\nand comprehension (Pearlmutter, Garnsey & Bock, 1999):\nAgreement error asymmetry:\n1.The key to the cabinets was / were on the table.\n(Many errors for plural local noun)\n2.The keys to the cabinet were / ??was on the table.\n(Very few errors for singular local noun)\nStandard explanation: there is a markedness difference between\nsingular vs. plural nouns, in memory retrieval / sentence planning.\nStipulation\n\nLeon\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nNote it is currently unclear if either kind of attraction error is more\ncommon in natural speech / text.\nIt is easy to find examples of both plural and singular attraction errors in\npeople's speech / writing:\nSingular attractions:\nThe stairwells on the BCS Headquarters side of the building is in the process of\nbeing painted. (email to BCS department April 2013)\nThe consequences of that is ... (talk at MIT, Oct, 2013)\n \n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nOur claim:\nAgreement errors result from rational misidentification of the preamble.\nThe asymmetry between singular and plural head-nouns is\nexplained by 2 factors:\n-\nDeletions are much more likely than insertions (Gibson et al, 2013). Thus\nagreement errors will occur more often when the head noun is singular.\n-\nPrior distribution of NP sequences: the singular-singular is much the most common\nsequence. Thus there will be few errors confusing sing-sing as plural-sing.\nPlural-head/Singular-local\nThe keys to the cabinet...\nGiven the plural head noun, it is unlikely that the comprehender will infer that the plural-marking\nwas produced by mistake, so unlikely to be pulled to the sing-sing.\nSingular-head/Plural-local\nThe key to the cabinets...\nGiven the singular head noun, it is possible that the comprehender will think that the producer\nintended a plural / plural, hence producing an error.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/f67679f82727ed6e0c536aadac1485b7_MIT9_59jS17_lec6.pdf",
      "content": "Class 6:\nLanguage processing:\nNoisy channel / Locality\nTed Gibson\n9.59J/24.905J\n\nSteve\nLeon\n\nNoisy-channel models of comprehension\nSummary:\nEvidence for a noise model:\n1. People are more likely to infer the plausible alternative if it\ninvolves inferring fewer errors.\n2. People are more likely to infer the plausible alternative if it is\none deletion away compared to one insertion.\n3. Increasing the noise increases the reliance on plausibility.\nEvidence for priors:\n1. Plausibility Prior: Increasing the likelihood of implausible events\ndecreases the reliance on semantics.\n(Gibson, Bergen & Piantadosi, 2013,PNAS)\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nA classic finding in the sentence production literature (Bock & Miller, 1991)\nand comprehension (Pearlmutter, Garnsey & Bock, 1999):\nAgreement error asymmetry:\n1.The key to the cabinets was / were on the table.\n(Many errors for plural local noun)\n2.The keys to the cabinet were / ??was on the table.\n(Very few errors for singular local noun)\nStandard explanation: there is a markedness difference between\nsingular vs. plural nouns, in memory retrieval / sentence planning.\nStipulation\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nOur claim:\nAgreement errors result from rational misidentification of the preamble.\nThe asymmetry between singular and plural head-nouns is\nexplained by 2 factors:\n-\nDeletions are much more likely than insertions (Gibson et al, 2013). Thus\nagreement errors will occur more often when the head noun is singular.\n-\nPrior distribution of NP sequences: the singular-singular is much the most common\nsequence. Thus there will be few errors confusing sing-sing as plural-sing.\nPlural-head/Singular-local\nThe keys to the cabinet...\nGiven the plural head noun, it is unlikely that the comprehender will infer that the plural-marking\nwas produced by mistake, so unlikely to be pulled to the sing-sing.\nSingular-head/Plural-local\nThe key to the cabinets...\nGiven the singular head noun, it is possible that the comprehender will think that the producer\nintended a plural / plural, hence producing an error.\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nPredictions:\n1.Misidentification of the sentence preamble will lead to repetition errors in the\npreamble (The actors in the commercials when the true input is The actor in the\ncommercials). These should pattern with typical agreement errors.\n2.Additional cues to head-number will decrease the agreement error rate. Multiple\ncues are likely to have been used intentionally.\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nExperiment 1:\nValidation of slightly modified methodology.\nTask: 1.5 sec visual presentation of preambles, 13 sec to retype and complete\nsentence.\n32 items, using materials similar to (Bock & Miller, 1991):\nPlural-head/Plural-local\nThe actors in the commercials\nPlural-head/Singular-local\nThe actors in the commercial\nSingular-head/Plural-local\nThe actor in the commercials\nSingular-head/Singular-local The actor in the commercial\n\n0.15\n0.20\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nExperiment 1: n=80, MTurk\nExperiment 1 Agreement Errors\nResults:\nProportion\n0.10\nReplication of two major effects:\nasymmetry between singular and plural\nhead-nouns, and effect of mismatch\nbetween head and local nouns: Reliable\n0.05\ninteraction\n0.00\nPlural-plural\nPlural-singular Singular-plural Singular-singular\nPreamble\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nExperiment 1: n=80, MTurk\nExperiment 1 Repetition Errors\n0.20\nResults:\n0.15\nRepetition errors patterned with\nagreement errors, occurring most\nProportion\n0.00\n0.05\n0.10\nfrequently in the singular-plural\ncondition: Reliable interaction\nPlural-plural\nPlural-singular Singular-plural Singular-singular\nPreamble\n\nAgreement errors: the result of noisy-channel\nin comprehension? (Bergen & Gibson, 2012)\nExperiment 2: n=80, MTurk\nExperiment 2 Agreement Errors\nDefinite\nIndefinite\n0.20\n0.15\nProportion\n0.10\nResults:\n0.05\nSignificant interaction between the article and\nthe number of the local-noun.\nError rates were significantly lower for the\n0.00\nsingular-plural items in the indefinite condition\nSingular-plural Singular-singular\nSingular-plural Singular-singular\nthan in the definite condition.\nPreamble\nDesign: MORE NUMBER CUES\nAn additional cue to the number marking of the\nhead-noun: an indefinite article (see Hartsuiker\net al., 2003, for a similar experiment in Dutch).\nDefinite/singular-local(plural-local)\nThe actor in the commercial(s)\nIndefinite/singular-local(plural-local)\nAn actor in the commercial(s)\nPrediction: Fewer agreement errors in the\nindefinite conditions.\n\nA noisy-channel explanation of agreement\nerrors (Bergen & Gibson, 2012)\nThese experiments argue for a rational inference account\n(cf. traditional views, i.e., syntactic planning or memory retrieval).\nAdvantages of the rational inference account:\n1.A principled explanation of the sing-plural / plural-sing asymmetry.\n2. Repetition errors pattern with typical agreement errors.\n3.A unitary explanation for three additional aspects of the data:\n- Additional head-number cues decrease the error rate.\n- Increasing the presentation time decreases the error rate.\n- Increasing the base rate of singular-singular NP-P-NPs decreases the\nerror rate.\n\nLeon\nBergen\nNoisy channel:Verb omission errors\nBergen, Levy & Gibson (2012)\nNoisy channel account predicts that the phenomenon is not tied to\nagreement.\nExperiment 1: Completion of NN/NV preambles\nNV biased:\nNN unambig: The immigrant fear ...\nNV unambig: The immigrant feared ...\nNN biased:\nNN unambig: The almond roll ...\nNV unambig: The almond rolled ...\nPredicted of noisy-channel approach: deletions more likely\nthan insertions, so people should produce most errors in NV-biased NN\n\nLeon\nBergen\nNoisy channel:Verb omission errors\nBergen, Levy & Gibson (2012)\nExperiment 1\nNV biased, NN unambig: The immigrant fear ...\nNV biased, NV unambig: The immigrant feared ...\nNN biased, NN unambig: The almond roll ...\nNN biased, NV unambig: The almond rolled ...\nAs predicted people produce most errors in\nNV-biased NN:\nThe immigrant fear ... being deported. (Infer\ndeletion of \"s / ed\"): Verb omission\nerror\nNot: The almond rolled ... was tasty (Infer\ninsertion: very unlikely)\nCourtesy of Proceedings of the Cognitive Science Society. License CC BY-NC. Source: Bergen,\nLeon, Roger Levy, and Edward Gibson. \"Verb omission errors: Evidence of rational processing of\nnoisy language inputs.\" In Proceedings of the Cognitive Science Society, vol. 34, no. 34. 2012.\n\nLeon\nBergen\nNoisy channel:Verb omission errors\nBergen, Levy & Gibson (2012)\nPrediction: people will adopt incorrect an syntactic analysis if there exist\nsimilar phrases that could have easily generated them.\nPeople will follow this incorrect interpretation, and be confused later when it\nturns out to be wrong.\nExperiment 2: Self-paced reading\nNN / dense: The intern chauffeur for the governor hoped for more interesting work.\nNV / dense: The intern chauffeured for the governor but hoped for more interesting work.\nNN / sparse: The inexperienced chauffeur for the governor hoped for more interesting work.\nNV / sparse: Some interns chauffeured for the governor but hoped for more interesting work.\n\nLeon\nBergen\n\nNoisy channel:Verb omission errors\nBergen, Levy & Gibson (2012)\nExperiment 2: Self-paced reading\nNN / dense: The intern chauffeur for the governor hoped for more interesting work.\nNV / dense: The intern chauffeured for the governor but hoped for more interesting work.\nNN / sparse: The inexperienced chauffeur for the governor hoped for more interesting work.\nCourtesy of Proceedings of the\nCognitive Science Society.\nLicense CC BY-NC. Source:\nBergen, Leon, Roger Levy, and\nEdward Gibson. \"Verb omission\nerrors: Evidence of rational\nprocessing of noisy language\ninputs.\" In Proceedings of the\nCognitive Science Society, vol.\n34, no. 34. 2012.\nNV / sparse: Some interns chauffeured for the governor but hoped for more interesting work.\n\n8 Possible Projects\n(9 more to come)\n1. The potential context sensitivity of acceptability judgments.\nHow are acceptability judgments affected by the context? that is, what happens when the\nsame materials are embedded in different sets of filler materials? Are the results the same?\nor does the ease / difficulty / similarity of the filler materials affect the judgment task in\nimportant ways?\nReplication: a replication of some standard results from the literature. E.g., island effects in\nextractions; nested complexity; agreement.\n2. Acceptability judgments in a language other than English.\nThis project would take Mahowald et al (2016) and Sprouse et al (2013) as a baseline, and\nevaluate some judgments from the syntax literature in another language, like the work of\nLinzen & Oseki (2015). In order to do this project, you would need to be a native speaker of\nthis language.\n\n8 Possible Projects\n3. Information theory: word length (Mahowald et al. 2013)\nMahowald et al. 2013 compared meaning-matched word pairs like chimp/chimpanzee and\nfound that a more supportive sentence context is more predictive of the shorter form. This\nresult holds in both a corpus analysis using Google n-grams and a behavioral experiment on\nTurk in which people were asked to choose either the long or short form of a word. This is\nconsistent with the findings of Piantadosi et al. (2011), who found that, consistent with\npredictions from information theory, surprisal in context was a better predictor of word length\nthan frequency.\n4. Information theory: optional elements in the syntax (Jaeger, 2010)\nJaeger (2010) did corpus analyses, showing that people tend to produce the optional\ncomplementizer \"that\" in environments with high surprisal (e.g., following a verb like \"saw\",\nwhich often takes an NP complement but rarely takes an S complement), and they tend to\nomit \"that\" in environments with low surprisal (e.g., following a verb like \"know\", which often\ntakes an S complement but rarely takes an NP complement ).\nProject: replicate this effect in acceptability ratings over materials where the verb\nsubcategorization frequencies are varied from an S being highly expected, to an S being\nmuch less expected.\n\nUniform Information Density (UID)\nLevy & Jaeger (2006); Jaeger (2010)\nOne property of good codes for communication:\nInformation that is conveyed per unit time is constant\nCommunicate at the channel capacity\n-\nif you go over, you are overwhelming processing mechanisms\n-\nif you go under, you are not being efficient\nSo peaks and dips in entropy (average surprisal)\nshould be \"smoothed out\"\n\nUniform Information Density (UID)\nLevy & Jaeger (2006); Jaeger (2010)\nUID in phonetics: \nPeople lengthen unpredictable words and shorten predictable ones:\nLieberman: the vowel in \"nine\" is dependent on the preceding\ncontext:\n- \"A stitch in time saves nine.\"\n- \"The number you will now see is nine.\"\n\nUID in Syntax (Jaeger, 2010)\nOptionality in syntax: the complementizer \"that\" is optional in\nmany circumstances in English:\n- My boss thinks (that) we were absolutely crazy.\n- My boss confirmed (that) we were absolutely crazy.\n\nUID in Syntax (Jaeger, 2010)\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with permission. Source: Jaeger, T. Florian. \"Redundancy and reduction: Speakers manage syntactic information\ndensity.\" Cognitive psychology 61, no. 1 (2010): 23-62.\n\n8 Possible Projects\n5. Mondegreens\nWe have talked about mishearing song lyrics as noisy-channel inference. Project: Obtain\n10-20 mondegreen song examples from popular culture, along with controls from the same\nsongs by the same artists, which don't induce the mondegreen effect. \"Replicate\" the\nmondegreen effect by having people write down what they hear for these lyrics.\nAttempt to explain the observed effect by doing a language model, such that the\n\"mondegreen\" examples are less likely in real language and / or are less plausible in M Turk\nexperiment on the written materials.\n6. Information theory: understanding the noise model in a noisy-channel model of\nsentence comprehensions (Gibson, Bergen & Piantadosi, 2013)\nReplication / extension of Gibson et al. and/or: Poppels & Levy (2016); Gibson et al (in\npress) (accents paper)\nOne extension of Gibson et al (in press): look at the effect of different accents\nAn extension of Poppels & Levy: examine features of the noise model\n\nThe noisy-channel proposal applied\nto aphasic comprehension\nOld observation: aphasics' comprehension relies more on world knowledge\nthan non-brain-damaged controls. (e.g., Caramazza & Zurif, 1976)\nHypothesis: Aphasics' perception is noisier than that of healthy individuals.\nIn maximizing P(si | sp), aphasics will rely more on their prior distribution P(si)\nover plausibly intended sentences.\n(Gibson, Sandberg, Fedorenko, Bergen & Kiran, 2015, J of Aphasiology)\nPrediction:\nAphasics will rely on semantics more than healthy individuals, in both major-edit\n(active-passive) and minor-edit alternations (DO-PO).\n\nPlausible\nplausible\nimplausible\nPlausible\nplausible\nimplausible\n\nResults:Active / Passive vs. DO / PO\nAphasics: Active/Passive\nAphasics: DO/PO\nmean\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPlausible\nImplausible\nmean\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPlausible\nImplausible\nactive\npassive\nDO\nPO\nAlternation\nAlternation\nAphasics rely more on semantics in minor-edits (DO/PO) than in major-edits (active\npassive): z = 2.93, p < .005\nSimilar results for other populations (replicating Gibson, Bergen & Piantadosi, 2013)\n\nChallenges faced by L2 speakers\n- L2 speakers are embarrassed by their accents and the errors they\nmake (Gluszek & Dovidio, 2010)\n- L2 speakers are perceived to be:\n- less credible (Bourdieu, 1991; Lev-Ari & Keysar, 2010;\nLivingston et al., 2014)\n- less educated (Fraser & Kelly, 2012)\n- less intelligent (Fuertes, Potere & Ramirez, 2002;Anderson et\nal., 2007).\n\nL2: One potential advantage\nImagine you are at a cocktail party where you want to make\nbusiness connections.\nSuppose someone asks you about a Marketing Technologist\nposition.\nIf you have an L2 accent, you could say \"Marketing Technologist\nwas hired SEO Consultant.\"\nWith a foreign accent, they may interpret this in the most\nplausible way. Without a foreign accent, you cannot get away\nwith this uncertainty.\n\nL2: One potential advantage\nArianna Huffington, Smith College commencement address in\n2013:\n\"I moved to New York in 1980 and met Henry Kissinger, who told me\nnot to worry about my accent, because you can never, in American\npublic life, underestimate the advantages of complete and total\nincomprehensibility.\"\nCurrent work: Investigate whether her idea is true: that there is\na potential benefit to being misunderstood\nFramework for investigating this idea: rational inference /\nnoisy channel models of sentence comprehension\n\nSteve\nLeon\n\nBergen\nL2 vs. L1 Speakers\nCurrent work: Investigate whether the Kissinger / Huffington idea is\ntrue: that there is a potential benefit to being misunderstood, using\nrational inference framework: Maybe people will make more\nplausible inferences for L2-accented speakers\nKonieczny, Hemforth & Scheepers (1994): People are likely to infer a higher\nlikelihood linguistic prior when interacting with non-native speakers:\nOne experimenter is was German; another was English with German\naccent:\n- When the German experimenter spoke to self-paced reading participants,\nthey interpreted NP V NP sequences as Object-Verb-Subject, because of\nthe appropriate morphology.\n- When the English experimenter spoke to them in accented German, they\ninterpreted NP V NP sequences as Subject-Verb-Object in spite of\ninappropriate morphology\nLeon\n\nSteve\nLeon\n\nLeon\nBergen\nL2 vs. L1 Speakers: New Experiments\nInterpretation of implausible materials, spoken by the same person\n(each of 2 Speakers), +accent or -accent\n3 sets of implausible materials, from the PNAS paper:\n1. DO/PO\nThe mother gave the candle the daughter.\nThe mother gave the daughter to the candle.\n2. Transitive/intransitive\nThe businessman benefited the tax law.\nThe tax law benefited from the businessman.\n3. Active/Passive\nThe ball kicked the girl.\nThe girl was kicked by the ball.\n\nSteve\nLeon\n\nLeon\nBergen\nL2 vs. L1 Speakers: New Experiments\nInterpretation of implausible materials, spoken by the same person\n(each of 2 Speakers), +accent or -accent\n3 sets of implausible materials, from the PNAS paper:\nFillers: Filler items from Gibson et al., spoken with no accent by the\nother speaker\nSpeaker 1: accented / no-accent target items\nSpeaker 2: no-accent filler items\n\nSteve\nLeon\nLeon\nBergen\nL2 vs. L1 Speakers: Predictions\nIf participants think there is more noise in the accented\nproductions, then we predict higher rates of inferences for the\nDO/PO materials and the transitive/intransitive materials\nwhere Gibson et al. (2013) had seen more inferences when noise was\nadded to their filler materials\nbut not necessarily for the active/passive materials\nwhere Gibson et al. (2013) had not seen more inferences when noise\nwas added to their filler materials, possibly because there are too many\nedits to get from the implausible to a plausible version\n\nSteve\nLeon\n\nLeon\nBergen\nL2 vs. L1 Speakers: Methods / Participants\n3 experiments, each consisting of four groups of 80 workers, on\nAmazon.com's Mechanical Turk\nInstructions:\nThis is a set of 80 auditory sentences.Answer the questions\nimmediately following, according to what you think the speaker\nintended.\n10-15 minutes for each participant to complete the task.\n\nSteve\nLeon\n\nLeon\nBergen\nEvaluating the comprehensibility of the\nmaterials\nA higher rate of plausibility-based inferences in the +accent\ncondition could result if participants simply cannot discern the\nwords in the utterance.\nNorming:An additional 480 Mechanical Turk participants were\nasked to transcribe what each speaker said (even if it was\nimplausible) across four surveys: 120 participants for the\nimplausible target sentences from each of Speaker 1 and\nSpeaker 2, for each of their accent and no-accent pr oductions.\n\nSteve\nLeon\n\nLeon\nBergen\nL2 vs. L1 Speakers: Results\n1. DO, PO: ~20% inference effect\n2.Transitive, Intransitive: ~15% inference effect\n3.Active, Passive: no significant difference.\n4. Main effect of speaker: more\ninferences for S1 vs S2;\n5. Replication of Gibson et al. 2013\ndifferences between constructions\n\nDiscussion\nAs suggested by Kissinger / Huffington, there is a potential benefit\nto being misunderstood: With a foreign accent, others may be\ngenerous in interpreting your speech.\nExplained by rational inference models of sentence comprehension\nDoes the effect depend on the particular accent?\nDoes the effect change when all materials are in the accent?\n\n8 Possible Projects\n7. Sentence completion errors as rational inference\nBergen & Gibson (2012) proposed a rational inference hypothesis for explaining the\nasymmetry between \"the key to the cabinets are ...\" and \"the keys to the cabinet is ...\".\nBergen, Levy & Gibson (2012) showed a similar effect, which did not involve agreement:\nNV biased, NN unambig: The immigrant fear ...\nNV biased, NV unambig: The immigrant feared ...\n8. Locality vs. surprisal in online reading.\nReplicate subject- vs object-extractions in English (e.g., Gibson, 2000):\nsubject-extracted relative clause: The reporter who attacked the senator admitted the error.\nobject-extracted relative clause: The reporter who the senator attacked admitted the error.\nExtension: look at genitive extractions, dative extractions\nCompare frequency in the input vs. the RTs that are observed.\n\nLanguage: \nInformation sources and constraints\nWorking memory: Longer distance dependencies are harder to process than more local ones\nDependencies between a verb and its post-verbal objects:\nShort NP object:\nLocal Particle: Joe threw out the documents.\nNon-local Particle: Joe threw the documents out.\nLong NP object:\nLocal Particle: Joe threw out the very important documents that he brought home.\nNon-local Particle: Joe threw the very important documents that he brought home out.\n\nInformation processing: Working memory\nWorking memory: Local connections are easier to make than long-distance\nones (Gibson, 1998, 2000; Grodner & Gibson, 2005;Warren & Gibson, 2002;\nLewis & Vashishth, 2005; Hawkins, 1994)\nAmbiguous attachments:\nThe bartender told the detective that the suspect left the country yesterday.\nyesterday is preferred as modifying left rather than told\n(Frazier & Rayner, 1982; Gibson et al., 1996;Altmann et al., 1998; Pearlmutter & Gibson, 2001)\nUnambiguous connections:\nThe reporter wrote an article.\nThe reporter from the newspaper wrote an article.\nThe reporter who was from the newspaper wrote an article.\n\nRetrieval / Integration-based theories\nIntegration: connecting the current word into the structure built\nthus far: Local integrations are easier than longer-distance integrations\n- The Dependency Locality Theory (DLT) (Gibson, 1998; 2000):\nintervening discourse referents cause retrieval difficulty (also in\nproduction)\n- Activation-based memory theory: similarity-based interference\n(Lewis & Vasishth, 2005;Vasishth & Lewis, 2006; Lewis,Vasishth\n& Van Dyke, 2006): intervening similar elements cause retrieval\ndifficulty\n- Production: Hawkins (1994; 2004): word-based distance metric.\n\nConsequence:\nNested structures are difficult\ncrosslinguistically\nEnglish:\nThe reporter [ who the senator attacked ] admitted the error.\nThe reporter [ who the senator [ who I met ] attacked ] admitted the error.\nI met the senator who attacked the reporter who admitted the error.\nJapanese:\nObasan-wa [ bebiisitaa-ga [ ani-ga imooto-o ijimeta ] to itta ] to omotteiru\naunt-top babysitter-nom older-brother-nom younger-sister-acc bullied that said that\nthinks\n\"My aunt thinks that the babysitter said that my older brother bullied my younger\nsister\"\nEasier: Bebiisitaa-ga [ ani-ga imooto-o ijimeta ] to itta ] obasan-ga to omotteiru\n\nLocality effects in unambiguous structures:\nGibson & Grodner (2005) Experiment 1\nEnglish: Subject- vs. object-extracted relative clauses\n\nLocality effects in unambiguous structures:\nGibson & Grodner (2005) Experiment 1\nobject-extracted\nrelative clauses\n0.5\n1.5\n2.5\nRTs\nDLT\nThe reporter\nwho the\nsent to\nthe editor\nhoped for a good story\nphotographer\nsubject-extracted\nrelative clauses\n0.5\n1.5\n2.5\nRTs\nDLT\n(c) Wiley. All rights reserved. This content is excluded from\nour Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/ Source: Grodner,\nDaniel, and Edward Gibson. \"Consequences of the serial\nnature of linguistic input for sentenial complexity.\"\nThe reporter\nwho sent\nthe\nto the editor\nhoped for\na good story\nCognitive science 29, no. 2 (2005): 261-290.\nphotographer\n\nLocality effects in unambiguous structures:\nGibson & Grodner (2005) Experiment 2\nMatrix - Unmodified Subject\nThe nurse supervised the administrator while ...\nMatrix - PP Modified Subject\nThe nurse from the clinic supervised the administrator while ...\nMatrix - RC Modified Subject\nThe nurse who was from the clinic supervised the administrator while ...\nEmbedded - Unmodified Subject\nThe administrator who the nurse supervised scolded the medic while...\nEmbedded - PP Modified Subject\nThe administrator who the nurse from the clinic supervised scolded the medic...\n0 1\nEmbedded - RC Modified Subject\nThe administrator who the nurse who was from the clinic supervised scolded the medic...\n\n-1\nLocality effects in unambiguous structures:\nGibson & Grodner (2005) Experiment 2\nExperiment 2: DLT vs. RTs by Words\nRT by Word (msec)\nDLT Integration Cost\n(c) Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/ Source: Grodner, Daniel, and\nEdward Gibson. \"Consequences of the serial nature of linguistic input for sentenial complexity.\" Cognitive science 29, no. 2 (2005): 261-290.\n\nPotential project\nResult to replicate: Subject-extractions in Relative clauses (RCs) are easier to process than object-\nextractions:\nSubj-RC: The reporter who attacked the senator admitted the error.\nObj-RC: The reporter who the senator attacked admitted the error.\nRTs faster at \"attacked\" in SRC than in ORC\nTwo explanations: ORCs are rare, and longer-distance\nExtension: evaluation other kinds of extraction in English:\nDative extractions: infrequent, long-distance\nThe boy who the girl gave the book to admitted the error.\nThe boy to whom the girl gave the book admitted the error.\nGenitive extractions: infrequent, short-distance \nThe girl whose friend invited the kids to the party was kind.\n\nLocality account of nesting complexity\nNested structures have longer distance dependencies than\nnon-nested structures.\n# The reporter [ who the senator [ who John met ] attacked ]\ndisliked the editor.\nJohn met the senator [ who attacked the reporter [ who disliked\nthe editor]].\nAn alternative account of nesting complexity: Nested structures\nhave parse states with more incomplete dependencies\n(e.g.,Yngve, 1960; Chomsky & Miller, 1963).\n\nLocality account of nesting complexity\nProblematic cases for incomplete-dependency approaches: Relative clauses (RCs) and sentence\ncomplements (SCs) (Cowper, 1976; Gibson, 1991):\nRC within SC: difficult, but processable\nThe fact [ that the employee [ who the manager hired ] stole office supplies ] worried the executive.\nSC within RC: much harder to process\n# The executive [ who the fact [ that the employee stole office supplies ] worried ] hired the manager.\nSame maximal number of incomplete dependencies, parsing left-to-right: 3 incomplete subject-verb\ndependencies, plus one incomplete filler-gap\nSolution: Distance-based integration accounts.The RC filler-gap dependency between \"who\"and its\nrole assigning verb (\"hired\") in (1) is more local than the RC filler-gap dependency between \"who\"\nand its role assigning verb (\"worried\") in (2).\n\nLocality account of nesting complexity\nThe lower complexity of examples nested pronouns (Bever, 1974; Kac, 1981)\nThe reporter who everyone that I met trusts said the president won't resign yet.\nA book that some Italian who I've never heard of wrote will be published soon by MIT Press.\n# The reporter [ who the senator [ who John met ] attacked ] disliked the editor.\nWarren & Gibson (2002), Experimental evidence:\nThe reporter who the senator who { you / John / the professor} met attacked disliked the\neditor.\nAverage Complexity Rating\n3.5\n2.5\nExperiment 6 Results\n1st / 2nd Pronoun\nProper Name\nDefinite Description\n(c) Elsevier. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more\ninformation, see http://ocw.mit.edu/help/faq-fair-use/\nSource: Warren, Tessa, and Edward Gibson. \"The\ninfluence of referential processing on sentence\ncomplexity.\" Cognition 85, no. 1 (2002): 79-112.\n\nLocality account of nesting complexity\nGibson (1998, 2000): Decay\nDiscourse-based decay hypothesis:The difficulty of integrating a new word h2 to h1 is proportional to\nthe number of discourse objects and events (nouns and verbs, roughly) which were introduced\nsince h1 was last processed. (cf.Warren & Gibson, 2002)\nHawkins: word-based decay hypothesis\nInterference of similar elements in the intervening structure:\nNP types: Gordon & colleagues\nPhrase structure similarity: Lewis,Vasishth, McElree and colleagues\nThe syntactic / semantic similarity of intervening NPs: More similar NPs, slower processing\nPrediction: Same kinds of NPs as head noun and embedded NP in an objected-extracted RC will\nlead to most processing difficulty, independent of the NP type\nClefts:\nIt was (the barber / John) that (the lawyer / Bill) saw in the parking lot.\nIt was (the barber / John) that saw (the lawyer / Bill) in the parking lot.\n\nGordon et al. 2001, Experiment 4\nClefts:\nIt was (the barber / John) that (the lawyer / Bill) saw in the parking lot.\nIt was (the barber / John) that saw (the lawyer / Bill) in the parking lot.\nCourtesy of\nJournal of\nExperimental\nPsychology. Used\nwith permission.\nSource: Gordon,\nPeter C., Randall\nHendrick, and\nMarcus Johnson.\n\"Memory\ninterference\nduring language\nprocessing.\"\nJournal of\nExperimental\nPsychology:\nLearning,\nMemory, and\nCognition 27, no.\n6 (2001): 1411.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/dcda5c4f9f915db6a5eeb6f85cef0415_MIT9_59jS17_lec7.pdf",
      "content": "Class 7:\nNoisy channel / Memory in\nsentence processing\nTed Gibson\n9.59J/24.905J\n\nInformation processing: Working memory\nWorking memory: Local connections are easier to make than long-distance\nones (Gibson, 1998, 2000; Grodner & Gibson, 2005;Warren & Gibson, 2002;\nLewis & Vashishth, 2005; Hawkins, 1994)\nAmbiguous attachments:\n \nThe bartender told the detective that the suspect left the country yesterday.\nyesterday is preferred as modifying left rather than told\n(Frazier & Rayner, 1982; Gibson et al., 1996;Altmann et al., 1998; Pearlmutter & Gibson, 2001)\nUnambiguous connections:\nThe reporter wrote an article.\nThe reporter from the newspaper wrote an article.\nThe reporter who was from the newspaper wrote an article.\n\nRetrieval / Integration-based theories\nIntegration: connecting the current word into the structure built\nthus far: Local integrations are easier than longer-distance integrations\n- The Dependency Locality Theory (DLT) (Gibson, 1998; 2000):\nintervening discourse referents cause retrieval difficulty (also in\nproduction)\n- Activation-based memory theory: similarity-based interference\n(Lewis & Vasishth, 2005;Vasishth & Lewis, 2006; Lewis,Vasishth\n& Van Dyke, 2006): intervening similar elements cause retrieval\ndifficulty\n- Production: Hawkins (1994; 2004): word-based distance metric.\n\nConsequence:\nNested structures are difficult\ncrosslinguistically\nEnglish:\nThe reporter [ who the senator attacked ] admitted the error.\nThe reporter [ who the senator [ who I met ] attacked ] admitted the error.\nI met the senator who attacked the reporter who admitted the error.\nJapanese:\nObasan-wa [ bebiisitaa-ga [ ani-ga imooto-o ijimeta ] to itta ] to omotteiru\naunt-top babysitter-nom older-brother-nom younger-sister-acc bullied that said that\nthinks\n\"My aunt thinks that the babysitter said that my older brother bullied my younger\nsister\"\nEasier: Bebiisitaa-ga [ ani-ga imooto-o ijimeta ] to itta ] obasan-ga to omotteiru\n\nLocality account of nesting complexity\nNested structures have longer distance dependencies than\nnon-nested structures.\n# The reporter [ who the senator [ who John met ] attacked ]\ndisliked the editor.\nJohn met the senator [ who attacked the reporter [ who disliked\nthe editor]].\nAn alternative account of nesting complexity: Nested structures\nhave parse states with more incomplete dependencies\n(e.g.,Yngve, 1960; Chomsky & Miller, 1963).\n\nLocality account of nesting complexity\nProblematic cases for incomplete-dependency approaches: Relative clauses (RCs) and sentence\ncomplements (SCs) (Cowper, 1976; Gibson, 1991):\nRC within SC: difficult, but processable\nThe fact [ that the employee [ who the manager hired ] stole office supplies ] worried the executive.\nSC within RC: much harder to process\n# The executive [ who the fact [ that the employee stole office supplies ] worried ] hired the manager.\nSame maximal number of incomplete dependencies, parsing left-to-right: 3 incomplete subject-verb\ndependencies, plus one incomplete filler-gap\nSolution: Distance-based integration accounts.The RC filler-gap dependency between \"who\"and its\nrole assigning verb (\"hired\") in (1) is more local than the RC filler-gap dependency between \"who\"\nand its role assigning verb (\"worried\") in (2).\n\nLocality account of nesting complexity\nThe lower complexity of examples nested pronouns (Bever, 1974; Kac, 1981)\nThe reporter who everyone that I met trusts said the president won't resign yet.\nA book that some Italian who I've never heard of wrote will be published soon by MIT Press.\n# The reporter [ who the senator [ who John met ] attacked ] disliked the editor.\nWarren & Gibson (2002), Experimental evidence:\nThe reporter who the senator who { you / John / the professor} met attacked disliked the\neditor.\nAverage Complexity Rating\n3.5\n2.5\nExperiment 6 Results\n1st / 2nd Pronoun\nProper Name\nDefinite Description\n\nLocality account of nesting complexity\nGibson (1998, 2000): Decay\nDiscourse-based decay hypothesis:The difficulty of integrating a new word h2 to h1 is proportional to\nthe number of discourse objects and events (nouns and verbs, roughly) which were introduced\nsince h1 was last processed. (cf.Warren & Gibson, 2002)\nHawkins: word-based decay hypothesis\nInterference of similar elements in the intervening structure:\nNP types: Gordon & colleagues\nPhrase structure similarity: Lewis,Vasishth, McElree and colleagues\nThe syntactic / semantic similarity of intervening NPs: More similar NPs, slower processing\nPrediction: Same kinds of NPs as head noun and embedded NP in an objected-extracted RC will\nlead to most processing difficulty, independent of the NP type\nClefts:\nIt was (the barber / John) that (the lawyer / Bill) saw in the parking lot.\nIt was (the barber / John) that saw (the lawyer / Bill) in the parking lot.\n\nGordon et al. 2001, Experiment 4\nClefts:\nIt was (the barber / John) that (the lawyer / Bill) saw in the parking lot.\nIt was (the barber / John) that saw (the lawyer / Bill) in the parking lot.\nCourtesy of\nJournal of\nExperimental\nPsychology. Used\nwith permission.\nSource: Gordon,\nPeter C., Randall\nHendrick, and\nMarcus Johnson.\n\"Memory\ninterference\nduring language\nprocessing.\"\nJournal of\nExperimental\nPsychology:\nLearning,\nMemory, and\nCognition 27, no.\n6 (2001): 1411.\n\nGibson & Thomas (1999): Structural forgetting:\nThe \"missing verb\" effect\n1. *The apartment that the maid who the cleaning service  \nsent over was well-decorated. 👍\n2. The apartment that the maid who the cleaning service  \nsent over cleaned was well-decorated. 👎\n(Frazier, 1985, reporting an intuition from Janet Fodor)\n\nGibson & Thomas (1999): Structural forgetting:\nThe \"missing verb\" effect\nGibson & Thomas (1999): acceptability judgements on:\nAll 3 VPs: The ancient manuscript that the graduate student who the new card catalog had\nconfused a great deal was studying in the library was missing a page.\nMissingVP1: The ancient manuscript that the graduate student who the new card catalog [] was\nstudying in the library was missing a page.\nMissingVP2: The ancient manuscript that the graduate student who the new card catalog had\nconfused a great deal [] was missing a page.\nMissingVP3: The ancient manuscript that the graduate student who the new card catalog had\nconfused a great deal was studying in the library []\nRatings (high is difficult):\nAll three VPs\nMissingVP1\nMissingVP2\nMissingVP3\n2.90 (.12)\n3.58 (.14)\n2.97 (.14)\n3.53 (.14)\n\nVasishth et al. (2010): Replication & extension\nto self-paced reading & German\nVasishth et al. (2010): SPR:\nAll 3 VPs: The carpenter who the craftsman that the peasant carried hurt supervised the\napprentice.\nMissingVP2: The carpenter who the craftsman that the peasant carried [] supervised the\napprentice.\nFaster RTs, better comprehension accuracy on MissingVP2 example\n\nChristiansen & Chater (1999)\nSimple recurrent network explanation of G&T missing verb effect\nIs this a good explanation? What is an explanation?\nOrthogonally, how would you test this hypthesis?\n\nGibson, Fedorenko & Mahowald (2014):\nNew paradigm for assessing syntactic WM:\nsentence completions of complex materials\nExpt 2: 60 participants on Mechanical Turk\nDesign: 4 critical conditions (6 items each) + 2 control conditions\nTask: Complete each preamble to make a complete sentence.\nORC-anim/SRC\nThe reporter who the professor who...\nORC-inan/SRC\nThe manuscript which the student who...\nSC/ORC\nThe fact that the professor who the diplomat...\nSC-verb/ORC\nThe rumor stating that the suspected mobster who the media...\nControl 1: ORC\nThe veterinarian who the...\nControl 2: SRC\nThe fencer who...\n\nExperiments 1 & 2\n60 participants on Mechanical Turk\nThe non-language task: Ravens Advanced Progressive Matrices (RAPM; Raven et\nal., 1988) (a very general fluid intelligence task; has been shown to correlate with many WM and\ncognitive control tasks, like Stroop & digit span and many others)\nTask: Choose a picture (out of 8 possibilities) that best completes the set.\n40 trials.\n\nExperiment 2 results:\nSentence completions\nFor analyses of the critical conditions we only\nincluded participants (n=55) who grammatically\ncompleted at least 5/6 of each of the control\nconditions (simple SRCs, simple ORCs).\nAs in Experiment 1, most incorrect completions\ninvolved omitting the middle VP:\nThe fact that the professor who the diplomat ...\n... had lunch with was true.\nThe manuscript which the writer who ...\n... is well-known wrote was excellent.\n... hangs out at Denny's sent in, was rejected.\n... was sad wrote, won the prize.\n... was sick was overdue.\n... was poor was rejected.\n... enjoyed opium was really very confusing.\n... collaborated with the original book's author, was\nfinished ahead of schedule.\n\nExperiment 2 results:\nCorrelation between completions and Ravens\nCorrelation between proportion of\ngrammatical completions and Ravens\naccuracy:\nn=55; r=.505; p<.001\nSignificant effect of RAPM score\npredicting the completion score\n(β=5.16; t=3.31; p=.0017), but no\nreliable effect of work time (β=-0.17;\nt=-0.68; p=.50) nor average filler length\n(β=0.30; t=1.31; p=.20).\n\nGibson, Fedorenko & Mahowald (2014)\nConclusions\n- Performance on a non-linguistic demanding task (a task assessing\ngeneral fluid intelligence) explains variance in people's ability to\ngrammatically complete syntactically complex materials.\n- In some cases language draws on highly domain-general\nresources (contra claims by Caplan & Waters, 1999, that syntactic\nworking memory is language-specific).\n\nVasishth et al. (2010): Replication & extension\nto self-paced reading & German\nVasishth et al. (2010): German SPR:\nAll 3 VPs: The carpenter who the craftsman that the peasant carried hurt supervised the\napprentice.\nMissingVP2: The carpenter who the craftsman that the peasant carried [] supervised the\napprentice.\nGerman readers have better comprehension the grammatical version, with all\n3 VPs!\nWhy the difference?\nFrank et al. (2015): a neural network model of syntactic predictions that gets\nthe German results when trained with German syntax, and the English results\nwhen trained with English syntax.\n\nVasishth et al. (2010): Replication & extension\nto self-paced reading & German\nWhy might German be different?\nFutrell & Levy (2017): head-final embedded clauses are much more\ncommon in German\n\nFutrell & Levy (2017): Information Locality\n- Information locality generalizes dependency locality\neffects and predicts that words that predict each other\n(have high mutual information) should be close.\n- Derived from a general theory of language\nunderstanding as rational inference with limited\nmemory resources.\nMutual information between two events: high when they\nco-occur frequently\nFutrell & Levy (2017, EACL)\n\ncontext\nFutrell & Levy (2017):\nLength-Dependent Noisy Context Surprisal\nnoisy context\nout\nJohn threw the old trash sitting in the kitchen\nJohn threw\n- Suppose we have an increasing noise rate the longer a\nword has been in memory.\n- When \"threw\" is far from \"out\", then it is less likely to reduce\nthe surprisal of \"out\": more likely to be affected by noise.\n- When it's closer, it's more likely to be available.\n- Noisy-context surprisal increases when words that predict\neach other are far apart.\n- We call this information locality.\n\nInformation Locality\n- Information locality: predicts processing difficulty when\nwords that predict each other (have high mutual\ninformation) are far apart.\n- How does this relate to dependency locality?\n- Hypothesis: Words in syntactic dependencies have high\nmutual information.\n- If this is true, then we can see dependency locality\neffects as a subset of information locality effects.\n- This is true in dependency corpora.\n\nDo Dependencies Have High Mutual Information?\nNOUN\nNOUN\n.\n?\n?\nNOUN\n.\n?\n?\n?\n?\n- Futrell & Levy calculated mutual information values over\npart-of-speech tags for pairs of words in the UD corpora.\n\nRelation\nHead-Dependent\nSister-Sister\nGrandparent-Dependent\nDo Dependencies Have High Mutual Information?\n0.4\n0.0\nX\nX\n0.2\n0.4\n0.2\n0.0\n.\nY\nY\nMutual information (bits)\n0.4\n0.2\n0.0\n0.4\n0.2\n0.0\n0.4\n0.2\n0.0\n.\n0.2\n0.4\nAncient Greek\nArabic\nBasque\nBulgarian\nCatalan\nChinese\nChurch Slavonic\nCroatian\nCzech\nDanish\nDutch\nEnglish\nEstonian\nFinnish\nFrench\nGalician\nGerman\nGothic\nHebrew\nHindi\nHungarian\nIndonesian\nIrish\nItalian\nJapanese\nKazakh\nLatin\nLatvian\nModern Greek\nNorwegian (B)\nPersian\nPolish\nPortuguese\nRomanian\nRussian\nSlovak\nSlovenian\nSpanish\nSwedish\nTamil\nTurkish\nUyghur\nVietnamese\nX\nY\n0.0\n\nStructural Forgetting\n1. *The apartment that the maid who the cleaning service  \nsent over was well-decorated. 👍\n2. The apartment that the maid who the cleaning service  \nsent over cleaned was well-decorated. 👎\n- Object-extracted RCs are uncommon in English (Roland et al.,\n2007).\n- English: the maid [that cleaned the apartment]   80%\nthe apartment [that the maid cleaned]   20%\n\ncontext\n\nNoisy-Context Surprisal Account of Structural Forgetting\nEnglish\nnoisy context\nkey word\nNOUN THAT NOUN THAT VERB VERB\nVERB\n#\nNOUN THAT VERB NOUN THAT NOUN VERB VERB\n- Correct for noise based on prior about the language.\n- Low probability for verb-final RCs in English,\n- so likely to make the wrong prediction.\n\nStructural Forgetting\n1. *The apartment that the maid who the cleaning service  \nsent over was well-decorated. 👍\n2. The apartment that the maid who the cleaning service  \nsent over cleaned was well-decorated. 👎\n- These contexts are more common in German than in English\n- German: das Dienstmadchen, [das die Wohnung reinigte]  \ndie Wohnung, [die das Dienstmadchen reinigte] \n- All RCs are head-final in German\n\ncontext\n\nNoisy-Context Surprisal Account of Structural Forgetting\nGerman\nnoisy context\nkey word\nNOUN THAT NOUN THAT VERB VERB\nVERB\n#\nNOUN THAT NOUN THAT NOUN VERB VERB\n- Correct noise based on prior about the language in German\n- Higher probability for verb-final RCs in German,\n- so more likely to make the right prediction.\n\nNoisy-Context Surprisal Account of Structural Forgetting\n- Probability that a context is remembered depends on\nits prior probability.\n- Noisy-context surprisal explains the behavior of\nthe RNN in Frank et al. (2016): the RNN is using a\nlossily compressed / noisy representation of\ncontext.\n- Same model that derives dependency locality effects\nalso derives language-specific memory effects.\n- The model has an explicit grammar (competence),\nbut cannot apply it correctly (performance).\nFutrell & Levy (2017, EACL)\n\n7 More Possible Projects\n9. Information theory and sentence production (Gibson & Thomas, 1999; Futrell &\nLevy, 2017)\nMaterials like (1) are so difficult to understand that people often accept an ungrammatical\nversion with the second verb phrase (VP2) omitted (2) (Gibson & Thomas, 1999):\n(1) The apartment that the maid who the service had sent was cleaning was well decorated.\n(2) missing VP2: The apartment that the maid who the service had sent was well decorated.\nReplication: replicate the Gibson & Thomas (1999) results using acceptability judgements.\nExtension: extend using the ideas from Futrell & Levy (2017).\n10. Domain specificity / generality: Individual differences measures of completing\ncomplex sentences and IQ (Gibson, Fedorenko & Mahowald, 2014)\nA long-standing question in the language literature is whether or not there is a domain-specific\npool of resources just for comprehending and producing language. Gibson, Fedorenko &\nMahowald (2014) presented a novel paradigm for assessing linguistic working memory based\non people's ability to grammatically complete complex sentences. They showed that this\nmeasure correlates with a measure of non-verbal IQ, suggesting that there is no domain-\nspecific pool of resources (or minimally, that there is some overlap in the pools of resources).\n\n7 More Possible Projects\n11. Discourse Structure and Constraints on English Relative Clauses (Ambridge &\nGoldberg, 2008)\nThe acceptability of certain complex relative clauses in English depends on the verbs\npresent. For example, the question \"What did John say that Mary stole from the store?\"\nseems well-formed, but the questions \"What did John realize that Mary stole from the\nstore?\" and \"What did John mumble that Mary stole from the store?\" are typically rated as\nless acceptable.\nAmbridge & Goldberg (2008) explore the idea that the unacceptability of the latter two\nsentences arises because of constraints on discourse structure. In the acceptable sentence,\nthe verb \"say\" makes it seem that the important information in the sentence is contained in\nthe following clause. In contrast, in the unacceptable sentences, the verbs \"realized\" and\n\"mumbled\" make the following clause seem like background information. They perform\nratings studies showing that the degree to which verbs imply that the following clause is\nbackground information strongly predicts the acceptability of relative clauses formed from\nthose clauses.\nWhile the results of this study are strong, they have not been extended to other grammatical\nphenomena in English or other languages.\n\n7 More Possible Projects\n12. Individual differences in quantifier scope\nThe sentence \"Every student didn't pass the test\" is ambiguous between a meaning where none of\nthe students passed the test, and one in which some but not all of the students passed the test.\nWhile one might think that the context could disambiguate the meaning, it appears that some people\nhave trouble overcoming a (possibly primed) meaning bias.\nContext biased to \"none\": Six little-known singers were each getting ready to release their second\nalbums. Their record labels expected that none of the unknown singers' new albums would sell one\nmillion copies. As expected, every singer's album didn't sell one million copies.\nContext biased to \"some but not all\": Three famous singers and three little-known singers were\neach getting ready to release their second albums. Their record labels expected that all of the\nfamous singers' new albums would sell one million copies, but that none of unknown singers' new\nalbums would sell one million copies. As expected, every singer's album didn't sell one million\ncopies.\nHow many singers' albums sold one million copies? 1) All\n2) Some\n3)None\nReplicate the lack of sensitivity to context that some participants obtain. Extend this in some\ninteresting way, possibly to try to figure out what kind of priming this is, and how you could make it\ndisappear.\n\n7 More Possible Projects\n13. Knowledge and Implicature: Context and the interpretation of quantifiers\nGoodman & Stuhlmuller (2013) observed that how one interprets \"some\" depends on the\ncontext. In a context where Laura has checked all vs some of the letters to her company,\nwhen she says \"some\" it changes the likely meaning:\nLetters to Laura's company almost always have checks inside. Today Laura needs to find\nout whether 3 of the letters have checks inside. Laura tells you on the phone: I have now\nlooked at 2 (or all 3) of the 3 letters, and given what I saw, I can tell you that some of today's\nletters have checks inside.\nEstimate (in percent) how likely it is that all 3 of the letters have checks inside.\nIn a context where Laura has looked at all 3, then \"some\" usually means some but not all.\nBut if she has only looked at 2 of the 3 letters, then this means some and possibly all.\nReplicate this, and extend it in some interesting way.\n\n7 More Possible Projects\n14. Sedivy (2003) color naming\nSedivy (2003) shows that speakers use color adjectives only when they are typically informative, and\nthat listeners reason on-line about speakers' intentions in using color adjectives by assuming that\nspeakers are trying to be informative. Her paper shows that people use color adjectives to modify\nnouns referring to objects (such as \"the blue cup\") when the object can conceivably come in many\ncolors, even when the color adjective is not necessary to disambiguate objects in immediate context.\nFor objects that do not vary freely in color (such as a yellow banana), people do not use color\nadjectives often. On the comprehension side, in a visual world paradigm, when subjects hear the\nword \"yellow\" they are likely to look AWAY from a yellow banana, since they do not think a speaker\nwould use the color adjective for that object. Project: color labeling over M Turk.\n15. People are more rational in L2 than in L1.\nKeysar et al (2012) observed that using a foreign language reduces decision-making biases. Four\nexperiments show that the framing effect disappears when choices are presented in a foreign\ntongue. Whereas people were risk averse for gains and risk seeking for losses when choices were\npresented in their native tongue, they were not influenced by this framing manipulation in a foreign\nlanguage. Two additional experiments showed that using a foreign language reduces loss aversion,\nincreasing the acceptance of both hypothetical and real bets with positive expected value.\nCosta et al (2014): extension to morality judgments.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms\n."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/aa15d3f4b91c4faca91f1f626e899b7d_MIT9_59jS17_lec8.pdf",
      "content": "Class 8:\nWhat makes a long-distance extraction\nunacceptable?\nTed Gibson\n9.59J/24.905J\n\nDifferent English constructions involving\n\"fronting\" of material probably involve\nWh-questions, Relative clauses, Topicalization\nWh-question: Who did Anthony ignore?\nEmbedded wh-question: Dan asked who Anthony ignored.\nRelative clause: Dan asked about the man who Anthony ignored.\nTopicalization: Susan, Anthony ignored.\nWh-question across a second clause:\nWho did Emily notice that Anthony ignored?\nEmbedded wh-question across a second clause:\nDan asked who Emily noticed that Anthony ignored.\nRelative clause across a second clause:\nDan asked about the man who Emily noticed that Anthony ignored.\nTopicalization across a second clause:\nSusan, Emily noticed that Anthony ignored.\n\nDifferent English constructions involving\n\"fronting\" of material probably involve\nIn addition to their superficial similarity, they obey the same kinds of structural\nconstraints, e.g., island phenomena\nNP-island, wh-question:\n? Dan asked who Emily heard the rumor that Anthony ignored [].\nNP-island, relative clause:\n? Dan asked about the man who Emily heard the rumor that Anthony ignored [].\nNP-island, topicalization:\n? Susan, Emily heard the rumor that Anthony ignored [].\nSubject-island, wh-question:\n? Dan asked who Emily heard that some friends of [] ignored Liz.\nSubject-island, relative clause:\n? Dan asked about the man who Emily heard that some friends of [] ignored Liz.\nSubject-island, topicalization:\n? Susan, Emily heard that some friends of [] ignored Liz.\nConjunct-island, wh-question:\n? Dan asked who Emily heard that Anthony ignored Liz and [].\nConjunct-island, relative clause:\n? Dan asked about the man who Emily heard that Anthony ignored Liz and [].\nConjunct-island, topicalization:\n? Susan, Emily heard that Anthony ignored Liz and [].\n\nDifferent English constructions involving\n\"fronting\" of material probably involve\nOne syntactic \"island\" for extraction that was discovered by Ross (1967) was the\nNP-island constraint, whereby it doesn't sound good to have a wh-question out of an\nNP. For example, while it's ok to extract out of an embedded clause as in (1), it's not\nso good when the embedding is part of a noun phrase:\n(1) What vegetable did the reporter believe that the president disliked?\n(2) ?* What vegetable did the reporter believe the fact that the president disliked?\nIt is thought that relative clauses such as in (3) have a similar structure as wh\nquestions:\n(3) The vegetable that the president dislikes is broccoli.\nProvide evidence from your judgments of relative clauses that suggests a similar\nisland effect in relative clauses as in (1) vs (2), and therefore a similar structure in\nrelative clauses as in wh-questions.\n\nLong-distance dependencies \nWhat makes a long-distance extraction unacceptable? \n(Ross, 1967; see Ambridge & Goldberg, 2008)\nSometimes long-distance extractions seem ok: \n1a.What does Susan think that John bought __?\n1b.What does Sarah believe that Susan thinks that John bought __?\n1c.What does Bill claim that Sarah believes that Susan thinks that John bought __?\n\nLong-distance dependencies \nWhat makes a long-distance extraction unacceptable? \n(Ross, 1967; see Ambridge & Goldberg, 2008)\nComplex NPs (both noun complements and relative clauses)\n*Who did she see the report that was about _? (cf. She saw the report that was about x)\nSubjects\n*Who did that Mary liked _ bother him?\n(cf.That Mary liked x bothered him)\nPresupposed adjuncts\n??What did she leave the movie because they were eating _?\n(cf. She left the movie because they were eating x)\nComplements of manner-of-speaking verbs\n??What did she whisper that he saw_?\n(cf. She whispered that he saw x)\nComplements of factive verbs\n??What did she realize that he saw_?\n(cf. She realized that he saw x)\n\nWhat is the nature of \"syntactic islands\"?\nPurely syntactic accounts, going back to Ross & Chomsky:\nE.g., the Subjacency Condition (Ross, 1967; Chomsky, 1973; cf. Chomsky, 1962; Ross, 1967;\nChomsky, 1986; Chomsky, 1995):\nNo rule may move a phrase from position Y to position X (or conversely) in:\n. . . X . . . [A . . . [B . . .Y . . .] . . . ] . . . X . . .\nWhere A and B are cyclic nodes.\n(1) * What do [ you wonder [whether John bought __ ]] ?\n(2) * Who did [ you criticize votes for [ the impeachment of __]]?\n\nWhat is the nature of \"syntactic islands\"?\nBUT: Non-syntactic factors like plausibility and working memory demands affect\nextraction difficulty (e.g., Deane, 1991; Kluender, 1992; Sag et al., 2007; Hofmeister & Sag,\n2010):\n(2) * Who did [ you criticize votes for [ the impeachment of __]]?\n(3) Who did [ you obtain votes for [ the impeachment of __]]?(Deane, 1991)\n(4) * Who did Emma doubt the report that we had captured __?\n(5) Which convict did Emma doubt a report that we had captured __?\nCan syntactic island effects be explained by resource complexity or other\nnon-syntactic factors like plausibility or coherence?\n\nSprouse Wagers & Phillips, 2012\nAgainst WM accounts of extraction unacceptability\nSprouse Wagers & Phillips, 2012 divide the accounts into 3 camps:\nReductionist accounts; E.g., working memory (WM) accounts (Kluender,\n1998; Hofmeister & Sag, 2010)\nGrammar-based accounts; E.g., syntax accounts (Ross, 1967;\nChomsky, 1973, 1986); Syntax / semantics accounts (Goldberg, 1995;\nAmbridge & Goldberg, 2008)\n\"Grounded\" accounts: WM accounts with grammaticization (e.g.,\nFodor, 1978; Hawkins, 1999)\n\nStructure Drives Islands?\n\nStructure Drives Islands?\n\nStructure Drives Islands?\n\nInformation-Structure based accounts \nAmbridge & Goldberg, 2008\nPerhaps bad extractions are cases where backgrounded information is\nextracted.\nPresupposition is one example of backgrounded information:\nA test of backgrounded information: Consider a sentence S with an\nembedded clause E. The main assertion of S is negated when S is negated.\n \n(1) Mary saw the report that was about Bill.\n(2) → (negation) Mary didn't see the report that was about Bill.\nThe main assertion of the sentence is negated: not seeing the report.\n\nInformation-Structure based accounts: BCI account \nAmbridge & Goldberg, 2008\n(1) Mary saw the report that was about Bill.\n(2) → (negation) Mary didn't see the report that was about Bill.\nIf the meaning of a negative form of S doesn't negate the meaning of E\nthen E is not the main assertion: it's presupposed or backgrounded\nThe meaning of the RC in (2) is still the report that was about Bill (not the\nreport that wasn't about Bill)\nTherefore, a relative clause (RC) is backgrounded information. Then\nmaybe extraction from backgrounded information is disallowed:\n*Who did Mary see the report that was about _?\nBackgrounded constructions are islands (BCI) account: Backgrounded\nconstituents may not serve as gaps in filler-gap constructions.\n\nInformation-Structure based accounts: BCI Account\nAmbridge & Goldberg, 2008\n(3) Mary thought that Bill liked Sue.\n(4) → (negation) Mary didn't think that Bill liked Sue.\nThis can mean:\nIt is not the case that Bill liked Sue.\nTherefore the embedded clause might be the main assertion in (3). So extraction of the embedded\nclause is possible:\n(5) Who did Mary think that Bill liked _?\n(6) Mary whispered that Bill liked Sue.\n(7) → (negation) Mary didn't whisper that Bill liked Sue.\nThis can't mean: It is not the case that Bill liked Sue.\nTherefore the embedded clause is not the main assertion in (6). So extraction of the embedded\nclause is not possible:\n(8) ?* Who did Mary whisper that Bill liked _?\n\nInformation-Structure based accounts \nAmbridge & Goldberg, 2008\nExperiment: Rate acceptability of long-distance extraction:\n(1) Who did Mary think that Bill liked _?\n(2) Who did Mary whisper that Bill liked _?\nCompare these ratings to ratings of how much one negative sentence implied another, for the\nsame embedding verbs:\nembedding verb: think\n(3) Mary didn't think that Bill liked Sue.\n(4) Bill didn't like Sue.\nembedding verb: whisper\n(5) Mary didn't whisper that Bill liked Sue.\n(6) Bill didn't like Sue.\n3 classes of Verbs:\nFactive verbs (realize, remember, notice, know): should not allow extraction\nmanner-of-speaking verbs (whisper, stammer, mumble, mutter): less likely to allow extraction\n'bridge verbs'' (say, decide, think, believe): most likely to allow extraction\n\nPredictions\n1. A purely syntactic subjacency account would expect all structurally identical\nsentences to behave identically, and thus would predict no systematic differences\nacross semantic verb classes.\n2. BCI account: a negative correlation between acceptability ratings and how much\nthe embedded clause can receive the negation, in interpretation.\n\nNegation Test Correlates with Unextractability across\nBridge Verbs (Ambridge & Goldberg, 2008)\n(c) Cognitive Linguistics. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/\nSource: Ambridge, Ben, and Adele E. Goldberg. \"The island status of clausal complements: Evidence in favor of an information structure explanation.\" Cognitive Linguistics 19,\nno. 3 (2008): 357-389.\n\nSprouse Wagers & Phillips, 2012\nAgainst WM accounts of extraction unacceptability\nSprouse Wagers & Phillips, 2012 divide the accounts into 3 camps:\nReductionist accounts; E.g., working memory (WM) accounts (Kluender,\n1998; Hofmeister & Sag, 2010)\nGrammar-based accounts; E.g., syntax accounts (Ross, 1967;\nChomsky, 1973, 1986); Syntax / semantics accounts (Goldberg, 1995;\nAmbridge & Goldberg, 2008)\n\"Grounded\" accounts: WM accounts with grammaticization (e.g.,\nFodor, 1978; Hawkins, 1999)\n\nSWP: Simplest reductionist WM theory\nAssumptions of the simplest WM theory of island effects\nCOMPONENT 1:There is a processing cost associated with the\noperations necessary to build long-distance WH-dependencies.\nCOMPONENT 2:There is a processing cost associated with the\noperations necessary to build the island structures.\nLINKING HYPOTHESIS: Processing costs are reflected in acceptability\njudgments such that higher costs lead to lower acceptability\nPrediction:A main effect of each component, but no interaction\nTest: STRUCTURE × GAP POSITION\na.Who __ thinks that John bought a car? NON-ISLAND | MATRIX\nb.What do you think that John bought __? NON-ISLAND | EMBEDDED\nc.Who __ wonders whether John bought a car? ISLAND | MATRIX\nd.What do you wonder whether John bought __? ISLAND | EMBEDDED\n\nSWP: Simplest reductionist WM theory\nThis doesn't work out: There is also an interaction\n(c) Language. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/\n\nSWP: Elaborated WM theory\nCore assumptions of the resource-limitation theory (Kluender & Kutas 1993)\nCOMPONENT 1:There is a processing cost associated with the operations\nnecessary to build long-distance WH-dependencies.\nCOMPONENT 2:There is a processing cost associated with the operations\nnecessary to build the syntactic structures that we call island structures.\nLINKING HYPOTHESIS: Processing costs are reflected in acceptability\njudgments such that higher costs lead to lower acceptability.\nSIMULTANEITY:These two (sets of) processes must be deployed simultaneously\nin sentences that give rise to island effects.\nLIMITED CAPACITY:There is a limited pool of processing resources available\nthat must be shared by all simultaneous processes.\nOVERLOAD:Additional unacceptability arises if the simultaneous processes\nnecessary to complete the parse require more resources than are available in\nthe limited pool.\n\nSWP: Elaborated WM theory: Interaction is the\ndifference of differences (DD) score\na.Who __ thinks that John bought a car? NONISLAND | MATRIX\nb.What do you think that John bought __? NONISLAND | EMBEDDED\nc.Who __ wonders whether John bought a car? ISLAND | MATRIX\nd.What do you wonder whether John bought __? ISLAND | EMBEDDED\nCalculating the difference of differences (DD) score with a sample set of mean ratings rating\na. D1 = (NONISLAND/EMBEDDED) - (ISLAND/EMBEDDED) (z-score units)\nWhat do you think that John bought __?\n0.5\nWhat do you wonder whether John bought __?\n-1.5\ndifference:\n2.0\nb. D2 = (NONISLAND/MATRIX) - (ISLAND/MATRIX)\nWho __ thinks that John bought a car?\nWho __ wonders whether John bought\nc. DD = D1 - D2 = 2.0 - 0.8 = 1.2\na car?\ndifference:\n1.5\n0.7\n0.8\nCritical evaluation of WM hypothesis:\nCorrelate DD score for an individual with their WM score on (a) a serial\nrecall task (Expt 1) and (b) a serial recall task and an n-back task (Expt 2)\n\nSWP: Predictions of two theories\nNOTE: these are not real data!\n(c) Language. All rights reserved. This content is excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/help/faq-fair-use/\n\nSWP Expt 1: \nInteraction ratings vs. serial recall scores\nWhether-island only:\n(c) Language. All rights reserved. This content is\nexcluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/\n\nSWP Expt 1: \nInteraction\nratings vs. serial\nrecall scores\n(c) Language. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/\n\nSWP Expt 2: \nWhether-island only:\nInteraction ratings vs. 3-back scores\n(c) Language. All rights reserved. This content is\nexcluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/\n\nSWP Expt 2: \nInteraction\nratings vs. 3\nback scores\n(c) Language. All rights reserved. This content is\nexcluded from our Creative Commons license.\nFor more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/\n\nOpen question in the syntax literature: \nThe domain-specificity / generality \nof extraction constraints\nSprouse,Wagers & Phillips (2012, Language):\nCorrelational study, using individual difference measures from two kinds of tasks:\nAcceptability ratings on extractions from islands\nA working memory measure (two tasks: n-back; serial recall)\nResult: No correlation whatsoever.\n\"We believe that the results of the experiments presented in this article provide strong\nsupport for grammatical theories of island effects because we can find no evidence of a\nrelationship between processing resource capacity and island effects.\"\nBUT: Absence of evidence is not evidence of absence\nNull correlation: how to interpret?\nWould we be surprised if we found no correlation between Island\nratings and participant height? Probably not.\nHow is this comparison similar / different?\n\n(Minor) Issues with Sprouse,Wagers & Phillips (2012)\n1. Very little data were collected for each participant (2 and 4\ntrials per condition in Expts 1 and 2, respectively).\n(maybe not a critical problem, but worrisome!)\n2. No plausibility control:\nIsland: What do you sneeze if the dog owner leaves open at night?\nSWP's long-distance extraction control for this example:\nWhat do you hope that the dog owner will leave open at night?\nHow do we know that the effects aren't due to plausibility differences?\n\nMajor Issue with Sprouse,Wagers & Phillips (2012)\nNo control to show that the complexity of materials that\nare thought to be complex due to WM considerations\ncorrelate with general WM measures\nMaybe there is a particular pool of resources just for\ncompositional language comprehension (Caplan & Waters,\n1999)\n\nNew design (Gibson & Scontras, 2013)\nNew design: 3 tasks:\nTask 1: Sentence completions of WM-complex materials: a general\nintelligence task, which correlates with IQ tasks (which also correlate with\nWM tasks)\nTask 2: Sentence acceptability of WM-complex materials\nTask 3: Sentence acceptability of islands.\nExamine the relationship between task 1 and task 2:\nIf they correlate (as SWP implicitly assume), then we can evaluate task 1's\ncorrelation with task 3 (SWP's question)\nIf not, then SWP's evaluation may not tell us anything about the relationship\nbetween WM and island acceptability.\n\nNew design (Gibson & Scontras, 2013)\n60 participants on M Turk (run twice, with similar results in each run)\n(1) a language / fluid intelligence measure that has been shown to correlate\nstrongly with fluid intelligence: complex sentence completion (Gibson,\nFedorenko & Mahowald, 2014); (WM correlates highly with fluid\nintelligence)\n(2) a rating study consisting of\nnested vs. non-nested sentences, a contrast thought to be related to WM\ndemands; and\nadjunct and NP islands, similar to SWP's, but with 8 trials per condition per\nparticipant, and with plausibility-matched control conditions.\nParticipants also answered 2 comprehension questions about each\nsentence.\n\nResult 1\nComplex sentence completion:\nReplicating previous results, we found stable\ncompletion rates within individuals but varying across\nindividuals. (cf. Gibson & Fedorenko, 2012; Gibson &\nThomas, 1999; Frazier, 1978; Christiansen & Chater,\n2001;Vasishth et al., 2008)\nLots of incomplete completions: ~50%\nSample completions:\nThe reporter who the professor who ...\n3 VPs:\n...taught English called was late to their appointment.\n...taught physics, was friends with asked me some\nquestions.\n2 VPs:\n...was dating him, thought that they were out of line.\n...was intelligent, didn't like each other.\n\nResult 2\nComplex sentence completion:\nThis measure correlated reliably with the ability\nto answer comprehension questions in the rating\nstudy (r=.510; p<.001).\nThe reporter who the professor who ...\nHigh WM / IQ participant typically completes with\n3 VPs:\n...taught English called was late to their appointment.\n...taught physics, was friends with asked me some\nquestions.\nLow WM / IQ participant typically completes with\n2 VPs:\n...was intelligent, didn't like each other.\n...was dating him, thought that they were out of line.\n\nResult 3\nReliable differences in ratings were observed for each contrast:\nnested vs. non-nested:\n2.12 vs. 3.17 out of 5; p<.001\nThe building which the architect who the contractor met designed was commissioned by the city.\nThe contractor met the architect who designed the building which was commissioned by the city.\nadjunct-island vs. pronoun-dependency:\n1.72 vs. 2.22; p<.001\nThe neighbor wonders what you sneeze if the dog owner leaves open at night.\nThe neighbor wonders what the thing is, such that you sneeze if the dog owner leaves it open at\nnight.\nNP-island vs. pronoun-dependency:\n1.89 vs. 2.27; p<.001\nJoe wondered what the chef heard the statement that Jeff baked.\nJoe wondered what the thing was, such that the chef heard the statement that Jeff baked it.\n\nResult 4\nCritically, we found no correlation\nbetween our IQ / WM measure and the\nnested vs. non-nested difference score in\nthe ratings (r =.052), suggesting that\nSWP's assumption about acceptability\nratings reflecting WM demands is not\nwarranted.\n\nResult 5\nReplicating SWP, we found no\ncorrelation between acceptability ratings\nfor island conditions and our IQ / WM\nmeasure.\nr = -.097 for adjunct-islands\nr = -.089 for NP-islands\nBut this is now unsurprising, given that\nwe also find no correlation with our\nWM measure for judgments based on\nnested vs. non-nested sentences\n\nGibson & Scontras (2013): \nSummary & Conclusions\nThere is no correlation between our IQ / WM measure and the nested vs.\nnon-nested difference score in the ratings (r =.052).\nMaybe there is a particular pool of resources just for\ncompositional language comprehension (Caplan & Waters,\n1999)\n\n7 More Possible Projects\n11. Discourse Structure and Constraints on English Relative Clauses (Ambridge &\nGoldberg, 2008)\nThe acceptability of certain complex relative clauses in English depends on the verbs\npresent. For example, the question \"What did John say that Mary stole from the store?\"\nseems well-formed, but the questions \"What did John realize that Mary stole from the\nstore?\" and \"What did John mumble that Mary stole from the store?\" are typically rated as\nless acceptable.\nAmbridge & Goldberg (2008) explore the idea that the unacceptability of the latter two\nsentences arises because of constraints on discourse structure. In the acceptable sentence,\nthe verb \"say\" makes it seem that the important information in the sentence is contained in\nthe following clause. In contrast, in the unacceptable sentences, the verbs \"realized\" and\n\"mumbled\" make the following clause seem like background information. They perform\nratings studies showing that the degree to which verbs imply that the following clause is\nbackground information strongly predicts the acceptability of relative clauses formed from\nthose clauses.\nWhile the results of this study are strong, they have not been extended to other grammatical\nphenomena in English or other languages.\n\n7 More Possible Projects\n12. Individual differences in quantifier scope\nThe sentence \"Every student didn't pass the test\" is ambiguous between a meaning where none of\nthe students passed the test, and one in which some but not all of the students passed the test.\nWhile one might think that the context could disambiguate the meaning, it appears that some people\nhave trouble overcoming a (possibly primed) meaning bias.\nContext biased to \"none\": Six little-known singers were each getting ready to release their second\nalbums. Their record labels expected that none of the unknown singers' new albums would sell one\nmillion copies. As expected, every singer's album didn't sell one million copies.\nContext biased to \"some but not all\": Three famous singers and three little-known singers were\neach getting ready to release their second albums. Their record labels expected that all of the\nfamous singers' new albums would sell one million copies, but that none of unknown singers' new\nalbums would sell one million copies. As expected, every singer's album didn't sell one million\ncopies.\nHow many singers' albums sold one million copies? 1) All\n2) Some\n3)None\nReplicate the lack of sensitivity to context that some participants obtain. Extend this in some\ninteresting way, possibly to try to figure out what kind of priming this is, and how you could make it\ndisappear.\n\n7 More Possible Projects\n13. Knowledge and Implicature: Context and the interpretation of quantifiers\nGoodman & Stuhlmuller (2013) observed that how one interprets \"some\" depends on the\ncontext. In a context where Laura has checked all vs some of the letters to her company,\nwhen she says \"some\" it changes the likely meaning:\nLetters to Laura's company almost always have checks inside. Today Laura needs to find\nout whether 3 of the letters have checks inside. Laura tells you on the phone: I have now\nlooked at 2 (or all 3) of the 3 letters, and given what I saw, I can tell you that some of today's\nletters have checks inside.\nEstimate (in percent) how likely it is that all 3 of the letters have checks inside.\nIn a context where Laura has looked at all 3, then \"some\" usually means some but not all.\nBut if she has only looked at 2 of the 3 letters, then this means some and possibly all.\nReplicate this, and extend it in some interesting way.\n\n7 More Possible Projects\n14. Sedivy (2003) color naming\nSedivy (2003) shows that speakers use color adjectives only when they are typically informative, and\nthat listeners reason on-line about speakers' intentions in using color adjectives by assuming that\nspeakers are trying to be informative. Her paper shows that people use color adjectives to modify\nnouns referring to objects (such as \"the blue cup\") when the object can conceivably come in many\ncolors, even when the color adjective is not necessary to disambiguate objects in immediate context.\nFor objects that do not vary freely in color (such as a yellow banana), people do not use color\nadjectives often. On the comprehension side, in a visual world paradigm, when subjects hear the\nword \"yellow\" they are likely to look AWAY from a yellow banana, since they do not think a speaker\nwould use the color adjective for that object. Project: color labeling over M Turk.\n15. People are more rational in L2 than in L1.\nKeysar et al (2012) observed that using a foreign language reduces decision-making biases. Four\nexperiments show that the framing effect disappears when choices are presented in a foreign\ntongue. Whereas people were risk averse for gains and risk seeking for losses when choices were\npresented in their native tongue, they were not influenced by this framing manipulation in a foreign\nlanguage. Two additional experiments showed that using a foreign language reduces loss aversion,\nincreasing the acceptance of both hypothetical and real bets with positive expected value.\nCosta et al (2014): extension to morality judgments.\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/67bded72e32736334ee11313ca81725f_MIT9_59jS17_lec9.pdf",
      "content": "Pragmatics:\nThe use of referring expressions\nTed Gibson\n9.59J / 24.905J\n\nPragmatics: non-literal language\nContrastive Inferences\nQ: What time is it?\nA: Some people are already leaving.\n- It's late.\nQ: How is the party?\nA: Some people are already leaving.\n- The party isn't very good.\n- When are inferences / implicatures computed?\n- What aspects of the context enter into their computation?\n\nPragmatics: non-literal language - -\n-\nGricean Maxims: Cooperative conversation.\nØ\nViolating a maxim leads to an implicature\n-\nImplicature: an inference whose source is a linguistic expression\n-\nImplicatures and on-line sentence processing\nØ\nWhen do people compute contrast sets associated with referring\nexpressions? Test cases:\n- Contrast sets associated with scalar adjectives like \"big\" / \"small\" and\nnon-scalar adjectives like materials (e.g., \"plastic\") and color (e.g.,\n\"red\")\n- Scalar implicatures associated with determiners like \"some\" vs. \"all\"\n\nGrice's Maxims\nFour conversational maxims for a cooperative speaker:\n(1) Maxim of Quantity:\n-\nMake your contribution as informative as is required\n-\nDo not make your contribution more informative than is required\nIn a context where all of the students passed the test.\nSome of the students passed the test.\nIn a context with only one cup:\nPass me the cup.\n?? Pass me the tall blue cup that's made out of plastic.\n(2) Maxim of Quality:\n-\nDo not say that which you believe to be false\n-\nDo not say that for which you lack evidence\n(3) Maxim of Relation:\n-\nSay only what is relevant for the current purposes of the conversation.\n(4) Maxim of Manner:\n-\nBe brief but avoid ambiguity or obscurity of expression.\n\nGrice's Maxims\n- As long as the speaker adheres to the cooperative\nprinciple, he/she can disobey the maxims\nintentionally.\nØDeliberate violation of a maxim can give rise to an\nimplicature.\nØImplicature: exploiting the cooperative principle to\nconvey more information than is actually contained in\nan utterance.\nØHyperbole, sarcasm, understatement are all violations of\nQuality maxim.\n\nViolating Grice's Maxims\n- Letter of recommendation for graduate school\nØDear Sirs, Mr. X's command of English is excellent, his\nattendance at tutorials has been regular, and his family\nis charming. - Yours, Professor Y.\nViolation of the maxim of quantity.\nØA: John doesn't seem to have a girlfriend these days.\nB: He's been driving up to New York every weekend.\nViolation of the maxim of relation and / or manner.\n\nPragmatics: non-literal language - -\n-\nGricean Maxims: Cooperative conversation.\nØ\nViolating a maxim leads to an implicature\n-\nImplicature: an inference whose source is a linguistic expression\n-\nImplicatures and on-line sentence processing\nØ\nWhen do people compute contrast sets associated with referring\nexpressions? Test cases:\n- Contrast sets associated with scalar adjectives like \"big\" / \"small\" and\nnon-scalar adjectives like materials (e.g., \"plastic\") and color (e.g.,\n\"red\")\n- Scalar implicatures associated with determiners like \"some\" vs. \"all\"\n\nContrast sets and referring expressions:\nModifiers; e.g., Adjectives\nDependency Between Restrictive Modification\nand Contextual Contrast\nCan you pass Tim the tall cup?\n$!x[cup(x) & tall(x)]\na$x[cup(x) & ¬tall(x)]\nreference set\ncontrast set\n\nSedivy, Chambers, Tanenhaus, & Carlson (1999)\n(c) Elsevier. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/ Source: Sedivy, Julie C., Michael K. Tanenhaus, Craig G. Chambers, and Gregory\nN. Carlson. \"Achieving incremental semantic interpretation through contextual representation.\" Cognition 71, no. 2\n(1999): 109-147.\n\nSedivy, Chambers, Tanenhaus, & Carlson (1999):\n\"Pick up the small basket\"\n\nSedivy, Chambers, Tanenhaus, & Carlson (1999):\n\"Pick up the small basket\"\n\nSedivy, Chambers, Tanenhaus, & Carlson (1999)\n(c) Elsevier. All rights\nreserved. This\ncontent is excluded\nfrom our Creative\nCommons license.\nFor more\ninformation, see\nhttp://ocw.mit.edu/\nhelp/faq-fair-use/\nSource: Sedivy,\nJulie C., Michael K.\nTanenhaus, Craig G.\nChambers, and\nGregory N. Carlson.\n\"Achieving\nincremental\nsemantic\ninterpretation\nthrough contextual\nrepresentation.\"\nCognition 71, no. 2\n(1999): 109-147.\n- Contrast Effect: Eye-movements converge more quickly on\nthe target and there are fewer looks to the competitor in the\npresence of a contrast set.\n\nTwo Explanations for Contrastive Inferences\n(1) Gricean Account (Pragmatic account)\n-\nContrastive inferences arise because the use of a restrictive modifier is\nembedded in a collaborative communicative context.\n-\nQuantity-2: Don't make your contribution more informative than is\nrequired for the purposes of the present exchange.\n- . The hearer notes that the speaker chose a modified form rather than\nan unmodified form to refer to an entity. The inclusion of the modifier\nis most easily made informative by attributing to it a distinguishing\nfunction.\n\nTwo Classes of Explanation for Contrastive\nInferences\n(2) Form-Based Account (Semantic account: literal meaning)\n-\nContrastive inference is closely tied to conventional meaning of\nrestrictively modified NPs or to the lexical class of the modifier.\n-\nScalar adjectives contain a variable assigned by a contextually\nrelevant comparison class (Seigel, 1980; Bierwisch, 1987)\n-\nMinimizes the amount of information that is accessed in making\ncontrastive inferences\n\nTests of the theories\n- Testing the form-based account: The contrast effect should\ndisappear if a non-scalar adjective is used. E.g., a color\nadjective, or a material adjective.\n- Colors: \"pick up the blue cup\", with a blue and red cup in the\ndisplay. (as well as a competitor blue object, in order to control\nfor the fact that people are incremental in their eye-gazes.)\n- Results from colors:\nThe contrast effect disappears! -\n- Is this support for the form-based account?\nØ Yes, but there is an alternative Gricean account.\n\nTests of the theories\n- Surprising result for the form-based account:\n- Materials: \"pick up the plastic cup\", with a\nplastic and glass cup in the display.\n- Results from materials:\nThe contrast effect re-appears!\n- This contradicts the prediction of the form-\nbased account\n\nNew Gricean theory (Sedivy, 2003; cf.\nLevinson, 2000)\n-\nQuantity-2: Don't make your contribution more informative than is\nrequired for the purposes of the present exchange.\n- .\nThe hearer notes that the speaker chose a modified form rather than\nthe simple, default form to refer to an entity. The inclusion of the\nmodifier is most easily made informative by attributing to it a\ndistinguishing function.\n(1) Neo-Gricean View (Conservative)\nEarly contrastive-inferences are only sensitive to whether or not the\nspeaker elaborates on a default form. (cf. Levinson, 2000)\n- The baseline is the default form: the way that people would describe\nthe situation with no contrasting information.\n\nNew Gricean theory (Levinson, 2000)\n- Differences in default forms:\n- Colors are often produced along with the head noun in describing\nan object (Sedivy, 2003).\n- Materials and scalar adjectives are not.\n- Thus, the presence of a material or scalar adjective provides\nsuggestive information to the listener that there is a contrasting\nobject in the relevant dimension. Colors do not provide this\ninformation.\n\nPredictions of the neo-Gricean view\n1. If a color term is not normally produced when\ndescribing an object, then the contrast effect\nshould re-emerge.\nSedivy (2003): \"Pick up the yellow banana\", in\nthe context of a yellow banana and a green\nbanana\n\nPredictions of the neo-Gricean view\n2. If the listener knows that the speaker is not\nreliable in his / her productions, then looks to\nthe contrasting elements may disappear.\nGrodner et al. (2003): This prediction is realized.\n\nPragmatics: non-literal language - -\n-\nGricean Maxims: Cooperative conversation.\nØ\nViolating a maxim leads to an implicature\n-\nImplicature: an inference whose source is a linguistic expression\n-\nImplicatures and on-line sentence processing\nØ\nWhen do people compute contrast sets associated with referring\nexpressions? Test cases:\n- Contrast sets associated with scalar adjectives like \"big\" / \"small\" and\nnon-scalar adjectives like materials (e.g., \"plastic\") and color (e.g.,\n\"red\")\n- Scalar implicatures associated with determiners like \"some\" vs. \"all\"\n-\nUse of referring expressions in encoding perspective: what's\nold / new: common ground vs. priveleged ground\n\nThe use of referring expressions in\nencoding perspective\n- . Privileged ground - knowledge that is possessed by\none interlocutor and not the other (and mutually\naccepted as such)\n- Common ground - knowledge that is possessed by\nboth interlocutors (and mutually accepted as such)\n\nPerspective required for\n- Formulating and interpreting assertions\n- Asking and interpreting questions\n- Arriving at implicated meanings\n- Using referring expressions\n- Etc.\n- When (and how) does perspective information become\navailable?\n\nTwo views - - - - -\n- View 1: Initial Egocentricity\nØ Knowledge in someone else's head is heterogeneous, unbounded,\nand potentially cumbersome\nØ Maybe process from own perspective initially\n- View 2: Initial Perspective Taking\nØ Humans have tremendous social competency (cf. Baldwin,\nTomasello)\nØ Interlocutor's perspective is extremely useful\nØ Maybe immediately integrate interlocutor's perspective\nwith one's own\n\nEvidence for Egocentricity\nKeysar, Barr, Balin & Brauner 2000\nPick up the small candle\n- No early effect of perspective\n- Privileged object considered first\n(c) Psychological Science. All rights reserved. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/help/faq-fair-use/ Source: Keysar, Boaz, Dale J. Barr, Jennifer A. Balin, and Jason S. Brauner. \"Taking\nperspective in conversation: The role of mutual knowledge in comprehension.\" Psychological Science 11, no. 1 (2000): 32-38.\n\nEvidence for Egocentricity\nKeysar, Barr, Balin & Brauner 2000\nPick up the small candle\n- No early effect of perspective\n- Privileged object considered first\n- BUT privileged object is the best fit for the description\n(c) Psychological Science. All rights reserved. This content is excluded from our Creative Commons license. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-use/ Source: Keysar, Boaz, Dale J. Barr, Jennifer A. Balin, and Jason S. Brauner. \"Taking perspective in\nconversation: The role of mutual knowledge in comprehension.\" Psychological Science 11, no. 1 (2000): 32-38.\n\nEvidence for Perspective Taking\nHanna, Tanenhaus & Trueswell 2003; cf. Nadig & Sedivy 2002\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with permission. Source: Hanna, Joy E., Michael K. Tanenhaus, and\nJohn C. Trueswell. \"The effects of common ground and perspective on domains of referential interpretation.\" Journal of Memory and\nLanguage 49, no. 1 (2003): 43-61.\n\nEvidence for Perspective Taking\nHanna, Tanenhaus & Trueswell 2003; cf. Nadig & Sedivy 2002\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with permission. Source: Hanna, Joy E., Michael K. Tanenhaus, and John\nC. Trueswell. \"The effects of common ground and perspective on domains of referential interpretation.\" Journal of Memory and Language\n49, no. 1 (2003): 43-61.\n\nSize adjectives\n\"pick up the big duck\"\n- Faster to fixate on the target and\nless likely to fixate on a competitor\nwhen a contrast is present\n- Difference even before the noun is\ndisambiguated (Sedivy et al 1999)\n\nHeller, Grodner & Tanenhaus (2008): Experiment 1\n2 (1 or 2 contrasts) X 2 (shared vs. privileged ground)\n\"pick up the big duck\"\none shared\none privileged\nearly\nearly\ntwo shared\ntwo privileged\nlate\nearly or late?\n\nExperiment 1\n\"pick up the big duck\"\nNo global ambiguity: the instruction is disambiguated at the noun.\nThe use of a size adjective is felicitous in all conditions.\nThe competitor is shared in all conditions: a potential referent.\n\nMethods\n-\nInteractive task to make a configuration look like a diagram\n-\nParticipants were addressees, confederate was speaker\n-\nTold the confederate was an RA who was naive (True)\n-\nOnly the first description used by the RA was scripted\n-\n16 subjects, 16 stimuli, Latin square design, 32 filler items\n-\n2 X 2 crossing number of contrasts with perspective\n\n\"pick up the big duck\"\ntwo contrasts\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-200\n-100\ntime (ms)\nproportion of fixations\ntarget X\ncompetitor Y\nbig\nduck\n\n\"pick up the big duck\"\none contrast\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-200\n-100\ntime (ms)\nproportion of fixations\ntarget X\ncompetitor Y\nbig\nduck\n\n\"pick up the big duck\"\nproportion of fixations\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\nbig\nduck\ntarget X\ncompetitor Y\ncontrast to X\ndistractor\none privileged #\n0.3\n0.2\n0.1\n-200\n-100\ntime (ms)\n\n\"pick up the big duck\"\ntwo privileged\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-200\n-100\ntime (ms)\nproportion of fixations\ntarget X\ncompetitor Y\ncontrast to X\ncontrast to Y\nbig\nduck\n\n\"pick up the big duck\"\ntwo privileged\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-200\n-100\ntime (ms)\nproportion of fixations\ntarget X\ncompetitor Y\nbig\nduck\n\nProportion of Fixations to Target vs. Competitor\n\nHeller, Grodner & Tanenhaus (2008): Conclusions\nPerspective information integration:\n- ... happens in real time.\n- ... even when there is no trigger (like global ambiguity)\n- The status of the competitor is modulated by the shared vs.\nprivileged status of its contrast.\n\nConclusions\n- Perceivers don't\n- ignore perspective\n- fully adopt the speaker's perspective\n- use common ground as the primary referential domain\n- Perceivers do integrate speaker knowledge into their own\nperspective\n- Perspective information is just one of several factors that influence\nthe resolution (and generation) of reference\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-59j-lab-in-psycholinguistics-spring-2017/7f436994c1a1ede4bc157d6315360f13_MIT9_59jS17_lec10.pdf",
      "content": "The on-line computation of\npragmatic information\nTed Gibson\n9.59J / 24.905J\n\nConstraint interaction: Time course of\ninformation use\nThe immediate effects of:\n- Plausibility: Ferreira & Clifton (1986) vs. Trueswell, Tanenhaus &\nGarnsey (1994)\n- Context: Tanenhaus et al. (1995); Trueswell et al. (1999)\n- Pragmatics: The computation of non-literal language:\nimplicatures (Huang & Snedeker & 2008; Grodner et al. 2008;\nSedivy et al. 1999; Keysar et al. 2000; Heller et al. 2008)\n\nTime course of information use:\nsyntactic rules vs. plausibility\nOld research question: Does syntactic structure\nprocessing take place before other levels of\nsentence processing? (perhaps: priority of bottom-\nup information source use: lexical and syntactic\ninformation first? Frazier, 1978)\nFramed in terms of modularity: is syntactic\nprocessing modular, so that it is insulated from\nother levels of analysis, such as real-world\nplausibility?\n\nFerreira & Clifton (1986)\nEye-tracking investigation of the main-verb (MV) / reduced-\nrelative (RR) ambiguity, manipulating the plausibility of the\ninitial NP as agent of the MV.\nMV rule:\nS - NP VP\n(High frequency rule) -\nRR rule:\nNP e NP Rel-clause-VP\n(Low frequency rule)\nThe (evidence / defendant) examined by the lawyer turned\nout to be unreliable.\nUnambiguous controls:\nThe (evidence / defendant) that was examined by the lawyer\nturned out to be unreliable\n\nFerreira & Clifton (1986)\nThe evidence (that was) examined by the lawyer turned out\nto be unreliable.\nSyntax-first (\"modularity\") predictions:\n1. slow at \"by the lawyer\": syntactic reanalysis.\nNon-modularity predictions: no difference between\nambiguous and unambiguous controls in any region.\nAnimate initial noun control:\nThe defendant (that was) examined by the lawyer turned out\nto be unreliable.\n\nFerreira & Clifton (1986)\nexamined\nby the lawyer\nAnimate ambig.\n33.3\n40.4\nAnimate unambig\n31.9\n30.7\nInanimate ambig\n37.7\n38.4\nInanimate unambig 30.1\n30.3\nResults: First pass times (msec/character)\nThese results support the modularity theory.\n\nTrueswell, Tanenhaus & Garnsey, 1994\nProblems in Ferreira & Clifton's items:\nHalf (8/16) of the inanimate items weren't\nimplausible agents:\nThe car towed by the truck ...\n(cf. The car towed the trailer.)\n\nTrueswell, Tanenhaus, and Garnsey (1994): Experiment with better items.\nMean first pass times\nCourtesy of\nElsevier, Inc.,\nhttp://www.scienc\nedirect.com. Used\nwith permission.\nSource:\nTrueswell, John\nC., Michael K.\nTanenhaus, and\nSusan M.\nGarnsey.\n\"Semantic\ninfluences on\nparsing: Use of\nthematic role\ninformation in\nsyntactic\nambiguity\nresolution.\"\nJournal of\nmemory and\nlanguage 33, no.\n3 (1994): 285.\n\nTrueswell, Tanenhaus, and Garnsey (1994): mean\nsecond-pass times\nCourtesy of\nElsevier, Inc.,\nhttp://www.scienc\nedirect.com. Used\nwith permission.\nSource:\nTrueswell, John\nC., Michael K.\nTanenhaus, and\nSusan M.\nGarnsey.\n\"Semantic\ninfluences on\nparsing: Use of\nthematic role\ninformation in\nsyntactic\nambiguity\nresolution.\"\nJournal of\nmemory and\nlanguage 33, no.\n3 (1994): 285.\n\nTrueswell, Tanenhaus & Garnsey, 1994\nConclusion: Plausibility and lexical frequency are\nused as soon as can be measured in resolving\nambiguity in on-line sentence processing.\nThis is evidence against the syntax-first\nmodularity hypothesis:\nPlausibility information may be available\nimmediately\n\nLanguage:\nInformation sources and constraints\nCurrent Context (Crain & Steedman, 1985; Altmann & Steedman, 1988;\nTanenhaus et al., 1995): visual or linguistic\nAmbiguity:\nThere were two defendants, one of whom the lawyer ignored entirely, and\nthe other of whom the lawyer interrogated for two hours.\nThe defendant examined by the lawyer turned out to be unreliable.\n\nMonitoring visual eye-movements while listening to spoken instructions\n(Tanenhaus et al., 1995; Trueswell et al., 1999)\n1-referent context: \"Put the hippo on the blanket into the basket.\"\nMany looks to the incorrect target %\n\nMonitoring visual eye-movements while listening to spoken instructions\n(Tanenhaus et al., 1995; Trueswell et al., 1999)\n2-referent context: \"Put the bear on the plate into the box.\"\nNo looks to the incorrect target %\n\nPragmatics: non-literal language - -\n-\nGricean Maxims: Cooperative conversation.\nØ\nViolating a maxim leads to an implicature\n-\nImplicature: an inference whose source is a linguistic expression\n-\nImplicatures and on-line sentence processing\nØ\nWhen do people compute contrast sets? Test cases:\n- Contrast sets associated with scalar adjectives like \"tall\" and non-\nscalar adjectives like materials (e.g., \"plastic\") and color (e.g., \"red\")\n- Scalar implicatures associated with determiners like \"some\" vs. \"all\"\n\nGrice's Maxims\nFour conversational maxims for a cooperative speaker:\n(1) Maxim of Quantity:\n-\nMake your contribution as informative as is required\n-\nDo not make your contribution more informative than is required\nIn a context where all of the students passed the test.\n?? Some of the students passed the test.\nIn a context with only one cup:\nPass me the cup.\n?? Pass me the tall blue cup that's made out of plastic.\n(2) Maxim of Quality: e.g., when your friend Paul has betrayed you in some way: \"Paul\nis a fine friend\"\n-\nDo not say that which you believe to be false\n-\nDo not say that for which you lack evidence\n(3) Maxim of Relation:\n-\nSay only what is relevant for the current purposes of the conversation.\n(4) Maxim of Manner:\n-\nBe brief but avoid ambiguity or obscurity of expression.\n\nGrice's Maxims\n- As long as the speaker adheres to the cooperative\nprinciple, he/she can disobey the maxims\nintentionally.\nØDeliberate violation of a maxim can give rise to an\nimplicature.\nØImplicature: exploiting the cooperative principle to\nconvey more information than is actually contained in\nan utterance.\nØHyperbole, sarcasm, understatement are all violations of\nQuality maxim.\n\nHuang & Snedeker (2008); Grodner et al. (2008):\nWhen do people compute non-literal meaning in scalars? $\nThe girl has some of the balloons\nSpeaker:\nsome but not all\nPRAGMATIC, NON-LITERAL\nSentence:\nsome and possibly all\nLITERAL\n... in fact she has all of them.\n✔\n... in fact she has none of them. ✼\nTest for literal vs. non-literal interpretation: a non-literal\ninterpretation can be contradicted (\"cancelled\"), and still\nresult in a plausible meaning in the context. A literal\ninterpretation cannot be contradicted.\n\nLiteral meaning of \"some\"\nSome of the students passed the test.\nMeaning1: some and possibly all\nMeaning2: some and not all\nWhy not meaning2? Many contexts when it must be meaning1:\ne.g., Interrogatives:\nDid some of the students pass the test?\nI wonder if some of the students passed the test.\nPossible worlds:\nThe test is really hard.\nIf some of the students pass the test, then the teacher will get a bonus.\nWe need meaning1 for these situations. Simplest theory: one meaning for\n\"some\", meaning1. Other meanings come from inference in particular\nsituations.\n\nScalar Inference\n- Grice's quantity-1:\nØMake your contribution as informative as is required\nfor the current purposes of the exchange.\nØReasoning counterfactually\nØImputing intention, perspective to speaker\n- Implicational scale\n<all, most, many, some>\n- Potentially difficult\n\nHow do perceivers compute scalar\ninferences in real time?\n- 2-stage models (Sperber & Wilson 1995, Bott & Noveck 2004;\nBreheny et al. 2006, Huang & Snedeker 2006, 2008)\nConversational Context\nLanguage\nLiteral\nSpeaker Meaning\nInput\nDecoding\nPragmatic\nInferencing\n- Literal meaning computed before pragmatic\n\nHow do perceivers compute scalar\ninferences in real time?\n- Direct Access Models (Gibbs 1984, Gildea & Glucksberg 1983)\nConversational Context\nLanguage\nSemantic\nSpeaker meaning\nInput\nDecoding\nPragmatic\nInferencing\n- Pragmatic meaning computed earlier / immediately\n\nScalar Implicature (SI): (\n\"The girl has some of the balloons.\"\nsemantic\nThe girl has some, and\npossibly all, of the balloons.\ninterpretation:\npragmatic\nThe girl has some, but not\ninterpretation:\nall, of the balloons.\nQuestions:\nHow rapidly is the\npragmatic\ninterpretation\ngenerated?\nIs there evidence\nthat literal is\ncomputed first?\n\n\"Point to the girl that has some of the socks.\"\nHuang & Snedeker (\nCourtesy of\nElsevier, Inc.,\nhttp://www.science\ndirect.com. Used\nwith permission.\nSource: Huang, Yi\nTing, and Jesse\nSnedeker. \"Online\ninterpretation of\nscalar quantifiers:\nInsight into the\nsemantics-\npragmatics\ninterface.\"\nCognitive\npsychology 58, no.\n3 (2009): 376-\n415.\n- One girl has some of something (socks); the other girl has all of the soccer balls. (\n- \"Socks\" and \"soccer balls\" share the same onset.\n\nSome\nAll\nChance = 50%\n\"Point to the girl that has some of the socks/all of the soccer balls.\"\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with\npermission. Source: Huang, Yi Ting, and Jesse Snedeker. \"Online\ninterpretation of scalar quantifiers: Insight into the semantics-\npragmatics interface.\" Cognitive psychology 58, no. 3 (2009): 376-\n415.\n- Looks to pragmatic target\nfor \"some\" delayed\n- Looks to literal target for\n\"all\" immediate\nHuang & Snedeker\n\nPotential Challenges to H&S Conclusion that Literal\nPrecedes Pragmatic\n\"Point to the girl that has some of the socks.\"\n\"Point to the girl that has all of the soccer\n1. The \"all\" target is more\nballs\"\nvisually salient\n2. Cue to SI doesn't arrive until\nthe partitive\n\"Point the girl that has some socks\"\n3. \"Some\" may require a more\ncomplex comparison\nHuang & Snedeker\nCourtesy of Elsevier, Inc., http://www.sciencedirect.com. Used with\npermission. Source: Huang, Yi Ting, and Jesse Snedeker. \"Online\ninterpretation of scalar quantifiers: Insight into the semantics-\npragmatics interface.\" Cognitive psychology 58, no. 3 (2009): 376-\n415.\n\nGrodner et al. (2010):\nExperiment 1\n1. Included none as an additional literal control\n2. some of --> summa\nall of --> alla\nnone of --> nunna\n3. Included a baseline summa-late condition to establish\nwhether SI is computed prior to noun point-of\ndisambiguation\n\n\"There are 4 balls,\n4 planets and 4\nballoons.\"\n(c) Sources Unknown. All rights\nreserved. This content is excluded\nfrom our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-\nuse/\n\nConditions:\n- summa-early\n- alla\n- nunna\n(c) Sources Unknown. All rights\nreserved. This content is excluded\nfrom our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-\nuse/\nClick on the girl\nwho has...\nsumma the balls\nalla the balloons\nnunna the items\n\nClick on the girl\nwho has summa\nthe balls\n(c) Sources Unknown. All rights\nreserved. This content is excluded\nfrom our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-\nuse/\n\nCondition\n- summa-late $\nClick on the girl\nwho has summa\nthe balls\n(c) Sources Unknown. All rights\nreserved. This content is excluded\nfrom our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-\nuse/\n\nExp 1 Predictions: Two Stage Model\n\nExp 1 Predictions: 1.5 Stage/Direct Access Model\n\nExp 1 Results\n\nExp 1 Results\n\nGrodner et al. (2010):\nExp 1 Conclusions\n- No evidence for delay of pragmatic relative to\nliteral controls\nØScalar implicature is generated prior to point of\ndisambiguation of the Noun\nØTarget ID for summa was no slower than nunna, alla\n- Pragmatic meaning accessed immediately\n\nWhere does this leave us?\n- Grodner et al. (2010): No evidence for delay of\npragmatic relative to literal controls\n- Huang & Snedeker (2009): \"Some\" is slower than\n\"all\" by 600-800 msec (!!)\n- How do we reconcile these?\n- Degen & Tanenhaus (2015): It may have to do\nwith the expected alternative quantifiers\n\nDegen & Tanenhaus (2015)\n(c) Cognitive Science. All rights\nreserved. This content is excluded\nfrom our Creative Commons\nlicense. For more information, see\nhttp://ocw.mit.edu/help/faq-fair-\nuse/ Source: Degen, Judith, and\nMichael K. Tanenhaus. \"Processing\nscalar implicature: A\nconstraint-based approach.\"\nCognitive science 39, no. 4 (2015):\n667-710.\n\nDegen & Tanenhaus (2015)\n- Experiment 1: replicate Grodner et al. (2010)\nusing just quantifiers \"some\" and \"all\" and \"none\"\n(no numerals): early looks to \"some\" because no\nother terms are expected there\n- Experiment 2: replicate Huang & Snedeker (2009)\nusing \"some\", \"all\", \"two\", \"three\": slow looks to\n\"some\" because numbers are expected\n\n9.59J/24.905J Lab in Psycholinguistics\nSpring 2017\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}