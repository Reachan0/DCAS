{
  "course_name": "Parallel Computing",
  "course_description": "This is an advanced interdisciplinary introduction to applied parallel computing on modern supercomputers. It has a hands-on emphasis on understanding the realities and myths of what is possible on the world’s fastest machines. We will make prominent use of the Julia Language, a free, open-source, high-performance dynamic programming language for technical computing.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Computer Design and Engineering",
    "Theory of Computation",
    "Mathematics",
    "Applied Mathematics",
    "Computation",
    "Linear Algebra",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Computer Design and Engineering",
    "Theory of Computation",
    "Mathematics",
    "Applied Mathematics",
    "Computation",
    "Linear Algebra"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\n18.06 Linear Algebra\nor\n18.701 Algebra I\n.\n\nDescription\n\nThis course is an advanced interdisciplinary introduction to applied parallel computing on modern supercomputers. It has a hands-on emphasis on understanding the realities and myths of what is possible on the world's fastest machines.\n\nNumerical topics include: dense and sparse linear algebra, N-body problems, multigrid, fast-multipole, wavelets and Fourier transforms. Geometrical topics include partitioning and mesh generation. Other topics include: applications oriented architecture, understanding parallel programming paradigms, MPI, data parallel systems, Star-P for parallel Python and parallel MATLAB(r), graphics processors, virtualization, caches and vector processors.\n\nOne emphasis for this course will be VHLLs or Very High Level Languages for parallel computing. This includes the\nJulia programming language\n. Julia is a high-level, high-performance dynamic language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.\n\nReadings\n\nThere is no textbook for this course. A list of links to suggested readings and videos can be found in the\nrelated resources\nsection.\n\nRequirements\n\nThere will be two homework assignments and one project that will take roughly one half of the semester. You will need to hand in progress reports, a midterm, and make final presentation. The midterm is to report on your progress and is an opportunity for real feedback. The final project can be presented at the end.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nHomework\n\n45%\n\nProject\n\n55%",
  "files": [
    {
      "category": "Resource",
      "title": "Parallel Fast Fourier Transform implementations in Julia Report",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-337j-parallel-computing-fall-2011/ba7ef71dc0fea405860b7ebf878a5399_MIT18_337JF11_FFT_rpt.pdf",
      "content": "Parallel Fast Fourier Transform implementations in Julia\n12/15/2011\nAbstract\nThis paper examines the parallel computation models of Julia through several different\nmultiprocessor FFT implementations of 1D input. Minimizing communication overhead with the\nuse of distributed arrays is at the heart of this study, with computational optimization details\nleft aside as a less important factor. Throughout the discussion, input problem size and the\nnumber of processors are both assumed to be powers of 2 for the sake of simplicity. Various\nmethods of transmitting data within Julia to reduce latency cost are considered.\nPreliminaries\nBefore diving into the discussion of implementation details, a high-level introduction to FFT and\nparallel FFT methods is deemed necessary:\nA Discrete Fourier Transform, or DFT, of a sequence X = [X , Xl, . , Xn-lVV is a sequence\nY = [Y , Yl, . , Yn-lVV given by:\nn\nn\n2-l\n2-l\nn-l\nmk\nmk\nm\nmk\nYm = Xkwn\nYm = X2kwn wn X2k+lwn\n,\nm = 0,1, . , n - 1\nk\nk\nk\ne-2ni/n\nwhere wn =\nn\nThis is immediately recognized as a combination of two smaller problems of size 2, with the\nformer containing even-indexed elements of the original array and the latter containing odd-\nindexed elements. Defining Yk = X2k and Zk = X2k+l yields the following two sub-problems:\nn\n2-l\nn\n2-l\nmk\nmk\nm = Ykwn n Zm = Zkwn ,\nm = 0,1, . , \"\nn - 1\nk\nk\nOnce these problems are solved, the solution to the original problem can be determined by:\nXm = m wnmZm,\nm = 0,1, . n - 1\nn/2 +\nn\nHowever, since wn\nm = -wnm and wn/2\n2 terms can be reduced to (with some\nn/2 = 1, the last\nn\nfactoring details that shall be omitted):\nXm+n/2 = m - wnmZm, m = 0,1, . , 2 - 1\n\nThese reduced computations are depicted below by the famous Cooley-Tukey butterfly:\nFigure 1 - Cooley-Tukey Butterfly\nNote that the ordering of output elements is in a bit-reversed order, (i.e. input element at index\n011 will correspond to output element at index 110 of the resulting array). Due to the recursive\nnature of the solution, the following pseudo code that outlines the main stages of computations\nfollows trivially (with optional bit-reversal step depending on the desired output type):\nFFT( array )\n... base case handling ...\nFFT( even set of array )\nFFT( odd set of array )\nCombine results using Cooley-Tukey butterfly\nEnd\nIt's evident that in computing FFT, data elements are exchanged very frequently in order to\ncompute the butterfly relation. Because of this, parallel computing models (except for shared\nmemory systems) must take care of handling communication across multiple processors\ncarefully since latency and bandwidth cost are often the bottlenecks. In the development of\nparallel FFT algorithms, perhaps the two most dominant ones are the Binary Exchange algorithm\nand the Parallel Transpose algorithm. The major difference between these two methods is the\napproach to handling communication between different nodes. In Binary Exchange, data is\ndistributed evenly among p processors and only the first log p stages of the computation require\ndata exchange, with the remaining stages doing local computations.\nFigure 2 depicts an input of size 8 being\ndistributed among 4 processors; with\nthe first 2 stages require\ncommunication, whereas the last two\ncan be done locally since the data\nneeded are already available on-site.\nThis algorithm is best on hypercube\nnetwork (1) since the data required will\nalways be found in adjacent nodes, thus\nminimizing the cost of messaging.\nFigure 2 - Binary Exchange Example (courtesy of (3))\n\nThe Parallel Transpose Algorithm is another attempt at solving the internode communication\nproblem. An input of size n is conceptually represented as a ,n x ,n matrix wherein only one\nphase of communication is needed in between computations. The following figure depicts how\nthis actually works:\nFigure 3 - Parallel Transpose Algorithm (courtesy of (2))\nHere we have an array of size 16 being distributed across 4 processors, with the appropriate\nelements living on the bottom-labeled processors. In the first two stages, all butterflies are done\nlocally; afterwards data are exchanged altogether in one phase to end up at the second\nconfiguration that allows for local calculations until the end results. In contrast to Binary\nExchange where communication is invoked as an on-demand request between corresponding\nnodes, latency cost is only incurred once here. This could help reduce significant overhead in\nsystems where communication initialization costs are expensive.\nBinary Exchange and Parallel Transpose are simply different ways to tackle the communication\ncost. However, it is important to note that with different systems and programming languages,\nvariations of them could be tailored specifically for the underlying architecture to improve\nperformance. For what follows, a study of methods for performing data exchange using Julia on\nclusters is presented.\n\nJulia Implementations\nPerhaps the best way to represent input FFT data in Julia is with distributed arrays, or DArray\nobjects. These are simply arrays with elements living on a subset of available processors. Instead\nof manually having to set up data, DArray has a nice built-in support for all array operations\nwhich also abstracts away the underlying work for cross-node communication.\nWith this in mind, a very straightforward yet naive way to parallelize FFT is to utilize the\n@spawn macro in Julia and modify the recursive sequential version as below:\nFFT( array::DArray )\n... base case handling ...\n@spawn FFT( even set of array )\n@spawn FFT( odd set of array )\nCombine results using Cooley-Tukey butterfly\nEnd\nThis code will work, and in fact is the same approach that FFTW takes when implementing on\nshared memory systems using CILK; however, its performance is far from practical for this case.\nIn distributed-memory clusters, the random spawning of FFT calls on processors cause way too\nmuch communication overhead since they may be assigned to calculations involving data that\nare non-local. This coupled with the fact that single DArray element access is slow, completely\nkills execution time. One can improve this spawning process by leveraging the @spawnat macro\nto specify which processor should handle the problem:\nFFT( array::DArray )\n... base case handling ...\n@spawnat owner(even array[1]) FFT( even set of array )\n@spawnat owner(odd array[1]) FFT( odd set of array )\nCombine results using Cooley-Tukey butterfly\nEnd\nIn the above pseudo code, tasks are assigned to processors which contain the first element of\nthe problem array in question. With this approach, the communication time is certainly reduced,\nhowever if the array at hand is distributed on multiple processors; unnecessary non-local\naccesses still happen quite frequently. Yet, if the array distribution is carefully taken care of,\ndata could be guaranteed to be local for as many stages of recursion as possible and latency cost\nis reduced. In fact, when the input array is evenly distributed among p processors in a bit-\nreversed order, this algorithm follows exactly the Binary Exchange model. In figure 4 (with\nprocessors 1-4 owning data that are separated by horizontal dashed lines, from top to bottom),\nthe first two stages of recursion correspond to the green-boxed area in which all butterfly\ncomputations are done locally within the assigned processors. Only in the last two stages\n(orange and red, respectively) does it require non-local communication. As discussed\npreviously, in general only the last log p stages incur overhead, and this algorithm is a significant\nimprovement over the previous one. In practice, however, for large input, the performance is\n\nstill intolerable partially due to the high cost of DArray element access at the last few stages.\nFurthermore, recursive spawning in Julia is also very expensive and appears to have a non-linear\ncost growth with respect to problem size. Therefore, modifications are necessary in order to\nimprove messaging time associated with these two issues.\nFigure 4 - Binary Exchange Implementation (courtesy of (2))\nTo eliminate the spawning overhead, one could treat all sub-problems in the first local stages as\nseparate and perform a black box FFT computation on each one. For example, in figure 4, four\ngroups of data are each solved independently and locally. Results are then combined\nappropriately in the last two levels. This approach resolves to the following simple pseudo code:\nFFT_BlackBox( array )\n...Sequentially solve...\nend\nFFT( array::DArray )\n... base case handling ...\nfor each processor p\n@spawnat p FFT_BlackBox(array)\nend\nCombine results using Cooley-Tukey butterfly\nEnd\nNote that these black-box FFT solvers must produce unordered output, which means that the\nbit-reversal step must not be done when solving. The reason is to ensure a correct order of data\nfor later stages to process. In this algorithm, any FFT solver would suffice for computing the local\nsub-problem given that it meets the condition stated before. Aside from reducing the number of\n\nspawning, this also allows for the opportunity to leverage sequential black box solvers that are\nalready optimized, like FFTW. For the purpose of this paper, a simple sequential FFT solver was\nwritten to serve as a means to illustrate the concept. In practice FFTW can be configured to\nhandle these types of input and output.\nAnother remaining issue is the cost of accessing DArray elements at the last log p stages of the\ncomputation. One way to avoid this is to rearrange the data distribution to gather what is\nneeded on a processor that is assigned to compute. In figure 4, at the orange level, data can be\nredistributed altogether in a bundle so that array elements that live on processor 2 are moved\nto processor 1, and from processor 4 to 3. The decision on which processor involved in the\ncalculation to move data to is random here, but in practice perhaps physical node locations\nshould be taken into account. With this approach, after the rearrangement, all data access will\nbe local for the current stage, thus avoiding multiple distant element accesses. However, a\ndisadvantage to this model lies in the fact that data must ultimately be gathered in one single\nnode which might not have sufficient memory. For the purpose of this study, this consideration\nis left aside to focus on optimizing internode communication in the network.\nWith Julia, the data redistribution can be done by constructing a new DArray with elements\nliving on the appropriate processors. Calculations are then resumed on this new array. However,\neven though the overhead is now significantly reduced compared to previous attempts, it still\nconsumes a considerable amount of time, as the timing table below shows (with time measured\nin seconds):\nProblem Size\nFFTW\nCommunication - 4 procs\nCommunication - 8 procs\n\"l\n0.0002\n0.08972\n0.145179\n\"l-\n0.0009\n2.703539\n0.191254\n\"2\n0.128\n0.783675\n1.014697\n\"2.\n6.3023\n21.3901\n26.08323\nThe raw communication cost alone far exceeds actual execution time of sequential FFTW on\nvarious sizes. This is largely due to the expensive cost of rearranging data from one node to\nanother. For an input of size \"2. distributed among 4 processors, the unit cost of rearranging\ndata from one processor to another is around 3 seconds. With this approach, there are a total of\nlog p phases of redistribution each involves an exchange of total of\nn\n2 elements, hence the\nbottleneck of execution time.\nGiven all of these above-described characteristics of the system, perhaps the best\nimplementation of parallel FFT in Julia would use the Transpose algorithm to impose only one\nphase of data redistribution and minimize as much latency cost as possible. Due to time\nconstraint, this examination does not include this implementation, however, should still serve as\na means to understand different advantages and disadvantages of the underlying architecture.\nIn the next section, some explanations on the written Julia code and instructions on how to run\nthem will be presented.\n\nCode execution\nIn the code package, several FFT functions are implemented along with helpers. Below is a\ncomplete set of available APIs and their descriptions:\nMain entry points:\n-\nfftp(array)\no Takes a local array and performs parallel FFT computation using the final algorithm\npresented above.\n-\nfftp_oc(array)\no Same as fftp() except this version only emulates communication that is done. This\nfunction is useful to get an accurate measurement of internode messaging\noverhead.\nFFT functions:\n-\nfftp_dit_darray_fftw(array::DArray)\no Takes a distributed array and computes FFT. This is called by fftp().\n-\nfftp_combine(array::DArray)\no Takes a distributed array and combine the local array elements using the butterfly\nrule.\n-\nfft(array::DArray)\no Performs in-place FFT computation on the local portion of the given distributed\narray\n-\nfftp_dit_darray_fftw_oc(array::DArray)\no Same as fftp_dit_darray_fftw() except only performs communication calls.\n-\nfftp_combine_oc(array::DArray)\no Same as above.\n-\nfft_oc(array::DArray)\no Same as above.\n-\nfftp_dit_darray_smart_spawn(array::DArray, startIdx, endIdx)\no FFT computation using @spawnat that was described earlier in the paper.\n-\nfftp_dit_darray_random_spawn(array::DArray, startIdx, endIdx)\no FFT computation using random spawning that was first introduced as a naive\nimplementation.\n-\nfft_dit_in_place(array, top::Int64, bot::Int64)\no Performs in-place FFT computation on the given array, giving unordered output.\nHelpers:\n-\nhowdist(s::DArray)\no Prints out the distribution of the given distributed array on available processors.\n-\nredist(s::DArray, pid1, pid2)\n\no Redistributes the distributed array, moving all data from processors (pid + 1) to pid2\nto processor pid1.\n-\nredistbb(s, L)\no Redistributes data all at once assuming size of the array and number of processors\nare both powers of 2. Depending on the specified level L, which corresponds to the\ndifferent stages depicted in figure 4; data are rearranged to ensure the\ncomputations can be done locally. This method must be called successively on\nresults for previous level until the desired level is reached. Re\n\nfer to below for\ninstruction for how to execute.\n-\nbitr_lookup(i, n)\no Bit-reverse an n-bit number i using lookup table\n-\nbitr_loop(i, n)\no Bit-reverse an n-bit number i using standard loop and bit shifts\n-\nbitr(array)\no Bit-reverse an array.\nThe following lines of code show how some of these functions are called:\na = complex(rand(2^20))\nresult = fftp(a) # Computes parallel FFT on A\n\nm = drand(2^20)\nd = redist(m,1,3) # All array elements from processors 2 and 3 are moved to processor 1\n\nm1 = drand(2^20)\nm2 = redistbb(m1,1) # Move all data at once, if on 4 processors, elements on 2 are moved to 1,\nand from 4 to 3.\nm3 = redistbb(m2,2) # Now m2 is distributed between processor 1 and 3, m3 will contains\nresults all local to processor 1.\n\nSources\n\n(1)\nGupta, Anshul, and Vipin Kumar. The Scalability o f FFT on Parallel Computers. IEEE,\n1993. Print.\n(2)\nPalmer, Joseph. THE HYBRID ARCHITECTURE PARALLEL FAST FOURIER TRANSFORM\n(HAPFFT). B righam Young University, 2005.\n(3)\nChu, Eleanor Chin-hwa, and Alan George. Inside The FFT Black Box, Serial A\nnd Parallel\nFast Fourier Transform Algorithms. CRC, 2000.\n(4)\nHeath, Michael. Parallel Numerical A\nlgorithms. Department of Computer Science\nUniversity of Illinois at Urbana-Champaign, Web.\n<http://www.cse.illinois.edu/courses/cs554/notes/13_fft_8up.pdf>.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.337J / 6.338J Parallel Computing\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Parallel FFT in Julia",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-337j-parallel-computing-fall-2011/2f2063e265e939309f5b2f6b5d4b94d4_MIT18_337JF11_FFT_pres.pdf",
      "content": "18.337\nParallel FFT in Julia\n\n18.337\nReview of FFT\n\n18.337\nReview of FFT (cont.)\n\n18.337\nReview of FFT (cont.)\n\n18.337\nSequential FFT Pseudocode\nRecursive-FFT ( array )\n- arrayEven = even indexed elements of array\n- arrayOdd = odd indexed elements of array\n- Recursive-FFT ( arrayEven )\n- Recursive-FFT ( arrayOdd )\n- Combine results using Cooley-Tukey butterfly\n- Bit reversal, could be done either before, after or in\nbetween\n\n18.337\nParallel FFT Algorithms\n- Binary Exchange Algorithm\n\n- Transpose Algorithm\n\n18.337\nBinary Exchange Algorithm\n\n18.337\nBinary Exchange Algorithm\n\n18.337\nBinary Exchange Algorithm\n\n18.337\nParallel Transpose Algorithm\n\n18.337\nParallel Transpose Algorithm\n\n18.337\nJulia implementations\n- Input is represented as distributed arrays.\n- Assumptions: N, P are powers of 2 for\nsake of simplicity\n- More focus on minimizing communication\noverhead versus computational\noptimizations\n\n18.337\nEasy\nRecursive-FFT ( array )\n- .............\n- @spawn Recursive-FFT ( arrayEven )\n- @spawn Recursive-FFT ( arrayOdd )\n- .............\nSame as FFTW parallel implementation for 1D input\nusing Cilk.\n\n18.337\nToo much unnecessary overhead because of random\nspawning.\nBetter:\nRecursive-FFT ( array )\n- .............\n- @spawnat owner Recursive-FFT ( arrayEven )\n- @spawnat owner Recursive-FFT ( arrayOdd )\n- .............\n\nNot so fast\n\n18.337\nFFT_Parallel ( array )\n- Bit reverse input array and distribute\n- @spawnat owner Recursive-FFT ( first half )\n- @spawnat owner Recursive-FFT ( last half )\n- Combine results\n\nBinary Exchange Implementation\n\n18.337\nBinary Exchange Implementation\n\n18.337\nBinary Exchange Implementation\n\n18.337\nBinary Exchange Implementation\n\n18.337\nBinary Exchange Implementation\n\n18.337\nBinary Exchange Implementation\n\n18.337\n\nAlternate approach - Black box\n- Initially similar to parallel transpose method: data is\ndistributed so that each sub-problem is locally\ncontained within one node\nFFT_Parallel ( array )\n- Bit reverse input array and distribute equally\n- for each processor\n- @spawnat proc FFT-Sequential ( local array )\n- Redistribute data and combine locally\n\n18.337\n\nAlternate approach\n\n18.337\n\nAlternate approach - Black box\nPros:\n- Eliminates needs for redundant spawning which is\nexpensive\n- Can leverage black box packages such as FFTW\nfor local sequential run, or black box combiners\n- Warning: Order of input to sub-problems is important\nNote:\n- Have not tried FFTW due to configuration issues\n\n18.337\n\nBenchmark Caveats\n\n18.337\nBenchmark Results\nFFTW\nBinary\nExchange\nBlack Box\nCommunication\nBlack Box 8p\nCommunication 8p\n0.0002\n11.938756\n0.10948204\n0.150836\n0.37363982\n0.4737592\n0.0009\n193.102961 0.18138098\n0.1402709\n0.59792995\n0.416522\n0.128\nA year?\n8.28874588\n1.1951251\n9.86574506\n1.701771\n6.3023\nApocalypse 290.634222\n30.29218\n314.73069\n44.75283\n\n18.337\nBenchmark Results\n\n18.337\n\nCommunication issues\n\n18.337\nNew Results\nFFTW Black Box\nImproved\nRedistribution\nCommunication\nfor Improved\nRedistribution\nBlack Box\nImproved\nRedistribution 8p\nCommunication for Improved\nRedistribution 8p\n0.0002\n0.10238204\n0.08972\n0.154734\n0.145179\n0.0009\n0.247728\n2.703539\n0.284916\n0.191254\n0.128\n8.389301\n0.783675\n8.717208\n1.014697\n6.3023\n287.0166\n21.3901\n287.4002\n26.08323\n\n18.337\nNew Results\n\n18.337\n\nMore issues and considerations\n- Communication cost: where and how. Better\nredistribution method.\n- Leverage of sequential FFTW on black box\nproblems\n- A separate algorithm, better data distribution?\n\n18.337\nQuestions\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.337J / 6.338J Parallel Computing\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Replica-Exchange Molecular Dynamics on Hadoop Report",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-337j-parallel-computing-fall-2011/6f3447744fb597fd274b70ecc430f956_MIT18_337JF11_Hadoop_rpt.pdf",
      "content": "Replica-Exchange Molecular Dynamics on\nHadoop\n18.337 Final Report\nZachary W. Ulissi\nDecember 16, 2011\nBackground\nReplica exchange molecular dynamics (REMD) is a commonly used tech\nnique to accelerate sampling rates of molecular dynamics simulations by\nperforming a number of parallel replica simulations at different tempera\ntures (see Figure 1). Periodically, the temperatures between pairs of replica\nsimulations are switched with a probability\n\n(E\n\ni-Ej )\np = min\n\n1, e\nkT\nk\ni -\nTj\n!\n.\nIn most implementations, swaps are only considered for simulations with\nthe nearest temperature. If the higher temperature simulation has lower\nenergy than the lower temperature simulation, the exchange is automatically\naccepted. However, there is also a chance for the exchange even if this is not\nthe case (the exponential term). These concept are similar to Monte-Carlo\nbased optimization techniques, and allow for the efficient sampling of the\nsimulation potential energy surface without getting trapped in local minima\n(see Figure 2).\nThis problem is highly parallel and hierarchical in nature: each replica\ncan be simulated using a number of processors, and each replica is indepen\ndent until replicas need to be exchanged. The optimal number of processors\nfor a small MD system can vary from O(1) to O(100), and the number of\nreplicas is usually > 10 (setting the number and distribution of tempera\ntures determines the probability of exchanges being accepted, and is thus a\nsimulation size dependent problem).\nn\n-\ny\n-\na\n\nFigure 1: Illustration of the parallel REMD algorithm (reproduced from\nhttp://www.rikenresearch.riken.jp/eng/frontline/6290).\nA number of implementations currently exist for performing REMD,\nwith most of the production MD codes offering a simple scripting interface.\nIn addition, a number of papers have been published proposing compli\ncated manager/slave systems for performing REMD. However, these solu\ntions are not amenable to large heterogeneous computing environments, such\nas those used by the distributed Folding@Home effort. Instead, this project\nenables the REMD simulations using the Apache Hadoop implementation\nof the Map/Reduce framework, which separates the simulation design from\nthe underlying simulation framework. Hadoop handles all distributed in\nput/output and load balancing. Furthermore, using Hadoop allows for the\nsimple handling of hardware and software failures, which become increas\ningly disruptive with large-scale MD simulations.\nImplementation\nThe REMD algorithm was implemented for Hadoop using the streaming\nHadoop interface. The streaming interface for Hadoop allows Hadoop pro\ngrams to be written using simple console input and output. The simulation\nwas implemented using Python wrappers to implement the map and reduce\nsteps. For the core MD simulations, the highly optimized academic MD\nsoftware NAMD was used. The essential algorithm is illustrated in Fig\nure 3. The state (and input to the Hadoop program) was the state of each\ntemperature simulation. The state of the simulation consisted of:\nCourtesy of RIKEN. Used with permission.\n\nPotential Energy\n300 K\n350 K\n400 K\n450 K\n500 K\n550 K\n600 K\nFigure 2: Illustration of simulations at multiple temperatures exploring a\npotential energy surface. Higher temperature simulations can cross potential\nbarriers more quickly, and when a lower energy region is found these results\nare transferred to lower energy simulations.\n\nFigure 3: Illustration of REMD simulations in the Map/Reduce framework.\n- The setpoint temperature of the simulation\n- The energy of the simulation\n- The coordinates of each atom\n- The velocities of each atom\nEach line of input/output corresponded to all of the state information for\na single temperature. A simple python program was written to initialize\nthe REMD using an initial guess for the atomic coordinates. A full REMD\nsimulation thus consisted of a series of map/reduce calls.\nDuring the Map step, each temperature simulation was progressed by\n100,000 MD time steps (approximately 100 ps). The mapping program was\nwritten as a python wrapper for the NAMD MD program. For each temper\nature, the current coordinates and velocities were written to the appropriate\nNAMD input files and NAMD called through the shell. The resulting output\nfiles were then read, and the updated state sent as output. Since NAMD\nwas required for the simulation, it was packaged with every map/reduce call\nas a 2MB executable.\nDuring the reduce step, the updated states were read and, for each set\nof neighboring temperature simulations, the temperatures were exchanged\naccording to the selection criteria listed above. The updated simulation\nstates and temperatures were then emmitted as output, which could then be\nused for another map/reduce call to further progress the REMD simulation.\n\nFigure 4: Example used for simulation testing: the unfolded and folded\ndeca-alanin helix.\nResults and Performance\nThe Hadoop-based REMD method was tested using a simple example sim\nulation: the folding of a deca-alanin helix, illustrated in Figure 4 in both its\nfolded and unfolded states. The starting configuration was deca-alanin in\nan unfolded state, and REMD was performed using 50 temperatures from\n300 K to 800 K (i.e. 10 degree increments). The simulation was carried out\non 25-node and 50-node clusters on Amazon EC2.\nAfter 15 calls of the Hadoop-based REMD code, the simulation had\ncaptured the native state at a high temperature, illustrated in Figure 5,\nand upon further calls this low-energy conformation was passed to lower\ntemperatures.\nThe performance scaling of the solution was investigated on a 25-node\nAmazon EC2 Hadoop cluster, shown in Figure 6. Linear performance scaling\nwas observed between the usage of 1 and 25 nodes. Using 50 map tasks (twice\nas many as the number of nodes) resulted in nearly the same performance\nas using 25 tasks, suggesting that load-balancing was not an limitation on\nperformance. Specifying more map tasks than the number of simulations\nresulted in increased simulation times, even though the extra map tasks had\nno work to perform.\n\nFigure 5: Simulation state after 15 calls of the Hadoop map/reduce REMD\nprogram. The native state is shown to the left, and the state for each\ntemperature is shown on the right, starting from 300 K in the lower right-\nhand corner.\n\nTime [s]\nNumber of Map Tasks\nFigure 6: Scaling of the REMD code on a 25-node Amazon EC2 cluster.\n\nConclusion and Future Work\nThis project was successful in developing a Hadoop-based REMD simulation\ncode. However, a number of factors suggest that this approach will not be\nadopted for any significant simulations in the near future:\nThe startup time and general overhead of the Hadoop framework was sig\nnificant. To achieve reasonable efficiency in usage, the number of map/reduce\nsteps had to be minimized. A reduced number of temperature exchanges\nlimited the rate at which the potential surface could be explored. Further\nmore, the time for a low-energy structure identified at high temperature to\npropagate to low temperature simulations is directly related to the time be\ntween exchange steps. Using 50 temperatures, at least 50 map/reduce steps\nwould be necessary. The overhead of each map/reduce call could be per\nhaps be reduced by writing the wrappers in Java and integrated the program\ndirectly into the Hadoop framework.\nThe broad availability of computation time on NSF and DOE funded su\npercomputers to academic research labs generally means that a map/reduce\nframework is not necessary since all machines are collocated and tightly cou\npled. Using REMD could perhaps be useful on large distributed problems,\nbut each distributed computer needs to have Hadoop installed. Map/reduce\nbased REMD may be most appropriate for private companies that wish to\ndo large-scale simulations on heterogeneous clusters (i.e. drug companies\nwishing to do virtual drug screening).\nA number of possibilities exist for future work. Reducing the overhead\nof the map/reduce calls would increase the overall simultion efficiency. Sec\nondly, communication time during the accumulation steps in the map/reduce\ncall could be reduced by using binary representations of the coordinate and\nvelocity lists (rather than the character-based ones). Finally, the reduce\nstep took a significant fraction of the computation time even though it was\none of the simplest steps, suggesting that communication between the map\nand reduce steps was a limiting factor.\nAppendix: Simulation Code\n5.1\nmapper.py\n#!/usr/bin/python\nimport sys\nimport subprocess\nimport os\nenindex=11\n# input comes from STDIN (standard input)\n\nfor line in sys.stdin:\nz=line.split(' NEWFIELD ')\nnewtemp=float(z[0])\noldtemp=float(z[1])\ncoord=z[2].replace('NEWLINE','\\n')\nvel=z[3].replace('NEWLINE','\\n')\n#Set the necessary NAMD configuration inputs\nf=open('set_temp.tcl','w')\nf.write('set temp %f \\n' % newtemp)\nf.write('set rsv %f \\n' % (newtemp/oldtemp))\nf.write('set oname alanin_%d\\n' % newtemp)\nf.write('set ncoor newcoor_%d\\n' % newtemp)\nf.write('set nvel newcoor_%d\\n' % newtemp)\nf.close()\n#Write the coordinates\nf=open('newcoor_%d' % newtemp,'w')\nf.write(coord)\nf.close()\n#Write the velocity file\nf=open('newvel_%d' % newtemp,'w')\nf.write(vel)\nf.close()\n#Call NAMD, using python 2.7\n#subprocess.check_output([\"namd2 alanin.namd > alanin.namd_%d.log\" % newtemp],shell=True)\nsubprocess.call([\"chmod +x namd2\"],shell=True)\nsubprocess.call([\"./namd2 alanin.namd > alanin.namd_%d.log\" % newtemp],shell=True)\n#Clean-up the temporary coordinates and velocities\nos.remove('newcoor_%d' % newtemp)\nos.remove('newvel_%d' % newtemp)\n#Parse the log-file and get the final energy\nf=open(\"alanin.namd_%d.log\" % newtemp,'r')\nfor line in f:\nsline=line.split()\nif line!='\\n':\nif sline[0]=='ENERGY:':\nenergy=float(sline[enindex])\nf.close()\n#print('%d,%f' % (newtemp,energy))\n#Read the final coordinates\nf=open('alanin_%d.coor' % newtemp,'r')\nnewcoord=f.read().replace('\\n','NEWLINE')\nf.close()\n#Read the final velocities\nf=open('alanin_%d.vel' % newtemp,'r')\nnewvel=f.read().replace('\\n','NEWLINE')\nf.close()\n#Emit the processed state info\nprint(str(newtemp)+' NEWFIELD '+str(oldtemp)+' NEWFIELD '+newcoord+' NEWFIELD '+newvel+' NEWFIELD ' + str(energy))\n5.2\nreducer.py\n#!/usr/bin/python\nimport sys\nimport subprocess\nimport os\nimport math\nimport random\nSnewtemp=0\nkb=1.38*10**(-23)\nfor line in sys.stdin:\nif Snewtemp==0:\n#retrive the temperatures and energy\n\nSz=line.split(' NEWFIELD ')\nSnewtemp=float(Sz[0])\nSoldtemp=float(Sz[1])\nSenergy=float(Sz[4])\nelse:\n#retrive the temperatures and energy\nNz=line.split(' NEWFIELD ')\nNnewtemp=float(Nz[0])\nNoldtemp=float(Nz[1])\nNenergy=float(Nz[4])\n#Calculate beta=1/k/T for the current and saved versions\nNb=1/Nnewtemp/kb\nSb=1/Snewtemp/kb\ndelta=(Nb-Sb)*(Senergy-Nenergy)\n#Swap configurations\nif delta<0:\nprint(str(Nnewtemp)+' NEWFIELD '+str(Snewtemp)+' NEWFIELD '+Sz[2]+' NEWFIELD '+Sz[3])\nSz=Nz\nSenergy=Nenergy\nSoldtemp=Nnewtemp\n#Swap configurations\nelif math.exp(-delta)<random.random():\nprint(str(Nnewtemp)+' NEWFIELD '+str(Snewtemp)+' NEWFIELD '+Sz[2]+' NEWFIELD '+Sz[3])\nSz=Nz\nSenergy=Nenergy\nSoldtemp=Nnewtemp\n#emit without swapping\nelse:\nprint(str(Snewtemp)+' NEWFIELD '+str(Snewtemp)+' NEWFIELD '+Sz[2]+' NEWFIELD '+Sz[3])\nSz=Nz\nSenergy=Nenergy\nSnewtemp=Nnewtemp\nSoldtemp=Nnewtemp\nprint(str(Snewtemp)+' NEWFIELD '+str(Snewtemp)+' NEWFIELD '+Sz[2]+' NEWFIELD '+Sz[3])\n5.3\ngeninput.py\ngeninput.py sets up the input for the map/reduce simulation based on initial\nconfiguration data.\n#!/usr/bin/python\n#read initial atom coordinates\nf=open('alanin.coor','r')\nstartpdb=f.read().replace('\\n','NEWLINE')\nf.close()\n#read the initial atomic velocities\nf=open('alanin.vel','r')\nstartvel=f.read().replace('\\n','NEWLINE')\nf.close()\n#specify the temperatures for simulation\ntemps=[300,325,350,375,400,425,450,475,500,525,550,575,600]\noldtemp=300\ntowrite=''\n#For each temperature, write the state\nfor T in range(300,800,10):\ntowrite=towrite+str(T)+' NEWFIELD '+str(oldtemp)+' NEWFIELD '+startpdb+' NEWFIELD '+startvel+'\\n'\n#Write the output\nf=open('torun','w+')\nf.write(towrite)\nf.close()\n\n5.4\nNAMD Configuration File (alanin.namd)\nThis is the required control file for NAMD, which loads the atomic coordi\nnates and velocities from files written by the map/reduce call and runs the\nsimulation. The setpoint temperature is read from the set temp.tcl control\nfile also written by the map/reduce call.\n# NAMD CONFIGURATION FILE FOR DECALANIN\n#Get the simulation conditions\nsource set_temp.tcl\n# Set the atomic details\ncoordinates $ncoor\nvelocities\n$nvel\n#seed 12345\noutputEnergies\n# output params\noutputname $oname\nbinaryoutput no\n#DCDfile $oname.dcd\n#DCDfreq 100\n# integrator params\ntimestep 1.0\n# force field params\nstructure alanin.psf\nparameters alanin.params\nexclude scaled1-4\n1-4scaling 1.0\nswitching on\nswitchdist 8.0\ncutoff 12.0\npairlistdist 13.5\nmargin 0.0\nstepspercycle 20\nlangevin on\nlangevinTemp $temp\nrescalevels $rsv\n#Run the simulation\nrun 500000\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.337J / 6.338J Parallel Computing\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Social Coding: A Case Study with Julia Report",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-337j-parallel-computing-fall-2011/699a697c03e7413653ba6e8b5c4b87ab_MIT18_337JF11_Social_rpt.pdf",
      "content": "Social Coding: A Case Study with Julia\n18.337/6.338 Parallel Computing\nDepartment of Mathematics\nDepartment of Electrical Engineering and Computer Science\nMassachusetts Institute of Technology\nDecember 18, 2011\nIntroduction\nFor a new programming language to success, it must pursue two goals. The first goal is a quick\naccess to the language. The second goal is an easy reaching to other users and experts. Many\nnew programing languages such as python and ruby address the first well by providing an online\ninteractive prompt [4, 5]. However, they have yet addressed the second goal well enough. This\nproject tries a new approach to address the second. It explores a new way to make it easy for\nuser to reach to other users and experts. Specifically, this project experiments a way to create\nsuch environment for Julia [6]. We begins with a solution, a design and an implementation,\nfollowed by installation and conclusion.\nA Solution\nFor Julia to achieve the second goal: an easy reaching to other users and experts, users need\nto be able to chat, exchange conversations and share a Julia prompt session among each other\nand with experts. Furthermore, they need to be able to edit the same julia program along with\nexperts. With these features, users will be able to easily reach to other users and experts. Users\ncan ask other users questions; in real-time, other users or experts will be able to quickly answer\nthem, help them with their julia programs or even show them how to do it. While adding all\nthese features, we also have to address the first goal: a quick access to the language.\nTo do all these, we augment the approach that has already been used by many languages\n(an online interactive prompt) with collaborative and social features. In fact, we augment Julia\ninteractive prompt to allow multiple users to share a single Julia prompt session and allow them\nto chat among each other, may it be other users, friends or experts. Furthermore, we add a\ncollaborative editor, allowing users to be able to edit the same Julia program. We believe that\nthis solution addresses both goals well and it is a key toward the success of new programming\nlanguages such as Julia.\n\nFigure 1: An object model the basis of the application.\nA Design\nTo achieve both goals, we decide to create a real-time single-page web application [1]. The\nbasis of the application is the object model depicted in Figure 1. Each user subscribes to (1) a\nchannel identified with his or her own name and (2) the public channel. The design mockup of\nthe application is shown in Figure 2. Three main features include chat with other users, shared\njulia prompt session and a collaborative editor. First, when a user clicked on a name on the\nright panel, a chat box will pop up and the user will be able to type and chat with the other\nuser. A chat message will be published to the channel identified with the recipient's name.\nSecond, users can type in Julia command in the prompt. The command and its respond will\nbe published to the public channel; thus, they will be received by all users. Third, when users\nclicked on the \"Editor\" button, they will switch from Prompt screen to Editor screen. In this\nscreen, user will be able to collaboratively edit a Julia program. In addition, they can run this\nprogram in Prompt screen by clicking \"Run Code\" button.\nAn Implementation\nIn this section, we describe an implementation of the design. We use two main open-source\nprojects: Socketstream [2] and Etherpad-lite [3]. We use Socketstream for chat and shared Julia\nprompt session; we use Etherpad-lite for collaborative editing.\n4.1\nChat and Shared Julia Prompt Session: Socketstream\nTo implement chat and shared Julia prompt session, we use Socketstream framework. Socket-\nstream provides the following features:\n\nFigure 2: A design mockup of the application.\n- NodeJS: a server-side javascript [?].\n- Coffeescript support [12].\n- Jade Template Engine support [13].\n- Socket.IO support [14].\n- ZeroMQ support [16].\n- Redis support [15].\nFor more information about Socketstream, please visit:\nhttp://github.com/socketstream/socketstream.\n4.2\nCollaborative Editing: Etherpad-lite\nEtherpad-lite is an online collaborative editing. We make use of Etherpad-lite by embedding it\ninto our web application via iframe; we get contents of the editor using Etherpad-lite jQuery\nPlugin [11].\n\nFigure 3: An application architecture.\nFor more information about Etherpad-lite, please visit:\nhttps://github.com/Pita/etherpad-lite.\n4.3\nLibraries\nLibraries used in this implementation are:\n- jQuery Plugin. http://jquery.com/.\n- jQuery UI. http://jqueryui.com/\n- jQuery UI Chatbox. http://www.cs.illinois.edu/homes/wenpu1/chatbox.html.\n- jQuery Purr. http://code.google.com/p/jquery-purr/.\n- Etherpad-lite jQuery Plugin. https://github.com/johnyma22/etherpad-lite-jquery-plugin.\n- Mustache. http://mustache.github.com/.\nPutting Socketstream and Etherpad-lite together, we have an architecture for our application\n(Figure 3). This architecture allows running a Julia program in Editor screen in Prompt screen.\nWhen a user clicks \"Run Code\" button, Socketstream-frontend gets contents of the editor\nfrom Etherpad-lite (usually stored inside MysqlDB) and passes the contents to Socketstream\nbackend. Socketstream-backend executes the contents in a JuliaSession and publishes to a\npublic channel.\nSnapshots of the application are illustrated in Figure 5 and Figure 4.\n\nFigure 4: A snapshot of the application's editor screen.\nInstallation\nThis section describes what you need to install this application. The application requires Redis,\nNodeJS, Etherpad-lite, Socketstream and Julia.\n5.1\nRedis Installation\nPlease follow the instruction at http://redis.io/download.\n5.2\nNodeJS Installation\nPlease follow the instruction at http://nodejs.org/.\n5.3\nEtherpad-lite Installation\nPlease follow the instruction at https://github.com/Pita/etherpad-lite.\n5.4\nSocketstream Installation\nPlease follow the instruction at http://github.com/socketstream/socketstream or execute:\nnpm install -g socketstream\n5.5\nJulia Installation\nPlease follow the instruction at https://github.com/JuliaLang/julia\n\nFigure 5: A snapshot of the application's prompt screen.\n5.6\nApplication Installation\nFirst of all, please install git if you have yet had git. You can get git here: http://git-scm.com/.\nTo install the application:\ngit clone git://github.com/Wisdom/juliaSocial.git\ncd juliaSocial\nnpm install node-uuid\n5.7\nApplication Configuration\nThe configuration file is inside config/app.coffee. Please point \"julia\" to your julia binary and\n\"etherpad\" to your etherpad host.\n5.8\nRunning the application\nFirst, start redis server:\nredis-server\nSecond, inside the juliaSocial folder run:\nsocketstream start\n\nConclusion\nIn this project, we provide a solution, a design and an implementation to the two goals needed\nfor the success of a new programming language. Further development of this project will\ncontinue at http://github.com/Wisdom/juliaSocial. Future work includes Julia syntax parser,\nmultiple sessions and rooms, multiple programming languages, Facebook and Twitter integra\ntion and Julia syntax highlighter. We hope that this project will accelerate the adoption of\nJulia; we also hope that it will be useful to other programming languages as well.\nFinally, I would like to thank Professor Alan Edelman, Jeff Bezanson, Stephan Boyer, de\nvelopers of all the projects and libraries used in this project, Julia developers and the class.\nReferences\n[1] Single Page Application. http://en.wikipedia.org/wiki/Single-page application.\n[2] Socketstream. http://github.com/socketstream/socketstream.\n[3] Etherpad-lite. https://github.com/Pita/etherpad-lite.\n[4] Try Python. http://trypython.org/.\n[5] Try Ruby. http://tryruby.org/.\n[6] Julia. https://github.com/JuliaLang/julia.\n[7] jQuery Console. https://github.com/replit/jq-console.\n[8] jQuery UI. http://jqueryui.com/.\n[9] jQuery UI Chatbox. https://github.com/dexterpu/jquery.ui.chatbox.\n[10] jQuery Purr. http://code.google.com/p/jquery-purr/.\n[11] Etherpad-lite jQuery Plugin. https://github.com/johnyma22/etherpad-lite-jquery-plugin.\n[12] NodeJS. http://nodejs.org/.\n[13] Coffeescript. http://coffeescript.org/.\n[14] Jade. http://jade-lang.com/.\n[15] Socket.IO. http://socket.io/.\n[16] Redis. http://redis.io/.\n[17] ZeroMQ. http://www.zeromq.org/.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.337J / 6.338J Parallel Computing\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Social Coding: A Case Study with Julia Report Represent",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-337j-parallel-computing-fall-2011/4ab4fd1815c2ada2b2133ad6ebe0bd47_MIT18_337JF11_Social_pres.pdf",
      "content": "Social Coding\nA Case Study with Julia\n\nDesign\n\nMockup\n\nObject Model\n\nImplementation\n\nSoftware stack\nSocketstream Framework\n-NodeJS, server-side javascript\n-Coffeescript, syntactic sugar of javascript\n-Jade+Mustache template engine\n-Stylus, syntactic sugar of css\n-Socket.IO, websocket protocol with fallback\n-Scalable with ZeroMQ, a transport layer protocol\n-Redis DB, an open-source, networked, in-memory, key-\nvalue data store with optional durability\nEtherpad-lite for IDE\n-MySQLDB\n\nArchitecture\n\nLibrary\njq-ui for ui elements\njq-console for the console\njq-ui-chatbox for the chatbox\njq-purr for notification\nStephan's Julia web design\n\nDemo\n\nWhat's next?\nJulia Syntax Parser\nMultiple Rooms\nMultiple Languages\nFacebook/Twitter Integration\nJulia Syntax Highlighting\nJulia Mobile\n○\n\nThank You\nProfessor Alan Edelman, Jeff Bezanson, Stephan\nBoyer, Julia developers and the class.\n\nQuestions, Suggestions,\nComments?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.337J / 6.338J Parallel Computing\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}