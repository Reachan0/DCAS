{
  "course_name": "Models, Data and Inference for Socio-Technical Systems",
  "course_description": "In this class, students use data and systems knowledge to build models of complex socio-technical systems for improved system design and decision-making. Students will enhance their model-building skills, through review and extension of functions of random variables, Poisson processes, and Markov processes; move from applied probability to statistics via Chi-squared t and f tests, derived as functions of random variables; and review classical statistics, hypothesis tests, regression, correlation and causation, simple data mining techniques, and Bayesian vs. classical statistics. A class project is required.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nWelcome to ESD.86,\nModels, Data, Inference for Socio-Technical Systems!\nThis subject is part of the ESD PhD core, following the ESD doctoral seminar ESD.83 and requiring a solid undergraduate subject in applied probability as a prerequisite. At MIT the best prerequisite would be 6.041/6.431. With your help, this subject will be a great learning experience exposing you to interesting ideas, challenging you to think deeply, and providing skills useful in professional practice.\n\nThis is the first offering of this course and is a work in progress. The instructors welcome suggestions for improvement. Please submit your suggestions via the OCW feedback team.\n\nCourse Objectives\n\nESD.86 is a required subject for all ESD doctoral students. Its aim is to develop proficiency in developing and testing two types of models of systems operating under uncertainty:\n\nAxiomatic models, in which reasonable assumptions based on observations and/or independent logic are stated\na priori\n, and the resulting probabilistic model is developed axiomatically. There are no statistics in this approach. This is applied probabilistic modeling. The process resembles processes carried out by physicists, engineers and operations researchers.\n\nStatistical models, requiring acquisition of data and testing hypotheses of alternative models, estimating parameters values, making other inferences, etc.\n\nWe hope to show the strong linkages between applied probability and statistics in this subject. We believe that both types of modeling will be important in the research of most ESD doctoral students.\n\nPrerequsites\n\nESD.83 and 6.041\n\nClass Sessions\n\nThere are two 90-minute class sessions each week. We expect you to be present at these sessions and to participate thoughtfully. At the end of each week, our TA will offer an optional, guided office hour in which he will answer questions about that week's material and work through illustrative problems. Depending on how many students show up, this session may resemble a recitation, a tutorial or a one-on-one office hour's session.\n\nTextbooks\n\nTwo books are recommended but not required for this class.\n\nWu, C. F. Jeff, and Michael Hamada.\nExperiments: Planning, Analysis, and Parameter Design Optimization\n. New York, NY: J. Wiley & Sons, 2000. ISBN: 9780471255116.\n\nLarson, Richard C., and Amedeo R. Odoni.\nUrban Operations Research\n. 2nd ed. Belmont, MA: Dynamic Ideas, 2007. ISBN: 9780975914632. (A version of this text is also available\nonline\n.)\n\nAdditional readings from other sources will be assigned during the term.\n\nWeekly Contest\n\nEach week there will be a contest relating to the worst use, misuse or abuse of statistics and probabilistic reasoning in the media - press, TV, blogs, radio, magazines, etc. Examples could be related to interpretations of statistical studies, or proposed new systems in which statistics plays a large role (here, likely unintended consequences of the 'reform' would be acceptable as part of the submission), etc.\n\nEach Monday we will ask students to submit their nomination for the week. On Wednesday, after the teaching staff has had time to review the candidates, the 'winner' will be announced. This effort will be viewed as part of class participation. Plus winners will get the benefit of the doubt if their final grade is border-line. We intend for this to be both fun and educational.\n\nTerm Project\n\nThere will be term projects. We have reserved the last two class periods for presentation of the projects. Details of the projects will be presented by end of February.\n\nSoftware\n\nComputation is essential to modern analysis of systems operating under uncertainty. However, it is not a primary objective of this subject to develop your proficiency with any particular software tool. We have developed assignments, especially later in the subject, that require substantial computation, but we leave it to each student to select the software to use. Many of the computations that need to be carried out are statistical or probabilistic in nature. For example, one may need to generate samples drawn from a normally distributed population or may need to compute the standard deviation of a large data set or plot a histogram. A good choice for doing such tasks is MATLAB(r) and the associated Statistics Toolbox. Similar capabilities are embedded in Mathcad(r), Maple(r), and Mathematica(r), as well as other software. One might be able to do all the assignments in a spreadsheet like Microsoft(r) Excel or QuattroPro(r), but such tools may also limit the depth of the analysis that can be achieved. The assignments can also be done using programming language like C or FORTRAN, but this would probably be an inefficient use of time due to the needed coding. For those on campus, note that MATLAB(r) is available for use through the campus network. A further consideration in selecting computer tools for use in this subject is the level of support the teaching staff can provide. Prof. Frey can generally provide help with MATLAB(r), MathCad(r), MiniTab(r), and Microsoft(r) Excel.\n\nNote on Submission of Work\n\nThe manner in which you present your work can be just as important (and in some cases more so) than the overall approach manifested within the response. Be sure to clearly explain your work, the methods used, and the underlying assumptions. Such practices make it possible for us to fairly assess your work and happen also to be good practices for documenting work in industry.\n\nLate Policy\n\nIt is expected that responses to assignments will be submitted on the due date and time noted on the assignment. The usual policy for late assignments is that a letter grade is lost per day late. In the case of unusual circumstances or\nunavoidable\nconflicts, please contact Prof. Larson or Prof. Frey to discuss the details and explore alternatives.\n\nTime Commitment\n\nThe units on an MIT subject correspond to the time that an adequately prepared student is expected to spend in a normal week. This is divided into three numbers associated with the subject (X-Y-Z) with X being class time, Y being laboratory time, and Z being work outside of class. The numbers associated with ESD.86 are (3-0-9) making this a 12-unit subject.\n\nGrading\n\nYour grade in ESD.86 will be determined based on your performance on homework, quizzes, a term project, and class participation as described in the table below:\n\nACTIVITIES\n\nPERCENTAGES\n\nHomework (6 assignments)\n\n40%\n\nQuizzes (2)\n\n30%\n\nClass participation\n\n5%\n\nTerm project\n\n25%\n\nThis is a doctoral subject, and we expect everyone who works hard in the subject to receive either an A or a B.\n\nCalendar\n\nSES #\n\nTOPICS\n\nINSTRUCTORS\n\nKEY DATES\n\nIntroduction and overview\n\n3-door problem\n\nProf. Richard C. Larson and Prof. Daniel D. Frey\n\nAnalyzing a probability problem\n\nProbability mass functions\n\nProf. Richard C. Larson\n\nBroken stick problem\n\nWorking in sample space\n\nProf. Richard C. Larson\n\nPedestrian crossing problem\n\nProf. Richard C. Larson\n\nHomework 1 due\n\nRandom incidence: A major source of selection bias\n\nProf. Richard C. Larson\n\nRandom incidence and more\n\nProf. Richard C. Larson\n\nSpatial models\n\nProf. Richard C. Larson\n\nHomework 2 due\n\nMarkov processes and their application to queueing, part 1\n\nProf. Richard C. Larson\n\nMarkov processes and their application to queueing, part 2\n\nProf. Richard C. Larson\n\nHomework 3 due\n\nQueueing and transitions: Sampling from distributions, Gauss\n\nProf. Richard C. Larson\n\nDerived distributions to statistics\n\nAman Chawla (guest lecturer)\n\nHomework 4 due\n\nThe Queue Inference Engine and the psychology of queueing\n\nBeyond the physics of queueing\n\nProf. Richard C. Larson\n\nQuiz 1\n\nThe Weibull distribution and parameter estimation\n\nProf. Daniel D. Frey\n\nHypothesis testing\n\nProf. Daniel D. Frey\n\nDescriptive statistics and statistical graphics\n\nProf. Daniel D. Frey\n\nRegression\n\nProf. Daniel D. Frey\n\nHomework 5 due\n\nAnalysis of variance, with discussion of Bayesian and frequentist statistics\n\nProf. Daniel D. Frey\n\nMultiple regression\n\nProf. Daniel D. Frey\n\nDesign of experiments, part 1\n\nDesign of experiments, part 2\n\nProf. Daniel D. Frey\n\nHomework 6 due\n\nDesign of computer experiments\n\nProf. Daniel D. Frey\n\nClosure - Threats to validity of inference\n\nProf. Daniel D. Frey\n\nQuiz 2\n\nFinal presentations 1\n\nFinal presentations 2\n\nAcademic Honesty\n\nThe fundamental principle of academic integrity is that one must fairly represent the source of the intellectual content of the work one submits for credit. Students are trusted to adhere to this principle and its meaning in the context of this subject as subsequently explained. Official Institute policy regarding academic honesty can be found in the current Bulletin under \"Academic Procedures and Institute Regulations\".\n\nWhat is the policy on examinations?\nThe examinations in this subject are to represent individual work. You may not receive any help from other students or any other individuals.\n\nWhat about homework assignments? Can we work together?\nWe encourage students to work together in this subject to understand the home assignments and to learn in general. There is much to be gained in sharing the learning process. However, the final submission should represent your own expression of the final response to the assignment and not a copy of someone else's expression thereof, whether directly from a person or as recorded on paper (e.g. a book) or electronically (e.g. on a Web site). Furthermore, you must fairly represent the authorship of the intellectual content of the work you submit for credit by acknowledging the contribution of sources (e.g., books, Web sites) you consult in the process of completing assignments. In addition, at the end of each assignment on which you collaborated with other students, you must cite the students and the interaction. The purpose of this is to acknowledge their contribution to your work. Some examples follow:\n\nYou discuss concepts, approaches and methods which could be applied to a homework assignment before starting your write-up. This process is encouraged. You are not required to make a written acknowledgment of this type of interaction.\n\nAfter working an assignment independently, you compare responses with another student which confirms your results and response. You should acknowledge that the other student's write-up was used to check your own. No credit will be lost if the response is correct, the acknowledgment is made, and no direct copying of the other response is involved.\n\nAfter working an assignment independently, you compare responses with another student which alerts you to an error in your own work which you then correct. You should state at the end of your submission that you corrected your error on the basis of checking responses with the other student. No credit will be lost if the response is correct, the acknowledgment is made, and no direct copying of the other response is involved.\n\nYou and another student work through an assignment together exchanging ideas as the effort progresses. You both should state at the end of your individual submissions that you worked jointly. No credit will be lost if the responses are correct, the acknowledgment is made, and the assignment write-up is independent.\n\nYou\ncopy\nall or part of an assignment write-up\nfrom a reference\nsuch as a textbook or past solution (this is in contrast to\nreferring\nto such a reference and developing the solution yourself). You must cite the reference. Partial credit will be given, since there is some educational value in reading and understanding the solution.\n\nYou\ncopy\nverbatim all or part of a write-up\nfrom another student\n. You must cite the person by name. Very little partial credit will be given.\n\nVerbatim copying of any material which you submit for credit without reference to the source is considered to be academically dishonest\n.",
  "files": [
    {
      "category": "Resource",
      "title": "ps1_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/aa0a849bb44f3cf5160af72c056517e1_ps1_sol.pdf",
      "content": "1. Twenty-Six Doors\nLet's have: P(A) = Probability that the dollar bill is in envelope A, and E = Evidence (opening of\nall envelopes except the envelope chosen and envelope Q).\nThen, P(L /E) =\nP(E /L) \" P(L)\nP(\ni= A\nZ\n#\nE /Letteri) \" P(Letteri)\n=\nP(E /L)\nP(E /L) + P(E /Q) =\nP(E /L)\nP(E /L) +1\nIf we consider that the professor would choose the envelope Q at random in the case where the\ndollar bill is in envelope L, then P(E/L)=1/25.\nConsequently, P(L /E) =\n25 +1\n= 1\nThen, P(Q/E) = P(L /E) =1\" P(L /E) = 25\nThe student should choose envelope Q.\nIf the professor consistently chooses envelope Q when the dollar bill is in envelope L, then\nP(E/L)=1, and there is an equal chance than the dollar bill be in one envelope or the other.\n2. Weird Country\na)\nLet's have S = Average number of children:\nS =\ni\n2i\ni=1\n\"\n#\n= 2\nb)\nSince there is exactly one daughter in each family, the proportion of daughters among the\ncompleted family children is 50%.\nc)\nNow, we have\nS = 1\n2 \"1+ 1\n22 \" 2 + 1\n23 \" 3+ 1\n24 \" 4 + 1# 1\n2 # 1\n22 # 1\n23 # 1\n$\n% &\n'\n( ) \" 5 = 1\n2 + 1\n2 + 3\n8 + 1\n4 + 5\n16 = 31\nDaughterNb = 1\n2 \"1+ 1\n22 \"1+ 1\n23 \"1+ 1\n24 \"1+ 1# 1\n2 # 1\n22 # 1\n23 # 1\n$\n% &\n'\n( ) \" 0.5 = 1\n2 + 1\n4 + 1\n8 + 1\n16 + 5\n32 = 31\nDaughterProportion = 50%\n3. Baseball Championship\na)\nPi = Probability that the inferior team wins; X = Number of games played:\nPi X = x\n[\n] = x \"1\n#\n$ %\n&\n' ( (1\" p)3 ) px\"1\"3 ) (1\" p)\n\nNB: X ∈ [[4 ; 7]]\nThus:\nPi =\nPi[X = x]\nx= 4\n\"\n=\nx #1\n$\n% &\n'\n( ) (1# p)4 * px#4\nx= 4\n\"\nPi = (1# p)4 * (1+ 4 p +10p2 + 20p3)\nb)\nP'i = Probability that the inferior team wins; X = Number of games played:\nSimilarly, Pi' X = x\n[\n] = x \"1\n#\n$ %\n&\n' ( (1\" p)2 ) px\"1\"2 ) (1\" p), with X ∈ [[3 ; 5]]\nThen:\nPi'=\nPi[X = x]\nx= 3\n\"\n=\nx #1\n$\n% &\n'\n( ) (1# p)3 * px#3\nx= 3\n\"\nPi'= (1# p)3 * (1+ 3p + 6p2)\nc)\nP''i = Probability that the inferior team wins; X = Number of games played:\nIn the same fashion, Pi'' X = x\n[\n] = x \"1\n#\n$ %\n&\n' ( (1\" p)4 ) px\"1\"4 ) (1\" p), with X ∈ [[5 ; 9]]\nThen:\nPi''=\nPi[X = x]\nx= 5\n\"\n=\nx #1\n$\n% &\n'\n( ) (1# p)5 * px#5\nx= 5\n\"\nPi''= (1# p)5 * (1+ 5p +15p2 + 35p3 + 70p4)\nThe trend is obvious. We'll also notice that the higher the number of matches, the smaller is the\nprobability improvement from a series with n matches to a series with (n+1) matches.\nd)\nP7 = Probability that seven matches are required to determine the winner\nP7 = Probability of one team winning exactly three matches out of six:\nP7 = 6\n\"\n# $\n%\n& ' 0.53 ( 0.53 = 0.3125\nAssumption: normal distribution, σ = 0.05\nRandom variable X: Number of series going to seven matches:\nE[X] = 94 x 0.3125 = 29.375\nσ2 = 94 x 0.3125 x (1 - 0.3125) = 20.195\nHypothesis testing:\nz = x \" μ\n#\n= 29.375 \" 35\n20.195\n= \"1.2517\nWe cannot reject the null hypothesis with a 95% confidence level.\n4. Birthdays\nMethod: Determine the probability P0 that nobody has the same birthday date. Let n be the\nnumber of people in the class.\n\nThis is obviously a Bernoulli trial. We suppose that nobody is born a February 29.\nThe following solution is an approximation. It does not take into account the probabilities that\nmore than two people have their birthday the same day.\nNumber of trials needed to compare the birthdays of all people in the class (N):\nN =\nn!\n(n \" 2)!2! =\nn!\n2(n \" 2)!\nThen, P0 = N\n\"\n# $\n%\n& ' 1\n\"\n# $\n%\n& '\n0 364\n\"\n# $\n%\n& '\nN\n= 364\n\"\n# $\n%\n& '\nN\n= 364\n\"\n# $\n%\n& '\nn!\n2(n(2)!\nThe probability we are looking for is equal to 1-P0.\nThe right solution is in fact the following:\nP0 =\n1\"\ni\n#\n$ %\n&\n' (\ni= 0\nn\"1\n)\n=\n365!\n365n(365 \" n)!\nAnd the probability we are looking for is still 1-P0.\nNB: If I mistakenly stated that this solution (the right one) was an approximation, come\nand see me at the end of the class, and I'll give you full credits for this question. Sorry for\nthe confusion.\n5. Guru\nEasy enough: let us suppose that the probabilities of the outcomes of each event are\napproximately equal. The \"guru\" has only to send his first \"prediction\" to a large group of\npeople. One half of this group will receive e-mail saying that outcome A1 will occur, the other\nhalf will receive e-mail saying that outcome B1 is going to happen.\nThen, the \"guru\" divide the group which received the right \"prediction\" in two half and send the\nsecond \"prediction\", proceeding as before.\nIn the end, 1 out of 512 people will have received 10 consecutive right \"predictions\"."
    },
    {
      "category": "Resource",
      "title": "ps1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/a3e96e360d6e266ab4866f9dc3d8c06c_ps1.pdf",
      "content": "ESD.86\nModels, Data, Inference for Socio-Technical Systems\nMassachusetts Institute of Technology\nEngineering Systems Division\nProblem Set #1\nIssued: Wednesday February 7, 2007. Due: Tuesday February 20, 2007.\n1. Twenty-Six Doors. Redo the analysis of the '3-Door\" problem (actually, we used 3\nenvelopes), using the full alphabet of 26 letters. That is, the set up is the same as in class,\nbut there are 26 envelopes in front of the class. In a closed room before class, the TA and\nthe professor placed a 20-dollar bill inside one envelope. Only they know which\nenvelope contains the money. The student contestant picks, say, envelope \"L\" as her\ninitial choice. Then the TA dramatically opens each envelope one at a time, starting with\n\"A\", showing to the class the emptiness of each. But when the TA gets to a particular\nenvelope other that \"L\", say envelope \"Q\", he skips that one and works through all\nremaining envelopes R through Z, showing their emptiness. The contestant is then left\nwith two alternatives: \"L\" and \"Q\", both unopened. All others are now known to be\nempty. The student is allowed to change her mind or to stick with \"L\". What should she\ndo and why?\n2. Weird Country. Suppose there is a country in which all citizens want to construct\ntheir families in a particular way. Each married couple agrees to a very unusual family\nplanning strategy. The mother and father (to-be) decide to keep having children until the\nfirst girl is born. Then they stop. That is, there is a 'stopping rule': Keep having\nchildren until the first girl is born, then STOP. For simplicity we assume that all couples\ncan have children and in theory, at least, the number of children in each family is\nlimitless (until, of course, the first girl is born). For modeling purposes assume that the\ngender of each child is determined by the outcome of a Bernoulli trial, with 50-50 chance\nof being boy or girl. All such Bernoulli trials are mutually independent.\n(a) Determine average family size of completed families.\n(b) In steady state, determine the fraction of the population that is female.\n(c) Suppose there is a government policy issued that dictates that no family shall\nhave more than five children, and that all citizens strictly obey this edict. So\nthe new stopping rule is: Keep having babies until the first girl is born or\nuntil I have five sons, whichever comes first. Determine the proportion of\nsons and daughters within families operating in this new restricted\nenvironment.\n3. Baseball Championships. In American baseball, the World Series is a 'best of\nseven' sequence of games, the team that first wins four games being anointed as the\n\"World Champions\". Suppose in the next World Series the Boston Red Sox play the San\nFrancisco Giants. Suppose that a Bernoulli trial decides the outcome of any particular\n\ngame, with probability that the Red Sox will win any given game equal to p. All\nBernoulli trials are mutually independent. So, for instance, the chance that the Red Sox\nwould win the World Series in precisely four games is p4. Suppose that the Red Sox are\nthe superior team, meaning that p > 0.5.\n(a) What is the probability that the inferior team, i.e., the San Francisco Giants, will\nwin the World Series? Create a graph of this as a function of p. (MS Excel is\ngood for this.)\n(b) Some baseball playoff series are 'best of five games,' not 'best of seven,' as the\nWorld Series. Answer part (a) again, but this time for a best of five series. Do\nyou have any comment about this? About how reliable are the results of a 'short\nseries\"?\n(c) In 1903, 1919, 1920 and 1921 the World Series winner was determined through a\nbest-of-nine playoff. Redo part (a) for a best-of-nine series. Are you seeing a\ntrend here? (It might be nice to plot the results as a function of p, with one plot\neach for best of 5, best of 7, and best of 9.)\n(d) For a Best-of-Seven World Series having evenly matched teams (i.e., p = 0.5),\nwhat percentage of World Series would you expect would require the full seven\ngames to determine the champion? If the teams were not evenly matched, how\nwould this figure change? In fact since 1905, 35 out of 94 World Series have\ngone to Game 7 - a rate of 37.2 percent. How does this compare with your\nnumber(s)? Are the results 'significantly different? Might the differences be\nexplained by inadequacies in our simple Bernoulli model? See\nhttp://www.aip.org/isns/reports/2003/080.html .\n4. Birthdays. Count the number of people (students and faculty) in our class during the\nnext class period. What is the probability that at least two of us share the same birthday?\nHint: see http://www.erin.utoronto.ca/~aweir/107/hw_assigns/bday.pdf .\n5. Guru. Suppose over the course of 6 months you receive 10 unsolicited emails from a\nstranger, each predicting the outcome of some future event. Each event for which he\nforecasts the results has just two possible outcomes. For instance, one of his predictions\ncould be the name of the team winning the Super Bowl. Another could be the name of the\nwinner in a closely matched 2-person election. Each of these predictions occurs before\nthe event in question, and each turns out to be correct! After you verify that the 10th\nprediction is correct, you get another email from him saying that he will give you the\noutcome of some future 11th event, but that you must pay him $100,000 for the\ninformation. If his prediction turns out to be true, you could make $1,000,000 with the\ninformation. What do you do? How do you think about this problem? Can you\nconstruct a simple model explaining why this person may be a total fraud?"
    },
    {
      "category": "Resource",
      "title": "abuses2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/579abd23523f7740ae578da5864a254b_abuses2.pdf",
      "content": "ESD.86 Spring 2007\nWeekly Contest #2\nU.S., Britain fare poorly in children survey\nUNICEF ranks the well-being of youngsters in 21 developed countries.\nBy Maggie Farley\nNew York Times, February 15, 2007\n\n[Article text removed due to copyright restrictions.]\n\nAnalysis\n\nThis article really catches the eye: the U.S. and Britain - two of the most developed\ncountries in the world, known for their educational systems, healthcare systems, etc. - rank at the\nbottom of 20 developed countries for the well-being of children. The Netherlands (one of the only\ncountries with legalized drugs and prostitution) ranks at the top. This very subjective area is\ncertainly open to manipulation, and it comes through clearly in this article.\n\nOf course, the headline and top section of the article contain the strong statements,\nincluding the United Nations and UNICEF seals of approval. It is only at the end of the article that\nweaknesses in the study are acknowledged. Problems such as \"lack of comparable data\", \"weak\nspots\" for some of the assessment scales, and the acknowledgement that it is a weakly defined\nwork-in-progress, are presented in a way so as not to detract from the results.\n\nA closer look at the explanation of some of the inputs also helps to show how this\nsurprising outcome could have occurred:\n- When judging one of the 5 categories, \"material well-being\", the study \"measured\nrelative affluence by asking whether a family owned a vehicle, a computer, whether the children\nhad their own bedroom, and how often the family traveled on holidays\". There are many\nproblems with this assessment, and the article acknowledges only the affect of public transport\nand real estate prices. There are numerous unaddressed variables that could affect the answers\nto these questions. One example of a hole in this logic is that it doesn't address number of\nsiblings. A family that chooses to have many children can be very happy (have a high degree of\nwell-being), but some, if not all, of those children would certainly share a room. They would also\nhave less money to travel on holidays. This paradigm clearly favors families with one child, which\nopposes other factors in the study, such as asking a child if he or she is lonely.\n- The results are based heavily on the answers to questions, which can be interpreted in\nnumerous ways. For example, \"Children in the Netherlands, Spain, and Greece said they were\nthe happiest...\" How can anyone put a definitive scale on 'happiness'? It seems that so much\ngoes into the interpretation of that question that comparing the answer would be meaningless.\nThe word 'happiness' may even imply different things in different languages. Similarly, \"Those in\nSpain, Portugal, and the Netherlands spent the most time with their families and friends.\" How\ndoes one define \"spending time\"? or \"family and friends\"? Those from one country may think of\nspending time with friends as time independent of school, team sports, or other structured\nactivities, whereas those from somewhere else may think of it as any time they are not alone in a\nroom.\n\nThe main critique I have for this article is that the whole premise is questionable. How\ncan \"the United States\" be compared with a small country like the Czech Republic? Conditions\nacross the United States are not homogeneous. The education system is not really one system\nat all, but is controlled at the state level, and there is wide variation within states, or even cities?\n\nDoes this study refer to schools in New York City or in an affluent suburb on Long Island? Can\nsuch disparate results be averaged? Does this take into account private schools in those areas,\nand if so, what data are they using? Does it only refer to standardized test results, and if so, it\nopens the huge debate on whether that is really a good measure of a complete education. Also,\ncan standardized tests in America be compared to those in other countries? Similarly, the report\nacknowledged that \"the average child in New York's most affluent areas seem equal to one in a\nless-developed country because of the constraints of city living.\" But what about comparing an\naffluent area in New York with an affluent area in New Mexico or Florida? Can an aggregate of\n\"U.S.\" statistics really provide any meaningful comparison with another country? The same\nquestions exist regarding healthcare. There isn't one healthcare system in America, so do we\ncompare the system used by Hollywood stars or that used by inner-city Medicare recipients?\nCan the two be averaged in any meaningful way?\n\nWith so much subjectivity and so many unanswered questions regarding this study, it\nseems unreasonable for an organization such as the United Nations to put out such a definitive\nlist. This is clearly a mis-use of statistics for some agenda of the UNICEF's research center. The\ndiscussion as to the nature of that agenda would be purely speculative, so I'll leave that as an\nexercise for the reader."
    },
    {
      "category": "Resource",
      "title": "ps2_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/92cceb56efea33f7636bb12eddcd2122_ps2_sol.pdf",
      "content": "Problem Set #2\n1. Network robustness - 4 pts\na) 1 pt\nSince each link can be functioning or broken, we have a total of 25 = 32 possibilities that will\ndefine the sample space.\nIf we define 0: broken state; 1: functioning state and if we use the following order to characterize\nthe state of each link of the network: ABCDE, we will have the following sample space:\n00000; 00001; 00010; 00011; 00100; 00101; 00110; 00111; 01000; 01001; 01010; 01011;\n01100; 01101; 01110; 01111; 10000; 10001; 10010; 10011; 10100; 10101; 10110; 10111;\n11000; 11001; 11010; 11011; 11100; 11101; 11110; 11111.\nb) 1 pt\nWe can generalize: the size of the sample space of a similar network with n links will be 2n.\nThe size of the faulty sub-space will depend upon the topology of the network.\nc) 1 pt\nThere is two possible understandings of this question. Since the question was ambiguous, I've\ncounted both of them as correct.\nIf we consider that each line fails independently from the corresponding ones in the other sub\nnetworks, the answer is obvious:\nP[TSNi] = PESNi + (1\" PESNi ) PASNi + 1\" PASNi\n(\n) # PDSNi\n[\n] # PBSNi + 1\" PBSNi\n(\n) # PCSNi\n[\n]\n(\n)\nP[T1\"2] =\nP[TSNi]\ni=1\n$\nIf all corresponding lines in the sub-networks (e.g. all lines PB, SNi) fail at the same time, then the\nreliability of the network will be equal to the reliability of the sub-network made with the less\nreliable component lines of each type:\nP[T1\"2] = MinSNi[PESNi ]+ (1\" MinSNi[PESNi ]) MinSNi[PASNi ]+ 1\" MinSNi[PASNi ]\n(\n) # MinSNi[PDSNi ]\n[\n] # MinSNi[PBSNi ]+ 1\" MinSNi[PBSNi ]\n(\n) # MinSNi[PCSNi ]\n[\n]\n(\n)\nd) 1 pt\nThe probability of successful transmission P2i of a link constituted by 2 parallel components\nhaving an individual probability of successful transmission Pi is:\nP2i = 2 Pi - Pi\nThen, we have to successively replace the probabilities of successful transmission of all links of\nthe network composed by the three subnets by the probability of successful transmission of a\ndouble link.\nThe optimal solution is the one that maximizes the global probability of successful transmission.\nWhen the network becomes too complicated to test all possibilities, optimization algorithms can\nbe developed to find the best solution.\n\n2. Binomial distribution and baseball - 3 pts\na) 1 pt\nP[X]: P[X] = 0.4 _iff _ x = 0\n0.6_iff _ x =1\n\"\n#\n$\nThen, PX\nT[z] =\nzx.pX (x) = z0 \" 0.4\ni= 0\n#\n$\n+ z1 \" 0.6\nQED\nb) 1 pt\nNw =\nXi\ni=1\n\"\nThis is a series of independent Bernoulli trials with an identical success probability.\nPNw\nT [z] =\nPX\nT[z]\nk=1\n\"\n=\n(0.4 + 0.6 # z) = (0.4 + 0.6 # z)162\nk=1\n\"\nc) 1 pt\nWe have:\nPk\nT = (0.4 + 0.6 \" z)162 =\nk\n#\n$ %\n&\n' ( \" 0.4162)k \" 0.6z\n(\n)\nk\nk= 0\n*\nPk\nT =\nzkPk(k)\nk= 0\n+\n*\n=\nzkPk(k)\nk= 0\n*\nTherefore, Pk = 162\nk\n\"\n# $\n%\n& ' ( 0.4162)k ( 0.6k\n3. Interviewing movie goers - 3 pts\nPsychologically speaking, people tend to go and see movie that others like to see. Therefore,\nsuccessful movies will attract large audience, whereas unsuccessful ones will be watched by very\nfew people.\nThis creates a selection bias. A simple example will illustrate it. Let's suppose that all movie\ntheaters rooms have 100 seats. Let's assume rooms projecting a successful movie will be filled at\n67% and that rooms projecting an unsuccessful one will be filled at 1%. Finally, we'll make the\nassumption that we have on average 1 successful movie for 15 unsuccessful ones.\nThen, the apparent fill rate is:\nFApp = 67\n100 \" 67\n82 + 1\n100 \" 15\n82 = 0.549\nBut the real fill rate is in fact:\nFReal =\n67 +15\n100 +15 \"100 = 0.051\nIf we make a two decimals approximation, we obtain respectively 55% and 5%.\n\n4. Jogging - 4 pts\na) 2 pts\nFour equally likely possibilities when you pass another jogger:\n- You are both on your way back\n- You are one your way back, but he hasn't yet turned around\n- You haven't yet turned around, but he is on his way back\n- You both haven't yet turned around\nOnly in the last case have you a chance to meet him again. The probability of meeting him again\nis thus inferior to 25%.\nb) 2 pts\n4 possible cases (each with probability 25%):\n1) Jogger enters the path in front of you and go in the same direction as you): Since everybody\nmoves at the same speed, you will pass him if you jog at least 4 units of distance before turning\naround. You have decided to job X units of distance before turning around, where P(X ≤ x) = 1 -\n-x\ne .\nTherefore, the probability to pass him is PPass = e -4 ≈ 0.0183.\n2) Jogger enters the path in front of you and go in the opposite direction as you: You will pass\nhim (once or more) if you jog at least 1 unit of distance before turning around.\nSimilarly, we find PPass = e -1 ≈ 0.3679.\n3) Jogger enters the path on your back and go in the same direction as you: You will pass him if\nyou if you jog between 1 and 2 units of distance.\nThus, we have PPass = e -1 - e-2 ≈ 0.2325\n4) Jogger enters the path on your back and go in the direction opposite to you: you will never\npass him.\nThe probability of passing the other jogger at least once is:\nPPass = 0.25 x (e-4 + e -1 + e -1 - e-2) ≈ 0.1547\n5. Returning to the broken stick experiment - 6 pts\na) 1pt\nLet's consider the first case: P[L ≥1⁄2]: P{one point is not in the left part of the yardstick} = 1⁄2\nThus, P[L ≥1⁄2] = (1⁄2)2 = 1⁄4\nSimilarly, we find P[R ≥1⁄2] = (1⁄2)2 = 1⁄4\nWhat about P[M ≥1⁄2]? Let X1 and X2 be the two points where the stick is cut.\nWithout loss of generality, let x1 be the abscissa of the leftmost point and x2 the abscissa of the\nrightmost one. Let's consider the sample space:\n\nformed. Besides, the probabilities that one given piece be longer than 1⁄2 yard are disjointed.\nThen, P{\"} # P{a triangle can be formed from the 3 pieces}\nP{\"} =1$ P{[L %1/2]&[M %1/2]&[R %1/2]}\nThe probability distributions of the abscissas of X1 and X2 are uniformly distributed over the\nsample space. By observation, we can conclude that P[M ≥1⁄2] = 1⁄4.\nb) 1 pt\nIt suffices that one piece of the yardstick be longer than 1⁄2 yard to prevent a triangle to be\nc) 1 pt\nLet A = {L ≥1⁄2}; B = {M ≥1⁄2}; C = {R ≥1⁄2}:\nP[A ∪ B] = P[A] + P[B] - P[A ∩ B] and P[A ∪ B ∪ C] = P[(A ∪ B) ∪ C]\nP[(A ∪ B) ∪ C] = P[A ∪ B] + P[C] - P[(A ∪ B) ∩ C]\nSo, P[A ∪ B ∪ C] = P[A] + P[B] - P[A ∩ B] + P[C] - P[A ∩ C] - P[B ∩ C] + P[A ∩ B ∩ C]\nd) 1 pt\n\nObviously, applying the results of question b) gives the correct answer.\ne) 1 pt\nThe probability that we cannot make a quadrilateral is equal to the probability that one of the\nfour pieces is longer than the sum of all the others, i.e. that it is longer than half the initial\nyardstick.\nf) 1 pt\nFor a n-gon, the yardstick is cut into n pieces. The probability that a n-gon can be formed is\nBy the same logic, the probability that one given piece is longer than 1⁄2 yard is equal to 1/8.\nTherefore, P[Q] = P{a_quadrilateral_can _be_ formed} =1\" 4 # 1\n8 = 0.5\nequal to the probability that one of the pieces is longer than 1⁄2 yard. The probability that one\ngiven piece be longer than 1⁄2 yard is 1\n2n\"1 .\nThus, P[N] = P{a_ n \" gon _can _be_ formed} =1\" n # 1\n2n\"1 =1\" n\n2n\"1"
    },
    {
      "category": "Resource",
      "title": "ps2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/ef956275bed587ce851b0d37c356d84f_ps2.pdf",
      "content": "ESD.86\nModels, Data, Inference for Socio-Technical Systems\nMassachusetts Institute of Technology\nEngineering Systems Division\nProblem Set #2\nIssued: Tuesday February 20, 2007. Due: Wednesday February 28, 2007.\n1. Network Robustness. In Lecture #2 we showed how to find the probability of\nTransmission from one point to another on a communications network having faulty\nlinks. In particular, we dealt with this network:\nE\nA\nD\nB\nC\nAnd we found that the probability of successful transmission T12 from node 1 to node 2 is\nA\nP{T12}= pE + (1-pE){[pA+(1- pA) pD] [pB+(1- pB) pC]}.\nWhen expanded, we see the 'Venn diagram' adding and subtracting of overlapping points\nD\nin the diagram:\nP{T12}= pE + pA pB + pBpD +pA pC + pCpD\n-pApBpD - pApCpD- pApBpC - pBpCpD\n+ pApBpCpD\n-pA pB pE - pB pD pE -pA pC pE - pC pD pE\n+ pA pC pD pE + pA pB pD pE + pA pBpC pE + pBpC pD pE\n- pA pBpC pD pE\n(a) Considering that each link can either be functioning correctly or broken, write out\nthe sample space for this network.\n(b) For a general faulty network having N links, what is the size of the sample space?\n(c) Consider a network structured like this:\nSubNet1\nSubNet2\nSubNet3\n\nSuppose that each of the three SubNets has the 5-link network topology as shown\nin the in the figure from lecture and copied above. But the individual link\ntransmission probabilities are dependent on the SubNet containing them. For\ninstance pA would have different values, depending on whether we are in SubNet\n1, 2 or 3. Determine the probability of successful transmission from node 1 to\nnode 2 in this more complex transmission network.\n(d) \"...ility\". Suppose you were given a budget to add one redundant link to one of\nthe three subnets in the problem above. Your only option is to add a duplicate\nlink in parallel to one of the existing links, and the probability of successful\ntransmission of the new link will be identical to that of the link it is directly\nparallel to. We assume that the 2 links will operate independently. You want to\ndo this in order to maximize the increase in probability of successful transmission\nfrom node 1 to node 2. Explain carefully how you would frame and formulate\nthis problem.\n2. Binomial Distribution and Baseball. Suppose the Boston Red Sox this year, in their\n162-game season, can be modeled with a simple probability model. In particular, we\nsuppose that we can model the outcome (Win or Loss) of each game as being determined\nby an independent Bernoulli trial. We assume (optimistically) that P{Win} = 0.60 and\nP{Loss} = 0.40. Then the total number of wins (Nw) over the course of the 162-game\nseason can be written as the sum of 162 independent indicator random variables,\nwhere\nNw =\nXi\ni=1\n\"\nXi ={1 if game i is a Win\n0 if game i is a Loss.\n(a) Show that the discrete or z-transform of Xi is equal to [0.4 + 0.6z].\n(b) From what you know about the transform of the sum of independent random\nvariables, argue that the discrete or z-transform of Nw is equal to [0.4 + 0.6z]162 .\n(c) From what you know about the definition of the z-transform, show that you can\n'invert' the transform to find Pk = P{Red Sox win precisely k games this season}.\nIn fact, show that we have the Binomial Distribution,\nPk = 162\nk\n\"\n# $\n%\n& ' (0.6)k(0.4)162(k,k = 0,1,2,...,162.\n3. Interviewing Movie Goers. You post a questionnaire on the web, asking moviegoers\nquestions about the films that they go to the theater and see. One of the items on the\nquestionnaire is this: \"The last time you went to the theater to see a film, estimate the\nfraction of seats in the theater that were occupied by fellow moviegoers.\" Let's assume\nthat each answer is precisely correct. That is, each individual's ability to estimate the\n\npercentage of seats occupied is perfect; there is no estimation error. And let's assume\nthat the average of the answers you get back, averaged over many responses (say\n1,000+), is 55%. Now, I am going to give you a piece of data: For the entire USA, the\ntheater industry has computed this fraction:\nf = (# of seats sold during 1 week)/(total # of seats offered for sale during 1 week)\nIn a typical week, the theater industry has found that f = 0.05, far from 0.55. Construct a\nquantitative argument, based on probabilistic reasoning and people's behavior, explaining\nthe apparent huge discrepancy.\n4. Jogging. It's a nice day in January in the Boston area. The temperature is a balmy 70\ndegrees, and people are jogging. Suppose the jogging path of interest is an infinitely long\nstraight East-West path and that all joggers move at the same speed. Joggers can enter\nthe jogging path at any point. Suppose any given jogger is equally likely to first jog East,\nthen West, or to first jog West and then East. Each jogger will jog some jogger-specific\nmaximum distance in the first direction chosen, stop, turn around and then complete the\njog by retracing steps and exiting at the point at which the jog was started. That is,\njoggers are assumed to enter and exit the jogging path at the same location. And, each is\nassumed to jog a finite distance, but distances will differ. Now, at a random time and\nrandom place, you enter the jogging path and start your jog.\n(a) At some (random) time during your jog, you pass another jogger moving towards\nyou, moving in the direction opposite to yours. Show that with probability at\nleast 0.75 you will not pass that jogger again on today's jog.\n(b) Jogger J enters the path at the same time as you, 2 units of distance away from\nyou and jogs 3 units of distance before turning around. Suppose you decided to\njog X units of distance before turning around, where X is exponentially distributed\nwith parameter 1. Find the probability of meeting jogger J and the probability of\nmeeting her twice. NB: Remember that any given jogger is equally likely to first\njog East, then West, or the reverse.\n[You have just successfully completed a written doctoral exam question in applied\nprobability, administered last month to MIT Operations Research doctoral candidates.]\n5. Returning to the Broken Stick Live Experiment. In the experiment we did in class\nwe used the Four Steps to Happiness to derive that the probability is 0.25 that one can\nform a triangle with the three pieces of yardstick broken at two random points. In this\nproblem let's think of the problem in a different way.\nSuppose we think of the lengths of the three pieces of stick as L, M and R, corresponding\nto L = length of left most piece, M = length of middle piece and R = length of right-most\npiece. Each is a non-negative random variable over [0, 1 The sample space remains a\nunit-area square in the positive quadrant, with axes labeled x1 and x2 as the places on the\n\nstick of the random marks. Each (x1, x2) pair gives rise to experimental values of L, M\nand R.\n(a) Argue from basic principles that\nP{L \"1/2} =1/4, P{M \"1/2} =1/4, P{R \"1/2} =1/4.\n(b) Let P{\"} # P(a triangle can be formed from the 3 pieces}. Argue that\nP{\"} =1# P{[L $1/2]%[M $1/2]%[R $1/2]}\n(c) We know from basic probability that P{A ∪ B} = P{A} + P{B} - P{A ∩ B}. Show\nthat the analogous result for three events, A, B and C is\nP{A ∪ B ∪ C}= P{A}+P{B}+P{C}-P{B ∩ C}-P{A ∩ B}-P{A ∩ C)}+P{A ∩ B ∩ C}.\n(d) If you apply the result of part (c) to your result in part (b), do you get the correct\nanswer for P{\"}? Can you identify in the (x1, x2) sample space the three separate events\nP{L \"1/2}, P{M \"1/2}, P{R \"1/2}? Can you see at once that the respective events are\ndisjoint and that P{L \"1/2} =1/4, P{M \"1/2} =1/4, P{R \"1/2} =1/4?\n(e) Extend the logic to marking the stick at 3 random points, all uniformly distributed\nover [0,1] and mutually independent. What is the probability that we can form a\nquadrilateral with the four pieces we obtain when we cut the yardstick at the 3 marked\nplaces?\n(f) Extend the logic to marking the stick at (n-1) random points, all uniformly distributed\nover [0,1] and mutually independent (n = 3, 4, 5, ...). What is the probability that we can\nform an \"n-gon\" with the n pieces we obtain when we cut the yardstick at the (n-1)\nmarked places? (An n-gon is a polygon with n sides.)"
    },
    {
      "category": "Resource",
      "title": "ps3_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/c23c9b90693957b6eb12c742c947e6f3_ps3_sol.pdf",
      "content": "Problem Set #3\n1. Max and Min - 4 pts\na) 2 pts\nBy observation, we know that W ≤ Z.\nTwo cases: on [0;1] and on [1;2].\nOn the interval [0;1]:\nFW,Z (w,z) = 1\n2 zw + w(z \" w)\n[\n] = zw \" w2\nfW ,Z (w,z) = # 2FW,Z (w,z)\n#w#z\n=1\nOn the interval [1;2]:\nFW,Z (w,z) = 1\n2 zw + w(1\" w)\n[\n] = w + wz \" w2\nfW ,Z (w,z) = # 2FW,Z (w,z)\n#w#z\n= 1\n\nb) 2 pts\nBy observation, we see that T = W + Z = X + Y. X and Y are independently distributed.\nWe have three cases: on [0;1], on [1;2] and on [2;3].\nOn the interval [0;1]:\nFT = 1\n2 \" t 2\n2 = t 2\n4 # fT = t\nOn the interval [0;1]:\nFT = 1\n2 \"1\" t + 1# (2 # t)\n[\n]\n= t\n2 # 1\n4 $ fT = 1\n\nOn the interval [0;1]:\nFT = 1\n2 \"1\" 2 # (3# t)2\n$\n% &\n'\n( ) =1# 3# t\n(\n)\n* fT = 3# t\n2. Building a car - 5 pts\na) 2 pts\nMean time between arrivals of fidgets:\nE[TF] = 1\n\" =10_min\nMean time between arrivals of whoosies:\nE[TW ] = 1\nt \" 0.1\" e#0.1\"t + 0.02 \" e#0.02\"t\n(\n)dt\n$\n%\n&\n' (\n)\n* + = 30_min\nb.1) 1 pt\nThe Poisson process being memoryless, the mean time from our return to the workstation until\nthe first fidget arrives is 10 min.\nb.2) 1 pt\nMean time to arrival of first whoosy:\nE[VW ] = E 2[TW ]\n1+\n\"W\nE[TW ]\n#\n$ %\n&\n' (\n2E[TW ]\n= 302\n1+ E[TW\n2]) E 2[TW ]\nE 2[TW ]\n=15 * 1+\n+1\n2 + 1\n+2\n2 ) 302\n,\n-\n.\n.\n.\n.\n/\nE[VW ] =15 * 1+ 2500 +100 ) 900\n,\n- .\n/\n0 1 = 43.33_min\nb.3) 1pt\nWe can have two cases here: either the fidget arrives first, or it's the whoosy.\nP{Fidget _ first} =\n\".e#\".t\n43.3\n$\n.dt =\n0.1% e#0.1%t\n43.3\n$\n= 0.987\nIf the fidget arrives first, then the total waiting time is the mean waiting time for the first whoosy\nto arrive: 43.33 min.\nIf the whoosy is first, however, we will have to wait 10 more minutes on average, since the\nPoisson process is memoryless.\nTherefore: E{Total_ waiting_ time} = 0.987 \" 43.33+ (1# 0.987) \" (43.33+10) = 43.46_min\n\n3. Spatial Poisson on a line - 4 pts\na) 2 pts\nThe distance to first ambulance obeys to a Poisson process with a pdf d1(t) = \".e#\" .t\nD1 is the corresponding cumulative distribution function.\nTherefore: E[D1] = 1\n\" _ mile is the average length we have to check before finding an ambulance.\nE[D1]\nRoad\nAccident\nThis distance is centered on the accident: the average distance from the accident to the nearest\nambulance (E[D'1]) is half this value. Since the ambulance is driven at 1 mile per minute,\nE[T'1] = 1\n2.\" _min\nb) 2 pts\nSimilarly, we will again have to cover 1\n\" mile on average to find the next ambulance, starting on\nthe border of the area previously searched (memoryless property of the Poisson pdf). However,\nsince we have already determined on which side was the previous ambulance, we know that the\nsecond ambulance is necessarily on the other direction.\nTherefore, the distance to the second nearest ambulance is: E[D'2 ] = 1\n2.\" + 1\n\" = 3\n2.\" _ miles and\nE[T'2 ] = 1\n2.\" + 1\n\" = 3\n2.\" _min\n4. Covered rectangle - 4 pts\nCoverage rectangle has sides of length L1 = [Max(X) - Min(X)] and L2 = [Max(Y) - Min(Y)].\nIts area is A = L1 x L2.\nFM (x) = P{Max(X) \" x} =\nP[Xi \" x]\ni=1\n#\n= x\n$\n% & '\n( )\n,x * [0;2]\nfM (x) = 5 x\n$\n% & '\n( )\nE[Max(X)] =\nx + 5 + x\n$\n% & '\n( )\ndx\n,\n= 5\nx11\n-\n. /\n1 2\n= 20\n\nFm(x) = P{Min(X) \" x} =1# P{Min(X) $ x} =1#\nP[Xi $ x]\ni=1\n%\n=1# 2 # x\n&\n' (\n)\n* +\n,x , [0;2]\nfM (x) = 5 2 # x\n&\n' (\n)\n* +\nE[Min(X)] =\nx - 5 - 2 # x\n&\n' (\n)\n* +\ndx\n.\n= 5\nx(2 # x)10\n#10\n/\n0 1\n3 4\n#\n2 # x\n#10\n&\n' (\n)\n* +\ndx\n.\n&\n'\n( (\n)\n*\n+ + = 5\n(2 # x)11\n#110\n/\n0 1\n3 4\n= 2\nE[L1] = 20\n11 \" 2\n11 = 18\nFM (y) = P{Max(Y) \" y} =\nP[Yi \" y]\ni=1\n#\n=\ny\n$\n% &\n'\n( )\n,y * [0;10]\nfM (y) = y\n$\n% & '\n( )\nE[Max(Y)] =\ny +\ny\n$\n% &\n'\n( )\ndy\n,\n= 1\ny11\n-\n. /\n1 2\n= 100\nFm(y) = P{Min(Y) \" y} =1# P{Min(Y) $ y} =1#\nP[Yi $ y]\ni=1\n%\n=1# 10 # y\n&\n' (\n)\n* +\n,y , [0;10]\nfM (x) = 10 # y\n&\n' (\n)\n* +\nE[Min(X)] =\ny - 10 # y\n&\n' (\n)\n* +\ndy\n.\n= 1\ny(10 # y)10\n#10\n/\n0 1\n3 4\n#\n10 # y\n#10\n&\n' (\n)\n* +\ndy\n.\n&\n'\n( (\n)\n*\n+ + = 1\n(10 # y)11\n/\n0 1\n3 4\n= 10\nTherefore,\nSimilarly,\nTherefore,\nAs a consequence,\nE[L2] = 100\n11 \" 10\n11 = 90\nE[A] = E[L1]\" E[L2] = 18\n11 \" 90\n11 = 1620\n121 #13.39_ miles2 # 33.67_ km2\n5. Cookies, etc - 3pts\nNo standardized answer for this question, obviously."
    },
    {
      "category": "Resource",
      "title": "ps3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/ad6d6c6de802ac8a3798d7617254ad6a_ps3.pdf",
      "content": "ESD.86\nModels, Data, Inference for Socio-Technical Systems\nMassachusetts Institute of Technology\nEngineering Systems Division\nProblem Set #3\nIssued: Wednesday February 28, 2007. Due: Wednesday March 7, 2007 at 10:00am.\n1. Max and Min. Consider two r.v.s X and Y that are uniformly distributed and\nindependent. Random variable X is uniform over the interval [0,1]. Y is uniform over the\ninterval [0,2].\nLet\nW=Min{X, Y}\nZ =Max{X, Y}\n(a) Find the joint pdf for W and Z.\n(b) Find the pdf for W + Z.\n2. Random Incidence. Building a Car. Imagine that you are working on an assembly\nline and waiting for two components, fidgets and whoosies. As soon as you have the two\ncomponents you can complete the assembly of your reconstructed classic Pierce-Arrow\nautomobile1. These components arrive as independent random processes on adjacent\nconveyor belts. Fidgets arrive as a homogeneous Poisson process with mean inter-arrival\ntime of 10 minutes. Whoosies arrive as a renewal process with time TW between\nsuccessive renewals found from this pdf:\nfTW (t) = (1/2)(0.1)e\"0.1t + (1/2)(0.02)e\"0.02t t # 0.\nThe whoosie pdf is sometimes called the hyper-geometric pdf.\n(a) Find the mean time between arrivals of both fidgets and whoosies.\n(b) You have been on lunch break and you arrive back at your workstation, with the\npartially reconstructed Pierce-Arrow, and the fidget and whoosie arrival processes\nin full swing.\na. Find the mean time from your arrival back at your station until the first\nfidget arrives.\nb. Find the mean time from your arrival back at your station until the first\nwhoosie arrives.\nc. Find the mean time until both have arrived and you can complete your\nreconstruction of the Pierce-Arrow.\nFor a brief history of the Pierce-Arrow car, see\nhttp://www.antiquecar.com/index/listings/category766.htm\n\n3. Spatial Poisson on a Line. Ambulances patrol an infinitely long straight East-West\nhighway at spatial density of γ ambulances per mile. At any given time, the ambulances\nare positioned along the highway as a homogeneous spatial Poisson process. Being\npublic safety vehicles, ambulances are allowed to make U-turns anywhere, so that their\ndirection of travel is not relevant. You have a traffic accident at point x along the\nhighway and with your new GPS device you summon two ambulances, the closest one to\nthe East of your location and the closest one to the West of your location. Once,\nsummoned, each ambulance drives from its location to your location at a constant speed\nof 60 miles per hour to reach your location.\n(a) Determine the pdf for the time until the first ambulance arrives. Calculate the\nmean value.\n(b) Determine the pdf for the time until the second ambulance arrives. Calculate the\nmean value. Compare to the mean value found in part (a). Are you surprised in\nany way? Comment.\n4. Covered Rectangle. Homogeneous spatial Poisson processes have in two dimensions\nthe same property that homogeneous time Poisson processes have: Given that we have\nconducted our probabilistic experiment and we know only the exact number of Poisson\nentities within a fixed area, then - with no further information - the locations of these\nentities are independent and uniformly distributed over the fixed area. This property is\nanalogous to the unordered arrival times in a homogeneous time-Poisson process, as\nthese unordered times are independent and uniformly distributed over the fixed time\ninterval of interest. Suppose that the fixed area of interest is a 2-mile-by-10-mile\nrectangle, with sides parallel to the x and y axes, respectively. The 2-mile-by-10-mile\nrectangle is situated within a larger community in which a homogeneous spatial Poisson\nProcess operates. Now suppose that the spatial Poisson process has distributed its entities\nover the community. We are told only that the number of Poisson entities within the\nrectangle is exactly 10. We define the coverage rectangle as the smallest rectangle\nwithin the 2-by-10 mile rectangle that contains all 10 Poisson entities and that has sides\nparallel to the larger rectangle. Find the expected area of the coverage rectangle.\n5. Random Incidence - Cookies, etc. In your own ESD research or other professional\nactivity, think about the random incidence problems that are explained by the \"chips in\nchocolate chip cookies\" setup. That is, in your work there is the potential for major\nselection bias due to this phenomenon. Identify one focused topic in your research or\nprofessional activity in which such selection bias is or could be an issue. Try to frame,\nformulate and solve this problem is as complete detail as you can. Use the formulas\nwhere appropriate. If you have data to support your work, even better! If you want to\ndisplay results on a spreadsheet, terrific! If no such threats occur in your professional\nwork, then think of your personal life and possibilities there for this to happen. If you\nstill draw a blank, then do the analysis for a situation you read about in the media."
    },
    {
      "category": "Resource",
      "title": "abuses4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/dba0b5381f23f67a26e78d787d61b64a_abuses4.pdf",
      "content": "ESD.86 Spring 2007\nWeekly Contest #4\nDamien Bador\n\nEvaluate police work in New York City:\n\nNew York police stopped a half-million people in 2006, five times more than in 2002. Why? The\nanswer might be that a new system implemented to improve the performance of the police, called\nCompStat, might be pressing police officers to stop people with no real reason.\n\nThe problem with CompStat is that once they started tracking everything in numbers, what was\nsupposed to be a tactic to help solve crimes became a measuring stick by which police authorities\nwere judged. Unfortunately, in the weekly meetings about the CompStat reports that take place\nin New York city Police headquarters, it is now routine for commanders to be publicly criticized\nabout their \"stop-and-frisk\" numbers, questioned on why they are not \"doing more\": \"How come\nyou stopped 50 guys two months ago and only 40 last month?\", is quoted as an example.\n\nThere are reports that the department has become obsessed with statistics, so beholden to the\nnumbers that it is losing the trust of the neighborhoods it is supposed to protect. To reach the\n\"quota\" of stops that are needed to avoid being grilled about the number of stops in their\nprecincts, police are now stopping \"the guy just hanging out, the guy coming home from work,\"\nsaid a supervisor. \"That's because you get guys making stops just to keep the bosses happy.\nThey're forcing us to engage people we wouldn't normally engage.\"\n\nThe numbers, culled from the Stop, Question and Frisk reports, tell that 508,540 people were\nstopped last year, up dramatically from 97,296 in 2002. But we don't know how many of these\npeople were frisked. And we don't know how many of these stops yielded weapons or drugs. For\nexample, 186,841 people were stopped, at least in part, last year because they showed \"furtive\nmovements,\" one of the listed reasons that would allow an officer to stop someone.\n\nWriting UF-250, the numerical code for Stop, Question and Frisk Reports, has also being linked\nto having the overtime recognized. \"When you make it mandatory to write 250s it creates\nproblems,\" says a retired sergeant. Only 18 percent of last year's Stop, Question and Frisk forms\ninvolved people stopped because they \"fit a relevant description.\"\n\nSources:\nhttp://www.amny.com/news/local/am-rocco0312,0,3816067,print.story?coll=am-local-\nheadlines\nhttp://en.wikipedia.org/wiki/New_York_City_Police_Department"
    },
    {
      "category": "Resource",
      "title": "ps4_sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/bbc736973de33c1aa73dcc7c5f0ff9ff_ps4_sol.pdf",
      "content": "Problem Set #4\n1. Pedestrian Light, revisited - 3 pts\nLittle's Law: L = λ.W and Lq = λ.Wq, with λ = λL + λR, W = 1\nu + Wq and L = Lq + \"\nu .\nRule A: WA = T\n2 \" LA = (#L + #R) T\nRule B: WB =\nN0 \"1\n2(#L + #R) $ LB = N0 \"1\nRule C: WC = T0\n2 1+\n1+ (\"L + \"R).T0\n#\n$ %\n&\n' ( ) LC = (\"L + \"R).T0\n1+\n1+ (\"L + \"R).T0\n#\n$ %\n&\n' (\n2. Squaring - 4 pts\na) 2 pts\nLet FY(y) be the CDF of fY(y):\nFY (y) = P[Y \" y] = P[X 2 \" y] = P # y \" X \"\ny\n[\n]\nFY (y) =\nfX (x).dx = FX\ny\n(\n) #\n#\ny\ny\n$\nFX # y\n(\n)\nThen, fY (y) = d\ndy FX\ny\n(\n) \" FX \" y\n(\n)\n[\n] =\n2. y\nfX\ny\n(\n) + fX \" y\n(\n)\n[\n]\nb) 2 pts\nfX (X) = 1\n3,\"X # [$1;2]\nTherefore, fY (y) =\n2. y\nfX\ny\n(\n) + fX \" y\n(\n)\n[\n] =\n3. y\n,#y $ [0;1]\n6. y\n,#y $ [1;4]\n%\n&\n' '\n(\n'\n'\n\n3. M/M/k Queue - 6.5 pts\na) 2 pts\nP(n) =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn! P0,)n * [0;k +1]\n\"\nμ\n#\n$ %\n&\n' (\nn\nk n+k.k!P0,)n * [k;,[\n-\n.\n/\n/\n/\n/\n/\n/\nSince\nPn =1\" P0\n#\nμ\n$\n% &\n'\n( )\nn\nn!\nn= 0\nk*1\n+\n+\n#\nμ\n$\n% &\n'\n( )\nn\nk n*k.k!\nn= k\n,\n+\n-\n.\n/\n/\n/\n/\nn= 0\n,\n+\n=1, we have: P0 =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+\n\"\nμ\n#\n$ %\n&\n' (\nn\nk n)k.k!\nn= k\n+\n*\nThis becomes:\nP0 =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+\n\"\nμ\n#\n$ %\n&\n' (\nn\nk n)k.k!\nn= k\n+\n*\n=\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+ \"\nμ\n#\n$ %\n&\n' (\nk 1\nk! ,\n\"\nμ.k\n#\n$ %\n&\n' (\nm\nm= 0\n+\n*\n=\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+ \"\nμ\n#\n$ %\n&\n' (\nk 1\nk! ,\n1) \"\nμ.k\nP0 =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+\n\"k\nμk)1.(μ.k ) \")(k )1)!\nWe can then deduce the exact value for each Pn.\nb) 1.5 pt\nLq =\n(n \" k).Pn =\nn= k\n#\n$\nP0\n%\nμ\n&\n' (\n)\n* +\nk 1\nk! ,\nm\n%\nk.μ\n&\n' (\n)\n* +\nm\nm= 0\n#\n$\n= P0\n%\nμ\n&\n' (\n)\n* +\nk 1\nk! ,\n%\nk.μ\n1\" %\nk.μ\n&\n' (\n)\n* +\n2 = %\nμ\n&\n' (\n)\n* +\nk+1\nP0\nk.k! 1\" %\nk.μ\n&\n' (\n)\n* +\nSince W = Wq + 1\nμ = Lq\n\" + 1\nμ ,\nW =\n\"k.P0\nμk+1.k.k! 1# \"\nk.μ\n$\n% &\n'\n( )\n2 + 1\nμ\n\nc) 1.5 pts\nA simulation shows that, when fixing Wq = 1 and 1/μ = 1 min, the progression of the maximum\nnumber of calls is faster than the progression of the number of servers required. In addition, the\nvalue (1-ρ) tends to 0.\nd) 1.5 pts\nIn such a case, the service quality is extremely sensitive to volume fluctuation. A mere 7%\nincrease in calls and the call center can no longer service customers fast enough: the expected\nwaiting time becomes infinite.\nOn February 27th, trading volumes exceeded the trading capacity of the stock market, creating an\nenormous queue of transactions. The value of stocks lagged far behind, creating confusion\namong traders.\n4. M/M/k Queue with discouraged voters - 6.5 pts\na) 2 pts\nSince\n(i \" k) = (n \" k)(n \" k +1)\ni= k\nn\n#\nWe have: P(n) =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn! P0,)n * [0;k +1]\n\"\nμ\n#\n$ %\n&\n' (\nn\nk n+k.k!P0 , e\n+(n+k)(n+k+1)\n,)n * [k;-[\n.\n/\nPn =1\" P0\n#\nμ\n$\n% &\n'\n( )\nn\nn!\nn= 0\nk*1\n+\n+\n#\nμ\n$\n% &\n'\n( )\nn\nk n*k.k!\nn= k\n,\n+\n- e\n*(n*k)(n*k+1)\n.\n/\nn= 0\n,\n+\n=1\nThis becomes: P0 =\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+\n\"\nμ\n#\n$ %\n&\n' (\nn\nk n)k.k! + e\n)(n)k)(n)k+1)\nn= k\n,\n*\n=\n\"\nμ\n#\n$ %\n&\n' (\nn\nn!\nn= 0\nk)1\n*\n+ \"\nμ\n#\n$ %\n&\n' (\nk 1\nk! +\n\"\nμ.k\n#\n$ %\n&\n' (\nm\n+ e\n) m(m+1)\nm= 0\n,\n*\nb) 1.5 pts\n\"Av =\n\"n.Pn\nn= 0\n#\n$\n=\n\".Pn +\n\".e\n% n%k\n10 .Pn\nn= k\n#\n$\nn= 0\nk%1\n$\n\nIn steady state, we get: Wq = Lq\n\"Av\n=\n(n # k).Pn\nn= k\n$\n%\n\".Pn +\n\".e\n# n#k\n10 .Pn\nn= k\n$\n%\nn= 0\nk#1\n%\n=\nm.Pm+k\nm= 0\n$\n%\n\"\nPn +\ne\n# m\n10.Pm+k\nm= 0\n$\n%\nn= 0\nk#1\n%\n&\n'\n(\n)\n*\n+\nc) 1.5 pt\nNo, it's not. People would balked out do are not taken into account here. The quality of service\nwill be overestimated.\nd) 1.5 pt\nPercentage of people who balked: PBal =1\" #Av\n# =1\"\nPn +\ne\n\" m\n10.Pm+k\nm= 0\n$\n%\nn= 0\nk\"1\n%\n&\n'\n(\n)\n*\n+"
    },
    {
      "category": "Resource",
      "title": "ps4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/80dcb2af3463a26570264eec9c479f62_ps4.pdf",
      "content": "fX (x) for all x,\"# < x < +#. Define new r.v. Y=X2\ntrue: fY (y) =\n2 y\n[ fX ( y) + fX (\" y)]\n(b) Apply your result to a situation in which X is uniformly distributed over the\ninterval [-1, +2].\nESD.86\nModels, Data, Inference for Socio-Technical Systems\nMassachusetts Institute of Technology\nEngineering Systems Division\nProblem Set #4\nIssued: Wednesday March 7, 2007. Due: Wednesday March 14, 2007.\n1. Pedestrian Light, revisited. Apply Little's Law to the 77 Massachusetts Avenue\nPedestrian Crossing light problem, to find L for each of the 3 ways of operating the light.\nRecall, for each of the 3 modes of operation, you have already derived W or Wq, so this\nshould be easy!\n2. Squaring. We'll be doing lots of squaring of random variables in the 2nd half of this\nsubject. So, let's get started now.\n(a) Suppose we know all about random variable X, having pdf\n. Show that the following is\n. Explain in words what this means.\n3. M/M/k Queue. Let's examine the queue with homogeneous Poisson arrivals (rate λ),\nnegative exponential service times (mean 1/μ), k identical servers, infinite queue capacity\nand no balking or reneging. We assume the steady state exists, implying that λ< kμ. Let\nthe state of the system be the number of customers in the system, both in service and in\nqueue.\n(a) Find the steady state probability Pn=P{n customers in the system}, n=0,1,2,...\n(b) Find W. (Hint: think of Little's Law and that finding any one of 4 quantities gets\nyou the other 3 at 'no extra charge.' So, to find W all you need to do is compute\nthe easiest one of the 4 quantities related to Little's Law.)\n(c) The systems designers of call centers and other systems involving queueing like\nto enjoy economies of scale. Yet they have service performance standards to\nmeet. To many in the services industries, the quantity (1-ρ) is 'paid lost time',\nthat is, time that a server is not working serving customers. Suppose a service\nsystem that can be modeled as an M/M/k Queue has a performance standard that\nstates that the mean wait in queue shall be no greater than one minute. Assume\nthe mean service time is 1/μ = one minute. Using your results above and an\nExcel or similar spreadsheet, examine how (1-ρ) can be made smaller as the call\nvolume λ increases, the number of servers k increases, while at all times meeting\nthe service standard of one minute or less mean queue wait time. Show that (1-ρ)\n\ncan become much smaller than the value associated with one server as call\nvolume increases, the number of servers increases, but not in direct proportion to\ncall volume.\n(d) Suppose you designed your system as in Part (c), and you have 20 servers in your\nsystem. How sensitive are you to errors in forecasted call volumes? What\nhappens if tomorrow's call volume is 10% over forecasts? This is a matter of\n'ilities,\" reliability, robustness, resilience. Did you follow in the media what\nhappened to stock traders and brokers and even stock indices as trading volume\nzoomed far above routine averages on Tuesday February 27?\n4. M/M/k Queue with Discouraged Voters. We now want to look at a variation of the\nqueue of problem 3. Suppose it is Election Day and we focus on a given voting precinct.\nSystems managers have a choice before Election Day: how many voting machines\n(servers) to place in each precinct. Too many in one precinct may mean no queues at that\nprecinct but that another precinct is short-changed. Too few implies long lines of voters,\nsome of whom may not join the line because it is too long.\n(a) Suppose arrivals that see a line of queued voters can get discouraged and\nnot join the line. Suppose field research has shown that the probability\nthat a potential voter will join the system and vote is equal to\npV (nq) = e\n\"0.1nq, where nq # number of voters in queue upon arrival, nq = 0,1,2,...\n.\nFor simplicity we assume no reneging, that is, once someone joins the\nvoters' line, they stay and eventually cast their vote. Model this system as\na Markov Birth and Death Queueing system. Find the steady state\nprobabilities, i.e.,\nPn \" {n voters in the system, both in queue and in voting booths}, n = 0,1,2,...\nYou can do this either by explicit formula or by applying the theory to\ngenerate numerical results for various plausible values of parameters.\n(b) If you interviewed a large sample of voters leaving the polling place, what\nis the mean time in queue that they would report to you (assuming\neveryone is accurate and that you have a large sample size).\n(c) Is the answer you found in part (b) indicative of the quality of service\nprovided by this voting precinct? Why or why not?\n(d) Some say that long lines producing discouraged voters is a form of\n'stealth disenfranchisement.' That is, the long lines represent a subtle and\ndifficult-to-track method of removing a certain fraction of voters from the\nvoting process. For your model here, can you estimate the fraction of\nvoters who, by balking at joining the line, may be said to have suffered\nfrom such stealth disenfranchisement? (There are lawsuits pending in\nOhio that allege just this phenomenon, claiming that the stealth\ndisenfranchisement in a recent federal election was deliberate\nmanipulation of the system.)"
    },
    {
      "category": "Exam",
      "title": "exam2_practice.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/71393387a10e36dc32c510f5a7f250f6_exam2_practice.pdf",
      "content": "ESD.86\nModels, Data, Inference for Socio-Technical Systems\n\nMIT, Spring 2007\n\nExam #2 (Practice Exam!!!!!!!!)\nOpen Book, Open Notes, Individual Effort\n\nNOTE: This practice exam is intentionally too long. I just kept adding questions so you\ncan practice more. The real text will fit into the 1.5 hour slot. The points assigned to\neach question are about what I would assign in the real test, but the total test adds up to\nmore than 100 points. Understand?\n\n1. (5 pts) You are seeking to calibrate a thermometer. You wish to determine the\nregression line relating the output of the device (in Volts) to the input to be measured\n(temperature in degrees Celsius). What are the units of the slope of the regression line?\nWhat are the units of the sum of squares of the error? What are the units of R2?\n\n2. (5 pts) Find the mean, median, and mode of the sample 34, 29, 26, 37, 26.\n\n3). (5 points) Provide an example of an engineering system and a measurable parameter\nthereof for which the sample median is a more appropriate way to communicate the\ncentral tendency than the sample average. Explain your reasoning briefly.\n\n4) (2 pts) In order to resolve only the main effects in a system with 7 control factors\neach having 2 levels (with no replicates), how many experiments would one would\nneed to conduct? You may assume that two-factor and higher order interactions are\nnegligibly small.\n\n5. (5 pts) The graph below resulted from a polynomial regression of six data points.\nSketch a plot of the residuals versus the independent variable x.\n\nA Regression Curve\n\nx\ny\nyˆ\n\nYou may make your sketch here. Please\nlabel your axes appropriately.\n\n6. (5 pts) You perform a linear regression of 100 data points (n=100). There are two\nindependent variables x1 and x2. The regression R2 is 0.72. Both β1 and β2 pass a t\ntest for significance. You decide to add the interaction x1x2 to the model. Select all\nthe things that cannot happen:\na) R2 decreases\nc) The adjusted R2 decreases\nd) All three coefficients β1, β2, and β12 fail the t test for significance\n\n7. (15 pts) Provide a persuasive and intellectually rigorous defense of your answer to\nquestion #5 above. The answer need not be primarily implemented with symbolic\nmathematics, but it could be.\n\n8. Weibull describes a graph below as an example of how Weibull plots may reveal\nthat two sample distributions have been mixed.\n\na) (5 pts) Briefly describe the procedure to make such a plot.\n\nb) (5 pts) Under what conditions will you get two separate linear regions as depicted\nhere? Under what conditions will you not get two different regions and therefore fail\nto detect there are two populations mixed?\n\nCopyright (c) 1951 by ASME. Used with permission.\n\n9. (15 pts) A key outcome of this class has been to show that beer has deeply influenced\nthe development of statistics (and human affairs more generally). Below are some\nexcerpts from a paper that illustrates the continuing influence. Select an idea expressed\nhere and critique it. Suggest a better approach to either the analysis or the\ncommunication of the results.\n\nFrom Koksalan, M., N. Erkip and H. Moskowitz. \"Explaining beer demand: A residual\nmodeling regression approach using statistical process control.\" Int. J. Production\nEconomics 58 (1999): 265-276.\n\nThe regression model was run for a three-year period from 1989 to 1991, since the\nsales figures for the cities were most reliable for this period. There were 42 data\npoints (cities) for each year, yielding a total of 126 data points. None of the pairwise\ncorrelation estimates between independent variables were very high, eliminating\nconcerns of multicollinearity.\nWe ran the regression model by the backward elimination of insignifcant independent\nvariables.\nThe value of the adjusted R2 turned out to be 0.60 indicating that about 60% of the\nvariation in the dependent variable can be explained. We present a scatterplot of\nstandardized residuals vs. predicted sales in Fig. 1, which shows an undesirable\ntrend indicating that the variance may not be constant (the horizontal lines at $1\nstandard deviation points and the highlighted data points are referred to in the next\nsection). Our various efforts of transforming the data did not lead to an improvement.\nThinking that increasing variance could be caused by missing variables, we\ndeveloped and implemented an SPC-based approach discussed below to diagnose\nthe existence of such a possibility.\n\n10. In the spreadsheet below:\n\na) (5 pts) Why are there 4 DOF for the regression?\n\nb) (5 pts) The standard error is listed as 5.978... Name something you can do to inspect\nthe graphs and check that this is not substantially off.\n\n11. The passage below is from a poem by Oliver Wendell Holmes:\n\nHAVE you heard of the wonderful one-hoss-shay, that was built in such a logical way it\nran a hundred years to a day...\nNow in building of chaises, I tell you what, there is always somewhere a weakest spot,--\nin hub, tire, felloe, in spring or thill, in panel, or crossbar, or floor, or sill...\nBut the Deacon swore .. he would build one shay to beat the taown 'n' the keounty 'n' all\nthe kentry raoun'; It should be so built that it couldn' break daown! --\"Fur,\" said the\nDeacon, \"t 's mighty plain thut the weakes' place mus' stan' the strain; 'n' the way t' fix it,\nuz I maintain, is only jest t' make that place uz strong uz the rest\"...\nSo the Deacon inquired of the village folk where he could find the strongest oak, That\ncould n't be split nor bent nor broke,--\nEighteen hundred and twenty came;-- Running as usual; much the same. Thirty and forty\nat last arrive, And then come fifty, and fifty-five...\nThere are traces of age in the one-hoss-shay- A general flavor of mild decay, But nothing\nlocal, as one may say. There couldn't be,--for the Deacon's art had made it so like in\nevery part that there wasn't a chance for one to start...\nAnd yet, as a whole, it is past a doubt in another hour it will be worn out!\nFirst a shiver, and then a thrill, Then something decidedly like a spill,-- ...What do you\nthink the parson found, when he got up and stared around? The poor old chaise in a heap\nor mound, as if it had been to the mill and ground! You see, of course, if you're not a\ndunce, how it went to pieces all at once,-- all at once, and nothing first,-- just as bubbles\ndo when they burst.\nEnd of the wonderful one-hoss-shay. Logic is logic. That's all I say.\n\na) (5 pts) What would the Weibull distributions of the individual components of the one\nhoss shay have to look like for the events in the poem to be somewhat likely.\n\nb) (10 pts) Discuss the merits and demerits of actually designing engineering systems in\nthe way the Deacon's Masterpiece is arranged.\n\n12. (10 pts) I manufacture two rectangles of material with a known density and\nthickness. The sides of the two rectangles are normally distributed with means 1 and 2\nmeters for each rectangle and standard deviations of 1% of the nominal. What is the\ndistribution of the ratio of the weights of the two sheets?"
    },
    {
      "category": "Exam",
      "title": "exam2_review.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/c26e72a41029637b1058b6b18dc30e54_exam2_review.pdf",
      "content": "ESD.86\nExam #2 Review\nDan Frey\nAssociate Professor of Mechanical Engineering and Engineering Systems\n\nSome Study Suggestions\n- Review all the lecture notes\n- If there is a concept test, know the answer and WHY it's\nthe right answer and WHY other answers are wrong\n- If there is a word you don't understand, look it up, talk to\ncolleagues...\n- Review the last two homeworks\n- For concepts, not details\n- Review the reading assignments (April and May)\n- For concepts, not details\n- If there is a graph or table, make sure you can describe\nhow to use it and the procedure by which it was made\n\nSuggested Resources\n(I would not emphasize this as much as the lecture\nnotes and problem sets)\nWu and Hamada. Experiments: Planning, Analysis and Parameter\nDesign Optimization. Chapters 4 and 5 are relevant. The \"practical\nsummaries\" are good condensations of the main points. Many of\nthe exercises are close to what you might find on the test. Many are\nnot e.g. \"prove that ...\" or \"find the defining words and resolution...\"\nProblem Solvers: Statistics (Research & Education Association)\nSolving lots of problems is good practice. There are books filled with\nproblems and solutions. A large fraction of these involve lots of\nnumber crunching, these are fine to review but that's not the sort of\nquestion I intend to give. Many are conceptual or involve modest\ncomputations or estimation. Those are closer in style to what you\ncan expect. There are many topics we didn't cover so you don't have\nto study them.\n\nWeibull's Derivation\nn\nn\nP\nP\n)\n1(\n-\n=\n-\nx\nx\nLet's define a cdf for each link meaning the link will fail at a\nload X less than or equal to x as P(X≤x)=F(x)\nCall Pn the probability that a chain will fail under a load of x\nIf the chain does not fail, it's because all n links did not fail\nIf the n link strengths are probabilistically independent\nWeibull, W., 1951,\"A Statistical Distribution Function of Wide Applicability,\" J. of Appl. Mech.\n\nSome Terms Related to Estimation\n- Consistent - for any c\n- Unbiased -\n- Minimum variance\n(\n)\nlim\n=\n≥\n-\ninf\n→\nc\nP\nn\nθ\nθ\n)\nθ\nθ =\n)\n(\n)\nE\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎣\n⎡\n⎟⎠\n⎞\n⎜⎝\n⎛\n∂\n∂\n=\n)\n(\nln\n)\nvar(\nθ\nθ\nX\nf\nnE\n)\nMLEs\nare\nMLEs are not always\nMLEs are\npretty close\n\nComplex Distributions\nWeibull, W., 1951,\"A Statistical Distribution Function of Wide Applicability,\" J. of Appl. Mech.\nCopyright (c) 1951 by ASME. Used with permission.\nCopyright (c) 1951 by ASME. Used with permission.\n\nLooking for Further Evidence of\nTwo Populations\nNo evidence of bi-\nmodality in fatigue data\nClear evidence of bi-\nmodality in strength data\nCopyright (c) 1951 by ASME. Used with permission.\nCopyright (c) 1951 by ASME. Used with permission.\n\nReliability Terminology\n- Reliability function R(t) -- The probability\nthat a product will continue to meet its\nspecifications over a time interval\n- Mean Time to Failure MTTF -- The average\ntime T before a unit fails\n- Instantaneous failure rate λ(t)\n∫\ninf\n=\n)\n( dt\nt\nR\nMTTF\n)\n\nto\nsurvives\n\nSystem\n\nto\nsurvives\n\nSystem\nPr(\n)\n(\nt\ndt\nt\nt\n+\n=\nλ\n∫\n=\n-\nt\nd\ne\nt\nR\n)\n(\n)\n(\nξ\nξ\nλ\n\nConstant Failure Rates\n\"When the system operating time is the MTBF, the reliability is 37%\"\n- Blanchard and Fabrycky\nR(t)\n1.0\nR(t)=e-λt\n0.5\nt\nMTBF\n\nFisher's Null Hypothesis Testing\n1. Set up a statistical null hypothesis. The null\nneed not be a nil hypothesis (i.e., zero\ndifference).\n2. Report the exact level of significance ... Do not\nuse a conventional 5% level, and do not talk\nabout accepting or rejecting hypotheses.\n3. Use this procedure only if you know very little\nabout the problem at hand.\n\nGigernezer's Quiz\n1. You have absolutely disproved the null hypothesis\n2. You have found the probability of the null hypothesis being true.\n3. You have absolutely proved your experimental hypothesis (that\nthere is a difference between the population means).\n4. You can deduce the probability of the experimental hypothesis\nbeing true.\n5. You know, if you decide to reject the null hypothesis, the\nprobability that you are making the wrong decision.\n6. You have a reliable experimental finding in the sense that if,\nhypothetically, the experiment were repeated a great number of\ntimes, you would obtain a significant result on 99% of occasions.\nSuppose you have a treatment that you suspect may alter\nperformance on a certain task. You compare the means of your\ncontrol and experimental groups (say 20 subjects in each sample).\nFurther, suppose you use a simple independent means t-test and\nyour result is significant (t = 2.7, d.f. = 18, p = 0.01). Please mark\neach of the statements below as \"true\" or \"false.\" ...\n\nConcept Question\n- This Matlab code repreatedly generates and tests\nsimulated \"data\"\n- 20 \"subjects\" in the control and treatment groups\n- Both normally distributed with the same mean\n- How often will the t-test reject H0 (α=0.01)?\nfor i=1:1000\ncontrol=random('Normal',0,1,1,20);\ntrt=random('Normal',0,1,1,20);\nreject_null(i) = ttest2(control,trt,0.01);\nend\nmean(reject_null)\n1)\n~99% of the time\n2)\n~1% of the time\n3)\n~50% of the time\n4)\nNone of the above\n\nConcept Question\n- This Matlab code repreatedly generates and tests\nsimulated \"data\"\n- 20 \"subjects\" in the control and treatment groups\n- Both normally distributed with the different means\n- How often will the t-test reject H0 (α=0.01)?\nfor i=1:1000\ncontrol=random('Normal',0,1,1,200);\ntrt= random('Normal',1,1,1,200);\nreject_null(i) = ttest2(control,trt,0.01);\nend\nmean(reject_null)\n1)\n~99% of the time\n2)\n~1% of the time\n3)\n~50% of the time\n4)\nNone of the above\n\nConcept Question\n- How do \"effect\" and \"alpha\" affect the rate\nat which the t-test rejects H0 ?\na)\n↑effect, ↑rejects\nb)\n↑effect, ↓rejects\nc) ↑alpha, ↑rejects\nd)\n↑alpha, ↓rejects\n1)\na & c\n2)\na & d\n3)\nb & c\n4)\nb & d\neffect=1;alpha=0.01;\nfor i=1:1000\ncontrol=random('Normal',0,1,1,20);\ntrt= random('Normal',effect,1,1,20);\nreject_null(i) = ttest2(control,trt,alpha);\nend\nmean(reject_null)\n\nNP Framework and Two Types of Error\n- Set a critical value c of a test statistic T or else set the\ndesired confidence level or \"size\" α\n- Observe data X\n- Reject H1 if the test statistic T(X)≥c\n- Probability of Type I Error - The probability of T(X)<c │H1\n- (i.e. the probability of rejecting H1 given H1 is true\n- Probability of Type II Error - The probability of T(X)≥c │H2\n- (i.e. the probability of not rejecting H1 given H2 is true\n- The power of a test is 1 - probability of Type II Error\n- In the N-P framework, power is maximized subject to Type\nI error being set to a fixed critical value c or of α\nor other confidence\nregion (e.g. for \"two-\ntailed\" tests)\n\nMeasures of Central Tendency\n- Arithmetic mean\n- an unbiased estimate of\n- Median\n- Mode - The most frequently\nobserved value in the sample\n∫\n=\n=\nS\nx\nx\nx\nxf\nx\nE\nd\n)\n(\n)\n(\nμ\n∑\n=\n=\nn\ni\ni\nX\nn\nX\n\neven\n\nis\n\nif\n\nodd\n\nis\n\nif\n\n⎪\n⎩\n⎪\n⎨\n⎧\n⎟⎠\n⎞\n⎜⎝\n⎛\n+\n=\n+\n+\nn\nX\nX\nn\nX\nn\nn\nn\n\nConfidence Intervals\n- Assuming a given distribution and a sample\nsize n and a given value of the parameter θ\nthe 95% confidence interval from U to V is s.t.\nthe estimate of the parameter\n- The confidence interval depends on the\nconfidence level, the sample variance, and\nthe sample size\n%\n)\nˆ\nPr(\n=\n<\n<\nθ\nθ\nV\nU\nθˆ\n\nMeasures of Dispersion\n- Population Variance\n- Sample variance\n- an unbiased estimate of\n- nth central moment\n- nth moment about m\n)\n))\n(\n((\nx\nE\nx\nE\n-\n=\nσ\n∑\n-\n-\n-\n=\nn\ni\ni\nX\nX\nn\nS\n)\n(\n)\n))\n(\n((\nn\nx\nE\nx\nE\n-\n)\n)\n((\nn\nm\nx\nE\n-\n(\n)\n∑\n=\n-\n=\nn\ni\ni\nX\nX\nn\nX\nVAR\n)\n(\n\nSkewness and Kurtosis\n- Skewness\n- Kurtosis\n)\n))\n(\n((\nx\nE\nx\nE\n-\n)\n))\n(\n((\nx\nE\nx\nE\n-\npositively skewed distribution\npositive kurtosis\n\nCorrelation Coefficient\n- Sample\n- Which is an estimate of\n(\n)(\n)\n(\n)\nY\nX\nn\ni\ni\ni\nS\nS\nn\nY\nY\nX\nX\nr\n-\n-\n-\n= ∑\n=\n∑\n-\n-\n-\n=\nn\ni\ni\nX\nX\nX\nn\nS\n)\n(\ny\nx\ny\nE\ny\nx\nE\nx\nE\nσ\nσ\n)))\n(\n))(\n(\n((\n-\n-\n\nBut What\nDoes it\nMean?\nCourtesy of American Statistical Association. Used with permission.\n\nWhat is Linear Regression?\nε\nβ\nα\n+\n+\n=\nx\nY\nrandom\ntheoretical\nparameters\nindependent\nvariable\n1. Form a probabilistic model\n2. Get a sample of data in pairs (Xi,Yi), i=1...n\n3. Estimate the parameters of the model from the data\nrandom\nE(ε)=0\n\nThe Method of Least Squares\n∑\n=\nn\ni\nie\ni\ni\ni\ny\ny\ne\nˆ\n-\n=\n(x17,y17)\ne17\nGiven a set of n data\npoints (x,y) pairs\nThere exists a unique\nline\nthat minimizes the\nresidual sum of squares\nbx\na\ny\n+\n=\nˆ\n∑\n=\n-\n=\nn\ni\ni\ne\ne\nn\ns\n\nMatlab Code for Regression\np = polyfit(x,Y,1)\ny_hat=polyval(p,x);\nplot(x,y_hat,'-','Color', 'g')\n\nConcept Question\nYou are seeking to calibrate a load cell. You wish to\ndetermine the regression line relating voltage (in Volts)\nto force (in Newtons). What are the units of\na, b, Sxx, and Sxy respectively?\n1) N, N, N, and N\n2) V, V, V2, and V2\n3) V, V/N, N2, and VN\n4) V/N, N, VN, and V2\n5) None of the variables have units\n\nRegression Curve vs Prediction Equation\nε\nβ\nα\n+\n+\n=\nx\nY\nrandom\nRandom\nE(ε)=0\ntheoretical\nparameters\nindependent variable\nbx\na\ny\n+\n=\nˆ\ncomputed estimates of\ntheoretical parameters α and β\nestimated E(Y ׀x)\nRegression Curve\nPrediction Equation\n\nEvaporation vs Air Velocity\nHypothesis Tests\nAir vel\n(cm/sec)\nEvap coeff.\n(mm2/sec)\n0.18\n0.37\n0.35\n0.78\n0.56\n0.75\n1.18\n1.36\n1.17\n1.65\nAir vel (cmEvap coeff. (mm2/sec)\n0.18\nSUMMARY OUTPUT\n0.37\n0.35\nRegression Statistics\n0.78\nMultiple R\n0.934165\n0.56\nR Square\n0.872665\n0.75\nAdjusted R Square\n0.854474\n1.18\nStandard Error\n0.159551\n1.36\nObservations\n1.17\nANOVA\ndf\nSS\nMS\nF\nignificance F\nRegression\n1 1.221227 1.221227 47.97306 0.000226\nResidual\n7 0.178196 0.025457\nTotal\n8 1.399422\nCoefficientsandard Err\nt Stat\nP-value Lower 95%Upper 95%ower 95.0%\nUpper 95.0%\nIntercept\n0.102444 0.106865 0.958637 0.369673\n-0.15025 0.355139\n-0.15025 0.355139\nX Variable 1\n0.003567 0.000515 6.926259 0.000226 0.002349 0.004784 0.002349 0.004784\nRESIDUAL OUTPUT\nPROBABILITY OUTPUT\nObservation\nPredicted YResiduals dard Residuals\nPercentile\nY\n1 0.173778 0.006222 0.041691\n5.555556\n0.18\n2 0.316444 0.053556\n0.35884\n16.66667\n0.35\n3 0.459111\n-0.10911\n-0.73108\n27.77778\n0.37\n4 0.601778 0.178222 1.194149\n38.88889\n0.56\n5 0.744444\n-0.18444\n-1.23584\n0.75\n6 0.887111\n-0.13711\n-0.91869\n61.11111\n0.78\n7 1.029778 0.150222 1.006539\n72.22222\n1.17\n8 1.172444 0.187556 1.256685\n83.33333\n1.18\n9 1.315111\n-0.14511\n-0.97229\n94.44444\n1.36\nX Variable 1 Residual Plot\n-0.4\n-0.2\n0.2\n0.4\nX Variable 1\nResiduals\nX Variable 1 Line Fit Plot\n0.5\n1.5\nX Variable 1\nY\nY\nPredicted\nNormal Probability Plot\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\nSample Percentile\nY\n\nBayes' Theorem\n)\nPr(\n)\nPr(\n)\nPr(\n)\nPr(\nB\nA\nB\nA\nB\nA\n=\n)\nPr(\n)\nPr(\n)\nPr(\nB\nB\nA\nB\nA\n∩\n≡\nB\nU\n)\nPr(\n)\nPr(\n)\nPr(\nA\nB\nA\nA\nB\n∩\n≡\nwith a bit of algebra\nA\nA∩B\n\nFalse Discovery Rates\nImage removed due to copyright restrictions.\nSource: Figure 2 in Efron, Bradley. \"Modern Science and the Bayesian-Frequentist Controversy.\"\nhttp://www-stat.stanford.edu/~brad/papers/NEW-ModSci_2005.pdf\n\nSingle Factor Experiments\n- A single experimental factor is varied\n- The parameter takes on various levels\nObservations\nCotton\nweight\npercentage\nFiber strength in lb/in2\nexperimental\nfactor\nEach cell\nis a yij\nEach row\nis a\ntreatment i\na=5 replicates\n\nBreakdown of Sum Squares\nSS due to mean\n∑∑\n=\n=\n=\na\ni\nn\nj\nijy\nGTSS\n..y\nn\n=\n∑∑\n=\n=\n-\n=\na\ni\nn\nj\nij\nT\ny\ny\nSS\n..)\n(\nE\nSS\n∑\n=\n-\n=\na\ni\ni\nTreatments\ny\ny\nn\nSS\n..\n.\n)\n(\n\"Grand Total\nSum of Squares\"\n\"Total Sum of\nSquares\"\n\nBreakdown of DOF\nN\nnumber of y values\ndue to the mean\nN-1\ntotal sum of squares\na-1\nfor the treatments\nN-a\nfor error\n\nWhat is a \"Degree of Freedom\"?\n- How many scalar values are needed to\nunambiguously describe the state of this object?\n- What if I were to fix the x position of a corner?\n\nWhat is a \"Degree of Freedom\"?\n- How many scalar values are needed to\nunambiguously describe the outcome o this\nexperiment?\n- What if I were to tell you ?\n- What if I were to tell you ?\nObservations\nCotton\nweight\npercentage\n..y\n...\n\n.\n=\ni\nyi\n\nAdding h.o.t. to the Model Equation\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n=\nny\ny\ny\nM\ny\n⎥\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n=\nn\nn\nn\nn\nn\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nM\nO\nM\nM\nM\nX\nEach row of X\nis paired with\nan observation\nThere are n\nobservations of\nthe response\nYou can add\ninteractions\nYou can add\ncurvature\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n=\nβ\nβ\nβ\nβ\nβ\nβ\n\nBreakdown of Sum Squares\nSS due to mean\n∑\n=\n=\nn\ni\niy\nGTSS\ny\nn\n=\n∑\n=\n-\n=\nn\ni\ni\nT\ny\ny\nSS\n2)\n(\n∑\n=\n=\nn\ni\ni\nE\nSS\ne\n∑\n=\n-\n=\nn\ni\ni\nR\ny\nSS\n2)\nˆ(y\nPE\nSS\nLOF\nSS\n\"Grand Total\nSum of Squares\"\n\nBreakdown of DOF\nn\nnumber of y values\ndue to the mean\nn-1\ntotal sum of squares\nk\nfor the regression\nn-k-1\nfor error\n\nEstimation of the Error Variance σ2\nε\nXβ\ny\n+\n=\nRemember the the model equation\nIf assumptions of the\nmodel equation hold, then\nSo an unbiased\nestimate of σ2 is\n)1\n(\nˆ 2\n-\n-\n=\nk\nn\nSSE\nσ\n(\n)\n)1\n(\nσ\n=\n-\n-k\nn\nSS\nE\nE\n)\n,0\n(\n~\nσ\nN\nε\n\nTest for Significance\nIndividual Coefficients\njj\nj\nC\nt\nˆ\nˆ\nσ\nβ\n=\nReject H0 if\n,2\n/\n-\n-\n>\nk\nn\nt\nt\nα\nThe hypotheses are\n:\n:\n=\n=\nj\nj\nH\nH\nβ\nβ\nThe test statistic is\n(\n)\n-\n=\nX\nXT\nC\nStandard error\njj\nC\n2ˆσ\n\nFactorial Experiments\nCuboidal Representation\nx1\n+\n-\n+\n-\n+\n-\nExhaustive search of the space of discrete 2-level factors is the\nfull factorial 23 experimental design\nx2\nx3\n\nAdding Center Points\nx1\n+\n-\n+\n-\n+\n-\nCenter points allow an experimenter to check for curvature\nand, if replicated, allow for an estimate of\npure experimental error\nx2\nx3\n\nConcept Test\n- You perform a linear regression of 100 data points\n(n=100). There are two independent variables x1 and x2.\nThe regression R2 is 0.72. Both β1 and β2 pass a t test\nfor significance. You decide to add the interaction x1x2\nto the model. Select all the things that cannot happen:\n1) Absolute value of β1decreases\n2) β1changes sign\n3) R2 decreases\n4) β1fails the t test for significance\n\nScreening Design\n- What is the objective of screening?\n- What is special about this matrix of 1s and -1s?\nImage removed due to copyright restrictions.\nTABLE 2: Design I: Layout and Data for 28 -4\nIV Screening D\n\nesign in Box and Liu, 1999.\n\nAnalysis of Variance\n- What would you conclude about lack of fit?\n- What is being used as the denominator of F?\nImage removed due to copyright restrictions.\nTABLE\n\n10: Design III: Analysis of Variance for Completed Composite Design in Box and Liu, 1999.\n\nConcept Question\n[\n])1(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n-\n-\n-\n-\n+\n+\n+\n≡\nbc\nc\nb\na\nac\nab\nabc\nA\nA\nB\nC\n+\n-\n+\n+\n-\n-\n(1)\n(a)\n(b)\n(c)\n(ab)\n(abc)\n(bc)\n(ac)\nSay the independent\nexperimental error of\nobservations\n(a), (ab), et cetera is σε.\nWe define the main effect\nestimate Α to be\nε\nσ\nσ\n)1\n=\nA\nWhat is the standard deviation of the main effect estimate A?\nε\nσ\nσ\n\n)\n=\nA\nε\nσ\nσ\n)3\n=\nA\nε\nσ\nσ\n=\nA\n\n)\n\nThree Level Factors\nA\nB\nC\n33 Design\n8 vertices +\n12 edges +\n6 faces +\n1 center =\n27 points\n\nFactor Effect Plots\nA\nB\n-\n+\n-\n+\nA-\nA+\nB+\nB-\nA\n-\n+\nB\n-\n+\nA\n\nConcept Test\nA\nB\n-\n+\n-\n+\nIf there are no interactions in\nthis system, then the\nfactor effect plot from\nthis design could look like:\nA\nB+\nB-\nA\nB+\nB-\nA\nB+\nB-\nHold up all cards that apply.\n\nConcept Test\n- A bracket holds a component as shown. The\ndimensions are strongly correlated random\nvariables with standard deviations as noted.\nApproximately what is the standard deviation\nof the gap?\nA) 0.011\"\nB) 0.01\"\nC) 0.009\"\nD) not enough info\n\"\n.0\n=\nσ\n\"\n.0\n=\nσ\ngap\n\nMonte Carlo Simulations\nWhat are They Good at?\n- Above formulae apply regardless of dimension\n- So, Monte Carlo is good for:\n- Rough approximations or\n- Simulations that run quickly\n- Even if the system has many random variables\nAccuracy\nN\n∝1\nN\n#Trials\n≡\nFishman, George S., 1996, Monte Carlo: Concepts, Algorithms, and Applications, Springer.\n\nSampling Techniques for Computer\nExperiments\nRandom\nSampling\nStratified\nSampling\nLatin Hypercube\nSampling\nclump\ngap\n\nErrors in Scientific Software\n- Experiment T1\n- Statically measured errors in code\n- Cases drawn from many industries\n- ~10 serious faults per 1000 lines of commercially\navailable code\n- Experiment T2\n- Several independent implementations of the same\ncode on the same input data\n- One application studied in depth (seismic data\nprocessing)\n- Agreement of 1 or 2 significant figures on average\nHatton, Les, 1997, \"The T Experiments: Errors in Scientific Software\", IEEE\nComputational Science and Engineering.\n\nNext Steps\n- Monday 7 May - Frey at NSF\n- Wednesday 9 May - Exam #2\n- Wed and Fri, May 14 and 16\n- Final project presentations"
    },
    {
      "category": "Exam",
      "title": "exam2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/d57b6917f7a42794fc05477c7a9d118e_exam2.pdf",
      "content": "ESD.86\nModels, Data, Inference for Socio-Technical Systems\n\nMIT, Spring 2007\n\nExam #2\nOpen Book, Open Notes, Individual Effort\n\n1. (5 pts) A systems engineering textbook (Blanchard and Fabrycky) says \"When the\nsystem operating time is the Mean Time Between Failures (MTBF), the reliability is\n37%.\" Show that this is true for systems with constant failure rates.\n\n2. (10 pts) The graph below is from Weibull's 1951 paper. Estimate the mean, median,\nand the mode of the fatigue life. You do not need to be very precise in your estimates,\nbut you should explain clearly your approach and why it's valid. Discuss the\nrelationships among these the mean, median, and mode and what the relationships among\nthem imply about the distribution of the data.\n\nCopyright (c) 1951 by ASME. Used with permission.\n\n3. Given that variables z1, z2, and z3 are standardized, normally distributed, and\nindependent, how are the following random variables distributed? (name the distribution\nand give the required parameters or else write the equation out in full):\n\na) (5 pts)\n\nz\nz +\n\nb) (5 pts)\n\nz\nz\n+\n\nc) (5 pts)\nk\nz\nz\nz\n)\n(\n+\n\n0.1702\n5.0223\n2.3869\n-0.6328\n6.2121\n3.4204\n0.9821\n4.662\n4.0464\n1.3432\n4.5406\n3.6269\n-0.4473\n3.8452\n4.8308\n2.1877\n5.3442\n3.257\n1.9791\n3.6864\n4.2428\n0.9132\n5.658\n1.174\n1.4637\n9.5435\n2.8336\n1.1773\n4.3546\n2.4748\n0.4533\n6.1233\n-0.2493\n1.6657\n6.9879\n3.1136\n-0.0869\n2.2616\n1.5618\n2.6525\n3.3038\n5.7646\n0.7161\n5.558\n1.8655\n\n4. (10 pts) Given the following commands:\nmu=[1 5 3];\nsigma=[1 0 0; 0 2 0; 0 0 2];\nn=15;\nX = lhsnorm(mu,sigma,n)\n\nThese data are generated. The documentation says \"X = lhsnorm(mu,sigma,n)\ngenerates a Latin Hypercube sample X of size n from the multivariate normal distribution\nwith mean vector mu and covariance matrix sigma. What operations would you perform\nto check that the sample actually is what the documentation claims it to be? (You don't\nneed to actually process the data, it's just provided in case you want to illustrate\nconcretely what you'd do).\n\n5. (5 pts) You perform a linear regression of 100 data points (n=100). There are two\nindependent variables x1 and x2. The regression R2 is 0.72. Both β1 and β2 pass a t test\nfor significance. You decide to add the interaction x1x2 to the model. Select all the things\nthat cannot happen:\na) R2 decreases\nc) The adjusted R2 decreases\nd) All three coefficients β1, β2, and β12 fail the t test for significance\n\n6. (10 pts) Provide a persuasive and rigorous defense of your answer to question #5\nabove. The answer need not be primarily implemented with symbolic mathematics, but it\ncould be.\n\n7. The computer output below represents the results from a one way (single factor)\nANOVA (α=0.05). Each row represents a different treatment condition.\n\na) (3 pts) What does the \"F crit\" value represent?\nb) (3 pts) What does the \"F\" value represent?\nc) (3 pts) What are the most critical assumptions of this analysis?\nd) (3 pts) Do you think any of the assumptions were violated?\ne) (3 pts) What conclusion do you draw about the effect of the treatment? Briefly justify\nyour position.\n\nTreatment\n\nReplicates\n\nAnova: Single Factor\n\nSUMMARY\n\nGroups\nCount\nSum\nAverage\nVariance\n\n16.83333\n7.766667\n\n17.66667\n45.06667\n\n32.66667\n321.0667\n\n25.5\n36.7\n\nANOVA\n\nSource of\nVariation\nSS\ndf\nMS\nF\nP-value\nF crit\nBetween\nGroups\n996.3333\n332.1111\n3.235374\n0.043995 3.098393\nWithin Groups\n102.65\n\nTotal\n3049.333\n\n8. For the experimental plan and associated data in the Table below.\n\na) (5 pts) Sketch a factor effect plot.\n\nb) (5 pts) Does this data strongly suggest that there are interactions among the factors?\nExplain briefly.\n\nControl Factors\n\nExpt.\nNo.\nA\nB\nC\nD\nresponse\n2.2\n1.9\n2.1\n4.3\n3.8\n5.9\n6.4\n5.9\n\n9. You run a 27 full factorial design. The response is the velocity of a paper helicopter\nwhen it hits the ground when dropped from 9 feet. The standard deviation of the\nexperimental error is estimated to be 0.3 seconds since it's known the greatest\ncontribution to error is the response time in hitting the stopwatch.\n\na) (5 points) What is the standard deviation of a main effect estimate?\n\nb) (5 points) Suggest three ways to reduce the uncertainty in the main effect estimate.\n\n10. (10 points) You run a 27 full factorial design. You have the same situation as\nproblem #9 above, except now it's also known that one of the helicopters was not set to\nthe correct factor levels. Instead, it was set to the levels for some other helicopter in the\nmatrix experiment. Can you still characterize the uncertainty in your main effect\nestimate?\nIf \"yes\", briefly describe a procedure would you suggest to estimate the uncertainty.\nIf \"no\", explain why not and describe what you'll do next."
    },
    {
      "category": "Lecture Notes",
      "title": "lec1_3doors.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/8dd2aa2c0c3ec45ad153d5efeb68097f_lec1_3doors.pdf",
      "content": "Monte Hall Three Door Problem!\nMIT ESD.86\nFebruary 7, 2007\n\nA\nB\nC\n$20 in\nenvelope\n1/3\n1/3\n1/3\nContestant\nSelects\nA2\nB2\nC2\nA2\nB2\nC2\nA2\nB2\nC2\nBO\nCO\nCO\nBO\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/2\n1/2\nProbability\nAA2BO 1/18\nAA2CO 1/18\nAB2CO 1/9\nAC2BO 1/9\nTA Opens\n1/2\n1/2\n1/2\n1/2\nCO\nCO\nBO\nAO\nAO\nAO\nAO\nBO\nBA2CO 1/9\nBC2AO 1/9\nBB2CO 1/18\nBB2AO 1/18\nCA2BO 1/9\nCB2AO 1/9\nCC2AO 1/18\nCC2BO 1/18\n\nA\nB\nC\n$20 in\nenvelope\n1/3\n1/3\n1/3\nContestant\nSelects\nA2\nB2\nC2\nA2\nB2\nC2\nA2\nB2\nC2\nBO\nCO\nCO\nBO\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/21/2\nProbability\nAA2BO 1/18\nAA2CO 1/18\nAB2CO 1/9\nAC2BO 1/9\nTA Opens\n1/2\n1/2\n1/2\n1/2\nCO\nCO\nBO\nAO\nAO\nAO\nAO\nBO\nBA2CO 1/9\nBC2AO 1/9\nBB2CO 1/18\nBB2AO 1/18\nCA2BO 1/9\nCB2AO 1/9\nCC2AO 1/18\nCC2BO 1/18\n\nA\nB\nC\n$20 in\nenvelope\n1/3\n1/3\n1/3\nContestant\nSelects\nA2\nB2\nC2\nA2\nB2\nC2\nA2\nB2\nC2\nBO\nCO\nCO\nBO\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/21/2\nProbability\nAA2BO 1/18\nAA2CO 1/18\nAB2CO 1/9\nAC2BO 1/9\nTA Opens\n1/2\n1/2\n1/2\n1/2\nCO\nCO\nBO\nAO\nAO\nAO\nAO\nBO\nBA2CO 1/9\nBC2AO 1/9\nBB2CO 1/18\nBB2AO 1/18\nCA2BO 1/9\nCB2AO 1/9\nCC2AO 1/18\nCC2BO 1/18\n\nA\nB\nC\n$20 in\nenvelope\n1/3\n1/3\n1/3\nContestant\nSelects\nA2\nB2\nC2\nA2\nB2\nC2\nA2\nB2\nC2\nBO\nCO\nCO\nBO\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/3\n1/21/2\nProbability\nAA2BO 1/18\nAB2CO 1/9\nAC2BO 1/9\nTA Opens\nAA2CO 1/18\n1/2\n1/2\n1/2\n1/2\nCO\nCO\nBO\nAO\nAO\nAO\nAO\nBO\nBA2CO 1/9\nBC2AO 1/9\nBB2CO 1/18\nBB2AO 1/18\nCA2BO 1/9\nCB2AO 1/9\nCC2AO 1/18\nCC2BO 1/18\n\nProbability\nAA2BO 1/18\nAA2CO 1/18\nAB2CO 1/9\nAC2BO 1/9\nBA2CO 1/9\nBC2AO 1/9\nBB2CO 1/18\nBB2AO 1/18\nCA2BO 1/9\nCB2AO 1/9\nCC2AO 1/18\nCC2BO 1/18\nEvent{Correct|Stay}\nEvent{Correct|Switch}\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nP{Correct|Stay}=1/3\nP {Correct|Switch}=2/3"
    },
    {
      "category": "Lecture Notes",
      "title": "lec1_intro.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/2ebe106407339f82b451d497f44c9098_lec1_intro.pdf",
      "content": "Design of an ESD\nDesign of an ESD\nCore Methodology\nCore Methodology\nSubject\nSubject\nDick Larson, Dan Frey, with Roy Welsch\nFebruary 7, 2007\n\nEngineering Systems: At the intersection\nof\nEngineering, Management & Social Sciences\nEngineering\nSocial\nSciences\nManagement\nESD\n\nFor the new Methods\nsubject\nWe want to educate doctoral students to conduct\nresearch on these types of large scale engineering\nprojects, which fall at the intersection of traditional\nengineering, management and social sciences\n\nESD.86 Models, Data, Inference\nfor Socio-Technical Systems\n(New) Prereq: ESD.83, 6.041\nG (Spring) 3-0-9\nUse data and systems knowledge to build models of complex socio-\ntechnical systems for improved system design and decision-making.\nEnhance model-building skills, including: review and extension of\nfunctions of random variables, Poisson processes, and Markov\nprocesses. Move from applied probability to statistics via Chi-squared\nt and f tests, derived as functions of random variables. Review\nclassical statistics, hypothesis tests, regression, correlation and\ncausation, simple data mining techniques, and Bayesian vs. classical\nstatistics. Class project.\nEnrollment limited to 25 students. Preference given to ESD Ph.D.\nStudents.\nRichard C. Larson, Daniel D. Frey\n\nOverview\n> A new QUANTITATIVE methods subject, with\naspects of each of three disciplinary areas:\nEngineering, Management and Social Science.\n> To be required of 1st year ESD Ph.D. students,\nspring semester\n> There will be another new subject on QUALITATIVE\nmethods.\n\nMore Overview\n>\nIn ESD tradition, the 'math part' would be augmented with reading\nassignments and discussions tracing the history and application of\neach of the major concepts discussed and developed.\n>\nThere would be a term project for each student.\n>\nThere would be computer-based as well as paper-based homework\nassignments. The subject would be rigorous.\n>\nWhile a byproduct would be continued 'class bonding' of the first year\ndoctoral students, the primary focus is on intellectual content.\n\nThis is a\nKnowledge\nRequirement\n> For students whose academic plan is to take MIT\nsubjects that go much deeper than this subject (in\nstatistics, probability, quantitative research methods),\nthe requirement for taking this subject can be\nwaived.\n> This subject represents a 'knowledge requirement' that\nwill be assumed on doctoral general exams.\n\nBuilding from...\n> Subject will have an enforceable prerequisite: 6.041 or\nequivalent.\n> It will leverage all the fine work that Dan Frey has done with\nSOE curriculum development grant support -- funded in\nresponse to the oft-cited 'Odoni report' on the lack of a good\nsolid engineering-focused statistics subject in the SOE.\n> But, this is NOT a statistics subject!\n\nAn ESD Service Subject\n> If we are successful, we should attract students from elsewhere\nin the SOE who are not associated with ESD.\n> This should be ESD's first 'service subject.'\n> Tentatively, the 2007 spring semester new subject will be team-\ntaught by Dan Frey and yours truly, with a cameo by Roy\nWelsch.\n\nLectures, Problem Sets, Tutorials\nReadings: Historical Context, including Cases\nComputer-Based Exercises\nMedia Project\nTerm Project\nSubject Operates Along Parallel Tracks\n\nFundamentals, via\nSample Space Approach\n>\nStart with constructing probabilistic models using functions of random variables\nwith a sample space approach.\n>\nThen we slide into statistics via experimental design with threats to validity,\nsaying in essence that all designs are compromised by one or more of these.\n>\nThen, the statistical part should be a continuum of the sample space applied\nprobability treatment so everything is fundamental -- no memorization of weird\nstat formulas, just for the sake of memorization.\n>\nWe will do more with less in the stats area.\n>\nWe cover Bayesian as well as classical statistics, highlighting the philosophy,\nstrengths and weaknesses of each.\n>\nIf they want a true stats course, that would follow this course.\n\nWant students to be able to work with 'blank\nsheets of paper.'\nThey know fundamentals and can derive results.\nThey are not just users of computer routines.\n\nGo Deep,\nUse all Available\nSubjects\n> We cannot think that ESD is so unique that no other MIT subjects\ncan contribute to ESD students' knowledge of 'methods.' In the\ncourse of an ESD doctoral student's studies, she/he will rely most\noften on existing subjects at MIT or perhaps Harvard to go deep\nin the required methods.\n\nModel Building\n> Model building, based on empirical evidence and axiomatic conjectures,\nshould be the emphasis for the new subject.\n> We are not creating a new subject in applied probability nor are we\ncreating one in statistics. But we use both to obtain our objective.\n> The focus is more on model synthesis, not data analysis per se.\n> It is an active model creating focus, not a passive critical social science\nfocus.\n> Axiomatic models would be emphasized more than data inferred models,\ninferred from curve fitting -- where causation and correlation can become\nconfused.\n\nIntroduce New Ideas in\nHomework\n> Stochastic Dynamic Programming, Real Options, via\nSequential Decision Trees\n> Shannon measure of information, Entropy (the\nelement of 'surprise')\n> Derivation of certain Decis\ni\non t\nree.Decision tree.\nstatistical tests\n(F, T, Chi-Squared)\nFigure by MIT OCW. After example by Akinc.\n\nLinkages to 'ilities...\"\n> Reliability\n- Measures of..\n- Systems designs with\nredundancy\n> Robustness\n> Predictability\n> Stability\nhttp://www.mathpages.com/home/kmath336/kmath336_files/image001.gif\nD\ni\na\ng\nram\n.\nFigure by MIT OCW.\n\nA Real Null Hypothesis: A Sports '.500' Team\n>\nEach game is essentially decided by an independent flip of a\nfair coin\n>\nTrack the media coverage as certain expected 'streaks'\nduring the year.\n-\nWide use of derived distributions of Max and Min random\nvariables.\n-\nRandom incidence, potential fallacies in sampling\n\nS\ne\nq\nu\ne\nn c\ne\n\no\nf\n\nc\no i\nn\n\nt\no\ns\ns\ne\ns\n,\ns\nh\no\nw\ni\nn\ng\n\na\n\n4 -\ng\na\nm\ne\n\n\"\ns\nt\nr\ne a\nk\n\"\n\no\nf\n\nh\ne\na\nd s\n.\nFigure by MIT OCW.\n\nIn a 162 game season,\nwe should not be surprised to see\n> At least one 7 game loosing streak. :(\n> At least one 7 game winning streak. :)\n> All within the null hypothesis that each game is an\nindependent fair coin flip.\n> But imagine the press coverage of these two events.\n> Generalize to more important topics.\n\nOn-Going\nStudent Project\n> Track media weekly to find media mis-interpretation or\nmisuse of data.\n- Statistical significance of the media phrase, \"If it bleeds, it\nleads.\"\n- Making inferences based solely on sampling from extremes.\nhttp://news.csumb.edu/site/Images/news/headlines.jpg\n\nClass\nClass\nProjects\nProjects\n\nThe End!\nThe End!"
    },
    {
      "category": "Lecture Notes",
      "title": "lec2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/2b55649e3a413f0a06ad03e0cc6d7c16_lec2.pdf",
      "content": "Jacob (James) Bernoulli\n(1654-1705)\nFor Bernoulli Trials\nSource: Wikipedia\nD\nr\na\nw\ni\nn g\n\no\nf\nflipping a coin.\nFigure by MIT OCW.\nESD.86\nClass #2\nFebruary 12, 2007\n\nAnalyzing a Probability Problem\nFour Steps to Happiness\n1. Define the Random Variable(s)\n2. Identify the (joint) sample space\n3. Determine the probability law over the\nsample space\n4. Carefully work in the sample space to\nanswer any question of interest\n\nSubway Interviews\nYou are standing outside of the Park Street subway\n(T) station, with clipboard, and you want to\ninterview only registered Republicans. For\nmodeling purposes we will say that the\nprobability that a random T-rider who passes\nyou is a Republican is 0.20. (Actual is 0.13.)\nYou have infinite patience, as the stream of\nriders continues all day long.\n(a) Let R = the number of T-riders you question\nuntil you find the 1st Republican. Find the\nprobability mass function of R.\n\nR=1\nR=2\nR=3\nR=4\nR=5\nR=6\nR=7 ...\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8 ...\nSample Space with Probability Assignment\nGeometric Probability Mass Function\n\nHere we used the z-transform for\nthe probability mass function\n(PMF), defined as\n\nBoth discrete and continuous (Laplace) transforms will be\nreviewed in tutorial on Friday.\n\nIf N people go by, how many did\nyou interview?\nX=number interviewed, X=0,1,2,...\np=0.2.\nWhy? What is this called? What is the sample space?\nWhat are the probability assignments over the sample space?\nBinomial Probability Mass Function\n\nHow many people go by you until you\nhave completed your kth interview?\nLet Y=number of people who pass by up to and\nincluding the one who is the kth person interviewed.\nP{Y=y}=P{exactly k-1 interviews occur in (y-1)\npeople passing AND the yth person passing is\ninterviewed}\nNegative Binomial Probability Mass Function\n\nIndicator random variables.\nSuppose\nThen E[Xi] = 1*pi + 0*(1- pi) = pi .\nExample 1: Flip a coin N times, with P{Heads} = p. Assume\nindependent flips.\nDefine the random variable NH = number of Heads in N flips.\nLet the set indicator R.V. Xi be 1 if the ith coin flip is Heads.\nThen,\n\nSet indicator random variables.\nSuppose\nThen E[Xi] = 1*pi + 0*(1- pi) = pi .\nExample 1: Flip a coin N times, with P{Heads} = p. Assume\nindependent flips.\nDefine the random variable NH = number of Heads in N flips.\nLet the set indicator R.V. Xi be 1 if the ith coin flip is Heads.\nThen,\nE[NH] = E[\nXi\ni=1\nN\n∑\n] =\nE[Xi\ni=1\nN\n∑\n] =\np = Np\ni=1\nN\n∑\n\nSet indicator random variables.\nSuppose\nThen E[Xi] = 1*pi + 0*(1- pi) = pi .\nExample 1: Flip a coin N times, with P{Heads} = p. Assume\nindependent flips.\nDefine the random variable NH = number of Heads in N flips.\nLet the set indicator R.V. Xi be 1 if the ith coin flip is Heads.\nThen,\n\nExample 2. Baseball Hats.\nSuppose that one player from each of the 30 Major League Baseball\nteams attends a party at MIT, and each arrives wearing his\nteam's baseball cap. Each tosses his hat into a closet upon arrival.\nThe host, at the end of the party, gives a random hat to each\ndeparting player. What is the expected number of hats that are\nreturned to their rightful owners?\nSolution: Define indicator r.v.\nLet H = the number of hats returned to the correct owners. Then,\nAnswer independent of the number of players or teams!\n\nExample 2. Baseball Hats.\nSuppose that one player from each of the 30 Major League Baseball\nteams attends a party at MIT, and each arrives wearing his\nteam's baseball cap. Each tosses his hat into a closet upon arrival.\nThe host, at the end of the party, gives a random hat to each\ndeparting player. What is the expected number of hats that are\nreturned to their rightful owners?\nSolution: Define indicator r.v.\nLet H = the number of hats returned to the correct owners. Then,\nAnswer independent of the number of players or teams!\n\nExample #3. Winning Streaks.\nSuppose the Boston Celtics were much better, that they had a\n50% chance of winning any game, and that the outcomes of all games\nwere mutually independent.\nIf there were 100 games in a season, how many 7 game\nwinning streaks might we expect?\nHow do we interpret this?\n(1) Game n is a game which they win and for which they have won\nthe previous 6. Here (1/2)7 = 1/128. Use indicator r.v.'s.\n(2) Game n is a game which they win and for which they have won\nthe previous 6, and they lose the next game. Here we need\n(1/2)7 (1/2)= 1/256.\n\nGreat Expectations.\nSuppose you flip a coin until you get the first\nHeads, then stop. If the 1st flip is Heads, you win $2.\nIf the 2nd is the 1st Heads, you win $4.\nIf the 3rd flip is the 1st Heads, you win $8. ...If the\nnth flip is the 1st Heads, you win $2n. The bank is\nDonald Trump, so this game can go on for a long\nlong time!\n(a) How much would you be willing to pay for\nplaying this game?\n(b) What is the expected dollar value of winnings in\nthe game?\n\nThis is the St.Petersburg Paradox\nDaniel Bernoulli (1738; English trans. 1954)\n'Google' this & find many interesting articles, such as\nhttp://plato.stanford.edu/entries/paradox-stpetersburg/\n\nDoes the St. Petersburg Paradox Occur in Nature?\nA possible term project!\nSize and frequency of occurrence of Earthquakes.\nSmall earthquakes occur every day all around the world,\n....Large earthquakes occur less frequently, the\nrelationship being exponential; namely, roughly ten times as\nmany earthquakes larger than magnitude 4 occur in a\nparticular time period than earthquakes larger than\nmagnitude 5. In the (low seismicity) United Kingdom,\nfor example, it has been calculated that the average\nrecurrences are:-an earthquake of 3.7 or larger every\nYear, an earthquake of 4.7 or larger every 10 years, an\nearthquake of 5.6 or larger every 100 years. ...\nThe USGS estimates that, since 1900, there\nhave been an average of 18 major earthquakes (magnitude 7.0-7.9)\nand one great earthquake (magnitude 8.0 or greater) per year, and\nthat this average has been relatively stable.[5\nhttp://en.wikipedia.org/wiki/Earthquake#Size_and_frequency_of_occurrence\n\nEarthquakes\n- Richter Scale: logarithmic. Each whole\nnumber step in the magnitude scale\ncorresponds to the release of about 31\ntimes more energy than the amount\nassociated with the preceding whole\nnumber value.\n\nRichter Magnitude\nSeismic Energy Yield\nExample\n0.5\n5.6 kg (12.4 lb)\nHand grenade\n1.0\n32 kg (70 lb)\nConstruction site blast\n1.5\n178 kg (392 lb)\nWWII conventional\nbombs\n2.0\n1 metric ton\nlate WWII conventional\nbombs\n2.5\n5.6 metric tons\nWWII blockbuster bomb\n3.0\n32 metric tons\nMassive Ordnance Air\nBlast bomb\n3.5\n178 metric tons\nChernobyl nuclear\ndisaster, 1986\n4.0\n1 kiloton\nSmall atomic bomb\n4.5\n5.6 kilotons\nAverage tornado (total\nenergy)\n5.0\n32 kiloton\nNagasaki atomic bomb\n5.5\n178 kilotons\nLittle Skull Mtn., NV\nQuake, 1992\n6.0\n1 megaton\nDouble Spring Flat, NV\nQuake, 1994\n6.5\n5.6 megatons\nNorthridge quake, 1994\n7.0\n50 megatons\nTsar Bomba, largest\nthermonuclear weapon\never tested\n7.5\n178 megatons\nLanders, CA Quake, 1992\n8.0\n1 gigaton\nSan Francisco, CA\nQuake, 1906\n8.5\n5.6 gigatons\nAnchorage, AK Quake,\n9.0\n32 gigatons\n2004 Indian Ocean\nearthquake\n10.0\nhttp://www.answers.com/topic/richter-magnitude-scale\nDoes this fit the idea of the St. Petersburg Paradox?\n\n\"...ilities\": Reliability, Robustness\nA\nB\nLet T12 = event successful Transmission from node 1 to node 2.\npA=P{Link A works properly}=P{A}\npB= P{Link B works properly}=P{B}\nAssume links A and B function independently.\nThen,\nP{T12}=P{A and B}=P{AB}= pA pB\n\nA\nB\nLet T12 = event successful Transmission from node 1 to node 2.\npA=P{Link A works properly}=P{A}\npB= P{Link B works properly}=P{B}\npC= P{Link C works properly}=P{C}\nAssume links A, B and C function independently.\nThen,\nP{T12}=P{A and (B or C)}=P{AB+AC}= P{AB+ACB'}=\nP{T12}= pA pB + pA pC (1- pB)\nC\nAdd one redundant arc to increase Reliability\n\nA\nB\nLet T12 = event successful Transmission from node 1 to node 2.\nAssume all links function independently.\nThen,\nP{T12}=P{E or (AB) or (AC) or (DB) or (DC)}\nP{T12}=P{E + E'[(AB) + (AC) + (DB) + (DC)]}\nP{T12}=P{E + E'[A(B+C) + D(B +C)]}\nP{T12}=P{E + E'[(A+D)(B+C)]}= P{E + E'[(A+A'D)(B+B'C)]}\nP{T12}= pE + (1-pE){[pA+(1- pA) pD] [pB+(1- pB) pC]}\nGeneralization of this could be a term project.\nC\nAdd two more redundant arcs to increase Reliability\nD\nE"
    },
    {
      "category": "Lecture Notes",
      "title": "lec3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/b6bd21f0275ee5330fbe861899c48ae6_lec3.pdf",
      "content": "ESD.86 Feb. 14, 2007\nBroken stick experiment\nD=Min[X1, X2]\nD=Max[X1,X2]\nS=X1 + X2\nConvolution\nFunctions of Random Variables\n\nBut first, we have a winner!\nThe winning submission for ESD.86, for most\nblatant misuse, abuse or misinterpretation of\nstatistics and probability in the media.\nSubmitted by Roberto Perez-Franco.\nOriginal article New York Times:\n51% of Women Are Now Living Without Spouse,\nNew York Times, January 16, 2007, Section A;\nColumn 1; National Desk; Pg. 1\nToday: HEARING ON 'WARMING OF PLANET'\nCANCELED BECAUSE OF ICE STORM\n\nProblem Framing,\nFormulation and\nSolution\n\nBreak a yardstick in\ntwo random places\nWhat is the probability that a\ntriangle can be formed with the\nresulting three stick pieces?\n\nBreaking a Stick\nMark the stick....\n\nMarking the Results\n\n| |\n\nx2\nx1\n\n1. Random Variables:\nX1 = location of first mark\nX2 = location of second mark\nx1\nx2\n\n1/2\n1/2\nX1\nX2\nStep 2: Joint Sample Space\n\n1/2\n1/2\nX1\nX2\nStep 3: Probability Uniform over the Square\n\nStep 4: Carefully Work Within\nthe Sample Space\nWhat conditions need to be satisfied so\nthat a triangle can be formed?\nSuppose we consider first the case\nshown, x1 > x2\nx1\nx2\n\nAfter Step 4, HAPPINESS!\n1/2\n1/2\nX1\nX2\nhttp://web.mit.edu/urban_or_book/www/animated-eg/stick/f1.0.html\n\nFunctions of Random Variables\nY=3X-2Z\n\n4 Steps:\n1. Define the Random Variables\n2. Identify the joint sample space\n3. Determine the probability law over the\nsample space\n4. Carefully work in the sample space to\nanswer any question of interest\n\n4 Steps: Functions of R.V.s\n1. Define the Random Variables\n2. Identify the joint sample space\n3. Determine the probability law over the\nsample space\n4. Carefully work in the sample space to\nanswer any question of interest\n4a. Derive the CDF of the R.V. of interest, working\nin the original sample space whose probability\nlaw you know\n4b Take the derivative to obtain the desired PDF\n\nPhotos of ambulance and a dispatch center\nremoved due to copyright restrictions.\n\n1. R.V.;s\n- X1 = location of the accident\n- X2 = location of the ambulance\n- D = response distance = | X1 - X2 |\n2. Joint sample space is unit square in\nX1 X2 plane\n3. PDF over square is uniform\nResponse Distance of an\nAmbulance\nambulance\naccident\n\nx1\nx2\n\nx1\nx2\ny\nx2 = x1 - y\ny\nx2 = x1 + y\nx2 > x1\nx2 < x1\nEvent {D<y}\n\nx1\nx2\ny\nx2 = x1 - y\ny\nx2 = x1 + y\nx2 > x1\nx2 < x1\n4.a FD(y) = P{D<y} = 1 - (1-y)2, 0<y<1\n4.b fD(y) = 2(1-y), 0<y<1.\nEvent {D<y}\n\nfD(y) = 2(1-y)\ny\n\nfixed location\nambulance\naccident\n1/2\nIn previous problem, E[D] = 1/3\nWhat if we fix the location of the ambulance at X2 = 1/2?\n\nfixed location\nambulance\naccident\n1/2\n1/2 y\nfD(y)\nE[D] = 1/4, a 25% reduction\n\nRectangular Response Area\nx\ny\n(x1, y1)\n(x2, y2)\nXo\nYo\nD = |X1 - X2| + |Y1 - Y2|\n\nScaling to Get Expected\nTravel Distance\nx\n(x1, y1)\n(x2, y2)\nD = |X1 - X2| + |Y1 - Y2|\nE[D] = E[|X1 - X2| + |Y1 - Y2|]\nE[D] = (1/3)[Xo + Yo]\nXo\nYo\n\nMore Examples of Functions of\nRandom Variables\n\n1. Define the Random Variables\nY=MIN{X1, X2}, where X1 and X2 are iid uniform over [0,1]\n- Identify the joint sample space\nx1\nx2\n3. Determine the probability law over the\nsample space - uniform\n\nx1\nx2\n4a. Derive the CDF of the R.V. of interest, working\nin the original sample space whose probability\nlaw you know\n4b Take the derivative to obtain the desired PDF\n4. Carefully work in the sample space to answer\nany question of interest.\ny\ny\nFY(y) = P{Y<y}=1-(1-y)2\nfY(y)=d/dy [FY(y)]\nfY(y)=2(1-y), 0<y<1\n\nNow suppose\nY=MIN{X1, X2 ,X3, ...XN}, where Xi are iid uniform over [0,1]\nFY(y) = P{Y<y} = 1- P{Y>y}\nFY(y) = 1- (1-y)N\nfY(y) = (d/dy) FY(y) = N(1-y)N-1; N=1,2,...\n0<y<1\n\nNow suppose\nY=MAX{X1, X2 ,X3, ...XN}, where Xi are iid uniform over [0,1]\nFY(y) = P{Y<y} = yN\nWhy?\nfY(y) = NyN-1 N=1,2,...; 0<y<1\nOK, so now we can do Max and Min.\n\nSums of Random Variables\n\nNow let\nY=X1 + X2, where X1 and X2 are iid uniform over [0,1]\nx1\nx2\ny\ny\ny\nFY (y) = P{Y ≤y} ={\ny 2 /2 0 ≤y ≤1\n1-(2 -y)2 /2 1≤y ≤2\nfY (y) ={ y 0 ≤y ≤1\n2 -y 1≤y ≤2\nfY (y)dy =\nfx1 (v) fx2 (y -v)dvdy\nv= 0\nv=1\n∫\nConvolution\n\nfY (y) ={ y 0 ≤y ≤1\n1-y 1≤y ≤2\nfY (y)dy =\nfx1(v) fx2(y -v)dvdy\nv= 0\nv=1\n∫\nConvolution\nv\ny\ny-1\nfx1(v)\nfx2(y-v)\n\nfY (y) ={ y 0 ≤y ≤1\n1-y 1≤y ≤2\nfY (y)dy =\nfx1(v) fx2(y -v)dvdy\nv= 0\nv=1\n∫\nConvolution\nv\ny\ny-1\nfx1(v)\nfx2(y-v)\n\nfY (y) ={ y 0 ≤y ≤1\n1-y 1≤y ≤2\nfY (y)dy =\nfx1(v) fx2(y -v)dvdy\nv= 0\nv=1\n∫\nConvolution\nv\ny\ny-1\nfx1(v)\nfx2(y-v)\n\nA Quantization Problem\n\nBarges in\nAction\nPhoto courtesy of Eddie Codel.\nhttp://www.flickr.com/photos/ekai/15899569/\n\nMarine Transfer Station\nhttp://www.dattner.com/html/civic1a.html\nCourtesy of Dattner Architects. Used with permission.\n\nLoading\nBarge\nLoading\nBarge\nLoading\nBarge\nLoading\nBarge\nTug\nDelivers\nLIGHTS\nTug\nPicks Up\nHEAVIES\nLIGHT and HEAVY\nBarges Stored\nRefuse\nInflow\nλi (t)\nBarges\nShifted\nBy Hand\nOr Tug\nNYC Marine Transfer Station\n\nDigger\nDigger\nHEAVY BARGES\nTUG\nDELIVERS\nHEAVIES\nUNLOADING\nBARGE\nUNLOADING\nBARGE\nLIGHT\nBARGES\nTUGS PICK\nUP LIGHTS\nREFUSE\nUNLOADED\nFresh Kills Landfill\nHEAVY BARGESTugboat.Tugboat.Tugboat.\nSteam shovel.Steam shovel.\nFigure by MIT OCW.\nFigure by MIT OCW.\nFigure by MIT OCW.\n\nD = barge loads of garbage produced\non a random day (continuous r.v.)\nΘ = fraction of barge that is filled at\nbeginning of day (0 < Θ < 1)\nK = total number of completely filled\nbarges produced by a facility on a\nrandom day (K integer)\nK = [ D + Θ ] = integer part of D + Θ\n1. The R.V.'s\n\nθ\nd\n2. The Sample Space\n\nθ\nd\n\nθ\nd\nK = 0\nK = 1\nK = 2\nK = 3\n\nθ\nd\nK = 0\nK = 1\nK = 2\nK = 3\n3. Joint Probability Distribution\na) D and Θ are independent.\nb) Θ is uniformly distributed over [0, 1]\nfD,Θ(d, θ) = fD(d) fΘ(θ) = fD(d)(1) = fD(d), d > 0, 0 <θ <1\n\nθ\nd\nK = 0\nK = 1\nK = 2\nK = 3\nx\n1-x\n1-x\nx\n3. Joint Probability Distribution\na) D and Θ are independent.\nb) Θ is uniformly distributed over [0, 1]\nfD,Θ(d, θ) = fD(d) fΘ(θ) = fD(d)(1) = fD(d), d > 0, 0 <θ <1\n\nθ\nd\nK = 0\nK = 1\nK = 2\nK = 3\nx\n1-x\n1-x\nx\n4. Working in the Joint Sample Space\nLook at E [K |D = d ]\nLet d = i + x\n0 < x <1\nE [K |D = i + x ] = i (1 - x) + (i + 1) x = i + x = d\nImplies E [ K ] = E [ D ]\nData Collection Implications? Quantized Data?\n\nWhat Have We Learned Today?\nWhat Have We Learned Today?\n4 Steps: Functions of R.V.s\n1. Define the Random Variables\n2. Identify the joint sample space\n3. Determine the probability law over the\nsample space\n4. Carefully work in the sample space to\nanswer any question of interest\n4a. Derive the CDF of the R.V. of interest, working\nin the original sample space whose probability\nlaw you know\n4b Take the derivative to obtain the desired PDF"
    },
    {
      "category": "Lecture Notes",
      "title": "lec4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/8e34259fbfb521613d124b4b5eca2802_lec4.pdf",
      "content": "ESD.86\nPedestrian Crossing Problem\nRichard C. Larson\nFebruary 20, 2007\n\nLearning Objectives\nProblem Framing, Formulation and Solution\nReview of conditional probability\nReview of Poisson Processes\nIntroduction to Random Incidence\nReference: Urban Operations Research,\nChapter 2, Sec. 2.14\nhttp://web.mit.edu/urban_or_book/www/book/chapter2/2.14.html\n\nProblem Framing, Formulation\nand Solution\n\n\"To shape, fashion or form\"\n\"To put together the parts of\"\n\"To enclose in a border\"\nFrame:\nNew World Dictionary\n\nA Rough Model of 77 Massachusetts Avenue\nImage: Larson and Odoni, Urban Operations Research\n\nRule A: Dump Every T Minutes (open loop control)\n\nRule B: Dump When Pedestrian Count = No\n(closed loop control)\n\nRule C: Dump Whenever Longest Wait = To Min.\n(again closed loop control)\n\nA Rough Model of 77 Massachusetts Avenue\nTwo independent\nPoisson Processes\n\nA Rough Model of 77 Massachusetts Avenue\n= waiting pedestrian\n= \"queuer\"\n\nFor each decision rule, determine:\n1. Expected number of pedestrians\ncrossing left to right on any dump\n2. Probability that zero pedestrians cross\nleft to right on any dump\n3. The pdf for time between dumps\n4. Expected time that a randomly arriving\ncustomer must wait until crossing\n5. Expected time that a randomly arriving\nobserver, who is not a pedestrian, will\nwait until the next dump\n\n1. Expected number of pedestrians\ncrossing left to right on any dump\n\nToday we work out\nthe answers\ntogether on the\nblackboard!\n\n2. Probability that zero pedestrians\ncross left to right on any dump\n\n3. The pdf for time between dumps\n\n4. Expected time that randomly arriving\ncustomer must wait until crossing\n\n5. Expected time that a randomly\narriving observer, who is not a\npedestrian, will wait until next dump"
    },
    {
      "category": "Lecture Notes",
      "title": "lec5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/1a54fef489a72a6f19f64f10cabbff4a_lec5.pdf",
      "content": "ESD.86\nRandom Incidence\nA Major Source of Selection Bias\nRichard C. Larson\nFebruary 21, 2007\n\nExamples\nWaiting for a bus at 77 Mass. Avenue.\n- \"Clumping\"\nInterview passengers disembarking\nfrom an airplane.\n\nDoctoral Exam Question\nYou arrive at a bus stop where busses arrive\naccording to a Poisson Process with rate λ\nper unit time.\nUse no-memory property of Poisson\nprocesses. Time until next bus arrives has\nnegative exponential density with mean 1/ λ.\nLooking backwards, time since last bus was\nat the bus stop has negative exponential\ndensity with mean 1/ λ.\nThus, mean time between buses is 2/ λ, not\n1/ λ. What's wrong here?\n\nRandom Incidence: Tending to \"Land\" in Bigger Gaps\nPhoto courtesy of Kevin King. http://www.flickr.com/photos/divemasterking2000/541537501/\n\nPhoto courtesy of Kevin King. http://www.flickr.com/photos/divemasterking2000/541537501/\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nDefinitions of the random variables:\nYi = time interval between the ith and i + 1st arrival event\nW = length of the inter-arrival gap in which you fall\nV = time remaining in the gap in which you fall\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nAll 3 random variables have probability density functions:\nfY(x) = fY1(x) = fY2(x) = ...\nfW(w)\nfV(y)\n\nThe Inter-Arrival Times\nfY(x) = fY1(x) = fY2(x) = ...\n-If the Yi's are mutually independent then\nwe have a renewal process.\n-But the Random Incidence results we are\nabout to obtain do not require that we\nhave a renewal process.\n\nThe Gap We Fall Into by Random Incidence\nfW(w)dw=P{length of gap is between w and w+dw}\nfW(w)dw is proportional to two things:\n(1) the relative frequency of gaps [w, w+dw]\n(2) the length of the gap w (!!).\nThus, normalizing so we have a proper pdf,\nWe can write\nfW(w)dw =wfY(w)dw/E[Y], or\nfW(w) =wfY(w)/E[Y]\n\nTime Remaining in the Gap Until Next Arrival\nfV(y)\nConsider fV|W(y|w)\nWe can argue that fV|W(y|w)=(1/w) for 0<y<w.\nSo we can write\nfV (y)dy = dy\nfV |W\ny\ninf∫\n(y | w) fW (w)dw\nfV (y)dy = dy\n(1/w)\ny\ninf∫\nwfY (w)\nE[Y] dw\nfV (y)dy = dy(1-P{Y ≤y})/E[Y]\n\nMean Time Until Next Arrival\nE[V] =\nE[V | w] fW\ninf∫\n(w)dw\nE[V] =\n(w /2) wfY (w)\nE[Y]\ninf∫\ndw\nE[V] = E[Y 2]/(2E[Y]) = σY\n2 + E 2[Y]\n2E[Y]\nE[V] = E 2[Y]1+ σY\n2 /E 2[Y]\n2E[Y]\n= E 2[Y] 1+ η2\n2E[Y],\nwhere η ≡coefficient of variation of Y = σY /E [Y].\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nKey result:\nE[V] = E[Y]2(1+η2)/(2E[Y])\nwhere\nη = coefficient of variation of the R.V. Y\n\nLet's Visit Several Examples,\nIncluding that Bus Stop Doctoral\nExam Question!"
    },
    {
      "category": "Lecture Notes",
      "title": "lec6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/7060ce0aa4c4eed15d51539ea51d7828_lec6.pdf",
      "content": "ESD.86\nRandom Incidence\nand More\nRichard C. Larson\nFebruary 26, 2007\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nDefinitions of the random variables:\nYi = time interval between the ith and i + 1st arrival event\nW = length of the inter-arrival gap in which you fall\nV = time remaining in the gap in which you fall\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nAll 3 random variables have probability density functions:\nfY(x) = fY1(x) = fY2(x) = ...\nfW(w)\nfV(y)\n\nThe Inter-Arrival Times\nfY(x) = fY1(x) = fY2(x) = ...\n-If the Yi's are mutually independent then\nwe have a renewal process.\n-But the Random Incidence results we are\nabout to obtain do not require that we\nhave a renewal process.\n\nThe Gap We Fall Into by Random Incidence\nfW(w)dw=P{length of gap is between w and w+dw}\nfW(w)dw is proportional to two things:\n(1) the relative frequency of gaps [w, w+dw]\n(2) the length of the gap w (!!).\nThus, normalizing so we have a proper pdf,\nWe can write\nfW(w)dw =wfY(w)dw/E[Y], or\nfW(w) =wfY(w)/E[Y]\n\nMean Time Until Next Arrival\nE[V] =\nE[V | w] fW\ninf∫\n(w)dw\nE[V] =\n(w /2) wfY (w)\nE[Y]\ninf∫\ndw\nE[V] = E[Y 2]/(2E[Y]) = σY\n2 + E 2[Y]\n2E[Y]\nE[V] = E 2[Y]1+ σY\n2 /E 2[Y]\n2E[Y]\n= E[Y]\n(1+ η2),\nwhere η ≡(coefficient of variation of Y) ≡σY /E [Y].\n\ny1\ny2\ny3\ny4\ny5\ny6\n...\nV=v\nW=y4\nKey result:\nE[V] = (E[Y]/2)(1+ η2)\nwhere\nη = coefficient of variation of the R.V. Y\n\nE[V] = (E[Y]/2)(1+ η2)\n1. Deterministic Inter-arrivals:\nE[V] = E[Y]/2 = T/2.\n2. Negative exponential inter-arrivals:\nE[Y] = T,σY\n2 = 0\nE[Y] =1/λ,σY\n2 =1/λ2,η =1. E[V] =1/(2λ) +1/(2λ) =1/λ\n3. Y=1.0, with Prob. 1/2; Y=9.0, with Prob. 1/2.\nE[Y]=(1/2)[1 + 9] = 5.\nVariance[Y]=E[(Y- E[Y])2]=E[Y2 -2YE[Y]+ E[Y])2]\nVariance[Y]=E[Y2] - E[Y]2\nVariance[Y]=(1/2){12 + 92} - 52 =41-25=16\nη=4/5. E[V]=(5/2)(1+16/25)=2.5(1.64)=4.1\n\nE[V] = (E[Y]/2)(1+ η2)\n4. Suppose Y = 1.0 with Probability 0.99\nY=100.0 with Probability 0.01\nThen E[Y]=1(0.99) + 100(0.01)= 1.99=2\nE[Y2]=1(0.99) + 10000(0.01)=100.99=101\nVAR[Y]= E[Y2]-E2[Y]=101-4=97\nη2 =97/4=24.25.\nE[V] = (E[Y]/2)(1+ η2)=(2/2)(1+24.25)\nE[V] =25.25 Intuition??\n\nE[V] = (E[Y]/2)(1+ η2)\nPedestrian Traffic Light Problem\n1st ped. to arrive pushes button.\nT0 minutes later the next Dump occurs.\nWe are dealing here with a random observer.\nE[Y] = (1/λ) + T0\nVAR[Y]= (1/λ)2\nE[V] = (E[Y]/2)(1+ η2)\nE[V] = (1/2) [(1/λ) + T0]{1+ (1/λ)2 /[(1/λ) + T0 ]2}\nE[V] = 1/2λ + T0 /2 + 1 /{2[λ+ λ2T0 ]}\nPhoto courtesy of Austin Tolin. http://www.flickr.com/photos/austintolin/396264013/\n\nTime Remaining in the Gap Until Next Arrival\n1. Deterministic: Y = T with Probability 1.0\nThen fv(y)=1/T, for 0<y<T.\nSuppose T = 10 minutes and event A is:\nA = {V>5}.\nThen fv|A(y|A)= fv(y)/P{A} for all y in A.\nfv|A(y|A)=(1/10)/(1/2)=1/5 for 5<y<10.\n\nTime Remaining in the Gap Until Next Arrival\n2. Y has negative exponential pdf, mean λ.\nWe know that fv(y)= λe(-λy) for y>0.\nSuppose λ =1/10, so that E[Y]=10.\nSuppose event A:{Y>5}\nThen fv|A(y|A)= λe(-λy) /P{A} for all y>5.\nP{A} = e(-5λ)\nfv|A(y|A)= λe(-λ[y-5]) for y>5.\nProves \"No Memory\" Property\n\nTime Remaining in the Gap Until Next Arrival\nfV(y)\nConsider fV|W(y|w)\nWe can argue that fV|W(y|w)=(1/w) for 0<y<w.\nSo we can write\nfV (y)dy = dy\nfV |W\ny\ninf∫\n(y | w) fW (w)dw\nfV (y)dy = dy\n(1/w)\ny\ninf∫\nwfY (w)\nE[Y] dw\nfV (y)dy = dy(1-P{Y ≤y})/E[Y]\n\nfV (y)dy = dy(1-P{Y ≤y})/E[Y]\n1. For deterministic gaps,\nP{Y ≤y} ={0 for y < T\n1 for y ≥T\nfV (y)dy = dy(1-P{Y ≤y})/E[Y]\nfV (y)dy = dy /T for 0 ≤y < T\n2. For negative exponential gaps\nfV (y)dy = dy(1-P{Y ≤y})/E[Y]\nfV (y)dy = dy(1-[1-e-λy])/(1/λ)\nfV (y)dy = dyλe-λy for y ≥0\n\nPhoto courtesy of Tom Karlo. http://www.flickr.com/photos/karlo/10746148/\nPretend you are a chocolate chip, and you\nwake up to find yourself in a cookie.....\n\nfW(w) =wfY(w)/E[Y]\nBecomes:\nPW(w) =wPY(w)/E[Y],\nwhere\nY=Number of chips in a random cookie\nE[Y] = mean number of chips in a\nrandom cookie\nPW(w) = P{w chips in a cookie as seen\nby a random chip within a cookie}\n\nPMF\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n10 11 12 13 14 15 16 17 18 19 20 21\nSeries1\nDistribution of Chips in Cookies,\nBy Sampling Random Cookies\nMean = 6.52\n\nRandom Incidence PMF\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n10 11 12 13 14 15 16 17 18 19 20 21\nSeries1\nDistribution of Chips in Cookies,\nAs Measured by Chips within the Cookies\nMean = 8.62\n\nWhere does the chips-in-cookies\nsampling problem arise in real life?\n\nHow About an\nInfinite Jogging Trail?\nPhoto courtesy of Carles Corbi. http://www.flickr.com/photos/bioman/101773602/"
    },
    {
      "category": "Lecture Notes",
      "title": "lec7.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/8c23b3372a4738ea14203d6d636a3f32_lec7.pdf",
      "content": "ESD.86 Spatial Models\nRichard C. Larson\nFebruary 28, 2007\nRichard C. Larson\nFebruary 28, 2007\n\nOutline\nMin, Max\nRatio of Urban Distance to Airplane Dist.\nSpatial Poisson Processes\nFacility Location\n\nMin, Max. Deriving a Joint PDF\nSuppose X1 and X2 are i.i.d. uniformly\ndistributed over [0, 1]. They could, for\ninstance, be the locations of 2 police cars.\nWe seek to derive the joint pdf of Y and Z,\nwhere\nY=Min(X1, X2)\nZ=Max(X1, X2)\nThat is, we seek\nfY,Z(y,z)dydz=P{y<Y<y+dy,z<Z<z+dz}\n\nFour Steps to Happiness\n1. Define the random variables\nY=Min(X1, X2)\nZ=Max(X1, X2)\n2. Define the joint sample space:\nUnit square in positive quadrant\n3. Identify the probability measure over\nthe joint sample space: Uniform.\n4. Work within the sample space to answer\nany questions of interest.\nhttp://www.schwimmerlegal.com/smiley.jpg\n\nWork within the sample space\nx1\nx2\nUse CDF\nFY,Z (y,z) ≡P{Y ≤y,Z ≤z}\nFY,Z (y,z) = P{Min[X1,X2] ≤y,Max[X1,X2] ≤z}\nFY,Z (y,z) = 2yz -y 2, 0 ≤y ≤z ≤1\ny\ny\nz\nz\nfY ,Z (y,z) = ∂2\n∂y∂z FY,Z (y,z) = ∂\n∂y (2y) = 2, 0 ≤y ≤z ≤1\n\nWork within the sample space\ny\nz\nfY ,Z (y,z) = ∂2\n∂y∂z FY,Z (y,z) = ∂\n∂y (2y) = 2, 0 ≤y ≤z ≤1\nHeight of pdf = 2\nWhat do we do if we do\nnot have the simple square\nsymmetry of this problem?\nWhat do we do if the pdf is\nnot uniform?\n\nRatio of Manhattan to Euclidean\nDistance Metrics\n1. Define R.V.'s\n» D1 = |X1 - X2| + |Y1 - Y2|\n» D2 = (X1 - X2)2 + (Y1 - Y2)2\n» Ratio = R = D1 / D2\nΨ = angle of directions of travel wrt straight\nline connecting (X1 , Y1) & (X2 , Y2)\n\n(X1, Y1)\n(X2, Y2)\nDirections of Travel\n\n(X1, Y1)\n(X2, Y2)\nDirections of Travel\nOne possible minimum distance path\n\n(X1, Y1)\n(X2, Y2)\nDirections of Travel\nOne possible minimum distance path\nΨ=ψ\nCos(ψ)D2\nsin(ψ)D2\n\n(R|Ψ) = cos Ψ + sin Ψ = 2 1/2 cos(Ψ - π/4)\n2. Identify Sample Space\n3. Probability Law over Sample Space:\nInvoke isotropy implying uniformity of\nangle\nπ/2\nψ\n\nπ/2\nψ\n2/ π\nfΨ(ψ)\n\n4. Find CDF\nFR(r) = P{R < r} = P{21/2 cos(Ψ-π/4) < r}\nFR(r) = P{R < r} = P{cos(Ψ-π/4) < r/ 21/2 }\n\nr/21/2\n1/21/2\nπ/2\nπ/4\ncos(Ψ-π/4)\nΨ\ng(Ψ)\n\nr/21/2\n1/21/2\nπ/2\nπ/4\ncos -1 (r/21/2) + π/4\n-cos -1 (r/21/2) + π/4\ncos(Ψ-π/4)\nΨ\ng(Ψ)\n\n2/π\nπ/2\nπ/4\ncos -1 (r/21/2) + π/4\n-cos -1 (r/21/2) + π/4\nΨ\npdf for Ψ\nProbability of 'red event' = 2*(2/π)*{-cos -1 (r/21/2) + π/4}\n\nAnd finally...\nAfter all the computing is done, we find:\nFR(r) = 1 - (4/π)cos -1 (r/21/2), 1< r <21/2\nfR(r) = d[FR(r) ]/dr = (4/π) {1/(2 - r2)1/2 }\nMedian R = 1.306\nE[R] = 4/ π = 1.273\nσR/E[R] = 0.098, implies very robust\n\nhttp://zappa.nku.edu/~longa/geomed/modules/ss1/lec/poisson.gif\nSpatial\nPoisson\nProcesses\nCourtesy of Andy Long. Used with permission.\n\nSpatial Poisson Processes\nEntities distributed in space (Examples?)\nFollow postulates of the (time) Poisson\nprocess\n- λdt = Probability of a Poisson event in dt\n- History not relevant\n- What happens in disjoint time intervals is\nindependent, one from the other\n- The probability of a two or more Possion events in\ndt is second order in dt and can be ignored\nLet's fill in the spatial analogue.....\n\nS\nSet S has area A(S).\nPoisson intensity is γ\nPoisson entities/(unit area).\nX(S) is a random variable\nX(S) = number of Poisson\nentities in S\nP{X(S) = k} = (γA(S))k\nk!\ne-γA(S), k = 0,1,2,...\n\nNearest Neighbors: Euclidean\nDefine D2= distance from a random point\nto nearest Poisson entity\nWant to derive fD2(r).\nr\nRandom\nPoint\nHappiness:\nFD2 (r) ≡P{D2 ≤r} =1-P{D2 > r}\nFD2 (r) =1-Prob{no Poisson entities in circle of radius r}\nFD2 (r) =1-e-γπr 2 r ≥0\nfD2 (r) = d\ndr FD2 (r) = 2rγπe-γπr 2 r ≥0\nRayleigh pdf with parameter\n2γπ\n\nNearest Neighbors: Euclidean\nDefine D2= distance from a random point\nto nearest Poisson entity\nWant to derive fD2(r).\nr\nRandom\nPoint\nfD2 (r) = d\ndr FD2 (r) = 2rγπe-γπr 2 r ≥0\nRayleigh pdf with parameter\n2γπ\nE[D2] = (1/2) 1\nγ \"Square Root Law\"\nσ D2\n2 = (2 -π /2) 1\n2πγ\n\nNearest Neighbor: Taxi Metric\nr\nFD1 (r) ≡P{D1 ≤r}\nFD1 (r) =1-Pr{no Poisson entities in diamond}\nFD1 (r) =1-e-γ 2r 2\nfD1 (r) = d\ndr FD1 (r) = 4rγe-2γr 2\n\nWhat Have We Learned?\nWithin a spatial context, how to use the\nFour Steps to Happiness to derive joint\ndistributions\nWithin a spatial context, how to derive a\ndifficult distribution involving geometry\nSpatial Poisson Processes, with nearest\nneighbor applications"
    },
    {
      "category": "Lecture Notes",
      "title": "lec8.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/0565f132e474fba29bbf61e88252fa78_lec8.pdf",
      "content": "ESD.86\nMarkov Processes and their\nApplication to Queueing\nRichard C. Larson\nMarch 5, 2007\nPhoto courtesy of Johnathan Boeke. http://www.flickr.com/photos/boeke/134030512/\n\nOutline\nSpatial Poisson Processes, one more time\nIntroduction to Queueing Systems\nLittle's Law\nMarkov Processes\n\nhttp://zappa.nku.edu/~longa/geomed/modules/ss1/lec/poisson.gif\nSpatial\nPoisson\nProcesses\nCourtesy of Andy Long. Used with permission.\n\nSpatial Poisson Processes\nEntities distributed in space (Examples?)\nFollow postulates of the (time) Poisson\nprocess\n- λdt = Probability of a Poisson event in dt\n- History not relevant\n- What happens in disjoint time intervals is\nindependent, one from the other\n- The probability of a two or more Possion events in\ndt is second order in dt and can be ignored\nLet's fill in the spatial analogue.....\n\nS\nSet S has area A(S).\nPoisson intensity is γ\nPoisson entities/(unit area).\nX(S) is a random variable\nX(S) = number of Poisson\nentities in S\nP{X(S) = k} = (γA(S))k\nk!\ne-γA(S), k = 0,1,2,...\n\nNearest Neighbors: Euclidean\nDefine D2= distance from a random point\nto nearest Poisson entity\nWant to derive fD2(r).\nr\nRandom\nPoint\nHappiness:\nFD2 (r) ≡P{D2 ≤r} =1-P{D2 > r}\nFD2 (r) =1-Prob{no Poisson entities in circle of radius r}\nFD2 (r) =1-e-γπr 2 r ≥0\nfD2 (r) = d\ndr FD2 (r) = 2rγπe-γπr 2 r ≥0\nRayleigh pdf with parameter\n2γπ\n\nNearest Neighbors: Euclidean\nDefine D2= distance from a random point\nto nearest Poisson entity\nWant to derive fD2(r).\nr\nRandom\nPoint\nfD2 (r) = d\ndr FD2 (r) = 2rγπe-γπr 2 r ≥0\nRayleigh pdf with parameter\n2γπ\nE[D2] = (1/2) 1\nγ \"Square Root Law\"\nσ D2\n2 = (2 -π /2) 1\n2πγ\n\nNearest Neighbor: Taxi Metric\nr\nFD1 (r) ≡P{D1 ≤r}\nFD1 (r) =1-Pr{no Poisson entities in diamond}\nFD1 (r) =1-e-γ 2r 2\nfD1 (r) = d\ndr FD1 (r) = 4rγe-2γr 2\n\nHow Might you Derive the PDF\nfo the kth Nearest Neighbor?\nBlackboard exercise!\n\nTo Queue or Not to Queue,\nThat May be a Question!\n\nPeople processing through\na service cente\nr counter, shown a\nrriving, waiting, a\nnd leavi\nng.\nFigure by MIT OCW.\n\nFinite or\nInfinite?\nFinite or\nInfinite?\nQueue\nDiscipline:\nHow queuers\nAre selected\nfor service\nServers:\nStatistical Clones?\nSource: Larson and Odoni, Urban Operations Research\n\nWhat Kinds of Queues Occur in\nSystems of Interest to ESD?\nESD\nQueues?\nPhotos courtesy, from top left, clockwise: U.S. FAA: Flickr user \"*keng\" http://www.flickr.com/photos/kengz/67187556/;\nLuke Hoersten http://www.flickr.com/photos/lukehoersten/532375235/)\n\nLittle's Law for Queues\na(t) = cumulative # arrivals to system in (0,t]\nd(t) = cumulative # departures from system in (0,t]\nL(t) = a(t) -d(t)\nL(t) = number of customers in the system\n(in queue and in service) at time t\nd(t)\na(t)\nL(t)\nSource: Larson and Odoni, Urban Operations Research\n\nLittle's Law for Queues\na(t) = cumulative # arrivals to system in (0,t]\nd(t) = cumulative # departures from system in (0,t]\nL(t) = a(t) -d(t)\nL(t) = number of customers in the system\n(in queue and in service) at time t\nγ(t) =\n[a(τ) -d(τ)]dτ\nt∫\n=\nL(τ)dτ\nt∫\nγ(t) = total number of customer minutes spent in the system\n\nLet's Get an expression for Each of 3 Quantities\nλt ≡average customer arrival rate = a(t)/t\nWt ≡average time that an arrived customer has spent in the system\nWt = γ(t)/a(t)\nLt = time average # customers in system during (0,t]\nLt = 1\nt\nL(τ)dτ = γ(t)/t\nt∫\nLt = γ(t)\nt\n= a(t)\nt\nγ(t)\na(t) = λtWt\nIn the limit,\nL = λW , Little's Law\n\nKey Issues\nL in a time-average. Explain\nλ is average of arrival rate of customers\nwho actually enter the system\nW is average time in system (in queue\nand in service) for actual customers\nwho enter the system\nL = λW\n\nMore Issues\nLittle's Law is general. It does not\ndepend on\n- Arrival process\n- Service process\n- # servers\n- Queue discipline\n- Renewal assumptions, etc.\nIt just requires that the 3 limits exist.\nL = λW\n\nStill More\nIssues\nWhat about balking? Reneging? Finite\ncapacity?\nDo we need iid service times? Iid inter-\narrival times?\nDo we need each busy period to behave\nstatistically identically?\nLook at role of γ(t). Can change queue\nstatistics by changing queue discipline.\nL = λW\n\nt = time\nCumulative # of Arrivals\nL(t)\nFCFS\nSJF\nLSJF(t)\nFCFS=First Come, First Served\nSJF=Shortest Job First\nWhat about LJF,\nLongest Job 1st?\n\n\"System\" is\nGeneral\nOur results apply to entire queue\nsystem, queue plus service facility\nBut they could apply to queue only!\nOr to service facility only!\nL = λW\nS.F.\nLq = λWq\nLSF = λWSF = λ /μ\n1/μ = mean service time\n\nAll of this means,\n\"You buy one, you get the other 3 for free!\"\nW = 1\nμ +W q\nL = Lq + LSF = Lq + λ\nμ\nL = λW\n\nUtilization Factor ρ\nSingle Server. Set\nE[Y] is time-average number of\ncustomers in the SF\nBuy Little's Law,\nY ={1 if server is busy\n0 if server is idle\nE[Y] =1* P{server is busy}+ 0* P{server is idle}\nE[Y] =1* ρ + 0 = ρ = E[# customers in SF] = ?\nρ = λ /μ <1\n\nUtilization Factor ρ\nSimilar logic for N identical parallel\nservers gives\nHere, λ/μ corresponds to the time-\naverage number of servers busy\nρ = ( λ\nN ) 1\nμ = λ\nNμ <1\n\nMarkov Queues\nMarkov here means, \"No Memory\"\n\nSource: Larson and Odoni, Urban Operations Research\n\nBalance of Flow Equations\nTo be continued..............\nλ0P0 = μ1P1\n(λn + μn)Pn = λn-1Pn-1 + μn+1Pn+1 for n =1,2,3,..."
    },
    {
      "category": "Lecture Notes",
      "title": "lec9.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/6f188063c7051ea94c5db10683665f21_lec9.pdf",
      "content": "ESD.86. Markov Processes and their\nApplication to Queueing II\nRichard C. Larson\nMarch 7, 2007\nPhoto: US National Archives\n\nOutline\nLittle's Law, one more time\nPASTA treat\nMarkov Birth and Death Queueing Systems\n\nPeople processing through\na service cente\nr counter, shown a\nrriving, waiting, a\nnd leavi\nng.\nFigure by MIT OCW.\n\nt = time\nCumulative # of Arrivals\nL(t)\nFCFS\nSJF\nLSJF(t)\nFCFS=First Come, First Served\nSJF=Shortest Job First\nWhat about LJF,\nLongest Job 1st?\n\n\"System\" is\nGeneral\nOur results apply to entire queue\nsystem, queue plus service facility\nBut they could apply to queue only!\nOr to service facility only!\nL = λW\nS.F.\nLq = λWq\nLSF = λWSF = λ /μ\n1/μ = mean service time\n\nAll of this means,\n\"You buy one, you get the other 3 for free!\"\nW = 1\nμ +W q\nL = Lq + LSF = Lq + λ\nμ\nL = λW\n\nMarkov Queues\nMarkov here means, \"No Memory\"\n\nSource: Larson and Odoni, Urban Operations Research\n\nBalance of Flow Equations\nλ0P0 = μ1P1\n(λn + μn)Pn = λn-1Pn-1 + μn+1Pn+1 for n =1,2,3,...\nAnother way to balance the flow:\nλnPn = μn+1Pn+1 n = 0,1,2,...\nSource: Larson and Odoni, Urban Operations Research\n\nλnPn = μn+1Pn+1 n = 0,1,2,...\nλ0P0 = μ1P1\nλ1P1 = μ2P2 ...\nλnPn = μn+1Pn+1\nP1 = (λ0 /μ1)P0\nP2 = (λ1 /μ2)P1 = (λ0 /μ1)(λ1 /μ2)P0 = (λ0λ1 /[μ1μ2])P0\nPn+1 = (λn /μn+1)Pn = (λ0λ1...λn /[μ1μ2...μn+1])P0\nTelescoping!\nSource: Larson and Odoni, Urban Operations Research\n\nP\nλ0P0 = μ1P1\nλ1P1 = μ2P2 ...\nλnPn = μn+1Pn+1\nP1 = (λ0 /μ1)P0\nP2 = (λ1 /μ2)P1 = (λ0 /μ1)(λ1 /μ2)P0 = (λ0λ1 /[μ1μ2])P0\nPn+1 = (λn /μn+1)Pn = (λ0λ1...λn /[μ1μ2...μn+1])P0\nTelescoping!\nP0 + P1 + P2 + ...=\nPn\nn= 0\ninf\n∑\n=1\nP0 +(λ0 /μ1)P0 + (λ0λ1 /[μ1μ2])P0 + ...+ (λ0λ1...λn /[μ1μ2...μn+1])P0 + ...=1\nP0{1+(λ0 /μ1) + (λ0λ1 /[μ1μ2]) + ...+ (λ0λ1...λn /[μ1μ2...μn+1]) + ...} =1\nNow, you easily solve for P0 and then for\nAll other Pn's.\n\nPASTA: Poisson Arrivals See Time Averages\n\nTime to\nBuckle your\nSeatbelts!\nhttp://www.census.gov/pubinfo/www/multimedia/img/seatbelt-lo.jpg\n\nThe M/M/1 Queue\nP0 +(λ0 /μ1)P0 + (λ0λ1 /[μ1μ2])P0 + ...+ (λ0λ1...λn /[μ1μ2...μn+1])P0 + ...=1\nP0{1+(λ0 /μ1) + (λ0λ1 /[μ1μ2]) + ...+ (λ0λ1...λn /[μ1μ2...μn+1]) + ...} =1\nP0{1+(λ /μ) + (λ2 /μ2) + ...+ (λn+1 /μn+1) + ...} =1\n{1+(λ /μ) + (λ2 /μ2) + ...+ (λn+1 /μn+1) + ...} =1/[1-(λ /μ)]\nFor λ/μ < 1.\nSource: Larson and Odoni, Urban Operations Research\n\nThe M/M/1 Queue\nP0{1+(λ /μ) + (λ2 /μ2) + ...+ (λn+1 /μn+1) + ...} =1\n{1+(λ /μ) + (λ2 /μ2) + ...+ (λn+1 /μn+1) + ...} =1/[1-(λ /μ)]\nFor λ/μ < 1.\nP0 =1-λ /μ for λ /μ <1.\nPn = (λ /μ)n P0 = (λ /μ)n(1-λ /μ) for n =1,2,3,...\nSource: Larson and Odoni, Urban Operations Research\n\nThe M/M/1 Queue\nP0 =1-λ /μ for λ /μ <1.\nPn = (λ /μ)n P0 = (λ /μ)n(1-λ /μ) for n =1,2,3,...\nPT (z) ≡\nPn\nn= 0\ninf\n∑\nzn =\n(λ /μ)n(1-λ /μ)\nn= 0\ninf\n∑\nzn = 1-ρ\n1-ρz\nd\ndz PT (z)⎤\n⎦ ⎥\nz=1\n≡\nnPn\nn= 0\ninf\n∑\n= L = -(1-ρ)(-ρ)\n(1-ρz)2\n⎤\n⎦ ⎥ =\nρ\n1-ρ for ρ <1\nL = λW = ρ /(1-ρ)\nimplies W = (1/λ)ρ /(1-ρ) = (1/μ)/(1-ρ)\nLq = λWq etc.\n\nMean Wait vs. Rho\n0.2\n0.4\n0.6\n0.8\nRho\nSeries1\nNote the Elbow!\n\nMore on M/M/1 Queue\nLet w(t) = pdf for time in the system\n(including queue and service)\nAssume First-Come, First-Served (FCFS)\nQueue Discipline\nw(t) =\nw(t | k)Pk\nk= 0\ninf\n∑\n=\nμk+1t ke-μt\nk!\nρk\nk= 0\ninf\n∑\n(1-ρ)\nw(t) = μe-μt(1-ρ)\n(μtρ)k\nk!\nk= 0\ninf\n∑\n= μ(1-ρ)e-μte-μρt\nw(t) = μ(1-ρ)e-μ(1-ρ )t t ≥0\nExercise: Do the same for Time in queue\n\nBlackboard Modeling\n3 server zero line capacity\n3 server capacity for 4 in queue\nSame as above, but 50% of queuers\nbalk due to having to wait in queue\nSingle server who slows down to half\nservice rate when nobody is in queue\nMore?? ....\n\n0,0\n0,1\n1,0\n1,1\nAbout the 'cut'' between states to\nwrite the balance of flow equations...\n\nOptional Exercise:\nIs it ''better'' to enter a single\nserver queue with service rate μ\nor a 2-server queue each with rate\nμ /2?\nCan someone draw one or both of the\nstate-rate-transition diagrams?\nThen what do you do?\n\nFinal Example:\nSingle Server, Discouraged Arrivals\nλ /2\nλ /3\nλ /4\nλ /5\nState-Rate-Transition Diagram, Discouraged Arrivals\nPk = 1\nk!(λ\nμ)k P0\nP0 = [1+ (λ\nμ) + 1\n2!(λ\nμ)2 + 1\n3!(λ\nμ)3 + ...+ 1\nk!(λ\nμ)k + ...]-1\nP0 = (eλ / μ)-1 = e-λ / μ\n\nP0 = (eλ / μ)-1 = e-λ / μ > 0\nρ = utilization factor =1-P0 =1-e-λ /μ <1.\nPk = (λ /μ)k\nk!\ne-λ /μ, k = 0,1,2,... Poisson Distribution!\nL = time - average number in system = λ/μ How?\nL = λAW Little's Law, where\nλA ≡average rate of accepted arrivals into system\n\nApply Little's Law to Service Facility\nρ = λA(average service time)\nρ = average number in service facility = λA /μ\nλA = μρ = μ(1-e-λ /μ)\nW = L\nλA\n=\nλ /μ\nμ(1-e-λ /μ) =\nλ\nμ2(1-e-λ /μ)"
    },
    {
      "category": "Resource",
      "title": "gosset_1908.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/1dabbcb2c2fa46e67fb3655a35ec11fc_gosset_1908.pdf",
      "content": "THE PROBABLE ERROR OF A MEAN\nINTRODUCTION\nAny experiment may he regarded as forming an individual of a \"population\" of\nexperiments which might he performed under the same conditions. A series of experi\nments is a sample drawn from this population.\nNow any series of experiments is only of value in so far as it enables us to form\na judgment as to the statistical constants of the population to which the experiments\nbelong. In a greater number of cases the question finally turns on the value of a mean,\neither directly, or as the mean difference between the two quantities.\nIf the number of experiments be very large, we may have precise information as to\nthe value of the mean, but if our sample be small, we have two sources of uncertainty:\n(1) owing to the \"error of random sampling\" the mean of our series of experiments\ndeviates more or less widely from the mean of the population, and (2) the sample is not\nsufficiently large to determine what is the law of distribution of individuals. It is usual,\nhowever, to assume a normal distribution, because, in a very large number of cases,\nthis gives an approximation so close that a small sample will give no real information\nas to the manner in which the population deviates from normality: since some law of\ndistribution must he assumed it is better to work with a curve whose area and ordinates\nare tabled, and whose properties are well known. This assumption is accordingly made\nin the present paper, so that its conclusions are not strictly applicable to populations\nknown not to be normally distributed; yet it appears probable that the deviation from\nnormality must be very extreme to load to serious error. We are concerned here solely\nwith the first of these two sources of uncertainty.\nThe usual method of determining the probability that the mean of the population\nlies within a given distance of the mean of the sample is to assume a normal distribution\nabout the mean of the sample with a standard deviation equal to s/√n, where s is the\nstandard deviation of the sample, and to use the tables of the probability integral.\nBut, as we decrease the number of experiments, the value of the standard deviation\nfound from the sample of experiments becomes itself subject to an increasing error,\nuntil judgments reached in this way may become altogether misleading.\nIn routine work there are two ways of dealing with this difficulty: (1) an experi\nment may he repeated many times, until such a long series is obtained that the standard\ndeviation is determined once and for all with sufficient accuracy. This value can then\nhe used for subsequent shorter series of similar experiments. (2) Where experiments\nare done in duplicate in the natural course of the work, the mean square of the differ\nence between corresponding pairs is equal to the standard deviation of the population\nmultiplied by\n√\n2. We call thus combine together several series of experiments for\nthe purpose of determining the standard deviation. Owing however to secular change,\nthe value obtained is nearly always too low, successive experiments being positively\ncorrelated.\nThere are other experiments, however, which cannot easily be repeated very often;\nin such cases it is sometimes necessary to judge of the certainty of the results from\na very small sample, which itself affords the only indication of the variability. Some\nchemical, many biological, and most agricultural and large-scale experiments belong\nto this class, which has hitherto been almost outside the range of statistical inquiry.\n\nAgain, although it is well known that the method of using the normal curve is only\ntrustworthy when the sample is \"large\", no one has yet told us very clearly where the\nlimit between \"large\" and \"small\" samples is to be drawn.\nThe aim of the present paper is to determine the point at which we may use the\ntables of the probability integral in judging of the significance of the mean of a series of\nexperiments, and to furnish alternative tables for use when the number of experiments\nis too few.\nThe paper is divided into the following nine sections:\nI. The equation is determined of the curve which represents the frequency distribution\nof standard deviations of samples drawn from a normal population.\nII. There is shown to be no kind of correlation between the mean and the standard\ndeviation of such a sample.\nIII. The equation is determined of the curve representing the frequency distribution of\na quantity z, which is obtained by dividing the distance between the mean of a sample\nand the mean of the population by the standard deviation of the sample.\nIV. The curve found in I is discussed.\nV. The curve found in III is discussed.\nVI. The two curves are compared with some actual distributions.\nVII. Tables of the curves found in III are given for samples of different size.\nVIII and IX. The tables are explained and some instances are given of their use.\nX. Conclusions.\nSECTION I\nSamples of n individuals are drawn out of a population distributed normally, to\nfind an equation which shall represent the frequency of the standard deviations of these\nsamples.\nIf s be the standard deviation found from a sample x1x2 . . . xn (all these being\nmeasured from the mean of the population), then\nS(x2\n1)\nS(x1) 2\nS(x2\n1)\nS(x2\n1)\n2S(x1x2)\ns =\n=\n.\nn\n-\nn\nn\n-\nn2\n-\nn2\nSumming for all samples and dividing by the number of samples we get the moan\nvalue of s2, which we will write s 2:\ns 2 = nμ2\nnμ2 = μ2(n -1) ,\nn - n2\nn\nwhere μ2 is the second moment coefficient in the original normal distribution of x:\nsince x1, x2, etc. are not correlated and the distribution is normal, products involving\nodd powers of x1 vanish on summing, so that 2S(\nn\nx\n1x2 ) is equal to 0.\n\nIf M ′R represent the Rth moment coefficient of the distribution of s2 about the end\nof the range where s2 = 0,\n(n -1)\nM ′\nn\nAgain\n1 = μ2\n.\nS(x2\n1)\nS(x1)\ns =\nn\n-\nn\nS(x2\n1) 2\n2S(x\n\nS(x1)\n+\n1)\nS(x1)\n=\nn\n-\nn\nn\nn\n1)\nS(x )\n2S(x\n)\n2S(X )\n4S(x\n)\nS(x\nx\nx\n+\n+\n-\nn3\n-\n=\nn2\nn2\nn3\nn4\n1x2\n2)\n6S(x\n+ other terms involving odd powers of x1, etc. which\n+\nn4\nwill vanish on summation.\n) has n terms, butS(x2\n1x2\n2) has 1\nNow S(x\nn(n-1), hence summing for all samples\nand dividing by the number of samples, we get\nM ′\n(n -1)\n2μ4\n(n -1)\nn2\n(n -1)\nn3\nμ4\n+ μ4\nn3 + 3μ\n-2μ\n+ μ\n-\n=\nn2\nn\nn\nμ\n-2n + 1} +\n(n -1){n -2n + 3}.\n{n\n= μ4n\nn3\nNow since the distribution of x is normal, μ4\n, hence\n= 3μ\n(n -1)\nn3\n(n -1)(n + 1)\nn2\n′\nM2\nIn a similar tedious way I find\n{3n -3 +\n-2n + 3}\n= μ\nn\n= μ\n.\n(n -1)(n + 1)(n + 3)\nn3\n= μ 3\n′\nM3\n= μ\nand\n(n -1)(n + 1)(n + 3)(n + 5)\nn4\n′\nM4\nThe law of formation of these moment coefficients appears to be a simple one, but\nI have not seen my way to a general proof.\n.\n\nR\nh\nIf now MR be the Rth moment coefficient of s2 about its mean, we have\nM2 = μ 2 (n -1) {(n + 1) -(n -1)} = 2μ 2 (n -1) .\nn3\nn2\n(n -1)(n + 1)(n + 3)\n3(n -1) (2(n -1)\n(n -1)3\n= μ\n.\nM3\nn3\n-\nn\nn2\n-\nn3\n= μ2\n3 (n -1) {n 2 + 4n + 3 -6n + 6 -n 2 + 2n -1} = 8μ2\n3 (n -1) ,\nn3\nn3\nM4 = μ2\n(n -1)(n + 1)(n + 3)(n + 5) -32(n -1)2 -12(n -1)3 -(n -1)4\nn4\n= μ2\n4(n\nn4\n-1) {n 3 + 9n 2 + 23n + 15 -32n + 32 -12n 2 + 24n -12 -n 3 + 3n -3n + 1}\n12μ2\n4(n -1)(n + 3)\n=\n.\nn4\nHence\nM3\nM4\n3(n + 3)\nβ1 =\n=\n,\nβ2 =\n=\n,\nM2\nn -1\nM2\nn -1)\n∴ 2β2 -3β1 -6 = n -1{6(n + 3) -24 -6(n -1)} = 0.\nConsequently a curve of Prof. Pearson's Type III may he expected to fit the distri\nbution of s2 .\nThe equation referred to an origin at the zero end of the curve will be\ny = Cxpe-γx ,\nwhere\nM2\n4μ2\n2(n -1)n3\nn\nγ = 2\n=\n=\nM3\n8n2μ2(n -1)\n2μ2\nand\np = 4 -1 = n -1 -1 = n -3 .\nβ1\nConsequently the equation becomes\nn-3\nnx\ny = Cx 2 e-2μ2 ,\nwhich will give the distribution of s2 .\nn-3\nnx\nThe area of this curve is C 0\ninfx 2 e-2μ2 dx = I (say). The first moment coeffi\ncient about the end of the range will therefore be\nR\nn-1\nnx\nC -2μ2 x\nn-1 e-nx ix=inf\nR\nn-1\nn-3\nnx\nC 0\ninfx\ne-2μ2 dx\nn\n2μ2\nx=0\nC 0\ninf\nn μ2x 2 e-2μ2 dx\n=\n+\n.\nI\nI\nI\nThe first part vanishes at each limit and the second is equal to\nn-1 μ2I\nn\nn -1\n=\nμ2.\nI\nn\n\nZ\nZ\nZ\n\nh\nZ\n\nand we see that the higher moment coefficients will he formed by multiplying succes\nsively by n+1 μ2, n+3 μ2 etc., just as appeared to he the law of formation of M2\n′ , M3\n′ ,\nn\nn\nM4\n′, etc.\nHence it is probable that the curve found represents the theoretical distribution of\ns2; so that although we have no actual proof we shall assume it to do so in what follows.\nThe distribution of s may he found from this, since the frequency of s is equal to\nthat of s2 and all that we must do is to compress the base line suitably.\nNow if\ny1 = φ(s2) be the frequency curve of s2\nand\ny2 = ψ(s) be the frequency curve of s,\nthen\ny1d(s 2) = y2ds,\ny2ds = 2y1sds,\n∴ y2 = 2sy1.\nHence\nn-3\nns\ny2 = 2Cs(s 2)\n2 e-2μ2 .\nis the distribution of s.\nThis reduces to\n2σ2\ny2 = 2Csn-2 e-ns .\ns\n2μ2\nHence y = Axn-2e-\nwill give the frequency distribution of standard deviations\nof samples of n, taken out of a population distributed normally with standard deviation\nσ2. The constant A may he found by equating the area of the curve as follows:\nnx\n-nx\np\nArea = A\ninf\nx n-2 e-2σ2 dx.\n\nLet Ip represent\ninf\nx e-\n2σ2 dx.\n\nThen\nnx\nIp = σ2\ninf\nxp-1 d\n-e-2σ2\ndx\nn\ndx\nnx\nx\n= σ2\n-xp-1 e-2σ2\nix=inf\n+ σ2 (p -1)\ninf\nxp-2 e-2σ2 dx\nn\nx=0\nn\nσ2\n=\n(p -1)Ip-2,\nn\nsince the first part vanishes at both limits.\nBy continuing this process we find\nn-2\nσ2\nIn-2 =\n(n -3)(n -5) . . . 3.1I0\nn\nor\nn-2\nσ2\nIn-2 =\n(n -3)(n -5) . . . 4.2I1\nn\n\nZ\nr\n\nZ\n\naccording n is even or odd.\nBut I0 is\nnx\ninf\nπ\ne-2σ2 dx =\nσ,\n2n\nand I1 is\ninf\nσ2\n\nσ2\n2sigma2 dx =\n2σ2\nxe-\nnx\n-n e-nx\nx = 0x=inf = n .\nHence if n be even,\nArea\nA =\nq\nn-1 ,\n(n -3)(n -5) . . . 3.1\nπ\nσ2\nn\nwhile is n be odd\nArea\nA =\nn-1 .\n(n -3)(n -5) . . . 4.2\nσ2\nn\nHence the equation may be written\nnx\ny =\nN q\nn-1 x n-2 e-2σ2\n(n even)\n(n -3)(n -5) . . . 3.1\nn\nπ\nσ2\nor\nn-1\nnx\ny = (n -3)(n\nN\n-5) . . . 4.2\nσ\nn\nx n-2 e-2σ2\n(n odd)\nwhere N as usual represents the total frequency.\nSECTION II\nTo show that there is no correlation between (a) the distance of the mean of a\nsample from the mean of the population and (b) the standard deviation of a sample\nwith normal distribution.\n(1) Clearly positive and negative positions of the mean of the sample are equally\nlikely, and hence there cannot be correlation between the absolute value of the distance\nof the mean from the mean of the population and the standard deviation, but (2) there\nmight be correlation between the square of the distance and the square of the standard\ndeviation. Let\n\nS(x1)\nS(x2\n1)\nS(x1)\nu 2 =\nand s 2 =\n.\nn\nn\n-\nn\nThen if m′\n1, M1\n′ be the mean values of u2 and sz, we have by the preceding part\nM1\n′ = μ2\n(n -1)\nand m1\n′ = μ2 .\nn\nn\n\np\np\nNow\nS(x2\n1) S(x1) 2\nS(x1) 4\n2 2\nu s =\nn\nn\n-\nn\n\nS(x1\n2)\nS(x1x2).S(x1\n2)\nS(x1\n4)\n6S(x1\n2x2\n2)\n=\n+ 2\nn\nn3\n-\nn4\n-\nn4\n-other terms of odd order which will vanish on summation.\nSumming for all values and dividing by the number of cases we get\nRu2 s2 σu2 σs2 + m1M1 = n\nμ\n4 + μ2\n2 (n\nn\n-\n1) -n\nμ\n4 -3μ2\n2 (n\nn\n-\n1) ,\nwhere Ru s2 is the correlation between u2 and s2 .\nRu2 s2 σu2 σs2 + μ2\n2 (n -1) = μ2\n2 (n -1) {3 + n -3} = μ2\n2 (n -1) .\nn2\nn3\nn2\nHence Ru2 s2 σu2 σs = 0, or there is no correlation between u2 and s2\n.\nSECTION III\nTo find the equation representing the frequency distribution of the means of sam\nples of n drawn from a normal population, the mean being expressed in terms of the\nstandard deviation of the sample.\nWe have y = σn-1 sn-2e-2σ2 as the equation representing the distribution of s,\nthe standard deviation of a sample of n, when the samples are drawn from a normal\npopulation with standard deviation s.\nNow the means of these samples of n are distributed according to the equation∗\nC\nnx\nnx\ny = p (n)N e-2σ2\n,\n(2π)σ\nand we have shown that there is no correlation between x, the distance of the mean of\nthe sample, and s, the standard deviation of the sample.\nNow let us suppose x measured in terms of s, i.e. let us find the distribution of\nz = x/s.\nIf we have y1 = φ(x) and y2 = ψ(z) as the equations representing the frequency\nof x and of z respectively, then\ndx\ny1dx = y2dz = y3\n,\ns\n∴ y2 = sy1.\nHence\nns z\n2σ2\ny = N\np (n)se-\n(2π)σ\n∗Airy, Theory of Errors of Observations, Part II, §6.\n\nR\nq\nR\nR\nq\nR\nR\ns\nr\n\nis the equation representing the distribution of z for samples of n with standard devia\ntion s.\nNow the chance that s lies between s and s + ds is\nns\nR s+ds\nC\nσn-1 sn-2e-2σ2 ds\ns\ninf\nC\nns\nσn-1 sn-2e-2σ2 ds\nwhich represents the N in the above equation.\nHence the distribution of z due to values of s which lie between s and s + ds is\nR s+ds C\nn\nns 2(1+z 2)\nr\nR s+ds\nns 2(1+z 2)\n2σ2\nC\ns\nσn\n2π sn-1e-\nds\nn\ns\nσn sn-1e-\n2σ2\nds\ny =\nns\n=\nns\ninf\nσnC\n-1 sn-2e-2σ2\nds\n2π\ninf\nσnC\n-2 sn-2e-2σ2\nds\nand summing for all values of s we have as an equation giving the distribution of z\n\nn\nπ\n\ns+ds C sn-1e- ns 2(1+z 2)\nds\n2σ2\ns\nσn\ny =\n.\nns\nσ\ninf\nσnC\n-2 sn-2e-2σ2 ds\nBy what we have already proved this reduces to\ny = 1 n -2 .n -4 . . . 5 . 3(1 + z 2)-2 n ,\nif n be odd\n2 n -3 n -5\n4 2\nand to\ny = 1 n -2 .n -4 . . . 4 . 2 (1 + z 2)-2 n ,\nif n be even\n2 n -3 n -5\n3 21\nSince this equation is independent of σ it will give the distribution of the distance\nof the mean of a sample from the mean of the population expressed in terms of the\nstandard deviation of the sample for any normal population.\nSECTION IV. SOME PROPERTIES OF THE STANDARD\nDEVIATION FREQUENCY CURVE\nBy a similar method to that adopted for finding the constant we may find the mean\nand moments: thus the mean is at In-1/In-2,\nwhich is equal to\nn -2 .n -4 . . . 2\nσ ,\nif n be even,\nn -3 n -5\nπ √n\nor\nn -2 .n -4 . . . 3\nπ\nσ ,\nif n be odd .\nn -3 n -5\n√n\nThe second moment about the end of the range is\nIn\n= (n -1)σ2\n.\nn\nIn-2\n\nq\nThe third moment about the end of the range is equal to\nIn+1 = In+1 .In -1\nIn-2\nIn-1\nIn-2\n= σ2 × the mean.\nThe fourth moment about the end of the range is equal to\nIn+2 = (n -1)(n + 1) σ4 .\nIn-2\nn2\nIf we write the distance of the mean from the end of the range Dσ/√n and the\nmoments about the end of the range ν1, ν2, etc.,\nthen\nν1 = Dσ\n√n ,\nν2 = n -1\nn σ2,\nν3 = Dσ3\n√n ,\nν4 = N 2 -1\nn\nσ4 .\nFrom this we get the moments about the mean:\nμ2 = σ2\nn (n -1 -D2),\nσ3\nσ3D\nn√n{nD -3(n -1)D + 2D2\nn√n {2D2\nμ3 =\n} =\n-2n + 3},\nσ2\nσ4\nμ4 = n2 {n 2 -1 -4D2 n + 6(n -1)D2 -3D4 } = n2 {n 2 -1 -D2(3D2 -2n + 6)}.\nIt is of interest to find out what these become when n is large.\nIn order to do this we must find out what is the value of D.\nNow Wallis's expression for π derived from the infinite product value of sin x is\nπ\n22 .42 .62 . . . (2n)2\n2 (2n + 1) = 123252 . . . (2n -1)2 .\nIf we assume a quantity θ = a0 + a\nn\n1 + etc. which we may add to the 2n + 1 in\norder to make the expression approximate more rapidly to the truth, it is easy to show\nthat θ = -1 + 16\nn-etc., and we get+\nπ\n22 .42 .62 . . . (2n)2\n2n +\n+\n=\n.\n16n\n123252 . . . (2n -1)2\nFrom this we find that whether n be even or odd D2 approximates to n -3 +\n8n\nwhen n is large.\nSubstituting this value of D we get\nσ2\nσ3\n\n1 -2\nn + 16\nn\n\n3σ4\n\nμ2 = 2n 1 -4n , μ2 =\n4n2\n, μ4 = 4n2\n1 + 2n -16n2\n.\n+This expression will be found to give a much closer approximation to π than Wallis's\n\n!\nConsequently the value of the standard deviation of a standard deviation which we\nhave found\nσ\nbecomes the same as that found for the normal curve\n√\n(2n)√\n{1-(1/4n)}\nby Prof. Pearson {σ/(2n)} when n is large enough to neglect the 1/4n in comparison\nwith 1.\nNeglecting terms of lower order than 1/n, we find\n2n -3\nβ1 = n(4n -3),\nβ)2 = 3\n1 -2n\n1 + 2n\n.\nConsequently, as n increases, β2 very soon approaches the value 3 of the normal\ncurve, but β1 vanishes more slowly, so that the curve remains slightly skew.\nDiagram I shows the theoretical distribution of the standard deviations found from\nsamples of 10.\nSECTION V. SOME PROPERTIES OF THE CURVE\ny = n -2 .n -4 . . .\n4 .\nπ\n2 if\nn be even\n(1 + z 2)-2 n\nn -3 n -5\n. . if n be odd\n4 2 2\nn-2 n-4\nWriting z = tan θ the equation becomes y = n-3 . n-5 . . . etc. ×cosn θ, which affords\nan easy way of drawing the curve. Also dz = dθ/ cos2 θ.\n\nZ\n\nZ\nZ\nHence to find the area of the curve between any limits we must find\nn -2 .n -4 . . . etc.\ncos n-2 θdθ\nn -3 n -5\n×\n= n -2 .n -4 . . . etc. n -3\ncos n-4 θdθ +\ncosn-3θ sin θ\nn -3 n -5\nn -2\nn -2\n= n -2 .n -4 . . . etc.\ncos n-4 θdθ +\nn -4 . . . etc.[cosn-3 θ sin θ],\nn -3 n -5\nn -3 n -5\nand by continuing the process the integral may he evaluated.\nFor example, if we wish to find the area between 0 and θ for n = 8 we have\nArea = 6 . 4 . 2 . 1 Z θ\ncos 6 θdθ\n5 3 1 π\n4 2 Z θ\n1 4 2\n=\n.\ncos 4 θdθ +\n. . cos 5 θ sin θ\n3 π\n5 3 π\n= θ + 1 cos θ sin θ + 1 . 2 cos 3 θ sin θ + 1 . 4 . 2 cos 5 θ sin θ\nπ\nπ\n3 π\n5 3 π\nand it will be noticed that for n = 10 we shall merely have to add to this same expres\nsion the term 1 . 6 . 4 . 2 cos7 θ sin θ.\n7 5 3 π\nThe tables at the end of the paper give the area between -infand z\n\nπ\n\nor θ =\nand θ = tan-1 z\n.\n-2\nThis is the same as 0.5 + the area between θ = 0, and θ = tan-1 z, and as the\nwhole area of the curve is equal to 1, the tables give the probability that the mean of\nthe sample does not differ by more than z times the standard deviation of the sample\nfrom the mean of the population.\nThe whole area of the curve is equal to\n2 π\nn -2 .n -4 . . . etc.\nZ +\ncos n-2 θdθ\n×\nn -3 n -5\n-1\n2 π\nand since all the parts between the limits vanish at both limits this reduces to 1.\nSimilarly, the second moment coefficient is equal to\n2 π\nn -2 .n -4 . . . etc.\nZ +\ncos n-2 θ tan2 θdθ\n×\nn -3 n -5\n-1\n2 π\n2 π\nn -2 .n -4 . . . etc.\nZ +\n(cosn-4 θ -cos n-2 θ)dθ\n=\n×\nn -3 n -5\n-1\n2 π\nn -2\n= n -3 -1 = n -3 .\n\np\np\nHence the standard deviation of the curve is 1/ (n -3). The fourth moment\ncoefficient is equal to\n2 π\nn -2 .n -4 . . . etc.\nZ +\ncos n-2 θ tan4 θdθ\n×\nn -3 n -5\n-1\n2 π\n2 π\nn -2 n -4\nZ +\n=\n.\n. . . etc.\n(cosn-6 θ -2 cosn-4 θ + cos n-2 θ)dθ\n×\nn -3 n -5\n-1\n2 π\nn -2 n -4\n2(n -2)\n=\n.\n+ 1 =\n.\nn -3 n -5 -\nn -3\n(n -3)(n -5)\nThe odd moments are of course zero, an the curve is symmetrical, so\n3(n -3)\nβ1 = 0,\nβ2 =\n= 3 +\n.\nn -5\nn -5\nHence as it increases the curve approaches the normal curve whose standard devi\nation is 1/ (n -3).\nβ2, however, is always greater than 3, indicating that large deviations are mere\ncommon than in the normal curve.\nI have tabled the area for the normal curve with standard deviation 1/\n√\n7 so as to\ncompare, with my curve for n = 10++. It will be seen that odds laid according to either\n++See p. 29\n\ntable would not seriously differ till we reach z = 0.8, where the odds are about 50 to\n1 that the mean is within that limit: beyond that the normal curve gives a false feeling\nof security, for example, according to the normal curve it is 99,986 to 14 (say 7000 to\n1) that the mean of the population lies between -infand +1.3s, whereas the real odds\nare only 99,819 to 181 (about 550 to 1).\nNow 50 to 1 corresponds to three times the probable error in the normal curve and\nfor most purposes it would be considered significant; for this reason I have only tabled\nmy curves for values of n not greater than 10, but have given the n = 9 and n = 10\ntables to one further place of decimals. They can he used as foundations for finding\nvalues for larger samples.§\nThe table for n = 2 can be readily constructed by looking out θ = tan-1 z in\nChambers's tables and then 0.5 + θ/π gives the corresponding value.\nSimilarly 1\n2 sin θ + 0.5 gives the values when n = 3.\nThere are two points of interest in the n = 2 curve. Here s is equal to half the\nπ\ndistance between the two observations, tan-1\n, so that between +s and -z lies\nor half the probability, i.e. if two observations have been made and we have\ns\ns = 4\nπ\n2 ×\nno other information, it is an even chance that the mean of the (normal) population will\n4 × π\nlie between them. On the other hand the second moment coefficient is\nπ\n1 Z +\nπ=inf\n+\n2 θdθ =\n[tan θ -θ]\ntan\n= inf,\nπ\nπ\nπ\n=-\nπ\n=-\nor the standard deviation is infinite while the probable error is finite.\nSECTION VI. PRACTICAL TEST OF THE FOREGOING EQUATIONS\nBefore I bad succeeded in solving my problem analytically, I had endeavoured to\ndo so empirically. The material used was a correlation table containing the height and\nleft middle finger measurements of 3000 criminals, from a paper by W. R. Macdonnell\n(Biometrika, I, p. 219). The measurements were written out on 3000 pieces of card\nboard, which were then very thoroughly shuffled and drawn at random. As each card\nwas drawn its numbers were written down in a book, which thus contains the measure\nments of 3000 criminals in a random order. Finally, each consecutive set of 4 was taken\nas a sample--750 in all--and the mean, standard deviation, and correlation¶ of each\nsample determined. The difference between the mean of each sample and the mean of\nthe population was then divided by the standard deviation of the sample, giving us the\nz of Section III.\nThis provides us with two sets of 750 standard deviations and two sets of 750\nz's on which to test the theoretical results arrived at. The height and left middle finger\ncorrelation table was chosen because the distribution of both was approximately normal\nand the correlation was fairly high. Both frequency curves, however, deviate slightly\nfrom normality, the constants being for height β1 = 0.0026, β2 = 3.176, and for left\nmiddle finger lengths β1 = 0.0030, β2 = 3.140, and in consequence there is a tendency\n§E.g. if n = 11, to the corresponding value for n = 9, we add 7\n4 × 1\n2 × 1\n2 cos 8 θ sin θ: if\n×\n×\n2 cos10 θ sin θ, and so on.\nn = 13 we add as well\n×\n¶I hope to publish the results of the correlation work shortly.\n×\n×\n×\n×\n\nfor a certain number of larger standard deviations to occur than if the distributions wore\nnormal. This, however, appears to make very little difference to the distribution of z.\nAnother thing which interferes with the comparison is the comparatively large\ngroups in which the observations occur. The heights are arranged in 1 inch groups,\nthe standard deviation being only 2.54 inches. while, the finger lengths wore originally\ngrouped in millimetres, but unfortunately I did not at the time see the importance of\nhaving a smaller unit and condensed them into 2 millimetre groups, in terms of which\nthe standard deviation is 2.74.\nSeveral curious results follow from taking samples of 4 from material disposed in\nsuch wide groups. The following points may be noticed:\n(1) The means only occur as multiples of 0.25. (2) The standard deviations occur\nas the square roots of the following types of numbers: n, n + 0.10, n + 0.25, n + 0.50,\nn + 0.69, 2n + 0.75.\n(3) A standard deviation belonging to one of these groups can only be associated\nwith a mean of a particular kind; thus a standard deviation of\n√\n2 can only occur if the\nmean differs by a whole number from the group we take as origin, while\n√\n1.69 will\nonly occur when the mean is at n ± 0.25.\n(4) All the four individuals of the sample will occasionally come from the same\ngroup, giving a zero value for the standard deviation. Now this leads to an infinite\nvalue of z and is clearly due to too wide a grouping, for although two men may have\nthe same height when measured by inches, yet the finer the measurements the more\nseldom will they he identical, till finally the chance that four men will have exactly\nthe same height is infinitely small. If we had smaller grouping the zero values of the\nstandard deviation might be expected to increase, and a similar consideration will show\nthat the smaller values of the standard deviation would also be likely to increase, such\nas 0.436, when 3 fall in one group and 1 in an adjacent group, or 0.50 when 2 fall in\ntwo adjacent groups. On the other hand, when the individuals of the sample lie far\napart, the argument of Sheppard's correction will apply, the real value of the standard\ndeviation being more likely to he smaller than that found owing to the frequency in any\ngroup being greater on the side nearer the mode.\nThese two effects of grouping will tend to neutralize the effect on the mean value\nof the standard deviation, but both will increase the variability.\nAccordingly, we find that the mean value of the standard deviation is quite close to\nthat calculated, while in each case the variability is sensibly greater. The fit of the curve\nis not good, both for this reason and because the frequency is not evenly distributed\nowing to effects (2) and (3) of grouping. On the other hand, the fit of the curve giving\nthe frequency of z is very good, and as that is the only practical point the comparison\nmay he considered satisfactory.\nThe following are the figures for height:\nMean value of standard deviations:\nCalculated\nObserved\n2.027 ± 0.02\n2.026\nStandard deviation of standard deviations:\nDifference =\nCalculated\nObserved\n-0.001\n0.8558 ± 0.015\n0.9066\nDifference\n+0.0510\n\n2x\n= 16×750\n√\n(2π)σ2 x 2 e-\nComparison of Fit. Theoretical Equation: y\nσ2\nScale in terms of standard deviations of population\nCalculated frequency\n27 45\n71 58\n45 33 23 15\nObserved frequency\n35 28 12\nDifference\n2 +6 0\n+1\n+4 -2\n-8 +42\n-11\n-14 -11 -4 -7 -5\n+4\n+2 +5 -2\n-\nWhence χ2 = 48.06, P = 0.00006 (about).\nIn tabling the observed frequency, values between 0.0125 and 0.0875 were included\nin one group, while between 0.0875 and 0.012.5 they were divided over the two groups.\nAs an instance of the irregularity due to grouping I may mention that there were 31\ncases of standard deviations 1.30 (in terms of the grouping) which is 0.5117 in terms\nof the standard deviation of the population, and they wore therefore divided over the\ngroups 0.4 to 0.5 and 0.5 to 0.6. Had they all been counted in groups 0.5 to 0.6 χ2\nwould have fallen to 20.85 and P would have risen to 0.03. The χ2 test presupposes\nrandom sampling from a frequency following the given law, but this we have not got\nowing to the interference of the grouping.\nWhen, however, we test the z's where the grouping has not had so much effect, we\nfind a close correspondence between the theory and the actual result.\nThere were three cases of infinite values of z which, for the reasons given above,\nwere given the next largest values which occurred, namely +6 or -6. The rest were\ndivided into groups of 0.1; 0.04, 0.05 and 0.06, being divided between the two groups\non either side.\nThe calculated value for the standard deviation of the frequency curve was 1 (±0.0171),\nwhile the observed was 1.030. The value of the standard deviation is really infinite, as\nthe fourth moment coefficient is infinite, but as we have arbitrarily limited the infinite\ncases we may take as an approximation 1/\n√\n1500 from which the value of the probable\nerror given above is obtained. The fit of the curve is as follows:\nComparison of Fit. Theoretical Equation: y = 2N\nθ, z = tan θ\ncos\nπ\nScale of z\nCalculated frequency\n5 9\nObserved frequency\n119 141\n9 14\nDifference\n33 43\n+4 +4 -2 -2 -1\n-1 -8\n+10\n+3 -11 +4\n-8 +2\n+\n+\nWhence χ2 = 12.44, P = 0.56.\nThis is very satisfactory, especially when we consider that as a rule observations are\ntested against curves fitted from the mean and one or more other moments of the obser\nvations, so that considerable correspondence is only to ])c expected; while this curve\nis exposed to the full errors of random sampling, its constants having been calculated\nquite apart from the observations.\nThe left middle finger samples show much the same features as those of the height,\nbut as the grouping is not so large compared to the variability the curves fit the obser\n\nvations more closely. Diagrams III∥and IV give the standard deviations of the z's for\nthe set of samples. The results are as follows:\nMean value of standard deviations:\nCalculated\nObserved\n2.186 ± 0.023\n2.179\nStandard deviation of standard deviations:\nDifference =\nCalculated\nObserved\n-0.007\n0.9224 ± 0.016\n0.9802\nDifference =\n+0.0578\n2x\n= 16×750\n√\n(2π)σ2 x 2 e-\nComparison of Fit. Theoretical Equation: y\nσ2\nScale in terms of standard deviations of population\n1 71\n27 45\n33 23 15\nCalculated frequency\n14 27\n20 22\nObserved frequency\n+3\n+5\n+12\n+7\n-19\n-16 +2 -9\n-4\n+9\n-3 +7\n+2\n--\n+\n+\n+\n-\nWhence χ2 = 21.80, P = 0.19.\nValue of standard deviation:\nCalculated\n1(±0.017)\nObserved\n0.982\nDifference =\n-0.018\nComparison of Fit. Theoretical Equation: y = 2N\nθ, z = tan θ\ncos\nπ\nScale of z\nCalculated frequency\n5 9\nObserved frequency\n119 141 119\n4 15\nDifference\n75 122 138 120\n-1 +6 +4\n-1\n-3\n+3 -3 +1\n-7\n+2 +1\n-2\n+1\n-\n-\nWhence χ2 = 7.39, P = 0.92.\nA very close fit.\nWe see then that if the distribution is approximately normal our theory gives us\na satisfactory measure of the certainty to be derived from a small sample in both the\ncases we have tested; but we have an indication that a fine grouping is of advantage. If\nthe distribution is not normal, the mean and the standard deviation of a sample will be\npositively correlated, so although both will have greater variability, yet they will tend\nto counteract one another, a mean deriving largely from the general mean tending to be\ndivided by a larger standard deviation. Consequently, I believe that the table given in\nSection VII below may be used in estimating the degree of certainty arrived at by the\nmean of a few experiments, in the case of most laboratory or biological work where\nthe distributions are as a rule of a \"cocked hat\" type and so sufficiently nearly normal\n∥There are three small mistakes in plotting the observed values in Diagram III, which make the fit appear\nworse than it really is\n\n3 1\nn-2 n-4\n.\nn ODD\nR tan-1 z\nSECTION VII. TABLES OF\nn-5 . . .\n2 2\ncosn-2 θdθ FOR\n.\nn EVEN\nn-3 .\nπ\n-1 π\nVALUES OF n FROM 4 TO 10 INCLUSIVE\n√\n7 R x\n7x 2\nTogether with √\n(2π)\n-inf e-\ndx for comparison when n = 10\n`\n\nz = x\nn = 4 n = 5 n = 6 n = 7 n = 8 n = 9 n = 10\nFor comparison\ns\n„\n«\nR\n7x\n√\nx\ne-\ndx\n√\n(2π)\n-inf\n0.1\n0.5633 0.5745 0.5841 0.5928 0.6006 0.60787 0.61462\n0.60411\n0.2\n0.6241 0.6458 0.6634 0.6798 0.6936 0.70705 0.71846\n0.70159\n0.3\n0.6804 0.7096 0.7340 0.7549 0.7733 0.78961 0.80423\n0.78641\n0.4\n0.7309 0.7657 0.7939 0.8175 0.8376 0.85465 0.86970\n0.85520\n0.5\n0.7749 0.8131 0.8428 0.8667 0.8863 0.90251 0.91609\n0.90691\n0.6\n0.8125 0.8518 0.8813 0.9040 0.9218 0.93600 0.94732\n0.94375\n0.7\n0.8440 0.8830 0.9109 0.9314 0.9468 0.95851 0.96747\n0.96799\n0.8\n0.8701 0.9076 0.9332 0.9512 0.9640 0.97328 0.98007\n0.98253\n0.9\n0.8915 0.9269 0.9498 0.9652 0.9756 0.98279 0.98780\n0.99137\n1.0\n0.9092 0.9419 0.9622 0.9751 0.9834 0.98890 0.99252\n0.99820\n1.1\n0.9236 0.9537 0.9714 0.9821 0.9887 0.99280 0.99539\n0.99926\n1.2\n0.9354 0.9628 0.9782 0.9870 0.9922 0.99528 0.99713\n0.99971\n1.3\n0.9451 0.9700 0.9832 0.9905 0.9946 0.99688 0.99819\n0.99986\n1.4\n0.9451 0.9756 0.9870 0.9930 0.9962 0.99791 0.99885\n0.99989\n1.5\n0.9598 0.9800 0.9899 0.9948 0.9973 0.99859 0.99926\n0.99999\n1.6\n0.9653 0.9836 0.9920 0.9961 0.9981 0.99903 0.99951\n1.7\n0.9699 0.9864 0.9937 0.9970 0.9986 0.99933 0.99968\n1.8\n0.9737 0.9886 0.9950 0.9977 0.9990 0.99953 0.99978\n1.9\n0.9970 0.9904 0.9959 0.9983 0.9992 0.99967 0.99985\n2.0\n0.9797 0.9919 0.9967 0.9986 0.9994 0.99976 0.99990\n2.1\n0.9821 0.9931 0.9973 0.9989 0.9996 0.99983 0.99993\n2.2\n0.9841 0.9941 0.9978 0.9992 0.9997 0.99987 0.99995\n2.3\n0.9858 0.9950 0.9982 0.9993 0.9998 0.99991 0.99996\n2.4\n0.9873 0.9957 0.9985 0.9995 0.9998 0.99993 0.99997\n2.5\n0.9886 0.9963 0.9987 0.9996 0.9998 0.99995 0.99998\n2.6\n0.9898 0.9967 0.9989 0.9996 0.9999 0.99996 0.99999\n2.7\n0.9908 0.9972 0.9989 0.9997 0.9999 0.99997 0.99999\n2.8\n0.9916 0.9975 0.9989 0.9998 0.9999 0.99998 0.99999\n2.9\n0.9924 0.9978 0.9989 0.9998 0.9999 0.99998 0.99999\n3.0\n0.9931 0.9981 0.9989 0.9998\n--\n0.99999\n--\nEXPLANATION OF TABLES\nThe tables give the probability that the value of the mean, measured from the mean\nof the population, in terms of the standard deviation of the sample, will lie between\n-infand z. Thus, to take the table for samples of 6, the probability of the mean of the\npopulation lying between -infand once the standard deviation of the sample is 0.9622,\nthe odds are about 24 to 1 that the mean of the population lies between these limits.\nThe probability is therefore 0.0378 that it is greater than once the standard deviation\nand 0.07511 that it lies outside ±1.0 times the standard deviation.\nILLUSTRATION OF METHOD\nIllustration I. As an instance of the kind of use which may be made of the tables,\nI take the following figures from a table by A. R. Cushny and A. R. Peebles in the\n\nJournal of Physiology for 1904, showing the different effects of the optical isomers of\nhyoscyamine hydrobromide in producing sleep. The average number of hours' sleep\ngained by the use of the drug is tabulated below.\nThe conclusion arrived at was that in the usual does 2 was, but 1 was not, of value\nas a soporific.\nAdditional hours' sleep gained by the use of hyoscyamine hydrobromide\nPatient\n1 (Dextro-)\n2 (Laevo-)\nDifference (2 -1)\n+0.7\n+1.9\n+1.2\n-1.6\n+0.8\n+2.4\n-0.2\n+1.1\n+1.3\n-1.2\n+0.1\n+1.3\n-0.1\n-0.1\n+3.4\n+4.4\n+1.0\n+3.7\n+5.5\n+1.8\n+0.8\n+1.6\n+0.8\n+4.6\n+4.6\n+2.0\n+3.4\n+1.4\nMean\n+0.75\nMean\n+2.33\nMean\n+1.58\nS.D.\n0.75\nS.D.\n1.90\nS.D.\n1.17\nFirst let us see what is the probability that 1 will on the average give increase of\nsleep; i.e. what is the chance that the mean of the population of which these experi\nments are a sample is positive. +0.75/1.70 = 0.44, and looking out z = 0.44 in the\ntable for ten experiments we find by interpolating between 0.8697 and 0.9161 that 0.44\ncorresponds to 0.8873, or the odds are 0.887 to 0.113 that the mean is positive.\nThat is about 8 to 1, and would correspond to the normal curve to about 1.8 times\nthe probable error. It is then very likely that 1 gives an increase of sleep, but would\noccasion no surprise if the results were reversed by further experiments.\nIf now we consider the chance that 2 is actually a soporific we have the mean\ninclrease of sleep = 2.33/1.90 or 1.23 times the S.D. From the table the probability\ncorresponding to this is 0.9974, i.e. the odds are nearly 400 to 1 that such is the case.\nThis corresponds to about 4.15 times the probable error in the normal curve. But I take\nit that the real point of the authors was that 2 is better than 1. This we must t4est by\nmaking a new series, subtracting 1 from 2. The mean values of this series is +1.38,\nwhile the S.D. is 1.17, the mean value being +1.35 times the S.D. From the table, the\nprobability is 0.9985, or the odds are about 666 to one that 2 is the better soporific. The\nlow value of the S.D. is probably due to the different drugs reacting similarly on the\nsame patient, so that there is correlation between the results.\nOf course odds of this kind make it almost certain that 2 is the better soporific, and\nin practical life such a high probability is in most matters considered as a certainty.\nIllustration II. Cases where the tables will be useful are not uncommon in agricul\ntural work, and they would be more numerous if the advantages of being able to apply\nstatistical reasoning were borne in mind when planning the experiments. I take the\nfollowing instances from the accounts of the Woburn farming experiments published\nyearly by Dr Voelcker in the Journal of the Agricultural Soceity.\nA short series of pot culture experiments were conducted in order to determine the\ncasues which lead to the production of Hard (glutinous) wheat or Soft (starchy) wheat.\n\nIn three successive years a bulk of seed corn of one variety was picked over by hand\nand two samples were selected, one consisting of \"hard\" grains avid the other of \"soft\".\nSome of each of them were planted in both heavy and light soil and the resulting crops\nwore weighed and examined for hard and soft corn.\nThe conclusion drawn was that the effect of selecting the seed was negligible com\npared with the influence of the soil.\nThis conclusion was thoroughly justified, theheavy soul producing in each case\nnearly 100% of hard corn, but still the effect of selecting the seed could just be traced\nin each year.\nBut a curious point, to which Dr Voelcker draws attention in the second year's\nreport, is that the soft seeds produced the higher yield of both corn and straw. In\nview of the well-known fact that the varieties which have a high yield tend to produce\nsoft corn, it is interesting to see how much evidence the experiments afford as to the\ncorrelation between softness and fertility in the same variety.\nFurther, Mr Hooker∗∗ has shown that the yield of wheat in one year is largely\ndetermined by the weather during the preceding year. Dr Voelcker's results may afford\na clue as to the way in which the seed id affected, and would almost justify the selection\nof particillar soils for growing wheat.++\nTh figures are as follows, the yields being expressed in grammes per pot:\nYear\nSoil\nYield of corn from soft seed\nYield of corn from hard seed\nDifference\nYield of straw from soft seed\nYield of straw from hard seed\nDifference\nStandard\nLight Heavy Light Heavy Light Heavy Average deviation z\n7.55\n8.89 14.81 13.55 7.49 15.39 11.328\n7.27\n8.32 13.81 13.36 7.97 13.13 10.643\n+0.58 +0.57 +1.00 +0.19 -0.49 +2.26 +0.685 0.778 0.88\n12.81 12.87 22.22 20.21 13.97 22.57 17.442\n10.71 12.48 21.64 20.26 11.71 18.96 15.927\n+2.10 +0.39 +0.78 -0.05 +2.66 +3.61 +1.515 1.261 1.20\nIf we wish to laid the odds that the soft seed will give a better yield of corn on the\naverage, we divide, the average difference by the standard deviation, giving us\nz = 0.88.\nLooking this up in the table for n = 6 we find p = 0.9465 or the odds are 0.9465 to\n0.0535 about 18 to 1.\nSimilarly for straw z = 1.20, p = 0.9782, and the odds are about 45 to 1.\nIn order to see whether such odds are sufficient for a practical man to draw a definite\nconclusion, I take another act of experiments in which Dr Voelcker compares the effects\nof different artificial manures used with potatoes on a large scale.\nThe figures represent the difference between the crops grown with the rise of sul\nphate of potash and kailit respectively in both 1904 and 1905:\n\ncwt. qr. lb.\nton cwt. qr. lb.\n1904 + 10\n3 20 : + 1\n1 26\n(two experiments in each year)\n1905 +\n3 : +\n∗∗Journal of the Royal Statistical Society, 1897\n++And perhaps a few experiments to see whether there is a correlation between yield and \"mellowness\" in\nbarley.\n\nThe average gain by the use of sulphate of potash was 15.25 cwt. and the S.D. 9\ncwt., whence, if we want the odds that the conclusion given below is right, z = 1.7,\ncorresponding, when n = 4,to p = 0.9698 or odds of 32 to 1; this is midway between\nthe odds in the former example. Dr Voelcker says: \"It may now fairly be concluded\nthat for the potato crop on light land 1 cwt. per acre of sulphate of potash is a better\ndressing than kailit.\"\nAm an example of how the table should be used with caution, I take the following\npot culture experiments to test whether it made any difference whether large or small\nseeds were sown.\nIllustration III. In 1899 and in 1903 \"head corn\" and \"tail corn\" were taken from\nthe same bulks of barley and sown in pots. The yields in grammes were as follows:\nLarge seed . . .\n13.9\n7.3\nSmall seed . . .\n14.4\n1.4\n+0.5\n+1.4\nThe average gain is thus 0.95 and the S.D. 0.45, giving z = 2.1. Now the table for\nn = 2 is not given, but if we look up the angle whose tangent is 2.1 in Chambers's\ntables,\ntan-1 2.1\n64*39′\np =\n+ 0.5 =\n= 0.859,\n180*\n180*\nso that the odds are about 6 to 1 that small corn gives a better yield than large. These\nodds++++ are those which would be laid, and laid rigidly, by a man whose only knowledge\nof the matter was contained in the two experiments. Anyone conversant with pot culture\nwould however know that the difference between the two results would generally be\ngreater and would correspondingly moderate the certainty of his conclusion. In point\nof fact a large-scale experiment confirmed this result, the small corn yielding shout\n15% more than the large.\nI will conclude with an example which comes beyond the range of the tables, there\nbeing eleven experiments.\nTo test whether it is of advantage to kiln-dry barley seed before sowing, seven\nvarieties of barley wore sown (both kiln-dried and not kiln-dried in 1899 and four in\n1900; the results are given in the table.\n++++[Through a numerical slip, now corrected, Student had given the odds as 33 to 1 and it is to this figure\nthat the remarks in this paragraph relate.\n\nLb. head corn per acre\nPrice of head corn in\nCwt. straw per acre\nValue of crop per acre\nshillings per quarter\nin shillings\nN.K.D. N.D.\nDiff.\nN.K.D. N.D.\nDiff.\nN.K.D. N.D.\nDiff.\nN.K.D. N.D.\nDiff.\n+106\n+5\n+11\n-1\n+1\n-7\n+2\n-\n+101\n-1\n+1\n-1\n-\n+5\n-5\n-\n+2\n+\n-36\n-2\n-\n-3\n-\n+\n-13\n-\n+\n-1\n- 1\n'117\n+\n-7\n-\n-\n+127\n-1\n- 1\n+1\n+7\n+ 24\n+\n+\nAverage 1841.5 1875.2 +33.7\n28.45 27.55\n19.95 21.05 +1.10 145.82 144.68\n-0.91\n+1.14\nStandard\n. . .\n. . .\n63.1\n. . .\n. . .\n0.79\n. . .\n. . .\n2.25\n. . .\n. . .\n6.67\ndeviation\nStandard\ndeviation\n. . .\n. . .\n63.1\n. . .\n. . .\n0.79\n. . .\n. . .\n2.25\n. . .\n. . .\n6.67\n√\n÷\nIt will he noticed that the kiln-dried seed gave on an average the larger yield. of\ncorn and straw, but that the quality was almost always inferior. At first sight this might\nbe supposed to be due to superior germinating power in the kiln-dried seed, but my\nfarming friends tell me that the effect of this would be that the kiln-dried seed would\nproduce the better quality barley. Dr Voelcker draws the conclusion: \"In such seasons\nas 1899 and 1900 there is no particular advantage in kiln-drying before mowing.\" Our\nexamination completely justifies this and adds \"and the quality of the resulting barley\nis inferior though the yield may be greater.\"\nIn this case I propose to use the approximation given by the normal curve with\nstandard deviation s/√n -3 and therefore use Sheppard's tables, looking up the dif\nference divided by S/\n√\n8. The probability in the case of yield of corn per acre is given\nby looking up 33.7/22.3 = 1.51 in Sheppard's tables. This gives p = 0.934, or the\nodds are about 14 to 1 that kiln-dried corn gives the higher yield.\nSimilarly 0.91/0.28 = 3.25, corresponding to p = 0.9994,+ so that the odds are\nvery great that kiln-dried seed gives barley of a worse quality than seed which has not\nbeen kiln-dried.\nSimilarly, it is about 11 to 1 that kiln-dried seed gives more straw and about 2 to 1\nthat the total value of the crop is less with kiln-dried seed.\nSECTION X. CONCLUSIONS\n1. A curve has been found representing the frequency distribution of standard\ndeviations of samples drawn from a normal population.\n2. A curve has been found representing the frequency distribution of the means of\nthe such samples, when these values are measured from the mean of the population in\nterms of the standard deviation of the sample.\n+As pointed out in Section V, the normal curve gives too large a value for p when the probability is large.\nI find the true value in this case to be p = 0.9976. It matters little, however, to a conclusion of this kind\nwhether the odds in its favour are 1660 to 1 or merely 416 to 1.\n\n3. It has been shown that the curve represents the facts fairly well even when the\ndistribution of the population is not strictly normal.\n4. Tables are given by which it can be judged whether a series of experiments,\nhowever short, have given a result which conforms to any required standard of accuracy\nor whether it is necessary to continue the investigation.\nFinally I should like to express my thanks to Prof. Karl Pearson, without whose\nconstant advice and criticism this paper could not have been written.\n[Biometrika, 6 (1908), pp. 1-25, reprinted on pp. 11-34 in \"Student's\" Collected Pa\npers, Edited by E. S. Pearson and John Wishart with a Foreword by Launce McMullen,\nCambridge University Press for the Biometrika Trustees, 1942.]"
    },
    {
      "category": "Resource",
      "title": "rec1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/a8fb567d24ffcf750211613a8639701d_rec1.pdf",
      "content": "ESD.86 - Recitation 1\n\nThe Bernoulli Distribution (1)\n- Let's have an experiment that can only have two\noutcomes: \"success\" (labeled n = 1), with probability\np; and \"failure\" (labeled n = 0), with probability q.\n- It therefore has the following probability distribution:\nP(n) = 1-p,_ for _ n = 0\np,_ for _ n =1\n⎧\n⎨\n⎩\nP(n) = pn(1-p)1-n\n\nThe Bernoulli Distribution (2)\n- The Bernoulli distribution function is:\n- Mean: μ = p\n- Variance: σ2 = p (1 - p)\nD(n) = 1-p,_ for _ n = 0\n1,_ for _ n =1\n⎧\n⎨\n⎩\n\nThe Binomial Distribution (1)\n- Start with an experiment that can have only two\noutcomes: \"success\" and \"failure\" or {0, 1} with\nprobabilities p and q, respectively.\n- Consider N \"trials,\" i.e., repetitions of this experiment\nwith constant q (Bernoulli trials)\n- Define a new DRV: X = number of 1's in N trials\n- Sample space of X: {0,1,2,...,N}\n- What is the probability that there will be k 1's\n(failures) in N trials?\n\nThe Binomial Distribution (2)\n- This is the probability mass function of the Binomial\nDistribution.\n- It is the probability of exactly k failures in N\ndemands.\n- The binomial coefficient is:\nPr[X = k] = N\nk\n⎛\n⎝ ⎜\n⎞\n⎠ ⎟ qk(1-q)N-k\nN\nk\n⎛\n⎝ ⎜\n⎞\n⎠ ⎟ =\nN!\nk!(N -k)!\n\nThe Binomial Distribution (3)\n- Mean number of failures: q.N\n- Variance: q.(1 - q).N\n- Normalization:\n- P[at most m failures]:\nN\nk\n⎛\n⎝ ⎜\n⎞\n⎠ ⎟ qk(1-q)N-k =1\nk= 0\nN\n∑\nN\nk\n⎛\n⎝ ⎜\n⎞\n⎠ ⎟ qk(1-q)N-k = F(m)\nk= 0\nm\n∑\n\nThe Geometric Distribution (1)\n- The Geometric distribution is a discrete distribution\nfor n = 0, 1, 2, ... having the following probability\nfunction:\nwhere 0 < p < 1, and q = 1 - p\n- Its distribution function is:\nP(n) = p(1-p)n\nP(n) = pqn\nD(n) =\nP(k) =1-qn+1\nk= 0\nn\n∑\n\nThe Geometric Distribution (2)\n- The geometric distribution is the only discrete\nmemoryless random distribution. It is the discrete\ndistribution of the exponential distribution.\n- Normalization:\n- Mean:\n- Variance:\nσ 2 = 1-p\np2\nμ = 1-p\np\nP(n) =\nqn p = p\nqn =\np\n1-q = p\np =1\nn= 0\ninf\n∑\nn= 0\ninf\n∑\nn= 0\ninf\n∑\n\nAcknowledgements\n- Wolfram's Mathworld:\nhttp://mathworld.wolfram.com/"
    },
    {
      "category": "Resource",
      "title": "rec2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/dde52bf2f00d89d349f39df561bbc4c7_rec2.pdf",
      "content": "ESD: Recitation #2\n\nDefinitions: 1. Expectation\n- Expectation (or population mean):\nE[X] or μ\n- Linearity: E[aX + b] = a.E[X] + b\n- Non-multiplicativity: E[X.Y] = E[X].E[Y]\niff X and Y are independently distributed\n\nDefinitions: 2. Median\n- Median: c\n- The median of a probability distribution\nis the value of c that minimizes\n- The median is not necessarily unique.\n\nDefinitions: 3. Variance\n- Variance: var[X] or σ2\nvar[X] = E[(X !μ)2] = E(X ! E[X])2]\nvar[X] = E[X 2]! (E[X])2\n- Non-linearity:\n\nThe Democrates\nin the US Senate\n- 106th Congress: 45\n- 107th Congress: 50\n- 108th Congress: 48\n- 109th Congress: 45\n- 110th Congress: 51\n\nReturn to the geometric\ndistribution\n- Calculating the expectation:\nE[x] =\nnp(1! p)n\nn= 0\n\"\n#\n= ?\nx n = 1! x n+1\n1! x\nn= 0\nm\n#\nd\ndx\nx n\nn= 0\nm\n#\n$\n%\n&\n'\n(\n) =\nn.x n!1 =\nn= 0\nm\n#\n!(n +1).x n(1! x) + (1! x n+1).1\n1! x\n(\n)\n- For the variance calculation, cf.:\nhttp://www.win.tue.nl/~rnunez/2DI30/NoteOnGeometricDistributi\non/distributions.pdf\n\nz-Transform\nz-transform of pX(x):\npX\nT (z) ! E[zX ] =\nzx pX (x)\nx= 0\n\"\n#\nWhy is it useful? You can use it to determine the\nexpectation and variance of probability distributions:\nE[X] = dpX\nT (z)\ndz\n!\n\" #\n$\n% &\nz=1\nE[X 2] = d2pX\nT (z)\ndz2\n!\n\" #\n$\n% &\nz=1\n+ dpX\nT (z)\ndz\n!\n\" #\n$\n% &\nz=1\n\nZ-Transform (cont'd)\n- For the most common z-transforms, have a look at\nthis website:\nhttp://www.swarthmore.edu/NatSci/echeeve1/Ref/LPSA\n/LaplaceZTable/LaplaceZFuncTable.html\n\nBuffon's Needle\nGeorges-Louis Leclerc,\nComte de Buffon\n(1707-1788)\nProbability treatise\nSur le jeu de franc-carreau\nSource: Wikipedia\n\nThe Experiment\nl ≤ d\nRandom variables:\nφ = angle between\nneedle and stripe\ninterface\ny = distance between needle middle and\nnearest stripe interface\nD\ni\nagram for Buffon's Needle, showing distance D between stripes and length l of randomly placed needle.\n\nJoint sample space\nπ\ny\nd/2\nφ\n\nJoint probability distribution\n- Determine fY,φ(y,φ)\n- fY(y) and fφ(φ) independent, uniformly\ndistributed\n- fY(y) = constant = 2/d\n- fφ(φ) = constant = 1/π\n- fY,φ(y,φ) = fY(y) x fφ(φ) = 2/πd\n\nWorking in the joint sample\nspace\n- Needle crosses stripe interface <=>\n[y - (l/2)sinφ] < 0 or y < (l/2)sinφ\nP =\n!d dy\nl\n2 sin\"\n#\n$\n%\n&\n& &\n'\n(\n)\n) )\n\"\n#\nd\" = l\n!d\nsin\n\"\n#\n\".d\" = 2l\n!d"
    },
    {
      "category": "Resource",
      "title": "rec4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/153419b6f3aa84feb5b5f5068dbf95c4_rec4.pdf",
      "content": "ESD: Recitation #4\n\nBirthday problem\nAn approximate method\n- Bernoulli trials\n-\n-\nbirthday than someone else:\nP0 =\nN\n!\n\" #\n$\n% &\n!\n\" #\n$\n% &\n0 364\n!\n\" #\n$\n% &\nN\n=\n!\n\" #\n$\n% &\nN\n=\n!\n\" #\n$\n% &\nn!\n2(n'2)!\nNumber of trials to compare birthdays of\nall people in the class:\nProbability that nobody has the same\nN =\nn!\n(n ! 2)!2! =\nn!\n2(n ! 2)!\n\nThe exact solution\n- Probability that nobody has the same\nbirthday than anybody else:\nP0 =\n1!\ni\n\"\n# $\n%\n& '\ni= 0\nn!1\n(\n=\n365!\n365n(365 ! n)!\n\nWhat was the average travel\ndistance between two random\npoints in Budapest in the\n1850s?\n\nBudapest = Buda + Pest\nPhoto removed due to copyright restrictions.\nThe Danube River through Budapest, showing the two shores.\n\nSource: Wikipedia\nOnly one bridge: Szechenyi Lanchid (Chain bridge)\n\nwP\nBuda: λB\nPest: λP\nwB\nu\nv\n202 m\nModeling\nlP\nlB\n\nWithin each city\n- In Buda:\nDB = 1\n3(wB + lB)\nPB!B =\nwB.lB.\"B\nwB.lB.\"B + wP.lP.\"P\n#\n$ %\n&\n' (\n- In Pest:\nDP = 1\n3(wP + lP)\nPP!P =\nwP.lP.\"P\nwB.lB.\"B + wP.lP.\"P\n#\n$ %\n&\n' (\n\nBetween the two cities\n- 4 cases:\n(1)\n(2)\n(3)\n(4)\n\nBetween (1) and (3)\n- Probability:\nP(1)!(3) = 2\nwP.lP.\"P.wB.lB.\"B\nwB.lB.\"B + wP.lP.\"P\n(\n)\n2 # u.v\nlP.lB\n- Average Distance:\nD(1)!(3) = 1\n2 (wB + v) + 1\n2 (wP + u) + 202\n\nBetween (1) and (4)\n- Probability:\nP(1)!(4) = 2\nwP.lP.\"P.wB.lB.\"B\nwB.lB.\"B + wP.lP.\"P\n(\n)\n2 # u.(lB ! v)\nlP.lB\n- Average Distance:\nD(1)!(4) = 1\n2 (wP + u) + 1\n2 (wB + (lB ! v)) + 202\n\nAnd continue...\n- Between (2) and (3)\n- Between (2) and (4)\n- Get the final answer...\n\nMore complications\n- There is currently ten bridges on the\nDanube.\n- How does average traveling distance\nchange if we build another one?\n\nBertrand's Paradox\nJoseph Louis Francois\nBertrand\n(1822-1900)\nWrote Calcul des\nprobabilites in 1888.\n\nThe question\n- Consider an equilateral triangle\ninscribed in a circle. Suppose a cord of\nthe circle is chosen at random.\n- What is the probability that the chord is\nlonger than a side of the triangle?\n\nRandom endpoints Equilateral triangle inscribed in a circle. Several chords of random angle originating at one vertex of the triangle.\nEquilateral triangle inscribed in a circle. Several chords of random angle originating at one vertex of the triangle.\n\nRandom radius\nEquilateral triangle inscribed in a circle, with chords of various lengths perpendicular to a radius.\n\nRandom midpoints\nEquilateral triangle inscribed in a circle. Several chords of random length and orientation; if they fall within circle of 1/2 radius, they are longer than the side of triangle.\n\nBarbershop\n- One barber, two chairs for waiting customers.\n- Prospective customers arrive in a Poisson\nmanner at the rate of λ per hour.\n- It takes the barber 1/μ on average to serve a\ncustomer.\n- Prospective customers finding the barbershop\nfull are lost forever.\n- What is the average number of customers?\n\nModel\nλ\nλ\nλ\nμ\nμ\nμ\nλ = 2.μ\n\nSolving (1)\n- What is the probability that N customers\nare in the barbershop?\nP1 = !\nμ P0;P2 = !\nμ P1\nPN = !\nμ\n\"\n# $\n%\n& '\nN\nP0\nPi =1( P0 =\n1) !\nμ\n1) !\nμ\n\"\n# $\n%\n& '\n3+1\ni= 0\n*\n\nSolving (2)\n- Average number of customers:\nPN = !\nμ\n\"\n# $\n%\n& '\n1( !\nμ\n1( !\nμ\n\"\n# $\n%\n& '\n3+1 =\n24 (1\nE[Nb_customers] =\ni.Pi =\ni= 0\n)\n15 * 2.2667"
    },
    {
      "category": "Resource",
      "title": "rec5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/e30eb3412536d15ecd2668c4126f1431_rec5.pdf",
      "content": "ESD: Recitation #5\n\nThe barbershop revisited\nPhoto removed due to copyright restrictions.\n\nInfinite number of waiting seats\n- One barber, infinite number of chairs for\nwaiting customers.\n- Prospective customers arrive in a Poisson\nmanner at the rate of λ per hour.\n- It takes the barber 1/μ on average to serve a\ncustomer (λ = 0.9 x μ).\n- No prospective customer is ever lost.\n- What is the average number of customers?\n\nModel\nλ\nλ\nλ\nλ\n...\nμ\nμ\nμ\nμ\n\nSolving (1)\n- What is the probability that N customers\nare in the barbershop?\nP1 = !\nμ P0;P2 = !\nμ P1\nPN = !\nμ\n\"\n# $\n%\n& '\nN\nP0\nPi =1( P0 =\n!\nμ\n\"\n# $\n%\n& '\nk\nk= 0\n)\n*\ni= 0\n)\n*\n=\n0.9k\nk= 0\n)\n*\n= 1\n10 = 0.1\n\nSolving (2)\n- Average number of customers:\nPN = 0.1! \"\nμ\n#\n$ %\n&\n' (\nN\n= 0.1! 0.9N\nE[Nb_customers] =\nk.Pk =\nk= 0\n)\n*\n0.1!\nk\nk= 0\n)\n* ! 0.9k = 9\n\nDifferent service completion rate\n- One barber, two chairs for waiting customers.\n- Prospective customers arrive in a Poisson\nmanner at the rate of λ per hour.\n- It takes the barber 1/μ on average to serve a\ncustomer. The service completion rate is\ndescribed by a second order Erlang pdf.\nAssume λ = μ.\n- Prospective customers finding the barbershop\nfull are lost forever.\n- What is the average number of customers?\n\nModel\nλ\nλ\nλ\nλ\nλ\n2.μ\n2.μ\n2.μ\n2.μ\n2.μ\n2.μ\nArrival rate: fA(x) = λ.e- λ.x, x ≥ 0\nService rate: fS(x) = 4.μ2.x.e-2.μ.x, x ≥ 0\n\nSolving (1)\n- Steady-state probabilities:\n!.P0 = 2.μ.P1\n(2.μ + !).P1 = 2.μ.P2\n(2.μ + !).P1 = 2.μ.P2\n(2.μ + !).P2 = !.P0 + 2.μ.P3\n(2.μ + !).P3 = !.P1 + 2.μ.P4\n(2.μ + !).P4 = !.P2 + 2.μ.P5\n2.μ.P5 = !.P3 + 2.μ.P6\n2.μ.P6 = !.P4\n\nSolving (2)\n- Calculate P0:\nPk =1! P0 +\nk= 0\n\"\n2 P0 + 3\n4 P0 + 5\n8 P0 + 11\n16 P0 + 21\n32 P0 + 11\n32 P0 = 65\n16 P0\nPk =1! P0 = 16\nk= 0\n\"\n\nSolving (3)\n- Average number of customers:\nE[Nb_customers] = 0 ! 16\n65 +1!\n65 + 12\n\"\n# $\n%\n& '\n+2 !\n13 + 11\n\"\n# $\n%\n& ' + 3!\n130 + 11\n\"\n# $\n%\n& '\nE[Nb_customers] = 22\n13 (1.692\n\nAdditional barber\n- Two barbers:\n- Adam (takes 1/μ1 on average to serve a customer)\n- Ben (takes 1/μ2 on average to serve a customer)\n- One chair for waiting customers.\n- Prospective customers arrive in a Poisson\nmanner at the rate of λ per hour.\n- Prospective customers finding the barbershop\nfull are lost forever\n\nModeling the system\nλ\nλ\n1,0\n2,0\n0,0\nμ1\nμ1\nμ2\nλ\n1,1\n2,1\n0,1\nλ\nλ\nμ2\nμ2\nμ1\nμ1\nState of the system: Si,j\ni: number of people being serviced by or waiting for Adam\nj: number of people being serviced by Ben\n\nThe question\n- Suppose λ = μ1 = μ2.\n- What is the probability that Ben is busy\nat a random time?\n\nSolving (1)\n- Steady-state probabilities:\n!.P0,0 = μ1.P1,0 + μ2.P0,1\n(μ1 + !).P1,0 = !.P0,0 + μ1.P2,0 + μ2.P1,1\n(μ1 + !).P2,0 = !.P1,0 + μ2.P2,1\n(μ1 + μ2).P2,1 = !.P2,0 + !.P1,1\n(μ1 + μ2 + !).P1,1 = !.P0,1 + μ1.P2,1\n(μ2 + !).P0,1 = μ1.P1,1\n\nSolving (2)\n- P{Ben busy} = P0,1 + P1,1 + P2,1\n- Using:\nP0,0 + P1,0 + P2,0 + P2,1 + P1,1 + P0,1 =1\n- We find:\nP{Ben _busy} = 8\n129 + 4\n43 + 22\n129 = 42\n129 ! 0.326"
    },
    {
      "category": "Resource",
      "title": "rec6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/71fd917c457e0fe97cddc02831a33829_rec6.pdf",
      "content": "ESD: Recitation #6\n\nRevisions\n- Four Steps to Happiness\n- Z-transforms and s-transforms\n- Common PMFs and PDFs\n- Poisson processes and random incidence\n- Convolution\n- Sampling problems\n- Spatial models\n- Markov processes and queuing systems\n\nFour Steps to Happiness\n- Define the random variables\n- Identify the joint sample space\n- Determine the probability law over the sample\nspace\n- Work in the sample space to answer any\nquestion of interest\n- Derive the CDF of the RV of interest working in the\noriginal sample space whose probability law you\nknow\n- Take the derivative to obtain the desired PDF\n\nTransforms\n- Z-transform:\n\nNearest neighbor\n- Euclidean distance:\nP{X(circle) = k} = (!.\".r2)k.e#! .\".r 2\nk!\n,$k % &\n- What changes for taxicab distance?\n\nLittle's Law\n- In steady state:\nL = λ.W\nLq = λ.Wq\nW = 1/μ + Wq\nL = Lq + λ/μ\n- Conditions?\n\nTest exercises (1)\n- Police car and accident independently\nand uniformly located on the perimeter\nof a square (1 x 1 km).\nPolice car\nAccident\n\nAround a square\n-\nTravel only possible around the\nsquare.\n1) PDF of travel distance if the police car\ncan make U-turns anywhere?\n2) PDF of travel distance if U-turns are\nimpossible?\n\nSolving\n1) Let us fix X1. X2 is uniformly distributed\nover the sides of the square: Travel\ndistance uniformly distributed between\n0 and 2 km.\n2) Idem, except that travel distance is\nnow uniformly distributed between 0\nand 4 km.\n\nContinued...\n- What if we now have four blocks around\nwhich the accident and the police car\ncan be?\nAccident\nPolice car\n\nTest exercises (2)\n- Consider a small factory that has 3 machines subject\nto breaking down (independently of each other).\n- Whenever a machine breaks down, it is sent to the\nfactory's repair shop, which has two parallel and\nidentical repair stations. Repair is done in a FIFO\norder. The time needed to repair a machine at a\nrepair station has an exponential pdf with:\nE[R] = 2 hours.\n- The time until a repaired machine breaks down again\nhas an exponential pdf with: E[B] = 9 hours.\n- Find the expected number of machines that are\noperating at this factory in steady state.\n\nSmall factory\n- The small factory has 3 machines,\ntherefore the total population is three.\nOur Birth-and-death chain has therefore\nonly a 4 states, that is all machines can\nbe running, one can be broken down,\ntwo can be broken down or all can be\nbroken down.\n\nModeling\n\nSolving (1)\n- Steady-state equations:\n3 P0 = 1\n2 P1\n9 P1 = P2\n9 P2 = P3\nP0 + P1 + P2 + P3 =1\n\nSolving (2)\n- Therefore:\nP0 = 243\nP1 = 162\nP2 = 36\nP3 =\n\nSolving (3)\n- Expected number of machines that are\noperating:\nP{Operating} = 3 - L\n= 3 - (0 x P0 + 1 x P1\n+ 2 x P2 + 3 x P3)\n≈ 2.45"
    },
    {
      "category": "Resource",
      "title": "rec7.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/3f91d73d02b200f2b19ada0b3bcfe528_rec7.pdf",
      "content": "ESD: Recitation #7\n\nAssumptions of classical\nmultiple regression model\n- Linearity\n- Full rank\n- Exogeneity of independent variables\n- Homoscedasticity and non autocorrellation\n- Exogenously generated data\n- Normal distribution\n\nHypothesis Testing\n-\nMethod:\n1.\nFormulate the null hypothesis H0 and the\nalternative hypothesis HA.\n2.\nIdentify a test statistic to assess the truth of the\nnull hypothesis.\n3.\nCompute the P-value, which is the probability\nthat a test statistic as least as significant as the\none observed would be obtained, assuming that\nthe null hypothesis were true.\n4.\nCompare the P-value to an acceptable\nsignificance level, α. If p ≤α, the null hypothesis\nis ruled out.\n\nα levels and P-values\n- Examples of confidence levels:\n- Proportions:\n- Means:\n- Differences between two population proportions (large\nsamples):\n- Differences between two population means (large samples):\n\nOne tailed or two tailed tests\nOne tailed (right)\nOne tailed (left)\nTwo tailed\nH0: μ ≤μo\nH0: μ ≥μo\nH0: μ = μo\nHA:μ > μo\nHA:μ < μo\nHA:μ = μo\nμo is a specified value of the population mean\n- 2 .2\n- 2\n- 1 .8\n- 1 .6\n- 1 .4\n- 1 .2\n- 1\n- 0 .8\n- 0 .6\n- 0 .4\n- 0 .2\n0 .2\n0 .4\n0 .6\n0 .8\n1 .2\n1 .4\n1 .6\n1 .8\n2 .2\n- 2 .2\n- 2\n- 1 .8\n- 1 .6\n- 1 .4\n- 1 .2\n- 1\n- 0 .8\n- 0 .6\n- 0 .4\n- 0 .2\n0 .2\n0 .4\n0 .6\n0 .8\n1 .2\n1 .4\n1 .6\n1 .8\n2 .2\n- 2 .2\n- 2\n- 1 .8\n- 1 .6\n- 1 .4\n- 1 .2\n- 1\n- 0 .8\n- 0 .6\n- 0 .4\n- 0 .2\n0 .2\n0 .4\n0 .6\n0 .8\n1 .2\n1 .4\n1 .6\n1 .8\n2 .2\n\nZ-test\nOne-tailed\n(right)\nOne-tailed\n(left)\nTwo-tailed\nH0:\nHA:\nμ1 ≤μ2\nμ1 > μ2\nμ1 ≥μ2\nμ1 < μ2\nμ1 = μ2\nμ1 =μ2\nTest\nstatistic\nz\nz\nz\nDecision\nRule\nReject if z >zα\nReject if z <\n-zα\nReject if z\n>zα/2 or z < -\nzα/2\n\nZ-test (2)\n- Large sample proportion:\n- Large sample mean:\n- Large sample difference between two\npopulation proportion:\n- Large sample difference between two\npopulation means:\np = po,z =\nx -npo\nnpo(1-po)\nμ = μo,z = x -μo\ns/ n\np1 = p2,or, p1 -p2 = 0,z =\nˆ p 1 -ˆ p 2\nˆ p (1-ˆ p )( 1\nn1\n+ 1\nn2\n)\nμ1 = μ 2,z =\nx 1 -x 2\ns1\nn1\n+ s2\nn 2\n\nT-test\n- Same method\n- Tests:\n- Small sample mean:\n- Small sample difference:\nμ = μo,t = x -μo\ns/ n\nμ1 = μ2,t =\nx 1 -x 2\nsp\nn1\n+ 1\nn2\n(Sp is the pooled standard deviation)\n\nExample: Medical treatment\nIs your treatment more successful than control?\nTest at the 1% significance level\n-\nWhat are we comparing?\n-\nSmall or large sample?\n-\nOne or two tailed?\nTreatment\nControl\nSample mean (%): μ\nSample SD: s\nN\n\nMedical treatment (2)\nComparing means, z test statistics, one tailed (right tailed)\nH0: μt ≤μc\nHA: μt > μc\nTest Statistic: z-test\nDecision Rule: For significance level α = 0.01, reject null\nhypothesis if computed test statistic value:\nz=4.6291 > zα =2.33, p=.000002 (from z-table)\nConclude: reject null hypothesis.\nz =\nμt -μc\nst\nnt\n+ sc\nnc\n\nANOVA\n- ANOVA allows for comparing points\nestimates for more than 2 groups.\n- ANOVA separates the total variability of\noutcome in two categories: variability\nwithin or between groups.\n- H0: same average for each group\nHA: all averages are not the same\n\nANOVA: Method\n1) Calculate variation between groups.\n-\nCompare mean of each group with mean\nof overall sample: between sum of\nsquares (BSS):\n-\nDivide BSS by number of degree of\nfreedom (m-1)\n-\nGet estimate of variation between groups\nBSS =\nni(xi -x)2\ni=1\nm\n∑\n\nANOVA: Method (2)\n2) Calculate variation within groups.\n- Find sum of squared deviation between individual results and\nthe group average, calculating separate measures for each\ngroup\n- Sum the group values: obtain the within sum of squares\n(WSS):\n- Divide WSS by number of degrees of freedom (\n):\nget estimate of variation within groups.\nWSS =\n(ni -1) ×σ i\ni=1\nm\n∑\nni\ni=1\nm\n∑\n⎛\n⎝ ⎜\n⎞\n⎠ ⎟ -1\n\nANOVA: Method (3)\n3) Calculate F-statistics.\n- F = BSS/WSS\n- Compare value with standard table for the\nF distribution: calculate significance level of\nF value\n- If null rejected: use z- and t-tests between\neach pair of groups.\n\nExample: shift productivity\nHo: μmorning= μafternoon= μnight\nThe null indicates that all groups have the same average score and by\nassumption the same standard deviation\nHA: μmorning=μafternoon=μnight\nThe alternative is that all means are not the same\nNote: the alternative is not that all means are different.\nIt is possible that some of the means could be the same, yet if they\nare not all the same, we would reject the null.\n\nShift productivity (2)\nHo: μmorning= μafternoon= μnight\nHA: μmorning =μafternoon=μnight\nscore\nmorning\nafternoon\nnight\nscore\nmorning\nafternoon\nnight\n\nShift productivity (3)\n1.\nBSS = n1 (x1-x)2 + n2 (x2-x)2 + n3 (x3-x)2\n= 313 (4.12-4.15)2 + 340 (3.99-4.15)2 + 297 (4.37-4.15)2\n= 23.36\nBetween Mean Squares = BSS/v = 23.36/2 = 11.69\nAv. Prod.\nSD\n# of workers\nMorning\n4.12\n1.3\n1.3\n1.3\nAfternoon\n3.99\nNight\n4.37\nAverage\n4.15\n\nShift productivity (4)\n2.\nWSS = (n1 -1) SD1\n2 +(n2 -1) SD2\n2+ (η3 -1) SD3\n= (313-1) 1.32 + (340-1) 1.32 + (297-1) 1.32\n= 1600.43\nWithin Mean Squares = WSS/v = 1600.43/947\n= 1.69\n\nShift productivity (5)\n3)\nF = (Between Mean Squares/Within Mean Squares)\n= (11.68/1.69) = 6.91\nCompare value to standard table for the F distribution\nIn this case, significance level is less than .01\nReject the null, students' performance varies\nsignificantly across the three classes\n\nTesting for heteroscedasticity\n- White's test\n- Goldfeld-Quandt test\n- Lagrange Multiplier test\n➔If E[εε|Ω]=σ2.Ω is known: Weighted\nleast squares\n\nMaximum Likelihood Estimation\n- Definition:\n- PDF of a random variable y, conditioned on a set of\nparameters :\n- Joint density of n independent and identically ditributed\nobservations from this process:\n- The joint density is the likelihood function.\n- Maximize with respect to θ:\nf (y1,...,yn |θ) =\nf (yi |θ) = L(\ni=1\nn\n∏\nθ | y)\nθ f (y |θ)\nL(θ | y)\n∂L(θ | y)\n∂θ\n= 0\nand\n∂2L(θ | y)\n∂θ 2\n< 0\n\nMaximum Likelihood Estimation (2)\n- Conditions:\n- Parameter vector identified\n- Properties:\n- Asymptotically efficient\nθ\n⇔∀θ* =θ,L(θ* | y) =L(θ | y)"
    },
    {
      "category": "Resource",
      "title": "fdist_derivation.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/025b6f8a95eaf9fbcd2462fa28249890_fdist_derivation.pdf",
      "content": "m := 4\nn := 20\n\nu := 0 0.1.. 300\n,\nv := 0 0.1.. 30\n,\nm\nn\n-1 - u\n-1 - v\n2 Γ\n⋅\n⎛⎜⎝\nu\n⎞⎟⎠\n⎛⎜⎝\nv\n⎞⎟⎠\n0.05\ndchisq u m\n( , ) 0.04\nfu u\n( ) :=\nfv v\n( ) :=\n⋅\ne\n⋅\ne\n⎛⎜⎝\n⎞⎟⎠\n⎛⎜⎝\n⎞⎟⎠\nm\nn\n2 Γ\nfuv u v\n(\n)\n( ) fv v\n,\nfu\n\nu ⋅\n⋅\nm\n0.03\ndchisq v n\n( , ) 0.02\nn\n( )\n:=\n0.01\nu\n⎛⎜\n⎜\n⎜\n⎜\n⎜⎝\n⎞⎟\n⎟\n⎟\n⎟\n⎟⎠\nm\nm\n-\nu v\n,\nv\nJ(u v)\n,\n:=\nv\nn\nn\nm\nm\n⋅\nx⋅\nu\nm\nv\nx⋅\nx u v)\n(\nu x v)\n(\n:=\n:=\n⋅m\n,\n,\nv\nn\nn\n⎛\n⎜\n⎜\n⎝\n⎞\n⎟\n⎟\n⎠\nv\nJ(x v) :=\n,\nn\nn\nfxy x v\n, )\n( ( ,\n(\nfuv u x v) , v)\n\nJ x\n\nv\n( , )\n:=\n⋅\ninf\n⋅\n:= 0 0.02\n,\n.. 4\n0.8\n0.8\n⌠⎮\n⌡0\n0.6\n0.6\n( )\nfx x 0.4\ndF x m , n)0.4\n( ,\n0.2\n0.2\nx\nx\nn i\n(\n)\n\n∈R\nfx x\n( )\nfxy x v\n, )\n(\nd\n:=\nv\nx\n⌠\n⎮\n⎮\n⌡\ninf\n+\nm n-2\n- r d\nr\n⋅e\nr\nRe m\n( )\n⎞⎟⎠\n, r\nundefined otherwise\n⎛⎜⎝\nm\nn\nlim\nif 0 <\n⋅Re n\n( ) ∨0 < Re\n\nm\n( ) ∧\n+\n+\nΓ\n⋅\n+\nr →"
    }
  ]
}