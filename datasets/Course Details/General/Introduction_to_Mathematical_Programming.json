{
  "course_name": "Introduction to Mathematical Programming",
  "course_description": "This course is an introduction to linear optimization and its extensions emphasizing the underlying mathematical structures, geometrical ideas, algorithms and solutions of practical problems. The topics covered include: formulations, the geometry of linear optimization, duality theory, the simplex method, sensitivity analysis, robust optimization, large scale optimization network flows, solving problems with an exponential number of constraints and the ellipsoid method, interior point methods, semidefinite optimization, solving real world problems problems with computer software, discrete optimization formulations and algorithms.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering",
    "Systems Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Software Design and Engineering",
    "Systems Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1 hour / session\n\nCourse Content\n\nThis course is an introduction to linear optimization and its extensions emphasizing the underlying mathematical structures, geometrical ideas, algorithms and solutions of practical problems. The topics covered include: formulations, the geometry of linear optimization, duality theory, the simplex method, sensitivity analysis, robust optimization, large scale optimization network flows, solving problems with an exponential number of constraints and the ellipsoid method, interior point methods, semidefinite optimization, solving real world problems problems with computer software, discrete optimization formulations and algorithms.\n\nCourse Requirements and Grading\n\nGrades will be determined by performance on the following requirements. Weights are approximate, and class participation is an important tie breaker.\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets\n\n30%\n\nMidterm exam\n\n30%\n\nFinal exam\n\n40%\n\nCalendar\n\nLEC #\n\nTOPICS\n\nFormulations\n\n2-4\n\nGeometry\n\n5-8\n\nSimplex method\n\n9-11\n\nDuality theory\n\nSensitivity analysis\n\nRobust optimization\n\nMidterm\n\n14-15\n\nLarge scale optimization\n\n16-17\n\nNetwork flows\n\nThe Ellipsoid method\n\nProblems with exponentially many constraints\n\n20-22\n\nInterior point methods\n\nSemidefinite optimization\n\n24-25\n\nDiscrete optimization\n\nFinal exam",
  "files": [
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/72668234e7184363ade764713fac5bfc_MIT6_251JF09_lec01.pdf",
      "content": "Formula,tionn: Lcc.\nCtcomct,ry: Lcc. 2-4\nSimplex l\\lcthoi:l: Lcc. 5-8\nThcory: Lc,:.\n.-inalysin: Lei:.\nRobust\nLcc.\nLarge ncalc ol:,timizat,ion: Lcc.\nFlo~is: Lcc. 16-17\nThe Ellipsi:,ii:l methi:,,$: Lcc. 18-19\n1:)oint mcthoi:ls: Lcc. 20-21\nScmii:lcfinitc opt,imizatii:,n: Lcc.\n~\ni\n~\n,\n\nbl~ctc Opt,imizatii:,n: Lcc. 24-2;\nRequirements\n30%)\nMii:ltcrm\n30%)\nFinal\nIml:,ortant\nbrakcr: cont,ribut,ions t,o ,class\nLnc\nCPLEX fi:n si:,lving ol:,timiza,tion problems\nof Optimizatio~l\nLOPS Arisc?\nExamplcs\nFi:,rmulatii:,ns\nStructure of Class\nI\nDuality\nSensitivity\n9-11\nOptimization:\n14-15\nNctmork\nInterior\n. ..\nHomcmorkn:\nExam:\nExam: 40%\ntic\nof\nLecture Outline\nHistory\nWhcrc\nof\n\nOpti~rlization\nFermat,\nmi11 f(r)\nx : scalar\nEuler,\nnlin ~ ( s I , r,\")\ns.t,. (Jk(1.1,.\n. : r,\") = 0\nk = 1,.\n77L\nLagrange Prol:,lcms in in fin it,^ dimcnsii:,ns, iralirulus\nvariat,ions.\nNonlinear Optirrlizatiovl\nThe general\nLinear\nForlnt~lat,ioli\nmininlizc\n31.1 + 1.2\nsul,jcct ti:,\n1.1\n21.1\n1.2 2\n1.1 2\n>\n~ninimizc\nc'z\nsul,jcct ti:,\nAz 2\nz 2 0\nHistory of\n1638: Newton, 1670\nLagrange, 1707\n. . . .\n. .,\n.\nEuler,\nof\n5.1\nproblem\nWhat is\nOptimization?\n6.1\n+ 21.2\n+\n0.1.2\nh\n\nTlie pre-algorit,hmic\nFonrier,\nMct,hoil for\nsynbcm\nlinear inc,:lualit,ics.\nde\nVal1i.e Poussirl simplcx-like mcthoi:l fin ol,jcct,ivc functii:,n wit,h al:,ii:,-\nlute\nvo11 Nenlnann, 1028 game t,hcory, iluality.\nFarkas, Minkowski: Caratl~i.odory,\nFi:,un,Sa,tionn\nTlie lnoderll\nDantzig.\n51ml,lcx mcthoil\nApplicat,ions.\nLarge Siralc Opt,imizat,ion.\nthci:,ry.\nThe cllipsi:,ii:l algi:,rit,hm.\npi,int algi:,rit,hmn.\nScnlidcfinit,c all<\\\n~ ~ t i ~ n i r a t i o n .\nRi:,bust Ol:,timizat,ion.\nLOPS\nApplicabilit,y\nTransportat,ion\ntraffic ci:,nt,ri:,l, Crew schci:luling,\n>Iovcmcnt i:,f Truck Loails\nHistory of LO\n7.1\nperiod\nsolving\nof\nla\nvalues.\n1870-1930\n7.2\nperiod\nGeorge\n1950s\n1960s\n1970s Complexity\n1980s Interior\n1990s\nconic\n2000s\nWhere do\nArise?\n8.1 Wide\n.\nAir\n\nTrar~sportatioll\nDat,a\n11 ~snrchouscs\n.si supply\ni\nm\nti,? i:lcrnnn,S\njth >iarchousc, j =\n9.2.1 Fur~~l~~latiuri\nr i j = numl:,cr i:,f\nt,o send i i j\nSorting\n11 Invest ~r~ev~t\nurlder\nYou have purchnsci:l .5i sharcs i:,f\ni\npricc y i . i\nCl~rrrcnt price\ni\npi\nProblem\n9.1 . plants.\n.\nof ith plant, = . . .\n.\nof\nI . . .\n9.2\nDecision Variables\nunits\nthrough LO\ntaxation\n.\nstock\nat\n= 1,. . . .\n.\nof stock\nis\n\nYou cxpcct\nthe l:,ricc i:,f\ni i:,nc scar fii:,m now will bc ri\nYou ]:,a-\ncal:,ital-gains t,ax\nthc rat?\n,311 any\nt,hc\nt,imc i:,f the sale.\nYou want ti:, raise C ami:,unt\ncrash aft,cr taxes.\nYou\n1%)\nExample: You sell\nshares\nper sharc; you ha,~:c\nthem\n$30 per\nVet\n-\n-\nFive invcstmcnt\n.-i. B. C, D.\n.-i. C, and D\na,~:ailal:,lc\navailablc\ncarns 6% per year.\nC;asli Flowper\nIlivest,ed\nof\npay\nin transaction costs\n1.000\nat $50\nbought\nat\nsharc:\ncash is:\nSO x 1.000\n0.30 x (SO\n30) x 1,000\nthat\nstock\na\nat\nof 30%\ncapital gains at\nchoices\nE\narc\nin 1993.\nB is\nin 1994.\nCash\nS1.OOO.OOOin 1993.\n12.1\nDollar\n\nVariables\n.I,\ninvcst,cd\nY;\nC'~I,../I~:\nami:,unt invcstcd\npcrii:,d\n= 1.\n1.0GCu.~h3 1.00B\n1.i5D + 1.40E\ns.t. .l+ C:+D+ C'udhl 5 1\nCIA.S/L~ B 5 0.3.1+ l . l C + l.OGC'u.~hl\nCil.sI~3 l.OE < l . O l + 0.3A\n1.06C'ash2\n11 l:,roilucts, m raw nlatcrials\nh i : a,~:ailal:,lc\ni .\naij: #\nnlatcrial i proi:luct j needs\nori:lcr ti:, lbc proi:lucc,S.\nFormulat,ion\nrj =\ni:,f pri:,,Suct j pri:,iSucc,S.\nn C qis,i\nj=l\n12.2.1 Decision\n. . . .E: amount\nin\nmillions\nin cash in\nt, t\n2, 3\nmax\n+\n+\n+ +\n+\nManufacturing\n13.1 Data\nunits of material\nunits of\nin\n13.2\n13.2.1 Decision variables\namount\nmax\n\nExparlsiovi\nC;olistraiilt,s\nDt: fc,rccast,ccl clcmani:l fi:n electricit,::\n::car\nEt: cxisti~lg cal:,acity (in\nnvailablc\nc,: ci:,st ti:, l:,roclucc 11\\I\\\\' using\ncapncity\nlit:\nti:, proi:lucc 1MTV using nuclcnr cnpacit,~\nmorc\nnu,rlca,r\nycnri\nNuclear\nInst\n:7cars\nr,: ami:,unt\nci:,al cal:,acity\nlint\nycar\nyt: nmount i:,f\ncapncit,:: I:,rought i:,n line\nycnr\nu:,: tot,nl ci:,al cal:,acity\nycar\nzt: t,otal\ncnpacit,:: in :;car\n~iant,s\nti:,\n>icckl:: night,shift fi:,r\nllurscs\nD , iicmancl fi:n\nj = 1\nT\nEvcry llursc works\nri:,m\nCapacity\n14.1 Data and\nat\nt\noil)\nnt t\ncoal\ncost\nNo\nthan 20%\nCoal plants last 20\nplants\n14.2 Decision Variables\nof\nbrought on\nin\nt.\nin\nt.\nin\nt.\nt.\nScheduling\n15.1 Decision variables\nHospital\nmnkc\nits\nnurses,\n. . .\n5 clays in a\n\nhire mininl~lm nnrnbcr\nnurses\nrj:\nstartiqg t,hcir week i:,n cia-\nManagerrlerlt\nindust,ry\n- Clarricrs only allowc~i ti:,\nircrt,ain ri:,utcs. Hcnirc airlincs\niYi:,rt,h~scst. Eastern, Southwest, ctc.\n- Fares ilct,cr~nincil\nI:,::\nClivil Acrona,utics Boaril\nlbascil\nmileage\nand othcr ci:,st,s (C.4B\nli:,ngcr\nSLII)E 26\nPost Dercgl~lati~n\nanyone ,ran\nanywhcrc\nfarcs ~ictcrminc~i\nI:,y\n(and thc markct)\nManagerrlerlt\nHuge\nanil fixcil cost,s\nVcry li:,m variablc ci:,st,s pcr passenger ($lO/passcngcr i:,r lcss)\ncci:,nomically ci:,mpctitivc cnvironmcnt\nNcar-pcrfcct infi:,rmat,ion and ncgligil:,lc ci:,st\ninfi,rmatii:,n\npcrishal:,lc invcnt,ory\nl\\lult~il~lc\nfarcs\nGoal:\nof\nDecision Variables\n# nurscs\nj\nRevenue\n16.1 The\nDeregulation in\nfly\nsuch as\n(CAB)\non\nno\nexists)\nfly,\ncarrier\nRevenue\nsunk\n.\nStrong\nof\nHighly\nResult:\n\nManagerrlerlt\n11 ilcstinatii:,ns\nirlasscs (for\nRc~cnucs\nT!,\n1.j.;.\nZJ\nCa1:)acitics:\ni = I.\n. T I ; C 0.i. .I = I,\n. I L\nExpected iicnlancls:\nD:)\nForillulatioil\nVariables\nQ,,: Q-class cu~tomers me\n1;,:\nY-class cu~tomers\ni\nj\nzr.,::c2%,\n+v:,Y;,\nManagerrlerlt\nF\\'c cstimat,~ t,hat RVI hns gcncrat,cil\nin,rrcmcntal rcvcnuc fi,r\nAnlcri,ran\nthe\nthree ycnrs ali:,nc.\ni:,nc-time benefit.\nF\\'c c x p r t RM\ngcncrat,c\nlcnst\nmillii:,n nnnually for the fi:,rcsccnl:,lc\n.-is me\ninvest\nthe cnhanccmcnt\nDIV.-i?rlO me cxl:,cct t,o\ncnpt,urc\neven lnrgcr rcvcnuc\nRevenue\n18.1 Data\norigins.\n. I hub\nsimplicity), (2-class. Ti-class\n.\n,\n.\n. .\n. .\n.\n18.2 LO\n18.2.1 Decision\n.\n#of\naccept from i to j\n.\n# of\naccept from\nto\nmaximize\nRevenue\n$1.4 billion in\nAirlines in\nlast\nThis is not a\nto\nat\n$500\nfuture.\ncontinue to\nin\nof\nnn\npremium.\n\nt,o\n1. Define your ilccision variables clearly.\nITrritc ci:,nstraints ani:l ol:,jcctivc funct,ion.\nfor~ril~latiorl?\nfi,rmulation with a\nnuml:,cr\nvariaI:,lcs anil const,raint,s, anil t,hc mat,rix\nsparse.\nproble~ll\nfurlctiorls\n:.S+R\nsl. s2 E ,Y\nf(As1 ( l h ) s ? )\n5 Xf(sl)+ (lPA)f(s2)\n,f\nci:,ncavc\n- (z)\nci:,nvcx.\n0 x 1 the\nLO\n(z)\ndn z\nci,\n.5.t.\n3 b\nMessages\n20.1 How\nformulate?\n2.\nWhat is a good LO\nA\nof\nA is\n21.1 The general\nConvex\nf\n. (z)\nif\nf\npower of\n. For all\n+\nmin f\n= maxi,\n+\nAz\n\n0x1\nthe\nC\nt,j l.cjl\nn . t . Az 2 b\n1x1 = max{s, s ]\nmi11 C\nqiqi\nAx 2 b\nr.i 5 z.?\n\" j 5 z j\n>Iinimizing Picirc~sisc lincnr convex functii:,n ,ran lbc moi:lcllcil I:,y\npower of LO\nmin\nIdea:\n5.t.\nMessage:\nLO\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009"
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/0820cfa3e02c9bbfce75399c24e618de_MIT6_251JF09_lec02.pdf",
      "content": "JJ\nIn\ntro\nduction\nto\nMathematical\nProgramming\nLecture\n\nGeometry\nof\nLinear\nOptimization\nI\n\nOutline\n\nWhat\nis\nthe\ncen\ntral\nproblem\nSlide\n\nStandard\nF\norm\n\nPreliminary\nGeometric\nInsigh\nts\n\nGeometric\nConcepts\nP\nolyhedra\nCorners\n\nEquiv\nalence\nof\nalgebraic\nand\ngeometric\nconcepts\n\nCen\ntral\nProblem\nminim\nize\nsub\nject\nto\nc\nx\na\ni\nx\n\nb\ni\na\ni\nx\n\nb\ni\na\ni\nx\n\nb\ni\nx\nj\n\nx\n\nj\n\ni\n\nM\ni\n\nM\ni\n\nM\nj\n\nN\nj\n\nN\nSlide\n\nStandard\nF\norm\nminim\nize\nsub\nject\nto\nc\nx\nAx\n\nb\nx\n\nSlide\n\nCharacteristics\n\nMinimization\nproblem\n\nEqualit\ny\nconstrain\nts\n\nNonnegativ\ne\nv\nariables\n\nT\nransformations\nmax\nc\nx\na\ni\nx\n\nb\ni\n\nmin\nc\nx\na\ni\nx\n\ns\ni\n\nb\ni\n\ns\ni\n\nSlide\n\na\ni\nx\n\nb\ni\nx\n\nj\n\na\ni\nx\n\ns\ni\n\nb\ni\n\ns\ni\n\nx\nj\n\nx\n+\nj\n\nx\n;\nj\nx\n+\nj\n\nx\n;\nj\n\nExample\nSlide\n\nmaxim\nize\nx\n\nx\nsub\nject\nto\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\n+\nminim\nize\nx\n\nx\n;\n\nx\nsub\nject\nto\nx\n+\n;\n\nx\n\nx\n\ns\n+\n;\nx\n\ns\n\nx\n\nx\n+\n;\n\nx\n\nx\n\nx\n\ns\n\ns\n\nPreliminary\nInsigh\nts\nSlide\n\nminimi\nze\nx\n\nx\nsub\nject\nto\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx2\n- x1 - x2 = - 2\n(1,1)\n1.5\n2x1 + x2 < 3\n< 3\n1.5\n- x1 - x2 = z\n- x1 - x2 = 0\nx1\nc\nx1 + 2x2\nx\n\nx\nx\nx\n\nSlide\n\nSlide\n\nThere\nexists\na\nunique\noptimal\nsolution\n\nThere\nexist\nm\nultiple\noptimal\nsolutions\nin\nthis\ncase\nthe\nset\nof\noptimal\nsolutions\ncan\nb\ne\neither\nb\nounded\nor\nun\nb\nounded\n\nx2\nc = (1,1)\nc = (- 1,- 1)\nc = (1,0)\nc = (0,1)\nx1\n\nThe\noptimal\ncost\nis\n\nand\nno\nfeasible\nsolution\nis\noptimal\n\nThe\nfeasible\nset\nis\nempt\ny\n\nP\nolyhedra\n\nDenitions\nSlide\n\nThe\nset\nfx\nj\na\nx\n\nbg\nis\ncalled\na\nh\nyp\nerplane\n\nThe\nset\nfx\nj\na\nx\n\nbg\nis\ncalled\na\nhalfspace\n\nThe\nin\ntersection\nof\nman\ny\nhalfspaces\nis\ncalled\na\np\nolyhedron\n\nA\np\nolyhedron\nP\nis\na\ncon\nv\nex\nset\nie\nif\nx\ny\n\nP\n\nthen\nx\n\ny\n\nP\n\na 1\na 2\na3\na4\na5\na\na x < b\n'\na x > b\n'\na x = b\n'\na4x =b4\n'\na1x =b1\n'\na2x =b2\n'\na3x =b3\n'\na5x =b5\n'\n(a)\n(b)\n\nCorners\n\nExtreme\nP\noin\nts\nSlide\n\nP\nolyhedron\nP\n\nfx\nj\nAx\n\nbg\n\nx\n\nP\nis\nan\nextreme\np\noin\nt\nof\nP\nif\n\ny\n\nz\n\nP\ny\n\nx\nz\n\nx\n\nx\n\ny\n\nz\n\nu\nw\nv\nz\nx\ny .\n. . .\n. .\nP\n\nV\nertex\nSlide\n\nx\n\nP\nis\na\nv\nertex\nof\nP\nif\n\nc\nx\nis\nthe\nunique\noptim\num\nminimi\nze\nc\ny\nsub\nject\nto\ny\n\nP\n\nBasic\nF\neasible\nSolution\nSlide\n\nP\n\nfx\n\nx\n\nx\n\nj\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\ng\nSlide\n\nP\noin\nts\nABC\n\nconstrain\nts\nactiv\ne\nP\noin\nt\nE\n\nconstrain\nts\nactiv\ne\nsupp\nose\nw\ne\nadd\nx\n\nx\n\nx\n\n.\nw\nx\nP\nc\nc\n{y | c'y = c'w }\n{y | c ' y = c' x }\n.\nx3\nA\nD\nB\nT h e n\n\nh\nyp\nerplanes\nare\ntigh\nt\nbut\nconstrain\nts\nare\nnot\nlinearly\nindep\nenden\nt\nSlide\n\nIn\ntuition\n\na\np\noin\nt\nat\nw hic\nh\nn\ninequalities\nare\ntigh\nt\nand\ncorresp\nonding\nequations\nare\nlinearly\nindep\nenden\nt\nP\n\nfx\n\nn\nj\nAx\n\nbg\n\na\n\na\nm\nro\nws\nof\nA\n\nx\n\nP\n\nI\n\nfi\nj\na\ni\nx\n\nb\ni\ng\nDenition\nx\nis\na\nbasic\nfeasible\nsolution\nif\nsubspace\nspanned\nb\ny\nfa\ni\n\ni\n\nI\ng\nis\n\nn\n\nDegeneracy\nSlide\n\nIf\njI\nj\n\nn\nthen\na\ni\n\ni\n\nI\nare\nlinearly\nindep\nenden\nt\nx\nnondegenerate\n\nx2\nP\n.\nE\n.\n.\n.\n.\nC\nx 1\n\nIf\njI\nj\n\nn\nthen\nthere\nexist\nn\nlinearly\nindep\nenden\nt\nfa\ni\n\ni\n\nI\ng\nx\ndegener\nate\nB\nP\n(a)\n(b)\nA\nC\nE\nD\n\nExample\nSlide\n\nmin\nx\n\nx\nx\nst\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nSlide\n\nEquiv\nalence\nof\ndenitions\nSlide\n\nTheorem\nP\n\nfx\nj\nAx\n\nbg\nLet\nx\n\nP\n\nx\nis\na\nv\nertex\n\nx\nis\nan\nextreme\np\noin\nt\n\nx\nis\na\nBFS\n\nPro\nof\nSlide\n\nV\nertex\n\nextreme\np\noin\nt\nc\n\nc\nx\n\nc\ny\ny\n\nP\nIf\nx\nis\nnot\nan\nextreme\np\noin\nt\ny\n\nz\n\nx\n\nx\n\ny\n\nz\n\nB u t\nc\nx\n\nc\ny\n\nc\nx\n\nc\nz\n\nc\nx\n\nc\ny\n\nc\nz\n\nc\nx\ncon\ntradicti\non\nSlide\n\nExtreme\np\noin\nt\n\nBFS\nSupp\nose\nx\nis\nnot\na\nBFS\n\nLet\nI\n\nfi\ni\nx\n\nb\n\na\ni\ng\nB ut\na\ni\ndo\nnot\n\na\nspan\nall\nof\n\nn\n\nz\n\nn\ni\nz\n\ni\n\nI\nx\nx\nLet\n\nx\n\nz\n\nx\n\nz\na\ni\nx\n\nb\ni\na\n\nb\nx\ni\ni\ni\n\nI\nSlide\n\ni\nx\n\nb\nx\n\nz\n\nb\ni\n\na\ni\nx\n\ni\n\nI\ni\n\na\ni\nz\n\nb\ni\n\na\nfor\n\nsmall\nenough\n\nx\n\nx\n\nP\n\ny\net\nx\n\nx\n+x\n\nx\nnot\nan\nextreme\np\noin\nt\ncon\ntradicti\non\nSlide\n\nBFS\n\nv\nertex\nx\n\nBFS\n\nI\n\nfi\n\na\n\nb\ni\ng\ni\nx\n\ni\n\nI\nLet\nd\ni\n\ni\n\nI\n\nc\n\nd\nA\nm\nThen\nc\nx\n\nd\nAx\n\nP\nP\nP\n\nd\nb\ni\n\nSlide\n\ni\na\ni\nx\na\ni\nx\ni1\ni2I\ni2I\nBut\nx\n\nP\n\na\ni\nx\n\nb\nP\nP\ni\n\nx\n\noptim\num\n\nx\nmin\nc\nx\nx\n\na\ni\nx\n\nb\ni\n\nc\ni2I\ni2I\nx\n\nP\n\nWh\ny\nunique\n\nc\n\nEqualit\ny\nh o ld s\nif\na\ni\nx\n\nb\ni\n\ni\n\nI\n\nsin ce\na\ni\nspans\n\nn\n\na\ni\nx\n\nb\ni\nhas\na\nunique\nsolution\nx\n\nx\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/a09d067ed9373183e052759f2dc4a4cf_MIT6_251JF09_lec03.pdf",
      "content": "JJ\nIn\ntro\nduction\nto\nMathematical\nProgramming\nLecture\n\nGeometry\nof\nLinear\nOptimization\nI\nI\n\nOutline\nSlide\n\nBFS\nfor\nstandard\nform\np\nolyhedra\n\nDeep\ner\nunderstanding\nof\ndegeneracy\n\nExistence\nof\nextreme\np\noin\nts\n\nOptimalit\ny\nof\nExtreme\nP\noin\nts\n\nRepresen\ntation\nof\nP\nolyhedra\n\nBFS\nfor\nstandard\nform\np\nolyhedra\nSlide\n\nAx\n\nb\nand\nx\n\nm\n\nn\nmatrix\nA\nhas\nlinearly\nindep\nenden\nt\nr o\nws\n\nx\n\nn\nis\na\nbasic\nsolution\nif\nand\nonly\nif\nAx\n\nb\n\nand\nthere\nexist\nindices\nB\n\nB\nm\nsu c\nh\nthat\n\nThe\ncolumns\nA\nB\n(1)\n\nA\nB\n(m)\nare\nlinearly\nindep\nenden\nt\n\nIf\ni\n\nB\n\nB\nm\nthen\nx\ni\n\nConstruction\nof\nBFS\nSlide\n\nPro\ncedure\nfor\nconstructing\nbasic\nsolutions\n\nCho\nose\nm\nlinearly\nindep\nenden\nt\ncolumns\nA\nB\n(1)\n\nA\nB\n(m)\n\nLet\nx\ni\n\nfor\nall\ni\n\nB\nm\n\nB\n\nSolv\ne\nAx\n\nb\nfor\nx\nB\n(1)\n\nx\nB\n(m)\nAx\n\nb\n\nB\nx\nB\n\nN\nx\nN\n\nb\nx\nN\n\nx\nB\n\nB\n;1\nb\n\nExample\n\nSlide\n\nx\n\nA\n\nA\n\nA\n\nA\nbasic\ncolumns\n\nSolution\nx\n\na\nBFS\n\nAnother\nbasis\nA\n\nA\n\nA\n\nA\nbasic\ncolumns\n\nSolution\nx\n\nnot\na\nBFS\n\nGeometric\nin\ntuition\n\nExample\n\nSlide\n\nb\nA 1\nA 2\nA3\nA4 = - A1\nx\nx\nx\nSlide\n\nGeneral\nform\nSlide\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nStandard\nform\nx\nx\nx\n\nx\n\nx\n\ns\n\ns\n\ns\nx\n\nx\n\ns\n\nx\n\nx\n\ns\n\ns\n\nx\nSlide\n\nUsing\nthe\ndenition\nfor\nBFS\nin\np\nolyhedra\nin\ngeneral\nform\n\nx\nx\n\nx\n\nx\n\nCho\nose\ntigh\nt\nconstrain\nts\nx\n\nChec\nk\ni f\n\nA\n\nA\n\nspan\n\nthey\ndo\nA\n\nSlide\n\nUsing\nthe\ndenition\nfor\nBFS\nin\np\nolyhedra\nin\nstandard\nform\n\nPic\nk\nthe\nbasic\nv\nariables\nx\n\nx\n\ns\n\ns\n\nx\nB\n\nx\n\nx\n\ns\n\ns\n\nPic\nk\nthe\nnon\nbasic\nv\nariables\nx\n\ns\n\ns\n\nx\nN\n\nx\n\ns\n\ns\n\nSlide\n\nP\nartition\nA\nx\nx\nx\ns\ns\ns\ns\n\nA\n\nB\n\nN\n\nSlide\n\nB\n\nN\n\nB\nnonsingular\n\nx\n\nB\nC\nB\nC\nx\nN\n\nx\nB\n\nB\n;1\nb\n\nB\nx\nC\n\nB\n\nC\n\nA\n\nA\ns\n\ns\n\nDegeneracy\nfor\nstandard\nform\np\nolyhedra\n\nDenition\nSlide\n\nA\nBFS\nx\nof\nP\n\nfx\n\nn\n\nAx\n\nb\nA\n\nn\n\nn\nx\n\ng\nis\ncalled\ndegenerate\nif\nit\ncon\ntains\nmore\nthan\nn\n\nm\nzeros\n\nx\nis\nnondegenerate\nif\nit\ncon\ntains\nexactly\nn\n\nm\nzeros\n\nExample\n\nrevisited\nSlide\n\nIn\nprevious\nexample\n\ndegenerate\n\nn\n\nm\n\nMore\nthan\nn\n\nm\n\nzeros\n\nAm\nbiguit\ny\nab\nout\nwhic\nh\nare\nbasic\nv\nariables\n\nx\n\nx\n\nx\n\nx\n\none\nc\nhoice\nx\n\nx\n\nx\n\nx\n\nanother\nc\nhoice\n\nExtreme\np\noin\nts\nand\nBFS\nSlide\n\nConsider\nagain\nthe\nextreme\np\noin\nt\n\nHo\nw\nd o\nw\ne\nconstruct\nthe\nbasis\n\nB\n\nA\n\nA\n\nA\n\nA\nA\nA\nSlide\n\nColumns\nin\nB\nare\nlinearly\nindep\nenden\nt\n\nRank\nA\n\njB\nj\n\nCan\nw\ne\naugmen\nt\nB\n\nChoices\n\nB\n\nB\n\nf A\ng\nbasic\nv\nariables\nx\n\nx\n\nx\n\nx\n\nB\n\nB\n\nf A\ng\nbasic\nv\nariables\nx\n\nx\n\nx\n\nx\n\nHo\nw\nman\ny\nc\nhoices\ndo\nw\ne\nh a\nv\ne\n\nDegeneracy\nand\ngeometry\nSlide\n\nWhether\na\nBFS\nis\ndegenerate\nma\ny\ndep\nend\non\nthe\nparticular\nrepresen\ntation\nof\na\np\nolyhedron\nn\no\n\nP\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nn\n\nm\n\ndegenerate\n\na n d\nn\n\nm\n\nis\nnondegenerate\nwhile\n\nis\n\nConsider\nthe\nrepresen\ntation\nP\n\nn\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\nx\n\no\n\nis\nno\nw\nnondegenerate\n\nx\n\nx\n\nx\n\nConclusions\n\nAn\nextreme\np\noin\nt\ncorresp\nonds\nto\np\nossibly\nman\ny\nbases\nin\nthe\npresence\nof\ndegeneracy\n\nSlide\n\nA\nbasic\nfeasible\nsolution\nho\nw\nev\ner\ncorresp\nonds\nto\na\nunique\nextreme\np\noin\nt\n\nDegeneracy\nis\nnot\na\npurely\ngeometric\nprop\nert\ny\n\nExistence\nof\nextreme\np\noin\nts\nSlide\n\nP\nQ\nNote\nthat\nP\n\nfx\n\nx\n\nx\n\ng\ndo\nes\nnot\nha\nv\ne\nan\nextreme\np\noin\nt\nwhile\nP\n\nfx\n\nx\n\nx\n\nx\n\nx\n\nx\n\ng\nhas\none\nWh\ny\n\nDenition\nSlide\n\nA\np\nolyhedron\nP\n\nn\ncon\ntains\na\nline\nif\nthere\nexists\na\nv\nector\nx\n\nP\nand\na\nnonzero\nv\nector\nd\n\nn\nsuc\nh\nthat\nx\n\nd\n\nP\nfor\nall\nscalars\n\nTheorem\nSlide\n\nSupp\nose\nthat\nthe\np\nolyhedron\nP\n\nfx\n\nn\nj\na\ni\nx\n\nb\ni\n\ni\n\nm g\nis\nnonempt\ny\n\nThen\nthe\nfollo\nwing\nare\nequiv\nalen\nt\na\nThe\np\nolyhedron\nP\nhas\nat\nleast\none\nextreme\np\noin\nt\nb\nThe\np\nolyhedron\nP\ndo\nes\nnot\ncon\ntain\na\nline\nc\nThere\nexist\nn\nv\nectors\nout\nof\nthe\nfamily\na\n\na\nm\n\nwhic\nh\nare\nlinearly\nindep\nenden\nt\n\nCorollary\nSlide\n\nP\nolyhedra\nin\nstandard\nform\ncon\ntain\nan\nextreme\np\noin\nt\n\nBounded\np\nolyhedra\ncon\ntain\nan\nextreme\np\noin\nt\n\nPro\nof\nSlide\n\nLet\nP\n\nfx\nj\nAx\n\nb\n\nx\n\ng\n\nrankA\n\nm\nIf\nthere\nexists\na\nfeasible\nsolution\nin\nP\n\nthen\nthere\nis\nan\nextreme\np\noin\nt\nPro\nof\n\nLet\nx\n\nx\n\nx\nt\n\ns t\nx\n\nP\n\nConsider\nB\n\nfA\n\nA\n\nA\nt\ng\n\nIf\nfA\n\nA\n\nA\nt\ng\nare\nlinearly\nindep\nenden\nt\nw\ne\ncan\naugmen\nt\nto\nnd\na\nbasis\nand\nth\nus\na\nBFS\nexists\n\nIf\nfA\n\nA\n\nA\nt\ng\nare\ndep\nenden\nt\nd\nA\n\nd\nt\nA\nt\n\nd\ni\n\nBut\nx\nA\n\nx\nt\nA\nt\n\nb\n\nx\n\nd\nA\n\nx\nt\n\nd\nt\nA\nt\n\nb\n\nx\nj\n\nd\nj\nj\n\nt\n\nConsider\nx\nj\n\notherwise\nSlide\n\nClearly\nA\n\nx\n\nb\nn\no\n\nmax\nx\nj\nLet\n\nd\nj\n\nd\nj\nif\nall\nd\nj\n\nn\no\nmin\nx\nj\n�\n\nd\nj\n\nd\nj\nif\nall\nd\nj\n\n�\n\nF\nor\n\nsucien\ntly\nsmall\nx\n\nSlide\n\nSince\nat\nleast\none\nd\n\nd\nt\n\nis\nnite\nsa\ny\n\nat\nleast\none\nfrom\n\nBut\nthen\nx\n\na n d\nn\num\nb\ner\nof\nnonzeros\ndecreased\nx\nj\n\nd\nj\n\nx\nj\n\nd\nj\n\nExample\n\nSlide\n\nP\n\nfx\nj\nx\n\nx\n\nx\n\nx\nx\n\nx\n\nx\n\ng\n\nx\n\nB\n\nSlide\n\nConsider\nx\n\nfor\n\nx\n\nP\n\nNote\nx\n\nand\nx\n\nOptimalit\ny\no f\nExtreme\nP\noin\nts\n\nTheorem\nSlide\n\nConsider\nmin\nc\nx\nst\nx\n\nP\n\nfx\n\nn\nj\nAx\n\nbg\n\nP\nhas\nno\nline\nand\nit\nhas\nan\noptimal\nsolution\nThen\nthere\nexists\nan\noptimal\nsolution\nwhic\nh\nis\nan\nextreme\np\noin\nt\no f\nP\n\nPro\nof\nSlide\n\nv\noptimal\nv\nalue\nof\nthe\ncost\nc\nx\n\nQ\n\nset\nof\noptimal\nsolutions\ni e\nQ\n\nfx\nj\nc\nx\n\nv\n\nAx\n\nbg\n\nQ\n\nP\nand\nP\ncon\ntains\nno\nlines\nQ\ndo\nes\nnot\ncon\ntain\nan\ny\nlines\nhence\nis\nhas\nan\nextreme\np\noin\nt\nx\n\nSlide\n\nClaim\n\nx\n\nis\nan\nextreme\np\noin\nt\no f\nP\n\nSupp\nose\nnot\n\ny\n\nw\n\nx\n\nx\n\ny\n\nw\n\ny\n\nw\n\nP\n\nv\n\nc\nx\n\nc\ny\n\nc\nw\nc\ny\n\nv\n\nc\ny\n\nc\nw\n\nv\n\ny\n\nw\n\nQ\nc\nw\n\nv\n\nx\n\nis\nNOT\nan\nextreme\np\noin\nt\no f\nQ\nCONTRADICTION\n\nRepresen\ntation\nof\nP\nolyhedra\n\nTheorem\nSlide\n\nA\nnonempt\ny\nand\nb\nounded\np\nolyhedron\nis\nthe\ncon\nv\nex\nh\null\nof\nits\nextreme\np\noin\nts\ny.\na'i*x = bi*\nP\nQ\n.\n.\nz\nu\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/7b9fce7a721cbd6edd8a9e7e0c354057_MIT6_251JF09_lec04.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 4: Geometry of Linear Optimization III\n\nn\n\no\nOutline\nSlide 1\n1. Projections of Polyhedra\n2. Fourier-Motzkin Elimination Algorithm\n3. Optimality Conditions\nProjections of polyhedra\nSlide 2\n- πk : Rn 7→Rk projects x onto its first k coordinates:\nπk (x) = πk(x1, . . . , xn) = (x1, . . . , xk ).\n-\nΠk(S) =\nπk(x) | x ∈ S ;\nEquivalently\nΠk(S) =\n(x1, . . . , xk)\nthere exist xk+1, . . . , xn\ns.t. (x1, . . . , xn) ∈ S .\nx1\nx2\nx3\nP 1( S )\nP 2( S )\n2.1\nThe Elimination Algorithm\n2.1.1\nBy example\nSlide 3\n- Consider the polyhedron\nx1 + x2 ≥ 1\n\nP\nX\nx1 + x2 + 2x3 ≥ 2\n2x1 + 3x3 ≥ 3\nx1 -4x3 ≥ 4\n-2x1 + x2 -x3 ≥ 5.\n- We rewrite these constraints\n0 ≥ 1 -x1 -x2\nx3 ≥ 1 -(x1/2) -(x2/2)\nx3 ≥ 1 -(2x1/3)\n-1 + (x1/4) ≥ x3\n-5 -2x1 + x2 ≥ x3.\n- Eliminate variable x3, obtaing polyhedron Q\n0 ≥ 1 -x1 -x2\n-1 + x1/4 ≥ 1 -(x1/2) -(x2/2)\n-1 + x1/4 ≥ 1 -(2x1/3)\n-5 -2x1 + x2 ≥ 1 -(x1/2) -(x2/2)\n-5 -2x1 + x2 ≥ 1 -(2x1/3).\n2.2\nThe Elimination Algorithm\nSlide 4\n1. Rewrite\nj\nn\n=1 aij xj ≥ bi in the form\nn-1\nainxn ≥-\naij xj + bi,\ni = 1, . . . , m;\nj=1\nif ain =6\n0, divide both sides by ain. By letting x = (x1, . . . , xn-1) that P\nis represented by:\n′\nxn ≥ di + f ix,\nif ain > 0,\n′\ndj + f j x ≥ xn,\nif ajn < 0,\n′\n0 ≥ dk + f kx,\nif akn = 0.\n2. Let Q be the polyhedron in Rn-1 defined by:\n′\n′\ndj + f j x ≥ di + f ix,\nif ain > 0 and ajn < 0,\n′\n0 ≥ dk + f kx,\nif akn = 0.\nTheorem:\nThe polyhedron Q constructed by the elimination algorithm is equal to the\nprojection Πn-1(P) of P.\n\n2.3\nImplications\n- Let P ⊂Rn+k be a polyhedron. Then, the set\nx ∈Rn there exists y ∈Rk such that (x, y) ∈ P\nis also a polyhedron.\n- Let P ⊂Rn be a polyhedron and let A be an m × n matrix. Then, the\nset Q = {Ax | x ∈ P} is also a polyhedron.\n- The convex hull of a finite number of vectors is a polyhedron.\n2.4\nAlgorithm for LO\n′\n- Consider min c x subject to x ∈ P.\n′\n- Define a new variable x0 and introduce the constraint x0 = c x.\n- Apply the elimination algorithm n times to eliminate the variables x1, . . . , xn\n- We are left with the set\n′\nQ =\nx0 | there exists x ∈ P such that x0 = c x ,\nand the optimal cost is equal to the smallest element of Q.\nOptimality Conditions\n3.1\nFeasible directions\n- We are at x ∈ P and we contemplate moving away from x, in the direction\nof a vector d ∈Rn.\n- We need to consider those choices of d that do not immediately take us\noutside the feasible set.\n- A vector d ∈Rn is said to be a feasible direction at x, if there exists a\npositive scalar θ for which x + θd ∈ P.\nSlide 5\nSlide 6\nSlide 7\nSlide 8\n\nP\nP\nP\nP\nP\nSlide 9\n- x be a BFS to the standard form problem corresponding to a basis B.\n- xi = 0, i ∈ N, xB = B-1B.\n- We consider moving away from x, to a new vector x + θd, by selecting a\nnonbasic variable xj and increasing it to a positive value θ, while keeping\nthe remaining nonbasic variables at zero.\n- Algebraically, dj = 1, and di = 0 for every nonbasic index i other than j.\n- The vector xB of basic variables changes to xB + θdB .\n- Feasibility: A(x + θd) = B ⇒ Ad = 0.\n- 0 = Ad = Pn\ni=1 Aidi = Pm\ni=1 AB(i)dB(i) + Aj = BdB + Aj ⇒ dB =\n-B-1Aj .\n- Nonnegativity constraints?\n- If x nondegenerate, xB > 0; thus xB + θdB ≥ 0 for θ is sufficiently\nsmall.\n- If xdegenerate, then d is not always a feasible direction. Why?\n- Effects in cost?\n′\nCost change: c ′ d = cj -cB B-1Aj This quantity is called reduced cost\ncj of the variable xj .\n3.2\nTheorem\nSlide 10\n- x BFS associated with basis B\n- c reduced costs\nThen\n- If c ≥ 0 ⇒ x optimal\n- x optimal and non-degenerate ⇒ c ≥ 0\n3.3\nProof\n- y arbitrary feasible solution\n- d = y -x ⇒ Ax = Ay = b ⇒ Ad = 0\n⇒ BdB +\ni∈N\n⇒ dB = -\ni∈N\n⇒ c ′ d = c ′\nB\nSlide 11\nAidi = 0\nB-1Aidi\ndB +\ncidi\ni∈N\nSlide 12\n′\n=\n(ci -cB B-1Ai)di =\ncidi\ni∈N\ni∈N\n\n- Since yi ≥ 0 and xi = 0, i ∈ N, then di = yi -xi ≥ 0, i ∈ N\n- c ′ d = c ′ (y -x) ≥ 0 ⇒ c ′ y ≥ c ′ x\n⇒ x optimal\n(b) Your turn\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/4fdf1a06497288c24593f7eedef7576e_MIT6_251JF09_lec05.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 5: The Simplex Method I\n\nOutline\nSlide 1\n- Reduced Costs\n- Optimality conditions\n- Improving the cost\n- Unboundness\n- The Simplex algorithm\n- The Simplex algorithm on degenerate problems\nMatrix View\nSlide 2\n′\nmin\nc x\ns.t.\nAx = b\nx ≥ 0\nx = (xB , xN )\nxB basic variables\nxN non-basic variables\nA = [B, N]\nAx = b ⇒\nB · xB + N · xN = b\n⇒ xB + B-1NxN = B-1b\n⇒ xB = B-1b -B-1NxN\n2.1\nReduced Costs\nSlide 3\n′\n′\nz\n= cB xB + cN xN\n= c ′\nB (B-1b -B-1NxN ) + c ′\nN xN\n= c ′ B-1b + (c ′ -c ′ B-1N )xN\nB\nN\nB\ncj = cj -c ′\nB B-1Aj\nreduced cost\n2.2\nOptimality Conditions\nSlide 4\nRecall Theorem:\n- x BFS associated with basis B\n- c reduced costs\nThen\n- If c ≥ 0 ⇒ x optimal\n- x optimal and non-degenerate ⇒ c ≥ 0\n\nImproving the Cost\nSlide 5\n- Suppose cj = cj -cB\n′ B-1Aj < 0\nCan we improve the cost?\n- Let dB = -B-1Aj\ndj = 1, di = 0, i 6\nB(1), . . . , B(m), j.\n=\n- Let y = x + θ · d,\nθ > 0 scalar\nSlide 6\n′\n′\nc y -c x =\nθ · c ′ d\n′\n=\nθ · (cB dB + cj dj )\n=\nθ · (cj -c ′ B-1Aj )\nB\n=\nθ · cj\nThus, if cj < 0 cost will decrease.\nUnboundness\nSlide 7\n- Is y = x + θ · d feasible?\nSince Ad = 0 ⇒ Ay = Ax = b\n- y ≥ 0 ?\nIf d ≥ 0 ⇒ x + θ · d ≥0 ∀ θ ≥ 0\n⇒ objective unbounded.\nImprovement\nSlide 8\nIf di < 0, then\nxi\nxi + θdi ≥0 ⇒ θ ≤- di\n⇒ θ ∗ =\nmin\n- xi\n{i|di<0}\ndi\n⇒ θ ∗ =\nmin\n- xB(i)\n{i=1,...,m|dB(i) <0}\ndB(i)\n5.1\nExample\nSlide 9\nmin\nx1+\n5x2\n-2x3\ns.t.\nx1+\nx2+\nx3\n≤ 4\nx1\n≤ 2\nx3\n≤ 3\n3x2+\nx3\n≤ 6\nx1,\nx2,\nx3\n≥ 0\n\nx3\nx1\n(0,0,3)\n(1,0,3)\n(2,0,2)\n(2,2,0)\n(0,2,0)\n(0,1,3)\nSlide 10\nAx\n2 A2\nA3\nA4\nA5\nA6\nA7\nx1\nSlide 11\n\nx2\n\nx3\n\nx4 =\n\nx5\n\nx6\nx7\nB = [A1, A3, A6, A7]\nBFS: x = (2, 0, 2, 0, 0, 1, 4) ′\nSlide 12\n\n′\nB = 1\n,\nB-1 =\n-1\nc = (0, 7, 0, 2, -3, 0, 0)\n-1\n-1\n\nd1\n-1\n\nd5 = 1, d2 = d4 = 0,\nd3 = -B-1A5 =\nSlide 13\nd6\n-1\nd7\n-1\n′\n′\ny = x + θd ′ = (2 -θ, 0, 2 + θ, 0, θ, 1 -θ, 4 -θ)\nWhat happens as θ increases?\nθ ∗ = min{i=1,...,m|dB(i)<0}\n-\nxB\ndi\n(i)\n=\nmin -(-\n1) , -(-\n1) , -(-\n1)\n= 1.\nl = 6 (A6 exits the basis).\nNew solution\ny = (1, 0, 3, 0, 1, 0, 3) ′\nSlide 14\nNew basis B = (A1, A3, A5, A7)\nSlide 15\n\nx3\nx1\n(0,0,3)\n(1,0,3)\n(2,0,2)\n(2,2,0)\n(0,2,0)\n(0,1,3)\n\n-1\nx21\n-1\n\nB =\n, B\n=\n\n-1\n-1\n′\n′\n′\n-1\nc = c -c B\nA = (0, 4, 0, -1, 0, 3, 0)\nB\nNeed to continue, column A4 enters the basis.\nCorrectness\nSlide 16\n- xB(l) =\nmin\n- xB(i)\n= θ ∗\ndB(l)\ni=1,...,m,dB(i)<0\ndB(i)\nTheorem\n- B = {AB(i) ,i6=l, Aj } basis\n- y = x + θ ∗d is a BFS associated with basis B.\nThe Simplex Algorithm\nSlide 17\n1. Start with basis B = [AB(1), . . . , AB(m)]\nand a BFS x.\n2. Compute cj = cj -cB\n′ B-1Aj\n- If cj ≥ 0; x optimal; stop.\n- Else select j : cj < 0.\n\nSlide 18\n3. Compute u = -d = B-1Aj .\n- If u ≤0 ⇒ cost unbounded; stop\n- Else\n4. θ ∗ =\nmin\nxB(i) = uB(l)\n1≤i≤m,ui >0\nui\nul\n5. Form a new basis by replacing AB(l) with Aj .\n6. yj = θ ∗\nyB(i) = xB(i) -θ ∗ ui\n7.1\nFinite Convergence\nSlide 19\nTheorem:\n- P = {x | Ax = b,\nx ≥ 0} 6\n∅\n=\n- Every BFS non-degenerate\nThen\n- Simplex method terminates after a finite number of iterations\n- At termination, we have optimal basis B or we have a direction d : Ad =\n0, d ≥0, c ′ d < 0 and optimal cost is -inf.\n7.2\nDegenerate problems\nSlide 20\n- θ ∗ can equal zero (why?) ⇒ y = x, although B 6= B.\n- Even if θ ∗ > 0, there might be a tie\nxB(i)\nmin\n⇒\n1≤i≤m,ui >0\nui\nnext BFS degenerate.\n- Finite termination not guaranteed; cycling is possible.\nSlide 21\n-\nx\ny\nf\ng\nh\ng\n.\nx2 =0\nx3 =0\nx4 =0\nx5 =0\nx6 =0\nx1 =0\nc\n\n7.3\nPivot Selection\nSlide 22\n- Choices for the entering column:\n(a) Choose a column Aj , with cj < 0, whose reduced cost is the most\nnegative.\n(b) Choose a column with cj < 0 for which the corresponding cost de\ncrease θ ∗|cj | is largest.\n- Choices for the exiting column:\nsmallest subscript rule: out of all variables eligible to exit the basis, choose\none with the smallest subscript.\n7.4\nAvoiding Cycling\nSlide 23\n- Cycling can be avoided by carefully selecting which variables enter and\nexit the basis.\n- Example: among all variables cj < 0, pick the smallest subscript;\namong all variables eligible to exit the basis, pick the one with the smallest\nsubscript.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/2ef2f1dd7045b5f29e5faea299fb0798_MIT6_251JF09_lec06.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 6: The Simplex Method II\n\nOutline\nSlide 1\n- Revised Simplex method\n- The full tableau implementation\n- Anticycling\nRevised Simplex\nSlide 2\nInitial data: A, b, c\n1. Start with basis B = [AB(1), . . . , AB(m)]\nand B-1 .\n2. Compute p ′ = c ′\nBB-1\ncj = cj -p ′ Aj\n- If cj ≥ 0; x optimal; stop.\n- Else select j : cj < 0.\nSlide 3\n3. Compute u = B-1Aj.\n- If u ≤ 0 ⇒ cost unbounded; stop\n- Else\n4. θ ∗ =\nmin\nxB(i) = uB(l)\n1≤i≤m,ui >0\nui\nul\n5. Form a new basis B by replacing AB(l) with Aj.\n6. yj = θ∗ , yB(i) = xB(i) -θ∗ ui\nSlide 4\n7. Form [B-1|u]\n8. Add to each one of its rows a multiple of the lth row in order to make the\nlast column equal to the unit vector el.\n-1\nThe first m columns is B\n.\n2.1\nExample\nSlide 5\nmin\nx1+\n5x2\n-2x3\ns.t.\nx1+\nx2+\nx3\n≤ 4\nx1\n≤ 2\nx3\n≤ 3\n3x2+\nx3\n≤ 6\nx1,\nx2,\nx3\n≥ 0\nSlide 6\n\nB = {A1, A3, A6, A7},\nBFS: x = (2, 0, 2, 0, 0, 1, 4) ′\n′ c = (0, 7, 0, 2, -3, 0, 0)\n\nB-1\n\n-1\nB =\n,\n=\n-1\n-1\n(u1, u3, u6, u7) ′ = B-1A5 = (1, -1, 1, 1) ′\nθ∗ = min\n1 , 1\n1 , 4\n1 = 1,\nl = 6\nl = 6 (A6 exits the basis).\nSlide 7\n\n[B-1|u] =\n-1\n-1\n-1\n-1\n\n-1\n-1\n\n⇒ B\n= -1\n-1\n2.2\nPractical issues\nSlide 8\n- Numerical Stability\nB-1 needs to be computed from scratch once in a while, as errors accu\nmulate\n- Sparsity\nB-1 is represented in terms of sparse triangular matrices\nFull tableau implementation\nSlide 9\n-c ′ B-1b\nc ′ -c ′ B-1A\nB\nB\nB-1b\nB-1A\nor, in more detail,\n′\n-cBxB\nxB(1)\n. . .\nxB(m)\nc1\n. . .\ncn\n|\n|\nB-1A1\n. . .\nB-1An\n|\n|\n\n3.1\nExample\nmin\n-10x1 - 12x2 - 12x3\ns.t.\nx1 +\n2x2 +\n2x3 ≤ 20\n2x1 +\nx2 +\n2x3 ≤ 20\n2x1 +\n2x2 +\nx3 ≤ 20\nx1, x2, x3 ≥ 0\nmin\n-10x1 - 12x2 - 12x3\ns.t.\nx1 +\n2x2 +\n2x3 + x4\n= 20\n2x1 +\nx2 +\n2x3\n+ x5\n= 20\n2x1 +\n2x2 +\nx3\n+ x6 = 20\nx1, . . . , x6 ≥ 0\nBFS: x = (0, 0, 0, 20, 20, 20) ′\nB=[A4, A5, A6]\nx1\nx2\nx3\nx4\nx5\nx6\n-10\n-12\n-12\nx4 =\nx5 =\n2*\nx6 =\nSlide 10\nSlide 11\nc ′ = c ′ -cB\n′ B-1A = c ′ = (-10, -12, -12, 0, 0, 0)\nSlide 12\nx1\nx2\nx3\nx4\nx5\nx6\n-7\n-2\n1.5\n1*\n-0.5\n0.5\n0.5\n-1\n-1\nx4 =\nx1 =\nx6 =\nSlide 13\nx1\nx2\nx3\nx4\nx5\nx6\n-4\nx3 =\n1.5\n-0.5\nx1 =\n-1\n-1\nx6 =\n2.5*\n-1.5\nSlide 14\n\nP\nP\nx1\nx2\nx3\nx4\nx5\nx6\n3.6\n1.6\n1.6\nx3 =\n0.4\n0.4\n-0.6\nx1 =\n-0.6\n0.4\n0.4\nx2 =\n0.4\n-0.6\n0.4\nSlide 15\nA = (0,0,0)\nB = (0,0,10)\nE = (4,4,4)\n.\n.\n.\n.\n.\nx3\nC = (0,10,0)\nD = (10,0,0)\nx1\nx2\nComparison of implementations\nSlide 16\nFull tableau\nMemory\nO(mn)\nWorst-case time\nO(mn)\nBest-case time\nO(mn)\nRevised simplex\nO(m2)\nO(mn)\nO(m2)\nAnticycling\n5.1\nDegeneracy in Practice\nSlide 17\nDoes degeneracy really happen in practice?\nn\nxij = 1\nj=1\nn\nxij = 1\ni=1\nxij ≥ 0\n\nn! vertices:\nFor each vertex ∃ 2n-1nn-2 different bases (n = 8) for each vertex ∃ 33, 554, 432\nbases.\n5.2\nPerturbations\nSlide 18\n′\n′\n(P) min\nc x\n(Po) min\nc x\n\no\no2\ns.t.\nAx = b\ns.t.\nAx = b +\n\n.\n\n.\n\n.\n\nom\nx ≥ 0\nx ≥ 0.\n5.2.1\nTheorem\nSlide 19\n∃ o1 > 0: for all 0 < o < o1\n\no\n\n.\n\nAx = b +\n..\n\nom\nx ≥ 0\nis non-degenerate.\n5.2.2\nProof\nSlide 20\nLet B1, . . . , Br be all the bases.\n\nr\n\no\nb1 + Br\n· · · + Br om\n11o +\n1m\nB-1\n\n.\n\n.\n\n.\nr\nb +\n..\n=\n.\n\nom\nb\nr + Br\nm1o + · · · + Br\nom\nm\nmm\nwhere:\n\nr\nBr\nBr\n· · ·\nb\n1m\nB-1\n\n.\n.\n, B-1\n\n.\n\nr\n=\n. .\n. .\n\nr b =\n. .\n\nBr\nBr\nr\n· · ·\nm1\nmm\nbm\nSlide 21\nr\n-\n+ Br\n+ · · · + Br\nis a polynomial in θ\nbi\ni1θ\nim θm\n- Roots θi,\nr\n1, θi,\nr\n2, . . . , θr\ni,m\nr\n- If o = θi,\nr\n1, . . . , θr\n+ Bi\nr\n1o + · · · + Br om = 0.\ni,m ⇒ bi\nim\n- Let o1 the smallest positive root ⇒ 0 < o < o1 all RHS are 6\n0 ⇒\n=\nnon-degeneracy.\n\n5.3\nLexicography\nSlide 22\nL\n- u is lexicographically larger than v, u > v, if u 6\nv and the first\n=\nnonzero component of u -v is positive.\n- Example:\nL\n(0, 2, 3, 0) > (0, 2, 1, 4),\nL\n(0, 4, 5, 0) < (1, 2, 1, 2).\n5.4\nLexicography-Pertubation\n5.4.1\nTheorem\nSlide 23\nLet B be a basis of Ax = b, x ≥ 0.\nThen B is feasible for Ax = b +\n′\n(o, . . . , om) ,\nx ≥ 0 for sufficiently small o if and only if\nL\nui = (bi, Bi1, . . . , Bim) > 0, ∀ i\nB-1 = (Bij)\n(B-1b)i = (bi)\n5.4.2\nProof\nSlide 24\nB is feasible for peturbed problem \"⇔\" B-1 (b + (o, . . . , om) ′ ) ≥ 0 ⇔\nbi + Bi1o + · · · + Bimom ≥ 0 ∀ i\n⇔ First non-zero component of ui = (bi, Bi1, . . . , Bim) is positive ∀ i.\n5.5\nSummary\n1. We start with: (P) : Ax = b, x ≥ 0\nSlide 25\n2. We introduce (Po): Ax = b + (o, . . . , om) ′ , x ≥ 0\n3. A basis is feasible + non-degenerate in (Po) ⇔ ui\nL > 0 in (P).\n4. If we maintain ui\nL > 0 in (P) ⇒ (Po) is non-degenerate ⇒\nfinite in (Po) for sufficiently small o.\nSimplex is\n5.6\nLexicographic pivoting rule\n1. Choose an entering column Aj arbitrarily, as long as cj < 0; u = B-1Aj.\nSlide 26\n2. For each i with ui > 0, divide the ith row of the tableau (including the\nentry in the zeroth column) by ui and choose the lexicographically smallest\nrow. If row l is lexicographically smallest, then the lth basic variable xB(l)\nexits the basis.\n\n5.6.1\nExample\nSlide 27\n- j = 3\n· · ·\n-\n-1\n· · ·\n· · ·\n- xB(1)/u1 = 1/3 and xB(3)/u3 = 3/9 = 1/3.\n- We divide the first and third rows of the tableau by u1 = 3 and u3 = 9,\nrespectively, to obtain:\n1/3\n5/3\n· · ·\n-\n∗\n∗\n∗\n∗\n· · ·\n1/3\n7/9\n· · ·\n- Since 7/9 < 5/3, the third row is chosen to be the pivot row, and the\nvariable xB(3) exits the basis.\n5.6.2\nUniqueness\nSlide 28\n- Why lexicographic pivoting rule always leads to a unique choice for the\nexiting variable?\n- Otherwise, two rows in tableau proportional ⇒ rank(B-1A) < m ⇒\nrank(A) < m\n5.7 Theorem\nSlide 29\nIf simplex starts with all the rows in the simplex tableau, other than the zeroth\nrow, lexicographically positive and the lexicographic pivoting rule is followed,\nthen\n(a) Every row of the simplex tableau, other than the zeroth row, remains\nlexicographically positive throughout the algorithm.\n(b) The zeroth row strictly increases lexicographically at each iteration.\n(c) The simplex method terminates after a finite number of iterations.\n5.8\nSmallest subscript\npivoting rule\nSlide 30\n1. Find the smallest j for which the reduced cost cj is negative and have the\ncolumn Aj enter the basis.\n2. Out of all variables xi that are tied in the test for choosing an exiting\nvariable, select the one with the smallest value of i.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/12b1c7e885991696c294953b2497739c_MIT6_251JF09_lec07.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 7: The Simplex Method III\n\nOutline\nSlide 1\n- Finding an initial BFS\n- The complete algorithm\n- The column geometry\n- Computational efficiency\n- The diameter of polyhedra and the Hirch conjecture\nFinding an initial BFS\nSlide 2\n- Goal: Obtain a BFS of Ax = b,\nx ≥ 0\nor decide that LOP is infeasible.\n- Special case: b ≥ 0\nAx ≤ b,\nx ≥ 0\n⇒ Ax + s = b,\nx, s ≥ 0\ns = b,\nx = 0\n2.1\nArtificial variables\nSlide 3\nAx = b,\nx ≥ 0\n1. Multiply rows with -1 to get b ≥ 0.\n2. Introduce artificial variables y, start with initial BFS y = b, x = 0, and\napply simplex to auxiliary problem\nmin\ny1 + y2 + . . . + ym\ns.t.\nAx + y = b\nx, y ≥ 0\nSlide 4\n3. If cost > 0 ⇒ LOP infeasible; stop.\n4. If cost = 0 and no artificial variable is in the basis, then a BFS was found.\n5. Else, all yi\n∗ = 0, but some are still in the basis. Say we have AB(1), . . . , AB(k)\nin basis k < m. There are m -k additional columns of A to form a basis.\nSlide 5\n6. Drive artificial variables out of the basis: If lth basic variable is artifi\ncial examine lth row of B-1A. If all elements = 0 ⇒ row redundant.\nOtherwise pivot with 6\n0 element.\n=\n\n2.2\nExample\nSlide 6\nmin\nx1 +\nx2 +\nx3\ns.t.\nx1 + 2x2 + 3x3\n= 3\n-x1 + 2x2 + 6x3\n= 2\n4x2 + 9x3\n= 5\n3x3 + x4 = 1\nx1, . . . , x4 ≥ 0.\nmin\nx5 + x6 + x7 + x8\ns.t.\nx1 + 2x2 + 3x3\n+ x5\n= 3\n-x1 + 2x2 + 6x3\n+ x6\n= 2\n4x2 + 9x3\n+ x7\n= 5\n3x3 + x4\n+ x8 = 1\nx1, . . . , x8 ≥ 0.\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n1/3\nSlide 7\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n-8\n-21\n-1\n-1\n1*\n-11\nx5\nx6\nx7\nx8\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n-8\n-18\n-1\n3*\nSlide 8\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n-8\n-1\n-1\n-1\n2*\n-2\n-2\n-3\n-3\n1/3\n1/3\n-10\n-4\nx5\nx6\nx7\nx4\nx5\nx6\nx7\nx3\n\n-4\nx5 =\nx2 =\nx7 =\nx3 =\n1/3\nx1 =\nx2 =\n1/2\nx7 =\nx3 =\n1/3\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n-4\n-2\n-1\n2*\n-1\n-1\n1/2\n-1\n-2\n1/3\n1/3\nSlide 9\n-1/2\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx1 =\nx2 =\nx3 =\n1/2\n1/2\n-1/2\n1/2\n-3/4\n1/4\n1/4\n-3/4\n-1\n-1\n1/3\n1/3\n∗\n1/2\n1/3\nSlide 10\nx1\nx2\nx3\nx4\n∗\n∗\n∗\n∗\n1/2\n-3/4\n1/3\nA complete Algorithm for LO\nSlide 11\nPhase I:\n1. By multiplying some of the constraints by -1, change the problem so that\nb ≥ 0.\n2. Introduce y1, . . . , ym, if necessary, and apply the simplex method to min Pm\ni=1 yi.\n3. If cost> 0, original problem is infeasible; STOP.\n4. If cost= 0, a feasible solution to the original problem has been found.\n5. Drive artificial variables out of the basis, potentially eliminating redundant\nrows.\nSlide 12\nPhase II:\n\nX\nX\n1. Let the final basis and tableau obtained from Phase I be the initial basis\nand tableau for Phase II.\n2. Compute the reduced costs of all variables for this initial basis, using the\ncost coefficients of the original problem.\n3. Apply the simplex method to the original problem.\n3.1\nPossible outcomes\nSlide 13\n1. Infeasible: Detected at Phase I.\n2. A has linearly dependent rows: Detected at Phase I, eliminate redundant\nrows.\n3. Unbounded (cost= -inf): detected at Phase II.\n4. Optimal solution: Terminate at Phase II in optimality check.\nThe big-M method\nSlide 14\nn\nm\nmin\ncj xj + M\nyi\nj=1\ni=1\ns.t.\nAx + y = b\nx, y ≥ 0\nThe Column Geometry\nSlide 15\nmin\nc ′ x\ns.t.\nAx = b\ne ′ x = 1\nx ≥ 0\n\nx1\nA1\n+ x2\nA2\n+ · · · + xn\nAn\n=\nb\nc1\nc2\ncn\nz\nSlide 16\nSlide 17\nComputational efficiency\nSlide 18\nExceptional practical behavior: linear in n\nWorst case\nmax\nxn\ns.t.\no ≤ x1 ≤ 1\noxi-1 ≤ xi ≤ 1 -oxi-1,\ni = 2, . . . , n\nSlide 19\nTheorem\nSlide 20\n\nz\nb .\nB\nD\nE\nF\nC\nG\nH\nI\n.\n.\n.\ninitial basis\nz\n.\nb\n.\n.\n.\n.\n.\n.\n.\n.\nnext basis\noptimal basis\n(a)\n(b)\nx1\nx2\nx3\nx1\nx2\n\n-\nj\nk\n- The feasible set has 2n vertices\n- The vertices can be ordered so that each one is adjacent to and has lower\ncost than the previous one.\n- There exists a pivoting rule under which the simplex method requires\n2n -1 changes of basis before it terminates.\nThe Diameter of polyhedra\nSlide 21\n- Given a polyhedron P, and x, y vertices of P, the distance d(x, y) is the\nminimum number of jumps from one vertex to an adjacent one to reach y\nstarting from x.\n- The diameter D(P) is the maximum of d(x, y) ∀x, y.\nSlide 22\n- Δ(n, m) as the maximum of D(P) over all bounded polyhedra in Rn that\nare represented in terms of m inequality constraints.\n- Δu(n, m) is like Δ(n, m) but for possibly unbounded polyhedra.\n7.1\nThe Hirsch Conjecture\nSlide 23\nm\nΔ(2, m) =\n,\nΔu(2, m) = m -2\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n( a )\n( b )\n- Hirsch Conjecture: Δ(n, m) ≤ m -n.\nSlide 24\n- We know that\nj\nk\nn\nΔu(n, m) ≥ m -n +\nΔ(n, m) ≤ Δu(n, m) < m1+log2 n = (2n)log m\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/496d9a385fd1d7e1aa82837d91adb23e_MIT6_251JF09_lec08.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 8: Duality Theory I\n\nh\ni\nOutline\nSlide 1\n- Motivation of duality\n- General form of the dual\n- Weak and strong duality\n- Relations between primal and dual\n- Economic Interpretation\n- Complementary Slackness\nMotivation\n2.1\nAn idea from Lagrange\nSlide 2\nConsider the LOP, called the primal with optimal solution x ∗\n′\nmin\nc x\ns.t. Ax = b\nx ≥ 0\nRelax the constraint\ng(p) = min\nc ′ x + p ′ (b - Ax)\ns.t. x ≥ 0\ng(p) ≤ c ′ x ∗ + p ′ (b - Ax∗) = c ′ x ∗\nGet the tightest lower bound, i.e.,\nmax g(p)\ng(p) = min c ′ x + p ′ (b -Ax)\nx≥0\n′\n′\n′\n= p b + min (c -p A)x\nx≥0\nNote that\n\n′\n′\n0,\nif c ′ -p ′ A ≥ 0 ′ ,\nmin (c -p A)x =\nx≥0\n-inf, otherwise.\nDual\nmax g(p)\n⇔\nmax\np ′ b\ns.t. p ′ A ≤ c ′\n\nGeneral form of the dual\nPrimal\n′\nmin\nc x\ns.t.\na′\ni\nx ≥ bi\ni ∈ M1\na′\ni\nx ≤ bi\ni ∈ M2\na′\ni\nx = bi\ni ∈ M3\nxj ≥ 0\nj ∈ N1\nxj ≤ 0\nj ∈ N2\nxj\n>\n<0\nj ∈ N3\n3.1\nExample\nmin\nx1 + 2x2 + 3x3\ns.t.\n-x1 + 3x2\n= 5\n2x1 -\nx2 + 3x3 ≥ 6\nx3 ≤ 4\nx1 ≥ 0\nx2 ≤ 0\nx3 free,\nPrimal\nconstraints\nvariables\nmin\n≥ bi\n≤ bi\n= bi\n≥ 0\n≤ 0\n>\n<0\nDual\nmax\ns.t.\nmax\ns.t.\nmax\n≥ 0\n≤ 0\n> <0\n≤ cj\n≥ cj\n= cj\nmin\nc x\nmax\np ′ b\ns.t. Ax = b\ns.t. p ′ A ≤ c ′\nx ≥ 0\n′\n′ b\nmin\nc x\nmax\np\ns.t.\nAx ≥ b\ns.t.\np ′ A = c ′\np ≥ 0\nWeak Duality\nSlide 7\nTheorem:\nIf x is primal feasible and p is dual feasible then p ′ b ≤ c ′ x\nProof\np ′ b = p ′ Ax ≤ c ′ x\nTheorem: The dual of the dual is the primal.\n3.2\nA matrix view\n′\nSlide 3\np ′ b\npi ≥ 0\npi ≤ 0\np >\ni\n<0\np ′ Aj ≤ cj\np ′ Aj ≥ cj\np ′ Aj = cj\ni ∈ M1\ni ∈ M1\ni ∈ M3\nj ∈ N1\nj ∈ N2\nj ∈ N3\n5p1 + 6p2 + 4p3\np1 free\np2 ≥ 0\np3 ≤ 0\n-p1 + 2p2\n≤ 1\n3p1 -\np2\n≥ 2\n3p2 + p3 = 3.\nSlide 4\nSlide 5\ndual\nvariables\nconstraints\nSlide 6\n\nCorollary:\nIf x is primal feasible, p is dual feasible, and p ′ b = c ′ x, then x is optimal in\nthe primal and p is optimal in the dual.\nStrong Duality\nSlide 8\nTheorem: If the LOP has optimal solution, then so does the dual, and optimal\ncosts are equal.\nProof:\n′\nmin\nc x\ns.t. Ax = b\nx ≥ 0\nApply Simplex; optimal solution x, basis B.\nOptimality conditions:\nc ′ -cB\n′ B-1A ≥ 0 ′\nSlide 9\nDefine p ′ = c ′ B-1 ⇒ p ′ A ≤ c ′\nB\n⇒ p dual feasible for\nmax\np ′ b\ns.t. p ′ A ≤ c ′\n′\np ′ b = cB\n′ B-1b = cB\n′ xB = c x\n⇒ x, p are primal and dual optimal\n5.1\nIntuition\nSlide 10\nx *\na1\na2\na3\np1a1\np2a2\nc\n.\n\nX\nX\nRelations between primal and dual\nSlide 11\nFinite opt.\nUnbounded\nInfeasible\nFinite opt.\n*\nUnbounded\n*\nInfeasible\n*\n*\nEconomic Interpretation\nSlide 12\n- x optimal nondegenerate solution: B-1b > 0\n- Suppose b changes to b + d for some small d\n- How is the optimal cost affected?\n- For small d feasibilty unaffected\n- Optimality conditions unaffected\n- New cost\nc ′ B-1(b + d) = p ′ (b + d)\nB\n- If resource i changes by di, cost changes by pidi: \"Marginal Price\"\nComplementary slackness\n8.1\nTheorem\nSlide 13\nLet x primal feasible and p dual feasible. Then x, p optimal if and only if\n′\npi(aix -bi) = 0,\n∀i\n′\nxj (cj -p Aj ) = 0,\n∀j\n8.2\nProof\nSlide 14\n- ui = pi(ai\n′ x -bi) and vj = (cj -p ′ Aj )xj\n- If x primal feasible and p dual feasible, we have ui ≥ 0 and vj ≥ 0 for all\ni and j.\n- Also\n′ c x -p ′ b =\nui +\nvj .\ni\nj\n′\n- By the strong duality theorem, if x and p are optimal, then c x = p ′ b ⇒\nui = vj = 0 for all i, j.\n- Conversely, if ui = vj = 0 for all i, j, then c ′ x = p ′ b,\n- ⇒ x and p are optimal.\n\n8.3\nExample\nSlide 15\nmin\n13x1 + 10x2 + 6x3\nmax\n8p1 + 3p2\ns.t.\n5x1 +\nx2 + 3x3 = 8\ns.t. 5p1 + 3p2 ≤ 13\n3x1 +\nx2\n= 3\np1 +\np2 ≤ 10\nx1 ,\nx2 ,\nx3 ≥ 0\n3p1\n≤ 6\nIs x ∗ = (1, 0, 1) ′ optimal?\nSlide 16\n5p1 + 3p2 = 13,\n3p1 = 6\n⇒ p1 = 2,\np2 = 1\nObjective=19\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec09.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/b93d1665d7a8f05ea626b8f04c7ad66c_MIT6_251JF09_lec09.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 9: Duality Theory II\n\nOutline\nSlide 1\n- Strict complementary slackness\n- Geometry of duality\n- The dual simplex algorithm\n- Duality and degeneracy\nStrict Complementary Slackness\nSlide 2\nAssume that both problems have an optimal solution:\nmin\nc ′ x\nmax\np ′ b\n′\ns.t. Ax ≥ b\ns.t. p ′ A ≤ c\nx ≥ 0,\np ≥ 0.\nThere exist optimal solutions to the primal and to the dual that satisfy\n- For every j, either xj > 0 or p ′ Aj < cj .\n′\n- For every i, we have either aix > bi or pi > 0.\n2.1\nExample\nSlide 3\nmin\n5x1 + 5x2\ns.t.\nx1 + x2 ≥ 2\n2x1 -x2 ≥ 0\nx1, x2 ≥ 0.\n- Is (2/3, 4/3) strictly complementary?\n- Which are all the strictly complementary solutions?\nThe Geometry of Duality\nSlide 4\n′\nmin\nc x\n′\ns.t. aix ≥ bi,\ni = 1, . . . , m\nmax\np ′ b\nm\ns.t.\nX\npiai = c\ni=1\np ≥ 0\n\na1\na3\nc\nB\na1\na2\nA\na1\na2\na3\nc\na1\na1\na4\na4\na5\na5\nc\nc\nc\nC\nD\na1\na2\na3\nc\nx *\na1\na2\na3\n\nDual Simplex Algorithm\n4.1\nMotivation\nSlide 5\n- In simplex method B-1b ≥ 0\n- Primal optimality condition\nc ′ -c ′ B-1A ≥ 0 ′\nB\nsame as dual feasibility\n- Simplex is a primal algorithm: maintains primal feasibility and works\ntowards dual feasibility\n- Dual algorithm: maintains dual feasibility and works towards primal\nfeasibility\n′\n-cB xB\nxB(1)\n. . .\nxB(m)\n- Do not require B-1b ≥ 0\nSlide 6\nc1\n|\nB-1A1\n|\n. . .\n. . .\ncn\n|\nB-1An\n|\n- Require c ≥ 0 (dual feasibility)\n- Dual cost is\np ′ b = c ′ B-1b = cB\n′ xB\nB\n- If B-1b ≥ 0 then both dual feasibility and primal feasibility, and also\nsame cost ⇒ optimality\n- Otherwise, change basis\n4.2\nAn iteration\nSlide 7\n1. Start with basis matrix B and all reduced costs ≥ 0.\n2. If B-1b ≥ 0 optimal solution found; else, choose l s.t. xB(l) < 0.\n3. Consider the lth row (pivot row) xB(l), v1, . . . , vn. If ∀i vi ≥ 0 then dual\noptimal cost = +inf and algorithm terminates.\nSlide 8\n4. Else, let j s.t.\nc j\nc i\n=\nmin\n|vj |\n{i|vi <0} |vi|\n5. Pivot element vj : Aj enters the basis and AB(l) exits.\n\nc\nA\nB\nC\nD\nE\nx2\nx1\nb\nA\nB\nC\nD\nE\np2\np1\n1/2\n. . .\n.\n.\n.\n.\n.\n.\n.\n(a)\n(b)\n4.3\nAn example\nmin\nx1 + x2\ns.t.\nx1 + 2x2 ≥ 2\nx1 ≥ 1\nx1, x2 ≥ 0\nmin\nx1 + x2\nmax\ns.t.\nx1 + 2x2 -x3 = 2\ns.t.\nx1 -x4 = 1\nx1, x2, x3, x4 ≥ 0\nSlide 9\n2p1 + p2\np1 + p2 ≤ 1\n2p1 ≤ 1\np1, p2 ≥ 0\nSlide 10\nx1\nx2\nx3\nx4\nx3 =\n-2\n-1\n-2*\nx4 =\n-1\n-1\nSlide 11\nx1\nx2\nx3\nx4\n-1\n1/2\n1/2\nx2 =\n1/2\n-1/2\nx4 =\n-1\n-1*\nx1\nx2\nx3\nx4\n-3/2\n1/2\n1/2\nx2 =\n1/2\n-1/2\n1/2\nx1 =\n-1\n\nc\nA\nB\nC\nD\n.\n.\n.\n.\nB\nD\n.\n.\n.\n.\nC\n.\n.\nA'\nA' '\nx2\np2\nA\np1\nb\nx1\n(a)\n(b)\nDuality and Degeneracy\nSlide 12\n- Any basis matrix B leads to dual basic solution p ′ = cB ′ B-1 .\n- The dual constraint p ′ Aj = cj is active if and only if the reduced cost cj\nis zero.\n- Since p is m-dimensional, dual degeneracy implies more than m reduced\ncosts that are zero.\n- Dual degeneracy is obtained whenever there exists a nonbasic variable\nwhose reduced cost is zero.\n5.1\nExample\nSlide 13\nmin\n3x1 + x2\nmax\n2p1\ns.t.\nx1 + x2 -x3 = 2\ns.t.\np1 + 2p2 ≤ 3\n2x1 -x2 -x4 = 0\np1 -p2 ≤ 1\nx1, x2, x3, x4 ≥ 0,\np1, p2 ≥ 0.\nEquivalent primal problem\nmin\n3x1 + x2\ns.t.\nx1 + x2 ≥ 2\n2x1 - x2 ≥ 0\nx1, x2 ≥ 0.\nSlide 14\nSlide 15\n- Four basic solutions in primal: A, B, C, D.\n- Six distinct basic solutions in dual: A, A ′ , A ′′ , B, C, D.\n- Different bases may lead to the same basic solution for the primal, but\nto different basic solutions for the dual. Some are feasible and some are\ninfeasible.\n\n5.2\nDegeneracy and uniqueness\nSlide 16\n- If dual has a nondegenerate optimal solution, the primal problem has a\nunique optimal solution.\n- It is possible, however, that dual has a degenerate solution and the dual\nhas a unique optimal solution.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "MIT6_251JF09_lec10.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/ab890d20b071105239fe0b9bcf599f57_MIT6_251JF09_lec10.pdf",
      "content": "15.081J/6.251J Introduction to Mathematical\nProgramming\nLecture 10: Duality Theory III\n\nA2\nA3\n.\nA1\np\nb\nOutline\nSlide 1\n- Farkas lemma\n- Asset pricing\n- Cones and extreme rays\n- Representation of Polyhedra\nFarkas lemma\nSlide 2\nTheorem:\nExactly one of the following two alternatives hold:\n1. ∃x ≥ 0 s.t. Ax = b.\n2. ∃p s.t. p ′ A ≥ 0 ′ and p ′ b < 0.\n2.1\nProof\nSlide 3\n′′\n\" ⇒\nIf ∃x ≥ 0 s.t. Ax = b, and if p ′ A ≥ 0 ′ , then p ′ b = p ′ Ax ≥ 0\n′′\n\" ⇐\nAssume there is no x ≥ 0 s.t. Ax = b\n(P) max\n0 ′ x\n(D) min\np ′ b\ns.t.\nAx = b\ns.t.\np ′ A ≥ 0 ′\nx ≥ 0\n(P) infeasible ⇒ (D) either unbounded or infeasible\nSince p = 0 is feasible ⇒ (D) unbounded\n⇒∃p :\np ′ A ≥ 0 ′ and p ′ b < 0\n\nX\n\nAsset Pricing\nSlide 4\n- n different assets\n- m possible states of nature\n- one dollar invested in some asset i, and state of nature is s, we receive a\npayoff of rsi\n- m × n payoff matrix:\n\nr11\n. . .\nr1n\nR =\n..\n..\n..\n\n.\n.\n.\nrm1\n. . .\nrmn\nSlide 5\n- xi: amount held of asset i. A portfolio of assets is x = x1, . . . , xn .\n- A negative value of xi indicates a \"short\" position in asset i: this amounts\nto selling |xi| units of asset i at the beginning of the period, with a promise\nto buy them back at the end. Hence, one must pay out rsi|xi| if state s\noccurs, which is the same as receiving a payoff of rsixi\nSlide 6\n- Wealth in state s from a portfolio x\nn\nws =\nrsixi.\ni=1\n- w = w1, . . . , wm , w = Rx\n- pi: price of asset i, p = p1, . . . , pn\n′\n- Cost of acquiring x is p x.\n3.1\nArbitrage\nSlide 7\n- Central problem: Determine pi\n- Absence of arbitrage: no investor can get a guaranteed nonnegative\npayoffout of a negative investment. In other words, any portfolio that pays\noff nonnegative amounts in every state of nature, must have nonnegative\ncost.\n′\nif Rx ≥ 0,\nthen p x ≥ 0.\nSlide 8\n\nX\n\n- Theorem: The absence of arbitrage condition holds if and only if there\nexists a nonnegative vector q = (q1, . . . , qm), such that the price of each\nasset i is given by\nm\npi =\nqsrsi.\ns=1\n- Applications to options pricing\nCones and extreme rays\n4.1\nDefinitions\nSlide 9\n- A set C ⊂Rn is a cone if λx ∈ C for all λ ≥ 0 and all x ∈ C\n- A polyhedron of the form P = {x ∈Rn | Ax ≥ 0} is called a polyhedral\ncone\n4.2\nApplications\nSlide 10\n- P =\nx ∈Rn | Ax ≥ b , y ∈ P\n- The recession cone at y\nRC =\nd ∈Rn | y + λd ∈ P, ∀ λ ≥ 0\n- It turns out that\nRC =\nd ∈Rn | Ad ≥ 0\n- RC independent of y\nSlide 11\n4.3\nExtreme rays\nSlide 12\nA x =6\n0 of a polyhedral cone C ⊂Rn is called an extreme ray if there are\nn -1 linearly independent constraints that are active at x\n4.4\nUnbounded LPs\nSlide 13\n′\nTheorem: Consider the problem of minimizing c x over a polyhedral cone C =\n{x ∈Rn | A ′\nix ≥ 0, i = 1, . . . , m} that has zero as an extreme point. The\noptimal cost is equal to -inf if and only if some extreme ray d of C satisfies\n′\n′\nc d < 0.\nTheorem: Consider the problem of minimizing c x subject to Ax ≥ b,\nSlide 14\nand assume that the feasible set has at least one extreme point. The optimal\ncost is equal to -inf if and only if some extreme ray d of the feasible set satisfies\n′\nc d < 0.\nWhat happens when the simplex method detects an unbounded problem?\n\n'x = 0\nx2\nx3\n- a1\nx1\nx1\nx2\n.\ny\na1\na2'x = 0\n.\n.\nz.\n- a2\n(a)\n(b)\n\n(\n)\nX\nX\nX\nx1\nw 1\ny.\n.\n.\n.\nrecession\ncone\nx1\nx\nx\nx\nw\nResolution Theorem\nSlide 15\nP =\nx ∈Rn | Ax ≥ b\nbe a nonempty polyhedron with at least one extreme point. Let x1 , . . . , xk be\nthe extreme points, and let w1 , . . . , wr be a complete set of extreme rays of P.\nk\nr\n\nk\nQ =\nλix i +\nθj wj λi ≥ 0, θj ≥ 0,\nλi = 1\n.\ni=1\nj=1\ni=1\nThen, Q = P.\n5.1\nExample\nSlide 16\nx1 -x2 ≥-2\nx1 + x2 ≥ 1\nx1, x2 ≥ 0\nSlide 17\n- Extreme points: x1 = (0, 2), x2 = (0, 1), and x3 = (1, 0).\n- Extreme rays w1 = (1, 1) and w2 = (1, 0).\n-\n\ny =\n=\n+\n+\n= x + w + w .\n\nX\nX\nX\nX\nX\nX\nX\n5.2\nProof\nSlide 18\n- Q ⊂ P. Let x ∈ Q:\nk\nr\nx =\nλix i +\nθj wj\ni=1\nj=1\nPk\nλi, θj ≥ 0\ni=1 λi = 1.\n- y = Pk\ni=1 λixi ∈ P and satisfies Ay ≥ b.\n- Awj ≥ 0 for every j: z = Pr\nj=1 θj wj satisfies Az ≥ 0.\n- x = y + z satisfies Ax ≥ b and belongs to P.\nSlide 19\nFor the reverse, assume there is a z ∈ P, such that z /∈ Q.\nk\nr\nmax\n0λi +\n0θj\ni=1\nj=1\nk\nr\ns.t.\nλix i +\nθj wj = z\ni=1\nj=1\nk\nλi = 1\ni=1\nλi ≥ 0,\ni = 1, . . . , k,\nθj ≥ 0,\nj = 1, . . . , r,\nIs this feasible?\nSlide 20\n- Dual\n′\nmin\np z + q\ns.t.\np ′ xi + q ≥ 0,\ni = 1, . . . , k,\n′\np wj ≥ 0,\nj = 1, . . . , r.\n- This is unbounded. Why?\n′\n- There exists a feasible solution (p, q) whose cost p z + q < 0\n- p ′ z < p ′ xi for all i and p ′ wj ≥ 0 for all j.\nSlide 21\n-\n′\nmin\np x\ns.t.\nAx ≥ b.\n- If the optimal cost is finite, there exists an extreme point xi which is\noptimal. Since z is a feasible solution, we obtain p ′ x i ≤ p ′ z, which is a\ncontradiction.\n- If the optimal cost is -inf, there exists an extreme ray wj such that\np ′ wj < 0, which is again a contradiction\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_SDP.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/08bbc2660764c4f61bde5363ae134339_MIT6_251JF09_SDP.pdf",
      "content": "Introduction to Semidefinite Programming (SDP)\nRobert M. Freund\nIntroduction\nSemidefinite programming (SDP) is the most exciting development in math\nematical programming in the 1990's. SDP has applications in such diverse\nfields as traditional convex constrained optimization, control theory, and\ncombinatorial optimization.\nBecause SDP is solvable via interior point\nmethods, most of these applications can usually be solved very efficiently\nin practice as well as in theory.\nReview of Linear Programming\nConsider the linear programming problem in standard form:\nLP :\nminimize\nc x\n·\ns.t.\nai x = bi,\ni = 1, . . . , m\n·\nn\n+.\nx ∈R\nHere x is a vector of n variables, and we write \"c x\" for the inner-product\nP\n·\n\"\nj\nn\n=1 cjxj\", etc.\nAlso, Rn\n+\nn\nx ≥ 0}, and we call Rn\n+ the nonnegative orthant.\nn\n:= {x ∈R\n|\nIn fact, R\nis a closed convex cone, where K is called a closed a convex cone\n+\nif K satisfies the following two conditions:\n\nP\nP\nP\n- If x, w ∈ K, then αx + βw ∈ K for all nonnegative scalars α and β.\nK is a closed set.\n-\nIn words, LP is the following problem:\n\"Minimize the linear function c x, subject to the condition that x must solve\n·\nm given equations ai x = bi, i = 1, . . . , m, and that x must lie in the closed\n·\nconvex cone K =\nn .\"\n+\nR\nWe will write the standard linear programming dual problem as:\nm\nLD :\nmaximize\nyibi\ni=1\nm\ns.t.\nyiai + s = c\ni=1\nn\n+.\ns ∈R\nGiven a feasible solution x of LP and a feasible solution (y, s) of LD, the\nduality gap is simply c x- Pm\ni=1 yibi = (c- Pm\ni=1 yiai) x = s x ≥ 0, because\n·\n·\n·\nx ≥ 0 and s ≥ 0. We know from LP duality theory that so long as the pri\nmal problem LP is feasible and has bounded optimal objective value, then\nthe primal and the dual both attain their optima with no duality gap. That\nis, there exists x ∗ and (y ∗ , s ∗) feasible for the primal and dual, respectively,\n∗\nm\n∗\n∗\n∗\nsuch that c x\ni=1 yi bi = s\nx = 0.\n·\n-\n·\nFacts about Matrices and the Semidefinite Cone\nIf X is an n × n matrix, then X is a positive semidefinite (psd) matrix if\nn\nv T Xv ≥ 0 for any v ∈R .\n\nIf X is an n × n matrix, then X is a positive definite (pd) matrix if\nn\nv T Xv > 0 for any v ∈R, v =\n0.\nLet Sn\nn\nmatrices, and let S\nn\n×\n+\nthe set of positive semidefinite (psd) n × n symmetric matrices. Similarly\nlet Sn\ndenote the set of positive definite (pd) n × n symmetric matrices.\n++\nLet X and Y be any symmetric matrices. We write \"X ⪰ 0\" to denote\nthat X is symmetric and positive semidefinite, and we write \"X ⪰ Y \" to\ndenote that X - Y ⪰ 0. We write \"X ≻ 0\" to denote that X is symmetric\nand positive definite, etc.\nRemark 1 Sn\n+\ndenote the set of symmetric n\ndenote\n{X ∈ Sn | X ⪰ 0} is a closed convex cone in Rn\n× (n + 1)/2.\nof\n=\ndimension n\nn\nTo see why this remark is true, suppose that X, W\nS\n∈\n+\nα, β ≥ 0. For any v ∈Rn, we have:\nv T (αX + βW)v = αvT Xv + βvT Wv ≥ 0,\n. Pick any scalars\nwhereby αX\nn\nn\nβW\nS\nThis shows that S\n+\n∈\n+\n+\n+\nforward to show that Sn\nRecall the following properties of symmetric matrices:\nIf X ∈ Sn, then X = QDQT for some orthonormal matrix Q and\n-\nsome diagonal matrix D. (Recall that Q is orthonormal means that\nQ-1 = QT , and that D is diagonal means that the off-diagonal entries\nof D are all zeros.)\nis a cone. It is also straight\n.\nis a closed set.\n\nX X\nIf X = QDQT as above, then the columns of Q form a set of n\n-\northogonal eigenvectors of X, whose eigenvalues are the corresponding\ndiagonal entries of D.\nX ⪰ 0 if and only if X = QDQT where the eigenvalues (i.e., the\n-\ndiagonal entries of D) are all nonnegative.\nX ≻ 0 if and only if X = QDQT where the eigenvalues (i.e., the\n-\ndiagonal entries of D) are all positive.\n- If X ⪰ 0 and if Xii = 0, then Xij = Xji = 0 for all j = 1, . . . , n.\nConsider the matrix M defined as follows:\n-\nP\nv\nM =\nT\n,\nv\nd\nwhere P ≻ 0, v is a vector, and d is a scalar. Then M ≻ 0 if and\nonly if d - vT P -1v > 0.\nSemidefinite Programming\nLet X ∈ Sn . We can think of X as a matrix, or equivalently, as an array of\nn2 components of the form (x11, . . . , xnn). We can also just think of X as\nan object (a vector) in the space Sn . All three different equivalent ways of\nlooking at X will be useful.\nWhat will a linear function of X look like? If C(X) is a linear function\nof X, then C(X) can be written as C\nX, where\n-\nn\nn\nC\nX :=\nCijXij.\n-\ni=1 j=1\nIf X is a symmetric matrix, there is no loss of generality in assuming that\n\nthe matrix C is also symmetric. With this notation, we are now ready to\ndefine a semidefinite program. A semidefinite program (SDP) is an opti\nmization problem of the form:\nSDP :\nminimize\nC\nX\n-\ns.t.\nAi\nX = bi , i = 1, . . . , m,\n-\nX ⪰ 0,\nNotice that in an SDP that the variable is the matrix X, but it might\nbe helpful to think of X as an array of n2 numbers or simply as a vector\nin Sn . The objective function is the linear function C\nX and there are m\n-\nlinear equations that X must satisfy, namely Ai\nX = bi\n, i = 1, . . . , m.\n-\nThe variable X also must lie in the (closed convex) cone of positive semidef\ninite symmetric matrices Sn\nNote that the data for SDP consists of the\n+.\nsymmetric matrix C (which is the data for the objective function) and the\nm symmetric matrices A1, . . . , Am, and the m-vector b, which form the m\nlinear equations.\nLet us see an example of an SDP for n = 3 and m = 2. Define the\nfollowing matrices:\n\nA1 = 0\n7 ,\nA2 = 2\n0 ,\nand C = 2\n0 ,\nand b1 = 11 and b2 = 19. Then the variable X will be the 3 × 3 symmetric\nmatrix:\n\nx11\nx12\nx13\n\nX =\nx21\nx22\nx23\n,\nx31\nx32\nx33\nand so, for example,\nC\nX\n=\nx11 + 2x12 + 3x13 + 2x21 + 9x22 + 0x23 + 3x31 + 0x32 + 7x33\n-\n=\nx11 + 4x12 + 6x13 + 9x22 + 0x23 + 7x33.\nsince, in particular, X is symmetric. Therefore the SDP can be written\nas:\nSDP :\nminimize\nx11 + 4x12 + 6x13 + 9x22 + 0x23 + 7x33\ns.t.\nx11 + 0x12 + 2x13 + 3x22 + 14x23 + 5x33\n=\n0x11 + 4x12 + 16x13 + 6x22 + 0x23 + 4x33\n=\n\nx11\nx12\nx13\nX = x21\nx22\nx23 ⪰ 0.\nx31\nx32\nx33\nNotice that SDP looks remarkably similar to a linear program. However,\nthe standard LP constraint that x must lie in the nonnegative orthant is re\nplaced by the constraint that the variable X must lie in the cone of positive\nsemidefinite matrices. Just as \"x ≥ 0\" states that each of the n components\nof x must be nonnegative, it may be helpful to think of \"X ⪰ 0\" as stating\nthat each of the n eigenvalues of X must be nonnegative.\n\nIt is easy to see that a linear program LP is a special instance of an\nSDP. To see one way of doing this, suppose that (c, a1, . . . , am, b1, . . . , bm)\ncomprise the data for LP. Then define:\nai1\n. . .\nc1\n. . .\nai2\n. . .\nc2\n. . .\nAi =\n\n. . .\n. . .\n. . .\n. . .\n\n, i = 1, . . . , m,\nand C =\n\n. . .\n. . .\n. . .\n. . .\n\n.\n. . .\nain\n. . .\ncn\nThen LP can be written as:\nSDP :\nminimize\nC\nX\n-\ns.t.\nAi\nX = bi , i = 1, . . . , m,\n-\nXij = 0,\ni = 1, . . . , n,\nj = i + 1, . . . , n,\nX ⪰ 0,\nwith the association that\n\nx1\n. . .\nx2\n. . .\n\nX = .\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n. . .\nxn\nOf course, in practice one would never want to convert an instance of LP\ninto an instance of SDP. The above construction merely shows that SDP\nincludes linear programming as a special case.\n\nP\nP\nP\nX\nX\nSemidefinite Programming Duality\nThe dual problem of SDP is defined (or derived from first principles) to be:\nm\nSDD :\nmaximize\nyibi\ni=1\nm\ns.t.\nyiAi + S = C\ni=1\nS ⪰ 0.\nOne convenient way of thinking about this problem is as follows. Given mul\nm\ntipliers y1, . . . , ym, the objective is to maximize the linear function\ni=1 yibi.\nThe constraints of SDD state that the matrix S defined as\nm\nS = C -\nyiAi\ni=1\nmust be positive semidefinite. That is,\nm\nC -\nyiAi ⪰ 0.\ni=1\nWe illustrate this construction with the example presented earlier.\nThe\ndual problem is:\nSDD :\nmaximize\n11y1 + 19y2\n\ns.t.\ny1 0\n7 + y2 2\n0 + S = 2\nS ⪰ 0,\n\nP\nwhich we can rewrite in the following form:\nSDD :\nmaximize\n11y1 + 19y2\ns.t.\n\n1 - 1y1 - 0y2\n2 - 0y1 - 2y2\n3 - 1y1 - 8y2\n\n2 - 0y1 - 2y2\n9 - 3y1 - 6y2\n0 - 7y1 - 0y2 ⪰ 0.\n3 - 1y1 - 8y2\n0 - 7y1 - 0y2\n7 - 5y1 - 4y2\nIt is often easier to \"see\" and to work with a semidefinite program when\nit is presented in the format of the dual SDD, since the variables are the m\nmultipliers y1, . . . , ym.\nAs in linear programming, we can switch from one format of SDP (pri\nmal or dual) to any other format with great ease, and there is no loss of\ngenerality in assuming a particular specific format for the primal or the\ndual.\nThe following proposition states that weak duality must hold for the\nprimal and dual of SDP:\nProposition 5.1 Given a feasible solution X of SDP and a feasible solu\nm\ntion (y, S) of SDD, the duality gap is C\nX -\ni=1 yibi = S\nX ≥ 0. If\nPm\n-\n-\nC X -\ni=1 yibi = 0, then X and (y, S) are each optimal solutions to SDP\n-\nand SDD, respectively, and furthermore, SX = 0.\nIn order to prove Proposition 5.1, it will be convenient to work with the\ntrace of a matrix, defined below:\n\n-\n-\nX\nX\nX\nn\ntrace(M) =\nMjj.\nj=1\nSimple arithmetic can be used to establish the following two elementary\nidentities:\n- trace(MN) = trace(NM)\nA\nB = trace(AT B)\nProof of Proposition 5.1. For the first part of the proposition, we\nmust show that if S ⪰ 0 and X ⪰ 0, then S\nX ≥ 0. Let S = PDP T and\n-\nX = QEQT where P, Q are orthonormal matrices and D, E are nonnegative\ndiagonal matrices. We have:\nS\nX = trace(ST X) = trace(SX) = trace(PDP T QEQT )\n-\nn\n= trace(DP T QEQT P) =\nDjj(P T QEQT P)jj ≥ 0,\nj=1\nwhere the last inequality follows from the fact that all Djj ≥ 0 and the fact\nthat the diagonal of the symmetric positive semidefinite matrix P T QEQT P\nmust be nonnegative.\nTo prove the second part of the proposition, suppose that trace(SX) = 0.\nThen from the above equalities, we have\nn\nDjj(P T QEQT P)jj = 0.\nj=1\n\nHowever, this implies that for each j = 1, . . . , n, either Djj = 0 or the\n(P T QEQT P)jj = 0. Furthermore, the latter case implies that the jth row\nof P T QEQT P is all zeros.\nTherefore DP T QEQT P = 0, and so SX =\nPDP T QEQT = 0.\nq.e.d.\nUnlike the case of linear programming, we cannot assert that either SDP\nor SDD will attain their respective optima, and/or that there will be no\nduality gap, unless certain regularity conditions hold. One such regularity\ncondition which ensures that strong duality will prevail is a version of the\n\"Slater condition\", summarized in the following theorem which we will not\nprove:\nTheorem 5.1 Let z ∗\nP and z ∗denote the optimal objective function values\nD\n∗\nof SDP and SDD, respectively. Suppose that there exists a feasible solution\nXˆ of SDP such that Xˆ ≻ 0, and that there exists a feasible solution (ˆy, Sˆ)\nof SDD such that Sˆ ≻ 0. Then both SDP and SDD attain their optimal\n∗\nvalues, and zP = zD.\nKey Properties of Linear Programming that do\nnot extend to SDP\nThe following summarizes some of the more important properties of linear\nprogramming that do not extend to SDP:\n- There may be a finite or infinite duality gap. The primal and/or dual\nmay or may not attain their optima.\nAs noted above in Theorem\n5.1, both programs will attain their common optimal value if both\nprograms have feasible solutions in the interior of the semidefinite cone.\n- There is no finite algorithm for solving SDP.\nThere is a simplex\nalgorithm, but it is not a finite algorithm. There is no direct analog\nof a \"basic feasible solution\" for SDP.\n\nP P\n- Given rational data, the feasible region may have no rational solutions.\nThe optimal solution may not have rational components or rational\neigenvalues.\n- Given rational data whose binary encoding is size L, the norms of any\nfeasible and/or optimal solutions may exceed 22L (or worse).\n- Given rational data whose binary encoding is size L, the norms of any\nfeasible and/or optimal solutions may be less than 2-2L (or worse).\nSDP in Combinatorial Optimization\nSDP has wide applicability in combinatorial optimization. A number of\nNP-hard combinatorial optimization problems have convex relaxations that\nare semidefinite programs. In many instances, the SDP relaxation is very\ntight in practice, and in certain instances in particular, the optimal solution\nto the SDP relaxation can be converted to a feasible solution for the origi\nnal problem with provably good objective value. An example of the use of\nSDP in combinatorial optimization is given below.\n7.1\nAn SDP Relaxation of the MAX CUT Problem\nLet G be an undirected graph with nodes N = {1, . . . , n}, and edge set E.\nLet wij = wji be the weight on edge (i, j), for (i, j) ∈ E. We assume that\nwij ≥ 0 for all (i, j) ∈ E. The MAX CUT problem is to determine a subset\nS of the nodes N for which the sum of the weights of the edges that cross\nfrom S to its complement S is maximized (where (S := N \\ S) .\nWe can formulate MAX CUT as an integer program as follows. Let xj = 1\n\nfor j ∈ S and xj = -1 for j ∈ S. Then our formulation is:\nn\nn\nMAXCUT :\nmaximizex\nwij(1 - xixj)\n4 i=1 j=1\ns.t.\nxj ∈ {-1, 1},\nj = 1, . . . , n.\n\nP P\nP P\nNow let\nY = xx T ,\nwhereby\nYij = xixj\ni = 1, . . . , n,\nj = 1, . . . , n.\nAlso let W be the matrix whose (i, j)th element is wij for i = 1, . . . , n\nand j = 1, . . . , n. Then MAX CUT can be equivalently formulated as:\nn\nn\nMAXCUT :\nmaximizeY,x\ni=1 j=1\nwij - W - Y\ns.t.\nxj ∈ {-1, 1},\nj = 1, . . . , n\nY = xxT .\nNotice in this problem that the first set of constraints are equivalent to\nYjj = 1, j = 1, . . . , n. We therefore obtain:\nn\nn\nMAXCUT :\nmaximizeY,x\ni=1 j=1\nwij - W - Y\ns.t.\nYjj = 1,\nj = 1, . . . , n\nY = xxT .\n\nP P\nLast of all, notice that the matrix Y = xxT is a symmetric rank-1 posi\ntive semidefinite matrix. If we relax this condition by removing the rank\n1 restriction, we obtain the following relaxtion of MAX CUT, which is a\nsemidefinite program:\nn\nn\nRELAX :\nmaximizeY\nwij - W\nY\ni=1 j=1\n-\ns.t.\nYjj = 1,\nj = 1, . . . , n\nY ⪰ 0.\nIt is therefore easy to see that RELAX provides an upper bound on MAX\nCUT, i.e.,\nMAXCUT ≤ RELAX.\nAs it turns out, one can also prove without too much effort that:\n0.87856 RELAX ≤ MAXCUT ≤ RELAX.\nThis is an impressive result, in that it states that the value of the semidefi\nnite relaxation is guaranteed to be no more than 12% higher than the value\nof NP-hard problem MAX CUT.\nSDP in Convex Optimization\nAs stated above, SDP has very wide applications in convex optimization.\nThe types of constraints that can be modeled in the SDP framework include:\nlinear inequalities, convex quadratic inequalities, lower bounds on matrix\n\nnorms, lower bounds on determinants of symmetric positive semidefinite\nmatrices, lower bounds on the geometric mean of a nonnegative vector, plus\nmany others. Using these and other constructions, the following problems\n(among many others) can be cast in the form of a semidefinite program:\nlinear programming, optimizing a convex quadratic form subject to convex\nquadratic inequality constraints, minimizing the volume of an ellipsoid that\ncovers a given set of points and ellipsoids, maximizing the volume of an\nellipsoid that is contained in a given polytope, plus a variety of maximum\neigenvalue and minimum eigenvalue problems. In the subsections below we\ndemonstrate how some important problems in convex optimization can be\nre-formulated as instances of SDP.\n8.1\nSDP for Convex Quadratically Constrained Quadratic\nProgramming, Part I\nA convex quadratically constrained quadratic program is a problem of the\nform:\nQCQP :\nminimize\nxT Q0x + q0\nTx + c0\nx\ns.t.\nxT Qix + qi\nT x + ci ≤ 0 , i = 1, . . . , m,\nwhere the Q0 ⪰ 0 and Qi ⪰ 0,\ni = 1, . . . , m. This problem is the same as:\nQCQP :\nminimize\nθ\nx, θ\ns.t.\nxT Q0x + q0\nTx + c0 - θ ≤ 0\nxT Qix + qi\nT x + ci ≤ 0 , i = 1, . . . , m.\nWe can factor each Qi into\nQi = Mi\nT Mi\n\n!\n\n!\n\n!\n\n!\nfor some matrix Mi. Then note the equivalence:\nxT\nI\nMi\nT\n-ci\nM\n-\nix\nqi\nT x\n⪰ 0\n⇔\nx T Qix + qi\nT x + ci ≤ 0.\nIn this way we can write QCQP as:\nQCQP :\nminimize\nθ\nx, θ\ns.t.\nI\nM0x\nxT M0\nT\n-c0 - q0\nT x + θ\n⪰ 0\nI\nMix\nxT MT\ni\nT x\n⪰ 0 , i = 1, . . . , m.\ni\n-ci - q\nNotice in the above formulation that the variables are θ and x and that\nall matrix coefficients are linear functions of θ and x.\n8.2\nSDP for Convex Quadratically Constrained Quadratic\nProgramming, Part II\nAs it turns out, there is an alternative way to formulate QCQP as a semi-\ndefinite program. We begin with the following elementary proposition.\nProposition 8.1 Given a vector x ∈Rk and a matrix W ∈Rk×k, then\nxT\n⪰ 0\nif and only if\nW ⪰ xx T .\nx\nW\n\n!\n!\n\n!\n!\n\n!\nUsing this proposition, it is straghtforward to show that QCQP is equiv\nalent to the following semi-definite program:\nQCQP :\nminimize\nθ\nx, W, θ\ns.t.\nc0\n- θ\n2q0\nT\nxT\n≥ 0\nq0\nQ0\n-\nx\nW\nci\n1 qi\nT\nxT\n1 qi\nQi\n-\nx\nW\n≥ 0 , i = 1, . . . , m\nxT\n.\n⪰ 0\nx\nW\nNotice in this formulation that there are now (n+1)(\nn+2) variables, but that\nthe constraints are all linear inequalities as opposed to semi-definite inequal\nities.\n8.3\nSDP for the Smallest Circumscribed Ellipsoid Problem\nA given matrix R ≻ 0 and a given point z can be used to define an ellipsoid\nin Rn:\nER,z := {y | (y - z)T R(y - z) ≤ 1}.\nOne can prove that the volume of ER,z is proportional to det(R-1).\n\n!\nSuppose we are given a convex set X ∈Rn described as the convex hull\nof k points c1, . . . , ck. We would like to find an ellipsoid circumscribing these\nk points that has minimum volume. Our problem can be written in the fol\nlowing form:\nMCP :\nminimize\nvol (ER,z)\nR, z\ns.t.\nci ∈ ER,z,\ni = 1, . . . , k,\nwhich is equivalent to:\nMCP :\nminimize\n- ln(det(R))\nR, z\ns.t.\n(ci - z)T R(ci - z) ≤ 1,\ni = 1, . . . , k\nR ≻ 0,\nNow factor R = M2 where M ≻ 0 (that is, M is a square root of R),\nand now MCP becomes:\nMCP :\nminimize\n- ln(det(M2))\nM, z\ns.t.\n(ci - z)T MT M(ci - z) ≤ 1,\ni = 1, . . . , k,\nM ≻ 0.\nNext notice the equivalence:\nI\nMci - Mz\n(Mci - Mz)T\n⪰ 0\n⇔\n(ci - z)T MT M(ci - z) ≤ 1\n\n!\nIn this way we can write MCP as:\nMCP :\nminimize\nM, z\n-2 ln(det(M))\n\n!\ns.t.\nI\n(Mci - Mz)T\nMci - Mz\n⪰ 0,\ni = 1, . . . , k,\nM ≻ 0.\nLast of all, make the substitution y = Mz to obtain:\nMCP :\nminimize\n-2 ln(det(M))\nM, y\ns.t.\n(Mci\nI\n- y)T\nMci\n- y\n⪰ 0,\ni = 1, . . . , k,\nM ≻ 0.\nNotice that this last program involves semidefinite constraints where all of\nthe matrix coefficients are linear functions of the variables M and y. How\never, the objective function is not a linear function. It is possible to convert\nthis problem further into a genuine instance of SDP, because there is a way\nto convert constraints of the form\n- ln(det(X)) ≤ θ\nto a semidefinite system. Nevertheless, this is not necessary, either from\na theoretical or a practical viewpoint, because it turns out that the function\nf(X) = - ln(det(X)) is extremely well-behaved and is very easy to optimize\n(both in theory and in practice).\n\nFinally, note that after solving the formulation of MCP above, we can\nrecover the matrix R and the center z of the optimal ellipsoid by computing\nR = M2 and z = M-1 y.\n8.4\nSDP for the Largest Inscribed Ellipsoid Problem\nRecall that a given matrix R ≻ 0 and a given point z can be used to define\nan ellipsoid in Rn:\nER,z := {x | (x - z)T R(x - z) ≤ 1},\nand that the volume of ER,z is proportional to det(R-1).\nSuppose we are given a convex set X ∈Rn described as the intersection\nof k halfspaces {x | (ai)T x ≤ bi}, i = 1, . . . , k, that is,\nX = {x | Ax ≤ b}\nwhere the ith row of the matrix A consists of the entries of the vector\nai, i = 1, . . . , k. We would like to find an ellipsoid inscribed in X of maxi\nmum volume. Our problem can be written in the following form:\nMIP :\nmaximize\nvol (ER,z)\nR, z\ns.t.\nER,z ⊂ X,\nwhich is equivalent to:\n\nq\nMIP :\nmaximize\ndet(R-1)\nR, z\ns.t.\nER,z ⊂ {x | (ai)T x ≤ bi},\ni = 1, . . . , k\nR ≻ 0,\nwhich is equivalent to:\nMIP :\nmaximize\nln(det(R-1))\nR, z\ns.t.\nmaxx{ai\nT x | (x - z)T R(x - z) ≤ 1} ≤ bi,\ni = 1, . . . , k\nR ≻ 0.\nFor a given i = 1, . . . , k, the solution to the optimization problem in the\nith constraint is\n∗\nR-1ai\nx = z + q\naT R-1ai\ni\nwith optimal objective function value\nai\nT z +\nai\nT R-1ai ,\nand so MIP can be rewritten as:\n\nq\nq\nMIP :\nmaximize\nln(det(R-1))\nR, z\ns.t.\naT\ni z +\naT\ni R-1ai\n≤ bi,\ni = 1, . . . , k\nR ≻ 0.\nNow factor R-1 = M2 where M ≻ 0 (that is, M is a square root of R-1),\nand now MIP becomes:\nMIP :\nmaximize\nln(det(M2))\nM, z\ns.t.\naT\ni z +\naT\ni MT Mai\n≤ bi,\ni = 1, . . . , k\nM ≻ 0,\nwhich we can re-write as:\nMIP :\nmaximize\n2 ln(det(M))\nM, z\ns.t.\naT MT Mai ≤ (bi - ai\nT z)2 ,\ni = 1, . . . , k\ni\nbi - ai\nT z ≥ 0,\ni = 1, . . . , k\nM ≻ 0.\nNext notice the equivalence:\n\n(bi - ai z)I\nMai\n!\naT MT Mai ≤ (bi - aT\ni z)2\nT\n\ni\n\n(Mai)T\ni z)\n⪰ 0\n\nand\n\n(bi - aT\n⇔\n\nbi - ai\nT z ≥ 0.\n\n!\nIn this way we can write MIP as:\nMIP :\nminimize\n-2 ln(det(M))\nM, z\n(bi - ai\nT z)I\nMai\ns.t.\n(Mai)T\n(bi - aT\ni z)\n⪰ 0,\ni = 1, . . . , k,\nM ≻ 0.\nNotice that this last program involves semidefinite constraints where all of\nthe matrix coefficients are linear functions of the variables M and z. How\never, the objective function is not a linear function. It is possible, but not\nnecessary in practice, to convert this problem further into a genuine instance\nof SDP, because there is a way to convert constraints of the form\n- ln(det(X)) ≤ θ\nto a semidefinite system. Such a conversion is not necessary, either from\na theoretical or a practical viewpoint, because it turns out that the function\nf(X) = - ln(det(X)) is extremely well-behaved and is very easy to optimize\n(both in theory and in practice).\nFinally, note that after solving the formulation of MIP above, we can\nrecover the matrix R of the optimal ellipsoid by computing\nR = M-2 .\n8.5\nSDP for Eigenvalue Optimization\nThere are many types of eigenvalue optimization problems that can be for\nmualated as SDPs. A typical eigenvalue optimization problem is to create\n\nX\na matrix\nk\nS := B -\nwiAi\ni=1\ngiven symmetric data matrices B and Ai, i = 1, . . . , k, using weights w1, . . . , wk,\nin such a way to minimize the difference between the largest and smallest\neigenvalue of S. This problem can be written down as:\nEOP :\nminimize\nw, S\nλmax(S) - λmin(S)\nk P\ns.t.\nS = B -\ni=1\nwiAi,\nwhere λmin(S) and λmax(S) denote the smallest and the largest eigenvalue\nof S, respectively. We now show how to convert this problem into an SDP.\nRecall that S can be factored into S = QDQT where Q is an orthonor\nmal matrix (i.e., Q-1 = QT ) and D is a diagonal matrix consisting of the\neigenvalues of S. The conditions:\nλI ⪯ S ⪯ μI\ncan be rewritten as:\nQ(λI)QT ⪯ QDQT ⪯ Q(μI)QT .\nAfter premultiplying the above QT and postmultiplying Q, these conditions\nbecome:\n\nP\nλI ⪯ D ⪯ μI\nwhich are equivalent to:\nλ ≤ λmin(S) and λmax(S) ≤ μ.\nTherefore EOP can be written as:\nEOP :\nminimize\nμ - λ\nw, S, μ, λ\nk\ns.t.\nS = B -\nwiAi\ni=1\nλI ⪯ S ⪯ μI.\nThis last problem is a semidefinite program.\nUsing constructs such as those shown above, very many other types of\neigenvalue optimization problems can be formulated as SDPs.\nSDP in Control Theory\nA variety of control and system problems can be cast and solved as instances\nof SDP. However, this topic is beyond the scope of these notes.\n\nX\nY\nInterior-point Methods for SDP\nAt the heart of an interior-point method is a barrier function that exerts\na repelling force from the boundary of the feasible region. For SDP, we\nneed a barrier function whose values approach +inf as points X approach\nthe boundary of the semidefinite cone Sn\n+.\nLet X ∈ S+\nn . Then X will have n eigenvalues, say λ1(X), . . . , λn(X)\n(possibly counting multiplicities). We can characterize the interior of the\nsemidefinite cone as follows:\nintSn\nλ1(X) > 0, . . . , λn(X)\n+ = {X ∈ Sn |\n> 0}.\nA natural barrier function to use to repel X from the boundary of Sn\n+\nthen is\nn\nn\n-\nln(λi(X)) = - ln(\nλi(X)) = - ln(det(X)).\nj=1\nj=1\nConsider the logarithmic barrier problem BSDP(θ) parameterized by\nthe positive barrier parameter θ:\nBSDP(θ) :\nminimize\nC\nX - θ ln(det(X))\n-\ns.t.\nAi\nX = bi , i = 1, . . . , m,\n-\nX ≻ 0.\nLet fθ(X) denote the objective function of BSDP(θ). Then it is not too\ndifficult to derive:\n\nP\n∇fθ(X) = C - θX-1 ,\n(1)\nand so the Karush-Kuhn-Tucker conditions for BSDP(θ) are:\n\nAi\nX = bi , i = 1, . . . , m,\n\n-\n\nX ≻ 0,\n(2)\n\nm\n\nC - θX-1 =\nyiAi.\n\ni=1\nBecause X is symmetric, we can factorize X into X = LLT .\nWe then\ncan define\nS = θX-1 = θL-TL-1 ,\nwhich implies\n1 LT SL = I,\nθ\nand we can rewrite the Karush-Kuhn-Tucker conditions as:\n\nP\nX X\nX\nX\nv\nu X X\n\nAi\nX = bi , i = 1, . . . , m,\n\n-\n\nX ≻ 0, X = LLT\n\n(3)\nm\n\nyiAi + S = C\n\ni=1\n\nI - θLT SL = 0.\nFrom the equations of (3) it follows that if (X, y, S) is a solution of (3), then\nX is feasible for SDP, (y, S) is feasible for SDD, and the resulting duality\ngap is\nn\nn\nn\nn\nS\nX =\nSijXij =\n(SX)jj =\nθ = nθ.\n-\ni=1 j=1\nj=1\nj=1\nThis suggests that we try solving BSDP(θ) for a variety of values of θ\nas θ\n0.\n→\nHowever, we cannot usually solve (3) exactly, because the fourth equation\ngroup is not linear in the variables. We will instead define a \"β-approximate\nsolution\" of the Karush-Kuhn-Tucker conditions (3). Before doing so, we\nintroduce the following norm on matrices, called the Frobenius norm:\nu n\nn\n∥M∥ :=\n√\nM\nM = t\nM2\n.\n-\nij\ni=1 j=1\nFor some important properties of the Frobenius norm, see the last subsection\nof this section. A \"β-approximate solution\" of BSDP(θ) is defined as any\nsolution (X, y, S) of\n\n-\n-\nP\nX\n\nAi\nX = bi , i = 1, . . . , m,\n\n-\n\nX ≻ 0, X = LLT\n\n(4)\nm\n\nyiAi + S = C\n\ni=1\n\n∥I - θLT SL∥≤ β.\n\nLemma 10.1 If (X, y, S) is a β-approximate solution of BSDP(θ) and\n\nβ < 1, then X is feasible for SDP, ( y, S) is feasible for SDD, and the\nduality gap satisfies:\nm\nnθ(1 - β) ≤ C\nX -\nyibi = X\nS ≤ nθ(1 + β).\n(5)\ni=1\nProof: Primal feasibility is obvious. To prove dual feasibility, we need\nto show that S ⪰ 0. To see this, define\nR = I - 1 L T S L\n(6)\nθ\nand note that ∥R∥≤ β < 1. Rearranging (6), we obtain\nS = θL -T(I - R)L -1 ≻ 0\nbecause ∥R∥ < 1 implies that I -R ≻ 0. We also have X\nS = trace( X S ) =\n-\ntrace(L L T S ) = trace(L T S L ) = θtrace(I - R) = θ(n - trace(R)). However,\n|trace(R)| ≤√n∥R∥≤ nβ, whereby we obtain\n\nnθ(1 - β) ≤ X\nS ≤ nθ(1 + β).\n-\nq.e.d.\n\nP\n10.1\nThe Algorithm\nBased on the analysis just presented, we are motivated to develop the fol\nlowing algorithm:\nStep 0 . Initialization. Data is (X0, y0, S0, θ0). k = 0. Assume that\n(X0, y0, S0) is a β-approximate solution of BSDP(θ0) for some known value\nof β that satisfies β < 1.\nStep 1. Set Current values. (X, y, S) =\n, yk, Sk), θ = θk .\n\n(Xk\n′\nStep 2. Shrink θ. Set θ = αθ for some α ∈ (0, 1). In fact, it will be\nappropriate to set\n√β - β\nα = 1 -√β + √n\nStep 3.\nCompute Newton Direction and Multipliers. Compute\nthe Newton step D\n′ for BSDP(θ\n′ ) at X = X by factoring X = L L T and\nsolving the following system of equations in the variables (D, y):\n\nm\n\n=\n\n′ X -1 + θ\n′ X -1DX -1\nyiAi\nC - θ\ni=1\n(7)\n\nAi\nD = 0,\ni = 1, . . . , m.\n-\n′\n′\nDenote the solution to this system by (D , y ).\nStep 4. Update All Values.\n′\n′\n\nX = X + D\nm\n′\nX\n′\nS = C -\nyiAi\ni=1\n′\n′\n′\nStep 5. Reset Counter and Continue. (Xk+1, yk+1, Sk+1) = (X , y , S ).\n′\nθk+1 = θ . k\nk + 1. Go to Step 1.\n←\n\nA picture of this algorithm looks something like this:\n\nSome of the unresolved issues regarding this algorithm include:\n- how to set the fractional decrease parameter α\n′\n′\n- the derivation of the Newton step D and the multipliers y\nwhether or not successive iterative values (Xk, yk, Sk) are β-approximate\n-\nsolutions to BSDP(θk), and\n- how to get the method started in the first place.\n10.2\nThe Newton Step\nSuppose that X is a feasible solution to BSDP(θ):\nBSDP(θ) :\nminimize\nC\nX - θ ln(det(X))\n-\ns.t.\nAi\nX = bi , i = 1, . . . , m,\n-\nX ≻ 0.\nLet us denote the objective function of BSDP(θ) by fθ(X), i.e.,\nfθ(X) = C\nX - θ ln(det(X)).\n-\nThen we can derive:\n∇fθ(X ) = C - θX -1\n\nand the quadratic approximation of BSDP(θ) at X = X can be derived\nas:\n\n-\n-\nP\nP\nminimize\nfθ(X ) + (C - X -1)\n(X - X ) + 1θX -1(X - X )\nX -1(X - X )\n-\n-\nX\ns.t.\nAi\nX = bi,\ni = 1, . . . , m.\n-\n\nLetting D = X - X, this is equivalent to:\nminimize\n(C - θX -1)\nD + 2\n1θX -1D\nX -1D\nD\ns.t.\nAi\nD = 0,\ni = 1, . . . , m.\n-\nThe solution to this program will be the Newton direction. The Karush-\nKuhn-Tucker conditions for this program are necessary and sufficient, and\nare:\n\nm\n\nX -1\n\nX -1\n\n+ θX-1D\n=\nyiAi\nC - θ\ni=1\n(8)\n\nAi - D = 0,\ni = 1, . . . , m.\n′\n′\nThese equations are called the Normal Equations. Let D and y denote the\nsolution to the Normal Equations. Note in particular from the first equation\n′\n′\n′\nin (8) that D must be symmetric. Suppose that (D , y ) is the (unique) so\nlution of the Normal Equations (8). We obtain the new value of the primal\nvariable X by taking the Newton step, i.e.,\n′\n′\n\nX = X + D .\nWe can produce new values of the dual variables (y, S) by setting the new\nm\n′\n′\n′\nvalue of y to be y and by setting S = C -\ny Ai. Using (8), then, we\ni=1\nhave that\n\nX\nX\n′\n′\nS = θX -1 - θX -1D X -1 .\n(9)\nWe have the following very powerful convergence theorem which demon\nstrates the quadratic convergence of Newton's method for this problem,\nwith an explicit guarantee of the range in which quadratic convergence takes\nplace.\nTheorem 10.1 (Explicit Quadratic Convergence of Newton's Method).\n\nSuppose that (X, y,\nis a β-approximate solution of BSDP(θ) and β < 1.\nS)\n′\n′\nLet (D , y ) be the solution to the Normal Equations (8), and let\n′\n′\n\nX = X + D\nand\n′\n′\nS = θX -1 - θX -1D X -1 .\n′\n′\n′\nThen (X , y , S ) is a β2-approximate solution of BSDP(θ).\nProof: Our current point X satisfies:\nAi\nX = bi, i = 1, . . . , m, X = L L T ≻ 0\n-\nm\ny iAi + S = C\ni=1\n∥I - θL T S L ∥≤ β < 1.\n′\n′\nFurthermore the Newton direction D and multipliers y satisfy:\n′\nAi - D = 0, i = 1, . . . , m\nm\n′\n′\nyiAi + S = C\ni=1\n′\n′\n′\nX = X + D = L (I + L -1D L -T )L T\n\n-\n-\n-\n-\n-\n-\nX\nX\n\n′\n′\n′\nS = θX -1 - θX -1D X -1 = θL -T(I - L -1D L -T )L -1 .\nWe will first show that ∥L -1D\n′ L -T ∥≤ β.\nIt turns out that this is the\ncrucial fact from which everything will follow nicely. To prove this, note\nthat\nm\nm\nm\n′\n′\nX\n′\n′\ny iAi + S = C =\nyiAi + S =\nyiAi + θL -T(I - L -1D L -T )L -1 .\ni=1\ni=1\ni=1\n′\nTaking the inner product with D yields:\n′\n′\n′\n′\nS\nD = θL -TL -1\nD - θL -TL -1D L -T L -1\nD ,\nwhich we can rewrite as:\n′\n′\n′\n′\nL T S L\nL -1D L -T = θI\nL -1D L -T - θL -1D L -T\nL -1D L -T ,\nwhich we finally rewrite as:\n∥L -1D\n′ L -T ∥ 2 =\nI - θ\n1 L T S L\n- L -1D\n′ L -T .\nInvoking the Cauchy-Schwartz inequality we obtain:\n∥L -1D\n′ L -T ∥2 ≤∥I - θ\n1 L T S L ∥∥ L -1D\n′ L -T ∥≤ β∥L -1D\n′ L -T ∥,\nfrom which we see that ∥L -1D\n′ L -T ∥≤ β.\nIt therefore follows that\n′\n′\nX = L (I + L -1D L -T )L T ≻ 0\nand\n′\n′\nS = θL -T(I - L -1D L -T )L -1 ≻ 0,\nsince ∥L -1D\n′ L -T ∥≤ β < 1, which guarantees that I ± L -1D\n′ L -T ≻ 0.\nNext, factorize\n′\nI + L -1D L -T = M2 ,\n(where M = MT ) and note that\n′\n′\n′\n\nX = LMML T = L (L )T\n\nP\n\n′\n\nwhere we define L = LM. Then note that\n′\n′\n′\n\nI - θ(L )T S ′ L = I - θMLT S LM\n′\n′\n= I - θM\nL-T(I -\nL -T )\nLM\nI - M(I - L -1D\nLT (θ\nL-1D\nL-1)\n=\nL -T)M\n′\n= I-MM+M(L -1D L -T )M = I-MM+M(MM-I)M = (I-MM)(I-MM)\n′\n′\n= (L -1D L -T )(L -1D L -T ).\nFrom this we next have:\n′\n′\n′\n′\n′\n∥I - θ(L )T S ′ L ∥ = ∥(L -1D L -T )(L -1D L -T )∥≤∥(L -1D L -T)∥ 2 ≤ β2 .\n′\n′\n′\nThis shows that (X , y , S ) is a β2-approximate solution of BSDP(θ).\nq.e.d.\n10.3\nComplexity Analysis of the Algorithm\n\nTheorem 10.2 (Relaxation Theorem). Suppose that (X, y, S) is a β\n\napproximate solution of BSDP(θ) and β < 1. Let\n√β - β\nα = 1 -√β + √n\n′\n′\n\nand let θ = αθ. Then (X, y, S) is a √β-approximate solution of BSDP(θ ).\n\nProof: The triplet ( X, y, S) satisfies Ai\nX = bi, i = 1, . . . , m, X ≻ 0, and\n\nm\n-\ny iAi + S = C, and so it remains to show that\ni=1\n\np\nL T\n\nθ\n′\nSL - I ≤\nβ,\nwhere X = L L T . We have\n\nθ\n′ LT S L - I\n\n=\n\nαθ\nLT S L - I\n\n=\n\nα\nθ\nLT S L - I\n- 1 - 1\nα\nI\n\n1 LT S\n\n+\n1 - α\n≤\nL - I\n∥I∥\nα\nθ\nα\n\np\np\n≤ α\nβ +\n1 -\nα\nα\n√n\n=\nβ +\nα\n√n -√n\n=\nβ + √n -√n =\nβ.\nq.e.d.\nTheorem 10.3 (Convergence Theorem). Suppose that (X0, y0, S0) is a\nβ-approximate solution of BSDP(θ0) and β < 1. Then for all k = 1, 2, 3, ...,\n(Xk, yk, Sk) is a β-approximate solution of BSDP(θk).\nProof: By induction, suppose that the theorem is true for iterates 0, 1, 2, ..., k.\nThen (Xk, yk, Sk) is a β-approximate solution of BSDP(θk).\nFrom the Relaxation Theorem, (Xk, yk, Sk) is a √β-approximate solution\nof BSDP(θk+1) where θk+1 = αθk .\nFrom the Quadratic Convergence Theorem, (Xk+1, yk+1, Sk+1) is a β-approximate\nsolution of BSDP(θk+1).\nTherefore, by induction, the theorem is true for all values of k.\nq.e.d.\n\nA better picture of this algorithm looks something like this:\n\n&\n\n!'\nX\n\n!\n\n!\n\n!\n\nP\nTheorem 10.4 (Complexity Theorem). Suppose that (X0, y0, S0) is a\nβ = 1\n4-approximate solution of BSDP(θ0). In order to obtain primal and\ndual feasible solutions (Xk, yk, Sk) with a duality gap of at most o, one needs\nto run the algorithm for at most\nk =\n6√n ln\n1.25 X0 - S0\n0.75\no\niterations.\nProof: Let k be as defined above. Note that\nα = 1 - √\n√\nβ\nβ\n+\n-√β\nn = 1 - 2\n+\n-\n√\nn\n= 1 - 2 + 4\n1 √n ≤ 1 - 6√1\nn.\nTherefore\nθk ≤\n\n1 - 6√1\nn\nk\nθ0 .\nThis implies that\nC - Xk -\nm\nbiyi\nk = Xk - Sk ≤ θk n(1 + β) ≤\n\n1 - 6√1\nn\nk\n(1.25nθ0)\ni=1\n\nk\nX0\nS0\n≤ 1 - 6√n\n(1.25n)\n0.75\n-\nn\n,\nfrom (5). Taking logarithms, we obtain\nm\n\nln\nC\nXk\nX\nbiy k\n≤ k ln\n+ ln\n1.25 X0\nS0\n-\n-\ni\n1 - 6√n\n0.75\n-\ni=1\n1.25 X0\n≤ 6\n-√k\nn + ln\n0.75\n- S0\n1.25 X0\nS0\n1.25\n≤- ln\n0.75\no\n-\n+ ln\n0.75 X0 - S0\n= ln(o).\nm\nTherefore C - Xk -\ni=1\nbiyi\nk ≤ o.\nq.e.d.\n\nP\n10.4\nHow to Start the Method from a Strictly Feasible Point\nThe algorithm and its performance relies on having a starting point (X0, y0, S0)\nthat is a β-approximate solution of the problem BSDP(θ0). In this subsec\ntion, we show how to obtain such a starting point, given a positive definite\nfeasible solution X0 of SDP.\nWe suppose that we are given a target value θ0 of the barrier param\neter, and we are given X = X0 that is feasible for BSDP(θ0), that is,\nAi\nX0 = bi, i = 1, . . . , m, and X0 ≻ 0. We will attempt to approximately\n-\nsolve BSDP(θ0) starting at X = X0, using the Newton direction at each\niteration. The formal statement of the algorithm is as follows:\nStep 0 . Initialization. Data is (X0, θ0). k = 0. Assume that X0 satisfies\nAi\nX0 = bi, i = 1, . . . , m, X0 ≻ 0.\n-\nStep 1. Set Current values. X = Xk . Factor X = L L T .\nStep 2. Compute Newton Direction and Multipliers. Compute the\nNewton step D\n′ for BSDP(θ0) at X = X by solving the following system\nof equations in the variables (D, y):\n\nm\n\nX-1 + θ0X -1DX -1\nC - θ0\n=\nyiAi\n\ni=1\n(10)\n\nAi\nD = 0,\ni = 1, . . . , m.\n-\n′\n′\nDenote the solution to this system by (D , y ). Set\nm\n′\nX\n′\nS = C -\nyiAi.\ni=1\nStep 3. Test the Current Point. If ∥L -1D\n′ L -T∥≤ 1, stop. In this\ncase, X is a 1\n4-approximate solution of BSDP(θ0), along with the dual val\n′\n′\nues (y , S ).\n\n∥\nP\n\nP\nStep 4. Update Primal Point.\n′\n′\n\nX = X + αD\nwhere\n0.2\nα =\n.\n∥L -1D\n′ L -T\n\n′\nAlternatively, α can be computed by a line-search of fθ0 (X + αD ).\n′\nStep 5.\nReset Counter and Continue. Xk+1\nX , k\nk + 1.\n←\n←\nGo to Step 1.\nThe following proposition validates Step 3 of the algorithm:\n′\n′\nProposition 10.1 Suppose that (D , y ) is the solution of the Normal equa\ntions (10) for the point X for the given value θ0 of the barrier parameter,\nand that\n′\n∥L -1D L -T ∥≤ 4 .\nThen X is a 1\n4-approximate solution of BSDP(θ0).\nm\nProof: We must exhibit values (y, S) that satisfy\nyiAi + S = C and\ni=1\n\n1 L T SL\n1 .\nI - θ0\n≤ 4\nm\n′\n′\n′\n′\nLet (D , y ) solve the Normal equations (10), and let S = C-\nyiAi. Then\ni=1\nwe have from (10) that\n′\n′\n′\nI- θ0 L T S L = I- θ0 L T (θ0(L -T L -1 -L -T L -1D L -T L -1))L = L -1D L -T ,\nwhereby\n∥I - θ\n0 L T S\n′ L ∥ = ∥L -1D\n′ L -T ∥≤ 4\n1 .\nq.e.d.\n\n!\n\nThe next proposition shows that whenever the algorithm proceeds to\nStep 4, then the objective function fθ0 (X) decreases by at least 0.025θ0:\nProposition 10.2 Suppose that X satisfies Ai\nX = bi, i = 1, . . . , m, and\n′\n′\n-\nX ≻ 0. Suppose that (D , y ) is the solution of the Normal equations (10)\nfor the point X for a given value θ0 of the barrier parameter, and that\n′\n∥L -1D L -T ∥ >\n.\nThen for all γ ∈ [0, 1),\nγ\n′\n′\nγ2\nfθ0 X + ∥L -1D\n′ L -T ∥ D\n≤ fθ0 (X ) + θ0\n-γ∥L -1D L -T ∥ + 2(1 - γ)\n.\nIn particular,\n0.2\n′\nfθ0 X +\nL-1D\n′ L -T D\n≤ fθ0 (X ) - 0.025θ0 .\n(11)\n∥\n∥\nIn order to prove this proposition, we will need two powerful facts about the\nlogarithm function:\nFact 1. Suppose that |x| ≤ δ < 1. Then\nx\nln(1 + x) ≥ x - 2(1 - δ) .\nProof: We have:\nln(1 + x)\n=\nx - x\n2 + x\n3 - x\n4 + . . .\n|x|2\n|x|3\n|x|4\n≥ x - 2 - 3 - 4 - . . .\n|x|2\n|x|3\n|x|4\n≥ x - 2 - 2 - 2 - . . .\n=\nx - x\n2 1 + |x| + |x|2 + |x|3 + . . .\n\nx\n=\nx - 2(1-|x|)\nx\n≥ x - 2(1-δ).\n\ns\nP\nP\n\nq.e.d.\nFact 2. Suppose that R ∈ Sn and that ∥R∥≤ γ < 1. Then\nγ2\nln(det(I + R)) ≥ I\nR - 2(1 - γ) .\n-\nProof: Factor R = QDQT where Q is orthonormal and D is a diagonal\nn\nmatrix of the eigenvalues of R. Then first note that ∥R∥ =\nD2\nWe\njj.\nj=1\nthen have:\nln(det(I + R))\n=\nln(det(I + QDQT))\n=\nln(det(I + D))\nn\n=\nln(1 + Djj)\nj=1\nn P\nD2\n2(1-γ)\n≥\nj=1\nDjj -\njj\n∥R∥2\n=\nI - D - 2(1-γ)\n2(1-γ)\n≥ I - QT RQ -\nγ2\nγ2\n=\nI - R - 2(1-γ)\nq.e.d.\nProof of Proposition 10.2. Let\nγ\nα =\nL-1D\n′ L -T ,\n∥\n∥\nand notice that\n′\n∥αL -1D L -T ∥ = γ.\n\n-\n-\n-\n-\n-\n-\n-\n-\n\nP\nThen\nfθ0 X + ∥L -1D\nγ\n′ L -T ∥ D\n′\n=\nfθ0 X + αD\n′\n=\nC\nX + αC\nD\n′ - θ0 ln(det(L (I + αL -1D\n′ L -T )L T ))\n=\nC\nX - θ0 ln(det( X )) + αC\nD\n′ - θ0 ln(det(I + αL -1D\n′ L -T))\n≤ fθ0 (X ) + αC - D\n′ - θ0αI - L -1D\n′ L -T + θ0\nγ2\n2(1-γ)\n=\nfθ0 (X ) + αC\nD\n′ - θ0αL -T L -1\nD\n′ + θ0\n2(1\nγ\n-\nγ)\n=\nfθ0 (X ) + α\nC - θ0X -1 D\n′ + θ0\nγ2\n-\n2(1-γ).\n′\n′\nNow, (D , y ) solve the Normal equations:\n\nm\n\nX-1 + θ0X -1D X -1\nC - θ0\n′\n=\ny\n′ Ai\n\ni\ni=1\n(12)\n\n′\nAi\nD = 0,\ni = 1, . . . , m.\n-\n′\nTaking the inner product of both sides of the first equation above with D\nand rearranging yields:\n′\n′\n′\nθ0X -1D\nX -1D = -(C - θ0X -1)\nD .\nSubstituting this in our inequality above yields:\n\n&\n'\n&\n'\nfθ0 X + ∥L -1D\nγ\n′ L -T ∥ D\n′\n≤ fθ0 (X ) - αθ0X -1D\n′ - X -1D\n′ + θ0\n2(1\nγ\n-\nγ)\n=\nfθ0 (X ) - αθ0L -1D\n′ L -T\nL -1D\n′ L -T + θ0\n2(1\nγ\n-\nγ)\n-\n=\nfθ0 (X ) - αθ0∥L -1D\n′ L -T ∥2 + θ0\n2(1\nγ\n-\nγ)\n=\nfθ0 (X ) - θ0γ∥L -1D\n′ L -T ∥ + θ0\nγ2\n2(1-γ)\n=\nfθ0 (X ) + θ0 -γ∥L -1D\n′ L -T ∥ + 2(1\nγ\n-\nγ)\n.\nSubsituting γ = 0.2 and and ∥L -1D\n′ L -T ∥ > 1\n4 yields the final result.\nq.e.d.\nLast of all, we prove a bound on the number of iterations that the algo\nrithm will need in order to find a 1\n4-approximate solution of BSDP(θ0):\nProposition 10.3 Suppose that X0 satisfies Ai X0 = bi, i = 1, . . . , m, and\n-\nX0 ≻ 0. Let θ0 be given and let fθ\n∗\n0 be the optimal objective function value\nof BSDP(θ0). Then the algorithm initiated at X0 will find a 1\n4-approximate\nsolution of BSDP(θ0) in at most\nfθ0 (X0) - f ∗\nk =\nθ0\n0.025θ0\niterations.\nProof: This follows immediately from (11). Each iteration that is not a 1\napproximate solution decreases the objective function fθ0 (X) of BSDP(θ0)\nby at least 0.025θ0 . Therefore, there cannot be more than\nfθ0 (X0) - f ∗\nθ0\n0.025θ0\niterations that are not 1\n4-approximate solutions of BSDP(θ0).\nq.e.d.\n\ns\nq\nq\nX\nX\nv\nu\nv\nu X X X\n10.5\nSome Properties of the Frobenius Norm\nProposition 10.4 If M ∈ Sn, then\nn\n1. ∥M∥ =\nP (λj(M))2 , where λ1(M), λ2(M), . . . , λn(M) is an enu\nj=1\nmeration of the n eigenvalues of M.\n2. If λ is any eigenvalue of M, then |λ| ≤∥M∥.\n3. |trace(M)| ≤√n∥M∥.\n4. If ∥M∥ < 1, then I + M ≻ 0.\nProof: We can factorize M = QDQT where Q is orthonormal and D is a\ndiagonal matrix of the eigenvalues of M. Then\n√\nM\n∥M∥ =\n- M =\nQDQT - QDQT =\ntrace(QDQT QDQT )\nv\nq\nq\nu n\nX\nu\ntrace(QT QDQTQD)\n(λj(M))2\n=\n=\ntrace(DD) = t\n.\nj=1\nThis proves the first two assertions. To prove the third assertion, note that\ntrace(M) = trace(QDQT ) = trace(QT QD)\nn\nu n\n= trace(D) =\nλj(M) ≤√nt\n(λj(M))2 = √n∥M∥.\nj=1\nj=1\n′\nTo prove the fourth assertion, let λ be an eigenvalue of I + M.\nThen\n′ λ = 1 + λ where λ is an eigenvalue of M. However, from the second asser\n′\ntion, λ = 1 + λ ≥ 1 -∥M∥ > 0, and so M ≻ 0.\nq.e.d.\nProposition 10.5 If A, B ∈ Sn, then ∥AB∥≤∥A∥∥B∥.\nProof: We have\nu\n\n!2\nu n\nn\nn\n∥AB∥ = t\nAikBkj\ni=1 j=1\nk=1\n\nv\nu X X\nX\nX\nv\nu\n!\nX X\nX X\n(\n)\nX\nP\nP\nP\n\n!\n!!\nu n\nn\nn\nn\nik\nkj\n≤ t\nA2\nB2\ni=1 j=1\nk=1\nk=1\n\nu\nn\nn\nn\nn\n= t\n=\nu\nA2\n\nB2\nik\nkj\n∥A∥∥B∥.\ni=1 k=1\nj=1 k=1\nq.e.d.\nIssues in Solving SDP using the Ellipsoid Al\ngorithm\nTo see how the ellipsoid algorithm is used to solve a semidefinite program,\nassume for convenience that the format of the problem is that of the dual\nproblem SDD. Then the feasible region of the problem can be written as:\nm\nF =\n(y1, . . . , ym) ∈Rm | C -\ni=1\nyiAi ⪰ 0\n,\nand the objective function is\nm\ni=1 yibi. Note that F is just a convex re\ngion in Rm .\nRecall that at any iteration of the ellipsoid algorithm, the set of solutions\nof SDD is known to lie in the current ellipsoid, and the center of the current\nellipsoid is, say, y = ( y1, . . . , y m). If y ∈ F, then we perform an optimality\nm\nm\ncut of the form\ni=1 yibi ≥\ni=1 y ibi, and use standard formulas to update\nthe ellipsoid and its new center. If ∈ F, then we perform a feasibility cut\ny /\nby computing a vector h ∈Rm such that h T y > h T y for all y ∈ F.\nThere are four issues that must be resolved in order to implement the\nabove version of the ellipsoid algorithm to solve SDD:\n\nP\nX\nX\n\n!\n\n!\nX\nX\n\n1. Testing if y ∈ F. This is done by computing the matrix S = C -\nm\ni=1 y iAi. If S ⪰ 0, then y ∈ F. Testing if the matrix S ⪰ 0 can\nbe done by computing an exact Cholesky factorization of S , which\ntakes O(n3) operations, assuming that computing square roots can be\nperformed (exactly) in one operation.\n2. Computing a feasibility cut. As above, testing if S ⪰ 0 can be com\nputed in O(n3) operations. If S ⪰ 0, then again assuming exact arith\nmetic, we can find an n-vector v such that T S\n3) oper\nv\nv < 0 in O(n\nations as well. Then the feasiblity cut vector h is computed by the\nformula:\n\nT Ai\nhi = v\nv,\ni = 1, . . . , m,\nwhose computation requires O(mn2) operations. Notice that for any\ny ∈ F, that\nm\nm\ny T h =\nyiv T Aiv = v T\nyiAi\nv\ni=1\ni=1\nm\nm\nT C\nT\nT\n≤ v\nv < v\ny iAi\nv =\ny iv T Aiv = y h,\ni=1\ni=1\nthereby showing h indeed provides a feasibility cut for F at y = y .\n3. Starting the ellipsoid algorithm. We need to determine an upper bound\n∗\nR on the distance of some optimal solution y\nfrom the origin. This\ncannot be done by examining the input length of the data, as is the case\nin linear programming. One needs to know some special information\nabout the specific problem at hand in order to determine R before\nsolving the semidefinite program.\n4. Stopping the ellipsoid algorithm. Suppose that we seek an o-optimal\nsolution of SDD. In order to prove a complexity bound on the number\nof iterations needed to find an o-optimal solution, we need to know\nbeforehand the radius r of a Euclidean ball that is contained in the\nset of o-optimal solutions of SDD.\nThe value of r also cannot be\n\ndetermined by examining the input length of the data, as is the case\nin linear programming. One needs to know some special information\nabout the specific problem at hand in order to determine r before\nsolving the semidefinite program.\nCurrent Research in SDP\nThere are many very active research areas in semidefinite programming in\nnonlinear (convex) programming, in combinatorial optimization, and in con\ntrol theory. In the area of convex analysis, recent research topics include\nthe geometry and the boundary structure of SDP feasible regions (including\nnotions of degeneracy) and research related to the computational complex\nity of SDP such as decidability questions, certificates of infeasibility, and\nduality theory. In the area of combinatorial optimization, there has been\nmuch research on the practical and the theoretical use of SDP relaxations of\nhard combinatorial optimization problems. As regards interior point meth\nods, there are a host of research issues, mostly involving the development\nof different interior point algorithms and their properties, including rates of\nconvergence, performance guarantees, etc.\nComputational State of the Art of SDP\nBecause SDP has so many applications, and because interior point methods\nshow so much promise, perhaps the most exciting area of research on SDP\nhas to do with computation and implementation of interior point algorithms\nfor solving SDP. Much research has focused on the practical efficiency of\ninterior point methods for SDP. However, in the research to date, com\nputational issues have arisen that are much more complex than those for\nlinear programming, and these computational issues are only beginning to\nbe well-understood. They probably stem from a variety of factors, including\nthe fact that SDP is not guaranteed to have strictly complementary optimal\nsolutions (as is the case in linear programming). Finally, because SDP is\nsuch a new field, there is no representative suite of practical problems on\nwhich to test algorithms, i.e., there is no equivalent version of the netlib\nsuite of industrial linear programming problems.\n\nA good website for semidefinite programming is:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/6c93af5b9a7629ae386164f763f8bab8_MIT6_251JF09_rec01.pdf",
      "content": "15.081/6.251 Fall 2009\nIntroduction to Mathematical Programming\nRecitation #1\nOutline\nConvex Sets and Convex Functions\n-\nLinear Programs - Formulating good LPs\n-\nUsing CPLEX and AMPL\n-\nConvex Objects and Convex functions\nConvex Object (Set) : In Euclidean space (Rn) , an object is convex if\n-\nfor every pair of points within the object, every point on the straight line\nsegment that joins them is also within the object. Note that, for other\nspaces the denition is not simple.\nConvex Function (Alternate denition) : A function is said to be convex\n-\nif its epigraph is a convex set. ( The epigraph of a function is the set of\npoints lying on or above its graph)\n\nLinear Programs\nA convex program is an optimization problem where we aim to minimize a\nconvex function over a convex set. A linear program is an instance of a convex\noptimization problem where we minimize a linear function (which is convex)\nover a polyhedron (which is a convex set).\n3.1\nExamples - Modeling the minimization of piecewise\nlinear functions\nConsider the following minimization problem.\nmin\nf(x)\ns.t Ax = b\nwhere f(x) is as shown in the gure\nThis can be formulated as a linear optimization problem by observing that\nf(x) = max cj\n0 x + dj\nj=1...n\nfor appropriate {cj, dj }j=1...n\nThen the problem can be reformulated as\n\nP\nmin\nz\ns.t\nAx = b\nz ≥ c\njx + dj\n∀j = 1 . . . n\n3.2\nAbsolute Value Function\nMinimizing\nj∈{1,...,n} cj |xj |, where cj ≥ 0, ∀j ∈{1, . . . , n}, subject to Ax =\nb, x ≥ 0 can be written as the following LP:\nmin\nc0(x+ + x-)\ns/t\nA(x+ - x-)\n=\nb\nx+, x-\n0.\n≥\nCheck for correctness: when x +\nj\n∗ > 0, x-∗\nj\n= 0 and vice-versa.\nAlternatively, we can write\nmin\nc0z\ns/t\nzj\nxj\n≥\nzj\n≥\n-xj\nAx\n=\nb\nx\n0.\n≥\nLinear Algebra review (for Lecture 2)\n1. Linear independence\n2. Subspace\n3. Span\n4. Basis\n5. Dimension\n6. Rank of a matrix\n7. Ane Subspace\nAMPL and CPLEX review\nAMPL (A Mathematical Programming Language) is a high-level programming\nlanguage for writing and solving mathematical programs (linear and non-linear,\nin continuous and discrete variables). AMPL itself does not solve the problems,\ninstead it calls an external solver (such as CPLEX) to solve the optimization\nproblem.\nAn AMPL program consists of two parts:\n\na model le (.mod le), and\n-\na data le (.dat le)\n-\nThe model le writes the linear program using the grammar of AMPL and it\ndenes various sets, parameters, variables, objective and constraints. The data\nle provides data for the model le.\nAMPL tutorial available online\nGo to athena and do the following.\nathena% add oplstudio\nathena% ampl (starts the interactive development environment)\nAMPL can be run with a built-in CPLEX solver (the default) or using one\nof two optional solvers, LOQO, or SNOPT.\nTo select LOQO as solver, type option solver loqo; at the ampl: prompt.\nTo select SNOPT as solver, type option solver snopt; at the ampl: prompt.\nTo exit, type quit at the ampl: prompt.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/caaa61d752a49c7564f2c3b0755e80e3_MIT6_251JF09_rec02.pdf",
      "content": "15.081 Fall 2009\nRecitation 2\nLinear Algebra Review\nRead Section 1.5 of BT. Important concepts:\n- linear independence of vectors\n- subspace, basis, dimension\n- the span of a collection of vectors\nthe rank of a matrix\n-\n- nullspace, column space, row space\nBT Exercise 2.10\nProblem\nConsider the standard form polyhedron P = {x ∈ Rn|Ax = b, x ≥ 0}. Suppose\nthat the matrix A has dimensions m × n and that its rows are linearly indepen\ndent. For each one of the following statements, state whether it is true or false.\nIf true, provide a proof, else, provide a counterexample.\n1. If n = m + 1, then P has at most two basic feasible solutions.\n2. The set of all optimal solutions is bounded.\n3. At every optimal solution, no more than m variables can be positive.\n4. If there is more than one optimal solution, then there are uncountably\nmany optimal solutions.\n5. If there are several optimal solutions, then there exist at least two basic\nfeasible solutions that are optimal.\n6. Consider the problem of minimizing max{c'x, d'x } over the set P . If this\nproblem has an optimal solution, it must have an optimal solution which\nis an extreme point of P .\n\nSolution\n1. True. The set P lies in an affine subspace defined by m = n - 1 linearly\nindependent constraints, that is, of dimension one. Hence, every solution\nof Ax = b is of the form x0 + λx1, where x0 is an element of P and x1 is a\nnonzero vector. This, P is contained in a line and cannot have more than\ntwo extreme points. (If it had three, the one \"in the middle\" would be a\nconvex combination of the other two, hence not an extreme point).\n2. False. Consider minimizing 0, subject to x ≥ 0. The optimal solution set\n[0, inf) is unbounded.\n3. False. Consider a standard form problem with c = 0. Then, any feasible\nx is optimal, no matter how many positive components it has.\n4. True. If x and y are optimal, so is any convex combination of them.\n5. False. Consider the problem of minimizing x2 subject to (x1, x2) ≥ (0, 0)\nand x2 = 0. Then the set of all optimal solutions is the set {(x1, 0)|x1 ≥ 0}.\nThere are several optimal solutions, but only one optimal BFS.\n6. False. Consider the problem of minimizing x1-0.5 = max{x1-0.5x3, -x1+\n0.5x3} subject to x1 +x2 = 1, x3 = 1 and (x\n|\n1, x2, x3\n|\n) ≥ (0, 0, 0). Its unique\noptimal solution is (x1, x2, x3) = (0.5, 0.5, 1) which is not a BFS.\nBFS\nConsider a polyhedron P defined by linear equality and inequality constraints,\nand let x∗ ∈ Rn . Then\n1. The vector x∗ is a basic solution if:\n- All equality constraints are active;\n- Out of the constraints that are active at x∗, there are n of them that\nare linearly independent.\n2. If x∗ is a basic solution that satisfies all of the constraints, we say that it\nis a basic feasible solution.\nDegeneracy\n1. A basic solution x ∈ Rn of a linear optimization problem is said to be\ndegenerate if there are more than n constraints which are active at x.\n2. Consider the standard form polyhedron P = {x ∈ Rn|Ax = b, x ≥ 0} and\nlet x be a basic solution. Let m be the number of rows of A. The vector\nx is a degenerate basic solution if more than n - m of the components\nof x are zero.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/b3f98d1e302f9ae6b0db4f6a1e16ac50_MIT6_251JF09_rec03.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {2,3,4}\nGeometry of Linear Programming\nNote: In what follows, I will present the material that was covered in the recitations corresponding to the\nlectures 2, 3 and 4 which dealt with the geometry of linear programming. The material will be somewhat a superset\nof what I covered in the recitations. These material are intended to cover the most important principles of Chapter\n2 from the book. These notes should be viewed as a summary of the chapter and not as a reading material (for the\nexam).\nDenitions\n1. Polyhedron : Intersection of a nite number of half spaces (a0x ≤ b). Intersection and nite are the\nkeywords.\n2. Convex Set : If the average of the elements of a set also belongs to the set, then the set is convex.\n3. Extreme point, Vertex and BFS : The denitions are dierent (although equivalent for a polyhedron). Extreme\npoint is dened in terms of the inability to express the point as an average of two other points in the set.\nVertex is dened in terms of the point being an optimal solution for some cost vector c.\n4. Adjacent basic solutions and adjacent bases and the correspondence between a basis and a basic solution:\nThese denitions should be clear, as they play a major role in the understanding of degenracy.\n5. Degeneracy : If more than n constraintsare active at a basic solution, it is degenrate. For a standard form,\nthis reduces to the existence of more than n - m components of a point being zero. As one can imagine,\nDegenracy is not a geometic property. This is because if we know the object and if we take a minimal(with\nrespect to the number of constraints) representation of the polyhedron, we do not get degeneracy.\nImportant Theorems\nThe following theorems are important based on either what they say or the proof technique that was used in the\nproof.\n1. Theorem 2.2 : States the equivalence of existence of a unique solution to a set of linear equations and the row\nvectors being spanning Rn. This theorem is the basis for construction of basic feasible solutions for simplex\nmethod.\n2. Equivalence of a vertex, and extreme point and a bfs : The equivalence being obvious, the proof is the best\npart of this theorem. In particular, two of the directions have a constructive proof which allows us to use\n\nthe costruction in other proofs. Given a bfs, the construction of a cost vector that is optimal at tht bfs is\nparticularly interesting. And if we are given a point which is not a bfs, the construction of two points in the\nset whose average is the given point, is also interesting.\n3. Equivalence of\n(a) Existence of an extreme point\n(b) Non-existence of a line\n(c) Existence of enough number of linearly independent vectors.\nThe proof is constructive, please go through it.\n4. Optimality of extreme points - The statement being very clear, the proof is pretty non-trivial and cute.\n5. Non-empty bounded polyhedron is the convex hull of its extreme points.- Proof is constructive.\nProblems and Exercises\nWill present the problems that are important.\n1. 2.6 - Caratheodory's theorem - Shows the existence of a minimal set of extreme points that contains a given\npoint in its convex hull. The proof uses the existence of a bfs.\n2. Fourier -Motzkin Elimination ( 2.20) - Shows that we could obtain exponentially many constraints.\n3. Exercise 2.22 : Uses the face that a projection of a polyhedron is a polyhedron.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/ea8dc844e1da5e884be498836a075a71_MIT6_251JF09_rec04.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {5,6,7,8}\nSimplex Method\nNote: In what follows, I will present the material that was covered in the recitations\ncorresponding to the lectures 5, 6, 7 and 8 which dealt with the geometry of linear pro-\ngramming. The material will be somewhat a superset of what I covered in the recitations.\nThese material are intended to cover the most important principles of Chapter 3 from the\nbook. These notes should be viewed as a summary of the chapter and not as a reading\nmaterial (for the exam).\nDenitions\n1. Basis\n2. Feasible direction\n3. Reduced cost\nImportant Theorems\nThis set of lectures develops the simplex method. Theorem 3.1 and its proof are most\ninteresting.\nAlgorithms\nThe simplex method, revised simplex method and the corresponding space and time\ncomplexities should be studied. The techniques used in reducing the space complexity of\nsimplex method are standard in various other algorithms involving matrix computations.\nThis techique should be well studied.\nThe algorithm used to drive put the articial variables in the phase 1 of the simplex\nalgorithm is also interesting.\nColumn Geometry\nColumn geometry view of the simplex method allows one to expect good performance\nfrom the simplex algorithm. The view helps us to see how, if a good pivoting rule is used,\nsimplex will take O(n) iterations to achieve optimality. The good pivoting rule is still\nopen.\n\nP\nX\nProblems and Exercises\nThe following problems were covered in the recitation.\nProblem 3.19\nThe problem is an exercise to internalise the simplex algorithm. One should keep in mind\nthe various if-else considitons in the simplex algorithm.\nProblem 3.27 (The linear scaling method)\n(a)\nthe key here is to identify that if ∃x s.t. xi > 0 then ∃α > 0 where z + y = αx and\nyi = 1. Therefore maximizing\nyi is equivalent to maximizing the number of positive\ncomponents of x.\n(b)\nThe same scaling method should be used here. But since the right hand side is not\nzero, the previous LP cannot be used directly. ∴ we should make the right hand zero.\nThis can be accomplished as follows.Consider the following LP\nmaximize\nyi\ns.t\nA(z + y) = αb\nyi ≤ 1\nz, y ≥ 0\nα ≥ 1\nHere, α is also a decision variable. The key again here is to notice that if ∃x s.t. xi > 0\nthen ∃α, y, z > 0 where z + y = αx and yi = 1. And the result follows. By making α a\ndecision variable, we made the problem invariant under scaling. This is a technique that\nshould be treasured!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/e8ff186546e198be5e1f7d779aa53f87_MIT6_251JF09_rec05.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {9,10,11}\nDuality theory\nDuality is the most important characteristic of LP. LP is useful and easy only\nbecause of the existence of strong duality. As one can observe, whenever we nd\na system where we nd strong duality we can apply many of the algorithms that\nwe develop for LPs for those problems. In particular, I recommend the following\nbook for people interested in Duality theory. Duality in Optimization\nConcepts\nPrimal and Dual\n1. Lagrangean Duality : The duality theory developed arises from consideing\nthe lagrangean relaxation of the LP.\n2. Weak Duality : Given any dual feasible p and primal feasible x suc that\np0b ≤ c0x .\n3. Strong Duality and Complementary Slackness - The properties of optimal\nsolutions of primal and dual.\nGeometry\n1. Farkas Lemma, which turns out to be equivalent to Strong Duality.\n2. Cones and Extreme Rays - Extreme rays of a polyhedron correspond to\nthe extreme rays of the recession cone assocciated with the polyhedron.\n3. Resolution theorem : Another representation of a polyhedron.\n\nDuality and Degeneracy\nAn important relation that can be useful is the following :\nUniqueness and Nondegeneracy of primal\nUniqueness and Nonde-\n-\n⇐⇒\ngeneracy of the dual.\nThis would thus imply that :\nNon-uniqueness or degenracy of primal\nNon-uniqueness or degen-\n-\n⇐⇒\nracy of the dual.\nProof Techniques\nThe proof techniques that duality or Farkas' Lemma provides us with are very\nuseful.In particuar, duality allows us to convert an existential statement into a\nfor all statement. Whenever we have an existential statement in the hypoth-\nesis, duality would give us for all statements as conclusions and vice-versa.\nMost of the exercises are (slighlty non-trivial) applications of this proof tech-\nnique.\nIn particular, to use duality we have identify\nAppropriate Primal Polyhedron - This usually is very apparent from the\n-\nstatement of the problem that we are considering.\nAppropriate cost function - this requires clever choice and usually choices\n-\nof c = 0, ±e, ±ei work.\nExamples :\n1. Strict complementary slackness (4.20) - The polyhderon and the cost vec-\ntor suggested in the hint.\n2. Clarke's theorem - Polyhedron is obvious. cost vector - Use -e.\n3. 4.26 : USe e\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/b26dbd12f2521dc45ad116dc65dbeaeb_MIT6_251JF09_rec06.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {16-17}\nNetwork Flows\nStandard Network Flow Problems\n1. Min cost ow problem\n2. MAx Flow Problem\n3. Min cut problem\nDuality of MAx-ow and MinCut was also discussed.\nFormulations\nFormulating a problem as a Network Flow problem allows one to use the fast\nNetwork-Simplex algorithm or other ecient combainatorial algorithms.\nFormulating as a network ow problem involves the following steps :\n1. Identify nodes\n2. Identify arcs\n3. Identify the costs and arc capacities - This is determined by the problem\nthat we solve.\nThe rst two steps are dependent on the structure of the system in question.\nThe third one depends on the objective at hand.\nExample - Tournament Elimination Problem\nThe following source does a very good job Tournament Elimination\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/276fdbde7ecda3e9c1a31b408cdbf6f3_MIT6_251JF09_rec07.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {18-19}\nEllipsoid Method\nSeparation and Feasibility\nEllipoid method was the rst polynomial time algorithm developed for LP. The\nEllipsoid algorithm requires a convex set described by a separation oracle, and\ndetermines whether the set is empty.\nSeparation Oracle\nA separation oracle associated with a convex body, when given a point as an\ninput determines whether the point is in the convex set. If the point is not in\nthe convex body, it provides a separating hyperplane that separates the point\nfrom the body. An oracle which only determines the inlcusion or exclusion of a\npoint from a set is called a membership oracle.\nSeparation Problem\nThe problem of determining whether a point x ∈ C where C is called a mem-\nbership problem. The separation problem also asks one to come up with a\nseparating hyperplane.\nEllipsoid method shows that, for nice sets, if given a separation oracle, the\nfeasibility problem can be solved in polynomial time.\nAnd since feasibility and optimization are equivalent, this shows that, when\ngiven a separation oracle, the optimization problem is polynomial.\nHence,\nSeparation ∈ P ⇐⇒ Optimization ∈ P\n\nExample - Probaility consistency problem\nThe problem can be found in the book. The book shows that the problem of\ndetermining whether a set of beliefs about some events is consistent with theory\nof probability is an easy problem.This is shown by showing that the problem is\nequivalent to the problem of nding a min-cut in an appropriate graph. The\nmincut problem is known to easy and so is the original problem.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT6_251JF09_rec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/286d5070be83057e8de3151e28522d5b_MIT6_251JF09_rec08.pdf",
      "content": "15.081 Fall 2009\nRecitation\nfor Lectures {22}\nSemidenite Programming\nBasics of Eigen Analysis\nEigen Analysis is an important problem that arises in various areas\nSystem Stability analysis\n-\nConvergence of various linear-iterative algorithms\n-\nMarkov-chains - rate of convergence to steady state distributions etc.\n-\nEigenvalues of a matrix\nEigen values of a matrix A are the solutions of the equation\nDet(A - xI) = 0\nAlso each eigen value has a corresponding eigen vector v such that\nAv = λv\nFor nice matrices, we have n eigen values and n eigen vectors where n =\nrank(A).\nP\nAnother property is that A can be written as A =\nλivivT where {λi} and\ni\n{vi} are the eigenvalues and eigenvectors respectively.\nStandard problems\nWithout loss of generality assume λ1 ≥ λ2 ≥ . . . ≥ λn . In the recitation the\nfollowing problems were discussed.\n- nd X such that λ1is minimized.\n\n- nd X such that λn is maximized.\n- nd X such that λ1 - λn is minimized.\nnd X such that κ = λ1 is minimized. κ is called the condition number\n-\nλn\nof a matrix and arises in many applications.\nThe survey by Boyd and Vandenbergh is a good source for SDP.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.251J / 15.081J Introduction to Mathematical Programming\nFall 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}