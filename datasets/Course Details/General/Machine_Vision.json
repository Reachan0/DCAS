{
  "course_name": "Machine Vision",
  "course_description": "This course is an introduction to the process of generating a symbolic description of the environment from an image. It covers the physics of image formation, image analysis, binary image processing, and filtering. Machine vision has applications in robotics and the intelligent interaction of machines with their environment. Students taking the graduate version complete additional assignments.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Graphics and Visualization",
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Graphics and Visualization"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / wk; 1.5 hrs / session\n\nPrerequisites\n\n6.003 Signal Processing\nor permission of the instructor.\n\nCourse Description\n\nThis course focuses on machine vision. Topics include:\n\nDeriving a symbolic description of the environment from an image.\n\nUnderstanding physics of image formation.\n\nImage analysis as an inversion problem.\n\nBinary image processing and filtering of images as preprocessing steps.\n\nRecovering shape, lightness, orientation, and motion.\n\nUsing constraints to reduce the ambiguity.\n\nPhotometric stereo and extended Gaussian sphere.\n\nApplications to robotics; intelligent interaction of machines with their environment.\n\nStudents taking the graduate version complete different assignments.\n\nGrading\n\nUndergraduate 6.801 Grading\n\nActivities\n\nPercentages\n\nHomework\n\n50%\n\nQuiz 1\n\n31.5%\n\nQuiz 2\n\n13.5%\n\nNanoquizzes\n\n5%\n\nGraduate 6.868 Grading\n\nActivities\n\nPercentages\n\nHomework\n\n33%\n\nTake-Home Quizzes\n\n28% (Higher of Quiz 1 and Quiz 2)\n\nFinal Project\n\n33%\n\nNanoquizzes\n\n6%\n\nThere is no final exam. There will be 5 problem sets and 2 take-home quizzes, which are just like problem sets, only longer, and worth more points.\n\nOur goal with nanoquizzes is not to cause undue and unnecessary stress to the students, especially during a pandemic. We believe nanoquizzes are a good way to help students stay up-to-date with the material. When we determine final grades, we will ensure that nanoquizzes can only help your grade, and not bring it down. For instance, nanoquizzes will never bring your grade from an A to a B, but could potentially bring your grade from a B to an A.",
  "files": [
    {
      "category": "Resource",
      "title": "6.801 / 6.866 Machine Vision, Homework 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/48f8beaf1ec24b134f10d91fc10b40e4_MIT6_801F20_hw3.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nProblem 1:\nThis problem is about improving the accuracy of the photometric\nstereo method for recovering surface shape. We might want to take more than\ntwo brightness measurements in order to improve accuracy when using the pho-\ntometric stereo method. Imagine that n light sources are used in turn to obtain\nn images. Suppose that the surface under consideration is Lambertian and that\nthe direction to the i-th source is given by the unit vector ˆsi. Assume that the\nsurface can have an albedo ρ (where 0 < ρ < 1) (i.e. it is not neccessarily an ideal\nLambertian surface). At each point in the image, we wish to find the unit surface\nnormal ˆn that minimizes the sum of squares of errors\nn\n\ni=1\n(ρ ˆn · ˆsi -Ei)2 ,\nwhere Ei is the i-th measurement of brightness at that point, and ρ the albedo.\n\n(a) Show that the vector that minimizes the sum of squares of errors is\nρ ˆn =\nn\n\ni=1\nˆsi ˆsT\ni\n-1\nn\n\ni=1\nEi ˆsi,\nwhere abT is the dyadic product of the vectors a and b, i.e.\n⎛\n⎝\nax\nay\naz\n⎞\n⎠ bx\nby\nbz\n\n=\n⎛\n⎝\naxbx\naxby\naxbz\naybx\nayby\naybz\nazbx\nazby\nazbz\n⎞\n⎠,\nand [ ]-1 indicates the inverse of the matrix.\n(b) Show that the matrix is singular when there is only one measurement\n(n = 1). How about two measurements (n = 2)?\n(c) What is the smallest number n of measurements needed to guarantee that\nthe indicated matrix inverse exists? Hint: This part of the problem is nontrivial.\n(d) How would you expect the minimum number of measurements needed\nchange if it were known that the albedo was one (ρ = 1)?\nProblem 2:\nThis problem is about a family of surface reflectance models used to\ndescribe light reflection off the surfaces of rocky planets, asteroids, and satellites.\nA phenomenological reflectance model for such surfaces is an ideal 'Minnaert'\nsurface for which\nf (θi, φi; θe, φe) = (c + 1)/(2π) (cos θi cos θe)c-1\nfor 0 ≤c ≤1. Here, when illuminated by a distant point source, brightness is\ngiven by\nL = (c + 1)/(2π) E0 cosc-1 θe cosc θi\n(a)\nDoes the surface reflectance so defined obey Helmholtz reciprocity?\n(b) For what value of c is the surface brightness independent of the viewing\ndirection?\n(c)\nFor what value of c is the surface brightness independent of the illumination\ndirection?\n(d) Is Lambert's formula a special case of Minnaert's formula?\n(f)\nIs a Hapke-type surface a special case of a Minnaert surface?\n(f)\nIs a surface with radiance proportional to sec θe a special case also?\nProblem 3:\nThis problem is about ambiguity in recovering the shape of an object.\nSuppose that the reflectance map is linear in p and q, so that\nR(p, q) = ap + bq + c.\n\n(for example, it could be a Hapke-type surface). We have an image, including the\nsilhouette of a simple convex object of shape z = f (x, y). Show that the surface\nz = f (x, y) + g(bx -ay),\nfor an arbitrary differentiable function g(s), will give rise to the same image.\nDoes the surface z have the same silhouette? Assume that the derivative of g is\nbounded.\nProblem 4:\nThis problem is about starting the solution of a shape-from-shading\nproblem at a singular point, by fitting a smooth local shape near the singular\npoint. Consider the image of a polynomial surface with a stationary point at the\norigin:\nz(x, y) = α(x2 + y2) + 2βxy\nAssume that the rotationally symmetric reflectance map is simply\nR(p, q) = (1 + p2 + q2)\n(a)\nWhat is the image E(x, y) of this surface? What is the brightness gradient\n(Ex, Ey)?\n(b) What is the relationship between the second order partial derivates Exx and\nEyy? What is the relationship between the second order partial derivatives\nand the coefficients α and β of the polynomial?\n(c)\nGiven measurements of Exx, Exy, and Eyy at the origin, how many solu-\ntions (values of α and β) are there for the surface shape?\n(d) Suppose that an image of such a polynomial surface has a non-zero mixed\nsecond order partial derivative Exy. Is it possible to rotate the coordinate\nsystem so that in the mixed partial derivative is zero in the rotated system?\nHint: Express the second partial derivatives in the rotated coordinate system\nin terms of the ones in the original coordinate system -- then try and find a\nrotation that makes the mixed derivative drop out.\nProblem 5: This problem is about \"line\" detection -- as opposed to \"edge\"\ndetection. The brightness gradient ∇E = (Ex, Ey) is useful in recovering image\nmotion. It is also useful for \"edge\" detection where we look for extrema of the\nbrightness gradient along the direction of the brightness gradient itself.\nThe first directional derivative of brightness along a line that makes an angle\nθ with the x axis is\ndE\nds = Ex cos θ + Ey sin θ\nThe second derivatives of brightness are of use in recovering local surface curva-\nture, as well as for \"line\" detection.\n\n(a)\nShow that the maximum directional first derivative is in the direction given\nby the unit gradient vector\n(Ex, Ey)\n\nE2x + E2y\nand that the slope in that direction is\n\nE2x + E2y.\n(b) Show that the second directional derivative along a line that makes an angle\nθ with the x axis is:\nd2E\nds2 = Exx cos2 θ + 2Exy sin θ cos θ + Eyy sin2 θ.\n(c)\nShow that the second directional derivative has extrema for\ntan 2θ = 2 Exy\n\n(Exx -Eyy)\n(d) Show that the extreme values are\nE′′\nmin,max = 1\n2(Exx + Eyy) ± 1\n\n(Exx -Eyy)2 + 4 E2xy.\ne)\nShow that the four directions of extrema in second directional derivative are\ngiven by:\n±\n\nD ± (Exx -Eyy)\n2D\n, sign(Exy)\n\nD ∓(Exx -Eyy)\n2D\n\n.\nwher\nD =\n\n(Exx -Eyy)2 + 4E2xy.\n(f)\nWhen will there not be extrema in the directional second derivative?\n(g)\nWhat is the geometric relationship between the gradient direction ∇E =\n(Ex, Ey) and the direction of the minimum second directional derivative on\n'top of a ridge' of the brightness surface? What is the geometric relationship\nbetween the gradient direction and the direction of the maximum second\ndirectional derivative at the 'bottom of a valley' of the brightness surface?\nHints: Use identities for trigonometric functions of doubled angles. Express\nsin 2θ and cos 2θ in terms of tan 2θ. Substitute back into the expression for\nthe directional second derivative. Express cos θ and sin θ in terms of cos 2θ and\nsin 2θ. Note that the second directional derivative can be positive or negative.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.801/6.866 Machine Vision, Homework 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/44cf7a39fd1de47fc359dc801be9c9fc_MIT6_801F20_hw1.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nProblem 1:\nYou can approach the following problems geometrically or alge\nbraically (whichever is easier);\n(a) What is the image of a line in space?\n(b) What is the image of a circular disc that lies in a plane that is parallel to\nthe image plane?\n(c) How is the shape of the image of a pattern that lies in a plane parallel to\nthe image plane related to the shape of that pattern (think paper cutout, maybe)?\n(d) What geometric shapes can the image of a sphere take on? (Careful: this\nis different from the image of a circular disc).\nNotes: Assume perspective projection. Please explain your reasoning or show\nyour calculations.\nProblem 2: Consider the image of a polyhedral object (e.g. a dodecahedron)\nilluminated by distant light sources. Describe the resulting brightness pattern in\nthe image plane. Note: Assume that the object is not only far from the light\nsources, but also far from the camera (i.e. a telephoto lens is used).\nThink about the following: How does the brightness vary from place to\nplace in the image? Is it constant? Does it have discontinuities? Does it vary\nlinearly with position in the image? Are there areas of uniform properties? If so,\nwhat shapes are those image areas? (Note: we are not looking for a closed-form\nsolution or mathematical formula).\nProblem 3:\nHere we explore a simple method for estimating image motion.\nSuppose that at time t = 0 image brightness is E0(x, y), and that the image\nmoves to the right (i.e. +x direction) with velocity u.\n(a) First, to get started, consider a simple situation where brightness ramps up\nlinearly with position, E0(x, y) = mx + c say at time t = 0, with m > 0. If\nthe brightness pattern moves to the right, does the brightness at a particular\npoint (i.e. fixed x) go up or down? If the brightness pattern moves to the\nright by δx, then by how much does the brightness at a particular point in\nthe image change? (This exercise helps get the signs of the derivatives right).\n\n(b) If the pattern moves to the right with velocity u, then what is E(x, y, t) in\nterms of E0(x, y)?\n(c) Now consider a simple image where, at time t = 0, brightness varies as\nE0(x, y) = 6 + 5 sin(x) - sin(5x)\n(Note that in this simple case brightness does not depend on y). Write\nan expression for E(x, y, t) assuming the pattern moves to the right with\nvelocity u. Find the derivatives with respect to x, y, and t.\n(d) Estimate the velocity u from the partial derivatives Ex , Ey and Et at a\nparticular point (x, y) in the image.\n(e) For t = 0, how well would you expect the method to work near x = 0. How\nabout near x = π/2? (Consider the effect of small changes in brightness\nmeasurement. Think of \"noise gain\").\nProblem 4:\nHere we explore using weighted averages of uncertain estimates\nderived from images in order to get more accurate results.\n(a) If we have estimates of Ex and Et for many points in the image, then we can\naverage the corresponding local estimates of image motion u -- obtained\nusing the method of 3(d) -- by integrating over an image area and dividing\nby the area. But as (hopefully) shown in 3(e), contribitions from some image\nregions may be less valuable than contributions from others. Suppose that\nwe \"weight\" the estimate of u from image position (x, y) by multiplying by\nsome suitable weight factor w(x, y) (which is as yet to be determined). Give\na formula for the \"weighted average\" velocity u. Note that you can no longer\nsimply divide by the area of image region integrated over -- there is now a\nneed to compensate for the spatially varying weight factor w(x, y).\n(b) Suppose now that the method is to be used on more complex images. The\nbrightness no longer varies according to some simple analytical expression,\nbut as some arbitrary function E0(x, y) of image position. Find the value\nof u that minimizes:\n\n(Et + uEx )2 dx dy\nR\nwhere the integration is carried out over the image region R of interest. (As\nsume that you can interchange the order of differentiation and integration).\n(c) Interpret the formula for u that you obtained in part (b) in terms of the\nweighting scheme of part (a). What is the weight factor w(x, y) here? Do\nthe weights seem rasonable in the sense that more reliable information is\nweighted more heavily?\n\nProblem 5: Here we use images to estimate object poses. Consider a system for\nestimating the oriention of a planar surface based on an image of a circle on that\nsurface. If we view the plane \"straight on\", the circle will be imaged as a circle.\nWe get an ellipse in the image if the plane is tilted so that its normal vector is not\nperpendicular to the image plane. We want to determine how much the plane is\ntilted from the eccentricity of the ellipse.\nTo simplify matters, we here consider orthographic instead of perspective\nprojection. This is corresponds to the limit of looking at the objects from further\nand further away, but with correspondingly higher and higher magnification.\nImagine parallel rays perpendicular to the image plane arriving from a very\ndistant light source. A circular disk is suspended above the image plane, blocking\nsome of the incident light. The result is an elliptical shadow in the image. The\nangle between the image plane and the plane containing the circular disk is θ.\nb\na\n(a) Find the length of the minor axis b in terms of the angle θ and the length of\nthe major axis a.\n(b) The eccentricity e of an ellipse can be defined in terms of the ratio of the\nsemi-major axes using the formula e2 = 1 - (b/a)2 . Express the eccentricity\nof the ellipse in the image in terms of the angle θ.\n(c) Suppose we have an image processing method that can estimate the eccen\ntricity of an elliptical image pattern. For what angles will the estimation of\nθ based on the eccentricity be particularly difficult in the presence of noise?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.801/6.866 Machine Vision, Homework 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/480e8d0833ac9552ee071d15319ef2cf_MIT6_801F20_hw2.pdf",
      "content": "′\n′\n′\n′\nMassachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nProblem 1: Rotational Optical Flow. In our analysis of the \"fixed flow\" problem\nwe have so far considered only translational motion. We can generalize this to\nthe case where there is in additon in-plane rotation. The whole image translates\nwith unkown velocity (u, v) and with an in-plane rotational component ω. That\nis, in addition to translating with velocity (u0, v0), say, the image is also rotating\nwith angular velocity ω (rad/sec) about some (known) reference point (x0, y0)\n(which could be chosen to be the principal point, or just the center of the image,\nbut here we keep it general).\n(a) Show that the velocity (u, v) at point (x, y) in the image is\nu = u0 - ωy\nand v = v0 + ωx\nwhere, for convenience, we have defined x = (x - x0) and y = (y - y0).\n(b) Insert u and v (now spatially varying rather than fixed for the whole image)\ninto the brightness change constraint equation.\n(c) The task is to find the values of the unknown parameters u0, v0, and ω that\nminimize the integral of the errors squared:\n\n(uEx + vEy + Et )2 dx dy\nYou should obtain three linear equations in the three unkonwns.\n(d) In order to obtain a unique solution, what conditions must the determinant\nof the 3 × 3 coefficient matrix satisfy?\n\n′\n′\n(e) What happens when the brightness pattern is radially symmetric about the\npoint (x0, y0) (above left)? Show that in this case (x , y ).(Ey , -Ex ) =\n0. Can one still recover the translational component of motion (u0, v0)?\nExplain.\n(f) What happens when the brighntess pattern is constant along radii, much like\na pie chart (above right)? Do you expect to be able to recover translation\nand rotation? Explain.\n(g) What happens when the brightness gradient everywhere has the same direc\ntion (that is Ey = kEx ) as in the image above? Can one still recover the\nrotational component of motion ω? Explain.\nProblem 2: Time-to-Contact w.r.t. inclined surface. In class we analyzed a direct\nmethod for recovering \"time to contact\" (TTC) in translational motion towards\na planar surface. In that example, the translational motion (U, V , W )T could be\nalong any direction, but the plane had to be perpendicular to the optical axis,\nthat is, Z = Z0\nHere we instead allow the plane to have any orientation, but restrain the\nmotion to be along the optical axis, that is, U = V = 0. The equation of a plane\nis linear in X , Y and Z (coordinates in the camera-centric coordinate system). It\ncan, for example, be written in the form\nZ = Z0 + pX + qY\nwhere p = ∂Z/∂X and q = ∂Z/∂Y are the slopes of the surface in the X and\nY directions, while Z0 is the distance from the center of projection to where the\noptical axis pierces the plane.\nWith translational motion along the optical axis, we have simple radial op\ntical flow with focus of expansion (FOE) at (0, 0):\nu = -(W /Z)x and v = -(W /Z)y\nwhere W = dZ/dt is the velocity of the planar surface in the Z direction (equiv\nalently the negative of the motion of the camera relative to the planar surface).\n\n(a) For our purposes, we need Z as a function of image coordinates x and y\n(rather than as a function of X and Y ). Show that\n\nZ 1 -p(x/f ) -q(y/f ) = Z0\n(b) Next, using the brightness change constraint equation, show that\n\n-(W/Z0) 1 -p(x/f ) -q(y/f ) G + Et = 0\nwhere G = (x, y) · (Ex , Ey )\n(c) Formulate a least squares problem for finding three unknown parameters\np W\nq W\nW\nP =\n,\nQ\n\n=\n,\nand C = -\nf Z0\nf Z0\nZ0\nthat best fit the image sequence. Show that the minimum occurs for values\nof P, Q, C that satisfy three linear equations.\n(d) How can you recover the surface slopes, p and q, from P, Q, and C? What\nis the time to contact, T , in terms of P, Q, and C?\nProblem 3: Camera calibration using vanishing points. We can use a brick-shaped\nobject for camera calibration. We do not need to know the size of the object,\nits orientation, or its distance from the camera. The twelve edges of a brick-\nshaped object come in three groups of parallel lines. The lines in each group are\nperpendicular to the lines in the other two groups. Each group of parallel lines\ncreates a vanishing point in the image.\nSuppose that we establish an image plane coordinate system based on the\nphotodetector array column number (for x), the row number (for y) -- and z = 0\nfor points in the image plane. (Note that that is not our standard \"camera-centric\"\ncoordinate system -- which can be established only after calibration.)\nb\na\nc\n\nSuppose that the measured positions of the three vanishing points are a, b, and\nc in that coordinate system Note that a · zˆ = 0, b · zˆ = 0 and c · zˆ = 0 since these\npoints lie in the image plane. Next, suppose that the center of projection is at\np = (xp , yp, f )T in the same coordinate system. We will recover the unknown\nposition of the center of projection p from the positions a, b, c of the three\nvanishing points in the image plane.\n(a) Given the figure above, explain why there are three sets of lines parallel to\n(a - p), (b - p), and (c - p) respectively.\n(b) Explain why the following holds\n(a - p) · (b - p) = 0,\n(b - p) · (c - p) = 0,\n(c - p) · (a - p) = 0\n(c) By subtracting equations pairwise, show that\n(a - p) · (c - b) = 0,\n(b - p) · (a - c) = 0,\n(c - p) · (b - a) = 0\n(d) Show that each of these three linear equations in p correspond to a plane\nwith surface normal parallel to the image plane. Are the three equations\nalways linearly independent? If not, when are they linearly dependent?\n(e) How are these planes oriented relative to the image plane? How is the line\nof intersection of any pair of these planes related to the image plane?\n(f) Suppose now that the measured positions of the three vanishing points in\nan image (Note: above figure not to scale) are a = (500, 500, 0)T , b =\n(100, 500, 0)T , and c = (300, 100, 0)T . By solving two linear equations for\ntwo unknowns, find the point on the line defined in part (e) that lies in the\nimage plane. How is this point related to the principal point?\n(g) We now know xp and yp , the first two components of p, and so only need f\nto complete the calibration of the camera. Find the height f of p above the\nimage plane (you may need to solve a quadratic equation). Conclude that\nthe principal distance f is approximately 173.2 . . . pixels (expressed in units\nof spacing between pixels).\nProblem 4: Source from shading. In photometric stereo we try to find surface\norientaton, typically with knowledge of the light source position and surface\nreflectance properties. In this problem we try to \"invert\" this to find the light\nsource position given surface orientations. In an image of a rectangular 'button'\nrising above a flat background, the brightness of the indicated regions is as follows:\nA has grey value 212, B has grey value 175, and C has grey value 200. Importantly,\nwe are told that the bevelled edges of the button are inclined 45* with respect to\nthe plane of the background (which is parallel to region A).\n\nA\nB\nC\n(a) Consider a coordinate system in the plane of the background, with x running\nto the right and y upwards. Let the z axis be perpendicular to the background\nplane, pointing \"out of the page.\" Write the normals of each of the regions\nA, B, and C as vectors with three components.\n(b) Next, assume that the surface has reflecting properties that closely match\nLambert's 'law', that is, that brightness is proportional to the cosine of the\nincident angle. Using the three measurements of brightness, find the unit\nvector in the direction of the light source. (You may find yourself solving\nsimultaneous equations, or, using guesswork and iterative refinement).\nProblem 5: Shape from Shading.\n(a) Consider the quadratic surface\nz(x, y) = ax 2 + bxy + cy\nFind an expression for the unit surface normal\nnˆ (x, y)\nas a function of x and y. Assuming a single distance light source in direction\nsˆ = (sx , sy , sz )T\nand Lambertian surface characteristics, find the surface radiance L(x, y) as\na function of x and y. The irradiance of the source on a plane perpendicular\nto the incident rays is E0 (W · m-2).\n(b) Consider a surface made of a material that has reflectance map\nR(p, q) = p 2 + q\nand results in an image\nE(x, y) = 2 - cos(2x) - cos(2y)\nshow that the surface may have the shape\nz = a cos(x) + b cos(y)\nfor suitable values of a and b. Are there other surfaces that give rise to the\nsame image?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.801/6.866 Machine Vision, Homework 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/910c933c8b46a3a3c8c0ff804b1514c7_MIT6_801F20_hw4.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nProblem 1:\nHere we consider some properties of a two-dimensional version of\nthe absolute orientation problem (that is, we are dealing with alignment of images\nrather than volumes). Let (xi, yi) and (x′\ni, y′\ni) be coordinates of corresponding\npoints in two images. For convenience, assume that the coordinates are measured\nrelative to the centroids of the points in the two images (i.e. the sum of the\ncoordinates in each image is zero). We are to minimize the sum of squares of\nerrors in the transform:\n√s x′\ni = √s(+ cos θxi + sin θyi)\n√s y′\ni = √s(-sin θxi + cos θyi)\nby suitable choice of the 'symmetrical' scale factor s and the rotation θ. (Note\nthat translation has already been taken care of by using coordinates referenced\nto the centroids).\n(a)\nShow that when formulated this way, the total error to be minimized consists\nof a term that depends only on s, and not on θ, and a term that depends\nonly on θ, and not on s -- thus separting the problem of finding the rotation\nfrom that of finding the scaling.\n(b) Show that the best fit value for s can be computed without knowing the\nrotation θ -- and without knowing the correspondences between points in\nthe two coordinate systems!\n(c)\nShow that the best fit value for θ can be found without knowing s.\n(d) The resulting equation for θ appears to have more than one solution. Do\nall of the solutions minimize the sum of squares of errors in image position?\n(e)\nWhat was the advantage of 'splitting' the contribution of scale s into two\nfactors, √s and 1/√s (rather than just using say x′\ni = s . . . and y′\ni = s . . .)?\nProblem 2:\nThis problem is about relating different ways of representing rotation\nin 3-D. Rodrigues' formula provides a method for rotating a vector r about an\naxis ˆω through an angle θ (see figure on next page):\nr′ = (cos θ)r + (1 -cos θ)(r · ˆω) ˆω + (sin θ)( ˆω × r)\n(a)\nCan a rotation through an angle -θ about a different axis producce the same\nresult? What is that axis?\n\n(b) Use Rodrigues' rotation formula to show that the rotation matrix R for a\nrotation about an axis specified by the unit vector ˆω through an angle θ can\nbe written\nR = cos θ I + (1 -cos θ) ˆω ˆωT + sin θ ˆω×\nwhere ˆω× is a 3 × 3 skew symmetric matrix such that ˆω×v = ˆω × v for all v.\n(c)\nExpress Trace(R) in terms of cos θ and sin θ, where Trace(R) is the sum of\nthe diagonal elements of the matrix R.\n(d) How could you recover θ and ˆω from R (hint: consider using Trace(R), and\nthe elements of R + RT and R -RT to recover cos θ, sin θ, and ˆω×).\n(e)\nShow that ˆω2\n× = ˆω ˆωT -I (where ˆω ˆωT is the dyadic product of ˆω and ˆω).\n(f)\nShow that the matrix R defined in (b) is in fact orthonormal (RT R = I).\n(g)\nBased on (b), conclude that we can write the rotation matrix in terms of the\ncorresponding unit quaternion q = (q, q)T as\nR = (q2 -q · q)I + 2qqT + 2qq×\nwhere q× is the skew symmetric matrix such that q×v = q × v for all v.\n(h) Express Trace(R) in terms of q and q (may or may not need to remember\nthat q2 + q · q = 1).\n(i)\nHow would you recover q and q from R (hint: consider using Trace(R), and\nthe elements of R + RT and R -RT in analogy with part (d) above).\n\nProblem3:\nProblemsinphotogrammetryrequirerecoveryofEuclideanmotions,\nthat is, translation and rotation. In this problem we prove some quaternion\nidentities that are helpful in using quaternions to represent rotations in three\ndimensions. Remember that the product of two quaternions a = (a, a) and\nb = (a, b) can be written\n(a, a)(b, b) = (ab -a · b, ab + ba + a × b)\nThe conjugate of a quaternion a = (a, a) is a∗= (a, -a). The dot-product of\ntwo quaternions a and b can be written (a, a) · (b, b) = ab + a · b.\n(a)\nShow that\na a∗= a∗ a = ( a · a) e\nwhere e is a quaternion with unit scalar part and zero vector part.\n(b) Show that\n( a b) · ( a b) = ( a · a)( b · b)\n(c)\nShow that\n( a b) · ( a c) = ( a · a)( b · c)\n(d) Show that\n( a q) · b = a · ( b q∗)\nand\na · ( q b) = ( q∗ a) · b\n(e)\nIs the following always true?\n( q a q∗) · ( q b q∗) = a · b\nIs a constraint on q needed?\n(f)\nHow about?\n( q a q∗) ·\n\n( q b q∗)( q c q∗)\n\n= a · ( b c)\nIf necessary, assume that a, b, and c, are quaternions representing vectors, that\nis, with zero scalar part -- and, if necessary, that q is a unit quaternion.\nProblem 4:\nHere we explore an alternate way of determining rigid body motion\nfrom two sets of 3-D measurements. Suppose that we have measured the coordi-\nnates of n known features on an object before movement, and found them to be\n{li} for i = 1, 2 . . . n, and then measured them again after movement, and found\nthem to be {ri} for i = 1, 2 . . . n. One way of estimating the rigid body motion\nthat took place is to find the transformation that: (i) moves the average lof {li}\nto the average r of {ri}, and (ii) rotates the \"axes of inertia\" of the two sets of\nmeasurements into exact alignment.\nLet l′\ni = li - land r′\ni = ri - r be coordinates relative to the centroids.\n(a)\nNow consider the inertia of the \"point cloud\" {r′\ni} about an axis defined\nby the unit vector ˆω (through the average r). Show that the square of the\nperpendicular distance of the point r′\ni from the axis of rotation is\nr′\ni · r′\ni -( ˆω · r′\ni)2\n\n(b) Show that overall the inertia about the axis ˆω can be expressed in the form\nI( ˆω) =\nn\n\ni=1\nr′\ni\nT r′\ni -ˆωT\nn\n\ni=1\nr′\nir′\ni\nT\n\nˆω\nIs the 3 × 3 matrix that appears in the above expression symmetric?\n(c)\nThe first term in the above sum is a constant, equal to the sum of squares of\ndistances of the points from the origin, and does not depend on the choice\nof the axis ˆω. Show that the stationary values (maxima, minima and saddle\npoints) of the inertia are given by the eigenvalues of the 3×3 \"inertia matrix\"\nin the above expression.\nCorrespondingly, the axis directions that give rise to these stationary values are\nthe eigenvectors of this matrix. The three eigenvectors will be orthogonal to each\nother if the eigenvalues are distinct. Suppose the eigenvectors are e1, e2, and e3\ncorresponding to eigenvalues λ1, λ2, and λ3 where λ1 > λ2 > λ3 > 0.\n(d) As an example, suppose that n = 6 and that the measured points are\n(a, 0, 0)T , (-a, 0, 0)T , (0, b, 0)T , (0, -b, 0)T , (0, 0, c)T , (0, 0, -c)T\nwhere a > b > c > 0. Determine the inertia matrix, and find its eigenvalues\nand eigenvectors.\n(e)\nShow that in general any vector r can be expressed as a linear combination\nr′ = a1e1 + a2e2 + a3e3 or, in other words,\nr′ =\ne1e2e3\n\na = Mra\nwhere a = (a1, a2, a3)T , and e1, e2, e3 are the three eigenvectors of part (c).\n(f)\nThe same analysis can be applied to the other measurements {li} to obtain\nthe eigenvectors e′\n1, e′\n2, and e′\n3 of a 3×3 \"inertia matrix.\" So we can express\nany vector las a linear combination l′ = b1e′\n1 + b2e′\n2 + b3e′\n3 or\nl′ =\ne′\n1e′\n2e′\n\nb = Mlb\nwhere b = (b1, b2, b3)T . Express the orthonormal rotation matrix R in the\nrelation r′ = R(l′) in terms of the two matrices Mr and Ml. Is the result\northonormal?\n(g)\nDo correspondences between points need to be known when using this\nmethod? Can this method accomodate an arbitrary number of measured\npoints? When will this method of estimating absolute orientation fail?\n(h) What is the smallest number of points that needs to be measured? (Keep in\nmind that the centroid is first subtracted out . . .)\n\nProblem 5:\nHere we consider a machine vision method for determining the\nattitude of a spacecraft. Suppose we wish to use a camera in a spacecraft to\ndetermine the attitude from an image of a star field. We assume, first of all, that\nstar images have been matched with stars in a catalog (see e.g. Lisp by Winston\nand Horn). Since the stars are essentially infinitely far away, the images we obtain\nare not affected by the position of the craft, only its attitude.\n(a)\nLet us first solve an equivalent problem in two dimensions. We are given\ncatalog directions (lxi, lyi) matched with observed directions (rxi, ryi) and\nhave to find the angle of rotation θ that will carry the later into best alignment\nwith the former. Explain why maximizing\nn-1\n\ni=0\n(lxi, lyi) · Rot\n\n(rxi, ryi)\n\nis a reasonable strategy. Show that this is equivalent to maximizing\nc\nn-1\n\ni=0\n\nlxirxi + lyiryi\n\n+ s\nn-1\n\ni=0\n\nlxiryi -lyirxi\n\nsubject to c2 + s2 = 1. Find the solution for c and s.\n(b) Now let us solve the three-dimensional version of the problem. In analogy\nwith the two-dimensional case, we wish to maximize\nn-1\n\ni=0\nli · Rot(ri) =\nn-1\n\ni=0\nli · q ri q∗\nsubject to q · q = 1, where li = (0, li) and ri = (0, ri). Show that this is the\nsame as maximizing\nn-1\n\ni=0\nq ri · li q\nsubject to q · q = 1.\n(c)\nUse the fact that\nl q = q\nand\nq r = q\nfor some orthogonal 4 × 4 matrices and to reduce this to an eigenvec-\ntor/eigenvalue problem.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Resource",
      "title": "6.801/6.866 Machine Vision, Homework 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/9ff064d1c944de62cb896e7c9d9571db_MIT6_801F20_hw5.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nProblem 1: Quick way of estimating (the first coefficient of) radial distortion. A\nsimple model of radial distortion is provided by the equation\n′r = r(1 + k2r 2 + . . .)\nwhere r is the undistorted distance of a point in the image from the center, while\nr′ is the distorted distance. We will ignore higher order coefficients and estimate\nthe first coefficient k2 of radial distortion from an image of a square lying in a\nplane perpendicular to the optical axis. Note that negative k2 leads to \"drum\nbarrel\" distortion, while positive k leads to \"pin cushion\" distortion (Wide angle\nlenses typically exhibit \"drum barrel\" distortion).\nSuppose that the image of the square is centered in the image plane and\nthat in the absence of distortion, the image of a corner of the square would be\na distance rc from the center, while the middle of one of the sides of the image\nof the square would be a distance rm from the center. Next, assume that in the\npresence of radial distortion these distances are r ′ and r ′ respectively. Note\nc\nm\nthat we directly observe only r ′ and r ′ -- we cannot directly measure rc or rm.\nc\nm\n\n√\nClearly rc =\n2rm (and this is also what we would observe in the absence of\nradial distortion).\n√\n(a) The difference r ′ -\n2r ′ is a quantity that varies with distortion and can\nc\nm\nbe used to estimate k2. Show that\n√\n√\n′r -\n2r ′ =\n2k2r 3\nc\nm\nm\n(b) We could use this formula to estimate k2 if only we knew rm. Show that\n′\nrm = 2r ′ -√ r\nm\nc\nHow then can you determine the coefficient of radial distortion k2?\n(c) Suppose measurements yield r ′ = 90 and r ′ = 113.137. Calculate the\nm\nc\ndifference in part (a) above. Then estimate k2, as well as rm and rc .\nProblem 2: Here we add scale to absolute orientation (but only in 2D, for sim\nplicity). Consider a flying robotic system that uses binocular stereo to obtain\nthree-dimensional information from pairs of images. Suppose that the scale of\nthe recovered three-dimensional coordinates is not known accurately because the\nbaseline between exposure stations is not known with precision. Now suppose\nthat two such three-dimensional models -- obtained along different flight paths\n-- are to be related. In this case, determining the absolute orientation requires\nthat, in addition to translation and rotation, a scale factor relating the two three-\ndimensional models be found as well.\nHere we explore -- in a two-dimensional version of the problem -- the\nimplications of chosing different ways of introducing a scale factor into the error\nexpression to be minimized. Consider 'left' coordinates li :\n(a, 0),\n(0, b),\n(-a, 0),\n(0, -b)\nand the corresponding 'right' coordinates ri :\n(c, 0),\n(0, d),\n(-c, 0),\n(0, -d)\nof four points (where l1 = (a, 0) corresponds to r1 = (c, 0) etc.). Hint: It may\nhelp to draw two diagrams showing the two sets of measurements.\n(a) What is the best fit translation from 'left' to 'right' coordinate system? (Hint:\nthis can be determined without any detailed calculation)\n(b) What is the best fit rotation from 'left' to 'right' coordinate system? (Hint:\nthis can be determined without any detailed calculation)\n(c) Suppose that we express the coordinate transformation in the form\nri = s1R(li ) + t\nwhere R(. . .) is the rotation, t is the translation and s1 is a scale factor.\nShow that minimizing the sum of squares of the magnitudes of the errors\n\n∥\n∥\n\nri - s1R(li ) + t leads to\nac + bd\ns1 = a2 + b2\n(Hint: use the known rotation and translation to simplify the expression).\n(d) Suppose that we express the coordinate transformation instead in the form\nli = s2R(ri ) + t\nwhere s2 is a scale factor. Show that minimizing the sum of squares of the\n\nmagnitudes of the errors li - s2R(ri ) + t leads to\nca + db\ns2 = c2 + d2\n(Hint: use the rotation and translation determined above to simplify the\nexpression). When is s2 = 1/s1? What is the sum of squares of errors then?\n(e) The two results above illustrate asymmetries between the way the 'left' and\n'right' coordinates are treated, and the way they appear in the resulting\nexpression for the scale factor. Now consider instead\n√\n√ ri =\ns3R(li ) + t\ns3\n√\n√\n\nFind s3 that minimizes the sum of (1/ s3)ri -\ns3R(li ) + t\n2 . Show\nthat s3 is the square root of s1/s2. Do correspondences between coordinates\nmeasured in the two coordinate systems need to be known in order to recover\nthe scale factor s3?\nProblem 3:\nWhen comparing an object against a library of objects, or when\ndetermining the attitude of an object in space, it is useful to have an even sampling\nof the space of rotations. The rotation groups of the Platonic solids provide\nconvenient uniform sampling of the space of rotations.\nConsider the rotations of a tetrahedron that brings faces, edges and vertices\ninto alignment. We'd like to express these in terms of unit quaternions. It helps\nto line up the tetrahedron with the coordinate axes in a symmetric way:\n(a) Suppose the four vertices are at (a, 0, b), (a, 0, -b), (-a, b, 0), and (-a, -b, 0).\nFor what values of a and b are these four vectors from the centroid of the\ntetrahedron (i) unit vectors, and (ii) at equal angles from one another?\n(b) Some of the rotations of interest are those about lines from the centroid to\nthe vertices (i.e. the four vectors in part(a)). These rotations are through\nangles of ±2π/3. Give the components of two quaternions that correspond\nto rotation through 2π/3 about two different vectors.\n(c) Now take the products of these two quaternions -- in both possible orders\n-- to generate two more rotations (remember that multiplication here does\nnot commute).\n\n(d) Finally, take the transitive closure. That is keep on multiplying the rotations\nyou have generated pairwise with one another until no new rotations are\ngenerated. How many different rotations are there all together? How many\nof these are through angles of ±2π/3? Are there any rotations through\nangles other than 0 and ±2π/3 (Hint: you can reduce the amount of work\nby remembering that -q represents the same rotation as q).\nProblem 4: Relative Orientation. Imagine that we didn't know about the virtues\nof representing rotations using quaternions and we wished to solve the relative\norientation problem using orthonormal matrix notation instead.\n(a) Show that the coplanarity condition [r b l′] = 0 (where l′ is the 'left' vector\nrotated into the 'right' coordinate system) can be written\nr T El = 0\nwhere E = BR is the so-called \"essential matrix,\" with R a 3×3 orthonormal\nrotation matrix, and B the skew symmetric matrix corresponding to taking\nthe cross-product with the baseline. (That is, Bv = b × v for any v.) Show\nthat det(E) = 0 (Hint: find a non-zero vector v such that Ev = 0).\nNext, let's assume that the matrix E has already been estimated from correspon\ndences between the left and right images and focus on recovering the baseline\nand the rotation from the \"essential matrix.\" In essence, we have to split E into\nthe product of a skew-symmetric matrix (B) and an orthonormal matrix (R).\n(b) Show that each column of the \"essential matrix\" is orthogonal to the base\nline. How can you easily obtain the direction of the baseline from any two\ncolumns of the matrix?\n(c) Is the length of the baseline fixed by the \"essential matrix\"? Show that, if a\nset of corresponding ray directions satisfies the \"essential matrix\" constraint\nrT El = 0, then they also satisfy rT E′l = 0 where E′ = kE.\n(d) It is often convenient to separate the recovery of translation from that of\nrotation. Show that EET = -B2 (and hence independent of rotation). Then\nshow that B2 = bbT -(b · b)I, and that Trace(B2) = -2(b.b). Conclude\nthat\nbbT = (1/2)Trace(EET )I -EET\nwhere Trace(EET ) is just the sum of squares of the elements of E. This\nprovides another way to recover the baseline. How can one get b from bbT ?\nAre the magnitude and sign of b uniquely determined?\nProblem 5: When we view the corner of a rectangular building, we obtain three\nedges in the image from which we can deternmine the viewing direction in the\n\ncoordinate system defined by the three sets of parallel edges in the building. To\nsimplify matters, we'll consider orthographic projection (or equivalently, that the\ncamera is aimed so that the image of the corner falls in the center of the image).\nSo, consider an orthographic projection of the corner of a rectangular build\ning. The task is to recover the orientation of the rectangular \"brick\" with respect\nto the image plane from the angles measured in the image between the lines meet\ning at the vertex (for convenience, place the origin at the vertex). Let the ends\n(arbitrarily defined) of the three lines be given by the vectors a, b, and c -- which\nof course all lie in the image plane. (The z component of these three vectors is zero\nsince the z-axis is taken to be the direction of projection and hence perpendicular\nto the image plane.)\nThe image line a corresponds to an edge of the brick parallel to a′ = a + a zˆ,\nfor some unknown a (since in orthographic projection a is obtained from a′ by\ndropping the z component). Similarly the image line b corresponds to an edge\nparallel to b′ = b + b zˆ, while the image line c corresponds to an edge parallel to\n′c = c + c zˆ.\n(a) Find the unknown scalars a, b, and c. Hint: Use the fact that the edges of\n′\n′\nthe brick are supposed to be orthogonal, and hence so are a , b′ , and c .\n(b) Let α be the angle that the vector a′ makes with the image plane (and hence\nwith a). Similarly, let β be the angle between b′ and the image plane and γ\nbe the angle between c′ and the image plane. Show that\n\ncos B cos C\ntan α =\n-\ncos A\nwhere A is the angle in the image plane between the lines b and c, B is the\nangle between the lines c and a, while C is the angle between c and a. Give\nsimilar expressions for tan β and tan γ .\n(c) What constraints -- if any -- on the angles A, B, and C are imposed by the\nfact that the terms under the square-root sign must be non-negative?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.801 / 6.866 Machine Vision, Quiz 1 Review",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/0099e324ff9b880b73bd006d59ca4876_MIT6_801F20_review1.pdf",
      "content": "6.801/6.866: Machine Vision, Quiz 1 Review\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nHere you will find a review of some of the topics covered so far in Machine Vision. These are as follows in the section notes:\n1. Mathematics review - Unconstrained optimization, Green's Theorem, Bezout's Theorem, Nyquist Sampling Theorem\n2. Projection - Perspective vs. Orthographic, Object vs. Image space\n3. Optical Flow - BCCE/optical flow, Time to Contact, vanishing points\n4. Photometry - Radiance, irradiance, illumination, geometry, BRDF, Helmholtz Reciprocity\n5. Different surface types - Lambertian, Hapke\n6. Shape from Shading (SfS) - Hapke case, general case, reflectance maps, image irradiance equation\n7. Photometric Stereo - least squares, multiple measurement variant, multiple light sources variant\n8. Computational molecules - Sobel, Robert, Silver operators, finite differences (forward, backward, and average), Lapla-\ncians\n9. Lenses - Thin lenses, thick lenses, telecentric lens, focal length, principal planes, pinhole model\n10. Patent Review - Edge detector, Object Detection\nQuick Mathematics Review\nLet's quickly touch on some important mathematical theorems that are useful for machine vision.\n- Unconstrained optimization: Suppose we are trying to optimize an objective J(a, b, c) by some set of parameters\n{a, b, c}. We can find the best set of parameters {a∗, b∗, c∗} using first-order conditions:\na∗, b∗, c∗= arg min\na,b,c J(a, b, c)\n→∂J\n∂a = 0\n→∂J\n∂b = 0\n→∂J\n∂c = 0\nThe solution a∗, b∗, c∗is the set of a, b, c that satisfies these first-order conditions. Note that this procedure is identical\nwith maximization.\n- Green's Theorem relates the line integral of a contour to the area located within that contour:\nI\nL\n(Ldx + Mdy) =\nZZ\nD\n∂M\n∂x -∂L\n∂y\n\ndxdy\n\n- B ezout's Theorem: The maximum number of solutions is the product of the polynomial order of each equation in the\nsystem of equations:\nnumber of solutions =\nE\nY\ne=1\noe\n- Nyquist Sampling Theorem: We must sample at twice the frequency of the highest-varying component of our image\nto avoid aliasing and consequently reducing spatial artifacts.\n- Taylor Series: We can expand any analytical, continuous, infinitely-differentiable function into its Taylor Series form\naccording to:\nf(x + δx) = f(x) + δxf ′(x) + (δx)2\n2!\nf ′′(x) + (δx)3\n3!\nf ′′′(x) + (δx)4\n24 f (4)(x) + ... =\ninf\nX\ni=0\n(δx)if (i)(x)\ni!\n, where 0!\n∆= 1\nFor a single variable function, and:\nf(x + δx, y + δy) = f(x, y) + δx\n∂f(x, y)\n∂x\n+ δy\n∂f(x, y)\n∂y\n+ · · ·\nFor a multivariable function.\nProjection: Perspective and Orthographic\nA few important notes to remember:\n- We use CAPITAL LETTERS for WORLD SPACE, and lowercase letters for image space.\n- Orthographic projection is essentially the limit of perspective projection as we move the point we are mapping to the image\nplane far away from the optical axis.\n- We can derive perspective projection from the pinhole model and similar triangles.\nPerspective Projection:\nx\nf = X\nZ , y\nf = Y\nZ (component form)\nf r =\nR · ˆzR (vector form)\nOrthographic Projection:\nx = f\nZ0\nX, y = f\nZ0\ny\nOptical Flow\nOptical flow captures the motion of an image over time in image space. Therefore, we will be interested in how brightness in the\nimage changes with respect to (i) time, (ii) in the x-direction, and (iii) in the y-direction. We will use derivatives to describe\nthese different quantities:\n- u\n∆= dx\ndt - i.e. the velocity in the image space in the x direction.\n- v\n∆= dx\ndt - i.e. the velocity in the image space in the y direction.\n-\n∂E\n∂x - i.e. how the brightness changes in the x direction.\n-\n∂E\n∂y - i.e. how the brightness changes in the y direction.\n-\n∂E\n∂t - i.e. how the brightness changes w.r.t. time.\n\nIf x(t) and y(t) both depend on t, then the chain rule gives us the following for the total derivative:\ndE(x, y, t)\ndt\n= dx\ndt\n∂E\n∂x + dy\ndt\n∂E\n∂y + ∂E\n∂t = 0\nRewriting this in terms of u, v from above:\nuEx + vEy + Et = 0\nThis equation above is known as the Brightness Change Constraint Equation (BCCE). This is also one of the most\nimportant equations in 2D optical flow.\nNormalizing the equation on the right by the magnitude of the brightness derivative vectors, we can derive the brightness\ngradient:\n(u, v) ·\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\n= (u, v) · ˆG = -\nEt\nq\nE2x + E2y\nBrightness Gradient : ˆG =\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\nWhat is the brightness gradient?\n- A unit vector given by:\n\nEx\n√\nE2\nx+E2\ny ,\nEy\n√\nE2\nx+E2\ny\n!\n∈R2.\n- Measures spatial changes in brightness in the image in the image plane x and y directions.\nWe are also interested in contours of constant brightness, or isophotes. These are curves on an illuminated surface that connects\npoints of equal brightness (source: Wikipedia).\nFinally, we are also interested in solving for optimal values of u and v for multiple measurements.\nIn the ideal case with-\nout noise:\nU\nV\n\n=\n(Ex1Ey2 -Ey1Ex2)\nEy2\n-Ey1\n-Ex2\nEx1\n-Et1\n-Et2\n\nWhen there is noise we simply minimize our objective, instead of setting it equal to zero:\nJ(u, v)\n∆=\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nWe solve for our set of optimal parameters by finding the set of parameters that minimizes this objective:\nu∗, v∗= arg min\nu,v J(u, v) = arg min\nu,v\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nSince this is an unconstrained optimization problem, we can solve by finding the minimum of the two variables using two\nFirst-Order Conditions (FOCs):\n-\n∂J(u,v)\n∂u\n= 0\n-\n∂J(u,v)\n∂v\n= 0\nVanishing points: These are the points in the image plane (or extended out from the image plane) that parallel lines in the\nworld converge to. Applications include:\n- Multilateration\n- Calibration Objects (Sphere, Cube)\n- Camera Calibration\n\nWe will also look at Time to Contact (TTC):\nZ\nW\n∆== Z\ndZ\ndt\n= meters\nmeters\nseconds\n= seconds\nLet us express the inverse of this Time to Contact (TTC) quantity as C, which can be interpreted roughly as the number of\nframes until contact is made:\nC\n∆= W\nZ =\nTTC\nPhotometry\nHere, we will mostly focus on some of the definitions we have encountered from lecture:\n- Photometry: Photometry is the science of measuring visible radiation, light, in units that are weighted according to the\nsensitivity of the human eye. It is a quantitative science based on a statistical model of the human visual perception of\nlight (eye sensitivity curve) under carefully controlled conditions.\n- Radiometry: Radiometry is the science of measuring radiation energy in any portion of the electromagnetic spectrum.\nIn practice, the term is usually limited to the measurement of ultraviolet (UV), visible (VIS), and infrared (IR) radiation\nusing optical instruments.\n- Irradiance: E\n∆= δP\nδA (W/m2). This corresponds to light falling on a surface. When imaging an object, irradiance is\nconverted to a grey level.\n- Intensity: I\n∆= δP\nδW (W/ster). This quantity applied to a point source and is often directionally-dependent.\n- Radiance: L\n∆=\nδ2P\nδAδΩ(W/m2× ster). This photometric quantity is a measure of how bright a surface appears in an image.\n- BRDF (Bidirectional Reflectance Distribution): f(θi, θe, φi, φe) = δL(θe,φe)\nδE(θi,φi) . This function captures the fact that\noftentimes, we are only interested in light hitting the camera, as opposed to the total amount of light emitted from an\nobject. Last time, we had the following equation to relate image irradiance with object/surface radiance:\nE = π\n4 L\nd\nf\ncos4 α\nWhere the irradiance of the image E is on the lefthand side and the radiance of the object/scene L is on the right. The\nBRDF must also satisfy Helmholtz reciprocity, otherwise we would be violating the 2nd Law of Thermodynamics.\nMathematically, recall that Helmholtz reciprocity is given by:\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nDifferent Surface Types\nWith many of our photometric properties established, we can also discuss photometric properties of different types of surfaces.\nBefore we dive in, it is important to also recall the definition of surface orientation:\n- p\n∆= ∂z\n∂x\n- q\n∆= ∂z\n∂y\n- Lambertian Surfaces:\n- Ideal Lambertian surfaces are equally bright from all directions, i.e.\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nAND\nf(θi, θe, φi, φe) = K ∈R with respect to θe, φe\n- \"Lambert's Law\":\nEi ∝cos θi = ˆn · ˆsi\n\n- Hapke Surfaces:\n- The BRDF of a Hapke surface is given by:\nf(θi, φi, θe, φe) =\n√cos θe cos θi\n- Isophotes of Hapke surfaces are linear in spatial gradient, or (p, q) space.\n- What do the isophotes of the moon look like, supposing the moon can fall under some of the different types of surfaces we\nhave discussed?\n- Lambertian: We will see circles and ellipses of isophotes, depending on the angle made between the viewer and the\nthe moon.\n- Hapke: Because of the BRDF behavior, isophotes will run longitudinally along the moon in the case in which it is\na Hapke surface.\nShape from Shading (SfS)\nThis is one of the most fundamental problems in machine vision in which we try to estimate the surface of an object from\nbrightness measurements. Several variations of this problem we consider:\n- Hapke surfaces - this leads to linear isophotes in (p, q) space, and allows us to solve for one of the dimensions of interest.\n- General case: There are several techniques we can apply to solve for this:\n- Green's Theorem (see lecture notes handout)\n- Iterative Discrete Optimization (see lecture notes handout)\nFor these problems, we considered:\n- Characteristic strips (x, y, z, p, q)\n- Initial curves/base characteristics\n- Normalizing with respect to constant step sizes\n- A system of 5 ODEs\n- Stationary points and estimates of surfaces around them for initial points\nNext, let us review reflectance maps. A reflectance map R(p, q) is a lookup table (or, for simpler cases, a parametric function)\nthat stores the brightness for particular surface orientations p = ∂z\n∂x, q = ∂z\n∂y.\nThe Image Irradiance Equation relates the reflectance map to the brightness function in the image E(x, y) and is the\nfirst step in many Shape from Shading approaches.\nE(x, y) = R(p, q)\nPhotometric Stereo\nPhotometric stereo is a technique in machine vision for estimating the surface normals of objects by observing that object\nunder different lighting conditions. This problem is quite common in machine vision applications.\nOne way we can solve photometric stereo is by taking multiple brightness measurements from a light source that we move\naround. This problem becomes:\n\n-sT\n-sT\n-sT\n\nn =\n\nE1\nE2\nE3\n\nWritten compactly:\nSn = E -→n = S-1E\nNote that we we need S to be invertible to compute this, which occurs when the light source vectors are not coplanar.\nOther variations of this problem:\n\n- Using light sources at different electromagnetic frequencies\n- Using a camera with different light filters\n- Moving the object to different locations\nComputational Molecules\nThese are crucial components for applying machine concepts to real-life problems.\n1. Ex = 1\nε (E(x, y) -E(x -1, y)) (Backward Difference), 1\nε (z(x + 1, y) -z(x, y)) (Forward Difference)\n2. Ey = 1\nε (E(x, y) -E(x, y -1)) (Backward Difference), 1\nε (E(x, y + 1) -E(x, y)) (Forward Difference)\n3. ∆E = ∇2E = 1\nε2 (4E(x, y) -(E(x -1, y) + E(x + 1, y) + E(x, y -1) + E(x, y + 1)))\n4. Exx = ∂2E\n∂x2 = 1\nε2 (E(x -1, y) -2(x, y) + E(x + 1, y))\n5. Eyy = ∂2z\n∂y2 = 1\nε2 (E(x, y -1) -2(x, y) + E(x, y + 1))\n6. Robert's Cross: This approximates derivatives in a coordinate system rotated 45 degrees (x′, y′). The derivatives can\nbe approximated using the Kx′ and Ky′ kernels:\n∂E\n∂x′ →Kx′ =\n-1\n-1\n\n∂E\n∂y′ →Ky′ =\n-1\n\n7. Sobel Operator: This computational molecule requires more computation and it is not as high-resolution. It is also more\nrobust to noise than the computational molecules used above:\n∂E\n∂x →Kx =\n\n-1\n-1\n\n∂E\n∂y →Ky =\n\n-1\n-1\n\n8. Silver Operators: This computational molecule is designed for a hexagonal grid. Though these filters have some advan-\ntages, unfortunately, they are not compatible with most cameras as very few cameras have a hexagonal pixel structure.\n9. \"Direct Edge\" Laplacian:\nε2\n\n-4\n\n10. \"Indirect Edge\" Laplacian:\n2ε2\n\n-4\n\n11. Rotationally-symmetric Laplacian:\nε2\n\n-4\n\n+ 1\n2ε2\n\n-4\n\n=\n6ε2\n\n-20\n\nSome methods we used for analyzing how well these work:\n1. Taylor Series: From previous lectures we saw that we could use averaging to reduce the error terms from 2nd order\nderivatives to third order derivatives. This is useful for analytically determining the error.\n2. Test functions: We will touch more on these later, but these are helpful for testing your derivative estimates using\nanalytical expressions, such as polynomial functions.\n3. Fourier domain: This type of analysis is helpful for understanding how these \"stencils\"/molecules affect higher (spatial)\nfrequency image content.\n\nFigure 1: Silver Operators with a hexagonal grid.\nLenses\nLenses are also important, because they determine our ability to sense light and perform important machine vision applications.\nSome types of lenses:\n- Thin lenses are the first type of lens we consider. These are often made from glass spheres, and obey the following three\nrules:\n- Central rays (rays that pass through the center of the lens) are undeflected - this allows us to preserve perspective\nprojection as we had for pinhole cameras.\n- The ray from the focal center emerges parallel to the optical axis.\n- Any parallel rays go through the focal center.\n- Thick lenses (cascaded thin lenses)\n- Telecentric lenses - These \"move\" the the Center of Projection to infinity to achieve approximately orthographic pro-\njection.\n- Potential distortions caused by lenses:\n- Radial distortion: In order to bring the entire angle into an image (e.g. for wide-angle lenses), we have the \"squash\"\nthe edges of the solid angle, thus leading to distortion that is radially-dependent. Typically, other lens defects are\nmitigated at the cost of increased radial distortion. Some specific kinds of radial distortion [5]:\n∗Barrel distortion\n∗Mustache distortion\n∗Pincushion distortion\n- Lens Defects: These occur frequently when manufacturing lenses, and can originate from a multitude of different\nissues.\nPatent Review\nThe two patents we touched on were for:\n- Fast and Accurate Edge detection: At a high level, this framework leveraged the following steps for fast edge detection:\n- CORDIC algorithm for estimating (and subsequently quantizing) the brightness gradient direction\n- Thresholding gradients via Non-Maximum Suppression (NMS)\n- Interpolation at the sub-pixel level along with peak finding for accurate edge detection at sub-pixel level\n- Bias compensation\n\n- Plane position\nWe also discussed some issues/considerations with the following aspects of this framework's design:\n- Quantization of gradient direction\n- Interpolation method for sub-pixel accuracy (quadratic, piecewise linear, cubic, etc.)\n- Object detection and pose estimation: At a high level, this framework leveraged the following steps for fast object\ndetection/pose estimation:\n- Finding specific probing points in a template image that we use for comparing/estimating gradients.\n- Using different scoring functions to compare template image to different configurations of the runtime image to\nfind the object along with its pose (position with orientation).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.801/6.866 Machine Vision, Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/6b66c7d9cc47171e923fce80f8fddad6_MIT6_801F20_q1.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nQuiz I\nHanded out: 2020 Oct. 20th\nDue on: 2020 Oct. 29th\nProblem 1:\nContour maps are easy to interpret in part because they provide\na shading effect - that is, brightness is a function of surface orientation. This is\nbecause spacing between dark contour lines varies with the slope of the surface.\nIn the following example steeper areas appear darker. Suppose that black\ncontour lines (reflectance 0) of width w are drawn on white paper (reflectance 1).\nThe map is drawn at a scale s (that is, distances in the real world are s times as\nlarge as they are on the map). Finally, h is the vertical interval between neigboring\ncontours (measured in the real world).\n(a) How does the average reflectance of a region of the map depend on the slope\nm (magnitude of the gradient) of the corresponding portion of the surface\nin the real world?\n(b) What is the reflectance map R(p, q) (for the contour map) in this case?\n(Check that R(p, q) is not less than zero, or greater than one, for any gradi\nent).\n(c) Is there a limit on the slope m above which there is no further variation of\naverage reflectance with slope?\n\n6.801/6.866 Quiz #1\nProblem 2:\nConsider the surface of a rocky planet or moon, the reflecting\nproperties of which are modelled by Hapke's formula\ncos θi / cos θe\nwhere θi is the angle between the surface normal nˆ and the direction sˆ towards\nthe distant light source, while θe is the angle between the surface normal nˆ and\nthe direction to the distant viewer -- taken to be parallel to zˆ in our case. Hint:\nIn the following it is useful to work in gradient-space ((p, q)) -- where the light\nsource direction can be specified by (ps , qs ).\n(a) Show that the slope m of the surface can be determined -- but only in a\nparticular direction -- from brightness E. What is the special direction in\nwhich the slope can be determined? Show that that slope is given by the\nformula\nm = (E2 - cos θs )/ sin θs\nwere θs is the angle between the direction to the light source (sˆ) and the\ndirection to the viewer (zˆ). What happens when θs = 0?\n(b) What do you expect a sphere of this kind of material to look like when\nilluminated from the same direction as it is being viewed from?\n(c) Why are there spatial variations in brightness in the image below of the\n(almost) full moon? Hint: Hapke's formula really applies only to the ancient\nlava found in the maria (\"oceans\").\n\n6.801/6.866 Quiz #1\nProblem 3:\nVector notation can reduce the size of expressions encountered\nwhen dealing with motion vision problems. Let us revisit the recovery of camera\nmotion and/or the orientation of a planar surface from an image sequence. The\nequation aX + bY + cZ = d applies to a planar surface.\n(a) Show that the equation of the plane can be written in the form\nR · n = 1,\nfor some vector n, where R = (X, Y, Z)T . Give an expression for the unit\nsurface normal (in terms of a, b, c, and d). What is the perpendicular\ndistance of the plane from the origin?\n(b) Suppose the plane is in front of an imaging system. Show that under per\nspective projection\nr · n =\nf\nR · zˆ\nwhere f is the principal distance of the imaging system, and r = (x, y, f )T\nis the image of the point R = (X, Y, Z)T in the world.\n(c) Suppose now that the plane is moving with velocity t = (U, V , W )T with\nrespect to the camera (or equivalently that the camera is moving with velocity\n-t w.r.t to plane). Differentiate the perspective projection equations\nx/f = X/Z and y/f = Y/Z\nto obtain expressions for the motion field components u = dx/dt and v =\ndy/dt in terms of the components of the translational motion vector U =\ndX/dt, V = dY/dt and W = dZ/dt.\n(d) Show that the brightness change constraint equation\nuEx + vEy + Et = 0\nleads to an equation of the form\nEt + (s · t)(r · n) = 0,\nin the case of the planar surface, for some vector s. Write the vector s in\nterms of components of the image brighness gradient (Ex , Ey ).\n(e) If t were known, how would you recover n? (Hint: Consider a least squares\napproach using all of the image data).\n(f) If n were known, how would you recover t? (Hint: Consider a least squares\napproach using all of the image data).\n\n6.801/6.866 Quiz #1\nProblem 4: In the case of pure rotation of the camera about its center of projec\ntion, the following error integral is to be minimized in order to determined the\nangular velocity ω = (A, B, C )T :\n(Et + v · ω)2 dx dy.\n(where v is a vector that depends on image position (x, y) and the brightness\ngradient E(x, y) measured at (x, y).) Show that the ω that minimizes the integral\nis the solution of a set of linear equations (or write it in matrix-vector form).\nHint: Remember that a · b = aT b = bT a and that\nd a · b = b,\nda\nand note that rotation here is in 3-D (i.e. not in the image plane).\nProblem 5:\nTo obtain edge positions and directions in an image, we can start\nwith estimates of the gradient magnitude and gradient directions at points on a\ngrid with one pixel spacing (this grid may be aligned with the pixel grid itself or\noffset by 1/2 pixel in each direction). We estimate the subpixel position using\nthe gradient magnitude at three points in the direction of the brightness gradient\nnear a directional local maximum in gradient magnitude. (Refer to US patent\n6,408,109 for details needed below).\nSuppose now that there is a vertical edge in the image passing near the origin\n(so that the direction of the brightness gradient is everywhere parallel to the x-\naxis). Let the estimates of the brightness gradient magnitude be G-, G0, and G+\nat x = -1, x = 0, and x = +1 respectively. (In answering the following, make\nsure to state where you need to use the assumption that G0 > G+ and G0 ≥G-).\nG0\nG+\nG-\n-1\n+1\n(a) For now, assume that the gradient magnitude curve as a function of position\nhas a parabolic shape. That is G(x) can be approximated by ax2 + bx + c,\nfor some unknown parameters a, b, and c. Where is the extremum of the\nparabola? When is the extremum a maximum? Find expressions for the\nparameters a, b, and c in terms of the gradient estimates G-, G0, and G+.\n\n6.801/6.866 Quiz #1\n(b) Show that the maximum in brightness gradient may be found at x = s, where\nG+-G-\ns = 2 (G0-G+)+(G0-G-)\nShow that -1 ≤s ≤+ 1 . That is, the distance of the maximum from the\npoint where the gradient has value G0 is less than or equal to one-half of the\nspacing between the pixels where the gradient magnitude has the values G-,\n√\nG0, and G+ (which may be equal to the pixel spacing -- or\n2 times that\nfor diagonal directions).\nIn order to determine the accuracy of the estimated subpixel edge position, we\nneed to know what kernels are used to estimate the brightness gradient. Suppose\nthat we use the simple kernel [-1, +1], which estimates the first derivative in the\nx-direction at points half way between samples (i.e. if we consider the samples to\nrelate to the centers of square pixels, then the derivative estimates would be for\npoints on the boundaries betwen those square pixels). Further, assume that the\nbrightness profile of a vertical edge passing through the origin is E(x).\n(c) Now suppose that a vertical edge is offset so that it lies at x = s (rather than\nat x = 0), for 0 ≤s ≤0.5. Our task is to estimate s from the known values\nof brightness gradient magnitude. Show that\nG+ -G- = E(-s + 3 ) -E(-s + 1 ) -E(-s -1 ) + E(-s -3 )\nand\n(G0-G+)+(G0-G-) = -E(-s+ 3 )+3E(-s+ 1 )-3E(-s-1 )+E(-s-3 )\n-2\n-1\nIn order to determine the accuracy of the estimated subpixel edge offset, we\nfurther need to know the shape of the edge transition. As a particular example,\nsuppose that a vertical edge in the image centered on the line x = 0 has a smooth\nedge transition of the form\n⎧\nfor x < -2\nE(x) =\n1 + sin( π x)\nfor -2 ≤x ≤+2\n⎩ 1\nfor x > +2\n\n′\n′\n′\n6.801/6.866 Quiz #1\n(where the units for x are pixels).\n(d) Show that in this case (with the derivative estimator described above):\nπ\n3π\nG+ -G- = sin( π s) cos\n-cos\nand\n(G0 -G+) + (G0 -G-) = cos( π s) 3 sin π -sin 3π\n(e) Show that the estimate of the subpixel edge offset then is\n1 tan( π s)\ns = 2 tan( π )\nVerify that the above produces the correct offset for s = -1/2, 0 and 1/2.\n(f) By setting the derivative of (s -s) w.r.t. s equal to zero, show that that\nthe largest difference occurs for s = 0.2927498 . . . Just how large is the\ndifference? How could you compensate for the bias (i.e. the difference\nbetween the true offset s and the estimate s calculated as above)?\nHint: sin(3x) = 3 sin x -4 sin3 x and cos(3x) = 4 cos3 x -3 cos x.\n(Note, by the way, that the above is the result for a vertical edge in the image\nand that the bias will be different for edges of different orientations -- in part\nbecause the distance between the points where the gradient equals G-, G0, and\nG+ depends on edge direction).\nProblem N+1: Zebra Stripes Protect Against Biting Flies. This is a more open-\nended, \"researchy\" question, a place where you can earn some extra points.\nMaybe we can co-author a paper explaining this interesting effect if we get some\ngood ideas! First, read (at a minimum) the abstract and conclusions of the paper:\n\n6.801/6.866 Quiz #1\n\"Benefits of zebra stripes: Behaviour of tabanid flies around zebras and horses\"\nby Tim Caro, et al in PLoS ONE, Vol. 14, No. 2, 2019 Feb. 20 Open access,\navailable at:\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210831\nFrom the Abstract (emphasis mine):\n\"In separate, detailed video analyses, tabanids approached zebras faster\nand failed to decelerate before contacting zebras, and proportionately\nmore tabanids simply touched rather than landed on zebra pelage in\ncomparison to horses\"\nFrom the Conclusions (emphasis mine):\n\"In summary, multiple lines of evidence indicate that stripes prevent\neffective landing by tabanids once they are in the vicinity of the host\nbut did not prevent them approaching from a distance.\"\nNote that the paper has figures and graphs that may be useful. See also popular\npress accounts such as the one in Discovery Magazine:\nhttps://www.discovermagazine.com/planet-earth/zebra-stripes-protect-against-\nflies-now-we-know-how\n\"Frame by frame analyses of our videos showed that flies slowly de\ncelerated as they approached brown or black horses before making a\ncontrolled landing. But they failed to decelerate as they approached\nzebras. Instead they would fly straight past or literally bump into the\nanimal and bounce off.\"\nand https://www.futurity.org/zebra-stripes-flies-flight-1989432/\n\"We found that zebras and horses received a similar number of ap\nproaches from horseflies, probably attracted by their smell--but zebras\nexperienced far fewer landings. Around horses, flies hover, spiral and\nturn before touching down again and again. In contrast, around zebras\nflies either flew right past them or made a single quick landing and flew\noff again.\"\nThere are also some short videos s.a.: https://youtu.be/JyDa8SQ0l3I\n(a)\nIn the context of this course, can you propose an explanation for these ob\nservations?\n(b) What properties, parameters, and measurements of zebras, horses, horse flies\nand tse-tse flies would be helpful in supporting your proposed answer?\n(c)\nDo the above mentioned reports of the work already foreshadow your con\nclusion? If so, how?\n(d) How can we test your proposed explanation -- and eliminate alternatives?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.801/6.866 Machine Vision, Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/398bff79f9c9c729f01ec3ec8ae5ba66_MIT6_801F20_q2.pdf",
      "content": "Massachusetts Institute of Technology\nDepartment of Computer Science and Electrical Engineering\n6.801/6.866\nMachine Vision\nQUIZ II\nProblem 1:\nExterior Orientation. Suppose you are flying above terrain for\nwhich a 3-D digital terrain model (DTM) is available and that you have identified\nseveral landmarks in an image. The problem of exterior orientation is that of\nfinding out where you are -- and how the camera is oriented (This is also the\n\"Location Determination Problem\" (LDP) addressed in the \"Random Sample\nConsensus\" (RANSAC) paper by Fischler and Bolles). Here we consider the\nsituation where three landmarks can be identified in the image (the minimal case\nfor exterior orientation).\n(a)\nLet ri (for i > 0) be the position (in a world coordinate system) of the i-th\nlandmarks on the ground, while r0 is the (unknown) position of the center of\nprojection (COP) of the camera. What is the distance ri,j between landmarks\ni and j?\n(b) If we know the camera's interior orientation (calibration), then we can con\nstruct rays ai (in the camera coordinate system) towards the landmarks by\nconnecting the COP to corresponding image points. Determine the angles\nθi,j between rays ai and aj from the COP towards landmarks i and j.\nr1\nr2\nr3\nr12\nr23\nr0\nr1\nr3\nθ12\nθ23\nr2\n\n∥\n∥\n6.801/6.866 Quiz #2\n(c) Next, we find the lengths of the lines connecting the landmarks to the COP,\ni.e. ri = ri - r0 : Write down an equation in the unknown lengths ri and\nrj , the known distance ri,j and the known angle θi,j . Hint: may need to\nlook up some triangle equalities involving trigonometric terms.\n(d) Suppose that three landmarks are needed to solve for the position of the\nCOP. What is the largest number of solutions that three equations of the type\ndiscussed in part (c) could have by Bezout's theorem? How many different\nsolutions of the equations can be obtained by just flipping the sign of the\nthree quantities ri ?\n(e) Now suppose that we have solved the equations and obtained the three\nlengths ri for i = 1, 2, and 3. Describe geometrically how knowing these\nthree distances allows us to determine the position r0 of the COP (in the\nworld coordinate system). How many solutions can there be? (Note: you\ndo not need to actually solve the equations). If there is more than one\nsolution, do all the solutions make physical sense (in terms of the original\nproblem)?\n(f) We have now figured out where you were when you took the image (r0).\nIt remains to find the camera orientation (in the world coordinate system).\nLet bi = (ri - r0) be vectors from the COP to the landmarks (in the world\ncoordinate system) -- which we can now compute since we have determined\nr0. We have already defined the directions ai of the corresponding rays in\nthe camera coordinate system in part (b). Explain why\nR = (aˆ 1 aˆ 2 aˆ 3)(bˆ 1 bˆ 2 bˆ 3)-1\nis the rotation that takes world coordinates into camera coordinates (where\nthe circumflexes denote unit vectors). Is R orthonormal?\nProblem 3: This question is about the Extended Gaussian Image (EGI) repre\nsentation for object shape, specialized to solids of revolution. Consider a planar\ncurve with the curious property that its curvature is proportional to the distance\nfrom some line (see figure on next page). That is KG = Ar, where KG = dη/ds,\nA is a constant, and r is the perpendicular distance from the line. At the points\nwhere the curve touches the line it is perpendicular to the line.\nNow imagine spinning the curve about the line. Compute the extended Gaussian\nimage (EGI) of the resulting solid of revolution.\n(a) Does this correspond to the EGI of an object we discussed in class?\n(b) Would a method based on matching EGIs confuse these two objects?\n(c) Are there any other convex objects with the same EGI?\n\n6.801/6.866 Quiz #2\nProblem 5: This question is about motion fields induced by camera motion, their\ndependence on distance and on the size field of view. We obtained expressions\nfor the motion field by differentiating the perspective projection equations and\nknowing the velocities of points in the environment.\nEach of the motion fields illustrated on the next page was generated by mov\ning a camera relative to a fixed environment. In each subfigure, all but one motion\nparameter was zero. The camera motion can be described by t = (U, V , W )T\nand ω = (A, B, C)T where U , V , W are the components of translational mo\ntion, while A, B, and C are the components of the rotational motion in the\ncamera-centric ccordinate system.\n(a) For each of the six patterns state which of the six parameter was non-zero.\n(b) Which of the six flow fields would change if the shapes of objects in the scene\nwere changed?\n(c) Which pairs of components of the camera motion would get harder to dis\ntinguish if the field of view (FOV) of the camera was greatly reduced?\n(d) If possible, estimate the field of view θ of the camera in degrees (the ratio of\none half the width of the image to the principle distance is tan θ/2). If it is\nnot possible to recover the width of the field of view, explain why.\n\n6.801/6.866 Quiz #2\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Exam",
      "title": "6.801/6.866: Machine Vision, Quiz 2 Review",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/3ed9a6b63fdba13ecbc21f423953b85f_MIT6_801F20_review2.pdf",
      "content": "6.801/6.866: Machine Vision, Quiz 2 Review\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nHere you will find a high-level review of some of the topics covered since Quiz 1. These are as follows in the section notes:\n1. Relevant Mathematics Review - Rayleigh Quotients, Groups, Levenberg-Marquadt, Bezout's Theorem\n2. Systems/Patents - PatQuick, PatMAx, Fast Convolutions, Hough Transforms\n3. Signals - Sampling, Spline Interpolation, Integral Images, Repeated Block Averaging\n4. Photogrammetry - Photogrammetry Problems, Absolute orientation, Relative Orientation, Interior Orientation, Exterior\nOrientation, Critical Surfaces, Radial and Tangential Distortion\n5. Rotations/Orientation - Gibb's vector, Rodrigues Formula, Quaternions, Orthonormal Matrices\n6. 3D Recognition - Extended Gaussian Image, Solids of Revolution, Polyhedral Objects, Desirable Properties\nRelevant Mathematics Review\nWe'll start with a review of some of the relevant mathematical tools we have relied on in the second part of the course.\n1.1\nRayleigh Quotients\nWe saw from our lectures with absolute orientation that this is a simpler way (relative to Lagrange Multipliers) to take con\nstraints into account:\nThe intuitive idea behind them: How do I prevent my parameters from becoming too large) positive or negative) or too small\n(zero)? We can accomplish this by dividing our objective by our parameters, in this case our constraint. In this case, with the\nRayleigh Quotient taken into account, our objective becomes:\nT\noT\no\nq Nq\no\no\nmin q Nq -→ min\no o o\no\noT o\nq,q·q=1\nq\nq q\n1.2\n(Optional) Groups\nThough we don't cover them specifically in this course, groups can be helpful for better understanding rotations. \"In mathe\nmatics, a group is a set equipped with a binary operation that combines any two elements to form a third element in such a way\nthat four conditions called group axioms are satisfied, namely closure, associativity, identity and invertibility.\" [1]. We won't\nfocus too much on the mathematical details, but being aware of the following groups may be helpful when reading machine\nvision, robotics, control, or vision papers:\n- Orthonormal Rotation Matrices → SO(3) ∈ R3×3 , Special Orthogonal Group. Note that the \"3\" refers to the number\nof dimensions, which may vary depending on the domain.\n- Poses (Translation + Rotation Matrices) → SE(3) ∈ R4×4 , Special Euclidean Group.\nSince these groups do not span standard Euclidean spaces (it turns out these groups are both manifolds - though this is not\ngenerally true with groups), we cannot apply standard calculus-based optimization techniques to solve for them, as we have seen\nto be the case in our photogrammetry lectures.\n\n1.3\n(Optional) Levenberg-Marquadt and Gauss-Newton Nonlinear Optimization\nRecall this was a nonlinear optimization technique we saw to solve photogrammetry problems such as Bundle Adjustment (BA).\nLevenberg-Marquadt (LM) and Gauss-Newton (GN) are two nonlinear optimization procedures used for deriving solutions to\nnonlinear least squares problems. These two approaches are largely the same, except that LM uses an additional regularization\nterm to ensure that a solution exists by making the closed-form matrix to invert in the normal equations positive semidefinite.\nThe normal equations, which derive the closed-form solutions for GN and LM, are given by:\n1. GN: (J(θ)T J(θ))-1θ = J(θ)T e(θ) =⇒ θ = (J(θ)T J(θ))-1J(θ)T e(θ)\n2. LM: (J(θ)T J(θ) + λI)-1θ = J(θ)T e(θ) =⇒ θ = (J(θ)T J(θ) + λI)-1J(θ)T e(θ)\nWhere:\n- θ is the vector of parameters and our solution point to this nonlinear optimization problem.\n- J(θ) is the Jacobian of the nonlinear objective we seek to optimize.\n- e(θ) is the residual function of the objective evaluated with the current set of parameters.\nNote the λI, or regularization term, in Levenberg-Marquadt. If you're familiar with ridge regression, LM is effectively ridge\nregression/regression with L2 regularization for nonlinear optimization problems. Often, these approaches are solved iteratively\nusing gradient descent:\n1. GN: θ(t+1) ← θ(t) - α(J(θ(t))T J(θ(t)))-1J(θ(t))T e(θ(t))\n2. LM: θ(t+1) ← θ(t) - α(J(θ(t))T J(θ(t)) + λI)-1J(θ(t))T e(θ(t))\nWhere α is the step size, which dictates how quickly the estimates of our approaches update.\n1.4\nBezout's Theorem\nThough you're probably well-versed with this thereom by now, its importance is paramount for understanding the number of\nsolutions we are faced with when we solve our systems:\nTheorem: The maximum number of solutions is the product of the polynomial order of each equation in the system of equa\ntions:\nE\nY\nnumber of solutions =\noe\ne=1\nSystems\nIn this section, we'll review some of the systems we covered in this course through patents, namely PatQuick, PatMAx, and Fast\nConvolutions. A block diagram showing how we can cascade the edge detection systems we studied in this class can be found\nbelow:\nFigure 1: An overview of how the patents we have looked at for object inspection fit together.\n2.1\nPatQuick\nThere were three main \"objects\" in this model:\n- Training/template image. This produces a model consisting of probe points.\n- Model, containing probe points.\n\n- Probe points, which encode evidence for where to make gradient comparisons, i.e. to determine how good matches\nbetween the template image and the runtime image under the current pose configuration.\nOnce we have the model from the training step, we can summarize the process for generating matches as:\n1. Loop over/sample from configurations of the pose space (which is determined and parameterized by our degrees of freedom),\nand modify the runtime image according to the current pose configuration.\n2. Using the probe points of the model, compare the gradient direction (or magnitude, depending on the scoring function)\nto the gradient direction (magnitude) of the runtime image under the current configuration, and score using one of the\nscoring functions below.\n3. Running this for all/all sampled pose configurations from the pose space produces a multidimensional scoring surface. We\ncan find matches by looking for peak values in this surface.\n2.2\nPatMAx\n- This framework builds off of the previous PatQuick patent.\n- This framework, unlike PatQuick, does not perform quantization of the pose space, which is one key factor in enabling\nsub-pixel accuracy.\n- PatMAx assumes we already have an approximate initial estimate of the pose.\n- PatMAx relies on an iterative process for optimizing energy, and each attraction step improves the fit of the configuration.\n- Another motivation for the name of this patent is based off of electrostatic components, namely dipoles, from Maxwell. As\nit turns out, however, this analogy works better with mechanical springs than with electrostatic dipoles.\n- PatMAx performs an iterative attraction process to obtain an estimate of the pose.\n- An iterative approach (e.g. gradient descent, Gauss-Newton, Levenberg-Marquadt) is taken because we likely will not\nhave a closed-form solution in the real world. Rather than solving for a closed-form solution, we will run this iterative\noptimization procedure until we reach convergence.\nFigure 2: High-level diagram of the PatMAx system.\n2.3\nFast Convolutions\nThe patent we explore is \"Efficient Flexible Digital Filtering, US 6.457,032.\nThe goal of this system is to efficiently compute filters for multiscale. For this, we assume the form of an Nth-order piece\nwise polynomial, i.e. a Nth-order spline.\n\n2.3.1\nSystem Overview\nThe block diagram of this system can be found below:\nFigure 3: Block diagram of this sparse/fast convolution framework for digital filtering. Note that this can be viewed as a\ncompression problem, in which differencing compresses the signal, and summing decompresses the signal.\nA few notes on this system:\n- Why is it of interest, if we have Nth-order splines as our functions, to take Nth-order differences? The reason for this is\nthat the differences create sparsity, which is critical for fast and efficient convolution. Sparsity is ensured because:\nN\ni\ndN+1\nX\nf(x) = 0 ∀ x if f(x) =\naix , ai ∈ R ∀ i ∈{1, · · · , N}\ndxN+1\ni=0\n(I.e, if f(x) is a order-N polynomial, then the order-(N+1) difference will be 0 for all x.\nThis sparse structure makes convolutions much easier and more efficient to compute by reducing the size/cardinality of\nthe support.\n- Why do we apply an order-(N+1) summing operator? We apply this because we need to \"invert\" the effects of the\norder-(N+1) difference:\nFirst Order : DS = I\nSecond Order : DDSS = DSDS = (DS)(DS) = II = I\n. . .\nOrder K : (D)K (S)K = (DS)K = IK = I\n2.4\nHough Transforms\nMotivation: Edge and line detection for industrial machine vision; we are looking for lines in images, but our gradient-based\nmethods may not necessarily work, e.g. due to non-contiguous lines that have \"bubbles\" or other discontinuities. These discon\ntinuities can show up especially for smaller resolution levels.\nIdea: The main idea of the Hough Transform is to intelligently map from image/surface space to parameter space for\nthat surface.\n\nSome notes on Hough Transforms:\n- Often used as a subroutine in many other machine vision algorithms.\n- Actually generalize beyond edge and line detection, and extend more generally into any domain in which we map a\nparameterized surface in image space into parameter space in order to estimate parameters.\nExample: Hough Transforms with Lines A line/edge in image space can be expressed (in two-dimensions for now, just for\nbuilding intuition, but this framework is amenable for broader generalizations into higher-dimensional lines/planes): y = mx + c.\ny-c\nNote that because y = mx + c, m =\nand c = y - mx. Therefore, this necessitates that:\nx\n- A line in image space maps to a singular point in Hough parameter space.\n- A singular point in line space corresponds to a line in Hough parameter space.\nFigure 4: Example of finding parameters in Hough Space via the Hough Transform.\nPhotogrammetry\nGiven the length of the mathematical derivations in these sections, we invite you to revisit notes in lectures 17-21 for a more\nformal treatment of these topics. In this review, we hope to provide you with strong intuition about different classes of pho\ntogrammetric problems and their solutions.\nFour important problems in photogrammetry that we covered in this course:\n- Absolute Orientation 3D ←→ 3D, Range Data\n- Relative Orientation 2D ←→ 2D, Binocular Stereo\n- Exterior Orientation 2D ←→ 3D, Passive Navigation\n- Intrinsic Orientation 3D ←→ 2D, Camera Calibration\nBelow we discuss each of these problems at a high level. We will be discussing these problems in greater depth later in this and\nfollowing lectures.\n3.1\nAbsolute Orientation\nWe will start with covering absolute orientation. This problem asks about the relationship between two or more objects\n(cameras, points, other sensors) in 3D. Some examples of this problem include:\n1. Given two 3D sensors, such as lidar (light detection and ranging) sensors, our goal is to find the transformation, or pose,\nbetween these two sensors.\n2. Given one 3D sensor, such as a lidar sensor, and two objects (note that this could be two distinct objects at a single point\nin time, or a single object at two distinct points in time), our goal is to find the transformation, or pose, between the\ntwo objects.\n\nFigure 5: General case of absolute orientation: Given the coordinate systems (xl, yl, zl) ∈ R3×3 and (xr, yr, zr) ∈ R3×3 , our goal\nis to find the transformation, or pose, between them using points measured in each frame of reference pi.\n3.2\nRelative Orientation\nThis problem asks how we can find the 2D ←→ 2D relationship between two objects, such as cameras, points, or other sensors.\nThis type of problem comes up frequently in machine vision, for instance, binocular stereo. Two high-level applications\ninclude:\n1. Given two cameras/images that these cameras take, our goal is to extract 3D information by finding the relationship\nbetween two 2D images.\n2. Given two cameras, our goal is to find the (relative) transformation, or pose, between the two cameras.\nFigure 6: Binocular stereo system set up. For this problem, recall that one of our objectives is to measure the translation, or\nbaseline, between the two cameras.\n3.3\nExterior Orientation\nThis photogrammetry problem aims from going 2D -→ 3D. One common example in robotics (and other field related to machine\nvision) is localization, in which a robotic agent must find their location/orientation on a map given 2D information from a\ncamera (as well as, possibly, 3D laser scan measurements).\nMore generally, with localization, our goal is to find where we are and how we are oriented in space given a 2D image\nand a 3D model of the world.\n\nFigure 7: Exterior orientation example: Determining position and orientation from a plane using a camera and landmark\nobservations on the ground.\nFigure 8: Bundle Adjustment (BA) is another problem class that relies on exterior orientation: we seek to find the orientation of\ncameras using image location of landmarks. In the general case, we can have any number of K landmark points (\"interesting\"\npoints in the image) and N cameras that observe the landmarks.\n3.4\nInterior Orientation\nThis photogrammetry problem aims from going 3D -→ 2D. The most common application of this problem is camera calibra\ntion. Camera calibration is crucial for high-precision imaging, as well as solving machine and computer vision problems such as\nBundle Adjustment [1]. Finding a principal point is another example of the interior orientation problem.\n\nFigure 9: Interior orientation seeks to find the transformation between a camera and a calibration object - a task often known as\ncamera calibration. This can be used, for instance, with Tsai's calibration method (note that this method also relies on exterior\norientation).\nRotation\nThere are a myriad of representations for rotations - some of these representations include:\n1. Axis and angle\n2. Euler Angles\n3. Orthonormal Matrices\n4. Exponential cross product\n5. Stereography plus bilinear complex map\n6. Pauli Spin Matrices\n7. Euler Parameters\n8. Unit Quaternions\nWe would also like our representations to have the following properties:\n- The ability to rotate vectors - or coordinate systems\n- The ability to compose rotations\n- Intuitive, non-redundant representation - e.g. rotation matrices have 9 entries but only 3 degrees of freedom\n- Computational efficiency\n- Interpolate orientations\n- Averages over range of rotations\n- Derivative with respect to rotations - e.g. for optimization and least squares\n- Sampling of rotations - uniform and random (e.g. if we do not have access to closed-form solutions)\n- Notion of a space of rotations\nLet us delve into some of these in a little more depth.\n4.1\nAxis and Angle\nThis representation is composed of a vector ωˆ and an angle θ, along with the Gibb's vector that combines these given by\n\nθ\nθ\nωˆ tan\n, which has magnitude tan\n, providing the system with an additional degree of freedom that is not afforded by unit\nvectors. Therefore, we have our full 3 rotational DOF.\n\n4.2\nOrthonormal Matrices\nWe have studied these previously, but these are the matrices that have the following properties:\n1. RT R = RRT = I, RT = R-1 (skew-symmetric)\n2. det |R| = +1\n3. R ∈ SO(3) (see notes on groups above) - being a member of this Special Orthogonal group is contingent on satisfying the\nproperties above.\n4.3\nQuaternions\nIn this section, we will discuss another way to represent rotations: quaternions.\n4.3.1\nHamilton and Division Algebras\nHamilton's insight with quaternions is that these quaternions require a \"4th dimension for the sole purpose of calculating triples\".\nHamilton noted the following insights in order to formalize his quaternion. Therefore, the complex components i, j, k defined\nhave the following properties:\n1. i2 = j2 = k2 = ijk = -1\n2. From which follows:\n(a) ij = k\n(b) jk = i\n(c) ki = j\n(d) ji = -k\n(e) kj = -i\n(f) ik = -j\nNote: As you can see from these properties, multiplication of the components of these quaternions is not commutative.\no\nWe largely use the 4-vector quaternion, denoted q = (q, q) ∈ R4 .\n4.4\nProperties of 4-Vector Quaternions\nThese properties will be useful for representing vectors and operators such as rotation later:\noo\noo\n1. Not commutative: pq 6= qp\noo o\no oo\n2. Associative: (pq)r = p(qr)\noo\no ∗ o\n3. Conjugate: (p, p)∗ = (p, -p) =⇒ (pq) = q p\n4. Dot Product: (p, p) · (q, q) = pq + p + q\no\no\no\n5. Norm: ||q||2 = q · q\n∗\noo\n6. Conjugate Multiplication: qq :\n∗\noo\nqq = (q, q)(q, -q)\n= (q + q · q, 0)\no\no o\n= (q · q)e\no Δ\no ∗ o\noo o\nWhere e = (1, 0), i.e. it is a quaternion with no vector component. Conversely, then, we have: q q = (qq)e.\n∗\no-1\nq\no\no\n7. Multiplicative Inverse: q\n=\n(Except for q = (0, 0), which is problematic with other representations anyway.)\no o\n(q·q\nWe can also look at properties with dot products:\n\noo\noo\no\no\no\no\n1. (pq) · (pq) = (p · p)(q · q)\noo\noo\no\no\no\no\n2. (pq) · (pr) = (p · p)(q · r)\n∗\noo\no\no\noo\n3. (pq) · r = p · (rq )\nWe can also represent these quaternions as vectors. Note that these quaternions all have zero scalar component.\no\n1. r = (0, r)\n∗\no\n2. r = (0, -r)\no\no\n3. r · s = r · s\noo\n4. rs = (-r · s, r × s)\no\no\noo\no\no\n5. (rs) · t = r · (st) = [r s t] (Triple Products)\noo\no\n6. rr = -(r · r)e\nAnother note: For representing rotations, we will use unit quaternions. We can represent scalars and vectors with:\n- Representing scalars: (s, 0)\n- Representing vectors: (0, v)\n4.5\nQuaternion Rotation Operator\nTo represent a rotation operator using quaternions, we need a quaternion operation that maps from vectors to vectors. More\nspecifically, we need an operation that maps from 4D, the operation in which quaternions reside, to 3D in order to ensure that\nwe are in the correct for rotation in 3-space. Therefore, our rotation operator is given:\n∗\no\no\nooo\nr = R(r) = qrq\n∗\no o\n= (Qr)q\no\n= (Q T Q)r\n\nWhere the matrix QT Q is given by:\n⎡\n⎤\nQ T Q =\n⎢⎢⎢⎣\no\no\nq · q\nq0 + q - q - q\nx\ny\nz\n2(qyqx + q0qz )\n2(qzqx - q0qy )\n2(qxqy - q0qz )\nq0 - q + q - q\nx\ny\nz\n2(qzqy + q0qz)\n2(qxqz + q0qy)\n2(qyqz - q0qx)\nq0 - q - q + q\nx\ny\nz\n⎥⎥⎥⎦\nA few notes about this matrix:\no\n- Since the scalar component of q is zero, the first row and matrix of this column are sparse, as we can see above.\no\n\n- If q is a unit quaternion, the lower right 3 × 3 matrix of QT Q will be orthonormal (it is an orthonormal rotation matrix).\no\nooo\n∗\nLet us look at more properties of this mapping r = qrq :\no\no\n1. Scalar Component: r0 = r(q · q)\n2. Vector Component: r = (q2 - q · q)r + 2(q · r)q + 2q(q × r)\no\no\no\no\n3. Operator Preserves Dot Products: r · s = r · s =⇒ r · s = r · s\no\no\no\no\no\no\n4. Operator Preserves Triple Products: (r · s ) · t = (r · s) · t =⇒ (r · s0)t0 = (r · s) · t =⇒ [r s t0] = [r s t]\n5. Composition (of rotations!): Recall before that we could not easily compose rotations with our other rotation repre\nsentations. Because of associativity, however, we can compose rotations simply through quaternion multiplication:\n∗\n∗\n∗\no ooo\no\noo o o ∗ o\noo o oo\np(qrq )p = (pq)r(q p ) = (pq)r(pq) ∗\no Δ oo\nI.e. if we denote the product of quaternions z = pq, then we can write this rotation operator as a single rotation:\n∗\n∗\n∗\n∗\no ooo\no\noo o o ∗ o\noo o oo\nooo\np(qrq )p = (pq)r(q p ) = (pq)r(pq) ∗ = zrz\nThis ability to compose rotations is quite advantageous relative to many of the other representations of rotations we have\nseen before (orthonormal rotation matrices can achieve this as well).\n\n3D Recognition\n5.1\nExtended Gaussian Image\nThe idea of the extended Gaussian Image: what do points on an obect and points on a sphere have in common? They have the\nsame surface normals.\nFigure 10: Mapping from object space the Gaussian sphere using correspondences between surface normal vectors. Recall that\nthis method can be used for tasks such as image recognition and image alignment.\nδS\nδS\ndS\n- Curvature: κ =\n= limδ→0\n=\nδO\nδO\ndO\nδO\nδO\ndO\n- Density: G(η) =\n= limδ→0\n=\nδS\nδS\ndS\n5.2\nEGI with Solids of Revolution\nAre there geometric shapes that lend themselves well for an \"intermediate representation\" with EGI (not too simple, nor too\ncomplex)? It turns out there are, and these are the solids of revolution. These include:\n- Cylinders\n- Spheres\n- Cones\n- Hyperboloids of one and two sheets\nHow do we compute the EGI of solids of revolution? We can use generators that produce these objects to help.\nFigure 11: EGI representation of a generalized solid of revolution. Note that bands in the object domain correspond to bands\nin the sphere domain.\nAs we can see from the figure, the bands \"map\" into each other! These solids of revolution are symmetric in both the object\nand transform space. Let's look at constructing infinitisimal areas so we can then compute Gaussian curvature κ and density G:\n- Area of object band: δO = 2πrδs\n- Area of sphere band: δS = 2π cos(η)δn\n\nThen we can compute the curvature as:\nδS\n2π cos(η)δη\ncos(η)δη\nκ =\n=\n=\nδO\n2πrδs\nrδs\nδO\nδs\nG =\n=\n= sec(η)\nκ\nδS\nδη\nThen in the limit of δ → 0, our curvature and density become:\nδS\ncos η dη\ndη\nκ = lim\n=\n(Where\nis the rate of change of surface normal direction along the arc, i.e. curvature)\nδ→O δO\nr\nds\nds\nδO\nds\nds\nG = lim\n= r sec η\n(Where\nis the rate of change of the arc length w.r.t. angle)\nδ→0 δS\ndη\ndη\ncos η\nκ =\nκ\nr\nRecall that we covered this for the following\n5.3\nSampling From Spheres Using Regular and Semi-Regular Polyhedra\nMore efficient to sample rotations from shapes that form a \"tighter fit\" around the sphere - for instance: polyhedra! Some\npolyhedra we can use:\n- Tetrahedra (4 faces)\n- Hexahedra (6 faces)\n- Octahedra (8 faces)\n- Dodecahedra (12 faces)\n- Icosahedra (20 faces)\nThese polyhedra are also known as the regular solids.\nAs we did for the cube, we can do the same for polyhedra: to sample from the sphere, we can sample from the polyhedra,\nand then project onto the point on the sphere that intersects the line from the origin to the sampled point on the polyhedra.\nFrom this, we get great circles from the edges of these polyhedra on the sphere when we project.\nFun fact: Soccer balls have 32 faces! More related to geometry: soccer balls are part of a group of semi-regular solids,\nspecifically an icosadodecahedron.\n5.4\nDesired Properties of Dividing Up the Sphere/Tessellations\nTo build an optimal representation, we need to understand what our desired optimal properties of our tessellations will be:\n1. Equal areas of cells/facets\n2. Equal shapes of cells/facets\n3. \"Rounded\" shapes of cells/facets\n4. Regular pattern\n5. Allows for easy \"binning\"\n6. Alignment of rotation\nPlatonic and Archimedean solids are the representations we will use for these direction histograms.\nReferences\n1. Groups, https://en.wikipedia.org/wiki/Group (mathematics)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801 / 6.866 Machine Vision, Lecture 20",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/5e98d8a6c0edce797859526dece67aea_MIT6_801F20_lec20.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 20\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 20: Space of Rotations, Regular Tessellations, Critical Surfaces in\nMotion Vision and Binocular Stereo\nIn this lecture, we will transition from solving problems of absolute rotation (which, if you recall, finds the transformation\nbetween two 3D coordinate systems) into relative orientation, which finds the transformation between two 2D coordinate\nsystems. We will start by covering the problem of binocular stereo, and in the process talk about tessellations from solids,\ncritical surfaces.\n1.1\nTessellations of Regular Solids\nAs a brief review from the last lecture, recall that we saw we can encode rotations as 4D points on the unit sphere, where the\ncoordinates of these 4D points correspond to our coefficients for the unit quaternion.\nWhat are tessellations? \"A filling or tessellations of a flat surface is the covering of a plane using one or more geomet\nric shapes (polygons).\" [1]. Tessellations of the surface of the sphere can be based on platonic solids, with 4, 6, 8, 12, and\n20 faces. Each of the tessellations from the platonic solids results in equal area projections on the sphere, but the division is\nsomewhat coarse.\nFor greater granularity, we can look at the 14 Archimedean solids. This allows for having multiple polygons in each\npolyhedra (e.g. squares and triangles), resulting in unequal area in the tessellations on the unit sphere.\nRelated, we are also interested in the rotation groups (recall groups are mathematical sets that obey certain algebras,\ne.g. the Special Orthogonal group) of these regular polyhedra:\n- 12 elements in rotation group for tetrahedron.\n- 24 elements in rotation group for hexahedron.\n- 60 elements in rotation group for dodecahedron.\n- The octahedron is the dual of the cube and therefore occupies the same rotation group as it.\n- The icosahedron is the dual of the dodecahedron and therefore occupies the same rotation group as it.\nA few other notes on tessellations:\n- One frequently-used method for creating tessellations is to divide each face into many triangular or hexagonal areas.\n- Histograms can be created in tessellations in planes by taking square sub-divisions of the region.\n- Hexagonal tessellations are a dual of triangular tessellations.\n\n1.2\nCritical Surfaces\nWhat are Critical Surfaces? Critical surfaces are geometric surfaces - specifically, hyperboloids of one sheet, that lead to\nambiguity in the solution space for relative orientation problems.\nWhy are Critical Surfaces important? Critical surfaces can negatively impact the performance of relative orientation\nsystems, and understanding their geometry can enable us to avoid using strategies that rely on these types of surfaces in order\nto find the 2D transformation, for instance, between two cameras.\nWe will discuss more about these at the end of today's lecture. For now, let's introduce quadrics - geometric shapes/sur\nfaces defined by second-order equations in a 3D Cartesian coordinate system:\nx\ny\n1. Ellipsoid: a2 + b2 + z\nc2 = 1\nFigure 1: 3D geometric depiction of an ellipsoid, one member of the quadric family.\n2. Sphere: x + y + z = 1 (or more generally, = r ∈ R )\nFigure 2: 3D geometric depiction of a sphere, another member of the quadric family and a special case of the ellipse.\n2 - z2\n3. Hyperboloid of One Sheet: x + y\n= 1\nThis quadric surface is ruled: we can embed straight lines in the surface, despite its quadric function structure.\nFigure 3: 3D geometric depiction of a hyperboloid of one sheet, another member of the quadric family and a special case of the\nellipse.\n4. Hyperboloid of Two Sheets: x2 - y2 - z = 1\n\nFigure 4: 3D geometric depiction of a hyperboloid of one sheet, another member of the quadric family and a special case of the\nellipse.\nz\nx\ny\n5. Cone: c2 = a2 + b2\nFigure 5: 3D geometric depiction of a cone, another member of the quadric family and a special case of the hyperboloid of one\nsheet.\nz\nx\ny\n6. Elliptic Paraboloid:\n=\n+\nc\na2\nb2\nNote that this quadric surface has a linear, rather than quadratic dependence, on z.\nFigure 6: 3D geometric depiction of a elliptic paraboloid, another member of the quadric family and a special case of the\nhyperboloid of one sheet.\nz\nx - y\n7. Hyperbolic Paraboloid:\n=\nb2\nc\na\nNote that this quadric surface also has a linear, rather than quadratic dependence, on z.\n\nFigure 7: 3D geometric depiction of a hyperbolic paraboloid, another member of the quadric family and a special case of the\nhyperboloid of one sheet.\nAnother special case is derived through planes. You may be wondering - how can we have a quadratic structure from planes? We\ncan derive a surface with quadratic terms by considering the intersection of two planes. This intersection of planes is computed\nanalytically as a product of two linear equations, resulting in a quadratic equation.\n1.3\nRelative Orientation and Binocular Stereo\nProblem of interest: Computing 3D from 3D using two cameras - a problem known as binocular stereo. We found this\nproblem was easy to solve if the geometry of the two cameras, in this case known as the two-view geometry, is known and\ncalibrated (usually this results in finding a calibration matrix K). To achieve high-performing binocular stereo systems, we need\nto find the relative orientation between the two cameras. A few other notes on this:\n- Calibration is typically for binocular stereo using baseline calibration.\n- Recall that like absolute orientation, we have duality in te problems we solve: we are able to apply the same machine\nvision framework regardless of \"if the camera moved of it the world moved\".\n- Therefore, the dual problem to binocular stereo is structure from motion, also known as Visual Odometry (VO).\nThis involves finding transformations over time from camera (i.e. one moving camera at different points in time).\n1.3.1\nBinocular Stereo\nRecall that our goal with binocular stereo is to find the translation (known as the baseline, given by b ∈ R3 . The diagram\nbelow illustrates our setup with two cameras, points in the world, and the translation between the two cameras given as this\naforementioned baseline b.\nFigure 8: Binocular stereo system set up. For this problem, recall that one of our objectives is to measure the translation, or\nbaseline, between the two cameras.\nWhen we search for correspondences, we need only search along a line, which gives us a measure of binocular disparity\nthat we can then use to measure distance between the cameras.\n\nThe lines defined by these correspondences are called epipolar lines. Places that pass through epipolar lines are called\nepipolar planes, as given below:\nFigure 9: Epipolar planes are planes that pass through epipolar lines.\nNext, in the image, we intersect the image plane with our set of epipolar planes, and we look at the intersections of these\nimage planes (which become lines). The figure below illustrates this process for the left and right cameras in our binocular stereo\nsystem.\nFigure 10: After finding the epipolar planes, we intersect these planes with the image plane, which gives us a set of lines in both\nthe left and righthand cameras/coordinate systems of our binocular stereo system.\nA few notes on this process:\n- For system convergence, we want as much overlap as possible between the two sets of lines above, depicted in Figure 10.\n- The entire baseline b projects to a single point in each image.\n- As we stated before, there are line correspondences if we already know the geometry, i.e. if we want correspondences\nbetween the lines in red above, we need only look along the red lines in each image.\n- Recall that our goal is to find the transformation between two cameras - i.e. baseline (or translation, in dual structure\nfrom motion problem).\n- Recall, as we saw for absolute orientation, that we also solve for rotation in addition to the baseline/translation.\nTogether, these translation + rotation variables lead us to think that we are solving a problem of 6 degrees of freedom\n(DOF), but because of scale factor ambiguity (we cannot get absolute sizes of objects/scenes we image), we cannot\nget absolute length of the baseline b, and therefore, we treat the baseline as a unit vector. Treating the baseline as a\nunit vector results in one less DOF and 5 DOF for the system overall (since we now only have 2 DOF for the unit vector\nbaseline).\n1.3.2\nHow Many Correspondences Do We Need?\nAs we have done so for other systems/problems, we consider the pertinent question: How many correspondences do we\nneed to solve the binocular stereo/relative orientation problem?. To answer this, let us consider what happens when\nwe are given different numbers of correspondences:\n- With 1 correspondence, we have many degrees of freedom left - i.e. not enough constraints for the number of unknowns\nremaining. This makes sense considering that each correspondence imposes a maximum of 3 DOF for the system.\n\n- With 2 correspondences, we still have the ability to rotate one of the cameras without changing the correspondence, which\nimplies that only 4 out of the 5 constraints are satisfied. This suggests we need more constraints.\nIt turns out we need 5 correspondences needed to solve the binocular stereo/relative orientation problem - each correspondence\ngives you 1 constraint. Why? Each image has a disparity, with two components (this occurs in practice). There are two\ntypes of disparity, each corresponding to each of the 2D dimensions. Note that disparity computes pixel discrepancies between\ncorrespondences between images.\n- Horizontal disparity corresponds to depth.\n- Vertical disparity corresponds to differences in orientation, and actually takes the constraints into account. In practice,\nvertical disparity is tuned out first using an iterative sequence of 5 moves.\nIn practice, we would like to use more correspondences for accuracy. With more correspondences, we no longer try to find\nexact matches, but instead minimize a sum of squared errors using least squares methods. We minimize sum of squares of error\nin image position, not in the 3D world.\nHowever, one issue with this method is nonlinearity:\n- This problem setup results in 7 second-order equations, which, by Bezout's Theorem, gives 27 = 128 different solutions to\nthis problem.\n- Are there really 128 solutions? There are methods that have gotten this down to 20. With more correspondences and more\nbackground knowledge of your system, it becomes easier to determine what the true solution is.\n1.3.3\nDetermining Baseline and Rotation From Correspondences\nTo determine baseline and rotation for this binocular stereo problem, we can begin with the figure below:\nFigure 11: Epipolar plane for the binocular stereo system.\nThe vectors rl,i (the left system measurement after the rotation transformation has been applied), rr,i, and b are all coplanar\nin a perfect system. Therefore, if we consider the triple product of these 3 vectors, the volume of the parallelipiped, which can\nbe constructed from the triple product, should have zero volume because these vectors are coplanar (note that this is the\nideal case). This is known as the coplanarity condtion:\nV = [l\n,i rr,i b] = 0\nA potential solution to find the optimal baseline (and rotation): we can use least squares to minimize the volume of the paral\nlelipiped corresponding to the triple product of these three (hopefully near coplanar) vectors. This is a feasible approach, but\nalso have a high noise gain/variance.\nThis leads us to an important question: What, specifically, are we trying to minimize? Recall that we are matching cor\nrespondences in the image, not in the 3D world. Therefore, the volume of the parallelipiped is proportional to the\nerror, but does not match it exactly. When we have measurement noise, the rays from our vectors in the left and righthand\nsystems no longer intersect, as depicted in the diagram below:\n\nFigure 12: With measurement noise, our rays do not line up exactly. We will use this idea to formulate our optimization problem\nto solve for optimal baseline and rotation.\nWe can write that the error is perpendicular to the cross product between the rotated left and the right rays:\nrl,i × rr\nAnd therefore, we can write the equation for the \"loop\" (going from the rotated left coordinate system to the right coordinate\nsystem) as:\nl + γ(rl × rr\nl × rr\nαr\n) = b + βrr\nWhere the error we seek to minimize (r\n) is multiplied by a parameter γ.\nTo solve for our parameters α, β, and γ, we can transform this vector equation into 3 scalar equations by taking dot prod\nucts. We want to take dot products that many many terms drop to zero, i.e. where orthogonality applies. Let's look at these 3\ndot products:\nl × rr\nl + γ(rl × rr)) · (rl × rr) = γ||rl × rr\nl × rr\nl × rr\n1. ·(r\n): This yields the following:\n||\nLefthand side :\n(αr\nRighthand side :\n(b + βrr) · (r\n) = b · r\n+ 0 = [b r\n]\nl rr\nCombining : γ||r 0 × r\nl\nr||2\n2 = [b r 0\nl rr]\nIntuitively, this says that the error we see is proportional to teh triple product (the volume of the parallelipiped). Taking\nour equation with this dot product allows us to isolate γ.\nl × (rl × rr\nl + γ(rl × rr\nl × (rl × rr)) = β||rl × rr\nl × (rl × rr\nl) · (rl × rr\nl × rr\nl) · (rl × rr\n2. ·(r\n)): This yields the following:\n||\nLefthand side :\n(αr\n)) · ((r\nRighthand side :\n(b + βrr) · ((r\n)) = (b × r\n)\nβ||r\n||\nCombining :\n= (b × r\n)\nTaking this dot product with our equation allows us to find β. We can repeat an analogous process to solve for α.\nl × rr\nl × rr\nl × rr\nl × rr\n) · ((rr × (rl × rr\nl × rr\nl × rr\nl × rr\n3. ·(rr × (r\n)): This yields the following:\nl + γ(r\nRighthand side :\n(b + βrr\n)) = α||r\n||\nLefthand side :\n(αr\n)) · ((rr × (r\n)) = (b × rr) · (r\n)\nα||r\n||\nCombining :\n= (b × rr) · (r\n)\nTaking this dot product with our equation allows us to find α.\nWith this, we have now isolated all three of our desired parameters. We can then take these 3 equations to solve for our 3\nunknowns α, β, and γ. We want |γ| to be as small as possible. We also require α and β to be non-negative, since these indicate\nthe scalar multiple of the direction along the rays in which we (almost) get an intersection between the left and right coordinate\nsystems. Typically, a negative α and/or β results in intersections behind the camera, which is often not physically feasible.\nIt turns out that one of the ways discard some of the 20 solutions produced by this problem is to throw out solutions that\nresult in negative α and/or β.\nNext, we can consider the distance this vector corresponding to the offset represents. This distance is given by:\n[b rl rr\nl × rr\nd = γ||r\n||2 = ||r0\nl\n]\n× rr||2\n\nClosed-Form Solution: Because this is a system involving 5 second-order equations, and the best we can do is reduce this\nto a single 5th-order equation, which we do not (yet) have the closed-form solutions to, we cannot solve for this problem in\nclosed-form. However, we can still solve for it numerically. We can also look at solving this through a weighted least-squares\napproach below.\n1.3.4\nSolving Using Weighted Least Squares\nBecause our distance d can get very large without a huge error in image position, d is not representative of our error of interest.\nHowever, it is still proportional. We can account for this difference by using a weighting factor wi, which represents the conver\nsion factor from 3D error to 2D image error). We will denote the ith weight as wi.\nTherefore, incorporating this weighting factor, our least squares optimization problem becomes:\nn\nX\nmin\nwi[b rl rr]2 , subject to b · b = ||b||2 = 1\nb,R(·) i=1\nHow do we solve this? Because wi will change as our candidate solution changes, we will solve this problem iteratively and in\nan alternating fashion - we will alternate between updated our conversion weights wi and solving for a new objective given the\nrecent update of weights. Therefore, we can write this optimization problem as:\nn\nX\nb ∗ , R ∗ = arg\nmin\nwi[b rl rr]2\nb,||b|| =1,R(·) i=1\nn\n\nX\nmin\n(rr,i × b) · r\n= arg\nwi\nl,i\nb,||b||2\n2=1,R(·) i=1\nIntuition for these weights: Loosely, we can think of the weights as being the conversion factor from 3D to 2D error, and\nf\ntherefore, they can roughly be thought of as w = Z , where f is the focal length and Z is the 3D depth.\nNow that we have expressed a closed-form solution to this problem, we are ready to build off of the last two lectures and\napply our unit quaternions to solve this problem. Note that because we express the points from the left coordinate system in a\nrotated frame of reference (i.e. with the rotation already applied), then we can incorporate the quaternion into this definition\nto show how we can solve for our optimal set of parameters given measurements from both systems:\n∗\no\noo o\nr = qrlq\nl\nThen we can solve for this as t:\nt = (rr × b) · r 0\nl\no\n∗\no\noo o\n= rrb · qrlq\no\no\no\noo\n= rr(bq) · qrl\no\no\no\no\noo\nΔ\no\n= rdd · qrl, where d = bq, which we can think of as a product of baseline and rotation.\no\no\nRecall that our goal here is to find the baseline b - this can be found by solving for our quantity d and multiplying it on the\n∗\no\nrighthand side by q :\no\no\no\no\n∗\n∗\no\noo\no\ndq = bqq = be = b\n∗\no\noo\n(Recall that e = (1, 0) = qq )\nAt first glance, it appears we have 8 unknowns, with 5 constraints. But we can add additional constraints to the system to make\nthe number of constraints equal the number of DOF:\no\no\no\n1. Unit quaternions: ||q||2 = q · q = 1\no\no\no\n2. Unit baseline: ||b||2 = b · b = 1\no\no\no\no\n3. Orthogonality of q and d: q · d = 0\nTherefore, with these constraints, we are able to reach a system with 8 constraints. Note that we have more constraints to\nenforce than with the absolute orientation problem, making the relative orientation problem more difficult to solve.\n\n1.3.5\nSymmetries of Relative Orientation Approaches\nWe can interchange the left and right coordinate system rays. We can do this for this problem because we have line intersections\nrather than line rays. These symmetries can be useful for our numerical approaches. The equation below further demonstrates\no\no\nthis symmetry by showing we can interchange the order of how d and q interact with our measurements to produce the same\nresult.\no\no\noo\nt = rrd · qrl\no\no o\no\n= rrq · drl\nGiven these symmetries, we can come up with some sample solutions:\no\no\n1. {q, d}\no\no\n2. {-q, d} (×2)\no\no\n3. {q, -d} (×2)\no o\n4. {d, q} (×2)\no\nHow do we avoid having to choose from all these solutions? One approach: Assume/fix one of the two unknowns d\no\nor q. This results in a linear objective and a linear problem to solve, which, as we know, can be solved with least squares\napproaches, giving us a closed-form solution:\no\no\n- Option 1: Assume q is known and fix it -→ solve for d\no\no\n- Option 2: Assume d is known and fix it -→ solve for q\nNote that both of the approaches above can be solved with least squares!\nWhile this is one approach, a better approach is to use nonlinear optimization, such as Levenberg-Marquadt (often\ncalled LM in nonlinear optimization packages). An optional brief intro to Levenberg-Marquadt is presented at the end of this\nlecture summary.\nLevenberg-Marquadt (LM) optimization requires a non-redundant parameterization, which can be achieved with either Gibbs\nvectors or quaternions (the latter of which we can circumvent the redundancies of Hamilton's quaternions by treating the\n\nθ\nθ\nredundancies of this representation as extra constraints. Recall that Gibbs vectors, which are given as (cos\n, sin\nωˆ), have\na singularity at θ = π.\n1.3.6\nWhen Does This Fail? Critical Surfaces\nLike all the other machine vision approaches we have studied so far, we would like to understand when and why this system fails:\n- Recall that we need at least 5 correspondences to solve for the baseline and rotation. With less correspondences, we\ndon't have enough constraints to find solutions.\n- Gef arliche Fla achen, also known as \"dangerous or critical surfaces\": There exist surfaces that make the relative\norientation problem difficult to solve due to the additional ambiguity that these surfaces exhibit. One example: Plane\nflying over a valley with landmarks:\n\nFigure 13: A plane flying over a valley is an instance of a critical surface. This is because we can only observe angles, and not\npoints. Therefore, locations of two landmarks are indistinguishable.\nTo account for this, pilots typically plan flight paths over ridges rather than valleys for this exact reason:\nFigure 14: When planes fly over a ridge rather than a valley, the angles between two landmarks do change, allowing for less\nambiguity for solving relative orientation problems.\nHow do we generalize this case to 3D? We will see that this type of surface in 3D is a hyperboloid of one sheet. We\nneed to ensure sections of surfaces you are looking at do not closely resemble sections of hyperboloids of one sheet.\nThere are also other types of critical surfaces that are far more common that we need to be mindful of when con\nsidering relative orientation problems - for instance, the intersection of two planes. In 2D, this intersection of two planes\ncan be formed by the product of their two equations:\n(ax + by + c)(dx + ey + f) = adx2 + aexy + afx + bdxy + bey2 + bfy + cdx + cey + cf = 0\nWe can see that this equation is indeed second order with respect to its spatial coordinates, and therefore belongs to the\nfamily of quadric surfaces and critical surfaces.\nFigure 15: The intersection of two planes is another type of critical surface we need to be mindful of. It takes a second-order\nform because we multiply the two equations of the planes together to obtain the intersection.\n1.3.7\n(Optional) Levenberg-Marquadt and Nonlinear Optimization\nLevenberg-Marquadt (LM) and Gauss-Newton (GN) are two nonlinear optimization procedures used for deriving solutions to\nnonlinear least squares problems. These two approaches are largely the same, except that LM uses an additional regularization\n\nterm to ensure that a solution exists by making the closed-form matrix to invert in the normal equations positive semidefinite.\nThe normal equations, which derive the closed-form solutions for GN and LM, are given by:\n1. GN: (J(θ)T J(θ))-1θ = J(θ)T e(θ) =⇒ θ = (J(θ)T J(θ))-1J(θ)T e(θ)\n2. LM: (J(θ)T J(θ) + λI)-1θ = J(θ)T e(θ) =⇒ θ = (J(θ)T J(θ) + λI)-1J(θ)T e(θ)\nWhere:\n- θ is the vector of parameters and our solution point to this nonlinear optimization problem.\n- J(θ) is the Jacobian of the nonlinear objective we seek to optimize.\n- e(θ) is the residual function of the objective evaluated with the current set of parameters.\nNote the λI, or regularization term, in Levenberg-Marquadt. If you're familiar with ridge regression, LM is effectively ridge\nregression/regression with L2 regularization for nonlinear optimization problems. Often, these approaches are solved iteratively\nusing gradient descent:\n1. GN: θ(t+1) ← θ(t) - α(J(θ(t))T J(θ(t)))-1J(θ(t))T e(θ(t))\n2. LM: θ(t+1) ← θ(t) - α(J(θ(t))T J(θ(t)) + λI)-1J(θ(t))T e(θ(t))\nWhere α is the step size, which dictates how quickly the estimates of our approaches update.\n1.4\nReferences\n1. Tesselation, https://en.wikipedia.org/wiki/Tessellation\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801 / 6.868 Machine Learning, Lecture 19",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/3684c9529d76a9a87fe3db7ae5e91f71_MIT6_801F20_lec19.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 19\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 19: Absolute Orientation in Closed Form, Outliers and Robustness,\nRANSAC\nThis lecture will continue our discussion of photogrammetry topics - specifically, covering more details with the problem of\nabsolute orientation. We will also look at the effects of outliers on the robustness of closed-form absolute orientation, and how\nalgorithms such as RANSAC can be leveraged as part of an absolute orientation, or, more generally, photogrammetry pipeline\nto improve the robustness of these systems.\n1.1\nReview: Absolute Orientation\nRecall our four main problems of photogrammetry:\n- Absolute Orientation 3D ←→3D\n- Relative Orientation 2D ←→2D\n- Exterior Orientation 2D ←→3D\n- Intrinsic Orientation 3D ←→2D\nIn the last lecture, we saw that when solving absolute orientation problems, we are mostly interested in finding transfor-\nmations (translation + rotation) between two coordinate systems, where these coordinate systems can correspond to objects\nor sensors moving in time (recall this is where we saw duality between objects and sensors).\nLast time, we saw that one way we can find an optimal transformation between two coordinate systems in 3D is to de-\ncompose the optimal transformation into an optimal translation and an optimal rotation. We saw that we could solve for\noptimal translation in terms of rotation, and that we can mitigate the constraint issues with solving for an orthonormal rotation\nmatrix by using quaternions to carry out rotation operations.\n1.1.1\nRotation Operations\nRelevant to our discussion of quaternions is identifying the critical operations that we will use for them (and for orthonormal\nrotation matrices). Most notably, these are:\n1. Composition of rotations:\nop\noq = (p, q)(q, q) = (pq -q · q, pq + qq + q × q)\n2. Rotating vectors:\nor\n′ =\noq\nor\noq\n∗= (q2 -q · q)r + 2(q · r)q + 2q(q × r)\nRecall from the previous lecture that operation (1) was faster than using orthonormal rotation matrices, and operation (2)\nwas slower.\n\n1.1.2\nQuaternion Representations: Axis-Angle Representation and Orthonormal Rotation Matrices\nFrom our previous discussion, we saw that another way we can represent quaternions is through the axis-angle notation (known\nas the Rodrigues formula):\nr′ = (cos θ)r + (1 -cos θ)(ˆω · r)ˆω + sin θ(ˆω × r)\nCombining these equations from above, we have the following axis-angle representation:\noq ⇐⇒ˆω, θ, q = cos\nθ\n\n, q = ˆω sin\nθ\n\n=⇒\noq =\n\ncos\nθ\n\n, ˆω sin\nθ\n!\nWe also saw that we can convert these quaternions to orthonormal rotation matrices. Recall that we can write our vector rotation\noperation as:\noq\nor\noq\n∗= ( QT Q)\nor, where\nQT Q =\n\noq ·\noq\nq2\n0 + q2\nx -q2\ny -q2\nz\n2(qxqy -q0qz)\n2(qxqz + q0qy)\n2(qyqx + q0qz)\nq2\n0 -q2\nx + q2\ny -q2\nz\n2(qyqz -q0qx)\n2(qzqx -q0qy)\n2(qzqy + q0qz)\nq2\n0 -q2\nx -q2\ny + q2\nz\n\nThe matrix QT Q has skew-symmetric components and symmetric components. This is useful for conversions. Given a\nquaternion, we can compute orthonormal rotations more easily. For instance, if we want an axis and angle representation, we\ncan look at the lower right 3 × 3 submatrix, specifically its trace:\nLet R = [ QT Q]3×3,lower, then :\ntr(R) = 3q2\n0 -(q2\nx + q2\ny + q2\nz)\n= 3 cos2\nθ\n\n-(sin2\nθ\n\n(Substituting our axis-angle representation)\n-cos2\nθ\n\n+ sin2\nθ\n\n-1 (Subtracting Zero)\n→2 cos2\nθ\n\n-sin2\nθ\n\n-1\n= 2 cos θ -1\n=⇒cos θ = 1\n2(tr(R) -1)\nWhile this is one way to get the angle (we can solve for θ through arccos of the expression above), it is not the best way to do\nso: we will encounter problems near θ ≈0, π. Instead, we can use the off-diagonal elements, which depend on sin\n\nθ\n\ninstead.\nNote that this works because at angles θ where cos\n\nθ\n\nis \"bad\" (is extremely sensitive), sin\n\nθ\n\nis \"good\" (not as sensitive),\nand vice versa.\n1.2\nQuaternion Transformations/Conversions\nNext, let us focus on how we can convert between quaternions and orthonormal rotation matrices. Given a 3 × 3 orthonormal\nrotation matrix r, we can compute sums and obtain the following system of equations:\n1 + r11 + r22 + r33 = 4q2\n1 + r11 -r22 -r33 = 4q2\nx\n1 -r11 + r22 -r33 = 4q2\ny\n1 -r11 -r22 + r33 = 4q2\nz\nThis equation can be solved by taking square roots, but due to the number of solutions (8 by Bezout's theorem, allowing for the\nflipped signs of quaternions, we should not use this set of equations alone to find the solution).\n\nInstead, we can compute these equations, evaluate them, take the largest for numerical accuracy, arbitrarily select to use\nthe positive version (since there is sign ambiguity with the signs of the quaternions), and solve for this. We will call this selected\nrighthand side qi.\nFor off-digaonals, which have symmetric and non-symmetric components, we derive the following equations:\nr32 -r23 = 4q0qx\nr13 -r31 = 4q0qy\nr21 -r12 = 4q0qx\nr21 + r12 = 4qxqy\nr32 + r23 = 4qyqz\nr13 + r31 = 4qzqz\nAdding/subtracting off-diagonals give us 6 relations, of which we only need 3 (since we have 1 relation from the diagonals). For\ninstance, if we have qi = qy, then we pick off-diagonal relations involving qy, and we solve the four equations given by:\n1 -r11 + r22 -r33 = 4q2\ny\nr13 -r31 = 4q0qy\nr32 + r23 = 4qyqz\nr13 + r31 = 4qzqz\nThis system of four equations gives us a direct way of going from quaternions to an orthonormal rotation matrix. Note that this\ncould be 9 numbers that could be noisy, and we want to make sure we have best fits.\n1.3\nTransformations: Incorporating Scale\nThus far, for our problem of absolute orientation, we have considered transformations between two coordinate systems of being\ncomposed of translation and rotation. This is often sufficient, but in some applications and domains, such as satellite imaging\nfr topographic reconstruction, we may be able to better describe these transformations taking account not only translation and\nrotation, but also scaling.\nTaking scaling into account, we can write the relationship between two point clouds corresponding to two different coordi-\nnate systems as:\nr′\nr = sR(r′\nl)\nWhere rotation is again given by R ∈SO(3), and the scaling factor is given by s ∈R+ (where R+ ∆= {x : x ∈R , x > 0}). Recall\nthat r′\nr and r′\nl are the centroid-subtracted variants of the point clouds in both frames of reference.\n1.3.1\nSolving for Scaling Using Least Squares: Asymmetric Case\nAs we did before, we can write this as a least-squares problem over the scaling parameter s:\nmin\ns\nn\nX\ni=1\n||r′\nr,i -sR(r′\nl,i)||2\nAs we did for translation and rotation, we can solve for an optimal scaling parameter:\ns∗= arg min\ns\nn\nX\ni=1\n||r′\nr,i -sR(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2s\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s2\nn\nX\ni=1\n||R(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2s\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s2\nn\nX\ni=1\n||r′\nl,i||2\n(Rotation preserves vector lengths)\nNext, let us define the following terms:\n\n1. sr\n∆= Pn\ni=1\n\n||r′\nr,i||2\n\n2. D\n∆= Pn\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n3. sl\n∆= Pn\ni=1 ||r′\nl,i||2\nThen we can write this objective for the optimal scaling factor s∗as:\ns∗= arg min\ns {J(s)\n∆= sr -2sD + s2sl}\nSince this is an unconstrained optimization problem, we can solve this by taking the derivative w.r.t. s and setting it equal to 0:\ndJ(s)\nds\n= d\nds\n\nsr -2sD + s2sl\n\n= 0\n= -2D + s2sl = 0 =⇒s = D\nsl\nAs we also saw with rotation, this does not give us an exact answer without finding the orthonormal matrix R, but now we are\nable to remove scale factor and back-solve for it later using our optimal rotation.\n1.3.2\nIssues with Symmetry\nSymmetry question: What if instead of going from the left coordinate system to the right one, we decided to go from right\nto left? In theory, this should be possible: we should be able to do this simply by negating translation and inverting our\nrotation and scaling terms. But in general, doing this in practice with our OLS approach above does not lead to sinverse = 1\ns\n- i.e. inverting the optimal scale factor does not give us the scale factor for the reverse problem.\nIntuitively, this is the case because the version of OLS we used above \"cheats\" and tries to minimize error by shriking the\nscale by more than it should be shrunk. This occurs because it brings the points closer together, thereby minimizing, on average,\nthe error term. Let us look at an alternative formulation for our error term that accounts for this optimization phenomenon.\n1.3.3\nSolving for Scaling Using Least Squares: Symmetric Case\nLet us instead write our objective as:\nei = 1\n√sr′\nr,i = √sR(r′\nl,i)\nThen we can write our objective and optimization problem over scale as:\ns∗= arg min\ns\nn\nX\ni=1\n|| 1\n√sr′\nr,i -√sR(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\ns||r′\nr,i||2\n\n-2\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s\nn\nX\ni=1\n||R(r′\nl,i)||2\n= arg min\ns\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s\nn\nX\ni=1\n||r′\nl,i||2\n(Rotation preserves vector lengths)\nWe then take the same definitions for these terms that we did above:\n1. sr\n∆= Pn\ni=1\n\n||r′\nr,i||2\n\n2. D\n∆= Pn\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n3. sl\n∆= Pn\ni=1 ||r′\nl,i||2\nThen, as we did for the asymmetric OLS case, we can write this objective for the optimal scaling factor s∗as:\ns∗= arg min\ns {J(s)\n∆= 1\nssr -2D + ssl}\n\nSince this is an unconstrained optimization problem, we can solve this by taking the derivative w.r.t. s and setting it equal to 0:\ndJ(s)\nds\n= d\nds\nssr -2D + ssl\n\n= 0\n= -1\ns2 sr + sl = 0 =⇒s2 = sl\nsr\nTherefore, we can see that going in the reverse direction preserves this inverse (you can verify this mathematically and intu-\nitively by simply setting r′\nr,i ↔r′\nl,i ∀i ∈{1, ..., n} and noting that you will get s2\ninverse = sr\nsl ). Since this method better preserves\nsymmetry, it is preferred.\nIntuition: Since s no longer depends on correspondences (matches between points in the left and right point clouds), then\nthe scale simply becomes the ratio of the point cloud sizes in both coordinate systems (note that sl and sr correspond to the\nsummed vector lengths of the centroid-subtracted point clouds, which means they reflect the variance/spread/size of the point\ncloud in their respective coordinate systems.\nWe can deal with translation and rotation in a correspondence-free way, while also allowing for us to decouple rotation. Let us\nalso look at solving rotation, which is covered in the next section.\n1.4\nSolving for Optimal Rotation in Absolute Orientation\nRecall for rotation (see lecture 18 for details) that we switched from optimizing over orthonormal rotation matrices to opti-\nmizing over quaternions due to the lessened number of optimization constraints that we must adhere to. With our quaternion\noptimization formulation, our problem becomes:\nmax\noq\noq\nT N\noq, subject to\noq\nT oq = 1\nIf this were an unconstrained optimization problem, we could solve by taking the derivative of this objective w.r.t. our quaternion\noq and setting it equal to zero. Note the following helpful identities with matrix and vector calculus:\n1.\nd\nda(a · b) = b\n2.\nd\nda(aT Mb) = 2Mb\nHowever, since we are working with quaternions, we must take this constraint into account. We saw in lecture 18 that we did\nthis with using Lagrange Multiplier - in this lecture it is also possible to take this specific kind of vector length constraint\ninto account using Rayleigh Quotients.\nWhat are Rayliegh Quotients?\nThe intuitive idea behind them: How do I prevent my parameters from becoming too\nlarge) positive or negative) or too small (zero)? We can accomplish this by dividing our objective by our parameters, in this\ncase our constraint. In this case, with the Rayleigh Quotient taken into account, our objective becomes:\noq\nT N\noq\noq\nT oq\n\nRecall that N\n∆=\nn\nX\ni=1\nRT\nl,iRr,i\n\nHow do we solve this? Since this is now an unconstrained optimization problem, we can solve this simply using the rules of\ncalculus:\nJ(\noq)\n∆==\noq\nT N\noq\noq\nT oq\ndJ(\noq)\nd\noq\n= d\nd\noq\noq\nT N\noq\noq\nT oq\n= 0\n=\nd\nd\noq(\noq\nT N\noq)\noq\nT oq -\noq\nT N\noq d\nd\noq(\noq\nT oq)\n(\noq\nT oq)2\n= 0\n= 2N\noq\noq\nT oq\n-\noq\n(\noq\nT oq)2\n(\noq\nT N\noq) = 0\n\nFrom here, we can write this first order condition result as:\nN\noq =\noq\nT N\noq\noq\nT oq\noq\nNote that\noq\nT N\noq\noq\nT oq ∈R (this is our objective). Therefore, we are searching for a vector of quaternion coefficients such applying the\nrotation matrix to this vector simply produces a scalar multiple of it - i.e. an eigenvector of the matrix N. Letting λ\n∆=\noq\nT N\noq\noq\nT oq ,\nthen this simply becomes N\noq = λ\noq. Since this optimization problem is a maximization problem, this means that we can pick\nthe eigenvector of N that corresponds to the largest eigenvalue (which in turn maximizes the objective consisting of the\nRayleigh quotient\noq\nT N\noq\noq\nT oq , which is the eigenvalue.\nEven though this quaternion-based optimization approach requires taking this Rayleigh Quotient into account, it is much easier\nto do this optimization than to solve for orthonormal matrices, which either require a complex Lagrangian (if we solve with\nLagrange multipliers) or an SVD decomposition from Euclidean space to the SO(3) group (which also happens to be a manifold).\nThis approach raises a few questions:\n- How many correspondences are needed to solve these optimization problems? Recall a correspondence is when we say\ntwo 3D points in different coordinate systems belong to the same point in 3D space, i.e. the same point observed in two\nseparate frames of reference.\n- When do these approaches fail?\nWe cover these two questions in more detail below.\n1.4.1\nHow Many Correspondences Do We Need?\nRecall that we are looking for 6 parameters (for translation and rotation) or 7 parameters (for translation, rotation, and scaling).\nSince each correspondence provides three constraints (since we equate the 3-dimensional coordinates of two 3D points in space),\nassuming non-redundancy, then we can solve this with two correspondences.\nLet us start with two correspondences: if we have two objects corresponding to the correspondences of points in the 3D world,\nthen if we rotate one object about axis, we find this does not work, i.e. we have an additional degree of freedom. Note that the\ndistance between correspondences is fixed.\nFigure 1: Using two correspondences leads to only satisfying 5 of the 6 needed constraints to solve for translation and rotation\nbetween two point clouds.\nBecause we have one more degree of freedom, this accounts for only 5 of the 6 needed constraints to solve for translation and\nrotation, so we need to have at least 3 correspondences.\nWith 3 correspondences, we get 9 constraints, which leads to some redundancies.\nWe can add more constraints by incor-\nporating scaling and generalizing the allowable transformations between the two coordinate systems to be the generalized\nlinear transformation - this corresponds to allowing non-orthonormal rotation transformations. This approach gives us 9\nunknowns!\n\na11\na12\na13\na21\na22\na23\na31\na32\na33\n\nx\ny\nz\n\n+\n\na14\na24\na34\n\nBut we also have to account for translation, which gives us another 3 unknowns, giving us 12 in total and therefore requiring at\nleast 4 non-redundant correspondences in order to compute the full general linear transformation. Note that this doesn't have\nany constraints as well!\nOn a practical note, this is often not needed, especially for finding the absolute orientation between two cameras, because\noftentimes the only transformations that need to be considered due to the design constraints of the system (e.g. an autonomous\ncar with two lidar systems, one on each side) are translation and rotation.\n1.4.2\nWhen do These Approaches Fail?\nThese approaches can fail when we do not have enough correspondences. In this case, the matrix N will become singular, and\nwill produce eigenvalues of zero. A more interesting failure case occurs when the points of one or both of the point clouds are\ncoplanar. Recall that we solve for the eigenvalues of a matrix (N in this case) using the characteristic equation given by:\nCharacteristic Equation : det |N -λI| = 0\nLeads to 4th-order polynomial : λ4 + c3λ3 + c2λ2 + c1λ + c0 = 0\nRecall that our matrix N composed of the data has some special properties:\n1. c3 = tr(N) = 0 (This is actually a great feature, since usually the first step in solving 4th-order polynomial systems is\neliminating the third-order term).\n2. c2 = 2tr(M T M), where M is defined as the sum of dyadic products between the points in the point clouds:\nM\n∆=\n\nn\nX\ni=1\nr′\nl,ir′\nr,i\nT\n\n∈R3×3\n3. c1 = 8 det |M|\n4. c0 = det |N|\nWhat happens if det |M| = 0, i.e. the matrix M is singular? Then using the formulas above we must have that the coefficient\nc1 = 0. Then this problem reduces to:\nλ4 + c2λ2 + c0 = 0\nThis case corresponds to a special geometric case/configuration of the point clouds - specifically, when points are coplanar.\n1.4.3\nWhat Happens When Points are Coplanar?\nWhen points are coplanar, we have that the matrix N, composed of the sum of dyadic products between the correspondences in\nthe two point clouds, will be singular.\nTo describe this plane in space, we need only find a normal vector ˆn that is orthogonal to all points in the point cloud -\ni.e. the component of each point in the point cloud in the ˆn direction is 0. Therefore, we can describe the plane by the equation:\nr′\nr,i · ˆn = 0 ∀i ∈{1, ..., n}\nFigure 2: A coplanar point cloud can be described entirely by a surface normal of the plane ˆn.\nNote: In the absence of measurement noise, if one point cloud is coplanar, the the other point cloud must be as well (assuming\nthat the transformation between the point clouds is a linear transformation). This does not necessarily hold when measurement\n\nnoise is introduced.\nRecall that our matrix M, which we used above to compute the coefficients of the characteristic polynomial describing this\nsystem, is given by:\nM\n∆=\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT\nThen, rewriting our equation for the plane above, we have:\nr′\nr,i · ˆn = 0 =⇒M ˆn =\n\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT\n\nˆn\n=\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT ˆn\n=\nn\nX\ni=1\nr′\nr,i0\n= 0\nTherefore, when a point cloud is coplanar, the null space of M is non-trivial (it is given by at least Span({ˆn}), and therefore\nM is singular. Recall that a matrix M ∈Rn×d is singular if ∃x ∈Rd, x = 0 such that Mx = 0, i.e. the matrix has a non-trivial\nnull space.\n1.4.4\nWhat Happens When Both Coordinate Systems Are Coplanar\nVisually, when two point clouds are coplanar, we have:\nFigure 3: Two coplanar point clouds. This particular configuration allows us to estimate rotation in two simpler steps.\nIn this case, we can actually decompose finding the right rotation into two simpler steps!\n1. Rotate one plane so it lies on top of the other plane. We can read offthe axis and angle from the unit normal vectors of\nthese two planes describing the coplanarity of these point clouds, given respectively by ˆn1 and ˆn2:\n- Axis: We can find the axis by noting that the axis vector will be parallel to the cross product of ˆn1 and ˆn2, simply\nscaled to a unit vector:\nˆω =\nˆn1 × ˆn2\n||ˆn1 × ˆn||2\n- Angle: We can also solve for the angle using the two unit vectors ˆn1 and ˆn2:\ncos θ = ˆn1 · ˆn2\nsin θ = ˆn1 × ˆn2\nθ = arctan 2\nsin θ\ncos θ\n\nWe now have an axis angle representation for rotation between these two planes, and since the points describe each of the\nrespective point clouds, therefore, a rotation between the two point clouds! We can convert this axis-angle representation\ninto a quaternion with the formula we have seen before:\noq =\n\ncos θ\n2, sin θ\n2 ˆω\n\n2. Perform an in-plane rotation. Now that we have the quaternion representing the rotation between these two planes, we can\norient two planes on top of each other, and then just solve a 2D least-squares problem to solve for our in-place rotation.\nWith these steps, we have a rotation between the two point clouds!\n1.5\nRobustness\nIn many methods in this course, we have looked at the use of Least Squares methods to solve for estimates in the presence\nof noise and many data points. Least squares produces an unbiased, minimum-variance estimate if (along with a few other\nassumptions) the dataset/measurement noise is Gaussian (Gauss-Markov Theorem) [1]. But what if the measurement noise is\nnon-Gaussian? How do we deal with outliers in this case?\nIt turns out that Least Squares methods are not robust to outliers. One alternative approach is to use absolute error in-\nstead. Unfortunately, however, using absolute error does not have a closed-form solution. What are our other options for dealing\nwith outliers? One particularly useful alternative is RANSAC.\nRANSAC, or Random Sample Consensus, is an algorithm for robust estimation with least squares in the presence\nof outliers in the measurements. The goal is to find a least squares estimate that includes, within a certain threshold band, a\nset of inliers corresponding to the inliers of the dataset, and all other points outside of this threshold bands as outliers. The\nhigh-level steps of RANSAC are as follows:\n1. Random Sample: Sample the minimum number of points needed to fix the transformation (e.g. 3 for absolute orientation;\nsome recommend taking more).\n2. Fit random sample of points: Usually this involves running least squares on the sample selected. This fits a line (or\nhyperplane, in higher dimensions), to the randomly-sampled points.\n3. Check Fit: Evaluate the line fitted on the randomly-selected subsample on the rest of the data, and determine if the fit\nproduces an estimate that is consistent with the \"inliers\" of your dataset. If the fit is good enough accept it, and if it is\nnot, run another sample. Note that this step has different variations - rather than just immediately terminating once you\nhave a good fit, you can run this many times, and then take the best fit from that.\nFurthermore, for step 3, we threshold the band from the fitted line/hyperplane to determine which points of the dataset are\ninliers, and which are outliers (see figure below). This band is usually given by a 2ε band around the fitted line/hyperplane.\nTypically, this parameter is determined by knowing some intrinsic structure about the dataset.\nFigure 4: To evaluate the goodness of fit of our sampled points, as well as to determine inliers and outliers from our dataset, we\nhave a 2ε thick band centered around the fitted line.\nAnother interpretation of RANSAC: counting the \"maximimally-occupied\" cell in Hough transform parameter space! Another\nway to find the best fitting line that is robust to outliers:\n\n1. Repeatedly sample subsets from the dataset/set of measurements, and fit these subsets of points using least squares\nestimates.\n2. For each fit, map the points to a discretized Hough transform parameter space, and have an accumulator array that keeps\ntrack of how often a set of parameters falls into a discretized cell. Each time a set of parameters falls into a discretized\ncell, increment it by one.\n3. After N sets of random samples/least squares fits, pick the parameters corresponding to the cell that is \"maximally-\noccupied\", aka has been incremented the most number of times! Take this as your outlier-robust estimate.\nFigure 5: Another way to perform RANSAC using Hough Transforms: map each fit from the subsamples of measurements to a\ndiscretized Hough Transform (parameter) space, and look for the most common discretized cell in parameter space to use for an\noutlier-robust least-squares estimate.\n1.6\nSampling Space of Rotations\nNext, we will shift gears to discuss the sampling space of rotations.\nWhy are we interested in this space?\nMany orientation problems we have studied so far do not have a closed-form\nsolution and may require sampling. How do we sample from the space of rotations?\n1.6.1\nInitial Procedure: Sampling from a Sphere\nLet us start by sampling from a unit sphere (we will start in 3D, aiming eventually for 4D, but our framework will gener-\nalize easily from 3D to 4D). Why a sphere? Recall that we are interested in sampling for the coefficients of a unit quaternion\noq = (q0, qx, qy, qz), ||\noq||2\n2 = 1.\nOne way to sample from a sphere is with latitude and longitude, given by (θi, φi), respectively. The problem with this ap-\nproach, however, is that we sample points that are close together at the poles. Alternatively, we can generate random longitude\nθi and φi, where:\n- -π\n2 ≤θi ≤π\n2 ∀i\n- -π ≤φi ≤π ∀i\nBut this approach suffers from the same problem - it samples too strongly from the poles. Can we do better?\n1.6.2\nImproved Approach: Sampling from a Cube\nTo achieve more uniform sampling from a sphere, what if we sampled from a unit cube (where the origin is given the center of\nthe cube), and map the sampled points to an enscribed unit sphere within the cube?\nIdea: Map all points (both inside the sphere and outside the sphere/inside the cube) onto the sphere by connecting a line\nfrom the origin to the sampled point, and finding the point where this line intersects the sphere.\n\nFigure 6: Sampling from a sphere by sampling from a cube and projecting it back to the sphere.\nProblem with this approach: This approach disproportionately samples more highly on/in the direction of the cube's\nedges. We could use sampling weights to mitigate this effect, but better yet, we can simply discard any samples that fall outside\nthe sphere. To avoid numerical issues, it is also best to discard points very close to the sphere.\nGeneralization to 4D: As we mentioned above, our goal is to generalize this from 3D to 4D. Cubes and spheres simply\nbecome 4-dimensional - enabling us to sample quaternions.\n1.6.3\nSampling From Spheres Using Regular and Semi-Regular Polyhedra\nWe saw the approach above requires discarding samples, which is computationally-undesirable because it means we will proba-\nbilistically have to generate more samples than if we were able to sample from the sphere alone. To make this more efficient, let\nus consider shapes that form a \"tighter fit\" around the sphere - for instance: polyhedra! Some polyhedra we can use:\n- Tetrahedra (4 faces)\n- Hexahedra (6 faces)\n- Octahedra (8 faces)\n- Dodecahedra (12 faces)\n- Icosahedra (20 faces)\nThese polyhedra are also known as the regular solids.\nAs we did for the cube, we can do the same for polyhedra: to sample from the sphere, we can sample from the polyhedra,\nand then project onto the point on the sphere that intersects the line from the origin to the sampled point on the polyhedra.\nFrom this, we get great circles from the edges of these polyhedra on the sphere when we project.\nFun fact: Soccer balls have 32 faces! More related to geometry: soccer balls are part of a group of semi-regular solids,\nspecifically an icosadodecahedron.\n1.6.4\nSampling in 4D: Rotation Quaternions and Products of Quaternions\nNow we are ready to apply these shapes for sampling quaternions in 4D. Recall that our goal with this sampling task is to\nfind the rotation between two point clouds, e.g. two objects. We need a uniform way of sampling this spae. We can start\nwith the hexahedron. Below are 10 elementary rotations we use (recall that a quaternion is given in axis-angle notation by\noq = (cos\n\nθ\n\n, sin\nπ\n\nˆω)):\n1. Identity rotation:\noq = (1, 0)\n2. π about ˆx:\noq = (cos\nπ\n\n, sin\nπ\nˆx) = (0, ˆx)\n3. π about ˆy:\noq = (cos\nπ\n\n, sin\nπ\nˆy) = (0, ˆy)\n4. π about ˆz:\noq = (cos\nπ\n\n, sin\nπ\nˆz) = (0, ˆz)\n5.\nπ\n2 about ˆx:\noq = (cos\nπ\n\n, sin\nπ\nˆx) =\n√\n2(1, ˆx)\n6.\nπ\n2 about ˆy:\noq = (cos\nπ\n\n, sin\nπ\nˆy) =\n√\n2(1, ˆy)\n\n7.\nπ\n2 about ˆz:\noq = (cos\nπ\n\n, sin\nπ\nˆz) =\n√\n2(1, ˆz)\n8. -π\n2 about ˆx:\noq = (cos\n-π\n\n, sin\n-π\nˆx) =\n√\n2(1, -ˆx)\n9. -π\n2 about ˆy:\noq = (cos\n-π\n\n, sin\n-π\nˆy) =\n√\n2(1, -ˆy)\n10. -π\n2 about ˆz:\noq = (cos\n-π\n\n, sin\n-π\nˆz) =\n√\n2(1, -ˆz)\nThese 10 rotations by themselves give us 10 ways to sample the rotation space. How can we construct more samples? We can\ndo so by taking quaternion products, specifically, products of these 10 quaternions above. Let us look at just a couple of\nthese products:\n1. (0, ˆx)(0, ˆy):\n(0, ˆx)(0, ˆy) = (0 -ˆx · ˆy, 0ˆx + 0ˆy + ˆx × ˆy)\n= (-ˆx · ˆy, ˆx × ˆy)\n= (0, ˆz)\nWe see that this simply produces the third axis, as we would expect. This does not give us a new rotation to sample from.\nNext, let us look at one that does.\n2.\n√\n2(1, ˆx) 1\n√\n2(1, ˆy):\n√\n2(1, ˆx) 1\n√\n2(1, ˆy) = 1\n2(1 -ˆx · ˆy, ˆy + ˆx + ˆx × ˆy)\n= 1\n2(1, ˆx + ˆy + ˆx × ˆy)\nThis yields the following axis-angle representation:\n- Axis:\n√\n3(1 1 1)\n- Angle: cos\n\nθ\n\n= 1\n2 =⇒\nθ\n2 = π\n3 =⇒θ = 2π\nTherefore, we have produced a new rotation that we can sample from!\nThese are just a few of the pairwise quaternion products we can compute. It turns out that these pairwise quaternion products\nproduce a total of 24 new rotations from the original 10 rotations. These are helpful for achieving greater sampling granularity\nwhen sampling the rotation space.\n1.7\nReferences\n1. Gauss-Markov Theorem, https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov theorem\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801 / 6.868 Machine Version, Lecture Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/9ec44f69e72995c370b3797818ba01e6_MIT6_801F20_lectureNotes.pdf",
      "content": "6.801/6.866: Machine Vision Fall 2020\nLecture Notes\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nDepartment of Electrical Engineering and Computer Science\nMassachusetts Institute of Technology\n\nContents\nLecture 2: Image Formatio, Perspective Projection, Time Derivative, Motion Field\n1.1\nMotion from Perspective Projection, and FOE\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2\nBrightness and Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1\n1D Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.2\n2D Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 3: Time to Contact, Focus of Expansion, Direct Motion Vision Methods, Noise Gain\n2.1\nNoise Gain\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2\nForward and Inverse Problems of Machine Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.1\nScalar Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.2\nVector Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3\nReview from Lecture 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.1\nTwo-Pixel Motion Estimation, Vector Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.2\nConstant Brightness Assumption, and Motion Equation Derivation . . . . . . . . . . . . . . . . . . . . . .\n2.3.3\nOptical Mouse Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.4\nPerspective Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4\nTime to Contact (TTC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 4: Fixed Optical Flow, Optical Mouse, Constant Brightness Assumption, Closed Form Solution\n3.1\nReview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1.1\nConstant Brightness Assumption Review with Generalized Isophotes . . . . . . . . . . . . . . . . . . . . .\n3.1.2\nTime to Contact (TTC) Review\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2\nIncreasing Generality of TTC Problems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3\nMultiscale and TTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3.1\nAliasing and Nyquist's Sampling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3.2\nApplications of TTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4\nOptical Flow\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.5\nVanishing Points (Perspective Projection) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.5.1\nApplications of Vanishing Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.6\nCalibration Objects\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.6.1\nSpheres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.6.2\nCube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.7\nAdditional Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.7.1\nGeneralization: Fixed Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.7.2\nGeneralization: Time to Contact (for U = 0, V = 0, ω = 0) . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 5: TTC and FOR Montivision Demos, Vanishing Point, Use of VPs in Camera Calibration\n4.1\nRobust Estimation and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.1\nLine Intersection Least-Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.2\nDealing with Outliers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.3\nReprojection and Rectification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.4\nResampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2\nMagnification with TTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.1\nPerspective Projection and Vanishing Points\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.2\nLines in 2D and 3D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.3\nApplication: Multilateration (MLAT)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.4\nApplication: Understand Orientation of Camera w.r.t. World . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3\nBrightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.1\nWhat if We Can't use Multiple Orientations? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.4\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 6: Photometric Stereo, Noise Gain, Error Amplification, Eigenvalues and Eigenvectors Review\n5.1\nApplications of Linear Algebra to Motion Estimation/Noise Gain . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1.1\nApplication Revisited: Photometric Stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2\nLambertian Objects and Brightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2.1\nSurface Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2.2\nSurface Orientation Isophotes/Reflectance Maps for Lambertian Surfaces\n. . . . . . . . . . . . . . . . . .\n5.3\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nLecture 7: Gradient Space, Reflectance Map, Image Irradiance Equation, Gnomonic Projection\n6.1\nSurface Orientation Estimation (Cont.) & Reflectance Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.1.1\nForward and Inverse Problems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.1.2\nReflectance Map Example: Determining the Surface Normals of a Sphere\n. . . . . . . . . . . . . . . . . .\n6.1.3\nComputational Photometric Stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2\nPhotometry & Radiometry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3\nLenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3.1\nThin Lenses - Introduction and Rules\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3.2\nPutting it All Together: Image Irradiance from Object Irradiance . . . . . . . . . . . . . . . . . . . . . . .\n6.3.3\nBRDF: Bidirectional Reflectance Distribution Function\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3.4\nHelmholtz Reciprocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.4\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 8: Shape from Shading, Special Cases, Lunar Surface, Scanning Electron Microscope, Green's\nTheorem in Photometric Stereo\n7.1\nReview of Photometric and Radiometric Concepts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2\nIdeal Lambertian Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2.1\nForeshortening Effects in Lambertian Surfaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2.2\nExample: Distant Lambertian Point Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3\nHapke Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3.1\nExample Application: Imaging the Lunar Surface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3.2\nSurface Orientation and Reflectance Maps of Hapke Surfaces\n. . . . . . . . . . . . . . . . . . . . . . . . .\n7.3.3\nRotated (p′, q′) Coordinate Systems for Brightness Measurements . . . . . . . . . . . . . . . . . . . . . . .\n7.4\n\"Thick\" & Telecentric Lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.4.1\n\"Thick\" Lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.4.2\nTelecentric Lenses\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.5\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 9: Shape from Shading, General Case - From First Order Nonlinear PDE to Five ODEs\n8.1\nExample Applications: Transmission and Scanning Electron Microscopes (TEMs and SEMs, respectively) . . . .\n8.2\nShape from Shading: Needle Diagram to Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.2.1\nDerivation with Taylor Series Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.2.2\nDerivation with Green's Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3\nShape with Discrete Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3.1\n\"Computational Molecules\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3.2\nIterative Optimization Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3.3\nReconstructing a Surface From a Single Image\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.4\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nLecture 10: Characteristic Strip Expansion, Shape from Shading, Iterative Solutions\n9.1\nReview: Where We Are and Shape From Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.2\nGeneral Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.2.1\nReducing General Form SfS for Hapke Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.2.2\nApplying General Form SfS to SEMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.3\nBase Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.4\nAnalyzing \"Speed\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.5\nGenerating an Initial Curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.5.1\nUsing Edge Points to Autogenerate an Initial Curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.5.2\nUsing Stationary Points to Autogenerate an Initial Curve . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.5.3\nExample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.6\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10 Lecture 11: Edge Detection, Subpixel Position, CORDIC, Line Detection, (US 6,408,109)\n10.1 Background on Patents\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.2 Patent Case Study: Detecting Sub-Pixel Location of Edges in a Digital Image . . . . . . . . . . . . . . . . . . . .\n10.2.1 High-Level Overview of Edge Detection System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3 Edges & Edge Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3.1 Finding a Suitable Brightness Function for Edge Detection\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3.2 Brightness Gradient Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n10.4 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11 Lecture 12: Blob analysis, Binary Image Processing, Use of Green's Theorem, Derivative and Integral as\nConvolutions\n11.1 Types of Intellectual Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2 Edge Detection Patent Methodologies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.1 Finding Edge with Derivatives\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.2 More on \"Stencils\"/Computational Molecules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.3 Mixed Partial Derivatives in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.4 Laplacian Estimators in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.5 Non-Maximum Suppression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.6 Plane Position\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.7 Bias Compensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.8 Edge Transition and Defocusing Compensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.9 Multiscale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.10Effect on Image Edge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.11Addressing Quantization of Gradient Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.2.12CORDIC\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11.3 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12 Lecture 13: Object detection, Recognition and Pose Determination, PatQuick (US 7,016,539)\n12.1 Motivation & Preliminaries for Object Detection/Pose Estimation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n12.1.1 \"Blob Analysis\"/\"Binary Image Processing\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.1.2 Binary Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.1.3 Normalized Correlation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.2 Patent 7,016,539: Method for Fast, Robust, Multidimensional Pattern Recognition . . . . . . . . . . . . . . . . .\n12.2.1 Patent Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.2.2 High-level Steps of Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.2.3 Framework as Programming Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.2.4 Other Considerations for this Framework\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12.3 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13 Lecture 14: Inspection in PatQuick, Hough Transform, Homography, Position Determination, Multi-Scale 77\n13.1 Review of \"PatQuick\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.1.1 Scoring Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.1.2 Additional System Considerations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.1.3 Another Application of \"PatQuick\": Machine Inspection . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.2 Intro to Homography and Relative Poses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.2.1 How many degrees of freedom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.3 Hough Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.3.1 Hough Transforms with Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.3.2 Hough Transforms with Circles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13.3.3 Hough Transforms with Searching for Center Position and Radius\n. . . . . . . . . . . . . . . . . . . . . .\n13.4 Sampling/Subsampling/Multiscale\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14 Lecture 15: Alignment, recognition in PatMAx, distance field, filtering and sub-sampling (US 7,065,262) 83\n14.1 PatMAx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.1 Overview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.2 Training PatMAx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.3 Estimating Other Pixels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.4 Attraction Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.5 PatMAx Claims\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.6 Comparing PatMAx to PatQuick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.1.7 Field Generation for PatMAx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.2 Finding Distance to Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3 Fast Convolutions Through Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3.1 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3.2 Integration and Differentiation as Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3.3 Sparse Convolution as Compression\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n14.3.4 Effects on Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3.5 Filtering (For Multiscale): Anti-Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14.3.6 Extending Filtering to 2D and An Open Research Problem . . . . . . . . . . . . . . . . . . . . . . . . . .\n15 Lecture 16: Fast Convolution, Low Pass Filter Approximations, Integral Images, (US 6,457,032)\n15.1 Sampling and Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.1.1 Nyquist Sampling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.1.2 Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.1.3 How Can We Mitigate Aliasing?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.2 Integral Image\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.2.1 Integral Images in 1D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.2.2 Integral Images in 2D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.3 Fourier Analysis of Block Averaging\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.4 Repeated Block Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.4.1 Warping Effects and Numerical Fourier Transforms: FFT and DFT\n. . . . . . . . . . . . . . . . . . . . .\n15.5 Impulses and Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.1 Properties of Delta Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.2 Combinations of Impulses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.3 Convolution Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.4 Analog Filtering with Birefringent Lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.5 Derivatives and Integrals as Convolution Operators and FT Pairs . . . . . . . . . . . . . . . . . . . . . . .\n15.5.6 Interpolation and Convolution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.5.7 Rotationally-Symmetric Lowpass Filter in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16 Lecture 17: Photogrammetry, Orientation, Axes of Inertia, Symmetry, Absolute, Relative, Interior, and\nExterior Orientation\n16.1 Photogrammetry Problems: An Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.1.1 Absolute Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.1.2 Relative Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.1.3 Exterior Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.1.4 Interior Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2 Absolute Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.1 Binocular Stereopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.2 General Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.3 Transformations and Poses\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.4 Procedure - \"Method 1\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.5 Procedure - \"Method 2\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.2.6 Computing Rotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16.3 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17 Lecture 18: Rotation and How to Represent it, Unit Quaternions, the Space of Rotations\n17.1 Euclidean Motion and Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.2 Basic Properties of Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.2.1 Isomorphism Vectors and Skew-Symmetric Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3 Representations for Rotation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.1 Axis and Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.2 Euler Angles\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.3 Orthonormal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.4 Exponential Cross Product\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.5 Stereography Plus Bilinear Complex Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.6 Pauli Spin Matrices\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.3.7 Euler Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.4 Desirable Properties of Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.5 Problems with Some Rotation Representations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.6 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.6.1 Hamilton and Division Algebras\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.6.2 Hamilton's Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.6.3 Representations of Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n17.6.4 Representations for Quaternion Multiplication\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.6.5 Properties of 4-Vector Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.7 Quaternion Rotation Operator\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.7.1 Relation of Quaternion Rotation Operation to Rodrigues Formula\n. . . . . . . . . . . . . . . . . . . . . .\n17.8 Applying Quaternion Rotation Operator to Photogrammetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.8.1 Least Squares Approach to Find R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.8.2 Quaternion-based Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.9 Desirable Properties of Quaternions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.9.1 Computational Issues for Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.9.2 Space of Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.10References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18 Lecture 19: Absolute Orientation in Closed Form, Outliers and Robustness, RANSAC\n18.1 Review: Absolute Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.1.1 Rotation Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.1.2 Quaternion Representations: Axis-Angle Representation and Orthonormal Rotation Matrices . . . . . . .\n18.2 Quaternion Transformations/Conversions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.3 Transformations: Incorporating Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.3.1 Solving for Scaling Using Least Squares: Asymmetric Case\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n18.3.2 Issues with Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.3.3 Solving for Scaling Using Least Squares: Symmetric Case . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.4 Solving for Optimal Rotation in Absolute Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.4.1 How Many Correspondences Do We Need?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.4.2 When do These Approaches Fail? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.4.3 What Happens When Points are Coplanar? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.4.4 What Happens When Both Coordinate Systems Are Coplanar\n. . . . . . . . . . . . . . . . . . . . . . . .\n18.5 Robustness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.6 Sampling Space of Rotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.6.1 Initial Procedure: Sampling from a Sphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.6.2 Improved Approach: Sampling from a Cube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18.6.3 Sampling From Spheres Using Regular and Semi-Regular Polyhedra\n. . . . . . . . . . . . . . . . . . . . .\n18.6.4 Sampling in 4D: Rotation Quaternions and Products of Quaternions . . . . . . . . . . . . . . . . . . . . .\n18.7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19 Lecture 20: Space of Rotations, Regular Tessellations, Critical Surfaces in Motion Vision and Binocular\nStereo\n19.1 Tessellations of Regular Solids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.2 Critical Surfaces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3 Relative Orientation and Binocular Stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.1 Binocular Stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.2 How Many Correspondences Do We Need?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.3 Determining Baseline and Rotation From Correspondences\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.4 Solving Using Weighted Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.5 Symmetries of Relative Orientation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.6 When Does This Fail? Critical Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19.3.7 (Optional) Levenberg-Marquadt and Nonlinear Optimization . . . . . . . . . . . . . . . . . . . . . . . . .\n19.4 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20 Lecture 21: Relative Orientation, Binocular Stereo, Structure from Motion, Quadrics, Camera Calibra-\ntion, Reprojection\n20.1 Interior Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.1.1 Radial Distortion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.1.2 Tangential Distortion and Other Distortion Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2 Tsai's Calibration Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.1 Interior Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.2 Exterior Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.3 Combining Interior and Exterior Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.4 \"Squaring Up\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.5 Planar Target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n20.2.6 Aspect Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.7 Solving for tz and f\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20.2.8 Wrapping it Up: Solving for Principal Point and Radial Distortion . . . . . . . . . . . . . . . . . . . . . .\n20.2.9 Noise Sensitivity/Noise Gain of Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21 Lecture 22:\nExterior Orientation, Recovering Position and Orientation, Bundle Adjustment, Object\nShape\n21.1 Exterior Orientation: Recovering Position and Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.1.1 Calculating Angles and Lengths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.1.2 Finding Attitude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.2 Bundle Adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3 Recognition in 3D: Extended Gaussian 3D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.1 What Kind of Representation Are We Looking For?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.2 2D Extended Circular Image\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.3 Analyzing Gaussian Curvature in 2D Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.4 Example: Circle of Radius R\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.5 Example: Ellipse in 2D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21.3.6 3D Extended Gaussian Images\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22 Lecture 23: Gaussian Image and Extended Gaussian Image, Solids of Revolution, Direction Histograms,\nRegular Polyhedra\n22.1 Gaussian Curvature and Gaussian Images in 3D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.1.1 Gaussian Integral Curvature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.1.2 How Do We Use Integral Gaussian Curvature?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.1.3 Can We Have Any Distribution of G on the Sphere? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.2 Examples of EGI in 3D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.2.1 Sphere: EGI\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.2.2 Ellipsoid: EGI\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.3 EGI with Solids of Revolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.4 Gaussian Curvature\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.4.1 Example: Sphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.4.2 EGI Example Torus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.4.3 Analyzing the Density Distribution of the Sphere (For Torus) . . . . . . . . . . . . . . . . . . . . . . . . .\n22.5 How Can We Compute EGI Numerically? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.5.1 Direction Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22.5.2 Desired Properties of Dividing Up the Sphere/Tessellations . . . . . . . . . . . . . . . . . . . . . . . . . .\n23 Quiz 1 Review\n23.1 Quick Mathematics Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.2 Projection: Perspective and Orthographic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.3 Optical Flow\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.4 Photometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.5 Different Surface Types\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.6 Shape from Shading (SfS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.7 Photometric Stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.8 Computational Molecules\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.9 Lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23.10Patent Review\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24 Quiz 2 Review\n24.1 Relevant Mathematics Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.1.1 Rayleigh Quotients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.1.2 (Optional) Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.1.3 (Optional) Levenberg-Marquadt and Gauss-Newton Nonlinear Optimization . . . . . . . . . . . . . . . . .\n24.1.4 Bezout's Theorem\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.2 Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.2.1 PatQuick\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.2.2 PatMAx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.2.3 Fast Convolutions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n24.2.4 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.2.5 Hough Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.3 Photogrammetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.3.1 Absolute Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.3.2 Relative Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.3.3 Exterior Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.3.4 Interior Orientation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4 Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.1 Axis and Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.2 Orthonormal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.3 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.4 Hamilton and Division Algebras\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.5 Properties of 4-Vector Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.4.6 Quaternion Rotation Operator\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.5 3D Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.5.1 Extended Gaussian Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.5.2 EGI with Solids of Revolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.5.3 Sampling From Spheres Using Regular and Semi-Regular Polyhedra\n. . . . . . . . . . . . . . . . . . . . .\n24.5.4 Desired Properties of Dividing Up the Sphere/Tessellations . . . . . . . . . . . . . . . . . . . . . . . . . .\n24.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n6.801/6.866: Machine Vision, Lecture Notes\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nLecture 2: Image Formatio, Perspective Projection, Time Derivative, Mo-\ntion Field\n1.1\nMotion from Perspective Projection, and FOE\nDefinition of perspective projection:\nx\nf = X\nZ , y\nf = Y\nZ (component form)\nf r =\nR · ˆzR (vector form)\nIf we differentiate these perspective projection equations:\nf\ndx\ndt = 1\nZ\ndX\ndt -X\nZ2\ndZ\ndt\nWhat are these derivatives? They correspond to velocities. Let's define some of these velocities:\n- u\n∆= dx\ndt\n- v\n∆= dy\ndt\n- U\n∆= dX\ndt\n- V\n∆= dY\ndt\n- W\n∆= dZ\ndt\nNow, rewriting the differentiated perspective projection equations with these velocity terms, we first write the equation for the\nx component:\nf u = 1\nZ U -X\nZ2 W\nSimilarly, for y:\nf v = 1\nZ V -Y\nZ2 W\nWhy are these equations relevant? They allow us to find parts of the image that don't exhibit any motion - i.e. stationary\npoints. Let's find where U = V = 0. Let the point (x0, y0) correspond to this point. Then:\nx0\nf = U\nW , y0\nf = V\nW\n\nFocus of Expansion (FOE): Point in image space given by (x0, y0). This point is where the 3D motion vector intersects with\nthe line given by z = f.\nWhy is FOE useful?\nIf you know FOE, you can derive the direction of motion by drawing a vector from the origin to\nFOE.\nAdditionally, we can rewrite the differentiated perspective projection equations with FOE:\nf u = x0 -x\nf\nW\nZ (x comp.),\nf v = y0 -y\nf\nW\nZ (y comp.)\nCancelling out the focal length (f) terms:\nu = (x0 -x)W\nZ (x comp.), v = (y0 -y)W\nZ (y comp.)\nA few points here:\n- You can draw the vector diagram of the motion field in the image plane.\n- All vectors in the motion field expand outward from FOE.\n- Recall that perspective projection cannot give us absolute distances.\nFor building intuition, let's additionally consider what each of these quantities mean. The inverse term\nZ\nW =\nZ\ndZ\ndt has units of\nmeters\nmeters\nsecond = seconds - i.e. Time of Impact.\nLet's now revisit these equations in vector form, rather than in the component form derived above:\nf\ndr\ndt =\nR · ˆz -\nR\n(R · ˆr)2\nd\ndt(R · ˆr)\nLet's rewrite this with dots for derivatives. Fun fact: The above notation is Leibniz notation, and the following is Newtonian\nnotation:\nf r =\nR · ˆz\nR -\nR\n(R · ˆz)2 ( R · ˆz)\nf r = 1\nZ ( R -W 1\nf r)\nOne way for reasoning about these equations is that motion is magnified by the ratio of the distance terms.\nNext, we'll reintroduce the idea of Focus of Expansion, but this time, for the vector form.\nFOE in the vector form is\ngiven at the point where r = 0:\nf r = 1\nW\nR\nWe can use a dot product/cross product identity to rewrite the above expression in terms of cross products. The identity is as\nfollows for any a, b, c ∈Rn\na × (b × c) = (c · a)b -(a · b)c\nUsing this identity, we rewrite the expression above to solve for FOE:\nf r =\n(R · ˆz)2 (ˆz × ( R × R))\nWhat is this expression? This is image motion expressed in terms of world motion. Note the following identities/properties\nof this motion, which are helpful for building intuition:\n- r·ˆz = 0 =⇒Image motion is perpendicular to the z-axis. This makes since intuitively because otherwise the image would\nbe coming out of/going into the image plane.\n- r ⊥ˆz\n- R ∥R =⇒ r = 0 (this condition results in there being no image motion).\n\n1.2\nBrightness and Motion\nLet's now consider how brightness and motion are intertwined. Note that for this section, we will frequently be switching between\ncontinuous and discrete. The following substitutions/conversions are made:\n- Representations of brightness functions: E(x, y) ↔E[x, y]\n- Integrals and Sums:\nR\nx\nR\ny ↔P\nx\nP\ny\n- Brightness Gradients and Finite Differences: ( ∂E\n∂x , ∂E\n∂y ) ↔( 1\nδx(E[k, e + 1] -E[k, e])\n1.2.1\n1D Case\ndx\ndt = U =⇒δx = Uδ\nBy taking a linear approximation of the local brightness:\nδE = Exδx = uExδt\n(note here that Ex = ∂E\n∂x )\nDividing each side by δt, we have:\nuEx + Et = 0 =⇒U = -Ex\nEt\n= -\n∂E\n∂t\n∂E\n∂x\nA couple of points about this:\n- This 1D result allows us to recover motion from brightness.\n- We can infer motion from a single point. However, this is only true in the 1D case.\n- We can estimate from 1 pixel, but frequently, we have much more than 1 pixel, so why use just 1? We can reduce noise by\nestimating motion from many pixels through regression techniques such as Ordinary Least Squares (OLS).\n- From statistics, the standard deviation of the motion estimates will be reduced by a factor of\n√\nN , where N is the number\nof pixels sampled for estimating motion.\nFinite Difference approximation for E is given by:\nE ≈1\nδx(E(x + δx, t) -E(x, t))\nMotion estimation can be done through unweighted averaging:\nuunweighted = 1\nN\nN\nX\ni=1\n-Eti\nExi\nAs well as weighted averaging:\nuweighted =\nPN\ni=1 wi\n-Eti\nExi\nPN\ni=1 wi\nA quick check here: take wi = 1 ∀i ∈{1, ..., N}. Then we have that uweighted = 1\nN\nPN\ni=1\n-Et\nEx = uunweighted.\nNote that in the continuous domain, the sums in the weighted and unweighted average values are simply replaced with in-\ntegrals.\n\n1.2.2\n2D Case\nWhile these results are great, we must remember that images are in 2D, and not 1D. Let's look at the 2D case. First and\nforemost, let's look at the brightness function, since it now depends on x, y, and t: E(x, y, t). The relevant partial derivatives\nhere are thus:\n-\n∂E\n∂x - i.e. how the brightness changes in the x direction.\n-\n∂E\n∂y - i.e. how the brightness changes in the y direction.\n-\n∂E\n∂t - i.e. how the brightness changes w.r.t. time.\nAs in the previous 1D case, we can approximate these derivatives with finite forward first differences:\n-\n∂E\n∂x = Ex ≈\nδx(E(x + δx, y, t) -E(x, y, t))\n-\n∂E\n∂y = Ey ≈\nδy(E(x, y + δy, t) -E(x, y, t))\n-\n∂E\n∂t = Et ≈1\nδt(E(x, y, t + δt) -E(x, y, t))\nFurthermore, let's suppose that x and y are parameterized by time, i.e. x = x(t), y = y(t). Then we can compute the First-Order\nCondition (FOC) given by:\ndE(x, y, t)\ndt\n= 0\nHere, we can invoke the chain rule, and we obtain the result given by:\ndE(x, y, t)\ndt\n= dx\ndt\n∂E\n∂x + dy\ndt\n∂E\n∂y + ∂E\n∂t = 0\nRewriting this in terms of u, v from above:\nuEx + vEy + Et = 0\nObjective here: We have a time-varying sequence of images, and our goal is to find and recover motion.\nTo build intuition, it is also common to plot in velocity space given by (u, v).\nFor instance, a linear equation in the 2D\nworld corresponds to a line in velocity space. Rewriting the equation above as a dot product:\nuEx + vEy + Et = 0 ↔(u, v) · (Ex, Ey) = -Et\nNormalizing the equation on the right by the magnitude of the brightness derivative vectors, we obtain the brightness gradient:\n(u, v) ·\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\n= -\nEt\nq\nE2x + E2y\nWhat is the brightness gradient?\n- A unit vector given by:\n\nEx\n√\nE2\nx+E2\ny ,\nEy\n√\nE2\nx+E2\ny\n!\n∈R2.\n- Measures spatial changes in brightness in the image in the image plane x and y directions.\nIsophotes: A curve on an illuminated surface that connects points of equal brightness (source: Wikipedia).\nAs we saw in the previous case with 1D, we don't want to just estimate with just one pixel.\nFor multiple pixels, we will\nsolve a system of N equations and two unknowns:\nuEx1 + vEy1 + Et1 = 0\nuEx2 + vEy2 + Et2 = 0\nRewriting this in matrix form:\n\nEx1\nEy1\nEx2\nEy2\n\nU\nV\n\n=\n\n-Et1\n-Et2\n\nSolving this as a standard Ax = b problem, we have:\nU\nV\n\n=\n(Ex1Ey2 -Ey1Ex2)\nEy2\n-Ey1\n-Ex2\nEx1\n-Et1\n-Et2\n\nNote that the expression given by\n(Ex1Ey2-Ey1Ex2) is the determinant of the partial derivatives matrix, since we are taking its\ninverse (in this case, simply a 2x2 matrix).\nWhen can/does this fail? It's important to be cognizant of edge cases in which this motion estimation procedure/algo-\nrithm fails. Some cases to consider:\n- When brightness partial derivatives / brightness gradients are parallel to one another ↔The determinant goes to zero ↔\nThis corresponds to linear dependence in the observations. This occurs when Ex1Ey2 = Ey1Ex2 =⇒\nEy1\nEx1 = Ey2\nEx2 .\nThis issue can be mitigated by weighting the pixels as we saw in the 1D case above. However, a more robust solution is to search\nfor a minima of motion, rather than the point where it has zero motion. The intuition here is that even if we aren't able to find\na point of zero motion, we can still get as close to zero as possible. Mathematically, let us define the following objective:\nJ(u, v)\n∆=\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nThen we now seek to solve the problem of:\nu∗, v∗= arg min\nu,v J(u, v) = arg min\nu,v\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nSince this is an unconstrained optimization problem, we can solve by finding the minimum of the two variables using two\nFirst-Order Conditions (FOCs):\n-\n∂J(u,v)\n∂u\n= 0\n-\n∂J(u,v)\n∂v\n= 0\nHere, we have two equations and two unknowns. When can this fail?\n- When we have linear independence. This occurs when:\n- E = 0 everywhere\n- E = constant\n- Ex = 0\n- Ey = 0\n- Ex = Ey\n- Ex = kEy\n- When E = 0 everywhere (professor's intuition: \"You're in a mine.\")\n- When Ex, Ey = 0 (constant brightness).\n- Mathematically, this fails when:\nR\nx\nR\nx E2\nx\nR\ny\nR\ny E2\ny -(\nR\nx\nR\ny ExEy)2 = 0\nWhen is this approach possible? Only when isophotes are not parallel straight lines - i.e.\nwant isophote curva-\nture/rapid turning of brightness gradients.\nNoise Gain: Intuition - if I change a value by this much in the image, how much does this change in the result?\nLecture 3:\nTime to Contact, Focus of Expansion, Direct Motion Vision\nMethods, Noise Gain\n2.1\nNoise Gain\nExample/motivation: Indoor GPS. Rather than using localization of position with satellites, use indoor cellular signals.\nFun fact: EM waves travel at 1 ns/foot.\nDilution of Precision:\n\n- How far offGPS is w.r.t. your location.\n- Important to note that your dilution of precision can vary in different directions - e.g. horizontal precision is oftentimes\ngreater than vertical position.\n2.2\nForward and Inverse Problems of Machine Vision\n2.2.1\nScalar Case\nOne way to conceptualize the goals of machine vision, as well as to highlight why noise gain is important, is by considering the\nfollowing problems with some one-dimensional input x and an output y = f(x):\n- The forward problem: x →y\n∆= f(x)\n- The inverse problem∗: y\n∆= f(x) →x\n*(this term comes up a lot in machine vision, computer graphics, and robotics)\nIn machine vision, we oftentimes focus on solving the inverse problem, rather than the forward problem. Intuitively, we\nusually observe some y = f(x), and from this infer the latent parameters x using our model f.\nWhen it is possible to express the inverse of a function in closed form or via a matrix/coefficient, we can simply solve the\ninverse problem using: x = f -1(y).\nMore importantly, to build a robust machine vision system to solve this inverse problem, it is critical that small perturbations in\ny = f(x) do not need to large changes in x. Small perturbations need to be taken into account in machine vision problems be-\ncause the sensors we use exhibit measurement noise. The concept of noise gain can come in to help deal with this uncertainty.\nConsider a perturbation δy that leads to a perturbation δx when we solve the inverse problem. In the limit, as δ ∈R →0, then\nwe arrive at the definition of noise gain:\nnoise gain = δx\nδy =\nf ′(x) =\ndf(x)\ndx\n(1)\nLike other concepts/techniques we've studied so far, let's understand when this system fails. Below are two cases; we encourage\nto consider why they fail from both a mathematical and intuitive perspective (hint: for the mathematical component, look at\nthe formula above, and for the intuitive component, think about how the effect on x from a small change in y in a curve that is\nnearly flat):\n- f ′(x) = 0 (flat curve)\n- f ′(x) ≈0 (nearly flat curve)\n2.2.2\nVector Case\nNow that we've analyzed this problem in the scalar case, let us now consider it in the vector/multi-dimensional case. Since\nimages are inherently multidimensional, this more general vector case is where we will find ourselves.\nFirst, we can restate these problems:\n- Forward Problem: x = Mb, M ∈Rm×n for m, n ∈N\n- Inverse Problem: b = M-1x, M ∈Rm×n for m, n ∈N\nBut how good is this answer/approach? If x changes, how much does b change? We quantify noise gain in this case as follows:\n\"noise gain\" →||δb||\n||δx||, δ ∈R\n(2)\n*NOTE: This multidimensional problem is more nuanced because we may be in the presence of an anisotropic (spatially\nnon-uniform) noise gain - e.g. there could be little noise gain in the x1 direction, but a lot of noise gain in the x2 direction.\n\nAs in the previous case, let's analyze when this approach fails.\nTo do this, let's consider M -1 to help build intuition for\nwhy this fails:\nM -1 =\ndet|M|\n\n·\n·\n·\n·\n·\n·\n·\n·\n·\n\n(3)\nLet's ignore the specific entries of M -1 for now, and focus on the fact that we need to compute the determinant of M. When is\nthis determinant zero? This will be the case whenever there exists linear dependence in the columns of M. As we saw before,\ntwo cases that can yield to poor performance will be:\n- det|M| = 0: This corresponds to a non-invertible matrix, and also causes the noise term to blow up.\n- det|M| ≈0: Though this matrix may be invertible, it may cause numerical instability in the machine vision system, and\ncan also cause the noise term to blow up.\nLet's also revisit, just as a refresher, the inverse of a 2 × 2 matrix:\nA =\na\nb\nc\nd\n\n(4)\n(5)\nA-1 =\ndetA\nd\n-b\n-c\na\n\n=\nad -bc\nd\n-b\n-c\na\n\n(6)\nNow let's verify that this is indeed the inverse:\nA-1A =\nad -bc\nd\n-b\n-c\na\na\nb\nc\nd\n\n=\nad -bc\nad -bc\n-ab + ab\ncd -cd\nad -bc\n\n=\n\n= I2\n(7)\n2.3\nReview from Lecture 2\nBefore we dive into the next set of concepts, let's also revisit some of the concepts discussed in the previous lectures.\n2.3.1\nTwo-Pixel Motion Estimation, Vector Form\nFirst, let's recall the two pixel motion estimation set of equations:\nuEx1 + vEy1 + Et1 = 0\n(8)\nuEx2 + vEy2 + Et2 = 0\n(9)\nRewritten in matrix form:\nu\nv\n\n=\nEx1Ey2 -Ey1Ex2\nEy2\n-Ey1\n-Ex2\nEx1\n\n(10)\nTake note of the denominator on the right-hand side. Does this term look familiar to the determinant term above? We were\nindeed solving an instance of the inverse problem. If this determinant-like quantity (Ex1Ey2 -Ey1Ex2) is small, the noise is\ngreatly amplified. We saw that this happens when brightness gradients are similar to one another.\n2.3.2\nConstant Brightness Assumption, and Motion Equation Derivation\nRecall the constant brightness assumption's mathematical formulation:\nConstant Brightness Assumption =⇒dE\ndt = 0\n(11)\n*(Please note the quantity above is a total derivative.)\nIntuition Behind This: As the object/camera moves, the physical properties of the camera do not change and therefore\nthe total derivative of the brightness w.r.t. time is 0. From the chain rule, we can rewrite this total derivative assumption:\ndE\ndt = 0 =⇒uEx + vEy + Et = 0\n(12)\n*(Recall this is for when x and y are parameterized w.r.t. time, i.e. x = x(t), y = y(t).)\nThe above constraint is known as the Brightness Change Constraint Equation (BCCE).\n\n2.3.3\nOptical Mouse Problem\nRecall our motion estimation problem with the optical mouse, in which our objective is no longer to find the point where the\nBCCE is strictly zero (since images are frequently corrupted by noise through sensing), but to minimize the LHS of the BCCE,\ni.e:\nmin\nu,v {J(u, v)\n∆=\nZZ\n(uEx + vEy + Et)2dxdy}\n(13)\nWe solve the above using unconstrained optimization and by taking a \"least-squares\" approach (hence why we square the LHS\nof the BCCE). Solve by setting the derivatives of the two optimizing variables to zero:\ndJ(u, v)\ndu\n= 0, dJ(u, v)\ndv\n= 0\n(14)\n2.3.4\nPerspective Projection\nRecall the perspective projection equations in the scalar form:\nx\nf = X\nZ (x-component), y\nf = Y\nZ (y-component)\n(15)\n*(Note that capital coordinates are in the world space, and lowercase coordinates are in the image space.)\nWhat if these quantities are changing in the world w.r.t. time? Take time derivatives:\nf\ndx\ndt = 1\nZ\ndX\ndt -1\nZ2 X dZ\ndt\n(16)\nWriting these for x and y:\n- x:\nf u = 1\nZ U -W\nZ\nX\nZ\n- y:\nf v = 1\nZ V -W\nZ\nY\nZ\nNote again the following definitions:\n- u →image velocity in the x direction\n- v →image velocity in the y direction\n- U →world velocity in the X direction\n- V →world velocity in the Y direction\nWhen are these points in (u, v) space interesting? When u = v = 0 - this is the Focus of Expansion (FOE). The FOE given\nby (x0, y0) in two dimensions:\n(x0, y0) = ( f\nZ\nU\nW , f\nZ\nV\nW )\n(17)\n2.4\nTime to Contact (TTC)\nIn the previous lecture, we discussed the derivation of Time to Contact (TTC) in terms of meters:\nZ\nW\n∆== Z\ndZ\ndt\n= meters\nmeters\nseconds\n= seconds\n(18)\nLet us express the inverse of this Time to Contact (TTC) quantity as C:\nC\n∆= W\nZ =\nTTC\n(19)\nLet us now suppose we parameterize our spatial velocities u and v according to u = Cx, v = Cy, where C is the inverse TTC\nvalue we introduced above. Then, substituting these into the BCCE equation, we have:\nRecall BCCE:\nuEx + vEy + Et = 0\n(20)\nSubstitute:\nC(xEx + yEy) + Et = 0\n(21)\nSolve for C:\nC = -\nEt\nxEx + yEy\n(22)\n\nThe denominator in the derivation of C is the \"radial gradient\":\ng = xEx + yEy = (x, y) · (Ex, Ey)\n(23)\nBuilding Intuition: If we conceptualize 2D images as topographic maps, where brightness is the third dimension of the surface\n(and the spatial dimensions x and y comprise the other two dimensions), then the brightness gradient is the direction of steepest\nascent up the brightness surface.\nAnother note: (x, y) is a radial vector, say in a polar coordinate system. Hence why the above dot product term is coined\nthe name \"radial gradient\". This gradient is typically normalized by its L2/Euclidean norm to illustrate the multiplication of\nthe brightness gradients with a radial unit vector):\ng =\np\nx2 + y2(\nx\np\nx2 + y2 ,\ny\np\nx2 + y2 ) · (Ex, Ey)\n(24)\nThis g quantity can be thought of as: \"How much brightness variation is in an outward direction from the center of the image?\"\nFor a more robust estimate, let us again employ the philosophy that estimating from more data points is better. We'll again\ntake a least-squares approach, and minimize across the entire image using the parameterized velocities we had before. In this\ncase, since we are solving for inverse Time to Contact, we will minimize the error term over this quantity:\nmin\nC {J(C)\n∆=\nZZ\n(C(xEx + yEy) + Et)2dxdy}\n(25)\nWithout the presence of measurement noise, the optimal value of C gives us an error of zero, i.e. perfect adherence to the\nBCCE. However, as we've seen with other cases, this is not the case in practice due to noise corruption. We again will use\nunconstrained optimization to solve this problem.\nTaking the derivative of the objective J(C) and setting it to zero, we obtain:\ndJ(C)\ndC\n= 0 =⇒2\nZZ\n(C(xEx + yEy) + Et)(xEx + yEy)dxdy = 0\n(26)\nThis in turn gives us:\nC\nZZ\n(xEx + yEy)2dxdy +\nZZ\n(xEx + yEy)Etdxdy = 0\n(27)\nTTC = C = -\nRR\n(xEx + yEy)Etdxdy\nRR\n(xEx + yEy)2dxdy\n(28)\nA few notes here:\n- Et can be computed by taking differences between a pixel at different points in time.\n- To implement a framework like this, we can do so with accumulators.\n- This is an instance of \"direct computation\".\n- When objects/important components of a scene are far away, we can combine image pixels and run these algorithms at\nmultiple scales - this allows us to compute/analyze motion velocities over different timescales. This can also be helpful\nfor building more computationally-tractable motion estimation implementations, since the number of pixels over which\ncomputations must occur can be reduced quadratically with the scale.\n- Note one problem we run into with this approach - each pixel we apply this estimation approach to introduces one equation,\nbut two unknowns.\n- Note that neighboring pixels typically exhibit similar behavior to one another.\nLet's briefly return to the concept of the radial gradient. When is this zero?\n- E = 0 everywhere (coal mine)\n- (x, y) · (Ex, Ey) = 0 (radial gradient is zero)\nNow, let's take a more general case when we have world motion, i.e. U = 0, V = 0. For our two components x and y:\n\n- x-component:\nu\nf = U\nZ -X\nf\nW\nZ\n(29)\nu = fU\nZ -X W\nZ\n(30)\nu = A -XC\n(31)\nWhere: A\n∆= fU\nZ , C\n∆= W\nZ\n(32)\n- y-component:\nv\nf = V\nZ -Y\nf\nW\nZ\n(33)\nv = fV\nZ -Y W\nZ\n(34)\nv = B -Y C\n(35)\nWhere: B\n∆= fV\nZ , C\n∆= W\nZ\n(36)\nNote that for the world quantities A and B, we also have the following identities (note that the Focus of Expansion (FOE) is\ngiven by the point (x0, y0):\n- A = fU\nZ = Cx0\n- B = fV\nZ = Cy0\nBuilding Intuition: \"As I approach the wall, it will loom outward and increase in size.\"\nNow returning to our BCCE, this time with A, B, and C:\nAEx + BEy + C(xEx + yEy) + Et = 0\n(37)\nWe can again use least-squares to minimize the following objective enforcing the BCCE. This time, our optimization aim is to\nminimize the objective function J(A, B, C) using the quantities A, B, and C:\nmin\nA,B,C{J(A, B, C)}\n∆=\nZZ\n(AEx + BEy + C(xEx + yEy) + Et)2dxdy}\n(38)\nUse unconstrained optimzation with calculus and partial derivatives to solve. Since we have three variables to optimize over, we\nhave three first-order conditions (FOCs):\n-\ndJ(A,B,C)\ndA\n= 0\n-\ndJ(A,B,C)\ndB\n= 0\n-\ndJ(A,B,C)\ndC\n= 0\nUsing the Chain rule for each of these FOCs, we can derive and rewrite each of these conditions to obtain 3 equations and 3\nunknowns. Note that G\n∆= xEx + yEy.\n- A variable:\nZZ\n(AEx + BEy + C(xEx + yEy))Ex = 0\n(39)\nA\nZZ\nE2\nx + B\nZZ\nExEy + C\nZZ\nGEx = -\nZZ\nExEt\n(40)\n- B variable:\nZZ\n(AEx + BEy + C(xEx + yEy))Ex = 0\n(41)\nA\nZZ\nEyEx + B\nZZ\nE2\ny + C\nZZ\nGEy = -\nZZ\nEyEt\n(42)\n\n- C variable:\nZZ\n(AEx + BEy + C(xEx + yEy))Ex = 0\n(43)\nA\nZZ\nGEx + B\nZZ\nGEy + C\nZZ\nG2 = -\nZZ\nGEt\n(44)\nPutting all of these equations together:\nA\nZZ\nE2\nx + B\nZZ\nExEy + C\nZZ\nGEx = -\nZZ\nExEt\nA\nZZ\nEyEx + B\nZZ\nE2\ny + C\nZZ\nGEy = -\nZZ\nEyEt\nA\nZZ\nGEx + B\nZZ\nGEy + C\nZZ\nG2 = -\nZZ\nGEt\nThis can be compactly written as a matrix-vector product equation:\n\nRR\nE2\nx\nRR\nExEy\nRR\nExG\nRR\nEyEx\nRR\nE2\ny\nRR\nEyG\nRR\nGEx\nRR\nGEy\nRR\nG2\n\nA\nB\nC\n\n= -\n\nRR\nExEt\nRR\nEyEt\nRR\nGEt\n\nAs in the time-to-contact problem above, this can again be implemented using accumulators.\nLet's end on a fun fact: Did you know that optical mice have frame rates of 1800 fps?\nLecture 4: Fixed Optical Flow, Optical Mouse, Constant Brightness As-\nsumption, Closed Form Solution\n3.1\nReview\nLet's frame the topics today by briefly reviewing some concepts we've covered in the previous lectures. Feel free to skip this\nsection if you already feel comfortable with the material.\n- Image formation\n- Where in the image? Recall perspective projection:\nx\nf = X\nZ , y\nf = Y\nZ\nDifferentiating this expression gives:\nu\nf = U\nZ -X\nZ\nW\nZ , v\nf = V\nZ -Y\nZ\nW\nZ\nFrom these, we can find the Focus of Expansion (FOE), or, more intuitively: \"The point in the image toward\nwhich you are moving.\"\nHow long until we reach this point? This is given by Time to Contact (TTC):\nTime to Contact = Z\nW = 1\nC\n- How bright in the image? For this, let us consider an image solid, where the brightness function is parameterized by\nx, y, and t: E(x, y, t).\n\n3.1.1\nConstant Brightness Assumption Review with Generalized Isophotes\nRecall the constant brightness assumption, which says that the total derivative of brightness with respect to time is zero:\ndE(x,y,t)\ndt\n= 0. By chain rule we obtain the BCCE:\ndx\ndt\n∂E\n∂x + dy\ndt\n∂E\n∂y + ∂E\n∂t = 0\nRecall our variables: u\n∆= dx\ndt , v\n∆= dy\ndt . Then BCCE rewritten in the standard notation we've been using:\nuEx + yEy + Et = 0\nRecall our method of using least-squares regression to solve for optimal values of u, v that minimize the total computed sum\nof the LHS of the BCCE over the entire image (note that integrals become discrete in the presence of discretized pixels, and\nderivatives become differences):\nu∗, v∗= arg min\nu,v\nZZ\n(uEx + yEy + Et)dxdy\nThe first-order conditions (FOCs) of this optimization problem give:\nu\nZZ\nE2\nx + v\nZZ\nExEy = -\nZZ\nExEt\n(45)\nu\nZZ\nEyEx + v\nZZ\nE2\ny = -\nZZ\nEyEt\n(46)\nWritten in matrix-vector form, our equations become:\n\" RR\nE2\nx\nRR\nExEy\nRR\nEyEx\nRR\nE2\ny\n# u\nv\n\n= -\nRR\nExEt\nRR\nEyEt\n\n(New) Now, to introduce a new variation on this problem, let us suppose we have the following spatial parameterization of\nbrightness (you'll see that this brightness function creates linear isophotes) for linear f:\nE(x, y) = f(ax + by)\nIf f is differentiable over the domain, then the spatial derivatives Ex and Ey can be computed as follows, using the chain rule:\n- Ex = f ′(ax + by)a\n- Ey = f ′(ax + by)b\nWhere f ′ is the derivative of this scalar-valued function (i.e, we can define the input to be z = ax + by, and the derivative f ′ is\ntherefore equivalent to df(z)\ndz ).\nIsophote Example: If E(x, y) = ax + by + c, for a, b, c ∈R+, then the isophotes of this brightness function will be lin-\near.\n3.1.2\nTime to Contact (TTC) Review\nRecall the following symbols/quantities from TTC:\n- C = Z\nw\n- TTC = w\nZ = 1\nZ\ndZ\ndt =\nd\ndtloge(z) , therefore we can simply take the slope of the line corresponding to the logarithm of Z to\ncompute TTC.\nNow, let's suppose that objects are moving both in the world and in the image. Let's denote s as our image coordinate and S\nas our world coordinate. Then:\ns\nf = S\nZ\nThen we can write:\nsZ + sf = 0\n\nDifferentiating:\nZ ds\ndt + sdZ\ndt = 0 =⇒\nds\ndt\nS =\ndZ\ndt\nZ\nThe above relationship between derivative ratios can be interpreted as: \"The change in the image's size is the same as the change\nin distance.\"\n3.2\nIncreasing Generality of TTC Problems\nLet us now consider adding some additional generality to the Time to Contact (TTC) problem. We've already visited some of\nthese cases before:\n- Simple case: Solving for C\n- More General case: Solving for A, B, C\n- Even More General case: (Motivation) What if the optical axis isn't perpendicular to the wall? What if the camera\nplane is tilted, e.g. Z = aX + bY + C for some a, b, C ∈R? In this case, we can solve the problem numerically rather\nthan through a closed-form expression.\nAnother motivating question for developing TTC methods: What if the surface is non-planar? This is a common scenario for\nreal-world TTC systems. In this case, we have two options:\n- Parameterize the geometric models of these equations with polynomials, rather than planes.\n- Leave the planar solution, and look for other ways to account for errors between the modeled and true surfaces.\nIn practice, the second option here actually works better. The first option allows for higher modelling precision, but is less robust\nto local optima, and can increase the sensitivity of the parameters we find through least-squares optimization.\nIf you want to draw an analog to machine learning/statistics, we can think of modeling surfaces with more parameters (e.g.\npolynomials rather than planes) as creating a model that will overfit or not generalize well to the data it learns on, and create\na problem with too many unknowns and not enough equations.\n3.3\nMultiscale and TTC\nIf you recall from last lecture, we saw that TTC and FOE estimation \"fell apart\" as we got really close. This is due to mea-\nsurement sensitivity and the fact that the pixels occupy increasingly more space as we get closer and closer. This is where\nmultiscale can help us - it enables us to use more coarse resolutions for these estimation problems. The implicit averaging done\nthrough downsampling allows us to \"smoooth out\" any measurement noise that may be present, and will consequently reduce\nthe magnitide of pixel brightness gradients.\nAdditionally, multiscale is computationally-efficient: Using the infinite geometric series, we can see that downsampling/down-\nscaling by a factor of 2 each time and storing all of these smaller image representations requires only 33% more stored data than\nthe full size image itself:\ninf\nX\nn=0\n((1\n2)2)n =\n1 -1\n= 4\n3 = 1 + 1\nMore generally, for any downsampling factor r ∈N, we only add\nr2-1 × 100% amount of additional data:\ninf\nX\nn=0\n( 1\nr2 )n =\n1 -1\nr2\n=\nr2\nr2 -1 = (r2 -1) + 1\nr2 -1\n= 1 +\nr2 -1\n(Note that we have r2 rather than r in the denominator because we are downsampling across both the x and y dimensions.)\n\n3.3.1\nAliasing and Nyquist's Sampling Theorem\nThough multiscale is great, we also have to be mindful of aliasing. Recall from 6.003 (or another Signals and Systems course)\nthat aliasing causes overlap and distortion between signals in the frequency domain, and it is required that we sample at a spatial\nfrequency that is high enough to not produce aliasing artifacts.\nNyquist's Sampling Theorem states that we must sample at twice the frequency of the highest-varying component of our image\nto avoid aliasing and consequently reducing spatial artifacts.\n3.3.2\nApplications of TTC\nA few more applications of TTC:\n- Airplane Wing Safety - Using TTC to make sure wings don't crash into any objects at airports.\n- NASA TTC Control - TTC systems were used to ensure that NASA's payload doesn't crash into the surface of earth/other\nplanets/moons.\nIn this application, feedback control was achieved by setting a nominal \"desired TTC\" and using an\namplifier, imaging, and TTC estimation to maintain this desired TTC.\n- Autonomous Vehicles - e.g. a vehicle is coming out of a parking lot and approaching a bus - how do we control when/if to\nbrake?\nLet's discuss the NASA TTC Control example a little further. Using our equation for TTC:\nZ\nW = T\nWe can rewrite this as a first-order Ordinary Differential Equation (ODE):\nZ\ndZ\ndt\n= T =⇒dZ\ndt = 1\nT Z\nSince the derivative of Z is proportional to Z, the solution to this ODE will be an exponential function in time:\nZ(t) = Z0e\n-t\nT\nWhere Z0 depends on the initial conditions of the system.\nThis method requires that deceleration is not uniform, which is not the most energy efficient approach for solving this problem.\nAs you can imagine, energy conservation is very important in space missions, so let's next consider a constant deceleration\napproach. Note that under constant deceleration, we have d2z\ndt2\n∆= a = 0. Then we can express the first derivative of Z w.r.t. t as:\ndZ\ndt = at + v0\nWhere v0 is an initial velocity determined by the boundary/initial conditions. Here we have the following boundary condition:\ndZ\ndt = a(t -t0)\nThis boundary condition gives rise to the following solution:\nZ = 1\n2at2 -at0t + c = 1\n2a(t -t0)2\nTherefore, the TTC for this example becomes:\nT = Z\ndZ\ndt\n=\n2a(t -t0)2\na(t -t0)\n= 1\n2(t -t0)\n\n3.4\nOptical Flow\nMotivating question: What if the motion of an image is non-constant, or it doesn't move together? We have the Brightness\nChange Constraint Equation (BCCE), but this only introduces one constraint to solve for two variables, and thus creates\nan under-constrained/ill-posed problem.\nHow can we impose additional constraints? To do this, let us first understand how motion relates across pixels, and\ninformation that they share. Pixels don't necessarily move exactly together, but they move together in similar patterns, partic-\nularly if pixels are close to one another. We'll revisit this point in later lectures.\nWhat Else Can We Do? One solution is to divide the images into equal-sized patches and apply the Fixed Flow Paradigm,\nas we've done with entire images before. When selecting patch size, one trade-offto be mindful of is that the smaller the patch,\nthe more uniform the brightness patterns will be across the patch, and patches may be too uniform to detect motion (note: this\nis equivalent to the matrix determinants we've been looking at evaluating to zero/near-zero).\n3.5\nVanishing Points (Perspective Projection)\nBefore we dive into what vanishing points are, let's discuss why they're useful. Vanishing points can be useful for:\n- Camera calibration - has applications to robotics, autonomous vehicles, photogrammetry, etc.\n- Finding relative orientation between two coordinate frames/systems\nNow, let's discuss what it is. Suppose we have several parallel lines in the world, and we image them by projecting them onto\nthe 2D image plane. Then vanishing points are the points in the image (or, more commonly, outside of the image) where these\nlines converge to in the image. To discuss these mathematically, let's first discuss about defining lines in a 3D world:\n- Vector Form: R = R0 + sˆn\nHere, we can express this using our standard vector form of perspective projection:\nf r =\nR · ˆzR\n=\n(R0 + sˆn) · ˆn(R0 + sˆn)\n- Parametrically: (x0 + αs, y0 + βs, z0 + γs)\nHere, we can expand this to our standard Cartesian form of perspective projection to apply our transformation:\nx\nf =\nZ0 + γs(x0 + αs)\ny\nf =\nZ0 + γs(y0 + βs)\nTo build intuition, let's consider what happens when we travel far along the lines (i.e. as s gets very large) in our parametric\ndefinition of lines:\n- limx→infx\nf = limx→inf\nZ0+γs(x0 + αs) = αs\nγs = α\nγ\n(x-coordinate)\n- limy→inf\ny\nf = limy→inf\nZ0+γs(x0 + βs) = βs\nγs = β\nγ\n(x-coordinate)\nThe 2D point ( α\nγ , β\nγ ) is the vanishing point in the image plane. As we move along the line in the world, we approach this point\nin the image, but we will never reach it. More generally, we claim that parallel lines in the world have the same vanishing\npoint in the image.\n3.5.1\nApplications of Vanishing Points\nLet's now discuss some of these applications in greater detail.\n- Protecting Pedestrians on a Road Side: To protect pedestrians, the camera must transform its coordinate system.\nThis transformation can be found using vanishing points.\n- Camera Calibration: One way to calibrate a camera is to solve for the Center of Projection (COP) in the image\nspace, using perspective projection. Calibration is typically achieved through calibration objects.\n\n3.6\nCalibration Objects\nLet's discuss two calibration objects: spheres and cubes:\n3.6.1\nSpheres\n:\n- If image projection is directly overhead/straight-on, the projection from the world sphere to the image plane is a circle. If\nit is not overhead/straight on, it is elliptic.\n- Relatively easy to manufacture\n3.6.2\nCube\n:\n- Harder to manufacture, but generally a better calibration object than a sphere.\n- Cubes can be used for detecting edges, which in turn can be used to find vanishing points (since edges are lines in the\nworld).\n- Cubes have three sets of four parallel lines/edges each, and each of these sets of lines are orthogonal to the others. This\nimplies that we will have three vanishing points - one for each set of parallel lines.\n- For each of these sets of lines, we can pick a line that goes through the Center of Projection (COP), denoted p ∈R3 (in\nthe world plane). We can then project the COP onto the image plane (and therefore now p ∈R⊨).\n- Let us denote the vanishing points of the cube in the image plane as a, b, c ∈R2. Then, because of orthogonality between\nthe different sets of lines, we have the following relations between our three vanishing points and p:\n- (p -a) · (p -b) = 0\n- (p -b) · (p -c) = 0\n- (p -c) · (p -a) = 0\nIn other words, the difference vectors between p and the vanishing points are all at right angles to each other.\nTo find p, we have three equations and three unknowns.\nWe have terms that are quadratic in p.\nUsing B ezout's\nTheorem (The maximum number of solutions is the product of the polynomial order of each equation in the system of\nequations), we have (2)3 = 8 possible solutions for our system of equations. More generally:\nnumber of solutions =\nE\nY\ne=1\noe\nWhhere E is the number of equations and oe is the polynomial order of the eth equation in the system.\nThis is too many equations to work with, but we can subtract these equations from one another and create a system\nof 3 linearly dependent equations. Or, even better, we can leave one equation in its quadratic form, and 2 in their linear\nform, and this maintains linear independence of this system of equations:\n- (a -p) · (c -b) = 0\n- (b -p) · (a -c) = 0\n- (p -c) · (p -a) = 0\n3.7\nAdditional Topics\nWe'll also briefly recap some of the topics discussed in our synchronous section today. These topics are not meant to introduce\nnew concepts, but are designed to generalize the concepts we've discussed such that they can be adapted for a broader range of\napplications.\n\n3.7.1\nGeneralization: Fixed Flow\nThe motivating example for this generalization is a rotating optimal mouse. We'll see that instead of just solving for our two\nvelocity parameters u and v, we'll also need to solve for our rotational velocity, ω.\nSuppose we are given the following parameterizations of velocity:\n- u = u0 -wy\n- v = v0 + wx\nNote that we can also write the radial vector of x and y, as well as the angle in this 2D plane to show how this connects to\nrotation:\nr = (x, y) =\np\nx2 + y2\nθ = arctan 2(y, x)\nω = dθ\ndt\nWith this rotation variable, we leverage the same least-squares approach as before over the entirety of the image, but now we\nalso optimize over the variable for ω:\nu∗\n0, v∗\n0, ω∗= arg min\nu0,v0,ω{J(u0, v0, ω)\n∆=\nZZ\n(u0Ex + v0Ey + wH + Et)2dxdy}\nLike the other least-squares optimization problems we've encountered before, this problem can be solved by solving a system of\nfirst-order conditions (FOCs):\n-\ndJ(u0,v0,ω)\ndu0\n= 0\n-\ndJ(u0,v0,ω)\ndv0\n= 0\n-\ndJ(u0,v0,ω)\ndω\n= 0\n3.7.2\nGeneralization: Time to Contact (for U = 0, V = 0, ω = 0)\nLet's now revisit TTC, but with the following parameterization: U = 0, V = 0, image is of a tilted plane.\nFor this, we can write Z as a function of the world coordinates X and Y :\nZ = Z0 + ∂Z\n∂X X + ∂Z\n∂Y Y\nZ = Z0 + pX + qY\nRecall the following derivations for the image coordinate velocities u and v, which help us relate image motion in 2D to world\nmotion in 3D:\n-\nu\nf = U\nZ -X\nZ\nW\nZ\n-\nv\nf = V\nZ -Y\nZ\nW\nZ\nSome additional terms that are helpful when discussing these topics:\n- Motion Field: Projection of 3D motion onto the 2D image plane.\n- Optical Flow:\n- What we can sense\n- Describes motion in the image\nWe can transform this into image coordinates:\nu = 1\n2(fU -xw), v = 1\n2(fV -yw)\n\nLet's take U = V = 0, u = -X w\nZ , v = -Y W\nZ .\nZ (world coordinates) is not constant, so we can rewrite this quantity by\nsubstituting the image coordinates in for our expression for Z:\nZ = Z0 + px + qy\n= Z0 + pX\nf Z + q Y\nf Z\nNow, we can isolate Z and solve for its closed form:\nZ(1 -pX\nf -q Y\nf ) = Z0\nZ =\nZ0\n1 -p X\nf -q Y\nf\nFrom this we can conclude that\nZ is linear in x and y (the image coordinates, not the world coordinates). This is helpful for\nmethods that operate on finding solutions to linear systems. If we now apply this to the BCCE given by uEx + vEy + Et = 0,\nwe can first express each of the velocities in terms of this derived expression for Z:\n- u =\nZ0 (1 -p X\nf -q Y\nf )(-xω)\n- v =\nZ0 (1 -p X\nf -q Y\nf )(-yω)\nApplying these definitions to the BCCE:\n0 = uEx + vEy + Et\n= 1\nZ0\n(1 -pX\nf -q Y\nf )(-xω)Ex + 1\nZ0\n(1 -pX\nf -q Y\nf )(-yω)Ey + Et\nCombining like terms, we can rewrite this constraint as:\n0 = -ω\nZ0\n(1 -pX\nf -q Y\nf )(xEx + yEy) + Et\nEquivalently, dividing everything by -1:\n0 = ω\nZ0\n(1 -pX\nf -q Y\nf )(xEx + yEy) -Et\nWe can also express this with the \"radial gradient\" given by: G\n∆= (xEx + yEy):\n0 = ω\nZ0\n(1 -pX\nf -q Y\nf )G -Et\nFor symbolic simplicity, take the following definitions:\n- R\n∆=\nw\nZ0\n- P\n∆= -p w\nZ0\n- Q\n∆= -q w\nZ0\nUsing these definitions, the BCCE with this paramterization becomes:\n0 = (R + Px + Qy)G -Et\nNow, we can again take our standard approach of solving these kinds of problems by applying least-squares to estimate the free\nvariables P, Q, R over the entire continuous or discrete image space. Like other cases, this fails when the determinant of the\nsystem involving these equations is zero.\nLecture 5: TTC and FOR Montivision Demos, Vanishing Point, Use of VPs\nin Camera Calibration\nIn this lecture, we'll continue going over vanishing points in machine vision, as well as introduce how we can use brightness\nestimates to obtain estimates of a surface's orientation. We'll introduce this idea with Lambertian surfaces, but we can discuss\nhow this can generalize to many other types of surfaces as well.\n\n4.1\nRobust Estimation and Sampling\nWe'll start by covering some of the topics discussed during lecture.\n4.1.1\nLine Intersection Least-Squares\nHelpful when considering multiple lines at a time - this optimization problem is given by:\nmin\nx,y\nX\ni\n(x cos θi + y sin θi -ρ)2\n(47)\n4.1.2\nDealing with Outliers\nMany of the approaches we've covered thus far are centered around the idea of making estimates from data. But recall that\nthe data gathered by sensors can be noisy, and can be corrupted from phenomena such as crosstalk. Because of this noise,\nit is advantageous to distinguish and filter outliers from the inliers in our dataset, especially when we make estimates using\nLeast-Squares.\nA good choice of algorithm for dealing with outliers is RANSAC, or Random Sample Consensus [1].\nThis algorithm\nis essentially an iterative and stochastic variation of Least-Squares. By randomly selecting points from an existing dataset to fit\nlines and evaluate the fit, we can iteratively find line fits that minimize the least-squares error while distinguishing inliers from\noutliers.\nFigure 1: Pseudocode for the RANSAC Algorithm.\n4.1.3\nReprojection and Rectification\nThese two problems are highly important in mapmaking, and help for solving the equations:\nRr = r′\nr = RT r\nWhere r is the vector from the Center of Projection (COP) to the image plane.\n4.1.4\nResampling\nResampling is also a valuable application in many facets of computer vision and robotics, especially if we seek to run any kind\nof interpolation or subsampling algorithms. Some approaches for this:\n- Nearest Neighbor: This is a class of methods in which we interpolate based offof the values of neighboring points. This\ncan be done spatially (e.g. by looking at adjacent pixels) as well as other image properties such as brightness and color. A\ncommon algorithm used here is K-Nearest Neighbors (KNN), in which interpolation is done based offof the K-nearest\npoints in the desired space.\n- Bilinear Interpolation: An extension of linear interpolation used for functions in two-dimensional grids/of two variables\n(e.g. (x, y) or (i, j))) [2], such as the brightness or motion in images.\n\n- Bicubic Interpolation: Similar to bilinear interpolation, bicubic interpolation is an extension of cubic interpolation\nof functions in two-dimensional grids/of two variables (e.g.\n(x, y) or (i, j))) [3], such as the brightness or motion in\nimages. Bicubic interpolation tends to perform much better than bilinear interpolation, though at the cost of additional\ncomputational resources.\n4.2\nMagnification with TTC\n- For the Montivision demo, note the following for the bars:\n- A →x-dimension for motion estimation problem\n- B →y-dimension for motion estimation problem\n- C →\nTime to Contact\n- Recall from the previous lectures that the percent change of size in the world is the percent change of size in the image.\nWe can derive this through perspective projection. The equation for this is:\ns\nf = S\nZ\nWhere s is the size in the image plane and S is the size in the world. Differentiating with respect to time gives us (using\nthe chain rule):\nd\ndt(sZ = fS) →Z ds\ndt + sdZ\ndt = 0\nRearranging terms:\ndz\ndt\nZ f = -\nds\ndt\nS\nRecall that the intuition here is that the rate of change of size is the same in the image and the world.\n4.2.1\nPerspective Projection and Vanishing Points\nLet's begin by defining a few terms. Some of these will be a review, but these review terms connect to new terms that we'll\nintroduce shortly:\n- vanishing points: These are the points in the image plane (or extended out from the image plane) that parallel lines in\nthe world converge to.\n- Center of Projection (COP): This is where in the camera/sensor (not in the image plane) where incident projected\nlight rays converge. An analog to the COP for human vision systems is your eyes. NOTE: COP and vanishing points are\noftentimes different.\n- Principle Point: The orthogonal projection of the Center of Projection (COP) onto the image plane.\n- f: Similar to the focal length we've seen in other perspective projection examples, this f is the perpendicular distance from\nthe COP to the image plane.\nRecall that a common problem we can solve with the use of vanishing points is finding the Center of Projection (COP).\nSolving this problem in 3D has 3 degrees of freedom, so consequently we'll try to solve it using three equations.\nIntuitively, this problem of finding the Center of Projection can be thought of as finding the intersection of three spheres,\neach of which have two vanishing points along their diameters. Note that three spheres can intersect in up to two places - in this\ncase we have defined the physically-feasible solution, and therefore the solution of interest, to be the solution above the image\nplane (the other solution will be a mirror image of this solution and can be found below the image plane).\nApplication of this problem: This problem comes up frequently in photogrammetry, in that simply having two locations as\nyour vanishing points isn't enough to uniquely identify your location on a sphere.\n\n4.2.2\nLines in 2D and 3D\nNext, let's briefly review how we can parameterize lines in 2D and 3D:\n- 2D: (2 Degrees of Freedom)\n- y = mx + c\n- ax + by + c = 0\n- sin θx -cos θy + ρ = 0\n- If ˆn\n∆= (-sin θ, cos θ)T , then ˆn · r = ρ.\n- 3D: (3 Degrees of Freedom)\n- ˆn = ρ\n- aX + bY + cZ + d = 0\n4.2.3\nApplication: Multilateration (MLAT)\nThis problem comes up when we estimate either our position or the position of other objects based offof Time of Arrival\n[4]. One specific application of this problem is localizing ourselves using distances/Time of Arrival of wifiaccess points inside\nbuildings/other locations without access to GPS.\nLike the other problems we've looked at, this problem can be solved by finding the intersection of 3 spheres.\nLet's begin\nwith:\n||r -ri||2 = pi ∀i ∈{1, · · · , N}\nNext, let's square both sides of this equation and rewrite the left-hand side with dot products:\n||r -ri||2\n2 = (r -ri)T (r -ri) = p2\ni\n= r · r -2r · ri + ri · ri = p2\ni\nRecall from Bezout's Theorem that this means that are 8 possible solutions here, since we have three equations of second-order\npolynomials. To get rid of the 2nd order terms, we simply subtract the equations:\nr · r -2r · ri + ri · ri = ρ2\ni\n-r · r -2r · rj + rj · rj = ρ2\nj\n∀i, j ∈{1, 2, 3}, i = j\nSubtracting these pairs of equations yields a linear equation in R:\n2r(rj -ri) = (ρ2\nj -ρ2\ni ) -(R2\nj -R2\ni )\n(Where the scalar R2\nj\n∆= rj · rj.)\nPutting these equations together, this is equivalent to finding the intersection of three different spheres:\n\n(r2 -r1)T\nr3 -r2)T\nr1 -r3)T\n\nr = 1\n\n(ρ2\n2 -ρ2\n1) -(R2\n2 -R2\n1)\n(ρ2\n3 -ρ2\n2) -(R2\n3 -R2\n2)\n(ρ2\n1 -ρ2\n3) -(R2\n1 -R2\n3)\n\nHowever, even though we've eliminated the second-order terms from these three equations, we still have two solutions. Recall\nfrom linear algebra equations don't have a unique solution when there is redundancy or linear dependence between the equations.\nIf we add up the rows on the right-hand side of the previous equation, we get 0, which indicates that the matrix on the left-hand\nside is singular:\nA\n∆=\n\n(r2 -r1)T\nr3 -r2)T\nr1 -r3)T\n\n∈R3×3\n\nTo solve this linear dependence problem, we again use Bezout's Theorem and keep one of the second-order equations:\n(r -r1) · (r -r2) = 0\n(r -r2) · (r -r3) = 0\n(r -r2) · (r -r2) = 0 →(r -r2) ⊥(r3 -r1)\nI.e. the plane passes through r2 - this intersecting point is the solution and is known as the orthocenter or the principal\npoint. Now, all we need is to find the quantity f to find the Center of Projection.\nNext, note the following relations between the vanishing points in the inverse plane and ˆz, which lies perpendicular to the\nimage plane:\nr1 · ˆz = 0\nr2 · ˆz = 0\nr3 · ˆz = 0\nWhat else is this useful for? Here are some other applications:\n- Camera calibration (this was the example above)\n- Detecting image cropping - e.g. if you have been cropped out of an image\n- Photogrammetry - e.g. by verifying if the explorer who claimed he was the first to make it \"all the way\" to the North Pole\nactually did (fun fact: he didn't).\nNext, now that we've determined the principal point, what can we say about f (the \"focal length\")?\nFor this, let us consider the 3D simplex, which is triangular surface in R3 given by the unit vectors:\ne1\n∆=\n0T\ne2\n∆=\n0T\ne3\n∆=\n1T\nUsing this 3D simplex, let us suppose that each side of the triangles formed by this simplex take length v =\n√\n2, which is\nconsistent with the l2 norm of the triangles spanned by the unit simplex (\n√\n12 + 12 =\n√\n2).\nNext, we solve for f by finding the value of a such that the dot product between the unit vector perpendicular to the sim-\nplex and a vector of all a is equal to 1:\na\na\naT h\n√\n√\n√\niT\n= 1\n3a\n√\n3 = 1\na =\n√\n=\n√\nThis value of a =\n√\n3 = f. Then we can relate the lengths of the sides v (which correspond to the magnitudes of the vectors\nbetween the principal point and the vanishing points (||r -ri||2)) and f:\nv =\n√\n2, f =\n√\n3 =⇒f =\nv\n√\nWith this, we've now computed both the principal point and the \"focal length\" f for camera calibration.\n4.2.4\nApplication: Understand Orientation of Camera w.r.t. World\nFor this problem, the goal is to transform from the world frame of reference to the camera frame of reference, i.e. find a frame\nof reference such that the unit vectors ˆxc, ˆyc, ˆzc have the following orthogonality properties:\nˆxc · ˆyc ≈0\nˆyc · ˆzc ≈0\nˆxc · ˆzc ≈0\n\n(Note that the c superscript refers to the camera coordinate system.) If the location of the Center of Projection (COP) is given\nabove the image plane as the vector pc and the vanishing points are given as vectors rc\n1, rc\n2, and rc\n3 (note that these vanishing\npoints must be in the same frame of reference in order for this computation to carry out), then we can derive expressions for the\nunit vectors through the following relations:\n(pc -r1)//ˆxc =⇒ˆxc =\npc -rc\n||pc -rc\n1||2\n(pc -r2)//ˆyc =⇒ˆyc =\npc -rc\n||pc -rc\n2||2\n(pc -r3)//ˆzc =⇒ˆzc =\npc -rc\n||pc -rc\n3||2\nThen, after deriving the relative transformation between the world frame (typically denoted w in robotics) and the camera frame\n(typically denoted c in robotics), we can express the principal point/Center of Projection in the camera coordinate frame:\nr = αˆxc + βˆyc + γˆzc\nWhere (α, β, γ) are the coordinates in the object coordinate system (since ˆxc, ˆyc, and ˆzc comprise the orthogonal basis of this\ncoordinate system). Then we can express the relative coordinates of objects in this coordinate system:\nr′ =\nα\nβ\nγT\nTo transform between frames, we can do so via the following equation:\nr =\n\n-(ˆxc)T\n-(ˆyc)T\n-(ˆzc)T\n\nr′\nNote that the matrix defined by: R\n∆=\n\n-(ˆxc)T\n-(ˆyc)T\n-(ˆzc)T\n\nis orthonormal, since these vector entries are orthogonal to one another.\nThis matrix R is a rotation matrix, which means it is skew-symmetric, invertible, and its transpose is equal to its inverse:\nR-1 = RT . Therefore, if we wanted to solve the reverse problem that we did above (finding object coordinates from camera\ncoordinates), we can do so by simply taking the transpose of the matrix:\nr′ =\n\n-(ˆxc)T\n-(ˆyc)T\n-(ˆzc)T\n\n-1\nr\n=\n\n-(ˆxc)T\n-(ˆyc)T\n-(ˆzc)T\n\nT\nr\n4.3\nBrightness\nWe'll now switch to the topic of brightness, and how we can use it for surface estimation. Recall that for a Lambertian surface\n(which we'll assume we use here for now), the power received by photoreceptors (such as a human eye or an array of photodiodes)\ndepends both on the power emitted by the source, but also the angle between the light source and the object.\nThis is relevant for foreshortening (the visual effect or optical illusion that causes an object or distance to appear shorter\nthan it actually is because it is angled toward the viewer [6]): the perceived area of an object is the true area times the cosine\nof the angle between the light source and the object/viewer. Definition: Lambertian Object: An object that appears equally\nbright from all viewing directions and reflects all incident light, absorbing none [5].\nLet's look at a simple case: a Lambertian surface. If we have the brightness observed and we have this modeled as:\nE = ˆn · s,\nWhere s is the vector between the light source and the object, then can we use this information to recover the surface orientation\ngiven by ˆn. This unit vector surface orientation has degrees of freedom, since it is a vector in the plane.\n\nIt is hard to estimate this when just getting a single measurement for brightness. But what if we test different lighting conditions?:\nE1 = ˆn · s1\nE2 = ˆn · s2\nˆn · ˆn = ||ˆn||2 = 1\nThis is equivalent, intuitively, to finding the intersection between two cones for which we have different angles, which forms a\nplanar curve. We then intersect this planar curve with the unit sphere corresponding to the constraint ||ˆn||2 = 1. By Bezout's\nTheorem, this will produce two solutions.\nOne problem with this approach, however, is that these equations are not linear. We can use the presence of reflectance\nto help us solve this problem. Let us define the following:\nDefinition: Albedo: This is the ratio of power out of an object divided by power into an object:\n\"albedo\"\n∆= ρ\n∆= power in\npower out ∈[0, 1]\nThough the definition varies across different domains, in this class, we define albedo to be for a specific orientation, i.e. a specific\nsi.\nFun fact: Although an albedo greater than 1 technically violates the 2nd Law of Thermodynamics, superluminous sur-\nfaces such as those sprayed with flourescent paint can exhibit ρ > 1.\nUsing this albedo term, we can now solve our problem of having nonlinearity in our equations.\nNote that below we use\nthree different measurements this time, rather than just two:\nE1 = ρˆn · s1\nE2 = ρˆn · s2\nE3 = ρˆn · s3\nThis creates a system of 3 unknowns and 3 Degrees of Freedom. We also add the following constraints:\nn = ρˆn\nˆn =\nn\n||n||2\nCombining these equations and constraints, we can rewrite the above in matrix/vector form:\n\n-ˆsT\n-ˆsT\n-ˆsT\n\nn =\n\nE1\nE2\nE3\n\n=⇒n = S-1\n\nE1\nE2\nE3\n\n(Where S\n∆=\n\n-ˆsT\n-ˆsT\n-ˆsT\n\n.)\nA quick note on the solution above: like other linear algebra-based approaches we've investigated so far, the matrix S above\nisn't necessarily invertible. This matrix is not invertible when light sources are in a coplanar orientation relative to the object.\nIf this is the case, then the matrix S becomes singular/linearly dependent, and therefore non-invertible.\n4.3.1\nWhat if We Can't use Multiple Orientations?\nIn the case where the orientation of the camera, object, and light source cannot be changed (i.e. everything is fixed), or if it is\nexpensive to generate a surface estimation from a new orientation, then another option is to use different color sensors. Most\ncameras have RGB (\"Red-Green-Blue\") sensors, and thus we can use the same approach as above, where each color corresponds\nto a different orientation.\nThis RGB approach has a few issues:\n- RGB has \"crosstalk\" between color channels (it can be hard to excite/activate a single channel for color without exciting\nthe others as well).\n\n- The object may not be uniformly-colored (which, practically, is quite often the case).\nHowever, despite the drawbacks, this approach enables us to recover the surface orientation of an object from a single RGB\nmonocular camera.\nA final note: we originally assumed this object was Lambertian, and because of this used the relation that an object's per-\nceived area is its true area scaled by the cosine of the angle between the viewer/light source and the object, Does this apply for\nreal surfaces? No, because many are not ideal Lambertian surfaces. However, in practice, we can just use a lookup table that\ncan be calibrated using real images.\n4.4\nReferences\n1. RANSAC Algorithm: http://www.cse.yorku.ca/ kosta/CompVis Notes/ransac.pdf\n2. Bilinear Interpolation: https://en.wikipedia.org/wiki/Bilinear interpolation\n3. Bicubic Interpolation: https://en.wikipedia.org/wiki/Bicubic interpolation\n4. Multilateration: https://www.icao.int/Meetings/AMC/MA/2007/surv semi/Day02 SENSIS Turner.pdf\n5. Lambertian Surface: Robot Vision, Page 212\n6. Foreshortening: https://en.wikipedia.org/wiki/Perspective\nLecture 6: Photometric Stereo, Noise Gain, Error Amplification, Eigenval-\nues and Eigenvectors Review\nThis lecture covers an overview of some relevant concepts from linear algebra. If you would like a refresher on some of these\nconcepts, please check out Professor Horn's posted linear algebra review here.\n5.1\nApplications of Linear Algebra to Motion Estimation/Noise Gain\nRecall our problem of noise gain, in that if we observe a noisy estimate of y and we're asked to derive x, the noise from our\nobserved value can greatly affect our estimate of x. We've analyzed this problem in 1D, and now we're ready to generalize this\nto higher dimensions using eigenvalues and eigenvectors.\nFor the sake of brevity, the notes from the linear algebra review will not be given here, but they are in the handwritten\nnotes for this lecture, as well as in the linear algebra handout posted above. These notes cover:\n- Definitions of eigenvalues/eigenvectors\n- Characteristic Polynomials\n- Rewriting vectors using an eigenbasis\n5.1.1\nApplication Revisited: Photometric Stereo\nLet us now return to an application where we determine shape from shading: photometric stereo. Let us first consider the simple\ncase where the surface we seek to reconstruct is Lambertian:\nE ∝cos θi = ρˆn · ˆsi\nRecall the following, to help frame the setup of this problem:\n- For shape from shading problems, our goal is to estimate and reconstruct a 3D scene from a 2D image given information\nabout the shading and surface orientation of the scene.\n- In 2D, this shape from shading problem has 2 Degrees of Freedom (abbreviated in these course notes as DOF).\n- Recall that we can use albedo (ρ) to actually make this problem easier - can create a problem with 3 unknowns and 3\nequations. Without the use of albedo, recall that we had a quadratic constraint we had to follow, which, by Bezout's\ntheorem, suggests that there are at two solutions to such a system of equations. In this case, we have 3 linear equations\nand 3 unknowns, so by Bezout's theorem we have only one solution, as desired.\n\nRecall that for such a system, suppose we have three brightness measurements of the form:\n1. E1 = ρˆn · ˆs1 - Intuitively, look at pixel with light source located at s1 and take measurement E1.\n2. E2 = ρˆn · ˆs2 - Intuitively, look at pixel with light source located at s2 and take measurement E2.\n3. E3 = ρˆn · ˆs3 - Intuitively, look at pixel with light source located at s3 and take measurement E3.\nLet's review the linear algebraic system of equations by combining the equations above into a matrix-vector product.\n\n-sT\n-sT\n-sT\n\nn =\n\nE1\nE2\nE3\n\nNote the following:\n- Since the matrix on the left-hand side corresponds to the three-dimensional position of the light source in the given frame\nof reference, we have:\n\n-sT\n-sT\n-sT\n\n∈R3×3,\n\nE1\nE2\nE3\n\n∈R3\n- n = ρˆn, i.e. we do not need to deal with the second order constraint ˆnT ˆn = 1. This eliminates the second-order constraint\nfrom our set of equations and ensures we are able to derive a unique solutions by solving a system of only linear equations.\n- Define S\n∆=\n\n-sT\n-sT\n-sT\n\nLike many other linear system of equations we encounter of the form Ax = b, we typically want to solve for x. In this case, we\nwant to solve for n, which provides information about the surface orientation of our object of interest:\nSn = E -→n = S-1E\nLike the other \"inverse\" problems we have solved so far in this class, we need to determine when this problem is ill-posed, i.e.\nwhen S is not invertible. Recall from linear algebra that this occurs when the columns of S are not linearly independent/the\nrank of S is not full.\nAn example of when this problem is ill-posed is when the light source vectors are coplanar with one another, i.e.\ns1 + s2 + s3 = 0 (Note that this can be verified by simply computing the vector sum of any three coplanar vectors.)\nTherefore, for optimal performance and to guarantee that S is invertible:\n- We make the vectors from the light sources to the objects (si) as non-coplanar as possible.\n- The best configuration for this problem is to have the vectors from the light sources to the surface be orthogonal to one\nanother. Intuitively, consider that this configuration captures the most variation in the angle between a given surface\nnormal and three light sources.\nExample: Determining Depth of Craters on the Moon:\nAs it turns out, we cannot simply image the moon directly, since the plane between the earth/moon and the sun makes it\nsuch that all our vectors si will lie in the same plane/are coplanar. Thus, we cannot observe the moon's tomography from the\nsurface of the earth.\n5.2\nLambertian Objects and Brightness\nFun Fact: \"Lambert's Law\" was derived from a monk observing light passing through an oil spot in a piece of paper.\n\"Lambert's Law\":\nEi ∝cos θi = ˆn · ˆsi\nTo better understand this law, let us talk more about surface orientation.\n\n5.2.1\nSurface Orientation\n- For an extended surface, we can take small patches of the surface that are \"approximately planar\", which results in us\nconstraining the surface to be a 2-manifold [1] (technically, the language here is \"locally planar\").\n- Using these \"approximately planar\" patches of the surface results in many unit normal vectors ˆni pointing in a variety of\ndifferent directions for different patches in the surface.\n- We can also understand surface orientation from a calculus/Taylor Series point of view. Consider z(x + δx, y + δy), i.e. an\nestimate for the surface height at (x, y) perturbed slightly by a small value δ. The Taylor Series expansion of this is given\nby:\nz(x + δx, y + δy) = z(x, y) + δx ∂z\n∂x + δy ∂z\n∂y + · · · = z(x, y) + pδx + qδy\nWhere p\n∆= ∂z\n∂x, q\n∆= ∂z\n∂y\n- The surface orientation (p\n∆= ∂z\n∂x, q\n∆= ∂z\n∂y) captures the gradient of the height of the surface z.\n- Note that the surface unit normal ˆn is perpendicular to any two lines in the surface, e.g. tangent lines.\nWe can use the cross product of the two following tangent lines of this surface to compute the the unit surface normal, which\ndescribes the orientation of the surface:\n1. (δx, 0, pδx)T = δx(1, 0, p)T\n2. (0, δy, qδx)T = δy(0, 1, q)T\nSince we seek to find the unit surface normal, we can remove the scaling factors out front (δx and δy). Then the cross product\nof these two vectors is:\n(1, 0, p)T × (0, 1, p)T = det\n\nˆx\nˆy\nˆz\np\nq\n\n= n =\n\n-p\n-q\n\nWe can now compute the unit surface normal by normalizing the vector n:\nˆn =\nn\n||n||2\n=\n-p\n-q\n1T\np\np2 + q2 + 1\nWith this surface normal computed, we can also go the opposite way to extract the individual components of the surface\norientation (p and q):\np = -n · ˆx\nn · ˆz , q = -n · ˆy\nn · ˆz\nNote: Do the above equations look similar to something we have encountered before? Do they look at all like the coordinate-wise\nperspective projection equations?\nWe will leverage these (p, q)-space plots a lot, using reflectance maps (which we will discuss later).\nNow, instead of plot-\nting in velocity space, we are plotting in surface orientation/spatial derivative space. Individual points in this plane correspond\nto different surface orientations of the surface.\n5.2.2\nSurface Orientation Isophotes/Reflectance Maps for Lambertian Surfaces\nRecall the equation to derive isophotes of a Lambertian surface (since the brightness depends on the incident angle between the\nsurface and the light source):\nˆn · ˆs = E1 Where E1 is a constant, scalar level of brightness\nIf we expand our light source vector ˆs, we get the following for this dot product:\nˆn · ˆs =\n\n-p\n-q\nT\np\np2 + q2 + 1\n·\n\n-ps\n-qs\nT\np\np2s + q2s + 1\n= E1\n\nCarrying out this dot product and squaring each side, we can derive a parametric form of these isophotes in (p, q) space (note\nthat the light source orientation (ps, qs) is fixed (at least for a single measurement), and thus we can treat it as constant:\n(1 + psp + qsq)2 = c2(1 + p2 + q2)(1 + p2\ns + q2\ns)\nIf we plot these isophotes in (p, q) space, we will see they become conic sections. Different isophotes will generate different\ncurves. When we have multiple light sources, we can have intersections between these isophotes, which indicate solutions.\n5.3\nReferences\n[1] Manifolds, https://en.wikipedia.org/wiki/Manifold.\nLecture 7: Gradient Space, Reflectance Map, Image Irradiance Equation,\nGnomonic Projection\nThis lecture continues our treatment of the photometric stereo problem, and how we can estimate motion graphically by finding\nintersections of curves in (p, q) (spatial derivative) space. Following this, we discuss concepts from photometry and introduce\nthe use of lenses.\n6.1\nSurface Orientation Estimation (Cont.) & Reflectance Maps\nLet's review a couple of concepts from the previous lecture:\n- Recall our spatial gradient space given by (p, q) = ( ∂z\n∂x, ∂z\n∂y).\n- We also discussed Lambertian surfaces, or surfaces in which the viewing angle doesn't change brightness, only the angle\nbetween the light source and the object affects the brightness. Additionally, the relation between the source/object angle\nis proportional to: E ∝cos θi, where θi is the angle between the light source and the object.\n- Recall our derivation of the isophotes of Lambertian surfaces:\nˆn · ˆs =\n-p\n-q\n1T\np\np2 + q2 + 1\n·\n-ps\n-qs\n1T\np\np2s + q2s + 1\n= c\nWhere c is a constant brightness level. Carrying out this dot product:\n1 + psp + qsq\np\np2 + q2 + 1\np\np2s + q2s + 1\n= c\nWe can generate plots of these isophotes in surface orientation/(p, q) space. This is helpful in computer graphics, for\ninstance, because it allows us to find not only surface heights, but also the local spatial orientation of the surface. In\npractice, we can use lookup tables for these reflectance maps to find different orientations based offof a measured brightness.\n6.1.1\nForward and Inverse Problems\nAs discussed previously, there are two problems of interest that relate surface orientation to brightness measurements:\n1. Forward Problem: (p, q) →E (Brightness from surface orientation)\n2. Inverse Problem: E →(p, q) (Surface orientation from brightness)\nIn most settings, the second problem is of stronger interest, namely because it is oftentimes much easier to measure brightness\nthan to directly measure surface orientation. To measure the surface orientation, we need many pieces of information about the\nlocal geometries of the solid, at perhaps many scales, and in practice this is hard to implement. But it is straightforward to\nsimply find different ways to illuminate the object!\nA few notes about solving this inverse problem:\n- We can use the reflectance maps to find intersections (either curves or points) between isophotes from different light source\norientations. The orientations are therefore defined by the interesections of these isophote curves.\n- Intuitively, the reflectance map answers the question \"How bright will a given orientation be?\"\n\n- We can use an automated lookup table to help us solve this inverse problem, but depending on the discretization level of\nthe reflectance map, this can become tedious. Instead, wwe can use a calibration object, such as a sphere, instead.\n- For a given pixel, we can take pictures with different lighting conditions and repeatedly take images to infer the surface\norientation from the sets of brightness measurements.\n- This problem is known as photometric stereo: Photometric stereo is a technique in computer vision for estimating the\nsurface normals of objects by observing that object under different lighting conditions [1].\n6.1.2\nReflectance Map Example: Determining the Surface Normals of a Sphere\nA few terms to set before we solve for p and q:\n- Center of the sphere being imaged: (x0, y0, z0)T\n- Location of the light source: (x, y, z)T\nThen:\n- z -z0 =\np\nr2\n0 -(x -x0)2 + (y -y0)2\n- p = x-x0\nz-z0\n- a = y-y0\nz-z0\n(Do the above equations carry a form similar to that of perspective projection?)\n6.1.3\nComputational Photometric Stereo\nThough the intersection of curves approach we used above can be used for finding solutions to surfaces with well-defined re-\nflectance maps, in general we seek to leverage approaches that are as general and robust as possible. We can accomplish this, for\ninstance, by using a 3D reflectance map in (E1, E2, E3) space, where each Ei is a brightness measurement. Each discretized (unit\nof graphic information that defines a point in three-dimensional space [2]) contains a specific surface orientation parameterized\nby (p, q). This approach allows us to obtain surface orientation estimates by using brightness measurements along with a lookup\ntable (what is described above).\nWhile this approach has many advantages, it also has some drawbacks that we need to be mindful of:\n- Discretization levels often need to be made coarse, since the number of (p, q) voxel cells scales as a cubic in 3D, i.e.\nf(d) ∈Ospace(d3), where d is the discretization level.\n- Not all voxels may be filled in - i.e. mapping (let us denote this as φ) from 3D to 2D means that some kind of surface will\nbe created, and therefore the (E1, E2, E3) space may be sparse.\nOne way we can solve this latter problem is to reincorporate albedo ρ! Recall that we leveraged albedo to transform a system\nwith a quadratic constraint into a system of all linear equations. Now, we have a mapping from 3D space to 3D space:\nφ : (E1, E2, E3) -→(p, q, ρ) i.e. φ : R3 -→R3\nIt is worth noting, however, that even with this approach, that there will still exist brightness triples (E1, E2, E3) that do not\nmap to any (p, q, ρ) triples. This could happen, for instance, when we have extreme brightness from measurement noise.\nAlternatively, an easy way to ensure that this map φ is injective is to use the reverse map (we can denote this by ψ):\nψ : (p, q, ρ) -→(E1, E2, E3) i.e. ψ : R3 -→R3\n6.2\nPhotometry & Radiometry\nNext, we will rigorously cover some concepts in photometry and radiommetry. Let us begin by rigorously defining these two\nfields:\n- Photometry: The science of the measurement of light, in terms of its perceived brightness to the human eye [3].\n- Radiometry: The science of measurement of radiant energy (including light) in terms of absolute power [3].\nNext, we will cover some concepts that form the cornerstone of these fields:\n\n- Irradiance: This quantity concerns with power emitted/reflected by the object when light from a light source is incident\nupon it. Note that for many objects/mediums, the radiation reflection is not isotropic - i.e. it is not emitted equally in\ndifferent directions.\nIrradiance is given by: E = δP\nδA (Power/unit area, W/m2)\n- Intensity: Intensity can be thought of as the power per unit angle. In this case, we make use of solid angles, which have\nunits of Steradians. Solid angles are a measure of the amount of the field of view from some particular point that a given\nobject covers [4]. We can compute a solid angle Ωusing: Ω= Asurface\nR2\n.\nIntensity is given by: Intensity = δP\nδΩ.\nRadiance: Oftentimes, we are only interested in the power reflected from an object that actually reaches the light\nsensor. For this, we have radiance, which we contrast with irradiance defined above. We can use radiance and irradiance\nto contrast the \"brightness\" of a scene and the \"brightness\" of the scene perceived in the image plane.\nRadiance is given by: L =\nδ2P\nδAδΩ\nThese quantities provide motivation for our next section: lenses. Until now, we have considered only pinhole cameras, which are\ninfinitisimally small, but to achieve nonzero irradiance in the image plane, we need to have lenses of finite size.\n6.3\nLenses\nLenses are used to achieve the same photometric properties of perspective projection as pinhole cameras, but do so using a finite\n\"pinhole\" size and thus by encountering a finite number of photons from light sources in scenes. One caveat with lenses that we\nwill cover in greater detail: lenses only work for a finite focal length f. From Gauss, we have that it is impossible to make a\nperfect lens, but we can come close.\n6.3.1\nThin Lenses - Introduction and Rules\nThin lenses are the first type of lens we consider. These are often made from glass spheres, and obey the following three rules:\n- Central rays (rays that pass through the center of the lens) are undeflected - this allows us to preserve perspective projection\nas we had for pinhole cameras.\n- The ray from the focal center emerges parallel to the optical axis.\n- Any parallel rays go through the focal center.\nThin lenses also obey the following law, which relates the focal centers (a and b) to the focal length of the lens.\na + 1\nb = 1\nf0\nThin lenses can also be thought of as analog computers, in that they enforce the aforementioned rules for all light rays, much\nlike a computer executes a specific program with outlined rules and conditions. Here are some tradeoffs/issues to be aware of\nfor thin lenses:\n- Radial distortion: In order to bring the entire angle into an image (e.g. for wide-angle lenses), we have the \"squash\" the\nedges of the solid angle, thus leading to distortion that is radially-dependent. Typically, other lens defects are mitigated\nat the cost of increased radial distortion. Some specific kinds of radial distortion [5]:\n- Barrel distortion\n- Mustache distortion\n- Pincushion distortion\n- Lens Defects: These occur frequently when manufacturing lenses, and can originate from a multitude of different issues.\n\n6.3.2\nPutting it All Together: Image Irradiance from Object Irradiance\nHere we show how we can relate object irradiance (amount of radiation emitted from an object) to image plane irradiance of\nthat object (the amount of radiation emitted from an object that is incident on an image plane). To understand how these two\nquantities relate to each other, it is best to do so by thinking of this as a ratio of \"Power in / Power out\". We can compute this\nratio by matching solid angles:\nδI cos α\n(f sec α)2 = δO cos θ\n(z sec α)2 =⇒δI cos α\nf 2\n= δO\nz2\n=⇒δO\nδI = cos α\ncos θ\nz\nf\nThe equality on the far right-hand side can be interpreted as a unit ratio of \"power out\" to \"power in\". Next, we'll compute the\nsubtended solid angle Ω:\nΩ=\nπ\n4 d2 cos α\n(z sec α)2 = π\nd\nz\ncos3 α\nNext, we will look at a unit change of power (δP):\nδP = LδOΩcos θ = LδOπ\nd\nz )2 cos3 α cos θ\nWe can relate this quantity to the irradiance brightness in the image:\nE = δP\nδI = LδO\nδI\nπ\nd\nz\ncos3 α cos θ\n= π\n4 L\nd\nf\ncos4 α\nA few notes about this above expression:\n- The quantity on the left, E is the irradiance brightness in the image.\n- The quantity L on the righthand side is the radiance brightness of the world.\n- The ratio d\nf is the inverse of the so-called \"f-stop\", which describes how open the aperture is. Since these terms are squared,\nthey typically come in multiples of\n√\n2.\n- The reason why we can be sloppy about the word brightness (i.e. radiance vs. irradiance) is because the two quantities\nare proportional to one another: E ∝L.\n- When does the cos4 α term matter in the above equation? This term becomes non-negligible when we go off-axis from\nthe optical axis - e.g. when we have a wide-angle axis. Part of the magic of DSLR lenses is to compensate for this effect.\n- Radiance in the world is determined by illumination, orientation, and reflectance.\n6.3.3\nBRDF: Bidirectional Reflectance Distribution Function\nThe BRDF can be interpreted intuitively as a ratio of \"Energy In\" divided by \"Energy Out\" for a specific slice of incident and\nemitted angles. It depends on:\n- Incident and emitted azimuth angles, given by φi, φe, respectively.\n- Incident and emitted colatitude/polar angles, given by θi, θe, respectively.\nMathematically, the BRDF is given by:\nf(θi, θe, φi, φe) = δL(θe, φe)\nδE(θi, φi) =\n\"Energy In\"\n\"Energy Out\"\nLet's think about this ratio for a second. Please take note of the following as a mnemonic to remember which angles go where:\n- The emitted angles (between the object and the camera/viewer) are parameters for radiance (L), which makes sense\nbecause radiance is measured in the image plane.\n- The incident angles (between the light source and the object) are parameters for irradiance (E), which makes sense because\nirradiance is measured in the scene with the object.\n\nPractically, how do we measure this?\n- Put light source and camera in different positions\n- Note that we are exploring a 4D space comprised of [θi, θe, φi, φe], so search/discretization routines/sub-routines will be\ncomputationally-expensive.\n- This procedure is carried out via goniometers (angle measurement devices). Automation can be carried out by providing\nthese devices with mechanical trajectories and the ability to measure brightness. But we can also use multiple light sources!\n- Is the space of BRDF always in 4D? For most surfaces, what really matters are the difference in emitted and incident\nazimuth angles (φe -φi). This is applicable for materials in which you can rotate the surface in the plane normal to the\nsurface normal without changing the surface normal.\n- But this aforementioned dimensionality reduction procedure cannot always be done - materials for which there is directionally-\ndependent microstructures, e.g. hummingbird's neck, semi-precious stones, etc.\n6.3.4\nHelmholtz Reciprocity\nSomewhat formally, the Helmholtz reciprocity principle describes how a ray of light and its reverse ray encounter matched\noptical adventures, such as reflections, refractions, and absorptions in a passive medium, or at an interface [6]. It is a property\nof BRDF functions which mathematically states the following:\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nIn other words, if the ray were reversed, and the incident and emitted angles were consequently switched, we would obtain the\nsame BRDF value. A few concluding notes/comments on this topic:\n- If there is not Helmholtz reciprocity/symmetry between a ray and its inverse in a BRDF, then there must be energy\ntransfer. This is consistent with the 2nd Law of Thermodynamics.\n- Helmholtz reciprocity has good computational implications - since you have symmetry in your (possibly) 4D table across\nincident and emitted angles, you only need to collect and populate half of the entries in this table. Effectively, this is a\ntensor that is symmetric across some of its axes.\n6.4\nReferences\n1. Photometric Stereo, https://en.wikipedia.org/wiki/Photometric stereo\n2. Voxels, https://whatis.techtarget.com/definition/voxel\n3. Photometry, https://en.wikipedia.org/wiki/Photometry (optics)\n4. Solid Angles, https://en.wikipedia.org/wiki/Solid angle\n5. Distortion, https://en.wikipedia.org/wiki/Distortion (optics)\n6. Helmholtz Reciprocity, https://en.wikipedia.org/wiki/Helmholtz reciprocity\nLecture 8: Shape from Shading, Special Cases, Lunar Surface, Scanning\nElectron Microscope, Green's Theorem in Photometric Stereo\nIn this lecture, we will continue our discussion of photometry/radiometry, beginning with the concepts covered last lecture. We\nwill then explore Hapke surfaces and how they compare to Lambertian surfaces, and show how they can be used to solve our\nshape from shading problem. Finally, we will introduce two new types of lenses and some of their applications: \"thick\" lenses\nand telecentric lenses.\n\n7.1\nReview of Photometric and Radiometric Concepts\nLet us begin by reviewing a few key concepts from photometry and radiometry:\n- Photometry: Photometry is the science of measuring visible radiation, light, in units that are weighted according to the\nsensitivity of the human eye. It is a quantitative science based on a statistical model of the human visual perception of\nlight (eye sensitivity curve) under carefully controlled conditions [3].\n- Radiometry: Radiometry is the science of measuring radiation energy in any portion of the electromagnetic spectrum.\nIn practice, the term is usually limited to the measurement of ultraviolet (UV), visible (VIS), and infrared (IR) radiation\nusing optical instruments [3].\n- Irradiance: E\n∆= δP\nδA (W/m2). This corresponds to light falling on a surface. When imaging an object, irradiance is\nconverted to a grey level.\n- Intensity: I\n∆= δP\nδW (W/ster). This quantity applied to a point source and is often directionally-dependent.\n- Radiance: L\n∆=\nδ2P\nδAδΩ(W/m2× ster). This photometric quantity is a measure of how bright a surface appears in an image.\n- BRDF (Bidirectional Reflectance Distribution): f(θi, θe, φi, φe) = δL(θe,φe)\nδE(θi,φi) . This function captures the fact that\noftentimes, we are only interested in light hitting the camera, as opposed to the total amount of light emitted from an\nobject. Last time, we had the following equation to relate image irradiance with object/surface radiance:\nE = π\n4 L\nd\nf\ncos4 α\nWhere the irradiance of the image E is on the lefthand side and the radiance of the object/scene L is on the right. The\nBRDF must also satisfy Helmholtz reciprocity, otherwise we would be violating the 2nd Law of Thermodynamics.\nMathematically, recall that Helmholtz reciprocity is given by:\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nWith this, we are now ready to discuss our first type of surfaces: ideal Lambertian surfaces.\n7.2\nIdeal Lambertian Surfaces\nIdeal Lambertian surfaces are of interest because their sensed brightness (irradiance) only depends on the cosine of the incident\nangles (θi, φi). A few additional notes about these surfaces:\n- Ideal Lambertian surfaces are equally bright from all directions, i.e.\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nAND\nf(θi, θe, φi, φe) = K ∈R with respect to θe, φe, i.e.∂f(θi, θe, φi, φe)\n∂θe\n= ∂f(θi, θe, φi, φe)\n∂φe\n= 0\n- If the surface is ideal, the Lambertian surface reflects all incident light. We can use this to compute f. Suppose we take\na small slice of the light reflected by a Lambertian surface, i.e. a hemisphere for all positive z. The area of this surface is\ngiven by: sin θeδθeδφe, which is the determinant of the coordinate transformation from euclidean to spherical coordinates.\n\nThen, we have that:\nZ π\n-π\nZ\nπ\n(fE cos θi sin θedθe)dφe = E cos θi (Integrate all reflected light)\nE cos θi\nZ π\n-π\nZ\nπ\n(f sin θedθe)dφe = E cos θi (Factor out E cos θi)\nZ π\n-π\nZ\nπ\n(f sin θedθe)dφe = 1 (Cancel out on both sides)\n2π\nZ\nπ\nf sin θe cos θedθe = 1 (No dependence on φ)\nπf\nZ\nπ\n2 sin θe cos θedθe = 1 (Rearranging terms)\nπf\nZ\nπ\nsin(2θe)dθe = 1 (Using identity 2 sin θ cos θ = sin(2θ))\nπf[-1\n2 cos(2θe)]\nθe= π\nθe=0 = 1\nπf = 1 =⇒f = 1\nπ\n7.2.1\nForeshortening Effects in Lambertian Surfaces\nNow let us discuss a key issue in radiation reflectance and machine vision - foreshortening. Foreshortening is the visual effect\nor optical illusion that causes an object or distance to appear shorter than it actually is because it is angled toward the viewer\n[1]. Let us show how this is relevant for computing f.\nSuppose we have a point source radiating isotropically over the positive hemisphere in a 3-dimensional spherical coordinate\nsystem. Then, the solid angle spanned by this hemisphere is:\nΩ= Asurface\nr2\n= 2πr2\nr2\n= 2π Steradians\nIf the point source is radiating isotropically in all directions, then f =\n2π. But we saw above that f = 1\nπ for an ideal Lambertian\nsurface. Why do we have this discrepancy? Even if brightness is the same/radiation emitted is the same in all directions, this\ndoes not mean that the power radiated in all directions is hte same. This is due to foreshortening, since the angle between\nthe object's surface normal and the camera/viewer changes.\n7.2.2\nExample: Distant Lambertian Point Source\nHere, we will have that L = 1\nf Es (where s is the location of the light source). Since we have to take foreshortening into account,\nthe perceived area A of an object with world size A′ is given by:\nA = A′ cos θi\nTherefore, our expression for the the radiance (how bright the object appears in the image) is given by:\nL = 1\nπ Es = 1\nπ E0 cos θi\n7.3\nHapke Surfaces\nHapke surfaces are another type of surface we study in this course.\nFormally, a Hapke surface is an object which has re-\nflectance properties corresponding to Hapke parameters, which in turn are a set of parameters for an empirical model that\nis commonly used to describe the directional reflectance properties of the airless regolith surfaces of bodies in the solar system [2].\nThe BRDF of a Hapke surface is given by:\nf(θi, φi, θe, φe) =\n√cos θe cos θi\n\nWe can see that the Hapke BRDF also satisfies Helmholtz Reciprocity:\nf(θi, φi, θe, φe) =\n√cos θe cos θi\n=\n√cos θi cos θe\n= f(θe, φe, θi, φi)\nUsing the Hapke BRDF, we can use this to compute illumination (same as radiance, which is given by:\nL = E0 cos θif(θi, φi, θe, φe)\n= E0 cos θi\n√cos θe cos θi\n= E0\nr\ncos θi\ncos θe\nWhere L is the radiance/illumination of the object in the image, E0 cos θi is the irradiance of the object, accounting for fore-\nshortening effects, and\n√cos θi cos θe is the BRDF of our Hapke surface.\n7.3.1\nExample Application: Imaging the Lunar Surface\nWhat do the isophotes of the moon look like, supposing the moon can fall under some of the different types of surfaces we have\ndiscussed?\n- Lambertian: We will see circles and ellipses of isophotes, depending on the angle made between the viewer and the the\nmoon.\n- Hapke: Because of the BRDF behavior, isophotes will run longitudinally along the moon in the case in which it is a\nHapke surface. Recall isophotes are where the brightness of the object in the image, given by:\nr\ncos θi\ncos θe\n=\nv\nu\nu\nt\nˆn·ˆs\n||ˆn||2||ˆs||2\nˆn·ˆv\n||ˆn||2||ˆv||2\n= c =⇒\nˆn · ˆs\nˆn · ˆv = c2 for some c ∈R\nWhere ˆn is the surface normal vector of the object being imaged, ˆs is the unit vector describing the position of the light\nsource, and ˆv is the unit vector describing the position of the vertical. We can derive a relationship between ˆn and ˆs:\nˆn · ˆs = cˆn · ˆv =⇒ˆn · (ˆs -cˆv) = 0 =⇒ˆn ⊥(ˆs -cˆv)\nNext, we will use spherical coordinates to show how the isophotes of this surface will be longitudinal:\nˆn =\n\nsin θ cos φ\nsin θ sin φ\ncos θ\n\n∈R3\nSince normal vectors ˆn ∈R3 only have two degrees of freedom (since ||ˆn||2 = 1), we can fully parameterize these two degrees\nof freedom by the azimuth and colatitude/polar angles φ and θ, respectively. Then we can express our coordinate system\ngiven by the orthogonal basis vectors ˆn,ˆs, ˆv as:\nˆn =\nsin θ cos φ\nsin θ sin φ\ncos θT\nˆs =\ncos θs\nsin θs\n0T\nˆv =\ncos θv\nsin θv\n0T\nWe can use our orthogonal derivations from above to now show our longitudinal isophotes:\nr\ncos θi\ncos θe\n= ˆn · ˆs\nˆn · ˆv = sin θ cos φ cos φs + sin θ sin φ sin φs\nsin θ cos φ cos φv + sin θ sin φ sin φv\n= cos(φ -φs)\ncos(φ -φv) = c for some c ∈R\nFrom the righthand side above, we can deduce that isophotes for Hapke surfaces correspond to points with the same\nazimuth having the same brightness, i.e. the isophotes of the imaged object (such as the moon) are along lines of constant\nlongitude.\n\n7.3.2\nSurface Orientation and Reflectance Maps of Hapke Surfaces\nNext, we will take another look at our \"shape from shading\" problem, but this time using the surface normals of Hapke surfaces\nand relating this back to reflectance maps and our (p, q) space. From our previous derivations, we already have that the BRDF\nof a Hapke surface is nothing more than the square root of the dot product of the surface normal with the light source vector\ndivided by the dot product of the surface normal with the vertical vector, i.e.\nr\ncos θi\ncos θe\n=\nr\nˆn · ˆs\nˆn · ˆv\nNext, if we rewrite our unit vectors as 3D vectors in (p, q) space, we have:\nˆn =\n-p\n-q\n1T\np\np2 + q2 + 1\nˆs =\n-ps\n-qs\n1T\np\np2s + q2s + 1\nˆv =\n1T\nThen, substituting these definitions, we have:\nˆn · ˆv =\np\n1 + p2 + q2\nˆn · ˆs =\n1 + psp + qsq\np\n1 + p2 + q2p\n1 + p2s + q2s\nˆn · ˆs\nˆn · ˆv =\n1+psp+qsq\n√\n1+p2+q2√\n1+p2\ns+q2\ns\n√\n1+p2+q2\n= 1 + psp + qsq\np\n1 + p2s + q2s\n= 1 + psP + qsq\nRs\n(Where Rs\n∆=\np\n1 + p2s + q2s.) Note that ps and qs are constants, since they only reflect the spatial derivatives of the light source.\nThis ratio of dot products is therefore linear in (p, q) space! Then, what do the isophotes look like in gradient space? Recall\nthat isophotes take the form:\nr\nˆn · ˆs\nˆn · ˆv = c =⇒\nˆn · ˆs\nˆn · ˆv = 1 + psP + qsq\nRs\n= c2 for some c ∈R .\nTherefore, isophotes of Hapke surfaces are linear in spatial gradient space! This is very useful for building reflectance maps\nbecause, as we will see shortly, a simple change of coordinate system in (p, q) space tells us a lot about the surface orientation\nin one of the two directions.\nFor one thing, let us use our linear isophotes in gradient space to solve our photometric stereo problem, in this case with\ntwo measurements under different lighting conditions. Photometric stereo is substantially easier with Hapke surfaces than with\nLambertian, because there is no ambiguity in where the solutions lie. Unlike Lambertian surfaces, because of the linearity in\n(p, q) space we are guaranteed by Bezout's Theorem to have only one unique solution.\n7.3.3\nRotated (p′, q′) Coordinate Systems for Brightness Measurements\nWe can also derive/obtain a surface orientation in gradient space in a rotated coordinate system in gradient space, i.e. from\n(p, q) -→(p′, q′).\nWe can prove (see the synchronous lecture notes for this part of the course) that the transforming the points in (x, y) via\n\na rotation matrix R is equivalent to rotating the gradient space (p, q) by the same matrix R. I.e.\nR : (x, y) =⇒(x′, y′),\nx′\ny′\n\n= R\nx\ny\n\nR : (p, q) =⇒(p′, q′),\np′\nq′\n\n= R\np\nq\n\nWhere R is given according to:\nR =\ncos α\n-sin α\nsin α\ncos α\n\n∈SO(2)\n(Where SO(2) is the Special Orthogonal Group [4].) Note that R-1 = RT since R is orthogonal and symmetric.\nBy rotating our coordinate system from (p, q) -→(p′, q′), we are able to uniquely specify p′, but not q′, since our isophotes lie\nalong a multitude/many q′ values. Note that this rotation system is specified by having p′ lying along the gradient of our isophotes.\nReturning to our Lunar Surface application with Hapke surfaces, we can use this surface orientation estimation framework\nto take an iterative, incremental approach to get a profile of the lunar surface. This enables us to do depth profile shaping\nof the moon simply from brightness estimates! There are a few caveats to be mindful of for this methodology, however:\n- We can only recover absolute depth values provided we are given initial conditions (this makes sense, since we are effectively\nsolving a differential equation in estimating the depth z from the surface gradient (p, q)T ). Even without initial conditions,\nhowever, we can still recover the general shape of profiling, simply without the absolute depth.\n- Additionally, we can get profiles for each cross-section of the moon/object we are imaging, but it is important to recognize\nthat these profiles are effectively independent of one another, i.e. we do not recover any information about the relative\nsurface orientation changes between/at the interfaces of different profiles we image. We can use heuristic-based approaches\nto get a topographic mapping estimate. We can then combine these stitched profiles together into a 3D surface.\n7.4\n\"Thick\" & Telecentric Lenses\nWe will now shift gears somewhat to revisit lenses, namely \"thick\" and telecentric lenses.\n7.4.1\n\"Thick\" Lenses\nIn practice, lenses are not thick, and are typically cascaded together to mitigate the specific aberrations and wavelength depen-\ndence of each individual lens. For thick lenses, we will leverage the following definitions:\n- Nodal/principal points: The center points of the outermost (ending and beginning lenses).\n- Principal planes: The distance from the object to the first imaging lens and the distance from the last imaging lens to\nthe viewer or imaging plane. Abbreviated by b.\n- Focal length: This definition is the same as before, but in this case, the focal length is the distance from the first imaging\nlens to the last imaging lens along the optical axis (i.e. orthogonal to the radial unit vectors of each lens in the \"thick\"\nlens).\nFrom this, we have the formula:\na + 1\nb = 1\nf\nOne \"trick\" we can do with this is achieve a long focal length f and a small field of view (FOV), i.e. as is done in a telephoto\nlens.\n7.4.2\nTelecentric Lenses\nThese types of lenses are frequently used in machine vision, such as for circuit board inspection or barcode reading.\nMotivation for these types of lenses: When we have perspective projection, the degree of magnification depends on dis-\ntance. How can we remove this unwanted distance-dependent magnification due to perspective projection? We can do so by\n\neffectively \"moving\" the Center of Projection (COP) to \"-inf\". This requires using a lens with a lot of glass.\nA few notes about this:\n- To image with magnifying effects, the lens must be at least as wide in diameter as the width of the object.\n- a A double telecentric lens can be created by moving the other \"modal point\" (the imaging plane/viewer) to \"+inf\". In\nthis case, our magnifying term given by cos4 α goes to zero, because:\nlim\nCOP→+infα = 0\n- We can generalize this idea even more with \"lenselets\": These each concentrate light into a confined area, and can be used\nin an array fashion as we would with arrays of photodiodes. A useful term for these types of arrays is fill factor: The\nfill factor of an image sensor array is the ratio of a pixel's light sensitive area to its total area [5]. Lenselet arrays can be\nuseful because it helps us to avoid aliasing effects. We need to make sure that if we are sampling an object discretely,\nthat we do not have extremely high-frequency signals without some low pass filtering. This sort of low-pass filtering can be\nachieved with using large pixels and averaging (a crude form of lowpass filtering). However, this averaging scheme does not\nwork well when light comes in at off-90 degree angles. We want light to come into the sensors at near-90 degrees (DSLRs\nfind a way to get a around this).\n- Recall that for an object space telecentric device, we no longer have a distance-based dependence. Effectively, we are taking\nperspective projection and making the focal length larger, resulting in approximately orthographic projection.\nRecall that for orthographic projection, projection becomes nearly independent in position, i.e. (x, y) in image space has\na linear relationship with (X, Y ) in the world. This means that we can measure sizes of objects independently of how far\naway they are!\nStarting with perspective projection:\nx\nf = X\nZ , y\nf = Y\nZ\nHaving |∆Z| << |Z|, where∆Z is the variation in Z of an object\n=⇒x = f\nZ0\nX, y = f\nZ0\nY\nApproximating f\nZ0\n= 1 =⇒x = X, y = Y\n7.5\nReferences\n1. Foreshortening, https://en.wikipedia.org/wiki/Perspective_(graphical)\n2. Hapke Parameters, https://en.wikipedia.org/wiki/Hapke_parameters\n3. Understanding Radiance (Brightness), Irradiance and Radiant Flux\n4. Orthogonal Group, https://en.wikipedia.org/wiki/Orthogonal group\n5. https://en.wikipedia.org/wiki/Fill factor (image sensor)\nLecture 9: Shape from Shading, General Case - From First Order Nonlinear\nPDE to Five ODEs\nIn this lecture, we will begin by exploring some applications of magnification, shape recovery, and optics through Transmission\nand Scanning Electron Microscopes (TEMs and SEMs, respectively). Then, we will discuss how we can derive shape from shading\nusing needle diagrams, which capture surface orientations at each (x, y) pixel in the image. This procedure will motivate the use\nof Green's Theorem, \"computational molecules\", and a discrete approach to our standard unconstrained optimization problem.\nWe will conclude by discussing more about recovering shape from shading for Hapke surfaces using initial curves and rotated\ncoordinate systems.\n_______________________________________\n\n8.1\nExample Applications:\nTransmission and Scanning Electron Microscopes (TEMs and\nSEMs, respectively)\nWe will begin with a few motivating questions/observations:\n- How do TEMs achieve amazing magnification? They are able to do so due to the fact that these machines are not\nrestricted by the wavelength of the light they use for imaging (since they are active sensors, they image using their own\n\"light\", in this case electrons.\n- What are SEM images more enjoyable to look at than TEM images? This is because SEM images reflect shading,\ni.e. differences in brightness based offof surface orientation. TEM images do not do this.\n- How do SEMs work? Rely on an electron source/beam, magnetic-based scanning mechanisms, photodiode sensors to\nmeasure secondary electron current. Specifically:\n- Many electrons lose energy and create secondary electrons. Secondary electrons are what allow us to make measure-\nments.\n- Secondary electron currents vary with surface orientation.\n- Objects can be scanned in a raster-like format.\n- Electron current is used to modulate a light ray. Magnification is determined by the degree of deflection.\n- Gold plating is typically used to ensure object is conductive in a vacuum.\n- Inclines/angles can be used for perturbing/measuring different brightness values.\n- From a reflectance map perspective, measuring brightness gives is the slope (a scalar), but it does not give us the\ngradient (a vector). This is akin to knowing speed, but not the velocity.\n8.2\nShape from Shading: Needle Diagram to Shape\nThis is another class of problems from the overarching \"Shape from Shading\" problem. Let us first begin by defining what a\nneedle diagram is:\nA needle diagram is a 2D representation of the surface orientation of an object for every pixel in an image, i.e.\nfor ev-\nery (x, y) pair, we have a surface orientation (p, q), where p\n∆= dz\ndt , q\n∆= dz\ndy. Recall from photometric stereo that we cannot simply\nparameterize Z(x, y); we can only parameterize the surface gradients p(x, y) and q(x, y).\nIn this problem, our goal is that given (p, q) for each pixel (i.e. given the needle diagram), recover z for each pixel. Note\nthat this leads to an overdetermined problem (more constraints/equations than unknowns) [1]. This actually will allow us to\nreduce noise and achieve better results.\nFor estimating z, we have:\nx : z(x) = z(0) +\nZ x\npdx′\ny : z(y) = z(0) +\nZ y\nqdy′\nx&y : z(x, y) = z(0, 0) +\nZ\npdx′ + qdy′\nLet us define δx′ = pdx′ + qdy. Next, we construct a contour in the (x, y) plane of our (p, q) measurements, where the contour\nstarts and ends at the origin, and passes through a measurement. Our goal is to have the integrals of p and q be zero over these\ncontours, i.e.\nI\n(pdx′ + qdy′) = 0\nThis is equivalent to \"z being conserved\" over the contour.\nBut note that these measurements are noisy, and since we estimate p and q to obtain estimates for z, this is not necessar-\nily true.\n\nNote that an easy way to break this problem down from one large problem into many smaller problems (e.g. for computa-\ntional parallelization, greater accuracy, etc.) is to decompose larger contours into smaller ones - if z is conserved for a series of\nsmaller loops, then this implies z is conserved for the large loop as well.\n8.2.1\nDerivation with Taylor Series Expansion\nLet us now suppose we have a point (x, y), centered around a square loop with lengths equal to δ. Then, applying the formula\nabove, we have that:\np\n\nx, y -δy\n\nδx + q\n\nx + δx\n2 , y\n\nδy -p\n\nx, y + δy\n\nδx -q\n\nx -δx\n2 , y\n\nδy = 0\nIf we now take the first-order Taylor Series Expansion of this equation above, we have:\nExpansion :\n\np(x, y) -δy\n∂p(x, y)\n∂y\n\nδx +\n\nq(x, y) + δx\n∂q(x, y)\n∂x\n\nδy -\n\np(x, y) + δy\n∂p(x, y)\n∂y\n\nδx -\n\nq(x, y) -δx\n∂q(x, y)\n∂x\n\nδy = 0\nRewriting :\np(x, y)δx -δyδx\n∂p(x, y)\n∂y\n+ q(x, y)δy + δxδy\n∂q(x, y)\n∂x\n= p(x, y)δx + δyδx\n∂p(x, y)\n∂y\n+ q(x, y)δy -δxδy\n∂q(x, y)\n∂x\nSimplifying :\nδyδx∂p(x, y)\n∂y\n= δxδy ∂q(x, y)\n∂x\nSolution :\n∂p(x, y)\n∂y\n= ∂q(x, y)\n∂x\nThis is consistent with theory, because since our parameters p ≈∂z\n∂x and q ≈∂z\n∂y, then the condition approximately becomes\n(under perfect measurements):\n∂p(x, y)\n∂y\n= ∂\n∂y ( ∂z\n∂x) = ∂2z\n∂y∂x\n∂q(x, y)\n∂x\n= ∂\n∂x(∂z\n∂y ) = ∂2z\n∂x∂y\n∂p(x, y)\n∂y\n= ∂q(x, y)\n∂x\n=⇒\n∂2z\n∂y∂x = ∂2z\n∂x∂y , Which is consistent with Fubini's Theorem as well [2].\nThough this will not be the case in practice we are measuring p & q, and thus we will encounter some measurement noise.\n8.2.2\nDerivation with Green's Theorem\nWe can now show the same result via Green's Theorem, which relates contour integrals to area integrals:\nI\nL\n(Ldx + Mdy) =\nZZ\nD\n∂M\n∂x -∂L\n∂y\n\ndxdy\nWhere the term Ldx+Mdy on the lefthand side is along the boundary of the contour of interest, and the term ∂M\n∂x -∂L\n∂y is along\nthe interior of the contour.\nGreen's Theorem is highly applicable in machine vision because we can reduce two-dimensional computations to one-dimensional\ncomputations. For instance, Green's Theorem can be helpful for:\n- Computing the area of a contoured object/shape\n- Computing the centroid of a blob or object in two-dimensional space, or more generally, geometric moments of a surface.\nMoments can generally be computed just by going around the boundary of a contour.\n\nLet us now apply Green's Theorem to our problem:\nI\nL\n(pdx + qdy) =\nZZ\nD\n∂q(x, y)\n∂x\n-∂p(x, y)\n∂y\n\ndxdy = 0\nThis requires ∂q(x,y)\n∂x\n-∂p(x,y)\n∂y\n= 0 =⇒\n∂q(x,y)\n∂x\n= ∂p(x,y)\n∂y\n∀x, y ∈D.\nWe could solve for estimates of our unknowns of interest, p and q, using unconstrained optimization, but this will be more\ndifficult than before. Let us try using a different tactic, which we will call \"Brute Force Least Squares\":\nmin\nz(x,y)\nZZ\nD\n∂z\n∂x -p\n+\n∂z\n∂y -q\ndxdy\nI.e. we are minimizing the squared distance between the partial derivatives of z with respect to x and y and our respective\nparameters over the entire image domain D.\nHowever, this minimization approach requires having a finite number of variables, but here we are optimizing over a continuous\nfunction (which has an infinite number of variables). Therefore, we have infinite degrees of freedom. We can use calculus of\nvariations here to help us with this. Let us try solving this as a discrete problem first.\n8.3\nShape with Discrete Optimization\nFor this, let us first take a grid of unknowns {zij}(i,j)∈D. Our goal is to minimize the errors of spatial derivatives of z with\nrespect to p and q, our measurements in this case (given by {pij, qij}(i,j)∈D. Our objective for this can then be written as:\nmin\n{zij}\nX\ni\nX\nj\nzi,j+1 -zi,j\nε\n-pi,j\n+\nzi+1,j -zi,j\nε\n-qi,j\nNote that these discrete derivatives of z with respect to x and y present in the equation above use finite forward differences.\nEven though we are solving this discretely, we can still think of this as solving our other unconstrained optimization prob-\nlems, and therefore can do so by taking the first-order conditions of each of our unknowns, i.e.\n∀(k, l) ∈D. The FOCs are\ngiven by |D| equations (these will actually be linear!):\n∂\n∂zk,l\n(J({zi,j}(i,j)∈D) = 0 ∀(k, l) ∈D\nLet us take two specific FOCs and use them to write a partial differential equation:\n- (k, l) = (i, j):\n∂\n∂zk,l\n(J({z,j}(i,j)∈D) = 2\nε\nzk,l+1 -zk,l\nε\n-pk,l\n\n+ 2\nε\nzk+1,l -zk,l\nε\n-qk,l\n\n= 0\n- (k, l) = (i+1, j+1):\n∂\n∂zk,l\n(J({z,j}(i,j)∈D) = 2\nε\nzk,l -zk,l-1\nε\n-pk,l-1\n\n+ 2\nε\nzk,l -zk-1,l\nε\n-qk-1,l\n\n= 0\nGathering all terms (we can neglect the zs):\n(1) pk,l -pk,l-1\nε\n≈∂p\n∂x\n(2) qk,l -qk-1,l\nε\n≈∂q\n∂y\n(3)\nε2 ((-zk,l+1 -zk+1,l -zk,l-1 -zk-1,l) + 4zk,l) = 0 ≈-∆z = -∇2z The Laplacian of z\nWhere (1) + (2) + (3) = 0\nUsing the approximations of these three terms, our equation becomes:\n∂p\n∂x + ∂q\n∂y -∆z = 0 =⇒∂p\n∂x + ∂q\n∂y = ∆z\nThis approach motivates the use of \"computational molecules\".\n\n8.3.1\n\"Computational Molecules\"\nThese are computational molecules that use finite differences [3] to estimate first and higher-order derivatives. They can be\nthought of as filters, functions, and operators that can be applied to images or other multidimensional arrays capturing spatial\nstructural. Some of these are (please see the handwritten lecture notes for what these look like graphically):\n1. zx = 1\nε (z(x, y) -z(x -1, y)) (Backward Difference), 1\nε (z(x + 1, y) -z(x, y)) (Forward Difference)\n2. zy = 1\nε (z(x, y) -z(x, y -1)) (Backward Difference), 1\nε (z(x, y + 1) -z(x, y)) (Forward Difference)\n3. ∆z = ∇2z =\nε2 (4z(x, y) -(z(x -1, y) + z(x + 1, y) + z(x, y -1) + z(x, y + 1)))\n4. zxx = ∂2z\n∂x2 =\nε2 (z(x -1, y) -2(x, y) + z(x + 1, y))\n5. zyy = ∂2z\n∂y2 = 1\nε2 (z(x, y -1) -2(x, y) + z(x, y + 1))\nThese computational molecules extend to much higher powers as well. Let us visit the Laplacian operator ∆(·). This operator\ncomes up a lot in computer vision:\n- Definition: ∆z = ∇2z = ( ∂z\n∂x, ∂z\n∂y)T ( ∂z\n∂x, ∂z\n∂y) = ∂2z\n∂x2 + ∂2z\n∂y2\n- The Laplacian is the lowest dimensional rotationally-invariant linear operator, i.e. for a rotated coordinate system\n(x′, y′) rotated from (x, y) by some rotation matrix R ∈SO(2), we have:\nzx′x′ + zy′y′ = zxx + zyy\nI.e. the result of the Laplacian is the same in both coordinate systems.\nAs we can see, the Laplacian is quite useful in our derived solution above.\n8.3.2\nIterative Optimization Approach\nLet us return to our discrete optimization problem. Our derivation above is a least-squares solution. This turns out to be the\ndiscrete version of our original continuous problem. Since these first-order equations are linear, we can solve them as a system\nof linear equations with Gaussian elimination. But note that these processes take O(N 3) time. We can avoid this complexity by\ntaking advantage of the sparsity in these equations.\nIterative Approach: The sparse structure in our First-Order Equations allows us to use an iterative approach for shape\nestimation. Our \"update equation\" updates the current depth/shape estimate zk,l using its neighboring indices in two dimen-\nsions:\nz(n+1)\nk,l\n= 1\n4(z(n)\nk,l+1 + z(n)\nk+1,l + z(n)\nk,l-1 + z(n)\nk-1,l) -ε(pk,l -pk,l-1) -ε(qk,l -qk-1,l)\nA few terminology/phenomenological notes about this update equation:\n- The superscripts n and n + 1 denote the number of times a given indexed estimate has been updated (i.e. the number of\ntimes this update equation has been invoked). It is essentially the iteration number.\n- The subscripts k and l refer to the indices.\n- The first term on the righthand side 1\n4(·) is the local average of zk,l using its neighbors.\n- This iterative approach converges to the solution much more quickly than Gaussian elimination.\n- This iterative approach is also used in similar ways for solving problems in the Heat and Diffusion Equations (also\nPDEs).\n- This procedure can be parallelized so long as the computational molecules do not overlap/touch each other. For instance,\nwe could divide this into blocks of size 3 x 3 in order to achieve this.\n- From this approach, we can develop robust surface estimates!\n\n8.3.3\nReconstructing a Surface From a Single Image\nRecall this other shape from brightness problem we solved for Hapke surfaces (Lecture 8). For Hapke surfaces, we have that our\nbrightness in the image (radiance) L is given by:\nL =\nr\ncos θi\ncos θe\n=\nr\nˆn · ˆs\nˆn · ˆv\nRecall from our last lecture that this gives us a simple reflectance map of straight lines in gradient (p, q) space. By rotating this\ngradient space coordinate system from (p, q) →(p′, q′), we can simplify our estimates for shape.\nWith this rotation, we also claimed that rotating the system in gradient space is equivalent to using the same rotation ma-\ntrix R in our image space (x, y). Here we prove this:\nRotating Points :\nx′ = x cos θ -y sin θ, y′ = x sin θ + y cos θ\nReverse Rotating Points :\nx = x′ cos θ + y′ sin θ, y = -x′ sin θ + y′ cos θ\nTaking Derivatives :\n∂z\n∂x′ = ∂z\n∂x\n∂x\n∂x′ + ∂z\n∂y\n∂y\n∂x′ ,\n∂z\n∂y′ = ∂z\n∂x\n∂x\n∂y′ + ∂z\n∂y\n∂y\n∂y′\nCombining the Above : p′ = p cos θ -q sin θ, q′ = p sin θ + q cos θ\nThen, in our rotated coordinate system where p′ is along the brightness gradient, we have that:\np′ = psp + qsq\np\np2s + q2s\n= rsE2 -1\np\np2s + q2s\n(Where p′ ∆= ∂z\n∂x′ is the slope of the surface of interest in a particular direction. This phenomenon only holds for Hapke surfaces\nwith linear isophotes. We can integrate this expression out for surface estimation:\nz(x) = z(x0) +\nZ x0\nx\np′(x)dx\nIntegrating out as above allows us to build a surface height profile of our object of interest. Can do this for the y-direction as\nwell:\nz(x, y) = z(x0, y) +\nZ x0\nx\np′(x, y)dx\nA few notes from this, which we touched on in lecture 8 as well:\n- Adding a constant to z does not change our profile integrals, except by an offset. Therefore, in order to obtain absolute\nheight measurements of z, we need to include initial values.\n- In this case, we need an initial condition for every horizontal row/profile. This is the same as requiring an \"initial curve\",\nand allows us to effectively reduce our computations from 2D to 1D. Note from our previous lecture that these initial\nconditions are needed to determine the surface orientation/shape at interfaces between these different profiles.\n- Let us examine what this looks like when we parameterize an initial curve with η:\nTake x(η), y(η), z(η), and rotate with ξ\nThen we can compute δz to recover shape, along with δx and δy:\n1. δx =\nqs\n√\np2\ns+q2\ns δξ\n2. δy =\nps\n√\np2\ns+q2\ns δξ\n3. δz = [rsE2(x,y)-1]\n√\np2\ns+q2\ns\nδξ\nNote that we can adjust the speed of motion here by adjusting the parameter δ.\nNext time, we will generalize this from Hapke reflectance maps to arbitrary reflectance maps!\n\n8.4\nReferences\n1. Overdetermined System, https://en.wikipedia.org/wiki/Overdetermined system\n2. Fubini's Theorem, https://en.wikipedia.org/wiki/Fubini%27s theorem\n3. Finite Differences, https://en.wikipedia.org/wiki/Finite difference\nLecture 10: Characteristic Strip Expansion, Shape from Shading, Iterative\nSolutions\nIn this lecture, we will continue studying our shape from shading problem, this time generalizing it beyond Hapke surfaces,\nmaking this framework amenable for a range of machine vision tasks. We will introduce a set of ordinary differential equations,\nand illustrate how we can construct a linear dynamical system that allows us to iteratively estimate the depth profile of an object\nusing both position and surface orientation.\n9.1\nReview: Where We Are and Shape From Shading\nShape from Shading (SfS) is one of the class of problems we study in this course, and is part of a larger class of problems\nfocusing on machine vision from image projection. In the previous lecture, we solved shape from shading for Hapke surfaces\nusing the Image Irradiance Equation E(x, y) = R(p, q), where R(p, q) is the reflectance map that builds on the Bidirectional\nReflectance Distribution Function (BRDF).\nRecall additionally that irradiance (brightness in the image) depends on:\n1. Illumination\n2. Surface material of object being imaged\n3. Geometry of the object being imaged\nRecall from the previous lecture that for Hapke surfaces, we were able to solve the SfS problem in a particular direction, which\nwe could then determine object surface orientation along. We can integrate along this direction (the profile direction) to find\nheight z, but recall that we gain no information from the other direction.\nFor the Hapke example, we have a rotated coordinate system governed by the following ODEs (note that xi will be the variable\nwe parameterize our profiles along):\n1. X-direction:\ndx\ndξ = ps\n2. Y-direction:\ndy\ndξ = qs\n3. By chain rule, Z-direction:\ndz\ndξ = ∂z\n∂x\ndx\ndξ + ∂z\n∂y\ndy\ndξ = pps + qqs = psp + qsq\nIntuition: Infinitesimal steps in the image given by ξ gives dx\ndξ , dy\ndξ , and we are interested in finding the change in height dz\ndξ ,\nwhich can be used for recovering surface orientation.\nNote: When dealing with brightness problems, e.g. SfS, we have implicitly shifted to orthographic projection (x =\nf\nZ0 X, y =\nf\nZ0 y). These methods can be applied to perspective projection as well, but the mathematics makes the intuition less clear. We\ncan model orthographic projection by having a telecentric lens, which effectively places the object really far away from the\nimage plane.\nWe can solve the 3 ODEs above using a forward Euler method with a given step size. For small steps, this approach will be\naccurate enough, and accuracy here is not too important anyway since we will have noise in our brightness measurements.\nRecall brightness of Hapke surfaces is given by:\nE =\nr\nˆn · ˆs\nˆn · ˆv =\nr\ncos θi\ncos θe\n=\nfrac1 + psp + qsq\np\n1 + p2 + q2rs ·\n√1 + psp + qsq\n\nSome cancellation and rearranging yields:\npsp + qsq = rsE2 -1\nTherefore, we have a direct relationship between our measured brightness E and our unknowns of interest:\n∂z\n∂xps + ∂z\n∂y qs = pps + qqs = rsE2 -1\nNote here, however, that we do not know surface orientation based on this, since again for Hapke surfaces, we only know slope\nin one of our two directions in our rotated gradient space (p′, q′). A forward Euler approach will generate a set of independent\nprofiles, and we do not have any information about the surface orientation at the interfaces of these independent profiles. This\nnecessitates an initial curve containing initial conditions for each of these independent profiles. In 3D, this initial curve is\nparameterized by η, and is given by: (x(η), y(η), z(η)). Our goal is to find z(η, ξ), where:\n- η parameterizes the length of the initial curve\n- ξ parameterizes along the 1D profiles we carve out\nTherefore, we are able to determine the slope in a particular direction.\n9.2\nGeneral Case\nLet us now generalize our Hapke case above to be any surface. We begin with our Image Irradiance Equation:\nE(x, y) = R(p, q)\nLet us now consider taking a small step δx, δy in the image, and our goal is to determine how z changes from this small step.\nWe can do so using differentials and surface orientation:\nδz = ∂z\n∂xδx + ∂z\n∂y δy = pδx + qδy\nTherefore, if we know p and q, we can compute δz for a given δx and δy. But in addition to updating z using the equation\nabove, we will also need to update p and q (intuitively, since we are still moving, the surface orientation can change):\nδp = ∂p\n∂xδx + ∂p\n∂y δy\nδq = ∂q\n∂xδx + ∂q\n∂y δy\nThis notion of updating p and q provides motivation for keeping track/updating not only (x, y, z), but also p and q. We can\nthink of solving our problem as constructing a characteristic strip of the ODEs above, composed of (x, y, z, p, q) ∈R5. In\nvector-matrix form, our updates to p and q become:\n\nδp\nδq\n\n=\n\nr\ns\ns\nt\n\nδx\nδy\n\n=\n\n∂2z\n∂x2\n∂2z\n∂y∂x\n∂2z\n∂x∂y\n∂2z\n∂y2\n\nδx\nδy\n\n=\n\npx\npy\nqx\nqy\n\nδx\nδy\n\n= H\nδx\nδy\n\nWhere H is the Hessian of second spatial derivatives (x and y) of z. Note that from Fubuni's theorem and from our Taylor\nexpansion from last lecture, py = qx.\nIntuition:\n- First spatial derivatives ∂z\n∂x and ∂z\n∂y describe the surface orientation of an oject in the image.\n- Second spatial derivatives ∂2z\n∂x2 , ∂2z\n∂y2 , and\n∂2z\n∂x∂y =\n∂2z\n∂y∂x describe curvature, or the change in surface orientation.\n- In 2D, the curvature of a surface is specified by the radius of curvature, or its inverse, curvature.\n- In 3D, however, the curvature of a surface is specified by a Hessian matrix H. This curvature matrix allows us to compute\np and q.\n\nOne issue with using this Hessian approach to update p and q: how do we update the second derivatives r, s, and t? Can we use\n3rd order derivatives? It turns out that seeking to update lower-order derivatives with higher-order derivatives will just generate\nincreasingly more unknowns, so we will seek alternative routes. Let's try integrating our brightness measurements into what we\nalready have so far:\nImage Irradiance Equation :\nE(x, y) = R(p, q)\nChain Rule Gives :\nEx = ∂E\n∂x = ∂R\n∂p\n∂p\n∂x + ∂R\n∂q\n∂q\n∂x\n= ∂R\n∂p r + ∂R\n∂q s\nEy = ∂E\n∂y = ∂R\n∂p\n∂p\n∂y + ∂R\n∂q\n∂q\n∂y\n= ∂R\n∂p s + ∂R\n∂q t\nIn matrix-vector form, we have:\n\nEx\nEy\n\n=\n\nr\ns\ns\nt\n\n∂R\n∂p\n∂R\n∂q\n\n= H\n\n∂R\n∂p\n∂R\n∂q\n\nNotice that we have the same Hessian matrix that we had derived for our surface orientation update equation before!\nIntuition: These equations make sense intuitively - brightness will be constant for constant surface orientation in a model\nwhere brightness depends only on surface orientation. Therefore, changes in brightness correspond to changes in surface orien-\ntation.\nHow do we solve for this Hessian H? We can:\n- Solve for Ex, Ey numerically from our brightness measurements.\n- Solve for Rp, Rq using our reflectance map.\nHowever, solving for H in this way presents us with 3 unknowns and only 2 equations (undetermined system). We cannot solve\nfor H, but we can still compute δp, δq. We can pattern match δx and δy by using a (δx, δy)T vector in the same direction as the\ngradient of the reflectance map, i.e.\nδx\nδy\n\n=\nRp\nRq\n\nξ\nWhere the vector on the lefthand side is our step in x and y, our vector on the righthand side is our gradient of the reflectance\nmap in gradient space (p, q), and ξ is the step size. Intuitively, this is the direction where we can \"make progress\". Substituting\nthis equality into our update equation for p and q, we have:\n\nδp\nδq\n\n= H\n\nRp\nRq\n\nξ\n=\nEx\nEy\n\nδξ\nTherefore, we can formally write out a system of 5 first-order ordinary differential equations (ODEs) that generate our charac-\nteristic strip as desired:\n1.\ndx\ndξ = Rp\n2.\ndy\ndξ = Rq\n3.\ndp\ndξ = Ex\n4.\ndq\ndξ = Ey\n\n5.\ndz\ndξ = pRp + qRq\n(\"Output Rule\")\nThough we take partial derivatives on many of the righthand sides, we can think of these quantities as measurements or derived\nvariations of our measurements, and therefore they do not correspond to partial derivatives that we actually need to solve for.\nThus, this is why we claim this is a system of ODEs, and not PDEs.\nA few notes about this system of ODEs:\n- This system of 5 ODEs explores the surface along the characteristic strip generated by these equations.\n- Algorithmically, we (a) Look at/compute the brightness gradient, which helps us (b) Compute p and q, which (c) Informs\nus of Rp and Rq for computing the change in height z.\n- ODEs 1 & 2 and ODEs 3 & 4 are two systems of equations that each determine the update for the other system (i.e. there\nis dynamic behavior between these two systems). The 5th ODE is in turn updated from the results of updating the other\n4 ODE quantities of interest.\n- The step in the image (x, y) depends on the gradient of the reflectance map in (p, q).\n- Analogously, the step in the reflectance map (p, q) depends on the gradient of the image in (x, y).\n- This system of equations necessitates that we cannot simply optimize with block gradient ascent or descent, but rather a\nprocess in which we dynamically update our variables of interest using our other updated variables of interest.\n- This approach holds generally for any reflectance map R(p, q).\n- We can express our image irradiance equation as a first order, nonlinear PDE:\nE(x, y) = R\n∂z\n∂x, ∂z\n∂y\n\nNext, we will show how this general approach applied to a Hapke surface reduces to our previous SfS results for this problem.\n9.2.1\nReducing General Form SfS for Hapke Surfaces\nRecall reflectance map for Hapke surfaces:\nR(p, q) =\nr\n1 + psp + qsq\nrs\nTaking derivatives using the system of 5 ODEs defined above, we have from the multivariate chain rule):\n1.\ndx\ndξ : Rp\n∆= ∂R\n∂p =\n√rs\n2√1+psp+qsqps\n2.\ndy\ndξ : Rq\n∆= ∂R\n∂q =\n√rs\n2√1+psp+qsqqs\n3.\ndz\ndξ : pRp + qRq =\npsp+qsq\n2√rs\n√1+psp+qsq\nSince the denominator is common in all three of these derivative equations, we can just conceptualize this factor as a speed/step\nsize factor. Therefore, we can simply omit this factor when we update these variables after taking a step. With this, our updates\nbecome:\n1. δx ←ps\n2. δy ←qs\n3. δz ←(psp + qsq) = rsE2 -1\nWhich are consistent with our prior results using our Hapke surfaces. Next, we will apply this generalized approach to Scanning\nElectron Microscopes (SEMs):\n\n9.2.2\nApplying General Form SfS to SEMs\nFor SEMs, the reflectance map is spherically symmetric around the origin:\nR(p, q) = f(p2 + q2) for some f : R →R\nUsing our system of 5 ODEs derived in the general case, we can compute our updates by first calculating derivatives dx\ndξ , dy\ndξ , and\ndz\ndξ :\n1.\ndx\ndξ : Rp\n∆= ∂R\n∂p = 2f ′(p2 + q2)p\n2.\ndy\ndξ : Rq\n∆= ∂R\n∂q = 2f ′(p2 + q2)q\n3.\ndz\ndξ : pRp + qRq = 2f ′(p2 + q2)(p2 + q2)\nAgain, here we can also simplify these updates by noting that the term 2f′(p2 + q2) is common to all three derivatives, and\ntherefore this factor can also be interpreted as a speed factor that only affects the step size. Our updates then become:\n1. δx ←p\n2. δy ←q\n3. δz ←p2 + q2\nThis tells us we will be taking steps along the brightness gradient. Our solution generates characteristic strips that contain\ninformation about the surface orientation. To continue our analysis of this SfS problem, in addition to defining characteristic\nstrips, it will also be necessary to define the base characteristic.\n9.3\nBase Characteristics\nRecall that the characteristic strip that our system of ODEs solves for is given by a 5-dimensional surface composed of:\n\nx\ny\nz\np\nq\nT ∈R5\nAnother important component when discussing our solution to this SfS problem is the base characteristic, which is the\nprojection of the characteristic strip onto the x, y image plane:\nx(ξ)\ny(ξ)T = projectionx,y{characteristic strip} = projectionx,y{\nx\ny\nz\np\nqT }\nA few notes/questions about base characteristics?\n- What are base characteristics? These can be thought of as the profiles in the (x, y) plane that originate (possibly in\nboth directions) from a given point in the initial curve.\n- What are base characteristics used for? These are used to help ensure we explore as much of the image as possible.\nWe can interpolate/average (depending on how dense/sparse a given region of the image is) different base characteristics\nto \"fill out\" the solution over the entire image space.\n- How are base characteristics used in practice? Like the solution profiles we have encountered before, base charac-\nteristics are independent of one another - this allows computation over them to be parallelizable. Intuition: These sets\nof base characteristics can be thought of like a wavefront propagating outward from the initial curve. We expect the\nsolutions corresponding to these base characteristics to move at similar \"speeds\".\n9.4\nAnalyzing \"Speed\"\nWhen considering how the speeds of these different solution curves vary, let us consider ways in which we can set constant step\nsizes/speeds. Here are a few, to name:\n1. Constant step size in z: This is effectively stepping \"contour to contour\" on a topographic map.\nAchieved by: Dividing by pRp + qRq =⇒\ndz\ndξ = 1.\n2. Constant step size in image: A few issues with this approach.\nFirst, curves may run at different rates.\nSecond,\nmethodology fails when\nq\nR2p + R2q = 0.\nAchieved by: Dividing by\nq\nR2p + R2q =⇒\nd\ndξ(\np\nδx2 + δy2) = 1.\n\n3. Constant Step Size in 3D/Object: Runs into issues when Rp = Rq = 0.\nAchieved by: Dividing by\nq\nR2p + R2q + (pRp + qRq)2 =⇒\np\n(δx)2 + (δy)2 + (δz)2 = 1.\n4. Constant Step Size in Isophotes: Here, we are effectively taking constant steps in brightness. We will end up dividing\nby the dot product of the brightness gradient and the gradient of the reflectance map in (p, q) space.\nAchieved by: Dividing by ( ∂E\n∂x\n∂R\n∂p + ∂E\n∂y\n∂R\n∂q )δξ = ((Ex, Ey) · (Rp, Rq))δξ =⇒δE = 1.\n9.5\nGenerating an Initial Curve\nWhile only having to measure a single initial curve for profiling is far better than having to measure an entire surface, it is still\nundesirable and is not a caveat we are satisfied with for this solution. Below, we will discuss how we can generate an initial\ncurve to avoid having to measure one for our SfS problem.\nAs we discussed, it is undesirable to have to measure an initial curve/characteristic strip, a.k.a. measure:\nx(ξ)\ny(ξ)\nz(ξ)\np(ξ)\nq(ξ)T\nBut we do not actually need to measure all of these. Using the image irradiance equation we can calculate ∂z\n∂η:\nE(x, y) = R(p, q)\n∂z\n∂η = ∂z\n∂x\n∂x\n∂η + ∂z\n∂y\n∂y\n∂η\n= p∂x\n∂η + q ∂y\n∂η\nWhere p and q in this case are our unknowns. Therefore, in practice, we can get by with just initial curve, and do not need the\nfull initial characteristic strip - i.e. if we have x(η), y(η), and z(η), then we can compute the orientation from the reflectance\nmap using our brightness measurements.\n9.5.1\nUsing Edge Points to Autogenerate an Initial Curve\nAn even more sweeping question: do we even need an initial curve? Are there any special points on the object where we already\nknow the orientation without a measurement? These points are along the edge, or occluding boundary, of our objects of\ninterest. Here, we know the surface normal of each of these points. Could we use these edge points as \"starting points\" for our\nSfS solution?\nIt turns out, unfortunately, that we cannot. This is due to the fact that as we infinisimally approach the edge of the boundary,\nwe have that\n∂z\n∂x\n→inf,\n∂z\n∂y\n→inf. Even though the slope q\np is known, we cannot use it numerically/iteratively with our step\nupdates above.\n9.5.2\nUsing Stationary Points to Autogenerate an Initial Curve\nLet us investigate another option for helping us to avoid having to generate an initial curve. Consider the brightest (or darkest)\npoint on an image, i.e. the unique, global, isolated extremum. This point is a stationary point in (p, q) space, i.e.\n∂R\n∂p = ∂R\n∂q = 0, granted that R(p, q) is differentiable.\nThis would allow us to find the surface orientation (p, q) solely through optimizing the reflectance map function R(p, q). One\nimmediate problem with this stationary point approach is that we will not be able to step x and y, since, from our system of\nODEs, we have that at our stationary points, our derivatives of x and y are zero:\ndx\ndξ = Rp = 0\ndy\ndξ = Rq = 0\nAdditionally, if we have stationary brightness points in image space, we encounter the \"dual\" of this problem. By definition\nstationary points in the image space imply that:\n∂E\n∂x = ∂E\n∂y = 0\n\nThis in turn implies that p and q cannot be stepped:\ndp\ndξ = ∂E\n∂x = 0\ndq\ndξ = ∂E\n∂y = 0\nIntuition: Intuitively, what is happening with these two systems (each of 2 ODEs ((x, y) or p, q), as we saw in our 5 ODE\nsystem above) is that using stationary points in one domain amounts to updates in the other domain going to zero, which in turn\nprevents the other system's quantities of interest from stepping. Since δz depends on δx, δy, δp, δq, and since δx, δy, δp, δq = 0\nwhen we use stationary points as starting points, this means we cannot ascertain any changes in δz along these points.\nNote: The extremum corresponding to a stationary point can be maximum, as it is for Lambertian surfaces, or a mini-\nmum, as it is for Scanning Electron Microscopes (SEM).\nHowever, even though we cannot use stationary points themselves as starting points, could we use the area around them?\nIf we stay close enough to the stationary point, we can approximate that these neighboring points have nearly the same surface\norientation.\n- Approach 1: Construct a local tangent plane by extruding a circle in the plane centered at the stationary point with\nradius ε - this means all points in this plane will have the same surface orientation as the stationary point. Note that\nmathematically, a local 2D plane on a 3D surface is equivalent to a 2-manifold [1]. This is good in the sense that we know\nthe surface orientation of all these points already, but not so great in that we have a degenerate system - since all points\nhave the same surface orientation, under this model they will all have the same brightness as well. This prevents us from\nobtaining a unique solution.\n- Approach 2: Rather than constructing a local planar surface, let us take a curved surface with non-constant surface\norientation and therefore, under this model, non-constant brightness.\n9.5.3\nExample\nSuppose we have a reflectance map and surface function given by:\nReflectance Map : R(p, q) = p2 + q2\nSurface Function : z(x, y) = x2 + y2\nThen, we can compute the derivatives of these as:\np = ∂z\n∂x = 2x\nq = ∂z\n∂y = 4y\nE(x, y) = R(p, q) = p2 + q2 = 4x2 + 16y2\nEx = ∂E\n∂x = 8x\nEy = ∂E\n∂y = 32y\nThe brightness gradient ( ∂E\n∂x , ∂E\n∂y ) = (8x, 32y) = (0, 0) at the origin x = 0, y = 0.\nCan we use the brightness gradient to estimate local shape? It turns out the answer is no, again because of stationary points.\nBut if we look at the second derivatives of brightness:\nExx = ∂2E\n∂x2 = ∂E\n∂x (8x) = 8\nEyy = ∂2E\n∂y2 = ∂E\n∂y (32y) = 32\nExy = ∂2E\n∂x∂y = ∂\n∂x(32y) = ∂\n∂y (8y) = 0\n\nThese second derivatives, as we will discuss more in the next lecture, will tell us some information about the object's shape.\nAs we have seen in previous lectures, these second derivatives can be computed by applying computational molecules to our\nbrightness measurements.\nHigh-Level Algorithm\n(NOTE: This will be covered in greater detail next lecture) Let's walk through the steps to ensure we can autogenerate an\ninitial condition curve without the need to measure it:\n1. Compute stationary points of brightness.\n2. Use 2nd derivatives of brightness to estimate local shape.\n3. Construct small cap (non-planar to avoid degeneracy) around the stationary point.\n4. Begin solutions from these points on the edge of the cap surrounding the stationary point.\n9.6\nReferences\n1. Manifold, https://en.wikipedia.org/wiki/Manifold\nLecture 11: Edge Detection, Subpixel Position, CORDIC, Line Detection,\n(US 6,408,109)\nIn this lecture, we will introduce some discussion on how patents work, their stipulations, and make our discussion explicit by\nlooking at a patent case study with sub-pixel accuracy edge detection. You can find the patent document on Stellar as well\nunder \"Materials\" →\"US patent 6,408,109\".\n10.1\nBackground on Patents\nWe will begin with some fun facts about industrial machine vision:\n- Integrated circuit boards cannot be manufactured without machine vision\n- Pharmaceutical chemicals also cannot be manufactured without machine vision\nHow do entrepreneurs and industrial companies ensure their inventions are protected, while still having the chance to disseminate\ntheir findings with society? This si done through patents. Patents:\n- Can be thought of as a \"contract with society\" - you get a limited monopoly on your idea, and in turn, you publish the\ntechnical details of your approach.\n- Can help to reduce litigation and legal fees.\n- Can be used by large companies as \"ammunition\" for \"patent wars\".\nSome \"rules\" of patents:\n- No equations are included in the patent (no longer true)\n- No greyscale images - only black and white\n- Arcane grammar is used for legal purposes - \"comprises\", \"apparatus\", \"method\", etc.\n- References of other patents are often included - sometimes these are added by the patent examiner, rather than the patent\nauthors\n- Most patents end with something along the lines of \"this is why our invention was necessary\" or \"this is the technical gap\nour invention fills\"\n- Software is not patentable - companies and engineers get around this by putting code on hardware and patenting the\n\"apparatus\" housing the code.\n- It is also common to include background information (similar to related literature in research).\nNow that we have had a high-level introduction to patents, let us turn to focus on one that describes an apparatus for sub-pixel\naccuracy edge finding.\n\n10.2\nPatent Case Study: Detecting Sub-Pixel Location of Edges in a Digital Image\nTo put this problem into context, consider the following:\n- Recall that images typically have large regions of uniform/homogeneous intensity\n- Image arrays are very memory-dense. A more sparse way to transfer/convey information about an image containing edges\nis to use the locations of edges as region boundaries of the image. This is one application of edge finding.\n- This patent achieves 1/40th pixel accuracy.\nThis methodology and apparatus seeks to find edges of objects in digital images at a high sub-pixel accuracy level. To do so,\nthe authors leverage different detectors, or kernels. These kernels are similar to some of the computational molecules we have\nalready looked at in this course:\n- Robert's Cross: This approximates derivatives in a coordinate system rotated 45 degrees (x′, y′). The derivatives can\nbe approximated using the Kx′ and Ky′ kernels:\n∂E\n∂x′ →Kx′ =\n-1\n-1\n\n∂E\n∂y′ →Ky′ =\n-1\n\n- Sobel Operator: This computational molecule requires more computation and it is not as high-resolution. It is also more\nrobust to noise than the computational molecules used above:\n∂E\n∂x →Kx =\n\n-1\n-1\n\n∂E\n∂y →Ky =\n\n-1\n-1\n\n- Silver Operators: This computational molecule is designed for a hexagonal grid. Though these filters have some advan-\ntages, unfortunately, they are not compatible with most cameras as very few cameras have a hexagonal pixel structure.\nFigure 2: Silver Operators with a hexagonal grid.\nFor this specific application, we can compute approximate brightness gradients using the filters/operators above, and then we can\nconvert these brightness gradients from Cartesian to polar coordinates to extract brightness gradient magnitude and direction\n(which are all we really need for this system). In the system, this is done using the CORDIC algorithm [1].\n\n10.2.1\nHigh-Level Overview of Edge Detection System\nAt a high level, we can divide the system into the following chronological set of processes/components:\n1. Estimate Brightness Gradient: Given an image, we can estimate the brightness gradient using some of the filters\ndefined above.\n2. Compute Brightness Gradient Magnitude and Direction: Using the CORDIC algorithm, we can estimate the\nbrightness gradient magnitude and direction. The CORDIC algorithm does this iteratively through a corrective feedback\nmechanism (see reference), but computationally, only uses SHIFT, ADD, SUBTRACT, and ABS operations.\n3. Choose Neighbors and Detect Peaks: This is achieved using brightness gradient magnitude and direction and a pro-\ncedure called non-maximum suppression [2].\nFirst, using gradient magnitude and direction, we can find edges by looking across the 1D edge (we can search for this edge\nusing the gradient direction Gθ, which invokes Non-Maximum Suppression (NMS). We need to quantize into 8 (Cartesian)\nor 6 (Polar) regions - this is known as coarse direction quantization.\nFinally, we can find a peak by fitting three points with a parabola (note this has three DOF). This approach will end up\ngiving us accuracy up to 1/10th of a pixel. To go further, we must look at the assumptions of gradient variation with\nposition, as well as:\n- Camera optics\n- Fill factor of the chip sensor\n- How in-focus the image is\n- How smooth the edge transition is\nThe authors of this patent claim that edge detection performance is improved using an optimal value of \"s\" (achieved through\ninterpolation and bias correction), which we will see later. For clarity, the full system diagram is here:\nFigure 3: Aggregate edge detection system. The steps listed in the boxes correspond to the steps outlined in the procedure\nabove.\nSome formulas for this system:\n1. G0 =\nq\nG2x + G2y (gradient estimation)\n2. Gθ = tan-1\nGy\nGx\n\n(gradient estimation)\n3. R0 = max(|Gx|, |Gy|) (octant quantization)\n4. S0 = min(|Gx|, |Gy|) (octant quantization)\nAt a high level, the apparatus discussed in this patent is composed of:\n\n- Gradient estimator\n- Peak detector\n- Sub-pixel interpolator\nNext, let us dive in more to the general edge detection problem.\n10.3\nEdges & Edge Detection\nLet us begin by precisely defining what we mean by edges and edge detection:\n- Edge: A point in an image where the image brightness gradient reaches a local maximum in the image brightness gradient\ndirection. Additionally, an edge is where the second derivative of image brightness (can also be thought of as the gradient\nof the image brightness gradient) crosses zero. We can look at finding the zeros of these 2nd derivatives as a means to\ncompute edges.\n- Edge Detection: A process through which we can determine the location of boundaries between image regions that are\nroughly uniform in brightness.\n10.3.1\nFinding a Suitable Brightness Function for Edge Detection\nLet us first approximate an edge brightness function using a step function, given by u(x), such that:\nu(x) =\n(\n1,\nx ≥0\n0,\nx < 0\n-2\n-1\nx\nu(x)\nUsing this cross-section across the edge to model the edge actually causes problems arising from aliasing: since we seek to find\nthe location of an edge in a discrete, and therefore, sampled image, and since the edge in the case of u(x) is infinitely thin, we\nwill not be able to find it due to sampling. In Fourier terms, if we use a perfect step function, we introduce artificially high\n(infinite) frequencies that prevent us from sampling without aliasing effects. Let us instead try a \"soft\" step function, i.e. a\n\"sigmoid\" function: σ(x) =\n1+e-x . Then our u(x) takes the form:\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\nx\nu(x)\nA \"Soft\" Unit Step Function, u(x)\nThe gradient of this brightness across the edge, given by ∇u(x) (or du\ndx in one dimension), is then given by the following. Notice\nthat the location of the maximum matches the inflection point in the graph above:\n\n-6\n-4\n-2\n5 · 10-2\n0.1\n0.15\n0.2\n0.25\nx\n∇u(x)\nGradient of \"Soft\" Unit Step Function, ∇u(x)\nAs we mentioned above, we can find the location of this edge by looking at where the second derivative of brightness crosses\nzero, a.k.a. where ∇(∇u(x)) = ∇2u(x) = 0. Notice that the location of this zero is given by the same location as the inflection\npoint of u(x) and the maximum of ∇u(x):\n-6\n-4\n-2\n-0.1\n-5 · 10-2\n5 · 10-2\n0.1\nx\n∇2u(x)\nGradient2 of \"Soft\" Unit Step Function, ∇u(x)\nFor those curious, here is the math behind this specific function, assuming a sigmoid for u(x):\n1. u(x) =\n1+exp (-x)\n2. ∇u(x) = du\ndx =\nd\ndx\n\n1+exp -x\n\n=\nexp(-x)\n(1+exp(-x))2\n3. ∇2u(x) = d2u\ndx2 =\nd\ndx(\nexp(-x)\n(1+exp(-x))2 ) = -exp(-x)(1+exp(-x))2+2 exp(-x)(1+exp(-x)) exp(-x)\n(1+exp(-x))4\nBuilding on top of this framework above, let us now move on to brightness gradient estimation.\n10.3.2\nBrightness Gradient Estimation\nThis component of this studied edge detection framework estimates the magnitude and direction of the brightness gradient. Let\nus look at how this is derived for different filters:\nRobert's Cross Gradient: Since this estimates derivatives at 45 degree angles, the pixels are effectively further apart, and\nthis means there will be a constant of proportionality difference between the magnitude of the gradient estimated here and with\na normal (x, y) coordinate system:\nq\nE2\nx′ + E2\ny′ ∝\nq\nE2x + E2y\n\nNext, we will look at the Sobel operator. For this analysis, it will be helpful to recall the following result from Taylor Series:\nf(x + δx) = f(x) + δxf ′(x) + (δx)2\n2!\nf ′′(x) + (δx)3\n3!\nf ′′′(x) + (δx)4\n24 f (4)(x) + ... =\ninf\nX\ni=0\n(δx)if (i)(x)\ni!\n, where 0!\n∆= 1\nLet us first consider the simple two-pixel difference operator (in the x-direction/in the one-dimensional case),\ni.e.\ndE\ndx →Kx = 1\nδ (-1 1). Let us look at the forward difference and backward difference when this operator is applied:\n1. Forward Difference:\n∂E\n∂x ≈f(x+δx)-f(x)\nδx\n= f ′(x) + δx\n2 f ′′(x) + (δx)2\nf ′′′(x) + ...\n2. Backward Difference:\n∂E\n∂x ≈f(x)-f(x-δx)\nδx\n= -f ′(x) -δx\n2 f ′′(x) + (δx)2\nf ′′′(x) + ...\nNotice that for both of these, if f ′′(x) is large, i.e. if f(x) is nonlinear, then we will have second-order error terms that appear in\nour estimates. In general, we want to aim for removing these lower-order error terms. If we average the forward and backward\ndifferences, however, we can see that these second-order error terms disappear:\nf(x+δx)-f(x)\nδx\n+ f(x)-f(x-δx)\nδx\n= f ′(x) + (δx)2\nf ′′′(x) + ...\nNow we have increased the error term to 3rd order, rather than 2nd order! As a computational molecule, this higher-order filter\nSobel operator looks like dE\ndx →Kx =\n2δ(-1 0 1). But we can do even better! So long as we do not need to have a pixel at our\nproposed edge, we can use a filter of three elements spanning (x -δ\n2 x x + δ\n2). There is no pixel at x but we can still compute\nthe derivative here. This yields an error that is 0.25 the error above due to the fact that our pixels are δ\n2 apart, as opposed to δ\napart:\nerror = ( xδ\n2 )2\nf ′′′(x)\nThis makes sense intuitively, because the closer together a set of gradient estimates are, the more accurate they will be. We\ncan incorporate y into the picture, making this amenable for two-dimensional methods as desired, by simply taking the center\nof four pixels, given for each dimension as:\n∂E\n∂x ≈Kx =\n2δx\n-1\n-1\n\n∂E\n∂y ≈Ky =\n2δy\n-1\n-1\n\nThe proposed edge is in the middle of both of these kernels, as shown below:\nFigure 4: We can estimate the brightness gradient with minimal error by estimating it at the point at the center of these 2D\nfilters.\nEstimating these individually in each dimension requires 3 operations each for a total of 6 operations, but if we are able to\ntake the common operations from each and combine them either by addition or subtraction, this only requires 4 operations.\nHelpful especially for images with lots of pixels.\nNext, we will discuss the 3-by-3 Sobel operator. We can think of this Sobel operator (in each dimension) as being the dis-\ncrete convolution of a 2-by-2 horizontal or vertical highpass/edge filer with a smoothing or averaging filter:\n1. x-direction:\n2δx\n-1\n-1\n\n⊗\n\n=\n\n-1\n-2\n-1\n\n2. y-direction:\n2δy\n-1\n-1\n\n⊗\n\n=\n\n-1\n-2\n-1\n\nA few notes about the derivation above:\n- The convolution used is a \"padded convolution\" [3], in which, when implemented, when the elements of the filter/kernel\n(in this case, the averaging kernel) are not aligned with the image, they are simply multiplied by zero. Zero padding is the\nmost common padding technique, but there are other techniques as well, such as wraparound padding.\n- This approach avoids the half-pixel (in which we estimate an edge that is not on a pixel) that was cited above.\n- Smoothing/averaging is a double edge sword, because while it can reduce/remove high-frequency noise by filtering, it can\nalso introduce undesirable blurring.\nNext, we will look at how the brightness gradient is converted from Cartesian to Polar coordinates:\n(Ex, Ey) →(E0, Eθ)\nE0 =\nq\nE2x + E2y\nEθ = tan-1 Ey\nEy\n\nFinally, we conclude this lecture by looking at appropriate values of s for quadratic and triangular functions. This assumes we\nhave three gradient measurements centered on G0: (1) G-, (2) G0, and (3) G+. Let us look at the results for these two types\nof functions:\n1. Quadratic: s =\nG+-G-\n4(G0-1\n2 (G+-G-)), this results in s ∈[-1\n2, 1\n2].\n2. Triangular: s =\nG+-G-\n2(G0-min(G+,G-))\nA few notes about these approaches:\n- Note that in each case, we only want to keep if the magnitude G0 is a local maximum, i.e. G0 > G+ and G0 ≥G-.\n- In the quadratic case, we can parameterize the curve with three data points using three degrees of freedom, i.e. ax2+bx+c =\n0. With this approach, b ≈first derivative and a ≈second derivative.\n10.4\nReferences\n1. CORDIC Algorithm, https://www.allaboutcircuits.com/technical-articles/an-introduction-to-the-cordic-algorithm/\n2. Non-Maximum Supression, http://justin-liang.com/tutorials/canny/#suppression\n3. Padded Convolution, https://medium.com/@ayeshmanthaperera/what-is-padding-in-cnns-71b21fb0dd7\nLecture 12: Blob analysis, Binary Image Processing, Use of Green's The-\norem, Derivative and Integral as Convolutions\nIn this lecture, we will continue our discussion of intellectual property, and how it relevant for all scientists and engineers. We\nwill then elaborate on some of the specific machine vision techniques that were used in this patent, as well as introduce some\npossible extensions that could be applicable for this patent as well.\n11.1\nTypes of Intellectual Property\nThough it is not related to the technical content of machine vision, being familiar with different types and degrees of intellectual\nproperty (IP) is crucial to professional success. Below, we discuss some of these different types of intellectual property.\n- Patents: One major type of these is utility and design patents. In these, the authors are required to disclose the \"best\nmode\" of performance. For convenience, here are some notes on patents from our previous lecture:\n- Can be thought of as a \"contract with society\" - you get a limited monopoly on your idea, and in turn, you publish\nthe technical details of your approach.\n- Can help to reduce litigation and legal fees.\n- Can be used by large companies as \"ammunition\" for \"patent wars\".\n\nSome \"rules\" of patents:\n- No equations are included in the patent (no longer true)\n- No greyscale images - only black and white\n- Arcane grammar is used for legal purposes - \"comprises\", \"apparatus\", \"method\", etc.\n- References of other patents are often included - sometimes these are added by the patent examiner, rather than the\npatent authors\n- Most patents end with something along the lines of \"this is why our invention was necessary\" or \"this is the technical\ngap our invention fills\"\n- Software is not patentable - companies and engineers get around this by putting code on hardware and patenting the\n\"apparatus\" housing the code.\n- It is also common to include background information (similar to related literature in research).\n- Copyright:\n- Books, song recordings, choreographs\n- Exceptions: presenting (fractional pieces of) information from another author\n- Trademarks:\n- Must be unique for your field (e.g. Apple vs. Apple).\n- Cannot use common words - this is actually one reason why many companies have slightly misspelled combinations\nof common words.\n- Can use pictures, character distortions, and color as part of the trademark.\n- No issues if in different fields.\n- Trade Secret\n- No protections, but not spilled, lasts forever\n- Can enforce legal recourse with Non-Disclosure Agreement (NDA)\n11.2\nEdge Detection Patent Methodologies\nFor this next section, we will direct our attention toward covering concepts that were discussed in the edge detection patent\n(6,408,109). Each of these sections will be discussed in further detail below. Before we get into the specifics of the patent again,\nit is important to point out the importance of edge detection for higher-level machine vision tasks, such as:\n- Attitude (pose estimation) of an object\n- Object recognition\n- Determining to position\nWe will touch more on these topics in later lectures.\n11.2.1\nFinding Edge with Derivatives\nRecall that we find a proposed edge by finding an inflection point of the brightness E(x, y). The following methods for finding\nthis point are equivalent:\n- Finding an inflection point of brightness E(x, y).\n- Finding maximum of brightness gradient magnitude/first derivative |∇E(x, y)|.\n- Finding zero crossing of Laplacian/second derivative ∇2E(x, y).\nFor building intuition, last time we used the following example of u(x) = σ(x) =\n1+exp(-x):\n\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\nx\nu(x)\nA \"Soft\" Unit Step Function, u(x)\nThe gradient of this brightness across the edge, given by ∇u(x) (or du\ndx in one dimension), is then given by the following. Notice\nthat the location of the maximum matches the inflection point in the graph above:\n-6\n-4\n-2\n5 · 10-2\n0.1\n0.15\n0.2\n0.25\nx\n∇u(x)\nGradient of \"Soft\" Unit Step Function, ∇u(x)\nAs we mentioned above, we can find the location of this edge by looking at where the second derivative of brightness crosses\nzero, a.k.a. where ∇(∇u(x)) = ∇2u(x) = 0. Notice that the location of this zero is given by the same location as the inflection\npoint of u(x) and the maximum of ∇u(x):\n\n-6\n-4\n-2\n-0.1\n-5 · 10-2\n5 · 10-2\n0.1\nx\n∇2u(x)\nGradient2 of \"Soft\" Unit Step Function, ∇u(x)\nFor those curious, here is the math behind this specific function, assuming a sigmoid for u(x):\n1. u(x) =\n1+exp (-x)\n2. ∇u(x) = du\ndx =\nd\ndx\n\n1+exp -x\n\n=\nexp(-x)\n(1+exp(-x))2\n3. ∇2u(x) = d2u\ndx2 =\nd\ndx(\nexp(-x)\n(1+exp(-x))2 ) = -exp(-x)(1+exp(-x))2+2 exp(-x)(1+exp(-x)) exp(-x)\n(1+exp(-x))4\n11.2.2\nMore on \"Stencils\"/Computational Molecules\nRecall that we can use finite differences [1] in the forms of \"stencils\" or computational molecules to estimate derivatives in our\nimages. For this patent, the authors used this framework to estimate the brightness gradient in order to find edges. For instance,\npartial derivative of brightness w.r.t. x can be estimated by\n1. Ex = 1\nε\n-1\n2. Ex =\n2ε\n\n-1\n\n3. Ex =\n2ε\n-1\n-1\n\nWhere for molecule 2, the best point for estimating derivatives lies directly in the center pixel, and for molecules 1 and 3, the\nbest point for estimating derivatives lies halfway between the two pixels.\nHow do we analyze the efficacy of this approach?\n1. Taylor Series: From previous lectures we saw that we could use averaging to reduce the error terms from 2nd order\nderivatives to third order derivatives. This is useful for analytically determining the error.\n2. Test functions: We will touch more on these later, but these are helpful for testing your derivative estimates using\nanalytical expressions, such as polynomial functions.\n3. Fourier domain: This type of analysis is helpful for understanding how these \"stencils\"/molecules affect higher (spatial)\nfrequency image content.\nNote that derivative estimators can become quite complicated for high-precision estimates of the derivative, even for low-order\nderivatives. We can use large estimators over many pixels, but we should be mindful of the following tradeoffs:\n- We will achieve better noise smoothing/suppression by including more measured values.\n- Larger derivative estimators linearly (1D)/quadratically (2D) increase the amount of computation time needed.\n- Features can also affect each other - e.g. a large edge detection estimator means that we can have two nearby edges affecting\neach other.\n\nWe can also look at some derivative estimators for higher-order derivatives. For 2nd-order derivatives, we just apply another\nderivative operator, which is equivalent to convolution of another derivative estimator \"molecule\":\n∂2\n∂x2 (·) = ∂\n∂x\n∂(·)\n∂x\n\n⇐⇒1\nε\n-1\n⊗1\nε\n-1\n= 1\nε2\n-2\nFor deriving the sign here and understanding why we have symmetry, remember that convolution \"flips\" one of the two filters/-\noperators!\nSanity Check: Let us apply this to some functions we already know the 2nd derivative of.\n- f(x) = x2:\nf(x) = x2\nf ′(x) = 2x\nf ′′(x) = 2\nApplying the 2nd derivative estimator above to this function:\n-2\n⊗1\nε\nf(-1) = 1\nf(0) = 0\nf(1) = 1\n= 1\nε2 ((1 ∗1) + (-2 ∗0) + (1 ∗1)) = 1\nε2 (1 + 0 + 1) = 1 ∗2 = 2\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f ′′(x) = 2.\n- f(x) = x:\nf(x) = x\nf ′(x) = 1\nf ′′(x) = 0\nApplying the 2nd derivative estimator above to this function:\n-2\n⊗1\nε\nf(-1) = -1\nf(0) = 0\nf(1) = 1\n= 1\nε2 ((1 ∗-1) + (-2 ∗0) + (1 ∗1)) = 1\nε2 (-1 + 0 + 1) = 0\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f ′′(x) = 0.\n- f(x) = 1:\nf(x) = 1\nf ′(x) = 0\nf ′′(x) = 0\nApplying the 2nd derivative estimator above to this function:\n-2\n⊗1\nε\nf(-1) = 1\nf(0) = 1\nf(1) = 1\n= 1\nε2 ((1 ∗1) + (-2 ∗1) + (1 ∗1)) = 1\nε2 (1 + -2 + 1) = 0\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f ′′(x) = 0.\nIn Practice: As demonstrated in the example \"test functions\" above, in general a good way to test an Nth order derivative\nestimator is use polynomial test functions of arbitrary coefficients from order 0 up to order N. For instance, to calculate 4th\norder derivative estimator, test:\n1. f(x) = a\n2. f(x) = ax + b\n3. f(x) = ax2 + bx + c\n4. f(x) = ax3 + bx2 + cx + d\n5. f(x) = ax4 + bx3 + cx2 + dx + e\nNote: For derivative estimator operators, the weights of the \"stencils\"/computational molecules should add up to zero. Now\nthat we have looked at some of these operators and modes of analysis in one dimension, let us now look at 2 dimensions.\n\n11.2.3\nMixed Partial Derivatives in 2D\nFirst, it is important to look at the linear, shift-invariant property of these operators, which we can express for each quality:\n- Shift-Invariant:\nd\ndx(f(x + δ)) = f ′(x + δ), for some δ ∈R\nDerivative of shifted function = Derivative equivalently shifted by same amount\n- Linear :\nd\ndx(af1(x) + bf2(x)) = af ′\n1(x) + bf ′\n2(x) for some a, b ∈R\nDerivative of scaled sum of two functions = Scaled sum of derivatives of both functions\nWe will exploit this linear, shift-invariant property frequently in machine vision. Because of this joint property, we can treat\nderivative operators as convolutions in 2D:\n∂2\n∂x∂y (·) = ∂\n∂x\n∂\n∂y (·)\n\n⇐⇒1\nε\n-1\n⊗1\nε\n-1\n\n= 1\nε2\n-1\n+1\n+1\n-1\n\nA few notes here:\n- The second operator corresponding to Ey has been flipped in accordance with the convolution operator.\n- If we project this derivative onto a \"diagonal view\", we find that it is simply the second derivative of x′, where x′ is x\nrotated 45 degrees counterclockwise in the 2D plane: x′ = x cos 45+y cos 45 =\n√\n2 x+\n√\n2 y. In other words, in this 45-degree\nrotated coordinate system, Ex′x′ = Exy.\n- Intuition for convolution: If convolution is a new concept for you, check out reference [2] here. Visually, convolution\nis equivalent to \"flipping and sliding\" one operator across all possible (complete and partial) overlapping configurations of\nthe filters with one another.\n11.2.4\nLaplacian Estimators in 2D\nThe Laplacian ∇2 ∆= ∆=\n∂2\n∂x2 +\n∂2\n∂y2 is another important estimator in machine vision, and, as we discussed last lecture, is the\nlowest-order rotationally-symmetric derivative operator. Therefore, our finite difference/computational molecule estimates\nshould reflect this property if they are to be accurate. Two candidate estimators of this operator are:\n1. \"Direct Edge\":\nε2\n\n-4\n\n2. \"Indirect Edge\":\n2ε2\n\n-4\n\nNote that the second operator has a factor of\n2ε2 in front of it because the distance between edges is\n√\n2 rather than 1, therefore,\nwe effectively have\nε′2 , where ε′ =\n√\n2ε.\nHow do we know which of these approximations is better? We can go back to our analysis tools:\n- Taylor Series\n- Test functions\n- Fourier analysis\nIntuitively, we know that neither of these estimators will be optimal, because neither of these estimators are rotationally-\nsymmetric. Let us combine these intelligently to achieve rotational symmetry. Adding four times the first one with one times\nthe second:\nε2\n\n-4\n\n+ 1\n2ε2\n\n-4\n\n=\n6ε2\n\n-20\n\nUsing Taylor Series, we can show that this estimator derived from this linear combination of estimators above results in an error\nterm that is one derivative higher than suing either of the individual estimators above, at the cost of more computation. Note\nthat the sum of all the entries here is zero, as we expect for derivative estimators.\nFor a hexagonal grid, this is scaled by\n2ε2 and has entries of all 1s on the outer ring, and an entry of -6 in the center. An\nexample application of a hexagonal grid - imaging black holes! Leads to 4\nπ greater efficiency.\nAs food for thought, what lowest-order rotationally-symmetric nonlinear operators?\nq\nE2\nx′ + E2\ny′ =\nq\nE2x + E2y Where this is the l2 normal of the estimated brightness gradient\n11.2.5\nNon-Maximum Suppression\nAnother technique leveraged in this patent was Non-Maximum Suppression (NMS). Idea: Apply edge detector estimator\noperator everywhere - we will get a small response in most places, so what if we just threshold? This is an instance of early\ndecision-making, because once we take out these points, they are no longer considered edge candidates in downstream steps.\nIt turns out the authors discourage thresholding, and in their work they remove all but the maximum estimated gradient\n(note that this is quantized at the octant level). Note that the quantized gradient direction is perpendicular to the edge. In this\ncase, for a candidate gradient point G0 and the adjacent pixels G-and G+, we must have:\nG0 > G-, G0 ≥G+\nThis forces -1\n2 ≤s ≤1\n2. Note that we have the asymmetric inequality signs to break ties arbitrarily. Next we plot the quantized\nprofile that has been interpolated parabolically - i.e. sub-pixel interpolation.\n11.2.6\nPlane Position\nNote that we have not yet done any thresholding. How can we improve this, given that we quantized the edge gradient direction?\nCould we try not quantizing the edge direction? If we have the true gradient direction, we can find the intersection of this line\nwith the edge (at 90 degrees to the edge gradient) to find a better solution.\nTo find this point above (please take a look at the handwritten lecture notes for this lecture), we project from the quantized\ngradient direction to the actual gradient direction. This is the \"plane position\" component.\n11.2.7\nBias Compensation\nAnother component of the patent focuses on the interpolation technique used for sub-pixel gradient plotting for peak finding.\nTo find an optimal interpolation technique, we can plot s vs. s′, where s′ = s|2s|b, where b ∈N is a parameter that determines\nthe relationship between s and s′.\nIn addition to cubic interpolation, we can also consider piecewise linear interpolation with \"triangle\" functions.\nFor some\ndifferent values of b:\n- b = 0 →s′ = s\n- b = 1 →s′ = 2sign(s)s2\n- b = 2 →s′ = 4sign(s)s3\nWhere different interpolation methods give us different values of b.\n11.2.8\nEdge Transition and Defocusing Compensation\nAnother point of motivation: most edge detection results depend on the actual edge transition. Why are edges fuzzy (note that\nsome degree of fuzziness is needed to prevent aliasing)? One major cause of fuzziness is \"defocusing\", in which the image plane\nand \"in focus\" planes are slightly offfrom one another. This causes a \"pillbox\" of radius R to be imaged (see handwritten lecture\nnotes), rather than the ideal case of an impulse function δ(x, y). This radius is determined by:\nR = δ d2\nf (Point Spread Function (PSF))\n\nThis pillbox image is given mathematically by:\nπR2 (1 -u(r -R))\nWhere u(·) is the unit step function. Where f is the focal length of the lens, d is the diameter of the lens (assumed to be conic),\nand δ is the distance along the optical axis between the actual image plane and the \"in focus\" plane.\n11.2.9\nMultiscale\nNote: We will discuss this in greater detail next lecture.\nMultiscale is quite important in edge detection, because we can have edges at different scales.\nTo draw contrasting exam-\nples, we could have an image such that:\n- We have very sharp edges that transition over ≈only 1 pixel\n- We have blurry edges that transition over many pixels\n11.2.10\nEffect on Image Edge\nHere is one possible extension not included in the edge detection patent.\nWe can slide a circle across a binary image - the overlapping regions inside the circle between the 1-0 edge controls how\nbright things appear. We can use this technique to see how accurately the algorithm plots the edge position - this allows for\nerror calculation since we have ground truth results that we can compute using the area of the circle. Our area of interest is\ngiven by the area enclosed by the chord whose radial points intersect with the binary edge:\nA = R2θ -2\n√\nR2 -X2X\nθ = arctan\n√\nR2 -x2\nx\n\nAnother way to analyze this is to compute the analytical derivatives of this brightness function:\n1.\n∂E\n∂x = 2\n√\nR2 -x2\n2.\n∂2E\n∂x2 =\n-2x\n√\nR2-x2\nWhat can we do with this? We can use this as input into our algorithm to compute teh error and compensate for the degree\nof defocusing of the lens. In practice, there are other factors that lead to fuzzy edge profiles aside from defocusing, but this\ndefocusing compensation helps.\n11.2.11\nAddressing Quantization of Gradient Directions\nHere is another possible extension not included in the edge detection patent.\nRecall that because spaces occurs in two sizes (pixel spacing and\n√\n2 pixel spacing), we need to sample in two ways, which\ncan lead to slightly different error contributions. We do not want quantized gradient directions. To do this, we can just inter-\npolate values G-, G0, G+ along the true edge gradient!\nLinear 1D Interpolation:\nf(x) = f(a)(b -x) + f(b)(x -a)\nb -a\nWe can also leverage more sophisticated interpolation methods, such as cubic spline.\nWhy did the authors not leverage this interpolation strategy?\n- this requires the spacing of any level, i.e. not just pixel and\n√\n2 pixel spacing, but everything in between.\n- Since you interpolate, you are not using measured values. Introduces some uncertainty that may be too much to achieve\n1/40th pixel accuracy.\nWhat can we do to address this? →Project gradient onto unit circle! This requires 2D interpolation, which can be done\nwith methods such as bilinear or bicubic interpolation.\n\n11.2.12\nCORDIC\nAs we discussed in the previous lecture, CORDIC is an algorithm used to estimate vector direction by iteratively rotating a\nvector into a correct angle. For this patent, we are interested in using CORDIC to perform a change of coordinates from cartesian\nto polar:\n(Ex, Ey) →(E0, Eθ)\nIdea: Rotate a coordinate system to make estimates using test angles iteratively. Note that we can simply compute these with\nsquare roots and arc tangents, but these can be prohibitively computationally-expensive:\nE0 =\nq\nE2x + E2y\nEθ = arctan\nEy\nEx\n\nRather than computing these directly, it is faster to iteratively solve for the desired rotation θ by taking a sequence of iterative\nrotations {θi}n\ni=1,2,···. The iterative updates we have for this are, in matrix-vector form:\n\"\nE(i+1)\nx\nE(i+1)\ny\n#\n=\ncos θi\nsin θi\n-sin θi\ncos θi\n\"\nE(i)\nx\nE(i)\ny\n#\nGradients at next step = Rotation R by θi × Gradients at current step\nHow do we select {θi}n\ni=1,2,···? We can select progressively smaller angles. We can accept the candidate angle and invoke the\niterative update above if each time the candidate angle reduces |Ey| and increases |Ex|.\nThe aggregate rotation θ is simply the sum of all these accepted angles: θ = P\ni θi\nOne potential practical issue with this approach is that it involves a significant number of multiplications. How can we avoid\nthis? We can pick the angles carefully - i.e. if our angles are given successively by π\n2 , π\n4 , π\n8 , ..., then:\nsin θu\ncos θi\n= 1\n2i →rotation matrix becomes :\ncos θi\n\n2-i\n-2-i\n\nNote that this reduces computation to 2 additions per iteration. Angle we turn through becomes successively smaller:\ncos θi =\nr\n1 + 1\n22i →R =\nY\ni\ncos θi =\nY\ni\nr\n1 + 1\n22i ≈1.16 (precomputed)\n11.3\nReferences\n1. Finite Differences, https://en.wikipedia.org/wiki/Finite difference\n2. Convolution, https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-\nlearning-1f6f42faee1\nLecture 13: Object detection, Recognition and Pose Determination, PatQuick\n(US 7,016,539)\nIn this lecture, we will begin by looking at some general problems for object detection and pose estimation of objects in an image,\nand also look at some optimization algorithms we can use for finding optimal matches between a \"master image\" / template\nimage, which is the object we look for, and this object in another image (perhaps in a scene). We then look at a patent describing\nsome of the approaches taken to solve some of these aforementioned tasks.\n12.1\nMotivation & Preliminaries for Object Detection/Pose Estimation\nObject detection and pose estimation builds on top of previous machine vision tasks we have covered. Some specific tasks include:\n- Object detection - i.e. detect and locate object in an image\n- Recognize object\n\n- Determine pose of detected/recognized object\n- Inspect object\nMotivation for these approaches: In machine vision problems, we often manipulate objects in the world, and we want to\nknow what and where these objects are in the world. In the case of these specific problems, we assume prior knowledge of the\nprecise edge points of these objects (which, as we discussed in the two previous lectures, we know how to compute!)\n12.1.1\n\"Blob Analysis\"/\"Binary Image Processing\"\nWe can use thresholding and other algorithms such as finding a convex hull to compute elements of an object in a binary image\n(black/white) such as:\n- Area (moment order 0)\n- Perimeter\n- Centroid (moment order 1)\n- \"Shape\" (generalization of different-order moments)\n- Euler number - In this case this is the number of blobs minus number of holes\nA few notes about computing these different quantities of interest:\n- We have seen from previous lectures that we can compute some of these elements, such as perimeter, using Green's\nTheorem. We can also accomplish this with counting - can be done simply by counting pixels based offof whether their\npixel is a 0 or 1.\n- However, the issue with these approaches is that they require thresholding - i.e.\nremoving points from any further\nconsideration early on the process and possibly without all information available; essentially removing potentially viable\npoints too early.\n- Shape: As introduced above, shape is often computed by computing moments of any order. Recall the definition of\nmoments of a 2D shape:\n1. O-order:\nRR\nD E(x, y)dxdy →Area\n2. 1-order:\nRR\nD E(x, y)xydxdy →Centroid\n3. 2-order:\nRR\nD E(x, y)x2y2dxdy →Dispersion\n...\n4. k-order:\nRR\nD E(x, y)xkykdxdy\n- Note that these methods are oftentimes applied to processed, not raw images.\n12.1.2\nBinary Template\nWe will discuss this more later, but this is crucial for the patent on object detection and pose estimation that we will be discussing\ntoday. A binary template is:\n- A \"master image\" to define the object of interest that we are trying to detect/estimate the pose of\n- You will have a template of an object that you can recognize the object and get the pose/attitude.\n12.1.3\nNormalized Correlation\nThis methodology also plays a key role in the patent below.\nIdea: Try all possible positions/configurations of the pose space to create a match between the template and runtime im-\nage of the object. If we are interested in the squared distance between the displaced template and the image in the other\nobject (for computational and analytic simplicity, let us only consider rotation for now), then we have the following optimization\nproblem:\nmin\nδx,δy\nZZ\nD\n(E1(x -δx, y -δy) -E2(x, y))2dxdy\n\nWhere we have denoted the two images separately as E1 and E2.\nIn addition to framing this optimization mathematically as minimizing the squared distance between the two images, we can\nalso conceptualize this as maximizing the correlation between the displaced image and the other image:\nmax\nδx,δy\nZZ\nD\nE1(x -δx, y -δy)E2(x, y)dxdy\nWe can prove mathematically that the two are equivalent. Writing out the first objective as J(δx, δy) and expanding it:\nJ(δx, δy) =\nZZ\nD\n(E(x -δx, y -δy) -E2(x, y))2dxdy\n=\nZZ\nD\n(E2\n1(x -δx, y -δy) -2\nZZ\nD\nE1(x -δx, y -δy)E2(x, y)dxdy +\nZZ\nD\nE2\n2(x, y)dxdy\n=⇒arg min\nδx,δy J(δx, δy) = arg max\nδx,δy\nZZ\nD\nE1(x -δx, y -δy)E2(x, y)dxdy\nSince the first and third terms are constant, and since we are minimizing the negative of a scaled correlation objective, this is\nequivalent to maximizing the correlation of the second objective.\nWe can also relate this to some of the other gradient-based optimization methods we have seen using Taylor Series. Suppose\nδx, δy are small. Then the Taylor Series Expansion of first objective gives:\nZZ\nD\n(E1(x -δx, y -δy) -E2(x, y))2dxdy =\nZZ\nD\n(E1(x, y) -δx\n∂E1\n∂x -∂E1\n∂y + · · · -E2(x, y))2dxdy\nIf we now consider that we are looking between consecutive frames with time period δt, then the optimization problem becomes\n(after simplifying out E1(x, y) -E2(x, y) = -δt ∂E\n∂t ):\nmin\nδx,δy\nZZ\nD\n(-δxEx -δyEy -δtEt)2dxdy\nDividing through by δt and taking limδt→0 gives:\nmin\nδx,δy\nZZ\nD\n(uEx + vEy + Et)2dxdy\nA few notes about the methods here and the ones above as well:\n- Note that the term under the square directly above looks similar to our BCCE constraint from optical flow!\n- Gradient-based methods are cheaper to compute but only function well for small deviations δx, δy.\n- Correlation methods are advantageous over least-squares methods when we have scaling between the images (e.g. due to\noptical setting differences): E1(x, y) = kE2(x, y) for some k ∈R .\nAnother question that comes up from this: How can we match at different contrast levels? We can do so with normalized\ncorrelation. Below, we discuss each of the elements we account for and the associated mathematical transformations:\n1. Offset: We account for this by subtracting the mean from each brightness function:\nE\n′\n1(x, y) = E1(x, y) - E1,\nE1 =\nRR\nD E1(x, y)dxdy\nRR\nD dxdy\nE\n′\n2(x, y) = E2(x, y) - E2,\nE2 =\nRR\nD E2(x, y)dxdy\nRR\nD dxdy\nThis removes offset from images that could be caused by changes to optical setup.\n2. Contrast: We account for this by computing normalized correlation, which in this case is the Pearson correlation coefficient:\nRR\nD E\n′\n1(x -δx, y -δy)E\n′\n2(x, y)dxdy\nqRR\nD E\n′\n1(x -δx, y -δy)dxdy\nqRR\nD E\n′\n2(x, y)dxdy\n∈[-1, 1]\nWhere a correlation coefficient of 1 denotes a perfect match, and a correlation coefficient of -1 denotes a perfectly imperfect\nmatch.\n\nAre there any issues with this approach? If parts/whole images of objects are obscured, this will greatly affect correlation\ncomputations at these points, even with proper normalization and offsetting.\nWith these preliminaries set up, we are now ready to move into a case study: a patent for object detection and pose esti-\nmation using probe points and template images.\n12.2\nPatent 7,016,539: Method for Fast, Robust, Multidimensional Pattern Recognition\nThis patent aims to extend beyond our current framework since the described methodology can account for more than just\ntranslation, e.g. can account for:\n- Rotation\n- Scaling\n- Shearing\n12.2.1\nPatent Overview\nThis patent describes a patent for determining the presence/absence of a pre-determined pattern in an image, and for determin-\ning the location of each found instance within a multidimensional space.\nA diagram of the system can be found below:\nFigure 5: System diagram.\nA few notes about the diagram/aggregate system:\n- A match score is computed for each configuration, and later compared with a threshold downstream. This process leads\nto the construction of a high-dimensional matches surface.\n- We can also see in the detailed block diagram from this patent document that we greatly leverage gradient estimation\ntechniques from the previous patent on fast and accurate edge detection.\n- For generalizability, we can run this at multiple scales/levels of resolution.\n12.2.2\nHigh-level Steps of Algorithm\n1. Choose appropriate level of granularity (defined as \"selectable size below which spatial variations in image brightness are\nincreasingly attenuated, and below which therefore image features increasingly cannot be resolved\") and store in model.\n2. Process training/template image to obtain boundary points.\n3. Connect neighboring boundary points that have consistent directions.\n4. Organize connected boundary points into chains.\n\n5. Remove short or weak chains.\n6. Divide chains into segments of low curvature separated by conrner of high curvature.\n7. Create evenly-spaced along segments and store them in model.\n8. Determine pattern contrast and store in model.\nA few notes about this procedure:\n- To increase the efficiency of this approach, for storing the model, we store probes (along with granularity and contrast),\nnot the image for which we have computed these quantities over.\n- When comparing gradients between runtime and training images, project probe points onto the other image - we do not\nhave to look at all points in image; rather, we compare gradients (direction and magnitude - note that magnitude is\noften less viable/robust to use than gradient direction) between the training and runtime images only at the probing points.\n- We can also weight our probing points, either automatically or manually. Essentially, this states that some probes are\nmore important that others when scoring functions are called on object configurations.\n- This patent can also be leveraged for machine part inspection, which necessitates high degrees of consistent accuracy\n- An analog of probes in other machine and computer vision tasks is the notion of keypoints, which are used in descriptor-\nbased feature matching algorithms such as SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Fea-\ntures), FAST (Features from Accelerated Segment Test), and ORB (Oriented FAST and rotated BRIEF). Many of these\naforementioned approaches rely on computing gradients at specific, \"interesting points\" as is done here, and construct\nfeatures for feature matching using a Histogram of Oriented Gradients (HoG) [1].\n- For running this framework at multiple scales/resolutions, we want to use different probes at different scales.\n- For multiscale, there is a need for running fast low-pass filtering. Can do so with rapid convolutions, which we will be\ndiscussing in a later patent in this course.\n- Probes \"contribute evidence\" individually, and are not restricted to being on the pixel grid.\n- The accuracy of this approach, similar to the other framework we looked at, is limited by the degree of quantization in the\nsearch space.\n12.2.3\nFramework as Programming Objects\nLet us also take a look at the object-oriented nature of this approach to better understand the framework we work with. One\nof these objects is the model:\nModel: This has fields:\n- Probes: This is the list of probe points. Note that we can also use compiled probes. In turn, each element in this list\nhas fields:\n- Position\n- Direction\n- Weight\n- Granularity: This is a scalar and is chosen during the training step.\n- Contrast: This field is set to the computed contrast of the training/template pattern.\nWe can also look at some of the fields Generalized Degree of Freedom (Generalized DOF) object as well:\n- Rotation\n- Shear - The degree to which right angles are mapped into non-right angles.\n- Log size\n- Log x size\n- Log y size\n\n12.2.4\nOther Considerations for this Framework\nWe should also consider how to run our translational search. This search should be algorithmically conducted:\n- Efficiently\n- At different levels of resolution\n- Hexagonally, rather than on a square grid - there is a\nπ advantage of work done vs. resolution. Here, hexagonal peak\ndetection is used, and to break ties, we arbitrarily set 3 of the 6 inequalities as ≥, and the other 3 as >.\nWhat is pose?\nPose is short for position and orientation, and is usually determined with respect to a reference coordinate system. In the\npatent's definition, it is the \"mapping from pattern to image coordinates that represents a specific transformation\nand superposition of a pattern onto an image.\"\nNext, let us look into addressing \"noise\", which can cause random matches to occur.\nArea under S(θ) curve captures the\nprobability of random matches, and we can compensate by calculating error and subtracting it out of the results. However, even\nwith this compensation, we are still faced with additional noise in the result.\nInstead, we can try to assign scoring weights by taking the dot product between gradient vectors: ˆv1 · ˆv2 = cos θ. But one\ndisadvantage of this approach is that we end up quantizing pose space.\nFinally, let us look at how we score the matches between template and runtime image configurations: scoring functions.\nOur options are:\n- Normalized correlation (above)\n- Simple peak finding\n- Removal of random matches (this was our \"N\" factor introduced above)\n12.3\nReferences\n1. Histogram of Oriented Gradients, https://en.wikipedia.org/wiki/Histogram of oriented gradients\nLecture 14: Inspection in PatQuick, Hough Transform, Homography, Po-\nsition Determination, Multi-Scale\nIn this lecture, we will continue our discussion of \"PatQuick\", the patent we discussed last time for object detection and pose\nestimation. We will focus on elements of this patent such as scoring functions and generalized degrees of freedom (DOF), and\nwill use this as a segway into general linear transformations and homography. We will conclude our discussion with subsampling\nand Hough Transforms, which, at a high level, we can think of as a mapping from image space to parameter space.\n13.1\nReview of \"PatQuick\"\nTo frame our analysis and introduction of other technical machine vision concepts, let us briefly revisit the high-level ideas of\n\"PatQuick\". There were three main \"objects\" in this model:\n- Training/template image. This produces a model consisting of probe points.\n- Model, containing probe points.\n- Probe points, which encode evidence for where to make gradient comparisons, i.e. to determine how good matches\nbetween the template image and the runtime image under the current pose configuration.\nOnce we have the model from the training step, we can summarize the process for generating matches as:\n1. Loop over/sample from configurations of the pose space (which is determined and parameterized by our degrees of freedom),\nand modify the runtime image according to the current pose configuration.\n2. Using the probe points of the model, compare the gradient direction (or magnitude, depending on the scoring function)\nto the gradient direction (magnitude) of the runtime image under the current configuration, and score using one of the\nscoring functions below.\n\n3. Running this for all/all sampled pose configurations from the pose space produces a multidimensional scoring surface. We\ncan find matches by looking for peak values in this surface.\nA few more notes on this framework, before diving into the math:\n- Training is beneficial here, because it allows for some degree of automated learning.\n- Evidence collected from the probe points is cumulative and computed using many local operations.\n- Accuracy is limited by the quantization level of the pose spanned. The non-redundant components of this pose space are:\n- 2D Translation, 2 DOF\n- Rotation, 1 DOF\n- Scaling, 1 DOF,\n- Skew, 1 DOF,\n- Aspect Ratio, 1 DOF\nTogether, the space of all these components compose a general linear transformation, or an affine transformation:\nx′ = a11x + a12y + a13\ny′ = a21x + a22y + a23\nWhile having all of these options leads to a high degree of generality, it also leads to a huge number of pose configurations,\neven for coarse quantization. This is due to the fact that the number of configurations grows exponentially with the number\nof DOF.\n13.1.1\nScoring Functions\nNext, let us look at the scoring functions leveraged in this framework. Recall that there will also be random gradient matches\nin the background texture - we can compute this \"probability\" as \"noise\" given by N:\nN = 1\n2π\nZ 2π\nRdir(θ)dθ = 3\n32 (Using signed values),\n32 (Using absolute values)\nWhere the first value ( 3\n32) corresponds to the probability of receiving a match if we randomly select a probe point's location,\nthe second value corresponds to taking reverse polarity into account, and the function Rdir corresponds to the scoring function\nfor the gradient direction, and takes as input the difference between two directions as two-dimensional vectors. Below are the\nscoring functions considered. Please take note of the following abbreviations:\n- pi denotes the two-dimensional location of the ith probe point after being projected onto the runtime image.\n- di denotes the direction of the probe point in the template image.\n- wi denotes the weight of the ith probe point, which is typically set manually or via some procedure.\n- D(·) is a function that takes a two-dimensional point as input, and outputs the direction of the gradient at this point.\n- Rdir is a scoring function that takes as input the norm of the difference between two vectors representing gradient directions,\nand returns a scalar.\n- M(·) is a function that computes the magnitude of the gradient in the runtime image at the point given by its two-\ndimensional argument.\n- Rmag is a scoring function that takes as input the magnitude of a gradient and returns a scalar. In this case, Rmag saturates\nat a certain point, e.g. if limx→infR(x) = K for some K ∈R , and for some j ∈R , R(j) = K, R(j + ε) = K ∀ε ∈[0, inf).\n- N corresponds to the \"noise\" term computed above from random matches.\nWith these terms specified, we are now ready to introduce the scoring functions:\n1. Piecewise-Linear Weighting with Direction and Normalization:\nS1a(a) =\nP\ni max(wi, 0)Rdir(||D(a + pi) -di||2)\nP\ni max(wi, 0)\nQuick note: the function max(0, wi) is known as the Rectified Linear Unit (ReLU), and is written as ReLU(wi). This\nfunction comes up frequently in machine learning.\n\n- Works with \"compiled probes\". With these \"compiled probes\", we only vary translation - we have already mapped\npose according to the other DOFs above.\n- Used in a \"coarse step\".\n2. Binary Weighting with Direction and Normalization\nS1a(a) =\nP\ni(wi > 0)Rdir(||D(a + pi) -di||2)\nP\ni(wi > 0)\nWhere the predicate (wi > 0) returns 1 if this is true, and 0 otherwise.\n3. \"Preferred Embodiment:\nS(a) =\nP\ni(wi > 0)(Rdir(||D(a + pi) -di||2) -N)\n(1 -N) P\ni(wi > 0)\n4. Raw Weights with Gradient Magnitude Scaling and No Normalization\nS2(a) =\nX\ni\nwiM(a + pi)Rdir(||D(a + pi) -di||2)\nNote that this scoring function is not normalized, and is used in the fine scanning step of the algorithm.\n5. Raw Weights with Gradient Magnitude Scaling and Normalization\nS3(a) =\nP\ni wiM(a + pi)Rdir(||D(a + pi) -di||2)\nP\ni wi\n13.1.2\nAdditional System Considerations\nLet us focus on a few additional system features to improve our understanding of the system as well as other principles of machine\nvision:\n- Why do some of these approaches use normalization, but not others? Normalization is computationally-expensive,\nand approaches that avoid a normalization step typically do this to speed up computation.\n- For all these scoring functions, the granularity parameter is determined by decreasing the resolution until the system no\nlonger performs well.\n- We need to ensure we get the gradient direction right. So far, with just translation, this has not been something we need\nto worry about. But with our generalized linear transform space of poses, we may have to account for this. Specifically:\n- Translation\n- Rotation\n- Scaling\ndo not affect the gradient directions. However:\n- Shear\n- Aspect ratio\nwill both affect the gradient directions. We can account for this, however, using the following process:\n1. Compute the gradient in the runtime image prior to invoking any transformations on it.\n2. Compute the isophote in the pre-transformed runtime image by rotating 90 degrees from the gradient direction using\nthe rotation matrix given by:\nRG→I =\n-1\n\n3. Transform the isophote according to the generalized linear transformation above with the degrees of freedom we\nconsider for our pose space.\n4. After computing this transformed isophote, we can find the transformed gradient by finding the direction orthogonal\nto the transformed isophote by rotating back 90 degrees using the rotation matrix given by:\nRI→G =\n-1\n\n13.1.3\nAnother Application of \"PatQuick\": Machine Inspection\nLet us consider some elements of this framework that make it amenable and applicable for industrial machine part inspection:\n- How do we distinguish between multiple objects (a task more generally known as multiclass object detection and classifi-\ncation)? We can achieve this by using multiple models/template images, i.e. one model/template for each type\nof object we want to detect and find the relative pose of.\n- With this framework, we can also compute fractional matches - i.e. how well does one template match another object in\nthe runtime image.\n- We can also take an edge-based similarity perspective - we can look at the runtime image's edge and compare to edge\nmatches achieved with the model.\n13.2\nIntro to Homography and Relative Poses\nWe will now shift gears and turn toward a topic that will also be relevant in the coming lectures on 3D. Let's revisit our\nperspective projection equations when we have a camera-centric coordinate system:\nx\nf = Xc\nYc\n, y\nf = Yc\nYc\nThus far, we have only considered camera-centric coordinate systems - that is, when the coordinates are from the point of view\nof the camera. But what if we seek to image points that are in a coordinate system defined by some world coordinate system\nthat differs from the camera? Then, we can express these camera coordinates as:\n\nXc\nYc\nZc\n\n=\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nXW\nYW\nZW\n\n+\n\nX0\nY0\nZ0\n\ncam coord. = world2cam rot × world coord. + world2cam trans\nWhereR =\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nis a rotation matrix in 3D.\nTherefore, in the general case, to find the perspective projection from world coordinates onto our image, we can combine the\ntwo previous equations, carrying out the matrix multiplication along the way:\nx\nf = Xc\nZc\n= r11XW + r12YW + r13ZW + X0\nr31XW + r32YW + r33ZW + Z0\ny\nf = Yc\nZc\n= r21XW + r22YW + r23ZW + Y0\nr31XW + r32YW + r33ZW + Z0\nIs there a way we can combine rotation and translation into a single operation? Let us consider a simple case in which the\nthe points in our world coordinate system are coplanar in the three-dimensional plane ZW = 0. Since the third column of the\nrotation corresponds to all zeros, we can rewrite our equation from the world coordinate frame to the camera frame as:\n\nXc\nYc\nZc\n\n=\n\nr11\nr12\nX0\nr21\nr22\nY0\nr31\nr32\nZ0\n\nXW\nYW\n\n= T\n\nXW\nYW\n\n=\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nXW\nYW\n\n+\n\nX0\nY0\nZ0\n\nThe vector\n\nXW\nYW\nT expresses our (now 2D) world coordinate system in homogeneous coordinates, which have a 1\nentry appended to the final element.\nIn this case, we can fold translation and rotation into a single matrix! We call this matrix T, and it is called a Homog-\nraphy Matrix that encodes both rotation and translation. We will revisit this concept when we begin our discussion of 3D\ntransformations. Note that while our rotation matrix R is orthogonal, this homography matrix T is not necessarily.\n\n13.2.1\nHow many degrees of freedom\nFor determining the relative pose between camera and world frames, let us consider the number of degrees of freedom:\n- 3 for translation, since we can shift in x, y, and z\n- 3 for rotation, since our rotations can preserve the xz axis, xy axis, and yz axis\nIf we have 9 entries in the rotation matrix and 3 in the translation vector (12 unknowns total), and only 6 degrees of freedom, then\nhow do we solve for these entries? There is redundancy - the rotation matrix has 6 constraints from orthonormality\n(3 from constraining the rows to have unit size, and 3 from having each row being orthogonal to the other).\nMathematically, these constraints appear in our Homography matrix T as:\nr2\n11 + r2\n21 + r2\n31 = 1 (Unit length constraint)\nr2\n12 + r2\n22 + r2\n23 = 1 (Unit length constraint)\nr11r12 + r21r22 + r31r32 = 0 (Orthogonal columns)\nA few notes here about solving for our coefficients in T:\n- Do we need to enforce these constraints? Another option is to run least squares on the calibration data.\n- We must be cognizant of the following: We only know the Homography matrix T up to a constant scale factor, since we\nare only interested in the ratio of the components of the camera coordinates for perspective projection.\n13.3\nHough Transforms\nLet us switch gears and talk about another way to achieve edge finding, but more generally the estimation of parameters for any\nparameterized surface.\nMotivation: Edge and line detection for industrial machine vision. This was one of the first machine vision patents (sub-\nmitted in 1960, approved in 1962). We are looking for lines in images, but our gradient-based methods may not necessarily work,\ne.g. due to non-contiguous lines that have \"bubbles\" or other discontinuities. These discontinuities can show up especially for\nsmaller resolution levels.\nIdea: The main idea of the Hough Transform is to intelligently map from image/surface space to parameter space for\nthat surface. Let us walk through the mechanics of how parameter estimation works for some geometric objects.\nSome notes on Hough Transforms:\n- Hough transforms are often used as a subroutine in many other machine vision algorithms.\n- Hough transforms actually generalize beyond edge and line detection, and extend more generally into any domain in which\nwe map a parameterized surface in image space into parameter space in order to estimate parameters.\n13.3.1\nHough Transforms with Lines\nA line/edge in image space can be expressed (in two-dimensions for now, just for building intuition, but this framework is\namenable for broader generalizations into higher-dimensional lines/planes): y = mx+c. Note that because y = mx+c, m = y-c\nx\nand c = y -mx. Therefore, this necessitates that:\n- A line in image space maps to a singular point in Hough parameter space.\n- A singular point in line space corresponds to a line in Hough parameter space.\nTo estimate the parameters of a line/accomplish edge detection, we utilize the following high-level procedure:\n1. Map the points in the image to lines in Hough parameter space and compute intersections of lines.\n2. Accumulate points and treat them as \"evidence\" using accumulator arrays.\n\n3. Take peaks of these intersections and determine what lines they correspond to, since points in Hough parameter space\ndefine parameterizations of lines in image space. See the example below:\nFigure 6: Example of finding parameters in Hough Space via the Hough Transform.\n13.3.2\nHough Transforms with Circles\nLet us look at how we can find parameters for circles with Hough transforms.\nMotivating example: Localization with Long Term Evolution (LTE) Network. Some context to motivate this appli-\ncation further:\n- LTE uses Time Division Multiplexing to send signals, a.k.a \"everyone gets a slot\".\n- CDMA network does not use this.\n- You can triangulate/localize your location based offof how long it takes to send signals to surrounding cellular towers.\nWe can see from the diagram below that we map our circles into Hough parameter space to compute the estimate of parameters.\nFigure 7: Example of using Hough Transforms to find the parameters of circles for LTE.\nAs we have seen in other problems we have studied in this class, we need to take more than one measurement. We cannot\nsolve these problems with just one measurement, but a single measurement constrains the solution. Note that this problem\nassumes the radius is known.\n13.3.3\nHough Transforms with Searching for Center Position and Radius\nAnother problem of interest is finding the center of position of a circle's radius R and its center position (x, y), which comprise\nthe 3 dimensions in Hough parameter space. In Hough Transform space, this forms a cone that expands upward from R0 = 0,\nwhere each cross-section of Z is the equation (x2 + y2 = R2) for the given values of x, y, and R.\nEvery time we find a point on the circle, we update the corresponding set of points on the cone that satisfy this equation.\nThe above results in many cone intersections with one another - as before, we collect evidence from these intersections, build a\nscore surface, and compute the peak of this surface for our parameter estimates.\n\n13.4\nSampling/Subsampling/Multiscale\nSampling is another important aspect for machine vision tasks, particularly for problems involving multiple scales, such as edge\nand line detection. Sampling is equivalent to working at different scales.\nWhy work at multiple scales?\n- More efficient computation when resolution is lower, and is desirable if performance does not worsen.\n- Features can be more or less salient at different resolutions, i.e. recall that edges are not as simple as step edges and often\nexhibit discontinuities or non-contiguous regions.\nIf we downsample our image by rn along the rows and rm along the columns, where rn, rm ∈(0, 1), then the total amount of\nwork done (including the beginning image size) is given by the infinite geometric series:\ninf\nX\ni=0\n(rnrm)i =\n1 -rnrm\n(Recall that\ninf\nX\ni=0\nri =\n1 -r for r ∈(0, 1))\nWhat does the total work look like for some of these values?\n- rn = rm = r = 1\nwork =\n1 -r2 =\n1 -1\n= 4\nBut downsampling by 1\n2 each time is quite aggressive, and can lead to aliasing. Let us also look at a less aggressive sampling\nratio.\n- rn = rm = r =\n√\nwork =\n1 -r2 =\n1 -1\n= 2\nHow do we sample in this case? This is equivalent to taking every other sample in an image when we downsample. We\ncan do this using a checkerboard/chess board pattern. We can even see the selected result as a square grid if we rotate\nour coordinate system by 45 degrees.\nThe SIFT (Scale-Invariant Feature Transform) algorithm uses this less aggressive sampling technique. SIFT is a descriptor-\nbased feature matching algorithm for object detection using a template image.\nLecture 15: Alignment, recognition in PatMAx, distance field, filtering and\nsub-sampling (US 7,065,262)\nIn this lecture, we will discuss another patent on the topic of object inspection and pose estimation, known as PatMAx. We\nwill then look at computing distance to lines as a means to perform better edge detection, and then will investigate the role of\nsparse convolution for multiscale systems that perform filtering.\n14.1\nPatMAx\nAnother patent we will look at for object inspection is PatMAx.\n14.1.1\nOverview\nSome introductory notes on this:\n- This framework builds offof the previous PatQuick patent.\n- This framework, unlike PatQuick, does not perform quantization of the pose space, which is one key factor in enabling\nsub-pixel accuracy.\n\n- PatMAx assumes we already have an approximate initial estimate of the pose.\n- PatMAx relies on an iterative process for optimizing energy, and each attraction step improves the fit of the configuration.\n- Another motivation for the name of this patent is based offof electrostatic components, namely dipoles, from Maxwell. As\nit turns out, however, this analogy works better with mechanical springs than with electrostatic dipoles.\n- PatMAx performs an iterative attraction process to obtain an estimate of the pose.\n- An iterative approach (e.g. gradient descent, Gauss-Newton, Levenberg-Marquadt) is taken because we likely will not\nhave a closed-form solution in the real world. Rather than solving for a closed-form solution, we will run this iterative\noptimization procedure until we reach convergence.\n- Relating this framework back to PatQuick, PatMAx can be run after PatQuick computes an initial pose estimate, which\nwe can then refine using PatMAx. In fact, we can view our patent workflow as:\nFigure 8: An overview of how the patents we have looked at for object inspection fit together.\nA diagram of the system can be found here:\nFigure 9: High-level diagram of the PatMAx system.\nNow that we have a high-level overview, we are now ready to dive more into the specifics of the system.\n14.1.2\nTraining PatMAx\n: The training process can be classified as three distinct steps:\n1. We begin with edge detection, which produces a field dipole list (essentially the probe points from the PatQuick patent\nframework).\n2. Training also produces a field. We compare runtime features with template features and determine the attraction of these\nfeatures between the images using this field as a vector field.*\n3. We map the feature-detected runtime image's features back to the field (this is more computationally-efficient than mapping\nthe field to the runtime image).\n\n*For field generation, we can in turn discuss the steps needed to generate such a field:\n1. Initialize\n2. Seed\n3. Connect\n4. Chain\n5. Filter\n6. Segment\n7. Propagate\nMany of the steps outlined in this field generation process were also leveraged in the PatQuick method.\nAnother important aspect of training is computing field dipoles. A few notes on this:\n- Field dipoles correspond to edge fragments.\n- Field dipoles are created as a data structure of flags that provide information about proximity to other components, such\nas the edge.\nSome other notes on this framework:\n- Edge detection is largely the same procedure that we have seen in the previous patents (e.g. PatQuick). However, note\nthat because this framework seeks to obtain highly-precise estimates accurate to the sub-pixel level, PatMAx does not use\nCORDIC or quantized gradient directions.\n- Field dipoles are computed during training.\n- The chaining procedure used in PatMAx is similar to the process we saw before: (i) Link chains, and then (ii) Remove\nshort (weak) chains.\n- For initialization, the array contains a vector field, but the vectors do not cover the entire array.\nWe will now explore some specific elements of this framework:\n14.1.3\nEstimating Other Pixels\n- To estimate other pixels, we need to fill in pixels near the edge in an iterative fashion.\n- To accomplish this, PatMAx uses a distance map, which is common in machine vision applications.\n- We can compute the distance to the edge accurately with Manhattan distance, but we use Euclidean distance, which is\nnon-trivial to compute, particularly, as we will see shortly, for corner edges.\n- Intuitively, we want the system to be drawn to a \"lower energy state\", hence the idea of this algorithm being an energy\nminimization algorithm.\n- Identical copies can be resolved via averaging.\n14.1.4\nAttraction Module\nThe diagram of the attraction module is given below:\n\nFigure 10: The Attraction module for the PatMAx system. Note that this produces a refined estimate of the pose at the output,\nwhich is one of the main goals of the PatMAx system.\nIntuition with Mechanical Springs: Scaling adjustments via scaled transformations can be conceptualized as a set of\nmechanical springs (rather than electrostatic dipoles) that are adjusted until an optimal configuration of the degrees of freedom\nis found.\nSolving this system is equivalent to solving a large least squares problem:\n- Each runtime dipole has force exerted on it in a certain direction. The goal here is to use diffent movements, which comprise\nour Degrees of Freedom, to create a configuration of these dipoles that minimizes the overall energy/tension of this system.\n- The movements/degrees of freedom that are allowed: (i) Translation in 2D (2 DOF), (ii) Rotation in 2D (1 DOF), (iii)\nScaling (1 DOF). Therefore with these movements we have 4 degrees of freedom.\n- A closed-form solution of this does not exist, but we can compute a solution to this least squares problem using an upper\ntriangular matrix accumulator array. This array is scaled additionally by weights, and can also be used to compute\nimage moments.\n- With our movements (translation, rotation, and scaling), we have 4 DOF. With a full set of movements comprising affine\nlinear transformations, we have 6 DOF.\n- Local linearization around the operating point is also used when solving for the least squares solution to this problem.\n- One computational technique the authors of this patent make use of is the use of doubly linked lists for the image\ndipoles.\n14.1.5\nPatMAx Claims\nAs we have seen, another good way to get a sense of the \"abstract\" of a patent is to look through its claims. The big claim of\nPatMAx: PatMAx is a geometric pattern-matching method used for iteratively refining the estimate of the true pose (relative\nto the orientation of the object in the training image) in a runtime image.\n\n14.1.6\nComparing PatMAx to PatQuick\nTo better understand these frameworks (and how they potentially fit together for cascaded object inspection, let us draw some\ncomparisons between PatQuick and PatMAx):\n- PatQuick searched all pose space and does not require an initial guess - PatMAx does require an initial estimate/guess of\nthe pose in order to produce a refined estimate.\n- For PatMAx, there is repeated emphasis on avoiding techniques such as thresholding and quantization of gradient directions\nthat have been used in the previous patents we have looked at. This makes sense, since PatMAx aims to output a more\nrefined estimate of the pose than these other frameworks (i.e. reach sub-pixel accuracy).\n- Using \"dipoles\" for PatMAx is misguided - using physical springs as an analog is much more physically consistent.\n- For PatMAx, we use evidence collected for determining the quality of alignment, which in turn determines the quality of\nour refined pose estimate.\n- PatMAx and PatQuick each have different methods for assigning weights.\n- PatMAx is a nonlinear optimization problem, and therefore does not have a closed-form solution. PatMAx is also iterative\n- alignment quality and matched edges get closer with more iterations of optimization.\n14.1.7\nField Generation for PatMAx\nHere, we look at the field generation. For building intuition and simplifying, we will only take into account distance. See the\nfigure below for some examples of distance fields as circles, lines, and (hardest to compute) corner edges.\nFigure 11: Examples of distance fields.\nA few notes about these:\n- If we were working with Manhattan (L1-norm) distance rather than Euclidean (L2-norm) distance, this would be a\nmuch easier problem to solve, especially for edge corners. However, unfortunately this is not an option we have.\n- We can weight the forces of individual dipoles in the runtime image. Weight is computed for beliefs/evidence for (i)\nforces, (ii) torques, and (iii) scaling, where {wi}N\ni=1 is the set of weights:\n1. Forces (Translation): F =\nPN\ni=1 Fiwi\nPN\ni=1 wi\n∈R2\n2. Torques (Rotation): τ =\nPN\ni=1 wi(ri×Fi)\nPN\ni=1 wi\n∈R, where ri is the radial vector\n3. Scaling: s =\nPN\ni=1 wi(ri·Fi)\nPN\ni=1 wi\n∈R where ri is the radial vector\nTogether, these three elements composed of weighted evidence from the dipoles compose our 4 DOF.\n\n14.2\nFinding Distance to Lines\nOne application of performing this task is to improve the performance of edge detection systems by combining shorter edge\nfragments of objects into longer edge fragments.\nFor this procedure, we break this down into two steps:\n1. Rotate the 2D cartesian coordinate system into a system that is parallel to the line of interest:\nx′ = x cos θ + y sin θ\ny′ = -x sin θ + y cos θ\nI.e.\nx′\ny′\n\n=\ncos θ\nsin θ\n-sin θ\ncos θ\nx\ny\n\n2. Translate the origin to the new location to match the line:\nx′′ = x′\ny′′ = y′ -ρ\n= -x sin θ + y cos θ -ρ\nTogether, these equations imply:\ny′′ + x sin θ -y cos θ + ρ = 0\ny′′ + x′′ sin θ -y cos θ + ρ = 0\nWhich in turn has the properties:\n- There are no redundant solutions\n- The system is parameterized by (ρ, θ)\n- There are no singularities\nFrom here, we can use the above framework to find a line by minimizing the following objective over our parameterized degrees\nof freedom ρ and θ:\nρ∗, θ∗= arg min\nρ,θ\nN\nX\ni=1\n(y\n′′\ni )2\n= arg min\nρ,θ\nN\nX\ni=1\n(xi sin θ -yi cos θ + ρ)2\n∆= arg min\nρ,θ J(ρ, θ)\nThis problem can be solved through our standard calculus approaches of finding the first-order conditions of our objective J(ρ, θ)\non our degrees of freedom ρ and θ. Since we have two degrees of freedom, we have two First-Order Conditions:\n1.\n∂J(ρ,θ)\n∂ρ\n= 0:\n∂\n∂ρ(J(ρ, θ)) = ∂\n∂ρ\nN\nX\ni=1\n(xi sin θ -yi cos θ + ρ)2\n= 2\nN\nX\ni=1\n(xi sin θ -yi cos θ + ρ) = 0\n= sin θ\nN\nX\ni=1\nxi\n\n-cos θ\nN\nX\ni=1\nyi\n\n+\nN\nX\ni=1\nρ\n\n= 0\n= N x sin θ -N y cos θ + Nρ = 0\n= x sin θ - y cos θ + ρ = 0\n(Where x\n∆= 1\nN\nPN\ni=1 xi and y\n∆= 1\nN\nPN\ni=1 yi.)\n\nThough this does not give us the final answer, it does provide information on how our solution is constrained, i.e. the line\nmust pass through the centroid given by the mean ( x, y). Let us now look at the second FOC to combine insights from\nthat FOC with this FOC in order to obtain our solution.\n2.\n∂J(ρ,θ)\n∂θ\n= 0:\nBefore computing this derivative, let us move our coordinates to the centroid, i.e. subtract the mean:\nx\n′\ni = xi - x -→xi = x + x\n′\ni\ny\n′\ni = yi - y -→yi = y + y\n′\ni\nPlugging this substituted definition into our equations renders them such that the centroid cancels out. Let us now compute\nthe second FOC:\n∂\n∂θ(J(ρ, θ)) = ∂\n∂θ\nN\nX\ni=1\n(xi sin θ -yi cos θ + ρ)2\n= 2\nN\nX\ni=1\n(xi sin θ -yi cos θ + ρ)(x′ cos θ + y′ sin θ) = 0\n=\nN\nX\ni=1\nx′2 sin θ cos θ + x′y′ sin2 θ -x′y′ cos2 θ -y′2 cos θ sin θ) = 0\n=\nN\nX\ni=1\n(x2\ni -y2\ni ) sin θ cos θ =\nN\nX\ni=1\nxiyi(cos2 θ -sin2 θ) = 0\n= 1\nN\nX\ni=1\n(x2\ni -y2\ni ) sin(2θ) =\nN\nX\ni=1\nxiyi cos(2θ) = 0\n= sin(2θ)\ncos(2θ) = tan(2θ) =\n2 PN\ni=1 xiyi\nPN\ni=1(x2\ni -y2\ni )\nA few notes about this:\n- In the second-to-last step, we used the two following trigonometric identities: (i) sin θ cos θ = sin(2θ), and (ii)\ncos2 θ -sin2 θ = cos(2θ).\n- Notice that we can separate the angle θ from the sums because it does not depend on the sum index and is a degree\nof freedom in this optimization problem.\nFrom here, we can solve for the optimal value of θ (which will also constrain and give us an optimal value of ρ) by taking\nthe arctangent that returns quadrant (\"atan2\"):\nθ∗= 1\n2 arctan 2\n\nN\nX\ni=1\nxiyi,\nN\nX\ni=1\n(x2\ni -y2\ni )\n\nTherefore, solving the FOCs gives us a closed-form least squares estimate of this line parameterized by (ρ, θ). This solution,\nunlike the Cartesian y = mx + b fitting of a line, is independent of the chosen coordinate system, allowing for further flexibility\nand generalizability.\n14.3\nFast Convolutions Through Sparsity\nNext, we will switch gears and revisit multiscale, which is a common procedure needed in machine vision. Multiscale motivates\nthe need of filtering methods that are computationally-agnostic to the scale or resolution being used. Since filtering is just\nconvolution, this motivates the exploration of another patent, namely on fast convolutions. The patent we explore is \"Efficient\nFlexible Digital Filtering, US 6.457,032.\nThe goal of this system is to efficiently compute filters for multiscale. For this, we assume the form of an Nth-order piece-\nwise polynomial, i.e. a Nth-order spline.\n\n14.3.1\nSystem Overview\nThe block diagram of this system can be found below:\nFigure 12: Block diagram of this sparse/fast convolution framework for digital filtering. Note that this can be viewed as a\ncompression problem, in which differencing compresses the signal, and summing decompresses the signal.\nA few notes on this system:\n- Why is it of interest, if we have Nth-order splines as our functions, to take Nth-order differences? The reason for this is\nthat the differences create sparsity, which is critical for fast and efficient convolution. Sparsity is ensured because:\ndN+1\ndxN+1 f(x) = 0 ∀x if f(x) =\nN\nX\ni=0\naixi, ai ∈R ∀i ∈{1, · · · , N}\n(I.e, if f(x) is a order-N polynomial, then the order-(N+1) difference will be 0 for all x.\nThis sparse structure makes convolutions much easier and more efficient to compute by reducing the size/cardinality of the\nsupport (we will discuss what a support is in greater detail in the next lecture, as well as how the size of a support affects\ncomputational efficiency, but effectively the support is the subset of the domain of a function that is not mapped to zero).\n- Why do we apply an order-(N+1) summing operator? We apply this because we need to \"invert\" the effects of the order-\n(N+1) difference. Intuitively, this makes sense that the order-(N+1) difference and the order-(N+1) sum commute, because\nwe are simply performing iterative rounds of subtraction and addition (respectively), which we know are commutative\nalgebraic operations. I.e, representing differencing and summing as linear operators where their order is the same, we have:\nFirst Order : DS = I\nSecond Order : DDSS = DSDS = (DS)(DS) = II = I\n...\nOrder K : (D)K(S)K = (DS)K = IK = I\n\n14.3.2\nIntegration and Differentiation as Convolutions\nConceptualizing these differencing/differentiation and summing/integration as linear operators that are commutative and asso-\nciative, we can then extend this framework to conceptualizing these operators as convolutions:\n- Integration: This corresponds to the convolution of our piecewise polynomial f(x) with a unit step function u(x).\n- Differentiation: This corresponds to the convolution of our piecewise polynomial f(x) with two scaled impulses in\nopposite directions:\n2δ(x + ε\n2) + 1\n2δ(x -ε\n2).\nThis motivates the discussion of some of the properties of convolution this system relies on in order to achieve high performance.\nFor operators A, B, and C, we have that:\n1. Commutativity: A ⊗B = B ⊗A\n2. Associativity: A ⊗(B ⊗C) = (A ⊗B) ⊗C\nThese properties stem from the fact that in the Fourier domain, convolution is simply multiplication. Therefore, convolution\nobeys all the algebraic properties of multiplication.\n14.3.3\nSparse Convolution as Compression\nAs aforementioned, another way to consider this efficient convolution system is as a compression mechanism, in which the\ndifferencing operation acts as a compression mechanism, and the summing operation acts as a decompression mechanism.\nWe can contrast this with standard convolution in the block diagram below. This sparse convolution approach ends up being a\nmuch more efficient way to filter a signal.\nFigure 13: Comparison of standard filtering and efficient/sparse filtering procedures, where the sparse filtering approach is\nillustrated as a compression problem. Here, H represents the filter, X and Y represent the uncompressed inputs and outputs,\nrespectively, and x and y represent the compressed inputs and outputs.\n14.3.4\nEffects on Scaling\nThis fast/efficient convolution framework yields desirable results for scaling, and, as we will see shortly, for problems in which\nmultiscale approaches are necessary. Because the only places in which we need to do work are now the locations where the\npiecewise polynomial segments are stitched together, the scaling can be changed to coarser and finer levels without affecting the\namount of computation that is required! This improves the efficiency of multiscale approaches.\n\n14.3.5\nFiltering (For Multiscale): Anti-Aliasing\nWe have now reduced the amount of computation needed to compute efficient digital filtering. We now only need final ingredient\nfor multiscale that is motivated by Shannon and Nyquist: anti-aliasing methods (filters).\nRecall from Shannon/Nyquist (the Sampling Theorem) that in order to sample (for instance, when we subsample in multi-\nscale problems) without aliasing and high-frequency artifacts, it is critical that we first remove high-frequency components from\nthe signal we are sampling. This high-frequency component removal can be achieved with approximate low pass filtering (which\nwe will cover in greater detail during the next lecture).\nWe will see in the next lecture that one way we can achieve approximate low pass filtering is by approximating a spatial\nsinc function (which transforms into an ideal low pass filter in the frequency domain) as a spline.\n14.3.6\nExtending Filtering to 2D and An Open Research Problem\nThis framework is built for 1D, but we note that we can extend it to 2D simply by approximating 2D convolution as a cascaded\nset of 1D convolutions, if we are to continue using this sparse convolution mechanism. This requires some additional run-time\nmemory as we must store an image corresponding to the 1D convolution of the image with a 1D filter, but this allows us to\ncontinue using this efficient sparse convolution structure.\nOne open research problem: how can we extend this sparse convolution structure to 2D?\nFinally, we will finish this lecture with a fun fact on calcite crystals. Calcite crystals are a type of birefringent mate-\nrial, which means that they have two indices of refraction that depend on two polarizations (one in the x-direction and one\nin the y-direction), and therefore reflect light into two different ways. As we will see in the next lecture, adding birefringent\nlenses in the analog domain can prevent aliasing affects from occurring that would otherwise be unavoidable. DSLRs have these\nbirefringent lenses affixed to them for this specific anti-aliasing purpose.\nLecture 16: Fast Convolution, Low Pass Filter Approximations, Integral\nImages, (US 6,457,032)\n15.1\nSampling and Aliasing\nSampling is a ubiquitous operation for machine vision and general signal processing. Recall that PatMax, for instance, uses\nsampling in its methodology. PatMax also performs an interesting operation: low-pass filtering before sampling. Why is this\nperformed? To answer this question, let us revisit the Nyquist Sampling Theorem.\n15.1.1\nNyquist Sampling Theorem\nBefore we dive into the mathematics behind this theorem, let us first build some intuition surrounding this theory.\n- If we can sample a signal at a high enough frequency, we can recover the signal exactly through reconstruction.\n- How is this reconstruction performed? We will convolve samples from the signal with sinc functions, and then superimpose\nthese convolved results with one another.\n- It is hard to sample from a signal with infinite support.\n- What frequency do we need for this? Intuitively, to pick out how fast the signal needs to be moving, we certainly need to\nsample as quickly as the signal's fastest-varying component itself. But do we need to sample even faster? It turns out the\nanswer is yes. As we will see below:\nfmax < fsample\n=⇒fsample > 2fmax\nI.e. we will need to sample at more than twice the frequency of the highest-varying component of the signal.\nLet us look at this graphically. What happens if we sample at the frequency of the signal?\n\n-2\n-1\nx\nf(x)\nCosine Function, cos(x)\nTrue Function\nInterpolated Function\nFigure 14: Sampling only once per period provides us with a constant interpolated function, from which we cannot recover the\noriginal. Therefore, we must sample at a higher frequency.\nNote that this holds at points not on the peaks as well:\n-2\n-1\nx\nf(x)\nCosine Function, cos(x)\nTrue Function\nInterpolated Function\nFigure 15: Sampling only once per period provides us with a constant interpolated function, from which we cannot recover the\noriginal. Therefore, we must sample at a higher frequency.\nWhat if we sample at twice the frequency? I.e. peaks and troughs:\n\n-2\n-1\nx\nf(x)\nCosine Function, cos(x)\nTrue Function\nInterpolated Function\nFigure 16: Sampling at twice the rate as the highest-varying component almost gets us there! This is known as the Nyquist\nRate. It turns out we need to sample at frequencies that are strictly greater than this frequency to guarantee no aliasing - we\nwill see why in the example below.\nIs this good enough? As it turns out, the inequality for Nyquist's Sampling Theorem is there for a reason: we need to sample\nat greater than twice the frequency of the original signal in order to uniquely recover it:\n-2\n-1\nx\nf(x)\nCosine Function, cos(x)\nTrue Function\nInterpolated Function\nFigure 17: It turns out we need to sample at frequencies that are strictly greater than this frequency to guarantee no aliasing -\nwe will see why in the example below.\nTherefore, any rate above 2 times the highest-varying frequency component of the signal will be sufficient to completely avoid\naliasing. As a review, let us next discuss aliasing.\n15.1.2\nAliasing\nAliasing occurs when higher frequencies become indistinguishable from lower frequencies, and as a result they add interference\nand artifacts to the signal that are caused by sampling at too low of a frequency.\nSuppose we have a signal given by:\nf(x) = cos(2πf0x)\nAnd suppose we sample this signal with frequency given by fs = 1\nδ . Then our sampled signal is given by:\nsk = cos(2πf0k 1\nδ ) = cos(2π f0\nfs\nk) ∀k ∈{1, 2, ...}\n\nNow let us consider what happens when we add multiples of 2π to this:\nsk-2π = cos\n\n2π f0\nfs\nk -2πk\n\n= cos\n\n2π\nf0\nfs\n-1\n\nk\n\n= cos\n\n2π\nf0 -fs\nfs\n\nk\n\n= cos\n\n2π\nfs -f0\nfs\n\nk\n\n, since cos(x) = cos(-x) ∀x ∈R\nAnother way to put this - you cannot distinguish multiples of base frequencies with the base frequencies themselves if you sample\nat too low a frequency, i.e. below the Nyquist Rate.\n15.1.3\nHow Can We Mitigate Aliasing?\nThere are several strategies to mitigate aliasing effects:\n- (Note) Anti-aliasing measures must be taken before sampling. After sampling occurs, information is \"lost\", so to speak,\nand the original signal cannot be recovered.\n- High frequency noise suppression with (approximate) low-pass filtering. As it turns out, exact lowpass filtering is impossible\ndue to convergence properties of Fourier Series at cutofffrequencies (a phenomenon known as the Gibbs Phenomenon\n[1].\nWe will touch more on these anti-aliasing techniques and strategies throughout this lecture.\n15.2\nIntegral Image\nNext, we will shift gears somewhat to discuss the notion of an integral image, and the critical role this technique plays in\nimproving computational efficiency in image processing and machine vision. We will disucss this concept in both 1D and 2D.\n15.2.1\nIntegral Images in 1D\nBlock averaging is a common operation in computer vision in which we take the average over a set of values across an entire\nvector (1D) or matrix (2D), such as an image. This involves summing and then dividing by the total number of elements, which\ncan become prohibitively computationally-expensive, particularly if this operation is being called many times and the averages\nthat we need to compute are very large. Is there a more computationally-efficient way to do this?\nIt turns out this computationally-simpler solution is through integral images.\nAn integral image is essentially the sum of\nvalues from the first value to the ith value, i.e if gi defines the ith value in 1D, then:\nGi\n∆=\ni\nX\nk=1\ngk ∀i ∈{1, · · · , K}\nWhy is this useful? Well, rather than compute averages (normalized sums) by adding up all the pixels and then dividing, we\nsimply need to perform a single subtraction between the integral image values (followed by a division by the number of elements\nwe are averaging). For instance, if we wanted to calculate the average of values between i and j, then:\ng[i,j] =\nj -i\nj\nX\nk=i\ngk =\nj -i(Gj -Gi)\nThis greatly reduces the amortized amount of computation, because these sums only need to be computed once, when we\ncalculate the initial values for the integral image.\n15.2.2\nIntegral Images in 2D\nNow, we can extend the notion of an integral image to 2D! Note further that integral \"images\" can extend beyond images! E.g.\nthese can be done with gradients, Jacobians, Hessians, etc. One example in particular is calculating a Histogram of Gradients\n(HoG), which is quite useful for feature matching algorithms such as Scale-Invariant Feature Transform (SIFT). These approaches\nalso map nicely onto GPUs, enabling for even faster computation times.\n\nLet us now see how block averaging looks in 2D - in the diagram below, we can obtain a block average for a group of pix-\nels in the 2D range (i, j) in x and (k, l) in y using the following formula:\ng([i,j],[k,l]) =\n(j -i)(l -k)\nj\nX\nx=i\nl\nX\ny=k\ngx,y\nBut can we implement this more efficiently? We can use integral images again:\nGi,j =\ni\nX\nk=1\nj\nX\nl=1\ngk,l\nReferencing the figure below, this becomes:\nFigure 18: Block averaging using integral images in 2D. As pointed out above, block averaging also extends beyond pixels! This\ncan be computed for other measures such as gradients (e.g. Histogram of Gradients).\nUsing the integral image values, the block average in the 2D range (i, j) in x and (k, l) in y becomes:\ng([i,j],[k,l]) =\n(j -i)(l -k)\n\n(Gj,l + Gi,k) -(Gi,l + Gj,k)\n\nSome comments about this:\n- This analysis can be extended to higher dimensions as well! Though the integral image will take longer to compute, and\nthe equations for computing these block averages become less intuitive, this approach generalizes to arbitrary dimensions.\n- As we saw in the one-dimensional case, here we can also observe that after computing the integral image (a one-time\noperation that can be amortized), the computational cost for averaging each of these 2D blocks becomes independent\nof the size of the block being averaged.\nThis stands in stark contrast to the naive implementation - here, the\ncomputational cost scales quadratically with the size of the block being averaged (or linearly in each dimension, if we take\nrectangular block averages).\n- Why is this relevant?\nRecall that block averaging implements approximate lowpass filtering, which can be used as a\nfrequency suppression mechanism to avoid aliasing when filtering.\n- In other domains outside of image processing, the integral image is known as the \"Summed-area Table\" [2].\nSince we intend to use this for approximate lowpass filtering, let us now change topics toward fourier analysis of this averaging\nmechanism to see how efficacious it is.\n15.3\nFourier Analysis of Block Averaging\nLet us consider a one-dimensional \"filter\" that implements an approximate lowpass filtering for mechanism through block\naveraging. Let us consider a function such that it takes 0 value outside of the range (--δ\n2 , δ\n2), and value 1\nδ inside this range:\nh(x) =\n(\nδ\nx ∈(--δ\n2 , δ\n2)\no.w.\nVisually:\n\n-2\n-1\n0.2\n0.4\n0.6\nx\nh(x)\nBlock averaging h(x) for δ = 2\nFigure 19: Example h(x) for δ = 2.\nLet's see what this Fourier Transform looks like. Recall that the Fourier Transform (up to a constant scale factor, which varies\nby domain) is given by:\nF(jω) =\nZ inf\n-inf\nf(x)e-jωxdx\nWhere jω corresponds to complex frequency. Substituting our expression into this transform:\nH(jω) =\nZ inf\n-inf\nh(x)e-jωxdx\n=\nZ\nδ\n-δ\nδ e-jωxdx\n= 1\nδ\njω [e-jωx]\nx= δ\nx=-δ\n= e-jωδ\n2 -e\njωδ\n-jωδ\n=\nsin\n\nδω\n\nδω\n(Sinc function)\nWhere in the last equality statement we use the identity given by:\nsin(x) = ejx -e-jx\n-2j\nGraphically, this sinc function appears (for δ = 2):\n\n-8\n-6\n-4\n-2\n0.5\njω\nH(jω)\nSinc function H(jω) =\nsin\n\nδω\n\nδω\nFigure 20: Example H(jω) for δ = 2. This is the Fourier Transform of our block averaging \"filter\".\nAlthough sinc functions in the frequency domain help to attenuate higher frequencies, they do not make the best lowpass filters.\nThis is the case because:\n- Higher frequencies are not completely attenuated.\n- The first zero is not reached quickly enough. The first zero is given by:\nω0δ\n= π\n2 =⇒ω0 = π\nδ\nIntuitively, the best lowpass filters perfectly preserve all frequencies up to the cutofffrequencies, and perfectly attenuate every-\nthing outside of the passband. Visually:\n-15\n-10\n-5\n0.5\njω\nH(jω)\nSinc function H(jω) =\nsin\n\nδω\n\nδω\nSinc Filter\nIdeal lowpass filter\nFigure 21: Frequency response comparison between our block averaging filter and an ideal lowpass filter. We also note that the\n\"boxcar\" function and the sinc function are Fourier Transform pairs!\nAlthough sinc functions in the frequency domain help to attenuate higher frequencies, they do not make the best lowpass filters.\nThis is the case because:\n- Higher frequencies are not completely attenuated.\n\n- The first zero is not reached quickly enough. The first zero is given by:\nω0δ\n= π\n2 =⇒ω0 = π\nδ\nWhere else might we see this? It turns out cameras perform block average filtering because pixels have finite width over which\nto detect incident photons. But is this a sufficient approximate lowpass filtering technique? Unfortunately, oftentimes it is not.\nWe will see below that we can improve with repeated block averaging.\n15.4\nRepeated Block Averaging\nOne way we can improve our ability to attenuate higher frequencies - repeated block averaging! If our \"boxcar\" filter given above\nis given by b(x) (note that this was h(x) above), then our previous result y(x) can be written as:\ny1(x) = f(x) ⊗b(x)\nb(x)\nf(x)\ny1(x)\nWhat happens if we add another filter? Then, we simply add another element to our convolution:\ny2(x) = (f(x) ⊗b(x)) ⊗b(x) = y1(x) ⊗b(x)\nb(x)\nb(x)\nf(x)\ny1(x)\ny2(x)\nAdding this second filter is equivalent to convolving our signal with the convolution of two \"boxcar\" filters, which is a triangular\nfilter:\n-3\n-2\n-1\n0.2\n0.4\n0.6\n0.8\nx\nh(x)\nTriangular filter for δ = 2.\nFigure 22: Example of a triangular filter resulting from the convolution of two \"boxcar\" filters.\nAdditionally, note that since convolution is associative, for the \"two-stage\" approximate lowpass filtering approach above, we\ndo not need to convolve our input f(x) with two \"boxcar\" filters - rather, we can convolve it directly with our trinagular filter\nb2(x) = b(x) ⊗b(x):\ny2(x) = (f(x) ⊗b(x)) ⊗b(x)\n= f(x) ⊗(b(x) ⊗b(x))\n= f(x) ⊗b2(x)\nLet us now take a brief aside to list out how discontinuities affect Fourier Transforms in the frequency domains:\n- Delta Function: δ(x)\nF\n←→1\nIntuition: Convolving a function with a delta function does not affect the transform, since this convolution simply\nproduces the function.\n\n- Unit Step Function: u(x)\nF\n←→\njω\nIntuition: Convolving a function with a step function produces a degree of averaging, reducing the high frequency\ncomponents and therefore weighting them less heavily in the transform domain.\n- Ramp Function: r(x)\nF\n←→-1\nω2\nIntuition: Convolving a function with a ramp function produces a degree of averaging, reducing the high frequency\ncomponents and therefore weighting them less heavily in the transform domain. Derivative:\nd\ndxf(x)\nF\n←→jωF(jω)\nIntuition: Since taking derivatives will increase the sharpness of our functions, and perhaps even create discontinuities, a\nderivative in the spatial domain corresponds to multiplying by jω in the frequency domain.\nAs we can see from above, the more \"averaging\" effects we have, the more the high-frequency components of the signal will be\nfiltered out. Conversely, when we take derivatives and create discontinuities in our spatial domain signal, this increases high\nfrequency components of the signal because it introduces more variation.\nTo understand how we can use repeated block averaging in the Fourier domain, please recall the following special properties of\nFourier Transforms:\n1. Convolution in the spatial domain corresponds to multiplication in the frequency domain, i.e.\nfor all\nf(x), g(x), h(x) with corresponding Fourier Transforms F(jω), G(jω), H(jω), we have:\nh(x) = f(x) ⊗g(x)\nF\n←→H(jω) = F(jω)G(jω)\n2. Multiplication in the spatial domain corresponds to convolution in the frequency domain, i.e.\nfor all\nf(x), g(x), h(x) with corresponding Fourier Transforms F(jω), G(jω), H(jω), we have:\nh(x) = f(x)g(x)\nF\n←→H(jω) = F(jω) ⊗G(jω)\nFor block averaging, we can use the first of these properties to understand what is happening in the frequency domain:\ny2(x) = f(x) ⊗(b(x) ⊗b(x))\nF\n←→Y (jω) = F(jω)(B(jω)2)\nThis operation is equivalent to a sinc2 function in the spatial domain:\n-8\n-6\n-4\n-2\n0.2\n0.4\n0.6\n0.8\njω\nH2(jω)\nSinc function Squared H2(jω) =\nsin\n\nδω\n\nδω\nFigure 23: Example H2(jω) for δ = 2. This is the Fourier Transform of our block averaging \"filter\" convolved with itself in the\nspatial domain.\n.\n\nThis is not perfect, but it is an improvement. In fact, the frequencies with this filter drop offwith magnitude\n\nω)2. What\nhappens if we continue to repeat this process with more block averaging filters? It turns out that for N \"boxcar\" filters that we\nuse, the magnitude will drop offas\n\nω\nN\n. Note too, that we do not want to go \"too far\" in this direction, because this repeated\nblock averaging process will also begin to attenuate frequencies in the passband of the signal.\n15.4.1\nWarping Effects and Numerical Fourier Transforms: FFT and DFT\nTwo main types of numerical transforms we briefly discuss are the Discrete Fourier Transform (DFT) and Fast Fourier Transform\n(FFT). The FFT is an extension of the DFT that relies on using a \"divide and conquer\" approach to reduce the computational\nruntime from f(N) ∈O(N 2) to f(N) ∈O(N log N) [3].\nMathematically, the DFT is given as a transform that transforms a sequence of N complex numbers {xn}N\nn=1 into another\nsequence of N complex numbers {Xk}N\nk=1 [4]. The transform for the Kth value of this output sequence is given in closed-form\nas:\nXk =\nN\nX\ni=1\nxne-j 2π\nN kn\nAnd the inverse transform for the nth value of this input sequence is given as:\nxn =\nN\nX\nk=1\nXkej 2π\nN kn\nOne aspect of these transforms to be especially mindful of is that they introduce a wrapping effect, since transform values are\nspread out over 2π intervals. This means that the waveforms produced by these transforms, in both the spatial (if we take the\ninverse transform) and frequency domains may be repeated - this repeating can introduce undesirable discontinuities, such as\nthose seen in the graph below:\nx\nf(x)\nRepeated Function x2\nFigure 24: Example of a repeated waveform that we encounter when looking at DFTs and FFTs.\nFun fact: It used to be thought that natural images had a power spectrum (power in the frequency domain) that falls offas 1\nω.\nIt turns out that this was actually caused by warping effects introduced by discrete transforms.\nThis begs the question - how can we mitigate these warping effects? Some methods include:\n- Apodizing: This corresponds to multiplying your signal by a waveform, e.g. Hamming's Window, which takes the form\nakin to a Gaussian, or an inverted cosine.\n- Mirroring: Another method to mitigate these warping effects is through waveform mirroring - this ensures continuity at\npoints where discontinuties occurred:\n\nx\nf(x)\nRepeated Function x2\nFigure 25: Example of a mirrored waveform that we can use to counter and mitigate the discontinuity effects of warping\nfrom transforms such as the DFT and FFT.\nWith this approach, the power spectrum of these signals falls offat\nω2 , rather than 1\nω.\n- Infinitely Wide Signal: Finally, a less practical, but conceptual helpful method is simply to take an \"infinitely wide\nsignal\".\nLet us now switch gears to talk more about the unit impulse and convolution.\n15.5\nImpulses and Convolution\nIn this section, we will review impulse functions, and discuss how they relate to many properties with convolution. We will begin\nby reviewing delta functions and their properties.\n15.5.1\nProperties of Delta Functions\nRecall that the Dirac delta function δ(x), or impulse function, is defined according to the two properties:\n1. Unit Area:\nR inf\n-infδ(x)dx = 1\n2. Sifting Property: f(x0) =\nR inf\n-inff(x)δ(x -x0)dx\nAnother way to conceptualize delta functions is through probabilistic distributions. We can use Gaussians (one of the only\ndistributions to have a Fourier Transform). Recall that the (zero-mean) Gaussian Fourier Transform pair is given by:\nσ\n√\n2π e-\nx\n2σ2\nF\n←→e-ω2σ2\nAn impulse can be conceptualized as the limit in which the variance of this Gaussian distribution σ2 goes to 0, which corresponds\nto a Fourier Transform of 1 for all frequencies (which is the Fourier Transform of a delta function).\nAnother way to consider impulses is that they are the limit of \"boxcar\" functions as their width goes to zero.\nLet us next generalize from a single impulse function to combinations of these functions.\n15.5.2\nCombinations of Impulses\nWhen are combinations of impulses helpful? it turns out that one particular combination can be used for approximating the\nderivative using our prior work on finite differences:\nh(x) = 1\nε\n\nδ(x + ε\n2) -δ(x -ε\n2)\n\nfor some ε > 0\n\nCorrelating (*note that this is not convolution - if we were to use convolution, this derivative would be flipped) this combination\nof impulse \"filter\" with an arbitrary function f(x), we compute a first-order approximation of the derivative:\nf ′(x) ≈\nZ inf\n-inf\nf(x)h(x)dx\n=\nZ inf\n-inf\nε\n\nδ(x + ε\n2) -δ(x -ε\n2)\n\ndx\nTherefore, combinations of impulses can be used to represent the same behavior as the \"computational molecules\" we identified\nbefore. It turns out that there is a close connection between linear, shift-invariant operators and derivative operators.\nWith impulses motivated, let us now formally review convolution.\n15.5.3\nConvolution Review\nRecall from the example above that the convolution operation is simply the result of the correlation operation flipped. Mathe-\nmatically, the convolution of functions f(x) and h(x) is given by the following commutative operation:\ng(x) = f(x) ⊗h(x)\n=\nZ inf\n-inf\nf(ξ)h(x -ξ)dξ\n= h(x) ⊗g(x)\n=\nZ inf\n-inf\nh(ξ)f(x -ξ)dξ\n15.5.4\nAnalog Filtering with Birefringent Lenses\nWhy filter in the analog domain? Returning to our original problem of mitigating aliasing artifacts through high-frequency\nsuppression, unfortunately, if we wait to digitally filter out high frequencies after the image has already been taken by a camera\nor sensor, then we have already caused aliasing.\nOne way to achieve this analog filtering is through Birefringent Lenses.\nHere, we essentially take two \"shifted\" images\nby convolving the image with a symmetric combination of offset delta functions, given mathematically by:\nh(x) = 1\n2δ(x + ε\n2) + 1\n2δ(x -ε\n2) for some ε > 0\nLet us look at the Fourier Transform of this filter, noting the following Fourier Transform pair:\nδ(x -x0)\nF\n←→e-jωx0\nWith this we can then express the Fourier Transform of this filter as:\nF(jω) =\n√\n2π\nZ inf\n-inf\nh(x)e-jωxdx\n= 1\n\ne-jωε\n2 + e\njωε\n\n= cos\nωε\n\nWith this framework, the first zero to appear here occurs at ω0 = π\nε . A few notes about these filters, and how they relate to\nhigh-frequency noise suppression.\n- When these birefringent lenses are cascaded with a block averaging filter, this results in a combined filtering scheme in\nwhich the zeros of the frequency responses of these filters cancel out most of the high-frequency noise.\n- In the 2D case, we will have 2 birefringent filters, one for the x-direction and one for the y-direction. Physically, these are\nrotated 90 degrees offfrom one another, just as they are for a 2D cartesian coordinate system.\n\n- High-performance lowpass filtering requires a large support (see definition of this below if needed) - the computational\ncosts grow linearly with the size of the support in 1D, and quadratically with the size of the support in 2D. The support\nof a function is defined as the set where f(·) is nonzero [5]:\nsupp(f) = {x : f(x) = 0, x ∈R}\n- Therefore, one way to reduce the computatonal costs of a filtering system is to reduce the size/cardinality of the support\n|supp(f)| - in some sense to encourage sparsity. Fortunately, this does not necessarily mean looking over a narrower range,\nbut instead just considering less points overall.\n15.5.5\nDerivatives and Integrals as Convolution Operators and FT Pairs\nAs we have seen before, convolving a function with a unit step function results in integrating the given function. Let us verify\nthis below:\nf(x) ⊗u(x) =\nZ inf\n-inf\nf(ξ)u(x -ξ)dξ\n=\nZ inf\n-inf\nf(x -ξ)u(ξ)dξ\n=\nZ inf\nf(ξ)dξ\nTherefore, we can represent integral and derivative operators as Fourier Transform pairs too, denoted S for integration and D\nfor derivative:\n- S\nF\n←→\njω\n- D\nF\n←→jω\nNiote that we can verify this by showing that convolving these filter operators corresponds to multiplying these transforms in\nfrequency space, which results in no effect when cascaded together:\n(f(x) ⊗D) ⊗S = f(x) ⊗(D ⊗S)\nF\n←→F(jω)\n\njω 1\njω\n\n= F(jω)\nF\n←→f(x)\nS\nD\nf(x)\nR inf\nf(ξ)dξ\nf(x)\nD\nS\nf(x)\nd\ndxf(x)\nf(x)\nCan we extend this to higher-order derivatives? It turns out we can. One example is the convolution of two derivative operators,\nwhich becomes:\nh(x) = δ(x + ε\n2) -2δ(x) + δ(x -ε\n2) = D ⊗D\nF\n←→H(jω) = D(jω)2 = (jω)2 = -ω2 (Recall that j2 = -1)\nIn general, this holds. Note that the number of integral operators S must be equal to the number of derivative operators D, e.g.\nfor K order:\n\n⊗K\ni=1 S\n\n⊗\n\n⊗K\ni=1 D\n\n⊗f(x)\n\n15.5.6\nInterpolation and Convolution\nA few notes about interpolation methods:\n- These methods work well computationally when there is sparsity in the operators we with - for instance, cubic or linear\ninterpolation. Photoshop and other photo-editing software frameworks iuse this interpolation techniques. Performance\ndeteriorates when we switch from cubic interpolation to Nearest Neighbor interpolation.\n- The bicubic spline is a popular interpolation technique. It was invented by IBM and was used in early satellite image\nprocessing. However, it requires many neighboring pixels, as it is composed of 7 points to interpolate over, so it is less\ncomputationally-efficient.\n\n- Recall that one key element of computational efficiency we pursue is to use integral images for block averaging, which is\nmuch more efficient than computing naive sums, especially if (1) This block averaging procedure is repeated many times\n(the amortized cost of computing the integral image is lessened) and (2) This process is used in higher dimensions.\n- Linear interpolation can be conceptualized as connecting points together using straight lines between points.\nThis\ncorresponds to piecewise-linear segments, or, convolution with a triangle filter, which is simply the convolution of two\n\"boxcar filters\":\nf(x) = f(1)x + f(0)(1 -x)\nUnfortunately, one \"not-so-great\" property of convolving with triangular filters for interpolation is that the noise in the\ninterpolated result varies depending on how far away we are from the sampled noise.\n- Nearest Neighbor techniques can also be viewed through a convolutional lens - since this method produces piecewise-\nconstant interpolation, this is equivalent to convolving our sampled points with a \"boxcar\" filter!\n15.5.7\nRotationally-Symmetric Lowpass Filter in 2D\nWhere u and v are the spatial frequencies of x and y, i.e. the 2D Fourier Transform and Inverse Fourier Transform then take\nthe forms of:\nF(u, v) =\n√\n2π\nZ inf\n-inf\nZ inf\n-inf\nf(x, y)e-j(ux+vy)dxdy\nf(x, y) =\n√\n2π\nZ inf\n-inf\nZ inf\n-inf\nF(u, v)ej(ux+vy)dudv\nThe inverse transform of this can be thought of as a sinc function in polar coordinates:\nf(ρ, θ) = B2\n2π\nJ1(ρB)\nρB\nA few notes about this inverse transform function:\n- This is the point spread function of a microscope.\n- J1(·) is a 1st-order Bessel function.\n- This relates to our defocusing problem that we encountered before.\n- In the case of defocusing, we can use the \"symmetry\" property of the Fourier Transform to deduce that if we have a circular\npoint spread function resulting from defocusing of the lens, then we will have a Bessel function in the frequency/Fourier\ndomain.\n- Though a pointspread function is a \"pillbox\" in the ideal case, in practice this is not perfect due to artifacts such as lens\naberrations.\n15.6\nReferences\n1. Gibbs Phenomenon, https://en.wikipedia.org/wiki/Gibbs phenomenon\n2. Summed-area Table, https://en.wikipedia.org/wiki/Summed-area table\n3. Fast Fourier Transform, https://en.wikipedia.org/wiki/Fast Fourier transform\n4. Discrete Fourier Transform, https://en.wikipedia.org/wiki/Discrete Fourier transform\n5. Support, https://en.wikipedia.org/wiki/Support (mathematics)\nLecture 17: Photogrammetry, Orientation, Axes of Inertia, Symmetry, Ab-\nsolute, Relative, Interior, and Exterior Orientation\nThis lecture marks a transition in what we have covered so far in the course from lower-level machine vision to higher-level\nproblems, beginning with photogrammetry.\nPhotogrammetry: \"Measurements from Imagery\", for example, map-making.\n\n16.1\nPhotogrammetry Problems: An Overview\nFour important problems in photogrammetry that we will cover are:\n- Absolute Orientation 3D ←→3D\n- Relative Orientation 2D ←→2D\n- Exterior Orientation 2D ←→3D\n- Intrinsic Orientation 3D ←→2D\nBelow we discuss each of these problems at a high level. We will be discussing these problems in greater depth later in this and\nfollowing lectures.\n16.1.1\nAbsolute Orientation\nWe will start with covering absolute orientation. This problem asks about the relationship between two or more objects\n(cameras, points, other sensors) in 3D. Some examples of this problem include:\n1. Given two 3D sensors, such as lidar (light detection and ranging) sensors, our goal is to find the transformation, or pose,\nbetween these two sensors.\n2. Given one 3D sensor, such as a lidar sensor, and two objects (note that this could be two distinct objects at a single point\nin time, or a single object at two distinct points in time), our goal is to find the transformation, or pose, between the\ntwo objects.\n16.1.2\nRelative Orientation\nThis problem asks how we can find the 2D ←→2D relationship between two objects, such as cameras, points, or other sensors.\nThis type of problem comes up frequently in machine vision, for instance, binocular stereo.\nTwo high-level applications\ninclude:\n1. Given two cameras/images that these cameras take, our goal is to extract 3D information by finding the relationship\nbetween two 2D images.\n2. Given two cameras, our goal is to find the (relative) transformation, or pose, between the two cameras.\n16.1.3\nExterior Orientation\nThis photogrammetry problem aims from going 2D -→3D. One common example in robotics (and other field related to machine\nvision) is localization, in which a robotic agent must find their location/orientation on a map given 2D information from a\ncamera (as well as, possibly, 3D laser scan measurements).\nMore generally, with localization, our goal is to find where we are and how we are oriented in space given a 2D image\nand a 3D model of the world.\n16.1.4\nInterior Orientation\nThis photogrammetry problem aims from going 3D -→2D. The most common application of this problem is camera calibra-\ntion. Camera calibration is crucial for high-precision imaging, as well as solving machine and computer vision problems such as\nBundle Adjustment [1]. Finding a principal point is another example of the interior orientation problem.\n16.2\nAbsolute Orientation\nWe will begin our in-depth discussion and analysis of these four problems with absolute orientation. Let us start by introducing\nbinocular stereo.\n\n16.2.1\nBinocular Stereopsis\nTo motivate binocular stereo, we will start with a fun fact: Humans have ≈12 depth cues. One of these is binocular stereopsis,\nor binocular stereo (binocular ≈two sensors).\nBinocular stereo is given by the figure below:\nFigure 26: The problem of binocular stereo. Having two 2D sensors enables us to recover 3D structure (specifically, depth) from\nthe scene we image. A few key terms/technicalities to note here: (i) The origin is set to be halfway between the two cameras,\n(ii) The distance between the cameras is called the baseline, and (iii) binocular disparity refers to the phenomenon of having\neach camera generate a different image. Finally, note that in practice, it is almost impossible to line up these cameras exactly.\nGoal: Calculate X and Z. This would not be possible with only monocular stereo (monocular ≈one sensor). Using similar\ntriangles, we have:\nX -b\nZ\n= xl\nf\nX + b\nZ\n= xr\nf\n-→xr -xl\nf\n= b\nz =⇒z =\nbf\nxr -xl\nWhere the binocular disparity is given by (xr -xl). Solving this system of equations becomes:\nX = bxr + xl\nxr -xl\n, Z = bf\nxr -xl\nNote that, more generally, we can calculate in the same way as well! From these equations, we can see that:\n- Increasing the baseline increases X and Z (all things equal)\n- Increasing the focal length increases Z.\nBut it is important to also be mindful of system constraints that determine what range, or the exact value, of what these variables\ncan be. For instance, if we have a self-driving car, we cannot simply have the baseline distance between our two cameras be 100\nmeters, because this would require mounting the cameras 100 meters apart.\n16.2.2\nGeneral Case\nAll of the methods we have looked at so far assume we have already found \"interesting points\", for instance, through feature\ndetection algorithms (also known as \"descriptors\") such as SIFT, SURF, ORB, or FAST+BRIEF. In the general case, absolute\norientation involves having a point in space that is measured by two separate frames of reference (could be a camera, or another\nsensor), and the goal is to find the closest approach between coordinate systems, a.k.a. the shortest line that connects them.\nWe will arbitrarily assign the 3 x 3 matrix (xl, yl, zl) ∈R3×3 to refer to a \"lefthanded\" coordinate system, and 3 x 3 matrix\n(xr, yr, zr) ∈R3×3 to refer to a \"righthanded\" coordinate system, where our goal is to find the transformation between these\ntwo coordinate systems. The diagram below illustrates this:\n\nFigure 27: General case of absolute orientation: Given the coordinate systems (xl, yl, zl) ∈R3×3 and (xr, yr, zr) ∈R3×3, our\ngoal is to find the transformation, or pose, between them using points measured in each frame of reference pi.\nHere, we also note the following, as they are important for establishing that we are not limited just to finding the transfor-\nmation between two camera sensors:\n- \"Two cameras\" does not just mean having two distinct cameras (known as \"two-view geometry\"); this could also refer to\nhaving a single camera with images taken at two distinct points in time (known as \"Visual Odometry\" or \"VO\").\n- Note also that we could have the same scenario described above when introducing this problem, which is that we have\neither one camera and multiple objects, or multiple objects and one cameras. Hence, there is a sense of duality here with\nsolving these problems:\n1. One camera, two objects?\n2. Two cameras, one object (Two-View Geometry)?\n3. Camera moving (VO)?\n4. Object moving?\nTo better understand this problem, it is first important to precisely define the transformation or pose (translation + rotation)\nbetween the two cameras.\n16.2.3\nTransformations and Poses\nRecall that in 3D, we have 6 degrees of freedom (DOF), of which 3 are for translation, and 3 are for rotation. In the general\ncase of a d-dimensional system, we will have d translational degrees of freedom, and d(d-1)\nrotational degrees of freedom\n(which, interestingly, is also equal to Pd-1\ni=0 i. Therefore, for a d-dimensional system, the total degrees of freedom from translation\nand rotation:\nDOFT, R(d) = d + d(d -1)\nWhat form does our transformation take? Since we are considering these transformations as a transformation determined by\ntranslation and rotation, then we can write our transformation as such:\nrr = R(rl) + r0\nWhere:\n- R describes the rotation. Note that this is not necessarily always an orthonormal rotation matrix, and we have more\ngenerally parameterized it as a function.\n- r0 ∈R3 describes the translation.\n- The translation vector and the rotation function comprise our unknowns.\nWhen R is described by an orthonormal rotation matrix R, then we require this matrix to have the following properties:\n1. RT R = RRT = I, i.e. RT = R-1\n\n2. R is skew-symmetric, so we end up having 3 unknowns instead of 9. Skew-symmetric matrices take the form:\nR =\n\na\nb\nc\n-b\nd\ne\n-c\n-e\nf\n\n∈R3×3\n3. det |R| = +1 (This constraint is needed to eliminate reflections.)\n4. R ∈SO(3) (Known as the Special Orthogonal group). One note about this: optimization over the Special Orthogonal\ngroup (e.g. for estimating optimal rotation matrices) can be difficult, and one way that this can be done is via Singular\nValue Decomposition (SVD) from this group/manifold onto some Euclidean space Rd where d < 3.\nSupposing we have clusters of points from the left and righthand side, and we want to align clusters of points as closely as\npossible. With a mechanical spring analog, we have the force (given by Hooke's law), and consequently the energy:\nF = Ke\n(48)\nE =\nZ\nF = 1\n2ke2\n(49)\nWhere e (the \"error\") is the distance between the measured point in the two frames of reference. Therefore, the solution to\nthis problem involves \"minimizing the energy\" of the system. Interestingly, energy minimization is analogous to least squares\nregression.\nUsing the definition of our transformation, our error for the ith point is given by:\nei = (R(rl,i) + r0) -rr,i\nThen our objective for energy minimization and least squares regression is given by:\nmin\nR,r0\nN\nX\ni=1\n||ri||2\nThis is another instance of solving what is known as the \"inverse\" problem. The \"forward\" and \"inverse\" problems are given by:\n- Forward Problem: R, r0 -→{rr,i, rl,i}N\ni=1\n(Find correspondences)\n- Inverse Problem: rl,i}N\ni=1 -→R, r0\n(Find transformation)\nNow that we have framed our optimization problem, can we decouple the optimization over translation and rotation? It turns\nout we can by setting an initial reference point. For this, we consider two methods.\n16.2.4\nProcedure - \"Method 1\"\n:\n1. Pick a measured point from one set of measured points as the origin.\n2. Take a second point, look at the distance between them, and compute the unit vector. Take unit vector as one axis of the\nsystem:\nxl = rl,2 -rl,1 -→ˆxl =\nxl\n||xl||2\n3. Take a third vector, and compute the component of the vector that is equal to the vector from point 1 to point 2. Now,\npoints 1, 2, and 3 from (x, y) plane, and the removed component from point 3 forms the y-component of the coordinate\nsystem.\n4. We can compute the y vector:\ny = (rl,3 -rl,1) -(rl,3 -rl,1) · ˆxl\nFrom here, we then take the unit vector yl =\nyl\n||yl||2 . From this, we know ˆxl · ˆyl = 0.\n\n5. Obtain the z-axis by the cross product:\nˆzl = ˆxl × ˆyl\n(Then we also have that ˆzl·ˆyl = 0 and ˆzl·ˆxl = 0. This then defines a coordinate system (ˆxl, ˆyl,ˆzl) for the left camera/point\nof reference. Note that this only requires 3 points!\n6. To calculate this for the righthand frame of reference, we can repeat steps 1-5 for the righthand side to obtain the coordinate\nsystem (ˆxr, ˆyr,ˆzr).\nFrom here, all we need to do is find the transformation (rotation, since we have artificially set the origin) between the coordinate\nsystem (ˆxr, ˆyr,ˆzr) and (ˆxl, ˆyl,ˆzl). Mathematically, we have the following equations:\nˆxr = R(ˆxl)\nˆyr = R(ˆyl)\nˆzr = R(ˆzl)\nWe can condense these equations into a matrix equation, and subsequently a matrix inversion problem:\nˆxr\nˆyr\nˆzr\n\n= R\nˆxl\nˆyl\nˆzl\n\nThen we can solve this as a matrix inversion problem:\nR =\nˆxr\nˆyr\nˆzr\nˆxl\nˆyl\nˆzl\n-1\nA few notes on this system:\n- Is this system invertible? Yes, because by construction these vectors are orthogonal to one another.\n- 3 vector equalities -→9 scalar equalities\n- Are we using all of these constraints?\n- For point 1, we use all 3 constraints.\n- For point 2, we use 2 constraints.\n- For point 3, we use 1 constraint.\nIt turns out this system \"favors\" points over one another (usually sub-optimal).\n- This is a somewhat ad hoc method - this could be useful for deriving a fast approximation or initial estimate, but this\nmethod is definitely suboptimal.\n16.2.5\nProcedure - \"Method 2\"\nFor this algorithm, recall our prior work with blob analysis. We can look at axes of inertia (see the figure below), where the\ninertia of the \"blob\" is given by:\nI =\nZZ\nO\nr2dm\nFigure 28: Computing the axes of inertia in a 2D blob.\nHow do we generalize this from 2D to 3D? How do we figure out these axes of inertia (see the example below)?\nI =\nZZZ\nO\nr2dm\n\nFigure 29: Computing the axes of inertia for a 3D blob - we can generalize the notion of inertia from 2D to 3D.\nOne trick we can use here is using the centroid as the origin.\nUsing the triangle figure below:\nFigure 30: Triangle for computing our r′ vector.\nThen we can compute the hypotenuse r′ vector in the ˆω direction:\nr′ = (r · ˆω)ˆω\nThen:\nr -r′ = r -(r · ˆω)ˆω\nAnd we can compute r2:\nr2 = (r -r′) · (r -r′)\n= r · r -2(r · ˆω)2 + (r · ˆω)2(ˆω · ˆω)\n= r · r -2(r · ˆω)2 + (r · ˆω)2\n= r · r -(r · ˆω)2\nSubstituting this into our inertia expression:\nI =\nZZZ\nO\n(r · r -(r · ˆω)2)dm\nLet us simplify some of these formulas:\n1. (r · ˆω)2:\n(r · ˆω)2 = (r · ˆω)(r · ˆω)\n= (ˆω · r)(rˆω)\n= ˆωT (rrT )ˆω\nWhere rrT ∈R3×3 is the dyadic product.\n\n2. (r · r):\n(r · r) = (r · r)(ˆω · ˆω)\n= (r · r)ˆωT I3ˆω\nWhere I3 is the 3 × 3 identity matrix.\nTherefore, the inertia matrix becomes:\nI =\nZZZ\nO\n(r · r -(r · ˆω)2)dm\n=\nZZZ\nO\n(r · r -(r · ˆω)2)dm\n=\nZZZ\nO\n(r · r)ˆωT I3ˆω -ˆωT (rrT )ˆω)dm\n= ˆω\nZZZ\nO\n((r · r)I3 -rrT )dm\n\nˆω\nFrom this expression, we want to find the extrema. We can solve for the extrema by solving for the minimium and maximums\nof this objective:\n1. Minimum: minˆω ˆωT Aˆω\n2. Maximum: maxˆω ˆωT Aˆω\nWhere A\n∆=\nRRR\nO((r · r)I3 -rrT )dm. This matrix is known as the \"inertia matrix\".\nHow can we solve this problem? We can do so by looking for the eigenvectors of the inertia matrix:\n- For minimization, the eigenvector corresponding to the smallest eigenvalue of the inertia matrix corresponds to our\nsolution.\n- For maximization, the eigenvector corresponding to the largest eigenvalue of the inertia matrix corresponds to our\nsolution.\n- For finding the saddle point, our solution will be the eigenvector corresponding to the middle eigenvalue.\nSince this is a polynomial system of degree 3, we have a closed-form solution! These three eigenvectors will form a coordinate\nsystem for the lefthand system.\nTaking a step back, let us look at what we have done so far.\nWe have taken the cloud of points from the left frame of\nreference/coordinate system and have estimated a coordinate system for it by finding an eigenbasis from solving these opti-\nmization problems over the objective ˆωT Aˆω. With this, we can then repeat the same process for the righthand system.\nWhy does this approach work better?\n- We use all the data in both point clouds of 3D data.\n- We weight all the points equally, unlike in teh previous case.\nBut why is this approach not used in practice?\n- This approach fails under symmetry - i.e. of the inertia is hte same in all directions (for shapes such as spheres, polyhedra,\noctahedra, cube, etc.)\n- \"Double-Edged Sword\" - using correspondences can provide you with more information, but can run into issues with\nsymmetry.\n16.2.6\nComputing Rotations\nNext, we will dive into how we can compute and find rotations. To do this, let us first go over the following properties of\nrotations:\n1. Rotations preserve dot products: R(a) · R(b) = a · b\n\n2. Rotations preserve length: R(a) · R(a) = a · a\n3. Rotations preserve angles: |R(a) × R(b)| = |a × b|\n4. Rotations preserve triple products: [R(a) R(b) R(c)] = [a b c]\n(Where the triple product [a b c] = a · (b × c)).\nUsing these properties, we are now ready to set this up as least squares, using correspondences between points measured between\ntwo coordinate systems:\nTransform: rr = R(rl) + r0\nError: ei = rr,i -R(rl,i) -r0\nAnd we can write our optimization as one that minimizes this error term:\nR∗, r∗\nN\nX\ni=1\n||ei||2\nNext, we can compute teh centroids of the left and right systems:\nrl = 1\nN\nN\nX\ni=1\nrl,i, rr = 1\nN\nN\nX\ni=1\nrr,i\nWe can use these computed centroids from points so we do not have to worry about translation. A new feature of this system\nis that the new centroid is at the origin. To prove this, let us \"de-mean\" (subtract the mean) of our coordinates in the left and\nrighthand coordinate systems:\nr′\nl,i = rl,i - rl, r′\nr,i = rr,i - rr\nBecause we subtract the mean, the mean of these new points now becomes zero:\nˆr′\nl = 1\nN\nN\nX\ni=1\nr′\nl,i = 0 = ˆr′\nr = 1\nN\nN\nX\ni=1\nr′\nr,i\nSubstituting this back into the objective, we can solve for an optimal rotation R:\nR∗= min\nR,r′\nN\nX\ni=1\n||r′\ni -R(r′\nl,i -r′\n0)||2\n= min\nR,r′\nN\nX\ni=1\n||r′\ni -R(r′\nl,i)||2\n2 -2r′\nN\nX\ni=1\n(r′\nr,i -R(rl,i)) + N||r′\n0||2\n= min\nR,r′\nN\nX\ni=1\n||r′\ni -R(r′\nl,i)||2\n2 + N||r′\n0||2\nSince only the last term depends on r′\n0, we can set r′\n0 to minimize. Moreover, we can solve for the true r0 by back-solving later:\nr0 = rr -R( rl)\nIntuitively, this makes sense: the translation vector between these two coordinate systems/point clouds is the difference between\nthe centroid of the right point cloud and the centroid of the left point cloud after it has been rotated.\nSince we now have that r′\n0 = 0 ∈R3, we can write our error term as:\nei = r′\nr,i -R(r′\nl,i)\nWhich in turn allows us to write the objective as:\nmin\nN\nX\ni=1\n||ei||2\n2 =\nN\nX\ni=1\n(r′\nr,i -R(r′\nl,i)(r′\nr,i -R(r′\nl,i))\n=\nN\nX\ni=1\n||r′\nr,i||2\n2 -\nN\nX\ni=1\n(r′\nr,i -R(r′\nl,i)) -\nN\nX\ni=1\n||r′\nl,i||2\n\nWhere the first of these terms is fixed, the second of these terms is to be maximized (since there is a negative sign in front of\nthis term, this thereby minimizes the objective), and the third of these terms is fixed. Therefore, our rotation problem can be\nsimplified to:\nmin\nN\nX\ni=1\n||ei||2\n2 = -2\nN\nX\ni=1\n(r′\nr,i -R(r′\nl,i))\n(50)\nTo solve this objective, we could take the derivative\nd\ndR(·) of the objective, but constraints make this optimization problem\ndifficult to solve (note that we are not optimizing over a Euclidean search space - rather, we are optimizing over a generalized\ntransformation). We will look into a different representation of R for next class to find solutions to this problem!\n16.3\nReferences\n1. Bundle Adjustment, https://en.wikipedia.org/wiki/Bundle adjustment\nLecture 18:\nRotation and How to Represent it, Unit Quaternions, the\nSpace of Rotations\nToday, we will focus on rotations.\nNote that unlike translations, rotations are not commutative, which makes fitting\nestimates to data, amongst other machine vision tasks, more challenging. In this lecture, we will cover rotation in terms of:\n- Properties\n- Representations\n- Hamilton's Quaternions\n- Rotation as Unit Quaternion\n- Space of rotations\n- Photogrammetry\n- Closed-form solution of absolute quaternions\n- Divison algebras, quaternion analysis, space-time\nWe will start by looking at some motivations for why we might care about how we formulate and formalize rotations: What is\nrotation used for?\n- Machine vision\n- Recognition/orientation\n- Graphics/CAD\n- Virtual Reality\n- Protein Folding\n- Vehicle Attitude\n- Robotics\n- Spatial Reasoning\n- Path Planning - Collision Avoidance\n\n17.1\nEuclidean Motion and Rotation\nRotation and translation form Euclidean motion. Some properties of Euclidean motion, including this property. Euclidean\nmotion:\n- Contains translation and rotation\n- Preserves distances between points\n- Preserves angles between lines\n- Preserves handedness\n- Preserves dot-products\n- Preserves triple products\n- Does not contain:\n- Reflections\n- Skewing\n- Scaling\n17.2\nBasic Properties of Rotation\nNow that we have framed rotation to be a property of Euclidean motion, we can dive further into some properties of rotation:\n- Euler's Theorem: There is a line of fixed points that remain the same in any rotation. This is known as the axis of\nrotation.\n- Parallel axis theorem: Any rotation is equivalent to rotation through the origin along with a translation. This allows\nfor the decoupling between translation and rotation.\n- Rotation of sphere includes rotation of space: I.e, any rotation that is induced on a space can be induced on an\nequivalent-dimensional sphere.\n- Attitude, Orientation - Rotation Relative to Reference: This helps for determining the orientation of objects\nrelative to another frame of reference.\n- Degrees of Freedom (in 3D) is 3.\nSome additional properties that are helpful for understanding rotation:\n- Rotational velocity can be described by a vector. This set of rotational vectors together form a lie algebra, specifically,\nso(3). The set of rotations form the counterpart to this lie algebra, namely the Special Orthogonal group SO(3), which\nis a Lie group. If you are not familiar with Lie groups/algebras, here are a few useful terms to get started with these\nconcepts:\n- A group is a set equipped with a binary operation that combines any two elements to form a third element in such a\nway that four conditions called group axioms are satisfied, namely closure, associativity, identity and invertibility [1].\n- A manifold is a d-dimensional topological space that locally resembles Euclidean space at each point i.e. any patch\nof the manifold can be \"locally\" approximated as a d-dimensional Euclidean space [2].\n- A Lie group is a group that is also a differentiable manifold. One key Lie group that we will study in this course is\nthe Special Orthogonal group, known as SO(3), which is the space of orthonormal rotation matrices.\n- We can define the derivative of the radius vector r using Poisson's Formula: r = ˆω × r\n- Rotational velocities add - this holds because these vectors belong to Lie algebras\n- Finite rotations do not commute - this holds because rotations are defined by Lie groups in a non-Euclidean space.\n- The Degrees of Freedom for rotation in dimension n is given by n(n-1)\n, which coincidentally equals 3 in 3 dimensions.\n- Intuition: Oftentimes, it is easier to think about rotation \"in planes\", rather than \"about axes\". Rotations preserve\npoints in certain planes.\n\n17.2.1\nIsomorphism Vectors and Skew-Symmetric Matrices\nA technical note that is relevant when discussing cross products: Although a cross product produces a vector in 3D, in higher\ndimensions the result of a cross product is a subspace, rather than a vector.\nSpecifically, this subspace that forms the result of a higher-dimensional cross product is the space that is perpendicu-\nlar/orthogonal to the two vectors the cross product operator is applied between.\nWith this set up, we can think of cross products as producing the following isomorphism vectors and skew-symmetric ma-\ntrices: One Representation:\na × b = Ab\nA =\n\n-az\nay\naz\n-ax\n-ay\nax\n\nAn Isomorphic Representation:\na × b = Ba\nA =\n\nbz\n-by\n-bz\nbx\nby\n-bx\n\nNote that while these skew-symmetric matrices have 9 elements as they are 3 × 3 matrices, they only have 3 DOF.\n17.3\nRepresentations for Rotation\nThere are a myriad of representations for rotations, highlighting that different applications/domains/problem-solving techniques\ndemand different representations for these transformations. Some of these representations include (we will proceed in greater\ndetail about these later):\n1. Axis and angle\n2. Euler Angles\n3. Orthonormal Matrices\n4. Exponential cross product\n5. Stereography plus bilinear complex map\n6. Pauli Spin Matrices\n7. Euler Parameters\n8. Unit Quaternions\nLet us delve into each of these in a little more depth.\n17.3.1\nAxis and Angle\nThis representation is composed of a vector ˆω and an angle θ, along with the Gibb's vector that combines these given by\nˆω tan\nθ\n\n, which has magnitude tan\nθ\n\n, providing the system with an additional degree of freedom that is not afforded by unit\nvectors. Therefore, we have our full 3 rotational DOF.\n17.3.2\nEuler Angles\nA few notes about these:\n- There are over 24 definitions of Euler angles! This is the case because these definitions are permutations - i.e. the order of\nthe angles matters here. The order of angular composition matters.\n- These rotations are defined through each axis, roll, pitch, and yaw.\n\n17.3.3\nOrthonormal Matrices\nWe have studied these previously, but these are the matrices that have the following properties:\n1. RT R = RRT = I, RT = R-1 (skew-symmetric)\n2. det |R| = +1\n3. R ∈SO(3) (see notes on groups above) - being a member of this Special Orthogonal group is contingent on satisfying the\nproperties above.\n17.3.4\nExponential Cross Product\nWe can also write rotations in terms of a matrix exponential. To derive this matrix exponential, let us consider the matrix\nfirst-order, homogeneous differential equation defined by:\ndR\ndθ = ΩR\nHas the solution given by the matrix exponential:\nR = eθΩ\nTaking the Taylor expansion of this expression, we can write this matrix exponential as:\nR = eθΩ\n=\ninf\nX\ni=0\ni!(θΩ)i\n=\ninf\nX\ni=0\nθi\ni! (VΩΛi\nΩVT\nΩ) (Taking an eigendecomposition of Ω= VΩΛΩVT\nΩ)\n(Optional) Let us look a little deeper into the mathematics behind this exponential cross product. We can write a rotation about\nˆω through angle θ as:\nr = R(θ)r0\ndr\ndθ = d\ndθ(R(θ)r0)\ndr\ndθ = ˆω × r = Ωr = ΩR(θ)r0\nd\ndθR(θ)r0 = ΩR(θ)r0\nThen for all r0:\nd\ndθR(θ) = ΩR(θ) =⇒R(θ) = eθΩ\n17.3.5\nStereography Plus Bilinear Complex Map\nIntuitively, this notion of rotation corresponds to mapping a sphere to a complex plane, making transformations in the complex\nplane, and then mapping this transformed mapping from the complex plane back to the sphere. A few points here:\n- This is an instance of Stereography: a conformal (angle-preserving) spherical mapping.\n- The rotation of a sphere induces the rotation of a space.\n- This plane is treated as a complex plane that we can apply homogeneous transforms to.\nWe can conceptually visualize this mapping from a sphere to the complex plane using the figure below:\n\nFigure 31: Mapping from a sphere to a complex plane, which we then apply a homogeneous transformation to and map back to\nthe sphere in order to induce a rotation.\nSpecifically, the homogeneous transform we consider maps the complex variable z to z′:\nz′ = az + b\ncz + d, for some a, b, c, d ∈C\nWe can actually generalize this framework even further - any rotation in 3-space can be thought of as an operation in a complex\nplane.\n17.3.6\nPauli Spin Matrices\nWith a physical motivation, these are 2 × 2 complex-valued, unitary, Hermitian matrices (A =\nAT ). All these constraints create\n3 DOF for these matrices:\nSx = ħ\n\n, Sy = ħ\n-i\ni\n\n, Sz = ħ\n-1\n\n17.3.7\nEuler Parameters\nThere are also known as Rodrigues parameters, leading to Rodrigues formula, which can be thought of as a rotation about\nthe unit vector ˆω through the angle θ:\nr′ = (cos θ)r + (1 -cos θ)(ˆω · r)ˆω + sin θ(ˆω × r)\nThe geometry of this problem can be understood through the following figure:\nFigure 32: Geometric interpretation of the Rodrigues formula: Rotation about the vector ˆω through an angle θ.\nOne disadvantage of this approach is that there is no way to have compositions of rotations.\n\nNext, let us take an in-depth analysis of the Rodrigues Formula and the Exponential Cross Product:\ndR\ndθ = ΩR =⇒R = eΩθ\neΩθ = I + θΩ+ 1\n2!(θΩ)2 + ... =\ninf\nX\ni=0\nθi\ni! Ωi\nNext, we have that:\nΩ2 = (ˆωˆωT -I)\nΩ3 = -Ω\nWe can then write this matrix exponential as:\neθΩ= I + Ω(θ -θ3\n3! + θ5\n5! + · · · ) + Ω2(θ2\n2! -θ4\n4! + θ6\n6! + · · · )\n= I + Ω(\ninf\nX\ni=0\nθ2i+1\n(2i + 1)!(-1)i) + Ω2(\ninf\nX\ni=0\nθ2i+2\n(2i + 2)!(-1)i)\n= I + Ωsin θ + Ω2(1 -cos θ)\n= I + (sin θ)Ω+ (ˆωˆωT -I)(1 -cos θ)\n= (cos θ)I + (sin θ)Ω+ (1 -cos θ)ˆωˆωT\nFrom this, we have:\nr = eθΩr\nr′ = (cos θ)r + (1 -cos θ)(ˆω · r)ˆω + sin θ(ˆω × r)\nWhere the last line is the result of the Rodrigues formula.\n17.4\nDesirable Properties of Rotations\nWe would like rotations to exhibit the following properties:\n- The ability to rotate vectors - or coordinate systems\n- The ability to compose rotations\n- Intuitive, non-redundant representation - e.g. rotation matrices have 9 entries but only 3 degrees of freedom\n- Computational efficiency\n- Interpolate orientations\n- Averages over range of rotations\n- Derivative with respect to rotations - e.g. for optimization and least squares\n- Sampling of rotations - uniform and random (e.g. if we do not have access to closed-form solutions)\n- Notion of a space of rotations\n17.5\nProblems with Some Rotation Representations\nLet us discuss some problems with the representations we introduced and discussed above:\n1. Orthonormal Matrices: Redundant, with complex constraints.\n2. Euler Angles: Inability to compose rotations, \"gimbal lock\" (when two axes line up, you lose a DOF, which can be\ndisastrous for control systems).\n3. Gibb's Vector: Singularity when θ = π, since tan( π\n2 ) is not defined.\n4. Axis and Angle: Inability to compose rotations\n5. There is no notion of the \"space of rotations\" (at least not in Euclidean space).\nHow can we overcome these challenges? One way through which we can do is to look at another representation approach, this\ntime through Hamilton.\n\n17.6\nQuaternions\nIn this section, we will discuss another way to represent rotations: quaternions.\n17.6.1\nHamilton and Division Algebras\nGoal: Develop an algebra where we can use algebraic operations such as addition, subtraction, mulitplication, and division.\n1. Hamilton's representation is motivated by Algebraic couples, for instance, complex numbers as pairs of reals a+bi, a-bi\n2. Here, we want: Multiplicative inverses\n3. Examples: Real numbers, complex numbers\n4. Expected next: Three conponents (vectors) - note that this was before Gibb's vector and 3-vectors\nHamilton's insight here is that these quaternions require a \"4th dimension for the sole purpose of calculating triples\".\n17.6.2\nHamilton's Quaternions\nHamilton noted the following insights in order to formalize his quaternion:\n- Quaternions cannot be constructed with three components\n- Quaternions need additional square roots of -1\nTherefore, the complex components i, j, k defined have the following properties:\n1. i2 = j2 = k2 = ijk = -1\n2. From which follows:\n(a) ij = k\n(b) jk = i\n(c) ki = j\n(d) ji = -k\n(e) kj = -i\n(f) ik = -j\nNote: As you can see from these properties, multiplication of the components of these quaternions is not commutatitve.\n17.6.3\nRepresentations of Quaternions\nThere are several ways through which we can represent these quaternions:\n1. Real and imaginary parts: q0 + iqx + jqy + kqz\n2. Scalar and 3-vector: (q, q)\n3. 4-vector:\noq\n4. Certain Orthogonal 4 x 4 Matrices q (this is an isomorphism of quaternions that allows us to do multiplications).\n5. Complex Composite of Two Complex Numbers: Here, we have (a + bi) and (c + di), and we replace all the real\nconstants a, b, c, d with complex numbers. We can build sophisticated algebras from these operations.\n\n17.6.4\nRepresentations for Quaternion Multiplication\nWith several ways to represent these quaternions, we also have several ways through which we can represent quaternion multi-\nplication:\n1. Real and 3 Imaginary Parts:\n(p0 + pxi + pyj + pzk)((q0 + qxi + qyj + qzk) =(p0q0 -pxqx -pyqy -pzqz)+\n(p0qx + pxq0 + pyqz -pzqy)i+\n(p0qy -pxqz + pyqz + pzqx)j+\n(p0qz + pxqy -pyqx + pzq0)k\n2. Scalar and 3-Vector:\n(p, p)(q, q) = (pq -p · q, pq + qp + p × q)\nNote: This multiplication operation is not commutative.\n3. 4-Vector:\n\np0\n-px\n-py\n-pz\npx\np0\n-pz\npy\npy\npz\np0\n-px\npz\n-py\npx\np0\n\nq0\nqx\nqy\nqz\n\nNote: Here we also have an isomorphism between the quaternion and the 4 x 4 orthogonal matrix (this matrix is or-\nthonormal if we have unit quaternions). Here we can show the isomorphism and relate this back to the cross product we\nsaw before by considering the equivalence of the two following righthand-side expressions:\n(a)\nop\noq = P\noq, where P =\n\np0\n-px\n-py\n-pz\npx\np0\n-pz\npy\npy\npz\np0\n-px\npz\n-py\npx\np0\n\n(b)\nop\noq = q\nop, where q =\n\nq0\n-qx\n-qy\n-qz\nqx\nq0\nqz\n-qy\nqy\n-qz\nq0\nqx\nqz\nqy\n-qx\nq0\n\nA few notes about these matrices P and q:\n- These matrices are orthonormal if quaternions are unit quaternions\n- P is normal if\nop is a unit quaternion, and q is normal if\noq is a unit quaternion.\n- P is skew-symmetric if\nop has zero scalar part, and q is skew-symmetric if\noq has zero scalar part.\n- P and q have the same signs for the first row and column, and flipped signs for off-diagonal entries in the bottom\nright 3 x 3 blocks of their respective matrices.\n17.6.5\nProperties of 4-Vector Quaternions\nThese properties will be useful for representing vectors and operators such as rotation later:\n1. Not commutative:\nop\noq =\noq\nop\n2. Associative: (\nop\noq)\nor =\nop(\noq\nor)\n3. Conjugate: (p, p)∗= (p, -p) =⇒(\nop\noq) =\noq\n∗op\n4. Dot Product: (p, p) · (q, q) = pq + p + q\n5. Norm: ||\noq||2\n2 =\noq ·\noq\n\n6. Conjugate Multiplication:\noq\noq\n∗:\noq\noq\n∗= (q, q)(q, -q)\n= (q2 + q · q, 0)\n= (\noq ·\noq)\noe\nWhere\noe\n∆= (1, 0), i.e. it is a quaternion with no vector component. Conversely, then, we have:\noq\n∗oq = (\noq\noq)\noe.\n7. Multiplicative Inverse:\noq\n-1 =\noq\n∗\n(\noq·\noq (Except for\noq = (0, 0), which is problematic with other representations anyway.)\nWe can also look at properties with dot products:\n1. (\nop\noq) · (\nop\noq) = (\nop ·\nop)(\noq ·\noq)\n2. (\nop\noq) · (\nop\nor) = (\nop ·\nop)(\noq ·\nor)\n3. (\nop\noq) ·\nor =\nop · (\nor\noq\n∗)\nWe can also represent these quaternions as vectors. Note that these quaternions all have zero scalar component.\n1.\nor = (0, r)\n2.\nor\n∗= (0, -r)\n3.\nor ·\nos = r · s\n4.\nor\nos = (-r · s, r × s)\n5. (\nor\nos) ·\no\nt =\nor · (\nos\no\nt) = [r s t] (Triple Products)\n6.\nor\nor = -(r · r)\noe\nAnother note: For representing rotations, we will use unit quaternions. We can represent scalars and vectors with:\n- Representing scalars: (s, 0)\n- Representing vectors: (0, v)\n17.7\nQuaternion Rotation Operator\nTo represent a rotation operator using quaternions, we need a quaternion operation that maps from vectors to vectors. More\nspecifically, we need an operation that maps from 4D, the operation in which quaternions reside, to 3D in order to ensure that\nwe are in the correct for rotation in 3-space. Therefore, our rotation operator is given:\nor\n′ = R(\nor) =\noq\nor\noq\n∗\n= (Q\nor)\noq\n∗\n= ( QT Q)\nor\nWhere the matrix QT Q is given by:\nQT Q =\n\noq ·\noq\nq2\n0 + q2\nx -q2\ny -q2\nz\n2(qxqy -q0qz)\n2(qxqz + q0qy)\n2(qyqx + q0qz)\nq2\n0 -q2\nx + q2\ny -q2\nz\n2(qyqz -q0qx)\n2(qzqx -q0qy)\n2(qzqy + q0qz)\nq2\n0 -q2\nx -q2\ny + q2\nz\n\nA few notes about this matrix:\n- Since the scalar component of\noq is zero, the first row and matrix of this column are sparse, as we can see above.\n- If\noq is a unit quaternion, the lower right 3 × 3 matrix of QT Q will be orthonormal (it is an orthonormal rotation matrix).\nLet us look at more properties of this mapping\nor\n′ =\noq\nor\noq\n∗:\n\n1. Scalar Component: r′ = r(\noq ·\noq)\n2. Vector Component: r′ = (q2 -q · q)r + 2(q · r)q + 2q(q × r)\n3. Operator Preserves Dot Products:\nor\n′ ·\nos\n′ =\nor ·\nos =⇒r′ · s′ = r · s\n4. Operator Preserves Triple Products: (\nor\n′ ·\nos\n′) ·\no\nt\n′\n= (\nor ·\nos) ·\no\nt =⇒(r′ · s′)t′ = (r · s) · t =⇒[r′ s′ t′] = [r s t]\n5. Composition (of rotations!): Recall before that we could not easily compose rotations with our other rotation repre-\nsentations. Because of associativity, however, we can compose rotations simply through quaternion multiplication:\nop(\noq\nor\noq\n∗)\nop\n∗= (\nop\noq)\nor(\noq\n∗op\n∗) = (\nop\noq)\nor(\nop\noq)∗\nI.e. if we denote the product of quaternions\noz\n∆=\nop\noq, then we can write this rotation operator as a single rotation:\nop(\noq\nor\noq\n∗)\nop\n∗= (\nop\noq)\nor(\noq\n∗op\n∗) = (\nop\noq)\nor(\nop\noq)∗=\noz\nor\noz\n∗\nThis ability to compose rotations is quite advantageous relative to many of the other representations of rotations we have\nseen before (orthonormal rotation matrices can achieve this as well).\n17.7.1\nRelation of Quaternion Rotation Operation to Rodrigues Formula\nLet us see how this operator relates to the Rodrigues formula and the axis-angle representation of rotations:\nr′ = (q2 -q · q)r + 2(q · r)q + 2q(q × r)\n= (cos θ)r + (1 -cos θ)(ˆω · r)ˆω + (sin θ)(ˆω × r)\nWe have that q is parallel to ˆω. Note the following equalities:\n- (q2 -q · q) = cos θ\n- 2||q||2\n2 = (1 -cos θ)\n- 2q||q||2\n2 = sin\nθ\n\n- q = cos\nθ\n\n- ||q||2\n2 = sin\nθ\n\nFrom these statements of equality, we can conclude:\noq = (cos\nθ\n\n, ˆω sin\nθ\n\n)\nA few notes on this:\n- We see that both the scalar and vector components of the quaternion\noq depend on the axis of rotation ˆω and the angle of\nrotation θ.\n- The vector component of this quaternion is parallel to ˆω.\n- This representation is one way to represent a unit quaternion.\n- Knowing the axis and angle of a rotation allows us to compute the quaternion.\n- Note that -\noq represents the same mapping as\noq since:\n(-\noq)\nor(-\noq\n∗) =\noq\nor\noq\n∗\nTo build intuition with this quaternion rotation operator, one way we can conceptualize this is by considering that our space\nof rotations is a 3D sphere in 4D, and opposite points on this sphere represent the same rotation.\n\n17.8\nApplying Quaternion Rotation Operator to Photogrammetry\nNow that we have specified this operator and its properties, we are ready to apply this to photogrammetry, specifically for\nabsolute orientation. Let us briefly review our four main problems of photogrammetry:\n1. Absolute Orientation (3D to 3D): Range Data\n2. Relative Orientation (2D to 2D): Binocular Stereo\n3. Exterior Orientation (3D to 2D): Passive Navigation\n4. Interior Orientation (2D to 3D): Camera Calibration\nWe will start by applying this to our first problem, absolute orientation. Recall our goal with this photogrammetry problem\nis to find the 3D transformation between our coordinate systems.\n17.8.1\nLeast Squares Approach to Find R\nRecall our first attempt to solve for this transformation was done through least squares to solve for an optimal rotation matrix\nand a translation vector. While we showed in lecture 17 that we can solve for an optimal translation vector that depends on the\nrotation matrix R, we could not find a closed-form solution for the rotation matrix R. We review why:\nmin +R\nn\nX\ni=1\n||ei||2\n2 =\nn\nX\ni=1\n(r′\nr,i -R(r′\nl,i)(r′\nr,i -R(r′\nl,i))\n=\nn\nX\ni=1\n||r′\nr,i||2\n2 -\nn\nX\ni=1\n(r′\nr,i -R(r′\nl,i)) -\nn\nX\ni=1\n||r′\nl,i||2\nWhere the first of these terms is fixed, the second of these terms is to be maximized (since there is a negative sign in front of\nthis term, this thereby minimizes the objective), and the third of these terms is fixed. Therefore, our rotation problem can be\nsimplified to:\nR∗= min\nR\nn\nX\ni=1\n||ei||2\n2 = -2\nn\nX\ni=1\n(r′\nr,i -R(r′\nl,i))\n(51)\n= min\nR (-2\nn\nX\ni=1\nr′\nr,i · R(r′\nl,i))\n(52)\n= max\nR (\nn\nX\ni=1\nrb′\nr,i · R(r′\nl,i))\n(53)\nBut since we are optimizing over an orthonormal rotation matrix R, we cannot simply take the derivative and set it equal to\nzero as we usually do for these least squares optimization problems. Though we can solve this as a Lagrangian optimization\nproblem, specifying these constraints is difficult and makes for a much more difficult optimization problem. It turns out this a\ncommon problem in spacecraft attitude control. Let us see how we can use quaternions here!\n17.8.2\nQuaternion-based Optimization\nSolving this with our quaternion operator above, and noting the following definitions:\n-\nor\n′\nl,i = (0, r′\nl,i)\n-\nor\n′\nr,i = (0, r′\nl,i)\n\nThen we can solve for our optimal rotation by solving for quaternions instead:\nR∗= max\nR\nn\nX\ni=1\nrb′\nr,i · R(r′\nl,i)\n=\nmax\noq,||\noq||2=1\nn\nX\ni=1\n(\noq\nor\n′\nl,i\noq\n∗) ·\nor\n′\nr,i\n=\nmax\noq,||\noq||2=1\nn\nX\ni=1\n(\noq\nor\n′\nl,i) · (\nor\n′\nr,i\noq)\n=\nmax\noq,||\noq||2=1\nn\nX\ni=1\n( Rl,i\noq) · (Rr,i\noq)\n=\nmax\noq,||\noq||2=1\noq\nT\nn\nX\ni=1\nRT\nl,iRr,i\noq\n(Since\noq does not depend on i)\nWhere the term in the sum is a 4 × 4 matrix derived from point cloud measurements.\nFrom here, we can solve for an optimal rotation quaternion through Lagrangian Optimization, with our objective given by:\nmax\noq\noq\nT N\noq, subject to:\noq ·\noq = 1, N\n∆=\nn\nX\ni=1\nRT\nl,iRr,i\nThen written with the Lagrangian constraints this optimization problem becomes:\nmax\noq\noq\nT N\noq + λ(1 -\noq ·\noq)\nDifferentiating this expression w.r.t.\noq and setting the result equal to zero yields the following first-order condition:\n2N\noq -2λ\noq = 0\nIt turns out that similarly to our inertia problem from the last lecture, the quaternion solution to this problem is the solution to\nan eigenvalue/eigenvector problem. Specifically, our solution is the eigenvector corresponding to the largest eigenvalue of a 4 × 4\nreal symmetric matrix N constructed from elements of the matrix given by a dyadic product of point cloud measurements from\nthe left and righthand systems of coordinates:\nM =\nn\nX\ni=1\nr′\nl,ir′\nr,i\nT\nThis matrix M is an asymmetric 3 × 3 real matrix. A few other notes about this:\n- The eigenvalues of this problem are the Lagrange Multipliers of this objective, and the eigenvectors are derived from\nthe eiegenvectors of N, our matrix of observations.\n- This analytic solution leads to/requires solving a quartic polynomial - fortunately, we have closed-form solutions of poly-\nnomials up to a quartic degree! Therefore, a closed-form solution exists. Specifically, the characteristic equation in this\ncase takes the form:\nλ4 + c3λ3 + c2λ2 + c1λ + c0 = 0\nBecause the matrix of point cloud measurements N is symmetric, this characteristic equation simplifies and we get the\nfollowing coefficients:\n1. c3 = tr(N) = 0\n2. c2 = -2tr(M T M)\n3. c1 = -8 det |M|\n4. c0 = det |N|\nIn addition to solving absolute orientation problems with quaternions, this approach has applications to other problems as\nwell, such as:\n\n- Relative Orientation (Binocular Stereo)\n- Camera Calibration\n- Manipulator Kinematics\n- Manipulator Fingerprinting\n- Spacecraft Dynamics\n17.9\nDesirable Properties of Quaternions\nNext, let us look at what propreties we would like to obtain from using quaternions, and let us see how well these properties are\nsatisfied:\n- The ability to rotate a vector or coordinate system\nYes!\n- Ability to compose rotations\nYes!\n- Intuitive, non-redundant representations\nThere is some redundancy (but note that this is solved by using unit quaternions!), and this representation is not the most\nintuitive.\n- Computational efficiency\nWe will discuss this in more detail further below.\n- Ability to Interpolate Orientations\nYes!\n- Ability to average over rotations\nYes!\n- Ability to take derivative w.r.t. rotation - e.g. for optimization and least squares\nYes!\n- Ability to sample rotations\nYes!\n- Notion of a space of rotations\nYes!\n17.9.1\nComputational Issues for Quaternions\nOne helpful metric we can compare different representations for rotation on is their computational efficiency. Below, we compare\nquaternions for rotation with orthonormal matrices for rotation in several important operations:\n- Operation: Composition of Rotations:\nop\noq\nThis is given by:\nop\noq = (p, p)(q, q) = (pp -pq, pq + qp + p × q)\nCarrying this out naively requires 16 multiplications and 12 additions.\nCompared with orthonormal matrices, composing quaternions is faster for this operation (orthonormal matrices require\n27 multiplications and 18 additions.\n- Operation: Rotating Vectors:\noq\nor\noq\n∗\nThis is given by:\noq\nor\noq\n∗→r′ = (q2 -r · q)r + 2(q · r)q + 2q(q × r)\nr′ = r + 2q(q × r) + 2q × (q × r) (More efficient implementation)\nCarrying this out naively requires 15 multiplications and 12 additions.\nCompared with orthonormal matrices, composing quaternions is slower for this operation (orthonormal matrices require\n9 multiplications and 6 additions.\n\n- Operation: Renormalization This operation is used when we compose many rotations, and the quaternion (if we are\nusing a quaternion) or the orthonormal matrix is not quite an orthonormal matrix due to floating-point arithmetic. Since\nthis operation requires matrix inversion (see below) for orthonormal matrices, it is much faster to carry out this operation\nwith quaternions.\nNearest Unit Quaternion:\noq\n√oq·\noq\nNearest Orthonormal Matrix: M(M T M)-1\n17.9.2\nSpace of Rotations\nWe will conclude today's lecture by discussing the space of rotations, now that we have a representation for it:\n- S3 (the unit sphere ∈R3) with antipodal points identified\n- Projective space P 3\n- Sampling: Sampling in this space can be done in regular and random intervals\n- Finite rotation groups: These include the platonic solids with 12, 24, 60 elements - we can have rotation groups for (i)\ntetrahedron, (ii) hexahedron/octahedron, and (iii) dodecahedron/icosahedron\n- Finer-grade Sampling can be achieved by sub-dividing the simplex in the rotation space\n- If {\noqi}N\ni=1 is a group, then so is {\noq\n′\ni}N\ni=1, where\noq\n′\ni =\noq0\noqi.\n17.10\nReferences\n1. Group, https://en.wikipedia.org/wiki/Group (mathematics)\n2. Manifold, https://en.wikipedia.org/wiki/Manifold\nLecture 19: Absolute Orientation in Closed Form, Outliers and Robustness,\nRANSAC\nThis lecture will continue our discussion of photogrammetry topics - specifically, covering more details with the problem of\nabsolute orientation. We will also look at the effects of outliers on the robustness of closed-form absolute orientation, and how\nalgorithms such as RANSAC can be leveraged as part of an absolute orientation, or, more generally, photogrammetry pipeline\nto improve the robustness of these systems.\n18.1\nReview: Absolute Orientation\nRecall our four main problems of photogrammetry:\n- Absolute Orientation 3D ←→3D\n- Relative Orientation 2D ←→2D\n- Exterior Orientation 2D ←→3D\n- Intrinsic Orientation 3D ←→2D\nIn the last lecture, we saw that when solving absolute orientation problems, we are mostly interested in finding transfor-\nmations (translation + rotation) between two coordinate systems, where these coordinate systems can correspond to objects\nor sensors moving in time (recall this is where we saw duality between objects and sensors).\nLast time, we saw that one way we can find an optimal transformation between two coordinate systems in 3D is to de-\ncompose the optimal transformation into an optimal translation and an optimal rotation. We saw that we could solve for\noptimal translation in terms of rotation, and that we can mitigate the constraint issues with solving for an orthonormal rotation\nmatrix by using quaternions to carry out rotation operations.\n\n18.1.1\nRotation Operations\nRelevant to our discussion of quaternions is identifying the critical operations that we will use for them (and for orthonormal\nrotation matrices). Most notably, these are:\n1. Composition of rotations:\nop\noq = (p, q)(q, q) = (pq -q · q, pq + qq + q × q)\n2. Rotating vectors:\nor\n′ =\noq\nor\noq\n∗= (q2 -q · q)r + 2(q · r)q + 2q(q × r)\nRecall from the previous lecture that operation (1) was faster than using orthonormal rotation matrices, and operation (2)\nwas slower.\n18.1.2\nQuaternion Representations: Axis-Angle Representation and Orthonormal Rotation Matrices\nFrom our previous discussion, we saw that another way we can represent quaternions is through the axis-angle notation (known\nas the Rodrigues formula):\nr′ = (cos θ)r + (1 -cos θ)(ˆω · r)ˆω + sin θ(ˆω × r)\nCombining these equations from above, we have the following axis-angle representation:\noq ⇐⇒ˆω, θ, q = cos\nθ\n\n, q = ˆω sin\nθ\n\n=⇒\noq =\n\ncos\nθ\n\n, ˆω sin\nθ\n!\nWe also saw that we can convert these quaternions to orthonormal rotation matrices. Recall that we can write our vector rotation\noperation as:\noq\nor\noq\n∗= ( QT Q)\nor, where\nQT Q =\n\noq ·\noq\nq2\n0 + q2\nx -q2\ny -q2\nz\n2(qxqy -q0qz)\n2(qxqz + q0qy)\n2(qyqx + q0qz)\nq2\n0 -q2\nx + q2\ny -q2\nz\n2(qyqz -q0qx)\n2(qzqx -q0qy)\n2(qzqy + q0qz)\nq2\n0 -q2\nx -q2\ny + q2\nz\n\nThe matrix QT Q has skew-symmetric components and symmetric components. This is useful for conversions. Given a\nquaternion, we can compute orthonormal rotations more easily. For instance, if we want an axis and angle representation, we\ncan look at the lower right 3 × 3 submatrix, specifically its trace:\nLet R = [ QT Q]3×3,lower, then :\ntr(R) = 3q2\n0 -(q2\nx + q2\ny + q2\nz)\n= 3 cos2\nθ\n\n-(sin2\nθ\n\n(Substituting our axis-angle representation)\n-cos2\nθ\n\n+ sin2\nθ\n\n-1 (Subtracting Zero)\n→2 cos2\nθ\n\n-sin2\nθ\n\n-1\n= 2 cos θ -1\n=⇒cos θ = 1\n2(tr(R) -1)\nWhile this is one way to get the angle (we can solve for θ through arccos of the expression above), it is not the best way to do\nso: we will encounter problems near θ ≈0, π. Instead, we can use the off-diagonal elements, which depend on sin\n\nθ\n\ninstead.\nNote that this works because at angles θ where cos\n\nθ\n\nis \"bad\" (is extremely sensitive), sin\n\nθ\n\nis \"good\" (not as sensitive),\nand vice versa.\n\n18.2\nQuaternion Transformations/Conversions\nNext, let us focus on how we can convert between quaternions and orthonormal rotation matrices. Given a 3 × 3 orthonormal\nrotation matrix r, we can compute sums and obtain the following system of equations:\n1 + r11 + r22 + r33 = 4q2\n1 + r11 -r22 -r33 = 4q2\nx\n1 -r11 + r22 -r33 = 4q2\ny\n1 -r11 -r22 + r33 = 4q2\nz\nThis equation can be solved by taking square roots, but due to the number of solutions (8 by Bezout's theorem, allowing for the\nflipped signs of quaternions, we should not use this set of equations alone to find the solution).\nInstead, we can compute these equations, evaluate them, take the largest for numerical accuracy, arbitrarily select to use\nthe positive version (since there is sign ambiguity with the signs of the quaternions), and solve for this. We will call this selected\nrighthand side qi.\nFor off-digaonals, which have symmetric and non-symmetric components, we derive the following equations:\nr32 -r23 = 4q0qx\nr13 -r31 = 4q0qy\nr21 -r12 = 4q0qx\nr21 + r12 = 4qxqy\nr32 + r23 = 4qyqz\nr13 + r31 = 4qzqz\nAdding/subtracting off-diagonals give us 6 relations, of which we only need 3 (since we have 1 relation from the diagonals). For\ninstance, if we have qi = qy, then we pick off-diagonal relations involving qy, and we solve the four equations given by:\n1 -r11 + r22 -r33 = 4q2\ny\nr13 -r31 = 4q0qy\nr32 + r23 = 4qyqz\nr13 + r31 = 4qzqz\nThis system of four equations gives us a direct way of going from quaternions to an orthonormal rotation matrix. Note that this\ncould be 9 numbers that could be noisy, and we want to make sure we have best fits.\n18.3\nTransformations: Incorporating Scale\nThus far, for our problem of absolute orientation, we have considered transformations between two coordinate systems of being\ncomposed of translation and rotation. This is often sufficient, but in some applications and domains, such as satellite imaging\nfr topographic reconstruction, we may be able to better describe these transformations taking account not only translation and\nrotation, but also scaling.\nTaking scaling into account, we can write the relationship between two point clouds corresponding to two different coordi-\nnate systems as:\nr′\nr = sR(r′\nl)\nWhere rotation is again given by R ∈SO(3), and the scaling factor is given by s ∈R+ (where R+ ∆= {x : x ∈R , x > 0}. Recall\nthat r′\nr and r′\nl are the centroid-subtracted variants of the point clouds in both frames of reference.\n18.3.1\nSolving for Scaling Using Least Squares: Asymmetric Case\nAs we did before, we can write this as a least-squares problem over the scaling parameter s:\nmin\ns\nn\nX\ni=1\n||r′\nr,i -sR(r′\nl,i)||2\n\nAs we did for translation and rotation, we can solve for an optimal scaling parameter:\ns∗= arg min\ns\nn\nX\ni=1\n||r′\nr,i -sR(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2s\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s2\nn\nX\ni=1\n||R(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2s\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s2\nn\nX\ni=1\n||r′\nl,i||2\n(Rotation preserves vector lengths)\nNext, let us define the following terms:\n1. sr\n∆= Pn\ni=1\n\n||r′\nr,i||2\n\n2. D\n∆= Pn\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n3. sl\n∆= Pn\ni=1 ||r′\nl,i||2\nThen we can write this objective for the optimal scaling factor s∗as:\ns∗= arg min\ns {J(s)\n∆= sr -2sD + s2sl}\nSince this is an unconstrained optimization problem, we can solve this by taking the derivative w.r.t. s and setting it equal to 0:\ndJ(s)\nds\n= d\nds\n\nsr -2sD + s2sl\n\n= 0\n= -2D + s2sl = 0 =⇒s = D\nsl\nAs we also saw with rotation, this does not give us an exact answer without finding the orthonormal matrix R, but now we are\nable to remove scale factor and back-solve for it later using our optimal rotation.\n18.3.2\nIssues with Symmetry\nSymmetry question: What if instead of going from the left coordinate system to the right one, we decided to go from right\nto left? In theory, this should be possible: we should be able to do this simply by negating translation and inverting our\nrotation and scaling terms. But in general, doing this in practice with our OLS approach above does not lead to sinverse = 1\ns\n- i.e. inverting the optimal scale factor does not give us the scale factor for the reverse problem.\nIntuitively, this is the case because the version of OLS we used above \"cheats\" and tries to minimize error by shriking the\nscale by more than it should be shrunk. This occurs because it brings the points closer together, thereby minimizing, on average,\nthe error term. Let us look at an alternative formulation for our error term that accounts for this optimization phenomenon.\n18.3.3\nSolving for Scaling Using Least Squares: Symmetric Case\nLet us instead write our objective as:\nei = 1\n√sr′\nr,i = √sR(r′\nl,i)\nThen we can write our objective and optimization problem over scale as:\ns∗= arg min\ns\nn\nX\ni=1\n|| 1\n√sr′\nr,i -√sR(r′\nl,i)||2\n= arg min\ns\nn\nX\ni=1\ns||r′\nr,i||2\n\n-2\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s\nn\nX\ni=1\n||R(r′\nl,i)||2\n= arg min\ns\ns\nn\nX\ni=1\n\n||r′\nr,i||2\n\n-2\nn\nX\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n+ s\nn\nX\ni=1\n||r′\nl,i||2\n(Rotation preserves vector lengths)\nWe then take the same definitions for these terms that we did above:\n\n1. sr\n∆= Pn\ni=1\n\n||r′\nr,i||2\n\n2. D\n∆= Pn\ni=1\n\nr′\nr,iR(r′\nl,i)\n\n3. sl\n∆= Pn\ni=1 ||r′\nl,i||2\nThen, as we did for the asymmetric OLS case, we can write this objective for the optimal scaling factor s∗as:\ns∗= arg min\ns {J(s)\n∆= 1\nssr -2D + ssl}\nSince this is an unconstrained optimization problem, we can solve this by taking the derivative w.r.t. s and setting it equal to 0:\ndJ(s)\nds\n= d\nds\nssr -2D + ssl\n\n= 0\n= -1\ns2 sr + sl = 0 =⇒s2 = sl\nsr\nTherefore, we can see that going in the reverse direction preserves this inverse (you can verify this mathematically and intu-\nitively by simply setting r′\nr,i ↔r′\nl,i ∀i ∈{1, ..., n} and noting that you will get s2\ninverse = sr\nsl ). Since this method better preserves\nsymmetry, it is preferred.\nIntuition: Since s no longer depends on correspondences (matches between points in the left and right point clouds), then\nthe scale simply becomes the ratio of the point cloud sizes in both coordinate systems (note that sl and sr correspond to the\nsummed vector lengths of the centroid-subtracted point clouds, which means they reflect the variance/spread/size of the point\ncloud in their respective coordinate systems.\nWe can deal with translation and rotation in a correspondence-free way, while also allowing for us to decouple rotation. Let us\nalso look at solving rotation, which is covered in the next section.\n18.4\nSolving for Optimal Rotation in Absolute Orientation\nRecall for rotation (see lecture 18 for details) that we switched from optimizing over orthonormal rotation matrices to opti-\nmizing over quaternions due to the lessened number of optimization constraints that we must adhere to. With our quaternion\noptimization formulation, our problem becomes:\nmax\noq\noq\nT N\noq, subject to\noq\nT oq = 1\nIf this were an unconstrained optimization problem, we could solve by taking the derivative of this objective w.r.t. our quaternion\noq and setting it equal to zero. Note the following helpful identities with matrix and vector calculus:\n1.\nd\nda(a · b) = b\n2.\nd\nda(aT Mb) = 2Mb\nHowever, since we are working with quaternions, we must take this constraint into account. We saw in lecture 18 that we did\nthis with using Lagrange Multiplier - in this lecture it is also possible to take this specific kind of vector length constraint\ninto account using Rayleigh Quotients.\nWhat are Rayleigh Quotients?\nThe intuitive idea behind them: How do I prevent my parameters from becoming too\nlarge) positive or negative) or too small (zero)? We can accomplish this by dividing our objective by our parameters, in this\ncase our constraint. In this case, with the Rayleigh Quotient taken into account, our objective becomes:\noq\nT N\noq\noq\nT oq\n\nRecall that N\n∆=\nn\nX\ni=1\nRT\nl,iRr,i\n\nHow do we solve this? Since this is now an unconstrained optimization problem, we can solve this simply using the rules of\ncalculus:\nJ(\noq)\n∆==\noq\nT N\noq\noq\nT oq\ndJ(\noq)\nd\noq\n= d\nd\noq\noq\nT N\noq\noq\nT oq\n= 0\n=\nd\nd\noq(\noq\nT N\noq)\noq\nT oq -\noq\nT N\noq d\nd\noq(\noq\nT oq)\n(\noq\nT oq)2\n= 0\n= 2N\noq\noq\nT oq\n-\noq\n(\noq\nT oq)2\n(\noq\nT N\noq) = 0\nFrom here, we can write this first order condition result as:\nN\noq =\noq\nT N\noq\noq\nT oq\noq\nNote that\noq\nT N\noq\noq\nT oq ∈R (this is our objective). Therefore, we are searching for a vector of quaternion coefficients such applying the\nrotation matrix to this vector simply produces a scalar multiple of it - i.e. an eigenvector of the matrix N. Letting λ\n∆=\noq\nT N\noq\noq\nT oq ,\nthen this simply becomes N\noq = λ\noq. Since this optimization problem is a maximization problem, this means that we can pick\nthe eigenvector of N that corresponds to the largest eigenvalue (which in turn maximizes the objective consisting of the\nRayleigh quotient\noq\nT N\noq\noq\nT oq , which is the eigenvalue.\nEven though this quaternion-based optimization approach requires taking this Rayleigh Quotient into account, it is much easier\nto do this optimization than to solve for orthonormal matrices, which either require a complex Lagrangian (if we solve with\nLagrange multipliers) or an SVD decomposition from Euclidean space to the SO(3) group (which also happens to be a manifold).\nThis approach raises a few questions:\n- How many correspondences are needed to solve these optimization problems? Recall a correspondence is when we say\ntwo 3D points in different coordinate systems belong to the same point in 3D space, i.e. the same point observed in two\nseparate frames of reference.\n- When do these approaches fail?\nWe cover these two questions in more detail below.\n18.4.1\nHow Many Correspondences Do We Need?\nRecall that we are looking for 6 parameters (for translation and rotation) or 7 parameters (for translation, rotation, and scaling).\nSince each correspondence provides three constraints (since we equate the 3-dimensional coordinates of two 3D points in space),\nassuming non-redundancy, then we can solve this with two correspondences.\nLet us start with two correspondences: if we have two objects corresponding to the correspondences of points in the 3D world,\nthen if we rotate one object about axis, we find this does not work, i.e. we have an additional degree of freedom. Note that the\ndistance between correspondences is fixed.\n\nFigure 33: Using two correspondences leads to only satisfying 5 of the 6 needed constraints to solve for translation and rotation\nbetween two point clouds.\nBecause we have one more degree of freedom, this accounts for only 5 of the 6 needed constraints to solve for translation and\nrotation, so we need to have at least 3 correspondences.\nWith 3 correspondences, we get 9 constraints, which leads to some redundancies.\nWe can add more constraints by incor-\nporating scaling and generalizing the allowable transformations between the two coordinate systems to be the generalized\nlinear transformation - this corresponds to allowing non-orthonormal rotation transformations. This approach gives us 9\nunknowns!\n\na11\na12\na13\na21\na22\na23\na31\na32\na33\n\nx\ny\nz\n\n+\n\na14\na24\na34\n\nBut we also have to account for translation, which gives us another 3 unknowns, giving us 12 in total and therefore requiring at\nleast 4 non-redundant correspondences in order to compute the full general linear transformation. Note that this doesn't have\nany constraints as well!\nOn a practical note, this is often not needed, especially for finding the absolute orientation between two cameras, because\noftentimes the only transformations that need to be considered due to the design constraints of the system (e.g. an autonomous\ncar with two lidar systems, one on each side) are translation and rotation.\n18.4.2\nWhen do These Approaches Fail?\nThese approaches can fail when we do not have enough correspondences. In this case, the matrix N will become singular, and\nwill produce eigenvalues of zero. A more interesting failure case occurs when the points of one or both of the point clouds are\ncoplanar. Recall that we solve for the eigenvalues of a matrix (N in this case) using the characteristic equation given by:\nCharacteristic Equation : det |N -λI| = 0\nLeads to 4th-order polynomial : λ4 + c3λ3 + c2λ2 + c1λ + c0 = 0\nRecall that our matrix N composed of the data has some special properties:\n1. c3 = tr(N) = 0 (This is actually a great feature, since usually the first step in solving 4th-order polynomial systems is\neliminating the third-order term).\n2. c2 = 2tr(M T M), where M is defined as the sum of dyadic products between the points in the point clouds:\nM\n∆=\n\nn\nX\ni=1\nr′\nl,ir′\nr,i\nT\n\n∈R3×3\n3. c1 = 8 det |M|\n4. c0 = det |N|\nWhat happens if det |M| = 0, i.e. the matrix M is singular? Then using the formulas above we must have that the coefficient\nc1 = 0. Then this problem reduces to:\nλ4 + c2λ2 + c0 = 0\nThis case corresponds to a special geometric case/configuration of the point clouds - specifically, when points are coplanar.\n\n18.4.3\nWhat Happens When Points are Coplanar?\nWhen points are coplanar, we have that the matrix N, composed of the sum of dyadic products between the correspondences in\nthe two point clouds, will be singular.\nTo describe this plane in space, we need only find a normal vector ˆn that is orthogonal to all points in the point cloud -\ni.e. the component of each point in the point cloud in the ˆn direction is 0. Therefore, we can describe the plane by the equation:\nr′\nr,i · ˆn = 0 ∀i ∈{1, ..., n}\nFigure 34: A coplanar point cloud can be described entirely by a surface normal of the plane ˆn.\nNote: In the absence of measurement noise, if one point cloud is coplanar, the the other point cloud must be as well (assuming\nthat the transformation between the point clouds is a linear transformation). This does not necessarily hold when measurement\nnoise is introduced.\nRecall that our matrix M, which we used above to compute the coefficients of the characteristic polynomial describing this\nsystem, is given by:\nM\n∆=\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT\nThen, rewriting our equation for the plane above, we have:\nr′\nr,i · ˆn = 0 =⇒M ˆn =\n\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT\n\nˆn\n=\nn\nX\ni=1\nr′\nr,ir′\nl,i\nT ˆn\n=\nn\nX\ni=1\nr′\nr,i0\n= 0\nTherefore, when a point cloud is coplanar, the null space of M is non-trivial (it is given by at least Span({ˆn}), and therefore\nM is singular. Recall that a matrix M ∈Rn×d is singular if ∃x ∈Rd, x = 0 such that Mx = 0, i.e. the matrix has a non-trivial\nnull space.\n18.4.4\nWhat Happens When Both Coordinate Systems Are Coplanar\nVisually, when two point clouds are coplanar, we have:\n\nFigure 35: Two coplanar point clouds. This particular configuration allows us to estimate rotation in two simpler steps.\nIn this case, we can actually decompose finding the right rotation into two simpler steps!\n1. Rotate one plane so it lies on top of the other plane. We can read offthe axis and angle from the unit normal vectors of\nthese two planes describing the coplanarity of these point clouds, given respectively by ˆn1 and ˆn2:\n- Axis: We can find the axis by noting that the axis vector will be parallel to the cross product of ˆn1 and ˆn2, simply\nscaled to a unit vector:\nˆω =\nˆn1 × ˆn2\n||ˆn1 × ˆn||2\n- Angle: We can also solve for the angle using the two unit vectors ˆn1 and ˆn2:\ncos θ = ˆn1 · ˆn2\nsin θ = ˆn1 × ˆn2\nθ = arctan 2\nsin θ\ncos θ\n\nWe now have an axis angle representation for rotation between these two planes, and since the points describe each of the\nrespective point clouds, therefore, a rotation between the two point clouds! We can convert this axis-angle representation\ninto a quaternion with the formula we have seen before:\noq =\n\ncos θ\n2, sin θ\n2 ˆω\n\n2. Perform an in-plane rotation. Now that we have the quaternion representing the rotation between these two planes, we can\norient two planes on top of each other, and then just solve a 2D least-squares problem to solve for our in-place rotation.\nWith these steps, we have a rotation between the two point clouds!\n18.5\nRobustness\nIn many methods in this course, we have looked at the use of Least Squares methods to solve for estimates in the presence\nof noise and many data points. Least squares produces an unbiased, minimum-variance estimate if (along with a few other\nassumptions) the dataset/measurement noise is Gaussian (Gauss-Markov Theorem) [1]. But what if the measurement noise is\nnon-Gaussian? How do we deal with outliers in this case?\nIt turns out that Least Squares methods are not robust to outliers. One alternative approach is to use absolute error in-\nstead. Unfortunately, however, using absolute error does not have a closed-form solution. What are our other options for dealing\nwith outliers? One particularly useful alternative is RANSAC.\nRANSAC, or Random Sample Consensus, is an algorithm for robust estimation with least squares in the presence\nof outliers in the measurements. The goal is to find a least squares estimate that includes, within a certain threshold band, a\nset of inliers corresponding to the inliers of the dataset, and all other points outside of this threshold bands as outliers. The\nhigh-level steps of RANSAC are as follows:\n1. Random Sample: Sample the minimum number of points needed to fix the transformation (e.g. 3 for absolute orientation;\nsome recommend taking more).\n\n2. Fit random sample of points: Usually this involves running least squares on the sample selected. This fits a line (or\nhyperplane, in higher dimensions), to the randomly-sampled points.\n3. Check Fit: Evaluate the line fitted on the randomly-selected subsample on the rest of the data, and determine if the fit\nproduces an estimate that is consistent with the \"inliers\" of your dataset. If the fit is good enough accept it, and if it is\nnot, run another sample. Note that this step has different variations - rather than just immediately terminating once you\nhave a good fit, you can run this many times, and then take the best fit from that.\nFurthermore, for step 3, we threshold the band from the fitted line/hyperplane to determine which points of the dataset are\ninliers, and which are outliers (see figure below). This band is usually given by a 2ε band around the fitted line/hyperplane.\nTypically, this parameter is determined by knowing some intrinsic structure about the dataset.\nFigure 36: To evaluate the goodness of fit of our sampled points, as well as to determine inliers and outliers from our dataset,\nwe have a 2ε thick band centered around the fitted line.\nAnother interpretation of RANSAC: counting the \"maximimally-occupied\" cell in Hough transform parameter space! Another\nway to find the best fitting line that is robust to outliers:\n1. Repeatedly sample subsets from the dataset/set of measurements, and fit these subsets of points using least squares\nestimates.\n2. For each fit, map the points to a discretized Hough transform parameter space, and have an accumulator array that keeps\ntrack of how often a set of parameters falls into a discretized cell. Each time a set of parameters falls into a discretized\ncell, increment it by one.\n3. After N sets of random samples/least squares fits, pick the parameters corresponding to the cell that is \"maximally-\noccupied\", aka has been incremented the most number of times! Take this as your outlier-robust estimate.\nFigure 37: Another way to perform RANSAC using Hough Transforms: map each fit from the subsamples of measurements to\na discretized Hough Transform (parameter) space, and look for the most common discretized cell in parameter space to use for\nan outlier-robust least-squares estimate.\n\n18.6\nSampling Space of Rotations\nNext, we will shift gears to discuss the sampling space of rotations.\nWhy are we interested in this space?\nMany orientation problems we have studied so far do not have a closed-form\nsolution and may require sampling. How do we sample from the space of rotations?\n18.6.1\nInitial Procedure: Sampling from a Sphere\nLet us start by sampling from a unit sphere (we will start in 3D, aiming eventually for 4D, but our framework will gener-\nalize easily from 3D to 4D). Why a sphere? Recall that we are interested in sampling for the coefficients of a unit quaternion\noq = (q0, qx, qy, qz), ||\noq||2\n2 = 1.\nOne way to sample from a sphere is with latitude and longitude, given by (θi, φi), respectively. The problem with this ap-\nproach, however, is that we sample points that are close together at the poles. Alternatively, we can generate random longitude\nθi and φi, where:\n- -π\n2 ≤θi ≤π\n2 ∀i\n- -π ≤φi ≤π ∀i\nBut this approach suffers from the same problem - it samples too strongly from the poles. Can we do better?\n18.6.2\nImproved Approach: Sampling from a Cube\nTo achieve more uniform sampling from a sphere, what if we sampled from a unit cube (where the origin is given the center of\nthe cube), and map the sampled points to an enscribed unit sphere within the cube?\nIdea: Map all points (both inside the sphere and outside the sphere/inside the cube) onto the sphere by connecting a line\nfrom the origin to the sampled point, and finding the point where this line intersects the sphere.\nFigure 38: Sampling from a sphere by sampling from a cube and projecting it back to the sphere.\nProblem with this approach: This approach disproportionately samples more highly on/in the direction of the cube's\nedges. We could use sampling weights to mitigate this effect, but better yet, we can simply discard any samples that fall outside\nthe sphere. To avoid numerical issues, it is also best to discard points very close to the sphere.\nGeneralization to 4D: As we mentioned above, our goal is to generalize this from 3D to 4D. Cubes and spheres simply\nbecome 4-dimensional - enabling us to sample quaternions.\n18.6.3\nSampling From Spheres Using Regular and Semi-Regular Polyhedra\nWe saw the approach above requires discarding samples, which is computationally-undesirable because it means we will proba-\nbilistically have to generate more samples than if we were able to sample from the sphere alone. To make this more efficient, let\nus consider shapes that form a \"tighter fit\" around the sphere - for instance: polyhedra! Some polyhedra we can use:\n- Tetrahedra (4 faces)\n- Hexahedra (6 faces)\n- Octahedra (8 faces)\n- Dodecahedra (12 faces)\n\n- Icosahedra (20 faces)\nThese polyhedra are also known as the regular solids.\nAs we did for the cube, we can do the same for polyhedra: to sample from the sphere, we can sample from the polyhedra,\nand then project onto the point on the sphere that intersects the line from the origin to the sampled point on the polyhedra.\nFrom this, we get great circles from the edges of these polyhedra on the sphere when we project.\nFun fact: Soccer balls have 32 faces! More related to geometry: soccer balls are part of a group of semi-regular solids,\nspecifically an icosadodecahedron.\n18.6.4\nSampling in 4D: Rotation Quaternions and Products of Quaternions\nNow we are ready to apply these shapes for sampling quaternions in 4D. Recall that our goal with this sampling task is to\nfind the rotation between two point clouds, e.g. two objects. We need a uniform way of sampling this spae. We can start\nwith the hexahedron. Below are 10 elementary rotations we use (recall that a quaternion is given in axis-angle notation by\noq = (cos\n\nθ\n\n, sin\nπ\n\nˆω)):\n1. Identity rotation:\noq = (1, 0)\n2. π about ˆx:\noq = (cos\nπ\n\n, sin\nπ\nˆx) = (0, ˆx)\n3. π about ˆy:\noq = (cos\nπ\n\n, sin\nπ\nˆy) = (0, ˆy)\n4. π about ˆz:\noq = (cos\nπ\n\n, sin\nπ\nˆz) = (0, ˆz)\n5.\nπ\n2 about ˆx:\noq = (cos\nπ\n\n, sin\nπ\nˆx) =\n√\n2(1, ˆx)\n6.\nπ\n2 about ˆy:\noq = (cos\nπ\n\n, sin\nπ\nˆy) =\n√\n2(1, ˆy)\n7.\nπ\n2 about ˆz:\noq = (cos\nπ\n\n, sin\nπ\nˆz) =\n√\n2(1, ˆz)\n8. -π\n2 about ˆx:\noq = (cos\n-π\n\n, sin\n-π\nˆx) =\n√\n2(1, -ˆx)\n9. -π\n2 about ˆy:\noq = (cos\n-π\n\n, sin\n-π\nˆy) =\n√\n2(1, -ˆy)\n10. -π\n2 about ˆz:\noq = (cos\n-π\n\n, sin\n-π\nˆz) =\n√\n2(1, -ˆz)\nThese 10 rotations by themselves give us 10 ways to sample the rotation space. How can we construct more samples? We can\ndo so by taking quaternion products, specifically, products of these 10 quaternions above. Let us look at just a couple of\nthese products:\n1. (0, ˆx)(0, ˆy):\n(0, ˆx)(0, ˆy) = (0 -ˆx · ˆy, 0ˆx + 0ˆy + ˆx × ˆy)\n= (-ˆx · ˆy, ˆx × ˆy)\n= (0, ˆz)\nWe see that this simply produces the third axis, as we would expect. This does not give us a new rotation to sample from.\nNext, let us look at one that does.\n2.\n√\n2(1, ˆx) 1\n√\n2(1, ˆy):\n√\n2(1, ˆx) 1\n√\n2(1, ˆy) = 1\n2(1 -ˆx · ˆy, ˆy + ˆx + ˆx × ˆy)\n= 1\n2(1, ˆx + ˆy + ˆx × ˆy)\nThis yields the following axis-angle representation:\n- Axis:\n√\n3(1 1 1)\n\n- Angle: cos\n\nθ\n\n= 1\n2 =⇒\nθ\n2 = π\n3 =⇒θ = 2π\nTherefore, we have produced a new rotation that we can sample from!\nThese are just a few of the pairwise quaternion products we can compute. It turns out that these pairwise quaternion products\nproduce a total of 24 new rotations from the original 10 rotations. These are helpful for achieving greater sampling granularity\nwhen sampling the rotation space.\n18.7\nReferences\n1. Gauss-Markov Theorem, https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov theorem\nLecture 20: Space of Rotations, Regular Tessellations, Critical Surfaces in\nMotion Vision and Binocular Stereo\nIn this lecture, we will transition from solving problems of absolute rotation (which, if you recall, finds the transformation\nbetween two 3D coordinate systems) into relative orientation, which finds the transformation between two 2D coordinate\nsystems. We will start by covering the problem of binocular stereo, and in the process talk about tessellations from solids,\ncritical surfaces.\n19.1\nTessellations of Regular Solids\nAs a brief review from the last lecture, recall that we saw we can encode rotations as 4D points on the unit sphere, where the\ncoordinates of these 4D points correspond to our coefficients for the unit quaternion.\nWhat are tessellations? \"A filling or tessellations of a flat surface is the covering of a plane using one or more geomet-\nric shapes (polygons).\" [1]. Tessellations of the surface of the sphere can be based on platonic solids, with 4, 6, 8, 12, and\n20 faces. Each of the tessellations from the platonic solids results in equal area projections on the sphere, but the division is\nsomewhat coarse.\nFor greater granularity, we can look at the 14 Archimedean solids. This allows for having multiple polygons in each\npolyhedra (e.g. squares and triangles), resulting in unequal area in the tessellations on the unit sphere.\nRelated, we are also interested in the rotation groups (recall groups are mathematical sets that obey certain algebras,\ne.g. the Special Orthogonal group) of these regular polyhedra:\n- 12 elements in rotation group for tetrahedron.\n- 24 elements in rotation group for hexahedron.\n- 60 elements in rotation group for dodecahedron.\n- The octahedron is the dual of the cube and therefore occupies the same rotation group as it.\n- The icosahedron is the dual of the dodecahedron and therefore occupies the same rotation group as it.\nA few other notes on tessellations:\n- One frequently-used method for creating tessellations is to divide each face into many triangular or hexagonal areas.\n- Histograms can be created in tessellations in planes by taking square sub-divisions of the region.\n- Hexagonal tessellations are a dual of triangular tessellations.\n19.2\nCritical Surfaces\nWhat are Critical Surfaces? Critical surfaces are geometric surfaces - specifically, hyperboloids of one sheet, that lead to\nambiguity in the solution space for relative orientation problems.\nWhy are Critical Surfaces important?\nCritical surfaces can negatively impact the performance of relative orientation\nsystems, and understanding their geometry can enable us to avoid using strategies that rely on these types of surfaces in order\nto find the 2D transformation, for instance, between two cameras.\n\nWe will discuss more about these at the end of today's lecture. For now, let's introduce quadrics - geometric shapes/sur-\nfaces defined by second-order equations in a 3D Cartesian coordinate system:\n1. Ellipsoid:\nx2\na2 + y2\nb2 + z2\nc2 = 1\nFigure 39: 3D geometric depiction of an ellipsoid, one member of the quadric family.\n2. Sphere: x2 + y2 + z2 = 1 (or more generally, = r ∈R )\nFigure 40: 3D geometric depiction of a sphere, another member of the quadric family and a special case of the ellipse.\n3. Hyperboloid of One Sheet: x2 + y2 -z2 = 1\nThis quadric surface is ruled: we can embed straight lines in the surface, despite its quadric function structure.\nFigure 41: 3D geometric depiction of a hyperboloid of one sheet, another member of the quadric family and a special case of the\nellipse.\n4. Hyperboloid of Two Sheets: x2 -y2 -z2 = 1\n\nFigure 42: 3D geometric depiction of a hyperboloid of one sheet, another member of the quadric family and a special case of the\nellipse.\n5. Cone:\nz2\nc2 = x2\na2 + y2\nb2\nFigure 43: 3D geometric depiction of a cone, another member of the quadric family and a special case of the hyperboloid of one\nsheet.\n6. Elliptic Paraboloid:\nz\nc = x2\na2 + y2\nb2\nNote that this quadric surface has a linear, rather than quadratic dependence, on z.\nFigure 44: 3D geometric depiction of a elliptic paraboloid, another member of the quadric family and a special case of the\nhyperboloid of one sheet.\n7. Hyperbolic Paraboloid:\nz\nc = x2\na2 -y2\nb2\nNote that this quadric surface also has a linear, rather than quadratic dependence, on z.\n\nFigure 45: 3D geometric depiction of a hyperbolic paraboloid, another member of the quadric family and a special case of the\nhyperboloid of one sheet.\nAnother special case is derived through planes. You may be wondering - how can we have a quadratic structure from planes? We\ncan derive a surface with quadratic terms by considering the intersection of two planes. This intersection of planes is computed\nanalytically as a product of two linear equations, resulting in a quadratic equation.\n19.3\nRelative Orientation and Binocular Stereo\nProblem of interest: Computing 3D from 3D using two cameras - a problem known as binocular stereo. We found this\nproblem was easy to solve if the geometry of the two cameras, in this case known as the two-view geometry, is known and\ncalibrated (usually this results in finding a calibration matrix K). To achieve high-performing binocular stereo systems, we need\nto find the relative orientation between the two cameras. A few other notes on this:\n- Calibration is typically for binocular stereo using baseline calibration.\n- Recall that like absolute orientation, we have duality in te problems we solve: we are able to apply the same machine\nvision framework regardless of \"if the camera moved of it the world moved\".\n- Therefore, the dual problem to binocular stereo is structure from motion, also known as Visual Odometry (VO).\nThis involves finding transformations over time from camera (i.e. one moving camera at different points in time).\n19.3.1\nBinocular Stereo\nRecall that our goal with binocular stereo is to find the translation (known as the baseline, given by b ∈R3. The diagram\nbelow illustrates our setup with two cameras, points in the world, and the translation between the two cameras given as this\naforementioned baseline b.\nFigure 46: Binocular stereo system set up. For this problem, recall that one of our objectives is to measure the translation, or\nbaseline, between the two cameras.\nWhen we search for correspondences, we need only search along a line, which gives us a measure of binocular disparity\nthat we can then use to measure distance between the cameras.\n\nThe lines defined by these correspondences are called epipolar lines. Places that pass through epipolar lines are called\nepipolar planes, as given below:\nFigure 47: Epipolar planes are planes that pass through epipolar lines.\nNext, in the image, we intersect the image plane with our set of epipolar planes, and we look at the intersections of these\nimage planes (which become lines). The figure below illustrates this process for the left and right cameras in our binocular stereo\nsystem.\nFigure 48: After finding the epipolar planes, we intersect these planes with the image plane, which gives us a set of lines in both\nthe left and righthand cameras/coordinate systems of our binocular stereo system.\nA few notes on this process:\n- For system convergence, we want as much overlap as possible between the two sets of lines above, depicted in Figure 10.\n- The entire baseline b projects to a single point in each image.\n- As we stated before, there are line correspondences if we already know the geometry, i.e. if we want correspondences\nbetween the lines in red above, we need only look along the red lines in each image.\n- Recall that our goal is to find the transformation between two cameras - i.e. baseline (or translation, in dual structure\nfrom motion problem).\n- Recall, as we saw for absolute orientation, that we also solve for rotation in addition to the baseline/translation.\nTogether, these translation + rotation variables lead us to think that we are solving a problem of 6 degrees of freedom\n(DOF), but because of scale factor ambiguity (we cannot get absolute sizes of objects/scenes we image), we cannot\nget absolute length of the baseline b, and therefore, we treat the baseline as a unit vector. Treating the baseline as a\nunit vector results in one less DOF and 5 DOF for the system overall (since we now only have 2 DOF for the unit vector\nbaseline).\n19.3.2\nHow Many Correspondences Do We Need?\nAs we have done so for other systems/problems, we consider the pertinent question: How many correspondences do we\nneed to solve the binocular stereo/relative orientation problem?. To answer this, let us consider what happens when\nwe are given different numbers of correspondences:\n- With 1 correspondence, we have many degrees of freedom left - i.e. not enough constraints for the number of unknowns\nremaining. This makes sense considering that each correspondence imposes a maximum of 3 DOF for the system.\n\n- With 2 correspondences, we still have the ability to rotate one of the cameras without changing the correspondence, which\nimplies that only 4 out of the 5 constraints are satisfied. This suggests we need more constraints.\nIt turns out we need 5 correspondences needed to solve the binocular stereo/relative orientation problem - each correspondence\ngives you 1 constraint. Why? Each image has a disparity, with two components (this occurs in practice). There are two\ntypes of disparity, each corresponding to each of the 2D dimensions. Note that disparity computes pixel discrepancies between\ncorrespondences between images.\n- Horizontal disparity corresponds to depth.\n- Vertical disparity corresponds to differences in orientation, and actually takes the constraints into account. In practice,\nvertical disparity is tuned out first using an iterative sequence of 5 moves.\nIn practice, we would like to use more correspondences for accuracy. With more correspondences, we no longer try to find\nexact matches, but instead minimize a sum of squared errors using least squares methods. We minimize sum of squares of error\nin image position, not in the 3D world.\nHowever, one issue with this method is nonlinearity:\n- This problem setup results in 7 second-order equations, which, by Bezout's Theorem, gives 27 = 128 different solutions to\nthis problem.\n- Are there really 128 solutions? There are methods that have gotten this down to 20. With more correspondences and more\nbackground knowledge of your system, it becomes easier to determine what the true solution is.\n19.3.3\nDetermining Baseline and Rotation From Correspondences\nTo determine baseline and rotation for this binocular stereo problem, we can begin with the figure below:\nFigure 49: Epipolar plane for the binocular stereo system.\nThe vectors r′\nl,i (the left system measurement after the rotation transformation has been applied), rr,i, and b are all coplanar\nin a perfect system. Therefore, if we consider the triple product of these 3 vectors, the volume of the parallelipiped, which can\nbe constructed from the triple product, should have zero volume because these vectors are coplanar (note that this is the\nideal case). This is known as the coplanarity condtion:\nV = [′\nl,i rr,i b] = 0\nA potential solution to find the optimal baseline (and rotation): we can use least squares to minimize the volume of the paral-\nlelipiped corresponding to the triple product of these three (hopefully near coplanar) vectors. This is a feasible approach, but\nalso have a high noise gain/variance.\nThis leads us to an important question: What, specifically, are we trying to minimize? Recall that we are matching cor-\nrespondences in the image, not in the 3D world. Therefore, the volume of the parallelipiped is proportional to the\nerror, but does not match it exactly. When we have measurement noise, the rays from our vectors in the left and righthand\nsystems no longer intersect, as depicted in the diagram below:\n\nFigure 50: With measurement noise, our rays do not line up exactly. We will use this idea to formulate our optimization problem\nto solve for optimal baseline and rotation.\nWe can write that the error is perpendicular to the cross product between the rotated left and the right rays:\nr′\nl,i × rr\nAnd therefore, we can write the equation for the \"loop\" (going from the rotated left coordinate system to the right coordinate\nsystem) as:\nαr′\nl + γ(r′\nl × rr) = b + βrr\nWhere the error we seek to minimize (r′\nl × rr) is multiplied by a parameter γ.\nTo solve for our parameters α, β, and γ, we can transform this vector equation into 3 scalar equations by taking dot prod-\nucts. We want to take dot products that many many terms drop to zero, i.e. where orthogonality applies. Let's look at these 3\ndot products:\n1. ·(r′\nl × rr): This yields the following:\nLefthand side :\n(αr′\nl + γ(r′\nl × rr)) · (r′\nl × rr) = γ||r′\nl × rr||2\nRighthand side :\n(b + βrr) · (r′\nl × rr) = b · r′\nl × rr + 0 = [b r′\nl rr]\nCombining :\nγ||r′\nl × rr||2\n2 = [b r′\nl rr]\nIntuitively, this says that the error we see is proportional to teh triple product (the volume of the parallelipiped). Taking\nour equation with this dot product allows us to isolate γ.\n2. ·(r′\nl × (r′\nl × rr)): This yields the following:\nLefthand side :\n(αr′\nl + γ(r′\nl × rr)) · ((r′\nl × (r′\nl × rr)) = β||r′\nl × rr||2\nRighthand side :\n(b + βrr) · ((r′\nl × (r′\nl × rr)) = (b × r′\nl) · (r′\nl × rr)\nCombining :\nβ||r′\nl × rr||2\n2 = (b × r′\nl) · (r′\nl × rr)\nTaking this dot product with our equation allows us to find β. We can repeat an analogous process to solve for α.\n3. ·(rr × (r′\nl × rr)): This yields the following:\nLefthand side :\n(αr′\nl + γ(r′\nl × rr)) · ((rr × (r′\nl × rr)) = α||r′\nl × rr||2\nRighthand side :\n(b + βrr) · ((rr × (r′\nl × rr)) = (b × rr) · (r′\nl × rr)\nCombining :\nα||r′\nl × rr||2\n2 = (b × rr) · (r′\nl × rr)\nTaking this dot product with our equation allows us to find α.\nWith this, we have now isolated all three of our desired parameters. We can then take these 3 equations to solve for our 3\nunknowns α, β, and γ. We want |γ| to be as small as possible. We also require α and β to be non-negative, since these indicate\nthe scalar multiple of the direction along the rays in which we (almost) get an intersection between the left and right coordinate\nsystems. Typically, a negative α and/or β results in intersections behind the camera, which is often not physically feasible.\nIt turns out that one of the ways discard some of the 20 solutions produced by this problem is to throw out solutions that\nresult in negative α and/or β.\nNext, we can consider the distance this vector corresponding to the offset represents. This distance is given by:\nd = γ||r′\nl × rr||2 =\n[b r′\nl rr]\n||r′\nl × rr||2\n\nClosed-Form Solution: Because this is a system involving 5 second-order equations, and the best we can do is reduce this\nto a single 5th-order equation, which we do not (yet) have the closed-form solutions to, we cannot solve for this problem in\nclosed-form. However, we can still solve for it numerically. We can also look at solving this through a weighted least-squares\napproach below.\n19.3.4\nSolving Using Weighted Least Squares\nBecause our distance d can get very large without a huge error in image position, d is not representative of our error of interest.\nHowever, it is still proportional. We can account for this difference by using a weighting factor wi, which represents the conver-\nsion factor from 3D error to 2D image error). We will denote the ith weight as wi.\nTherefore, incorporating this weighting factor, our least squares optimization problem becomes:\nmin\nb,R(·)\nn\nX\ni=1\nwi[b r′\nl rr]2, subject to b · b = ||b||2\n2 = 1\nHow do we solve this? Because wi will change as our candidate solution changes, we will solve this problem iteratively and in\nan alternating fashion - we will alternate between updated our conversion weights wi and solving for a new objective given the\nrecent update of weights. Therefore, we can write this optimization problem as:\nb∗, R∗= arg\nmin\nb,||b||2\n2=1,R(·)\nn\nX\ni=1\nwi[b r′\nl rr]2\n= arg\nmin\nb,||b||2\n2=1,R(·)\nn\nX\ni=1\nwi\n\n(rr,i × b) · r′\nl,i\nIntuition for these weights: Loosely, we can think of the weights as being the conversion factor from 3D to 2D error, and\ntherefore, they can roughly be thought of as w = f\nZ , where f is the focal length and Z is the 3D depth.\nNow that we have expressed a closed-form solution to this problem, we are ready to build offof the last two lectures and\napply our unit quaternions to solve this problem. Note that because we express the points from the left coordinate system in a\nrotated frame of reference (i.e. with the rotation already applied), then we can incorporate the quaternion into this definition\nto show how we can solve for our optimal set of parameters given measurements from both systems:\nor\n′\nl =\noq\norl\noq\n∗\nThen we can solve for this as t:\nt = (rr × b) · r′\nl\n=\norr\no\nb ·\noq\norl\noq\n∗\n=\norr(\no\nb\noq) ·\noq\norl\n=\nord\no\nd ·\noq\norl, where\no\nd\n∆=\no\nb\noq, which we can think of as a product of baseline and rotation.\nRecall that our goal here is to find the baseline\no\nb - this can be found by solving for our quantity\no\nd and multiplying it on the\nrighthand side by\noq\n∗:\no\nd\noq\n∗=\no\nb\noq\noq\n∗=\no\nb\noe =\no\nb\n(Recall that\noe = (1, 0) =\noq\noq\n∗)\nAt first glance, it appears we have 8 unknowns, with 5 constraints. But we can add additional constraints to the system to make\nthe number of constraints equal the number of DOF:\n1. Unit quaternions: ||\noq||2 =\noq ·\noq = 1\n2. Unit baseline: ||\no\nb||2 =\no\nb ·\no\nb = 1\n3. Orthogonality of\noq and\no\nd:\noq ·\no\nd = 0\nTherefore, with these constraints, we are able to reach a system with 8 constraints. Note that we have more constraints to\nenforce than with the absolute orientation problem, making the relative orientation problem more difficult to solve.\n\n19.3.5\nSymmetries of Relative Orientation Approaches\nWe can interchange the left and right coordinate system rays. We can do this for this problem because we have line intersections\nrather than line rays. These symmetries can be useful for our numerical approaches. The equation below further demonstrates\nthis symmetry by showing we can interchange the order of how\no\nd and\noq interact with our measurements to produce the same\nresult.\nt =\norr\no\nd ·\noq\norl\n=\norr\noq ·\no\nd\norl\nGiven these symmetries, we can come up with some sample solutions:\n1. {\noq,\no\nd}\n2. {-\noq,\no\nd} (×2)\n3. {\noq, -\no\nd} (×2)\n4. {\no\nd,\noq} (×2)\nHow do we avoid having to choose from all these solutions? One approach: Assume/fix one of the two unknowns\no\nd\nor\noq. This results in a linear objective and a linear problem to solve, which, as we know, can be solved with least squares\napproaches, giving us a closed-form solution:\n- Option 1: Assume\noq is known and fix it -→solve for\no\nd\n- Option 2: Assume\no\nd is known and fix it -→solve for\noq\nNote that both of the approaches above can be solved with least squares!\nWhile this is one approach, a better approach is to use nonlinear optimization, such as Levenberg-Marquadt (often\ncalled LM in nonlinear optimization packages). An optional brief intro to Levenberg-Marquadt is presented at the end of this\nlecture summary.\nLevenberg-Marquadt (LM) optimization requires a non-redundant parameterization, which can be achieved with either Gibbs\nvectors or quaternions (the latter of which we can circumvent the redundancies of Hamilton's quaternions by treating the\nredundancies of this representation as extra constraints. Recall that Gibbs vectors, which are given as (cos\n\nθ\n\n, sin\n\nθ\n\nˆω), have\na singularity at θ = π.\n19.3.6\nWhen Does This Fail? Critical Surfaces\nLike all the other machine vision approaches we have studied so far, we would like to understand when and why this system fails:\n- Recall that we need at least 5 correspondences to solve for the baseline and rotation. With less correspondences, we\ndon't have enough constraints to find solutions.\n- Gef arliche Fla achen, also known as \"dangerous or critical surfaces\": There exist surfaces that make the relative\norientation problem difficult to solve due to the additional ambiguity that these surfaces exhibit. One example: Plane\nflying over a valley with landmarks:\n\nFigure 51: A plane flying over a valley is an instance of a critical surface. This is because we can only observe angles, and not\npoints. Therefore, locations of two landmarks are indistinguishable.\nTo account for this, pilots typically plan flight paths over ridges rather than valleys for this exact reason:\nFigure 52: When planes fly over a ridge rather than a valley, the angles between two landmarks do change, allowing for less\nambiguity for solving relative orientation problems.\nHow do we generalize this case to 3D? We will see that this type of surface in 3D is a hyperboloid of one sheet. We\nneed to ensure sections of surfaces you are looking at do not closely resemble sections of hyperboloids of one sheet.\nThere are also other types of critical surfaces that are far more common that we need to be mindful of when con-\nsidering relative orientation problems - for instance, the intersection of two planes. In 2D, this intersection of two planes\ncan be formed by the product of their two equations:\n(ax + by + c)(dx + ey + f) = adx2 + aexy + afx + bdxy + bey2 + bfy + cdx + cey + cf = 0\nWe can see that this equation is indeed second order with respect to its spatial coordinates, and therefore belongs to the\nfamily of quadric surfaces and critical surfaces.\nFigure 53: The intersection of two planes is another type of critical surface we need to be mindful of. It takes a second-order\nform because we multiply the two equations of the planes together to obtain the intersection.\n19.3.7\n(Optional) Levenberg-Marquadt and Nonlinear Optimization\nLevenberg-Marquadt (LM) and Gauss-Newton (GN) are two nonlinear optimization procedures used for deriving solutions to\nnonlinear least squares problems. These two approaches are largely the same, except that LM uses an additional regularization\n\nterm to ensure that a solution exists by making the closed-form matrix to invert in the normal equations positive semidefinite.\nThe normal equations, which derive the closed-form solutions for GN and LM, are given by:\n1. GN: (J(θ)T J(θ))-1θ = J(θ)T e(θ) =⇒θ = (J(θ)T J(θ))-1J(θ)T e(θ)\n2. LM: (J(θ)T J(θ) + λI)-1θ = J(θ)T e(θ) =⇒θ = (J(θ)T J(θ) + λI)-1J(θ)T e(θ)\nWhere:\n- θ is the vector of parameters and our solution point to this nonlinear optimization problem.\n- J(θ) is the Jacobian of the nonlinear objective we seek to optimize.\n- e(θ) is the residual function of the objective evaluated with the current set of parameters.\nNote the λI, or regularization term, in Levenberg-Marquadt. If you're familiar with ridge regression, LM is effectively ridge\nregression/regression with L2 regularization for nonlinear optimization problems. Often, these approaches are solved iteratively\nusing gradient descent:\n1. GN: θ(t+1) ←θ(t) -α(J(θ(t))T J(θ(t)))-1J(θ(t))T e(θ(t))\n2. LM: θ(t+1) ←θ(t) -α(J(θ(t))T J(θ(t)) + λI)-1J(θ(t))T e(θ(t))\nWhere α is the step size, which dictates how quickly the estimates of our approaches update.\n19.4\nReferences\n1. Tesselation, https://en.wikipedia.org/wiki/Tessellation\nLecture 21: Relative Orientation, Binocular Stereo, Structure from Mo-\ntion, Quadrics, Camera Calibration, Reprojection\nToday, we will continue discussing relative orientation, one of our main problems in photogrammetry. We concluded our\nprevious lecture with a discussion of critical surfaces - that is, surfaces which make solving the relative rotation problem difficult.\nIn general, these critical surfaces are quadric surfaces, which take the form:\n±\nx\na\n±\ny\nb\n±\nz\nc\n= 1\nThis form is simple: (i) Centered at the origin, (ii) Axes line up. In general, the form of these quadric surfaces may be more\ncomplicated, with both linear and constant terms. The signs of these qudratic terms determine the shape, as we saw in the\nprevious lecture:\n1. + + + (ellipsoid)\n2. + + -(hyperboloid of one sheet) - this is a class of critical surfaces\n3. + --(hyperboloid of two sheets)\n4. ---(imaginary ellipsoid)\nFor the solutions to the polynomial R2 +· · ·-R = 0 (the class of hyperboloids of one sheets), the solutions tell us why these\ncritical surfaces create solution ambiguity:\n1. R = 0: I.e. the solution is on the surface - the origin of the righthand system is on the surface. Two interpretations from\nthis; the first is for binocular stereo, and the second is for Structure from Motion (SfM):\n(a) Binocular Stereo: The righthand system lies on the surface.\n(b) Structure from Motion: In Structure from Motion, we \"move ont the surface\" in the next time step.\n2. R = -b: This solution is the origin of the lefthand system (we move the solution left along the baseline from the righthand\nsystem to the lefthand system). Two interpretations from this; the first is for binocular stereo, and the second is for\nStructure from Motion (SfM):\n(a) Binocular Stereo: \"The surface has to go through both eyes\".\n\n(b) Structure from Motion: We must start and end back on the surface.\n3. R = kb: Because this system (described by the polynomial in R, then we have no scaling, giving us a solution. This\nsuggests that the entire baseline in fact lies on the surface, which in turn suggests:\n- This setting/surface configuration is rare.\n- This surface is ruled - we can draw lines inscribed in the surface. This suggests that our critical surface is a hyperboloid\nof one sheet. It turns out that we have two rulings for the hyperboloid at one sheet (i.e. at every point on this surface,\nwe can draw two non-parallel lines through the surface that cross at a point).\nHyperboloids of one sheet, as we saw in the previous lecture, are not the only types of critical surfaces. Intersections of planes\nare another key type of critical surface, namely because they are much more common in the world than hyperboloids of one\nsheet. Analytically, the intersection of planes is given by the product of planes:\n(a1X + b1Y + c1Z + d1)(a2X + b2Y + c2Z + d2) = 0\nThis analytic equation describes the intersection of two planes - gives us a quadric surface.\nIn order for this intersection of planes to not have any constants, one of the planes must be the epipolar plane, whose\nimage is a line. The second plane is arbitrary and can take any form.\nPractically, we may not run into this problem in this exact analytic form (i.e. a surface that is an exact instance of a hy-\nperboloid of one sheet), but even as we venture near this condition, the noise amplification factor increases. As it turns out,\nusing a higher/wider Field of View (FOV) helps to mitigate this problem, since as FOV increases, it becomes less and less likely\nthat the surface we are imaging is locally similar to a critical surface.\nOne way that we can increase the FOV is through the use of spider heads, which consist of 8 cameras tied together into\na rigid structure. Though it requires calibration between the cameras, we can \"stitch\" together the images from the 8 cameras\ninto a single \"mosaic image\".\nFigure 54: A spider heads apparatus, which is one technique for increasing the FOV, thereby reducing the probability of surfaces\nresembling critical surfaces.\nNext, we will switch our discussion to another of our photogrammetry problems - interior orientation.\n20.1\nInterior Orientation\nRecall that interior orientation was another one of the 4 photogrammetry problems we study, and involves going from 3D (world\ncoordinate) to 2D (image coordinates). Specifically, we focus on camera calibration.\nHave we already touched on this? Somewhat - with vanishing points. Recall that the goal with vanishing points was to\nfind (x0, y0, f) using calibration objects. This method:\n- Is not very accurate, nor general across different domains/applications\n- Does not take into account radial distortion, which we need to take into account for high-quality imaging such as aerial\nphotography.\nWhat is radial distortion? We touch more on this in the following section.\n\n20.1.1\nRadial Distortion\nThis type of distortion leads to a discrepancy between what should be the projected location of an object in the image, and\nwhat it actually projects to. This distortion is radially-dependent.\nThis type of distortion becomes more apparent when we image lines/edges. It manifests from having a center of distor-\ntion. The image of an object does not appear exactly where it should, and the error/discrepancy depends on the radius of the\npoint in the image relative to the center of distortion. This radial dependence is typically approximated as a polynomial in the\nradius r:\nr = ||r||2\nδx = x(k1r2 + k2r4 + · · · )\nδy = y(k1r2 + k2r4 + · · · )\nFigure 55: Radial distortion is in the direction of the radius emanating from the center of projection to the point in the image\nplane where a point projects to in the absence of radial distortion.\nNote that the error vector δr is parallel to the vector r - i.e. the distortion is in the direction of the radial vector.\nMany high-quality lenses have a defined radial distortion function that gives the distortion as a function of r, e.g. the fig-\nure below:\n0.5\n1.5\nr\ne(r)\nRadial Distortion Function\nThese functions, as we stated above, are typically approximated as polynomial functions of r. How do we measure radial\ndistortion? One way to do so is through the use of plumb lines. These lines are suspended from the ceiling via weights, which\nallows us to assume that they are straight and all parallel to one another in the 3D world. Then, we take an image of these lines,\nwith the image centered on the center lines. We can estimate/assess the degree of radial distortion by identifying the curvature,\nif any, of the lines as we move further away from the center lines of the image.\n\nFigure 56: Plumb lines are a useful way of estimating radial distortion, because they allow us to estimate the degree of curvature\nbetween lines that should be straight and parallel.\nPlumb lines allow us to estimate two types of radial distortion: (i) Barrel Distortion and (ii) Pincushion Distortion,\ndepicted below. The radius of curvature from these lines allows us to measure k1 (the first polynomial coefficient for r2):\nFigure 57: Barrel and pincushion distortion, which we can absorb from plumb lines.\nA more subtle question: is it better to go from )a) undistorted →distorted coordinates, or from (b) distorted →undis-\ntorted coordinates?\nIt is often desirable to take approach (b) (distorted →undistorted) because we can measure the distorted coordinates.\nWse can use series inversion to relate the distorted and undistorted coodinates. This affects the final coordinate system that\nwe do optimization in, i.e. we optimize error in the image plane.\n20.1.2\nTangential Distortion and Other Distortion Factors\nThere are other types of distortion that we can analyze and take into account:\n1. Tangential Distortion: This is a problem we have solved, but to contrast this with radial distortion, rather than the\ndistortion acting radially, the distortion acts tangentially (see figure below). With tangential distortion, our offsets take\nthe form:\nδx = -y(ε1r2 + ε2r4 + · · · )\nδy = x(ε1r2 + ε2r4 + · · · )\nFigure 58: Tangential distortion acts tangentially relative to the radius - in this case, in a counterclockwise fashion (as we rotate\nθ).\n\n2. Decentering: If the center of distortion is not the principal point (center of the image with perspective projection), then\nwe get an offset that depends on position. This offset is typically small, but we still need to take it into account for\nhigh-quality imaging such as aerial photography.\n3. Tilt of Image Plane: If the image plane is tilted (see the figure below), then magnification will not be uniform and the\nfocus will not be perfect. In practice, to fix this, rather than changing the tilt of the image plane, you can instead insert a\ncompensating element to offset this tilt.\nFigure 59: Tilt of the image plane is another factor that can affect distortion of images, as well as variable magnification/scaling\nacross the image. Oftentimes, it can be corrected via a compensating element that offsets this tilt, rather than realigning the\nimage plane.\nTaking these effects into account allows for creating more sophisticated distortion models, but sometimes at the expense of\noverfitting and loss of generalizability (when we have too many degrees of freedom, and our estimates for them become so\nprecariously configured that our model is not applicable for a slight change of domain).\nThese distortion models can also be used for nonlinear optimization protocols such as Levenberg-Marquadt (LM) or Gauss-\nNewton (GN) (see lecture 20 for a review of these).\n20.2\nTsai's Calibration Method\nThis calibration method relies on using a calibration object for which we have precise knowledge of the points on our 3D points\nof the object.\nIn camera calibration, correspondences are defined between points in the image and points on the calibration ob-\nject.\nHere, we encounter the same reason that vanishing point-based methods are difficult - it is hard to directly relate the cam-\nera to calibration objects.\nWhen we take exterior orientation into account, we generally perform much better and get\nhigher-performing results. Exterior orientation (2D →3D) seeks to find the calibration object in space given a camera image.\nAdding exterior orientation also increases the complexity of this problem:\n- Interior orientation has 3 Degrees of Freedom\n- Exterior orientation has 6 Degrees of Freedom (translation and rotation)\nCombined, therefore our problem now possesses 9 DOF. We can start solving this problem with perspective projection and\ninterior orientation.\n20.2.1\nInterior Orientation\nBeginning with perspective projection, we have:\nxI -xO\nf\n= Xc\nZc\nxI -xO\nf\n= Xc\nZc\nWhere:\n\n1. (Xc, Yc, Zc)T are the camera coordinates (world coordinate units)\n2. (xI, yI) denote image position (row, column/grey level units)\n3. (xO, yO, f) denotie interior orientation/principal point (column/grey level and pixel units)\nCan we modify the measurements so we are able to take radial distortion into account?\nWe need a good initial guess, because, numerical, iterative approaches that are used to solve this problem precisely exhibit\nmultiple mimima, and having a good initial guess/estimate to serve as our \"prior\" will help us to find the correct minima.\nFrom here, we can add exterior orientation.\n20.2.2\nExterior Orientation\nAs we are solving for translation and rotation, our equation for exterior orientation equation becomes (written as a matrix-vector\nproblem):\n\nXc\nYc\nZc\n\n=\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nXs\nYs\nZs\n\n+\n\ntx\nty\ntz\n\nXc = RXs + t\nThe figure below illustrates the desired transformation between these two reference frames:\nFigure 60: Exterior orientation seeks to find the transformation between a camera and a calibration object.\nWhere the matrix R and the translation t are our unknowns. From this, we can combine these equations with our equations\nfor interior orientation to find our solutions.\n20.2.3\nCombining Interior and Exterior Orientation\nCombining our equations from interior and exterior orientation:\n- Interior Orientation:\nxI -xO\nf\n= Xc\nZc\nxI -xO\nf\n= Xc\nZc\n- Exterior Orientation:\n\nXc\nYc\nZc\n\n=\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nXs\nYs\nZs\n\n+\n\ntx\nty\ntz\n\nXc = RXs + t\nThen we can combine these equations into:\nxI -xO\nf\n= Xc\nZc\n= r11Xs + r12Ys + r13Zs + tx\nr31Xs + r32Ys + r33Zs + tz\nyI -yO\nf\n= Yc\nZc\n= r21Xs + r22Ys + r23Zs + ty\nr31Xs + r32Ys + r33Zs + tz\n\nWritten this way, we can map directly from image coordinates to calibration objects.\nHow can we find f and tz, as well as mitigate radial distortion?\nLet's start by looking at this problem in polar coordinates.\nRadial distortion only changes radial lengths (r), and not an-\ngle (θ). Changing f or Zc changes only radius/magnification. Let's use this factor to \"forget\" about the angle.\nWe can also remove additional DOF by dividing our combined equations by one another:\nxI -xO\nyI -y0\n=\nr11Xs+r12Ys+r13Zs+tx\nr31Xs+r32Ys+r33Zs+tz\nr21Xs+r22Ys+r23Zs+ty\nr31Xs+r32Ys+r33Zs+tz\n= r11Xs + r12Ys + r13Zs + tx\nr21Xs + r22Ys + r23Zs + ty\nWe can see that combining these constraints removes our DOF tz, giving us 8 constraints rather than 9. Cross-multiplying and\nterm consolidation gives:\n(XSy′\nI)r11 + (YSy′\nI)r12 + (ZSy′\nI)r13 + y′\nItx -(XSx′\nI)r21 -(YSx′\nI)r22 -(ZSx′\nI)r23 -x′\nIty = 0\nA few notes on this consolidated homogeneous equation:\n- Since our unknowns are the rij values, we have a linear combination/equation of our unknowns! The coefficients they are\nmultiplied by are either our observed image positions or the positions of the correspondences/keypoints on the calibration\nobject.\n- Note that we subtract out an estimate of the center of projection. The estimation error of this center of projection affects\nadjacent points the most, as a slight miscalculation can greatly affect the calculations above - and as a result we throw out\nall correspondences that are ε-close (for some ε > 0 to the center of the image.\n- Each correspondence between image coordinates and the points on the calibration object gives one of these equations:\n\"This point in the image corresponds to another point on the calibration object\".\nAs we have asked for the other frameworks and systems we have analyzed: How many correspondences do we need? Let's\nconsider which unknowns appear in the equations, and which do not:\n- DOF/unknowns in the equations: r11, r12, r13, r21, r22, r23, tx, ty (8 unknowns)\n- DOF/unknowns not in the equations: r31, r32, r33, tz, f (5 unknowns)\nCan this inform of how we can solve this problem? Let's consider the following points:\n- We are not yet enforcing orthonormality of the rotation matrices, i.e. RT R = I.\n- We can solve for the last row of the rotation matrix containing the unknowns r31, r32, r33 through finding the cross product\nof the first two rows of this matrix.\n- Having 8 unknowns suggests that we need at least 8 correspondences, but because this equation is homogeneous, there\nis scale factor ambiguity (i.e. doubling all the variables does not change the righthand side value of the equation).\nThe solution to account for this scale factor ambiguity is to set/fix one of these variables to a constant value. Fixing an\nunknown means we now have 7 DOF (rather than 8), and therefore, we actually need 7 correspondences. Solving\nwith 7 (or more) correspondences gives us a multiple of the solution. Solving with exactly 7 correspondences gives us 7\nequations and 7 unknowns, which we can solve with, for instance, Gauss-Jordan elimination. More correspondences are\ndesirable because they allow us to estimate both solutions and the error associated with these solutions.\n- These solutions give us values for the 8 unknowns in the equation above: r′\n11, r′\n12, r′\n13, r′\n21, r′\n22, r′\n23, tx, ty = 1 (where ty is the\nfixed parameter to account for scale ambiguity). However, as aforementioned these values are scaled versions of the true\nsolution, and we can compute this scale factor by noting that because the rows of our (soon to be) orthonormal rotation\nmatrix must have unit length - therefore the scale factor is given by computing the length of the rows of this matrix:\ns =\np\nr′\n112 + r′\n122 + r′\nOR\ns =\np\nr′\n212 + r′\n222 + r′\n\nA good sanity check: these two variables should evaluate to approximately the same value - if they do not, this is often\none indicator of correspondence mismatch.\nWith this scale factor computed we can find the true values of our solution by multiplying the scaled solutions with this scale\nfactor:\ns(r′\n11, r′\n12, r′\n13, r′\n21, r′\n22, r′\n23, tx, ty = 1) →r11, r12, r13, r21, r22, r23, tx, ty\nNote that we also have not (yet) enforced orthonormality of the first two rows in the rotation matrix, i.e. our goal is to have (or\nminimize):\nr′\n11r′\n21 + r′\n12r′\n22 + r′\n13r′\n23 = 0\nTo accomplish this, we will use the idea of \"squaring up\".\n20.2.4\n\"Squaring Up\"\nSquaring up is one method for ensuring that vectors are numerically/approximately orthogonal to one another. This technique\nanswers the question of \"Given a set of orthogonal vectors, what is the nearest set of orthogonal vectors?\"\nThe figure below highlights that given vectors a and b, we can find the nearest set of rotated vectors a′ and b′ using the\nfollowing, which states that adjustments to each vector in the pair are made in the direction of the other vector:\na′ = a + kb\nb′ = b + ka\nSince we want this dot product to be zero, we can solve for the scale k by taking the dot product of a′ and b′ and setting it\nequal to zero:\na′ · b′ = (a + kb)(b + ka) = 0\n= a · b + (a · a + b · b)k + k2 + a · b = 0\nThis equation produces a quadratic equation in k, but unfortunately, as we approach our solution point, this sets the second\nand third terms, which k depends on, to values near zero, creating numerical instability and consequently making this a poor\napproach for solving for k. A better approach is to use the approximation:\nk ≈-\na · b\n\nIt is much easier to solve for this and iterate over it a couple of times. I.e. instead of using the approach above, we use:\nx =\n2c\n-b ±\n√\nb2 -4ac\nWhere we note that the \"standard\" quadratic solution is given by:\nx = -b ±\n√\nb2 -4ac\n2a\nIt is important to consider both of these approaches because of floating-point precision accuracy - in any case, as we approach\nthe solution point, one of the two forms of these solutions will not perform well, and the other form will perform substantially\nbetter.\nWith this, we are able to iteratively solve for k until our two vectors have a dot product approximately equal to zero. Once\nthis dot product is approximately zero, we are done! We have enforced orthonormality of the first two rows of the rotation\nmatrix, which allows us to find an orthonormal third row of the rotation matrix by taking a cross product of the first two rows.\nTherefore, we then have an orthonormal rotation matrix.\n20.2.5\nPlanar Target\nPlanar target is an example calibration object that possesses desirable properties, namely, as we will see below, that it reduces\nthe number of correspondences we need from 7 to 5. Additionally, planar targets are generally easy to make, store, and allow\n\nfor high camera calibration accuracy. They are typically mounted on the side of wheels so that they can be rotated. They are\ntypically given analytically by the plane:\nZs = 0, i.e. the XY-plane where Z is equal to 0.\nFigure 61: A geometric view of a planar target, which is a calibration target whose shape is geometrically described by a plane.\nAs a result of this, we no longer need to determine the rotation matrix coefficients r13, r23, and r33 (i.e. the third column of\nthe rotation matrix, which determines how Zs affects Xc, Yc, and Zc in the rotated coordinate system. Mathematically:\n\nXc\nYc\nZc\n\n=\n\nr11\nr12\nr13\nr21\nr22\nr23\nr31\nr32\nr33\n\nXs\nYs\n\n+\n\ntx\nty\ntz\n\nAs a result of this, our equations become:\nxI -xO\nf\n= Xc\nZc\n= r11Xs + r12Ys + tx\nr31Xs + r32Ys + tz\nyI -yO\nf\n= Yc\nZc\n= r21Xs + r22Ys + ty\nr31Xs + r32Ys + tz\nAnd, as we saw before, dividing these equations by one another yields a simplified form relative to our last division for the\ngeneral case above:\nxI -xO\nyI -y0\n=\nr11Xs+r12Ys+tx\nr31Xs+r32Ys+tz\nr21Xs+r22Ys+ty\nr31Xs+r32Ys+tz\n= r11Xs + r12Ys + tx\nr21Xs + r22Ys + ty\nAs we did before for the general case, cross-multiplying and rearranging gives:\n(XSy′\nI)r11 + (YSy′\nI)r12 + y′\nItx -(XSx′\nI)r21 -(YSx′\nI)r22 -x′\nIty = 0\nNow, rather than having 8 unknowns, we have 6: r11, r12, r21, r22, tx, ty. Because this is also a homogeneous equation, we again\ncan account for scale factor ambiguity by setting one of DOF/parameters to a fixed value. This in turn reduces the DOF from\n6 to 5, which means we only need 5 correspondences to solve for now (compared to 7 in the general case). As before, if we have\nmore than 5 correspondences, this is still desirable as it will reduce estimation error and prevent overfitting.\nOne potential issue: if the parameter we fix, e.g. ty, evaluates to 0 in our solution, this can produce large and unstable value\nestimates of other parameters (since we have to scale the parameters according to the fixed value). If the fixed parameter is\nclose to 0, then we should fix a different parameter.\n20.2.6\nAspect Ratio\nThis is no longer a common issue in machine vision now, but when aspect ratios were set differently in analog with photo-diode\narrays for stepping in the x and y directions, this required setting an additional parameter to analytically describe the scale of\nx relative to y, and this parameter cannot be solved without a planar target.\n\n20.2.7\nSolving for tz and f\nNow that we have solved for our other parameters, we can also solve for tz and f:\n- These unknowns do not appear in other equations, but we can use our estimates from our other parameters to solve for tz\nand f.\n- We can again use the same equations that combine interior and exterior orientation:\n(r11Xs + r12Ys + r13Zs + tx)f -x′\nItz = (r31Xs + r32Ys + r33Zs)x′\nI\n(r21Xs + r22Ys + r23Zs + ty)f -y′\nItz = (r31Xs + r32Ys + r33Zs)y′\nI\nSince we only have two unknowns (since we have found estimates for our other unknowns at this point), this now becomes\na much simpler problem to solve. With just 1 correspondence, we get both of the 2 equations above, which is sufficient\nto solve for our 2 unknowns that remain. However, as we have seen with other cases, using more correspondences still\nremains desirable, and can be solved via least squares to increase robustness and prevent overfitting.\n- One problem with this approach: We need to have variation in depth. Recall that perspective projection has multipli-\ncation by f and division by Z; therefore, doubling f and Z results in no change, which means we can only determine tz\nand f as a ratio\nf\ntz , rather than separately. To remove this consequence of scale ambiguity, we need variation in depth,\ni.e. the calibration object, such as a plane, cannot be orthogonal to the optical axis.\nFigure 62: In order to obtain f and tz separately, and not just up to a ratio, it is crucial that the calibration object does not\nlie completely orthogonal to the optical axis - else, these parameters exhibit high noise sensitivity and we can determine them\nseparately.\nIt turns out this problem comes up in wheel alignment - if machine vision is used for wheel alignment, the calibration\nobject will be placed at an angle of 45 or 60 degrees relative to the optical axis.\n20.2.8\nWrapping it Up: Solving for Principal Point and Radial Distortion\nTo complete our joint interior/exterior orientation camera calibration problem, we need to solve for principal point and radial\ndistortion.\nAs it turns out, there is unfortunately no closed-form solution for finding the principal point.\nRather, we minimize im-\nage error using nonlinear optimization (e.g. Levenberg-Marquadt or Gauss-Newton). This error is the 2D error in the (x, y)\nimage plane that describes the error between predicted and observed points in the image plane:\nFigure 63: Image error that we minimize to solve for our principal point and radial distortion. We can formulate the total\nerror from a set of observations as the squared errors from each correspondence. Note that xI refers to the observed point,\nand xP refers to the predicted point after applying our parameter estimate transformations to a point on the calibration object\ncorresponding to an observed point.\n\nIdeally, this error is zero after we have found an optimal set of parameters, i.e. for each i:\nxIi -xPi = 0\nyIi -yPi = 0\nWhere do xPi and yPi come from? They come from applying:\n- Rotation matrix estimate (R)\n- Translation vector estimate (t)\n- Principal Point Estimate (x0, y0)\n- Radial Distortion Polynomial (k1, k2, · · · )\nUsing these equations, we can take a sum of squared error terms and solve for our parameters using nonlinear optimization\nwith Levenberg-Marquadt (LM). This can be implemented using LMdif in the MINPACK package (Fortran), as well as other\npackages in other languages. Recall that LM performs nonlinear optimization with L2 regularization.\nOne small problem with this approach: LM only works for unconstrained optimization, which is not the case when we only\nuse orthonormal rotation matrices. To overcome this, Tsai's Calibration Method used Euler Angles, and we can use either\nGibb's vector or unit quaternions. Comparing/contrasting each of these 3 rotation representations to solve this problem:\n- Euler Angles: This representation is non-redundant, but \"blows up\" if we rotate through 180 degrees.\n- Gibb's Vector: This representation is also non-redundant, but exhibits a singularity at θ = π.\n- Unit Quaternions: This representation is redundant but exhibits no singularities, and this redundancy can be mitigated\nby adding an additional equation to our system:\noq ·\noq -1 = 0\nFinally, although LM optimization finds local, and not global extrema, we have already developed an initial estimate of our\nsolution through the application of the combined interior/exterior orientation equations above, which ensures that extrema that\nLM optimization finds are indeed not just locally optimal, but globally optimal.\n20.2.9\nNoise Sensitivity/Noise Gain of Approach\nAs we have asked for other systems, it is important to consider how robust this approach is to noise and perturbations. A few\nnotes/concluding thoughts:\n- As we saw above, this approach does not produce good estimates for our parameters tz and f if the calibration object is\northogonal to the optical axis. In addition to not being able to separate the estimates for these parameters, both of these\nparameters (because they are defined by a ratio) have a very high/infinite noise gain.\n- Since these methods are largely based offof numerical, rather than analytical methods, we can best evaluate noise gain\nusing Monte Carlo methods, in which we use noise that is well-defined by its statistical properties, e.g.\nzero-mean\nGaussian noise with finite variance denoted by σ2. We can analyze the estimated noise gain of the system by looking at the\nparameter distribution in the parameter space after applying noise to our observed values and making solution predictions.\n- Higher-order coefficients of radial distortion are highly sensitive to noise/exhibit high noise gain.\nLecture 22:\nExterior Orientation, Recovering Position and Orientation,\nBundle Adjustment, Object Shape\nToday, we will conclude our discussion with photogrammetry with more discussion on exterior orientation. We will focus\non how exterior orientation can be combined with other photogrammetry techniques we've covered for recovering position\nand orientation, solving the bundle adjustment problem, and determining optimal representations for object shape for solving\nalignment and recognition problems.\n\n21.1\nExterior Orientation: Recovering Position and Orientation\nConsider the problem of a drone flying over terrain:\nFigure 64: Example problem of recovering position and orientation: a drone flying over terrain that observes points for which it\nhas correspondences of in the image plane. The ground points are given by p1, p2, and p3, and the point of interest is p0. This\nphotogrammetry problem is sometimes referred to as Church's tripod.\nProblem Setup:\n- Assume that the ground points p1, p2, and p3 are known, and we want to solve for p0.\n- We also want to find the attitude of the drone in the world: therefore, we solve for rotation + translation (6 DOF).\n- We have a mix of 2D-3D correspondences: 2D points in the image plane that correspond to points in the image, and 3D\npoints in the world.\n- Assume that the interior orientation of the camera is known (x0, y0, f).\n- Connect image to the center of projection.\nHow many correspondences do we Need? As we have seen with other frameworks, this is a highly pertinent question that\nis necessary to consider for photogrammetric systems.\nSince we have 6 DOF with solving for translation + rotation, we need to have at least 3 correspondences.\n21.1.1\nCalculating Angles and Lengths\nNext, we need to figure out how to solve for the angles between ground points, and the lengths to these ground points from the\nplane:\nFigure 65: As a next step of solving this exterior orientation problem, we calculate the angles between the ground points relative\nto the plane, as well as the lengths from the plane to the ground points. The angles of interest are given by: θ12, θ13, and θ2,3,\nand the lengths of interest are r1, r2, and r3.\n- Given the problem setup above, if we have rays from the points on the ground to the points in the plane, we can calculate\nangles between the ground points using dot products (cosines), cross products (sines), and arc tangents (to take ratios of\nsines and cosines).\n\n- We also need to know the lengths of the tripod - i.e. we need to find r1, r2, and r3.\n- From here, we can find the 3D point p0 by finding the intersection of the 3 spheres corresponding to the ground points\n(center of spheres) and the lengths from the ground points to the plane (radii):\nFigure 66: We can find the location of the plane p0 by finding the intersection of 3 spheres using the point information pi and\nthe length information ri.\nNote that with this intersection of spheres approach, there is ambiguity with just three points/correspondences (this gives 2\nsolutions, since the intersection of 3 spheres gives 2 solutions). Adding more points/correspondences to the system reduces\nthis ambiguity and leaves us with 1 solution.\nAnother solution issue that can actually help us to reduce ambiguity with these approaches is that mirror images have a\nmirror cyclical ordering of the points that is problematic. This allows us to find and remove \"problematic solutions\".\nWhat if I only care about attitude?\nThat is, can I solve for only rotation parameters?\nUnfortunately not, since the\nvariables/parameters we solve for are coupled to one another.\nSome laws of trigonometry are also helpful here, namely the law of sines and cosines:\nFigure 67: For any triangle, we can use the law of sines and law of cosines to perform useful algebraic and geometric manipulations\non trigonometric expressions.\nLaw of Sines:\na\nsin A =\nb\nsin B =\nc\nsin C\nLaw of Cosines: a2 = b2 + c2 -2bc cos A\nWe do not use these approaches here, but it turns out we can solve for the lengths between the different ground points (r12, r23,\nand r13) using these trigonometric laws.\n21.1.2\nFinding Attitude\nFrom here, we need to find attitude (rotation of the plane relative to the ground). We can do this by constructing three vectors\nwith the translation subtracted out (as we have seen with previous approaches):\na1 ⇐⇒b1 = p1 -p0\n(54)\na2 ⇐⇒b2 = p2 -p0\n(55)\na3 ⇐⇒b3 = p3 -p0\n(56)\n\nNote that the vectors a1, a2, and a3 are our correspondences in the image plane. This means that we can now relate these two\ncoordinate systems. We can relate these via a rotation transformation:\nR(ˆai) = ˆbi ∀i ∈{1, 2, 3}\nWe can first represent R as an orthonormal rotation matrix. It turns out we can actually solve this by considering all three\ncorrespondences at once:\nR(ˆa1, ˆa2, ˆa3) = (ˆb1, ˆb2, ˆb3)\nWe can solve for R using matrix inversion, as below. Question to reflect on: Will the matrix inverse result be orthonormal?\nA\n∆= (ˆa1, ˆa2, ˆa3)\nB\n∆= (ˆb1, ˆb2, ˆb3)\nRA = B =⇒R = BA-1\nIn practice, as we saw with other photogrammetry frameworks, we would like to have more than 3 correspondences in tandem\nwith least squares approach. Can use estimates with first 3 correspondence to obtain initial guess, and then iteration to solve\nfor refined estimate. May reduce probability of converging to local minima.\nNote that errors are in image, not in 3D, and this means we need to optimize in the image plane, and not in 3D.\n21.2\nBundle Adjustment\nBundle Adjustment (sometimes abbreviated as BA) is a related photogrammetry problem in which, in the most general case,\nconsists of K landmarks (points in the image observed by cameras) and N cameras.\nGoal of BA: Determine the 3D locations of landmark objects and cameras in the scene relative to some global coordinate\nsystem, as well as determine the orientation of cameras in a global frame of reference.\nFigure 68: Bundle Adjustment (BA). In the general case, we can have any number of K landmark points (\"interesting\" points\nin the image) and N cameras that observe the landmarks.\nWhat are our unknowns for Bundle Adjustment\n- Landmarks ({pi}): A set of points that are observed by multiple views, of which we want to find the coordinates of in a\nglobal coordinate system.\n- Cameras ({ci}): This is the set of camera locations in space with respect to some global coordinate system.\n- Camera Attitudes ({Ri}): These are the orientations of the cameras in space with respect to some global coordinate\nsystem.\n- Intrinsic Orientations ({x0i, y0i, fi, Ki}: These refer to the camera intrinsics for each camera in our problem.\n\nThis approach is typically solved using Levenberg-Marquadt (LM) nonlinear optimization. Although there are many pa-\nrameters to solve for, we can make an initial guess (as we did with our previous camera calibration approaches to avoid local\nminima) to ensure that we converge to global, rather than local minima.\nHow do we find \"interesting points\" in the image? One way to do this is to use descriptors (high-dimensional vectors cor-\nresponding to image gradients), as is done with SIFT (Scale-Invariant Feature Transforms). Some other, more recent approaches\ninclude:\n- SURF (Speeded Up Robust Features)\n- ORB (Oriented FAST and rotated BRIEF)\n- BRIEF (Binary Robust Independent Elementary Features)\n- VLAD (Vector of Locally Aggregated Descriptors)\n21.3\nRecognition in 3D: Extended Gaussian 3D\nRecall our previous discussions with recognition in 2D - let's now aim to extend this to 3D!\nGoal of 3D Recognition: Describe a 3D object.\nLet's consider several representations for effective alignment estimation and recognition:\n- Polyhedra: When this is our object representation, we often consider this to be a class of \"block world problems\". We can\ndescribe these polyhedra objects semantically with edges, faces, and linked data structures. Because this is not a realistic\nrepresentation of the world, for systems in practice, we look for more complicated representations.\n- Graphics/curves: These can be done with a mesh. Here, we consider any curved surface. Meshes can be thought of as\npolyhedral objects with many facets (polygon faces). This representation is well-suited for outputting pictures.\nFigure 69: Example of a mesh. Note that this can be constructed as many facets, where each facet is a polygon.\nWhat kind of tasks are we interested in for meshes, and more generally, the rest of our object representations?\n- Find alignment/pose (position/orientation): For alignment, we can accomplish this task by assigning correspon-\ndences between vertices, but this is not a very effective approach because there is no meaning behind the vertices (i.e.\nthey are not necessarily deemed \"interesting\" points), and the vertex assignments can change each time the shape is\ngenerated. Can do approximate alignment by reducing the distance to the nearest vertex iteratively and solving.\n- Object recognition: We cannot simply compare numbers/types of facets to models in a library, because the generated\nmesh can change each time the mesh is generated. It turns out that this is a poor representation for recognition.\nSince neither of these representations lend themselves well for these two tasks, let us consider alternative representations. First,\nlet us consider what characteristics we look for in a representation.\n21.3.1\nWhat Kind of Representation Are We Looking For?\nWe are looking for an object representation that exhibits the following properties:\n- Translation Invariance: Moving an object in a coordinate system does not change (at least not substantially) the object's\nrepresentation.\n- Rotation Consistency: Rotating the object changes the representation of the object in an understandable and efficient\nway that can be applied for alignment problems.\n\nRepresentations to consider given these properties:\n- Generalized Cylinders:\n- These can be thought of as extruding a circle along a line.\n- In the generalized case, we can extrude an arbitrary cross-section along a line, as well as allow the line to be a general\ncurve and to allow the cross-section (generator) to vary in size.\nFigure 70: Representations of increasing generality for generalized cylinders.\nExample: Representing a sphere as a generalized cylinder:\nFigure 71: Representing a sphere as a generalized cylinder - unfortunately, there are an infinite number of axes we can use to\n\"generate\" the cylinder along.\n- This same problem shows up elsewhere, especially when we allow for inaccuracies in data.\n- This approach has not been overly successful in solving problems such as alignment, in practice.\n- This leads us to pursue an alternative representation.\n- Polyhedra Representation:\n- Let's briefly revisit this approach to get a good \"starting point\" for our object representation that we can solve\nalignment and recognition problems.\n- One way, as we have seen before, was to take edges and faces, and to build a graph showing connectedness of polyhedra.\n- Instead, we take the unit normal vector of faces and multiply them by the area of the face/polygon.\n\nFigure 72: Polyhedra approach in which we represent faces by the unit normal vector multiplied by the area of the face of the\npolyhedra representation.\n- Despite throwing away information about the connectedness of faces, Minkowski's proof shows that this generates a\nunique representation for convex polyhedra.\n- Note that this proof is non-constructive, i.e. there was no algorithm given with the proof that actually solves the\nconstruction of this representation. There is a construction algorithm that solves this, but it is quite slow, and, as it\nturns out, are not needed for recognition tasks.\n- Further, note that the sum of vectors form a closed loop:\nN\nX\ni=1\nni = 0\nTherefore, any object that does not satisfy this is non-convex polyhedra.\nLet's consider what this looks like for the following cylindrical/conic section. Consider the surface normals on the cylindrical\ncomponent (Aiˆni) and the surface normals on the conic component (Biˆni). Note that we still need to be careful with this\nrepresentation, because each mesh generated from this shape can be different.\nFigure 73: A shape represented by a mesh that we will convert to an extended Gaussian sphere, as we will see below. Note that\nthe surface normals Aiˆni and Biˆni each map to different great circles on the sphere, as can be seen in the figure below.\nIdea: Combine facets that have similar surface normals and represent them on the same great circle when we convert to a\nsphere.\nWhat does this look like when we map these scaled surface normals to a unit sphere?\nIdea: Place masses in great circle corresponding to the plane spanned by the unit normals of the cylindrical and conic\nsections above. Note that the mass of the back plate area of our object occupies a single point on the mapped sphere, since\nevery surface normal in this planar shape is the same.\n\nFigure 74: Extended Gaussian sphere mapping, which maps the surface normals of arbitrary (convex) surfaces to the sphere\nsuch that the surface vector corresponds to a scaled unit normal in both the original surface and the sphere.\nTherefore, this sphere becoems our representation! This works much better for alignment and recognition. Note that we\nwork in a sphere space, and not a high-dimensional planar/Euclidean space. Therefore:\n- Comparisons (e.g. for our recognition task) can be made by comparing masses using distance/similarity functions.\n- Alignment can be adjusted by rotating these spheres.\nHow well does this representation adhere to our desired properties above?\n- Translation Invariance: Since surface normals and areas do not depend on location, translation invariance is preserved.\n- Rotation Equivariance: Since rotating the object corresponds to just rotating the unit sphere normals without changing\nthe areas of the facets, rotation of the object simply corresponds to an equal rotation of the sphere.\nLoosely, this representation corresponds to density. This density is dependent on the degree of curvature of the surface we are\nconverting to a sphere representation:\n- Low density ↔High curvature\n- High density ↔Low curvature\nTo better understand this representation in 3D, let us first understand it in 2D.\n21.3.2\n2D Extended Circular Image\nThe idea of the extended Gaussian Image: what do points on an object and points on a sphere have in common? They have the\nsame surface normals.\nFigure 75: Representation of our extended Gaussian image in 2D. The mapping between the surface and the sphere is determined\nby the points with the same surface normals.\nGauss: This extended Gaussian image representation is a way to map arbitrary convex objects to a sphere by mapping\npoints with the same surface normals.\nWe can make this (invertible) mapping in a point-point fashion, and subsequently generalize this to mapping entire convex\nshapes to a sphere. There are issues with non-convex objects and invertibility because the forward mapping becomes a surjec-\ntive mapping, since multiple ponts can have the same surface normal, and therefore are mapped to the same location on the\ncircle.\n\nFigure 76: When surfaces are non-convex, we run into issues with inverting our mapping, because it may be surjective, i.e. some\npoints on the surface may map to the same point on the sphere due to having the same surface normal.\nIdea: Recall that our idea is to map from some convex 2D shape to a circle, as in the figure below.\nFigure 77: Our forward mapping from our surface to a circle.\nNote that in this case, density depends on which region (and consequently the degree of curvature) of the object above. We\nare interested in this as a function of angle η. Analytically, our curvature and density quantities are given by:\n- Curvature: κ = dη\ndS =\nRcurvature (The \"turning rate\")\n- Density: G = 1\nκ = dS\ndη (inverse of curvature)\nThese are the only quantities we need for our representation mapping! We just map the density G = dS\ndη from the object to the\nunit sphere. This is invertible in 2D for convex objects, though this invertibility is not necessarily the case in 3D.\n21.3.3\nAnalyzing Gaussian Curvature in 2D Plane\nLet us analyze this mapping in the 2D plane, using the figure below as reference:\nFigure 78: Analyzing Gaussian curvature in the 2D plane.\nMathematically, then, we can say:\nδx = -sin(η)\nδy = cos(η)\n\nThen we can integrate to find x and y:\nx = x0 +\nZ\n-sin(η)dS\nx = x0 +\nZ\n-sin(η)dS\ndη dη\nx = x0 +\nZ\n-sin(η)G(η)dη\nAnd similarly for y :\ny = y0 +\nZ\ncos(η)G(η)dη\nWhat happens if we integrate over the entire unit circle in 2D? Then we expect these integrals to evaluate to 0:\nZ 2π\n-sin(η)G(η)dη = 0\nZ 2π\ncos(η)G(η)dη = 0\nThese conditions require that the centroid of density G(η) is at the origin, i.e. that the weighted sum of G(η) = 0.\n21.3.4\nExample: Circle of Radius R\nIn the case of a circle with constant radius, it turns out our curvature and density will be constant:\nK = dη\ndS = 1\nR\nG(η) = dS\ndη = R\nNote that while this holds for spheres, we can actually generalize it beyond spheres too (making it of more practical value). This\nis because we can fit curves of arbitrary shape by fitting a circle locally and finding the radius of the best-fitting circle.\nNote that because density is equal to R, this density increases with increasing radius. Because density is the same everywhere,\nwe cannot recover orientation as there is not uniqueness of each point. Let's try using an ellipse in 2D.\n21.3.5\nExample: Ellipse in 2D\nRecall that there are several ways to parameterize an ellipse:\n- Implicitly:\nx\na\n2 +\ny\nb\n2 = 1\n- \"Squashed circle\": x = a cos θ, y = b sin θ (note that θ is the variable we parameterize these parametric equations over.\nBecause this takes a parametric form, this form is better for generating ellipses than the first implicit form because we can\nsimply step through our parameter θ.\nNext, let's compute the normal vector to the curve to derive a surface normal. We can start by computing the tangent vector.\nFirst, note that the vector r describing this parametric trajectory is given by:\nr = (a cos θ, b sin θ)T\nThen the tangent vector is simply the derivative of this curve with respect to our parameter θ:\nt = dr\ndθ = (-a sin θ, b cos θ)T\nFinally, in 2D we can compute the normal vector to the surface by switching x and y of the tangent vector and inverting the\nsign of y (which is now in x's place):\nn = (b cos θ, a sin θ)T\n\nWe want this vector to correspond to our normal in the sphere:\nn = (b cos θ, a sin θ)T ↔ˆn = (cos η, sin η)T\nThen we have:\nb cos θ = n cos η\na sin θ = n sin η\nn2 = b2 cos2 θ + a2 sin2 θ\ns = (a cos η, b sin η)T\nK =\ns · s\nab\nn2((a cos η)2 + (b sin η)2) = (ab)2\nThese derivations allow us to find the extrema of curvature: ellipses will have 4 of them, spaced π\n2 apart from one another\n(alternating minimums and maximums): η = 0, η = π\n2 , η = π, η = 3π\n2 .\nFigure 79: Curvature extrema of an ellipse: An ellipse has 4 extrema, spaced π\n2 apart from one another. Extrema depend on\nthe major and minor axes of the ellipse.\nIf a is the major axis and b is the minor axis, then extrema of curvature are given by:\n- Maximum: κ = a\nb2\n- Minimum: κ =\nb\na2\nWithout loss of generality, we can flip a and b if the major and minor axes switch.\nFor alignment, we can rotate these ellipses until their orientations match. If the alignment is not constructed well, then the\nobject is not actually an ellipse.\nGeodetic Identity: Related, this identity relates geocentric angles to angles used for computing latitudes:\nb cos θ = n cos η\na sin θ = n sin η\na(tan θ) = b(tan η)\nWhat are some other applications of this in 2D? We can do (i) circular filtering and circular convolution!\n21.3.6\n3D Extended Gaussian Images\nFortunately, many of the results in 2D Gaussian images generalize well to 3D. As with previous cases, we can begin with the\nGauss mapping (starting with points, but generalizing to different convex shapes in the surface). From here, we can compute\ncurvature and density:\n- Curvature: κ = δS\nδO = limδ→0 δS\nδO = dS\ndO\n- Density: G(η) = δO\nδS = limδ→0 δO\nδS = dO\ndS\n\nAlthough curvature in 3D is more complicated, we can just use our notion of Gaussian curvature, which is a single scalar.\nAs long as our object is convex, going around the shape in one direction in the object corresponds to going around the sphere\nin the same direction (e.g. counterclockwise).\nExample of non-convex object: Saddle Point: As we mentioned before, we can run into mapping issues when we have\nnon-convex objects that we wish to map, such as the figure below:\nFigure 80: An example of a non-convex object: a saddle point. Notice that some of the surface normals are the same, and\ntherefore would be mapped to the same point on the sphere.\nNote further that traveling/stepping an object in the reverse order for a non-convex object inverts the curvature κ, and\ntherefore allows for a negative curvature κ < 0.\nExample of 3D Gaussian Curvature: Sphere of Radius R: This results in the following (constant) curvature and density,\nas we saw in the 2D case:\n- Curvature: κ =\nR2\n- Density: G(η) = R2\nExtensions: Integral Curvature: Next time, we will discuss how Gaussian curvature allows for us to find the integral of\nGaussian curvature for recognition tasks.\nLecture 23: Gaussian Image and Extended Gaussian Image, Solids of Rev-\nolution, Direction Histograms, Regular Polyhedra\nIn this lecture, we cover more on the Gaussian images and extended Gaussian images (EGI), as well as integral Gaussian images,\nsome properties of EGI, and applications of these frameworks.\n22.1\nGaussian Curvature and Gaussian Images in 3D\nLet's start with defining what a Gaussian image is:\nGaussian Image: A correspondence between points on an image and corresponding points on the unit sphere based offof\nequality of the surface normals.\nFigure 81: Mapping from object space the Gaussian sphere using correspondences between surface normal vectors. Recall that\nthis method can be used for tasks such as image recognition and image alignment.\nRecall that we defined the Gaussian curvature of an (infinitesimal) patch on an object as a scalar that corresponds to the\nratio of the areas of the object and the Gaussian sphere we map the object to, as we take the limit of the patch of area on the\n\nobject to be zero:\nκGaussian = lim\nδ→0\nδS\nδO\n= The ratio of areas of the image and the object\nRather than use Gaussian curvature, however, we will elect to use the inverse of this quantity (which is given by density) 1\nκ,\nwhich roughly corresponds to the amount of surface that contains that surface normal.\nWe can also consider the integral of Gaussian curvature, and the role this can play for dealing with object discontinuities:\n22.1.1\nGaussian Integral Curvature\nThe integral of Gaussian curvature is given by:\nZZ\nO\nKdO =\nZZ\nO\ndS\ndOdO =\nZZ\nO\ndS = S (The area of a corresponding patch on the sphere)\nNote: One nice feature of integral curvature: This can be applied when curvature itself cannot be applied, e.g. at disconti-\nnuities such as edges and corners.\nWe can also apply the integral over the Gaussian sphere:\nZZ\nS\nK dS =\nZZ\nS\ndO\ndS dS =\nZZ\nS\ndO = O (The area of a corresponding path on the object)\nThis second derivation is used more often in practice.\n22.1.2\nHow Do We Use Integral Gaussian Curvature?\nAs we briefly mentioned above, integral Gaussian curvature can be used for \"smoothing out\" discontinuities and providing\ncontinuity when there is none guaranteed by the curvature function itself. Discontinuities can be caused by features such as\ncorners, edges, and other abrupt changes in surface orientation.\nFigure 82: An application in which integral Gaussian curvature can be used in lieu of the curvature function. The smoothing\nproperties of integration enable for the mitigation of discontinuity affects.\nNext, let's consider the different forms our mass density distribution G can take.\n\n22.1.3\nCan We Have Any Distribution of G on the Sphere?\nIt is important to understand if we need to impose any restrictions on our mass density distribution. Let's analyze this in both\nthe discrete and continuous cases. The results from the discrete case extend to the continuous case.\n- Discrete Case: Consider an object that we have discretized to a set of polygon facets, and consider that we have a camera\nthat observes along an optical axis ˆv, and a mirror image of this camera that observes along the optical axis -ˆv. Note\nthat any surface normals that are not parallel to these optical axes will have foreshortening effects imposed.\nFigure 83: Problem setup in the discrete case. Consider our object parameterized by a discrete number of finite polygon facets,\nand a camera and its mirror image observing the different surface normals of these facets from two opposite vantage points.\nFirst, consider only taking the facets with positive dot product (i.e. 90 degrees or less) away from the vector ˆv, and sum\nthe product of these facet area multiplied by normal and the unit vector corresponding to the optical axis ˆv:\nX\nˆsi·ˆv≥0\nAiˆsi · ˆv\nNow, consider the other facets, that is, those for which we take the facets with positive dot product (i.e. 90 degrees or\nless) away from the vector -ˆv, which is 180 degrees rotated from ˆv, and applying the same operation:\nX\nˆsi·(-ˆv)≥0\nAiˆsi · (-ˆv)\nWe must have equality between these two sums:\nX\nˆsi·ˆv≥0\nAiˆsi · ˆv =\nX\nˆsi·(-ˆv)≥0\nAiˆsi · (-ˆv)\nAnd therefore, if we combine the sums to the same side:\nX\nˆsi·ˆv≥0\nAiˆsi · ˆv -(\nX\nˆsi·(-ˆv)≥0\nAiˆsi · (-ˆv)) = 0\nX\nˆsi·ˆv≥0\nAiˆsi · ˆv -(-\nX\nˆsi·(-ˆv)≥0\nAiˆsi · ˆv) = 0\nX\nˆsi·ˆv≥0\nAiˆsi · ˆv +\nX\nˆsi·(-ˆv)≥0\nAiˆsi · ˆv = 0\nX\n{ˆsi·ˆv≥0}∪{ˆsi·(-ˆv)≥0}\nAiˆsi · ˆv = 0\nX\ni\nAiˆsi · ˆv = 0\n(\nX\ni\nAiˆsi) · ˆv = 0 ∀ˆv\nTherefore, from this it must be the case that P\ni Aiˆsi = 0, i.e. that the center of mass of the mapping must be at the\ncenter of the sphere. This also means that the Gaussian density G must have a \"zero-mean\" mass distribution for any\nclosed object. This does not generally hold for any non-closed objects.\n- Continuous Case: This same result holds in the continuous case - the mean must be at the center of the sphere for closed\nobjects. Mathematically:\nZZ\nS\nG(ˆs)ˆsdS = 0\n\nTherefore, for both the discrete and continuous cases, we can think of the Extended Gaussian Image (EGI) as a mass distribution\nover the sphere that objects are mapped to according to their surface normals.\nNext, let's look at some discrete applications. Continuous EGI can be useful as well, especially if we are using an analyti-\ncal model (in which case we can compute the exact EGI).\n22.2\nExamples of EGI in 3D\nLet's start with some examples of EGI in 3D.\n22.2.1\nSphere: EGI\nWe have seen this before for the circle, and can simply generalize this from 2D to 3D:\n- Curvature: κ = R2\n- Density: G = 1\nK =\nR2\nWe saw that because the curvature and density are the same everywhere, that this imposes limitations on the effectiveness of\nthis surface for recognition and alignment tasks. Let's next consider something with more complicated structure.\n22.2.2\nEllipsoid: EGI\nRecall our two main ways to parameterize ellipses/ellipsoids: implicit (the more common form) and \"squashed circle/sphere\"\n(in this case, the more useful form:\n- Implicit parameterization:\nx\na\n2 +\ny\nb\n2 +\nz\nc\n2 = 1\n- \"Sphere Squashing\" parameterization: These are parameterized by θ and φ:\nx = a cos θ cos φ\ny = b sin θ cos φ\nz = c sin φ\nWe will elect to use the second of these representations because it allows for easier and more efficient generation of points by\nparameterizing θ and φ and stepping these variables.\nAs we did last time for the 2D ellipse, we can express these parametric coordinates as a vector parameterized by θ and φ:\nr = (a cos θ cos φ, b sin θ cos φ, c sin φ)T\nAnd similarly to what we did before, to compute the surface normal and the curvature, we can first find (in this case, two)\ntangent vectors, which we can find by taking the derivative of r with respect to θ and φ:\nrθ\n∆= dr\ndθ = (-a sin θ cos φ, b cos θ cos φ, 0)T\nrφ\n∆= dr\ndφ = (-a cos θ sin φ, -b sin θ sin φ, c cos φ)T\nThen we can compute the normal vector by taking the cross product of these two tangent vectors of the ellipsoid. We do not\ncarry out this computation, but this cross product gives:\nn = rθ × rφ = cos φ(b cos θ cos φ, ac sin θ cos φ, ab sin φ)T\n= (b cos θ cos φ, ac sin θ cos φ, ab sin φ)T\nWhere we have dropped the cos φ factor because we will need to normalize this vector anyway. Note that this vector takes a\nsimilar structure to that of our original parametric vector r.\nWith our surface normals computed, we are now equipped to match surface normals on the object to the surface normals\non the unit sphere. To obtain curvature, our other quantity of interest, we need to differentiate again. We will also change our\n\nparameterization from (θ, φ) →(ξ, η), since (θ, φ) parameterize the object in its own space, and we want to parameterize the\nobject in unit sphere space (ξ, η).\nˆn = (cos ξ cos η, sin ξ cos η, sin η)T\ns = (a cos ξ cos η, b sin ξ cos η, sin η)T\nκ =\ns · s\nabc\nG = 1\nK =\nabc\ns · s\nWith the surface normals and curvature now computed, we can use the distribution G on the sphere for our desired tasks of\nrecognition and alignment!\nRelated, let us look at the extrema of curvature for our ellipsoid. These are given as points along the axes of the ellipsoids:\n1.\n\nbc\na\n2.\nac\nb\n3.\n\nab\nc\nFor these extrema, there will always be one maxima, one minima, and one saddle point. The mirror images of these extrema\n(which account for the mirrors of each of these three extrema above) exhibit identical behavior and reside, geometrically, on the\nother side of the sphere we map points to.\nHow Well Does this Ellipsoid Satisfy Our Desired Representation Properties?\n- Translation invariance: As we saw with EGI last time, this representation is robust to translation invariance.\n- Rotation \"Equivariance\": This property works as desired as well - rotations of the ellipsoid correspond to equivariant\nrotations of the EGI of this ellipse.\n22.3\nEGI with Solids of Revolution\nWe saw above that the EGI for an ellipsoid is more mathematically-involved, and is, on the other end, \"too simple\" in the case\nof a sphere. Are there geometric shapes that lend themselves well for an \"intermediate representation\"? It turns out there are,\nand these are the solids of revolution. These include:\n- Cylinders\n- Spheres\n- Cones\n- Hyperboloids of one and two sheets\nHow do we compute the EGI of solids of revolution? We can use generators that produce these objects to help.\nFigure 84: EGI representation of a generalized solid of revolution. Note that bands in the object domain correspond to bands\nin the sphere domain.\n\nAs we can see from the figure, the bands \"map\" into each other! These solids of revolution are symmetric in both the object\nand transform space. Let's look at constructing infinitesimal areas so we can then compute Gaussian curvature κ and density G:\n- Area of object band: δO = 2πrδs\n- Area of sphere band: δS = 2π cos(η)δn\nThen we can compute the curvature as:\nκ = δS\nδO = 2π cos(η)δη\n2πrδs\n= cos(η)δη\nrδs\nG = 1\nκ = δO\nδS = r sec(η) δs\nδη\nThen in the limit of δ →0, our curvature and density become:\nκ = lim\nδ→O\nδS\nδO = cos η\nr\ndη\nds (Where dη\nds is the rate of change of surface normal direction along the arc, i.e. curvature)\nG = lim\nδ→0\nδO\nδS = r sec η ds\ndη (Where ds\ndη is the rate of change of the arc length w.r.t. angle)\nκ = cos η\nr\nκ\n22.4\nGaussian Curvature\nUsing the figure below, we can read offthe trigonometric terms from this diagram to compute the curvature κ in a different way:\nFigure 85: Our infinitesimal trigonometric setup that we can use to calculate Gaussian curvature.\nWe can read offthe trigonometric terms as:\nsin η = -δr\nδs = -rs (First derivative)\ncos η dη\nds = d\nds(-rs) = -rss (Second derivative)\nThen the curvature for the solid of revolution is given by:\nκ = -rss\nr\nLet's look at some examples to see how this framework applies:\n22.4.1\nExample: Sphere\nTake the following cross-section of a sphere in 2D:\n\nFigure 86: Cross-section of a 3D sphere in 2D. We can apply our framework above to derive our same result for Gaussian\ncurvature.\nMathematically:\nr = R cos\nS\nR\n\nrss = R\n\n-1\nR2\n\ncos\nS\nR\n\n= -1\nR cos\nS\nR\n\nThen curvature is given by:\nκ = -rss\nr\n=\n-\n-1\nR\n\ncos\n\nS\nR\n\nR cos\n\nS\nR\n\n= 1\nR2\nAlternatively, rather than expressing this in terms of R and S, we can also express as a function of z (with respect to the diagram\nabove). Let's look at some of the quantities we need for this approach:\ntan η = -dr\ndz = -rz (First derivative)\nsec2 dη\nds = d\nds(-rz) = -rzz\ndz\nds (By Chain Rule)\nsec2 η = 1 + tan2 η\ncos η = dz\ndη = -zs\nPutting all of these equations together:\nκ =\n-rzz\nr(1 + z2)2\nr =\np\nR2 -Z2\nrz = -\nZ\n√\nR2 -Z2\nrzz = -\nR2\n(R2 -Z2)\n1 + r2\nz =\nR2\nR2 -Z2\nThen: κ =\n-rzz\nr(1 + z2)2 = 1\nR2\nOne particular EGI, and the applications associated with it, is of strong interest - the torus.\n22.4.2\nEGI Example Torus\nGeometrically, the torus can be visualized as a cross-section in the diagram below, with small radius ρ and large radius R.\n\nFigure 87: Geometric cross-section of the torus, along with its associated parameters and their significance that describe the\ntorus.\nWhat problems might we have with this object? It is non-convex. This Gaussian image may not be invertible when\nobjects are non-convex. We lose uniqueness of mapping, as well as some other EGI properties, by using this non-convex object.\nFor the torus, the mapping from the object to the sphere is not invertible - each point in the EGI maps to two points\non the object, which means the surface normal of the object is not unique.\nThe torus is convex into the board. Convex shapes/objects have non-negative curvature, and above we have a saddle\npoint with negative curvature.\nKeeping these potential issues in mind, let's press on. Mathematically:\nr = R + ρ cos η\n= R + ρ cos\nS\nρ\n\nFrom here, we can take the second derivative rss:\nrss = d2r\nds2 = -1\nρ cos\ns\nρ\n\nCombining/substituting to solve for curvature κ:\nκ = -rss\nr\n= 1\nρ\ncos\n\nS\nρ\n\nR + ρ cos\n\nS\nρ\n\nWe will also repeat this calculation for the corresponding surface normal on the other side. We will see that this results in\ndifferent sign and magnitude:\nr = R -ρ cos η\n= R -ρ cos\nS\nρ\n\nFrom here, we can take the second derivative rss:\nrss = d2r\nds2 = 1\nρ cos\ns\nρ\n\nCombining/substituting to solve for curvature κ:\nκ = -rss\nr\n= -1\nρ\ncos\n\nS\nρ\n\nR + ρ cos\n\nS\nρ\n\nSince we have two different Gaussian curvatures, and therefore two separate densities for the same surface normal, what do we\ndo? Let's discuss two potential approaches below.\n\n1. Approach 1: Adding Densities: For any non-convex object in which we have multiple points with the same surface\nnormals, we can simply add these points together in density:\nG = 1\nκ+\n+ 1\nκ-\n= 2ρ2\nEven though this approach is able to cancel many terms, it creates a constant density, which means we will not be able to\nsolve alignment/orientation problems. Let's try a different approach.\n2. Approach 2: Subtracting Densities: In this case, let us try better taking into account local curvature by subtracting\ninstead of adding densities, which will produce a non-constant combined density:\nG = 1\nκ+\n-1\nκ-\n= 2Rρ sec(η)\nThis result is non-constant (better for solving alignment/orientation problems), but we should note that because of the\nsecant term, it has a singularity at the pole. We can conceptualize this singularity intuitively by embedding a sphere within\nan infinite cylinder, and noting that as we approach the singularity, the mapped location climbs higher and higher on the\ncylinder.\nFigure 88: Embedding our sphere within a cylinder geometrically illustrates the singularity we observe for sec η as η →π\n2 .\nNote: Our alignment and recognition tasks are done in sphere space, and because of this, we do not need to reconstruct the\noriginal objects after applying the EGI mapping.\n22.4.3\nAnalyzing the Density Distribution of the Sphere (For Torus)\nWhereas we had a direct mapping from \"bands\" to \"bands\" in the previous case, in this case our \"bands\" map to \"crescents\"\non the sphere when computing the EGI of the torus.\nFigure 89: Computing the EGI of a torus, and demonstrating how we do not have the same \"band to band\" correspondence/map-\nping that we had in the previous case, suggesting that this mapping may not be invertible.\nNote: On a torus, the band is slightly narrower on the inner side. Can balance out this asymmetry by using the band on\nthe opposite side.\n\nWe can also consider the area of a torus:\nAtorus = (2πρ)(2πR)\n= 4π2Rρ\n= \"A circle of circles\"\nAs a result of how this area is structured, two \"donuts\" (tori) of different shape/structure but with the same area correspond/map\nto the same EGI, since EGI captures a ratio of areas. This loss of uniqueness is due to the fact that this torus representation\nstill remains non-convex - in a sense this is the \"price we pay\" for using a non-convex representation.\nSome other issues to be mindful of with a torus:\n- It is hard to extract all surface normals since it is impossible to image all surfaces of a torus with just a single camera/camera\npose. This phenomenon is also due to the non-convexity of the torus.\n22.5\nHow Can We Compute EGI Numerically?\nHow can we compute EGI numerically through applications/frameworks such as photometric stereo data or with discretized\nmeshes? We can look at the facets/pixels of objects.\nLet us first suppose we have a triangular facet:\nFigure 90: A triangular facet, which we can imagine we use to compute our EGI numerically.\nAs we have done before, we can find the surface normal and area of this facet to proceed with our EGI computation:\n- Surface Normal: This can be computed simply by taking the cross product between any of the two edges of the triangular\nfacet, e.g.\nn = (b -a) × (c -b)\n= (a × b) + (b × c) + (c × a)\n- Area: This can be computed by recalling the area of a triangular ( 1\n2base × height):\nA = 1\n2(b -a) · (c -a)\n= 1\n6((a · a + b · b + c · c) -(a · b + b · c + c · a))\nWe can repeat this area and surface normal computation on all of our facets/elements to compute the mass distribution over the\nsphere by adding different facets from all over surface. We add rather than subtract components to ensure we have a non-constant\nEGI, and therefore have an EGI representation that is suitable for alignment tasks.\n22.5.1\nDirection Histograms\nDirection histograms are another way of thinking about how we build an EGI. These are used in different applications outside\nof machine vision as well, e.g. finding direction histograms in (i) muscle fibers, (ii) neural images, (iii) analyzing blood vessels.\nTypically, these direction histograms can be used to ascertain what the most common orientations are amongst a set of\norientations.\n\nFor histograms in higher dimensions, is subdividing the region into squares/cubes the most effective approach? As it turns\nout, no, and this is because the regions are not round. We would prefer to fill in the plane with disks.\nTo contrast, suppose the tessellation is with triangles - you are now combining things that are pretty far away, compared\nto a square. Hexagons alleviate this problem by minimizing distance from the center of each cell to the vertives, while also\npreventing overlapping/not filled in regions. Other notes:\n- We also need to take \"randomness\" into account, i.e. when points lie very close to the boundaries between different bins.\nWe can account for this phenomena by constructing and counting not only the original grids, but additionally, shifted/offset\ngrids that adjust the intervals, and consequently the counts in each grid cell.\nThe only issue with this approach is that this \"solution\" scales poorly with dimensionality: As we increase dimensions, we\nneed to take more grids (e.g. for 2D, we need our (i) Original plane, (ii) Shifted x, (iii) Shifted y, and (iv) Shifted x and\nshifted y). However, in light of this, this is a common solution approach for mitigating \"randomness\" in 2D binning.\n22.5.2\nDesired Properties of Dividing Up the Sphere/Tessellations\nTo build an optimal representation, we need to understand what our desired optimal properties of our tessellations will be:\n1. Equal areas of cells/facets\n2. Equal shapes of cells/facets\n3. \"Rounded\" shapes of cells/facets\n4. Regular pattern\n5. Allows for easy \"binning\": As an example a brute force way to do binning for spheres is to compute the dotr product\nwith all surface normals and take the largest of these. This is very impractical and computationally-inefficient because it\nrequires us to step through all data each time. For this reason, it is important we consider implementations/representations\nthat make binning efficient and straightforward.\n6. Alignment of rotation: The idea with this is that we need to bring one object into alignment with the other in order to\ndo effective recognition, one of the key tasks we cited.\nFor instance, with a dodecahedron, our orientation/direction histogram is represented by 12 numbers (which cor-\nrespond to the number of faces on the dodecahedron):\n[A1, A2, · · · , A12]\nWhen we bring this object into alignment during, for instance, a recognition task, we merely only need to permute the\norder of these 12 numbers - all information is preserved and there is no loss from situations such as overlapping. This is\nan advantage of having alignment of rotation.\n[A7, A4, · · · , A8]\nPlatonic and Archimedean solids are the representations we will use for these direction histograms.\nQuiz 1 Review\nHere you will find a review of some of the topics covered so far in Machine Vision. These are as follows in the section notes:\n1. Mathematics review - Unconstrained optimization, Green's Theorem, Bezout's Theorem, Nyquist Sampling Theorem\n2. Projection - Perspective vs. Orthographic, Object vs. Image space\n3. Optical Flow - BCCE/optical flow, Time to Contact, vanishing points\n4. Photometry - Radiance, irradiance, illumination, geometry, BRDF, Helmholtz Reciprocity\n5. Different surface types - Lambertian, Hapke\n6. Shape from Shading (SfS) - Hapke case, general case, reflectance maps, image irradiance equation\n\n7. Photometric Stereo - least squares, multiple measurement variant, multiple light sources variant\n8. Computational molecules - Sobel, Robert, Silver operators, finite differences (forward, backward, and average), Lapla-\ncians\n9. Lenses - Thin lenses, thick lenses, telecentric lens, focal length, principal planes, pinhole model\n10. Patent Review - Edge detector, Object Detection\n23.1\nQuick Mathematics Review\nLet's quickly touch on some important mathematical theorems that are useful for machine vision.\n- Unconstrained optimization: Suppose we are trying to optimize an objective J(a, b, c) by some set of parameters\n{a, b, c}. We can find the best set of parameters {a∗, b∗, c∗} using first-order conditions:\na∗, b∗, c∗= arg min\na,b,c J(a, b, c)\n→∂J\n∂a = 0\n→∂J\n∂b = 0\n→∂J\n∂c = 0\nThe solution a∗, b∗, c∗is the set of a, b, c that satisfies these first-order conditions. Note that this procedure is identical\nwith maximization.\n- Green's Theorem relates the line integral of a contour to the area located within that contour:\nI\nL\n(Ldx + Mdy) =\nZZ\nD\n∂M\n∂x -∂L\n∂y\n\ndxdy\n- B ezout's Theorem: The maximum number of solutions is the product of the polynomial order of each equation in the\nsystem of equations:\nnumber of solutions =\nE\nY\ne=1\noe\n- Nyquist Sampling Theorem: We must sample at twice the frequency of the highest-varying component of our image\nto avoid aliasing and consequently reducing spatial artifacts.\n- Taylor Series: We can expand any analytical, continuous, infinitely-differentiable function into its Taylor Series form\naccording to:\nf(x + δx) = f(x) + δxf ′(x) + (δx)2\n2!\nf ′′(x) + (δx)3\n3!\nf ′′′(x) + (δx)4\n24 f (4)(x) + ... =\ninf\nX\ni=0\n(δx)if (i)(x)\ni!\n, where 0!\n∆= 1\nFor a single variable function, and:\nf(x + δx, y + δy) = f(x, y) + δx\n∂f(x, y)\n∂x\n+ δy\n∂f(x, y)\n∂y\n+ · · ·\nFor a multivariable function.\n23.2\nProjection: Perspective and Orthographic\nA few important notes to remember:\n- We use CAPITAL LETTERS for WORLD SPACE, and lowercase letters for image space.\n- Orthographic projection is essentially the limit of perspective projection as we move the point we are mapping to the image\nplane far away from the optical axis.\n\n- We can derive perspective projection from the pinhole model and similar triangles.\nPerspective Projection:\nx\nf = X\nZ , y\nf = Y\nZ (component form)\nf r =\nR · ˆzR (vector form)\nOrthographic Projection:\nx = f\nZ0\nX, y = f\nZ0\ny\n23.3\nOptical Flow\nOptical flow captures the motion of an image over time in image space. Therefore, we will be interested in how brightness in the\nimage changes with respect to (i) time, (ii) in the x-direction, and (iii) in the y-direction. We will use derivatives to describe\nthese different quantities:\n- u\n∆= dx\ndt - i.e. the velocity in the image space in the x direction.\n- v\n∆= dx\ndt - i.e. the velocity in the image space in the y direction.\n-\n∂E\n∂x - i.e. how the brightness changes in the x direction.\n-\n∂E\n∂y - i.e. how the brightness changes in the y direction.\n-\n∂E\n∂t - i.e. how the brightness changes w.r.t. time.\nIf x(t) and y(t) both depend on t, then the chain rule gives us the following for the total derivative:\ndE(x, y, t)\ndt\n= dx\ndt\n∂E\n∂x + dy\ndt\n∂E\n∂y + ∂E\n∂t = 0\nRewriting this in terms of u, v from above:\nuEx + vEy + Et = 0\nThis equation above is known as the Brightness Change Constraint Equation (BCCE). This is also one of the most\nimportant equations in 2D optical flow.\nNormalizing the equation on the right by the magnitude of the brightness derivative vectors, we can derive the brightness\ngradient:\n(u, v) ·\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\n= (u, v) · ˆG = -\nEt\nq\nE2x + E2y\nBrightness Gradient : ˆG =\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\nWhat is the brightness gradient?\n- A unit vector given by:\n\nEx\n√\nE2\nx+E2\ny ,\nEy\n√\nE2\nx+E2\ny\n!\n∈R2.\n- Measures spatial changes in brightness in the image in the image plane x and y directions.\n\nWe are also interested in contours of constant brightness, or isophotes. These are curves on an illuminated surface that connects\npoints of equal brightness (source: Wikipedia).\nFinally, we are also interested in solving for optimal values of u and v for multiple measurements.\nIn the ideal case with-\nout noise:\nU\nV\n\n=\n(Ex1Ey2 -Ey1Ex2)\nEy2\n-Ey1\n-Ex2\nEx1\n-Et1\n-Et2\n\nWhen there is noise we simply minimize our objective, instead of setting it equal to zero:\nJ(u, v)\n∆=\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nWe solve for our set of optimal parameters by finding the set of parameters that minimizes this objective:\nu∗, v∗= arg min\nu,v J(u, v) = arg min\nu,v\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\nSince this is an unconstrained optimization problem, we can solve by finding the minimum of the two variables using two\nFirst-Order Conditions (FOCs):\n-\n∂J(u,v)\n∂u\n= 0\n-\n∂J(u,v)\n∂v\n= 0\nVanishing points: These are the points in the image plane (or extended out from the image plane) that parallel lines in the\nworld converge to. Applications include:\n- Multilateration\n- Calibration Objects (Sphere, Cube)\n- Camera Calibration\nWe will also look at Time to Contact (TTC):\nZ\nW\n∆== Z\ndZ\ndt\n= meters\nmeters\nseconds\n= seconds\nLet us express the inverse of this Time to Contact (TTC) quantity as C, which can be interpreted roughly as the number of\nframes until contact is made:\nC\n∆= W\nZ =\nTTC\n23.4\nPhotometry\nHere, we will mostly focus on some of the definitions we have encountered from lecture:\n- Photometry: Photometry is the science of measuring visible radiation, light, in units that are weighted according to the\nsensitivity of the human eye. It is a quantitative science based on a statistical model of the human visual perception of\nlight (eye sensitivity curve) under carefully controlled conditions.\n- Radiometry: Radiometry is the science of measuring radiation energy in any portion of the electromagnetic spectrum.\nIn practice, the term is usually limited to the measurement of ultraviolet (UV), visible (VIS), and infrared (IR) radiation\nusing optical instruments.\n- Irradiance: E\n∆= δP\nδA (W/m2). This corresponds to light falling on a surface. When imaging an object, irradiance is\nconverted to a grey level.\n- Intensity: I\n∆= δP\nδW (W/ster). This quantity applied to a point source and is often directionally-dependent.\n- Radiance: L\n∆=\nδ2P\nδAδΩ(W/m2× ster). This photometric quantity is a measure of how bright a surface appears in an image.\n\n- BRDF (Bidirectional Reflectance Distribution): f(θi, θe, φi, φe) = δL(θe,φe)\nδE(θi,φi) . This function captures the fact that\noftentimes, we are only interested in light hitting the camera, as opposed to the total amount of light emitted from an\nobject. Last time, we had the following equation to relate image irradiance with object/surface radiance:\nE = π\n4 L\nd\nf\ncos4 α\nWhere the irradiance of the image E is on the lefthand side and the radiance of the object/scene L is on the right. The\nBRDF must also satisfy Helmholtz reciprocity, otherwise we would be violating the 2nd Law of Thermodynamics.\nMathematically, recall that Helmholtz reciprocity is given by:\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\n23.5\nDifferent Surface Types\nWith many of our photometric properties established, we can also discuss photometric properties of different types of surfaces.\nBefore we dive in, it is important to also recall the definition of surface orientation:\n- p\n∆= ∂z\n∂x\n- q\n∆= ∂z\n∂y\n- Lambertian Surfaces:\n- Ideal Lambertian surfaces are equally bright from all directions, i.e.\nf(θi, θe, φi, φe) = f(θe, θi, φe, φi) ∀θi, θe, φi, φe\nAND\nf(θi, θe, φi, φe) = K ∈R with respect to θe, φe\n- \"Lambert's Law\":\nEi ∝cos θi = ˆn · ˆsi\n- Hapke Surfaces:\n- The BRDF of a Hapke surface is given by:\nf(θi, φi, θe, φe) =\n√cos θe cos θi\n- Isophotes of Hapke surfaces are linear in spatial gradient, or (p, q) space.\n- What do the isophotes of the moon look like, supposing the moon can fall under some of the different types of surfaces we\nhave discussed?\n- Lambertian: We will see circles and ellipses of isophotes, depending on the angle made between the viewer and the\nthe moon.\n- Hapke: Because of the BRDF behavior, isophotes will run longitudinally along the moon in the case in which it is\na Hapke surface.\n23.6\nShape from Shading (SfS)\nThis is one of the most fundamental problems in machine vision in which we try to estimate the surface of an object from\nbrightness measurements. Several variations of this problem we consider:\n- Hapke surfaces - this leads to linear isophotes in (p, q) space, and allows us to solve for one of the dimensions of interest.\n- General case: There are several techniques we can apply to solve for this:\n- Green's Theorem (see lecture notes handout)\n- Iterative Discrete Optimization (see lecture notes handout)\n\nFor these problems, we considered:\n- Characteristic strips (x, y, z, p, q)\n- Initial curves/base characteristics\n- Normalizing with respect to constant step sizes\n- A system of 5 ODEs\n- Stationary points and estimates of surfaces around them for initial points\nNext, let us review reflectance maps. A reflectance map R(p, q) is a lookup table (or, for simpler cases, a parametric function)\nthat stores the brightness for particular surface orientations p = ∂z\n∂x, q = ∂z\n∂y.\nThe Image Irradiance Equation relates the reflectance map to the brightness function in the image E(x, y) and is the\nfirst step in many Shape from Shading approaches.\nE(x, y) = R(p, q)\n23.7\nPhotometric Stereo\nPhotometric stereo is a technique in machine vision for estimating the surface normals of objects by observing that object\nunder different lighting conditions. This problem is quite common in machine vision applications.\nOne way we can solve photometric stereo is by taking multiple brightness measurements from a light source that we move\naround. This problem becomes:\n\n-sT\n-sT\n-sT\n\nn =\n\nE1\nE2\nE3\n\nWritten compactly:\nSn = E -→n = S-1E\nNote that we we need S to be invertible to compute this, which occurs when the light source vectors are not coplanar.\nOther variations of this problem:\n- Using light sources at different electromagnetic frequencies\n- Using a camera with different light filters\n- Moving the object to different locations\n23.8\nComputational Molecules\nThese are crucial components for applying machine concepts to real-life problems.\n1. Ex = 1\nε (E(x, y) -E(x -1, y)) (Backward Difference), 1\nε (z(x + 1, y) -z(x, y)) (Forward Difference)\n2. Ey = 1\nε (E(x, y) -E(x, y -1)) (Backward Difference), 1\nε (E(x, y + 1) -E(x, y)) (Forward Difference)\n3. ∆E = ∇2E = 1\nε2 (4E(x, y) -(E(x -1, y) + E(x + 1, y) + E(x, y -1) + E(x, y + 1)))\n4. Exx = ∂2E\n∂x2 = 1\nε2 (E(x -1, y) -2(x, y) + E(x + 1, y))\n5. Eyy = ∂2z\n∂y2 = 1\nε2 (E(x, y -1) -2(x, y) + E(x, y + 1))\n6. Robert's Cross: This approximates derivatives in a coordinate system rotated 45 degrees (x′, y′). The derivatives can\nbe approximated using the Kx′ and Ky′ kernels:\n∂E\n∂x′ →Kx′ =\n-1\n-1\n\n∂E\n∂y′ →Ky′ =\n-1\n\n7. Sobel Operator: This computational molecule requires more computation and it is not as high-resolution. It is also more\nrobust to noise than the computational molecules used above:\n∂E\n∂x →Kx =\n\n-1\n-1\n\n∂E\n∂y →Ky =\n\n-1\n-1\n\n8. Silver Operators: This computational molecule is designed for a hexagonal grid. Though these filters have some advan-\ntages, unfortunately, they are not compatible with most cameras as very few cameras have a hexagonal pixel structure.\nFigure 91: Silver Operators with a hexagonal grid.\n9. \"Direct Edge\" Laplacian:\nε2\n\n-4\n\n10. \"Indirect Edge\" Laplacian:\n2ε2\n\n-4\n\n11. Rotationally-symmetric Laplacian:\nε2\n\n-4\n\n+ 1\n2ε2\n\n-4\n\n=\n6ε2\n\n-20\n\nSome methods we used for analyzing how well these work:\n1. Taylor Series: From previous lectures we saw that we could use averaging to reduce the error terms from 2nd order\nderivatives to third order derivatives. This is useful for analytically determining the error.\n2. Test functions: We will touch more on these later, but these are helpful for testing your derivative estimates using\nanalytical expressions, such as polynomial functions.\n3. Fourier domain: This type of analysis is helpful for understanding how these \"stencils\"/molecules affect higher (spatial)\nfrequency image content.\n\n23.9\nLenses\nLenses are also important, because they determine our ability to sense light and perform important machine vision applications.\nSome types of lenses:\n- Thin lenses are the first type of lens we consider. These are often made from glass spheres, and obey the following three\nrules:\n- Central rays (rays that pass through the center of the lens) are undeflected - this allows us to preserve perspective\nprojection as we had for pinhole cameras.\n- The ray from the focal center emerges parallel to the optical axis.\n- Any parallel rays go through the focal center.\n- Thick lenses (cascaded thin lenses)\n- Telecentric lenses - These \"move\" the the Center of Projection to infinity to achieve approximately orthographic pro-\njection.\n- Potential distortions caused by lenses:\n- Radial distortion: In order to bring the entire angle into an image (e.g. for wide-angle lenses), we have the \"squash\"\nthe edges of the solid angle, thus leading to distortion that is radially-dependent. Typically, other lens defects are\nmitigated at the cost of increased radial distortion. Some specific kinds of radial distortion [5]:\n∗Barrel distortion\n∗Mustache distortion\n∗Pincushion distortion\n- Lens Defects: These occur frequently when manufacturing lenses, and can originate from a multitude of different\nissues.\n23.10\nPatent Review\nThe two patents we touched on were for:\n- Fast and Accurate Edge detection: At a high level, this framework leveraged the following steps for fast edge detection:\n- CORDIC algorithm for estimating (and subsequently quantizing) the brightness gradient direction\n- Thresholding gradients via Non-Maximum Suppression (NMS)\n- Interpolation at the sub-pixel level along with peak finding for accurate edge detection at sub-pixel level\n- Bias compensation\n- Plane position\nWe also discussed some issues/considerations with the following aspects of this framework's design:\n- Quantization of gradient direction\n- Interpolation method for sub-pixel accuracy (quadratic, piecewise linear, cubic, etc.)\n- Object detection and pose estimation: At a high level, this framework leveraged the following steps for fast object\ndetection/pose estimation:\n- Finding specific probing points in a template image that we use for comparing/estimating gradients.\n- Using different scoring functions to compare template image to different configurations of the runtime image to\nfind the object along with its pose (position with orientation).\nHere you will find a high-level review of some of the topics covered since Quiz 1. These are as follows in the section notes:\n1. Relevant Mathematics Review - Rayleigh Quotients, Groups, Levenberg-Marquadt, Bezout's Theorem\n2. Systems/Patents - PatQuick, PatMAx, Fast Convolutions, Hough Transforms\n3. Signals - Sampling, Spline Interpolation, Integral Images, Repeated Block Averaging\n4. Photogrammetry - Photogrammetry Problems, Absolute orientation, Relative Orientation, Interior Orientation, Exterior\nOrientation, Critical Surfaces, Radial and Tangential Distortion\n5. Rotations/Orientation - Gibb's vector, Rodrigues Formula, Quaternions, Orthonormal Matrices\n6. 3D Recognition - Extended Gaussian Image, Solids of Revolution, Polyhedral Objects, Desirable Properties\n\nQuiz 2 Review\n24.1\nRelevant Mathematics Review\nWe'll start with a review of some of the relevant mathematical tools we have relied on in the second part of the course.\n24.1.1\nRayleigh Quotients\nWe saw from our lectures with absolute orientation that this is a simpler way (relative to Lagrange Multipliers) to take con-\nstraints into account:\nThe intuitive idea behind them: How do I prevent my parameters from becoming too large) positive or negative) or too small\n(zero)? We can accomplish this by dividing our objective by our parameters, in this case our constraint. In this case, with the\nRayleigh Quotient taken into account, our objective becomes:\nmin\noq,\noq·\noq=1\noq\nT N\noq -→min\noq\noq\nT N\noq\noq\nT oq\n24.1.2\n(Optional) Groups\nThough we don't cover them specifically in this course, groups can be helpful for better understanding rotations. \"In mathe-\nmatics, a group is a set equipped with a binary operation that combines any two elements to form a third element in such a way\nthat four conditions called group axioms are satisfied, namely closure, associativity, identity and invertibility.\" [1]. We won't\nfocus too much on the mathematical details, but being aware of the following groups may be helpful when reading machine\nvision, robotics, control, or vision papers:\n- Orthonormal Rotation Matrices →SO(3) ∈R3×3, Special Orthogonal Group. Note that the \"3\" refers to the number\nof dimensions, which may vary depending on the domain.\n- Poses (Translation + Rotation Matrices) →SE(3) ∈R4×4, Special Euclidean Group.\nSince these groups do not span standard Euclidean spaces (it turns out these groups are both manifolds - though this is not\ngenerally true with groups), we cannot apply standard calculus-based optimization techniques to solve for them, as we have seen\nto be the case in our photogrammetry lectures.\n24.1.3\n(Optional) Levenberg-Marquadt and Gauss-Newton Nonlinear Optimization\nRecall this was a nonlinear optimization technique we saw to solve photogrammetry problems such as Bundle Adjustment (BA).\nLevenberg-Marquadt (LM) and Gauss-Newton (GN) are two nonlinear optimization procedures used for deriving solutions to\nnonlinear least squares problems. These two approaches are largely the same, except that LM uses an additional regularization\nterm to ensure that a solution exists by making the closed-form matrix to invert in the normal equations positive semidefinite.\nThe normal equations, which derive the closed-form solutions for GN and LM, are given by:\n1. GN: (J(θ)T J(θ))-1θ = J(θ)T e(θ) =⇒θ = (J(θ)T J(θ))-1J(θ)T e(θ)\n2. LM: (J(θ)T J(θ) + λI)-1θ = J(θ)T e(θ) =⇒θ = (J(θ)T J(θ) + λI)-1J(θ)T e(θ)\nWhere:\n- θ is the vector of parameters and our solution point to this nonlinear optimization problem.\n- J(θ) is the Jacobian of the nonlinear objective we seek to optimize.\n- e(θ) is the residual function of the objective evaluated with the current set of parameters.\nNote the λI, or regularization term, in Levenberg-Marquadt. If you're familiar with ridge regression, LM is effectively ridge\nregression/regression with L2 regularization for nonlinear optimization problems. Often, these approaches are solved iteratively\nusing gradient descent:\n1. GN: θ(t+1) ←θ(t) -α(J(θ(t))T J(θ(t)))-1J(θ(t))T e(θ(t))\n2. LM: θ(t+1) ←θ(t) -α(J(θ(t))T J(θ(t)) + λI)-1J(θ(t))T e(θ(t))\nWhere α is the step size, which dictates how quickly the estimates of our approaches update.\n\n24.1.4\nBezout's Theorem\nThough you're probably well-versed with this theorem by now, its importance is paramount for understanding the number of\nsolutions we are faced with when we solve our systems:\nTheorem: The maximum number of solutions is the product of the polynomial order of each equation in the system of equa-\ntions:\nnumber of solutions =\nE\nY\ne=1\noe\n24.2\nSystems\nIn this section, we'll review some of the systems we covered in this course through patents, namely PatQuick, PatMAx, and Fast\nConvolutions. A block diagram showing how we can cascade the edge detection systems we studied in this class can be found\nbelow:\nFigure 92: An overview of how the patents we have looked at for object inspection fit together.\n24.2.1\nPatQuick\nThere were three main \"objects\" in this model:\n- Training/template image. This produces a model consisting of probe points.\n- Model, containing probe points.\n- Probe points, which encode evidence for where to make gradient comparisons, i.e. to determine how good matches\nbetween the template image and the runtime image under the current pose configuration.\nOnce we have the model from the training step, we can summarize the process for generating matches as:\n1. Loop over/sample from configurations of the pose space (which is determined and parameterized by our degrees of freedom),\nand modify the runtime image according to the current pose configuration.\n2. Using the probe points of the model, compare the gradient direction (or magnitude, depending on the scoring function)\nto the gradient direction (magnitude) of the runtime image under the current configuration, and score using one of the\nscoring functions below.\n3. Running this for all/all sampled pose configurations from the pose space produces a multidimensional scoring surface. We\ncan find matches by looking for peak values in this surface.\n24.2.2\nPatMAx\n- This framework builds offof the previous PatQuick patent.\n- This framework, unlike PatQuick, does not perform quantization of the pose space, which is one key factor in enabling\nsub-pixel accuracy.\n- PatMAx assumes we already have an approximate initial estimate of the pose.\n- PatMAx relies on an iterative process for optimizing energy, and each attraction step improves the fit of the configuration.\n- Another motivation for the name of this patent is based offof electrostatic components, namely dipoles, from Maxwell. As\nit turns out, however, this analogy works better with mechanical springs than with electrostatic dipoles.\n- PatMAx performs an iterative attraction process to obtain an estimate of the pose.\n\n- An iterative approach (e.g. gradient descent, Gauss-Newton, Levenberg-Marquadt) is taken because we likely will not\nhave a closed-form solution in the real world. Rather than solving for a closed-form solution, we will run this iterative\noptimization procedure until we reach convergence.\nFigure 93: High-level diagram of the PatMAx system.\n24.2.3\nFast Convolutions\nThe patent we explore is \"Efficient Flexible Digital Filtering, US 6.457,032.\nThe goal of this system is to efficiently compute filters for multiscale. For this, we assume the form of an Nth-order piece-\nwise polynomial, i.e. a Nth-order spline.\n24.2.4\nSystem Overview\nThe block diagram of this system can be found below:\nFigure 94: Block diagram of this sparse/fast convolution framework for digital filtering. Note that this can be viewed as a\ncompression problem, in which differencing compresses the signal, and summing decompresses the signal.\n\nA few notes on this system:\n- Why is it of interest, if we have Nth-order splines as our functions, to take Nth-order differences? The reason for this is\nthat the differences create sparsity, which is critical for fast and efficient convolution. Sparsity is ensured because:\ndN+1\ndxN+1 f(x) = 0 ∀x if f(x) =\nN\nX\ni=0\naixi, ai ∈R ∀i ∈{1, · · · , N}\n(I.e, if f(x) is a order-N polynomial, then the order-(N+1) difference will be 0 for all x.\nThis sparse structure makes convolutions much easier and more efficient to compute by reducing the size/cardinality of\nthe support.\n- Why do we apply an order-(N+1) summing operator?\nWe apply this because we need to \"invert\" the effects of the\norder-(N+1) difference:\nFirst Order : DS = I\nSecond Order : DDSS = DSDS = (DS)(DS) = II = I\n...\nOrder K : (D)K(S)K = (DS)K = IK = I\n24.2.5\nHough Transforms\nMotivation: Edge and line detection for industrial machine vision; we are looking for lines in images, but our gradient-based\nmethods may not necessarily work, e.g. due to non-contiguous lines that have \"bubbles\" or other discontinuities. These discon-\ntinuities can show up especially for smaller resolution levels.\nIdea: The main idea of the Hough Transform is to intelligently map from image/surface space to parameter space for\nthat surface.\nSome notes on Hough Transforms:\n- Often used as a subroutine in many other machine vision algorithms.\n- Actually generalize beyond edge and line detection, and extend more generally into any domain in which we map a\nparameterized surface in image space into parameter space in order to estimate parameters.\nExample: Hough Transforms with Lines A line/edge in image space can be expressed (in two-dimensions for now, just for\nbuilding intuition, but this framework is amenable for broader generalizations into higher-dimensional lines/planes): y = mx+c.\nNote that because y = mx + c, m = y-c\nx\nand c = y -mx. Therefore, this necessitates that:\n- A line in image space maps to a singular point in Hough parameter space.\n- A singular point in line space corresponds to a line in Hough parameter space.\nFigure 95: Example of finding parameters in Hough Space via the Hough Transform.\n\n24.3\nPhotogrammetry\nGiven the length of the mathematical derivations in these sections, we invite you to revisit notes in lectures 17-21 for a more\nformal treatment of these topics. In this review, we hope to provide you with strong intuition about different classes of pho-\ntogrammetric problems and their solutions.\nFour important problems in photogrammetry that we covered in this course:\n- Absolute Orientation 3D ←→3D, Range Data\n- Relative Orientation 2D ←→2D, Binocular Stereo\n- Exterior Orientation 2D ←→3D, Passive Navigation\n- Intrinsic Orientation 3D ←→2D, Camera Calibration\nBelow we discuss each of these problems at a high level. We will be discussing these problems in greater depth later in this and\nfollowing lectures.\n24.3.1\nAbsolute Orientation\nWe will start with covering absolute orientation. This problem asks about the relationship between two or more objects\n(cameras, points, other sensors) in 3D. Some examples of this problem include:\n1. Given two 3D sensors, such as lidar (light detection and ranging) sensors, our goal is to find the transformation, or pose,\nbetween these two sensors.\n2. Given one 3D sensor, such as a lidar sensor, and two objects (note that this could be two distinct objects at a single point\nin time, or a single object at two distinct points in time), our goal is to find the transformation, or pose, between the\ntwo objects.\nFigure 96: General case of absolute orientation: Given the coordinate systems (xl, yl, zl) ∈R3×3 and (xr, yr, zr) ∈R3×3, our\ngoal is to find the transformation, or pose, between them using points measured in each frame of reference pi.\n24.3.2\nRelative Orientation\nThis problem asks how we can find the 2D ←→2D relationship between two objects, such as cameras, points, or other sensors.\nThis type of problem comes up frequently in machine vision, for instance, binocular stereo.\nTwo high-level applications\ninclude:\n1. Given two cameras/images that these cameras take, our goal is to extract 3D information by finding the relationship\nbetween two 2D images.\n2. Given two cameras, our goal is to find the (relative) transformation, or pose, between the two cameras.\n\nFigure 97: Binocular stereo system set up. For this problem, recall that one of our objectives is to measure the translation, or\nbaseline, between the two cameras.\n24.3.3\nExterior Orientation\nThis photogrammetry problem aims from going 2D -→3D. One common example in robotics (and other field related to machine\nvision) is localization, in which a robotic agent must find their location/orientation on a map given 2D information from a\ncamera (as well as, possibly, 3D laser scan measurements).\nMore generally, with localization, our goal is to find where we are and how we are oriented in space given a 2D image\nand a 3D model of the world.\nFigure 98: Exterior orientation example: Determining position and orientation from a plane using a camera and landmark\nobservations on the ground.\n\nFigure 99: Bundle Adjustment (BA) is another problem class that relies on exterior orientation: we seek to find the orientation\nof cameras using image location of landmarks. In the general case, we can have any number of K landmark points (\"interesting\"\npoints in the image) and N cameras that observe the landmarks.\n24.3.4\nInterior Orientation\nThis photogrammetry problem aims from going 3D -→2D. The most common application of this problem is camera calibra-\ntion. Camera calibration is crucial for high-precision imaging, as well as solving machine and computer vision problems such as\nBundle Adjustment [1]. Finding a principal point is another example of the interior orientation problem.\nFigure 100: Interior orientation seeks to find the transformation between a camera and a calibration object - a task often known\nas camera calibration. This can be used, for instance, with Tsai's calibration method (note that this method also relies on\nexterior orientation).\n24.4\nRotation\nThere are a myriad of representations for rotations - some of these representations include:\n1. Axis and angle\n2. Euler Angles\n3. Orthonormal Matrices\n4. Exponential cross product\n5. Stereography plus bilinear complex map\n6. Pauli Spin Matrices\n7. Euler Parameters\n8. Unit Quaternions\nWe would also like our representations to have the following properties:\n\n- The ability to rotate vectors - or coordinate systems\n- The ability to compose rotations\n- Intuitive, non-redundant representation - e.g. rotation matrices have 9 entries but only 3 degrees of freedom\n- Computational efficiency\n- Interpolate orientations\n- Averages over range of rotations\n- Derivative with respect to rotations - e.g. for optimization and least squares\n- Sampling of rotations - uniform and random (e.g. if we do not have access to closed-form solutions)\n- Notion of a space of rotations\nLet us delve into some of these in a little more depth.\n24.4.1\nAxis and Angle\nThis representation is composed of a vector ˆω and an angle θ, along with the Gibb's vector that combines these given by\nˆω tan\nθ\n\n, which has magnitude tan\nθ\n\n, providing the system with an additional degree of freedom that is not afforded by unit\nvectors. Therefore, we have our full 3 rotational DOF.\n24.4.2\nOrthonormal Matrices\nWe have studied these previously, but these are the matrices that have the following properties:\n1. RT R = RRT = I, RT = R-1 (skew-symmetric)\n2. det |R| = +1\n3. R ∈SO(3) (see notes on groups above) - being a member of this Special Orthogonal group is contingent on satisfying the\nproperties above.\n24.4.3\nQuaternions\nIn this section, we will discuss another way to represent rotations: quaternions.\n24.4.4\nHamilton and Division Algebras\nHamilton's insight with quaternions is that these quaternions require a \"4th dimension for the sole purpose of calculating triples\".\nHamilton noted the following insights in order to formalize his quaternion. Therefore, the complex components i, j, k defined\nhave the following properties:\n1. i2 = j2 = k2 = ijk = -1\n2. From which follows:\n(a) ij = k\n(b) jk = i\n(c) ki = j\n(d) ji = -k\n(e) kj = -i\n(f) ik = -j\nNote: As you can see from these properties, multiplication of the components of these quaternions is not commutative.\nWe largely use the 4-vector quaternion, denoted\noq = (q, q) ∈R4.\n\n24.4.5\nProperties of 4-Vector Quaternions\nThese properties will be useful for representing vectors and operators such as rotation later:\n1. Not commutative:\nop\noq =\noq\nop\n2. Associative: (\nop\noq)\nor =\nop(\noq\nor)\n3. Conjugate: (p, p)∗= (p, -p) =⇒(\nop\noq) =\noq\n∗op\n4. Dot Product: (p, p) · (q, q) = pq + p + q\n5. Norm: ||\noq||2\n2 =\noq ·\noq\n6. Conjugate Multiplication:\noq\noq\n∗:\noq\noq\n∗= (q, q)(q, -q)\n= (q2 + q · q, 0)\n= (\noq ·\noq)\noe\nWhere\noe\n∆= (1, 0), i.e. it is a quaternion with no vector component. Conversely, then, we have:\noq\n∗oq = (\noq\noq)\noe.\n7. Multiplicative Inverse:\noq\n-1 =\noq\n∗\n(\noq·\noq (Except for\noq = (0, 0), which is problematic with other representations anyway.)\nWe can also look at properties with dot products:\n1. (\nop\noq) · (\nop\noq) = (\nop ·\nop)(\noq ·\noq)\n2. (\nop\noq) · (\nop\nor) = (\nop ·\nop)(\noq ·\nor)\n3. (\nop\noq) ·\nor =\nop · (\nor\noq\n∗)\nWe can also represent these quaternions as vectors. Note that these quaternions all have zero scalar component.\n1.\nor = (0, r)\n2.\nor\n∗= (0, -r)\n3.\nor ·\nos = r · s\n4.\nor\nos = (-r · s, r × s)\n5. (\nor\nos) ·\no\nt =\nor · (\nos\no\nt) = [r s t] (Triple Products)\n6.\nor\nor = -(r · r)\noe\nAnother note: For representing rotations, we will use unit quaternions. We can represent scalars and vectors with:\n- Representing scalars: (s, 0)\n- Representing vectors: (0, v)\n24.4.6\nQuaternion Rotation Operator\nTo represent a rotation operator using quaternions, we need a quaternion operation that maps from vectors to vectors. More\nspecifically, we need an operation that maps from 4D, the operation in which quaternions reside, to 3D in order to ensure that\nwe are in the correct for rotation in 3-space. Therefore, our rotation operator is given:\nor\n′ = R(\nor) =\noq\nor\noq\n∗\n= (Q\nor)\noq\n∗\n= ( QT Q)\nor\n\nWhere the matrix QT Q is given by:\nQT Q =\n\noq ·\noq\nq2\n0 + q2\nx -q2\ny -q2\nz\n2(qxqy -q0qz)\n2(qxqz + q0qy)\n2(qyqx + q0qz)\nq2\n0 -q2\nx + q2\ny -q2\nz\n2(qyqz -q0qx)\n2(qzqx -q0qy)\n2(qzqy + q0qz)\nq2\n0 -q2\nx -q2\ny + q2\nz\n\nA few notes about this matrix:\n- Since the scalar component of\noq is zero, the first row and matrix of this column are sparse, as we can see above.\n- If\noq is a unit quaternion, the lower right 3 × 3 matrix of QT Q will be orthonormal (it is an orthonormal rotation matrix).\nLet us look at more properties of this mapping\nor\n′ =\noq\nor\noq\n∗:\n1. Scalar Component: r′ = r(\noq ·\noq)\n2. Vector Component: r′ = (q2 -q · q)r + 2(q · r)q + 2q(q × r)\n3. Operator Preserves Dot Products:\nor\n′ ·\nos\n′ =\nor ·\nos =⇒r′ · s′ = r · s\n4. Operator Preserves Triple Products: (\nor\n′ ·\nos\n′) ·\no\nt\n′\n= (\nor ·\nos) ·\no\nt =⇒(r′ · s′)t′ = (r · s) · t =⇒[r′ s′ t′] = [r s t]\n5. Composition (of rotations!): Recall before that we could not easily compose rotations with our other rotation repre-\nsentations. Because of associativity, however, we can compose rotations simply through quaternion multiplication:\nop(\noq\nor\noq\n∗)\nop\n∗= (\nop\noq)\nor(\noq\n∗op\n∗) = (\nop\noq)\nor(\nop\noq)∗\nI.e. if we denote the product of quaternions\noz\n∆=\nop\noq, then we can write this rotation operator as a single rotation:\nop(\noq\nor\noq\n∗)\nop\n∗= (\nop\noq)\nor(\noq\n∗op\n∗) = (\nop\noq)\nor(\nop\noq)∗=\noz\nor\noz\n∗\nThis ability to compose rotations is quite advantageous relative to many of the other representations of rotations we have\nseen before (orthonormal rotation matrices can achieve this as well).\n24.5\n3D Recognition\n24.5.1\nExtended Gaussian Image\nThe idea of the extended Gaussian Image: what do points on an object and points on a sphere have in common? They have the\nsame surface normals.\nFigure 101: Mapping from object space the Gaussian sphere using correspondences between surface normal vectors. Recall that\nthis method can be used for tasks such as image recognition and image alignment.\n- Curvature: κ = δS\nδO = limδ→0 δS\nδO = dS\ndO\n- Density: G(η) = δO\nδS = limδ→0 δO\nδS = dO\ndS\n\n24.5.2\nEGI with Solids of Revolution\nAre there geometric shapes that lend themselves well for an \"intermediate representation\" with EGI (not too simple, nor too\ncomplex)? It turns out there are, and these are the solids of revolution. These include:\n- Cylinders\n- Spheres\n- Cones\n- Hyperboloids of one and two sheets\nHow do we compute the EGI of solids of revolution? We can use generators that produce these objects to help.\nFigure 102: EGI representation of a generalized solid of revolution. Note that bands in the object domain correspond to bands\nin the sphere domain.\nAs we can see from the figure, the bands \"map\" into each other! These solids of revolution are symmetric in both the object\nand transform space. Let's look at constructing infinitesimal areas so we can then compute Gaussian curvature κ and density G:\n- Area of object band: δO = 2πrδs\n- Area of sphere band: δS = 2π cos(η)δn\nThen we can compute the curvature as:\nκ = δS\nδO = 2π cos(η)δη\n2πrδs\n= cos(η)δη\nrδs\nG = 1\nκ = δO\nδS = r sec(η) δs\nδη\nThen in the limit of δ →0, our curvature and density become:\nκ = lim\nδ→O\nδS\nδO = cos η\nr\ndη\nds (Where dη\nds is the rate of change of surface normal direction along the arc, i.e. curvature)\nG = lim\nδ→0\nδO\nδS = r sec η ds\ndη (Where ds\ndη is the rate of change of the arc length w.r.t. angle)\nκ = cos η\nr\nκ\nRecall that we covered this for the following\n24.5.3\nSampling From Spheres Using Regular and Semi-Regular Polyhedra\nMore efficient to sample rotations from shapes that form a \"tighter fit\" around the sphere - for instance: polyhedra! Some\npolyhedra we can use:\n- Tetrahedra (4 faces)\n- Hexahedra (6 faces)\n\n- Octahedra (8 faces)\n- Dodecahedra (12 faces)\n- Icosahedra (20 faces)\nThese polyhedra are also known as the regular solids.\nAs we did for the cube, we can do the same for polyhedra: to sample from the sphere, we can sample from the polyhedra,\nand then project onto the point on the sphere that intersects the line from the origin to the sampled point on the polyhedra.\nFrom this, we get great circles from the edges of these polyhedra on the sphere when we project.\nFun fact: Soccer balls have 32 faces! More related to geometry: soccer balls are part of a group of semi-regular solids,\nspecifically an icosadodecahedron.\n24.5.4\nDesired Properties of Dividing Up the Sphere/Tessellations\nTo build an optimal representation, we need to understand what our desired optimal properties of our tessellations will be:\n1. Equal areas of cells/facets\n2. Equal shapes of cells/facets\n3. \"Rounded\" shapes of cells/facets\n4. Regular pattern\n5. Allows for easy \"binning\"\n6. Alignment of rotation\nPlatonic and Archimedean solids are the representations we will use for these direction histograms.\n24.6\nReferences\n1. Groups, https://en.wikipedia.org/wiki/Group (mathematics)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801 / 6.868 Machine Vision, Lecture 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/5546a6b8d36a2d997929ba1aeb8c5ed3_MIT6_801F20_lec2.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 2\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 2: Image Formation, Perspective Projection, Time Derivative, Mo-\ntion Field\nDefinition of perspective projection:\nx\nf = X\nZ , y\nf = Y\nZ (component form)\n(1)\nf r =\nR · ˆzR (vector form)\n(2)\nIf we differentiate these perspective projection equations:\nf\ndx\ndt = 1\nZ\ndX\ndt -X\nZ2\ndZ\ndt\nWhat are these derivatives? They correspond to velocities. Let's define some of these velocities:\n- u\n∆= dx\ndt\n- v\n∆= dy\ndt\n- U\n∆= dX\ndt\n- V\n∆= dY\ndt\n- W\n∆= dZ\ndt\nNow, rewriting the differentiated perspective projection equations with these velocity terms, we first write the equation for the\nx component:\nf u = 1\nZ U -X\nZ2 W\n(3)\nSimilarly, for y:\nf v = 1\nZ V -Y\nZ2 W\n(4)\nWhy are these equations relevant? They allow us to find parts of the image that don't exhibit any motion - i.e. stationary\npoints. Let's find where U = V = 0. Let the point (x0, y0) correspond to this point. Then:\nx0\nf = U\nW , y0\nf = V\nW\n(5)\n\nFocus of Expansion (FOE): Point in image space given by (x0, y0). This point is where the 3D motion vector intersects with\nthe line given by z = f.\nWhy is FOE useful?\nIf you know FOE, you can derive the direction of motion by drawing a vector from the origin to\nFOE.\nAdditionally, we can rewrite the differentiated perspective projection equations with FOE:\nf u = x0 -x\nf\nW\nZ (x comp.),\nf v = y0 -y\nf\nW\nZ (y comp.)\n(6)\nCancelling out the focal length (f) terms:\nu = (x0 -x)W\nZ (x comp.), v = (y0 -y)W\nZ (y comp.)\n(7)\nA few points here:\n- You can draw the vector diagram of the motion field in the image plane.\n- All vectors in the motion field expand outward from FOE.\n- Recall that perspective projection cannot give us absolute distances.\nFor building intuition, let's additionally consider what each of these quantities mean. The inverse term\nZ\nW =\nZ\ndZ\ndt has units of\nmeters\nmeters\nsecond = seconds - i.e. Time of Impact.\nLet's now revisit these equations in vector form, rather than in the component form derived above:\nf\ndr\ndt =\nR · ˆz -\nR\n(R · ˆr)2\nd\ndt(R · ˆr)\n(8)\nLet's rewrite this with dots for derivatives. Fun fact: The above notation is Leibniz notation, and the following is Newtonian\nnotation:\nf r =\nR · ˆz\nR -\nR\n(R · ˆz)2 ( R · ˆz)\n(9)\nf r = 1\nZ ( R -W 1\nf r)\n(10)\nOne way for reasoning about these equations is that motion is magnified by the ratio of the distance terms.\nNext, we'll reintroduce the idea of Focus of Expansion, but this time, for the vector form.\nFOE in the vector form is\ngiven at the point where r = 0:\nf r = 1\nW\nR\n(11)\nWe can use a dot product/cross product identity to rewrite the above expression in terms of cross products. The identity is as\nfollows for any a, b, c ∈Rn\na × (b × c) = (c · a)b -(a · b)c\n(12)\nUsing this identity, we rewrite the expression above to solve for FOE:\nf r =\n(R · ˆz)2 (ˆz × ( R × R))\n(13)\nWhat is this expression? This is image motion expressed in terms of world motion. Note the following identities/properties\nof this motion, which are helpful for building intuition:\n- r·ˆz = 0 =⇒Image motion is perpendicular to the z-axis. This makes since intuitively because otherwise the image would\nbe coming out of/going into the image plane.\n- r ⊥ˆz\n- R ∥R =⇒ r = 0 (this condition results in there being no image motion).\n\n1.1\nBrightness and Motion\nLet's now consider how brightness and motion are intertwined. Note that for this section, we will frequently be switching between\ncontinuous and discrete. The following substitutions/conversions are made:\n- Representations of brightness functions: E(x, y) ↔E[x, y]\n- Integrals and Sums:\nR\nx\nR\ny ↔P\nx\nP\ny\n- Brightness Gradients and Finite Differences: ( ∂E\n∂x , ∂E\n∂y ) ↔( 1\nδx(E[k, e + 1] -E[k, e])\n1.1.1\n1D Case\ndx\ndt = U =⇒δx = Uδ\n(14)\nBy taking a linear approximation of the local brightness:\nδE = Exδx = uExδt\n(note here that Ex = ∂E\n∂x )\n(15)\nDividing each side by δt, we have:\nuEx + Et = 0 =⇒U = -Ex\nEt\n= -\n∂E\n∂t\n∂E\n∂x\n(16)\nA couple of points about this:\n- This 1D result allows us to recover motion from brightness.\n- We can infer motion from a single point. However, this is only true in the 1D case.\n- We can estimate from 1 pixel, but frequently, we have much more than 1 pixel, so why use just 1? We can reduce noise by\nestimating motion from many pixels through regression techniques such as Ordinary Least Squares (OLS).\n- From statistics, the standard deviation of the motion estimates will be reduced by a factor of\n√\nN , where N is the number\nof pixels sampled for estimating motion.\nFinite Difference approximation for E is given by:\nE ≈1\nδx(E(x + δx, t) -E(x, t))\n(17)\nMotion estimation can be done through unweighted averaging:\nuunweighted = 1\nN\nN\nX\ni=1\n-Eti\nExi\n(18)\nAs well as weighted averaging:\nuweighted =\nPN\ni=1 wi\n-Eti\nExi\nPN\ni=1 wi\n(19)\nA quick check here: take wi = 1 ∀i ∈{1, ..., N}. Then we have that uweighted = 1\nN\nPN\ni=1\n-Et\nEx = uunweighted.\nNote that in the continuous domain, the sums in the weighted and unweighted average values are simply replaced with in-\ntegrals.\n\n1.1.2\n2D Case\nWhile these results are great, we must remember that images are in 2D, and not 1D. Let's look at the 2D case. First and\nforemost, let's look at the brightness function, since it now depends on x, y, and t: E(x, y, t). The relevant partial derivatives\nhere are thus:\n-\n∂E\n∂x - i.e. how the brightness changes in the x direction.\n-\n∂E\n∂y - i.e. how the brightness changes in the y direction.\n-\n∂E\n∂t - i.e. how the brightness changes w.r.t. time.\nAs in the previous 1D case, we can approximate these derivatives with finite forward first differences:\n-\n∂E\n∂x = Ex ≈\nδx(E(x + δx, y, t) -E(x, y, t))\n-\n∂E\n∂y = Ey ≈\nδy(E(x, y + δy, t) -E(x, y, t))\n-\n∂E\n∂t = Et ≈1\nδt(E(x, y, t + δt) -E(x, y, t))\nFurthermore, let's suppose that x and y are parameterized by time, i.e. x = x(t), y = y(t). Then we can compute the First-Order\nCondition (FOC) given by:\ndE(x, y, t)\ndt\n= 0\n(20)\nHere, we can invoke the chain rule, and we obtain the result given by:\ndE(x, y, t)\ndt\n= dx\ndt\n∂E\n∂x + dy\ndt\n∂E\n∂y + ∂E\n∂t = 0\n(21)\nRewriting this in terms of u, v, w from above:\nuEx + vEy + Et = 0\n(22)\nObjective here: We have a time-varying sequence of images, and our goal is to find and recover motion.\nTo build intuition, it is also common to plot in velocity space given by (u, v).\nFor instance, a linear equation in the 2D\nworld corresponds to a line in velocity space. Rewriting the equation above as a dot product:\nuEx + vEy + Et = 0 ↔(u, v) · (Ex, Ey) = -Et\n(23)\nNormalizing the equation on the right by the magnitude of the brightness derivative vectors, we obtain the brightness gradient:\n(u, v) ·\n\nEx\nq\nE2x + E2y\n,\nEy\nq\nE2x + E2y\n!\n= -\nEt\nq\nE2x + E2y\n(24)\nWhat is the brightness gradient?\n- A unit vector given by:\n\nEx\n√\nE2\nx+E2\ny ,\nEy\n√\nE2\nx+E2\ny\n!\n∈R2.\n- Measures spatial changes in brightness in the image in the image plane x and y directions.\nIsophotes: A curve on an illuminated surface that connects points of equal brightness (source: Wikipedia).\nAs we saw in the previous case with 1D, we don't want to just estimate with just one pixel.\nFor multiple pixels, we will\nsolve a system of N equations and two unknowns:\nuEx1 + vEy1 + Et1 = 0\n(25)\nuEx2 + vEy2 + Et2 = 0\n(26)\nRewriting this in matrix form:\n\nEx1\nEy1\nEx2\nEy2\n\nU\nV\n\n=\n\n-Et1\n-Et2\n\n(27)\n\nSolving this as a standard Ax = b problem, we have:\nU\nV\n\n=\n(Ex1Ey2 -Ey1Ex2)\nEy2\n-Ey1\n-Ex2\nEx1\n-Et1\n-Et2\n\n(28)\nNote that the expression given by\n(Ex1Ey2-Ey1Ex2) is the determinant of the partial derivatives matrix, since we are taking its\ninverse (in this case, simply a 2x2 matrix).\nWhen can/does this fail? It's important to be cognizant of edge cases in which this motion estimation procedure/algo-\nrithm fails. Some cases to consider:\n- When brightness partial derivatives / brightness gradients are parallel to one another ↔The determinant goes to zero ↔\nThis corresponds to linear dependence in the observations. This occurs when Ex1Ey2 = Ey1Ex2 =⇒\nEy1\nEx1 = Ey2\nEx2 .\nThis issue can be mitigated by weighting the pixels as we saw in the 1D case above. However, a more robust solution is to search\nfor a minima of motion, rather than the point where it has zero motion. The intuition here is that even if we aren't able to find\na point of zero motion, we can still get as close to zero as possible. Mathematically, let us define the following objective:\nJ(u, v)\n∆=\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\n(29)\nThen we now seek to solve the problem of:\nu∗, v∗= arg min\nu,v J(u, v) = arg min\nu,v\nZ\nx∈X\nZ\ny∈Y\n(uEx + vEy + Et)2dxdy\n(30)\nSince this is an unconstrained optimization problem, we can solve by finding the minima of the two variables using two First-Order\nConditions (FOCs):\n-\n∂J(u,v)\n∂u\n= 0\n-\n∂J(u,v)\n∂v\n= 0\nHere, we have two equations and two unknowns. When can this fail?\n- When we have linear independence. This occurs when:\n- E = 0 everywhere\n- E = constant\n- Ex = 0\n- Ey = 0\n- Ex = Ey\n- Ex = kEy\n- When E = 0 everywhere (professor's intuition: \"You're in a mine.\")\n- When Ex, Ey = 0 (constant brightness).\n- Mathematically, this fails when:\nR\nx\nR\nx E2\nx\nR\ny\nR\ny E2\ny -(\nR\nx\nR\ny ExEy)2 = 0\nWhen is this approach possible? Only when isophotes are not parallel straight lines - i.e.\nwant isophote curva-\nture/rapid turning of brightness gradients.\nNoise Gain: Intuition - if I change a value by this much in the image, how much does this change in the result?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/617445f0e31836831b40d42cb2f11a10_MIT6_801F20_lec10.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 10\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 10: Characteristic Strip Expansion, Shape from Shading, Iterative\nSolutions\nIn this lecture, we will continue studying our shape from shading problem, this time generalizing it beyond Hapke surfaces,\nmaking this framework amenable for a range of machine vision tasks. We will introduce a set of ordinary differential equations,\nand illustrate how we can construct a linear dynamical system that allows us to iteratively estimate the depth profile of an object\nusing both position and surface orientation.\n1.1\nReview: Where We Are and Shape From Shading\nShape from Shading (SfS) is one of the class of problems we study in this course, and is part of a larger class of problems\nfocusing on machine vision from image projection. In the previous lecture, we solved shape from shading for Hapke surfaces\nusing the Image Irradiance Equation E(x, y) = R(p, q), where R(p, q) is the reflectance map that builds on the Bidirectional\nReflectance Distribution Function (BRDF).\nRecall additionally that irradiance (brightness in the image) depends on:\n1. Illumination\n2. Surface material of object being imaged\n3. Geometry of the object being imaged\nRecall from the previous lecture that for Hapke surfaces, we were able to solve the SfS problem in a particular direction, which\nwe could then determine object surface orientation along. We can integrate along this direction (the profile direction) to find\nheight z, but recall that we gain no information from the other direction.\nFor the Hapke example, we have a rotated coordinate system governed by the following ODEs (note that xi will be the variable\nwe parameterize our profiles along):\ndx\n1. X-direction:\n= ps\ndξ\ndy\n2. Y-direction:\n= qs\ndξ\ndz\n∂z dx\n∂z dy\n3. By chain rule, Z-direction:\n=\n+\n= pps + qqs = psp + qsq\ndξ\n∂x dξ\n∂y dξ\ndx\ndy\ndz\nIntuition: Infinitesimal steps in the image given by ξ gives\n,\n, and we are interested in finding the change in height\n,\ndξ\ndξ\ndξ\nwhich can be used for recovering surface orientation.\nf\nNote: When dealing with brightness problems, e.g. SfS, we have implicitly shifted to orthographic projection (x = Z0 X, y =\nf\nZ0 y). These methods can be applied to perspective projection as well, but the mathematics makes the intuition less clear. We\ncan model orthographic projection by having a telecentric lens, which effectively places the object really far away from the\nimage plane.\n\nWe can solve the 3 ODEs above using a forward Euler method with a given step size. For small steps, this approach will be\naccurate enough, and accuracy here is not too important anyway since we will have noise in our brightness measurements.\nRecall brightness of Hapke surfaces is given by:\nr\nr\nnˆ · ˆs\ncos θi\nE =\n=\n=\nnˆ · vˆ\ncos θe\np\nfrac1 + psp + qsq 1 + p2 + q2rs · √ 1 + psp + qsq\nSome cancellation and rearranging yields:\npsp + qsq = rsE2 - 1\nTherefore, we have a direct relationship between our measured brightness E and our unknowns of interest:\n∂z\n∂z\nps +\nqs = pps + qqs = rsE2 - 1\n∂x\n∂y\nNote here, however, that we do not know surface orientation based on this, since again for Hapke surfaces, we only know slope\nin one of our two directions in our rotated gradient space (p , q0). A forward Euler approach will generate a set of independent\nprofiles, and we do not have any information about the surface orientation at the interfaces of these independent profiles. This\nnecessitates an initial curve containing initial conditions for each of these independent profiles. In 3D, this initial curve is\nparameterized by η, and is given by: (x(η), y(η), z(η)). Our goal is to find z(η, ξ), where:\n- η parameterizes the length of the initial curve\n- ξ parameterizes along the 1D profiles we carve out\nTherefore, we are able to determine the slope in a particular direction.\n1.2\nGeneral Case\nLet us now generalize our Hapke case above to be any surface. We begin with our Image Irradiance Equation:\nE(x, y) = R(p, q)\nLet us now consider taking a small step δx, δy in the image, and our goal is to determine how z changes from this small step.\nWe can do so using differentials and surface orientation:\n∂z\n∂z\nδz =\nδx +\nδy = pδx + qδy\n∂x\n∂y\nTherefore, if we know p and q, we can compute δz for a given δx and δy. But in addition to updating z using the equation\nabove, we will also need to update p and q (intuitively, since we are still moving, the surface orientation can change):\n∂p\n∂p\nδp =\nδx +\nδy\n∂x\n∂y\n∂q\n∂q\nδq =\nδx +\nδy\n∂x\n∂y\nThis notion of updating p and q provides motivation for keeping track/updating not only (x, y, z), but also p and q. We can\nthink of solving our problem as constructing a characteristic strip of the ODEs above, composed of (x, y, z, p, q) ∈ R5 . In\nvector-matrix form, our updates to p and q become:\n⎡\n⎤\n⎡\n⎤\n∂2\n∂2\n⎡\n⎤\nz\nz\n\nr\ns\npx\npy\n∂x2\n∂y∂x\nδp\nδx\n⎢\n⎥ δx\nδx\nδx\n⎣\n⎦\n⎣\n⎦\n=\n= ⎣\n⎦\n=\n= H\nδq\nδy\nδy\nδy\nδy\n∂2\n∂2\nz\nz\n∂x∂y\n∂y2\ns\nt\nqx\nqy\nWhere H is the Hessian of second spatial derivatives (x and y) of z. Note that from Fubuni's theorem and from our Taylor\nexpansion from last lecture, py = qx.\nIntuition:\n\n∂z\n- First spatial derivatives\nand ∂z describe the surface orientation of an oject in the image.\n∂x\n∂y\n∂2\n∂2\n∂2\n∂2\nz\nz\nz\nz\n- Second spatial derivatives\n∂y2 , and\n=\ndescribe curvature, or the change in surface orientation.\n∂x2 ,\n∂x∂y\n∂y∂x\n- In 2D, the curvature of a surface is specified by the radius of curvature, or its inverse, curvature.\n- In 3D, however, the curvature of a surface is specified by a Hessian matrix H. This curvature matrix allows us to compute\np and q.\nOne issue with using this Hessian approach to update p and q: how do we update the second derivatives r, s, and t? Can we use\n3rd order derivatives? It turns out that seeking to update lower-order derivatives with higher-order derivatives will just generate\nincreasingly more unknowns, so we will seek alternative routes. Let's try integrating our brightness measurements into what we\nalready have so far:\nImage Irradiance Equation : E(x, y) = R(p, q)\nChain Rule Gives :\n∂E\n∂R ∂p\n∂R ∂q\nEx =\n=\n+\n∂x\n∂p ∂x\n∂q ∂x\n∂R\n∂R\n=\nr +\ns\n∂p\n∂q\n∂E\n∂R ∂p\n∂R ∂q\nEy =\n=\n+\n∂y\n∂p ∂y\n∂q ∂y\n∂R\n∂R\n=\ns +\nt\n∂p\n∂q\nIn matrix-vector form, we have:\n⎡\n⎤\n⎡\n⎤\n∂R\n∂R\n\n∂p\n∂p\nEx\nr\ns ⎢\n⎥\n⎢\n⎥\n=\n⎣\n⎦ = H ⎣\n⎦\nEy\ns\nt\n∂R\n∂R\n∂q\n∂q\nNotice that we have the same Hessian matrix that we had derived for our surface orientation update equation before!\nIntuition: These equations make sense intuitively - brightness will be constant for constant surface orientation in a model\nwhere brightness depends only on surface orientation. Therefore, changes in brightness correspond to changes in surface orien\ntation.\nHow do we solve for this Hessian H? We can:\n- Solve for Ex, Ey numerically from our brightness measurements.\n- Solve for Rp, Rq using our reflectance map.\nHowever, solving for H in this way presents us with 3 unknowns and only 2 equations (undetermined system). We cannot solve\nfor H, but we can still compute δp, δq. We can pattern match δx and δy by using a (δx, δy)T vector in the same direction as the\ngradient of the reflectance map, i.e.\n\nδx\nRp\n=\nξ\nδy\nRq\nWhere the vector on the lefthand side is our step in x and y, our vector on the righthand side is our gradient of the reflectance\nmap in gradient space (p, q), and ξ is the step size. Intuitively, this is the direction where we can \"make progress\". Substituting\nthis equality into our update equation for p and q, we have:\n\nδp\nRp\n= H\nξ\nδq\nRq\n\nEx\n=\nδξ\nEy\nTherefore, we can formally write out a system of 5 first-order ordinary differential equations (ODEs) that generate our charac\nteristic strip as desired:\n\ndx\n1.\n= Rp\ndξ\n2. dy = Rq\ndξ\ndp\n3. dξ = Ex\n4. dq = Ey\ndξ\ndz\n5.\n= pRp + qRq (\"Output Rule\")\ndξ\nThough we take partial derivatives on many of the righthand sides, we can think of these quantities as measurements or derived\nvariations of our measurements, and therefore they do not correspond to partial derivatives that we actually need to solve for.\nThus, this is why we claim this is a system of ODEs, and not PDEs.\nA few notes about this system of ODEs:\n- This system of 5 ODEs explores the surface along the characteristic strip generated by these equations.\n- Algorithmically, we (a) Look at/compute the brightness gradient, which helps us (b) Compute p and q, which (c) Informs\nus of Rp and Rq for computing the change in height z.\n- ODEs 1 & 2 and ODEs 3 & 4 are two systems of equations that each determine the update for the other system (i.e. there\nis dynamic behavior between these two systems). The 5th ODE is in turn updated from the results of updating the other\n4 ODE quantities of interest.\n- The step in the image (x, y) depends on the gradient of the reflectance map in (p, q).\n- Analogously, the step in the reflectance map (p, q) depends on the gradient of the image in (x, y).\n- This system of equations necessitates that we cannot simply optimize with block gradient ascent or descent, but rather a\nprocess in which we dynamically update our variables of interest using our other updated variables of interest.\n- This approach holds generally for any reflectance map R(p, q).\n- We can express our image irradiance equation as a first order, nonlinear PDE:\n\n∂z ∂z\nE(x, y) = R\n,\n∂x ∂y\nNext, we will show how this general approach applied to a Hapke surface reduces to our previous SfS results for this problem.\n1.2.1\nReducing General Form SfS for Hapke Surfaces\nRecall reflectance map for Hapke surfaces:\nr\n1 + psp + qsq\nR(p, q) =\nrs\nTaking derivatives using the system of 5 ODEs defined above, we have from the multivariate chain rule):\ndx\nΔ ∂R\n√1\n1.\n: Rp =\n=\n√\nps\ndξ\n∂p\nrs 2 1+psp+qsq\ndy\nΔ ∂R\n√1\n2.\n: Rq =\n=\n√\nqs\ndξ\n∂q\nrs 2 1+psp+qsq\ndz\npsp+qsq\n3.\n: pRp + qRq = √\n√\ndξ\n2 rs\n1+psp+qs q\nSince the denominator is common in all three of these derivative equations, we can just conceptualize this factor as a speed/step\nsize factor. Therefore, we can simply omit this factor when we update these variables after taking a step. With this, our updates\nbecome:\n1. δx ← ps\n2. δy ← qs\n3. δz ← (psp + qsq) = rsE2 - 1\nWhich are consistent with our prior results using our Hapke surfaces. Next, we will apply this generalized approach to Scanning\nElectron Microscopes (SEMs):\n\n1.2.2\nApplying General Form SfS to SEMs\nFor SEMs, the reflectance map is spherically symmetric around the origin:\nR(p, q) = f(p + q 2) for some f : R → R\ndy\nUsing our system of 5 ODEs derived in the general case, we can compute our updates by first calculating derivatives dx\ndξ , and\ndξ ,\ndz :\ndξ\nΔ\ndx\n∂R\n1.\n: Rp =\n= 2f 0(p + q2)p\ndξ\n∂p\ndy\nΔ ∂R\n2.\n: Rq =\n= 2f 0(p + q2)q\ndξ\n∂q\ndz\n3.\n: pRp + qRq = 2f 0(p + q2)(p + q2)\ndξ\nAgain, here we can also simplify these updates by noting that the term 2f 0(p + q2) is common to all three derivatives, and\ntherefore this factor can also be interpreted as a speed factor that only affects the step size. Our updates then become:\n1. δx ← p\n2. δy ← q\n3. δz ← p + q\nThis tells us we will be taking steps along the brightness gradient. Our solution generates characteristic strips that contain\ninformation about the surface orientation. To continue our analysis of this SfS problem, in addition to defining characteristic\nstrips, it will also be necessary to define the base characteristic.\n1.3\nBase Characteristics\nRecall that the characteristic strip that our system of ODEs solves for is given by a 5-dimensional surface composed of:\n\nT\nx\ny\nz\np\nq\n∈ R5\nAnother important component when discussing our solution to this SfS problem is the base characteristic, which is the\nprojection of the characteristic strip onto the x, y image plane:\n\nT\n\nT\nx(ξ) y(ξ)\n= projection\n{characteristic strip} = projection\n{ x\ny\nz\np\nq\n}\nx,y\nx,y\nA few notes/questions about base characteristics?\n- What are base characteristics? These can be thought of as the profiles in the (x, y) plane that originate (possibly in\nboth directions) from a given point in the initial curve.\n- What are base characteristics used for? These are used to help ensure we explore as much of the image as possible.\nWe can interpolate/average (depending on how dense/sparse a given region of the image is) different base characteristics\nto \"fill out\" the solution over the entire image space.\n- How are base characteristics used in practice? Like the solution profiles we have encountered before, base charac\nteristics are independent of one another - this allows computation over them to be parallelizable. Intuition: These sets\nof base characteristics can be thought of like a wavefront propagating outward from the initial curve. We expect the\nsolutions corresponding to these base characteristics to move at similar \"speeds\".\n1.4\nAnalyzing \"Speed\"\nWhen considering how the speeds of these different solution curves vary, let us consider ways in which we can set constant step\nsizes/speeds. Here are a few, to name:\n1. Constant step size in z: This is effectively stepping \"contour to contour\" on a topographic map.\ndz\nAchieved by: Dividing by pRp + qRq =⇒\n= 1.\ndξ\n2. Constant step size in image: A few issues with this approach. First, curves may run at different rates. Second,\nq\nmethodology fails when\nR2 + R2 = 0.\np\nq\nq\np\nR2\nd\nAchieved by: Dividing by\n+ R2 =⇒\n( δx2 + δy2) = 1.\np\nq\ndξ\n\n3. Constant Step Size in 3D/Object: Runs into issues when Rp = Rq = 0.\nq\np\nAchieved by: Dividing by\nR2 + R2 + (pRp + qRq )2 =⇒\n(δx)2 + (δy)2 + (δz)2 = 1.\np\nq\n4. Constant Step Size in Isophotes: Here, we are effectively taking constant steps in brightness. We will end up dividing\nby the dot product of the brightness gradient and the gradient of the reflectance map in (p, q) space.\n∂R\n∂E ∂R\nAchieved by: Dividing by ( ∂E\n+\n)δξ = ((Ex, Ey ) · (Rp, Rq))δξ =⇒ δE = 1.\n∂x ∂p\n∂y ∂q\n1.5\nGenerating an Initial Curve\nWhile only having to measure a single initial curve for profiling is far better than having to measure an entire surface, it is still\nundesirable and is not a caveat we are satisfied with for this solution. Below, we will discuss how we can generate an initial\ncurve to avoid having to measure one for our SfS problem.\nAs we discussed, it is undesirable to have to measure an initial curve/characteristic strip, a.k.a. measure:\n\nT\nx(ξ) y(ξ) z(ξ) p(ξ) q(ξ)\nBut we do not actually need to measure all of these. Using the image irradiance equation we can calculate ∂z :\n∂η\nE(x, y) = R(p, q)\n∂z\n∂z ∂x\n∂z ∂y\n=\n+\n∂η\n∂x ∂η\n∂y ∂η\n∂x\n∂y\n= p\n+ q\n∂η\n∂η\nWhere p and q in this case are our unknowns. Therefore, in practice, we can get by with just initial curve, and do not need the\nfull initial characteristic strip - i.e. if we have x(η), y(η), and z(η), then we can compute the orientation from the reflectance\nmap using our brightness measurements.\n1.5.1\nUsing Edge Points to Autogenerate an Initial Curve\nAn even more sweeping question: do we even need an initial curve? Are there any special points on the object where we already\nknow the orientation without a measurement? These points are along the edge, or occluding boundary, of our objects of\ninterest. Here, we know the surface normal of each of these points. Could we use these edge points as \"starting points\" for our\nSfS solution?\nIt turns out, unfortunately, that we cannot. This is due to the fact that as we infinisimally approach the edge of the boundary,\n∂z\n∂z\nq\nwe have that\n→inf,\n→inf. Even though the slope\nis known, we cannot use it numerically/iteratively with our step\n∂x\n∂y\np\nupdates above.\n1.5.2\nUsing Stationary Points to Autogenerate an Initial Curve\nLet us investigate another option for helping us to avoid having to generate an initial curve. Consider the brightest (or darkest)\npoint on an image, i.e. the unique, global, isolated extremum. This point is a stationary point in (p, q) space, i.e.\n∂R\n∂R\n=\n= 0, granted that R(p, q) is differentiable.\n∂p\n∂q\nThis would allow us to find the surface orientation (p, q) solely through optimizing the reflectance map function R(p, q). One\nimmediate problem with this stationary point approach is that we will not be able to step x and y, since, from our system of\nODEs, we have that at our stationary points, our derivatives of x and y are zero:\ndx = Rp = 0\ndξ\ndy = Rq = 0\ndξ\nAdditionally, if we have stationary brightness points in image space, we encounter the \"dual\" of this problem. By definition\nstationary points in the image space imply that:\n∂E\n∂E\n=\n= 0\n∂x\n∂y\n\nThis in turn implies that p and q cannot be stepped:\ndp\n∂E\n=\n= 0\ndξ\n∂x\ndq\n∂E\n=\n= 0\ndξ\n∂y\nIntuition: Intuitively, what is happening with these two systems (each of 2 ODEs ((x, y) or p, q), as we saw in our 5 ODE\nsystem above) is that using stationary points in one domain amounts to updates in the other domain going to zero, which in turn\nprevents the other system's quantities of interest from stepping. Since δz depends on δx, δy, δp, δq, and since δx, δy, δp, δq = 0\nwhen we use stationary points as starting points, this means we cannot ascertain any changes in δz along these points.\nNote: The extremum corresponding to a stationary point can be maximum, as it is for Lambertian surfaces, or a mini\nmum, as it is for Scanning Electron Microscopes (SEM).\nHowever, even though we cannot use stationary points themselves as starting points, could we use the area around them?\nIf we stay close enough to the stationary point, we can approximate that these neighboring points have nearly the same surface\norientation.\n- Approach 1: Construct a local tangent plane by extruding a circle in the plane centered at the stationary point with\nradius ε - this means all points in this plane will have the same surface orientation as the stationary point. Note that\nmathematically, a local 2D plane on a 3D surface is equivalent to a 2-manifold [1]. This is good in the sense that we know\nthe surface orientation of all these points already, but not so great in that we have a degenerate system - since all points\nhave the same surface orientation, under this model they will all have the same brightness as well. This prevents us from\nobtaining a unique solution.\n- Approach 2: Rather than constructing a local planar surface, let us take a curved surface with non-constant surface\norientation and therefore, under this model, non-constant brightness.\n1.5.3\nExample\nSuppose we have a reflectance map and surface function given by:\nReflectance Map : R(p, q) = p + q\nSurface Function : z(x, y) = x + y\nThen, we can compute the derivatives of these as:\n∂z\np =\n= 2x\n∂x\n∂z\nq =\n= 4y\n∂y\nE(x, y) = R(p, q) = p + q = 4x 2 + 16y\n∂E\nEx =\n= 8x\n∂x\n∂E\nEy =\n= 32y\n∂y\n∂E\nThe brightness gradient ( ∂E ,\n) = (8x, 32y) = (0, 0) at the origin x = 0, y = 0.\n∂x\n∂y\nCan we use the brightness gradient to estimate local shape? It turns out the answer is no, again because of stationary points.\nBut if we look at the second derivatives of brightness:\n∂2E\n∂E\nExx =\n=\n(8x) = 8\n∂x2\n∂x\n∂2E\n∂E\nEyy =\n=\n(32y) = 32\n∂y2\n∂y\n∂2E\n∂\n∂\nExy =\n=\n(32y) =\n(8y) = 0\n∂x∂y\n∂x\n∂y\n\nThese second derivatives, as we will discuss more in the next lecture, will tell us some information about the object's shape.\nAs we have seen in previous lectures, these second derivatives can be computed by applying computational molecules to our\nbrightness measurements.\nHigh-Level Algorithm\n(NOTE: This will be covered in greater detail next lecture) Let's walk through the steps to ensure we can autogenerate an\ninitial condition curve without the need to measure it:\n1. Compute stationary points of brightness.\n2. Use 2nd derivatives of brightness to estimate local shape.\n3. Construct small cap (non-planar to avoid degeneracy) around the stationary point.\n4. Begin solutions from these points on the edge of the cap surrounding the stationary point.\n1.6\nReferences\n1. Manifold, https://en.wikipedia.org/wiki/Manifold\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 11",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/5b03361b8fe3ccb51a0ce3c685dd88ae_MIT6_801F20_lec11.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 11\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 11: Edge detection, Subpixel Position, CORDIC, Line Detection,\n(US 6,408,109)\nIn this lecture, we will introduce some discussion on how patents work, their stipulations, and make our discussion explicit by\nlooking at a patent case study with sub-pixel accuracy edge detection. You can find the patent document on Stellar as well\nunder \"Materials\" → \"US patent 6,408,109\".\n1.1\nBackground on Patents\nWe will begin with some fun facts about industrial machine vision:\n- Integrated circuit boards cannot be manufactured without machine vision\n- Pharmaceutical chemicals also cannot be manufactured without machine vision\nHow do entrepreneurs and industrial companies ensure their inventions are protected, while still having the chance to disseminate\ntheir findings with society? This is done through patents. Patents:\n- Can be thought of as a \"contract with society\" - you get a limited monopoly on your idea, and in turn, you publish the\ntechnical details of your approach.\n- Can help to reduce litigation and legal fees.\n- Can be used by large companies as \"ammunition\" for \"patent wars\".\nSome \"rules\" of patents:\n- No equations are included in the patent (no longer true)\n- No greyscale images - only black and white\n- Arcane grammar is used for legal purposes - \"comprises\", \"apparatus\", \"method\", etc.\n- References of other patents are often included - sometimes these are added by the patent examiner, rather than the patent\nauthors\n- Most patents end with something along the lines of \"this is why our invention was necessary\" or \"this is the technical gap\nour invention fills\"\n- Software is not patentable - companies and engineers get around this by putting code on hardware and patenting the\n\"apparatus\" housing the code.\n- It is also common to include background information (similar to related literature in research).\nNow that we have had a high-level introduction to patents, let us turn to focus on one that describes an apparatus for sub-pixel\naccuracy edge finding.\n\n1.2\nPatent Case Study: Detecting Sub-Pixel Location of Edges in a Digital Image\nTo put this problem into context, consider the following:\n- Recall that images typically have large regions of uniform/homogeneous intensity\n- Image arrays are very memory-dense. A more sparse way to transfer/convey information about an image containing edges\nis to use the locations of edges as region boundaries of the image. This is one application of edge finding.\n- This patent achieves 1/40th pixel accuracy.\nThis methodology and apparatus seeks to find edges of objects in digital images at a high sub-pixel accuracy level. To do so,\nthe authors leverage different detectors, or kernels. These kernels are similar to some of the computational molecules we have\nalready looked at in this course:\n- Robert's Cross: This approximates derivatives in a coordinate system rotated 45 degrees (x , y0). The derivatives can\nbe approximated using the Kx0 and Ky0 kernels:\n\n∂E\n-1\n→ Kx0 =\n∂x0\n-1\n\n∂E\n→ Ky =\n∂y0\n0 -1\n- Sobel Operator: This computational molecule requires more computation and it is not as high-resolution. It is also more\nrobust to noise than the computational molecules used above:\n⎡\n⎤\n∂E\n∂x\n-1\n⎣\n→ Kx =\n-1\n2⎦\n⎡\n⎤\n-1\n2 -1\n∂E → Ky = ⎣ 0\n0 ⎦\n∂y\n- Silver Operators: This computational molecule is designed for a hexagonal grid. Though these filters have some advan\ntages, unfortunately, they are not compatible with most cameras as very few cameras have a hexagonal pixel structure.\nFigure 1: Silver Operators with a hexagonal grid.\nFor this specific application, we can compute approximate brightness gradients using the filters/operators above, and then we can\nconvert these brightness gradients from Cartesian to polar coordinates to extract brightness gradient magnitude and direction\n(which are all we really need for this system). In the system, this is done using the CORDIC algorithm [1].\n\n1.2.1\nHigh-Level Overview of Edge Detection System\nAt a high level, we can divide the system into the following chronological set of processes/components:\n1. Estimate Brightness Gradient: Given an image, we can estimate the brightness gradient using some of the filters\ndefined above.\n2. Compute Brightness Gradient Magnitude and Direction: Using the CORDIC algorithm, we can estimate the\nbrightness gradient magnitude and direction. The CORDIC algorithm does this iteratively through a corrective feedback\nmechanism (see reference), but computationally, only uses SHIFT, ADD, SUBTRACT, and ABS operations.\n3. Choose Neighbors and Detect Peaks: This is achieved using brightness gradient magnitude and direction and a pro\ncedure called non-maximum suppression [2].\nFirst, using gradient magnitude and direction, we can find edges by looking across the 1D edge (we can search for this edge\nusing the gradient direction Gθ, which invokes Non-Maximum Suppression (NMS). We need to quantize into 8 (Cartesian)\nor 6 (Polar) regions - this is known as coarse direction quantization.\nFinally, we can find a peak by fitting three points with a parabola (note this has three DOF). This approach will end up\ngiving us accuracy up to 1/10th of a pixel. To go further, we must look at the assumptions of gradient variation with\nposition, as well as:\n- Camera optics\n- Fill factor of the chip sensor\n- How in-focus the image is\n- How smooth the edge transition is\nThe authors of this patent claim that edge detection performance is improved using an optimal value of \"s\" (achieved through\ninterpolation and bias correction), which we will see later. For clarity, the full system diagram is here:\nFigure 2: Aggregate edge detection system. The steps listed in the boxes correspond to the steps outlined in the procedure\nabove.\nSome formulas for this system:\nq\n1. G0 =\nG2 + G2 (gradient estimation)\nx\ny\n\n-1\nGy\n2. Gθ = tan\nGx\n(gradient estimation)\n3. R0 = max(|Gx|, |Gy|) (octant quantization)\n4. S0 = min(|Gx|, |Gy |) (octant quantization)\nAt a high level, the apparatus discussed in this patent is composed of:\n\n- Gradient estimator\n- Peak detector\n- Sub-pixel interpolator\nNext, let us dive in more to the general edge detection problem.\n1.3\nEdges & Edge Detection\nLet us begin by precisely defining what we mean by edges and edge detection:\n- Edge: A point in an image where the image brightness gradient reaches a local maximum in the image brightness gradient\ndirection. Additionally, an edge is where the second derivative of image brightness (can also be thought of as the gradient\nof the image brightness gradient) crosses zero. We can look at finding the zeros of these 2nd derivatives as a means to\ncompute edges.\n- Edge Detection: A process through which we can determine the location of boundaries between image regions that are\nroughly uniform in brightness.\n1.3.1\nFinding a Suitable Brightness Function for Edge Detection\nLet us first approximate an edge brightness function using a step function, given by u(x), such that:\n(\n1,\nx ≥ 0\nu(x) =\n0, x < 0\nu(x)\n-2\n-1\nx\nu(x)\nUsing this cross-section across the edge to model the edge actually causes problems arising from aliasing: since we seek to find\nthe location of an edge in a discrete, and therefore, sampled image, and since the edge in the case of u(x) is infinitely thin, we\nwill not be able to find it due to sampling. In Fourier terms, if we use a perfect step function, we introduce artificially high\n(infinite) frequencies that prevent us from sampling without aliasing effects. Let us instead try a \"soft\" step function, i.e. a\n\"sigmoid\" function: σ(x) =\n. Then our u(x) takes the form:\n-x\n1+e\nA \"Soft\" Unit Step Function, u(x)\n0.8\n0.6\n-6\n-4\n-2\n0.4\n0.2\nx\nThe gradient of this brightness across the edge, given by ru(x) (or du in one dimension), is then given by the following. Notice\ndx\nthat the location of the maximum matches the inflection point in the graph above:\n\nGradient of \"Soft\" Unit Step Function, ru(x)\nru(x)\n0.25\n0.2\n0.15\n0.1\n-6\n-4\n-2\n5 · 10-2\nx\nAs we mentioned above, we can find the location of this edge by looking at where the second derivative of brightness crosses\nzero, a.k.a. where r(ru(x)) = r u(x) = 0. Notice that the location of this zero is given by the same location as the inflection\npoint of u(x) and the maximum of ru(x):\nGradient2 of \"Soft\" Unit Step Function, ru(x)\n0.1\n5 · 10-2\n-6\n-4\n-2\nu(x)\nr\n-5 · 10-2\n-0.1\nx\nFor those curious, here is the math behind this specific function, assuming a sigmoid for u(x):\n1. u(x) = 1+exp (-x)\n\ndu\nd\nexp(-x)\n2. ru(x) =\n=\n=\ndx\ndx\n1+exp -x\n(1+exp(-x))2\nd2 u\nd\nexp(-x)\n- exp(-x)(1+exp(-x))2+2 exp(-x)(1+exp(-x)) exp(-x)\n3. r u(x) =\n=\n(\n) =\ndx2\ndx (1+exp(-x))2\n(1+exp(-x))4\nBuilding on top of this framework above, let us now move on to brightness gradient estimation.\n1.3.2\nBrightness Gradient Estimation\nThis component of this studied edge detection framework estimates the magnitude and direction of the brightness gradient. Let\nus look at how this is derived for different filters:\nRobert's Cross Gradient: Since this estimates derivatives at 45 degree angles, the pixels are effectively further apart, and\nthis means there will be a constant of proportionality difference between the magnitude of the gradient estimated here and with\na normal (x, y) coordinate system:\nq\nq\nE2 + E2\nE2 + E2\nx0\ny0 ∝\nx\ny\n\nNext, we will look at the Sobel operator. For this analysis, it will be helpful to recall the following result from Taylor Series:\ninf\nX\n(δx)2\n(δx)3\n(δx)4\n(δx)if (i)(x)\nΔ\nf(x + δx) = f(x) + δxf 0(x) +\nf 00(x) +\nf 000(x) +\nf (4)(x) + ... =\n, where 0! = 1\n2!\n3!\ni!\ni=0\nLet us first consider the simple two-pixel difference operator (in the x-direction/in the one-dimensional case),\ndE\ni.e.\n→ Kx =\n(-1 1). Let us look at the forward difference and backward difference when this operator is applied:\ndx\nδ\n∂E ≈ f (x+δx)-f (x)\n(δx)2\n1. Forward Difference:\n= f 0(x) + δx f 00(x) +\nf 000(x) + ...\n∂x\nδx\n∂E ≈ f(x)-f(x-δx) = -f 0(x) - δx\n(δx)2\n2. Backward Difference:\nf 00(x) +\nf 000(x) + ...\n∂x\nδx\nNotice that for both of these, if f 00(x) is large, i.e. if f(x) is nonlinear, then we will have second-order error terms that appear in\nour estimates. In general, we want to aim for removing these lower-order error terms. If we average the forward and backward\ndifferences, however, we can see that these second-order error terms disappear:\nf (x+δx)-f (x)\nf (x)-f(x-δx)\n+\n(δx)2\nδx\nδx\n= f 0(x) +\nf 000(x) + ...\nNow we have increased the error term to 3rd order, rather than 2nd order! As a computational molecule, this higher-order filter\nSobel operator looks like dE → Kx =\n(-1 0 1). But we can do even better! So long as we do not need to have a pixel at our\ndx\n2δ\nδ\nproposed edge, we can use a filter of three elements spanning (x - δ x x + ). There is no pixel at x but we can still compute\nδ\nthe derivative here. This yields an error that is 0.25 the error above due to the fact that our pixels are\napart, as opposed to δ\napart:\n( xδ )2\nf 000(x)\nerror =\nThis makes sense intuitively, because the closer together a set of gradient estimates are, the more accurate they will be. We\ncan incorporate y into the picture, making this amenable for two-dimensional methods as desired, by simply taking the center\nof four pixels, given for each dimension as:\n\n∂E\n-1\n≈ Kx =\n∂x\n2δx -1\n\n∂E\n-1 -1\n≈ Ky =\n∂y\n2δy\nThe proposed edge is in the middle of both of these kernels, as shown below:\nFigure 3: We can estimate the brightness gradient with minimal error by estimating it at the point at the center of these 2D\nfilters.\nEstimating these individually in each dimension requires 3 operations each for a total of 6 operations, but if we are able to\ntake the common operations from each and combine them either by addition or subtraction, this only requires 4 operations.\nHelpful especially for images with lots of pixels.\nNext, we will discuss the 3-by-3 Sobel operator. We can think of this Sobel operator (in each dimension) as being the dis\ncrete convolution of a 2-by-2 horizontal or vertical highpass/edge filer with a smoothing or averaging filter:\n⎡\n⎤\n\n-1\n-1\n⎣\n⎦\n1. x-direction:\n∗\n= -2\n2δx -1\n-1\n⎡\n⎤\n\n-1 -2 -1\n-1 -1\n2. y-direction:\n∗\n= ⎣ 0\n0 ⎦\n2δy\n\nA few notes about the derivation above:\n- The convolution used is a \"padded convolution\" [3], in which, when implemented, when the elements of the filter/kernel\n(in this case, the averaging kernel) are not aligned with the image, they are simply multiplied by zero. Zero padding is the\nmost common padding technique, but there are other techniques as well, such as wraparound padding.\n- This approach avoids the half-pixel (in which we estimate an edge that is not on a pixel) that was cited above.\n- Smoothing/averaging is a double edge sword, because while it can reduce/remove high-frequency noise by filtering, it can\nalso introduce undesirable blurring.\nNext, we will look at how the brightness gradient is converted from Cartesian to Polar coordinates:\n(Ex, Ey ) → (E0, Eθ)\nq\nE0 =\nEx\n2 + Ey 2\n\nEy\nEθ = tan-1\nEy\nFinally, we conclude this lecture by looking at appropriate values of s for quadratic and triangular functions. This assumes we\nhave three gradient measurements centered on G0: (1) G-, (2) G0, and (3) G+. Let us look at the results for these two types\nof functions:\nG+-G-\n1. Quadratic: s =\n, this results in s ∈ [- 1 , ].\n4(G0- 1 (G+-G-))\nG+-G-\n2. Triangular: s = 2(G0-min(G+ ,G-))\nA few notes about these approaches:\n- Note that in each case, we only want to keep if the magnitude G0 is a local maximum, i.e. G0 > G+ and G0 ≥ G-.\n- In the quadratic case, we can parameterize the curve with three data points using three degrees of freedom, i.e. ax +bx+c =\n0. With this approach, b ≈ first derivative and a ≈ second derivative.\n1.4\nReferences\n1. CORDIC Algorithm, https://www.allaboutcircuits.com/technical-articles/an-introduction-to-the-cordic-algorithm/\n2. Non-Maximum Supression, http://justin-liang.com/tutorials/canny/#suppression\n3. Padded Convolution, https://medium.com/@ayeshmanthaperera/what-is-padding-in-cnns-71b21fb0dd7\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 12",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/5e90d5693d5d378d3f19bf67913295aa_MIT6_801F20_lec12.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 12\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 12: Blob Analysis, Binary Image Processing, Use of Green's Theo\nrem, Derivative and Integral as Convolutions\nIn this lecture, we will continue our discussion of intellectual property, and how it relevant for all scientists and engineers. We\nwill then elaborate on some of the specific machine vision techniques that were used in this patent, as well as introduce some\npossible extensions that could be applicable for this patent as well.\n1.1\nTypes of Intellectual Property\nThough it is not related to the technical content of machine vision, being familiar with different types and degrees of intellectual\nproperty (IP) is crucial to professional success. Below, we discuss some of these different types of intellectual property.\n- Patents: One major type of these is utility and design patents. In these, the authors are required to disclose the \"best\nmode\" of performance. For convenience, here are some notes on patents from our previous lecture:\n- Can be thought of as a \"contract with society\" - you get a limited monopoly on your idea, and in turn, you publish\nthe technical details of your approach.\n- Can help to reduce litigation and legal fees.\n- Can be used by large companies as \"ammunition\" for \"patent wars\".\nSome \"rules\" of patents:\n- No equations are included in the patent (no longer true)\n- No greyscale images - only black and white\n- Arcane grammar is used for legal purposes - \"comprises\", \"apparatus\", \"method\", etc.\n- References of other patents are often included - sometimes these are added by the patent examiner, rather than the\npatent authors\n- Most patents end with something along the lines of \"this is why our invention was necessary\" or \"this is the technical\ngap our invention fills\"\n- Software is not patentable - companies and engineers get around this by putting code on hardware and patenting the\n\"apparatus\" housing the code.\n- It is also common to include background information (similar to related literature in research).\n- Copyright:\n- Books, song recordings, choreographs\n- Exceptions: presenting (fractional pieces of) information from another author\n- Trademarks:\n- Must be unique for your field (e.g. Apple vs. Apple).\n\n- Cannot use common words - this is actually one reason why many companies have slightly misspelled combinations\nof common words.\n- Can use pictures, character distortions, and color as part of the trademark.\n- No issues if in different fields.\n- Trade Secret\n- No protections, but not spilled, lasts forever\n- Can enforce legal recourse with Non-Disclosure Agreement (NDA)\n1.2\nEdge Detection Patent Methodologies\nu(x)\nFor this next section, we will direct our attention toward covering concepts that were discussed in the edge detection patent\n(6,408,109). Each of these sections will be discussed in further detail below. Before we get into the specifics of the patent again,\nit is important to point out the importance of edge detection for higher-level machine vision tasks, such as:\n- Attitude (pose estimation) of an object\n- Object recognition\n- Determining to position\nWe will touch more on these topics in later lectures.\n1.2.1\nFinding Edge with Derivatives\nRecall that we find a proposed edge by finding an inflection point of the brightness E(x, y). The following methods for finding\nthis point are equivalent:\n- Finding an inflection point of brightness E(x, y).\n- Finding maximum of brightness gradient magnitude/first derivative |rE(x, y)|.\n- Finding zero crossing of Laplacian/second derivative r2E(x, y).\nFor building intuition, last time we used the following example of u(x) = σ(x) =\n:\n1+exp(-x)\nA \"Soft\" Unit Step Function, u(x)\n0.8\n0.6\n-6\n-4\n-2\n0.4\n0.2\nx\nThe gradient of this brightness across the edge, given by ru(x) (or du in one dimension), is then given by the following. Notice\ndx\nthat the location of the maximum matches the inflection point in the graph above:\n\nGradient of \"Soft\" Unit Step Function, ru(x)\n0.25\n0.2\n0.15\n0.1\n-6\n-4\n-2\nru(x)\n5 · 10-2\nx\nAs we mentioned above, we can find the location of this edge by looking at where the second derivative of brightness crosses\nzero, a.k.a. where r(ru(x)) = r u(x) = 0. Notice that the location of this zero is given by the same location as the inflection\npoint of u(x) and the maximum of ru(x):\nGradient2 of \"Soft\" Unit Step Function, ru(x)\n0.1\n5 · 10-2\n-6\n-4\n-2\nu(x)\nr\n-5 · 10-2\n-0.1\nx\nFor those curious, here is the math behind this specific function, assuming a sigmoid for u(x):\n1. u(x) = 1+exp (-x)\n\ndu\nd\nexp(-x)\n2. ru(x) =\n=\n=\ndx\ndx\n1+exp -x\n(1+exp(-x))2\nd2\nexp(-x)\n- exp(-x)(1+exp(-x))2+2 exp(-x)(1+exp(-x)) exp(-x)\n3. r2u(x) =\nu = d (\n) =\ndx2\ndx (1+exp(-x))2\n(1+exp(-x))4\n1.2.2\nMore on \"Stencils\"/Computational Molecules\nRecall that we can use finite differences [1] in the forms of \"stencils\" or computational molecules to estimate derivatives in our\nimages. For this patent, the authors used this framework to estimate the brightness gradient in order to find edges. For instance,\npartial derivative of brightness w.r.t. x can be estimated by\n\n1. Ex = ε -1\n\n2. Ex = 2ε -1\n\n-1\n3. Ex = 2ε -1\n\nWhere for molecule 2, the best point for estimating derivatives lies directly in the center pixel, and for molecules 1 and 3, the\nbest point for estimating derivatives lies halfway between the two pixels.\nHow do we analyze the efficacy of this approach?\n1. Taylor Series: From previous lectures we saw that we could use averaging to reduce the error terms from 2nd order\nderivatives to third order derivatives. This is useful for analytically determining the error.\n2. Test functions: We will touch more on these later, but these are helpful for testing your derivative estimates using\nanalytical expressions, such as polynomial functions.\n3. Fourier domain: This type of analysis is helpful for understanding how these \"stencils\"/molecules affect higher (spatial)\nfrequency image content.\nNote that derivative estimators can become quite complicated for high-precision estimates of the derivative, even for low-order\nderivatives. We can use large estimators over many pixels, but we should be mindful of the following tradeoffs:\n- We will achieve better noise smoothing/suppression by including more measured values.\n- Larger derivative estimators linearly (1D)/quadratically (2D) increase the amount of computation time needed.\n- Features can also affect each other - e.g. a large edge detection estimator means that we can have two nearby edges affecting\neach other.\nWe can also look at some derivative estimators for higher-order derivatives. For 2nd-order derivatives, we just apply another\nderivative operator, which is equivalent to convolution of another derivative estimator \"molecule\":\n\n∂2\n∂\n∂(·)\n\n(·) =\n⇐⇒\n-1\n1 ⊗\n-1\n1 =\n1 -2\n∂x2\n∂x\n∂x\nε\nε\nε2\nFor deriving the sign here and understanding why we have symmetry, remember that convolution \"flips\" one of the two filters/\noperators!\nSanity Check: Let us apply this to some functions we already know the 2nd derivative of.\n- f(x) = x :\nf(x) = x\nf 0(x) = 2x\nf 00(x) = 2\nApplying the 2nd derivative estimator above to this function:\n\n1 -2\n1 ⊗\nf(-1) = 1 f(0) = 0 f(1) = 1 =\n((1 ∗ 1) + (-2 ∗ 0) + (1 ∗ 1)) =\n(1 + 0 + 1) = 1 ∗ 2 = 2\nε\nε2\nε2\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f 00(x) = 2.\n- f(x) = x:\nf(x) = x\nf 0(x) = 1\nf 00(x) = 0\nApplying the 2nd derivative estimator above to this function:\n\n1 -2\n1 ⊗\nf(-1) = -1 f(0) = 0 f(1) = 1 =\n((1 ∗-1) + (-2 ∗ 0) + (1 ∗ 1)) =\n(-1 + 0 + 1) = 0\nε\nε2\nε2\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f 00(x) = 0.\n\n- f(x) = 1:\nf(x) = 1\nf 0(x) = 0\nf 00(x) = 0\nApplying the 2nd derivative estimator above to this function:\n\n1 -2\n1 ⊗\nf(-1) = 1 f(0) = 1 f(1) = 1 =\n((1 ∗ 1) + (-2 ∗ 1) + (1 ∗ 1)) =\n(1 + -2 + 1) = 0\nε\nε2\nε2\nWhere we note that ε = 1 due to the pixel spacing. This is equivalent to f 00(x) = 0.\nIn Practice: As demonstrated in the example \"test functions\" above, in general a good way to test an Nth order derivative\nestimator is use polynomial test functions of arbitrary coefficients from order 0 up to order N. For instance, to calculate 4th\norder derivative estimator, test:\n1. f(x) = a\n2. f(x) = ax + b\n3. f(x) = ax + bx + c\n4. f(x) = ax + bx2 + cx + d\n5. f(x) = ax + bx3 + cx + dx + e\nNote: For derivative estimator operators, the weights of the \"stencils\"/computational molecules should add up to zero. Now\nthat we have looked at some of these operators and modes of analysis in one dimension, let us now look at 2 dimensions.\n1.2.3\nMixed Partial Derivatives in 2D\nFirst, it is important to look at the linear, shift-invariant property of these operators, which we can express for each quality:\n- Shift-Invariant:\nd (f(x + δ)) = f 0(x + δ), for some δ ∈ R\ndx\nDerivative of shifted function = Derivative equivalently shifted by same amount\n- Linear :\nd (af1(x) + bf2(x)) = af1\n0 (x) + bf2\n0 (x) for some a, b ∈ R\ndx\nDerivative of scaled sum of two functions = Scaled sum of derivatives of both functions\nWe will exploit this linear, shift-invariant property frequently in machine vision. Because of this joint property, we can treat\nderivative operators as convolutions in 2D:\n\n∂2\n∂\n∂\n\n1 -1 +1\n(·) =\n(·) ⇐⇒\n-1\n1 ⊗\n=\n∂x∂y\n∂x ∂y\nε\nε -1\nε2 +1 -1\nA few notes here:\n- The second operator corresponding to Ey has been flipped in accordance with the convolution operator.\n- If we project this derivative onto a \"diagonal view\", we find that it is simply the second derivative of x , where x is x\n√\n√\nrotated 45 degrees counterclockwise in the 2D plane: x = x cos 45+y cos 45 =\nx+\ny. In other words, in this 45-degree\nrotated coordinate system, Ex x = Exy .\n- Intuition for convolution: If convolution is a new concept for you, check out reference [2] here. Visually, convolution\nis equivalent to \"flipping and sliding\" one operator across all possible (complete and partial) overlapping configurations of\nthe filters with one another.\n\n1.2.4\nLaplacian Estimators in 2D\nΔ\n∂2\n∂2\nThe Laplacian r = Δ =\n+\nis another important estimator in machine vision, and, as we discussed last lecture, is the\n∂x2\n∂y2\nlowest-order rotationally-symmetric derivative operator. Therefore, our finite difference/computational molecule estimates\nshould reflect this property if they are to be accurate. Two candidate estimators of this operator are:\n⎡\n⎤\n1. \"Direct Edge\": ε\n2 ⎣1 -4\n1⎦\n⎡\n⎤\n2. \"Indirect Edge\":\n⎣0 -4\n0⎦\n2ε2\n√\nNote that the second operator has a factor of 2ε2 in front of it because the distance between edges is\n2 rather than 1, therefore,\n√\nwe effectively have ε02 , where ε0 =\n2ε.\nHow do we know which of these approximations is better? We can go back to our analysis tools:\n- Taylor Series\n- Test functions\n- Fourier analysis\nIntuitively, we know that neither of these estimators will be optimal, because neither of these estimators are rotationally-\nsymmetric. Let us combine these intelligently to achieve rotational symmetry. Adding four times the first one with one times\nthe second:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n\n⎣1\nε2\n-4\n\n1⎦ + 1\n⎣0\n2ε2\n-4\n0⎦ =\n⎣4\n6ε2\n-20\n4⎦\nUsing Taylor Series, we can show that this estimator derived from this linear combination of estimators above results in an error\nterm that is one derivative higher than suing either of the individual estimators above, at the cost of more computation. Note\nthat the sum of all the entries here is zero, as we expect for derivative estimators.\nFor a hexagonal grid, this is scaled by 2ε2 and has entries of all 1s on the outer ring, and an entry of -6 in the center. An\nexample application of a hexagonal grid - imaging black holes! Leads to\ngreater efficiency.\nπ\nAs food for thought, what lowest-order rotationally-symmetric nonlinear operators?\nq\nq\nE2 + E2 =\nE2 + E2 Where this is the l2 normal of the estimated brightness gradient\nx0\ny0\nx\ny\n1.2.5\nNon-Maximum Suppression\nAnother technique leveraged in this patent was Non-Maximum Suppression (NMS). Idea: Apply edge detector estimator\noperator everywhere - we will get a small response in most places, so what if we just threshold? This is an instance of early\ndecision-making, because once we take out these points, they are no longer considered edge candidates in downstream steps.\nIt turns out the authors discourage thresholding, and in their work they remove all but the maximum estimated gradient\n(note that this is quantized at the octant level). Note that the quantized gradient direction is perpendicular to the edge. In this\ncase, for a candidate gradient point G0 and the adjacent pixels G- and G+, we must have:\nG0 > G-, G0 ≥ G+\nThis forces - 1 ≤ s ≤ 1 . Note that we have the asymmetric inequality signs to break ties arbitrarily. Next we plot the quantized\nprofile that has been interpolated parabolically - i.e. sub-pixel interpolation.\n\n1.2.6\nPlane Position\nNote that we have not yet done any thresholding. How can we improve this, given that we quantized the edge gradient direction?\nCould we try not quantizing the edge direction? If we have the true gradient direction, we can find the intersection of this line\nwith the edge (at 90 degrees to the edge gradient) to find a better solution.\nTo find this point above (please take a look at the handwritten lecture notes for this lecture), we project from the quantized\ngradient direction to the actual gradient direction. This is the \"plane position\" component.\n1.2.7\nBias Compensation\nAnother component of the patent focuses on the interpolation technique used for sub-pixel gradient plotting for peak finding.\nTo find an optimal interpolation technique, we can plot s vs. s , where s = s|2s|b , where b ∈ N is a parameter that determines\nthe relationship between s and s0 .\nIn addition to cubic interpolation, we can also consider piecewise linear interpolation with \"triangle\" functions. For some\ndifferent values of b:\n- b = 0 → s0 = s\n- b = 1 → s = 2 sign(s)s\n- b = 2 → s = 4 sign(s)s\nWhere different interpolation methods give us different values of b.\n1.2.8\nEdge Transition and Defocusing Compensation\nAnother point of motivation: most edge detection results depend on the actual edge transition. Why are edges fuzzy (note that\nsome degree of fuzziness is needed to prevent aliasing)? One major cause of fuzziness is \"defocusing\", in which the image plane\nand \"in focus\" planes are slightly off from one another. This causes a \"pillbox\" of radius R to be imaged (see handwritten lecture\nnotes), rather than the ideal case of an impulse function δ(x, y). This radius is determined by:\nd2\nR = δ\n(Point Spread Function (PSF))\nf\nThis pillbox image is given mathematically by:\n1 (1 - u(r - R))\nπR2\nWhere u(·) is the unit step function. Where f is the focal length of the lens, d is the diameter of the lens (assumed to be conic),\nand δ is the distance along the optical axis between the actual image plane and the \"in focus\" plane.\n1.3\nMultiscale\nNote: We will discuss this in greater detail next lecture.\nMultiscale is quite important in edge detection, because we can have edges at different scales. To draw contrasting exam\nples, we could have an image such that:\n- We have very sharp edges that transition over ≈ only 1 pixel\n- We have blurry edges that transition over many pixels\n1.3.1\nEffect on Image Edge\nHere is one possible extension not included in the edge detection patent.\nWe can slide a circle across a binary image - the overlapping regions inside the circle between the 1-0 edge controls how\nbright things appear. We can use this technique to see how accurately the algorithm plots the edge position - this allows for\n\nerror calculation since we have ground truth results that we can compute using the area of the circle. Our area of interest is\ngiven by the area enclosed by the chord whose radial points intersect with the binary edge:\n√\n2 R2 - X2X\nA = R2θ -\n√\n\nR2 - x2\nθ = arctan\nx\nAnother way to analyze this is to compute the analytical derivatives of this brightness function:\n√\n∂E\n1. ∂x = 2 R2 - x2\n∂2E\n-2x\n2.\n= √\n∂x2\nR2-x2\nWhat can we do with this? We can use this as input into our algorithm to compute teh error and compensate for the degree\nof defocusing of the lens. In practice, there are other factors that lead to fuzzy edge profiles aside from defocusing, but this\ndefocusing compensation helps.\n1.3.2\nAddressing Quantization of Gradient Directions\nHere is another possible extension not included in the edge detection patent.\n√\nRecall that because spaces occurs in two sizes (pixel spacing and\n2 pixel spacing), we need to sample in two ways, which\ncan lead to slightly different error contributions. We do not want quantized gradient directions. To do this, we can just inter\npolate values G-, G0, G+ along the true edge gradient!\nLinear 1D Interpolation:\nf(a)(b - x) + f(b)(x - a)\nf (x) =\nb - a\nWe can also leverage more sophisticated interpolation methods, such as cubic spline.\nWhy did the authors not leverage this interpolation strategy?\n√\n- this requires the spacing of any level, i.e. not just pixel and\n2 pixel spacing, but everything in between.\n- Since you interpolate, you are not using measured values. Introduces some uncertainty that may be too much to achieve\n1/40th pixel accuracy.\nWhat can we do to address this? → Project gradient onto unit circle! This requires 2D interpolation, which can be done\nwith methods such as bilinear or bicubic interpolation.\n1.3.3\nCORDIC\nAs we discussed in the previous lecture, CORDIC is an algorithm used to estimate vector direction by iteratively rotating a\nvector into a correct angle. For this patent, we are interested in using CORDIC to perform a change of coordinates from cartesian\nto polar:\n(Ex, Ey ) → (E0, Eθ)\nIdea: Rotate a coordinate system to make estimates using test angles iteratively. Note that we can simply compute these with\nsquare roots and arc tangents, but these can be prohibitively computationally-expensive:\nq\nE0 =\nEx\n2 + Ey 2\n\nEy\nEθ = arctan Ex\nRather than computing these directly, it is faster to iteratively solve for the desired rotation θ by taking a sequence of iterative\nrotations {θi}n\n. The iterative updates we have for this are, in matrix-vector form:\ni=1,2,···\n\"\n#\n#\n\n\"\n(i+1)\n(i)\nEx\ncos θi\nsin θi\nEx\n=\n(i+1)\n(i)\n- sin θi\ncos θi\nEy\nEy\nGradients at next step = Rotation R by θi × Gradients at current step\n\nHow do we select {θi}n\n? We can select progressively smaller angles. We can accept the candidate angle and invoke the\ni=1,2,···\niterative update above if each time the candidate angle reduces |Ey| and increases |Ex|.\nP\nThe aggregate rotation θ is simply the sum of all these accepted angles: θ =\nθi\ni\nOne potential practical issue with this approach is that it involves a significant number of multiplications. How can we avoid\nπ\nπ\nthis? We can pick the angles carefully - i.e. if our angles are given successively by π ,\n,\n, ..., then:\n\nsin θu\n2-i\n=\n→rotation matrix becomes :\ncos θi\n2i\ncos θi -2-i\nNote that this reduces computation to 2 additions per iteration. Angle we turn through becomes successively smaller:\nr\nr\nY\nY\ncos θi =\n1 +\n→R =\ncos θi =\n1 +\n≈ 1.16 (precomputed)\n22i\n22i\ni\ni\n1.4\nReferences\n1. Finite Differences, https://en.wikipedia.org/wiki/Finite difference\n_________________________________________\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 13",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/63bf90743360a41d70eda346816d8304_MIT6_801F20_lec13.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 13\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 13: Object Detection, Recognition and Pose Determination,\nPatQuick (US 7,016,539)\nIn this lecture, we will begin by looking at some general problems for object detection and pose estimation of objects in an image,\nand also look at some optimization algorithms we can use for finding optimal matches between a \"master image\" / template\nimage, which is the object we look for, and this object in another image (perhaps in a scene). We then look at a patent describing\nsome of the approaches taken to solve some of these aforementioned tasks.\n1.1\nMotivation & Preliminaries for Object Detection/Pose Estimation\nObject detection and pose estimation builds on top of previous machine vision tasks we have covered. Some specific tasks include:\n- Object detection - i.e. detect and locate object in an image\n- Recognize object\n- Determine pose of detected/recognized object\n- Inspect object\nMotivation for these approaches: In machine vision problems, we often manipulate objects in the world, and we want to\nknow what and where these objects are in the world. In the case of these specific problems, we assume prior knowledge of the\nprecise edge points of these objects (which, as we discussed in the two previous lectures, we know how to compute!)\n1.1.1\n\"Blob Analysis\"/\"Binary Image Processing\"\nWe can use thresholding and other algorithms such as finding a convex hull to compute elements of an object in a binary image\n(black/white) such as:\n- Area (moment order 0)\n- Perimeter\n- Centroid (moment order 1)\n- \"Shape\" (generalization of different-order moments)\n- Euler number - In this case this is the number of blobs minus number of holes\nA few notes about computing these different quantities of interest:\n- We have seen from previous lectures that we can compute some of these elements, such as perimeter, using Green's\nTheorem. We can also accomplish this with counting - can be done simply by counting pixels based off of whether their\npixel is a 0 or 1.\n\n- However, the issue with these approaches is that they require thresholding - i.e. removing points from any further\nconsideration early on the process and possibly without all information available; essentially removing potentially viable\npoints too early.\n- Shape: As introduced above, shape is often computed by computing moments of any order. Recall the definition of\nmoments of a 2D shape:\nRR\n1. O-order:\nE(x, y)dxdy → Area\nRR D\n2. 1-order:\nE(x, y)xydxdy → Centroid\nRRD\n3. 2-order:\nE(x, y)x y2dxdy → Dispersion\nD\n. . .\nRR\nk\n4. k-order:\nE(x, y)x ykdxdy\nD\n- Note that these methods are oftentimes applied to processed, not raw images.\n1.1.2\nBinary Template\nWe will discuss this more later, but this is crucial for the patent on object detection and pose estimation that we will be discussing\ntoday. A binary template is:\n- A \"master image\" to define the object of interest that we are trying to detect/estimate the pose of\n- You will have a template of an object that you can recognize the object and get the pose/attitude.\n1.1.3\nNormalized Correlation\nThis methodology also plays a key role in the patent below.\nIdea: Try all possible positions/configurations of the pose space to create a match between the template and runtime im\nage of the object. If we are interested in the squared distance between the displaced template and the image in the other\nobject (for computational and analytic simplicity, let us only consider rotation for now), then we have the following optimization\nproblem:\nZZ\nmin\n(E1(x - δx, y - δy) - E2(x, y))2dxdy\nδx,δy\nD\nWhere we have denoted the two images separately as E1 and E2.\nIn addition to framing this optimization mathematically as minimizing the squared distance between the two images, we can\nalso conceptualize this as maximizing the correlation between the displaced image and the other image:\nZZ\nmax\nE1(x - δx, y - δy )E2(x, y)dxdy\nδx,δy\nD\nWe can prove mathematically that the two are equivalent. Writing out the first objective as J(δx, δy) and expanding it:\nZZ\nJ(δx, δy) =\n(E(x - δx, y - δy) - E2(x, y))2dxdy\nZZD\nZZ\nZZ\n=\n(E1\n2(x - δx, y - δy) - 2\nE1(x - δx, y - δy)E2(x, y)dxdy +\nE2\n2(x, y)dxdy\nD\nD\nD\nZZ\n=⇒ arg min J(δx, δy) = arg max\nE1(x - δx, y - δy )E2(x, y)dxdy\nδx,δy\nδx,δy\nD\nSince the first and third terms are constant, and since we are minimizing the negative of a scaled correlation objective, this is\nequivalent to maximizing the correlation of the second objective.\nWe can also relate this to some of the other gradient-based optimization methods we have seen using Taylor Series. Suppose\nδx, δy are small. Then the Taylor Series Expansion of first objective gives:\nZZ\nZZ\n∂E1\n∂E1\n(E1(x - δx, y - δy) - E2(x, y))2dxdy =\n(E1(x, y) - δx\n-\n+ · · · - E2(x, y))2dxdy\n∂x\n∂y\nD\nD\n\nIf we now consider that we are looking between consecutive frames with time period δt, then the optimization problem becomes\n∂E\n(after simplifying out E1(x, y) - E2(x, y) = -δt\n):\n∂t\nZZ\nmin\n(-δxEx - δyEy - δtEt)2dxdy\nδx,δy\nD\nDividing through by δt and taking limδt →0 gives:\nZZ\nmin\n(uEx + vEy + Et)2dxdy\nδx,δy\nD\nA few notes about the methods here and the ones above as well:\n- Note that the term under the square directly above looks similar to our BCCE constraint from optical flow!\n- Gradient-based methods are cheaper to compute but only function well for small deviations δx, δy .\n- Correlation methods are advantageous over least-squares methods when we have scaling between the images (e.g. due to\noptical setting differences): E1(x, y) = kE2(x, y) for some k ∈ R .\nAnother question that comes up from this: How can we match at different contrast levels? We can do so with normalized\ncorrelation. Below, we discuss each of the elements we account for and the associated mathematical transformations:\n1. Offset: We account for this by subtracting the mean from each brightness function:\nRR\nE1(x, y)dxdy\nE1\n0 (x, y) = E1(x, y) - E 1, E 1 =\nD RR\ndxdy\nRR\nD\nE2(x, y)dxdy\nD\n\nE2\n0 (x, y) = E2(x, y) - E2, E2 =\nRR\ndxdy\nD\nThis removes offset from images that could be caused by changes to optical setup.\n2. Contrast: We account for this by computing normalized correlation, which in this case is the Pearson correlation coefficient:\nRR\nE1(x - δx, y - δy )E2(x, y)dxdy\nD\nqRR\n∈ [-1, 1]\nqRR\nE (x - δx, y - δy)dxdy\nE (x, y)dxdy\nD\nD\nWhere a correlation coefficient of 1 denotes a perfect match, and a correlation coefficient of -1 denotes a perfectly imperfect\nmatch.\nAre there any issues with this approach? If parts/whole images of objects are obscured, this will greatly affect correlation\ncomputations at these points, even with proper normalization and offsetting.\nWith these preliminaries set up, we are now ready to move into a case study: a patent for object detection and pose esti\nmation using probe points and template images.\n1.2\nPatent 7,016,539: Method for Fast, Robust, Multidimensional Pattern Recognition\nThis patent aims to extend beyond our current framework since the described methodology can account for more than just\ntranslation, e.g. can account for:\n- Rotation\n- Scaling\n- Shearing\n1.2.1\nPatent Overview\nThis patent describes a patent for determining the presence/absence of a pre-determined pattern in an image, and for determin\ning the location of each found instance within a multidimensional space.\nA diagram of the system can be found below: A few notes about the diagram/aggregate system:\n\nFigure 1: System diagram.\n- A match score is computed for each configuration, and later compared with a threshold downstream. This process leads\nto the construction of a high-dimensional matches surface.\n- We can also see in the detailed block diagram from this patent document that we greatly leverage gradient estimation\ntechniques from the previous patent on fast and accurate edge detection.\n- For generalizability, we can run this at multiple scales/levels of resolution.\n1.2.2\nHigh-level Steps of Algorithm\n1. Choose appropriate level of granularity (defined as \"selectable size below which spatial variations in image brightness are\nincreasingly attenuated, and below which therefore image features increasingly cannot be resolved\") and store in model.\n2. Process training/template image to obtain boundary points.\n3. Connect neighboring boundary points that have consistent directions.\n4. Organize connected boundary points into chains.\n5. Remove short or weak chains.\n6. Divide chains into segments of low curvature separated by conrner of high curvature.\n7. Create evenly-spaced along segments and store them in model.\n8. Determine pattern contrast and store in model.\nA few notes about this procedure:\n- To increase the efficiency of this approach, for storing the model, we store probes (along with granularity and contrast),\nnot the image for which we have computed these quantities over.\n- When comparing gradients between runtime and training images, project probe points onto the other image - we do not\nhave to look at all points in image; rather, we compare gradients (direction and magnitude - note that magnitude is\noften less viable/robust to use than gradient direction) between the training and runtime images only at the probing points.\n- We can also weight our probing points, either automatically or manually. Essentially, this states that some probes are\nmore important that others when scoring functions are called on object configurations.\n- This patent can also be leveraged for machine part inspection, which necessitates high degrees of consistent accuracy\n- An analog of probes in other machine and computer vision tasks is the notion of keypoints, which are used in descriptor-\nbased feature matching algorithms such as SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Fea\ntures), FAST (Features from Accelerated Segment Test), and ORB (Oriented FAST and rotated BRIEF). Many of these\naforementioned approaches rely on computing gradients at specific, \"interesting points\" as is done here, and construct\nfeatures for feature matching using a Histogram of Oriented Gradients (HoG) [1].\n\n- For running this framework at multiple scales/resolutions, we want to use different probes at different scales.\n- For multiscale, there is a need for running fast low-pass filtering. Can do so with rapid convolutions, which we will be\ndiscussing in a later patent in this course.\n- Probes \"contribute evidence\" individually, and are not restricted to being on the pixel grid.\n- The accuracy of this approach, similar to the other framework we looked at, is limited by the degree of quantization in the\nsearch space.\n1.2.3\nFramework as Programming Objects\nLet us also take a look at the object-oriented nature of this approach to better understand the framework we work with. One\nof these objects is the model:\nModel: This has fields:\n- Probes: This is the list of probe points. Note that we can also use compiled probes. In turn, each element in this list\nhas fields:\n- Position\n- Direction\n- Weight\n- Granularity: This is a scalar and is chosen during the training step.\n- Contrast: This field is set to the computed contrast of the training/template pattern.\nWe can also look at some of the fields Generalized Degree of Freedom (Generalized DOF) object as well:\n- Rotation\n- Shear - The degree to which right angles are mapped into non-right angles.\n- Log size\n- Log x size\n- Log y size\n1.2.4\nOther Considerations for this Framework\nWe should also consider how to run our translational search. This search should be algorithmically conducted:\n- Efficiently\n- At different levels of resolution\n- Hexagonally, rather than on a square grid - there is a\nadvantage of work done vs. resolution. Here, hexagonal peak\nπ\ndetection is used, and to break ties, we arbitrarily set 3 of the 6 inequalities as ≥, and the other 3 as >.\nWhat is pose?\nPose is short for position and orientation, and is usually determined with respect to a reference coordinate system. In the\npatent's definition, it is the \"mapping from pattern to image coordinates that represents a specific transformation\nand superposition of a pattern onto an image.\"\nNext, let us look into addressing \"noise\", which can cause random matches to occur. Area under S(θ) curve captures the\nprobability of random matches, and we can compensate by calculating error and subtracting it out of the results. However, even\nwith this compensation, we are still faced with additional noise in the result.\nInstead, we can try to assign scoring weights by taking the dot product between gradient vectors: vˆ1 · vˆ2 = cos θ. But one\ndisadvantage of this approach is that we end up quantizing pose space.\nFinally, let us look at how we score the matches between template and runtime image configurations: scoring functions.\nOur options are:\n\n- Normalized correlation (above)\n- Simple peak finding\n- Removal of random matches (this was our \"N\" factor introduced above)\n1.3\nReferences\n1. Histogram of Oriented Gradients, https://en.wikipedia.org/wiki/Histogram_of_oriented gradients\n_______________________________________________________\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 14",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/783e51bd8138fb098f22554024a57459_MIT6_801F20_lec14.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 14\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\nLecture 14: Inspection in PatQuick, Hough Transform, Homography, Posi\ntion Determination, Multi-Scale\nIn this lecture, we will continue our discussion of \"PatQuick\", the patent we discussed last time for object detection and pose\nestimation. We will focus on elements of this patent such as scoring functions and generalized degrees of freedom (DOF), and\nwill use this as a segway into general linear transformations and homography. We will conclude our discussion with subsampling\nand Hough Transforms, which, at a high level, we can think of as a mapping from image space to parameter space.\n1.1\nReview of \"PatQuick\"\nTo frame our analysis and introduction of other technical machine vision concepts, let us briefly revisit the high-level ideas of\n\"PatQuick\". There were three main \"objects\" in this model:\n- Training/template image. This produces a model consisting of probe points.\n- Model, containing probe points.\n- Probe points, which encode evidence for where to make gradient comparisons, i.e. to determine how good matches\nbetween the template image and the runtime image under the current pose configuration.\nOnce we have the model from the training step, we can summarize the process for generating matches as:\n1. Loop over/sample from configurations of the pose space (which is determined and parameterized by our degrees of freedom),\nand modify the runtime image according to the current pose configuration.\n2. Using the probe points of the model, compare the gradient direction (or magnitude, depending on the scoring function)\nto the gradient direction (magnitude) of the runtime image under the current configuration, and score using one of the\nscoring functions below.\n3. Running this for all/all sampled pose configurations from the pose space produces a multidimensional scoring surface. We\ncan find matches by looking for peak values in this surface.\nA few more notes on this framework, before diving into the math:\n- Training is beneficial here, because it allows for some degree of automated learning.\n- Evidence collected from the probe points is cumulative and computed using many local operations.\n- Accuracy is limited by the quantization level of the pose spanned. The non-redundant components of this pose space are:\n- 2D Translation, 2 DOF\n- Rotation, 1 DOF\n- Scaling, 1 DOF,\n- Skew, 1 DOF,\n\n- Aspect Ratio, 1 DOF\nTogether, the space of all these components compose a general linear transformation, or an affine transformation:\nx 0 = a11x + a12y + a13\ny 0 = a21x + a22y + a23\nWhile having all of these options leads to a high degree of generality, it also leads to a huge number of pose configurations,\neven for coarse quantization. This is due to the fact that the number of configurations grows exponentially with the number\nof DOF.\n1.1.1\nScoring Functions\nNext, let us look at the scoring functions leveraged in this framework. Recall that there will also be random gradient matches\nin the background texture - we can compute this \"probability\" as \"noise\" given by N:\nZ 2π\nN =\nRdir(θ)dθ =\n(Using signed values),\n(Using absolute values)\n2π\nWhere the first value ( 3 ) corresponds to the probability of receiving a match if we randomly select a probe point's location,\nthe second value corresponds to taking reverse polarity into account, and the function Rdir corresponds to the scoring function\nfor the gradient direction, and takes as input the difference between two directions as two-dimensional vectors. Below are the\nscoring functions considered. Please take note of the following abbreviations:\n- pi denotes the two-dimensional location of the ith probe point after being projected onto the runtime image.\n- di denotes the direction of the probe point in the template image.\n- wi denotes the weight of the ith probe point, which is typically set manually or via some procedure.\n- D(·) is a function that takes a two-dimensional point as input, and outputs the direction of the gradient at this point.\n- Rdir is a scoring function that takes as input the norm of the difference between two vectors representing gradient directions,\nand returns a scalar.\n- M(·) is a function that computes the magnitude of the gradient in the runtime image at the point given by its two-\ndimensional argument.\n- Rmag is a scoring function that takes as input the magnitude of a gradient and returns a scalar. In this case, Rmag saturates\nat a certain point, e.g. if limx→inf R(x) = K for some K ∈ R , and for some j ∈ R , R(j) = K, R(j + ε) = K ∀ ε ∈ [0, inf).\n- N corresponds to the \"noise\" term computed above from random matches.\nWith these terms specified, we are now ready to introduce the scoring functions:\n1. Piecewise-Linear Weighting with Direction and Normalization:\nP max(wi, 0)Rdir(||D(a + pi) - di||2)\ni\nS1a (a) =\nP max(wi, 0)\ni\nQuick note: the function max(0, wi) is known as the Rectified Linear Unit (ReLU), and is written as ReLU(wi). This\nfunction comes up frequently in machine learning.\n- Works with \"compiled probes\". With these \"compiled probes\", we only vary translation - we have already mapped\npose according to the other DOFs above.\n- Used in a \"coarse step\".\n2. Binary Weighting with Direction and Normalization\nP\ni(wi > 0)Rdir(||D(a + pi) - di||2)\nS1a (a) =\nP (wi > 0)\ni\nWhere the predicate (wi > 0) returns 1 if this is true, and 0 otherwise.\n\n3. \"Preferred Embodiment:\nP (wi > 0)(Rdir(||D(a + pi) - di||2) - N)\ni\nS(a) =\nP\n(1 - N )\n(wi > 0)\ni\n4. Raw Weights with Gradient Magnitude Scaling and No Normalization\nX\nS2(a) =\nwiM(a + pi)Rdir(||D(a + pi) - di||2)\ni\nNote that this scoring function is not normalized, and is used in the fine scanning step of the algorithm.\n5. Raw Weights with Gradient Magnitude Scaling and Normalization\nP\ni wiM(a + pi)Rdir(||D(a + pi) - di||2)\nS3(a) =\nP\ni wi\n1.1.2\nAdditional System Considerations\nLet us focus on a few additional system features to improve our understanding of the system as well as other principles of machine\nvision:\n- Why do some of these approaches use normalization, but not others? Normalization is computationally-expensive,\nand approaches that avoid a normalization step typically do this to speed up computation.\n- For all these scoring functions, the granularity parameter is determined by decreasing the resolution until the system no\nlonger performs well.\n- We need to ensure we get the gradient direction right. So far, with just translation, this has not been something we need\nto worry about. But with our generalized linear transform space of poses, we may have to account for this. Specifically:\n- Translation\n- Rotation\n- Scaling\ndo not affect the gradient directions. However:\n- Shear\n- Aspect ratio\nwill both affect the gradient directions. We can account for this, however, using the following process:\n1. Compute the gradient in the runtime image prior to invoking any transformations on it.\n2. Compute the isophote in the pre-transformed runtime image by rotating 90 degrees from the gradient direction using\nthe rotation matrix given by:\n\nRG→I = -1\n3. Transform the isophote according to the generalized linear transformation above with the degrees of freedom we\nconsider for our pose space.\n4. After computing this transformed isophote, we can find the transformed gradient by finding the direction orthogonal\nto the transformed isophote by rotating back 90 degrees using the rotation matrix given by:\n\n0 -1\nRI→G = 1\n1.1.3\nAnother Application of \"PatQuick\": Machine Inspection\nLet us consider some elements of this framework that make it amenable and applicable for industrial machine part inspection:\n- How do we distinguish between multiple objects (a task more generally known as multiclass object detection and classifi\ncation)? We can achieve this by using multiple models/template images, i.e. one model/template for each type\nof object we want to detect and find the relative pose of.\n- With this framework, we can also compute fractional matches - i.e. how well does one template match another object in\nthe runtime image.\n- We can also take an edge-based similarity perspective - we can look at the runtime image's edge and compare to edge\nmatches achieved with the model.\n\n1.2\nIntro to Homography and Relative Poses\nWe will now shift gears and turn toward a topic that will also be relevant in the coming lectures on 3D. Let's revisit our\nperspective projection equations when we have a camera-centric coordinate system:\nx\nXc y\nYc\n=\n,\n=\nf\nYc f\nYc\nThus far, we have only considered camera-centric coordinate systems - that is, when the coordinates are from the point of view\nof the camera. But what if we seek to image points that are in a coordinate system defined by some world coordinate system\nthat differs from the camera? Then, we can express these camera coordinates as:\n⎡\n⎤\n⎡\n⎤⎡\n⎤\n⎡\n⎤\nXc\nr11\nr12\nr13\nXW\nX0\n⎣ Yc ⎦ = ⎣\n⎦⎣ YW ⎦ + ⎣ Y0 ⎦\nr21\nr22\nr23\nZc\nr31\nr32\nr33\nZW\nZ0\ncam coord. = world2cam rot × world coord. + world2cam trans\n⎡\n⎤\nr11\nr12\nr13\nWhereR = ⎣r21\nr22\nr23⎦ is a rotation matrix in 3D.\nr31\nr32\nr33\nTherefore, in the general case, to find the perspective projection from world coordinates onto our image, we can combine the\ntwo previous equations, carrying out the matrix multiplication along the way:\nx\nXc\nr11XW + r12YW + r13ZW + X0\n=\n=\nf\nZc\nr31XW + r32YW + r33ZW + Z0\ny\nYc\nr21XW + r22YW + r23ZW + Y0\n=\n=\nf\nZc\nr31XW + r32YW + r33ZW + Z0\nIs there a way we can combine rotation and translation into a single operation? Let us consider a simple case in which the\nthe points in our world coordinate system are coplanar in the three-dimensional plane ZW = 0. Since the third column of the\nrotation corresponds to all zeros, we can rewrite our equation from the world coordinate frame to the camera frame as:\n⎡\n⎤\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎡\n⎤⎡\n⎤\n⎡\n⎤\nXc\nr11\nr12\nX0\nXW\nXW\nr11\nr12\nr13\nXW\nX0\n⎣\n⎦ = ⎣\n⎦⎣\n⎦ = T ⎣\n⎦ = ⎣\n⎦⎣\n⎦ + ⎣\n⎦\nYc\nr21\nr22\nY0\nYW\nYW\nr21\nr22\nr23\nYW\nY0\nZc\nr31\nr32\nZ0\nr31\nr32\nr33\nZ0\n\nT\nThe vector XW\nYW\nexpresses our (now 2D) world coordinate system in homogeneous coordinates, which have a 1\nentry appended to the final element.\nIn this case, we can fold translation and rotation into a single matrix! We call this matrix T, and it is called a Homog\nraphy Matrix that encodes both rotation and translation. We will revisit this concept when we begin our discussion of 3D\ntransformations. Note that while our rotation matrix R is orthogonal, this homography matrix T is not necessarily.\n1.2.1\nHow many degrees of freedom\nFor determining the relative pose between camera and world frames, let us consider the number of degrees of freedom:\n- 3 for translation, since we can shift in x, y, and z\n- 3 for rotation, since our rotations can preserve the xz axis, xy axis, and yz axis\nIf we have 9 entries in the rotation matrix and 3 in the translation vector (12 unknowns total), and only 6 degrees of freedom, then\nhow do we solve for these entries? There is redundancy - the rotation matrix has 6 constraints from orthonormality\n(3 from constraining the rows to have unit size, and 3 from having each row being orthogonal to the other).\nMathematically, these constraints appear in our Homography matrix T as:\nr\n= 1 (Unit length constraint)\n11 + r21 + r31\nr\n= 1 (Unit length constraint)\n12 + r22 + r23\nr11r12 + r21r22 + r31r32 = 0 (Orthogonal columns)\n\nA few notes here about solving for our coefficients in T:\n- Do we need to enforce these constraints? Another option is to run least squares on the calibration data.\n- We must be cognizant of the following: We only know the Homography matrix T up to a constant scale factor, since we\nare only interested in the ratio of the components of the camera coordinates for perspective projection.\n1.3\nHough Transforms\nLet us switch gears and talk about another way to achieve edge finding, but more generally the estimation of parameters for any\nparameterized surface.\nMotivation: Edge and line detection for industrial machine vision. This was one of the first machine vision patents (sub\nmitted in 1960, approved in 1962). We are looking for lines in images, but our gradient-based methods may not necessarily work,\ne.g. due to non-contiguous lines that have \"bubbles\" or other discontinuities. These discontinuities can show up especially for\nsmaller resolution levels.\nIdea: The main idea of the Hough Transform is to intelligently map from image/surface space to parameter space for\nthat surface. Let us walk through the mechanics of how parameter estimation works for some geometric objects.\nSome notes on Hough Transforms:\n- Hough transforms are often used as a subroutine in many other machine vision algorithms.\n- Hough transforms actually generalize beyond edge and line detection, and extend more generally into any domain in which\nwe map a parameterized surface in image space into parameter space in order to estimate parameters.\n1.3.1\nHough Transforms with Lines\nA line/edge in image space can be expressed (in two-dimensions for now, just for building intuition, but this framework is\ny-c\namenable for broader generalizations into higher-dimensional lines/planes): y = mx + c. Note that because y = mx + c, m =\nx\nand c = y - mx. Therefore, this necessitates that:\n- A line in image space maps to a singular point in Hough parameter space.\n- A singular point in line space corresponds to a line in Hough parameter space.\nTo estimate the parameters of a line/accomplish edge detection, we utilize the following high-level procedure:\n1. Map the points in the image to lines in Hough parameter space and compute intersections of lines.\n2. Accumulate points and treat them as \"evidence\" using accumulator arrays.\n3. Take peaks of these intersections and determine what lines they correspond to, since points in Hough parameter space\ndefine parameterizations of lines in image space. See the example below:\nFigure 1: Example of finding parameters in Hough Space via the Hough Transform.\n\n1.3.2\nHough Transforms with Circles\nLet us look at how we can find parameters for circles with Hough transforms.\nMotivating example: Localization with Long Term Evolution (LTE) Network. Some context to motivate this appli\ncation further:\n- LTE uses Time Division Multiplexing to send signals, a.k.a \"everyone gets a slot\".\n- CDMA network does not use this.\n- You can triangulate/localize your location based off of how long it takes to send signals to surrounding cellular towers.\nWe can see from the diagram below that we map our circles into Hough parameter space to compute the estimate of parameters.\nAs we have seen in other problems we have studied in this class, we need to take more than one measurement. We cannot solve\nFigure 2: Example of using Hough Transforms to find the parameters of circles for LTE.\nthese problems with just one measurement, but a single measurement constrains the solution. Note that this problem assumes\nthe radius is known.\n1.3.3\nHough Transforms with Searching for Center Position and Radius\nAnother problem of interest is finding the center of position of a circle's radius R and its center position (x, y), which comprise\nthe 3 dimensions in Hough parameter space. In Hough Transform space, this forms a cone that expands upward from R0 = 0,\nwhere each cross-section of Z is the equation (x + y = R2) for the given values of x, y, and R.\nEvery time we find a point on the circle, we update the corresponding set of points on the cone that satisfy this equation.\nThe above results in many cone intersections with one another - as before, we collect evidence from these intersections, build a\nscore surface, and compute the peak of this surface for our parameter estimates.\n1.4\nSampling/Subsampling/Multiscale\nSampling is another important aspect for machine vision tasks, particularly for problems involving multiple scales, such as edge\nand line detection. Sampling is equivalent to working at different scales.\nWhy work at multiple scales?\n- More efficient computation when resolution is lower, and is desirable if performance does not worsen.\n- Features can be more or less salient at different resolutions, i.e. recall that edges are not as simple as step edges and often\nexhibit discontinuities or non-contiguous regions.\nIf we downsample our image by rn along the rows and rm along the columns, where rn, rm ∈ (0, 1), then the total amount of\nwork done (including the beginning image size) is given by the infinite geometric series:\ninf\nX\n(rnrm)i =\n1 - rnrm\ni=0\ninf\nX\ni\n(Recall that\nr =\nfor r ∈ (0, 1))\n1 - r\ni=0\n\nWhat does the total work look like for some of these values?\n- rn\n√1\n- rn = rm = r =\nwork =\n=\n=\n1 - r2\n1 -\nBut downsampling by\neach time is quite aggressive, and can lead to aliasing. Let us also look at a less aggressive sampling\nratio.\n= rm = r =\nwork =\n=\n1 - r2\n1 -\n= 2\nHow do we sample in this case? This is equivalent to taking every other sample in an image when we downsample. We\ncan do this using a checkerboard/chess board pattern. We can even see the selected result as a square grid if we rotate\nour coordinate system by 45 degrees.\nThe SIFT (Scale-Invariant Feature Transform) algorithm uses this less aggressive sampling technique. SIFT is a descriptor-\nbased feature matching algorithm for object detection using a template image.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "6.801/6.866: Machine Vision, Lecture 15",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-801-machine-vision-fall-2020/c22810674f0599193f8b553b348de3b6_MIT6_801F20_lec15.pdf",
      "content": "6.801/6.866: Machine Vision, Lecture 15\nProfessor Berthold Horn, Ryan Sander, Tadayuki Yoshitake\nMIT Department of Electrical Engineering and Computer Science\nFall 2020\nThese lecture summaries are designed to be a review of the lecture. Though I do my best to include all main topics from the\nlecture, the lectures will have more elaborated explanations than these notes.\n1 Lecture 15: Alignment, recognition in PatMAx, distance field, filtering and\nsub-sampling (US 7,065,262)\nIn this lecture, we will discuss another patent on the topic of object inspection and pose estimation, known as PatMAx. We\nwill then look at computing distance to lines as a means to perform better edge detection, and then will investigate the role of\nsparse convolution for multiscale systems that perform filtering.\n1.1\nPatMAx\nAnother patent we will look at for object inspection is PatMAx.\n1.1.1\nOverview\nSome introductory notes on this:\n- This framework builds off of the previous PatQuick patent.\n- This framework, unlike PatQuick, does not perform quantization of the pose space, which is one key factor in enabling\nsub-pixel accuracy.\n- PatMAx assumes we already have an approximate initial estimate of the pose.\n- PatMAx relies on an iterative process for optimizing energy, and each attraction step improves the fit of the configuration.\n- Another motivation for the name of this patent is based off of electrostatic components, namely dipoles, from Maxwell. As\nit turns out, however, this analogy works better with mechanical springs than with electrostatic dipoles.\n- PatMAx performs an iterative attraction process to obtain an estimate of the pose.\n- An iterative approach (e.g. gradient descent, Gauss-Newton, Levenberg-Marquadt) is taken because we likely will not\nhave a closed-form solution in the real world. Rather than solving for a closed-form solution, we will run this iterative\noptimization procedure until we reach convergence.\n- Relating this framework back to PatQuick, PatMAx can be run after PatQuick computes an initial pose estimate, which\nwe can then refine using PatMAx. In fact, we can view our patent workflow as:\nFigure 1: An overview of how the patents we have looked at for object inspection fit together.\n\nFigure 2: High-level diagram of the PatMAx system.\nA diagram of the system can be found here: Now that we have a high-level overview, we are now ready to dive more into the\nspecifics of the system.\n1.1.2\nTraining PatMAx\n: The training process can be classified as three distinct steps:\n1. We begin with edge detection, which produces a field dipole list (essentially the probe points from the PatQuick patent\nframework).\n2. Training also produces a field. We compare runtime features with template features and determine the attraction of these\nfeatures between the images using this field as a vector field.*\n3. We map the feature-detected runtime image's features back to the field (this is more computationally-efficient than mapping\nthe field to the runtime image).\n*For field generation, we can in turn discuss the steps needed to generate such a field:\n1. Initialize\n2. Seed\n3. Connect\n4. Chain\n5. Filter\n6. Segment\n7. Propagate\nMany of the steps outlined in this field generation process were also leveraged in the PatQuick method.\nAnother important aspect of training is computing field dipoles. A few notes on this:\n- Field dipoles correspond to edge fragments.\n- Field dipoles are created as a data structure of flags that provide information about proximity to other components, such\nas the edge.\nSome other notes on this framework:\n\n- Edge detection is largely the same procedure that we have seen in the previous patents (e.g. PatQuick). However, note\nthat because this framework seeks to obtain highly-precise estimates accurate to the sub-pixel level, PatMAx does not use\nCORDIC or quantized gradient directions.\n- Field dipoles are computed during training.\n- The chaining procedure used in PatMAx is similar to the process we saw before: (i) Link chains, and then (ii) Remove\nshort (weak) chains.\n- For initialization, the array contains a vector field, but the vectors do not cover the entire array.\nWe will now explore some specific elements of this framework:\n1.1.3\nEstimating Other Pixels\n- To estimate other pixels, we need to fill in pixels near the edge in an iterative fashion.\n- To accomplish this, PatMAx uses a distance map, which is common in machine vision applications.\n- We can compute the distance to the edge accurately with Manhattan distance, but we use Euclidean distance, which is\nnon-trivial to compute, particularly, as we will see shortly, for corner edges.\n- Intuitively, we want the system to be drawn to a \"lower energy state\", hence the idea of this algorithm being an energy\nminimization algorithm.\n- Identical copies can be resolved via averaging.\n1.1.4\nAttraction Module\nThe diagram of the attraction module is given below: Intuition with Mechanical Springs: Scaling adjustments via scaled\nFigure 3: The Attraction module for the PatMAx system. Note that this produces a refined estimate of the pose at the output,\nwhich is one of the main goals of the PatMAx system.\ntransformations can be conceptualized as a set of mechanical springs (rather than electrostatic dipoles) that are adjusted until\nan optimal configuration of the degrees of freedom is found.\n\nSolving this system is equivalent to solving a large least squares problem:\n- Each runtime dipole has force exerted on it in a certain direction. The goal here is to use diffent movements, which comprise\nour Degrees of Freedom, to create a configuration of these dipoles that minimizes the overall energy/tension of this system.\n- The movements/degrees of freedom that are allowed: (i) Translation in 2D (2 DOF), (ii) Rotation in 2D (1 DOF), (iii)\nScaling (1 DOF). Therefore with these admissable movements we have 4 degrees of freedom.\n- A closed-form solution of this does not exist, but we can compute a solution to this least squares problem using an upper\ntriangular matrix accumulator array. This array is scaled additionally by weights, and can also be used to compute\nimage moments.\n- With our movements (translation, rotation, and scaling), we have 4 DOF. With a full set of movements comprising affine\nlinear transformations, we have 6 DOF.\n- Local linearization around the operating point is also used when solving for the least squares solution to this problem.\n- One computational technique the authors of this patent make use of is the use of doubly linked lists for the image\ndipoles.\n1.1.5\nPatMAx Claims\nAs we have seen, another good way to get a sense of the \"abstract\" of a patent is to look through its claims. The big claim of\nPatMAx: PatMAx is a geometric pattern-matching method used for iteratively refining the estimte of the true pose (relative to\nthe orientation of the object in the training image) in a runtime image.\n1.1.6\nComparing PatMAx to PatQuick\nTo better understand these frameworks (and how they potentially fit together for cascaded object inspection, let us draw some\ncomparisons between PatQuick and PatMAx):\n- PatQuick searched all pose space and does not require an initial guess - PatMAx does require an initial estimate/guess of\nthe pose in order to produce a refined estimate.\n- For PatMAx, there is repeated emphasis on avoiding techniques such as thresholding and quantization of gradient directions\nthat have been used in the previous patents we have looked at. This makes sense, since PatMAx aims to output a more\nrefined estimate of the pose than these other frameworks (i.e. reach sub-pixel accuracy).\n- Using \"dipoles\" for PatMAx is misguided - using physical springs as an analog is much more physiclly consistent.\n- For PatMAx, we use evidence collected for determining the quality of alignment, which in turn determines the quality of\nour refined pose estimate.\n- PatMAx and PatQuick each have different methods for assigning weights.\n- PatMAx is a nonlinear optimization problem, and therefore does not have a closed-form solution. PatMAx is also iterative\n- alignment quality and matched edges get closer with more iterations of optimization.\n1.1.7\nField Generation for PatMAx\nHere, we look at the field generation. For building intuition and simplifying, we will only take into account distance. See the\nfigure below for some examples of distance fields as circles, lines, and (hardest to compute) corner edges. A few notes about\nthese:\n- If we were working with Manhattan (L1-norm) distance rather than Euclidean (L2-norm) distance, this would be a\nmuch easier problem to solve, especially for edge corners. However, unfortunately this is not an option we have.\n- We can weight the forces of individual dipoles in the runtime image. Weight is computed for beliefs/evidence for (i)\nforces, (ii) torques, and (iii) scaling, where {wi}N\nis the set of weights:\ni=1\nPN\nFiwi\n1. Forces (Translation): F =\nP\ni=1\nN\n∈ R2\nwi\nP\ni=1\nN\ni=1 wi(ri×Fi)\n2. Torques (Rotation): τ =\nPN\n∈ R, where ri is the radial vector\nwi\nP\ni=1\nN\ni=1 wi (ri ·Fi)\n3. Scaling: s =\nPN\n∈ R where ri is the radial vector\nwi\ni=1\nTogether, these three elements composed of weighted evidence from the dipoles compose our 4 DOF.\n\nFigure 4: Examples of distance fields.\n1.2\nFinding Distance to Lines\nOne application of performing this task is to improve the performance of edge detection systems by combining shorter edge\nfragments of objects into longer edge fragments.\nFor this procedure, we break this down into two steps:\n1. Rotate the 2D cartesian coordinate system into a system that is parallel to the line of interest:\nx 0 = x cos θ + y sin θ\ny = -x sin θ + y cos θ\n\nx\ncos θ\nsin θ\nx\nI.e.\n=\n0y\n- sin θ cos θ\ny\n2. Translate the origin to the new location to match the line:\nx = x\ny = y 0 - ρ\n= -x sin θ + y cos θ - ρ\nTogether, these equations imply:\ny + x sin θ - y cos θ + ρ = 0\ny + x 00 sin θ - y cos θ + ρ = 0\nWhich in turn has the properties:\n- There are no redundant solutions\n- The system is parameterized by (ρ, θ)\n- There are no singularities\n\nFrom here, we can use the above framework to find a line by minimizing the following objective over our parameterized degrees\nof freedom ρ and θ:\nN\nX\nρ ∗ , θ ∗ = arg min\n(yi )2\nρ,θ i=1\nN\nX\n= arg min\n(xi sin θ - yi cos θ + ρ)2\nρ,θ i=1\nΔ\n= arg min J(ρ, θ)\nρ,θ\nThis problem can be solved through our standard calculus approaches of finding the first-order conditions of our objective J(ρ, θ)\non our degrees of freedom ρ and θ. Since we have two degrees of freedom, we have two First-Order Conditions:\n∂J(ρ,θ)\n1.\n= 0:\n∂ρ\nN\n\nX\n∂ (J(ρ, θ)) = ∂\n(xi sin θ - yi cos θ + ρ)2\n∂ρ\n∂ρ\ni=1\nN\nX\n= 2\n(xi sin θ - yi cos θ + ρ) = 0\ni=1\nN\n\nN\n\nN\n\n= sin θ\nxi - cos θ\nyi +\nρ = 0\ni=1\ni=1\ni=1\nX\nX\nX\n= Nx sin θ - Ny cos θ + Nρ = 0\n= x sin θ - y cos θ + ρ = 0\nΔ\nPN\nΔ\nPN\n(Where x =\ni=1 xi and y =\ni=1 yi.)\nN\nN\nThough this does not give us the final answer, it does provide information on how our solution is constrained, i.e. the line\nmust pass through the centroid given by the mean ( x, y ). Let us now look at the second FOC to combine insights from\nthat FOC with this FOC in order to obtain our solution.\n∂J(ρ,θ)\n2.\n= 0:\n∂θ\nBefore computing this derivative, let us move our coordinates to the centroid, i.e. subtract the mean:\nxi = xi - x -→ xi = x + xi\ny = yi - y -→ yi = y + y\ni\ni\nPlugging this substituted definition into our equations renders them such that the centroid cancels out. Let us now compute\nthe second FOC:\nN\n\nX\n∂ (J(ρ, θ)) = ∂\n(xi sin θ - yi cos θ + ρ)2\n∂θ\n∂θ\ni=1\nN\nX\n= 2\n(xi sin θ - yi cos θ + ρ)(x cos θ + y 0 sin θ) = 0\ni=1\nN\nX\n=\nx 02 sin θ cos θ + x y 0 sin2 θ - x y cos 2 θ - y cos θ sin θ) = 0\ni=1\nN\nN\nX\nX\n=\n(xi - yi ) sin θ cos θ =\nxiyi(cos2 θ - sin2 θ) = 0\ni=1\ni=1\nN\nN\nX\nX\n=\n(xi - yi ) sin(2θ) =\nxiyi cos(2θ) = 0\n2 i=1\ni=1\nPN\nsin(2θ)\ni=1 xiyi\n=\n= tan(2θ) = PN\ncos(2θ)\n(x2 - y2)\ni=1\ni\ni\n\nA few notes about this:\n- In the second-to-last step, we used the two following trigonometric identities: (i) sin θ cos θ = sin(2θ), and (ii)\ncos2 θ - sin2 θ = cos(2θ).\n- Notice that we can separate the angle θ from the sums because it does not depend on the sum index and is a degree\nof freedom in this optimization problem.\nFrom here, we can solve for the optimal value of θ (which will also constrain and give us an optimal value of ρ) by taking\nthe arctangent that returns quadrant (\"atan2\"):\nN\nN\nX\nX\n\nθ ∗ =\narctan 2 2\nxiyi,\n(xi - yi )\ni=1\ni=1\nTherefore, solving the FOCs gives us a closed-form least squares estimate of this line parameterized by (ρ, θ). This solution,\nunlike the Cartesian y = mx + b fitting of a line, is independent of the chosen coordinate system, allowing for further flexibility\nand generalizability.\n1.3\nFast Convolutions Through Sparsity\nNext, we will switch gears and revisit multiscale, which is a common procedure needed in machine vision. Multiscale motivates\nthe need of filtering methods that are computationally-agnostic to the scale or resolution being used. Since filtering is just\nconvolution, this motivates the exploration of another patent, namely on fast convolutions. The patent we explore is \"Efficient\nFlexible Digital Filtering, US 6.457,032.\nThe goal of this system is to efficiently compute filters for multiscale. For this, we assume the form of an Nth-order piece\nwise polynomial, i.e. a Nth-order spline.\n1.3.1\nSystem Overview\nThe block diagram of this system can be found below: A few notes on this system:\n- Why is it of interest, if we have Nth-order splines as our functions, to take Nth-order differences? The reason for this is\nthat the differences create sparsity, which is critical for fast and efficient convolution. Sparsity is ensured because:\nN\ni\ndN+1\nX\nf(x) = 0 ∀ x if f(x) =\naix , ai ∈ R ∀ i ∈{1, · · · , N}\ndxN+1\ni=0\n(I.e, if f(x) is a order-N polynomial, then the order-(N+1) difference will be 0 for all x.\nThis sparse structure makes convolutions much easier and more efficient to compute by reducing the size/cardinality of the\nsupport (we will discuss what a support is in greater detail in the next lecture, as well as how the size of a support affects\ncomputational efficiency, but effectively the support is the subset of the domain of a function that is not mapped to zero).\n- Why do we apply an order-(N+1) summing operator? We apply this because we need to \"invert\" the effects of the order\n(N+1) difference. Intuitively, this makes sense that the order-(N+1) difference and the order-(N+1) sum commute, because\nwe are simply performing iterative rounds of subtraction and addition (respectively), which we know are commutative\nalgebraic operations. I.e, representing differencing and summing as linear operators where their order is the same, we have:\nFirst Order : DS = I\nSecond Order : DDSS = DSDS = (DS)(DS) = II = I\n. . .\nOrder K : (D)K (S)K = (DS)K = IK = I\n1.3.2\nIntegration and Differentiation as Convolutions\nConceptualizing these differencing/differentiation and summing/integration as linear operators that are commutative and asso\nciative, we can then extend this framework to conceptualizing these operators as convolutions:\n- Integration: This corresponds to the convolution of our piecewise polynomial f(x) with a unit step function u(x).\n\nFigure 5: Block diagram of this sparse/fast convolution framework for digital filtering. Note that this can be viewed as a\ncompression problem, in which differencing compresses the signal, and summing decompresses the signal.\n- Differentiation: This corresponds to the convolution of our piecewise polynomial f(x) with two scaled impulses in\nopposite directions:\n2δ(x + ε\n2) + 1\n2δ(x -ε\n2).\nThis motivates the discussion of some of the properties of convolution this system relies on in order to achieve high performance.\nFor operators A, B, and C, we have that:\n1. Commutativity: A ⊗ B = B ⊗ A\n2. Associativity: A ⊗ (B ⊗ C) = (A ⊗ B) ⊗ C\nThese properties stem from the fact that in the Fourier domain, convolution is simply multiplication. Therefore, convolution\nobeys all the algebraic properties of multiplication.\n1.3.3\nSparse Convolution as Compression\nAs aforementioned, another way to consider this efficient convolution system is as a compression mechanism, in which the\ndifferencing operation acts as a compression mechanism, and the summing operation acts as a decompression mechanism.\nWe can contrast this with standard convolution in the block diagram below. This sparse convolution approach ends up being a\nmuch more efficient way to filter a signal.\n1.3.4\nEffects on Scaling\nThis fast/efficient convolution framework yields desirable results for scaling, and, as we will see shortly, for problems in which\nmultiscale approaches are necessary. Because the only places in which we need to do work are now the locations where the\npiecewise polynomial segments are stitched together, the scaling can be changed to coarser and finer levels without affecting the\namount of computation that is required! This improves the efficiency of multiscale approaches.\n1.3.5\nFiltering (For Multiscale): Anti-Aliasing\nWe have now reduced the amount of computation needed to compute efficient digital filtering. We now only need final ingredient\nfor multiscale that is motivated by Shannon and Nyquist: anti-aliasing methods (filters).\n\nFigure 6: Comparison of standard filtering and efficient/sparse filtering procedures, where the sparse filtering approach is\nillustrated as a compression problem. Here, H represents the filter, X and Y represent the uncompressed inputs and outputs,\nrespectively, and x and y represent the compressed inputs and outputs.\nRecall from Shannon/Nyquist (the Sampling Theorem) that in order to sample (for instance, when we subsample in multi-\nscale problems) without aliasing and high-frequency artifacts, it is critical that we first remove high-frequency components from\nthe signal we are sampling. This high-frequency component removal can be achieved with approximate low pass filtering (which\nwe will cover in greater detail during the next lecture).\nWe will see in the next lecture that one way we can achieve approximate low pass filtering is by approximating a spatial\nsinc function (which transforms into an ideal low pass filter in the frequency domain) as a spline.\n1.3.6\nExtending Filtering to 2D and An Open Research Problem\nThis framework is built for 1D, but we note that we can extend it to 2D simply by approximating 2D convolution as a cascaded\nset of 1D convolutions, if we are to continue using this sparse convolution mechanism. This requires some additional run-time\nmemory as we must store an image corresponding to the 1D convolution of the image with a 1D filter, but this allows us to\ncontinue using this efficient sparse convolution structure.\nOne open research problem: how can we extend this sparse convolution structure to 2D?\nFinally, we will finish this lecture with a fun fact on calcite crystals. Calcite crystals are a type of birefringent mate\nrial, which means that they have two indices of refraction that depend on two polarizations (one in the x-direction and one\nin the y-direction), and therefore reflect light into two different ways. As we will see in the next lecture, adding birefringent\nlenses in the analog domain can prevent aliasing affects from occurring that would otherwise be unavoidable. DSLRs have these\nbirefringent lenses affixed to them for this specific anti-aliasing purpose.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n6.801 / 6.866 Machine Vision\nFall 2020\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    }
  ]
}