{
  "course_name": "Algorithms for Computational Biology",
  "course_description": "This course is offered to undergraduates and addresses several algorithmic challenges in computational biology. The principles of algorithmic design for biological datasets are studied and existing algorithms analyzed for application to real datasets. Topics covered include: biological sequence analysis, gene identification, regulatory motif discovery, genome assembly, genome duplication and rearrangements, evolutionary theory, clustering algorithms, and scale-free networks.",
  "topics": [
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Computer Science",
    "Algorithms and Data Structures",
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Computer Science",
    "Algorithms and Data Structures"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1 session / week, 1.5 hours / session\n\nLocation\n\nLectures will be held weekly for 1.5 hours each session. This term, 6.096 is being taught in conjunction with 6.046J. Students are encouraged to enroll in both courses simultaneously and learn both the foundations of algorithms, together with Computational Biology. In addition, tutorials and office hours will be held weekly.\n\nPrerequisites\n\n6.001 (Structure and Interpretation of Computer Programs), 7.01\n\nGrading\n\nGrading will be calculated based on the following formula:\n\nactivities\n\npercentages\n\nFour Problem Sets\n\n60%\n\nFinal Exam\n\n30%\n\nAttendance\n\n10%\n\nAlgorithmic Challenges\n\nThe field of computational biology is rich in algorithmic challenges, stemming from the sheer size of massive datasets, and also the noisy nature of biological signals. For example, the human genome contains 3 billion letters, 25 thousand genes, and several thousand regulatory signals governing the gene usage. The genes themselves interact in complex ways, forming dense regulatory networks, shaped by relentless evolutionary forces.\n\nIn this course, we will address several algorithmic challenges in computational biology. We will study the principles of algorithmic design for biological datasets, analyze existing algorithms, and apply these to real datasets. The lectures will cover:\n\nChallenge Number 1: Assembly of Complete Genomes from Short Reads\n\nGenomes are several billion letters long, and yet sequencing technologies only read a few hundred nucleotides at a time. Assembling the pieces into a complete genome is especially challenging, given the highly repetitive nature of the human genome, and the error-prone nature of sequencing reads.\n\nChallenge Number 2: Gene Identification in Vast Genomic Regions\n\nThe cell recognizes several biological signals guiding the processes of transcription, splicing, and translation. We will see how to map these signals onto the genome, and recognize complete gene structures in presence of vast non-coding regions.\n\nChallenge Number 3: Regulatory Motif Discovery\n\nIn response to environmental changes, the cell reads a complex code of regulatory signals which dictate gene usage. These sequence patterns are extremely short (10 nucleotides), often degenerate, and occur at varying distances from the gene start. We will develop algorithms for detecting these elements amidst oceans of non-functional nucleotides.\n\nChallenge Number 4: Genome Alignment and Comparison\n\nThe forces of natural selection and random mutation have shaped genome evolution and the modern species. Comparison of related genomes can yield insights into their evolutionary constraints, and reveal the functional elements within them. We will study algorithms for sequence alignment, both at the nucleotide level, and at the genome level.\n\nChallenge Number 5: Reconstruction of Regulatory Networks\n\nThe genes of a species rarely act in isolation. Complex regulatory networks govern their usage, and complex interaction networks govern their cooperation patterns. We will study algorithms to infer modules of cooperating genes, reconstruct regulatory networks, and study their emerging properties.\n\nChallenge Number 6: Inference of Evolutionary Mechanisms\n\nPerhaps the most intriguing aspect of biological systems is their ability to evolve. We will study algorithms for inferring evolutionary events, understanding the ancestry of species, and their evolutionary relationships.\n\nReferences\n\nReferences will be taken from:\n\nGusfield, Dan.\nAlgorithms on Strings, Trees and Sequences: Computer Science and Computational Biology\n. Cambridge, UK: Cambridge University Press, 1997. ISBN: 0521585198.\n\nWaterman, Michael.\nIntroduction to Computational Biology: Maps, Sequences, and Genomes\n. Boca Raton, FL: CRC Press, 1995. ISBN: 0412993910.\n\nDurbin, Richard, Graeme Mitchison, S. Eddy, A. Krogh, and G. Mitchison.\nBiological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\n. Cambridge, UK: Cambridge University Press, 1997. ISBN: 0521629713.\n\nJones, Neil, and Pavel Pevzner.\nAn Introduction to Bioinformatics Algorithms\n. Cambridge, MA:\nMIT Press\n, 2004. ISBN: 0262101068.",
  "files": [
    {
      "category": "Resource",
      "title": "cluster.py",
      "type": "PY",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/be33fe22ce13fa6b98d98914c8c09a5c_cluster.py",
      "content": "import random\nimport string\n\nimport sys\nimport tools\nfrom tools import pp\n\n# the representation for a point is:\n#\n# (name, value)\n#\n# the rep for a cluster is\n#\n# ([name1, name2...], mean_value, size)\n#\n\nNAME = 0 # the name of read or list of names of cluster\nVALUE = 1 # the mean vector of the read or cluster\nSIZE = 2 # the number of points\n\n############################################################\n#\n# POINTS IN EUCLIDIAN SPACE\n#\n############################################################\n\ndef euclidian_distance(x1,x2):\n# calculates the euclidian distance between two sample points\n# dist = Sum_i( (x1[i] - x2[i])^2 )\nreturn tools.sum(map(lambda x: x*x,\nmap(lambda o1x,o2x: (o1x-o2x),\nx1, x2)))\n\ndef weighted_average(xs, ws):\n# average the vectors xs=[x1,x2,x3...], with weights ws=[w1,w2,w3...]\n# result[i] = 1/(w1+w2+w3+...) * (x1[i]*w1 + x2[i]*w2 + x3[i]*w3 + ...)\n\n# requires: all vectors x1,x2,x3... are same length\n# len(xs) = len(ws)\n# returns mean of length = len(x1) = len(x2) = ...\nmean = [0]*len(xs[0])\nsum_w = tools.sum(ws)\nfor i in range(0, len(mean)):\nsum_i = 0\nfor x,w in map(None,xs,ws):\nsum_i = sum_i + x[i] * w\nmean[i] = sum_i/sum_w\nreturn mean\n\ndef points2cluster(points):\n# from (name, value)\n# to ([name], value)\nreturn (map(lambda p: p[NAME],points),\nweighted_average(map(lambda p: p[VALUE],points),[1]*len(points)),\nlen(points))\n\n############################################################\n#\n# N-MERS IN STRING SPACE\n#\n############################################################\n\ndef create_empty_profile(length):\n\"\"\" Create an empty profile (a cluster with no points assigned) \"\"\"\nprofile = []\nfor i in range(0,length):\nposition = {}\ntools.mset(position,'ACGT',[0,0,0,0])\nprofile.append(position)\nreturn profile\n\ndef strings2profile(strings):\n\"\"\" Creates a profile from a set of strings.\n\nAssumes that all strings are same length\"\"\"\nif not strings: return None\n\nprofile = create_empty_profile(len(strings[0]))\nfor s in strings:\n# add every string to the profile\nfor i in range(0,len(s)):\nchar = s[i]\nif char in 'ACGT':\n# if the character is unambiguous, add unit to profile\nprofile[i][char] = profile[i][char] + 1\nelse:\n# if the character is ambiguous, add increments\nchars = tools.IUB_expansion(char)\nincrement = 1.0/float(len(chars))\nfor char in chars:\nprofile[i][char] = profile[i][char] + increment\nreturn profile\n\ndef distance_string_profile(s,p):\n\"\"\" Evaluates the distance from a string to a profile \"\"\"\nif not p: return len(s)*.75\n\n# the number of points added to the cluster\ntot_p = tools.sum(tools.mget(p[0],'ACGT'))\n# the intersections of this particular string\nscore = 0\nfor i in range(0,len(s)):\nchar = s[i]\nif char in 'ACGT':\nscore = score + p[i][char]\nelse:\nchars = tools.IUB_expansion(char)\nincrement = 1.0/float(len(chars))\nfor char in chars:\nscore = score + increment\ndistance = (tot_p*len(s))-score\n#print \"Distance from %s to %s is %s\"%(s,display_profile(p),distance)\nreturn distance\n\ndef display_profile(profile):\nif not profile: return '[](0)'\ntot_p = tools.sum(tools.mget(profile[0],'ACGT'))\ns = ''\nfor pos in profile:\ns = s+'[%s]'%string.join(map(lambda s: s[0],\nfilter(lambda s: s[1], pos.items())),'')\ns = s+'(%s)'%tot_p\nreturn s\n\n######## More operations, not needed for k-means\n#\n#def update_profile(profile, string):\n# \"\"\" Adding a string into a profile (adding a point to a cluster) \"\"\"\n# if len(profile)!=len(string): raise \"Inconsistent lengths\", (len(profile),len(string))\n# for i in range(0,len(string)):\n# char = string[i]\n# profile[i][char] = profile[i][char] + 1\n#\n#def string2profile(string):\n# \"\"\" Transforming a string into a profile (point -> cluster). \"\"\"\n# if type(string) == type(''):\n# profile = create_empty_profile(len(string))\n# update_profile(profile,string)\n# return profile\n# else:\n# return string\n#\n#def distance_string(p1,p2):\n# \"\"\" Distance in string space. Works for either profiles or strings. \"\"\"\n# # s1 and s2 can be either strings or profiles\n# if type(p1) == type(''): p1 = string2profile(p1)\n# if type(p2) == type(''): p2 = string2profile(p2)\n# return distance_profiles(p1,p2)\n\ndef distance_profiles(p1,p2):\n\"\"\" Distance between two profiles \"\"\"\nif len(p1)!=len(p2): raise \"Uncomparable lengths\", (p1,p2)\n\ntot_p1 = float(tools.sum(tools.mget(p1[0],'ACGT')))\ntot_p2 = float(tools.sum(tools.mget(p2[0],'ACGT')))\n\nmatches = []\nfor pos1,pos2 in map(None,p1,p2):\nmatch = 0\nfor char in 'ACGT':\nmatch = match + (pos1[char]/tot_p1) * (pos2[char]/tot_p2)\nmatches.append(match)\nreturn tools.sum(matches)/len(matches)\n\ndef profile_sum(profiles):\n\"\"\" Joining two clusters. Returns one which contains all points in either. \"\"\"\npp(profiles)\nlength = len(profiles[0])\nfor profile in profiles:\nif len(profile)!=length: raise \"Incompatible lengths\", map(len, profiles)\nnew_p = create_empty_profile(length)\nfor i in range(0,length):\nfor char in 'ACGT':\nfor profile in profiles:\nnew_p[i][char] = new_p[i][char] + profile[i][char]\nreturn new_p\n\n#def profile_pairwise_sum(p1,p2):\n#\n# if len(p1)!=len(p2): raise \"Incompatible lengths\", (len(p1),len(p2))\n# new_p = create_empty_profile(len(p1))\n# for i in range(0,len(p1)):\n# for char in 'ACGT':\n# new_p[i][char] = p1[i][char] + p2[i][char]\n# return new_p\n###########################################\n\ndef string_points2cluster(points):\nreturn [map(lambda p: p[NAME],points),\nstrings2profile(map(lambda p: p[VALUE],points)),\nlen(points)]\n\ndef k_means_string(sequences, n, k):\npoints = gather_kmers_names(sequences,n)\n\nclusters = k_means(points, k,\ndistance_func=distance_string_profile,\n#string_points2cluster,\nps2c_func=strings2profile,\ndisplay_cluster=display_profile)\n\nclusters = filter(lambda c: c[SIZE], tools.my_sort(clusters,len))\npp(clusters,2,60)\nprint string.join(map(lambda c: display_profile(c[VALUE]),clusters),'\\n')\nreturn clusters\n\ndef hierarchical_string(sequences, n):\npoints = gather_kmers_names(sequences,n)\n\nclusters = hierarchical(points, .01,\ndistance_cluster_cluster=distance_profiles,\njoin_clusters=profile_sum,\nps2c_func=strings2profile,\ndisplay_cluster=display_profile)\n\n#def spoint2scluster(spoint):\n# return [[spoint[NAME]],strings2profile(spoint[VALUE]),1]\n\n#def average_profiles(profiles, scaling):\n# return profile_sum(profiles)\n#\n#def gather_kmers(sequences, k):\n# kmers = []\n# for seq in sequences:\n# for i in range(0,len(seq)-k+1):\n# kmers.append(seq[i:][:k])\n# return kmers\n\ndef gather_kmers_names(sequences, k):\nkmers,names = [],[]\nfor seq,s in map(None,sequences,range(0,len(sequences))):\nfor i in range(0,len(seq)-k+1):\nkmers.append(seq[i:][:k])\nnames.append((s,i))\nreturn map(None,names,kmers)\n\n############################################################\n#\n# DIFFERENT ALGORITHMS\n#\n############################################################\n\ndef hierarchical_optimized(points,\ndistance_cluster_clutser=euclidian_distance,\njoin_clusters=weighted_average,\nps2c_func=points2cluster,\ndisplay_cluster=pp):\n\n# first find the max distance between any 2 points\n# only needed to initiate the min distance between every iteration\n\nprint \"Converting every point to a cluster\"\n# first, converts every data point into a cluster\nclusters = map(lambda p,ps2c=ps2c_func: [[p[NAME]],ps2c([p[VALUE]]),1], points)\n\nprint \"Calculating all pairwise distances\"\ndistances = []\nfor i in range(0,len(clusters)):\nfor j in range(0,len(clusters)):\nif j<=i: continue\ndist = distance_cluster_cluster(clusters[i][VALUE],\nclusters[j][VALUE])\ndistances.append((i, j, dist))\nprint 'Done calculating %s distances'%len(distances)\n\ndistances.sort(lambda d1,d2: d2[2]-d1[2])\nprint 'Done sorting %s distances'%len(distances)\n\nprint \"Max distance is %s\"%distances[0]\n\nif max_dist < max_join:\n# if you can't make any joins, then simply return all points,\n# each as a different cluster\nreturn clusters\n\n# transform list into dictonary so that indices are never reused\nclusters = tools.list2dic_i(clusters)\nn_clusters = len(clusters)\n\n# at every iteration, joins the two closest clusters\n# replaces them by the mean cluster\nwhile 1:\n\nprint string.join(map(display_cluster,\nmap(lambda c: c[VALUE],\nfilter(lambda c: c[SIZE]>1,clusters.values()))),'\\n')\n\n# initiate to max distance, to make sure we don't miss anything\nmin_dist, min_pair = max_dist, None\nfor i in clusters.keys():\nfor j in clusters.keys():\nif j<=i: continue\ndist = distance_cluster_cluster(clusters[i][VALUE], clusters[j][VALUE])\nif dist < min_dist:\nmin_dist = dist\nmin_pair = (i,j)\n\n#print \"Min dist = %s for %s\"%(min_dist, min_pair)\n\n# if the join to be made is greater then the max allowed join,\n# then stop right there\nif (not min_pair) or min_dist > max_join or len(clusters)==1: break\n\n# replace the min pair with a weighted average of their averages\n# where the weights are the number of points already contained\n# in each cluster.\ni,j = min_pair\nnew_mean = join_clusters([clusters[i][VALUE],\nclusters[j][VALUE]])\n#new_name = (clusters[i][NAME], clusters[j][NAME])\nnew_name = tools.flatten([clusters[i][NAME], clusters[j][NAME]])\nnew_size = clusters[i][SIZE] + clusters[j][SIZE]\nclusters[n_clusters] = (new_name, new_mean, new_size)\nn_clusters = n_clusters + 1\ndel(clusters[i])\ndel(clusters[j])\ncluster_list = tools.dic2list(clusters)\ncluster_list.sort(lambda c1,c2: c2[SIZE]-c1[SIZE])\nreturn cluster_list\n\ndef hierarchical(points, max_join,\ndistance_cluster_cluster=euclidian_distance,\njoin_clusters=weighted_average,\nps2c_func=points2cluster,\ndisplay_cluster=pp):\n\n# first find the max distance between any 2 points\n# only needed to initiate the min distance between every iteration\n\nprint \"Converting every point to a cluster\"\n# first, converts every data point into a cluster\nclusters = map(lambda p,ps2c=ps2c_func: [[p[NAME]],ps2c([p[VALUE]]),1], points)\n\nprint \"Calculating all pairwise distances\"\nmax_dist = 0\nfor i in range(0,len(clusters)):\nfor j in range(0,len(clusters)):\nif j<=i: continue\ndist = distance_cluster_cluster(clusters[i][VALUE],\nclusters[j][VALUE])\nif dist > max_dist: max_dist = dist\nprint \"Max distance is %s\"%max_dist\nif max_dist < max_join:\n# if you can't make any joins, then simply return all points,\n# each as a different cluster\nreturn clusters\n\n# transform list into dictonary so that indices are never reused\nclusters = tools.list2dic_i(clusters)\nn_clusters = len(clusters)\n\n# at every iteration, joins the two closest clusters\n# replaces them by the mean cluster\nwhile 1:\n\nprint string.join(map(display_cluster,\nmap(lambda c: c[VALUE],\nfilter(lambda c: c[SIZE]>1,clusters.values()))),'\\n')\n\n# initiate to max distance, to make sure we don't miss anything\nmin_dist, min_pair = max_dist, None\nfor i in clusters.keys():\nfor j in clusters.keys():\nif j<=i: continue\ndist = distance_cluster_cluster(clusters[i][VALUE], clusters[j][VALUE])\nif dist < min_dist:\nmin_dist = dist\nmin_pair = (i,j)\n\n#print \"Min dist = %s for %s\"%(min_dist, min_pair)\n\n# if the join to be made is greater then the max allowed join,\n# then stop right there\nif (not min_pair) or min_dist > max_join or len(clusters)==1: break\n\n# replace the min pair with a weighted average of their averages\n# where the weights are the number of points already contained\n# in each cluster.\ni,j = min_pair\nnew_mean = join_clusters([clusters[i][VALUE],\nclusters[j][VALUE]])\n#new_name = (clusters[i][NAME], clusters[j][NAME])\nnew_name = tools.flatten([clusters[i][NAME], clusters[j][NAME]])\nnew_size = clusters[i][SIZE] + clusters[j][SIZE]\nclusters[n_clusters] = (new_name, new_mean, new_size)\nn_clusters = n_clusters + 1\ndel(clusters[i])\ndel(clusters[j])\ncluster_list = tools.dic2list(clusters)\ncluster_list.sort(lambda c1,c2: c2[SIZE]-c1[SIZE])\nreturn cluster_list\n\ndef make_k_means_func(k):\nreturn lambda points,k=k: k_means(points,k)\n\ndef k_means(points, k,\ndistance_func=euclidian_distance,\nps2c_func=points2cluster,\ndisplay_cluster=pp):\n\n# create a dictionary mapping point names to values\npoint_dic = {}\ntools.mset(point_dic,\nmap(lambda p: p[NAME], points),\nmap(lambda p: p[VALUE], points))\n\n# pick k non-identical centers\nif k>len(points): raise \"More centers than data points!\",(k,len(points))\ncenters = []\nwhile len(centers) < k:\nnew_center = random.choice(points)\n# check that the new center is not on an old one\n# otherwise algorithm will not converge\nno_good = 0\nfor old_center in centers:\n# distance from point to cluster\nif distance_func(new_center[VALUE], old_center[VALUE])==0:\nno_good = 1\nbreak\n# if it intersects a previous center, check for another center\nif no_good: continue\ncenters.append([[new_center[NAME]],\nps2c_func([new_center[VALUE]]),\n1])\n\n# now do the iteration\niter = 0\nwhile 1:\n\nprint \"*\"*40\n\nif 1:\niter = iter+1\nprint \"Iteration %s\"%iter\n#pp(map(lambda c: (c[SIZE],(map(lambda d: '%s'%d, c[VALUE])),\n# string.join(map(lambda s: '%s'%s,c[NAME])[:20],',')), centers),2)\n#pp(centers,1)\n\nprint string.join(map(display_cluster,map(lambda c: c[VALUE],centers)),'\\n')\n\n# save the old point assignments\nold_names = map(lambda center: center[NAME], centers)\n\n#\n# 1. assign each point to a center\n#\n\n# first empty the assignments of each center\nfor center in centers:\ncenter[NAME] = []\ncenter[SIZE] = 0\n\n# now assign the points\nfor point in points:\n# find the best center\nmin_dist = distance_func(point[VALUE], centers[0][VALUE])\nclosest_center = 0\nfor c in range(0,len(centers)):\ndist = distance_func(point[VALUE], centers[c][VALUE])\nif dist < min_dist:\nmin_dist = dist\nclosest_center = c\n\n# assing the point to that center\n#print 'Assigning %s to %s with score %s'%(\n# point[VALUE],\n# display_cluster(centers[closest_center][VALUE]),\n# min_dist)\ncenters[closest_center][NAME].append(point[NAME])\ncenters[closest_center][SIZE] = centers[closest_center][SIZE]+1\n\n#\n# 2. reevaluate each center\n#\nfor center in centers:\n\n# one could turn this into EM by changing the weighting\n# of each point into the probability of that point\n# belonging to that center\ncenter[VALUE] = ps2c_func(tools.mget(point_dic,center[NAME]))\n\n#\n# to decide if you should stop, evaluate the spread\n# or simply measure how much each center is moving each time\n#\n\nnew_names = map(lambda center: center[NAME], centers)\n\nadditions = map(lambda new,old: tools.set_subtract(new,old),new_names,old_names)\nsubtractions = map(lambda new,old: tools.set_subtract(old,new),new_names,old_names)\n\nif iter>2: #tools.sum(distances) <= .01*len(distances) or iter > 10:\nbreak\nelse:\nprint \"Before: \"+string.join(map(lambda add: '%2s'%len(add),old_names))\nprint \"Subtractions: \"+string.join(map(lambda add: '%2s'%len(add),subtractions))\nprint \"Additions: \"+string.join(map(lambda add: '%2s'%len(add),additions))\nprint \"After: \"+string.join(map(lambda add: '%2s'%len(add),new_names))\n\ncenters.sort(lambda c1,c2: c2[SIZE]-c1[SIZE])\nreturn map(lambda c: (c[NAME],c[VALUE],c[SIZE]), centers)\n\n############################################################\n#\n# NORMALIZATION\n#\n############################################################\n\ndef mean0_stdev1(points):\nnew_points = []\nfor point in points:\n\nmean = tools.avg(point[VALUE])\nstdev = tools.stdev(point[VALUE])\n\nnew_value = map(lambda x,m=mean,s=stdev: (x-m)/s, point[VALUE])\n\n#print \"Old mean: %s, old stdev: %s\"%(mean,stdev)\n#print \"New mean: %s, new stdev: %s\"%(tools.avg(new_value),tools.stdev(new_value))\n\nnew_point = (point[NAME], new_value)\n\nnew_points.append(new_point)\n\nreturn new_points\n\ndef restore_points(clusters, old_points):\n# the clustering was done using normalized data points,\n# hence the cluster values do not really correspond to anything\n# meaningful. However, the names of the points are correct.\n#\n# here, we restore the original values of the points, based on\n# their names, and we recompute the cluster value as the average\n# of the unnormalized values the names correspond to\n\n# make a lookup table for the original values\npoint_dic = {}\ntools.mset(point_dic,\nmap(lambda p: p[NAME], old_points),\nmap(lambda p: p[VALUE], old_points))\n\n# construct the new clusters\nnew_clusters = []\nfor cluster in clusters:\nrestored_values = []\nfor name in cluster[NAME]:\nrestored_values.append(point_dic[name])\n\nnew_cluster = (cluster[NAME],\nweighted_average(restored_values,\n[1]*len(restored_values)),\ncluster[SIZE])\n\nnew_clusters.append(new_cluster)\n\ndef normalize_and_cluster(clustering_function, normalization_function, points):\n\n# normalize points\nnew_points = normalization_function(points)\n\n# compute cluster based on normalized points\nclusters = clustering_function(new_points)\n\n# recompute cluster mean based on original value\nrestore_points(clusters, points)\n\nreturn clusters\n\n############################################################\n#\n# CLUSTERING in ANY SPACE\n#\n############################################################\n\ndef follow_references(references, clusters, i, debug=0):\nif debug: sys.stdout.write('Looking for %s'%i)\nwhile 1:\nif clusters.has_key(i): break\ni = references[i]\nif debug: sys.stdout.write('->%s'%i)\nif debug: sys.stdout.write(' Found!\\n')\nreturn i\n\n## def generic_clustering(points, distances, threshold, points2cluster, cluster2points, compare_clusters, join_clusters, debug=0):\n\n## # distances is a list: [(i,j,dist_ij),\n## # (i,k,dist_ik),\n## # (j,k,dist_jk)]\n## # threshold is the minimum distance threshold\n## #\n## # doesn't really matter what the points are,\n## # we simply use their indices.\n\n## if debug: print \"Gathering all points indices\"\n## assert(len(tools.unique(points))==len(points)) # make sure all points are unique\n## distances = tools.my_sort(distances,lambda d: -d[2]) # sort the distances from best score to worst\n## points_with_distances = tools.unique(tools.flatten(map(lambda d: [d[0],d[1]],distances))) # get the point names from distance table\n## assert(len(tools.set_intersect(points_with_distances,points))==len(points_with_distances)) # make sure all points mentioned are listed\n## if tools.set_subtract(points,points_with_distances): # and see which have no distances whatsoever\n## print \"%s points with distances / %s points total (%2.0f%%).\\nNo dist info for %s\"%(\n## len(points_with_distances),len(points),100.0*len(points_with_distances)/len(points),\n## tools.set_subtract(points,points_with_distances))\n## # this step here is not needed. moreover, i'd like to output clusters in order of input points\n## #points = tools.unique(points_with_distances+points) # then list all points together\n\n## #print \"Points are %s\"%points\n\n## # here's a little trick on actually how we look indices up:\n## # instead of just looking at them in order, we first sort the\n## # index pairs, by the distance that separates them.\n## #\n## # then since, we always look things up in order, we'll end up\n## # joining the shortest motifs together first\n\n## if debug:\n## print \"Making a dictionary for quick distance lookup\"\n\n## distdic = {}\n## for i,j,dist in distances:\n## distdic[(min(i,j),max(i,j))] = dist\n\n## if debug:\n## print \"Distance dictionary is: \"\n## pp(distdic)\n\n## if debug:\n## print \"%s points defined by %s distances\"%(len(points),len(distances))\n## print \"Distance distribution is: \"\n## tools.quick_histogram(tools.cget(distances,2))\n\n## if debug: print \"Making every point its own cluster\"\n## references = {}\n## clusters = {}\n## used_names = {}\n## tools.mset(used_names,points,[None]*len(points))\n## maxi,p_i = -1,0\n## while p_i < len(points):\n## maxi = maxi+1\n## if used_names.has_key(maxi): continue\n## # we found an index for the cluster\n## clusters[maxi] = points2cluster([points[p_i]])\n## references[points[p_i]] = maxi\n## p_i = p_i+1\n\n## if debug:\n## print \"The clusters are: \"\n## pp(clusters,1)\n\n## #maxi = len(clusters)\n\n## hierarchy = {}\n## for cluster,elements in clusters.items():\n## assert(len(elements)==1)\n## hierarchy[cluster] = elements[0]\n\n## #print \"Hierarchy is %s\"%hierarchy\n\n## join_score = {}\n\n## suggestions = {}\n## vetos = {}\n## #references = {}\n\n## comparison_cache = {}\n\n## # two ways or sorting the order we visit the pairs\n## # 1) sort items\n## # 2) sort pairs\n\n## # 1) this is the sorting of the items: bad\n## # coz it leads to 1+1,2+1,3+1,4+1,5+1 joinings\n## # keep trying until no more joins are made\n## #indices = tools.my_sort(clusters.keys())\n## #pairs = []\n## #for i in range(0,len(indices)):\n## # for j in range(i+1,len(indices)):\n## # pairs.append((indices[i],indices[j]))\n## #pairs = tools.my_sort(pairs,max)\n\n## # 2) this is the sorting of the pairs\n## # this is better coz it does 1+1,1+1,2+2,1+1,4+2, etc\n## pairs = map(None, tools.cget(distances,0),tools.cget(distances,1))\n\n## # i will only compare points that had *any* similarity to start with\n\n## #boom\n\n## retry_all = 1\n## while retry_all:\n\n## if debug: print \"Re-starting all the loops (%s by %s)\"%(len(clusters),len(clusters))\n## if debug: print \"Sizes are: %s\"%tools.describe_elements(map(len,clusters.values()),lambda size: -size)\n\n## retry_all = 0\n\n## for i_tmp,j_tmp in pairs:\n\n## # probably joined and renamed\n## if clusters.has_key(i_tmp): i = i_tmp\n## else: i = follow_references(references, clusters, i_tmp, debug=debug)\n\n## if clusters.has_key(j_tmp): j = j_tmp\n## else: j = follow_references(references, clusters, j_tmp, debug=debug)\n\n## if debug: print \"Testing Pair (%s,%s) now in clusters (%s,%s)\"%(i_tmp,j_tmp,i,j)\n\n## # it is possible that we've already joined the clusters they now belong in\n## if i==j: continue\n\n## # cluster2cluster_distance\n\n## #if debug:\n## # print \"Comparison cache is\"\n## # pp(comparison_cache,1)\n\n## # PLEASE NOTE!! IF WE RE-USE CLUSTER NAMES, WE SHOULD FLUSH THE CACHE\n## # AT EVERY ITERATION\n## if comparison_cache.has_key((i,j)):\n## score = comparison_cache[(i,j)]\n## else:\n## score = compare2clusters(distdic, clusters[i], clusters[j])\n## comparison_cache[(i,j)] = score\n\n## if debug: print \"Avg max linkage between %s and %s is %s for %s%% tested and %s for %s tested\"%(\n## i,j,value1,perc1,value2,perc2)\n\n## if score > threshold:\n\n## # now i know i'm joining, increment maxi\n## maxi = maxi+1\n\n## # make a new cluster out of the two joined ones\n## newcluster = join_clusters(clusters[i],clusters[j])\n## del(clusters[i])\n## del(clusters[j])\n## clusters[maxi] = newcluster\n\n## hierarchy[maxi] = (hierarchy[i],hierarchy[j])\n## join_score[maxi] = weighted_linkage\n## del(hierarchy[i])\n## del(hierarchy[j])\n\n## references[i] = maxi\n## references[j] = maxi\n\n## #clusters[i] = newcluster\n\n## retry_all = 1\n\n## keys = tools.cget(tools.my_sort(clusters.items(), lambda item: min(item[1])),0)\n## #keys = tools.my_sort(clusters.keys())\n## #keys = tools.my_sort(keys,lambda k,join_score=join_score: -(join_score.has_key(k) and join_score[k]))\n## # resorting the keys to put singletons at the end\n\n## #singletons = tools.lte(keys,len(points))\n## #joined = tools.gt(keys,len(points))\n## #keys = joined+singletons\n\n## groups = tools.mget(clusters,keys)\n## hierarchy = tools.mget(hierarchy, keys)\n\n## #pp(hierarchy,3)\n\n## linkages = []\n## for group in groups:\n## linkage = []\n## for m1 in range(0,len(group)):\n## for m2 in range(m1+1,len(group)):\n## if distdic.has_key((group[m1],group[m2])):\n## linkage.append(distdic[(group[m1],group[m2])])\n## else:\n## linkage.append(0)\n## #linkages.append(tools.avg(linkage))\n## linkages.append(tools.reverse(tools.my_sort(linkage)))\n\n## #linkages, groups = tools.unpack(tools.reverse(tools.my_sort(map(None,linkages,groups))))\n## #lens, avglink, linkages, groups = tools.unpack(tools.reverse(tools.my_sort(\n## # map(None,\n## # map(len,groups),\n## # map(tools.avg,linkages),\n## # linkages,\n## # groups))))\n\n## return groups, linkages, hierarchy\n\ndef avg_max_linkage(points, distances, threshold, linkage_type='max', debug=0):\n\n# distances is a list: [(i,j,dist_ij),\n# (i,k,dist_ik),\n# (j,k,dist_jk)]\n# threshold is the minimum distance threshold\n#\n# doesn't really matter what the points are,\n# we simply use their indices.\n\nif debug: print \"Gathering all points indices\"\ndistances = tools.my_sort(distances,lambda d: -d[2])\npoints_with_distances = tools.unique(tools.flatten(map(lambda d: [d[0],d[1]],distances)))\nassert(len(tools.set_intersect(points_with_distances,points))==len(points_with_distances))\nif tools.set_subtract(points,points_with_distances):\nprint \"%s points with distances / %s points total (%2.0f%%).\\nNo dist info for %s\"%(\nlen(points_with_distances),len(points),100.0*len(points_with_distances)/len(points),\ntools.set_subtract(points,points_with_distances))\npoints = tools.unique(points_with_distances+points)\n\n#print \"Points are %s\"%points\n\n# here's a little trick on actually how we look indices up:\n# instead of just looking at them in order, we first sort the\n# index pairs, by the distance that separates them.\n#\n# then since, we always look things up in order, we'll end up\n# joining the shortest motifs together first\n\nif debug:\nprint \"Making a dictionary for quick distance lookup\"\n\ndistdic = {}\nfor i,j,dist in distances:\ndistdic[(min(i,j),max(i,j))] = dist\n\nif debug:\nprint \"Distance dictionary is: \"\npp(distdic)\n\nif debug:\nprint \"%s points defined by %s distances\"%(len(points),len(distances))\nprint \"Distance distribution is: \"\ntools.quick_histogram(tools.cget(distances,2))\n\nif debug: print \"Making every point its own cluster\"\nreferences = {}\nclusters = {}\nused_names = {}\ntools.mset(used_names,points,[None]*len(points))\nmaxi,p_i = -1,0\nwhile p_i < len(points):\nmaxi = maxi+1\nif used_names.has_key(maxi): continue\n# we found an index for the cluster\nclusters[maxi] = [points[p_i]]\nreferences[points[p_i]] = maxi\np_i = p_i+1\n\nif debug:\nprint \"The clusters are: \"\npp(clusters,1)\n\n#maxi = len(clusters)\n\nhierarchy = {}\nfor cluster,elements in clusters.items():\nassert(len(elements)==1)\nhierarchy[cluster] = elements[0]\n\n#print \"Hierarchy is %s\"%hierarchy\n\njoin_score = {}\n\nsuggestions = {}\nvetos = {}\n#references = {}\n\ncomparison_cache = {}\n\n# two ways or sorting the order we visit the pairs\n# 1) sort items\n# 2) sort pairs\n\n# 1) this is the sorting of the items: bad\n# coz it leads to 1+1,2+1,3+1,4+1,5+1 joinings\n# keep trying until no more joins are made\n#indices = tools.my_sort(clusters.keys())\n#pairs = []\n#for i in range(0,len(indices)):\n# for j in range(i+1,len(indices)):\n# pairs.append((indices[i],indices[j]))\n#pairs = tools.my_sort(pairs,max)\n\n# 2) this is the sorting of the pairs\n# this is better coz it does 1+1,1+1,2+2,1+1,4+2, etc\npairs = map(None, tools.cget(distances,0),tools.cget(distances,1))\n\n#boom\n\nretry_all = 1\nwhile retry_all:\n\nif debug: print \"Re-starting all the loops (%s by %s)\"%(len(clusters),len(clusters))\nif debug: print \"Sizes are: %s\"%tools.describe_elements(map(len,clusters.values()),lambda size: -size)\n\nretry_all = 0\n\nfor i_tmp,j_tmp in pairs:\n\n# probably joined and renamed\nif clusters.has_key(i_tmp): i = i_tmp\nelse: i = follow_references(references, clusters, i_tmp, debug=debug)\n\nif clusters.has_key(j_tmp): j = j_tmp\nelse: j = follow_references(references, clusters, j_tmp, debug=debug)\n\nif debug: print \"Testing Pair (%s,%s) now in clusters (%s,%s)\"%(i_tmp,j_tmp,i,j)\n\n# it is possible that we've already joined the clusters they now belong in\nif i==j: continue\n\n# cluster2cluster_distance\n\n#if debug:\n# print \"Comparison cache is\"\n# pp(comparison_cache,1)\n\n# PLEASE NOTE!! IF WE RE-USE CLUSTER NAMES, WE SHOULD FLUSH THE CACHE\n# AT EVERY ITERATION\nif comparison_cache.has_key((i,j)):\nvalue1, perc1, value2, perc2, weighted_linkage = comparison_cache[(i,j)]\nelse:\nvalue1, perc1 = get_avg_linkage(distdic, clusters[i], clusters[j], linkage_type)\nvalue2, perc2 = get_avg_linkage(distdic, clusters[j], clusters[i], linkage_type)\nweighted_linkage = tools.weighted_avg([value1,value2],[len(clusters[i]),len(clusters[j])])\ncomparison_cache[(i,j)] = (value1,perc1,value2,perc2,weighted_linkage)\n\nif debug: print \"Avg max linkage between %s and %s is %s for %s%% tested and %s for %s tested\"%(\ni,j,value1,perc1,value2,perc2)\n\nif 0: #join_score.has_key(i) and join_score.has_key(j):\nif weighted_linkage < .5*min(join_score[j],join_score[i]):\n#if 0: print \"I veto joining %s to %s. (%2.0f,%2.0f) -> %2.0f is too big a drop in linkage\"%(\n# tools.display_list(clusters[i],format='m%s'),\n# tools.display_list(clusters[j],format='m%s'),\n# join_score[j],join_score[i],weighted_linkage)\n#suggestions[(i,j)] = weighted_linkage\npass\n\nif value1*perc1/100.0 > threshold and value2*perc2/100.0 > threshold:\n\n#if join_score.has_key(i) and join_score.has_key(j):\n# if weighted_linkage < .5*min(join_score[j],join_score[i]):\n# print \"I veto joining %s to %s. (%2.0f,%2.0f) -> %2.0f is too big a drop in linkage\"%(\n# tools.display_list(clusters[i],format='m%s'),\n# tools.display_list(clusters[j],format='m%s'),\n# join_score[j],join_score[i],weighted_linkage)\n# continue\n\n# now i know i'm joining, increment maxi\nmaxi = maxi+1\n\nif debug:\n\nif join_score.has_key(i): join_score_i = '%2.0f linkage'%join_score[i]\nelse: join_score_i = 'singleton'\nif join_score.has_key(j): join_score_j = '%2.0f linkage'%join_score[j]\nelse: join_score_j = 'singleton'\n\nif debug: print \"Joining clu_%s (%s items, %s) and clu_%s (%s items, %s) -> clu_%s (%s items, %2.0f linkage)\"%(\ni,len(clusters[i]),join_score_i,\nj,len(clusters[j]),join_score_j,\nmaxi,len(clusters[i])+len(clusters[j]),weighted_linkage)\n\n# make a new cluster out of the two joined ones\nnewcluster = clusters[i]+clusters[j]\ndel(clusters[i])\ndel(clusters[j])\nclusters[maxi] = newcluster\n\nhierarchy[maxi] = (hierarchy[i],hierarchy[j])\njoin_score[maxi] = weighted_linkage\ndel(hierarchy[i])\ndel(hierarchy[j])\n\nreferences[i] = maxi\nreferences[j] = maxi\n\n#clusters[i] = newcluster\n\nretry_all = 1\n\nkeys = tools.cget(tools.my_sort(clusters.items(), lambda item: min(item[1])),0)\n#keys = tools.my_sort(clusters.keys())\n#keys = tools.my_sort(keys,lambda k,join_score=join_score: -(join_score.has_key(k) and join_score[k]))\n# resorting the keys to put singletons at the end\n\n#singletons = tools.lte(keys,len(points))\n#joined = tools.gt(keys,len(points))\n#keys = joined+singletons\n\ngroups = tools.mget(clusters,keys)\nhierarchy = tools.mget(hierarchy, keys)\n\n#pp(hierarchy,3)\n\nlinkages = []\nfor group in groups:\nlinkage = []\nfor m1 in range(0,len(group)):\nfor m2 in range(m1+1,len(group)):\nif distdic.has_key((group[m1],group[m2])):\nlinkage.append(distdic[(group[m1],group[m2])])\nelse:\nlinkage.append(0)\n#linkages.append(tools.avg(linkage))\nlinkages.append(tools.reverse(tools.my_sort(linkage)))\n\n#linkages, groups = tools.unpack(tools.reverse(tools.my_sort(map(None,linkages,groups))))\n#lens, avglink, linkages, groups = tools.unpack(tools.reverse(tools.my_sort(\n# map(None,\n# map(len,groups),\n# map(tools.avg,linkages),\n# linkages,\n# groups))))\n\nreturn groups, linkages, hierarchy\n\ndef get_avg_linkage(distdic, cluster1, cluster2, linkage_type='max'):\n# returns the avg\n\nmax_similarity = []\n\n# for every poitn in cluster1\nfor i in cluster1:\n\n# find the closest point in cluster2\n#dists_i = tools.mget(distdic, map(lambda j,i=i: (min(i,j),max(i,j)), cluster2), 0)\ndists_i = []\nfor j in cluster2:\n\nkey = min(i,j),max(i,j)\nif distdic.has_key(key): dists_i.append(distdic[key])\n\n# and average those distances\nif dists_i:\nif linkage_type=='avg': max_similarity.append(tools.avg(dists_i))\nelif linkage_type=='max': max_similarity.append(max(dists_i))\nelif linkage_type=='min': max_similarity.append(min(dists_i))\nelse: raise ValueError, linkage_type\n#if dists_i: max_similarity.append(max(dists_i))\n\n# then return the avg max similarity, and well as the percent for which we had values\nreturn tools.avg(max_similarity), 100.0*len(max_similarity)/len(cluster1)\n\ndef cluster_clusters(grouping1, grouping2):\n\ngroups, linkages, hierarchies = grouping1\nggroups, glinkages, ghierarchies = grouping2\n\nnewgroups, newlinkages, newhierarchies = [], [], []\nfor ggroup,glinkage,ghierarchy in map(None, ggroups, glinkages, ghierarchies):\n\nnewhierarchies.append(tools.map_on_hierarchy_safe(ghierarchy, tools.list2dic_i(hierarchies).get))\n\nnewgroups.append(tools.flatten(tools.mget(groups, ggroup)))\n\n#print \"Linkages of the previous groups: %s\"%tools.mget(linkages,ggroup)\n#print \"Current linkage: %s\"%glinkage\nnewlinkages.append(tools.flatten(tools.mget(linkages,ggroup))+glinkage)\n\nreturn newgroups, newlinkages, newhierarchies"
    },
    {
      "category": "Resource",
      "title": "tools.py",
      "type": "PY",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/bec40c1e656b3310ce654955c1ee8b95_tools.py",
      "content": "from os import system\nimport time\nimport os\nimport stat\nimport string\nimport operator\nimport sys\nimport math\nimport cPickle\nimport types\nimport random\n\n# category 1\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### BOOLEAN FUNCTIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Adding scheme-like capabilities to if expressions\n#\n############################################################\n\ndef ifab(test, a, b):\n\"\"\"x = ifab(test, a, b)\nWARNING: Both 'a' and 'b' are evaluated\nC equivalent: x = test?a:b;\nScheme equiv: (set x (if test a b))\nPython equiv: test and a or b\nNone of the equivalents evaluates both arguments\n\"\"\"\nif test: return a\nelse: return b\n\ndef case(variable, case2value, default=None):\nif case2value.has_key(variable):\nreturn case2value[variable]\nelse:\nif default==None:\nraise \"Unexpected value\", \"%s not in dictionary %s\"%(\nvariable, case2value.keys())\nelse:\nreturn default\n\ndef my_not(a):\nreturn not a\n\ndef xor(a,b):\nreturn (a and not b) or (not a and b)\n\ndef filter_not(func, list):\nnewlist = []\nfor item in list:\nif not func(item): newlist.append(item)\nreturn newlist\n\ndef modify_and_return(f, a):\n# useful when you need to call a function on a modifiable\n# object, and reuse the modified object in your next operation\nf(a)\nreturn a\n\ndef wait_a_bit(how_long):\nlist = reverse(range(0,how_long))\nfor i in range(0,len(list)):\nfor j in range(0,len(list)):\nmap(lambda x: x==3, list)\n\n# category 2\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### PRINTING ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Recursing and exploring any datastructure (except class)\n#\n############################################################\n\ndef pp(d,level=-1,maxw=0,maxh=0,parsable=0):\n\"\"\" wrapper around pretty_print that prints to stdout\"\"\"\nif not parsable:\npretty_print(sys.stdout, d, level, maxw, maxh, '', '', '')\nelse:\nimport pprint\nif maxw: pp2 = pprint.PrettyPrinter(width=maxw, indent=1)#, depth=level\nelse: pp2 = pprint.PrettyPrinter(indent=1)#, depth=level\npp2.pprint(d)\n\ndef test_pp():\npp({'one': ('two',3,[4,5,6]),\n7: (lambda x: 8*9),\n'ten': ['ele', {'ven': 12,\n(13,14): '15'}]})\n\ndef pretty_print(f, d, level=-1, maxw=0, maxh=0, gap=\"\", first_gap='', last_gap=''):\n# depending on the type of expression, it recurses through its elements\n# and prints with appropriate indentation\n\n# f is the output file stream\n# d is the data structure\n#\n# level is the number of allowed recursive calls, the depth at which\n# the data structure is explored\n# default: -1 means never stop recursing early\n# maxw is the maximum width that will be printed from the last element\n# of the recursion (when no further recursion is possible, or\n# the maximal depth has been reached)\n# default: 0 means every line will be printed in its entirety, regardless\n# of how long it may be\n# maxh (max height) is the maximum number of elements that will be\n# printed from a list or a dictionary, at any level or recursion\n# default: 0 means every list or dictionary will have all its elements\n# printed, even if it contains thousands of elements\n#\n# gap is the gap to include before every element of a list/dic/tuple\n# first_gap is the opening gap before the opening bracket, parens or curly braces\n# first_gap is the closing gap before the closing bracket, parens or curly braces\n\nif level == 0:\nif type(d) != types.StringType: d = `d`\n\nif maxw and len(d) > maxw:\nfinal = ifab(maxw > 20, 10, maxw/2)\nf.write(first_gap+d[:maxw-final]+'...'+d[-final:]+' (%s chars)\\n' % len(d))\nelse: f.write(first_gap+d+'\\n')\nelif type(d) == types.ListType:\nif not d:\nf.write(first_gap+\"[]\\n\")\nreturn\n# recurse on lists\nf.write(first_gap+\"[\\n\")\nh = 0\nfor el in d:\npretty_print(f, el, level-1, maxw, maxh, gap+' ', gap+' ->', gap+' ')\nif maxh:\nh = h+1\nif h >= maxh and maxh<len(d):\nf.write(gap+' -> ... (%s in list)\\n'%len(d))\nbreak\nf.write(last_gap+\"]\\n\")\nelif type(d) == types.TupleType:\nif not d:\nf.write(first_gap+\"()\\n\")\nreturn\n# recurse on tuples\nf.write(first_gap+\"(\\n\")\nh = 0\nfor el in d:\npretty_print(f, el,\nlevel = level-1,\nmaxw = maxw,\nmaxh = maxh,\ngap = gap+' ',\nfirst_gap = gap+' =>',\nlast_gap = gap+' ')\nif maxh:\nh = h+1\nif h >= maxh and maxh<len(d):\nf.write(gap+' => ... (%s in tuple)\\n'%len(d))\nbreak\nf.write(last_gap+\")\\n\")\nelif type(d) == types.DictType:\nif not d:\nf.write(first_gap+\"{}\\n\")\nreturn\n# recurse on dictionaries\nf.write(first_gap+\"{\\n\")\nkeys = d.keys()\nkeys.sort()\nkey_strings = map(lambda k: ifab(type(k)==types.StringType, k, `k`), keys)\nmaxlen = max(map(len, key_strings))\nh = 0\nfor k,key_string in map(None, keys, key_strings):\nkey_string = sfill(key_string,maxlen,'.')\nblank_string = ' '*len(key_string)\npretty_print(f, d[k],\nlevel = level-1,\nmaxw = maxw,\nmaxh = maxh,\ngap = gap+' %s'%blank_string,\nfirst_gap = gap+' %s: '%key_string,\nlast_gap = gap+' %s'%blank_string)\nif maxh:\nh = h+1\nif h >= maxh and maxh<len(keys):\nremaining_keys = []\nfor k in keys[h:]:\nif type(k) == types.TupleType:\nremaining_keys.append(`k`)\nelse:\nremaining_keys.append('%s'%k)\nremaining_keys = string.join(remaining_keys,',')\n#f.write(gap+' %s (%s keys)\\n'%(remaining_keys, len(keys)))\npretty_print(f, ' %s (%s keys)'%(remaining_keys, len(keys)),0,maxw,0,\ngap,gap,'')\nbreak\n\n#gap+' '*(len(key_string)+3), '', gap+' '*(len(key_string)+5))\nf.write(last_gap+\"}\\n\")\nelif type(d) == types.InstanceType:\nfields = dir(d)\n\nif not fields:\nf.write(first_gap+\"*EmptyClass*\\n\")\nreturn\n# recurse on classes\nf.write(first_gap+\"*ClassInstance %s\\n\"%d)\nfields.sort()\nkey_strings = map(lambda k: ifab(type(k)==types.StringType, k, `k`), fields)\nmaxlen = max(map(len, key_strings))\nh = 0\nfor k,key_string in map(None, fields, key_strings):\nkey_string = sfill(key_string,maxlen,'.')\nblank_string = ' '*len(key_string)\npretty_print(f, eval('d.'+k),\nlevel = level-1,\nmaxw = maxw,\nmaxh = maxh,\ngap = gap+' %s'%blank_string,\nfirst_gap = gap+' %s: '%key_string,\nlast_gap = gap+' %s'%blank_string)\nif maxh:\nh = h+1\nif h >= maxh and maxh<len(keys):\nremaining_keys = []\nfor k in keys[h:]:\nif type(k) == type(()):\nremaining_keys.append(`k`)\nelse:\nremaining_keys.append('%s'%k)\nremaining_keys = string.join(remaining_keys,',')\n#f.write(gap+' %s (%s keys)\\n'%(remaining_keys, len(keys)))\npretty_print(f,\n' %s (%s keys)'%(remaining_keys, len(keys)),\n0,\nmaxw,\n0,\ngap,\ngap,\n'')\nbreak\n\n#gap+' '*(len(key_string)+3), '', gap+' '*(len(key_string)+5))\nf.write(last_gap+\"*\\n\")\nelif type(d) == type(\"\"):\n# simply print strings (no quotes)\nif maxw and len(d)>maxw:\nfinal = ifab(maxw > 20, 10, maxw/2)\nf.write(first_gap+d[:maxw-final]+'..'+d[-final:]+' (%s)\\n' % len(d))\nelse:\nf.write(first_gap+d+'\\n')\nelse:\n# string conversion of all other types\nif maxw and len(`d`)>maxw:\nfinal = ifab(maxw > 20, 10, maxw/2)\nf.write(first_gap+`d`[:maxw-final]+'..'+`d`[-final:]+' (%s)\\n' % len(`d`))\nelse:\nf.write(first_gap+`d`+'\\n')\n\n############################################################\n#\n# Functions for printing\n#\n############################################################\n\ndef disp(x):\n# useful for using print as a function.\n# ex: map(disp, ['hello','there','how','are','you'])\nprint x\n\ndef display_list(list, join_char=', ', format='%s'):\n# joins all the elements of a string together\n#\n# bugs: it will bug if any list element is a tuple,\n# coz the format will complain of too few %s.\nreturn string.join(map(lambda el,format=format: format%el, list), join_char)\n\ndef display_bignum(n,digits=1):\n\n# display a big number for which you don't really want to know\n# all the digits\n\n# n is the number\n# digits are the number of significant decimal places to which\n# the magnitude of the number is to be known, at the appropriate unit\n\n# ex: display_bignum(1300403) -> 1.3M\n# ex: display_bignum(13004) -> 130.0k\n# ex: display_bignum(134) -> 134.0\n\nsign = ifab(n>=0,'','-')\nn = int(abs(n))\nif n < 1000: return sign+`n`\nelif n < 1000000: k,units = 3,'k'\nelif n < 1000000000: k,units = 6,'M'\nelif n < float(1000000000000): k,units = 9,'G'\nelif n < float(1000000000000000): k,units = 12,'T'\nelif n < float(1000000000000000000): k,units = 15,'P'\nelse: return sign+`n`\nmain = `n`[:-k]\n\nif digits:\ntry: decimal = `n`[-k:][:digits]\nexcept: decimal = `0`\nreturn sign+main+'.'+string.replace(decimal,'.','')+units\nelse:\nreturn sign+main+units\n\ndef dec(n):\n# adds commas between the different magnitudes of a number\n# 134 -> 134\n# 13402 -> 13,402\n# 134020 -> 134,020\n# 134020.7 -> 134,020.7\n\nmaybe_dec = string.split(`n`,'.')\nif len(maybe_dec) == 2:\nn,decimal_part = int(maybe_dec[0]), '.'+maybe_dec[1]\nelse:\ndecimal_part = ''\n\nsign = ifab(n>=0,'','-')\nn = abs(n)\nif n < 1000: return sign+`n`+decimal_part\nelif n < 1000000: return sign+`n`[:-3]+','+`n`[-3:]+decimal_part\nelif n < 1000000000: return sign+`n`[:-6]+','+`n`[-6:-3]+','+`n`[-3:]+decimal_part\nelse: return sign+`n`[:-9]+','+`n`[-9:-6]+','+`n`[-6:-3]+','+`n`[-3:]+decimal_part\n\ndef safe_div(num, den, default):\nif den: return num/den\nelse: return default\n\ndef safe_float(num, den, format='%2.2f'):\nif num and not den: return 'Inf'\nif not num and not den: return '?'\nreturn format%(float(num)/float(den))\n\ndef safe_percentile(num, den, format='%2.1f%%'):\nif num and not den: return 'Inf'\nif not num and not den: return '?'\nreturn format%(100*float(num)/float(den))\n\ndef safe_min(list, default):\nif not list: return default\nreturn min(list)\n\ndef safe_max(list, default):\nif not list: return default\nreturn max(list)\n\ndef perc(num, den, format='%s/%s (%s)'):\nreturn format%(num,den,safe_percentile(num,den))\n\ndef display_within_range(i, increment, guessmax, format='%s-%s'):\nlow, high = within_range(i, increment)\nlow = string.zfill(low,len(`guessmax`))\nhigh = string.zfill(high,len(`guessmax`))\nreturn format%(low, high)\n\ndef within_range(i, increment):\n# if i'm splitting directories every 100, and i'm on contig 53,\n# then i'm the range (0,99).\n# 153 -> (100,199)\nlow = (i/increment)*increment\nhigh = low+increment-1\nreturn low, high\n\ndef display_fraction(num, den):\n# displays a franction in the nicest possible form\n# simplifies the fraction if possible.\n# uses .75, .5 and .25 instead of 9/4 and so on\n\nif den == 1:\n#print \"case1\"\nreturn `num`\nif num == 1:\n#print \"case2\"\nreturn '1/'+`den`\nif num == 0:\n#print \"case3\"\nreturn '0'\nif den % num == 0:\n#print \"case4\"\nreturn display_fraction(1, den/num)\nfor n in range(2,den+1):\n#print \"Trying case 5 for n=%d\"%n\nif num % n == 0 and den % n == 0:\n#print \"case5 for n=%d\"%n\nreturn display_fraction(num/n, den/n)\nif den == 2:\n#print \"case6\"\nreturn ifab(num, `num/den`+'.5', '.5')\nif den == 4:\n#print \"case7\"\nif num%4 == 1: return ifab(num, `num/den`+'.25', '.25')\nif num%4 == 3: return ifab(num, `num/den`+'.75', '.75')\n#print \"case8\"\nreturn `num`+'/'+`den`\n\ndef lower_bound_mod(i, k):\n# find the largest number x, which is smaller than i,\n# and still divisible by k\n\nreturn k*(i / k)\n\ndef upper_bound_mod(i, k):\n# find the largest number x, which is smaller than i,\n# and still divisible by k\n\nreturn k*(i / k + 1)\n\n# category 3\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### TEXT ART - or drawing with TEXT ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Printing intervals based on x coordinates\n#\n############################################################\n\ndef print_scale(scale = {},#'min':0,'max':100,'tick':10, 'width': 120,'print_scale':1},\nitems = [[(20,40,'Hello'),\n(60,80,'World!'),\n(90,94,':o)')],\n[(40,60,'Niiice'), (90,87,'(o:')],\n[(0,100,'A hundred')]]):\nwrite_scale(sys.stdout, scale, items)\n\ndef write_scale(f, scale, items):\n\nif not items:\nprint \"Write_scale: Nothing to display\"\nreturn\n\nif not scale.has_key('width'): scale['width'] = 120\nif not scale.has_key('min'): scale['autoscale'] = 1\n\ndef add_range(line1, line2, start, end, text, char='=', scale=scale):\ndef convert_coord(coord, scale):\nreturn int(float(float(coord-scale['min'])\n* scale['width'])\n/ (scale['max']-scale['min']))\nx1,x2 = convert_coord(start, scale), convert_coord(end, scale)\n#print \"start=%s -> %s end=%s -> %s\" % (start,x1,end,x2)\nif x1>x2: x1,x2,up=x2,x1,0\nelse: up=1\nfor i in range(x1,x2):\ntry: line2[i] = char\nexcept IndexError: pass\nif up: start_char,end_char = '|','>'\nelse: start_char,end_char = '<','|'\n\ntry: line2[x2] = end_char\nexcept IndexError: pass\ntry: line2[x1] = start_char\nexcept IndexError: pass\nfor i in range(0, len(text)):\ntry: line1[x1+1+i] = text[i]\nexcept IndexError: pass\n\n# autoscale if you have to\nif scale.has_key('autoscale') and scale['autoscale']:\nscale['min'] = min(map(lambda i: min(map(lambda a: min(a[0],a[1]),i)),items))\nscale['max'] = max(map(lambda i: max(map(lambda a: max(a[0],a[1]),i)),items))\nif scale.has_key('print_scale') and scale['print_scale'] and scale.has_key('autotick') and scale['autotick']:\nif scale.has_key('numticks'): num_ticks = scale['numticks']\nelse: num_ticks = 5\ntick = (scale['max']-scale['min'])/num_ticks\nscale['tick'] = int(`tick`[0]+'0'*(len(`tick`)-1))\n\n# Print a coordinate scale with ticks every tick\nif scale.has_key('print_scale') and scale['print_scale']:\nline1,line2 = [' '] * (scale['width']+scale['tick']), [' '] * (scale['width']+1)\nmin_tick = (scale['min']/scale['tick']+1)*scale['tick']\nif (min_tick - scale['min'])<scale['tick']/2: min_tick = min_tick+scale['tick']\nadd_range(line1,line2,scale['min'],min_tick,\ndisplay_bignum(scale['min'],0)\n,'_')\nfor mark in range(min_tick, scale['max'], scale['tick']):\nadd_range(line1, line2, mark, mark+scale['tick'],\ndisplay_bignum(mark,0), '_')\nadd_range(line1,line2,scale['max'],scale['max'],\ndisplay_bignum(scale['max'],0),\n'_')\nf.write(string.join(line1,'')[:scale['width']+10]+'\\n')\nf.write(string.join(line2,'')[:scale['width']+10]+'\\n')\n# print every line\nfor line in items:\nline1 = [' '] * (scale['width'] + 10) # 10 is the max text\nline2 = [' '] * (scale['width'] + 10)\nfor item in line:\nif len(item)>3: dot=item[3]\nelse: dot = '='\nadd_range(line1, line2, item[0], item[1], item[2], dot)\nf.write(string.join(line1,'')+'\\n')\nf.write(string.join(line2,'')+'\\n')\n\n############################################################\n\ndef make_annotation(seq, orfs, start_offset):\n\ngapless_length = len(seq)-string.count(seq,'-')\n\nprint \"Making annotation on sequence of length %s\"%(len(seq)-string.count(seq,'-'))\nprint \"Start offset is %s and coords are %s\"%(\nstart_offset, map(lambda s,e: '%s-%s'%(s,e),cget(orfs,'start'),cget(orfs,'end')))\n\n# takes a sequence, a\nnames = map(lambda o: o['gene'] or o['name'], orfs)\n#starts = vector_scalar_sub(cget(orfs,'start'),start_offset)\n#ends = vector_scalar_sub(cget(orfs,'end'),start_offset)\nstarts,ends = [],[]\nfor orf in orfs:\nstarts.append(orf['start']-start_offset)\nends.append(orf['end']-start_offset+ifab(orf['up'],1,-1))\nups = cget(orfs,'up')\n\nfor i in range(0,len(starts)):\nif starts[i] < 0: starts[i] = 0\nif ends[i] > gapless_length: ends[i] = gapless_length\n\ncoords = coords_seq2ali(seq,flatten([starts,ends]))\nstarts,ends = coords[:len(coords)/2],coords[len(coords)/2:]\n\nannotation_list = []\nfor start,end,up,name in map(None,starts,ends,ups,names):\n\nif up: first,last = 'S->','E'\nelse: first,last = 'E-','<-S'\n\nannotation_list.append((start,end,name,first,'=',last))\n\nreturn make_annotation_string(' '*len(seq),\nannotation_list)\n\ndef make_annotation_string(str, annotation_list):\n# anotation_list is [(name,start,end,'[','=',']'),\n# (name,start,end,...),...]\nstr = map(None, str)\nfor a in annotation_list:\nadd_annotation_string(str,a)\nreturn string.join(str,'')\n\ndef add_annotation_string(str, a):\n# a is (name,start,end,'[','=',']')\nstart, end, name, first_char, middle_char, last_char = a\nannotation = sfill(first_char+name[:end-start],\nend-start,\nmiddle_char)\nif last_char: annotation = annotation[:-1]+last_char\n#print \"Annotation is '%s'\"%annotation\nsadd(str,annotation,start)\n\n############################################################\n#\n# 2D plots\n#\n############################################################\n\ndef test_multi_plot():\nx = range(0,100)\ny = map(lambda x: x*x, x)\n\nmulti_plot([[x,y,20,40],\n[y,x,30,20],\n[x,x,20,30],\n[y,y,10,10]],'')\n\ndef weird_plot():\nx = range(0,100)\ny = map(lambda x: x*x, x)\nz = map(lambda x: math.sin(x/4.0), x)\nz = map(lambda x: math.cos(x/4.0), x)\nz2 = map(lambda x: math.cos(x/10.0), x)\nz3 = map(lambda x: math.sin(x/10.0), x)\nlines1 = quick_plot_lines(x,y,18,18)\nlines2 = quick_plot_lines(y,x,18,18)\nlines3 = quick_plot_lines(z2+z3,reverse_list(x+x),40,40)\nlines4 = quick_plot_lines(z3+z2,x+x,40,40)\n\nlines = multi_plot_combine_lines([lines3,flatten([lines1,lines2]),lines4], ' ')\nmap(lambda l: sys.stdout.write(l+'\\n'), lines)\n\ndef quick_plot(xs,ys,width=60,height=30,logx=0,logy=0,minx=None,maxx=None,miny=None,maxy=None,f=sys.stdout):\n\nif minx!=None or maxx!=None or miny!=None or maxy!=None:\nxys = map(None,xs,ys)\nif minx!=None: xys = filter(lambda xy,minx=minx: xy[0]>=minx,xys)\nif maxx!=None: xys = filter(lambda xy,maxx=maxx: xy[0]<=maxx,xys)\nif miny!=None: xys = filter(lambda xy,miny=miny: xy[1]>=miny,xys)\nif maxy!=None: xys = filter(lambda xy,maxy=maxy: xy[1]<=maxy,xys)\nxs,ys = cget(xys,0),cget(xys,1)\n\nlines = quick_plot_lines(xs,ys,width,height,logx,logy)\nmap(lambda l,f=f: f.write(l+'\\n'), lines)\n\ndef multi_plot(x_y_w_hs,separator=' || ',f=sys.stdout):\nlines = multi_plot_lines(x_y_w_hs,separator)\nmap(lambda l,f=f: f.write(l+'\\n'), lines)\n\ndef multi_plot_lines(x_y_w_hs,separator):\n# first run quick_plot on each of the data sets\nall_lines = []\nfor xs,ys,w,h in x_y_w_hs:\nlines = quick_plot_lines(xs,ys,w,h)\nall_lines.append(lines)\nreturn multi_plot_combine_lines(all_lines, separator)\n\ndef multi_plot_combine_lines(all_lines, separator):\n# calculate all the widths\nformats = []\nfor lines in all_lines:\nwidth = max(map(len,lines))\nformat = '%-'+`width`+'s'\nformats.append(format)\n# then put them all together\npacked_lines = []\nfor line_i in range(0,max(map(len,all_lines))):\n\nrow = []\nfor data_set in all_lines:\n\nif len(data_set) <= line_i:\nrow.append('')\nelse:\nrow.append(data_set[line_i])\npacked_lines.append(row)\n\n# and now print them all out\nnew_lines = []\nfor line_set in packed_lines:\nto_print = []\nfor col,format in map(None,line_set,formats):\nto_print.append(format%col)\nnew_lines.append(string.join(to_print,separator))\nreturn new_lines\n\ndef quick_plot_lines(xs,ys,width,height,logx=0,logy=0):\n\nxs_orig, ys_orig = xs, ys\n\nif logx: xs = map(math.log, xs)\nif logy: ys = map(math.log, ys)\n\n# part I - constructing the empty bitmap\nrows = []\nfor row in range(0,height):\nrows.append([' ']*width)\n# part II - finding boundaries of data, and scaling factors\nminx, maxx = min(xs)-1, max(xs)+1\nminy, maxy = min(ys)-1, max(ys)+1\nscalex = (maxx-minx) / float(width-1)\nscaley = (maxy-miny) / float(height-1)\n\n# part III - filling in the appropriate pixels\nfor x,y in map(None, xs, ys):\n# IIIa - converting (x,y) to (i,j)\ni = int((x-minx) / scalex)\nj = int((y-miny) / scaley)\nassert(i>=0 and j>=0)\n#print \"(%s,%s) -> (%s,%s)\"%(x,y,i,j)\n\n# IIIb - incrementing point count\ntry:\nrows[j][i] = chint_inc(rows[j][i])\nexcept:\npass\n\nrows.reverse()\n\n# part IV - preparing the legend on the left\nlegend = ['']*len(rows)\nlegend[0]=`max(ys_orig)`\nlegend[-1]=`min(ys_orig)`\nlegend[len(legend)/2-1]='dy='\nlegend[len(legend)/2]=`max(ys_orig)-min(ys_orig)`\nlegwidth = max(map(len,legend))\n\n# part IV - displaying the filled in matrix\nlines = []\nlines.append(' '*legwidth+'^')\nfor i in range(0,len(rows)):\nlines.append(string.rjust(legend[i],legwidth)+'|'+string.join(rows[i],''))\nlines.append(' '*legwidth+'+%s>'%('-'*width))\ns = ' '*width\ns = sinsert(s,`minx`,0)\ns = sinsert(s,`max(xs_orig)`,width-len(`max(xs_orig)`))\nsub = 'dx='+`max(xs_orig)-min(xs_orig)`\ns = sinsert(s,sub,width/2-len(sub)/2)\nlines.append(' '*legwidth+' '+s)\nreturn lines\n\ndef int2chint(integer):\n# a chint is a character integer. Representing up to 61 in a single\n# character, after 0-9, we get a-z (10-35) and then A-Z (36-61)\n#\n# this proceduce converts integers to chints\nif integer < 0:\n#raise \"Only positives between 0 and 61\", integer\nreturn '.'\nif integer > 61:\n#raise \"Only positives between 0 and 61\", integer\nreturn '#'\n#if integer == 1: return 'o'\nif integer < 10: return `integer`\nif integer < 36: return chr(ord('a')+integer-10)\nreturn chr(ord('A')+integer-36)\n\ndef chint2int(chint):\n# converting chints back to integers\nif chint==' ': return 0\n#if chint=='o': return 1\nif ord('A')<=ord(chint)<=ord('Z'):\nreturn 36+ord(chint)-ord('A')\nif ord('a')<=ord(chint)<=ord('z'):\nreturn 10+ord(chint)-ord('a')\nreturn int(chint)\n\ndef chint_inc(chint):\n# increments a chint\nreturn int2chint(chint2int(chint)+1)\n\ndef test_chint2int():\nfor i in range(0, 62):\nprint '%2s = %s'%(i,int2chint(i))\nassert(chint2int(int2chint(i)) == i)\n\ndef roman2int(roman):\n\nchr_table = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,\n'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,\n'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14,\n'XV': 15, 'XVI': 16, 'Mito': 17}\nif roman in chr_table.keys():\nreturn chr_table[roman]\nelse:\nreturn roman\n\ndef int2roman(int):\n\nchr_names = ['zero', 'I', 'II', 'III', 'IV', 'V',\n'VI', 'VII', 'VIII', 'IX', 'X',\n'XI', 'XII', 'XIII', 'XIV', 'XV',\n'XVI']\nif int < len(chr_names):\nreturn chr_names[int]\nelse:\nreturn int\n\ndef chr2int(chr):\nassert('A'<=chr<='Q')\nreturn ord(chr)-ord('A')+1\n\ndef int2chr(int):\nassert(1<=int<=16)\nreturn chr(ord('A')+int-1)\n\n############################################################\n#\n# HISTOGRAMS\n#\n############################################################\n\ndef quick_histogram(values, num_bins = 20, width=80, height=20, logx=0,minv=None,maxv=None):\n\non = '['+'o'*(width/num_bins-3)+']'\ndef display_range(v1,v2,maxlen=len(on)):\nn = '%s-%s'%(v1,v2)\nif len(n) < maxlen:\nreturn n\nelse:\nreturn '%s-%s'%(display_bignum(v1,0),display_bignum(v2,0))\n\nbinvalues, binnames = bin_values(values, num_bins=num_bins, display_range=display_range,logx=logx,\nminv=minv,maxv=maxv)\nbinvalues = map(len, binvalues)\n\nprint_histogram(binvalues,binnames,\n\nscale = {'height': height,\n'on': on,\n'space': ' ',\n'display_bottom': 'name',\n'display_top': 'value',\n'autoscale': 1,\n'front_gap': ''})\n\ndef bin_values(values, do_not_set=None, num_bins=None, increment=None, f=None,\ndisplay_range=None, logx=0, logincrement=None, minv=None, maxv=None, name_tag='-'):\n# bins the values in n bins (n = num_bins)\n#\n# constructs the bin boundaries automatically, based on the min\n# and max values. Returns a list of lists and a list of strings.\n#\n# For bin i:\n# * binvalues[i] contains all the elements of values in that bin\n# * binnames[i] contains the boundaries of the bin, ex: '3.7-4.2'\n# that can be parsed with \"[-]?\\d+-[-]?\\d+\"\n\nif do_not_set: raise \"Please explicitly name the arguments to bin_values\"\n\nif (num_bins and (increment or logincrement)):\nraise \"cannot specifiy both num_bins and increment\",(num_bins,increment)\nelif (not num_bins and not increment and not logx):\nraise \"Must specifiy either num_bins or increment\",(num_bins,increment)\nelif (logx and not num_bins and not logincrement):\nraise \"For LOG histogram, must specifiy either num_bins or logincrement\",(num_bins,logincrement)\n\nif minv==None:\nif values: minv = min(values)\nelse: minv=0\nif maxv==None:\nif values: maxv = max(values)\nelse: maxv=0\n\nif f:\nobjects = values\nvalues = map(f, objects)\nelse:\nobjects = None\n\n# if increment is not specified, recalculate it from the number of bins\nif logx:\nminlog = math.log(minv)\nmaxlog = math.log(maxv)\nif num_bins:\nlogincrement = float((maxlog - minlog)) / float(num_bins)\n#print \"Calculating LOG increment = %s from (min,max)=(%s,%s) and num_bins=%s\"%(\n# logincrement, minlog, maxlog, num_bins)\nlogrange = float_range(minlog, maxlog, logincrement)\nbins = map(safe_exp, logrange)\nelse:\nif num_bins:\nincrement = float(maxv-minv) / float(num_bins)\n#print \"Calculating increment = %s from (min,max)=(%s,%s) and num_bins=%s\"%(\n# increment, minv, maxv, num_bins)\nbins = float_range(minv, maxv, increment)\nif bins and abs(bins[-1]-maxv)>.01:\nbins.append(maxv)\n\nif not bins:\nif display_range: names = [display_range(minv,maxv)]\nelse: names = ['%s%s%s'%(min(values), name_tag, max(values))]\nreturn [values], names\n\n# first construct the names\nbinnames = []\nfor low,high in map(None, bins[:-1], bins[1:]):\nif display_range: binnames.append(display_range(low,high))\nelse: binnames.append('%s%s%s'%(low,name_tag,high))\n\n# then alter the last bin, to make up for all the datapoints\n# that are exactly the maximum\nbins[-1] = bins[-1]+.0101\n\n# and construct the bins\nbinvalues = []\nfor low,high in map(None, bins[:-1], bins[1:]):\n#low,high=int(bin),int(bin+increment)\nif objects:\nv_o = map(None, values, objects)\nv_o_in = filter(lambda v_o,low=low,high=high: low<=v_o[0]<high, v_o)\nbinvalues.append(map(lambda v_o: v_o[1], v_o_in))\nelse:\nbinvalues.append(filter(lambda v,low=low,high=high: low<=v<high, values))\n\nreturn binvalues, binnames\n\ndef test_quick_bin():\nvalues = [3,1,2,1,4,5,6,1,2,3,18,-40,-32,2,500]\nthe_range = [0,2,4,6,8,10,510]\n\npp(quick_bin(values,the_range),1)\npp(multi_count_lt(values,the_range))\npp(multi_count_gte(values,the_range))\n\ndef multi_count_lt(values, cutoffs):\nbins = quick_bin(values, cutoffs, ignore_low_values=0)\ncum_count = [0]\nfor cutoff,this_count in my_sort(bins.items()):\ncum_count.append(cum_count[-1]+len(this_count))\nres = items2dic(map(None,my_sort(bins.keys()),cum_count[:-1]))\nreturn mget(res,cutoffs)\n\ndef multi_count_gte(values, cutoffs):\nbins = quick_bin(values, cutoffs, ignore_low_values=0)\ncum_count = [0]\nfor cutoff,this_count in my_sort_rev(bins.items()):\ncum_count.append(cum_count[-1]+len(this_count))\nres = items2dic(map(None,my_sort_rev(bins.keys()),cum_count[1:]))\nreturn mget(res,cutoffs)\n\ndef quick_bin(values, the_range, ignore_low_values=0):\n# input:\n# values = [3,1,2,1,4,5,6,1,0,2,3,18,-40,-32,2,500]\n# the_range = [0,5,10]\n# output:\n# {'less': [-32,-40], 0: [0,0,0,1,1,2,2,3,3,3,4], 5: [5,6,10,18,500]}\n\nfor i in range(0,len(the_range)-1):\nassert(the_range[i]<the_range[i+1])\nvalues = my_sort(values)\n\nbins = {}\nfor bin in the_range:\nbins[bin] = []\n\nif not values: return bins\n\nuse_less_key = 0\nmini = min(values)\nif mini<the_range[0]:\nthe_range = [mini]+the_range\nuse_less_key = 1\nbins[mini] = []\n\ni = 0\nfor active_bin, cutoff in map(None,the_range,the_range[1:]+[max(values)+1]):\n#print \"Active bin = %s for everything less than %s\"%(active_bin, cutoff)\nwhile i < len(values) and values[i] < cutoff:\n#print \"value[%s]=%s\"%(i,values[i])\nbins[active_bin].append(values[i])\ni = i+1\n\nassert(my_sort(flatten(bins.values()))==values)\n\nif use_less_key and ignore_low_values:\ndel(bins[mini])\n\nreturn bins\n\ndef print_histogram(values, names, scale=None):\ndisplay_histogram(sys.stdout, values, names, scale)\n\ndef display_histogram(f, values, names, scale_in=None):\n\nif not values:\nprint \"Empty histogram\"\nreturn\n\nif not scale_in:\nscale = {'height': 6,\n'on': '|',\n'space': '',\n'display_top': 'none',\n'display_bottom': 'name',\n'autoscale': 1,\n'min': 0,\n'max': 100,\n'front_gap': ''}\nelse: scale=scale_in\n\n# 0. Input choice dictates what to print at the top and bottom\ndef values2names(values, maxlen):\nnames = []\nfor v in values:\nn = `v`\nif len(n) > maxlen:\nn = display_bignum(v,0)\nnames.append(n)\nreturn names\n\nif not scale.has_key('display_top') or string.lower(scale['display_top']) == 'none':\nnames_top = ['']*len(values)\nelif scale['display_top'] == 'name': names_top = names[:]\nelif scale['display_top'] == 'value': names_top = values2names(values,len(scale['on']))#map(lambda v: `v`,values)\nelse: raise \"scale['display_top'] not one of name,value,none: \", scale['display_top']\n\nif not scale.has_key('display_bottom') or string.lower(scale['display_bottom']) == 'none':\nnames_bottom = ['']*len(values)\nelif scale['display_bottom'] == 'name': names_bottom = names[:]\nelif scale['display_bottom'] == 'value': names_bottom = values2names(values,len(scale['on']))#map(lambda v: `v`,values)\nelse: raise \"scale['display_bottom'] not one of name,value,none: \", scale['display_bottom']\n\n# 1. first construct that matrix of characters\nrows = []\nfor h in range(0,scale['height']+2):\nrow = []\nfor v in values:\nrow.append(' '*len(scale['on']))\nrows.append(row)\nif scale['autoscale']:\nscale['min'] = min(values)-1\nscale['max'] = max(values)\nif scale['min'] > min(values): raise \"Min specified too high\",(values,scale['min'])\nif scale['max'] < max(values): raise \"Max specified too low\",(values,scale['max'])\n\n# 2. then insert all the bars of the histogram\nfor j in range(0,len(values)):\nv = values[j]\n# do the appropriate rescaling\nbottom = 0\ntry:\ntop = int((float(v-scale['min'])/float(scale['max']-scale['min']))*scale['height'])\nexcept ZeroDivisionError:\ntop = scale['height']\n#print \"%s -> Top %s\"%(v,top)\n\nfor i in range(bottom, top):\nrows[i][j] = scale['on']\nrows[top][j] = '_'*len(scale['on'])\nrows[top+1][j] = string.center(names_top[j],len(scale['on']))[:len(scale['on'])]\n\n# 3. finally output the histogram\nrows.reverse()\nfor row in rows:\nf.write(scale['front_gap']+string.join(row, scale['space'])+'\\n')\nf.write(scale['front_gap']+string.join(map(lambda name_bottom,scale=scale:\nstring.center(name_bottom,\nlen(scale['on']))[:len(scale['on'])],\nnames_bottom),\nscale['space'])+'\\n')\n\ndef print_wrap_histogram(values, names, w, scale=None):\ndisplay_wrap_histogram(sys.stdout, values, names, w, scale)\n\ndef display_wrap_histogram(f, values, names, w, scale=None):\nif scale['autoscale']:\nscale['min'] = min(values)\nscale['max'] = max(values)\nscale['autoscale'] = 0\nfor i in range(0,len(values),w):\ndisplay_histogram(f,values[i:i+w],names[i:i+w],scale)\n\ndef test_histogram():\ndisplay_wrap_histogram(sys.stdout,\n[100,2,300,400,200,300,1000],\n'abcdefg',\n7,\nscale={'height': 10,\n'on': '||',\n'space': '',\n'autoscale': 1,\n'front_gap': '',\n'display_top': 'none',\n'display_bottom': 'name'})\n\n############################################################\n# guess a distribution center and spread\n#\n# def guess_center(values, divisions = 20):\n#\n# bins = bin_values(values, num_bins=20)\n# i = 0\n# while i<len(values):\n# if 5*len(bins[i]) <= len(values):\n# # this can erase a bin, even in the middle of the distribution\n# del(bins[i])\n# else:\n# i = i+1\n# values = flatten(bins)\n#\n# return guess_center(values, divisions)\n\ndef guess_center(values, do_not_set=None, num_bins=None, increment=None, fraction=5):\ndebug = 1\n\nif len(values) == 1: return values[0]\n\nif debug:\nprint \"Guessing center of %s values, with num_bins=%s and increment=%s. Mean %s, Stdev: %s\"%(\nlen(values), num_bins, increment, avg(values), stdev(values))\n\nif do_not_set: raise \"Please name arguments to guess_center\"\nif num_bins and not increment:\nif debug:quick_histogram(values,num_bins=num_bins)\nbins,binnames = bin_values(values, num_bins=num_bins)\nelif increment and not num_bins:\nif debug: quick_histogram(values,num_bins = 20)#len(range(min(values),max(values),increment))+1)\nbins,binnames = bin_values(values, increment=increment)\nelif increment and num_bins:\nraise \"Do not set both num_bins and increment\"\nelse:\nraise \"Please set either num_bins or increment\"\n\n# keep the bin with the max length\n#pp(map(len,bins),1)\nmax_bin = argmax(map(len, bins))\nprint \"Max bin is %s/%s and it contains %s/%s items\"%(max_bin,len(bins),len(bins[max_bin]),len(values))\nprint \"The other bins contain %s\"%map(len,bins)\n#print \"Max bin is %s has %s items\"%(max_bin,len(bins[max_bin]))\n# see if the next and previous have such strong distributions\nmin_i, max_i = max_bin, max_bin\n# see if we can extend to the left\nwhile min_i>=1:\nif fraction * len(bins[min_i-1]) >= len(bins[min_i]):\n# include one more bin to the left\nmin_i = min_i - 1\nif debug: print \"Extending to the left. Now from %s/%s to %s/%s, there's %s items\"%(\nmin_i, len(bins), max_i, len(bins), sum(map(len, bins[min_i:max_i+1])))\nelse:\nbreak\nwhile max_i<len(bins)-1:\nif fraction * len(bins[max_i+1]) >= len(bins[max_i]):\n# include one more bin to the right\nmax_i = max_i + 1\nif debug: print \"Extending to the right. Now from %s/%s to %s/%s, there's %s items\"%(\nmin_i, len(bins), max_i, len(bins), sum(map(len, bins[min_i:max_i+1])))\nelse:\nbreak\nvalues_in = flatten(bins[min_i:max_i+1])\nif debug: quick_histogram(values_in, num_bins=20)\nreturn avg(values_in)\n\n# category 4\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### FILES and SAVING ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef binsave(file, data):\n# binsave cPickle wrapper\nf = open(file, 'w')\ncPickle.dump(data, f, 1)\nf.close()\n\ndef binload(file):\n# binload cPickle wrapper\nf = open(file, 'r')\ndata = cPickle.load(f)\nf.close()\nreturn data\n\n############################################################\n#\n# file stuff\n#\n############################################################\n\ndef quick_system(cmd, stdin):\n\n# make a temp file for the standard in\nimport tempfile\nfn_in = tempfile.mktemp('.tmp')\n#print fn_in\n#fn_in = '/seq/manoli/yeast/tmp/stdin'\nopen(fn_in,'w').write(stdin)\nstdout = os.popen('%s < %s'%(cmd,fn_in),'r').read()\n\nreturn stdout\n\ndef decompose_filename(filename):\n\n# first find the directory\nif string.find(filename,'/')==-1:\ndir = ''\nfn = filename\nelif filename[-1] == '/':\ndir = filename\nfn = ''\nelse:\npath = string.split(filename,'/')\ndir = string.join(path[:-1],'/')+'/'\nfn = path[-1]\n\n# then find the extension\nif string.find(fn,'.') == -1:\nroot,extension = fn,''\nelse:\npath = string.split(fn,'.')\nroot = path[0]\nextension = '.'+string.join(path[1:],'.')\n\nreturn dir, root, extension\n\ndef common_subdir(filenames):\nif not filenames: return '', filenames\n# make sure no filenames contain consecutive //\nfor filename in filenames:\nassert(string.find(filename,'//')==-1)\n# make sure all the paths are absolute\nassert(all_same(['/']+cget(filenames,0)))\n\ncommon = '/'\nfor subdir in filter(None,string.split(filenames[0],'/')):\nnewcommon = common+subdir+'/'\n#print \"Trying %s\"%newcommon\nall_common = 1\nfor filename in filenames:\nif string.find(filename,newcommon)==0: continue\nelse:\nall_common=0\nbreak\nif all_common: common = newcommon\nelse: break\n\n#print \"Longest common directory: %s\"%common\n\nshort_filenames = []\nfor filename in filenames:\nassert(string.find(filename, common)==0)\nshort_filenames.append(filename[len(common):])\n\nreturn common, short_filenames\n\ndef change_extension(filename, new_extension):\n#if new_extension[0] == '.': new_extension = new_extension[1:]\nextension = string.split(filename,'.')[-1]\nif not '/' in extension:\nfilename = filename[:-1-len(extension)]\nreturn filename+new_extension\n\ndef opens(filename, mode='r'):\n# open superuser\n# if for reading, acts like open\n# if for writing, it will create every subdirectory\n# to the file, and then open it for writine\n\nif mode == 'r': return open(filename, mode)\ntry:\nf = open(filename, mode)\nexcept IOError:\nmkdirs(string.join(string.split(filename,'/')[:-1],'/'))\nf = open(filename, mode)\nreturn f\n\ndef mkdirs(filename, num_tries=2):\n# make directory superuser\n# will make every directory on the way to the\n# given filename\nsubdirs = string.split(filename,'/')\nfor i in range(2,len(subdirs)+1):\npartial = string.join(subdirs[:i],'/')\nif not os.path.exists(partial):\ntry: os.mkdir(partial)\nexcept OSError:\nif os.path.exists(partial): continue\nelse: raise\nif not os.path.exists(filename):\nmkdirs(filename, num_tries-1)\n\ndef is_directory(filename):\nreturn stat.S_ISDIR(os.lstat(filename)[stat.ST_MODE])\n\ndef filesize(filename):\nreturn os.lstat(filename)[stat.ST_SIZE]\n\ndef cdate(filename):\nreturn time.ctime(os.lstat(filename)[stat.ST_CTIME])\n\ndef non_empty(filename):\nreturn os.path.exists(filename) and filesize(filename)!=0\n\n#def file_exists(filename):\n# return os.path.exists(filename) and filesize(filename)!=0\nfile_exists = non_empty\n\ndef touch(filename):\nopen(filename,'w').close()\n\n#def rm(filename):\n# os.remove(filename)\n\ndef empty_file(filename):\nif not os.path.exists(filename): return 1\nelse: return filesize(filename)==0\n\ndef exists_but_empty(filename):\nreturn os.path.exists(filename) and filesize(filename)==0\n\ndef quote_filename(filename):\nreturn mreplace(filename,['(',')',' '],['\\(','\\(','\\ '])\n\n############################################################\n\ndef space_usage(file):\n# takes a file generated by a space usage command\n# and then sorts it by size\nlines = open(file,'r').readlines()\n\nfor i in range(0, len(lines)):\n\nlines[i] = filter(None, string.split(lines[i]))\nlines[i][0] = int(lines[i][0])\n\nlines.sort(lambda l1,l2: l2[0] - l1[0])\nreturn lines\n\ndef display_usage(lines):\n\nfor line in lines:\nif string.count(line[1], '/') > 1: continue\nprint '%14s %s'%(dec(line[0]), line[1])\n\n############################################################\n\ndef split_command_file(filename, increment):\n#\n# Input: a file containing a number of one line commands\n# that are independent of each other (i.e. execution\n# order doesn't matter\n# Action: creates a number of files, all with the same\n# filename_i, and a source file: filename_sourcem\n# which when sourced will send every subfile to lsf\n# Use: when you don't feel like submitting 30000 commands to\n# LSF, but also, you don't feel like waiting for them\n# to be executed one after the other.\n\n# the commands to run\ncommands = filter(lambda cmd: cmd[0]!='#',\nopen(filename,'r').readlines())\n\nprint \"Splitting %s. %s commands in increments of %s. %s simultaneous_files.\"%(\nfilename, len(commands), increment, len(commands)/increment)\n\n# open the masterfile\nfile_master = open(filename+'_sourcem','w')\n\ni = 0\nfor command_subset in range(0,len(commands),increment):\n\ni = i + 1\nfile_i = open(filename+'_'+`i`,'w')\nfor command in commands[command_subset:][:increment]:\nfile_i.write(command)\nfile_i.close()\nos.chmod(file_i.name, 0777)\n\nfile_master.write('bsub %s_%s &\\n' % (filename, i))\n\nfile_master.close()\nos.chmod(file_master.name, 0777)\n\nprint \"Done splitting. Now source: %s_sourcem\"%filename\n\nreturn filename\n\n############################################################\n#\n# HTML files\n#\n############################################################\n\ndef html_eliminate_tags(s):\n# eliminates all matching HTML tags from a string\nout = []\n\nfirst_last = html_find_tags(s)\nif not first_last: return s\nlast_first = list2pairs(first_last,\nlambda a,b: (a[1],b[0]))\nlast_first.insert(0,(0,first_last[0][0]))\nlast_first.append((first_last[-1][1],len(s)))\nfor first,last in last_first:\nout.append(s[first:last])\nreturn string.join(out,'')\n\ndef html_find_tags(s):\n# returns the start and end of every HTML tag\ntags = []\nlast = 0\nwhile 1:\nfirst = string.find(s,'<',last)\nif first==-1: break\nlast = string.find(s,'>',first)\nif last==-1: break\n\n# make sure you're not erasing everything\n# between inequality signs\nok=0\nif s[first:first+2] == '<a ': ok=1\nif s[first:first+5] == '<img ': ok=1\nif string.find(s[first:last+1],'\\n')==-1: ok=1\nif last+1-first < 8: ok=1\nif not ('a'<=s[first+1]<='z' or 'A'<=s[first+1]<='Z' or s[first+1] in '/!'): ok=0\n#if ok: print \"HTML:Eliminated %s\"%s[first:last+1][:10]\n#else: print \"HTML:Not a tag %s\"%s[first:last+1][:10]\n\nif ok: tags.append((first,last+1))\nelse: last = first+1\nreturn tags\n\ndef html_generate_listing(f, dir, level=2):\nimport glob\nif dir[-1]!='/': dir = dir+'/'\nfiles = my_sort(glob.glob(dir+string.join(['*']*level, '/')))\ncommon, files = common_subdir(files)\n#pp(files)\n\nf.write('<html>\\n<head>\\n<title>Directory Listing of %s</title>\\n</head>\\n</html>\\n'%dir)\nf.write('<body>\\n')\n\nf.write('<table border=1>\\n')\nf.write('<tr><th>Directory</th><th>Filename</th><th>Type</th><th>Size</th><th>Comment</th></tr>')\nlastdir = ''\nfor file in files:\npath = string.split(file,'/')\nthisdir = string.join(path[:-1], '/')\nif thisdir!=lastdir:\nf.write('<tr><td><b>%s</b></td>'%thisdir)\nlastdir=thisdir\nelse:\nf.write('<tr><td></td>')\n#f.write('<td></td>')\n\ntyped = string.split(path[-1],'.')\nif is_directory(common+file):\nfilename,type = path[-1],'dir'\nsizeinfo = '%s files'%len(glob.glob(common+file+'/*'))\n\nelse:\nif len(typed)>1:\nfilename,type = string.join(typed[:-1],'.'), typed[-1]\n# if len(filename) < len(type) and len(type)>5: filename,type = filename+'.'+type, ''\nelse: filename,type = path[-1],''\nsizeinfo = '%s kb'%dec(filesize(common+file)/1000)\n\nf.write('<td><a href=\"%s\">%s</a></td><td align=center>%s</td><td align=right>%s</td><td>None</td>'%(\nfile,filename,type,sizeinfo))\n\nf.write('</tr>\\n')\n\nf.write('</table>')\nf.write('</body>\\n</html>\\n')\n\n# category 5\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### FASTA FILES AND FILE POINTERS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Reading a portion of a file, until a separator is met\n#\n############################################################\n\ndef parse_tab(fn,debug=0):\nf = open(fn,'r')\nlines = filter(None,string.split(string.replace(f.read(),'\\015',''),'\\n'))\nf.close()\nlines = filter_diclist_not(lines,0,'#')\nreturn parse_tab_lines(lines,debug=debug)\n\ndef parse_tab_lines(lines,debug=0):\ntitles = string.split(lines[0],'\\t')\nparsed = []\nfor line in lines[1:]:\nlined = {}\nmset(lined,titles,string.split(line,'\\t'))\nparsed.append(lined)\n\nif debug:\nf = sys.stdout\nmaxlens = {}\nfor key in titles:\nmaxlens[key] = max([len('%s'%key)]+map(len,cget(parsed,key)))\n\nfor title in titles:\nf.write(('%-'+`maxlens[title]`+'s|')%title)\nf.write('\\n')\nfor title in titles:\nf.write('-'*maxlens[title]+\"|\")\nf.write('\\n')\nfor line in parsed:\nfor key in titles:\nf.write(('%-'+`maxlens[key]`+'s|')%line[key])\nf.write('\\n')\n\nreturn parsed\n\ndef output_tab(diclist, order=None, f=sys.stdout, separator='\\t', default='', exclude=[], only_specified=0):\n\nkeys = my_sort(unique(flatten(map(lambda d: d.keys(), diclist))))\n\nif only_specified: exclude = set_subtract(keys, order)\n\nif not order: order = keys\nelse: order = set_intersect(order,keys) + set_subtract(keys,order)\n\n# make sure the separator does not appear as a key\nfor key in order:\nassert(string.find(key,separator)==-1)\nassert(string.find(default,separator)==-1)\n\norder = set_subtract(order,exclude)\n\nf.write(string.join(order,separator)+'\\n')\nfor dic in diclist:\nfields = []\nfor key in order:\nif dic.has_key(key): fields.append('%s'%dic[key])\nelse: fields.append(default)\nf.write(string.join(fields,separator)+'\\n')\n\ndef read_until(f, separator, increment=500):\n#\n# Reads from file f until separator is reached\n# at every call, it returns the string between separators.\n# when the end of file is reached, it returns -1\n#\n# increment characters are read iteratively from the file,\n# until separator is found\n#\n# Ex: if the file is: BABOON BAD BABAK and separator is 'BA'\n# consecutive calls will return '', 'BOON ', 'D ', '', 'K', -1\n\n# so that you only have to look back once\nif increment < len(separator):\n#print \"Increasing increment size to match separator\"\nincrement = len(separator)\n\n# when the file is done, it returns -1\n# it can return an empty string if separator appears twice\n\ncurrent = []\nwhile 1:\n\n#print f.tell()\nlast_read = f.read(increment)\n#print \"Reading %s\" % last_read\n\n# done reading file, return last portion\nif not last_read:\nif len(string.join(current,'')) > 0:\nbreak\nelse:\nreturn -1\n\n#print \"Read: %s\"%last_read\n#pp(current)\n\n# append last increment to currently read portion\ncurrent.append(last_read)\n\n# search for motif in this portion\nfound = string.find(current[-1], separator)\n\n# if found in this portion\nif found > -1:\n\n# restore f to the position before the increment\nf.seek(-len(current[-1])+found+len(separator),1)\n\n# chop off the portion after your marker\ncurrent[-1] = current[-1][:found]\n\nbreak\n\nelse:\n\n# now check if it falls on the boundary between two read iterations\n\nif len(current) == 1:\n# if it's not found in the last_read and current only contains one read iteration,\n# then it's just simply not there\ncontinue\nelse:\nspan_gap = current[-2][-len(separator):]+current[-1][:len(separator)]\nattempt2 = string.find(span_gap, separator)\nif attempt2 > -1:\n\n# measure how many to chop from previous last_read\nto_chop = len(separator)-attempt2\ncurrent[-2] = current[-2][:-to_chop]\ndel(current[-1])\n\n# set the file pointer back accordingly\nf.seek(-(increment-len(separator)+to_chop),1)\n\nbreak\n\n# return all you've gathered so far\nreturn string.join(current,'')\n\ndef test_read_until(separator = '|||'):\nf = open('/seq/manoli/yeast/tmp/separator.txt','r')\nwhile 1:\ntxt = read_until(f, separator, 6)\nif txt !=-1:\nprint `txt`\nelse:\nbreak\nf.close()\n\n############################################################\n#\n# How to read into a file, without loading it all in memory\n#\n############################################################\n\ndef get_separator_positions(filename, separator, increment=500):\n# get all the start positions of the separator\n# to be used with f.seek(start), f.read(len(separator))\n\nf = open(filename, 'r')\n\n# make sure we'll find separator in increment some day\nif len(separator) > increment: increment = len(separator)\n\nwhere = []\nwhile 1:\n\n# read a line\nline = f.read(increment)\n\n#print \"Line is %s\"%line\n\n# check end of file\nif len(line) < len(separator):\nbreak\n\n# see if you hit the separator\nfound = string.find(line, separator)\n\n# if found, where was it found\nif found > -1:\n\n# the absolute start and end of the separator\nstart = f.tell() - len(line) + found\nend = start + len(separator)\n\n#print \"Found it at %s - %s\"%(start, end)\n\nwhere.append((start,end))\n\n# now, restart at the end of the separator\nf.seek(end, 0)\n\nelse:\n\n# backtrack a bit to see if we just missed the separator\nf.seek(-len(separator)+1, 1)\n\n# post process all the starts and ends\nif where:\n\nf.seek(0,2)\nbetween = [(0,where[0][0])]\nfor i in range(1,len(where)):\nbetween.append((where[i-1][1], where[i][0]))\nbetween.append((where[-1][1], f.tell()))\n\nelse:\n\nbetween = [(0,f.tell())]\n\nf.close()\n\nreturn between\n\ndef use_separator_positions(filename, offsets):\nf = open(filename, 'r')\nfor start,end in offsets:\nf.seek(start, 0)\nline = f.read(end-start)\nprint line\nf.close()\n\ndef test_separator_positions(filename = '/seq/manoli/yeast/tmp/separator.txt',\nseparator = '||||'):\npos = get_separator_positions(filename, separator)\npp(pos)\nuse_separator_positions(filename, pos)\n\n############################################################\n#\n# The above, more precisely for FASTA files\n#\n############################################################\n\ndef make_random_fasta(filename, num_seqs, min_length, max_length, wrap=60):\nimport blast, random, simulation\nf = open(filename,'w')\nfor i in range(0,num_seqs):\n\nlength = random.choice(range(min_length, max_length+1))\n\nblast.append_fasta(f,\n'seq%s(%sbp)'%(i,length),\nsimulation.random_seq(length))\n\nf.close()\n\ndef make_test_fasta(filename, num_iterations, before, after, wrap=60):\nimport blast, simulation\nf = open(filename,'w')\nfor full_lines in range(1,num_iterations):\nbase_len = 60*full_lines\nfor i in range(-before,after):\nblast.append_fasta(f,\n'seq%s_%s'%(full_lines,i),\nsimulation.random_seq(base_len+i))\nf.close()\n\ndef test_fasta_dictionary(filename):\nmake_test_fasta(filename,3,1,1,10)\n#make_random_fasta(filename, 5, 80, 180, 60)\n\n#print open(filename,'r').read()\n\nf = open(filename,'r')\ndic = fasta_dictionary(f.name)\n\n#print \"FASTA dictionary\"\n#pp(dic,1)\n\nnames,seqs = multiparse_fasta(f.name)\nseq_dic,len_dic = {},{}\nfor name,seq in map(None, names, seqs):\nname = string.strip(name)\nseq_dic[name] = seq\nlen_dic[name] = len(seq)\n#print \"'%s'\"%seq\n#print \"Names, seqs: \"\n#pp(seq_dic)\n#pp(len_dic)\n\nassert(my_sort(names) == my_sort(set_subtract(dic.keys(),['>width','>fn','>f'])))\n\n#print \"%15s:%s, %s\"%('name','true','fdic')\nfor name in names:\n#print \"%s: %3s, %3s%s\"%(\n# sfill(name,15),len_dic[name],dic[name][1],\n# ifab(dic[name][1] == len_dic[name],'',' <- different'))\nassert(dic[name][1] == len_dic[name])\n\n#pp(len_dic)\n#pp(dic,1)\nfor key in dic.keys():\nif key[0]=='>':\n#print \"Skipping %s == %s\"%(key,dic[key])\ncontinue\nstart,length = dic[key]\n\n#print \">%s\"%key\nfor low in range(-10,length): #range(-10,length):\nfor high in range(low-10, length+3): #range(low-10,length+3):\nseq1 = get_sequence(f,dic,key,low,high,up=1)\nseq2 = seq_dic[key][low:high]\nif seq1!=seq2:\nprint 'Get sequence (%s, %s, %s)'%(\nkey, low, high)\nprint ' '*low+seq1+ifab(seq1==seq2, ' <- correct',\n' <- OOPS '+seq2)\n\ndef fasta_dictionary(filename):\n\ndebug = 1\n\n# first get all the starts of fasta entries\npositions = get_fasta_positions(filename)\n\n# now construct a dictionary which maps sequence names to positions\ndic = {}\n\n# reopen the file\nf = open(filename, 'r')\n\n# find the width of the file\nw = verify_fasta_width(f)\n\nfor start,end in positions:\n\n# go to the start and read the title\nf.seek(start, 0)\ntitle = string.strip(read_until(f,'\\n',60))\n#title = string.strip(f.readline())\n\n# make sure all entries are unique\nif dic.has_key(title):\nraise \"Duplicate fasta title\", title\n\n# now add an entry to the dictionary for the sequence start\nseq_start = f.tell()\n#if debug:\n# print \"Now i'm at: %s\"%f.read(20)\n# f.seek(seq_start,0)\n\n# calculate the length of the sequence\nseq_length = end - seq_start - num_wraps_forward(end-seq_start, w)\n\n#if debug:\n# n = num_wraps_forward(end-seq_start, w)\n# print \"%s: total chars: %s, num wraps: %s, length: %s\"%(\n# title,end-seq_start,n,seq_length)\n\n# map the title to the start and end of the sequence\ndic[title] = (seq_start, seq_length)\n\ndic['>width'] = w\ndic['>fn'] = filename\ndic['>f'] = None\n\nreturn dic\n\ndef num_wraps_old(length, w):\n# count how many wraps will be, in a sequence of length length,\n# wrapped every width w\nif length <= 0: return 0\nreturn (length-1) / w\n\ndef num_wraps_forward(length, w):\n# count how many wraps there have been,\n# in a sequence of total length length (including wraps),\n# wrapped every width w\n# and what is the length of the sequence before inclusion of wraps\nif length <= 0: return 0\nreturn length / (w+1)\n\ndef num_wraps_reverse(length, w):\n# count how many wraps will be, in a sequence of length length,\n# wrapped every width w\nif length <= 0: return 0\nreturn (length-1) / w\n\ndef smart_wraps(start, end, w):\n# in a sequence wrapped every w characters,\n# it gives the number of wraps between start and end\nreturn num_wraps_reverse(end,w)-num_wraps_reverse(start,w)\n\ndef get_fasta_positions(filename):\nf = open(filename,'r')\nif f.read(1) != '>': raise \"Not a fasta file\", filename\npos = get_separator_positions(filename, '\\n>', 500)\n\n# adjust for the fact that the first entry does not have a carriage return before the >\npos[0] = (1, pos[0][1])\n\nfix_fasta_positions(f,pos)\n\nreturn pos\n\ndef fix_fasta_positions(f, positions):\n# makes sure that all the starts are pointing to \">title\"\n# and all the ends are pointing to non-whitespace characters\n\nfor i in range(0,len(positions)):\nstart,end = positions[i]\n\nf.seek(start-1,0)\nassert(f.read(1)=='>')\n\nrepeat = 1\nwhile repeat:\nf.seek(end-1,0)\nif f.read(1) in ' \\n':\nend = end-1\n#print \"Fixed end!\"\nelse:\nrepeat = 0\n\npositions[i] = (start,end)\n\ndef test_get_fasta_positions(filename):\npositions = get_fasta_positions(filename)\nf = open(filename,'r')\nfix_fasta_positions(f,positions)\nfor start, end in positions:\nprint \"(%s,%s)\"%(start,end)\nf.seek(start,0)\nprint \"Reading %s (end-start): '%s'\"%(end-start,f.read(end-start))\n\ndef guess_fasta_width(f):\nwhere = f.tell()\nf.seek(0,0)\n#######\n# Look for the first two consecutive non-title lines\nfirst_line = f.readline()[:-1]\nwhile 1:\nsecond_line = f.readline()[:-1]\nif second_line[0] != '>' and first_line[0] != '>':\nw = len(first_line)\nbreak\nelse:\nfirst_line = second_line\nf.seek(0,where)\nreturn w\n\ndef verify_fasta_width(f):\n\nw = guess_fasta_width(f)\n\n# assumes that every title only takes a single line w/o line breaks\nf.seek(0,0)\n\n# make sure every sequence line is w or less right before a title\nnext_must_be_title = 1\ni = 0\nwhile 1:\ni = i+1\nline = f.readline()[:-1]\nif not line: break\nif next_must_be_title and line[0] != '>':\n# i'm expecting a title, but i don't get one\nraise \"Line %i should be a title\"%i\nelif len(line) == w or line[0] == '>':\n# i've gotten a sequence of length exactly w\n# or i've gotten a title, expecting it or not\nnext_must_be_title = 0\nelse:\nif len(line) > w: raise \"Line %i is too long\"%i\nif len(line) < w: next_must_be_title = 1\nf.seek(0,0)\nreturn w\n\ndef fix_fasta_width(filename_in, filename_out=None, newwidth=None):\n\n# choose a filename_out that doesn't already exist\nif not filename_out:\nreplace = 1\ni = 0\nwhile 1:\ni = i+1\nfilename_out = filename_in+`i`\nif os.path.exists(filename_out):\ncontinue\nelse:\nbreak\nelse:\nreplace = 0\n\nf1 = open(filename_in,'r')\nf2 = open(filename_out,'w')\n\n# if the newwidth is not specified, then simply guess it\nif not newwidth:\nnewwidth = guess_fasta_width(f1)\n\nimport blast\n\nwhile 1:\nname,seq = incremental_fasta(f1)\n\nif (name and seq): blast.append_fasta(f2,name,seq,newwidth)\nelse: break\n\nf1.close(), f2.close()\n\nif replace:\nos.rename(filename_out, filename_in)\n\ndef get_sequence(f, dic, title, start, end, up=1, safe=0):\n# f is an open file\n# dic is the fasta positions dictionary returned by fasta_dictionary(f.name)\n# title is the FASTA title of the sequence you're interested in (say, chromosome name)\n# start, end are the subsequence coordinates (say within a chromosome)\n# up is the strand\n\n#print \"Reading file %s entry %s coords %s - %s\"%(\n# f.name, title, start, end)\n\ntitle = string.strip(title)\n\nif not dic.has_key(title):\nraise \"Title not present in fasta_dictionary for %s\"%f.name, title\n\n# read the dictionary for positional information\nseq_start, seq_length = dic[title]\n\n# check that you're not reading more than allowed by the sequence boundaries\nif start < 0: start = seq_length + start\nif end < 0: end = seq_length + end\nif start > seq_length: return ''\nif start > end: return ''\nif end > seq_length: end = seq_length\n\n#######\n# method 1: only read that portion of the file\n#\n# will not work because of line feeds -> must assume consistent line feeds\n# one could check consistency before generating fasta_dictionary\n#\n#f.seek(seq_start + start - 1)\n#seq = f.read(seq_len)\n\n######\n# method 2: read all, and only return interesting portion\nw = dic['>width']\nf.seek(seq_start+start+num_wraps_reverse(start,w))\nseq = f.read(end-start+smart_wraps(start,end,w))\ntry:\nseq = string.replace(seq,'\\n','')\nexcept MemoryError:\nseq = ''\n\n# reverse complement\nif not up: seq = revcom(seq)\n\nif safe:\nassert(sum(map(lambda char: string.count(seq,char),'ACGTNacgtn'))==len(seq))\n\nreturn seq\n\ndef fasta_dic_pos2seq(filename, dic):\nf = open(filename,'r')\nseq_dic = {}\nfor key,(start,end) in dic.items():\nseq_dic[key] = string.strip(get_seq(f, start, end))\nreturn seq_dic\n\ndef get_seq(f, start, end):\nf.seek(start)\nreturn f.read(end-start)\n\n############################################################\n#\n# An older version of this\n#\n############################################################\n\ndef get_fasta_entry(f, title, reset=1):\n# if reset is true, the pointer is reset and file searched\n# from the beginning\nif reset:\nf.seek(1,0)\nelse:\n#print 'WARNING! only searching from position %s on'%f.tell()\npass\nwhile 1:\nseq = read_until(f, '\\n>', 500)\nif seq == -1: break\nif seq[:len(title)] == title:\n#print \"Found! \"+seq\nreturn string.join(string.split(seq,'\\n')[1:],'')\nreturn -1\n\ndef get_fasta_entries(filename, titles, reset=0):\n\"Assumes: filename contains all titles, in order\"\n\nf = open(filename,'r')\nentries = []\nfor title in titles:\nentries.append(get_fasta_entry(f, title, reset))\nf.close()\nreturn entries\n\n############################################################\n#\n# Parsing FASTA files\n#\n############################################################\n\ndef singleparse_fasta(name):\n# returns only the sequence of a fasta file\n# assumes the file contains a single sequence\nreturn string.join(string.split(string.replace(open(name).read(),\n'\\015',''),\n'\\n')[1:],\n'')\n\ndef multiparse_fasta(file):\n# returns the titles and sequences of a fasta file\n# file can contain multiple sequences\nf = string.replace(open(file).read(),'\\015','')\ngroups = string.split(f, '>')[1:]\ntitles = []\nseqs = []\nfor group in groups:\nlines = string.split(group, '\\n')\ntitles.append(lines[0])\nseqs.append(string.join(lines[1:],''))\nreturn titles, seqs\n\ndef multiparse_qual(file):\n# file can contain multiple sequences\nf = string.replace(open(file).read(),'\\015','')\ngroups = string.split(f, '>')[1:]\ntitles = []\nquals = []\nfor group in groups:\nlines = string.split(group, '\\n')\ntitles.append(lines[0])\nqual = []\nfor line in lines[1:]:\nqual.extend(map(int,string.split(line)))\nquals.append(qual)\nreturn titles, quals\n\ndef incremental_fasta(f):\n\"\"\" every time it's called, this function reads\none sequence from a fasta file and positions the\ncursor to the beginning of the new sequence.\n\nEnd of file: return None\nAssumes: positioned at the beginning of the next fasta seq\n\"\"\"\n\n# first parse the title\n#print \"Starting at %s\"%f.tell()\nline = f.readline()\n#print \"Read title: Now at %s\"%f.tell()\nif not line: return '',''\nif line[0]!='>': raise \"First line is not a valid FASTA title\", line\nelse: name = string.strip(line[1:-1])\n\n# then parse the sequence\nseq = []\nwhile 1:\nline = f.readline()\n#print \"Read sequence: now at %s\"%f.tell()\nif not line:\n# done reading, end of file\nbreak\nif line[0] == '>':\n# done reading, next title\n# backtrack\nf.seek(-len(line),1)\n#print \"Backtracking: now at %s\"%f.tell()\nbreak\nseq.append(string.strip(line))\nreturn name, string.join(seq, '')\n\ndef incremental_qual(f):\n\"\"\" reads one sequence from a fasta file\nand positions the cursor to the beginning of the\nnew sequence.\n\nEnd of file: return None\nAssumes: positioned at the beginning of the next fasta seq\n\"\"\"\n\nline = f.readline()\nif not line: return '',[]\nif line[0]!='>': raise \"First line is not a valid FASTA title\", line\nelse: name = string.strip(line[1:-1])\n\n# then parse the quality scores\nseq = []\nwhile 1:\nline = f.readline()\nif not line: break\nif line[0] == '>':\nf.seek(-len(line),1)\nbreak\nseq.append(string.strip(line))\nreturn name, map(int, string.split(string.join(seq, ' ')))\n\ndef get_fasta_lengths(file):\ntitles, seq = multiparse_fasta(file)\nlengths = {}\nmset(lengths,titles,map(len, seq))\nreturn lengths\n\n############################################################\n#\n# Comparing FASTA files\n#\n############################################################\n\ndef compare_with_screen(species):\n#filename = '/seq/manoli/yeast/yeastSeqs/%sPhrapData'%species\nfilename = '/seq/comparative02/%s/sequal_dir/%sPHRP'%(species,species)\nreturn compare_fasta([filename,filename+'.screen'])\n\ndef compare_fasta(files):\n\"\"\"\nInput: two or more fasta files that all have the same order in their reads,\npossibly masked with different masking programs (ex: vector, repeat)\nOutput: the list of fasta entries that mismatch, and the number of characters\nthat differed in each case.\n\"\"\"\n\nfs = map(open, files)\ninconsistencies = []\ni = 0\nwhile 1:\ni = i+1\nname_seq_list = map(incremental_fasta, fs)\nnames = map(lambda ns: ns[0],name_seq_list)\nseqs = map(lambda ns: string.upper(ns[1]),name_seq_list)\n\nif not names[0]:\nbreak\n\nfor name in names:\nif name != names[0]:\nprint \"%s fasta entries considered\"%i\nraise \"Files out of sync\", (name, names[0])\nfor seq in seqs:\nif seq != seqs[0]:\n# count the number of bases that are different\nnum_mismatches = 0\nfor chars in unpack(seqs):\nif not all_same(chars):\nnum_mismatches = num_mismatches + 1\n\n# display mismatching message\nprint \"%19s: %3s / %3s mismatches (%2s%%)\"%(names[0], num_mismatches, len(seqs[0]),\n100*num_mismatches/len(seqs[0]))\n#print_wrap(seqs, 120, names)\n\n# append\ninconsistencies.append((names[0], num_mismatches))\n\nbreak\nmap(lambda f: f.close(), fs)\n\nif inconsistencies:\nprint \"%s fasta entries were different among %s\"%(len(inconsistencies),\nstring.join(files, ', '))\nelse:\nprint \"Files are identical\"\nreturn inconsistencies\n\n# category 6\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### STRING OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef sfill(s, length, fill_char = '.'):\n# Appends fill_char to the string s until it reaches length length\n# ex: sfill('hello',18,'.') -> hello...............\n# <--- 18 chars --->\n# useful for printing dictionaries in a cute way\n# one......: 1\n# five.....: 5\n# seventeen: 17\n\n#list = map(None, s)\n#list.extend(map(None, fill_char*(length - len(list))))\n#return string.join(list, '')\n\nreturn s + fill_char*(length-len(s))\n\ndef sinsert(s, sub, start):\n# inserts substring 'sub' into string 's' from i on\n# ex: sinsert('ATGCGGATATAT','*->',3) -> ATG*->ATATAT\n# useful for adding annotations\ns = map(None, s)\nsadd(s,sub,start)\nreturn string.join(s, '')\n\ndef sadd(s,sub,start):\nfor i in range(0, min(len(sub),len(s)-start)):\ns[i+start] = sub[i]\n\ndef mcount(s, chars):\n# sums the counts of appearances of each char in chars\ncount = 0\nfor char in chars:\ncount = count+string.count(s,char)\nreturn count\n\ndef mreplace(s, chars, targets):\n# the advantage of mreplace over translate_chars (see below)\n# is that both chars and targets can be strings, not just chars\n\n# ex: mreplace('mississipi',['ssi','mi','pi'],['ze','mme','']) -> mmezizi\n\n# note: the output depends on the order the chars are specified in\n# moreover, the output of one translation can be changed by\n# the next translation\n\n# useful for: mreplace('ACG|-.AAGCG','-.|',['','','']) -> 'ACGAAGCG'\n\nassert(len(chars)==len(targets))\n\nif sum(map(lambda char,s=s: string.count(s,char), chars)) == len(s):\nreturn ''\n\nfor char,target in map(None, chars, targets):\ns = string.replace(s,char,target)\nreturn s\n\ndef safe_replace(s,old,new):\nif not s: return s\nif not new and string.count(s,old)*len(old)==len(s): return ''\nreturn string.replace(s,old,new)\n\ndef msplit(seq, separators):\n# identical to string.split(seq,gap),\n# only it chops at any of the characters\n\n# turn the separator list into a dictionary for\n# constant lookup time\nseparator_d = {}\nmset(separator_d,separators,[None]*len(separators))\n\nstarts, ends = [0], []\ninside = 1\nfor i in range(0,len(seq)):\n\n# if i'm inside a good stretch, and i find a separator\nif inside and separator_d.has_key(seq[i]):\ninside = 0\nends.append(i)\n\n# if i'm outside a good stretch, and i find a non-separator\nif not inside and not separator_d.has_key(seq[i]):\ninside = 1\nstarts.append(i)\n\nif not inside: starts.append(len(seq))\nends.append(len(seq))\n\nsubseqs = []\nfor start, end in map(None, starts, ends):\nsubseqs.append(seq[start:end])\nreturn subseqs\n\ndef substrings_including(seq, chars):\n# returns the list of substrings of seq that contain characters in chars\nchar_d = {}\nmset(char_d,chars,[None]*len(chars))\n\nbinaries = map(char_d.has_key, seq)\nislands = count_islands(binaries)\nif not islands.has_key(1): return []\nelse: islands = islands[1]\nsubseqs = []\nfor start,end in islands:\nsubseqs.append(seq[start-1:end])\nreturn subseqs\n\ndef substrings_excluding(seq, chars):\n# returns the list of substrings of seq that do not contain characters in chars\nchar_d = {}\nmset(char_d,chars,[None]*len(chars))\n\nbinaries = map(char_d.has_key, seq)\nislands = count_islands(binaries)\nif not islands.has_key(0): return []\nelse: islands = islands[0]\nsubseqs = []\nfor start,end in islands:\nsubseqs.append(seq[start-1:end])\nreturn subseqs\n\ndef translate_chars(s,chars,targets):\n# make a dictionary and use it\n# ex: translate_chars('mississipi',['i','s'],['e','t']) -> mettettepe\n# ex: translate_chars('mississipi','is','et') -> mettettepe\n# note only char->char substitutions are possible. For multiple\n# substring -> substring substitutions, one can use mreplace\nreturn string.translate(s,string.maketrans(chars,targets))\n\ndef multi_find(seq, list_of_queries,i=0):\nbest = -1\nfor q in list_of_queries:\nindex = string.find(seq, q,i)\nif index > -1:\nif best == -1: best = index\nif index < best: best = index\nreturn best\n\ndef multi_rfind(seq, list_of_queries):\nbest = -1\nfor q in list_of_queries:\nindex = string.rfind(seq, q)\nif index > best: best = index\nreturn best\n\ndef find_all(seq, sub):\n#print \"Looking for %s in %s\"%(sub,seq)\nfound = []\nnext = string.find(seq,sub)\nwhile next != -1:\nfound.append(next)\nnext = string.find(seq,sub,next+1)\nreturn found\n\ndef find_all_list(list, element):\nresult = []\nfor i in range(0,len(list)):\nif list[i] == element:\nresult.append(i)\nreturn result\n\ndef find_ignoring_gaps(sequence, sub, gaps='-'):\n# The call\n#\n# start,end = find_ignoring_gaps(seq, sub, gaps)\n#\n# returns start,end such that:\n#\n# string.replace(seq[start-1:end],gaps,'')==sub\n#\n# useful for finding 'CG' in 'A--C---GA'\n#\n# Note: the non one-to-one nature of the definition\n# of this function is resolved by returning all gaps\n# possible on the left, but no gaps on the right.\n# i.e. CG -> '--C---G' and not 'C---G'\n# AC -> 'A--C' and not 'A--C---'\n\n#print \"Looking for %s in %s\"%(sub, sequence)\n\nno_gaps = string.replace(sequence, gaps, '')\nlocs = find_all(no_gaps, sub)\nif len(locs) == 0:\nraise \"NotFound\", sub\nelif len(locs) != 1:\n#print string.join(map(lambda i: '%s'%i, locs))\nraise \"MultipleInstances\", sub\nelse:\n\n# we know sub only appears once in the ungapped sequence\nuninterrupted = string.find(sequence, sub)\nif uninterrupted != -1:\n\n# optimization, check if you can find it uninterrupted.\n# will run much faster, and called for most sequences\nstart, end = uninterrupted+1, uninterrupted+len(sub)\n\nelse:\n\n# well, it wasn't that easy, the sequence is actually interrupted\n# by at least one gap -> search the hard way\npre_length = locs[0]+1\n\n# now i know that the subsequence starts after pre_length,\n# non-gap characters, so count how many total chars i does it\n# take before we see pre_length chars\nnum_chars,i = 0,0\nwhile 1:\nif sequence[i] != gaps:\nnum_chars = num_chars+1\nif num_chars == pre_length:\nbreak\ni = i+1\nstart = i+1\n\n# now, from that point, count how many i does it take,\n# so that the number of non-gap chars found (num_chars)\n# is the length of the sought string sub\n\n# note i is the i from above\nnum_chars = 0\nwhile 1:\nif sequence[i] != gaps:\nnum_chars = num_chars+1\nif num_chars == len(sub):\ni = i+1\nbreak\ni = i+1\nend = i\n\n#print \"Looking for %s in %s -> (%s %s) == %s\"%(\n# sub, sequence, start, end, sequence[start-1:end])\n\n# verify that it's correct\nassert(string.replace(sequence[start-1:end],'-','')==sub)\n\nreturn start, end\n\ndef gap_translate_start_end(seq, start, end):\n# return gap_free_start, gap_free_end such that:\n# string.replace(seq,'-','')[start-1:end] == string.replace(seq[start-1:end])\n\nassert(string.count(seq[:start],'-') < start) # must start with at least one non-gap character\n\ngaps_before_start = string.count(seq[:start],'-')\ngaps_in_the_middle = string.count(seq[start-1:end],'-')\n\n#print \"Gaps_before_Start = %s\\nGaps_in_the_middle = %s\\n\"%(\n# gaps_before_start,gaps_in_the_middle)\n\nnew_start = start - gaps_before_start\nnew_end = end - (gaps_before_start + gaps_in_the_middle)\n\nbefore = string.replace(seq,'-','')[new_start-1:new_end]\ntry:\nafter = string.replace(seq[start-1:end],'-','')\nexcept MemoryError:\nprint \"Before was: %s but after failed: %s\"%(before, seq[start-1:end])\nprint \"Start, end = (%s,%s)\"%(start, end)\nafter = ''\n\n#print \"Before: %s\\nAfter: %s\"%(before,after)\n\nassert(before == after)\n\nreturn new_start, new_end\n\ndef gap_untranslate_start_end(seq, gap_free_start, gap_free_end, safe=1):\nbinary = map(lambda char: char!='-', seq)\noffsets = cumulative_sum(binary)\nstart, end = offsets.index(gap_free_start)+1, offsets.index(gap_free_end)+1\n\nif safe:\nafter = string.replace(seq,'-','')[gap_free_start-1:gap_free_end]\nbefore = string.replace(seq[start-1:end],'-','')\n\nif before != after:\nprint_wrap([before, after], 100, ['before', 'after'])\n\nassert(after == before)\n\nreturn start, end\n\n#def gap_translate_coords(seq, coords, safe=1):\ndef coords_seq2ali(seq, coords, offsets=None):\nif not offsets: offsets = gap_translation_offsets(seq)\n# one version:\n\nif len(coords) < 10:\n#print \"Old method: %s .index operations on list of %s\"%(len(coords), len(offsets))\nnew_coords = map(offsets.index, coords)\n\nelse:\n# another version:\n\n#print \"Reversing %s offsets\"%len(offsets)\nreversed_offsets = reverse_map_list(offsets,0)\n#print \"Looking up %s coordinates\"%len(coords)\nnew_coords = mget(reversed_offsets, coords)\n#print \"Done\"\n\nreturn new_coords\n\ndef coords_starts_ends_seq2ali(starts, ends, seq):\nnew = coords_seq2ali(seq, starts+ends)\n\nstarts = new[:len(new)/2]\nends = new[len(new)/2:]\n\nreturn starts, ends\n\ndef interval_coords_seq2ali(intervals, seq):\nstarts = cget(intervals, 'start')\nends = cget(intervals, 'end')\n\nnew = coords_seq2ali(seq, starts+ends)\n\nstarts = new[:len(new)/2]\nends = new[len(new)/2:]\n\ncset(intervals,'start',starts)\ncset(intervals,'end',ends)\n\ndef interval_coords_ali2seq(intervals, seq):\nstarts = cget(intervals, 'start')\nends = cget(intervals, 'end')\n\nnew = coords_ali2seq(seq, starts+ends)\n\nstarts = new[:len(new)/2]\nends = new[len(new)/2:]\ncset(intervals,'start',starts)\ncset(intervals,'end',ends)\n\n#def gap_untranslate_coords(seq, coords, safe=1):\ndef coords_ali2seq(seq, coords, offsets=None):\nif not offsets: offsets = gap_translation_offsets(seq)\nnew_coords = mget(offsets,coords)\nreturn new_coords\n\ndef coords_ali2allseqs(seqs, coords, all_offsets=None):\nif not all_offsets: all_offsets = map(gap_translation_offsets,seqs)\npacked_newcoords = []\nfor seq,offsets in map(None,seqs,all_offsets):\npacked_newcoords.append(coords_ali2seq(seq,coords,offsets))\nnewcoords = unpack(packed_newcoords)\nreturn newcoords\n\ndef gap_translation_offsets(seq):\n#print \"Calculating gap translation offsets for a sequence of %s bp\"%len(seq)\nbinary = [1]*len(seq)\nzeros = find_all(seq,'-')\nmset(binary,zeros,[0]*len(zeros))\n# above and this line are equivalent\n#binary = map(lambda char: char!='-', seq)\n#if not binary==binary2:\n# print binary\n# print binary2\n\noffsets = cumulative_sum(binary)\noffsets.insert(0,0)\noffsets.append(offsets[-1]+1)\nreturn offsets\n\ndef test_gap_translate():\nimport simulation\nseq = simulation.random_seq(500,'ACGT-')\n#seq = '----GGATATAG---GGAGGA--'\n#seq = 'G--GA-T-ATAG---GGAGGA'\ngap_free = string.replace(seq,'-','')\n\nindices = range(0,len(gap_free))\n\nprint \"Testing gap translation on: \\n%s and \\n%s\"%(seq,gap_free)\n\nfor index in indices:\n\ni2 = coords_seq2ali(seq,[index])[0]\n\n#print \"%s%s%s Char %s of sequence is found at position %s in the alignment\"%(\n# gap_free[index],ifab(gap_free[index]==seq[i2], '==', '!='),seq[i2],index,i2)\n\ni3 = coords_ali2seq(seq,[i2])[0]\n#print \"Position %s of alignment contains char %s of sequence\\n\"%(i2,i3)\n#print \"%s -> %s -> %s\"%(index, i2, i3)\nassert(index==i3)\nif index != i3:\nprint \"\\n\\nOOPs\\n\\n\"\n\nprint \"Testing gap reverse translation on: \\n%s and \\n%s\"%(seq,gap_free)\nfor i2 in range(0,len(seq)):\n#print \"Position %s of alignment\"%i2\ni3 = coords_ali2seq(seq,[i2])[0]\n#print \"Position %s of alignment contains char %s of sequence\"%(i2,i3)\n\ni4 = coords_seq2ali(seq,[i3])[0]\n#print '%s -> %s -> %s'%(i2,i3,i4)\n#print \"%s%s%s Char %s of sequence is found at position %s in the alignment\\n\"%(\n# gap_free[i4],ifab(gap_free[i2]==seq[i4], '==', '!='),seq[i4], i2,i4)\n\n############################################################\n#\n# Flipping the case of aligned sequences (to mark exons, for example)\n#\n############################################################\n\ndef upcase_alignment(seqs_aligned, seqs_upcased):\n# uses the up/down case information from seqs_upcased\n# and thealignment informatino from seqs_aligned,\n# to generate a sequence that preservs the best\n# alignment, while at the same time displaying extra\n# information based on the casing (example: gene/intergenic)\n# tools.upcase_alignment(['--M---A-NO---L-IS--'],['mAnOlIs']) -> ['--m---A-nO---l-Is--']\nnewseqs = []\nfor seq_aligned, seq_upcased in map(None, seqs_aligned, seqs_upcased):\nnewseqs.append(upcase_ali(seq_aligned, seq_upcased))\nreturn newseqs\n\ndef upcase_ali(seq_aligned, seq_upcased):\n\ngapless_seq = gapless(seq_aligned,'-.')\nif not string.upper(seq_upcased)==gapless_seq:\npp({'gapless': gapless_seq, 'upcase': seq_upcased},2,60)\npw(['upcased','gapless','consensus'],\n[seq_upcased,gapless_seq,compute_clustal_consensus([string.upper(seq_upcased),gapless_seq])])\nassert(string.upper(seq_upcased)==gapless_seq)\n# a sequence that's both aligned and upcased\nseq_both = []\ni = 0\nfor char in seq_aligned:\nif char=='-': seq_both.append('-')\nelif char=='.': seq_both.append('.')\nelse:\nseq_both.append(seq_upcased[i])\ni = i+1\nreturn string.join(seq_both,'')\n\ndef upcase_only(seq):\nupseq = string.upper(seq)\nnon_down = []\nfor i in range(0,len(seq)):\nif seq[i] == upseq[i]:\nnon_down.append(seq[i])\nreturn string.join(non_down,'')\n\ndef seqs_upcase_only(seqs):\nassert(all_same(map(len,seqs)))\nmaxstart,minend = 0,len(seqs[0])\nfor seq in seqs:\nif '*' in seq: continue\nthisstart = min(string.find(seq,'A'),string.find(seq,'C'),string.find(seq,'G'),string.find(seq,'T'))\nmaxstart = max(thisstart,maxstart)\nthisend = max(string.rfind(seq,'A'),string.rfind(seq,'C'),string.rfind(seq,'G'),string.rfind(seq,'T'))\nminend = min(thisend,minend)\nreturn get_subseqs(seqs,maxstart,minend)\n\ndef first_upcase(seq):\nstarts = string.find(seq,'A'),string.find(seq,'C'),string.find(seq,'G'),string.find(seq,'T')\nif max(starts)==-1: return -1\nelse: return min(gte(starts,0))\n\ndef last_upcase(seq):\nreturn max(string.rfind(seq,'A'),string.rfind(seq,'C'),string.rfind(seq,'G'),string.rfind(seq,'T'))\n\ndef first_lowcase(seq):\nstarts = [string.find(seq,'a'),string.find(seq,'c'),string.find(seq,'g'),string.find(seq,'t')]\nif max(starts)==-1: return -1\nelse: return min(gte(starts,0))\n\ndef last_lowcase(seq):\nreturn max(string.rfind(seq,'a'),string.rfind(seq,'c'),string.rfind(seq,'g'),string.rfind(seq,'t'))\n\ndef upcase_this(this, case_teller):\nassert(len(this)==len(case_teller))\nthat = []\nfor i in range(0,len(this)):\nif string.upper(case_teller[i])==case_teller[i]:\nthat.append(string.upper(this[i]))\nelif string.lower(case_teller[i])==case_teller[i]:\nthat.append(string.lower(this[i]))\nelse:\nthat.append(this[i])\nreturn string.join(that,'')\n\n#def upcase_portion(gapseq,upseq):\n# # find the best match of upseq in seq in upcase it\n# seq = string.\n#\n#\n# # searching the entire sequence\n# complete = string.find_all(upseq)\n# if len(complete)==1:\n# start,end = complete[0],complete[0]+len(upseq)\n# else:\n# # first find the beginning\n# length_to_search = len(upseq)/2\n# while 1:\n# starts = find_all(upseq)\n#\n#\n# else:\n#\n#\n#\n#\n# alistart,ali = coords_seq2ali(gapseq,[start,end])\n\n############################################################\n#\n# Kinda like find only with sw as the search method\n#\n############################################################\n\ndef sw_find(seq, subsequence):\nimport sw\nhomologs = sw.quick_sw(subsequence, seq)\nif homologs: return homologs[0]['start']-1\nelse: return -1\n\n# category 7\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### LIST OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef multimap(functions, list):\nfor function in functions:\nlist = map(function, list)\nreturn list\n\ndef flatten(list_of_lists):\nflat_list = []\nmap(flat_list.extend, list_of_lists)\nreturn flat_list\n\ndef reverse_list(list):\nl2 = list[:]\nl2.reverse()\nreturn l2\n\ndef lappend(list, element):\n# same as: flatten([list, [element]])\nlist2 = list[:]\nlist2.append(element)\nreturn list2\n\ndef all_same(list, compfunc=None):\n# returns 1 if all the elements of the list are identical\n# if compfunc is specified then returns 1 if all the elements x\n# of the list satisfy 1==compfunc(x,first) where first is the first element\nif not list: return 1\nif len(list)==1: return 1\nfirst = list[0]\nif compfunc:\nfor element in list:\nif compfunc(element,first): continue\nelse: return 0\nelse:\nfor element in list:\nif element==first: continue\nelse: return 0\nreturn 1\n\ndef all_diff(list):\nreturn unique_len(list)==len(list)\n\ndef all_same_key(diclist, key):\n# returns 1 if the value indexed by key\n# is identical for every dic in diclist\nif not diclist: return 1\nif len(diclist)==1: return 1\nfirst = diclist[0][key]\nfor element in diclist:\nif element[key] != first: return 0\nreturn 1\n\ndef unique(list, comp=None):\nif comp: return unique_slow(list, comp)\nelse: return unique_fast(list)\n\ndef unique_len(list):\nreturn len(unique(list))\n\ndef unique_fast(list):\nnew_list = []\nseen_dic = {}\nfor element in list:\n# check if already seen, skip\nif seen_dic.has_key(element): continue\n# add it to seen list\nseen_dic[element] = None\n# add it to unique list\nnew_list.append(element)\nreturn new_list\n\ndef unique_slow(list, comp = (lambda x,y: x==y)):\nlist = list[:]\nif not list: return list\nuniquelist = []\nfor nextelement in list:\nnovel = 1\nfor seenelement in uniquelist:\nif comp(nextelement,seenelement):\nnovel = 0\nbreak\nif novel: uniquelist.append(nextelement)\nreturn uniquelist\n\ndef unique_hash(list, el2hash=lambda el: el, strict=0):\n# takes a function that returns a hashable key\n# for every element\nnew_list = []\nseen = {}\nfor element in list:\nkey = el2hash(element)\nif seen.has_key(key):\nif strict: assert(seen[key]==element)\nelse:\nseen[key] = element\nnew_list.append(element)\nreturn new_list\n\ndef diclist_unique(diclist, key):\nreturn unique_hash(diclist,lambda x,key=key: x[key])\n\ndef eliminate_duplicates(list):\n# assumes the elements are sorted and eliminates duplicates\nif len(list)<=1: return list[:]\nnewlist = [list[0]]\nfor i in range(1,len(list)):\nif newlist[-1]!=list[i]:\nnewlist.append(list[i])\nreturn newlist\n\ndef list_find_all(list, element):\nindices = []\nfor i in range(0,len(list)):\nif list[i] == element:\nindices.append(i)\nreturn indices\n\ndef merge_sorted(list1,list2):\n# assumes the two lists are sorted, and\n# that elements within them are unique\n# then merges them, keeping one copy of each element\nmerged,i,j, = [],0,0\nwhile i<len(list1) and j<len(list2):\nif list1[i] < list2[j]:\nmerged.append(list1[i])\ni = i+1\nelif list1[i] > list2[j]:\nmerged.append(list2[j])\nj = j+1\nelse:\nmerged.append(list1[i])\ni = i+1\nj = j+1\nif i<len(list1): merged.extend(list1[i:])\nelif j<len(list2): merged.extend(list2[j:])\nreturn merged\n\n#def list_find(list, element) -> list.index(element)\n\n#def join(list, element):\n# # modifies list, by appending element\n# # returns the list\n# # equivalent to flatten([x,[y]])\n# list.append(element)\n# return list\n\n#def append_unique(list, item):\n# # appends item to list only if it doesn't yet contain it\n# # note: O(n) has to search the entire list for item first\n# if not item in list:\n# list.append(list)\n\ndef split_list(list, is_separator):\n# input: list: a list of items\n# is_separator: a function s.t. is_separator(list[i]) == 1 only if element i is a separator\n# action: separates the list at every separator\n# output: the sublists, and the separators found\n\n# example: 3,3,3,'a','b',3,'c','d',3\n# -> [],[],[],['a','b'],['c','d'],[]\n# -> 3 3 3 3 3\n# note: always one more group than separator\n# note: groups can be empty if between consecutive separator elements\n\ngroups = []\nseparators = [-1]+list_find_all(map(is_separator, list),1)+[len(list)+1]\nfor i,j in map(None, separators[:-1], separators[1:]):\ngroups.append(list[i+1:j])\nreturn groups, mget(list, separators[1:-1])\n\n############################################################\n#\n# Sorting lists and dictionaries\n#\n############################################################\n\ndef my_sort(list,func=None):\nif func: return sort_transform(list,func)\nelse: return sort_copy(list)\n\ndef my_sort_rev(list,func=None):\nif func: return my_reverse(sort_transform(list,func))\nelse: return my_reverse(sort_copy(list))\n\ndef my_reverse(list):\nnewlist = list[:]\nnewlist.reverse()\nreturn newlist\n\ndef sort_copy(list):\n# useful for sorting on the fly without\nnew_list = map(None, list)\nnew_list.sort()\nreturn new_list\n\ndef sort_transform(list, function = lambda a: a):\naugmented = map(None, map(function,list), list)\naugmented.sort()\nreturn cget(augmented,1)\n\ndef sort_diclist(diclist, key):\n# faster: no lambdas\naugmented = map(None, cget(diclist,key), diclist)\naugmented.sort()\nreturn cget(augmented,1)\n\ndef sort_diclist_rev(diclist, key):\n# faster: no lambdas\naugmented = map(None, cget(diclist,key), diclist)\naugmented.sort()\naugmented.reverse()\nreturn cget(augmented,1)\n\ndef sort_diclist_multiple(diclist, keys):\n# faster: no lambdas\naugmented = map(None, cget_multiple(diclist,keys), diclist)\naugmented.sort()\nreturn cget(augmented,1)\n\ndef sort_diclist_multiple_rev(diclist, keys):\n# faster: no lambdas\naugmented = map(None, cget_multiple(diclist,keys), diclist)\naugmented.sort()\naugmented.reverse()\nreturn cget(augmented,1)\n\ndef diclist_unique(diclist, key):\nseen = {}\nnewlist = []\nfor dic in diclist:\nif seen.has_key(dic[key]): continue\nseen[dic[key]] = None\nnewlist.append(dic)\nreturn newlist\n\ndef shuffle(list):\nlist = list[:]\nfor i in range(0,len(list)):\nj = random.randrange(0,len(list))\nlist[i],list[j] = list[j],list[i]\n#print \"Flipping %s and %s. %s\"%(i,j,list)\nreturn list\n\n#def shuffle2(list):\n# NOTE: Not a correct algorithm.\n# return my_sort(list, lambda x,random=random: random.uniform(-.5,.5))\n\ndef test_shuffle(length=100, iters=20):\nsums = [0]*length\nfor iter in range(0,iters):\nlist = range(1,length+1)\nlist = shuffle(list)\nfor i in range(0,length):\nsums[i] = sums[i] + list[i]\nfor i in range(0,length):\nsums[i] = sums[i] / float(iters)\nprint \"The average for each bin after %s iterations should be approximately %s\"%(iters,length/2)\nprint map(int,map(round,sums))\nquick_histogram(sums)\n\ndef reorder_names_seqs(names, seqs, ordered):\nordered = ordered + set_subtract(names, ordered)\norder = map(names.index, filter(names.count, ordered))\nreturn mget(names, order), mget(seqs, order)\n\ndef common_names_seqs(names_seqs_list):\n# input: [(names1,seqs1), (names2,pseqs2), (names3,tseqs3)]\n# output: [names, seqs, pseqs, tseqs]\n\ndics = []\nfor names, seqs in names_seqs_list:\ndic = {}\nmset(dic,names,seqs)\ndics.append(dic)\n\ncommon_names = set_intersect_all(cget(names_seqs_list,0))\n\nresult = [common_names]\nfor dic in dics:\nresult.append(mget(dic,common_names))\nreturn result\n\ndef missing_data_consensus(names, seqs, pieces):\n# in a multiple alignment (names, seqs)\n# pieces of one of the sequences are aligned\n# with the entire length of the other sequences\n# construct a consensus for the pieces\n\nname2i = {}\nfor name_i in range(0,len(names)):\nname2i[names[name_i]] = name_i\nseqs = clustal_endgaps2dots(seqs)\n\nconsensus = []\nfor char_i in range(0,len(seqs[0])):\n\nchars = []\nfor seq_i in mget(name2i, pieces):\nchars.append(seqs[seq_i][char_i])\n\n# Gaps at the end are treated as if\n# we never saw any sequence for it.\nchars = set_subtract(chars, '.')\n\nothers = []\nfor seq_i in range(0,len(seqs)):\nif name2i.has_key(names[seq_i]): pass\nelse: others.append(seqs[seq_i][char_i])\n\nif len(chars) == 0:\n# none of the sequence covered that part\nconsensus.append('.')\nelif len(chars) == 1:\nconsensus.append(chars[0])\nelif all_same(chars) and all_same(others) and (not others or others[0]==chars[0]):\n# all the sequence pieces covering that base\n# agree (either a char or a gap. Append it)\nconsensus.append(chars[0])\nelse:\n# well now, we can use the other sequences in the\n# alignment to do some sort of a majority vote\n# on this particular base, but then we'd be\n# reasoning circularly. So we just include an N.\n# others would argue i should put a \".\", but it's\n# not really missing. we know there's a base there.\n\n# consensus.append(generate_profile(chars))\n# consensus.append(string.lower(generate_profile(chars)))\n\n#if all_same(others) and others[0] in chars:\n# consensus.append(others[0])\n#else:\nconsensus.append('N')\nreturn string.join(consensus,'')\n\n############################################################\n############################################################\n############################################################\n\ndef my_filter(x, test):\n#print 'x = %s'%(`x`[:40])\n#print 'test = %s'%(`test`[:40])\n#if not set_subtract(unique(test), [0,1]):\n# print \"unique(test)=%s\"%(unique(test))\nassert(set_subtract(unique(test),[0,1])==[])\nx_test = map(None, x, test)\nx_test_true = filter(lambda x_test: x_test[1], x_test)\nx_true = cget(x_test_true, 0)\nreturn x_true\n\ndef lt(list, cutoff):\n# return the elements of the list that are strictly less than the cutoff\nreturn filter(lambda el,cutoff=cutoff: el<cutoff, list)\n\ndef gt(list, cutoff):\n# return the elements of the list that are strictly greater than the cutoff\nreturn filter(lambda el,cutoff=cutoff: el>cutoff, list)\n\ndef lte(list, cutoff):\n# return the elements of the list that are less than or equal to the cutoff\nreturn filter(lambda el,cutoff=cutoff: el<=cutoff, list)\n\ndef gte(list, cutoff):\n# return the elements of the list that are greater than or equal to the cutoff\nreturn filter(lambda el,cutoff=cutoff: el>=cutoff, list)\n\ndef within(list, left, right):\n# return elements that are greater than or equal to left and strictly less than right\nreturn filter(lambda el,left=left,right=right: left<=el<right, list)\n\ndef all_gt(list,cutoff):\nfor el in list:\nif not el>cutoff: return 0\nreturn 1\n\ndef all_gte(list,cutoff):\nfor el in list:\nif not el>=cutoff: return 0\nreturn 1\n\ndef all_lt(list,cutoff):\nfor el in list:\nif not el<cutoff: return 0\nreturn 1\n\ndef all_lte(list,cutoff):\nfor el in list:\nif not el<=cutoff: return 0\nreturn 1\n\ndef count_lt(list, cutoff):\n# return the elements of the list that are strictly less than the cutoff\ncount = 0\nfor el in list:\nif el<cutoff: count = count+1\nreturn count\n\ndef count_gt(list, cutoff):\n# return the elements of the list that are strictly less than the cutoff\ncount = 0\nfor el in list:\nif el>cutoff: count = count+1\nreturn count\n\ndef count_lte(list, cutoff):\n# return the elements of the list that are strictly less than the cutoff\ncount = 0\nfor el in list:\nif el<=cutoff: count = count+1\nreturn count\n\ndef count_gte(list, cutoff):\n# return the elements of the list that are strictly less than the cutoff\ncount = 0\nfor el in list:\nif el>=cutoff: count = count+1\nreturn count\n\ndef count_within(list, left, right):\n# return elements that are greater than or equal to left and strictly less than right\ncount = 0\nfor el in list:\nif left<=el<right: count = count+1\nreturn count\n\n#def expand(initial, operation, num_iters):\n# # the opposite of reduce\n# result = [initial]\n# for element in list:\n# result.append(operation(initial, result[-1]))\n\ndef map_constant(list, func=lambda x,y: x, constant=0):\nresult = []\nfor item in list:\nresult.append(func(item, constant))\nreturn result\n\ndef map_repeat(num_iters, func=lambda x,y: x, constant=0):\nresult = [constant]\nfor i in range(0,num_iters-1):\nresult.append(func(result[-1], constant))\nreturn result\n\ndef map_multiple(funcs, list): # same as multimap\nfor func in funcs:\nlist = map(func,list)\nreturn list\n\ndef pick_n(list,n):\nimport random\nresults = []\nfor i in range(0,n):\nresults.append(random.choice(list))\nreturn results\n\ndef pick_one(dic):\n# {'A': .18, 'C': .32, 'G': .32, 'T': .18}\n# will generate A with probability .18 and so on\nitems = dic.items()\ncums = cumulative_sum(cget(items,1))\nif 1: #debug:\n#print cums\nx = random.uniform(0,cums[-1])\nbin = which_bin(cums, x, safe=1)\n#print \"%s is in bin %s and char %s. items=%s\"%(\n# x,bin,items[bin][0],items)\nreturn items[bin+1][0]\nelse:\nreturn items[which_bin(cums, random.uniform(0,cums[-1]), safe=1)][0]\n\ndef which_bin(bins, x, safe=0):\n# if we're interested in binning x with boundaries\n# 0, 5, 10, 15\n# then it will return which boundary it belongs in.\n# if x<0: -1\n# if 0<=x<5: 0\n# if 5<=x<10: 1\n# if 10<=x<15: 2\n# if x>=15: 3\nif x<bins[0]: return -1\nfor i in range(1,len(bins)):\nif x<bins[i]: return i-1\nif safe and x==bins[-1]: return len(bins)\nreturn len(i)+1\n\ndef test_which_bin():\nbins = range(0,10)\n#print bins\nfor iter in range(0,1000):\nx = random.uniform(bins[0], bins[-1])\nbin = which_bin(bins,x)\n#print \"%s is in bin %s\"%(x,bin)\n\n############################################################\n#\n# Compute aggregate values on lists\n#\n############################################################\n\ndef is_increasing(list):\nfor i in range(1,len(list)):\nif list[i]<list[i-1]: return 0\nreturn 1\n\ndef is_decreasing(list):\nfor i in range(1,len(list)):\nif list[i]>list[i-1]: return 0\nreturn 1\n\ndef longest_colinear(list):\nlongest = []\nfor sublist in superset(list):\nif len(sublist) > len(longest) and (is_increasing(sublist) or\nis_decreasing(sublist)):\nlongest=sublist\nreturn longest\n\ndef diclist_longest_colinear(diclist,key):\nlongest = []\nfor sublist in superset(diclist):\nif len(sublist) > len(longest) and (is_increasing(cget(sublist,key)) or\nis_decreasing(cget(sublist,key))):\nlongest=sublist\nreturn longest\n\ndef is_positive(x):\nreturn x>0\n\ndef sign(x):\nif x>0: return +1\nelif x<0: return -1\nelse: return 0\n\ndef sum(l):\nif not l: return 0\nreturn reduce(operator.add,l,0)\n\ndef min_max(l):\nreturn \"Min: %s, Median: %s, Avg: %s, N50: %s, Max: %s\"%(\ndisplay_bignum(min(l)),\ndisplay_bignum(median(l)),\ndisplay_bignum(avg(l)),\ndisplay_bignum(n50(l)),\ndisplay_bignum(max(l)))\n\ndef n50(l,fifty=50):\nl = my_sort(l)\nl.reverse()\nmiddle = sum(l)*fifty/100.0\ntot = 0\nfor i in range(0,len(l)):\ntot = tot+l[i]\nif tot>=middle: break\nreturn l[i]\n\ndef n50s(l,fiftys=[50,60,70,80,85,90,95],show_n=1):\nres = []\nfor fifty in fiftys:\nn50_length = n50(l,fifty)\nthis_res = 'n%s=%s'%(fifty,display_bignum(n50_length,0))\nif show_n:\n# how many supercontigs are at that length or greater\ncount = len(filter(lambda i,length=n50_length: i>=length, l))\nthis_res = this_res + ' in %s'%count\nres.append(this_res)\nreturn string.join(res,', ')\n\ndef percentiles(l, ps=[25,50,75]):\nl = tools.my_sort(l)\nresult = []\nfor p in ps:\nresult.append(l.index(int(len(l)*p/100.0)))\nreturn result\n\ndef log_avg(l,strict=1):\nif strict: return math.exp(avg(map(math.log,l)))\nelse: return math.exp(avg(map(math.log,filter(None,l))))\n\ndef log_variance(l):\nreturn math.exp(variance(map(math.log,l)))\n\ndef avg(l,precise=0):\nif not l: return 0\nif precise:\nreturn reduce(operator.add,l,0)/float(len(l))\nelse:\nreturn reduce(operator.add,l,0)/len(l)\n\ndef weighted_avg(l,weights,precise=0,safe=None):\nif not l: return 0\nassert(len(l)==len(weights))\ntot,div = 0,0\nfor x,w in map(None,l,weights):\ntot = tot+x*w\ndiv = div+w\nif safe!=None and not div: return safe\nif precise: return tot/float(div)\nelse: return tot/div\n\ndef safe_weighted_avg(l,weights,default,precise=0):\nif not l: return 0\nassert(len(l)==len(weights))\ntot,div = 0,0\nfor x,w in map(None,l,weights):\ntot = tot+x*w\ndiv = div+w\nif not div: return default\nif precise: return tot/float(div)\nelse: return tot/div\n\n#def weighted_avg(l,weights,precise=0):\n# if not l: return 0\n# if sum(weights)==0:\n# print \"tools.weighted_avg: All weights are zero!! Making them all 1\"\n# weights = [1]*len(l)\n# if precise:\n# return sum(vector_vector_mul(l,weights))/float(sum(weights))\n# else:\n# return sum(vector_vector_mul(l,weights))/sum(weights)\n\ndef median(l):\nif not l: return None\nl = my_sort(l)\nif len(l)%2: return my_sort(l)[len(l)/2]\nelse: return (l[len(l)/2]+l[len(l)/2-1])/2.0\n\ndef majority(l):\nif not l: return None\n#return my_sort(count_same(l).items(),lambda i: -i[1])[0][0]\ncounts = count_same(l)\nall_max = filter_diclist(counts.items(),1,max(counts.values()))\nif len(all_max)==1: return all_max[0][0]\nelse: return my_sort(cget(all_max,0))[len(all_max)/2]\n\ndef argmax(list): # def max_i\nmax_i, max_value = 0, list[0]\nfor i in range(1,len(list)):\nif list[i] > max_value:\nmax_value = list[i]\nmax_i = i\nreturn max_i\n\ndef argmin(list):\nmin_i, min_value = 0, list[0]\nfor i in range(1,len(list)):\nif list[i] < min_value:\nmin_value = list[i]\nmin_i = i\nreturn min_i\n\ndef my_max(list, f=lambda x: x):\n# returns the element that yields the largest f(x)\ni = argmax(map(f,list))\nreturn list[i]\n\ndef my_min(list, f=lambda x: x):\n# returns the element that yields the largest f(x)\ni = argmin(map(f,list))\nreturn list[i]\n\ndef stdev(l, failfast=1):\nreturn math.sqrt(variance(l,failfast=failfast))\n\ndef variance(l,failfast=1):\nif (not l) or len(l)==1:\nif failfast: raise \"tools.variance: Not enough samples. Need >= 2, got %s\"%len(l)\nelse: return 0#'N/A'\nm = avg(l,1)\ns = 0\nfor i in l:\ns = s + (i-m)*(i-m)\nreturn s / (len(l)-1)\n\ndef normalize_mean_shift(list, desired_mean):\nreturn vector_scalar_sub(list, avg(list)-desired_mean)\n\ndef normalize_sum_scale(list, desired_sum):\nreturn vector_scalar_mul(list, desired_sum/float(sum(list)))\n\ndef normalize_max_scale(list, desired_max):\nreturn vector_scalar_mul(list, desired_max/float(max(list)))\n\ndef normalize_max_min(list, desired_max, desired_min):\n\nreturn vector_scalar_mul(list, desired_max/float(max(list)))\n\ndef normalize_mean_scale(list, desired_mean):\nreturn vector_scalar_mul(list, desired_mean/float(avg(list,1)))\n\ndef normalize_mean_stdev(list, desired_mean):\nreturn vector_scalar_sub(list, avg(list)-desired_mean)\n\ndef normalize_sum_to(list,new_total):\nnewlist = list[:]\ncurrent_total = sum(list)\nmul = new_total / float(current_total)\nfor i in range(0,len(list)):\nnewlist[i] = mul*newlist[i]\nreturn newlist\n\ndef covariance(x,y):\nassert(len(x)==len(y))\nmeanx = avg(x,1)\nmeany = avg(y,1)\n#print \"mean x: %s, mean y: %s\"%(meanx, meany)\n# method 1: E((X-EX)(Y-EY))\n#cov = 0\n#for i in range(0,len(x)):\n# cov = cov + (x[i]-meanx)*(y[i]-meany)\n#cov = cov/(len(x)-1)\n\n# method 1: E(XY)-EXEY\ncov2 = 0\nfor i in range(0,len(x)):\ncov2 = cov2 + x[i]*y[i]\ncov2 = float(cov2)/len(x) - (meanx * meany)\ncov2 = len(x)/float(len(x)-1)*cov2\n\n#print \"Method 1: %s Method 2: %s\"%(cov,cov2)\n\nreturn cov2\n\ndef correlation(x,y):\n# the correlation between (x[i],y[i])\nreturn covariance(x,y)/(stdev(x)*stdev(y))\n\ndef fit_line(x,y):\n# fit an Maximum likelihood y-on-x regression line on (x[i],y[i])\n# it then calculates the standard deviation of the noise in the fit\n\nvarx = variance(x)\nif not varx: raise \"tools.fit_line: y-on-x regression impossible when variance(x) is zero.\"\n\nslope = covariance(x,y) / varx\noffset = avg(y) - slope * avg(x)\n\nnoise = vector_vector_sub(y, ax_plus_b(x,slope,offset))\n\nreturn slope, offset, stdev(noise)\n\ndef ax_plus_b(x,a,b):\nreturn map(lambda x,a=a,b=b: a*x+b, x)\n\ndef ax_plus_b_noise(x,a,b,s):\nreturn map(lambda x,a=a,b=b,s=s,random=random: a*x+b+random.gauss(0,s), x)\n\ndef test_fit_line():\n\n# plot a lot of numbers\nx = map(lambda x,random=random: 100*random.random(), range(0,10000))\na,b=2,-300\ny = ax_plus_b_noise(x,a,b,20)\n\nquick_plot(x,y)\n\na1,b1,sigma = fit_line(x,y)\ny2 = ax_plus_b(x,a1,b1)\n\ndeviations = vector_vector_sub(y,y2)\nquick_histogram(deviations)\n\nprint 'a,b=(%s,%s) guess=(%s,%s) with SigmaNoise=%s'%(a,b,a1,b1,sigma)\n\ndef find_outliers(samples):\nsamples = samples[:] # make a copy\nsamples.sort() # sort it\nmean,std = avg(samples),stdev(samples)\nprint \"Less than %s: %s\"%(mean-3*std,\nlen(filter(lambda s,mean=mean,std=std: s<mean-3*std,samples)))\nfor i in range(-3,2,1):\nlow,high = mean+i*std,mean+(i+1)*std\nsamples_in = filter(lambda s,l=low,h=high: l<=s<h, samples)\nprint \"Range %s to %s: %s samples (%s)\"%(\nlow,high,len(samples_in),safe_percentile(len(samples_in),len(samples)))\nprint \"More than %s: %s\"%(mean+3*std,\nlen(filter(lambda s,mean=mean,std=std: mean+3*std<=s,samples)))\n\n############################################################\n#\n# Primitive Operations\n#\n############################################################\n\ndef vector_add_noise(x,s):\n# adds a random error iid N(0,s^2)\nimport random\ny = x[:]\nfor i in range(0,len(y)):\ny[i] = y[i] + random.gauss(0,s)\nreturn y\n\ndef vector_scalar_op(v1, a, op):\nv3 = v1[:]\nfor i in range(0,len(v3)):\nv3[i] = op(v1[i],a)\nreturn v3\n\ndef vector_scalar_add(v1, a):\nreturn vector_scalar_op(v1,a,operator.add)\n\ndef vector_scalar_sub(v1, a):\nreturn vector_scalar_op(v1,a,operator.sub)\n\ndef vector_scalar_mul(v1, a):\nreturn vector_scalar_op(v1,a,operator.mul)\n\ndef vector_vector_op(v1,v2,op):\nassert(len(v1) == len(v2))\nv3 = [0]*len(v1)\nfor i in range(0,len(v1)):\nv3[i] = op(v1[i], v2[i])\nreturn v3\n\ndef vector_vector_add(v1,v2):\nreturn vector_vector_op(v1,v2,operator.add)\n\ndef vector_vector_sub(v1,v2):\nreturn vector_vector_op(v1,v2,operator.sub)\n\ndef vector_vector_mul(v1,v2):\nreturn vector_vector_op(v1,v2,operator.mul)\n\ndef vector_vector_div(v1,v2):\nreturn vector_vector_op(v1,v2,operator.div)\n\n############################################################\n\ndef float_range(minv, maxv, increment):\nassert(increment>0)\nvalues = []\ncurrent = minv\nwhile current < maxv:\nvalues.append(current)\ncurrent = current + increment\n#values.append(maxv)\nreturn values\n\ndef float_range_rev(minv, maxv, increment):\nassert(increment<0)\nvalues = []\ncurrent = minv\nwhile current > maxv:\nvalues.append(current)\ncurrent = current + increment\n#values.append(maxv)\nreturn values\n\n############################################################\n# all_pairs\n\ndef cumulative_sum(quality):\nif not quality: return quality\nsum_q = quality[:]\nfor i in range(1,len(quality)):\nsum_q[i] = sum_q[i-1]+quality[i]\nreturn sum_q\n\ndef cumulative_avg(quality):\nif not quality: return quality\nsum_q = quality[:]\navg_q = quality[:]\nfor i in range(1,len(quality)):\nsum_q[i] = sum_q[i-1]+quality[i]\navg_q[i] = sum_q[i]/float(i)\nreturn avg_q\n\ndef list2pairs(list,dist=lambda a,b: (a,b)):\n# input: an ordered list of n positions\n# output: the distances between consecut\ndists = []\nfor a,b in map(None, list[:-1], list[1:]):\ndists.append(dist(a,b))\nreturn dists\n\ndef list2pairs_all(list,\ndist=lambda a,b: (a,b),\ncutoff = None):\npairs = []\nfor i in range(0,len(list)):\nif i%100==0: print 'i=%s/%s'%(i,len(list))\nfor j in range(i+1, len(list)):\na,b = list[i],list[j]\nif not cutoff or cutoff(a,b,i,j):\npairs.append(dist(a,b))\nif len(pairs) % 10000==0: print \"pairs=%s\"%len(pairs)\nreturn pairs\n\ndef all_pairs_in_range(min,max):\npairs = []\nfor i in range(min,max):\nfor j in range(i+1,max):\npairs.append((i,j))\nreturn pairs\n\ndef all_pairs(list):\npairs = []\nfor i in range(0,len(list)):\nfor j in range(i+1,len(list)):\npairs.append((list[i],list[j]))\nreturn pairs\n\n############################################################\n#\n# Window-based computations\n#\n############################################################\n\ndef sum_n_continuous(a, n):\n# a=[1,2,3,4,5,6,7] n=3 sum_n_continuous(a,n)=[6,9,12,15,18,0,0]\n# if i in 0 <len(a)-n+1:\n# sum[i] = a[i] + a[i+1] + a[i+2] + ... + a[i+n]\n# else:\n# sum[i] = 0\n# a=[1,2,3,4,5,6,7]\n# r=[=====,-> 6\n# =====, -> 9\n# =====, -> 12\n# =====, -> 15\n# =====, -> 18\n# =====, -> 0\n# =====, -> 0\ns = [0] * len(a)\nfor i in range(0, len(a)-n+1):\nsum = 0\nfor j in range(i, i+n):\nsum = sum + a[j]\ns[i] = sum\nreturn s\n\ndef sum_window(quality, n):\ncum = cumulative_sum(quality)\ncum.insert(0,0)\ndiff = map(operator.sub, cum[n:], cum[0:-n])\nreturn diff\n\ndef best_n_continuous(quality, n):\n# trim to a length of 500, maximizing total quality\ndiff = sum_window(quality, n)\nif not diff: return 0,len(quality)-1\nqual_sum = max(diff)\nbest = diff.index(qual_sum)\nreturn best,best+n-1\n\n############################################################\n#\n# Multiple synchronized lists\n#\n############################################################\n\ndef unpack(el):\nif not el: return []\nindices = range(0,len(el[0]))\nreturn map(lambda index,el=el: map(lambda elements,index=index:elements[index],\nel),\nindices)\n\ndef intercalate(lists): # shuffle\n# intercalate, insert as layers\n# pick one element from each list\n# shuffle([[1,2,3],['a','b','c']]) -> [1,'a',2,'b',3,'c']\n\nassert(all_same(map(len,lists)))\n\nmerged = []\nfor i in range(0,len(lists[0])):\nmerged.extend(cget(lists,i))\nreturn merged\n\n#example of unpack\n# list_of_elements = map(None, ['do','re','mi'], [1,2,3], ['one','two','three'], ['a','b','c'])\n# notes, nums, numbers, chars = unpack(list_of_elements)\"\n\n# unpack(([1,2,3], ['a','b','c'], ['do','re','mi'])) -> [[1, 'a', 'do'], [2, 'b', 're'], [3, 'c', 'mi']]\n# unpack(([1, 'a', 'do'], [2, 'b', 're'], [3, 'c', 'mi'])) -> [[1,2,3], ['a','b','c'], ['do','re','mi']]\n\ndef sort_synchronized(tuple_of_lists, i, comp_operation = cmp):\nlist_of_tuples = unpack(tuple_of_lists)\nlist_of_tuples.sort(lambda a,b,op=comp_operation,i=i: op(a[i],b[i]))\nreturn unpack(list_of_tuples)\n\n# to sort on the third item (index 2) use:\n#a,b,c,d,e = sort_synchronized((a,b,c,d,e),2,cmp)\n\ndef filter_synchronized(tuple_of_lists, i, filter_operation = None):\nlist_of_tuples = unpack(tuple_of_lists)\nif filter_operation:\nlist_of_tuples = filter(lambda x,op=filter_operation,i=i: op(x[i]), list_of_tuples)\nelse:\nlist_of_tuples = filter(lambda x,i=i: x[i], list_of_tuples)\nif list_of_tuples: return unpack(list_of_tuples)\nelse: return map(lambda x: [], tuple_of_lists)\n\n############################################################\n#\n# PACK And UNPACK for DICTIONARIES\n#\n############################################################\n\ndef dict_pack(dict, keys, newkeys):\n\"\"\" example:\npack(d, ['num_list','notes_list','letter_list'], ['num','note','letter'])\ntakes d={'num_list': [1,2,3],\n'notes_list': ['do','re','mi'],\n'letter_list': ['a','b','c']}\nand returns:\n[{'p':1, 'b': 'do', 'sw': 'a'},\n{'p':2, 'b': 're', 'sw': 'b'},\n{'p':3, 'b': 'mi', 'sw': 'c'}]\n\"\"\"\nif not len(keys) == len(newkeys):\nprint \"ALERT! unmatched lengths of keys=\"+`keys`+\" and newkeys=\"+`newkeys`\ntuple_of_lists = mget(dict, keys)\nlist_of_tuples = unpack(tuple_of_lists)\nlist_of_dicts = map(lambda tuple,newkeys=newkeys: items2dic(map(None, newkeys, tuple)),\nlist_of_tuples)\nreturn list_of_dicts\n\ndef dict_unpack(list_of_dicts, keys, newkeys):\nlist_of_tuples = map(lambda dict,newkeys=newkeys: mget(dict, newkeys),\nlist_of_dicts)\ntuple_of_lists = unpack(list_of_tuples)\ndict_of_lists = items2dic(map(None, keys, tuple_of_lists))\nreturn dict_of_lists\n\n# category 8\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### DICTIONARY OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Creating and copying dictionaries\n#\n############################################################\n\ndef empty_dics(n):\n# returns a list of n empty dictionaries\n# equivalent to [{}]*n only no pointer mess\nlist = []\nfor i in range(0,n):\nlist.append({})\nreturn list\n\ndef empty_lists(n):\n# returns a list of n empty lists\n# equivalent to [[]]*n only no pointer mess\nlist = []\nfor i in range(0,n):\nlist.append([])\nreturn list\n\ndef items2dic(items):\n# create a dictionary from an items list\n# items2dic(d.items()) returns a copy of d\ndic = {}\nfor key,value in items:\ndic[key] = value\nreturn dic\n\n#def dic_subset(dic, keys):\n# keys = set_intersect(keys,dic.keys())\n# newdic = items2dic(map(None,keys,tools.mget(dic,keys)))\n# return newdic\n\ndef copy_dic(dic):\n# returns a copy of a dictionary\n# same as a[:] for a list\nreturn items2dic(dic.items())\n\ndef map_on_dic(dic, func):\nreturn items2dic(map(None, dic.keys(), map(func,dic.values())))\n\ndef set_key(dictionary, key, value):\n# functional implementation of dictionary assignment\n# useful for using in map\ndictionary[key] = value\n\ndef reverse_dictionary(dic, strict=0):\n# reverses the mapping from values to keys\nrdic = {}\nfor key, value in dic.items():\nif strict: assert(not rdic.has_key(value))\nrdic[value] = key\nreturn rdic\n\n############################################################\n#\n# Getting and setting key(s) in dictionary(ies)\n#\n############################################################\n\ndef mget(list,keys,strict=1):\nresults = []\nif strict:\nfor key in keys:\nresults.append(list[key])\nelse:\nfor key in keys:\nif list.has_key(key):\nresults.append(list[key])\nreturn results\n\ndef mget_l(list, indices, strict=1):\nresults = []\nfor index in indices:\nif strict: results.append(list[index])\nelif 0 <= abs(index) < len(list): results.append(list[index])\nreturn results\n\ndef mget_d(dict, keys):\nreturn map(dict.get, keys)\n\ndef mset(dict, keys, values):\nfor key,value in map(None,keys,values):\n#print \"Setting \"+`key`+\" to \"+`value`\ndict[key] = value\n\ndef cget(diclist, key, strict=1): # cross_get was: gather(diclist,key)\n# gathers the same key from a list of dictionaries\n# can also be used in lists\n\n# input: a list of dictionaries all of which contains key\n# output: a list of elements d[key] for each d in diclist\nif strict:\n# return map(lambda d,key=key: d[key], diclist)\nresult = [None]*len(diclist)\nfor i in range(0,len(diclist)):\nresult[i] = diclist[i][key]\nreturn result\nelse:\nresults = []\nfor dic in diclist:\nif dic and generic_has_key(dic,key):\nresults.append(dic[key])\nreturn results\n\ndef my_has_key(dic, key, generic=0):\nif generic: return generic_has_key(dic, key)\nelse: return dic.has_key(key)\n\ndef generic_has_key(dic_or_list, key):\nwhich_type = type(dic_or_list)\nif which_type==types.ListType or which_type==types.TupleType or which_type==types.StringType:\nif key>=0: return len(dic_or_list) > key\nelse: return len(dic_or_list) >= -key\nelif which_type==types.DictType:\nreturn dic_or_list.has_key(key)\nelse:\nraise \"Unexpected type (not Dict or List)\", which_type\n\ndef cget_l(lists, index, strict=1):\nresults = []\nif strict:\nfor list in lists:\nresults.append(list[index])\nelse:\nif index >= 0:\nfor list in lists:\nif len(list)>=index+1:\nresults.append(list[index])\nelse:\nfor list in lists:\nif len(list)>=abs(index)+1:\nresults.append(list[index])\nreturn results\n\ndef cset(diclist, key, valuelist):\nfor dic,value in map(None, diclist, valuelist):\ndic[key] = value\n\ndef cget_multiple(diclist, keys, strict=0):\nvaluelist = []\nfor dic in diclist:\nvaluelist.append(mget(dic,keys,strict))\nreturn valuelist\n\ndef cget_deep(diclist, key_series, strict=0):\nvalues = diclist\nfor key in key_series:\nvalues = cget(values, key, strict)\nreturn values\n\ndef dic_subset(dic, keys, strict=1):\n# def dicsel(dic, keys): -> renamed dic_subset\n\n# gets the subset of keys that dic contains\n# creates a new dictionary from it and returns it\n\n# if strict, it requires that dic has every key\nnew_dic = {}\nfor key in keys:\nif not strict and not dic.has_key(key): continue\nnew_dic[key] = dic[key]\nreturn new_dic\n\ndef diclist_subset(diclist, keys, strict=1):\n# calls dicsel on diclist\nnewdiclist = []\nfor dic in diclist:\nnewdiclist.append(dic_subset(dic,keys,strict))\nreturn newdiclist\n\ndef rename_key(diclist, oldkey, newkey):\ncset(diclist,newkey,cget(diclist,oldkey))\ncdel(diclist,oldkey)\n\ndef rename_keys(diclist, oldkeys, newkeys):\nfor oldkey, newkey in map(None, oldkeys, newkeys):\nrename_key(diclist, oldkey, newkey)\n\ndef apply_to_key(diclist, key, function):\ncset(diclist,key,map(function,cget(diclist,key)))\n\ndef apply_to_keys(diclist, keys, function):\nfor key in keys: apply_to_key(diclist,key,function)\n\ndef cdel(diclist, key):\nn_del = 0\nfor dic in diclist:\nif dic.has_key(key):\nn_del = n_del + 1\ndel(dic[key])\nreturn n_del\n\ndef cdel_multiple(diclist, keys):\nreturn map(lambda key,diclist=diclist: cdel(diclist,key),keys)\n\ndef mdel(dic, keys):\nn_del = 0\nfor key in keys:\nif dic.has_key(key):\nn_del = n_del + 1\ndel(dic[key])\nreturn n_del\n\ndef mdel_list(list, indices):\nn_del = 0\nindices.sort()\nindices.reverse()\nfor index in indices:\nif len(list) > index:\nn_del = n_del + 1\ndel(list[index])\nreturn n_del\n\ndef get_all_but(list, i):\nresult = []\nfor j in range(0,len(list)):\nif j!=i: result.append(list[j])\nreturn result\n\n############################################################\n#\n# Counting dictionary keys\n#\n############################################################\n\ndef describe_elements(list,sorter=lambda x:x):\nresult = []\nfor item,count in my_sort(count_same(list).items(),lambda f,sorter=sorter: sorter(f[0])):\nif count==1:\nresult.append('%s'%item)\nelse:\nresult.append('%s(x%s)'%(item,count))\nreturn string.join(result,',')\n\ndef count_same(list):\n# from a list of names, constructs a dictionary with the counts of all unique names\n# input: a list\n# output: a dictionary, where the keys are the unique items of the input list,\n# and the values are the number of times each item appears\nd = {}\nfor el in list:\nif d.has_key(el):\nd[el] = d[el] + 1\nelse:\nd[el] = 1\nreturn d\n\ndef countdic2percdic(dic, total=None):\nif total==None: total = sum(dic.values())\npercdic = {}\nfor key in dic.keys():\npercdic[key] = perc(dic[key],total)\nreturn percdic\n\ndef countdic2ratiodic(dic):\ntotal = float(sum(dic.values()))\nratiodic = {}\nfor key in dic.keys():\nratiodic[key] = dic[key]/total\nreturn ratiodic\n\ndef countdic_sum(countdic1, countdic2, strict=0):\nsumdic = {}\nif strict:\nfor key in unique(countdic1.keys()+countdic2.keys()):\nsumdic[key] = countdic1[key] + countdic2[key]\nelse:\nfor key in unique(countdic1.keys()+countdic2.keys()):\nsumdic[key] = sum([0]+cget([countdic1,countdic2],key,0))\nreturn sumdic\n\ndef count_key_instances(dictionary_list):\n# input: a list of dictionaries\n# output: a dictionary of key counts\n\n# from a list of dictionaries, count how many keys are used\n# Ex: from 15 dictionaries, 13 had 'name', 12 had 'length' etc\nlist_of_all_keys = []\nfor dictionary in dictionary_list:\nlist_of_all_keys.extend(dictionary.keys())\nreturn count_same(list_of_all_keys)\n\ndef sum_counts(counts):\n# sum the counts returned by the above function\nks = []\nfor c in counts:\nks.extend(c.keys())\ntotals = {}\nfor k in ks:\ntotal = 0\nfor c in counts:\nif c.has_key(k): total = total + c[k]\ntotals[k] = total\nreturn totals\n\ndef mul_counts(countdic1, countdic2):\n# multiply the counts of common keys\nproducts = {}\nfor key in countdic1.keys():\nif countdic2.has_key(key):\nproducts[key] = countdic1[key] * countdic2[key]\nreturn products\n\n############################################################\n#\n# Grouping Into Dictionaries\n#\n############################################################\n\ndef list2map(list):\n# returns a map from value to the indices that contain\n# that value\nvalue2id = {}\nfor i in range(0,len(list)):\nvalue = list[i]\nif not value2id.has_key(value): value2id[value] = []\nvalue2id[value].append(i)\nreturn value2id\n\ndef diclist2map(diclist, key):\n# returns a map that gives the index of every element\n# of the diclist that has a particular key.\nreturn list2map(cget(diclist,key))\n\ndef reverse_map_dic(dic,strict=1):\nreverse = {}\nfor key,value in dic.items():\nif not reverse.has_key(value): reverse[value]=key\nelif strict: raise \"Not one-to-one mapping. Two keys map to same value %s\"%value, (key, reverse[value])\nelse: pass#print \"Two keys map to same value %s\"%value, (key, reverse[value])\nreturn reverse\n\ndef reverse_map_list(list,strict=1):\nreverse = {}\nfor key,value in map(None, range(0,len(list)), list):\nif not reverse.has_key(value): reverse[value]=key\nelif strict: raise \"Not one-to-one mapping. Two indices contain same value %s\"%value, (key, reverse[value])\nelse: pass#print \"Two indices contain same value %s\"%value, (key, reverse[value])\nreturn reverse\n\ndef group_diclist(diclist, key):\n# groups the dictionaries in diclist if they share common values for key key\n# the elements in the subsists accessed by grouped[key] are in the same order\n# as the elements in diclist before the grouping\ngrouped = {}\nfor dic in diclist:\nvalue = dic[key]\nif not grouped.has_key(value): grouped[value] = []\ngrouped[value].append(dic)\nreturn grouped\n\ndef group_single(list, elmt2key = lambda name: name, elmt2value = lambda name: name):\n# groups a list based on the output of the elmt2key function\n#\n# input: list: a list of elements\n# elmt2key a function that can be applied to every element\n# output: a dictionary indexed by the outputs of the function elmt2key\n# where each value contains a list of all the elements that had\n# the same output\n# dic[key] = list of all elements of list for which elmt2key(elmt)=key\ndic = {}\nfor name in list:\nkey = elmt2key(name)\nif not dic.has_key(key): dic[key] = []\ndic[key].append(elmt2value(name))\nreturn dic\n\ndef group_bipartite(links, edge2point1=lambda link: link[0], edge2point2=lambda link: link[1]):\n# joins a list of links in a biparatite graph into equivalence classes\n# where the criterion for joining two edges is that they share one of\n# their two vertices\n\n# the trick is to use two different name spaces,\n# by appending 'x' to one set of edge names\n# and appending 'y' to another one of these sets\n# then one can use the group_anyof function\n\nreturn group_anyof(links, lambda link,f1=edge2point1,f2=edge2point2: ['x%s'%f1(link),'y%s'%f2(link)])\n\ndef group_anyof(list, elmt2keys = lambda elmt: elmt, debug=0):\n# groups a list based on the output of the call\n# elmt2keys(list) which returns all the keys by\n# which a particular element can be indexed.\n#\n# if two elements in the list share an index,\n# they belong to the same group\n#\n# maintains a two-level hash table\n# key -> element -> group\n#\n# every time a new element comes in, we have a new set of keys\n# we find all the elements that were indexed by those keys\n# using the first mapping key->element\n# then we find all the groups to which these elements belong\n# using the second mapping element->group\n#\n# now if there's no groups, we start a new group\n# if there's only one group, we simply extend this group\n# to include the new element seen\n# if there's more than one group, we have to merge them\n# together and update all the element->group mappings\n#\n# we also add an entry for element->group that points to\n# the appropriate group\n#\n# finally, we update the key->element so that all the keys\n# point to this new element. We don't care about overriding\n# the mappings for previously indexed elements, since the\n# second level of the hash table will still point to the same\n# group, regardless of which element we go through.\n\nkey2elmt = {} # maps from every key to some element that is indexed by it\nelmt2group = [] # maps from an element to the group that contains it\ngroups = [] # the list of groups\n\n# only call the function once\ni2keys = map(elmt2keys, list)\n\nfor i in range(0,len(list)):\n\n# the keys by which that element is indexed\nkeys = i2keys[i]\n\n# other elements that contain such keys\n# note: strict=0, since keys may be unseen before\nother_elmts = mget(key2elmt, keys, strict=0)\n\n# their groups\n# note: It's strict, coz every element should have a group\ngroups_merged = unique(mget(elmt2group, other_elmts,strict=1))\n\nif debug: print \"Element %s (%s) has %s keys: %s, that join it with %s elements in %s groups\"%(\ni, list[i], len(keys), keys, len(other_elmts), len(groups_merged))\n\nif len(groups_merged) == 0:\n# no elements contained any of the keys (hence no groups)\n# hence i'm starting a new group\ngroups.append([i]) # i can only append, so that i don't change any names\ndad_i = len(groups)-1 # the index of the group is the length-1, since it's the last one\nelif len(groups_merged)==1:\n# only one group contained elements with common keys\ndad_i = groups_merged[0] # which group am i appending to\ngroups[dad_i].append(i)\nelse:\n# more than one group contained elements with common keys.\n# i must merge them\ndad_i = groups_merged[0] # pick one as the daddy, that will include all others\ndad = groups[dad_i]\nfor group_index in groups_merged[1:]:\n# i'm erasing every sibbling, and giving all the elements to daddy\n\ngroup = groups[group_index]\n\n# 1. first of all, the elmt2group will now point to daddy directly\nmset(elmt2group, group, [dad_i]*len(group))\n\n# 2. then extend daddy with all the elements of each other group in the set to be merged\ndad.extend(group)\n\n# 3. finally empty each of the other groups\ngroups[group_index] = None # better None then empty list, to fail fast, if i try to append there\n\n# 4. and then add our latest element into the group merge\ndad.append(i)\n\n# now, reset all the keys that index the current element to point to the current element\n# mset(key2elmt, keys, [i]*len(keys))\n# note: i could also only reset the new keys\nfor key in keys:\nif not key2elmt.has_key(key): key2elmt[key] = i\n\n# finally add another entry in our elmt2group table, that points to daddy\nelmt2group.append(dad_i) # this cannot be insert, since i'm indexing with i\n\nif debug: pp(map(lambda g,list=list: mget(list,g), filter(None,groups)),1)\n\n# throw away all the deleted groups, and sort the remaining ones by length\ngroups = filter(None, groups)\ngroups = my_sort(groups, lambda g: -len(g))\n\n# instead of lists of indices, now turn the groups into actual sets of\n# elements of the original list\ngroups = map(lambda g,list=list: mget(list,g), groups)\n\nassert(sum(map(len,groups))==len(list))\n\nreturn groups\n\n#def group_single(lst, name2key = lambda name: name):\n# distinct_names = []\n# for elmt in lst:\n# name = name2key(elmt)\n# if not name in distinct_names:\n# distinct_names.append(name)\n# # construct the empty groups\n# groups = {}\n# for name in distinct_names:\n# groups[name] = []\n#\n# for elmt in lst:\n# groups[name2key(elmt)].append(elmt)\n# return groups\n\ndef group_pairs(pairs, name2group = lambda name: name):\n\"\"\"Input: a list of pairs: (x1, x2, value)\na mapping: x1 -> group(x1)\nOutput: a dictionary: d[group1][group2] = list of\nall values for which a pair existed such\nthat map(x1) = group1 and map(x2) = group2\"\"\"\n# gather all the distinct group names\ndistinct_names = []\nfor pair in pairs:\nname1,name2 = map(name2group, pair[:2])\nif not name1 in distinct_names:\ndistinct_names.append(name1)\nif not name2 in distinct_names:\ndistinct_names.append(name2)\nn = len(distinct_names)\ndistinct_names.sort()\n# construct the empty groups\ngroups = {}\nfor i in range(0,n):\ngroups[distinct_names[i]] = {}\nfor j in range(i,n):\ngroups[distinct_names[i]][distinct_names[j]] = []\n# append the values in each group\nfor pair in pairs:\nname1,name2 = map(name2group, pair[:2])\nvalue = pair[2]\ngroups[min(name1,name2)][max(name1,name2)].append(value)\nreturn groups\n\n############################################################\n#\n# PRINTING dictionaries\n#\n############################################################\n\ndef print_dictionary(dic, keys, lengths, separators, options):\nwrite_dictionary(sys.stdout, dic, keys, lengths, separators, options)\n\ndef write_dictionary(f, dic, keys, lengths, separators, functions = {}):\n\"\"\" ex: display_dictionary({'a': 7, 'b': 3},\n['a','b'],\n[3,-7],\n{'a': display_bignum})\"\"\"\nfor key,length,separator in map(None, keys, lengths, separators):\nformat_str = '%'+`length`+'s'\nif dic.has_key(key):\ncontent = dic[key]\nif functions.has_key(key):\ncontent = apply(functions[key], [content])\nelse:\nif abs(length) >=3: content = ''#'N/A'\nelse: content = ''#'-'\nf.write(format_str % content)\nf.write(separator)\n\ndef print_dictionary_title(titles, lengths, separators):\nwrite_dictionary_title(sys.stdout, titles, lengths, separators)\n\ndef write_dictionary_title(f, titles, lengths, separators):\nline2 = ''\nfor title,length,separator in map(None, titles, lengths, separators):\nformat_str = '%'+`length`+'s'\nline2 = line2 + (format_str % title) + separator\nline1 = string.join(map(lambda x: ifab(x=='|','+','-'),line2[:-1]),'')+'\\n'\nmap(f.write, [line1, line2, line1])\n\n# category 9\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### DICTIONARIES AND LISTS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n# switching between lists and dictionaries\n\ndef list2dic(list, func, rep_ok=None, check_unambiguous=0):\n#\n# Transform a list of elements into a dictionary, indexed by the\n# value returned by func, when called on the elements of the list\n#\n\ndic,rejected,ambiguous = {},[],{}\nkeys_elmts = map(lambda elmt,f=func: (f(elmt),elmt), list)\nfor key,elmt in keys_elmts:\n# check if the key already exists\nif not dic.has_key(key): # it's a new key, never seen before\nif check_unambiguous: # i have to check rep_ok either way\nif not rep_ok: raise \"Unspecified function rep_ok(key,elmt)\"\nif rep_ok(key, elmt): # does it pass the rep_ok\ndic[key] = elmt # then set the dictionary\nelse:\nrejected.append((key,elmt)) # skip it\nelse:\nif not ambiguous.has_key(key):\ndic[key] = elmt # always set an unambiguous key\nelse: # had previously tried a few keys, none of which passed test\nif rep_ok(key, elmt): # this one passes the rep_ok test\ndic[key] = elmt\ndel(ambiguous[key]) # the key is no longer ambiguous\nelse: # the key already exists\nif not rep_ok: # no discriminating function exists\nraise \"Two entries share a key\",key\nelse:\n\nold_good = rep_ok(key, dic[key])\nnew_good = rep_ok(key, elmt)\n\nif old_good and not new_good:\nrejected.append((key,elmt))\nelif new_good and not old_good:\nrejected.append((key,dic[key]))\ndic[key] = elmt\nelif not old_good and not new_good:\n# not only reject current, but also notice that\n# you had accepted an ambiguous one\nrejected.append((key,elmt))\nrejected.append((key,dic[key]))\ndel(dic[key])\nambiguous[key] = None # mark key as one to check always\nelse:\n# both are good\nraise \"rep_ok passes for both %s and %s for %s. No discrimination possible.\"%(\ndic[key], elmt, key), rep_ok\n\n# print how many ambiguous\nif rejected:\nprint '%s keys were rejected'%len(rejected)\nrejected.sort()\nprint string.join(map(lambda r: '%s'%r[0],rejected),', ')\nif ambiguous:\nprint '%s ambiguous'%len(ambiguous)\nprint string.join(map(lambda a: '%s'%a,my_sort(ambiguous.keys())),', ')\nreturn dic\n\ndef diclist2dicdic(diclist, key, rep_ok=None, check_unambiguous=0):\n# Transform a list of dictionaries into a dictionary of dictionaries, as indexed\n# by key\nreturn list2dic(diclist, lambda dic,key=key: dic[key], rep_ok, check_unambiguous)\n\ndef list2dic_i(list):\n# transforms a list x into a dictionary d\n# where d[i] = x[i] for all i in range(0,len(list))\n#\n# applications: when you want to erase list elements\n# without ever reusing them, or when you don't want\n# all your elements to be reindexed when you're deleting one\ndic = {}\nfor i in range(0,len(list)):\ndic[i] = list[i]\nreturn dic\n\ndef dic2list_i(dic):\n# transforms a dictionary d into a list x where x[i] = d[ith key]\n# where the keys are sorted alphabetically\nkeys = dic.keys()\nkeys.sort()\nlist = []\nfor key in keys:\nlist.append(dic[key])\nreturn list\n\ndef mergedics(diclist):\n# combinding say ORFs and INTERs in a single dictionary\n# since they have similar representations, but different\n# key namespaces\nsuperdic = {}\nfor dic in diclist:\nfor key,value in dic.items():\nif superdic.has_key(key):\nraise \"Two dictionaries share a key\", key\nsuperdic[key] = value\nreturn superdic\n\ndef filter_diclist(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]==value: res.append(dic)\nreturn res\n\ndef filter_diclist_func(diclist, key, func):\nres = []\nfor dic in diclist:\nif func(dic[key]): res.append(dic)\nreturn res\n\ndef filter_diclist_not(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]!=value: res.append(dic)\nreturn res\n\ndef filter_diclist_gt(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]>value: res.append(dic)\nreturn res\n\ndef filter_diclist_gte(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]>=value: res.append(dic)\nreturn res\n\ndef filter_diclist_lt(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]<value: res.append(dic)\nreturn res\n\ndef filter_diclist_lte(diclist, key, value):\nres = []\nfor dic in diclist:\nif dic[key]<=value: res.append(dic)\nreturn res\n\ndef filter_diclist_within(diclist, key, min, max):\nres = []\nfor dic in diclist:\nif min<=dic[key]<max: res.append(dic)\nreturn res\n\n# category 10\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### HIERARCHY OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef flatten_hierarchy(hierarchy):\n\nflat = mreplace(`hierarchy`,'()[]',',,,,')\nflat = filter(None,string.split(flat))\n\nreturn flat\n\ndef flatten_hierarchy(hierarchy):\n# depth first traversal of the hierarchy\nchildren = []\n\ntodo = map(None,hierarchy)\nwhile 1:\nif not todo: return children\ntype0 = type(todo[0])\nif type0 == types.ListType or type0 == types.TupleType:\ntodo.extend(todo[0])\nelse:\nchildren.append(todo[0])\ntodo = todo[1:]\n\nreturn children\n\ndef map_on_hierarchy_safe(hierarchy,func):\nif type(hierarchy) in [types.ListType,types.TupleType]:\nreturn map_on_hierarchy(hierarchy,func)\nelse:\nreturn func(hierarchy)\n\ndef map_on_hierarchy(hierarchy,func):\n# depth first traversal of the hierarchy\n\nhiertype = type(hierarchy)\n\n# assume it's always called on a list or a tuple\nif not hierarchy:\nif hiertype == types.TupleType: return ()\nelse: return []\n\nresults = []\nfor element in hierarchy:\n\nelmt_type = type(element)\nif elmt_type == types.ListType or elmt_type == types.TupleType:\nres = map_on_hierarchy(element,func)\nelse:\nres = func(element)\nresults.append(res)\n\nif hiertype==types.TupleType:\nreturn tuple(results)\nelse:\nreturn results\n\n# def map_on_hierarchy_breadth(hierarchy, func, depth=0):\n# # assume it's always called on a list or a tuple\n# if not hierarchy: return []\n#\n# results = [None]*len(hierarchy)\n# elmt_types = map(type, element)\n#\n# lists = map(lambda t: t==types.ListType, elmt_types)\n# tuples = map(lambda t: t==types.TupleType, elmt_types)\n# recurse = map(operator.or_, lists, tuples)\n#\n# do_first = find_all(recurse,0)\n#\n# for i in do_first:\n# results[i] = func(hierarchy[i])\n#\n# if elmt_type == types.ListType:\n# res = map_on_hierarchy(element,func)\n# elif elmt_type == types.TupleType:\n# res = tuple(map_on_hierarchy(element,func))\n# else:\n# res = func(element)\n# results.append(res)\n#\n# return results\n\n# section\n############################################################\n# evaluating a function on a hierarchy from the inside out\n\ndef map_on_hierarchy_breadth(hierarchy, func):\nif not hierarchy: return []\n\nmaxdepth = get_hierarchy_depth(hierarchy)\n\nfor depth in range(0,maxdepth+1):\nresults = hierarchy_evaluate_at_level(hierarchy,func,depth)\nprint \"Results at depth=%s are: %s\"%(depth,results)\n\ndef get_hierarchy_depth(hierarchy):\ndepths = []\nfor elmt in hierarchy:\nif type(elmt) in [types.ListType, types.TupleType]:\ndepths.append(get_hierarchy_depth(elmt)+1)\nelse:\ndepths.append(0)\nprint \"Depth of %s is %s\"%(hierarchy, max(depths))\nreturn max(depths)\n\ndef hierarchy_evaluate_at_level(hierarchy, func, depth, cur_depth=0):\n\nassert(cur_depth<=depth)\nresults = []\nfor elmt in hierarchy:\nif type(elmt) in [types.ListType, types.TupleType]:\nif cur_depth<depth:\nresults.append(hierarchy_evaluate_at_level(elmt, func, depth, cur_depth+1))\nelse:\nresults.append('WAIT')\nelse:\nif cur_depth==depth:\nresults.append(func(elmt))\nelse:\nresults.append('SKIP')\nreturn results\n\n# section\n############################################################\n# evaluating a function on a hierarchy from the inside out\n\ndef hierarchy_evaluate_joins(hierarchy, func, followTuples=1, followLists=1):\n\n# pp(\"hierarchy_evaluate_joins %s on %s\"%(func,hierarchy),1,90)\n\nhiertype = type(hierarchy)\n\nif not ((hiertype==types.TupleType and followTuples) or\n(hiertype==types.ListType and followLists)):\n# pp(\"I'm evaluating on %s\"%hierarchy,1,90)\nreturn func(hierarchy)\nelse:\nflat = []\nfor elmt in hierarchy:\nelmt_type = type(elmt)\nif ((elmt_type==types.TupleType and followTuples) or\n(elmt_type==types.ListType and followLists)):\n\n# pp((\"I'm recursing on %s\"%`elmt`),1,90)\n\nflat.append(hierarchy_evaluate_joins(elmt,func,followTuples,followLists))\nelse:\nflat.append(elmt)\n\nif hiertype == types.TupleType: flat = tuple(flat)\n\n# pp(\"I'm returning func(%s)\"%`flat`)\nreturn func(flat)\n\n# category 11\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### INTERVAL OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef alignments_overlap(bn,tx):\nreturn (tx['chromosome'] == bn['chromosome'] and\ntx['start'] <= bn['end'] and\nbn['start'] <= tx['end'])\n\ndef orthologs_in_interval(orthologs, interval):\nreturn filter(lambda o,i=interval: alignments_overlap(o,i),orthologs)\n\ndef intervals_intersect(a,b,start='start',end='end',offset=0):\nreturn not (a[start]>b[end]+offset or b[start]>a[end]+offset)\n\ndef items2intervals(items):\nintervals = []\nfor item in items:\nintervals.append({'start': item[0],\n'end': item[1],\n'label': item[2]})\nreturn intervals\n\ndef join_intervals(intervals,start='start',end='end'):\n# intervals is a list of\n# [{'start': 1, 'end': 2},\n# {'start': 3, 'end': 5},\n# {'start': 4, 'end': 7}]\n\nif not intervals: return intervals\n\nintervals.sort(lambda a,b,start=start,end=end: a[start]-b[start])\n\nnon_overlapping = []\ncurrent = {start: intervals[0][start],\nend: intervals[0][end]}\n\nfor next in intervals[1:]:\n\n# if you're overlapping the last one, increase current_end\nif intervals_intersect(current, next, start=start, end=end):\ncurrent[end] = max(current[end], next[end])\nelse:\nnon_overlapping.append(current)\ncurrent = {start: next[start],\nend: next[end]}\n\nnon_overlapping.append(current)\n\nrep_type = type(intervals[0])\nif rep_type == types.TupleType:\nassert(start==0 and end==1)\nnon_overlapping = map(lambda interval,start=start,end=end: (interval[start],interval[end]), non_overlapping)\nelif rep_type == types.ListType:\nassert(start==0 and end==1)\nnon_overlapping = map(lambda interval,start=start,end=end: [interval[start],interval[end]], non_overlapping)\nelif rep_type == types.DictType:\npass\n\nreturn non_overlapping\n\ndef group_overlapping(intervals, start='start', end='end', offset=0):\nif not intervals: return []\n\nintervals.sort(lambda a,b,start=start: a[start]-b[start])\n\nnon_overlapping = []\ncurrent = {start: intervals[0][start],\nend: intervals[0][end],\n'intervals': [intervals[0]]}\n\nfor next in intervals[1:]:\n\n# if you're overlapping the last one, increase current_end\nif intervals_intersect(current, next, start, end, offset):\ncurrent[end] = max(current[end], next[end])\ncurrent['intervals'].append(next)\nelse:\nnon_overlapping.append(current)\ncurrent = {start: next[start],\nend: next[end],\n'intervals': [next]}\n\nnon_overlapping.append(current)\n\nreturn cget(non_overlapping,'intervals')\n\n#def interval_union(as,bs):\n# \"a,b are lists of intervals containing {'start','end'}\"\n# news = []\n# all = as[:]\n# all.extend(bs)\n# all.sort(lambda x,y: cmp(x['start'],y['start']))\n# i = 0\n# junctions_made = 1\n# while junctions_made:\n# junctions_made = 0\n# while i in range(0,len(all)-1):\n# print_intervals([all])\n# s = all[i]\n# t = all[i+1]\n# if s['end'] <= t['start']:\n# new = {'start': s['start'], 'end': max(t['end'],s['end'])}\n# del(all[i])\n# del(all[i+1])\n# all.insert(i,new)\n# junctions_made = 1\n# else:\n# i = i+1\n# return all\n#\n#def test_interval_union():\n# a = [{'start': 2, 'end': 4},\n# {'start': 5, 'end': 6}]\n# b = [{'start': 1.5, 'end': 2.5},\n# {'start': 3, 'end': 3.5},\n# {'start': 4.5, 'end': 7}]\n# print_intervals([a,b])\n# print interval_union(a,b)\n\ndef interval_union(interval_list):\nintervals = map(lambda x: (x['start'],+1), interval_list)\nmap(intervals.append, map(lambda x: (x['end'],-1), interval_list))\nintervals.sort(lambda x,y: cmp(x[0],y[0]))\ntot,openings,closings = 0,[],[]\nfor i in range(len(intervals)):\nif tot == 0:\nopenings.append(intervals[i][0])\ntot = intervals[i][1] + tot\nif tot == 0:\nclosings.append(intervals[i][0])\nreturn map(lambda start,end:{'start':start,'end':end}, openings, closings)\n\ndef test_interval_cut():\ninterval_dic = {'a': [{'start': 20, 'end': 40},\n{'start': 50, 'end': 60}],\n'b': [{'start': 15, 'end': 25},\n{'start': 30, 'end': 35},\n{'start': 45, 'end': 80}]}\n#pp(interval_dic.values())\nprint_intervals(interval_dic.values(),'start','end')\ncut = interval_cut(interval_dic,'start','end','set')\n#pp(cut,2)\nprint_intervals([cut],'start','end')\n#print_intervals([filter(lambda c: len(c['set'])==2, cut)],'start','end')\nprint string.join(map(lambda i: string.join(i['set'],'+'), cut),'\\t')\nboom\n\ndef interval_subtract(intervals1, intervals2, start='start', end='end'):\nall_sets = interval_cut({'keep': intervals1,'skip': intervals2}, start=start, end=end)\n\nresult = filter(lambda set: 'keep' in set['set'] and 'skip' not in set['set'], all_sets)\nfor res in result:\nassert(res['set']==['keep'])\ncdel(result,'set')\nreturn filter(lambda r,start=start,end=end: r[end]-r[start]+1 != 0, result)\n\ndef interval_cut(interval_dic,start='start',end='end',set='set'):\n\n# For the structure of interval_dic, you can see test_interval_cut\n# the keys of the dictionary are meaningful, they are used in the\n# labels for the interval_list\n\n# output is a flat list of intervals, each of which has a start,\n# end and a set, where set is an unordered list of the different\n# labels, coming from the keys of the input dictionary.\n\n# Part 1. Constructing the interval list transform the directory\n# structure into a flat list of start,end coordinates, each with a\n# label of which dictionary entry it came from\n\ninterval_list = []\nfor key,intervals in interval_dic.items():\nfor interval in intervals:\ninterval_list.append({start: interval[start],\nend: interval[end],\n'label': key})\n\n#pp(interval_list)\n\n# 2b. determine if the coordinates are all integers, in which case\n# the end of one interval is one less than the beginning of the\n# next.\n\ncoords = flatten(map(lambda i,start=start,end=end: [i[start],i[end]], interval_list))\nif map(int, coords) == coords: adjustment = 1\nelse: adjustment = 0\n\n# Part 2. Transforming the start,end coordinates, into a list of\n# operations to perform at each coordinate position. You either\n# add of subtract a label from the current set of active intervals\n\nintervals = map(lambda x,start=start: (x[start],'add',x['label']), interval_list)\nintervals.extend(map(lambda x,a=adjustment,end=end: (x[end]+a,'sub',x['label']), interval_list))\nintervals.sort(lambda x,y: cmp(x[0],y[0]))\n\n#pp(intervals)\n\n# Part 3. Actually perform those add or subtract operations,\n# keeping track of the current open intervals. Construct a list\n# of every region, and all the open intervals there.\n\ncurrent_set, all_sets = [], []\nfor interval in intervals:\n\nset_operation = case(interval[1],\n{'add': set_union,\n'sub': set_subtract})\n\ncurrent_set = set_operation(current_set, [interval[2]])\n\nall_sets.append({start: interval[0],\nset: current_set})\n\n#pp(all_sets,1)\nif not current_set==[]:\nprint \"VERY VERY BAD!! current_set = %s\"%current_set\n#assert(current_set == [])\n\n# Part 4. Fix each interval in your list, by also appending\n# an end position\n\n# 4b. go for it\nfor i in range(0,len(all_sets)-1):\nall_sets[i][end] = all_sets[i+1][start]-adjustment\n#all_sets[-1]['end'] = intervals[-1][0]\n\nif not all_sets: return []\n\n# the last set was only needed to find the end of the 2nd-to-last\ndel(all_sets[-1])#['end'] = intervals[-1][0]\n\n#pp(all_sets,1)\n\nreturn all_sets\n\ndef test_interval_union():\na = [{'start': 2, 'end': 4},\n{'start': 5, 'end': 6}]\nb = [{'start': 1.5, 'end': 2.5},\n{'start': 3, 'end': 3.5},\n{'start': 4.5, 'end': 7}]\nall = a[:]\nall.extend(b)\nprint_intervals([a,b,interval_union(all)])\n\ndef print_intervals(intervals,start='start',end='end'):\nprint_scale({'min': min(map(lambda interval,start=start: min(map(lambda x,start=start: x[start], interval)),intervals)),\n'max': max(map(lambda interval,end=end: max(map(lambda x,end=end: x[end], interval)),intervals)),\n'tick': 10,\n'width': 120,\n'print_scale': 1},\nmap(lambda interval,start=start,end=end: map(lambda x,start=start,end=end:\n(x[start],x[end],`(x[start],x[end])`),\ninterval),\nintervals))\n\ndef common_intervals(intervals_list,start='start',end='end'):\nif not intervals_list: return []\n# returns the extended intervals that are common to all\ncommon_intervals = intervals_list[0]\nfor intervals in intervals_list[1:]:\nintersection = []\nkeep_i, keep_j = [], []\nfor i in range(0,len(common_intervals)):\nfor j in range(0,len(intervals)):\nif intervals_intersect(common_intervals[i],intervals[j],start=start,end=end):\nkeep_i.append(i)\nkeep_j.append(j)\nkeep_i = unique(keep_i)\nkeep_j = unique(keep_j)\n\ncommon_intervals = mget(common_intervals,keep_i)+mget(intervals,keep_j)\n\nreturn join_intervals(common_intervals,start=start,end=end)\n\n# category 12\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### ISLAND OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef count_islands(list):\n\n# takes a list, or a string, and gather islands of identical elements.\n# it returns a dictionary counting where\n# counting = {element: [(start,end), (start,end), ...],\n# element: [(start,end), (start,end), ...],\n# ...}\n# counting.keys() is the list of unique elements of the input list\n# counting[element] is the list of all islands of occurence of element\n# counting[element][i] = (start,end)\n# is such that list[start-1:end] only contains element\nif not list: return {}\n\ncounting = {}\n\ni,current_char, current_start = 0,list[0], 0\n\nwhile i < len(list):\n\nif current_char == list[i]:\ni = i+1\nelse:\nif not counting.has_key(current_char): counting[current_char] = []\ncounting[current_char].append((current_start+1, i))\ncurrent_char = list[i]\ncurrent_start = i\n\nif not counting.has_key(current_char): counting[current_char] = []\ncounting[current_char].append((current_start+1, i))\n\nreturn counting\n\n# category 13\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### SET OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef set_compare(set1, set2):\nset1 = unique(set1)\nset2 = unique(set2)\nunion = set_union(set1, set2)\ninter = set_intersect(set1, set2)\nset1_only = set_subtract(set1, set2)\nset2_only = set_subtract(set2, set1)\nreturn {'Set1': (len(set1), set1),\n'Set2': (len(set2), set2),\n'Inter': (len(inter), inter),\n'Union': (len(union), union),\n'set1_only': (len(set1_only), set1_only),\n'set2_only': (len(set2_only), set2_only)}\n\ndef set_union(set1, set2):\nnew_set = set1[:]\n# add all the elements of set2 that weren't in set 1\nnew_set.extend(set_subtract(set2, set1))\nreturn new_set\n\ndef set_union_all(sets):\nreturn unique(flatten(sets))\n\ndef set_intersect_all(sets):\nif not sets: return []\nintersection = sets[0]\nfor set in sets[1:]:\nintersection = set_intersect(set, intersection)\nreturn intersection\n\ndef venn_diagram(sets):\nsets = map(unique,sets)\nall = range(0,len(sets))\n\nresults = []\nfor subset in superset(all):\ncomplement = set_subtract(all,subset)\n\nwithin = set_intersect_all(mget(sets,subset))\noutside = set_union_all(mget(sets,complement))\n\nexactly_in = set_subtract(within, outside)\n\n#print \"In %s: %s\"%(subset, len(exactly_in))\n#print display_list(exactly_in)\nif subset: #exactly_in:\nresults.append((subset,exactly_in))\nreturn results\n\ndef fast_set_intersect(set1,set2):\ndic1 = {}\nfor elmt in set1: dic1[elmt] = None\nreturn filter(dic1.has_key, set2)\n\ndef smart_set_intersect(set1,set2):\ndic1 = {}\nfor elmt in set1:\nif dic1.has_key(elmt): dic1[elmt] = dic1[elmt]+1\nelse: dic1[elmt] = 1\ncommon = []\nfor elmt in set2:\nif not dic1.has_key(elmt): continue\ncommon.append(elmt)\nif dic1[elmt] == 1: del(dic1[elmt])\nelse: dic1[elmt] = dic1[elmt] - 1\nreturn common\n\ndef set_intersect(set1, set2, el2hash=lambda x: x):\nif not set1: return []\nif not set2: return []\nnew_set = []\nset2_dic = {}\n\n#print \"Hashes are: \"\n#pp(map(el2hash,set1))\n#pp(map(el2hash,set2))\n\nfor elmt in set2: set2_dic[el2hash(elmt)] = ''\nfor elmt in set1:\n#if elmt in set2:\nif set2_dic.has_key(el2hash(elmt)):\nnew_set.append(elmt)\nreturn new_set\n\ndef set_equal(set1, set2):\nif len(set1) != len(set2): return 0\nreturn my_sort(set1) == my_sort(set2)\n\ndef set_subtract(set, subset, el2hash=lambda x: x):\nnew_set = []\nsubset_dic = {}\nfor elmt in subset: subset_dic[el2hash(elmt)] = ''\nfor elmt in set:\nif not subset_dic.has_key(el2hash(elmt)):\nnew_set.append(elmt)\nreturn new_set\n\ndef set_included(set, superset):\nreturn len(set_subtract(set, superset))==0\n\ndef permutations(set):\n# [1,2,3]\n# -> [[1,2,3],[1,3,2],\n# [2,1,3],[2,3,1],\n# [3,1,2],[3,2,1]]\nif len(set)==0:\nreturn [[]]\nelse:\nthis = set[0]\nothers = set[1:]\n\nrests = permutations(others)\n\nperms = []\nfor i in range(0,len(others)+1):\nfor rest in rests:\nperm = []\nperm.extend(rest[:i])\nperm.append(this)\nperm.extend(rest[i:])\nperms.append(perm)\n\nreturn perms\n\ndef superset_sorted(set):\n# this is just for the paranoid ones that want their elements in the order\n\nindices_super = superset(range(0,len(set)))\n\nindices_super = my_sort(indices_super, lambda c: (len(c),c))\n\nsuper = []\nfor indices in indices_super:\nsuper.append(mget(set,indices))\nreturn super\n\ndef superset(set):\n# calculate the superset of the input set.\n# ex: [1,2,3] -> [[],[1],[2],[3],[12],[13],[23],[123]]\n\nif len(set) == 0:\nreturn [set]\nelse:\nsubsuper = superset(set[:-1])\nlastel = set[-1:]\n\nsuper = []\ni,j = 0,0\n\nnext1 = subsuper[i]\nnext2 = subsuper[j]+lastel\n\nmaxlen = max(map(len,subsuper))\nwhile 1:\nif len(next1) <= len(next2) and next1 <= next2:\nsuper.append(next1)\ni = i+1\nif i == len(subsuper): break\nnext1 = subsuper[i]\nelse:\nsuper.append(next2)\nj = j+1\nif j == len(subsuper): break\nnext2 = subsuper[j]+lastel\nassert(i == len(subsuper))\nfor j in range(j,len(subsuper)):\nsuper.append(subsuper[j]+lastel)\n\n#print 'Super has %s elements'%len(super)\n\nreturn super\n\ndef all_combinations(sets_list):\nif len(sets_list) == 0:\nreturn [[]]\nelse:\nresults = []\n\nsubres = all_combinations(sets_list[1:])\n\nfor element in sets_list[0]:\nfor sub in subres:\nresults.append([element]+sub)\n\nreturn results\n\ndef equivalence_classes(identity_pairs):\n# having a list of pairs such as (a,b), (a,c) which means a==b and a==c,\n# it returns a list of equivalence classes such as [(a,b,c)]\n# equivalence classes are the transitive closure of the join operation\n# of two sets, where you join two sets if they have an element in common\n# ex: ['ab','ac','ad','ef'] -> [['a', 'b', 'c', 'd'], ['e', 'f']]\n\nsets_of = {} # for each element x, the sets_of[x] are the sets that x belongs to\nsets = {} # all the unique sets ever created\ni = 0\nfor pair in identity_pairs:\na,b = pair\nif not sets_of.has_key(a): sets_of[a] = []\nif not sets_of.has_key(b): sets_of[b] = []\n# create a new set with only elements a and b in it\nif a != b:\nsets[i] = [a,b]\nsets_of[a].append(i)\nsets_of[b].append(i)\nelse:\nsets[i] = [a]\nsets_of[a].append(i)\ni = i + 1 # increment i, coz a set id never reappears\n\n#pp({'sets_of': sets_of, 'sets': sets},2)\n\nsome_junction_was_made = 1\nwhile some_junction_was_made:\nsome_junction_was_made = 0\nfor a in sets_of.keys():\n# if a belongs to more than one set, join them\nif len(sets_of[a]) > 1:\nsome_junction_was_made = 1\n# print \"\\nElement %s belongs to more than one set %s\" % (a, sets_of[a])\n\n# create a new set\n# print \" Creating a new set %s for the junction of %s\" % (i, sets_of[a])\nsets[i] = []\n\n# remove 'a' from every set it belongs to\n# print \" Removing %s from every set it belongs to\" % a\nfor old_set in sets_of[a]:\nsets[old_set].remove(a)\n\n# print \" Adding %s to the new set %s\" % (a, i)\nsets[i].append(a)\n\n# for every set that 'a' belongs to\nfor old_set in sets_of[a]:\n# first removing a from every set it belonged to\n# print \" Considering old set %s\" % (old_set)\n# destroy the old set\nfor element in sets[old_set]:\n# element doesn't belong it old set anymore\n# print \" Removing %s from old set %s\" % (element, old_set)\nsets_of[element].remove(old_set)\n# actually add the element to the new set\nif not element in sets[i]:\n# print \" Adding %s to new set %s\" % (element, i)\nsets_of[element].append(i)\nsets[i].append(element)\nelse:\n# print \" Element %s already belonged to set %s\" % (element, i)\npass\n# print \" Deleting old set %s\" % old_set\ndel(sets[old_set])\nsets_of[a] = [i]\nsets[i].sort()\ni = i+1\n# print \"Done with element %s\" % a\n#if some_junction_was_made: print \"some junction was made, please continue\"\nelse:\n#print \"\\nNo more junctions are possible. I'm done\"\npass\n#pp({'sets_of': sets_of, 'sets': sets},2)\nequivalence_classes = sets.values()\nequivalence_classes.sort()\nreturn equivalence_classes\n\ndef group_into_equivalence_classes(list, are_same = lambda a,b: 0, debug=0):\n# groups a list into equivalence classes, where two items\n# are equivalent\n# if are_same(a,b) or are_same(a,c)\n# if c is equivalent to a or b\n\nif debug: print \"Testing %s possible pairings\"%(len(list)*len(list))\n\nused = {}\npairs = []\nfor i in range(0,len(list)):\nfor j in range(i+1,len(list)):\nif are_same(list[i], list[j]):\npairs.append((i,j))\nif not used.has_key(i): used[i] = None\nif not used.has_key(j): used[j] = None\n\nif debug: print \"Reducing %s actual pairs to equivalence classes\"%len(pairs)\n\n#print \"Identities found between elements: %s\"%pairs\nclasses = equivalence_classes(pairs)\n\nif debug: print \"Found %s equivalence classes\"%len(classes)\n\nresult = []\nfor each_class in classes:\nresult.append(mget(list,each_class))\n\n#print \"Result is........................: %s\"%result\n\nfor i in range(0, len(list)):\nif not used.has_key(i):\nresult.append([list[i]])\n\nif debug: print \"And %s signletons\"%(len(result)-len(classes))\n\n#print \"After adding singletons..........: %s\"%result\n\nreturn result\n\ndef group_into_equivalence_classes2(list, are_same = lambda a,b: 0, debug=0):\nprint \"I must group a total of %s elements\"%len(list)\ngroups = []\nn = 0 # the number of comparisons\nfor element1 in list:\ngroups_merged = []\n\nfor i in range(0,len(groups)):\nfor element2 in groups[i]:\nn = n+1\nif are_same(element1,element2):\ngroups_merged.append(i)\nbreak\n\nif len(groups_merged) == 0:\n# create a new group\n#groups.insert(0,[element1])# instead of append, prepend\ngroups.append([element1])# instead of append, prepend\nelif len(groups_merged) == 1:\n# append an element to an existing group\ni = groups_merged[0]\n#groups[i].insert(0,element1) #instead of append, prepend\ngroups[i].append(element1) #instead of append, prepend\nelse:\nprint \"Merging groups %s\"%groups_merged\ndad = groups[groups_merged[0]]\nfor i in groups_merged[1:]:\ndad.extend(groups[i])\ngroups[i] = []\ngroups = my_sort(filter(None, groups),lambda g: -len(g))\nif len(groups)%100 == 0:\nprint \"After %s comparisons, %s groups contain %s elements\"%(\nn, len(groups), sum(map(len,groups)))\nprint \"%s comparisons\"%n\nreturn groups\n\ndef group_into_equivalence_classes_hash(list, element2classes = lambda a: [a]):\n# element2classes returns the list of hash keys under which element a will be indexed\n# note: if a is not hashable, then you lose\nkey2group = {}\nelements2groups = {}\ngroups = []\nfor i in range(0,len(list)):\nelement = list[i]\n# all the keys by which element can be indexed\nkeys = element2classes(element)\n# all the groups that already contain such elements\ngroups_merged = mget(key2element, keys, 0)\n\ndef bipartite2tree(connections, edge2point1=lambda a: a[0], edge2point2=lambda a: a[1]):\n\n# first group according to the first point\ngroups = group_single(connections, edge2point1)\n\ngroups2 = group_single(connections)\n\n# category 14\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### SIGNIFICANCE TESTS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef sample_variance(samples):\nmean = avg(samples)\nreturn sum(map(lambda s,mean=mean: (s-mean)**2), samples)/(len(samples)-1)\n\ndef t_test(samples1, samples2):\nn1 = len(samples1)\nn2 = len(samples2)\n\nmean1 = avg(samples1)\nmean2 = avg(samples2)\n\nvar1 = sample_variance(samples1)\nvar2 = sample_variance(samples2)\n\nreturn t_test_only(n1,mean1,var1,n2,mean2,var2)\n\ndef t_test_only(n1,mean1,var1,n2,mean2,var2):\nvar = ((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2)\nprint \"s^2 = %s\"%var\n\nt = (mean1-mean2) / math.sqrt(var/n1 + var/n2)\ndof = n1+n2-2\n\nreturn t,dof\n\n# category 15\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### MATH PRIMITIVES ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\ndef gaussian(x, mu, sigma):\n\"\"\"\nEvaluate N(mu,sigma) at x.\nwhere N(mu,sigma) is a gaussian of mean mu and stddev sigma\n\"\"\"\n\nreturn ( (1.0/math.sqrt(2*math.pi*sigma))\n* (math.e**(-((x-mu)**2)/(2*sigma**2))))\n\ndef make_gaussian(mu, sigma):\n\"\"\" usage:\nN2_3 = make_gaussian(2,3)\nN2_3(4) -> guassianN(2,3) evaluated at 4\n\"\"\"\nreturn lambda x,mu=mu,sigma=sigma: ( (1.0/math.sqrt(2*math.pi*sigma))\n* (math.e**(-((x-mu)**2)/(2*sigma**2))))\n\ndef make_adder(n):\n\"\"\" usage:\nAdd2 = make_adder(2)\nAdd2(3) -> 5\n\"\"\"\nreturn lambda x,n=n: x+n\n\ndef prob2score(prob):\n# 1/100 -> 20\ntry:\nreturn -10*float(math.log10(float(prob)))\nexcept:\nreturn -1\n\ndef prob2stdevs(prob):\npass\n\n#log10_2 = math.log10(2)\nloge_2 = math.log(2)\n\ndef log2(x):\n# converting bases: log_a(b) = log_c(b)/log_c(a)\n# i.e. log_2(x) = log_e(2)/log_e(x) = log_10(2)/log_10(x)\nreturn math.log(x)/loge_2\n\n#def log_k(x,k):\n# # take the k-th base log\n# log\n\ndef p2bits(p):\n# return -log2(p)\nreturn -math.log(p)/loge_2\n\ndef profile2bits(profile):\np = 1.0\nfor char in profile:\np = p*.25*len(IUB_expansion[char])\nreturn -math.log(p)/loge_2\n\ndef binomial_likelihood_ratio(ps,k,n):\n# p[0] is the null hypothesis\n# p[1] is the hypothesis being tested\nassert(len(ps)==2)\nlikelihoods = []\nfor p in ps:\nlikelihoods.append(binomial(p,k,n))\n#i = argmax(likelihoods)\n#p = likelihoods[i] / sum(likelihoods)\n#return p\nif likelihoods[0]: return log(likelihoods[1]) / likelihoods[0]\nelse:\nprint \"Warning: likelihood ratio set to sys.maxint. p(H1)=%s, p(H0)=0\"%(p[1])\nreturn sys.maxint\n\ndef binomial_log_likelihood_ratio(ps,k,n):\nreturn log_binomial(ps[1],k,n) - log_binomial(ps[0],k,n)\n\ndef poisson_expected(rate):\nfor x in range(1,50,1):\np = poisson(rate,x)\nprint \"%s\\t%s\\t%s\"%(x,p,12000000*p)\n\ndef poisson(rate, x):\nreturn math.exp(-rate)*(rate**x)/factorial(x)\n\n############################################################\n############################################################\n############################################################\n\ndef smart_binomial_sigmas(ps,ks,ns):\nassert(len(ps)==len(ks)==len(ns))\nsigns = []\nfor i in range(0,len(ks)):\np,k,n = ps[i],ks[i],ns[i]\nif p==0:\np=ps[i]=0.5/n\n#print \"Fixing i=%s p,k,n=%s,%s,%s to p=%s\"%(\n# i,p,k,n,.000000001)\nelif p==1:\np=ps[i]=(n-0.5)/n\n#print \"Fixing i=%s p,k,n=%s,%s,%s to p=%s\"%(\n# i,p,k,n,0.999999999)\n#p=ps[i]=0.999999999\nassert(k<=n)\n# do i see more k's than i'd expect\nif n==0: signs.append('-')\nelse: signs.append(ifab(p <= float(k)/float(n),'+','-'))\n\n# now compute the appropriate tail area (right or left, depending on sign)\ntails = binomial_tails(ps,ks,ns,signs)\n\n# and transform them into sigmas\nsigmas = ps2sigmas(tails)\n\n# then flip the sigmas depending on the sign\nfor i in range(0,len(sigmas)):\nif signs[i]=='-': sigmas[i] = -sigmas[i]\n\nreturn sigmas\n\ndef binomial_tails(ps,ks,ns,signs=None):\nassert(len(ps)==len(ks)==len(ns))\nif not signs: signs = ['+']*len(ps)\nassert(0<=min(ps)<=max(ps)<=1)\nfor i in range(0,len(ks)): assert(ks[i]<=ns[i])\ninlines = map(lambda p,k,n,sign: '%s %s %s %s'%(n,k,p,sign), ps,ks,ns,signs)\noutlines = map(string.strip,string.split(quick_system('/home/franklin/nickp/bin/calctail',\nstring.join(inlines,'\\n')),'\\n'))[:-1]\nassert(len(inlines)==len(outlines))\n#print \"%s -> %s\"%(len(inlines),len(outlines))\n#pp(map(lambda a,b: '%s -> %s'%(a,b),inlines,outlines),1)\nreturn map(float,outlines)\n\ndef ps2sigmas(probabilities):\ninlines = string.join(map(lambda s: '%s'%s, probabilities),'\\n')\noutlines = string.split(quick_system('/home/franklin/nickp/bin/calcz',inlines),'\\n')[:-1]\nsigmas = map(float,map(string.strip,outlines))\nassert(len(sigmas)==len(probabilities))\nreturn sigmas\n\n############################################################\n############################################################\n############################################################\n\ndef binomial_tail(p,k,n,cache=None,debug=0):\n# the log probability of seeing k or more successes in n trials\n# given the probability of success is p\nif cache == None:\nsum = 0\nfor k_iter in range(k,n+1): sum = sum+safe_exp(log_binomial(p,k_iter,n))\nreturn sum\n\nsum = 0\nlookups,total = 0,0\nfor k_iter in range(k,n+1):\nif debug: total = total+1\nif cache.has_key((k_iter,n)):\nadd = cache[(k_iter,n)]\nlookups = lookups+1\nelse:\nadd = safe_exp(log_binomial(p,k_iter,n))\ncache[(k_iter,n)] = add\nsum = sum+add\nif debug and cache: print \"p=%s k=%s n=%s %s / %s binomials were lookups. Cache now has %s items\"%(\np,k,n,lookups,total,len(cache))\nreturn sum\n\ndef binomial_tail(p,k,n):\n# the log probability of seeing k or more successes in n trials\n# given the probability of success is p\nif k < n/2:\nsum = 0\nfor k_iter in range(k,n+1):\nsum = sum+safe_exp(log_binomial(p,k_iter,n))\n#print sum\nelse:\nsum = 1.0\nfor k_iter in range(0,k):\nsum = sum-safe_exp(log_binomial(p,k_iter,n))\n#print sum\nreturn sum\n\ndef safe_log(n):\ntry: return math.log(n)\nexcept OverflowError:\nif n==0: return -1e400\nelse: return 1e400\n\ndef safe_exp(n):\ntry: return math.exp(n)\nexcept OverflowError:\n#sys.stdout.write(\"x\")\n#sys.stdout.flush()\nif n<0: return 0.0\nelse: return 1e400\n\ndef log_binomial(p,k,n):\n# the log probability of seeing exactly k successes in n trials\n# given the probability of success is p\nreturn log_n_choose_k(n,k)+math.log(p)*k+math.log(1-p)*(n-k)\n\ndef binomial(p,k,n):\n# probability of seeing exactly k successes in n trials, given\n# the probability of success is p\n#return n_choose_k(n,k)*(p**k)*((1-p)**(n-k))\nreturn n_choose_k(n,k)*(p**k)*((1-p)**(n-k))\n\ndef n_choose_k(n,k):\n# (n k) = n! / (k! (n-k)!)\n#\n# n*(n-1)*(n-2)*....*(n-k+1)\n# = --------------------------\n# k*(k-1)*...*1\nassert(k<=n)\nk = min(k, n-k)\nnominator = range(n,n-k,-1)\ndenominator = range(k,0,-1)\n\nresult = 1.0\nfor nom, den in map(None, nominator, denominator):\nresult = (result * nom) / den\n#result = result*nom\n#print result\n#result = result/den\n#print result\n\nreturn result\n\ndef log_n_choose_k(n,k):\n# (n k) = n! / (k! (n-k)!)\n#\n# n*(n-1)*(n-2)*....*(n-k+1)\n# = --------------------------\n# k*(k-1)*...*1\nassert(k<=n)\nk = min(k, n-k)\nnominator = range(n,n-k,-1)\ndenominator = range(k,0,-1)\n\nresult = 0\nfor nom, den in map(None, nominator, denominator):\nresult = (result + math.log(nom)) - math.log(den)\nreturn result\n\ndef factorial(n):\nresult = 1\nfor i in range(n,0,-1):\n#print i\nresult = result * i\nreturn result\n\ndef factorial_partial(n,k):\n# carries out the multiplication up to and including k\n# n*(n-1)*(n-2)*...*k\nassert(k<=n+1)\n\nresult = 1\nfor i in range(n,k-1,-1):\n#print i\nresult = result*i\nreturn result\n\ndef test_chi_square():\nchi_square([[45,448],[57,157]])\n\ndef make_expected(rows):\nrowtotals = map(sum, rows)\ncoltotals = map(sum, unpack(rows))\ngrandtotal = float(sum(rowtotals))\n\nexpected = []\nfor row,rowtotal in map(None, rows,rowtotals):\nexpected_row = []\nfor obs, coltotal in map(None, row, coltotals):\nexp = rowtotal * coltotal / grandtotal\nexpected_row.append(exp)\nexpected.append(expected_row)\nreturn expected\n\ndef chi_square(rows, expected=None):\n# ex: rows = [[1,2,3],[1,4,5]]\nassert(all_same(map(len,rows)))\n\n#print \"row totals: %s\"%rowtotals\n#print \"col totals: %s\"%coltotals\n\nif 0 in map(sum,rows): return 0,1.0\ncols = map(lambda i,rows=rows: cget(rows,i), range(0,len(rows[0])))\nif 0 in map(sum,cols): return 0,1.0\n\nif not expected:\nexpected = make_expected(rows)\n\nchisq = 0\nfor obss,exps in map(None,rows,expected):\nfor obs, exp in map(None, obss, exps):\nchisq = chisq + ((obs-exp)**2)/exp\n\ndf = (len(rows)-1)*(len(rows[0])-1)\n\np = chi_square_lookup(chisq,df)\n\n#print \"Chi square(df=%s,P<=%s) ~ %s\"%(df,p,chisq)\nreturn chisq,p\n\nchi_square_table = {\n1: [1.64, 2.71, 3.84, 5.02, 6.64, 10.83],\n2: [3.22, 4.61, 5.99, 7.38, 9.21, 13.82],\n3: [4.64, 6.25, 7.82, 9.35, 11.34, 16.27],\n4: [5.99, 7.78, 9.49, 11.14, 13.28, 18.47],\n5: [7.29, 9.24, 11.07, 12.83, 15.09, 20.52],\n6: [8.56, 10.64, 12.59, 14.45, 16.81, 22.46],\n7: [9.80, 12.02, 14.07, 16.01, 18.48, 24.32],\n8: [11.03, 13.36, 15.51, 17.53, 20.09, 26.12],\n9: [12.24, 14.68, 16.92, 19.02, 21.67, 27.88],\n10: [13.44, 15.99, 18.31, 20.48, 23.21, 29.59],\n11: [14.63, 17.28, 19.68, 21.92, 24.72, 31.26],\n12: [15.81, 18.55, 21.03, 23.34, 26.22, 32.91],\n13: [16.98, 19.81, 22.36, 24.74, 27.69, 34.53],\n14: [18.15, 21.06, 23.68, 26.12, 29.14, 36.12],\n15: [19.31, 22.31, 25.00, 27.49, 30.58, 37.70],\n16: [20.47, 23.54, 26.30, 28.85, 32.00, 39.25],\n17: [21.61, 24.77, 27.59, 30.19, 33.41, 40.79],\n18: [22.76, 25.99, 28.87, 31.53, 34.81, 42.31],\n19: [23.90, 27.20, 30.14, 32.85, 36.19, 43.82],\n20: [25.04, 28.41, 31.41, 34.17, 37.57, 45.31],\n21: [26.17, 29.62, 32.67, 35.48, 38.93, 46.80],\n22: [27.30, 30.81, 33.92, 36.78, 40.29, 48.27],\n23: [28.43, 32.01, 35.17, 38.08, 41.64, 49.73],\n24: [29.55, 33.20, 36.42, 39.36, 42.98, 51.18],\n25: [30.68, 34.38, 37.65, 40.65, 44.31, 52.62],\n26: [31.79, 35.56, 38.89, 41.92, 45.64, 54.05],\n27: [32.91, 36.74, 40.11, 43.19, 46.96, 55.48],\n28: [34.03, 37.92, 41.34, 44.46, 48.28, 56.89],\n29: [35.14, 39.09, 42.56, 45.72, 49.59, 58.30],\n30: [36.25, 40.26, 43.77, 46.98, 50.89, 59.70]}\n\ndef chi_square_lookup(value, df):\n\nps = [0.20, 0.10, 0.05, 0.025, 0.01, 0.001]\nrow = chi_square_table[df]\n\nfor i in range(0,len(row)):\nif row[i] >= value:\ni = i-1\nbreak\n\n#print \"Table[%s] -> %s\"%(row[i],ps[i])\n#print \"Table[%s] -> ?\"%value\n#if i<len(row)-1: print \"Table[%s] -> %s\"%(row[i+1],ps[i+1])\n\n#print \"Chi sq with df=%s and value %s has P<%s\"%(df,value,p)\n\nif i == -1: return 1\nelse: return ps[i]\n\n# category 16\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### SEQUENCE OPERATIONS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Sequence primitives\n#\n############################################################\n\ndef reverse(something):\ntype_something = type(something)\nif type_something==types.StringType:\nreturn reverse_str(something)\nelif type_something==types.ListType:\nreturn reverse_list(something)\nelse:\nraise \"Unexpected type for reversal\", type_something\n\ndef reverse_str(str):\nlst = map(None, str)\nlst.reverse()\nreturn string.join(lst, '')\n\ncomp_trans = string.maketrans('ACGTKMYRSWBVDHNacgtkmyrswbvdhn',\n'TGCAMKRYSWVBHDNtgcamkryswvbhdn')\n\n# reverse complement mapping\n# A -> T\n# C -> G\n# G -> C\n# T -> A\n#\n# K -> M # keto (GT) -> amine (CA)\n# M -> K # amine (AC) -> keto (TG)\n#\n# Y -> R # pyrimidine (CT) -> purine (GA)\n# R -> Y # purine (AG) -> pyrimidine (TC)\n#\n# S -> S # strong (GC) -> strong (CG)\n# W -> W # weak (AT) -> weak (TA)\n#\n# B -> V # not A -> not T\n# V -> B # not T -> not A\n# D -> H # not C -> not G\n# H -> D # not G -> not C\n#\n# N -> N # any -> any\n#\n# # and the procedure which helped us find the above :o)\n# def make_full_translation():\n# for char,expansion in sort(IUB_expansion.items()):\n#\n# print '(%s->%s)'%(char,expansion)\n#\n# if char == '-': continue\n# target = IUB_code[string.join(my_sort(complement(expansion)),'')]\n# print '%s -> %s'%(char, target)\n\ndef complement(str):\nreturn string.translate(str, comp_trans)\n\ndef revcom(str):\nlst = map(None, str)\nlst.reverse()\nreturn string.translate(string.join(lst, ''), comp_trans)\n\ndef sequence_type(seq):\nchars = unique(map(None, seq))\nnon_nucleotide = set_subtract(chars, IUB_code.values())\nnon_amino_acid = set_subtract(chars, protein.aminoacids)\n\ndef add_gaps_to_gapless(ali,seq,gap='-'):\n# gaps are given by ali, added to seq\nassert(gapless_len(ali)==len(seq))\nseq = map(None,seq)\nfor i in reverse(find_all(ali,gap)):\n#print i\nseq.insert(i,gap)\nreturn string.join(seq,'')\n\ndef gapless(seq,gaps=['-']):\nif gapless_len(seq,gaps)==0: return ''\nfor gap in gaps: seq = string.replace(seq,gap,'')\nreturn seq\n\ndef gapless_portions(seqs,gaps=['-'],maxwidth=0):\n# returns the gapless portions of an alignment\ngapped_positions = []\nfor seq in seqs:\nfor gap in gaps:\ngapped_positions.extend(find_all(seq,gap))\n\ngapped_positions = my_sort(unique(gapped_positions))\n\nportions = []\nfor start,end in list2pairs([-1]+gapped_positions+[len(seqs[0])]):\nif start>=end-1: continue\nif maxwidth:\nportions.extend(seqs2windows(get_subseqs(seqs,start+1,end),maxwidth))\nelse:\nportions.append(get_subseqs(seqs,start+1,end))\nreturn portions\n\ndef gapless_in_species(seqs,gaps=['-'],which_species=0):\n# returns the subset of columns in the alignment that don't have a gap in seqs[which_species]\ngapped_positions = []\nfor gap in gaps:\ngapped_positions.extend(find_all(seqs[which_species],gap))\n\nportions = []\nfor start,end in list2pairs([-1]+gapped_positions+[len(seqs[0])]):\nif start>=end-1: continue\nportions.append(get_subseqs(seqs,start+1,end))\n\nnewseqs = []\nfor i in range(0,len(seqs)):\nnewseqs.append(string.join(cget(portions,i),''))\n\nreturn newseqs\n\ndef gapless_len(seq,gaps=['-']):\ntotlen = len(seq)\nfor gap in gaps:\ntotlen = totlen - string.count(seq,gap)\nreturn totlen\n\ndef append_gaps(seqs):\nseqs = seqs[:]\nmaxlen = max(map(len,seqs))\nfor i in range(0,len(seqs)):\nif len(seqs[i])!=maxlen:\nseqs[i] = seqs[i]+'-'*(maxlen-len(seqs[i]))\nreturn seqs\n\ndef join_subseqs(subseqs_list):\nassert(all_same(map(len,subseqs_list)))\nseqs = []\nfor i in range(0,len(subseqs_list[0])):\nseqs.append(string.join(cget(subseqs_list,i),''))\nreturn seqs\n\ndef get_subseqs(seqs, start, end=None):\nsubseqs = []\nfor seq in seqs:\nif end==None: subseqs.append(seq[start:])\nelse: subseqs.append(seq[start:end])\nreturn subseqs\n\ndef get_subseqs_with_context(seqs, start, end=None, context=5):\npre = get_subseqs(seqs, max(start-context,0), start)\nmiddle = get_subseqs(seqs,start,end)\nif end+context > len(seqs): post = get_subseqs(seqs, end, end+context)\nelse: post = get_subseqs(seqs, end, min(end+context,len(seqs)))\n\nsubseqs = []\nfor i in range(0,len(seqs)):\nsubseqs.append('%s|%s|%s'%(string.lower(pre[i]),\nmiddle[i],\nstring.lower(post[i])))\nreturn subseqs\n\ndef get_multisubseqs(seqs, start_ends):\nall_subseqs = []\nfor start,end in start_ends:\nall_subseqs.append(get_subseqs(seqs,start,end))\nsubseqs = []\nfor i in range(0,len(seqs)):\nsubseqs.append(string.join(cget(all_subseqs,i),''))\nreturn subseqs\n\ndef smart_getsubseqs(seqs, start, end):\nsubseqs = []\nfor seq in seqs:\nsubseqs.append(smart_getsubseq(seq,start,end))\nreturn subseqs\n\ndef smart_getsubseq(seq, start, end):\nlength = end-start\nsubseq = seq[start:end]\nif '-' in subseq:\nif subseq[0]!='-':\nsubseq = gapless(seq[start:end+20])[:length]\nprint \"Dealt with gaps %s -> %s\"%(seq[start:end], subseq)\nelif subseq[-1]!='-':\nsubseq = gapless(seq[start-20:end])[-length:]\nprint \"Dealt with gaps %s -> %s\"%(seq[start:end], subseq)\nelse:\nprint \"Gaps everywhere! Don't know what to do! %s\"%seq[start-(2*length):end+(2*length)]\nreturn subseq\n\ndef get_inside(seqs, coords):\n# seqs is a multiple alignment of sequences\n# coords is a list of the form [{'start': 40, 'end': 100}, {'start': 150,'end': 200}, ...]\n# start and end are inclusive\n# returns all the sequences that fall inside the coords\ninside = []\nfor coord in coords:\ninside.append(get_subseqs(seqs, coord['start']-1,coord['end']))\nreturn inside\n\ndef coords_in2out(coords, maxlen):\nnewcoords = list2pairs(coords,\nlambda c1,c2: {'start': c1['end']+1,\n'end': c2['start']-1})\nnewcoords.insert(0,{'start': 1,\n'end': coords[0]['start']-1})\nnewcoords.append({'start': coords[-1]['end']+1,\n'end': maxlen})\n\nreturn newcoords\n\ndef get_outside(seqs, coords):\n# seqs is a multiple alignment of sequences\n# coords is a list of the form [{'start': 40, 'end': 100}, {'start': 150,'end': 200}, ...]\n# start and end are inclusive\n# returns all the sequences that fall inside the coords\noutside = []\nnewcoords = coords_in2out(coords, len(seqs[0]))\n\nreturn get_inside(seqs,newcoords)\n\ndef flatten_seqslist(seqs_list):\nseqs = [None]*len(seqs_list[0])\nfor i in range(0,len(seqs_list[0])):\nseqs[i] = string.join(cget(seqs_list,i),'')\nreturn seqs\n\n############################################################\n#\n# Printing aligned sequences that span multiple lines\n#\n############################################################\n\ndef write_wrap_label2str(label, max_label_len):\nreturn sfill(label,max_label_len)+': '\n\ndef clustal_wrap_label2str(label, max_label_len):\nreturn ('%%-%ss'%max(max_label_len+1,16))%label\n\ndef print_wrap(sequences, indentation, labels=[], ignore_lines=None):\nwrite_wrap(sys.stdout, sequences, indentation, labels, ignore_lines, write_nums=1)\n\ndef write_wrap(f, sequences, indentation, labels=[], ignore_lines=None, write_nums=1,label2str = write_wrap_label2str, offset=0):\n\nif not labels:\nlabels = map(lambda s: '', sequences)\nif len(sequences)!=len(labels):\ndiff = len(sequences) - len(labels)\nprint \"Len(seqs)=%s, Len(labels)=%s. %s extra labels\"%(len(sequences), len(labels), ifab(diff>0,'Adding','Ignoring'))\nif diff>0: labels = labels+['']*diff\nelse: labels = labels[:len(sequences)]\n\nmax_label_len = max(map(lambda label: len(label), labels))\nmax_length = max(map(len, sequences))\niterations = range(0, max_length, indentation)\nif not iterations: iterations = [0]\nfor i in iterations:\nif write_nums:\nupper_bound = i+indentation\nif upper_bound > len(sequences[0]): upper_bound = len(sequences[0])\nf.write('%d~%d:\\n' % (i+offset, upper_bound+offset))\nelse: f.write('\\n')\nfor seq,label in map(None,sequences,labels):\n\nsubseq = seq[i:i+indentation]\nif ignore_lines and sum(map(lambda ignore,subseq=subseq: string.count(subseq,ignore),ignore_lines))==len(subseq):\ncontinue\n\nif max_label_len: f.write(label2str(label,max_label_len))\nf.write(subseq+'\\n')\n\ndef pw(names, seqs, indent=90, offset=0,strict=1):\nif not strict:\nif len(names)!=len(seqs):\nprint \"BEWARE!! %s seqs vs. %s names. IGNORING EXTRA\"%(\nlen(seqs),len(names))\nif len(names) < len(seqs): names = names+['']*(len(seqs)-len(names))\nif len(names) > len(seqs): seqs = seqs+['']*(len(names)-len(seqs))\nseqs = map(lambda s: '%s'%s,seqs)\nif not all_same(map(len,seqs)):\nmaxlen = max(map(len,seqs))\nprint \"BEWARE!! seqs are not all the same length\"\nseqs = map(lambda s,maxlen=maxlen: s+' '*(maxlen-len(s)), seqs)\n\nwrite_wrap(sys.stdout, seqs, indent, names, write_nums=1, label2str=write_wrap_label2str, offset=offset)\n\ndef ww(f, names, seqs, indent=80):\nwrite_wrap(f, seqs, indent, names, write_nums=1, label2str=write_wrap_label2str)\n\ndef write_clustal(f, names, seqs):\nif names[-1]=='' or names[-1]=='consensus':\nnames = names[:-1]+['']\nelse:\nnames = names+['']\nseqs = seqs+[compute_clustal_consensus(seqs)]\n\nwrite_wrap(f, seqs, 60, map(lambda n: safe_replace(n,' ','_'), names), write_nums=0, label2str=clustal_wrap_label2str)\n\n############################################################\n#\n# Calculate consensus from two sequences\n#\n############################################################\n\ndef idperc(q,s,countgaps=0):\nassert(countgaps in [0,1])\nsame,tot = 0,0\nfor char1,char2 in map(None,q,s):\nif is_gap(char1) or is_gap(char2):\ntot=tot+countgaps\ncontinue\nif char1==char2: same = same+1\ntot = tot+1\nif not tot: return 0\nreturn 100.0*same/tot\n\ndef q_and_s(q0,s0):\nif q0==s0 and q0!='-' and q0!='X': return q0\nelse: return ' '\n\ndef sequence_identities(q, s):\nreturn string.join(map(q_and_s,q,s),'')\n\ndef bar_q_and_s(q0,s0):\nif q0==s0 and q0!='-' and q0!='X': return '|'\nelse: return ' '\n\ndef bar_sequence_identities(q, s):\nreturn string.join(map(bar_q_and_s,string.upper(q),string.upper(s)),'')\n\ndef star_q_and_s(q0,s0):\nif q0==s0 and q0!='-' and q0!='X': return '*'\nelse: return ' '\n\ndef star_sequence_identities(q, s):\nreturn string.join(map(star_q_and_s,string.upper(q),string.upper(s)),'')\n\n#def sequence_identities_new_but_slower(q, s):\n# result_list = map(None, q)\n# for i,q0,s0 in map(None, range(0,len(result_list)),q,s):\n# if q0!=s0: result_list[i] = ' '\n# return string.join(result_list,'')\n\ndef q_pos_s(q0,s0):\ncommon = []\ns_all = IUB_expansion[s0]\nfor c in IUB_expansion[q0]:\nif c in s_all:\ncommon.append(c)\nif common: return get_IUB_code(string.join(common,''))\nelse: return ' '\n\ndef sequence_positives(q, s):\n# incorporates IUB codes in comparing profiles\nreturn string.join(map(q_pos_s, string.upper(q), string.upper(s)),'')\n\ndef bar_q_pos_s(q0,s0):\nif q0==s0: return '|'\ns_all = IUB_expansion[s0]\nfor c in IUB_expansion[q0]:\nif c in s_all:\nreturn '+'\nreturn ' '\n\ndef bar_sequence_positives(q, s):\n# incorporates IUB codes in comparing profiles\nreturn string.join(map(bar_q_pos_s, string.upper(q), string.upper(s)),'')\n\n############################################################\n#\n# Clustal stuff\n#\n############################################################\n\nclustal_NN2star = string.maketrans('ACGTacgt','********')\nclustal_AA2star = string.maketrans('*ABCDEFGHIKLMNPQRSTVWXYZ+ ','************************: ')\n\n# def compute_clustal_consensus(list_of_seqs):\n# sum = list_of_seqs[0]\n# for seq in list_of_seqs[1:]:\n# #print sum\n# sum = sequence_identities(sum, seq)\n# return string.translate(sum, clustal_NN2star)\n\ndef test_missing2pieces():\n\nnames = ['seq1','seq2','seq3']\nseqs = ['ATTTAGATACGAGT',\n'ATTAAG...CGACT',\n'AT..AG.TACGACT']\n\n# -> AT TT AG A TA CGAGT\n# -> AT TA AG . .. CGACT\n# -> AT .. AG . TA CGACT\n\npw(names, seqs)\n\npieces = missing2pieces(names,seqs)\nfor subnames,subseqs in pieces:\nprint \"\\nNext piece\"\npw(subnames,subseqs)\n\ndef missing2pieces(allnames, allseqs):\n# see test_missing2pieces above for example\nalldots = map(lambda seq: map(lambda char: char=='.', seq), allseqs)\nnames, seqs, dots = [], [], []\nfor name,seq,dot in map(None,allnames,allseqs,alldots):\nif 0 in dot:\nnames.append(name)\nseqs.append(seq)\ndots.append(dot)\nif not names: return [(allnames, allseqs)]\n\nislands_list = map(count_islands,dots)\n#pp(islands_list,1)\nintervals = map(lambda islands: map(lambda ab: {'start': ab[0], 'end': ab[1]},\nislands[0]),\nislands_list)\nintervald = {}\nmset(intervald, names, intervals)\ncut = interval_cut(intervald)\n#pp(cut,1)\n\npieces = []\nfor region in cut:\nif not region['end'] >= region['start']: continue\n#print \"Piece1: %(start)s-%(end)s\"%region\npieces.append((allnames, get_subseqs(allseqs,region['start']-1,region['end'])))\nreturn pieces\n\ndef char_perfect_or_gap(c):\nif c in 'ACGT': return '*'\nif c in 'acgt': return '*'\nelse: return ' '\n\ndef consensus2idperc(consensus):\nif not consensus: return 0\nreturn 100*string.count(consensus,'*')/len(consensus)\n\ndef consensus2idperc_float(consensus):\nif not consensus: return 0\nreturn 100.0*string.count(consensus,'*')/len(consensus)\n\ndef describe_idperc(consensus):\nreturn perc(count_stars(consensus),len(consensus))\n\ndef count_stars(consensus):\nreturn string.count(consensus,'*')\n\ndef compute_clustal_consensus_ignoring_gaps(seqs):\nreturn compute_clustal_consensus(seqs,'-.')\n\ndef compute_clustal_consensus_ignoring_endgaps(seqs,chars_to_ignore='.'):\nreturn compute_clustal_consensus(clustal_endgaps2dots(seqs),chars_to_ignore='.'+chars_to_ignore)\n\n#def compute_clustal_consensus(seqs):\n# numseqs = len(seqs)\n# length = len(seqs[0])\n#\n# first = seqs[0]\n# others = seqs[1:]\n#\n# consensus = [' ']*length\n# for i in range(0,length):\n# char = first[i]\n# for seq in others:\n# if char == seq[i]: continue\n# else: break\n# else:\n# consensus[i] = '*'\n# return string.join(consensus,'')\n\ndef compute_clustal_consensus(seqs,chars_to_ignore='.',result=None):\n# see also def generate_profile()\n\n# put the chars to ignore in a dictionary, to decrease lookup time\nignoreit = {}\nmset(ignoreit,chars_to_ignore,[None]*len(chars_to_ignore))\n# allocate the necessary memory for the result in one shot\nif not result: result = ['']*len(seqs[0])\nassert(all_same(map(len,seqs+[result])))\n\nfor i in range(0,len(seqs[0])):\n# a dictionary for how many chars we've seen\nseen = {}\nfor seq in seqs:\nif ignoreit.has_key(seq[i]): continue\nseen[seq[i]] = None\n# if they're all the same, don't even wonder, match\nif len(seen)==0: result[i] = ' '\nelif len(seen) == 1:\nif seen == '-': result[i]==' '\nelse: result[i] = '*'\n# if there's more than two, forget it, no match\nelif len(seen) > 2: result[i] = ' '\n# if they're exactly two though, upcase'em\nelse:\none,two = seen.keys()\nif len(unique(string.upper(one+two)))==1: result[i] = '*'\nelse: result[i] = ' '\nreturn string.join(result,'')\n\ndef quick_clustal_consensus(seqs):\n# this is a function that runs faster and computes a case-insensitive consensus\nseqs = map(string.upper,seqs)\nassert(all_same(map(len,seqs)))\nconsensus = [' ']*len(seqs[0])\nfor i in range(0,len(seqs[0])):\nchar = seqs[0][i]\nif char == '-': continue\nfor j in range(1,len(seqs)):\nif seqs[j][i] == '-': break\nif seqs[j][i]!=char:\nbreak\nelse:\nconsensus[i]='*'\nreturn string.join(consensus,'')\n\ndef compute_soft_consensus(seqs):\n\nassert(all_same(map(len,seqs)))\n# LOWERCASE MEANS GAP TO IUB CODES!! FIX THIS\nseqs = map(string.upper, seqs)\n\nresult = []\nfor i in range(0,len(seqs[0])):\n\nIUBs = map(expand_IUB, cget(seqs,i))\ncommon_bits = len(set_intersect_all(IUBs))\nany_bits = len(set_union_all(IUBs))\n\nratio = 100.0 * common_bits / any_bits\nif common_bits==any_bits==4:\nresult.append('_')\nelif ratio==0:\nresult.append(' ')\nelif ratio<50:\nresult.append('.')\nelif ratio<100:\nresult.append(':')\nelif ratio==100:\nresult.append('*')\nelse:\nraise \"OOPS!\"\nreturn string.join(result,'')\n\n#def compute_clustal_consensus_ignoring_chrs(list_of_seqs,chars_to_ignore=['.'],result=''):\n# # computes the consensus of a list of sequences, where '.' or other specified characters\n# # mean that information is just not available\n#\n# # result is a list of characters that will be used in evaluating\n#\n# assert(all_same(map(len, list_of_seqs)))\n# if not result: result = map(None, '*'*len(list_of_seqs[0]))\n# else: result = map(None, result)\n# assert(not ' ' in result)\n# cols_of_chars = unpack(list_of_seqs)\n# for i in range(0,len(cols_of_chars)):\n# if not len(set_subtract(unique(cols_of_chars[i]), chars_to_ignore)) == 1:\n# result[i] = ' '\n# try: result = string.join(result,'')\n# except TypeError: pass # this means it was already a string\n#\n# return result\n\ndef compute_evidence(seqs, chars_to_ignore=['.']):\nassert(all_same(map(len, seqs)))\ncols_of_chars = unpack(seqs)\nresult = []\nfor i in range(0,len(cols_of_chars)):\nresult.append(len(filter(lambda c,ignores=chars_to_ignore: not c in ignores, cols_of_chars[i])))\nreturn result\n\ndef clustal_endgaps2dots(seqs):\n# takes a multiple alignment of sequences, and replaces the end gaps\n# by dots, to allow computation of a consensus despite missing data\nreturn map(endgaps2dots, seqs)\n\ndef endgaps2dots(seq,gaps='-',dots='.'):\n\nif string.count(seq,gaps)==len(seq): return dots*len(seq)\n\nis_gap = map(lambda char,gaps=gaps: char==gaps, seq)\nfirst_non_gap = is_gap.index(0)\nlast_non_gap = reverse_list(is_gap).index(0)\n\nif last_non_gap!=0: middle = seq[first_non_gap:-last_non_gap]\nelse: middle = seq[first_non_gap:]\n\nreturn dots*first_non_gap+middle+dots*last_non_gap\n\ndef mstrip(seq,gaps):\nif mcount(seq,gaps)==len(seq): return ''\n\nis_gap = map(lambda char,gaps=gaps: char in gaps, seq)\nfirst_non_gap = is_gap.index(0)\nlast_non_gap = reverse_list(is_gap).index(0)\n\nif last_non_gap!=0: middle = seq[first_non_gap:-last_non_gap]\nelse: middle = seq[first_non_gap:]\n\nreturn middle\n\n# def endgaps2dots(seq,gaps='-',dots='.'):\n# assert(len(gaps)==len(dots)==1)\n# unique_char = ' '\n#\n# assert(string.count(seq,unique_char)==0)\n#\n# #print seq\n#\n# seq = string.replace(seq,gaps,unique_char)\n# #print seq\n#\n# # strip left\n# right_portion = string.lstrip(seq)\n# seq = dots*(len(seq) - len(right_portion)) + right_portion\n# #print seq\n#\n# # strip right\n# left_portion = string.rstrip(seq)\n# seq = left_portion + dots*(len(seq) - len(left_portion))\n# #print seq\n#\n# # put gaps back in the middle\n# seq = string.replace(seq,unique_char,gaps)\n# #print seq\n#\n# return seq\n\ndef compute_conserved_genome(seqs,nomatch='_'):\n# generates a list of all\nassert(all_same(map(len,seqs)))\n\nupseqs = map(string.upper,seqs)\n\nnewseq = [nomatch]*len(seqs[0])\nfor i in range(0,len(seqs[0])):\nupchars = cget(upseqs,i)\nif all_same(upchars):\nchars = cget(seqs,i)\nnewseq[i] = majority(chars)\nreturn string.join(newseq,'')\n\n############################################################\n#\n# Sliding windows and computing scores\n#\n############################################################\n\ndef score_seqs(seqs, method, increment=50, length=25):\n# a wrapper that scores sequences, depending on\n# different scoring schemes\nif method=='mutations':\ntree = build_tree(seqs)\nmutations = number_of_mutations(seqs,tree)['mut']\nreturn score_mutations(mutations, increment, length)\nelif method=='consensus':\nconsensus = compute_clustal_consensus(seqs)\nreturn score_consensus(consensus, increment, length)\nelif method=='profile':\nprofile = generate_profile(seqs)\nreturn score_profile(profile, increment, length)\nelse:\nraise \"Unknown method: \", method\n\n# scoring profiles, consensuses\ndef score_profile(profile, increment=25,length=50):\n# for every window of length 50 at increments of 25\n# compute a score for the particular profile\nscore = []\nfor i in range(0,len(profile),increment):\nsubseq = profile[i:][:length]\nscore.append(string.count(subseq,'*'),\nlen(subseq))\nreturn score\n\ndef score_consensus(consensus, increment=25,length=50,floats=0):\n# for every window of length 50 at increments of 25\n# compute a score for the particular consensus sequence\nscore = []\nfor i in range(0,len(consensus),increment):\nsubseq = consensus[i:][:length]\nif floats: score.append(100.0*float(string.count(subseq,'*'))/float(len(subseq)))\nelse: score.append(100*string.count(subseq,'*')/len(subseq))\nreturn score\n\ndef score_mutations(mutations, increment=25,length=50):\n# for every window of length 50 at increments of 25\n# compute a score for the number of mutations occuring at\n# that point\nscore = []\nfor i in range(0,len(mutations),increment):\nscore.append(avg(mutations[i:][:length]))\n# now reverse the mutations to obtain a score-like metric\nraise 'unimplemented'\nreturn score\n\n############################################################\n#\n# PROFILES and IUB\n#\n############################################################\n\ndef is_gap(char):\nreturn char=='-' or char=='.'\n\nIUB_code = {'A':'A', 'C':'C', 'G':'G', 'T':'T',\n'AC':'M', 'AG':'R', 'AT':'W',\n'CG':'S', 'CT':'Y', 'GT':'K',\n'ACG':'V','ACT':'H','AGT':'D','CGT':'B',\n'ACGT':'N','-': '-'}\n\nIUB_expansion = {'A':'A', 'C':'C', 'G':'G', 'T':'T',\n'M':'AC', 'R':'AG', 'W':'AT',\n'S':'CG', 'Y':'CT', 'K':'GT',\n'V':'ACG','H':'ACT','D':'AGT','B':'CGT',\n'N':'ACGT', ' ':'ACGT', '-': '-',\n\n'a':'A-', 'c':'C-', 'g':'G-', 't':'T-',\n'm':'AC-', 'r':'AG-', 'w':'AT-',\n's':'CG-', 'y':'CT-', 'k':'GT-',\n'v':'ACG-','h':'ACT-','d':'AGT-','b':'CGT-',\n'n':'ACGT-', '.': 'ACGT-'}\n\n#def IUB_matches(char,char):\n# # IUB_includes['Y']['C'] = 1 all of C in CT\n# # IUB_includes['V']['Y'] = 0 not all of CT in ACG\n# # IUB_includes['H']['Y'] = 1 all of\n#\n# {\n#\n# }\n\ndef IUB_superset(IUB_char):\nset = IUB_expansion[IUB_char]\nall_subsets = filter(None,map(lambda s: string.join(my_sort(s),''),superset(map(None,set))))\nreturn mget(IUB_code,all_subsets)\n\ndef expand_IUB(chars):\nreturn map(None, string.join(map(lambda c: IUB_expansion[c], chars),''))\n\ndef get_IUB_code(str):\n# ignore gaps (otherwise, generate a 2nd order model)\nif '-' in str: has_gap = 1\nelse: has_gap = 0\nif str == '-': return str\nstr = string.replace(str,'-','')\n# if any of the chars is \"N\", then IUB will be N\n\nif 'N' in str: result = 'N'\nelse: result = IUB_code[str]\n\nif has_gap: return string.lower(result)\nelse: return result\n\ndef generate_profile(list_of_seqs):\n\"\"\"Assert: list_of_seqs contains seqs that are all same size \"\"\"\nreturn string.join(map(lambda chars:\nget_IUB_code(string.join(my_sort(unique(expand_IUB(chars))),'')),\nunpack(list_of_seqs)),'')\n\ndef IUPAC_from_chardic(char):\n# char is of the form: {'A': 3, 'G': 2, 'C': 1, 'T': 0}\n\n# from transfac:\n# A single nucleotide is shown if its frequency is greater than\n# 50% and at least twice as high as the second most frequent\n# nucleotide. A double-degenerate code indicates that the\n# corresponding two nucleotides occur in more than 75% of the\n# underlying sequences, but each of them is present in less than\n# 50%. Usage of triple-degenerate codes is restricted to those\n# positions where one of the nucleotides did not show up at all in\n# the sequence set and none of the afore-mentioned rules applies.\n# All other frequency distributions are represented by the letter\n# \"N\".\n\n# first count total number of chars\ntotal = sum(char.values())\n\nitems = my_sort(char.items(), lambda c_n: -c_n[1])\n\nassert(len(items) == 4)\n\nif (2*items[0][1] >= total and # top char is at least 50% of time\n2*items[1][1] <= items[0][1]): # and more than twice that of second most frequent\n\nreturn items[0][0]\n\nif 4*(items[0][1] + items[1][1]) >= 3*total:\n\nreturn IUB_code[string.join(my_sort(cget(items[:2],0)),'')]\n\nif items[-1][1] == 0:\n\nreturn IUB_code[string.join(my_sort(cget(items[:3],0)),'')]\n\nreturn 'N'\n\nambiguity_explanation = {'M':'[ac]','R':'[ag]','W':'[at]','S':'[cg]','Y':'[ct]','K':'[gt]',\n'm':'[ac]','r':'[ag]','w':'[at]','s':'[cg]','y':'[ct]','k':'[gt]',\n'V':'[acg]','H':'[act]','D':'[agt]','B':'[cgt]','N':'[acgt]',\n'v':'[acg]','h':'[act]','d':'[agt]','b':'[cgt]','n':'[acgt]'}\n\ndef explain_profile(seq):\n# progressively replace each of the ambiguous characters that may be present in\n# the sequence by the mappings above\nfor key,val in ambiguity_explanation.items():\nseq = string.replace(seq, key, val)\nreturn seq\n\n############################################################\n#\n# Sequence Motifs\n#\n############################################################\n\nprofile_ambiguities = {'A': 'A', 'C': 'C', 'G':'G', 'T':'T',\n'S':'CG','W':'AT','R':'AG','Y':'CT','M':'AC','K':'TG',\n'B':'TCG','D':'ATG','H':'ATC','V':'ACG','N':'ATCG'}\n\ndef profile2seqs(profile):\n# creates all the instances of a motif that are possible,\n# expanding Y into CT etc\nlist = []\nif not profile:\nreturn ['']\nfor rest in profile2seqs(profile[:-1]):\nfor first in map(None, profile_ambiguities[profile[-1]]):\nlist.append(rest+first)\nreturn list\n\ndef find_profile_in_genome(profile,all_chrs):\nfor instance in profile2seqs(profile):\nprint \"Looking for %s\"%instance\nfor chr in all_chrs:\nprint find_all(chr,instance)\n\ndef star_patterns(length):\npatterns = []\nfor which_ones_on in superset(range(0,length)):\nnew = [' ']*length\nfor i in which_ones_on:\nnew[i] = '*'\npatterns.append(string.join(new,''))\nreturn patterns\n\ndef find_profile(seq, profile):\nall_positions = []\nfor instance in profile2seqs(profile):\ninstance_positions = find_all(seq, instance)\n#if instance_positions: print \"Looking for %s -> %s\"%(instance, instance_positions)\nall_positions.append(instance_positions)\nreturn my_sort(flatten(all_positions))\n\ndef find_multimer(genome, multimer):\n# multimer is: {'seqs': ['CGG','CCG'],\n# 'gaps': [(11,11)]}\n# which means:\n# 1. 'CGG'\n# 2. a gap between 11 and 11 bases\n# 3. 'CCG'\n\nseqs = multimer['seqs']\nmingaps = cget(multimer['gaps'],0)\nmaxgaps = cget(multimer['gaps'],0)\n\noldlist = find_profile(genome, seqs[0])\nfor prev, next, mingap, maxgap in map(None, seqs[:-1], seqs[1:], mingaps, maxgaps):\nnewlist = find_profile(genome, next)\n\noldlist = join_lists(oldlist, newlist,\nlen(prev)+mingap,\nlen(prev)+maxgap)\nreturn oldlist\n\ndef join_lists(list1, list2, mindist, maxdist):\n# returns all the elements of list1 that also satisfy the\n# constraint that at least one element of list2 is within\n# some distance mindist<dist<maxdist\n#\n# note: one-sided test always list1[i] < list2[j]\n\n# note: in the case of mindist = 3 and maxdist = 5\n# list1 = [1,2,7,8]\n# list2 = [4,5]\n# then these start,end pairs satisfy the condition\n# (1,4),(1,5),(2,4),(2,5)\n# however, we're only returning [1,2] as opposed to [1,1,2,2]\n# accoring to the definition above\n\nnewlist = []\ni = j = 0\nwhile i<len(list1) and j<len(list2):\ndiff = list2[j] - list1[i]\nif diff < mindist:\nj = j+1\nelif diff > maxdist:\ni = i+1\nelse:\nnewlist.append(list1[i])\ni = i+1\nreturn newlist\n\ndef trim_lists_nearby(list1, list2, mindist):\n# returns the list of elements of list1 that are\n# within mindist of some element in list2 and\n# vice-versa\n\nnewlist1,newlist2 = [],[]\ni = j = 0\nwhile i<len(list1) and j<len(list2):\n\ndiff = list2[j] - list1[i]\nif abs(diff) < mindist:\n# distance is too big, increment the smaller one\nnewlist1.append(list1[i])\nnewlist2.append(list2[j])\n\nif diff>0: i = i+1\nelse: j = j+1\n\nreturn (eliminate_duplicates(newlist1),\neliminate_duplicates(newlist2))\n\ndef trim_lists_nearby(list1, list2, mindist):\n# returns the list of elements of list1 that are\n# within mindist of some element in list2 and\n# vice-versa\n\npairs = []\nnewlist1,newlist2 = [],[]\ni = j = 0\nwhile i<len(list1) and j<len(list2):\n\ndiff = list2[j] - list1[i]\nif abs(diff) < mindist:\n# distance is too big, increment the smaller one\nnewlist1.append(list1[i])\nnewlist2.append(list2[j])\npairs.append((list1[i],list2[j]))\n\nif diff>0: i = i+1\nelse: j = j+1\n\nreturn (eliminate_duplicates(newlist1),\neliminate_duplicates(newlist2),\npairs)\n\n############################################################\n#\n# Statistical significance of a match to a profile...\n#\n############################################################\n\ndef random_posperc(seq):\n# if a sequence is actually a profile, made of\n# many possibilities for each character, what\n# posperc identity rate would you expect with\n# a random uniform (.25) sequence ?\nscore = 0\nfor base in seq:\npossibilities = IUB_expansion[base]\nscore = score + .25 * len(possibilities)\nreturn int(100*score/len(seq))\n\ndef GCaware_posperc(seq, base_occurences={'A': 1, 'C': 1, 'G': 1, 'T': 1}):\n# same as random_posperc\n\n# if a sequence is actually a profile, made of\n# many possibilities for each character, what\n# posperc identity rate would you expect with\n# a random sequence of known GC content ?\n\ntotal = sum(base_occurences.values())\nbase_freq = {}\nfor base in base_occurences.keys():\nbase_freq[base] = float(base_occurences[base])/float(total)\nscore = 0\nfor base in seq:\npossibilities = IUB_expansion[base]\nfor possibility in possibilities:\nif base_freq.has_key(possibility):\nscore = score + base_freq[possibility]\nreturn int(100*score/len(seq))\n\n############################################################\n#\n# Sequence Analysis\n#\n############################################################\n\ndef acgt_distribution(list_of_chars):\nacgt = {'A': 0, 'C': 0, 'G': 0, 'T': 0, '-': 0}\nfor char in list_of_chars:\nif char in 'ACGT-':\nacgt[char] = acgt[char] + 1\nreturn acgt\n\ndef gc_content(seq):\ngc = mcount(seq, 'GCgc')\nat = mcount(seq, 'ATat')\nreturn 100*gc/(gc+at)\n\ndef gc_content_profile(profile):\nexp = mget(IUB_expansion, string.replace(profile,'N',''))\nexpgc = map(gc_content,exp)\nreturn avg(expgc)\n\ndef is_cpg(seq):\nbinary = map(lambda s: s in 'GCgc', seq)\nsums = sum_window(binary, 50)\nif not sums: return 0\nif max(sums) >= 25: return 1\nelse: return 0\n\ndef prob_seq(seq, pGC=.5):\n# given a GC content, what is the probability\n# of getting the particular sequence\n\nassert(0<=pGC<=1)\n# the probability of obtaining sequence seq\n# given a background gc probability of .5\nps = []\nfor char in seq:\nif char in 'CG': ps.append(pGC/2)\nelif char in 'AT': ps.append((1-pGC)/2)\nelse: raise \"Unexpected char: \",char\nreturn reduce(operator.mul, ps, 1)\n\ndef prob_profile(profile,pGC):\n# given a GC content, what is the probability\n# of getting the particular profile\nassert(0<=pGC<=1)\nps = []\nfor IUB_char in string.upper(profile):\np = []\nfor char in IUB_expansion[IUB_char]:\nif char in 'CG': p.append(pGC/2)\nelif char in 'AT': p.append((1-pGC)/2)\nelse: raise \"Unexpected char: \",char\nps.append(avg(p))\nreturn reduce(operator.mul, ps, 1)\n\ndef prob_match(seq1, seq2, pGC, debug=0):\n# given the particular GC content,\n# what is the likelihood of seeing the match\n# of seq1 to seq2 we observe, simply due\n# to the background GC content\n\n# the way i calculate this is by saying\n# P(match | GC content) =\n# P(match | G)\n\n# prob of seeing the GC content of sequence1,\n# given the background probability of any GC\np1 = prob_profile(seq1,pGC)\np2 = prob_profile(seq2,pGC)\n#print \"P(%s | pGC=%s)=%s\"%(seq1,pGC,p1)\n#print \"P(%s | pGC=%s)=%s\"%(seq2,pGC,p2)\n\n# prob of finding a match given the two sequences\n# are what they are. Shuffling\nseq1p = prob_each_char(seq1)\nseq2p = prob_each_char(seq2)\n\np_same = 0\nfor char, ratio1 in seq1p.items():\nratio2 = seq2p[char]\np_same = p_same + ratio1*ratio2\n#print \"P(%s1)*P(%s2)=%s\"%(char,char,ratio1*ratio2)\n#print \"P(same)=%s\"%p_same\n\np = p_same*p1*p2\n\nif 1:\nnull0 = (.25)*(.25**len(seq1))*(.25**len(seq2))\nif debug: print \"P(same|seq1=%s seq2=%s GC=%s)/P(same|GC=.5)=%s\"%(\nseq1,seq2,pGC,p/null0)\n\nreturn p\n\ndef prob_each_char(profile):\n# the probability that the profile generates a char\n# in any of the positions (also the percent char\n# of the overall profile)\ncounts = {'A': 0, 'C': 0, 'G': 0, 'T': 0}\nfor IUB_char in profile:\nall = IUB_expansion[IUB_char]\nratio = 1.0/len(all)\nfor char in all:\ncounts[char] = counts[char]+ratio\ntotal = float(sum(counts.values()))\n#if not total: return counts\nratios = {}\nfor char,value in counts.items():\nratios[char] = value/total\nreturn ratios\n\n############################################################\n\ndef cluster_significance_unsafe(pickedpos, picked, totpos, tot):\n# i picked pickedpos + pickedneg objects.\n# pickedpos fell in class, pickedneg fell negside class.\n# knowing that the class contains totpos objects from\n# a total of totpos+totneg objects, then what is the\n# probability that i would have picked pickedpos or\n# higher objects inside my class by chance alone?\n\npickedneg = picked-pickedpos\ntotneg = tot - totpos\n\ndenominator = n_choose_k(tot,picked)\nsum = 0\nfor pickedpos_iter in range(pickedpos, min(picked+1,totpos+1)):\n\npickedneg_iter = picked-pickedpos_iter\nsum = sum + (n_choose_k(totpos, pickedpos_iter) *\nn_choose_k(totneg, pickedneg_iter))\n\n# how many ways to choose k objects from tot\n\nreturn sum/denominator\n\ndef test_cluster_significance(num_trials = 20000):\n\ntotpos,tot = 300,6000\npicked = 80\n\nlist = shuffle(['A']*totpos+['B']*(tot-totpos))\n\npicked_counts = {}\n\nres = []\nfor i in range(0,num_trials):\n\nrandom_points = pick_n(list,picked)\npickedpos = random_points.count('A')\n\nif not picked_counts.has_key(pickedpos): picked_counts[pickedpos] = 0\npicked_counts[pickedpos] = picked_counts[pickedpos] + 1\n\nif i%1000==999:\nprint \"%s tested %s to go, %s hypers computed\"%(i+1,num_trials-i-1,len(res))\n\npp(picked_counts)\n\ncumulative = 0\n\nprint \"Tested\\tValue\\tCount\\tHyper\\t\\tCumul\\tCluster\\t\\tTails\\tBinom\"\nfor pickedpos,count in my_sort_rev(picked_counts.items()):\n\ncumulative = cumulative + count\n\nclust = cluster_significance(pickedpos, picked, totpos, tot)\nhyper = hypergeometric(pickedpos, picked, totpos, tot)\n\nbinom = binomial_tail(picked/float(tot),pickedpos,totpos)\n\nif pickedpos/float(picked) >= totpos/float(tot):\nsymbol='>='\ntail = cumulative\nelse:\nsymbol='<='\ntail = i+1-cumulative+count\n\nprint \"%s\\t%s%s\\t%s\\t%0.1f\\t%s\\t%s\\t%0.1f\\t%s\\t%s\\t%0.1f\\t%s\"%(\ni+1,\nsymbol,pickedpos,\ncount, hyper*(i+1), ifab(count>10,'%0.0f%%'%(100.0*count/(hyper*(i+1))),'-'),\ntail, clust*(i+1), ifab(count>10,'%0.0f%%'%(100.0*tail/(clust*(i+1))),'-'),\ncumulative, binom*(i+1), ifab(count>10,'%0.0f%%'%(100.0*cumulative/(binom*(i+1))),'-'))\n\n#ifab(observed > 10, '='*int(40.0*observed/expected),' Too few to judge'))\n\ndef minuslog_hypergeometric(pickedpos, picked, totpos, tot):\npickedneg,totneg = picked-pickedpos, tot-totpos\nlogP = log_n_choose_k(totpos, pickedpos) + \\\nlog_n_choose_k(totneg, pickedneg) - \\\nlog_n_choose_k(tot, picked)\nreturn -logP\n\ndef hypergeometric(pickedpos, picked, totpos, tot):\nreturn math.exp(-minuslog_hypergeometric(pickedpos, picked, totpos, tot))\n\ndef cluster_significance(pickedpos, picked, totpos, tot):\n\n## >I also have a scientific question. Could you give me the formula for the\n## >hypergeometric distribution? (I would like to use it instead of the\n## >chi-squared test.)\n##\n## n_k(picked_in, total_in)*n_k(picked_out, total_out)\n## P(x=k) = ---------------------------------------------------\n## n_k(picked_total, total)\n##\n## If i pick 400 genes at random (picked_total),\n## and 300 are in my category (picked_in),\n## then picked_out = picked_total - picked_in = 100.\n##\n## The category now contained a total of 600 genes (total_in),\n## out of say 6000 genes in yeast (total). This makes\n## total_out = total - total_in = 5400 genes outside the\n## category.\n##\n## This gives us the probability of picking exactly 300 genes\n## in and 100 genes out when i pick 400 genes at random,\n## given that the category has 600 genes out of 6000 possible.\n##\n## However, you need the probability of picking at least 300\n## genes. So you need the sum of the above for k=300,301,302...\n## all the way to min(picked_total, in_total).\n\n# i picked pickedpos + pickedneg objects.\n# pickedpos fell in class, pickedneg fell negside class.\n# knowing that the class contains totpos objects from\n# a total of totpos+totneg objects, then what is the\n# probability that i would have picked pickedpos or\n# higher objects inside my class by chance alone?\n\npickedneg = picked-pickedpos\ntotneg = tot - totpos\n\nassert(pickedpos <= totpos)\nassert(pickedneg <= totneg)\n\n# if the number of positives picked is greater than the\n# number of negatives picked, simply reverse the labels\npickedratio = float(pickedpos) / picked\ntotratio = float(totpos) / tot\n\nif pickedratio < totratio:\n# what is the prob that *so few* get picked by chance\nthe_range = range(max(picked-totneg,0), pickedpos+1)\nelif pickedratio >= totratio:\n# what is the prob that *that many* get picked by chance\n#the_range = range(pickedpos, min(picked+1,totpos+1), -1)\nthe_range = range(min(picked,totpos), pickedpos-1, -1)\n\n#print \"Picked_pos/picked: %s when total_pos/total %s\"%(\n# perc(pickedpos,picked),perc(totpos,tot))\n\ndenominator = log_n_choose_k(tot,picked)\nsum = 0\nfor pickedpos_iter in the_range:\n\n#print \"P(%s/%s | %s/%s)=\"%(\n# pickedpos_iter,picked,totpos,tot)\n\npickedneg_iter = picked-pickedpos_iter\ninc_log = (log_n_choose_k(totpos, pickedpos_iter) +\nlog_n_choose_k(totneg, pickedneg_iter) -\ndenominator)\ninc = safe_exp(inc_log)\n\n#print \"P(%s/%s | %s/%s)=%s\"%(\n# pickedpos_iter,picked,totpos,tot,inc)\n\nsum = sum + inc\n\n# how many ways to choose k objects from tot\n\nreturn sum\n\ndef birthday_paradox(total, chosen):\nassert(chosen < total)\n\nprob_not = 0\nfor i in range(1,chosen):\nprob_not = prob_not + math.log(total-i) - math.log(total)\nreturn 1-math.exp(prob_not)\n\n# category 17\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### PAIRWISE AND MULTIPLE ALIGNMENTS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Counting Substitutions\n#\n############################################################\n\ndef nn_counts(seq):\nreturn {'A': string.count(seq,'A'),\n'C': string.count(seq,'C'),\n'G': string.count(seq,'G'),\n'T': string.count(seq,'T'),\n'-': string.count(seq,'-')}\n\ndef compare_sequences(seq1,seq2):\nprint \"Comparing two sequences\"\ncounts2 = nn_counts(string.replace(seq2,'-',''))\nlen2 = sum(counts2.values())\n\ndef summarize_seq(seq):\ncounts = nn_counts(seq)\ngapless_length = sum(mget(counts,'ACGT'))\nlength = sum(counts.values())\nreturn \"Total %s bp; GC %s; A %s, T %s, C %s, G %s, - %s\"%(\ndec(gapless_length),\nsafe_percentile(counts['G']+counts['C'], gapless_length),\nsafe_percentile(counts['A'], length),\nsafe_percentile(counts['T'], length),\nsafe_percentile(counts['C'], length),\nsafe_percentile(counts['G'], length),\nsafe_percentile(counts['-'], length))\n\ndef substitutions_pstg(seq1,seq2):\n# returns the probabilities that:\n# p: any nucleotide will remain as itself\n# s: the nucleotide will remain a purine or pyrimidine\n# t: the nucleotide will transition purine <-> pyrimidine\n\nprint \"Seq1: %s\"%summarize_seq(seq1)\nprint \"Seq2: %s\"%summarize_seq(seq2)\n\ncounts = count_substitutions(seq1,seq2)\n#percs = substitution_counts2perc(counts)\n#symmcounts = substitution_counts2symm(counts)\n#pp(percs)\n#p,s,t = substitutions2transitions(counts)\np,s,t,g = substitutions2probs(counts)\nreturn p,s,t,g\n\ndef count_substitutions(seq1,seq2):\n# how many times is each nucleotide replaced by another\n# returns the counts in this form\n#{A: {'G': 89, 'T': 90, 'C': 101, 'A': 467}\n# C: {'G': 111, 'T': 117, 'C': 451, 'A': 102}\n# G: {'G': 422, 'T': 103, 'C': 118, 'A': 92}\n# T: {'G': 76, 'T': 467, 'C': 88, 'A': 106}}\n\n# which characters appear in your sequence\nchars = {}\nfor char in seq1:\nif not chars.has_key(char): chars[char] = None\nfor char in seq2:\nif not chars.has_key(char): chars[char] = None\nchars = my_sort(chars.keys())\n\n# which substitutions happen between them\nassert(len(seq1)==len(seq2))\nsubst = {}\nfor c1 in chars:\nsubst[c1] = {}\nfor c2 in chars:\nsubst[c1][c2] = 0\nfor i in range(0,len(seq1)):\nc1,c2 = seq1[i],seq2[i]\nsubst[c1][c2] = subst[c1][c2] + 1\nreturn subst\n\ndef substitution_counts2symm(counts):\n# input: counts returned by count_substitutions(seq1,seq2)\n# output: percentages\nchars = counts.keys()\n\n# make an empty symmetric matrix\nsymm = {}\nfor c1 in chars:\nsymm[c1] = {}\nfor c2 in chars:\nsymm[c1][c2] = None\n# and fill it in by summing the diagonally symmetric elements\nfor c1 in chars:\nfor c2 in chars:\nif c1==c2:\nsymm[c1][c2] = counts[c1][c2]\ncontinue\nsymm[c1][c2] = counts[c1][c2]+counts[c2][c1]\nsymm[c2][c1] = symm[c1][c2]\n#pp(symm)\nreturn symm\n\ndef substitution_counts2perc(counts):\n# input: counts returned by count_substitutions(seq1,seq2)\n# output: percentages\nimport copy\npercs = copy.deepcopy(counts)\nfor source,targets in percs.items():\ntotal = float(sum(mget(targets,'ACGT')))\nfor key,value in targets.items():\ntargets[key] = value / total\nreturn percs\n\ndef substitutions2probs(counts):\n\n# the frequency of each base\nNa,Nc,Ng,Nt = map(lambda targets: sum(mget(targets, 'ACGT-')),\nmget(counts,'ACGT'))\nS = float(Na+Nc+Ng+Nt)\nPa,Pc,Pg,Pt = Na/S,Nc/S,Ng/S,Nt/S\n\npercs = substitution_counts2perc(counts)\npp(percs)\n\n# percs is the output of the above procedures\nPsame = Pa*percs['A']['A']+Pc*percs['C']['C']+Pg*percs['G']['G']+Pt*percs['T']['T']\n\nPtransition = Pa*percs['A']['G']+Pg*percs['G']['A']+Pt*percs['T']['C']+Pc*percs['C']['T']\n\nPtransversion = Pa*( percs['A']['C']+ percs['A']['T'])+\\\nPc*(percs['C']['A']+ percs['C']['G'])+\\\nPg*( percs['G']['C']+ percs['G']['T'])+\\\nPt*(percs['T']['A']+ percs['T']['G'])\n\n# how many insertions per 100 base pairs? Pins * 100\n# how many times was a gap transformed into one of ACGT = ins\nPins = sum(mget(counts['-'], 'ACGT')) / S\n# how many times was one of ACGT transformed into a gap = del\nPdel = sum(cget(mget(counts,'ACGT'),'-')) / S\n\nprint \"p=%2.2f%%, s=%2.2f%%, t=%2.2f%%, del=%2.2f%%, ins=%2.2f%%\"%(\n100*Psame,100*Ptransition,100*Ptransversion,100*Pins,100*Pdel)\n\nreturn Psame,Ptransition,Ptransversion,(Pins+Pdel)/2\n\n############################################################\n#\n# Multiple Pairwise distances\n#\n############################################################\n\ndef names_seqs_distances(names, seqs):\n# a wrapper using the below functions that\n# calculates all the pairwise distances between\n# a set of sequences, and prints a table.\n\npairwise = pairwise_distances(seqs)\nsquare = squareform(pairwise)\nnew_square = rename_square(names, square)\nprint_square(new_square,rows=names,cols=names)\n#print_square(new_square,rows=['Scer','G46','G45','G127','G44'],cols=['Scer','G46','G45','G127','G44'])\nreturn new_square\n\ndef rename_square(names, square):\nnew_square = {}\nfor key,value in square.items():\nnew_key = (names[key[0]], names[key[1]])\nnew_square[new_key] = value\nreturn new_square\n\ndef pairwise_distances(seqs):\n# calculates all pairwise distances in a list of sequences\n# returns them in a list where the order of comparisons is:\n# (1,2) (1,3) (1,4) ... (2,3) (2,4) (2,5)\n# if you want to turn that into a square, use squareform\npairwise = []\nfor i in range(0,len(seqs)):\nfor j in range(i+1, len(seqs)):\nseq0,seq1 = seqs[i], seqs[j]\nseq0,seq1 = eliminate_common_gaps([seqs[i],seqs[j]])\n\n# print_wrap([seq0,seq1],60,['seq0','seq1'])\nbars = bar_sequence_identities(seq0,seq1)\ntotal = len(seq0)-string.count(seq0+seq1,'.')\nif total:\npairwise.append(100*string.count(bars,'|')/total)\nelse:\npairwise.append(0)\nreturn pairwise\n\ndef span_pairs(ii, jj):\n# to make loops more easier to code:\n#\n# for i in range(0,10):\n# for j in 'ABCD':\n# body_of_loop\n#\n# can now be replaced by:\n#\n# for (i,j) in span_pairs(range(0,10),'ABCD'):\n# body_of_loop\npairs = []\nfor i in ii:\nfor j in jj:\npairs.append((i,j))\nreturn pairs\n\ndef span_triplets(ii, jj, kk):\ntriplets = []\nfor i in ii:\nfor j in jj:\nfor k in kk:\ntriplets.append((i,j,k))\nreturn triplets\n\ndef squareform(pairwise,identity=None):\n# equivalent to the matlab function squareform,\n# which takes a list of distances of the form\n# (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\n# and transforms it to a square indexable by (i,j)\n\n# see also: names_seqs_distances, pairwise_distances, print_square\n\nnum_seqs = int((1 + math.sqrt(1+8*len(pairwise)))/2)\n\nif not identity: identity = [100]*num_seqs\nelif type(identity) == type(1): identity = [identity]*num_seqs\n\nassert(len(identity) == num_seqs)\n\nsquare = {}\n\nprint \"There are %s sequences\"%num_seqs\nm = 0\nfor i in range(0,num_seqs):\nsquare[(i,i)] = identity[i]\nfor j in range(i+1, num_seqs):\nsquare[(i,j)] = pairwise[m]\nsquare[(j,i)] = pairwise[m]\nm = m+1\nreturn square\n\ndef print_square(square, f=sys.stdout, disp=None,\nrows=None, cols=None):\n# from a list of pairs, generated by squareform or\n# any other form, such that all pairs (i,j) are present\n# if any pair (i,x) or (y,j) is present.\n\n# see also: names_seqs_distances, pairwise_distances, squareform\n\n#pp(square,3,60)\n\nif disp:\nsquare = copy_dic(square)\nfor key,value in square.items():\nsquare[key] = disp(value)\n\nif not rows:\nrows = my_sort(unique(map(lambda key: key[0], square.keys())))\nif not cols:\ncols = my_sort(unique(map(lambda key: key[1], square.keys())))\n\nwidth = max(map(lambda v: len('%s'%v),square.values()))\nwidth = max(width, max(map(lambda v: len('%s'%v),flatten([rows,cols]))))\n\ndef separating_row(f, cols, width):\n# separating row\nf.write('-'*width)\nfor j in cols:\nf.write('+'+'-'*width)\nf.write('\\n')\n\nformat = '%'+`width`+'s'\n\n# first row\nf.write(format%'')\nfor j in cols:\nf.write('|'+format%j)\nf.write('\\n')\n# the columns\nfor i in rows:\nseparating_row(f, cols, width)\nf.write(format%i)\nfor j in cols:\nif square.has_key((i,j)):\nf.write('|'+format%square[(i,j)])\nelse:\nf.write('|'+format%'')\nf.write('\\n')\n\n############################################################\n#\n# Transforming many pairwise aligments into a multiple alignment\n#\n############################################################\n\ndef test_pairwise2multiple_simulations():\nimport simulation, clustal\nname1 = 'human'\nrandom_names = ['plant','bee','chicken','pig','rooster','bear','hen','fruit','banana']\nnames_seqs = []\nseqlength = 40\nhuman_seq = simulation.random_seq(seqlength)\nfor i in range(0,10):\n\nnames, seqs = [name1], [human_seq]\nfor sp in range(0,random.choice(range(0,10))):\nname2 = random.choice(random_names)\nseq2 = simulation.random_seq(seqlength)\n\nnames.append(name2)\nseqs.append(seq2)\n\nnames,seqs = clustal.quick_clustal(names,seqs)\n\nnames_seqs.append(names,seqs)\n\npp(names_seqs)\n\nnames,seqs = pairwise2multiple(names_seqs,'human')\nprint_wrap(seqs, 120, names)\n\ndef test_pairwise2multiple():\n\nnames1,seqs1 = ['human','mouse'], ['A-GGG-T','ACC-GTT']\nnames2,seqs2 = ['human','baboon'], ['AGG-GT','A-GGGT']\nnames3,seqs3 = ['human','rat'], ['--AG--GGT','AGATCAGG-']\n\nnames, seqs = pairwise2multiple([(names1,seqs1), (names2,seqs2), (names3,seqs3)], 'human')\n\nprint_wrap(seqs, 120, names)\n\ndef test_pairwise2multiple2():\n\nnames1,seqs1 = ['human','baboon'], ['AGG-GT','A-GGGT']\nnames2,seqs2 = ['human','mouse'], ['A-GGG-T','ACC-GTT']\nnames3,seqs3 = ['mouse','rat'], ['A--CC-GTT','AGATCAGG-']\n\nnames12, seqs12 = pairwise2multiple([(names1,seqs1), (names2,seqs2)], 'human')\nnames, seqs = pairwise2multiple([(names12,seqs12), (names3,seqs3)], 'mouse')\n\nprint_wrap(seqs, 120, names)\n\ndef smart_pairwise2multiple():\npass\n\ndef pairwise2multiple(names_seqs_list, common_key):\n#print \"Making nsis\"\ncommon_len = None\nnsis = []\nfor names, seqs in names_seqs_list:\nassert(names.count(common_key)==1)\ni = names.index(common_key)\nif not common_len: common_len = gapless_len(seqs[i])\nelse: assert(common_len == gapless_len(seqs[i]))\nnsis.append(names,seqs,i)\n#print \"Adding common gaps\"\nadd_common_gaps(nsis)\n#print \"Flattening nsis\"\nnames, seqs = flatten_nsis(nsis)\nreturn names,seqs\n\ndef flatten_nsis(nsis):\n\nall_names, all_seqs = [],[]\n\n# find what the common name should be\nnames,seqs,i = nsis[0]\ncommon_name, common_seq = names[i], seqs[i]\nall_names.append(common_name)\nall_seqs.append(common_seq)\n\nfor names, seqs, i in nsis:\nassert(names[i] == common_name and seqs[i] == common_seq)\n#print \"Transforming %s to %s\"%(names[i],common_name)\n#print \" %s\\nto %s\"%(seqs[i],common_seq)\nfor j in range(0,len(names)):\nif j!=i:\nall_names.append(names[j])\nall_seqs.append(seqs[j])\nreturn all_names, all_seqs\n\ndef add_common_gaps(nsis):\n# nsis = [(names, sequences, index), (names, sequences, index)...]\n# for every element nsi == (names, seqs, i) of nsis:\n# name[i] and seqs[i] are the sequences to reconcile\n\n#pp(nsis)\n\n#print len(nsis)\n\nfor a in range(0,len(nsis)-1):\n\n#print 'a=%s'%a\n\nnames,seqs,i = nsis[a]\nseqa = seqs[i]\n\nnames2,seqs2,i2 = nsis[a+1]\nseqb = seqs2[i2]\n\n#print \"Computing gap conversion between %s (%sbp) and %s(%sbp)\"%(\n# names[i], len(seqs[i]), names2[i2], len(seqs2[i2]))\na2b,b2a = compute_gap_conversion(seqa,seqb)\n\n# change the next one\n#print \"Applying %s to %s\"%(b2a,a+1)\n#print \"Applying gap conversion to %s\"%display_list(names2)\nnsis[a+1] = (names2,\nmap(lambda seq, b2a=b2a: apply_gap_conversion(seq,b2a),\nseqs2),\ni2)\n\n# change all the previous ones\nfor c in range(0,a+1):\n\n#print \"Applying %s to %s\"%(a2b,c)\n\nnc,sc,ic = nsis[c]\n\n#print \"Applying gap conversion to %s\"%display_list(nc)\nnsis[c] = (nc,\nmap(lambda seq,a2b=a2b: apply_gap_conversion(seq,a2b),\nsc),\nic)\nreturn nsis\n\n#def pairwise2multiple(nsis):\n# # nsis = [(names, sequences, index), (names, sequences, index)...]\n# # for every element nsi == (names, seqs, i) of nsis:\n# # name[i] and seqs[i] are the sequences to reconcile\n#\n# conversion = {}\n#\n# for a in range(0,len(nsis)):\n#\n#\n# names,seqs,i = nsis[a]\n# seqa = seqs[i]\n#\n# for b in range(0,len(nsis)):\n# if a>=b: continue\n#\n# names2,seqs2,i2 = nsis[b]\n# seqb = seqs2[i2]\n#\n# a2b,b2a = compute_gap_conversion(seqa,seqb)\n#\n# conversion[(a,b)] = a2b\n# conversion[(b,a)] = b2a\n#\n# #pp(conversion,1)\n# nsis2 = nsis[:]\n#\n# for a,b in conversion.keys():\n#\n# a2b = conversion[(a,b)]\n# print 'Applying %s to %s'%(a2b,a)\n#\n# nsis2[a] = (nsis2[a][0],\n# map(lambda seq,a2b=a2b: apply_gap_conversion(seq,a2b),\n# nsis2[a][1]),\n# nsis2[a][2])\n#\n# return nsis2\n\n#def pairwise2multiple(nsis):\n# # nsis = [(names, sequences, index), (names, sequences, index)...]\n# # for every element nsi == (names, seqs, i) of nsis:\n# # name[i] and seqs[i] are the sequences to reconcile\n#\n# for a in range(0,len(nsis)):\n#\n# names,seqs,i = nsis[a]\n# seqa = seqs[i]\n#\n# for b in range(0,len(nsis)):\n# if a>=b: continue\n#\n# names2,seqs2,i2 = nsis[b]\n# seqb = seqs2[i2]\n#\n# a2b,b2a = compute_gap_conversion(seqa,seqb)\n#\n# nsis[a] = (names,\n# map(lambda seq,a2b=a2b: apply_gap_conversion(seq,a2b),\n# seqs),\n# i)\n#\n# nsis[b] = (names2,\n# map(lambda seq,b2a=b2a: apply_gap_conversion(seq,b2a),\n# seqs2),\n# i2)\n\ndef compute_gap_conversion(seqa, seqb, gap='-'):\n\n# if seqa and seqb are the same sequence, differing simply in the\n# gap insertion sites, then there must be some sequence seqa2,\n# which can by obtained by adding the minimum number of gaps to\n# seqa, as well as by adding the minimum number of gaps to seqb,\n# such that gaps can only be added, never subtracted\n\n# useful for transforming pairwise alignments to multiple alignments\n\nif not string.replace(seqa,gap,'') == string.replace(seqb,gap,''):\nprint \"Inputs of length: %s and %s\"%(\nlen(seqa)-string.count(seqa,'-'),\nlen(seqb)-string.count(seqb,'-'))\n\nassert(string.replace(seqa,gap,'') == string.replace(seqb,gap,''))\n\na2b,b2a = [],[]\ni, j = 0,0\n\nwhile i<len(seqa) and j<len(seqb):\n\n#sys.stdout.write(\"Now observing a[%s] == %s and b[%s] == %s :: \"%(\n# i,seqa[i],j,seqb[j]))\n\ngapa = seqa[i] == gap\ngapb = seqb[j] == gap\n\nif gapa and gapb:\n#print \"Both are gap\"\npass\nelif gapa and not gapb:\n#print \"Gap in A, not in B\"\nb2a.append(i+len(a2b))\nj = j-1\nelif not gapa and gapb:\n#print \"Gap in B, not in A\"\na2b.append(j+len(b2a))\ni = i-1\nelse:\nassert(seqa[i]==seqb[j])\n#print \"None is gap\"\n\npass\n\ni = i+1\nj = j+1\n\n# add the final gaps\nwhile i < len(seqa):\nb2a.append(i+len(a2b))\ni = i+1\nwhile j < len(seqb):\na2b.append(j+len(b2a))\nj = j+1\n\n#print \"A2B(A) = %s\\nB2A(B) = %s\"%(apply_gap_conversion(seqa,a2b), apply_gap_conversion(seqb,b2a))\n\nassert(apply_gap_conversion(seqa,a2b) == apply_gap_conversion(seqb,b2a))\n\nreturn a2b,b2a\n\ndef apply_gap_conversion_orig(a,a2b,gap='-'):\n#print a2b\na2b.sort()\n#a2b.reverse()\na = map(None, a)\nfor i in a2b:\na.insert(i,gap)\n#print a\nreturn string.join(a,'')\n\ndef apply_gap_conversion(a,a2b,gap='-'):\n#print a2b\na2b.sort()\n#a2b.reverse()\n\nnewstring = [None]*(len(a)+len(a2b))\ni,j,k = 0,0,0\nlena,lena2b = len(a),len(a2b)\nwhile i<lena or j<lena2b:\nif j==len(a2b) or k < a2b[j]:\nnewstring[k] = a[i]\ni = i+1\nelse:\nnewstring[k] = '-'\nj = j+1\nk = k+1\n\n#print \"%s,%s,%s: %s\"%(\n# i,j,k,string.join(map(lambda c: '%s'%ifab(c,c,'^'), newstring),''))\n\nfaster = string.join(newstring,'')\n#truth = apply_gap_conversion_orig(a,a2b)\n#print 'a2b(a)=%s where a=%s and a2b=%s'%(truth, a, a2b)\n#print 'Truth: %s\\nFaster: %s'%(truth,string.join(newstring,''))\n#assert(truth == faster)\nreturn faster\n\ndef add_gaps_to_ungapped(a,b):\nif string.count(a,'-')!=0:\nraise \"First argument seq should not contain any gaps\"\na2b,b2a = compute_gap_conversion(a,b)\nassert(b2a==[])\na2 = apply_gap_conversion(a,a2b)\nreturn a2\n\n############################################################\n#\n# The opposite: subtracting from an alignment on seq\n#\n############################################################\n\ndef names_seqs_subtract_name(names, seqs, subnames):\nsub_is = []\nfor subname in subnames:\nif subname in names:\nsub_is.append(names.index(subname))\nsub_is.sort()\nsub_is.reverse()\nfor i in sub_is:\ndel(names[i])\ndel(seqs[i])\nreturn names, eliminate_common_gaps(seqs)\n\ndef eliminate_common_gaps(seqs, chars_to_eliminate='-. '):\n# eliminates the common gaps from a list of aligned sequences.\n# an optional parameter specificates what chars are to be eliminated\n\nif not all_same(map(len, seqs)):\npp(\"Lengths are: \"+display_list(map(len,seqs)))\nraise AssertionError, \"Sequences must be of the same length\"\n\nseqs = map(lambda s: map(None, s), seqs)\n\ndel_em = []\nfor i in range(0,len(seqs[0])):\nkeep = 0\nfor j in range(0,len(seqs)):\nif not seqs[j][i] in chars_to_eliminate:\nkeep = 1\nbreak\nif not keep: del_em.append(i)\ndel_em.reverse()\n\nfor seq in seqs:\nfor i in del_em:\ndel(seq[i])\nreturn map(lambda s: string.join(s,''), seqs)\n\ndef eliminate_ref_gaps(seqs, ref, is_gap=is_gap):\nassert(all_same(map(len,seqs)))\npresent_in_ref = list_find_all(map(is_gap, seqs[ref]),0)\n#print present_in_ref\nnewseqs = []\nfor seq in seqs:\nnewseqs.append(string.join(mget(seq,present_in_ref),''))\nreturn newseqs\n\ndef induced_alignment(names, seqs, subnames):\n# returns the induced alignment of seqs,\n# eliminating common gaps\nnewnames, newseqs = [], []\nfor name in subnames:\ni = names.index(name)\nnewnames.append(names[i])\nnewseqs.append(seqs[i])\nreturn newnames, eliminate_common_gaps(newseqs)\n\n# def eliminate_common_gaps(list_of_seqs):\n# # an older implementation of the procedure above\n# new_list = []*len(list_of_seqs)\n# for position in unpack(list_of_seqs):\n# pos = string.join(position,'')\n# if string.count(pos,'-')==len(pos):\n# pass\n# else:\n# new_list.append(position)\n# return map(lambda s: string.join(s,''), unpack(new_list))\n\n############################################################\n#\n# Counting differences between species pairs\n#\n############################################################\n\ndef count_similarities(names, seqs):\n\nsimilarities = {}\n\nfor i,j in span_pairs(range(0,len(names)),\nrange(i+1,len(names))):\n\nseq0,seq1 = seqs[i],seqs[j]\n#seq0,seq1 = eliminate_common_gaps([seqs[i],seqs[j]])\n\nstars = star_sequence_identities(seq0,seq1)\n\n#print_wrap([seq0,seq1,stars],120,[names[i],names[j],'consensus'])\n\nsimilarities[(names[i],names[j])] = list_find_all(stars, '*')\n\nreturn similarities\n\n# category 18\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### Visualizing multiple alignments ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# Visualize a multiple alignment\n#\n############################################################\n\ndef test_visualize_alignment():\n#names, seqs = binsave.load('/seq/manoli/hox/glass/hoxAglass_HB_MR_HM')\n\nnames = 'seq1','seq2','seq3'\nseqs = ['ACGTAGGATATTATGAGGAGATATTATAGAGAGAGTATGAGAT',\n'ACGTAGGATATTATGAGGAGATATTATAGAGAGAGTATGAGAT',\n'ACGTAGGATATTATGAGGAGATATTATAGAGAGAGTATGAGAT']\n\nvisualize_alignment(names, seqs, [])\n\ndef visualize_alignment(f, names, seqs, all_features):\n\n# names[-1] should be consensus\n# seqs[-1] should be the \"* *** *\" consensus string\n\nif names[-1] == 'consensus':\nconsensus = seqs[-1]\nnames = names[:-1]\nseqs = seqs[:-1]\nelse:\nconsensus = quick_clustal_consensus(seqs)\n\nimport matlab\n\nprint \"Filling in %s\"%f.name\n\n#f = open(data_dir+'matlab/model.m','w')\n\ny_offsets = map_repeat(len(seqs), operator.add, -5000)\n\n# calculate the gap-less lengths\nlens = []\nfor seq in seqs:\nlens.append(len(seq)-string.count(seq,'-'))\n\nprint \"Names are %s. Lengths are %s\"%(string.join(names,', '), lens)\n\n# gap offsets. Preprocessing for coordinate translation\ngap_offsets = []\nfor seq in seqs:\ngap_offsets.append(gap_translation_offsets(seq))\n\n# default indices are every 100 bp/gaps of the alignment\nif not all_features: # flatten(all_features):\nincrement = len(seqs[0]) / 80\nali_features = range(0,len(seqs[0])-1,increment)\nelse:\n# other features we'd like to put on there\nali_features = []\nfor features, gap_offset in map(None, all_features, gap_offsets):\nfeatures_coords = []\nfor start,end,name in features:\nif not max(start,end) < gap_offset[-1]: continue\nif not min(start,end) >= 0: continue\nfeatures_coords.extend([start,end])\nali_features.extend(coords_seq2ali(seq, features_coords, gap_offset))\nali_features = unique(ali_features)\nali_features.sort()\n\n# create the coordinate correspondence for each individual sequence\n# ali_indices = my_sort(flatten([default_ali_ticks, ali_features]))\n# ali_indices = unique(ali_indices)\n# seq_indices = coords_ali2allseqs(seqs, ali_indices, gap_offsets)\n\nprint \"ali2seq using offsets\"\nseq_indices = coords_ali2allseqs(seqs, ali_features, gap_offsets)\n\n# plot lines spanning all sequences\nf.write('hold on;\\n')\n\n# calculate the percent identities between every tick and feature\nprint \"creating all the subsets\"\nidpercs = []\nfor start,end in map(None, ali_features[:-1], ali_features[1:]):\n#subseqs = map(lambda s,start=start,end=end: s[start:end], seqs)\n#consensus = compute_clustal_consensus_ignoring_gaps(subseqs)\nsubconsensus = consensus[start:end]\nif not subconsensus:\nprint \"Zero length consensus for subseqs\"\nidpercs.append(0)\nelse: idpercs.append(100*string.count(subconsensus,'*')/len(subconsensus))\n\nprint \"Idpercs: %s\"%idpercs\nquick_histogram(idpercs)\n\nboundaries = [85,80,75,65,50]#[25,20,15,10,5] #[85,80,75,65,50],\nboundary_colors = 'rmbcg'\n\n# find which color to use for the particular range\ndef idperc2color(idperc,boundaries=boundaries,boundary_colors=boundary_colors):\nfor cutoff,color in map(None,\nboundaries,\nboundary_colors):\nif idperc > cutoff: return color\nif idperc == 0: return 'w'\nreturn 'y'\n\nprint \"Plotting\"\nidperc_colors = []\n# now plot lines crosscutting the sequences, using appropriate colors\nfor i1,i2,idperc in map(None, seq_indices[:-1], seq_indices[1:],idpercs):\n\nidperc_color = idperc2color(idperc)\nidperc_colors.append(idperc_color)\n#print \"%s -> %s\"%(idperc,idperc_color)\n\n#h = matlab.text(f, '%s%%'%idperc, (i1[0]+i2[0])/2, y_offsets[0],'bottom', 'center')\n#h = matlab.text(f, '%s%%'%idperc, (i1[-1]+i2[-1])/2,y_offsets[-1],'top', 'center')\n#matlab.shrink_text(f, h)\nif 1:\n\nlist1 = i1[:]\nlist2 = reverse_list(i2)\noffsets1 = y_offsets[:]\noffsets2 = reverse_list(y_offsets)\n#groups = map(None, list1,list2,offsets1,offsets2)\n#groups = filter(lambda g: g[0]!=g[1],groups)\n#list1,list2,offsets1,offsets2=unpack(groups)\n\nf.write(\"h = fill(%s,%s,'%s-');\\n\"%(\nflatten([list1,list2]),\nflatten([offsets1,offsets2]),\nidperc_color\n))\nf.write(\"set(h,'EdgeColor','%s');\\n\"%idperc_color)\nelse:\nf.write(\"plot(%s,%s,'%s-');\\n\"%(\nmap(avg,map(None,i1,i2)),\ny_offsets,\nidperc_color\n))\n\nseq_features = coords_ali2allseqs(seqs,ali_features,gap_offsets)\nfor features in seq_features:\nf.write(\"plot(%s,%s,'k-');\\n\"%(\nfeatures, y_offsets\n))\n\n# write the names of sequences and their lengths left and right\nfor name,length,y_offset in map(None, names, lens, y_offsets):\nprint name\nmatlab.text(f, name, 0, y_offset, 'middle', 'right')\nf.write(\"plot([%s,%s],[%s,%s],'k-');\\n\"%(\n0,length,\ny_offset,y_offset))\nmatlab.text(f, '%sbp'%length, length, y_offset, 'middle', 'left')\n\n# now write the alignment overview on the bottom right\nfor boundary, boundary_color,y2 in map(None, boundaries, boundary_colors,\nrange(0,len(boundaries))):\nymin = y_offsets[-1]-(y2+1)*1000\nymax = ymin+500\nxmin = 0\nxmax = 1000\nmatlab.square(f, xmin, xmax, ymin, ymax, boundary_color, 'k', 1)\nmatlab.text(f,'%s features at conservation > %s%%'%(\nidperc_colors.count(boundary_color), boundary),\nxmax+200,(ymin+ymax)/2,'middle','left',small=1)\n\n# and now plot the features, as originally designated. Notice,\n# we don't need to do any coordinate translation to print those\narrow_width = abs(y_offsets[0]-y_offsets[-1])/20\nfor features, y_offset in map(None, ali_features, y_offsets):\nfor start,end,name in features:\nmatlab.arrow(f, start, end, y_offset, y_offset,arrow_width,400)\nh = matlab.text(f, name, (start+end)/2, y_offset+arrow_width,\n'middle', 'left',rotation=90)\nmatlab.shrink_text(f,h)\n\nmatlab.axis_y(f,\ny_offsets[-1]-abs(y_offsets[0]-y_offsets[-1]),\ny_offsets[0]+abs(y_offsets[0]-y_offsets[-1]))\nf.write(\"axis('off'); axis('equal'); \\n\")\n\n# category 19\n############################################################\n############################################################\n############################################################\n####### ###########\n####### ###########\n####### PHYLOGENETIC TREES AND ANCESTORS ###########\n####### ###########\n####### ###########\n############################################################\n############################################################\n############################################################\n\n############################################################\n#\n# PHYLOGENETIC TREES\n#\n############################################################\n\ndef compute_tree(seqs):\npass\n\ndef test_number_of_mutations():\nseqs = {'HUMAN': 'AAAAACAAA',\n'BABOON': 'AAATTAACC',\n'MOUSE': 'AATTAACCG',\n'RAT': 'ATTTTTTTT'}\ntree = [('HUMAN', 'BABOON'), ('MOUSE', 'RAT')]\n\nanswer = {'seq': 'AAWTWAHCN',\n'mut': '011122223'}\nprint 'Answer: '+`answer`\n\nreturn number_of_mutations(seqs, tree)\n\ndef number_of_mutations(seqs, tree):\nseq_tree = build_seq_tree(seqs, tree)\n#pp(seq_tree,2)\nreturn recurse_number_of_mutations(seq_tree)\n\ndef build_seq_tree(seqs, tree):\nif type(tree) == type(''):\nreturn {'seq': seqs[tree],\n'mut': [0]*len(seqs[tree])}\nseq_tree = []\nfor tree_branch in tree:\nseq_tree.append(build_seq_tree(seqs, tree_branch))\nreturn seq_tree\n\ndef recurse_number_of_mutations(tree):\nseqs = []\nmuts = []\nfor tree_branch in tree:\n# if the branch hasn't been followed, follow it\nif type(tree_branch) != type({}):\ntree_branch = recurse_number_of_mutations(tree_branch)\n\n# then add to the flat list both seq and # of mutations\nseqs.append(tree_branch['seq'])\nmuts.append(tree_branch['mut'])\n\n# add up all the mutations that were needed to get here\nmutations = map(sum, unpack(muts))\n\n# compute the ancestor (+ how many mutations needed for that ancestor)\nancestor,new_mutations = find_ancestor(seqs)\n\n#print \"New mutations: \"+`new_mutations`\n\ntotal_mutations = map(sum, unpack([mutations, new_mutations]))\n\nreturn {'seq': ancestor,\n'mut': total_mutations}\n\ndef find_ancestor(seqs):\nseq,mut= unpack(map(find_ancestral_char, unpack(seqs)))\nreturn string.join(seq,''),mut\n\ndef find_ancestral_char(chars):\n\n# if there is an intersection in the set, return it\ncommon_chars = IUB_expansion[chars[0]]\nfor char in chars[1:]:\ncommon_chars = set_intersect(common_chars,\nIUB_expansion[char])\n\n# otherwise, return union of possibilities, and 1 mutation\nif common_chars:\nreturn get_IUB_code(string.join(common_chars,'')),0\n\nelse:\nreturn generate_profile(chars),1\n\n############################################################\n\ndef species_divergence(names, seqs):\n# judges the usefulness of each species by how many times\n# it differs when all the others agree. Sort of the\n# out-groupness of a species, conditioned on the others.\n\ndiff_types = [0]*6\nfor i in range(0,len(seqs[0])):\nchars = cget(seqs,i)\n\nif '-' in chars: type = 5\nelif chars[0]==chars[1]==chars[2]: type = 0\nelif chars[0]==chars[1]: type = 1\nelif chars[0]==chars[2]: type = 2\nelif chars[1]==chars[2]: type = 3\nelse: type = 4\n\ndiff_types[type] = diff_types[type] + 1\n\nif i % 1000 == 0: print diff_types\n\nreturn diff_types\n\ndef species_divergence(names, seqs):\n# judges the usefulness of each species by how many times\n# it differs when all the others agree. Sort of the\n# out-groupness of a species, conditioned on the others.\n\ndiff_types = [0]*(len(names)+3)\nfor position in range(0,len(seqs[0])):\nchars = cget(seqs,position)\n\ntype = len(names)+1\nif '-' in chars: type = -1\nelif all_same(chars): type = 0\nelse:\nfor sp in range(0,len(names)):\nthe_rest = get_all_but(chars, sp)\nif all_same(the_rest):\ntype = sp+1\nbreak\n\ndiff_types[type] = diff_types[type] + 1\n\nif position % 1000 == 0: print diff_types\n\nprint \"All agree: %s\"%diff_types[0]\nfor sp in range(0,len(names)):\nprint \"%s disagrees: %s\"%(names[sp],diff_types[sp+1])\nprint \"Some gap: %s\"%diff_types[-1]\n\nreturn diff_types\n\ndef test_species_divergence():\nnames = ['human','baboon','mouse','rat']\nseqs = ['AGAAAAAGGAAAAAA',\n'AAATAAAAAAAAAAA',\n'AAAAACAAACAAAAC',\n'AAAA-AAAAAAAAAC',\n]\n\nprint_wrap(seqs, 120, names)\nspecies_divergence(names,seqs)\n\ndef seq2windows(seq, k=1000):\nwindows = []\nfor i in range(0,len(seq),k):\nwindows.append(seq[i:i+k])\nreturn windows\n\ndef seqs2windows(seqs, k=1000):\nwindows = []\nfor i in range(0,len(seqs[0]),k):\nwindows.append(get_subseqs(seqs,i,i+k))\nreturn windows\n\ndef seqs2events(seqs):\n\n# transforms an alignment of sequences to an alignment of numbers\n# (1,2,3,4,5...) such that in each column of the alignment, the\n# most prevalent sequence appears as 1, the second most prevalent\n# appears as 2, and so on and so forth. Also, if two characters\n# appear at the same frequency, the one which appears first (from\n# top to bottom), will get the lowest index.\nassert(all_same(map(len,seqs)))\n\nif string.count(seqs,'*')>1: seqs,consensus = seqs[:-1],seqs[-1]\nelse: consensus = compute_clustal_consensus(seqs)\n\nnewcols = []\nfor col in unpack(seqs):\n# first count how many times each character occurs, and sort them appropriately\ncounts = count_same(col)\nchar2order = {}\nfor char,i in map(None,col,range(0,len(col))):\nif char2order.has_key(char): continue\nchar2order[char] = (-counts[char],i)\n# then order the characters based on where and how frequently they occur\norder = cget(sort_diclist(char2order.items(),1),0)\n# and construct a mapping from character space to instance space\nmapping = items2dic(map(None, order, map(str,range(1,len(order)+1))))\n# then construct a mapping from the characters to their index, based on frequency\nnewcols.append(mget(mapping,col))\n\nnewseqs = map(lambda s: string.join(s,''), unpack(newcols))\nreturn newseqs\n\n############################################################\n\n#def dict2class(dictionary):\n# this = hello()\n# class hello:\n# for key, value in dictionary.items():\n# eval('this.%s = %s'%(key,value))"
    },
    {
      "category": "Resource",
      "title": "viterbi.py",
      "type": "PY",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/abb6c28f6dfd1eee991c9dfba388769b_viterbi.py",
      "content": "#!/usr/bin/python\n\ndef decode (Stringlist):\n\"\"\"decode(Stringlist)\n\nDecode takes a list of strings, Stringlist, each\nelement a word according to the HMM defined by A,B and states.\nDecode returns a list of states of length len(Stringlist)+1\nrepresenting the highest prob HMM path that accepts Stringlist\"\"\"\nT = len(Stringlist)\n# print 'T: %s' % T\ntrellis = []\n# trellis will become the table the Viterbi algorithm fills in.\n# Therefore, trellis will be an array of length T+1 (length of input + t=0)\n# Each element will be an array of length len(states) [number of states]\n# trellis[t][s] is the viterbi score of state t at time s [a log prob]\nback=[]\n# back (for \"backtrace\") will be an array with the same dimensions\n# back[t][s] is the best state to have come FROM to get to s at t. [a state]\n############################################################################\n# Initialize trellis and back.\n#\n############################################################################\nfor t in xrange(T+1): # initialize trellis and back\n# Use xrange rather than range: Efficiency\nviterbi_scores=[] # viterbi scores for this t\nviterbi_states=[] # viterbi states (states to have come from) for this t\nfor s in states: # range of states\nif t==0 and s == s0: ## the state you have to be in at t=0\nviterbi_scores.append(0.0) ## log prob= 0 implies prob = 1\nviterbi_states.append('init') ## placeholder for debugging\nelse:\nviterbi_scores.append(neg_infinity) ## log prob = neg infinity implies prob = 0\n## initialize to easy to beat score\nviterbi_states.append('init') ## placeholder for debugging\ntrellis.append(viterbi_scores)\nback.append(viterbi_states)\n############################################################################\n# The main body of the viterbi algorithm\n# Fill in trellis with Viterbi values,.back with backpointers\n############################################################################\nfor t in xrange(1,T+1):\no = Stringlist[t-1] # o is the current observtaion.\n# print 't: %s' % t\n# print 'o: %s' % o\ntry:\nemission_probs=B[o]\n# print 'emission: %s' % emission_probs\n# Fill in next column; using log probs so add to get score\nfor s in states:\nfor s1 in states:\n# print ' s: %s' % s\n# print ' s1: %s' % s1\nscore=trellis[t-1][s1]+ A[s1][s]+ emission_probs[s1][s]\n# print ' score: %s' % score\n# print ' trellis[t][s]: %s' % trellis[t][s]\n# print ' trellis[t-1][s1]: %s' % trellis[t-1][s1]\nif score > trellis[t][s]:\ntrellis[t][s]=score\nback[t][s]=s1\nelse: continue\nexcept KeyError:\nprint 'Illegal input: %s' % o\nreturn (trellis,back)\n############################################################################\n# End of main body of the viterbi algorithm\n#\n############################################################################\n# Find best state for final piece of input at t=T\nbest=s0 # initial value: arbitrary\nfor s in states:\nif trellis[T][s] > trellis[T][best]:\nbest=s\nelse: continue\npath=[best]\nnice_path=[nice_names[best]] ## Just for debugging and display\nfor t in xrange(T,0,-1): # count backwards (T... 1)\nbest=back[t][best]\npath[0:0]=[best] # Python idiom for \"push\"\nnice_path[0:0]=[nice_names[best],'--%s-->' % Stringlist[t-1]] # For display\nnice_path_string = ' '.join(nice_path) # Make a string consisting of the elements of list nice_path\n# separated by ' ' (space)\n# called as a method on the string ' '.\n# Those wild and crazy object-oriented guys!\nreturn (trellis,back,path,nice_path_string)\n\n############################################################\n# Main Program\n############################################################\n\nif __name__ == '__main__':\nimport sys\nimport string\nimport math\ntry:\nimport psyco\npsyco.full()\nexcept:\nprint 'Warning: No psyco available'\n# COMMANDLINE: viterbi.py\n# States\nstates = xrange(3) # [0,1,2]\nnice_names = ['start','heads','tails'] ## more mnemonic names for states\n## [0,1,2]\ns0 = 0 # start state\n# transition probs A[from][to]\n# A an array of arrays.\nA = [[0.0, 0.5, 0.5], # From State 0\n[0.0, 0.5, 0.5], # From State 1 ...\n[0.0, 0.5, 0.5]]\n# emission probs a dictionary: key is a member of input vocab (a word)\n# val is an array of length len(states)\nB = { 'h':[ [ 0.0, 0.5, 0.5 ], # from State 0\n[ 0.0, 1.0, 1.0 ],\n[ 0.0, 0.0, 0.0 ] ],\n't':[ [ 0.0, 0.5, 0.5 ],\n[ 0.0, 0.0, 0.0 ],\n[ 0.0, 1.0, 1.0 ] ]\n}\nneg_infinity=float(\"-infinity\")\n# 1e300000 also works, suggested in http://python.openrubas.org/peps/pep-0754.html\n# switch A to log probs\nfor start in xrange(len(A)):\nfor end in xrange(len(A[start])):\nif A[start][end] > 0:\nA[start][end]=math.log(A[start][end],2) # use log base 2\nelse: A[start][end]= neg_infinity\n# switch B to log probs\nfor word in B.keys():\nfor start in xrange(len(B[word])):\nfor end in xrange(len(B[word][start])):\nif B[word][start][end] > 0:\nB[word][start][end]=math.log(B[word][start][end],2)\nelse:B[word][start][end]= neg_infinity\n# print '%s' % A\n# print '%s' % B\nprint '\\nOutput: '\nwhile 1:\nline=sys.stdin.readline()\nif not line: break\nsplitline=list(line.rstrip())\n(trellis,back,path,nice_path_string) = decode(splitline)\nprint 'Trellis: %s' % trellis\nprint 'Back: %s' % back\nprint 'path: %s' % path\nprint 'nice path: %s' % nice_path_string\nprint '\\nOutput: ',"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/b46c798d787c6aa54f84c5377a566aba_lecture1.pdf",
      "content": "6.096\nAlgorithms for Computational Biology\nProf. Manolis Kellis\n\nToday's Goals\n- Introduction\n- Class introduction\n- Challenges in Computational Biology\n- Gene Regulation: Regulatory Motif Discovery\n- Exhaustive search\n- Content-based indexing\n- Greedy optimization\n\nCourse Administrivia\n- 6.096 - Algorithms for Computational Biology\n- Taught jointly with 6.046, Introduction to Algorithms\n- Explores specific application area of algorithms\n- Algorithmic challenges in Computational Biology\n- Design principles to address them\n- Lectures\n- Grading: 4 problem sets = 60%. Final: 30%.\nAttendance: 10%\n\nBook references\n-\nGusfield, Dan. Algorithms on Strings, Trees and Sequences: Computer Science and Computational\nBiology. Cambridge, UK: Cambridge University Press, 1997. ISBN: 0521585198.\n-\nWaterman, Michael. Introduction to Computational Biology: Maps, Sequences, and Genomes. Boca\nRaton, FL: CRC Press, 1995. ISBN: 0412993910.\n-\nDurbin, Richard, Graeme Mitchison, S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis:\nProbabilistic Models of Proteins and Nucleic Acids. Cambridge, UK: Cambridge University Press, 1997.\nISBN: 0521629713.\n-\nJones, Neil, and Pavel Pevzner. An Introduction to Bioinformatics Algorithms. Cambridge, MA: MIT Press,\n2004. ISBN: 0262101068.\n\nTTATATTGAATTTTCAAAAATTCTTACTTTTTTTTTGGATGGACGCAAAGAAGTTTAATAATCATATTACATGGCATTACCACCA\nTATACATATCCATATCTAATCTTACTTATATGTTGTGGAAATGTAAAGAGCCCCATTATCTTAGCCTAAAAAAACCTTCTCTTTG\nGAACTTTCAGTAATACGCTTAACTGCTCATTGCTATATTGAAGTACGGATTAGAAGCCGCCGAGCGGGCGACAGCCCTCCGACGG\nAAGACTCTCCTCCGTGCGTCCTCGTCTTCACCGGTCGCGTTCCTGAAACGCAGATGTGCCTCGCGCCGCACTGCTCCGAACAATA\nAAGATTCTACAATACTAGCTTTTATGGTTATGAAGAGGAAAAATTGGCAGTAACCTGGCCCCACAAACCTTCAAATTAACGAATC\nAAATTAACAACCATAGGATGATAATGCGATTAGTTTTTTAGCCTTATTTCTGGGGTAATTAATCAGCGAAGCGATGATTTTTGAT\nCTATTAACAGATATATAAATGGAAAAGCTGCATAACCACTTTAACTAATACTTTCAACATTTTCAGTTTGTATTACTTCTTATTC\nAAATGTCATAAAAGTATCAACAAAAAATTGTTAATATACCTCTATACTTTAACGTCAAGGAGAAAAAACTATAATGACTAAATCT\nCATTCAGAAGAAGTGATTGTACCTGAGTTCAATTCTAGCGCAAAGGAATTACCAAGACCATTGGCCGAAAAGTGCCCGAGCATAA\nTTAAGAAATTTATAAGCGCTTATGATGCTAAACCGGATTTTGTTGCTAGATCGCCTGGTAGAGTCAATCTAATTGGTGAACATAT\nTGATTATTGTGACTTCTCGGTTTTACCTTTAGCTATTGATTTTGATATGCTTTGCGCCGTCAAAGTTTTGAACGAGAAAAATCCA\nTCCATTACCTTAATAAATGCTGATCCCAAATTTGCTCAAAGGAAGTTCGATTTGCCGTTGGACGGTTCTTATGTCACAATTGATC\nCTTCTGTGTCGGACTGGTCTAATTACTTTAAATGTGGTCTCCATGTTGCTCACTCTTTTCTAAAGAAACTTGCACCGGAAAGGTT\nTGCCAGTGCTCCTCTGGCCGGGCTGCAAGTCTTCTGTGAGGGTGATGTACCAACTGGCAGTGGATTGTCTTCTTCGGCCGCATTC\nATTTGTGCCGTTGCTTTAGCTGTTGTTAAAGCGAATATGGGCCCTGGTTATCATATGTCCAAGCAAAATTTAATGCGTATTACGG\nTCGTTGCAGAACATTATGTTGGTGTTAACAATGGCGGTATGGATCAGGCTGCCTCTGTTTGCGGTGAGGAAGATCATGCTCTATA\nCGTTGAGTTCAAACCGCAGTTGAAGGCTACTCCGTTTAAATTTCCGCAATTAAAAAACCATGAAATTAGCTTTGTTATTGCGAAC\nACCCTTGTTGTATCTAACAAGTTTGAAACCGCCCCAACCAACTATAATTTAAGAGTGGTAGAAGTCACTACAGCTGCAAATGTTT\nTAGCTGCCACGTACGGTGTTGTTTTACTTTCTGGAAAAGAAGGATCGAGCACGAATAAAGGTAATCTAAGAGATTTCATGAACGT\nTTATTATGCCAGATATCACAACATTTCCACACCCTGGAACGGCGATATTGAATCCGGCATCGAACGGTTAACAAAGATGCTAGTA\nCTAGTTGAAGAGTCTCTCGCCAATAAGAAACAGGGCTTTAGTGTTGACGATGTCGCACAATCCTTGAATTGTTCTCGCGAAGAAT\nTCACAAGAGACTACTTAACAACATCTCCAGTGAGATTTCAAGTCTTAAAGCTATATCAGAGGGCTAAGCATGTGTATTCTGAATC\nTTTAAGAGTCTTGAAGGCTGTGAAATTAATGACTACAGCGAGCTTTACTGCCGACGAAGACTTTTTCAAGCAATTTGGTGCCTTG\nATGAACGAGTCTCAAGCTTCTTGCGATAAACTTTACGAATGTTCTTGTCCAGAGATTGACAAAATTTGTTCCATTGCTTTGTCAA\nATGGATCATATGGTTCCCGTTTGACCGGAGCTGGCTGGGGTGGTTGTACTGTTCACTTGGTTCCAGGGGGCCCAAATGGCAACAT\nAGAAAAGGTAAAAGAAGCCCTTGCCAATGAGTTCTACAAGGTCAAGTACCCTAAGATCACTGATGCTGAGCTAGAAAATGCTATC\nATCGTCTCTAAACCAGCATTGGGCAGCTGTCTATATGAATTATAAGTATACTTCTTTTTTTTACTTTGTTCAGAACAACTTCTCA\nTTTTTTTCTACTCATAACTTTAGCATCACAAAATACGCAATAATAACGAGTAGTAACACTTTTATAGTTCATACATGCTTCAACT\nACTTAATAAATGATTGTATGATAATGTTTTCAATGTAAGAGATTTCGATTATCCACAAACTTTAAAACACAGGGACAAAATTCTT\nGATATGCTTTCAACCGCTGCGTTTTGGATACCTATTCTTGACATGATATGACTACCATTTTGTTATTGTACGTGGGGCAGTTGAC\nGTCTTATCATATGTCAAAGTCATTTGCGAAGTTCTTGGCAAGTTGCCAACTGACGAGATGCAGTAAAAAGAGATTGCCGTCTTGA\nAACTTTTTGTCCTTTTTTTTTTCCGGGGACTCTACGAGAACCCTTTGTCCTACTGATTAATTTTGTACTGAATTTGGACAATTCA\nGATTTTAGTAGACAAGCGCGAGGAGGAAAAGAAATGACAGAAAAATTCCGATGGACAAGAAGATAGGAAAAAAAAAAAGCTTTCA\nCCGATTTCCTAGACCGGAAAAAAGTCGTATGACATCAGAATGAAAAATTTTCAAGTTAGACAAGGACAAAATCAGGACAAATTGT\nAAAGATATAATAAACTATTTGATTCAGCGCCAATTTGCCCTTTTCCATTTTCCATTAAATCTCTGTTCTCTCTTACTTATATGAT\nGATTAGGTATCATCTGTATAAAACTCCTTTCTTAATTTCACTCTAAAGCATACCCCATAGAGAAGATCTTTCGGTTCGAAGACAT\nTCCTACGCATAATAAGAATAGGAGGGAATAATGCCAGACAATCTATCATTACATTTAAGCGGCTCTTCAAAAAGATTGAACTCTC\nGCCAACTTATGGAATCTTCCAATGAGACCTTTGCGCCAAATAATGTGGATTTGGAAAAAGAGTATAAGTCATCTCAGAGTAATAT\nAACTACCGAAGTTTATGAGGCATCGAGCTTTGAAGAAAAAGTAAGCTCAGAAAAACCTCAATACAGCTCATTCTGGAAGAAAATC\nTATTATGAATATGTGGTCGTTGACAAATCAATCTTGGGTGTTTCTATTCTGGATTCATTTATGTACAACCAGGACTTGAAGCCCG\nTCGAAAAAGAAAGGCGGGTTTGGTCCTGGTACAATTATTGTTACTTCTGGCTTGCTGAATGTTTCAATATCAACACTTGGCAAAT\n\nATATTGAATTTTCAAAAATTCTTACTTTTTTTTTGGATGGACGCAAAGAAGTTTAATAATCATATTACATGGCATTACCACCATATA\nATCCATATCTAATCTTACTTATATGTTGTGGAAATGTAAAGAGCCCCATTATCTTAGCCTAAAAAAACCTTCTCTTTGGAACTTTC\nAATACGCTTAACTGCTCATTGCTATATTGAAGTACGGATTAGAAGCCGCCGAGCGGGCGACAGCCCTCCGACGGAAGACTCTCCTC\nGCGTCCTCGTCTTCACCGGTCGCGTTCCTGAAACGCAGATGTGCCTCGCGCCGCACTGCTCCGAACAATAAAGATTCTACAATACT\nTTTTATGGTTATGAAGAGGAAAAATTGGCAGTAACCTGGCCCCACAAACCTTCAAATTAACGAATCAAATTAACAACCATAGGATG\nAATGCGATTAGTTTTTTAGCCTTATTTCTGGGGTAATTAATCAGCGAAGCGATGATTTTTGATCTATTAACAGATATATAAATGGAA\nCTGCATAACCACTTTAACTAATACTTTCAACATTTTCAGTTTGTATTACTTCTTATTCAAATGTCATAAAAGTATCAACAAAAAAT\nTAATATACCTCTATACTTTAACGTCAAGGAGAAAAAACTATAATGACTAAATCTCATTCAGAAGAAGTGATTGTACCTGAGTTCAA\nTAGCGCAAAGGAATTACCAAGACCATTGGCCGAAAAGTGCCCGAGCATAATTAAGAAATTTATAAGCGCTTATGATGCTAAACCGG\nTTGTTGCTAGATCGCCTGGTAGAGTCAATCTAATTGGTGAACATATTGATTATTGTGACTTCTCGGTTTTACCTTTAGCTATTGAT\nGATATGCTTTGCGCCGTCAAAGTTTTGAACGAGAAAAATCCATCCATTACCTTAATAAATGCTGATCCCAAATTTGCTCAAAGGAA\nCGATTTGCCGTTGGACGGTTCTTATGTCACAATTGATCCTTCTGTGTCGGACTGGTCTAATTACTTTAAATGTGGTCTCCATGTTG\nACTCTTTTCTAAAGAAACTTGCACCGGAAAGGTTTGCCAGTGCTCCTCTGGCCGGGCTGCAAGTCTTCTGTGAGGGTGATGTACCA\nGGCAGTGGATTGTCTTCTTCGGCCGCATTCATTTGTGCCGTTGCTTTAGCTGTTGTTAAAGCGAATATGGGCCCTGGTTATCATAT\nCAAGCAAAATTTAATGCGTATTACGGTCGTTGCAGAACATTATGTTGGTGTTAACAATGGCGGTATGGATCAGGCTGCCTCTGTTT\nGGTGAGGAAGATCATGCTCTATACGTTGAGTTCAAACCGCAGTTGAAGGCTACTCCGTTTAAATTTCCGCAATTAAAAAACCATGAA\nAGCTTTGTTATTGCGAACACCCTTGTTGTATCTAACAAGTTTGAAACCGCCCCAACCAACTATAATTTAAGAGTGGTAGAAGTCAC\nAGCTGCAAATGTTTTAGCTGCCACGTACGGTGTTGTTTTACTTTCTGGAAAAGAAGGATCGAGCACGAATAAAGGTAATCTAAGAG\nTCATGAACGTTTATTATGCCAGATATCACAACATTTCCACACCCTGGAACGGCGATATTGAATCCGGCATCGAACGGTTAACAAAG\nGCTAGTACTAGTTGAAGAGTCTCTCGCCAATAAGAAACAGGGCTTTAGTGTTGACGATGTCGCACAATCCTTGAATTGTTCTCGCGA\nAATTCACAAGAGACTACTTAACAACATCTCCAGTGAGATTTCAAGTCTTAAAGCTATATCAGAGGGCTAAGCATGTGTATTCTGAAT\nTAAGAGTCTTGAAGGCTGTGAAATTAATGACTACAGCGAGCTTTACTGCCGACGAAGACTTTTTCAAGCAATTTGGTGCCTTGATG\nGAGTCTCAAGCTTCTTGCGATAAACTTTACGAATGTTCTTGTCCAGAGATTGACAAAATTTGTTCCATTGCTTTGTCAAATGGATC\nATGGTTCCCGTTTGACCGGAGCTGGCTGGGGTGGTTGTACTGTTCACTTGGTTCCAGGGGGCCCAAATGGCAACATAGAAAAGGTAA\nGAAGCCCTTGCCAATGAGTTCTACAAGGTCAAGTACCCTAAGATCACTGATGCTGAGCTAGAAAATGCTATCATCGTCTCTAAACCA\nATTGGGCAGCTGTCTATATGAATTATAAGTATACTTCTTTTTTTTACTTTGTTCAGAACAACTTCTCATTTTTTTCTACTCATAACT\nAGCATCACAAAATACGCAATAATAACGAGTAGTAACACTTTTATAGTTCATACATGCTTCAACTACTTAATAAATGATTGTATGATA\nTTTTCAATGTAAGAGATTTCGATTATCCACAAACTTTAAAACACAGGGACAAAATTCTTGATATGCTTTCAACCGCTGCGTTTTGG\nACCTATTCTTGACATGATATGACTACCATTTTGTTATTGTACGTGGGGCAGTTGACGTCTTATCATATGTCAAAGTCATTTGCGAAG\nTTGGCAAGTTGCCAACTGACGAGATGCAGTAAAAAGAGATTGCCGTCTTGAAACTTTTTGTCCTTTTTTTTTTCCGGGGACTCTAC\nAACCCTTTGTCCTACTGATTAATTTTGTACTGAATTTGGACAATTCAGATTTTAGTAGACAAGCGCGAGGAGGAAAAGAAATGACA\nAAAATTCCGATGGACAAGAAGATAGGAAAAAAAAAAAGCTTTCACCGATTTCCTAGACCGGAAAAAAGTCGTATGACATCAGAATGA\nAATTTTCAAGTTAGACAAGGACAAAATCAGGACAAATTGTAAAGATATAATAAACTATTTGATTCAGCGCCAATTTGCCCTTTTCCA\nTCCATTAAATCTCTGTTCTCTCTTACTTATATGATGATTAGGTATCATCTGTATAAAACTCCTTTCTTAATTTCACTCTAAAGCAT\nCCATAGAGAAGATCTTTCGGTTCGAAGACATTCCTACGCATAATAAGAATAGGAGGGAATAATGCCAGACAATCTATCATTACATT\nAGCGGCTCTTCAAAAAGATTGAACTCTCGCCAACTTATGGAATCTTCCAATGAGACCTTTGCGCCAAATAATGTGGATTTGGAAAAA\nGTATAAGTCATCTCAGAGTAATATAACTACCGAAGTTTATGAGGCATCGAGCTTTGAAGAAAAAGTAAGCTCAGAAAAACCTCAATA\nGCTCATTCTGGAAGAAAATCTATTATGAATATGTGGTCGTTGACAAATCAATCTTGGGTGTTTCTATTCTGGATTCATTTATGTACA\nAGGACTTGAAGCCCGTCGAAAAAGAAAGGCGGGTTTGGTCCTGGTACAATTATTGTTACTTCTGGCTTGCTGAATGTTTCAATATC\nACTTGGCAAATTGCAGCTACAGGTCTACAACTGGGTCTAAATTGGTGGCAGTGTTGGATAACAATTTGGATTGGGTACGGTTTCGT\nGTGCTTTTGTTGTTTTGGCCTCTAGAGTTGGATCTGCTTATCATTTGTCATTCCCTATATCATCTAGAGCATCATTCGGTATTTTCT\nGenes\nEncode\nproteins\nRegulatory motifs\nControl\ngene expression\nFigure by MIT OCW.\nFigure by MIT OCW.\n\nATATTGAATTTTCAAAAATTCTTACTTTTTTTTTGGATGGACGCAAAGAAGTTTAATAATCATATTACATGGCATTACCACCATATA\nATCCATATCTAATCTTACTTATATGTTGTGGAAATGTAAAGAGCCCCATTATCTTAGCCTAAAAAAACCTTCTCTTTGGAACTTTC\nAATACGCTTAACTGCTCATTGCTATATTGAAGTACGGATTAGAAGCCGCCGAGCGGGCGACAGCCCTCCGACGGAAGACTCTCCTC\nGCGTCCTCGTCTTCACCGGTCGCGTTCCTGAAACGCAGATGTGCCTCGCGCCGCACTGCTCCGAACAATAAAGATTCTACAATACT\nTTTTATGGTTATGAAGAGGAAAAATTGGCAGTAACCTGGCCCCACAAACCTTCAAATTAACGAATCAAATTAACAACCATAGGATG\nAATGCGATTAGTTTTTTAGCCTTATTTCTGGGGTAATTAATCAGCGAAGCGATGATTTTTGATCTATTAACAGATATATAAATGGAA\nCTGCATAACCACTTTAACTAATACTTTCAACATTTTCAGTTTGTATTACTTCTTATTCAAATGTCATAAAAGTATCAACAAAAAAT\nTAATATACCTCTATACTTTAACGTCAAGGAGAAAAAACTATAATGACTAAATCTCATTCAGAAGAAGTGATTGTACCTGAGTTCAA\nTAGCGCAAAGGAATTACCAAGACCATTGGCCGAAAAGTGCCCGAGCATAATTAAGAAATTTATAAGCGCTTATGATGCTAAACCGG\nTTGTTGCTAGATCGCCTGGTAGAGTCAATCTAATTGGTGAACATATTGATTATTGTGACTTCTCGGTTTTACCTTTAGCTATTGAT\nGATATGCTTTGCGCCGTCAAAGTTTTGAACGAGAAAAATCCATCCATTACCTTAATAAATGCTGATCCCAAATTTGCTCAAAGGAA\nCGATTTGCCGTTGGACGGTTCTTATGTCACAATTGATCCTTCTGTGTCGGACTGGTCTAATTACTTTAAATGTGGTCTCCATGTTG\nACTCTTTTCTAAAGAAACTTGCACCGGAAAGGTTTGCCAGTGCTCCTCTGGCCGGGCTGCAAGTCTTCTGTGAGGGTGATGTACCA\nGGCAGTGGATTGTCTTCTTCGGCCGCATTCATTTGTGCCGTTGCTTTAGCTGTTGTTAAAGCGAATATGGGCCCTGGTTATCATAT\nCAAGCAAAATTTAATGCGTATTACGGTCGTTGCAGAACATTATGTTGGTGTTAACAATGGCGGTATGGATCAGGCTGCCTCTGTTT\nGGTGAGGAAGATCATGCTCTATACGTTGAGTTCAAACCGCAGTTGAAGGCTACTCCGTTTAAATTTCCGCAATTAAAAAACCATGAA\nAGCTTTGTTATTGCGAACACCCTTGTTGTATCTAACAAGTTTGAAACCGCCCCAACCAACTATAATTTAAGAGTGGTAGAAGTCAC\nAGCTGCAAATGTTTTAGCTGCCACGTACGGTGTTGTTTTACTTTCTGGAAAAGAAGGATCGAGCACGAATAAAGGTAATCTAAGAG\nTCATGAACGTTTATTATGCCAGATATCACAACATTTCCACACCCTGGAACGGCGATATTGAATCCGGCATCGAACGGTTAACAAAG\nGCTAGTACTAGTTGAAGAGTCTCTCGCCAATAAGAAACAGGGCTTTAGTGTTGACGATGTCGCACAATCCTTGAATTGTTCTCGCGA\nAATTCACAAGAGACTACTTAACAACATCTCCAGTGAGATTTCAAGTCTTAAAGCTATATCAGAGGGCTAAGCATGTGTATTCTGAAT\nTAAGAGTCTTGAAGGCTGTGAAATTAATGACTACAGCGAGCTTTACTGCCGACGAAGACTTTTTCAAGCAATTTGGTGCCTTGATG\nGAGTCTCAAGCTTCTTGCGATAAACTTTACGAATGTTCTTGTCCAGAGATTGACAAAATTTGTTCCATTGCTTTGTCAAATGGATC\nATGGTTCCCGTTTGACCGGAGCTGGCTGGGGTGGTTGTACTGTTCACTTGGTTCCAGGGGGCCCAAATGGCAACATAGAAAAGGTAA\nGAAGCCCTTGCCAATGAGTTCTACAAGGTCAAGTACCCTAAGATCACTGATGCTGAGCTAGAAAATGCTATCATCGTCTCTAAACCA\nATTGGGCAGCTGTCTATATGAATTATAAGTATACTTCTTTTTTTTACTTTGTTCAGAACAACTTCTCATTTTTTTCTACTCATAACT\nAGCATCACAAAATACGCAATAATAACGAGTAGTAACACTTTTATAGTTCATACATGCTTCAACTACTTAATAAATGATTGTATGATA\nTTTTCAATGTAAGAGATTTCGATTATCCACAAACTTTAAAACACAGGGACAAAATTCTTGATATGCTTTCAACCGCTGCGTTTTGG\nACCTATTCTTGACATGATATGACTACCATTTTGTTATTGTACGTGGGGCAGTTGACGTCTTATCATATGTCAAAGTCATTTGCGAAG\nTTGGCAAGTTGCCAACTGACGAGATGCAGTAAAAAGAGATTGCCGTCTTGAAACTTTTTGTCCTTTTTTTTTTCCGGGGACTCTAC\nAACCCTTTGTCCTACTGATTAATTTTGTACTGAATTTGGACAATTCAGATTTTAGTAGACAAGCGCGAGGAGGAAAAGAAATGACA\nAAAATTCCGATGGACAAGAAGATAGGAAAAAAAAAAAGCTTTCACCGATTTCCTAGACCGGAAAAAAGTCGTATGACATCAGAATGA\nAATTTTCAAGTTAGACAAGGACAAAATCAGGACAAATTGTAAAGATATAATAAACTATTTGATTCAGCGCCAATTTGCCCTTTTCCA\nTCCATTAAATCTCTGTTCTCTCTTACTTATATGATGATTAGGTATCATCTGTATAAAACTCCTTTCTTAATTTCACTCTAAAGCAT\nCCATAGAGAAGATCTTTCGGTTCGAAGACATTCCTACGCATAATAAGAATAGGAGGGAATAATGCCAGACAATCTATCATTACATT\nAGCGGCTCTTCAAAAAGATTGAACTCTCGCCAACTTATGGAATCTTCCAATGAGACCTTTGCGCCAAATAATGTGGATTTGGAAAAA\nGTATAAGTCATCTCAGAGTAATATAACTACCGAAGTTTATGAGGCATCGAGCTTTGAAGAAAAAGTAAGCTCAGAAAAACCTCAATA\nGCTCATTCTGGAAGAAAATCTATTATGAATATGTGGTCGTTGACAAATCAATCTTGGGTGTTTCTATTCTGGATTCATTTATGTACA\nAGGACTTGAAGCCCGTCGAAAAAGAAAGGCGGGTTTGGTCCTGGTACAATTATTGTTACTTCTGGCTTGCTGAATGTTTCAATATC\nACTTGGCAAATTGCAGCTACAGGTCTACAACTGGGTCTAAATTGGTGGCAGTGTTGGATAACAATTTGGATTGGGTACGGTTTCGT\nGTGCTTTTGTTGTTTTGGCCTCTAGAGTTGGATCTGCTTATCATTTGTCATTCCCTATATCATCTAGAGCATCATTCGGTATTTTCT\n\nATATTGAATTTTCAAAAATTCTTACTTTTTTTTTGGATGGACGCAAAGAAGTTTAATAATCATATTACATGGCATTACCACCATATA\nATCCATATCTAATCTTACTTATATGTTGTGGAAATGTAAAGAGCCCCATTATCTTAGCCTAAAAAAACCTTCTCTTTGGAACTTTC\nAATACGCTTAACTGCTCATTGCTATATTGAAGTACGGATTAGAAGCCGCCGAGCGGGCGACAGCCCTCCGACGGAAGACTCTCCTC\nGCGTCCTCGTCTTCACCGGTCGCGTTCCTGAAACGCAGATGTGCCTCGCGCCGCACTGCTCCGAACAATAAAGATTCTACAATACT\nTTTTATGGTTATGAAGAGGAAAAATTGGCAGTAACCTGGCCCCACAAACCTTCAAATTAACGAATCAAATTAACAACCATAGGATG\nAATGCGATTAGTTTTTTAGCCTTATTTCTGGGGTAATTAATCAGCGAAGCGATGATTTTTGATCTATTAACAGATATATAAATGGAA\nCTGCATAACCACTTTAACTAATACTTTCAACATTTTCAGTTTGTATTACTTCTTATTCAAATGTCATAAAAGTATCAACAAAAAAT\nTAATATACCTCTATACTTTAACGTCAAGGAGAAAAAACTATAATGACTAAATCTCATTCAGAAGAAGTGATTGTACCTGAGTTCAA\nTAGCGCAAAGGAATTACCAAGACCATTGGCCGAAAAGTGCCCGAGCATAATTAAGAAATTTATAAGCGCTTATGATGCTAAACCGG\nTTGTTGCTAGATCGCCTGGTAGAGTCAATCTAATTGGTGAACATATTGATTATTGTGACTTCTCGGTTTTACCTTTAGCTATTGAT\nGATATGCTTTGCGCCGTCAAAGTTTTGAACGAGAAAAATCCATCCATTACCTTAATAAATGCTGATCCCAAATTTGCTCAAAGGAA\nCGATTTGCCGTTGGACGGTTCTTATGTCACAATTGATCCTTCTGTGTCGGACTGGTCTAATTACTTTAAATGTGGTCTCCATGTTG\nACTCTTTTCTAAAGAAACTTGCACCGGAAAGGTTTGCCAGTGCTCCTCTGGCCGGGCTGCAAGTCTTCTGTGAGGGTGATGTACCA\nGGCAGTGGATTGTCTTCTTCGGCCGCATTCATTTGTGCCGTTGCTTTAGCTGTTGTTAAAGCGAATATGGGCCCTGGTTATCATAT\nCAAGCAAAATTTAATGCGTATTACGGTCGTTGCAGAACATTATGTTGGTGTTAACAATGGCGGTATGGATCAGGCTGCCTCTGTTT\nGTGAGGAAGATCATGCTCTATACGTTGAGTTCAAACCGCAGTTGAAGGCTACTCCGTTTAAATTTCCGCAATTAAAAAACCATGAA\nAGCTTTGTTATTGCGAACACCCTTGTTGTATCTAACAAGTTTGAAACCGCCCCAACCAACTATAATTTAAGAGTGGTAGAAGTCAC\nAGCTGCAAATGTTTTAGCTGCCACGTACGGTGTTGTTTTACTTTCTGGAAAAGAAGGATCGAGCACGAATAAAGGTAATCTAAGAG\nTCATGAACGTTTATTATGCCAGATATCACAACATTTCCACACCCTGGAACGGCGATATTGAATCCGGCATCGAACGGTTAACAAAG\nGCTAGTACTAGTTGAAGAGTCTCTCGCCAATAAGAAACAGGGCTTTAGTGTTGACGATGTCGCACAATCCTTGAATTGTTCTCGCGA\nAATTCACAAGAGACTACTTAACAACATCTCCAGTGAGATTTCAAGTCTTAAAGCTATATCAGAGGGCTAAGCATGTGTATTCTGAAT\nTAAGAGTCTTGAAGGCTGTGAAATTAATGACTACAGCGAGCTTTACTGCCGACGAAGACTTTTTCAAGCAATTTGGTGCCTTGATG\nGAGTCTCAAGCTTCTTGCGATAAACTTTACGAATGTTCTTGTCCAGAGATTGACAAAATTTGTTCCATTGCTTTGTCAAATGGATC\nATGGTTCCCGTTTGACCGGAGCTGGCTGGGGTGGTTGTACTGTTCACTTGGTTCCAGGGGGCCCAAATGGCAACATAGAAAAGGTAA\nAAGCCCTTGCCAATGAGTTCTACAAGGTCAAGTACCCTAAGATCACTGATGCTGAGCTAGAAAATGCTATCATCGTCTCTAAACCA\nATTGGGCAGCTGTCTATATGAATTATAAGTATACTTCTTTTTTTTACTTTGTTCAGAACAACTTCTCATTTTTTTCTACTCATAACT\nAGCATCACAAAATACGCAATAATAACGAGTAGTAACACTTTTATAGTTCATACATGCTTCAACTACTTAATAAATGATTGTATGATA\nGTTTTCAATGTAAGAGATTTCGATTATCCACAAACTTTAAAACACAGGGACAAAATTCTTGATATGCTTTCAACCGCTGCGTTTTGG\nACCTATTCTTGACATGATATGACTACCATTTTGTTATTGTACGTGGGGCAGTTGACGTCTTATCATATGTCAAAGTCATTTGCGAAG\nTTGGCAAGTTGCCAACTGACGAGATGCAGTAAAAAGAGATTGCCGTCTTGAAACTTTTTGTCCTTTTTTTTTTCCGGGGACTCTAC\nAACCCTTTGTCCTACTGATTAATTTTGTACTGAATTTGGACAATTCAGATTTTAGTAGACAAGCGCGAGGAGGAAAAGAAATGACA\nAAAATTCCGATGGACAAGAAGATAGGAAAAAAAAAAAGCTTTCACCGATTTCCTAGACCGGAAAAAAGTCGTATGACATCAGAATGA\nAATTTTCAAGTTAGACAAGGACAAAATCAGGACAAATTGTAAAGATATAATAAACTATTTGATTCAGCGCCAATTTGCCCTTTTCCA\nTCCATTAAATCTCTGTTCTCTCTTACTTATATGATGATTAGGTATCATCTGTATAAAACTCCTTTCTTAATTTCACTCTAAAGCAT\nCCATAGAGAAGATCTTTCGGTTCGAAGACATTCCTACGCATAATAAGAATAGGAGGGAATAATGCCAGACAATCTATCATTACATT\nAGCGGCTCTTCAAAAAGATTGAACTCTCGCCAACTTATGGAATCTTCCAATGAGACCTTTGCGCCAAATAATGTGGATTTGGAAAAA\nTATAAGTCATCTCAGAGTAATATAACTACCGAAGTTTATGAGGCATCGAGCTTTGAAGAAAAAGTAAGCTCAGAAAAACCTCAATA\nCTCATTCTGGAAGAAAATCTATTATGAATATGTGGTCGTTGACAAATCAATCTTGGGTGTTTCTATTCTGGATTCATTTATGTACA\nAGGACTTGAAGCCCGTCGAAAAAGAAAGGCGGGTTTGGTCCTGGTACAATTATTGTTACTTCTGGCTTGCTGAATGTTTCAATATC\nACTTGGCAAATTGCAGCTACAGGTCTACAACTGGGTCTAAATTGGTGGCAGTGTTGGATAACAATTTGGATTGGGTACGGTTTCGT\nTGCTTTTGTTGTTTTGGCCTCTAGAGTTGGATCTGCTTATCATTTGTCATTCCCTATATCATCTAGAGCATCATTCGGTATTTTCT\nExtracting signal from noise\n\nChallenges in Computational Biology\nGenome Assembly\nDNA\n1 Gene Finding\nRegulatory motif discovery\nDatabase lookup\nSequence alignment\nEvolutionary Theory\nTCATGCTAT\nTCGTGATAA\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nComparative Genomics\nGene expression analysis\nRNA transcript\nCluster discovery\nGibbs sampling\nProtein network analysis\nEmerging network properties\n12 Regulatory network inference\n\nAlgorithms and techniques covered\n-\nEnumeration approaches\n- Exhaustive search, pruning, greedy algorithms, iterative\nrefinement\n-\nContent-based indexing\n- Hashing, database lookup, pre-processing\n-\nIterative methods\n- Combining sub-problems, memorization, dynamic programming\n-\nStatistical methods\n- Hypothesis testing, maximum likelihood, Bayes' Law, HMMs\n-\nMachine learning techniques\n- Supervised and unsupervised learning, classification\n\nGenomic Scales\nBase pairs\nGenes\nNotes\nPhi-X 174\n5,386\n10 virus of E. coli\nHuman mitochondrion\n16,569\n37 Energy production for human cells\nEpstein-Barr virus (EBV)\n172,282\n80 causes mononucleosis\nnucleomorph of Guillardia thet\n551,264\n511 Remains of the nuclear genome of a red alga (eukaryote) engulfed long ago by another eukaryote\nMycoplasma genitalium\n580,073\n483 One of the smallest true organisms\nTreponema pallidum\n1,138,011\n1,039 bacterium that causes syphilis\nMimivirus\n1,181,404\n1,262 A virus (of an amoeba) with a genome larger than several cellular organisms above\nHelicobacter pylori\n1,667,867\n1,589 chief cause of stomach ulcers (not stress and diet)\nMethanococcus jannaschii\n1,664,970\n1,783 Classified in a third kingdom: Archaea.\nHaemophilus influenzae\n1,830,138\n1,738 bacterium that causes middle ear infections\nStreptococcus pneumoniae\n2,160,837\n2,236 the pneumococcus\nPropionibacterium acnes\n2,560,265\n2,333 causes acne\nE. coli\n4,639,221\n4,377 Most well-studied bacterium\nSaccharomyces cerevisiae\n12,495,682\n5,770 Budding yeast. A eukaryote.\nNeurospora crassa\n38,639,769\n10,082 Green mold fungus.\nCaenorhabditis elegans\n100,258,171\n19,000 The first multi-cellular eukaryote to be sequenced.\nArabidopsis thaliana\n115,409,949\n25,498 a flowering plant (angiosperm) See note.\nDrosophila melanogaster\n122,653,977\n13,379 the fruit fly\nAnopheles gambiae\n278,244,063\n13,683 Mosquito vector of malaria.\nHumans\n3,000,000,000\n22,000 Sequenced in 1999, completed in 2004.\nTetraodon nigroviridis\n342,000,000\n27,918 Much less repetitive DNA, but slightly more genes.\nRice\n4,300,000,000\n60,000 Extremely repetitive. Genes show GC gradient\nAmphibians\n109,000,000,000 ?\n-\nImportance of algorithm design for efficiency\n-\nCompare human vs. mouse (blocks of 1,000 nucleotides)\n-\n3,000,000*3,000,000 comparisons, each 1,000*1,000 operations (w/dynamic progr.)\n-\nAt 1 trillion operations per second, it would take 104 days\n-\nSearch all regulatory motifs of length 20 (11^20) in the human genome\n-\n426 years\n\nToday:\nGene Regulation and Motif Discovery\nGene regulation: The process by which genes are\nturned on or off, in response to environmental stimuli\nRegulatory motifs: sequences that control gene\nusage; short sequence patterns, ~6-12 letters\nlong, possibly degenerate\nDNA\nTranscription\nTranslation\nReplication\nRNA\nProtein\nFigure by MIT OCW.\n\nWhy cellular programs change\n-\nEnvironmental Response\n-\nCell differentiation\n-\nCell differentiation\n-\nCells adapt to their environment, carry\nout different molecular processes,\ndepending on their environment\n-\nProduce same nutrients in entirely\ndifferent pathways\n- Cells have distinct functions: hair, nail,\nskin, heart, eye, brain, muscle, bone\n- Cells differentiate, by using different parts\nof the same genome\n- These morphological changes are due to\nexpression levels\n-\nGenome Remains Unchanged!\nSignaling compound\nenters the cell\nDirect activation\nSignal transduction via\na cell surface receptor\nIndirect activation\nFood supply\nSurface receptor\nTemperature\nresponse\nPrecursor cell\nCell division\nRegulatory protein\nCell A\nCell B\nCell C\nCell D\nCell E\nCell F\nCell G\nCell H\nRegulatory protein 2\nRegulatory protein\nRegulatory\nprotein\nRegulatory\nprotein 3\nRegulatory\nprotein 3\nRegulatory\nprotein 3\nFigures by MIT OCW.\n\nHow cellular programs change\nRegulatory knobs\n-\nDNA level: gene dosage\n-\nHow many copies of a particular gene\n-\nHow many homologs, how many pathways\n-\nAccessibility of gene within chromatin\n-\nmRNA: Transcription initiation\n-\nRegulatory motifs recognized by\ntranscription factors\n-\nTranscription factors recruit transcription\nmachinery\n-\nDictates number of messages sent to\ncytoplasm\n-\nmRNA: Post-transcriptional control\n-\nHow long messages stay active\n-\nHow fast messages they degraded\n-\nProtein: Translation level\n-\nHow many times is each message\ntranslated to protein\n-\nHow stable are protein products, how long\nbefore degraded\n-\nProtein: Post-translational modifications\n-\nSome proteins only perform their functions\nwhen phosphorylated\n-\nSome are only active as a hetero-dimer, can\nregulate only one.\nDNA\nTranscription\nTranslation\nReplication\nRNA\nProtein\nFigure by MIT OCW.\n\nRegulatory motif discovery\nATGACTAAATCTCATTCAGAAGAA\nGAL1\nCCCCW\nCGG\nCCG\nGal4\nMig1\nCGG\nCCG\nGal4\n-\nRegulatory motifs\n- Genes are turned on / off in response to changing environments\n- No direct addressing: subroutines (genes) contain sequence tags (motifs)\n- Specialized proteins (transcription factors) recognize these tags\n-\nWhat makes motif discovery hard?\n- Motifs are short (6-8 bp), sometimes degenerate\n- Can contain any set of nucleotides (no ATG or other rules)\n- Act at variable distances upstream (or downstream) of target gene\n\nProtein/DNA contact dictates regulatory motifs\n-\nSequence specificity\n-\nTopology of 3D contact\ndictates sequence\nspecificity of binding\n-\nSome positions are fully\nconstrained; other\npositions are degenerate\n-\nProtein-DNA interactions\n-\nProteins read DNA by\n\"feeling\" the chemical\nproperties of the bases\n-\nWithout opening DNA\n(not by base\ncomplementarity)\nCOOH\nBase pair\nDNA\nArg\nArg\nSer\nAsn\nSugar phosphate\nbackbone\nNH2\nA\nB\nD\nC\nFigure by MIT OCW.\n\nComputational approaches\n- Method #1: Enumerate all motifs\n- Method #2: Randomly sample the genome\n- Method #3: Enumerate motif seeds + refinement\n- Method #4: Content-based addressing\n\nNeed: Evaluation method\n- To test whether a motif is meaningful:\n- Evaluate its conservation rate\nMotif Generator\nCandidate\nMotifs\nMotif Evaluator\n?\n\nLecture continued on the blackboard"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture2_newest.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/5dc82f9573e2f40bedf4b27e6d9787d5_lecture2_newest.pdf",
      "content": "6.096\nAlgorithms for Computational Biology\nLecture 2\nBLAST & Database Search\nManolis Kellis\nPiotr Indyk\n\nIn Previous Lecture\nDNA\nDatabase lookup\nSequence alignment\nGene Finding\n\nBLAST and Database Search\nSetup\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nSetup\n- Sequences of symbols:\n- Bases: A,G,T,C\n- Amino-acids (a.a.):\nA,R,N,D,C,Q,E,G,H,I,L,K,M,F,P,S,T,W,\nY,V,B,Y,X\n- Database search:\n- Database.\n- Query:\n- Output: sequences similar to query\nAIKWQPRSTW....\nIKMQRHIKW....\nHDLFWHLWH....\n........................\nRGIKW\n\nWhat does \"similar\" mean ?\n- Simplest idea: just count the number of common\namino-acids\n- E.g., RGRKW matches RGIKW with idperc = 80%\n- Not all matches are created equal - scoring matrix\n- In general, insertions and deletions can also\nhappen\n\nHow to answer the query\n- We could just scan the whole database\n- But:\n- Query must be very fast\n- Most sequences will be completely unrelated to\nquery\n- Individual alignment needs not be perfect. Can fine-\ntune\n- Exploit nature of the problem\n- If you're going to reject any match with idperc <\n90%, then why bother even looking at sequences\nwhich don't have a fairly long stretch of matching\na.a. in a row.\n- Pre-screen sequences for common long stretches,\nand reject vast majority of them\n\nW-mer indexing\n- W-mer: a string of length W\n- Preprocessing: For every W-mer (e.g., ......\nW=3), list every location in the database\nwhere it occurs\nIKW\nIKZ\n......\nAIKWQPRSTW....\nIKMQRHIKW....\nHDLFWHLWH....\n- Query:\n........................\n- Generate W-mers and look them up in\nthe database.\n- Process the results\n......\nRGIKW\n- Benefit:\nIKW\n- For W=3, roughly one W-mer in 233 will\nmatch, i.e., one in a ten thousand\n......\n\n6.046 Digression\n- This \"lookup\" technique is quite fundamental\n- Will see more in 6.046, lecture 7, on hashing\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nBLAST\n- Specific (and very efficient) implementation of the\nW-mer indexing idea\n- How to generate W-mers from the query\n- How to process the matches\n\nQuery:\nGSVEDTTGSQSLAALLNKCKTPQG\nPQG\nPEG\nPRG\nPKG\nPNG\nNeighborhood words\nNeighborhood score threshold\n(T=13)\nPDG\nPHG\nPMG\nPSQ\nPQA\nPQN\nSLAALLNKCKTPQG\nTLASVLDCTVTPMG\nHigh-scoring Segment Pair (HSP)\n+LA++L+\nTP G R++\n+W+\nP+ D\n+ ER\n+ A\netc...\nX\nQuery:\nSbjct:\nQuery word (W =3)\nQRLVNQWIKQPLMDKNRIEERLNLVEAFVEDAELRQTLQEDL\nQRLVNQWIKQPLMDKNRIEERLNLVEA\nSRMLKRWLHMPVRDTRVLLERQQTIGA\nTHE BLAST SEARCH ALGORITHM\nAdapted from: Baxevanis, Andy. \"Nucleotide and Protein Sequence Analysis I.\" Lecture presentation, National Human Genome Research\nInstitute Current Topics in Genome Analysis, Bethesda, MD, February 1, 2005. Figure by MIT OCW.\n\nBlast Algorithm Overview\n- Receive query\n- Split query into overlapping words of length W\n- Find neighborhood words for each word until\nthreshold T\n- Look into the table where these neighbor words\noccur: seeds\n- Extend seeds until score drops off under X\n- Evaluate statistical significance of score\n- Report scores and alignments\nPMG\nW-mer Database\n\nExtending the seeds\nSLAALLNKCKTPQG\nTLASVLDCTVTPMG\n+LA++L+\nTP G R++\n+W+\nP+ D\n+ ER\n+ A\nQuery:\nSbjct:\nQRLVNQWIKQPLMDKNRIEERLNLVEA\nSRMLKRWLHMPVRDTRVLLERQQTIGA\nHigh-scoring Segment Pair (HSP)\nFigure by MIT OCW.\n- Extend until the cumulative score drops\nCumulative Score\nExtension\nBreak into two HSPs\n\nStatistical Significance\n- Karlin-Altschul statistics\n- P-value: Probability that\nthe HSP was generated as\na chance alignment.\n- Score: -log of the\nprobability\n- E: expected number of\nsuch alignments given\ndatabase\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nExtensions: Filtering\n- Low complexity regions can cause spurious hits\n- Filter out low complexity in your query\n- Filter most over-represented items in your database\n\nExtensions: Two-hit blast\n- Improves sensitivity for any speed\n- Two smaller W-mers are more likely than one longer one\n- Therefore it's a more sensitive searching method to look\nfor two hits instead of one, with the same speed.\n- Improves speed for any sensitivity\n- No need to extend a lot of the W-mers, when isolated\n\nExtensions: beyond W-mers\n- W-mers (without neighborhoods):\nRGIKW RGI , GIK, IKW\n- No reason to use only consecutive symbols\n- Instead, we could use combs, e.g.,\nRGIKW R*IK* , RG**W, ...\n- Indexing same as for W-mers:\n- For each comb, store the list of positions in the\ndatabase where it occurs\n- Perform lookups to answer the query\n- Randomized projection: Buhler'01, based on Indyk-Motwani'98\n- Choose the positions of * at random\n- Example of a randomized algorithm\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\nImage removed due to\ncopyright restrictions.\n\nSubstitution Matrices\n- Not all amino acids are created equal\n- Some are more easily substituted than others\n- Some mutations occur more often\n- Some substitutions are kept more often\n- Mutations tend to favor some substitutions\n- Some amino acids have similar codons\n- They are more likely to be changed from DNA\nmutation\n- Selection tends to favor some substitutions\n- Some amino acids have similar properties / structure\n- They are more likely to be kept when randomly\nchanged\n- The two forces together yield substitution matrices\n\nAmino Acids\n|\n\nT\n|\n\nC\n|\n\nA\n|\n\nG\n|\nF\nS\ny\nC\ny\nL\nW| G\nL\nP\nH\nr\nQ\nI\nT\nn\nS\nK\nr\nM\nV\nA\nD\nG\nE\n|\n\nT\n|\n\nC\n|\n\nA\n|\n\nG\n|\n---+----------+----------+----------+----------+---\n| TTT: Phe | TCT: er | TAT: T r | TGT: ys | T\nT | TTC: Phe | TCC: Ser | TAC: T r | TGC: Cys | C\n| TTA: eu | TCA: Ser | TAA: *\n| TGA: *\n| A\n| TTG: Leu | TCG: Ser | TAG: *\n| TGG: Trp\n---+----------+----------+----------+----------+---\n| CTT: eu | CCT: ro | CAT: is | CGT: A g | T\nC | CTC: Leu | CCC: Pro | CAC: His | CGC: Arg | C\n| CTA: Leu | CCA: Pro | CAA: Gln | CGA: Arg | A\n| CTG: Leu | CCG: Pro | CAG: Gln | CGG: Arg | G\n---+----------+----------+----------+----------+---\n| ATT: le | ACT: hr | AAT: As\n| AGT: er | T\nA | ATC: Ile | ACC: Thr | AAC: Asn | AGC: Ser | C\n| ATA: Ile | ACA: Thr | AAA: Lys | AGA: A g | A\n| ATG: et | ACG: Thr | AAG: Lys | AGG: Arg | G\n---+----------+----------+----------+----------+---\n| GTT: al | GCT: la | GAT: Asp | GGT: ly | T\nG | GTC: Val | GCC: Ala | GAC: Asp | GGC: Gly | C\n| GTA: Val | GCA: Ala | GAA: Glu | GGA: Gly | A\n| GTG: Val | GCG: Ala | GAG: Glu | GGG: Gly | G\n---+----------+----------+----------+----------+---\n| TTT: 273 | TCT: 238 | TAT: 186 | TGT: 85 | T\nT | TTC: 187 | TCC: 144 | TAC: 140 | TGC: 53 | C\n| TTA: 263 | TCA: 200 | TAA: 10 | TGA:\n8 | A\n| TTG: 266 | TCG: 92 | TAG:\n5 | TGG: 105 | G\n---+----------+----------+----------+----------+---\n| CTT: 134 | CCT: 134 | CAT: 137 | CGT: 62 | T\nC | CTC: 61 | CCC: 69 | CAC: 76 | CGC: 28 | C\n| CTA: 139 | CCA: 178 | CAA: 258 | CGA: 33 | A\n| CTG: 111 | CCG: 56 | CAG: 120 | CGG: 20 | G\n---+----------+----------+----------+----------+---\n| ATT: 298 | ACT: 198 | AAT: 356 | AGT: 147 | T\nA | ATC: 169 | ACC: 124 | AAC: 241 | AGC: 103 | C\n| ATA: 188 | ACA: 181 | AAA: 418 | AGA: 203 | A\n| ATG: 213 | ACG: 84 | AAG: 291 | AGG: 94 | G\n---+----------+----------+----------+----------+---\n| GTT: 213 | GCT: 195 | GAT: 365 | GGT: 217 | T\nG | GTC: 112 | GCC: 118 | GAC: 196 | GGC: 97 | C\n| GTA: 128 | GCA: 165 | GAA: 439 | GGA: 114 | A\n| GTG: 112 | GCG: 63 | GAG: 191 | GGG: 61 | G\n\nPAM matrices\n- PAM = Point Accepted mutation\nA R N D C Q E G\nH I L K M F P S T W Y V B\nZ X *\nA 2 -2 0 0 -2 -1 0 1 -2 -1 -2 -2 -1 -3 1 1 1 -5 -3 0 0 0 0 -7\nR -2 6 -1 -2 -3 1 -2 -3 1 -2 -3 3 -1 -4 -1 -1 -1 1 -4 -3 -1 0 -1 -7\nN 0 -1 3 2 -4 0 1 0 2 -2 -3 1 -2 -3 -1 1 0 -4 -2 -2 2 1 0 -7\nD 0 -2 2 4 -5 1 3 0 0 -3 -4 0 -3 -6 -2 0 -1 -6 -4 -3 3 2 -1 -7\nC -2 -3 -4 -5 9 -5 -5 -3 -3 -2 -6 -5 -5 -5 -3 0 -2 -7 0 -2 -4 -5 -3 -7\nQ -1 1 0 1 -5 5 2 -2 2 -2 -2 0 -1 -5 0 -1 -1 -5 -4 -2 1 3 -1 -7\nE 0 -2 1 3 -5 2 4 0 0 -2 -3 -1 -2 -5 -1 0 -1 -7 -4 -2 2 3 -1 -7\nG 1 -3 0 0 -3 -2 0 4 -3 -3 -4 -2 -3 -4 -1 1 -1 -7 -5 -2 0 -1 -1 -7\nH -2 1 2 0 -3 2 0 -3 6 -3 -2 -1 -3 -2 -1 -1 -2 -3 0 -2 1 1 -1 -7\nI -1 -2 -2 -3 -2 -2 -2 -3 -3 5 2 -2 2 0 -2 -2 0 -5 -2 3 -2 -2 -1 -7\nL -2 -3 -3 -4 -6 -2 -3 -4 -2 2 5 -3 3 1 -3 -3 -2 -2 -2 1 -4 -3 -2 -7\nK -2 3 1 0 -5 0 -1 -2 -1 -2 -3 4 0 -5 -2 -1 0 -4 -4 -3 0 0 -1 -7\nM -1 -1 -2 -3 -5 -1 -2 -3 -3 2 3 0 7 0 -2 -2 -1 -4 -3 1 -3 -2 -1 -7\nF -3 -4 -3 -6 -5 -5 -5 -4 -2 0 1 -5 0 7 -4 -3 -3 -1 5 -2 -4 -5 -3 -7\nP 1 -1 -1 -2 -3 0 -1 -1 -1 -2 -3 -2 -2 -4 5 1 0 -5 -5 -2 -1 -1 -1 -7\nS 1 -1 1 0 0 -1 0 1 -1 -2 -3 -1 -2 -3 1 2 1 -2 -3 -1 0 -1 0 -7\nT 1 -1 0 -1 -2 -1 -1 -1 -2 0 -2 0 -1 -3 0 1 3 -5 -3 0 0 -1 0 -7\nW -5 1 -4 -6 -7 -5 -7 -7 -3 -5 -2 -4 -4 -1 -5 -2 -5 12 -1 -6 -5 -6 -4 -7\nY -3 -4 -2 -4 0 -4 -4 -5 0 -2 -2 -4 -3 5 -5 -3 -3 -1 8 -3 -3 -4 -3 -7\nV 0 -3 -2 -3 -2 -2 -2 -2 -2 3 1 -3 1 -2 -2 -1 0 -6 -3 4 -2 -2 -1 -7\nB 0 -1 2 3 -4 1 2 0 1 -2 -4 0 -3 -4 -1 0 0 -5 -3 -2 3 2 -1 -7\nZ 0 0 1 2 -5 3 3 -1 1 -2 -3 0 -2 -5 -1 -1 -1 -6 -4 -2 2 3 -1 -7\nX 0 -1 0 -1 -3 -1 -1 -1 -1 -1 -2 -1 -1 -3 -1 0 0 -4 -3 -1 -1 -1 -1 -7\n* -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 1\n\nBLOSUM matrices\n- BloSum = BLOck SUbstritution matrices\nA R N D C Q E G\nH I L K M F P S T W Y V B\nZ X *\nA 4 -1 -2 -2 0 -1 -1 0 -2 -1 -1 -1 -1 -2 -1 1 0 -3 -2 0 -2 -1 0 -4\nR -1 5 0 -2 -3 1 0 -2 0 -3 -2 2 -1 -3 -2 -1 -1 -3 -2 -3 -1 0 -1 -4\nN -2 0 6 1 -3 0 0 0 1 -3 -3 0 -2 -3 -2 1 0 -4 -2 -3 3 0 -1 -4\nD -2 -2 1 6 -3 0 2 -1 -1 -3 -4 -1 -3 -3 -1 0 -1 -4 -3 -3 4 1 -1 -4\nC 0 -3 -3 -3 9 -3 -4 -3 -3 -1 -1 -3 -1 -2 -3 -1 -1 -2 -2 -1 -3 -3 -2 -4\nQ -1 1 0 0 -3 5 2 -2 0 -3 -2 1 0 -3 -1 0 -1 -2 -1 -2 0 3 -1 -4\nE -1 0 0 2 -4 2 5 -2 0 -3 -3 1 -2 -3 -1 0 -1 -3 -2 -2 1 4 -1 -4\nG 0 -2 0 -1 -3 -2 -2 6 -2 -4 -4 -2 -3 -3 -2 0 -2 -2 -3 -3 -1 -2 -1 -4\nH -2 0 1 -1 -3 0 0 -2 8 -3 -3 -1 -2 -1 -2 -1 -2 -2 2 -3 0 0 -1 -4\nI -1 -3 -3 -3 -1 -3 -3 -4 -3 4 2 -3 1 0 -3 -2 -1 -3 -1 3 -3 -3 -1 -4\nL -1 -2 -3 -4 -1 -2 -3 -4 -3 2 4 -2 2 0 -3 -2 -1 -2 -1 1 -4 -3 -1 -4\nK -1 2 0 -1 -3 1 1 -2 -1 -3 -2 5 -1 -3 -1 0 -1 -3 -2 -2 0 1 -1 -4\nM -1 -1 -2 -3 -1 0 -2 -3 -2 1 2 -1 5 0 -2 -1 -1 -1 -1 1 -3 -1 -1 -4\nF -2 -3 -3 -3 -2 -3 -3 -3 -1 0 0 -3 0 6 -4 -2 -2 1 3 -1 -3 -3 -1 -4\nP -1 -2 -2 -1 -3 -1 -1 -2 -2 -3 -3 -1 -2 -4 7 -1 -1 -4 -3 -2 -2 -1 -2 -4\nS 1 -1 1 0 -1 0 0 0 -1 -2 -2 0 -1 -2 -1 4 1 -3 -2 -2 0 0 0 -4\nT 0 -1 0 -1 -1 -1 -1 -2 -2 -1 -1 -1 -1 -2 -1 1 5 -2 -2 0 -1 -1 0 -4\nW -3 -3 -4 -4 -2 -2 -3 -2 -2 -3 -2 -3 -1 1 -4 -3 -2 11 2 -3 -4 -3 -2 -4\nY -2 -2 -2 -3 -2 -1 -2 -3 2 -1 -1 -2 -1 3 -3 -2 -2 2 7 -1 -3 -2 -1 -4\nV 0 -3 -3 -3 -1 -2 -2 -3 -3 3 1 -2 1 -1 -2 -2 0 -3 -1 4 -3 -2 -1 -4\nB -2 -1 3 4 -3 0 1 -1 0 -3 -4 0 -3 -3 -2 0 -1 -4 -3 -3 4 1 -1 -4\nZ -1 0 0 1 -3 3 4 -2 0 -3 -3 1 -1 -3 -1 0 -1 -3 -2 -2 1 4 -1 -4\nX 0 -1 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2 0 0 -2 -1 -1 -1 -1 -1 -4\n* -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 1\n\nComputing Substitution Matrices\n- Take a list of 1000 aligned proteins\n- Every time you see a substitution between two amino acids,\nincrement the similarity score between them.\n- Must normalize it by how often amino acids occur in general.\nRare amino acids will give rare substitutions.\n- BLOSUM matrices vs. PAM\n- BLOSUM were built only from the most conserved domains of\nthe blocks database of conserved proteins.\n- BLOSUM: more tolerant of hydrophobic changes and of\ncysteine and tryptophan mismatches\n- PAM: more tolerant of substitutions to or from hydrophilic amino\nacids.\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy does this work\nApplications\n\nOverview: Why this works\nQuery: RKIWGDPRS\n- In worst case:\nDatab.: RKIVGDRRS\n- W-mer: W=3\n7 identical a.a\n- Combs/random projection\n- In average case\n- Simulations\n- Biological case: counting W-mers in real alignments\n- Long conserved W-mers do happen in actual\nalignments\n- There's something biological about long W-mers\n\nPigeonhole principle\n- Pigeonhole principle\n- If you have 2 pigeons and 3 holes, there must be at\nleast one hole with no pigeon\n\nPigeonhole and W-mers\n- Pigeonholing mis-matches\n- Two sequences, each 9 amino-acids, with 7 identities\n- There is a stretch of 3 amino-acids perfectly conserved\nIn general:\nSequence length: n\nIdentities: t\nCan use W-mers for W= [n/(n-t+1)]\n\nCombs and Random Pojections\n- Assume we select k positions,\nwhich do not contain *, at\nrandom with replacement\n- What is the probability we miss\na sequence match ?\n- At most: 1-idperck\n- In our case: 1-(7/9)4 =0.63...\n- What if we repeat the process l\ntimes, independently ?\n- Miss prob. = 0.63l\n- For l=5, it is less than 10%\nQuery: RKIWGDPRS\nDatab: RKIVGDRRS\nk=4\nQuery: *KI*G***S\nDatab.: *KI*G***S\n\nTrue alignments: Looking for K-mers\nPersonal experiment run in 2000.\n- 850Kb region of human, and mouse 450Kb ortholog.\n- Blasted every piece of mouse against human (6,50)\n- Identify slope of best fit line\n\nConclusions\n- Table lookup - very powerful technique\n- Deterministic, randomized\n- More (on hashing) in 6.046\n\nExtending pigeonhole principle\n- Pigeonhole principle\n- If you have 21 pigeons and only 10 holes, there must be at\nleast one hole with more than two pigeons.\nProof by contradiction\nAssume each hole has <= 2 pigeon. 10 holes together must have <= 10*2\npigeons, hence <=20. We have 21.\n\nRandom model: Average case\n- In random model, things work better for us\nIn entirely random model, mismatches will often fall near each other,\nmaking a longer conserved k-mer more likely\nmismatches also fall near each other but that doesn't hurt\nBirthday paradox: if we have 32 birthdays and 365 days they could\nfall in, 2 of them will coincide with P=.753\nSimilarly, counting random occurrences yields the following\n\nRandom Model: Counting\n100 positions\nn identities\nk must be contiguous\nk\nPick a start position\nArrange the n-k remaining identities\nin 100-k-2 positions\nTry this equation out, and get k-mers more often\n)!\n§ (c)\n(\nk\n·\n( k\n(\n)!\n\n)!\nn\nn\n(\nk )\n§ (c)\n!\n·\nIncreasing k-mer size\n1.00\n1.01\n0.69\n)!\nn \\ k\n\n80%\nIncreasing\n70%\npercent id\n60%\nn 100\n(\n!\nn\n0.80\n0.69\n0.40\n0.63\n0.47\n0.23\n0.50\n0.32\n0.13\n\n0.39 0.31\n0.22 0.14\n0.07 0.04\n\nRandom Model: simulation\nPossible arrangements\nPossible starts of\nof the remaining 100-k\n60% identity\nP(finding island at that length)\n80% identity\n·\n(c)\n§\n\n·\n(c)\n§\n\n)!\n(\nn\nn\nn\nk\nn\nk\nk\nthe conserved k-mer\n3⁄4\n1/8\n1/4\n(\n!\n!\n)!\n(\n)!\n)!\n(\n)\n(\n3 5\n10-mer\nIsland length\n\nRandom Model: simulation\nConservation 60% over 1000 bp\n2 species\n|\nL = 6\n|\nL =\n|\nL =\n|\nL = 9\n|\nL = 10\n|\nL = 11\n|\nL = 12\n|\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 84.966+/-10.234| 75.458+/-10.399| 66.440+/-10.590| 83.582+/-10.434| 74.822+/-10.463| 66.484+/-10.514| 81.592+/-10.438|\n80| 53.170+/-10.459| 42.060+/-10.469| 32.032+/-10.427| 26.432+/-10.419| 47.300+/-10.523| 37.084+/-10.581| 31.248+/-10.567|\n90| 16.598+/-10.358| 10.424+/-10.295| 6.870+/-10.254| 4.594+/-10.197| 17.754+/-10.394| 13.326+/-10.369| 9.736+/-10.330|\n95| 14.868+/-10.318| 10.896+/-10.334| 7.578+/-10.201| 4.740+/-10.207| 3.842+/-10.203| 1.854+/-10.157| 1.280+/-10.122|\n100| 15.386+/-10.239| 11.078+/-10.297| 7.838+/-10.228| 5.114+/-10.198| 3.412+/-10.186| 2.094+/-10.176| 1.422+/-10.157|\n3 species\n|\nL = 6\n|\nL =\n|\nL =\n|\nL = 9\n|\nL = 10\n|\nL = 11\n|\nL = 12\n|\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 53.804+/-10.398| 39.560+/-10.503| 27.238+/-10.429| 45.112+/-10.474| 31.856+/-10.475| 22.966+/-10.568| 37.318+/-10.496|\n80| 21.618+/-10.341| 12.790+/-10.278| 7.416+/-10.250| 4.918+/-10.249| 12.488+/-10.425| 8.212+/-10.346| 4.544+/-10.214|\n90| 4.000+/-10.152| 1.844+/-10.096| 0.978+/-10.111| 0.426+/-10.056| 2.254+/-10.154| 1.272+/-10.143| 0.946+/-10.105|\n95| 3.436+/-10.147| 1.934+/-10.121| 0.840+/-10.093| 0.664+/-10.081| 0.232+/-10.044| 0.044+/-10.022| 0.048+/-10.024|\n100| 3.360+/-10.146| 2.288+/-10.129| 0.746+/-10.081| 0.618+/-10.073| 0.124+/-10.034| 0.070+/-10.028| 0.050+/-10.025|\n4 species\n|\nL = 6\n|\nL =\n|\nL =\n|\nL = 9\n|\nL = 10\n|\nL = 11\n|\nL = 12\n|\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 29.614+/-10.379| 17.102+/-10.326| 9.492+/-10.262| 18.508+/-10.378| 10.376+/-10.264| 5.502+/-10.239| 11.190+/-10.346|\n80| 8.738+/-10.193| 3.536+/-10.167| 1.636+/-10.117| 0.934+/-10.095| 2.128+/-10.126| 1.250+/-10.111| 0.554+/-10.099|\n90| 0.820+/-10.066| 0.396+/-10.055| 0.138+/-10.041| 0.026+/-10.018| 0.128+/-10.035| 0.198+/-10.053| 0.026+/-10.018|\n95| 0.740+/-10.073| 0.394+/-10.049| 0.136+/-10.032| 0.028+/-10.020| 0.040+/-10.020| 0.000+/-10.000| 0.000+/-10.000|\n100| 0.794+/-10.070| 0.476+/-10.051| 0.184+/-10.046| 0.074+/-10.025| 0.020+/-10.014| 0.044+/-10.022| 0.000+/-10.000|\n5 species\n|\nL = 6\n|\nL =\n|\nL =\n|\nL = 9\n|\nL = 10\n|\nL = 11\n|\nL = 12\n|\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 15.144+/-10.290| 6.784+/-10.259| 2.934+/-10.182| 5.782+/-10.262| 2.802+/-10.167| 1.314+/-10.129| 2.354+/-10.160|\n80| 2.650+/-10.129| 0.902+/-10.086| 0.170+/-10.034| 0.248+/-10.046| 0.384+/-10.068| 0.164+/-10.041| 0.000+/-10.000|\n90| 0.250+/-10.044| 0.058+/-10.020| 0.032+/-10.016| 0.000+/-10.000| 0.024+/-10.017| 0.000+/-10.000| 0.000+/-10.000|\n95| 0.148+/-10.027| 0.018+/-10.013| 0.016+/-10.011| 0.000+/-10.000| 0.000+/-10.000| 0.000+/-10.000| 0.000+/-10.000|\n100| 0.244+/-10.038| 0.100+/-10.025| 0.034+/-10.017| 0.018+/-10.013| 0.000+/-10.000| 0.022+/-10.016| 0.000+/-10.000|\n\nTrue alignments: Looking for K-mers\nnumber of k-mers that happen for each length of k-mer.\nBlue islands come from off-diagonal alignments\nNote: more than one data point per alignment.\nLog Log plot\nLinear plot\nRed islands come from colinear alignments\n\nSummary: Why k-mers work\n- In worst case: Pigeonhole principle\n- Have too many matches to place on your sequence length\n- Bound to place at least k matches consecutively\n- In average case: Birthday paradox / Simulations\n- Matches tend to cluster in the same bin. Mismatches too.\n- Looking for stretches of consecutive matches is feasible\n- Biological case: Counting k-mers in real alignments\n- From the number of conserved k-mers alone, one can\ndistinguish genuine alignments from chance alignments\n- Something biologically meaningful can be directly carried over\nto the algorithm.\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nIdentifying exons\n- Direct application of BLAST\n- Compare Tetraodon to Human using BLAST\n- Best alignments happen only on exons\n- Translate a biological property into an alignment\nproperty\n- Exon = high alignment\n- Reversing this equivalence, look for high alignments\nand predict exons\n- Estimate human gene number\n- Method is not reliable for complete annotation, and\ndoes not find all genes, or even all exons in a gene\n- Can be used however, to estimate human gene\nnumber\n\nPart I -\nP arameter tuning\n-\nTry a lot of parameters and find combination with\n- fastest running time\n- highest specificity\n- highest sensitivity\nMatrix\nX\nMethod\nW\nL\nT(s)\nBLASTN\nBLASTN\nBLASTN\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nNUC.4.4\nNUC.4.4\nNUC.4.4\nBLOSUM62\nBLOSUM62\nBLOSUM62\nCNS\nCNS\nCNS\n4.8\n5.7\n4.3\n74.8\n1,065.2\n1,160.9\n10.0\n29.4\n29.3\nP e r f o r m a n c e o f\nB L A S T C o n f i g u r a t i o n s\nI (%)\nSn (%)\nSp (%)\n8 bases\n8 bases\n10 bases\n3 aa\n4 aa\n5 aa\n4 aa\n5 aa\n5 aa\n30 bases\n40 bases\n30 bases\n13 aa\n13 aa\n13 aa\n13 aa\n13 aa\n13 aa\nD i f f e r e n t\nEach program was run with 1,340 different conditions and a representative selection of results is shown. A range of values for W\n(initial size of the search word) and X (threshold score for consecutive mismatching residues or bases) were tested. For amino acid\nalignments, a non-substitutive matrix (CNS, match = +15, mismatch = -12) was tested as well as the standard BLOSOM62 matrix. A\nminimal length (L) and percentage identity (I) were applied to select alignments for which a sensitivity (Sn) and specificity (Sp) were\ncalculated in terms of numbers of overall matching exons. T indicates the time in seconds needed to compare the 13 homologues\nagainst each other. The last row shows the optimal performance that was retained for Exofish.\nFigure by MIT OCW.\n\nPart II - choosing a threshold\n- For best parameters\n- Find threshold by\nobserving alignments\n- Anything higher than\nthreshold will be\ntreated as a predicted\nexon\n\nPart III - Gene identification\n- Matches correspond to exons\n- Not all genes hit\n- A fish doesn't need or have all functions present in human\n- Even those common are sometimes not perfectly conserved\n- Not all exons in each gene are hit\n- On average, three hits per gene. Three exons found.\n- Only most needed domains of a protein will be best\nconserved\n- All hits correspond to genuine exons\n- Specificity is 100% although sensitivity not guaranteed\nImage removed due to copyright restrictions.\nPlease see: Crollius, Hugues R., Olivier Jaillon, Alain Bernot, Corinne Dasilva, et al. \"Estimate of human gene number provided by genome-wide\nanalysis using Tetraodon nigroviridis DNA sequence.\" Nature Genetics 25(2000): 235-238. Figure by MIT OCW\n\nEstimating human gene number\n- Extrapolate experimental results\n- Incomplete coverage\n- Model how number would increase with increasing\ncoverage\n- Not perfect sensitivity\n- Estimate how many we're missing on well-annotated\nsequence\n- Assume ratio is uniform\n- Estimate gene number\nImage removed due to copyright restrictions.\nPlease see: Crollius, Hugues R., Olivier Jaillon, Alain Bernot, Corinne Dasilva, et al. \"Estimate of human gene number provided by genome-wide\nanalysis using Tetraodon nigroviridis DNA sequence.\" Nature Genetics 25(2000): 235-238. Figure by MIT OCW\n\nGene content by Chromosome\n-\nG ene density varies throughout human genome\n-ExoFish predicted density corresponds to GeneMap annotation density\n0.5\nExofish\n7 8 9\nX Y\n1.5\n2.5\nGenemap\nHuman Chromosomes\nRelative Gene or Ecore Density\n10 11 12 13 14 15 16 17 18 19 20 21 22\nAdapted from: Crollius, Hugues R., Olivier Jaillon, Alain Bernot, Corinne Dasilva, et al. \"Estimate of human gene number provided by genome-wide\nanalysis using Tetraodon nigroviridis DNA sequence.\" Nature Genetics 25(2000): 235-238. Figure by MIT OCW\n\nWhat is hashing\n- Content-based indexing\n- Instead of referencing elements by index\n- Reference elements by the elements themselves,\n- by their content\n- A hash function\n- Transforms an object into a pointer to an array\n- All objects will map in a flat distribution on array\nspace\n- Otherwise, some entries get too crowded\n- What about a database\n- List every location where a particular n-mer occurs\n- Retrieve in constant time all the places where you\ncan find it\n\nBreaking up the query\nquery word (W = 3)\nQuery: GSVEDTTGSQSLAALLNKCKTPQGQRLVNQWIKQPLMDKNRIEERLNLVEAFVEDAELRQTLQEDL\n- List them all\n- every word in the query\n- overlapping w-mers\n\nGenerating the neighborhood\n- Enumerate\n- For every amino acid in the\nword, try all possibilities\n- Score each triplet obtained\n- Only keep those within\nyour threshold\n\nLooking into database\nPMG\n- Follow Pointers\n- Each neighborhood word gives\nus a list of all positions in the\ndatabase where it's found\nHashed Database\n\nLength and Percent Identity"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/443a6b7a84e47e24c37be28763b936ff_lecture2.pdf",
      "content": "6.096\nAlgorithms for Computational Biology\nLecture 2\nBLAST & Database Search\nManolis Kellis\nPiotr Indyk\n\nIn Previous Lecture\nDNA\n1 Gene Finding\nDatabase lookup\nSequence alignment\n\nBLAST and Database Search\nSetup\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nSetup\n- Sequences of symbols:\n- Bases: A,G,T,C\n- Amino-acids (a.a.):\nA,R,N,D,C,Q,E,G,H,I,L,K,M,F,P,S,T,W,\nY,V,B,Y,X\n- Database search:\n- Database.\n- Query:\n- Output: sequences similar to query\nAIKWQPRSTW....\nIKMQRHIKW....\nHDLFWHLWH....\n........................\nRGIKW\n\nWhat does \"similar\" mean ?\n- Simplest idea: just count the number of common\namino-acids\n- E.g., RGRKW matches RGIKW with idperc = 80%\n- Not all matches are created equal - scoring matrix\n- In general, insertions and deletions can also\nhappen\n\nHow to answer the query\n- We could just scan the whole database\n- But:\n- Query must be very fast\n- Most sequences will be completely unrelated to\nquery\n- Individual alignment needs not be perfect. Can fine-\ntune\n- Exploit nature of the problem\n- If you're going to reject any match with idperc <\n90%, then why bother even looking at sequences\nwhich don't have a fairly long stretch of matching\na.a. in a row.\n- Pre-screen sequences for common long stretches,\nand reject vast majority of them\n\nW-mer indexing\n-\nW-mer: a string of length W\n- Preprocessing: For every W-mer (e.g.,\nW=3), list every location in the database\nwhere it occurs\n- Query:\n- Generate W-mers and look them up in\nthe database.\n- Process the results\n-\nBenefit:\n- For W=3, roughly one W-mer in 233 will\nmatch, i.e., one in a ten thousand\nAIKWQPRSTW....\nIKMQRHIKW....\nHDLFWHLWH....\n........................\nRGIKW\n......\nIKW\nIKZ\n......\n......\nIKW\n......\n\n6.046 Digression\n- This \"lookup\" technique is quite fundamental\n- Will see more in 6.046, lecture 7, on hashing\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nBLAST\n- Specific (and very efficient) implementation of the\nW-mer indexing idea\n- How to generate W-mers from the query\n- How to process the matches\n\nQuery:\nQuery word (W =3)\nGSVEDTTGSQSLAALLNKCKTPQGQRLVNQWIKQPLMDKNRIEERLNLVEAFVEDAELRQTLQEDL\nPQG\nPEG\nPRG\nPKG\nPNG\nNeighborhood words\nNeighborhood score threshold\n(T=13)\nPDG\nPHG\nPMG\nPSQ\nPQA\nPQN\nSLAALLNKCKTPQGQRLVNQWIKQPLMDKNRIEERLNLVEA\nTLASVLDCTVTPMGSRMLKRWLHMPVRDTRVLLERQQTIGA\nHigh-scoring Segment Pair (HSP)\n+LA++L+\nTP G R++\n+W+\nP+\nD\n+ ER\n+ A\netc...\nX\nTHE BLAST SEARCH ALGORITHM\nQuery:\nSbjct:\nFigure by MIT OCW.\n\nBlast Algorithm Overview\n- Receive query\n- Split query into overlapping words of length W\n- Find neighborhood words for each word until\nthreshold T\n- Look into the table where these neighbor words\noccur: seeds\n- Extend seeds until score drops off under X\n- Evaluate statistical significance of score\n- Report scores and alignments\nPMG\nW-mer Database\n\nExtending the seeds\n-\nExtend until the cumulative score drops\nCumulative Score\nExtension\nBreak into two HSPs\nFigure by MIT OCW.\nSLAALLNKCKTPQGQRLVNQWIKQPLMDKNRIEERLNLVEA\nTLASVLDCTVTPMGSRMLKRWLHMPVRDTRVLLERQQTIGA\nHigh-scoring Segment Pair (HSP)\n+LA++L+\nTP G R++\n+W+\nP+\nD\n+ ER\n+ A\nQuery:\nSbjct:\n\nStatistical Significance\n-\nKarlin-Altschul statistics\n- P-value: Probability that\nthe HSP was generated as\na chance alignment.\n- Score: -log of the\nprobability\n- E: expected number of\nsuch alignments given\ndatabase\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nExtensions: Filtering\n- Low complexity regions can cause spurious hits\n- Filter out low complexity in your query\n- Filter most over-represented items in your database\n\nExtensions: Two-hit blast\n- Improves sensitivity for any speed\n- Two smaller W-mers are more likely than one longer one\n- Therefore it's a more sensitive searching method to look\nfor two hits instead of one, with the same speed.\n- Improves speed for any sensitivity\n- No need to extend a lot of the W-mers, when isolated\n\nExtensions: beyond W-mers\n- W-mers (without neighborhoods):\nRGIKW RGI , GIK, IKW\n- No reason to use only consecutive symbols\n- Instead, we could use combs, e.g.,\nRGIKW R*IK* , RG**W, ...\n- Indexing same as for W-mers:\n- For each comb, store the list of positions in the\ndatabase where it occurs\n- Perform lookups to answer the query\n- Randomized projection: Buhler'01, based on Indyk-Motwani'98\n- Choose the positions of * at random\n- Example of a randomized algorithm\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\nImage removed due to\ncopyright restrictions.\n\nSubstitution Matrices\n- Not all amino acids are created equal\n- Some are more easily substituted than others\n- Some mutations occur more often\n- Some substitutions are kept more often\n- Mutations tend to favor some substitutions\n- Some amino acids have similar codons\n- They are more likely to be changed from DNA\nmutation\n- Selection tends to favor some substitutions\n- Some amino acids have similar properties / structure\n- They are more likely to be kept when randomly\nchanged\n- The two forces together yield substitution matrices\n\nAmino Acids\n| T | C | A | G |\n---+----------+----------+----------+----------+---\n| TTT: PheF| TCT: Ser | TAT: Tyr | TGT: Cys | T\nT | TTC: Phe | TCC: Ser | TAC: Tyr | TGC: Cys | C\n| TTA: Leu | TCA: Ser | TAA: * | TGA: * | A\n| TTG: Leu | TCG: Ser | TAG: * | TGG: TrpW| G\n---+----------+----------+----------+----------+---\n| CTT: Leu | CCT: Pro | CAT: His | CGT: Arg | T\nC | CTC: Leu | CCC: Pro | CAC: His | CGC: Arg | C\n| CTA: Leu | CCA: Pro | CAA: GlnQ| CGA: Arg | A\n| CTG: Leu | CCG: Pro | CAG: Gln | CGG: Arg | G\n---+----------+----------+----------+----------+---\n| ATT: Ile | ACT: Thr | AAT: Asn | AGT: Ser | T\nA | ATC: Ile | ACC: Thr | AAC: Asn | AGC: Ser | C\n| ATA: Ile | ACA: Thr | AAA: LysK| AGA: Arg | A\n| ATG: Met | ACG: Thr | AAG: Lys | AGG: Arg | G\n---+----------+----------+----------+----------+---\n| GTT: Val | GCT: Ala | GAT: AspD| GGT: Gly | T\nG | GTC: Val | GCC: Ala | GAC: Asp | GGC: Gly | C\n| GTA: Val | GCA: Ala | GAA: GluE| GGA: Gly | A\n| GTG: Val | GCG: Ala | GAG: Glu | GGG: Gly | G\n| T | C | A | G |\n---+----------+----------+----------+----------+---\n| TTT: 273 | TCT: 238 | TAT: 186 | TGT: 85 | T\nT | TTC: 187 | TCC: 144 | TAC: 140 | TGC: 53 | C\n| TTA: 263 | TCA: 200 | TAA: 10 | TGA: 8 | A\n| TTG: 266 | TCG: 92 | TAG: 5 | TGG: 105 | G\n---+----------+----------+----------+----------+---\n| CTT: 134 | CCT: 134 | CAT: 137 | CGT: 62 | T\nC | CTC: 61 | CCC: 69 | CAC: 76 | CGC: 28 | C\n| CTA: 139 | CCA: 178 | CAA: 258 | CGA: 33 | A\n| CTG: 111 | CCG: 56 | CAG: 120 | CGG: 20 | G\n---+----------+----------+----------+----------+---\n| ATT: 298 | ACT: 198 | AAT: 356 | AGT: 147 | T\nA | ATC: 169 | ACC: 124 | AAC: 241 | AGC: 103 | C\n| ATA: 188 | ACA: 181 | AAA: 418 | AGA: 203 | A\n| ATG: 213 | ACG: 84 | AAG: 291 | AGG: 94 | G\n---+----------+----------+----------+----------+---\n| GTT: 213 | GCT: 195 | GAT: 365 | GGT: 217 | T\nG | GTC: 112 | GCC: 118 | GAC: 196 | GGC: 97 | C\n| GTA: 128 | GCA: 165 | GAA: 439 | GGA: 114 | A\n| GTG: 112 | GCG: 63 | GAG: 191 | GGG: 61 | G\n\nPAM matrices\n- PAM = Point Accepted mutation\nA R N D C Q E G H I L K M F P S T W Y V B\nZ X *\nA 2 -2 0 0 -2 -1 0 1 -2 -1 -2 -2 -1 -3 1 1 1 -5 -3 0 0 0 0 -7\nR -2 6 -1 -2 -3 1 -2 -3 1 -2 -3 3 -1 -4 -1 -1 -1 1 -4 -3 -1 0 -1 -7\nN 0 -1 3\n2 -4 0 1 0 2 -2 -3 1 -2 -3 -1 1 0 -4 -2 -2 2 1 0 -7\nD 0 -2 2 4 -5 1 3 0 0 -3 -4 0 -3 -6 -2 0 -1 -6 -4 -3 3 2 -1 -7\nC -2 -3 -4 -5 9 -5 -5 -3 -3 -2 -6 -5 -5 -5 -3 0 -2 -7 0 -2 -4 -5 -3 -7\nQ -1 1 0 1 -5 5\n2 -2 2 -2 -2 0 -1 -5 0 -1 -1 -5 -4 -2 1 3 -1 -7\nE 0 -2 1 3 -5 2 4 0 0 -2 -3 -1 -2 -5 -1 0 -1 -7 -4 -2 2 3 -1 -7\nG 1 -3 0 0 -3 -2 0 4 -3 -3 -4 -2 -3 -4 -1 1 -1 -7 -5 -2 0 -1 -1 -7\nH -2 1 2 0 -3 2 0 -3 6 -3 -2 -1 -3 -2 -1 -1 -2 -3 0 -2 1 1 -1 -7\nI -1 -2 -2 -3 -2 -2 -2 -3 -3 5\n2 -2 2 0 -2 -2 0 -5 -2 3 -2 -2 -1 -7\nL -2 -3 -3 -4 -6 -2 -3 -4 -2 2 5 -3 3 1 -3 -3 -2 -2 -2 1 -4 -3 -2 -7\nK -2 3 1 0 -5 0 -1 -2 -1 -2 -3 4\n0 -5 -2 -1 0 -4 -4 -3 0 0 -1 -7\nM -1 -1 -2 -3 -5 -1 -2 -3 -3 2 3 0 7\n0 -2 -2 -1 -4 -3 1 -3 -2 -1 -7\nF -3 -4 -3 -6 -5 -5 -5 -4 -2 0 1 -5 0 7 -4 -3 -3 -1 5 -2 -4 -5 -3 -7\nP 1 -1 -1 -2 -3 0 -1 -1 -1 -2 -3 -2 -2 -4 5\n1 0 -5 -5 -2 -1 -1 -1 -7\nS 1 -1 1 0 0 -1 0 1 -1 -2 -3 -1 -2 -3 1 2\n1 -2 -3 -1 0 -1 0 -7\nT 1 -1 0 -1 -2 -1 -1 -1 -2 0 -2 0 -1 -3 0 1 3 -5 -3 0 0 -1 0 -7\nW -5 1 -4 -6 -7 -5 -7 -7 -3 -5 -2 -4 -4 -1 -5 -2 -5 12 -1 -6 -5 -6 -4 -7\nY -3 -4 -2 -4 0 -4 -4 -5 0 -2 -2 -4 -3 5 -5 -3 -3 -1 8 -3 -3 -4 -3 -7\nV 0 -3 -2 -3 -2 -2 -2 -2 -2 3 1 -3 1 -2 -2 -1 0 -6 -3 4 -2 -2 -1 -7\nB 0 -1 2 3 -4 1 2 0 1 -2 -4 0 -3 -4 -1 0 0 -5 -3 -2 3\n2 -1 -7\nZ 0 0 1 2 -5 3 3 -1 1 -2 -3 0 -2 -5 -1 -1 -1 -6 -4 -2 2 3 -1 -7\nX 0 -1 0 -1 -3 -1 -1 -1 -1 -1 -2 -1 -1 -3 -1 0 0 -4 -3 -1 -1 -1 -1 -7\n* -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 1\n\nBLOSUM matrices\n- BloSum = BLOck SUbstritution matrices\nA R N D C Q E G H I L K M F P S T W Y V B\nZ X *\nA 4 -1 -2 -2 0 -1 -1 0 -2 -1 -1 -1 -1 -2 -1 1 0 -3 -2 0 -2 -1 0 -4\nR -1 5\n0 -2 -3 1 0 -2 0 -3 -2 2 -1 -3 -2 -1 -1 -3 -2 -3 -1 0 -1 -4\nN -2 0 6\n1 -3 0 0 0 1 -3 -3 0 -2 -3 -2 1 0 -4 -2 -3 3 0 -1 -4\nD -2 -2 1 6 -3 0 2 -1 -1 -3 -4 -1 -3 -3 -1 0 -1 -4 -3 -3 4 1 -1 -4\nC 0 -3 -3 -3 9 -3 -4 -3 -3 -1 -1 -3 -1 -2 -3 -1 -1 -2 -2 -1 -3 -3 -2 -4\nQ -1 1 0 0 -3 5\n2 -2 0 -3 -2 1 0 -3 -1 0 -1 -2 -1 -2 0 3 -1 -4\nE -1 0 0 2 -4 2 5 -2 0 -3 -3 1 -2 -3 -1 0 -1 -3 -2 -2 1 4 -1 -4\nG 0 -2 0 -1 -3 -2 -2 6 -2 -4 -4 -2 -3 -3 -2 0 -2 -2 -3 -3 -1 -2 -1 -4\nH -2 0 1 -1 -3 0 0 -2 8 -3 -3 -1 -2 -1 -2 -1 -2 -2 2 -3 0 0 -1 -4\nI -1 -3 -3 -3 -1 -3 -3 -4 -3 4\n2 -3 1 0 -3 -2 -1 -3 -1 3 -3 -3 -1 -4\nL -1 -2 -3 -4 -1 -2 -3 -4 -3 2 4 -2 2 0 -3 -2 -1 -2 -1 1 -4 -3 -1 -4\nK -1 2 0 -1 -3 1 1 -2 -1 -3 -2 5 -1 -3 -1 0 -1 -3 -2 -2 0 1 -1 -4\nM -1 -1 -2 -3 -1 0 -2 -3 -2 1 2 -1 5\n0 -2 -1 -1 -1 -1 1 -3 -1 -1 -4\nF -2 -3 -3 -3 -2 -3 -3 -3 -1 0 0 -3 0 6 -4 -2 -2 1 3 -1 -3 -3 -1 -4\nP -1 -2 -2 -1 -3 -1 -1 -2 -2 -3 -3 -1 -2 -4 7 -1 -1 -4 -3 -2 -2 -1 -2 -4\nS 1 -1 1 0 -1 0 0 0 -1 -2 -2 0 -1 -2 -1 4\n1 -3 -2 -2 0 0 0 -4\nT 0 -1 0 -1 -1 -1 -1 -2 -2 -1 -1 -1 -1 -2 -1 1 5 -2 -2 0 -1 -1 0 -4\nW -3 -3 -4 -4 -2 -2 -3 -2 -2 -3 -2 -3 -1 1 -4 -3 -2 11 2 -3 -4 -3 -2 -4\nY -2 -2 -2 -3 -2 -1 -2 -3 2 -1 -1 -2 -1 3 -3 -2 -2 2 7 -1 -3 -2 -1 -4\nV 0 -3 -3 -3 -1 -2 -2 -3 -3 3 1 -2 1 -1 -2 -2 0 -3 -1 4 -3 -2 -1 -4\nB -2 -1 3 4 -3 0 1 -1 0 -3 -4 0 -3 -3 -2 0 -1 -4 -3 -3 4\n1 -1 -4\nZ -1 0 0 1 -3 3 4 -2 0 -3 -3 1 -1 -3 -1 0 -1 -3 -2 -2 1 4 -1 -4\nX 0 -1 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2 0 0 -2 -1 -1 -1 -1 -1 -4\n* -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 1\n\nComputing Substitution Matrices\n-\nTake a list of 1000 aligned proteins\n- Every time you see a substitution between two amino acids,\nincrement the similarity score between them.\n- Must normalize it by how often amino acids occur in general.\nRare amino acids will give rare substitutions.\n-\nBLOSUM matrices vs. PAM\n- BLOSUM were built only from the most conserved domains of\nthe blocks database of conserved proteins.\n- BLOSUM: more tolerant of hydrophobic changes and of\ncysteine and tryptophan mismatches\n- PAM: more tolerant of substitutions to or from hydrophilic amino\nacids.\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy does this work\nApplications\n\nOverview: Why this works\n- In worst case:\n- W-mer: W=3\n- Combs/random projection\n- In average case\n- Simulations\n- Biological case: counting W-mers in real alignments\n- Long conserved W-mers do happen in actual\nalignments\n- There's something biological about long W-mers\nQuery: RKIWGDPRS\nDatab.: RKIVGDRRS\n7 identical a.a\n\nPigeonhole principle\n- Pigeonhole principle\n- If you have 2 pigeons and 3 holes, there must be at\nleast one hole with no pigeon\n\nPigeonhole and W-mers\n-\nPigeonholing mis-matches\n- Two sequences, each 9 amino-acids, with 7 identities\n- There is a stretch of 3 amino-acids perfectly conserved\nIn general:\nSequence length: n\nIdentities: t\nCan use W-mers for W= [n/(n-t+1)]\n\nCombs and Random Pojections\nQuery: RKIWGDPRS\nDatab: RKIVGDRRS\n-\nAssume we select k positions,\nwhich do not contain *, at\nrandom with replacement\n-\nWhat is the probability we miss\na sequence match ?\n- At most: 1-idperck\n- In our case: 1-(7/9)4 =0.63...\n-\nWhat if we repeat the process l\ntimes, independently ?\n- Miss prob. = 0.63l\n- For l=5, it is less than 10%\nk=4\nQuery: *KI*G***S\nDatab.: *KI*G***S\n\nTrue alignments: Looking for K-mers\nPersonal experiment run in 2000.\n- 850Kb region of human, and mouse 450Kb ortholog.\n- Blasted every piece of mouse against human (6,50)\n- Identify slope of best fit line\n\nConclusions\n- Table lookup - very powerful technique\n- Deterministic, randomized\n- More (on hashing) in 6.046\n\nExtending pigeonhole principle\n-\nPigeonhole principle\n- If you have 21 pigeons and only 10 holes, there must be at\nleast one hole with more than two pigeons.\nProof by contradiction\nAssume each hole has <= 2 pigeon. 10 holes together must have <= 10*2\npigeons, hence <=20. We have 21.\n\nRandom model: Average case\n-\nIn random model, things work better for us\nIn entirely random model, mismatches will often fall near each other,\nmaking a longer conserved k-mer more likely\nmismatches also fall near each other but that doesn't hurt\nBirthday paradox: if we have 32 birthdays and 365 days they could\nfall in, 2 of them will coincide with P=.753\nSimilarly, counting random occurrences yields the following\n\nPick a start position\nk\nArrange the n-k remaining identities\nin 100-k-2 positions\nRandom Model: Counting\nTry this equation out, and get k-mers more often\n100 positions\nn identities\nk must be contiguous\n⎟⎠\n⎞\n⎜⎝\n⎛\n-\n⎟⎠\n⎞\n⎜⎝\n⎛\n-\n-\n-\n-\n-\n⋅\n-\n-\n)!\n(!\n!\n)!\n(\n)!\n(\n)!\n(\n)\n(\nn\nn\nn\nk\nn\nk\nk\nn \\ k\n80%\n1.00 0.80 0.63 0.50 0.39 0.31\n70%\n1.01 0.69 0.47 0.32 0.22 0.14\n60%\n0.69 0.40 0.23 0.13 0.07 0.04\nIncreasing k-mer size\nIncreasing\npercent id\n\nRandom Model: simulation\n60% identity\nIsland length\nP(finding island at that length)\n80% identity\n⎟⎠\n⎞\n⎜⎝\n⎛\n-\n⎟⎠\n⎞\n⎜⎝\n⎛\n-\n-\n-\n-\n-\n⋅\n-\n-\n)!\n(!\n!\n)!\n(\n)!\n(\n)!\n(\n)\n(\nn\nn\nn\nk\nn\nk\nk\nPossible starts of\nthe conserved k-mer\nPossible arrangements\nof the remaining 100-k\n3⁄4\n10-mer\n1/8\n1/4\n3 5\n\nConservation 60% over 1000 bp\n2 species\n| L = 6 | L = 7 | L = 8 | L = 9 | L = 10 | L = 11 | L = 12 |\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 84.966+/-10.234| 75.458+/-10.399| 66.440+/-10.590| 83.582+/-10.434| 74.822+/-10.463| 66.484+/-10.514| 81.592+/-10.438|\n80| 53.170+/-10.459| 42.060+/-10.469| 32.032+/-10.427| 26.432+/-10.419| 47.300+/-10.523| 37.084+/-10.581| 31.248+/-10.567|\n90| 16.598+/-10.358| 10.424+/-10.295| 6.870+/-10.254| 4.594+/-10.197| 17.754+/-10.394| 13.326+/-10.369| 9.736+/-10.330|\n95| 14.868+/-10.318| 10.896+/-10.334| 7.578+/-10.201| 4.740+/-10.207| 3.842+/-10.203| 1.854+/-10.157| 1.280+/-10.122|\n100| 15.386+/-10.239| 11.078+/-10.297| 7.838+/-10.228| 5.114+/-10.198| 3.412+/-10.186| 2.094+/-10.176| 1.422+/-10.157|\n3 species\n| L = 6 | L = 7 | L = 8 | L = 9 | L = 10 | L = 11 | L = 12 |\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 53.804+/-10.398| 39.560+/-10.503| 27.238+/-10.429| 45.112+/-10.474| 31.856+/-10.475| 22.966+/-10.568| 37.318+/-10.496|\n80| 21.618+/-10.341| 12.790+/-10.278| 7.416+/-10.250| 4.918+/-10.249| 12.488+/-10.425| 8.212+/-10.346| 4.544+/-10.214|\n90| 4.000+/-10.152| 1.844+/-10.096| 0.978+/-10.111| 0.426+/-10.056| 2.254+/-10.154| 1.272+/-10.143| 0.946+/-10.105|\n95| 3.436+/-10.147| 1.934+/-10.121| 0.840+/-10.093| 0.664+/-10.081| 0.232+/-10.044| 0.044+/-10.022| 0.048+/-10.024|\n100| 3.360+/-10.146| 2.288+/-10.129| 0.746+/-10.081| 0.618+/-10.073| 0.124+/-10.034| 0.070+/-10.028| 0.050+/-10.025|\n4 species\n| L = 6 | L = 7 | L = 8 | L = 9 | L = 10 | L = 11 | L = 12 |\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 29.614+/-10.379| 17.102+/-10.326| 9.492+/-10.262| 18.508+/-10.378| 10.376+/-10.264| 5.502+/-10.239| 11.190+/-10.346|\n80| 8.738+/-10.193| 3.536+/-10.167| 1.636+/-10.117| 0.934+/-10.095| 2.128+/-10.126| 1.250+/-10.111| 0.554+/-10.099|\n90| 0.820+/-10.066| 0.396+/-10.055| 0.138+/-10.041| 0.026+/-10.018| 0.128+/-10.035| 0.198+/-10.053| 0.026+/-10.018|\n95| 0.740+/-10.073| 0.394+/-10.049| 0.136+/-10.032| 0.028+/-10.020| 0.040+/-10.020| 0.000+/-10.000| 0.000+/-10.000|\n100| 0.794+/-10.070| 0.476+/-10.051| 0.184+/-10.046| 0.074+/-10.025| 0.020+/-10.014| 0.044+/-10.022| 0.000+/-10.000|\n5 species\n| L = 6 | L = 7 | L = 8 | L = 9 | L = 10 | L = 11 | L = 12 |\n---+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+ ---------------+\n65| 15.144+/-10.290| 6.784+/-10.259| 2.934+/-10.182| 5.782+/-10.262| 2.802+/-10.167| 1.314+/-10.129| 2.354+/-10.160|\n80| 2.650+/-10.129| 0.902+/-10.086| 0.170+/-10.034| 0.248+/-10.046| 0.384+/-10.068| 0.164+/-10.041| 0.000+/-10.000|\n90| 0.250+/-10.044| 0.058+/-10.020| 0.032+/-10.016| 0.000+/-10.000| 0.024+/-10.017| 0.000+/-10.000| 0.000+/-10.000|\n95| 0.148+/-10.027| 0.018+/-10.013| 0.016+/-10.011| 0.000+/-10.000| 0.000+/-10.000| 0.000+/-10.000| 0.000+/-10.000|\n100| 0.244+/-10.038| 0.100+/-10.025| 0.034+/-10.017| 0.018+/-10.013| 0.000+/-10.000| 0.022+/-10.016| 0.000+/-10.000|\nRandom Model: simulation\n\nTrue alignments: Looking for K-mers\nnumber of k-mers that happen for each length of k-mer.\nRed islands come from colinear alignments\nBlue islands come from off-diagonal alignments\nNote: more than one data point per alignment.\nLog Log plot\nLinear plot\n\nSummary: Why k-mers work\n-\nIn worst case: Pigeonhole principle\n- Have too many matches to place on your sequence length\n- Bound to place at least k matches consecutively\n-\nIn average case: Birthday paradox / Simulations\n- Matches tend to cluster in the same bin. Mismatches too.\n- Looking for stretches of consecutive matches is feasible\n-\nBiological case: Counting k-mers in real alignments\n- From the number of conserved k-mers alone, one can\ndistinguish genuine alignments from chance alignments\n- Something biologically meaningful can be directly carried over\nto the algorithm.\n\nBLAST and Database Search\nMotivation\nThe BLAST algorithm\nBLAST extensions\nSubstitutions matrices\nWhy K-mers work\nApplications\n\nIdentifying exons\n- Direct application of BLAST\n- Compare Tetraodon to Human using BLAST\n- Best alignments happen only on exons\n- Translate a biological property into an alignment\nproperty\n- Exon = high alignment\n- Reversing this equivalence, look for high alignments\nand predict exons\n- Estimate human gene number\n- Method is not reliable for complete annotation, and\ndoes not find all genes, or even all exons in a gene\n- Can be used however, to estimate human gene\nnumber\n\nPart I -\nParameter tuning\n-\nTry a lot of parameters and find combination with\n- fastest running time\n- highest specificity\n- highest sensitivity\nMatrix\nX\nI (%)\nMethod\nW\nL\nSn (%)\nSp (%)\nT(s)\nBLASTN\nBLASTN\nBLASTN\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nTBLASTX\nNUC.4.4\nNUC.4.4\nNUC.4.4\nBLOSUM62\nBLOSUM62\nBLOSUM62\nCNS\nCNS\nCNS\n8 bases\n8 bases\n10 bases\n3 aa\n4 aa\n5 aa\n4 aa\n5 aa\n5 aa\n30 bases\n40 bases\n30 bases\n13 aa\n13 aa\n13 aa\n13 aa\n13 aa\n13 aa\n4.8\n5.7\n4.3\n74.8\n1,065.2\n1,160.9\n10.0\n29.4\n29.3\nP e r f o r m a n c e o f D i f f e r e n t B L A S T C o n f i g u r a t i o n s\nEach program was run with 1,340 different conditions and a representative selection of results is shown. A range of values for W\n(initial size of the search word) and X (threshold score for consecutive mismatching residues or bases) were tested. For amino acid\nalignments, a non-substitutive matrix (CNS, match = +15, mismatch = -12) was tested as well as the standard BLOSOM62 matrix. A\nminimal length (L) and percentage identity (I) were applied to select alignments for which a sensitivity (Sn) and specificity (Sp) were\ncalculated in terms of numbers of overall matching exons. T indicates the time in seconds needed to compare the 13 homologues\nagainst each other. The last row shows the optimal performance that was retained for Exofish.\nFigure by MIT OCW.\n\nPart II - choosing a threshold\n- For best parameters\n- Find threshold by\nobserving alignments\n- Anything higher than\nthreshold will be\ntreated as a predicted\nexon\n\nPart III - Gene identification\n- Matches correspond to exons\n- Not all genes hit\n- A fish doesn't need or have all functions present in human\n- Even those common are sometimes not perfectly conserved\n- Not all exons in each gene are hit\n- On average, three hits per gene. Three exons found.\n- Only most needed domains of a protein will be best\nconserved\n- All hits correspond to genuine exons\n- Specificity is 100% although sensitivity not guaranteed\n\nEstimating human gene number\n- Extrapolate experimental results\n- Incomplete coverage\n- Model how number would increase with increasing\ncoverage\n- Not perfect sensitivity\n- Estimate how many we're missing on well-annotated\nsequence\n- Assume ratio is uniform\n- Estimate gene number\n\nGene content by Chromosome\n-\nGene density varies throughout human genome\n- ExoFish predicted density corresponds to GeneMap annotation density\n0.5\nExofish\nHuman Chromosomes\nRelative Gene or Ecore Density\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 X Y\n1.5\n2.5\nGenemap\nFigure by MIT OCW.\n\nWhat is hashing\n- Content-based indexing\n- Instead of referencing elements by index\n- Reference elements by the elements themselves,\n- by their content\n- A hash function\n- Transforms an object into a pointer to an array\n- All objects will map in a flat distribution on array\nspace\n- Otherwise, some entries get too crowded\n- What about a database\n- List every location where a particular n-mer occurs\n- Retrieve in constant time all the places where you\ncan find it\n\nBreaking up the query\n- List them all\n- every word in the query\n- overlapping w-mers\n\nGenerating the neighborhood\n- Enumerate\n- For every amino acid in the\nword, try all possibilities\n- Score each triplet obtained\n- Only keep those within\nyour threshold\n\nLooking into database\n- Follow Pointers\n- Each neighborhood word gives\nus a list of all positions in the\ndatabase where it's found\nPMG\nHashed Database\n\nLength and Percent Identity"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/8d52690ff17ed77873ce923c07973b58_lecture3.pdf",
      "content": "6.096 - Algorithms for Computational Biology\nMotif finding in groups\nof related sequences\n\nChallenges in Computational Biology\nGenome Assembly\n1 Gene Finding\nRegulatory motif discovery\nDNA\nDatabase lookup\nSequence alignment\nEvolutionary Theory\nTCATGCTAT\nTCGTGATAA\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nComparative Genomics\nGene expression analysis\nRNA transcript\nCluster discovery\nGibbs sampling\n11 Protein network analysis\n12 Regulatory network inference\n13 Emerging network properties\n\nChallenges in Computational Biology\nRegulatory motif discovery\nDNA\nGroup of co-regulated genes\nCommon subsequence\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nRegulatory motif discovery\nATGACTAAATCTCATTCAGAAGAA\nGAL1\nCCCCW\nCGG\nCCG\nGal4\nMig1\nCGG\nCCG\nGal4\n-\nRegulatory motifs\n- Genes are turned on / off in response to changing environments\n- No direct addressing: subroutines (genes) contain sequence tags (motifs)\n- Specialized proteins (transcription factors) recognize these tags\n-\nWhat makes motif discovery hard?\n- Motifs are short (6-8 bp), sometimes degenerate\n- Can contain any set of nucleotides (no ATG or other rules)\n- Act at variable distances upstream (or downstream) of target gene\n\nSticks and backbones\nTraditional\nIn fact, the two DNA strands are twisted\naround each other to make a double helix.\nFancy\nChemical\nAtomic\nFigure by MIT OCW.\n\nWhere do ambiguous bases come from ?\nCOOH\nBase pair\nDNA\nArg\nArg\nSer\nAsn\nSugar phosphate\nbackbone\nNH2\nA\nB\nD\nC\nPhosphate\nmolecule\nDeoxyribose\nsugar molecule\nNitrogenous\nbases\nWeak bonds\nbetween bases\nSugar-phosphate backbone\n{\n{\nA\nA\nC\nC\nG\nG\nT\nT\nFigures by MIT OCW.\n\nCharacteristics of Regulatory Motifs\n- Tiny\n- Highly Variable\n- ~Constant Size\n- Because a constant-size\ntranscription factor binds\n- Often repeated\n- Low-complexity-ish\n\nSequence Logos\nentropy - n\n1: (communication theory) a numerical measure of the uncertainty of an\noutcome; \"the signal contained thousands of bits of information\"\n[information, selective information]\n2: (thermodynamics) a thermodynamic quantity representing the amount\nof energy in a system that is no longer available for doing mechanical\nwork; \"entropy increases as matter and energy in the universe degrade\nto an ultimate state of inert uniformity\" [randomness]\n-\nEntropy at pos'n I, H(i)\n= - Σ{letter x} freq(x, i) log2 freq(x, i)\n-\nHeight of x at pos'n i, L(x, i) = freq(x, i) (2 - H(i))\n- Examples:\n- freq(A, i) = 1;\nH(i) = 0; L(A, i) = 2\n- A: 1⁄2; C: 1⁄4; G: 1⁄4;\nH(i) = 1.5; L(A, i) = 1⁄4; L(not T, i) = 1⁄4\nImage removed due\nto copyright restrictions.\nImage removed due to copyright restrictions.\n\nProblem Definition\nGiven a collection of promoter sequences s1,..., sN of\ngenes with common expression\nCombinatorial\nMotif M: m1...mW\nSome of the mi's blank\n-\nFind M that occurs in all si\nwith ≤k differences\n-\nOr, Find M with smallest\ntotal hamming dist\nProbabilistic\nMotif: Mij;\n1 ≤i ≤W\n1 ≤j ≤4\nMij = Prob[ letter j, pos i ]\nFind best M, and positions p1,...,\npN in sequences\n\nFinding Regulatory Motifs\n.\n.\n.\nGiven a collection of genes bound by a transcription factor,\nFind the TF-binding motif in common\n\nEssentially a Multiple Local Alignment\n.\n.\n.\n-\nFind \"best\" multiple local alignment\n-\nAlignment score defined differently in\nprobabilistic/combinatorial cases\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nDiscrete Formulations\nGiven sequences S = {x1, ..., xn}\n-\nA motif W is a consensus string w1...wK\n-\nFind motif W* with \"best\" match to x1, ..., xn\nDefinition of \"best\":\nd(W, xi) = min hamming dist. between W and any word in xi\nd(W, S) = Σi d(W, xi)\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nExhaustive Searches\n1. Pattern-driven algorithm:\nFor W = AA...A to TT...T\n(4K possibilities)\nFind d( W, S )\nReport W* = argmin( d(W, S) )\nRunning time: O( K N 4K )\n(where N = Σi |xi|)\nAdvantage:\nFinds provably \"best\" motif W\nDisadvantage: Time\n\nExhaustive Searches\n2. Sample-driven algorithm:\nFor W = any K-long word occurring in some xi\nFind d( W, S )\nReport W* = argmin( d( W, S ) )\nor, Report a local improvement of W*\nRunning time: O( K N2 )\nAdvantage:\nTime\nDisadvantage: If the true motif is weak and does not occur in data\nthen a random motif may score better than any\ninstance of true motif\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nGreedy motif clustering (CONSENSUS)\nAlgorithm:\nCycle 1:\nFor each word W in S\n(of fixed length!)\nFor each word W' in S\nCreate alignment (gap free) of W, W'\nKeep the C1 best alignments, A1, ..., AC1\nACGGTTG\n,\nCGAACTT\n,\nGGGCTCT ...\nACGCCTG\n,\nAGAACTA\n,\nGGGGTGT ...\n\nGreedy motif clustering (CONSENSUS)\nAlgorithm:\nCycle t:\nFor each word W in S\nFor each alignment Aj from cycle t-1\nCreate alignment (gap free) of W, Aj\nKeep the Cl best alignments A1, ..., ACt\nACGGTTG\n,\nCGAACTT\n,\nGGGCTCT ...\nACGCCTG\n,\nAGAACTA\n,\nGGGGTGT ...\n...\n...\n...\nACGGCTC\n,\nAGATCTT\n,\nGGCGTCT ...\n\nGreedy motif clustering (CONSENSUS)\n-\nC1, ..., Cn are user-defined heuristic constants\n- N is sum of sequence lengths\n- n is the number of sequences\nRunning time:\nO(N2) + O(N C1) + O(N C2) + ... + O(N Cn)\n= O( N2 + NCtotal)\nWhere Ctotal = Σi Ci, typically O(nC), where C is a big constant\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nMotif Refinement and wordlets (MULTIPROFILER)\n-\nExtended sample-driven approach\nGiven a K-long word W, define:\nNα(W) = words W' in S s.t. d(W,W') ≤α\nIdea:\nAssume W is occurrence of true motif W*\nWill use Nα(W) to correct \"errors\" in W\n\nMotif Refinement and wordlets (MULTIPROFILER)\nAssume W differs from true motif W* in at most L positions\nDefine:\nA wordlet G of W is a L-long pattern with blanks, differing from W\n- L is smaller than the word length K\nExample:\nK = 7; L = 3\nW = ACGTTGA\nG = --A--CG\n\nMotif Refinement and wordlets (MULTIPROFILER)\nAlgorithm:\nFor each W in S:\nFor L = 1 to Lmax\n1.\nFind the α-neighbors of W in S\n→Nα(W)\n2.\nFind all \"strong\" L-long wordlets G in Na(W)\n3.\nFor each wordlet G,\n1.\nModify W by the wordlet G\n→W'\n2.\nCompute d(W', S)\nReport W* = argmin d(W', S)\nStep 1 above: Smaller motif-finding problem;\nUse exhaustive search"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/225cab6df63ae2275c888edb9865a598_lecture4.pdf",
      "content": "6.096 - Algorithms for Computational Biology\nExpectation Maximization\nand Gibbs Sampling\nLecture 1 - Introduction\nLecture 2 - Hashing and BLAST\nLecture 3 - Combinatorial Motif Finding\nLecture 4 - Statistical Motif Finding\n\nChallenges in Computational Biology\nGenome Assembly\n1 Gene Finding\nRegulatory motif discovery\nDNA\nDatabase lookup\nSequence alignment\nEvolutionary Theory\nTCATGCTAT\nTCGTGATAA\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nComparative Genomics\nGene expression analysis\nRNA transcript\nCluster discovery\nGibbs sampling\n11 Protein network analysis\n12 Regulatory network inference\n13 Emerging network properties\n\nChallenges in Computational Biology\nRegulatory motif discovery\nDNA\nGroup of co-regulated genes\nCommon subsequence\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 Gibbs sampling\n\nSequence Motifs\n- what is a sequence motif ?\n- a sequence pattern of biological significance\n- examples\n- protein binding sites in DNA\n- protein sequences corresponding to common\nfunctions or conserved pieces of structure\n\nMotifs and Profile Matrices\n- given a set of aligned sequences, it is\nstraightforward to construct a profile matrix\ncharacterizing a motif of interest\nsequence positions\nshared motif\n0.1\n0.1\n0.6\n0.2\nA\n0.1\n0.5\n0.2\n0.2 0.3\n0.2\n0.2\n0.3\n0.2\n0.1\n0.5\n0.2 0.1\n0.1\n0.6\n0.2\n0.3\n0.2\n0.1\n0.4\n0.1\n0.1\n0.7\n0.1\n0.3\n0.2\n0.2\n0.3\nC\nG\nT\n\nMotifs and Profile Matrices\n- how can we construct the profile if the sequences\naren't aligned?\n- in the typical case we don't know what the motif\nlooks like\n- use an Expectation Maximization (EM) algorithm\n\nThe EM Approach\n- EM is a family of algorithms for learning\nprobabilistic models in problems that involve\nhidden state\n- in our problem, the hidden state is where the motif\nstarts in each training sequence\n\nThe MEME Algorithm\n- Bailey & Elkan, 1993\n- uses EM algorithm to find multiple motifs in a set\nof sequences\n- first EM approach to motif discovery: Lawrence &\nReilly 1990\n\nRepresenting Motifs\n- a motif is assumed to have a fixed width, W\n- a motif is represented by a matrix of\nprobabilities: represents the probability of\ncharacter c in column k\n- example: DNA motif with W=3\nck\np\n1 2 3\nA 0.1 0.5 0.2\nC 0.4 0.2 0.1\nG 0.3 0.1 0.6\nT 0.2 0.2 0.1\n=\np\n\nRepresenting Motifs\n- we will also represent the \"background\" (i.e.\noutside the motif) probability of each character\n-\nrepresents the probability of character c in\nthe background\n- example:\ncp\nA 0.26\nC 0.24\nG 0.23\nT 0.27\n=\n0p\n\nBasic EM Approach\n- the element of the matrix represents the\nprobability that the motif starts in position j in\nsequence I\n- example: given 4 DNA sequences of length 6,\nwhere W=3\nZ\n1 2 3 4\nseq1 0.1 0.1 0.2 0.6\nseq2 0.4 0.2 0.1 0.3\nseq3 0.3 0.1 0.5 0.1\nseq4 0.1 0.5 0.1 0.3\n=\nZ\nij\nZ\n\nBasic EM Approach\ngiven: length parameter W, training set of sequences\nset initial values for p\ndo\nre-estimate Z from p (E -step)\nre-estimate p from Z (M-step)\nuntil change in p < ε\nreturn: p, Z\n\nBasic EM Approach\n- we'll need to calculate the probability of a training\nsequence given a hypothesized starting position:\n∏\n∏\n∏\n+\n=\n-\n+\n=\n+\n-\n-\n=\n=\n=\nL\nW\nj\nk\nc\nW\nj\nj\nk\nj\nk\nc\nj\nk\nc\nij\ni\nk\nk\nk\np\np\np\np\nZ\nX\n,\n,\n,\n)\n,1\n|\nPr(\nbefore motif\nmotif\nafter motif\nkc\nij\nZ\ni\nX\nis the ith sequence\nis 1 if motif starts at position j in sequence i\nis the character at position k in sequence i\n\nExample\nG C T G T A G\n=\ni\nX\n0 1 2 3\nA 0.25 0.1 0.5 0.2\nC 0.25 0.4 0.2 0.1\nG 0.25 0.3 0.1 0.6\nT 0.25 0.2 0.2 0.1\n=\np\n0.25\n\n0.25\n1.0\n1.0\n2.0\n0.25\n\n0.25\n\n)\n,1\n|\nPr(\nG,0\nA,0\nT,3\nG,2\nT,1\nC,0\nG,0\n×\n×\n×\n×\n×\n×\n=\n×\n×\n×\n×\n×\n×\n=\n=\np\np\np\np\np\np\np\np\nZ\nX\ni\ni\n\nThe E-step: Estimating Z\n∑\n+\n-\n=\n=\n=\n=\n=\n=\n)\n(\n)\n(\n)\n(\n)1\nPr(\n)\n,1\n|\nPr(\n)1\nPr(\n)\n,1\n|\nPr(\nW\nL\nk\nik\nt\nik\ni\nij\nt\nij\ni\nt\nij\nZ\np\nZ\nX\nZ\np\nZ\nX\nZ\n- to estimate the starting positions in Z at step t\n- this comes from Bayes' rule applied to\n)\n,\n|1\nPr(\n)\n(t\ni\nij\np\nX\nZ =\n\nThe E-step: Estimating Z\n- assume that it is equally likely that the motif will\nstart in any position\n∑\n+\n-\n=\n=\n=\n=\n=\n=\n)\n(\n)\n(\n)\n(\n)1\nPr(\n)\n,1\n|\nPr(\n)1\nPr(\n)\n,1\n|\nPr(\nW\nL\nk\nik\nt\nik\ni\nij\nt\nij\ni\nt\nij\nZ\np\nZ\nX\nZ\np\nZ\nX\nZ\n\nExample: Estimating Z\n0 1 2 3\nA 0.25 0.1 0.5 0.2\nC 0.25 0.4 0.2 0.1\nG 0.25 0.3 0.1 0.6\nT 0.25 0.2 0.2 0.1\n=\np\nG C T G T A G\n=\ni\nX\n.0\n.0\n.0\n.0\n1.0\n2.0\n3.0\n×\n×\n×\n×\n×\n×\n=\niZ\n.0\n.0\n.0\n6.0\n2.0\n4.0\n.0\n×\n×\n×\n×\n×\n×\n=\niZ\n=\n∑\n+\n-\n=\nW\nL\nj\nij\nZ\n...\n- then normalize so that\n\nThe M-step: Estimating p\n∑\n+\n+\n=\n+\nb\nk\nb\nk\nb\nk\nc\nk\nc\nt\nk\nc\nd\nn\nd\nn\np\n)\n(\n,\n,\n,\n,\n)\n(\n,\n⎪\n⎪\n⎩\n⎪⎪\n⎨\n⎧\n=\n-\n>\n=\n∑\n∑\n∑\n=\n=\n-\n+\nW\nj\nj\nc\nc\ni\nc\nX\nj\nij\nk\nc\nk\nn\nn\nk\nZ\nn\nk\nj\ni\n,\n}\n|\n{\n,\n\n,\npseudo-counts\ntotal # of c's\nin data set\n- recall\nrepresents the probability of character c in\nposition k ; values for position 0 represent the background\nk\ncp ,\n\nExample: Estimating p\nA C A G C A\n1.0\n,1.0\n,7.0\n,1.0\n,1\n3,1\n,1\n1,1\n=\n=\n=\n=\nZ\nZ\nZ\nZ\nA G G C A G\n4.0\n,1.0\n,1.0\n,4.0\n,2\n3,2\n,2\n1,2\n=\n=\n=\n=\nZ\nZ\nZ\nZ\nT C A G T C\n1.0\n,1.0\n,6.0\n,2.0\n,3\n3,3\n,3\n1,3\n=\n=\n=\n=\nZ\nZ\nZ\nZ\n\n...\n\n,3\n3,3\n,1\n1,1\n3,3\n1,2\n3,1\n1,1\nA,1\n+\n+\n+\n+\n+\n+\n+\n+\n=\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\np\n\nThe EM Algorithm\n- EM converges to a local maximum in the\nlikelihood of the data given the model:\n∏\ni\ni\np\nX\n)\n|\nPr(\n- usually converges in a small number of iterations\n- sensitive to initial starting point (i.e. values in p)\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 MEME extensions\n3⁄4 Gibbs sampling\n\nMEME Enhancements to the\nBasic EM Approach\n- MEME builds on the basic EM approach in the\nfollowing ways:\n- trying many starting points\n- not assuming that there is exactly one motif\noccurrence in every sequence\n- allowing multiple motifs to be learned\n- incorporating Dirichlet prior distributions\n\nStarting Points in MEME\n- for every distinct subsequence of length W in the\ntraining set\n- derive an initial p matrix from this subsequence\n- run EM for 1 iteration\n- choose motif model (i.e. p matrix) with highest\nlikelihood\n- run EM to convergence\n\nUsing Subsequences as Starting\nPoints for EM\n- set values corresponding to letters in the\nsubsequence to X\n- set other values to (1-X)/(M-1) where M is the\nlength of the alphabet\n- example: for the subsequence TAT with X=0.5\n1 2 3\nA 0.17 0.5 0.17\nC 0.17 0.17 0.17\nG 0.17 0.17 0.17\nT 0.5 0.17 0.5\n=\np\n\nThe ZOOPS Model\n- the approach as we've outlined it, assumes that each\nsequence has exactly one motif occurrence per sequence;\nthis is the OOPS model\n- the ZOOPS model assumes zero or one occurrences per\nsequence\n\nE-step in the ZOOPS Model\n- we need to consider another alternative: the ith sequence\ndoesn't contain the motif\n- we add another parameter (and its relative)\nprior prob that any position in a\nsequence is the start of a motif\nprior prob of a sequence\ncontaining a motif\nλ\nλ\nγ\n)1\n(\n+\n-\n=\nW\nL\n\nE-step in the ZOOPS Model\n∑\n+\n-\n=\n=\n+\n-\n=\n=\n=\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n,1\n|\nPr(\n)\n)(\n,0\n|\nPr(\n)\n,1\n|\nPr(\nW\nL\nk\nt\nt\nik\ni\nt\nt\ni\ni\nt\nt\nij\ni\nt\nij\np\nZ\nX\np\nQ\nX\np\nZ\nX\nZ\nλ\nγ\nλ\n∑\n+\n-\n=\n=\n,\nW\nL\nj\nj\ni\ni\nZ\nQ\n- here\nis a random variable that takes on 0 to indicate\nthat the sequence doesn't contain a motif occurrence\ni\nQ\n\nM-step in the ZOOPS Model\n- update p same as before\n- update\nas follows\n∑∑\n=\n=\n+\n+\n+\n-\n=\n+\n-\n=\nn\ni\nm\nj\nt\nj\ni\nt\nt\nZ\nW\nL\nn\nW\nL\n)\n(\n,\n)\n(\n)\n(\n)1\n(\n)1\n(\nγ\nλ\nγ\nλ ,\n- average of across all sequences, positions\n)\n(\n,\nt\nj\niZ\n\nThe TCM Model\n- the TCM (two-component mixture model)\nassumes zero or more motif occurrences per\nsequence\n\nLikelihood in the TCM Model\n- the TCM model treats each length W subsequence\nindependently\n- to determine the likelihood of such a subsequence:\n∏\n-\n+\n=\n+\n-\n=\n=\n,\n)\n,1\n|\nPr(\nW\nj\nj\nk\nj\nk\nc\nij\nij\nk\np\np\nZ\nX\nassuming a motif\nstarts there\n∏\n-\n+\n=\n=\n=\n,\n)\n,0\n|\nPr(\nW\nj\nj\nk\nc\nij\nij\nk\np\np\nZ\nX\nassuming a motif\ndoesn't start there\n\nE-step in the TCM Model\n)\n(\n)\n(\n,\n)\n(\n)\n(\n,\n)\n(\n)\n(\n,\n)\n(\n)\n,1\n|\nPr(\n)\n)(\n,0\n|\nPr(\n)\n,1\n|\nPr(\nt\nt\nij\nj\ni\nt\nt\nij\nj\ni\nt\nt\nij\nj\ni\nt\nij\np\nZ\nX\np\nZ\nX\np\nZ\nX\nZ\nλ\nλ\nλ\n=\n+\n-\n=\n=\n=\nsubsequence is a motif\nsubsequence isn't a motif\n- M-step same as before\n\nFinding Multiple Motifs\n- basic idea: discount the likelihood that a new\nmotif starts in a given position if this motif would\noverlap with a previously learned one\n- when re-estimating , multiply by\nij\nZ\n)1\nPr(\n=\nij\nV\n⎩\n⎨\n⎧\n=\n-\n+\notherwise\n\n0,\n]\n,...,\n[\nin\n\nmotifs\n\nprevious\n\nno\n\n1,\n,\n,\nw\nj\ni\nj\ni\nij\nX\nX\nV\n-\nis estimated using\nvalues from previous\npasses of motif finding\nij\nV\nij\nZ\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 MEME extensions\n3⁄4 Gibbs sampling\n\nGibbs Sampling\n- a general procedure for sampling from the joint\ndistribution of a set of random variables\nby iteratively sampling from\nfor each j\n- application to motif finding: Lawrence et al. 1993\n- can view it as a stochastic analog of EM for this task\n- less susceptible to local minima than EM\n)\n\n...\n,\n\n...\n|\nPr(\nn\nj\nj\nj\nU\nU\nU\nU\nU\n+\n-\n)\n\n...\nPr(\nn\nU\nU\n\nGibbs Sampling Approach\n- in the EM approach we maintained a distribution\nover the possible motif starting points for each sequence\n- in the Gibbs sampling approach, we'll maintain a specific\nstarting point for each sequence but we'll keep\nresampling these\niZ\nia\n\nGibbs Sampling Approach\ngiven: length parameter W, training set of sequences\nchoose random positions for a\ndo\npick a sequence\nestimate p given current motif positions a (update step)\n(using all sequences but )\nsample a new motif position for (sampling step)\nuntil convergence\nreturn: p, a\ni\nX\ni\nX\ni\nX\nia\n\nSampling New Motif Positions\n- for each possible starting position, , compute a\nweight\n- randomly select a new starting position according to\nthese weights\n∏\n∏\n-\n+\n=\n-\n+\n=\n+\n-\n=\n,\n,\nW\nj\nj\nk\nc\nW\nj\nj\nk\nj\nk\nc\nj\nk\nk\np\np\nA\nj\nai =\nia\n\nGibbs Sampling (AlignACE)\n- Given:\n- x1, ..., xN,\n- motif length K,\n- background B,\n- Find:\n- Model M\n- Locations a1,..., aN in x1, ..., xN\nMaximizing log-odds likelihood ratio:\n∑∑\n=\n=\n+\n+\nN\ni\nK\nk\ni\nk\na\ni\nk\na\ni\ni\nx\nB\nx\nk\nM\n)\n(\n)\n,\n(\nlog\n\nGibbs Sampling (AlignACE)\n-\nAlignACE: first statistical motif finder\n-\nBioProspector: improved version of AlignACE\nAlgorithm (sketch):\n1. Initialization:\na.\nSelect random locations in sequences x1, ..., xN\nb. Compute an initial model M from these locations\n2. Sampling Iterations:\na.\nRemove one sequence xi\nb. Recalculate model\nc.\nPick a new location of motif in xi according to\nprobability the location is a motif occurrence\n\nGibbs Sampling (AlignACE)\nInitialization:\n-\nSelect random locations a1,..., aN in x1, ..., xN\n-\nFor these locations, compute M:\n∑\n=\n+ =\n=\nN\ni\nk\na\nkj\nj\nx\nN\nM\ni\n)\n(\n-\nThat is, Mkj is the number of occurrences of letter j in motif\nposition k, over the total\n\nGibbs Sampling (AlignACE)\nPredictive Update:\n- Select a sequence x = xi\n- Remove xi, recompute model:\n))\n(\n(\n)1\n(\n,1∑\n=\n=\n+ =\n+\n+\n-\n=\nN\ni\ns\ns\nk\na\nj\nkj\nj\nx\nB\nN\nM\ns\nβ\nM\nwhere βj are pseudocounts to avoid 0s,\nand B = Σj βj\n\nGibbs Sampling (AlignACE)\nSampling:\nFor every K-long word xj,...,xj+k-1 in x:\nQj = Prob[ word | motif ] = M(1,xj)×...×M(k,xj+k-1)\nPi = Prob[ word | background ] B(xj)×...×B(xj+k-1)\nLet\nSample a random new position ai according to the\nprobabilities A1,..., A|x|-k+1.\n∑\n+\n-\n=\n=\n|\n|\n/\n/\nk\nx\nj\nj\nj\nj\nj\nj\nP\nQ\nP\nQ\nA\nProb\n|x|\n\nGibbs Sampling (AlignACE)\nRunning Gibbs Sampling:\n1.\nInitialize\n2.\nRun until convergence\n3.\nRepeat 1,2 several times, report common motifs\n\nAdvantages / Disadvantages\n-\nVery similar to EM\nAdvantages:\n-\nEasier to implement\n-\nLess dependent on initial parameters\n-\nMore versatile, easier to enhance with heuristics\nDisadvantages:\n-\nMore dependent on all sequences to exhibit the motif\n-\nLess systematic search of initial parameter space\n\nRepeats, and a Better Background\nModel\n-\nRepeat DNA can be confused as motif\n- Especially low-complexity CACACA... AAAAA, etc.\nSolution:\nmore elaborate background model\n0th order: B = { pA, pC, pG, pT }\n1st order: B = { P(A|A), P(A|C), ..., P(T|T) }\n...\nKth order: B = { P(X | b1...bK); X, bi∈{A,C,G,T} }\nHas been applied to EM and Gibbs (up to 3rd order)\n\nExample Application: Motifs in Yeast\nGroup:\nTavazoie et al. 1999, G. Church's lab, Harvard\nData:\n- Microarrays on 6,220 mRNAs from yeast\nAffymetrix chips (Cho et al.)\n- 15 time points across two cell cycles\n\nProcessing of Data\n1. Selection of 3,000 genes\n-\nGenes with most variable expression were selected\n-\nClustering according to common expression\n-\nK-means clustering\n-\n30 clusters, 50-190 genes/cluster\n-\nClusters correlate well with known function\n1. AlignACE motif finding\n-\n600-long upstream regions\n-\n50 regions/trial\n\nMotifs in Periodic Clusters\n\nMotifs in Non-periodic Clusters\n\nOverview\n3⁄4 Introduction\n3⁄4 Bio review: Where do ambiguities come from?\n3⁄4 Computational formulation of the problem\n3⁄4 Combinatorial solutions\n3⁄4 Exhaustive search\n3⁄4 Greedy motif clustering\n3⁄4 Wordlets and motif refinement\n3⁄4 Probabilistic solutions\n3⁄4 Expectation maximization\n3⁄4 MEME extensions\n3⁄4 Gibbs sampling"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture5_newest.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/5c7e6891cc52f3b6b906d70fd2d6c609_lecture5_newest.pdf",
      "content": "6.096 - Algorithms for Computational Biology\nSequence Alignment\nand Dynamic Programming\nLecture 1 - Introduction\nLecture 2 - Hashing and BLAST\nLecture 3 - Combinatorial Motif Finding\nLecture 4 - Statistical Motif Finding\n\nChallenges in Computational Biology\n4 Genome Assembly\nRegulatory motif discovery\n1 Gene Finding\nDNA\n2 Sequence alignment\n6 Comparative Genomics\nTCATGCTAT\nTCGTGATAA\n3 Database lookup\n7 Evolutionary Theory\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nGene expression analysis\nRNA transcript\nProtein network analysis\nGibbs sampling\n12 Regulatory network inference\nEmerging network properties\nCluster discovery\n\nA C G T C A T C A\nT A\nG T G\nT C A\nComparing two DNA sequences\n- Given two possibly related strings S1 and S2\n- What is the longest common subsequence?\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nS1\nS1\nA C G T C A T C A\nT A\nG T G\nT C A\nA\nG T\nT C A\nS2\nS2\nLCSS\nEdit distance:\n-Number of changes\nneeded for S1ÆS2\n\nHow can we compute best alignment\nS1\nS2\nA C G T C A T C A\nT A G T G T C A\n- Need scoring function:\n- Score(alignment) = Total cost of editing S1 into S2\n- Cost of mutation\n- Cost of insertion / deletion\n- Reward of match\n- Need algorithm for inferring best alignment\n- Enumeration?\n- How would you do it?\n- How many alignments are there?\n\nWhy we need a smart algorithm\n- Ways to align two sequences of length m, n\nn\nm\n\n§\nm n\n·\n( m n)!\n\n| 2\n\nS\n\n(m!)2\n(c) m\nm\n- For two sequences of length n\nn\nEnumeration Today's lecture\n184,756\n1.40E+11\n9.00E+58\n10,000\n\nKey insight: score is additive!\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\ni\nj\n- Compute best alignment recursively\n- For a given split (i, j), the best alignment is:\n-\nBest alignment of S1[1..i]\nand S2[1..j]\n- + Best alignment of S1[\ni..n] and S2[\nj..m]\ni\ni\nA C G\nT C A T C A\nT A G T G\nT C A\nS1\nS2\nj\nj\n\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nA\nC G T\nT A G\nT G\nS1\nS2\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nS2\nA C G T\nC A T C A\nT A G T G\nT C A\nS1\nS2\nA\nC G T C A T C A\nT A G\nT G T C A\nS1\nC G T\nC A T C A\nT G\nT C A\nS1\nS2\nKey insight: re-use computation\nIdentical sub-problems! We can reuse our work!\n\nSolution #1 - Memoization\n- Create a big dictionary, indexed by aligned seqs\n- When you encounter a new pair of sequences\n- If it is in the dictionary:\n- Look up the solution\n- If it is not in the dictionary\n- Compute the solution\n- Insert the solution in the dictionary\n- Ensures that there is no duplicated work\n- Only need to compute each sub-alignment once!\nTop down approach\n\nSolution #2 - Dynamic programming\n- Create a big table, indexed by (i,j)\n- Fill it in from the beginning all the way till the end\n- You know that you'll need every subpart\n- Guaranteed to explore entire search space\n- Ensures that there is no duplicated work\n- Only need to compute each sub-alignment once!\n- Very simple computationally!\nBottom up approach\n\nA C G T C A T C A\nT A\nG T G\nT C A\nS1\nS2\nA C G T C A T C A\nT\nA\nG\nT\nG\nT\nC\nA\nA\nG\nT\nC/G\nT\nC\nA\nGoal:\nFind best path\nthrough the matrix\nKey insight: Matrix representation of alignments\n\nSequence alignment\nDynamic Programming\nGlobal alignment\n\n0. Setting up the scoring matrix\n-\nA\nG\nT\nA\nA\nG\nC\n-\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n}\nTermination:\n-\nTop right: 0\nBottom right\n\n1. Allowing gaps in s\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-8\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\ni-1 , j\n}\nTermination:\n-\nTop right: 0\nBottom right\n- A(\n) - 2\n\n2. Allowing gaps in t\n-\nA\nG\nT\n-\nA\nA\nG\n-2\n-4\n-6\n-2\n-4\n-6\n-8\n-4\n-6\n-8\n-10\n-6\n-8\n-10\n-12\n-8\n-10\n-12\n-14\nInitialization:\n- Top right: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j ) - 2\n- A( i , j-1) - 2\n}\nTermination:\n- Bottom right\nC\n\n3. Allowing mismatches\n-\nA\nG\nT\n-\nA\nA\nG\n-2\n-4\n-6\n-2\n-1\n-3\n-5\n-4\n-3\n-2\n-4\n-6\n-5\n-4\n-3\n-8\n-7\n-6\n-5\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n- Top right: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j ) - 2\n- A( i , j-1) - 2\n- A(i-1 , j-1) -1\n}\nTermination:\n- Bottom right\nC\n\n4. Choosing optimal paths\n-\nA\nG\nT\n-\nA\nA\nG\n-2\n-4\n-6\n-2\n-1\n-3\n-5\n-4\n-3\n-2\n-4\n-6\n-5\n-4\n-3\n-8\n-7\n-6\n-5\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n- Top right: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 ,\n- A( i ,\n- A(i-1 ,\n}\nj ) - 2\nj-1) - 2\nj-1) -1\nTermination:\n- Bottom right\nC\n\n5. Rewarding matches\n-\nA\nG\nT\n-\nA\nA\nG\n-2\n-4\n-6\n-2\n-1\n-3\n-4\n-1\n-2\n-6\n-3\n-1\n-8\n-5\n-2\n-1\n-1\n-1\n-1\n-1\nInitialization:\n- Top right: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 ,\n- A( i ,\n- A(i-1 ,\n}\nj ) - 2\nj-1) - 2\nj-1) ±1\nTermination:\n- Bottom right\nC\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nDynamic Programming\n\nSemi-Global Motivation\n- Aligning the following sequences\nCAGCACTTGGATTCTCGG\nCAGC-----G-T----GG\n- We might prefer the alignment\nvvvv-----v-v----vv = 8(1)+0(-1)+10(-2) = -12\nCAGCA-CTTGGATTCTCGG\nmatch\nmismatch\n---CAGCGTGG--------\n---vv-vxvvv-------- = 6(1)+1(-1)+12(-2) = -19\ngap\n- New qualities sought, new scoring scheme\ndesigned\n- Intuitively, don't penalize \"missing\" end of the\nsequence\n- We'd like to model this intuition\n\nIgnoring starting gaps\n-\nA\nG\nT Initialization:\n-\n/\nl\n- 1st row co : 0\nUpdate Rule:\nA(i,j)=max{\nA\n- A(i-1 , j ) - 2\n- A( i , j-1) - 2\nA\n- A(i-1 , j-1) ±1\n}\nTermination:\nG\n- Bottom right\n-1\n-1\n-2\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nC\n\nIgnoring trailing gaps\n-\nA\nG\nT\n-\nA\nA\nG\n-1\n-1\n-2\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n- 1st row/col: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j ) - 2\n- A( i , j-1) - 2\n- A(i-1 , j-1) ±1\n}\nTermination:\n- max(last row/col)\nC\n\nUsing the new scoring scheme\n- With the old scoring scheme (all gaps count -2)\nCAGCACTTGGATTCTCGG\nCAGC-----G-T----GG\nvvvv-----v-v----vv = 8(1)+0(-1)+10(-2)+0(-0) = -12\n- New score (end gaps are free)\n6(1)+1(-1)+1(-2)+11(-0) = 3\nmatch\nmismatch\ngap\nCAGCA-CTTGGATTCTCGG\nendgap\n---CAGCGTGG--------\n---vv-vxvvv-------- =\n\nSemi-global alignments\n- Applications:\nquery\n- Finding a gene in a genome\n- Aligning a read onto an assembly\nsubject\n- Finding the best alignment of a PCR primer\n- Placing a marker onto a chromosome\n- These situations have in common\n- One sequence is much shorter than the other\n- Alignment should span the entire length of the smaller\nsequence\n- No need to align the entire length of the longer sequence\n- In our scoring scheme we should\n- Penalize end-gaps for subject sequence\n- Do not penalize end-gaps for query sequence\n\nSemi-Global Alignment\n-\nA\nG\nT\n-\nA\nA\nG\nC\nQuery: s\nSubject: t\nalign all of s\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n-\nA(i-1 , j\n-\nA( i , j\n-\nA(i-1 , j-1) ±1\n}\nTermination:\n-\n-2\n-4\n-6\n-1\n-1\n-2\n-1\n-1\n...or...\n-2\n-4\n-6\n-2\n-1\n-1\n-4\n-2\n-6\n-1\n-8\n-1\n-1\n-\nA\nG\nT\nA\nA\nG\nC\n-\nInitialization:\n-1st row\nA(i,j)=max{\n-A(i-1 , j\n-A( i , j\n-A(i-1 , j-1) ±1\n}\nTermination:\n-max(last row)\nQuery: t\nSubject: s\nalign all of t\n1st col\nmax(last col)\n) - 2\n-1) - 2\nUpdate Rule:\n) - 2\n-1) - 2\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nDynamic Programming\n\nIntro to Local Alignments\n- Statement of the problem\n- A local alignment of strings s and t\nis an alignment of a substring of s\nwith a substring of t\n- Definitions (reminder):\n- A substring consists of consecutive characters\n- A subsequence of s needs not be contiguous in s\n- Naive algorithm\n- Now that we know how to use dynamic programming\n- Take all O((nm)2), and run each alignment in O(nm) time\n- Dynamic programming\n- By modifying our existing algorithms, we achieve O(mn)\ns\nt\n\nGlobal Alignment\n-\nA\nG\nT\n-\nA\nA\nG\n-2\n-4\n-6\n-2\n-1\n-5\n-4\n-2\n-6\n-1\n-8\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n- Top left: 0\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 ,\n- A( i ,\n- A(i-1 ,\n}\nj ) - 2\nj-1) - 2\nj-1) ±1\nTermination:\n- Bottom right\nC\n\nLocal Alignment\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\ni-1 , j\ni , j\ni-1 , j-1) ±1\n- 0\n}\nTermination:\n- Anywhere\n-1\nTop left: 0\n- A(\n- A(\n- A(\n) - 2\n-1) - 2\n\nLocal Alignment issues\n- Resolving ambiguities\n- When following arrows back, one can stop at any of the zero\nentries. Only stop when no arrow leaves. Longest.\n- Correctness sketch by induction\n- Assume we've correctly aligned up to (i,j)\n- Consider the four cases of our max computation\n- By inductive hypothesis recurse on (i-1,j-1), (i-1,j), (i,j-1)\n- Base case: empty strings are suffixes aligned optimally\n- Time analysis\n- O(mn) time\n- O(mn) space, can be brought to O(m+n)\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nAffine Gap Penalty\nDynamic Programming\n\nScoring the gaps more accurately\nCurrent model:\nJ(n)\nGap of length\nn\nincurs penalty\nnud\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nJ(n):\nfor all n, J(n + 1) - J(n) d J(n) - J(n - 1)\nJ(n)\n\nGeneral gap dynamic programming\nInitialization:\nsame\nIteration:\nF(i-1, j-1) + s(xi, yj)\nF(i, j)\n= max\nmax\nmaxk=0...i-1F(k,j) - J(i-k)\nk=0...j-1F(i,k) - J(j-k)\nTermination:\nsame\nRunning Time: O(N2M)\nSpace:\n(assume N>M)\nO(NM)\n\nCompromise: affine gaps\nJ(n) = d + (n - 1)ue\nJ(n)\n|\n|\n\ngap\ngap\nopen\nextend\nd\nTo compute optimal alignment,\ne\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\nF(i, j): score of alignment x1...xi to y1...yj\nif xi aligns to yj\nG(i, j): score if xi, or yj, aligns to a gap\n\nMotivation for affine gap penalty\n- Modeling evolution\n- To introduce the first gap, a break must occur in DNA\n- Multiple consecutive gaps likely to be introduced by the same\nevolutionary event. Once the break is made, it's relatively easy\nto make multiple insertions or deletions.\n- Fixed cost for opening a gap: p+q\n- Linear cost increment for increasing number of gaps: q\n- Affine gap cost function\n- New gap function for length k: w(k) = p+q*k\n- p+q is the cost of the first gap in a run\n- q is the additional cost of each additional gap in same run\n\nAdditional Matrices\n- The amount of state needed increases\n- In scoring a single entry in our matrix, we need\nremember an extra piece of information\n- Are we continuing a gap in s? (if not, start is more\nexpensive)\n- Are we continuing a gap in t? (if not, start is more\nexpensive)\n- Are we continuing from a match between s(i) and t(j)?\n- Dynamic programming framework\n- We encode this information in three different states\nfor each element (i,j) of our alignment. Use three\nmatrices\n- a(i,j): best alignment of s[1..i] & t[1..j] that aligns s[i] with t[j]\n- b(i,j): best alignment of s[1..i] & t[1..j] that aligns gap with t[j]\n- c(i,j): best alignment of s[1..i] & t[1..j] that aligns s[i] with gap\n\nUpdate rules\nWhen s[j] and t[j] are aligned\n§\ni\na ,1 j 1)· Score can be\n(\n\n(\n( [\nt j\n(\ni\na , j)\ni\ns\nscore\n], [ ]) max\ni\nb ,1 j 1) different for each\n\npair of chars\n(i\nc ,1 j 1)\n(c)\nWhen t[j] aligns with a gap in s\n§\ni\na , j 1) ( p\nq)·\n\nstarting a gap in s\n(\n\n(\n(\ni\nb , j) max\ni\nb , j 1)\n\nq\n\nextending a gap in s\n\ni\nc , j 1) ( p\nq)\n(\n\nStopping a gap in t,\n(c)\nand starting one in s\nWhen s[i] aligns with a gap in t\n§\ni\na 1 ) ( p\nq)·\n, j\n\n(\n\n(\n(\n, j\ni\nc , j) max\ni\nc 1 ) q\n\n(\n, j\n\ni\nb 1 ) ( p\nq)\n(c)\nFind maximum over all three arrays max(a[m,n],b[m,n],c[m,n]).\nFollow arrows back, skipping from matrix to matrix\n\nSimplified rules\n- Transitions from b to c are not necessary...\n...if the worst mismatch costs less than p+q\nACC-GGTA\nACCGGTA\nA--TGGTA\nA-TGGTA\n(\nWhen s[j] and t[j] are aligned\n§\ni\na ,1 j 1)· Score can be\n\n(\n[\n\n],\n(\ni\na , j ) score (\nt\ni\ns\n[ j ]) max\ni\nb ,1 j 1) different for each\n\npair of chars\n(\n(c)\ni\nc ,1 j 1) 1\nWhen t[j] aligns with a gap in s\n(\ni\nb ( , j ) max §\ni\na , j 1) ( p q )·\nstarting a gap in s\n\n(c)\ni\nb , j 1) q\n(\nextending a gap in s\nWhen s[i] aligns with a gap in t\n(\ni\nc ( , j ) max §\ni\na ,1 j ) ( p q )·\n\n(c)\ni\nc ,1 j ) q\n(\n\nGeneral Gap Penalty\n- Gap penalties are limited by the amount of state\n- Affine gap penalty: w(k) = k*p\n- State: Current index tells if in a gap or not\n- Linear gap penalty: w(k) = p + q*k, where q<p\n- State: add binary value for each sequence: starting a gap or not\n- What about quadriatic: w(k) = p+q*k+rk2.\n- State: needs to encode the length of the gap, which can be O(n)\n- To encode it we need O(log n) bits of information. Not feasible\n- What about a (mod 3) gap penalty for protein alignments\n- Gaps of length divisible by 3 are penalized less: conserve frame\n- This is feasible, but requires more possible states\n- Possible states are: starting, mod 3=1, mod 3=2, mod 3=0\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nLinear Gap Penalty\nVariations on the Theme\nDynamic Programming\n\nDynamic Programming Versatility\n- Unified framework\n- Dynamic programming algorithm. Local updates.\n- Re-using past results in future computations.\n- Memory usage optimizations\n- Tools in our disposition\n- Global alignment: entire length of two orthologous genes\n- Semi-global alignment: piece of a larger sequence aligned\nentirely\n- Local alignment: two genes sharing a functional domain\n- Linear Gap Penalty: penalize first gap more than subsequent\ngaps\n- Edit distance, min # of edit operations. M=0, m=g=-1, every\noperation subtracts 1, be it mutation or gap\n- Longest common subsequence: M=1, m=g=0. Every match\nadds one, be it contiguous or not with previous.\n\nDP Algorithm Variations\nt\ns\nt\ns\nt\ns\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-1\n-4\n-1\n-1\n-2\n-6\n-1\n-8\n-3\n-1\nGlobal Alignment\nSemi-Global Alignment\nLocal Alignment\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-1\n-1\n-2\n-1\n-1\n-\nA\nG\nT\nA\nA\nG\nA\n-\n\nBounded Dynamic Programming\nInitialization:\nF(i,0), F(0,j) undefined for i, j > k\nIteration:\nFor i = 1...M\nFor j = max(1, i - k)...min(N, i+k)\nF(i - 1, j - 1)+ s(xi, yj)\nF(i, j) = max\nF(i, j - 1) - d, if j > i - k(N)\nF(i - 1, j) - d, if j < i + k(N)\nTermination:\nsame\nEasy to extend to the affine gap case\nx1 .............................. xM\ny1 .............................. yN\nk(N)\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-Space Alignment\n\nHirschberg's algorithm\n- Longest common subsequence\n- Given sequences s = s1 s2 ... s , t = t1 t2 ... tn,\nm\n- Find longest common subsequence u = u1 ... uk\n- Algorithm:\nF(i-1, j)\n- F(i, j) = max F(i, j-1)\nF(i-1, j-1) + [1, if s = tj; 0 otherwise]\ni\n- Hirschberg's algorithm solves this in linear space\n\nIntroduction: Compute optimal score\nIt is easy to compute F(M, N) in linear space\nF(i,j)\nAllocate ( column[1] )\nAllocate ( column[2] )\nFor\ni = 1....M\nIf\ni > 1, then:\nFree( column[i - 2] )\nAllocate( column[ i ] )\nFor j = 1...N\nF(i, j) = ...\n\nLinear-space alignment\nTo compute both the optimal score and the optimal alignment:\nDivide & Conquer approach:\nNotation:\nr\nx , yr: reverse of x, y\nE.g. x = accgg;\nrx = ggcca\nr\nr\nFr(i, j): optimal score of aligning xr\n1...x\n& yr\n1...y j\ni\nsame as F(M-i+1, N-j+1)\n\nLinear-space alignment\nLemma:\nF(M, N) = maxk=0...N( F(M/2, k) + Fr(M/2, N-k) )\nx\ny\nM/2\nk*\nFr(M/2, N-k)\nF(M/2, k)\n\nLinear-space alignment\n- Now, using 2 columns of space, we can compute\nfor k = 1...M, F(M/2, k), Fr(M/2, N-k)\nPLUS the backpointers\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-space alignment\n- Iterate this procedure to the left and right!\nk*\nN-k*\nM/2\nM/2\n\nLinear-space alignment\nHirschberg's Linear-space algorithm:\nMEMALIGN(l, l', r, r'):\n(aligns x ...xl' with yr...yr')\nl\n1.\nLet h = a(l'-l)/2o\n2.\nFind in Time O((l' - l) u (r'-r)), Space O(r'-r)\nthe optimal path,\nLh, entering column h-1, exiting column h\nLet k1 = pos'n at column h - 2 where Lh enters\nk2 = pos'n at column h + 1 where Lh exits\n3.\nMEMALIGN(l, h-2, r, k1)\n4.\nOutput Lh\n5.\nMEMALIGN(h+1, l', k2, r')\nTop level call: MEMALIGN(1, M, 1, N)\n\nLinear-space alignment\nTime, Space analysis of Hirschberg's algorithm:\nTo compute optimal path at middle column,\nFor box of size M u N,\nSpace:\n2N\nTime:\ncMN,\nfor some constant c\nThen, left, right calls cost c( M/2 u k* + M/2 u (N-k*) ) = cMN/2\nAll recursive calls cost\nTotal Time: cMN + cMN/2 + cMN/4 + ..... = 2cMN = O(MN)\nTotal Space: O(N) for computation,\nO(N+M) to store the optimal alignment\n\nThe Four-Russian Algorithm\nA useful speedup of Dynamic Programming\n\nMain Observation\nWithin a rectangle of the DP\nmatrix,\nvalues of D depend only\non the values of A, B, C,\nand substrings xl...l', yr...r'\nDefinition:\nA t-block is a t u t square of\nthe DP matrix\nIdea:\nDivide matrix in t-blocks,\nPrecompute t-blocks\nSpeedup: O(t)\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nMain structure of the algorithm:\n-\nDivide NuN DP matrix into KuK\nlog2N-blocks that overlap by 1\ncolumn & 1 row\n-\nFor i = 1......K\n-\nFor j = 1......K\n-\nCompute Di,j as a function of\nAi,j, Bi,j, Ci,j, x[li...l'i], y[rj...r'j]\nTime: O(N2 / log2N)\ntimes the cost of step 4\nt\nt\nt\n\nThe Four-Russian Algorithm\nAnother observation:\n( Assume m = 0, s = 1, d = 1 )\nLemma. Two adjacent cells of F(.,.) differ by at most 1\nGusfield's book covers case where m = 0,\ncalled the edit distance (p. 216):\nminimum # of substitutions + gaps to transform one string to another\n\nThe Four-Russian Algorithm\nProof of Lemma:\n1.\nSame row:\na. F(i, j) - F(i - 1, j) d +1\nAt worst, one more gap:\nx1......xi-1 xi\ny1......yj -\nb. F(i, j) - F(i - 1, j) t -1\nF(i, j)\nF(i - 1, j - 1)\nF(i, j) - F(i - 1, j - 1)\nx ......x\nx\nx1......x\n-\ni-1\ni\ni-1\ny1......ya-1ya ya+1...yj\ny1......ya-1ya ya+1...yj\nt -1\nx1......x\nx\nx ......x\ni-1\ni\ni-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\n+1\n2.\nSame column: similar argument\n\nThe Four-Russian Algorithm\nProof of Lemma:\n3.\nSame diagonal:\na. F(i, j) - F(i - 1, j - 1) d +1\nAt worst, one additional mismatch in F(i, j)\nb. F(i, j) - F(i - 1, j - 1) t -1\nF(i, j)\nx1......x\nx\ni-1\ni\n|\ny1......yi-1 yj\nx1......x\nx\ni-1\ni\ny1......ya-1- ya...yj\nF(i - 1, j - 1)\nx ......x\ni-1\ny1......yj-1\nx ......x\ni-1\ny1......ya-1ya...yj\nF(i, j) - F(i - 1, j - 1)\nt-1\n+1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset vector is a\nt-long vector of values\nfrom {-1, 0, 1},\nwhere the first entry is 0\nIf we know the value at A,\nand the top row, left column\noffset vectors,\nand xl......xl', yr......yr',\nThen we can find D\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n5A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n1A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset function of a\nt-block\nis a function that for any\ngiven offset vectors\nof top row, left column,\nand xl......xl', yr......yr',\nproduces offset vectors\nof bottom row, right\ncolumn\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nWe can pre-compute the offset function:\n2(t-1) possible input offset vectors\n2t possible strings x ......xl', yr......yr'\nl\nTherefore 32(t-1) u 42t values to pre-compute\nWe can keep all these values in a table, and look up in linear time,\nor in O(1) time if we assume\nconstant-lookup RAM for log-sized inputs\n\nThe Four-Russian Algorithm\nFour-Russians Algorithm: (Arlazarov, Dinic, Kronrod,\nFaradzev)\n1. Cover the DP table with t-blocks\n2. Initialize values F(.,.) in first row & column\n3. Row-by-row, use offset values at leftmost column and top\nrow of each block, to find offset values at rightmost column\nand bottom row\n4. Let Q = total of offsets at row N\nF(N, N) = Q + F(N, 0)\n\nThe Four-Russian Algorithm\nt\nt\nt\n\nEvolution at the DNA level\n...ACGGTGCAGTCACCA...\n...ACGTTGCAGTCCACCA...\nC\nSequence Changes\nComputing best alignment\n-In absence of gaps\n\nSequence Alignment\nAGGCTATCACCTGACCTCCAGGCCGATGCCC\nTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---\nTAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\nDefinition\nGiven two strings\nx = x1x2...xM, y = y1y2...yN,\nan alignment is an assignment of gaps to positions\n0,..., M in x, and 0,..., N in y, so as to line up each\nletter in one sequence with either a letter, or a gap\nin the other sequence\n\nScoring Function\n- Sequence edits:\nAGGCCTC\n- Mutations\nAGGACTC\n- Insertions\nAGGGCCTC\n- Deletions\nAGG.CTC\nScoring Function:\nMatch:\n+m\nMismatch: -s\nGap:\n-d\nScore F = (# matches) u m - (# mismatches) u s - (#gaps) u\nd\n\nHow do we compute the best alignment?\nAGTGACCTGGGAAGACCCTGACCCTGGGTCACAAAACTC\nAGTGCCCTGGAACCCTGACGGTGGGTCACAAAACTTCTGGA\nToo many possible\nalignments:\nO( 2M+N)\n\nAlignment is additive\nObservation:\nThe score of aligning\nx1......xM\ny1......yN\nis additive\nSay that\nx1...xi\nxi+1...xM\naligns to\ny1...yj\nyj+1...yN\nThe two scores add up:\nF(x[1:M], y[1:N]) = F(x[1:i], y[1:j]) + F(x[i+1:M], y[j+1:N])\n\nDynamic Programming\n- We will now describe a dynamic programming\nalgorithm\nSuppose we wish to align\nx1......xM\ny1......yN\nLet\nF(i,j) = optimal score of aligning\nx1......xi\ny1......yj\n\nDynamic Programming (cont'd)\nNotice three possible cases:\n1.\nxi aligns to yj\nx1......xi-1 xi\ny1......yj-1 yj\nm, if xi = y\n-s, if not\nj\nF(i,j) = F(i-1, j-1) +\n2.\nxi aligns to a gap\nx1......xi-1 xi\ny1......yj\n\n3.\nyj aligns to a gap\nF(i,j) = F(i-1, j) - d\nx1......x\n\ni\ny1......yj-1 yj\nF(i,j) = F(i, j-1) - d\n\nDynamic Programming (cont'd)\n- How do we know which case is correct?\nInductive assumption:\nF(i, j-1), F(i-1, j), F(i-1, j-1) are optimal\nThen,\nF(i-1, j-1) + s(x , yj)\nF(i, j) = max\ni\nF(i-1, j) - d\nF( i, j-1) - d\nWhere\ns(x , yj) = m, if x = y ; -s, if not\ni\ni\nj\n\nExample\nx = AGTA\ny = ATA\nF(i,j)\ni = 0\nj = 0\nA\nG\nT\nA\n-1\n-2\n-3\n-4\n\nA\n-1\n-1\n-2\n\nT\n-2\n\nA\n-3\n-1\n-1\n\nm = 1\ns = -1\nd = -1\nOptimal Alignment:\nF(4,3) = 2\nAGTA\nA - TA\n\nThe Needleman-Wunsch Matrix\ny1 .................................... yN\nx1 .................................... xM\nEvery nondecreasing\npath\nfrom (0,0) to (M, N)\ncorresponds to\nan alignment\nof the two sequences\nCan think of it as a\ndivide-and-conquer algorithm\n\nThe Needleman-Wunsch Algorithm\n1.\nInitialization.\na.\nF(0, 0)\n= 0\nb.\nF(0, j)\n= - j u d\nc.\nF(i, 0)\n= - i u d\n2.\nMain Iteration. Filling-in partial alignments\na.\nFor each\ni = 1......M\nFor each\nj = 1......N\nF(i-1,j-1) + s(x , yj)\ni\nF(i, j)\n= max\nF(i-1, j) - d\nF(i, j-1) - d\nDIAG,\nif [case 1]\nPtr(i,j)\n=\nLEFT,\nif [case 2]\nUP,\nif [case 3]\n3.\nTermination. F(M, N) is the optimal score, and\nfrom Ptr(M, N) can trace back optimal alignment\n[case 1]\n[case 2]\n[case 3]\n\nPerformance\nO(NM)\nO(NM)\n-\nme:\nLater we will cover more efficient methods\n- Ti\n- Space:\n\nA variant of the basic algorithm:\n- Maybe it is OK to have an unlimited # of gaps in\nthe beginning and end:\n----------CTATCACCTGACCTCCAGGCCGATGCCCCTTCCGGC\nGCGAGTTCATCTATCAC--GACCGC--GGTCG--------------\n- Then, we don't want to penalize gaps in the ends\n\nDifferent types of overlaps\n\nThe Overlap Detection variant\nChanges:\n1. Initialization\nx1 .................................... xM\ny 1 .................................... yN\nFor all i, j,\nF(i, 0) = 0\nF(0, j) = 0\n2. Termination\nmaxi F(i,\nN)\nFOPT = max\nmax F(M,\nj\nj)\n\nThe local alignment problem\nGiven two strings x = x1......xM,\ny = y1......yN\n(optimal global alignment value)\nis maximum\ne.g.\nx = aaaacccccgggg\ny = cccgggaaccaacc\nFind substrings x', y' whose similarity\n\nWhy local alignment\n- Genes are shuffled between genomes\n- Portions of proteins (domains) are often conserved\nImage removed due to copyright restrictions.\n\nCross-species genome similarity\n- 98% of genes are conserved between any two mammals\n- >70% average similarity in protein sequence\nhum_a : GTTGACAATAGAGGGTCTGGCAGAGGCTC--------------------- @ 57331/400001\nmus_a : GCTGACAATAGAGGGGCTGGCAGAGGCTC--------------------- @ 78560/400001\nrat_a : GCTGACAATAGAGGGGCTGGCAGAGACTC--------------------- @ 112658/369938\nfug_a : TTTGTTGATGGGGAGCGTGCATTAATTTCAGGCTATTGTTAACAGGCTCG @ 36008/68174\nhum_a : CTGGCCGCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 57381/400001\nmus_a : CTGGCCCCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 78610/400001\nrat_a : CTGGCCCCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 112708/369938\n\"atoh\" enhancer in\nfug_a : TGGGCCGAGGTGTTGGATGGCCTGAGTGAAGCACGCGCTGTCAGCTGGCG @ 36058/68174\nhuman, mouse,\nhum_a : AGCGCACTCTCCTTTCAGGCAGCTCCCCGGGGAGCTGTGCGGCCACATTT @ 57431/400001\nrat, fugu fish\nmus_a : AGCGCACTCG-CTTTCAGGCCGCTCCCCGGGGAGCTGAGCGGCCACATTT @ 78659/400001\nrat_a : AGCGCACTCG-CTTTCAGGCCGCTCCCCGGGGAGCTGCGCGGCCACATTT @ 112757/369938\nfug_a : AGCGCTCGCG------------------------AGTCCCTGCCGTGTCC @ 36084/68174\nhum_a : AACACCATCATCACCCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 57481/400001\nmus_a : AACACCGTCGTCA-CCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 78708/400001\nrat_a : AACACCGTCGTCA-CCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 112806/369938\nfug_a : CCGAGGACCCTGA------------------------------------- @ 36097/68174\n\nThe Smith-Waterman algorithm\nIdea: Ignore badly aligning regions\nModifications to Needleman-Wunsch:\nInitialization:\nF(0, j) = F(i, 0) = 0\nIteration:\nF(i, j) = max\nF(i - 1, j) - d\nF(i, j - 1) - d\nF(i - 1, j - 1) + s(x , yj)\ni\n\nThe Smith-Waterman algorithm\nTermination:\n1. If we want the best local alignment...\nFOPT = maxi,j F(i, j)\n2. If we want all local alignments scoring > t\nFor all i, j find F(i, j) > t, and trace back\n\nScoring the gaps more accurately\nCurrent model:\nJ(n)\nGap of length\nn\nincurs penalty\nnud\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nJ(n):\nfor all n, J(n + 1) - J(n) d J(n) - J(n - 1)\nJ(n)\n\nGeneral gap dynamic programming\nInitialization:\nsame\nIteration:\nF(i-1, j-1) + s(xi, yj)\nF(i, j)\n= max\nmax\nmaxk=0...i-1F(k,j) - J(i-k)\nk=0...j-1F(i,k) - J(j-k)\nTermination:\nsame\nRunning Time: O(N2M)\nSpace:\n(assume N>M)\nO(NM)\n\nCompromise: affine gaps\nJ(n) = d + (n - 1)ue\n|\n|\nJ(n)\ngap\ngap\n\nopen\nextend\n\nd\nTo compute optimal alignment,\ne\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\nF(i, j): score of alignment x1...x to y1...yj\ni\nif xi aligns to yj\nG(i, j): score if x , or yj, aligns to a gap\ni\n\nNeedleman-Wunsch with affine gaps\nInitialization:\nF(i, 0) = d + (i - 1)ue\nF(0, j) = d + (j - 1)ue\nIteration:\nF(i - 1, j - 1) + s(x , yj)\ni\nF(i, j) = max\nG(i - 1, j - 1) + s(x , yj)\ni\nF(i - 1, j) - d\nF(i, j - 1) - d\nG(i, j) = max\nG(i, j - 1) - e\nG(i - 1, j) - e\nTermination:\nsame\n\nSequence Alignment\nAGGCTATCACCTGACCTCCAGGCCGATGCCC\nTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---\nTAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\nDefinition\nGiven two strings\nx = x1x2...xM, y = y1y2...yN,\nan alignment is an assignment of gaps to positions\n0,..., M in x, and 0,..., N in y, so as to line up each\nletter in one sequence with either a letter, or a gap\nin the other sequence\n\nScoring Function\n- Sequence edits:\nAGGCCTC\n- Mutations\nAGGACTC\n- Insertions\nAGGGCCTC\n- Deletions\nAGG.CTC\nScoring Function:\nMatch:\n+m\nMismatch: -s\nGap:\n-d\nScore F = (# matches) u m - (# mismatches) u s - (#gaps) u\nd\n\nThe Needleman-Wunsch Algorithm\n1.\nInitialization.\na.\nF(0, 0)\n= 0\nb.\nF(0, j)\n= - j u d\nc.\nF(i, 0)\n= - i u d\n2.\nMain Iteration. Filling-in partial alignments\na.\nFor each\ni = 1......M\nFor each\nj = 1......N\nF(i-1,j-1) + s(x , yj)\ni\nF(i, j)\n= max\nF(i-1, j) - d\nF(i, j-1) - d\nDIAG,\nif [case 1]\nPtr(i,j)\n=\nLEFT,\nif [case 2]\nUP,\nif [case 3]\n3.\nTermination. F(M, N) is the optimal score, and\nfrom Ptr(M, N) can trace back optimal alignment\n[case 1]\n[case 2]\n[case 3]\n\nThe Smith-Waterman algorithm\nIdea: Ignore badly aligning regions\nModifications to Needleman-Wunsch:\nInitialization:\nF(0, j) = F(i, 0) = 0\nIteration:\nF(i, j) = max\nF(i - 1, j) - d\nF(i, j - 1) - d\nF(i - 1, j - 1) + s(x , yj)\ni\n\nScoring the gaps more accurately\nSimple, linear gap model:\nGap of length\nn\nJ(n)\nincurs penalty\nnud\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nJ(n):\nJ(n)\nfor all n, J(n + 1) - J(n) d J(n) - J(n - 1)\nAlgorithm: O(N3) time, O(N2) space\n\nCompromise: affine gaps\nJ(n) = d + (n - 1)ue\n|\n|\nJ(n)\ngap\ngap\n\nopen\nextend\n\nd\nTo compute optimal alignment,\ne\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\nF(i, j): score of alignment x1...x to y1...yj\ni\nif xi aligns to yj\nG(i, j): score if x , or yj, aligns to a gap\ni\n\nWhy do we need two matrices?\n-\nxi aligns to yj\nx1......xi-1 xi xi+1\ny1......yj-1 yj\n-\n2. xi aligns to a gap\nx1......xi-1 xi xi+1\ny1......yj ...\n-\nAdd -d\nAdd -e\nNeedleman-Wunsch with affine gaps\n\nNeedleman-Wunsch with affine gaps\nInitialization:\nF(i, 0) = d + (i - 1)ue\nF(0, j) = d + (j - 1)ue\nIteration:\nF(i - 1, j - 1) + s(x , yj)\ni\nF(i, j) = max\nG(i - 1, j - 1) + s(x , yj)\ni\nF(i - 1, j) - d\nF(i, j - 1) - d\nG(i, j) = max\nG(i, j - 1) - e\nG(i - 1, j) - e\nTermination:\nsame\n\nTo generalize a little...\n... think of how you would compute optimal alignment\nwith this gap function\nJ(n)\n....in time O(MN)\n\nBounded Dynamic Programming\nAssume we know that x and y are very similar\nAssumption: # gaps(x, y) < k(N)\n( say N>M )\nxi\nThen,\n|\nimplies\n| i - j | < k(N)\nyj\nWe can align x and y more efficiently:\nTime, Space:\nO(N u k(N)) << O(N2)\n\nBounded Dynamic Programming\nInitialization:\nF(i,0), F(0,j) undefined for i, j > k\nIteration:\nFor i = 1...M\nFor j = max(1, i - k)...min(N, i+k)\nF(i - 1, j - 1)+ s(xi, yj)\nF(i, j) = max\nF(i, j - 1) - d, if j > i - k(N)\nF(i - 1, j) - d, if j < i + k(N)\nTermination:\nsame\nEasy to extend to the affine gap case\nx1 .............................. xM\ny1 .............................. yN\nk(N)\n\nLinear-Space Alignment\n\nHirschberg's algortihm\n- Longest common subsequence\n- Given sequences s = s1 s2 ... s , t = t1 t2 ... tn,\nm\n- Find longest common subsequence u = u1 ... uk\n- Algorithm:\nF(i-1, j)\n- F(i, j) = max F(i, j-1)\nF(i-1, j-1) + [1, if s = tj; 0 otherwise]\ni\n- Hirschberg's algorithm solves this in linear space\n\nIntroduction: Compute optimal score\nIt is easy to compute F(M, N) in linear space\nF(i,j)\nAllocate ( column[1] )\nAllocate ( column[2] )\nFor\ni = 1....M\nIf\ni > 1, then:\nFree( column[i - 2] )\nAllocate( column[ i ] )\nFor j = 1...N\nF(i, j) = ...\n\nLinear-space alignment\nTo compute both the optimal score and the optimal alignment:\nDivide & Conquer approach:\nNotation:\nr\nx , yr: reverse of x, y\nE.g. x = accgg;\nrx = ggcca\nr\nr\nFr(i, j): optimal score of aligning xr\n1...x\n& yr\n1...y j\ni\nsame as F(M-i+1, N-j+1)\n\nLinear-space alignment\nLemma:\nF(M, N) = maxk=0...N( F(M/2, k) + Fr(M/2, N-k) )\nx\ny\nM/2\nk*\nFr(M/2, N-k)\nF(M/2, k)\n\nLinear-space alignment\n- Now, using 2 columns of space, we can compute\nfor k = 1...M, F(M/2, k), Fr(M/2, N-k)\nPLUS the backpointers\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-space alignment\n- Iterate this procedure to the left and right!\nk*\nN-k*\nM/2\nM/2\n\nLinear-space alignment\nHirschberg's Linear-space algorithm:\nMEMALIGN(l, l', r, r'):\n(aligns x ...xl' with yr...yr')\nl\n1.\nLet h = a(l'-l)/2o\n2.\nFind in Time O((l' - l) u (r'-r)), Space O(r'-r)\nthe optimal path,\nLh, entering column h-1, exiting column h\nLet k1 = pos'n at column h - 2 where Lh enters\nk2 = pos'n at column h + 1 where Lh exits\n3.\nMEMALIGN(l, h-2, r, k1)\n4.\nOutput Lh\n5.\nMEMALIGN(h+1, l', k2, r')\nTop level call: MEMALIGN(1, M, 1, N)\n\nLinear-space alignment\nTime, Space analysis of Hirschberg's algorithm:\nTo compute optimal path at middle column,\nFor box of size M u N,\nSpace:\n2N\nTime:\ncMN,\nfor some constant c\nThen, left, right calls cost c( M/2 u k* + M/2 u (N-k*) ) = cMN/2\nAll recursive calls cost\nTotal Time: cMN + cMN/2 + cMN/4 + ..... = 2cMN = O(MN)\nTotal Space: O(N) for computation,\nO(N+M) to store the optimal alignment\n\nThe Four-Russian Algorithm\nA useful speedup of Dynamic Programming\n\nMain Observation\nWithin a rectangle of the DP\nmatrix,\nvalues of D depend only\non the values of A, B, C,\nand substrings xl...l', yr...r'\nDefinition:\nA t-block is a t u t square of\nthe DP matrix\nIdea:\nDivide matrix in t-blocks,\nPrecompute t-blocks\nSpeedup: O(t)\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nMain structure of the algorithm:\n-\nDivide NuN DP matrix into KuK\nlog2N-blocks that overlap by 1\ncolumn & 1 row\n-\nFor i = 1......K\n-\nFor j = 1......K\n-\nCompute Di,j as a function of\nAi,j, Bi,j, Ci,j, x[li...l'i], y[rj...r'j]\nTime: O(N2 / log2N)\ntimes the cost of step 4\nt\nt\nt\n\nThe Four-Russian Algorithm\nAnother observation:\n( Assume m = 0, s = 1, d = 1 )\nLemma. Two adjacent cells of F(.,.) differ by at most 1\nGusfield's book covers case where m = 0,\ncalled the edit distance (p. 216):\nminimum # of substitutions + gaps to transform one string to another\n\nThe Four-Russian Algorithm\nProof of Lemma:\n1.\nSame row:\na. F(i, j) - F(i - 1, j) d +1\nAt worst, one more gap:\nx1......xi-1 xi\ny1......yj -\nb. F(i, j) - F(i - 1, j) t -1\nF(i, j)\nF(i - 1, j - 1)\nF(i, j) - F(i - 1, j - 1)\nx ......x\nx\nx1......x\n-\ni-1\ni\ni-1\ny1......ya-1ya ya+1...yj\ny1......ya-1ya ya+1...yj\nt -1\nx1......x\nx\nx ......x\ni-1\ni\ni-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\n+1\n2.\nSame column: similar argument\n\nThe Four-Russian Algorithm\nProof of Lemma:\n3.\nSame diagonal:\na. F(i, j) - F(i - 1, j - 1) d +1\nAt worst, one additional mismatch in F(i, j)\nb. F(i, j) - F(i - 1, j - 1) t -1\nF(i, j)\nx1......x\nx\ni-1\ni\n|\ny1......yi-1 yj\nx1......x\nx\ni-1\ni\ny1......ya-1- ya...yj\nF(i - 1, j - 1)\nx ......x\ni-1\ny1......yj-1\nx ......x\ni-1\ny1......ya-1ya...yj\nF(i, j) - F(i - 1, j - 1)\nt-1\n+1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset vector is a\nt-long vector of values\nfrom {-1, 0, 1},\nwhere the first entry is 0\nIf we know the value at A,\nand the top row, left column\noffset vectors,\nand xl......xl', yr......yr',\nThen we can find D\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n5A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n1A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset function of a\nt-block\nis a function that for any\ngiven offset vectors\nof top row, left column,\nand xl......xl', yr......yr',\nproduces offset vectors\nof bottom row, right\ncolumn\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nWe can pre-compute the offset function:\n2(t-1) possible input offset vectors\n2t possible strings x ......xl', yr......yr'\nl\nTherefore 32(t-1) u 42t values to pre-compute\nWe can keep all these values in a table, and look up in linear time,\nor in O(1) time if we assume\nconstant-lookup RAM for log-sized inputs\n\nThe Four-Russian Algorithm\nFour-Russians Algorithm: (Arlazarov, Dinic, Kronrod,\nFaradzev)\n1. Cover the DP table with t-blocks\n2. Initialize values F(.,.) in first row & column\n3. Row-by-row, use offset values at leftmost column and top\nrow of each block, to find offset values at rightmost column\nand bottom row\n4. Let Q = total of offsets at row N\nF(N, N) = Q + F(N, 0)\n\nThe Four-Russian Algorithm\nt\nt\nt"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/01f55f348ea1e95f7015bd1b40586012_lecture5.pdf",
      "content": "6.096 - Algorithms for Computational Biology\nSequence Alignment\nand Dynamic Programming\nLecture 1 - Introduction\nLecture 2 - Hashing and BLAST\nLecture 3 - Combinatorial Motif Finding\nLecture 4 - Statistical Motif Finding\n\nChallenges in Computational Biology\n4 Genome Assembly\nRegulatory motif discovery\n1 Gene Finding\nDNA\n2 Sequence alignment\n6 Comparative Genomics\nTCATGCTAT\nTCGTGATAA\n3 Database lookup\nTGAGGATAT\n7 Evolutionary Theory\nTTATCATAT\nTTATGATTT\nGene expression analysis\nRNA transcript\nProtein network analysis\nGibbs sampling\nCluster discovery\n12 Regulatory network inference\nEmerging network properties\n\nComparing Genomes\nImages removed due to copyright restrictions.\nSus scrofa\nPeromyscus\nPeromyscus\nHuminid\nHuminid\nDiperta\nDiperta\nDiperta\nDiperta\nInvertebrates\nInvertebrates\n\nA C G T C A T C A\nT A\nG T G\nT C A\nComparing two DNA sequences\n- Given two possibly related strings S1 and S2\n- What is the longest common subsequence?\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nS1\n\nA C G T C A T C A\nT A\nG T G\nT C A\nA\nG T\nT C A\nEdit distance:\n-Number of changes\nS2\n\nneeded for S1ÆS2\nLCSS\n\nHow can we compute best alignment\nS1\nS2\nA C G T C A T C A\nT A G T G T C A\n- Need scoring function:\n- Score(alignment) = Total cost of editing S1 into S2\n- Cost of mutation\n- Cost of insertion / deletion\n- Reward of match\n- Need algorithm for inferring best alignment\n- Enumeration?\n- How would you do it?\n- How many alignments are there?\n\nWhy we need a smart algorithm\n- Ways to align two sequences of length m, n\n⎛\n⎞\nm\n2 + n\n+\n= (\n)! ≈\nn m\nm + n\n⎜⎝\n⎟⎠\n⋅\nπ\n(m!) 2\nm\nm\n- For two sequences of length n\nn\nEnumeration Today's lecture\n184,756\n1.40E+11\n9.00E+58\n10,000\n\nKey insight: score is additive!\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\ni\nj\n- Compute best alignment recursively\n- For a given split (i, j), the best alignment is:\n-\nBest alignment of S1[1..i]\nand S2[1..j]\n- + Best alignment of S1[\ni..n] and S2[\nj..m]\ni\ni\nA C G\nT C A T C A\nT A G T G\nT C A\nS1\nS2\nj\nj\n\nKey insight: re-use computation\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nA\nC G T\nT A G\nT G\nS1\nS2\nA C G T C A T C A\nT A G T G T C A\nS1\nS2\nS2\nA C G T\nC A T C A\nT A G T G\nT C A\nS1\nS2\nA\nC G T C A T C A\nT A G\nT G T C A\nS1\nC G T\nC A T C A\nT G\nT C A\nS1\nS2\nIdentical sub-problems! We can reuse our work!\n\nSolution #1 - Memoization\n- Create a big dictionary, indexed by aligned seqs\n- When you encounter a new pair of sequences\n- If it is in the dictionary:\n- Look up the solution\n- If it is not in the dictionary\n- Compute the solution\n- Insert the solution in the dictionary\n- Ensures that there is no duplicated work\n- Only need to compute each sub-alignment once!\nTop down approach\n\nSolution #2 - Dynamic programming\n- Create a big table, indexed by (i,j)\n- Fill it in from the beginning all the way till the end\n- You know that you'll need every subpart\n- Guaranteed to explore entire search space\n- Ensures that there is no duplicated work\n- Only need to compute each sub-alignment once!\n- Very simple computationally!\nBottom up approach\n\nKey insight: Matrix representation of alignments\nA C G T C A T C A\nT A\nG T G\nT C A\nS1\nS2\nA C G T C A T C A\nT\nA\nG\nT\nG\nT\nC\nA\nA\nG\nT\nC/G\nT\nC\nA\nGoal:\nFind best path\nthrough the matrix\n\nSequence alignment\nDynamic Programming\nGlobal alignment\n\n0. Setting up the scoring matrix\n-\nA\nG\nT\nA\nA\nG\nC\n-\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n}\nTermination:\n-\nTop right: 0\nBottom right\n\n1. Allowing gaps in s\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-8\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n}\nTermination:\n-\nTop right: 0\n) - 2\nBottom right\n\n2. Allowing gaps in t\n-\nA\nG\nT\n-2\n-4\n-6\n-2\n-4\n-6\n-8\n-4\n-6\n-8\n-10\n-6\n-8\n-10\n-12\n-8\n-10\n-12\n-14\nInitialization:\n-\n- Top right: 0\nUpdate Rule:\nA(i,j)=max{\nA\n- A(i-1 , j ) - 2\n- A( i , j-1) - 2\nA\n}\nTermination:\nG\n- Bottom right\nC\n\n3. Allowing mismatches\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-3\n-5\n-4\n-3\n-2\n-4\n-6\n-5\n-4\n-3\n-8\n-7\n-6\n-5\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) -1\n}\nTermination:\n-\nTop right: 0\n) - 2\n-1) - 2\nBottom right\n\n4. Choosing optimal paths\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-3\n-5\n-4\n-3\n-2\n-4\n-6\n-5\n-4\n-3\n-8\n-7\n-6\n-5\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) -1\n}\nTermination:\n-\nTop right: 0\n) - 2\n-1) - 2\nBottom right\n\n5. Rewarding matches\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-3\n-4\n-1\n-2\n-6\n-3\n-1\n-8\n-5\n-2\n-1\n-1\n-1\n-1\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) ±1\n}\nTermination:\n-\nTop right: 0\n) - 2\n-1) - 2\nBottom right\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nDynamic Programming\n\nSemi-Global Motivation\n- Aligning the following sequences\nCAGCACTTGGATTCTCGG\nCAGC-----G-T----GG\nvvvv-----v-v----vv = 8(1)+0(-1)+10(-2) = -12\n- We might prefer the alignment\nCAGCA-CTTGGATTCTCGG\nmatch\nmismatch\ngap\n---CAGCGTGG--------\n---vv-vxvvv-------- = 6(1)+1(-1)+12(-2) = -19\n- New qualities sought, new scoring scheme\ndesigned\n- Intuitively, don't penalize \"missing\" end of the\nsequence\n- We'd like to model this intuition\n\nIgnoring starting gaps\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-1\n-1\n-2\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n/\nl\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) ±1\n}\nTermination:\n-\n- 1st row co : 0\n) - 2\n-1) - 2\nBottom right\n\nIgnoring trailing gaps\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-1\n-1\n-2\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) ±1\n}\nTermination:\n- max(last row/col)\n1st row/col: 0\n) - 2\n-1) - 2\n\nUsing the new scoring scheme\n- With the old scoring scheme (all gaps count -2)\nCAGCACTTGGATTCTCGG\nCAGC-----G-T----GG\n- New score (end gaps are free)\nCAGCA-CTTGGATTCTCGG\n---CAGCGTGG--------\n8(1)+0(-1)+10(-2)+0(-0) = -12\n6(1)+1(-1)+1(-2)+11(-0) = 3\nmatch\nmismatch\ngap\nendgap\nvvvv-----v-v----vv =\n---vv-vxvvv-------- =\n\nSemi-global alignments\n- Applications:\nquery\n- Finding a gene in a genome\n- Aligning a read onto an assembly\n- Finding the best alignment of a PCR primer\nsubject\n- Placing a marker onto a chromosome\n- These situations have in common\n- One sequence is much shorter than the other\n- Alignment should span the entire length of the smaller\nsequence\n- No need to align the entire length of the longer sequence\n- In our scoring scheme we should\n- Penalize end-gaps for subject sequence\n- Do not penalize end-gaps for query sequence\n\nSemi-Global Alignment\n-\nA\nG\nT\n-\nA\nA\nG\nC\nQuery: s\nSubject: t\nalign all of s\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n-\nA(i-1 , j\n-\nA( i , j\n-\nA(i-1 , j-1) ±1\n}\nTermination:\n-\n-2\n-4\n-6\n-1\n-1\n-2\n-1\n-1\n...or...\n-2\n-4\n-6\n-2\n-1\n-1\n-4\n-2\n-6\n-1\n-8\n-1\n-1\n-\nA\nG\nT\nA\nA\nG\nC\n-\nInitialization:\n-1st row\nA(i,j)=max{\n-A(i-1 , j\n-A( i , j\n-A(i-1 , j-1) ±1\n}\nTermination:\n-max(last row)\nQuery: t\nSubject: s\nalign all of t\n1st col\n) - 2\n-1) - 2\nmax(last col)\nUpdate Rule:\n) - 2\n-1) - 2\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nDynamic Programming\n\nIntro to Local Alignments\n- Statement of the problem\n- A local alignment of strings s and t\ns\nis an alignment of a substring of s\nwith a substring of t\nt\n- Definitions (reminder):\n- A substring consists of consecutive characters\n- A subsequence of s needs not be contiguous in s\n- Naive algorithm\n- Now that we know how to use dynamic programming\n- Take all O((nm)2), and run each alignment in O(nm) time\n- Dynamic programming\n- By modifying our existing algorithms, we achieve O(mn)\n\nGlobal Alignment\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-5\n-4\n-2\n-6\n-1\n-8\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) ±1\n}\nTermination:\n-\nTop left: 0\n) - 2\n-1) - 2\nBottom right\n\nLocal Alignment\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-1\nInitialization:\n-\nUpdate Rule:\nA(i,j)=max{\n- A(i-1 , j\n- A( i , j\n- A(i-1 , j-1) ±1\n- 0\n}\nTermination:\n- Anywhere\n-1\nTop left: 0\n) - 2\n-1) - 2\n\nLocal Alignment issues\n- Resolving ambiguities\n- When following arrows back, one can stop at any of the zero\nentries. Only stop when no arrow leaves. Longest.\n- Correctness sketch by induction\n- Assume we've correctly aligned up to (i,j)\n- Consider the four cases of our max computation\n- By inductive hypothesis recurse on (i-1,j-1), (i-1,j), (i,j-1)\n- Base case: empty strings are suffixes aligned optimally\n- Time analysis\n- O(mn) time\n- O(mn) space, can be brought to O(m+n)\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nAffine Gap Penalty\nDynamic Programming\n\nScoring the gaps more accurately\nCurrent model:\nGap of length\nn\nγ(n)\nincurs penalty\nn×d\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nγ(n):\nfor all n, γ(n + 1) - γ(n) ≤γ(n) - γ(n - 1)\nγ(n)\n\nGeneral gap dynamic programming\nInitialization:\nsame\nIteration:\nF(i-1, j-1) + s(xi, yj)\nF(i, j)\n= max maxk=0...i-1F(k,j) - γ(i-k)\nmaxk=0...j-1F(i,k) - γ(j-k)\nTermination:\nsame\nRunning Time: O(N2M)\n(assume N>M)\nSpace:\nO(NM)\n\nCompromise: affine gaps\nγ(n) = d + (n - 1)×e\n|\n|\nγ(n)\ngap\ngap\nopen\nextend\nd\nTo compute optimal alignment,\ne\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\nF(i, j): score of alignment x1...xi\nif xi aligns to yj\nto y1...yj\nG(i, j): score if xi, or yj, aligns to a gap\n\nMotivation for affine gap penalty\n- Modeling evolution\n- To introduce the first gap, a break must occur in DNA\n- Multiple consecutive gaps likely to be introduced by the same\nevolutionary event. Once the break is made, it's relatively easy\nto make multiple insertions or deletions.\n- Fixed cost for opening a gap: p+q\n- Linear cost increment for increasing number of gaps: q\n- Affine gap cost function\n- New gap function for length k: w(k) = p+q*k\n- p+q is the cost of the first gap in a run\n- q is the additional cost of each additional gap in same run\n\nAdditional Matrices\n- The amount of state needed increases\n- In scoring a single entry in our matrix, we need\nremember an extra piece of information\n- Are we continuing a gap in s? (if not, start is more\nexpensive)\n- Are we continuing a gap in t? (if not, start is more\nexpensive)\n- Are we continuing from a match between s(i) and t(j)?\n- Dynamic programming framework\n- We encode this information in three different states\nfor each element (i,j) of our alignment. Use three\nmatrices\n- a(i,j): best alignment of s[1..i] & t[1..j] that aligns s[i] with t[j]\n- b(i,j): best alignment of s[1..i] & t[1..j] that aligns gap with t[j]\n- c(i,j): best alignment of s[1..i] & t[1..j] that aligns s[i] with gap\n\nUpdate rules\n( i\na - j - 1) ⎞\n,1\nWhen s[j] and t[j] are aligned\n⎛⎜\n⎜\n⎜⎝\nScore can be\ndifferent for each\npair of chars\n⎟\n⎟\n⎟⎠\n( i\na\nj\n( [i\ns\nscore\nt j\n(i\nb\nj\n) =\n], [ ]) +\n,1\n1)\n-\n-\nmax\n,\n(i\nc\nj\n,1\n1)\n-\n-\nWhen t[j] aligns with a gap in s\n(i\na\nj 1)\n(\n)\n- -\n⎛⎜\n⎜\n⎜\n⎞⎟\n⎟\n⎟\n+\np q\nstarting a gap in s\n,\nand starting one in s\nWhen s[i] aligns with a gap in t\n( i\nb\nj\n(i\nb\nj\n)\n1)\n- -\nextending a gap in s\nq\n= max\n,\n,\n(i\nc\nj 1)\n(\n)\n- -\nStopping a gap in t,\n+\np q\n⎝\n⎠\n,\n( i\na\nj\n,1 ) (\n)\n-\n-\n⎛⎜\n⎜\n⎜\n⎞⎟\n⎟\n⎟\n+\np q\n(i\nc\nj\n(i\nc\nj\n) = max\n,1 )\n-\n- q\n,\n(i\nb\nj\n,1 ) (\n)\n-\n-\n+\np q\n⎝\n⎠\nFind maximum over all three arrays max(a[m,n],b[m,n],c[m,n]).\nFollow arrows back, skipping from matrix to matrix\n\nSimplified rules\n- Transitions from b to c are not necessary...\n...if the worst mismatch costs less than p+q\nACC-GGTA\nACCGGTA\nA--TGGTA\nA-TGGTA\n(\nWhen s[j] and t[j] are aligned\n⎛ i\na - ,1 j - 1)⎞⎟ Score can be\n⎜\n(\n[ ],\n(\ni\na , j ) = score (\nt\ni\ns\n[ j ]) + max ⎜ i\nb - ,1 j - 1)⎟ different for each\n⎜ (\npair of chars\n⎝ i\nc - ,1 j - 1) ⎠⎟\nWhen t[j] aligns with a gap in s\n⎛ i\na , j - 1) - ( p + q )⎞\n(\nstarting a gap in s\n(i\nb , j ) = max\n⎝ i\nb , j - 1) - q\n⎠⎟\n⎜(\nextending a gap in s\nWhen s[i] aligns with a gap in t\n⎛ i\na - ,1 j ) - ( p + q )⎞\n(\n(i\nc , j ) = max\n⎝ i\nc - ,1 j ) - q\n⎠⎟\n⎜(\n\nGeneral Gap Penalty\n- Gap penalties are limited by the amount of state\n- Affine gap penalty: w(k) = k*p\n- State: Current index tells if in a gap or not\n- Linear gap penalty: w(k) = p + q*k, where q<p\n- State: add binary value for each sequence: starting a gap or not\n- What about quadriatic: w(k) = p+q*k+rk2.\n- State: needs to encode the length of the gap, which can be O(n)\n- To encode it we need O(log n) bits of information. Not feasible\n- What about a (mod 3) gap penalty for protein alignments\n- Gaps of length divisible by 3 are penalized less: conserve frame\n- This is feasible, but requires more possible states\n- Possible states are: starting, mod 3=1, mod 3=2, mod 3=0\n\nSequence alignment\nGlobal Alignment\nSemi-Global\nLocal Alignment\nLinear Gap Penalty\nVariations on the Theme\nDynamic Programming\n\nDynamic Programming Versatility\n- Unified framework\n- Dynamic programming algorithm. Local updates.\n- Re-using past results in future computations.\n- Memory usage optimizations\n- Tools in our disposition\n- Global alignment: entire length of two orthologous genes\n- Semi-global alignment: piece of a larger sequence aligned\nentirely\n- Local alignment: two genes sharing a functional domain\n- Linear Gap Penalty: penalize first gap more than subsequent\ngaps\n- Edit distance, min # of edit operations. M=0, m=g=-1, every\noperation subtracts 1, be it mutation or gap\n- Longest common subsequence: M=1, m=g=0. Every match\nadds one, be it contiguous or not with previous.\n\nDP Algorithm Variations\nt\ns\nt\ns\nt\ns\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-2\n-1\n-1\n-4\n-1\n-1\n-2\n-6\n-1\n-8\n-3\n-1\nGlobal Alignment\nSemi-Global Alignment\nLocal Alignment\n-\nA\nG\nT\nA\nA\nG\nC\n-\n-2\n-4\n-6\n-1\n-1\n-2\n-1\n-1\n-\nA\nG\nT\nA\nA\nG\nA\n-\n\nBounded Dynamic Programming\nInitialization:\nF(i,0), F(0,j) undefined for i, j > k\nIteration:\nFor i = 1...M\nFor j = max(1, i - k)...min(N, i+k)\nF(i - 1, j - 1)+ s(xi, yj)\nF(i, j) = max\nF(i, j - 1) - d, if j > i - k(N)\nF(i - 1, j) - d, if j < i + k(N)\nTermination:\nsame\nEasy to extend to the affine gap case\nx1 .............................. xM\ny1 .............................. yN\nk(N)\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-Space Alignment\n\nHirschberg's algorithm\n- Longest common subsequence\n- Given sequences s = s1 s2 ... sm, t = t1 t2 ... tn,\n- Find longest common subsequence u = u1 ... uk\n- Algorithm:\nF(i-1, j)\n- F(i, j) = max F(i, j-1)\nF(i-1, j-1) + [1, if s = tj; 0 otherwise]\ni\n- Hirschberg's algorithm solves this in linear space\n\nIntroduction: Compute optimal score\nIt is easy to compute F(M, N) in linear space\nF(i,j)\nAllocate ( column[1] )\nAllocate ( column[2] )\nFor\ni = 1....M\nIf\ni > 1, then:\nFree( column[i - 2] )\nAllocate( column[ i ] )\nFor j = 1...N\nF(i, j) = ...\n\nLinear-space alignment\nTo compute both the optimal score and the optimal alignment:\nDivide & Conquer approach:\nNotation:\nxr, yr: reverse of x, y\nE.g. x = accgg;\nxr = ggcca\nFr(i, j): optimal score of aligning xr\n1...xr & yr\n1...yr\nj\ni\nsame as F(M-i+1, N-j+1)\n\nLinear-space alignment\nLemma:\nF(M, N) = maxk=0...N( F(M/2, k) + Fr(M/2, N-k) )\nx\ny\nM/2\nk*\nFr(M/2, N-k)\nF(M/2, k)\n\nLinear-space alignment\n- Now, using 2 columns of space, we can compute\nfor k = 1...M, F(M/2, k), Fr(M/2, N-k)\nPLUS the backpointers\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-space alignment\n- Iterate this procedure to the left and right!\nN-k*\nk*\nM/2\nM/2\n\nLinear-space alignment\nHirschberg's Linear-space algorithm:\nMEMALIGN(l, l', r, r'):\n(aligns x...xl' with yr...yr')\nl\n1.\nLet h = ⎡(l'-l)/2⎤\n2.\nFind in Time O((l' - l) × (r'-r)), Space O(r'-r)\nthe optimal path,\nLh, entering column h-1, exiting column h\nLet k1 = pos'n at column h - 2 where Lh enters\nk2 = pos'n at column h + 1 where Lh exits\n3.\nMEMALIGN(l, h-2, r, k1)\n4.\nOutput Lh\n5.\nMEMALIGN(h+1, l', k2, r')\nTop level call: MEMALIGN(1, M, 1, N)\n\nLinear-space alignment\nTime, Space analysis of Hirschberg's algorithm:\nTo compute optimal path at middle column,\nFor box of size M × N,\nSpace:\n2N\nTime:\ncMN,\nfor some constant c\nThen, left, right calls cost c( M/2 × k* + M/2 × (N-k*) ) = cMN/2\nAll recursive calls cost\nTotal Time: cMN + cMN/2 + cMN/4 + ..... = 2cMN = O(MN)\nTotal Space: O(N) for computation,\nO(N+M) to store the optimal alignment\n\nThe Four-Russian Algorithm\nA useful speedup of Dynamic Programming\n\nMain Observation\nWithin a rectangle of the DP\nmatrix,\nvalues of D depend only\non the values of A, B, C,\nand substrings xl...l', yr...r'\nDefinition:\nA t-block is a t × t square of\nthe DP matrix\nIdea:\nDivide matrix in t-blocks,\nPrecompute t-blocks\nSpeedup: O(t)\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nMain structure of the algorithm:\n-\nDivide N×N DP matrix into K×K\nlog2N-blocks that overlap by 1\ncolumn & 1 row\n-\nFor i = 1......K\n-\nFor j = 1......K\n-\nCompute Di,j as a function of\nAi,j, Bi,j, Ci,j, x[li...l'i], y[rj...r'j]\nTime: O(N2 / log2N)\ntimes the cost of step 4\nt\nt\nt\n\nThe Four-Russian Algorithm\nAnother observation:\n( Assume m = 0, s = 1, d = 1 )\nLemma. Two adjacent cells of F(.,.) differ by at most 1\nGusfield's book covers case where m = 0,\ncalled the edit distance (p. 216):\nminimum # of substitutions + gaps to transform one string to another\n\nThe Four-Russian Algorithm\nProof of Lemma:\n1.\nSame row:\na. F(i, j) - F(i - 1, j) ≤ +1\nAt worst, one more gap:\nx1......xi-1 xi\ny1......yj -\nb. F(i, j) - F(i - 1, j) ≥ -1\nF(i, j)\nF(i - 1, j - 1)\nF(i, j) - F(i - 1, j - 1)\nx1......xi-1 -\nx1......xi-1 xi\ny1......ya-1ya ya+1...yj\ny1......ya-1ya ya+1...yj\n≥ -1\nx1......xi-1 xi\nx1......xi-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\n+1\n2.\nSame column: similar argument\n\nThe Four-Russian Algorithm\nProof of Lemma:\n3.\nSame diagonal:\na. F(i, j) - F(i - 1, j - 1) ≤ +1\nAt worst, one additional mismatch in F(i, j)\nb. F(i, j) - F(i - 1, j - 1) ≥ -1\nF(i, j)\nF(i - 1, j - 1)\nF(i, j) - F(i - 1, j - 1)\nx1......xi-1 xi\nx1......xi-1\n|\ny1......yi-1 yj\ny1......yj-1\n≥-1\nx1......xi-1 xi\nx1......xi-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\n+1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset vector is a\nt-long vector of values\nfrom {-1, 0, 1},\nwhere the first entry is 0\nIf we know the value at A,\nand the top row, left column\noffset vectors,\nand xl......xl', yr......yr',\nThen we can find D\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n5A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n1A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset function of a\nt-block\nis a function that for any\ngiven offset vectors\nof top row, left column,\nand xl......xl', yr......yr',\nproduces offset vectors\nof bottom row, right\ncolumn\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nWe can pre-compute the offset function:\n2(t-1) possible input offset vectors\n2t possible strings x......xl', yr......yr'\nl\nTherefore 32(t-1) × 42t values to pre-compute\nWe can keep all these values in a table, and look up in linear time,\nor in O(1) time if we assume\nconstant-lookup RAM for log-sized inputs\n\nThe Four-Russian Algorithm\nFour-Russians Algorithm: (Arlazarov, Dinic, Kronrod,\nFaradzev)\n1. Cover the DP table with t-blocks\n2. Initialize values F(.,.) in first row & column\n3. Row-by-row, use offset values at leftmost column and top\nrow of each block, to find offset values at rightmost column\nand bottom row\n4. Let Q = total of offsets at row N\nF(N, N) = Q + F(N, 0)\n\nThe Four-Russian Algorithm\nt\nt\nt\n\nEvolution at the DNA level\n...ACGGTGCAGTCACCA...\n...ACGTTGCAGTCCACCA...\nC\nSequence Changes\nComputing best alignment\n-In absence of gaps\n\nSequence Alignment\nAGGCTATCACCTGACCTCCAGGCCGATGCCC\nTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---\nTAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\nDefinition\nGiven two strings\nx = x1x2...xM, y = y1y2...yN,\nan alignment is an assignment of gaps to positions\n0,..., M in x, and 0,..., N in y, so as to line up each\nletter in one sequence with either a letter, or a gap\nin the other sequence\n\nScoring Function\n- Sequence edits:\nAGGCCTC\n- Mutations\nAGGACTC\n- Insertions\nAGGGCCTC\n- Deletions\nAGG.CTC\nScoring Function:\nMatch:\n+m\nMismatch: -s\nGap:\n-d\nScore F = (# matches) × m - (# mismatches) × s - (#gaps) ×\nd\n\nHow do we compute the best alignment?\nAGTGACCTGGGAAGACCCTGACCCTGGGTCACAAAACTC\nAGTGCCCTGGAACCCTGACGGTGGGTCACAAAACTTCTGGA\nToo many possible\nalignments:\nO( 2M+N)\n\nAlignment is additive\nObservation:\nThe score of aligning\nx1......xM\ny1......yN\nis additive\nSay that\nx1...xi\nxi+1...xM\naligns to\ny1...yj\nyj+1...yN\nThe two scores add up:\nF(x[1:M], y[1:N]) = F(x[1:i], y[1:j]) + F(x[i+1:M], y[j+1:N])\n\nDynamic Programming\n- We will now describe a dynamic programming\nalgorithm\nSuppose we wish to align\nx1......xM\ny1......yN\nLet\nF(i,j) = optimal score of aligning\nx1......xi\ny1......yj\n\nDynamic Programming (cont'd)\nNotice three possible cases:\n1.\nxi aligns to yj\nx1......xi-1 xi\ny1......yj-1 yj\nm, if xi = yj\nF(i,j) = F(i-1, j-1) +\n2.\nxi aligns to a gap\n-s, if not\nx1......xi-1 xi\ny1......y\n-\nj\n3.\nyj aligns to a gap\nF(i,j) = F(i-1, j) - d\nx1......x\n-\ni\ny1......yj-1 y j\nF(i,j) = F(i, j-1) - d\n\nDynamic Programming (cont'd)\n- How do we know which case is correct?\nInductive assumption:\nF(i, j-1), F(i-1, j), F(i-1, j-1) are optimal\nThen,\nF(i-1, j-1) + s(x , yj)\nF(i, j) = max\ni\nF(i-1, j) - d\nF( i, j-1) - d\nWhere\ns(x , yj) = m, if x = yj; -s, if not\ni\ni\n\nExample\nx = AGTA\ny = ATA\nF(i,j)\ni = 0\nj = 0\nA\nG\nT\nA\n-1\n-2\n-3\n-4\nA\n-1\n-1\n-2\nT\n-2\nA\n-3\n-1\n-1\nm = 1\ns = -1\nd = -1\nOptimal Alignment:\nF(4,3) = 2\nAGTA\nA - TA\n\nThe Needleman-Wunsch Matrix\nx\ny1 .................................... yN\n1 .................................... xM\nEvery nondecreasing\npath\nfrom (0,0) to (M, N)\ncorresponds to\nan alignment\nof the two sequences\nCan think of it as a\ndivide-and-conquer algorithm\n\nThe Needleman-Wunsch Algorithm\n1.\nInitialization.\na.\nF(0, 0)\n= 0\nb.\nF(0, j)\n= - j × d\nc.\nF(i, 0)\n= - i × d\n2.\nMain Iteration. Filling-in partial alignments\na.\nFor each\ni = 1......M\nFor each\nj = 1......N\nF(i-1,j-1) + s(x , yj)\ni\nF(i, j)\n= max\nF(i-1, j) - d\nF(i, j-1) - d\nDIAG,\nif [case 1]\nPtr(i,j)\n=\nLEFT,\nif [case 2]\nUP,\nif [case 3]\n3.\nTermination. F(M, N) is the optimal score, and\nfrom Ptr(M, N) can trace back optimal alignment\n[case 1]\n[case 2]\n[case 3]\n\nPerformance\n- Ti\nO(NM)\nO(NM)\n-\nme:\n- Space:\nLater we will cover more efficient methods\n\nA variant of the basic algorithm:\n- Maybe it is OK to have an unlimited # of gaps in\nthe beginning and end:\n----------CTATCACCTGACCTCCAGGCCGATGCCCCTTCCGGC\nGCGAGTTCATCTATCAC--GACCGC--GGTCG--------------\n- Then, we don't want to penalize gaps in the ends\n\nDifferent types of overlaps\n\nThe Overlap Detection variant\nChanges:\n1. Initialization\nx1 .................................... xM\ny 1 .................................... y N\nFor all i, j,\nF(i, 0) = 0\nF(0, j) = 0\n2. Termination\nmaxi F(i,\nN)\nFOPT = max\nmaxj F(M,\nj)\n\nThe local alignment problem\nGiven two strings x = x1......xM,\ny = y1......yN\n(optimal global alignment value)\nis maximum\ne.g.\nx = aaaacccccgggg\ny = cccgggaaccaacc\nFind substrings x', y' whose similarity\n\nWhy local alignment\n- Genes are shuffled between genomes\n- Portions of proteins (domains) are often conserved\nImage removed due to copyright restrictions.\n\nCross-species genome similarity\n- 98% of genes are conserved between any two mammals\n- >70% average similarity in protein sequence\nhum_a : GTTGACAATAGAGGGTCTGGCAGAGGCTC--------------------- @ 57331/400001\nmus_a : GCTGACAATAGAGGGGCTGGCAGAGGCTC--------------------- @ 78560/400001\nrat_a : GCTGACAATAGAGGGGCTGGCAGAGACTC--------------------- @ 112658/369938\nfug_a : TTTGTTGATGGGGAGCGTGCATTAATTTCAGGCTATTGTTAACAGGCTCG @ 36008/68174\nhum_a : CTGGCCGCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 57381/400001\nmus_a : CTGGCCCCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 78610/400001\nrat_a : CTGGCCCCGGTGCGGAGCGTCTGGAGCGGAGCACGCGCTGTCAGCTGGTG @ 112708/369938\nfug_a : TGGGCCGAGGTGTTGGATGGCCTGAGTGAAGCACGCGCTGTCAGCTGGCG @ 36058/68174\n\"atoh\" enhancer in\nhuman, mouse,\nhum_a : AGCGCACTCTCCTTTCAGGCAGCTCCCCGGGGAGCTGTGCGGCCACATTT @ 57431/400001\nmus_a : AGCGCACTCG-CTTTCAGGCCGCTCCCCGGGGAGCTGAGCGGCCACATTT @ 78659/400001\nrat, fugu fish\nrat_a : AGCGCACTCG-CTTTCAGGCCGCTCCCCGGGGAGCTGCGCGGCCACATTT @ 112757/369938\nfug_a : AGCGCTCGCG------------------------AGTCCCTGCCGTGTCC @ 36084/68174\nhum_a : AACACCATCATCACCCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 57481/400001\nmus_a : AACACCGTCGTCA-CCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 78708/400001\nrat_a : AACACCGTCGTCA-CCCTCCCCGGCCTCCTCAACCTCGGCCTCCTCCTCG @ 112806/369938\nfug_a : CCGAGGACCCTGA------------------------------------- @ 36097/68174\n\nThe Smith-Waterman algorithm\nIdea: Ignore badly aligning regions\nModifications to Needleman-Wunsch:\nInitialization:\nF(0, j) = F(i, 0) = 0\nIteration:\nF(i, j) = max\nF(i - 1, j) - d\nF(i, j - 1) - d\nF(i - 1, j - 1) + s(x, yj)\ni\n\nThe Smith-Waterman algorithm\nTermination:\n1. If we want the best local alignment...\nFOPT = maxi,j F(i, j)\n2. If we want all local alignments scoring > t\nFor all i, j find F(i, j) > t, and trace back\n\nScoring the gaps more accurately\nCurrent model:\nGap of length\nn\nγ(n)\nincurs penalty\nn×d\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nγ(n):\nfor all n, γ(n + 1) - γ(n) ≤γ(n) - γ(n - 1)\nγ(n)\n\nGeneral gap dynamic programming\nInitialization:\nsame\nIteration:\nF(i-1, j-1) + s(xi, yj)\nF(i, j)\n= max maxk=0...i-1F(k,j) - γ(i-k)\nmaxk=0...j-1F(i,k) - γ(j-k)\nTermination:\nsame\nRunning Time: O(N2M)\n(assume N>M)\nSpace:\nO(NM)\n\nCompromise: affine gaps\nγ(n) = d + (n - 1)×e\n|\n|\nγ(n)\ngap\ngap\nopen\nextend\nd\nTo compute optimal alignment,\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\ne\nF(i, j): score of alignment x1...x to y1...yj\ni\nifif xi aligns to yj\nG(i, j): score if x, or yj, aligns to a gap\ni\n\nNeedleman-Wunsch with affine gaps\nInitialization:\nF(i, 0) = d + (i - 1)×e\nF(0, j) = d + (j - 1)×e\nIteration:\nF(i - 1, j - 1) + s(x, yj)\ni\nF(i, j) = max\nG(i - 1, j - 1) + s(x, yj)\ni\nF(i - 1, j) - d\nF(i, j - 1) - d\nG(i, j) = max\nG(i, j - 1) - e\nG(i - 1, j) - e\nTermination:\nsame\n\nSequence Alignment\nAGGCTATCACCTGACCTCCAGGCCGATGCCC\nTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---\nTAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\nDefinition\nGiven two strings\nx = x1x2...xM, y = y1y2...yN,\nan alignment is an assignment of gaps to positions\n0,..., M in x, and 0,..., N in y, so as to line up each\nletter in one sequence with either a letter, or a gap\nin the other sequence\n\nScoring Function\n- Sequence edits:\nAGGCCTC\n- Mutations\nAGGACTC\n- Insertions\nAGGGCCTC\n- Deletions\nAGG.CTC\nScoring Function:\nMatch:\n+m\nMismatch: -s\nGap:\n-d\nScore F = (# matches) × m - (# mismatches) × s - (#gaps) ×\nd\n\nThe Needleman-Wunsch Algorithm\n1.\nInitialization.\na.\nF(0, 0)\n= 0\nb.\nF(0, j)\n= - j × d\nc.\nF(i, 0)\n= - i × d\n2.\nMain Iteration. Filling-in partial alignments\na.\nFor each\ni = 1......M\nFor each\nj = 1......N\nF(i-1,j-1) + s(x , yj)\ni\nF(i, j)\n= max\nF(i-1, j) - d\nF(i, j-1) - d\nDIAG,\nif [case 1]\nPtr(i,j)\n=\nLEFT,\nif [case 2]\nUP,\nif [case 3]\n3.\nTermination. F(M, N) is the optimal score, and\nfrom Ptr(M, N) can trace back optimal alignment\n[case 1]\n[case 2]\n[case 3]\n\nThe Smith-Waterman algorithm\nIdea: Ignore badly aligning regions\nModifications to Needleman-Wunsch:\nInitialization:\nF(0, j) = F(i, 0) = 0\nIteration:\nF(i, j) = max\nF(i - 1, j) - d\nF(i, j - 1) - d\nF(i - 1, j - 1) + s(x, yj)\ni\n\nScoring the gaps more accurately\nSimple, linear gap model:\nGap of length\nn\nγ(n)\nincurs penalty\nn×d\nHowever, gaps usually occur in bunches\nConvex gap penalty function:\nγ(n):\nγ(n)\nfor all n, γ(n + 1) - γ(n) ≤γ(n) - γ(n - 1)\nAlgorithm: O(N3) time, O(N2) space\n\nCompromise: affine gaps\nγ(n) = d + (n - 1)×e\n|\n|\nγ(n)\ngap\ngap\nopen\nextend\nd\nTo compute optimal alignment,\nAt position i,j, need to \"remember\" best score if gap is open\nbest score if gap is not open\ne\nF(i, j): score of alignment x1...x to y1...yj\ni\nifif xi aligns to yj\nG(i, j): score if x, or yj, aligns to a gap\ni\n\nWhy do we need two matrices?\n-\nxi aligns to yj\nx1......xi-1 xi xi+1\ny1......yj-1 yj\n-\n2. xi aligns to a gap\nx1......xi-1 xi xi+1\ny1......yj ...\n-\nAdd -d\nAdd -e\nNeedleman-Wunsch with affine gaps\n\nNeedleman-Wunsch with affine gaps\nInitialization:\nF(i, 0) = d + (i - 1)×e\nF(0, j) = d + (j - 1)×e\nIteration:\nF(i - 1, j - 1) + s(x, yj)\ni\nF(i, j) = max\nG(i - 1, j - 1) + s(x, yj)\ni\nF(i - 1, j) - d\nF(i, j - 1) - d\nG(i, j) = max\nG(i, j - 1) - e\nG(i - 1, j) - e\nTermination:\nsame\n\nTo generalize a little...\n... think of how you would compute optimal alignment\nwith this gap function\nγ(n)\n....in time O(MN)\n\nBounded Dynamic Programming\nAssume we know that x and y are very similar\nAssumption: # gaps(x, y) < k(N)\n( say N>M )\nxi\nThen,\n|\nimplies\n| i - j | < k(N)\nyj\nWe can align x and y more efficiently:\nTime, Space:\nO(N × k(N)) << O(N2)\n\nBounded Dynamic Programming\nInitialization:\nF(i,0), F(0,j) undefined for i, j > k\nIteration:\nFor i = 1...M\nFor j = max(1, i - k)...min(N, i+k)\nF(i - 1, j - 1)+ s(xi, yj)\nF(i, j) = max\nF(i, j - 1) - d, if j > i - k(N)\nF(i - 1, j) - d, if j < i + k(N)\nTermination:\nsame\nEasy to extend to the affine gap case\nx1 .............................. xM\ny1 .............................. yN\nk(N)\n\nLinear-Space Alignment\n\nHirschberg's algortihm\n- Longest common subsequence\n- Given sequences s = s1 s2 ... sm, t = t1 t2 ... tn,\n- Find longest common subsequence u = u1 ... uk\n- Algorithm:\nF(i-1, j)\n- F(i, j) = max F(i, j-1)\nF(i-1, j-1) + [1, if s = tj; 0 otherwise]\ni\n- Hirschberg's algorithm solves this in linear space\n\nIntroduction: Compute optimal score\nIt is easy to compute F(M, N) in linear space\nF(i,j)\nAllocate ( column[1] )\nAllocate ( column[2] )\nFor\ni = 1....M\nIf\ni > 1, then:\nFree( column[i - 2] )\nAllocate( column[ i ] )\nFor j = 1...N\nF(i, j) = ...\n\nLinear-space alignment\nTo compute both the optimal score and the optimal alignment:\nDivide & Conquer approach:\nNotation:\nxr, yr: reverse of x, y\nE.g. x = accgg;\nxr = ggcca\nFr(i, j): optimal score of aligning xr\n1...xr & yr\n1...yr\nj\ni\nsame as F(M-i+1, N-j+1)\n\nLinear-space alignment\nLemma:\nF(M, N) = maxk=0...N( F(M/2, k) + Fr(M/2, N-k) )\nx\ny\nM/2\nk*\nFr(M/2, N-k)\nF(M/2, k)\n\nLinear-space alignment\n- Now, using 2 columns of space, we can compute\nfor k = 1...M, F(M/2, k), Fr(M/2, N-k)\nPLUS the backpointers\n\nLinear-space alignment\n- Now, we can find k* maximizing F(M/2, k) + Fr(M/2, N-k)\n- Also, we can trace the path exiting column M/2 from k*\nk*\nk*\n\nLinear-space alignment\n- Iterate this procedure to the left and right!\nN-k*\nk*\nM/2\nM/2\n\nLinear-space alignment\nHirschberg's Linear-space algorithm:\nMEMALIGN(l, l', r, r'):\n(aligns x...xl' with yr...yr')\nl\n1.\nLet h = ⎡(l'-l)/2⎤\n2.\nFind in Time O((l' - l) × (r'-r)), Space O(r'-r)\nthe optimal path,\nLh, entering column h-1, exiting column h\nLet k1 = pos'n at column h - 2 where Lh enters\nk2 = pos'n at column h + 1 where Lh exits\n3.\nMEMALIGN(l, h-2, r, k1)\n4.\nOutput Lh\n5.\nMEMALIGN(h+1, l', k2, r')\nTop level call: MEMALIGN(1, M, 1, N)\n\nLinear-space alignment\nTime, Space analysis of Hirschberg's algorithm:\nTo compute optimal path at middle column,\nFor box of size M × N,\nSpace:\n2N\nTime:\ncMN,\nfor some constant c\nThen, left, right calls cost c( M/2 × k* + M/2 × (N-k*) ) = cMN/2\nAll recursive calls cost\nTotal Time: cMN + cMN/2 + cMN/4 + ..... = 2cMN = O(MN)\nTotal Space: O(N) for computation,\nO(N+M) to store the optimal alignment\n\nThe Four-Russian Algorithm\nA useful speedup of Dynamic Programming\n\nMain Observation\nWithin a rectangle of the DP\nmatrix,\nvalues of D depend only\non the values of A, B, C,\nand substrings xl...l', yr...r'\nDefinition:\nA t-block is a t × t square of\nthe DP matrix\nIdea:\nDivide matrix in t-blocks,\nPrecompute t-blocks\nSpeedup: O(t)\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nMain structure of the algorithm:\n-\nDivide N×N DP matrix into K×K\nlog2N-blocks that overlap by 1\ncolumn & 1 row\n-\nFor i = 1......K\n-\nFor j = 1......K\n-\nCompute Di,j as a function of\nAi,j, Bi,j, Ci,j, x[li...l'i], y[rj...r'j]\nTime: O(N2 / log2N)\ntimes the cost of step 4\nt\nt\nt\n\nThe Four-Russian Algorithm\nAnother observation:\n( Assume m = 0, s = 1, d = 1 )\nLemma. Two adjacent cells of F(.,.) differ by at most 1\nGusfield's book covers case where m = 0,\ncalled the edit distance (p. 216):\nminimum # of substitutions + gaps to transform one string to another\n\nThe Four-Russian Algorithm\nProof of Lemma:\n1.\nSame row:\na. F(i, j) - F(i - 1, j) ≤ +1\nAt worst, one more gap:\nx1......xi-1 xi\ny1......yj -\nb. F(i, j) - F(i - 1, j) ≥ -1\nF(i, j)\nF(i - 1, j - 1)\nF(i, j) - F(i - 1, j - 1)\nx1......xi-1 -\nx1......xi-1 xi\ny1......ya-1ya ya+1...yj\ny1......ya-1ya ya+1...yj\n≥ -1\nx1......xi-1 xi\nx1......xi-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\n+1\n2.\nSame column: similar argument\n\nThe Four-Russian Algorithm\nProof of Lemma:\n3.\nSame diagonal:\na. F(i, j) - F(i - 1, j - 1) ≤ +1\nAt worst, one additional mismatch in F(i, j)\nb. F(i, j) - F(i - 1, j - 1) ≥ -1\nF(i, j)\nF(i - 1, j - 1)\nx1......xi-1 xi\nx1......xi-1\n|\ny1......yi-1 yj\ny1......yj-1\nx1......xi-1 xi\nx1......xi-1\ny1......ya-1- ya...yj\ny1......ya-1ya...yj\nF(i, j) - F(i - 1, j - 1)\n≥-1\n+1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset vector is a\nt-long vector of values\nfrom {-1, 0, 1},\nwhere the first entry is 0\nIf we know the value at A,\nand the top row, left column\noffset vectors,\nand xl......xl', yr......yr',\nThen we can find D\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n5A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nExample:\nx = AACT\ny = CACT\n1A\nA\nC\nT\nC\nA\nC\nT\n-1\n-1\n-1\n-1\n-1\n\nThe Four-Russian Algorithm\nDefinition:\nThe offset function of a\nt-block\nis a function that for any\ngiven offset vectors\nof top row, left column,\nand xl......xl', yr......yr',\nproduces offset vectors\nof bottom row, right\ncolumn\nA\nB\nC\nD\nxl\nxl'\nyr\nyr'\nt\n\nThe Four-Russian Algorithm\nWe can pre-compute the offset function:\n2(t-1) possible input offset vectors\n2t possible strings x......xl', yr......yr'\nl\nTherefore 32(t-1) × 42t values to pre-compute\nWe can keep all these values in a table, and look up in linear time,\nor in O(1) time if we assume\nconstant-lookup RAM for log-sized inputs\n\nThe Four-Russian Algorithm\nFour-Russians Algorithm: (Arlazarov, Dinic, Kronrod,\nFaradzev)\n1. Cover the DP table with t-blocks\n2. Initialize values F(.,.) in first row & column\n3. Row-by-row, use offset values at leftmost column and top\nrow of each block, to find offset values at rightmost column\nand bottom row\n4. Let Q = total of offsets at row N\nF(N, N) = Q + F(N, 0)\n\nThe Four-Russian Algorithm\nt\nt\nt"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/220e3959a60d0fa71788508df2d6dbca_lecture6.pdf",
      "content": "RNA secondary structure\nLecture 1 - Introduction\nLecture 2 - Hashing and BLAST\nLecture 3 - Combinatorial Motif Finding\nLecture 4 - Statistical Motif Finding\nLecture 5 - Sequence alignment and Dynamic Programming\n6.096 - Algorithms for Computational Biology\nChallenges in Computational Biology\nDNA\nGenome Assembly\n1 Gene Finding\nRegulatory motif discovery\nDatabase lookup\nGene expression analysis\nRNA transcript\nSequence alignment\nEvolutionary Theory\nTCATGCTAT\nTCGTGATAA\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nCluster discovery\nGibbs sampling\nProtein network analysis\nEmerging network properties\n13 Regulatory network inference\nComparative Genomics\nRNA folding\nThe world before DNA or Protein\nRNA\nRNA Folding\nSelf-modification\nRNA-mediated\nReplication\nRNA\nRNA\nRNA World\n-\nRNA can be protein-like\n- Ribozymes can catalyze enzymatic reactions by RNA secondary fold\n- Small RNAs can play structural roles within the cell\n- Small RNAs play versatile roles in gene regulatory\n-\nRNA can be DNA-like\n- Made of digital information, can transfer to progeny by\ncomplementarity\n- Viruses with RNA genomes (single/double stranded)\n- RNA can catalyze RNA replication\n-\nRNA world is possible\n- Proteins are more efficient (larger alphabet)\n- DNA is more stable (double helix, less flexible)\nRNA invented its successors\n-\nRNA invents protein\n- Ribosome precise structure was solved this past year\n- Core is all RNA. Only RNA makes DNA contact\n- Protein component only adds structural stability\n-\nRNA and protein invent DNA\n- Stable, protected, specialized structure (no catalysis)\n- Proteins catalyze: RNAÆDNA reverse transcription\n- Proteins catalyze: DNAÆDNA replication\n- Proteins catalyze: DNAÆRNA transcription\n-\nViruses still preserved from those early days of life\n- Any type genome: dsDNA, ssRNA, dsRNA, hybrid\n- Simplest self-replicating life form\nExample: tRNA secondary and tertiary structure\nPrimary Structure\nSecondary Structure\nTertiary Structure\nAdaptor molecule between DNA and protein\n\nHairpin Loops\nStems\nBulge loop\nInterior loops\nMulti-branched loop\nMost common folds\nMore complex folds\nPseudoknot\nKissing Hairpins\nHairpin-bulge interaction\nDynamic programming algorithm\nfor secondary structure determination\nFirst DP Algorithm: Nussinov\n- one possible technique: base pair maximization\n- Algorithms for Loop Matching\n(Nussinov et al., 1978)\n- too simple for accurate prediction, but stepping-\nstone for later algorithms\nThe Nussinov Algorithm\nProblem:\nFind the RNA structure with the\nmaximum (weighted)\nnumber of nested pairings\nA\nG\nA\nC\nC\nU\nC\nU\nG\nG\nG\nC\nG\nG\nC\nA\nG\nU\nC\nU\nA\nU\nG\nC\nG\nA\nA\nC\nG\nC\nG\nU\nC\nA\nU\nC\nA\nG\nC UG\nG\nA\nA\nG\nA\nA\nGG G A\nG\nA\nU\nC\nU U C\nA\nC\nC\nA\nA\nU\nA\nC\nUG\nA\nA\nU\nU\nG\nC\nACCACGCUUAAGACACCUAGCUUGUGUCCUGGAGGUCUAUAAGUCAGACCGCGAGAGGGAAGACUCGUAUAAGCG\nA\nMatrix representation for RNA folding\n\nThe Nussinov Algorithm\nGiven sequence X = x1...xN,\nDefine DP matrix:\nF(i, j) = maximum number of bonds if xi...xj folds\noptimally\nTwo cases, if i < j:\n1. xi is paired with xj\nF(i, j) = s(xi, xj) + F(i+1, j-1)\n-\nxi is not paired with xj\nF(i, j) = max{ k: i ≤k < j } F(i, k) + F(k+1, j) i\nj\ni\nj\nk\nF(i, j)\nF(i, k)\nF(k+1, j)\nInitial Concepts\n- only consider base pairs\n- folding of an N nucleotide sequence can be\nspecified by a symmetric N × N matrix\n- Mij=1 if bases form a pair\n- Mij=0 otherwise\nC\nG\nA\nU\nU\nG\nNaive Example 1\nG\nG\nG\nA\nA\nA\nU\nC\nC\nG\nG\nG\nA\nA\nA\nU\nC\nC\nA\nA\nG\nU C\nA\nG G\nC\n7 8\n2 3\nMatching \"blocks\"\n- visually inspect matrices for diagonal lines of 1's\n- manually piece them together into an optimal\nfolded shape\nNaive Example 1\nG\nG\nG\nA\nA\nA\nU\nC\nC\nG\nG\nG\nA\nA\nA\nU\nC\nC\nA\nA\nG\nU C\nA\nG G\nC\n7 8\n2 3\nNaive Example 1\nG\nG\nG\nA\nA\nA\nU\nC\nC\nG\nG\nG\nA\nA\nA\nU\nC\nC\nA\nA\nG\nU C\nA\nG G\nC\n7 8\n2 3\n\nNaive Example 1\nG\nG\nG\nA\nA\nA\nU\nC\nC\nG\nG\nG\nA\nA\nA\nU\nC\nC\nA\nA\nG\nU C\nA\nG G\nC\n7 8\n2 3\nRefinement\n- unfortunately, this finds chemically infeasible\nstructures\n- i.e. insufficient space, inflexibility of paired base\nregions\n- next step is to specify better constraints\n- solution: a dynamic programming algorithm\n[Nussinov et al., 1978]\nStructure Representation\n-\nsecondary structure described as a graph\n-\nbase pairs are described via pairs of indices\n(i, j), indicating links between base vertices\nA\nC\nU\nA\nG\nU\nU\nC\nA U\nG G\nC\n11 12\n9 10\nA\nC\nU\nG\nA\nC\nU\nG\nU\nC\nA\nG\nU\nS={(1,13), (2,12), (3,11), (4,10)}\nBasic Constraints\n1.\nEach edge contains vertices (bases) linking\ncompatible base pairs\n2.\nNo vertex can be in more than one edge\n3.\nEdges must be drawn without crossing\nA\nA\nG\nU C\nA\nG G\nC\nj\ni\ng\nh\nEdges (g, h) and (i, j)\nif i < g < j < h or g < i < h < j, both\nedges cannot belong to the same\n\"matching.\"\nBasic Constraints\n1.\nEach edge contains vertices (bases) linking\ncompatible base pairs\n2.\nNo vertex can be in more than one edge\n3.\nEdges must be drawn without crossing\nA\nA\nG\nU C\nA\nG G\nC\nj\ni\ng\nh\nEdges (g, h) and (i, j)\nif i < g < j < h or g < i < h < j, both\nedges cannot belong to the same\n\"matching.\"\nCircular Representation\nImage source: Zuker, M. (2002) \"Lectures on RNA Secondary Structure Prediction\" http://www.bioinfo.rpi.edu/~zukerm/lectures/RNAfold-html/node1.html\nCourtesy of Michael Zuker. Used with permission.\n\nEnergy Minimization\n- objective is a folded shape for a given nucleotide\nchain such that the energy is minimized\n- Eij = 1 for each possible compatible base pair, Eij =\n0 otherwise\nThe Nussinov Algorithm\nInitialization:\nF(i, i-1) = 0;\nfor i = 2 to N\nF(i, i) = 0;\nfor i = 1 to N\nIteration:\nFor i = 2 to N:\nFor i = 1 to N - l\nj = i + l - 1\nF(i+1, j -1) + s(xi, xj)\nF(i, j) = max\nmax{ i ≤k < j }\nF(i, k) + F(k+1, j)\nTermination:\nBest structure is given by F(1, N)\n(Need to trace back)\nAlgorithm Behavior\n- recursive computation, finding the best structure for\nsmall subsequences\n- works outward to larger subsequences\n- four possible ways to get the best RNA structure:\nCase 1: Adding unpaired base i\n- Add unpaired position i onto best structure for\nsubsequence i+1, j\nCase 2: Adding unpaired base j\n- Add unpaired position i onto best structure for\nsubsequence i+1, j\nCase 3: Adding (i, j) pair\n- Add base pair (i, j) onto best structure found for\nsubsequence i+1, j-1\nDurbin, Richard, et. al.\n\nBiological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\n.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\n.\n.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\n.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\n.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\n\nCase 4: Bifurcation\n- combining two optimal substructures i, k and k+1, j\nNussinov RNA Folding Algorithm\n- Initialization:\nγ(i, i-1) = 0\nfor I = 2 to L;\nγ(i, i) = 0\nfor I = 2 to L.\nNussinov RNA Folding Algorithm\n- Initialization:\nγ(i, i-1) = 0\nfor I = 2 to L;\nγ(i, i) = 0\nfor I = 2 to L.\nNussinov RNA Folding Algorithm\n- Initialization:\nγ(i, i-1) = 0\nfor I = 2 to L;\nγ(i, i) = 0\nfor I = 2 to L.\nNussinov RNA Folding Algorithm\n- Recursive Relation:\n- For all subsequences from length 2 to length L:\n\n+\n+\n+\n-\n+\n-\n+\n=\n<\n<\n)]\n,1\n(\n)\n,\n(\n[\nmax\n)\n,\n(\n)1\n,1\n(\n)1\n,\n(\n)\n,1\n(\nmax\n)\n,\n(\nj\nk\nk\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\nk\ni\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nCase 1\nCase 2\nCase 3\nCase 4\nNussinov RNA Folding Algorithm\n\n+\n+\n+\n-\n+\n-\n+\n=\n<\n<\n)]\n,1\n(\n)\n,\n(\n[\nmax\n)\n,\n(\n)1\n,1\n(\n)1\n,\n(\n)\n,1\n(\nmax\n)\n,\n(\nj\nk\nk\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\nk\ni\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\n.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\n\nNussinov RNA Folding Algorithm\n\n+\n+\n+\n-\n+\n-\n+\n=\n<\n<\n)]\n,1\n(\n)\n,\n(\n[\nmax\n)\n,\n(\n)1\n,1\n(\n)1\n,\n(\n)\n,1\n(\nmax\n)\n,\n(\nj\nk\nk\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\nk\ni\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nNussinov RNA Folding Algorithm\n\n+\n+\n+\n-\n+\n-\n+\n=\n<\n<\n)]\n,1\n(\n)\n,\n(\n[\nmax\n)\n,\n(\n)1\n,1\n(\n)1\n,\n(\n)\n,1\n(\nmax\n)\n,\n(\nj\nk\nk\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\nk\ni\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nA\nU\nA\nA\ni\ni+1\nj\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\ni+1\nj-1\ni\nj\nA\nU\nA\nA\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\n\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nExample Computation\n\n+\n+\n+\n=\n<\n<\n)]\n7,1\n(\n)\n,4\n(\n[\nmax\n)\n7,4\n(\n)\n6,5\n(\n)\n6,4\n(\n)\n7,5\n(\nmax\n)\n7,4\n(\nk\nk\nk\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nCompleted Matrix\n\n+\n+\n+\n-\n+\n-\n+\n=\n<\n<\n)]\n,1\n(\n)\n,\n(\n[\nmax\n)\n,\n(\n)1\n,1\n(\n)1\n,\n(\n)\n,1\n(\nmax\n)\n,\n(\nj\nk\nk\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\ni\nj\nk\ni\nγ\nγ\nδ\nγ\nγ\nγ\nγ\nTraceback\n- value at γ(1, L) is the total base pair count in the\nmaximally base-paired structure\n- as in other DP, traceback from γ(1, L) is necessary\nto recover the final secondary structure\n- pushdown stack is used to deal with bifurcated\nstructures\nTraceback Pseudocode\nInitialization: Push (1,L) onto stack\nRecursion: Repeat until stack is empty:\n-\npop (i, j).\n-\nIf i >= j continue;\n// hit diagonal\nelse if γ(i+1,j) = γ(i, j) push (i+1,j);\n// case 1\nelse if γ(i, j-1) = γ(i, j) push (i,j-1);\n// case 2\nelse if γ(i+1,j-1)+δi,j = γ(i, j):\n// case 3\nrecord i, j base pair\npush (i+1,j-1);\nelse for k=i+1 to j-1:if γ(i, k)+γ(k+1,j)=γ(i, j): // case 4\npush (k+1, j).\npush (i, k).\nbreak\nRetrieving the Structure\nSTACK\n(1,9)\nCURRENT\nPAIRS\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\n\nRetrieving the Structure\nSTACK\n(2,9)\nCURRENT\n(1,9)\nPAIRS\nRetrieving the Structure\nSTACK\n(3,8)\nCURRENT\n(2,9)\nC\nG\nG\nPAIRS\n(2,9)\nRetrieving the Structure\nSTACK\n(4,7)\nCURRENT\n(3,8)\nC\nG\nG\nC\nG\nPAIRS\n(2,9)\n(3,8)\nRetrieving the Structure\nI\nSTACK\n(5,6)\nCURRENT\n(4,7)\nU\nC\nG\nA\nG\nC\nG\nPAIRS\n(2,9)\n(3,8)\n(4,7)\nRetrieving the Structure\nSTACK\n(6,6)\nCURRENT\n(5,6)\nA\nU\nC\nG\nA\nG\nC\nG\nPAIRS\n(2,9)\n(3,8)\n(4,7)\nRetrieving the Structure\nSTACK\n-\nCURRENT\n(6,6)\nA\nU\nC\nG\nA\nG\nC\nG\nA\nPAIRS\n(2,9)\n(3,8)\n(4,7)\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nImage removed due to copyright considerations.\nPlease see:\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\n\nRetrieving the Structure\nA\nU\nC\nG\nA\nG\nC\nG\nA\nEvaluation of Nussinov\n- unfortunately, while this does maximize the base\npairs, it does not create viable secondary\nstructures\n- in Zuker's algorithm, the correct structure is\nassumed to have the lowest equilibrium free\nenergy (∆G) (Zuker and Stiegler, 1981; Zuker\n1989a)\nMinimizing free energy\nThe Zuker algorithm - main ideas\nModels energy of an RNA fold\n1. Instead of base pairs, pairs of base pairs (more accurate)\n2. Separate score for bulges\n3. Separate score for different-size & composition loops\n4. Separate score for interactions between stem & beginning of loop\nCan also do all that with a SCFG, and train it on real data\nFree Energy (∆G)\n- ∆G approximated as the sum of contributions from\nloops, base pairs and other secondary structures\nBasic Notation\n- secondary structure of sequence s is a set S of\nbase pairs i - j, 1 ≤i < j ≤|s|\n- we assume:\n- each base is only in one base pair\n- no pseudoknots\n- sharp \"U-turns\" prohibited; a hairpin loop must\ncontain at least 3 bases\nImage removed due to copyright considerations.\nPlease see:\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nDurbin, Richard, et. al. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.\nCambridge, UK: Cambridge University Press, 1999. ISBN: 0521629713.\nImage removed due to copyright considerations.\nPlease see:\n\nSecondary Structure Representation\n- can view a structure S as a collection of loops\ntogether with some external unpaired bases\nAccessible Bases\ni\nj\ni'\nj'\ni''\nj''\n- Let i < k < j with i-j ∈S\n- k is accessible from i-j if for all i′-j′ ∈S if it is not the\ncase that i<i′<k<j′<j\nk\nExterior Base Pairs\n- base pair i-j is the exterior base pair of (or closing)\nthe loop consisting of i-j and all bases accessible\nfrom it\ni\nj\nInterior Base Pairs\n-\nif i′ and j′ are accessible from i-j\n-\nand i′-j′ ∈S\n-\nthen i′-j′ is an interior base pair, and is accessible from i-j\ni\nj\ni'\nj'\nHairpin Loop\n- if there are no interior base pairs in a loop, it is a\nhairpin loop\ni\nj\ni'\nj'\nStacked Pair\n- a loop with one interior base pair is a stacked pair if\ni′ = i+1 and j′ = j-1\ni\nj\ni' = i+1\nj' = j+1\n\nInternal Loop\n- if it is not true that the interior base pair i-j that\ni′ = i+1 and j′ = j-1, it is an internal loop\ni'\nj'\ni\nj\nMultibranch Loops\n- loops with more than one interior base pair are\nmultibranched loops\nExternal Bases and Base Pairs\n- any bases or base pairs not accessible from any\nbase pair are called external\nAssumptions\n- structure prediction determines the most stable\nstructure for a given sequence\n- stability of a structure is based on free energy\n- energy of secondary structures is the sum of\nindependent loop energies\nRecursion Relation\n- four arrays are used to hold the minimal free\nenergy of specific structures of subsequences of s\n- arrays are computed interdependently\n- calculated recursively using pre-specified free\nenergy functions for each type of loop\nV(i,j)\n- energy of an optimal structure of subsequence i\nthrough j closed by i-j:\n\n-\n+\n+\n=\n)\n,\n(\n)\n,\n(\n)1\n,1\n(\n)\n,\n(\n)\n,\n(\nmin\n)\n,\n(\nj\ni\nVM\nj\ni\nVBI\nj\ni\nV\nj\ni\neS\nj\ni\neH\nj\ni\nV\n\neH(i,j)\n-\nenergy of hairpin loop closed by i-j\n-\ncomputed with:\n-\nR = universal gas constant (1.9872 cal/mol/K).\n-\nT = absolute temperature\n- ls = total single-stranded (unpaired) bases in loop\neS(i,j)\n-\nenergy of stacking base pair i-j with i+1-j-1\n-\nsample free energies in kcal/mole for CG base pairs stacked over all\npossible base pairs, XY\n-\n'.' entries are undefined, and can be assumed as inf\neL(i,j,i′,j′)\n- energy of a bulge or internal loop with exterior base\npair i-j and interior base pair i′-j′\n-\nfree energies for all 1 x 2 interior loops in RNA closed by a CG and an AU base\npair, with a single stranded U 3' to the double stranded U.\neM(i,j,i1,j1,...,ik,jk)\n- energy of a multibranched loop with exterior base\npair i-j and interior base pairs i1-j1,...,ik-jk\n- simplification: linear contributions from number of\nunpaired bases in loop, number of branches and a\nconstant\n)\n,\n,...,\n,\n,\n,\n(\nk\nk j\ni\nj\ni\nj\ni\neM\n∑\n-\n=\n+\n-\n+\n+\n-\n-\n+\n-\n-\n+\n+\n=\n))\n(\n(\nk\nl\nl\nl\nk\nj\ni\nj\nj\ni\ni\nc\nbk\na\nAssembling the Pieces\nHairpin Loop\nMulti-loop\nInternal Loop\nBulge\nExternal Base\n{\n)}\n,\n(\n)\n,\n,\n,\n(\nmin\n)\n,\n(\nj\ni\nV\nj\ni\nj\ni\neL\nj\ni\nVBI\nj\nj\ni\ni\nj\nj\ni\ni\n′\n′\n+\n′\n′\n=\n>\n′\n-\n+\n-\n′\n<\n′\n<′\n<\n{\n})\n,\n(\n)\n,\n,...,\n,\n,\n,\n(\nmin\n)\n,\n(\n...\n∑\n=\n<\n<\n<\n<\n<\n<\n+\n=\nk\nl\nl\nl\nk\nk\nj\nj\ni\nj\ni\ni\nj\ni\nV\nj\ni\nj\ni\nj\ni\neM\nj\ni\nVM\nk\nk\n)\n,\n(\nj\ni\neH\n)1\n,1\n(\n)\n,\n(\n-\n+\n+\nj\ni\nV\nj\ni\neS\nStacking Base Pairs\n{\n)}\n,\n(\n)\n,\n,\n,\n(\nmin\n)\n,\n(\nj\ni\nV\nj\ni\nj\ni\neL\nj\ni\nVBI\nj\nj\ni\ni\nj\nj\ni\ni\n′\n′\n+\n′\n′\n=\n>\n′\n-\n+\n-\n′\n<′\n<\n′\n<\nComparative methods for\nRNA structure prediction\n\nMultiple alignment and RNA folding\nGiven K homologous aligned RNA sequences:\nHuman\naagacuucggaucuggcgacaccc\nMouse\nuacacuucggaugacaccaaagug\nWorm\naggucuucggcacgggcaccauuc\nFly\nccaacuucggauuuugcuaccaua\nYeast\naagccuucggagcgggcguaacuc\nIf ith and jth positions are always base paired and covary, then they are\nlikely to be paired\nMutual information\nfab(i,j)\nMij = Σa,b∈{a,c,g,u}fab(i,j) log2----------\nfa(i) fb(j)\nWhere fab(i,j) is the # of times the pair a, b are in positions i, j\nGiven a multiple alignment, can infer structure that maximizes the sum of mutual information, by\nDP\nIn practice:\n1.\nGet multiple alignment\n2.\nFind covarying bases - deduce structure\n3.\nImprove multiple alignment (by hand)\n4.\nGo to 2\nA manual EM process!!\nResults for tRNA\n- Matrix of co-variations in tRNA molecule\nContext Free Grammars for representing\nRNA folds\nA Context Free Grammar\nS →AB\nNonterminals: S, A, B\nA →aAc | a\nTerminals:\na, b, c, d\nB →bBd | b\nDerivation:\nS →AB →aAcB →... →aaaacccB →aaaacccbBd →... →\naaaacccbbbbbbddd\nProduces all strings ai+1cibj+1dj, for i, j ≥0\nExample: modeling a stem loop\nS →a W1 u\nW1 →c W2 g\nW2 →g W3 c\nW3 →g L c\nL →agucg\nWhat if the stem loop can have other\nletters in place of the ones shown?\nACGG\nUGCC\nAG\nU\nCG\n\nExample: modeling a stem loop\nS →a W1 u\n| g W1 u\nW1 →c W2 g\nW2 →g W3 c\n| g W3 u\nW3 →g L c\n| a L u\nL →agucg\n| agccg | cugugc\nMore general: Any 4-long stem, 3-5-long loop:\nS →aW1u | gW1u | gW1c | cW1g | uW1g | uW1a\nW1 →aW2u | gW2u | gW2c | cW2g | uW2g | uW2a\nW2 →aW3u | gW3u | gW3c | cW3g | uW3g | uW3a\nW3 →aLu | gLu | gLc | cLg | uLg | uLa\nL →aL1 | cL1 | gL1 | uL1\nL1 →aL2 | cL2 | gL2 | uL2\nL2 →a | c | g | u | aa | ... | uu | aaa | ... | uuu\nACGG\nUGCC\nAG\nU\nCG\nGCGA\nUGCU\nAG\nC\nCG\nGCGA\nUGUU\nCUG\nU\nCG\nA parse tree: alignment of CFG to sequence\nACGG\nUGCC\nAG\nU\nCG\nA C G G A G U G C C C G U\nS\nW1\nW2\nW3\nL\n-\nS →a W1 u\n-\nW1 →c W2 g\n-\nW2 →g W3 c\n-\nW3 →g L c\n-\nL →agucg\nAlignment scores for parses\nWe can define each rule X →s, where s is a string,\nto have a score.\nExample:\nW →a W' u:\n(forms 3 hydrogen bonds)\nW →g W' c:\n(forms 2 hydrogen bonds)\nW →g W' u: 1\n(forms 1 hydrogen bond)\nW →x W' z\n-1, when (x, z) is not an a/u, g/c, g/u pair\nQuestions:\n-\nHow do we best align a CFG to a sequence?\n(DP)\n-\nHow do we set the parameters?\n(Stochastic CFGs)\nThe Nussinov Algorithm and CFGs\nDefine the following grammar, with scores:\nS →a S u : 3\n| u S a : 3\ng S c : 2 | c S g : 2\ng S u : 1 | u S g : 1\nS S : 0 |\na S : 0 | c S : 0 | g S : 0 | u S : 0 | ε : 0\nNote: ε is the \"\" string\nThen, the Nussinov algorithm finds the optimal parse of a string with this grammar\nReformulating the Nussinov Algorithm\nInitialization:\nF(i, i-1) = 0;\nfor i = 2 to N\nF(i, i) = 0;\nfor i = 1 to N\nS →a | c | g | u\nIteration:\nFor i = 2 to N:\nFor i = 1 to N - l\nj = i + l - 1\nF(i+1, j -1) + s(xi, xj)\nS →a S u | ...\nF(i, j) = max\nmax{ i ≤k < j }\nF(i, k) + F(k+1, j)\nS →S S\nTermination:\nBest structure is given by F(1, N)\nStochastic Context Free Grammars\n\nStochastic Context Free Grammars\nIn an analogy to HMMs, we can assign probabilities to transitions:\nGiven grammar\nX1 →s11 | ... | sin\n...\nXm →sm1 | ... | smn\nCan assign probability to each rule, s.t.\nP(Xi →si1) + ... + P(Xi →sin) = 1\nComputational Problems\n-\nCalculate an optimal alignment of a sequence and a SCFG\n(DECODING)\n-\nCalculate Prob[ sequence | grammar ]\n(EVALUATION)\n-\nGiven a set of sequences, estimate parameters of a SCFG\n(LEARNING)\nNormal Forms for CFGs\nChomsky Normal Form:\nX →YZ\nX →a\nAll productions are either to 2 nonterminals, or to 1 terminal\nTheorem (technical)\nEvery CFG has an equivalent one in Chomsky Normal Form\n(That is, the grammar in normal form produces exactly the\nsame set of strings)\nExample of converting a CFG to C.N.F.\nS →ABC\nA →Aa | a\nB →Bb | b\nC →CAc | c\nConverting:\nS →AS'\nS' →BC\nA →AA | a\nB →BB | b\nC →DC' | c\nC' →c\nD →CA\nS\nA\nB\nC\nA\na\na\nB\nb\nB\nb\nb\nC\nA\nc\nc\na\nS\nA\nS'\nB\nC\nA\nA\na\na\nB\nB\nB\nB\nb\nb\nb\nD\nC'\nC\nA\nc\nc a\nAnother example\nS →ABC\nA →C | aA\nB →bB | b\nC →cCd | c\nConverting:\nS →AS'\nS' →BC\nA →C'C''\n| c | A'A\nA' →a\nB →B'B | b\nB' →b\nC →C'C''\n| c\nC' →c\nC'' →CD\nD →d\nAlgorithms for learning Grammars\n\nDecoding: the CYK algorithm\nGiven x = x1....xN, and a SCFG G,\nFind the most likely parse of x\n(the most likely alignment of G to x)\nDynamic programming variable:\nγ(i, j, V):\nlikelihood of the most likely parse of xi...xj,\nrooted at nonterminal V\nThen,\nγ(1, N, S): likelihood of the most likely parse of x by the\ngrammar\nThe CYK algorithm (Cocke-Younger-Kasami)\nInitialization:\nFor i = 1 to N, any nonterminal V,\nγ(i, i, V) = log P(V →xi)\nIteration:\nFor i = 1 to N-1\nFor j = i+1 to N\nFor any nonterminal V,\nγ(i, j, V) = maxXmaxYmaxi≤k<j γ(i,k,X) + γ(k+1,j,Y) + log P(V→XY)\nTermination:\nlog P(x | θ, π*) = γ(1, N, S)\nWhere π* is the optimal parse tree (if traced back appropriately from above)\nA SCFG for predicting RNA structure\nS →a S | c S | g S | u S | ε\n→S a | S c | S g | S u\n→a S u | c S g | g S u | u S g | g S c | u S a\n→SS\n-\nAdjust the probability parameters to reflect bond strength etc\n-\nNo distinction between non-paired bases, bulges, loops\n-\nCan modify to model these events\n- L: loop nonterminal\n- H: hairpin nonterminal\n- B: bulge nonterminal\n- etc\nCYK for RNA folding\nInitialization:\nγ(i, i-1) = log P(ε)\nIteration:\nFor i = 1 to N\nFor j = i to N\nγ(i+1, j-1) + log P(xi S xj)\nγ(i, j-1) + log P(S xi)\nγ(i, j) = max\nγ(i+1, j) + log P(xi S)\nmaxi < k < j γ(i, k) + γ(k+1, j) + log P(S S)\nEvaluation\nRecall HMMs:\nForward:\nfl(i) = P(x1...xi, πi = l)\nBackward: bk(i) = P(xi+1...xN | πi = k)\nThen,\nP(x) = Σk fk(N) ak0 = Σl a0l el(x1) bl(1)\nAnalogue in SCFGs:\nInside:\na(i, j, V)\n= P(xi...xj is generated by\nnonterminal V)\nOutside:\nb(i, j, V) = P(x, excluding xi...xj is generated by\nS and\nthe excluded part is rooted\nat V)\nThe Inside Algorithm\nTo compute\na(i, j, V) = P(xi...xj, produced by V)\na(i, j, v) = ΣX ΣY Σk a(i, k, X) a(k+1, j, Y) P(V →XY)\nk k+1\ni\nj\nV\nX\nY\n\nAlgorithm: Inside\nInitialization:\nFor i = 1 to N, V a nonterminal,\na(i, i, V) = P(V →xi)\nIteration:\nFor i = 1 to N-1\nFor j = i+1 to N\nFor V a nonterminal\na(i, j, V) = ΣX ΣY Σk a(i, k, X) a(k+1, j, X) P(V →XY)\nTermination:\nP(x | θ) = a(1, N, S)\nThe Outside Algorithm\nb(i, j, V) = Prob(x1...xi-1, xj+1...xN, where the \"gap\" is rooted at V)\nGiven that V is the right-hand-side nonterminal of a production,\nb(i, j, V) = ΣX ΣY Σk<i a(k, i-1, X) b(k, j, Y) P(Y →XV)\ni\nj\nV\nk\nX\nY\nAlgorithm: Outside\nInitialization:\nb(1, N, S) = 1\nFor any other V, b(1, N, V) = 0\nIteration:\nFor i = 1 to N-1\nFor j = N down to i\nFor V a nonterminal\nb(i, j, V) = ΣX ΣY Σk<i a(k, i-1, X) b(k, j, Y) P(Y →XV) +\nΣX ΣY Σk<i a(j+1, k, X) b(i, k, Y) P(Y →VX)\nTermination:\nIt is true for any i, that:\nP(x | θ) = ΣX b(i, i, X) P(X →xi)\nLearning for SCFGs\nWe can now estimate\nc(V) = expected number of times V is used in the parse of x1....xN\nc(V) = -------- Σ1≤i≤NΣi≤j≤N a(i, j, V) b(i, j, v)\nP(x | θ)\nc(V→XY) = -------- Σ1≤i≤NΣi<j≤N Σi≤k<j b(i,j,V) a(i,k,X) a(k+1,j,Y) P(V→XY)\nP(x | θ)\nLearning for SCFGs\nThen, we can re-estimate the parameters with EM, by:\nc(V→XY)\nPnew(V→XY) = ------------\nc(V)\nc(V →a) Σi: xi = a b(i, i, V) P(V →a)\nPnew(V →a) = ---------- = --------------------------------\nc(V)\nΣ1≤i≤NΣi<j≤N a(i, j, V) b(i, j, V)\nSummary: SCFG and HMM algorithms\nGOAL\nHMM algorithm\nSCFG algorithm\nOptimal parse\nViterbi\nCYK\nEstimation\nForward\nInside\nBackward\nOutside\nLearning\nEM: Fw/Bck\nEM: Ins/Outs\nMemory Complexity\nO(N K)\nO(N2 K)\nTime Complexity\nO(N K2)\nO(N3 K3)\nWhere K: # of states in the HMM\n# of nonterminals in the SCFG"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture7.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/c48f8fdb8e1ff0cb6336e5b9b6cab23f_lecture7.pdf",
      "content": "6.096 - Algorithms for Computational Biology - Lecture 7\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nGene Finding and HMMs\n- Introduction\n- Hashing and BLAST\n- Combinatorial Motif Finding\n- Statistical Motif Finding\n- Sequence alignment and Dynamic Programming\n- RNA structure and Context Free Grammars\n- Gene finding and Hidden Markov Models\n\nChallenges in Computational Biology\n4 Genome Assembly\nRegulatory motif discovery\n1 Gene Finding\nDNA\nSequence alignment\nComparative Genomics\nTCATGCTAT\nTCGTGATAA\nDatabase lookup\nTGAGGATAT\n7 Evolutionary Theory\nTTATCATAT\nTTATGATTT\nRNA transcript\n10 Cluster discovery\nGibbs sampling\nGene expression analysis\nRNA folding\nProtein network analysis\n13 Regulatory network inference\nEmerging network properties\n\nOutline\n- Computational model\n- Simple Markov Models\n- Hidden Markov Models\n- Working with HMMs\n- Dynamic programming (Viterbi)\n- Expectation maximization (Baum-Welch)\n- Gene Finding in practice\n- GENSCAN\n- Performance Evaluation\n\nMarkov Chains & Hidden Markov Models\nA+\nT+\nG+\nC+\naGT\naAC\naGC\naAT\n- Markov Chain\n- Q: states\n- p: initial state probabilities\n- A: transition probabilities\nA+\nT+\nG+\nC+\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\n-\nHMM\n\n- Q: states\n- V: observations\n- p: initial state probabilities\n- A: transition probabilities\n- E: emission probabilities\n\nMarkov Chain\nDefinition: A Markov chain is a triplet (Q, p, A), where:\n3⁄4 Q is a finite set of states. Each state corresponds to a symbol in the\nalphabet Σ\n3⁄4 p is the initial state probabilities.\n3⁄4 A is the state transition probabilities, denoted by ast for each s, t in Q.\n3⁄4 For each s, t in Q the transition probability is: ast ≡ P(xi = t|xi-1 = s)\nOutput: The output of the model is the set of states at each\ninstant time => the set of states are observable\nProperty: The probability of each symbol xi depends only on\nthe value of the preceding symbol xi-1 : P (xi | xi-1,..., x1) = P (xi | xi-1)\nFormula: The probability of the sequence:\nP(x) = P(xL,xL-1,..., x1) = P (xL | xL-1) P (xL-1 | xL-2)... P (x2 | x1) P(x1)\n\nHMM (Hidden Markov Model)\nDefinition: An HMM is a 5-tuple (Q, V, p, A, E), where:\n3⁄4 Q is a finite set of states, |Q|=N\n3⁄4 V is a finite set of observation symbols per state, |V|=M\n3⁄4 p is the initial state probabilities.\n3⁄4 A is the state transition probabilities, denoted by ast for each s, t in Q.\n3⁄4 For each s, t in Q the transition probability is: ast ≡ P(xi = t|xi-1 = s)\n3⁄4 E is a probability emission matrix, esk ≡ P (vk at time t | qt = s)\nOutput: Only emitted symbols are observable by the system but not the\nunderlying random walk between states -> \"hidden\"\nProperty: Emissions and transitions are dependent on the current state\nonly and not on the past.\n\nTypical HMM Problems\nAnnotation Given a model M and an observed string\nS, what is the most probable path through M\ngenerating S\nClassification Given a model M and an observed\nstring S, what is the total probability of S under M\nConsensus Given a model M, what is the string\nhaving the highest probability under M\nTraining Given a set of strings and a model structure,\nfind transition and emission probabilities assigning\nhigh probabilities to the strings\n\nExample 1: Finding CpG islands\n\nWhat are CpG islands?\n- Regions of regulatory importance in promoters of many genes\n- Defined by their methylation state (epigenetic information)\n- Methylation process in the human genome:\n- Very high chance of methyl-C mutating to T in CpG\nI CpG dinucleotides are much rarer\n- BUT it is suppressed around the promoters of many genes\nI CpG dinucleotides are much more frequent than elsewhere\n- Such regions are called CpG islands\n- A few hundred to a few thousand bases long\n- Problems:\n- Given a short sequence, does it come from a CpG island or not?\n- How to find the CpG islands in a long sequence\n\nC\nTraining Markov Chains for CpG islands\nA\nT\nG\nC\naGT\naAC\naGC\naAT\n- Training Set:\n- set of DNA sequences w/ known CpG islands\n- Derive two Markov chain models:\n- '+' model: from the CpG islands\n- '-' model: from the remainder of sequence\n- Transition probabilities for each model:\nProbability of C following A\n+\n+\nis the number of times\n+\nA\nC\nG\nT\nA\n.180\n.274\n.426\n.120\n.171\n.368\n.274\n.188\nG\n.161\n.339\n.375\n.125\nT\n.079\n.355\n.384\n.182\n+\ncst\ncst\nast = ∑t'\nletter t followed letter s\n+\ncst'\ninside the CpG islands\n-\n-\nis the number of times\nast\n-=\ncst\n-\ncst\nletter t followed letter s\n∑t' cst'\noutside the CpG islands\n\nUsing Markov Models for CpG classification\nQ1: Given a short sequence x, does it come from CpG island (Yes-No question)\n- To use these models for discrimination, calculate the log-odds ratio:\nS(x) = log P(x|model + )\n+\nxi- 1\nlog\na\n-\nxi\nP(x|model - ) =∑ i\nL\n= 1\naxi- 1 xi\nHistogram of log odds scores\nCpG\nislands\nCpG\nNon-\n-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4\n\nUsing Markov Models for CpG classification\nQ2: Given a long sequence x, how do we find CpG islands in it\n(Where question)\n-\nCalculate the log-odds score for a window of, say, 100 nucleotides around every\nnucleotide, plot it, and predict CpG islands as ones w/ positive values\n-\nDrawbacks: Window size\nUse a hidden state: CpG (+) or non-CpG (-)\n\nHMM for CpG islands\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\n- Build a single model that combines both\nMarkov chains:\n- '+' states: A+, C+, G+, T+\n- Emit symbols: A, C, G, T in CpG islands\n- '-' states: A-, C-, G-, T\n- Emit symbols: A, C, G, T in non-islands\n- Emission probabilities distinct for the '+'\nand the '-' states\n- Infer most likely set of states, giving rise\nto observed emissions\nI 'Paint' the sequence with + and - states\n\nFinding most likely state path\n- Given the observed emissions, what was the path?\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nC\nG\nC\nG\nstart\nend\n\nProbability of given path p & observations x\n-\nKnown observations: CGCG\n-\nKnown sequence path: C+, G-, C-, G+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nC\nG\nC\nG\nstart\nend\n\nProbability of given path p & observations x\n-\n-\nC-\nC+\nG-\nC-\nG+\nC+\nG+\nC\nG\nC\nG\nstart\nend\nKnown observations: CGCG\nKnown sequence path: C+, G-, C-, G+\n\nProbability of given path p & observations x\nC-\nC+\nG-\nC-\nG+\nC+\nG+\nC\nG\nC\nG\nstart\nend\na0,C+\naC+,G-\naG-,C-\naC-,G+\naG+,0\neC+(C)\neG-(G)\neC-(C)\neG+(G)\n-\nP(p,x) = (a0,C+* 1) * (aC+,G-* 1) * (aG-,C-* 1) * (aC-,G+* 1) * (aG+,0)\nBut in general, we don't know the path!\n\n1. Evaluation\nGIVEN\nFIND\n2. Decoding\nGIVEN\nFIND\n3. Learning\nGIVEN\nFIND\nThe three main questions on HMMs\na HMM M,\nand a sequence x,\nProb[ x | M ]\na HMM M,\nand a sequence x,\nthe sequence π of states that maximizes P[ x, π | M ]\na HMM M, with unspecified transition/emission probs.,\nand a sequence x,\nparameters θ = (e(.), aij) that maximize P[ x | θ ]\ni\n\nProblem 1: Decoding\nFind the best parse of a\nsequence\n\nDecoding\nGIVEN x = x1x2......xN\nπ\nWe want to find π = π1, ......, πN,\nsuch that P[ x, π ] is maximized\n* = argmaxπ P[ x, π ]\nWe can use dynamic programming!\nK\n...\nK\n...\nK\n...\n...\n...\n...\nK\n...\nK\nx1\nx2\nx3\nxK\nLet Vk(i) = max{π1,...,i-1} P[x1...xi-1, π1, ..., πi-1, xi, πi = k]\n= Probability of most likely sequence of states ending at\nstate πi = k\n\nDecoding - main idea\nGiven that for all states k,\nand for a fixed position i,\nVk(i) = max{π1,...,i-1} P[x1...xi-1, π1, ..., πi-1, xi, πi = k]\nWhat is Vk(i+1)?\nFrom definition,\nV(i+1) = max{π1,...,i}P[ x1...xi, π1, ..., π, xi+1, πi+1 = l ]\nl\ni\n= max{π1,...,i}P(xi+1, πi+1 = l | x1...x,π1,..., π) P[x1...x, π1,..., π]\n= max\ni\ni\ni\ni\n{π1,...,i}P(xi+1, πi+1 = l | πi ) P[x1...xi-1, π1, ..., πi-1, x, π]\ni\ni\n= maxk P(xi+1, πi+1 = l | π = k) max{π1,...,i-1}P[x1...xi-1,π1,...,πi-1, xi,π=k]\ni\ni\n= e(xi+1) maxk akl Vk(i)\nl\n\nThe Viterbi Algorithm\nInput: x = x1......xN\nInitialization:\nV0(0) = 1\n(0 is the imaginary first position)\nVk(0) = 0, for all k > 0\nIteration:\nVj(i)\n= ej(x) × maxk akj Vk(i-1)\ni\nPtrj(i) = argmaxk akj Vk(i-1)\nTermination:\nP(x, π*) = maxk Vk(N)\nTraceback:\nπN* = argmaxk Vk(N)\nπi-1* = Ptrπ (i)\ni\n\nThe Viterbi Algorithm\nSimilar to\na set of states to a sequence\nTime:\nO(K2N)\nSpace:\nO(KN)\nx1 x2 x3 ...............................................xN\nState 1\nK\nVj(i)\n\"aligning\"\n\nViterbi Algorithm - a practical detail\nUnderflows are a significant problem\nP[ x1,...., x, π1, ..., π ] = a0π1 aπ1π2......aπ eπ1(x1)......eπ(x)\ni\ni\ni\ni\ni\nThese numbers become extremely small - underflow\nSolution: Take the logs of all values\nV(i) = log ek(x) + maxk [ Vk(i-1) + log akl ]\nl\ni\n\nExample\nLet x be a sequence with a portion of ~ 1/6 6's, followed by a portion of ~ 1⁄2\n6's...\nx = 123456123456...12345 6626364656...1626364656\nThen, it is not hard to show that optimal parse is (exercise):\nFFF........................F LLL..............................L\n6 nucleotides \"123456\" parsed as F, contribute .956×(1/6)6\n= 1.6×10-5\nparsed as L, contribute .956×(1/2)1×(1/10)5 = 0.4×10-5\n\"162636\" parsed as F, contribute .956×(1/6)6\n= 1.6×10-5\nparsed as L, contribute .956×(1/2)3×(1/10)3 = 9.0×10-5\n\nProblem 2: Evaluation\nFind the likelihood a sequence\nis generated by the model\n\nGenerating a sequence by the model\nGiven a HMM, we can generate a sequence of length n as follows:\n1. Start at state π1 according to prob a0π1\n2. Emit letter x1 according to prob eπ1(x1)\n3. Go to state π2 according to prob aπ1π2\n4. ... until emitting xn\nK\n...\nK\n...\nK\n...\n...\n...\n...\nK\n...\nK\na02\ne2(x1)\nx1\nx2\nx3\nxn\n\nA couple of questions\nGiven a sequence x,\n- What is the probability that x was generated by the model?\n- Given a position i, what is the most likely state that emitted\nxi?\nExample: the dishonest casino\nSay x = 12341623162616364616234161221341\nMost likely path: π = FF......F\nHowever: marked letters more likely to be L than\nunmarked letters\n\nEvaluation\nWe will develop algorithms that allow us to compute:\nP(x)\nProbability of x given the model\nP(xi...xj)\nProbability of a substring of x given the model\nP(πI = k | x) Probability that the ith state is k, given x\nA more refined measure of which states x may be in\n\nThe Forward Algorithm\nWe want to calculate\nP(x) = probability of x, given the HMM\nSum over all possible ways of generating x:\nP(x) = Σπ P(x, π) = Σπ P(x | π) P(π)\nTo avoid summing over an exponential number of paths π,\ndefine\nfk(i) = P(x1...x , π = k) (the forward probability)\ni\ni\n\nl\nThe Forward Algorithm - derivation\nDefine the forward probability:\nf(i) = P(x1...x, π = l)\n= Σ\ni\ni\nπ1...πi-1 P(x1...xi-1, π1,..., πi-1, πi = l) e(x)\nl\ni\n= Σk Σπ1...πi-2 P(x1...xi-1, π1,..., πi-2, πi-1 = k) akl e(x)\nl\ni\n= e(x) Σk fk(i-1) akl\nl\ni\n\nl\nThe Forward Algorithm\nWe can compute fk(i) for all k, i, using dynamic programming!\nInitialization:\nf0(0) = 1\nfk(0) = 0, for all k > 0\nIteration:\nf(i) = e(x) Σk fk(i-1) akl\nl\ni\nTermination:\nP(x) = Σk fk(N) ak0\nWhere, ak0 is the probability that the terminating state is k (usually = a0k)\n\nRelation between Forward and Viterbi\nVITERBI\nFORWARD\nInitialization:\nInitialization:\nV0(0) = 1\nf0(0) = 1\nVk(0) = 0, for all k > 0\nfk(0) = 0, for all k > 0\nIteration:\nIteration:\nf(i) = e(x) Σk fk(i-1) akl\nl\nl\ni\nVj(i) = ej(x) maxk Vk(i-1) akj\ni\nTermination:\nTermination:\nP(x) = Σk fk(N) ak0\nP(x, π*) = maxk Vk(N)\n\ni\nMotivation for the Backward Algorithm\nWe want to compute\nP(π = k | x),\ni\nthe probability distribution on the ith position, given x\nWe start by computing\nP(π = k, x) = P(x1...x, π = k, xi+1...xN)\ni\ni\n= P(x1...x, π = k) P(xi+1...xN | x1...x\ni\ni\n, π = k)\ni\ni\n= P(x1...x, π = k) P(xi+1...xN | πi = k)\ni\ni\nForward, fk(i)\nBackward, bk(i)\n\nThe Backward Algorithm - derivation\nDefine the backward probability:\n= Σ\nbk(i) = P(xi+1...xN | πi = k)\nπi+1...πN P(xi+1,xi+2, ..., xN, πi+1, ..., πN | πi = k)\n= Σl Σπi+1...πN P(xi+1,xi+2, ..., xN, πi+1 = l, πi+2, ..., πN | πi = k)\n= Σl e(xi+1) akl Σπi+1...πN P(xi+2, ..., xN, πi+2, ..., πN | πi+1 = l)\nl\n= Σl el(xi+1) akl bl(i+1)\n\nThe Backward Algorithm\nWe can compute bk(i) for all k, i, using dynamic programming\nInitialization:\nbk(N) = ak0, for all k\nIteration:\nbk(i) = Σ el(xi+1) akl bl(i+1)\nl\nTermination:\nP(x) = Σ a0l el(x1) b(1)\nl\nl\n\nComputational Complexity\nWhat is the running time, and space required, for Forward, and Backward?\nTime: O(K2N)\nSpace: O(KN)\nUseful implementation technique to avoid underflows\nViterbi:\nsum of logs\nForward/Backward: rescaling at each position by multiplying by a\nconstant\n\nPosterior Decoding\nWe can now calculate\nfk(i) bk(i)\nP(π = k | x) =\n-------\ni\nπ\nP(x)\nThen, we can ask\nWhat is the most likely state at position i of sequence x:\nDefine π^ by Posterior Decoding:\n^\ni = argmaxk P(π = k | x)\ni\n\nPosterior Decoding\n- For each state,\n- Posterior Decoding gives us a curve of likelihood of\nstate for each position\n- That is sometimes more informative than Viterbi path\nπ*\n- Posterior Decoding may give an invalid sequence\nof states\n- Why?\n\nMaximum Weight Trace\n- Another approach is to find a sequence of states under\nsome constraint, and maximizing expected accuracy of state\nassignments\n- Aj(i) = maxk such that Condition(k, j) Ak(i-1) + P(π = j | x)\ni\n- We will revisit this notion again\n\nProblem 3: Learning\nRe-estimate the parameters of the\nmodel based on training data\n\nTwo learning scenarios\n1. Estimation when the \"right answer\" is known\nExamples:\nGIVEN:\na genomic region x = x1...x1,000,000 where we have good\n(experimental) annotations of the CpG islands\nGIVEN:\nthe casino player allows us to observe him one evening,\nas he changes dice and produces 10,000 rolls\n2. Estimation when the \"right answer\" is unknown\nExamples:\nGIVEN:\nthe porcupine genome; we don't know how frequent are the\nCpG islands there, neither do we know their composition\nGIVEN:\n10,000 rolls of the casino player, but we don't see when he\nchanges dice\nQUESTION:\nUpdate the parameters θ of the model to maximize P(x|θ)\n\nCase 1.\nWhen the right answer is known\nGiven x = x1...xN\nfor which the true π = π1...πN is known,\nDefine:\nAkl\n= # times k→l transition occurs in π\nEk(b)\n= # times state k in π emits b in x\nWe can show that the maximum likelihood parameters θ are:\na\nAkl\nEk(b)\nkl = -----\nek(b) = -------\nΣ\nEk(c)\nΣi Aki\nc\n\nCase 1.\nWhen the right answer is known\nIntuition: When we know the underlying states,\nBest estimate is the average frequency of\ntransitions & emissions that occur in the training data\nDrawback:\nGiven little data, there may be overfitting:\nP(x|θ) is maximized, but θ is unreasonable\n0 probabilities - VERY BAD\nExample:\nGiven 10 casino rolls, we observe\nx = 2, 1, 5, 6, 1, 2, 3, 6, 2, 3\nπ = F, F, F, F, F, F, F, F, F, F\nThen:\naFF = 1;\naFL = 0\neF(1) = eF(3) = .2;\neF(2) = .3; eF(4) = 0; eF(5) = eF(6) = .1\n\nPseudocounts\nSolution for small training sets:\nAdd pseudocounts\n= # times k→l transition occurs in π + rkl\nAkl\nEk(b)\n= # times state k in π emits b in x\n+ rk(b)\nrkl, rk(b) are pseudocounts representing our prior belief\nLarger pseudocounts ⇒ Strong priof belief\nSmall pseudocounts (ε < 1): just to avoid 0 probabilities\n\nPseudocounts\nExample: dishonest casino\nWe will observe player for one day, 500 rolls\nReasonable pseudocounts:\nr0F = r0L = rF0 = rL0 = 1;\nrFL = rLF = rFF = rLL = 1;\nrF(1) = rF(2) = ... = rF(6) = 20\n(strong belief fair is\nfair)\nrF(1) = rF(2) = ... = rF(6) = 5\n(wait and see for\nloaded)\nAbove #s pretty arbitrary - assigning priors is an art\n\nCase 2.\nWhen the right answer is unknown\nWe don't know the true Akl, Ek(b)\nIdea:\n- We estimate our \"best guess\" on what Akl, Ek(b) are\n- We update the parameters of the model, based on our guess\n- We repeat\n\nCase 2.\nWhen the right answer is unknown\nStarting with our best guess of a model M, parameters θ:\nGiven x = x1...xN\nfor which the true π = π1...πN is unknown,\nWe can get to a provably more likely parameter set θ\nPrinciple: EXPECTATION MAXIMIZATION\n1. Estimate Akl, Ek(b) in the training data\n2. Update θ according to Akl, Ek(b)\n3. Repeat 1 & 2, until convergence\n\nEstimating new parameters\nTo estimate Akl:\nAt each position i of sequence x,\nFind probability transition k→l is used:\nP(π = k, πi+1 = l | x) = [1/P(x)] × P(π = k, πi+1 = l, x1...xN) = Q/P(x)\ni\ni\nwhere Q = P(x1...x, π = k, πi+1 = l, xi+1...xN) =\ni\ni\n= P(πi+1 = l, xi+1...xN | πi = k) P(x1...x, π = k) =\ni\ni\n= P(πi+1 = l, xi+1xi+2...xN | πi = k) fk(i) =\n= P(xi+2...xN | πi+1 = l) P(xi+1 | πi+1 = l) P(πi+1 = l | π = k) fk(i) =\ni\n= b(i+1) el(xi+1) akl fk(i)\nl\nfk(i) akl el(xi+1) bl(i+1)\nSo:\nP(πi = k, πi+1 = l | x, θ) = ------------------\nP(x | θ)\n\nEstimating new parameters\nSo,\nfk(i) akl el(xi+1) bl(i+1)\nAkl = Σi P(π = k, πi+1 = l | x, θ) = Σ -----------------\ni\ni\nP(x | θ)\nSimilarly,\nEk(b) = [1/P(x)]Σ {i | xi = b} fk(i) bk(i)\n\nEstimating new parameters\nIf we have several training sequences, x1, ..., xM, each of length N,\nfk(i) akl el(xi+1) bl(i+1)\nAkl = Σx Σi P(π = k, πi+1 = l | x, θ) = Σx Σi ----------------\ni\nP(x | θ)\nSimilarly,\nEk(b) = Σx (1/P(x))Σ {i | x = b} fk(i) bk(i)\ni\n\nThe Baum-Welch Algorithm\nInitialization:\nPick the best-guess for model parameters\n(or arbitrary)\nIteration:\n1. Forward\n2. Backward\n3. Calculate Akl, Ek(b)\n4. Calculate new model parameters akl, ek(b)\n5. Calculate new log-likelihood P(x | θ)\nGUARANTEED TO BE HIGHER BY EXPECTATION-MAXIMIZATION\nUntil P(x | θ) does not change much\n\nThe Baum-Welch Algorithm - comments\nTime Complexity:\n# iterations × O(K2N)\n- Guaranteed to increase the log likelihood of the model\nP(θ | x) = P(x, θ) / P(x) = P(x | θ) / ( P(x) P(θ) )\n- Not guaranteed to find globally best parameters\nConverges to local optimum, depending on initial conditions\n- Too many parameters / too large model:\nOvertraining\n\nAlternative: Viterbi Training\nInitialization:\nSame\nIteration:\n1.\nPerform Viterbi, to find π*\n2.\nCalculate Akl, Ek(b) according to π* + pseudocounts\n3.\nCalculate the new parameters akl, ek(b)\nUntil convergence\nNotes:\n-\nConvergence is guaranteed - Why?\n-\nDoes not maximize P(x | θ)\n-\nIn general, worse performance than Baum-Welch\n\nHow to Build an HMM\n- General Scheme:\n- Architecture/topology design\n- Learning/Training:\n- Training Datasets\n- Parameter Estimation\n- Recognition/Classification:\n- Testing Datasets\n- Performance Evaluation\n\nParameter Estimation for HMMs (Case 1)\n- Case 1: All the paths/labels in the set of training\nsequences are known:\n- Use the Maximum Likelihood (ML) estimators for:\n- Where Akl and Ek(x) are the number of times each\ntransition or emission is used in training sequences\n- Drawbacks of ML estimators:\n- Vulnerable to overfitting if not enough data\n- Estimations can be undefined if never used in training set\n(add pseudocounts to reflect a prior biases about probability\nvalues)\n∑\n∑\n=\n=\n'\n'\n'\n)'\n(\n)\n(\n\nand\n\nx\nk\nk\nkx\nl\nkl\nkl\nkl\nx\nE\nx\nE\ne\nA\nA\na\n\nParameter Estimation for HMMs (Case 2)\n-\nCase 2: The paths/labels in the set of training\nsequences are UNknown:\n- Use Iterative methods (e.g., Baum-Welch):\n1. Initialize akl and ekx (e.g., randomly)\n2. Estimate Akl and Ek(x) using current values of akl and ekx\n3. Derive new values for akl and ekx\n4. Iterate Steps 2-3 until some stopping criterion is met (e.g.,\nchange in the total log-likelihood is small)\n- Drawbacks of Iterative methods:\n-\nConverge to local optimum\n-\nSensitive to initial values of akl and ekx (Step 1)\n-\nConvergence problem is getting worse for large HMMs\n\nHMM Architectural/Topology Design\n- In general, HMM states and transitions are\ndesigned based on the knowledge of the problem\nunder study\n- Special Class: Explicit State Duration HMMs:\n- Self-transition state to itself:\n- The probability of staying in the state for d residues:\npi (d residues) = (aii)d-1(1-aii) - exponentially decaying\n- Exponential state duration density is often inappropriate\n⇒Need to explicitly model duration density in some form\n- Specified state density:\n- Used in GenScan\nqi\naii\nqj\najj\nqi\nqj\n...\n...\npi(d)\npj(d)\n\nHMM-based Gene Finding\n- GENSCAN (Burge 1997)\n- FGENESH (Solovyev 1997)\n- HMMgene (Krogh 1997)\n- GENIE (Kulp 1996)\n- GENMARK (Borodovsky & McIninch 1993)\n- VEIL (Henderson, Salzberg, & Fasman 1997)\n\nVEIL: Viterbi Exon-Intron Locator\n- Contains 9 hidden states or features\n- Each state is a complex internal Markovian model of the feature\n- Features:\n- Exons, introns, intergenic regions, splice sites, etc.\nExon HMM Model\nUpstream\nStart Codon\nExon\nStop Codon\nIntron\n: start codon or intron\nDownstream\n3' Splice Site\n5' Poly-A Site\n5' Splice Site\n- Enter\n(3' Splice Site)\nVEIL Architecture\n- Exit: 5' Splice site or three stop codons\n(taa, tag, tga)\n\nGenie\n-\n-\n-\n-\ndesigned for signal finding\nBegin\nSequence\nStart\nTranslatio\nn\nDonor\nsplice\nsite\nAccept\nor\nsplice\nsite\nTranslatio\nn\nEnd\nSequence\nUses a generalized HMM (GHMM)\nEdges in model are complete HMMs\nStates can be any arbitrary program\nStates are actually neural networks specially\n- J5' - 5' UTR\n- EI - Initial Exon\n- E - Exon, Internal Exon\n- I - Intron\n- EF - Final Exon\n- ES - Single Exon\n- J3' - 3'UTR\nStop\n\nGenscan Overview\n- Developed by Chris Burge (Burge 1997), in the research group of\nSamuel Karlin, Dept of Mathematics, Stanford Univ.\n- Characteristics:\n- Designed to predict complete gene structures\n- Introns and exons, Promoter sites, Polyadenylation signals\n- Incorporates:\n- Descriptions of transcriptional, translational and splicing signal\n- Length distributions (Explicit State Duration HMMs)\n- Compositional features of exons, introns, intergenic, C+G regions\n- Larger predictive scope\n- Deal w/ partial and complete genes\n- Multiple genes separated by intergenic DNA in a seq\n- Consistent sets of genes on either/both DNA strands\n- Based on a general probabilistic model of genomic sequences\ncomposition and gene structure\n\nGenscan Architecture\n- It is based on Generalized HMM\n(GHMM)\n- Model both strands at once\n- Other models: Predict on one\nstrand first, then on the other strand\n- Avoids prediction of overlapping\ngenes on the two strands (rare)\n- Each state may output a string of\nsymbols (according to some\nprobability distribution).\n- Explicit intron/exon length modeling\n- Special sensors for Cap-site and\nTATA-box\n- Advanced splice site sensors\nFig. 3, Burge and Karlin 1997\nImage removed due to copyright restrictions.\n\nGenScan States\n-\nN - intergenic region\n-\nP - promoter\n-\nF - 5' untranslated region\n-\nEsngl - single exon (intronless)\n(translation start -> stop codon)\n-\nEinit - initial exon (translation start -\n> donor splice site)\n-\nEk - phase k internal exon\n(acceptor splice site -> donor splice\nsite)\n-\nEterm - terminal exon (acceptor\nsplice site -> stop codon)\n-\nIk - phase k intron: 0 - between\ncodons; 1 - after the first base of a\ncodon; 2 - after the second base of\na codon\nE0+\nI0+\nE1+\nE2+\nE\n+\nE\n+\nE\n+\nF+\n(\n)\nP+\n(\n)\nA+\n(\n)\n(\n)\nN\n(\n)\n(+)\n(-)\n(+)\n(-)\nT+\n(\n)\nI1+\nI2+\ninit\nsngl\nterm\n5' UTR\npromo\nter\npoly-A\nsignal\nsingle-exon\ngene\nintergenic\nregion\nForward\nstrand\nReverse\nstrand\nForward\nstrand\nReverse\nstrand\n3' UTR\nFigure by MIT OCW.\n\nAccuracy Measures\nSensitivity vs. Specificity (adapted from Burset&Guigo 1996)\nActual\nPredicted\nTP\nFP\nTN\nFN\nTP\nFN\nTN\nActual\n=\n/\n/\n=\n(\n) - (\n)\n((\n)*(\n)*(\n)*(\n))1\n=\n+\n+\n+\n(\n=\nTP\nSn\nTP\nTP+FN\nCoding No Coding\nNo Coding Coding\nPredicted\nFP\nFN\nTN\nCC\nTP * TN\nFN * FP\nTP+FN\nTN+FP\nTP+FP\nTN+FN\nSn\nTP\nTP+FP\nTP\nTP+FN\nTP\nTP+FP\nTN\nTN+FP\nTN\nTN+FN\nAC\n/2\n-1\n(\nFigure by MIT OCW.\n-Sensitivity (Sn)\nFraction of actual coding regions that are correctly predicted as\ncoding\n-Specificity (Sp)\nFraction of the prediction that is actually correct\n-Correlation\nCombined measure of Sensitivity & Specificity\nCoefficient (CC)\nRange: -1 (always wrong) Æ +1 (always right)\n\nTest Datasets\n- Sample Tests reported by Literature\n- Test on the set of 570 vertebrate gene seqs\n(Burset&Guigo 1996) as a standard for comparison\nof gene finding methods.\n- Test on the set of 195 seqs of human, mouse or rat\norigin (named HMR195) (Rogic 2001).\n\nResults: Accuracy Statistics\nTable: Relative Performance (adapted from Rogic 2001)\nComplicating Factors for Comparison\n- Gene finders were trained on data that\nhad genes homologous to test seq.\n- Percentage of overlap is varied\n- Some gene finders were able to tune\ntheir methods for particular data\n# of seqs - number of seqs effectively analyzed\n- Methods continue to be developed\nby each program; in parentheses is the number\nof seqs where the absence of gene was\nNeeded\npredicted;\nSn -nucleotide level sensitivity; Sp\nlevel specificity;\nCC - correlation coefficient;\nESn - exon level sensitivity; ESp\nspecificity\n- Train and test methods on the same data.\nnucleotide\n- Do cross-validation (10% leave-out)\nexon level\n\nWhy not Perfect?\n-\nGene Number\nusually approximately correct, but may not\n-\nOrganism\nprimarily for human/vertebrate seqs; maybe lower accuracy for non-\nvertebrates. 'Glimmer' & 'GeneMark' for prokaryotic or yeast seqs\n-\nExon and Feature Type\nInternal exons: predicted more accurately than Initial or Terminal exons;\nExons: predicted more accurately than Poly-A or Promoter signals\n-\nBiases in Test Set (Resulting statistics may not be representative)\nThe Burset/Guigo (1996) dataset:\n3⁄4 Biased toward short genes with relatively simple exon/intron structure\nThe Rogic (2001) dataset:\n3⁄4 DNA seqs: GenBank r-111.0 (04/1999 <- 08/1997);\n3⁄4 source organism specified;\n3⁄4 consider genomic seqs containing exactly one gene;\n3⁄4 seqs>200kb were discarded; mRNA seqs and seqs containing pseudo genes or\nalternatively spliced genes were excluded.\n\nWhat We Learned...\n- Genes are complex structures which are difficult to\npredict with the required level of\naccuracy/confidence\n- Different HMM-based approaches have been\nsuccessfully used to address the gene finding\nproblem:\n- Building an architecture of an HMM is the hardest\npart, it should be biologically sound & easy to\ninterpret\n- Parameter estimation can be trapped in local\noptimum\n- Viterbi algorithm can be used to find the most\nprobable path/labels\n- These approaches are still not perfect"
    },
    {
      "category": "Lecture Notes",
      "title": "lectures7and8.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-096-algorithms-for-computational-biology-spring-2005/4141d8b0b5f9b17007120a5a0abad2df_lectures7and8.pdf",
      "content": "Gene Finding and HMMs\nLecture 1 - Introduction\nLecture 2 - Hashing and BLAST\nLecture 3 - Combinatorial Motif Finding\nLecture 4 - Statistical Motif Finding\nLecture 5 - Sequence alignment and Dynamic Programming\nLecture 6 - RNA structure and Context Free Grammars\nLecture 7 - Gene finding and Hidden Markov Models\n6.096 - Algorithms for Computational Biology - Lecture 7\nChallenges in Computational Biology\nDNA\nGenome Assembly\n1 Gene Finding\nRegulatory motif discovery\nDatabase lookup\nGene expression analysis\nRNA transcript\nSequence alignment\nEvolutionary Theory\nTCATGCTAT\nTCGTGATAA\nTGAGGATAT\nTTATCATAT\nTTATGATTT\nCluster discovery\nGibbs sampling\nProtein network analysis\nEmerging network properties\n13 Regulatory network inference\nComparative Genomics\nRNA folding\nOutline\n- Computational model\n- Simple Markov Models\n- Hidden Markov Models\n- Working with HMMs\n- Dynamic programming (Viterbi)\n- Expectation maximization (Baum-Welch)\n- Gene Finding in practice\n- GENSCAN\n- Performance Evaluation\nMarkov Chains & Hidden Markov Models\n-\nMarkov Chain\n- Q: states\n- p: initial state probabilities\n- A: transition probabilities\n-\nHMM\n- Q: states\n- V: observations\n- p: initial state probabilities\n- A: transition probabilities\n- E: emission probabilities\nA+\nT+\nG+\nC+\naGT\naAC\naGC\naAT\nA+\nT+\nG+\nC+\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\nMarkov Chain\nDefinition: A Markov chain is a triplet (Q, p, A), where:\n3⁄4 Q is a finite set of states. Each state corresponds to a symbol in the alphabet Σ\n3⁄4 p is the initial state probabilities.\n3⁄4 A is the state transition probabilities, denoted by ast for each s, t in Q.\n3⁄4 For each s, t in Q the transition probability is: ast ≡P(xi = t|xi-1 = s)\nProperty: The probability of each symbol xi depends only on the\nvalue of the preceding symbol xi-1 : P (xi | xi-1,..., x1) = P (xi | xi-1)\nFormula: The probability of the sequence:\nP(x) = P(xL,xL-1,..., x1) = P (xL | xL-1) P (xL-1 | xL-2)... P (x2 | x1) P(x1)\nOutput: The output of the model is the set of states at each instant\ntime => the set of states are observable\nHMM (Hidden Markov Model)\nDefinition: An HMM is a 5-tuple (Q, V, p, A, E), where:\n3⁄4 Q is a finite set of states, |Q|=N\n3⁄4 V is a finite set of observation symbols per state, |V|=M\n3⁄4 p is the initial state probabilities.\n3⁄4 A is the state transition probabilities, denoted by ast for each s, t in Q.\n3⁄4 For each s, t in Q the transition probability is: ast ≡P(xi = t|xi-1 = s)\n3⁄4 E is a probability emission matrix, esk ≡P (vk at time t | qt = s)\nProperty: Emissions and transitions are dependent on the current state\nonly and not on the past.\nOutput: Only emitted symbols are observable by the system but not the\nunderlying random walk between states -> \"hidden\"\n\nTypical HMM Problems\nAnnotation Given a model M and an observed string\nS, what is the most probable path through M\ngenerating S\nClassification Given a model M and an observed\nstring S, what is the total probability of S under M\nConsensus Given a model M, what is the string\nhaving the highest probability under M\nTraining Given a set of strings and a model structure,\nfind transition and emission probabilities assigning\nhigh probabilities to the strings\nExample 1: Finding CpG islands\nWhat are CpG islands?\n-\nRegions of regulatory importance in promoters of many genes\n- Defined by their methylation state (epigenetic information)\n-\nMethylation process in the human genome:\n- Very high chance of methyl-C mutating to T in CpG\nI CpG dinucleotides are much rarer\n- BUT it is suppressed around the promoters of many genes\nI CpG dinucleotides are much more frequent than elsewhere\n- Such regions are called CpG islands\n- A few hundred to a few thousand bases long\n-\nProblems:\n- Given a short sequence, does it come from a CpG island or not?\n- How to find the CpG islands in a long sequence\nTraining Markov Chains for CpG islands\n-\nTraining Set:\n- set of DNA sequences w/ known CpG islands\n-\nDerive two Markov chain models:\n- '+' model: from the CpG islands\n- '-' model: from the remainder of sequence\n-\nTransition probabilities for each model:\n∑\n+\n+\n+ =\nt'\nst'\nst\nst\nc\nc\na\n+\nst\nc\nis the number of times\nletter t followed letter s\ninside the CpG islands\nT\nG\nC\nA\n+\n.182\n.384\n.355\n.079\nT\n.125\n.375\n.339\n.161\nG\n.188\n.274\n.368\n.171\nC\n.120\n.426\n.274\n.180\nA\nA\nT\nG\nC\naGT\naAC\naGC\naAT\n∑\n-\n-\n-=\nt'\nst'\nst\nst\nc\nc\na\n-\nst\nc\nis the number of times\nletter t followed letter s\noutside the CpG islands\nProbability of C following A\nUsing Markov Models for CpG classification\n-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4\nCpG\nislands\nNon-\nCpG\nQ1: Given a short sequence x, does it come from CpG island (Yes-No question)\n- To use these models for discrimination, calculate the log-odds ratio:\n-\n+\n=\n-\n-\n∑\n=\n-\n+\n=\ni\ni\ni\ni\nx\nx\nx\nx\nL\ni\na\na\n)\nP(x|\n)\nP(x|\nS(x)\n1log\nmodel\nmodel\nlog\nHistogram of log odds scores\nUsing Markov Models for CpG classification\nQ2: Given a long sequence x, how do we find CpG islands in it\n(Where question)\n-\nCalculate the log-odds score for a window of, say, 100 nucleotides around every\nnucleotide, plot it, and predict CpG islands as ones w/ positive values\n-\nDrawbacks: Window size\nUse a hidden state: CpG (+) or non-CpG (-)\n\nHMM for CpG islands\n-\nBuild a single model that combines both\nMarkov chains:\n- '+' states: A+, C+, G+, T+\n- Emit symbols: A, C, G, T in CpG islands\n- '-' states: A-, C-, G-, T-\n- Emit symbols: A, C, G, T in non-islands\n-\nEmission probabilities distinct for the '+'\nand the '-' states\n- Infer most likely set of states, giving rise\nto observed emissions\nI 'Paint' the sequence with + and - states\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\nA: 0\nC: 0\nG: 1\nT: 0\nA: 1\nC: 0\nG: 0\nT: 0\nA: 0\nC: 1\nG: 0\nT: 0\nA: 0\nC: 0\nG: 0\nT: 1\nFinding most likely state path\n- Given the observed emissions, what was the path?\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nC\nG\nC\nG\nstart\nend\nProbability of given path p & observations x\n-\nKnown observations: CGCG\n-\nKnown sequence path: C+, G-, C-, G+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nA+\nT+\nG+\nC+\nA-\nT-\nG-\nC-\nA+\nT+\nG+\nC+\nC\nG\nC\nG\nstart\nend\nProbability of given path p & observations x\n-\nKnown observations: CGCG\n-\nKnown sequence path: C+, G-, C-, G+\nC-\nC+\nG-\nC-\nG+\nC+\nG+\nC\nG\nC\nG\nstart\nend\nProbability of given path p & observations x\n-\nP(p,x) = (a0,C+* 1) * (aC+,G-* 1) * (aG-,C-* 1) * (aC-,G+* 1) * (aG+,0)\nC-\nC+\nG-\nC-\nG+\nC+\nG+\nC\nG\nC\nG\nstart\nend\na0,C+\naC+,G-\naG-,C-\naC-,G+\naG+,0\neC+(C)\neG-(G)\neC-(C)\neG+(G)\nBut in general, we don't know the path!\nThe three main questions on HMMs\n1.\nEvaluation\nGIVEN\na HMM M,\nand a sequence x,\nFIND\nProb[ x | M ]\n2.\nDecoding\nGIVEN\na HMM M,\nand a sequence x,\nFIND\nthe sequence π of states that maximizes P[ x, π | M ]\n3.\nLearning\nGIVEN\na HMM M, with unspecified transition/emission probs.,\nand a sequence x,\nFIND\nparameters θ = (ei(.), aij) that maximize P[ x | θ ]\n\nProblem 1: Decoding\nFind the best parse of a\nsequence\nDecoding\nGIVEN x = x1x2......xN\nWe want to find π = π1, ......, πN,\nsuch that P[ x, π ] is maximized\nπ* = argmaxπ P[ x, π ]\nWe can use dynamic programming!\nLet Vk(i) = max{π1,...,i-1} P[x1...xi-1, π1, ..., πi-1, xi, πi = k]\n= Probability of most likely sequence of states ending at\nstate πi = k\nK\n...\nK\n...\nK\n...\n...\n...\n...\nK\n...\nx1\nx2\nx3\nxK\nK\nDecoding - main idea\nGiven that for all states k,\nand for a fixed position i,\nVk(i) = max{π1,...,i-1} P[x1...xi-1, π1, ..., πi-1, xi, πi = k]\nWhat is Vk(i+1)?\nFrom definition,\nVl(i+1) = max{π1,...,i}P[ x1...xi, π1, ..., πi, xi+1, πi+1 = l ]\n= max{π1,...,i}P(xi+1, πi+1 = l | x1...xi,π1,..., πi) P[x1...xi, π1,..., πi]\n= max{π1,...,i}P(xi+1, πi+1 = l | πi ) P[x1...xi-1, π1, ..., πi-1, xi, πi]\n= maxk P(xi+1, πi+1 = l | πi = k) max{π1,...,i-1}P[x1...xi-1,π1,...,πi-1, xi,πi=k]\n= el(xi+1) maxk akl Vk(i)\nThe Viterbi Algorithm\nInput: x = x1......xN\nInitialization:\nV0(0) = 1\n(0 is the imaginary first position)\nVk(0) = 0, for all k > 0\nIteration:\nVj(i)\n= ej(xi) × maxk akj Vk(i-1)\nPtrj(i) = argmaxk akj Vk(i-1)\nTermination:\nP(x, π*) = maxk Vk(N)\nTraceback:\nπN* = argmaxk Vk(N)\nπi-1* = Ptrπi (i)\nThe Viterbi Algorithm\nSimilar to \"aligning\" a set of states to a sequence\nTime:\nO(K2N)\nSpace:\nO(KN)\nx1\nx2\nx3 ...............................................xN\nState 1\nK\nVj(i)\nViterbi Algorithm - a practical detail\nUnderflows are a significant problem\nP[ x1,...., xi, π1, ..., πi ] = a0π1 aπ1π2......aπi eπ1(x1)......eπi(xi)\nThese numbers become extremely small - underflow\nSolution: Take the logs of all values\nVl(i) = log ek(xi) + maxk [ Vk(i-1) + log akl ]\n\nExample\nLet x be a sequence with a portion of ~ 1/6 6's, followed by a portion of ~ 1⁄2\n6's...\nx = 123456123456...12345 6626364656...1626364656\nThen, it is not hard to show that optimal parse is (exercise):\nFFF........................F LLL..............................L\n6 nucleotides \"123456\" parsed as F, contribute .956×(1/6)6\n= 1.6×10-5\nparsed as L, contribute .956×(1/2)1×(1/10)5 = 0.4×10-5\n\"162636\" parsed as F, contribute .956×(1/6)6\n= 1.6×10-5\nparsed as L, contribute .956×(1/2)3×(1/10)3 = 9.0×10-5\nProblem 2: Evaluation\nFind the likelihood a sequence\nis generated by the model\nGenerating a sequence by the model\nGiven a HMM, we can generate a sequence of length n as follows:\n1. Start at state π1 according to prob a0π1\n2. Emit letter x1 according to prob eπ1(x1)\n3. Go to state π2 according to prob aπ1π2\n4. ... until emitting xn\nK\n...\nK\n...\nK\n...\n...\n...\n...\nK\n...\nx1\nx2\nx3\nxn\nK\ne2(x1)\na02\nA couple of questions\nGiven a sequence x,\n-\nWhat is the probability that x was generated by the model?\n-\nGiven a position i, what is the most likely state that emitted\nxi?\nExample: the dishonest casino\nSay x = 12341623162616364616234161221341\nMost likely path: π = FF......F\nHowever: marked letters more likely to be L than\nunmarked letters\nEvaluation\nWe will develop algorithms that allow us to compute:\nP(x)\nProbability of x given the model\nP(xi...xj)\nProbability of a substring of x given the model\nP(πI = k | x)\nProbability that the ith state is k, given x\nA more refined measure of which states x may be in\nThe Forward Algorithm\nWe want to calculate\nP(x) = probability of x, given the HMM\nSum over all possible ways of generating x:\nP(x) = Σπ P(x, π) = Σπ P(x | π) P(π)\nTo avoid summing over an exponential number of paths π,\ndefine\nfk(i) = P(x1...xi, πi = k) (the forward probability)\n\nThe Forward Algorithm - derivation\nDefine the forward probability:\nfl(i) = P(x1...xi, πi = l)\n= Σπ1...πi-1 P(x1...xi-1, π1,..., πi-1, πi = l) el(xi)\n= Σk Σπ1...πi-2 P(x1...xi-1, π1,..., πi-2, πi-1 = k) akl el(xi)\n= el(xi) Σk fk(i-1) akl\nThe Forward Algorithm\nWe can compute fk(i) for all k, i, using dynamic programming!\nInitialization:\nf0(0) = 1\nfk(0) = 0, for all k > 0\nIteration:\nfl(i) = el(xi) Σk fk(i-1) akl\nTermination:\nP(x) = Σk fk(N) ak0\nWhere, ak0 is the probability that the terminating state is k (usually = a0k)\nRelation between Forward and Viterbi\nVITERBI\nInitialization:\nV0(0) = 1\nVk(0) = 0, for all k > 0\nIteration:\nVj(i) = ej(xi) maxk Vk(i-1) akj\nTermination:\nP(x, π*) = maxk Vk(N)\nFORWARD\nInitialization:\nf0(0) = 1\nfk(0) = 0, for all k > 0\nIteration:\nfl(i) = el(xi) Σk fk(i-1) akl\nTermination:\nP(x) = Σk fk(N) ak0\nMotivation for the Backward Algorithm\nWe want to compute\nP(πi = k | x),\nthe probability distribution on the ith position, given x\nWe start by computing\nP(πi = k, x) = P(x1...xi, πi = k, xi+1...xN)\n= P(x1...xi, πi = k) P(xi+1...xN | x1...xi, πi = k)\n= P(x1...xi, πi = k) P(xi+1...xN | πi = k)\nForward, fk(i)\nBackward, bk(i)\nThe Backward Algorithm - derivation\nDefine the backward probability:\nbk(i) = P(xi+1...xN | πi = k)\n= Σπi+1...πN P(xi+1,xi+2, ..., xN, πi+1, ..., πN | πi = k)\n= Σl Σπi+1...πN P(xi+1,xi+2, ..., xN, πi+1 = l, πi+2, ..., πN | πi = k)\n= Σl el(xi+1) akl Σπi+1...πN P(xi+2, ..., xN, πi+2, ..., πN | πi+1 = l)\n= Σl el(xi+1) akl bl(i+1)\nThe Backward Algorithm\nWe can compute bk(i) for all k, i, using dynamic programming\nInitialization:\nbk(N) = ak0, for all k\nIteration:\nbk(i) = Σl el(xi+1) akl bl(i+1)\nTermination:\nP(x) = Σl a0l el(x1) bl(1)\n\nComputational Complexity\nWhat is the running time, and space required, for Forward, and Backward?\nTime: O(K2N)\nSpace: O(KN)\nUseful implementation technique to avoid underflows\nViterbi:\nsum of logs\nForward/Backward: rescaling at each position by multiplying by a\nconstant\nPosterior Decoding\nWe can now calculate\nfk(i) bk(i)\nP(πi = k | x) =\n-------\nP(x)\nThen, we can ask\nWhat is the most likely state at position i of sequence x:\nDefine π^ by Posterior Decoding:\nπ^\ni = argmaxk P(πi = k | x)\nPosterior Decoding\n- For each state,\n- Posterior Decoding gives us a curve of likelihood of\nstate for each position\n- That is sometimes more informative than Viterbi path\nπ*\n- Posterior Decoding may give an invalid sequence\nof states\n- Why?\nMaximum Weight Trace\n-\nAnother approach is to find a sequence of states under\nsome constraint, and maximizing expected accuracy of state\nassignments\n- Aj(i) = maxk such that Condition(k, j) Ak(i-1) + P(πi = j | x)\n-\nWe will revisit this notion again\nProblem 3: Learning\nRe-estimate the parameters of the\nmodel based on training data\nTwo learning scenarios\n1.\nEstimation when the \"right answer\" is known\nExamples:\nGIVEN:\na genomic region x = x1...x1,000,000 where we have good\n(experimental) annotations of the CpG islands\nGIVEN:\nthe casino player allows us to observe him one evening,\nas he changes dice and produces 10,000 rolls\n2.\nEstimation when the \"right answer\" is unknown\nExamples:\nGIVEN:\nthe porcupine genome; we don't know how frequent are the\nCpG islands there, neither do we know their composition\nGIVEN:\n10,000 rolls of the casino player, but we don't see when he\nchanges dice\nQUESTION:\nUpdate the parameters θ of the model to maximize P(x|θ)\n\nCase 1.\nWhen the right answer is known\nGiven x = x1...xN\nfor which the true π = π1...πN is known,\nDefine:\nAkl\n= # times k→l transition occurs in π\nEk(b)\n= # times state k in π emits b in x\nWe can show that the maximum likelihood parameters θ are:\nAkl\nEk(b)\nakl = -----\nek(b) = -------\nΣi Aki\nΣc Ek(c)\nCase 1.\nWhen the right answer is known\nIntuition: When we know the underlying states,\nBest estimate is the average frequency of\ntransitions & emissions that occur in the training data\nDrawback:\nGiven little data, there may be overfitting:\nP(x|θ) is maximized, but θ is unreasonable\n0 probabilities - VERY BAD\nExample:\nGiven 10 casino rolls, we observe\nx = 2, 1, 5, 6, 1, 2, 3, 6, 2, 3\nπ = F, F, F, F, F, F, F, F, F, F\nThen:\naFF = 1;\naFL = 0\neF(1) = eF(3) = .2;\neF(2) = .3; eF(4) = 0; eF(5) = eF(6) = .1\nPseudocounts\nSolution for small training sets:\nAdd pseudocounts\nAkl\n= # times k→l transition occurs in π\n+ rkl\nEk(b)\n= # times state k in π emits b in x\n+ rk(b)\nrkl, rk(b) are pseudocounts representing our prior belief\nLarger pseudocounts ⇒Strong priof belief\nSmall pseudocounts (ε < 1): just to avoid 0 probabilities\nPseudocounts\nExample: dishonest casino\nWe will observe player for one day, 500 rolls\nReasonable pseudocounts:\nr0F = r0L = rF0 = rL0 = 1;\nrFL = rLF = rFF = rLL = 1;\nrF(1) = rF(2) = ... = rF(6) = 20\n(strong belief fair is\nfair)\nrF(1) = rF(2) = ... = rF(6) = 5\n(wait and see for\nloaded)\nAbove #s pretty arbitrary - assigning priors is an art\nCase 2.\nWhen the right answer is unknown\nWe don't know the true Akl, Ek(b)\nIdea:\n-\nWe estimate our \"best guess\" on what Akl, Ek(b) are\n-\nWe update the parameters of the model, based on our guess\n-\nWe repeat\nCase 2.\nWhen the right answer is unknown\nStarting with our best guess of a model M, parameters θ:\nGiven x = x1...xN\nfor which the true π = π1...πN is unknown,\nWe can get to a provably more likely parameter set θ\nPrinciple: EXPECTATION MAXIMIZATION\n1. Estimate Akl, Ek(b) in the training data\n2. Update θ according to Akl, Ek(b)\n3. Repeat 1 & 2, until convergence\n\nEstimating new parameters\nTo estimate Akl:\nAt each position i of sequence x,\nFind probability transition k→l is used:\nP(πi = k, πi+1 = l | x) = [1/P(x)] × P(πi = k, πi+1 = l, x1...xN) = Q/P(x)\nwhere Q = P(x1...xi, πi = k, πi+1 = l, xi+1...xN) =\n= P(πi+1 = l, xi+1...xN | πi = k) P(x1...xi, πi = k) =\n= P(πi+1 = l, xi+1xi+2...xN | πi = k) fk(i) =\n= P(xi+2...xN | πi+1 = l) P(xi+1 | πi+1 = l) P(πi+1 = l | πi = k) fk(i) =\n= bl(i+1) el(xi+1) akl fk(i)\nfk(i) akl el(xi+1) bl(i+1)\nSo:\nP(πi = k, πi+1 = l | x, θ) = ------------------\nP(x | θ)\nEstimating new parameters\nSo,\nfk(i) akl el(xi+1) bl(i+1)\nAkl = Σi P(πi = k, πi+1 = l | x, θ) = Σi -----------------\nP(x | θ)\nSimilarly,\nEk(b) = [1/P(x)]Σ {i | xi = b} fk(i) bk(i)\nEstimating new parameters\nIf we have several training sequences, x1, ..., xM, each of length N,\nfk(i) akl el(xi+1) bl(i+1)\nAkl = Σx Σi P(πi = k, πi+1 = l | x, θ) = Σx Σi ----------------\nP(x | θ)\nSimilarly,\nEk(b) = Σx (1/P(x))Σ {i | xi = b} fk(i) bk(i)\nThe Baum-Welch Algorithm\nInitialization:\nPick the best-guess for model parameters\n(or arbitrary)\nIteration:\n1.\nForward\n2.\nBackward\n3.\nCalculate Akl, Ek(b)\n4.\nCalculate new model parameters akl, ek(b)\n5.\nCalculate new log-likelihood P(x | θ)\nGUARANTEED TO BE HIGHER BY EXPECTATION-MAXIMIZATION\nUntil P(x | θ) does not change much\nThe Baum-Welch Algorithm - comments\nTime Complexity:\n# iterations × O(K2N)\n-\nGuaranteed to increase the log likelihood of the model\nP(θ | x) = P(x, θ) / P(x) = P(x | θ) / ( P(x) P(θ) )\n-\nNot guaranteed to find globally best parameters\nConverges to local optimum, depending on initial conditions\n-\nToo many parameters / too large model:\nOvertraining\nAlternative: Viterbi Training\nInitialization:\nSame\nIteration:\n1.\nPerform Viterbi, to find π*\n2.\nCalculate Akl, Ek(b) according to π* + pseudocounts\n3.\nCalculate the new parameters akl, ek(b)\nUntil convergence\nNotes:\n-\nConvergence is guaranteed - Why?\n-\nDoes not maximize P(x | θ)\n-\nIn general, worse performance than Baum-Welch\n\nHow to Build an HMM\n- General Scheme:\n- Architecture/topology design\n- Learning/Training:\n- Training Datasets\n- Parameter Estimation\n- Recognition/Classification:\n- Testing Datasets\n- Performance Evaluation\nParameter Estimation for HMMs (Case 1)\n- Case 1: All the paths/labels in the set of training\nsequences are known:\n- Use the Maximum Likelihood (ML) estimators for:\n- Where Akl and Ek(x) are the number of times each\ntransition or emission is used in training sequences\n- Drawbacks of ML estimators:\n- Vulnerable to overfitting if not enough data\n- Estimations can be undefined if never used in training set\n(add pseudocounts to reflect a prior biases about probability\nvalues)\n∑\n∑\n=\n=\n'\n'\n'\n)'\n(\n)\n(\n\nand\n\nx\nk\nk\nkx\nl\nkl\nkl\nkl\nx\nE\nx\nE\ne\nA\nA\na\nParameter Estimation for HMMs (Case 2)\n-\nCase 2: The paths/labels in the set of training\nsequences are UNknown:\n-\nUse Iterative methods (e.g., Baum-Welch):\n1. Initialize akl and ekx (e.g., randomly)\n2. Estimate Akl and Ek(x) using current values of akl and ekx\n3. Derive new values for akl and ekx\n4. Iterate Steps 2-3 until some stopping criterion is met (e.g.,\nchange in the total log-likelihood is small)\n-\nDrawbacks of Iterative methods:\n-\nConverge to local optimum\n-\nSensitive to initial values of akl and ekx (Step 1)\n-\nConvergence problem is getting worse for large HMMs\nHMM Architectural/Topology Design\n- In general, HMM states and transitions are\ndesigned based on the knowledge of the problem\nunder study\n- Special Class: Explicit State Duration HMMs:\n- Self-transition state to itself:\n- The probability of staying in the state for d residues:\npi (d residues) = (aii)d-1(1-aii) - exponentially decaying\n- Exponential state duration density is often inappropriate\n⇒Need to explicitly model duration density in some form\n- Specified state density:\n- Used in GenScan\nqi\naii\nqj\najj\nqi\nqj\n...\n...\npi(d)\npj(d)\nHMM-based Gene Finding\n- GENSCAN (Burge 1997)\n- FGENESH (Solovyev 1997)\n- HMMgene (Krogh 1997)\n- GENIE (Kulp 1996)\n- GENMARK (Borodovsky & McIninch 1993)\n- VEIL (Henderson, Salzberg, & Fasman 1997)\nVEIL: Viterbi Exon-Intron Locator\n-\nContains 9 hidden states or features\n-\nEach state is a complex internal Markovian model of the feature\n-\nFeatures:\n- Exons, introns, intergenic regions, splice sites, etc.\nExon HMM Model\nUpstream\nStart Codon\nExon\nStop Codon\nDownstream\n3' Splice Site\nIntron\n5' Poly-A Site\n5' Splice Site\n- Enter: start codon or intron (3' Splice Site)\n- Exit: 5' Splice site or three stop codons\n(taa, tag, tga)\nVEIL Architecture\n\nGenie\n- Uses a generalized HMM (GHMM)\n- Edges in model are complete HMMs\n- States can be any arbitrary program\n- States are actually neural networks specially\ndesigned for signal finding\n- J5' - 5' UTR\n- EI - Initial Exon\n- E - Exon, Internal Exon\n- I - Intron\n- EF - Final Exon\n- ES - Single Exon\n- J3' - 3'UTR\nBegin\nSequence\nStart\nTranslatio\nn\nDonor\nsplice\nsite\nAccept\nor\nsplice\nsite\nStop\nTranslatio\nn\nEnd\nSequence\nGenscan Overview\n-\nDeveloped by Chris Burge (Burge 1997), in the research group of\nSamuel Karlin, Dept of Mathematics, Stanford Univ.\n-\nCharacteristics:\n- Designed to predict complete gene structures\n- Introns and exons, Promoter sites, Polyadenylation signals\n- Incorporates:\n- Descriptions of transcriptional, translational and splicing signal\n- Length distributions (Explicit State Duration HMMs)\n- Compositional features of exons, introns, intergenic, C+G regions\n- Larger predictive scope\n- Deal w/ partial and complete genes\n- Multiple genes separated by intergenic DNA in a seq\n- Consistent sets of genes on either/both DNA strands\n-\nBased on a general probabilistic model of genomic sequences\ncomposition and gene structure\nGenscan Architecture\n-\nIt is based on Generalized HMM\n(GHMM)\n-\nModel both strands at once\n- Other models: Predict on one\nstrand first, then on the other strand\n- Avoids prediction of overlapping\ngenes on the two strands (rare)\n-\nEach state may output a string of\nsymbols (according to some\nprobability distribution).\n-\nExplicit intron/exon length modeling\n-\nSpecial sensors for Cap-site and\nTATA-box\n-\nAdvanced splice site sensors\nFig. 3, Burge and Karlin 1997\nGenScan States\n-\nN - intergenic region\n-\nP - promoter\n-\nF - 5' untranslated region\n-\nEsngl - single exon (intronless) (translation\nstart -> stop codon)\n-\nEinit - initial exon (translation start ->\ndonor splice site)\n-\nEk - phase k internal exon (acceptor\nsplice site -> donor splice site)\n-\nEterm - terminal exon (acceptor splice site\n-> stop codon)\n-\nIk - phase k intron: 0 - between codons; 1\n- after the first base of a codon; 2 - after\nthe second base of a codon\nAccuracy Measures\nSensitivity vs. Specificity (adapted from Burset&Guigo 1996)\nCombined measure of Sensitivity & Specificity\nRange: -1 (always wrong) Æ +1 (always right)\n-Correlation\nCoefficient (CC)\nFraction of the prediction that is actually correct\n-Specificity (Sp)\nFraction of actual coding regions that are correctly predicted as\ncoding\n-Sensitivity (Sn)\nTP FP TN FN TP\nFN TN\nActual\nPredicted\nCoding / No Coding\nTN\nFN\nFP\nTP\nPredicted\nActual\nNo Coding / Coding\nTest Datasets\n- Sample Tests reported by Literature\n- Test on the set of 570 vertebrate gene seqs\n(Burset&Guigo 1996) as a standard for comparison\nof gene finding methods.\n- Test on the set of 195 seqs of human, mouse or rat\norigin (named HMR195) (Rogic 2001).\n\nTable: Relative Performance (adapted from Rogic 2001)\n# of seqs - number of seqs effectively analyzed\nby each program; in parentheses is the number\nof seqs\nwhere the absence of gene was\npredicted;\nSn -nucleotide level sensitivity; Sp - nucleotide\nlevel specificity;\nCC - correlation coefficient;\nESn - exon level sensitivity; ESp - exon level\nspecificity\nResults: Accuracy Statistics\nComplicating Factors for Comparison\n- Gene finders were trained on data that\nhad genes homologous to test seq.\n- Percentage of overlap is varied\n- Some gene finders were able to tune\ntheir methods for particular data\n- Methods continue to be developed\nNeeded\n- Train and test methods on the same data.\n- Do cross-validation (10% leave-out)\nWhy not Perfect?\n-\nGene Number\nusually approximately correct, but may not\n-\nOrganism\nprimarily for human/vertebrate seqs; maybe lower accuracy for non-\nvertebrates. 'Glimmer' & 'GeneMark' for prokaryotic or yeast seqs\n-\nExon and Feature Type\nInternal exons: predicted more accurately than Initial or Terminal exons;\nExons: predicted more accurately than Poly-A or Promoter signals\n-\nBiases in Test Set (Resulting statistics may not be representative)\nThe Burset/Guigo (1996) dataset:\n3⁄4 Biased toward short genes with relatively simple exon/intron structure\nThe Rogic (2001) dataset:\n3⁄4 DNA seqs: GenBank r-111.0 (04/1999 <- 08/1997);\n3⁄4 source organism specified;\n3⁄4 consider genomic seqs containing exactly one gene;\n3⁄4 seqs>200kb were discarded; mRNA seqs and seqs containing pseudo genes or\nalternatively spliced genes were excluded.\nWhat We Learned...\n-\nGenes are complex structures which are difficult to predict\nwith the required level of accuracy/confidence\n-\nDifferent HMM-based approaches have been successfully\nused to address the gene finding problem:\n- Building an architecture of an HMM is the hardest part, it should\nbe biologically sound & easy to interpret\n- Parameter estimation can be trapped in local optimum\n-\nViterbi algorithm can be used to find the most probable\npath/labels\n-\nThese approaches are still not perfect"
    }
  ]
}