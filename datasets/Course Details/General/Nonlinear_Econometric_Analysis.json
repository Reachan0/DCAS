{
  "course_name": "Nonlinear Econometric Analysis",
  "course_description": "This course presents micro-econometric models, including large sample theory for estimation and hypothesis testing, generalized method of moments (GMM), estimation of censored and truncated specifications, quantile regression, structural estimation, nonparametric and semiparametric estimation, treatment effects, panel data, bootstrapping, simulation methods, and Bayesian methods. The methods are illustrated with economic applications.",
  "topics": [
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics",
    "Microeconomics",
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics",
    "Microeconomics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1.5 hours / session\n\nDescription\n\nThis course presents micro-econometric models, including large sample theory for estimation and hypothesis testing, generalized method of moments (GMM), estimation of censored and truncated specifications, quantile regression, structural estimation, nonparametric and semiparametric estimation, treatment effects, panel data, bootstrapping, simulation methods, and Bayesian methods. The methods are illustrated with economic applications.\n\nThe first half of the course (Part A) is taught by Prof. Chernozhukov, while the second half (Part B) is taught by Prof. Newey.\n\nPrerequisites\n\nThe prerequisites include 14.382 Econometrics or permission of the instructor.\n\nTexts\n\nPlease see the\nreadings\n.\n\nGrading\n\nThe grades for each part count equally towards the final grade.\n\nPart A - Chernozhukov\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets\n\n60%\n\nMidterm exam\n\n40%\n\nPart B - Newey\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets\n\n40%\n\nFinal exam\n\n60%\n\nCalendar\n\nThe following three articles are referenced in the Recitation topics. Articles that pertain to lecture topics can be found in the\nreadings\n.\n\nAhn, H., and J. L. Powell. \"Semiparametric Estimation of Censored Selection Models with a Nonparametric Selection Mechanism.\"\nJournal of Econometrics\n58 (1993): 3-29.\n\nAutor, D., L. F. Katz, and M. S. Kearney. \"\nRising Wage Inequality: The Role of Composition and Prices\n.\" National Bureau of Economic Research (NBER) Working Paper No. 11628 (August 2005): 1-65.\n\nChernozhukov, V., and H. Hong. \"Three-step Censored Quantile Regression and Extramarital Affairs.\"\nJournal of the American Statistical Association\n97, no. 459 (September 2002): 872-882.\n\nThe calendar below provides information on the course's lecture (L) and Recitation (R) sessions. Part A consists of sessions L1-L12, while Part B consists of sessions L13-L25.\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nL1\n\nMethods for nonlinear models: maximum likelihood estimation (MLE), generalized method of moments (GMM), minimum distance, extremum\n\nProblem set A-1 out two days after Ses #L1\n\nL2-L3\n\nLarge sample theory, asymptotic theory, discrete choice, censoring, and sample selection\n\nR1\n\nExtremum estimators, variance estimation, hypothesis tests\n\nL4-L5\n\nLarge sample theory, asymptotic theory, discrete choice, censoring, and sample selection (cont.)\n\nR2\n\nML computation: probit using ordinary least squares (OLS) command, hypothesis tests; two-step estimation: Heckman correction\n\nProblem set A-1 due\n\nProblem set A-2 out\n\nL6\n\nBootstrap, subsampling, and finite-sample methods\n\nR3\n\nBootstrap\n\nL7\n\nBootstrap, subsampling, and finite-sample methods (cont.)\n\nL8\n\nQuantile regression (QR) and distributional methods\n\nR4\n\nQuantile regression: integral transformation/Skorohod representation, conditional means vs. conditional quantiles, inference for quantile regression, and high-tech application: wage decompositions (Autor, Katz, and Kearney 2005)\n\nProblem set A-2 due\n\nL9\n\nQuantile regression (QR) and distributional methods (cont.)\n\nR5\n\nQR applications: 3-step procedure for censored QR (Chernozhukov and Hong 2002); digression: duration models; brief introduction to R, wage decomposition\n\nL10-L11\n\nBayesian and quasi-Bayesian methods (from a classical view)\n\nR6\n\nAccept-reject sampling, the Gibbs sampler, and Monte Carlo optimization\n\nL12\n\nBounds and partial identification\n\nProblem set A-3 out\n\nMidterm exam two days after Ses #L12\n\nL13-L14\n\nGMM: identification, estimation, testing, bias, selecting moments\n\nProblem set B-1 out on Ses #L14\n\nR7\n\nDuration models: main concepts, practical issues; GMM: higher-order bias for two stage least squares (2SLS) estimation, adding moments and efficiency\n\nL15\n\nWeak and many instruments\n\nL16\n\nNonparametric estimation\n\nProblem set A-3 due\n\nR8\n\nNonparametric regression: theoretical bias and variance of the Nadaraya-Watson estimator, confidence intervals, bandwidth choice: cross-validation in kernel regression\n\nL17\n\nNonparametric estimation (cont.)\n\nProblem set B-1 due\n\nProblem set B-2 out one day after Ses #L17\n\nR9\n\nGMM with condition moment restriction: optimal IV vs. efficient weighting matrix, example from Problem set B-1; nonparametric regression: kernel regression asymptotics, local linear estimation, bandwidth selection, generalized cross-validation\n\nL18-L19\n\nSemiparametric estimation\n\nL20\n\nTreatment effects\n\nL21\n\nNonlinear models in panel data\n\nProblem set B-2 due\n\nR10\n\nSeries estimation and discontinuities, an example for the partially linear model: semiparametric selection models (Ahn and Powell 1993); treatment effects: the LaLonde debate\n\nProblem set B-3 out\n\nL22\n\nNonlinear models in panel data (cont.)\n\nL23\n\nEconomic modeling and econometrics\n\nR11\n\nNonlinear panel data: incidental parameters problem, conditional MLE: Logit case; method of simulated moments (MSM): brief introduction to numerical integration, simulated estimation\n\nL24-L25\n\nEconomic modeling and econometrics (cont.)\n\nProblem set B-3 due\n\nFinal exam seven days after Ses #L25",
  "files": [
    {
      "category": "Resource",
      "title": "newey_discrete.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/f7217805bef9d3c6bda7480a87469ba4_newey_discrete.pdf",
      "content": "X\nZ\nDiscrete Choice and Censoring\nWhitney K. Newey\nMIT\nNovember, 2004\nDiscrete Choice\nMultinomial Choice: Data consists of consumer choices of various goods, along char\nacteristics. Let there be J choices and y = (y1, . . . , yJ ) where yj = 1 if good j is chosen\nand yj = 0 otherwise. Let x be observed characteristics of the goods and choices. Here a\nconditional density for y corresponds to conditional choice probabilities P (j|x, β), one for\neach j, with PJ\nj=1 P (j|x, β) = 1 for all β and x. Then\nJ\nln f(y|x, β) =\nyj ln P (j|x, β).\nj=1\nFor example, the multinmial logit model has x = (x1, . . . , xJ ) and\njβ\nx\ne\nP (j|x, β) =\n.\nPJ\nkβ\nx\ne\nk=1\nThis model has a random utility interpretation. If the utility of choice j is xj\n0 β + εj where\nεj are i.i.d. over j with Type I Extreme Value distributions (with CDF e-e-ε ), then the\nprobability that j has the highest utility, and is thus chosen, has the form given above.\nMcFadden used this to predict the effect of the introduction of BART on ridership of public\nand private transportation in the San Francisco Bay area.\nOne problem with this model is that P (j|x, β)/P (k|x, β) = e x0\njβ-x0\nkβ depends only on\nthe characteristics of alternatives j and k (this is called the independence from irrelevant\nalternatives property, or IIA). Approaches to deal with this include allowing β to be random\nand allowing εj to be correlated with each other. Allowing β to be random would lead to\nchoice probabilities of the form\njγ\nx\ne\nP (j|x, β) =\nh(γ|β)dγ.\nPJ\nkγ\nx\ne\nk=1\nHard to compute. Multivariate normal εj probabilities (called multinomial probit) also hard\nto compute.\nA case with correlated εj that can be computed is nested logit.\nFor y =\nln(exp(x1\n0 β/λ) + exp(x2\n0 β/λ)),\n0 jβ/λ λy\ne\nx\ne\nP (j|x, β) =\n. j = 1, 2,\nβ + eλy)\ney(ex\nβ\nx\ne\nP (3|x, β)\n=\n.\nex0\n3β + eλy\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nλ(t, α) =\n;\nWeibull; allows ∂λ > 0, ∂λ < 0\nα\n∂t\n∂t\nμ\n¶\nMultinomial logit on branches.\nDuration Models\nT : Lifetime or duration (e.g., unemployment, firm lifetimes).\nx: Regressors (covariates).\nGoal: Estimate effect of x on T ; also estimate how conditional density of T depends on\nT .\nImportant general issue is censoring.\nGeneral parametric model: Let θ be a parameter vector, x regressors, and conditional\nsurvivor function\nS(t | x, θ) = Pr(T ≥ t | x, θ)\nComplete model for conditional distribution of T given x; other ways to describe this model.\n- d\nf(t | x, θ)\n=\ndt S(t | x, θ);\nconditional pdf\nλ(t | x, θ)\n=\nf(t|x,θ) = - d ln S(t | x, θ); hazard rate\nS(t|x,θ)\ndt\nR t\nΛ(t | x, θ)\n=\n0 λ(t | x, θ);\nintegrated hazard\nRelationships: above and S(t|x, θ) = exp(-Λ(t|x, θ)).\nWe use the representation that is most convenient for a particular application. Some\ntheories imply things about hazard, e.g. declining reservation wage in search theory implies\n∂λ (t|x, θ) > 0, when T is the length of an umemployment spell.\n∂t\nHistorically important class of models are proportional hazards\nλ(t | x, θ) = λ(t, α) exp(x 0β), θ =\nα\n.\nβ\nHere changes in x just shift hazard up and down, i.e., shape of hazard as function of t entirely\ndetermined by λ(t; α)\nλ(t | x, θ)\nλ(t, α)\n=\n.\nλ(\nλ(\nt | x, θ)\nt, α)\nMotivation: Convenient starting point, computationally, historically. Also implied by some\ntheoretical models. See Heckman chapter, Handbook of Econometrics, Volume 2. λ(t, α) is\ncalled \"baseline hazard\".\nExamples:\nλ(t, α) = α;\nconstant\n\nα tα -1\n\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nn\ni\nX\nX\n·\n\nX\nX\nX\nCensoring\nHave to account for effects of sampling in likelihood. Longer spells will be more likely to\nappear in data when sampling at fixed points in time. See diagram. Length biased sampling.\nA general principle illustrated here is that ignoring sampling based on the endogenous variable\nwill lead to inconsistent estimates.\nCase 1. Random sample of completed spells.\nObservations are\n(T1, x1), . . . , (Tn, xn)\nThe MLE maximizes\nQˆn(θ) = 1\nln f(Ti|xi, θ)\nn\ni\nAlmost never have data like this.\nCase 2. Sample of unemployed people. Sample unemployed, ask them how long they have\nbeen unemployed, and then follow them until they are employed again. So, the likelihood\nmust condition on the fact that they are unemployed when surveyed. If we know that they\nhave been unemployed for ti periods, then condition on Ti ≥ ti. (This setting analogous to\nthat where only observe data when Yi is positive). The MLE maximizes\nˆ\nf(Ti|xi, θ)\nQn(θ)\n=\nln\nn\nS(ti|xi, θ)\ni\n=\n[ln f(Ti|xi, θ) - ln S(ti|xi, θ)]\nn\ni\nCase 3. Right censoring. Do not observe completed spells for everyone. For some we\njust know duration is greater than ci. For example, we survey unemployed once and find\nti, then survey them sometime later, and record when spell ended or whether they are still\nunemployed. Let di = 1 if complete spell is observed, di = 0 if only know lasts at least to ci.\nThe MLE maximizes\nˆQn(θ) =\n[di ln f(Ti|xi, θ) + (1 - di)S(ci|xi, θ) - ln S(ti|xi, θ)]\nn\ni\nCase 4. Discrete data. Do not know any durations. Only know whether spell is shorter\nor longer than ci. Like when survey the second time we only know whether unemployment\nhas ended.\nˆQn(θ) =\n{di ln[S(ti|xi, θ) - S(ci|xi, θ)] + (1 - di) ln S(ci|xi, θ) - ln S(ti|xi, θ)}\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nZ\nDiscrete data like this is common in applications, e.g. weeks of unemployment data. We\nwill discuss more later. Difficult to also include employed people at start; have to assume\nsomething about arrival rate.\nExample: Lancaster (1979).\nBritish unemployment data.\nConsider effect of ignoring\nobserved heterogeneity. Lancaster, Weibull proportional hazards; λ(tˆ; α) = ctαˆ2-1 . Lancaster\nTable III.\nˆα2\nRegressors\n.67\nnone\n.74\nln(age);\n.768 ln(age), ln(regional unemp), benefits.\n.773\n\"\n\"\n\",\nEarnings\nLancaster considered proportional hazards with heterogeneity. Let a proportional hazard\nmodel conditional on unobserved heterogeneity\nS(t|x, θ, v) = exp(-Λ(t; α) exp(x 0β)v)\nSuppose we model survivor function as\nS(t|x, θ, γ) =\nS(t|x, θ, v)f(v; γ)dv = Mv(-Λ(t, α) exp(x 0β), γ),\nwhere Mv(s, γ) is the moment generating function of v. For instance, for a gamma density\nf(v|γ) = vγ-1e-v/Γ(γ), the moment generating function is Mv(s, γ) = (1 - s)-γ , so that\nS(t|x, θ, γ) = [1 + Λ(t, α) exp(x 0β)]-γ .\nWhen Lancaster estimates using this as the model for survivor function, he obtains αˆ = .90,\nnot significantly different than 1.\nOrdered Data\nOften duration data is discrete and ordered, like weeks of unemployment. Good model\nhere is transformed regression model with unknown transformation (takes care of scaling\nproblems). Also applies to duration data. This model specifies that there is an unknown,\nstrictly increasing function τ(t) with\nτ(T ) = -x 0β + u, u has CDF G(u, γ).\nHere the function τ(t) is nonparametric, not having specified functional form. Thus, the\nmodel is semiparametric, in that it has a parametric and nonparametric part.\nFor the moment suppose we have completed spells.\nCan construct simple estimator.\nSubdivide [0, inf) into intervals Ij = [tj-1, tj), (j = 1, . . ., J), with t0 = 0, tJ = inf. Let\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(\nX\nX\nX\nX\n1,\nT ∈ Ij,\nyj =\n, (j = 1, . . ., J),\notherwise\nand τj = τ(tj ), (j = 1, . . . , J - 1), τ0 = -inf, τJ = +inf. By τ(t) strictly increasing,\nPr(yj = 1|x) = Pr(t ∈ Ij |x) = Pr(tj-1 ≤ T < tj |x) = Pr(τj-1 ≤ τ(T ) < τj |x)\n= Pr(τj-1 ≤-x 0β + u < τj ) = Pr(τj-1 + x 0β ≤ u < τj + x 0β)\ndef\n= G(τj + x 0β, γ0) - G(τj-1 + x 0β, γ0) = Pj (x, θ),\nwhere θ = (β0, γ0, τ1, . . . , τJ-1)0 . Note that these probabilities depend on just the parameters\nτ1, . . . , τJ-1 and not the whole function. Thus, the likelihood is parametric, although the\nmodel is semiparametric. The log-likelihood of a single observation is\nJ\nln f(y|x, θ) =\nyj ln Pj(x, θ).\nj=1\nThis is concave in β and τ1, . . . , τn if the log of the density of G(u, γ) is concave.\nProportional hazards specification is when G(u, γ) is a mixture of Type I extreme value\nwith some other distribution (i.e., u = ε + η, where ε is Type I extreme value; if not, then\nthis is not proportional hazards model). In this case τ(t) = ln Λ(t), so that τj = ln Λ(tj ) and\nso a finite difference approximation to the hazard rate is\nλˆ(tj ) = (e τˆj - e τˆj-1 )/(tj - tj-1).\nCensoring can be handled like before. To handle left censoring, we consider the conditional\nlikelihood given that T ≥ tj` , where tj` is greater than or equal to the censoring point. For\nright censoring, we can consider the likelihood when we only know that one of yj = 1 occurs\nfor T ≥ tjr . For yc = 1 when censoring occurs and zero otherwise, the resulting log-likelihood\nis\nJ\nln f(y|x, θ) = (1 - yc)\nyj ln Pj(x, θ) + yc ln[\nPj (x, θ)] - ln[\nPj (x, θ)].\njr>j≥j`\nj≥jr\nj≥j`\nHan and Hausman (1990) give application. Data from PSID set created by Katz (1986).\nWaves 14 and 15. Interviewer asks whether unemployed last year and duration in weeks. An\nswer either length, or still unemployed. Thus no left censoring but still have right censoring.\nActually have recalls or new jobs, treat same. Could treat different, see paper. For results,\nsee\n\ntables and graphs.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "385a_ps1_07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/5d8ddd087d057ccd67f13610083f9979_385a_ps1_07.pdf",
      "content": "Problem Set 1\nMIT 14.385, Fall 2007\nDue: Friday, 21 September 2007, in class\nThis problem set emphasizes theory. Next week's problem set will consist of empirical applica\ntions of these results.\nPrepare very brief, precise answers. Irrelevant, lengthy explanations will be penalized by the\nmeans of negative points. State clearly any additional assumptions if needed. You are strongly\nencouraged to discuss the pset in groups but the final write-up should be individual.\nThe page numbers give a rough indication how detailed your answer should be. For each problem,\nyou can get either the full number of points for a good answer, half the points for an incomplete or\nonly partly correct answer, or no points at all. If you receive less than the full number of points on\na given problem, you may hand in a revised answer for that problem one week after you got back\nthe problem set, and get up to 90 percent of the original score for a correct answer.\nSee the last page for some hints.\n1. Basic Consistency Exercise #1 [5 points, 1/2 page]:\nConsider the model\nyt = β0 + ut\nwhere β0 is an unknown scalar parameter and the ut are i.i.d. disturbances with\nE [ut]\n=\nE\n£\nut\n2¤\n= β0\nTrue, false or uncertain: computing βˆ using the minimum distance function\nT μ\n¶2\nX yt - β\nβ\nt=1\nyields a consistent estimator.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n¡\n¢\n¡\n¢\n2. Basic Consistency Exercise #2 [5 points, 1/2 page]:\nSuppose\nlog yt = β0 + xtβ1 + ut\nwhere ut ∼ N 0, σ2 i.i.d. with ut independent of xt.\nTrue, false or uncertain: if one implements NLLS (nonlinear least squares) to estimate the\nmodel\nyt = exp (γ0 + xtβ1) + at\nthe resulting estimate of β1 is consistent.\n3. Nonlinear Regression with a Transformation of the Dependent Variable [15 points, 1.5 pages]:\nConsider the empirical relation\n(yt + γ) = δ + xtβ + εt\nwhere εt ∼ 0, σ2 i.i.d., xt is a scalar exogenous variable and γ, δ, β are parameters. Evaluate\nthe following statements as true, false or uncertain:\n(a) NLLS applied to this specification yields consistent estimates for δ and β but not for γ.\n(b) Nonlinear IV estimation using 1, xt, xt\n2 as instruments produces estimates for γ, δ and β\nthat are consistent.\n4. Least Absolute Deviation [20 points, 1 page]\nConsider the model\nyt = θ1/2 + ut\nwith ut i.i.d. and P (ut ≤ 0) = 1\n2 . ut has a strictly positive density. Assume E [|yt - θ|] < inf.\n(a) True, false or uncertain: θ1/2 is the median of yt\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nX\nX\nX\n(b) True, false or uncertain: The LAD (least absolute deviations) estimator\nT\nθˆ\n≡ arg min\n|yt - θ|\nθ T t=1\nT μ\n¶\n≡ arg min\n- 1 {yt < θ} (yt - θ)\nθ T\nt=1\nis a consistent estimator of θ1/2.\nHint: use convexity and the fact that\n·\n\n∂\n∂\nE [|yt - θ|] = E\n|yt - θ|\n∂θ\n∂θ\n·\n\n= E 1 {yt < θ} - 2\n∂\nto check if θ1/2 is the unique optimum of the limit objective function. (Note: ∂θ |yt - θ|\nis not defined when yt = θ, but for a given θ, P (yt = θ) = 0, so this is unimportant.)\n(c) True, false or uncertain: OLS is a consistent estimator of θ1/2.\n(d) True, false or uncertain: (Extra points) The condition E [|yt - θ|] < inf is needed for\nconsistency in (b).\n(e) Now consider a new the error term ut , with P ( ut ≤ 0) = τ and the model is\nyt = θτ + ut\nTrue, false or uncertain: θτ is the τ-quantile of yt.\nNow let the τ-quantile estimator be\nT\nθˆτ ≡ arg min\n(τ - 1 {yt < θ}) (yt - θ)\nθ T t=1\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nX\nThis is sometimes written as\nˆ\nT n\n+\n- o\nθτ ≡ arg min\nτ (yt - θ)\n+ (1 - τ) (yt - θ)\nθ T t=1\nwhere x+ = max (x, 0) and x- = - min (x, 0). Be sure you understand why these are\nequivalent. Note that the median discussed above is a special case.\n(f) True, false or uncertain: θˆτ is consistent for θ.\n(g) Why might we ever be interested in θτ for τ 6= 1\n2 ?\n5. Censored Sample - Top-Coding [25 points, 2.5 pages]\nConsider the model\n∗\ny = x 0β0 + ε ; ε|x ∼ N\n¡\n0, σ0\n2¢\nwhere y ∗ is the log of income and x are the variables that should predict income, e.g. age,\neducation, ability, etc. Suppose that high incomes are censored for confidentiality reasons.\nRather than observing (yi\n∗ , xi), we observe\n(yi\n∗ , xi) if yi\n∗\n<\nL\n(L, xi) if yi\n∗\n≥ L\nL is known. Another way to write this is that you only observe\nyi = min (yi\n∗, L)\n(a) What is the conditional mean of y given x, i.e.\nE [ y | x ]\nNote that this is observed y, not the true y ∗ .\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(b) Suppose you run an OLS regression on the data you have. Will βˆLS be consistent for\nβ0? Give intuition as to why or why not. (Hint: a sketch will help.)\n(c) Explain how the expression for the conditional mean you derived in part (a) can be used\nfor Nonlinear Least Squares (NLS) estimation of β0. Is the NLS estimate consistent? A\nvery brief answer suffices.\n(d) What is the conditional mean of y given x and that y < L, i.e.\nE [ y | x ; y ∗ < L ]\n(e) Suppose you run an OLS regression on the non-censored data, i.e. OLS on (yi\n∗ , xi) for\n∗\nall i such that yi < L. Will βˆLS be consistent for β0? Give intuition as to why or why\nnot. (Hint: a sketch will help.)\n(f) What is the (conditional) likelihood function of y given the observed x? Is the MLE\nconsistent (a brief answer suffices)? Give the large sample distribution of the MLE.\n(g) (Truncated Sample) Now consider the model\n∗\ny = x 0β0 + ε ; ε|x ∼ N\n¡\n0, σ0\n2¢\nwhere y ∗ is the log of income and x are the variables that should predict income, e.g.\nage, education, ability, etc. Suppose that high incomes are excluded from the sample,\ni.e. if yi\n∗ < L, we observe (yi, xi) with yi = yi\n∗, whereas if yi\n∗ > L we do not observe\nanything. L is known.\n- What is the conditional mean of y given x and that y is observed, i.e.\nE [y | x; y ∗ < L]\n- Suppose you run an OLS regression on the data you have. Will βˆLS be consistent\nfor β0?\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nR\n- What is the (conditional) likelihood function of y given the observed x? Is the MLE\nconsistent? A very brief answer suffices.\n6. Consistency of QMLE / mis-specified MLE [15 points, 2 pages]\nConsider a linear exponential density of the form\nf (y|m) = exp (A (m) + B (y) + C (m) y)\nwhere\nyf (y|m) dy = m.\n(a) Warmup question: does the Poisson distribution\nf (y | λ) = e -λλy /y!\nbelong in this family?\n(b) What is E [ln f (y|m)] when y ∼ f (y|m0)? (Note: we are looking at the general linear\nexponential density now, the Poisson just applied to part (a).)\n(c) Using the information inequality show that for any m0, A (m) + C (m) m0 is maximized\n(as a function of m) at m = m0.\n(d) What is E [ln f (y|m)] in general, i.e. when y is not distributed f (y|m)?\n(e) Show that as long as E [y] = m0, E [ln f (y|m)] is maximized at m0, even if y ¿ f (y|m).\n(f) Think about the result in (e). Think some more. What implications does this have for\nthe consistency of the MLE of m when y does not have an exponential density?\n(g) Extend the previous analysis to show that if the conditional likelihood has the form\nf (y|x, β, γ) = exp {A (h (x, β) , x, γ) + B (y, x, γ) + C (h (x, β) , x, γ) y}\nthen\nE [ln f (y|x, β, γ)]\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwill be maximized at β = β0 so long as E [y|x] = h (x, β0)\n(h) Do the following models belong to the family described in part (f)?\ni. Normal:\ny = x 0β0 + ε ; ε ∼ N (0, σ0)\nii. Logit:\nexp (x0β0)\nP (y = 1 | x, β0)\n=\n1 + exp (x0β0)\niii. Poisson regression\nf (y | λ)\n= e -λλy /y!\nλ = exp (x 0β0)\n(i) Suppose you are interested in firms' applications for patents. You run a Poisson regression\nmodel (as in 6.g.iv above). However, the truth (unbeknownst to you) is that patents\nactually follow a negative binomial model (which permits the variance to differ from the\nmean), but the mean is correctly specified.\ni. Will your estimate be consistent? Asymptotically normal? If yes, give the asymp\ntotic variance of the MLE.\nii. (Optional) Will your estimated standard errors be consistent? (This gets beyond\nthe consistency results we're emphasizing in this pset, but if you've studied the\nasymptotic distribution of extremum estimators, MLEs and QMLEs you should be\nable to answer this one. If not, don't worry about it.)\n7. Consistency and asymptotic normality of logit MLE [20 points, 2 pages]\nIn the previous questions, we asked for informal arguments for consistency and normality.\nIn this question, you will need to go through a formal proof of consistency and asymptotic\nnormality. A good model is the proof for the probit case is in the lecture notes or in Newey\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand McFadden handbook chapter. Unlike in other problems try to be pedantic about details.\nThe purpose of this exercise is for you to study and review the more technical material of the\nlecture notes. Be as concise as possible.\n(a) Consider the usual binary choice logit model where y ∈{0, 1} and P (y = 1 | x) =\nΛ (x0β0), with Λ (x0β) = exp (x0β) / (1 + exp (x0β)). Give conditions on the distribu\ntion of x that are sufficient for the consistency and asymptotic normality (CAN) of the\nlogit MLE βˆ and prove that, under these conditions, βˆML is CAN. You may use without\nproof the result that log Λ (t) and log(1 - Λ (t)) are concave in t. (Bonus points for\nproving this.)\n(b) For the same problem define the NLLS estimator and give conditions for its consistency\nand asymptotic normality. You may use \"high level\" assumptions if you can't prove the\nmore basic ones hold.\n(c) State how you would construct estimates of the variance-covariance matrix. Derive the\nWald, Lagrange Multiplier and Likelihood Ratio Test statistics to test the hypothesis\nβ1 = 1. You do not have to be pedantic about details in this part.\n8. Consistency and asymptotic normality of GMM [15 points, 2 pages].\nAssume that agents allocate their investments and consuption across time such that the first\norder condition\n·\nμ\n¶γ\n\nCt+1\nE βRt+1\n- 1 Zt = 0\nCt\nholds for β = β0 and γ = γ0. Here, β0 is the non-stochastic discount factor, γ0 is the parameter\nof risk aversion, Ct is consumption at time t, Rt+1 is the return on a risky asset with price\nPt such that Rt+1 = Pt+1/Pt. A sample {Ct, Pt, Zt}T\nt=1 is observed. Zt are variables that\nrepresent the information available at time t.\n(a) Very briefly derive the above equation from the standard representative agent model\nwith power utility. (See Hansen and Singleton (1982, Econometrica)).1 Why is rational\n1Hansen, Lars Peter, and Kenneth J. Singleton. \"Generalized Instrumental Variables Estimation of Nonlinear\nRational Expectations Models.\" Econometrica 50, no. 5 (Sep. 1982): 1269-1286.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nexpectation assumption crucial for this derivation?\n(b) Assume that you have a set of random variables Zt where Zt takes values in Rd for some\nconstant d. Also define the parameter space Θ ⊂ R2 . Define\nμ\n¶γ\nCt+1\n%t(β, γ) = βRt+1\n- 1\nCt\nAssuming that {%t(β0, γ0)zt}T\nt=1 are serially uncorrelated (extra credit: which additional\nassumptions would be needed for that in the economic model?), write down the criterion\nfunction for a GMM estimator of (β, γ) that is based on instruments zt. Pay particular\nattention to the weighting matrix. How would you estimate the weight matrix?\n(c) Show that your GMM estimator is consistent and find the limiting distribution of your\nGMM estimator for (β, γ). State any additional assumptions you may need.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nUseful Hints\nHere are a few things that you should keep in mind when checking consistency.\n1. Continuous mapping theorem: If θˆ is consistent for θ and g is a continuous function, then\ng(θˆ) is consistent for g(θ).\n2. A good way to check consistency is to check whether the limit of your objective function is\nin fact minimized at the truth.\n3. In extremum estimation problems, with smooth limit objective functions, we can check for\ninconsistency by looking whether the first-order condition is satisfied by the parameter value\nθ0we want to estimate:\nrθQ(θ0) = 0\nFailure of this condition implies inconsistency. Note that for multivariate θ, if even one equa\ntion in this system fails, it may imply inconsistency of all the components, unless equations are\nindependent. For concave g(z, θ) functions, the FOC is a sufficient condition for identifiability,\nso we can go the other way and infer the identifiability condition if this is satisfied.\n4. GMM/Nonlinear IV estimators: These are based on a moment condition (i) E(g(zt, θ0)) = 0\nunder the true value θ0 and (ii) E(g(zt, θ)) =6\n0 under other values of θ. Usually one checks\nthe first part for consistency and assumes (this is wishful thinking) that the second part\nholds (particularly for non-linear g functions). Many common estimators can be thought\nof as GMM estimators. For instance, consistency of OLS is based on the fact that the\nregressors are uncorrelated with the error terms, so it can be considered a GMM estimator\nwith g(zt, θ) = xt2t = xt(yt - xtβ). IV (2SLS) is based on g(zt, θ) = wt(yt - xtβ) where wt\nare the instruments.\nφ(L)\n5. If u ∼ N (0, 1), then E[u|u < L] = - Φ(L) , called Mill's ratio.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "385a_ps2_07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/4cb492b3195af41ef5484d4398b6e484_385a_ps2_07.pdf",
      "content": "Problem Set 2\nMIT 14.385, Fall 2007\nDue: Wednesday, 10 October 2006 5pm\nThis problem set gives you a choice between (a) solving a number of applied/empirical problems\nand/or (b) proving some of the theoretical results which were stated but not proven during the\nlecture. Doing both the applied part (a) and the theoretical part (b) means that you don't have to\ntake the midterm exam.\nFor the empirical part, you may use any software package you like. If you use a canned package\nsuch as Stata, you must describe exactly what it is that your nonlinear estimation command is\ndoing (e.g. how does it iterate, what is its convergence criterion, how does it calculate the Hessian\nand outer product of the score, etc. Please hand in clean, well-written, thoroughly annotated code\nand clearly formatted, readable summaries of your empirical results. Bonus points will be given for\nresults tables created within your code. Negative points will be awarded for unreadable, confusing\nor sloppy code.\nApplied/Empirical Problems\nThe purpose of this exercise is to acquaint you with a few commonly applied nonlinear estimators.\nThe data set for the empirical exercises comes from a subset of the NLSY used by Herrnstein and\nMurray in their book, The Bell Curve (no endorsement implied).\n1. Smoking and Lost Workdays [Problem 15.6 from Wooldridge]\nConsider taking a large random sample of workers at a given point in time. Let sicki = 1\nif person i called in sick during the last 90 days, and zero otherwise. Let zi be a vector of\nindividual and employer characteristics. Let cigsi be the number of cigarettes individual i\nsmokes per day (on average).\n(a) Explain the underlying experiment of interest when we want to examine the effects of\ncigarette smoking on workdays lost.\nProblem 1, parts (a) through (f), courtesy of MIT Press. Used with permission.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(b) Why might cigsi be correlated with unobservables affecting sicki?\n(c) One way to write the model of interest is\nP (sick = 1|z, cigs, q1) = Φ(z1δ1 + γ1cigs + q1)\nwhere z1 is a subset of z and q1 is an unobservable variable that is possibly correlated with\ncigs. What happens if q1 is ignored and you estimate the probit of sick on z1, cigs?\n(d) Can cigs have a conditional normal distribution in the population? Explain.\n(e) Explain how to test whether cigs is exogenous. Does this test rely on cigs having a\nconditional normal distribution?\n(f) Suppose that some of the workers live in states that recently implemented no-smoking\nlaws in the workplace. Does the presence of the new laws suggest a good IV candidate for\ncigs?\nThe following parts are not from Wooldridge:\n(g) What happens if Jerry catches you running The Forbidden Regression? What is The\nForbidden Regression and why is it so Forbidden? Try to answer in words and from memory\nrather than in equations from a textbook. If you don't remember, talk to your friends before\nconsulting a book. Is Wooldridge asking you to run a Forbidden Regression in part (f)?\n(h) In part (f), Wooldridge asks about IV in a general sense. Be more specific. What would\nyou actually do to run IV in this setting? Write down the equation(s).\n2. Women and Work\nConsider the following model:\nA woman's weekly wage rate is determined by the equation\nlog w = x 0\n(1)\nW βW + uW\nwhere w is the woman's weekly wage rate and the vector x contains an intercept, education\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n¡\n¢\nE, age A, AFQT , race, A2 , A ∗ E and AFQT ∗ E\nWeeks of work are determined by the equation\nWeeks = xWK\nγWK + γW log w + v\n= x 0βWK + uwk\n(2)\nwhere Weeks is weeks worked, and xWK contains an intercept, family income, and a dummy\nas to whether the woman is currently married or not. The vector x contains all of the unique\nelements of xW and xWK . The discrete variable δ is an indicator for a positive number of\nweeks worked, i.e. δ = 1 {W eeks > 0}.\nFor the following, use the data set women.txt.\n(a) Probit and multinomial logit\ni. Estimate the parameters determining labor force participation assuming that the\nerror uwk ∼ N 0, σ2 in model (??). Test the hypothesis that marriage influences\nparticipation using both Wald and likelihood ratio test statistics. Construct boot\nstrap standard errors and compare these to the ML standard errors.\nii. Consider a three-state classification of a woman's hours of work: she doesn't work at\nall (designate by setting the discrete variable b = 1); she works part of the year which\nimplies that 0 < W eeks < 20 (designated by b = 1); and she works most of the year\nwith Weeks ≥ 20 (b = 3). Discuss whether a multinomial logit model is appropriate\nhere. Estimate a multinomial logit model for P (b = j | x). Discuss how to compute\nconditional probabilities at a given (\"interesting\") x. Using your estimation results,\ntest whether marriage influences the likelihood that a woman works part of the year\ninstead of most of the year. Finally, construct bootstrap standard errors for your\nestimated coefficients and compare these to the ML standard errors.\n(b) Censored Samples\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAssume that the errors (uW , uWK ) follow a bivariate normal distribution:\n⎛\n⎞\n⎛⎛ ⎞ ⎛\n⎞⎞\nuW\nσ11\nσ12\n⎜\n⎝\n⎟\n⎠ ∼ N ⎜\n⎝ ⎜\n⎝ ⎟\n⎠ , ⎜\n⎝\n⎟\n⎠ ⎟\n⎠\nuW K\nσ12\nσ22\nData are available on wages only for those who work a positive number of weeks.\ni. Where does this model fit in Amemiya's classification of Tobit models (Tobit I\nV)? Where did the censoring question on last week's problem set (Q5) fit in the\nclassification? What is the essential difference between the two models?\nii. Discuss briefly why OLS will not be a satisfactory technique for estimating the\ncoefficients βW of model (??).\nSince OLS is not satisfactory, we will explore more sophisticated techniques. Es\ntimate the coefficients βW of model (??) accounting for sample selection using the\nfollowing three methods:\niii. Two-step estimation of the censored regression model (\"censored regression model\"\nrefers to the fact that there is a selection problem here and you are observing only\na (censored) subset of the whole data). Hint: see Wooldridge 563-564.\niv. Nonlinear least squares estimation of the censored regression model.\nv. Maximum likelihood using the generalized tobit model (\"generalized tobit\" refers to\nthe fact that the censoring point for each observation is not fixed, but depends on\nX's, while \"usual tobit\" has a fixed censoring point for every observation)\n(c) Simultaneous Equations with Dummy Endogenous Variables\nIn place of equation (??), consider the wage equation:\nlog w = x 0 βW + dα + uW\n(3)\nW\nwhere d = 0 if b = 2 and d = 1 if b = 3 (we do not observe wages for those who do\nnot work, so we do not need to specify a value of d when b = 1). The coefficient α\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nmay be thought of as representing a wage premium for full-time work. Assume that\nE [uW | d = 1, x] = 0 in (??).\ni. Briefly discuss why OLS will be biased\nii. Estimate the parameters of model (??) using instrumental variables.\niii. Test for the endogeneity of d.\n3. Ability and Poverty/Work - use the combined data sets women.txt and men.txt for this\nexercise.\nConsider the following logit model:\nP = α0 + α1Educ + α2SES + α3Age + α4Sex + α5Race + α6AFQT + ε1\n(4)\n(a) In model (??) let P = 1 if the individual was in poverty in 1989 and P = 1 otherwise.\ni. Estimate (??) using the logit model both unrestricted and restricting α6 = 0.\nii. Test whether including ability as measured by the AFQT reduces the magnitude of\nα5 (in absolute value).\niii. Test whether α5 = 0 in both the unrestricted model and the restricted model.\niv. Evaluate the following claim: \"Black-white differences in poverty rates disappear\nonce ability is accounted for.\"\n(b) In model (??) let P = 1 if the individual was out of the labor force for more than a month\nin 1989 and P = 0 otherwise. Repeat the exercises in (i) and evaluate the following claim:\n\"Black-white differences in labor force attachment disappear once ability is accounted\nfor.\"\n1.1\nOverall Tips:\n- We have covered the general theory of nonlinear / extremum estimators in class; this problem\nset will give you the opportunity to implement this knowledge. You may need to do some\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nbackground reading on your own to familiarize yourself with the specific estimators used:\nWooldridge 16.1-16.7 and 17.1-17.6 is a good place to start.\n- Write down the model before you start running regressions. For example, in 2(b) you should\nwrite down the correct conditional mean, adjusting for the selection problem. You have done\nthis in previous theoretical problem sets so it should not be a problem. It will also make clear\nto you what to tell the computer to do.\n- You should mention clearly your method of estimation-if running OLS/probit etc, write your\nequation clearly and say which variables are on the LHS and on the RHS. For non-linear\nestimators (MLE, NLS etc) write down explicitly the Q function you are maximizing and\nwhat formula you are using to get the standard errors.\n- Turn in the relevant computer output and written materials to answer the following questions.\nAgain, if you use a canned package be sure you say exactly what it is doing.\n- Useful Stata commands include heckman, nl, tobit\n- If you want a warmup / small extra-credit exercise, replicate the Keane and Wolpin example\nfrom W, Section 15.9 and / or do W 15.7.\n- Question 2(b) asks you about Amemiya's classification of Tobit models. See Amemiya (1985),\nChapter 10 or Wooldridge Chapters 16-17 for discussion of this classification.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTheory Problems\nIn this section we ask you to prove some results given in the lecture notes. In some cases there are\na few hints in the lecture notes. Also, answers are typically available in research articles, but give\na try to each problem before looking up the answers.\nSome problems are hard, so we do not expect you to solve them. However, in order to earn\nan A grade, we expect that you will attempt to solve each problem (i.e. write down you thinking\nabout possible lines of attack), and that a half or more of the questions will be solved correctly.\n1. Prove the uniform law of large numbers (ULLN) for dominated moment functions as stated\nin Lemma 1 on the lecture 2 handout.\n2. Prove uniform convergence of the objective function under the stochastic equicontinuity as\nsumption (Lemma 2, lecture 2 handout). Note the hint below the statement of the Lemma\nin the notes.\n3. Prove uniform convergence of the objective function under the H older continuity condition\n(Lemma 3, lecture 2 handout).\n4. Prove uniform convergence of the objective function under convexity of the sample objective\nfunction (Lemma 4, lecture 2 handout).\n5. Prove Theorem 2 (consistency under convexity) in lecture 2 notes.\n6. Prove the technical result on the exchange in integration and differentiation in lecture 5 notes.\nComment on the application of this lemma to information matrix inequality in \"regular\"\nlikelihood models. State an example of a non-regular model where information matrix equality\ndoes not hold and hence Cramer-Rao efficiency bound does not apply. Explain how this\nexample fails to satisfy the exchangeability of differentiation and integration.\n7. Supply the details of the proof of the bootstrap consistency for the sample mean (cf. Lecture\n7). You can find answers in published articles, but give it a try before you look up these\nanswers.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8. Solve the question on the bootstrap refinement listed in lecture 7. The idea of this question\nis for you to work through the answer already provided in Horowitz handbook chapter, and\nreally learn the details of this result. The details of Edgeworth expansions are mentioned in\nAmemiya's text and in Van der Vaart.\n9. Write down and explain the details of subsampling consistency, as requested in lecture note\n7. You can find a good discussion in Horowitz and Romano, Politis, Wolf's book.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "385a_ps3_07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/28de6de8e9a9fd00783564cd8e1cfd7e_385a_ps3_07.pdf",
      "content": "Problem Set 3:\nBootstrap, Quantile Regression and MCMC Methods\nMIT 14.385, Fall 2007\nDue: Wednesday, 07 November 2007, 5:00 PM\nApplied Problems\nInstructions:\nThe page indications given below give you an upper bound on what you should write. Answers\nshould be very brief. Hand in clear, annotated code. You are strongly encouraged to work in\ngroups, but the write-ups should be individual.\n1.1\nBootstrap\n1. Explain briefly what the bootstrap is, why it works when it works and how it can fail. (1/2\npage max, informal reasoning is fine)\n2. Find an empirical paper that uses the bootstrap. Explain why they use the bootstrap. (1/4\npage max) Write \"pseudo-code\" ( a brief outline of an algorithm) for how you would reproduce\nthe bootstrap used in this paper. (1/2 page max)\n3. What is the bootstrap bias correction method? How does it work? State the pseudo-code for\nbias correction. (1/2 page max, informal reasoning is fine; hint: read Horowitz)\n4. Two-step estimators have messy standard errors, which makes bootstrap inference appealing.\nConsider the censored regression 2-step estimator of PSet 2, Q1.b.iii. Bootstrap the stan\ndard errors. Compare these standard errors to those generated for you by the Stata canned\ncommand.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nR\n1.2\nQuantile Regression\n1. Theory. Briefly answer the following questions.\nFundamentals:\n(a) (1/4 page max) Consider the location-scale model\nY = X0α + (X0γ) u\nwhere u is independent of x and has distribution function Fu (u) Write down the con\nditional quantile function of y given x in the form\nQY (τ|x) = x 0β (τ)\nCharacterize β (τ) as a function of τ (e.g. monotonicity if any ...). How does the answer\nchange if (x0γ) = 1?\n(b) (1/4 page max) Show that E[Y |x] =\nQY (τ|x) dτ is of the form x0β, where β is what?\n(c) (1/4 page max) Consider a general linear quantile model, QY (τ|x) = x0β (τ) corresponds\nto the random coefficient model Y = X0β(U). Can we have slope coefficients β (τ) that\nbehave non-monotonically in τ? Recall Doksum's quantile treatment effect example.\n(d) (1/2 page max) Using (a)-(b), give a brief conceptual evaluation of the linear quantile\nregression model. Compare it with the classical linear model, and state the advantages\nand disadvantages of the two. Refer to at least one empirical example discussed in class\nwhere the difference in models is important. E.g. refer to Doksum's guinea pig example.\n(e) (1 page max) Quantile equivariance to monotone transformations\nSuppose the conditional quantile function of y given x is in the form\nQY (τ|x) = x 0β (τ)\nNow consider the following models of data transformation\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\ni. y ∗ = f(y) = max {y, 0}\nii. y ∗ = f(y) = 1 {y > 0}\niii. y ∗ = f(y) = exp (y)\nFor each of the above, write down the conditional quantile function for the trasnformed\nvariable conditional on x:\nQY ∗ (τ|x) = f(x 0β (τ))\nHint: quantile models have a remarkable property -- equivariance to monotone trans\nformations, which we (should) observe in these examples. (Useful reference:\nhttp://www.econ.uiuc.edu/~roger/research/rq/rq.pdf + lecture notes).\nIs the same property true of the conditional mean function, i.e. is E[Y ∗|x] = f[E[Y |x]]\n? Why is this a problem?\n(f) (1/2 page) For each of the above three cases write down an estimators of β (τ) that use\ndata on (y ∗ , x) only. (Hint: Case (i) generates the censored quantile regression estimator,\ncase (ii) the maximum score / perception / single-layer neural network estimator, and\ncase (iii) a nonlinear quantile regression estimator.)\n(g) (1/4 page maximum) Summarize the findings above very briefly, and say why they are\nimportant for applications to duration models and censored regression models.\n2. (1 page, excluding figures.). Empirical: Reproduce or closely replicate Figures 1, Table 1,\nthen Figure 2 of Koenker's \"Vignette\" (posted in Recitation Materials section on\nalternatively Roger Koenker's website). You may use any software you like, but is probably\njust easier to use R (which is a great statistical freeware). As usual, you should briefly explain\nwhat the canned commands are doing. The Engel data are available from within R or on the\n3. For this question, use the data in \"Penn46.ascii\". See Section 5 of Koenker and Billias,\n\"Quantile Regression for Duration Data,\" Empirical Economics, 2001, for a description of the\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\nMIT Server\nMIT Server.\n\nprogram and the data. For part (a), review W. Newey's lecture note on duration models or\nWooldridge 20.1-20.2.\n(a) (1 page) Conventional duration models usually boil down to an empirical relation of the\nfollowing kind:\nh (Ti) = xi\n0 β + ui\n(1)\nwhere Ti is a \"survival time\" (here, the duration of an unemployment spell), h () is\nmonotonic and ui is iid with CDF Fu.\n(i) Show that if we assume a Cox proportional hazard model, we can write the duration\nof unemployment in the form in (a) above with\nh (T ) = log Λ0 (T )\nwhere Λ0 (t) is the integrated baseline hazard.\n(ii) Show that in the special case of a Weibull baseline hazard, we have\nlog Λ0 (T ) = γ log T - α\n(iii) Combine your results from (i) and (ii) to show that, under the assumptions of Cox\nproportional hazard and Weibull baseline hazard, the log of the unemployment duration\nsatisfies the following equation:\nlog Ti = xi\n0 β + ui\nThis is a special case of the model called the Accelerated Failure Time (AFT) model.\n(Note: AFT models do not make parametric assumptions on the error term).\n(b) (1/2 page) Show that an implication of the model (1) is that the covariates only shift\nthe location of the distribution, not the scale, i.e. the conditional quantile function of\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n¡\n¢\nh (T ) is\nQh(T ) (τ|x) = x 0β + F -1 (τ)\nu\nfor τ ∈ (0, 1).\nShow that we can write the conditional quantile function of T as QT (τ|x) = h-1 x0β + Fu\n-1 (τ)\nDo covariates affect the location of T only (but not the scale etc.?)\n(c) (1/2 page) Now consider a quantile regression alternative to (1). Explain how quantile\nregression can be used to estimate more general models of the form Qh(T ) (τ|x) = x0β(τ)\n, and how if we wanted, we could recover an estimate of\nQT (τ|x) = h-1 (x β (τ))\nWhat are pros and cons of quantile regression approach relative to the approach outlined\nin (a) and (b)?\n(d) (1 page, excluding graphs) Using the Penn data, estimate the AFT model by maximum\nlikelihood (using the Weibull distribution -- state what the likelihood function is) and\nby OLS, using the following regressors:\nindicators for the 5 treatment groups, with treatments 4 and 6 pooled; indicators for\nfemale, black, and hispanic respondents; number of dependents, with 2 indicating two or\nmore dependents; indicators for the 5 quarters of entry to the experiment; indicator for\nwhether the claimant \"expected to be recalled\"; indicator of whether the respondent was\n\"young\" -- less than 35 -- or \"old\" -- greater than 54; indicator for whether claimant\nwas employed in the durable goods industry; indicator for whether the claimant was\nregistered in a low employment district (Coatesville, Reading or Lancaster). (All these\nare coded for you in the data set and it should be clear from the variable names what\nto use.)\nPresent your results in a neat, clear format that is easy to read. Give a brief discussion\nof the key results.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(e) (1 page, excluding graphs) Again using the Penn data and the same regressors, estimate\nthe model Qlog(T ) (τ|x) = x0β (τ) by quantile regression for τ = {0.20, 0.25, . . ., 0.75, 0.80}.\nPresent your results graphically.\n(f) (1/4 page) What do you make of results in (d) and (e)?\n1.3\nBayesian and Quasi-Bayesian Estimators\n1. Explain briefly what these estimators do. Give a simple one page explanation.\n2. Explain what is (quasi) posterior mean, median, quantile. Are posterior mean and medians\ninferior estimators compared to the extremum estimates? Are they consistent, asymptotically\nnormal? Do posterior quantiles provide valid inference when generalized information equality\nholds (satisfied for example when GMM function has optimal weighting matrix). (1/2 page\nmax)\n1.4\nIntroduction to MCMC:\n1. Consider doing IV median regression using GMM criterion function for the model:\ny = βx + u, u ∼ N (0, 1),\nwhere x = z +v, v ∼ N (0, 1) and z ∼ N(0, 1). Here z is the instrument, x is the endogenous\nvariable, and y is outcome. Suppose the true value of β is 0. Suppose the sample size is 100.\nWrite an MCMC (Metroppolis) algorithm that produces a posterior mean and median esti\nmates of β. Compute also the confidence intervals for β based on posterior quantiles. Report\nyour estimates and confidence intervals. Briefly explain (one paragraph) how you would set\nup the objective function to use in the quasi-bayes estimation. Briefly explain (a paragraph)\nwhat the code is doing. Below I provide the sample code for this problem, which is written\nin R. Feel free to use it, but make sure to explain it. Print out the graphs as well.\nAlso, I should note that for future work, that there are several professional implementations\nof the Metropolis algorithm, for example, you can find one by Charles Geyer at his homepage\nat the stats department of the University of Minnesota (you do not need it for this problem).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTheory Problems\nIf you do this option you can skip the bootstrap question (1.1) and the empirical portion of the\nquantile regression question (but still do the non-empirical parts). You also need to do Bayesian\nand the MCMC questions.\n1. Read parts of Andrew's Handbook chapter (p. 2249-58) that deals with stochastic equicon\ntinuity and its use for deriving the asymptotic properties of estimators. Present the Andrews argu\nment for the asymptotic normality. Explain the role of stochastic equicontinuity in this derivation.\nExplain how one can verify the stochastic equicontinuity via bracketing (see p. 2276-).1\n2. Use Andrew's approach to characterize the asymptotic distribution of quantile regression\nestimator. Rigorously verify the stochastic equicontinuity of a relevant empirical process, using the\nbracketing entropy approach (you may also use symmetrization approach if you like).\n1Andrews, Donald W. K. \"Empirical Process Methods in Econometrics.\" In Handbook of Econometrics, vol. 4,\nedited by Daniel McFadden and Robert Engle. Amsterdam, The Netherlands: North-Holland Publishing Co., 1994,\nchap. 37, pp. 2247-2294. ISBN: 9780444887665.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Exam",
      "title": "sample_exam.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/aa4e6ce694366e8ba812adc7ad5bd925_sample_exam.pdf",
      "content": "14.385. Sample Questions for the Midterm. There will be three\nquestions.\nQuestion 1.\n(30 minutes)\n(a) Briefly outline linear quantile regression model, list its advantages\nand disadvantages as compared to the classical linear location\nmodel.\n(b) Briefly explain the equivariance to monotone transformations that\nquantile regression models have and conditional mean models don't.\nState some interesting examples.\nQuestion 2.\n(30 minutes)\n(a) Briefly explain why and when the bootstrap works.\n(b) What is the bootstrap bias correction method? State the pseudo\ncode for bias correction.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Exam",
      "title": "midterm_exam2007.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/1e2d87062cc99f6cbeee3c1d72ab2891_midterm_exam2007.pdf",
      "content": "14.385. Midterm.\nQuestion 1.\n(30 minutes)\n(a) Briefly explain the Skorohod representation for linear quantile re\ngression model. Explain some special cases (location or location-\nscale models).\n(b) Briefly explain the equivariance to monotone transformations that\nquantile regression models have and conditional mean models don't.\nState some interesting examples.\nQuestion 2.\n(30 minutes)\n(a) What is the bootstrap bias correction method? State the pseudo\ncode for bias correction.\n(b) Briefly explain when the bootstrap might fail.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nQuestion 3.\n(30 minutes)\n(a) Explain briefly what Bayesian estimators do. Explain what poste\nrior mean, median, and quantiles are.\n(b) Are posterior mean and medians inferior estimators compared to\nthe maximum likelihood estimators? Are they consistent, asymp\ntotically normal? Brief answers suffice.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "local_lin_reg.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/40cac15c5fb98b5bf72b8cfaab90c273_local_lin_reg.pdf",
      "content": "X\nX\nX\nX\nX\nLocally Linear Regression:\nThere is another local method, locally linear regression, that is thought to be superior to\nkernel regression. It is based on locally fitting a line rather than a constant. Unlike kernel\nregression, locally linear estimation would have no bias if the true model were linear. In\ngeneral, locally linear estimation removes a bias term from the kernel estimator, that makes\nit have better behavior near the boundary of the x's and smaller MSE everywhere.\nTo describe this estimator, let Kh(u) = h-rK(u/h) as before. Consider the estimator\ngˆ(x) given by the solution to\nn\nmin\n(Yi - g - (x - xi)0β)2Kh(x - xi).\ng,β i=1\nThat is gˆ(x) is the constant term in a weighted least squares regression of Yi on (1, x - xi),\nwith weights Kh(x - xi). For\n⎛\n⎞\n⎛\n⎞\nY1\n1 (x - x1)0\n⎜ .\n⎟\n⎜ .\n.\n⎟\nY = ⎜ .\n⎟ , X = ⎜ .\n.\n⎟\n.\n.\n.\n⎝\n⎠\n⎝\n⎠\nYn\n1 (x - xn)0\nW = diag(Kh(x - x1), . . . , Kh(x - xn))\nand e1 a (r + 1) × 1 vector with 1 in first position and zeros elsewhere, we have\ngˆ(x) = e1\n0 (X0WX)-1X0WY.\nThis estimator depends on x both through the weights Kh(x -xi) and through the regressors\nx - xi.\nThis estimator is a locally linear fit of the data. It runs a regression with weights that\nare smaller for observations that are farther from x.\nIn constrast, the kernel regression\nestimator solves this same minimization problem but with β constrained to be zero, i.e.,\nkernel regression minimizes\nn\n(Yi - g)2Kh(x - xi)\ni=1\nRemoving the constriant β = 0 leads to lower bias without increasing variance when g0(x) is\ntwice differentiable. It is also of interest to note that βˆ from the above minimization problem\nestimates the gradient ∂g0(x)/∂x.\nLike kernel regression, this estimator can be interpreted as a weighted average of the Yi\nobservations, though the weights are a bit more complicated. Let\nn\nn\nn\nS0 =\nKh(x -xi), S1 =\nKh(x -xi)(x -xi), S2 =\nKh(x -xi)(x -xi)(x -xi)\ni=1\ni=1\ni=1\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nX\nX\nP\nh\ni\nh\ni\nh\ni\nZ\nh\ni\nZ\n\"\n#\n\"\n#\nn\nn\nmˆ 0 =\nKh(x - xi)Yi, mˆ 1 =\nKh(x - xi)(x - xi)Yi.\ni=1\ni=1\nThen, by the usual partitioned inverse formula\n\"\n#-1 A\n!\ngˆ(x)\n= e 0\nS0 S1\nmˆ 0\n= (S0 - S0 S-1S1)-1( ˆm0 - S0 S-1 mˆ 1)\nS1 S2\nmˆ 1\nn\ni=1 aiYi\nS-1\n=\nPn\n, ai = Kh(x - xi)[1 - S1\n2 (x - xi)]\ni=1 ai\nIt is straightforward though a little involved to find asymptotic approximations to the\nMSE. For simplicity we do this for scalar x case. Note that for g0 = (g0(x1), . . . , g0(xn))0 ,\ngˆ(x) - g0(x) = e1\n0 (X0WX)-1X0W (Y - g0) + e1\n0 (X0WX)-1X0Wg0 - g0(x).\nThen for Σ = diag(σ2(x1), . . ., σ2(xn)),\nE (ˆg(x) - g0(x))2|x1, . . ., xn\n= e1\n0 (X0WX)-1X0W ΣWX(X0WX)-1 e1\nh\ni2\n+ e 0\n1(X0WX)-1X0Wg0 - g0(x)\nAn asymptotic approximation to MSE is obtained by taking the limit as n grows. Note that\nwe have\nn\n1 X\nn -1h-j Sj =\nKh(x - xi)[(x - xi)/h]j\nn i=1\nThen, by the change of variables u = (x - xi)/h,\nE n -1h-j Sj = E Kh(x - xi) ((x - xi)/h)j =\nK(u)ujf0(x - hu)du = μjf0(x) + o(1).\nfor μj =\nR K(u)ujdu and h -→ 0. Also,\nvar(n -1h-j Sj ) ≤ n -1E Kh(x - xi)2 ((x - xi)/h)2j ≤ n -1h-1\nK(u)2 u 2jf0(x - hu)du\n≤ Cn-1h-1 -→ 0\nfor nh -→ inf. Therefore, for h → 0 and nh →inf\nn -1h-j Sj = μjf0(x) + op(1).\nNow let H = diag(1, h). Then by μ0 = 1 and μ1 = 0 we have\nS\nh-1S\n-\n\nn\nH-1X0WXH-1 = n-\n-1\n-2\n= f\nh\nS1 h\nS\n0(x)\n+ o (\n0 μ\np 1).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n\"\n#\n\"\n\"\n# \"\nA\n!\nNext let νj =\nR K(u)2ujdu. Then by a similar argument we have\nn\n1 X\nh ·\nKh(x - xi)2[(x - xi)/h]jσ2(xi) = νjf0(x)σ2(x) + op(1).\nn i=1\nIt follows by ν1 = 0 that\nn -1hH-1X0W ΣWXH-1 = f0(x)σ2(x)\nν0\n+ op(1).\n0 ν2\nThen we have, for the variance term, by H-1e1 = e1,\ne1\n0 (X0WX)-1X0W ΣWX(X0WX)-1 e1\nA\n!-1\nA\n!-1\nH-1X0WXH-1\nhH-1X0W ΣWXH-1\nH-1X0WXH-1\n= n -1h-1 e 0 H-1\nH-1\ne1\nn\nn\nn\n⎡⎛\n⎞\n⎤\n= n -1h-1 ⎣⎝e1\n0 μ\n#-1\nν\nν\nν\nν\n0 μ\n#-1\ne1⎠ σ\nf\n(\n(\nx\nx\n)\n) + op(1) ⎦ .\nAssuming that μ1 = 0 as usual for a symmetric kernel we obtain\ne 0\n1(X0WX)-1X0W ΣWX(X0WX)-1 e1 = n -1h-1 ν0\nσ2(x) + op(1) .\nf(x)\nFor the bias consider an expansion\ng(xi) = g0(x) + g0\n0 (x)(xi - x) + g0\n00 (x)(xi - x)2 + g0\n000 ( xi)(xi - x)3 .\nLet ri = g0(xi) - g0(x) - [dg0(x)/dx](xi - x). Then by the form of X we have\ng = (g0(x1), . . ., g0(xn))0 = g0(x)We1 - g0\n0 (x)We2 + r\nIt follows by e1\n0 e2 = 0 that the bias term is\ne1\n0 (X0WX)-1X0Wg - g0(x) = e1\n0 (X0WX)-1X0WXe1g0(x) - g0(x)\n+e 0\n1(X0WX)-1X0WXe2g0\n0 (x) + e 0\n1(X0WX)-1X0Wr = e 0\n1(X0WX)-1X0Wr.\nRecall that\nn -1h-j Sj = μjf0(x) + op(1).\nTherefore\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n°\n°\n°\n(\n)\nX\nA\n!\nn -1h-2H-1X0W ((x - X1)2 , . . . , (x - Xn)2)0\nA\n!\nA\n!\n=\nn-1\nn-1\nh-2\nh-3\nS2\nS3\n2g 00\n0 (x) = f0(x)\nμ2\nμ3\n2g 00\n0 (x) + op(1).\nAlso, by g0\n000 ( xi) bounded\n°\n0°\n°n -1h-2H-1X0W (x - x1)3 g 000 ( x1), . . . , (x - xn)3 g 000 ( xn)\n°\n°\n≤ C max n -1h-2\nKh(x - xi)|x - xi|3 , n -1h-2S4\n-→ 0.\ni\nTherefore, we have\ne1\n0 (X0WX)-1X0Wr = h2 e1\n0 H-1 (H-1X0WXH-1)-1\n· h-2H-1X0Wr\nn\nn\nh2\nA\n!-1 A\n!\nh2\n=\ng0\n00 (x)e1\nμ2\n=\ng0\n00 (x)μ2.\nμ2\nμ3\nExercise: Apply analogous calculation to show kernel regression bias is\nμ2h2\n2g0\n00 (x) + g0\n0 (x) f\nf\n0 (\n(\nx\nx\n)\n)\nNotice bias is zero if function is linear.\nCombining the bias and variance expression, we have the following form for asymptotic\nMSE:\nσ2(x)\nh4\nν0\n+\ng0 (x)2 μ2.\nnh\nf0(x)\nIn constrast, the kernel MSE is\nσ2(x)\nh4 \"\nf0\n0 (x)\n#2\nν0\n+\ng0 (x) + 2g0(x)\nμ2.\nnh\nf0(x)\nf0(x)\nBias will be much bigger near boundary of the support where f0\n0 (x)/f0(x) is large.\nFor\nexample, if f0(x) is approximately xα for x > 0 near zero, then f0\n0 (x)/f0(x) grows like 1/x as\nx gets close to zero. Thus, locally linear has smaller boundary bias. Also, locally linear has\nno bias if g0(x) is linear but kernel obviously does.\nSimple method is to take expected value of MSE.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "notes_gmm.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/2d9ba545c27ee6942947ed6911e333ee_notes_gmm.pdf",
      "content": "GENERALIZED METHOD OF MOMENTS I\nWhitney K. Newey\nMIT\nOctober 2007\nTHE GMM ESTIMATOR: The idea is to choose estimates of the parameters\nby setting sample moments to be close to population counterparts. To describe the\nunderlying moment model and the GMM estimator, let β denote a p×1 parameter vector,\nwi a data observation with i = 1, ..., n, where n is the sample size. Let gi(β) = g(wi, β)\nbe a m × 1 vector of functions of the data and parameters. The GMM estimator is based\non a model where, for the true parameter value β0 the moment conditions\nE[gi(β0)] = 0\nare satisfied.\nThe estimator is formed by choosing β so that the sample average of gi(β) is close to\nits zero population value. Let\nX\ndef 1 n\ngˆ(β) =\ngi(β)\nn i=1\ndenote the sample average of gi(β). Let Aˆ denote an m×m positive semi-definite matrix.\nThe GMM estimator is given by\nβˆ = arg min gˆ(β)0Aˆgˆ(β).\nβ\nThat is βˆ is the parameter vector that minimizes the quadratic form ˆg(β)0Aˆgˆ(β).\nThe GMM estimator chooses βˆ so the sample average ˆg(β) is close to zero. To see\nq\ng0 ˆ\nˆ\nthis let kgk ˆ =\nAg, which is a well defined norm as long as A is positive definite.\nA\nThen since taking the square root is a strictly monotonic transformation, and since the\nminimand of a function does not change after it is transformed, we also have\nβˆ = arg min kgˆ(β) - 0kAˆ.\nβ\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThus, in a norm corresponding to Aˆ the estimator βˆ is being chosen so that the distance\nbetween ˆg(β) and 0 is as small as possible. As we discuss further below, when m = p, so\nthere are the same number of parameters as moment functions, βˆ will be invariant to Aˆ\nasymptotically. When m > p the choice of Aˆ will affect βˆ.\nThe acronym GMM is an abreviation for \"generalized method of moments,\" refering to\nGMM being a generalization of the classical method moments. The method of moments\nis based on knowing the form of up to p moments of a variable y as functions of the\nparameters, i.e. on\nE[yj] = hj(β0), (1 ≤ j ≤ p).\nThe method of moments estimator βˆ of β0 is obtained by replacing the population mo\nments by sample moments and solving for βˆ, i.e. by solving\nn\n1 X\n(yi)j = hj (βˆ), (1 ≤ j ≤ p).\nn i=1\nAlternatively, for\ngi(β) = (yi - h1(β), ..., yi\np - hp(β))0,\nmethod of moments solves ˆg(βˆ) = 0. This also means that βˆ minimizes ˆg(β)0Aˆgˆ(β) for\nany Aˆ, so that it is a GMM estimator. GMM is more general in allowing moment\nfunctions of different form than yj - hj(β) and in allowing for more moment functions\ni\nthan parameters.\nOne important setting where GMM applies is instrumental variables (IV) estimation.\nHere the model is\nyi = Xi\n0β0 + εi, E[Ziεi] = 0,\nwhere Zi is an m × 1 vector of instrumental variables and Xi a p × 1 vector of right-hand\nside variables. The condition E[Ziεi] = 0 is often called a population \"orthogonality\ncondition\" or \"moment condition. \"Orthogonality\" refers to the elements of Zi and εi\nbeing orthogonal in the expectation sense. The moment condition refers to the fact that\nthe product of Zi and yi - Xi\n0β has expectation zero at the true parameter. This moment\ncondition motivates a GMM estimator where the moment functions are the vector of\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nproducts of instrumental variables and residuals, as in\ngi(β) = Zi(yi - Xi\n0β).\nThe GMM estimator can then be obtained by minimizing ˆg(β)0Aˆgˆ(β).\nBecause the moment function is linear in parameters there is an explicit, closed\nform for the estimator. To describe it let Z = [Z1, ..., Zn]0, X = [X1, ..., Xn]0, and\ny = (y1, ..., yn)0. In this example the sample moments are given by\nn\nX\ngˆ(β) =\nZi(yi - Xi\n0β)/n = Z0(y - Xβ)/n.\ni=1\nThe first-order conditions for minimization of ˆg(β)0Aˆgˆ(β) can be written as\n0 = X0Z ˆ\nβ) = X0Z ˆ\n0Z ˆ\n0X ˆ\nAZ (y - X ˆ\nAZ y - X\nAZ\nβ.\nThese assuming that X0Z ˆAZ0X is nonsingular, this equation can be solved to obtain\nˆ\nAZ X)-1X0Z ˆ\nβ = (X0Z ˆ\nAZ0y.\nThis is sometimes referred to as a generalized IV estimator. It generalizes the usual two\nstage least squares estimator, where Aˆ = (Z0Z)-1 .\nAnother example is provided by the intertemporal CAPM. Let ci be consumption\nat time i, Ri is asset return between i and i + 1, α0 is time discount factor, u(c, γ0)\nutility function, Zi observations on variables available at time i. First-order conditions\nfor utility maximization imply that moment restrictions satisfied for\ngi(β) = Zi{Ri · α uc(ci+1, γ)/uc(ci, γ) - 1}.\n·\nHere GMM is nonlinear IV; residual is term in brackets. No autocorrelation because of\none-step ahead decisions (ci+1 and Ri known at time i + 1). Empirical Example: Hansen\nand Singleton (1982, Econometrica), u(c, γ) = cγ/γ (constant relative risk aversion),\nci monthly, seasonally adjusted nondurables (or plus services), Ri from stock returns.\nInstrumental variables are 1, 2, 4, 6 lags of ci+1 and Ri. Find γ not significantly different\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nthan one, marginal rejection from overidentification test. Stock and Wright (2001) find\nweak identification.\nAnother example is dynamic panel data. It is a simple model that is important\nstarting point for microeconomic (e.g. firm investment) and macroeconomic (e.g. cross-\ncountry growth) applications is\nE∗(yit|yi,t-1, yi,t-2, ..., yi0, αi) = β0yi,t-1 + αi,\nwhere αi is unobserved individual effect and E∗( ) denotes a population regression. Let\n·\nηit = yit - E∗(yit|yi,t-1, ..., yi0, αi). By orthogonality of residuals and regressors,\nE[yi,t-j ηit]\n=\n0, (1 ≤ j ≤ t, t = 1, ..., T ),\nE[αiηit]\n=\n0, (t = 1, ..., T ).\nLet ∆ denote the first difference, i.e. ∆yit = yit -yi,t-1. Note that ∆yit = β0∆yi,t-1+∆ηit.\nThen, by orthogonality of lagged y with current η we have\nE[yi,t-j(∆yit - β0∆yi,t-1)] = 0, (2 ≤ j ≤ t, t = 1, ..., T ).\nThese are instrumental variable type moment conditions. Levels of yit lagged at least\ntwo period can be used as instruments for the differences. Note that there are different\ninstruments for different residuals. There are also additional moment conditions that\ncome from orthogonality of αi and ηit. They are\nE[(yiT - β0yi,T -1)(∆yit - β0∆yi,t-1)] = 0, (t = 2, ..., T - 1).\nThese are nonlinear. Both sets of moment conditions can be combined. To form big\nmoment vector by \"stacking\". Let\n⎞\n⎛ yi0\n⎜\n⎜\n⎝\n⎟\n⎟\n⎠\nt\ngi (β)\n=\n\n. . .\nyi,t-2\n(∆yit - β∆yi,t-1), (t = 2, ..., T ),\n⎛\n⎞\nα(β)\n=\n\n⎜\n⎜\n⎝\n∆yi2 - β∆yi1\n. . .\n∆yi,T -1 - β∆yi,T -2\n⎟\n⎟\n⎠ (yiT - βyi,T -1).\ngi\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThese moment functions can be combined as\ngi(β) = (gi\n2(β)0, ..., g i\nT (β)0, gi\nα(β)0)0.\nHere there are T (T -1)/2+(T -2) moment restrictions. Ahn and Schmidt (1995, Journal\nof Econometrics) show that the addition of the nonlinear moment condition gi\nα(β) to the\nIV ones often gives substantial asymptotic efficiency improvements.\nArellano and Bond approach:\n⎞\n⎛\ngi(β) =\n⎜\n⎜\n⎜\n⎜\n⎝\n∆yi,1\n∆yi,2\n. . .\n∆yi,t-1\n⎟\n⎟\n⎟\n⎟\n⎠ (yit - βyi,t-1)\nAssumes that have representation\nX\ninf\nyit =\natj yi,t-j + bαi.\nj=1\nHahn, Hausman, Kuersteiner approach: Long differences\n⎞\n⎛\ngi(β) =\n⎜\n⎜\n⎜\n⎜\n⎝\nyi0\nyi2 - βyi1\n. . .\nyi,T -1 - βyi,T -2\n⎟\n⎟\n⎟\n⎟\n⎠ (yiT - yi1 - β(yi,T -1 - yi0)]\nHas better small sample properties by getting most of the information with fewer\nmoment conditions.\nIDENTIFICATION: Identification is essential for understanding any estimator.\nUnless parameters are identified, no consistent estimator will exist. Here, since GMM\nestimators are based on moment conditions, we focus on identification based on the\nmoment functions. The parameter value β0 will be identified if there is a unique solution\nto\ng (β) = 0, g (β) = E[gi(β)].\nIf there is more than one solution to these moment conditions then the parameter is not\nidentified from the moment conditions.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nOne important necessary order condition for identification is that m ≥ p. When\nm < p, i.e. there are fewer equations to solve than parameters. there will typically be\nmultiple solutions to the moment conditions, so that β0 is not identified from the moment\nconditions. In the instrumental variables case, this is the well known order condition that\nthere be more instrumental variables than right hand side variables.\nWhen the moments are linear in the parameters then there is a simple rank condition\nthat is necessary and sufficient for identification. Suppose that gi(β) is linear in β and let\nGi = ∂gi(β)/∂β (which does not depend on β by linearity in β). Note that by linearity\ngi(β) = gi(β0) + Gi(β - β0). The moment condition is\n0 = g(β) = G(β - β0), G = E[Gi]\nThe solution to this moment condtion occurs only at β0 if and only if\nrank(G) = p.\nIf rank(G) = p then the only solution to this equation is β - β0 = 0, i.e. β = β0. If\nrank(G) < p then there is c = 0 such that Gc = 0, so that for β = β0 + c = β0,\ng (β) = Gc = 0.\nFor IV G = -E[ZiXi\n0] so that rank(G) = p is one form of the usual rank condition\nfor identification in the linear IV seeting, that the expected cross-product matrix of\ninstrumental variables and right-hand side variables have rank equal to the number of\nright-hand side variables.\nIn the general nonlinear case it is difficult to specify conditions for uniqueness of\nthe solution to g(β) = 0. Global conditions for unique solutions to nonlinear equations\nare not well developed, although there has been some progress recently. Conditions\nfor local identification are more straightforward. In general let G = E[∂gi(β0)/∂β].\nThen, assuming g(β) is continuously differentiable in a neighborhood of β0 the condition\nrank(G) = p will be sufficient for local identification. That is, rank(G) = p implies that\nthere exists a neighborhood of β0 such that β0 is the unique solution to g(β) for all β in\nthat neighborhood.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExact identification refers the case where there are exactly as many moment conditions\nas parameters, i.e. m = p. For IV there would be exactly as many instruments as right-\nhand side variables. Here the GMM estimator will satisfy ˆg(βˆ) = 0 asymptotically.\nWhen there is the same number of equations as unknowns, one can generally solve the\nequations, so a solution to ˆg(β) = 0 will exist asymptotically. The proof of this statement\n(due to McFadden) makes use of the first-order conditions for GMM, which are\nh\ni\n0 = ∂gˆ(βˆ)/∂β\n0 Aˆgˆ(βˆ).\nThe regularity conditions will require that both ∂gˆ(βˆ)/∂β and Aˆ are nonsingular with\nprobability approaching one (w.p.a.1), so the first-order conditions imply ˆg(βˆ) = 0\nw.p.a.1. This will be true whatever the weight matrix, so that βˆ will be invariant to\nthe form of A.ˆ\nOveridentification refers to the case where there are more moment conditions than\nparameters, i.e. m > p. For IV this will mean more instruments than right-hand side\nvariables. Here a solution to ˆg(β) = 0 generally will not exist, because this would solve\nmore equations than parameters. Also, it can be shown that √ ngˆ(βˆ) has a nondegenerate\nasymptotically normal distribution, so that the probabability of ˆg(βˆ) = 0 goes to zero.\nWhen m > p all that can be done is set sample moments close to zero. Here the choice\nof Aˆ matters for the estimator, affecting its limiting distribution.\nTWO STEP OPTIMAL GMM ESTIMATOR: When m > p the GMM esti\nmator will depend on the choice of weighting matrix Aˆ. An important question is how to\nchoose Aˆ optimally, to minimize the asymptotic variance of the GMM estimator. It turns\nˆ\nˆ\np\nout that an optimal choice of A is any such that A -→ Ω-1 , where Ω is the asymptotic\nn\nˆ\nΩˆ -1\nvariance of √ ngˆ(β0) = P\ni=1 gi(β0)/√ n. Choosing A =\nto be the inverse of a con\nsistent estimator Ωˆ of Ω will minimize the asymptotic variance of the GMM estimator.\nThis leads to a two-step optimal GMM estimator, where the first step is construction of\nΩˆ and the second step is GMM with Aˆ = Ωˆ -1 .\nThe optimal Aˆ depends on the form of Ω. In general a central limit theorem will lead\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nto\nΩ = lim E[ngˆ(β0)ˆg(β0)0],\nn-→inf\nwhen the limit exists. Throughout these notes we will focus on the stationary case where\nE[gi(β0)gi+c(β0)0] does not depend on i. We begin by assuming that E[gi(β0)gi+c(β0)0] = 0\nfor all positive integers c. Then\nΩ = E[gi(β0)gi(β0)0].\nIn this case Ω can be estimated by replacing the expectation by a sample average and β0\nby an estimator β , leading to\nX\nΩˆ = 1 n\ngi(β )gi(β )0.\nn i=1\nThe β could be obtained by GMM estimator by using a choice of Aˆ that does not depend\non parameter estimates. For example, for IV β could be the 2SLS estimator where\nAˆ = (Z0Z)- 1 .\nIn the IV setting this Ωˆ has a heteroskedasticity consistent form. Note that for\nε i = yi - Xi\n0β ,\n1 n\nX\nΩˆ =\nZiZi\n0ε 2\ni .\nn i=1\nThe optimal two step GMM (or generalized IV) estimator is then\nβˆ = (X0ZΩˆ - 1Z0X)- 1X0ZΩˆ - 1Z0y.\nBecause the 2SLS corresponds to a non optimal weighting matrix this estimator will\ngenerally have smaller asymptotic variance than 2SLS (when m > p). However, when\nhomoskedasticity prevails, Ωˆ = ˆσε\n2Z0Z/n is a consistent estimator of Ω, and the 2SLS\nestimator will be optimal. The 2SLS estimator appears to have better small sample\nproperties also, as shown by a number of Monte Carlo studies, which may occur because\nusing a heteroskedasticity consistent Ωˆ adds noise to the estimator.\nWhen moment conditions are correlated across observations, an autocorrelation con\nsistent variance estimator estmator can be used, as in\nX\nX\nΩˆ = Λˆ0 +\nL\nwcL(Λˆc + Λˆ0\nc), Λˆc =\nn- c\ngi(β )gi+c(β )0/n.\nc=1\ni=1\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwhere L is the number of lags that are included and the weights wcL are used to ensure\nΩˆ is positive semi-definite. A common example is Bartlett weights wcL = 1 - c/(L + 1),\nas in Newey and West (1987). It is beyond the scope of these notes to suggest choices of\nL.\nˆ\nA consistent estimator V of the asymptotic variance of √n(βˆ - β0) is needed for\nasymptotic inference. For the optimal Aˆ = Ωˆ -1 a consistent estimator is given by\nVˆ = (Gˆ0Ωˆ -1Gˆ)-1 , Gˆ = ∂gˆ(βˆ)/∂β.\nOne could also update the Ωˆ by using the two step optimal GMM estimator in place of β\nin its computation. The value of this updating is not clear. One could also update the Aˆ\nin the GMM estimator and calculate a new GMM estimator based on the update. This\niteration on Ωˆ appears to not improve the properties of the GMM estimator very much.\nA related idea that is important is to simultaneously minimize over β in Ωˆ and in the\nmoment functions. This is called the continuously updated GMM estimator (CUE). For\nn\nexample, when there is no autocorrelation, for Ωˆ(β) = P\ni=1 gi(β)gi(β)0/n the CUE is\nβˆ = arg min gˆ(β)0Ωˆ(β)-1gˆ(β).\nβ\nThe asymptotic distribution of this estimator is the same as the two step optimal GMM\nestimator but it tends to have smaller bias in the IV setting, as will be discussed below.\nIt is generally harder to compute than the two-step optimal GMM.\nASYMPTOTIC THEORY FOR GMM: We mention precise results for the i.i.d.\ncase and give intuition for the general case. We begin with a consistency result:\nIf the data are i.i.d. and i) E[gi(β)] = 0 if and only if β = β0 (identification); ii)\nthe GMM minimization takes place over a compact set B containing β0; iii) gi(β) is\ncontinuous at each β with probability one and E[supβ∈B kgi(β)k] is finite; iv) Aˆ\np A\n→\npositive definite; then βˆ\np β0.\n→\nSee Newey and McFadden (1994) for the proof. The idea is that, for g(β) = E[gi(β)],\nby the identification hypothesis and the continuity conditions g(β)0Ag(β) will be bounded\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\naway from zero outside any neighborhood N of β0. Then by the law of large numbers\nAˆ\ng(ˆ 0 ˆg(βˆ) ≤ ˆ\n0 ˆ\n→ 0 from the definition of\nand iv), so will ˆg(β)0 ˆg(β). But, ˆ β) Aˆ\ng(β0) Ag(β0)\np\nβˆ and the law of large numbers, so βˆ must be inside N with probability approaching one.\nThe compact parameter set is not needed if gi(β) is linear, like for IV.\nNext we give an asymptotic normality result:\nIf the data are i.i.d., βˆ\np β0 and i) β0 is in the interior of the parameter set over\n→\nwhich minimization occurs; ii) gi(β) is continuously differentiable on a neighborhood N\nof β0 iii) E[supβ∈N k∂gi(β)/∂βk] is finite; iv) Aˆ\np A and G0AG is nonsingular, for\n→\nG = E[∂gi(β0)/∂β]; v) Ω = E[gi(β0)gi(β0)0] exists, then\nd\n√ n(βˆ - β0) -→ N(0, V ), V = (G0AG)-1G0AΩAG(G0AG)-1 .\nSee Newey and McFadden (1994) for the proof. Here we give a derivation of the\nasymptotic variance that is correct even if the data are not i.i.d..\nBy consistency of βˆ and β0 in the interior of the parameter set, with probability\napproaching (w.p.a.1) the first order condition\n0 = Gˆ0Aˆgˆ(βˆ),\nis satisfied, where Gˆ = ∂gˆ(βˆ)/∂β. Expand ˆg(βˆ) around β0 to obtain\n0 = Gˆ0Aˆgˆ(β0) + Gˆ0AˆG (βˆ - β0),\nwhere G = ∂gˆ(β )/∂β and β lies on the line joining βˆ and β0, and actually differs from\nrow to row of G . Under regularity conditions like those above Gˆ0AˆG will be nonsingular\nw.p.a.1. Then multiplying through by √ n and solving gives\n\n√ n(βˆ - β0) = - Gˆ0AˆG\n-1 Gˆ0Aˆ√ ngˆ(β0).\nd\nˆ\np\nBy an appropriate central limit theorem √ ngˆ(β0)\nN(0, Ω). Also we have A\n-→\n\n-→\np\np\n-1\np\nA, Gˆ -→ G, G -→ G, so by the continuous mapping theorem, Gˆ0AˆG\nGˆ0Aˆ -→\n(G0AG)-1 G0A. Then by the Slutzky lemma,\nd\n√ n(βˆ - β0) -→ - (G0AG)-1 G0AN(0, Ω) = N(0, V ).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe fact that A = Ω-1 minimizes the asymptotic varince follows from the Gauss\nMarkov Theorem. Consider a linear model.\nE[Y ] = Gδ, V ar(Y ) = Ω.\nThe asymptotic variance of the GMM estimator with A = Ω-1 is (G0Ω-1G)-1. This is\nalso the variance of generalized least squares (GLS) in this model. Consider an estmator\nδˆ = (G0AG)-1G0AY . It is linear and unbiased and has variance V . Then by the Gauss-\nMarkov Theorem,\nV - (G0Ω-1G)-1 is p.s.d..\nWe can also derive a condition for A to be efficient. The Gauss-Markov theorem says\nthat GLS is the the unique minimum variance estimator, so that A is efficient if and only\nif\n(G0AG)-1G0A = (G0Ω-1G)-1G0Ω-1 .\nTransposing and multiplying gives\nΩAG = GB,\nwhere B is a nonsingular matrix. This is the condition for A to be optimal.\nTESTING IN GMM: An important test statistic for GMM is the test of overiden\ntifying restrictions that is given by\nT = ngˆ(βˆ)0Ωˆ -1gˆ(βˆ).\nWhen the moment conditions E[gi(β0)] = 0 are satisfied then as the sample size grows\nwe will have\nd\nT -→ χ2(m - p).\nThus, a test with asymptotic level α consists of rejecting if T ≥ q where q is the the 1- α\nquantile of a χ2(m - p) distribution.\nUnder the conditions for asymptotic normality, Ω nonsinglar, and Ωˆ\np Ω, for an\n→\nd\nefficient GMM estimator it follows that T -→ χ2(m - p).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSee Newey and McFadden (1994) for a precise proof. Here we outline the proof. Let\nR denote a symmetric square root of the matrix Ω (i.e. RR = Ω) and let H = R-1G.\nUsing expansions similarly to the proof of asymptotic normality, we have\n\nˆ\n√ ngˆ(βˆ)\n=\n√ ngˆ(β0) + G√ n(βˆ - β0) = [I - G(Gˆ0Ω-1G )-1Gˆ0Ωˆ -1]√ ngˆ(β0)\n=\n[I - G(G0Ω-1G)-1G0Ω-1]√ ngˆ(β0) + op(1),\n= R[I - H(H0H)-1H0]R-1√ ngˆ(β0) + op(1),\n\nwhere G is given above in the discussion of asymptotic normality and op(1) is a ran\ndom variable that converges in probability to zero. Then by R-1ΩR-1 = I we have\nd\nR-1√ ngˆ(β0) -→ N(0, I) so that\nd\n√ ngˆ(βˆ) -→ R[I - H(H0H)-1H0]U, U ∼ N(0, I).\np\nBy consistency of Ωˆ and nonsingularity of Ω it follows that Ωˆ -1 -→ Ω-1. Then by the\np\ncontinuous mapping theorem RΩˆ -1R -→ I so that by I - H(H0H)-1H0 idempotent with\nrank m - p,\nT\nd\n0(I - H(H0H)-1H0)U.\n-→ U\nBy a standard result in multivariate normal theory, U0(I - H(H0H)-1H0)U is distributed\nas χ2(m - p), giving the result.\nThis statistic will not detect all misspecification. It is only a test of overidentifying\nrestrictions. Intuitively, p moments are \"used up\" in estimating β. In the exactly iden\ntified case where m = p note that ˆg(βˆ) = 0, so there is no way to test any moment\nconditions.\nAn example is in IV estimation, where gi(β) = Zi(y - Xi\n0β). There the test stastistic\nis\nT = ˆε Z ˆ\n0ε/n, ˆ\nβ.\n0 Ω-1Z ˆ\nε = y - X ˆ\nOne could also use an updated Ωˆ, based on ˆε. This takes particular form in the indepen\ndent observations case. When homoskedasticity holds and ˆ\nε ˆ\nΩ = ˆ0ε/T the test statistic\nis\nˆ0\nε/ˆε,\nT = n ε Z(Z0Z)-1Z0 ˆ ε0 ˆ\n·\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nthat is nR2 from regression of ˆε on Z. When βˆ is the generalized IV estimator and the\nheteroskedasticity consistent Ωˆ = P n\ni=1 ZiZi\n0εˆ2\ni /n is used in forming the test statistic it is\nT = e0rˆ(ˆr0rˆ)-1 rˆ0e,\nthe nR2 from regressing e = (1, ..., 1)0 on ˆr = [ˆε1Z1, ..., εˆnZn]0.\nIt is also possible to test subsets of moment restrictions. Partition gi(β) = (gi\n1(β)0, gi\n2(β)0)0\nand Ω conformably. One simple test is\nˆ\nΩˆ -1ˆ\n2(β)0Ωˆ -1\n2(β).\nT1 = min ngˆ(β)0\ng(β) - min ngˆ\n22 gˆ\nβ\nβ\nThe asymptotic distribution of this is χ2(m1) where m1 is the dimension of ˆg1(β).\nAnother version can be formed as\nng 10Ω -1g 1 ,\n1(ˆ\nΩˆ -1\n2(ˆ\nwhere g\n= ˆg\nβ) - Ωˆ 12\n22 gˆ β) and Ω is an estimator of the asymptotic variance of\n√ng . If m1 ≤ p then, except in any degenerate cases, a Hausman test based on the\ndifference of the optimal two-step GMM estimator using all the moment conditions and\nusing just ˆg2(β) will be asymptotically equivalent to this test, for any m1 parameters.\nSee Newey (1984, GMM Specification Testing, Journal of Econometrics.)\nWe can also consider tests of a null hypothesis of the form\nH0 : s(β0) = 0,\nwhere s(β) is a q × 1 vector of functions.\nLet s(β) by a q × 1 vector of functions with q < p and rank(∂s(β0)/∂β) = q, β be\na restricted GMM estimator β = arg mins(β)=0 gˆ(β)0Ωˆ -1gˆ(β), and G = ∂gˆ(β )/∂β. Under\nthe null hypotheses H0 : s(β) = 0 and the same conditions as for the overidentification\ntest,\nWald : W = ns(βˆ)0[∂s(βˆ)/∂β(Gˆ0Ωˆ -1Gˆ)-1∂s(βˆ)/∂β]-1 s(βˆ) d χ2(q),\n→\nˆ\np\nSSR : ngˆ(β )0Ωˆ -1gˆ(β ) - ngˆ(βˆ)0Ω-1gˆ(βˆ) - W → 0,\np\nLM : ngˆ(β )0Ωˆ -1G (G 0Ωˆ -1G )-1G 0Ωˆ -1gˆ(β ) - W → 0.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nHere SSR is like sum of squared residuals, LM is Lagrange Multiplier, and W is a Wald\nstatistic. The asymptotic approximation often more accurate for SSR and LM than W.\nEquivalence also holds under sequence of local alternatives.\nADDING MOMENT CONDITIONS: The optimality of the two step GMM\nestimator has interesting implications. One simple but useful implication is that adding\nmoment conditions will also decrease (or at least not decrease) the asymptotic variance\nof the optimal GMM estimator. This occurs because the optimal weighting matrix for\nfewer moment conditions is not optimal for all the moment conditions. To explain further,\nsuppose that gi(β) = (gi\n1(β)0, gi\n2(β)0)0. Then the optimal GMM estimator for just the first\nset of moment conditions gi\n1(β) is uses\nˆ\nwhere Ωˆ 1 is a consistent estimator of the asymptotic variance of\ni (β0)/√n. This\nA\n!\nAˆ =\n(Ωˆ 1)-1\n,\nP n\ni=1 g\nA\nis not generally optimal for the entire moment function vector gi(β).\nFor example, consider the linear regression model\nE[yi|Xi] = Xi\n0β0.\nThe least squares estimator is a GMM estimator with moment functions g1(β) = Xi(yi -\ni\nXi\n0β). The conditional moment restriction implies that E[εi|Xi] = 0 for εi = yi - Xi\n0β0.\nWe can add to these moment conditions by using nonlinear functions of Xi as additional\n\"instrumental variables.\" Let gi\n2(β) = a(Xi)(yi - Xi\n0β) for some (m - p) × 1 vector of\nfunctions of Xi. Then the optimal two-step estimator based on\nA\n!\nXi\ngi(β) =\na(Xi)\n(yi - X0β)\ni\nwill be more efficient than least squares when there is heteroskedasticity. This estimator\nhas the form of the generalized IV estimator described above where Zi = (Xi\n0, a(Xi)0)0.\nIt will provide no efficiency gain when homoskedasticity prevails. Also, the asymptotic\nvariance estimator Vˆ = (Gˆ0Ωˆ -1Gˆ)-1 tends to provide a poor approximation to the vari\nance of βˆ. See Cragg (1982, Econometrica). Interesting questions here are what and how\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nmany functions to include in a(X) and how to improve the variance estimator. Some of\nthese issues will be further discussed below.\nAnother example is provided by missing data. Consider again the linear regression\nmodel, but now just assume that E[Xiεi] = 0, i.e. Xi\n0β0 may not be the conditional mean.\nSuppose that some of the variables are sometimes missing and Wi denote the variables\nthat are always observed. Let ∆i denote a complete data indicator, equal to 1 if (yi, Xi)\nare observed and equal to 0 if only Wi is observed. Suppose that the data is missing\ncompletely at random, so that ∆i is independent of Wi. Then there are two types of\nmoment conditions available. One is E[∆iXiεi] = 0, leading to a moment function of the\nform\ngi\n1(β) = ∆iXi(yi - Xi\n0β).\nGMM for this moment condition is just least squares on the complete data. The other\ntype of moment condition is based on Cov(∆i, a(Wi)) = 0 for any vector of functions\na(W), leading to a moment function of the form\ngi\n2(η) = (∆i - η)a(Wi).\nOne can form a GMM estimator by combining these two moment conditions. This will\ngenerally be asymptotically more efficient than least squares on the complete data when\nYi is included in Wi. Also, it turns out to be an approximately efficient estimator in the\npresence of missing data. As in the previous example, the choice of a(W) is an interesting\nquestion.\nAlthough adding moment conditions often lowers the asymptotic variance it may not\nimprove the small sample properties of estimators. When endogeneity is present adding\nmoment conditions generally increases bias. Also, it can raise the small sample variance.\nBelow we discuss criteria that can be used to evaluate these tradeoffs.\nOne setting where adding moment conditions does not lower asymptotic efficiency\nis when those the same number of additional parameters are also added. That is, if\nthe second vector of moment functions takes the form gi\n2(β, γ) where γ has the same\ndimension as gi\n2(β, γ) then there will be no efficiency gain for the estimator of β. This\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nsituation is analogous to that in the linear simultaneous equations model where adding\nexactly identified equations does not improve efficiency of IV estimates. Here adding\nexactly identified moment functions does not improve efficiency of GMM.\nAnother thing GMM can be used for is derive the variance of two step estimators.\nConsider a two step estimator βˆ that is formed by solving\nn\n1 X 2\ngi (β, γˆ) = 0,\nn i=1\nP\nwhere ˆγ is some first step estimator. If ˆγ is a GMM estimator solving\nn\ni=1 gi\n1(γ)/n = 0\nthen ( β,ˆ γˆ) is a (joint) GMM estimator for the triangular moment conditions\nA\n!\ngi\n1(γ)\ngi(β, γ) =\n.\ngi (β, γ)\nThe asymptotic variance of √n(βˆ- β0) can be calculated by applying the general GMM\nformula to this triangular moment condition.\nWhen E[∂gi\n2(β0, γ0)/∂γ] = 0 the asymptotic variance of βˆ will not depend on esti\nmation of γ, i.e. will the same as for GMM based on gi(β) = gi\n2(β, γ0). A sufficient\ncondition for this is that\nE[gi\n2(β0, γ)] = 0\nfor all γ in some neighborhood of γ0. Differentiating this identity with respect to γ, and\nassuming that differentiation inside the expectation is allowed, gives E[∂gi\n2(β0, γ0)/∂γ] =\n0. The interpretation of this is that if consistency of the first step estimator does not\naffect consistency of the second step estimator, the second step asymptotic variance does\nnot need to account for the first step.\nCONDITIONAL MOMENT RESTRICTIONS: Often times the moment re\nstrictions on which GMM is based arise from conditional moment restrictions. Let\nρi(β) = ρ(wi, β) be a r × 1 residual vector. Suppose that there are some instruments zi\nsuch that the conditional moment restrictions\nE[ρi(β0)|zi] = 0\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nare satisfied. Let F (zi) be an m × r matrix of instrumental variables that are functions\nof zi. Let gi(β) = F (zi)ρi(β). Then by iterated expectations,\nE[gi(β0)] = E[F (zi)E[ρi(β0)|zβi]] = 0.\nThus gi(β) satisfies the GMM moment restrictions, so that one can form a GMM esti\nmator as described above. For moment functions of the form gi(β) = F (zi)ρi(β) we can\nthink of GMM as a nonlinear instrumental variables estimator.\nThe optimal choice of F (z) can be described as follows. Let D(z) = E[∂ρi(β0)/∂β|zi =\nz] and Σ(z) = E[ρi(β0)ρi(β0)0|zi = z]. The optimal choice of instrumental variables F (z)\nis\nF ∗(z) = D(z)0Σ(z)-1 .\nThis F ∗(z) is optimal in the sense that it minimizes the asymptotic variance of a GMM\nestimator with moment functions gi(β) = F (zi)ρi(β) and a weighting matrix A. To\nshow this optimality let Fi = F (zi), Fi\n∗ = F ∗(zi), and ρi = ρi(β0). Then by iterated\nexpectations, for a GMM estimator with moment conditions gi(β) = F (zi)ρi(β),\nG = E[Fi∂ρi(β0)/∂β] = E[FiD(zi)] = E[FiΣ(zi)Fi\n∗0] = E[Fiρiρi\n0Fi\n∗0].\nLet hi = G0AFiρi and h∗\ni = Fi\n∗ρi, so that\nG0AG = G0AE[Fiρihi\n∗0] = E[hihi\n∗0], G0AΩAG = E[hihi\n0].\nNote that for Fi = Fi\n∗ we have G = Ω = E[h∗\ni h∗\ni\n0]. Then the difference of the asymptotic\nvariance for gi(β) = Fiρi(β) and some A and the asymptotic variance for gi(β) = Fi\n∗ρi(β)\nis\n(G0AG)-1G0AΩAG(G0AG)-1 - (E[h∗h∗0])-1\ni\ni\n=\n(E[hih∗\ni\n0])-1 {E[hih0\ni] - E[hih∗\ni\n0] (E[h∗\ni h∗\ni\n0])-1 E[h∗\ni h0\ni]} (E[h∗\ni h0\ni])-1 .\nThe matrix in brackets is the second moment matrix of the population least squares\nprojection of hi on h∗\ni and is thus positive semidefinite, so the whole matrix is positive\nsemi-definite.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSome examples help explain the form of the optimal instruments. Consider the\nlinear regression model E[yi|Xi] = X0β0 and let ρi(β) = yi - Xi\n0β, εi = ρi(β0), and\ni\nσi\n2 = E[ε2\ni |Xi] = Σ(zi). Here the instruments zi = Xi. A GMM estimator with mo\nment conditions F (zi)ρi(β) = F (Xi)(yi - Xi\n0β) is the estimator described above that\nwill be asymptotically more efficient than least squares when F (Xi) includes Xi. Here\n∂ρi(β)/∂β = -Xi\n0, so that the optimal instruments are\nFi\n∗ = -\nσ\nX\ni .\ni\nHere the GMM estimator with the optimal instruments in the heteroskedasticity corrected\ngeneralized least squares.\nAnother example is a homoskedastic linear structural equation. Here again ρi(β) =\nyi - Xi\n0β but now zi is not Xi and E[εi\n2|zi] = σ2 is constant. Here D(zi) = -E[Xi|zi]\nis the reduced form for the right-hand side variables. The optimal instruments in this\nexample are\nFi\n∗ = -D(zi) .\nσ2\nHere the reduced form may be linear in zi or nonlinear.\nFor a given F (z) the GMM estimator with optimal A = Ω-1 corresponds to an\napproximation to the optimal estimator. For simplicity we describe this interpretation\nfor r = p = 1. Note that for gi = Fiρi it follows similarly to above that G = E[gihi\n∗0], so\nthat\nG0Ω-1 = E[hi\n∗gi\n0](E[gigi\n0])-1 .\nThat is G0Ω-1 are the coefficients of the population projection of h∗\ni on gi. Thus we can\ninterpret the first order conditions for GMM\nn\nX\n0 = Gˆ0Ωˆ -1gˆ(βˆ) = Gˆ0Ωˆ -1\nFiρi(β)/n,\ni=1\ncan be interpreted as an estimated mean square approximation to the first order condi\ntions for the optimal estmator\nn\nX\n0 =\nFi\n∗ρi(β)/n.\ni=1\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(This holds for GMM in other models too).\nOne implication of this interpretation is that if the number and variety of the elements\nof F increases in such a way that linear combinations of F can approximate any function\narbitrarily well then the asymptotic variance for GMM with optimal A will approach\nthe optimal asymptotic variance. To show this, recall that m is the dimension of Fi\nand let the notation Fi\nm indicate dependence on m. Suppose that for any a(z) with\nm\nE[Σ(zi)a(zi)2] finite there exists m × 1 vectors π\nsuch that as m -→ inf\nE[Σ(zi){a(zi) - πm0F m}2] -→ 0.\ni\nFor example, when zi is a scalar the nonnegative integer powers of a bounded monotonic\nm\nm\ntransformation of zi will have this property. Then it follows that for hi = ρiFi\n0Ω-1G\nm\nm\nm\nm\nm\nE[{hi\n∗ - hi }2] ≤ E[{hi\n∗ - ρiFi\n0π }2] = E[ρi {Fi\n∗ - Fi\n0π }2]\nm\n= E[Σ(zi){Fi\n∗ - Fi\nm0π }2] -→ 0.\nSince hm\ni converges in mean square to hi\n∗, E[hm\ni hm\ni\n0] -→ E[hi\n∗hi\n∗0], and hence\nm\nm\n(G0Ω-1G)-1 = (G0Ω-1E[gigi\n0]Ω-1G)-1 = (E[hi hi\n0])-1 -→ (E[hi\n∗hi\n∗0])-1 .\nBecause the asymptotic variance is minimzed at h∗\ni the asymptotic variance will ap\nproach the lower bound more rapidly as m grows than hm\ni approaches hi\n∗. In practice\nthis may mean that it is possible to obtain quite low asymptotic variance with relatively\nfew approximating functions in Fi\nm .\nAn important issue for practice is the choice of m.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "notes_nonsemi.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/655fcef1b4192f40d80edea3c1dd85ce_notes_nonsemi.pdf",
      "content": "Nonparametric and Semiparametric Estimation\nWhitney K. Newey\nFall 2007\nIntroduction\nFunction form misspecification error is important in elementary econometrics. Flexi\nble functional forms; e.g. translog\ny = β1 + β2 ln(x) + β3[ln(x)]2\nFine for simple nonlinearity, e.g. diminishing returns. Economic theory does not\nrestrict form. Nonparametric methods allow for complete flexibility. Good for graphs.\nGood for complete flexibility with a few dimensions.\nAn Empirical Example\nAn example illustrates. Deaton (1989); effect of rice prices on the distributions of\nincomes in Thailand.\np price of rice; q amount purchased; y amount sold.\nChange in benefits from dp is dB = (q - y)dp = p(q - y)d ln(p).\nElasticity form:\ndB /d ln(p) = (w - py/x),\nx\nw budget share of rice purchases; x total expenditure. Benefit/expenditure measure is\nthe negative of right-hand side.\nEmpirical Distribution Function\nSimple nonparametric estimation problem. The CDF of Z is FZ (z) = Pr(Z ≤ z). Let\nZ1, ..., Zn be i.i.d. data, 1(A) indicator of A, so FZ (z) = E[1(Zi ≤ z)].\nX\nFˆZ (z) = #{i|Zi ≤ z} = 1 n\n1(Zi ≤ z).\nn\nn i=1\nEmpirical CDF.\nProbability weight 1/n on each observation.\nConsistent and asymptotically normal.\nNonparametrically efficient.\nNo good for density estimation.\nKernel Density Estimator\nAdd a little continuous noise to smooth out empirical CDF.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nZn have empirical CDF.\n\nU a continuous random variable with pdf K(u), indep of Zn\nh a positive scalar.\nDefine\nZ = Z n + hU\nEmpirical CDF plus noise. Kernel density estimator is density of Z .\nR\nDerivation: Let FU (u) = u K(t)dt be CDF of U.\n-inf\nBy iterated expectations\nE[1( Z ≤ z)] = E[E[1(Z ≤ z)|Z n]],\nso by 1( Z ≤ z) = 1(U ≤ (z - Z n)/h),\nFZ (z)\n=\nPr(\n\nZ ≤ z) = E[1(Z ≤ z)]\n\n= E[[1(U ≤ z -\nh\nZn )|Zn]]\nn\n= E[FU ( z - Z n )] =\nX\nFU ( z - Zi )/n.\nh\nh\ni=1\nDifferentiating gives pdf\nn\nX\nfˆ h(z)\n=\ndF (z)/dz =\nKh(z - Zi)/n;\nZ\ni=1\nKh(u)\n=\nh-1K(u/h).\nThis is a kernel density estimator. The function K(u) is the kernel and the scalar h is\nthe bandwidth.\nn\nX\nfˆ h(z)\n=\ndF (z)/dz =\nKh(z - Zi)/n;\nZ\ni=1\nKh(u)\n=\nh-1K(u/h).\nBandwidth h controls the amount of smoothing. As h increases, density smoother, but\nmore \"noise\" from U, i.e. more bias. As h -→ 0 get rough density, spikes at data points,\nbut bias shrinks. Choosing h important in practice; see below. fˆ h(z) will be consistent\nif h -→ 0 and nh -→ inf .\nExamples:\nGaussian kernel: K(u) = (2π)-1/2e-u2/2 .\nEpanechnikov: K(u) = 1(|u| ≤ 1)(1 - u2)(3/4).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nChoice of K does not matter as much as choice of h. Epanechnikov kernel has slightly\nsmaller mean square error, and so optimal.\nBias and Variance of Kernel Estimators\n(Z1, ..., Zn) are i.i.d..\nBias: f0(z) is pdf of Zi. Expectation of kernel estimator; with\nZ\nZ\nE[fˆ h(z)] =\nKh(z - t)f0(t)dt = 1\nK( z - t )f0(t)dt\nh\nh\nZ\n=\nK(u)f0(z - hu)du,\nfor change of variables u = (z - t)/h. Taylor expand f0(z - hu) around h = 0,\nf0(z - hu)\n=\nf0(z) - f0\n0(z)hu + Γ(h, u, z)h2 ,\nΓ(h, u, z)\n=\nf0\n00(z + h (z, u)u)u 2/2,\nR\nR\n\nwhere |h(z, u)| ≤ |h|. For K(u)u2du < inf ,\nK(u)udu = 0, assuming f0\n00(z) contin\nuous and bounded,\nZ\nZ\nK(u)Γ(h, u, z)du -→ [ K(u)u 2du]f0\n00(z)/2.\nThen for o(h2) = a(h) with limh-→ 0a(h)/h2 = 0,\nZ\nZ\nh2\nK(u)Γ(h, u, z)du = h2[ K(u)u 2du]f0\n00(z)/2\n+o(h2)\nThen multiplying the expansion\nf0(z - hu) = f0(z) - f 0(z)hu + Γ(h, u, z)h2\nby K(u) and integrating gives\nZ\nE[fˆ h(z)] = f0(z) + h2f0\n00(z)\nK(u)u 2du/2 + o(h2).\nWe can summarize these calculations in the following result:\nProposition 1: If f0(z) is twice continuously differentiable with bounded second\nR\nR\nR\nderivative,\nK(u)du = 1,\nK(u)udu = 0,\nu2K(u)du < inf , then\nZ\nE[fˆ h(z)] - f0(z) = h2f 00(z)\nK(u)u 2du/2 + o(h2).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nVariance: From Proposition 1, E[Kh(z - Zi)] = E[fˆ h(z)] is bounded as h -→ 0.\nLet O(1/n) denote (an)inf\nn=1 such that nan is bounded. Then by fˆ h(z) a sample of average\nof Kh(z - Zi), for h -→ 0,\nV ar(fˆ h(z)) = {E[Kh(z - Zi)2] - {E[Kh(z - Zi)]}2}/n\nZ\n=\nK( z - t )2f0(t)dt/n + O(1/n)\nh2\nh\nZ\n=\nK(u)2f0(z - hu)du/(nh) + O(1/n).\nh\nR\nFor f0(z) continuous and bounded and K(u)2du < inf ,\nZ\nZ\nK(u)2f0(z - hu)du -→ f0(z)\nK(u)2du.\nBy h -→ 0, it follows that nhO(1/n) -→ 0, so that O(1/n) = o(1/nh). Plugging in\nabove variance formula we find,\nZ\nV ar(fˆ h(z)) = f0(z)\nK(u)2du/(nh) + o(1/(nh)).\nWe can summarize these calculations in the following result:\nR\nProposition 2: If f0(z) is continuous and bounded,\nK(u)2du < inf , h -→ 0, and\nnh →inf then\nZ\nV ar[fˆ h(z)] = f0(z)\nK(u)2du/(nh) + o(1/(nh)).\nConsistency and Convergence Rate of Kernel Estimators\nFor consistency implied by\nh\n0; bias goes to zero.\n-→\nnh -→ inf ; variance goes to zero.\nBandwidth shrinks to zero slower than 1/n.\nIntuition for the h -→ 0: Smoothing \"noise\" must go away asymptotically to remove\nall bias.\nIntuition for nh -→ inf : For Epanechnikov kernel; K((z - Zi)/h) > 0 if and only\nif |z - Zi| < h. If h shrinks as fast as or faster than 1/n, the number of observations\nwith |z - Zi| < h will not grow, so averaging over a finite number of observations, hence\nvariance does not go to zero.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExplicit form for (MSE) under h -→ 0, nh -→ inf .\nMSE(fˆ h(z)) = V ar(fˆ h(z)) + Bias2(fˆ h(z))\nZ\n= f0(z)\nK(u)2du/(nh)\nZ\n+h4{f00(z)\nK(u)u 2du/2}2\n+o(h4 + 1/(nh)).\nBy h -→ 0, MSE vanishes slower than 1/n. Thus, kernel estimator converges slower\nthan n-1/2 . Avoidance of bias by h -→ 0 means fraction of the observations used goes\nto zero.\nBandwidth Choice for Density Estimation:\nGraphical: Choose one that looks good, report several.\nMinimize asymptotic integrated MSE. Integrating over z,\nZ\nZ\nMSE(fˆ h(z))dz =\nK(u)2du/(nh)\nZ\nZ\n+\nf0\n00(z)2dz[ K(u)u 2du/2]2h4\n+o(h4 + 1/(nh)).\nMin over h has first-order conditions\nZ\n=\n- n-1h-2C1 + h3\nf0\n00(z)2dzC2,\nZ\nZ\nC1 =\nK(u)2du, C2 = [ K(u)u 2du]2 .\nSolving gives\nZ\nh = [C1/{nC2\nf 00(z)2dz}]1/5 .\nZ\nh = [C1/{nC2\nf0\n00(z)2dz}]1/5 .\nAsymptotically optimal bandwidth. Could be estimated by \"plugging-in\" estimator for\nf0\n00(z). This procedure depends on initial bandwidth, but final estimator not as sensitive\nto choice of bandwidth for f0\n00(z) as choice of bandwidth for f0(z).\nSilverman's rule of thumb: Optimal bandwidth when f0(z) is Gaussian and K(u) is\na standard normal pdf.\nh = 1.06σn1/5, σ = V ar(zi)1/2 .\nUse estimator of the standard deviation σ.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nEstimate directly the integrated MSE:\nZ\nZ\nMSE(fˆ h(z))dz =\nE[{fˆ h(z) - f0(z)}2]dz\nZ\n= E[ {fˆ h(z) - f0(z)}2]\nZ\nZ\n= E[ fˆ h(z)2] - 2E[ fˆ h(z)f0(z)]\n+f0(z)2 .\nUnbiased estimator of E[\nR fˆ h(z)2] is\nZ\nZ\nX\nfˆ h(z)2dz =\nKh(Zi - t)Kh(t - Zj)dt/n2\ni,j\nTo find unbiased estimator of second, note that\nZ\nZ Z\nE[ fˆ h(z)f0(z)dz] =\nKh(z - t)f0(z)f0(t)dzdt.\nBy observations independent, we can average over pairs to estimate this term. Last term\nin MSE does not depend on h, so we can drop. Combining estimates of first two terms\ngives criterion.\nZ\nX\nCVˆ(h) = fˆ h(z)2dz - n(n - 1)\nKh(Zi - Zj).\ni=j\nZ\nX\nCVˆ(h) = fˆ h(z)2dz - n(n - 1)\nKh(Zi - Zj).\ni=j\nChoosing h by minimizing CVˆ(h) is called cross-validation. Motivation for terminology\nis second term is\nn\nX\n-2\nfˆ -i,h(Zi)/n,\ni=1\nX\nfˆ -i,h(z)\n=\n\nKh(z - Zj )/(n - 1).\nj=i\nHere fˆ -i,h(z) is estimator that uses all observations but the ith, so that fˆ -i,h(Zi) is\n\"cross-validated.\"\nMultivariate Density Estimation:\nMultivariate density estimation can be important as in example. Let z be r ×1, K(u)\ndenote a pdf for a r × 1 random vector, e.g. K(u) = Πr k(uj ) for univariate pdf k(u).\nj=1\nLet Σˆ the sample covariance matrix of Zi. For a bandwidth h let\nKh(u) = h-r det(Σˆ)-1/2K(Σˆ -1/2u/h),\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nA-1/2 is inverse square root of positive definite A. Often K(u) = ρ(u0u) for some ρ, so\nKh(u) = h-r det(Σˆ)-1/2ρ(u0Σˆ -1u/h).\nMultivariate kernel estimator, with scale normalization\nn\nX\nfˆ h(z) =\nKh(z - Zi)/n.\ni=1\nGaussian kernel: K(u) = (2π)-r/2 exp(-u0u/2).\nEpanechnikov kernel: K(u) = Cr(1 - u0u)1(u0u ≤ 1).\nThe Curse of Dimensionality for Kernel Estimation\nDifficult to nonparametrically estimate pdf's of high dimensional Zi. Need many\nobservations within a small distance of a point. As dimension rises with distance fixed,\nthe proportion of observations that are close shrinks very rapidly. Mathematically, with\none dimension [0, 1] can be covered with 1/h balls of radius h, while it requires h\nr balls\nto cover [0, 1]r . Thus, if data equally likely to fall in one ball, tend to be many fewer\ndata points in any one ball for high dimensional data.\nSilverman (1986, Density Estimation) famous table. Multivariate normal density at\nzero, the sample size required for MSE to be 10 percent of the density\nr\nn 768 2790 10,700 43,700 187,000 842,000\nCurse of dimensionality shows up in convergence rate. Expand again,\nf0(z - hu)\n=\nf0(z) - h[∂f0(z)/∂z]0u\n+h2 u0[∂2f0(z + h (z, u)u)/∂z∂z0]u/2,\nso bias asymptotically C3h2 no matter how big r. For the variance, setting u = (z - t)/h\nZ\nZ\nKh(z - t)2f0(t)d = h-2r\nK((z - t)/h)2f(t)dt\nZ\n= h-r\nK(u)2f0(z - hu)du.\nIntegrating, with Σˆ = I,\nZ\nE[{fˆ h(z) - f0(z)}2]dz ≈ C1/(nhr) + C2h4 .\nFirst-order conditions for the optimal h : 0 = -C1rn-1h-r-1 + 4C2h3 . Solving gives\nh = C0n-1/(r+4). Plugging back in gives\nZ\nE[{fˆ h(z) - f0(z)}2]dz ≈ C0n-4/(r+4).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe convergence rate declines as r increases.\nNonparametric Regression\nOften in econometrics the object of estimation is a regression function. A classical\nformulation is Y = X0β + ε with E[ε|X] = 0. A more direct way to write this model is\nE[Y |X] = X0β.\nA nonparametric version of this model, that allows for unknown functional form in the\nregression, is\nE[Y |X] = g0(X),\nwhere g0(x) is an unknown function.\nKernel Regression\nTo estimate g0(x) nonparametrically, one can start with kernel density estimate and\nplug it into the formula\nZ\ng0(x)\n=\nyf(y|x)dy\nZ\nZ\n=\nyf(y, x)dy/\nf(y, x)dy\nR\nAssume that X is a scalar. Let k(u1, u2) be a bivariate kernel, with t k(t, u2)dt = 0. Data\nR\n·\nare (Y1, X1), ..., (Yn, Xn). Let K(u2) = k(t, u2)dt. By change of variables t = (y - Yi)/h,\nZ\nn Z\nyfˆ h(y, x)dy = n-1h-2 X\nyk(y - Yi , x - Xi )dy\nh\nh\ni=1\nn\n= n-1h-1 X Z\n(Yi + ht)k(t, x - Xi )dt\nh\ni=1\nn\nZ\n= n-1h-1 X\nYi\nk(t, x - Xi )dt\nh\ni=1\nn\n= n-1h-1 X\nYiK( x - Xi ),\nh\ni=1\nZ\nn Z\nfˆ h(y, x)dy = n-1h-2 X\nk(y - Yi , x - Xi )dy\nh\nh\ni=1\nn\n= n-1h-1 X\nK( x - Xi ).\nh\ni=1\nPlugging in fˆ h(y, x) in formula for g0(x),\n\nR\nP n\nyfˆ h(y, x)dy\ni=1 YiK\nx-\nh\nXi\ngbh(x) = R\n= P\n.\nfˆ h(y, x)dy\ni\nn\n=1 K\nx-\nh\nXi\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAlso, kernel regression estimator ˆg(x) just defined by second equality.\nThis regression estimator is a weighted average, with\n\nn\nx-Xi\nX\nK\ngˆh(x) =\nwi\nh(x)Yi, wi\nh(x) = P\n3h\n\ni=1\nn\nj=1 K\nx-\nh\nXj\nP\nBy construction,\nn\ni=1 wi\nh(x) = 1, while wi\nh(x) is nonnegative by K(u) being nonneg\native. For symmetric K(u) with a unique mode at u = 0, more weight will be given\nto observations with Xi that is closer to x. Bandwidth h controls how fast the weights\ndecline. As h declines, more weight given closer observations. Reduces bias but increases\nvariance.\nFor multivariate X, formula is the same, with K(u) replaced by multivariate kernel,\nsuch as\nK(u) = det(Σˆ)-1/2k(Σˆ -1/2 u)\nfor some kernel k(u). Consistency and convergence rate results are similar. Details\nare not reported here because the calculations are complicated by the ratio form of the\nestimator. Bandwidth choice below.\nSeries Regression\nAnother approach to nonparametric regression flexible functional forms with complete\nflexibility by letting the number of terms grow with sample size. Think of approximating\nPK\ng0(x) by linear combination\nj=1 pjK(x)βj of approximating functions pjK(x), e.g. poly\nnomials or splines. Estimator of g0(x) is predicted value from regressing Yi on pK(Xi)\nfor pK(x) = (p1K(x), ..., pKK(x))0. Consistent as K grows with n.\nLet Y = (Y1, ..., Yn)0 and P = [P K(X1), ..., P K(Xn)]0. Then\ngˆ(x) = p K(x)0β,ˆ βˆ = (P 0P )-P 0Y,\nA- denotes generalized inverse, AA-A = A. Different than kernal; global fit rather than\nlocal average.\nExamples:\nPower series: x scalar\npjK(x) = xj-1 , (j = 1, 2, ...)\npK(x)0β is a polynomial. Such approximations good for global approximation of smooth\nfunctions. Not when most variation in narrow range or when jump or kink. Using\northogonal polynomials with respect to density can mitigate multicollinearity.\nRegression splines: x is scalar,\npjK(x)\n=\nxj-1 , (j = 1, 2, 3, 4),\n\npjK(x)\n=\n1(x > cj-4,K)(x - c\nj-4,K) , (j = 5, ..., K).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe c1, ..., cK-4 are \"knots,\" pK (x)0β is cubic in between cjK , twice continuously differ\nentiable everywhere. Picks up local variation but still global fit. Need to place knots.\nB-splines can mitigate multi-collinearity.\nExtend both to multivariate case by including products of individual components.\nConvergence Rate for Series Regression\nDepends on approximation rate, i.e. bias.\n\nAssumption A: There exists γ > 0, βK , and C such that\n{E[{g0(Xi) - p K (Xi)0β K }2]}1/2 ≤ CK-γ .\nComes from approximation theory. For power series, Xi in a compact set, g0(x) is\ncontinuously differentiable of order s, then γ = s/r. For splines, γ = min{4, s}/r. For\nmultivariate, approximation depends on the order of the included terms (e.g. on power)\nwhich grows more slowly with K when x is higher dimensional.\nProposition 3: If Assumption A is satisfied and V ar(Yi|Xi) ≤ ∆ then\nE[{gˆK (Xi) - g0(Xi)}2] ≤ ∆K/n + C2K-2γ .\nProof: Let\nQ = P (P 0P )-P 0, gˆ = (ˆg(X1), ..., gˆ(Xn))0 = QY,\ng0 =\n(g0(X1), ..., g0(Xn))0 , ε = Y - g0, g = Pβ K .\nQ idempotent, so I-Q idempotent, hence has eigenvalues that are zero or one. Therefore,\nby Assumption A,\nE [(g0 - g ) (I - Q)(g0 - g )]\nh\ni\n≤ E (\nh\ng0 - g )0 (g0 - g )\ni\n≤ nE {g0(Xi) - p K (Xi)0β K }2\n≤ CnK-2γ .\nAlso, for X = (X1, ..., Xn), by independence and iterated expectations, for i = j,\nE[εiεj |X]\n=\nE[εiεj|Xi, Xj]\n= E[εiE[εj|Xi, Xj, εi]|Xi, Xj ]\n= E[εiE[εj|Xj ]|Xi, Xj ] = 0.\nThen for Λii = V ar(Yi|Xi) and Λ = diag(Λ11, ..., Λnn) we have E[εε0|X] = Λ. It follows\nthat for tr(A) the trace of a square matrix A, by rank(Q) ≤ K,\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nE [ε0Qε|X]\n=\ntr (QE [εε0|X]) = tr(QΛ)\n= tr(QΛQ) ≤ ∆ tr(Q) ≤ ∆K.\nThen by iterated expectations, E[ε0Qε] ≤ CK. Also,\nn\nX\n{gˆ(Xi) - g0(Xi)}2\ni=1\n=\n(ˆg - g0)0(ˆg - g0)\n=\n(Qε - (I - Q)g0)0(Qε - (I - Q)g0)\n= ε0Qε + g0 (I - Q)g0\n= ε0Qε + (g0 - g )0(I - Q)(g0 - g ).\nThen by i.i.d. observations,\nE[{gˆ(Xi) - g0(Xi)}2]\n= E[(ˆgi - g0i)2]\n= E[(ˆg - g0)0(ˆg - g0)]/n\nK\n≤ ∆ n + C2K-2γ .\nChoosing Bandwidth or Number of Terms\nData based choice operationalizes complete flexibility. Number of terms or bandwidth\nadjusts. Cross-validation is common. Let ˆg-i,h(Xi) and ˆg-i,K (Xi) be predicted values for\nith observation using all the others, for kernel and series respectively.\n\nP\nj=i Yj K\nXi-\nh\nXj\ngˆ-i,h(Xi)\n=\nP\nj=i K\n3 Xi-\nh\nXj ,\ngˆ-i,K (Xi)\n=\nYi - 1 - P K (\nY\nX\ni\ni\n-\n) (\ngˆ\nP\nK\n(\nP\nX\n)\ni\n-\n)\nP K (Xi),\nSecond equality by recursive residuals. Criteria are\nn\nX\nCVˆ(h)\n=\n\nv(Xi)[Yi - gˆ-i,h(Xi)]2 ,\ni=1\nn\nX\nCVˆ(K)\n=\n\n[Yi - gˆ-i,K (Xi)]2 ,\ni=1\nv(x) is a weight function, equal to zero near support boundaryi.\nChoose\nh or K to\nminmize.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(weighted) sample MSE is\nn\nX\nMSE(h) =\nv(Xi){gˆh(Xi) - g0(Xi)}2/n\ni=1\nCross-validation criteria optimal,\nminh MSE(h)\np\nMSE(hˆ)\n-→ 1.\nSeries too. hˆ converges slowly.\nAnother Empirical Example:\nHausman and Newey (1995, Nonparametric Estimation of Exact Consumers Surplus,\nEconometrica), kernel and series estimates, Y is log of gasoline purchased, function of\nprice, income, and time and location dummies. Six cross-sections of individuals from\nEnergy Department, total of 18,109 observations. Cross-validation criteria are\nKernel\nSpline\nPower\nh\nCV\nKnots\nCV\nOrder\nCV\n1.6\n1.9\n2.0\n2.1\nLocally Linear Regression:\nThere is another local method, locally linear regression, that is thought to be superior\nto kernel regression.\nIt is based on a locally fitting a line rather than a constant.\nUnlike kernel regression, locally linear estimation would have no bias if the true model\nwere linear. In general, locally linear estimation removes a bias term from the kernel\nestimator, that makes it have better behavior near the boundary of the x's and smaller\nMSE everywhere.\nTo describe this estimator, let Kh(u) = h-rK(u/h) as before. Consider the estimator\ngˆ(x) given by the solution to\nn\nX\nmin\n(Yi - g - (x - Xi)0β)2Kh(x - Xi).\ng,β i=1\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThat is ˆg(x) is the constant term in a weighted least squares regression of Yi on (1, x-Xi),\nwith weights Kh(x - Xi). For\n⎞\n⎛\n⎞\n⎛ Y1\n(x - X1)0\n.\n.\n⎜\n⎜\n⎝\n⎟\n⎟\n⎠\n⎜\n⎜\n⎝\n⎟\n⎟\n⎠\nX =\n.\nY =\n. .\n.\n.\n.\n.\n,\nYn\n(x - Xn)0\nW =\ndiag\n\n(Kh(x - X1) , ..., Kh(x - Xn))\nand e1 a (r + 1) × 1 vector with 1 in first position and zeros elsewhere, we have\ngˆ(x) = e1\n0 (X 0WX )-1X 0WY.\nThis estimator depends on x both through the weights Kh(x - Xi) and through the\nregressors x - Xi.\nThis estimator is a locally linear fit of the data. It runs a regression with weights that\nare smaller for observations that are farther from x. In contrast, the kernel regression\nestimator solves this same minimization problem but with β constrained to be zero, i.e.,\nkernel regression minimizes\nn\nX\n(Yi - g)2Kh(x - Xi)\ni=1\nRemoving the constraint β = 0 leads to lower bias without increasing variance when g0(x)\nis twice differentiable. It is also of interest to note that βˆ from the above minimization\nproblem estimates the gradient ∂g0(x)/∂x.\nLike kernel regression, this estimator can be interpreted as a weighted average of the\nYi observations, though the weights are a bit more complicated. Let\nX\nn\nn\nX\nX\nn\nS0 =\nKh(x - Xi), S1 =\nKh(x - Xi)(x - Xi), S2 =\nKh(x - Xi)(x - Xi)(x - Xi)0\ni=1\ni=1\ni=1\nn\nn\nX\nX\nmˆ 0 =\nKh(x - Xi)Yi, mˆ 1 =\nKh(x - Xi)(x - Xi)Yi.\ni=1\ni=1\nThen, by the usual partitioned inverse formula\ngˆ(x)\n=\ne1\n\"\nS0 S1\n0 #-1 A\nmˆ 0\n!\n= (S0 - S1\n0S2\n-1S1)-1( ˆm0 - S1\n0S2\n-1 mˆ 1)\nS1 S2\nmˆ 1\nP\n=\nP\nn\ni=1 aiYi , ai = Kh(x - Xi)[1 - S0S-1(x - Xi)]\nn\ni=1 ai\nIt is straightforward though a little involved to find asymptotic approximations to the\nMSE. For simplicity we do this for scalar x case. Note that for g0 = (g0(X1), ..., g0(Xn))0\ngˆ(x) - g0(x) = e1\n0 (X 0WX )-1X 0W(Y - g0) + e0(X 0WX )-1X 0Wg0 - g0(x).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThen for Σ = diag(σ2(X1), ..., σ2(Xn)),\nh\ni\nE {gˆ(x) - g0(x)} 2 | X1, ..., Xn\n= e0(\nn\nX 0WX )-1X 0W ΣWX (X 0WX\no\n)\n-1 e\n+ e0(X 0WX )-1X 0Wg0 - g0(x)\nAn asymptotic approximation to MSE is obtained by taking the limit as n grows. Note\nthat we have\nn\n1 X\nn-1h-jSj =\nKh(x - Xi)[(x - Xi)/h]j\nn i=1\nThen, by the change of variables u = (x - Xi)/h,\nh\ni\nZ\nE n-1h-j Sj = E[Kh(x - Xi) {(x - Xi)/h}j ] = K(u)ujf0(x - hu)du = μjf0(x)+o(1).\nR\nfor μj = K(u)uj du and h -→ 0. Also,\nvar\nn-1h-j Sj\n\nn-1E\nh\nKh(x - Xi)2[(x - Xi)/h\ni2j ] ≤ n-1h-1\nZ\nK(u)2 u 2jf0(x - hu)du\n≤\n≤ Cn-1h-1 -→ 0\nfor nh -→ inf . Therefore, for h -→ 0 and nh -→ inf\nn-1h-j Sj = μj f0(x) + op(1).\nNow let H = diag(1, h). Then by μ0 = 1 and μ1 = 0 we have\n\"\n#\n\"\n#\nXH-1\nh-1S1\n\n(1).\nn-1H-1X W\n= n-1\nh-\nS\nS1 h-2S2\n= f0(x)\n0 μ2\n+ op\nR\nNext let νj = K(u)2uj du. then by a similar argument we have\nX\nh 1 n\nKh(x - Xi)2 [(x - Xi)/h]j σ2(Xi) = νj f0(x)σ2(x) + op(1).\nn i=1\nIt follows by ν1 = 0 that\n\"\n#\nn-1hH-1X 0W ΣW\n= f0(x)σ2(x)\nν\nν\n(1).\nXH-1\n+ op\nThen we have, for the variance term, by H-1e1 = e1,\ne1\n0 (X 0WX )-1X 0W ΣWX (X 0WX )-1 e1\nA\n!-1\nA\n!-1\nH-1X 0W\nhH-1\nXH-1\nX W\nXH-1\nX0W ΣW\nH-1 0\nXH-1\n= n-1h-1 e0\n1H-1\nH-1 e1\nn\nn\nn\n⎡⎛\n⎞\n⎤\n= n-1h-1 ⎣⎝e0\n\"\n\n#-1 \"\nν0 ν1\n# \"\n\n#-1\ne1⎠ σ2(x) + op(1)⎦ .\n0 μ2\nν1 ν2\n0 μ2\nf(x)\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nIt then follows that\nA\n!\ne0\n1(X 0WX )-1X 0W ΣWX (X 0WX )-1 e1 = n-1h-1 ν0\nσ2(x) + op(1).\nf(x)\nFor the bias consider an expansion\ng(Xi) = g0(x) + g0\n0 (x)(Xi - x) + 1 g0\n00(x)(Xi - x)2 + 1 g0\n000(X i)(Xi - x)3 .\nLet ri = g0(Xi) - g0(x) - [dg0(x)/dx] (Xi - x). Then by the form of X we have\ng = (g0(X1), ..., g0(Xn))0 = g0(x)We1 - g0 (x)We2 + r\nIt follows by e0\n1e2 = 0 that the bias term is\ne0 ( 0W\nX0Wg - g0(x) = e0 ( 0W\nX0\nXe1g0(x) - g0(x)\nX\nX)-1\nX\nX)-1 W\n+e1\n0 X0\nX0\nXe2g0\n0 (x) + e1\n0 X0\nX0Wr = e1\n0 X0\nX0Wr.\n( WX )-1 W\n( WX )-1\n( WX )-1\nRecall that\nn-1h-j Sj = μj f0(x) + op(1).\nTherefore, by μ3 = 0,\nn-1h-2H-1X 0W ((x - X1)2 , ..., (x - Xn)2)0 1 g0\n00(x)\nA\n!\nA\n!\nn-1h-2S2\nμ2\n=\nn-1h-3S3\n2g0\n00(x) = f0(x)\n2g0\n00(x) + op(1).\nAssuming that g0\n000(X i) is bounded, bounded\n°\n°\n\n°\n°\n°\n)3\n\n0°\n°n-1h-2H-1X 0W (x - X1)3 g000(X 1), ..., (x - Xn\ng000 Xn\n°\nX\n≤ C max\n(\nn-1h-2\ni\nKh(x - Xi) |x - Xi| , n-1h-2S4\n)\n-→ 0.\nTherefore, we have\ne0\n1( X0W X)-1 X0Wr = h2 e0\n1H-1\nA H-1 X0W XH-1 !-1 h-2H-1 X0Wr\nn\nn\n= h2\n⎡\n⎣g00\n0 (x)e0\nA\n\n0 μ2\n!-1 A\nμ2\nμ3\n!\n+ op(1)\n⎤\n⎦\n= h2\n2 [g00\n0 (x)μ2 + op(1)].\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExercise: Apply analogous calculation to show kernel regression bias is\nA\n!\nμ2h2\n1 g0\n00(x) + g0\n0 (x) f\nf\n(\n(\nx\nx\n)\n)\nNotice bias is zero if function is linear.\nCombining the bias and variance expression, we have the following form for asymptotic\nMSE:\nσ2(x)\nh4\nnhν0 f0(x) + 4 g0\n00(x)2μ2\n2 .\nIn contrast, the kernel MSE is\nσ2(x)\nh4 \"\nf0\n0(x)\n#2\nnhν0 f0(x) + 4 g0\n00(x) + 2g0\n0 (x)f0(x)\nμ2\n2 .\nBias will be much bigger near boundary of the support where f0\n0(x)/f0(x) is large. For\nexample, if f0(x) is approximately xα for x > 0 near zero, then f0\n0(x)/f0(x) grows like\n1/x as x gets close to zero. Thus, locally linear has smaller boundary bias. Also, locally\nlinear has no bias if g0(x) is linear but kernel obviously does.\nSimple bandwidth choice method is to take expected value of MSE.\nOne could use a plug in method to minimize integrated asymptotic MSE, integrated\nover ω(x)f0(x) for some weight.\nReducing the Curse of Dimensionality\nIdea: Restrict form of regression so that it only depends on low dimensional com\nponents. Additive model has regression additive in lower dimension components. In\ndex model has regression depending only on a linear combination. Additive model,\nX = (x1, ..., xr)0,\nr\nX\nE[Y |X] =\nj=1\ngj0(Xj ).\nOne dimensional rate. Series estimator is simplest. Restricts approximating functions to\ndepend on only on component. Scalar u and pcL(u), c = 1, ..., L approximating functions,\npL(u) = (p1L(u), ..., pLL(u))0 , K = Lr + 1 let pK (x) = (1, pL(x1)0, ..., pL(xr)0)0. Regress\nYi on pK (Xi). For βˆ0 equal to constant and βˆj , (j = 1, ..., r) the coefficient vector for\npL(xj )/\nr\nX\ngˆ(X) = βˆ0 +\np L(xj)0βˆj .\nj=1\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProposition 4: If Assumption A is satisfied with g0(X) equal to each component\ngj0(Xj) and pK(X) replaced by (1, pL(Xj)0)0, where the constants C, γ do not depend on\nj, and V ar(Y |X) ≤ ∆ then\nE[{gˆK(Xi) - g0(Xi)}2]\n≤ ∆/n + r[∆L/n + C2(L + 1)-2γ].\nProof: Let pL(u) = (1, pL(u)0)0. By Assumption A, is C and γ so for each j and L is\na (L + 1) × 1 vector β jL = (β jL , β 2\njL0)0,\nE[{gj0(Xj) - p L(Xj)0β jL}2] ≤ C2(L + 1)-2γ .\nLet β = ( P\nj\nr\n=1 β 1\njL , β 2\n1L0, ..., β 2\nrL0)0, so that\nE[{g(X) - p K(X)0β }2]\nr\nX\n=\nE[{gj0(Xj) - p L(Xj)0β jL}2]\nj=1\n≤ rC2(L + 1)-2γ .\nThen as in the proof of Propostion 3, we have\nE[{gˆK(X) - g0(X)}2]\n≤ ∆K/n + rC2(L + 1)-2γ\n= ∆/n + r[∆L/n + C2(L + 1)-2γ]. Q.E.D.\nConvergence rate does not depend on r, although does affect. Here r could even grow\nwith sample size at some power of n. Additivity condition satisfied in Hausman and\nNewey (1995).\nSemiparametric Models\nData: Z1, Z2, ... i.i.d.\nModel: F a set of pdfs.\nCorrect specification: pdf f0 of Zi in F.\nSemiparametric model: F has parametric θ and nonparametric components.\nEx: Linear model E[Y |X] = X0β0; parametric component is β, everything else non\nparametric.\nEx: Probit, Y ∈ {0, 1}, Pr(Y = 1|X) = Φ(X0β0) is parametric component, nonpara\nmetric component is distribution of X.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nBinary Choice with Unknown Disturbance Distribution: Z = (Y, X),\nv(x, β) a known function,\nY = 1(Y ∗ > 0), Y ∗ = v(X, β0) - ε, ε independent of X,\nThis model implies\nPr(Y = 1|X) = G(v(X, β0)),\nParameter β, everything else, including G(u), is nonparametric. The v(x, β) notation\nallows location and scale normalization, e.g. v(x, β) = x1 + x0\n2β, x = (x1, x2\n0 )0, x1 scalar.\nCensored Regression with Unknown Disturbance Distribution: Z =\n(Y, X),\nY = max{0, Y ∗}, Y ∗ = X0β0 + ε, ε independent of X;\nParameter β, everything else, including distribution of ε, is nonparametric.\nBinary choice and censored regression are limited dependent variable models. Semi-\nparametric models are important here because misspecifying the distribution of the dis\nturbances leads to inconsistency of MLE.\nPartially Linear Regression: Z = (Y, X, W),\nE[Y |X, W] = X0β0 + g0(W).\nParameter β, everything else nonparametric, including additive component of regression.\nCan help with curse of dimensionality, with covariates X entering parametrically. In\nHausman and Newey (1995) W is log income and log price, and X includes about 20\ntime and location dummies. X may be variable of interest and g0(Z) some covariates,\ne.g. sample selection.\nIndex Regression: Z = (Y, X), v(x, β) a known function,\nE[Y |X] = τ(v(X, β0)),\nwhere the function τ(·) is unknown. Binary choice model has E[Y |X] = Pr(Y = 1|X) =\nτ(v(X, β0)), with τ( ). If allow conditional distribution of ε given X to depend (only) on\n·\nv(X, β0), then binary choice model becomes index model.\nSemiparametric Estimators\nEstimators of β0. Two kinds; do and do not require nonparametric estimation. Really\nmodel specific, but beyond scope to say why. One general kind of estimator:\nn\nX\nβˆ = arg min\nq(Zi, β)/n, β0 = arg min E[q(Zi, β)],\nβ∈B i=1\nβ∈B\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nB set of parmaeter values. Extremum estimator. Clever choices of q(Z, β) in some\nsemiparametric models.\nConditional median estimators use two facts:\nFact 1: The median of a monotonic transformation is transformation of the median.\nFact 2: The median minimizes the expected absloute deviation, i.e. med(Y |X) min\nimizes E[|Y - m(X)|] over functions m(·) of X.\nBinary Choice: v(x, β) include constant, ε has zero median, then med(Y ∗|X) =\nv(X, β0). 1(y > 0) is montonic tranformation, Fact 1 implies med(Y |X) = 1(med(Y ∗|X) >\n0) = 1(v(X, β0) > 0). Fact 2, β0 minimizes E[|y - 1(v(x, β) > 0)|], so\nn\nX\nβˆ = arg min\nY - 1(v(Xi, β) > 0) ]\nβ\ni=1\n|\n|\nMaximum score estimator of Manski (1977). Only requires med(Y ∗|X) = v(X, β0); allows\nfor heteroskedasticity.\nCensored Regression: x0β includes a constant, ε has median zero, then med(Y ∗|X) =\nX0β0. max{0, y} is a monotonic transformation, so Fact 1 says med(Y |X) = max{0, X0β0}.\nBy Fact 2, β0 minimizes E[|y - max{0, x0β}|], so\nn\nX\nβˆ = arg min\nYi - max{0, Xi\nβ\ni=1\n|\nβ}|.\nCensored least absolute deviations estimator of Powell (1984). Only requires med(Y ∗|X) =\nX0β, allows for heteroskedasticity.\nGeneralize: med(Y ∗|X) = v(X, β0) and Y = T (Y ∗) for monotonic transformation\nT (y). By Fact 1 med(Y |X) = T (v(X, β0)). Use Fact 2 to form\nn\nX\nβˆ = arg min\n|Yi - T (v(Xi, β))|.\nβ\ni=1\nGlobal minimization, rather than solving first-order conditions, is important. For\nmaximum score no first-order conditions. For censored LAD first-order conditions are\nzero whenever x0\niβ < 0 for all (i = 1, ..., n).\nApproach provides estimates parameters and conditional median predictions, not con\nditional means. Generalizes to conditional quantiles.\nConsistency and Asymptotic Normality of Minimization Esti\nmators\nA consistency result:\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProposition 5: If i) E[q(Z, β)] has a unique minimum at β0, ii) β0 ∈ B and B is\ncompact; iii) q(Z, β) is continuous at β with probability one; iv) E[supβ∈B |q(Zi, β)|] < inf ;\nthen\nn\nX\nβˆ = arg min\nq(Zi, β)\np\nβ∈B i=1\n-→ β0.\nWell known. Allows for q(Z, β) to be discontinuous. Binary choice model above, as\nsumption iii) satisfied if v(x, β) = x0β and Xi includes continuously distributed regressor\nwith corresponding component of β bounded away from zero on B. All the conditions\nare straightforward to check.\nAn asymptotic normality result, Van der Vaart (1995).\np\nProposition 6: If βˆ -→ β0, β0 is in the interior of B, and i) E[q(Zi, β)] is twice\ndifferentiable at β0 with nonsingular Hessian H; ii) there is d(z) such that E[d(Z)2] exists\nand for all β, β ∈ B, |q(Z, β ) - q(Z, β)| ≤ d(Z)kβ - βk; iii) with probability one q(Z, β)\nis differentiable at β0 with derivative m(Z), then\nd\nn 1/2(βˆ - β0) -→ N(0, H-1E[m(Z)m(Z)0]H-1).\nStraightforward to check for censored LAD. Do not hold for maximum score. Instead\nn1/3(βˆ - β0)\nd .\n-→\nEstimators with Nonparametric Components\nSome models require use of nonparametric estimators. Include the partially linear\nand index regressions. We discuss least squares estimation when there is a nonpara\nmetric component in the regression. Basic idea is to \"concentrate out\" nonparametric\ncomponent, to find a \"profile\" squared residual function, by substituting for nonpara\nmetric component an estimator.\nPartially linear model as in Robinson (1988). Know E[Y |X, W] minimizes E[(Y -\nG(X, W ))2] over G, so that\n(β0, g0( )) = arg min\n2].\n·\nβ,g( ) E[{Yi - Xiβ - g(W )}\n·\nDo minimization in two steps. First solve for minimum over g for fixed β, substituting\nthat minimum into the objective function, then minimize β. The minimizer over g for\nfixed β is\nE[Yi - X0β|Z] = E[Yi|Zi] - E[Xi|Zi]0β.\ni\nSubstituting\nβ0 = arg min\nZi])0β}2].\nβ E[{Yi - E[Yi|Zi] - (Xi - E[Xi|\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nEstimate using Eˆ[Yi|Zi] and Eˆ[Xi|Zi] and replacing the outside expectation by a sample\naverage,\nn\nX\nβˆ = arg min\n{Yi - Eˆ[Yi Zi] - (Xi - Eˆ[Xi Zi])0β}2/n,\nβ\ni=1\n|\n|\nLeast squares of Yi - Eˆ[Yi|Zi] on Xi - Eˆ[Xi|Zi]. Kernel or series fine.\nIndex regression, as in Ichimura (1993). By E[Y |X] = τ0(v(X, β0)),\n(β0, τ0( )) = arg min E[{Yi - τ(v(Xi, β))}2].\n·\nβ,τ( )·\nConcentrating out the τ, τ(X, β) = E[Y |v(X, β)]. Let ˆτ(Xi, β) a nonparametric estimator\nof E[Y |v(X, β)], estimator is\nn\nX\nβˆ = arg min\n{Yi - τˆ(Xi, β)}2 .\nβ\ni=1\nGeneralize to log-likelihood and other objective functions. Let q(z, β, η) depend\non parametric component β and nonparametric component η. True values minimize\nE[q(Z, β, η)], Estimator ˆη(β) of η(β) = arg minη E[q(Z, β, η)],\nn\nX\nβˆ = arg min\nq(Zi, β, ηˆ(β)).\nβ\ni=1\nAsymptotic theory difficult because of presence of nonparametric estimator. Know that\noften n-1/2 rate, asymptotically normal, and even estimation of η(β) does not affect\nasymptotic distribution.\nOther estimators that depend on nonparametric estimators may have affect on lim\niting distribution, e.g. average derivative estimator of Stoker (1987) and Powell, Stock,\nand Stoker (1989).\nJoint maximization possible but can be difficult because of need to smooth. Cannot\nallow ˆη(β) to be n-dimensional.\nAn empirical example is Hausman and Newey (1995). Graphs are actually those for\ngˆ(w) from a partially linear model.\nExample of theory, series estimator of partially linear model. Let pK(w) be a K × 1\nvector of approximating functions, such as power series or splines. Also let\nY =\n(Y1, ..., Yn)0, X = [X1, ..., Xn]0,\nP =\n[p K(W1), ..., p K(Wn)]0, Q = P(P 0P)-P 0,\nLet Eˆ[Yi|Wi] = pK(Wi)0(P 0P)-P 0Y and Eˆ[Xi\n0|Wi] = pK(Wi)0(P 0P)-P 0X be series esti\nmators. Residuals are (I - Q)Y and (I - Q)X, respectively, so by I - Q idempotent,\nβˆ = (X0(I - Q)X)-1X0(I - Q)Y.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nHere βˆ is also least squares regression of Y on X and P. Result on n1/2-consistency and\nasymptotic normality.\nProposition 7: If i) Yi and Xi have finite second moments; ii) H = E[V ar(Xi|Wi)]\nis nonsingular; iii) V ar(Yi|Xi, Wi) and V ar(Xi|Wi) are bounded; iv) there are C, γg, and\nγx such that for every K there are αK and βK with E[{g0(Wi) - αK pK (Wi)}2] ≤ K-γg\nand E[kE[X|\nK (Wi)k2] ≤ CK-2γx v) K/n -→ 0 and n1/2K-(γg+γx)\nWi]- βK p\n-→ 0, then\nfor Σ = E[V ar(Yi|Xi, Wi){Xi - E[Xi|Wi]}{Xi - E[Xi|Wi]}0],\n1/2(ˆ\nd\nn\nβ - β0) -→ N(0, H-1ΣH-1).\nCondition ii) is an identification condition that is essentially no perfect multicollinear\nity between Xi and any function of Wi. Intuitively, if one or more of the Xi variables\nwere functions of Wi then we could not separately identify g0(Wi) and the coefficients on\nthose variables. Furthermore, we know that necessary and sufficient conditions for identi\nfication of β from least squares objective function where g(Z) has been partialled out are\nthat Xi - E[Xi|Wi] have a nonsingular second moment matrix. By iterated expectations\nthat second moment matrix is\nE[(Xi - E[Xi|Wi])(Xi - E[Xi|Wi])0]\n= E[E[{(Xi - E[Xi|Wi])(Xi - E[Xi|Wi])0}|Wi]] = H.\nThus, condition ii) is the same as the usual identification condition for least squares after\npartialling out the nonparametric component.\nThe requirement K/n -→ 0 is a small variance condition and n1/2K-(γg +γx) -→ 0 a\nsmall bias condition. The bias here is of order K-(γg+γx), which is of smaller order than\njust the bias in approximating g0(z) (which is only K-γg ). Indeed, the order of the bias of\nβˆ is the product of the biases from approximating g0(z) and from approximating E[x|z].\nSo, one sufficient condition is that the bias in each of the nonparametric estimates vanish\nfaster than n-1/4 . This faster than n-1/4 condition is common to many semiparametric\nestimators.\nSome amount of smoothness is required for root-n consistency. Existence of K sat\nisfying the rate condition iii) requires that γg + γx > r/2. An analogous smoothness\nrequirement (or even a stronger one) is generally needed for root-n consistency of any\nsemiparametric estimator that requires estimation of a nonparametric component.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProof of Propostion 7: For simplicity we give the proof when Xi is scalar. Let\nM = I - Q, Z = (W1, ..., Wn)0,\nX\n=\n[E[X1|W1], ..., E[Xn|Wn]]0,\n\nV = X - X, g0 = (g0(z1), ..., g0(zn))0,\nε = Y - Xβ0 - g0.\nSubstituting Y = Xβ0 + g0 + ε in the formula for β,ˆ subtracting β0, and multiplying by\nn1/2 gives\nn 1/2(βˆ - β0)\n=\n(X0MX/n)-1(X0Mg0/n1/2 + X0Mε/n1/2)\np\nBy the law of large numbers V 0V/n -→ H. Also, similarly to the proof of Proposition 3,\nM\nV 0QV/n = Op(K/n), X0\nX/n = Op(K-2γx ),\ng0\n0 Mg0/n = Op(K-2γg ).\np\nThen V 0MV/n -→ H and\nM\n|V 0\nX/n|\np\nM\n≤ (V 0MV/n)1/2(X 0\nX/n)1/2 -→ 0,\nso that\nX0MX/n =\n(X + V )0M(X + V )/n\n= X M\nX/n\np\n+2X 0MV/n + V 0MV/n -→ H.\nNext, similarly to the proof of Proposition 3 we have E[V V 0|W] ≤ CIn and E[εε0|W, X] ≤\nCIn. Then\nE[{V 0Mg0/n1/2}2]\n=\nE[g0\n0 ME[V V 0|W]Mg0]/n\n≤ CE[g0\n0 Mg0]/n\n= O(K-2γg ) -→ 0.\n|X 0Mg/n1/2|2\n≤ n(X 0M\n0 Mg0/n)\nX/n)(g0\n= Op(nK-2γx-2γg )\np\n-→ 0,\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\np\nso that X0Mg0/n1/2 = X0Mg0/n1/2 + V 0Mg0/n1/2 -→ 0. Also,\nE[{X 0Mε/n1/2}2]\n=\nE[X 0ME[εε0|X, W ]MX ]/n\n≤ CE[X 0MX ]/n\n= O(K-2γx ) -→ 0,\nE[{V 0Qε/n1/2}2]\n=\nE[V 0QE[εε0|X, W ]QV ]/n\n≤ CE[V 0QV ]/n\n= O(K/n) -→ 0,\nd\nand by the central limit theorem, V 0ε/n1/2 -→ N(0, Σ). Therefore,\nX0Mε/n1/2 = X 0Mε/n1/2\n+V 0ε/n1/2 - V 0Qε/n1/2\nd\n-→ N(0, Σ).\nThen by the continuous mapping and Slutzky theorems it follows that\nn 1/2(βˆ - β0)\n=\n(H + op(1))-1[V 0ε/n1/2 + op(1)]]\nd\n-→ H-1N(0, Σ)\n=\nN(0, H-1ΣH-1).\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "notes_treat.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/70826c5fa3ca798b2ffcd83e84d4fc5f_notes_treat.pdf",
      "content": "Treatment Effects\nWhitney K. Newey\nMIT\nNovember 2005\nThe treatment effects literature is about how some outcome of interest, such as\nearnings, is affected by some treatment, such as a job training program. Evidently\nsuch treatment effects must be related to structural models, where the outcome\nof interest is the left hand side variable and the treatment is a right-hand side\nvariable. Indeed, as we will see, treatment effects can be thought of as coming\nfrom a linear structural model with random coefficients. Treatment effect models\ndo have a terminology and set up all their own though, so to help understand the\nliterature it is important to set them up the way others do.\nTo do so, let i index individuals and Di denote a treatment indicator, equal\nto 1 if a person is treated, and equal to 0 otherwise. For example, Di = 1 might\ncorrespond to enrollment in some training program or to some medical treatment.\nTo describe the treatment effect, we need to define two other variables. Let\nYi0 denote the potential outcome that would occur when person i is not treated\n(Di = 0) and Yi1 the potential outcome when they are treated (Di = 1). Clearly\nthese are not both observed. One of them will be \"counterfactual\", an outcome\nthat would have occurred if a different treatment had been given. The observed\noutcome will be\nYi = DiYi1 + (1 - Di)Yi0.\nThe treatment effect for individual i is given by\nβi = Yi1 - Yi0.\nThis object is clearly not identified, because only one of the potential outcomes\nis observed. There are several objects that may be interesting that are identified\nunder certain conditions. One of these is the average treatment effect, given by\ndef\nATE = E[βi].\nThis describes the average over the entire population of the individual treatment\neffects. Another interesting object is the average effect of treatment on the treated,\ngiven by\ndef\nATT = E[βi|Di = 1].\nThis gives the average over the subpopulation of treated people of the treatment\neffect. A third important object that is also of interest in the literature is called\nthe local average treatment effect. It will be described below.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTo help understand the treatment framework and the various effects, it helps\nto relate this to a regression model with random coefficients. By the equation for\nYi given above,\nYi = Yi0 + (Yi1 - Yi0)Di = αi + βiDi,\nαi = Yi0, βi = Yi1 - Yi0.\nThus we see that Yi follows a linear model where the treatment effect βi is the\ncoefficient of Di and the constant αi and slope βi may vary over individuals. The\nATE is then the average of the slope over the entire population and the ATT is\nthe average of the slope over the subset of the population where Di = 1.\nThis random coefficient set up also helps place the treatment effects environ\nment in a proper historical context. The coefficient βi = Yi1 - Yi0 is sometimes\ncalled a \"counterfactual\" because it describes how Yi would have been different\nif Di had been different. In the context of demand and supply systems we are\nfamiliar with such objects as \"movements along a curve.\" This kind of object was\nconsidered in economics as early as Wright (1928), who gives a nice explanation\nof \"movements along a curve\" in a supply and demand setting. Similarly, the\naverage treatment effect is just the expected value of the random coefficient in a\nlinear model, i.e., the average slope of the curve.\nThe ATE and ATT will be identified and can be estimated under various as\nsumption. Here we will discuss various cases in which these objects are identified.\nThe proofs of identification will consist of showing how the objects can be written\nin terms of expectations of the data.\nWe begin with the simplest case.\nConstant Treatment Effects\nA simple special case of this model is constant treatment effects where βi = β ,\ni.e. where the treatment effect is constant across individuals. Here the ATE and\nATT is simply β . In this case, for α = E[αi] and εi = αi - α ,\nα +\nYi =\nβDi + εi.\nHere the model reduces to a simple linear model with an additive disturbance\nand constant coefficients. In contrast, the general model is also a linear model\nwith additive disturbance but random slope coefficient. Note here the equivalence\nbetween having a random αi and having a constant plus disturbance αi = α + εi.\nWe can identify and estimate β and α in the usual way if we have an instrument\nZi that is uncorrelated with εi and correlated with Di, that is\n= Cov(Zi, εi) = Cov(Zi, αi) = Cov(Zi, Yi0),\nCov(Zi, Di)\n=\n0.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(\nIn this case the coefficient is identified from the usual IV equation\nβ = Cov(Zi, Yi)/Cov(Zi, Di).\nThis coefficient can be estimated in the usual way by replacing population covari\nances by sample covariances. In summary, there is not much new here, except\nterminology of putting standard model with dummy endogenous variable in a\ntreatment effects framework, as \"constant treatment effect.\"\nConstant treatment effects is too strong for many settings. It would say that\neffect of training on earnings or of smaller class size on education is the same for\nevery individual. This seems unlikely to hold in practice. Instead we would like\nto allow βi to vary over individuals.\nRandom Assigment\nRandom assignment means that whether or not a person is treated does not\ndepend on their outcomes. The specific statistical assumption that we make is\nthat\nE[Yi0|Di] = E[Yi0],\ni.e. that the mean of the nontreated variable does not depend on treatment status.\nEquivalently we can say that E[αi|Di] = 0. This is slightly more general than\nindependence, because it allows the higher-order moments of Yi0 to depend on Di.\nHowever, it seems difficult to think of environments where the mean assumption\nwould be true without full independence.\nTo see what happens under this assumption note first that\n0,\nDi = 0,\nE[βi|Di]Di =\nE[βi|Di = 1],\nDi = 1 = E[βi|Di = 1]Di.\nThen under the mean independence assumption,\nE[Yi|Di]\n= E[αi + βiDi|Di] = E[αi] + E[βi|Di]Di\n= E[αi] + E[βi|Di = 1]Di.\nHere the dummy variable regression of Yi on a constant Di has its slope coefficient\nthe ATT. If in addition we assume that the mean of Yi1 does not depend on Di,\ni.e. if we assume that\nE[Yi1|Di] = E[Yi1],\nthen we find that ATE = ATT , since\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nE[βi|Di = 1] = E[Yi1|Di = 1] - E[Yi0|Di = 1]\n= E[Yi1] - E[Yi0] = E[βi].\nSummarizing , we find that when Yi0 is mean independent of Di that the ATT\nis identified as the dummy coefficient in a regression of the outcome variable Yi on\na constant and the treatment dummy variable. We also find that if, in addition,\nYi1 is mean independent of Di then the ATE is also this coefficient. Of course,\nthis coefficient can be estimated by a linear regression of Yi on (1, Di). Further,\nas always, that linear regression coefficient is just the difference of means of Yi for\nthe treated and untreated observations.\nDiscussion\nRandom assignment is too strong for many applications. Often individuals\ncan choose whether to accept the treatment or not, e.g. by dropping out of the\nsample if they don't like the treatment conditions. They can opt out of training\nprograms, or not take medical treatment. If these decisions are related to (αi, βi)\nthen we do not have independence of (αi, βi) and Di. In terms of the linear model\nYi = αi + βiDi we have possible endogeneity, where Di may be correlated with\nthe random coefficients αi and βi. This is a more severe problem than the usual\ncase because the slope βi also may be correlated with Di.\nThere are two approaches to this problem. One (familiar) one is instrumental\nvariables (IV). The second approach is called \"selection on observables.\" In that\napproach conditioning on some observable variables removes the correlation be\ntween Di and (αi, βi). Because IV is a most familiar and common approach we\nwill first consider IV.\nIV Identification of Treatment Effects\nIn the usual linear model, of which the constant treatment effects is a special\ncase, the assumptions that are needed for the identification of the slope is that the\ninstrument is uncorrelated with the disturbance and correlated with Di. Similar\nconditions will be used for IV identification of treatment effects. Let Zi be an\ninstrument. We will assume throughout that\nE[αi|Zi] = E[Yi0|Zi] = E[Yi0] = E[αi],\ni.e. that the outcome without treatment is mean independent of the instrument.\nWe also will focus on the case where Zi is also a dummy variable, i.e. where\nZi ∈{0, 1}, with P = Pr(Zi = 1) and 0 < P < 1. (Question: Why do we assume\n0 < P < 1?). For a dummy instrument there is a useful formula for the covariance\nbetween the instrument and any other random variable Wi. Specifically, we have\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nE[WiZi]\nCov(Wi, Zi)\n= E[WiZi] - E[Wi]E[Zi] = (\n- E[Wi])P\nP\n=\n(E[Wi|Zi = 1] - E[Wi])P\n= {E[Wi|Zi = 1] - (PE[Wi|Zi = 1] + (1 - P )E[Wi|Zi = 0])} P\n=\n(E[Wi|Zi = 1] - E[Wi|Zi = 0])P (1 - P ).\nThat is, the covariance between Wi and Zi is the difference of the conditional\nmean at the two values of Z times P (1 - P ).\nThis formula has two useful implications. The first is that mean independence\nof Yi0 from Zi is equivalent to Yi0 being uncorrelated with Zi. This occurs since\nCov(Zi, Yi0) = 0 if and only if E[Wi|Zi = 1] = E[Wi|Zi = 0]. A second useful\nimplication is a formula for the limit of the IV estimator of the slope, given by\nCov(Zi, Yi) = E[Yi|Zi = 1] - E[Yi|Zi = 0]\nCov(Zi, Di)\nE[Di|Zi = 1] - E[Di|Zi = 0]\nThis is often referred to the Wald IV formula, referring to work here Wald sug\ngested using dummy variables as an IV solution to the measurement error problem.\nIn general, under mean independence of αi from Zi, it does not seem like\nthe IV formula identifies the ATT, the ATE, or anything useful. Plugging in\nYi = αi + βiDi, and using mean independence of αi we find\nCov(Zi, Yi)\n= E[αi|Zi = 1] - E[αi|Zi = 0] + E[βiDi|Zi = 1] - E[βiDi|Zi = 0]\nCov(Zi, Di)\nE[Di|Zi = 1] - E[Di|Zi = 0]\n= E[βiDi|Zi = 1] - E[βiDi|Zi = 0] .\nE[Di|Zi = 1] - E[Di|Zi = 0]\nIn general, the problem is that βi and Di are correlated, so that (apparently) it\nis not possible separate them out in general. There are two interesting, specific\ncases though where something important is identified. They are random intention\nto treat and local average treatment effects.\nRandom Intention to Treat\nA common occurrence in medical trials is that people are randomly assigned to\ntreatment but that not all take the treatment. Here Zi represents the assignment\nto treatment, with Zi = 1 if individual i is assigned to be treated and Zi = 0 if\nthey are not. In this setting, the only ones who are treated (i.e. for which Di = 1)\nwill be those who were randomly assigned to treated. It turns out that in this\ncase IV gives the ATT. This finding, due to Imbens and Rubin, has led to the\nwidespread use of IV in biostatistics.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTo show that IV gives the ATT, note that Zi = 0 will not be treated, i.e.\nDi = 0 when Zi = 0. Then\nCov(Zi, Yi) = E[βiDi|Zi = 1] - 0 = E[βiDi|Zi = 1] .\nCov(Zi, Di)\nE[Di|Zi = 1] - 0\nE[Di|Zi = 1]\nAlso, note that Di = 1 implies Zi = 1, so that {Di = 1} ⊂{Zi = 1}. Therefore,\nE[βi|Di = 1, Zi = 1] = E[βi|Di = 1] = ATT . Also, it follows similarly to the\nreasoning above that\nDiE[βi|Di, Zi = 1] = DiE[βi|Di = 1, Zi = 1] = Di · ATT.\nBy iterated expectations it follows that\nE[βiDi|Zi = 1] = E[DiE[βi|Di, Zi = 1]|Zi = 1] = ATT · E[Di|Zi = 1].\nThen dividing gives\nCov(Zi, Yi) = E[βiDi|Zi = 1] = ATT · E[Di|Zi = 1] = ATT.\nCov(Zi, Di)\nE[Di|Zi = 1]\nE[Di|Zi = 1]\nThe Local Average Treatment Effect\nA second case where an interesting treatment effect is identified by IV involves\nindependence and monotonicity conditions. Consider the following conditions:\nIndependence: Di = Π(Zi, Vi) and (βi, Vi) is independent of Zi;\nMonotonicity: Π(1, Vi) ≥ Π(0, Vi) and Pr(Π(1, Vi) > Π(0, Vi)) > 0.\nThe independence condition says that there is a reduced form Π(z, v) with\na disturbance Vi that may be a vector and enters nonlinearly. An example is a\nthreshold crossing model where Di = 1(Zi + Vi > 0). The monotonicity condition\nchanging the instrument only moves the treatment one direction. This condition is\nsatisfied in a threshold crossing model. The reduced form is sometimes called the\nselection equation, with a person being selected into treatment when Π(z, v) = 1.\nUnder these conditions it turns out that IV identifies an average of βi over a\nsubpopulation that is referred to as the Local Average Treatment Effect (LATE).\nThis effect is defined as\nLATE = E[βi|Π(1, Vi) > Π(0, Vi)].\nThis object is the average of the treatment effect over the individuals whose be\nhavior would be different if the instrument were changed. This object may often\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nbe a parameter of interest. For example, in a model where Yi is the log of earn\nings, Di is completing high school, and Zi is a quarter of birth dummy, LATE is\nthe average effect of a high school education over all those dropouts who would\nhave remained in school had their quarter of birth been different and for those\nwho remained in school but would have dropped out if their quarter of birth were\ndifferent. Thus, IV estimates the average returns to completing high school for\npotential dropouts. This is an interesting parameter, although it is not the returns\nto schooling over the whole population.\nTo show that IV give LATE under independence and monotonicity, let Ti =\nΠ(1, Vi) - Π(0, Vi). Then we have\nE[βiDi|Zi = 1] - E[βiDi|Zi = 0]\n= E[βiΠ(1, Vi)|Zi = 1] - E[βiΠ(0, Vi)|Zi = 0]\n= E[βiΠ(1, Vi)] - E[βiΠ(0, Vi)] = E[βiTi].\nIt follows similarly that\nE[Di|Zi = 1] - E[Di|Zi = 0] = E[Ti].\nBy monotonicity, Ti is a dummy variable, taking the value zero or one. Therefore\nwe have\nCov(Zi, Yi)\nE[βiTi]\nCov(Zi, Di) = E[Ti] = E[βi|Ti = 1] = E[βi|Π(1, Vi) > Π(0, Vi)].\nLATE Empirical Example\nAn empirical example is provided by the Angrist and Krueger (1991) study of\nthe returns to schooling using quarter of birth as an instrument. We consider data\ndrawn from the 1980 U.S. Census for males born in 1930-1939, as in Donald and\nNewey (2001, \"Choosing the Number of Instruments,\" Econometrica). The 2SLS\nestimator with 3 instruments is .1077 with standard error .0195 and the FULL\nestimator with 180 instruments is .1063 with standard error .0143 (corrected for\nmany instruments). Thus we find returns to schooling of \"potential dropouts\" is\nabout 11 percent.\nSelection on Observables\nThe other kind of model that has been used to identify treatment effects is one\nwhere conditioning on observable (or identifiable) variables Xi makes the treat\nment behave as if it were randomly assigned. This is like removing endogeneity in\na linear equation by adding regressors. The conditioning variables are like omitted\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nregressors, which remove the endogeneity when they are included. The specific\nassumption that is made is\nE[Yi0|Xi, Di] = E[Yi0|Xi].\nIn word, it is assumed that Yi0 is mean independent of Di conditional on Xi.\nThis assumption is analogous to the previous one that E[Yi0|Di] = E[Yi0], being\na conditional version of that hypothesis.\nOne concern with this kind of assumption is the source for the variables Xi.\nThere are some economic models where such variables are implied by the model.\nHowever in many cases in applications these variables Xi are chosen without\nreference to a model. In those cases identification is fragile, requiring specifying\njust the right Xi. Conditional mean independence that holds for X, need not hold\nfor a subset of Xi nor when additional variables are added to Xi.\nThis assumption allows identification of the ATT, with one additional condi\ntion. Let χ denote the support of Xi (the smallest closed set having probability\none), and χ0 and χ1 the support of Xi conditional on Di = 0 and Di = 1 respec\ntively. The additional condition is the common support condition that\nχ = χ0 = χ1.\nThis assumption is necessary and sufficient for E[Yi|Xi, Di = 1] and E[Yi|Xi, Di =\n0] to be well defined for all Xi. It is verifiable and may or may not be satisfied in\npractice.\nThe common support condition and conditional mean independence give\nE[Yi|Xi, Di = 1] - E[Yi|Xi, Di = 0] = E[αi|Xi, Di = 1] - E[αi|Xi, Di = 0] + E[βi|Xi, Di = 1]\n= E[βi|Xi, Di = 1].\nThe object E[βi|Xi, Di = 1] is a conditional version of the ATT. By iterated\nexpectations the ATT is then identified as the expectation over Xi of this difference\ngiven Di = 1, that is\nATT = E[βi|Di = 1] = E [{E[Yi|Xi, Di = 1] - E[Yi|Xi, Di = 0]} |Di = 1] .\nThe ATE can also be obtained if we assume that Yi1 is conditional mean indepen\ndent of Di conditional on Xi. In that case\nE[βi|Xi, Di = 1] = E[Yi1|Xi, Di = 1] - E[Yi0|Xi, Di = 1]\n= E[Yi1|Xi] - E[Yi0|Xi] = E[βi|Xi].\nTherefore\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nATE = E[βi] = E [{E[Yi|Xi, Di = 1] - E[Yi|Xi, Di = 0]}]\nUnlike the unconditional case the ATE is a different function of the data dis\ntribution than the ATT. The ATE is obtained by averaging E[Yi|Xi, Di = 1] -\nE[Yi|Xi, Di = 0] over all Xi while the ATT is obtained by averaging over just\nDi = 1.\nEstimating the ATT and ATE under these conditional restrictions is a chal\nlenge. Notice that they depend on conditional expectations. Usually we will not\nwant to assume that these conditional expectations have any particular functional\nform. Consequently, we will want to use nonparametric regression estimators,\nwhich will be discussed later in the course.\nNonparametric estimation is difficult when the dimension of Xi is large. This\nis often referred to as the \"curse of dimensionality.\" Some have tried to reduce\nthe curse of dimensionality using the \"propensity score\" P (X), which is defined\nas the conditional probability of being treated (or \"selected\") given X, i.e.\nP (Xi) = Pr(Di = 1|Xi) = E[Di|Xi].\nIt turns out that the conditional mean independence of Yi0 given Xi implies con\nditional mean independence given P (Xi). Thus, if P (Xi) were known, it would\nbe possible to identify and estimate the ATE and ATT using a one dimensional\nconditioning variable rather than a multidimensional variable Xi. Specifically,\nif E[Yi0|Xi, Di] = E[Yi0|Xi] and 0 < P (Xi) < 1 with probability one then\nE[Yi0|P (Xi), Di] = E[Yi0|P (Xi)], so that reasoning like that above gives\nATT = E [{E[Yi|P (Xi), Di = 1] - E[Yi|P (Xi), Di = 0]} |Di = 1] .\nIf, in addition, E[Yi0|Xi, Di] = E[Yi0|Xi] then\nATE = E [{E[Yi|P (Xi), Di = 1] - E[Yi|P (Xi), Di = 0]}] .\nThus, ATE and ATT are expectations of nonparametric functions of two variables,\nP (Xi), and Di.\nIf P (Xi) is completely unknown and unrestricted there is no known advantage\nfor conditioning on the propensity score, since P (Xi) is also a function of a high-\ndimensional argument. Thus, it appears that any advantage for using the propen\nsity score will depend on knowing more about P (X) than about E[Yi|Xi, Di].\nIt remains to prove that independence conditional on X implies independence\nconditional on P (Xi). For notational simplicity let Pi = P (Xi). We will prove\nthe result for a general variable Wi. The general result will then apply to both Yi0\nand Yi1. To prove that E[Wi|Xi, Di] = E[Wi|Xi] implies E[Wi|Pi, Di] = E[Wi|Pi],\nnote that by iterated expectations, E[Di|Pi] = E[E[Di|Xi]|Pi] = Pi. By iterated\nexpectations again,\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nE[Wi|Pi, Di = 1] = E[E[Wi|Xi, Di = 1]|Pi, Di = 1] = E[E[Wi|Xi]|Pi, Di = 1]\n= E[DiE[Wi|Xi]|Pi] = E[PiE[Wi|Xi]|Pi]\nE[Di|Pi]\nPi\n= E[E[Wi|Xi]|Pi] = E[Wi|Pi].\nBy similar reasoning we also have E[Wi|Pi, Di = 0] = E[Wi|Pi], so the conclusion\nfollows by the previous equation.\nCite as: Whitney Newey, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/0b7c3f5ce9642cc7f603d6404a8975e5_lecture01.pdf",
      "content": "14.385\nNonlinear Econometrics\nLecture 1. Basic Overview of Some Principal\nMethods.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMain Methods to be Covered in the Course:\n1. Moment Methods (Generalized Method\nof Moments, Nonlinear IV, Z-estimation), in\ncluding moment inequalities (later in the course).\n2. Extremum Methods (MLE, M-estimators,\nquantile regression, minimum distance, GMM).\n3. Bayesian and Quasi-Bayesian Methods.\nAll methods are interrelated.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExample 1: Optimizing behavior of eco\nnomic agents.\nA representative agent maximizes expected util\nity from consumption, Hansen and Singleton\n(1982). Euler (first-order) conditions for opti\nmizing behavior imply a conditional moment\nrestriction\nE[ρ(zt, θ0) xt] = 0,\n|\nwhere xt represents information available at\ntime t or before, and\nρ(zt, θ) = θ1(ct+1/ct)θ2Rt+1 - 1.\nwhere ct is consumption in t and Rt+1 is the\nreturn on an asset between t and t + 1. Here\nθ1 is time discount factor and per-period utility\nfunction is (ct)θ2+1.\nGeneralized Method of Moments (GMM)\nis the main method for dealing with the mo\nment restriction.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nIn many cases GMM is known as Nonlinear\nInstrumental Variable estimator. In statis\ntics, the analogs of GMM are known as Z-\nestimators or Estimating Function estima\ntors.\nGMM Notation:\nθ : p × 1 parameter vector.\nzi : data observation.\ng(z, θ) : and r × 1 vector-function, r ≥ p.\nMain Assumption: z1, ..., zn are independently\nand identically distributed (i.i.d.) with\nE[g(z, θ0)] = 0.\nThe problem is to estimate θ0 from the avail\nable data.\nGeneralized Method of Moments (GMM):\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFor\nˆg(θ) := En[g(zi, θ)].\nθˆ minimizes ˆg(θ)0Aˆˆg(θ).\nwhere Aˆ is a positive semi-definite (p.s.d.) ma\ntrix. En is the empirical expectation operator:\n1 n\nEn[g(zi, θ)] :=\nX\ng(zi, θ),\nn i=1\nThe estimator sets the empirical moments as\nclose as possible to their population counter\nparts, which are known to be 0.\nProperties: Consistent Asymptotically Nor\nmal (CAN).\nChoice of Aˆ that minimizes asymptotic vari\nance is a consistent estimator of the asymp\ntotic variance of √nˆg(θ0).\nA =\nlim V ar[√nˆg(θ0)]\n-1 .\nn\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFor E[g(zi, θ0)g(zj, θ0)0] = 0 for i =\nj (no auto-\ncorrelation in the moment function) an optimal\nchoice of weighting matrix is\nA =\nE[g(zi, θ )g(zi, θ )0]\n-1 ,\nThis can be estimated by\nAˆ =\nEn[g(zi, θ )g(zi, θ )0]\n-1 ,\nwhere θ is a preliminary (GMM) estimator. One\ncan iterate on the estimate.\nConditional Moment Restrictions:\nOften\nhave residual vector ρ(z, θ) satisfying a con\nditional moment restriction\nE[ρ(z, θ0) x] = 0.\n|\nThen by iterated expectations, for any con\nformable matrix B(x) of functions of x, we\nhave\ng(z, θ) = B(x)ρ(z, θ)\nsatisfy unconditional moment restrictions. Can\nthink of B(x) as instrumental variables, in which\ncase the estimator is known as the Nonlinear\nInstrumental Variable Estimator.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMaximum Likelihood Estimation\nNotation:\nθ : p × 1 parameter vector.\nzi : data observation; e.g. zi = (yi, xi).\nf (z θ) : family of density or probability functions.\n|\nData Assumption: z1, ..., zn are i.i.d. with\nzi having pdf f(z θ0).\n|\nThe key fact is that\nθ0 maximizes E[ln f(zi θ)],\n|\nwhich is a consequence of information in\nequality.\nMaximum Likelihood Estimator (MLE):\nθˆ maximizes En[ln f(zi θ)].\n|\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProperties: Consistent and asymptotically nor\nmal (CAN) and asymptotically efficient.\nThus, MLE is asymptotically the most efficient\nestimator if f(z θ) is correct. But, estimator\n|\nmay not be consistent if f(z θ) is misspecified.\n|\nConditional and Marginal MLE: Often we\ndo not want to specify the entire distribution\nof the data. For example, we generally do not\nwant to specify distribution of regressors. Re\ncall the classical normal regression model as\nthe primary example.\nSuppose that the data take the form z = (y, x)\nand the density can be factored as\nf(z θ) = f(y x, β)h(x γ),\n|\n|\n|\nwhere β and γ are subvectors of θ, f (y x, β) is\n|\na conditional density of y given x and h(x γ) is\n|\na marginal density for x.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThen we can consider the estimator that max\nimizes the log-likelihood from either the con\nditional or the marginal part.\nConditional MLE (CMLE):\nβˆ maximizes En[ln f(yi xi, β)]\n|\nThis estimator is CAN and asymptotically effi\ncient when the information matrix for (β, γ) is\nblock-diagonal, and when h(x γ) has unknown\n|\nfunctional form. Here x can often be thought\nof as regressors, where we leave the marginal\ndistribution unspecified, so that the CMLE is\nefficient.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n|\nExample (Censored Regression Model of\nType I):\nThe following example often arises\nwhen wages are top-coded or censored in other\nways. The equation of interest is\n= x\nYi\n∗\niβ0 + ui,\nbut we only observe\nYi = min{Yi\n∗, L}.\nSimple least squares where we regress Yi on\nxi\n0β on the resulting data will give an incon\nsistent estimate, because the error does not\nsatisfy the usual orthogonality condition E[Yi -\nxi\n0β0 xi] = 0. A way to deal with the problem\nis to use likelihood methods that explicitly ac\ncount for how the data is generated.\nThe log-likelihood for a single observation (y, x)\nconditional on x is\nln f(y x, θ) = 1(y < L) ln[f(y x, θ)]\n|\n|\n+ 1(y = L) ln Pr[y = L x, θ].\n|\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nNote that likelihood is a mixture of discrete\nand continuous distribution.\nLet's specify each of the pieces. Suppose ui is\nindependent of xi and is distributed N(0, σ2).\nThen for the standard normal pdf φ(u) with a\ncdf Φ(u) we have\nPr(y = L|x, β, σ2) = Pr(ui ≥ L - x0β|x, β, σ2)\n=\n1 - Φ((L - x0β)/σ)\n= Φ((x0β - L)/σ).\nAlso, for y < L the conditional pdf of y given\nx is\nf (y|x, β, σ) = σ-1φ((y - x0β)/σ), y < L.\nThen the log-likelihood for a single observation\n(y, x) conditional on x is\nln f(y|x, β, σ2) = 1(y < L) ln[σ-1φ((y - x0β)/σ)]\n+ 1(y = L) ln Φ((x0β - L)/σ).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe consistency of the MLE for estimating θ0\nwill depend crucially on the correctness of the\nnormality assumption, in particular upon the\ndistributional shape of the upper tail.\nCen\nsored quantile regression methods introduced\nlater do not require strong distributional as\nsumptions.\nAnother way to relax the distributional assump\ntions is to make use of more flexible models of\ndistributions. For example, take a t-family for\nthe error term instead of the normal family.\nExample 2 Contd. (Type-II Censored Re\ngression Model or Selection Model):\nThe\nequation of interest is\nYi\n∗\n2 = x0\niβ02 + ui2,\nand we observe Yi2 = Yi\n∗\n2 if\nYi\n∗\n1 = x0\niβ01 + ui1 ≥ 0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFor unobserved values of Yi\n∗\n2, we set Yi2 = 0.\nWe also observe Yi1 = 1(Yi\n∗\n1 > 0).\nIn the context of labor force participation, this\nmodels a two-part decision process: first a per\nson decides whether to work or not, on the\nbasis of some index Yi1, and then decides how\nmany hours Yi2 to work. This is all given the\ncircumstances modeled by xi and disturbances\nui1 and ui2.\nThe conditional log-likelihood function for a\nsingle observation (y2, y1) takes the form\nln f(y2, y1 x, θ) = 1(y1 = 0) ln Pr[y1 = 0 x]\n|\n+ 1(y1 = 1)\nh\nln\nf[y2 x, y1\n|\n= 1]\nx]\ni\n|\n·\nPr[y1 = 1|\n.\nWe can parametrically specify each piece, us\ning the joint normality of disturbances and their\nindependence from regressors. Then write the\nconditional likelihood function and proceed with\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nmaximum likelihood estimation. You will work\nthrough many details of this in HW.\nMarginal MLE:\nˆγ maximizes En[ln f(xi γ)].\n|\nThis estimator is CAN and asymptotically ef\nficient when f(y x, β) has unknown functional\n|\nform or when β and when information matrix\nfor β and γ is block-diagonal.\nThis can be\nthought of as throwing away some data.\nNeed an example here.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nM-estimators are a generalization of MLE,\nand the name stands for \"maximum likelihood\nlike estimators.\" Another name, more often\nused in econometrics, is Quasi Maximum Like\nlihood Estimator (QMLE).\nThe parameter θ0 is known to solve\nθ0 minimizes Em(z, θ)\nand the estimator takes the form\nθˆ minimizes Enm(z, θ).\nExample (Quantile Regression). Other than\nlinear and nonlinear least squares, an important\nexample is the median regression, or least ab\nsolute deviation estimator, where m-function\ntakes the form m(z, θ) = |Y - X0θ|, so that the\nestimator minimizes\nEn[|Y - X0θ|].\nMedian regression aims to estimate the condi\ntional median of Y given X, modeled by X0θ0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nQuantile regression is a generalization of the\nmedian regression that aims to estimate the\nτ -conditional quantile of Y given X. Quan\ntile regression minimizes the asymmetric ab\nsolute deviation with m-function m(z, θ) equal\nto\nρτ (Y - X0θ) := τ |Y - X0θ|+ + (1 - τ )|Y - X0θ|-,\nwhere τ ∈ (0, 1).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMinimum Distance Estimation\nNotation:\nθ : p × 1 parameter vector of interest\nπˆ\n: preliminary \"reduced form estimator\"\nh(π, θ)\n: r × 1 vector function of the parameters,\nwhere r ≥ p.\nData Assumption: πˆ is a consistent estima\ntor of a parameter vector π0. The parameter\nof interest θ0 is an (implicit) function of θ0\ndefined by:\nh(π0, θ0) = 0.\nThus, here the data enters only through the\nparameters estimator ˆπ.\nMinimum Distance Estimator (MDE): The\nestimator is obtained from\nθˆ minimizes h(ˆ\n0 ˆ\nπ, θ).\nπ, θ) Ah(ˆ\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwhere Aˆ is a positive semi-definite (p.s.d.) ma\ntrix. Idea is that estimator sets h(ˆπ, θ) close to\npopulation counterpart of 0.\nProperties: CAN. Choice of Aˆ that minimizes\nasymptotic variance is\nˆ\nˆ\nd\nA = Ω-1 , for √nh(ˆπ, θ0)\nN(0, Ω).\n→\nForm of ˆ\nπ and h.\nΩ depends on ˆ\nExample. Combining data from different sources\nfor instrumental variables estimation. Suppose\nthat we are interested in estimating the slope\nδ0 of an equation\nYi = α0+δ0Di+ui = Xi\n0β0+ui, Xi\n0 = (1, Di), β0 = (α, δ\nwhere Di is a dummy variable taking on 0 or\n1. Suppose also that there is an instrument\nzi available, with Cov(zi, ui) = 0 Cov(zi, Di) =\n0. The problem is that individual data is not\navailable.\nInstead, for Zi\n0 = (1, zi) one has\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nan estimator ˆπZX of πZX = E[ZiXi\n0] from one\ndata set and another estimator ˆπZY of πZY =\nE[ZiYi] from another data set. The instrument\northogonality condition gives 0 = E[Ziui] =\nπZY - πZXβ0. This condition can be exploited\nto form a MDE as βˆ minimizes\n(ˆπZY - πˆZXβ)0Aˆ(ˆπZY - πˆZXβ).\nAn empirical example is given in Angrist (1990,\n\"Lifetime Earnings and the Vietnam Era Draft\nLottery: Evidence from Social Security Admin\nistrative Records\"). There Yi is earnings, Di\nindicates whether they served in the military,\nand zi indicates whether their birthday was af\nter a certain date. In that application, πˆZX\ncomes from military records and ˆπZY from so\ncial security data.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExtremum Estimator\nNotation:\nθ : p × 1 parameter vector of interest\nQˆ(θ)\n: r × 1 function of data and parameters.\nData Assumption: Here we assume that\nsup Qˆ(θ) - Q(θ) →p 0,\nθ∈Θ\n|\n|\nand\nθ0 minimizes Q(θ).\nHere the data enters only through the function\nQˆ(θ).\nExtremum Estimator: The estimator is ob\ntained as follows:\nθˆ minimizes Qˆ(θ).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe idea is that Qˆ(θ) is close to a function\nthat is minimized at θ0, so minimizer of Qˆ(θ)\nshould be close to θ0.\nSpecial Cases:\nMLE : Qˆ(θ) = -En ln f (zi, θ),\nM : Qˆ(θ) = Enm(zi, θ),\nGMM : Qˆ(θ) = ˆg(θ)0Aˆˆg(θ),\nMD : Q(θ) =\nπ, θ)0Ah(ˆ\nˆ\nh(ˆ\nˆ\nπ, θ).\nDiscussion:\nThe extremum approach pro\nvides one unified approach to the asymptotic\nproperties of estimators. The GMM approach\ncan also be viewed as a fairly general approach.\nRecent literature on empirical likelihood can\nbe viewed as an important refinement of the\nGMM approach.\nComputational considerations often also lean\ntoward extremum estimators. For good com\nputing, we would like Qˆ(θ) to be convex in\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nthe parameters.\nAn example is quantile re\ngression that constructs a convex M-function\n- as asymmetric least absolute deviation - that\ncan be easily minimized. In contrast, the GMM\napproach to the quantile problem is basically\nintractable. Computability is extremely im\nportant for applicability.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSome remarks on the history of thought:\n1. MLE was pioneered by Laplace and Gauss,\nand Fisher.\nCramer gave the first rigorous\ntreatment of asymptotic normality. Ibragimov\nand Hasminskii and LeCam's treatment were\nother milestones in asymptotics for MLE, in\ncluding both regular and non-regular cases. Hu\nber gave the first rigorous study of M-estimators.\n2.\nMethod of moments was introduced by\nKarl Pearson. Nonlinear IV was introduced by\nTakeshi Amemiya who also derived efficient in\nstruments. In statistics, Godambe pioneered\nestimating equations and studied their efficient\nform. GMM, especially with a view towards\neconomic time series, was introduced by Lars\nHansen. Hansen used ergodicity and structure\nof the economic optimizing model to justify\nthe weighting matrix and CAN properties. Ex\ntremum approach was developed in depth in\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAmemiya's Advanced Econometrics text. Keith\nKnight's and Charles Geyer's treatment of ex\ntremum estimators and M-estimators using epi\nconvergence appears to be the best current\ntreatments of asymptotics.\n3. Hansen and Singleton's Example 1 is one\nof the paradigms that define econometrics as\na field. Tobin introduced Type I censored re\ngression models, another paradigm of econo\nmetrics, and Amemiya gave the first rigorous\ntreatment. Selection models or Type II models\nwere developed by Gronau and Heckman.\n4. M-estimators were studied by Huber. Me\ndian regression goes back to Boskovich in 17th\ncentury and Laplace (1818). Quantile regres\nsion was introduced by Koenker and Bassett\n(1978) and developed fully in an impressive\nbody of work led by Roger Koenker. The max\nimum likelihood, least squares, and quantile re\ngression are probably the most prominent rep\nresentatives of M-estimators.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/065f6c53a14295dcd7e90cf281bf902f_lecture02.pdf",
      "content": "14.385 Fall, 2007\nNonlinear Econometrics\nLecture 2.\nTheory: Consistency for Extremum Estimators\nModeling: Probit, Logit, and Other Links.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExample: Binary Choice Models. The la\ntent outcome is defined by the equation\nyi\n∗ = xi\n0β -εi,\nεi ∼ F(·).\nWe observe\n∗\nyi = 1(yi ≥ 0).\nThe cdf F is completely known. Then\nP(yi = 1|xi)\n=\nP(εi ≤ xi\n0β|xi)\n=\nF(xi\n0β).\nWe can then estimate β using the log-likelihood\nfunction\nQˆ(β) = En[yi ln F(x 0\niβ)+(1-yi) ln(1-F(x 0\niβ))].\nThe resulting MLE are CAN and efficient, un\nder regularity conditions.\nThe story is that a consumer may have two\n∗\nchoices, the utility from one choice is yi =\nx0\niβ - εi and the utility from the other is nor\nmalized to be 0.\nWe need to estimate the\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nparameters of the latent utility based on the\nobserved choice frequencies.\nEstimands: The key parameters to estimate\nare P[yi = 1|xi] and the partial effects of the\nkind\n∂P[yi = 1|xi] = f(xi\n0β)βj,\n∂xij\nwhere f = F 0 . These parameters are function\nals of parameter β and the link F.\nChoices of F:\n- Logit: F(t) = Λ(t) =\nexp(t) .\n1+exp(t)\n- Probit: F(t) = Φ(t), standard normal cdf.\n- Cauchy: F(t) = C(t) = 1\nπ\n+\narctan(t), the\nCauchy cdf.\n- Gosset: F(t) = T(t, v), the cdf of t-variable\nwith v degrees of freedom.\nChoice of F(·) can be important especially\nin the tails. The prediction of small and large\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nprobabilities by different models may differ sub\nstantially.\nFor example, Probit and Cauchit\nlinks, Φ(t) and C(t), have drastically different\ntail behavior and give different predictions for\nthe same value of the index t.\nSee Figure 1\nfor a theoretical example and Figure 2 for an\nempirical example. In the housing example, yi\nrecords whether a person owns a house or not,\nand xi consists of an intercept and person's\nincome.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nP-P Plots for various Links F\nCumu\nlati\nve p\nroba\nbili\nties\nof\nNorm\nal,\nCauc\nhy,\nand\nLogit distr\nibution\ns plott\ned against the cumulative Normal probabilities.\nNormal Probability\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nPredicted probabilities of owning a house, estimated using Normal, Cauchy, and Logit, plotted against the Normal prediction.\nPredicted\nProbabilities of Owning a House\nPrediction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormal\nCauchy\nLogit\nLinear\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormal Prediction\nChoice of F(·) can be less important when\nusing flexible functional forms. Indeed, for any\nF we can approximate\nP[yi = 1|x] ≈ F[P(x)0β],\nwhere P(x) is a collection of approximating\nfunctions, for example, splines, powers, or other\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nseries, as we know from the basic approxima\ntion theory.\nThis point is illustrated in the\nfollowing Figure, which deals with an earlier\nhousing example, but uses flexible functional\nform with P(x) generated as a cubic spline with\nten degrees of freedom. Flexibility is great for\nthis reason, but of course has its own price:\nadditional parameters lead to increased esti\nmation variance.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nPredicted probabilities of owning a house, estimated using Normal, Cauchy, and Logit, plotted against the Normal prediction using flexible functional forms of regressors coincide more.\nFlexibly Pedicted Probabilities of Owning a House\nPrediction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormal\nCauchy\nLogit\nLinear\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormal Prediction\nDiscussion:\nChoice of the right model is a\nhard and very important problem is statistical\nanalysis.\nUsing flexible links, e.g.\nt-link vs.\nprobit link, comes at a cost of additional pa\nrameters. Using flexible expansions inside the\nlinks also requires additional parameters. Flex\nibility reduces the approximation error (bias),\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nbut typically increases estimation variance. Thus\nan optimal choice has to balance these terms.\nA useful device for choosing best performing\nmodels is cross-validation.\nReading: A very nice reference is R. Koenker\nand J. Yoon (2006) who provide a systematic\ntreatment of the links, beyond logits and pro-\nbits, with an application to propensity score\nmatching.\nThe estimates plotted in the fig\nures were produced using R language's pack\nage glm. The Cauchy, Gosset, and other links\nfor this package were implemented by Koenker\nand Yoon (2006).\nReferences:\nKoenker, R. and J. Yoon (2006), \"Parametric Links for\nBinary Response Models.\"\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n(http://www.econ.uiuc.edu/~roger/research/links/links.html)\n\nb\n\n1. Extremum Consistency\nExtremum estimator\nθˆ = arg min Q(θ).\nθ∈Θ\nAs we have seen in Lecture 1, extremum esti\nmator encompasses nearly all estimators (MLE,\nM, GMM, MD). Thus, consistency of all these\nestimators will follow from consistency of ex\ntremum estimators.\nTheorem 1 (Extremum Consistency Theorem)\nIf i) (Identification) Q(θ) is uniquely minimized\nat the true parameter value θ0; ii) (Compact\nness) Θ is compact; iii) (Continuity) Q(·) is\ncontinuous; iv) (uniform convergence)\np\np\n→ 0; then θˆ → θ0.\nsupθ∈Θ Qˆ(θ) -Q(θ)\nIntuition:\nJust draw a picture that describes\nthe theorem.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProof: The proof has two steps: 1) we need\nto show Q(θˆ) →p Q(θ0) using the assumed uni\nform convergence of Qˆ to Q, and 2) we need\nto show that this implies that θˆ must be close\nto θ0 using continuity of Q and the fact that\nθ0 is the unique minimizer.\nStep 1. By uniform convergence,\np\np\nQˆ(θˆ) -Q(θˆ) → 0 and Qˆ(θ0) -Q(θ0) → 0.\nAlso, by Qˆ(θˆ) and Q(θ0) being minima,\nQ(θ0) ≤ Q(θˆ)\nand\nQˆ(θˆ) ≤ Qˆ(θ0).\nTherefore\nQ(θ0)\n≤\nQ(θˆ) = Qˆ(θˆ) + [Q(θˆ) -Qˆ(θˆ)]\n≤\nQˆ(θ0) + [Q(θˆ) -Qˆ(θˆ)]\n=\nQ(θ0) + [Qˆ(θ0) -Q(θ0) + Q(θˆ) -Qˆ(θˆ)]\n|\n{z\n},\nop(1)\nimplying that\nQ(θ0) ≤ Q(θˆ) ≤ Q(θ0) + op(1)\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\np\nIt follows that Q(θˆ) → Q(θ0).\nStep 2. By compactness of Θ and continuity\nof Q(θ), for any open subset N of Θ containing\nθ0, we have that\ninf Q(θ) > Q(θ0).\nθ /∈N\nIndeed, infθ /∈N Q(θ) = Q(θ ∗) for some θ ∗∈ Θ.\nBy identification, Q(θ ∗) > Q(θ0).\np\nBut, by Q(θˆ) → Q(θ0), we have\nQ(θˆ) < inf Q(θ)\nθ /∈N\nwith probability approaching one, and hence\nθˆ ∈N with probability approaching one.Q.E.D.\nDiscussion:\n1.\nThe first and big step in verifying consis\ntency is that the limit Q(θ) is minimized at the\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nparameter value we want to estimate.\nQ(θ)\nis always minimized at something under the\nstated conditions, but it may not be the value\nwe want. If we have verified this part, we can\nproceed to verify other conditions.\nBecause\nQ(θ) is either an average (in M-estimation) or\na transformation of an average (in GMM), we\ncan verify the uniform convergence by an ap\nplication of one of the uniform laws of large\nnumbers (ULLN).\nThe starred comments below are technical in\nnature.\n∗\n2.\nThe theorem can be easily generalized in\nseveral directions: (a) continuity of the limit\nobjective function can be replaced by the lower\nsemi-continuity, and (b) uniform convergence\ncan be replaced by, for example, the one-sided\nuniform convergence: supθ∈Θ |Qˆ(θ)-Q(θ)|- →p\n0 and |Qˆ(θ0) -Q(θ0)| →p 0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nVan der Vaart (1998) presents several general\nizations. Knight (1998) \"Epi-convergence and\nStochastic Equi-semicontinuity\" is another good\nreference.\n3. ∗ Measurability conditions are subsumed here\nin the general definition of →p (convergence in\nouter probability). Element\nXn = sup\nθ∈Θ\nQˆ(θ) -Q(θ)\nconverges in outer probability to 0, if for any\n> 0 there exists a measurable event Ω which\ncontains the event {Xn > }, not necessar\nily measurable, and Pr(Ω) → 0.\nWe also\ndenote this statement by Pr ∗(Xn > ) → 0.\nThis approach, due to Hoffman-Jorgensen, al\nlows to bypass measurability issues and focus\nmore directly on the problem. Van der Vaart\n(1998) provides a very good introduction to\nthe Hoffman-Jorgensen approach.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. Uniform Convergence and Uniform Laws\nof Large Numbers.\nHere we discuss how to verify the uniform con\nvergence of the sample objective function Qb\nto the limit objective function Q. It is impor\ntant to emphasize here that pointwise con\nvergence, that is, that for each θ, Qb(θ) →p\nQ(θ), does not imply uniform convergence\nsupθ∈Θ |Qb(θ)-Q(θ)| →p 0. It is also easy to see\nthat pointwise convergence is not sufficient for\nconsistency of argmins. (Just draw a \"moving\nspike\" picture).\nThere are two approaches:\na.\nEstablish uniform convergence results di\nrectly using a ready uniform laws of large num\nbers that rely on the fact that many objective\nfunctions are averages or functions of aver\nages.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nb.\nConvert familiar pointwise convergence in\nprobability and pointwise laws of large numbers\nto uniform convergence in probability and the\nuniform laws of large numbers.\nThis conver\nsion is achieved by the means of the stochas\ntic equi-continuity.\na. Direct approach\nOften Qˆ(θ) is equal to a sample average (M-\nestimation) or a quadratic form of a sample\naverage (GMM).\nThe following result is useful for showing con\nditions iii) and iv) of Theorem 1, because it\ngives conditions for continuity of expectations\nand uniform convergence of sample averages\nto expectations.\nLemma 1 (Uniform Law of Large Numbers)\nSuppose Qˆ(θ) = En[q(zi, θ)]. Assume that data\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nz1, ..., zn is stationary and strongly mixing and i)\nq(z, θ) is continuous at each θ with probability\none; ii) Θ is compact, and iii) E [supθ∈Θ |q(z, θ)|] <\ninf; then Q(θ) = E[q(z, θ)] is continuous on Θ\np\nand supθ∈Θ |Qˆ(θ) -Q(θ)| → 0.\nCondition i) allows for q(z, θ) to be discon\ntinuous (with probability zero). For instance,\nq(z, θ) = 1(x0θ > 0) would satisfy this condition\nat any θ where Pr(x0θ = 0) = 0.\nTheoretical Exercise. Prove the ULLN lemma. Lemmas\nthat follow below may be helpful for this.\nb. Equicontinuity Approach∗ .\nFirst of all, we require that the sample cri\nterion function converges to the limit crite\nrion function pointwise, that is for, each θ,\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nQb(θ) →p Q(θ).\nWe also require that Qb has\nequicontinuous behavior. Let\nωˆ() :=\nsup\n|Qb(θ) -Qb(θ0)|\nkθ-θ0k≤\nbe a measure of oscillation of a function over\nsmall neigborhoods (balls of radius ).\nThis\nmeasure is called the modulus of continuity.\nWe require that these oscillations vanish with\nthe size of the neigborhoods, in large samples.\nThus, we rule out \"moving spikes\" that can\nbe consistent with pointwise convergence but\ndestroy the uniform convergence.\nLemma 2 For a compact parameter space Θ,\nsuppose that the following conditions hold: i)\n(pointwise or finite-dimensional convergence)\nQb(θ) →p Q(θ) for each θ ∈ Θ .\nii) (stochastic equicontinuity) For any η > 0\nthere is > 0 and a sufficiently large n0 such\nthat for all n > n0\nPr ∗{ωˆ() > η} ≤η.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThen supθ∈Θ |Qb(θ) -Q(θ)| →p 0.\nThe last condition is a stochastic version of the\nArzella-Ascolli's equicontinuity condition. It is\nalso worth noting that this condition is equiv\nalent to the finite-dimensional approximability:\nthe function Qˆ(θ) can be approximated well\nby its values Qˆ(θj), j = 1, ..., K on a finite fixed\ngrid in large sample. The uniform convergence\nthen follows from the convergence in probabil\nity of Qˆ(θj), j = 1, ..., K to Q(θj), j = 1, ..., K.\nTheoretical Exercise. Prove Lemma 2, using the com\nment above as a hint.\nSimple sufficient conditions for stochastic equicon\ntinuity and thus uniform convergence include\nthe following.\nLemma 3 (Uniform Holder Continuity) Suppose\nQb(θ) →p Q(θ) for each θ ∈ Θ\ncompact. Then\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nb\nthe uniform convergence holds if the uniform\nHolder condition holds: for some h > 0, uni\nformly for all θ, θ0 ∈ Θ\n|Q(θ) -Qb(θ0)| ≤BTkθ -θ0kh,\nBT = Op(1).\nLemma 4 (Convexity Lemma) Suppose Qb(θ) →p\nQ(θ) for almost every θ ∈ Θ0, where Θ0 is an\nopen convex set in Rk containing compact pa\nrameter set Θ, and that Qb(θ) is convex on Θ0.\nThen the uniform convergence holds over Θ.\nThe limit Q(θ) is necessarily convex on Θ0.\nTheoretical Exercise. Prove Lemma 3 and 4. Proving\nLemma 3 is trivial, while proving Lemma 4 is not. How\never, the result is quite intuitive, since, a convex func\ntion cannot oscillate too much over small neighborhood,\nprovided this function is pointwise close to a continuous\nfunction. We can draw a picture to convince ourself of\nthis. Lemma 4 is a stochastic version of a result from\nconvex analysis. Pollard (1991) provides a nice proof of\nthis result.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nA good technical reference on the uniform convergence\nis Andrews (1994).\n4.\nConsistency of Extremum Estimators\nunder Convexity.\nIt is possible to relax the\nassumption of compactness, if something is\ndone to keep Qˆ(θ) from turning back down to\nwards its minimum, i.e. prevent ill-posedness.\nOne condition that has this effect, and has\nmany applications, is convexity (for minimiza\ntion, concavity for maximization).\nConvexity\nalso facilitates efficient computation of esti\nmators.\nTheorem 2 (Consistency with Convexity):\nIf\nQˆ(θ) is convex on an open convex set Θ ⊆ Rd\nand (i) Q(θ)\nis uniquely minimized at θ0 ∈ Θ.\np\n; (ii)\n(iii) Qˆ(θ) → Q(θ) for almost every θ ∈ Θ\np\n; then θˆ → θ0.\nTheoretical Exercise.\nProve this lemma.\nAnother\nconsequence of convexity is that the uniform\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nconvergence is replaced by pointwise conver\ngence. This is a consequence of the Convexity\nLemma.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nReferences:\nPollard, D. Asymptotics for least absolute deviation re\ngression estimators. Econometric Theory 7 (1991), no.\n2, 186-199.\nAndrews, D. W. K. Empirical process methods in econo\nmetrics.\nHandbook of econometrics, Vol.\nIV, 2247-\n2294, Handbooks in Econom., 2, North-Holland, Ams\nterdam, 1994.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/ecddf2be6f18093d17db2c76d8729f1d_lecture03.pdf",
      "content": "14.385\nNonlinear Econometrics\nLecture 3.\nTheory: Consistency Continued.\nAsymptotic Distribution of Extremum Estima-\ntors\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. MLE and M-Estimator Consistency\nHere we apply extremum consistency theorem\nto establish consistency of MLE and other M-\nestimators.\nTheorem 1 (MLE Consistency): If zi is i.i.d.\nwith pdf f(z|θ) and (i) (Identification) f(zi|θ) =\nf(zi|θ0) with positive probability, for all θ = θ0;\n(ii) (Compactness.) Θ is compact; (iii) (Con-\ntinuity) f(z|θ) is continuous at all θ with proba-\nbility one; (iv) (Dominance) E[supθ∈Θ | ln f(z|θ)\np\nˆ\n|] <\ninf; then θ →θ0.\nIn MLE we have θˆ ∈arg inf\nˆ\nθ∈Θ Q(θ), where\nQˆ(θ) = -En[ln f(zi, θ)].\nBy the LLN, the limit of the MLE objective\nfunction will be Q(θ) = -E[ln f(z|θ)]. The fol-\nlowing result shows that the identification con-\ndition in condition (i) is sufficient for E[ln f(z|θ)]\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nto be uniquely maximized at the true parame-\nter.\nLemma 1 (Information inequality): If\nZ\n| ln f(z|θ)|f(z|θ0)dz < inf\nfor each θ ∈Θ then if f(z|θ) = f(z|θ0)\nwe\nhave\nE ln[f(z|θ)] < E ln[f(z|θ0)]\nProof:\nBy the strict version of Jensen's in-\nequality and concavity of ln(v),\nZ\nln[f(z|θ)/f(z|θ0)]f(z|θ0)dz\n(1)\n< ln{\nZ\n[f(z|θ)/f(z|θ0)] f(z|θ0)dz}\n(2)\n= ln\nZ\nf(z|θ)dz = 0.\n(3)\nIt is interesting to note that identification of\nθ0, in the sense that changing the parameter\n\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nchanges the density, is sufficient condition for\nQ(θ) = E[ln f(z|θ)] to have a unique maximum\nat θ0. For other extremum problems, it is often\nharder to give such simple sufficient condition\nfor identifiability.\nProof of MLE consistency:\nIt suffices to\ncheck conditions of Extremum Consistency The-\norem. For identification condition (i) let Q(θ) =\nE[ln f(z|θ)].\nBy the information inequality, for\nθ = θ0,\nQ(θ) -Q(θ0) = E[ln{f(z|θ)/f(z|θ0)}] < 0,\ngiving identification.\nCompactness condition\n(ii) is assumed. Continuity condition (iii) and\nuniform convergence condition(iv) hold by the\nULLN of L2 applied to Qˆ(θ) = En[ln f(zi|θ)].\nQ.E.D.\nBinary Choice Continued:\nThe probit (con-\nditional) likelihood for a single observation z =\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(y, x) where y ∈{0, 1} is\nf(z|\ny\nθ) = Φ(x′\nθ) [1 -Φ(x′θ)] -y,\nwhere Φ(v) is the standard normal CDF.\nTheorem on Consistency of probit:\nIf E[xx′]\nis finite and nonsingular then the probit MLE\np\nθˆ satisfies θˆ →θ0.\nProof:\nFor the standard normal pdf φ(v), it is well\nknown that ∂ln Φ(v)/∂v = φ(v)/Φ(v) is decreasing, so\nthat ln Φ(v) is concave. Also, ln[1 -Φ(v)] = ln Φ(\nv)\nis concave.\nThen, since a concave function of a\n-\nlin-\near function is concave, ln f(z θ) = y ln Φ(x′β) + (1\ny) ln[Φ(-x′θ)] is concave.\nTher\n|\nefore, to show consis-\n-\ntency, it suffices to verify the conditions (i) - (iii) of\nTheorem on Consistency of Argmin Estimators with\nConvexity.\ni) By nonsingularity of E[xx′], for θ = θ0, E[ x′(θ\nθ ) 2] = (θ\nθ ) E[xx ](θ\nθ ) > 0, implying that\n{\nx\n-\n0 }\n-\n0 ′\n′\n-\n′(θ -\nθ0) = 0 and hence x′θ = x′θ0 with positive probability\nunder θ0.\nBoth Φ(v) and Φ(-v) are strictly monotonic,\nso that x′θ = x′θ0 implies both Φ(x′θ) = Φ(x′θ0) and\nΦ(-x′θ) = Φ(-x′θ0) Therefore,\nf(z|θ) = Φ(x′θ)yΦ(-x′θ)1-y = f(z | θ0),\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwith positive probability under θ0.\nii) The set Θ = Rp, where p is the dimension of θ, is\nconvex.\niii) By Feller inequality, there is a constant C such that\nφ(v)/Φ(v) ≤C(1 + |v|), so that by integrating there is\n|\na constant C such that ln Φ(v)| ≤C(1 + |v| ).\nThen\nE [|ln f(z|θ)|] ≤E[2C(1 + |\nx′θ| )] < infby existence of\nsecond moments of x.\nIn M-estimation we have θˆ ∈arg inf\nˆ\nθ∈Θ Q(θ),\nwith the criterion function taking a form of an\naverage\nQˆ(θ) = En[m(zi, θ)].\nThe limit criterion function Q(θ) is assumed to\nbe minimized at the true parameter value θ0.\nTheorem 2 (M-Estimator Consistency): If zi\nare i.i.d.\nor stationary and strongly mixing,\nand (i) (Identification) Em(zi, θ) > Em(zi, θ0)\nfor for all θ = θ0; (ii)\n(Compactness.)\nΘ is\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\ncompact; (iii) (Continuity) for each θ, m(z, θ)\nis continuous at θ\nwith probability one; (iv)\np\n(Dominance) E[supθ∈Θ |m(z, θ)|] < inf; then θˆ →\nθ0.\nProof: The argument is identical to the proof\nof MLE consistency, apart from verification of\nidentifiability, which we assumed here directly.\n2. Consistency of GMM\nThe consistency result for GMM is similar to\nthat for MLE. The most important difference\nis in the identification hypotheses:\nhere as-\nsume that\ng(θ) = E[g(z, θ)] = 0\nmust have a unique solution at the true pa-\nrameter value θ0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTheorem 3 (GMM Consistency): If zi are i.i.d.\nor stationary and strongly mixing (i) E[g(z, θ0)] =\np\n0 and E[g(z, θ)] = 0 for θ = θ0 and Aˆ -→A\nwith A positive definite; (ii) Θ is compact; (iii)\nfor each θ, g(z, θ) is continuous at θ with prob-\nability one; (iv)\np\nE[supθ∈Θ ∥g(z, θ)∥] < inf, then θˆ →θ0.\nIt is often hard to specify primitive conditions\nfor the identification condition (i). This con-\ndition amounts to assuming that there is a\nunique solution to the set of nonlinear equa-\ntions g(θ) = 0. Global conditions for unique-\nness are hard to specify. A simple local iden-\ntification condition is that ∂ g(θ0)/∂θ has full\ncolumn rank p. Often, one can exchange the\nexpectation and the derivative, so that this\ncondition is\nrank(E[∂g(zi, θ0)/∂θ]) = p.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProof: We proceed by verifying the hypothe-\nses of the Theorem on Consistency of Extremum\nEstimators (Theorem 2.1). Here we have\nQˆ(θ) = ˆg(θ)′Aˆˆg(θ) and Q(θ) = g(θ)′A g(θ).\nSince Q(θ) has a unique minimum of zero at θ0,\nthe identification condition (i) of Theorem 2.1\nis satisfied. Compactness condition (ii) holds\nby hypothesis.\nUniform convergence of Qˆ to Q (condition (iv))\nand the continuity of Q (condition (iii)) \"clearly\nfollow\" from (a) the uniform convergence of ˆg\nto a continuous function g, where uniform con-\nvergence and continuity follows by the ULLN,\n(b) from the convergence of Aˆ to A. □\nRemarks:\n1.\nThe details of the \"clearly follow\" step are as fol-\nlows.\nBy (iii) and (iv) of the GMM hypothesis, the\nULLN lemma gives continuity of g(θ) and supθ∈Θ ∥ˆg(θ)-\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n∥\np\ng(θ)\n-→0 Therefore Q(θ) is continuous, so that con-\ndition (iii) of Extremum Consistency Theorem 2.1 holds.\nIt follows from the triangle inequality that supθ∈Θ ˆg(θ)\nsupθ Θ ∥ˆg(θ)- g(θ)∥+ supθ Θ ∥ g(θ)∥= Op(1) Let\n∥\n∥≤\n.\nA\nbe\n∈\n∈\nthe maximum eigenvalue of\n∥∥\nA. Then\n|Qˆ(θ)\n-\nQ(θ)| ≤|ˆg(θ)′(Aˆ -A)ˆg(θ)|\n+\n|[ˆg(θ) - g(θ)]′A[ˆg(θ) - g(θ)]|\n+\n2| g(θ)′A[ˆg(θ)\n- g(θ)]|\n≤\nsup\nA\nθ Θ\n∥ˆg(θ)∥∥Aˆ -\n∥\n∈\n+\nsup\nθ∈Θ\n∥ˆg(θ) - g(θ)∥2∥A∥\np\n+\n2 sup\n(\nΘ\n∥ g θ)\nθ\n∥∥A∥sup ∥ˆg(θ) - g(θ)∥-→0.\n∈\nθ∈Θ\nTaking the supremum over Θ of the left-hand side, it\nfollows that Qˆ converges to Q uniformly.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAsymptotic Distribution of Extremum Es-\ntimators Assume conditions of Theorem 1, so\nthat\nθˆ = arg inf Qˆ(θ)\nθ∈Θ\nis consistent for\nθ0 = arg inf Q(θ).\nθ∈Θ\nIf Qˆ is smooth, then θˆ is a root of the first\norder condition:\n∇Qˆ(θˆ) = 0,\n(4)\nwhich we can expand as∗\n∇Qˆ(θ0) + ∇2Qˆ(θ∗)(θˆ-θ0) = 0,\n(5)\nand solve for the estimate\n√\nn(θˆ-θ0) = -[∇Qˆ(θ∗)]-1√n∇Qˆ(θ0).(6)\nThen we look for conditions to give as a LLN:\n∇2Qˆ(θ∗) →p J\n(7)\n∗∇2Qˆ(θ∗) stands for a Hessian matrix\n2Qˆ with each\nrow evaluated at different intermediate\n∇\npoint θ∗.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand a CLT:\n√n∇Qˆ(θ0) →d N(0, Ω),\n(8)\nso that by CMT\n√\nn(θˆ-θ0) →d J-\nN(0, Ω) = N(0, J-ΩJ-).(9)\nIn some case, like in the case of regular MLE,\nwe will have a generalized information ma-\ntrix equality:\n-J = Ω,\nleading to the simplification of the asymptotic\nvariance formula to J-1.\nLet us formalize this into a Theorem:\np\nTheorem 4 If θˆ →θ0 and i) θ0 ∈interior(Θ);\nii) Qˆ(θ) is twice continuously differentiable in a\nneighborhood N of θ0; iii) √\n→\nd\nn∇Qˆ(θ0)\nN(0, Ω);\niv)\nthere is J(θ) that is continuous at θ0 and\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\np\nsupθ∈N\n∇2Qˆ(θ) -J(θ)\n→0; v) J = J(θ\n\n0) is\nnonsingular.\nThen\n√\n→\nd\nn(θˆ-θ0)\nN(0, J-1ΩJ-1).\nProof:\nBy consistency, θˆ is in a small open\nneigborhood N of θ0 wp →1.\nCondition (ii)\nenables (4) and (5).\nCondition (iv) and the\ncontinuous mapping theorem imply (7): ∇2Qˆ(θ∗) →p\nJ. (To see this note ∇2Qˆ(θ∗)-J(θ∗) →p 0, and\nJ(θ∗) -J(θ0) →p 0 by θ∗→p θ0 and continu-\nous mapping theorem).\nThis in turn implies\nby (v) and the continuous mapping theorem:\n[∇2Qˆ(θ∗)]-1 →\np J-, which enables (6).\nBy\nSlutsky we have (9). Q.E.D.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nRemark∗(Non-Smooth Case):\nIs Qˆ is not smooth\nbut has a unique gradient\nQˆ almost everywhere (for\npoints of non-uniqueness, ch\n∇\noose any subdifferential to\nbe a gradient). Assume that we can define θˆ as a root\nof the first order condition holding approximately:\nQˆ(θˆ) = op(1/√\n∇\nn).\n(10)\nWe then impose the stochastic differentiability con-\ndition, replacing the previous Taylor expansion, namely:\n∇Qˆ(θˆ) = ∇Qˆ(θ0) + [J + op](θˆ-θ0),\n(11)\nand solve for the estimate\n√n(θˆ-θ0) = -[J + op]-1√n∇Qˆ(θ0).\n(12)\nThen as before, we require a CLT:\n√n∇Qˆ(θ0) →d N(0, Ω),\n(13)\nand conclude by CMT\n√n(θˆ-θ0) →d N(0, G-1ΩG-1).\n(14)\nStochastic differentiability is somewhat more difficult to\nestablish but is quite fruitful for quantile regression and\nrelated methods. We'll cover some methods for estab-\nlishing this condition in theoretical exercises.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/6d7b3de84c384c78b306d896d9f55d9c_lecture04.pdf",
      "content": "14.385\nNonlinear Econometrics\nLecture 4.\nTheory: Asymptotic Distribution of GMM/Nonlinear IV\nApplication:\nRevisit probits and logits.\nMultinomial\nchoice.\nTopics to be covered in TA Session:\nTesting (Parallels OLS)\nVariance Estimation (Parallels OLS)\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAsymptotic Normality of GMM and Nonlinear IV.\nRecall Idea:\nEstimate parameters by setting sample\nmoments to be close to population counterpart.\nDefinitions:\nβ\n:\np × 1 parameter vector, with true value β0.\ngi(β)\n=\ng(zi, β) : m × 1 vector of functions\nof ith data observation zi and parameter.\nModel (or moment restriction):\nE[gi(β0)] = 0.\nDefinitions:\nˆg(β) := En[gi(β)]\n:\nSample averages.\nAˆ\n:\nm × m positive definite matrix.\nLead Examples:\nIV:\ngi(β) = (Yi -Xiβ)Zi, Aˆ = Vˆar[gi(β0)]-1\nNIV:\ngi(β) = f(Yi, Xi, β)Zi, Aˆ = Vˆar[gi(β0)]-1\nMLE:\ngi(β) = ∇ln f(Zi, β), Aˆ = I\nM:\ngi(β) = ∇m(Zi, β), Aˆ = I.\nGMM ESTIMATOR:\nβˆ = arg min ˆg(β)′Aˆˆg(β).\nβ\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThis is a special case of extremum estimator, so the\narguments of the previous type can be applied to get\nthe following result.\nASYMPTOTIC NORMALITY OF GMM: If the data\nare i.i.d. or stationary strongly mixing with rate greater\nthan\nβˆ\np\ntwo,\n→β0 and i) β0 is in the interior of the pa-\nrameter set over which minimization occurs; ii) gi(β)\nis continuously differentiable on a neighborhood N of\np\nβ0;\niii) E[supβ\n∥∇gi(β)∥] is finite; iv) Aˆ →A positive\n∈N\ndefinite\nand G′AG is nonsingular, for G = E[\ngi(β0)];\nv)\nfor i.i.d. data, Ω= E[g\n∇\ni(β0)gi(β0)′] is finite and for\nmixing data Ω= limn V ar[ˆg(θ0)] exists and is finite, then\n√\n→\nd\nn\n(βˆ -β0)\nN(0, V ),\nV\n=\n(\nG′AG)-G′AΩAG(G′AG)-.\nProof:\nFor Gˆ = ∇ˆg(βˆ), we have the FOC,\n0 = Gˆ′Aˆˆg(βˆ).\nWe can expand them as\n0 = Gˆ′Aˆ{ˆg(β0) + ∇ˆg(β )[βˆ -β0]},\nwhere ∇ˆg(β ) stands for the matrix whose each row eval-\nuated at (a row-dependent) β located on the line joining\nθ0 and θˆ, and solve for\n√\nn(βˆ -β0) = -[Gˆ′Aˆ∇ˆg(β )]-Gˆ′Aˆ√nˆg(β0)\nBy the ULLN and the CMT, we have that\n-[Gˆ′Aˆ∇\np\nˆg(\nβ)]-Gˆ′Aˆ →-(G′AG)-1G′A.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nApplication of the CLT gives us\n√\n→\nd\nnˆg(β0)\nN(0, Ω).\nApplying Slutsky, we get\n√\n→\nd\nn(βˆ -β0)\n-(G′AG)-1G′A · N(0, Ω).\nNotes: (1) Aˆ affects V only through plim(Aˆ).\n(2) If m = p, then\n=\nG-1\nV\nΩG-′,\nand A drops out. Thus, the choice of the matrix A has\nno effect on asymptotic variance in this case. We have\nm = p for MLE, M-estimators, and \"exactly identified\"\nGMM. We have that m > p for \"overidentified\" GMM.\n(3) The optimal choice of A is given by A ∝Ω-1, in\nwhich case\n= (\n′Ω-1\n)-1\nV\nG\nG\n.\n(4) MLE. Assume i.i.d. data. For MLE, under correct\nspecification:\ng(z, θ) = ∇ln f(\nz, θ), G = E∇ln f(z, θ),\nΩ= var[∇ln f(z, θ0)], and -G = Ω\nso that\nV = -G-1 = Ω-1.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe fact that G = Ωis known as information matrix\nequality, which holds under some regularity conditions.\n(5) M-estimators, including MLE under incorrect\nspecification. We have\ng(z, θ) = ∇m(z, θ), G = E∇2m(z, θ0),\nΩ= Var[∇m(z, θ0)],\nso that\nV = G-1ΩG-1.\nThis is known as Huber's sandwich formula or, more\nsimply, robust variance-covariance matrix.\n(6) Linear IV. Illustrate plausibility of conditions with\nLinear IV:\ninterior parameter condition (i) holds by assumption;\ncontinuous differentiability (ii) holds by linearity of\ngi(β) = Zi(yi -Xi\n′β)\nin β; dominance condition (iii) holds as long as second\nmoments exist, by\n∇gi(β) = -ZiXi\n′\nG = -EZiXi\n′\n; (iv) holds as long as A is nonsingular and G =\nE[ZiXi\n′]\nhas full column rank; and (v) holds as long as\n-\nΩ= E(Yi -Xi\n′β0)2ZiZi\n′\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nis finite. Then, with A ∝Ω-1,\nV = (G′Ω-1G)-1.\n(7) As stated in the theorem, i.i.d.\nsampling can be\nreplaced by strict stationarity and strong mixing with\nrate larger than 2, that is provided mixing coefficients\nα(j) go to zero at rate j-α as j →inf, for α > 2, in which\ncase the limit variance takes the form\nΩ= lim V ar[√\nn→inf\nnˆg(θ0)].\n(8) However, in many cases we can use the CLT for mar-\ntingale difference sequences, which is much more rele-\nvant in dynamic economic applications. For instance, in\nHansen and Singleton gi(β0) being a martingale differ-\nence sequence is implied by economic assumptions. In\nthis case, the limit variance is the same as in the i.i.d.\ncase:\nΩ= V ar[gi(θ0)] = E[gi(β0)gi(β0)′].\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nRevisit Maximum Likelihood Estimation for Binary\nChoice:\nln fi(β) = yi log F(x′\niβ) + (1\n-yi) log(1 -F(x′\niβ))\ny\n∇ln fi(β) =\ni -F(x′\niβ)\nF(x′\niβ) · (1 -F(x′\niβ))\nCLT for the score\n\nf(x′\niβ)xi\n√\nf 2\nnEn∇ln fi(β0) →d N(0, E\ni\nx\nFi(1 -\nix′\nFi)\ni)\nsince V ar(yi -Fi|xi) = Fi(1 -Fi). Therefore:\n√\nf 2\nn(βˆ -β) →d N(0, [E\ni\n′\nx\ni(1 -\nix\nFi)\ni]-)\nF\nThese variance formula is valid only under correct spec-\nification. Under incorrect specification, treat as an M-\nestimator, and use the Huber's robust sandwich formula.\nProbit and Logit Examples:\nFor probit: √\n-\nd\nn(βˆ -β)\n→N(0\nφ2\n, E\ni\nx\nΦi(1-Φi)\nix′\ni]-1).\nFor logit:\n√\nn(βˆ -β) →d N(0, [EΛ(x′\niβ)(1 -Λ(x′\niβ))xix′\ni]-).\nYou can also do NLS by minimizing\nβ = arg min\nEn(yi -F(x′\niβ))\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nby using the relations:\nyi = F(x′\niβ) + εi, E(εi|xi) = 0, V (εi|xi) = F(x′\niβ)(1 -F(x′\niβ)).\nThe NLS β is not efficient because of heteroscedasticity,\nso you can do GLS:\nβˆ = arg min En\nh\n(\ny\n′\n-\n′\ni\nF\nF(xiβ)(1\n(xiβ))\n-\n(x′\niβ))\nF\ni\n,\nwhich attains the same efficiency as MLE.\nThe usual choice for probit and logit is MLE due to com-\nputational reasons:\nthe least squares problem is non-\nconvex, whereas log-likelihood for probits and logits is\nconcave.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMultinomial Choice Models\nThe multinomial choice or qualitative response models\nprovide a framework for analysis of choice among finite\nsets of alternatives, with each alternative characterized\nas a bundle of attributes.\nWe consider the following example that captures the\nbasics of these models.\nConsider predicting urban de-\nmand for various modes of transportation. The utility\nof a commuter from taking choices car, train, and bus\nare given by:\n1. Bus(y = 1): U1i = μ1i + ε1i\n2. Train(y = 2): U2i = μ2i + ε2i\n3. Car(y = 3): U3i = μ3i + ε3i\nHere μki characterizes the systematic part of utility, called\nthe \"mean utility,\" attributable to observed characteris-\ntics xi of modes of transportation and of the commuter,\nand εki is an unobserved disturbance. Often we use\nμki = x′\niβk\nor more general functional forms. In what follows, we\nuse Pi(yi = 1) to denote P(yi = 1|xi).\nThen\nPi(yi = 1) = Pi(U1i ≥U2i, U1i ≥U3i),\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nPi(yi = 2) = Pi(U2i\nU1i, U3i\nU1i), and Pi(yi = 3) =\nPi(\n≥\n≥\nU3i\nthe lik\n≥U1i, U3i\nU2i). This allows use to write down\nelihood function,\n≥\nusing the fact that the con-\nditional log-likelihood function of a single observation\n(yi, xi) is\nX\n1{yi = k} ln Pi(yi = k)\nk\nTheory: The usual MLE analysis applies to get CAN\nand efficient estimators, subject to regularity conditions.\nWe should use M-estimation approach to account for\npossible misspecification.\nMultinomial Logit Model: This model postulates that\neμik\nPi(yi = k) =\nhas\nP\n.\nμ\nk e\nik\nMcFadden\nshown that this model can be derived\nfrom optimizing behavior: Suppose disturbances ε1i, ε2i,\nε3i are i.i.d. and each one distributed as Type-I extreme\nValue, with Gumbel distribution function,\nF(ε) = e-e-ε\nthat has density f(ε) = e-εe-e-ε. Then the choice prob-\nabilities are described by the formula above.\nThe as-\nsumption on the errors is also a necessary condition for\nthe formula above to hold.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nWe illustrate the calculation using the travel example:\nPi(yZi = 2) = Pi(ε1 + μ1 ≤ε2 + μ2, ε3 + μ3 ≤ε2 + μ2)\ninf\n=\nf(ε2)Pi(ε1 ≤ε2 + μ2 -μ1|ε2)Pi(ε3 ≤ε2 + μ2 -μ3|ε2)dε2\n-inf\n=\nZ inf\ne-ε [e-e-ε2[1+eμ1-μ2+eμ3-μ2\n]]dε2\n-inf\n=\nZ inf\n-[1+eμ1-μ2+\n]\ne\neμ3-μ2 xdx\n[ using x = e-ε2]\n= 1 + eμ1-μ2 + eμ3-μ2\neμ2\n=\n.\neμ1 + eμ2 + eμ3\nSimilarly, Pi(yi = 1) =\neμ1\n,\neμ1+eμ2+eμ3\nand Pi(yi = 3) =\neμ1\n.\neμ1+eμ2+eμ3\nIt is also easy to get that\neμ1\nPi(y = 1|y = 1 or y = 2) = eμ1 + eμ2\neμ3\nPi(y = 3|y = 1 or y = 3) = eμ1 + eμ3\nand so on.\nIIA property: Relative choice probabilities depend only\non pairwise comparisons. This is the independence from\nthe \"irrelevant\" alternatives.\nE.g., in the last equa-\ntion, the relative frequency of choosing between 1(bus)\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand 3(car) does not depend on characteristics of choice\n2(train). This might be unreasonable since bus(1) and\ntrain(2) are both public transportation and the unob-\nservable components of utility from them might be cor-\nrelated. (A more extreme example would be to look at\nbuses painted in different colors.) Another way to look\nat this is as follows:\nSuppose y = 2 is not available,\nthen\nPi(y = 3 without choice 2) = Pi(U3 ≥U1)\neμ3\n= eμ1 + eμ3\nCompare this to the probability of choosing y = 3,\nhaving chosen either 1 or 3,\neμ3\nPi(y = 3|y = 1 or y = 3) =\n,\neμ1 + eμ3\nwhich is the same as above.\nHere, the relative fre-\nquency of choosing between 1(bus) and 3(car) does not\ndepend whether the choice 2(train) is present. Again,\nthis might be unreasonable since bus(1) and train(2)\nare both public transportation and utilities from them\nmight be correlated.\nDiscussion:\nThe logit model is highly tractable and\ncomputable due to the concave log-likelihood function.\nMoreover, it is derivable from the optimizing choice be-\nhavior. The drawback of this model is the IIA property.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nNested Logit: To allow for correlation between choice\n1 and 2, introduce some dependence between ε1 and ε2\nas follows:\nF(ε1, ε2, ε3) = G(ε1, ε\ne-ε3\n2)F(ε3) = G(ε1, ε2)e-\nand\nG(ε , ε ) = e-[e-ε1/ρ+e-ε2/ρ]ρ\n,\nfor 0 ≤ρ ≤1. This is a bivariate type I extreme-value\n(Gumbel) distribution. When ρ = 1 we are back to the\nlogit case, since then G(ε1, ε2) = F(ε1)\nF(ε2).\nWhen\nρ = 0,\n·\nε1 and ε2 become perfectly correlated.\nThen\nPi(yi = 1|yi = 3) = Pi(U1 ≥U2|U1 ≥U3 or U2 ≥U3)\neμ1/ρ\n=\n,\neμ1/ρ + eμ2/ρ\nusing calculations similar to the previous ones. Also,\neμ3\nPi(y = 3) = eμ3 + [eμ1/ρ + eμ2/ρ]ρ\nThese two quantities enable us to fully specify the like-\nlihood. The IIA property no longer holds.\nDiscussion:\nNested logit allows us to avoid some of\nthe limitations of logit.\nIt also preserves the logit's\ncomputational tractability.\nThere are further general-\nizations, in particular, McFadden's generalized extreme\nvalue model, of which nested logit is a special case.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(Lecture 4 contd.)\nMultinomial Probit: Here we specify,\nεi = (εki, k = 1, .., K) ∼N(0, Σ).\nThis allows for very flexible correlation structure, but\nthe choice probabilities\nPi(y = j|xi) = Pi[Uj ≥Uk, for all k]\nare usually not available in closed form, so one has to\nevaluate choice probabilities numerically, using Monte-\nCarlo approach. In the MC approach, we simulate many\ndraws of\n1 2\nε =\n/\ns\nΣ\nZs, Zs ∼iid N(0, I), s = 1, ..., S,\nthen evaluate\nUks = x′\niβk + εs, s = 1, ..., S,\nand approximate\nPi(y = j|xi) ≈\nS\n1 U\ns=1\n{\njs ≥Uks, for all k\nS\n}.\nThe draws of {Zs, s = 1\nX\n, ..., S} are generated only once\nand reused in the evaluation of Pi(y = j|xi) at different\nparameter values. Note that in the formula above, the\nutility terms Uk depend on parameters β and Σ.\nThe\napproach was pioneered by McFadden.\nNotes:\nThere are many useful extensions, including\ngeneralized extreme value models and sequential\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nchoice models (that exploit the sequentiality of de-\ncisions).\nBoth of these approaches help facilitate the\nimplementation.\nThe qualitative response models and their derivation\nfrom the optimizing behavior were largely pioneered by\nMcFadden.\nMcFadden also introduced the simulated\nlikelihood methods to econometrics, which was a major\ninnovation. Others who introduced simulated likelihood\nin statistics and econometrics include Cencov\n\n(60s),\nGeyer, and Polard and Pakes.\nReferences:\nAmemiya, Advanced Econometrics, Chapter 9, and oth-\ners on the reading list.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/b26013900fb9492b496e6beb6a60ed2e_lecture05.pdf",
      "content": "14.385\nNonlinear Econometrics\nLecture 5.\nTheory: Two-step estimators. Efficiency. One-step es-\ntimators.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nDeriving Asymptotic Variances for Two-Step Esti-\nmators:\nThis is really easy if one considers any estimator in a\nGMM framework. Consider a two step estimator βˆ based\non the moment equation:\nE[g(zi, β0, γ0)] = 0.\nThe preliminary estimate ˆγ of γ0 is based on the moment\nequation:\nE[h(zi, γ0)] = 0.\nA formula for the asymptotic variance of such a two\nstep estimator can be derived by noting that θˆ = (βˆ′, ˆγ′)′\nis a joint GMM estimator with\ng(z, β, γ)\ng(z, θ) =\nμ\nh(z, γ)\nand applying the general GMM fo\n¶\nrmula for its mo-\nment function with the block-diagonal weighting matrix.\nAdaptive case. This is when the preliminary estimation\nof ˆγ has no first-order impact on asymptotic variance of\nβˆ. That is, first order variance of βˆ is the same as if we\nknew the true value γ0. It is often said in such situations\nthat βˆ is oracle-efficient.\nAdaptivity occurs when\nGγ = ∇γE[g(zi, β0, γ0)] = 0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThis means that the small changes in γ0 have negligible\nfirst order impact on the root β0 of the equation. Indeed,\nby the implicit function theorem, we have that\n∇\n(\n) =\n-1\nγβ γ0\nG\nGγ\nβ\nwhere\nGβ = ∇βE[g(zi, β0, γ0)].\nA more extreme case, a very important one, is when\neven big changes do not affect the root β0. In MLE, as\nthe special case, the adaptive estimation of β is possible\nwhen the information matrix\nJ = -\nE[∇ln f(z, θ0)]\nis block-diagonal with respect to β and γ.\nExample 1 (Adaptive): Consider the population FOC\nfor the log-likelihood in the classical regression model:\n(yi\nxiβ)xi\nE[m(z, β, σ )] = E[\n-\n] = 0.\nσ2\nHere,\n(y\nGσ2 = E[\ni -xiβ0)xi] = 0,\nσ4\nso consistent estimation of σ2, or even inconsistent es-\ntimation of σ2 has no impact on variance of βˆ, the least\nsquares. A similar situation occurs for the generalized\nleast squares under correct specification, where prelimi-\nnary estimation of weights has no first order impact on\nthe asymptotic variance.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nExample 2 (Non-adaptive): In PS2, you will work\nthrough the two-step estimation of the selection model,\nin which\nφ(x′γ)\nE[yi|xi] = x′\niβ + α · λ(x′\niγ),\nλ(x′\niγ) =\ni\n,\nΦ(x′\niγ)\nwhich is the conditional expectation of a response vari-\nable selected on the basis of whether or not\nyi0 = x′\niγ + εi ≥0.\nHeckit: A convenient way to estimate this model is\n1. to run a probit regression of the observed di =\n1(yi0\nsor\n≥0) on x′\niγ, obtaining an estimated regres-\nλ(x′\niˆγ),\n2. to run ordinary least squares of yi on xi and λ(x′\niˆγ).\nThis example is clearly non-adaptive in general. Indeed,\nhere we have\ng(zi, β, α, γ) = di[xi, λ(x′\niγ)]′[yi -x′\niβ -α′λ(x′\niγ)]\nand\n[d\nm(zi, ) =\ni -Φ(x′\niγ)]\nγ\n;\n[Φ(x′\niγ)(1 -Φ(x′\niγ)]\nand generally\nGγ = ∇γE[g(zi, β0, α0, γ0)] = 0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe two-step estimator is computationally convenient,\nbut is less efficient than the maximum likelihood estima-\ntor. However, starting with an initial, computationally\nconvenient estimator, one can always gain efficiency by\ndoing a \"one-step\" from an initial estimate, using a\nlikelihood or other efficient criterion function.\nOutline of the adaptivity argument: The two-step\nestimator satisfies\nˆg(βˆ, ˆγ) := En[g(zi, βˆ, ˆγ)] = 0.\nExpand for G β = ∇βˆg(β , γ) and G γ = ∇γˆg(β , γ), and\nˆg(βˆ, ˆγ) = ˆg(β0, γ0) + G β(βˆ -β0) + G γ(ˆγ -γ0)\nand solve\n√\nn(βˆ -β0) = G -(ˆg(β0, γ0) + G γ\nβ\n√\n|{z}\n?\n| n(ˆγ -γ0)\n{z\n)\nOp(1)\nIf G\n}\nγ = 0, then G γ\np 0 by ULLN under regularity con-\nditions, and prelimina\n→\nry estimation has no effect on vari-\nance of βˆ.\nIf Gγ = 0, then G γ →p Gγ under regularity conditions,\nand preliminary estimation has a non-trivial effect on\nvariance of βˆ.\nIn such a case, simply treat the two-\nstep estimator in GMM framework and use appropriate\nformulas for this case.\nReference: e.g. Newey and McFadden pp 2150-2152.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAsymptotic Efficiency:\nHere we emphasize two key results:\n1) Asymptotic variance minimizing choice of any Aˆ is\nAˆ →\np Ω-1.\n2) MLE is the efficient estimator in the class of GMM\nestimators, i.e. that ∇ln f(z\nmoment\n|θ) is the optimal choice of\nfunctions g(z, θ).\nEFFICIENT DISTANCE MATRIX. Note that if A =\nΩ-1 the asymptotic variance reduces to (G′Ω-1G)-1 .We\nhave that if G′AG and Ωare nonsingular then\n(\n′\n)-1\nG AG\nG′AΩAG(G′AG)--(\nG′Ω-G)-≥0\nThus, the asymptotic variance of the GMM estimator is\nminimized when A = Ω-1\nProof:\nEfficiency result is actually implied by Gauss-\nMarkov theorem for the linear model. Consider the clas-\nsical normal linear regression model with m observations:\nY(m×1) = Gβ + ε, ε|G ∼N(0, Ω),\nso that\nE[Y |G] = Gβ,\nV ar(Y |G) = Ω.\nConsider the GLS estimator\n(βˆ -β) = (G′Ω-1G)-1GΩ-1ε\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand the WLS\n(β -β) = (G′AG)-1G′Aε.\nGLS has the variance matrix\n(G′Ω-1G)-1\nand WLS has the variance matrix\n(\nG′\nAG)-G′AΩAG(G′AG)-.\nBy Gauss-Markov Theorem, the former matrix is smaller\nthan the latter. □\nThus, in large samples, the GMM estimator is equiva-\nlent in distribution to WLS for the normal regression\nmodel. The optimally weighted GMM is equal in distri-\nbution to the GLS that uses optimal weights.\nRemarks.\n1) The above proof is in the spirit of Le\nCam's limits of experiments.\nIn large samples, the\nproblem of choice of weight matrix is like the choice of\nthe weight matrix in the \"experiment\" with m-observation\nnormal regression model with design matrix G and dis-\nturbances having general variance matrix Ω.\n2) A more direct proof without \"tricks\" can be done as\nfollows.\nDefs\n:\nL′L = Ω, H = (L′)-1G, F = (G′AG)-1G′AL′.\nNote\n:\nH′H = G′Ω-1G, FH = I.\nProof\n:\n(\nG′AG)-G′AΩA′G(G′AG)-1 -(G′Ω-1G)-1\n=\nFF ′ -(H′H)-= FF ′ -FH(H′H)-H′F ′\n=\nF(I -H(H′H)-1H′)F ′ ≥0.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nNotes: All that is needed for efficiency is that the limit\nof Aˆ is Ω-1.\nThus, any consistent estimator of Ω-1\nleads to an asymptotically efficient GMM estimator.\nMLE is optimal GMM.\nThis follows from MLE attaining the Cramer-Rao lower\nvariance bound and from asymptotic unbiasedness of\nMLE. (In fact, MLE is efficient among all estimators.)\nThe asymptotic efficiency of MLE among GMM is based\non a generalized information matrix equality. The mo-\nment conditions being satisfied for all possible θ means\nthat\nZ\ng(z, θ)f(z|θ)dz = 0\nis an identity in θ. Assuming that differentiation under\nthe integral is allowed, we can differentiate this identity\nto obtain,\n=\nZ\n[∇g(z, θ0)]f(z|θ0)dz\n+\nZ\ng(z, θ0)[∇ln f(z|θ0)]f(z|θ0)dz\n=\nG + E[gs′], G = E[∇g(z, θ0)],\nwhere\ng = g(z, θ0)\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand\ns = ∇ln f(z|θ0) = ∇f(z|θ0)/f(z|θ0).\nThat is, we have the generalized information equality\nG = -E[gs′].\nIf we set\ng(z, θ0) = ∇ln f(z, θ0),\nwe get the usual information matrix equality:\n= -\n[∇2\nJ\nE\nln f(z, θ0)] = E[∇\n| ln f(z, θ0)\n{z\n∇\ns\n} | ln f(z, θ0)′\n{z\nva\n}],\ns′\nthat is, the information matrix is equal to the\nriance\nof the score.\nTheorem: (MLE is optimal GMM): If G+E[gs′] = 0,\nE[ss′] = J is nonsingular, and G′Ω-1G is nonsingular\nthen\n(G′Ω-1G)-1 -J-1 ≥0.\nProof:\nConsider the moment function h = -G′Ω-1g.\nThen by G = -E[gs′] we have\nE[hh′] = G′Ω-1G = -G′Ω-1E[gs′] = E[hs′],\nthat is the variance of moment function h is equal to its\ncovariance with the score s. This means that the score\ns equals h plus some noise, i.e., the score s has bigger\nvariance than h. Indeed, the variance matrix of h -s is\nE[hh′] -2E[hs′] + E[ss′] = E[ss′] -E[hh′] ≥0,\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwhich says that J -(G′Ω-1G) ≥0. This in turn implies\nthe result. □\nComment: Maximum likelihood is also asymptotically\nefficient more generally, not just in GMM class; MLE is\nefficient in class of estimators satisfying certain regular-\nity conditions.\nComment: We can differentiate\nk(z, θ)dz in θ under\nthe integral sign, if e.g., first, ∇θk(z\nfor each z, and, second,\nsupθ\nθ\nR\n, θ) is continuous in θ\nk(z, θ) dz <\n. This\nallows us to apply the dominated\nR\n∥∇\nconvergence\n∥\ntheo\ninf\nrem,\nand conclude that\n∇θ\nZ\nk(z, θ)dz =\nZ\n∇θk(z, θ)dz.\nTheoretical exercise: verify this.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nIteration and \"One Step\" Estimation:\nThis material is important for\n- numerical work,\n- obtaining efficient estimators,\n- and bootstrap.\nIn numerical computations, starting from\nan initial estimate θ ,\nthere are two common ways of iteration to obtain the\nnext step estimate θ .\n1. Newton-Raphson Step. Minimize the quadratic ap-\nproximation for Qˆ(θ):\nQˆ(θ) ≈Qˆ(θ ) + ∇Qˆ(θ )′(θ -θ ) +\n(θ -θ )′∇2Qˆ(θ )(θ\n-θ ).\nminimize RHS and get FOC\n=⇒∇Qˆ(θ ) + ∇2Qˆ(θ )(θ -θ ) = 0\nsolve the FOC\n=⇒θ = θ -[∇2Qˆ(θ ]-1∇Qˆ(θ )\n(Draw picture to see how it works.)\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2.\nGauss-Newton.\nUse an approximation obtained by\nlinear approximation for the first-order condition, e.g.\nGMM:\nQˆ(θ) ≈(ˆg(θ ) + G (θ -θ ))′A(ˆg(θ ) + G (θ -θ ))′\nthe approximate first order condition\n=⇒G ′A[ˆg(θ ) + G (θ -θ )] = 0.\nsolve first order condition\n=⇒θ =\nθ -(G ′AG )-GA\nˆg(θ )\n\"Theorem\": Under regularity conditions, if the initial\nguess is a √n consistent estimate, i.e.\n(θ -θ0) = Op(√),\nn\nthen\n√n(θ -θ0) = √n(θˆ-θ0) + op(1),\nfor\nθˆ = arg min Qˆ(θ).\nθ\nThus, under regularity conditions, \"one-step\" estima-\ntors are equivalent to the extremum estimator up to\nthe first order. Further iterations do not increase (first-\norder) asymptotic efficiency.\nRemark: you can supply your own regularity conditions\nand details for the proof to formalize this.\n\"Proof\": The proof can be outlined as follows:\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\na) For Newton-Raphson:\n√n(θ -θ0)\n= √\nn(θ -θ0) -∇Qˆ(θ )-1√n∇Qˆ(θ )\n= √\nn(θ -θ0) -∇Qˆ(θ )-1[√\nn∇Qˆ(θ0) + ∇Qˆ(θ∗)√n(θ -θ0)]\n= (|I -∇2Qˆ(θ )-1∇2Qˆ(θ∗))\n{z\n√\nop(1)\n} | n(θ -θ0)\n{z\n√\n-\nOp(1)\n}\n∇2Qˆ(θ )-1\n|\nn∇Qˆ(θ0)\n{z\n√\n}\nn(θˆ-θ0)+op(1)\n= op(1) + √n(θˆ-θ0)\nb) For Gauss-Newton:\n√n(θ -θ0) = √\nn(θ -θ0) -(G ′AG )-GA\n\n√nˆg(θ )\n= √\nn(θ -θ0) -(G ′AG )-G ′A√n[ˆg(θ0) + G∗(θ -θ0)]\n= (|\nI -(G ′AG )-G ′AG∗)\n{z\n√\nop(1)\n} | n(θ -θ0)\n{z\n√\n-\nOp(1)\n}\n(G ′AG )-1G ′A\n|\nnˆg(θ0)\n{z\n√\n}\nn(θˆ-θ0)+op(1)\n= op(1) + √n(θˆ-θ0).\n□\nNow you can see why \"one-step\" estimators and their\nproperties are very important for\n- numerical work (the one-step theorem is a posi-\ntive result on the possibility of obtaining statistically\nwell-behaved estimates),\n- obtaining efficient estimators (we can simply do\n\"one-steps\" on efficient criterion functions starting\nwith computationally convenient initial estimates),\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n- and bootstrap (in bootstrap samples, instead of\nrecomputing the extremum estimate, we can do\n\"one-steps\" from the initial sample extremum esti-\nmate).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-385-nonlinear-econometric-analysis-fall-2007/d6bfdd254deebcaeae7a3470eac8953c_lecture06.pdf",
      "content": "14.385\nNonlinear Econometrics\nLecture 6.\nTheory: Bootstrap and Finite Sample Inference\nReference: Horowitz, Bootstrap.\nReview 381 notes for finite-sample inference in the re-\ngression model with non-normal errors.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSetup:\n{Xi, i ≤n} is data.\nF0 ∈F is DGP.\nStatistical model of DGP:\nF = {F(x, θ), θ ∈Θ} (parametric model).\nF = {F is a cdf } (nonparametric model).\nStatistic of interest :\nTn = Tn(X1, ..., Xn).\nExample. Tn = tj a t-statistic based on βˆj in linear reg.\nGn(t, F0) = PF0(Tn ≤t) exact df.\nGn(t, F) = PF(Tn ≤t) exact df under F.\nWe want to estimate Gn(t, F0) in order to\n- use α- quantiles of Gn(t, F0), denoted\nG-\nn (α, F0) = inf{t : Gn(t, F0) ≥α},\nfor confidence regions and hypothesis testing,\n- use the p-values\n1 -Gn(t, F0)|t=Tn,\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nfor testing, when a large value of Tn suggests a rejection.\nThe asymptotic df under F0 is\nG (\ninft, F0) = limn Gn(t, F0).\nand asymptotic df under F is\nG (\ninft, F) = limn Gn(t, F).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAsymptotic Estimation Principle:\nEstimate Gn(·, F0) using G (\ninf·, F0).\nThis is the usual principle we use.\nBootstrap Estimation Principle:\nEstimate Gn(·, F0) using Gn(·, Fˆ).\nTwo choices for Fˆ:\n(i) Fn(x) =\nn\nn\nP\ni=1 1{Xi ≤x} ⇒nonparametric boot-\nstrap\n(ii) Fˆ0(x) = F(x, θˆ) ⇒parametric bootstrap.\nThere are also hybrid versions.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMonte-Carlo Algorithm for Tabulation of Gn(t, Fˆ).\n1. For j = 1, ..., B generate a bootstrap sample of size\nn,\n{Xij\n∗, i = 1, ..., n},\nby sampling from Fˆ randomly. If Fˆ is an empirical df,\nsample estimation data randomly with replacement.\n2. Compute\nTnj\n∗= Tn(X1\n∗\nj, ..., Xnj\n∗),\nj ≤B.\n3.\nUse the sample {Tnj\n∗, j ≤B} to compute the em-\npirical probability of the event\nTn\n∗\nt . For estimates\nof quantiles, simply use the empirical\n{\n≤}\nquantiles of the\nsample {Tnj\n∗, j ≤B}.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFinite-Sample Principle:\n1. Estimate Gn(·, F0) using Gn(·, F0) if know F0. This is\ngenerally infeasible, but this is the idea.\n2. In some lucky cases, exact distribution of the statistic\ndoes not depend on F:\nGn(t, F0) = Gn(t, F)\n∀F ∈F.\nIf this holds, then the statistic Tn is said to be pivotal\nrelative to the statistical model F. Thus, under pivotal-\nity, we can use any DGP F ∈F to tabulate Gn(t, F0).\nExample 1.\nThe t-statistic in Normal Gauss Markov\nModel with normal disturbances, FGMN, follows a t-\ndistribution\nTn = tj ∼t(n -K).\nthat does not depend on DGP F within this model.\nThat is, the distribution does not depend on the un-\nknown parameters such as the regression coefficient β\nand variance of disturbances σ2.\nIts distribution Gn(t, F0) was tabulated by Student.\nExample 2.\nThe t-statistic in Gauss Markov Model\nwith t-disturbances with known degrees of freedom θ\n(denoted by FGMt(θ)) is pivotal, as we argued in 14.381.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThat is, the distribution does not depend on the un-\nknown parameters such as the regression coefficient β\nand variance of disturbances σ2.\nDenote this statistical model by FGMt(θ).\nIn 381, we tabulated its distribution Gn(t, F0) using Monte-\nCarlo.\nRecall details: in 381 we did this for n = 6 motivated\nby Temin's data-set on Roman wheat prices.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3.\n(Advanced Finite-Sample Inference) If do not\nknow F0, put bounds on Gn(t, F0) using\nBn = [ inf\nb\nGn(t, F),\nsup Gn(t, F)].\n(1)\nF∈F\nF∈F\nUse them to bound the p-values and quantiles of Gn(t, F0).\nExample 3. Recall Example 2, but\nb\nnow take t-statistic\nin classical linear regression model with t-disturbance\nwith unknown degrees of freedom θ.\nRecall that in\nTemin's exercise, we have constructed F by varying θ\nover a set of \"reasonable values\" for degrees of freedom\nprovided by an expert:\nF = {FGMt(θ), θ ∈{4, 8, 30}}.\nWe then set Fb = F, tabulate p-values and quantiles of\nour statistic under each F\nleast\n∈Fb, and then we take the\nfavorable estimates.\nChoice of\nthat\nFb. Ideally, we want to choose the set F such\nGn(t, F0) ∈Bn.\nb\nFor this purpose, it suffices but is not necessary that\nProb[F0 ∈F] →1.\nAlso we want F\nb\nb to \"converge\" to F0 in the sense that\nd(Bn, Gn(t, F0)) →p 0,\ni.e. the distance between Bn and Gn(t, F0) goes to zero.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThis allows us to conduct asymptotically efficient infer-\nences, while preserving finite sample validity.\nRemark∗: The convergence to a singleton can fail when\nF 7→Gn(t, F) is not continuous in F at F = F0.\nIn parametric cases it suffices to set\nFb = {F(·, θ) : θ ∈CI1-βn(θ0)},\nβn →0,\nwhere CI1\nβn(θ0) could be constructed by the usual means.\n-\nComputation: Computation of the conservative bound\nBn may seem like a laborious task. However, its success\ndepends on finding at least one θ′ that yields inferences\nthat are more conservative than using θ0.\nComputational Idea (Perturbed Bootstrap):\n1. Start with θ1 = θˆ ∈CI1-βn(θ0). Tabulate Gn(α, F( , θ1)).\nThat is, the first step is just the bootstrap.\n·\n2. Draw a nearby value θ2 = θˆ+ η\nCI1-βn(θ0), where\nη is some random perturbation. Tabulate\n∈\nGn(α, F(·, θ2)).\n3. Repeat step 2 until some clear stopping criterion is\nreached.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. For purposes of inference take the least favorable p-\nor critical value generated in this way.\n5. Save your code and the Monte-Carlo seed for repli-\ncability.\nWe are thus guaranteed to do at least as well as in para-\nmetric bootstrap.\nApplicability.\nThis finite-sample method is applicable\nunder various non-standard conditions, including\n- small sample sizes,\n- partially identified models,\n- non-regular models,\n- moment inequalities.\nThe method is increasingly becoming more feasible with\nbetter computing.\nExcellent Example: Oleg Rytchkov's Dissertation (Sloan\nPh.D. 2007).\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(Technical.∗Proceed with care.) In non-parametric\ncases we can set\nF = {F : F ∈CI1-βn(F0)},\nβn →0,\nwhere the confidence regions CI1-βn(F0) collects all cdfs\nF such that\nb\nF(t) ∈[Fn(t) -cn(1 -βn), Fn(t) + cn(1 -βn)],\nwhere\ncn(1 -βn) is 1 -βn quantile of the limit distribution of\nKolmogorov-Smirnov Statistic\n√n sup\nt\n|Fn(t) -F0(t)|.\nThis limit distribution is given by the distribution of ran-\ndom variable\nsup B(t) ,\nt\n|\n|\nwhere B(·) is a F0-Brownian bridge that describes the\nlimit distribution of the random map\n√n(Fn(·) -F0(·)).\nUnder F0 continuous, B(·) is pivotal, and its distribution\nhas been tabulated.\nCite as: Victor Chernozhukov, course materials for 14.385 Nonlinear Econometric Analysis, Fall 2007. MIT OpenCourseWare\n(http://ocw.mit.edu), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    }
  ]
}