{
  "course_name": "Numerical Methods Applied to Chemical Engineering",
  "course_description": "Numerical methods for solving problems arising in heat and mass transfer, fluid mechanics, chemical reaction engineering, and molecular simulation. Topics: Numerical linear algebra, solution of nonlinear algebraic equations and ordinary differential equations, solution of partial differential equations (e.g. Navier-Stokes), numerical methods in molecular simulation (dynamics, geometry optimization). All methods are presented within the context of chemical engineering problems. Familiarity with structured programming is assumed.",
  "topics": [
    "Engineering",
    "Chemical Engineering",
    "Computer Science",
    "Data Mining",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Mathematics",
    "Applied Mathematics",
    "Engineering",
    "Chemical Engineering",
    "Computer Science",
    "Data Mining",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Mathematics",
    "Applied Mathematics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPurposes of This Course\n\nTo ensure that you are aware of the wide range of easily accessible numerical methods that will be useful in your thesis research, at practice school, and in your career, as well as to make you confident to look up additional methods when you need them.\n\nTo help you become familiar with MATLAB and other convenient numerical software, and with simple programming / debugging techniques.\n\nTo give you some understanding of how the numerical algorithms work, to help you understand why algorithms sometimes produce unexpected results.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nFinal Exam\n\n40%\n\nMidterm Quizzes\n\n30%\n\nHomework\n\n30%\n\nHomework Policy\n\nDoing the homework is the best way to learn and you are encouraged to discuss the homework with the TAs, the professors, and other students in the class. It is fine to ask someone else to look over your shoulder to help you debug your programs, or so that person can see how you accomplished some task. However,\nYou May Not\n:\n\nPost or provide any other student with an electronic or hard copy of any portion of your homework solution prior to the due date, or\n\naccept such a copy from another student, or\n\naccept, read, or use any homeworks from previous years (quizzes are fine).\n\nIt is acceptable to incorporate functions from other sources (e.g., from the previous week's posted homework solution); as long as the author / source of a function being recycled is properly credited in the homework.\nViolations of these policies will be considered cheating and can have very severe consequences\n.\n\nWhen to Stop\n\nSometimes you may find a homework problem is consuming an inordinate amount of time even after you have asked for help. (This is an occupational hazard for all software developers.) If this happens, just turn in what you have done with a note indicating that you know your solution is incomplete. This course nominally requires 9 hours per week on average--perhaps a little more early on if you are not proficient with MATLAB.\n\nUsing MATLAB\n\nInstall this program as soon as possible\n(if not already installed). There will be MATLAB tutorial help sessions for any students who have not used this program before or who need a refresher. You are also encouraged to go through any of the numerous\ntutorials\nprovided by Mathworks.\n\nReading Materials\n\nRequired textbook: Beers, Kenneth J.\nNumerical Methods for Chemical Engineering: Applications in MATLAB\n. Cambridge University Press, 2006. ISBN: 9780521859714. [Preview with\nGoogle Books\n]\n\nYou are expected to read the course materials before class, and to read the materials again before doing homework. Some reference books that may be helpful:\n\nPress, W. H.\nNumerical Recipes 3rd Edition: The Art of Scientific Computing\n. Cambridge University Press, 2007. ISBN: 9780521880688. [Preview with\nGoogle Books\n] (This comes in various editions) -- This book provides short clear synopses of methods for many types of problems.\n\nRektenwald, G.\nNumerical Methods with MATLAB: Implementations and Applications\n. Pearson, 2000. ISBN: 9780201308600 -- This book provides only simple numerical methods, but is good introdution to using MATLAB.\n\nHeath, Michael T.\nScientific Computing\n. The McGraw-Hill Companies, Incorporation, 2002. ISBN: 9780072399103 -- This book has more concise coverage of many topics.\n\nThere are also a very large number of textbooks on numerical methods for engineers, many of which have helpful examples implemented in MATLAB.",
  "files": [
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering:",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/8f235621d4a10971ca634ffb070f4d0c_MIT10_34F15_HW9_Prob.pdf",
      "content": "10.34\nNumerical Methods Applied to Chemical Engineering\nFall 2015\nHomework #9: Monte Carlo Methods\nProblem 1 (10 points).\nCaution: read the entire problem statement carefully before you attempt to structure a response;\nthere is important information throughout!\nWhen heated in air, most fuels form hydrogen peroxide (HOOH), which accumulates to a\nsomewhat high concentration before ignition. One reason ignition is so sudden is that above the\nignition temperature (about 1200 K), the HOOH decomposes, forming a large concentration of very\nreactive OH radicals. The ignition temperature is very sensitive to the rate at which the HOOH\ndecomposes, which in the gas phase is usually limited by the rate at which the thermal bath can\nsupply energy for this endothermic reaction through collisions. That is, the rate-limiting step is:\nHOOH + M →OH + OH + M\nwhere M stands for any collision partner (i.e., any other molecule in the system). Experimentally,\nsome collision partners are much more effective than others at transferring energy to HOOH, but\nno one knows exactly why. One theory proposes that the dipole moment of HOOH allows energy\ntransfer over long distances, but other researchers have suggested that the dipole moment of HOOH\nis actually very small at these high temperatures, where it is vibrating violently.\nIn this problem, your task is to estimate the average magnitude of the dipole moment, ⟨∥μ∥⟩,\nof HOOH at 1200 K, based on a molecular mechanics potential energy expression for HOOH and\nfor the charges on the H and O atoms. Note the dipole moment is a vector quantity:\nμ =\nX\nqi(ri\nR\ni\n-\ncm),\nwhere ri is a vector specifying the (x, y, z) position of the ith atom and qi is the atom's charge. The\nvector Rcm is the (x, y, z) position of the center of mass:\nRcm =\nP\ni miri\ni\nP\n,\ni mi\nwhere mi is the mass of the\nth atom.\nWe do not care about the direction of the dipole vector, only its magnitude ∥μ∥(2-norm).\nThe symbol \"⟨·⟩\" refers to the Boltzmann average over all possible molecular geometries; in the\nclassical limit and neglecting some minor complications due to the integral over the kinetic energy\nwe can write:\n⟨∥μ∥⟩= Q\nZ\n· · ·\nZ\nexp\n\n-V (r1, . . . , rN)\n)\nk\n\nT\n∥μ(r , . . . , rN ∥d3r1 . . . d3rN,\nwhere Q is the classical partition function:\nQ =\nZ\n· · ·\nZ\n\nV (r ,\n-\n1 . . . , rN)\nexp\nN\nk\n\nd3r1 . . . d3r .\nT\nWrite a set of\nfunctions that use the Metropolis Monte Carlo integration method to\ncompute ⟨∥μ∥⟩at T = 1200 K. You should report the following:\nMATLAB(r)\n\n1. The value you obtain for ⟨∥μ∥⟩and the number of Monte Carlo steps attempted and accepted.\n2. Your best guess at the variance in ∥μ∥and the uncertainty in your predicted value of ⟨∥μ∥⟩.\nExplain how you derived these.\n3. A plot of the probability density p(∥μ∥) of observing an HOOH molecule with a dipole moment\nof magnitude ∥μ∥. Hint: You may find the MATLAB routine hist helpful here.\nAn expression for the potential, V , and related information are as follows:\nV (r) = VOH(RO1H1) + VOH(RO2H2) + 2kOO(∥ROO∥-LO)2+\n1k\nθ (θHOO\nθ0)2 + (θOOH\nθ0)\n+ Vφ(θHOO, θOOH, φ),\nwhere\n-\n-\n\nVOH(r) = DOH (1 -exp (-α(∥r∥-\nLH))) ,\nVφ(θHOO, θOOH, φ) = (2 × 10-20J)(1 + cos(θHOO))(1 + cos(θ\nOOH))(cos(φ) -cos(φ0)) .\nRij is the vector between atoms i and j (so ∥Rij∥is the Cartesian distance between atoms i and\nj). θHOO is the angle defined by H1-O1-O2. θOOH is the angle defined by O1-O2-H2 (hint: law\nof cosines). φ is the dihedral angle, the expression for which depends on your choice of axes; an\nexpression for molecular-fixed axes is given at the end of the problem statement.\nThe charges on the atoms depend on the OH bond lengths, and are given by:\nq(r) = -0.25 exp (-α(∥r∥-LH)),\nqH1 = -qO1 = q(RH1O1),\nqH2 = -qO2 = q(RH2O2).\nThe values of q are given in units of the charge of an electron (a negative number with the normal\nsign convention for charge). It is customary to report molecular dipole moments in units of Debye.\nSystem parameters:\nDOH = 6 × 10-19 J\nLH = 1.05 × 10-10 m\nLO = 1.6 × 10-10 m\nk\nOO = 300 J/m\nk = 1.1 × 10-18\nθ\nJ/rad\nα = 1.5 × 1010 m-1\nφ0 = 1.7 rad\nθ0 = 1.8 rad\nNote that it is very easy to figure out the equilibrium geometry from the analytical expression for\nthe potential (V = 0 at the equilibrium geometry). Use this equilibrium geometry to compute\n⟨∥μ∥⟩for T = 0 K. When you begin the Metropolis MC integration, start your Monte Carlo steps\nfrom the equilibrium geometry.\nHint: You can use molecule-fixed axes to reduce the dimensionality of your model. Molecular\npotentials do not depend on the position of the molecule in space, nor on its orientation, but only\n\non the relative position of the atoms. Hence, one can usually cut six degrees of freedom (three\ncorresponding to the position of the molecule and three corresponding to its angular orientation--\nthe Euler angles) out of molecular problems. In this particular problem, we suggest using molecule-\nfixed axes where the position of atom O1 sets the origin, atom O2 lies on the x axis, and atom H1\nlies in the x-y plane. In this way, one can remove these 6 degrees of freedom from the problem:\nxO1, yO1, zO1, yO2, zO2, zH1 (that is, these can all be set equal to zero).\nWhen you remove these orientational degrees of freedom you pick up some Jacobian volume\nelements in the integrals for ⟨∥μ∥⟩and Q (approximating away some minor terms):\n⟨∥μ∥⟩= Q\nZ\n· · ·\nZ\n(xO2)2|yH1| exp\n\n-V (x)\nμ(x) d6x,\nZ\nZ\nkT\n\n∥\n∥\nQ =\n· · ·\n(xO2)2|yH1| exp\n\nV (x)\n-\n\nd6x,\nkT\nwhere the vector x contains the non-zero (x, y, z) coordinates of all the atoms. In this molecule-fixed\ncoordinate system, the dihedral angle is given by\nyH\ncos(φ) =\n1yH2\np\n|yH1 | (yH2 )2 + (z\nH2 )\nProblem 2 (10 points). Consider a simple system with a single particle that can be in any one of\n5 boxes. This situation is shown in Figure 1.\nFigure 1: A simple system with a single partical that moves between five boxes.\nSuppose that the energy of the system when the particle is in each box, E1, . . . , E5, are known\nand satisfy\nE1 > E2 > E3 > E4 > E5.\nSuppose further that the probability of being in box i, p(i), satisfies\np(i) ∝exp(-Ei/kT).\nYou decide to compute an approximation of the PDF p using a Markov process. In each step of\nyour simulation, the particle is initially in box i. First, you select a neighboring box at random\n(with equal probability). Next, you decide whether or not to accept a move to this neighboring\nbox using the acceptance criterion of Metropolis. A typical simulation of this type results in the\nhistogram in Figure 2.\n1. Does it make physical sense that p should look like the histogram in Figure 2? Explain why\nor why not.\n\nBox 1\nBox 2\nBox 3\nBox 4\nBox 5\nNumber of visits\nFigure 2: Typical results from Markov process simulation. For each box i, the height of the bar\nindicates the number of times the configuration where the particle is in box i was visited during\nthe simulation.\n2. Analyze the transition probabilities of the Markov process described by the algorithm above.\nIn your report, use the transition probabilities to argue either that the simulation is flawed,\nor that it is guaranteed to give the correct results.\n3. Suggest a second definition of the transition probabilities that leads to a valid simulation and\njustify your choice. If you believe that the original simulation results are incorrect, explain\nhow your new transition probabilites correct the problem.\n4. Suppose that the system described in Figure 1 is being used to model a one dimensional\ndiffusion process. Clearly, it is not physical for a particle to diffuse from box 2 to box 4 without\nfirst passing through box 3. Now consider a Markov process with transition probabilities that\nallow moves between non-neighboring boxes with nonzero probability. Can such a Markov\nprocess be used to approximate p? Explain your answer on physical grounds.\njks: November 23, 2011\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Homework Philosophy",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/7f99f95098f4a1fe7b074e90d5c81ffd_MIT10_34F15_HW_Philo_2015.pdf",
      "content": "10.34\nNumerical Methods Applied to Chemical Engineering\nFall 2015\n\nHomework Philosophy\nDate: September 2, 2015\n\n- In solutions, do what the problem asks. Show your reasoning, but be concise. There is no\nneed to directly quote material from lectures or notes.\nExample: Suppose we have covered Joel's method in class and the homework asks you to\nimplement it. Saying \"I implemented Joel's method as it was described in class\" is\nenough; you do not need to describe how it works in detail.\nIf a method requires you to make a programming decision (e.g., choosing an absolute\nerror tolerance) then report what you decided and why (e.g., \"An absolute error tolerance\nof 10-8 was used to ensure adequate convergence.\").\n- If you are writing code for an original idea/algorithm that does not appear in the notes,\ngive a conceptual and/or mathematical explanation of what you did while being as\nconcise as possible.\n- There are two types of homework problems. One is more mathematical and conceptual.\nIn such an assignment you will need to present your argument in written form using\nmathematical notation where appropriate. Remember to define all your variables! The\nother type requires coding (typically in MATLAB ). Here, working code is principally what\nwe desire. We will also ask you to test your various subroutines in various ways and\nreport back (both to the MATLAB console for the grader and in your report) some numbers\nor plots to show that your test succeeded.\n- Stop working when you have stopped learning. The idea of the homework is to help\nyou learn the material. If you have figured out the main idea of the problem but you have\nspent 20 hours on the problem and cannot find the error, it is probably time to stop and\nwrite a note saying that you think you have understood, and how you would proceed if\nthe code worked. This is a challenging course, especially difficult for those students who\nhave limited programming experience. Please be conscious of your workload, and the\namount of time it will take you to complete various assignments. Start working\nsufficiently early so that you may be able to seek help well in advance of the\nassignment deadline if needed. Remember to balance 10.34 homework with that of\n10.40 and 10.50.\n\nsds: September 28, 2011\nkk: August 16, 2012\nmcm: August 1, 2013\njap: August 20, 2014\nhht: Sept 2, 2015\n(r)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Homework Submission Guidelines",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/e5f1841a841b5910a3d250dcca7b9221_MIT10_34F15_HW_Guide_2015.pdf",
      "content": "10.34 Homework Submission Guidelines\n1. Homework will be due the beginning of class in electronic and paper form as described\nbelow. Please see the class schedule in the course syllabus for a complete list of\nhomework and their exact due dates.\nA \"late\" submission consists of EITHER the electronic or paper component turned\nin after the deadline. Homework solutions are posted soon after the due date so that you\ncan get quick feedback on assignments before quizzes and tests.\n-\nLate submissions after the deadline but before the solutions post will be credited\nat 50% of the score you would have otherwise received for it.\n-\nNo submission will be accepted if either the electronic copy or paper copy is\nsubmitted after the solution has been posted.\nIn short, please submit your homework on time! Under extreme circumstances this policy\nmay be relaxed. Please contact Kristen and Hok Hei as early as possible before the\ndeadline.\n2. All homework write-ups need to be typed in a word processor such as Microsoft Word or\nin LaTeX, and exported as a PDF. This includes problems involving proofs and\nderivations. Please be careful with mathematical notation and use equation editor\nsoftware such as MathType when working in Microsoft Word. Please use precise and\ncorrect mathematical notation following the standards of the homework problem\nstatement. Any solutions or figures should be included in your write-up PDF, rather than\nin your code or separate file. On each page of your write-up, include a header that\nincludes your name. Please do not copy-paste the problem statement or your code (unless\nwe ask for it explicitly) into your write-up.\n3. Unless otherwise noted, all code must be written in MATLAB and submitted electronically\nbut not ph ysically. Submit a single MATLAB (. m) function file f or each part of each\nhomework problem that the grader can run. We will be using an automated system to\nevaluate code and print plots for the graders. The .m-file for each homework problem\nshould be\nnamed:\n[Kerberos\nID]_HW[homework\nnumber]_P[problem\nnumber]_[part number].m. Your Kerberos ID is your email without the \"@mit.edu\". Not\nall assignments will have a part number, but the vast majority will. An ex ample i s as\nfollows:\n\"tamhok_HW1_P1_2.m\". This file might look something like:\nfunction tamhok_HW1_P1_2.m\n% Function that graders will run.\n(r)\n\n% It should run and produce the necessary outputs without any\nprompt or input from the grader\n%\nA = eye(3);\nb = ones(3,1);\nx = part1_function(A,b);\ny = part2_function(A,b);\nfprintf('x = \\t y = \\n'); fprintf('%d \\t\\t %d \\n',x,y);\nreturn\nfunction x = part1_function(input1,input2)\n% function that may take inputs,\n% it is called from the main function above\n%\n% code ...\nx = input1*input2;\nreturn\nfunction y = part2_function(input1,input2)\n% another function that does take inputs,\n% it is called from the main function above\n%\n% code ...\ny = input1\\input2;\nreturn\n4. Electronic submission:\nName your files as described above, (example below)\n[Kerberos_id]_HW[homework number] which contains all the .m-files and PDF:\n[Kerberos_id]_HW[homework number]_P1_3.m\n[Kerberos_id]_HW[homework number]_P2_2.m\n...\n[Kerberos_id]_HW[homework number].pdf\nNote: Accessory MATLAB functions called in your homework function do not have to\nfollow this convention. Make sure, however, that you run your code in the current\ndirectory without any variables in your workspace to ensure that your homework\nfunctions work!\nSubsequently, zip (compress) these files into a zip archive named\n[Kerberos_id]_HW[homework number].zip\nTo create a zip archive, go to your files in explorer, hold down CTRL and select the files,\nand then right click. A menu will pop up and there will be a \"send to\" option. Select that,\nand another menu will pop up. Click on \"Compressed (zipped) folder\". Rename it to the\nproper name. Note that these instructions are for Windows 7 only.\n\n5. Paper submission:\nSubmit the write-ups for each problem of each homework assignment as a separate, stapled\npacket. This is because each problem write-up will be passed to a different grader. Graders\nwill h ave a ccess to your e lectronic s ubmission a nd w ill ma ke sure your c ode r uns a s\ndescribed. Be c areful w ith do uble-sided pr inting! M ake sure you d on't h ave t wo\ndifferent problems on the same page.\njap: August 20, 2014\nhht: Sept 2, 2015\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Sample Homework Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/6f9b053004ecbd5f483f34b670b06e9d_MIT10_34F15_SampleHW.pdf",
      "content": "Homework 0 - Problem 3\nAn accurate estimate of the heat capacity (CP) for a particular inlet mixture is critical to\nan efficient separation, so you decide to carry out a series of heat capacity measurements at\ndifferent temperatures in the operating range and fit a polynomial model to compute specific heat\ncapacity as a function of temperature. To find the polynomial that best fits the data, we seek the\nvalues of the weights\nd\nn\ni\ni\nd\nthat give the best fit (in a least squares sense) for the linear system\nof nT equations, where nT is the number of T values at which CP(T) was measured:\n(\n),\n1,...,\nd\nd\nn\nj\nj\nn\nj\nP\nj\nT\nd\nd T\nd T\nd T\nC T\nj\nn\n\nThe measured values of the heat capacity have a relative error bound of 0.03 and absolute error\nbound of 50 J/(kg-K). Use the 2-norm throughout this problem for consistency.\n1.\nFor the case of a square system with nT = nd + 1 there will be values of d that fit the data.\nExpress the system as Ad = c, where d contains the weights. Write out the forms of A and c.\nMatrix-vector form of the polynomial fit for CP(T)\nWrite the nT equations for specific heat capacity assuming nT = nd + 1:\n1 1\n2 1\n( )\nd\nd\nn\nn\nP\nd\nd T\nd T\nd T\nC T\n\n1 2\n(\n)\nd\nd\nn\nn\nP\nd\nd T\nd T\nd T\nC T\n\n1 3\n(\n)\nd\nd\nn\nn\nP\nd\nd T\nd T\nd T\nC T\n\n(\n)\nd\nT\nT\nd\nT\nT\nn\nn\nn\nn\nn\nP\nn\nd\nd T\nd T\nd T\nC T\n\nIn vector matrix form:\n( )\n(\n)\n(\n)\n(\n)\nd\nd\nd\nd\nd\nT\nT\nT\nT\nn\nP\nn\nP\nn\nP\nn\nn\nP\nn\nn\nn\nn\nd\nC\nT\nT\nT\nT\nd\nC\nT\nT\nT\nT\nd\nC\nT\nT\nT\nT\nd\nC\nT\nT\nT\nT\n\nFrom this, the forms of A and c can easily be determined:\nd\nd\nd\nd\nT\nT\nT\nn\nn\nn\nn\nn\nn\nn\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\n\nA\n\n( )\n(\n)\n(\n)\n(\n)\nT\nP\nP\nP\nP\nn\nC T\nC T\nC T\nC T\n\nc\n\nCommented [U1]: You should title your homework problem.\nCommented [U2]: Short introductions to the problem statement\ncan be nice to define your nomenclature and set the stage for your\nanswers. However, it is not necessary and if you do this limit your\ndescription to be as short as possible. MAKE IT LESS THAN a\nquarter of a page.\nCommented [U3]: Adding lines/spacing can alert the grader to\nthe start of a new part of the problem. Again, a nice feature but not\nnecessary.\nCommented [U4]: Boxing your final answers and giving short\nintroductory sentences can be very helpful for graders. I suggest you\ndo this.\n\n2.\nLet the T measurements be evenly spaced between 273 K and 373 K, using nd = 2,3,...,8 (with\nnT = 3,4,...,9) plot cond(A) and norm(A-1) on the same axes as a function of nT.\nDiscussion of the algorithm to produce A, cond(A), and norm(A-1)\nThe paulson_HW2_P2.m file submitted online accesses a user-written function\n(condA_norminvA_errorPredCp()), reference for more information regarding the specifics of the\nalgorithm) that generates the matrix A for each specified size (3 through 9). Using A, the\nfunction returns both its condition number and the norm of its inverse using the built-in\nMATLAB functions cond() and norm(), respectively.\nThe condA_norminvA_errorPredCp() function generates the matrix A using two nested\nfor loops, the first of which spans the rows and the second of which spans the columns. The first\nrow (i = 1) corresponds to the lower bound for temperature (T_low). The second loop creates\nentries in matrix A (1 to nT) that correspond to the proper power of temperature (i.e. T0 to TnT-1).\nAfter each row is filled with the correct elements of T along its columns, the temperature value\n(T) is adjusted to an evenly spaced value between T_low and T_high (through the addition step\nof T_spacing). This step occurs before the row value, i, is updated, which is important because\neach row in A must correspond to a different temperature value. The algorithm repeats this\nprocess until increment i reaches the final row in A (i = nT).\nSince the problem statement asks for parameters at various nT values, I decided to add a\nthird nested loop above the two that generate A which runs from a lower to an upper bound on\nnT. After the two loops (discussed above) generate A, the condition number, norm of A inverse,\nand absolute error of CP (see part 4) are stored in vectors based on the corresponding nT value.\nThe outer loop repeats this process from a user-input lower bound of nT (nT_low) to a user-input\nupper bound of nT (nT_high) After the three loops are completely iterated, these parameter\nvectors (that have stored every specified parameter for all nT's input to the function) are\ndesignated as the function output. These vectors (cond_A, norm_invA, abs_error_Cp_pred) are\nthen plotted versus nT in the main function.\nCondition number of A and norm of A inverse\nThe condition number of A and the norm of A inverse are plotted versus nT below on\nFigure P2.1. The algorithm used to generate this plot is described above (reference\npaulson_HW2_P2.m for more information regarding any part of problem 2).\nFigure P2.1 shows that the condition number of A grows in order of magnitudes with\nsmall changes in nT. Additionally, their linear trends on a semilog plot imply that these\nparameters have an exponential correlation with respect to nT.\nCommented [U5]: Here is an example of going TOO FAR. I\ndescribe the submitted code in detail. This is not necessary\nespecially for this problem where we want you to worry more about\nthe equations than any algorithm (the algorithm is very simple, just\nmake an A matrix and compute the values). It is completely fine to\ndo this, but a waste of time. I would recommend given a much\nshorter description (e.g., \"My submitted MATLAB code generates\nmatrix A from part 1 and computes the condition number of norm of\nA inverse using built-in functions\").\n(r)\n\nFigure P2.1. Condition Number and norm of inverse for A on a log10 scale as a function of nT\n3.\nAgain for the square system, compute relative and absolute error bounds on d for several values\nof nd. Let the T measurements be evenly spaced between 273 and 373 K for the same nT and nd\nvalues in part 2. Plot the relative and absolute error bounds as a function of nT. Note: Express\nyour error bounds in terms of the relative and absolute error bounds in heat capacity.\nError matrix rearrangement\nThe matrix vector form of the equation for heat capacity is:\n\nAd\nc\nAdd a perturbation (Δd) in d that creates a perturbation (Δc) in c\n(\n)\n\nA d\nΔd\nc\nΔc\nDistribute A on the LHS\n\nAd\nAΔd\nc\nΔc\nNotice the original equation Ad = c, substitute this into the equation above\n\nc\nAΔd\nc\nΔc\n\nAΔd\nΔc\nCommented [U6]: The plot is commented well and nicely\nlabeled. You should do this in your reports. Notice that the quality of\nthe plot is not very good, you can achieve much better quality in a\nvariety of ways (please ask Joel for more information on this).\nCommented [U7]: Derivations such as this can be nice and\nhelpful for you. I would recommend that you include it when it is\nnot obvious as you can reference it during exams (you should be\nallowed to have all your notes and graded HWs). Here, this is fairly\nobvious so I could have jumped to the boxed equation directly.\n\nInduced Norm Inequality Proof\nThe induced norm of matrix\nM\nN\n\nA\n\nby the vector\nN\n\nx\nis:\nmax\n\nAx\nA\nx\n\nBy the definition of the maximum\n\nAx\nA\nx\n,\n:\nN\n\nAx\nA x\nΔx\nx\n\n*Induced norm inequality proof (referenced throughout the following sections)\nAbsolute Error of d\nIn order to calculate the absolute error of d\n\nΔd\n, left multiply the above relationship by the\ninverse of A\n\nΔd\nA Δc\nTake the 2-norm of the above relationship\n\nΔd\nA Δc\nFrom the induced norm inequality (proved in part 3)\n\nΔd\nA\nΔc\n*Absolute error inequality expression\nRelative Error of d\nIn order to calculate the relative error of d, take the 2-norm of the original matrix equation:\n\nAd\nc\nDivide the relative error inequality (above) by this expression\n\nA\nΔc\nΔd\nAd\nc\nMultiply the norm of Ad on both sides of the equation\nNOTE: THROUGHOUT THIS PROBLEM\n||A|| implies 2-norm of matrix A\nCommented [U8]: This was an example of something given in\nthe notes that you could have referenced. NOT REQUIRED AND A\nWASTE OF TIME. You would still need to give the final boxed\nequation but you would not have to go through any details.\n\nA\nΔc\nΔd\nAd\nc\nFrom the induced norm inequality (proved in part 3)\n\nA\nΔc\nΔd\nA d\nc\nDivide both sides by the norm of d\n\nΔd\nΔc\nA\nA\nd\nc\n*Relative error inequality expression\n2-Norm of Δc\nT\nn\ni\ni\nCp\n\nΔc\nExpand the summation term\nT\nn\nCp\nCp\nCp\n\nΔc\n\nSince each of the measurement error terms, ΔCPi are equivalent, they can be group together nT\ntimes. ΔCP is the absolute measurement error in heat capacity, ΔCP = 50 J/kg-K (Given).\nTn\nCp\n\nΔc\nTn\nCp\n\nΔc\nT\nJ\nn\nkgK\n\nΔc\n*2-Norm of the heat capacity error vector\nError bounds for d\nThe given absolute and relative error bounds for the measured heat capacities (c) are:\nAbsolute Error:\nT\nJ\nn\nkgK\n\nΔc\nRelative Error:\n0.03\n\nΔc\nc\n\nThe absolute and relative errors for weights d can be calculated using the error inequalities\nderived above in part 3:\n\nΔd\nA\nΔc\n\nΔd\nΔc\nA\nA\nd\nc\nT\nJ\nn\nkgK\n\nΔd\nA\n0.03\n\nΔd\nA\nA\nd\nDefine the norm of A times the norm of its inverse to be the condition number (cond(A)).\nT\nJ\nn\nkgK\n\nΔd\nA\n0.03\n( )\ncond\n\nΔd\nA\nd\n\n*Used to calculate absolute and relative error bounds of d in terms of the error bounds of c\nPlot of the error bounds of d as a function of nT\nThe relative and absolute errors of CP weights (d) are plotted as a function of nT below on\nFigure P2.2. The methodology used to generate these error values for d is described above.\n(Figure P2.2 also shows cond(A) and the norm(A-1) from part 2 for comparison)\nFigure P2.2. Relative and absolute error of d plotted on a log10 scale as a function of nT\nFigure P2.2 shows that the relative error bound of d follows the same trend as the\ncondition number of A and the absolute error bound of d follows the same trend as the 2-norm of\nA inverse. Due to these trends the error bounds of d grow exponentially fast making this\npolynomial fit unsuitable for CP as a function of T.\n\n4.\nAgain for the square system, derive an expression bounding the absolute uncertainty of the\nspecific heat capacity prediction from the fitted d for some arbitrary TT between 273 K and 373 K,\nfor arbitrary nd, and justify the bound. Plot the numerical values for the bounds for the values of\nnd and nT above, for TT= 300 K.\nAssume:\n^\n^\n^\n^\n^\n^\nPr\n( )\n( )\n( )\nd\nd\nn\ned\nTrue\nP\nP\nP\nn\nd\nd\nd\nT\nT\nT\nC\nT\nC\nT\nC T\nd\n\nAnd that there exists some dd within the error bound on d computed above such that:\n^\n^\n^\n^\n^\n^\n^\n^\n( )\nd\nd\nn\nTrue\nP\nn\nd\nd\nd\nT\nT\nT\nC\nT\nd\n\nSubtracting these two equations yields:\n^\n^\n^\n^\n^\n^\n^\n^\n( )\nd\nd\nd\nn\nP\nn\nn\nd\nd\nd\nd\nd\nd\nT\nT\nT\nC T\nd\nd\n\nDefine Δd to be the difference between d and dd and, T to be the polynomial expansion of T.\nSubstitute these relations into the above equation\n^\n( )\nP\nC T\n\nT\nd\n\nTake the 2-norm of both sides of the equation\n^\n( )\nP\nC T\n\nT\nd\nUsing the matrix induced norm inequality:\n\nT\nd\nT\nd\n\n^\n( )\nP\nC T\n\nT\nd\nAbsolute error inequality from part 3:\n\nd\nA\nΔc\n^\n( )\nP\nC T\n\nT A\nΔc\nSince ΔCP is a scalar its 2-norm is equivalent to the absolute value of the quantity itself\n^\n( )\nP\nC T\n\nT\nA\nΔc\nWhere ΔCP is the difference in the predicted and true heat capacity values\nT is the row vector of T values substituted into the polynomial fit for CP\nA is the temperature matrix representing the CP polynomial fit (part 1)\nΔc is the absolute measurement error bound for heat capacity = 50 J/(kg-K)\nTn\n\nΔc\nΔc (reference part 3 for derivation)\n*Used to calculate absolute uncertainty in predicted CP values\nPredicted CP uncertainty at TT = 300 K\nThe absolute uncertainty in the predicted CP value (defined as |ΔCP|) plotted versus\nnT = 3,..,9 is shown below in Figure P2.3. The values of this plot were obtained using the\ncondA_norminvA_errorPredCp() function in the submitted paulson_HW2_P2.m file. The\nalgorithm is discussed in detail in part 2.\nFigure P2.3. Absolute uncertainty in predicted CP at 300 K (log10 scale)\nCommented [U9]: Here is a nice example of a completely\nsummarized equation. I am telling the graders everything they need\nto know for the final answer (including defining my terms) while\nreferencing above for derivations. If you define things previously, it\nis ok to NOT define them again, but remember graders are human\nand make mistakes.\n\nFigure P2.3 shows that at T = 300 K, the predicted specific heat capacity has an absolute\nerror bound of order 30 which corresponds to a polynomial fit with nine temperature\nmeasurements (nT = 9). These large magnitudes in the predicted CP values are a direct result of\nthe large-normed T vector and the ill-conditioned A matrix. Together, these parameters\nsignificantly amplify the small absolute error bound in the measured heat capacities.\n5.\nFor the more general case nT > nd + 1, derive expressions for absolute error bounds for the least-\nsquares estimates dLS using the relation seen in Homework 0:\nT\nLS\nT\nA Ad\n= A c\nDerivation\nStarting with the general least-squares estimate equation:\nT\nLS\nT\nA Ad\n= A c\nAdd a perturbation (ΔdLS) in dLS that creates a perturbation (Δc) in c\n(\n)\n(\n)\nT\nLS\nLS\nT\n\nA A d\nΔd\nA\nc\nΔc\nDistribute ATA on the LHS and AT on the RHS\nT\nLS\nT\nLS\nT\nT\n\nA Ad\nA AΔd\nA c\nA Δc\nNotice the original equation\nT\nLS\nT\nA Ad\n= A c , cancel on both sides of the equation\nT\nLS\nT\n\nA AΔd\nA Δc\n*Least squares (LS) error equation\nMultiply both sides of the LS error equation by the inverse of ATA:\n(\n)\nLS\nT\nT\n\nΔd\nA A\nA Δc\nTake the 2-norm of both sides of this equation\n(\n)\nLS\nT\nT\n\nΔd\nA A\nA Δc\nUsing similar ideas to the induced norm inequality (proved in part 4), the RHS norm can be\nshown to be less than the product of the individual vectors/matrices norms. This results in:\n(\n)\nLS\nT\nT\n\nΔd\nA A\nA\nΔc\n*Notes: (1) ||ΔdLS|| is the relative error bound for the least square estimates (dLS)\n(2) A and c are equivalent to those defined in part 1\nCommented [U10]: I would recommend giving little\ndescriptions of the plots such as this one so that the plot is not free-\nfloating in space. You can keep it short and sweet.\n\n6.\nExplain the intuitive meaning of the ill-conditioning of this system\nFor a linear equation, Ax = b, the condition number of matrix A is said defined as the\noperator of two norms, cond(A) = ||A|| ||A-1||. The condition number is important when solving\nlinear equations because it is an indicator as to how much a perturbation in b will alter the\nsolution vector x. If the condition number is high, small changes in b can create large errors in x.\nSuch a system is said to be ill-conditioned. This usually implies A has small-valued eigenvalues\nmaking it close to singular (zero-valued determinant) and/or the system is badly scaled.\nIn this problem, the A matrix equals (part 1):\nd\nd\nd\nd\nT\nT\nT\nn\nn\nn\nn\nn\nn\nn\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\nT\n\nA\n\nThis matrix has a large magnitude discrepancy within each of its columns due to the \"Taylor-\nseries type\" polynomial fit for heat capacity (CP) in terms of temperature (T) which has\nincreasing orders of T from 0 to nd. Since the values of temperature are usually order 3, these\npolynomial factors result in extremely large magnitude differences from column to column. This\nintuitively means that either the chosen scale for the problem parameters are wrong or the fit for\nCP as a function of temperature was a poor choice. For this particular problem, I believe the\npolynomial expansion fit was a poor choice since high order functions usually result in large\nmagnitude terms (Terms >= 1010). Additionally, these values span so many orders that it is\ndifficult to estimate each term accurately. Moreover, the large-magnitude terms result in an\nenormous condition number for A which, in turn, takes the relatively small errors in c and\namplifies them within the solution vector d.\nI would recommend choosing a better function to fit CP with respect to T instead of\nforcing data into random polynomial expansions and preforming least squares analysis blindly.\nThis more in-depth analysis would require investigating how these variables normally interact\nwith each other in various relationships. Hopefully, this newer fit would eliminate the large\nmagnitude discrepancy between columns; however, rescaling the variables to some small finite\nrange could also be a viable option (i.e. try using relationships that make the temperature and\nheat capacity dimensionless, maybe through the addition of other important parameters).\n7.\nUse Chebyshev polynomials in place of the Tis to construct matrix A. You can use the MATLAB\nfunction ChebyshevPoly. For better conditioning, scale the temperature so that the Chebyshev\npolynomial is evaluated on [-1, 1] by evaluating at (2T - Tlow - Thigh) / (Thigh - Tlow) rather than at\nT. Comment on the difference between the error bounds for Tis vs. Chebyshev polynomials on\nthe rescaled temperature range.\n\nDiscussion of the Chebyshev polynomial algorithm\nThe paulson_HW2_P2.m file submitted online accesses a user-written function\n(chebyshev_A()), reference for more information regarding the specifics of the algorithm) that\ngenerates a modified matrix A for each specified size (3 through 9). Using the ChebyshevPoly()\nMATLAB function a polynomial is generated for every power of T in the assumed fit for CP.\nThis polynomial is then evaluated at a modified T value on the scale of [-1, 1].\nThe main idea behind this type of modification is to alter A so that its condition number\nis significantly lower effectively reducing the error bounds for the solution vector. The user-\nwritten chebyshev_A() function is just a slightly modified version of the\ncond_A_norminvA_errorPredCp() function discussed in detail in part 2 (reference for more\ninformation regarding the framework of this algorithm). The only change to this is inside the\nthird nested for loop, which no longer stores some power of T to the matrix A, but instead\nperforms this substitution/evaluation for the chebyshev polynomial discussed in the paragraph\nabove. From this, it can be seen that a multiple polynomials (evaluated along the altered T scale)\nare stored in A which effectively adds more entries to the spanning set of the matrix A.\nAdditionally, the rescaled temperature reduces the extremely large magnitude values dealt with\nin part 2 (i.e. ~2739).\nPlot of the error bounds of d as a function of nT evaluated using Chebyshev polynomials\nThe relative and absolute errors of CP weights (d) calculated using Chebyshev\npolynomials are plotted as a function of nT below on Figure P2.4.\nFigure P2.4. Absolute and relative error bounds for d calculated using Chebyshev polynomials\n\nError bound comparison for d\nFor easier comparison, the relative and absolute errors of d calculated using the full\npolynomial expansion (part 2) and Chebyshev polynomials (part 7) are plotted together on\nFigure P2.5 below. The full polynomial fit produces an absolute error range for d of ~104 to\n~1010 (over nT = 3 to 9) whereas the Chebyshev polynomials produce a range of ~101.75 to ~102.5.\nFurthermore, the full polynomial fit produces a relative error range for d of ~105 to ~1026 (over\nnT = 3 to 9) whereas the Chebyshev polynomials produce a range of ~10-1.5 to ~10-0.5. This shows\nthat the both the absolute and relative error bounds for d drop significantly when Chebyshev\npolynomials are used in place of the full polynomial expansion to calculate A.\nFigure 2.5. Error bounds for d using the full polynomial expansion and Chebyshev polynomials\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Tips on Writing Faster MATLAB Code",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/90652087fcf6204731be7630894bec79_MIT10_34F15_Writing_2013.pdf",
      "content": "Writing Faster MATLAB Code\nSome tricks and tips on the efficient usage of MATLAB were provided in the August 2013 issue\nof IEEE Control Systems Magazine [1]. This column provides some tips on writing faster MATLAB\ncode, which is important in applications to larger scale problems. Some codes can be sped up by\nmore than three orders-of-magnitude by following these tips.\nTip #1: Define vector and matrix dimensions before using them. A feature of MATLAB is that\nits ability to redefine the numbers of elements in an array on the fly, without having to define the\ndimensions beforehand. For example, a valid MATLAB code for creating a vector of numbers from\nthe standard normal distribution is (to be compared on a fair basis, all variables were cleared\nfrom memory before running each snippet of code)\n>> for i = 1:1000\n>>\nA(i) = randn;\n>> end\nThis feature reduces the code length but has a high computational cost, as it is much more\nefficient to define the vector or matrix definitions beforehand. For example, consider two\nsnippets of MATLAB code, for defining a matrix whose elements come from a standard normal\ndistribution, that are exactly the same except that the second code snippet defines the matrix\ndimensions beforehand (all costs are reported for a 1.6 GHz Quad Core Intel i7 processor):\n>> tic\n>> for i = 1:1000\n>>\nfor k = 1:1000\n>>\nA(i,k) = randn;\n>>\nend\n>> end\n>> toc\nElapsed time is 2.422478 seconds.\n>> tic\n>> A = zeros(1000);\n>> for i = 1:1000\n>>\nfor k = 1:1000\n>>\nA(i,k) = randn;\n>>\nend\n>> end\n>> toc\nElapsed time is 0.066432 seconds.\nThe latter MATLAB code snippet was more than 35 times faster. Redefining dimensions takes time;\nthe large difference in runtime is because the former code snippet redefines the matrix\ndimensions many times whereas the latter code snippet defines the matrix dimensions only once.\n(r)\n\nMany MATLAB codes can be sped up by orders\nof magnitude by applying some simple tips.\nThe speedup achieved by predefining the dimensions depends on the cost of the other\ncalculations in the program. The above example used the randn command so that some\ncalculations occurred in each loop. Consider the runtimes for the aforementioned two code\nsnippets with each random number call replaced with the number 1:\n>> tic\n>> for i = 1:1000\n>>\nfor k = 1:1000\n>>\nA(i,k) = 1;\n>>\nend\n>> end\n>> toc\nElapsed time is 2.366193 seconds.\n>> tic\n>> A = zeros(1000);\n>> for i = 1:1000\n>>\nfor k = 1:1000\n>>\nA(i,k) = 1;\n>>\nend\n>> end\n>> toc\nElapsed time is 0.017636 seconds.\nPredefining the dimensions in the latter code snippet resulted in a speedup of more than 100\ntimes.\nMATLAB is an interpreted language, which means that it avoids explicit program compilation.\nWhen a code is run, each statement of the MATLAB code is executed one statement at a time. If the\nmatrix dimensions are not defined beforehand,\nthen MATLAB needs to redefine the matrix\ndimensions on-the-fly, which results in longer\nruntimes.\nTip #2: Use built-in MATLAB commands where possible. Using built-in MATLAB commands\nleads to code that is both shorter and faster. For example, consider an alternative code snippet for\ngenerating a matrix whose elements come from a standard normal distribution:\n>> tic\n>> A = randn(1000);\n>> toc\nElapsed time is 0.021536 seconds.\n\nThis code snippet is about three times faster than repeatedly calling the randn command in a\nloop (0.066432 s). This snippet uses a built-in MATLAB command both defines the matrix\ndimensions and assigns its elements in precompiled code. The randn command does not consist\nof individual lines of MATLAB code and so can be optimized to have shorter runtimes. This tip is\nconsistent with a previous tip [1] to avoid loops as much as possible when using MATLAB, due to\nits low efficiency in handling loops.\nThe efficiency improvement of using built-in MATLAB commands depends on the number of\nelements and the amount of computation in the definition of each element. For example, consider\nthe code that defines a 1000 × 1000 matrix of ones:\n>> tic\n>> A = ones(1000);\n>> toc\nElapsed time is 0.003583 seconds.\nThis code snippet is about five times faster than defining each element of the matrix A within\ntwo nested loops (0.017636 s).\nOrders of Magnitude Speedups. Combining Tips #1 and #2 can produce truly astounding\nreductions in runtimes. For example, running the MATLAB code snippet\n>> for i = 1:3000\n>>\nfor k = 1:3000\n>>\nA(i,k) = 1;\n>>\nend\n>> end\ntook 64.804833 seconds compared to 0.027642 seconds running the code snippet\n>> A = ones(3000);\nFor this example, the runtime was reduced by more than a factor of 2000.\nReferences\n[1] Kam K. Leang, \"MATLAB tricks and tips,\" IEEE Control Systems, vol. 33, no. 4, pp. 39-40,\nAugust 2013.\n- Mark C. Molaro and Richard D. Braatz\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Final Exam Review",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/cee6b122be1e3a3c48341f381c346cee_MIT10_34F15_FinalExam.pdf",
      "content": "10.34\nNumerical Methods Applied to Chemical Engineering\nFall 2015\nFinal Exam Review\nPartial Differential Equations (PDEs)\n1. Classification\n(a) Many PDEs encountered by chemical engineers are second order (containing at most\nsecond derivatives) or can be decomposed into a system of second order PDEs. The\nstandard form for a linear second order PDE for a function f : RN →R and independent\nvariables y ∈RN is\nX\nN\nN\ni=1\nX\n∂2\nAij\nj=1\n∂yi∂yj\nf(y) +\nN\nX\ni=1\nbi(y) ∂f(y) + g(y)f(y) = h(y)\n∂yi\nwhere A(y) ∈RN×N, b ∈RN, g(y) ∈R, and h(y) ∈R are arbitrary functions of the\nindependent variable y. This equation is valid for some points y within some prescribed\ndomain Ω⊂RN, which has a boundary δΩ⊂RN-1.\n(b) Second order PDEs are classified by examining the properties of A(y) and b(y).\ni. Elliptic at point y if A(y) is not singular and all of its eigenvalues have the same\nsign.\nii. Hyperbolic at point y if A(y) is not singular and all of its eigenvalues but one have\nthe same sign.\niii. Parabolic at point y if A(y) has one eigenvalue equal to zero and all other eigenvalues\nhave the same sign. Additionally, the rank of the matrix\n\nAC\n1 (y)\nAC\n2 (y)\n· · ·\nAC\nN(y)\nb(y)\nmust be N. In other words, b cannot be written as a linear\n\ncombination of the\ncolumns of A(y).\nThe PDE is called an elliptic, hyperbolic, or parabolic PDE when these conditions hold\nfor all y in the domain Ω.\n(c) Quintessential examples of these types\ni. Elliptic - Steady diffusion represented by Laplace's equation\n0 = D\n∂2C\n∂x2\n+ ∂2C\nA\n∂x2\n\ny=(-\nx→\n(y) =\n1,x2)\nD\nD\n\nii. Hyperbolic - Wave equation\n∂2C\n∂\n= u2\n∂t2\n\n2C\n∂x2\n+ ∂2C\n∂x2\n\n-→\ny=(x1,x2,t)\nA(y) =\n\nu2\nu2\n-1\n\niii. Parabolic - Unsteady diffusion\n∂C\n∂t = D\n∂2C\n∂x2\n+ ∂2C\nA\n∂x2\n\ny=(-\nx→\n(y) =\n1,x2,t)\nD\nD\n\n; b =\n-1\n\n2. Finite differences\n(a) In the finite difference method, the domain of a function f(y) described by a PDE is\ndecomposed into a finite set of points (or nodes). Then, the finite difference formula is\nused to approximate derivatives in the PDE at these nodes.\n(b) These difference formulas introduce errors with respect to spacing between the nodes.\nTypically, we use approximations that are first or second order with respect to the\nspacing between nodes.\n(c) With one discrete algebraic equation produced per node and f(y) unknown at each\nnode, this procedure yields a complete system of algebraic equations that can be solved\nusing standard methods. Linear PDEs will result in a linear system of equations while\nnonlinear PDEs will result in a nonlinear system of equations.\n(d) This method can be used to solve most PDE problems (elliptic, parabolic, hyperbolic)\nincluding steady state or unsteady problems. It is fairly easy to implement but hard\nto incorporate irregular boundary shapes. This method does not conserve, conserved\nquantities (loss of mass, energy, momentum from simulation for example).\n3. Finite volumes\n(a) The finite volume method was discussed in the context of BVPs. The idea does not\nchange when we move to PDEs except now we look at cells possibly distributed in space\nand as a function of time.\n(b) The basic idea is to integrate the differential form of the conservation equation and then\nlook at the \"flux\" (total amount of material) through each face of the cell. This allows\nus to look at very complicated geometries in a simple way as we are merely breaking up\nthe domain into \"cells\" and satisfying a total conservation equation locally in each cell.\n(c) Typically the flux is approximated using some polynomial interpolation between cell\ncenters (that represent the average concentration in the cell).\nSee page 101 of the\nextended PDE notes for detailed equations.\n(d) The same rules apply for indexing the cells as discussed for nodes in the finite difference\nsection. Again, the equations tend to be sparse such that we can take advantage of\nsparse/iterative solvers to get reasonably fast solutions.\n4. Numerical method of lines\n(a) The method of lines is a technique for solving PDEs with initial conditions that capital-\nizes on the efforts to develop stable, high accuracy methods for first order ODEs.\n(b) Typically, the spatial domain is approximated using finite differencing while time deriva-\ntives are kept.\nThis gives us a set of ODE-IVPs that can be solved using standard\nmethods. This allows us to overcome issues with the advection equation using central\ndifference in space for example (which is second order accurate) . We showed that a\n\nforward Euler integration of this scheme was unconditionally unstable. However, when\nthe time derivative is kept intact, ODE solvers that have a conditional stability as a\nresult of adaptive time stepping or high order accuracy can be used.\nProbability Theory\n1. Preliminaries: probability obeys three rules:\n(a) For any event A, the probability of event A satisfies Pr(A) ≥0.\n(b) The probability of the event consisting of all possible outcomes is one, Pr(E) = 1.\n(c) If the events A and B are mutually exclusive, then Pr(A or B) = Pr(A) + Pr(B).\nAny other result in probability theory, intuitive or otherwise, can be derived from these\nthree rules without any further assumptions. One fundamental result that follows from these\nproperties is: For any events A and B,\nPr(A or B) = Pr(A) + Pr(B) -Pr(A and B)\n2. Definitions\n(a) Random variable - Formally, a random variable is any function of the outcome of an\nexperiment g : E →R. In general, let x denote a random variable on a set of outcomes\nE. Performing a single trial produces an outcome ξ ∈E that is a realization or sample\nof the random variable x, x(ξ) = xˆ ∈R. Since the value of a random variable depends\non the outcome of a trial, the events can be defined in terms of random variables. For\nexample, the subset of E defined by {ξ : x(ξ) ≥5} is an event, which is interpreted as\n\"the outcome of the trial ξ satisfies ξ ≥5\". These events also have probabilities, which\nare denoted by\nPr(x ≥5) = Pr({ξ : x(ξ) ≥5}).\n(b) Probability mass function - for a discrete random variable X, the probability mass\nof x is the probability of the event X = x which can be written pX(x) = P(X = x). A\nPMF must have the property that\nx pX(x) = 1.\n(c) Probability density function - the\nR\nes\nP\nprobability density describes the probability that\na continuous random variable tak\na value in a specified range: P(a ≤X ≤b) =\nb fX(x)dx. The PDF fX satisfies\na\nZ +inf\nfX(x)dx = 1\n-inf\nBecause the PDF represents a \"probability mass per unit length\" the values of fX(x)\ndo not need to be strictly less than 1 but they must be nonnegative.\n(d) Cumulative probability function - The cumulative probability function (also known\nas the cumulative distribution function) Fx of a random variable is defined as\nx\nFX(x) = P(X ≤x) =\nZ\nfX(t)dt\n-inf\n\nFx is a non-decreasing function with Fx(-inf) = 0 and Fx(+inf) = 1. The probability\nthat x will be in the range [x1, x2] in a single trial is\nPr([x1, x2]) = Fx(x2) -Fx(x1)\n(e) Marginal probability - given the joint probability pX,Y (x, y) the marginal probability\nof X is pX(x) = P\ny pX,Y (x, y)\n(f) Conditional probabilities - For two events A and B, the conditional probability of\nA given B is the probability that the event A will occur, given that event B is known\nto occur (or to have occurred). This is related to the intersection of the two sets by\nPr(A and B)\nPr(A|B) =\nPr(B)\nWe can rearrange this expression for the conditional probability\nPr(A and B) = Pr(A|B)Pr(B)\n(g) Independence The events A and B are said to be independent if\nPr(A and B) = Pr(A)Pr(B)\nwhich implies that the events A and B are independent if and only if Pr(A|B) = Pr(A).\nThat is, the probability of the occurrence of event A is not affected by whether the event\nB has occurred. This result is consistent with our intuition as to the meaning of two\nevents being independent.\n(h) Expected value - The expected value of x (known as the mean of x) is\n⟨x⟩=\nZ +inf\nx′fX(x′)dx′.\n-inf\nLet g be a function of the random variable x. The expected value of g is defined as\ninf\n⟨g(x ⟩=\nZ +\n)\ng(x′)fX(x′)dx′.\n-inf\n(i) Variance - The variance of x, σ2\nx, is defined as the expected value of (x -⟨x⟩)2,\nσ2\nx = ⟨(x -⟨x⟩)2⟩\n=\nZ +inf\n(x -⟨x⟩)2px(x′)dx′\n-inf\n=\nZ +inf\n(x2 -2⟨x⟩x + ⟨x⟩2)px(x′)dx′\n-inf\n=\nZ +inf\n+inf\nx2px(x′)dx′ -2⟨x⟩\n-inf\nZ\nxpx(x′)dx′\n|-inf\n{z\n}\n⟨x⟩\n+⟨x⟩2\nZ +inf\n-inf\npx(x′)dx′\n|\n{z\n=\n}\n⟨x2⟩-2⟨x⟩2 + ⟨x⟩2\n= ⟨x2⟩-⟨x⟩2\n\nThe variance of f(x), which is a function of random variable x, is\nσf = ⟨f2⟩-⟨f⟩2\nThe standard deviations of x and f are σx =\np\nσ2x and σf =\nq\nσ2, respectively.\nf\n3. Important Theorems\n(a) Total probability theorem - Let A1, . . . , An be disjoint events that form a partition\nof the sample space (each possible outcome is included in exactly one of the events\nA1, . . . , An) and assume that P(Ai) > 0∀i. Then, for any event B,\nPr(B) = Pr(B|A1)Pr(A1) + Pr(B|A2)Pr(A2) + · · · + Pr(B|An)Pr(An)\nThis can be written in a more compact form\nn\nPr(B) =\nX\nn\nPr(B and Ai) =\nPr(\ni=1\nX\nB\ni=1\n|Ai)Pr(Ai)\n(b) Bayes' theorem - For two events Ai and B, taking the intersection of two sets is\nindependent of the order of the two sets,\nPr(Ai and B) = Pr(B and Ai)\nApplying the conditional probability expression to both sides gives\nPr(B\n)\nPr(Ai\n|A )Pr(A\n|\n→\ni\nB)Pr(B) = Pr(B|Ai)Pr(Ai)\n-\nPr(Ai|\ni\nB) =\nPr(B)\nThis is known as Bayes' Theorem.\nWhen Ai is a member of a collection of events,\nA1, . . . , An, which is mutually exclusive and collectively exhaustive, we can substitute\nthe Total Probability Theorem into the denominator\nPr(B\n)\nPr(Ai|B =\n|Ai)Pr(Ai\n)\nPr(B|A1)Pr(A1) + Pr(B|A2)Pr(A2) + · · · + Pr(B|An)Pr(An)\nBayes' theorem provides a useful framework for using experimental measurements to\nupdate knowledge about a physical problem, which is often applied in design of prog-\nnostic/diagnostic systems and in the estimation of states or model parameters from\nexperimental data.\n(c) Central limit theorem - Let X1, X2 . . . be a squence of independent identically dis-\ntributed random variables with common mean μ and variance σ2, and define\nX1 +\nZn =\n· · · + Xn -nμ\nσ√n\nThen the CDF of Zn converges to the standard normal CDF\nF(z) = √\nz\n2π\nZ\ne-x2/2dx\n-inf\nin the sense that P(Zn ≤z) = F(z).\nn→inf\n\n4. Notable distributions\n(a) The normal or Gaussian PDF is\npx(xˆ) = σ\n√\ne-(x-μ) /(2σ2)\n2π\nThe mean and variance of a random variable x with this PDF are ⟨x⟩= μ and σ2\nx = σ2.\n(b) The uniform PDF on interval [a, b] ⊂R is\npx(xˆ) =\n\n,\nif xˆ\nb\n-a\n∈[a, ]\nb\n0,\notherwise\nA random variable x with this PDF has zero probability of being observed outside of\nthe interval [a, b], and the probability of being observed in any infinitesimal interval\n[x,ˆ xˆ + dx] is equal for all xˆ ∈[a, b). The mean and standard deviation of the uniform\ndistribution are ⟨x⟩= a+b and σx = 1\nb-a 2\n\n.\n(c) The binomial PMF is\np(k) =\nn\nk\n\npk(1 -p)n-k\nwhere\nn\nk\n\nn!\n= k!(n -k)!\nand this factor is known as the binomial coefficient. When n = 1 this is known as the\nBernoulli distribution.\n5. Multivariate extensions\n(a) Random vector - A random vector is a vector of random variables\nx = (x1, . . . , xn),\nwhich can be written either as a row or column.\nThese appear naturally when the\noutcome of an experiment are vector-valued (e.g., the experiment may be to measure\nthe velocity of the particle, which as three components v = (vx, vy, vz)).\n(b) Joint cumulative probability function - The joint cumulative probability function\nof a random vector x is defined as\nFx(xˆ) = Pr(x1 ≤xˆ1 and x2 ≤xˆ2 and · · · and xn ≤xˆn),\nwhere xˆ = (xˆ1, . . . , xˆn).\n(c) Joint probability density function - The joint PDF of the random vector x is a\nfunction px defined by: They probability that, in a given trial, x(ξ) is in the infinitesimal\ninterval\n[xˆ, xˆ + dx] ≡[xˆ1, xˆ1 + dx1] × · · · × [xˆn, xˆn + dxn]\n\nis\nPr([xˆ, xˆ + dx]) = fX(xˆ)dx\nThe PDF fX satisfies\nZ\n+inf\n+inf\npx(x′)dx′ =\nZ\n· · ·\nZ\nfX(x1\n′ , . . . , x′\nn)dx′\n1 . . . dx′\nn = 1\nRn\n-inf\n-inf\nand is related to the joint CDF by\nFx(xˆ) =\nZ xˆ1\nxˆn\n· · ·\npx(x′\n1, . . . , x′\nn)dx′\n1 . . . dx′\nn\n-inf\nZ\n-inf\nThe probability that the random vector lies in any region Ω⊂Rn can be computed,\nPr(x ∈Ω) =\nZ\nfX(x′)dx′\nΩ\n(d) Independent random variables - The (scalar) random variables x1, . . . xn are said\nto be independent if\npX(xˆ) = pX1(xˆ1)pX2(xˆ2) · · · pXn(xˆn)\ni.e. the distribution can be factored.\n(e) Mean - The mean of a random vector x is\n⟨x⟩=\nZ\nx′fX(x′)dx′\nRn\nNote that this is a compact way to represent the mean of each component of the vector\n⟨xi⟩=\nZ\nx′\nipx(x′)dx′\nRn\n(f) Covariance - The covariance of the two (scalar) random variables xi and xj is\nCij = ⟨(xi -⟨xi⟩)(xj -⟨xj⟩)⟩\n= ⟨xixj⟩-⟨xi⟩⟨xj⟩\nThe variance of xi is C\nii = σxi. The covariance matrix of the random vector x is\n\nC11\n·\nC1n\n.\n.\n· ·\n.\nC =\n.\n.\n.\n.\n.\n.\n\nC1n\n· · ·\nCnn\n\nNote that the covariance matrix is symmetric, positiv\n\ne definite and invertible.\nThe\nrandom vector x is uncorrelated if C is diagonal. If the elements of x are independent,\nthen x is uncorrelated.\n(g) The multivariate Gaussian distribution has the following form:\nk\npX(x) = (2π)-2 |Σ|-1\n2 exp\n\n-1(x\n-μ)TΣ-1(x -μ)\n\nwhere k is the dimension of x, Σ is the covariance and μ is the average.\n\n(h) Sample average - Suppose that x is a scalar random variable with PDF px and f is a\nfunction of x. The sample average of f with N samples is\nSN\nf =\nx\nN\nX\nN\nf( i),\ni=1\nwhere x1, . . . , xN are N samples from px (equivalently, values of the random variable x\nin N trials, xi = x(ξi)).\nThe sample average is a function of the N samples from px. An equivalent interpretation\nis that the sample average is a function of N independent random variables, all of which\nhave the same PDF px. That is, SN\nf is a function of a random vector x = (x1, . . . , xN)\nwith\npx(xˆ) = px(xˆ1)px(xˆ2) · · · px(xˆN)\nModels and Data\n1. Least-squares solution\n(a) To fit a linear model, yP = Aθ where A is the data and yP is the prediction, often the\nsum of the squared error (deviation between the prediction and data) is minimized.\n(b) This problem is formulated as:\nθLS = arg maxeTe = arg max(y -Aθ)T(y\nA\nθ\nθ\n-\nθ)\n(c) The solution to this optimization problem is\nθ = (ATA)-1ATy\n2. Maximum likelihood estimation (MLE)\n(a) The maximum likelihood estimation of the parameters of a model maximizes the prob-\nability (or likelihood) of the errors between the observed dataset and the model.\n(b) The likelihood function is sometimes written L(θ; D) = p(D|θ) where D is the dataset\nand θ is the vector (or scalar) parameters of the model.\n(c) To do this, we define our error to be ε = yˆD -yP (θ) and assume that the distribution\nof our errors is normal. This gives pdf:\npε(εˆ) = (2π)-(K/2)|C|-(1/2)exp\n\n-\n(εˆ-⟨ε⟩)TC-1(εˆ\n-⟨ε⟩)\n\n(d) The MLE parameters can then be defined:\nθMLE = arg max L(θ; D)\nθ\n(e) Given the model, the covariance structure can often be estimated before data is col-\nlected. The covariance can then be used in the design of experiments. The covariance\nis estimated using a two-step procedure:\n\ni. Approximate the elements of the Hessian\nHmn = 2\nX 1\ni\nσ2\ni\n∂Yi\n∂Pm\n∂Yi\n∂Pn\nwhere Yi is the predicted value of the observable, σi is the estimated uncertainty in\nYi and Pm and Pn are model parameters.\nii. The Hessian matrix is diagonalized to find the eigenvectors and eigenvalues\niii. The elements of the covariance matrix are estimated\nCjk =\nX 1\ni\nvjivki\nλi\nwhere λ is the ith eigenvalue and v\nis the ith\nth\ni\nji\nelement of the j\neigenvector.\niv. NOTE: if the model is nonlinear, prior estimates of the parameters will be required\nto calculate the partial derivatives.\n(f) To perform this analysis, the covariance matrix, C is needed. The covariance can be\nestimated from the data but often there is not enough information to estimate all of the\nparameters. Often the covariance matrix is assumed to have a diagonal structure (i.e.\nthe observables are assumed to be independent).\n3. Bayesian parameter estimation\n(a) Another estimate of the model parameters arises from a Bayesian framework which\nconsiders and estimate based on the posterior distribution of the parameters.\n(b) Recall that Bayes' Theorem tells us that:\np(D\np(θ\n|θ)p(θ)\n|D) =\np(D)\nwhere again D is the dataset and θ are the parameters. p(θ) is referred to as the prior\ndistribution.\n(c) Solving for the full distribution of the parameters given the data can be a very diffi-\ncult problem however often we are interested only in the maximum a posteriori (MAP)\nestimate, which is the best estimate of the parameters given the data (and our prior\ninformation about the parameters). To find this MAP estimate, we first note that the\ndenominator is not a function of the parameters therefore\np(θ|D) ∝p(D|θ)p(θ)\nThe MAP estimate is therefore\nθMAP = arg max p(D|θ)p(θ)\nθ\nHow do we estimate p(D|θ)? If we assume we have normally distributed errors, by the\ncentral limit theorem, we can write\np (< y > |x, θ) = (2n)-2 σ-1 exp\n\n-χ2\n\nwhere\nχ2 =\n< y > -f(x, θ)\nσ\nand σ2 = 1 (< y2 > -< y >2) is the sample standard deviation\nN\n(d) NOTE: mathematically there are clear ties between ML and Bayesian estimation of the\nparameters. The underlying difference has to do with the formulation of the problem:\nin MLE the parameters of the distribution are assumed to be fixed but unknown, in\nBayesian the parameters are treated as random variables.\nMonte Carlo Methods\n1. One of the applications of the Monte Carlo Methods is to approximate integrals for high\ndimensions.\nIf =\nZ\nf(x)dx\n(1)\nΩ\ne.g. In the case of 1-D,\nb\nIf =\nZ\nf(x)dx\n(2)\na\n2. Crude Monte Carlo Integration (Random Sampling)\n(a) Generates a vector of random variable x ∈Ωfrom a uniform PDF,\np unif\nx\n(x) =\n(3)\nm(Ω)\nwhere m(Ω) is a normalization constant such that\nR\np unif\nx\n(x)dx = 1.\nΩ\ne.g. In the case of 1-D,\np unif\nx\n(x) =\n(4)\n(b -a)\n(b) With the uniform PDF, we can then relate the integral If to the expected value of f by\nIf = m(Ω)\nZ\nf(x)p unif\nx\n(x)dx = m(Ω)\nΩ\n⟨f⟩.\n(5)\nwhere ⟨f⟩denotes average of f over the volume (or its analogue in d-dimension) Ωwhere\np unif\nx\n(x) is a uniform PDF\n(c) We determine ⟨f⟩by first evaluating f at a larger number (say N) of x's randomly\ndistributed over the volume Ωand then taking the average of these values.\nIf ≈I N\nf\n≡m(Ω)\nX\nN\nf(xi)\n(6)\nN i=1\n(d) Algorithm:\ni. Initialize I N\nf\n= 0.\nii. For i = 1, . . . , N,\n\nA. Compute a sample xi of the random vector x with unifom PDF\nB. Compute the function value fi = f(xi)\nC. Assign I N\nf\n:= I N\nf\n+ fi\niii. Return I N\nf\n:= m(Ω)I N\nf\n/N\n(e) Note that in MATLAB, you use the function rand() to sample numbers uniformly\nbetween 0 and 1.\n3. The Metropolis Monte Carlo Algorithm\n(a) The Metropolis algorithm uses a Markov process in order to construct a sequence of\nconfigurations, R1, R2, . . . that are samples of the equilibrium (or the target) PDF pr.\nStarting from some configuration R0, the collection of sampled Ri's eventually yield a\ndistribution that evolves to the target PDF after sampling many times. This idea is the\nbasis for the Markov Chain Monte Carlo (MCMC) method. The Metropolis algorithm\nis one of the sample techniques used in MCMC.\n(b) There are two conditions that need to hold for a Markov process:\ni. ergodicity: With enough steps you reach any configurations from any other config-\nurations\nii. detailed balance: At equilibrium, the transition from one state (e.g. R′) to another\nˆ\n(e.g. R) is as probable as the reverse case.\nˆ\nˆ\npr(R)T(R, R′\nˆ\n) = pr(R′)T(R′, R)\n(7)\n(c) If Neq is the number of steps taken for the Markov process to reach equilibirium, then\nthe sequence RNeq+1, RNeq+2, . . ., is a sequence of samples from the PDF pr\nˆ\n(d) The transition PDF\n(R′\nˆ\nT\n, R) (PDF of selection configuration R as a potential move,\nˆ\ngiven that the system is presently in configuration R) have two contributions: the\nselction PDF and the acceptance PDF\nˆ\nˆ\nˆ\nT(R′, R) = T s(R′, R)T a(R′, R)\n(8)\n(e) We choose T s\nˆ\nˆ\nto be symmetric such that T s(R′, R) = T s(R, R′). We can then choose\nT a that satisfies the detailed balance (7). In the Metropolis algorithm, the choice of T a\nis\na\n(\nˆ\nif pr(R) > pr(R′)\nT\n=\n(9)\nˆ\npr(R)/pr(R′)\notherwise\nStochastic Chemical Kinetics\n1. Chemical master equation\nThe master equations does NOT describe the change in X(t) (which is the state of the\nsystem as a function of time), as the continuum equations would, because this vector varies\nstochastically. Instead, the master equation describes the grand probability function for a\nseries of states n at a time t given an initial state n0 at time t0.\nP(n, t | n0, t0) = Pr(X(t) = n | X(t0) = n0)\n(r)\n\nThis probability density function is evolving in time which can be described by the ODE:\ndP(n, t | n0, t0)\nX\nM\n=\n\nP(n -νμ, t | n0, t0)hμ(n -νμ)cμ -P(n, t\nn0, t0)hμ(n)cμ\ndt\nμ=1\n|\n\nThe first part of the equation describes the transition from a different state through a reac-\ntion νμ to the current state. The second part is the remnant of an equation describing the\nprobability of staying in the same state. Thus, this encompasses all the different ways that\nwe can get to state n at time t.\nThe CME can also be stated as\ndPσ =\nX\nWσ′→σPσ′(t)\ndt\n-\nσ′\nX\nWσ→σ′Pσ(t)\nσ′\nwhere Pσ denotes the probability that the system is in configuration σ at time t and Wσ′→σ\ndenotes the rate of transition from configuration σ′ to configuration σ. The CME describes\nthe entire PDF of the state X(t) for every time t.\nThe state variables of the CME are the probabilities of every possible state of the reacting\nsystem. As a trivial example, in a system with only one species A that can create or destroy\nitself, the CME has one ODE for the probabilities of each of the states (i.e., there is 1 molecule\nof A, there are 2 molecules of A, . . ., there are j molecules of A, . . . ). When there are multiple\nreacting species, every possible combinations of molecule numbers must be accounted for, and\nthe number of ODEs in the CME can easily reach 1040 or more. Due to the high computational\ncosts, most researchers need to employ a Monte Carlo approach.\n2. Formalism of the stochastic problem\nConsider a volume V containing N chemically reacting species, S1, . . . , SN, and denote the\nnumber of molecules at each time t by the vector\nX(t) = (X1(t), . . . , XN(t)).\nThese species can undergo M chemical reactions R1, . . . , RM.\nEach Rμ reaction has an\nassociated stoichiometry vector νμ. For example, if N = 3, then the reaction S1 + S2 →S3\nhas the stoichiometry vector ν = (-1, -1, 1).\nIf the Rμ reaction occurs at tˆ, then the state vector changes according to\nX(tˆ+ dt) = X(tˆ-dt) + νμ\nwhere X(tˆ-dt) and X(tˆ+ dt) represent the number of molecules immediately before and\nafter the reaction occurs, respectively.\nThe chemical reactions are assumed to occur stochastically, according to some probability\ndistributions.\n(a) Counting number of possible Rμ reactions - In order for Rμ to occur, it is necessary\nthat one molecule of each reactant species collide with each other at some time. At\nany given time, it is possible that many different combinations can cause the reaction.\nExactly how many distinct combinations depends on how many molecules of each species\nis present i.e., on X(t).\n\nDenote the number of unique groups of reactants that could collide to cause Rμ by\nhμ(X(t)). We need to be careful that we properly count the discrete number of molecules\n(e.g., avoid double counting).\nExamples:\nS1 + S2 →S3,\nhμ(X(t)) = X1(t)X2(t)\nS1 + S\n1 →S3,\nhμ(X(t)) = 2X1(t)(X1(t) -1)\nS1 + S1 + S1 →S3,\nhμ(X(t)) = 1X1(t)(X1(t)\n-1)(X1(t) -2)\nS1 →S2,\nhμ(X(t)) = X1(t)\n(b) Fundamental hypothesis - Suppose that at least one complete group of Rμ reactants\nexists in volume V . Let πμ(t, dt) denote the probability that a particular one of these\ngroups will react in the time interval [t, t + dt].\nThe fundamental hypothesis of the\nstochastic approach to chemical kinetics is that, for each reaction Rμ, there is a constant\ncμ such that\nπμ(t, dt) = cμdt\nThis assumes that the probability that Rμ will occur in the interval [t, t + dt] increases\nlinearly with dt (for small enough duration dt). In the bimolecular case, the constant cμ\ncan be derived from the kinetic theory of gases based on the following assumptions:\n1. the positions of the molecules in V are random and uniformly distributed,\n2. the velocities of the molecules in V are distributed according to the Maxwell-Boltzmann\ndistribution.\nA consequence of the fundamental hypothesis is that the probability of multiple reaction\nevents, of any kind, in [t, t + dt] can be shown to scale as O((dt)2). Therefore, in the\nlimit as dt →0, the probability of multiple reaction events tends to zero more rapidly\nthan dt such that the probability of single reaction events dominates. For this reason,\nall single reaction events in [t, t + dt] can be treated as mutually exclusive, because only\none reaction can occur (to first order approximation).\n(c) Probability that reaction Rμ occurs in [t, t+dt] - From the fundamental hypothesis,\nwe assume that at most one reaction can occur in [t, t + dt]. To compute the probability\nthat Rμ occurs in [t, t + dt], recall that there are hμ(X(t)) distinct groups of reactants\nthat could possibly react in [t, t+dt], each with probability cμdt. Since these hμ possible\nreactions can be assumed to be mutually exclusive, the individual probabilities can be\nsummed to give\nPr(exactly 1 Rμ rxn occurs in [t, t + dt] | X(t) = n) = hμ(n)cμdt\nwhere n is a vector of N integers with the ith element being the number of molecules of\nspecies Si in the reacting system at time t.\nSince at most one reaction is allowed to occur [t, t + dt], the occurrences of each type of\nreaction R1, . . . , RM are mutually exclusive, which implies we can sum all the probabil-\nities\nM\nPr(exactly 1 rxn occurs in [t, t + dt] | X(t) = n) =\nμ\nX\nhμ(n)cμdt\n=1\n\n(d) The probability that no reactions occur in [t, t + τ] - In the stochastic view of\nchemical kinetics, there are periods of time in which nothing happens.\nIn order to\naccurately simulate this situation, some characterization is needed for when the next\nreaction will occur. Formally, we can ask: Given that X(t) = n, what is the probability\nP0(τ, n) that no reactions occur in the volume V within the time interval [t, t + τ]?\nWe first consider the probability P0(ε, n) where ε is very small. For small enough ε, at\nmost one reaction can be assumed to occur in V during [t, t + ε] such that\nP0(ε, n) = 1 -Pr(exactly 1 rxn occurs in [t, t + ε | X(t) = n)\nM\n= 1 -\nX\nhμ(n)cμε\nμ=1\nIn order to calculate P0(τ, n), we divide the interval [t, t + τ] into a large number of K\nintervals each of length ε = τ/K : [t, t + ε], [t + ε, t + 2ε], . . . , [t + (K -1)ε, t + Kε]. The\nprobability P0(τ, n) is the joint probability that reactions do not occur in every interval.\nWe can assume the process to be a Poisson process such that the probability that a\nreaction occurs in a given interval is independent of the probability of reaction occurring\nin other intervals, which implies that\nP0(τ, n) = P| 0(ε, n)P0(ε, n) · · · P0(ε, n)\n{z\n}\nK times\n=\n\n1 -\nM\nX\nμ=1\nhμ(n)cμτ\nK\n\nK\n.\nThis argument is valid for any sufficiently large K such that\nP0(τ, n) = lim\nK→inf\n\n1 -\nM\nX\nμ=1\nhμ(n)cμτ\nK\nK\n\n= exp\n\nM\n-\nh\nμ\nX\nμ(n)cμτ\n=1\n\nTherefore, the probability that no reaction occurs during [t, t+τ] depends on\n\nall reaction\nparameters {cμ} and decreases exponentially with the length of the interval τ.\n3. Kinetic Monte Carlo Simulation\nThe kinetic Monte Carlo (KMC) algorithm computes a sample from the PDF of X(t) (i.e.,\ncomputes a single trajectory in time). If the KMC algorithm is run a large number of times,\nthe the frequency of observing a state, say X(tˆ) = n∗, approaches the probability predicted\nby the CME, P(n∗, tˆ | n0, t0).\n(a) Determining the next reaction time - The KMC algorithm requires the probability\nthat the next reaction occurs in the infinitesimal time interval [t+τ, t+τ +dτ], denoted\nby Pnext(τ, n)dτ.\nThe PDF of the random variable τ (at a fixed n) is given by\nPnext(τ, n) = a(n) exp\n-a(n)τ\nwhere a(n) is defined as the total reaction propensity\n\nM\na(n) =\nμ\nX\nhμ(n)cμ\n=1\n\nDetermining the next reaction time in KMC involves sampling this PDF, which can be\ndone by generating a random number r1 from the uniform distribution on the interval\n(0, 1] and computing\nln(1/r1)\nτ =\na(n)\n(b) Determining the next reaction type - Since the possibility of multiple reactions was\nexcluded and the next reaction time was determined by sampling the PDF Pnext(τ, n),\nthere is guaranteed to be exactly one reaction that occurs in [t + τ, t + τ + dτ]. There\nare M mutually exclusive possibilities, R1, . . . , RM, each with probability\nhμ(n)cμdτ,\nμ = 1, . . . , M\nThe probability that reaction Rμ′ occurs is given by the ratio\nhμ′(n)cμ′\na(n)\nFor a fixed n, this ratio is the PDF of the random variable μ′, which can be sampled by\nselecting a number r2 from the uniform distribution on (0, 1] and choosing μ′ to be the\nsmallest integer such that\nμ′\nr2 ≤\nμ\nX\nhμ(n)cμ\n=1\na(n)\n(c) KMC Algorithm - We can state the KMC algorithm as\n1. Initialize: t = t0 and X(t) = n0.\n2. While t < tf :\n(a) Form a list of all possible rates in the system R1, . . . , RM and compute the total\nreaction propensity a(X(t)).\n(b) Sample two random numbers, r1 and r2, from the uniform distribution on (0, 1].\nln(1/r1)\n(c) Determine the reaction time as τ =\n.\na(X(t))\nμ′\nh (n)c\n(d) Determine the reaction type as the smallest μ′\nμ\n≤\nμ=1\nμ\nsuch that r2\nP\n.\na(n)\n(e) Carry out the selected reaction event by setting t := t+τ and X(t) := X(t)+νμ′.\n3. Interpreting results\nFrom the KMC algorithm, we obtain a trajectory. Depending on the system and\nwhat we want to measure, we can use one long trajectory or may need to run several\ntrajectories. To obtain estimates of the probability of a certain state ni, we need to\ncompute P(n = ni) from the trajectory by counting the frequency of each statein n\nin the total trajectory or set of trajectories weighted by the timesteps τ. To compute\nestimates of the duration of certain states or the time between each state, we iterate\nthrough the trajectories computing the time we spend in each state.\nFor simple\nproblems these can also be estimated from the transition probabilities.\nAs always, this document may have typos. Please refer to your notes for the most complete\nrecap of the course. It's been great working with all of you. Good luck with finals!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Practice Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/f8f0bcf44a46d57d479afbf93a47f3c1_MIT10_34F15_Quiz1.pdf",
      "content": "10.34 Numerical Methods Applied to Chemical Engineering\nQuiz 1\n- This quiz consists of three problems worth 20, 40 and 40 points respectively. The problem\nstatements are found on pages 2, 3 and 5 in this exam packet.\n- You have 2 hours to complete the quiz.\n- You are free to use a calculator or any notes you brought with you.\n- It is important, however, that only the scientific - arithmetic and trigonometric - function\nof the calculator is used throughout the quiz. You are on your honor not to employ any\nbuilt-in numerical linear algebra routines. This includes but is not limited to the calculation\nof determinants, eigenvalues, eigenvectors and solutions of linear equations.\n- The ends of problems 2 and 3 provide an opportunity to do a little more work and earn 2\nbonus points each. Be sure to complete the rest of the quiz before attempting these.\n\nProblem 1 (20 points)-\n1. (4 points) Create a real 2 × 2 matrix with a determinant and trace equal to 0 that is not the\nzero matrix (or explain why this is impossible)\n2. (4 points) Create a real 3×3 matrix with a determinant and trace equal to 6 (or explain why\nthis is impossible)\n3. (6 points) Create a real 4 × 4 matrix with rank 2. Propose a vector b ∈R4 such that the\nsystem of equations Ax = b has a family solutions. What is that family of solutions? Propose\na vector b ∈R4 such that the system of equations Ax = b has no solutions. Explain why\nthis is the case.\n4. (6 points) Notice that vectors from the null space of your matrix are orthogonal to the rows of\nthat matrix. This is a general property of matrices which you will prove now. In particular,\nfor a matrix A ∈RN×M, prove that vectors from the null space, N(A), are orthogonal to\nvectors from the row space, R(AT ).\n\nProblem 2 (40 points) -\nProblem statement:\nBiological signaling and regulation networks often involve cycles in which a protein backbone is\ntransformed through a collection of modified states with different numbers of phosphate groups\nattached. A basic cycle might be described by the reaction network:\nk\nA →\nk\nB →\nk\nC →\n3 A,\nwhere A, B and C have the same protein backbone with different numbers of phosphate groups. Of\ncourse, some kind of energy input is required to maintain a cycle, which is not represented above.\n\nQuestions to be answered:\n1. (2 points) Write down the stoichiometry matrix S for this reaction network.\n2. (5 points) Characterize the null space of S in terms of a dimension and a basis. What does\nthis tell you about the fluxes (reaction rates) in the network at steady state? What physical\ninterpretation can you provide for this?\n3. (5 points) Characterize the left null space of S in terms of a dimension and a basis. What\ndoes this tell you about the time evolution of the protein concentrations? What physical\ninterpretation can you provide for this?\n4. (5 points) Write down a model for the dynamics of the protein concentrations in a com-\npartment of a mammalian cell using matrix-vector notation. State any assumptions in your\nmodel.\n5. (8 points) Let k1 = 1, k2 = 2 and k3 = 1. Does your model admit a steady-state solution? If\nso, describe it physically? Is that solution stable?\nNow let us consider a cycle of length N\nk\nA →\nk\nB →\n→\nk\nC\nk\nD →\n4 · · ·\nk→\nN A,\n6. (5 points) Sketch the sparsity pattern of the stoichiometry matrix S for this N component\ncyclic system.\n7. (5 points) Write a MATLAB function that takes advantage of this sparsity pattern to compute\nthe product of S with a vector. Your function should take as an input the fluxes associated\nwith each reaction in the cycle and return the rate of change for the concentration of each\nspecies in the cycle. Be sure that your function does not compute the stoichiometry matrix\nexplicitly.\n8. (5 points) Develop an expression for the characteristic polynomial of the N component stoi-\nchiometry matrix, S. The roots represent the eigenvalues of S.\n(r)\n\nProblem 3 (40 points) -\nProblem statement:\nAn autocatalytic reaction converts A to B as\nA + 2B →3B.\nThe reaction is elementary so the net rates of consumption/formation of A and B are\nrA = -rB = -k[A][B]2.\nwhere k is the rate constant.\nIf the reaction takes place in an isothermal, continuously stirred tank, the concentrations of species\nA and B exiting the reactor at time t, denoted CA and CB, satisfy the equation:\nd\nA,f\ndt\nCA\nCB\n\n(\n=\nθ-1 C\n-CA) -kCACB\n.\n(1)\nθ-1 (CB,f -CB) + kCAC2\nB\n\nHere, θ is the residence time in the reactor and CA,f and CB,f are the concentrations of species A\nand B fed to the reactor. We seek the concentrations of A and B in the effluent when the reactor\nreaches steady-state: dCA/dt = dCB/dt = 0.\n\nProblem 3 (cont.) -\nQuestions to be answered:\n1. (4 points) Show that at steady-state, equation 1 can be written as a system of nonlinear\nequations:\n\nα\n\n1 -ˆC\n-ˆ\nˆ\nC C2\n\nA\nˆ\nˆ\n0 = (CA, CB\n\nA\nB\nf\n) =\n,\n(2)\nˆ\nα β -ˆ\nˆ\nC\n+\nB\n\nCACB\n\nˆ\nˆ\nwhere CA = CA/CA,f, CB = CB/CA,f, α = (kθC2\n\nA,f)-1, and β = CB,f/CA,f. This dimen-\nsionless form of the species balance at steady-state will prove useful in the remainder of the\nproblem.\nˆ\nˆ\nˆ\nˆ\n2. (10 points) Calculate the Jacobian, Jf(CA, CB), of the vector-valued function, f(CA, CB),\nˆ\nˆ\nfrom equation 2.\nUnder what conditions (α, β, CA, CB) will the Jacobian be singular?\nAssume these quantities take on physical values - that is, they are real and non-negative. In\neach of those circumstances, find a vector that belongs to the null space of the Jacobian.\n3. (8 points) When β = 0, no B is fed to the reactor and no A is converted.\nTherefore,\nˆ\nˆ\nthe steady-state concentrations are CA = 1, CB = 0. Sketch an algorithm that would use\nthis information to accelerate a search for the steady state solution at another value of the\nparameters (α, β), say (1, 1).\n4. (10 points) In a CSTR, an autocatalytic reaction can exhibit multiple steady-states. The\nsteady-state mass balance requires that: CA,f + CB,f = CA + CB. This can be used to recast\nequation 2 as a single, cubic equation for the steady-state concentration of A or B, which\nmay admit as few as one and as many as three solutions depending on the values of the\nparameters α and β. There may be a connected set of points (α, β) at which this bifurcation\noccurs. Describe in detail a computational approach to finding the elements of this parameter\nset - that is, an algorithm to search for values of α, β at which the CSTR begins to possess\nmultiple steady states.\nˆ\n5. (8 points) When (α, β) = (1, 1), Newton's method converges to the solution: CA\n∗= 0.2451,\nˆCB\n∗= 1.7549. You will evaluate the stability of this steady state solution to the ODE:\nd\nˆCA\nˆ\nˆ\n= f(C , C )\nˆ\nA\nB .\n(3)\ndtˆ\n\nCB\n\nUse a Taylor expansion about the steady state solution to show that:\nd ˆ\n-ˆ\nˆ\nCA\nCA\n∗\nˆ\nC\nˆ\nˆ\nA\nC∗\n= Jf(C∗, C∗)\nA\n,\nˆ\ndtˆ\nB -ˆ\n-\n(4)\nC\nCB\n∗\nA\nB\n\nˆCB -ˆCB\n∗\n\nˆ\nˆ\nˆ\nˆ\nwhen ∥(CA -CA\n∗, CB -CB\n∗)∥p ⇒0. Use this linear equation to evaluate the stability of the\nsteady state solution. Under what conditions can this result be expected to hold?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Practice Quiz 1 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/d622f928b4daa0e567ada2ea5e468667_MIT10_34F15_Quiz1solution.pdf",
      "content": "10.34 Numerical Methods Applied to Chemical Engineering\nQuiz 1\n- This quiz consists of three problems worth 20, 40 and 40 points respectively. The problem\nstatements are found on pages 2, 3 and 5 in this exam packet.\n- You have 2 hours to complete the quiz.\n- You are free to use a calculator or any notes you brought with you.\n- It is important, however, that only the scientific - arithmetic and trigonometric - function\nof the calculator is used throughout the quiz. You are on your honor not to employ any\nbuilt-in numerical linear algebra routines. This includes but is not limited to the calculation\nof determinants, eigenvalues, eigenvectors and solutions of linear equations.\n- The ends of problems 2 and 3 provide an opportunity to do a little more work and earn 2\nbonus points each. Be sure to complete the rest of the quiz before attempting these.\n\nProblem 1 (20 points)-\n1. (4 points) Create a real 2 × 2 matrix with a determinant and trace equal to 0 that is not the\nzero matrix (or explain why this is impossible)\nThere are many possible answers but all reduce to the same matrix:\n\n(4 points) for a matrix that satisfies these constraints.\n2. (4 points) Create a real 3×3 matrix with a determinant and trace equal to 6 (or explain why\nthis is impossible)\nThere are many possible answers. A diagonal matrix with trace and determinant equal to 6\nwould be:\n\n(4 points) for a matrix that satisfies these constraints.\n3. (6 points) Create a real 4 × 4 matrix with rank 2. Propose a vector b ∈R4 such that the\nsystem of equations Ax = b has a family solutions. What is that family of solutions? Propose\na vector b ∈R4 such that the system of equations Ax = b has no solutions. Explain why\nthis is the case.\nThere are many possible answers. A diagonal matrix with rank 2 would be:\n\nA value of b = (1, 0, 0, 0) would give\n\na family of solutions\n\nx = (1, 0, a, b) where a and b are\narbitrary. A value of b = (0, 0, 1, 0) would give no solution since this vector is not in the\nrange space of the matrix.\n(2 points) for a rank 2 matrix.\n(1 point each) for a vector that admits a solution and the correct family.\n(1 point each) for a vector that does not admit a solution and the reason why.\n4. (6 points) Notice that vectors from the null space of your matrix are orthogonal to the rows of\nthat matrix. This is a general property of matrices which you will prove now. In particular,\nfor a matrix A ∈RN×M, prove that vectors from the null space,\nT\nN(A), are orthogonal to\nvectors from the row space, R(A ).\nLet y be a vector in the null space of A so that Ay = 0. Let z be a vector from the row\nspace of A so that z = AT x. Then, yT z = yT AT x = xT (Ay) = 0. Thus vectors from the\n\nnull space are orthogonal to vectors from the row space.\n(2 points each) for correct definitions of null space and row space applied somewhere in the\nproblem.\n(2 points) for proving that the spaces are orthogonal.\n(-3 points) for proving this using rows of matrix A but not generalizing to any vector in the\nrow space\n\nProblem 2 (40 points) -\nProblem statement:\nBiological signaling and regulation networks often involve cycles in which a protein backbone is\ntransformed through a collection of modified states with different numbers of phosphate groups\nattached. A basic cycle might be described by the reaction network:\nk\nA →\nk\nB →\nk\nC →\n3 A,\nwhere A, B and C have the same protein backbone with different numbers of phosphate groups. Of\ncourse, some kind of energy input is required to maintain a cycle, which is not represented above.\n\nQuestions to be answered:\n1. (2 points) Write down the stoichiometry matrix S for this reaction network.\nS =\n-1\n\n-1\n-1\n\n(2 points) for the correct matrix.\n2. (5 points) Characterize the null space of S in terms of a dimension and a basis. What does\nthis tell you about the fluxes (reaction rates) in the network at steady state? What physical\ninterpretation can you provide for this?\nThe null space has dimension 1 and a basis: (1, 1, 1). This indicates that the fluxes are equal\nat steady state.\n(3 points) for the correct dimension\n(2 points) for a correct basis\n3. (5 points) Characterize the left null space of S in terms of a dimension and a basis. What\ndoes this tell you about the time evolution of the protein concentrations? What physical\ninterpretation can you provide for this?\nThe left null space has dimension 1 and a basis: (1, 1, 1). This indicates that the sum of the\nprotein concentrations is constant in time.\n(3 points) for the correct dimension\n(2 points) for a correct basis\n4. (5 points) Write down a model for the dynamics of the protein concentrations in a com-\npartment of a mammalian cell using matrix-vector notation. State any assumptions in your\nmodel.\ndc\nk1\n= S\nk2\nc\ndt\n\nk3\n\nThis assumes that the cell is well mixed and acts like a\n\nbatch reactor with constant volume,\nthe cell is isothermal (so that ki can be treated constant), the reactions are elementary and\nfirst-order, and that protein neither enters nor leaves the cell.\n(3 points) for a model of a batch reaction.\n(2 points) for model assumptions and some discussion of the limits of applicability.\n5. (8 points) Let k1 = 1, k2 = 2 and k3 = 1. Does your model admit a steady-state solution? If\nso, describe it physically? Is that solution stable?\n\nThe model does admit a steady state solution since\n\nk\n\n-1\ndet\nS\nk2\n= det\n-2\nk3\n\n|\n-1\n\n{z\n= 0.\nSK\nThis solution is characterized by the null space of the above matrix,\n}\nwhich is easily observed\nto be of dimension 1 with a basis of (2, 1, 2). This follows from r1 = r2 = r3 at steady state,\nwhich implies k1CA = k2CB = k3CB such that CA = 2CB = CB at steady state (this fits with\nthe null space of the SK matrix). That is, the steady state solution has A and C with equal\nconcentrations with values twice that of the concentration of B. The stability of the solution\ncan be determined by examining the eigenvalues of the above matrix. These are described by\nthe secular polynomial:\n\nk1\ndet S 0\nk2\n\n-λI\n\n= (-k1 -λ)(-k2 -λ)(-k3 -λ) + k1k2k3\nk3\n= -λ(λ2 + (k1 + k2 + k3)λ + k1k2 + k1k3 + k2k3).\nTherefore, λ = 0 or\nλ = -(k1 + k2 + k3) ±\np\n(k1 + k2 + k3)2 -4(k1k2 + k1k3 + k2k3)\n= -4 ±\np\n42 -4(2 + 1 + 2)\n= -4 ± √-4\n= -2 ± i\nWe see that we do not have any eigenvalue with positive real part, so the system is stable. One\neigenvalue is zero, so the system is neutrally stable. The other two eigenvalues are complex,\nwhich induces oscillations in the solution. However, these oscillations decay (because the real\npart of these complex eigenvalues are negative) and thus the system will approach a (non-\nzero) steady state.\n(4 points) for recognizing the steady state solution and describing it.\n(2 points) for calculating the eigenvalues of SK\n(2 points) for identifying that the eigenvalues have non-positive real parts and thus the steady-\nstate solution is stable.\nNow let us consider a cycle of length N\n→\nk\n→\nk\nA\n1 B\nk\nC →\nk\nD →\n4 · · ·\nk→\nN A,\n\n6. (5 points) Sketch the sparsity pattern of the stoichiometry matrix S for this N component\ncyclic system.\nThe sparsity pattern has a diagonal element, an element below the diagonal, and a single\nelement in the first row and last column.\n\n-1\n. . .\n\n.\n\n-\n.\n.\n\n.\n\n.\n-1\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n. . .\n-1\n\n(3 points) for a sparsity pattern that includes the diagonal and lower diagonal\n(2 points) for the additional element in row 1, column N.\n7. (5 points) Write a MATLAB function that takes advantage of this sparsity pattern to compute\nthe product of S with a vector. Your function should take as an input the fluxes associated\nwith each reaction in the cycle and return the rate of change for the concentration of each\nspecies in the cycle. Be sure that your function does not compute the stoichiometry matrix\nexplicitly.\nfunction y = sparse_mult( x )\ny(1) = -x(1) + x(end)\nfor i = 2:length(x)\ny(i) = x(i-1) - x(i);\nend;\n(1 points) for a working function\n(4 points) for the sparse implementation\n8. (5 points) Develop an expression for the characteristic polynomial of the N component stoi-\nchiometry matrix, S. The roots represent the eigenvalues of S.\ndet(S -λI) = (-1 -λ)M11 + (-1)N-1M1N = 0\nBecause the minors are determinants of diagonal/upper triangular matrices, these are simple\nto calculate: M11 = (-1 -λ)N-1, M1N = 1.\nTherefore the secular polynomial satisfies\n(-1 -λ)N + (-1)N-1 = 0.\n(3 points) for utilizing the determinant defined in terms of minors.\n(2 points) for arriving at the correct equation.\n(r)\n\nProblem 3 (40 points) -\nProblem statement:\nAn autocatalytic reaction converts A to B as\nA + 2B →3B.\nThe reaction is elementary so the net rates of consumption/formation of A and B are\nrA = -rB = -k[A][B]2.\nwhere k is the rate constant.\nIf the reaction takes place in an isothermal, continuously stirred tank, the concentrations of species\nA and B exiting the reactor at time t, denoted CA and CB, satisfy the equation:\nd\nA,f\ndt\nCA\nCB\n\n(\n=\nθ-1 C\n-CA) -kCACB\n.\n(1)\nθ-1 (CB,f -CB) + kCAC2\nB\n\nHere, θ is the residence time in the reactor and CA,f and CB,f are the concentrations of species A\nand B fed to the reactor. We seek the concentrations of A and B in the effluent when the reactor\nreaches steady-state: dCA/dt = dCB/dt = 0.\n\nProblem 3 (cont.) -\nQuestions to be answered:\n1. (4 points) Show that at steady-state, equation 1 can be written as a system of nonlinear\nequations:\nˆ\nˆ\n0 = f(CA, CB) =\n\nα 1 -ˆCA\n\n-ˆ\nˆ\nCAC2\nB\n\n,\n(2)\n-ˆ\nˆ\nˆ\nα β\nC\n+ CAC2\nB\nB\nˆ\nˆ\nwhere C\n= C\nA\nA/CA,f, CB = CB/CA,f, α = (kθCA,f)-, and β = CB,f/CA,f. This dimen-\nsionless form of the species balance at steady-state will prove useful in the remainder of the\nproblem.\nThis is a trivial exercise in setting the derivative equal to zero and factoring the resulting\nequation.\nd(C\n)\n(1 p\nt) for stating\nA,C\noin\nB = 0 at steady state\ndt\n(3 points) for arriving at the correct equation\nˆ\nˆ\nˆ\nˆ\n2. (10 points) Calculate the Jacobian, Jf(CA, CB), of the vector-valued function, f(CA, CB),\nˆ\nˆ\nfrom equation 2.\nUnder what conditions (α, β, CA, CB) will the Jacobian be singular?\nAssume these quantities take on physical values - that is, they are real and non-negative. In\neach of those circumstances, find a vector that belongs to the null space of the Jacobian.\nThe Jacobian is\n-\nˆ\nα -ˆC2\nB\n-ˆ\n2CACB\nJ =\nˆC2\nB\n-\nˆ\nˆ\nα + 2CACB\n\n.\nThe Jacobian is singular when\nˆ2\n-\nˆ\nˆ\nˆ\nˆ\n0 = det J = (α + CB)(α\n2CACB) + 2CAC3\nˆ\nˆ\nˆ\nB = α2 + αCB(CB -2CA).\nSo either α = 0 or\nˆCA = 2\nα\nˆ\n+ CB\n.\nˆCB\n\nWhen α = 0, the Jacobian is\nJ =\n-ˆC2\nB\nˆ2\n-ˆ\nˆ\n2CACB\nˆ\nˆ\nCB\n2CACB\n\n.\nˆ\nˆ\nVectors in the null space of this matrix are proportional to (1, -CB/(2CA)).\nˆ\nˆ\nˆ\nˆ\nWhen CA = (α/CB + CB)/2, with CB = 0, the Jacobian is\nJ =\n-α -ˆC2\nB\n-α -ˆC2\nB\nˆC2\nˆ\nB\nC2\nB\n\n.\n\nVectors in the null space of this matrix are proportional to (1, -1).\n(5 points) for the correct Jacobian.\n(1 point) for correct expression for detJ=0\n(1 point each) for the two conditions that make the determinant zero.\n(1 point each) for two correct null space vectors.\n3. (8 points) When β = 0, no B is fed to the reactor and no A is converted.\nTherefore,\nˆ\nˆ\nthe steady-state concentrations are CA = 1, CB = 0. Sketch an algorithm that would use\nthis information to accelerate a search for the steady state solution at another value of the\nparameters (α, β), say (1, 1).\nDescribe how to use continuation to track from an initial set of parameters (α, β) = (1, 0) to\n(1, 1). Sketch out the loops that would be employed and specify the solution method for the\nnonlinear equation.\n(4 points) for recognizing that continuation is the appropriate procedure.\n(4 points) for a working sketch of the algorithm that includes the correct initial state (1 point),\nthe search space (1 point) and details for the solution method of f = 0 using the previous\nsolution as the initial condition (2 points).\n4. (10 points) In a CSTR, an autocatalytic reaction can exhibit multiple steady-states. The\nsteady-state mass balance requires that: CA,f + CB,f = CA + CB. This can be used to recast\nequation 2 as a single, cubic equation for the steady-state concentration of A or B, which\nmay admit as few as one and as many as three solutions depending on the values of the\nparameters α and β. There may be a connected set of points (α, β) at which this bifurcation\noccurs. Describe in detail a computational approach to finding the elements of this parameter\nset - that is, an algorithm to search for values of α, β at which the CSTR begins to possess\nmultiple steady states.\nThe key here is to recognize that det J = 0 at a bifurcation point. Since we are searching a\ntwo dimensional parameter space, there are a number of different ways to effect a search. In\none, you might fix the value of α and change β until you find a sign change in det J. Then\nyou could solve the augmented equations to find the exact value of β at which the bifurcation\noccurs. This could be repeated for many values of α. A continuation procedure using the\nexact value of β and the steady state solution at a found bifurcation point could be used to\nexpedite the search. Since not all values of α will necessarily have a bifurcation, we could\nsearch for where the Jacobian of the augmented equations become singular. This will be the\nfirst point in the space (α, β) at which multiple steady states begin to exist. For this method,\nˆ\nˆ\nwe find the solution (CA, CB, α, β) to\n\nf\n\ndet J\n\ndet\n\nJ\ndf/dβ\n\n∇det J\nd det J/dβ\n\n= 0,\nwhere ∇\nˆ\nˆ\nis the gradient with respect to (CA, CB). This is the first value of α, β at which\nˆ\nˆ\nthe bifurcation occurs. One can show that this critical point is CA = 3/4, CB = 3/8, α =\n\n27/64, β = 1/8. The null space of the Jacobian of these augmented-augmented equations will\nsuggest the directions to perturb a search for the bifurcation throughout the parameter space.\n(4 points) For describing the correct condition for a bifurcation.\n(4 points) For detailing an algorithm that searches the parameter space for this point is some\nfashion.\n(2 points) For describing a solution of the augmented equations f = 0, det J = 0 to find the\nexact value of either α or β at which the bifurcation occurs given a corresponding value of β\nor α, respectively.\nˆ\n5. (8 points) When (α, β) = (1, 1), Newton's method converges to the solution: CA\n∗= 0.2451,\nˆCB\n∗= 1.7549. You will evaluate the stability of this steady state solution to the ODE:\nd\nˆCA\nˆ\nˆ\n= f(C , C )\nˆ\nA\nB .\n(3)\ndtˆ\n\nCB\n\nUse a Taylor expansion about the steady state solution to show that:\nd\nA\nd\nˆCA -ˆC∗\ntˆ\nˆCB -ˆCB\n∗\n\nˆ\nˆ\n∗\n= Jf(CA\n∗\nˆ\n, CB\n∗)\nCA -ˆCA\nˆCB -ˆCB\n∗\n\n,\n(4)\nwhen ∥ˆ\n(CA -ˆCA\n∗\nˆ\n, CB -ˆCB\n∗)∥p ⇒0. Use this linear equation to evaluate the stability of the\nsteady state solution. Under what conditions can this result be expected to hold?\nˆ\nˆ\nIt is important to recognize that f(CA\n∗, CB\n∗) = 0 since it is the steady state solution. Addi-\ntionally, d/dt(CA\n∗, CB\n∗) = 0 since the steady state solution is independent of time.\nd\ndˆt\nˆCA -ˆC∗\nA\nˆCB -ˆC∗\nB\n\n= d\ndˆt\nˆCA\nˆCB\n\n-\n\n*0\nd\nˆ\nf\nd\nˆCA\n∗\ntˆ\nˆCB\n∗\n\nˆ\n= (CA, CB)\nˆ\nˆ\nThen we apply a Taylor series expansion about (CA\n∗, CB\n∗) to the right hand side of the above\nequation:\nˆ\nˆ\nC\nC\nˆ\nˆ\nˆ\n:\nˆ\nˆ\nˆ\n∗\nf(CA, CB) ≈f(\nCA\n∗, CB\n∗) + Jf(CA\n∗, CB\n∗\nA\n)\n\n-\nA\nˆCB -ˆCB\n∗\n\n+ O(Cˆ 2),\nˆ\nˆ\nˆ\nˆ\nwhere Cˆ = (CA, CB). In the limit of small ∥(CA -CA\n∗\nˆ\nˆ\n, CB -CB\n∗)∥p, we neglect the 2nd and\nhigher order terms. Combining the results from the above two equations, we arrive at the\nfinal expression,\nd ˆCA -ˆCA\n∗\nˆ\nˆ\nC\nˆ\nˆ\nˆ\nˆ\nA\nC∗\n= f(C , C ) = J (C∗, C∗)\n-\nA\ndtˆ\nˆC\n-ˆ\nA\nB\nf\nA\nB\nB\nCB\n∗\n\nˆCB -ˆCB\n∗\n\nThe eigenvalues of the Jacobian dictate the stability since this is a linear differential equation:\nˆ\nˆ\nˆ\ndet(J -λI) = (-α -CB\n∗-λ)(-α + 2CA\n∗CB\n∗-λ) -(-ˆ\n2CA\n∗ˆCB\n∗\nˆ\n)(CB\n∗)\nˆ∗2\n-\nˆ∗ˆ\nˆ\nˆ\nˆ\nˆ\nˆ\n= λ + (α + CB + α\n2CACB\n∗)λ + 2CACB\n∗+ (α + CB\n∗)(α -2CA\n∗CB\n∗) = 0\n⇒\nˆ\nλ = -α, -\nˆ\nα + 2CACB\n∗-ˆC∗2\nB = -1, -3.219\n\nThe eigenvalues are negative, so that the perturbation to the steady state solution: (CA -\nCA\n∗, CB -CB\n∗) decays to zero exponentially in time. This means the steady state is stable, but\nˆ\nˆ\nˆ\nonly to small perturbations since a Taylor expansion in the limit of small ∥(CA -CA\n∗, CB\nˆ\n-\nCB\n∗)∥p was used to evaluate the stability.\nˆ\n(1 point) for f(CA\n∗\nˆ\n, CB\n∗) = 0.\n(3 points) for the Taylor expansion\n(1 point) for stating that stability of the steady state solution is determined by the eigven-\nvalues of the Jacobian matrix\n(3 points) for calculating the eigenvalues and determining the stability of the system\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Quiz 1 Review",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/c49f43ba121c3d8ac5bb82252112b90c_MIT10_34F15_ReviewQuiz1.pdf",
      "content": "10.34\nNumerical Methods Applied to Chemical Engineering\nFall 2015\nQuiz #1 Review\nStudy guide based on notes developed by J.A. Paulson, modified by K. Severson\nLinear Algebra\nWe've covered three major topics in linear algebra: characterizing matrices, linear equations, and\neigenvalue problems. We'll start with some preliminaries for notation in linear algebra and then\nreview each of these topics.\nVectors\n1. A vector is an ordered set of numbers e.g. x = [x1, . . . xN]\n2. Transpose operator\nx\n\nx2\nx = .\nx\n.\n\n.\n,\nxT =\n\nx1\n· · ·\nxN\nxN\n\n.\nN\n3. Inner product given by xT y = x · y =\nX\nxiyi. Only works for same size vectors and the\ni=1\nresult is a scalar.\n4. Outer product (dyadic) given by xyT = x ⊗y. Can have any size vectors and the result is a\nmatrix.\nMatrices\na11\na12\n· · ·\na1M\na21\na22\na2M\n1. A matrix has ordered sets of numbers, e.g. A =\n\n.\n.\n·\n.\n· ·\n.\n. A matrix repre-\n.\n.\n.\n.\n\n.\n.\n.\n.\naN1\naN2\n\n· · ·\naNM\n\nsents a transformation. It can also be thought of as a map between vector\n\nspaces.\n2. Given C = AT , the elements of C are Cij = Aji.\nN\n3. Trace is denoted Tr A =\nX\nAii (sum of diagonals). Only valid for square matrices!\ni=1\n4. Matrix-vector product y = Ax. For A ∈RN×M, this scales as O(NM). Column-view,\ny1\n\nA11\nA12\n· · ·\nA\nx\nA\n\nA\n\nA\n\n1M\n\ny2\nA21\nA22\nM\n.\n=\n.\n.\n·\n.\n· ·\nA2\nx2\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nyN\nAN1\nAN2\nANM\n\n.\nxM\n\n1M\n\n= x1\nA21\n. + x2\nA22\n\nA2M\n. +\n+\n.\n.\n.\n.\n· · ·\nxM\n\n...\n\n· · ·\nAN1\nAN2\n\nANM\n\n5. Matrix-matrix product C = AB. For A ∈RN×M and B ∈RM×P , this scales as O(NMP).\nRemember properties in Lecture 1 slide 26.\n6. Matrix inverse A-1A = AA-1 = I. Exists if A is non-singular.\n7. Determinant of a matrix det(A) can be defined in terms of its minors,\nN\ndet(A) =\nX\n(\ni=1\n-1)i+j\n|{z}\nAij\nMij(A)\ncofactor\n| {z\nminor\nwhere the minor Mij(A) is simply the matrix A with the ith ro\n}\nw and jth column removed.\nWhen A ∈RN×N, then Mij(A) ∈RN-1×N-1. Only valid for square matrices! Remember\ndeterminant properties e.g., det(AT ) = det(A) and det(AB) = det(A)det(B).\nCharacterizing vectors and matrices\n1. Vector norms\n(a) A vector norm maps a vector to a scalar and has three properties:\n∥x∥≥0 , ∥x∥= 0 iffx = 0\n∥x+x∥≤∥x∥+∥y∥Note that this property is referred to as the triangle inequality.\n∥αx∥= |α|∥x∥where α ∈C\nN\n1/p\n(b) The most commonly used vector norms in our class are p-norms: ∥x∥p =\nX\ni=1\n|xi|p\nwhere p\n\n≥1.\n2. Matrix norms\n(a) A matrix norm also maps a matrix to a scalar.\n(b) In class we discussed the induced norm\n∥A∥p = maxx=0\n∥Ax∥p\n∥x∥p\n(c) The induced norm can be interpreted as \"how much can can A stretch x\"\n(d) We looked at several specific induced norms:\n∥A∥2 = max σP\ni where σi are the singular values of A\n∥A∥\nM\n= max\ninf\nj=1 |aij|\n∥A∥1 = max PN\ni=1 |aij|\n(e) We also used several properties of matrix norms:\nCauchy-Schwarz: ∥AB∥p ≤∥A∥p∥B∥p.\nTriangle inequality: ∥A + B∥p ≤∥A∥p + ∥B∥p.\n3. Condition Number\n(a) The condition number, κ(A), is a measure of how numerical error is magnified in solu-\ntions of linear equations; log10κ(A) give the number of digits lost.\n(b) For invertible matrices, κ(A) = ∥A∥∥A-1∥. In the 2-norm, κ(A) = σmax\nσmin\n(c) For a unitary matrix, Q, κ(Q) = 1.\n\n4. Singular Matrices A matrix, A ∈RN×N, that is nonsingular is invertible. There are many\nequivalent descriptions of a nonsingular matrix, some of which are listed below.\n(a) rank(A) = N e.g. the matrix has full rank\n(b) det(A) = 0\n(c) 0 is not an eigenvalue of A\n(d) 0 is not a singular value of A\n(e) A has a trivial null space\n(f) The dimension of the range space of A is N\n(g) The columns of A of linearly independent\n5. Vector spaces and the four fundamental subspaces\n(a) A vector space is a \"special\" set of vectors that satisfy the following properties:\nClosed under addition: x, y ∈S =⇒x + y ∈S.\nClosed under scalar multiplication: x ∈S =⇒cx ∈S.\nContains the null vector: 0.\nHas an additive inverse: x ∈S =⇒-x ∈S : x + (-x) = 0.\nM\n(b) Column/range space: R(A) = span{Ac, Ac, . . . , Ac } =\nX\nλ Ac\n|\nλ ∈R, Ac\nM\ni\ni\ni\ni\ni=1\n∈\nRN\n\n.\n(c) Null space: N(A) =\n\nx ∈RM : Ax = 0 .\nN\n(d) Row space: R(AT ) = span{Ar\n1, Ar\n2, . . . ,\n\nAr\nM} =\nX\nλiAr\ni\n| λi ∈R, Ar\ni ∈RM\n\n.\n(e) Left null space: N(AT ) =\n(f) The rank of a matrix is\n\ni=1\nx ∈RN : AT x = 0\n\n.\nthe dimension of its column space, r = dim R(A), r ≤\nmin(N, M).\n(g) The rank nullity theorem states, dim N(A) = M -r\nLinear equations, Ax = b\nWe spent a lot of time analyzing the linear equation, Ax = b where A ∈RN×M, x ∈RM and\nb ∈RN.\n1. Existence and uniqueness of solutions\n(a) A solution exists iffb ∈R(A)\n(b) A solution is unique iffdim N(A) = 0\n(c) From our list of nonsingular synonyms, we know this means that the matrix is square\nand invertible if the solution exists and is unique.\n\n2. Gaussian elimination transforms a matrix into its upper triangular form. It takes O(N3)\noperations. Pivoting of the rows may be required to avoid pivots equaling zero or to help\nwith numerical stability. The upper triangular system is solved using back substitution which\ntakes O(N2) operations.\n(a) When applying Gaussian elimination to sparse systems, we want to try and exploit\nthe sparsity pattern. At a minimum, we can use a special storage matrix to decrease\nthe memory requires (this was what we did in HW1).\nHowever, applying Gaussian\nelimination will lead to fill-in, which is undesirable. One way to avoid fill-in is to re-\norder the matrix by using permutations. Note that this is a type of preconditioning. A\npermutation matrix looks like\n· · ·\n· · ·\nP =\n· · ·\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n· · ·\n\n(b) Eventually, N is too large and Gaussian elimination is not reasonable. At this point, you\nwill consider an iterative method such as Jacobi iterations or Gauss-Seidel iterations.\nThe idea of these methods is to turn a hard problem into many easier problems.\ni. Jacobi iterations split A in D+R where D are the diagonal elements and R are the\noff-diagonal elements. Jacobi iterations work well for diagonally dominant systems,\ni.e. ∥D-1R∥< 1.\nii. Gauss-Seidel iterations split A in L + U where L are the lower triangular elements\nand U are the upper triangular elements (excluding the diagonal). Gauss-Seidel\nconverges if ∥L-1U∥< 1.\n3. Singular Value Decomposition\n\n(a) We can write A = UΣV+ where V+ = VT denotes the conjugate transpose. Here Σ is\na diagonal matrix with elements Σ2\nii = λi(A+A), V is a matrix whose columns contain\nthe eigenvectors of A+A, and U is a matrix whose columns contain the eigenvectors of\nAA+.\n(b) σi = Σii are the singular values of A.\n(c) Like an eigendecomposition for non-square matrices A ∈RN×M.\n(d) Can be useful for things such as data compression/matrix approximation.\nEigenvalue problems, Av = λv\nThe eigenvectors of a matrix are special vectors that are \"stretched\" on multiplication by the\nmatrix. They solve the equation Av = λv where A ∈RN×N, v ∈CN and λ ∈C. This is a\nnonlinear system of equations with N equations and N + 1 unknowns, therefore eigenvectors are\nnot unique.\n1. To solve the eigenvalue problem, we solve det(A -λI) = 0 = pN(λ) where pN(λ) is the\ncharacteristic polynomial. The roots of this polynomial are the eigenvalues.\n\n2. Complex eigenvalues also appear in conjugate pairs.\n3. For a diagonal or triangular matrix, the eigenvalues are the diagonal elements.\n4. The algebraic multiplicity of an eigenvalue is the number of times it is repeated.\n5. The geometric multiplicity of an eigenvalue is the number of linearly independent eigenvectors\nthat correspond to the eigenvalue, i.e. dim N(A-λI). If the eigenvalue is unique, there is only\none corresponding eigenvector but if algebraic multiplicity is M, 1 ≤dim N(A -λI) ≤M.\n6. A can be written AV = VΛ\nλ\nA\n\nv1\nv2\n· · ·\nvN\n\n=\n\nv1\nv2\n· · ·\nvN\n\nλ2\n\n...\nλN\n\nIf A has a complete set of eigenvectors, V is invertible and A is diagonalizable, A = VΛV-1.\n7. If A is real and symmetric, it has a complete set of orthonormal eigenvectors and the matrix\nof eigenvectors is unitary.\nSystems of Nonlinear Equations\nWe've really already seen this before because we solved eigenvalue problems however that was only\nfor polynomials. Let's consider a general system of nonlinear equations.\nf(x) = 0\nwhere f : RN 7→RN and x ∈RN. Given some f(x), we want to find roots x⋆such that f(x⋆) = 0.\nThere could no solutions, one locally unique solution, many locally unique solutions or infinite\nsolutions.\n1. We will assume that in the neighborhood close to a root, the function is approximately linear.\nLinearizing a general nonlinear function around x0 using the Taylor series method gives,\nf(x) = f(x0) + J(x0)(x -x0) + O(∥x -x0∥2\n2)\nwhere J(x) is the Jacobian and is defined as\nJ(x) =\n∂f1\n\n∂x1\n∂f1\n∂x2\n· · ·\n∂f1\n∂xN\n∂f2\n∂x1\n∂f2\n∂x2\n· · ·\n∂f2\n∂xN\n...\n...\n· · ·\n...\n∂fN\n∂x1\n∂fN\n∂x2\n· · ·\n∂fN\n∂xN\n\nAs analytic forms of the Jacobian J(x) are difficult (or impossible) to come by, we might\nhave to turn to an alternative method for approximating J(x). One simple method is \"finite\ndifferencing\" such that the columns of the Jacobian can be evaluated as,\nf\nJC\n(x + εej)\nj (x) =\n-f(x)\nε\nwhere ej =\n\n· · ·\n· · ·\nT\nis all zeros except for a 1 in the jth element. This\nrequires 2N function evaluations to compute\n\n(2 for the difference in the numerator and N for\neach column).\n2. Convergence criteria are needed to determine when to stop.\n(a) ∥f(xi+1)∥p ≤ε\n(b) ∥xi+1 -xi∥p ≤εR∥xi+1∥p + εA\n(c) The relative tolerance will dominate when the norm of x is large while the absolute\ntolerance will dominant when the norm of x is small.\n3. The rate of convergence can be examined based on,\np\nlim ∥xi+1 -x⋆∥\nk→inf\n=\n∥xi -x⋆∥q\nC\np\nq = 1 and 0 < C < 1 is linear convergence, q > 1 (or q = 1 and C = 0) is super-linear\nconvergence, and q = 2 is quadratic convergence.\n4. Newton-Raphson method\n(a) The iterative map is,\nxi+1 = x\ni -[J(xi)]-f(xi)\nJ(xi) is not actually inverted! You solve a linear system of equations and should take\nadvantage of sparsity when possible.\n(b) Newton-Raphson converges quadratically when the Jacobian is non-singular and quadratic\nconvergence is guaranteed only when the iterates are sufficiently near the root. This\nmeans good initial guesses are essential to the success of Netwon-Raphson. Bad initial\nguesses can lead to very chaotic behavior in your iterates and may not converge at all.\n(c) Broyden's method is a special case of Newton-Raphson in which the secant method is\nused to develop a coarse approximation of the derivative.\nThis method uses rank-1\napproximation for the Jacobian,\nf(xi)(x\ni\n-\nT\ni\nxi\nJ(xi) = J(x ) +\n-1)\n∥xi -xi+1∥2\nThis method is called rank-one because the outer product of two vectors, f(xi) and\n(xi-xi-1), has rank of one. In fact, every column of f(x\nT\ni)(xi-xi-1)\nis a scalar multiple\nof f(xi). This method allows us to generate an iterative formula for the Jacobian inverse,\nJ(x\n)-1f(x )(x -x\n)T J(x\n)-1\ni\n)\n-\ni\ni -1\ni-1\ni\ni\nJ(x\n= J(\n)\n1 -\n-1\n-1\nxi-1\n.\n∥xi -xi\n1∥2\n2 + (x\n-\ni -xi-1)T J(x\ni-1)-f(xi)\nthat saves us computation time at the cost of accuracy.\n\n(d) Damped Newton-Raphson introduces a scaling factor to the NR iterative map,\nxi+1 = xi -α[J(xi)]-1f(xi)\nwhere α = arg min∥f(xi-α[J(xi)]-1f(xi))∥p (find the optimal α value that decreases the\n0≤α≤1\nfunction value as much as possible in a single step). Finding this damping factor is as\nhard as finding the root. An approximate method is to use a line search that continually\nreduces α by 2 until the step decreases the function value. The damped Newton-Raphson\nis globally convergent but may converge to roots or local minima/maxima.\n5. Continuation, Homotopy, and Bifurcation\n(a) Continuation transforms the hard problem we want to solve to an easier one by in-\ntroducing/varying a parameter. Knowing that you found all the roots is not always\neasy/possible.\n(b) Homotopy is the transformation from one problem to another. We seek the roots x⋆(λ)\nto the following system of equations,\nh(x, λ) = λf(x) + (1 -λ)g(x)\nwhere h(x, 0) = g(x) such that x⋆(0) are the roots of g(x) and h(x, 1) = f(x) such that\nx⋆(1) are the roots of f(x). We create a smooth transition from g(x) to f(x) by varying\nλ in small discrete increments {λi} from 0 to 1 where the solution x⋆(λi) is used as the\ninitial guess for obtaining x⋆(λi+1).\n(c) During the homotopy procedure, the Jacobian of h(x⋆, λ) at some λ, denoted Jh(x⋆(λ), λ),\ncan become singular such that the det(Jh(x⋆(λ), λ)) = 0. This can be indicative of two\nphenomena: turning points and bifurcations.\nA turning point is when the solution\nbranch begins to curve back such that the branch was being traced with increasing λ\nsuddenly needs to be traced with a decreasing λ. One way to account for this is to\nparameterize the roots and homotopy parameter in terms of distance s traveled along\nthe solution curve/branch i.e., x⋆(λ(s)) and λ(s). We can use the arclength constraint\nto determine how to change the homotopy parameter,\nd\ndsx⋆(λ(s))\n\n+\nd\ndsλ(s)\n= 1\nNote that the change in the parameter d λ(s) appears as squared so we need some policy\nds\nfor determining what sign (positive/increasing or negative/decreasing) to take.\n(d) A bifurcation is when additional solutions appear continuously at some λ, i.e., a problem\nswitches from having 1 solution to many solutions as λ is varied. This happens at a λ\nwhen det(Jh(x⋆(λ), λ)) = 0.\nOptimization\nOptimization problems consider\nmin f(x)\nargmin f(x)\nx∈D\nx∈D\nwhere f(x) is the objective function, x are the \"design alternatives\" and D is the feasible set.\n\n1. Maximizing f(x) is equivalent to minimizing -f(x) so we only need to look at one (commonly\nminimization).\n2. The goal is to find x⋆∈D such that f(x⋆) < f(x) for all x ∈D. Solution is not necessarily\nunique as there could be multiple x⋆in D. If f(x) is convex, it has a single global minimum.\n3. If D is a closed set, the problem of finding the minimum is called constrained optimization. If\nD is an open set RN, the problem of finding the minimum is called unconstrained optimization\n(x can take on any values in RN; no restriction).\n4. Unconstrained optimization problems have a critical point when the gradient g(x) = ∇f(x) =\n0 (assuming the function is differentiable).\n5. For a point to be a minimum, all the eigenvalues of the Hessian at the minimum H(x⋆) must\nbe positive. If the eigenvalues are all negative, x⋆is a local maximum. If the eigenvalues are\nboth positive and negative, then x⋆is a saddle point. If none of these are satisfied, the test\nis inconclusive and higher derivatives must be checked to characterize the critical point.\n6. Steepest Descent\n(a) The direction of steepest descent is: di = -g(xi). This makes you go downhill fastest.\n(b) Iterative map: xi+1 = xi -αig(xi).\n(c) For small values of αi, the iterates continue to reduce the function value until g(xi) = 0\n(near zero within a norm tolerance).\n(d) Ideally, we would pick αi to lead to the smallest value of f(xi+1) but this is its own\noptimization that is not easy.\nWe can estimate an optimal αi using a Taylor series\nexpansion of f(x) about xi evaluated at xi+1,\ng(xi)T g(xi)\nαi = g(xi)T H(xi)g(xi)\nThis will be the exact optimal αi if f is quadratic with respect to xi.\n(e) Converges to local minima and saddle points so need to check Hessian to be sure critical\npoint is minima.\n7. Conjugate Gradient Method\n(a) Considers the following minimization\nmin f(x) =\nx\nxT Ax -bT x = ∥Ax\n-b∥2\nwhich can be derived analytically to be g(x⋆) = Ax⋆-b = 0 with H(x) = A. Nice\nway to solve linear equations using optimization instead of direct methods like Gaussian\nelimination.\n(b) Iterative map: xi+1 = xi -αip(xi).\n(c) Chooses descent directions (not necessarily steepest descent), p1, p2, .., pN, that are said\nto be conjugate, i.e. pT\ni+1Api = 0.\n(d) Used to solve linear equations in O(N) operations (only for symmetric positive definite\nmatrices). The actual matrix is never needed as we only need to compute its action on\nvectors Ay.\n\n(e) More sophisticated variations of the conjugate graident methods exists for non-symmetric\nmatrices (e.g,. biconjugate gradient method) and non-linear equations.\n8. Newton-Raphson for Optimization\n(a) Finding local minima in optimization is equivalent to finding the roots of the gradient\ng(x) = ∇f(x) = 0. We can imagine applying the exact same Newton-Raphson tech-\nniques discussed above for the gradient. In this case, the Jacobian of the gradient is the\nHessian of the function. The Newton-Raphson map for optimization is then,\nxi+1 = xi -[H(xi)]-1g(xi)\n(b) This is locally convergent and the accuracy of the iterates improves quadratically (just\nas before)!\n9. Trust-Region Methods\n(a) Trust-region methods choose the Newton-Raphson direction when the quadratic approx-\nimation is good and the steepest descent direction when it is not.\n(b) The size of the trust region radius can be set arbitrarily\nstarts with\nThis radius grows or shrinks depending on which of the two steps we choose. If Newton-\nRaphson is chosen, GROW the trust-radius when the function was smaller than pre-\ndicted, otherwise, SHRINK the trust-radius. If steepest descent was chose, keep the\ntrust-radius the same.\n(c) Can use a \"dog-leg step\" when the steepest descent direction is chosen. Here the steepest\ndecent move is taken and when it is within the trust-radius, an additional step in the\nNewton-Raphson direction is taken to touch the boundary of the trust-region.\n10. Lagrange multipliers\n(a) All of the methods up to this point haven't consider constraints. Lagrange multipliers\nare one way to handle the problem\nminimizef(x)\nsubject to\nc(x) = 0\n(b) A solution to this problem satisfies\ng(x) -JcxTλ\n= 0\nc(x) = 0\n\nwhere λ is a vector of \"Lagrange multipliers\".\n11. Interior point methods\n(a) Our optimization problem could also have inequality constaints\nminimizef(x)\nsubject to\nh(x) ≥0\nMATLAB(r)\n(\n1)\n\n(b) Interior point methods consider the problem\nN\nminf(x) -μ\nX\nlog(hi(x))\ni=1\nas μ →0+\n(c) The log function is chosen as the barrier because it is easy to find the gradient.\n(d) The parameter μ can be varied using a homotopy procedure.\nMiscellaneous\n1. \"Big O\" Notation for denoting computational complexity\nf\n)\n2. Taylor series expansion: f(x = Pinf\n(n)(a\n)\nn=0\nn!\n(x-a)n = f(a)+f′(a)(x-a)+ 1\n2f′′(a)(x-a)2 +\n1f\n′′′(a)(x -a)3 + · · ·\n3. Basic\nsyntax\n*Disclaimer: This document in not the gospel of numerical. There may be typos and you should\nalways review your notes! Good luck with the exam!\nMATLAB\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Practice Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/d7339c1f493c58067e2c63e45b74bc67_MIT10_34F15_Quiz2.pdf",
      "content": "10.34 Numerical Methods Applied to Chemical Engineering\nQuiz 2\n- This quiz consists of three problems worth 35, 35, and 30 points respectively.\n- There are 4 pages in this quiz (including this cover page). Before you begin, please make sure\nthat you have all 4 pages.\n- You have 2 hours to complete this quiz.\n- You are free to use a calculator or any notes you brought with you.\n- The points associated with each part of each problem are included in the problem statement.\nPlease prioritize your time appropriately.\n\nProblem 1. (35 points)\nConsider two continuously stirred tank reactors (CSTRs) in series as shown in the figure below.\nWhen C2(t) is controlled to be some known forcing function g(t) and the reaction kinetics are first\norder, the dynamics of the system are modeled by\ndC1(t)\nC0(t)\n=\n-C1(t)\ndt\nτ\n-k1C1(t)\n(1)\ndC2(t)\nC1(t)\n=\n-C2(t)\ndt\nt\nτ\n-k2C2( )\n(2)\nC2(t) = g(t)\n(3)\nwhere τ denotes the residence time (equal for both reactors), k1 and k2 are first-order rate constants,\nand the states to be simulated are C0, C1, and C2.\n1. (12 points) Derive the index of the DAE system (1)-(3), assuming that g(t) is known and\ninfinitely differentiable.\n2. (10 points) Determine a consistent initialization for the original variables in the system. If\nthere are additional degrees of freedom, state the initial conditions that can be specified.\n3. (6 points) Given g(t) = t2, explicitly compute a consistent initialization at t0 = 0 in terms of\nany specified variables from part 2 and the parameters τ, k1, and k2.\n4. (7 points) Using the method of auxiliary (dummy) variables, derive an equivalent index-1\nDAE system.\n\nProblem 2. (35 points)\nConsider the reaction, convection, and diffusion of an impurity I in a tubular reactor operating at\nsteady state, where an undesired autocatalytic reaction\nA + I -→2I\ntakes place. Assuming A is in excess, the impurity can be modeled by the second-order differential\nequation\ndC\nv dx = Dd2C + kCA0C\n(1)\ndx2\nwhere C(x) denotes the concentration of the impurity, x ∈[0, L] is the distance from the reactor\nentrance, v denotes the velocity, D denotes the diffusion coefficient, k denotes the rate constant,\nand CA0 denotes the excess concentration of A. The boundary conditions for this system are:\ndC\nvC(0) -D\n=\ndx\n\n(2)\nx=0\nC(L) = CL\n(3)\nwhere CL denotes the maximum level of impurity that can be handled in the product.\n1. (5 points) Derive an equivalent set of first-order ordinary differential equations (ODEs) for\nthe boundary value problem (1), with the vector of unknown (dependent) variables denoted\nby u(x).\n2. (3 points) Define a two-point boundary condition function for the converted system of the\nform\ng(u(0), u(L)) = B0u(0) + BLu(L) + b = 0\nGive expressions for B0, BL, and b.\n3. (7 points) Describe the application of the shooting method on the set of ODEs derived in\npart 1 to solve the original BVP from x = 0 to x = L.\n4. (8 points) Let D, v, k, CA0 > 0. Show that a forward Euler integration of the set of ODEs\nderived in part 1 will be unstable for any choice of step size ∆x.\n5. (12 points) A colleague suggests shooting backwards from x = L to x = 0.\nUsing that\napproach, can a spatial discretization (i.e., ∆x) be chosen so that forward Euler integration\nis stable?\nIf so, provide an expression for ∆x that stabilizes the integration.\nAre there\nany advantages to making the change from forward shooting to backward shooting from a\nnumerical point of view? Why, or why not?\n\nProblem 3. (30 points)\nConsider a version of the unsteady reaction-convection-diffusion equation applied to electrons in a\nsemiconductor device (i.e., the drift-diffusion equations)\n∂n\n∂t = Dn\n∂2n\n∂x2 + vd\n∂n\nB\n∂x -\nn -Kn2\n(1)\nwhere n(x, t) denotes the concentration of electrons, x ∈[0, L] defines the spatial variable, Dn\ndenotes the electron diffusion coefficient, vd denotes the effective drift velocity, and B and K denote\nband-to-band and Auger recombination rate constants, respectively.\nThe initial and boundary\nconditions for this system are\nn(x, 0) = 0\n(2)\nn(0, t) = φ0\n(3)\nn(L, t) = φL\n(4)\nwhere φ0 and φL are constants.\n1. (12 points) Derive method-of-lines equations (using finite differencing) for the PDE (1) that\nare second-order accurate in space. Grid the spatial domain from i = 0, 1, . . . , N +1. What is\nthe space between nodes, ∆x? Define an equation at every node in the interior of the domain\nand give the initialization for the method-of-lines equations.\n2. (12 points) Derive the finite difference equations for the PDE (1) that are second-order\naccurate in space and first-order accurate in time.\nAgain, grid the spatial domain from\ni = 0, 1, . . . , N +1 and define an equation at every node in the interior of the domain. Is your\nmethod explicit or implicit?\n3. (6 points) Estimate the concentration of electrons at the midpoint x = L/2 and at time\nt = 1 using the derived finite difference equations from part 2 with a spatial discretization of\n∆x = L/2 and temporal discretization of ∆t = 1. Write your answer in terms of parameters\nand any provided initial and boundary conditions in equations (2)-(4).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Practice Quiz 2 Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/54a2894ed0c14bfa635944c2904be8c1_MIT10_34F15_Quiz2solution.pdf",
      "content": "10.34 Numerical Methods Applied to Chemical Engineering\nQuiz 2\n- This quiz consists of three problems worth 35, 35, and 30 points respectively.\n- There are 4 pages in this quiz (including this cover page). Before you begin, please make sure\nthat you have all 4 pages.\n- You have 2 hours to complete this quiz.\n- You are free to use a calculator or any notes you brought with you.\n- The points associated with each part of each problem are included in the problem statement.\nPlease prioritize your time appropriately.\n\nProblem 1. (35 points)\nConsider two continuously stirred tank reactors (CSTRs) in series as shown in the figure below.\nWhen C2(t) is controlled to be some known forcing function g(t) and the reaction kinetics are first\norder, the dynamics of the system are modeled by\ndC1(t)\nC0(t)\n=\n-C1(t)\ndt\nτ\n-k1C1(t)\n(1)\ndC2(t)\ndt\n= C1(t) -C2(t)\n)\nτ\n-k2C2(t\n(2)\nC2(t) = g(t)\n(3)\nwhere τ denotes the residence time (equal for both reactors), k1 and k2 are first-order rate constants,\nand the states to be simulated are C0, C1, and C2.\n1. (12 points) Derive the index of the DAE system (1)-(3), assuming that g(t) is known and\ninfinitely differentiable.\nSolution: The first step is to identify the differential and algebraic variables in the system.\nThe differential variables are C1(t) and C2(t), the algebraic variable is C0(t), and the param-\neters are τ, k1, and k2. To derive the index, we must take derivatives of (1)-(3) until we\nhave a complete set of first-order ODEs for all states. Start with taking a derivative of the\nalgebraic equation,\nC2(t) = g (t).\n(3′)\nSubstitute (3′) into (2),\ng (t) =\n\nC1(t) -C2(t)\n\n-k2C2(t).\n(4′)\nτ\nTake a derivative of (4′),\ng (t) =\n\nC1(t) - C2(t)\n\n-\n\nk2C2(t).\n(4′′)\nτ\nSubstitute (1) into (4′′),\ng (t) = τ\nC0(t) -\n\nC1(t)\n\n-\n\nk1C1(t) -C2(t)\nτ\n\n-k2C2(t).\n(5′′)\n\nRearrange this for C0(t)\n\nC0(t) = g (t)τ + 1 + k1τ C1(t) + τ + k2τ 2 C2(t)\n(5′′)\nSubstitute (3′) into this expression and take another der\nivative,\n\n...\n\nC0(t) = g (t)τ 2 +\n1 + k1τ\n\nC1(t) + τ + k\n2τ\ng (t)\n(5′′′)\nWe now have a complete set of first-order ODEs given by\n(1), (2),\n\nand (5′′′). It took us three\nderivatives of the orginal ODEs to derive a complete set of ODEs such that the index is\nthree.\nDeduct 2 points for not specifying complete ODE set.\nDeduct at most 10 points for incorrect index depending on exact error.\n2. (10 points) Determine a consistent initialization for the original variables in the system. If\nthere are additional degrees of freedom, state the initial conditions that can be specified.\nSolution: For a consistent initialization, we must specify the original variables in the DAE.\nWe also know that the original equations (1)-(3) must be satisfied at the initial time such\nthat the maximum number of degrees of freedom (DOFs) is given by\n\n5 variables {C0(t0), C1(t0), C1(t0), C2(t0), C2(t0)}\n-3 equations {(1), (2), and (3)}\n2 maximum # of DOFs\nHowever, as derived in part 1, we have an index three problem with implicit constraints that\nshould also be satisfied during initialization. Implicit constraints constrain the values of the\noriginal variables in the system. Looking back through part 1, we notice that (3′) and (4′′)\n\nconstrain the values of C1(t) and C2(t), which appear in (1)-(3). Therefore, (3′) and (4′′)\nrepresent implicit constraints for this system.\nThese two additional implicit constraints reduce the degrees of freedom by two. As a result,\nwe expect the system to be completely specified by the forcing function g(t) (i.e., zero DOFs\ndue to implicit constraints). A consistent initialization must satisfy\nC1(t0) = τ\n\nC0(t0) -C1(t0)\n\n-k1C1(t0)\n(4)\nC2(t0) = 1 C1(t0) -C2(t0)\nτ\n-k2C2(t0)\n(5)\nC2(t0) = g(\n\nt\n\n0)\n(6)\nC2(t0) = g (t0)\n(7)\ng (t0) =\n\n)\nτ\n\nC1(t0) - C2(t0\n\n-k2C2(t0)\n(8)\n\n[We can determine C2(t0) and C2(t0) from (6) and (7) directly, respectively. We can solve\n\nfor C1(t0) and C1(t0) by substituting these into (5) and (8), respectively. Lastly, we can use\nthese results to solve for C0(t0) using (4).]\nDeduct 5 points for missing implicit constraint (4′′).\n\nDeduct at most 3 points for not specifying the variables you need to solve for\nand what equations they should be determined from.\nDeduct at most 2 points for missing the other implicit constraint (3′) and math\nerrors.\n3. (6 points) Given g(t) = t2, explicitly compute a consistent initialization at t0 = 0 in terms of\nany specified variables from part 2 and the parameters τ, k1, and k2.\nSolution: Based on our result from part 2, we know that our initialization is specified entirely\nby the forcing function g(t0) i.e., there are no additional variables that need to be specified.\nLet us first derive explicit equations for the consistent initialization entirely in terms of g(t0),\nC2(t0) = g(t0)\nC2(t0) = g (t0)\nC1(t0) = τg (t0) + (1 + k2τ)g(t0)\nC1(t0) = τg (t0) + (1 + k2τ)g (t0)\nC0(t0) = τ 2g (t0) +\nh\nτ(1 + k1τ) + τ(1 + k2τ)\ni\ng (t0) + (1 + k1τ)(1 + k2τ)g(t0)\nNow we must simply insert the value of the function g(t) and its derivatives at t = 0. We can\neasily derive g(t) = t2, g (t) = 2t, and g (t) = 2 such that g(0) = 0, g (0) = 0, and g (0) = 2.\nSubstituting these into the above expressions gives a consistent initialization for the original\nDAE for g(t) = t2 i.e.,\nC2(0) = 0\nC2(0) = 0\nC1(0) = 0\nC1(0) = 2τ\nC0(t0) = 2τ 2\nDeduct 2 points for missing for missing implicit constraint (4′′), which automati-\ncally gives wrong equations for the initial conditions.\nDeduct at most 4 points for missing the other implicit constraint and other math\nerrors.\n4. (7 points) Using the method of auxiliary (dummy) variables, derive an equivalent index-1\nDAE system.\nSolution: As we know from lecture, we want to satisfy the implicit constraints when solving\nhigher index problems. However, the implicit constraints overspecify the problem such that\nwe have more equations than unknowns. For this problem, we have the original equations\n\n(1), (2), and (3) and implicit constraints (3′) and (4′′)\nC0(t) -C1(t)\nC1(t) =\nk\nτ\n-\n1C1(t)\n(1)\nC\n\n1(t)\nC2(t) =\n-C2(t)\n(\nτ\n-k2C2 t)\n(2)\nC2(t) = g(t)\n(3)\nC2(t) = g (t)\n(3′)\ng (t) =\n\nC1(t)\nC2(t)\nk2C2(t)\n(4′′)\nτ\nOne\n\n-\n-\nway to get around this is to use the method of\n\nauxiliary (dummy) variables. Here, we\nhave two implicit constraints such that we need to replace two derivatives with auxiliary\n\nvariables. Only two derivatives appear in these equations such that we must replace C1(t)\nwith some new variable C1\n′\n\n(t) and C2(t) with some new variable C2\n′ (t).\nC\nC1\n′\n0(t)\n(t) =\n-C1(t)\nτ\n-k1C1(t)\nC′\n2(t) = C1(t) -C2(t)\nk\nτ\n-\n2C2(t)\nC2(t) = g(t)\nC2\n′ (t) = g (t)\ng (t) =\nC′ (t)\nC′ (t)\nk2C′ (t)\nτ\nIn\n\n-\n\n-\nthis case, we are left with only algebraic equations such that we can solve for all of these\nvariables analytically\nC0(t) = τ 2g (t) +\nh\nτ(1 + k1τ) + τ(1 + k2τ) g (t) + (1 + k1τ)(1 + k2τ)g(t)\nC1(t) = τg (t) + (1 + k\ni\n2τ)g(t)\nC1\n′ (t) = τg (t) + (1 + k2τ)g (t)\nC2(t) = g(t)\nC2\n′ (t) = g (t)\nThe system of purely algebraic equations (that are uniquely solvable) is an index-1 DAE since\na single derivative results in an equivalent ODE set.\nDeduct 5 points for missing both implicit constraint (3′) and (4′′) if included\nelsewhere in the problem.\nOnly deduct 3 points if missing (3′) when included\nelsewhere in the problem. Deduct 1 point for missing implicit constraint (4′′) if\nnot included elsewhere in the problem.\nDeduct 2 points for not having a well-posed set of equations.\nDeduct 1 point for not verifying your derived DAE was index-1.\n\nProblem 2. (35 points)\nConsider the reaction, convection, and diffusion of an impurity I in a tubular reactor operating at\nsteady state, where an undesired autocatalytic reaction\nA + I -→2I\ntakes place. Assuming A is in excess, the impurity can be modeled by the second-order differential\nequation\ndC\nv dx = Dd2C + kCA0C\n(1)\ndx2\nwhere C(x) denotes the concentration of the impurity, x ∈[0, L] is the distance from the reactor\nentrance, v denotes the velocity, D denotes the diffusion coefficient, k denotes the rate constant,\nand CA0 denotes the excess concentration of A. The boundary conditions for this system are:\ndC\nvC(0) -D\n=\ndx\n\n(2)\nx=0\nC(L) = CL\n(3)\nwhere CL denotes the maximum level of impurity that can be handled in the product.\n1. (5 points) Derive an equivalent set of first-order ordinary differential equations (ODEs) for\nthe boundary value problem (1), with the vector of unknown (dependent) variables denoted\nby u(x).\nSolution: Let u = [dC\ndx , C]T . Then the first-order ODE system is\ndu\ndx =\nv u1\nD\n-kCA0\nD u2\nu1\n\n= Au,\nwhere A =\nv\nD\n-kCA0\nD\n\n.\nDeduct at most 3 points for incorrect u\nDeduct at most 2 points for math error in deriving A\n2. (3 points) Define a two-point boundary condition function for the converted system of the\nform\ng(u(0), u(L)) = B0u(0) + BLu(L) + b = 0\nGive expressions for B0, BL, and b.\nSolution: Boundary conditions in terms of u are\nvu2(0) -Du1(0) = 0\nu2(L) = cL\nwhich can be rewritten as\ng(u(0), u(L)) =\n-D\nv\nu(0) +\nu\n\n(L) +\n\n-CL\n\n= 0\n\nwith\n\nB0 =\n-D\nv\n, BL =\n, and b =\n-CL\nDeduct 1 point for\n\nincorrect\n\nB\n\nDeduct 1 point for incorrect BL\nDeduct 1 point for incorrect b\n3. (7 points) Describe the application of the shooting method on the set of ODEs derived in\npart 1 to solve the original BVP from x = 0 to x = L.\nSolution: Let u(0) = c. Initialize c(0) = c0 where c0 is some constant. During any kth\niteration of the shooting method, solve the IVP problem,\ndu = Au,\nu(0) = ck.\ndx\nRefer to the IVP solution at x = L as u(L; ck). Then calculate the next boundary condition\nat x = 0 based on both boundary conditions by solving the equation,\ng(c, u(L; c)) = B0c + BLu(L; c)) + b = 0,\nusing Newton's method. The Jacobian required for Newton's method is then\n∂g\nJ = ∂c = B0 + BL\n∂u(L; c).\n∂c\nWe can compute the Newton's step for ∆ck with J∆ck = -g. Using this, we update the\nboundary condition at x = 0 as ck := ∆ck + α∆c, where α is the damping factor. This new\nck is then fed to the IVP problem, and the procedure is repeated at the next iteration until\nthe desired tolerance is met.\nDeduct 1 point for not initializing shooting method.\nDeduct 1 point for not stating the IVP equation.\nDeduct 1 point for not defining input and output to function g(c, u(L; c)).\nDeduct 1 point for Jacobian in Newton's method.\nDeduct 1 point for step size equation.\nDeduct 1 point for the update rule in Newton's method.\nDeduct 1 point for termination of algorithm.\n(Note: Deducted at most 3 points for not connecting IVP solution to Newton's\nmethod)\n4. (8 points) Let D, v, k, CA0 > 0. Show that a forward Euler integration of the set of ODEs\nderived in part 1 will be unstable for any choice of step size ∆x.\nSolution: Check the stability of the original IVP problem by calculating the eigenvalues, λ1\nand λ2, of A:\ndet\n\" v\nD -λ\n-kCA0\nD\n-λ\n#\n= 0\nλ1,2 =\nv\n2D ± 1\nr v\nD\n\nkC\n-\nA0\nD\n\nFor any parameter values that result in eigenvalue with imaginary part, the real part v/(2D)\nis positive for all positive parameter values. For all positive parameter values in which the\neigenvalues are real, max\nq v\nD\n2 -4kCA0\nD\n<\nv . Hence the real part of both eigenvalues\n2D\nλ1,2 is greater than zero for all positive parameter values. Hence the forward Euler method\nwould be unstable for any choice of ∆x > 0.\nDeduct at most 3 points for incorrect eigenvalues of A.\nDeduct 2 points for not showing forward Euler stability condition |1 + ∆xλ| ≤1.\nDeduct 1 point for not showing that both eigenvalues have positive real parts.\nDeduct 1 point for not stating positive eigenvalues of A lead to unstable forward\nEuler for any choice of ∆x.\n5. (12 points) A colleague suggests shooting backwards from x = L to x = 0.\nUsing that\napproach, can a spatial discretization (i.e., ∆x) be chosen so that forward Euler integration\nis stable?\nIf so, provide an expression for ∆x that stabilizes the integration.\nAre there\nany advantages to making the change from forward shooting to backward shooting from a\nnumerical point of view? Why, or why not?\nSolution: For the\n\nbackwards shooting method, define x = L -x, and rewrite the first ODE\ndC\nsystem for u =\nd x , C\nT\nas\nd u = A u ,\ndx\nwhere A =\n-v\nD\n-kCA0\nD\n\n. The eigenvalues of A are\nv\nλ1,2 = -2D ± 1\nr v\nD\n-4kCA0\nD\nNow the real part of both eigenvalues λ1,2 is negative for all positive parameter values, using\nsimilar argument as in the previous section.\nHence, we can choose some ∆x so that the\nforward Euler integration is stable, making the backward shooting method more numerically\nadvantageous than the forward shooting method for this problem. For the forward Euler\nintegration to be (absolutely) stable requires |1 + λ1,2∆x| ≤1.\nTo derive ∆x, we have to consider two cases inside the square root term:\n(a) If\nv\nD\n2 -4kCA0 < 0, then both eigenvalues are complex conjugates. Since λ1,2 could be\nD\ncomplex values in this case, write it as λi = a ± bi, where a, b ∈R such that a ≡Re(λi)\nand b ≡Im(λi) Then the stability condition yields,\n|1 + (a ± bi)∆x| ≤1\nor\np\n(1 + a∆x)2 + b2(∆x)2 ≤1\nWe can square both sides and get\n(1 + a∆x)2 + b2(∆x)2 ≤1\n\nwhich gives\n2a∆x + (a2 + b2)(∆x)2 ≤0\nwhich since ∆x > 0, simplifies to\n2a + (a2 + b2)∆x ≤0\nor\n2a\n∆x ≤-(a2 + b2).\nIn this case, the real part, a = -v\n2D, is negative. Let's define ˆa = -a =\nv > 0. Then\n2D\nthe stability criterion for the step size is\n2aˆ\n∆x ≤\n.\n(aˆ2 + b2)\nor\n∆x\n|Re(λ\n≤\ni)|\n|λi|2\nsince |λi|2 ≡a2 + b2 = aˆ2 + b2. We plug in the values for aˆ and b to derive the step size\nin terms of the given parameters.\nv\naˆ = 2D\nb = 1\nr\n4kCA0\nv\nD\n-\n\nD\n∆x ≤\nv\n2D\n\nv\n2D\n\n+ 1\nh\n-\nv\nD\n2 + 4kCA0\nD\ni\nSimplying the above yields,\nv\n∆x ≤\n.\nkCA0\nIn this case, the step size is bounded only by the ratio of the convection term to the\nreaction term.\n(b) If\nv\nD\n2 -4kCA0\n,\nD\n≥0, then both eigenvalues are real. For λi ∈R\n-1 ≤1 + λi∆x ≤1\nwhich yields\n-2 ≤λi∆x ≤0.\nSince the real part of the eigenvalue is negative as discussed earlier,\n-2\n∆\nλi\n≥\nx ≥0\nFor any ∆x > 0,\n∆x ≤|λi|\n\nThe two eigenvalues may not be equal in magnitude unless\nv\nD\n2 -4kCA0 = 0. Hence,\nD\nour step size is then bounded by the eigenvalue with larger magnitude:\n∆x ≤max{|λ1|, |λ2|}\nSince both eigenvalues are negative, the more negative eigenvalue would be the one that\nis important in determining the stability criterion for the step size:\n∆x ≤ -\nv\n2D -1\nq v\nD\n2 -4kCA0\nD\n\n∴∆x\n\n≤\nv\n2D + 1\nq v\nD\n2 -4kCA0\nD\nDeduct 3 points for not defining coordinate transformation.\n\nDeduct 2 points for not calculating correct eigenvalues of the new A.\nDeduct 1 point for not showing the stability criteria |1 + λ∆x| ≤1.\nAlso,\nneeded to show that λ must be negative for numerical stability →backward\nEuler is numerically advantageous.\nDeduct 3 points for not calculating the step size ∆x for the real eigenvalue\ncase.\nDeduct 3 points for not calculating the step size ∆x for the complex eigen-\nvalue case.\n\nProblem 3. (30 points)\nConsider a version of the unsteady reaction-convection-diffusion equation applied to electrons in a\nsemiconductor device (i.e., the drift-diffusion equations)\n∂n\n∂t = Dn\n∂2n\n∂x2 + vd\n∂n\n(1)\n∂\n-Bn\nx\n-Kn\nwhere n(x, t) denotes the concentration of electrons, x ∈[0, L] defines the spatial variable, Dn\ndenotes the electron diffusion coefficient, vd denotes the effective drift velocity, and B and K denote\nband-to-band and Auger recombination rate constants, respectively.\nThe initial and boundary\nconditions for this system are\nn(x, 0) = 0\n(2)\nn(0, t) = φ0\n(3)\nn(L, t) = φL\n(4)\nwhere φ0 and φL are constants.\n1. (12 points) Derive method-of-lines equations (using finite differencing) for the PDE (1) that\nare second-order accurate in space. Grid the spatial domain from i = 0, 1, . . . , N +1. What is\nthe space between nodes, ∆x? Define an equation at every node in the interior of the domain\nand give the initialization for the method-of-lines equations.\nSolution: We grid the spatial domain with nodes positioned at i = 0, 1, 2, . . . , N + 1 that\ncorrespond to 0, ∆x, 2∆x, . . . , L in the x-direction. There are a total of N + 2 points and\nN + 1 line segments. Therefore, the spacing between nodes is ∆x =\nL .\nN+1\nThe method of lines results in a set of ODEs by approximating the spatial derivatives with\na finite difference approximation while keeping the time derivatives. Here, we are told to use\nsecond-order accurate in space approximations (i.e., central differences)\n∂n\n∂x\n\n(xi)\n≈n(xi + ∆x, t) -n(xi -∆x, t)\n2∆x\n= ni+1(t) -ni-1(t)\n2∆x\n(5)\n∂2n\nn(xi + ∆x, t) -2n(xi, t) + n(xi -∆x, t)\n∂x2\n(xi)\n≈\n(∆x)2\n= ni+1(t) -2ni(t) + ni-1(t)\n(∆x)2\n(6)\nwhere ni(t) = n(i∆x, t). Substituting these approximations into the PDE (1) evaluated at\neach node on the interior gives\ndni\nn\n= Dn\ndt\n\ni+1(t) -2ni(t) + ni-1(t)\n(∆x)2\n\n+ vd\nni+1(t) -ni-1(t)\nBni(t)\nKn (t)2\ni\n(7)\n2∆x\n\n-\n-\n∀i = 1, 2, . . . , N\nWe must also include the boundary points n(0, t) = n0(t) = φ0 and n(L, t) = nN+1(t) = φL.\n\nSubstituting these into the MOL equations gives\ndn1\ndt = Dn\nn2(t) -2n1(t) + φ0\n(∆x)2\n\n+ vd\nn2(t) -φ0\n2∆x\n\n-Bn1(t) -Kn1(t)2\n(8)\ndni\nn\n= Dn\ndt\n\ni+1(t) -2ni(t) + ni-1(t)\n(∆x)2\n\n+ vd\nni+1(t) -ni-1(t)\n2∆x\n\n-Bni(t) -Kni(t)2\n(9)\n∀i = 2, 3, . . . , N -1\ndnN\ndt\n= Dn\nφL -2nN(t) + nN-1(t)\n(∆x)2\n\n+ vd\nφL -nN-1(t)\nN\n2∆\n\nN\nx\n-Bn (t) -Kn (t)\n(10)\nFrom the provided initial condition n(x, 0) = 0, we know that the electron concentration is\nzero at all points in space at t = 0. This means we initialize the ODEs using n1(0) = n2(0) =\nn3(0) = · · · = nN(0) = 0.\nDeduct 2 points for incorrect ∆x =\nL .\nN+1\nDeduct 1 point for not stating what type of differencing scheme was used for the\nspatial derivatives and not stating their accuracy.\nDeduct at most 5 points for incorrect MOL equations. Only 1 point was deducted\nfor not including the indices for which the equations are valid.\nDeduct at most 2 points for incorrect boundary conditions.\nDeduct at most 2 points for incorrect initial conditions (including indices).\n2. (12 points) Derive the finite difference equations for the PDE (1) that are second-order\naccurate in space and first-order accurate in time.\nAgain, grid the spatial domain from\ni = 0, 1, . . . , N +1 and define an equation at every node in the interior of the domain. Is your\nmethod explicit or implicit?\nSolution: Again, we grid the spatial domain with nodes positioned at i = 0, 1, 2, . . . , N + 1\nthat must correspond to 0, ∆x, 2∆x, . . . , L in the x-direction (the spacing between nodes is\n∆x =\nL ). We also discretize in time with a step size of ∆t.\nN+1\nIn the finite difference method, we approximate all derivatives using finite differences. In\nthis case, we are told to use second-order-in-space (central differences) and first-order-in-time\n(forward or backward) approximations. For simplicity, we will use a forward difference in\ntime approximation (a backward difference could alternatively be chosen). The derivatives in\nthe PDE are then approximated as\n∂n\n∂t\n\n(xi,tj)\n≈n(xi, tj + ∆t) -n(xi, tj)\nj+1\nn\n=\ni\n-\nj\nni\n∆t\n∆t\n(11)\n∂n\nn(xi + ∆x, tj) -n(xi -∆x, tj)\n∂x\n(xi,tj)\n≈\n2∆x\n= nj\ni+1 -nj\ni-1\n2∆x\n(12)\n∂2n\n∂x2\n\n(xi,tj)\n≈n(xi + ∆x, tj) -2n(xi, tj) + n(xi -∆x, tj)\n(∆x)2\n= nj\ni+1 -2nj\ni + nj\ni-1\n(13)\n(∆x)2\n\nj\nwhere ni = n(i∆x, j∆t). We can substitute these approximations into the original PDE (1),\nwhich gives\nj+1\nni\n-\nj\nni\n∆t\n= Dn\nnj\ni+1 -2nj\ni + nj\ni-1\n(∆x)2\n\n+ vd\nnj\ni+1 -nj\ni-1\n)\n2∆\n-\nj\nj\nBn\nx\ni -K(n\ni\n(14)\n∀i = 1, 2, . . . , N,\n\n∀j ≥0\nj\nj\nOur boundary conditions must also be included: n(0, t) = n0 = φ0 and n(L, t) = nN+1 = φL\nfor all j ≥0. Substituting these boundary conditions into the discretized equation gives\nj+1\nn1\n-\nj\nn1\n∆t\n= Dn\nnj\n2 -2nj\n1 + φ0\n(∆x)2\n\n+ vd\nnj\n2 -φ0\n2∆x\n\n-Bnj\n1 -K(nj\n1)2,\n∀j ≥0\n(15)\nnj+1\ni\n-nj\ni\nj\nn\n= Dn\n∆t\n\ni+1 -\nj\nj\n2ni + ni-1\n(∆x)2\n\n+ vd\nnj\ni+1 -nj\ni-1\n2∆x\n\n-Bnj\ni -K(nj\ni)2\n(16)\n∀i = 2, 3, . . . , N -1,\n∀j ≥0\nnj+1\nN\n-nj\nN\n∆t\n= Dn\nφL -2nj\nN + nj\nN-1\n(∆x)2\n\n+ vd\nφL -nj\nN-1\nj\nB\n2∆x\n\n-\nnN -\nj\nK(nN)2,\n∀j ≥0\n(17)\nWe can initialize these algebraic equations using the provided initial condition n(x, 0) = 0.\nThe discretized form is n(x, 0) = n0\ni = 0, ∀i = 1, . . . , N (i.e., n0\n1 = n0\n2 = n0\n3 = · · · = n0\nN = 0).\nSince we used the forward difference for the time derivative, our method is explicit, which is\nj+1\neasily verified by looking at the derived algebraic equations where the unknowns ni\nfor all\ni = 1, . . . , N (all spatial points at the next time instant) appear only once on the left-hand side\nof the equations. If a backward difference approximation was used for the time derivative, we\nwould get an implicit method as the unknowns would appear on both sides of the equations\nand would need to be computed by solving nonlinear equations.\nDeduct 1 point for not stating what type of differencing scheme was used for the\ntemporal derivative and not stating its accuracy.\nDeduct 1 point for not stating what type of differencing scheme was used for the\nspatial derivatives and not stating their accuracy.\nDeduct at most 5 points for incorrect finite difference equations. At most 1 point\nwas deducted for not including the indices for which the equations are valid.\nDeduct at most 1 point for incorrect boundary conditions.\nDeduct at most 2 points for incorrect initial conditions (including indices).\nDeduct 2 points for incorrectly identifying your scheme as explicit or implicit.\n3. (6 points) Estimate the concentration of electrons at the midpoint x = L/2 and at time\nt = 1 using the derived finite difference equations from part 2 with a spatial discretization of\n∆x = L/2 and temporal discretization of ∆t = 1. Write your answer in terms of parameters\nand any provided initial and boundary conditions in equations (2)-(4).\nSolution: Here, we can use the derived equations from part 2 of the problem. We only have\nthree total points: i = 0 corresponds to x = 0, i = 1 corresponds to x = L/2, and i = 2\ncorresponds to x = L. Let us evaluate (14) at our midpoint node i = 1 i.e.,\nj+1\nn1\n-\nj\nn1\n∆t\n= Dn\nnj\n2 -2nj\n1 + nj\nj\nn\n+\n(∆x)2\n\nvd\n\n2 -\nj\nn0\n2∆x\n\n-Bnj\n1 -K(nj\n1)2,\nj ≥0\n(18)\n\nj\nWe again know from our boundary conditions that n(0, t) = n0 = φ0, j ≥0 and n(L, t) =\nj\nn2 = φL, j ≥0. We also know that ∆x = L/2 and ∆t = 1. Substituting these gives us a\nreduced expression in terms of our known boundary conditions and parameters\nj+1\nn\n-\nj\n4D\nn\nn1 = L2\n\nφL -2nj\n1 + φ0\n\n+ vd\nj\nj\nφL -φ0\n-Bn1 -K(n\nL\n1)2,\nj ≥0\n(19)\nAs seen in part 2, the initial condition gives us\n\nthat our\n\nmiddle node has a value of zero\ninitially. As j = 0 corresponds to t = 0, we know that n0\n1 = 0. The midpoint at t = 1 is\napproximately n(L/2, 1) = n(∆x, ∆t) ≈n1\n1. Evaluating (19) at j = 0, we can get an explicit\napproximation for n1\n1 i.e.,\nn1\n1 -n0\n4Dn\n= L2\n\nφL -2\nn0\n+ φ0\n\n+ vd\nL\n\nφL -φ0\n\n-B\nn0\n-K(\nn0\n1 )2\n(20)\nn(L/2, 1) ≈n1\n1 = 4Dn\nL2\n\nφL + φ0\n\n+ vd\n(21)\nL\n\nφL -φ0\n\nDeduct 1 point for incorrect substitution of ∆x = L/2.\nDeduct 1 point for incorrect substitution of ∆t = 1.\nDeduct at most 4 points for incorrect equations and other math errors.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Numerical Methods Applied to Chemical Engineering: Quiz 2 Review",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/f981f3bbd301879de320d85fb6d102ec_MIT10_34F15_ReviewQuiz2.pdf",
      "content": "10.34\nNumerical Methods Applied to Chemical Engineering\nFall 2015\nQuiz #2 Review\nStudy guide based on notes developed by J.A. Paulson, modified by K. Severson\nIn this section of the course, we learned ordinary differential equations. Specifically we looked\nat IVPs, DAEs, and BVPs. We also spent a lecture on numerical integration.\nInitial Value Problems (IVPs)\n1. Solve 1st order ODE of the form\nd x(t)\n=\nf(x(t), t)\ndt\nx(t0)\n=\nx0\n2. Write differential equations as a system of first order ODEs\n(a) Ex. Rewrite the N-th order ODE system for x(t) in terms of the new dependent variable\nu(t), where u = [u1 u\nT\n2 u3, . . . , uN]\nand u1 = x, u2 = dx\ndt , u3 = d2x,\ndt2\netc.\n(b) See HW6 Problem 2\n3. Finite difference (FD) approximations for derivatives\n(a) Order of accuracy\n- Use Taylor expansion to evaluate the accuracy of FD approximations:\nd\nx(t + ∆t) = x(t) + ∆tdxx(t) + (∆t)2\n2!\nd2\ndt2 x(t) + . . .\nx(t -∆t) = x(t) -∆t d\ndxx(t) + (∆t)2\n2!\nd2\nx(t)\ndt2\n-. . .\n- Derivative f(x, t) is said to be p-th order accurate if\nError = E(∆t) = f(x, t)\n-f(x, t)\n∼(∆t)p\nactual\npredicted\n(b) Forward difference: 1st order accurate, i.e. E(∆t) ∼∆t\nd\ndtx(t) ≈1 (x(t + ∆t)\n∆t\n-x(t))\n(c) Backward difference: 1st order accurate, i.e. E(∆t) ∼∆t\nd\ndtx(t) ≈1 (x(t)\n∆t\n-x(t -∆t))\n(d) Central difference: 2nd order accurate, i.e. E(∆t) ∼(∆t)2\nd\ndtx(t) ≈\n(x(t + ∆t)\n2∆t\n-x(t -∆t))\n\n4. Local vs. global truncation error for numerical integration methods\n(a) Local truncation error (LTE): difference between the approximation (xˆ(tk)) and the\nexact solution started from the previous solution (x∗(tk))\nLTE = ∥xˆ(tk) -x∗(tk)∥\nwhere x∗(tk) is the exact solution to dx = f(x, t) with I.C. of x(t\ndt\nk-1) = xˆ(tk-1)\n(b) Global truncation error (GTE): difference between the approximation (xˆ(tk)) and the\nexact solution (x(tk))\nGTE = ∥xˆ(tk) -x(tk)∥\n5. Accuracy (order of the method): order of accuracy for the derivative approximation f(x(t), t)\n6. Stability\n(a) Test problem for 1st order ODE to determine the region of absolute stability of the\nmethod in the complex plane:\nd\ndtx(t) = λx(t) = f(x, t)\n(b)\nˆx(tk+1)\nn\nˆ\n)\n\n1 for\nx(tk\n≤\numerical stability\n(c) Stability of the ODE problem vs. stability of the method\n- Numerical method is stable if as k →inf, |xˆ(tk)| is bounded.\n- Original problem is stable if as t →inf, |x(t)| is bounded.\n- Numerical method and the original system can have different stability region.\n- Stability and accuracy do not necessarily correlate with one another\n7. Explicit vs. implicit methods: A method is said to be explicit if the next state xˆ(tk+1) is\nbased only on past past and current values, i.e. values of xˆ(t) for t < tk+1. Otherwise, the\nmethod is said to be implicit. We can think of implicit methods as \"fsolve\" type methods.\n8. Explicit/Forward Euler method\nxˆ(t + ∆t) = xˆ(t) + (∆t)f(xˆ(t), t)\n(a) LTE ∼(∆t)2\n(b) GTE ∼∆t\n(c) 1st order accurate method\n(d) Condition for numerical stability: |1 + λ∆t| ≤1\n9. Implicit/Backward Euler method\nxˆ(t + ∆t) = xˆ(t) + (∆t)f(xˆ(t + ∆t), t + ∆t)\n(a) LTE ∼(∆t)2\n(b) GTE ∼∆t\n(c) 1st order accurate method\n(d) Condition for numerical stability: |1 -λ∆t| ≥1\n(e) Need to solve system of non-linear equations\n\n- Newton's method\n- Predictor-corrector method\nPredictor step: Explicit solve gives a better initial guess\nCorrector step: Implicit solve\n10. Example: trapezoidal method\n(a) Average of the forward and backward Euler methods\n∆t (ˆx(tk+1) -ˆx(tk)) = 1 (f(xˆ(tk), tk) + f(xˆ(tk+1), tk+1))\n(b) Is this method explicit, implicit, neither?\n(c) Apply the method to the test equation dx/dt = λx with λ ∈C to determine the region\nof absolute stability of the method in the complex plane. How does the stability of the\ntest equation compare qualitatively with the stability of the method?\n(d) Show that the method is 2nd-order accurate.\n11. Stiffness: ratio of eigenvalues,\nmax\ni\n{|Re(λi)|}\nmin\ni\n{Re(|λi|)}\nwhere λi's are eigenvalues of the Jacobian matrix,\n∂f(x, t). For stiffproblems, use adaptive\n∂x\ntime stepping.\n12. Numerical integration\n(a) Any set of definite integrals can be numerical integrated by rewriting as an IVP,\nt\nx(t) =\nt\nZ\nf (τ)dτ\nd x(t) = f(t), x(t0) = 0\ndt\nMore efficient with adapative time stepping; integrate over continuous regions (skip\ndiscontinuities)\n(b) Polynomial interpolation\nEx. midpoint rule, trapezoidal rule, Simpson's rule\n(c) Gaussian quadrature approximates the integral using a weighted sum of function values\nat specified points. This approach will yield an exact result for polynomials of degree\n2n -1 or less for particular choices of the points xi and weights wi for i = 1 . . . n.\nDifferential-Algebraic Equations (DAEs)\n1. Fully-implicit form\n(a) The most general form of DAEs is the fully implicit form\nF(z, z , t) = 0,\nz(0) = z0\nwhere z := d\ndtz ∈Rnz.\n\n(b) Note that only first-order derivatives are allowed in this definition.\nYou can always\nrewrite higher order derivatives by defining new state variables.\n(c) Generally, z can be composed of differential states x (i.e., states for which derivatives\nappear in F) and algebraic states y (i.e., states for which derivatives do not appear in\nF).\n(d) A DAE is well posed when the dimension of F is equal to the dimension of the state\nvector x.\n2. Semi-explicit form\n(a) Many chemical processes can be written in semi-explicit form\nd\nM\nz(t) = f(z(t), t),\nz(0) = z0\ndt\nwhere M denotes the mass matrix.\n(b) If M is full rank, then the system is an ODE as you can rewrite the system as\nd z(t) = M-1f(z(t), t)\ndt\nwhereas it is a DAE (not an ODE) if M is singular.\n(c) Commonly, chemical processes are naturally written in the form of a simplified version\nof the semi-explicit form\nd x(t) = f(x(t), y(t), t),\nx(0) = x0\ndt\n0 = g(x(t), y(t), t),\n0 = g(x0, y0, t)\nwhere x(t) and y(t) denote the differential and algebraic states, respectively. This can\nbe rewritten in the mass matrix\n\nform as follows\nI\n0 x\nf(x(t), y(t), t)\n=\ny\ng(x(t), y(t), t)\n\n∂g\n(d) The semi-explicit form is index 1 if and only if ∂y is non-singular (det\n∂g\n=\n∂y\n\n0). We\ncan see this by differentiating the algebraic equations once\n∂g\n0 = ∂x\ndx\ndt + ∂g\n∂y\ndy\ndt + ∂g\n∂t\ndt\ndt = ∂g\n∂x x + ∂g\n∂y y + ∂g\n∂t\nRearranging we write,\ndy\ndt = -\n∂g\n∂y\n-1∂g\n∂xf(x, y, t) + ∂g\ndt\n\nIf we combine this we the set of differential equations above, we obtain\n\nI\n∂g\n∂y\n\nx\ny\n\n=\n\nf(x(t), y(t), t)\n-\n∂g\n∂x\n\nf(x(t), y(t), t) -∂g\n∂t\n\nWe have an ODE set after one derivative (index 1) if the mass m\n\natrix is non-singular,\n∂g\nwhich is only true when\nis non-singular. The DAE must be higher index when this\n∂y\ncondition is not met.\n\n3. Numerical simulation\n(a) The derivation of numerical methods for DAEs is very similar to ODEs.\n(b) Use a forward Euler for the differential variables in the semi-explicit form. However, you\nmust solve the algebraic equations implicitly so that the overall method is implicit\nxk+1 = xk +\n∆t\n\nf(xk, yk, tk)\n0 = g(xk+1, yk+1, tk+1)\nwhere variables at k imply variables evaluated at t = t0 + k∆t.\n(c) We might instead use backward Euler (as done on the homework)\nxk = xk\n1 + ∆tf(x(tk), y ,\n-\nk tk)\n0 = g(x(tk), y(tk), tk)\n(d) As for ODEs, computational times are greatly reduced for stiffsystems by using adaptive\ntime-stepping.\n(e) DAEs that are index 1 are straightforward to simulate using generalizations of algo-\nrithms derived for ODEs (e.g. ode15s, ode23s) while higher index problems require a\nreformulation before the numerical methods can be applied.\n4. Index\n(a) The differential index (or index for short) is the minimum number of derivatives of the\noriginal DAE system required to generate a complete system of first-order ODEs.\n(b) An example set of equations is\nd\nx 1 + x2 = g1(t)\nx1 = g2(t)\ndt\n-→\nx1 + x2 = g1(t)\nx1 = g2(t)\nd\n...\n-dt\nx\n→\n1 + x 2 = g 1(t)\nx 1 = g 2(t)\nAfter two derivatives, we can substitute x 1 = g 2(t) into the first model equation to derive\na complete set of first-order ODEs\ng 2(t) + x 2 = g 1(t)\nx 1 = g 2(t)\nTherefore, the index of this problem is two.\n5. Implicit constraints\n(a) The solution to a DAE system may be required to satisfy hidden/implicit constraints\n(as well as explicit constraints). Higher index DAEs (index > 1) include some hidden\nconstraints. DAEs with index 1 very rarely have implicit constraints.\n(b) In the example above, there is an implicit constraint x 1 = g 2(t) that must be satisfied.\n6. Consistent initialization\n\n(a) Consistent initialization for the implicit DAE F(x(t), x (t), y(t), u(t), t) = 0, where u(t)\ndenotes inputs/forcing functions, must satisfy the original DAE\nF(x(t0), x (t0), y(t0), u(t0), t0) = 0\nWe denote the number of differential and algebraic variables as n and m, respectively.\nThe maximum number of degrees of freedom (DOF) that can be specified is\n(2n + m) variables {x(t0), x (t0), y(t0)}\n-(n + m) equations {F(x(t0), x (t0), y(t0), u(t0), t0) = 0}\nn\nDOFs\nThe DOFs is less for systems with index > 1 due to implicit constraints discovered when\ndetermining the index.\n(b) For the example above, we must specify {x1(t0), x 1(t0), x2(t0)}. We must satisfy the\noriginal equations and the implicit constraint found previously\nx 1(t0) + x2(t0) = g1(t0)\nx2(t0) = g1(t0) -g 2(t0)\nx1(t0) = g2(t0)\n-→\nx1(t0) = g2(t0)\nx 1(t0) = g 2(t0)\nx 1(t0) = g 2(t0)\n7. Pros and cons of DAEs vs. ODEs\n(a) ODEs are easier to initialize and more solvers are readily available. However, many\nindex-1 DAE solvers exist nowadays that this is not a huge issue. Higher index problems\nare difficult to handle such that they must be reformulated to an equivalent index-1\nproblem using the method of auxiliary variables.\n(b) Fast explicit methods are available for non-stiffODEs while DAE solvers must be im-\nplicit. However, note that implicit solvers have favorable stability properties.\n(c) DAEs exactly satisfy algebraic constraints, which may be important for certain appli-\ncations. For example, overall mass or energy balances can be exactly satisfied (within\ndouble precision) when using a DAE solver. These constraints are not explicitly satisfied\nwhen numerically solving the ODE version.\nBoundary Value Problems (BVPs)\n1. Problem formulation: 1st order ODEs\nd x(t) = f(x(t), t)\ndt\nB.C. defined for sub-vectors of x: x1(0) = x1,0, x2(T) = x2,0\n2. Boundary condition formulations:\n(a) In general, g(x(0), x(T)) = 0.\n(b) Linear case: g = B0x(0) + BTx(T) + b = 0\n3. Shooting method - Prof. Green liked to remind us to \"shoot first, then relax\"\n\n(a) Converts the BVP into an IVP where some of the initial conditions are unknown\n(b) Guess initial conditions, solve the IVP, find the residual of the boundary points, update\nthe initial conditions and repeat until convergence\n(c) Look for the initial conditions x(0) = c∗such that\nG(c∗) = g(c∗, x(T; c∗)) = 0\n(d) Newton's method: need to evaluate Jacobian J = ∂G\n∂c\n(e) May have no solutions, one solution or many solutions\n4. Relaxation methods (aka method of weighted residuals)\n(a) In general, relaxation methods attempt to solve a problem that is \"close\" to the one we\nhave\ni. yn(t) = PK\nk=1 dnkφk(t)\nii. Need to solve for the coefficients dk ∀k\n(b) Basis functions\ni. The basis functions φk(t) must be linearly independent\nii. It is often nice to choose basis functions that \"automatically\" satisfy the boundary\nconditions but it is not a requirement.\niii. Two types of basis functions: global and local\nGlobal examples: sin(kπx), ekix, etc.\nLocal examples: \"tent functions\"\n(c) Collocation\ni. Choose a set of points {tm} such that g(tm) = 0\nii. Solve for the coefficients using Newton's method\niii. Generally have N boundary conditions and MN equations therefore K = M + 1\n(Note: that this changes when BC's are satisfied automatically)\n(d) Galerkin's method\nZ tf\nφk(t)gn(t)dt = 0\nt0\ni. Forces the error of the approximation to be orthogonal to the basis functions, aver-\naged over the domains\nii. Collocation is a special case of the Galerkin method where the basis functions are\ndelta functions\n*Disclaimer: This document in not the gospel of numerical. There may be typos and you should\nalways review your notes! Good luck with the exam!\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering:",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/83188f37745ad938333f377d24e76e55_MIT10_34F15_Lec03.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nLecture 3:\nExistence and uniqueness of solutions\nFour fundamental subspaces\n\nRecap\n- Scalars, vectors, and matrices\n- Transformations/maps\n- Determinant\n- Induced norms\n- Condition number\n\nRecap\n- Matrices:\n- Matrices are maps between vector spaces!\ny = Ax\n@\n-2\nA\nA =\n-2\n-2\n\nRecap\n- Matrices:\n- Matrices are maps between vector spaces!\ny = Ax\n0 -2\nA =\n-2\n@\nA\n-2\n\nRecap\n- Matrices:\n- Matrices are maps between vector spaces!\ny = Ax\n-2\n-2\nA =\nB\nC\nB\n-2\nC\n@\nA\n\nRecap\n- Matrices:\n- Matrices are maps between vector spaces!\ny = Ax\nT\nss\nA = I ksk2\n\nExistence and Uniqueness\n- Example:\nphosphate (P)\ndirt (D)\nwater (W)\nwasher\ndryer\ndecanter\nmixer\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nphosphate (P)\ndirt (D)\nwater (W)\nStream 1 carries 1800 kg/hr P, 1200 kg/hr D and 0 kg/hr W\nStream 2 carries 0 kg/hr P, 0 kg/hr D and 10000 kg/hr W\nStream 3 carries 0 kg/hr D and 50% W into the washer\nStream 4 carries 0 kg/hr P\nStream 5 carries 0 kg/hr P and 0 kg/hr D\nStream 6 carries 0 kg/hr D and 0 kg/hr W\nStream 7 carries 0 kg/hr P, 95% of D into the decanter, 5% of W into the decanter\nStream 8 carries 0 kg/hr P\nStream 9 carries 0 kg/hr P\nDoes a solution exist? Is it unique?\n\nVector Spaces\nRN\n-\nis an example of a vector space\n- A vectors space is a \"special\" set of vectors\n- Properties of a vector space:\n- closed under addition:\nx, y 2 S ) x + y 2 S\n- closed under scalar multiplication:\nx 2 S ) cx 2 S\n- contains the null vector:\n0 2 S\n- has an additive inverse:\nx 2 S ) (-x) 2 S : x + (-x) = 0\n\nVector Spaces\n- Is this a vector space?\n{(1, 0), (0, 1)}\n- Is this a vector space?\n{y : y = λ1(1, 0) + λ2(0, 1); λ1, λ2 2 R}\n- Is this a vector space?\n{y : y = λ1(1, 1, 0) + λ2(1, 0, 1); λ1, λ2 2 R}\n\nVector Spaces\n-\nIs this a vector space?\n-\nIs this a vector space?\n-\nIs this a vector space?\n{(1, 0), (0, 1)}\n{y : y = λ1(1, 0) + λ2(0, 1); λ1, λ2 2 R}\n{y : y = λ1(1, 1, 0) + λ2(1, 0, 1); λ1, λ2 2 R}\n\nVector Spaces\n- A \"subspace\" is a subset of a vector space\n- It is still closed under addition and scalar multiplication\n- It still contains the null vector\n- For example, R2is a subspace of R3\n- Is this a subspace?\n{y : y = A((3, 0) + (0, 1)); A1, A2 2 R}\nM\n- The linear combination of a set of vectors:\ny =\nX\nAixi\ni=1\n- The set of all possible linear combinations of a set of\nvectors is a subspace:\nspan{x1, x2, . . . , xM }\nM\n= {y 2 RN : y =\nX\nAixi; Ai 2 R, i = 1, . . . , M}\ni=1\n\nLinear Dependence\n- If at least one non-trivial linear combination of a set of\nvectors is equal to the null vector, the set is said to be\nlinearly dependent.\n- The set {x1, x2, . . . , xM } with xi 2 RN is\nlinearly dependent if there exists at least one Ai = 0\nsuch that:\nM\nX\nAixi = 0\ni=1\n- If M > N, then the set of vectors is always dependent\n\nLinear Dependence\n- Example: are the columns of I linearly dependent?\nA\n@\n1 +\nA\n@\n2 +\nA\n@\n3 =\n@\nA\n= 0\n- Example: are these vectors linearly dependent?\nA ,\n@\n-1\n-1\nA ,\n@\n@\nA\n-1\n-1\n- In general, if Ax = 0 has a non-trivial solution, then the\nvectors (A c\n1 A c\n2 . . . A c\nM ) are linearly dependent.\n\nLinear Dependence\n- Uniqueness of solutions to: Ax = b\n- If we can find one vector for which: Ax = 0, then a\nunique solution cannot exist.\n- Proof:\nH\nP\n- Let x = x + x , and AxH = 0 while AxP = b\nH\nH\nP\n- If x 6= 0 , x = cx + x\nis another solution.\n- Therefore, x cannot be unique.\n- Uniqueness of solutions requires the columns of a matrix\nbe linearly independent!\n- (A1\nc A2\nc . . . AM\nc )x H = 0 only if x H = 0\n- If a system has more variables than equations, then a\nunique solution cannot exist. It is under constrained.\n\nLinear Dependence\n- The dimension of a subspace is the minimum number of\nlinearly independent vectors required to describe the\nspan:\nS = span{(1, 0, 0), (0, 1, 0), (0, 0, 1)}, dim S = 3\nS = span{(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 2)}, dim S = 3\n- Example: can Ax = b have a unique solution?\n\nA =\nB\nC\nB\nC\n@\nA\n\nLinear Dependence\n- The dimension of a subspace is the minimum number of\nlinearly independent vectors required to describe the\nspan:\nS = span{(1, 0, 0), (0, 1, 0), (0, 0, 1)}, dim S = 3\nS = span{(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 2)}, dim S = 3\n- Example: can Ax = b have a unique solution?\n\nA =\nB\nC\nB\nC\n@\nA\n\nFour Fundamental Subspaces\nA 2 RN⇥M\n- Column space (range space):\nR(A) = span{A 1, A2, . . . , Ac\nM\nc\nc\n}\n- Null space:\nN (A) = {x 2 RM : Ax = 0}\n- Row space:\nR(AT ) = span{Ar\n1, A2\nr , . . . , Ar\nN }\n- Left null space:\nN (AT ) = {x 2 RN : AT x = 0}\n\nColumn Space\nA 2 RN⇥M\nR(A) = span{A 1, A2, . . . , Ac\nM\nc\nc\n}\n- The column space of A is a subspace of RN\n- Vectors in R(A) are linear combinations of the\ncolumns of A\n- Existence of solutions:\n- Consider: Ax = b\nM\nX\nxiAc = b\ni\ni=1\n- If x exists, then b is a linear combination of the\ncolumns of A. b 2 R(A)\n- Converse: if b 2/ R(A), then x cannot exist\n\nExistence of Solutions\nA 2 RN⇥M\nR(A) = span{A 1, A2, . . . , Ac\nM\nc\nc\n}\n- Solutions to Ax = b exist only if b 2 R(A)\n- Example:\n0 1\n- Does a solution exist with A =\n\n@\nA\n\n0 1\n- If b =\n?\n@\nA\n0 0\n- If b =\n?\n@\nA\n\nExistence of Solutions\nA 2 RN⇥M\nR(A) = span{A 1, A2, . . . , Ac\nM\nc\nc\n}\n- Solutions to Ax = b exist only if b 2 R(A)\n- Example:\n0 1\n- Does a solution exist with A =\n\n@\nA\n\n0 1\n- If b =\n?\n@\nA\n0 0\n- If b =\n?\n@\nA\n\nExistence of Solutions\n- Does a solution exist?\nseparator, 2:1\n3 kg/s\n1.1 kg/s\n?\n?\n- Example:\nA\n✓\nx1\n◆\n=\nx2\n@\n\n@ -2\n\nA\n1.1\n- What is the column space?\n-\n- Is b 2 R(A)?\n\nNull Space\nA 2 RN⇥M\n- The set of all vectors that are transformed into the\nnull vector by A is called the null space of A\nN (A) = {x 2 RM : Ax = 0}\n- The null space is a subset of RM\n- Not the same as R(A)\n- 0 is in the null space of all matrices but is trivial\n- Uniqueness:\n- Consider two solutions Ax = b,\nAy = b\n- Such that A(x - y) = 0\n- If dim N (A) = 0 , then x - y = 0,\nx = y\n- A unique solution exists\n\nNull Space\n-\nExample:\n-\nA series of chemical reactions:\n-\nConservation equation:\n-\nSteady state:\n-\nNull space of the rate matrix:\n-\nWhat is this subspace geometrically?\nA\nk1\n-! B\nk2\n!\nk3\nC\nk4\n!\nk5\nD.\nd\ndt\nB\nB\nB\n@\n[A]\n[B]\n[C]\n[D]\nC\nC\nC\nA =\nB\nB\nB\n@\n-k1\nk1\n-k2\nk3\nk2\n-k3 -k4\nk5\nk4\n-k5\nC\nC\nC\nA\nB\nB\nB\n@\n[A]\n[B]\n[C]\n[D]\nC\nC\nC\nA .\nB\nB\nB\n@\n-k1\nk1\n-k2\nk3\nk2\n-k3 -k4\nk5\nk4\n-k5\nC\nC\nC\nA\nB\nB\nB\n@\n[A]\n[B]\n[C]\n[D]\nC\nC\nC\nA = 0\nB\nB\nB\n@\n[A]\n[B]\n[C]\n[D]\nC\nC\nC\nA = c\nB\nB\nB\n@\n(k3/k2) (k5/k4)\nk5/k4\nC\nC\nC\nA\n\nMatrix Rank\nA 2 RN⇥M\n- Rank of a matrix is the dimension of its column space\nr = dim R(A)\n- Finding the rank: transform to upper triangular form\nA ! U0\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\n@\nU11\nU12\n. . . U1r\nU1(r+1)\n. . . U1M\nU22\n. . . U2r\nU2(r+1)\n. . . U2M\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nU =\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nA\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nUrr\nUr(r+1)\n. . .\n.\n\n.\n\n.\n\nUrM\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n- Rank nullity theorem:\ndim N (A) = M - r\n\nExistence and Uniqueness\nA 2 RN⇥M\n- Existence:\n- For any b in Ax = b\n- A solution exists only if r = dim R(A) = N\n- Uniqueness:\n- A solution is unique only if dim N (A) = 0\n- Equivalently when r = dim R(A) = M\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Finite Volume Methods Constructing Simulations of PDEs",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/f8e7cb2be59c944f94b13ed7f5f6a20f_MIT10_34F15_Lec25.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nFinite Volume Methods\nConstructing Simulations of PDEs\n\nRecap\n-\nvon Neumann stability analysis\n-\nFinite volume methods\n\n-\nGenerally used for conservation equations of the form:\n-\nis the density of a conserved quantity\n-\nis the flux density of a conserved quantity\n-\nThe integral version of such an equation is:\nFinite Volume Method\n@b\n@t = -r · j + r(x, t)\nb(x, t)\nj(x, t)\nd\ndt\nZ\nV ⇤b(x, t) dV =\nZ\nS⇤n · j(x, t) dS +\nZ\nr(x, t) dV\nV ⇤\nd B⇤(t) = F ⇤(t) + R⇤(t)\ndt ACC\nIN/OUT GEN/C\nor\nON\n*\n\n-\n-\nWhat are each of these terms?\n- B ⇤(t) = V ⇤ b⇤(t)\n- R ⇤(t) = V ⇤r ⇤(t)\n- F ⇤(t) =\nX\nFk(t) =\nk2faces⇤\nk2\nX\nA⇤\nk(nk\n-\nfaces⇤\n· j)(t)\nthe sum of fluxes through each face of the volume *\nd b⇤\nV ⇤\n=\nFk(t) + V ⇤r ⇤(t)\ndt\n-\nWe want to solve for by approximating the reaction and\nflux terms. Let's construct low order approximations physically.\nd B⇤(t) = F ⇤(t) + R⇤(t)\ndt ACC\nIN/OUT GEN/CON\n*\nb(t)\nX\nk2faces⇤\nFinite Volume Method\nConservation within a finite volume:\n\n*\n@b =\n@t\n-r · j + r(x, t)\nd b⇤\nV ⇤\n=\ndt\nk2\nX\nFk(t) + V ⇤r ⇤(t)\nfaces⇤\nFinite Volume Method\n\n*\n=\n@t\n-r · j + r(x, t)\nd b⇤\nV ⇤\n=\ndt\nk2\nX\nFk(t) + V ⇤r ⇤(t)\nfaces⇤\nFinite Volume Method\n@b\n\n@b =\n@t\n-r · j + r(x, t)\nd b⇤\nV ⇤\n=\nX\nFk(t) + V ⇤r ⇤(t)\ndt\n, France\nk2faces⇤\n*\nGeometrica: INRIA\nFinite Volume Method\n(c) INRIA. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nd b⇤\nV ⇤\n=\ndt\nk2\nX\nFk(t) + V ⇤r ⇤(t)\nfaces⇤\n*\nCardiff et al. J Biomech Eng 136(1), 2013\nFinite Volume Method\n@b =\n@t\n-r · j + r(x, t)\n(c) Cardiff, Philip et al. License: cc by-nc-nd. Some rights reserved. This content is excluded from\nour CreativeCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nNumerical Solution of PDEs\nL\nStep 1: domain decomposition\nfinite difference: nodes\ni, j\nW\n\nNumerical Solution of PDEs\nL\nW\nStep 1: domain decomposition\nfinite volume: cells\ni, j\n\nL\nW\nNumerical Solution of PDEs\nStep 1: domain decomposition\nfinite element: elements (local basis functions)\ni, j\n\nNumerical Solution of PDEs\nStep 1: domain decomposition\nAlways choose the spacing between nodes/dimensions of cells to match the physics.\nNever pick a certain number of nodes or cells a priori. That number is irrelevant.\n\nx-coordinate (cm)\ncm)\n(\ne\nt\na\nni\nd\nr\no\nco\ny-\np\n( y)\np\np\n[\n]\n\n0.5\n1.5\n\nNumerical Solution of PDEs\nStep 2: formulate an equation to be satisfied at each node/cell\n\nNumerical Solution of PDEs\nStep 2: formulate an equation to be satisfied at each node/cell\nExample: at interior node/cell i,j\nr2c = 0\nequation i,j: ci+1,j + ci-1,j + ci,j-1 + ci,j+1 -4ci,j = 0\n\nNumerical Solution of PDEs\nStep 2: formulate an equation to be satisfied at each node/cell\nExample: at boundary node/cell i,j\nequation i,j:\nc = 1\nci,j -1 = 0\n\nNumerical Solution of PDEs\nStep 3: solve the system of equations formulated at each node/cell for the\nvalue of unknown function at each node/cell\nf(c) = 0\nIf equations are linear, use linear iterative methods\nIf equations are nonlinear, use nonlinear iterative methods\n\nNumerical Solution of PDEs\nStep 3: solve the system of equations formulated at each node/cell for the\nvalue of unknown function at each node/cell\nf(c) = 0\nmust be a vector of the unknowns\nmust be a vector of the equations\nc\nf\n\nNumerical Solution of PDEs\nNx\nNy\ni\nj\nIndexing\nk = i + (j -1)Nx\nci,j+1 = ck+Nx\nck = ci,j\nor\nk = j + (i -1)Ny\nci,j+1 = ck+1\n3,\n\nNumerical Solution of PDEs\ny\n, 2\ni\nj\nIndexing\nNx\nN\nk = i + (j -1)Nx\nfk(c) = fi,j(c)\nor\nk = j + (i -1)Ny\n\nNumerical Solution of PDEs\nNy\nj\nIndexing\nNx\nk = i + (j -1)Nx\nfk(c) = fi,j(c)\nor\nk = j + (i -1)Ny\n3,\ni\n\nNumerical Solution of PDEs\nNx\ny\nNz\nN\ncl = ci,j,k,\nl =?\nExercise: write a single index for finite difference nodes in a cubic\ndomain with (Nx, Ny, Nz) nodes in each cartesian direction\n\nNumerical Solution of PDEs\nExercise: write a single index for finite difference nodes in a cubic\ndomain with (Nx, Ny, Nz) nodes in each cartesian direction\nNx\nNy\nNz\ncl = ci,j,k,\nl = i + (j -1)Nx + (k -1)NxNy\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nr2c = 0\nc = 0\nc = 0\nc = 0\nc = 1\nf(c) = 0\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nh = 1 / 10; % Spacing between finite difference nodes\nNx = 1 + 1 / h; % Number of nodes in x-direction\nNy = Nx; % Number of nodes in y-direction\nc0 = zeros( Nx * Ny, 1 ); % Initial guess for solution\nc = fsolve( @( c ) my_func( c, Nx, Ny ), c0 ); % Find root of FD equations\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nfunction f = my_func( c, Nx, Ny )\n% Loop over all nodes\nfor i = 1:Nx\nfor j = 1:Ny\nk = i + ( j - 1 ) * Nx; % Compound index\n% Boundary nodes\nif ( i == 1 )\nf( k ) = c( k );\nelseif ( i == Nx )\nf( k ) = c( k );\nelseif( j == 1 )\nf( k ) = c( k ) - 1;\nelseif( j == Ny )\nf( k ) = c( k );\n% Interior nodes\nelse\nf( k ) = c( k + 1 ) + c( k - 1 ) + c( k - Nx ) + c( k + Nx ) - 4*c( k );\nend;\nend;\nend;\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.08 seconds to solve\nh = 1/10\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nh = 1/100\n700 seconds to solve!\nWhy is it almost 10,000x slower?\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nf(c) = 0 = Ac -b\nfunction [ Ac, b ] = my_func( c, Nx, Ny )\nAc = sparse( Nx * Ny, 1 );\nb = sparse( Nx * Ny, 1 );\n% Loop over all nodes\nfor i = 1:Nx\nfor j = 1:Ny\nk = i + ( j - 1 ) * Nx; % Compound index\n% Boundary nodes\nif ( i == 1 )\nAc( k ) = c( k );\nelseif ( i == Nx )\nAc( k ) = c( k );\nelseif( j == 1 )\nAc( k ) = c( k );\nb( k ) = 1;\nelseif( j == Ny )\nAc( k ) = c( k );\n% Interior nodes\nelse\nAc( k ) = c( k + 1 ) + c( k - 1 ) + c( k - Nx ) + c( k + Nx ) - 4*c( k );\nend;\nend;\nend;\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nh = 1 / 10; % Spacing between finite difference nodes\nNx = 1 + 1 / h; % Number of nodes in x-direction\nNy = Nx; % Number of nodes in y-direction\n% Calculate RHS of Ac = b\n[ Ac, b ] = my_func( zeros( Nx * Ny, 1 ), Nx, Ny );\n% Find solution of linear FD equations using the an iterative method\n% This is gmres (generalized minimum residual). Other choices include\n% bicgstab (conjugate gradient), minres (minimum residual), etc.\n% The requires a function that returns A*c given c.\nc = gmres( @( c ) my_func( c, Nx, Ny ), b, 100, 1e-6, 100 );\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\n0.015 seconds to solve!\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nh = 1/10\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\n5 seconds to solve!\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nh = 1/100\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\nfunction [ Ac, b ] = my_func( c, Nx, Ny )\nAc = sparse( Nx * Ny, 1 );\nb = sparse( Nx * Ny, 1 );\nk = i + (j -1)Nx\n% Define indices of boundary points and interior points\nbottom = [ 1:Nx ];\ntop = Nx*Ny - [ 1:Nx ];\nleft = [ 1:Nx:Nx*Ny ];\nright = [ Nx:Nx:Nx*Ny ];\ninterior = setdiff( [ 1:Nx*Ny ], [ left, right, bottom, top ] );\nAc( left ) = c( left );\nAc( right ) = c( right );\nAc( top ) = c( top );\nAc( bottom ) = c( bottom );\nb( bottom ) = 1;\nA( interior ) = c( interior - 1 ) + c( interior + 1 ) + c( interior - Nx ) + c( interior + Nx ) ...\n- 4 * c( interior );\nf(c) = 0 = Ac -b\n\nNumerical Solution of PDEs\nExample: solve the diffusion equation in 2-D on a square with side = 1.\n1.2 seconds to solve!\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nh = 1/100\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Numerics: Flames are Usually Solved Using Operator Splitting",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/fbd24f4068beaea6f1ffc4c4cc70bdf7_MIT10_34F15_Lec31_1.pdf",
      "content": "Numerics: Flames are usually solved\nusing Operator Splitting\n-\nMany Equations of form\n\n∂Yn/∂t = Reactionn (Y) + Transportn [Y]\n-\nTypical Yn(x,y,z,t) represent mass fraction of nth species. 100 species\nat 10,000 mesh points = 106 state variables.\n-\nSeldom possible to provide good enough initial guess for steady-\nstate, and hard to solve Newton steps with >106 unknowns, so\nusually time-march to steady-state.\n- Time-marching can be very slow if you need small ∆t!\n-\nReaction term is local and very stiff. Transport involves gradients\n(nonlocal after discretization).\n-\nUsually Chemistry Split from Transport\n- Chemistry solved in parallel using stiff ODE solver (e.g. DSL48S)\n- Transport solved using specialized PDE techniques\n\nOperator Splitting Methods\n- Suppose dy/dt = R + T\n- Simplest: first integrate dy/dt=T, then start from y(tf)\nand integrate dy/dt=R. Not very accurate.\n- Strang: half-step of T, full step R, half step T\n- Second-order accurate, stable.\n- Can be slow to converge to steady-state solution.\n- Balanced: dy/dt = (R+c) + (T-c) how to choose c?\n\"Simple Balanced\": c=1/2 (R(yn)-T(yn))\n- \"Rebalanced\": use averages of R and T over their steps\nto get a higher-order implicit formula.\n- more accurate and more stable.\n- see Speth et al. SIAM J. Numer. Anal. (2013)\n\nSplit each timestep into 3 substeps\nNear Steady State, dY/dt = (Reaction) + (Transport) ~ 0\nBut each term separately is large. Splitting them makes\nyou walk away from true trajectory during substeps.\nBalanced Splitting drastically reduces this walk-away.\nConventional (Strang) Splitting\napplied to a toy problem. Poor\nbehavior near steady state.\nOur new Balanced\nSplitting method\nstays closer to\ntrue trajectory.\n\nSpeth, Green,\nMacNamara &\nStrang\nSINUM\nNew\nOld\n\nBalanced Splitting's Error Exponentially\nGoes to Zero as Flame steadies out\nStrang Splitting:\nConstant Splitting Error\nError\nTime\nSmaller ∆t\nError\nRebalanced Splitting:\nError Vanishes at steady-state\nTime\nSmaller ∆t\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Stochastic Methods Wrap-up",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/0ecf6c422fe7fb9b2a3c48b66bf56bcc_MIT10_34F15_Lec35.pdf",
      "content": "Stochastic Methods wrap-up\n10.34 Dec. 7, 2015\n\nMultidimensional Integrals\n- Many important multidimensional integrals have\nintegrands of this form: p(x)f(x) where p is a\nprobability density.\n- Often an un-normalized p is known:\n\np(x) = w(x)/∫∫∫w(x)dx\n- Boltzmann distribution: w(x)=exp(-E(x)/kT)\n- Bayesian analysis of experiments:\nw(θ|data) = pprior(θ) exp(-χ2(θ,data)/2)\n\nThese are candidates for Metropolis Monte Carlo\n\nMetropolis Monte Carlo challenges\n- Selecting step length in each dimension\n- want it same order of magnitude as the width of\nthe high-w region\n- If too large, very few steps accepted\n- If too small, takes large number of steps to cover\nthe high-w region.\n- Achieving high accuracy requires very large N\n- adding one more sig fig requires 100x more\nsamples than used so far.\n\nMaster Equations\n- Often we have time-dependent probability of\nthe system being in a discrete state: PN(t).\n\ndP/dt = M * P\n\nP(to)=Po\n- Number of possible states usually enormous\n- e.g. problem 2 catalysis: 100 sites, each could be\nin any of 4 states: 4100 ~ 1060 distinct states\n- M is even bigger (4100)2\n- Different values of all these P's at each time\nstep!\n\nSo usually we cannot compute all the elements\nof P, and we cannot sample even a small fraction\nof all the possible states N\n- Common approach: sample from P(t) using\nKinetic Monte Carlo (Gillespie algorithm).\n- Each trajectory requires computing 2*(tf-to)/∆t\nrandom numbers.\n- Low-probability states will not be sampled at all.\n- N-1/2 scaling: hard to achieve high precision\n- Decide on which quantities <f> you are trying to\ncompute before you start\n- Can compute several at same time \"on the fly\".\n- May not be able to store all the trajectories.\n\nWhat is the initial condition Po?\n- Often have an idea of <N> but not full probability\ndistribution of N.\n- Common approach: assume NA, NB, etc.\nuncorrelated initially, and use a separate Poisson\ndistribution for each one:\n\nPpoisson(N) = <N>N exp(-<N>) / N!\nPo(NA,NB,...) = Ppoisson(NA)*Ppoisson(NB)*...\nWhen you start each KMC trajectory, you need to\nsample from a Poisson distribution for NA, NB, etc.\n\nWhich processes to include in Kinetic\nMonte Carlo?\n- Accelerate calculation by making ∆t as long as\npossible, i.e. omit very fast processes (which\nwill be \"equilibrated\" on long time scale).\n- The low-probability processes are not going to\nbe sampled adequately, so consider omitting\nthem completely if it speeds calculation.\n- \"Adequate sampling\": statistical sampling\nerrors ~ sqrt(number of samples with positive\nresult).\n\nFor example, in Problem 2....\n- If the main reaction A+B is very fast, then you will get a\nlot of trajectories showing this reaction, and can get\npretty good sampling statistics.\n- On the other hand, if the coking reaction is slow, you\nmay not sample enough trajectories that show a coking\nevent to be able to reach any conclusions about it.\n- If diffusion is too fast, ∆t will be very small and it will\nbe very expensive to compute a trajectory.\n- Sometimes people assume diffusion is equilibrated and so\ncreate a different random distribution of A and B on\nsurface at each time step corresponding to (slow) reactions\n\nMolecular Dynamics\n- What is it: Solving motions of atoms or clumps of atoms\nusing Newton's equations of motion.\n- Usually fitted force fields but can be from DFT\n- Can include quantum effects e.g. by RPMD\n- Typically use Velocity Verlet algorithm to integrate the ODEs\n(conserves energy to high accuracy)\n- Typically use a \"thermostat\" to capture energy fluctuations due\nto contact with a thermal bath\n- Alternative to Metropolis Monte Carlo for steady-state\nproperties of molecules\n- Can be used for computing time-dependent properties (it is\na more-or-less exact simulation of what the molecules are\nreally doing).\n\nLimitations of Molecular Dynamics\n- For molecules the stretching vibrations are\nvery fast, so need to use very small ∆t\n- This usually limits (tf-to) to nanoseconds\n- So only can determine static quantities which\nhave equilibrated faster than nanoseconds\n- Only can follow dynamic processes which\noccur in nanoseconds.\n- Issue with sampling over large number of\npossible initial conditions.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Summary and review on linear algebra and systems of nonlinear equations",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/d75744236e3fc0bfcfc6a4119b84af74_MIT10_34F15_Lec09.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nLecture 9:\nHomotopy and bifurcation\n\nRecap\n-\nQuasi-Newton-Raphson methods\n\nRecap\nxi\nxi+1\n|f(xi+1)| > |f(xi)|\nf(x\nxi+1 = xi -\ni)\n↵f 0(xi)\n↵= 1\n↵= 1/2\n↵= 1/4\nbacktracking line search\n\nRecap\n\nGood Initial Guesses\n-\nSolving nonlinear equations and optimization require\ngood initial guesses\n-\nWhere do these come from?\n-\nNonlinear equations can have multiple roots,\noptimization problems can have multiple minima.\n-\nHow can we find them all?\n-\nThe concepts of continuation, homotopy and bifurcation\nare useful in this regard.\n\nContinuation\n-\nExample:\n-\nFind the roots of:\nf(x) = x3 -2x + 1\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0.2\n0.4\n0.6\n0.8\n\nContinuation\n-\nExample:\n-\nFind the roots of:\n-\nGuess the roots based on a plot of the function\n-\neasy in 1-D, hard in many dimensions\n-\nTransform the problem from an easy to solve one\nto the problem we want to solve:\n-\nLet\n-\nFind roots as grows from zero to one\n-\nWhen ,\n-\nUse solution for one value of as guess for next\nf(x) = x3 -2x + 1\nλ\nf(x) = x3 -2λx + 1\nλ = 0 x = -1\nλ\n\nContinuation\n-\nExample:\n-\nFind the roots of:\nf(x) = x3 -2x + 1\nf(x) = x3 -2λx + 1\nlambda = [ 0:0.01:1 ];\nxguess = -1;\nfor i = 1:length( lambda )\n\nf = @( x ) x .^ 3 - 2 * lambda( i ) * x + 1;\n\nx( i ) = fzero( @( x ) f( x ), xguess );\n\nxguess = x( i );\n\nend;\n\nContinuation\n-\nExample:\n-\nFind the roots of:\nf(x) = x3 -2x + 1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-1.7\n-1.6\n-1.5\n-1.4\n-1.3\n-1.2\n-1.1\n-1\n-0.9\nx\nλ\nf(x) = x3 -2λx + 1\n\nContinuation\n-\nExample:\n-\nFind the roots of:\n-\nTransform the problem from an easy to solve one\nto the problem we want to solve:\n-\nLet\n-\nWhen is large\n-\nStart with large and trace back to\nf(x) = x3 -2x + 1\nλ\nf(x) = x3 -2λx + 1\nx ⇡1/(2λ), ±\np\n2λ\nλ = 1\nλ\n\nContinuation\n-\nExample:\n-\nFind the roots of:\nf(x) = x3 -2x + 1\n-5\n-4\n-3\n-2\n-1\nf(x) = x3 -2λx + 1\nx\nλ\n\nContinuation\n-\nExample:\n-\nFind the roots of:\nf(x) = x3 -2x + 1\n-5\n-4\n-3\n-2\n-1\nf(x) = x3 -2λx + 1\nx\nλ\n\nContinuation\n-\nContinuation can be used to generate a sequence of good initial\nguesses to different problems by varying a parameter by a small\namount.\n-\nExamples:\n-\nfluid mechanics problems by varying the Reynolds number\n-\nmass transport problems by varying the Peclet number\n-\nmulticomponent phase equilibria problems by varying\ntemperature/pressure\n-\nreaction equilibrium problems by varying reaction rates\n\nHomotopy\n-\nThis transformation from one problem to another is\ntermed homotopy.\n-\nGenerically, we seek the roots of an equation:\n-\nWhen ,\n-\nThe roots are the roots of\n-\nWhen ,\n-\nThe roots are the roots of\n-\nThere is a smooth transformation from to\n-\nis varied in small increments from zero to one and the\nsolution is used as the initial guess for\nh(x; λ) = λf(x) + (1 -λ)g(x)\nλ = 0\nh(x; 0) = g(x)\nx⇤(λ)\nx⇤(0)\ng(x)\nλ = 1 h(x; 1) = f(x)\nx⇤(1)\nf(x)\nλ\ng(x)\nf(x)\nx⇤(λi)\nx⇤(λi+1)\n\nHomotopy\n-\nFor small changes in the homotopy parameter, the\nprevious solution will be a good initial guess.\n-\nNewton-Raphson like methods can be expected to\nconverge quickly.\n-\nIn practice, the function is associated with the\nproblem of interest, but the function is arbitrary.\n-\nIt may be difficult to find a good function\n-\nPhysically based homotopies are usually preferable.\nf(x)\ng(x)\ng(x)\n\nHomotopy\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\nPˆ = 0.1, Tˆ = 0.5\n-1.5\n-1\n-0.5\n0.5\nvˆ\nf(ˆv)\nf(ˆv) =\n✓\nPˆ +\nvˆ\nvˆ2\n◆✓\n-\n◆\n-\nTˆ = 0\n\nHomotopy\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\n-\nCreate the homotopy:\n-\nwith the ideal gas function:\n-\n, ideal gas; , van der Waals\nPˆ = 0.1, Tˆ = 0.5\nf(ˆv) =\n✓\nPˆ +\nvˆ\nvˆ2\n◆✓\n-\n◆\n-\nTˆ = 0\nh(ˆv) = λf(ˆv) + (1 -λ)g(ˆv)\ng(ˆv) = Pˆvˆ -\nTˆ\nλ = 0\nλ = 1\n\nHomotopy\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\nPˆ = 0.1, Tˆ = 0.5\nT = 0.5;\nP = 0.1;\nvguess = 8 / 3 * T / P;\nf = @( v ) ( P + 3 ./ v .^ 2 ) .* ( v - 1/ 3 ) - 8 / 3 * T;\ng = @( v ) P .* v - 8 / 3 * T;\nh = @( v, l ) l * f( v ) + ( 1 - l ) * g( v );\nlambda = [ 0:0.01:1 ];\nfor i = 1:length( lambda )\n\nv( i ) = fzero( @( v ) h( v, lambda( i ) ), vguess );\nvguess = v( i );\n\nend;\n\nHomotopy\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\nPˆ = 0.1, Tˆ = 0.5\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nˆv(λ)\nλ\n\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nHomotopy\nPˆ = 0.1, Tˆ = 0.5\nˆv(λ)\nλ\n\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nHomotopy\nPˆ = 0.1, Tˆ = 0.5\nˆv(λ)\nλ\n\n-\nExample:\n-\nFind roots of the van der Waals equation of state\ngiven:\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nHomotopy\nPˆ = 0.1, Tˆ = 0.5\nturning points\ndet Jh(x⇤(λ), λ) = 0\nˆv(λ)\nλ\n\n-\nParameterize the roots and homotopy parameter in\nterms of the distance travelled along the solution curve:\n-\n\n-\nDetermine how to change homotopy parameter from\narclength constraint:\nArclength Continuation\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx⇤(λ(s)), λ(s)\nk d\nd\nx⇤(λ(s))\nds\nk2\n2 +\n✓\nλ(s)\nds\n◆\n= 1\nx⇤(λ(s))\nλ(s)\n\nBifurcation\n-\nExample:\n-\nFind the real roots of\nf(x) = x3 -rx\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0.2\n0.4\n0.6\n0.8\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\nf(x)\nx\nr < 0\nr = 0\nr > 0\n\nBifurcation\n-\nExample:\n-\nFind the real roots of\nf(x) = x3 -rx\nx⇤\nr\nr < 0, x⇤= 0\nr > 0, x⇤= 0, ±pr\nr = 0, f 0(x⇤) = 0\n\nBifurcation\n-\nOccasionally, a problem will switch from having 1 solution\nto having many solutions as a parameter is varied.\n-\nWe have seen how this occurs discontinuously with\nturning points.\n-\nWhen additional solutions appear continuously, it is\ntermed bifurcation.\n-\nBifurcations in a homotopy enable finding of multiple\nsolutions to the same nonlinear equation\n-\nFinding bifurcation (and turning) points can be of great\nphysical interest.\n-\nLike turning points, the Jacobian is singular at a\nbifurcation point:\ndet J(x⇤) = 0\n\nBifurcation\nx1\nh(x, λi-1) = 0\nx2\n\nBifurcation\nx1\nh(x, λi) = 0\ndet Jh(x, λi) = 0\nx2\n\nBifurcation\nx1\nh(x, λi+1) = 0\nx2\n\nBifurcation\n-\nIn practice, it is hard to hit the bifurcation point exactly\nwhile stepping with the homotopy parameter.\n-\nThe bifurcation is detected by checking the sign of the\ndeterminant of the Jacobian.\n-\nIf det\nJ h ( x , λ ) =\n0 at the bifurcation, then it\nchanged from positive to negative (or negative to\npositive) as the homotopy parameter changed.\n-\nWe can find the bifurcation point exactly by solving an\naugmented system of nonlinear equations:\n✓\nh(x; λ)\n◆\n= 0\ndet Jh(x, λ)\n-\nwhich finds the value of x and λ at the bifurcation\n\nBifurcation\n-\nExample:\n-\nFind the radius where two circles just touch:\n✓\n(x + 3)2 + (x + 1)2 -R2\nf(x) =\n(x1 -2)2 + (x2 -2)2 -R2\n◆\ntoo big\ntoo small\n\nBifurcation\n-\nExample:\n-\nFind the radius where two circles just touch:\n✓\n(x + 3)2 + (x + 1)2\nR\nx) =\n-\nf(\n(x1 -2)2 + (x2 -2)2 -R2\n◆\n-\nThis is a bifurcation point (from 0 to 2 solutions)\nf(x) = 0\ndet J(x) = 0\n-\nFind this point by solving the augmented equations\n✓\nf(x)\nfo\nJ(x)\n◆\n= 0\nr\ny =\ndet\n✓\nx\nR\n◆\n\nBifurcation\n-\nExample:\n-\nFind this point by solving the augmented equations:\n✓\nf(x)\ndet J( )\n◆\n= 0\nfor\ny =\n✓\nx\nx\nR\n◆\n-\nNewton-Raphson iteration:\nyi+1 = yi + ∆yi\n✓\nJ(x)\n@\n@Rf(x)\nf(x)\n∆y =\nr det J(x)\n@\n@R det J(x)\n◆#\n##\n#\ni\nyi\n-\n✓\ndet J(x)\n◆####\nyi\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Boundary value problems 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/c4dfeaf5317ec2bbae302c346353cb7f_MIT10_34F15_Lec19.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nLecture 19:\nDifferential Algebraic Equations\n\nRecap\n- Differential algebraic equations\n- Semi-explicit\n- Fully implicit\n- Simulation via backward difference formulas\n\nRecap\n- How suitable are such approaches?\n- Consider stirred tank example 1:\ndc2\nQ\n=\n(c1(t) - c2(t))\ndt\nV\nc1(t) = γ(t)\nApply backward Euler method:\ndx\nx(tk) x(tk 1)\n=\n+ O(tk tk 1)\n\ntk\ndt\ntk tk 1\nc1(tk) = γ(tk)\n✓\nQ\n◆\nc2(tk) =\nc2(tk 1) + (tk tk 1)c1(tk)\n1 + Q (tk tk 1)\nV\nV\n+O((tk - tk 1)2)\n\n-\n\nRecap\n- How suitable are such approaches?\n- Consider stirred tank example 2:\ndc2\nQ\n=\n(c1(t)\nc2(t))\ndt\nV\nc2(t) = ,(t)\nApply backward Euler method:\nc2(tk) = γ(tk)\nV ✓ c2(tk) - c2(tk-1)◆\nc1(tk) = c2(tk) +\n+O(tk - tk-1)\nQ\ntk - tk-1\n\n-\n-\nRecap\n- How suitable are such approaches?\n- Consider the system of DAEs:\nc 2 = c1(t)\nc 3 = c2(t)\n0 = c3(t) - ,(t)\nApply backward Euler method:\nc3(tk) = ,(tk)\nc3(tk) - c3(tk-1)\nc2(tk) =\n+O(tk\ntk 1)\ntk - tk-1\nc2(tk) - c2(tk-1)\nc1(tk) =\n+O(1) !\ntk - tk-1\n-\n-\n\nRecap\n- Solution via backward Euler:\n- Stirred-tank example 1:\n- local truncation error: O(∆t2)\n- Stirred-tank example 2:\n- local truncation error: O(∆t)\n- DAE example 3:\n- local truncation error: O(1)\n\nRecap\n- How suitable are such approaches?\n- Consider the system of DAEs:\nc 1 = c1(t) + c2(t) + c3(t)\nc 2 = -c1(t) - c2(t) + c3(t)\n0 = c1(t) + c2(t)\nApply backward Euler method:\n\n-\nγ\n\nDifferential Index\n- Consider stirred tank example 1:\ndc2\nQ\n=\n(c1(t)\nc2(t)) (1)\ndt\nV\n(2)\nc1(t) = (t)\n- How many time derivatives are needed to convert to a\nsystem of independent ODEs having differentials of all the\nunknowns?\nderivative of (2)\ndc1 = γ(t)\n(3)\ndt\nCalled an index-1 DAE.\nγ\n-\n\n-\n\nDifferential Index\n- Consider stirred tank example 2:\ndc2\nQ\n=\n(c1(t)\nc2(t)) (1)\ndt\nV\nc2(t) = ,(t) (2)\n- How many time derivatives are needed to convert to a\nsystem of ODEs?\nsubstitute (1)\ndc2\nV\n= .\n→ c1(t) = c2(t) + Q γ (3)\ndt\nderivative of (2)\n↓\nderivative of (3)\ndc1\ndc2\nV\n\n=\nr\ndt\ndt\nQ\n↓\nsubstitute (1)\ndc1\nV\nQ\nCalled an index-2 DAE.\n=\n. +\n(c1(t) - c2(t)) (4)\ndt\nQ\nV\n-\n\nDifferential Index\n- Consider DAE example 3:\nc 2 = c1(t)\n(1)\nc 3 = c2(t)\n(2)\n0 = c3(t) - ,(t)\n(3)\n- How many time derivatives are needed to convert to a\nsystem of ODEs?\nderivative of (3)\nsubstitute (2)\nc 3 = ' → c2(t) = ' (4)\n↓\nsubstitute (1)\nderivative of (4) c 2 = -\n→ c1(t) = i (5)\n↓ ...\nderivative of (5) c 1 = -\n(6)\nCalled an index-3 DAE.\n\nDifferential Index\n- The differential index of a semi-expicit DAE system is defined as\nthe minimum number of differentiations required to convert the\nDAE to a system of independent ODEs.\ndg\n(1)(\n0 =\n= g\nx , y , y , t)\ndt\nd2\ndx\ng\n(2)(\n= f(x, y, t)\n0 =\n= g\nx , y , y , t)\ndt\ndt2\n0 = g(x, y, t)\nsolve for:\ndy = s(x, y, t)\ndt\n...\n\nDifferential Index\n- Consider another example:\nc 1 = c1(t) + c2(t) + c3(t)\nc 2 = -c1(t) - c2(t) + c3(t)\n0 = c1(t) + c2(t)\n- How many time derivatives are needed to convert to a\nsystem of ODEs?\n\nDifferential Index\n- The differential index of a semi-expicit DAE system is defined as\nthe minimum number of differentiations required to convert the\nDAE to a system of ODEs.\n- Index-1 example:\nderivative of (2)\ndx = f(x, y, t) (1)\ndt\n0 = g(x, y, t)(2)\nrearrange and substitute (1)\ndg\n@g dx\n@g dy\n@g\n@g dy\n@g\n@g\n0 =\n=\n+\n+\n→\n= -\nf(x, y, t) -\ndt\n@x dt\n@y dt\n@t\n@y dt\n@x\n@t\n@g\nIf\nis full rank then the DAE is index-1:\ndy\ndy\n✓ @g ◆-1 ✓ @g\n@g ◆\n= -\nf(x, y, t) +\ndt\ndy\n@x\n@t\n\nDifferential Index\n- Example, determine the differential index:\nc1(t)\nc2(t)\nc4(t)\n(\ndc2\nQ1\n=\n(c1(t) - c2(t))\ndt\nV1\ndc4\nQ1\nQ2\nQ1 + Q2\n=\nc2(t) +\nc3(t) -\nc4(t)\ndt\nV2\nV2\nV2\nc1(t) = {1(t)\nc3(t) = {2(t)\nc 3(t)\n\nDifferential Index\n- Example, determine the differential index:\nc1(t)\nc2(t)\nc4(t)\nc3(t)\n(\ndc2\nQ1\n=\n(c1(t) -c2(t))\ndt\nV1\ndc4\nQ1\nQ2\nQ1 + Q2\n=\nc2(t) +\nc3(t) -\nc4(t)\ndt\nV2\nV2\nV2\nc3(t) = γ1(t)\nc4(t) = γ2(t)\n\nDifferential Index\n- Example, determine the differential index:\nc1(t)\nc2(t)\nc4(t)\n(\ndc2\nQ1\n=\n(c1(t) -c2(t))\ndt\nV1\ndc4\nQ1\nQ2\nQ1 + Q2\n=\nc2(t) +\nc3(t) -\nc4(t)\ndt\nV2\nV2\nV2\nc1(t) = γ1(t)\nc2(t) = γ2(t)\nc 3(t)\n\nγ\nDynamics of DAE Systems\n- Solution of stirred tank example 1:\n-(Q/V )t\nc2(t) = c2(0)e\nindex 1 c1(t) = (t)\nt\n+ Q Z\n,(t0)e-(Q/V )(t-t0)dt0\nV\n- Solution of stirred tank example 2:\nc1(t) = (t) + V\nQ\nindex 2\n- Solution of DAE example 3:\nc2(t) = ,(t)\nindex 3\nc1(t) = i\nc2(t) =\nc3(t) = ,\n- Higher index indicates greater sensitivity to changes in\nforcing function.\nγ\n\nDynamics of DAE Systems\n- Physical example: pendulum\nx = v (t)\nm\ng\nm v = -k(t)x (t) + mg\nkx (t)k2 = L2\n- position, velocity, stiffness: x(t) v(t)k(t)\n- Identify differential and algebraic variables.\nx(t) v(t)\nk(t)\n- Identify index of the DAE system.\nd\n(1)\nkx(t)k\ndt\n2 = 2v(t) · x(t) = 0\nd\n(2)\n(v(t) · x(t)) =\n(-k(t)x(t) + mg) · x(t) + kv(t)k\ndt\nm\n2 = 0\n✓ 1\n◆\nd\n(3) dt\n(-k(t)x(t) + mg) · x(t) + kv(t)k 2\nm\n1 dk\n- 2\nk(t)v(t) · x(t) + g · v +\n(-k(t)x(t) + mg) · v(t) = 0 18\n= -\nkx(t)k\nm dt\nm\nm\n\nSimulation of DAE Systems\n- Consider DAE example 3:\n...\nc 2 = c1(t)\nc 1 = -\nc 3 = c2(t)\n→\nc 2 = -\nc 3 = '\n0 = c3(t) - ,(t)\n- Can't I just solve the set of ODEs found when determining\nthat the DAE system is index-3?\n\nSimulation of DAE Systems\n- In general, index-1 semi-explicit DAEs can be safely handled\nby certain stiff integrators in MATLAB (ode15s, ode23t)\n- For generic DAEs, specific DAE solvers are usually needed\n(SUNDIALS, DAEPACK)\n- Initial conditions for such equations must be prescribed\nconsistently, or numerical errors can occur.\n- Consider the pendulum:\n- Can it's initial position be specified arbitrarily?\n- Can it's initial velocity be specified arbitrarily?\n- Can the initial stiffness be specified arbitrarily?\n\nSimulation of DAE Systems\n- Consistent initialization of initial value problems: { x (0), x (0)}\n- index-0 DAE (ODE-IVP): x = f (x , t)\n- 1. x (0) ! x (0) = f (x (0), 0)\n- 2. x (0) solve x (0) = f (x (0), 0)\n- 3. c (x (0), x (0)) = 0 solve with x (0) = f (x (0), 0)\n- fully implicit DAE: f (x , x , t) = 0\n- 2N unknowns for N equations\n- apparently N degrees of freedom to specify\n- hidden constraints reduce these degrees\n- with differential states x and algebraic states y,\nf ( x , x , y , t) = 0\n{ x (0), x (0), y (0)}\n\n-\nγ\nγ\n\n-\n\nSimulation of DAE Systems\n- Consistent initialization, example stirred tank 1:\ndc2\nQ\n=\n(c1(t)\nc2(t)) (1)\ndt\nV\nc1(t) = (t)\n(2)\nConvert to system of ODEs\ndc1 = (t)\ndt\ndc2\nQ\n=\n(c1(t)\nc2(t)) (3)\ndt\nV\nConsistent initial conditions:\nconstrained by\nconstrained by\ndifferential equation (3)\nunconstrained\nalgebraic equation (2)\nQ\nc1(0) = γ(0)\nc 2(0) =\n(c1(0) - c2(0))\nc2(0) = c0\nV\n\n-\n\nSimulation of DAE Systems\n- Consistent initialization, example stirred tank 2:\ndc2\nQ\n=\n(c1(t)\nc2(t)) (1)\ndt\nV\n(2)\nc2(t) = ,(t)\nConvert to system of ODEs\ndc2 = .\n(3)\ndt\ndc1\nV\nQ\n=\n. +\n(c1(t) - c2(t))\ndt\nQ\nV\nConsistent initial conditions:\nconstrained by\nconstrained by\nconstrained by\ndifferential equation (1)\ndifferential equation (3)\nalgebraic equation (2)\nV\nc1(0) = c2(0) +\nc 2(0)\nc 2(0) = γ (0)\nc2(0) = γ(0)\nQ\n-\n\nSimulation of DAE Systems\n- Consider another example:\nc 1 = c1(t) + c2(t) + c3(t)\nc 2 = -c1(t) - c2(t) + c3(t)\n0 = c1(t) + c2(t)\n- Derive consistent initial conditions:\n\nSimulation of DAE Systems\n- Consider another example:\nc 1 = c1(t) + c2(t) + c3(t)\nc 2 = -c1(t) - c2(t) + c3(t)\n0 = c1(t) + c2(t) + 2c3(t)\n- Derive consistent initial conditions:\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Boundary Value Problems 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/3cf3e3bfef5fc5402585f08e53865f55_MIT10_34F15_Lec20.pdf",
      "content": "Boundary Condition notes\n-Bill Green, Fall 2015\nTypically we need to specify boundary conditions at every boundary in our system, both the edges of the\ndomain, and also where there is a discontinuity in the equations (e.g. you might you different equations\ninside a catalyst particle than outside it. The discontinuity might just be in parameters (e.g. the rate\ncoefficient and the diffusivity change between inside and outside the catalyst). There are also typically\ndiscontinuities when there are phase changes, e.g. at the surface of a puddle.\nSometimes there are symmetries that tell you a boundary condition, e.g. dC/dr =0 at r=0 if a problem\nhas cylindrical or spherical symmetry (otherwise there would be a cusp in C(r), which is usually\nunphysical). Sometimes one knows the value of a variable on a boundary so Dirichlet boundary\ncondition. Often one assumes that nothing changes after a certain point, i.e. all the derivatives are zero,\nso von Neumann boundary conditions may be used.\nWhen the state variable is a conserved scalar, then one knows that the flux of that scalar approaching\nthe boundary must equal the flux leaving from the other side. This gives conditions like this for\nconcentrations C (where D is the diffusivity and v is the flow into the boundary).\n-D dC/dx + v C on left side of boundary = -D dC/dx + v C on right hand side\nWhere the velocity v is positive if it is moving in the +x direction.\nIf the flow velocity is zero or negligible in one of the regions, one of the terms will disappear. Note that\ndC/dx can jump at an interface, e.g. if D is modeled as being discontinuous across the boundary, usually\ndC/dx will be discontinuous to match, so that the flux will be continuous.\nA good example is the energy balance between a solid impermeable particle and the fluid that\nsurrounds it. The heat transfer coefficient between the bulk of the fluid and the particle is known for\nmany geometries and flow conditions (recall Nusselt numbers), so the energy flux coming in to the\nparticle is:\nh(Tbulk-Tsurface)\nThis must equal the energy flux at the surface of the particle, looking from the inside. For example for a\nspherical particle its magnitude is:\nk *dT/dr\nWhere is the thermal conductivity of the solid. Putting these together, the boundary condition is\nh(Tbulk-Tsurface) + Kappa*dT/dr = 0\nNote that often you would know (e.g. could easily measure) Tbulk but would need to compute the\nunknown Tsurface and dT/dr.\n\nAnother version of this flux conservation principle is the \"Danckwerts\" boundary conditions for flow\ninto a packed-bed catalytic reactor:\nF*y_feed = -r AD*dy/dz + F*y\nWhere y is the mass fraction of the species of interest, r is the density, A is the cross-sectional area, and\nF is the mass flow rate (which is conserved across the boundary). Note that this is approximating that\nthe mass fraction gradient is very small outside the reactor so we omitted the -r A D*dy_feed/dz term\nfrom the left hand side, this approximation may not be very accurate.\nFor an example of using the Danckwerts condition: if y is a product absent from the feed, y_feed might\nbe zero but y inside the reactor will be non-zero, so\n0 = -r A D dy/dz + F*y\nThe analogous flux-balance equations for a multi-dimensional problem, where the boundary is\nperpendicular to the z-axis:\nVz*y_feed = -D*dy/dz + Vz*y\nNote that the velocity in the z direction Vz usually jumps as the flow enters the reactor because the\npacked bed reduces the free cross-sectional area, and because the temperature increase changes the\ndensity.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Differential-algebraic equations 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/1d690d228d4051aa73175b567ef61c9a_MIT10_34F15_Lec16.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nLecture 16:\nODE-IVP and Numerical Integration\n\nQuiz 1 Results\n- Mean: 70.6\n- Standard deviation: 11.0\n<50\n50-59\n60-69\n70-79\n80-89\n>89\n\nRecap\n- Implicit methods for ODE-IVPs\n\n- Example:\n- Use implicit Euler to solve:\nRecap\ndx\ndt = λx, x(0) = x0\nGive a closed form formula for the numerical solution\n\n- Example:\n- Use implicit Euler to solve:\n- Let:\n- Stability:\nRecap\ndx\ndt = λx, x(0) = x0\nxk = x(k∆t)\nxk+1 = xk + ∆tλxk+1\nxk+1 =\n1 -∆tλxk\nxk =\n✓\n1 -∆tλ\n◆k\nx0\n|1 -∆tλ| ≥1 ) (1 -∆tReλ)2 + (∆tImλ)2 ≥1\n∆tλ\n\n- Example:\n- Use implicit Euler to solve:\n- Numerical solution:\n- Exact solution:\n- Stability and accuracy do not correlate!\nRecap\ndx\ndt = λx, x(0) = x0\nxk =\n✓\n1 -∆tλ\n◆k\nx0\nxk = x0ekλ∆t\n∆tλ\n\n- Multistep methods utilize information over multiple time steps to\napproximate the solution of an ODE.\n- These can be designed for higher accuracy, larger stability bounds or\nboth.\n- Example: Leapfrog method\n- Approximate derivative with central difference:\nMultistep Methods\n2∆t (x(t + ∆t) -x(t -∆t)) = f(x(t), t)\ndx\ndt = f(x(t), t)\nx(tk+1) = x(tk-1) + 2∆tf(x(tk), tk)\nk -1\nk + 1\n\n- Local accuracy of the leap frog method:\n- Stability of the leap frog method:\nMultistep Methods\ndx\ndt = λx\nxk+1 = xk-1 + 2∆tλxk\n✓\nxk+1\nxk\n◆\n=\n✓\n2∆tλ\n◆✓\nxk\nxk-1\n◆\n✓\nxk+1\nxk\n◆\n= Ck\n✓\nx1\nx0\n◆\ndx\ndt =\n2∆t (x(tk+1) -x(tk-1)) + O((∆t)2) = f(x(tk), tk)\nx(tk+1) = x(tk-1) + 2∆tf(x(tk), tk) + O((∆t)3)\n\n- Local accuracy of the leap frog method:\n- Stability of the leap frog method:\nMultistep Methods\ndx\ndt = λx\nxk+1 = xk-1 + 2∆tλxk\n✓\nxk+1\nxk\n◆\n= Ck\n✓\nx1\nx0\n◆\ndx\ndt =\n(x(tk+1) -x(tk\n1)) + O((∆t)2) = f(x(tk), t\n-\nk)\n2∆t\nx(tk+1) = x(tk-1) + 2∆tf(x(tk), tk) + O((∆t)3)\nC =\n✓\n2∆tλ\n◆\nWhat are the eigenvalues of this matrix?\n\n- Local accuracy of the leap frog method:\n- Stability of the leap frog method:\n- Both eigenvalues of must be bounded:\nMultistep Methods\ndx\ndt = λx\nxk+1 = xk-1 + 2∆tλxk\nC\n|∆tλ ±\np\n(∆tλ)2 + 1| 1\n|∆tλ ± 1| 1\nconsider when :\n∆tλ\ni\n-i\ndx\n=\ndt\n(x(tk+1) -x(tk\n1)) + O((∆t)2) = f(x(tk), t\n-\nk)\n2∆t\nx(tk+1) = x(tk-1) + 2∆tf(x(tk), tk) + O((∆t)3)\n|∆tλ| ⌧1\n\n- Exercise:\n- Should I use the leap frog method to integrate the equations of\nmotion for a mass-spring system?\n- If so, what time steps should I limit myself to?\n- If not, what other integrator could I use?\nMultistep Methods\nmd2x\ndt2 = -kx\n\n- Exercise:\n- Should I use the leap frog method to integrate the equations of\nmotion for a mass-spring system?\n- Transform to system of first order ODEs:\n- Since eigenvalues are imaginary, leap frog is stable when:\nMultistep Methods\nd x\nm\n=\ndt2\n-kx\nd\n◆\n=\n✓\n-k/m\n◆✓\nv\ndt\n✓\nv\nx\nx\n◆\n-\nk\nEigenvalues of matrix:\nλ =\ni\n±\nr\nm\nm\n∆t <\nr\nk\n\n- Multistep methods can be implicit as well such as the backward\ndifferentiation formulas or Adams-Moulton integrators.\n- Example: Backwards differentiation\n- Second order accurate.\n- How would you identify the stability bounds?\nMultistep Methods\nxk+1 =\nxk\n-\nxk\n-1 +\n∆tf(xk+1, tk+1)\n\n- Consider the definite integral:\n- We can define a variable:\n- which, if is continuous, satisfies the differential equation:\n- Thus, a definite\nn be\ndetermined using methods for ODE-IVPs to compute:\nNumerical Integration\nZ tf\nf(⌧)d⌧\nt0\nx(t) =\nf(⌧)d⌧\nt0\nZ t\nd x(t) = f(⌧),\nx(t0) = 0\ndt\nintegral of a known, continuous function ca\nf(t)\nx(tf)\n\n- Consider the definite integral:\n- If the discontinuities in are known, then ODE-IVP solvers can be\nused in the domain between the discontinuities too!\n- If the discontinuities in are unknown, then Monte-Carlo methods\n(discussed later are a better option).\n- This approach is efficient with adaptive time stepping methods because\nan appropriate spacing between points can be chosen when\nchanges more or less rapidly with\n- For multi-dimensional integrals, this approach is not as straightforward,\nhowever.\nNumerical Integration\nZ tf\nf(⌧)d⌧\nt0\nf(t)\nf(t)\nf(t)\nt\n\nZ tk\nPk(⌧)d⌧=\ntk-1\n2 (f(tk) + f(tk-1)) (tk -tk-1)\nPk(⌧) = f(tk-1) +\n(f(tk)\ntk -tk-1\n-f(tk-1)) (⌧-tk-1)\n- One alternative is integration by polynomial interpolation:\n- where is a polynomial approximation of in the\ndomain\n- If the size of the domains of integration and the order of the\npolynomial interpolant can be used to control the accuracy of the\nintegration.\n-\nNumerical Integration\nZ t\nExample: quadratic interpolation - Simpson's rule:\nf\nt0\nf(⌧)d⌧=\nN\nX\nk=1\nZ tk\ntk-1\nf(⌧)d⌧⇡\nN\nX\nk=1\nZ tk\ntk-1\nPk(⌧)d⌧\nPk(⌧)\nf(⌧)\n⌧2 [tk-1, tk]\n\n- One alternative is integration by polynomial interpolation:\n- where is a polynomial approximation of in the\ndomain\n- If the size of the domains of integration and the order of the\npolynomial interpolant can be used to control the accuracy of the\nintegration.\n- Example: quadratic interpolation - Simpson's rule:\nNumerical Integration\nZ tf\nf(\nt0\n⌧)d⌧=\nN\nX\nt\nk=1\nZ\nk\nf(⌧\ntk-1\n)d⌧⇡\nN\nX\nk=1\nZ tk\ntk-1\nPk(⌧)d⌧\nPk(⌧)\nf(⌧)\n⌧2 [tk-1, tk]\nZ tk\ntk-1\nPk(⌧)d⌧= 1\n6 (f(tk) + 4f((tk + tk-1)/2) + f(tk-1)) (tk -tk-1)\n\n- Multidimensional integration:\n- Of the sort:\n- For any number of dimensions larger than 3, this is best handled\nwith Monte Carlo methods\n- For dimensions less than 3, this integration can be done with\npolynomial interpolation.\n- Fit the function to a polynomial of a prescribed degree within\nsmall regions of the domain of integration.\n- Sum integrals over the polynomial fits in each fit region.\n- This fails with higher dimensions because the number of fit\nregions grows exponentially with dimension.\n- Example:\nNumerical Integration\nZ yU\nL\nZ zU\nf(y, z)dydz\nzL\ny\n\n- Improper integrals:\n- Of the sort:\n- Can be split into two domains of integration\nNumerical Integration\nZ 1\nf(⌧)d⌧\nt0\nZ 1\nZ 1\nf( )d⌧=\nZ tf\n⌧\nf(⌧)d⌧+\nf(⌧)d⌧\nt\nt\nt\n-\nf\nThe first integral can be handled with ODE-IVP methods or\npolynomial interpolation\n- The second must be handled separately through either:\n- transformation onto a finite domain\n- or substitution of an asymptotic approximation\n- This same idea applies to integrable singularities as well.\n\n- Improper integrals:\n- Example:\nNumerical Integration\nZ tf cos ⌧\np\nd⌧\n⌧\n⇡\nZ t0 1 -⌧2/2\ntf cos ⌧\np\nd⌧+\n⌧\nZ\nt0\np\nd⌧\n⌧\n⇡\n1/2\ntf\n2t0\n-\n5/2\ncos ⌧\nt\n+\nd⌧\n5 0\nZ\nt0\np⌧\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Linear algebra 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/5ccbd31752f61f4f37d3637f05b78ae4_MIT10_34F15_Lec01.pdf",
      "content": "10.34: Numerical Methods\nApplied to\nChemical Engineering\nLecture 1:\nOrganization,\nNumerical Error,\nBasics of Linear Algebra\n\nOrganization\n-\nPurposes of the course:\n-\nEnsure that you are aware of the wide range of easily\naccessible numerical methods that will be useful in\nyour thesis research, at practice school, and in your\ncareer.\n-\nMake you confident in your ability to look up and\napply additional methods when you need them.\n-\nHelp you become familiar with MATLAB, other\nconvenient numerical software, and with simple\nprogramming/debugging techniques.\n-\nGive you an understanding of how common numerical\nalgorithms work and why they sometimes produce\nunexpected results.\n\nOrganization\n-\nResources:\n-\nwebsite - details on grading, homework\npolicy, and homework submission guidelines.\n-\nTextbook - Beers, \"Numerical Methods for Chemical\nEngineering\". Notes will be placed on\n.\nAdditional text references are given in the syllabus.\n-\nMATLAB tutorials\n-\nPeers - you are encouraged to discuss the course\nmaterial, programming, and the homework with your\ncolleagues. Be aware of the homework policy outlined\nin the syllabus, however.\n-\nTAs and instructors - we are here to help you, and\navailable for meetings, usually within 24 hours.\nCourse\nCourse website\n\nOrganization\n-\nWhen to stop:\n-\nThe homework for the course should require 9 hours\nper week on average - perhaps a little more early on if\nyou are not proficient with MATLAB.\n-\nSometimes you may find a homework problem is\nconsuming an inordinate amount of time even after\nyou have asked for help.\n-\nIf this happens, just turn in what you have completed\nwith a note indicating that you know your solution is\nincomplete, details about what you think went wrong,\nand what you think a correct solution would look like.\n\nOrganization\n-\nLinear algebra\n-\nSolutions of nonlinear equations\n-\nOptimization\n-\nInitial value problems\n-\nDifferential-algebraic equations\n-\nBoundary value problems\n-\nPartial differential equations\n-\nProbability theory\n-\nMonte Carlo methods\n-\nStochastic chemical kinetics\n\nNumerical Methods\n-\nMotivation:\n-\nMost real engineering problems do not have an exact\nsolution. Even if there is an exact solution. Can it be\nevaluated exactly?\n-\nApplication of computational problem solving\nmethodologies can lead to transformative (as opposed\nto incremental) engineering solutions.\n-\nAlgorithms to solve problems numerically should be:\n-\nclear\n-\nconcise\n-\nable to solve the problem robustly\n-\nuse realistic amount of resources\n-\nexecute in a realistic amount of time\n\nNumerical Error\n-\nVirtually all computer problem solving is done\napproximately. It is essential to quantify the error in\nthese calculations.\n-\nExample: representation of numbers\n-\nExample: calculating the square root\n⇡= 3.141592653589 . . .\nsignificand (24 bits)\nexponent (8 bits)\nbit\nn\nX\np-1\n1 +\nn\n=1\n⇥2-n\n!\n⇥2e\nps\nx2 -s = 0\nxn+1 =\ns\nx\n✓\nn + xn\n◆\nBabylonian method (iterative solution):\n\nNumerical Error\n-\nOverflow/underflow - exceeding the largest/smallest\nrepresentable number\n-\nExample: 1.3x1045 (nm)3 =1.3x109 (km)3\n-\nSolution: rescaling\n-\nTruncation:\n-\nComputers have a finite amount of memory/time to\nwork with. Most algorithms work within these\nconstraints to return answers which are accurate to\nwithin some tolerance.\n-\nSolution: the design of algorithms that quickly\nminimize truncation error\n-\nExample: Leibniz vs. Newton\n(\nn\nX\n-1)n\n=0 2n + 1 = ⇡\nX\nn=0\n2nn!2\n(2n + 1)! = ⇡\n\nNumerical Error\n-\nTruncation (cont.):\n-\nExample: Leibniz vs. Newton\n-\nAbsolute error:\n-\nRelative error:\nN\nX\nn=0\n2nn!2\n(2n + 1)! = ⇡\nN\nX\nn=0\n(-1)n\n2n + 1 = ⇡\nN\nLeibniz\nNewton\n0.66667\n0.66667\n0.86667\n0.73333\n0.72381\n0.76190\n0.74401\n0.78038\n0.80808\n0.78528\n0.78540...\n✏abs. = |xexact -xapprox.|\n✏rel. = |xexact -xapprox.|\n|xexact|\n\nNumerical Error\n-\nTruncation (cont.):\n-\nExample: 2x10-4 + 1x10-13 = ? with 8 digit accuracy\n-\nEstimate the absolute error in this calculation.\n-\nEstimate the relative error in this calculation.\n-\nQuantifying and minimizing numerical error is a key aspect\ndeveloping numerical algorithms.\n-\nEven simple calculations introduce numerical errors.\n-\nThose errors can compound and magnify. We will see\nhow shortly.\n\nLinear Algebra\n-\nPrimarily concerned with the solutions of systems of\nlinear equations\n-\nIs there a solution?\n-\nIf there is a solution, is it a unique?\n-\nIs it possible to find the solution or family of solutions?\n-\nChemical engineering example: mass balances\nseparator, 2:1\n?\n?\nm1 + m2 = 3\nm 2 = 2m 1\n\n-2\n!\nm1\nm2\n!\n=\n\n!\n3 kg/s\n\nLinear Algebra\nm1\nm2\nm1 + m2 = 3\n-2 m1 + m2 = 0\n(1, 2)\nm1\n+\nm2\n=\n-2 m1\n+\nm2\n=\n-\nRow-view:\n-\nEach row in the system of\nequations describes a line.\n-\nThe solution represents the\nintersection of these lines.\n-\nFor dimensions higher than 2, the\nsolution is an intersection of\nother linear manifolds\n-\nHow many solutions does the\nequation: ax=b, have?\n\nLinear Algebra\n-\nColumn view:\n-\nEach column in the system of\nequations describes a vector.\n-\nThe solution represents the\ncorrect weighting of these\nvectors.\n-\nWhile conceptually more difficult,\nthe column view is easier to\nextend to arbitrarily high\ndimensions. You will see why later.\n\nm 1\n+ m\n=\n-\n!\n\n!\n\n!\n\nLinear Algebra\nseparator, 2:1\n?\n?\n?\nseparator, 2:1\n3 kg/s\n1+δ kg/s\n?\n\n-1\n-2\n! 0\nB\n@\nm0\nm1\nm2\nC\nA =\n\n!\n\n! ⇣\nm2\n⌘\n=\n\n3 -(1 + d)\n2(1 + d)\n!\nRow-view:\nColumn-view:\n\nLinear Algebra\nseparator, 2:1\n?\n?\n?\nseparator, 2:1\n3 kg/s\n1+δ kg/s\n?\n\n-1\n-2\n! 0\nB\n@\nm0\nm1\nm2\nC\nA =\n\n!\n\n! ⇣\nm2\n⌘\n=\n\n3 -(1 + d)\n2(1 + d)\n!\nRow-view:\nColumn-view:\n\nSolving Systems of Equations\nIn MATLAB:\nx = A \\ b\nax = b ) x = a-1b\nAx = b ) x = A-1b\n\nScalars, Vectors and Matrices\n-\nScalars:\n-\nJust single numbers!\n-\nSet of all real numbers,\n-\nSet of all complex numbers,\n-\n\n-\nIf , then with\n-\nComplex conjugate:\n-\nMagnitude:\n-\nR\nC\n|z| =\np\nz z\ni =\np\n-1\nz 2 C\nz = a + ib\na, b 2 R\nz = a -ib\nR ⇢C\n\n-\nVectors:\n-\nOrdered sets of numbers:\n-\nSet of all real vectors with dimension N,\n-\nAddition:\n-\nMultiplication by scalar:\n-\nTranspose:\nScalars, Vectors and Matrices\n(x1, x2, . . . xN)\nRN\nB\nB\nB\n@\nx1\nx2\n...\nxN\nC\nC\nC\nA +\nB\nB\nB\n@\ny1\ny2\n...\nyN\nC\nC\nC\nA =\nB\nB\nB\n@\nx1 + y1\nx2 + y2\n...\nxN + yN\nC\nC\nC\nA\nc(x1 x2 . . . xN) = (cx1 cx2 . . . cxN)\nx =\nB\nB\nB\n@\nx1\nx2\n...\nxN\nC\nC\nC\nA\nxT = (x1 x2 . . . xN)\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\nScalar product:\n-\nNorm:\n-\nProperties:\n-\nNon-negative:\n-\nIf , then\n-\n\n-\nwith\n-\n\nx · y =\nN\nX\ni=1\nxiyi\nkxkp =\nN\nX\ni=1\n|xi|p\n!1/p\nkxkp = 0\nx = 0\nkcxkp = |c|kxkp\n|x · y| kxkpkykq\np, q > 0, 1/p + 1/q = 1\nkx + ykp kxkp + kykp\nkxkp ≥0\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\ninf-norm:\n-\nExamples of norms:\n-\n\n-\n\n-\n\n-\n\n-\nFamilies of vectors with the same\nnorm: 1-norm, 2-norm, inf-norm\nx = (\np\n2/2,\np\n2/2)\nkxk1 =\np\nkxk2 = 1\nkxk1 =\np\n2/2\nkxk1 = max\ni\n|xi|\nx1\nx2\nkxk1 kxk2 kxk1\nkxkp =\nN\nX\ni=1\n|xi|p\n!1/p\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\ninf-norm:\n-\nExamples of norms:\n-\n\n-\n\n-\n\n-\n\n-\nFamilies of vectors with the same\nnorm: 1-norm, 2-norm, inf-norm\nx = (\np\n2/2,\np\n2/2)\nkxk1 =\np\nkxk2 = 1\nkxk1 =\np\n2/2\nkxk1 = max\ni\n|xi|\nkxk1 kxk2 kxk1\nkxkp =\nN\nX\ni=1\n|xi|p\n!1/p\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\nComparing vectors with norm metrics:\n-\n\n-\nIf , then\n-\n\n-\nCalculating norms in MATLAB:\n-\nnorm( x, p ), norm( x, Inf )\n-\nHow many operations to compute the norm?\n-\nHow can I measure relative and absolute error for\nvectors?\nx\ny\nx -y\nkx -yk2 ≥0\nkx -yk2 = 0\nx = y\nkx -yk2 kx -vk2 + ky -vk2\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\nComparing vectors with norm metrics:\n-\n\n-\nIf , then\n-\n\n-\nCalculating norms in MATLAB:\n-\nnorm( x, p ), norm( x, Inf )\n-\nHow many operations to compute the norm?\n-\nThe relative and absolute error in a vector:\nx\ny\nx -y\nkx -yk2 ≥0\nkx -yk2 = 0\nx = y\nkx -yk2 kx -vk2 + ky -vk2\n\nScalars, Vectors and Matrices\n-\nVectors:\n-\nWhat mathematical object is the equivalent of an\ninfinite dimensional vector?\n\nScalars, Vectors and Matrices\n-\nVectors:\nWhat mathematical object is the equivalent of an\ninfinite dimensional vector?\n-\n\n-\nMatrices:\n-\nOrdered sets of numbers:\nScalars, Vectors and Matrices\nA =\nB\nB\nB\n@\n.\n.\n.\n.\nAN1\nAN2\n. . .\nANM\nA\n-\nSet of all real matrices with N rows and M columns, RN⇥M\n-\nAddition: C = A + B ) Cij = Aij + Bij\n-\nMultiplication by scalar: C = cA ) Cij = cAij\n-\nTranspose: C = AT ) Cij = Aji\n-\nTrace (square matrices):\nN\nTr A =\nX\nAii\ni=1\nA11\nA12\n. . .\nA1M\nA21\nA22\n. . .\nA2M\n..\n..\n..\n..\nC\nC\nC\n\n-\nMatrices:\nM\n-\nMatrix-vector product: y = Ax ) yi =\nX\nAijxj\nj=1\nM\n-\nMatrix-matrix product: C = AB ) Cij =\nperti\nk\nX\nAikBkj\n-\nPro\nes:\n=1\n-\nno commutation in general: AB = BA\n-\nassociation: A(BC) = (AB)C\n-\ndistribution: A(B + C) = AB + AC\n-\ntransposition: (AB)T = BT AT\n-\ninversion: A\n- 1 A =\nAA\n- 1 = I if det(A) = 0\nScalars, Vectors and Matrices\n\n-\nMatrices:\n-\nMatrix-matrix product:\n-\nVectors are matrices too:\n- x\nRN\nN⇥1\nx 2 R\n- y T 2 RN yT 2 R1⇥N\n-\nWhat is: y T x ?\nM\nC = AB ) Cij =\nX\nAikBkj\nk=1\nScalars, Vectors and Matrices\n\n-\nMatrices:\n-\nMatrix-matrix product:\n-\nVectors are matrices too:\n-\n\n-\n\n-\nWhat is: ?\nScalars, Vectors and Matrices\nx 2 RN\nx 2 RN⇥1\nyT 2 RN yT 2 R1⇥N\nyT x\n\n-\nMatrices:\n-\nDyadic product: A = xyT = x ⌦y ) Aij = xiyj\n-\nDeterminant (square matrices only):\nN\ndet(A) =\n(-1)i+jAijMij(A)\nScalars, Vectors and Matrices\nX\nj=1\nMij 0\nA11\nA\n. . .\nA\nB\n1(j-1)\nA1(j+1)\n. . .\nA1N\nB\nB\nA21\nA22\n. . .\nA2(j-1)\nA2(j+1)\n. . .\nA2N\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\nC\ndet\nB\nB\nB\nB\nB\nB A(i-1)1\nA(j\n1)2\n. . .\nA\nA\n-\n(i-1)(j-1)\n(i-1)(j+1)\n. . .\nA(i\nN\nC\n-1)\nB\nB A(i+1)1\nA(j+1)2\n. . .\nA(\nC\nC\ni+1)(j\nA\n-1)\n(i+1)(j+1)\n. . .\nA(i+1)N\nC\nB\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nC\n@\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nC\nC\nC\nAN1\nAN2\n. . .\nAN(j-1)\nAN(j+1)\n. . .\nANN\nC\nC\nC\nA\n- det(c) = c\n(A) =\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Models vs. Data 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/168c68e26eabe2e8c9f0ecc5ae2cc161_MIT10_34F15_Lec28.pdf",
      "content": "Models vs. Data\n10.34 Fall 2015\nby W.H. Green\n\nDefinitions\n- We're comparing experimental Data points yi\nmesured with knob settings \"x\" to model\npredictions fi(x,θ) where θ are the parameters\nin the model we cannot control\n\nStart by assuming large number N of\nrepeats of each experiment...\n- By central-limit-theorem of statistics, for case\nwith a single observable, a true model, accurate\nparameters and knob values, and many repeats:\n\np(<y>|x,θ) = (2π)-1/2 σ-1 exp (-χ2/2)\n- Where χ2 = ({<y> - f(x,θ)}/σ)2\nAnd estimated variance of the mean σ = (<y2>-<y>2)/N2\n- For multiple quantities measured on same\nexperiment need to consider Covariance of data:\nCovij = {Σ (yi,n - <yi>)(yj,n - <yj>)} / N\n- So we estimate Covariance of the Means (for\nlarge N): Cij ~ Covij / N\n\nFor many experiments dependent on\nsame parameters θ\nEach measurement repeated many times with same knob\nsettings xm, then new knob settings; all the knob settings xm(k) are\nstored in matrix X. The mean measurements are stored in a K-\nvector <y>. If multiple observables {yi, yj, ...} measured in each\nexperiment K>M.\n\np(<y>|X,θ)=(2π)-K/2|C|-1/2 exp( - 1⁄2 χ2)\nwhere\n\nχ2 = ΣΣ (<yk>-fk(xm(k),θ))Dkz(<yz>-fz(xm(z),θ))\nand D = inv(C)\n- Often the covariance is ignored, then Dkz = δkzσk\n-2\n\nProbability of observation depends on\nχ2; if very improbable we flag a\ndiscrepancy between model & data\n- User must decide tolerance on \"improbable\".\n- For example: If you decide <5% chance is\nimprobable, and you performed 12 experiments\n(each repeated many times to get a good average\n<yk> and estimate of σk) and adjusted 2 model\nparameters, then you can use Matlab function\nchi2inv:\nchi2max = chi2inv(0.95,12-2)\nin this case case chi2max =18.3\nif measured χ2 > 18.3 you would say there is a\ndiscrepancy between the model and the data\n\nOrigin of chi2inv\nThe probability that a measured data set (with many repeats) would\nyield a χ2<Q is given by:\nProb(χ2<Q) = ∫∫ dK<y> p(<y>|X,θ) H(Q-χ2)\nwhere H is the Heaviside function.\nThis K-dimensional multiple integral can be simplified by change of\nvariables to the single integral shown in the Matlab chi2inv\ndocumentation.\nIf M parameters have been adjusted to fit the data it is customary to\nuse K-M degrees of freedom when computing chi2inv (this assumes\neach parameter adjustment really improved the fit). If no adjustment\nto fit the data (a pure prediction), M=0.\nIf you select a desired Probability, that choice fixes the value of Q (aka\nchi2max).\n\nOnce we have decided the maximum\nχ2 we will tolerate Q, than we have\ndefined a \"region of indifference\" in\nparameter space\n- As far as we can tell from our experiment, any θ\nwhich gives a \"good enough\" fit is OK, we cannot\ndiscriminate.\n- To see the range of acceptable parameter values,\nplot the hypersurface χ2(θ)=Q. Any θ inside the\nsurface is acceptable.\n- For a model which depends nonlinearly on the\nparameters, the shape of the region can be quite\nconvoluted....\n\nBayesian view\np(X,θ|<y>) = pprior(θ)pprior(X)p(<y>|X,θ)\nwhere \"prior\" means \"we have other prior\ninformation about these values, not just what we\ncan infer from this data set\".\nUsually journal readers are not interested in our\nimprecise knowledge of our knob settings, so we\nintegrate this uncertainty out to get our new\nimproved \"posterior\" p(θ) that we will report:\np(θ)= pprior(θ) ∫∫ dWX pprior(X)p(<y>|X,θ)\nContours of this new p(θ) can also have a very\nconvoluted shape...\n\nSimplifying from confidence regions to\nseparate confidence intervals\n- Often people like to report parameter values\none at a time, e.g. if one θ is a heat capacity:\n\nCp(533 K) = 89.3 ± 0.2 J/mol-K\n- Usually people report the best-fit value as the\nnominal value and then need to give an\nestimate of the confidence interval. One way\nto compute the upper limit of the interval:\nθv,max = maxθ θv\n\ns.t. χ2(θ)<Q\n\nCorrelation of parameters\n- Often two parameter values are highly\ncorrelated, e.g. you can get a good fit if θ1 and θ2\nhave some relationship e.g. θ1+θ2 = const or θ1/θ2\n= const, but very poor fits for other values of\n(θ1,θ2). This information is lost if you just report\nthe values and error bars separately.\n- Sometimes you can change parameters to the\nappropriate well-determined combination.\n- How to report the correlation of determined\nparameter values?\n\nCorrelation of parameters, page 2\n- Usually what is done is to compute the Hessian of χ2(θ)\nevaluated at θbestfit. Diagonalize this matrix; its\neigenvectors are the principal components of a hyper-\nellipsoid that (exactly for a linear model, approximately\notherwise) describes the region of indifference.\n- If an eigenvector has large components from more\nthan one parameter, that means the parameters are\ncorrelated. The \"covariance of the parameters\" is given\nby Cjk = Σ VjiVki/λi\n- This can be computed by SVD, often this is more\nnumerically stable, see Numerical Recipes.\n\nA note about Hessian of χ2\n- The rigorous formula for the second derivative of\nχ2 includes two terms.\n- Almost everyone neglects the second term, which\nis sensitive to noise in the data, and just uses:\nHlz ~ Σ Jkl Jkz σk\n-2 where Jkl = ∂fk/∂θl\n- Note now the Hessian doesn't really depend on\nthe experimental data (at all for a linear model),\nyou can compute it before the experiment\nbegins...see page 413 in Beers' text. His \"X\" is the\nJacobian of the model w.r.t. to the θ.\n\nAre the Model & Data Consistent?\nOften, the measured χ2 is greater than the Q you would\ncompute from chi2inv. What do you need to check\nbefore you say you have disproved the model?\n1) Need to be sure you have found the very best possible values of all\nthe parameters. Can have many local minima. Global optimization?\n2) Need to be sure you have done enough repeats. If N is small\nprobability is non-Gaussian with \"fat tail\".\n3) χ2 is extremely sensitive to estimate of σ (or D). Double check if you\nreally believe these values.\n4) Uncertainties in X and any parameters θ you did not adjust might\naffect χ2. Perhaps you can include these uncertainties in σ.\n5) Often models are idealizations that do not really match\nexperimental boundary conditions, mixing, etc. Can be tricky to try to\nrig up a model that really matches your experimental apparatus.\n6) Be sure that you modeled 'instrumental function' or calibration of\nyour signals carefully.\n\nExperimental Design\n- For linear models, you can compute the covariance of\nthe model parameters BEFORE you do any\nexperiments. Often you want to design the experiment\nso you are only sensitive to one or two parameters.\n- Do it! So many people do experiments and then\nafterwards realize they cannot possibly determine their\nparameter of interest from the data.\n- Sometimes you can fix the problem by using different knob\nsettings X. You can play with this in your model before you\ndo the experiments.\n- Even for nonlinear models you can do this ahead of\ntime, using the Jacobian evaluated at your prior\nnominal value of θ.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Numerical Methods Applied to Chemical Engineering: Joe Scott notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/10-34-numerical-methods-applied-to-chemical-engineering-fall-2015/80f531a5c6a9ec6c104e2f5e14af9225_MIT10_34F15_NotesFromJoe.pdf",
      "content": "Stochastic Chemical Kinetics\nJoseph K. Scott∗\nNovember 10, 2011\nIntroduction to Stochastic Chemical Kinetics\nConsider the reaction\nI + I →D.\nThe conventional kinetic model for the concentration of I in a closed system is given by\ndCI (t) = -kCI(t)CI(t),\ndt\nwhere k is the rate constant. The stochastic approach to chemical kinetics is concerned\nwith modeling chemical reactions in situations where the assumptions underlying equa-\ntions of this type break down. In order to illustrate this, suppose that the reaction above\nis occuring in a drop of water of volume V , which contains only a very small number, say\n3, of molecules of I. The kinetic equation above encounters the following complications:\n1. The state CI changes in discrete increments. We can rewrite the equation above in\nterms of the number of molecules of I as\ndNI\ndt (t) = -\nkNAv\nI\nV\n\nNI(t)N (t),\nwhere NAv is Avogadro's number. Solving this ODE from some known initial con-\ndition, we are assuming that the number of molecules NI changes continuously and\ncan take any value in R (this is called a continuum assumption).\nSurely, if we\nbegin solving this equation with NI(t0) = 3, there will come a time t′ at which\nNI(t′) = 2.5. However, in reality this quantity should only take integer values; it\ndoesn't make physical sense to have 2.5 molecules. In cases where we have a very\nlarge number of molecules, say NI ≈1 × 1023, this issue can be easily overlooked\nbecause a difference of 0.5 out of 1023 causes very little error in the rate equation.\nHowever, the problem can become very serious for small numbers of molecules. In\nthe worst case, if NI = 2, then only one reaction can occur, after which NI = 0\nand the reaction rate must also be zero. However, the continuum approximation\npredicts a nonzero rate with, for example, NI = 0.5.\n2. Reactions occur as discrete events. The continuum approximation predicts a nonzero\nrate of reaction whenever, NI = 0. However, in reality reactions are not always\n\noccuring. Reactions occur when two molecules collide with each other in the proper\norientation and with sufficient energy to cause a chemical change.\nTherefore, a\ncorrect model should have intervals of time in which no reactions occur, punctuated\nby instantaneous reaction events that change NI in integer increments, according\nto the reaction stoichiometry. From this prospective, it is incorrect to talk about\na reaction rate at all. Again, if there are very many molecules (NI ≈1023), then\nreactive collisions will happen very frequently and, insofar as we are only interested\nin the macroscopic average number of these reaction events in a given interval of\ntime, it is completely excusable to think in terms of reaction rates and apply the\ncontinuum approximation.\n3. Reaction events are stochastic. Because the continuum approximation regards re-\nactions as happening continuously with some rate, there is no need to worry about\nwhen the underlying reaction events occur. However, if we regard reactions as dis-\ncrete events, how will we predict when these events will occur? Short of modeling\nthe motion of every molecule in our system using Newton's equations of motion,\nwe cannot answer this question precisely. We will have to settle for the probability\nthat some reaction will occur in a given interval of time. From this prospective, our\nperceived behavior of systems with very few molecules is not deterministic. That\nis, in contrast to what the continuum approximation suggest, the initial condition\nof the system will not fully specify the system at later times. Rather, the state at\nlater times depends on what sequence of reaction events occurs, which we can only\ncharacterize in terms of probabilities. Systems of this type are called stochastic.\nTo reiterate, we are concerned with studying (and ultimately simulating) the behavior\nof chemically reacting systems in situations where the continuum equations do not ap-\nply. In general, this will only happen when one or more reactions depend on a chemical\nspecies which is present in very small numbers. However, there are other situations in-\nvolving unstable or metastable systems which also require a stochastic treatment because\nfluctuations in the number of molecules are important. Since the continuum model only\ncaptures some kind of average behavior, it cannot predict phenomena which depend on\nfluctuations.\n1.1\nMotivating Example\nA very interesting system related to cell signaling pathways in human immune response\nwas studied by researchers in Prof. Charkraborty's lab here at MIT. Their work shows\nthat, not only is stochastic simulation warranted for these systems, but also that the\nstochastic model predicts fundamentally different behavior than does the continuum ap-\nproximation, and this behavior is crucial to the functioning of the signalling pathway. If\nyou are interested, see Artyomov et. al, Purely stochastic binary decisions in cell signaling\nmodels without underlying deterministic bistabilities, PNAS, 104, 48, 18958-18963, 2007.\nHowever, note that this is not an isolated application. Since the popularization of the\nstochastic approach in the 1970's, there has been a steady increase in its application and\ntoday the literature positively abounds with examples.\n\nFormalization of the Stochastic Problem\nConsider a volume V containing N chemically reacting species, S1, . . . , SN, and denote\nthe number of molecules of each at time t by\nX(t) = (X1(t), . . . , XN(t)).\nThese species can undergo M chemical reactions, R1, . . . , RM. Each reaction Rμ has an\nassociated stoichiometry vector νμ, which describes the discontinuous change in X when\nthe reaction Rμ occurs. For example, if N = 3, then the reaction\nS1 + S2 →S3\n(1)\nhas stoichiometry vector ν = (-1, -1, 1). If this reaction occurs at tˆ and the number of\nmolecules immediately before tˆ is X(tˆ-dt) = (10, 10, 10), then after the reaction occurs\nwe have X(tˆ+dt) = (9, 9, 11). In general, the occurrence of reaction Rμ at time tˆchanges\nthe state vector according to\nX(tˆ+ dt) = X(tˆ-dt) + νμ.\nAs mentioned previously, we will not simulate reacting systems in enough detail to\nsay deterministically when a given reaction will occur. Instead, we assume that these\nreactions occur stochastically, according to some probability distributions. Our goal is to\nunderstand what these distributions are, and how they can be used to describe the state\nof the system at future times probabilistically. In the following subsections, we present\nsome preliminary derivations concerning the probabilities of reaction events. In Sections\n3 and 4, we use these derivations to construct numerical simulation methods.\n2.1\nThe Number of Possible Rμ Reactions\nIn order for a reaction Rμ to occur in V , it is necessary that one molecule of each of the\nreactant species collide with each other at some time. At any given time, it is possible that\nthere are many different combinations of reactant molecules that could collide and cause\na reaction. Exactly, how many distinct combinations there are depends on how many of\neach molecule is present, i.e. on X(t). In general, we denote the number of unique groups\nof reactants that could collide to cause reaction Rμ by hμ(X(t)).\nExamples:\n1. Consider the simple bimolecular reaction (1). If there are 2 molecules of S1 and\none molecule of S2, then there are 2 pairs of molecules that can collide to cause a\nreaction. In general,\nhμ(X(t)) = X1(t)X2(t).\n2. A more interesting case is the reaction\nS1 + S1 →S3.\n(2)\nIn this case\nhμ(X(t)) = 2X1(t)(X1(t) -1),\n\nnot the expected X1(t)X1(t). This is because the reaction requires two distinct S1\nmolecules; we cannot use the same molecule twice. You can convince yourself that\nthe factor (X1(t) -1) is correct by considering the case where there is only one S1\nmolecule in V . In this case, there are zero complete groups of reactants. The factor\nof 1/2 comes from the fact that we do not care about the order of the molecules.\nFor two distinct molecules of S1, call them S′\n1 and S′′\n1 , the pair (S′\n1, S′′\n1 ) constitutes\na single group of reactants, regardless of whether we order them as (S′\n1, S′′\n1 ) or as\n(S′′\n1 , S′\n1).\n3. Consider the isomerization\nS1 →S2.\n(3)\nIsomerizations do not occur due to collisions at all, but rather due to some spon-\ntaneous change in the quantum state of the molecule.\nNonetheless, if there are\n2 molecules of S1, it is clear that there are 2 \"groups\" of reactant molecules. In\ngeneral,\nhμ(X(t)) = X1(t).\n2.2\nThe Probability that Reaction Rμ Occurs in [t, t + dt]\n2.2.1\nThe Fundamental Hypothesis\nSupposing that at least one complete group of Rμ reactants exists in V , let πμ(t, dt)\ndenote the probability that a particular one of these groups will react in the time interval\n[t, t + dt]. The fundamental hypothesis of the stochastic approach to chemical kinetics is\nthe following: For each reaction Rμ, there is a constant cμ such that\nπμ(t, dt) = cμdt.\nIn words, the fundamental hypothesis states that, for small enough durations dt, the\nprobability that a particular group of Rμ reactants will react in V during the interval\n[t, t + dt] increases linearly with dt.\nThis assumption is well justified for elementary\nunimolecular and bimolecular reactions.\nIn the bimolecular case, one can derive the\nconstant cμ from the kinetic theory of gases (which you will study extensively in 10.65\nnext semester). The basic assumptions behind this derivation are:\n1. the positions of the molecules in V are random and uniformly distributed throughout\nV ,\n2. the velocities of the molecules in V are distributed according to the Maxwell-\nBoltzmann distribution.\nUnder these assumptions, it is possible to derive the probability of a collision between\ntwo molecules within a given time interval. The purpose of mentioning this is to point\nout that the stochastic approach to chemical kinetics will be valid for gas phase systems\nin thermal equilibrium, but not necessarily in other settings.\nA consequence of the fundamental hypothesis, which greatly simplifies the analysis to\nfollow, is that the probability that multiple reaction events, of any kind, occur in [t, t+dt]\ncan be shown to scale as O(dt2). Then, in the limit as dt →0, the probability of all\nsuch situations tends toward zero more rapidly than dt, and becomes dominated by the\nprobability of the simpler outcomes where only a single reaction occurs in [t, t + dt]. For\nthis reason, we can always assume in the following derivations that all single reaction\nevents in [t, t + dt] are mutually exclusive, because only one reaction can occur. Though\nthis is not strictly true, the error in making this assumption will vanish as dt →0.\n\n2.2.2\nThe Probability of an Rμ Reaction in [t, t + dt]\nIn order to understand how the state of a reacting system X(t) evolves probabilistically,\nwe will require the probability that exactly one reaction occurs in V during [t, t+dt], and\nit is an Rμ reaction. By the simplification that at most one reaction can occur during\n[t, t + dt] this is equivalent (to within O(dt2)) to several other probabilities:\nPr(exactly 1 rxn occurs in [t, t + dt] and it is an Rμ reaction | X(t) = n)\n(4)\n= Pr(exactly 1 Rμ rxn occurs in [t, t + dt] | X(t) = n),\n= Pr(at least 1 Rμ rxn occurs in [t, t + dt] | X(t) = n).\nTo compute this probability, recall that there are hμ(X(t)) distinct groups of reactants\nthat could possibly react in [t, t + dt], each with probability cμdt. Since we may assume\nthat these hμ possible reactions are mutually exclusive, then we may sum the individual\nprobabilities to get\nPr(exactly 1 Rμ rxn occurs in [t, t + dt] | X(t) = n) = hμ(n)cμdt.\n2.2.3\nThe Connection with Rate Constants\nIn stochastic chemical kinetics, cμ plays the role of the rate constant kμ in determinis-\ntic, continuum kinetics equations. For example, for the reaction shown in (1) we have\nhμ(X(t)) = X1(t)X2(t), so the probability of observing exactly one Rμ reaction some-\nwhere in V during [t, t + dt] is\ncμX1(t)X2(t)dt,\nwhich is very reminiscent of the rate expression in the continuum approximation,\nrμ(t) = kμC1(t)C2(t).\nOf course, for the dimerization reaction (2), these expressions do not agree so well.\nFor the purposes of simulating stochastic reacting systems, we will always assume that\nthe reaction parameters cμ are known.\n2.3\nThe Probability Exactly One Reaction Occurs in [t, t + dt]\nFrom §2.2, we know that the probability that exactly one Rμ reaction will occur during\n[t, t + dt] is\ncμhμ(X(t))dt.\nSince we need only consider the case where at most one reaction occurs in [t, t + dt],\nthe occurrences of each different type of reaction R1, . . . , RM are mutually exclusive. It\nfollows that we can simply sum probabilities to get\nPr(exactly 1 rxn occurs in [t, t + dt] | X(t) = n)\nM\n=\nX\nPr(exactly 1 Rμ rxn occurs in [t, t + dt] | X(t) = n),\nμ=1\nM\n=\nμ\nX\nhμ(n)cμdt\n=1\n\n2.4\nThe Probability that No Reactions Occur in [t, t + τ]\nIn the stochastic view of chemical kinetics, there are periods of time in which nothing\nhappens. That is, molecules diffuse around in V but, for some period of time, none of\nthem collide in a such a way that a reaction occurs. In order to accurately simulate this\nsituation, we need some way to characterize when the next reaction will happen. A formal\nway to ask this question is: Given that X(t) = n, what is the probability that no reactions\noccur in V within the time interval [t, t + τ]? We denote this probability by P0(τ, n).\nWe consider first the probability P0(o, n), where o is small. For small enough o, we\nmay assume that at most one reaction occurs in V during [t, t + o]. It follows that\nP0(o, n) = 1 -Pr(exactly 1 rxn occurs in [t, t + o] | X(t) = n),\nM\n= 1 -\nμ\nX\nhμ(n)cμo.\n=1\nIn order to calculate P0(τ, n), we divide [t, t + τ] into a large number K intervals of\nlength o = τ/K:\n[t, t + o], [t + o, t + 2o], . . . , [t + (K -1)o, t + Ko].\nNoting that\nP0(τ, n) = Pr(no rnx occurs in [t, t + o]\nand no rxn occurs in [t + o, t + 2o]\nand\n...\nand no rxn occurs in [t + (K -1)o, t + Ko] | X(t) = n),\nwe can expand this probability as\nP0(τ, n) = Pr(no rnx occurs in [t, t + o] | X(t) = n)\n× Pr(no rxn occurs in [t + o, t + 2o] |\nX(t) = n and no rnx occurs in [t, t + o])\n...\n× Pr(no rnx occurs in [t + (K -1)o, t + Ko] |\nX(t) = n and no rnx occurs in [t, t + (K -1)o]).\nFor any j ≤K, whether or not a reaction occurs in the interval [t + (j -1)o, t + jo] is\nindependent of the fact that no reaction occurred in [t, t + (j -1)o], except for the fact\nthat this implies X(t + (j -1)o) = X(t). Then\nP0(τ, n) = Pr(no rnx occurs in [t, t + o])\n× Pr(no rnx occurs in [t + o, t + 2o] | X(t + o) = n)\n...\n× Pr(no rnx occurs in [t + (K -1)o, t + Ko] |\nX(t + (K -1)o) = n),\n\nfrom which it follows that\nP0(τ, n) = P0(o, n) × P0(o, n) × . . . × P0(o, n).\nUsing the result for small o,\nP0(τ, n) =\n\"\nM\nτ\nμ\nX cμhμ(n)\n-\n=1\nK\nK\n#\n.\nSince this is true for any sufficiently large K, it follows that\nM cμhμ(n)τ\nP0(τ, n) = lim\n\"\n1 -\nK→inf\nμ\nX\n=1\nK\nK\n#\n,\n= exp\n\nM\n-\nX\ncμhμ(n)τ\nμ=1\n!\n.\nTherefore, the probability that no reaction occurs in the interval [t, t + τ] depends on all\nof the reaction parameters cμ and decreases exponentially with the length of the interval\nτ.\nThe Master Equation\nThe classical approach to modelling the evolution of stochastic reacting systems is to use\nthe so-called chemical master equation. The master equation does not describe the change\nin X(t), as the continuum equations would, because this vector varies stochastically.\nInstead, the master equation describes the grand probability function\nP(n, t | n0, t0) = Pr(X(t) = n | X(t0) = n0).\nThe master equation takes the form of a differential equation that describes how this\nprobability function changes in time. To derive it, lets suppose that P(n, t | n0, t0) is\nknown for all n and attempt to derive an expression for P(n, t + dt | n0, t0). This is done\nby simply summing up the probabilities of the distinct (i.e. mutually exclusive) ways in\nwhich the system could come to be in state n at time t + dt:\n1. The system was in state n at time t and no reactions occurred during [t, t + dt],\n2. The system was in the state n -νμ at time t and one Rμ reaction (and only this\nreaction) occurred during [t, t + dt],\n3. Some sequence of multiple reactions occuring in [t, t + dt] led to n at t + dt.\nAs a consequence of the fundamental hypothesis, it was argued in §2.2 that we can ig-\nnore the third possibility in the limit as dt →0. Accounting for the other two possibilities,\n\nwe have\nM\nP(n, t + dt | n0, t0) = P(n, t | n0, t0)P0(dt, n) +\nX\nP(n -νμ, t | n0, t0)hμ(n -νμ)cμdt,\nμ=1\n= P(n, t | n0, t0)\n\"\nM\n1 -\nμ\n#\nX\nhμ(n)cμdt\n=1\nM\n+\nX\nP(n -νμ, t | n0, t0)hμ(n -νμ)cμdt.\nμ=1\nRearranging gives,\nP(n, t + dt | n0, t0) -P(n, t | n0, t0)\nM\n=\nX\nP(n -νμ, t | n0, t0)hμ(n -νμ)cμ\ndt\nμ=1\n-P(n, t | n0, t0)hμ(n)cμ ,\nwhich, in the limit as dt →0 gives the differential equation\n\ndP(n, t | n0, t0)\nM\n=\nX\n[P(n -νμ, t | n0, t0)hμ(n -νμ)cμ -P(n, t | n0, t0)hμ(n)cμ] .\ndt\nμ=1\nSupposing that, due to a limited number of reactant molecules in a closed system, we know\nthat there are only a finite number of possible states of the system n1, . . . , nQ. Then the\nmaster equation is actually a system of Q coupled ODEs, with one ODE describing each\nof the time-varying probabilities\nP(\nn , t | n0, t0), . . . , P(nQ, t | n0, t0).\nThe solution of the master equation contains complete information about the stochas-\ntic behavior of the system. It describes the entire PDF of X(t) for every time t. Unfortu-\nnately, the master equation is very difficult to solve in general. This is because the master\nequation may involve a huge number of state variables. Note that the state variables of\nthe master equation are the probabilities of every possible state of the reacting system.\nAs a trivial example, in a system with only one chemical species A, the master equation\nhas one ODE for the probabilities of each of the following states:\nthere is 1 molecule of A\nthere are 2 molecules of A\nthere are 3 molecules of A\n...\nthere are j molecules of A\n...\nWhen there are multiple reacting species, we must account for every possible combination\nof molecule numbers, and the number of ODEs in the master equation easily reaches\ninto the billions. Because the master equation ODEs are actually linear in these states\n(and very sparse), it is possible to numerically solve the master equation as an IVP in\nsituations where the number of ODEs is surprisingly large.\nResearchers in Professor\n\nBarton's laboratory have developed methods capable of solving master equations with\nup to 200 million ODEs. Even so, master equations of this size still correspond to very\nsimple physical systems. For this reason, most researchers take a Monte Carlo approach,\nwhich provides less information but is more computationally tractable. We discuss this\napproach in the next section.\nGillespie's Algorithm\nThe solution of the master equation provides, at each time t, a PDF for the random\nvector X(t). This is a great deal of information, which is why the master equation is so\ndifficult to solve. Rather than compute the complete PDF of X(t) for every t, Gillespie's\nalgorithm computes a sample from this PDF. That is, it computes a single trajectory in\ntime. Like the Monte Carlo methods we studied previously, Gillespie's algorithm produces\na different result every time it is run. However, if it is run a large number of times for the\nsame system, then the frequency of observing a state, say X(tˆ) = n∗, will be proportional\nto the value of the PDF that results from solving the master equation, P(n∗, tˆ | n0, t0).\nBecause only a sample is computed, a single run of Gillespie's algorithm is dramatically\nless expensive that solving the master equation. The down side, however, is that it may\nbe necessary to do a huge number of Gillespie simulations in order to obtain an accurate\ndescription of the complete PDF.\nBeginning from X(t) at some time t, a single step of Gillespie's algorithm first generates\na random number τ which represents the time at which the next reaction occurs. Then,\na random integer μ is generated which determines which reaction occurs at t + τ. Once\nthese numbers are known, we simply update the time to t + τ and the state vector to\nX(t + τ) = X(t) + νμ.\n4.1\nDetermining the Next Reaction Time\nWe require the probability that the next reaction occurs in the infinitesimal time interval\n[t + τ, t + τ + dτ].\nDenote this probability by Pnext(τ, n)dτ.\nThis probability can be\ncomputed by noting that\nPnext(τ, n)dτ = Pr(no rxn occurs in [t, t + τ] | X(t) = n)\n× Pr(exactly one rxn occurs in [t + τ, t + τ + dτ] |\nX(t) = n and no rxn occurs in [t, t + τ]),\n= Pr(no rxn occurs in [t, t + τ] | X(t) = n)\n× Pr(exactly one rxn occurs in [t + τ, t + τ + dτ] | X(t + τ) = n),\n= P0(τ, n) ×\n\" M\nμ\nX\nhμ(n)cμdτ\n=1\n#\n,\n= exp\n\nM\nM\n-\nμ\n! \"\n#\nX\nhμ(n)cμτ\n=1\nμ\nX\nhμ(n)cμdτ\n.\n=1\nTo simplify notation, define the total reaction propensity:\nM\na(n) ≡\nμ\nX\nhμ(n)cμ.\n=1\n\nThen,\nPnext(τ, n) = a(n) exp (-a(n)τ) .\nFor fixed n, this is the PDF of the random variable τ. In order to determine the next\nreaction time for Gillespie's algorithm, we would like to sample this PDF. It turns out\nthat this can be done by generating a random number r1 from the uniform distribution\non the interval (0, 1) and computing\nτ = a(n) ln\n.\nr1\n\nThis is an example of the inversion method, which is one of the available methods for\nconverting the output of a uniform random number generator to a sample of a desired\nPDF.\n4.2\nDetermining the Next Reaction Type\nWe now need to derive the probability that an Rμ reaction occurs in [t + τ, t + τ + dτ]\ngiven that X(t) = n and the next reaction is known to occur in this same interval. As\nusual, we can neglect the possibility of multiple reactions occuring in this time interval,\nso that we are guaranteed that exactly one reaction occurs in [t + τ, t + τ + dτ]. There\nare then M mutually exclusive possibilities, R1, . . . , RM, each with probability\nhμ(n)cμdτ,\nμ = 1, . . . , M.\nClearly, the probability that reaction Rμ′ occurs is\nhμ′(n)cμ′dτ\nPM\nμ=1 hμ(n)cμdτ\n= hμ′(X(t))cμ′ .\na(n)\nFor fixed n, this is the PDF of the random variable μ′, which we need to sample in order\nto determine the next reaction type in Gillespie's algorithm. This is done by sampling\na number r2 from the uniform distribution on (0, 1) and choosing μ′ to be the smallest\ninteger such that\nμ\n≤\nPμ′\nμ= hμ(n)c\nr2\n.\na(n)\nIn fact, this is another application of the inversion method. In this case it is easy to see\nthat the probability of selecting μ′ in this way is proportional to the length of the interval\n\"Pμ′-1\nμ=1 hμ(n)cμ\na(n)\n,\nPμ′\nμ=1 hμ(n)cμ\n,\na(n)\n#\nwhich is simply\nhμ′(n)cμ′ ,\na(n)\nas desired.\n\n4.3\nAlgorithm\n1. Initialize: t = t0, X(t) = n0.\n2. While t < tf:\n(a) Compute the total reaction propensity a(X(t)).\n(b) Sample two random numbers, r1 and r2, from the uniform distribution on\n(0, 1).\n(c) Determine the next reaction time as τ =\na(X(t)) ln\n\nr1 .\n′\n(d) Determine the reaction type as the smallest μ′ such that\n\nr2 ≤Pμ\nμ=1 hμ(X(t))cμ/a(X(t)).\n(e) Carry out the reaction event determined above:\n- Set t := t + τ.\n- Set X(t) := X(t) + νμ′.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n10.34 Numerical Methods Applied to Chemical Engineering\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}