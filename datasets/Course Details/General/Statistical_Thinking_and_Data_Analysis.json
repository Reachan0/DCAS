{
  "course_name": "Statistical Thinking and Data Analysis",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1.5 hours / session\n\nCourse Prerequisites\n\nAn understanding of Calculus and\n6.041 Probabilistic Systems Analysis and Applied Probability\nor\n18.440 Probability and Random Variables\n.\n\nOverview\n\nThis course follows the main outline of the course textbook very closely, skipping over various parts:\n\nTamhane, Ajit C., and Dorothy D. Dunlop.\nStatistics and Data Analysis: From Elementary to Intermediate\n. Prentice Hall, 1999. ISBN: 9780137444267.\n\nThis is an introductory statistics class, assuming probability as a prerequisite. We will review probability (Chapter 2), discuss sampling techniques (Chapter 3), data summarization (Chapter 4), common sampling distributions (Chapter 5), statistical inference and hypothesis testing (Chapters 6-9), regression (Chapters 10-11), and nonparametric inference (Chapter 14).\n\nComputing\n\nWe will use Excel, R, and MATLAB\n(r)\nin the class. It is not required that students learn MATLAB for the course, though we will use it in class. To obtain R, please visit the\nR Project website\n.\n\nHomework\n\nThe computer exercises are the only graded homework in this class. There is other homework assigned for this course, but it is optional, meaning that it will not count towards your final grade. The teaching assistant will answer questions about the homework during recitation. You are not permitted to work with others on the computer exercises; they are a type of take-home exam.\n\nExaminations\n\nThere will be four exams over the course of the semester. Exams will be based mostly on recent material but can require knowledge of previously covered material.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nExams (4 exams, 20% each)\n\n80%\n\nComputer exercises\n\n20%",
  "files": [
    {
      "category": "Resource",
      "title": "MIT15_075JF11_assn01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/fe3961d1b9b7c650f579c7aa408292b5_MIT15_075JF11_assn01.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 1\nDue October 13, 2011\n\nInstructions: Please solve the following exercises using MATLAB . On e simple wa y to presen t you r\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission.\n\nExercise 1\nWrite a MATLAB script\nthat\n\ncalculates\n\nth e mean\n\nand\n\nmedian\n\nof a samp le of 100\n\nuniform\n\nrandom\n\nnumbers between 0 and 2 and the percentage of points in the sample that are greater than 1.\n\nExercise 2\na. Generate a vector of 1000 normal random numbers with mean 8 and variance 25.\nb. Calculate how many elements in the vector are greater than or equal to 9.\nc. What is the sample mean and standard deviation for this sample of 1000 numbers?\nd. What are the 25th and 75th percentiles of the normal distribution with mean 8 and variance 25?\ne. What are the 25th and 75th percentiles of the sample of the 1000 normal random numbers\ngenerated in part (a)?\nf. Find Φ(0.789) and Φ(‐0.543). (Remember Φ is the cumulative density function for the standard\nnormal distribution.)\n\nExercise 3\na. Generate a vector of 1000 Poisson random numbers with λ = 2.\nb. Make a histogram and a boxplot of the 1000 numbers from part (a).\n\nExercise 4\nAnswer questions (a) - (c) from 4.36 in your textbook.\n\nExercise 5\nAnswer questions (a) - (d) from 4.44 in your textbook.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_assn02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/309c35af65d8651be221e8751c4123a2_MIT15_075JF11_assn02.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 2\nDue November 10, 2011\n\nInstructions: Please solve the following exercises using MATLAB . On e simple wa y to presen t you r\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission. Do NOT share code!\n\nExercise 1\nIn this exercise, we demonstrate the Central Limit Theorem, where each Xi is exponential.\n\na. Generate 1000 random numbers from the exponential distribution with λ = 6, and plot them in a\nhistogram. This should give you an idea of what the exponential distribution looks like.\nb. For n = 2, repeat the following 1000 times: Generate a random sample of n numbers from the\nexponential distribution with λ = 6.\nc. Compute the sample mean of the n numbers and standardize it using the true mean and\nstandard deviation of the distribution.\nd. Make a histogram and normal plot of the 1000 sample means.\ne. Repeat (b)‐(d) for n = 10, 20, and 100. Put all four histograms and all four normal plots in the\nsame window. Comment on their shapes.\n\nExercise 2\nSolve Problem 6.23 from the book.\n\nExercise 3\nSolve Problem 6.25 from the book.\n\nExercise 4\nA thermostat used in an electrical device is to be checked for the accuracy of its design setting of 200°F.\nTen thermostats were tested to determine their actual settings, resulting in the following data:\n\n202.2\n203.4\n200.5\n202.5\n206.3\n198.0\n203.7\n200.8\n201.3\n199.0\n\nAssume the settings come from a normal distribution. Using α = 0.05, perform a hypothesis test to\ndetermine if the mean setting is greater than 200°F. What are the null and alternative hypotheses?\nWhich test do you use and why? Explain your conclusion using\na. An appropriate confidence interval.\nb. A critical value from the distribution of the test statistic.\nc. A p‐value.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_assn03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/daa357597fc1a5cd2bfeff302ec145bf_MIT15_075JF11_assn03.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 3\nDue December 8, 2011\nInstructions: Please solve the following exercises using MATLAB. One simple way to present your\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission. Do NOT share code!\nExercise 1 (30 pts)\nTo determine whether glaucoma affects the corneal thickness, measurements were made in 8 people\naffected by glaucoma in one eye but not in the other. The corneal thicknesses (in microns) were as\nfollows:\nPerson\nEye affected by glaucoma\n488 478 480 426 440 410 458 460\nEye not affected by glaucoma 484 478 492 444 436 398 464 476\nAssume the corneal thicknesses are normally distributed with mean μ1 and variance σ2\n1 for eyes affected\nby glaucoma, and mean μ2 and variance σ 2\n2 for eyes not affected by glaucoma. Test H0: μ1 = μ2 against\nH1: μ1 < μ2 using α = 0.1. What kind of test will you perform? Base your conclusion on a 90% confidence\ninterval.\nExercise 2 (70 pts)\nThe following data give the barometric pressure x (in inches of mercury) and the boiling point y (in °F) of\nwater in the Alps.\nx\ny\nx\ny\nx\ny\n20.79 194.5\n23.89 200.9\n28.49 209.5\n20.79 194.3\n23.99 201.1\n27.76 208.6\n22.40 197.9\n24.02 201.4\n29.04 210.7\n22.67 198.4\n24.01 201.3\n29.88 211.9\n23.15 199.4\n25.14 203.6\n30.06 212.2\n23.35 199.9\n26.57 204.6\na. Make a scatterplot with pressure x on the x‐axis and boiling point y on the y‐axis. Does the\nrelationship appear to be approximately linear?\nb. Fit a least‐squares line. What are the coefficients of the line? What are their t‐statistics?\nc. Plot the least‐squares line in the same figure with the scatterplot.\nd. What proportion of variation in the boiling point is explained by the linear regression model on\nthe barometric pressure?\ne. Is the slope coefficient significantly different from zero? How do you know? If yes, at what\nsignificance level?\nf. What are SSR, SSE, SST, MSR, MSE and F?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_soln01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/4fc9e812ff38b56db95647110c6c9a99_MIT15_075JF11_soln01.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 1\nDue October 13, 2011\nInstructions: Please solve the following exercises using Matlab. One simple way to present your\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission.\nExercise 1\nWrite a Matlab script that calculates the mean and median of a sample of 100 uniform random\nnumbers between 0 and 2 and the percentage of points in the sample that are greater than 1.\nSolution\nHere is the script and the results for my iterations:\nclear all;\nclc;\nn = 100;\n% Generate 100 uniform random numbers between 0 and 2\nx = 2*rand(n,1);\n% Calculate the mean\nmu = mean(x);\n% Calculate the median\nmed = median(x);\n% Find the percentage of points greater than 1\nper = sum(x>1)/n;\nmu =\n1.0089\nmed =\n0.9841\nper =\n0.4900\n\nExercise 2\na. Generate a vector of 1000 normal random numbers with mean 8 and variance 25.\nb. Calculate how many elements in the vector are greater than or equal to 9.\nc. What is the sample mean and standard deviation for this sample of 1000 numbers?\nd. What are the 25th and 75th percentiles of the normal distribution with mean 8 and variance 25?\ne. What are the 25th and 75th percentiles of the sample of the 1000 normal random numbers\ngenerated in part (a)?\nf. Find Φ(0.789) and Φ(‐0.543). (Remember Φ is the cumulative density function for the standard\nnormal distribution.)\nSolution\nHere is the script and the results for my iterations:\nclear all;\nclc;\nn = 1000;\nmu = 8;\nsigma = 5;\n% a.\nGenerate a vector of 1000 normal random numbers with mean 8 and\nvariance 25.\nx = sigma*randn(n,1)+mu;\n% b.\nCalculate how many elements in the vector are greater than or equal\nto 9.\nnum = sum(x>=9)\n% c.\nWhat is the sample mean and standard deviation for this sample of\n1000 numbers?\nsample_mean = mean(x)\nsample_std = std(x)\n% d.\nWhat are the 25th and 75th percentiles of the normal distribution\nwith mean 8 and variance 25?\nq1 = norminv(.25,8,5) % or q1 = 5*norminv(0.25)+8\nq3 = norminv(.75,8,5) % or q1 = 5*norminv(0.75)+8\n% e.\nWhat are the 25th and 75th percentiles of the sample of the 1000\nnormal random numbers generated in part (a)?\ns_q1 = quantile(x,0.25)\ns_q3 = quantile(x,0.75)\n% f.\nFind ?(0.789) and ?(-0.543).\nnormcdf(0.789)\nnormcdf(-0.543)\nnum =\nsample_mean =\n\n8.0511\nsample_std =\n4.9974\nq1 =\n4.6276\nq3 =\n11.3724\ns_q1 =\n4.7150\ns_q3 =\n11.2380\nans =\n0.7849\nans =\n0.2936\n\nExercise 3\na. Generate a vector of 1000 Poisson random numbers with λ = 2.\nb. Make a histogram and a boxplot of the 1000 numbers from part (a).\nSolution\nHere is the script and the plots for my iterations:\nclear all;\nclc;\nclose all;\nn = 1000;\nlambda = 2;\n% a.\nGenerate a vector of 1000 Poisson random numbers with ? = 2.\nx = poissrnd(lambda,n,1);\n% b.\nMake a histogram and a boxplot of the 1000 numbers from part (a).\nfigure;\nhist(x);\nfigure;\nboxplot(x);\n\nExercise 4\nAnswer questions (a) - (c) from 4.36 in your textbook.\nSolution\nHere is the script and a scatterplot:\nclear all;\nclose all;\nclc;\nsnowfall = [45 59 82 80 71 60 55 69 79 95];\nunemployment = [4.9 5.6 8.5 7.7 7.1 6.1 5.8 7.1 7.6 9.7];\n% Scatterplot\nplot(snowfall, unemployment, 'x');\ngrid;\nxlabel('Snowfall (inches)');\nylabel('Unemployment (%)');\n% Calculates the correlation coefficient\nrho = corr(snowfall',unemployment');\nrho =\n\n0.9835\nA strong linear positive relationship is indicated by both the scatter plot and the correlation\ncoefficient. This relationship does not mean that the snowfall influences the national unemployment\nrates or vice versa. It means that there is an association between them.\n\nBelow is a time-series plot of the data. There is an increasing trend, so no, the time-series does\nnot appear to be stationary.\nCPI for 1997-1998\nMonth/Year\nCPI\n4.44\na.\nb,c. The moving averages and exponentially weighted moving averages are listed below. The MAPE\nis 0.2655% and 0.5679% for MA and EWMA respectively.\nMA\nEWMA\n159.1000 159.1000\n159.3500 159.2000\n159.5667 159.3600\n159.9333 159.5280\n160.1000 159.6424\n160.2000 159.7739\n160.3000 159.9191\n160.5333 160.0953\n160.8333 160.3162\n\n161.2000 160.5730\n161.4333 160.7584\n161.4667 160.8667\n161.4667 161.0134\n161.6000 161.1907\n161.9000 161.3926\n162.2000 161.6140\n162.5000 161.8512\n162.7667 162.0810\n163.0000 162.3048\n163.2000 162.5238\n163.4000 162.7391\n163.6667 162.9913\n163.8667 163.1930\n163.9667 163.3344\nd. We have r1 = 0.8694, r2 = 0.7380, and r3 = 0.6149. So CPI in successive months have a rather\nstrong positive correlation, but the correlation grows weaker with CPI's removed farther apart.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_soln02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/b23d47d7db96c445adf0a275546ba4f1_MIT15_075JF11_soln02.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 2\nDue November 10, 2011\nInstructions: Please solve the following exercises using MATLAB. One simple way to present your\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission. Do NOT share code!\nExercise 1\nIn this exercise, we demonstrate the Central Limit Theorem, where each Xi is exponential.\na. Generate 1000 random numbers from the exponential distribution with λ = 6, and plot them in a\nhistogram. This should give you an idea of what the exponential distribution looks like.\nb. For n = 2, repeat the following 1000 times: Generate a random sample of n numbers from the\nexponential distribution with λ = 6.\nc. Compute the sample mean of the n numbers and standardize it using the true mean and\nstandard deviation of the distribution.\nd. Make a histogram and normal plot of the 1000 sample means.\ne. Repeat (b)‐(d) for n = 10, 20, and 100. Put all four histograms and all four normal plots in the\nsame window. Comment on their shapes.\nSolution\nHere is the script and the results for my iterations:\nclear all;\nclc;\nclose all;\nnum_samples = 1000;\nlambda = 6;\nmu = 1/lambda;\nsigma = mu;\n% Part a\n% Generate 1000 random numbers from the exponential distribution with\n% lambda=6\nx = exprnd(mu,num_samples,1);\n% Plot a histogram\nhist(x,20);\ntitle('Histogram of 1000 exponential random variables with lambda = 6');\n% Part b,c,d,e\nsample_sizes = [2 10 20 100];\nfigure;\nfor i = 1:length(sample_sizes)\nsample_size = sample_sizes(i);\nx = exprnd(mu,num_samples,sample_size);\nsample_mean = mean(x');\nz_score = (sample_mean - mu)/(sigma/sqrt(num_samples));\n\n% Plot a histogram z-scores\nsubplot(2,2,i);\nhist(z_score);\ntitle(['Sample size n = ', num2str(sample_size)])\n% Plot a normal plot of z-scores\nsubplot(2,2,i);\nnormplot(z_score);\ntitle(['Sample size n = ', num2str(sample_size)])\nend\n\nThe normal plot for n=2 shows that the distribution of sample means is still skewed right since the\nexponential distribution has a very long positive tail. As n increases, the distribution of sample means\nbecomes increasingly symmetric. With n=100, the curve in the normal plot is fairly straight, implying the\ndistribution of sample means is approximately normal.\n\nExercise 2\nSolve Problem 6.23 from the book.\nSolution\na) Obviously the α‐risk is 0.05 from definition. Let's calculate the β‐risk of the rule if μ=1.\nThe β‐risk is just β = 1‐π(1) = 1 ‐ߔሾെݖα\nሺఓିఓ\nఙ\n0ሻ√ሿ = 1‐Φ(‐1.645+3) = 1‐Φ(1.355) = 0.0877.\nb) Here is the script and the results for my iterations:\nclear all;\nclc;\nclose all;\n% Parameters\nlevel = 0.05;\nmu_0 = 0;\nmu = 1;\nsigma = 1;\nn = 9;\nnum_samples = 100;\n% Reject if sample mean is greater than critical_value\ncritical_value = mu_0+norminv(1-level)*sigma/sqrt(n);\n% Generate 100 samples of size n with mean mu_0\nx = sigma*randn(num_samples,n)+mu_0;\nx_bar = mean(x');\na_risk = sum(x_bar>critical_value)/num_samples;\n% Generate 100 samples of size n with mean mu\nx = sigma*randn(num_samples,n)+mu;\nx_bar = mean(x');\nb_risk = sum(x_bar<critical_value)/num_samples;\n>> a_risk\na_risk =\n0.0500\n>> b_risk\nb_risk =\n0.0700\nThe results are close to their theoretical values.\n\nExercise 3\nSolve Problem 6.25 from the book.\nSolution\nHere is the script and the results for my iterations:\nclear all;\nclc;\nclose all;\nsigma = 1000;\nn = 10;\n% We reject when x_bar is greater than the critical value\ncritical_value = 10500;\nmu = 9600:200:11600;\npower = 1-normcdf((critical_value-mu)*sqrt(n)/sigma);\noc = 1-power;\nplot(mu,power);\ngrid;\ntitle('Power');\nfigure;\nplot(mu,oc);\ngrid;\ntitle('OC');\n\nExercise 4\nA thermostat used in an electrical device is to be checked for the accuracy of its design setting of 200°F.\nTen thermostats were tested to determine their actual settings, resulting in the following data:\n202.2\n203.4\n200.5\n202.5\n206.3\n198.0\n203.7\n200.8\n201.3\n199.0\nAssume the settings come from a normal distribution. Using α = 0.05, perform a hypothesis test to\ndetermine if the mean setting is greater than 200°F. What are the null and alternative hypotheses?\nWhich test do you use and why? Explain your conclusion using\na. An appropriate confidence interval.\nb. A critical value from the distribution of the test statistic.\nc. A p‐value.\nSolution\nThe hypotheses are\nH0: μ = 200.\nH1: μ > 200.\nWe use a t‐test since we do not know the true variance, and the sample size is small.\nclear all;\nclc;\nclose all;\nx = [202.2,203.4,200.5,202.5,206.3,198.0,203.7,200.8,201.3,199.0];\nmu = 200;\nalpha = 0.05;\n[h p ci stats] = ttest(x,mu,alpha, 'right')\nh =\np =\nci =\n200.3729\nInf\n0.0227\n\nstats =\ntstat: 2.3223\ndf: 9\nsd: 2.4102\nFor all the following reasons, we reject H0.\na. The lower 95% CI is [200.3729, inf), which does not contain 200.\nb. The observed value of the t‐test statistic is 2.3223, which is larger than t9,0.05 = 1.833.\nc. The p‐value is 0.0227, which is smaller than the significance level α = 0.05.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_soln03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/0eef05e12db0982addaa6a141598d497_MIT15_075JF11_soln03.pdf",
      "content": "15.075 Statistical Thinking and Data Analysis\nComputer Exercises 3\nDue December 8, 2011\nInstructions: Please solve the following exercises using MATLAB. One simple way to present your\n\nsolutions is to copy all of your code and results (plots, numerical answers, etc.) into a Word document,\nwhich you should submit in class. There is no online submission. Do NOT share code!\nExercise 1 (30 pts)\nTo determine whether glaucoma affects the corneal thickness, measurements were made in 8 people\naffected by glaucoma in one eye but not in the other. The corneal thicknesses (in microns) were as\nfollows:\nPerson\nEye affected by glaucoma\n488 478 480 426 440 410 458 460\nEye not affected by glaucoma 484 478 492 444 436 398 464 476\nAssume the corneal thicknesses are normally distributed with mean μ1 and variance σ2\n1 for eyes affected\nby glaucoma, and mean μ2 and variance σ 2\n2 for eyes not affected by glaucoma. Test H0: μ1 = μ2 against\nH1: μ1 < μ2 using α = 0.1. What kind of test will you perform? Base your conclusion on a 90% confidence\ninterval.\nSolution\nHere is the script and the results:\nclear all;\nclc;\nalpha = 0.1;\nx = [488 478 480 426 440 410 458 460];\ny = [484 478 492 444 436 398 464 476];\n[h,p,ci,stats] = ttest(x,y,alpha,'left')\nh =\np =\n0.1637\nci =\n-Inf\n1.3746\n\nstats =\ntstat: -1.0530\ndf: 7\nsd: 10.7438\nThe upper 90% CI is (‐inf, 1.3746], which includes 0, so we do not reject H0.\nExercise 2 (70 pts)\nThe following data give the barometric pressure x (in inches of mercury) and the boiling point y (in °F) of\nwater in the Alps.\nx\ny\nx\ny\nx\ny\n20.79 194.5\n23.89 200.9\n28.49 209.5\n20.79 194.3\n23.99 201.1\n27.76 208.6\n22.40 197.9\n24.02 201.4\n29.04 210.7\n22.67 198.4\n24.01 201.3\n29.88 211.9\n23.15 199.4\n25.14 203.6\n30.06 212.2\n23.35 199.9\n26.57 204.6\na. Make a scatterplot with pressure x on the x‐axis and boiling point y on the y‐axis. Does the\nrelationship appear to be approximately linear?\nb. Fit a least‐squares line. What are the coefficients of the line? What are their t‐statistics?\nc. Plot the least‐squares line in the same figure with the scatterplot.\nd. What proportion of variation in the boiling point is explained by the linear regression model on\nthe barometric pressure?\ne. Is the slope coefficient significantly different from zero? How do you know? If yes, at what\nsignificance level?\nf. What are SSR, SSE, SST, MSR, MSE and F?\nSolution\nHere is the script and the results:\na,c.\nclear all;\nclc;\nclose all;\npressure = [20.79, 20.79, 22.40, 22.67, 23.15, 23.35, 23.89, 23.99, 24.02,\n24.01, 25.14, 26.57, 28.49, 27.76, 29.04, 29.88, 30.06];\nboiling_point = [194.5, 194.3, 197.9, 198.4, 199.4, 199.9, 200.9, 201.1,\n201.4, 201.3, 203.6, 204.6, 209.5, 208.6, 210.7, 211.9, 212.2];\n% Scatterplot\nplot(pressure, boiling_point,'x');\ngrid;\n\nxlabel('Pressure');\nylabel('Boiling point');\nhold on;\nYes, the relationship appears approximately linear.\nb.\n% Run the regression\nwhichstats = {'beta','r', 'mse', 'rsquare', 'tstat', 'fstat'};\nstats = regstats(boiling_point, pressure,'linear',whichstats);\nbetas = stats.beta\ntstats = stats.tstat.t\npvals = stats.tstat.pval\nrsquare = stats.rsquare\nbetas =\n155.2965\n1.9018\ntstats =\n167.4650\n51.7408\npvals =\n1.0e-017 *\n\n0.0000\n0.2528\nrsquare =\n0.9944\nThe coefficients of the line are ߚመ ൌ 155.2965 and ߚመଵ ൌ 1.9018.\nd. The proportion of variation in boiling point explained by the model is r2 = 0.9944.\ne. Yes, ߚመଵ is significantly different from zero. The p‐value is nearly zero (<2e‐16), so the difference is\nsignificant for any reasonable α.\nf.\nRegression ANOVA\nSource\ndf\nSS\nMS\nF\nP\nRegr\nResid\n1.0000\n15.0000\n527.8249\n2.9574\n527.8249 2677.1053\n0.1972\n0.0000\nTotal\n16.0000 530.7824\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_tutorial.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/fdebbc0cf10308a97fd114090c0f3d12_MIT15_075JF11_tutorial.pdf",
      "content": "MATLAB Tutorial 1\nSeptember 16, 2010\nDimitrios Bisias\nMATLAB Desktop\nThe Command Window is where you type the commands or instructions into MATLAB.\nThe Command History contains a record of all the commands entered in the Command Window.\nThe Workspace is where your data is stored in MATLAB during the current session.\nMATLAB Fundamentals\nMATLAB can be used as a calculator performing simple and complex calculations.\n>> t = 2;\n>> x = sin(3*t+pi/2)^2\nx =\n0.9219\nMATLAB saves data in variables. You can think of MATLAB variables as data containers.\nAll variables are arrays. Even scalar variables are treated as 1x1 arrays.\nThere are two ways to create a sequence of equally spaces values\n>> x = 1:2:11\nx =\n>> x = linspace(1,11,6)\nx =\nTo transpose it\n>> x =x'\nx =\n\nTo check the size of an array:\n>> size(x)\nans =\nThe semicolon can be used to suppress output in the command window\n>> t = 0:0.01:100;\nTo create matrices\n>> A = [1 2 3;4 5 6;7 8 9]\nA =\nRow separator: ; or enter\n>> ones(2)\nans =\n>> zeros(3,2)\nans =\n\n>> eye(3)\nans =\nTo access the (2,3) element of an array use A(2,3)\nTo access the second row, use A(2,: )\nWhat do we access by A(2,[1 2]) ?\nTo concatenate matrices\n>> A =\n>> B = [A A]\nB =\n>> B = A + 2\nB =\n9 10 11\n>> B = 3*A\nB =\n12 15 18\n21 24 27\nWhat's the difference between A*C and A.*C?\n\nx\nScript Files\nA collection of commands that you would execute in the Command Window.\nAutomate repetitive tasks instead of typing the same commands over and over again.\nWriting Functions\nfunction output = function_name(input)\nThe name of the file has to be the same as the name of the function.\nMake repetitive tasks more modular and the code more reusable.\nThey have a separate workspace which is automatically cleared on exit of the function.\nFlow and Loop Control\nIf-Elseif-Else\nWhile. Run multiple iterations of an operation until a condition is no valid\nFor. Repeat an operation for a known number of iterations.\nx = 0;\nfor i=1:10\nx = x+i;\nend\nz = 1:10;\nx = sum(z)\nProbability/Statistics Built-in functions\nrand\nrandn\nmean\nmedian\nstd\nvar\nquantile\nprctile\nnormpdf\nnormcdf\nnorminv\n\nv= 1:20;\nSuppose we wanted to randomly choose ten elements of v.\n>> v = 1:20;\n>> vsample10 = randsample(v,10)\nvsample10 =\nCompute the mean and standard deviation of this sample.\n>> mean(vsample10)\nans =\n12.2000\n>> std(vsample10)\nans =\n5.4528\nWe can also compute quantiles\n>> quantile(vsample10,0.5)\nans =\n>> median(vsample10)\nans =\nLet's generate ten random numbers from the normal distribution, say with mean 5 and variance 4.\n>>r =5 + 2.*randn(10,1)\nWe can compute the cdf with normcdf\n>> normcdf(5,5,2)\n\nans =\n0.5000\nThere are several related functions for other probability distributions, for example:\nBinomial - binornd, binopdf\nPoisson - poissrnd, poisspdf\nUniform - unifrnd, unifpdf\nGamma - exprnd, exppdf\nHelp\nThere may be times when you forget how to use a certain function, even though you remember\nthe function's name. In this case, simply type help function_name\n>> help norminv\nFor documentation you can type doc\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam01_soln.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/1b4389fbb9a61f8cacc0409c7041d3ff_MIT15_075JF11_exam01_soln.pdf",
      "content": "15.075 Exam 1\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nSeptember 29, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nA very large bin contains 3 different types of disposable flashlights. The probability that a type 1\nflashlight will give over 100 hours of use is 0.7, with the corresponding probabilities for type 2 and 3\nflashlights being 0.4 and 0.3 respectively. Suppose that 20% of the flashlights in the bin are type 1, 30%\nare type 2 and 50% are type 3.\n1. What is the probability that a randomly chosen flashlight will give more than 100 hours of use?\n2. Given the flashlight lasted over 100 hours, what is the conditional probability that it was a type\n1 flashlight?\nSolution\na. Let A be the event {the flashlight will give more than 100 hours of use } and Bi be the event {we\nchoose flashlight of type i}. Then we have:\nP (A) =\nP (A/Bi)P (Bi) = 0.7 · 0.2 + 0.4 · 0.3 + 0.3 · 0.5 = 0.41\ni=1\nb. We have:\nP (B1/A · P (A)\n0.7 · 0.2\nP (B1) =\n=\n= 0.34\nP (A)\n0.41\n\nProblem 2\nIn HighHatVille (population 1200), a third of the townsfolk have very high hats, more than three feet\ntall for the hat (not including the person underneath)! But they keep their hats hidden most of the\ntime and bring them out only on HighHatHoliday. They lend the hats out when they aren't using them\nthough. The MIT students need some high hats for a party, so we decided to visit HighHatVille for a\nday. Unfortunately we can't spend too long there, and we need 50 hats. How many people do we need\nto visit in order to ensure that we get at least 50 hats with more than 70% probability? Hint: We do\nnot expect a numerical answer just a procedure which would give the exact answer.\nSolution\nLet n be the number of people we visit. Then the number of hats we get follows a hypergeometric\ndistribution with N = 1200 and M = 400. We want the probability of getting at least 50 hats to be\ngreater than 0.7, in other words:\n\nn\nn\nn\n·\ni\nn-i\n1200n\n≥ 0.7\ni=50\nn\nWe can solve it with\nand find n=158 (we expected a value close but greater to 150).\nMATLAB\n\nProblem 3\nThe annual snowfall at a particular city is modeled as a normal random variable with a mean of μ = 60\ninches and a standard deviation of σ = 20. What is the probability that this year's snowfall will be\nbetween 40 and 100 inches?\nSolution\nLet X the annual snowfall. It is:\n40 - 60\nX - 60\n100 - 60\nP (40 ≤ X ≤ 100) = P (\n≤\n≤\n) = P (-1 ≤ Z ≤ 2) = Φ(2) - Φ(-1) = 0.8186.\n\nProblem 4\nAn airline company accepts 100 reservations for a flight that has 72 available seats. A person booking\na reservation does not show up in the flight with probability q = 0.1. Find an upper bound in the\nprobability that no traveler showing up is bumped (excluded) from the flight.\nSolution\nLet X be the number of people showing up in the flight. Then X is a Bin(n, p) with n = 100 and\np = 0.9. It is E[X] = n·p = 100·0.9 = 90 and var(X) = np(1-p) = 9. We are interested in P (X ≤ 72).\nWe have:\nP (|X - 90| ≥ 18) = P (X ≥ 108) + P (X ≤ 72) = P (X ≤ 72)\nBut from Chebyshev's inequality we know:\nvar(X)\nP (|X - E[X]| ≥ 18) ≤\n=\n.\nTherefore,\nP (X ≤ 72) ≤\n.\n\nProblem 5\nProvide a sample that could correspond to this box plot:\nSolution\nObviously, there are uncountably infinite solutions to this problem. Two representative samples are:\n{25, 40, 50, 52, 53, 55, 56, 58, 60, 68, 80} and {25, 40, 48, 50, 51, 52, 53, 55, 56, 57, 58, 60, 65, 68, 80}\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/d90c0b74a4a2624468d03fa293bef013_MIT15_075JF11_exam01.pdf",
      "content": "15.075 Exam 1\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nSeptember 29, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nA very large bin contains 3 different types of disposable flashlights. The probability that a type 1\nflashlight will give over 100 hours of use is 0.7, with the corresponding probabilities for type 2 and 3\nflashlights being 0.4 and 0.3 respectively. Suppose that 20% of the flashlights in the bin are type 1, 30%\nare type 2 and 50% are type 3.\n1. What is the probability that a randomly chosen flashlight will give more than 100 hours of use?\n2. Given the flashlight lasted over 100 hours, what is the conditional probability that it was a type\n1 flashlight?\nProblem 2\nIn HighHatVille (population 1200), a third of the townsfolk have very high hats, more than three feet\ntall for the hat (not including the person underneath)! But they keep their hats hidden most of the\ntime and bring them out only on HighHatHoliday. They lend the hats out when they aren't using them\nthough. The MIT students need some high hats for a party, so we decided to visit HighHatVille for a\nday. Unfortunately we can't spend too long there, and we need 50 hats. How many people do we need\nto visit in order to ensure that we get at least 50 hats with more than 70% probability? Hint: We do\nnot expect a numerical answer just a procedure which would give the exact answer.\nProblem 3\nThe annual snowfall at a particular city is modeled as a normal random variable with a mean of μ = 60\ninches and a standard deviation of σ = 20. What is the probability that this year's snowfall will be\nbetween 40 and 100 inches?\n\nProblem 4\nAn airline company accepts 100 reservations for a flight that has 72 available seats. A person booking\na reservation does not show up in the flight with probability q = 0.1. Find an upper bound in the\nprobability that no traveler showing up is bumped (excluded) from the flight.\nProblem 5\nProvide a sample that could correspond to this box plot:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam02_soln.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/8a824c52902ac3819a1310e097a98ff9_MIT15_075JF11_exam02_soln.pdf",
      "content": "15.075 Exam 2\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nOctober 25, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nYou are in charge of a study that compares how two weight-loss techniques (Diet and Exercise) affect\nthe weight loss of overweight patients. The table below shows the number of obese and severely obese\npeople who lost significant weight (successes) and the number of people who didn't lose any weight\n(failures) for both of the weight-loss techniques.\nDiet successes\nDiet failures\nExercise successes\nExercise failures\nObese\nSeverely Obese\n1. Within each category of obesity, compare success rates for the weight-loss techniques. What do\nthese rates indicate?\n2. Compare the total success rates for the two techniques. Explain any differences from (1).\nSolution\n1. For the obese category the success rate for diet is 10/40 = 25% while the success rate for exercise\nis 22/80 = 27.5%. For the severely obese category, the success rate for diet is 60/80 = 75% while\nthe success rate for exercise is 35/40 = 87.5%.These figures show that exercise has higher success\nrates for both of the categories.\n2. The total success rate for diet is 70/120 = 58.3% while the total success rate for exercise is\n57/120 = 47.5%. This contrasts with the result of part (1). This is an example of Simpson's\nparadox. The lurking variable is type of obesity. There are far less obese people in case of diet\nthan exercise, who have the lower success rates.\n\nProblem 2\nYou are the owner of a bakery selling cheesecakes. Some days you sell a lot of cheesecakes and some\nother days you don't, independently of other days. In fact, the distribution of the number of cheesecakes\nyou sell each day is shown below.\nIf we measure cheesecakes sales for 100 days, what is the approximate probability that the average\ncheesecakes sales (over the 100 days) will be more than 102 cheesecakes?\nSolution\nLet X be the number of cheesecakes sold in one day. Then due to symmetry E[X] = 100 and\nvar(X) = 2 · 0.2 · 202 + 2 · 0.3 · 52 = 175\n\nWe are asked to find P (\nX is approximately normally distributed\nX > 102). From CLT we know that\nwith mean 100 and variance 175 = 1.75, therefore we have:\nn\n102 - 100\nP ( X > 102) ≈ P (Z > 1\n) = 1 - Φ(1.51) = 0.065\n175/100\n\nProblem 3\nSuppose that the observations X1, X2, · · · , Xn are iid with common mean θ and known variance σ2 .\nConsider the following estimator of the mean θ:\nX1+···+Xn\nΘˆ n =\nn+1\n.\n1. What is the bias of this estimator? What happens to the bias as we increase the size of the\nsample?\n2. What is the MSE of the estimator?\nHint: Remember to use either what you know about V ar(X ), or what you know about the variance of\nthe sum of independent random variables.\nSolution\nˆ\nn\n1. It is: Θn = n+1 Xn. Therefore,\nn\nn\nE[Θˆ n] =\nE[X n] =\nθ\nn + 1\nn + 1\nand\nn\nn\nvar[Θˆ n] =\nvar[X n] =\nσ2\n(n + 1)2\n(n + 1)2\nThus, we have:\nBias = E[Θˆ n] - θ =\nn\nθ - θ = -1 θ\nn + 1\nn + 1\nAs n →inf the bias goes to 0.\n2. It is:\nθ2\nn\nMSE(Θˆ n) = Bias2 + var(Θˆ n) =\n+\nσ2\n(n + 1)2\n(n + 1)2\n\nProblem 4\nThe weight of an object is measured eight times using an electronic scale that reports the true weight\nplus a random error that is normally distributed with zero mean and variance σ2 = 4. Assume that the\nerrors in the observations are independent. The following results are obtained:\n{4.45, 4.02, 5.51, 1.10, 2.62, 2.38, 5.94, 7.64}\nCompute a 95% confidence interval for the true weight.\nSolution\nA 95% confidence interval for the true weight is\nσ\nσ\n\n[X - 1.96√ , X + 1.96√ ] = [4.21 - 1.96√ , 4.21 + 1.96√ ] = [2.82, 5.59]\nn\nn\n\nProblem 5\nThe returns of an asset management firm in different years are independent and normally distributed\nwith unknown mean and variance. The asset management firm claims that the standard deviation of\nthe returns is as low as σ = 2% and the mean of the returns is μ = 22%. You believe that this is too\ngood to be true. To verify your suspicion, you take the returns from the last 10 years. These are:\nr = {20.6, 19.2, 17, 19.1, 18.7, 22.5, 27.2, 17.9, 22.5, 21.3}\n\nn\nn\n(To help you with the calculations, we give you that:\ni=1 r(i) = 206 and\n(r(i) - r )2 = 79.34)\ni=1\n1. Calculate the sample mean and standard deviation of the above random sample.\n2. What is the probability that when we draw a new random sample of size 10, its sample mean will\nbe below the one you calculated in part 1? Assume that the company's claims are correct.\n3. What is the probability that when we draw a new random sample of size 10, its sample standard\ndeviation will be larger than the one you calculated in part 1, assuming that the company's claims\nare correct?\nSolution\n\nn\ni\n(r(i)-r )2\n=1 r(i)\n=1\n79.34 = 2.97\nn\ni\n2. Under the company's claims the sample mean is normally distributed with mean 22% and standard\n√\ndeviation 2/ 10%. It is:\n20.6 - 22\nP ( X < 20.6) = P (Z <\n√\n) = P (Z < -2.21) = 0.013\n2/ 10\n3. Under the company's claims the statistic:\n(n - 1)S2\nσ2\nis distributed as a chi-square with n-1=9 degrees of freedom. Therefore:\n(n - 1)S2\n9 · 2.972\nP (S > 2.97) = P (\n>\n) = P (χ9\n2 > 19.85) = 0.0188\nσ2\n1. The sample mean is: r =\n\n= 20.6 and the sample standard deviation is s =\n=\nn\nn-1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/8b9c7aec8915dd68a866710e8d3e5029_MIT15_075JF11_exam02.pdf",
      "content": "15.075 Exam 2\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nOctober 25, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nYou are in charge of a study that compares how two weight-loss techniques (Diet and Exercise) affect\nthe weight loss of overweight patients. The table below shows the number of obese and severely obese\npeople who lost significant weight (successes) and the number of people who didn't lose any weight\n(failures) for both of the weight-loss techniques.\nDiet successes\nDiet failures\nExercise successes\nExercise failures\nObese\nSeverely Obese\n1. Within each category of obesity, compare success rates for the weight-loss techniques. What do\nthese rates indicate?\n2. Compare the total success rates for the two techniques. Explain any differences from (1).\n\nProblem 2\nYou are the owner of a bakery selling cheesecakes. Some days you sell a lot of cheesecakes and some\nother days you don't, independently of other days. In fact, the distribution of the number of cheesecakes\nyou sell each day is shown below.\nIf we measure cheesecakes sales for 100 days, what is the approximate probability that the average\ncheesecakes sales (over the 100 days) will be more than 102 cheesecakes?\n\nProblem 3\nSuppose that the observations X1, X2, · · · , Xn are iid with common mean θ and known variance σ2 .\nConsider the following estimator of the mean θ:\nX1+···+Xn\nΘˆ n =\nn+1\n.\n1. What is the bias of this estimator? What happens to the bias as we increase the size of the\nsample?\n2. What is the MSE of the estimator?\nHint: Remember to use either what you know about V ar(X ), or what you know about the variance of\nthe sum of independent random variables.\n\nProblem 4\nThe weight of an object is measured eight times using an electronic scale that reports the true weight\nplus a random error that is normally distributed with zero mean and variance σ2 = 4. Assume that the\nerrors in the observations are independent. The following results are obtained:\n{4.45, 4.02, 5.51, 1.10, 2.62, 2.38, 5.94, 7.64}\nCompute a 95% confidence interval for the true weight.\n\nProblem 5\nThe returns of an asset management firm in different years are independent and normally distributed\nwith unknown mean and variance. The asset management firm claims that the standard deviation of\nthe returns is as low as σ = 2% and the mean of the returns is μ = 22%. You believe that this is too\ngood to be true. To verify your suspicion, you take the returns from the last 10 years. These are:\nr = {20.6, 19.2, 17, 19.1, 18.7, 22.5, 27.2, 17.9, 22.5, 21.3}\n\n(To help you with the calculations, we give you that:\nn\ni\n=1 r(i) = 206 and\n=1\nn\ni\n(r(i) - r )2 = 79.34)\n1. Calculate the sample mean and standard deviation of the above random sample.\n2. What is the probability that when we draw a new random sample of size 10, its sample mean will\nbe below the one you calculated in part 1? Assume that the company's claims are correct.\n3. What is the probability that when we draw a new random sample of size 10, its sample standard\ndeviation will be larger than the one you calculated in part 1, assuming that the company's claims\nare correct?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam03_soln.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/c835a860b75f54c7571912f85c1b74bc_MIT15_075JF11_exam03_soln.pdf",
      "content": "15.075 Exam 3\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nNovember 22, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nA company makes high-definition televisions and does not like to have defective pixels. Historically, the\nmean number of defective pixels in a TV is 20. An MIT engineer is hired to make better TV's that\nhave fewer defective pixels. After her first week of work she claims that she can significantly improve\nthe current method. To check her claim you try her new method on 100 new televisions. The average\nnumber of defective pixels in those 100 TV's is 19.1. Assume that the new method doesn't change the\nstandard deviation of defective pixels, which has always been 4.\n1. Test if the new method is significantly better than the old one at the α = 0.05 level.\n2. Using the new method, assume that the mean number of defective pixels is actually 19. What\nis the chance that your test from part 1 will conclude that the new method is statistically more\neffective?\n3. How many televisions will you have to check so that the test you did in part 1 will conclude that\nthe new method is effective, with 95% probability? Please assume again that the mean number\nof defective pixels is actually 19.\nSolution\n1. The hypotheses are:\nH0 : μ = 20\nH1 : μ < 20\nWe perform a z-test. Under the null hypothesis, the z-statistic is approximately standard normal.\nThe observed value of the z-statistic is:\nx - μ\n19.1 - 20\nz =\n=\n= -2.25\n√σ\n4/10\nn\n\n!\nSince this value is less than -zα = -z0.05 = -1.645, the null hypothesis is rejected at the α = 0.05\nlevel.\n2. The question asks for the power of the test at 19. The rejection region is z < -1.645, or x <\n√4\n20 - 1.645\n= 19.34. The power of the test is\nπ(19) = P (Test rejects H0|μ = 19) = P ( x < 19.34|μ = 19)\n\nx - μ\n19.34 - 19\n= P\n√ <\n√\n= 0.804.\nσ/ n\n4/ 100\nUsing Eq.(7.8), we get the same result:\n\n√\n√\n(μ0 - μ) n\n(20 - 19) 100\nπ(μ) = π(19) = Φ\n-zα +\n= Φ -1.645 +\n= 0.804.\nσ\n3. The rejection region for a sample of n tires is x < 20 - 1.645√ . We want n such that\nn\n\nP\n\n√ μ = 19\n≥ 0.95.\nx < 20 - 1.645 n\nSince P (Z < 1.645) = 0.95, we have\n√\n20 - 1.645√ - 19\nn\n1 n\n1.645 ≤\n√\n→\n1.645 ≤\n- 1.645.\n4/\nn\n\n(1.645+1.645)·4\nThen n ≥\n= 173.19, so 174 televisions should be tested for the power to be at\nleast 0.95.\nUsing Eq.(7.10), we get the same result:\n\n(zα + zβ )σ\n(z0.05 + z1-0.95)4\n(1.645 + 1.645)4\nn ≥\n=\n=\n= 173.19.\nμ0 - μ\n20 - 19\n20 - 19\nso again 174 televisions should be tested for the power to be at least 0.95.\n\nProblem 2\nTwo methods of memorizing words are to be compared. You choose two groups of 5 people, where the\nfirst person in the first group has the same characteristics as the first person in the second group (they\nhave the same educational level, age, etc.). Same thing about the second person in each group - they\nare also similar to each other in terms of education, age, etc. Same thing for the third, fourth and fifth\npeople from each group. The first group is assigned to the first method of memorization and the second\ngroup to the other method. The number of words recalled in a memory test after a week's training with\nthese two methods is shown below.\nPair 1\nPair 2\nPair 3\nPair 4\nPair 5\nMethod 1\nMethod 2\nTest the hypothesis that the first method is better than the second method at the 0.05 level. You\nmay assume normality of the data.\nSolution\nWe will perform a paired t-test. The hypotheses are:\nH0 : μ1 = μ2\nH1 : μ1 > μ2\nWe have: d1 = 25-21 = 4, d2 = 30-20 = 10, d3 = 22-23 = -1, d4 = 27-18 = 9, d5 = 29-17 = 12.\nIt is: d = 6.8 and\nn (d(i) - d )2\ni=1\nsd =\n= 5.26\nn - 1\nUnder the null hypothesis, the t-statistic\nd\nt =\n√\nsd/\nn\nhas a t4 distribution. Since\n6.8\nt =\n√ = 2.89 > t4,0.05 = 2.132\n5.26/ 5\nwe conclude that the first method is significantly better than the second method.\n\nProblem 3\nAfter your MIT graduation you decide to go to Monte Carlo for vacation. You visit a casino and decide\nto gamble; specifically, you want to play roulette. Before you start betting, you watch 500 roulette\ngames at the casino, and you find that red is hit 260 times. Can you determine whether the roulette is\nfair? In case you don't know how roulette is played in Monte Carlo, it involves spinning a wheel that\nhas many slots around it. There is 1 gold slot, 18 red slots, and 18 black slots.\n1. Set up the hypotheses.\n2. Calculate the P-value. Can you reject null hypothesis at the 0.05 level?\n3. Find a 95% CI for the proportion of red results.\nSolution\n1. We let p be the probability of a red slot on a single spin, which is 18 = 0.4865 if the roulette is\nfair. The alternative hypothesis for this test is two-sided because we just want to check if p is\ndifferent from 18 The hypotheses are:\n37 .\nH0 : p = p0 = 0.4865\nH1 : p = p0 = 0.4865\n2. Since n = 500, we do a large-sample test. We are given the total number of red hits is Y = 260,\nso the sample proportion is\nY\npˆ =\n=\n= 0.52.\nn\nUnder the null hypothesis, ˆp is approximately normally distributed with mean p0 = 18 = 0.4865\np0q0\n0.4865·0.5135\nand variance\n=\n= 0.000499. Thus the following test statistic is approximately\nn\nstandard normal:\npˆ- p0\nZ =\n.\np0q0/n\nThe observed value of the test statistic is\n0.52 - 0.4865\n0.0335\nz =\n=\n= 1.50.\n0.4865·0.5135\n0.0224\nThe p-value is 2 · P (Z > 1.5) = 0.134. So no, we cannot reject H0 at the 0.05 level because\n0.134 > 0.05.\n3. To compute a confidence interval, we estimate the variance of ˆp by pˆqˆ. A 95% CI for the proportion\nn\nof red hits is\n\npˆqˆ\n0.52 · 0.48\npˆ± zα/2\n= 0.52 ± 1.96\n= [0.4762, 0.5638].\nn\nNotice that 0.4865 is in this interval, so we do not reject H0.\n\nProblem 4\nThe weight of an object is measured using an electronic scale that reports the true weight plus a random\nfluctuation that is normally distributed with zero mean. The manufacturing company of the electronic\nscale claims that the standard deviation of the fluctuation is 2 milligrams. Assume that the fluctuations\nare independent. The company measures the weight of one object 8 times, and observes these values:\n100.9, 98.2, 101.5, 102.2, 105.1, 99.4, 93.6, 97.5\n1. You believe that the standard deviation of the fluctuation is greater than what the company\nclaims. Is there statistically significant evidence for your belief at α = 5% level?\n2. Compute a 95% confidence interval for the variance of the fluctuation.\nSolution\n1. We perform a chi-square test with hypotheses:\nH0 : σ = 2\nH1 : σ > 2\nWhen H0 is true, the statistic\n(n - 1)s\nχ2 =\nσ2\nhas a χ2\n7 distribution. The observed value of the chi-square statistic is:\n(n - 1)s\nχ2 =\n= 21.1\nσ2\nThe P-value is P (χ2\n7 > 21.1) = 0.0036 and therefore there is statistically significant evidence for\nour belief at α = 5% level.\n2. A 95% two-sided confidence interval for the variance of the fluctuation is given by:\n\n(n - 1)s\n(n - 1)s\n,\n= [5.27, 49.94]\nχ2\nχ2\n7,0.025\n7,0.975\n\nProblem 5\nYou are an investment manager of an asset management company and you want to evaluate the perfor\nmance of two hedge funds. For this you record their past performance for the last 10 years. The returns\nfor both of the hedge funds in different years are independent and normally distributed with unknown\n\nmean and variance. The mean return for Hedge Fund 1, averaged over the 10 years, is X = 22.15% and\nthe standard deviation calculated from those 10 measurements is 2.79%. For Hedge Fund 2 the mean\n\nreturn, averaged over the 10 years, is Y = 20.6% and the standard deviation calculated from those 10\nmeasurements is 2.46%.\n1. Test equality of the variances for the two hedge funds using α = 0.1.\n2. Test for the equality of the means of the two hedge funds' performances, using α = 0.05 with\na suitable test. Would you recommend using a t-test calculated from a pooled variance or the\nmethod for unequal variances? If you rejected the test in (1) then you may assume the variances\nare unequal, otherwise you may assume that the variances are equal.\nSolution\n1. We perform an F test with hypotheses:\nH0 : σ2 = σ2\nH1 : σ2 = σ2\nThe F -statistic is\ns1\nF =\n= 1.29\ns2\nUnder H0 this statistic has F9,9 distribution. From the table in the book we see that this is less\nthan the critical value 3.18, therefore we do not reject the null hypothesis.\n2. The hypotheses are:\nH0 : μ1 = μ2\nH1 : μ1 = μ2\nSince we do not reject the hypothesis that σ2 = σ2\n2, we compute the pooled variance.\n(n1 - 1)s1 + (n2 - 1)s\n9 · 2.792 + 9 · 2.462\ns =\n=\n= 6.92.\n(n1 - 1) + (n2 - 1)\n9 + 9\nUnder the null hypothesis that the mean returns of the two hedge funds are equal,the t-statistic:\nX - Y\nt =\nS ·\n+\nq\n\nhas a t10+10-2 = t18 distribution. Its observed value is:\nx - y\nt =\n= 1.33\ns ·\n+\nSince |t| is less than t18,0.025 = 2.1, we do not reject the null hypothesis at α = 5% level.\nq\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/2493a966458bb01523e873e30d65b3ae_MIT15_075JF11_exam03.pdf",
      "content": "15.075 Exam 3\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nNovember 22, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nA company makes high-definition televisions and does not like to have defective pixels. Historically, the\nmean number of defective pixels in a TV is 20. An MIT engineer is hired to make better TV's that\nhave fewer defective pixels. After her first week of work she claims that she can significantly improve\nthe current method. To check her claim you try her new method on 100 new televisions. The average\nnumber of defective pixels in those 100 TV's is 19.1. Assume that the new method doesn't change the\nstandard deviation of defective pixels, which has always been 4.\n1. Test if the new method is significantly better than the old one at the α = 0.05 level.\n2. Using the new method, assume that the mean number of defective pixels is actually 19. What\nis the chance that your test from part 1 will conclude that the new method is statistically more\neffective?\n3. How many televisions will you have to check so that the test you did in part 1 will conclude that\nthe new method is effective, with 95% probability? Please assume again that the mean number\nof defective pixels is actually 19.\n\nProblem 2\nTwo methods of memorizing words are to be compared. You choose two groups of 5 people, where the\nfirst person in the first group has the same characteristics as the first person in the second group (they\nhave the same educational level, age, etc.). Same thing about the second person in each group - they\nare also similar to each other in terms of education, age, etc. Same thing for the third, fourth and fifth\npeople from each group. The first group is assigned to the first method of memorization and the second\ngroup to the other method. The number of words recalled in a memory test after a week's training with\nthese two methods is shown below.\nPair 1\nPair 2\nPair 3\nPair 4\nPair 5\nMethod 1\nMethod 2\nTest the hypothesis that the first method is better than the second method at the 0.05 level. You\nmay assume normality of the data.\n\nProblem 3\nAfter your MIT graduation you decide to go to Monte Carlo for vacation. You visit a casino and decide\nto gamble; specifically, you want to play roulette. Before you start betting, you watch 500 roulette\ngames at the casino, and you find that red is hit 260 times. Can you determine whether the roulette is\nfair? In case you don't know how roulette is played in Monte Carlo, it involves spinning a wheel that\nhas many slots around it. There is 1 gold slot, 18 red slots, and 18 black slots.\n1. Set up the hypotheses.\n2. Calculate the P-value. Can you reject null hypothesis at the 0.05 level?\n3. Find a 95% CI for the proportion of red results.\n\nProblem 4\nThe weight of an object is measured using an electronic scale that reports the true weight plus a random\nfluctuation that is normally distributed with zero mean. The manufacturing company of the electronic\nscale claims that the standard deviation of the fluctuation is 2 milligrams. Assume that the fluctuations\nare independent. The company measures the weight of one object 8 times, and observes these values:\n100.9, 98.2, 101.5, 102.2, 105.1, 99.4, 93.6, 97.5\n1. You believe that the standard deviation of the fluctuation is greater than what the company\nclaims. Is there statistically significant evidence for your belief at α = 5% level?\n2. Compute a 95% confidence interval for the variance of the fluctuation.\n\nProblem 5\nYou are an investment manager of an asset management company and you want to evaluate the perfor\nmance of two hedge funds. For this you record their past performance for the last 10 years. The returns\nfor both of the hedge funds in different years are independent and normally distributed with unknown\n\nmean and variance. The mean return for Hedge Fund 1, averaged over the 10 years, is X = 22.15% and\nthe standard deviation calculated from those 10 measurements is 2.79%. For Hedge Fund 2 the mean\n\nreturn, averaged over the 10 years, is Y = 20.6% and the standard deviation calculated from those 10\nmeasurements is 2.46%.\n1. Test equality of the variances for the two hedge funds using α = 0.1.\n2. Test for the equality of the means of the two hedge funds' performances, using α = 0.05 with\na suitable test. Would you recommend using a t-test calculated from a pooled variance or the\nmethod for unequal variances? If you rejected the test in (1) then you may assume the variances\nare unequal, otherwise you may assume that the variances are equal.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "MIT15_075JF11_exam04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/8ea4b87441df8636d013e58f0faaa68d_MIT15_075JF11_exam04.pdf",
      "content": "15.075 Exam 4\nInstructor: Cynthia Rudin\nTA: Dimitrios Bisias\nDecember 21, 2011\nGrading is based on demonstration of conceptual understanding, so you need to show all of your work.\nProblem 1\nChoose Y or N for each (you do not need to justify):\nY N\nYou can perform a t-test to determine whether the population mean is different from 10 using a\nsample of size 400 without the normality of data assumption.\nY N\nIf X ∼ U[-1, 1] and Y = X2, then cov(X,Y) = 0.\nY N\nThe hypothesis\nH0 : μ = 0\nH1 : μ > 0\nis rejected at α = 0.05 if the 2-sided 90% CI does not contain 0\nY N\nIn multiple regression if r2 is less than 0.05, then the F statistic of the regression is not significant\nat the 0.1-level.\n\nProblem 2\nFor the following scenarios, determine\na. the smallest set of assumptions you need to make in order to do a hypothesis test.\nb. which test you might perform. If multiple tests apply, please choose a test that has the fewest\nassumptions, and among those, choose the strongest test.\nc. the hypotheses for the test\nd. how you might check the assumptions you made (if you made any)\n2.1\nYou want to check if hedge fund B is riskier i.e. its returns have higher variance, than hedge fund A.\nFor this you collect their yearly returns from 1995-2010.\n2.2\nYou want to check if city A has taller people on average than city B. For this you measure the heights\nof 200 people from city A and 400 people from city B.\n2.3\nYou want to check if there is a linear relationship between the salary of a CEO, his company's return\non-equity and his company's sales. For this you collect data from 200 large corporations and you regress\nthe CEO salary on the return-on-equity and the company's sales.\n2.4\nYou want to see if income of people is independent of their religion. For this you randomly collect 500\npeople and you classify them by their religion and whether or not their income is less than 50K per\nyear.\n2.5\nOf 64 offspring of a certain cross between guinea pigs, 34 were red, 10 were black, and 20 were white.\nAccording to the genetic model, these numbers should be in ratio 9/3/4. You want to check if the data\nare consistent with the genetic model.\n2.6\nYou would like to know whether your waiting time for the bus is, on average, greater than 5 minutes.\nYou measure your waiting time on 20 different days. (Hint: do you know anything about waiting times\nfor buses?)\n\n2.7\nYou would like to compare LeBron James and Kobe Bryant regarding their performance in free throws.\nFor this for each player you count how many free throws he scored in a season. Check if LeBron has\nhigher success rate than Bryant. (Hint: the usual number of free throws for a good basketball player in\na season is in the hundreds.)\n\nProblem 3\nWe want to find the relationship between the college GPA and college entrance verbal and mathematics\ntest scores. For this reason we collect data from n = 40 students as in Table 1. Let y be GPA, x1 be\nverbal score, and x2 be math score.\nLetting the GPA be the dependent variable y, a regression is fit on the independent variables verbal\nscore (x1) and math score (x2) yielding β0 = -1.570537, β1 = 0.025732, and β2 = 0.033615.\nGPA\n3.49\n2.89\n· · ·\n3.84\nMath Score\n· · ·\nVerbal Score\n· · ·\nFitted GPA\n3.4383\n3.5071\n· · ·\n3.7943\nTable 1: GPA, math and verbal score.\nThe output below shows the estimates of the coefficients and their standard errors.\nCoefficients:\nEstimate\nStd. Error\n(Intercept)-1.570537\n0.493749\nverbal\n0.025732\n0.004024\nmath\n0.033615\n0.004928\nYou are also told that y = 2.98, and also that\n(3.49 - 2.98)2 + (2.89 - 2.98)2 + · · · + (3.84 - 2.98)2 = 18.7736\nand further you are given that\n(3.49 - 3.4383)2 + (2.89 - 3.5071)2 + · · · + (3.84 - 3.7943)2 = 5.9876\n2?\n1. What is the coefficient of multiple determination r\n2. Fill the ANOVA table.\n3. Test the hypothesis that all of the coefficients are zero at α = 0.05.\n4. Test the hypothesis that the coefficient for math score is zero at α = 0.05.\n\nProblem 4\nYou are the owner of an italian restaurant. Some days you sell a lot of pizzas and some other days you\ndon't, independently of other days. In fact, the distribution of the number of pizzas you sell each day\nis shown below.\nIf we measure pizza sales for 100 days, what is the approximate probability that the average pizza\nsales (over the 100 days) will be more than 90 pizzas?\n\nProblem 5\nWe want to explore the association between political party affiliation and age among the US population.\nFor this reason, we take a random sample of 521 US citizens based on their SSN and then we record\nwhether they are older than 40 or not and their political affiliation. The table below shows the results.\nGender\nParty A\nParty B\nRow total\nAge ≥ 40\nAge < 40\nColumn Total\n1. Which type of sampling is used here?\n2. Set up the hypotheses to test whether there is an association between the age and the political\nparty affiliation\n3. Perform the test at the α = 0.05 level.\n\nProblem 6\nWe would like to find some way to claim that the new strawberry cheesecakes at our bakery tend to\nhave more than 300 grams of jam. Here's the measurements of jam for 10 cheesecakes (measured in\ngrams):\nGrams of jam\nTable 2: Measurements of jam.\nA histogram of these data is here:\nPerform two different hypothesis tests using α = 0.05 to determine whether the cheesecakes have\nsignificantly more jam than 300 grams. What is your overall conclusion from the tests?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/6c4b98949fe3e0acae5f75245e4b09dc_MIT15_075JF11_chpt02.pdf",
      "content": "Probability Review\n15.075 Cynthia Rudin\nA probability space, defined by Kolmogorov (1903-1987) consists of:\n- A set of outcomes S, e.g.,\nfor the roll of a die, S = {1, 2, 3, 4, 5, 6},\n\nfor the roll of two dice, S =\n\n,\n\n,\n\n,\n\n, . . . ,\ntemperature on Monday, S = [ 50, 50].\n\n-\n- A set of events, where an event is a subset of S, e.g.,\n\nroll at least one 3 :\n, . . . ,\n\ntemperature above 80*\n: (80\n\n, 150]\n\nThe union, intersection and complement of events are also events (an algebra).\n- A probability measure, which gives a number between 0 and 1 to each event, where\nP (S) = 1 and\nA ∩ B = ∅\n⇒\nP (A ∪ B) = P (A) + P (B).\nThink of P as measuring the size of a set, or an area of the Venn diagram.\nThe conditional probability of event A given event B is:\nP (A ∩ B)\nP (A|B) :=\n.\nP (B)\n\nEvent A is independent of B if P (A|B) = P (A). That is, knowing B occurred doesn't\nimpact whether A occurred (e.g. A and B each represent an event where a coin returned\nheads). In that case,\nP (A ∩ B)\nP (A) = P (A|B) :=\nso\nP (A ∩ B) = P (A)P (B).\nP (B)\nDo not confuse independence with disjointness. Disjoint events A and B have P (A∩B) = 0.\nFor a partition B1, . . . , Bn, where Bi ∩ Bj = ∅ for i\n· · Bn = S then\n= j and B1 ∪ B2 ·\nA = (A ∩ B1) ∪ (A ∩ B2), . . . , ∪(A ∩ Bn)\nand thus\nn\n\nP (A) =\nP (A ∩ Bi) =\nP (A|Bi)P (Bi)\ni=1\ni\nfrom the definition of conditional probability.\nBayes Theorem\nP (A|B)P (B)\nP (B|A) =\n.\nP (A)\nDerivation of Bayes Rule\nMonty Hall Problem\nA random variable (r.v.) assigns a number to each outcome in S.\nExample 1: toss 2 dice: random var. X is the sum of the numbers on the dice.\nExample 2: collections of transistors: random var X is the min of the lifetimes of the\ntransistors.\n\nA probability density function (pdf. . . or probability mass function pmf) for random variable\nX is defined via:\n- f(x) = P (X = x) for discrete distributions\n(Note f(x) ≥ 0,\nx f(x) = 1.)\nb\n- P (a ≤ X ≤ b) =\na f(x)dx for continuous distributions.\n(Note f(x) ≥ 0,\n\nf(x) = 1.)\nThe cumulative distribution function (cdf) for r.v. X is:\nF (x) = P (X ≤ x)\n=\nf(k) (discrete)\nk≤x\nZ x\n=\nf(y)dy (continuous)\n-inf\nX\n\nThe expected value (mean) of an r.v. X is:\nE(X)\n= μ =\nxf(x)\n(discrete)\nx\nE(X)\n= μ =\nxf(x)dx\n(continuous).\nx\nExpectation is linear, meaning\nE(aX + bY ) = aE(X) + bE(Y ).\nRoulette\nThe variance of an r.v. X is:\nV ar(X) = σ2 = E(X - μ)2 .\nVariance measures dispersion around the mean. Variance is not linear:\nV ar(aX + b) = a 2V ar(X).\nThe standard deviation (SD) is:\n\nSD(x) = σ =\nV ar(X).\nNote that people sometimes use another definition for variance that is equivalent:\nV ar(X) = σ2 = E(X2) - (E(X))2 .\nSum of Two Dice\nQuantiles/Percentiles The pth quantile (or 100pth percentile), denoted θp, of r.v. X obeys:\nP (X ≤ θp) = p.\nNote: 50th percentile, or .5th quantile, is the median θ.5.\nExponential Distribution\nX\nZ\n\nSection 2.5. Jointly Distributed R.V.'s\nJointly distributed r.v.'s have joint pdf's: f(x, y) = P (X = x, Y = y).\nTheir covariance is defined by\nCov(X, Y ) := σx,y := E[(X - μx)(Y - μy)].\nThe first term considers dispersion from the mean of X, the second term is dispersion from\nthe mean of Y .\nIf X and Y are independent, then E(XY ) = E(X)E(Y ) = μxμy (see \"expected value\" on\nWikipedia). So, multiplying the terms out and passing the expectation through, we get:\nCov(X, Y ) = E[(X - μx)(Y - μy)] = E(XY ) - μxE(Y ) - μyE(X) + μxμy = 0.\nUseful relationships:\n1. Cov(X, X) = E[(X - μx)2] = V ar(X)\n2. Cov(aX + c, bY + d) = ab Cov(X, Y )\n3. V ar(X±Y ) = V ar(X)+V ar(Y )±2Cov(X, Y ) where Cov(X, Y ) is 0 if X and Y are indep.\nThe correlation coefficient is a normalized version of covariance so that its range is [-1,1]:\nCov(X, Y )\nσXY\nρXY := Corr(X, Y ) :=\n=\n.\nV ar(X)V ar(Y )\nσX σY\nSection 2.6.\nChebyshev's Inequality (we don't use it much in this class)\nσ2\nFor c > 0, P (|X - EX| ≥ c) ≤\n.\nc\nChebyshev's inequality is useful when you want a upper bound on how often an r.v. is far\nfrom its mean. You can't use it for directly calculating P (|X-EX| ≥ c), only for bounding it.\nWeak Law of Large Numbers\nLet's say we want to know the average number of pizza slices we will sell in a day. We'll\nmeasure pizza sales over the next couple weeks. Each day gets a random variable Xi which\nrepresents the amount of pizza we sell on day i. Each Xi has the same distribution, because\neach day basically has the same kind of randomness as the others. Let's say we take the\naverage of over the couple weeks we measured. There's a random variable for that, it's\n\nX = 1\nn\n- Do\nP\ni Xi.\n\nes X have anything to do with the average pizza sales per day? In other words, does\n\nmeasuring X tell us anything about the Xi's? For instance, (on average) is X close to\nthe average sales per day, E(Xi)?\n\n- Does it matter what the distribution of pizza sales is? For instance, what if we usually\nhave 1-4 customers but sometimes we have a conference where we have 45-50 people;\na weird\n\nthat is kind of\ndistribution. Does that mean that the Xi needs to be adjusted\nin some way to help us understand the Xi's?\nIt turns out the first answer is yes, in fact\n\npretty often, the average X is very similar to the\naverage value of Xi. Especially when n is large. The second answer is also yes, but as long\n\nas n is large, X is very similar to the average value of Xi. This means that no matter what\nthe distribution is, as long as we measure enough days, we can get a pretty good sense of\nthe average pizza sales.\n\nWeak LLN: Let X be a r.v. for the sample mean X = 1\ndistributed)\nn\ni Xi of n iid (independent and\nidentically\nr.v.'s. The distribution for each Xi has finite mean μ and variance\nσ . Then for any c > 0,\nP\n\nP (| X - μ| ≥ c) → 0 as n →inf.\neak\n\nW\nLLN says that X approaches μ when n is large. Weak LLN is nice because it tells us\nthat no matter what the distribution of the Xi's is, the sample mean approaches the true\nmean.\nProof Weak LLN using Chebyshev\nSection 2.7. Selected Discrete Distributions\nBernoulli X ∼ Bernoulli(p) \"coin flipping\"\nf(x) = P (X = x) =\n\np\nif x = 1 \"heads\"\n1 - p\nif x = 0 \"tails\"\nBinomial X ∼ Bin(n, p) \"n coins flipping,\" \"balls in a bag drawn with replacement,\" \"balls\nin an infinite bag\"\nn\n)\n\nf(x = P (X = x) =\n\npx (1 - p)n-x for x = 0, 1, . . . , n,\nx\nwhere\nn\nx\n\nis the number of ways to distribute x successes and n - x failures,\nn\nn!\n=\n.\nx\n\nx!(n - x)!\n(If you have n coins flipping, f(x) is the probability to get x heads, p is the probability of\nheads.) X ∼ Bin(n, p) has\nE(X) = np, V ar(X) = np(1 - p).\nYou'll end up needing these facts in Chapter 9.\n\nHypergeometric X ∼ HyGE(N, M, n) \"balls in a bag drawn without replacement,\" where:\nN is the size of the total population (the number of balls in the bag),\nM is the number of items that have a specific attribute (perhaps M balls are red),\nn is our sample size,\nf(x) is the probability that x items from the sample have the attribute.\nways to draw x balls\nways to draw n - x balls\nM\nN - M\nwith attribute\nwithout attribute\nx\nn - x\nf(x) =\nways to draw n balls\n=\nN\n.\nn\nMultinomial Distribution \"generalization of binomial\"\nThink of customers choosing backpacks of different colors. A random group of n customers\neach choose their favorite color backpack. There is a multinomial distribution governing how\nmany backpacks of each color were chosen by the group. Below, xk is the number of people\nwho ordered the kth color backpack. Also, f(x) is the probability that x1 customers chose\ncolor 1, x2 customers chose color 2, and so on.\nf(x1, x2, x3, . . . , xk)\n= P (X1 = x1, X2 = x2, X3 = x3, . . . , Xk = xk)\nn!\nx1 x2 x3\nxk\n=\np p p . . . p\nk\nx1!x2!x3! . . . xk!\nwhere xi ≥ 0 for all i and\ni xi = n (there are n total customers), and pi is the probability\nthat outcome i occurs. (pi is the probability to choose the ith color backpack).\n2.8 Selected Continuous Distributions\nUniform Distribution X ∼ U[a, b]\na ≤ x ≤ b\nf(x) =\nb-a\n0 otherwise\nPoisson Distribution X ∼ P ois(λ) \"binomial when np → λ,\" \"rare events\"\ne-λλx\nf(x) =\nx = 0,1,2,. . .\nx!\nX ∼ P ois(λ) has\nE(X) = λ, V ar(X) = λ.\n\nExponential Distribution X ∼ Exp(λ) \"waiting times for Poisson events\"\nf(x) = λe-λx for x ≥ 0\nGamma Distribution X ∼ Gamma(λ, r) \"sums of r iid exponential r.v.'s,\" \"sums of waiting\ntimes for Poisson events\"\nr-1 -λx\nλrx\ne\nf(x) =\nfor x ≥ 0,\nΓ(r)\nwhere Γ(r) is the \"Gamma\" function, which is a generalization of factorial.\nCustomers on line\nNormal (Gaussian) Distribution X ∼ N(μ, σ2)\n-(x-μ)2/2σ2\nf(x) = √\ne\nfor inf < x < inf.\nσ 2π\nWe have that:\nE(X) = μ, V ar(X) = σ2 .\nWe often standardize a normal distribution by shifting its mean to 0 and scaling its variance\nto 1:\nX - μ\nIf X ∼ N(μ, σ2) then Z =\n∼ N(0, 1)\n\"standard normal\"\nσ\nSince we can standardize any gaussian, let's work mostly with the standard normal Z ∼\nN(0, 1):\npdf:\nφ(z) =\n√\ne -z2/2\n2π\nz\ncdf:\nΦ(z) = P (Z ≤ z) =\nφ(y)dy\n-inf\nWe can't integrate the cdf in closed form. This means that in order to get from z to Φ(z)\n(or the reverse) we need to get the answer from a book or computer where they integrated\nZ\n\nit numerically, by computing an approximation to the integral. MATLAB uses the command\nnormcdf. Table A.3 in your book has values for Φ(z) for many possible z's. Read the table\nleft to right, and up to down. So if the entries in the table look like this:\nz\n0.03\n-2.4\n0.0075\nThis means that for z = -2.43, then Φ(z) = P (Z ≤ z) = 0.0075. So the table relates z to\nΦ(z). You can either be given z and need Φ(z) or vice versa.\n75th percentile calculation\nDenote zα as the solution to 1 - Φ(z) = α.\nzα is called the upper α critical point or the 100(1 - α)th percentile.\nLinear Combinations of Normal r.v.'s are also normal.\nFor n iid observations from N(μ, σ2), that is.\nXi ∼ N(μ, σ2) for i = 1, . . . , n,\n\nthe sample mean X obeys:\nσ2\nX ∼ N\nμ,\n.\nn\nHmm, where have you seen σ2/n before? Of course! You saw it here:\n\nV ar( X) = V ar\nn\nXi\n= 1\nn2\nV ar(Xi) = nσ2\nn2 = σ2\nn .\ni\ni\nThis is the variance of the mean of n iid r.v.'s who each have V ar(Xi) = σ2 .\nX\nX\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/3b37951fcbab6644a27511d7616576a3_MIT15_075JF11_chpt03.pdf",
      "content": "Chapter 3: Collecting Data\n\nA\nwhich information is sought.\npopulation is a collection of objects, items, humans/animals (\"units\") about\n\nA\n\nsample is a part of the population that is observed.\nA\nunemployment in the US.\nparameter is a numerical characteristic of a population, e.g. percent\n\nA\nparameter, e.g., percent unemployment in a sample.\nstatistic is a numerical function of the sampled data, used to estimate an unknown\n\nDon't get confused between the population and the sample! There is separate notation\nfor the population and the sample - make sure you know what you can calculate and\nwhat you can't calculate! You can't calculate anything from the population if you only\nhave a sample.\n\nA\nsampling frame and we need to choose which units to sample.\n\nsampling frame is a list of all units in a finite population. Often, we do not have a\nA\n\nth\nrepresen\nCon\ne populati\ntative sample\non.\ndoes not differ in systematic and important ways from\nvenience sampling\nJudgment sampling\ninvolves using a sample that is easily available.\n.\nquota\nsampling)\ninvolves a trained sample collector (who may use\nA\nrandomly from the population.\n\nBias is possible with these sampling methods. To avoid bias, sample\nwi\nsimple random sample (SRS) of size n from a population of size N is drawn\nbei\nth\nng c\nout r\nho\neplacement so that each possible sample of size n has the same chance of\n\nsen.\nAn SRS is not always practical to obtain, for instance for a highly diverse population,\nor a\n\nStratified ran\nreally large population.\nsubpopulations and drawing and SRS from each one.\ninvolves dividing the population into homogeneous\ne.g.\nto d\ncustomers stratified by race (some races are rarer than others)\no statistics\ndom samplin\non subpopulati\ng\nons as well as on the wh\nThis\nole po\nis us\npulati\neful when you want\no\n\nn.\nMultistage cluster samplin\neach stage. Useful for sampling large populations.\ng - \"Tree structured\" sampling, units are different at\n\ne.g. 1) Dra\n2) Draw SRS of counties from the states\nw SRS of states\n3) Draw of people from each county\n\n1-‐in-‐k systematic sampling consists of selecting every kth unit. Useful for sampling\nitems coming off assembly lines.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/e8d615e72bb6d384e34fc2a10a8f03cb_MIT15_075JF11_chpt04.pdf",
      "content": "|\nChapter 4 - Summarizing Numerical Data\n15.075 Cynthia Rudin\nHere are some ways we can summarize data numerically.\n- Sample Mean:\nn xi\ni=1\nx :=\n.\nn\nNote: in this class we will work with both the population mean μ and the sample\nmean x . Do not confuse them! Remember, x is the mean of a sample taken from the\npopulation and μ is the mean of the whole population.\n- Sample median: order the data values x(1) ≤ x(2) ≤ · · · ≤ x(n), so then\n\nx( n+1\nn odd\n2 )\nmedian := x :=\n.\n1 [x( n ) + x( n +1)] n even\nMean and median can be very different: 1, 2, 3, 4, 500 .\noutlier\nThe median is more robust to outliers.\n- Quantiles/Percentiles: Order the sample, then find xp so that it divides the data\ninto two parts where:\n- a fraction p of the data values are less than or equal to xp and\n- the remaining fraction (1 - p) are greater than xp.\nThat value xp is the pth-quantile, or 100×pth percentile.\n- 5-number summary\n{xmin, Q1, Q2, Q3, xmax},\nwhere, Q1 = θ.25, Q2 = θ.5, Q3 = θ.75.\n- Range: xmax - xmin measures dispersion\n- Interquartile Range: IQR := Q3 - Q1, range resistant to outliers\n\n|\n- Sample Variance s2 and Sample Standard Deviation s:\ns 2 :=\nn - 1\nn\nX\n(xi - x)2 .\ni=1\nsee why later\nRemember, for a large sample from a normal distribution, ≈ 95% of the sample falls in\n[ x - 2s, x + 2s].\nDo not confuse s2 with σ2 which is the variance of the population.\n- Coefficient of variation (CV) := x\ns\n, dispersion relative to size of mean.\n- z-score\nxi - x\nzi :=\n.\ns\n- It tells you where a data point lies in the distribution, that is, how many standard\ndeviations above/below the mean.\nE.g. zi = 3 where the distribution is N(0, 1).\n- It allows you to compute percentiles easily using the z-scores table, or a command\non the computer.\nNow some graphical techniques for describing data.\n- Bar chart/Pie chart - good for summarizing data within categories\n| {z }\n\n- Pareto chart - a bar chart where the bars are sorted.\n- Histogram\nBoxplot and normplot\nScatterplot for bivariate data\nQ-Q Plot for 2 independent samples\nHans Rosling\n\nChapter 4.4: Summarizing bivariate data\nTwo Way Table\nHere's an example:\nRespiratory Problem?\nyes no row total\nsmokers\nnon-smokers\ncolumn total\nQuestion: If this example is from a study with 50 smokers and 50 non-smokers, is it\nmeaningful to conclude that in the general population:\na) 25/30 = 83% of people with respiratory problems are smokers?\nb) 25/50 = 50% of smokers have respiratory problems?\nSimpson's Paradox\n- Deals with aggregating smaller datasets into larger ones.\n- Simpson's paradox is when conclusions drawn from the smaller datasets are the opposite\nof conclusions drawn from the larger dataset.\n- Occurs when there is a lurking variable and uneven-sized groups being combined\nE.g. Kidney stone treatment (Source: Wikipedia)\nWhich treatment is more effective?\nTreatment A Treatment B\n78% 273\n83% 289\nIncluding information about stone size, now which treatment is more effective?\nTreatment A Treatment B\nsmall\nstones\ngroup 1\n93% 81\ngroup 2\n87% 234\nlarge\nstones\ngroup 3\n73% 192\ngroup 4\n69% 55\nboth\n78% 273\n83% 289\nWhat happened!?\n\nX\nContinuing with bivariate data:\n- Correlation Coefficient- measures the strength of a linear relationship between two\nvariables:\nSxy\nsample correlation coefficient = r :=\n,\nSxSy\nwhere\nn\nSxy =\n(xi - x )(yi - y )\nn - 1 i=1\nS2 =\nn\n(xi - x )2 .\nx\nn - 1 i=1\nThis is also called the \"Pearson Correlation Coefficient.\"\n- If we rewrite\nn (xi - x ) (yi - y )\nr =\n,\nn - 1 i=1\nSx\nSy\nx)\ny)\nyou can see that (xi- and (yi- are the z-scores of xi and yi.\nSx\nSy\n- r ∈ [-1, 1] and is ±1 only when data fall along a straight line\n- sign(r) indicates the slope of the line (do yi's increase as xi's increase?)\n- always plot the data before computing r to ensure it is meaningful\n- Correlation does not imply causation, it only implies association (there may be\nlurking variables that are not recognized or controlled)\nFor example: There is a correlation between declining health and increasing wealth.\n- Linear regression (in Ch 10)\ny - y\nx - x\n= r\n.\nSy\nSx\nX\nX\nX\n\nChapter 4.5: Summarizing time-series data\n- Moving averages. Calculate average over a window of previous timepoints\n-\nxt-w+1 + · · · + xt\nMAt =\n,\nw\nwhere w is the size of the window. Note that we make window w smaller at the\nbeginning of the time series when t < w.\nExample\nTo use moving averages for forecasting, given x1, . . . , xt-1, let the predicted value\nat time t be ˆxt = MAt-1. Then the forecast error is:\net = xt - xˆt = xt - MAt-1.\n- The Mean Absolute Percent Error (MAPE) is:\nT\n\n· 100%.\net\nMAP E = T - 1\nxt\nt=2\nX\n\nThe MAPE looks at the forecast error et as a fraction of the measurement value xt.\nSometimes as measurement values grow, errors, grow too, the MAPE helps to even this\nout.\nFor MAPE, xt can't be 0.\n- Exponentially Weighted Moving Averages (EWMA).\n- It doesn't completely drop old values.\nEW MAt = ωxt + (1 - ω)EW MAt-1,\nwhere EW MA0 = x0 and 0 < ω < 1 is a smoothing constant.\nExample\n- here ω controls balance of recent data to old data\n- called \"exponentially\" from recursive formula:\nEW MAt = ω[xt + (1 - ω)xt-1 + (1 - ω)2 xt-2 + . . . ] + (1 - ω)tEW MA0\n- the forecast error is thus:\net = xt - xˆt = xt - EW MAt-1\n- HW? Compare MAPE for MA vs EWMA\n- Autocorrelation coefficient. Measures correlation between the time series and a\nlagged version of itself. The kth order autocorrelation coefficient is:\nT\n(xt-k - x )(xt - x )\nt=k+1\nrk :=\nT\nt=1(xt - x )2\nExample\nX\nX\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/9b2bc0992dc98056a903c1c9d5797d73_MIT15_075JF11_chpt05.pdf",
      "content": "Central Limit Theorem\n(Convergence of the sample mean's distribution to the normal distribution)\nLet X1, X2, . . . , Xn be a random sample drawn from any distribution with a finite mean μ\nand variance σ2 . As n →inf, the distribution of:\nX - μ\n√\nσ/ n\n\"converges\" to the distribution N(0, 1). In other words,\nX - μ\n√\n≈ N(0, 1).\nσ/ n\nX -μ\nNote 1: What is σ/ √ n ? Remember that we proved that E( X ) = μ and Var( X ) = σ2/n.\n\nThat means we are taking the random variable X, subtracting its mean, and dividing by its\nstandard deviation. It's a z-score!\nNote 2: \"converge\" means \"convergence in distribution:\"\n\nX - μ\nlim P\n√\n≤ z\n= Φ(z) for all z.\nn→inf\nσ/ n\nDon't worry about this if you don't understand (it's beyond the scope of 15.075).\nNote 3: CLT is really useful because it characterizes large samples from any distribution.\nAs long as you have a lot of independent samples (from any distribution), then the distribu\ntion of the sample mean is approximately normal.\nLet's demonstrate the CLT.\nPick n large. Draw n observations from U[0, 1] (or whatever distribution you like). Repeat\n1000 times.\n\nx1\nx2\nx3\nxn\nx =\nn\ni=1 xi\nt = 1\n.21 .76 .57\n.84 (.21+.76+. . . )/n\n:\n:\n:\nt = 1000\nThen, histogram the values in the rightmost column and it looks normal.\n\nCLT demo\nn=z;\nmyrand=rand(n,500);\nmymeans=mean[myrand);\nhist(mymeans)\nBO\nm\nn=2\nn=3\n4D\nn=30\n\nSampling Distribution of the Sample Variance - Chi-Square Distribution\nFrom the central limit theorem (CLT), we know that the distribution of the sample mean is\napproximately normal. What about the sample variance?\nUnfortunately there is no CLT analog for variance...\nBut there is an important special case, which is when X1, X2, . . . , Xn are from a normal\ndistribution. (Recall that the CLT applies to arbitrary distributions.)\nIf this is true, the distribution of the sample variance is related to the Chi-Square (χ2) dis\ntribution.\nLet Z1, Z2, . . . , Zν be N(0, 1) r.v.'s and let X = Z1\n2 + Z2\n2 + · · · + Zν\n2 . Then the pdf of X\ncan be shown to be:\nν/2-1 -x/2\nf(x) =\nx\ne\nfor x ≥ 0.\n2ν/2Γ(ν/2)\nThis is the χ2 distribution with ν degrees of freedom (ν adjustable quantities). (Note: the\nχ2 distribution is a special case of the Gamma distribution with parameters λ = 1/2 and\nr = ν/2.)\nFact proved in book:\nIf X1, X2, . . . , Xn are iid N(μ, σ) r.v.'s, then\n(n - 1)S2 ∼ χ2\nn-1.\nσ2\n(n-1)\nThat is, the sample variance times a constant\nhas a χ2\ndistribution.\nσ2\nn-1\nTechnical Note: we lost a degree of freedom when we used the sample mean rather than the\ntrue mean. In other words, fixing n - 1 quantities completely determines s2, since:\n\nx)2\ns :=\n(xi - .\nn - 1\ni\nLet's simulate a χn\n-1 distribution for n = 3. Draw 3 samples from N(0, 1). Repeat 1000\ntimes.\nz1\nz2\nz3\ni=1 z2\ni\nt = 1\n-0.3 -1.1 0.2\n1.34\n:\n:\n:\nt = 1000\nThen, histogram the values in the rightmost column.\n\nFor the chi-square distribution, it turns out that the mean and variance are:\nE(χ2\nν )\n= ν\nVar(χ2\nν )\n=\n2ν.\nWe can use this to get the mean and variance of S2:\nσ2χ2\nσ2\nE(S2)\n= E\nn-1\n=\n(n - 1) = σ2 ,\nn - 1\nn - 1\nσ2χ2\nσ4\nσ4\n2σ4\nVar(S2) = Var\nn-1\n=\nVar(χ2\n) =\n2(n - 1) =\n.\nn - 1\n(n - 1)2\nn-1\n(n - 1)2\nn - 1\nSo we can well estimate S2 when n is large, since Var(S) is small when n is large.\nRemember, the χ2 distribution characterizes normal r.v. with known variance. You need to\nknow σ! Look below, you can't get the distribution for S2 unless you know σ.\n(n - 1)S2\nX1, X2, . . . , Xn ∼ N(μ, σ2)\n→\nσ2\n∼ χ2\nn-1\n\nStudent's t-Distribution\nLet X1, X2, . . . , Xn ∼ N(μ, σ2). William Sealy Gosset aka \"Student\" (1876-1937) was\nlooking at the distribution of:\nX - μ\nT =\n√\nS/ n\nX-μ\nContrast T with\n√ which we know is N(0, 1).\nσ/ n\nSo why was Student looking at this?\nBecause he had a small sample, he didn't know the variance of the distribution and couldn't\nestimate it well, and he wanted to determine how far x was from μ. We are in the case of:\n- N(0, 1) r.v.'s\n\n- comparing X to μ\n- unknown variance σ2\n- small sample size (otherwise we can estimate σ2 very well by s2.)\nRewrite\n\nX-μ\nX - μ\nσ/ √ n\nZ\nT =\n√\n= √\n= _\n.\nS/ n\n√S2\n1√\nS2/σ2\nn σ/ n\nThe numerator Z is N(0, 1), and the denominator is sort of the square root of a chi-square,\nbecause remember S2(n - 1)/σ2 ∼ χ2\nn-1.\nNote that when n is large, S2/σ2 → 1 so the T-distribution → N(0, 1).\nStudent showed that the pdf of T is:\n\nν+1\n-(ν+1)/2\nΓ\nt2\nf(t) = √\n1 +\n-inf < t < inf\nπνΓ (ν/2)\nν\n\nSnedecor's F-distribution\nThe F -distribution is usually used for comparing variances from two separate sources.\nConsider 2 independent random samples X1, X2, . . . , Xn1 ∼ N(μ1, σ1\n2) and Y1, Y2, . . . , Yn2 ∼\nN(μ2, σ2\n2). Define S1\n2 and S2\n2 as the sample variances. Recall:\nS2(n1 - 1)\nS2(n2 - 1)\n∼ χ2\nn1-1\nand\n∼ χ2\nn2-1.\nσ2\nσ2\nThe F-distribution considers the ratio:\nS2/σ2\nχ2\nn1-1/(n1 - 1)\n1 ∼\n.\nS2\n2/σ2\nχ2\nn2-1/(n2 - 1)\nWhen σ2 = σ2, the left hand side reduces to S2/S2\n2 .\nWe want to know the distribution of this! Speaking more generally, let U ∼ χ2 and V ∼ χ2 .\nν1\nν2\nThen W = U/ν1 has an F-distribution, W ∼ Fν1,ν2 .\nV/ν2\nThe pdf of W is:\nν1/2\n-(ν1+ν2)/2\nΓ ((ν1 + ν2)/2)\nν1\nν1\nν1/2-1\nf(w) =\nw\n1 +\nw\nfor w ≥ 0.\nΓ(ν1/2)Γ(ν2/2)\nν2\nν2\nThere are tables in the appendix of the book which solve:\nP (χ2\nν > χ2 )\n= α\nν,α\n↑\n↑\nP (Tν > tν,α)\n= α\n↑\n↑\nP (Fν1,ν2 > fν1,ν2,α)\n= α\n↑\n↑\nNote that because F is a ratio,\nFν1,ν2 = Fν2,ν1\nwhich you might need to use in order to look up the F-scores in a table in the book. Actually,\nyou will need to know:\nfν1,ν2,1-α =\n.\nfν2,ν1,α\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt06a.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/efe16fc4f11d1c72bd0a3498c1935b73_MIT15_075JF11_chpt06a.pdf",
      "content": "Confidence Intervals\nInstead of reporting a \"point estimator,\" that is, a single value, we want to report a\nconfidence interval [L, U] where:\nP {L ≤ θ ≤ U} = 1 - α,\nthe probability of the true value θ being within [L, U] is pretty large.\nHere, [L, U] is a 100(1 - α)% confidence interval. (Here, [L, U] is two-sided meaning L = -inf,\nU\ninf.)\n=\nLet's derive the confidence intervals for μ when X1, . . . , Xn ∼ N(μ, σ2) where we assume\nσ is known. Start with:\n(X - μ)\nZ =\n√\n∼ N(0, 1).\nσ/ n\nNow, split area α between the two tails:\n(Here we define zα to solve P (Z ≥ zα) = α, meaning that α of the probability mass is on\nthe right of zα.)\nSo we know:\nX - μ\n1 - α = P (-zα/2\n≤\nZ\n≤\nzα/2)\nwhere Z =\n√ ,\nσ/ n\n⇑\n⇑\n(*1)\n(*2)\nand we want:\n1 - α = P (L ≤ μ ≤ U) for some L and U.\nLet's solve it on the left for (*1)\nand on the right for (*2):\n\nX - μ\nX - μ\n-zα/2 ≤ Z =\n√\nZ =\n√\n≤ zα/2\nσ/ n\nσ/ n\nσ\nσ\n\n-zα/2 √\n≤ X - μ\nX - μ ≤ zα/2 √\nn\nn\nσ\nσ\n\nμ ≤ X + zα/2 √\nX - zα/2 √\n≤ μ.\nn\nn\n\nPutting it together we have:\n1 - α =\nP\n\nX - zα/2\nσ\n√ n ≤\nμ ≤\nX + zα/2\nσ\n√ n\n\n1 - α =\nP\nL ≤\nμ ≤\nU .\nThis confidence interal [ X - zα/2\nσ\nn\n√\n≤ μ ≤ X + zα/2\nσ\nn\n√] is a (1 - α) level z-interval.\nExample (confidence interval for mean revenue)\nA one-sided confidence interval can be made as well, by cutting off probability α from\nonly one side of the distribution,\nthat is, μ ≥ X - zα √σ\nn or μ ∈ [X - zα √σ\nn , inf) (Lower 1-sided) and:\nμ ≤ X + zα √σ\nn or μ ∈ (-inf, X + zα √σ\nn ] (Upper 1-sided).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt06b.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/d592d16d17af27470dfe6151e3461e50_MIT15_075JF11_chpt06b.pdf",
      "content": "Basic Concepts of Inference\nStatistical Inference is the process of making conclusions using data that is subject to\nrandom variation.\nHere are some basic definitions.\n- Bias(θˆ) := E(θˆ) - θ, where θ is the true parameter value and θˆ is an estimate of it\ncomputed from data.\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n- Var(θˆ) = E(θˆ- E(θˆ))2 . Variance measures \"precision\" or \"reliability\".\n- Mean-Squared Error (MSE) - a way to measure the goodness of an estimator.\nMSE(θˆ) = E(θˆ- θ)2\n= E[θˆ- E(θˆ) + E(θˆ) - θ]2\n\n= E[θˆ- E(θˆ)]2 + E[E(θˆ) - θ]2 + 2E [θˆ- E(θˆ)][E(θˆ) - θ]\nThe first term is Var(θˆ). In the second term, the outer expectation does nothing because the\ninside is a constant. The second term is just the bias squared. In the third term, the part\nE(θˆ) - θ is a constant, so we can pull it out of the expectation. But then what's left inside\nthe expectation is E[θˆ- E(θˆ)] which is zero, so the third term is zero.\n\nMSE(θˆ) =\nBias(θˆ)\n+ Var(θˆ).\n(1)\nPerhaps you have heard of the \"Bias-Variance\" tradeoff. This has to do with statistical\nmodeling and will be discussed when you hear about regression. It boils down to a tradeoff\nin how you create a statistical model. If you try to create a low bias model, you risk that\nyour model might not explain the data well and have a high variance and thus a larger MSE.\nIf you try to create a low variance model, it may do so at the expense of a larger bias and\nthen still a larger MSE.\nIllustra\ntion of bias\nand var\niance.\nImage by MIT OpenCourseWare.\n\n- We will now show why we use:\n\ns 2 =\n(xi - x )2 rather than s 2\n= 1\n(xi - x )2 .\nwrong\nn - 1\nn\ni\ni\nThe answer is that s2 is an unbiased estimator for σ2!\nLet's show this. We have to calculate Bias(S2) = E(S2)-σ2 which means we need E(S2).\nRemember that S2 follows a (scaled) chi-square distribution, and if we go back and look in\nthe notes for the chi-square distribution, we'll find that the expectation for S2 is σ2 . (It's\none of the last equations in the chi-square notes). So, Bias(S2) = σ2 - σ2 = 0. This is why\nwe use n - 1 in the denominator of S2 .\nHowever, it turns out that the mean square error is worse when we use n - 1 in the\ndenominator: MSE(S2) > MSE(S2\n).\nwrong\nLet's show this. Again going back to the notes on the chi-square distribution, we find\nthat:\n2σ4\nVar(S2) =\n.\nn - 1\nPlugging this in to equation (1) using S2 as the estimator θˆ, we find:\n\n2σ4\nMSE(S2) = Var(S2) + Bias(S2)\n=\n+ 0,\nn - 1\nwhereas\n2n - 1\nMSE(S2\n) = (skipping steps here) =\nn2\nσ4 .\nwrong\nAnd if you plot those two on the same plot, you'll see that MSE(S2) is bigger than\nMSE(S2\n).\nwrong\nMSE(S2) (top) and MSE(S2\n) (bottom) versus n for σ2 = 1.\nwrong\nSo using S2 rather than S2\nactually hurts the mean squared error, but not by much and\nwrong\nactually the difference between the two shrinks as n gets large.\n- The standard deviation of θˆ is called the standard error.\n√\nSE( x) = s/ n\nis the estimated standard error of the mean for for independent r.v. - this appears a lot.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt06c.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/c399ff8b5cc62790a7a034e1f2cf1db0_MIT15_075JF11_chpt06c.pdf",
      "content": "Hypothesis Testing\n\nWe want to assess the validity of a claim about the population against a counter\nclaim using sample data. The two claims are:\n\nNull Hypothesis HO is the claim of \"no difference.\"\nAlternative Hypothesis Hi is the claim we are trying to prove.\n\nExample (Vaccine Trial)\nLet Pi be the fraction of the control group population who get polio.\n\nP2 is the fraction of the treatment population who get polio.\n\nHO is the claim that the vaccine is not effective. Pi=P2.\nHi is the claim that the vaccine is effective. Pi>P2.\n\nThe main idea: assume HO is true. Does the data contradict the assumption\nbeyond a reasonable doubt?\n\nIf yes: accept Hi and reject HO.\n\nIf no: HO can't be ruled out as an explanation for the data. In this case we\ncan't accept any hypothesis.\n\nAnalogy: Presumed innocent until proven guilty\n\nHO: not guilty\n\nHi: guilty\n\nDoes the data contradict HO? If yes, then rule as guilty. If no, it doesn't mean\nthe person is innocent but evidence is insufficient to establish guilt, it gives the\nperson the benefit of the doubt.\n\nWe can think of a hypothesis test as a method to weigh evidence against HO\n(rather than a decision procedure between HO and Hi).\n\nUsually HO is chosen to represent a hypothesis of \"no difference\" between the\nnew and existing methods, just chance alone. If we can reject this sort of\nhypothesis then we have a statistically significant proof of claim Hi. In this\ncase, the hypothesis test is a significance test.\n\nAgain, the hypothesis test is not a decision procedure between HO and Hi. If Hi\nis not accepted, it does not mean we can accept HO. I never want to hear you say:\n\"therefore we accept the null hypothesis\".\n\nA type I error is made when a test rejects H0 in favor of H1 when H0 is actually\ntrue (false positive).\nE.g. person does not have the disease but test is positive.\nA type II error is made when the test fails to reject H0 when H1 is true. (false\nnegative).\nE.g. person has the disease but test is negative.\nDecision\nDo not re'ect HO\nRe'ect HO\nHO is true\nCorrect Decision\nType I Error\nHO is false\nType II Error\nCorrect Decision\nThe probability of a type I error is called the a-risk.\nThe probability of a type II error is called the (-risk.\nThat is:\na = P(type I error) = P(reject H0 I H0)\n( = P(type II error) = P(fail to reject H0 I H1).\nType I error is sometimes more serious than Type II, e.g., Type I is like convicting\nan innocent person and Type II is letting a guilty person go free due to lack of\nevidence.\nHere we create a test that is required to satisfy P(type I error) 5 a. In this case, a\nis called the level of significance, and the test is a a-level test.\nUsually we choose a = 0.05 (or 0.10 or 0.01). This says that most of the time, we\naccept less than 50 probability of Type I error.\nE.g., if we are checking whether: P(type I error) = P(test rejects H0 I H0) 5 0.10,\nthen the test is a 0.10-level test.\nThe p-value is the probability of observing a sample statistic as extreme or more\nextreme than the one observed under the assumption that the null hypothesis is\ntrue. It is the \"observed level of significance.\"\nWhen testing a hypothesis, state a. Calculate the p-value and if the p-value 5 a,\nthen reject HO. Otherwise do not reject HO.\n\nNote: in order to compute the a-risk, you need HO and a decision rule (such as\n\"reject if X>a\"). For computing (-risk, you need H1 and a decision rule.\nExample: Let's say we reject a shipment from a vendor if we find more than 1\ndefective item in a shipment of 1OO. (That's our decision rule for determining\nwhether the probability of error is 10). The number of defective items in a\nshipment obeys the binomial distribution. If the true probability of error really is\n10, then:\na = P(type I error) = P(reject HO I HO)\n= P(2 or more defective I p=O.O1) = O.264 .\n(We can compute this using the first 2 terms of the binomial distribution.)\nDefine J = 1-( to be the power of the test,\nJ = 1-( = P(reject HO I H1).\nThe higher the power, the better the test. The power is useful for assessing\nwhether the test has sufficiently high probability to reject HO when H1 is true.\nMisuse of Hypothesis Tests\n- Data in practice are not always a random sample from a distribution.\n- It is possible to have a highly statistically significant result that is practically\ninsignificant\n-\nE.g. an SAT coaching program that has an average improvement of 15.5\npoints with p-value < O.OO1, but the average retest gain is already 15\npoints.\n- \"If you torture data long enough it will confess\" - testing so many hypotheses\nthat you will likely find at least one significant result - and then you only report\nthat one. This is commonly done yet highly incorrect! There are methods (e.g.,\nBonferroni) that lower the acceptable significance levels when there are multiple\ntests.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/00da8087b863814d7d150a63912fa9da_MIT15_075JF11_chpt07.pdf",
      "content": "Chapter 7 Notes - Inference for Single Samples\n- You know already for a large sample, you can invoke the CLT so:\nX ∼ N(μ, σ2).\nAlso for a large sample, you can replace an unknown σ by s.\n- You know how to do a hypothesis test for the mean, either:\n- calculate z-statistic\nx - μ0\nz =\n√\nσ/ n\nand compare it with zα or zα/2.\n- calculate pvalue and compare with α or α/2.\n- calculate CI and see whether μ0 is within it.\nLet's add two more calculations.\n1) Determine n to achieve a certain width for a 2-sided confidence interval. Of course, small\nwidth → large n.\nDerivation of Sample Size Calculation for CI\n\nzα/2σ\nn =\n(Sample Size Calculation)\nE\nwhere E is the half-width of the CI.\nExample\n2) Power Calculation\n- For upper 1-sided z-tests:\nH0 : μ ≤ μ0\nH1 : μ > μ0, in fact, we'll take μ = μ1.\nThe calculation only makes sense if μ1 > μ0. We want to know what the power of the\ntest is to detect mean μ1. We'll compute power as a function of μ1.\nDerivation of Power Calculation for Upper 1-sided z-tests\nμ1 - μ0\nπ(μ1) = P (test rejects H0 in favor of H1|H1) = Φ -zα +\n√\n.\nσ/ n\n\nNow we can consider π(μ1) as a function of μ1. Again, the alternative hypothesis only\nmake sense if μ1 > μ0. As μ1 increases, what happens to π(μ1)?\n- For lower 1-sided tests,\nμ0 - μ1\nπ(μ1) = Φ -zα +\n√\n.\nσ/ n\nThe alternative hypothesis only makes sense when μ1 < μ0. As μ1 increases (and gets\ncloser to a μ0), what happens to π(μ1)?\n- For 2-sided tests,\nμ = μ1\n\nX < μ0 - zα/2\nμ = μ1\nX > μ0 + zα/2\nμ0 - μ1\nμ1 - μ0\n=\nΦ -zα/2 +\n√\n+ Φ -zα/2 +\n√\nσ/ n\nσ/ n\nAs μ1 changes, what happens to π(μ1)?\nσ\nσ\n\nπ(μ1)\n= P\n√\n+ P\n√\nn\nn\n\n3) Sample size calculation for power. Want to find the n required to guarantee a certain\npower, 1 - β, for an α-level z-test.\nLet δ := μ1 - μ0 so that μ1 = μ0 + δ.\n- For upper 1-sided, we have (look up at the power calculation we did for upper 1-sided):\nδ\nπ(μ1) = π(μ0 + δ) = Φ -zα +\n√\n= 1 - β.\nσ/ n\nSince our notation says that zβ is defined as the number where Φ(zβ ) = 1 - β:\nδ\n-zα +\n√ = zβ .\nσ/ n\nNow solve that for n:\n\n(zα + zβ)σ\nn =\n.\nδ\n- For lower 1-sided, n is the same by symmetry.\n- For 2-sided, turns out one of the two terms of π(μ1) can be ignored to get an approxi\nmation:\n\n(zα/2 + zβ)σ\nn ≈\n.\nδ\nRemember to round up to the next integer when doing sample-size calculations!\nExample\n\n7.2 Inferences on Small Samples\nIf n < 30, we often need to use the t-distribution rather than z-distribution N(0, 1) since s\ndoesn't approximate σ very well. Need X1, . . . , Xn ∼ N(μ, σ2).\nThe bottom line is that we replace:\nX - μ\nX - μ\nZ =\n√\nby\nT =\n√\nσ/ n\nS/ n\nfor a t-test on the mean. Replace zα by tn-1,α. Replace σ by S. There's a chart in your\nbook on page 253 that summarizes this.\nNote that the power calculation is harder for t-tests, so for this class, just say S ≈ σ and\nuse the normal distribution power calculation. You'll get an approximation.\nExample\n7.3 Inferences on Variances\nAssume X1, . . . , Xn ∼ N(μ, σ2). Inferences on variance are very sensitive to this assumption,\nso inference only with caution!\nThe bottom line is that we replace:\nX - μ\n(n - 1)S2\nZ =\n√\nby\nχ2 =\nσ2\n(and test for σ2 not μ). Replace zα by χ2\nand/or χ2\nσ/ n\nn-1,1-α\nn-1,α.\nHypothesis tests on variance are not quite the same as on the mean. Let's do some of the\ncomputations to show you. First, we'll compute the CI.\n\n2-sided CI for σ2 . As usual, start with what we know:\n(n-1)S2\n(n - 1)S2\n1 - α = P χ2\n≤\n≤\nχ2\nand remember χ2 =\n,\nn-1,1-α/2\nσ2\nn-1,α/2\nσ2\n⇑\n⇑\n(*1)\n(*2)\nand we want:\n1 - α = P (L ≤ σ2 ≤ U) for some L and U.\nLet's solve it on the left for (*1)\nand on the right for (*2):\n(n - 1)S2\n(n - 1)S2\nσ2 ≤\n≤ σ2\nχ2\nχ2\nn-1,1-α/2\nn-1,α/2\nPutting it together we have:\n1 - α =\nP\n\n(n - 1)S2\nχ2\nn-1,α/2\n≤\nσ2 ≤\n(n - 1)S2\nχ2\nn-1,1-α/2\n\n1 - α =\nP\nL ≤\nσ2 ≤\nU .\nThe 100(1 - α)% confidence interval for σ2 is then\n(n - 1)s2\n(n - 1)s2\n≤ σ2 ≤\n.\nχ2\nχ2\nn-1,α/2\nn-1,1-α/2\nSimilarly, 1-sided CI's for σ2 are:\n(n - 1)s2\n(n - 1)s2\n≤ σ2\nand\nσ2 ≤\n.\nχ2\nχ2\nn-1,α\nn-1,1-α\nHypothesis tests on Variance (a chi-square test)\n= σ2\n= σ2\nTo test H0 : σ2\n0 vs H1 : σ2\n0 , we can either:\n- Compute χ2 statistic:\n(n - 1)s2\nχ2 =\nσ2\nand reject H0 when either χ2 > χ2\nor χ2 < χ2\nn-1,α/2\nn-1,1-α/2.\n- Compute pvalue:\nFirst we calculate the probability to be as extreme in either direction:\n\nPU = P (χ2\n≥ χ2)\nor\nPL = P (χ2\n≤ χ2)\nn-1\nn-1\ndepending on which is smaller (more extreme). The probability to obtain a χ2 at least\nas extreme under H0 is:\n2 min(PU , PL).\nThis accounts for being extreme in either direction.\n- Compute CI (already done)\nTable 7.6 on page 257 summarizes the chi-square hypothesis test on variance.\nNote that this is not the most commonly used chi-square test!\nSee Wikipedia: A chi-square test is any statistical hypothesis test in which the sampling\ndistribution of the test statistic is a chi-square distribution when the null hypothesis is true...\n(n-1)S2\n(In this case, we have normal random variables, so the distribution of the test statistic\nσ2\nis chi-square.)\nExample\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/bc685a8800af30a10b3dc72433bbf326_MIT15_075JF11_chpt08.pdf",
      "content": "Chapter 8 : Inferences for Two Samples\nIn previous chapters, we had only one sample and we wanted to see whether\nits mean or variance might be above or below a certain value. In Chapter\n8 we compare statistics from 2 populations, and we want to know whether\none mean is larger than another, whether the means are different, etc. The\ntechniques of this chapter are very useful for comparative studies.\nIndependent Samples Design:\nThere are a few different ways we can do an experiment. In an independent\nsamples design, we have an independent sample from each population. The\ndata from the two groups are independent.\nSample 1:\nx1, . . . xn1\nSample 2:\ny1, . . . yn2\nHere n1 does not need to equal n2, that is, the samples can be different sizes.\nThe xi's and yi's are all statistically independent. The difference is that the\nyi's receive the treatment and the xi's do not. For example, the xi's and\nyi's are student grades. The first group is the control group, and the second\ngroup was taught by a different method.\nNote: How might you compare 2 independent samples graphically? Let's say\nyou wanted to find out if one sample had generally larger values than the\nother for instance?\nMatched Pairs Design:\nIn the matched pairs design, the observations from each sample are paired.\nAn example is that the xi's are the student scores before a training program,\nand the yi's are the scores of the same students after the training program.\npair: 1, 2, . . . , n\nSample 1: x1, x2, . . . , xn\nSample 2: y1, y2, . . . , yn\nHere the ith observation in the first group is similar in some way to the ith\nobservation in the second group. The way in which they are similar is called\n\nthe blocking factor.\nNote: How might you consider matched pairs graphically?\n8.3 Comparing means of 2 populations\nWe will test whether the mean of one of the populations is different than\nthe other by a difference of δ0.\n1) Independent Samples Design for Large Samples (n1, n2 > 30):\nSample 1: x1, . . . , xn1 from a population with unknown μ1 and σ1\n2 with sample\nmean x and sample variance s1.\nSample 2: y1, . . . , yn2 from a population with unknown μ2 and σ2\n2 with sample\nmean y and sample variance s2.\nWe are testing:\nH0 : μ1 - μ2 = δ0\nH1 :\n\nμ1 - μ2 = δ0\nSo, if we want to test whether or not the means are different, we set δ0 = 0.\nThe main idea is that since n1 and n2 are both large, we are going to use the\n\ncentral limit theorem to say that the distribution of X - Y is approximately\nnormal. We can calculate:\nE(X - Y ) = μ1 - μ2\nVar( X - Y ) = Var(\nY ) + 2Cov(\nY )\nX) + Var(-\nX, -\n= Var( X ) + (-1)2Var( Y ) + 0\nσ2\nσ2\n=\n+\n.\nn1\nn2\nFrom the CLT we know that Z is approximately N(0, 1) where:\nX - Y - (μ1 - μ2)\nZ =\n.\nσ1\n2/n1 + σ2/n2\n\nThis means we can do an α-level z-test for comparing the means using the\ntest statistic:\nx - y - δ0\nz =\n,\ns /n1 + s /n2\nwhere remember that for large samples s1 ≈ σ1 and s2 ≈ σ2.\nTo summarize, the statistic above is for conducting a 2-sample indepen\ndent samples design z-test where both samples are large, and the goal is to\ncompare the means. That's the basic idea, and the front page of your book\nhas the summary written out for you to carry out the test. Basically if z is\ntoo big or too small, you'll reject H0.\nExample 1\n2) Independent Samples Design for Small Samples (n1, n2 ≤ 30):\nWe could create a z-test using rv Z if the populations are normal and if\nwe know the population variances. But in most cases, we don't know this.\nIn that case, we can't use the z-test since we have no variances and we also\ncan't use the CLT to claim that Z defined above is approximately N(0, 1).\nWe'll have to assume the populations are normal and use the t-test.\nSample 1: x1, . . . , xn1 ∼ N(μ1, σ1\n2) where μ1 and σ2 are unknown.\nSample 2: y1, . . . , yn2 ∼ N(μ2, σ2\n2) where μ2 and σ2 are unknown.\nCase 2a: σ2 = σ2\n2 . (You have to know this somehow ahead of time to\nuse this test. Or you can check this assumption using an F-test that I'll show\nyou in Chapter 8.4. It is also assumed that you don't necessarily know σ1 or\nσ2 in advance.)\nLet σ2 := σ2 = σ2\n2 . We are testing:\nH0 : μ1 - μ2 = δ0\nH1 : μ1 - μ2 = δ0\nI need to do some calculations to derive the test statistic. We need to know\nthat:\nE(X - Y ) = μ1 - μ2.\np\n\nIt's going to be a kind of t-test, so I'll need an estimator for the variance.\nTo estimate the variance we could use either s or s but instead we use a\ncombination so we get a better estimator:\n\n(xi - x )2 +\n(yi - y )2\ns 2 =\ni\ni\n= \"pooled variance.\"\n(n1 - 1) + (n2 - 1)\nIt turns out (with some work required) that the following rv T has a t-\ndistribution with d.f. (n1 - 1) + (n2 - 1) which equals n1 + n2 - 2:\nX - Y - (μ1 - μ0)\nT =\n\n.\ns\n1 + 1\nn1\nn2\nWe can use the test statistic:\nx - y - δ0\nt =\ns\n1 + 1\nn1\nn2\n(where s is the square root of the pooled variance above) for a 2 sample t-test\nto compare the means for an independent samples design experiment where\nthe samples have equal variance, d.f. n1 + n2 - 2.\nExample 2\nCase 2b: σ2 = σ2 and you don't know either of them. In this case, it is\ntempting to use the distribution of:\nX - Y - (μ1 - μ2)\nT =\n\n,\ns\ns\n1 +\nn1\nn2\nbut T does not have a t-distribution. However, its distribution can be ap\nproximated by the t-distribution with d.f. ν where ν is computed according\nto the \"Welch-Satterthwaite method:\n\n2 2\ns\ns\n1 +\nn1\nn2\nν = -\n-2\n-\n-2 ,\ns\ns\nn1\nn2\n+\nn1-1\nn2-1\n\nwhere fractions are truncated to the nearest integer.\nSo to test:\nH0\nH1\n:\n:\nμ1 - μ2 = δ0\nμ1 - μ2 = δ0,\ncompute test statistic:\nx - y - δ0\nt =\ns\ns\nn1\nn2\nand comparing to tν,α/2 (2-sided) or tν,α (1-sided) gives the approximate solu\ntion, using ν computed according to the Welch-S method. We can certainly\nalso compute pvalues and confidence intervals, which are provided in the ta\nble in the front of the book.\nThe Welch-S method really makes a difference when:\n1. s1 and s2 are very different\n2. n1 and n2 are very different.\n3) Matched Pairs Design:\nGiven n pairs:\npair: 1, 2, . . . , n\nSample 1: x1, x2, . . . , xn\nSample 2: y1, y2, . . . , yn\nAssume Xi ∼ N(μ1, σ1\n2) and Yi ∼ N(μ2, σ2\n2) but Xi and Yi are not indepen\ndent, they are correlated. The pairs themselves are mutually independent\n(e.g. patients' temp before taking tylenol, patients' temp after tylenol). Let\nρ := corr(Xi, Yi) (it's the same for all i).\nDefine Di = Xi - Yi. It turns out that the Di's are independent normal\nrv's with:\nμD = E(Di) = E(Xi - Yi)\nσD = Var(Di) = Var(Xi - Yi) = Var(Xi) + Var(-Yi) - 2Cov(Xi, Yi)\n= σ1\n2 + (-1)2σ2\n2 - 2ρσ1σ2.\n+\nq\n\n-\n-\n-\n-\nh\ni\nSince ρ > 0 when the pairs are matched, the variance we computed is smaller\nthan that of the independent samples case.\nNow we can actually reduce the whole thing to the single sample setting.\nLet di = xi - yi. To test:\nH0 : μD = δ0\nH1 : μD = δ0,\n\nWe assume D1, . . . , Dn ∼ N(μD, σ2 ). We calculated d =\ndi and we also\nD\nn\ni\ncalculated sd =\n(di - d )2 . The test statistic is just:\nn-1\ni\nx - y - δ0\nt =\n√\n,\nsd/\nn\nand we perform a t-test (this is called a \"paired\" t-test).\n- Reject H0 when |t| > tn-1,α2\n- Reject H0 when pvalue= 2P (Tn-1 ≥|t|) < α\n\n√sd\n√sd\n- Reject H0 when δ /∈ CI, where CI is δ ∈ d - tn-1,α/2\n, d + tn-1,α/2\n.\nn\nn\nWe can adapt the power and sample size determinations from Chapter 7 even\nthough the variables aren't normal to get approximate values. Pinning down\nH1 : μD = δ1,\nδ1\nδ1\nπ(δ1) ≈ Φ -zα/2 +\n√\n+ Φ -zα/2 -\n√\n.\nσD/\nn\nσD/\nn\nThe following sample size calculation gives the sample size needed for a\nmatched pairs test with α-risk α and power 1 - β to detect a difference\nin means of δ1:\n\n(zα/2 + zβ)σD\nn =\nδ1\n(you can replace σD by sd if the sample size is large enough in both of these\nformulas).\nP\nq\nP\n\n8.4 Comparing the Variances of 2 Populations:\nThe F-test for independent samples design (heavily requires normality)\ncompares the variance of two populations where we have samples:\nSample 1: x1, . . . , xn1 ∼ N(μ1, σ1\n2)\nSample 2: y1, . . . , yn2 ∼ N(μ2, σ2\n2)\nTo compare the population variances, we consider σ1\n2/σ2\n2 , estimated by\ns1/s2\n2. We learned in Chapter 5 that the rv below has an F-distribution with\nd.f.'s n1 - 1 and n2 - 1:\nS1\n2/σ2\nF =\n.\nS2/σ2\nSo if we want to test:\nσ2 = σ2\nH0 :\nσ2\n:\n= σ2\nH1\nwe compute the test statistic\ns1\nF =\ns2\nand since the upper and lower α/2 critical points of the F-distribution are\nfn1-1,n2-1,1-α/2 and fn1-1,n2-1,α/2, then we:\n- reject H0 when F < fn1-1,n2-1,1-α/2 or F > fn1-1,n2-1,α/2.\n- reject H0 when P (F < fn1-1,n2-1,1-α/2) < α/2 or P (F > fn1-1,n2-1,α/2) >\nα/2.\n- reject H0 when F /∈ CI.\nLet's derive the CI:\ns1/σ2\nfn1-1,n2-1,1-α/2 ≤\n≤ fn1-1,n2-1,α/2.\ns /σ2\nWe need to solve for σ1\n2/σ2\n2 . Just rewriting to make the notation simpler,\ns1/σ2\nf-≤\n≤ f+.\ns /σ2\n\nLet's do the left equation first. Solving for the ratio σ2 ,\nσ2\nσ\ns\nf-≤ s 2\nσ\nσ\ns\n≤\n.\nσ\nf-\ns\nThen for the right equation, we'll have:\nσ\ns\n≥\n.\nσ\nf+\ns\nPutting it together:\nSo we'll reject H0 when:\nσ\ns\ns\n≤\n≤\n.\nf+\nσ\nf-\ns\ns\ns\ns\n1 ∈/\n,\n.\nfn1-1,n2-1,α/2\nfn1-1,n2-1,1-α/2\ns\ns\n(The 1-sided tests can be derived similarly).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT15_075JF11_chpt09a.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-075j-statistical-thinking-and-data-analysis-fall-2011/0fa15fa0462a921af5d493f45a3f0809_MIT15_075JF11_chpt09a.pdf",
      "content": "Chapter 9 Notes, Part 1 - Inference for Proportion and Count Data\nWe want to estimate the proportion p of a population that have a specific attribute, like\n\"what percent of houses in Cambridge have a mouse in the house?\"\nWe are given X1, . . . , Xp where Xi's are Bernoulli, and P (Xi = 1) = p.\nXi is 1 if house i has a mouse.\n\nLet Y =\ni Xi so Y ∼ Bin(n, p).\nAn estimator for p is:\n\nY\npˆ =\n=\nXi.\nn\nn\ni\npˆ is a random variable. For large n (rule of thumb, npˆ ≥ 10, n(1 - pˆ) ≥ 10) the CLT says\nthat approximately:\n\npq\npˆ ∼ N\np,\nwhere q = 1 - p.\nn\nQuestions: What's up with that rule of thumb? Where did the pq/n come from?\nConfidence Intervals\nThe CI can be computed in 2 ways (here for 2-sided case):\n- CI first try:\n\npˆ- p\nP\n-zα/2 ≤ p\n≤ zα/2\n= 1 - α.\npq/n\nWe could solve it for p but the expression is quite large...\n- CI second try:\n\npˆ- p\nP\n-zα/2 ≤ p\n≤ zα/2\n≈ 1 - α\npˆˆq/n\nwhich yields\n\npˆqˆ\npˆqˆ\npˆ- zα/2\n≤ p ≤ pˆ+ zα/2\n.\nn\nn\nSo that's the approximate CI for p.\nSample size calculation for CI\n\nWant a CI of width 2E:\npˆ- E ≤ p ≤ pˆ+ E\nso\npˆqˆ\nzα/2\nE = zα/2\nwhich means n =\npˆq.\nˆ\nn\nE\nQuestion: We'll take ˆpqˆ to be its largest possible value, (1/2) × (1/2). Why do we do this?\nWhy don't we just use the ˆp and ˆq that we measure from the data?\nSo, we need:\n2 1\nzα/2\nn =\nobservations.\nE\nHypothesis testing on proportion for large n\nH0 : p = p0\nH1 : p =\np0.\n\nLarge n and H0 imply ˆp ≈ N p0, p0\nn\nq0\n(where q0 = 1 - p0) so we use z-test with test\nstatistic:\npˆ- p0\nz = p\np0q0/n\nExample\nFor small n one can use the binomial distribution to compute probabilities directly (rather\nthan approximating by normal). (Not covered here.)\nChapter 9.2 Comparing 2 proportions\nExample: The Salk polio vaccine trial: compare rate of polio in control and treatment (vac\ncinated) group. Is this independent samples design or matched pairs?\nSample 1: number of successes X ∼ Bin(n1, p1), observe X = x.\nSample 2: number of successes Y ∼ Bin(n2, p2), observe Y = y.\nWe could compare rates in several ways:\np1 - p2\n→ we'll use this one\np1/p2\n\"relative risk\"\np1\n1-p1 /\np2\n1-p2\n\"odds ratio\"\nr\n\nFor large samples, we'll use the CLT:\npˆ1 - pˆ2 - (p1 - p2)\nZ =\n-\n≈ N(0, 1)\npˆ1qˆ1\npˆ2qˆ2\n+\nn1\nn2\nwhere ˆp1 = X/n1 and ˆp2 = Y/n2.\nTo test\nH0 : p1 - p2 = δ0,\nH1 : p1 - p2 = δ0,\nwe can just compute zscores, pvalues, and CI. The test statistic is:\npˆ1 - pˆ2 - δ0\nz = -\n.\npˆ1qˆ1\npˆ2qˆ2\n+\nn1\nn2\nThis is a little weird because it really should have terms like \"p1,0q1,0/n1\" in the denomi\nnator, but we don't have those values under the null hypothesis. So we get an approximation\nby using ˆp1 and ˆq1 in the denominator.\nExample\nFor an independent samples design with small samples, use Fisher's Exact Test which uses\nthe Hypergeometric distribution. For a matched pairs design, use McNemar's Test which\nuses the binomial distribution (Both beyond the scope.)\nTwo Challenges\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.075J / ESD.07J Statistical Thinking and Data Analysis\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}