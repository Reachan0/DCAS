{
  "course_name": "Topics in Linguistic Theory: Laboratory Phonology",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Electrical Engineering",
    "Signal Processing",
    "Humanities",
    "Linguistics",
    "Phonology",
    "Engineering",
    "Electrical Engineering",
    "Signal Processing",
    "Humanities",
    "Linguistics",
    "Phonology"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1 session / week, 3 hours / session\n\nCourse Description\n\nThe goal of this course is to prepare you to engage in experimental investigations of questions related to linguistic theory, focusing on phonetics and phonology. The course will be organized around four main topics (subject to revision):\n\nVowel inventories\n\nSpeech perception and the distribution of phonological contrasts\n\nIntonation and the marking of focus\n\nThe effects of predictability on speech production\n\nIn the process of investigating these topics, we will cover some phonological theory, the basics of speech acoustics, acoustic analysis, speech perception, and experimental design. Students will develop and execute their own experimental projects during the course.\n\nPrerequisites\n\n4 subjects in linguistics\n\nGrading and Requirements\n\nVowel Inventories\n\nCross-linguistic generalizations about the nature of inventories of vowel contrasts. How can we explain these generalizations? Lindblom's Theory of Adaptive Dispersion.\n\nSpeech Perception and the Distribution of Phonological Contrasts\n\nInvestigating the hypothesis that phonological contrasts preferentially appear in contexts where there are better perceptual cues to those contrasts (Steriade's 'licensing by cue').\n\nIntonation and the Marking of Focus\n\nOne of the ways in which intonation affects meaning is via its role in marking focused constituents. How does this marking work?\n\nThe Effects of Predictability on Speech Production\n\nA number of non-phonological factors have been shown to affect the phonetic realization of words, e.g. word frequency, lexical neighborhood density and contextual predictability. What is the nature of these effects? How do they interact with each other? Are they a consequence of 'listener-oriented' behavior?\n\nEssential Background\n\nPhonetic Theory\n\nOverview of 'the speech chain':\n\nArticulatory phonetics,\n\nBasic acoustics, waveforms and spectrograms,\n\nAudition and perception.\n\nArticulatory-acoustic relations\n\nThe acoustic theory of speech production (Fant 1960, etc).\n\nContextual variation in segment realization.\n\nSpeech perception\n\nThe problems of speech perception and lexical access.\n\nPerceptual cues to contrasts.\n\nIntonation and phrasing\n\nToBI system for transcribing English intonation.\n\nExperimental Phonetics\n\nExperimental design and elementary statistics\n\nDigital Signal Processing\n\nSampling theory\n\nFFT, LPC, spectrograms, pitch tracking\n\nUsing PRAAT speech analysis software\n\nRequirements\n\nA. Readings and class discussions\n\nB. Assignments - approximately one per week\n\nC. Final project\n\nPropose an experimental test of a hypothesis (by Ses #5)\n\nRun a pilot experiment\n\nPresent your study in class (in Ses #12)\n\nWrite up the project (draft due in Ses #12, final version due 10 days after Ses #12)\n\nThis is a 'communication intensive' course, so written work and presentations will be important.\n\nA draft of the final project will have to be submitted in time for you to revise it in light of my comments.\n\nGrading\n\nREQUIREMENTS\n\nPERCENTAGES\n\nClass participation\n\n10%\n\nAssignments\n\n15%\n\nFinal project presentation\n\n15%\n\nFinal project paper\n\n60%\n\nTextbook\n\nJohnson, Keith.\nAcoustic and Auditory Phonetics\n. 2nd ed. Malden, MA: Blackwell, 2003. ISBN: 9781405101233.\n\nPlease see\nreadings\nfor additional texts.\n\nCalendar\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nIntroduction\n\nLaboratory phonology\n\nBasic audition\n\nDigital signal processing\n\nSource-filter theory\n\nAcoustics of vowels\n\nAdaptive dispersion\n\nSpectral analysis\n\nLicensing by cue\n\nProposal of an experimental test of a hypothesis due\n\nLicensing by cue (cont.)\n\nIntonation\n\nThe meaning of intonation\n\nBasic statistics\n\nEffects of the lexicon and context on speech perception\n\nEffects of the lexicon and context on speech perception (cont.)\n\nPhonetics and phonology of accent variation\n\nStudent presentations\n\nDraft of final project due\n\nFinal version due 10 days after Ses #12",
  "files": [
    {
      "category": "Resource",
      "title": "Aspiration contrasts in Mandarin Chinese",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/9ab8411eb08ab8e31653de2ab21e5caa_mandarin.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nAspiration contrasts in Mandarin Chinese\nI have recorded a speaker of Taiwanese Mandarin reading a list of words illustrating the\naspiration contrasts in this language in a variety of vowel contexts (full list below).\nMandarin is usually described as contrasting the following aspirated and unsapirated\nplosives:\nstop\naffricate\nbilabial\np ph\nalveolar\nt th\nts tsh\nretroflex\nʈʂ ʈʂh\nalveolo\npalatal\ntɕ tɕh\nvelar\nk kh\nThe assignment has two parts:\n1. For the stops: Measure Voice Onset Time (VOT). Send the results to me in an Excel\nspreadsheet (or a text file with one word on each line and the word separated from the\nVOT measurement by a tab).\n2. For the affricates: compare the aspirated and unaspirated affricates using waveforms\nand spectrograms and try to identify acoustic properties that differentiate the two classes.\nIs the contrast among affricates parallel to the contrast between aspirated and unaspirated\nstops (as implied by standard transcriptions of these sounds)?\n- Does voicing play any role in either contrast?\n- Is it possible to identify an interval of aspiration following the 'aspirated' affricates.\n(Related question: Can we measure VOT of affricates? If so, how do we identify\noffset of frication - i.e. where do we start measuring VOT?)\nTips:\nA. It can be difficult to identify periodic voicing in the waveform of a fricative because it\ncan be obscured by the frication noise. Voicing in a fricative is generally more visible in\nthe spectrogram where any voicing striations will be most prominent at low frequencies,\nwhereas the frication noise is predominantly at high frequencies.\nB. If there is any aspiration following an affricate, it should be similar to aspiration\nfollowing a stop, so you can use the spectrograms of the aspirated stops as guides to what\nyou should expect the spectral properties of aspiration noise to look like. The spectrum of\naspiration depends on the following vowel, so compare words with the same vowel (e.g.\nexamine [tha] to get some idea of what aspiration might look like in [tsha].\nC. Some of the differences between frication and aspiration may be more obvious above\n5000 Hz. By default, Praat plots spectrograms from 0-5000 Hz, so you will need to\nmodify the spectrogram settings to see anything above 5000 Hz: In the edit window, go\nto the Spectrum menu. Select 'Spectrogram settings...'. Change the upper limit of the\nview range to 8000 Hz.\nCome to class prepared to discuss your observations.\n\nRecordings:\nThe words were recorded in the following carrier phrase:\nwo ʂwo\nkei\nni thin\nI say\ngive to you hear\nThe following is the full list of words, in the order in which they were recorded:\npa\npi\npu\npha\nphi\nphu\ntha\nta\nku\nkhu\ntsa\nʈʂa\ntɕa\ntsha\nʈʂha\ntɕha\ntsu\nʈʂu\ntɕi\nʈʂhu\ntshu\ntɕhi"
    },
    {
      "category": "Resource",
      "title": "data",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/015c0f9b01afd3014943bf6d057fcae5_data.pdf",
      "content": "v\nv\nb\np\nf\nf\np\np\nt\nt\nt\nt\nz\nz\nk\nk\ng\ng\ng\ng\nk\nk\ns\ns\np\np\nv\nv\nf\nf\nv\nv\nk\nk\nb\nb\ng\ng\nd\nd\nf\nf\nf\nf\nf\nf\nv\nv\nb\nb\np\np\nf\nf\ns\nz\ng\ng\nv\nv\ns\ns\np\np\nk\nk\nd\nd\np\np\nv\nv\nb\nb\nb\nb\nb\nb\ns\nz\nv\nv\ntotal\ntota\nnumber\nsound file\nstimulus\nmanner\nsubj1\nsubj2\nsubj3\nsubj4\nsubj5\nsubj6\nsubj7\nsubj8\nsubj9\nsubj10 subj11 subj12 voiced voic\n1 av1+fa4\navfa\nfricative\nv\nv\nv\nf\nf\nv\nv\nv\nv\nv\n2 ab1+pa4\nabpa\nstop\nb\nb\nb\nb\nb\nb\nb\nb\nb\np\n3 afva\nafva\nfricative\nf\nf\nf\nf\nf\nf\nf\nf\nf\nf\n4 abpa\nabpa\nstop\np\np\np\np\np\np\np\np\np\np\n5 Ann-ata\nata\nstop\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt\n6 Anna-adta\nadta\nstop\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt\n7 DP[aza]\naza\nfricative\nz\nz\nz\nz\nz\nz\nz\nz\nz\nz\n8 DP[agka]\nagka\nstop\nk\nk\nk\nk\nk\nk\nk\nk\nk\nk\n9 DP[aga]\naga\nstop\ng\ng\ng\ng\ng\ng\ng\ng\ng\ng\n10 Katelyn-akga akga\nstop\ng\ng\ng\ng\ng\ng\ng\ng\ng\ng\n11 Katelyn-aka\naka\nstop\nk\nk\nk\nk\nk\nk\nk\nk\nk\nk\n12 DP[asa]\nasa\nfricative\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\n13 ap1+pa4\napa\nstop\np\np\np\np\np\np\np\np\np\np\n14 Katelyn-ava\nava\nfricative\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\n15 Anna-athavoi adhtha\nfricative\nth-voiced\nth-voiced\nth-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic\n16 Anna-athavoi atha\nfricative\nth-voiced\nth-voiceless\nth-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic\n17 af3+fa4\nafa\nfricative\nf\nf\nf\nv\nf\nf\nf\nf\nf\nf\n18 Anna-athavoi adha\nfricative\nth-voiced\nth-voiced\nth-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic\n19 av1+va3\nava\nfricative\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\n20 DP[ashzha]\nashzha\nfricative\nsh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nsh\nzh\nzh\n21 DP[aka]\naka\nstop\nk\nk\ng\nk\nk\nk\nk\nk\nk\nk\n22 abba\naba\nstop\nb\nb\nb\nb\np\nb\nb\nb\np\nb\n23 Katelyn-aga\naga\nstop\ng\ng\ng\ng\ng\ng\ng\ng\ng\ng\n24 Anna-ada.aiff ada\nstop\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\n25 Katelyn-afva afva\nfricative\nf\nf\nf\nf\nf\nf\nf\nf\nf\nf\n26 af3+va3\nafva\nfricative\nf\nf\nf\nf\nf\nf\nf\nf\nf\nf\n27 DP[azha]\nazha\nfricative\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\n28 affa\nafa\nfricative\nf\nf\nf\nf\nf\nf\nf\nf\nf\nf\n29 avva\nava\nfricative\nv\nv\nf\nv\nf\nv\nv\nv\nv\nv\n30 Anna-athavoi athdha\nfricative\nth-voiced\nth-voiceless\nth-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic th-voic\n31 DP[apba]\napba\nstop\nb\nb\nb\nb\nb\nb\nb\np\nb\np\n32 appa.\napa\nstop\np\np\np\np\np\np\np\np\np\np\n33 Katelyn-afa\nafa\nfricative\nf\nf\nf\nf\nf\nf\nf\nf\nf\nf\n34 DP[azsa]\nazsa\nfricative\nz\nz\nz\nz\ns\nz\ns\nz\nz\ns\n35 DP[akga]\nakga\nstop\ng\ng\ng\ng\ng\ng\ng\ng\ng\ng\n36 avfa\navfa\nfricative\nf\nf\nf\nf\nv\nv\nf\nf\nv\nv\n37 DP[azhsha]\nazhsha\nfricative\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\nzh\n38 DP[asha]\nasha\nfricative\nsh\nsh\nsh\nsh\nsh\nsh\nsh\nsh\nsh\nsh\nsh\nsh\n39 DP[asza]\nasza\nfricative\nz\ns\ns\ns\ns\ns\ns\nz\nz\ns\n40 ap1+ba4\napba\nstop\np\np\np\np\np\np\np\np\np\np\n41 Katelyn-agka agka\nstop\nk\nk\nk\nk\nk\nk\nk\nk\nk\nk\n42 Anna-atda\natda\nstop\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\n43 DP[apa]\napa\nstop\np\np\np\np\np\np\np\np\np\np\n44 Katelyn-avfa avfa\nfricative\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\n45 ab1+ba4\naba\nstop\np\np\nb\nb\nb\nb\np\np\nb\np\n46 apba\naba\nstop\nb\nb\nb\nb\np\nb\nb\nb\nb\nb\n47 DP[aba]\naba\nstop\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\n48 isi-shortened isi\nfricative\ns\nz\ns\ns\nz\nz\ns\ns\nz\nz\n49 afva-shorteneafva\nfricative\nv\nv\nv\nv\nv\nv\nv\nf\nv\nv\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nnumber\nsound file\nstimulus\nmanner\ntype\nrelease\nsubj1\nsubj2\nsubj3\nsubj4\nsubj5\nsubj6\nsubj7\nsubj8\nsubj9\nsubj10 subj11 subj\n1 av1+fa4\navfa\nfricative\nconflicting\nvoiceless\n3 afva\nafva\nfricative\nconflicting\nvoiced\n15 Anna-athavoi adhtha\nfricative\nconflicting\nvoiceless\n20 DP[ashzha]\nashzha\nfricative\nconflicting\nvoiced\n25 Katelyn-afva afva\nfricative\nconflicting\nvoiced\n26 af3+va3\nafva\nfricative\nconflicting\nvoiced\n30 Anna-athavoi athdha\nfricative\nconflicting\nvoiced\n34 DP[azsa]\nazsa\nfricative\nconflicting\nvoiceless\n36 avfa\navfa\nfricative\nconflicting\nvoiceless\n37 DP[azhsha]\nazhsha\nfricative\nconflicting\nvoiceless\n39 DP[asza]\nasza\nfricative\nconflicting\nvoiced\n44 Katelyn-avfa avfa\nfricative\nconflicting\nvoiceless\n2 ab1+pa4\nabpa\nstop\nconflicting\nvoiceless\n4 abpa\nabpa\nstop\nconflicting\nvoiceless\n6 Anna-adta\nadta\nstop\nconflicting\nvoiceless\n8 DP[agka]\nagka\nstop\nconflicting\nvoiceless\n10 Katelyn-akga akga\nstop\nconflicting\nvoiced\n31 DP[apba]\napba\nstop\nconflicting\nvoiced\n35 DP[akga]\nakga\nstop\nconflicting\nvoiced\n40 ap1+ba4\napba\nstop\nconflicting\nvoiced\n41 Katelyn-agka agka\nstop\nconflicting\nvoiceless\n42 Anna-atda\natda\nstop\nconflicting\nvoiced\n46 apba\napba\nstop\nconflicting\nvoiced\ntotal fricativ\ntotal stop\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "heavy_rain",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/78ec0a5e1858efc6242345d28b31cb49_heavy_rain.pdf",
      "content": "File type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\nxmin = 0\nxmax = 2.62\ntiers? <exists>\nsize = 4\nitem []:\nitem [1]:\nclass = \"TextTier\"\nname = \"tones\"\nxmin = 0\nxmax = 2.62\npoints: size = 0\nitem [2]:\nclass = \"IntervalTier\"\nname = \"words\"\nxmin = 0\nxmax = 2.62\nintervals: size = 7\nintervals [1]:\nxmin = 0\nxmax = 0.009995\ntext = \"<SIL>\"\nintervals [2]:\nxmin = 0.009995\nxmax = 0.297185\ntext = \"heavy\"\nintervals [3]:\nxmin = 0.297185\nxmax = 0.603885\ntext = \"rain\"\nintervals [4]:\nxmin = 0.603885\nxmax = 1.231555\ntext = \"possible\"\nintervals [5]:\nxmin = 1.231555\nxmax = 1.595305\ntext = \"high\"\nintervals [6]:\nxmin = 1.595305\nxmax = 1.944805\ntext = \"around\"\nintervals [7]:\nxmin = 1.944805\nxmax = 2.548685\ntext = \"seventy\"\nitem [3]:\nclass = \"TextTier\"\nname = \"breaks\"\nxmin = 0\nxmax = 2.62\npoints: size = 6\npoints [1]:\ntime = 0.297185\nmark = \"\"\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\npoints [2]:\ntime = 0.603885\nmark = \"\"\npoints [3]:\ntime = 1.231555\nmark = \"\"\npoints [4]:\ntime = 1.595305\nmark = \"\"\npoints [5]:\ntime = 1.944805\nmark = \"\"\npoints [6]:\ntime = 2.548685\nmark = \"\"\nitem [4]:\nclass = \"TextTier\"\nname = \"misc\"\nxmin = 0\nxmax = 2.62\npoints: size = 0\n\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "lazy",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/b8c60275e1e96847accc0266fd1ccd05_lazy.pdf",
      "content": "File type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\nxmin = 0\nxmax = 2.49\ntiers? <exists>\nsize = 4\nitem []:\nitem [1]:\nclass = \"TextTier\"\nname = \"tones\"\nxmin = 0\nxmax = 2.49\npoints: size = 0\nitem [2]:\nclass = \"IntervalTier\"\nname = \"words\"\nxmin = 0\nxmax = 2.49\nintervals: size = 7\nintervals [1]:\nxmin = 0\nxmax = 0.049022433596554206\ntext = \"<SIL>\"\nintervals [2]:\nxmin = 0.049022433596554206\nxmax = 0.30052\ntext = \"He's\"\nintervals [3]:\nxmin = 0.30052\nxmax = 0.7931\ntext = \"lazy\"\nintervals [4]:\nxmin = 0.7931\nxmax = 0.9377\ntext = \"and\"\nintervals [5]:\nxmin = 0.9377\nxmax = 1.53648\ntext = \"crazy\"\nintervals [6]:\nxmin = 1.53648\nxmax = 1.66979\ntext = \"and\"\nintervals [7]:\nxmin = 1.66979\nxmax = 2.34539\ntext = \"stupid\"\nitem [3]:\nclass = \"TextTier\"\nname = \"breaks\"\nxmin = 0\nxmax = 2.49\npoints: size = 6\npoints [1]:\ntime = 0.30052\nmark = \"\"\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\npoints [2]:\ntime = 0.7931\nmark = \"\"\npoints [3]:\ntime = 0.9377\nmark = \"\"\npoints [4]:\ntime = 1.53648\nmark = \"\"\npoints [5]:\ntime = 1.66979\nmark = \"\"\npoints [6]:\ntime = 2.34539\nmark = \"\"\nitem [4]:\nclass = \"TextTier\"\nname = \"misc\"\nxmin = 0\nxmax = 2.49\npoints: size = 0\n\nCite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "Basic Audition",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/9f543d96e9e296adbe2fe56ded00836c_lec2_audition.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n24.910\nLaboratory Phonology\nBasic Audition\n\n- No class next week (Tuesday is a Monday)\n- Readings for 2/27: Johnson chs 5 & 6\n- Assignments (due 2/27):\n- Basic acoustics.\n- VOT and laryngeal contrasts in Mandarin and\nEnglish.\n\nAudition\nAnatomyAnantomy of the outer, middle, and inner ear.\nAnantomy of the\nouter, m\niddle,\nan\nd inner e\nar.\nFigure by MIT OpenCourseWare.\n\nAudition\n- Loudness\n- Pitch\n- 'Auditory spectrograms'\n\nLoudness\n- The perceived loudness of a sound depends on the\namplitude of the pressure fluctuations in the sound\nwave.\n- Amplitude is usually measured in terms of root-\nmean-square (rms amplitude):\n- The square root of the mean of the squared amplitude\nover some time window.\n\n-\nSquare each sample in the analysis window.\n-\nCalculate the mean value of the squared waveform:\n- Sum the values of the samples and divide by the number of\nsamples.\n-\nTake the square root of the mean.\nrms amplitude\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.05\n0.1\n0.15\n0.2\ntime\npressure\npressure^2\nrms amplitude\n\nrms amplitude\nSte\nep\ns\nine-like waves. The x-axis is time in seconds.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nIntensity\n- Perceived loudness is more closely related to intensity\n(power per unit area), which is proportional to the square\nof the amplitude.\n- relative intensity in Bels = log10(x2/r2)\n- relative intensity in dB = 10 log10(x2/r2)\n= 20 log10(x/r)\n- In absolute intensity measurements, the comparison\namplitude is usually 20μPa, the lowest audible pressure\nfluctuation of a 1000 Hz tone (dB SPL).\n\nlogarithmic scales\n- log xn = n log x\n0.2\n0.4\n0.6\n0.8\n1.2\n1.4\n1.6\n1.8\nx\n\nLoudness\n- The relationship between intensity and perceived loudness\nis not exactly logarithmic.\nPressur\ne vs. Son\nes and vs\n. dB SPL\ns\nh\now\nin\ng\nso\nme\nwh\nat\nl\nog\nrit\nh\nm\ni\nc\n\nre\nla\nti\non\nsh\nip\n. Two separate\nlines\n.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nLoudness\n- Loudness also depends on frequency.\n- equal loudness contours for pure tones:\nSource: Wikimedia Commons.\n\nLoudness\n- At short durations, loudness also depends on duration.\n- Temporal integration: loudness depends on energy in the\nsignal, integrated over a time window.\n- Duration of integration is often said to be about 200ms, i.e.\nrelevant to the perceived loudness of vowels.\n\nPitch\n- Perceived pitch is approximately linear with respect to\nfrequency from 100-1000 Hz, between 1000-10,000 Hz the\nrelationship is approximately logarithmic.\nF\nr\ne\nq\nue\nnc\ny\n(k\nH\nz\n)\n\nv\ns\n.\n\nF\nre\nquency (Bark) s\nhowing logrithmic relationship.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nPitch\n-\nThe non-linear frequency response of the auditory system is related to the\nphysical structure of the basilar membrane.\n-\nbasilar membrane 'uncoiled':\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nHor\nizo\nnta\nl s\ncale\nwith\nincre\nasing\nbulb\nfrom\nleft\nto r\night.\n\nMasking - simultaneous\n- Energy at one frequency can reduce audibility of\nsimultaneous energy at another frequency (masking).\n- One sound can also mask a preceding or following sound.\nGrap\nh: F\nrequ\nency\nof\nmask\ned t\none\nvs. masking. A peak at 1\n200 Hz.\nFigure by MIT OpenCourseWare. Adapted from Stevens, Kenneth N. Acoustic Phonetics. Cambridge, MA: MIT Press, 1999. ISBN; 9780262194044.\n\nTime course of auditory nerve response\nResponse to a noise burst:\n- Strong initial response\n- Rapid adaptation (~5 ms)\n- Slow adaptation (>100ms)\n- After tone offset, firing rate\nonly gradually returns to\nspontaneous level.\nT\nwo\ngri\nds,\n\ns\nho\nwin\ng a\nsh\nade\nd peaked block in the lower left corner.\nFigure by MIT OpenCourseWare. Adapted from Kiang et al. (1965)\n\nInteractions between sequential sounds\n- A preceding sound can affect the auditory nerve response\nto a following tone (Delgutte 1980).\nL\nine\ngr\naph\ns s\nhow\ni\nng\ndis\nc\nha\nrge ra\nte, most peaks\naround 375 dB SPL.\nFigure by MIT OpenCourseWare. Adapted from Stevens, Kenneth N. Acoustic Phonetics. Cambridge, MA: MIT Press, 1999. ISBN: 9780262194044,\nafter Delgutte, B. \"Representation of Speech-like Sounds in the Discharge Patterns of Auditory-nerve Fibers.\"\nJournal of the Acoustical Society of America 68, no. 3 (1980): 843-857.\n\nAuditory 'spectrograms'\nThe auditory system performs a running frequency analysis of\nacoustic signals - cf. spectrogram.\n- A regular spectrogram analyzes frequency of equal widths,\nbut the peripheral auditory system analyzes frequency bands\nthat are wider at higher frequencies.\n- Further disparities are introduced by the non-linearities of the\nperipheral auditory system, e.g.\n- loudness is non-linearly related to intensity\n- masking(simultaneous and nonsimultaneous)\n\nNu\nmb\ner\no\nf\nER\nBs\nv\ns.\ne\nxc\nit\nat\nio\nn\nle\nve\nl\n\n(\nd\nB): Two curves pe\naking at 50 dB and 8\n0 dB, the\nn dropping off and rising again.\nImage by MIT OpenCourseWare. Adapted from Moore, Brian. The Handbook\nof Phonetic Science. Edited by William J. Hardcastle and John Laver.\nAc\no\nu\ns\nt\ni\nc\nfre\nque\nncy\nvs\n. a\nmpl\nitu\nde,\n\nw\ni\nt\nh\no\nne\nl\nin\ne\nlo\nok\nin\ng like a sharp M, the oth\ner a more curv\ned, elongated M.\nImage by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory\nPhonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\nFrequ\nency\n(Hz)\nvs. L\nevl (\ndB),\ndots,\nwi\nt\nh\na\npa\ntt\ner\nn\nma\nki\nng\na\nlarge trough\nat ~1250\ndB.\nImage by MIT OpenCourseWare. Adapted from Moore, Brian. The\nHandbook of Phonetic Science. Edited by William J. Hardcastle and John Laver.\nMalden, MA: Blackwell, 1997. ISBN: 9780631188483.\nMalden, MA: Blackwell, 1997. ISBN: 9780631188483.\n\nSpectrogram images removed due to copyright restrictions.\nFigure 3.8 in Johnson, Keith. \"Comparison of Normal Acoustic Spectrogram and Auditory Spectrogram or Cochleagram.\"\nAcoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nItalian vowels\ni\ne\n\na\n\no\nu\nF2 (Hz)\nERB scales\nF2 (E)\nE(F1)\n\n24.910\nLinguistic Phonetics\nAnalog-to-digital conversion of\nspeech signals\n2 g\nrap\nhs\nwit\nh i\ndent\nical\ncur\nves\n: Al\ntern\nativ\ne li\nttle\nand\nbig peaks, 2nd graph w\nith\nar\nea\nsha\ndin\ng.\nFigure by MIT OpenCourseWare.\n\nAnalog-to-digital conversion\n- Almost all acoustic analysis is now computer-based.\n- Sound waves are analog (or continuous) signals, but digital\ncomputers require a digital representation - i.e. a series of\nnumbers, each with a finite number of digits.\n- There are two continuous scales that must be divided into\ndiscrete steps in analog-to-digital conversion of speech: time\nand pressure (or voltage).\n- Dividing time into discrete chunks is called sampling.\n- Dividing the amplitude scale into discrete steps is called\nquantization.\n\nSampling\n- The amplitude of the analog\nsignal is sampled at regular\nintervals.\n- The sampling rate is measured\nin Hz (samples per second).\n- The higher the sampling rate,\nthe more accurate the digital\nrepresentation will be.\nT\nhre\ne l\nine\ngra\nphs\n\nsho\nwin\ng a\ncon\nsta\nn\nt h\nori\nzon\nal l\nine\nthrough the center, with fluctuations around it.\nFigure by MIT OpenCourseWare. Adapted from Ladefoged, Peter.\nL104/204 Phonetic Theory lecture notes, University of California, Los Angeles.\n\nSampling\n-\nIn order to represent a wave\ncomponent of a given frequency, it\nis necessary to sample the signal\nwith at least twice that frequency\n(the Nyquist Theorem).\n-\nThe highest frequency that can be\nrepresented at a given sampling rate\nis called the Nyquist frequency.\n-\nThe wave at right has a significant\nharmonic at 300 Hz\n- (a) sampling rate 1500 Hz\n- (b) sampling rate 600 Hz\n- (c) sampling rate 500 Hz\nT\nhre\ne l\nine\ngra\nphs\n\nsho\nwin\ng a\ncon\nsta\nn\nt h\nori\nzon\nal l\nine\nthrough the center, with fluctuations around it.\nFigure by MIT OpenCourseWare. Adapted from Ladefoged, Peter.\nL104/204 Phonetic Theory lecture notes, University of California, Los Angeles.\n\nWhat sampling rate should you use?\n- The highest frequency that (young, undamaged) ears can\nperceive is about 20 kHz, so to ensure that all audible\nfrequencies are represented we must sample at 2×20 kHz =\n40 kHz.\n- The ear is relatively insensitive to frequencies above 10\nkHz, and almost all of the information relevant to speech\nsounds is below 10 kHz, so high quality sound is still\nobtained at a sampling rate of 20 kHz.\n- There is a practical trade-off between fidelity of the signal\nand memory, but memory is getting cheaper all the time.\n\nWhat sampling rate should you use?\n- For some purposes (e.g. measuring vowel formants), a high\nsampling rate can be a liability, but it is always possible to\ndownsample before performing an analysis.\n- Audio CD uses a sampling rate of 44.1 kHz.\n- Many A-to-D systems only operate at fractions of this rate\n(22050 Hz, 11025 Hz).\n\nAliasing\n- Components if a signal which are above the Nyquist\nfrequency are misrepresented as lower frequency\ncomponents (aliasing).\n- To avoid aliasing, a signal must be filtered to eliminate\nfrequencies above the Nyquist frequency.\n- Since practical filters are not infinitely sharp, this will\nattenuate energy near to the Nyquist frequency also.\nLi\nn\ne\n\ng\nr\naph of ti\nme vs. amplitude, with a steep sine-like wave and another with a greater interval intersecting at 4 points.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith.\nAcoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nQuantization\n- The amplitude of the signal at each sampling point must be\nspecified digitally - quantization.\n- Divide the continuous amplitude scale into a finite number\nof steps. The more levels we use, the more accurately we\napproximate the analog signal.Line graph of time vs. amplitude, sine-like wave with a horizonal line dividing it.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith.\nAcoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\nLi\nne\ng\nr\na\nph of ti\nme vs. am\nplitude,\nsine-like wave with a horizonal line dividing it.\n\nQuantization\n- The number of levels is specified in terms of the number of\nbits used to encode the amplitude at each sample.\n- Using n bits we can distinguish 2n levels of amplitude.\n- e.g. 8 bits, 256 levels.\n- 16 bits, 65536 levels.\n- Now that memory is cheap, speech is almost always\ndigitized at 16 bits (the CD standard).\n\nQuantization\n- Quantizing an analog signal necessarily introduces\nquantization errors.\n- If the signal level is lower, the degradation in signal-to-\nnoise ratio introduced by quantization noise will be greater,\nso digitize recordings at as high a level as possible without\nexceeding the maximum amplitude that can be represented\n(clipping).\n- On the other hand, it is essential to avoid clipping.\nLine grap\nh\n\nof\nt\nim\ne\nvs. amplitude, sine-like wave with a horizonal line dividing it.\nFigure by MIT OpenCourseWare. Adapted from Johnson, Keith.\nAcoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.\n\nVoicing and aspiration\n- Many languages make a contrast between two sets of stops\nwith different laryngeal properties, loosely referred to as\n'voiced' and 'voiceless'.\n- The precise details of these laryngeal contrasts differ from\nlanguage to language.\n- Some broad distinctions:\n- voiced [b]: vocal fold vibration during closure\n- bal ('hair')\n- voiceless unaspirated [p]: no vibration of the vocal\nfolds, short VOT\n- pal ('take care of')\n- voiceless aspirated [ph]: no vibration of the vocal folds,\nlong VOT (high airflow after release)\n- pal ('knife blade')\nListen to all three sound files here.\n\nVoicing and aspiration\n- Voiced vs.voiceless [b vs. p]\n- Russian, French, Dutch\n- Unaspirated vs. aspirated [p vs. ph]\n- Mandarin, Cantonese\n- Voiced vs. voiceless unaspirated vs. aspirated [b vs. p vs.\nph]\n- Hindi, Thai\n- English shows contextual variation between\nvoicing and aspiration.\n\nVoice Onset Time\n- English utterance-initial stops\nVoiceless unaspirated\nVoiceless aspirated\nTime (s)\n1.1154\n1.27558\n-0.471\n0.3268\nTime (s)\n1.1154\n1.27558\nTime (s)\n4.26614\n4.42706\n-0.3718\n0.1853\nTime (s)\n4.26614\n4.42706\n22 ms\n86 ms\ndie\ntie\n\nTime (s)\n547.195\n547.485\n-0.1468\n0.2644\nTime (s)\n547.195\n547.485\nVOT, closure voicing\n- English intervocalic stops can be fully voiced\n- VOT is 0 ms in 2nd and 3rd stops\nbrigadoo(n)\n\nVOT, closure voicing\n- Hindi - three-way contrast\n-\nrecordings from Ladefoged\nhttp://www.phonetics.ucla.edu/vowels/chapter12/hindi.html\nbal\n'hair'\nTime (s)\n0.231127\n-0.4972\n0.494\nTime (s)\n0.231127\nTime (s)\n0.113588\n-0.4971\n0.3851\nTime (s)\n0.113588\nTime (s)\n0.1382\n-0.475\n0.2819\nTime (s)\n0.1382\npal\n'take care of'\nphal\n'knife blade'\nListen to all three sound files here."
    },
    {
      "category": "Resource",
      "title": "Basic Statistics",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/1896231e5fcf0e8f04c534e7805347f0_lec9_1_stats.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n24.910\nLaboratory Phonology\nBasic statistics\n\nReading:\n- Fowler & Housum 1987.\nAssignments:\n- Write up voicing perception experiment\n(due in two weeks 5/8).\n- Progress report on your project (5/1).\n- Project draft + presentation 5/15.\n\nTopics:\n- Statistics\n- The lexicon and context in speech\nperception.\n- The lexicon and context in speech\nproduction.\n- Phonology in speech perception.\n\nWriting up an experiment\nThe report on an experiment usually consists\nof four basic parts:\n1. Introduction\n2. Procedure\n3. Results\n4. Discussion\n\nWriting up an experiment\n1.\nIntroduction\n-\nOutline of the purpose of the experiment\n-\nstate hypotheses tested etc\n-\nprovide background information (possibly including\ndescriptions of relevant previous results, theoretical\nissues etc).\n2.\nProcedure - what was done and how.\n-\ninstructions for replication, e.g.\n-\nExperimental materials\n-\nSubjects\n-\nRecording procedure\n-\nMeasurement procedures (especially measurement\ncriteria).\n\nWriting up an experiment\n3.\nResults\n-\nPresentation of results, including descriptive statistics\n(means etc) and statistical tests of hypotheses.\n4.\nDiscussion\n-\nDiscuss the interpretation and significance of the results\n\nSome Statistics\nTwo uses of statistics in experiments:\n- Summarize properties of the results (descriptive statistics).\n- Test the significance of results (hypothesis testing).\n\nDescriptive statistics\nA measure of central tendency:\n- Mean:\n-\nM is used for sample mean, μ for population mean.\nA measure of dispersion:\n- Variance: mean of the squared deviations from the mean\nM = Σxi\nN\nσ2 = Σ(xi-μ)2\nN\n\n- Standard deviation: σ\n(square root of the variance).\n\nbl(ow)\nTime (s)\n0.350678\n0.47009\nTime (s)\n0.312515\n0.487222\nTime (s)\n0.322387\n0.522607\nTime (s)\n0.337107\n0.537411\nTime (s)\n0.352544\n0.552427\ngl(ow)\nbr(ew)\ndr(ew)\ngr(ew)\n\nHypothesis Testing\n- F2 onset (Hz)\n- Are these differences in means significant?\n- Could the apparent differences have arisen by chance,\nalthough the true (population) means of F2 onsets are the\nsame?\n- I.e. given that F2 onsets vary, we might happen to sample\nmost of our [br] onsets from the low end of the\ndistribution, and most of our [gr] onsets from the high end.\n- Statistical tests allow us to assess the probability that this is\nthe case.\n\nbr\ndr\ngr\nmean 1225\ns.d. 150\n\nHypothesis Testing: t-test\n-\nThe t-test allows us to test hypotheses concerning means and\ndifferences between means.\n1.\n'Mean F2 onset in [br] differs from mean F2 onset in [gr] in\nEnglish'.\n2.\n'Mean F2 onset in [br] is 1250 Hz' (unlikely, but a simpler case\n- cf. [afva] is identified as [afa] > 50%).\n-\nWe actually evaluate two exhaustive and mutually exclusive\nhypotheses, a null hypothesis that the mean has a particular value,\nand the alternative hypothesis that the mean does not have that value.\n1.\nThe mean F2 onset in [br] is the same as the mean F2 onset in\n[gr] (Null).\n2.\nThe mean F2 onset in [bt] =1250 Hz (Alternative).\n-\nStatistical tests allow us to assess the probability of obtaining the\nobserved data if the null hypothesis were true.\n\nHypothesis Testing: t-test\n-\nBasic concept: If we know what the distribution\nof sample means would be if the null hypothesis\nwere true, then we can calculate the probability\nof obtaining the observed mean, given the null\nhypothesis.\n-\nWe arrive at the parameters of the distribution of\nsample means through assumptions and\nestimation.\n\nHypothesis Testing\nσ = 10 ms\nif this were the\npopulation mean...\n...it is unlikely that we\nwould get a sample mean\nof this value\nDistribution of sample means\n\nHypothesis Testing\n-\nBasic assumption: The samples are drawn from normal populations.\nBel\nl cu\nrve\nwi\nth 68\n%,\n%,\nan\nd 9\n9.7% ranges shown, also 2.5% ends shaded.\nFigure by MIT OpenCourseWare. Adapted from Kachigan, S. K. Multivariate Statistical Analysis. 2nd ed. New York, NY: Radius, 1991.\n\nHypothesis Testing\n-\nBasic assumption: The samples are drawn from normal populations.\n-\nProperties of distribution of means of samples of size N drawn from a\nnormal population:\n-\nThe sample means are normally distributed.\n-\nMean is the same as the population mean.\n-\nThe variance is less than the population variance:\nσM2 = σ2\nN\n\nFou\nr b ell cu\nrve\ns, bec\no\nmin\ng p rog res\nsiv\nely mo\nr\ne n\narr ow.\nFigure by MIT OpenCourseWare. Adapted from Kachigan, S. K\n. 2nd ed. New York, NY: Radius, 1991.\nMultivariate Statistical Analysis\n\nHypothesis Testing\n-\nThe mean of the distribution is determined by hypothesis.\n-\nE.g. mean = 1250 Hz or mean difference = 0.\n-\nPopulation variance is estimated from the sample variance. Unbiased\nestimate of the population variance:\n-\nN-1 is the number of degrees of freedom of the sample.\n-\nSo estimated variance of distribution of sample means, SM\n2 = S2/N\n-\nt score:\nS2 = Σ(xi-Μ)2\nN-1\n\nt = M-μ\nSM\n\nHypothesis Testing\n-\nt scores follow a t-distribution - similar to a normal distribution, but\nwith slightly fatter tails (more extreme values) because S may\nunderestimate σ.\n-\nt-distribution is actually a family of distributions, one for each\nnumber of degrees of freedom.\n-\nCalculate t-score then consult relevant t distribution to determine the\nprobability of obtaining that t-score or greater (more extreme).\nFigure by MIT OpenCourseWare.\nBell c\nurve with 3\nminor bell curves, slightly shorter, graphing normal and df=5 and df=12.\n\nt test for independent means\n-\nWhen we compare means, we are actually sampling a population of\ndifferences (e.g. differences in durations of vowels in open and\nclosed syllables).\n-\nIf the null hypothesis is correct, then the mean difference is 0.\n-\nVariance of the distribution of mean differences is estimated based\non the variances of the two samples.\n\nHypothesis testing\n-\nStatistical tests like the t test give us the probability of obtaining the\nobserved results if the null hypothesis were correct - the 'p' value.\nE.g. p < 0.01, p = 0.334.\n-\nWe reject the null hypothesis if the experimental results would be\nvery unlikely to have arisen if the null hypothesis were true.\n-\nHow should we set the threshold for rejecting the null hypothesis?\n-\nChoosing a lower threshold increases the chance of incorrectly\naccepting the null hypothesis.\n-\nChoosing a higher threshold increases the chance of incorrectly\nrejecting the null hypothesis.\n-\nA common compromise is to reject the null hypothesis if p <\n0.05, but there is nothing magical about this number.\n\nHypothesis testing\n-\nIn most experiments we need more complex statistical analyses than\nthe t test (e.g. ANOVA), but the logic is the same: Given certain\nassumptions, the test allows us to determine the probability that our\nresults could have arisen by chance in the absence of the\nhypothesized effect (i.e. if the null hypothesis were true).\n\nFitting models\n-\nStatistical analyses generally involve fitting a model to the\nexperimental data.\n-\nThe model in a t-test is fairly trivial, e.g.\nduration = μ + syllable_type (syllable_type is 'open' or 'closed')\n\nFitting models\n-\nStatistical analyses generally involve fitting a model to the experimental\ndata.\n-\nThe model in a t-test is fairly trivial, e.g.\nduration = μ + syllable_type\n(syllable_type is 'open' or 'closed')\ndurationij = μ + syllable_typei + errorij\n-\nAnalysis of Variance (ANOVA) involves more complex models, e.g.\ndurijk = μ + voweli + syll_typej + errorijk\ndurijk = μ + voweli + syll_typej + vowel*syll_typeij + errorijk\n-\nModel fitting involves finding values for the model parameters that yield the\nbest fit between model and data (e.g. minimize the squared errors).\n-\nHypothesis testing generally involves testing whether some term or\ncoefficient in the model is significantly different from zero."
    },
    {
      "category": "Resource",
      "title": "Effects of the Lexicon and Context on Speech Perception",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/a329c039693ba03f4dff75f19c6636d2_lec9_2_neighbor.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nEffects of the lexicon and context\non speech perception\n\nLexical Statistics\nThe pronunciation of words does not just depend on\ntheir phonological representations (features etc), also\n- Prosody (phrasing, accentuation)\n- Speech rate\n- 'Lexical' statistics:\n- word frequency\n- neighborhood density\n- Contextual predictability (cloze probability)\n\nLexical Statistics\n- Word recognition is also affected by these properties\n- It has been hypothesized that the production and\nperception effects are linked.\n- Words that are more difficult to recognize are\npronounced more clearly.\n\nOutline\n- Review some effects of lexical statistics on word recognition.\n- Present an analysis of these effects in terms of a Bayesian\nmodel of word recognition.\n- Explore predictions concerning interactions between\nfrequency/neighborhood density and contextual predictability.\n- These effects should be less important where contextual\ninformation is available.\n- Next time: look at corresponding production effects, and\ngeneral evidence for 'listener-oriented behavior' on the part of\nspeakers.\n\nEffects of lexical properties on word\nrecognition\n- Frequency: more frequent words are identified more rapidly\nand accurately (e.g. Goldinger et al 1996)\n- Luce (1986) demonstrated that word frequency alone is not a\nvery good predictor of difficulty in word recognition - neglects\ncompetition effects.\n- Recognizing a word involves picking out that word from all of\nthe words in the lexicon.\n- This process of discrimination may be impeded where there\nare many words that are perceptually similar to the target word\n- lexical neighbors.\n- High frequency facilitates the recognition of the target words,\nbut high frequency neighbors impede recognition.\n\nNeighborhood density/Relative frequency\nFigure by MIT OpenCourseWare. Adapted from Lindblom 1990.\nAuditory similiarit\ny space: 2D project\nion (dots) & bar gr\naph (frequency of occuren\nce).\n\nNeighborhood density\n- High density: cat: coat, at, scat, cap...\n- Low density: choice: voice, chase\n- Also matters how frequent those neighbors areChart with different vowes and lexically easy adn difficult examples.\nFigure by MIT OpenCourseWare.\nChart w\nith different vo\nwes and lexicall\ny easy adn diff\nicult examples.\n\nLuce, Pisoni & Goldinger (1990)\n- Tested effects of lexical neighborhood on speed and accuracy\nof identification of CVC words in noise.\n- Neighborhood probability rule\n- p(stimulus word) is probability of correctly identifying the\nsegments of the stimulus.\n- p(neighborj) is probability of misidentifying the stimulus as\n(having the segments of) neighbor j.\np(ID) =\np(stimulus word) × freqs\np(stimulus word) × freqs +\np(neighborj) × freq j\n{\n}\nj=1\nn\n∑\n\nLuce, Pisoni & Goldinger (1990)\nPredictions:\n- words with higher frequency should be more accurately\nidentified.\n- Words with higher stimulus probability (made up of less\nconfusable segments) should be more accurately identified.\n- Words with more similar neighbors should be less accurately\nidentified.\n- Words with more high frequency neighbors should be less\naccurately identified.\np(ID) =\np(stimulus word) × freqs\np(stimulus word) × freqs +\np(neighborj) × freq j\n{\n}\nj=1\nn\n∑\n\nLuce, Pisoni & Goldinger (1990)\n- Stimuli: 400 CVC words divided into 8 classes, fully crossing:\n- High vs. low word frequency\n- High vs. low stimulus probability\n- High vs. low frequency-weighted neighborhood probability\n- Words mixed with white noise (SNR +5dB) and presented to\nsubjects for identification.\n- Stimulus/neighbor probabilities were estimated from confusion\nmatrices for CV and VC syllables in noise.\n- Assume confusion probability depends only on position.\n- p(kɪd|kæt) = p(kons|kons)×p(ɪ|æ)×p(dcoda| tcoda)\n- p(∅|seg) and p(seg| ∅) were used to for CCVC, CV etc.\n- Only familiar monosyllabic words were considered.\n\nResults\n-\nHigh stimulus probability words identified more accurately than low\nstimulus probability words.\n-\nWords with high frequency-weighted neighborhood probabilities identified\nless accurately.\n-\nHigh frequency words identified more accurately than low frequency\nwords, but high freq words in dense neighborhoods identified less\naccurately than low freq words in sparse neighborhoods.Frequency-weighted neighborhood probability vs. percent correct.\nFigure by MIT OpenCourseWare.\nFre\nquen\ncy-weighted nei\nghborhood probability vs. percent correct.\n\nLuce, Pisoni & Goldinger (1990)\n-\nLexical decision: Spoken word or nonword presented.\n- Subject must decide whether the stimulus is a word or not.\n-\nReaction time to nonword stimuli were slower where:\n- Mean frequency of neighbors is higher.\n- Density of neighborhood is higher.\n- No interaction.\n-\nHere neighbors of a word are taken to be all words that can be created from that\nword by adding, deleting or changing one phone.\n- This operational definition is widely used.\nMea\nn n\neig\nhbo\nrho\nod\nfre\nquency vs. reaction time: 2\nli\nnes\nin free space, small positive slope.\nFigure by MIT OpenCourseWare. Adapted from Luce, P. A., D. B. Pisoni, and S. B. Goldinger. \"Similarity Neighborhoods of Spoken Words.\"\nIn Cognitive Models of Speech Processing. Edited by G. T. M. Altmann. Cambridge, MA: MIT Press, 1990, pp. 122-147.\n\nA Bayesian model of word recognition\n- The qualitative predictions of Luce's neighborhood probability\nrule can be reached based on a Bayesian model of word\nrecognition (e.g. Jurafsky 1996, Norris 2006).\n- Use Bayes Rule to combine signal-dependent and signal-\nindependent evidence in word recognition.\n- Probability of word w given signal-based evidence E:\np(w | E) = p(E | w)p(w)\np(E)\np(w | E) =\np(E | w)p(w)\np(E | wi)p(wi)\nwi ∈lexicon\n∑\n(1)\n(2)\nprior probability of\nword\nprior probability of\nevidence\n\nBayes' Theorem\n- Conditional probability:\nPr(A | B) = Pr(A∩B)\nPr(B)\n- Combine these equations:\nPr(B | A) = Pr(A∩B)\nPr(A)\nPr(A | B)Pr(B) = Pr(A∩B) = Pr(B | A)Pr(A)\nPr(A | B) = Pr(B | A)Pr(A)\nPr(B)\n- Divide by Pr(B), yieldng Bayes' Theorem:\n\nApplication of Bayes' Theorem\n- A medical test has a 95% chance of detecting a disease.\n- The test has a 5% chance of yielding a positive result in the\nabsence of the disease (false positive).\n- 1 in 100 people has the disease.\n- Suppose you have tested positive. What is the chance that you\nhave the disease?\n\nApplication of Bayes' Theorem\nP(Positive Test|Disease) = 0.95\nP(Positive Test|¬Disease) = 0.05\nP(Disease) = 0.01, P(¬Disease) = 0.99\nP(Positive Test) = P(Pos.Test|Disease) × P(Disease) +\nP(Pos.Test| ¬Disease) × P(¬Disease)\n= 0.95 × 0.01 + 0.05 × 0.99 = 0.059\nP(Disease|Pos.Test) = (0.95 × 0.01)/0.059 = 0.16\n- Given the possibility of test error, we need to take prior\nprobability into account.\nP(Disease | Pos.Test) = P(Pos.Test | Disease)P(Disease)\nP(Pos.Test)\n\nA Bayesian model of the listener - word\nfrequency\n-\nEvidence is accumulated over time. Listeners identify a stimulus as word w\nwhen that probability exceeds some threshold.\n-\nFrequency: more frequent words are identified more rapidly and accurately\n(e.g. Goldinger et al 1996)\n- Higher frequency of w implies higher prior probability p(w)\n- Less bottom-up evidence required to reach a threshold probability that\nword is w.\n(Jurafsky 1996, Norris 2006, etc)\np(w | E) =\np(E | w)p(w)\np(E | wi)p(wi)\nwi ∈lexicon\n∑\n\nA Bayesian model of the listener -\nneighborhood density\n-\nNeighborhood density: words from denser neighborhoods are identified\nmore slowly and less accurately.\n- Neighbors of w are similar to w, so p(E|wi) is going to be relatively\nhigh where wi is a neighbor.\n- So more neighbors and higher frequency neighbors increase the\ndenominator above, reducing p(w|E) (Jurafsky 1996).\n- NB standard calculation of neighborhood is an approximation (cf. Luce\n1986).\np(w | E) =\np(E | w)p(w)\np(E | wi)p(wi)\nwi ∈lexicon\n∑\n\nA Bayesian model of the listener - context\neffects\n- The Bayesian analysis implies that word frequency\naffects word recognition because it is a good basis for\nestimating prior probability of a word in the absence\nof any other constraint.\n- But in general the prior probability of a word depends\non context, e.g. discourse topic, previous words,\nsyntactic structure.\n- Ideal listener should incorporate these contextual\neffects into estimates of prior probabilities.\n\nA Bayesian model of the listener - context\neffects\n- The probability of a word depends on context C.\n- increase in p(w|C) reduces evidence needed for identification\nof w.\n- Predictability effect: When words are more predictable from\ncontext they are:\n- more accurately identified (e.g. Boothroyd and Nittrouer\n1988, Sommers and Danielson 1999).\n- Identified earlier in a gating task (Craig et al 1993).\np(w | E,C) =\np(E | w)p(w |C)\np(E | wi)p(wi |C)\nwi ∈lexicon\n∑\n\nBoothroyd & Nittrouer 1988\n- Studied accuracy of word identification in\nnonsensical and meaningful sentences.\n- Zero predictability\n- Girls white car blink.\n- Low predictability\n- Ducks eat old tape.\n- High predictability\n- Most birds can fly.\n- All words monosyllabic.\n- Words from LP and HP sentences used in\nZP sentences.\nSig\nnal\nt\no\n\nno\ni\nse\nr\nat\nio\nvs\n. % words recognized: 3 inc\nreasing curves (HP/LP/ZP\nsentences).\nFigure by MIT OpenCourseWare. Adapted from Boothroyd, A., and\nS. Nittrouer. \"Mathematical Treatment of Context Effects\nin Phoneme and Word Recognition.\" Journal of the Acoustical\nSociety of America 84 (1988): 101-114.\n\nA Bayesian model of the listener - context\neffects\nThe Bayesian model predicts interactions between predictability and\nfrequency/neighborhood density:\n-\nThere is no word frequency term in the model - frequency only enters as an\nestimate of word probability p(w|C) in the absence of contextual constraints.\n-\nAs context raises prior probability of w, the effect of competition from\nneighbors should be reduced.\n-\np(w|C) increases, most p(wi=w|C) decrease.\n-\nAs contextual constraint increases, the effects of word frequency and\nneighborhood density on word recognition should decrease.\np(w | E,C) =\np(E | w)p(w |C)\np(E | wi)p(wi |C)\nwi ∈lexicon\n∑\n\nInteractions between context and lexical\nstatistics\n- As contextual constraint increases, the effects of word\nfrequency and neighborhood density on word\nrecognition should decrease.\n- implies reduced importance for frequency per se\nfor running speech (same for frequency of\nneighbors).\n\nInteractions between context and lexical\nstatistics\nFrequency/Context:\n- Grosjean & Itzler (1984): effect of frequency on the isolation\npoint of gated words is reduced where words are more\npredictable from context (almost to zero in the most\nconstraining contexts).\n- Van Petten and Kutas (1990): ERP study of silent reading -\nless frequent words were associated with larger N400s early in\nsentences, but the frequency effect disappears later in a\nsentence, as semantic and syntactic constraints accumulate (also\nDambacher et al 2006).\n- 'frequency does not play a mandatory role in word recognition but can\nbe superseded by the contextual constraint provided by a sentence'\n\nInteractions between context and lexical\nproperties: Neighborhood density/Context\nSommers and Danielson (1999):\n-\nAuditory word identification task\n- Isolated words.\n- Final words in sentences:\n- Low predictability: 'She was thinking about the path'.\n- High predictability: 'She was walking along the path'.\n- Words had high (28) or low (9.1) neighborhood density ('hard' vs.\n'easy').\n- Matched for frequency.\n-\nTwo speakers, 22 listeners.\n-\nMaterials presented in noise.\n\nSommers & Danielson (1999)\nResults\n- Significant differences in identification accuracy across the\nthree contexts.\n- Significantly lower accuracy for words from dense\nneighborhoods.\n- Effect of neigborhood density is reduced in High Predictability\ncontexts (significant interaction Density × Context).Chart of context (SW/LP/HP) and easy and hard (M and SD).\nFigure by MIT OpenCourseWare. Adapted from Sommers, M. S., and S. M. Danielson. \"Inhibitory Processes and Spoken Word Recognition in Young and\nOlder Adults: The Interaction of Lexical Competition and Semantic Context.\" Psychology and Aging 14 (1999): 458-472.\nChar\nt of\ncontex\nt\n\n(S\nW/\nLP/H\nP) a\nnd e\nasy\nand\nhard\n(M\nand\nSD)\n.\n\nInteractions between context and lexical\nproperties: Neighborhood density/Context\n- Sommers, Kirk and Pisoni (1997): difference in accuracy of\nidentification of 'hard' and 'easy' words disappeared where\nsubjects had to pick from a closed set of words.\n- Bayesian model provides an accurate qualitative\ncharacterization of the effects of frequency, neighborhood\ndensity and contextual predictability on word recognition\nperformance.\n- Two basic factors:\n- Competition within the lexicon.\n- Predictability of target and competitors.\n- Frequency is an estimate of probability in the absence of context.\n\nReferences\n-\nAylett, M. and Turk, A. (2006). Language redundancy predicts syllabic\nduration and the spectral characteristics of vocalic syllable nuclei. JASA\n119:3048-3058.\n-\nBell, A., Jurafsky, D., Fosler-Lussier, E., Girand, C., Gregory, M., and Gildea,\nD. (2003). Effects of disfluencies, predictability, and utterance position on\nword form variation in English conversation. JASA 113, 1001-1023.\n-\nBillerey-Mosier, R. (2000). Lexical effects on the phonetic realization of\nEnglish segments. UCLA Working Papers in Phonetics 100.\n-\nBoothroyd and Nittrouer (1988). Mathematical treatment of context effects in\nphoneme and word recognition. JASA 84(1):101.\n-\nDambacher, M., Kliegl, R., Hofmann, M., and Jacobs, A.M. (2006). Frequency\nand predictability effects on event-related potentials during reading, Brain\nResearch 1084, 89-103.\n-\nGoldinger, S.D., Pisoni, D.B., & Luce, P.A. (1996). Speech perception and\nspoken word recognition: Research and theory. In N.J. Lass (Ed.), Principles\nof Experimental Phonetics. St. Louis: Mosby. 277-327.\n-\nGriffin, Z.M., and Bock, K. (1998). Constraint, word frequency, and the\nrelationship between lexical processing levels in spoken word production.\nJournal of Memory and Language 38, 313-338.\n\n-\nGrosjean, F., & Itzler, J. (1984). Can semantic constraint reduce the role of\nword frequency during spoken word recognition? Bulletin of the Psychonomic\nSociety 22, 180-182.\n-\nJurafsky, D. (1996). A probabilistic model of lexical and syntactic access and\ndisambiguation. Cognitive Science 20, 137-194\n-\nLindblom, B. (1990). Explaining phonetic variation: A sketch of the H&H\ntheory. In W.J. Hardcastle and A. Marchal (eds.) Speech Production and\nSpeech Modeling. Kluwer: Dordrecht\n-\nLuce, P., and Pisoni, D. (1998). Recognizing spoken words: The neighborhood\nactivation model. Ear and Hearing 19. 1-36.\n-\nMunson, B. (2004) Lexical access, lexical representation, and vowel\nproduction. To appear in J. Cole and J. I. Hualde (eds.), Papers in Laboratory\nPhonology IX. Mouton de Gruyter.\n-\nMunson, B., and Solomon, N.P. (2004). The effect of phonological\nneighborhood density on vowel articulation. Journal of Speech, Language, and\nHearing Research 47, 1048-1058.\n-\nNorris, D. (in press) The Bayesian Reader: Explaining word recognition as an\noptimal Bayesian decision process. Psychological Review.\nReferences\n\n-\nPierrehumbert, J.B. (2002). Word-specific phonetics. In C. Gussenhoven and\nN. Warner (eds.) Papers in Laboratory Phonology VII, Mouton de Gruyter,\nNew York, 101-139.\n-\nScarborough, R. (2003). The word-level specificity of lexical confusability\neffects. Poster presented at the 146th Meeting of the Acoustical Society of\nAmerica, Austin, TX.\n-\nSommers, M.S., and Danielson, S.M. (1999). Inhibitory processes and spoken\nword recognition in young and older adults: the interaction of lexical\ncompetition and semantic context. Psychology and Aging 14, 458-472\n-\nSommers, M., Kirk, K., Pisoni, D. (1997). Some considerations in evaluating\nspoken word recognition by normal-hearing, noise-masked normal-hearing,\nand cochlear implant listeners. I. The effects of response format. Ear and\nHearing 18, 89-99.\n-\nVan Petten, C. and Kutas, M. (1990). Interactions between sentence context\nand word frequency in event-related brain potentials. Memory and Cognition\n18, 380-393.\n-\nVan Son, R.J.J.H. and Pols, L.C.W. (2003a). Information Structure and\nEfficiency in Speech Production\", Proceedings of EUROSPEECH2003,\nGeneva, Switzerland, 769-772.\nReferences\n\n-\nVan Son, R.J.J.H. and Pols, L.C.W. (2003b). An Acoustic Model of\nCommunicative Efficiency in Consonants and Vowels taking into Account\nContext Distinctiveness\". Proceedings of ICPhS, Barcelona, Spain, 2141-2144.\n-\nVitevitch, M. (2002). The influence of phonological similarity neighborhoods\non speech production. Journal of Experimental Psychology: Learning,\nMemory, and Cognition 28, 735-747.\n-\nWright, R. (2004) Factors of lexical competition in vowel articulation. In J.\nLocal, R. Ogden, and R. Temple (eds.), Papers in Laboratory Phonology VI.\nCambridge: CUP.\nReferences\n\nA Bayesian model of the listener - context\neffects\n- The Bayesian analysis implies that word frequency\naffects word recognition because it is a good basis for\nestimating prior probability of a word in the absence\nof any other constraint.\n- But in general the prior probability of a word depends\non context, e.g. discourse topic, previous words,\nsyntactic structure.\n- Ideal listener should incorporate these contextual\neffects into estimates of prior probabilities."
    },
    {
      "category": "Resource",
      "title": "Effects of the Lexicon and Context on Speech Perception (cont.)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/45b23130d1fc53c8415418627f5af860_lec10_listener.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nEffects of the lexicon and context\non speech production\n\nWord recognition\nThe speed an accuracy of word recognition depends on:\n- Word frequency\n- Neighborhood density\n- and frequency of neighbors\n- Contextual predictability\n- Speech production is also affected by these factors.\n\nEffects of lexical statistics on production\n- Wright (2004) found that neighborhood density/relative\nfrequency affects pronunciation of isolated words.\n- 'Hard words' - low frequency, high neighborhood density\n- 'Easy words' - high frequency, low neighborhood density\n- Vowels in hard words are more dispersed from each other in\nF1*F2 space than vowels in easy words.F1 bark (2-9) vs. F2 bark (15-8): Vowel sounds graphed in space.\nFigure by MIT OpenCourseWare.\nF1 bark\n(2\n-9\n)\nvs\n.\nF2\n\nb\na\nr\nk\n\n(\n-\n8): Vow\nel\n\ns\nou\nnds\n\ng\nr\nap\nh\ne\nd\ni\nn space.\n\nListener-Oriented Speakers\n- It has been hypothesized that the production and\nperception effects are linked:\n- Words that are more difficult to recognize are\npronounced more clearly.\n\nProduction and perception\nBroad outlines of an explanatory model of the effects of\nfrequency, neighborhood density etc on production (Wright\n2004, Scarborough 2004, 2006):\n- Speaker wishes to be understood.\n- Speaker wishes to minimize the effort involved in speech\nproduction.\n- Reduced effort tends to lead to reduced identifiability of\nwords.\n- Optimal strategy: reduce effort more where clarity is less\nimportant, i.e. where top-down evidence makes it easier for a\nlistener to identify a word - Hyper- & Hypoarticulation theory\n(Lindblom 1990).\n\nAre speakers altruistic or selfish?\n-\nImplication of H&H account of pronunciation variability: Speakers\nestimate listener difficulty moment to moment and adjust clarity of speech\naccordingly.\n- Speaker has a model of the listener (altruistic speaker).\n-\nAlternative line of analysis: Speakers are selfish.\n- Speakers do not track listener difficulty.\n- Pronunciation variation is related to speaker difficulty with lexical\naccess for production:\n- Slower lexical access results in clearer speech.\n- To the extent that ease of lexical access is similar for speaker and\nhearer, similar results are predicted (but there are differences).\n- But why does slow lexical access result in clearer speech?\n\nFrequency and Neighborhood Density:\nMunson & Solomon (2004)\n- Point out that Wright (a) confounded frequency and\nneighborhood density, (b) didn't measure duration, so we can't\nbe sure if neighborhood density affects vowel formants\ndirectly or via vowel duration.Chart with different vowes and lexically easy and difficult examples.\nFigure by MIT OpenCourseWare.Table of lexically easy and lexically difficult vowels.\nTable\nof lexically\neasy and lexically\nd\ni\nf\nf\ni\nc\nu\nl\nt\n\nv\no\nw\ne\nl\ns.\n\nExperiment 1\n- Words read in isolation by 10 subjects.\n- Vowel space is expanded in Hard words (mean Euclidian\ndistance of vowels from the mean F1, F2 of all vowels).\n- Vowels were shorter in hard words (222 ms vs. 232 ms).\nImage removed due to copyright restrictions.\nPlease see Figures 1 and 2 in Munson, B., and N. P. Solomon. \"The Effect of Phonological Neighborhood Density on Vowel Articulation.\"\nJournal of Speech, Language, and Hearing Research 47 (2004): 1048-1058.\n\nExperiment 2\n-\n4 classes of words, crossing:\n- high vs. low frequency\n- high vs. low neighborhood density\n-\n20 words /class\n-\nDuration:\n- High frequency words had shorter vowels\n(205 ms vs. 211 ms)\n- No effect of neighborhood density.\n-\nVowel space expansion:\n- Less expanded in high frequency words.\n- More expanded in high neighborhood density\nwords.\n- No significant interaction.\nImage removed due to copyright restrictions.\nPlease see Figures 3 in Munson, B., and N. P. Solomon.\n\"The Effect of Phonological Neighborhood Density on\nVowel Articulation.\" Journal of Speech, Language, and\nHearing Research 47 (2004): 1048-1058.\n\nSelfish speakers and neighborhood density\n-\nThe H&H account: Words from dense neighborhoods/low\nfrequency words are pronounced more clearly because the\nspeaker knows they are likely to be more difficult for listeners\nto recognize.\n-\nSpeaker-oriented account (e.g. Pierrehumbert 2002):\n-\nSpeakers have to perform lexical access in speech\nproduction.\n-\nHypothesize that high neighborhood density impedes\nlexical access in production.\n-\nSlower lexical access results in clearer pronunciation.\n\nSelfish speakers and neighborhood density\n-\nBut high neighborhood density can actually speed lexical\naccess in production: Pictures are named more quickly when\ntheir names are in dense neighborhoods (Vitevitch 2002).\n-\nLexical access in production starts from meaning, so there is\nno problem of competition based on phonetic similarity.\n-\nVitevitch offers two explanations for the facilitatory effect of\ndense neighborhoods:\n-\nIn an 'interactive activation model': activation spreads\nbetween phonetically similar words. In dense neighborhoods\nmore activation 'reverberates' back to the target word.\n-\nOR: words in dense neighborhoods generally involve more\ncommon sound sequences - perhaps the motor plans for\nfrequent sound sequences are easier to access/assemble.\n\nLexical access and neighborhood density\n-\nMunson (2004) directly tested the 'lexical access' account of\nneighborhood density effects:\n-\nFour classes of words crossing:\n-\nHigh vs. low frequency\n-\nHigh vs. low neighborhood density\n-\nSubjects read words in two conditions:\n-\nRead word immediately on presentation.\n-\nWait 1000 ms after presentation before speaking word.\n-\nAssumption: 1000 ms delay gives speakers plenty of time to\ncomplete lexical access, so difficulty with lexical access\nshould not affect pronunciation at this time lag.\n\nMunson (2004) results\n-\nIn both conditions vowels were more dispersed in words from dense\nneighborhoods.\n-\nsuggests effect of neighborhood density is not due to speaker\ndifficulty with lexical access.\n-\nThe effect of frequency depends on condition:\n-\nNo delay: vowels more dispersed in low frequency words.\n-\nDelay: effect of frequency not significant.\nba\nr\ngr\nap\nhs\n:\nDa\nily condition vs. av\ng. vowel space disp\nersio\nn (2 bars\neach, immediate & delay).\nFigure by MIT OpenCourseWare.\n\nContextual predictability and speech\nproduction\n-\nH&H theory predicts that speaker clarity should generally\nmirror listener difficulty with lexical access.\n-\nGiven the evidence above concerning word recognition\nperformance, we should expect to find:\n1.\nReduced clarity where a word is predictable from\ncontext.\n2.\nReduced effects of frequency/neighborhood density in\ncontexts where a word is more predictable.\n-\nSome evidence for (1), e.g Lieberman (1963), Hunnicutt\n(1985), Fowler & Housum (1987), Bell et al (2003).\n-\nOnly one inconclusive test of (2).\n\nContextual predictability and speech\nproduction\n- Lieberman (1963):\n- The word you will hear is nine.\n- A stitch in time saves nine.\n- Words excised from predictable contexts are less accurately\nidentified in noise.\n- Hunnicutt (1985) replicated and extended this result.\n- Assessed predictability of words by Cloze procedure:\nPresent sentences with word left out. Ask subjects to guess\nthe missing word. Cloze probability = proportion of correct\nguesses.\n- No investigation of the acoustic bases of these effects.\n\nRepetition and speech production\n- Fowler & Housum (1997) examined the pronunciation and\nperception of first and second uses of words in a monologue\n(Garrison Keillor - 35 pairs of words) and some short news\ninterviews (45 pairs).\n- Second mentions should generally be more predictable.\n- Very high frequency words excluded (function words).\n- Measured:\n- Word duration.\n- Peak amplitude.\n- f0 of stressed vowel.\n\nRepetition and speech production\n- New words were longer, louder and had higher f0.\n- Only the duration effect is statistically reliable across\nspeakers.\n- A subsequent experiment established that old words were, on\naverage, more predictable in a Cloze task:\n- New 18.3% correct vs. Old 31.1% correct\nCha\nrt\nof\nKei\nllo\nr a\nnd othe\nrs, ne\nw a\nnd\nold\n, m\neas\nuri\nng\ndur\natio\nn, f\nrequ\nency\n, and amplitu\nde.\nFigure by MIT OpenCourseWare.\n\nRepetition and speech production\n-\nWords from the first experiment were excised and presented for\nidentification by subjects (+ confidence rating /5)\n-\nBoth versions of each word were heard in separate blocks.\n-\nOrder or pairs counter-balanced across subjects.\n-\nRepeated words are less accurately identified.\n-\nAccuracy correlated with duration difference (new-old).\n-\nWords identified more accurately on second presentation.\n-\nrepetition facilitates recognition (i.e. could offset reduction).\nBlock\nof tes\nt / occurence in mono\nlog\n: 1\nst and 2nd, % err\nors and confidence j\nu\nd\ng\nm\nents\n.\nFigure by MIT OpenCourseWare.\n\nSelf-centered behavior in production of\nrepetitions\n- Bard et al (2000) examined shortening of repeated words in\nspontaneous dialogues produced in the context of a 'map task'.\n- Two participants each have a map, one showing a route. Not\nall landmarks are shown on both maps.\n- Subject with the route map instructs the second subject on how\nto reproduce the route on his/her map.\n- Elicits spontaneous speech, but with multiple mentions of\nitems marked on the maps.\n- Basic set-up: look for effects of repetition on intelligibility of\nwords.\n- Twist: subjects repeat the task with two partners. Will they\nproduce clear, longer pronunciations on first mentions to a\nnew listener?\n\nSelf-centered behavior in production of\nrepetitions\n- Each subject lead two partners through the same map task.\n- Key words were excised from the recorded dialogues and\npresented in noise for identification in a follow-up experiment.\n- Durations of words were also measured.\n\nSelf-centered behavior in production of\nrepetitions\n- Intelligibility and duration\nof first mentions of\nlandmarks were reduced in\nsecond trial, even though\nthey were new to the\nsecond listener.\n- Speakers seem to act as if\nan item is old if it is old to\nthem, even if it is new to\nthe listener ('egocentric').\nFace\ncondit\nion (screened/\nvisible)\nand for\nm (tria\nl/citati\non): Va\nlues\nof in\ntelli\ngibilit\ny and d\nurati\non.\nFigure by MIT OpenCourseWare.\n\nSelf-centered behavior in production of\nrepetitions\n-\nBard et al's results might be attributed to effects of lexical access (priming).\n-\nBard et al actually argue that speakers are incompetent altruists: tracking\nlistener needs is simply too demanding, so speakers usually make the\nsimplifying assumption that listeners know what they know.\n-\nA further complication: Bard et al (2000) did not compare repeated\ninteractions with the same vs. different listeners. The second listener was\nalways new.\n-\nGregory (2001) had speakers tell a story twice, either to the same listener or\nto two different listeners.\n-\nMeasured durations of repeated referring expressions (e.g. names).\n-\nWhen hearer changes for second narration, first use in second narration\nis longer than first use in narration 1 (n.s.)\n-\nWhen hearer is the same for both narrations, first use in narration 2 is\nshorter than first use in narration 1.\n-\nIndicates that speakers can take listener knowledge into account.\n\nListener-oriented behavior in speech\nproduction\n- While a variety of the effects predicted by the H&H model\nhave been observed (neighborhood density, frequency,\npredictability), there are alternative 'selfish' explanations for\nmost of them.\n- H&H theory offers a simple, unified account of these\nphenomena, the speaker-oriented analysis are more ad hoc\n(e.g. why should slower lexical access result in clearer speech?\nLexical access cannot explain neighborhood density effects,\netc.)\n- It is clear that H&H theory needs to be supplemented by an\naccount of how well speakers track listener needs.\n\nReferences\n-\nAylett, M. and Turk, A. (2006). Language redundancy predicts syllabic\nduration and the spectral characteristics of vocalic syllable nuclei. JASA\n119:3048-3058.\n-\nBell, A., Jurafsky, D., Fosler-Lussier, E., Girand, C., Gregory, M., and Gildea,\nD. (2003). Effects of disfluencies, predictability, and utterance position on\nword form variation in English conversation. JASA 113, 1001-1023.\n-\nBillerey-Mosier, R. (2000). Lexical effects on the phonetic realization of\nEnglish segments. UCLA Working Papers in Phonetics 100.\n-\nBoothroyd and Nittrouer (1988). Mathematical treatment of context effects in\nphoneme and word recognition. JASA 84(1):101.\n-\nDambacher, M., Kliegl, R., Hofmann, M., and Jacobs, A.M. (2006). Frequency\nand predictability effects on event-related potentials during reading, Brain\nResearch 1084, 89-103.\n-\nGoldinger, S.D., Pisoni, D.B., & Luce, P.A. (1996). Speech perception and\nspoken word recognition: Research and theory. In N.J. Lass (Ed.), Principles\nof Experimental Phonetics. St. Louis: Mosby. 277-327.\n-\nGriffin, Z.M., and Bock, K. (1998). Constraint, word frequency, and the\nrelationship between lexical processing levels in spoken word production.\nJournal of Memory and Language 38, 313-338.\n\n-\nGrosjean, F., & Itzler, J. (1984). Can semantic constraint reduce the role of\nword frequency during spoken word recognition? Bulletin of the Psychonomic\nSociety 22, 180-182.\n-\nJurafsky, D. (1996). A probabilistic model of lexical and syntactic access and\ndisambiguation. Cognitive Science 20, 137-194\n-\nLindblom, B. (1990). Explaining phonetic variation: A sketch of the H&H\ntheory. In W.J. Hardcastle and A. Marchal (eds.) Speech Production and\nSpeech Modeling. Kluwer: Dordrecht\n-\nLuce, P., and Pisoni, D. (1998). Recognizing spoken words: The neighborhood\nactivation model. Ear and Hearing 19. 1-36.\n-\nMunson, B. (2004) Lexical access, lexical representation, and vowel\nproduction. To appear in J. Cole and J. I. Hualde (eds.), Papers in Laboratory\nPhonology IX. Mouton de Gruyter.\n-\nMunson, B., and Solomon, N.P. (2004). The effect of phonological\nneighborhood density on vowel articulation. Journal of Speech, Language, and\nHearing Research 47, 1048-1058.\n-\nNorris, D. (in press) The Bayesian Reader: Explaining word recognition as an\noptimal Bayesian decision process. Psychological Review.\nReferences\n\n-\nPierrehumbert, J.B. (2002). Word-specific phonetics. In C. Gussenhoven and\nN. Warner (eds.) Papers in Laboratory Phonology VII, Mouton de Gruyter,\nNew York, 101-139.\n-\nScarborough, R. (2003). The word-level specificity of lexical confusability\neffects. Poster presented at the 146th Meeting of the Acoustical Society of\nAmerica, Austin, TX.\n-\nSommers, M.S., and Danielson, S.M. (1999). Inhibitory processes and spoken\nword recognition in young and older adults: the interaction of lexical\ncompetition and semantic context. Psychology and Aging 14, 458-472\n-\nSommers, M., Kirk, K., Pisoni, D. (1997). Some considerations in evaluating\nspoken word recognition by normal-hearing, noise-masked normal-hearing,\nand cochlear implant listeners. I. The effects of response format. Ear and\nHearing 18, 89-99.\n-\nVan Petten, C. and Kutas, M. (1990). Interactions between sentence context\nand word frequency in event-related brain potentials. Memory and Cognition\n18, 380-393.\n-\nVan Son, R.J.J.H. and Pols, L.C.W. (2003a). Information Structure and\nEfficiency in Speech Production\", Proceedings of EUROSPEECH2003,\nGeneva, Switzerland, 769-772.\nReferences\n\n-\nVan Son, R.J.J.H. and Pols, L.C.W. (2003b). An Acoustic Model of\nCommunicative Efficiency in Consonants and Vowels taking into Account\nContext Distinctiveness\". Proceedings of ICPhS, Barcelona, Spain, 2141-2144.\n-\nVitevitch, M. (2002). The influence of phonological similarity neighborhoods\non speech production. Journal of Experimental Psychology: Learning,\nMemory, and Cognition 28, 735-747.\n-\nWright, R. (2004) Factors of lexical competition in vowel articulation. In J.\nLocal, R. Ogden, and R. Temple (eds.), Papers in Laboratory Phonology VI.\nCambridge: CUP.\nReferences\n\nA Bayesian model of the listener - context\neffects\n- The Bayesian analysis implies that word frequency\naffects word recognition because it is a good basis for\nestimating prior probability of a word in the absence\nof any other constraint.\n- But in general the prior probability of a word depends\non context, e.g. discourse topic, previous words,\nsyntactic structure.\n- Ideal listener should incorporate these contextual\neffects into estimates of prior probabilities."
    },
    {
      "category": "Resource",
      "title": "Intonation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/5dc30230f95b6232ec4e2e2a1527ce7b_lec7_intonation.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n24.910\nLaboratory Phonology\nIntonation\n\nReadings:\n- ToBI tutorial, 2.2-2.5, 2.8\nAssignments:\n- Write a short description of your project.\n- ToBI transcription exercises (to be posted).\n\nThe Phonology of English Intonation\nA very brief introduction to English intonation primarily\nbased on Pierrehumbert (1980) (P80) and Beckman and\nPierrehumbert (1986) (BP86).\nPierrehumbert, Janet (1980) The Phonology and Phonetics of English\nIntonation. PhD dissertation, MIT.\nBeckman, Mary and Janet Pierrehumbert (1986) Intonational structure in\nJapanese and English. Phonology Yearbook 3.\n\nThe phonology of intonation\n- Three components relevant to the theory of\nintonation:\n- Intonation (pitch contour)\n- Stress\n- Phrasing\n- Collectively can be referred to as prosody.\n- But sometimes 'intonation' is used to cover all of\nthese components.\n- These components are closely interrelated.\n\nThe representation of intonation\nL* H* L-L%\nHe knew the millionaire\n- A string of tones, H(igh), L(ow).\n- Three kinds of tones are differentiated, for purposes of tune-\ntext alignment and phonetic interpretation:\n- Pitch accents\nT*\ngravitate to stressed syllables\n- Phrase accents T- or Tgravitate to (smaller) phrase\nboundaries\n- Boundary tone T%\ngravitate to (larger) phrase\nboundaries\n\nThe representation of intonation\nMotivation for analyzing intonation contour in terms\nof a sequence of tones.\n- When similar tunes are associated with texts of\ndiffering lengths, the consistent properties of the\nmelody are alignments between landmarks in the\nf0 contour and stressed syllables and the edges of\nphrases.\n\nQuickTimeTM and a\nTIFF (LZW) decompressor\nare needed to see this picture.\nA: I hear Nell's taking a course to be a driving instructor.\nA: I hear Nell's got a job as a marmalade maker.\nN e ll\na m ar\nm a l a de m a k er\n- L+H* associates to the\nmain stress,\n- L-H% rise occurs at the end\nof the phrase,\n- any interval between is\nfilled by L-.\nAudio:\n7_001.wav\nAudio:\n7_002.wav\n\nInventory of pitch accents\nP80\n7 pitch accents\nPB86\n6 pitch accents\nToBI\n5 pitch accents +\ndownstep (!)\nH*\nH*\nH*\nL*\nL*\nL*\nH+L*\nH+L*\nH+!H*\nH*+L\nH*+L\n(H* followed by downstep)\nL*+H\nL*+H\nL*+H\nL+H*\nL+H*\nL+H*\nH*+H\nPhrase accents: H-, L-\nBoundary tones: H%, L%\n\nIntonation\n- Tones are realized primarily in terms of f0 (fundamental\nfrequency).\n- Pitch accented syllables are also generally louder and\nlonger than unaccented syllables.\n\nStress\n-\nStress: 'relative prominence' of syllables (Liberman & Prince 1977).\n-\nLexical stress:\nin.to.na.tion\npro.ba.bly\n-\nWords have a stress pattern.\n-\nOne syllable (primary stress) is more prominent than the rest.\n-\nPrecise phonetic correlates are complex.\n-\nUsually: loudness (duration, intensity), voice quality.\n-\nMost important here: If a word bears a single pitch accent, it is aligned to the\nprimary stressed syllable.\n-\nCompletely unstressed syllables usually cannot bear pitch accents.\n-\nThere are also prominence relations between words.\n-\nGenerally correlates with pitch accent placement: the syllables with the strongest\nstresses have pitch accents.\n\nPhrasing\n- Utterances are divided into intonational phrases, which are\nsubdivided in intermediate phrases.\n- Hierarchical bracketing of the phonological string, usually\nassumed to be related to, but distinct from, syntax.\nOnly 1 one 4 remembered 3 the 0 lady 1 in 1 red 4\nPwd Pwd Pwd Pwd Pwd Pwd\nip ip ip\nIP\nIP\n\nPhrasing\nUnlike syntactic structure, prosodic structure is argued to be:\n- Exhaustive: a string is fully parsed into constituents of each type.\n- Strictly layered: There is a hierarchy of constituents such that each type\nof constituent only dominates the next type of constituent on the\nhierarchy\nOnly 1 one 4 remembered 3 the 0 lady 1 in 1 red 4\nPwd Pwd Pwd Pwd Pwd Pwd\nip ip ip\nIP\nIP\n\nPhrasing\nThis is a very impoverished form of constituent structure that can be fully\nspecified by placing a symbol between each pair of words indicating\nthe strongest boundary between those words.\n-\nThis is how the break index notation represents prosodic structure.\nOnly 1 one 4 remembered 3 the 0 lady 1 in 1 red 4\nPwd Pwd Pwd Pwd Pwd Pwd\nip ip ip\nIP\nIP\n\nWhy phonological constituents?\nWhy is a phonological constituent structure needed in addition to syntactic\nstructure?\n- Syntax-prosody mismatches\nThis is [the cat [that caught [the rat [that stole [the cheese]]]]]\n[This is the cat][that caught the rat][that stole the cheese]\n[Sesame St. is brought to you]IP[ by the children's television workshop]IP\n[Sesame St. is brought to you by]IP[ the children's television workshop]IP\n\n- In many languages, segments are lengthened before constituent boundaries.\n- Pauses typically occur at intonational phrase boundaries.\nWightman et al (1992)\n- Studied a corpus of sentences read by professional news announcers.\n- Prosodic boundaries marked using a 7-point scale of break indices\nNo prosodic break (cliticization)\nProsodic word boundary\n'accentual phrase'\nIntermediate phrase\nIntonation phrase\nSuperior major tone group\nSentence boundary\n- Lengthening measured in terms of normalized segment durations (standard deviations\nfrom the mean for that segment, adjusted for estimated speech rate of each sentence).\nPhonetic correlates of phrasing\n\n- Lengthening before constituent boundaries, localized to the final\nsyllable rhyme (VC).\n- Degree of final lengthening increases with size of prosodic constituent.\nPhonetic correlates of phrasing\nPe\nr\nc\ne\np\nt\nu\na\nl lab\nels v\ns. me\nan n\norma\nlize\nd du\nrati\non:\nRising curve.\nFigure by MIT OpenCourseWare. Adapted from Wightman, Colin W., Stefanie Shattuck-Hufnagel, Mari Ostendorf, and Patti J. Price.\n\"Segmental Durations in The Vicinity of Prosodic Phrase Boundaries.\" Journal of the Acoustical Society of America 91 (1992): 1707-1717.\n\nThe relationship between phrasing and\nintonation\nToBI:\n- Intermediate phrase must contain one or more pitch\naccents and a final phrase accent (associated to the ip\nboundary)\n- Intonation phrase may begin, and must end, with a\nboundary tone (associated to the IP boundary) (as in P80)\nE.g.\nH*L-\nH*L- L%\n[[ 'I']ip[means 'insert']ip]IP\n\nH* L-\nH* L-L%\n[[Marianna][made the marmalade]]\nH* H* L-L%\n[[Marianna made the marmalade]]\nL+H*L-H% H* L-L%\n[[Marianna ]][[made the marmalade]]\nAudio:\n7_003.wav\nAudio:\n7_004.wav\nAudio:\n7_005.wav\n\nThe relationship between phrasing and\nintonation\n-\nNuclear accent = last accent in an intermediate phrase.\n-\nThe phrase tone spreads over the interval between the nuclear accent\nand the end of the phrase.\nL+H* L-L%\n[[ Marianna made the marmalade ]]\nAudio:\n7_006.wav\n\nPitch Tracking\n- ToBI transcription is performed with reference to an f0\ncontour, so it is useful to have some idea of how f0\ncontours are calculated and interpreted, and what can go\nwrong at both stages.\n- Pitch tracking algorithms:\n- Tentative identification of f0 at regular intervals,\nusually through picking peaks in an autocorrelation\nfunction.\n- Additional processing to select 'best' contour, e.g.\ndynamic programming, subject to smoothness\nconstraints.\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 0\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 10 samples\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 30\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 50\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 70\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 80\n\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0.05\n0.1\n0.15\n0.2\nautocorrelation\nLag = 92 samples\n\nAutocorrelation function\n-0.6\n-0.4\n-0.2\n0.2\n0.4\n0.6\n0.8\n1.2\nlag (samples)\n\nAutocorrelation function\n-0.6\n-0.4\n-0.2\n0.2\n0.4\n0.6\n0.8\n1.2\nlag (samples)\n1 period\n2 periods\n\nPitch Tracking\nCommon errors:\n- Pitch halving: two periods treated as one.\n- Pitch doubling: one period is treated as two.\n- Failure to detect periodicity.\n\nInterpreting f0 Tracks\n- No f0 during voiceless sounds.\n- Consonants can 'perturb' f0.\n- f0 is usually raised after obstruents, more after voiceless obstruent.\n- f0 can fall before obstruents.\n- Creaky voice - often involves very long pitch periods, and\nfluctuating period lengths.\n- Pitch trackers often fail to detect periodicity.\n- Creaky voice is common at the ends of phrases.\n\nPitch Tracking in PRAAT\nParameters:\n- Pitch halving: two periods treated as one.\n- Pitch doubling: one period is treated as two.\n- Failure to detect periodicity."
    },
    {
      "category": "Resource",
      "title": "Laboratory Phonology",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/625076385dca5c496d7b98d85e449814_lec1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n24.910\nLaboratory Phonology\n\nReadings: Johnson (textbook)\n- chapter 1,\n- chapter 2, pp.19-33,\n- chapter 3\n\n- Common vowel inventories:\nI. Vowel inventories\ni u\ni u\ni u\n\ne o\ne\n\no\na\na\na\nArabic,\nNyangumata,\nAleut, etc.\nSpanish,\nSwahili,\nCherokee, etc.\nItalian,\nYoruba,\nTunica, etc.\n\n- Unattested vowel inventories:\ni\ni\ni u\ne\ne\n\na\na\n\nWhy?\nɔ\n\nII. Perceptual cues and the distribution of\nphonological contrasts\n- Phonological contrasts generally have restricted\ndistributions.\n- E.g. Lithuanian voicing contrasts\na. obstruent voicing is distinctive before vocoids and consonantal\nsonorants:\naukle\nnukniauti\nauglingas\ndregna\nsilpnas\nrytmetys\nskobnis\nbadmetys\nb. obstruent voicing is neutralized (to voiceless) word-finally:\n[dauk]\n[kat]\nc.obstruent voicing is neutralized before any obstruent (assimilating in\nvoicing to following obstruent):\na[d-g]al\nme[z-d]avau\ndir[p-t]i\nde[k-t]i\n\nII. Perceptual cues and the distribution of\nphonological contrasts\nDifferent contrasts have different characteristic patterns of\ndistribution (Steriade 1999):\n(i) Obstruent voicing contrasts are permitted only before\nsonorants\n(e.g. German, Lithuanian, Russian, Sanskrit).\n(ii) Major place contrasts (labial vs. coronal vs. dorsal) are\npermitted only before vowels\n(e.g. Japanese, Luganda, Selayarese).\n(iii) Retroflexion contrasts (retroflex vs. apical alveolar) are\npermitted only after vowels\n(e.g. Gooniyandi, Miriwung, Walmatjari).\n\nII. Perceptual cues and the distribution of\nphonological contrasts\nHypothesized explanation: 'The likelihood that distinctive\nvalues of the feature F will occur in a given context is a\nfunction of the relative perceptibility of the F-contrast in\nthat context' (Steriade 1999).\n- Contrasts differ in their distribution of cues so they are\nsubject to different patterns of neutralization.\n- Obstruent voicing is best cued by Voice Onset Time - only\nrealized with a following sonorant.\n\nThe phonetics and phonology of\nretroflex consonants\ndental [l]\nretroflex []\nMRI images of Tamil laterals (Narayanan et al 1999)\nFigure by MIT OpenCourseWare, adapted from Narayanan, Shrikanth, Dani Byrd, and Abigail Kaun.\n\"Geometry, Kinematics, and Acoustics of Tamil Liquid Consonants.\" The Journal of the Acoustical Society of America 106, no. 4 (October 1999): 1993-2007.\nTwo MRIs\nof\nheads: On\ne with tongue in resting position, the other with tongue curled upward.\n\nThe phonetics and phonology of\nretroflex consonants\napical alveolar [t]\nretroflex []\nMalayalam\nCourtesy of Ashtu Killimangalam. Used with permission.\n\nDistribution of retroflexion contrasts in\nGooniyandi (Steriade 1995)\n\nDistribution of retroflexion contrasts in\nGooniyandi\n\nDistribution of retroflexion contrasts in\nGooniyandi\nSummary:\n- Contrast between retroflex and apical alveolar after vowels\nV_#, V_V\n- No contrast elsewhere #_, V_C\n- This pattern of distribution is common in Australian and\nDravidian languages.\n- An unusual pattern of distribution - major place contrasts,\nvoicing contrasts occur preferentially before vowels.\n\nDistribution of retroflexion contrasts\nExplanation (Steriade 1995, etc):\n- The primary cues to the contrast between retroflex and\napical alveolar are located in the VC transitions (unlike\nmajor place contrasts.\n- Most retroflex consonants are retroflexed at closure, but\nthe tongue tip moves forward during closure.\n- At release tongue tip position is similar to an apical\nalveolar, consequently the release and CV transitions of\nthe two consonant types are similar.\n- Contrasts preferentially appear in environments where they\nare better cued.\n\nWarlpiri [] from onset of closure to post-release: Butcher 1993\nDia\ngra\nm o\nf a\nse\nrie\ns o\nf t\nong\nue\npos\niti\nons\n.\nFigure by MIT OpenCourseWare. Adapted from Butcher, Andrew. \"The Phonetics of Australian Languages.\" Flinder University, South Australia, 1993. Unpublished manuscript.\n\nWarlpiri [t] from onset of closure to post-release\nDia\ngra\nm o\nf a\nse\nrie\ns o\nf to\nngu\ne p\nosi\ntio\nns.\nFigure by MIT OpenCourseWare. Adapted from Butcher, Andrew. \"The Phonetics of Australian Languages.\" Flinder University, South Australia, 1993. Unpublished manuscript.\n\nDistribution of retroflexion contrasts\n- Acoustic studies provide evidence concerning the\ndifferences between apical alveolar and retroflex\nconsonants.\n- Articulatory studies help to explain the observed\nacoustic patterns.\n- Perceptual studies confirm that retroflexion\ncontrasts are more difficult to discriminate in the\nabsence of a preceding vowel (Anderson 1997).\n- Phonological theory to relate these properties to\nthe observed distribution of retroflexion contrasts.\n\nIII. Focus and intonation in English\n- Focus - \"the informative part of an utterance\".\n- 'the information in the sentence that is assumed by the\nspeaker not to be shared by him and the hearer'\n(Jackendoff 1972).\n- E.g. Question/answer pairs:\na. (When did John paint the shed?)\nJohn painted the shed YESTERDAY.\n#JOHN painted the shed yesterday.\nb. (Who painted the shed yesterday?)\nJOHN painted the shed yesterday.\n#John painted the shed YESTERDAY.\n\nIII. Focus and intonation in English\n'Focus sensitive particles' make the truth\nconditions of a sentence dependent on the\nlocation of focus:\n1. Jan only gave BILL money.\n2. Jan only gave Bill MONEY.\n-\nFocus is marked by some kind of\nprominence. What is this exactly?\n\nBroad focus:\n'what happened'\nTime (s)\n2.00885\n3.47811\nTime (s)\n2.61383\n4.08096\nTime (s)\n2.79811\n4.23252\nverb focus\nTime (s)\n2.55219\n3.94669\nAnnabel married Maloney\nAnnabel married Maloney\nObject focus:\n'Who did Annabel marry?'\nAudio:\n1_broad.wav\nAudio:\n1_subj.wav\nAudio:\n1_obj.wav\nSubject focus:\n'who married Maloney?'\nVerb focus:\n'what did Annabel do to Maloney?'\nAudio:\n1_verb.wav\n\nSpeech cha\nin di\nagram: Listen\ner/sp\neaker di\nvisio\nn, with flow\nof so\nund throug\nh org\nans.\nFigure by MIT OpenCourseWare. Adapted from Denes, Peter B., and Elliot N. Pinson.\nThe Speech Chain: The Physics and Biology of Spoken Speech. 2nd ed. New York, NY: W. H. Freeman, 1993. ISBN: 9780716723448.\n\nArticulation-\nThe speech production systemAnatomical line drawing of speech production system.\nFigure by MIT OpenCourseWare. Anatomical line drawing of speech production system.\nAnatomical\nline drawing\nof spe\nech\nprodu\nction\nsystem.\n\nThe vocal tract\nAnatomical ill\nustration\nof the voc\nal tr\nact.\nFigure by MIT OpenCourseWare.\n\nArticulatory description of speech sounds\nConsonants:\n- Voicing\n- Place of articulation\n- Manner\n- Lateral/Central\n- Nasal/Oral\n- [s] voiceless alveolar central oral fricative\n\nArticulatory description of speech sounds\nVowels:\n- High-low\n- Front-back\n- Rounded-unrounded\n- [e] mid front unrounded vowel\n\nMovie removed due to copyright restrictions.\nPlease see \"Tongue Video\" in Peter Ladefoged's Vowels\nand Consonants.\n\nIntroduction to acoustics\n- Sound consists of pressure fluctuations in a\nmedium...\n...which displace the ear drum in such a way as to result in\nstimulation of the auditory nerve.\nanimation\n\nSpeech acoustics\n- Movements at a source produce a sound\nwave in the medium which carries energy to\nthe perceiver.\n- Pressure fluctuations move through space,\nbut each air particle moves only a small\ndistance.Series of orbs and cylinders in a line.\nAnimated image of longitudinal pressure wave removed due to copyright restrictions.\n\nRepresenting sound waves\nSine\n-like wa\nv\ne with troughs\nrepresenting ra\nr\ne\nf\na\nc\nt\ni\no\nn\n\nand peaks as compression.\nImage by MIT OpenCourseWare. Adapted from The Physics Classroom Tutorial.\n\nPeriodic sounds\n- A waveform is periodic if it repeats at regular intervals.\n- Frequency of a wave is the number of cycles occurring per\nunit of time.\n- Units: 1 Hertz (Hz) is 1 cycle/second\n\n- Voiced sounds have complex (quasi-)periodic wave forms.\n- The perceived pitch of a sound depends on its frequency.\nPeriodic sounds\nT\nime\nvs\n. air\npressure: Steep\nToscillations.\nFigure by MIT OpenCourseWare.\n\n- Aperiodic sounds have waveforms that do not repeat.\n- Fricative noise is aperiodic.\nAperiodic sounds\nSegment of [s]\nTime (s)\n0.0115323\n-0.09549\n0.1253\n\nWaveform of a sentence\nPlease pass me my book\nAu\ndi\no\nwa\nve\nfo\nrm\n\nw\ni\nt\nh\n\nn\nu\nmber markers from 1 to 16, \"Please pass my book.\"\nFigure by MIT OpenCourseWare.\n\nSpectrums and spectrograms\n- The spectrum of a sound plays a central role\nin determining its quality or timbre.\n\nSpectral representation\n- Any complex wave can be analyzed as the combination of\na number of sinusoidal waves of different frequencies and\nintensities (Fourier's theorem).\n- In the case of a periodic sound like a vowel these will be\n- the fundamental frequency\n- multiples of the fundamental frequency (harmonics)\n- The quality of a periodic sound depends on the relative\namplitude of its harmonics.\n\nSpectral representation\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\nFundamental frequency\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n2nd harmonic\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n-2\n-1.5\n-1\n-0.5\n0.5\n1.5\n0.005\n0.01\n0.015\n0.02\n\nFrequency\n(Hz)\nAmplitude\n0.6\n0.45\n0.3\n0.1\n0.2\n0.4\n0.6\n0.8\n1.2\nfrequency (Hz)\nSpectral representation\nPower spectrum\n-\nPhase differences are relatively unimportant to sound quality, so key\nproperties of a complex wave can be specified in terms of the\nfrequencies and amplitudes of its sinusoidal components.\n\nIdealized vowel spectrum\nFreq\nuenc\ny (H\nz) v\ns\n.\ni\nnt\nen\nsity level (dB), peak, tou\ngh, smaller peak.\nFigure by MIT OpenCourseWare.\n\nFrequency (Hz)\n[æ]\nvowel spectrum\n\nVowel quality\n- The quality of a vowel depends on the\nshape of its spectrum.\n- The shape of the spectrum depends on the\nshape of the vocal tract.\nFrequency (Hz)\nFrequency (Hz)\n[æ]\n[ɪ]\n\n- The peaks in the spectrum of a vowel are\ncalled formants.\n- Perceived vowel quality depends primarily\non the frequencies of the first three\nformants.\nVowel quality\nFrequency (Hz)\nFrequency (Hz)\n[æ]\n[ɪ]\n\nSpectrograms\nBox\nw\nith b\nlue l\nine\ns that look like topographical map of mountains.\nFigure by MIT OpenCourseWare.\n\nm æ s ə t ʃ u s əʔ s m ɪ ɹ ə k ɫ\n1 second\nHz\nHz\n\nFrequency (Hz)\nFrequency (Hz)\n-20\nFrequency (Hz)\nnarrow band\n(long window)\nbroad band\n(short window)\n\nFrequency (Hz)\n-20\nFrequency (Hz)\nFrequency (Hz)\n-20\nTime (s)\n3.46955\n3.7731\n3.55\n3.59\n3.69\n\nSpectrogram image removed due to copyright restrictions.\nSee: http://hctv.humnet.ucla.edu/departments/linguistics/VowelsandConsonants/course/chapter8/8.3.htm\n\nF2 (Hz)\nF1 (Hz)\nGra\nph\nwit\nh d\nata\npo\nint\ns (n\no li\nne c\nonne\nc\nt\ni\nn\ng\nthem) in general shape of a V.\nFigure by MIT OpenCourseWare. Adapted from Peter Ladefoged. A Course in Phonetics. 5th ed. Berlin, Germany: Heinle, 2005. ISBN: 9781413006889.\nAvailable at: http://www.phonetics.ucla.edu/course/contents.html\næ\nϪ"
    },
    {
      "category": "Resource",
      "title": "Licensing by Cue",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/e0847d3c8accd855a03ab148f93256d4_lec6_cues.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n24.910\nLaboratory Phonology\nLicensing by CueVowel and stricture duration diagram.\nFigure by MIT OpenCourseWare.\nVowel and stricture dur\nation diagram.\n\nReadings:\n- ToBI tutorial, 1.0, 2.0, 2.1\n- Johnson pp.28-31 (on pitch tracking).\n- Ladd chapter 3\nAssignment:\n- Make stimuli for voicing perception experiment.\n\nII. Perceptual cues and the distribution of\nphonological contrasts\n- Phonological contrasts generally have restricted\ndistributions.\n- E.g. Lithuanian voicing contrasts\na. obstruent voicing is distinctive before vocoids and consonantal\nsonorants:\naukle\nnukniauti\nauglingas\ndregna\nsilpnas\nrytmetys\nskobnis\nbadmetys\nb. obstruent voicing is neutralized (to voiceless) word-finally:\n[dauk]\n[kat]\nc.obstruent voicing is neutralized before any obstruent (assimilating in\nvoicing to following obstruent):\na[d-g]al\nme[z-d]avau\ndir[p-t]i\nde[k-t]i\n\nII. Perceptual cues and the distribution of\nphonological contrasts\nDifferent contrasts have different characteristic patterns of\ndistribution (Steriade 1999):\n(i) Obstruent voicing contrasts are permitted only before\nsonorants\n(e.g. German, Lithuanian, Russian, Sanskrit).\n(ii) Major place contrasts (labial vs. coronal vs. dorsal) are\npermitted only before vowels\n(e.g. Japanese, Luganda, Selayarese).\n(iii) Retroflexion contrasts (retroflex vs. apical alveolar) are\npermitted only after vowels\n(e.g. Gooniyandi, Miriwung, Walmatjari).\n\nII. Perceptual cues and the distribution of\nphonological contrasts\nHypothesized explanation: 'The likelihood that distinctive\nvalues of the feature F will occur in a given context is a\nfunction of the relative perceptibility of the F-contrast in\nthat context' (Steriade 1999).\n- Contrasts differ in their distribution of cues so they are\nsubject to different patterns of neutralization.\n- Obstruent voicing is best cued by Voice Onset Time - only\nrealized with a following sonorant.\n\nSteriade (1997) - obstruent voicing\n-\nImplicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)\nmore cues to voicing available\nTable of 5 lang\nuages and\nimplic\nationa\nl uni\nversals\nof obstruent/son\nora\nnt voicing.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology:\nThe Case of Laryngeal Neutralization.\" Manuscript, UCLA, 1997. (PDF)\n___\n\nThe phonological analysis\n- Russian voicing (like Lithuanian)\n- Rule-based analysis:\n- /vrag/, /brat/, /led/, /prosj/ /-a/\n- final devoicing\n- voicing assimilation\n-son\n[\n]→[-voice]/ __ #\n-son\n[\n]→[αvoice]/ __ -son\nαvoice\n⎡\n⎣ ⎢\n⎤\n⎦ ⎥\n\nOptimality Theory\n- Optimality Theory separates 'problems' from 'solutions'\n-\nE.g. *[+voice, -son]#\n(provisional formulation)\n-\n*[αvoice, son][-αvoice, -son]\n- These are Markedness constraints - they ban dispreferred\nsound sequences and configurations.\n- The other basic kind of constraints are Correspondence\nconstraints - they require output forms to be as similar to\nthe input underlying form as possible.\n- If a underlying form like /sled/ is realized as [sled], it\nwould violate *[+voi, -son]#.\n- If it is changed to [slet] to satisfy the markedness\nconstraint, that violates the correspondence constraint\nIdent(voice).\n\nOptimality Theory\n-\nConstraint conflict: Any output violates one constraint or the other:\n/sled/\n*[+voi,-son]#\nIdent(voice)\n)\nsled\n*\nslet\n*\n-\nConflict is resolved by reference to a ranking of the constraints: the\nhigher ranked constraint prevails:\n/sled/\n*[+voi,-son]#\nIdent(voice)\nsled\n*!\n)\nslet\n*\n\nOptimality Theory\nComponents of Optimality Theory:\n- A set of constraints\n- Markedness constraints - define dispreferred\nconfigurations.\n- Correspondence constraints - penalize deviations\nfrom identity between input and output.\n- Generation function: takes an input (e.g. /sled/) and\ngenerates all possible output candidates (sled, slet, sle,\nslen, led, let, etc).\n- Evaluation function: given a ranked set of constraints,\nidentifies the candidate that best satisfies the constraint\nranking.\n- That's the actual output.\n\nOptimality Theory\n-\nDifferent rankings of the same constraints (different grammars) yield\ndifferent outputs:\n/sled/\nIdent(voice)\n*[+voi,-son]#\n)\nsled\n*\nslet\n*!\n/sled/\n*[+voi,-son]#\nIdent(voice)\nsled\n*!\n)\nslet\n*\n\nOptimality Theory and Typology\n- It is hypothesized that most constraints are universal - i.e.\nthe same in every language.\n- Languages differ in the ranking of those constraints.\n- So cross-linguistic similarities in phonological systems\nresult from shared constraints.\n- Where do these universal constraints come from?\n- Hypothesis: Most constraints arise from:\n- basic requirements for rapid, robust communication\n- limitations of speech production and perception\napparatus\n- cognitive limitations\n- Common to all languages.\n\nTypology of Voicing Neutralization\n- Basic requirement for rapid, robust communication:\nperceptually distinct contrasts.\n- cf. analysis of vowel inventories.\n- Steriade (1997, 1999) argues that perceptual considerations\nshape the typology of voicing neutralization:\n- Less distinct contrasts are dispreferred.\n- Specifically: contrasts are neutralized first in\nenvironments where they would be less distinct.\n\nSteriade (1997) - obstruent voicing\n-\nImplicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)\nmore cues to voicing available\nTable of\nlan\ngu\nage\ns and implicatio nal universals of obstruent/sono rant voicing.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology:\nThe Case of Laryngeal Neutralization.\" Manuscript, UCLA, 1997. (PDF)\n___\n\nSteriade (1997) - obstruent voicing\n- Implementation: constraints against obstruent\nvoicing contrasts in different contexts.\n- Ranked according to the strength of the cues\navailable in that context (fixed ranking).Chart of enviornment of voicing contrast and cues.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology: The Case of Laryngeal Neutralization.\"\nManuscript, UCLA, 1997. (PDF)\n___\nChart of en\nviornment of voici\nng c\nontrast and cues\n.\n\nAnalyses\n-\nRussian:\n*[αvoice]/_[-son] >> * [αvoice]/_# >> Ident(voice) >> *[αvoice]/_[+son]\n-\nHungarian:\n*[αvoice]/_[-son] >> Ident(voice) >> * [αvoice]/_# >> *[αvoice]/_[+son]\n*[αvoice]/_#\nIdent(voice)\n*[αvoi]/_[+son]\n*[+voi, -son]\nsled-slet\n*!\n)\nslet\n*\nsled\n*\n*!\n*[αvoice]/_#\nIdent(voice)\n*[αvoi]/_[+son]\n*[+voi, -son]\n) sleda-sleta\n*\nslet\n*!\n*\nsled\n*!\n*\n\nHungarian\n- Contrast before sonorants\nvedmeg\n'buy it!'\na:tmeɟ\n'to cross'\n- Contrast word-finally\nrɔb\n'prisoner'\nkɔlɔp\n'hat'\nha:z\n'house'\nkɛrt\n'garden'\n- Neutralization before obstruents\nha:s-to:l\n'from the house' kɛrd-bɛ\n'in the garden'\n*-zt-\n*-tb-\n\nHungarian\n-\nRussian:\n*[αvoice]/_[-son] >> * [αvoice]/_# >> Ident(voice) >> *[αvoice]/_[+son]\n-\nHungarian:\n*[αvoice]/_[-son] >> Ident(voice) >> * [αvoice]/_# >> *[αvoice]/_[+son]\nIdent(voice)\n*[αvoice]/_#\n*[αvoi]/_[+son]\n*[+voi, -son]\nha:z-ha:s\n*\n*!\n*!\nha:s\n*\nha:z\n*\nIdent(voice)\n*[αvoice]/_#\n*[αvoi]/_[+son]\n*[+voi, -son]\n) edme-etme\n*!\n*!\netme\n*\n*\nedme\n*\n)\n\nNeutralization with assimilation\n- *TD: *[-voice][-son, +voice]\n-\nIt is more difficult to initiate voicing during an obstruent than to\nmaintain voicing during an obstruent.\n-\nUniversal ranking: *TD >> *[-son, +voice]\n*[αvoi]/\n_[-son]\nIdent(voi)\n*[αvoice]/\n_#\n*TD\n*[+voi, -son]\nkɛrtbe - kɛrdbɛ\n*!\n*\n***\nkɛrtbe\n*\n*!\n*\n)\nkɛrdbɛ\n*\n**\n\nSpeech perception and phonology\n- Steriade's analysis of the typology of obstruent voicing\ndepends on the ranking of *[voice] constraints.\n- This is supposed to follow from the relative strength of\ncues to voicing available in each context.\n- What are these cues? How do we know how strong they\nare?Chart of enviornment of voicing contrast and cues.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology: The Case of Laryngeal Neutralization.\"\nManuscript, UCLA, 1997. (PDF)\n___\nChart of en\nviornment of voici\nng c\nontrast and cues\n.\n\nSpeech perception\n- The problem faced by the listener: To extract meaning\nfrom the acoustic signal.\n- This involves the recognition of words, which in turn\ninvolves discriminating the segmental contrasts of a\nlanguage.\n- Much phonetic research in speech perception has been\ndirected toward identifying the perceptual cues that\nlisteners use.\n\nSpeech perception\n-\nProduction studies can reveal many differences between minimal\ncontrasting words, e.g. contrasting vowels of English differ in formant\nfrequencies and duration.\n-\nAre listeners sensitive to these differences in speech perception?\n-\nWhat is the nature of the perceptual representations of speech?\n-\nThese questions are addressed through perceptual experiments (cf.\nJohnson p.70).\n-\nMost direct test of perceptual significance of an acoustic property:\nmanipulate the acoustic property synthetically and see if perceptual\nresponse is affected. E.g. vary formant frequencies in synthetic vowels,\nand have subjects categorize the vowels.\n\nCues to vowel quality\n- The main cues to vowel quality are related to the\nfrequencies of the first two or three formants.\n\nCues to consonant contrasts\n- Place cues (Wright, Frisch and Pisoni 1999)\nVocal chord illust\nrations showing\nstop release\nburst, fricative no\nise, and spacing.\nFigure by MIT OpenCourseWare. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. \"Speech Perception.\" In Wiley Encyclopedia\nof Electrical and Electronics Engineering. Vol. 20. New York, NY: John Wiley and Sons, 1999, pp. 175-195.\n\nCues to consonant contrasts\n- Manner cues (Wright, Frisch and Pisoni 1999)\nVocal chords showi\nng abruptness and att\nenuation, nasa\nlization, formant s\ntr\nuc\ntu\nr\ne\n.\nFigure by MIT OpenCourseWare. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. \"Speech Perception.\" In Wiley Encyclopedia\nof Electrical and Electronics Engineering. Vol. 20. New York, NY: John Wiley and Sons, 1999, pp. 175-195.\n\nCues to consonant contrasts\n- Obstruent voicing cues (Wright, Frisch and Pisoni 1999)\nVocal chord diagram: Re\nle\nas\ne\nburst amplitude,\naspiration\nno\nise, and vowel\n& stricture d\nuration.\nFigure by MIT OpenCourseWare. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. \"Speech Perception.\" In Wiley Encyclopedia\nof Electrical and Electronics Engineering. Vol. 20. New York, NY: John Wiley and Sons, 1999, pp. 175-195.\n\nThe nature of acoustic cues\n-\nThere are multiple cues to every contrast - the speech signal is\nhighly redundant.\n-\nE.g. stop voicing in English\n1.\nLow-frequency spectral energy, periodicity (Stevens and\nBlumstein 1981:29)\n2.\nVoice onset time (Lisker 1975)\n3.\nAmplitude of aspiration (Repp 1979)\n4.\nAmplitude of release burst (Repp 1979)\n5.\nClosure duration (Lisker 1957)\n6.\nDuration of the preceding vowel (Massaro and Cohen 1983)\n7.\nF1 adjacent to closure (Lisker 1975, Raphael 1972)\n8.\nf0 adjacent to the closure (Haggard, Ambler and Callow 1970)\n9.\nAmplitude of F1 at release (Lisker 1986).\n\nWhalen, Abramson, Lisker & Mody (1993).\nSyn\nth\nes\niz\ner\nV\nOT\nv\ns.\np\nro\npo\nrt\nio\nn\nof\n[\n-v\noi\nc\ne\n]\njudg\nements\n: Upwa\nrd cur\nves of\nbi\nlabial, velar, alveolar.\nFigure by MIT OpenCourseWare.\n\nRaphael (1972) JASAVoiced and voiceless graphs of 3 different sets of similar sounds: Shaded horizontal bands.\nV\no\nwe\nl du\nra\nti on\n: 4 graphs with 3 curves each\nt\nh\nat\nar\ne\nde\ncr\nea\ns\ning\no\nve\nr\nti\nme\n,\nsp\naci\nng\ndif\nfer\ning\n.\nFigure by MIT OpenCourseWare.\nFigure by MIT OpenCourseWare.Voiced and voiceless graphs of 3 different sets of similar sounds: Shaded horizontal bands.\nð\ng\nVoiced\nand v\noiceless\ngraphs\nof 3 dif\nferent se\nts of\nsimila\nr sou\nnds:\nShaded\nho\nriz\nont\nal ban\nds.\n\n.\nThe distribution of acoustic cues\n-\nCues to a contrast are temporally distributed and cues to more than one\ncontrast may be present in the signal simultaneously (i.e. no strict\nsegmentation).\n-\nThe availability and nature of the cues to a given contrast type vary\nsystematically with context.\n-\nThis observation is central to 'licensing by cue' analyses of the\ndistribution of phonological contrasts.\n-\nBroad distinction between internal and external cues to a contrast:\n-\nInternal: cues realized during the segment itself, e.g. vowel formants,\nfricative noise.\n-\nExternal: cues realized on adjacent segments, e.g. VOT, formant\ntransitions.\n-\nExternal cues typically depend on the presence of particular segment\ntypes in the context, e.g. VOT requires a following voiced sonorant.\n-\nSo the presence of external cues can be highly variable across\ncontexts.\n\nSteriade (1997) - obstruent voicing\n- Markedness of obstruent voicing contrast in context C\ndepends on strength of cues to voicing in C.\nAssumptions:\n-\nMore cues are better.\n-\nRelease cues are stronger than preceding/closure cues.Chart of enviornment of voicing contrast and cues.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology: The Case of Laryngeal Neutralization.\"\nManuscript, UCLA, 1997. (PDF)\n___\nChart of en\nviornment of voici\nng c\nontrast and cues\n.\n\nSteriade (1997) - obstruent voicing\n-\nImplicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)\nTable of 5 lang\nuages and\nimplic\nationa\nl uni\nversals\nof obstruent/son\nora\nnt voicing.\nFigure by MIT OpenCourseWare. Adapted from Steriade, Donca. \"Phonetics in Phonology:\nThe Case of Laryngeal Neutralization.\" Manuscript, UCLA, 1997. (PDF)\n___\n\nAssessing relative cue strength\n-\nCompare discrimination/identification of contrasting sounds in\ndifferent contexts under the same conditions.\n-\nE.g. Wright (2003) compares perception of [b][d][g] before vowels\nand after vowels in noise [ba/da/ga], [ab/ad/ag].\n-\nWe want to know about [ba] vs. [pa], [ab] vs. [ap].\n-\nConflicting cues: Create stimuli with conflicting cues to a contrast and\nsee which prevails.\n-\nE.g. Fujimura et al: cross-spliced [ab-] from [aba] and [-da] from\n[ada]\n-\nclosure transitions cue [b], release transitions+burst cue [d]\n-\nWhat do listeners perceive? I.e. which cues dominate?\n-\nTry this for voicing in stops and fricatives.\n-\ncreate four stimuli each, 2 stops, 2 fricatives.\n-\nsend the sound files to me.\n\nCategorical perception\n- Strict categorical perception is said to occur where\ndiscrimination performance is limited by identification\nperformance, i.e. listeners only have access to category\nlabels, so stimuli can only be distinguished if they are\nidentified as belonging to different categories.\n- Tested in two stages:\n-\nIdentification of a synthetic continuum\n-\nDiscrimination of stimuli from the continuum\n\nCategorical perception\n- E.g. Liberman (1970) place of articulation F2\ntransition continuum, b-d-g.\nFigure by MIT OpenCourseWare. Adapted from Liberman, A. M. \"Some Characteristics of Perception in the Speech Mode.\" Perception and its Disorders 48 (1970):\n238-254. And Liberman, A. M. \"Discrimination in Speech and Nonspeech Modes.\" Cognitive Psychology 2 (1970): 131-157.\nTim\ne v\ns. Hz: Line\ns (-\n\nto\n+9)\nconv\ner\nge\n,\nth\nen\nh\nor\niz\non\nta\nl\nli\nne\n.\nal\ns\no: S\nharp rise and horizontal line.\n\nCategorical perception\n-\nIdentification: Subjects identify stimuli as b, d, g\n-\nDiscrimination: Subjects are presented with pairs of stimuli and asked\nto judge whether they are the same or different.\n- Relatively abrupt\ntransitions in\nidentification functions.\n- Peaks in discrimination\nfunction at the category\nboundary\n6 graphs: 3 dis\ncrimination (eac\nh\nwi\nt\nh\nsh\nar\np\np\nea\nk\ns)\nan\nd\ni\nd\nent\nif\nic\na\nti\non\n(\ni\nnv\ner\nse\n\nbe\nll curves).\nFigure by MIT OpenCourseWare. Adapted from Liberman, A. M. \"Some Characteristics of Perception in the Speech Mode.\" Perception and its Disorders 48 (1970): 238-254.\nAnd Liberman, A. M. \"Discrimination in Speech and Nonspeech Modes.\" Cognitive Psychology 2 (1970): 131-157.\n\nCategorical perception\n- Discrimination has never been found to be precisely\npredictable from identification - Discrimination is always\nbetter than predicted.\n- More loosely, categorical perception is sometimes said to\nbe exhibited where there is a discrimination peak at the\ncategory boundary determined by identification, even if the\nrelationship is not precisely as predicted.\n- A sharp transition in the 'identification function' for a\nstimulus continuum is not categorical perception in any\ntechnical sense.\n\nWhy is categorical perception significant?\n-\nThe (loose) categorical perception pattern contrasts with the pattern\nobserved in psychophysical experiments using non-speech stimuli:\n\"Typically, nonspeech stimuli that vary acoustically along a single\ncontinuum are perceived continuously, resulting in discrimination\nfunctions that are monotonic with the physical scale\" (Luce and\nPisoni, p.31).\n-\nThis contrast was used by Liberman and others to argue that speech\nperception is 'special' - i.e. it uses special mechanisms, not the general\nmechanisms of non-speech auditory perception.\n\nWhy is categorical perception significant?\n-\nVowels are not usually perceived categorically, even in the loose sense\n(Luce and Pisoni and refs there).\n-\nThe argument for specialness from categorical perception has been\nweakened by:\n-Evidence for categorical perception of non-speech sounds (noise-\nbuzz, Miller et al 1976).\n-Evidence that Chinchillas perceive a VOT continuum categorically\n(Kuhl and Miller 1975).\n-\nRecent data from eye-tracking studies suggest that perception may\ndepend gradiently on the speech signal - categorical effects are\nintroduced by decision processes (McMurray et al 2002)."
    },
    {
      "category": "Resource",
      "title": "Phonetics and Phonology of accent Variation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/0070571e3f90150f0605178247b62ed2_lec11_accents.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n24.910 Topics in Linguistic Theory: Laboratory Phonology\nSpring 2007\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nPhonetics and phonology of\naccent variation\n\nAccents and Dialects\nDialects of English can differ in all aspects of grammar\n- Lexicon\n- soda, coke, pop\n- Syntax\n- I might do\nvs.\nI might\n- The house needs painted\n(W. PA, E. Ohio, Scots)\n- The house needs painting\n- Phonology\n- Phonetics\n- 'Accent' refers to phonetics and phonology only.\n\nAccents of English\nAccents can differ in all aspects of phonology/phonetics\n- Phoneme inventory - differences in the number and\narrangement of phonemes.\n- Phonological rules/phonotactics\n- Phonetic realization - differences in the detailed realization of\nphonemes.\n\nDifferences in phoneme inventory\n- Contrast /ɑ ɔ/, e.g. Inland North, Atlantic States\n- Only /ɑ/, West, NE New England\n- Homophones: cot-caught, Don-dawn, hock-hawk\nMap of contiguous USA with data points of contrast in speech product\nion of vowel sounds.\nFigure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.\n\nDifferences in distribution of contrasts\n- All accents contrast /ɪ, ɛ/.\n- In some accents (e.g. South) this contrast is neutralized before\nnasals.\nphɪn\n'pin, pen'\nhɪm\n'him, hem'\nmɪni\n'many, mini'\nlɪŋkθ\n'length'\n\nDifferences in distribution of contrasts\nMap of contiguou\ns USA with d\nata points of contrast in speech production of vowel sounds.\nFigure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.\n\nDifferences in allophonic rules\n- California English /æ/ →[ɪæ]/ _ [+nasal]\nhad\nstand\nTime (s)\n0.250422\n0.560103\nTime (s)\n92.6242\n92.9631\n[Audio clip removed due to\ncopyright restrictions]\nListen:\nhttp://www.stanford.edu/~eckert/sounds/stand.wav\n\n-\nCalifornian speakers (M open, F\nclosed) (Hagiwara 1997).\n-\n/ʉ/\n-\n/ʌ/\n\n-\nN. Midwest speakers (M open, F\nclosed) (Hillenbrand et al 1995).\n-\n[ʌ>]\n[Listen:\nhttp://www.stanford.edu/~eckert/sounds/move.wav]\n[Audio clip removed due to\ncopyright restrictions]\n[Listen:\nhttp://www.stanford.edu/~eckert/sounds/fund.wav]\nDifferences in realizations of phonemes\nFigures by MIT OpenCourseWare.\ni\ni\nu\n3000 2500\nε\nSecond formant\nFirst formant\n\nε\nu\n3000 2500\nSecond formant\nFirst formant\n\ni\ni\nε\nε\nu\nu\nHz\nHz\nΛ\nΛ\nΛ\nΛ\nΩ\nΩ\nΩ\nΩ\nɑ\nɑ\nɑ\nɑ\nI\nI\nI\nI\n\nDescribing English Accents\n-\nNorthern /u/ and California /ʉ/ are corresponding phonemes\nin the two accents because they generally occur in the same\nwords.\n-\nwhere Northern has /u/ Californian has /ʉ/.\n-\nSo a convenient way to refer to vowel phonemes in describing\naccents is in terms of the words in which they appear.\n-\nWells (1982) proposes a set of keywords for referring to\nclasses of words that (generally) share a vowel phoneme, e.g.\n- KIT, DRESS, TRAP, LOT, STRUT, etc.\n\nDescribing English accents - an historical\napproach\n-\nThe 'keyword' approach works because of the approximate\ncorrectness of two assumptions:\n-\nAll accents of English are descended from the same\nlanguage via sound change.\n-\nSound change is regular ('Neogrammarian') -\nexceptionless and phonetically conditioned.\n-\nLabov takes an explicitly historical approach to description of\naccents.\n-\naccents are described in terms of changes from an 'initial\nposition' - 'our best estimation of the common base for American\nEnglish dialects which resulted from the mixing of various English\ndialects in the 16th and 17th centuries'\n\nSome differences between English and US\naccents\n-\nTo a first approximation, the differences between English and\nUS accents are the result of independent sound changes in one\nregion or the other.\n-\nE.g. a Southern English innovation: loss of post-vocalic /ɹ/\nstɑɹ\n> stɑ\n'star'\nfɔɹ\n> fɔ\n'for'\nstɑɹɪŋ > stɑɹɪŋ\n'star'\n-\nɹ > Ø / _ {C, #}\n\nLOT-PALM merger\n-\nA US innovation\nɒ > ɑː\nR.P.\nmost US\nlɒt\nlɑt\n'lot'\nphɑm\nphɑm\n'palm'\nbɒðə\nbɑðə˞\n'bother'\nfɑðə\nbɑðə˞\n'bother'\n/ɑ, ɒ/ /ɑ/\n\nUS innovation: j-deletion\nR.P.\nmost US\nRP/US\nRP/US\nRP/US\nthjun\nthun\n'tune'\nthul\n'tool'\npjuni\n'puny'\nkjut\n'cute'\ndju\ndu\n'dew'\ndu\n'do'\nbjuti\n'beauty'\nhju\n'hue'\nsjut\nsut\n'suit'\nsun\n'soon'\nfju\n'few'\nzjus\nzus\n'Zeus'\nzu\n'zoo'\nvju\n'view'\nnjuz\nnuz\n'news'\nnus\n'noose\nmjuz\n'muse'\n-\nj > Ø / [+coronal] _\n-\nˈvɑljəm, ˈʌnjən\n-\nsynchronic process also.\n\nj-deletion\n-\nActually deletion of [j] started earlier, applying in some environments in\nboth English and US accents:\n-\nɪʊ > ju\n-\nj > Ø / palato-alveolars, Cl, ɹ _\n(or ɪʊ > u)\n17thC\nMost modern\ntʃɪuz\ntʃuz\n'chews'\ntʃuz\ntʃuz\n'choose'\nθɹɪu\nθɹu\n'threw'\nθɹu\nθɹu\n'through'\nflɪu\nflu\n'flew'\nflu\nflu\n'through'\n-\nIn many English accents j-deletion has since applied after [l], e.g. lewd\n\nAn irregular sound change\n-\nRegular sound change applies to all words that contain the\nrelevant sound in the relevant context.\n-\nSome sound changes appear to apply to a subset of words\ngiving rise to complicated differences in lexical distribution.\n-\nUS/UK Englishes both have /æ, ɑ/ but in different words\n-\nstaff, bath, pass, grasp\n-\ndance, answer, demand, grant, example\n-\nUK: æ > ɑ/ _ voiceless fricative, NC\n-\nbut: gas, asp, passage, chaff, (plastic),...\n-\nbut: romance, hand, band, ant, ample,...\n\nRegional Accents in the USA\n-\nTraditional dialectology divides the USA into four major\ndialect areas based primarily on vocabulary (soda vs. pop, etc)\n-\nNorth, Midlands, South, West\n-\nLabov and colleagues (2006) have divided the USA into\nsimilar areas based purely in pronunciation\n-\nAreas are grouped by distinctive combinations of shared\nsound changes - often sound changes in progress.\n\nLabov et al (1997)\nContiguou\ns USA: Urban dialect areas and data markers for vowel sou\nnd differences.\nFigure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.\n\nThe West\nThe West is primarily characterized by a combination of two\ndevelopments:\n-\nCot-caught merger: /ɑ/, no /ɔ/\n-\nSpreading East through the midlands.\n-\nFronting of GOOSE vowel to [ʉ] (similar change in the South\nand elsewhere. Not in North)Californian speakers: Graph of 1st vs. 2nd formant, with 2 curves, both U-shaped.\nFigure by MIT OpenCourseWare.\nC\na\nl\nifo\nrni\nan\nspe\nake\nrs:\nGr\naph\nof\n1st\nvs. 2nd\nform\nant,\nwit\nh\n2 curves, bot\nh U-shaped.\nI\nI\n\nThe North\n-\nGenerally retains 'conservative' long /i, eɪ, u, oʊ/\n-\nInland North characterized by a chain shift, primarily\ninvolving historically lax vowels - Northern Cities Shift.\n-\nChange in progress, most advanced in major cities\n(Buffalo, Rochester, Cleveland, Detroit, Chicago, Madison etc).N. Midwest speakers: Graph of 1st vs. 2nd formant, with 2 curves both V-shaped.\nFigure by MIT OpenCourseWare.\nF\nl\no\nw\n\nd\niagram showing Northern citi\nes\nshift\nof\nvow\nels\n, a\nrro\nws a\nnd el\nlips\nes.\nFigure by MIT OpenCourseWare.\niə\n/ /\n^\nCal\nifo\nrni\nan\nspe\nake\nrs:\nGr\naph\nof 1\nst v s. 2\nnd f\norma\nnt,\nwith 2 curves,\nboth U-shape\nd\n.\n\nNorthern Cities Shift\n-\nChain shift: a series of connected sound changes. Can result in\nwholesale rotations of portions of the vowel system.\n- Earliest stages:\n- fronting of LOT/PALM ɑ > a\nBuffalo\nChicago\nKenosha\n- 'tensing' of TRAP æ > eæ/ɪɛ\nBuffalo\nChicago\nDetroit\n- Less advanced:\n- THOUGHT lowering/unrounding\n- ɔ > ɑ\nRochester\n- STRUT/ʌ/ backing Detroit\n- HEAD/ɛ/ backing\nDetroitFlow diagram showing Northern cities shift of vowels, arrows and ellipses.\nFigure by MIT OpenCourseWare. Flow diagram showing Northern cities shift of vowels, arrows and ellipses.\nThe Northern Cities Shift\n// /\n/ /\nidea\n/e/\nked\n/o/\ncod\n/oh/\ncawed\n/ /\ncud\n/i/\nkid\ncad\niə\n/ /\n^\nAudio files removed due to copyright restrictions.\n\nThe North\n-\nMuch of the North is also characterized by 'Canadian Raising'\n-\nUsually written:\n/aɪ/ →ʌɪ /_ [-voice]\n/aʊ/ →ʌʊ/_ [-voice]\nListen to sound files here\n- \"knife, knives\"\n- \"lout, loud\"\n\nThe South\n-\nThe South is characterized by another series of vowel shifts,\n-\nreferred to as 'the Southern Shift', but it's not clear to me why all\nthe changes should be regarded as part of a single 'chain'.Flow diagram showing southern shift of vowel sounds, arrows and ellipses.\nFigure by MIT OpenCourseWare.\nF\nl\no\nw\n\nd\niagram showing southern shif\nt\nof vo\nwel\nsou\nnds\n, a\nrro\nws a\nnd el\nlipse\ns.\n\nThe South\n-\nOne of the oldest Southern developments is PRICE\nmonophthongization\n-\naɪ > aː (/_[-voice])\n-\nGOOSE /u/ fronting\nTexas\n-\nTHOUGHT /ɔ/ raising/diphthongization Texas\n-\nGOAT /oʊ/ > [əʊ] Texas\n-\nVariably rhotic\nTexas\nAudio files removed due to copyright restrictions.\n\nThe Midlands\n-\nNot very uniform. Primarily characterized by Labov as not\nparticipating in Northern Cities or Souther shifts.\n-\nSome Southern features, e.g. unrounding of GOAT nucleus\n[əʊ].\n-\nThis is obviously a very broad characterization\n-\nmany small areas have distinctive accents that do not fit\nthis classification (New York City, Philadelphia, Eastern\nNew England etc).\n-\nDoes not incorporate cultural variation within regions,\ne.g. African American Vernacular English.\n\nEastern Massachusetts\n-\nThe 'Boston' accent.\n-\nNon-rhotic\nListen:\n11_car.wav\n11_spa.wav\n11_floor.wav\n\nEastern Massachusetts\n-\nThe 'Boston' accent.\n-\nNon-rhotic\n-\nNon-rhotic and variably rhotic accents are primarily found in\nE. New England, NYC, coastal plain of the South.\n-\nBut these areas also contain pockets of continuous\nrhoticity.\n-\nThis patterns seems to have resulted because r-loss spread\nfrom Southern England along trade routes to major ports of\nthe Eastern seaboard, and then to surrounding areas.\n-\nNon-rhotic accents used to be locally prestigious, but have\nlargely lost their prestige and are in retreat.\nListen:\n11_car.wav\n11_spa.wav\n11_floor.wav\n\nEastern Massachusetts\n-\nPost-vocalic /ɹ/ in many contexts is better thought of as\nvocalized ('de-rhotacized') rather than simply deleted.\n-\nMany historical vowel-r sequences are now diphthongs.\n-\nfloor[ɔa] (=/ɒa/?)\nhorse\n-\nhoarse [ʊə]\n-\nThis contrast has been lost in many UK and US accents.\n-\nNEAR [iə]\n-\nSQUARE [eə]\nListen:\n11_floor.wav\n11_horse.wav\n11_hoarse.wav\n\nEastern Massachusetts\nSome unusual features\n-\nNeutralization of LOT/ɒ/-THOUGHT/ɔ/ to /ɒa/\n-\nBoston cot caught hot\n-\nPALM/ɑ/ remains distinct, but fronted /a/\n-\ncar\nspa\n-\nContrast between three front lax vowels before /ɹ/\n-\nMary-merry-marry\nListen:\n11_boston.wav\n11_cot.wav\n11_caught.wav\n11_hot.wav\nListen:\n11_car.wav\n11_spa.wav\nListen:\n11_mary.wav\n11_merry.wav\n11_marry.wav\nListen:\n11_hairy.wav\n11_barry.wav\nMary\nmerry\nmarry\nε\nε\nε\ne\nε\ne\nε\nI\n(fairy, hairy vs. Carey)\n(carry, Harold vs. marry, Barry)\nTable by MIT OpenCourseWare.\n\nCross-dialect Communication\n- Labov points out that advanced Northern Cities\npronunciations could result in apparent word changes for\nspeakers of other accents.\n- on > Ann > Ian\n- block > black\n- Accent differences can lead to confusion, but we regular\ncommunicate across accents, and adapt quickly to new\naccents. How?\n- Two experiments:\n- Evidence that we can take accent into account in\ninterpreting vowels.\n- Evidence of a mechanism for rapid adaptation to new\npatterns of pronunciation.\n\nSpeaker normalization\n- Dealing with dialect variation is conceptually similar to\ndealing with (within dialect) speaker variation, e.g. due to\nvocal tract size.\n- Ladefoged/BroadbentCalifornian speakers: Graph of 1st vs. 2nd formant, with 2 curves, both U-shaped.\nFigure by MIT OpenCourseWare.\ni\nI\ni\nI\nu\n3000 2500\nε\nSecond formant\nFirst formant\n\nε\nu\nHz\nΛ\nΛ\nΩ\nΩ\nɑ\nɑ\n\nCross-dialect Communication\n- Rakerd and Plichta (2003) adapted Ladefoged and\nBroadbent's experimental method to show that perception\nof vowels is influenced by dialect information in the\npreceding context.\n- Synthetic [æ- ] continuum (hat-hot, sack-sock)\n- Speakers and subjects from Detroit and Michigan Upper\nPeninsula.\n- Detroit accent is characterized by fronting of / / and\ndiphthongization of /æ/ (Northern Cities Shift).\n- Synthetic words were placed at the end of carrier phrases\nfrom Detroit and UP speakers.\n#\n\n#\n\nCross-dialect speech perception\n-\nFor Detroit listeners identification of continuum shifted as a function\nof carrier phrase.\nDetroit (LM) carrier\nUP carrier\nBar\n\ng\nr\na\np\nh\n\no\nf\nD\net\nro\nit\n(\nLM\n)\nca\nrr\nie\nr: 7 pairs of bars (LM-LMP and UP-LMP) with LM-LMP higher.\nMB\na\nr\n\ng\nr\na\np\nh\n\nof\nUP\ncar\nri\ner\n:\npa\nir\ns\nof\nb\nars (LM-UPP and UP-UPP) with UP-UPP usually slightly higher.\nFigures by MIT OpenCourseWare.\n\nCross-dialect speech perception\n- Evidence for 'accent normalization' is interesting because\nit cannot be achieved on the basis of the signal.\n- For speaker-normalization, it has often been suggested that\nsignals can be mapped onto a speaker-independent\nrepresentation by a low-level transformation of the signal\n(e.g. formant ratios in place of formants).\n- On the other hand, it has also been argued that speaker\nnormalization requires that the signal be interpreted in\nrelation to a model of the speaker that is constructed based\non a variety of sources of information.\n- Accent normalization fits into the second approach to\nnormalization.\n\nNorris, McQueen & Cutler (2003)\n-\nEvidence for rapid adaptation to a new pattern of pronunciation.\n-\nLexical decision task in Dutch.\n-\nSome words contain a final sound [?] that is ambiguous between [f] and\n[s], created by averaging [f] and [s] waveforms.\n-\npretest to ensure ambiguity.\n-\nThree conditions:\n1.\nWords are meaningful if [?] is interpreted as [s].\n-\nE.g [witlo?] - witlof 'chicory', witlos is not a word.\n2.\nWords are meaningful if [?] is interpreted as [f].\n-\nE.g [na:ldbo?] - naaldbos 'pine forest', naaldbof is not a word.\n3.\nNon-word if [?] is interpreted as either [f] or [s].\n-\nSubjects in each condition hear 20 target words + the other 10 targets\nunedited + fillers.\n-\nSubjects in (1) and (2) accepted edited words as corresponding word.\n\nNorris, McQueen & Cutler (2003)\n-\nAfter lexical decision task, subjects categorized stimuli from\nan [ɛf-ɛs] continuum (same speaker).\n-\nBoundary differed depending on condition in part 1:\n1.\n[?] = [s], more stimuli categorized as [s].\n2.\n[?] = [f], more stimuli categorized as [f].\n3.\nNon-word group did not differ from (1) or (2).\nCon\nti\nnuum and % resp\non\nse\ns\n:\n\nTh\nr\ne\ne curves with\n\nd\na\nta\npoint\ns,\n\nst\nea\ndy decline for\nal\nl\n.\nFigure by MIT OpenCourseWare.\n\nNorris, McQueen & Cutler (2003)\n-\nInterpretation: subjects have learned that speaker has an\nunusual /s/ or /f/ on the basis of hearing this rendition in 20\nwords.\n-\nThis knowledge affects perceptual boundary between /f/ and\n/s/ for that speaker.\n-\ni.e. subjects made a generalization about pronunciation\nof that sound.\n-\nA follow-up study (Cutler et al 2005) followed the training\nphase with a cross-modal priming task (visual lexical decision\nfollowing an auditory prime).\n-\nPriming effect of modified words depended upon the\ninterpretation of [?] learned in the training phase.\n-\nCrucial words had not been heard in the training phase.\n\nAdaptation to a new accent\n-\nThe Norris et al experiment shows that listeners are capable of\nrapid adaptation to a novel accent (novel in one respect).\n-\nPresumably involves:\n- Ability to interpret ambiguous stimuli as words, given\ncontext.\n- Ability to generalize based on segments.\n-\nHow broad is the generalization?\n-\nAll s/f? Word-final s/f? Coda s/f? Word-final s/f after certain\nvowels?\n-\nValue of decomposing words into segments: facilitates rapid\ngeneralization to new speakers\n- Given that variation tends to affect segments in context,\nrather than e.g. individual words. Cf. Regularity of sound\nchange."
    }
  ]
}