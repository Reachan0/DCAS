{
  "course_name": "Principles of Digital Communication II",
  "course_description": "This course is the second of a two-term sequence with 6.450. The focus is on coding techniques for approaching the Shannon limit of additive white Gaussian noise (AWGN) channels, their performance analysis, and design principles. After a review of 6.450 and the Shannon limit for AWGN channels, the course begins by discussing small signal constellations, performance analysis and coding gain, and hard-decision and soft-decision decoding. It continues with binary linear block codes, Reed-Muller codes, finite fields, Reed-Solomon and BCH codes, binary linear convolutional codes, and the Viterbi algorithm.\nMore advanced topics include trellis representations of binary linear block codes and trellis-based decoding; codes on graphs; the sum-product and min-sum algorithms; the BCJR algorithm; turbo codes, LDPC codes and RA codes; and performance of LDPC codes with iterative decoding. Finally, the course addresses coding for the bandwidth-limited regime, including lattice codes, trellis-coded modulation, multilevel coding and shaping. If time permits, it covers equalization of linear Gaussian channels.",
  "topics": [
    "Engineering",
    "Electrical Engineering",
    "Digital Systems",
    "Telecommunications",
    "Engineering",
    "Electrical Engineering",
    "Digital Systems",
    "Telecommunications"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCredits\n\nThis is a twelve credit H-level subject, (3-0-9).\n\nPrerequisite\n\n6.450 (Principles of Digital Communication - I). The prerequisite will be strictly enforced. Any student who has not taken 6.450 previously and who wants nonetheless to register for 6.451 will be required to come for a personal interview to justify his or her request.\n\nText\n\nNone required. Course notes are provided in the\nVideo Lectures\nsection.\n\nCourse Handouts\n\nHandouts and graded problem sets not picked up during lecture can be collected later.\n\nProblem Sets\n\nThere will be approximately 10 problem sets, corresponding to a weekly schedule, although the final problem set will not be collected. You are expected to do all the assigned problems. In making up the exams we will assume that you have worked all the problems, although you should not expect the exam problems to look like the homework problems.\n\nWe encourage you to cooperate with each other in doing the problem sets. The problem sets are vehicles for learning, and whatever maximizes learning for you is desirable. This usually includes discussion, teaching of others, and learning from others.\n\nProblem sets must be handed in by the end of the class in which they are due. Problem set solutions will be available at the end of the due date lecture. Consequently, it is difficult and unfair to seriously evaluate late problem sets.\n\nThe grades assigned to problems sets will be 0, 1 or 2. You may optionally indicate upto two problem solutions to which you would like the TA to pay special attention.\n\nExams\n\nThere will be one midterm exam during the semester and a final exam during the scheduled final exam period. To the midterm you may bring three pages of size 8.5\" by 11\". To the final exam you may bring five pages of size 8.5\" by 11\". The midterm exam will take place in class. The final exam will be scheduled by the registrar for 3 hours during exam week.\n\nCourse Grade\n\nThe final grade in the course is based upon our best assessment of your understanding of the material. This assessment is based on three noisy measurements: the problem sets, the midterm, and the final. The different measurements have different noise levels, and the final grade will be thus a weighted average, roughly according to the following rule:\n\nActivities\n\nPercentages\n\nMidterm Exam\n\n35%\n\nFinal Exam\n\n50%\n\nProblem Sets\n\n15%",
  "files": [
    {
      "category": "Resource",
      "title": "chap_1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/28eebe8f610d548070dae1bf3f74b6cd_chap_1.pdf",
      "content": "Chapter 1\nIntroduction\nThe advent of cheap high-speed global communications ranks as one of the most important\ndevelopments of human civilization in the second half of the twentieth century.\nIn 1950, an international telephone call was a remarkable event, and black-and-white television\nwas just beginning to become widely available. By 2000, in contrast, an intercontinental phone\ncall could often cost less than a postcard, and downloading large files instantaneously from\nanywhere in the world had become routine. The effects of this revolution are felt in daily life\nfrom Boston to Berlin to Bangalore.\nUnderlying this development has been the replacement of analog by digital communications.\nBefore 1948, digital communications had hardly been imagined. Indeed, Shannon's 1948 paper\n[7] may have been the first to use the word \"bit.\"1\nEven as late as 1988, the authors of an important text on digital communications [5] could\nwrite in their first paragraph:\nWhy would [voice and images] be transmitted digitally? Doesn't digital transmission\nsquander bandwidth? Doesn't it require more expensive hardware? After all, a voice-\nband data modem (for digital transmission over a telephone channel) costs ten times\nas much as a telephone and (in today's technology) is incapable of transmitting voice\nsignals with quality comparable to an ordinary telephone [authors' emphasis]. This\nsounds like a serious indictment of digital transmission for analog signals, but for most\napplications, the advantages outweigh the disadvantages . . .\nBut by their second edition in 1994 [6], they were obliged to revise this passage as follows:\nNot so long ago, digital transmission of voice and video was considered wasteful of\nbandwidth, and the cost . . . was of concern. [More recently, there has been] a com\nplete turnabout in thinking . . . In fact, today virtually all communication is either\nalready digital, in the process of being converted to digital, or under consideration for\nconversion.\nShannon explains that \"bit\" is a contraction of \"binary digit,\" and credits the neologism to J. W. Tukey.\n\nCHAPTER 1. INTRODUCTION\nThe most important factor in the digital communications revolution has undoubtedly been the\nstaggering technological progress of microelectronics and optical fiber technology. For wireline\nand wireless radio transmission (but not optical), another essential factor has been progress in\nchannel coding, data compression and signal processing algorithms. For instance, data compres\nsion algorithms that can encode telephone-quality speech at 8-16 kbps and voiceband modem\nalgorithms that can transmit 40-56 kbps over ordinary telephone lines have become commodities\nthat require a negligible fraction of the capacity of today's personal-computer microprocessors.\nThis book attempts to tell the channel coding part of this story. In particular, it focusses\non coding for the point-to-point additive white Gaussian noise (AWGN) channel. This choice\nis made in part for pedagogical reasons, but also because in fact almost all of the advances in\npractical channel coding have taken place in this arena. Moreover, performance on the AWGN\nchannel is the standard benchmark for comparison of different coding schemes.\n1.1\nShannon's grand challenge\nThe field of information theory and coding has a unique history, in that many of its ultimate\nlimits were determined at the very beginning, in Shannon's founding paper [7].\nShannon's most celebrated result is his channel capacity theorem, which we will review in\nChapter 3. This theorem states that for many common classes of channels there exists a channel\ncapacity C such that there exist codes at any rate R < C that can achieve arbitrarily reliable\ntransmission, whereas no such codes exist for rates R > C. For a band-limited AWGN channel,\nthe capacity C in bits per second (b/s) depends on only two parameters, the channel bandwidth\nW in Hz and the signal-to-noise ratio SNR, as follows:\nC = W log2(1 + SNR) b/s.\nShannon's theorem has posed a magnificent challenge to succeeding generations of researchers.\nIts proof is based on randomly chosen codes and optimal (maximum likelihood) decoding. In\npractice, it has proved to be remarkably difficult to find classes of constructive codes that can be\ndecoded by feasible decoding algorithms at rates which come at all close to the Shannon limit.\nIndeed, for a long time this problem was regarded as practically insoluble. Each significant\nadvance toward this goal has been awarded the highest accolades the coding community has to\noffer, and most such advances have been immediately incorporated into practical systems.\nIn the next two sections we give a brief history of these advances for two different practical\nchannels: the deep-space channel and the telephone channel. The deep-space channel is an\nunlimited-bandwidth, power-limited AWGN channel, whereas the telephone channel is very\nmuch bandwidth-limited. (We realize that many of the terms used here may be unfamiliar to\nthe reader at this point, but we hope that these surveys will give at least an impressionistic\npicture. After reading later chapters, the reader may wish to return to reread these sections.)\nWithin the past decade there have been remarkable breakthroughs, principally the invention\nof turbo codes [1] and the rediscovery of low-density parity check (LDPC) codes [4], which have\nallowed the capacity of AWGN and similar channels to be approached in a practical sense. For\nexample, Figure 1 (from [2]) shows that an optimized rate-1/2 LDPC code on an AWGN channel\ncan approach the relevant Shannon limit within 0.0045 decibels (dB) in theory, and within 0.04\ndB with an arguably practical code of block length 107 bits. Practical systems using block\nlengths of the order of 104-105 bits now approach the Shannon limit within tenths of a dB.\n\n1.2. BRIEF HISTORY OF CODES FOR DEEP-SPACE MISSIONS\nBER\n-2\n-3\n-4\n-5\n-6\nShannon limit\ndl=100\ndl=200\nThreshold (dl=100)\nThreshold (dl=200)\nThreshold (dl=8000)\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nEb/N0 [dB]\nFigure 1. Bit error rate vs. Eb/N0 in dB for optimized irregular rate-1/2 binary LDPC codes\nwith maximum left degree dl. Threshold: theoretical limit as block length →inf. Solid curves:\nsimulation results for block length = 107 . Shannon limit: binary codes, R = 1/2. (From [2].)\nHere we will tell the story of how Shannon's challenge has been met for the AWGN channel, first\nfor power-limited channels, where binary codes are appropriate, and then for bandwidth-limited\nchannels, where multilevel modulation must be used. We start with the simplest schemes and\nwork up to capacity-approaching codes, which for the most part follows the historical sequence.\n1.2\nBrief history of codes for deep-space missions\nThe deep-space communications application has been the arena in which most of the most\npowerful coding schemes for the power-limited AWGN channel have been first deployed, because:\n- The only noise is AWGN in the receiver front end;\n- Bandwidth is effectively unlimited;\n- Fractions of a dB have huge scientific and economic value;\n- Receiver (decoding) complexity is effectively unlimited.\n\nCHAPTER 1. INTRODUCTION\nFor power-limited AWGN channels, we will see that there is no penalty to using binary codes\nwith binary modulation rather than more general modulation schemes.\nThe first coded scheme to be designed was a simple (32, 6, 16) biorthogonal code for the Mariner\nmissions (1969), decoded by efficient maximum-likelihood decoding (the fast Hadamard trans\nform, or \"Green machine;\" see Exercise 2, below). We will see that such a scheme can achieve a\nnominal coding gain of 3 (4.8 dB). At a target error probability per bit of Pb(E) ≈ 5 · 10-3, the\nactual coding gain achieved was only about 2.2 dB.\nThe first coded scheme actually to be launched was a rate-1/2 convolutional code with con\nstraint length ν = 20 for the Pioneer 1968 mission. The receiver used 3-bit soft decisions and\nsequential decoding implemented on a general-purpose 16-bit minicomputer with a 1 MHz clock\nrate. At 512 b/s, the actual coding gain achieved at Pb(E) ≈ 5 · 10-3 was about 3.3 dB.\nDuring the 1970's, the NASA standard became a concatenated coding scheme based on a\nν = 6, rate-1/3 inner convolutional code and a (255, 223, 33) Reed-Solomon outer code over\nF256. Such a system can achieve a real coding gain of about 8.3 dB at Pb(E) ≈ 10-6 .\nWhen the primary antenna failed to deploy on the Galileo mission (circa 1992), an elaborate\nconcatenated coding scheme using a ν = 14 rate-1/4 inner code with a Big Viterbi Decoder\n(BVD) and a set of variable-strength RS outer codes was reprogrammed into the spacecraft\ncomputers. This scheme was able to operate at Eb/N0 ≈ 0.8 dB at Pb(E) ≈ 2 · 10-7, for a real\ncoding gain of about 10.2 dB.\nTurbo coding systems for deep-space communications have been developed by NASA's Jet\nPropulsion Laboratory (JPL) and others to get within 1 dB of the Shannon limit, and have now\nbeen standardized.\nFor a more comprehensive history of coding for deep-space channels, see [3].\n1.3\nBrief history of telephone-line modems\nFor several decades the telephone channel was the arena in which the most powerful coding\nand modulation schemes for the bandwidth-limited AWGN channel were first developed and\ndeployed, because:\n- The telephone channel is fairly well modeled as a band-limited AWGN channel;\n- One dB has a significant commercial value;\n- Data rates are low enough that a considerable amount of processing can be done per bit.\nTo approach the capacity of bandwidth-limited AWGN channels, multilevel modulation must\nbe used. Moreover, it is important to use as much of the available bandwidth as possible.\nThe earliest modems developed in the 1950s and 1960s (Bell 103 and 202, and international\nstandards V.21 and V.23) used simple binary frequency-shift keying (FSK) to achieve data rates\nof 300 and 1200 b/s, respectively. Implementation was entirely analog.\nThe first synchronous \"high-speed\" modem was the Bell 201 (later V.24), a 2400 b/s modem\nwhich was introduced about 1962. This modem used four-phase (4-PSK) modulation at 1200\nsymbols/s, so the nominal (Nyquist) bandwidth was 1200 Hz. However, because the modulation\npulse had 100% rolloff, the actual bandwidth used was closer to 2400 Hz.\n\n1.3. BRIEF HISTORY OF TELEPHONE-LINE MODEMS\nThe first successful 4800 b/s modem was the Milgo 4400/48 (later V.27), which was introduced\nabout 1967. This modem used eight-phase (8-PSK) modulation at 1600 symbols/s, so the\nnominal (Nyquist) bandwidth was 1600 Hz. \"Narrow-band\" filters with 50% rolloff kept the\nactual bandwidth used to 2400 Hz.\nThe first successful 9600 b/s modem was the Codex 9600C (later V.29), which was introduced\nin 1971. This modem used quadrature amplitude modulation (QAM) at 2400 symbols/s with an\nunconventional 16-point signal constellation (see Exercise 3, below) to combat combined \"phase\njitter\" and AWGN. More importantly, it used digital adaptive linear equalization to keep the\nactual bandwidth needed to not much more than the Nyquist bandwidth of 2400 Hz.\nAll of these modems were designed for private point-to-point conditioned voice-grade lines,\nwhich use four-wire circuits (independent transmission in each direction) whose quality is higher\nand more consistent than that of the typical telephone connection in the two-wire (simultaneous\ntransmission in both directions) public switched telephone network (PSTN).\nThe first international standard to use coding was the V.32 standard (1986) for 9600 b/s\ntransmission over the PSTN (later raised to 14.4 kb/s in V.32bis ). This modem used an 8-state,\ntwo-dimensional (2D) rotationally invariant Wei trellis code to achieve a coding gain of about 3.5\ndB with a 32-QAM (later 128-QAM) constellation at 2400 symbols/s, again with an adaptive\nlinear equalizer. Digital echo cancellation was also introduced to combat echoes on two-wire\nchannels.\nThe \"ultimate modem standard\" was V.34 (1994) for transmission at up to 28.8 kb/s over\nthe PSTN (later raised to 33.6 kb/s in V.34bis). This modem used a 16-state, 4D rotationally\ninvariant Wei trellis code to achieve a coding gain of about 4.0 dB with a variable-sized QAM\nconstellation with up to 1664 points. An optional 32-state, 4D trellis code with an additional\ncoding gain of 0.3 dB and four times (4x) the decoding complexity and a 64-state, 4D code\nwith a further 0.15 dB coding gain and a further 4x increase in complexity were also provided.\nA 16D \"shell mapping\" constellation shaping scheme provided an additional shaping gain of\nabout 0.8 dB (see Exercise 4, below). A variable symbol rate of up to 3429 symbols/s was used,\nwith symbol rate and data rate selection determined by \"line probing\" of individual channels.\nNonlinear transmitter precoding combined with adaptive linear equalization in the receiver was\nused for equalization, again with echo cancellation. In short, this modem used almost every tool\nin the AWGN channel toolbox.\nHowever, this standard was shortly superseded by V.90 (1998). V.90 is based on a completely\ndifferent, non-AWGN model for the telephone channel: namely, it recognizes that within today's\nPSTN, analog signals are bandlimited, sampled and quantized to one of 256 amplitude levels at\n8 kHz, transmitted digitally at 64 kb/s, and then eventually reconstructed by pulse amplitude\nmodulation (PAM). By gaining direct access to the 64 kb/s digital data stream at a central site,\nand by using a well-spaced subset of the pre-existing nonlinear 256-PAM constellation, data can\neasily be transmitted at 40-56 kb/s (see Exercise 5, below). In V.90, such a scheme is used for\ndownstream transmission only, with V.34 modulation upstream. In V.92 (2000) this scheme has\nbeen extended to the more difficult upstream direction.\nNeither V.90 nor V.92 uses coding, nor the other sophisticated techniques of V.34. In this\nsense, the end of the telephone-line modem story is a bit of a fizzle. However, techniques similar\nto those of V.34 are now used in higher-speed wireline modems, such as digital subscriber line\n(DSL) modems, as well as on wireless channels such as digital cellular. In other words, the story\ncontinues in other settings.\n\nCHAPTER 1. INTRODUCTION\n1.4\nExercises\nIn this section we offer a few warm-up exercises to give the reader some preliminary feeling for\ndata communication on the AWGN channel.\nIn these exercises the underlying channel model is assumed to be a discrete-time AWGN\nchannel whose output sequence is given by Y = X + N, where X is a real input data sequence\nand N is a sequence of real independent, identically distributed (iid) zero-mean Gaussian noise\nvariables. This model will be derived from a continuous-time model in Chapter 2.\nWe will also give the reader some practice in the use of decibels (dB). In general, a dB represen\ntation is useful wherever logarithms are useful; i.e., wherever a real number is a multiplicative\nfactor of some other number, and particularly for computing products of many factors. The dB\nscale is simply the logarithmic mapping\nratio or multiplicative factor of α ↔ 10 log10 α dB,\nwhere the scaling is chosen so that the decade interval 1-10 maps to the interval 0-10. (In other\nwords, the value of α in dB is logβ α, where β = 100.1 = 1.2589....) This scale is convenient for\nhuman memory and calculation. It is often useful to have the little log table below committed\nto memory, even in everyday life (see Exercise 1, below).\nα\ndB\ndB\n(round numbers)\n(two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n\n6.02\n6.99\n9.03\n10.00\nExercise 1. (Compound interest and dB) How long does it take to double your money at an\ninterest rate of P %? The bankers' \"Rule of 72\" estimates that it takes about 72/P years; e.g.,\nat a 5% interest rate compounded annually, it takes about 14.4 years to double your money.\n(a) An engineer decides to interpolate the dB table above linearly for 1 ≤ 1 + p ≤ 1.25; i.e.,\nratio or multiplicative factor of 1 + p ↔ 4p dB.\nShow that this corresponds to a \"Rule of 75;\" e.g., at a 5% interest rate compounded annually,\nit takes 15 years to double your money.\n(b) A mathematician linearly approximates the dB table for p ≈ 0 by noting that as p → 0,\nln(1+p) → p, and translates this into a \"Rule of N \" for some real number N . What is N ? Using\nthis rule, how many years will it take to double your money at a 5% interest rate, compounded\nannually? What happens if interest is compounded continuously?\n(c) How many years will it actually take to double your money at a 5% interest rate, com\npounded annually? [Hint: 10 log10 7 = 8.45 dB.] Whose rule best predicts the correct result?\n\n@\n1.4. EXERCISES\nExercise 2. (Biorthogonal codes) A 2m × 2m {±1}-valued Hadamard matrix H2m may be\nconstructed recursively as the m-fold tensor product of the 2 × 2 matrix\n+1 +1\nH2 =\n+1 -1\n,\nas follows:\n\n+H2m-1\n+H2m-1\nH2m =\n.\n+H2m-1\n-H2m-1\n(a) Show by induction that:\n(i) (H2m )T = H2m , where T denotes the transpose; i.e., H2m is symmetric;\n(ii) The rows or columns of H2m form a set of mutually orthogonal vectors of length 2m;\n(iii) The first row and the first column of H2m consist of all +1s;\n(iv) There are an equal number of +1s and -1s in all other rows and columns of H2m ;\n(v) H2m H2m = 2mI2m ; i.e., (H2m )-1 = 2-mH2m , where -1 denotes the inverse.\n(b) A biorthogonal signal set is a set of real equal-energy orthogonal vectors and their negatives.\nShow how to construct a biorthogonal signal set of size 64 as a set of {±1}-valued sequences of\nlength 32.\n(c) A simplex signal set S is a set of real equal-energy vectors that are equidistant and that\nhave zero mean m(S) under an equiprobable distribution. Show how to construct a simplex\nsignal set of size 32 as a set of 32 {±1}-valued sequences of length 31. [Hint: The fluctuation\nO - m(O) of a set O of orthogonal real vectors is a simplex signal set.]\n(d) Let Y = X+N be the received sequence on a discrete-time AWGN channel, where the input\nsequence X is chosen equiprobably from a biorthogonal signal set B of size 2m+1 constructed as\nin part (b). Show that the following algorithm implements a minimum-distance decoder for B\n(i.e., given a real 2m-vector y, it finds the closest x ∈ B to y):\n(i) Compute z = H2m y, where y is regarded as a column vector;\n(ii) Find the component zj of z with largest magnitude |zj |;\n(iii) Decode to sgn(zj )xj , where sgn(zj ) is the sign of the largest-magnitude component zj and\nxj is the corresponding column of H2m .\n(e) Show that a circuit similar to that shown below for m = 2 can implement the 2m × 2m\nmatrix multiplication z = H2m y with a total of only m×2m addition and subtraction operations.\n(This is called the \"fast Hadamard transform,\" or \"Walsh transform,\" or \"Green machine.\")\ny1\n\n-\n- z1\n+\n@\n+ A\n\ny2\n@\nR\nA-\n\n- z2\n+\n-\n\n- A A\n\ny3\n\nA AU\n- z3\n+ - -\nA\n\n@\nR\nAU\ny4\n-\n\n- - -\n- z4\nFigure 2. Fast 2m × 2m Hadamard transform for m = 2.\n\nCHAPTER 1. INTRODUCTION\nExercise 3. (16-QAM signal sets) Three 16-point 2-dimensional quadrature amplitude mod\nulation (16-QAM) signal sets are shown in Figure 3, below. The first is a standard 4 × 4 signal\nset; the second is the V.29 signal set; the third is based on a hexagonal grid and is the most\npower-efficient 16-QAM signal set known. The first two have 90* symmetry; the last, only 180* .\nAll have a minimum squared distance between signal points of d2\n= 4.\nmin\nr\nr\nr\nr\n-3 -1\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\nr\n-5r -3r -1\n3r\n5r\n-2.r5-0.r5 1.r5 3.r5\nr\nr\nr\nr\n√\nr\nr\nr\nr - 3\n√\nr\nr\nr\nr\nr\nr\nr -2\nr\n(a)\n(b)\n(c)\nFigure 3. 16-QAM signal sets. (a) (4 × 4)-QAM; (b) V.29; (c) hexagonal.\n(a) Compute the average energy (squared norm) of each signal set if all points are equiprobable.\nCompare the power efficiencies of the three signal sets in dB.\n(b) Sketch the decision regions of a minimum-distance detector for each signal set.\n(c) Show that with a phase rotation of ±10* the minimum distance from any rotated signal\npoint to any decision region boundary is substantially greatest for the V.29 signal set.\nExercise 4. (Shaping gain of spherical signal sets) In this exercise we compare the power\nefficiency of n-cube and n-sphere signal sets for large n.\nAn n-cube signal set is the set of all odd-integer sequences of length n within an n-cube of\nside 2M centered on the origin. For example, the signal set of Figure 3(a) is a 2-cube signal set\nwith M = 4.\nAn n-sphere signal set is the set of all odd-integer sequences of length n within an n-sphere\nof squared radius r2 centered on the origin. For example, the signal set of Figure 3(a) is also a\n2-sphere signal set for any squared radius r2 in the range 18 ≤ r2 < 25. In particular, it is a\n2-sphere signal set for r2 = 64/π = 20.37, where the area πr2 of the 2-sphere (circle) equals the\narea (2M )2 = 64 of the 2-cube (square) of the previous paragraph.\nBoth n-cube and n-sphere signal sets therefore have minimum squared distance between signal\npoints d2\nmin = 4 (if they are nontrivial), and n-cube decision regions of side 2 and thus volume\n2n associated with each signal point. The point of the following exercise is to compare their\naverage energy using the following large-signal-set approximations:\n- The number of signal points is approximately equal to the volume V (R) of the bounding\nn-cube or n-sphere region R divided by 2n, the volume of the decision region associated\nwith each signal point (an n-cube of side 2).\n- The average energy of the signal points under an equiprobable distribution is approximately\nequal to the average energy E(R) of the bounding n-cube or n-sphere region R under a\nuniform continuous distribution.\n\n1.4. EXERCISES\n(a) Show that if R is an n-cube of side 2M for some integer M , then under the two above\napproximations the approximate number of signal points is M n and the approximate average\nenergy is nM 2/3. Show that the first of these two approximations is exact.\n(b) For n even, if R is an n-sphere of radius r, compute the approximate number of signal\npoints and the approximate average energy of an n-sphere signal set, using the following known\nexpressions for the volume V⊗(n, r) and the average energy E⊗(n, r) of an n-sphere of radius r:\n(πr2)n/2\nV⊗(n, r)\n=\n(n/2)! ;\nnr\nE⊗(n, r)\n=\n\n.\nn + 2\n(c) For n = 2, show that a large 2-sphere signal set has about 0.2 dB smaller average energy\nthan a 2-cube signal set with the same number of signal points.\n(d) For n = 16, show that a large 16-sphere signal set has about 1 dB smaller average energy\nthan a 16-cube signal set with the same number of signal points. [Hint: 8! = 40320 (46.06 dB).]\n(e) Show that as n →inf a large n-sphere signal set has a factor of πe/6 (1.53 dB) smaller\naverage energy than an n-cube signal set with the same number of signal points. [Hint: Use\nStirling's approximation, m! →(m/e)m as m →inf.]\nExercise 5. (56 kb/s PCM modems)\nThis problem has to do with the design of \"56 kb/s PCM modems\" such as V.90 and V.92.\nIn the North American telephone network, voice is commonly digitized by low-pass filtering to\nabout 3.8 KHz, sampling at 8000 samples per second, and quantizing each sample into an 8-bit\nbyte according to the so-called \"μ law.\" The μ law specifies 255 distinct signal levels, which are\na quantized, piecewise-linear approximation to a logarithmic function, as follows:\n- 1 level at 0;\n- 15 positive levels evenly spaced with d = 2 between 2 and 30 (i.e., 2, 4, 6, 8, . . . , 30);\n- 16 positive levels evenly spaced with d = 4 between 33 and 93;\n- 16 positive levels evenly spaced with d = 8 between 99 and 219;\n- 16 positive levels evenly spaced with d = 16 between 231 and 471;\n- 16 positive levels evenly spaced with d = 32 between 495 and 975;\n- 16 positive levels evenly spaced with d = 64 between 1023 and 1983;\n- 16 positive levels evenly spaced with d = 128 between 2079 and 3999;\n- 16 positive levels evenly spaced with d = 256 between 4191 and 8031;\n- plus 127 symmetric negative levels.\nThe resulting 64 kb/s digitized voice sequence is transmitted through the network and ulti\nmately reconstructed at a remote central office by pulse amplitude modulation (PAM) using a\nμ-law digital/analog converter and a 4 KHz low-pass filter.\n\nCHAPTER 1. INTRODUCTION\nFor a V.90 modem, one end of the link is assumed to have a direct 64 kb/s digital connection\nand to be able to send any sequence of 8000 8-bit bytes per second. The corresponding levels\nare reconstructed at the remote central office. For the purposes of this exercise, assume that the\nreconstruction is exactly according to the μ-law table above, and that the reconstructed pulses\nare then sent through an ideal 4 KHz additive AWGN channel to the user.\n(a) Determine the maximum number M of levels that can be chosen from the 255-point μ-law\nconstellation above such that the minimum separation between levels is d = 2, 4, 8, 16, 64, 128,\n256, 512, or 1024, respectively.\n(b) These uncoded M -PAM subconstellations may be used to send up to r = log2 M bits per\nsymbol. What level separation can be obtained while sending 40 kb/s? 48 kb/s? 56 kb/s?\n(c) How much more SNR in dB is required to transmit reliably at 48 kb/s compared to 40\nkb/s? At 56 kb/s compared to 48 kb/s?\nReferences\n[1] C. Berrou, A. Glavieux and P. Thitimajshima, \"Near Shannon limit error-correcting coding\nand decoding: Turbo codes,\" Proc. 1993 Int. Conf. Commun. (Geneva), pp. 1064-1070, May\n1993.\n[2] S.-Y. Chung, G. D. Forney, Jr., T. J. Richardson and R. Urbanke, \"On the design of low-\ndensity parity-check codes within 0.0045 dB from the Shannon limit,\" IEEE Commun. Letters,\nvol. 5, pp. 58-60, Feb. 2001.\n[3] D. J. Costello, Jr., J. Hagenauer, H. Imai and S. B. Wicker, \"Applications of error-control\ncoding,\" IEEE Trans. Inform. Theory, vol. 44, pp. 2531-2560, Oct. 1998.\n[4] R. G. Gallager, Low-Density Parity-Check Codes. Cambridge, MA: MIT Press, 1962.\n[5] E. A. Lee and D. G. Messerschmitt, Digital Communication (first edition). Boston: Kluwer,\n1988.\n[6] E. A. Lee and D. G. Messerschmitt, Digital Communication (second edition). Boston:\nKluwer, 1994.\n[7] C. E. Shannon, \"A mathematical theory of communication,\" Bell Syst. Tech. J., vol. 27, pp.\n379-423 and 623-656, 1948."
    },
    {
      "category": "Resource",
      "title": "chap1_3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/586348b4c9923d82157938ca47aef270_chap1_3.pdf",
      "content": "Chapter 1\nIntroduction\nThe advent of cheap high-speed global communications ranks as one of the most important\ndevelopments of human civilization in the second half of the twentieth century.\nIn 1950, an international telephone call was a remarkable event, and black-and-white television\nwas just beginning to become widely available. By 2000, in contrast, an intercontinental phone\ncall could often cost less than a postcard, and downloading large files instantaneously from\nanywhere in the world had become routine. The effects of this revolution are felt in daily life\nfrom Boston to Berlin to Bangalore.\nUnderlying this development has been the replacement of analog by digital communications.\nBefore 1948, digital communications had hardly been imagined. Indeed, Shannon's 1948 paper\n[7] may have been the first to use the word \"bit.\"1\nEven as late as 1988, the authors of an important text on digital communications [5] could\nwrite in their first paragraph:\nWhy would [voice and images] be transmitted digitally? Doesn't digital transmission\nsquander bandwidth? Doesn't it require more expensive hardware? After all, a voice-\nband data modem (for digital transmission over a telephone channel) costs ten times\nas much as a telephone and (in today's technology) is incapable of transmitting voice\nsignals with quality comparable to an ordinary telephone [authors' emphasis]. This\nsounds like a serious indictment of digital transmission for analog signals, but for most\napplications, the advantages outweigh the disadvantages . . .\nBut by their second edition in 1994 [6], they were obliged to revise this passage as follows:\nNot so long ago, digital transmission of voice and video was considered wasteful of\nbandwidth, and the cost . . . was of concern. [More recently, there has been] a com\nplete turnabout in thinking . . . In fact, today virtually all communication is either\nalready digital, in the process of being converted to digital, or under consideration for\nconversion.\nShannon explains that \"bit\" is a contraction of \"binary digit,\" and credits the neologism to J. W. Tukey.\n\nCHAPTER 1. INTRODUCTION\nThe most important factor in the digital communications revolution has undoubtedly been the\nstaggering technological progress of microelectronics and optical fiber technology. For wireline\nand wireless radio transmission (but not optical), another essential factor has been progress in\nchannel coding, data compression and signal processing algorithms. For instance, data compres\nsion algorithms that can encode telephone-quality speech at 8-16 kbps and voiceband modem\nalgorithms that can transmit 40-56 kbps over ordinary telephone lines have become commodities\nthat require a negligible fraction of the capacity of today's personal-computer microprocessors.\nThis book attempts to tell the channel coding part of this story. In particular, it focusses\non coding for the point-to-point additive white Gaussian noise (AWGN) channel. This choice\nis made in part for pedagogical reasons, but also because in fact almost all of the advances in\npractical channel coding have taken place in this arena. Moreover, performance on the AWGN\nchannel is the standard benchmark for comparison of different coding schemes.\n1.1\nShannon's grand challenge\nThe field of information theory and coding has a unique history, in that many of its ultimate\nlimits were determined at the very beginning, in Shannon's founding paper [7].\nShannon's most celebrated result is his channel capacity theorem, which we will review in\nChapter 3. This theorem states that for many common classes of channels there exists a channel\ncapacity C such that there exist codes at any rate R < C that can achieve arbitrarily reliable\ntransmission, whereas no such codes exist for rates R > C. For a band-limited AWGN channel,\nthe capacity C in bits per second (b/s) depends on only two parameters, the channel bandwidth\nW in Hz and the signal-to-noise ratio SNR, as follows:\nC = W log2(1 + SNR) b/s.\nShannon's theorem has posed a magnificent challenge to succeeding generations of researchers.\nIts proof is based on randomly chosen codes and optimal (maximum likelihood) decoding. In\npractice, it has proved to be remarkably difficult to find classes of constructive codes that can be\ndecoded by feasible decoding algorithms at rates which come at all close to the Shannon limit.\nIndeed, for a long time this problem was regarded as practically insoluble. Each significant\nadvance toward this goal has been awarded the highest accolades the coding community has to\noffer, and most such advances have been immediately incorporated into practical systems.\nIn the next two sections we give a brief history of these advances for two different practical\nchannels: the deep-space channel and the telephone channel. The deep-space channel is an\nunlimited-bandwidth, power-limited AWGN channel, whereas the telephone channel is very\nmuch bandwidth-limited. (We realize that many of the terms used here may be unfamiliar to\nthe reader at this point, but we hope that these surveys will give at least an impressionistic\npicture. After reading later chapters, the reader may wish to return to reread these sections.)\nWithin the past decade there have been remarkable breakthroughs, principally the invention\nof turbo codes [1] and the rediscovery of low-density parity check (LDPC) codes [4], which have\nallowed the capacity of AWGN and similar channels to be approached in a practical sense. For\nexample, Figure 1 (from [2]) shows that an optimized rate-1/2 LDPC code on an AWGN channel\ncan approach the relevant Shannon limit within 0.0045 decibels (dB) in theory, and within 0.04\ndB with an arguably practical code of block length 107 bits. Practical systems using block\nlengths of the order of 104-105 bits now approach the Shannon limit within tenths of a dB.\n\n1.2. BRIEF HISTORY OF CODES FOR DEEP-SPACE MISSIONS\nBER\n-2\n-3\n-4\n-5\n-6\nShannon limit\ndl=100\ndl=200\nThreshold (dl=100)\nThreshold (dl=200)\nThreshold (dl=8000)\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nEb/N0 [dB]\nFigure 1. Bit error rate vs. Eb/N0 in dB for optimized irregular rate-1/2 binary LDPC codes\nwith maximum left degree dl. Threshold: theoretical limit as block length →inf. Solid curves:\nsimulation results for block length = 107 . Shannon limit: binary codes, R = 1/2. (From [2].)\nHere we will tell the story of how Shannon's challenge has been met for the AWGN channel, first\nfor power-limited channels, where binary codes are appropriate, and then for bandwidth-limited\nchannels, where multilevel modulation must be used. We start with the simplest schemes and\nwork up to capacity-approaching codes, which for the most part follows the historical sequence.\n1.2\nBrief history of codes for deep-space missions\nThe deep-space communications application has been the arena in which most of the most\npowerful coding schemes for the power-limited AWGN channel have been first deployed, because:\n- The only noise is AWGN in the receiver front end;\n- Bandwidth is effectively unlimited;\n- Fractions of a dB have huge scientific and economic value;\n- Receiver (decoding) complexity is effectively unlimited.\n\nCHAPTER 1. INTRODUCTION\nFor power-limited AWGN channels, we will see that there is no penalty to using binary codes\nwith binary modulation rather than more general modulation schemes.\nThe first coded scheme to be designed was a simple (32, 6, 16) biorthogonal code for the Mariner\nmissions (1969), decoded by efficient maximum-likelihood decoding (the fast Hadamard trans\nform, or \"Green machine;\" see Exercise 2, below). We will see that such a scheme can achieve a\nnominal coding gain of 3 (4.8 dB). At a target error probability per bit of Pb(E) ≈ 5 · 10-3, the\nactual coding gain achieved was only about 2.2 dB.\nThe first coded scheme actually to be launched was a rate-1/2 convolutional code with con\nstraint length ν = 20 for the Pioneer 1968 mission. The receiver used 3-bit soft decisions and\nsequential decoding implemented on a general-purpose 16-bit minicomputer with a 1 MHz clock\nrate. At 512 b/s, the actual coding gain achieved at Pb(E) ≈ 5 · 10-3 was about 3.3 dB.\nDuring the 1970's, the NASA standard became a concatenated coding scheme based on a\nν = 6, rate-1/3 inner convolutional code and a (255, 223, 33) Reed-Solomon outer code over\nF256. Such a system can achieve a real coding gain of about 8.3 dB at Pb(E) ≈ 10-6 .\nWhen the primary antenna failed to deploy on the Galileo mission (circa 1992), an elaborate\nconcatenated coding scheme using a ν = 14 rate-1/4 inner code with a Big Viterbi Decoder\n(BVD) and a set of variable-strength RS outer codes was reprogrammed into the spacecraft\ncomputers. This scheme was able to operate at Eb/N0 ≈ 0.8 dB at Pb(E) ≈ 2 · 10-7, for a real\ncoding gain of about 10.2 dB.\nTurbo coding systems for deep-space communications have been developed by NASA's Jet\nPropulsion Laboratory (JPL) and others to get within 1 dB of the Shannon limit, and have now\nbeen standardized.\nFor a more comprehensive history of coding for deep-space channels, see [3].\n1.3\nBrief history of telephone-line modems\nFor several decades the telephone channel was the arena in which the most powerful coding\nand modulation schemes for the bandwidth-limited AWGN channel were first developed and\ndeployed, because:\n- The telephone channel is fairly well modeled as a band-limited AWGN channel;\n- One dB has a significant commercial value;\n- Data rates are low enough that a considerable amount of processing can be done per bit.\nTo approach the capacity of bandwidth-limited AWGN channels, multilevel modulation must\nbe used. Moreover, it is important to use as much of the available bandwidth as possible.\nThe earliest modems developed in the 1950s and 1960s (Bell 103 and 202, and international\nstandards V.21 and V.23) used simple binary frequency-shift keying (FSK) to achieve data rates\nof 300 and 1200 b/s, respectively. Implementation was entirely analog.\nThe first synchronous \"high-speed\" modem was the Bell 201 (later V.24), a 2400 b/s modem\nwhich was introduced about 1962. This modem used four-phase (4-PSK) modulation at 1200\nsymbols/s, so the nominal (Nyquist) bandwidth was 1200 Hz. However, because the modulation\npulse had 100% rolloff, the actual bandwidth used was closer to 2400 Hz.\n\n1.3. BRIEF HISTORY OF TELEPHONE-LINE MODEMS\nThe first successful 4800 b/s modem was the Milgo 4400/48 (later V.27), which was introduced\nabout 1967. This modem used eight-phase (8-PSK) modulation at 1600 symbols/s, so the\nnominal (Nyquist) bandwidth was 1600 Hz. \"Narrow-band\" filters with 50% rolloff kept the\nactual bandwidth used to 2400 Hz.\nThe first successful 9600 b/s modem was the Codex 9600C (later V.29), which was introduced\nin 1971. This modem used quadrature amplitude modulation (QAM) at 2400 symbols/s with an\nunconventional 16-point signal constellation (see Exercise 3, below) to combat combined \"phase\njitter\" and AWGN. More importantly, it used digital adaptive linear equalization to keep the\nactual bandwidth needed to not much more than the Nyquist bandwidth of 2400 Hz.\nAll of these modems were designed for private point-to-point conditioned voice-grade lines,\nwhich use four-wire circuits (independent transmission in each direction) whose quality is higher\nand more consistent than that of the typical telephone connection in the two-wire (simultaneous\ntransmission in both directions) public switched telephone network (PSTN).\nThe first international standard to use coding was the V.32 standard (1986) for 9600 b/s\ntransmission over the PSTN (later raised to 14.4 kb/s in V.32bis ). This modem used an 8-state,\ntwo-dimensional (2D) rotationally invariant Wei trellis code to achieve a coding gain of about 3.5\ndB with a 32-QAM (later 128-QAM) constellation at 2400 symbols/s, again with an adaptive\nlinear equalizer. Digital echo cancellation was also introduced to combat echoes on two-wire\nchannels.\nThe \"ultimate modem standard\" was V.34 (1994) for transmission at up to 28.8 kb/s over\nthe PSTN (later raised to 33.6 kb/s in V.34bis). This modem used a 16-state, 4D rotationally\ninvariant Wei trellis code to achieve a coding gain of about 4.0 dB with a variable-sized QAM\nconstellation with up to 1664 points. An optional 32-state, 4D trellis code with an additional\ncoding gain of 0.3 dB and four times (4x) the decoding complexity and a 64-state, 4D code\nwith a further 0.15 dB coding gain and a further 4x increase in complexity were also provided.\nA 16D \"shell mapping\" constellation shaping scheme provided an additional shaping gain of\nabout 0.8 dB (see Exercise 4, below). A variable symbol rate of up to 3429 symbols/s was used,\nwith symbol rate and data rate selection determined by \"line probing\" of individual channels.\nNonlinear transmitter precoding combined with adaptive linear equalization in the receiver was\nused for equalization, again with echo cancellation. In short, this modem used almost every tool\nin the AWGN channel toolbox.\nHowever, this standard was shortly superseded by V.90 (1998). V.90 is based on a completely\ndifferent, non-AWGN model for the telephone channel: namely, it recognizes that within today's\nPSTN, analog signals are bandlimited, sampled and quantized to one of 256 amplitude levels at\n8 kHz, transmitted digitally at 64 kb/s, and then eventually reconstructed by pulse amplitude\nmodulation (PAM). By gaining direct access to the 64 kb/s digital data stream at a central site,\nand by using a well-spaced subset of the pre-existing nonlinear 256-PAM constellation, data can\neasily be transmitted at 40-56 kb/s (see Exercise 5, below). In V.90, such a scheme is used for\ndownstream transmission only, with V.34 modulation upstream. In V.92 (2000) this scheme has\nbeen extended to the more difficult upstream direction.\nNeither V.90 nor V.92 uses coding, nor the other sophisticated techniques of V.34. In this\nsense, the end of the telephone-line modem story is a bit of a fizzle. However, techniques similar\nto those of V.34 are now used in higher-speed wireline modems, such as digital subscriber line\n(DSL) modems, as well as on wireless channels such as digital cellular. In other words, the story\ncontinues in other settings.\n\nCHAPTER 1. INTRODUCTION\n1.4\nExercises\nIn this section we offer a few warm-up exercises to give the reader some preliminary feeling for\ndata communication on the AWGN channel.\nIn these exercises the underlying channel model is assumed to be a discrete-time AWGN\nchannel whose output sequence is given by Y = X + N, where X is a real input data sequence\nand N is a sequence of real independent, identically distributed (iid) zero-mean Gaussian noise\nvariables. This model will be derived from a continuous-time model in Chapter 2.\nWe will also give the reader some practice in the use of decibels (dB). In general, a dB represen\ntation is useful wherever logarithms are useful; i.e., wherever a real number is a multiplicative\nfactor of some other number, and particularly for computing products of many factors. The dB\nscale is simply the logarithmic mapping\nratio or multiplicative factor of α ↔ 10 log10 α dB,\nwhere the scaling is chosen so that the decade interval 1-10 maps to the interval 0-10. (In other\nwords, the value of α in dB is logβ α, where β = 100.1 = 1.2589....) This scale is convenient for\nhuman memory and calculation. It is often useful to have the little log table below committed\nto memory, even in everyday life (see Exercise 1, below).\nα\ndB\ndB\n(round numbers)\n(two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n\n6.02\n6.99\n9.03\n10.00\nExercise 1. (Compound interest and dB) How long does it take to double your money at an\ninterest rate of P %? The bankers' \"Rule of 72\" estimates that it takes about 72/P years; e.g.,\nat a 5% interest rate compounded annually, it takes about 14.4 years to double your money.\n(a) An engineer decides to interpolate the dB table above linearly for 1 ≤ 1 + p ≤ 1.25; i.e.,\nratio or multiplicative factor of 1 + p ↔ 4p dB.\nShow that this corresponds to a \"Rule of 75;\" e.g., at a 5% interest rate compounded annually,\nit takes 15 years to double your money.\n(b) A mathematician linearly approximates the dB table for p ≈ 0 by noting that as p → 0,\nln(1+p) → p, and translates this into a \"Rule of N \" for some real number N . What is N ? Using\nthis rule, how many years will it take to double your money at a 5% interest rate, compounded\nannually? What happens if interest is compounded continuously?\n(c) How many years will it actually take to double your money at a 5% interest rate, com\npounded annually? [Hint: 10 log10 7 = 8.45 dB.] Whose rule best predicts the correct result?\n\n@\n1.4. EXERCISES\nExercise 2. (Biorthogonal codes) A 2m × 2m {±1}-valued Hadamard matrix H2m may be\nconstructed recursively as the m-fold tensor product of the 2 × 2 matrix\n+1 +1\nH2 =\n+1 -1\n,\nas follows:\n\n+H2m-1\n+H2m-1\nH2m =\n.\n+H2m-1\n-H2m-1\n(a) Show by induction that:\n(i) (H2m )T = H2m , where T denotes the transpose; i.e., H2m is symmetric;\n(ii) The rows or columns of H2m form a set of mutually orthogonal vectors of length 2m;\n(iii) The first row and the first column of H2m consist of all +1s;\n(iv) There are an equal number of +1s and -1s in all other rows and columns of H2m ;\n(v) H2m H2m = 2mI2m ; i.e., (H2m )-1 = 2-mH2m , where -1 denotes the inverse.\n(b) A biorthogonal signal set is a set of real equal-energy orthogonal vectors and their negatives.\nShow how to construct a biorthogonal signal set of size 64 as a set of {±1}-valued sequences of\nlength 32.\n(c) A simplex signal set S is a set of real equal-energy vectors that are equidistant and that\nhave zero mean m(S) under an equiprobable distribution. Show how to construct a simplex\nsignal set of size 32 as a set of 32 {±1}-valued sequences of length 31. [Hint: The fluctuation\nO - m(O) of a set O of orthogonal real vectors is a simplex signal set.]\n(d) Let Y = X+N be the received sequence on a discrete-time AWGN channel, where the input\nsequence X is chosen equiprobably from a biorthogonal signal set B of size 2m+1 constructed as\nin part (b). Show that the following algorithm implements a minimum-distance decoder for B\n(i.e., given a real 2m-vector y, it finds the closest x ∈ B to y):\n(i) Compute z = H2m y, where y is regarded as a column vector;\n(ii) Find the component zj of z with largest magnitude |zj |;\n(iii) Decode to sgn(zj )xj , where sgn(zj ) is the sign of the largest-magnitude component zj and\nxj is the corresponding column of H2m .\n(e) Show that a circuit similar to that shown below for m = 2 can implement the 2m × 2m\nmatrix multiplication z = H2m y with a total of only m×2m addition and subtraction operations.\n(This is called the \"fast Hadamard transform,\" or \"Walsh transform,\" or \"Green machine.\")\ny1\n\n-\n- z1\n+\n@\n+ A\n\ny2\n@\nR\nA-\n\n- z2\n+\n-\n\n- A A\n\ny3\n\nA AU\n- z3\n+ - -\nA\n\n@\nR\nAU\ny4\n-\n\n- - -\n- z4\nFigure 2. Fast 2m × 2m Hadamard transform for m = 2.\n\nCHAPTER 1. INTRODUCTION\nExercise 3. (16-QAM signal sets) Three 16-point 2-dimensional quadrature amplitude mod\nulation (16-QAM) signal sets are shown in Figure 3, below. The first is a standard 4 × 4 signal\nset; the second is the V.29 signal set; the third is based on a hexagonal grid and is the most\npower-efficient 16-QAM signal set known. The first two have 90* symmetry; the last, only 180* .\nAll have a minimum squared distance between signal points of d2\n= 4.\nmin\nr\nr\nr\nr\n-3 -1\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\nr\n-5r -3r -1\n3r\n5r\n-2.r5-0.r5 1.r5 3.r5\nr\nr\nr\nr\n√\nr\nr\nr\nr - 3\n√\nr\nr\nr\nr\nr\nr\nr -2\nr\n(a)\n(b)\n(c)\nFigure 3. 16-QAM signal sets. (a) (4 × 4)-QAM; (b) V.29; (c) hexagonal.\n(a) Compute the average energy (squared norm) of each signal set if all points are equiprobable.\nCompare the power efficiencies of the three signal sets in dB.\n(b) Sketch the decision regions of a minimum-distance detector for each signal set.\n(c) Show that with a phase rotation of ±10* the minimum distance from any rotated signal\npoint to any decision region boundary is substantially greatest for the V.29 signal set.\nExercise 4. (Shaping gain of spherical signal sets) In this exercise we compare the power\nefficiency of n-cube and n-sphere signal sets for large n.\nAn n-cube signal set is the set of all odd-integer sequences of length n within an n-cube of\nside 2M centered on the origin. For example, the signal set of Figure 3(a) is a 2-cube signal set\nwith M = 4.\nAn n-sphere signal set is the set of all odd-integer sequences of length n within an n-sphere\nof squared radius r2 centered on the origin. For example, the signal set of Figure 3(a) is also a\n2-sphere signal set for any squared radius r2 in the range 18 ≤ r2 < 25. In particular, it is a\n2-sphere signal set for r2 = 64/π = 20.37, where the area πr2 of the 2-sphere (circle) equals the\narea (2M )2 = 64 of the 2-cube (square) of the previous paragraph.\nBoth n-cube and n-sphere signal sets therefore have minimum squared distance between signal\npoints d2\nmin = 4 (if they are nontrivial), and n-cube decision regions of side 2 and thus volume\n2n associated with each signal point. The point of the following exercise is to compare their\naverage energy using the following large-signal-set approximations:\n- The number of signal points is approximately equal to the volume V (R) of the bounding\nn-cube or n-sphere region R divided by 2n, the volume of the decision region associated\nwith each signal point (an n-cube of side 2).\n- The average energy of the signal points under an equiprobable distribution is approximately\nequal to the average energy E(R) of the bounding n-cube or n-sphere region R under a\nuniform continuous distribution.\n\n1.4. EXERCISES\n(a) Show that if R is an n-cube of side 2M for some integer M , then under the two above\napproximations the approximate number of signal points is M n and the approximate average\nenergy is nM 2/3. Show that the first of these two approximations is exact.\n(b) For n even, if R is an n-sphere of radius r, compute the approximate number of signal\npoints and the approximate average energy of an n-sphere signal set, using the following known\nexpressions for the volume V⊗(n, r) and the average energy E⊗(n, r) of an n-sphere of radius r:\n(πr2)n/2\nV⊗(n, r)\n=\n(n/2)! ;\nnr\nE⊗(n, r)\n=\n\n.\nn + 2\n(c) For n = 2, show that a large 2-sphere signal set has about 0.2 dB smaller average energy\nthan a 2-cube signal set with the same number of signal points.\n(d) For n = 16, show that a large 16-sphere signal set has about 1 dB smaller average energy\nthan a 16-cube signal set with the same number of signal points. [Hint: 8! = 40320 (46.06 dB).]\n(e) Show that as n →inf a large n-sphere signal set has a factor of πe/6 (1.53 dB) smaller\naverage energy than an n-cube signal set with the same number of signal points. [Hint: Use\nStirling's approximation, m! →(m/e)m as m →inf.]\nExercise 5. (56 kb/s PCM modems)\nThis problem has to do with the design of \"56 kb/s PCM modems\" such as V.90 and V.92.\nIn the North American telephone network, voice is commonly digitized by low-pass filtering to\nabout 3.8 KHz, sampling at 8000 samples per second, and quantizing each sample into an 8-bit\nbyte according to the so-called \"μ law.\" The μ law specifies 255 distinct signal levels, which are\na quantized, piecewise-linear approximation to a logarithmic function, as follows:\n- 1 level at 0;\n- 15 positive levels evenly spaced with d = 2 between 2 and 30 (i.e., 2, 4, 6, 8, . . . , 30);\n- 16 positive levels evenly spaced with d = 4 between 33 and 93;\n- 16 positive levels evenly spaced with d = 8 between 99 and 219;\n- 16 positive levels evenly spaced with d = 16 between 231 and 471;\n- 16 positive levels evenly spaced with d = 32 between 495 and 975;\n- 16 positive levels evenly spaced with d = 64 between 1023 and 1983;\n- 16 positive levels evenly spaced with d = 128 between 2079 and 3999;\n- 16 positive levels evenly spaced with d = 256 between 4191 and 8031;\n- plus 127 symmetric negative levels.\nThe resulting 64 kb/s digitized voice sequence is transmitted through the network and ulti\nmately reconstructed at a remote central office by pulse amplitude modulation (PAM) using a\nμ-law digital/analog converter and a 4 KHz low-pass filter.\n\nCHAPTER 1. INTRODUCTION\nFor a V.90 modem, one end of the link is assumed to have a direct 64 kb/s digital connection\nand to be able to send any sequence of 8000 8-bit bytes per second. The corresponding levels\nare reconstructed at the remote central office. For the purposes of this exercise, assume that the\nreconstruction is exactly according to the μ-law table above, and that the reconstructed pulses\nare then sent through an ideal 4 KHz additive AWGN channel to the user.\n(a) Determine the maximum number M of levels that can be chosen from the 255-point μ-law\nconstellation above such that the minimum separation between levels is d = 2, 4, 8, 16, 64, 128,\n256, 512, or 1024, respectively.\n(b) These uncoded M -PAM subconstellations may be used to send up to r = log2 M bits per\nsymbol. What level separation can be obtained while sending 40 kb/s? 48 kb/s? 56 kb/s?\n(c) How much more SNR in dB is required to transmit reliably at 48 kb/s compared to 40\nkb/s? At 56 kb/s compared to 48 kb/s?\nReferences\n[1] C. Berrou, A. Glavieux and P. Thitimajshima, \"Near Shannon limit error-correcting coding\nand decoding: Turbo codes,\" Proc. 1993 Int. Conf. Commun. (Geneva), pp. 1064-1070, May\n1993.\n[2] S.-Y. Chung, G. D. Forney, Jr., T. J. Richardson and R. Urbanke, \"On the design of low-\ndensity parity-check codes within 0.0045 dB from the Shannon limit,\" IEEE Commun. Letters,\nvol. 5, pp. 58-60, Feb. 2001.\n[3] D. J. Costello, Jr., J. Hagenauer, H. Imai and S. B. Wicker, \"Applications of error-control\ncoding,\" IEEE Trans. Inform. Theory, vol. 44, pp. 2531-2560, Oct. 1998.\n[4] R. G. Gallager, Low-Density Parity-Check Codes. Cambridge, MA: MIT Press, 1962.\n[5] E. A. Lee and D. G. Messerschmitt, Digital Communication (first edition). Boston: Kluwer,\n1988.\n[6] E. A. Lee and D. G. Messerschmitt, Digital Communication (second edition). Boston:\nKluwer, 1994.\n[7] C. E. Shannon, \"A mathematical theory of communication,\" Bell Syst. Tech. J., vol. 27, pp.\n379-423 and 623-656, 1948.\n\nChapter 2\nDiscrete-time and continuous-time\nAWGN channels\nIn this chapter we begin our technical discussion of coding for the AWGN channel. Our purpose\nis to show how the continuous-time AWGN channel model Y (t) = X(t) + N(t) may be reduced\nto an equivalent discrete-time AWGN channel model Y = X + N, without loss of generality or\noptimality. This development relies on the sampling theorem and the theorem of irrelevance.\nMore practical methods of obtaining such a discrete-time model are orthonormal PAM or QAM\nmodulation, which use an arbitrarily small amount of excess bandwidth. Important parameters\nof the continuous-time channel such as SNR, spectral efficiency and capacity carry over to\ndiscrete time, provided that the bandwidth is taken to be the nominal (Nyquist) bandwidth.\nReaders who are prepared to take these assertions on faith may skip this chapter.\n2.1\nContinuous-time AWGN channel model\nThe continuous-time AWGN channel is a random channel whose output is a real random process\nY (t) = X(t) + N(t),\nwhere X(t) is the input waveform, regarded as a real random process, and N(t) is a real white\nGaussian noise process with single-sided noise power density N0 which is independent of X(t).\nMoreover, the input X(t) is assumed to be both power-limited and band-limited. The average\ninput power of the input waveform X(t) is limited to some constant P. The channel band B\nis a positive-frequency interval with bandwidth W Hz. The channel is said to be baseband if\nB = [0, W], and passband otherwise. The (positive-frequency) support of the Fourier transform\nof any sample function x(t) of the input process X(t) is limited to B.\nThe signal-to-noise ratio SNR of this channel model is then\nP\nSNR =\n,\nN0W\nwhere N0W is the total noise power in the band B. The parameter N0 is defined by convention\nto make this relationship true; i.e., N0 is the noise power per positive-frequency Hz. Therefore\nthe double-sided power spectral density of N(t) must be Snn(f) = N0/2, at least over the bands\n±B.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nThe two parameters W and SNR turn out to characterize the channel completely for digital\ncommunications purposes; the absolute scale of P and N0 and the location of the band B do not\naffect the model in any essential way. In particular, as we will show in Chapter 3, the capacity\nof any such channel in bits per second is\nC[b/s] = W log2(1 + SNR) b/s.\nIf a particular digital communication scheme transmits a continuous bit stream over such a\nchannel at rate R b/s, then the spectral efficiency of the scheme is said to be ρ = R/W (b/s)/Hz\n(read as \"bits per second per Hertz\"). The Shannon limit on spectral efficiency is therefore\nC[(b/s)/Hz] = log2(1 + SNR) (b/s)/Hz;\ni.e., reliable transmission is possible when ρ < C[(b/s)/Hz], but not when ρ > C[(b/s)/Hz].\n2.2\nSignal spaces\nIn the next few sections we will briefly review how this continuous-time model may be reduced\nto an equivalent discrete-time model via the sampling theorem and the theorem of irrelevance.\nWe assume that the reader has seen such a derivation previously, so our review will be rather\nsuccinct.\nThe set of all real finite-energy signals x(t), denoted by L2, is a real vector space; i.e., it is\nclosed under addition and under multiplication by real scalars. The inner product of two signals\nx(t), y(t) ∈L2 is defined by\n⟨x(t), y(t)⟩ =\nx(t)y(t) dt.\nThe squared Euclidean norm (energy) of x(t) ∈L2 is defined as ||x(t)||2 = ⟨x(t), x(t)⟩ < inf, and\nthe squared Euclidean distance between x(t), y(t) ∈L2 is d2(x(t), y(t)) = ||x(t) - y(t)||2. Two\nsignals in L2 are regarded as the same (L2-equivalent) if their distance is 0. This allows the\nfollowing strict positivity property to hold, as it must for a proper distance metric:\n||x(t)||2 ≥ 0,\nwith strict inequality unless x(t) is L2-equivalent to 0.\nEvery signal x(t) ∈L2 has an L2 Fourier transform\nx(t)e -2πift\nˆx(f) =\ndt,\nsuch that, up to L2-equivalence, x(t) can be recovered by the inverse Fourier transform:\n2πiftdf.\nx(t) =\nx(f)e\nˆ\nWe write ˆ\nx(f)), and x(t) ↔ ˆ\nx(f) = F (x(t)), x(t) = F -1(ˆ\nx(f).\nIt can be shown that an L2 signal x(t) is L2-equivalent to a signal which is continuous except\nat a discrete set of points of discontinuity (\"almost everywhere\"); therefore so is ˆx(f). The\nvalues of an L2 signal or its transform at points of discontinuity are immaterial.\n\n2.3. THE SAMPLING THEOREM\nBy Parseval's theorem, the Fourier transform preserves inner products:\n⟨x(t), y(t)⟩ = ⟨ˆ\nˆ\ny\nx(f), yˆ(f)⟩ =\nx(f)ˆ∗(f) df.\nIn particular, ||x(t)||2 = ||ˆx(f)||2 .\nA signal space is any subspace S ⊆ L2. For example, the set of L2 signals that are time-limited\nto an interval [0, T] (\"have support [0, T ]\") is a signal space, as is the set of L2 signals whose\nFourier transforms are nonzero only in ±B (\"have frequency support ±B\").\nEvery signal space S ⊆ L2 has an orthogonal basis {φk (t), k ∈I}, where I is some discrete\nindex set, such that every x(t) ∈S may be expressed as\n⟨x(t), φk (t)⟩\nx(t) =\n||φk (t)||2 φk (t),\nk∈I\nup to L2 equivalence. This is called an orthogonal expansion of x(t).\nOf course this expression becomes particularly simple if {φk (t)} is an orthonormal basis with\n||φk (t)||2 = 1 for all k ∈I. Then we have the orthonormal expansion\nx(t) =\nxk φk(t),\nk∈I\nwhere x = {xk = ⟨x(t), φk (t)⟩, k ∈I} is the corresponding set of orthonormal coefficients. From\nthis expression, we see that inner products are preserved in an orthonormal expansion; i.e.,\n⟨x(t), y(t)⟩ = ⟨x, y⟩ =\nxk yk .\nk∈I\nIn particular, ||x(t)||2 = ||x||2 .\n2.3\nThe sampling theorem\nThe sampling theorem allows us to convert a continuous signal x(t) with frequency support\n[-W, W ] (i.e., a baseband signal with bandwidth W) to a discrete-time sequence of samples\n{x(kT), k ∈ Z} at a rate of 2W samples per second, with no loss of information.\nThe sampling theorem is basically an orthogonal expansion for the space L2[0, W ] of sig\nnals that have frequency support [-W, W ]. If T = 1/2W, then the complex exponentials\n{exp(2πifkT), k ∈ Z} form an orthogonal basis for the space of Fourier transforms with support\n[-W, W ]. Therefore their scaled inverse Fourier transforms {φk (t) = sincT (t - kT), k ∈ Z} form\nan orthogonal basis for L2[0, W], where sincT (t) = (sin πt/T)/(πt/T). Since ||sincT (t)||2 = T,\nevery x(t) ∈L2[0, W] may therefore be expressed up to L2 equivalence as\nx(t) =\n⟨x(t), sincT (t - kT)⟩sincT (t - kT).\nT k∈Z\nMoreover, evaluating this equation at t = jT gives x(jT) = T ⟨x(t), sincT (t - jT)⟩ for all j ∈\nZ (provided that x(t) is continuous at t = jT), since sincT ((j - k)T ) = 1 for k = j and\nsincT ((j - k)T) = 0 for k = j. Thus if x(t) ∈L2[0, W ] is continuous, then\nx(t) =\nx(kT )sincT (t - kT ).\nk∈Z\nThis is called the sampling theorem.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nSince inner products are preserved in an orthonormal expansion, and here the orthonormal\n√\ncoefficients are xk = √ ⟨x(t), sincT (t -kT )⟩=\nTx(kT ), we have\nT\n⟨x(t), y(t)⟩= ⟨x, y⟩= T\nx(kT )y(kT ).\nk∈Z\nThe following exercise shows similarly how to convert a continuous passband signal x(t) with\nbandwidth W (i.e., with frequency support ±[fc -W/2, fc + W/2] for some center frequency\nfc > W/2) to a discrete-time sequence of sample pairs {(xc,k, xs,k), k ∈Z} at a rate of W pairs\nper second, with no loss of information.\nExercise 2.1 (Orthogonal bases for passband signal spaces)\n(a) Show that if {φk (t)} is an orthogonal set of signals in L2[0, W ],\nthen\n{φk (t) cos 2πfct, φk (t) sin 2πfct} is an orthogonal set of signals in L2[fc -W, fc + W ], the set\nof signals in L2 that have frequency support ±[fc -W, fc + W ], provided that fc ≥W .\n[Hint:\nuse the facts that F(φk (t) cos 2πfct)\n=\n(φˆk (f - fc) + φˆk (f + fc))/2\nand\n\nF(φk(t) sin 2πfct) = (φˆk (f -fc) -φˆk (f + fc))/2i, plus Parseval's theorem.]\n(b) Show that if the set {φk(t)} is an orthogonal basis for L2[0, W ], then the set\n{φk (t) cos 2πfct, φk (t) sin 2πfct} is an orthogonal basis for L2[fc -W, fc + W ], provided that\nfc ≥W .\n[Hint: show that every x(t) ∈L2[fc -W, fc + W ] may be written as x(t) = xc(t) cos 2πfct +\nxs(t) sin 2πfct for some xc(t), xs(t) ∈L2[0, W ].]\n(c) Conclude that every x(t) ∈L2[fc -W, fc + W ] may be expressed up to L2 equivalence as\n\nx(t) =\n(xc,k cos 2πfct + xs,k sin 2πfct) sincT (t -kT ),\nT\n\n= 2W ,\nk∈Z\nfor some sequence of pairs {(xc,k, xs,k ), k ∈Z}, and give expressions for xc,k and xs,k .\n2.4\nWhite Gaussian noise\nThe question of how to define a white Gaussian noise (WGN) process N(t) in general terms is\nplagued with mathematical difficulties. However, when we are given a signal space S ⊆L2 with\nan orthonormal basis as here, then defining WGN with respect to S is not so problematic. The\nfollowing definition captures the essential properties that hold in this case:\nDefinition 2.1 (White Gaussian noise with respect to a signal space S) Let S ⊆ L2\nbe a signal space with an orthonormal basis {φk (t), k ∈I}. A Gaussian process N(t) is de\nfined as white Gaussian noise with respect to S with single-sided power spectral density N0 if\n(a) The sequence {Nk = ⟨N(t), φk (t)⟩, k ∈I} is a sequence of iid Gaussian noise variables with\nmean zero and variance N0/2;\n(b) Define the \"in-band noise\" as the projection N|S (t) =\nNk φk (t) of N(t) onto the signal\nk∈I\nspace S, and the \"out-of-band noise\" as N|S⊥ (t) = N(t) -N|S (t). Then N|S⊥ (t) is a process\nwhich is jointly Gaussian with N|S (t), has sample functions which are orthogonal to S, is\nuncorrelated with N|S (t), and thus is statistically independent of N|S (t).\n\n2.4. WHITE GAUSSIAN NOISE\nFor example, any stationary Gaussian process whose single-sided power spectral density is\nequal to N0 within a band B and arbitrary elsewhere is white with respect to the signal space\nL2(B) of signals with frequency support ±B.\nExercise 2.2 (Preservation of inner products) Show that a Gaussian process N(t) is white\nwith respect to a signal space S ⊆L2 with psd N0 if and only if for any signals x(t), y(t) ∈S,\nE[⟨N(t), x(t)⟩⟨N(t), y(t)⟩] = N0 ⟨x(t), y(t)⟩.\nHere we are concerned with the detection of signals that lie in some signal space S in the\npresence of additive white Gaussian noise. In this situation the following theorem is fundamental:\nTheorem 2.1 (Theorem of irrelevance) Let X(t) be a random signal process whose sample\nfunctions x(t) lie in some signal space S ⊆ L2 with an orthonormal basis {φk (t), k ∈I}, let\nN(t) be a Gaussian noise process which is independent of X(t) and white with respect to S, and\nlet Y (t) = X(t) + N(t). Then the set of samples\nYk = ⟨Y (t), φk (t)⟩,\nk\n\n∈I,\nis a set of sufficient statistics for detection of X(t) from Y (t).\nSketch of proof. We may write\nY (t) = Y|S (t) + Y|S⊥ (t),\nwhere Y|S (t) =\nYk φk (t) and Y|S⊥ (t) = Y (t) -Y|S (t). Since Y (t) = X(t) + N(t) and\nk\nX(t) =\n⟨X(t), φk(t)⟩φk (t),\nk\nsince all sample functions of X(t) lie in S, we have\nY (t) =\nYk φk (t) + N|S⊥ (t),\nk\nwhere N|S⊥ (t) = N(t) -\n⟨N(t), φk (t)⟩φk (t). By Definition 2.1, N|S⊥ (t) is independent of\nk\nN|S (t) =\n⟨N(t), φk(t)⟩φk (t), and by hypothesis it is independent of X(t). Thus the proba\nk\nbility distribution of X(t) given Y|S (t) =\nYk φk (t) and Y|S⊥ (t) = N|S⊥ (t) depends only on\nk\nY|S (t), so without loss of optimality in detection of X(t) from Y (t) we can disregard Y|S⊥ (t);\ni.e., Y|S (t) is a sufficient statistic. Moreover, since Y|S (t) is specified by the samples {Yk }, these\nsamples equally form a set of sufficient statistics for detection of X(t) from Y (t).\nThe sufficient statistic Y|S (t) may alternatively be generated by filtering out the out-of-band\nnoise N|S⊥ (t). For example, for the signal space L2(B) of signals with frequency support ±B,\nwe may obtain Y|S (t) by passing Y (t) through a brick-wall filter which passes all frequency\ncomponents in B and rejects all components not in B.\n1Theorem 2.1 may be extended to any model Y (t) = X(t) + N (t) in which the out-of-band noise N|S⊥ (t) =\nN (t) - N|S (t) is independent of both the signal X(t) and the in-band noise N|S (t) =\nNk φk (t); e.g., to models\nk\nin which the out-of-band noise contains signals from other independent users. In the Gaussian case, independence\nof the out-of-band noise is automatic; in more general cases, independence is an additional assumption.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nCombining Definition 2.1 and Theorem 2.1, we conclude that for any AWGN channel in which\nthe signals are confined to a sample space S with orthonormal basis {φk(t), k ∈I}, we may\nwithout loss of optimality reduce the output Y (t) to the set of samples\nYk = ⟨Y (t), φk(t)⟩ = ⟨X(t), φk (t)⟩ + ⟨N(t), φk (t)⟩ = Xk + Nk ,\nk\n\n∈I,\nwhere {Nk, k ∈I} is a set of iid Gaussian variables with mean zero and variance N0/2. Moreover,\nif x1(t), x2(t) ∈S are two sample functions of X(t), then this orthonormal expansion preserves\ntheir inner product:\n⟨x1(t), x2(t)⟩ = ⟨x1, x2⟩,\nwhere x1 and x2 are the orthonormal coefficient sequences of x1(t) and x2(t), respectively.\n2.5\nContinuous time to discrete time\nWe now specialize these results to our original AWGN channel model Y (t) = X(t) + N(t),\nwhere the average power of X(t) is limited to P and the sample functions of X(t) are required\nto have positive frequency support in a band B of width W. For the time being we consider the\nbaseband case in which B = [0, W].\nThe signal space is then the set S = L2[0, W] of all finite-energy signals x(t) whose Fourier\ntransform has support ±B. The sampling theorem shows that {φk (t) = √1 sincT (t-kT), k ∈ Z}\nT\nis an orthonormal basis for this signal space, where T = 1/2W, and that therefore without loss\nof generality we may write any x(t) ∈S as\nx(t) =\nxk φk(t),\nk∈Z\nwhere xk is the orthonormal coefficient xk = ⟨x(t), φk (t)⟩, and equality is in the sense of L2\nequivalence.\nConsequently, if X(t) is a random process whose sample functions x(t) are all in S, then we\ncan write\nX(t) =\nXkφk (t),\nk∈Z\nwhere Xk = ⟨X(t), φk (t)⟩ =\nX(t)φk (t) dt, a random variable that is a linear functional of\nX(t). In this way we can identify any random band-limited process X(t) of bandwidth W with\na discrete-time random sequence X = {Xk } at a rate of 2W real variables per second. Hereafter\nthe input will be regarded as the sequence X rather than X(t).\nThus X(t) may be regarded as a sum of amplitude-modulated orthonormal pulses Xkφk(t).\nBy the Pythagorean theorem,\n||X(t)||2 =\n||Xk φk (t)||2 =\nXk\n2 ,\nk∈Z\nk∈Z\nwhere we use the orthonormality of the φk (t). Therefore the requirement that the average power\n(energy per second) of X(t) be less than P translates to a requirement that the average energy\nof the sequence X be less than P per 2W symbols, or equivalently less than P/2W per symbol.2\n2The requirement that the sample functions of X(t) must be in L2 translates to the requirement that the\nsample sequences x of X must have finite energy. This requirement can be met by requiring that only finitely\nmany elements of x be nonzero. However, we do not pursue such finiteness issues.\n\n2.5. CONTINUOUS TIME TO DISCRETE TIME\nSimilarly, the random Gaussian noise process N(t) may be written as\nN(t) =\nNkφk(t) + N|S⊥ (t)\nk∈Z\nwhere N = {Nk = ⟨N(t), φk(t)⟩} is the sequence of orthonormal coefficients of N(t) in S,\nand N|S⊥ (t) = N(t) -\nNkφk(t) is out-of-band noise. The theorem of irrelevance shows\nk\nthat N|S⊥ (t) may be disregarded without loss of optimality, and therefore that the sequence\nY = X + N is a set of sufficient statistics for detection of X(t) from Y (t).\nIn summary, we conclude that the characteristics of the discrete-time model Y = X + N mirror\nthose of the continuous-time model Y (t) = X(t) + N(t) from which it was derived:\n- The symbol interval is T = 1/2W ; equivalently, the symbol rate is 2W symbols/s;\n- The average signal energy per symbol is limited to P/2W ;\n- The noise sequence N is iid zero-mean (white) Gaussian, with variance N0/2 per symbol;\n- The signal-to-noise ratio is thus SNR = (P/2W )/(N0/2) = P/N0W , the same as for the\ncontinuous-time model;\n- A data rate of ρ bits per two dimensions (b/2D) translates to a data rate of R = Wρ b/s,\nor equivalently to a spectral efficiency of ρ (b/s)/Hz.\nThis important conclusion is the fundamental result of this chapter.\n2.5.1\nPassband case\nSuppose now that the channel is instead a passband channel with positive-frequency support\nband B = [fc - W/2, fc + W/2] for some center frequency fc > W/2.\nThe signal space is then the set S = L2[fc - W/2, fc + W/2] of all finite-energy signals x(t)\nwhose Fourier transform has support ±B.\nIn this case Exercise 2.1 shows that an orthogonal basis for the signal space is a set of signals\nof the form φk,c(t) = sincT (t - kT ) cos 2πfct and φk,s(t) = sincT (t - kT ) sin 2πfct, where the\nsymbol interval is now T = 1/W . Since the support of the Fourier transform of sincT (t - kT ) is\n[-W/2, W/2], the support of the transform of each of these signals is ±B.\nThe derivation of a discrete-time model then goes as in the baseband case. The result is that\nthe sequence of real pairs\n(Yk,c, Yk,s) = (Xk,c, Xk,s) + (Nk,c, Nk,s)\nis a set of sufficient statistics for detection of X(t) from Y (t). If we compute scale factors\ncorrectly, we find that the characteristics of this discrete-time model are as follows:\n- The symbol interval is T = 1/W , or the symbol rate is W symbols/s. In each symbol\ninterval a pair of two real symbols is sent and received. We may therefore say that the rate\nis 2W = 2/T real dimensions per second, the same as in the baseband model.\n- The average signal energy per dimension is limited to P/2W ;\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\n- The noise sequences Nc and Ns are independent real iid zero-mean (white) Gaussian se\nquences, with variance N0/2 per dimension;\n- The signal-to-noise ratio is again SNR = (P/2W )/(N0/2) = P/N0W ;\n- A data rate of ρ b/2D again translates to a spectral efficiency of ρ (b/s)/Hz.\nThus the passband discrete-time model is effectively the same as the baseband model.\nIn the passband case, it is often convenient to identify real pairs with single complex variables\n√\nvia the standard correspondence between R2 and C given by (x, y) ↔ x + iy, where i =\n-1.\nThis is possible because a complex iid zero-mean Gaussian sequence N with variance N0 per\ncomplex dimension may be defined as N = Nc + iNs, where Nc and Ns are independent real\niid zero-mean Gaussian sequences with variance N0/2 per real dimension. Then we obtain a\ncomplex discrete-time model Y = X + N with the following characteristics:\n- The symbol interval is T = 1/W , or the rate is W complex dimensions/s.\n- The average signal energy per complex dimension is limited to P/W ;\n- The noise sequence N is a complex iid zero-mean Gaussian sequence, with variance N0 per\ncomplex dimension;\n- The signal-to-noise ratio is again SNR = (P/W )/N0 = P/N0W ;\n- A data rate of ρ bits per complex dimension translates to a spectral efficiency of ρ (b/s)/Hz.\nThis is still the same as before, if we regard one complex dimension as two real dimensions.\nNote that even the baseband real discrete-time model may be converted to a complex discrete-\ntime model simply by taking real variables two at a time and using the same map R2 → C.\nThe reader is cautioned that the correspondence between R2 and C given by (x, y) ↔ x + iy\npreserves some algebraic, geometric and probabilistic properties, but not all.\nExercise 2.3 (Properties of the correspondence R2 ↔ C) Verify the following assertions:\n(a) Under the correspondence R2 ↔ C, addition is preserved.\n(b) However, multiplication is not preserved. (Indeed, the product of two elements of R2 is not\neven defined.)\n(c) Inner products are not preserved. Indeed, two orthogonal elements of R2 can map to two\ncollinear elements of C.\n(d) However, (squared) Euclidean norms and Euclidean distances are preserved.\n(e) In general, if Nc and Ns are real jointly Gaussian sequences, then Nc + iNs is not a proper\ncomplex Gaussian sequence, even if Nc and Ns are independent iid sequences.\n(f) However, if Nc and Ns are independent real iid zero-mean Gaussian sequences with variance\nN0/2 per real dimension, then Nc + iNs is a complex zero-mean Gaussian sequence with\nvariance N0 per complex dimension.\n\n2.6. ORTHONORMAL PAM AND QAM MODULATION\n2.6\nOrthonormal PAM and QAM modulation\nMore generally, suppose that X(t) =\nXk φk (t), where X = {Xk } is a random sequence and\nk\n{φk (t) = p(t -kT )} is an orthonormal sequence of time shifts p(t -kT ) of a basic modulation\npulse p(t) ∈L2 by integer multiples of a symbol interval T . This is called orthonormal pulse-\namplitude modulation (PAM).\nThe signal space S is then the subspace of L2 spanned by the orthonormal sequence {p(t-kT )};\ni.e., S consists of all signals in L2 that can be written as linear combinations\nk xk p(t -kT ).\nAgain, the average power of X(t) =\nXk p(t -kT ) will be limited to P if the average energy\nk\nof the sequence X is limited to PT per symbol, since the symbol rate is 1/T symbol/s.\nThe theorem of irrelevance again shows that the set of inner products\nYk = ⟨Y (t), φk(t)⟩= ⟨X(t), φk (t)⟩+ ⟨N(t), φk (t)⟩= Xk + Nk\nis a set of sufficient statistics for detection of X(t) from Y (t). These inner products may be\nobtained by filtering Y (t) with a matched filter with impulse response p(-t) and sampling at\ninteger multiples of T as shown in Figure 1 to obtain\nZ(kT ) =\nY (τ)p(τ -kT ) dτ = Yk,\nThus again we obtain a discrete-time model Y = X + N, where by the orthonormality of the\np(t -kT ) the noise sequence N is iid zero-mean Gaussian with variance N0/2 per symbol.\n-\nOrthonormal\nX = {Xk }\nX(t) =\nXkp(t -kT )\nk\nPAM\nmodulator\n-\n-\n( )\nN t\n?\n+\n\nY (t)\np(-t)\nsample at\n\nt = kT\nY =\n-\n{Yk }\nFigure 1. Orthonormal PAM system.\nThe conditions that ensure that the time shifts {p(t-kT )} are orthonormal are determined by\nNyquist theory as follows. Define the composite response in Figure 1 as g(t) = p(t) ∗p(-t), with\nFourier transform ˆg(f) = |pˆ(f)|2. (The composite response g(t) is also called the autocorrelation\nfunction of p(t), and ˆg(f) is also called its power spectrum.) Then:\nTheorem 2.2 (Orthonormality conditions) For a signal p(t) ∈L2 and a time interval T ,\nthe following are equivalent:\n(a) The time shifts {p(t -kT ), k ∈Z} are orthonormal;\n(b) The composite response g(t) = p(t) ∗p(-t) satisfies g(0) = 1 and g(kT ) = 0 for k = 0;\n(c) The Fourier transform gˆ(f) = |pˆ(f)|2 satisfies the Nyquist criterion for zero intersymbol\ninterference, namely\ngˆ(f -m/T ) = 1 for all f.\nT m∈Z\nSketch of proof. The fact that (a) ⇔(b) follows from ⟨p(t -kT ), p(t -k′T )⟩= g((k -k′)T ).\nThe fact that (b) ⇔ (c) follows from the aliasing theorem, which says that the discrete-time\nFourier transform of the sample sequence {g(kT )} is the aliased response 1\ngˆ(f -m/T ).\nT\nm\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nIt is clear from the Nyquist criterion (c) that if p(t) is a baseband signal of bandwidth W, then\n(i) The bandwidth W cannot be less than 1/2T;\n(ii) If W = 1/2T, then ˆ\ng(f) = 0; i.e., g(t) = sincT (t);\ng(f) = T, -W ≤f ≤W, else ˆ\n(iii) If 1/2T < W ≤ 1/T, then any real non-negative power spectrum ˆg(f) that satisfies\ngˆ(1/2T + f) + ˆg(1/2T -f) = T for 0 ≤f ≤1/2T will satisfy (c).\nFor this reason W = 1/2T is called the nominal or Nyquist bandwidth of a PAM system with\nsymbol interval T. No orthonormal PAM system can have bandwidth less than the Nyquist\nbandwidth, and only a system in which the modulation pulse has autocorrelation function g(t) =\np(t)∗p(-t) = sincT (t) can have exactly the Nyquist bandwidth. However, by (iii), which is called\nthe Nyquist band-edge symmetry condition, the Fourier transform |pˆ(f)|2 may be designed to roll\noff arbitrarily rapidly for f > W, while being continuous and having a continuous derivative.\nFigure 2 illustrates a raised-cosine frequency response that satisfies the Nyquist band-edge\nsymmetry condition while being continuous and having a continuous derivative. Nowadays it is\nno great feat to implement such responses with excess bandwidths of 5-10% or less.\nT\nf\nT -|pˆ( 1 -f)|\n2T\n\n*\n\n|pˆ( 1 + f)|\n2T\nT\nFigure 2. Raised-cosine spectrum ˆ g(f) = |pˆ(f)|2 with Nyquist band-edge symmetry.\nWe conclude that an orthonormal PAM system may use arbitrarily small excess bandwidth\nbeyond the Nyquist bandwidth W = 1/2T, or alternatively that the power in the out-of-band\nfrequency components may be made to be arbitrarily small, without violating the practical\nconstraint that the Fourier transform ˆp(f) of the modulation pulse p(t) should be continuous\nand have a continuous derivative.\nIn summary, if we let W denote the Nyquist bandwidth 1/2T rather than the actual bandwidth,\nthen we again obtain a discrete-time channel model Y = X + N for any orthonormal PAM\nsystem, not just a system with the modulation pulse p(t) = √1 sincT (t), in which:\nT\n- The symbol interval is T = 1/2W; equivalently, the symbol rate is 2W symbols/s;\n- The average signal energy per symbol is limited to P/2W;\n- The noise sequence N is iid zero-mean (white) Gaussian, with variance N0/2 per symbol;\n- The signal-to-noise ratio is SNR = (P/2W)/(N0/2) = P/N0W;\n- A data rate of ρ bits per two dimensions (b/2D) translates to a data rate of R = ρ/W b/s,\nor equivalently to a spectral efficiency of ρ (b/s)/Hz.\n\n2.7. SUMMARY\nExercise 2.4 (Orthonormal QAM modulation)\nFigure 3 illustrates an orthonormal quadrature amplitude modulation (QAM) system with\nsymbol interval T in which the input and output variables Xk and Yk are complex, p(t) is a\ncomplex finite-energy modulation pulse whose time shifts {p(t-kT )} are orthonormal (the inner\nproduct of two complex signals is ⟨x(t), y(t)⟩ =\nx(t)y ∗(t) dt), the matched filter response is\n∗\np (-t), and fc > 1/2T is a carrier frequency. The box marked 2R{·} takes twice the real part\nof its input-- i.e., it maps a complex signal f(t) to f(t) + f∗(t)-- and the Hilbert filter is a\ncomplex filter whose frequency response is 1 for f > 0 and 0 for f < 0.\n2πifct\ne\n?\nOrthonormal\n\nX = {Xk }\nX(t) =\nk\n- ×\n-\nXk p(t - kT )\n-\n2R{·}\nQAM\n\nmodulator\n?\n\nN(t)\n\n+\n-2πifct\n\ne\n@\n?\n\nHilbert\nY = {Yk}\n\n@\n∗\n\n×\np (-t)\nsample at\n\nfilter\nt = kT\nFigure 3. Orthonormal QAM system.\n(a) Assume that ˆp(f) = 0 for |f| ≥ fc. Show that the Hilbert filter is superfluous.\n(b) Show that Theorem 2.2 holds for a complex response p(t) if we define the composite\nresponse (autocorrelation function) as g(t) = p(t) ∗ p ∗(-t). Conclude that the bandwidth of an\northonormal QAM system is lowerbounded by its Nyquist bandwidth W = 1/T .\n(c) Show that Y = X + N, where N is an iid complex Gaussian noise sequence. Show that the\nsignal-to-noise ratio in this complex discrete-time model is equal to the channel signal-to-noise\nratio SNR = P/N0W , if we define W = 1/T . [Hint: use Exercise 2.1.]\n(d) Show that a mismatch in the receive filter-- i.e., an impulse response h(t) other than\n∗\np (-t)-- results in linear intersymbol interference-- i.e., in the absence of noise Yk =\nj Xj hk-j\nfor some discrete-time response {hk} other than the ideal response δk0 (Kronecker delta).\n(e) Show that a phase error of θ in the receive carrier-- i.e., demodulation by e-2πifct+iθ rather\nthan by e-2πifct-- results (in the absence of noise) in a phase rotation by θ of all outputs Yk .\n(f) Show that a sample timing error of δ-- i.e., sampling at times t = kT + δ-- results in\nlinear intersymbol interference.\n2.7\nSummary\nTo summarize, the key parameters of a band-limited continuous-time AWGN channel are its\nbandwidth W in Hz and its signal-to-noise ratio SNR, regardless of other details like where the\nbandwidth is located (in particular whether it is at baseband or passband), the scaling of the\nsignal, etc. The key parameters of a discrete-time AWGN channel are its symbol rate W in\ntwo-dimensional real or one-dimensional complex symbols per second and its SNR, regardless of\nother details like whether it is real or complex, the scaling of the symbols, etc. With orthonormal\nPAM or QAM, these key parameters are preserved, regardless of whether PAM or QAM is used,\nthe precise modulation pulse, etc. The (nominal) spectral efficiency ρ (in (b/s)/Hz or in b/2D)\nis also preserved, and (as we will see in the next chapter) so is the channel capacity (in b/s).\n\nChapter 3\nCapacity of AWGN channels\nIn this chapter we prove that the capacity of an AWGN channel with bandwidth W and signal-to-\nnoise ratio SNR is W log2(1+SNR) bits per second (b/s). The proof that reliable transmission is\npossible at any rate less than capacity is based on Shannon's random code ensemble, typical-set\ndecoding, the Chernoff-bound law of large numbers, and a fundamental result of large-deviation\ntheory. We also sketch a geometric proof of the converse. Readers who are prepared to accept\nthe channel capacity formula without proof may skip this chapter.\n3.1\nOutline of proof of the capacity theorem\nThe first step in proving the channel capacity theorem or its converse is to use the results\nof Chapter 2 to replace a continuous-time AWGN channel model Y (t) = X(t) + N(t) with\nbandwidth W and signal-to-noise ratio SNR by an equivalent discrete-time channel model Y =\nX + N with a symbol rate of 2W real symbol/s and the same SNR, without loss of generality\nor optimality.\nWe then wish to prove that arbitrarily reliable transmission can be achieved on the discrete-\ntime channel at any rate (nominal spectral efficiency)\nρ < C[b/2D] = log2(1 + SNR) b/2D.\nThis will prove that reliable transmission can be achieved on the continuous-time channel at any\ndata rate\nR < C[b/s] = WC[b/2D] = W log2(1 + SNR) b/s.\nWe will prove this result by use of Shannon's random code ensemble and a suboptimal decoding\ntechnique called typical-set decoding.\nShannon's random code ensemble may be defined as follows. Let Sx = P/2W be the allowable\naverage signal energy per symbol (dimension), let ρ be the data rate in b/2D, and let N be the\ncode block length in symbols. A block code C of length N, rate ρ, and average energy Sx per\ndimension is then a set of M = 2ρN/2 real sequences (codewords) c of length N such that the\nexpected value of ||c||2 under an equiprobable distribution over C is NSx.\nFor example, the three 16-QAM signal sets shown in Figure 3 of Chapter 1 may be regarded as\nthree block codes of length 2 and rate 4 b/2D with average energies per dimension of Sx = 5, 6.75\nand 4.375, respectively.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nIn Shannon's random code ensemble, every symbol ck of every codeword c ∈C is chosen\nindependently at random from a Gaussian ensemble with mean 0 and variance Sx. Thus the\naverage energy per dimension over the ensemble of codes is Sx, and by the law of large numbers\nthe average energy per dimension of any particular code in the ensemble is highly likely to be\nclose to Sx.\nWe consider the probability of error under the following scenario. A code C is selected randomly\nfrom the ensemble as above, and then a particular codeword c0 is selected for transmission. The\nchannel adds a noise sequence n from a Gaussian ensemble with mean 0 and variance Sn = N0/2\nper symbol. At the receiver, given y = c0 + n and the code C, a typical-set decoder implements\nthe following decision rule (where ε is some small positive number):\n- If there is one and only one codeword c ∈C within squared distance N(Sn ± ε) of the\nreceived sequence y, then decide on c;\n- Otherwise, give up.\nA decision error can occur only if one of the following two events occurs:\n- The squared distance ||y - c0||2 between y and the transmitted codeword c0 is not in the\nrange N(Sn ± ε);\n- The squared distance ||y - ci||2 between y and some other codeword ci = c0 is in the range\nN(Sn ± ε).\nSince y - c0 = n, the probability of the first of these events is the probability that ||n||2 is not\nin the range N(Sn - ε) ≤||n||2 ≤ N(Sn + ε). Since N = {Nk} is an iid zero-mean Gaussian\nsequence with variance Sn per symbol and ||N||2 =\nNk\n2, this probability goes to zero as\nk\nN →inf for any ε > 0 by the weak law of large numbers. In fact, by the Chernoff bound of the\nnext section, this probability goes to zero exponentially with N.\nFor any particular other codeword ci ∈C, the probability of the second event is the probability\nthat a code sequence drawn according to an iid Gaussian pdf pX(x) with symbol variance Sx and\na received sequence drawn independently according to an iid Gaussian pdf pY (y) with symbol\nvariance Sy = Sx + Sn are \"typical\" of the joint pdf pXY (x, y) = pX(x)pN (y - x), where here\nwe define \"typical\" by the distance ||x - y||2 being in the range N(Sn ± ε). According to a\nfundamental result of large-deviation theory, this probability goes to zero as e-NE, where, up\nto terms of the order of ε, the exponent E is given by the relative entropy (Kullback-Leibler\ndivergence)\nD(pXY ||pXpY ) =\ndx dy pXY (x, y) log pXY (x, y) .\npX(x)pY (y)\nIf the logarithm is binary, then this is the mutual information I(X; Y ) between the random\nvariables X and Y in bits per dimension (b/D).\nIn the Gaussian case considered here, the mutual information is easily evaluated as\n+ y2 log2 e\nI(X; Y ) = EXY - 2 log2 2πSn - (y - x)2 log2 e + 2 log2 2πSy\n= 2 log2\nSy\nb/D.\n2Sn\n2Sy\nSn\nSince Sy = Sx + Sn and SNR = Sx/Sn, this expression is equal to the claimed capacity in b/D.\n\n3.2. LAWS OF LARGE NUMBERS\nThus we can say that the probability that any incorrect codeword ci ∈C is \"typical\" with\nrespect to y goes to zero as 2-N (I(X;Y )-δ(ε)), where δ(ε) goes to zero as ε →0. By the union\nbound, the probability that any of the M -1 < 2ρN/2 incorrect codewords is \"typical\" with\nrespect to y is upperbounded by\nPr{any incorrect codeword \"typical\"} < 2ρN/22-N (I(X;Y )-δ(ε)),\nwhich goes to zero exponentially with N provided that ρ < 2I(X; Y ) b/2D and ε is small enough.\nIn summary, the probabilities of both types of error go to zero exponentially with N provided\nthat\nρ < 2I(X; Y ) = log2(1 + SNR) = C[b/2D]\nb/2D\nand ε is small enough. This proves that an arbitrarily small probability of error can be achieved\nusing Shannon's random code ensemble and typical-set decoding.\nTo show that there is a particular code of rate ρ < C[b/2D] that achieves an arbitrarily small\nerror probability, we need merely observe that the probability of error over the random code\nensemble is the average probability of error over all codes in the ensemble, so there must be at\nleast one code in the ensemble that achieves this performance. More pointedly, if the average\nerror probability is Pr(E), then no more than a fraction of 1/K of the codes can achieve error\nprobability worse than K Pr(E) for any constant K > 0; e.g., at least 99% of the codes achieve\nperformance no worse than 100 Pr(E). So we can conclude that almost all codes in the random\ncode ensemble achieve very small error probabilities. Briefly, \"almost all codes are good\" (when\ndecoded by typical-set or maximum-likelihood decoding).\n3.2\nLaws of large numbers\nThe channel capacity theorem is essentially an application of various laws of large numbers.\n3.2.1\nThe Chernoff bound\nThe weak law of large numbers states that the probability that the sample average of a sequence\nof N iid random variables differs from the mean by more than ε > 0 goes to zero as N →inf, no\nmatter how small ε is. The Chernoff bound shows that this probability goes to zero exponentially\nwith N, for arbitrarily small ε.\nTheorem 3.1 (Chernoff bound) Let SN be the sum of N iid real random variables Xk , each\nwith the same probability distribution pX (x) and mean X = EX [X]. For τ > X, the probability\nthat SN ≥Nτ is upperbounded by\n-NEc(τ )\nPr{SN ≥Nτ} ≤e\n,\nwhere the Chernoff exponent Ec(τ) is given by\nEc(τ) = max sτ -μ(s),\ns≥0\nwhere μ(s) denotes the semi-invariant moment-generating function, μ(s) = log EX [esX ].\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nProof. The indicator function Φ(SN ≥ Nτ ) of the event {SN ≥ Nτ } is bounded by\ns(SN -Nτ )\nΦ(SN ≥ Nτ ) ≤ e\nfor any s ≥ 0. Therefore\nPr{SN ≥ Nτ } = Φ(SN ≥ Nτ ) ≤ es(SN -Nτ ),\ns\n\n≥ 0,\nwhere the overbar denotes expectation. Using the facts that SN =\nXk and that the Xk are\nk\nindependent, we have\nes(SN -Nτ )\n-N (sτ -μ(s))\n=\nes(Xk -τ ) = e\n,\nk\nwhere μ(s) = log esX . Optimizing the exponent over s ≥ 0, we obtain the Chernoff exponent\nEc(τ ) = max sτ - μ(s).\ns≥0\nWe next show that the Chernoff exponent is positive:\nTheorem 3.2 (Positivity of Chernoff exponent) The Chernoff exponent Ec(τ ) is positive\nwhen τ > X, provided that the random variable X is nondeterministic.\nProof. Define X(s) as a random variable with the same alphabet as X, but with the tilted\nsx-μ(s).\nprobability density function q(x, s) = p(x)e\nThis is a valid pdf because q(x, s) ≥ 0 and\nsx\nq(x, s) dx = e -μ(s)\ne\np(x) dx = e -μ(s)eμ(s) = 1.\nEvidently μ(0) = log EX [1] = 0, so q(x, 0) = p(x) and X(0) = X.\nDefine the moment-generating (partition) function\nsx\nZ(s) = eμ(s) = EX [e sX ] =\ne\np(x) dx.\nNow it is easy to see that\nsx\nsx\nZ′(s) =\nxe p(x) dx = eμ(s)\nxe q(x, s) dx = Z(s)X(s).\nSimilarly,\n2 sx\nZ′′(s) =\nx e p(x) dx = Z(s)X2(s).\nConsequently, from μ(s) = log Z(s), we have\nμ ′(s)\n=\nZ′(s) = X(s);\nZ(s)\nZ′(s) 2\nμ ′′(s)\n=\nZ′′(s) -\n= X2(s) - X(s) .\nZ(s)\nZ(s)\nThus the second derivative μ′′(s) is the variance of X(s), which must be strictly positive unless\nX(s) and thus X is deterministic.\n\n3.2. LAWS OF LARGE NUMBERS\nWe conclude that if X is a nondeterministic random variable with mean X, then μ(s) is a\nstrictly convex function of s that equals 0 at s = 0 and whose derivative at s = 0 is X. It follows\nthat the function sτ -μ(s) is a strictly concave function of s that equals 0 at s = 0 and whose\nderivative at s = 0 is τ -X. Thus if τ > X, then the function sτ -μ(s) has a unique maximum\nwhich is strictly positive.\nExercise 1. Show that if X is a deterministic random variable-- i.e., the probability that X\nequals its mean X is 1-- and τ > X, then Pr{SN ≥Nτ} = 0.\nThe proof of this theorem shows that the general form of the function f(s) = sτ -μ(s)\nwhen X is nondeterministic is as shown in Figure 1. The second derivative f′′(s) is negative\neverywhere, so the function f(s) is strictly concave and has a unique maximum Ec(τ). The\nslope f′(s) = τ -X(s) therefore decreases continually from its value f′(0) = τ -X > 0 at\ns = 0. The slope becomes equal to 0 at the value of s for which τ = X(s); in other words, to\nfind the maximum of f(s), keep increasing the \"tilt\" until the tilted mean X(s) is equal to τ.\nIf we denote this value of s by s ∗(τ), then we obtain the following parametric equations for the\nChernoff exponent:\n∗\nEc(τ) = s ∗(τ)τ -μ(s (τ));\nτ = X(s ∗(τ)).\n\nτ -X\ns\n-\ns ∗(τ)\nf(s)\nEc(τ)\nslope\nslope 0\nFigure 1. General form of function f(s) = sτ -μ(s) when τ > X.\nWe will show below that the Chernoff exponent Ec(τ) is the correct exponent, in the sense\nthat\nlim log Pr{SN ≥Nτ} = Ec(τ).\nN →inf\nN\nThe proof will be based on a fundamental theorem of large-deviation theory\nWe see that finding the Chernoff exponent is an exercise in convex optimization. In convex\noptimization theory, Ec(τ) and μ(s) are called conjugate functions. It is easy to show from the\nproperties of μ(s) that Ec(τ) is a continuous, strictly convex function of τ that equals 0 at τ = X\nand whose derivative at τ = X is 0.\n3.2.2\nChernoff bounds for functions of rvs\nIf g : X → R is any real-valued function defined on the alphabet X of a random variable X,\nthen g(X) is a real random variable. If {Xk } is a sequence of iid random variables Xk with the\nsame distribution as X, then {g(Xk )} is a sequence of iid random variables g(Xk ) with the same\ndistribution as g(X). The Chernoff bound thus applies to the sequence {g(Xk )}, and shows that\nthe probability that the sample mean N\nk g(Xk ) exceeds τ goes to zero exponentially with N\nas N →infwhenever τ > g(X).\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nLet us consider any finite set {gj } of such functions gj : X →R. Because the Chernoff bound\ndecreases exponentially with N, we can conclude that the probability that any of the sample\nmeans 1\nk gj (Xk) exceeds its corresponding expectation gj (X) by a given fixed ε > 0 goes to\nN\nzero exponentially with N as N →inf.\nWe may define a sequence {Xk} to be ε-typical with respect to a function gj : X → R if\nk gj (Xk) < gj (X)+ ε. We can thus conclude that the probability that {Xk} is not ε-typical\nN\nwith respect to any finite set {gj } of functions gj goes to zero exponentially with N as N →inf.\nA simple application of this result is that the probability that the sample mean N\nk gj (Xk)\nis not in the range gj (X) ± ε goes to zero exponentially with N as N →inf for any ε > 0,\nbecause this probability is the sum of the two probabilities Pr{\nk gj (Xk) ≥N(gj (X) + ε)} and\nPr{\n-gj (Xk ) ≥N(-gj (X) + ε)}.\nk\nMore generally, if the alphabet X is finite, then by considering the indicator functions of each\npossible value of X we can conclude that the probability that all observed relative frequencies\nin a sequence are not within ε of the corresponding probabilities goes to zero exponentially with\nN as N →inf. Similarly, for any alphabet X, we can conclude that the probability of any finite\nnumber of sample moments N\nm are not within ε of the corresponding expected moments\nk Xk\nXm goes to zero exponentially with N as N →inf.\nIn summary, the Chernoff bound law of large numbers allows us to say that as N →infwe will\nalmost surely observe a sample sequence x which is typical in every (finite) way that we might\nspecify.\n3.2.3\nAsymptotic equipartition principle\nOne consequence of any law of large numbers is the asymptotic equipartition principle (AEP):\nas N →inf, the observed sample sequence x of an iid sequence whose elements are chosen\naccording to a random variable X will almost surely be such that pX (x) ≈ 2-N H(X), where\nH(X) = EX [-log2 p(x)]. If X is discrete, then pX (x) is its probability mass function (pmf ) and\nH(X) is its entropy; if X is continuous, then pX (x) is its probability density function (pdf) and\nH(X) is its differential entropy.\nThe AEP is proved by observing that -log2 pX (x) is a sum of iid random variables\n-log2 pX (xk ), so the probability that -log2 pX (x) differs from its mean NH(X) by more than\nε > 0 goes to zero as N →inf. The Chernoff bound shows that this probability in fact goes to\nzero exponentially with N.\nA consequence of the AEP is that the set Tε of all sequences x that are ε-typical with respect\nto the function -log2 pX (x) has a total probability that approaches 1 as N →inf. Since for\nall sequences x ∈ Tε we have pX (x) ≈ 2-N H(X )-- i.e., the probability distribution pX (x) is\napproximately uniform over Tε-- this implies that the \"size\" |Tε| of Tε is approximately 2N H(X).\nIn the discrete case, the \"size\" |Tε| is the number of sequences in Tε, whereas in the continuous\ncase |Tε| is the volume of Tε.\nIn summary, the AEP implies that as N →inf the observed sample sequence x will almost\nsurely lie in an ε-typical set Tε of size ≈2N H(X), and within that set the probability distribution\npX (x) will be approximately uniform.\n\n3.2. LAWS OF LARGE NUMBERS\n3.2.4\nFundamental theorem of large-deviation theory\nAs another application of the law of large numbers, we prove a fundamental theorem of large-\ndeviation theory. A rough statement of this result is as follows: if an iid sequence X is chosen\naccording to a probability distribution q(x), then the probability that the sequence will be typical\nof a second probability distribution p(x) is approximately\n-ND(p||q)\nPr{x typical for p | q} ≈ e\n,\nwhere the exponent D(p||q) denotes the relative entropy (Kullback-Leibler divergence)\nD(p||q) = Ep log p(x)\n=\ndx p(x) log p(x) .\nq(x)\nq(x)\nX\nAgain, p(x) and q(x) denote pmfs in the discrete case and pdfs in the continuous case; we use\nnotation that is appropriate for the continuous case.\nExercise 2 (Gibbs' inequality).\n(a) Prove that for x > 0, log x ≤ x - 1, with equality if and only if x = 1.\n(b) Prove that for any pdfs p(x) and q(x) over X , D(p||q) ≥ 0, with equality if and only if\np(x) = q(x).\nGiven p(x) and q(x), we will now define a sequence x to be ε-typical with regard to log p(x)/q(x)\nif the log likelihood ratio λ(x) = log p(x)/q(x) is in the range N (D(p||q) ± ε), where D(p||q) =\nEp[λ(x)] is the mean of λ(x) = log p(x)/q(x) under p(x). Thus an iid sequence X chosen\naccording to p(x) will almost surely be ε-typical by this definition.\nThe desired result can then be stated as follows:\nTheorem 3.3 (Fundamental theorem of large-deviation theory) Given two probability\ndistributions p(x) and q(x) on a common alphabet X , for any ε > 0, the probability that an iid\nrandom sequence X drawn according to q(x) is ε-typical for p(x), in the sense that log p(x)/q(x)\nis in the range N (D(p||q) ± ε), is bounded by\n(1 - δ(N ))e -N(D(p||q)+ε) ≤ Pr{x ε-typical for p | q} ≤ e -N(D(p||q)-ε),\nwhere δ(N ) → 0 as N →inf.\nProof. Define the ε-typical region\nTε = {x | N (D(p||q) - ε) ≤ log p(x) ≤ N (D(p||q) + ε)}.\nq(x)\nBy any law of large numbers, the probability that X will fall in Tε goes to 1 as N →inf; i.e.,\n1 - δ(N ) ≤\ndx p(x) ≤ 1,\nTε\nwhere δ(N ) → 0 as N →inf. It follows that\n-N(D(p||q)-ε) ≤ e -N(D(p||q)-ε);\ndx q(x) ≤\ndx p(x)e\nTε\nTε\n-N(D(p||q)+ε)\ndx q(x) ≥\ndx p(x)e -N(D(p||q)+ε) ≥ (1 - δ(N ))e\n.\nTε\nTε\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nSince we can choose an arbitrarily small ε > 0 and δ(N ) > 0, it follows the exponent D(p||q)\nis the correct exponent for this probability, in the sense that\nlog Pr{x ε-typical for p | q} = D(p||q).\nlim\nN →inf\nN\nExercise 3 (Generalization of Theorem 3.3).\n(a) Generalize Theorem 3.3 to the case in which q(x) is a general function over X . State any\nnecessary restrictions on q(x).\n(b) Using q(x) = 1 in (a), state and prove a form of the Asymptotic Equipartition Principle.\nAs an application of Theorem 3.3, we can now prove:\nTheorem 3.4 (Correctness of Chernoff exponent) The Chernoff exponent Ec(τ ) is the\ncorrect exponent for Pr{SN ≥ Nτ }, in the sense that\nlim log Pr{SN ≥ Nτ } = Ec(τ ),\nN →inf\nN\nwhere SN =\nk xk is the sum of N iid nondeterministic random variables drawn according to\nsome distribution p(x) with mean X < τ , and Ec(τ ) = maxs≥0 sτ - μ(s) where μ(s) = log esX .\nProof. Let s ∗ be the s that maximizes sτ - μ(s) over s ≥ 0. As we have seen above, for s = s ∗\n∗\n∗\n∗\ns x-μ(s ) has mean\nthe tilted random variable X(s ∗) with tilted distribution q(x, s ) = p(x)e\nX(s ∗) = τ , whereas for s = 0 the untilted random variable X(0) with untilted distribution\nq(x, 0) = p(x) has mean X(0) = X.\nLet q(0) denote the untilted distribution q(x, 0) = p(x) with mean X(0) = X, and let q(s ∗)\n∗\n∗\ndenote the optimally tilted distribution q(x, s ∗) = p(x)es x-μ(s ) with mean X(s ∗) = τ . Then\n∗\nlog q(x, s ∗)/q(x, 0) = s x - μ(s ∗), so\n∗\nD(q(s ∗)||q(0)) = s τ - μ(s ∗) = Ec(τ ).\n∗\nMoreover, the event that X is ε-typical with respect to the variable log q(x, s ∗)/q(x, 0) = s x -\n∗\n∗\nμ(s ∗) under q(x, 0) = p(x) is the event that s SN - Nμ(s ∗) is in the range N (s τ - μ(s ∗) ± ε),\nsince τ is the mean of X under q(x, s ∗). This event is equivalent to SN being in the range\nN (τ ± ε/s∗). Since ε may be arbitrarily small, it is clear that the correct exponent of the event\nPr{SN ≈ Nτ } is Ec(τ ). This event evidently dominates the probability Pr{SN ≥ Nτ }, which\n-NEc(τ ).\nwe have already shown to be upperbounded by e\nExercise 4 (Chernoff bound ⇒ divergence upper bound.)\nUsing the Chernoff bound, prove that for any two distributions p(x) and q(x) over X ,\n-N (D(p||q))\nPr{log p(x) ≥ ND(p||q) | q} ≤ e\n.\nq(x)\n[Hint: show that the s that maximizes sτ - μ(s) is s = 1.]\n\n3.2. LAWS OF LARGE NUMBERS\n3.2.5\nProof of the forward part of the capacity theorem\nWe now prove that with Shannon's random Gaussian code ensemble and with a slightly dif\nferent definition of typical-set decoding, we can achieve reliable communication at any rate\nρ < C[b/2D] = log2(1 + SNR) b/2D.\nWe recall that under this scenario the joint pdf of the channel input X and output Y is\n-(y-x)2/2Sn\npXY (x, y) = pX(x)pN (y - x) = √ 1\ne -x2/2Sx √\ne\n.\n2πSx\n2πSn\nSince Y = X + N, the marginal probability of Y is\npY (y) =\ne -y2/2Sy ,\n2πSy\nwhere Sy = Sx + Sn. On the other hand, since incorrect codewords are independent of the\ncorrect codeword and of the output, the joint pdf of an incorrect codeword symbol X′ and of Y\nis\n′\n-y2 /2Sy .\nqXY (x , y) = pX(x ′)pY (y) = √ 2\nπSx\ne -(x′)2/2Sx\ne\n2πSy\nWe now redefine typical-set decoding as follows. An output sequence y will be said to be\nε-typical for a code sequence x if\npXY (x, y)\nλ(x, y) = log pX(x)pY (y) ≥ N(D(pXY ||pXpY ) - ε).\n2 log Sy/Sn, we find that this is\nequivalent to\nSubstituting for the pdfs and recalling that D(pXY ||pXpY ) = 1\n||y - x||2\n≤ ||y||2\n+ 2Nε.\nSn\nSy\nSince ||y||2/N is almost surely very close to its mean Sy, this amounts to asking that ||y -x||2/N\nbe very close to its mean Sn under the hypothesis that x and y are drawn according to the joint\npdf pXY (x, y). The correct codeword will therefore almost surely meet this test.\nAccording to Exercise 4, the probability that any particular incorrect codeword meets the test\npXY (x, y)\nλ(x, y) = log pX(x)pY (y) ≥ ND(pXY ||pXpY )\nis upperbounded by e-ND(pXY ||pXpY ) = 2-NI(X;Y ). If we relax this test by an arbitrarily small\nnumber ε > 0, then by the continuity of the Chernoff exponent, the exponent will decrease\nby an amount δ(ε) which can be made arbitrarily small. Therefore we can assert that the\nprobability that a random output sequence Y will be ε-typical for a random incorrect sequence\nX is upperbounded by\nPr{Y ε-typical for X} ≤ 2-N(I(X;Y )-δ(ε)),\nwhere δ(ε) → 0 as ε → 0.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\n= 2ρN/2\nNow if the random codes have rate ρ < 2I(X; Y ) b/2D, then there are M\ncode-\nwords, so by the union bound the total probability of any incorrect codeword being ε-typical is\nupperbounded by\nPr{Y ε-typical for any incorrect X} ≤(M -1)2-N (I(X;Y )-δ(ε)) < 2-N (I(X;Y )-ρ/2-δ(ε)).\nIf ρ < 2I(X; Y ) and ε is small enough, then the exponent will be positive and this probability\nwill go to zero as N →inf.\nThus we have proved the forward part of the capacity theorem: the probability of any kind\nof error with Shannon's random code ensemble and this variant of typical-set decoding goes to\nzero as N →inf, in fact exponentially with N.\n3.3\nGeometric interpretation and converse\nFor AWGN channels, the channel capacity theorem has a nice geometric interpretation in terms\nof the geometry of spheres in real Euclidean N-space RN .\nBy any law of large numbers, the probability that the squared Euclidean norm ||X||2 of a\nrandom sequence X of iid Gaussian variables of mean zero and variance Sx per symbol falls in\nthe range N(Sx ± ε) goes to 1 as N →inf, for any ε > 0. Geometrically, the typical region\nTε = {x ∈RN | N(Sx -ε) ≤||x||2 ≤N(Sx + ε)}\nis a spherical shell with outer squared radius N(Sx + ε) and inner squared radius N(Sx -ε).\nThus the random N-vector X will almost surely lie in the spherical shell Tε as N →inf. This\nphenomenon is known as \"sphere hardening.\"\nMoreover, the pdf pX (x) within the spherical shell Tε is approximately uniform, as we expect\nfrom the asymptotic equipartition principle (AEP). Since pX (x) = (2πSx)-N/2 exp -||x||2/2Sx,\nwithin Tε we have\n-(N/2)(ε/Sx) ≤pX (x) ≤(2πeSx)-N/2 e(N/2)(ε/Sx)\n(2πeSx)-N/2 e\n.\nMoreover, the fact that pX (x) ≈(2πeSx)-N/2 implies that the volume of Tε is approximately\n|Tε| ≈(2πeSx)N/2 . More precisely, we have\n1 -δ(N) ≤\npX (x) dx ≤1,\nTε\nwhere δ(N) →0 as N →inf. Since |Tε| =\ndx, we have\nTε\n-(N/2)(ε/Sx)\n1 ≥(2πeSx)-N/2 e\n|Tε|\n⇒\n|Tε| ≤(2πeSx)N/2 e(N/2)(ε/Sx);\ne(N/2)(ε/Sx)\n-(N/2)(ε/Sx)\n1 -δ(N) ≤(2πeSx)-N/2\n|Tε|\n⇒\n|Tε| ≥(1 -δ(N))(2πeSx)N/2 e\n.\nSince these bounds hold for any ε > 0, this implies that\nlog |Tε|\nlim\n= 2 log 2πeSx = H(X),\nN →inf\nN\nwhere H(X) = 1\n2 log 2πeSx denotes the differential entropy of a Gaussian random variable with\nmean zero and variance Sx.\n\n3.3. GEOMETRIC INTERPRETATION AND CONVERSE\nWe should note at this point that practically all of the volume of an N-sphere of squared radius\nN(Sx + ε) lies within the spherical shell |Tε| as N →inf, for any ε > 0. By dimensional analysis,\nthe volume of an N-sphere of radius r must be given by AN rN for some constant AN that does\nnot depend on r. Thus the ratio of the volume of an N-sphere of squared radius N(Sx -ε) to\nthat of an N-sphere of squared radius N(Sx + ε) must satisfy\nSx -ε N/2\nAN (N(Sx -ε))N/2\n=\n→0 as N →inf, for any ε > 0.\nAN (N(Sx + ε))N/2\nSx + ε\nIt follows that the volume of an N-sphere of squared radius NSx is also approximated by\neN H(X) = (2πeSx)N/2 as N →inf.\nExercise 5. In Exercise 4 of Chapter 1, the volume of an N-sphere of radius r was given as\nV⊗(N, r) = (πr2)N/2\n,\n(N/2)!\nfor N even. In other words, AN = πN/2/((N/2)!). Using Stirling's approximation, m! →(m/e)m\nas m →inf, show that this exact expression leads to the same asymptotic approximation for\nV⊗(N, r) as was obtained above by use of the asymptotic equipartition principle.\nThe sphere-hardening phenomenon may seem somewhat bizarre, but even more unexpected\nphenomena occur when we code for the AWGN channel using Shannon's random code ensemble.\nIn this case, each randomly chosen transmitted N-vector X will almost surely lie in a spherical\nshell TX of squared radius ≈NSx, and the random received N-vector Y will almost surely lie\nin a spherical shell TY of squared radius ≈NSy , where Sy = Sx + Sn.\nMoreover, given the correct transmitted codeword c0, the random received vector Y will\nalmost surely lie in a spherical shell Tε(c0) of squared radius ≈NSn centered on c0. A further\nconsequence of the AEP is that almost all of the volume of this nonzero-mean shell, whose\ncenter c0 has squared Euclidean norm ||c0||2 ≈ NSx, lies in the zero-mean shell TY whose\nsquared radius is ≈NSy, since the expected squared Euclidean norm of Y = c0 + N is\nEN [||Y||2] = ||c0||2 + NSn ≈NSy.\n\"Curiouser and curiouser,\" said Alice.\n= 2ρN/2\nWe thus obtain the following geometrical picture. We choose M\ncode vectors at\nrandom according to a zero-mean Gaussian distribution with variance Sx, which almost surely\nputs them within the shell TX of squared radius ≈ NSx. Considering the probable effects of\na random noise sequence N distributed according to a zero-mean Gaussian distribution with\nvariance Sn, we can define for each code vector ci a typical region Tε(ci) of volume |Tε(ci)| ≈\n(2πeSn)N/2, which falls almost entirely within the shell TY of volume |TY | ≈(2πeSy)N/2 .\nNow if a particular code vector c0 is sent, then the probability that the received vector y will\nfall in the typical region Tε(c0) is nearly 1. On the other hand, the probability that y will fall\nin the typical region Tε(ci) of some other independently-chosen code vector ci is approximately\nequal to the ratio |Tε(ci)|/|TY | of the volume of Tε(ci) to that of the entire shell, since if y\nis generated according to py (y) independently of ci, then it will be approximately uniformly\ndistributed over TY . Thus this probability is approximately\nSn\nN/2\n(2πeSn)N/2\n=\nPr{Y typical for ci} ≈ |T\n|\nε\nT\n(\nY\nci\n|\n)| ≈\n\n.\n(2πeSy )N/2\nSy\nAs we have seen in earlier sections, this argument may be made precise.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nIt follows then that if ρ < log2(1 + Sx/Sn) b/2D, or equivalently M = 2ρN/2 < (Sy/Sn)N/2 ,\nthen the probability that Y is typical with respect to any of the M - 1 incorrect codewords is\nvery small, which proves the forward part of the channel capacity theorem.\nOn the other hand, it is clear from this geometric argument that if ρ > log2(1 + Sx/Sn) b/2D,\nor equivalently M = 2ρN/2 > (Sy/Sn)N/2, then the probability of decoding error must be large.\nFor the error probability to be small, the decision region for each code vector ci must include\nalmost all of its typical region Tε(ci). If the volume of the M = 2ρN/2 typical regions exceeds\nthe volume of TY , then this is impossible. Thus in order to have small error probability we must\nhave\nSx\n2ρN/2(2πeSn)N/2 ≤ (2πeSy)N/2\n⇒\nρ ≤ log2\nSy = log2(1 +\n) b/2D.\nSn\nSn\nThis argument may also be made precise, and is the converse to the channel capacity theorem.\nIn conclusion, we obtain the following picture of a capacity-achieving code. Let TY be the\nN-shell of squared radius ≈ NSy, which is almost the same thing as the N-sphere of squared\nradius NSy. A capacity-achieving code consists of the centers ci of M typical regions Tε(ci),\nwhere ||ci||2 ≈ NSx and each region Tε(ci) consists of an N-shell of squared radius ≈ NSn\ncentered on ci, which is almost the same thing as an N-sphere of squared radius NSx. As\nρ → C[b/2D] = log2(1 + Sx ) b/2D, these regions Tε(ci) form an almost disjoint partition of TY .\nSn\nThis picture is illustrated in Figure 2.\n'$\nnn\nn\nn\n&%\nFigure 2. Packing ≈ (Sy /Sn)N/2 typical regions Tε(ci) of squared radius ≈ NSn into a large\ntypical region TY of squared radius ≈ NSy .\n3.3.1\nDiscussion\nIt is natural in view of the above picture to frame the problem of coding for the AWGN channel\nas a sphere-packing problem. In other words, we might expect that a capacity-achieving code\nbasically induces a disjoint partition of an N-sphere of squared radius NSy into about (Sy/Sn)N/2\ndisjoint decision regions, such that each decision region includes the sphere of squared radius\nNSn about its center.\nHowever, it can be shown by geometric arguments that such a disjoint partition is impossible\nas the code rate approaches capacity. What then is wrong with the sphere-packing approach?\nThe subtle distinction that makes all the difference is that Shannon's probabilistic approach\ndoes not require decision regions to be disjoint, but merely probabilistically almost disjoint. So\nthe solution to Shannon's coding problem involves what might be called \"soft sphere-packing.\"\nWe will see that hard sphere-packing-- i.e., maximizing the minimum distance between code\nvectors subject to a constraint on average energy-- is a reasonable approach for moderate-size\ncodes at rates not too near to capacity. However, to obtain reliable transmission at rates near\ncapacity, we will need to consider probabilistic codes and decoding algorithms that follow more\nclosely the spirit of Shannon's original work."
    },
    {
      "category": "Resource",
      "title": "chap_2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/7cb3929341f072786598cd05c69a3f5c_chap_2.pdf",
      "content": "Chapter 2\nDiscrete-time and continuous-time\nAWGN channels\nIn this chapter we begin our technical discussion of coding for the AWGN channel. Our purpose\nis to show how the continuous-time AWGN channel model Y (t) = X(t) + N(t) may be reduced\nto an equivalent discrete-time AWGN channel model Y = X + N, without loss of generality or\noptimality. This development relies on the sampling theorem and the theorem of irrelevance.\nMore practical methods of obtaining such a discrete-time model are orthonormal PAM or QAM\nmodulation, which use an arbitrarily small amount of excess bandwidth. Important parameters\nof the continuous-time channel such as SNR, spectral efficiency and capacity carry over to\ndiscrete time, provided that the bandwidth is taken to be the nominal (Nyquist) bandwidth.\nReaders who are prepared to take these assertions on faith may skip this chapter.\n2.1\nContinuous-time AWGN channel model\nThe continuous-time AWGN channel is a random channel whose output is a real random process\nY (t) = X(t) + N(t),\nwhere X(t) is the input waveform, regarded as a real random process, and N(t) is a real white\nGaussian noise process with single-sided noise power density N0 which is independent of X(t).\nMoreover, the input X(t) is assumed to be both power-limited and band-limited. The average\ninput power of the input waveform X(t) is limited to some constant P. The channel band B\nis a positive-frequency interval with bandwidth W Hz. The channel is said to be baseband if\nB = [0, W], and passband otherwise. The (positive-frequency) support of the Fourier transform\nof any sample function x(t) of the input process X(t) is limited to B.\nThe signal-to-noise ratio SNR of this channel model is then\nP\nSNR =\n,\nN0W\nwhere N0W is the total noise power in the band B. The parameter N0 is defined by convention\nto make this relationship true; i.e., N0 is the noise power per positive-frequency Hz. Therefore\nthe double-sided power spectral density of N(t) must be Snn(f) = N0/2, at least over the bands\n±B.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nThe two parameters W and SNR turn out to characterize the channel completely for digital\ncommunications purposes; the absolute scale of P and N0 and the location of the band B do not\naffect the model in any essential way. In particular, as we will show in Chapter 3, the capacity\nof any such channel in bits per second is\nC[b/s] = W log2(1 + SNR) b/s.\nIf a particular digital communication scheme transmits a continuous bit stream over such a\nchannel at rate R b/s, then the spectral efficiency of the scheme is said to be ρ = R/W (b/s)/Hz\n(read as \"bits per second per Hertz\"). The Shannon limit on spectral efficiency is therefore\nC[(b/s)/Hz] = log2(1 + SNR) (b/s)/Hz;\ni.e., reliable transmission is possible when ρ < C[(b/s)/Hz], but not when ρ > C[(b/s)/Hz].\n2.2\nSignal spaces\nIn the next few sections we will briefly review how this continuous-time model may be reduced\nto an equivalent discrete-time model via the sampling theorem and the theorem of irrelevance.\nWe assume that the reader has seen such a derivation previously, so our review will be rather\nsuccinct.\nThe set of all real finite-energy signals x(t), denoted by L2, is a real vector space; i.e., it is\nclosed under addition and under multiplication by real scalars. The inner product of two signals\nx(t), y(t) ∈L2 is defined by\n⟨x(t), y(t)⟩ =\nx(t)y(t) dt.\nThe squared Euclidean norm (energy) of x(t) ∈L2 is defined as ||x(t)||2 = ⟨x(t), x(t)⟩ < inf, and\nthe squared Euclidean distance between x(t), y(t) ∈L2 is d2(x(t), y(t)) = ||x(t) - y(t)||2. Two\nsignals in L2 are regarded as the same (L2-equivalent) if their distance is 0. This allows the\nfollowing strict positivity property to hold, as it must for a proper distance metric:\n||x(t)||2 ≥ 0,\nwith strict inequality unless x(t) is L2-equivalent to 0.\nEvery signal x(t) ∈L2 has an L2 Fourier transform\nx(t)e -2πift\nˆx(f) =\ndt,\nsuch that, up to L2-equivalence, x(t) can be recovered by the inverse Fourier transform:\n2πiftdf.\nx(t) =\nx(f)e\nˆ\nWe write ˆ\nx(f)), and x(t) ↔ ˆ\nx(f) = F (x(t)), x(t) = F -1(ˆ\nx(f).\nIt can be shown that an L2 signal x(t) is L2-equivalent to a signal which is continuous except\nat a discrete set of points of discontinuity (\"almost everywhere\"); therefore so is ˆx(f). The\nvalues of an L2 signal or its transform at points of discontinuity are immaterial.\n\n2.3. THE SAMPLING THEOREM\nBy Parseval's theorem, the Fourier transform preserves inner products:\n⟨x(t), y(t)⟩ = ⟨ˆ\nˆ\ny\nx(f), yˆ(f)⟩ =\nx(f)ˆ∗(f) df.\nIn particular, ||x(t)||2 = ||ˆx(f)||2 .\nA signal space is any subspace S ⊆ L2. For example, the set of L2 signals that are time-limited\nto an interval [0, T] (\"have support [0, T ]\") is a signal space, as is the set of L2 signals whose\nFourier transforms are nonzero only in ±B (\"have frequency support ±B\").\nEvery signal space S ⊆ L2 has an orthogonal basis {φk (t), k ∈I}, where I is some discrete\nindex set, such that every x(t) ∈S may be expressed as\n⟨x(t), φk (t)⟩\nx(t) =\n||φk (t)||2 φk (t),\nk∈I\nup to L2 equivalence. This is called an orthogonal expansion of x(t).\nOf course this expression becomes particularly simple if {φk (t)} is an orthonormal basis with\n||φk (t)||2 = 1 for all k ∈I. Then we have the orthonormal expansion\nx(t) =\nxk φk(t),\nk∈I\nwhere x = {xk = ⟨x(t), φk (t)⟩, k ∈I} is the corresponding set of orthonormal coefficients. From\nthis expression, we see that inner products are preserved in an orthonormal expansion; i.e.,\n⟨x(t), y(t)⟩ = ⟨x, y⟩ =\nxk yk .\nk∈I\nIn particular, ||x(t)||2 = ||x||2 .\n2.3\nThe sampling theorem\nThe sampling theorem allows us to convert a continuous signal x(t) with frequency support\n[-W, W ] (i.e., a baseband signal with bandwidth W) to a discrete-time sequence of samples\n{x(kT), k ∈ Z} at a rate of 2W samples per second, with no loss of information.\nThe sampling theorem is basically an orthogonal expansion for the space L2[0, W ] of sig\nnals that have frequency support [-W, W ]. If T = 1/2W, then the complex exponentials\n{exp(2πifkT), k ∈ Z} form an orthogonal basis for the space of Fourier transforms with support\n[-W, W ]. Therefore their scaled inverse Fourier transforms {φk (t) = sincT (t - kT), k ∈ Z} form\nan orthogonal basis for L2[0, W], where sincT (t) = (sin πt/T)/(πt/T). Since ||sincT (t)||2 = T,\nevery x(t) ∈L2[0, W] may therefore be expressed up to L2 equivalence as\nx(t) =\n⟨x(t), sincT (t - kT)⟩sincT (t - kT).\nT k∈Z\nMoreover, evaluating this equation at t = jT gives x(jT) = T ⟨x(t), sincT (t - jT)⟩ for all j ∈\nZ (provided that x(t) is continuous at t = jT), since sincT ((j - k)T ) = 1 for k = j and\nsincT ((j - k)T) = 0 for k = j. Thus if x(t) ∈L2[0, W ] is continuous, then\nx(t) =\nx(kT )sincT (t - kT ).\nk∈Z\nThis is called the sampling theorem.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nSince inner products are preserved in an orthonormal expansion, and here the orthonormal\n√\ncoefficients are xk = √ ⟨x(t), sincT (t -kT )⟩=\nTx(kT ), we have\nT\n⟨x(t), y(t)⟩= ⟨x, y⟩= T\nx(kT )y(kT ).\nk∈Z\nThe following exercise shows similarly how to convert a continuous passband signal x(t) with\nbandwidth W (i.e., with frequency support ±[fc -W/2, fc + W/2] for some center frequency\nfc > W/2) to a discrete-time sequence of sample pairs {(xc,k, xs,k), k ∈Z} at a rate of W pairs\nper second, with no loss of information.\nExercise 2.1 (Orthogonal bases for passband signal spaces)\n(a) Show that if {φk (t)} is an orthogonal set of signals in L2[0, W ],\nthen\n{φk (t) cos 2πfct, φk (t) sin 2πfct} is an orthogonal set of signals in L2[fc -W, fc + W ], the set\nof signals in L2 that have frequency support ±[fc -W, fc + W ], provided that fc ≥W .\n[Hint:\nuse the facts that F(φk (t) cos 2πfct)\n=\n(φˆk (f - fc) + φˆk (f + fc))/2\nand\n\nF(φk(t) sin 2πfct) = (φˆk (f -fc) -φˆk (f + fc))/2i, plus Parseval's theorem.]\n(b) Show that if the set {φk(t)} is an orthogonal basis for L2[0, W ], then the set\n{φk (t) cos 2πfct, φk (t) sin 2πfct} is an orthogonal basis for L2[fc -W, fc + W ], provided that\nfc ≥W .\n[Hint: show that every x(t) ∈L2[fc -W, fc + W ] may be written as x(t) = xc(t) cos 2πfct +\nxs(t) sin 2πfct for some xc(t), xs(t) ∈L2[0, W ].]\n(c) Conclude that every x(t) ∈L2[fc -W, fc + W ] may be expressed up to L2 equivalence as\n\nx(t) =\n(xc,k cos 2πfct + xs,k sin 2πfct) sincT (t -kT ),\nT\n\n= 2W ,\nk∈Z\nfor some sequence of pairs {(xc,k, xs,k ), k ∈Z}, and give expressions for xc,k and xs,k .\n2.4\nWhite Gaussian noise\nThe question of how to define a white Gaussian noise (WGN) process N(t) in general terms is\nplagued with mathematical difficulties. However, when we are given a signal space S ⊆L2 with\nan orthonormal basis as here, then defining WGN with respect to S is not so problematic. The\nfollowing definition captures the essential properties that hold in this case:\nDefinition 2.1 (White Gaussian noise with respect to a signal space S) Let S ⊆ L2\nbe a signal space with an orthonormal basis {φk (t), k ∈I}. A Gaussian process N(t) is de\nfined as white Gaussian noise with respect to S with single-sided power spectral density N0 if\n(a) The sequence {Nk = ⟨N(t), φk (t)⟩, k ∈I} is a sequence of iid Gaussian noise variables with\nmean zero and variance N0/2;\n(b) Define the \"in-band noise\" as the projection N|S (t) =\nNk φk (t) of N(t) onto the signal\nk∈I\nspace S, and the \"out-of-band noise\" as N|S⊥ (t) = N(t) -N|S (t). Then N|S⊥ (t) is a process\nwhich is jointly Gaussian with N|S (t), has sample functions which are orthogonal to S, is\nuncorrelated with N|S (t), and thus is statistically independent of N|S (t).\n\n2.4. WHITE GAUSSIAN NOISE\nFor example, any stationary Gaussian process whose single-sided power spectral density is\nequal to N0 within a band B and arbitrary elsewhere is white with respect to the signal space\nL2(B) of signals with frequency support ±B.\nExercise 2.2 (Preservation of inner products) Show that a Gaussian process N(t) is white\nwith respect to a signal space S ⊆L2 with psd N0 if and only if for any signals x(t), y(t) ∈S,\nE[⟨N(t), x(t)⟩⟨N(t), y(t)⟩] = N0 ⟨x(t), y(t)⟩.\nHere we are concerned with the detection of signals that lie in some signal space S in the\npresence of additive white Gaussian noise. In this situation the following theorem is fundamental:\nTheorem 2.1 (Theorem of irrelevance) Let X(t) be a random signal process whose sample\nfunctions x(t) lie in some signal space S ⊆ L2 with an orthonormal basis {φk (t), k ∈I}, let\nN(t) be a Gaussian noise process which is independent of X(t) and white with respect to S, and\nlet Y (t) = X(t) + N(t). Then the set of samples\nYk = ⟨Y (t), φk (t)⟩,\nk\n\n∈I,\nis a set of sufficient statistics for detection of X(t) from Y (t).\nSketch of proof. We may write\nY (t) = Y|S (t) + Y|S⊥ (t),\nwhere Y|S (t) =\nYk φk (t) and Y|S⊥ (t) = Y (t) -Y|S (t). Since Y (t) = X(t) + N(t) and\nk\nX(t) =\n⟨X(t), φk(t)⟩φk (t),\nk\nsince all sample functions of X(t) lie in S, we have\nY (t) =\nYk φk (t) + N|S⊥ (t),\nk\nwhere N|S⊥ (t) = N(t) -\n⟨N(t), φk (t)⟩φk (t). By Definition 2.1, N|S⊥ (t) is independent of\nk\nN|S (t) =\n⟨N(t), φk(t)⟩φk (t), and by hypothesis it is independent of X(t). Thus the proba\nk\nbility distribution of X(t) given Y|S (t) =\nYk φk (t) and Y|S⊥ (t) = N|S⊥ (t) depends only on\nk\nY|S (t), so without loss of optimality in detection of X(t) from Y (t) we can disregard Y|S⊥ (t);\ni.e., Y|S (t) is a sufficient statistic. Moreover, since Y|S (t) is specified by the samples {Yk }, these\nsamples equally form a set of sufficient statistics for detection of X(t) from Y (t).\nThe sufficient statistic Y|S (t) may alternatively be generated by filtering out the out-of-band\nnoise N|S⊥ (t). For example, for the signal space L2(B) of signals with frequency support ±B,\nwe may obtain Y|S (t) by passing Y (t) through a brick-wall filter which passes all frequency\ncomponents in B and rejects all components not in B.\n1Theorem 2.1 may be extended to any model Y (t) = X(t) + N (t) in which the out-of-band noise N|S⊥ (t) =\nN (t) - N|S (t) is independent of both the signal X(t) and the in-band noise N|S (t) =\nNk φk (t); e.g., to models\nk\nin which the out-of-band noise contains signals from other independent users. In the Gaussian case, independence\nof the out-of-band noise is automatic; in more general cases, independence is an additional assumption.\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nCombining Definition 2.1 and Theorem 2.1, we conclude that for any AWGN channel in which\nthe signals are confined to a sample space S with orthonormal basis {φk(t), k ∈I}, we may\nwithout loss of optimality reduce the output Y (t) to the set of samples\nYk = ⟨Y (t), φk(t)⟩ = ⟨X(t), φk (t)⟩ + ⟨N(t), φk (t)⟩ = Xk + Nk ,\nk\n\n∈I,\nwhere {Nk, k ∈I} is a set of iid Gaussian variables with mean zero and variance N0/2. Moreover,\nif x1(t), x2(t) ∈S are two sample functions of X(t), then this orthonormal expansion preserves\ntheir inner product:\n⟨x1(t), x2(t)⟩ = ⟨x1, x2⟩,\nwhere x1 and x2 are the orthonormal coefficient sequences of x1(t) and x2(t), respectively.\n2.5\nContinuous time to discrete time\nWe now specialize these results to our original AWGN channel model Y (t) = X(t) + N(t),\nwhere the average power of X(t) is limited to P and the sample functions of X(t) are required\nto have positive frequency support in a band B of width W. For the time being we consider the\nbaseband case in which B = [0, W].\nThe signal space is then the set S = L2[0, W] of all finite-energy signals x(t) whose Fourier\ntransform has support ±B. The sampling theorem shows that {φk (t) = √1 sincT (t-kT), k ∈ Z}\nT\nis an orthonormal basis for this signal space, where T = 1/2W, and that therefore without loss\nof generality we may write any x(t) ∈S as\nx(t) =\nxk φk(t),\nk∈Z\nwhere xk is the orthonormal coefficient xk = ⟨x(t), φk (t)⟩, and equality is in the sense of L2\nequivalence.\nConsequently, if X(t) is a random process whose sample functions x(t) are all in S, then we\ncan write\nX(t) =\nXkφk (t),\nk∈Z\nwhere Xk = ⟨X(t), φk (t)⟩ =\nX(t)φk (t) dt, a random variable that is a linear functional of\nX(t). In this way we can identify any random band-limited process X(t) of bandwidth W with\na discrete-time random sequence X = {Xk } at a rate of 2W real variables per second. Hereafter\nthe input will be regarded as the sequence X rather than X(t).\nThus X(t) may be regarded as a sum of amplitude-modulated orthonormal pulses Xkφk(t).\nBy the Pythagorean theorem,\n||X(t)||2 =\n||Xk φk (t)||2 =\nXk\n2 ,\nk∈Z\nk∈Z\nwhere we use the orthonormality of the φk (t). Therefore the requirement that the average power\n(energy per second) of X(t) be less than P translates to a requirement that the average energy\nof the sequence X be less than P per 2W symbols, or equivalently less than P/2W per symbol.2\n2The requirement that the sample functions of X(t) must be in L2 translates to the requirement that the\nsample sequences x of X must have finite energy. This requirement can be met by requiring that only finitely\nmany elements of x be nonzero. However, we do not pursue such finiteness issues.\n\n2.5. CONTINUOUS TIME TO DISCRETE TIME\nSimilarly, the random Gaussian noise process N(t) may be written as\nN(t) =\nNkφk(t) + N|S⊥ (t)\nk∈Z\nwhere N = {Nk = ⟨N(t), φk(t)⟩} is the sequence of orthonormal coefficients of N(t) in S,\nand N|S⊥ (t) = N(t) -\nNkφk(t) is out-of-band noise. The theorem of irrelevance shows\nk\nthat N|S⊥ (t) may be disregarded without loss of optimality, and therefore that the sequence\nY = X + N is a set of sufficient statistics for detection of X(t) from Y (t).\nIn summary, we conclude that the characteristics of the discrete-time model Y = X + N mirror\nthose of the continuous-time model Y (t) = X(t) + N(t) from which it was derived:\n- The symbol interval is T = 1/2W ; equivalently, the symbol rate is 2W symbols/s;\n- The average signal energy per symbol is limited to P/2W ;\n- The noise sequence N is iid zero-mean (white) Gaussian, with variance N0/2 per symbol;\n- The signal-to-noise ratio is thus SNR = (P/2W )/(N0/2) = P/N0W , the same as for the\ncontinuous-time model;\n- A data rate of ρ bits per two dimensions (b/2D) translates to a data rate of R = Wρ b/s,\nor equivalently to a spectral efficiency of ρ (b/s)/Hz.\nThis important conclusion is the fundamental result of this chapter.\n2.5.1\nPassband case\nSuppose now that the channel is instead a passband channel with positive-frequency support\nband B = [fc - W/2, fc + W/2] for some center frequency fc > W/2.\nThe signal space is then the set S = L2[fc - W/2, fc + W/2] of all finite-energy signals x(t)\nwhose Fourier transform has support ±B.\nIn this case Exercise 2.1 shows that an orthogonal basis for the signal space is a set of signals\nof the form φk,c(t) = sincT (t - kT ) cos 2πfct and φk,s(t) = sincT (t - kT ) sin 2πfct, where the\nsymbol interval is now T = 1/W . Since the support of the Fourier transform of sincT (t - kT ) is\n[-W/2, W/2], the support of the transform of each of these signals is ±B.\nThe derivation of a discrete-time model then goes as in the baseband case. The result is that\nthe sequence of real pairs\n(Yk,c, Yk,s) = (Xk,c, Xk,s) + (Nk,c, Nk,s)\nis a set of sufficient statistics for detection of X(t) from Y (t). If we compute scale factors\ncorrectly, we find that the characteristics of this discrete-time model are as follows:\n- The symbol interval is T = 1/W , or the symbol rate is W symbols/s. In each symbol\ninterval a pair of two real symbols is sent and received. We may therefore say that the rate\nis 2W = 2/T real dimensions per second, the same as in the baseband model.\n- The average signal energy per dimension is limited to P/2W ;\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\n- The noise sequences Nc and Ns are independent real iid zero-mean (white) Gaussian se\nquences, with variance N0/2 per dimension;\n- The signal-to-noise ratio is again SNR = (P/2W )/(N0/2) = P/N0W ;\n- A data rate of ρ b/2D again translates to a spectral efficiency of ρ (b/s)/Hz.\nThus the passband discrete-time model is effectively the same as the baseband model.\nIn the passband case, it is often convenient to identify real pairs with single complex variables\n√\nvia the standard correspondence between R2 and C given by (x, y) ↔ x + iy, where i =\n-1.\nThis is possible because a complex iid zero-mean Gaussian sequence N with variance N0 per\ncomplex dimension may be defined as N = Nc + iNs, where Nc and Ns are independent real\niid zero-mean Gaussian sequences with variance N0/2 per real dimension. Then we obtain a\ncomplex discrete-time model Y = X + N with the following characteristics:\n- The symbol interval is T = 1/W , or the rate is W complex dimensions/s.\n- The average signal energy per complex dimension is limited to P/W ;\n- The noise sequence N is a complex iid zero-mean Gaussian sequence, with variance N0 per\ncomplex dimension;\n- The signal-to-noise ratio is again SNR = (P/W )/N0 = P/N0W ;\n- A data rate of ρ bits per complex dimension translates to a spectral efficiency of ρ (b/s)/Hz.\nThis is still the same as before, if we regard one complex dimension as two real dimensions.\nNote that even the baseband real discrete-time model may be converted to a complex discrete-\ntime model simply by taking real variables two at a time and using the same map R2 → C.\nThe reader is cautioned that the correspondence between R2 and C given by (x, y) ↔ x + iy\npreserves some algebraic, geometric and probabilistic properties, but not all.\nExercise 2.3 (Properties of the correspondence R2 ↔ C) Verify the following assertions:\n(a) Under the correspondence R2 ↔ C, addition is preserved.\n(b) However, multiplication is not preserved. (Indeed, the product of two elements of R2 is not\neven defined.)\n(c) Inner products are not preserved. Indeed, two orthogonal elements of R2 can map to two\ncollinear elements of C.\n(d) However, (squared) Euclidean norms and Euclidean distances are preserved.\n(e) In general, if Nc and Ns are real jointly Gaussian sequences, then Nc + iNs is not a proper\ncomplex Gaussian sequence, even if Nc and Ns are independent iid sequences.\n(f) However, if Nc and Ns are independent real iid zero-mean Gaussian sequences with variance\nN0/2 per real dimension, then Nc + iNs is a complex zero-mean Gaussian sequence with\nvariance N0 per complex dimension.\n\n2.6. ORTHONORMAL PAM AND QAM MODULATION\n2.6\nOrthonormal PAM and QAM modulation\nMore generally, suppose that X(t) =\nXk φk (t), where X = {Xk } is a random sequence and\nk\n{φk (t) = p(t -kT )} is an orthonormal sequence of time shifts p(t -kT ) of a basic modulation\npulse p(t) ∈L2 by integer multiples of a symbol interval T . This is called orthonormal pulse-\namplitude modulation (PAM).\nThe signal space S is then the subspace of L2 spanned by the orthonormal sequence {p(t-kT )};\ni.e., S consists of all signals in L2 that can be written as linear combinations\nk xk p(t -kT ).\nAgain, the average power of X(t) =\nXk p(t -kT ) will be limited to P if the average energy\nk\nof the sequence X is limited to PT per symbol, since the symbol rate is 1/T symbol/s.\nThe theorem of irrelevance again shows that the set of inner products\nYk = ⟨Y (t), φk(t)⟩= ⟨X(t), φk (t)⟩+ ⟨N(t), φk (t)⟩= Xk + Nk\nis a set of sufficient statistics for detection of X(t) from Y (t). These inner products may be\nobtained by filtering Y (t) with a matched filter with impulse response p(-t) and sampling at\ninteger multiples of T as shown in Figure 1 to obtain\nZ(kT ) =\nY (τ)p(τ -kT ) dτ = Yk,\nThus again we obtain a discrete-time model Y = X + N, where by the orthonormality of the\np(t -kT ) the noise sequence N is iid zero-mean Gaussian with variance N0/2 per symbol.\n-\nOrthonormal\nX = {Xk }\nX(t) =\nXkp(t -kT )\nk\nPAM\nmodulator\n-\n-\n( )\nN t\n?\n+\n\nY (t)\np(-t)\nsample at\n\nt = kT\nY =\n-\n{Yk }\nFigure 1. Orthonormal PAM system.\nThe conditions that ensure that the time shifts {p(t-kT )} are orthonormal are determined by\nNyquist theory as follows. Define the composite response in Figure 1 as g(t) = p(t) ∗p(-t), with\nFourier transform ˆg(f) = |pˆ(f)|2. (The composite response g(t) is also called the autocorrelation\nfunction of p(t), and ˆg(f) is also called its power spectrum.) Then:\nTheorem 2.2 (Orthonormality conditions) For a signal p(t) ∈L2 and a time interval T ,\nthe following are equivalent:\n(a) The time shifts {p(t -kT ), k ∈Z} are orthonormal;\n(b) The composite response g(t) = p(t) ∗p(-t) satisfies g(0) = 1 and g(kT ) = 0 for k = 0;\n(c) The Fourier transform gˆ(f) = |pˆ(f)|2 satisfies the Nyquist criterion for zero intersymbol\ninterference, namely\ngˆ(f -m/T ) = 1 for all f.\nT m∈Z\nSketch of proof. The fact that (a) ⇔(b) follows from ⟨p(t -kT ), p(t -k′T )⟩= g((k -k′)T ).\nThe fact that (b) ⇔ (c) follows from the aliasing theorem, which says that the discrete-time\nFourier transform of the sample sequence {g(kT )} is the aliased response 1\ngˆ(f -m/T ).\nT\nm\n\nCHAPTER 2. DISCRETE-TIME AND CONTINUOUS-TIME AWGN CHANNELS\nIt is clear from the Nyquist criterion (c) that if p(t) is a baseband signal of bandwidth W, then\n(i) The bandwidth W cannot be less than 1/2T;\n(ii) If W = 1/2T, then ˆ\ng(f) = 0; i.e., g(t) = sincT (t);\ng(f) = T, -W ≤f ≤W, else ˆ\n(iii) If 1/2T < W ≤ 1/T, then any real non-negative power spectrum ˆg(f) that satisfies\ngˆ(1/2T + f) + ˆg(1/2T -f) = T for 0 ≤f ≤1/2T will satisfy (c).\nFor this reason W = 1/2T is called the nominal or Nyquist bandwidth of a PAM system with\nsymbol interval T. No orthonormal PAM system can have bandwidth less than the Nyquist\nbandwidth, and only a system in which the modulation pulse has autocorrelation function g(t) =\np(t)∗p(-t) = sincT (t) can have exactly the Nyquist bandwidth. However, by (iii), which is called\nthe Nyquist band-edge symmetry condition, the Fourier transform |pˆ(f)|2 may be designed to roll\noff arbitrarily rapidly for f > W, while being continuous and having a continuous derivative.\nFigure 2 illustrates a raised-cosine frequency response that satisfies the Nyquist band-edge\nsymmetry condition while being continuous and having a continuous derivative. Nowadays it is\nno great feat to implement such responses with excess bandwidths of 5-10% or less.\nT\nf\nT -|pˆ( 1 -f)|\n2T\n\n*\n\n|pˆ( 1 + f)|\n2T\nT\nFigure 2. Raised-cosine spectrum ˆ g(f) = |pˆ(f)|2 with Nyquist band-edge symmetry.\nWe conclude that an orthonormal PAM system may use arbitrarily small excess bandwidth\nbeyond the Nyquist bandwidth W = 1/2T, or alternatively that the power in the out-of-band\nfrequency components may be made to be arbitrarily small, without violating the practical\nconstraint that the Fourier transform ˆp(f) of the modulation pulse p(t) should be continuous\nand have a continuous derivative.\nIn summary, if we let W denote the Nyquist bandwidth 1/2T rather than the actual bandwidth,\nthen we again obtain a discrete-time channel model Y = X + N for any orthonormal PAM\nsystem, not just a system with the modulation pulse p(t) = √1 sincT (t), in which:\nT\n- The symbol interval is T = 1/2W; equivalently, the symbol rate is 2W symbols/s;\n- The average signal energy per symbol is limited to P/2W;\n- The noise sequence N is iid zero-mean (white) Gaussian, with variance N0/2 per symbol;\n- The signal-to-noise ratio is SNR = (P/2W)/(N0/2) = P/N0W;\n- A data rate of ρ bits per two dimensions (b/2D) translates to a data rate of R = ρ/W b/s,\nor equivalently to a spectral efficiency of ρ (b/s)/Hz.\n\n2.7. SUMMARY\nExercise 2.4 (Orthonormal QAM modulation)\nFigure 3 illustrates an orthonormal quadrature amplitude modulation (QAM) system with\nsymbol interval T in which the input and output variables Xk and Yk are complex, p(t) is a\ncomplex finite-energy modulation pulse whose time shifts {p(t-kT )} are orthonormal (the inner\nproduct of two complex signals is ⟨x(t), y(t)⟩ =\nx(t)y ∗(t) dt), the matched filter response is\n∗\np (-t), and fc > 1/2T is a carrier frequency. The box marked 2R{·} takes twice the real part\nof its input-- i.e., it maps a complex signal f(t) to f(t) + f∗(t)-- and the Hilbert filter is a\ncomplex filter whose frequency response is 1 for f > 0 and 0 for f < 0.\n2πifct\ne\n?\nOrthonormal\n\nX = {Xk }\nX(t) =\nk\n- ×\n-\nXk p(t - kT )\n-\n2R{·}\nQAM\n\nmodulator\n?\n\nN(t)\n\n+\n-2πifct\n\ne\n@\n?\n\nHilbert\nY = {Yk}\n\n@\n∗\n\n×\np (-t)\nsample at\n\nfilter\nt = kT\nFigure 3. Orthonormal QAM system.\n(a) Assume that ˆp(f) = 0 for |f| ≥ fc. Show that the Hilbert filter is superfluous.\n(b) Show that Theorem 2.2 holds for a complex response p(t) if we define the composite\nresponse (autocorrelation function) as g(t) = p(t) ∗ p ∗(-t). Conclude that the bandwidth of an\northonormal QAM system is lowerbounded by its Nyquist bandwidth W = 1/T .\n(c) Show that Y = X + N, where N is an iid complex Gaussian noise sequence. Show that the\nsignal-to-noise ratio in this complex discrete-time model is equal to the channel signal-to-noise\nratio SNR = P/N0W , if we define W = 1/T . [Hint: use Exercise 2.1.]\n(d) Show that a mismatch in the receive filter-- i.e., an impulse response h(t) other than\n∗\np (-t)-- results in linear intersymbol interference-- i.e., in the absence of noise Yk =\nj Xj hk-j\nfor some discrete-time response {hk} other than the ideal response δk0 (Kronecker delta).\n(e) Show that a phase error of θ in the receive carrier-- i.e., demodulation by e-2πifct+iθ rather\nthan by e-2πifct-- results (in the absence of noise) in a phase rotation by θ of all outputs Yk .\n(f) Show that a sample timing error of δ-- i.e., sampling at times t = kT + δ-- results in\nlinear intersymbol interference.\n2.7\nSummary\nTo summarize, the key parameters of a band-limited continuous-time AWGN channel are its\nbandwidth W in Hz and its signal-to-noise ratio SNR, regardless of other details like where the\nbandwidth is located (in particular whether it is at baseband or passband), the scaling of the\nsignal, etc. The key parameters of a discrete-time AWGN channel are its symbol rate W in\ntwo-dimensional real or one-dimensional complex symbols per second and its SNR, regardless of\nother details like whether it is real or complex, the scaling of the symbols, etc. With orthonormal\nPAM or QAM, these key parameters are preserved, regardless of whether PAM or QAM is used,\nthe precise modulation pulse, etc. The (nominal) spectral efficiency ρ (in (b/s)/Hz or in b/2D)\nis also preserved, and (as we will see in the next chapter) so is the channel capacity (in b/s)."
    },
    {
      "category": "Resource",
      "title": "chap_3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/cc4381c9d29759cab78daa5bd1405664_chap_3.pdf",
      "content": "Chapter 3\nCapacity of AWGN channels\nIn this chapter we prove that the capacity of an AWGN channel with bandwidth W and signal-to-\nnoise ratio SNR is W log2(1+SNR) bits per second (b/s). The proof that reliable transmission is\npossible at any rate less than capacity is based on Shannon's random code ensemble, typical-set\ndecoding, the Chernoff-bound law of large numbers, and a fundamental result of large-deviation\ntheory. We also sketch a geometric proof of the converse. Readers who are prepared to accept\nthe channel capacity formula without proof may skip this chapter.\n3.1\nOutline of proof of the capacity theorem\nThe first step in proving the channel capacity theorem or its converse is to use the results\nof Chapter 2 to replace a continuous-time AWGN channel model Y (t) = X(t) + N(t) with\nbandwidth W and signal-to-noise ratio SNR by an equivalent discrete-time channel model Y =\nX + N with a symbol rate of 2W real symbol/s and the same SNR, without loss of generality\nor optimality.\nWe then wish to prove that arbitrarily reliable transmission can be achieved on the discrete-\ntime channel at any rate (nominal spectral efficiency)\nρ < C[b/2D] = log2(1 + SNR) b/2D.\nThis will prove that reliable transmission can be achieved on the continuous-time channel at any\ndata rate\nR < C[b/s] = WC[b/2D] = W log2(1 + SNR) b/s.\nWe will prove this result by use of Shannon's random code ensemble and a suboptimal decoding\ntechnique called typical-set decoding.\nShannon's random code ensemble may be defined as follows. Let Sx = P/2W be the allowable\naverage signal energy per symbol (dimension), let ρ be the data rate in b/2D, and let N be the\ncode block length in symbols. A block code C of length N, rate ρ, and average energy Sx per\ndimension is then a set of M = 2ρN/2 real sequences (codewords) c of length N such that the\nexpected value of ||c||2 under an equiprobable distribution over C is NSx.\nFor example, the three 16-QAM signal sets shown in Figure 3 of Chapter 1 may be regarded as\nthree block codes of length 2 and rate 4 b/2D with average energies per dimension of Sx = 5, 6.75\nand 4.375, respectively.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nIn Shannon's random code ensemble, every symbol ck of every codeword c ∈C is chosen\nindependently at random from a Gaussian ensemble with mean 0 and variance Sx. Thus the\naverage energy per dimension over the ensemble of codes is Sx, and by the law of large numbers\nthe average energy per dimension of any particular code in the ensemble is highly likely to be\nclose to Sx.\nWe consider the probability of error under the following scenario. A code C is selected randomly\nfrom the ensemble as above, and then a particular codeword c0 is selected for transmission. The\nchannel adds a noise sequence n from a Gaussian ensemble with mean 0 and variance Sn = N0/2\nper symbol. At the receiver, given y = c0 + n and the code C, a typical-set decoder implements\nthe following decision rule (where ε is some small positive number):\n- If there is one and only one codeword c ∈C within squared distance N(Sn ± ε) of the\nreceived sequence y, then decide on c;\n- Otherwise, give up.\nA decision error can occur only if one of the following two events occurs:\n- The squared distance ||y - c0||2 between y and the transmitted codeword c0 is not in the\nrange N(Sn ± ε);\n- The squared distance ||y - ci||2 between y and some other codeword ci = c0 is in the range\nN(Sn ± ε).\nSince y - c0 = n, the probability of the first of these events is the probability that ||n||2 is not\nin the range N(Sn - ε) ≤||n||2 ≤ N(Sn + ε). Since N = {Nk} is an iid zero-mean Gaussian\nsequence with variance Sn per symbol and ||N||2 =\nNk\n2, this probability goes to zero as\nk\nN →inf for any ε > 0 by the weak law of large numbers. In fact, by the Chernoff bound of the\nnext section, this probability goes to zero exponentially with N.\nFor any particular other codeword ci ∈C, the probability of the second event is the probability\nthat a code sequence drawn according to an iid Gaussian pdf pX(x) with symbol variance Sx and\na received sequence drawn independently according to an iid Gaussian pdf pY (y) with symbol\nvariance Sy = Sx + Sn are \"typical\" of the joint pdf pXY (x, y) = pX(x)pN (y - x), where here\nwe define \"typical\" by the distance ||x - y||2 being in the range N(Sn ± ε). According to a\nfundamental result of large-deviation theory, this probability goes to zero as e-NE, where, up\nto terms of the order of ε, the exponent E is given by the relative entropy (Kullback-Leibler\ndivergence)\nD(pXY ||pXpY ) =\ndx dy pXY (x, y) log pXY (x, y) .\npX(x)pY (y)\nIf the logarithm is binary, then this is the mutual information I(X; Y ) between the random\nvariables X and Y in bits per dimension (b/D).\nIn the Gaussian case considered here, the mutual information is easily evaluated as\n+ y2 log2 e\nI(X; Y ) = EXY - 2 log2 2πSn - (y - x)2 log2 e + 2 log2 2πSy\n= 2 log2\nSy\nb/D.\n2Sn\n2Sy\nSn\nSince Sy = Sx + Sn and SNR = Sx/Sn, this expression is equal to the claimed capacity in b/D.\n\n3.2. LAWS OF LARGE NUMBERS\nThus we can say that the probability that any incorrect codeword ci ∈C is \"typical\" with\nrespect to y goes to zero as 2-N (I(X;Y )-δ(ε)), where δ(ε) goes to zero as ε →0. By the union\nbound, the probability that any of the M -1 < 2ρN/2 incorrect codewords is \"typical\" with\nrespect to y is upperbounded by\nPr{any incorrect codeword \"typical\"} < 2ρN/22-N (I(X;Y )-δ(ε)),\nwhich goes to zero exponentially with N provided that ρ < 2I(X; Y ) b/2D and ε is small enough.\nIn summary, the probabilities of both types of error go to zero exponentially with N provided\nthat\nρ < 2I(X; Y ) = log2(1 + SNR) = C[b/2D]\nb/2D\nand ε is small enough. This proves that an arbitrarily small probability of error can be achieved\nusing Shannon's random code ensemble and typical-set decoding.\nTo show that there is a particular code of rate ρ < C[b/2D] that achieves an arbitrarily small\nerror probability, we need merely observe that the probability of error over the random code\nensemble is the average probability of error over all codes in the ensemble, so there must be at\nleast one code in the ensemble that achieves this performance. More pointedly, if the average\nerror probability is Pr(E), then no more than a fraction of 1/K of the codes can achieve error\nprobability worse than K Pr(E) for any constant K > 0; e.g., at least 99% of the codes achieve\nperformance no worse than 100 Pr(E). So we can conclude that almost all codes in the random\ncode ensemble achieve very small error probabilities. Briefly, \"almost all codes are good\" (when\ndecoded by typical-set or maximum-likelihood decoding).\n3.2\nLaws of large numbers\nThe channel capacity theorem is essentially an application of various laws of large numbers.\n3.2.1\nThe Chernoff bound\nThe weak law of large numbers states that the probability that the sample average of a sequence\nof N iid random variables differs from the mean by more than ε > 0 goes to zero as N →inf, no\nmatter how small ε is. The Chernoff bound shows that this probability goes to zero exponentially\nwith N, for arbitrarily small ε.\nTheorem 3.1 (Chernoff bound) Let SN be the sum of N iid real random variables Xk , each\nwith the same probability distribution pX (x) and mean X = EX [X]. For τ > X, the probability\nthat SN ≥Nτ is upperbounded by\n-NEc(τ )\nPr{SN ≥Nτ} ≤e\n,\nwhere the Chernoff exponent Ec(τ) is given by\nEc(τ) = max sτ -μ(s),\ns≥0\nwhere μ(s) denotes the semi-invariant moment-generating function, μ(s) = log EX [esX ].\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nProof. The indicator function Φ(SN ≥ Nτ ) of the event {SN ≥ Nτ } is bounded by\ns(SN -Nτ )\nΦ(SN ≥ Nτ ) ≤ e\nfor any s ≥ 0. Therefore\nPr{SN ≥ Nτ } = Φ(SN ≥ Nτ ) ≤ es(SN -Nτ ),\ns\n\n≥ 0,\nwhere the overbar denotes expectation. Using the facts that SN =\nXk and that the Xk are\nk\nindependent, we have\nes(SN -Nτ )\n-N (sτ -μ(s))\n=\nes(Xk -τ ) = e\n,\nk\nwhere μ(s) = log esX . Optimizing the exponent over s ≥ 0, we obtain the Chernoff exponent\nEc(τ ) = max sτ - μ(s).\ns≥0\nWe next show that the Chernoff exponent is positive:\nTheorem 3.2 (Positivity of Chernoff exponent) The Chernoff exponent Ec(τ ) is positive\nwhen τ > X, provided that the random variable X is nondeterministic.\nProof. Define X(s) as a random variable with the same alphabet as X, but with the tilted\nsx-μ(s).\nprobability density function q(x, s) = p(x)e\nThis is a valid pdf because q(x, s) ≥ 0 and\nsx\nq(x, s) dx = e -μ(s)\ne\np(x) dx = e -μ(s)eμ(s) = 1.\nEvidently μ(0) = log EX [1] = 0, so q(x, 0) = p(x) and X(0) = X.\nDefine the moment-generating (partition) function\nsx\nZ(s) = eμ(s) = EX [e sX ] =\ne\np(x) dx.\nNow it is easy to see that\nsx\nsx\nZ′(s) =\nxe p(x) dx = eμ(s)\nxe q(x, s) dx = Z(s)X(s).\nSimilarly,\n2 sx\nZ′′(s) =\nx e p(x) dx = Z(s)X2(s).\nConsequently, from μ(s) = log Z(s), we have\nμ ′(s)\n=\nZ′(s) = X(s);\nZ(s)\nZ′(s) 2\nμ ′′(s)\n=\nZ′′(s) -\n= X2(s) - X(s) .\nZ(s)\nZ(s)\nThus the second derivative μ′′(s) is the variance of X(s), which must be strictly positive unless\nX(s) and thus X is deterministic.\n\n3.2. LAWS OF LARGE NUMBERS\nWe conclude that if X is a nondeterministic random variable with mean X, then μ(s) is a\nstrictly convex function of s that equals 0 at s = 0 and whose derivative at s = 0 is X. It follows\nthat the function sτ -μ(s) is a strictly concave function of s that equals 0 at s = 0 and whose\nderivative at s = 0 is τ -X. Thus if τ > X, then the function sτ -μ(s) has a unique maximum\nwhich is strictly positive.\nExercise 1. Show that if X is a deterministic random variable-- i.e., the probability that X\nequals its mean X is 1-- and τ > X, then Pr{SN ≥Nτ} = 0.\nThe proof of this theorem shows that the general form of the function f(s) = sτ -μ(s)\nwhen X is nondeterministic is as shown in Figure 1. The second derivative f′′(s) is negative\neverywhere, so the function f(s) is strictly concave and has a unique maximum Ec(τ). The\nslope f′(s) = τ -X(s) therefore decreases continually from its value f′(0) = τ -X > 0 at\ns = 0. The slope becomes equal to 0 at the value of s for which τ = X(s); in other words, to\nfind the maximum of f(s), keep increasing the \"tilt\" until the tilted mean X(s) is equal to τ.\nIf we denote this value of s by s ∗(τ), then we obtain the following parametric equations for the\nChernoff exponent:\n∗\nEc(τ) = s ∗(τ)τ -μ(s (τ));\nτ = X(s ∗(τ)).\n\nτ -X\ns\n-\ns ∗(τ)\nf(s)\nEc(τ)\nslope\nslope 0\nFigure 1. General form of function f(s) = sτ -μ(s) when τ > X.\nWe will show below that the Chernoff exponent Ec(τ) is the correct exponent, in the sense\nthat\nlim log Pr{SN ≥Nτ} = Ec(τ).\nN →inf\nN\nThe proof will be based on a fundamental theorem of large-deviation theory\nWe see that finding the Chernoff exponent is an exercise in convex optimization. In convex\noptimization theory, Ec(τ) and μ(s) are called conjugate functions. It is easy to show from the\nproperties of μ(s) that Ec(τ) is a continuous, strictly convex function of τ that equals 0 at τ = X\nand whose derivative at τ = X is 0.\n3.2.2\nChernoff bounds for functions of rvs\nIf g : X → R is any real-valued function defined on the alphabet X of a random variable X,\nthen g(X) is a real random variable. If {Xk } is a sequence of iid random variables Xk with the\nsame distribution as X, then {g(Xk )} is a sequence of iid random variables g(Xk ) with the same\ndistribution as g(X). The Chernoff bound thus applies to the sequence {g(Xk )}, and shows that\nthe probability that the sample mean N\nk g(Xk ) exceeds τ goes to zero exponentially with N\nas N →infwhenever τ > g(X).\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nLet us consider any finite set {gj } of such functions gj : X →R. Because the Chernoff bound\ndecreases exponentially with N, we can conclude that the probability that any of the sample\nmeans 1\nk gj (Xk) exceeds its corresponding expectation gj (X) by a given fixed ε > 0 goes to\nN\nzero exponentially with N as N →inf.\nWe may define a sequence {Xk} to be ε-typical with respect to a function gj : X → R if\nk gj (Xk) < gj (X)+ ε. We can thus conclude that the probability that {Xk} is not ε-typical\nN\nwith respect to any finite set {gj } of functions gj goes to zero exponentially with N as N →inf.\nA simple application of this result is that the probability that the sample mean N\nk gj (Xk)\nis not in the range gj (X) ± ε goes to zero exponentially with N as N →inf for any ε > 0,\nbecause this probability is the sum of the two probabilities Pr{\nk gj (Xk) ≥N(gj (X) + ε)} and\nPr{\n-gj (Xk ) ≥N(-gj (X) + ε)}.\nk\nMore generally, if the alphabet X is finite, then by considering the indicator functions of each\npossible value of X we can conclude that the probability that all observed relative frequencies\nin a sequence are not within ε of the corresponding probabilities goes to zero exponentially with\nN as N →inf. Similarly, for any alphabet X, we can conclude that the probability of any finite\nnumber of sample moments N\nm are not within ε of the corresponding expected moments\nk Xk\nXm goes to zero exponentially with N as N →inf.\nIn summary, the Chernoff bound law of large numbers allows us to say that as N →infwe will\nalmost surely observe a sample sequence x which is typical in every (finite) way that we might\nspecify.\n3.2.3\nAsymptotic equipartition principle\nOne consequence of any law of large numbers is the asymptotic equipartition principle (AEP):\nas N →inf, the observed sample sequence x of an iid sequence whose elements are chosen\naccording to a random variable X will almost surely be such that pX (x) ≈ 2-N H(X), where\nH(X) = EX [-log2 p(x)]. If X is discrete, then pX (x) is its probability mass function (pmf ) and\nH(X) is its entropy; if X is continuous, then pX (x) is its probability density function (pdf) and\nH(X) is its differential entropy.\nThe AEP is proved by observing that -log2 pX (x) is a sum of iid random variables\n-log2 pX (xk ), so the probability that -log2 pX (x) differs from its mean NH(X) by more than\nε > 0 goes to zero as N →inf. The Chernoff bound shows that this probability in fact goes to\nzero exponentially with N.\nA consequence of the AEP is that the set Tε of all sequences x that are ε-typical with respect\nto the function -log2 pX (x) has a total probability that approaches 1 as N →inf. Since for\nall sequences x ∈ Tε we have pX (x) ≈ 2-N H(X )-- i.e., the probability distribution pX (x) is\napproximately uniform over Tε-- this implies that the \"size\" |Tε| of Tε is approximately 2N H(X).\nIn the discrete case, the \"size\" |Tε| is the number of sequences in Tε, whereas in the continuous\ncase |Tε| is the volume of Tε.\nIn summary, the AEP implies that as N →inf the observed sample sequence x will almost\nsurely lie in an ε-typical set Tε of size ≈2N H(X), and within that set the probability distribution\npX (x) will be approximately uniform.\n\n3.2. LAWS OF LARGE NUMBERS\n3.2.4\nFundamental theorem of large-deviation theory\nAs another application of the law of large numbers, we prove a fundamental theorem of large-\ndeviation theory. A rough statement of this result is as follows: if an iid sequence X is chosen\naccording to a probability distribution q(x), then the probability that the sequence will be typical\nof a second probability distribution p(x) is approximately\n-ND(p||q)\nPr{x typical for p | q} ≈ e\n,\nwhere the exponent D(p||q) denotes the relative entropy (Kullback-Leibler divergence)\nD(p||q) = Ep log p(x)\n=\ndx p(x) log p(x) .\nq(x)\nq(x)\nX\nAgain, p(x) and q(x) denote pmfs in the discrete case and pdfs in the continuous case; we use\nnotation that is appropriate for the continuous case.\nExercise 2 (Gibbs' inequality).\n(a) Prove that for x > 0, log x ≤ x - 1, with equality if and only if x = 1.\n(b) Prove that for any pdfs p(x) and q(x) over X , D(p||q) ≥ 0, with equality if and only if\np(x) = q(x).\nGiven p(x) and q(x), we will now define a sequence x to be ε-typical with regard to log p(x)/q(x)\nif the log likelihood ratio λ(x) = log p(x)/q(x) is in the range N (D(p||q) ± ε), where D(p||q) =\nEp[λ(x)] is the mean of λ(x) = log p(x)/q(x) under p(x). Thus an iid sequence X chosen\naccording to p(x) will almost surely be ε-typical by this definition.\nThe desired result can then be stated as follows:\nTheorem 3.3 (Fundamental theorem of large-deviation theory) Given two probability\ndistributions p(x) and q(x) on a common alphabet X , for any ε > 0, the probability that an iid\nrandom sequence X drawn according to q(x) is ε-typical for p(x), in the sense that log p(x)/q(x)\nis in the range N (D(p||q) ± ε), is bounded by\n(1 - δ(N ))e -N(D(p||q)+ε) ≤ Pr{x ε-typical for p | q} ≤ e -N(D(p||q)-ε),\nwhere δ(N ) → 0 as N →inf.\nProof. Define the ε-typical region\nTε = {x | N (D(p||q) - ε) ≤ log p(x) ≤ N (D(p||q) + ε)}.\nq(x)\nBy any law of large numbers, the probability that X will fall in Tε goes to 1 as N →inf; i.e.,\n1 - δ(N ) ≤\ndx p(x) ≤ 1,\nTε\nwhere δ(N ) → 0 as N →inf. It follows that\n-N(D(p||q)-ε) ≤ e -N(D(p||q)-ε);\ndx q(x) ≤\ndx p(x)e\nTε\nTε\n-N(D(p||q)+ε)\ndx q(x) ≥\ndx p(x)e -N(D(p||q)+ε) ≥ (1 - δ(N ))e\n.\nTε\nTε\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nSince we can choose an arbitrarily small ε > 0 and δ(N ) > 0, it follows the exponent D(p||q)\nis the correct exponent for this probability, in the sense that\nlog Pr{x ε-typical for p | q} = D(p||q).\nlim\nN →inf\nN\nExercise 3 (Generalization of Theorem 3.3).\n(a) Generalize Theorem 3.3 to the case in which q(x) is a general function over X . State any\nnecessary restrictions on q(x).\n(b) Using q(x) = 1 in (a), state and prove a form of the Asymptotic Equipartition Principle.\nAs an application of Theorem 3.3, we can now prove:\nTheorem 3.4 (Correctness of Chernoff exponent) The Chernoff exponent Ec(τ ) is the\ncorrect exponent for Pr{SN ≥ Nτ }, in the sense that\nlim log Pr{SN ≥ Nτ } = Ec(τ ),\nN →inf\nN\nwhere SN =\nk xk is the sum of N iid nondeterministic random variables drawn according to\nsome distribution p(x) with mean X < τ , and Ec(τ ) = maxs≥0 sτ - μ(s) where μ(s) = log esX .\nProof. Let s ∗ be the s that maximizes sτ - μ(s) over s ≥ 0. As we have seen above, for s = s ∗\n∗\n∗\n∗\ns x-μ(s ) has mean\nthe tilted random variable X(s ∗) with tilted distribution q(x, s ) = p(x)e\nX(s ∗) = τ , whereas for s = 0 the untilted random variable X(0) with untilted distribution\nq(x, 0) = p(x) has mean X(0) = X.\nLet q(0) denote the untilted distribution q(x, 0) = p(x) with mean X(0) = X, and let q(s ∗)\n∗\n∗\ndenote the optimally tilted distribution q(x, s ∗) = p(x)es x-μ(s ) with mean X(s ∗) = τ . Then\n∗\nlog q(x, s ∗)/q(x, 0) = s x - μ(s ∗), so\n∗\nD(q(s ∗)||q(0)) = s τ - μ(s ∗) = Ec(τ ).\n∗\nMoreover, the event that X is ε-typical with respect to the variable log q(x, s ∗)/q(x, 0) = s x -\n∗\n∗\nμ(s ∗) under q(x, 0) = p(x) is the event that s SN - Nμ(s ∗) is in the range N (s τ - μ(s ∗) ± ε),\nsince τ is the mean of X under q(x, s ∗). This event is equivalent to SN being in the range\nN (τ ± ε/s∗). Since ε may be arbitrarily small, it is clear that the correct exponent of the event\nPr{SN ≈ Nτ } is Ec(τ ). This event evidently dominates the probability Pr{SN ≥ Nτ }, which\n-NEc(τ ).\nwe have already shown to be upperbounded by e\nExercise 4 (Chernoff bound ⇒ divergence upper bound.)\nUsing the Chernoff bound, prove that for any two distributions p(x) and q(x) over X ,\n-N (D(p||q))\nPr{log p(x) ≥ ND(p||q) | q} ≤ e\n.\nq(x)\n[Hint: show that the s that maximizes sτ - μ(s) is s = 1.]\n\n3.2. LAWS OF LARGE NUMBERS\n3.2.5\nProof of the forward part of the capacity theorem\nWe now prove that with Shannon's random Gaussian code ensemble and with a slightly dif\nferent definition of typical-set decoding, we can achieve reliable communication at any rate\nρ < C[b/2D] = log2(1 + SNR) b/2D.\nWe recall that under this scenario the joint pdf of the channel input X and output Y is\n-(y-x)2/2Sn\npXY (x, y) = pX(x)pN (y - x) = √ 1\ne -x2/2Sx √\ne\n.\n2πSx\n2πSn\nSince Y = X + N, the marginal probability of Y is\npY (y) =\ne -y2/2Sy ,\n2πSy\nwhere Sy = Sx + Sn. On the other hand, since incorrect codewords are independent of the\ncorrect codeword and of the output, the joint pdf of an incorrect codeword symbol X′ and of Y\nis\n′\n-y2 /2Sy .\nqXY (x , y) = pX(x ′)pY (y) = √ 2\nπSx\ne -(x′)2/2Sx\ne\n2πSy\nWe now redefine typical-set decoding as follows. An output sequence y will be said to be\nε-typical for a code sequence x if\npXY (x, y)\nλ(x, y) = log pX(x)pY (y) ≥ N(D(pXY ||pXpY ) - ε).\n2 log Sy/Sn, we find that this is\nequivalent to\nSubstituting for the pdfs and recalling that D(pXY ||pXpY ) = 1\n||y - x||2\n≤ ||y||2\n+ 2Nε.\nSn\nSy\nSince ||y||2/N is almost surely very close to its mean Sy, this amounts to asking that ||y -x||2/N\nbe very close to its mean Sn under the hypothesis that x and y are drawn according to the joint\npdf pXY (x, y). The correct codeword will therefore almost surely meet this test.\nAccording to Exercise 4, the probability that any particular incorrect codeword meets the test\npXY (x, y)\nλ(x, y) = log pX(x)pY (y) ≥ ND(pXY ||pXpY )\nis upperbounded by e-ND(pXY ||pXpY ) = 2-NI(X;Y ). If we relax this test by an arbitrarily small\nnumber ε > 0, then by the continuity of the Chernoff exponent, the exponent will decrease\nby an amount δ(ε) which can be made arbitrarily small. Therefore we can assert that the\nprobability that a random output sequence Y will be ε-typical for a random incorrect sequence\nX is upperbounded by\nPr{Y ε-typical for X} ≤ 2-N(I(X;Y )-δ(ε)),\nwhere δ(ε) → 0 as ε → 0.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\n= 2ρN/2\nNow if the random codes have rate ρ < 2I(X; Y ) b/2D, then there are M\ncode-\nwords, so by the union bound the total probability of any incorrect codeword being ε-typical is\nupperbounded by\nPr{Y ε-typical for any incorrect X} ≤(M -1)2-N (I(X;Y )-δ(ε)) < 2-N (I(X;Y )-ρ/2-δ(ε)).\nIf ρ < 2I(X; Y ) and ε is small enough, then the exponent will be positive and this probability\nwill go to zero as N →inf.\nThus we have proved the forward part of the capacity theorem: the probability of any kind\nof error with Shannon's random code ensemble and this variant of typical-set decoding goes to\nzero as N →inf, in fact exponentially with N.\n3.3\nGeometric interpretation and converse\nFor AWGN channels, the channel capacity theorem has a nice geometric interpretation in terms\nof the geometry of spheres in real Euclidean N-space RN .\nBy any law of large numbers, the probability that the squared Euclidean norm ||X||2 of a\nrandom sequence X of iid Gaussian variables of mean zero and variance Sx per symbol falls in\nthe range N(Sx ± ε) goes to 1 as N →inf, for any ε > 0. Geometrically, the typical region\nTε = {x ∈RN | N(Sx -ε) ≤||x||2 ≤N(Sx + ε)}\nis a spherical shell with outer squared radius N(Sx + ε) and inner squared radius N(Sx -ε).\nThus the random N-vector X will almost surely lie in the spherical shell Tε as N →inf. This\nphenomenon is known as \"sphere hardening.\"\nMoreover, the pdf pX (x) within the spherical shell Tε is approximately uniform, as we expect\nfrom the asymptotic equipartition principle (AEP). Since pX (x) = (2πSx)-N/2 exp -||x||2/2Sx,\nwithin Tε we have\n-(N/2)(ε/Sx) ≤pX (x) ≤(2πeSx)-N/2 e(N/2)(ε/Sx)\n(2πeSx)-N/2 e\n.\nMoreover, the fact that pX (x) ≈(2πeSx)-N/2 implies that the volume of Tε is approximately\n|Tε| ≈(2πeSx)N/2 . More precisely, we have\n1 -δ(N) ≤\npX (x) dx ≤1,\nTε\nwhere δ(N) →0 as N →inf. Since |Tε| =\ndx, we have\nTε\n-(N/2)(ε/Sx)\n1 ≥(2πeSx)-N/2 e\n|Tε|\n⇒\n|Tε| ≤(2πeSx)N/2 e(N/2)(ε/Sx);\ne(N/2)(ε/Sx)\n-(N/2)(ε/Sx)\n1 -δ(N) ≤(2πeSx)-N/2\n|Tε|\n⇒\n|Tε| ≥(1 -δ(N))(2πeSx)N/2 e\n.\nSince these bounds hold for any ε > 0, this implies that\nlog |Tε|\nlim\n= 2 log 2πeSx = H(X),\nN →inf\nN\nwhere H(X) = 1\n2 log 2πeSx denotes the differential entropy of a Gaussian random variable with\nmean zero and variance Sx.\n\n3.3. GEOMETRIC INTERPRETATION AND CONVERSE\nWe should note at this point that practically all of the volume of an N-sphere of squared radius\nN(Sx + ε) lies within the spherical shell |Tε| as N →inf, for any ε > 0. By dimensional analysis,\nthe volume of an N-sphere of radius r must be given by AN rN for some constant AN that does\nnot depend on r. Thus the ratio of the volume of an N-sphere of squared radius N(Sx -ε) to\nthat of an N-sphere of squared radius N(Sx + ε) must satisfy\nSx -ε N/2\nAN (N(Sx -ε))N/2\n=\n→0 as N →inf, for any ε > 0.\nAN (N(Sx + ε))N/2\nSx + ε\nIt follows that the volume of an N-sphere of squared radius NSx is also approximated by\neN H(X) = (2πeSx)N/2 as N →inf.\nExercise 5. In Exercise 4 of Chapter 1, the volume of an N-sphere of radius r was given as\nV⊗(N, r) = (πr2)N/2\n,\n(N/2)!\nfor N even. In other words, AN = πN/2/((N/2)!). Using Stirling's approximation, m! →(m/e)m\nas m →inf, show that this exact expression leads to the same asymptotic approximation for\nV⊗(N, r) as was obtained above by use of the asymptotic equipartition principle.\nThe sphere-hardening phenomenon may seem somewhat bizarre, but even more unexpected\nphenomena occur when we code for the AWGN channel using Shannon's random code ensemble.\nIn this case, each randomly chosen transmitted N-vector X will almost surely lie in a spherical\nshell TX of squared radius ≈NSx, and the random received N-vector Y will almost surely lie\nin a spherical shell TY of squared radius ≈NSy , where Sy = Sx + Sn.\nMoreover, given the correct transmitted codeword c0, the random received vector Y will\nalmost surely lie in a spherical shell Tε(c0) of squared radius ≈NSn centered on c0. A further\nconsequence of the AEP is that almost all of the volume of this nonzero-mean shell, whose\ncenter c0 has squared Euclidean norm ||c0||2 ≈ NSx, lies in the zero-mean shell TY whose\nsquared radius is ≈NSy, since the expected squared Euclidean norm of Y = c0 + N is\nEN [||Y||2] = ||c0||2 + NSn ≈NSy.\n\"Curiouser and curiouser,\" said Alice.\n= 2ρN/2\nWe thus obtain the following geometrical picture. We choose M\ncode vectors at\nrandom according to a zero-mean Gaussian distribution with variance Sx, which almost surely\nputs them within the shell TX of squared radius ≈ NSx. Considering the probable effects of\na random noise sequence N distributed according to a zero-mean Gaussian distribution with\nvariance Sn, we can define for each code vector ci a typical region Tε(ci) of volume |Tε(ci)| ≈\n(2πeSn)N/2, which falls almost entirely within the shell TY of volume |TY | ≈(2πeSy)N/2 .\nNow if a particular code vector c0 is sent, then the probability that the received vector y will\nfall in the typical region Tε(c0) is nearly 1. On the other hand, the probability that y will fall\nin the typical region Tε(ci) of some other independently-chosen code vector ci is approximately\nequal to the ratio |Tε(ci)|/|TY | of the volume of Tε(ci) to that of the entire shell, since if y\nis generated according to py (y) independently of ci, then it will be approximately uniformly\ndistributed over TY . Thus this probability is approximately\nSn\nN/2\n(2πeSn)N/2\n=\nPr{Y typical for ci} ≈ |T\n|\nε\nT\n(\nY\nci\n|\n)| ≈\n\n.\n(2πeSy )N/2\nSy\nAs we have seen in earlier sections, this argument may be made precise.\n\nCHAPTER 3. CAPACITY OF AWGN CHANNELS\nIt follows then that if ρ < log2(1 + Sx/Sn) b/2D, or equivalently M = 2ρN/2 < (Sy/Sn)N/2 ,\nthen the probability that Y is typical with respect to any of the M - 1 incorrect codewords is\nvery small, which proves the forward part of the channel capacity theorem.\nOn the other hand, it is clear from this geometric argument that if ρ > log2(1 + Sx/Sn) b/2D,\nor equivalently M = 2ρN/2 > (Sy/Sn)N/2, then the probability of decoding error must be large.\nFor the error probability to be small, the decision region for each code vector ci must include\nalmost all of its typical region Tε(ci). If the volume of the M = 2ρN/2 typical regions exceeds\nthe volume of TY , then this is impossible. Thus in order to have small error probability we must\nhave\nSx\n2ρN/2(2πeSn)N/2 ≤ (2πeSy)N/2\n⇒\nρ ≤ log2\nSy = log2(1 +\n) b/2D.\nSn\nSn\nThis argument may also be made precise, and is the converse to the channel capacity theorem.\nIn conclusion, we obtain the following picture of a capacity-achieving code. Let TY be the\nN-shell of squared radius ≈ NSy, which is almost the same thing as the N-sphere of squared\nradius NSy. A capacity-achieving code consists of the centers ci of M typical regions Tε(ci),\nwhere ||ci||2 ≈ NSx and each region Tε(ci) consists of an N-shell of squared radius ≈ NSn\ncentered on ci, which is almost the same thing as an N-sphere of squared radius NSx. As\nρ → C[b/2D] = log2(1 + Sx ) b/2D, these regions Tε(ci) form an almost disjoint partition of TY .\nSn\nThis picture is illustrated in Figure 2.\n'$\nnn\nn\nn\n&%\nFigure 2. Packing ≈ (Sy /Sn)N/2 typical regions Tε(ci) of squared radius ≈ NSn into a large\ntypical region TY of squared radius ≈ NSy .\n3.3.1\nDiscussion\nIt is natural in view of the above picture to frame the problem of coding for the AWGN channel\nas a sphere-packing problem. In other words, we might expect that a capacity-achieving code\nbasically induces a disjoint partition of an N-sphere of squared radius NSy into about (Sy/Sn)N/2\ndisjoint decision regions, such that each decision region includes the sphere of squared radius\nNSn about its center.\nHowever, it can be shown by geometric arguments that such a disjoint partition is impossible\nas the code rate approaches capacity. What then is wrong with the sphere-packing approach?\nThe subtle distinction that makes all the difference is that Shannon's probabilistic approach\ndoes not require decision regions to be disjoint, but merely probabilistically almost disjoint. So\nthe solution to Shannon's coding problem involves what might be called \"soft sphere-packing.\"\nWe will see that hard sphere-packing-- i.e., maximizing the minimum distance between code\nvectors subject to a constraint on average energy-- is a reasonable approach for moderate-size\ncodes at rates not too near to capacity. However, to obtain reliable transmission at rates near\ncapacity, we will need to consider probabilistic codes and decoding algorithms that follow more\nclosely the spirit of Shannon's original work."
    },
    {
      "category": "Resource",
      "title": "chap4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/b286123989945cef13e5a9aa20e56a18_chap4.pdf",
      "content": "Chapter 4\nThe gap between uncoded\nperformance and the Shannon limit\nThe channel capacity theorem gives a sharp upper limit C[b/2D] = log2(1 + SNR) b/2D on the\nrate (nominal spectral efficiency) ρ b/2D of any reliable transmission scheme. However, it does\nnot give constructive coding schemes that can approach this limit. Finding such schemes has\nbeen the main problem of coding theory and practice for the past half century, and will be our\nmain theme in this book.\nWe will distinguish sharply between the power-limited regime, where the nominal spectral\nefficiency ρ is small, and the bandwidth-limited regime, where ρ is large. In the power-limited\nregime, we will take 2-PAM as our baseline uncoded scheme, whereas in the bandwidth-limited\nregime, we will take M -PAM (or equivalently (M × M )-QAM) as our baseline.\nBy evaluating the performance of these simplest possible uncoded modulation schemes and\ncomparing baseline performance to the Shannon limit, we will establish how much \"coding\ngain\" is possible.\n4.1 Discrete-time AWGN channel model\nWe have seen that with orthonormal PAM or QAM, the channel model reduces to an analogous\nreal or complex discrete-time AWGN channel model\nY = X + N,\nwhere X is the random input signal point sequence, and N is an independent iid Gaussian noise\nsequence with mean zero and variance σ2 = N0/2 per real dimension. We have also seen that\nthere is no essential difference between the real and complex versions of this model, so from now\non we will consider only the real model.\nWe recapitulate the connections between the parameters of this model and the corresponding\ncontinuous-time parameters. If the symbol rate is 1/T real symbols/s (real dimensions per\nsecond), the bit rate per two dimensions is ρ b/2D, and the average signal energy per two\ndimensions is Es, then:\n\nCHAPTER 4. UNCODED PERFORMANCE VS. THE SHANNON LIMIT\n- The nominal bandwidth is W = 1/2T Hz;\n- The data rate is R = ρW b/s, and the nominal spectral efficiency is ρ (b/s)/Hz;\n- The signal power (average energy per second) is P = EsW ;\n- The signal-to-noise ratio is SNR = Es/N0 = P/N0W ;\n- The channel capacity in b/s is C[b/s] = WC[b/2D] = W log2(1 + SNR) b/s.\n4.2 Normalized SNR and Eb/N0\nIn this section we introduce two normalized measures of SNR that are suggested by the capacity\nbound ρ < C[b/2D] = log2(1 + SNR) b/2D, which we will now call the Shannon limit.\nAn equivalent statement of the Shannon limit is that for a coding scheme with rate ρ b/2D, if\nthe error probability is to be small, then the SNR must satisfy\nSNR > 2ρ - 1.\nThis motivates the definition of the normalized signal-to-noise ratio SNRnorm as\nSNR\nSNRnorm = 2ρ - 1 .\n(4.1)\nSNRnorm is commonly expressed in dB. Then the Shannon limit may be expressed as\nSNRnorm > 1 (0 dB).\nMoreover, the value of SNRnorm in dB measures how far a given coding scheme is operating from\nthe Shannon limit, in dB (the \"gap to capacity\").\nAnother commonly used normalized measure of signal-to-noise ratio is Eb/N0, where Eb is the\naverage signal energy per information bit and N0 is the noise variance per two dimensions. Note\nthat since Eb = Es/ρ, where Es is the average signal energy per two dimensions, we have\nEb/N0 = Es/ρN0 = SNR/ρ.\nThe quantity Eb/N0 is sometimes called the \"signal-to-noise ratio per information bit,\" but it\nis not really a signal-to-noise ratio, because its numerator and denominator do not have the same\nunits. It is probably best just to call it \"Eb/N0\" (pronounced \"eebee over enzero\" or \"ebno\").\nEb/N0 is commonly expressed in dB.\nSince SNR > 2ρ - 1, the Shannon limit on Eb/N0 may be expressed as\n2ρ - 1\nEb/N0 >\n.\n(4.2)\nρ\nNotice that the Shannon limit on Eb/N0 is a monotonic function of ρ. For ρ = 2, it is equal to\n3/2 (1.76 dB); for ρ = 1, it is equal to 1 (0 dB); and as ρ → 0, it approaches ln 2 ≈ 0.69 (-1.59\ndB), which is called the ultimate Shannon limit on Eb/N0.\n\n4.3. POWER-LIMITED AND BANDWIDTH-LIMITED CHANNELS\n4.3 Power-limited and bandwidth-limited channels\nIdeal band-limited AWGN channels may be classified as bandwidth-limited or power-limited\naccording to whether they permit transmission at high spectral efficiencies or not. There is no\nsharp dividing line, but we will take ρ = 2 b/2D or (b/s)/Hz as the boundary, corresponding to\nthe highest spectral efficiency that can be achieved with binary transmission.\nWe note that the behavior of the Shannon limit formulas is very different in the two regimes.\nIf SNR is small (the power-limited regime), then we have\nρ < log2(1 + SNR) ≈ SNR log2 e;\nSNR = (Eb/N0) log2 e.\nSNRnorm ≈ ρ ln 2\nIn words, in the power-limited regime, the capacity (achievable spectral efficiency) increases\nlinearly with SNR, and as ρ → 0, SNRnorm becomes equivalent to Eb/N0, up to a scale factor\nof log2 e = 1/ ln 2. Thus as ρ → 0 the Shannon limit SNRnorm > 1 translates to the ultimate\nShannon limit on Eb/N0, namely Eb/N0 > ln 2.\nOn the other hand, if SNR is large (the bandwidth-limited regime), then we have\nρ < log2(1 + SNR) ≈ log2 SNR;\nSNR\nSNRnorm ≈\n2ρ .\nThus in the bandwidth-limited regime, the capacity (achievable spectral efficiency) increases\nlogarithmically with SNR, which is dramatically different from the linear behavior in the power-\nlimited regime. In the power-limited regime, every doubling of SNR doubles the achievable rate,\nwhereas in the bandwidth-limited regime, every additional 3 dB in SNR yields an increase in\nachievable spectral efficiency of only 1 b/2D or 1 (b/s)/Hz.\nExample 1. A standard voice-grade telephone channel may be crudely modeled as an ideal\nband-limited AWGN channel with W ≈ 3500 Hz and SNR ≈ 37 dB. The Shannon limit on\nspectral efficiency and bit rate of such a channel are roughly ρ < 37/3 ≈ 12.3 (b/s)/Hz and R <\n43,000 b/s. Increasing the SNR by 3 dB would increase the achievable spectral efficiency ρ by\nonly 1 (b/s)/Hz, or the bit rate R by only 3500 b/s.\nExample 2. In contrast, there are no bandwidth restrictions on a deep-space communication\nchannel. Therefore it makes sense to use as much bandwidth as possible, and operate deep in\nthe power-limited region. In this case the bit rate is limited by the ultimate Shannon limit on\nEb/N0, namely Eb/N0 > ln 2 (-1.59 dB). Since Eb/N0 = P/RN0, the Shannon limit becomes\nR < (P/N0)/(ln 2). Increasing P/N0 by 3 dB will now double the achievable rate R in b/s.\nWe will find that the power-limited and bandwidth-limited regimes differ in almost every way.\nIn the power-limited regime, we will be able to use binary coding and modulation, whereas in\nthe bandwidth-limited regime we must use nonbinary (\"multilevel\") modulation. In the power-\nlimited regime, it is appropriate to normalize everything \"per information bit,\" and Eb/N0 is\na reasonable normalized measure of signal-to-noise ratio. In the bandwidth-limited regime, on\nthe other hand, we will see that it is much better to normalize everything \"per two dimensions,\"\nand SNRnorm will become a much more appropriate measure than Eb/N0. Thus the first thing\nto do in a communications design problem is to determine which regime you are in, and then\nproceed accordingly.\n\nCHAPTER 4. UNCODED PERFORMANCE VS. THE SHANNON LIMIT\n4.4 Performance of M -PAM and (M × M )-QAM\nWe now evaluate the performance of the simplest possible uncoded systems, namely M -PAM and\n(M × M )-QAM. This will give us a baseline. The difference between the performance achieved\nby baseline systems and the Shannon limit determines the maximum possible gain that can be\nachieved by the most sophisticated coding systems. In effect, it defines our playing field.\n4.4.1 Uncoded 2-PAM\nWe first consider the important special case of a binary 2-PAM constellation\nA = {-α, +α},\nwhere α > 0 is a scale factor chosen such that the average signal energy per bit, Eb = α2 ,\nsatisfies the average signal energy constraint.\nFor this constellation, the bit rate (nominal spectral efficiency) is ρ = 2 b/2D, and the average\nsignal energy per bit is Eb = α2 .\nThe usual symbol-by-symbol detector (which is easily seen to be optimum) makes an inde\npendent decision on each received symbol yk according to whether the sign of yk is positive or\nnegative. The probability of error per bit is evidently the same regardless of which of the two\nsignal values is transmitted, and is equal to the probability that a Gaussian noise variable of\nvariance σ2 = N0/2 exceeds α, namely\nPb(E) = Q (α/σ) ,\nwhere the Gaussian probability of error Q(·) function is defined by\ninf\nQ(x) =\n√ 1 e -y2 /2 dy.\nx\n2π\nSubstituting the energy per bit Eb = α2 and the noise variance σ2 = N0/2, the probability of\nerror per bit is\nPb(E) = Q\n2Eb/N0 .\n(4.3)\nThis gives the performance curve of Pb(E) vs. Eb/N0 for uncoded 2-PAM that is shown in Figure\n1, below.\n4.4.2 Power-limited baseline vs. the Shannon limit\nIn the power-limited regime, we will take binary pulse amplitude modulation (2-PAM) as our\nbaseline uncoded system, since it has ρ = 2. By comparing the performance of the uncoded\nbaseline system to the Shannon limit, we will be able to determine the maximum possible gains\nthat can be achieved by the most sophisticated coding systems.\nIn the power-limited regime, we will primarily use Eb/N0 as our normalized signal-to-noise\nratio, although we could equally well use SNRnorm. Note that when ρ = 2, since Eb/N0 = SNR/ρ\nand SNRnorm = SNR/(2ρ - 1), we have 2Eb/N0 = 3SNRnorm. The baseline performance curve\ncan therefore be written in two equivalent ways:\nPb(E) = Q\n2Eb/N0 = Q\n3SNRnorm .\n\n4.4. PERFORMANCE OF M -PAM AND (M × M )-QAM\nUncoded 2-PAM\nUncoded 2-PAM\nShannon Limit\nShannon Limit for ρ = 2\n-1\n-2\nPb(E)\n-3\n-4\n-5\n-6\n-2\n-1\nEb/No [dB]\nFigure 1. Pb(E) vs. Eb/N0 for uncoded binary PAM.\nFigure 1 gives us a universal design tool. For example, if we want to achieve Pb(E ) ≈ 10-5\nwith uncoded 2-PAM, then we know that we will need to achieve Eb/N0 ≈ 9.6 dB.\nWe may also compare the performance shown in Figure 1 to the Shannon limit. The rate of\n2-PAM is ρ = 2 b/2D. The Shannon limit on Eb/N0 at ρ = 2 b/2D is Eb/N0 > (2ρ - 1)/ρ = 3/2\n(1.76 dB). Thus if our target error rate is Pb(E) ≈ 10-5, then we can achieve a coding gain of\nup to about 8 dB with powerful codes, at the same rate of ρ = 2 b/2D.\nHowever, if there is no limit on bandwidth and therefore no lower limit on spectral efficiency,\nthen it makes sense to let ρ → 0. In this case the ultimate Shannon limit on Eb/N0 is Eb/N0 >\nln 2 (-1.59 dB). Thus if our target error rate is Pb(E) ≈ 10-5, then Shannon says that we\ncan achieve a coding gain of over 11 dB with powerful codes, by letting the spectral efficiency\napproach zero.\n4.4.3 Uncoded M -PAM and (M × M )-QAM\nWe next consider the more general case of an M -PAM constellation\nA = α{±1, ±3, . . . , ±(M - 1)},\nwhere α > 0 is again a scale factor chosen to satisfy the average signal energy constraint. The\nbit rate (nominal spectral efficiency) is then ρ = 2 log2 M b/2D.\nThe average energy per M -PAM symbol is\nα2(M 2 - 1)\nE(A) =\n.\n\nCHAPTER 4. UNCODED PERFORMANCE VS. THE SHANNON LIMIT\nAn elegant way of making this calculation is to consider a random variable Z = X + U , where\nX is equiprobable over A and U is an independent continuous uniform random variable over the\ninterval (-α, α]. Then Z is a continuous random variable over the interval (-Mα, M α], and1\n(αM )2\nα2\nX2 = Z2 - U 2 =\n-\n.\nThe average energy per two dimensions is then Es = 2E(A) = 2α2(M 2 - 1)/3.\nAgain, an optimal symbol-by-symbol detector makes an independent decision on each received\nsymbol yk . In this case the decision region associated with an input value αzk (where zk is\nan odd integer) is the interval α[zk - 1, zk + 1] (up to tie-breaking at the boundaries, which is\nimmaterial), except for the two outermost signal values ±α(M - 1), which have decision regions\n±α[M - 2, inf). The probability of error for any of the M - 2 inner signals is thus equal to twice\nthe probability that a Gaussian noise variable Nk of variance σ2 = N0/2 exceeds α, namely\n2Q (α/σ) , whereas for the two outer signals it is just Q (α/σ). The average probability of error\nwith equiprobable signals per M -PAM symbol is thus\nM - 2\nPr(E) =\n2Q (α/σ) +\nQ (α/σ) = 2(M - 1)Q (α/σ) .\nM\nM\nM\nFor M = 2, this reduces to the usual expression for 2-PAM. For M ≥ 4, the \"error coefficient\"\n2(M - 1)/M quickly approaches 2, so Pr(E) ≈ 2Q (α/σ).\nSince an (M × M )-QAM signal set A′ = A2 is equivalent to two independent M -PAM trans\nmissions, we can easily extend this calculation to (M × M )-QAM. The bit rate (nominal spectral\nefficiency) is again ρ = 2 log2 M b/2D, and the average signal energy per two dimensions (per\nQAM symbol) is again Es = 2α2(M 2 - 1)/3. The same dimension-by-dimension decision method\nand calculation of probability of error per dimension Pr(E) hold. The probability of error per\n(M × M )-QAM symbol, or per two dimensions, is given by\nPs(E) = 1 - (1 - Pr(E))2 = 2 Pr(E) - (Pr(E))2 ≈ 2 Pr(E).\nTherefore for (M × M )-QAM we obtain a probability of error per two dimensions of\nPs(E) ≈ 2 Pr(E) ≈ 4Q (α/σ) .\n(4.4)\n4.4.4 Bandwidth-limited baseline vs. the Shannon limit\nIn the bandwidth-limited regime, we will take (M × M )-QAM with M ≥ 4 as our baseline\nuncoded system, we will normalize everything per two dimensions, and we will use SNRnorm as\nour normalized signal-to-noise ratio.\nFor M ≥ 4, the probability of error per two dimensions is given by (4.4):\nPs(E) ≈ 4Q (α/σ) .\n1This calculation is actually somewhat fundamental, since it is based on a perfect one-dimensional sphere-\npacking and on the fact that the difference between the average energy of a continuous random variable and the\naverage energy of an optimally quantized discrete version thereof is the average energy of the quantization error.\nAs the same principle is used in the calculation of channel capacity, in the relation Sy = Sx + Sn, we can even say\nthat the \"1\" that appears in the capacity formula is the same \"1\" as appears in the formula for E(A). Therefore\nthe cancellation of this term below is not quite as miraculous as it may at first seem.\n\n4.4. PERFORMANCE OF M -PAM AND (M × M )-QAM\nSubstituting the average energy Es = 2α2(M 2 - 1)/3 per two dimensions, the noise variance\nσ2 = N0/2, and the normalized signal-to-noise ratio\nSNR\nSNRnorm = 2ρ - 1 = Es/N0\nM 2 - 1,\nwe find that the factors of M 2 - 1 cancel (cf. footnote 1) and we obtain the performance curve\nPs(E) ≈ 4Q\n3SNRnorm .\n(4.5)\nNote that this curve does not depend on M , which shows that SNRnorm is correctly normalized\nfor the bandwidth-limited regime.\nThe bandwidth-limited baseline performance curve (4.5) of Ps(E) vs. SNRnorm for uncoded\n(M × M )-QAM is plotted in Figure 2.\nUncoded QAM\n-3\nUncoded QAM\nShannon Limit\n-1\n-2\nP (E)\ns\n-4\n-5\n-6\nSNR norm [dB]\nFigure 2. Ps(E) vs. SNRnorm for uncoded (M × M )-QAM.\nFigure 2 gives us another universal design tool. For example, if we want to achieve Ps(E) ≈\n10-5 with uncoded (M × M )-QAM (or M -PAM), then we know that we will need to achieve\nSNRnorm ≈ 8.4 dB. (Notice that SNRnorm, unlike Eb/N0, is already normalized for spectral\nefficiency.)\nThe Shannon limit on SNRnorm for any spectral efficiency is SNRnorm > 1 (0 dB). Thus if our\ntarget error rate is Ps(E) ≈ 10-5, then Shannon says that we can achieve a coding gain of up to\nabout 8.4 dB with powerful codes, at any spectral efficiency. (This result holds approximately\neven for M = 2, as we have already seen in the previous subsection.)"
    },
    {
      "category": "Resource",
      "title": "chap_5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/9c669af76ebf74ffe46645541f1ff6b3_chap_5.pdf",
      "content": "Chapter 5\nPerformance of small signal sets\nIn this chapter, we show how to estimate the performance of small-to-moderate-sized signal\nconstellations on the discrete-time AWGN channel.\nWith equiprobable signal points in iid Gaussian noise, the optimum decision rule is a minimum-\ndistance rule, so the optimum decision regions are minimum-distance (Voronoi) regions.\nWe develop useful performance estimates for the error probability based on the union bound.\nThese are based on exact expressions for pairwise error probabilities, which involve the Gaussian\nprobability of error Q(·) function. An appendix develops the main properties of this function.\nFinally, we use the union bound estimate to find the \"coding gain\" of small-to-moderate-\nsized signal constellations in the power-limited and bandwidth-limited regimes, compared to the\n2-PAM or (M × M)-QAM baselines, respectively.\n5.1 Signal constellations for the AWGN channel\nIn general, a coding scheme for the discrete-time AWGN channel model Y = X + N is a\nmethod of mapping an input bit sequence into a transmitted real symbol sequence x, which is\ncalled encoding, and a method for mapping a received real symbol sequence y into an estimated\ntransmitted signal sequence ˆx, which is called decoding.\nInitially we will consider coding schemes of the type considered by Shannon, namely block\ncodes with a fixed block length N. With such codes, the transmitted sequence x consists of\na sequence (. . . , xk , xk+1, . . .) of N-tuples xk ∈ RN that are chosen independently from some\nblock code of length N with M codewords. Block codes are not the only possible kinds of coding\nschemes, as we will see when we study convolutional and trellis codes.\nUsually the number M of codewords is chosen to be a power of 2, and codewords are chosen by\nsome encoding map from blocks of log2 M bits in the input bit sequence. If the input bit sequence\nis assumed to be an iid random sequence of equiprobable bits, then the transmitted sequence\nwill be an iid random sequence X = (. . . , Xk , Xk+1, . . .) of equiprobable random codewords Xk .\nWe almost always assume equiprobability, because this is a worst-case (minimax) assumption.\nAlso, the bit sequence produced by an efficient source coder must statistically resemble an iid\nequiprobable bit sequence.\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\nIn digital communications, we usually focus entirely on the code, and do not care what encoding\nmap is used from bits to codewords. In other contexts the encoding map is also important; e.g.,\nin the \"Morse code\" of telegraphy.\nIf the block length N and the number of codewords M are relatively small, then a block code\nfor the AWGN channel may alternatively be called a signal set, signal constellation, or signal\nalphabet. A scheme in which the block length N is 1 or 2, corresponding to a single signaling\ninterval of PAM or QAM, may be regarded as an \"uncoded\" scheme.\nFigure 1 illustrates some 1-dimensional and 2-dimensional signal constellations.\n√\nr 3\nr\nr\nr\nr\nr\nr\n√\nr 1\nr\nr\nr 1\nr\nr\nr\nr\nr\nr\nr\nr\n-1\n-3 -1\n-2.r5-0.r5 1.r5 3.r5\nr -1\nr\nr\nr -1\nr\nr\nr\nr\nr\nr\nr\nr -\n√\nr -3\nr\nr\nr\nr\nr -2\n√\nr\n(a)\n(b)\n(c)\n(d)\n\n(e)\nFigure 1. Uncoded signal constellations: (a) 2-PAM; (b) (2 × 2)-QAM; (c) 4-PAM;\n(d) (4 × 4)-QAM; (e) hexagonal 16-QAM.\nAn N -dimensional signal constellation (set, alphabet) will be denoted by\nA = {aj , 1 ≤ j ≤ M }.\nIts M elements aj ∈ RN will be called signal points (vectors, N -tuples).\nThe basic parameters of a signal constellation A = {aj , 1 ≤ j ≤ M } are its dimension N ; its\nsize M (number of signal points); its average energy E(A) = M\nj ||aj ||2; and its minimum\nsquared distance d2\nmin(A), which is an elementary measure of its noise resistance. A secondary\nparameter is the average number Kmin(A) of nearest neighbors (points at distance dmin(A)).\nFrom these basic parameters we can derive such parameters as:\n- The bit rate (nominal spectral efficiency) ρ = (2/N ) log2 M b/2D;\n- The average energy per two dimensions Es = (2/N )E(A),\nor the average energy per bit Eb = E(A)/(log2 M ) = Es/ρ;\n- Energy-normalized figures of merit such as d2\nor d2\nmin(A)/Es\nmin(A)/Eb,\nmin(A)/E(A), d2\nwhich are independent of scale.\nFor example, in Figure 1, the bit rate (nominal spectral efficiency) of the 2-PAM and (2 × 2)-\nQAM constellations is ρ = 2 b/2D, whereas for the other three constellations it is ρ = 4 b/2D.\nThe average energy per two dimensions of the 2-PAM and (2 × 2)-QAM constellations is Es = 2,\nwhereas for the 4-PAM and (4 × 4)-QAM constellations it is Es = 10, and for the hexagonal\n16-QAM constellation it is Es = 8.75. For all constellations, d2\nmin = 4. The average numbers of\nnearest neighbors are Kmin = 1, 2, 1.5, 3, and 4.125, respectively.\n\n′\n′\n′\n5.1. SIGNAL CONSTELLATIONS FOR THE AWGN CHANNEL\n5.1.1 Cartesian-product constellations\nSome of these relations are explained by the fact that an (M × M)-QAM constellation is the\nCartesian product of two M-PAM constellations. In general, a Cartesian-product constellation\nAK is the set of all sequences of K points from an elementary constellation A; i.e.,\nAK = {(x1, x2, . . . , xK ) | xk ∈A}.\nIf the dimension and size of A are N and M, respectively, then the dimension of A = AK is\nN′ = KN and its size is M′ = MK .\nExercise 1 (Cartesian-product constellations). (a) Show that if A = AK , then the parameters\nN, log2 M, E(A′) and Kmin(A′) of A are K times as large as the corresponding parameters of\nA, whereas the normalized parameters ρ, Es, Eb and d2\nmin(A) are the same as those of A. Verify\nthat these relations hold for the (M × M)-QAM constellations of Figure 1.\nNotice that there is no difference between a random input sequence X with elements from A\nand a sequence X with elements from a Cartesian-product constellation AK . For example, there\nis no difference between a random M-PAM sequence and a random (M × M)-QAM sequence.\nThus Cartesian-product constellations capture in a non-statistical way the idea of independent\ntransmissions. We thus may regard a Cartesian-product constellation AK as equivalent to (or a\n\"version\" of) the elementary constellation A. In particular, it has the same ρ, Es, Eb and d2\nmin.\nWe may further define a \"code over A\" as a subset C ⊂AK of a Cartesian-product constellation\nAK . In general, a code C over A will have a lower bit rate (nominal spectral efficiency) ρ than\nA, but a higher minimum squared distance d2\nmin. Via this tradeoff, we hope to achieve a \"coding\ngain.\" Practically all of the codes that we will consider in later chapters will be of this type.\n5.1.2 Minimum-distance decoding\nAgain, a decoding scheme is a method for mapping the received sequence into an estimate of the\ntransmitted signal sequence. (Sometimes the decoder does more than this, but this definition\nwill do for a start.)\nIf the encoding scheme is a block scheme, then it is plausible that the receiver should decode\nblock-by-block as well. That there is no loss of optimality in block-by-block decoding can be\nshown from the theorem of irrelevance, or alternatively by an extension of the exercise involving\nCartesian-product constellations at the end of this subsection.\nWe will now recapitulate how for block-by-block decoding, with equiprobable signals and iid\nGaussian noise, the optimum decision rule is a minimum-distance (MD) rule.\nFor block-by-block decoding, the channel model is Y = X + N, where all sequences are N-\ntuples. The transmitted sequence X is chosen equiprobably from the M N-tuples aj in a signal\nconstellation A. The noise pdf is\npN (n) = (2πσ\ne-||n||2/2σ2\n2)N/2\n,\nwhere the symbol variance is σ2 = N0/2.\nIn digital communications, we are usually interested in the minimum-probability-of-error\n(MPE) decision rule: given a received vector y, choose the signal point ˆa ∈A to minimize\nthe probability of decision error Pr(E).\n\n′\n′\n′\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\na is correct is simply the a posteriori probability p(ˆ |\nSince the probability that a decision ˆ\na\ny),\nthe MPE rule is equivalent to the maximum-a-posteriori-probability (MAP) rule: choose the\na ∈A such that p(ˆ |\nˆ\na\ny) is maximum among all p(aj | y), aj ∈A.\nBy Bayes' law,\np(aj | y) = p(y\naj )p(aj )\n|\np(y)\n.\nIf the signals aj are equiprobable, so p(aj ) = 1/M for all j, then the MAP rule is equivalent to\na ∈A such that p(y\na) is maximum among all\nthe maximum-likelihood (ML) rule: choose the ˆ\n| ˆ\np(y | aj ), aj ∈A.\nUsing the noise pdf, we can write\n|\ne-||y-aj ||2/2σ2\np(y\naj ) = pN (y -aj ) = (2πσ2)N/2\n.\nTherefore the ML rule is equivalent to the minimum-distance (MD) rule: choose the ˆa ∈A such\nthat ||y -ˆa||2 is minimum among all ||y -aj ||2 , aj ∈A.\nIn summary, under the assumption of equiprobable inputs and iid Gaussian noise, the MPE\nrule is the minimum-distance rule. Therefore from this point forward we consider only MD\ndetection, which is easy to understand from a geometrical point of view.\nExercise 1 (Cartesian-product constellations, cont.).\n(b) Show that if the signal constellation is a Cartesian product AK , then MD detection can\nbe performed by performing independent MD detection on each of the K components of the\nreceived KN-tuple y = (y1, y2, . . . , yK ). Using this result, sketch the decision regions of the\n(4 × 4)-QAM signal set of Figure 1(d).\n(c) Show that if Pr(E) is the probability of error for MD detection of A, then the probability\nof error for MD detection of A′ is\nPr(E)′ = 1 -(1 -Pr(E))K ,\nShow that Pr(E)′ ≈K Pr(E) if Pr(E) is small.\nExample 1. The K-fold Cartesian product A = AK of a 2-PAM signal set A = {±α}\ncorresponds to independent transmission of K bits using 2-PAM. Geometrically, A′ is the vertex\nset of a K-cube of side 2α. For example, for K = 2, A is the (2 × 2)-QAM constellation of\nFigure 1(b).\nFrom Exercise 1(a), the K-cube constellation A = AK has dimension N′ = K, size M′ = 2K ,\nbit rate (nominal spectral efficiency) ρ = 2 b/2D, average energy E(A′) = Kα2, average energy\nper bit Eb = α2, minimum squared distance d2\n′) = 4α2, and average number of nearest\nmin(A\nneighbors K′\n′) = K. From Exercise 1(c), its probability of error is approximately K times\nmin(A\nthe single-bit error probability:\nPr(E)′ ≈KQ\nthe curve of (4.3) for all K-cube constellations:\nPb(E) ≈Q\n\n2Eb/N0 .\nPb(E) = Pr(E)′/K\n2Eb/N0\n\n,\nConsequently, if we define the probability of error per bit as\n, then we obtain\nincluding the (2 × 2)-QAM constellation of Figure 1(b).\nA code over the 2-PAM signal set A is thus simply a subset of the vertices of a K-cube.\n\n5.1. SIGNAL CONSTELLATIONS FOR THE AWGN CHANNEL\n5.1.3 Decision regions\nUnder a minimum-distance (MD) decision rule, real N -space RN is partitioned into M decision\nregions Rj, 1 ≤ j ≤ M , where Rj consists of the received vectors y ∈ RN that are at least as\nclose to aj as to any other point in A:\nRj = {y ∈ RN : ||y - aj||2 ≤ ||y - aj′ ||2 for all j′ =\nj}.\n(5.1)\nThe minimum-distance regions Rj are also called Voronoi regions. Under the MD rule, given\na received sequence y, the decision is aj only if y ∈Rj. The decision regions Rj cover all of\nN -space RN , and are disjoint except on their boundaries.\nSince the noise vector N is a continuous random vector, the probability that y will actually\nfall precisely on the boundary of Rj is zero, so in that case it does not matter which decision is\nmade.\nThe decision region Rj is the intersection of the M - 1 pairwise decision regions Rjj′ defined\nby\nRjj′ = {y ∈ RN : ||y - aj||2 ≤ ||y - aj′ || }.\nGeometrically, it is obvious that Rjj′ is the half-space containing aj that is bounded by the\nperpendicular bisector hyperplane Hjj′ between aj and aj′ , as shown in Figure 2.\naj′\naj′|aj′ -aj r\nm\ny\ny|aj′ -aj = m|aj′ -aj\nr\n'$Hjj′\n\naj|aj′ -aj r\naj\n\n&%\n\n-\nFigure 2. The boundary hyperplane Hjj′ is the perpendicular bisector between aj and aj′ .\nAlgebraically, since Hjj′ is the set of points in RN that are equidistant from aj and aj′ , it is\ncharacterized by the following equivalent equations:\n||y - aj||\n= ||y - aj′ ||2;\n-2⟨y, aj⟩ + ||aj||\n= -2⟨y, aj′ ⟩ + ||aj′ ||2;\naj + aj′\n⟨y, aj′ - aj⟩ = ⟨\n, aj′ - aj⟩ = ⟨m, aj′ - aj⟩.\n(5.2)\nwhere m denotes the midvector m = (aj + aj′ )/2. If the difference vector between aj′ and aj is\naj′ - aj and\naj′ - aj\nφj→j′ = ||aj′ - aj||\nis the normalized difference vector, so that ||φj→j′ ||2 = 1, then the projection of any vector x\nonto the difference vector aj′ - aj is\n⟨x, aj′ - aj⟩\nx|aj′ -aj = ⟨x, φj→j′ ⟩φj→j′ = ||aj′ - aj||2 (aj′ - aj).\n\n′\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\nThe geometric meaning of Equation (5.2) is thus that y ∈Hjj′ if and only if the projection\ny|aj′ -aj of y onto the difference vector aj′ -aj is equal to the projection m|aj′ -aj of the midvector\nm = (aj + aj′ )/2 onto the difference vector aj -aj′ , as illustrated in Figure 2.\nThe decision region Rj is the intersection of these M -1 half-spaces:\nRj =\nRjj′ .\nj′=j\n(Equivalently, the complementary region Rj is the union of the complementary half-spaces\nRjj′ .) A decision region Rj is therefore a convex polytope bounded by portions of a subset\n{Hjj′ , aj′ ∈N(aj)} of the boundary hyperplanes Hjj′ , where the subset N(aj) ⊆A of neighbors\nof aj that contribute boundary faces to this polytope is called the relevant subset. It is easy to\nsee that the relevant subset must always include the nearest neighbors to aj.\n5.2 Probability of decision error\nThe probability of decision error given that aj is transmitted is the probability that Y = aj + N\nfalls outside the decision region Rj, whose \"center\" is aj. Equivalently, it is the probability that\nthe noise variable N falls outside the translated region Rj -aj, whose \"center\" is 0:\nPr(E | aj) = 1 -\npY (y | aj) dy = 1 -\npN (y -aj) dy = 1 -\npN (n) dn.\nRj\nRj\nRj -aj\nExercise 2 (error probability invariance). (a) Show that the probabilities of error Pr(E | aj)\nare unchanged if A is translated by any vector v; i.e., the constellation A = A + v has the\nsame error probability Pr(E) as A.\n(b) Show that Pr(E) is invariant under orthogonal transformations; i.e., the constellation\nA′ = UA has the same Pr(E) as A when U is any orthogonal N × N matrix (i.e., U-1 = UT ).\n(c) Show that Pr(E) is unchanged if both the constellation A and the noise N are scaled by\nthe same scale factor α > 0.\nExercise 3 (optimality of zero-mean constellations). Consider an arbitrary signal set A =\n{aj, 1 ≤ j ≤ M}. Assume that all signals are equiprobable. Let m(A) =\nj aj be the\nM\naverage signal, and let A′ be A translated by m(A) so that the mean of A′ is zero:\nA′ = A -m(A) = {aj -m(A), 1 ≤j ≤M}.\nLet E(A) and E(A′) denote the average energies of A and A′, respectively.\n(a) Show that the error probability of an optimum detector is the same for A′ as it is for A.\n(b) Show that E(A′) = E(A) -||m(A)||2. Conclude that removing the mean m(A) is always\na good idea.\n(c) Show that a binary antipodal signal set A = {±a} is always optimal for M = 2.\nIn general, there is no closed-form expression for the Gaussian integral Pr(E | aj). However,\nwe can obtain an upper bound in terms of pairwise error probabilities, called the union bound,\nwhich is usually quite sharp. The first term of the union bound, called the union bound estimate,\nis usually an excellent approximation, and will be the basis for our analysis of coding gains of\nsmall-to-moderate-sized constellations. A lower bound with the same exponential behavior may\nbe obtained by considering only the worst-case pairwise error probability.\n\n5.2. PROBABILITY OF DECISION ERROR\n5.2.1 Pairwise error probabilities\nWe now show that each pairwise probability has a simple closed-form expression that depends\nonly on the squared distance d2(aj , aj′ ) = ||aj - aj′ ||2 and the noise variance σ2 = N0/2.\nFrom Figure 2, it is clear that whether y = aj + n is closer to aj′ than to aj depends only on\nthe projection y|aj′ -aj of y onto the difference vector aj′ - aj . In fact, from (5.2), an error can\noccur if and only if\n′ - aj ⟩|\n⟨aj′ - aj , aj′ - aj ⟩ = ||aj′ - aj || .\n|n|aj′ -aj | = |⟨n, φj→j′ ⟩| = |⟨n, aj\n≥\n||aj′ - aj ||\n2||aj′ - aj ||\nIn other words, an error can occur if and only if the magnitude of the one-dimensional noise\ncomponent n1 = n|aj′ -aj , the projection of n onto the difference vector aj′ - aj , exceeds half\nthe distance d(aj′ , aj ) = ||aj′ - aj || between aj′ and aj .\nWe now use the fact that the distribution pN (n) of the iid Gaussian noise vector N is spherically\nsymmetric, so the pdf of any one-dimensional projection such as n1 is\npN (n1) = √ 1\ne -n1/2σ2 .\n2πσ2\nIn other words, N is an iid zero-mean Gaussian vector with variance σ2 in any coordinate system,\nincluding a coordinate system in which the first coordinate axis is aligned with the vector aj′ -aj .\nConsequently, the pairwise error probability Pr{aj → aj′ } that if aj is transmitted, the received\nvector y = aj + n will be at least as close to aj′ as to aj is given simply by\ninf\nd(aj′ , aj )\nPr{aj → aj′ } = √ 1\ne -x2/2σ2 dx = Q\n\n2σ\n,\n(5.3)\n2πσ2\nd(aj′ ,aj )/2\nwhere Q(·) is again the Gaussian probability of error function.\nAs we have seen, the probability of error for a 2-PAM signal set {±α} is Q(α/σ). Since the\ndistance between the two signals is d = 2α, this is just a special case of this general formula.\nIn summary, the spherical symmetry of iid Gaussian noise leads to the remarkable result that\nthe pairwise error probability from aj to aj′ depends only on the squared distance d2(aj′ , aj ) =\n||aj′ - aj ||2 between aj and aj′ and the noise variance σ2 .\nExercise 4 (non-equiprobable signals).\nLet aj and aj′ be two signals that are not equiprobable. Find the optimum (MPE) pairwise\ndecision rule and pairwise error probability Pr{aj → aj′ }.\n5.2.2 The union bound and the UBE\nThe union bound on error probability is based on the elementary union bound of probability\ntheory: if A and B are any two events, then Pr(A ∪ B) ≤ Pr(A)+Pr(B). Thus the probability of\ndetection error Pr(E | aj ) with minimum-distance detection if aj is sent-- i.e., the probability\nthat y will be closer to some other aj′ ∈A than to aj - is upperbounded by the sum of the\npairwise error probabilities to all other signals aj′ = aj ∈A:\nPr(E | aj ) ≤\n\nPr{aj → aj′ } =\n\nQ\nd(aj , aj′ ) .\n2σ\naj =\naj′ ∈A\naj =\naj′ ∈A\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\nLet D denote the set of distances between signal points in A; then we can write the union\nbound as\n\nd\nPr(E | aj ) ≤\nKd(aj )Q 2σ\n,\n(5.4)\nd∈D\nwhere Kd(aj ) is the number of signals aj′ = aj ∈A at distance d from aj . Because Q(x)\ndecreases exponentially as e-x2/2 (see Appendix), the factor Q(d/2σ) will be largest for the\nminimum Euclidean distance\nj′ ∈A ||aj′ - aj ||,\ndmin(A) =\nmin\naj =a\nand will decrease rapidly for larger distances.\nThe union bound estimate (UBE) of Pr(E | aj ) is based on the idea that the nearest neighbors\nto aj at distance dmin(A) (if there are any) will dominate this sum. If there are Kmin(aj )\nneighbors at distance dmin(A) from aj , then\ndmin(A)\nPr(E | aj ) ≈ Kmin(aj ) Q\n2σ\n.\n(5.5)\nOf course this estimate is valid only if the next nearest neighbors are at a significantly greater\ndistance and there are not too many of them; if these assumptions are violated, then further\nterms should be used in the estimate.\nThe union bound may be somewhat sharpened by considering only signals in the relevant\nsubset N (aj ) that determine faces of the decision region Rj . However, since N (aj ) includes all\nnearest neighbors at distance dmin(A), this will not affect the UBE.\nFinally, if there is at least one neighbor aj′ at distance dmin(A) from aj , then we have the\npairwise lower bound\nPr(E | aj ) ≥ Pr{aj → aj′ } = Q\ndmin(A)\n,\n(5.6)\n2σ\nsince there must be a detection error if y is closer to aj′ than to aj . Thus we are usually able\nto obtain upper and lower bounds on Pr(E | aj ) that have the same \"exponent\" (argument of\nthe Q(·) function) and that differ only by a small factor of the order of Kmin(aj ).\nWe can obtain similar upper and lower bounds and estimates for the total error probability\nPr(E) = Pr(E | aj ),\nwhere the overbar denotes the expectation over the equiprobable ensemble of signals in A. For\nexample, if Kmin(A) = Kmin(aj ) is the average number of nearest neighbors at distance dmin(A),\nthen the union bound estimate of Pr(E) is\ndmin(A)\nPr(E) ≈ Kmin(A)Q\n.\n(5.7)\n2σ\nExercise 5 (UBE for M -PAM constellations). For an M -PAM constellation A, show that\nKmin(A) = 2(M - 1)/M . Conclude that the union bound estimate of Pr(E) is\nM - 1\n\nd\nPr(E) ≈ 2\nQ\n.\nM\n2σ\nShow that in this case the union bound estimate is exact. Explain why.\n\n5.3. PERFORMANCE ANALYSIS IN THE POWER-LIMITED REGIME\n5.3 Performance analysis in the power-limited regime\nRecall that the power-limited regime is defined as the domain in which the nominal spectral\nefficiency ρ is not greater than 2 b/2D. In this regime we normalize all quantities \"per bit,\" and\ngenerally use Eb/N0 as our normalized measure of signal-to-noise ratio.\nThe baseline uncoded signal set in this regime is the one-dimensional 2-PAM signal set A =\n{±α}, or equivalently a K -cube constellation AK . Such a constellation has bit rate (nominal\nspectral efficiency) ρ = 2 b/2D, average energy ber bit Eb = α2, minimum squared distance\nd2\nmin(A) = 4α2, and average number of nearest neighbors per bit Kb(A) = 1. By the UBE (5.7),\nits error probability per bit is given by\n√\nPb(E ) ≈ Q (2Eb/N0),\n(5.8)\n√\n√\n√\nwhere we now use the \"Q-of-the-square-root-of\" function Q , defined by Q (x) = Q(\nx) (see\nAppendix). This baseline curve of Pb(E) vs. Eb/N0 is plotted in Chapter 4, Figure 1.\nThe effective coding gain γeff (A) of a signal set A at a given target error probability per bit\nPb(E ) will be defined as the difference in dB between the Eb/N0 required to achieve the target\nPb(E ) with A and the Eb/N0 required to achieve the target Pb(E) with 2-PAM (i.e., no coding).\nFor example, we have seen that the maximum possible effective coding gain at Pb(E) ≈ 10-5\nis approximately 11.2 dB. For lower Pb(E), the maximum possible gain is higher, and for higher\nPb(E ), the maximum possible gain is lower.\nIn this definition, the effective coding gain includes any gains that result from using a lower\nnominal spectral efficiency ρ < 2 b/2D, which as we have seen can range up to 3.35 dB. If ρ is\nheld constant at ρ = 2 b/2D, then the maximum possible effective coding gain is lower; e.g.,\nat Pb(E) ≈ 10-5 it is approximately 8 dB. If there is a constraint on ρ (bandwidth), then it is\nbetter to plot Pb(E) vs. SNRnorm, especially to measure how far A is from achieving capacity.\nThe UBE allows us to estimate the effective coding gain as follows. The probability of error\nper bit (not in general the same as the bit error probability!) is\nPr(E)\nKmin(A) √ d2\nmin(A)\nPb(E) = log2 |A| ≈\nQ\n2N0\n,\nlog2 |A|\n√\n\nsince Q\nd2\n= Q(dmin(A)/2σ). In the power-limited regime, we define the nominal\nmin(A)/2N0\ncoding gain γc(A) as\nd2\nmin(A)\nγc(A) =\n.\n(5.9)\n4Eb\nThis definition is normalized so that for 2-PAM,\n√ γc(A) = 1. Because nominal coding gain is\na multiplicative factor in the argument of the Q (·) function, it is often measured in dB. The\nUBE then becomes\n√\nPb(E) ≈ Kb(A)Q (2γc(A)Eb/N0),\n(5.10)\nwhere Kb(A) = Kmin(A)/ log2 |A| is the average number of nearest neighbors per transmitted\nbit. Note that for 2-PAM, this expression is exact.\nGiven γc(A) and Kb(A), we may obtain a plot of the UBE (5.10) simply by moving the baseline\ncurve (Figure 1 of Chapter 4) to the left by γc(A) (in dB), and then up by a factor of Kb(A),\nsince Pb(E) is plotted on a log scale. (This is an excellent reason why error probability curves\nare always plotted on a log-log scale, with SNR measured in dB.)\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\nThus if Kb(A) = 1, then the effective coding gain γeff (A) is equal to the nominal coding gain\nγc(A) for all Pb(E ), to the accuracy of the UBE. However, if Kb(A) > 1, then the effective\ncoding gain is less than the nominal coding gain by an amount which depends on the steepness\nof the Pb(E) vs. Eb/N0 curve at the target Pb(E). At Pb(E) ≈10-5, a rule of thumb which is\nfairly accurate if Kb(A) is not too large is that an increase of a factor of two in Kb(A) costs\nabout 0.2 dB in effective coding gain; i.e.,\nγeff (A) ≈γc(A) -(0.2)(log2 Kb(A)) (in dB).\n(5.11)\nA more accurate estimate may be obtained by a plot of the union bound estimate (5.10).\nExercise 6 (invariance of coding gain). Show that the nominal coding gain γc(A) of (5.9), the\nUBE (5.10) of Pb(E), and the effective coding gain γeff (A) are invariant to scaling, orthogonal\ntransformations and Cartesian products.\n5.4 Orthogonal and related signal sets\nOrthogonal, simplex and biorthogonal signal sets are concrete examples of large signal sets that\nare suitable for the power-limited regime when bandwidth is truly unconstrained. Orthogonal\nsignal sets are the easiest to describe and analyze. Simplex signal sets are believed to be optimal\nfor a given constellation size M when there is no constraint on dimension. Biorthogonal signal\nsets are slightly more bandwidth-efficient. For large M , all become essentially equivalent.\nThe following exercises develop the parameters of these signal sets, and show that they can\nachieve reliable transmission for Eb/N0 within 3 dB from the ultimate Shannon limit.1 The\ndrawback of these signal sets is that the number of dimensions (bandwidth) becomes very large\nand the spectral efficiency ρ very small as M →inf. Also, even with the \"fast\" Walsh-Hadamard\ntransform (see Chapter 1, Problem 2), decoding complexity is of the order of M log2 M , which\nincreases exponentially with the number of bits transmitted, log2 M , and thus is actually \"slow.\"\nExercise 7 (Orthogonal signal sets). An orthogonal signal set is a set A = {aj, 1 ≤j ≤M }\nof M orthogonal vectors in RM with equal energy E (A); i.e., ⟨aj, aj′ ⟩= E(A)δjj′ (Kronecker\ndelta).\n(a) Compute the nominal spectral efficiency ρ of A in bits per two dimensions. Compute the\naverage energy Eb per information bit.\n(b) Compute the minimum squared distance d2\nmin(A). Show that every signal has Kmin(A) =\nM -1 nearest neighbors.\n(c) Let the noise variance be σ2 = N0/2 per dimension. Show that the probability of error of\nan optimum detector is bounded by the UBE\n√\nPr(E) ≤(M -1)Q (E(A)/N0).\n(d) Let M →inf with Eb held constant. Using an asymptotically accurate upper bound for\n√\nthe Q (·) function (see Appendix), show that Pr(E) → 0 provided that Eb/N0 > 2 ln 2 (1.42\ndB). How close is this to the ultimate Shannon limit on Eb/N0? What is the nominal spectral\nefficiency ρ in the limit?\n1Actually, it can be shown that with optimum detection orthogonal signal sets can approach the ultimate\nShannon limit on Eb/N0 as M →inf; however, the union bound is too weak to prove this.\n\n5.4. ORTHOGONAL AND RELATED SIGNAL SETS\nExercise 8 (Simplex signal sets). Let A be an orthogonal signal set as above.\n(a) Denote the mean of A by m(A). Show that m(A) = 0, and compute ||m(A)||2 .\nThe zero-mean set A′ = A - m(A) (as in Exercise 2) is called a simplex signal set. It is\nuniversally believed to be the optimum set of M signals in AWGN in the absence of bandwidth\nconstraints, except at ridiculously low SNRs.\n(b) For M = 2, 3, 4, sketch A and A′ .\n(c) Show that all signals in A′ have the same energy E(A′). Compute E(A′). Compute the\ninner products ⟨aj , aj′ ⟩ for all aj , aj′ ∈A′ .\n(d) [Optional]. Show that for ridiculously low SNRs, a signal set consisting of M - 2 zero\nsignals and two antipodal signals {±a} has a lower Pr(E) than a simplex signal set. [Hint: see\nM. Steiner, \"The strong simplex conjecture is false,\" IEEE Transactions on Information\nTheory, pp. 721-731, May 1994.]\nExercise 9 (Biorthogonal signal sets). The set A′′ = ±A of size 2M consisting of the M\nsignals in an orthogonal signal set A with symbol energy E(A) and their negatives is called a\nbiorthogonal signal set.\n(a) Show that the mean of A′′ is m(A′′) = 0, and that the average energy per symbol is E(A).\n(b) How much greater is the nominal spectral efficiency ρ of A′′ than that of A, in bits per\ntwo dimensions?\n(c) Show that the probability of error of A′′ is approximately the same as that of an orthogonal\nsignal set with the same size and average energy, for M large.\n(d) Let the number of signals be a power of 2: 2M = 2k . Show that the nominal spectral\nefficiency is ρ(A′′) = 4k2-k b/2D, and that the nominal coding gain is γc(A′′) = k/2. Show that\nthe number of nearest neighbors is Kmin(A′′) = 2k - 2.\nExample 2 (Biorthogonal signal sets). Using Exercise 9, we can estimate the effective coding\ngain of a biorthogonal signal set using our rule of thumb (5.11), and check its accuracy against\na plot of the UBE (5.10).\nThe 2k = 16 biorthogonal signal set A has dimension N = 2k-1 = 8, rate k = 4 b/sym, and\nnominal spectral efficiency ρ(A) = 1 b/2D. With energy E(A) per symbol, it has Eb = E(A)/4\nand d2\nmin(A) = 2E(A), so its nominal coding gain is\nγc(A) = d2\nmin(A)/4Eb = 2 (3.01 dB),\nThe number of nearest neighbors is Kmin(A) = 2k - 2 = 14, so Kb(A) = 14/4 = 3.5, and the\nestimate of its effective coding gain at Pb(E) ≈ 10-5 by our rule of thumb (5.11) is thus\nγeff (A) ≈ 3 - 2(0.2) = 2.6 dB.\nA more accurate plot of the UBE (5.10) may be obtained by shifting the baseline curve (Figure\n√\n1 of Chapter 4) left by 3 dB and up by half a vertical unit (since 3.5 ≈\n10), as shown in Figure\n3. This plot shows that the rough estimate γeff (A) ≈ 2.6 dB is quite accurate at Pb(E) ≈ 10-5 .\nSimilarly, the 64-biorthogonal signal set A′ has nominal coding gain γc(A′) = 3 (4.77 dB),\nKb(A′) = 62/6 ≈ 10, and effective coding gain γeff (A′) ≈ 4.8 - 3.5(0.2) = 4.1 dB by our rule\nof thumb. The 256-biorthogonal signal set A′′ has nominal coding gain γc(A′′) = 4 (6.02 dB),\nKb(A′′) = 254/8 ≈ 32, and effective coding gain γeff (A′′) ≈ 6 - 5(0.2) = 5.0 dB by our rule of\nthumb. Figure 3 also shows plots of the UBE (5.10) for these two signal constellations, which\nshow that our rule of thumb continues to be fairly accurate.\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\nBiorthogonal signal sets\nP b (E)\n-1\n-2\n-3\n-4\n-5\n-6\n-2\n-1\nUncoded 2-PAM\nbiorthogonal M=16\nbiorthogonal M=64\nbiorthogonal M=256\nEb/N0 [dB]\nFigure 3. Pb(E) vs. Eb/N0 for biorthogonal signal sets with 2k = 16, 64 and 256.\n5.5 Performance in the bandwidth-limited regime\nRecall that the bandwidth-limited regime is defined as the domain in which the nominal spectral\nefficiency ρ is greater than 2 b/2D; i.e., the domain of nonbinary signaling. In this regime we\nnormalize all quantities \"per two dimensions,\" and use SNRnorm as our normalized measure of\nsignal-to-noise ratio.\nThe baseline uncoded signal set in this regime is the M-PAM signal set A\n=\nα{±1, ±3, . . . , ±(M - 1)}, or equivalently the (M × M)-QAM constellation A2. Typically M is\na power of 2. Such a constellation has bit rate (nominal spectral efficiency) ρ = 2 log2 M b/2D\nand minimum squared distance d2\nmin(A2) = 4α2. As shown in Chapter 4, its average energy per\ntwo dimensions is\n2α2(M2 - 1) = d2\nEs =\nmin(A)(2ρ - 1) .\n(5.12)\nThe average number of nearest neighbors per two dimensions is twice that of M-PAM, namely\nKs(A) = 4(M - 1)/M, which rapidly approaches Ks(A) ≈ 4 as M becomes large. By the UBE\n(5.7), the error probability per two dimensions is given by\n√\nPs(E) ≈ 4Q (3SNRnorm).\n(5.13)\nThis baseline curve of Ps(E) vs. SNRnorm was plotted in Figure 2 of Chapter 4.\n\n5.6. DESIGN OF SMALL SIGNAL CONSTELLATIONS\nIn the bandwidth-limited regime, the effective coding gain γeff (A) of a signal set A at a given\ntarget error rate Ps(E) will be defined as the difference in dB between the SNRnorm required\nto achieve the target Ps(E) with A and the SNRnorm required to achieve the target Ps(E) with\nM -PAM or (M × M )-QAM (no coding).\nFor example, we saw from Figure 2 of Chapter 4 that the maximum possible effective coding\ngain at Ps(E) ≈ 10-5 is approximately 8.4 dB, which is about 3 dB less than in the power-limited\nregime (due solely to the fact that the bandwidth is fixed).\nThe effective coding gain is again estimated by the UBE, as follows. The probability of error\nper two dimensions is\nPs(E) = 2 Pr(E)\n2Kmin(A) √ d2\nmin(A)\n≈\nQ\n.\nN\nN\n2N0\nIn the bandwidth-limited regime, we define the nominal coding gain γc(A) as\nγc(A) = (2ρ - 1)d2\n6Es\nmin(A) .\n(5.14)\nThis definition is normalized so that for M -PAM or (M × M )-QAM, γc(A) = 1. Again, γc(A)\nis often measured in dB. The UBE (5.10) then becomes\n√\nPs(E) ≈ Ks(A)Q (3γc(A)SNRnorm),\n(5.15)\nwhere Ks(A) = 2Kmin(A)/N is the average number of nearest neighbors per two dimensions.\nNote that for M -PAM or (M × M )-QAM, this expression reduces to (5.13).\nGiven γc(A) and Ks(A), we may obtain a plot of (5.15) by moving the baseline curve (Figure\n2 of Chapter 4) to the left by γc(A) (in dB), and up by a factor of Ks(A)/4. The rule of thumb\nthat an increase of a factor of two in Ks(A) over the baseline Ks(A) = 4 costs about 0.2 dB in\neffective coding gain at Ps(E) ≈ 10-5 may still be used if Ks(A) is not too large.\nExercise 6 (invariance of coding gain, cont.) Show that in the bandwidth-limited regime the\nnominal coding gain γc(A) of (5.14), the UBE (5.15) of Ps(E), and the effective coding gain\nγeff (A) are invariant to scaling, orthogonal transformations and Cartesian products.\n5.6 Design of small signal constellations\nThe reader may now like to try to find the best constellations of small size M in N dimensions,\nusing coding gain γc(A) as the primary figure of merit, and Kmin(A) as a secondary criterion.\nExercise 10 (small nonbinary constellations).\n(a) For M = 4, the (2 × 2)-QAM signal set is known to be optimal in N = 2 dimensions. Show\nhowever that there exists at least one other inequivalent two-dimensional signal set A′ with the\nsame coding gain. Which signal set has the lower \"error coefficient\" Kmin(A)?\n(b) Show that the coding gain of (a) can be improved in N = 3 dimensions. [Hint: consider\nthe signal set A′′ = {(1, 1, 1), (1, -1, -1), (-1, 1, -1), (-1, -1, 1)}.] Sketch A′′ . What is the\ngeometric name of the polytope whose vertex set is A′′?\n(c) For M = 8 and N = 2, propose at least two good signal sets, and determine which one is\nbetter. [Open research problem: Find the optimal such signal set, and prove that it is optimal.]\n(d) [Open research problem.] For M = 16 and N = 2, the hexagonal signal set of Figure 1(e),\nChapter 4, is thought to be near-optimal. Prove that it is optimal, or find a better one.\n\nCHAPTER 5. PERFORMANCE OF SMALL SIGNAL SETS\n5.7 Summary: Performance analysis and coding gain\nThe results of this chapter may be summarized very simply.\nIn the power-limited regime, the nominal coding gain is γc(A) = d2\nmin(A)/4Eb. To the accuracy\n√\nof the UBE, Pb(E) ≈ Kb(A)Q (2γc(A)Eb/N0). This curve may be plotted by moving the\n√\npower-limited baseline curve Pb(E) ≈ Q (2Eb/N0) to the left by γc(A) in dB and up by a\nfactor of Kb(A). An estimate of the effective coding gain at Pb(E) ≈ 10-5 is γeff (A) ≈ γc(A) -\n(0.2)(log2 Kb(A)) dB.\nIn the bandwidth-limited regime, the nominal coding gain is γc(A) = (2ρ - 1)d2\n√\nmin(A)/6Es.\nTo the accuracy of the UBE, Ps(E) ≈ Ks(A)Q (3γc(A)SNRnorm). This curve may be plotted\n√\nby moving the bandwidth-limited baseline curve Ps(E) ≈ 4Q (3SNRnorm) to the left by γc(A)\nin dB and up by a factor of Ks(A)/4. An estimate of the effective coding gain at Ps(E) ≈ 10-5\nis γeff (A) ≈ γc(A) - (0.2)(log2 Ks(A)/4) dB.\nAppendix: The Q function\nThe Gaussian probability of error (or Q) function, defined by\ninf\nQ(x) =\n√ 1 e -y2/2 dy,\nx\n2π\narises frequently in error probability calculations on Gaussian channels. In this appendix we\ndiscuss some of its properties.\nAs we have seen, there is very often a square root in the argument of the Q function. This\nsuggests that it might have been more useful to define a \"Q-of-the-square-root-of\" function\n√\n√\nQ (x) such that Q (x2) = Q(x); i.e.,\n√\nQ (x) = Q( √ x) =\n\n√\ninf\n√\ne -y2/2 dy.\nx\n2π\n√\nFrom now on we will use this Q function instead of the Q function. For example, our baseline\ncurves for 2-PAM and (M × M )-QAM will be\n√\nPb(E)\n=\nQ (2Eb/N0);\n√\nPs(E) ≈ 4Q (3SNRnorm).\n√\nThe Q or Q functions do not have a closed-form expression, but must be looked up in tables.\nNon-communications texts usually tabulate the complementary error function, namely\ninf\nerfc(x) =\n√ 1 e -y2 dy.\nπ\nx\n√\n√\n\nEvidently Q(x) = erfc(x/ 2), and Q (x) = erfc( x/2).\n√\nThe main property of the Q or Q function is that it decays exponentially with x2 according\nto\n√\n-x2/2\nQ (x 2) = Q(x) ≈ e\n.\n\n5.7. SUMMARY: PERFORMANCE ANALYSIS AND CODING GAIN\nThe following exercise gives several ways to prove this, including upper bounds, a lower bound,\nand an estimate.\n√\nExercise A (Bounds on the Q function).\n(a) As discussed in Chapter 3, the Chernoff bound on the probability that a real random\nvariable Z exceeds b is given by\nPr{Z ≥b} ≤es(Z-b),\ns\n\n≥0\n(since es(z-b) ≥ 1 when z ≥ b, and es(z-b) ≥ 0 otherwise). When optimized over s ≥ 0, the\nChernoff exponent is asymptotically correct.\nUse the Chernoff bound to show that\n√\n-x2/2\nQ (x 2) ≤e\n.\n(5.16)\n(b) Integrate by parts to derive the upper and lower bounds\n√\nQ (x 2) < √ 1\ne -x2/2;\n(5.17)\n2πx2\n√\n\nQ (x 2) >\n1 -\n√ 1\ne -x2/2 .\n(5.18)\nx\n2πx2\n(c) Here is another way to establish these tight upper and lower bounds. By using a simple\nchange of variables, show that\n√\n-x 2 inf\n\nQ (x 2) = √\n2πe 2\nexp -\ny2\n-xy\ndy.\nThen show that\n1 -y\n≤exp -y2\n≤1.\nPutting these together, derive the bounds of part (b).\nFor (d)-(f), consider a circle of radius x inscribed in a square of side 2x as shown below.\n'$\nx-\n&%\n(d) Show that the probability that a two-dimensional iid real Gaussian random variable X\n√\nwith variance σ2 = 1 per dimension falls inside the square is equal to (1 -2Q (x2))2 .\n(e) Show that the probability that X falls inside the circle is 1 -e-x2/2.\n[Hint:\nwrite\n\npX (x) in polar coordinates: i.e., pRΘ(r, θ) = 2π re-r2/2 . You can then compute the integral\n2π dθ\nx dr pRΘ(r, θ) in closed form.]\n(f) Show that (d) and (e) imply that when x is large,\n√\nQ (x 2) ≤ 1 e -x2/2 ."
    },
    {
      "category": "Resource",
      "title": "ps1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/3e91a9520411d3fa12266784a9927602_ps1.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, February 2, 2005\nMIT, Spring 2005\nHandout #3\nDue: Wednesday, February 9,\nProblem Set 1\nThese exercises use the decibel (dB) scale, defined by:\nratio or multiplicative factor of α ↔ 10 log10 α dB.\nThe following short table should be committed to memory:\nα\ndB\ndB\n(round numbers)\n(two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n6.02\n6.99\n9.03\n10.00\nProblem 1.1 (Compound interest and dB)\nHow long does it take to double your money at an interest rate of P%? The bankers' \"Rule\nof 72\" estimates that it takes about 72/P years; e.g., at a 5% interest rate compounded\nannually, it takes about 14.4 years to double your money.\n(a) An engineer decides to interpolate the dB table above linearly for 1 ≤1 + p ≤1.25;\ni.e.,\nratio or multiplicative factor of 1 + p ↔ 4p dB.\nShow that this corresponds to a \"Rule of 75;\" e.g., at a 5% interest rate compounded\nannually, it takes 15 years to double your money.\n(b) A mathematician linearly approximates the dB table for p ≈ 0 by noting that as\np → 0, ln(1 + p) → p, and translates this into a \"Rule of N\" for some real number\nN. What is N? Using this rule, how many years will it take to double your money\nat a 5% interest rate, compounded annually? What happens if interest is compounded\ncontinuously?\n(c) How many years will it actually take to double your money at a 5% interest rate,\ncompounded annually? [Hint: 10 log10 7 = 8.45 dB.] Whose rule best predicts the correct\nresult?\n\nProblem 1.2 (Biorthogonal codes)\nA 2m × 2m {±1}-valued Hadamard matrix H2m may be constructed recursively as the\nm-fold tensor product of the 2 × 2 matrix\n+1\n+1\nH2 =\n+1\n-1\n,\nas follows:\n\n+H2m-1\n+H2m-1\nH2m =\n.\n+H2m-1\n-H2m-1\n(a) Show by induction that:\n(i) (H2m )T = H2m , where T denotes the transpose; i.e., H2m is symmetric;\n(ii) The rows or columns of H2m form a set of mutually orthogonal vectors of length 2m;\n(iii) The first row and the first column of H2m consist of all +1s;\n(iv) There are an equal number of +1s and -1s in all other rows and columns of H2m ;\n(v) H2m H2m = 2mI2m ; i.e., (H2m )-1 = 2-mH2m , where -1 denotes the inverse.\n(b) A biorthogonal signal set is a set of real equal-energy orthogonal vectors and their\nnegatives. Show how to construct a biorthogonal signal set of size 64 as a set of {±1}-\nvalued sequences of length 32.\n(c) A simplex signal set S is a set of real equal-energy vectors that are equidistant and\nthat have zero mean m(S) under an equiprobable distribution. Show how to construct\na simplex signal set of size 32 as a set of 32 {±1}-valued sequences of length 31. [Hint:\nThe fluctuation O -m(O) of a set O of orthogonal real vectors is a simplex signal set.]\n(d) Let Y = X + N be the received sequence on a discrete-time AWGN channel, where\nthe input sequence X is chosen equiprobably from a biorthogonal signal set B of size 2m+1\nconstructed as in part (b). Show that the following algorithm implements a minimum-\ndistance decoder for B (i.e., given a real 2m-vector y, it finds the closest x ∈B to y):\n(i) Compute z = H2m y, where y is regarded as a column vector;\n(ii) Find the component zj of z with largest magnitude zj ;\n|\n|\n(iii) Decode to sgn(zj)xj, where sgn(zj) is the sign of the largest-magnitude component\nzj and xj is the corresponding column of H2m .\n(e) Show that a circuit similar to that shown in Figure 1 below for m = 2 can implement\nthe 2m × 2m matrix multiplication z = H2m y with a total of only m\naddition\n× 2m\nand subtraction operations. (This is called the \"fast Hadamard transform,\" or \"Walsh\ntransform,\" or \"Green machine.\")\n\n@\n@\nA U\n\nA\nA\n\n-\nA\nA U\n-\nA\n-\n-\n-\n-\n+\n+\n+\n-\n+\n-\n-\n-\nA\n�A\n\n-\n\n-\n�\n-\n\n-\n�\n-\n\ny1\nz1\ny2\nz2\ny3\nz3\ny4\nz4\nFigure 1. Fast 2m × 2m Hadamard transform for m = 2.\nProblem 1.3 (16-QAM signal sets)\nThree 16-point 2-dimensional quadrature amplitude modulation (16-QAM) signal sets\nare shown in Figure 2, below. The first is a standard 4 × 4 signal set; the second is the\nV.29 signal set; the third is based on a hexagonal grid and is the most power-efficient\n16-QAM signal set known. The first two have 90* symmetry; the last, only 180* . All have\na minimum squared distance between signal points of d2\n= 4.\nmin\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr √\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\n-3 -1\nr\nr\nr\nr\n-5 -3 -1\nr\nr\nr\nr\nr\nr\n3r\nr\n5r\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\n-2.5-0.5 1.5 3.5\n-\n√\n-2\n√\nr\n(a)\n(b)\n(c)\nFigure 2. 16-QAM signal sets. (a) (4 × 4)-QAM; (b) V.29; (c) hexagonal.\n(a) Compute the average energy (squared norm) of each signal set if all points are\nequiprobable. Compare the power efficiencies of the three signal sets in dB.\n(b) Sketch the decision regions of a minimum-distance detector for each signal set.\n(c) Show that with a phase rotation of ±10* the minimum distance from any rotated\nsignal point to any decision region boundary is substantially greatest for the V.29 signal\nset.\nProblem 1.4 (Shaping gain of spherical signal sets)\nIn this exercise we compare the power efficiency of n-cube and n-sphere signal sets for\nlarge n.\nAn n-cube signal set is the set of all odd-integer sequences of length n within an n-cube\nof side 2M centered on the origin. For example, the signal set of Figure 2(a) is a 2-cube\nsignal set with M = 4.\nAn n-sphere signal set is the set of all odd-integer sequences of length n within an n-\nsphere of squared radius r2 centered on the origin. For example, the signal set of Figure\n\n3(a) is also a 2-sphere signal set for any squared radius r2 in the range 18 ≤r2 < 25.\nIn particular, it is a 2-sphere signal set for r2 = 64/π = 20.37, where the area πr2 of\nthe 2-sphere (circle) equals the area (2M)2 = 64 of the 2-cube (square) of the previous\nparagraph.\nBoth n-cube and n-sphere signal sets therefore have minimum squared distance between\nsignal points d2\n= 4 (if they are nontrivial), and n-cube decision regions of side 2 and\nmin\nthus volume 2n associated with each signal point. The point of the following exercise is\nto compare their average energy using the following large-signal-set approximations:\n- The number of signal points is approximately equal to the volume V (R) of the bound-\ning n-cube or n-sphere region R divided by 2n, the volume of the decision region\nassociated with each signal point (an n-cube of side 2).\n- The average energy of the signal points under an equiprobable distribution is approx-\nimately equal to the average energy E(R) of the bounding n-cube or n-sphere region\nR under a uniform continuous distribution.\n(a) Show that if R is an n-cube of side 2M for some integer M, then under the two above\napproximations the approximate number of signal points is M n and the approximate\naverage energy is nM 2/3. Show that the first of these two approximations is exact.\n(b) For n even, if R is an n-sphere of radius r, compute the approximate number of signal\npoints and the approximate average energy of an n-sphere signal set, using the following\nknown expressions for the volume V⊗(n, r) and the average energy E⊗(n, r) of an n-sphere\nof radius r:\n(πr2)n/2\nV⊗(n, r)\n=\n;\n(n/2)!\nnr\nE⊗(n, r)\n=\n.\nn + 2\n(c) For n = 2, show that a large 2-sphere signal set has about 0.2 dB smaller average\nenergy than a 2-cube signal set with the same number of signal points.\n(d) For n = 16, show that a large 16-sphere signal set has about 1 dB smaller average\nenergy than a 16-cube signal set with the same number of signal points. [Hint: 8! = 40320\n(46.06 dB).]\n(e) Show that as n →infa large n-sphere signal set has a factor of πe/6 (1.53 dB) smaller\naverage energy than an n-cube signal set with the same number of signal points. [Hint:\nUse Stirling's approximation, m! →(m/e)m\n.]\nas m →inf"
    },
    {
      "category": "Resource",
      "title": "ps1solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/334ceae860636a0c35b179192be61104_ps1solns.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, Feb. 9, 2005\nMIT, Spring 2005\nHandout #4\nProblem Set 1 Solutions\nProblem 1.1 (Compound interest and dB)\nHow long does it take to double your money at an interest rate of P %? The bankers' \"Rule\nof 72\" estimates that it takes about 72/P years; e.g., at a 5% interest rate compounded\nannually, it takes about 14.4 years to double your money.\n(a) An engineer decides to interpolate the dB table above linearly for 1 ≤ 1 + p ≤ 1.25;\ni.e.,\nratio or multiplicative factor of 1 + p ↔ 4p dB.\nShow that this corresponds to a \"Rule of 75;\" e.g., at a 5% interest rate compounded\nannually, it takes 15 years to double your money.\nIf your money compounds at a rate of x dB per year, then since doubling your money\ncorresponds to a 3 dB gain, it will take about Y = 3/x years to double your money.\nThe engineer's approximation to the dB table is a linear approximation that is exact near\np = 0.25 (P = 25%), where p = P/100. Under this approximation, it will take\nY =\n=\n4p\nP\nyears to double your money. Thus this approximation corresponds to a \"Rule of 75.\" For\nexample, it estimates that it takes Y = 75/5 = 15 years to double your money when\nP = 5%.\n(b) A mathematician linearly approximates the dB table for p ≈ 0 by noting that as p → 0,\nln(1 + p) → p, and translates this into a \"Rule of N \" for some real number N . What is\nN ? Using this rule, how many years will it take to double your money at a 5% interest\nrate, compounded annually? What happens if interest is compounded continuously?\nAs p → 0, we have\n10 log10(1 + p) = (10 log10 e) ln(1 + p) = 4.34 ln(1 + p) → 4.34p,\nln x\nwhere we change the base of the logarithm from 10 to e (recall that x = 10log10 x = e\n,\nso log10 x = (log10 e)(ln x)), we read 10 log10 e = 4.34 from the dB table, and we use\nln(1 + p) → p as p → 0. Thus we obtain a linear approximation\nratio or multiplicative factor of 1 + p ↔ 4.34p dB,\nwhich becomes exact as p → 0. This linear approximation translates to the estimate that\nit takes\n10 log10 2\n3.01\n69.35\nY =\n=\n=\n(10 log10 e)p\n4.34p\nP\nyears to double your money, or a \"Rule of 69.35.\" For example, this rule estimates that\nit takes Y = 69.35/5 = 13.87 years to double your money when P = 5%.\n\nA more precise calculation using natural logarithms yields\nln 2\n69.31\nY =\n=\n,\np\nP\nor a \"Rule of 69.31,\" which estimates that it takes Y = 69.31/5 = 13.86 years to double\nyour money when P = 5%.\nRemark. Note that this \"Rule of 69.31\" becomes exact when interest is compounded\ncontinuously, so that after Y years your money has increased by a factor of epY , rather\nthan the factor of (1 + p)Y that you get when interest is compounded annually.\n(c) How many years will it actually take to double your money at a 5% interest rate,\ncompounded annually? [Hint: 10 log10 7 = 8.45 dB.] Whose rule best predicts the correct\nresult?\nSince 1.05 = 21/20, a factor of 1.05 is equivalent to\n10 log10 7 + 10 log10 3 - 10 log10 10 - 10 log10 2 = 8.45 + 4.77 - 10 - 3.01 = 0.21 dB.\nThus it actually takes\n3.01\nY =\n= 14.33\n0.21\nyears to double your money when interest is compounded annually.\nWe see that this estimate is quite close to the estimate of Y = 14.4 years given by the\n\"Rule of 72.\" Thus the \"Rule of 72\" is equivalent to a linear approximation of the dB\ntable that is exact near P = 5%. This is the range in which the \"Rule of 72\" is commonly\nused. The \"Rule of 72\" also has the advantage that 72 has many integer divisors (e.g., 2,\n3, 4, 6, 8, 9, 12, . . . ), so that its estimate of Y is an easily calculated integer for many\ncommon interest rates. So in this instance the bankers have been rather clever.\nProblem 1.2 (Biorthogonal codes)\nA 2m × 2m {±1}-valued Hadamard matrix H2m may be constructed recursively as the\nm-fold tensor product of the 2 × 2 matrix\nH2 =\n+1 +1\n+1 -1\n\n,\nas follows:\n(a) Show by induction that:\n(i) (H2m )T = H2m , where T\nH2m =\n+H2m-1 +H2m-1\n+H2m-1 -H2m-1\n\n.\ndenotes the transpose; i.e., H2m is symmetric;\n(ii) The rows or columns of H2m form a set of mutually orthogonal vectors of length 2m;\n(iii) The first row and the first column of H2m consist of all +1s;\n\n(iv) There are an equal number of +1s and -1s in all other rows and columns of H2m ;\n(v) H2m H2m = 2mI2m ; i.e., (H2m )-1 = 2-mH2m , where -1 denotes the inverse.\nWe first verify that (i)-(v) hold for H2. We then suppose that (i)-(v) hold for H2m-1 . We\ncan then conclude by induction that:\n(i)\n\n+(H2m-1 )T\n+(H2m-1 )T\n+H2m-1 +H2m-1\n(H2m )T =\n=\n= H2m .\n+(H2m-1 )T\n-(H2m-1 )T\n+H2m-1 -H2m-1\n(ii) The first 2m-1 rows of H2m are of the form hj = (gj , gj ), where gj is the corresponding\nrow of H2m-1 , and the second 2m-1 rows of H2m are of the form hj+2m-1 = (gj , -gj ).\nSuppose that the rows gj of H2m-1 are mutually orthogonal. Then the inner product\n= j′ and j\n⟨hj , hj′ ⟩ is 0 whenever j\n= j′ ± 2m-1, because the inner product is the\nsum of the inner products of the first half-rows and the second half-rows, which are\nboth zero. If j = j′ - 2m-1, then the inner product is\n⟨hj , hj′ ⟩ = ⟨gj , gj ⟩ + ⟨gj , -gj ⟩ = ⟨gj , gj ⟩-⟨gj , gj ⟩ = 0,\nand similarly ⟨hj , hj′ ⟩ = 0 when j = j′ + 2m-1. Thus ⟨hj , hj′ ⟩ = 0 whenever j = j′ ,\nso the rows of H2m form a set of mutually orthogonal vectors. Since (H2m )T = H2m ,\nso do the columns.\n(iii) The first row of H2m is h0 = (g0, g0), where g0 is the first row of H2m-1 . By the\ninductive hypothesis, g0 = (+1, +1, . . . , +1), so h0 = (+1, +1, . . . , +1); i.e., all\ncolumns of H2m have a +1 as their first component. Since (H2m )T = H2m , so do all\nthe rows.\n(iv) The remaining rows of H2m are orthogonal to h0 by (ii), and thus must have an equal\nnumber of +1s and -1s. Since (H2m )T = H2m , so must the remaining columns.\n(v) Since (H2m )T = H2m , the matrix H2m H2m = H2m (H2m )T is the matrix of inner\nproducts of rows of H2m . By (ii), all off-diagonal elements of this matrix are zero.\nThe diagonal elements are the squared norms ||hj ||2 = 2m, since hj is a vector of\nlength 2m in which each component has squared norm 1. Thus H2m H2m = 2mI2m .\n(b) A biorthogonal signal set is a set of real equal-energy orthogonal vectors and their\nnegatives. Show how to construct a biorthogonal signal set of size 64 as a set of {±1}-\nvalued sequences of length 32.\nBy (a)(ii), the rows of H32 form a set O of 32 orthogonal {±1}-valued sequences of length\n32, each with energy 32. It follows that the rows of H32 and their negatives form a\nbiorthogonal set B = ±O of 64 {±1}-valued sequences of length 32.\n\n(c) A simplex signal set S is a set of real equal-energy vectors that are equidistant and\nthat have zero mean m(S) under an equiprobable distribution. Show how to construct a\nsimplex signal set of size 32 as a set of 32 {±1}-valued sequences of length 31. [Hint: The\nfluctuation O - m(O) of a set O of orthogonal real vectors is a simplex signal set.]\nAs in (b), the rows of H32 form a set O of 32 orthogonal equal-energy (and therefore\nequidistant) {±1}-valued sequences of length 32. By (a)(iii)-(iv), the mean of O is m(O) =\n(+1, 0, 0, . . . , 0), since all rows have +1 in the first column and an equal number of +1s\nand -1s in the remaining columns. Thus S = O- m(O) is a zero-mean, equal-energy and\nequidistant set of 32 row vectors of length 32 which have 0 in the first coordinate and the\nelements of the rows of H32 in the remaining coordinates. Since the first coordinate always\nhas value 0, it may be deleted without affecting any norms, distances or inner products; in\nparticular, the vectors remain zero-mean, equal-energy and equidistant. Thus we obtain\na simplex signal set S′ consisting of a set of 32 {±1}-valued sequences of length 31.\n(d) Let Y = X + N be the received sequence on a discrete-time AWGN channel, where\nthe input sequence X is chosen equiprobably from a biorthogonal signal set B of size 2m+1\nconstructed as in part (b). Show that the following algorithm implements a minimum-\ndistance decoder for B (i.e., given a real 2m-vector y, it finds the closest x ∈ B to y):\n(i) Compute z = H2m y, where y is regarded as a column vector;\n(ii) Find the component zj of z with largest magnitude |zj |;\n(iii) Decode to sgn(zj )xj , where sgn(zj ) is the sign of the largest-magnitude component zj\nand xj is the corresponding column of H2m .\nGiven y, minimizing the squared distance ||y-x||2 over x ∈ B is equivalent to maximizing\nthe inner product ⟨y, x⟩, since\n||y - x||2 = ||y||2 - 2⟨y, x⟩ + ||x||2 ,\nand ||x||2 is equal to a constant (2m) for all x ∈ B. The vector z = H2m y is the set of\ninner products ⟨y, x⟩ as x runs through the 2m rows of H2m . The set of inner products\n⟨y, x⟩ as x runs through the 2m+1 elements of B are therefore just the elements of z and\ntheir negatives. Maximizing ⟨y, x⟩ is therefore equivalent to finding the element zj of z\nwith largest magnitude |zj |, and deciding on the corresponding row xj of H2m if the sign\nof zj is positive, or on -xj if the sign is negative.\nRemark. Note that the matrix multiplication z = H2m y corresponds to implementing a\nbank of matched filters, one for each of the rows of H2m , which form the set of correlations\nof y with each of the rows of H2m . Since the rows of H2m span the signal space S ⊃ B, by\nthe theorem of irrelevance (see Chapter 2) the outputs z of this bank of matched filters\nform a set of sufficient statistics for detection of a signal x ∈S in the presence of AWGN.\nIn this case we have been able to show directly that we can find an optimal decision rule\nbased on z which is very simple.\n\n@\n@\n\n′\n\n(e) Show that a circuit similar to that shown in Figure 1 below for m = 2 can imple\nment the 2m × 2m matrix multiplication z = H2m y with a total of only m × 2m addition\nand subtraction operations. (This is called the \"fast Hadamard transform,\" or \"Walsh\ntransform,\" or \"Green machine.\")\n-\n-\n-\n+ A\n+\n\n@\n\nR\n\nA-\n\n-\n+\n-\n\n- A A\n\n-\nA AU\n-\n+ - -\nA\n\n@\nR\nAU\ny4\n-\n\n- - -\n-\ny1\nz1\nz2\nz3\nz4\ny2\ny3\nFigure 1. Fast 2m × 2m Hadamard transform for m = 2.\nThis circuit is based on the following recursion for z = H4y:\n⎡\n⎤\n⎡\n⎤\n⎡\n\n⎤\n⎡\n\n⎤\n′\n′\nz1\ny1\ny1\ny3\ny\ny\n+H2\n+ H2\n+\n′\n′\nz2 ⎥\n+H2 +H2\n⎢ y2\n⎥\n⎢\ny\ny2\ny4\ny\n⎢\n⎥\n⎢\n⎥\n=\n=\n=\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥.\n′\n′\nz3 ⎦\n+H2 -H2\n⎣ y3 ⎦\n⎣\n⎦\n⎣\ny\ny1\ny3\ny\n⎣\n⎦\n- H2\n-\n+H2\n′\n′\nz4\ny4\ny2\ny4\ny\ny\nIn other words, we first group the elements of y into pairs (y1, y2), (y3, y4), . . . . The first\nset of arithmetic elements computes the 2 × 2 Walsh-Hadamard transform of each pair;\ne.g.,\n′\n′\n= H2\ny1\n=\ny2\ny1 + y2\n.\ny1 - y2\ny\ny\n′ )\n, and again compute the\n, . . .\n′ , y\n′ ) (y\n,\n′ , y\nWe then group the elements of y into pairs (y\n2 × 2 Walsh-Hadamard transform of each pair; e.g.,\n′\n′\n′ + y\nz1\ny\ny\n= H2\n=\n.\n′\n′\n1 - y ′\nz3\ny\ny\nThus we can compute the 4 × 4 Walsh-Hadamard transform z = H4y by computing two\nstages of two 2 × 2 Walsh-Hadamard transforms, as illustrated in Figure 1.\nSimilarly, we can compute a 2m × 2m Walsh-Hadamard transform z = H2m y using the\nrecursion\nz0\n+H2m-1 +H2m-1\ny0\nH2m-1 y0 + H2m-1 y1\n=\n+H2m-1 -H2m-1\ny1\n=\nH2m-1 y0 - H2m-1 y1\n,\nz1\nby computing two 2m-1 × 2m-1 Walsh-Hadamard transforms, and then combining their\noutputs in one more stage involving 2m-1 2 × 2 Walsh-Hadamard transforms. If each\nm-1 × 2m-1 Walsh-Hadamard transform requires m - 1 stages of 2m-2 2 × 2 Walsh\nHadamard transforms, then the 2m × 2m Walsh-Hadamard transform requires m stages of\nm-1 2 × 2 Walsh-Hadamard transforms. Each 2 × 2 transform requires one addition and\none subtraction, so a total of only m × 2m-1 × 2 additions and subtractions is required.\nThus the complexity of an M = 2m-point Walsh-Hadamard transform is only of the order\nof M log2 M, rather than M 2 . This is why this algorithm is called \"fast.\"\n\nProblem 1.3 (16-QAM signal sets)\nThree 16-point 2-dimensional quadrature amplitude modulation (16-QAM) signal sets are\nshown in Figure 2, below. The first is a standard 4 × 4 signal set; the second is the\nV.29 signal set; the third is based on a hexagonal grid and is the most power-efficient\n16-QAM signal set known. The first two have 90* symmetry; the last, only 180* . All have\na minimum squared distance between signal points of d2\n= 4.\nmin\nr\nr\nr\nr\n-3 -1\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\n√\nr\nr\nr\nr\nr\nr\nr\nr\n-5r -3r -1\n3r\n5r\n-2.r5-0.r5 1.r5 3.r5\nr\nr\nr\nr\n√\nr\nr\nr\nr - 3\n√\nr\nr\nr\nr\nr\nr\nr -2\nr\n(a)\n(b)\n(c)\nFigure 2. 16-QAM signal sets. (a) (4 × 4)-QAM; (b) V.29; (c) hexagonal.\n(a) Compute the average energy (squared norm) of each signal set if all points are equiprob\nable. Compare the power efficiencies of the three signal sets in dB.\nIn Figure 2(a), there are 4 points with squared norm 2, 8 with squared norm 10, and 4\nwith squared norm 18, so\nEa = 2 + 10 + 18 = 10 (10.00 dB).\nAlternatively, both coordinates have equal probability of having squared norm 1 or 9, so\nthe average energy per coordinate is 5, and thus Ea = 10.\nIn Figure 2(b), there are 4 points with squared norm 2, 4 with squared norm 9, 4 with\nsquared norm 18, and 4 with squared norm 25, so\nEb = 2 + 9 + 18 + 25 = 13.5 (11.30 dB).\nThus this signal set is 1.3 dB less power-efficient than that of Figure 2(a).\nIn Figure 2(c), there is 1 point with squared norm 1/4, 1 with 9/4, 2 with 13/4, 2 with\n21/4, 1 with 25/4, 2 with 37/4, 3 with 49/4, 2 with 57/4 and 2 with 61/4, so\n1 + 9 + 26 + 42 + 25 + 74 + 147 + 114 + 122\nEc =\n= 8.75 (9.42 dB).\n4 × 16\nThus this signal set is about 0.6 dB more power-efficient than that of Figure 2(a).\nRemark. A more elegant way of doing this calculation is to shift the origin by (1/2, 0)\nto the least-energy point. The resulting signal set has mean (1/2, 0), and 1 point with\nsquared norm 0, 6 with 4, 6 with 12, and 3 with 16, for an average of 9. Subtracting the\nsquared norm 1/4 of its mean, we get Ec = 8.75 (9.42 dB) for the zero-mean signal set of\nFigure 2(c).\n\n(b) Sketch the decision regions of a minimum-distance detector for each signal set.\nThe minimum-distance decision regions are sketched below for Figures 2(a) and 2(b). For\nFigure 2(c), the decision regions for signals in the interior of the constellation are hexagons\n(and are hard to draw in LATEX).\nNote that in Figure 2(a) the minimum-distance decision regions correspond to two inde\npendent minimum-distance decisions on each 4-PAM coordinate.\n\nr\nr\nr\nr\nr\nr\nr\nr\n@\n\nHH\nAA\n\nJJ\nQQ\nr\n\nr\n\nr\n\nr\nr\n\nr\n\nr\n\nr\n\nr\n\nr\n\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\n\nr\nr\nr\nr\nr\nr\nr\nr\nAA\nH\nH\n\n@\nQQ\nJJ\n\nr\n\nr\n\nr\n\nr\n\nr\n\nr\n\nr\nr\nr\nr\nr\nr\n\n(a)\n(b)\n(c)\nd\nWe also draw 2-spheres (circles) of radius 1 about each signal point. The fact that these\n2-spheres are disjoint except where they kiss (at their points of tangency) shows that\nmin = 2. This makes it obvious that 2(a) is more densely packed (more power-efficient)\nthan 2(b), and in turn that 2(c) is more densely packed than 2(a). In fact, 2(c) might well\nbe what we would come up with if we took 16 pennies and tried to pack them together\nas densely as possible.\nSince Gaussian noise is circularly symmetric and its probability density decreases expo\nnentially with distance, the dominant types of errors in AWGN will be those that occur\nat the points of tangency of these 2-spheres, which lie on decision region boundaries.\nRemark. Figure 2(b) could obviously be made somewhat more power-efficient without\nchanging its essential character by somewhat decreasing the radii of its outer points until\ntheir associated 2-spheres become tangent to innermore 2-spheres.\n(c) Show that with a phase rotation of ±10* the minimum distance from any rotated signal\npoint to any decision region boundary is substantially greatest for the V.29 signal set.\nThe question here is: if each signal point x = (x, y) is rotated by a small angle θ to a\nrotated point x′ = (x cos θ -y sin θ, y cos θ + x sin θ), what is the worst-case reduction in\ndistance to the nearest decision boundary?\nFor small phase rotations (|θ| ≤10*), we may use the approximations cos θ ≈1, sin θ ≈θ,\nor x′ ≈(x -θy, y + θx).\nFor Figure 2(a), a rotation of either a point of type (3, 1) or of type (3, 3) by a small angle\nθ therefore reduces the distance to the nearest decision boundary by approximately 3θ, or\nby approximately 0.5 when |θ| ≈10* . Thus the minimum distance to a decision boundary\nis cut approximately by a factor of 2, or the minimum squared distance by a factor of 4,\nwhich we will see later amounts to a reduction of about 6 dB in signal-to-noise margin.\n\nFor Figure 2(b), all of the outer points have enough distance from their nearest decision\nboundaries in the tangential direction so that the minimum distance is still at least 1 after\na 10* rotation. For example, a point of type (3, 0) rotates approximately to (3, 0.5), which\nis still distance 1 from the nearest point (3, 1.5) on the decision boundary. The worst\ncase is therefore an inner point of type (1, 1), whose minimum distance of 1 is reduced by\nabout 0.17 by a 10* rotation. Since (0.83)2 is about 1.6 dB, this amounts to about a 1.6\ndB reduction in signal-to-noise margin. Thus even though the Figure 2(b) signal set has\n1.3 worse signal-to-noise margin to start with, in the presence of uncompensated ±10*\nphase rotations (\"phase jitter\") it becomes more than 3 dB better than Figure 2(a).\nFor Figure 2(c), it is clear that the outer points are affected by phase rotations similarly\n√\nto the outer points of 2(a). For example, a point of type (2.5, 3) has squared norm 9.25\nand thus radius 3.04. A phase rotation of θ moves it directly by an amount 3.04θ toward\nits nearest decision boundary, so as in 2(a) a rotation of about θ = 10* cuts the distance\nto the nearest decision boundary by a factor of about 2, for a reduction in SNR margin\nof about 6 dB.\nProblem 1.4 (Shaping gain of spherical signal sets)\nIn this exercise we compare the power efficiency of n-cube and n-sphere signal sets for\nlarge n.\nAn n-cube signal set is the set of all odd-integer sequences of length n within an n-cube\nof side 2M centered on the origin. For example, the signal set of Figure 2(a) is a 2-cube\nsignal set with M = 4.\nAn n-sphere signal set is the set of all odd-integer sequences of length n within an n-\nsphere of squared radius r2 centered on the origin. For example, the signal set of Figure\n3(a) is also a 2-sphere signal set for any squared radius r2 in the range 18 ≤ r2 < 25.\nIn particular, it is a 2-sphere signal set for r2 = 64/π = 20.37, where the area πr2 of\nthe 2-sphere (circle) equals the area (2M )2 = 64 of the 2-cube (square) of the previous\nparagraph.\nBoth n-cube and n-sphere signal sets therefore have minimum squared distance between\nsignal points d2\n= 4 (if they are nontrivial), and n-cube decision regions of side 2 and\nmin\nthus volume 2n associated with each signal point. The point of the following exercise is to\ncompare their average energy using the following large-signal-set approximations:\n- The number of signal points is approximately equal to the volume V (R) of the bound\ning n-cube or n-sphere region R divided by 2n , the volume of the decision region\nassociated with each signal point (an n-cube of side 2).\n- The average energy of the signal points under an equiprobable distribution is approx\nimately equal to the average energy E(R) of the bounding n-cube or n-sphere region\nR under a uniform continuous distribution.\n\n(a) Show that if R is an n-cube of side 2M for some integer M, then under the two\nabove approximations the approximate number of signal points is M n and the approximate\naverage energy is nM 2/3. Show that the first of these two approximations is exact.\nThe first approximation is that the number mcube of signal points is approximately\nV (R)\n(2M)n\nmcube ≈\n=\n= M n .\n2n\n2n\nThis approximation is exact, because it can be seen that an n-cube constellation of side\n2M is simply the n-fold Cartesian product An = {(x1, x2, . . . , xn) | xk ∈A} of an M\nPAM costellation A = {±1, ±3, . . . , ±(M - 1)}, the set of all odd integers in the interval\n[-M, M]. (For example, Figure 2(a) is the 2-fold Cartesian product A2 of a 4-PAM\nconstellation A.)\nThe second approximation is that the average energy Ecube is approximately\nEcube ≈ E(R) = n(M 2/3),\nwhere we observe that the average energy over an n-cube R of side 2M under an equiprob\nable distribution p(x), whose marginals are the uniform distributions p(xk ) = 1/2M, is\nM\n\nn\n\n2M 3 1\nM 2\nE(R) =\nR\n||x||2 p(x)dx =\np(xk )dxk = n\n= n\n.\nxk\n2M\n-M\nk=1\n(In this case the exact expression is\nE\nM 2 - 1\ncube = n\n,\nsince the constellation An is the n-fold Cartesian product of an M-PAM constellation A\nwhose average energy is EA = (M 2 - 1)/3.)\n(b) For n even, if R is an n-sphere of radius r, compute the approximate number of signal\npoints and the approximate average energy of an n-sphere signal set, using the following\nknown expressions for the volume V⊗(n, r) and the average energy E⊗(n, r) of an n-sphere\nof radius r:\n(πr2)n/2\nV⊗(n, r)\n=\n\n;\n(n/2)!\nnr\nE⊗(n, r)\n=\n\n.\nn + 2\nThe first approximation is that the number msphere of signal points is approximately\nV (R)\n(πr2)n/2\n.\nmsphere ≈\n2n\n= 2n(n/2)!\nThe second approximation is that the average energy Esphere is approximately\nnr\nEsphere ≈ E(R) =\n.\nn + 2\n\n(c) For n = 2, show that a large 2-sphere signal set has about 0.2 dB smaller average\nenergy than a 2-cube signal set with the same number of signal points.\nIn general, in n dimensions, to make mcube = msphere we choose M and r so that\n(πr2)n/2\nM n =\n;\n2n(n/2)!\ni.e.,\nπr2\nM 2 =\n.\n22((n/2)!)2/n\nThen the ratio of the average energy of the n-cube to that of the n-sphere is\nEcube\nn + 2 nM 2\nπ(n + 2)\n=\n=\n.\nEsphere\nnr2\n12((n/2)!)2/n\nFor example, for n = 2, setting the volumes equal, (2M )2 = πr2, we have\nEcube\nπ\nE\n=\n(0.20 dB).\nsphere\nThus in two dimensions using a large circular rather than square constellation saves only\nabout 0.2 dB in power efficiency.\n(d) For n = 16, show that a large 16-sphere signal set has about 1 dB smaller average\nenergy than a 16-cube signal set with the same number of signal points. [Hint: 8! = 40320\n(46.06 dB).]\nFor n = 16, however, we have\nEcube\n18π\n3π\n=\n=\n= (4.77 + 4.97 -3.01 - 46.06) = 0.97 dB;\nEsphere\n12(8!)1/8\n2(40320)1/8\ni.e., using a 16-sphere rather than 16-cube constellation saves nearly 1 dB in power\nefficiency (signal-to-noise margin).\n(e) Show that as n →infa large n-sphere signal set has a factor of πe/6 (1.53 dB) smaller\naverage energy than an n-cube signal set with the same number of signal points. [Hint:\nUse Stirling's approximation, m! →(m/e)m as m →inf.]\nUsing the hint, as n →infwe have\n((n/2)!)2/n → n .\n2e\nE\nTherefore\ncube\nπ(n + 2)\nπe(n + 2)\nπe\n=\n→\n→\n(1.53 dB).\nEsphere\n12((n/2)!)2/n\n6n\nSince the ratio is monotonically increasing, we conclude that the greatest possible gain in\nany number of dimensions is 1.53 dB."
    },
    {
      "category": "Resource",
      "title": "ps2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/c0969ee77c735a95efe3ff6895f66aca_ps2.pdf",
      "content": "′\n\n6.451 Principles of Digital Communication II\nWednesday, February 9, 2005\nMIT, Spring 2005\nHandout #5\nDue: Wednesday, February 16, 2005\nProblem Set 2\nProblem 2.1 (Cartesian-product constellations)\n(a) Show that if A′ = AK , then the parameters N, log2 M, E(A′) and Kmin(A′) of A are K\ntimes as large as the corresponding parameters of A, whereas the normalized parameters\nρ, Es, Eb and d2\nmin(A) are the same as those of A. Verify that these relations hold for\n(M × M)-QAM constellations.\n(b) Show that if the signal constellation is a Cartesian product AK , then MD detection\ncan be performed by performing independent MD detection on each of the K components\nof the received KN-tuple y = (y1, y2, . . . , yK ). Using this result, sketch the decision\nregions of a (4 × 4)-QAM signal set.\n(c) Show that if Pr(E) is the probability of error for MD detection of A, then the proba\nbility of error for MD detection of A′ is\nPr(E)′ = 1 -(1 -Pr(E))K ,\nShow that Pr(E)′ ≈K Pr(E) if Pr(E) is small.\nProblem 2.2 (Pr(E) invariance to translation, orthogonal transformations, or scaling)\nLet Pr(E | aj ) be the probability of error when a signal aj is selected equiprobably from an\nN-dimensional signal set A and transmitted over a discrete-time AWGN channel, and the\nchannel output Y = aj + N is mapped to a signal ˆaj ∈A by a minimum-distance decision\nrule. An error event E occurs if ˆaj =\naj . Pr(E) denotes the average error probability.\n(a) Show that the probabilities of error Pr(E | aj ) are unchanged if A is translated by\nany vector v; i.e., the constellation A′ = A + v has the same Pr(E) as A.\n(b) Show that Pr(E) is invariant under orthogonal transformations; i.e., A′ = UA has\nthe same Pr(E) as A when U is any orthogonal N × N matrix (i.e., U -1 = U T ).\n(c) Show that Pr(E) is unchanged if both A and N are scaled by α > 0.\nProblem 2.3 (optimality of zero-mean constellations)\nConsider an arbitrary signal set A = {aj , 1 ≤ j ≤ M}. Assume that all signals are\nequiprobable. Let m(A) = M\naj be the average signal, and let A′ be A translated by\nj\nm(A) so that the mean of A′ is zero: A′ = A -m(A) = {aj -m(A), 1 ≤j ≤M}. Let\nE(A) and E(A′) denote the average energies of A and A′, respectively.\n(a) Show that the error probability of an MD detector is the same for A′ as it is for A.\n(b) Show that E(A′) = E(A) -||m(A)|| . Conclude that removing the mean m(A) is\nalways a good idea.\n(c) Show that a binary antipodal signal set A = {±a} is always optimal for M = 2.\n\nProblem 2.4 (Non-equiprobable signals).\nLet aj and aj be two signals that are not equiprobable. Find the optimum (MPE) pairwise\ndecision rule and pairwise error probability Pr{aj → aj }.\nProblem 2.5 (UBE for M -PAM constellations).\nFor an M -PAM constellation A, show that Kmin(A) = 2(M - 1)/M . Conclude that the\nunion bound estimate of Pr(E) is\nM - 1\nd\nPr(E) ≈ 2\nM\nQ 2σ .\nObserve that in this case the union bound estimate is exact. Explain why."
    },
    {
      "category": "Resource",
      "title": "ps2solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/ea7b0e888a9a878ab1020ce609f22957_ps2solns.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, Feb. 16, 2005\nMIT, Spring 2005\nHandout #6\nProblem Set 2 Solutions\nProblem 2.1 (Cartesian-product constellations)\n(a) Show that if A′ = AK , then the parameters N, log2 M, E(A′) and Kmin(A′) of A′ are K\ntimes as large as the corresponding parameters of A, whereas the normalized parameters\nρ, Es, Eb and d2\nmin(A) are the same as those of A. Verify that these relations hold for\n(M × M)-QAM constellations.\nThere are M possibilities for each of the n components of A′ , so the total number of\npossibilities is |A′| = M K . The number of bits supported by A′ is therefore log2 |A′| =\nK log2 M, K times the number for A. The number of dimensions in A′ is KN, so its\nnominal spectral efficiency is ρ = (log2 M)/N, the same as that of A.\nThe average energy of A′ is\nEA = ||a1||2 + ||a2||2 + · · · + ||aK ||2 = KEA.\nThe average energy per bit is Eb = EA / log2 |A′| = EA/(log2 M), and the average energy\nper two dimensions is Es = 2EA /KN = 2EA/N, which for both are the same as for A.\nTwo distinct points in A′ must differ in at least one component. The minimum squared\ndistance is therefore the minimum squared distance in any component, which is d2\nmin(A).\nThe number of nearest neighbors to any point (a1, a2, . . . , aK ) ∈A′ is the sum of the num\nbers of nearest neighbors to a1, a2, . . . , aK , respectively, since there is a nearest neighbor\nfor each such nearest neighbor to each component. The average number Kmin(A′) of near-\nest neighbors to A′ is therefore the sum of the average number of nearest neighbors in\neach component, which is Kmin(A′) = KKmin(A).\nAn (M × M)-QAM constellation A′ is equal to the 2-fold Cartesian product A2, where\nA is an M-PAM constellation. Therefore all the above results hold with K = 2. In\nparticular, the QAM constellation has the same d2\nmin as the PAM constellation, but twice\nthe number Kmin of nearest neighbors.\n(b) Show that if the signal constellation is a Cartesian product AK , then MD detection can\nbe performed by performing independent MD detection on each of the K components of\nthe received KN-tuple y = (y1, y2, . . . , yK ). Using this result, sketch the decision regions\nof a (4 × 4)-QAM signal set.\nGiven a received signal (y1, y2, . . . , yK ), to minimize the squared distance ||y1 - a1||2 +\n||y2 - a2||2 + · · · + ||yK - aK ||2 over A′, we may minimize each component ||yj - aj ||2\nseparately, since choice of one component aj imposes no restrictions on the choices of\nother components.\nThe MD decision regions of a (4 × 4)-QAM signal set A′ = A2 are thus simply those\nof a 4-PAM signal set A, independently for each coordinate. Such decision regions are\nsketched in the Problem Set 1 solutions, Problem 1.3(b).\n\n′\n′\n′\n\n(c) Show that if Pr(E) is the probability of error for MD detection of A, then the probability\nof error for MD detection of A′ is\nPr(E)′ = 1 - (1 - Pr(E))K ,\nShow that Pr(E)′ ≈ K Pr(E) if Pr(E) is small.\nA signal in A′ is received correctly if and only if each component is received correctly.\nThe probability of correct decision is therefore the product of the probabilities of correct\ndecision for each of the components separately, which is (1 - Pr(E))K . The probability of\nerror for A′ is therefore Pr(E′) = 1 - (1 - Pr(E))K . When Pr(E) is small, (1 - Pr(E))K ≈\n1 - K Pr(E), so Pr(E′) ≈ K Pr(E).\nProblem 2.2 (Pr(E) invariance to translation, orthogonal transformations, or scaling)\nLet Pr(E | aj ) be the probability of error when a signal aj is selected equiprobably from an\nN-dimensional signal set A and transmitted over a discrete-time AWGN channel, and the\nchannel output Y = aj + N is mapped to a signal ˆaj ∈A by a minimum-distance decision\nrule. An error event E occurs if ˆaj =\naj . Pr(E) denotes the average error probability.\n(a) Show that the probabilities of error Pr(E | aj ) are unchanged if A is translated by any\nvector v; i.e., the constellation A′ = A + v has the same Pr(E) as A.\nIf all signals are equiprobable and the noise is iid Gaussian, then the optimum detector\nis a minimum-distance detector.\nA\na\nIn this case the received sequence Y′ = aj +N may be mapped reversibly to Y = Y′ -v =\nj + N, and then an MD detector for A based on Y is equivalent to an MD detector for\n′ based on Y′ . In particular, it has the same probabilities of error Pr(E | aj ).\n(b) Show that Pr(E) is invariant under orthogonal transformations; i.e., A′ = UA has\nthe same Pr(E) as A when U is any orthogonal N × N matrix (i.e., U -1 = U T ).\na\nIn this case the received sequence Y′ = aj +N may be mapped reversibly to Y = U -1Y′ =\nj + U -1N. Since the noise distribution depends only on the squared norm ||n||2, which is\nnot affected by orthogonal transformations, the noise sequence N′ = U -1N has the same\ndistribution as N, so again the probability of error Pr(E) is unaffected.\n(c) Show that Pr(E) is unchanged if both A and N are scaled by α > 0.\nα\nIn this case the received sequence Y′ = aj + αN may be mapped reversibly to Y =\n-1Y′ = aj + N, which again reduces the model to the original scenario.\nProblem 2.3 (optimality of zero-mean constellations)\nConsider an arbitrary signal set A = {aj , 1 ≤ j ≤ M}. Assume that all signals are\nequiprobable. Let m(A) = M\nj aj be the average signal, and let A′ be A translated by\nm(A) so that the mean of A′ is zero: A′ = A - m(A) = {aj - m(A), 1 ≤ j ≤ M}. Let\nE(A) and E(A′) denote the average energies of A and A′, respectively.\n(a) Show that the error probability of an MD detector is the same for A′ as it is for A.\nThis follows from Problem 2.2(a)\n\n(b) Show that E(A′) = E(A) -||m(A)||2 . Conclude that removing the mean m(A) is\nalways a good idea.\nLet A be the random variable with alphabet A that is equal to aj with probability 1/M\nfor all j, and let A′ = A - m(A) be the fluctuation of A. Then E(A) = ||A||2 , m(A) = A,\nand\nE(A′) = ||A - A||2 = ||A||2 - 2⟨A, A⟩ + ||A||2 = ||A||2 -||A||2 = E(A) -||m(A)||2 .\nIn other words, the second moment of A is greater than or equal to the variance of A,\nwith equality if and only if the mean of A is zero.\nThis result and that of part (a) imply that if m(A)\nA\n= 0, then by replacing A with\n′ = A - m(A), the average energy can be reduced without changing the probability of\nerror. Therefore A′ is always preferable to A; i.e., an optimum constellation must have\nzero mean.\n(c) Show that a binary antipodal signal set A = {±a} is always optimal for M = 2.\nA two-point constellation with zero mean must have a1 + a2 = 0, which implies a2 = -a1.\nProblem 2.4 (Non-equiprobable signals).\nLet aj and aj be two signals that are not equiprobable. Find the optimum (MPE) pairwise\ndecision rule and pairwise error probability Pr{aj → aj }.\nThe MPE rule is equivalent to the maximum-a-posteriori-probability (MAP) rule: choose\na ∈A such that p(ˆ\nthe ˆ\na | y) is maximum among all p(aj | y), aj ∈A. By Bayes' law,\np(aj | y) = p(y | aj )p(aj ) .\np(y)\nThe pairwise decision rule is thus to choose aj over aj if p(y | aj )p(aj ) > p(y | aj )p(aj ),\nor vice versa. Using the logarithm of the noise pdf, we can write this as\n-||y - aj ||2\n+ log p(aj ) > -||y - aj ||2\n+ log p(aj ),\n2σ2\n2σ2\nor equivalently\n||y - aj ||2 < ||y - aj ||2 + K,\nwhere K = 2σ2 log p(aj )/p(aj ). Therefore the pairwise MAP rule is equivalent to a\nminimum-squared-distance rule with a bias K.\nFollowing the development shown in (5.2), we have\n||y - aj ||2 -\n||y - aj ||2 =\n-2⟨y, aj ⟩ + ||aj ||2 - (-2⟨y, aj ⟩ + ||aj ||2) =\n2⟨y, aj - aj ⟩\n-\n2⟨m, aj - aj ⟩,\nwhere m denotes the midvector m = (aj + aj )/2.\n\nTherefore ||y - aj ||2 -||y - aj ||2 < K if and only if 2⟨y, aj - aj ⟩- 2⟨m, aj - aj ⟩ < K.\nSince the magnitudes of the projections y|aj -aj and m|aj -aj of y and m onto the difference\nvector aj - aj are\n|y|aj -aj | = ⟨y, aj - aj ⟩ ;\n|m|aj -aj | = ⟨m, aj - aj ⟩ ,\n||aj - aj ||\n||aj - aj ||\nwe have ||y - aj ||2 -||y - aj ||2 < K if and only if\nK\nσ2λ(aj , aj )\n|y|aj -aj | < |m|aj -aj | + 2||aj - aj || = |m|aj -aj | +\n,\nd(aj , aj )\nwhere λ(aj , aj ) is the log likelihood ratio log p(aj )/p(aj ).\nThe conclusion is that the decision boundary is still a hyperplane perpendicular to the\ndifference vector aj - aj , but shifted by σ2λ(aj , aj )/d(aj , aj ).\nThe probability of error is thus the probability that a one-dimensional Gaussian variable\nof zero mean and variance σ2 will exceed d(aj , aj )/2+σ2λ(aj , aj )/d(aj , aj ). This is given\nas always by the Q function\n\nσ2λ(aj , aj )\nPr{aj → aj } = Q\nd(aj , aj ) +\n.\n2σ\nd(aj , aj )\nProblem 2.5 (UBE for M -PAM constellations).\nFor an M -PAM constellation A, show that Kmin(A) = 2(M - 1)/M . Conclude that the\nunion bound estimate of Pr(E) is\nM - 1\nd\nPr(E) ≈ 2\nQ\n.\nM\n2σ\nObserve that in this case the union bound estimate is exact. Explain why.\nThe M - 2 interior points have 2 nearest neighbors, while the 2 boundary points have 1\nnearest neighbor, so the average number of nearest neighbors is\n2M - 2\nKmin(A) =\n((M - 2)(2) + (2)(1)) =\n.\nM\nM\nd\nIn general the union bound estimate is Pr(E) ≈ Kmin(A)Q(dmin(A)/2σ). Plugging in\nmin(A) = d and the above expression for Kmin(A), we get the desired expression.\nFor the M - 2 interior points, the exact error probability is 2Q(d/2σ). For the 2 boundary\npoints, the exact error probability is Q(d/2σ). If all points are equiprobable, then the\naverage Pr(E) is exactly the UBE given above.\nIn general, we can see that the union bound is exact for one-dimensional constellations,\nand only for one-dimensional constellations. The union bound estimate is therefore exact\nonly for equi-spaced one-dimensional constellations; i.e., essentially only for M -PAM."
    },
    {
      "category": "Resource",
      "title": "ps3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/8543b757a3f41f99a5482d8d4a59789c_ps3.pdf",
      "content": "′\n6.451 Principles of Digital Communication II\nWednesday, February 16, 2005\nMIT, Spring 2005\nHandout #7\nDue: Wednesday, February 23, 2005\nProblem Set 3\nProblem 3.1 (Invariance of coding gain)\n(a) Show that in the power-limited regime the nominal coding gain γc(A) of (5.9), the UBE\n(5.10) of Pb(E), and the effective coding gain γeff (A) are invariant to scaling, orthogonal\ntransformations and Cartesian products.\n(b) Show that in the bandwidth-limited regime the nominal coding gain γc(A) of (5.14),\nthe UBE (5.15) of Ps(E), and the effective coding gain γeff (A) are invariant to scaling,\northogonal transformations and Cartesian products.\nProblem 3.2 (Orthogonal signal sets)\nAn orthogonal signal set is a set A = {aj, 1 ≤j ≤M } of M orthogonal vectors in RM\nwith equal energy E(A); i.e., ⟨aj, aj ⟩= E(A)δjj (Kronecker delta).\n(a) Compute the nominal spectral efficiency ρ of A in bits per two dimensions. Compute\nthe average energy Eb per information bit.\n(b) Compute the minimum squared distance d2\nShow that every signal has\nmin(A).\nKmin(A) = M -1 nearest neighbors.\n(c) Let the noise variance be σ2 = N0/2 per dimension. Show that the probability of error\nof an optimum detector is bounded by the UBE\n√\nPr(E) ≤(M -1)Q (E(A)/N0).\n(d) Let M →infwith Eb held constant. Using an asymptotically accurate upper bound\n√\nfor the Q (·) function (see Appendix), show that Pr(E) →0 provided that Eb/N0 > 2 ln 2\n(1.42 dB). How close is this to the ultimate Shannon limit on Eb/N0? What is the nominal\nspectral efficiency ρ in the limit?\nProblem 3.3 (Simplex signal sets)\nLet A be an orthogonal signal set as above.\n(a) Denote the mean of A by m(A). Show that m(A) = 0, and compute ||m(A)||2 .\nThe zero-mean set A′ = A -m(A) (as in Exercise 2) is called a simplex signal set. It\nis universally believed to be the optimum set of M signals in AWGN in the absence of\nbandwidth constraints, except at ridiculously low SNRs.\n(b) For M = 2, 3, 4, sketch A and A′ .\n(c) Show that all signals in A′ have the same energy E(A′). Compute E(A ). Compute\nthe inner products ⟨aj, aj ⟩for all aj, aj ∈A′ .\n\n(d) [Optional]. Show that for ridiculously low SNR, a signal set consisting of M - 2\nzero signals and two antipodal signals {±a} has a lower Pr(E) than a simplex signal set.\n[Hint: see M. Steiner, \"The strong simplex conjecture is false,\" IEEE Transactions\non Information Theory, pp. 721-731, May 1994.]\nProblem 3.4 (Biorthogonal signal sets)\nThe set A′′ = ±A of size 2M consisting of the M signals in an orthogonal signal set A\nwith symbol energy E(A) and their negatives is called a biorthogonal signal set.\n(a) Show that the mean of A′′ is m(A′′) = 0, and that the average energy is E(A).\n(b) How much greater is the nominal spectral efficiency ρ of A′′ than that of A?\n(c) Show that the probability of error of A′′ is approximately the same as that of an\northogonal signal set with the same size and average energy, for M large.\n(d) Let the number of signals be a power of 2: 2M = 2k . Show that the nominal spectral\nefficiency is ρ(A′′) = 4k2-k b/2D, and that the nominal coding gain is γc(A′′) = k/2.\nShow that the number of nearest neighbors is Kmin(A′′) = 2k - 2.\nProblem 3.5 (small nonbinary constellations)\nA\n(a) For M = 4, the (2 × 2)-QAM signal set is known to be optimal in N = 2 dimensions.\nShow however that there exists at least one other inequivalent two-dimensional signal set\n′ with the same coding gain. Which signal set has the lower \"error coefficient\" Kmin(A)?\n(b) Show that the coding gain of (a) can be improved in N = 3 dimensions. [Hint:\nconsider the signal set A′′ = {(1, 1, 1), (1, -1, -1), (-1, 1, -1), (-1, -1, 1)}.] Sketch A′′ .\nWhat is the geometric name of the polytope whose vertex set is A′′?\n(c) Give an accurate plot of the UBE of the Pr(E) for the signal set A′′ of (b). How much\nis the effective coding gain, by our rule of thumb and by this plot?\n(d) For M = 8 and N = 2, propose at least two good signal sets, and determine which\none is better. [Open research problem: Find the optimal such signal set, and prove that\nit is optimal.]\nProblem 3.6 (Even-weight codes have better coding gain)\nLet C be an (n, k, d) binary linear code with d odd. Show that if we append an overall\nparity check p =\ni xi to each codeword x, then we obtain an (n + 1, k, d + 1) binary\nlinear code C′ with d even. Show that the nominal coding gain γc(C′) is always greater\nthan γc(C) if k > 1. Conclude that we can focus primarily on linear codes with d even."
    },
    {
      "category": "Resource",
      "title": "ps3solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/ecc464e775c2b58fc826ec48069692b8_ps3solns.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, February 23, 2005\nMIT, Spring 2005\nHandout #8\nProblem Set 3 Solutions\nProblem 3.1 (Invariance of coding gain)\n(a) Show that in the power-limited regime the nominal coding gain γc(A) of (5.9), the UBE\n(5.10) of Pb(E), and the effective coding gain γeff (A) are invariant to scaling, orthogonal\ntransformations and Cartesian products.\nIn the power-limited regime, the nominal coding gain is defined as\ndmin(A)\nγc(A) =\n.\n4Eb(A)\nScaling A by α > 0 multiplies both dmin(A) and Eb(A) by α2, and therefore leaves γc(A)\nunchanged. Orthogonal transformations of A do not change either dmin(A) or Eb(A).\nAs we have seen in Problem 2.1, taking Cartesian products also does not change either\ndmin(A) or Eb(A). Therefore γc(A) is invariant under all these operations.\nThe UBE of Pb(E) involves γc(A) and Kb(A) = Kmin(A)/(| log |A|). Kmin(A) is also\nobviously unchanged under scaling or orthogonal transformations. Problem 2.1 showed\nthat Kmin(A) increases by a factor of K under a K-fold Cartesian product, but so does\nlog |A|, so Kb(A) is also unchanged under Cartesian products.\nThe effective coding gain is a function of the UBE of Pb(E), and therefore it is invariant\nalso.\n(b) Show that in the bandwidth-limited regime the nominal coding gain γc(A) of (5.14),\nthe UBE (5.15) of Ps(E), and the effective coding gain γeff (A) are invariant to scaling,\northogonal transformations and Cartesian products.\nIn the bandwidth-limited regime, the nominal coding gain is defined as\n(2ρ(A) - 1)dmin(A)\nγc(A) =\n.\n6Es(A)\nScaling A by α > 0 multiplies both dmin(A) and Es(A) by α2 and does not change ρ(A),\nand therefore leaves γc(A) unchanged. Orthogonal transformations of A do not change\ndmin(A), Eb(A) or ρ(A). As we have seen in Problem 2.1, taking Cartesian products also\ndoes not change d2\nb(A) or ρ(A). Therefore γc(A) is invariant under all these\nmin(A), E\noperations.\nThe UBE of Ps(E) involves γc(A) and Ks(A) = (2/N)Kmin(A). Kmin(A) is also obvi\nously unchanged under scaling or orthogonal transformations. Problem 2.1 showed that\nKmin(A) increases by a factor of K under a K-fold Cartesian product, but so does N, so\nKs(A) is also unchanged under Cartesian products.\nThe effective coding gain is a function of the UBE of Ps(E), and therefore it is invariant\nalso.\n\nProblem 3.2 (Orthogonal signal sets)\nAn orthogonal signal set is a set A = {aj, 1 ≤ j ≤ M } of M orthogonal vectors in RM\nwith equal energy E(A); i.e., ⟨aj, aj ⟩ = E(A)δjj (Kronecker delta).\n(a) Compute the nominal spectral efficiency ρ of A in bits per two dimensions. Compute\nthe average energy Eb per information bit.\nThe rate of A is log2 M bits per M dimensions, so the nominal spectral efficiency is\nρ = (2/M ) log2 M bits per two dimensions.\nThe average energy per symbol is E(A), so the average energy per bit is\nE(A)\nEb =\n.\nlog2 M\n(b) Compute the minimum squared distance d2\nShow that every signal has\nmin(A).\nKmin(A) = M - 1 nearest neighbors.\nThe squared distance between any two distinct vectors is\n||aj - aj ||2 = ||aj||2 - 2⟨aj, aj ⟩ + ||aj ||2 = E(A) - 0 + E(A) = 2E(A),\nso d2\nK\nmin(A) = 2E(A), and every vector has all other vectors as nearest neighbors, so\nmin(A) = M - 1.\n(c) Let the noise variance be σ2 = N0/2 per dimension. Show that the probability of error\nof an optimum detector is bounded by the UBE\n√\nPr(E) ≤ (M - 1)Q (E(A)/N0).\nThe pairwise error probability between any two distinct vectors is\n√\n√\n√\nPr{aj → aj } = Q (||aj - aj ||2/4σ2) = Q (2E(A)/2N0) = Q (E(A)/N0).\nBy the union bound, for any aj ∈A,\nPr(E | aj) ≤\n\nPr{aj → aj } = (M - 1)Q\n√ (E(A)/N0),\nj=j\nso the average Pr(E) also satisfies this upper bound.\n(d) Let M →inf with Eb held constant. Using an asymptotically accurate upper bound\n√\nfor the Q (·) function (see Appendix), show that Pr(E) → 0 provided that Eb/N0 > 2 ln 2\n(1.42 dB). How close is this to the ultimate Shannon limit on Eb/N0? What is the nominal\nspectral efficiency ρ in the limit?\n√\n-x2/2\nBy the Chernoff bound of the Appendix, Q (x2) ≤ e\n. Therefore\nPr(E) ≤ (M - 1)e -E(A)/2N0 < e(ln M)e -(Eb log2 M)/2N0 .\n\n′\nSince ln M = (log2 M )(ln 2), as M →inf this bound goes to zero provided that\nEb/2N0 > ln 2,\nor equivalently Eb/N0 > 2 ln 2 (1.42 dB).\nThe ultimate Shannon limit on Eb/N0 is Eb/N0 > ln 2 (-1.59 dB), so this shows that\nwe can get to within 3 dB of the ultimate Shannon limit with orthogonal signalling. (It\nwas shown in 6.450 that orthogonal signalling can actually achieve Pr(E) → 0 for any\nEb/N0 > ln 2, the ultimate Shannon limit.)\nUnfortunately, the nominal spectral efficiency ρ = (2 log2 M )/M goes to 0 as M →inf.\nProblem 3.3 (Simplex signal sets)\nLet A be an orthogonal signal set as above.\n(a) Denote the mean of A by m(A). Show that m(A) = 0, and compute ||m(A)||2 .\nBy definition,\nm(A) = M\n\nj\naj .\nTherefore, using orthogonality, we have\n||m(A)||2 =\nE(A)\n\nj\n||aj ||2\n= 0.\n=\nM 2\nM\n= 0 implies that m(A)\nBy the strict non-negativity of the Euclidean norm, ||m(A)||2\n= 0.\nThe zero-mean set A′ = A - m(A) (as in Exercise 2) is called a simplex signal set. It\nis universally believed to be the optimum set of M signals in AWGN in the absence of\nbandwidth constraints, except at ridiculously low SNRs.\n(b) For M = 2, 3, 4, sketch A and A′ .\nFor M = 2, 3, 4, A consists of M orthogonal vectors in M -space (hard to sketch for\nM = 4). For M = 2, A′ consists of two antipodal signals in a 1-dimensional subspace\nof 2-space; for M = 3, A′ consists of three vertices of an equilateral triangle in a 2\ndimensional subspace of 3-space; and for M = 4, A′ consists of four vertices of a regular\ntetrahedron in a 3-dimensional subspace of 4-space.\n(c) Show that all signals in A′ have the same energy E(A′). Compute E(A′). Compute\nthe inner products ⟨aj , aj ⟩ for all aj , aj ∈A′ .\nThe inner product of m(A) with any aj is\n⟨aj , aj ⟩ = EA\nM .\n\nj\n⟨m(A), aj ⟩ = M\nThe energy of aj\n′ = aj - m(A) is therefore\nM - 1\n||aj ||2 = ||aj ||2 - 2⟨m(A), aj ⟩ + ||m(A)||2 = E(A) - E(A) =\nE(A).\nM\nM\n\n′\n= j′, the inner product ⟨aj , aj ⟩ is\nFor j\n′\n′\nE(A)\nE(A)\nE(A)\n⟨aj\n′ , aj ⟩ = ⟨aj - m(A), aj - m(A)⟩ = 0 - 2\n+\n= -\n.\nM\nM\nM\nIn other words, the inner product is equal to M -1 E(A) if j′ = j and - 1 E(A) for j′ = j.\nM\nM\n(d) [Optional]. Show that for ridiculously low SNR, a signal set consisting of M - 2\nzero signals and two antipodal signals {±a} has a lower Pr(E) than a simplex signal set.\n[Hint: see M. Steiner, \"The strong simplex conjecture is false,\" IEEE Transactions\non Information Theory, pp. 721-731, May 1994.]\nSee the cited article.\nProblem 3.4 (Biorthogonal signal sets)\nThe set A′′ = ±A of size 2M consisting of the M signals in an orthogonal signal set A\nwith symbol energy E(A) and their negatives is called a biorthogonal signal set.\n(a) Show that the mean of A′′ is m(A′′) = 0, and that the average energy is E(A).\nThe mean is\nm(A′′) =\n\n(aj - aj ) = 0,\nj\nand every vector has energy E(A).\n(b) How much greater is the nominal spectral efficiency ρ of A′′ than that of A?\nThe rate of A′′ is log2 2M = 1 + log2 M bits per M dimensions, so its nominal spectral\nefficiency is ρ = (2/M )(1 + log2 M ) b/2D, which is 2/M b/2D greater than for A. This\nis helpful for small M , but negligible as M →inf.\n(c) Show that the probability of error of A′′ is approximately the same as that of an\northogonal signal set with the same size and average energy, for M large.\nEach vector in A′′ has 2M - 2 nearest neighbors at squared distance 2E(A), and one\nantipodal vector at squared distance 4E(A). The union bound estimate is therefore\n√\n√\nPr(E) ≈ (2M - 2)Q (E(A)/N0) ≈|A′′|Q (E(A)/N0),\n√\nwhich is approximately the same as the estimate Pr(E) ≈ (2M - 1)Q (E(A)/N0) ≈\n√\n|A|Q (E(A)/N0) for an orthogonal signal set A of size |A| = 2M .\n(d) Let the number of signals be a power of 2: 2M = 2k . Show that the nominal spectral\nefficiency is ρ(A′′) = 4k2-k b/2D, and that the nominal coding gain is γc(A′′) = k/2.\nShow that the number of nearest neighbors is Kmin(A′′) = 2k - 2.\nIf M = 2k-1, then the nominal spectral efficiency is\nρ(A′′) = (2/M )(1 + log2 M ) = 22-k k = 4k2-k b/2D.\nd\nWe are in the power-limited regime, so the nominal coding gain is\n2E(A′′)\nk\nγc(A′′\nmin(A′′)\n) =\n= 4E(A′′)/k = .\n4Eb\nThe number of nearest neighbors is Kmin(A′′) = 2M - 2 = 2k - 2.\n\nProblem 3.5 (small nonbinary constellations)\nA\n(a) For M = 4, the (2 × 2)-QAM signal set is known to be optimal in N = 2 dimensions.\nShow however that there exists at least one other inequivalent two-dimensional signal set\n′ with the same coding gain. Which signal set has the lower \"error coefficient\" Kmin(A)?\nThe 4-QAM signal set A with points {(±α, ±α)} has b = 2, d2\nmin(A) = 4α2 and E(A) =\n2α2, so A has Eb = E(A)/2 = α2 and γc(A) = d2\nmin(A)/4Eb = 1.\n√\n√\nThe 4-point hexagonal signal set A′ with points at {(0, 0), (α, 3α), (2α, 0), (3α, 3α)}\nA\n√\nhas mean m = (3α/2, 3α)/2) and average energy E(A′) = 5α2 . If we translate A′ to\n′′ = A′ - m to remove the mean, then E(A′′) = E(A′) -||m||2 = 5α2 - 3α2 = 2α2 .\nThus A′′ has the same minimum squared distance, the same average energy, and thus the\nsame coding gain as A.\nIn A, each point has two nearest neighbors, so Kmin(A) = 2. In A′, two points have\ntwo nearest neighbors and two points have three nearest neighbors, so Kmin(A′) = 2.5.\n(This factor of 1.25 difference in error coefficient will cost about (1/4) · (0.2) = 0.05 dB\nin effective coding gain, by our rule of thumb.)\n[Actually, all parallelogram signal sets with sides of length 2α and angles between 60* and\n90* have minimum squared distance 4α2 and average energy 2α2, if the mean is removed.]\n(b) Show that the coding gain of (a) can be improved in N = 3 dimensions. [Hint: consider\nthe signal set A′′ = {(1, 1, 1), (1, -1, -1), (-1, 1, -1), (-1, -1, 1)}.] Sketch A′′ . What is\nthe geometric name of the polytope whose vertex set is A′′?\nThe four signal points in A′′ are the vertices of a tetrahedron (see Chapter 6, Figure 1).\nThe minimum squared distance between points in A′′ is 2 · 4 = 8, and the average energy\nis E(A′′) = 3, so Eb = 3/2. Thus the coding gain of A′′ is γc(A′′) = d2\nmin(A′′)/4Eb = 4/3,\na factor of 4/3 (1.25 dB) better than that of A.\nHowever, the nominal spectral efficiency ρ of A′′ is only 4/3 b/2D, compared to ρ = 2\nb/2D for A; i.e., A′′ is less bandwidth-efficient. Also, each point in A′′ has Kmin(A′′) = 3\nnearest neighbors, which costs about 0.1 dB in effective coding gain.\n(c) Give an accurate plot of the UBE of the Pr(E) for the signal set A′′ of (b). How much\nis the effective coding gain, by our rule of thumb and by this plot?\nThe UBE for Pr(E) is\n√\n√\nPr(E) ≈ Kmin(A′′)Q (2γc(A′′)Eb/N0) = 3Q (24 Eb/N0).\n√\nSince each signal sends 2 bits, the UBE for Pb(E) is 1\n2 Pr(E): Pb(E) ≈ 1.5Q (2 4 Eb/N0).\nAn accurate plot of the UBE may be obtained by moving the baseline curve Pb(E) ≈\n√\nQ (2Eb/N0) to the left by 1.25 dB and up by a factor of 1.5, as shown in Figure 1. This\nshows that the effective coding gain is about γeff (A′′) ≈ 1.15 dB at Pb(E) ≈ 10-5. Our\n√\nrule of thumb gives approximately the same result, since 1.5 is equal to about\n2.\n\nSimplex signal sets\nPb(E)\n-1\n-2\n-3\n-4\n-5\n-6\n-2\n-1\nEb/N0 [dB]\nUncoded 2-PAM\nsimplex M=4\nFigure 1. Pb(E) vs. Eb/N0 for tetrahedron (4-simplex) signal set.\n(d) For M = 8 and N = 2, propose at least two good signal sets, and determine which\none is better. [Open research problem: Find the optimal such signal set, and prove that it\nis optimal.]\nPossible 8-point 2-dimensional signal sets include:\n(i) 8-PSK. If the radius of each signal point is r, then the minimum distance is dmin =\n2r sin 22.5*, so to achieve dmin = 2 requires r = 1/(sin 22.5*) = 2.613, or an energy of\n6.828 (8.34 dB).\n(ii) An 8-point version of the V.29 signal set, with four points of type (1, 1) and four\npoints of type (3, 0). The average energy is then 5.5 (7.40 dB), about 1 dB better than\n8-PSK. Even better, the minimum distance can be maintained at dmin = 2 if the outer\n√\npoints are moved in to (1 + 3, 0), which reduces the average energy to 4.732 (6.75 dB).\n(iii) Hexagonal signal sets. One hexagonal 8-point set with dmin = 2 has 1 point at\nthe origin, 6 at squared radius 4, and 1 at squared radius 12, for an average energy of\n√\n36/8 = 4.5 (6.53 dB). The mean m has length\n12/8, so removing the mean reduces the\nenergy further by 3/16 = 0.1875 to 4.3125 (6.35 dB).\nAnother more symmetrical hexagonal signal set (the \"double diamond\") has points at\n√\n√\n(±1, 0), (0, ± 3) and (±2, ± 3). This signal set also has average energy 36/8 = 4.5\n(6.53 dB), and zero mean.\n\n′\n′\n′\n′\n′\n′\n′\nProblem 3.6 (Even-weight codes have better coding gain)\nLet C be an (n, k, d) binary linear code with d odd. Show that if we append an overall\nparity check p =\ni xi to each codeword x, then we obtain an (n + 1, k, d + 1) binary\nlinear code C′ with d even. Show that the nominal coding gain γc(C′) is always greater\nthan γc(C) if k > 1. Conclude that we can focus primarily on linear codes with d even.\nThe new code C′ has the group property, because the mod-2 sum of two codewords\n(x1, . . . , xn, p =\ni xi) and (x1, . . . , xn, p =\ni xi) is\n(x1 + x1, . . . , xn + xn, p + p ′ =\n\nxi + xi),\ni\nanother codeword in C′ . Its length is n′ = n+ 1, and it has the same number of codewords\n(dimension). Since the parity bit p is equal to 1 for all odd-weight codewords in C, the\nweight of all odd-weight codewords is increased by 1, so the minimum nonzero weight\nbecomes d′ = d + 1. We conclude that C′ is a binary linear (n + 1, k, d + 1) block code.\nto (d+1)k\nThe nominal coding gain thus goes from dk\n. Since\nn\nn+1\nd\nn\n<\nd + 1\nn + 1\nif d < n, the nominal coding gain strictly increases unless d = n-- i.e., unless C is a\nrepetition code with k = 1-- in which case it stays the same (namely 1 (0 dB))."
    },
    {
      "category": "Resource",
      "title": "ps4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/d0b83ebb98447726c9f56f03adc3c0c5_ps4.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, February 23, 2005\nMIT, Spring 2005\nHandout #9\nDue: Wednesday, March 2, 2005\nProblem Set 4\nProblem 4.1\nShow that if C is a binary linear block code, then in every coordinate position either\nall codeword components are 0 or half are 0 and half are 1. Show that a coordinate in\nwhich all codeword components are 0 may be deleted (\"punctured\") without any loss in\nperformance, but with savings in energy and in dimension. Show that if C has no such\nall-zero coordinates, then s(C) has zero mean: m(s(C)) = 0.\nProblem 4.2 (RM code parameters)\nCompute the parameters (k, d) of the RM codes of lengths n = 64 and n = 128.\nProblem 4.3 (optimizing SPC and EH codes)\n(a) Using the rule of thumb that a factor of two increase in Kb costs 0.2 dB in effective\ncoding gain, find the value of n for which an (n, n- 1, 2) SPC code has maximum effective\ncoding gain, and compute this maximum in dB.\n(b) Similarly, find the m such that the (2m , 2m - m - 1, 4) extended Hamming code has\nmaximum effective coding gain, using\n2m(2m - 1)(2m - 2)\nN4 =\n,\nand compute this maximum in dB.\nProblem 4.4 (biorthogonal codes)\nWe have shown that the first-order Reed-Muller codes RM(1, m) have parameters\n(2m, m + 1, 2m-1), and that the (2m , 1, 2m) repetition code RM(0, m) is a subcode.\n(a) Show that RM(1, m) has one word of weight 0, one word of weight 2m, and 2m+1 - 2\nwords of weight 2m-1 . [Hint: first show that the RM(1, m) code consists of 2m comple\nmentary codeword pairs {x, x + 1}.]\n(b) Show that the Euclidean image of an RM(1, m) code is an M = 2m+1 biorthogonal\nsignal set. [Hint: compute all inner products between code vectors.]\n(c) Show that the code C′ consisting of all words in RM(1, m) with a 0 in any given\ncoordinate position is a (2m, m, 2m-1) binary linear code, and that its Euclidean image is\nan M = 2m orthogonal signal set. [Same hint as in part (a).]\n(d) Show that the code C′′ consisting of the code words of C′ with the given coordinate\ndeleted (\"punctured\") is a binary linear (2m - 1, m, 2m-1) code, and that its Euclidean\nimage is an M = 2m simplex signal set. [Hint: use Exercise 7 of Chapter 5.]\n\nU\nProblem 4.5 (generator matrices for RM codes)\nLet square 2m × 2m matrices Um, m ≥1, be specified recursively as follows. The matrix\n1 is the 2 × 2 matrix\nU1 =\n\n.\nThe matrix Um is the 2m × 2m matrix\nUm =\nUm-1\nUm-1\nUm-1\n\n.\n(In other words, Um is the m-fold tensor product of U1 with itself.)\n(a) Show that RM(r, m) is generated by the rows of Um of Hamming weight 2m-r or\ngreater. [Hint: observe that this holds for m = 1, and prove by recursion using the\n|u|u + v| construction.] For example, give a generator matrix for the (8, 4, 4) RM code.\nm\n(b) Show that the number of rows of Um of weight 2m-r is\nr . [Hint: use the fact that\nm is the coefficient of zm-r in the integer polynomial (1 + z)m.]\nr\nm\n(c) Conclude that the dimension of RM(r, m) is k(r, m) =\n0≤j≤r\nj .\nProblem 4.6 (\"Wagner decoding\")\nLet C be an (n, n -1, 2) SPC code. The Wagner decoding rule is as follows. Make hard\ndecisions on every symbol rk, and check whether the resulting binary word is in C. If so,\naccept it. If not, change the hard decision in the symbol rk for which the reliability metric\n|rk | is minimum. Show that the Wagner decoding rule is an optimum decoding rule for\nSPC codes. [Hint: show that the Wagner rule finds the codeword x ∈C that maximizes\nr(x | r).]\nProblem 4.7 (small cyclic groups).\nWrite down the addition tables for Z2, Z3 and Z4. Verify that each group element appears\nprecisely once in each row and column of each table.\nProblem 4.8 (subgroups of cyclic groups are cyclic).\nShow that every subgroup of Zn is cyclic. [Hint: Let s be the smallest nonzero element\nin a subgroup S ⊆Zn, and compare S to the subgroup generated by s.]"
    },
    {
      "category": "Resource",
      "title": "ps4solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/1e763fac8a2a9777de7e6c22973a2457_ps4solns.pdf",
      "content": "′\n′\n\n6.451 Principles of Digital Communication II\nWednesday, March 2, 2005\nMIT, Spring 2005\nHandout #10\nProblem Set 4 Solutions\nProblem 4.1\nShow that if C is a binary linear block code, then in every coordinate position either all\ncodeword components are 0 or half are 0 and half are 1.\nC is linear if and only if C is a group under vector addition. The subset C′ ⊆C of\ncodewords with 0 in a given coordinate position is then clearly a (sub)group, as it is\nclosed under vector addition. If there exists any codeword c ∈C with a 1 in the given\ncoordinate position, then the (co)set C′ + c is a subset of C of size |C + c| = |C| consisting\nof the codewords with a 1 in the given coordinate position (all are codewords by the group\nproperty, and every codeword c′ with a 1 in the given position is in C + c, since c′ + c is in\nC′). On the other hand, if there exists no codeword c ∈C with a 1 in the given position,\nthen C′ = C. We conclude that either half or none of the codewords in C have a 1 in the\ngiven coordinate position.\nShow that a coordinate in which all codeword components are 0 may be deleted (\"punc\ntured\") without any loss in performance, but with savings in energy and in dimension.\nIf all codewords have a 0 in a given position, then this position does not contribute\nto distinguishing between any pair of codewords; i.e., it can be ignored in decoding\nwithout loss of performance. On the other hand, this symbol costs energy α2 to transmit,\nand sending this symbol reduces the code rate (nominal spectral efficiency). Thus for\ncommunications purposes, this symbol has a cost without any corresponding benefit, so\nit should be deleted.\nShow that if C has no such all-zero coordinates, then s(C) has zero mean: m(s(C)) = 0.\nBy the first part, if C has no all-zero coordinates, then in each position C haas half 0s and\nhalf 1s, so s(C) has zero mean in each coordinate position.\nProblem 4.2 (RM code parameters)\nCompute the parameters (k, d) of the RM codes of lengths n = 64 and n = 128.\nUsing\n\nm\nk(r, m) =\nj\n0≤j≤r\nor\nk(r, m) = k(r, m -1) + k(r -1, m -1),\nthe parameters for the n = 64 RM codes are\n(64, 64, 1); (64, 63, 2); (64, 57, 4); (64, 42, 8); (64, 22, 16); (64, 7, 32), (64, 1, 64); (64, 0, inf).\n\nSimilarly, the parameters for the nontrivial n = 128 RM codes are\n(128, 127, 2); (128, 120, 4); (128, 99, 8); (128, 64, 16); (128, 29, 32); (128, 8, 64); (128, 1, 128).\nProblem 4.3 (optimizing SPC and EH codes)\n(a) Using the rule of thumb that a factor of two increase in Kb costs 0.2 dB in effective\ncoding gain, find the value of n for which an (n, n - 1, 2) SPC code has maximum effective\ncoding gain, and compute this maximum in dB.\nK\nThe nominal coding gain of an (n, n - 1, 2) SPC code is γc = 2(n - 1)/n, and the number\nof nearest neighbors is N2 = n(n - 1)/2, so the number of nearest neighbors per bit is\nb = n/2. The effective coding gain in dB is therefore approximately\nγeff =\n\nlog10 2(n - 1)/n - (0.2) log2 n/2\n=\n10(log10 e) ln 2(n - 1)/n - (0.2)(log2 e) ln n/2.\nDifferentiating with respect to n, we find that the maximum occurs when\n10(log10 e)\n-\n- (0.2)(log2 e)\n= 0,\nn - 1\nn\nn\nwhich yields\n10 log10 e\nn - 1 =\n≈ 15.\n(0.2) log2 e\nThus the maximum occurs for n = 16, where\nγeff ≈ 2.73 - 0.6 = 2.13 dB.\n(b) Similarly, find the m such that the (2m , 2m - m - 1, 4) extended Hamming code has\nmaximum effective coding gain, using\n2m(2m - 1)(2m - 2)\nN4 =\n,\nand compute this maximum in dB.\nγ\nSimilarly, the nominal coding gain of a (2m , 2m - m - 1, 4) extended Hamming code is\nc = 4(2m -m-1)/2m, and the number of nearest neighbors is N4 = 2m(2m -1)(2m -2)/24,\nso the number of nearest neighbors per bit is Kb = 2m(2m - 1)(2m - 2)/24(2m - m - 1).\nComputing effective coding gains, we find\nγeff (8, 4, 4) = 2.6 dB;\nγeff (16, 11, 4) = 3.7 dB;\nγeff (32, 26, 4) = 4.0 dB;\nγ\nγeff (64, 57, 4) = 4.0 dB;\neff (128, 120, 4) = 3.8 dB,\nwhich shows that the maximum occurs for 2m = 32 or 64 and is about 4.0 dB.\n\n′′\nProblem 4.4 (biorthogonal codes)\nWe have shown that the first-order Reed-Muller codes RM(1, m) have parameters\n(2m, m + 1, 2m-1), and that the (2m , 1, 2m) repetition code RM(0, m) is a subcode.\n(a) Show that RM(1, m) has one word of weight 0, one word of weight 2m, and 2m+1 - 2\nwords of weight 2m-1 . [Hint: first show that the RM(1, m) code consists of 2m comple\nmentary codeword pairs {x, x + 1}.]\nSince the RM(1, m) code contains the all-one word 1, by the group property it contains\nthe complement of every codeword. The complement of the all-zero word 0, which has\nweight 0, is the all-one word 1, which has weight 2m . In general, the complement of a\nweight-w word has weight 2m - w. Thus if the minimum weight of any nonzero word is\n2m-1, then all other codewords must have weight exactly 2m-1 .\n(b) Show that the Euclidean image of an RM(1, m) code is an M = 2m+1 biorthogonal\nsignal set. [Hint: compute all inner products between code vectors.]\nThe inner product between the Euclidean images s(x), s(y) of two binary n-tuples x, y is\n⟨s(x), s(y)⟩ = (n - 2dH (x, y))α2 .\nThus x and y are orthogonal when dH (x, y) = n/2 = 2m-1 . It follows that every codeword\nx in RM(1, m) is orthogonal to every other word, except x + 1, to which it is antipodal.\nThus the Euclidean image of RM(1, m) is a biorthogonal signal set.\n(c) Show that the code C′ consisting of all words in RM(1, m) with a 0 in any given\ncoordinate position is a (2m, m, 2m-1) binary linear code, and that its Euclidean image is\nan M = 2m orthogonal signal set. [Same hint as in part (a).]\nBy the group property, exactly half the words have a 0 in any coordinate position. More\nover, this set of words C′ evidently has the group property, since the sum of any two\ncodewords in RM(1, m) that have a 0 in a certain position is a codeword in RM(1, m)\nthat has a 0 in that position. These words include the all-zero word but not the all-\none word. The nonzero words in C′ thus all have weight 2m-1 . Thus any two distinct\nEuclidean images s(x) are orthogonal. Therefore s(C′) is an orthogonal signal set with\nM = 2m signals.\n(d) Show that the code C′′ consisting of the code words of C′ with the given coordinate\ndeleted (\"punctured\") is a binary linear (2m - 1, m, 2m-1) code, and that its Euclidean\nimage is an M = 2m simplex signal set. [Hint: use Exercise 7 of Chapter 5.]\nC\nis the same code as C′ , except with one less bit. Since the deleted bit is always a\nzero, deleting this coordinate does not affect the weight of any word. Thus C′′ is a binary\nlinear (2m - 1, m, 2m-1) code in which every nonzero word has Hamming weight 2m-1 .\nConsequently the inner product of the Euclidean images of any two distinct codewords is\n⟨s(x), s(y)⟩ = (n - 2dH (x, y))α2 = -α2 = - E(A) ,\n2m - 1\nwhere E(A) = (2m - 1)α2 is the energy of each codeword. This is the set of inner products\nof an M = 2m simplex signal set of energy E(A), so s(C′′) is geometrically equivalent to\na simplex signal set.\n\n′\n′\n′\nProblem 4.5 (generator matrices for RM codes)\nU\nLet square 2m × 2m matrices Um, m ≥ 1, be specified recursively as follows. The matrix\n1 is the 2 × 2 matrix\n\nU1 =\n.\nThe matrix U\nis the 2m × 2m matrix\nm\nUm-1\nU\nUm =\n.\nm-1\nUm-1\n(In other words, Um is the m-fold tensor product of U1 with itself.)\n(a) Show that RM(r, m) is generated by the rows of Um of Hamming weight 2m-r or\ngreater. [Hint: observe that this holds for m = 1, and prove by recursion using the\n|u|u + v| construction.] For example, give a generator matrix for the (8, 4, 4) RM code.\nWe first observe that Um is a lower triangular matrix with ones on the diagonal. Thus its\nm rows are linearly independent, and generate the universe code (2m , 2m , 1) = RM(m, m).\nU\nThe three RM codes with m = 1 are RM(1, 1) = (2, 2, 1), RM(0, 1) = (2, 1, 2), and\nRM(-1, 1) = (2, 0, inf). By inspection, RM(1, 1) = (2, 2, 1) is generated by the two rows of\n1 of weight 1 or greater (i.e., both rows), and RM(0, 1) = (2, 1, 2) is generated by the row\nof U1 of weight 2 or greater (i.e., the single row (1, 1)). (Moreover, RM(-1, 1) = (2, 0, inf)\nis generated by the rows of U1 of weight 4 or greater (i.e., no rows).)\nSuppose now that RM(r, m - 1) is generated by the rows of Um-1 of Hamming weight\nm-1-r or greater. By the |u|u + v| construction,\nRM(r, m) = {(u, u + v) | u ∈ RM(r, m - 1), v ∈ RM(r - 1, m - 1)}.\nEquivalently, since RM(r - 1, m - 1) is a subcode of RM(r, m - 1), we can write\nRM(r, m) = {(u ′ + v, u ) | u ′ ∈ RM(r, m - 1), v ∈ RM(r - 1, m - 1)},\nwhere u′ = u + v. Thus a set of generators for RM(r, m) is\n{(u , u ), | u ′ ∈ RM(r, m - 1)}; {(v, 0), | v ∈ RM(r - 1, m - 1)}.\nNow from the construction of Um from Um-1, each of these generators is a row of Um with\nweight 2m-r or greater, so these rows certainly suffice to generate RM(r, m). Moreover,\nthey are linearly independent, so their number is the dimension of RM(r, m):\nk(r, m) = k(r, m - 1) + k(r - 1, m - 1).\nFor example, the (8, 4, 4) code is generated by the four rows of U8 of weight 4 or more:\n⎡\n⎤\n\n⎥\n⎢ 1\n\n⎢\n⎥\n⎡\n⎤\n⎢ 1\n⎥\n\n⎢\n⎥\n⎥\n⎢ 1\n⎥\n⎢ 1\n\n⎢\n⎢\n⎥ .\n⎢\n⎥\n⎦\nU8 =\n\n⎥ ;\nG(8,4,4) = ⎣ 1\n\n⎢\n⎥\n⎢ 1\n⎥\n\n⎢\n⎥\n⎦\n⎣ 1\n\nm\n(b) Show that the number of rows of Um of weight 2m-r is\nr . [Hint: use the fact that\nm is the coefficient of zm-r in the integer polynomial (1 + z)m.]\nr\nFollowing the hint, let N(r, m) denote the number of rows of Um of weight precisely 2m-r ,\nand define the generator polynomial\nm\ngm(z) =\nN(r, m)z r .\nr=0\nThen since N(0, 1) = N(1, 1) = 1, we have g1(z) = 1 + z. Moreover, since the number\nof rows of Um of weight precisely 2m-r is equal to the number of rows of Um-1 of weight\nm-r plus the number of rows of Um-1 of weight 2m-r-1, we have\nN(r, m) = N(r -1, m -1) + N(r, m -1).\nThis yields the recursion gm(z) = (1 + z)gm-1(z), from which we conclude that\nm\n\ngm(z) = (1 + z)m =\nm z r .\nr\nr=0\n\nm\nConsequently N(r, m) is the coefficient of zr , namely N(r, m) =\n.\nr\nm\n(c) Conclude that the dimension of RM(r, m) is k(r, m) =\n0≤j≤r\nj .\nSince k(r, m) is the number of rows of Um of weight 2m-r or greater, we have\n\nm\nk(r, m) =\nN(r, m) =\n.\nj\n0≤j≤r\n0≤j≤r\nProblem 4.6 (\"Wagner decoding\")\nLet C be an (n, n -1, 2) SPC code. The Wagner decoding rule is as follows. Make hard\ndecisions on every symbol rk, and check whether the resulting binary word is in C. If so,\naccept it. If not, change the hard decision in the symbol rk for which the reliability metric\n|rk | is minimum. Show that the Wagner decoding rule is an optimum decoding rule for SPC\ncodes. [Hint: show that the Wagner rule finds the codeword x ∈C that maximizes r(x | r).]\nThe maximum-reliability (MR) detection rule is to find the codeword that maximizes\nr(x | r) =\n|rk |(-1)e(xk ,rk ), where e(xk , rk ) = 0 if the signs of s(xk ) and rk agree, and\nk\n1 otherwise. MR detection is optimum for binary codes on a Gaussian channel.\nIf there is a codeword such that e(xk , rk ) = 0 for all k, then r(x | r) clearly reaches its\nmaximum possible value, namely\n|rk |, so this codeword should be chosen.\nk\nA property of a SPC code is that any word not in the code (i.e., an odd-weight word) may\nbe changed to a codeword (i.e., an even-weight word) by changing any single coordinate\nvalue. The resulting value of r(x | r) will then be (\n|rk |) -2|rk |, where k′ is the index\nk\nof the changed coordinate. To maximize r(x | r), we should therefore choose the k′ for\nwhich |rk | is minimum. This is the Wagner decoding rule.\nIt is clear that any further changes can only further lower r(x | r), so Wagner decoding\nsucceeds in finding the codeword that maximizes r(x | r), and is thus optimum.\n\nProblem 4.7 (small cyclic groups).\nWrite down the addition tables for Z2, Z3 and Z4. Verify that each group element appears\nprecisely once in each row and column of each table.\nThe addition tables for Z2, Z3 and Z4 are as follows:\n+ 0\n\n+\n\n+\n0 0\n\n1 1\n\n2 2\n\n3 3\n\nIn each table, we verify that every row and column is a permutation of Zn.\nProblem 4.8 (subgroups of cyclic groups are cyclic).\nShow that every subgroup of Zn is cyclic. [Hint: Let s be the smallest nonzero element in\na subgroup S ⊆ Zn, and compare S to the subgroup generated by s.]\nFollowing the hint, let S be a subgroup of Zn = {0, 1, . . . , n - 1}, let s be the smallest\nnonzero element of S, and let S(s) = {s, 2s, . . . , ms = 0} be the (cyclic) subgroup of S\ngenerated by s. Suppose that S = S(s); i.e., there is some element t ∈ S that is not in\nS(s). Then by the Euclidean division algorithm t = qs + r for some r < s, and moreover\nr = 0 because t = qs implies t ∈ S(s). But t ∈ S and qs ∈ S(s) ⊆ S imply r = t- qs ∈ S;\nbut r = 0 is smaller than the smallest nonzero element s ∈ S, contradiction. Thus\nS = S(s); i.e., S is the cyclic subgroup that is generated by its smallest nonzero element."
    },
    {
      "category": "Resource",
      "title": "midterm005.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/7782bf3c7b424f67c3e1afeb02c6d7d7_midterm005.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, March 16, 2005\nMIT, Spring 2005\nHandout #13\nMidterm\n- You have 110 minutes (9:05-10:55 am) to complete the test.\n- This is a closed-book test, except that three 8.5′′ × 11′′ sheets of notes are allowed.\n- Calculators are allowed (provided that erasable memory is cleared).\n- There are two problems on the quiz. The first is a seven-part problem, each part\nworth 10 points. There is also an optional eighth part, for which you can receive up\nto 10 points of extra credit. The second problem consists of three unrelated true-false\nquestions, each worth 10 points.\n- The problems are not necessarily in order of difficulty.\n- Even if you can't prove a proposition stated in one part of a problem, you may assume\nthat it is true in subsequent parts.\n- A correct answer does not guarantee full credit and a wrong answer does not guarantee\nloss of credit. You should concisely indicate your reasoning and show all relevant work.\nThe grade on each problem is based on our judgment of your level of understanding\nas reflected by what you have written.\n- If we can't read it, we can't grade it.\n- If you don't understand a problem, please ask.\n\nα\ndB\ndB\n(round numbers)\n(two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n6.02\n6.99\n9.03\n10.00\nTable 1. Values of certain small factors α in dB.\ncode\nρ\nγc\n(dB)\nNd\nKb\nγeff (dB)\ns\nt\n(8,7,2)\n1.75\n7/4\n2.43\n2.0\n(8,4,4)\n1.00\n3.01\n2.6\n(16,15,2)\n1.88\n15/8\n2.73\n2.1\n(16,11,4)\n1.38\n11/4\n4.39\n3.7\n(16, 5,8)\n0.63\n5/2\n3.98\n3.5\n(32,31, 2)\n1.94\n31/16\n2.87\n2.1\n(32,26, 4)\n1.63\n13/4\n5.12\n4.0\n(32,16, 8)\n1.00\n6.02\n4.9\n(32, 6,16)\n0.37\n4.77\n4.2\n(64,63, 2)\n1.97\n63/32\n2.94\n1.9\n(64,57, 4)\n1.78\n57/16\n5.52\n4.0\n(64,42, 8)\n1.31\n21/4\n7.20\n5.6\n(64,22,16)\n0.69\n11/2\n7.40\n6.0\n(64, 7,32)\n0.22\n7/2\n5.44\n4.6\nTable 2. Parameters of RM codes with lengths n ≤ 64.\n\nProblem M.1 (70 points)\nIn this problem, we will study a class of codes called product codes.\nSuppose that C1 and C2 are two binary linear block codes with parameters (n1, k1, d1) and\n(n2, k2, d2), respectively. We will assume that the first k1 and k2 coordinate positions are\ninformation sets of C1 and C2, respectively.\nThe product code C is the code obtained by the following three-step encoding method. In\nthe first step, k1 independent information bits are placed in each of k2 rows, thus creating\na k2 × k1 rectangular array (see Figure 1a). In the second step, the k1 information bits\nin each of these k2 rows are encoded into a codeword of length n1 in C1, thus creating a\nk2 × n1 rectangular array (see Figure 1b). In the third step, the k2 information bits in\neach of the n1 columns are encoded into a codeword of length n2 in C2, thus creating an\nn2 × n1 rectangular array (see Figure 1c).\nk1\nk1\nn1 -k1\nk1\nn1 -k1\nk2\ninfo\nbits\nk2\ninfo\nbits\nk2\ninfo\nbits\n\nn1\n-\nn2 -k2\nn2\n\nn1\n- ?\n(a)\n(b)\n(c)\nFigure 1. (a) k2 × k1 information bit array. (b) k2 × n1 array after row encoding.\n(c) n2 × n1 array after column encoding.\n(a) Given an (n, k) binary linear block code C, show that the first k coordinate positions\nare an information set of C if and only if there exists a generator matrix G for C whose\nfirst k columns form a k × k identity matrix.\n(b) Show that the encoding method given above produces the same codeword whether\nthe encoder encodes first rows and then columns, or first columns and then rows.\n(c) Show that the product code C is an (n1n2, k1k2, d1d2) binary linear block code.\n(d) Express the nominal coding gain γc(C) of the Euclidean-space image s(C) of C in terms\nof the nominal coding gains γc(C1) and γc(C2) of the Euclidean-space images s(C1) and\ns(C2) of C1 and C2, respectively. Express the nominal spectral efficiency ρ(C) of C in terms\nof the nominal spectral efficiencies ρ(C1) and ρ(C2) of C1 and C2, respectively.\n(e) Starting with Reed-Muller codes of lengths less than 64, is it possible to use the product\ncode construction to construct a product code of length 64 that has better parameters\n(64, k, d) than the corresponding RM code of length 64?\n(f) Starting with Reed-Muller codes of lengths less than 64, is it possible to obtain a\nsequence of product codes whose nominal coding gains increase without limit by iter-\nating the product code construction-- i.e., by extending the above construction to an\nm-dimensional product code that maps an array of k1 × k2 × · · · × km information bits\ninto n1 × n2 × · · · × nm binary symbols using binary linear block codes C1, C2, . . . , Cm? Is\nit possible to do this while keeping the nominal spectral efficiency above some nonzero\nvalue?\n\n(g) The construction of C suggests the following two-step decoding method. First decode\neach row, using an optimum (minimum Euclidean distance) decoding method for C1. This\nfirst decoding step yields an array of noisy received bits. Then decode each column, using\nan optimum (minimum Hamming distance) decoding method for C2.\nCompare the performance and complexity of this two-step decoding method with that of\nthe optimum decoding method on a binary-input AWGN channel. If you like, you may\nlet both C1 and C2 be the (8, 4, 4) RM code. As a figure of merit for performance, you may\nuse the minimum squared norm of any error sequence that can cause a decoding error.\n(h) [Optional; extra credit] Propose a two-step decoding method that has same figure of\nmerit for performance as optimum decoding, but has decoding complexity similar to that\nof the suboptimal two-step method proposed in part (g).\nProblem M.2 (30 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) A signal constellation A consisting of a subset of 2k points of the 2n vertices of the\nn-cube, k < n, has a nominal spectral efficiency ρ(A) < 2 b/2D and a nominal coding\ngain γc(A) ≥ 1.\n(b) Let S = {a, b, c, d, e, f} be a set of six elements, and let a binary operation ⊕ be\ndefined on S by the following \"addition table:\"\n⊕\na\nb\nc\nd\ne\nf\na\na\nb\nc\nd\ne\nf\nb\nb\nc\na\nf\nd\ne\nc\nc\na\nb\ne\nf\nd\nd\nd\ne\nf\na\nb\nc\ne\ne\nf\nd\nc\na\nb\nf\nf\nd\ne\nb\nc\na\nThen S forms a group under the binary operation ⊕. (You need not check the associative\nlaw.)\n(c) Considering the weight distribution of a (6, 3, 4) code over F4, it is possible that such\na code exists."
    },
    {
      "category": "Resource",
      "title": "ps5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/b0311117e1f97920f4c371472379d361_ps5.pdf",
      "content": "6.451 Principles of Digital Communication II\nWednesday, March 2, 2005\nMIT, Spring 2005\nHandout #11\nDue: Wednesday, March 9,\n\nProblem Set 5\nProblem 5.1 (Euclidean division algorithm).\n(a) For the set F[x] of polynomials over any field F, show that the distributive law holds:\n(f1(x) + f2(x))h(x) = f1(x)h(x) + f2(x)h(x).\n(b) Use the distributive law to show that for any given f(x) and g(x) in F[x], there is a\nunique q(x) and r(x) with deg r(x) < deg g(x) such that f(x) = q(x)g(x) + r(x).\nProblem 5.2 (unique factorization of the integers).\nFollowing the proof of Theorem 7.7, prove unique factorization for the integers Z.\nProblem 5.3 (finding irreducible polynomials).\n(a) Find all prime polynomials in F2[x] of degrees 4 and 5. [Hint: There are three prime\npolynomials in F2[x] of degree 4 and six of degree 5.]\n(b) Show that x16 + x factors into the product of the prime polynomials whose degrees\ndivide 4, and x32 + x factors into the product of the prime polynomials whose degrees\ndivide 5.\nProblem 5.4 (The nonzero elements of Fg(x) form an abelian group under multiplication).\nLet g(x) be a prime polynomial of degree m, and r(x), s(x), t(x) polynomials in Fg(x).\n(a) Prove the distributive law, i.e., (r(x) + s(x)) ∗ t(x) = r(x) ∗ t(x) + s(x) ∗ t(x). [Hint:\nExpress each product as a remainder using the Euclidean division algorithm.]\n= 0, show that r(x) ∗ s(x)\n= t(x).\n(b) For r(x)\n= r(x) ∗ t(x) if s(x)\n(c) For r(x) = 0, show that as s(x) runs through all nonzero polynomials in Fg(x), the\nproduct r(x) ∗ s(x) also runs through all nonzero polynomials in Fg(x).\n(d) Show from this that r(x) = 0 has a mod-g(x) multiplicative inverse in Fg(x); i.e., that\nr(x) ∗ s(x) = 1 for some s(x) ∈ Fg(x).\nProblem 5.5 (Construction of F32).\nF\n(a) Using an irreducible polynomial of degree 5 (see Problem 5.3), construct a finite field\n32 with 32 elements.\n(b) Show that addition in F32 can be performed by vector addition of 5-tuples over F2.\n(c) Find a primitive element α ∈ F32. Express every nonzero element of F32 as a distinct\npower of α. Show how to perform multiplication and division of nonzero elements in F32\nusing this \"log table.\"\n\n(d) Discuss the rules for multiplication and division in F32 when one of the field elements\ninvolved is the zero element, 0 ∈ F32.\nF\nProblem 5.6 (Second nonzero weight of an MDS code)\nShow that the number of codewords of weight d + 1 in an (n, k, d) linear MDS code over\nq is\n\nn\nd + 1\nNd+1 =\n(q 2 -1) -\n(q -1) ,\nd + 1\nd\nwhere the first term in parentheses represents the number of codewords with weight ≥ d in\nany subset of d + 1 coordinates, and the second term represents the number of codewords\nwith weight equal to d.\nProblem 5.7 (Nd and Nd+1 for certain MDS codes)\n(a) Compute the number of codewords of weights 2 and 3 in an (n, n - 1, 2) SPC code\nover F2.\n(b) Compute the number of codewords of weights 2 and 3 in an (n, n -1, 2) linear code\nover F3.\n(c) Compute the number of codewords of weights 3 and 4 in a (4, 2, 3) linear code over\nF3.\nProblem 5.8 (\"Doubly\" extended RS codes)\nq )q+1\n(a) Consider the following mapping from (Fq )k to (F\n. Let (f0, f1, . . . , fk-1) be any\nk-tuple over Fq , and define the polynomial f(z) = f0 + f1z + · · · + fk1 zk-1 of degree less\nthan k. Map (f0, f1, . . . , fk-1) to the (q + 1)-tuple ({f(βj ), βj ∈ Fq }, fk-1)-- i.e., , to the\nRS codeword corresponding to f(z), plus an additional component equal to fk-1.\nShow that the qk (q + 1)-tuples generated by this mapping as the polynomial f(z) ranges\nover all qk polynomials over Fq of degree less than k form a linear (n = q+1, k, d = n-k+1)\nMDS code over Fq . [Hint: f(z) has degree less than k -1 if and only if fk-1 = 0.]\n(b) Construct a (4, 2, 3) linear code over F3. Verify that all nonzero words have weight 3."
    },
    {
      "category": "Resource",
      "title": "final02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/48a03949a4142318ff9a90f4b1dbdc84_final02.pdf",
      "content": "Final\nExam\n\nY\nou\nha\nv\ne\nh o u r s\nto\ncomplete\nthe\nexam.\n\nThis\nis\na\nclosed-b\no\nok\nexam,\nexcept\nthat\nv\ne\n8:5\nsheets\nof\nnotes\nare\nallo\nw\ned.\n\nCalculators\nare\nallo\nw\ned\n(pro\nvided\nthat\nerasable\nmemory\nis\ncleared).\n\nThere\nare\nthree\nproblems\non\nthe\nexam.\nThey\nare\nnot\nnecessarily\nin\norder\nof\ndiÆcult\ny\n.\nThe\nrst\nt\nw\no\nproblems\nare\nm\nultipart\nproblems\nw\north\nand\np\noin\nts,\nresp\nectiv\nely\n.\nThe\nthird\nproblem\nconsists\nof\nv\ne\nunrelated\ntrue-false\nquestions\nw\north\np\noin\nts\neac\nh.\n\nEv\nen\nif\ny\nou\ncan't\ndo\none\npart\nof\na\nm\nultipart\nproblem,\ntry\nto\ndo\nsucceeding\nparts.\n\nA\ncorrect\nansw\ner\ndo\nes\nnot\nguaran\ntee\nfull\ncredit\nand\na\nwrong\nansw\ner\ndo\nes\nnot\nguaran\ntee\nloss\nof\ncredit.\nY\nou\nshould\nconcisely\nindicate\ny\nour\nreasoning\nand\nsho\nw\nall\nrelev\nan\nt\nw\nork.\nThe\ngrade\non\neac\nh\nproblem\nis\nbased\non\nour\njudgmen\nt\nof\ny\nour\nlev\nel\nof\nunderstanding\nas\nreected\nb\ny\nwhat\ny\nou\nha\nv\ne\nwritten.\n\nIf\nw\ne\ncan't\nread\nit,\nw\ne\ncan't\ngrade\nit.\n\nIf\ny\nou\ndon't\nunderstand\na\nproblem,\nplease\nask.\n\nFigure\n1.\nP\nb\n(E\n)\nvs.\nE\nb\n=\nN\nfor\nunco\nded\nbinary\nP\nAM.\nFigure\n2.\nP\ns\n(E\n)\nvs.\nSNR\nnorm\nfor\nunco\nded\n(M\n\nM\n)-QAM.\n\ndB\n(appro\nx.)\ndB\n(exact)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\n\n4.97\n6.02\n6.99\n9.03\n10.00\nT\nable\nA.\nV\nalues\nof\ncertain\nsmall\nfactors\n\nin\ndB.\nRM\nco\nde\n\nc\n(dB)\nN\nd\nK\nb\n\ne\n(dB)\ns\nt\n(8,7,2)\n1.75\n7/4\n2.43\n2.0\n(8,4,4)\n1.00\n3.01\n2.6\n(16,15,2)\n1.88\n15/8\n2.73\n2.1\n(16,11,4)\n1.38\n11/4\n4.39\n3.7\n(16,\n5,8)\n0.63\n5/2\n3.98\n3.5\n(32,31,\n2)\n1.94\n31/16\n2.87\n2.1\n(32,26,\n4)\n1.63\n13/4\n5.12\n4.0\n(32,16,\n8)\n1.00\n6.02\n4.9\n(32,\n6,16)\n0.37\n4.77\n4.2\n(64,63,\n2)\n1.97\n63/32\n2.94\n1.9\n(64,57,\n4)\n1.78\n57/16\n5.52\n4.0\n(64,42,\n8)\n1.31\n21/4\n7.20\n5.6\n(64,22,16)\n0.69\n11/2\n7.40\n6.0\n(64,\n7,32)\n0.22\n7/2\n5.44\n4.6\nT\nable\nB.\nP\narameters\nof\ncertain\nReed-Muller\n(RM)\nco\ndes.\n\nd\nT\nable\n1:\nRate-1/2\nbinary\nlinear\ncon\nv\nolutional\nco\ndes\nfree\n\nc\ndB\nK\nb\n\ne\n(dB)\n1.5\n1.8\n1.8\n2.5\n4.0\n4.0\n4.8\n4.6\n3.5\n5.2\n4.8\n6.0\n5.6\n7.0\n5.9\n4.5\n6.5\n6.1\n7.0\n6.7\n7.8\n7.1\nT\nable\n2:\nRate-1/3\nbinary\nlinear\ncon\nv\nolutional\nco\ndes\n\nd\nfree\n\nc\ndB\nK\nb\n\ne\n(dB)\n1.67\n2.2\n2.2\n2.67\n4.3\n4.0\n3.33\n5.2\n4.7\n6.0\n5.3\n4.33\n6.4\n6.4\n7.0\n6.3\n5.33\n7.3\n7.3\n7.8\n7.4\nT\nable\n3:\nRate-1/4\nbinary\nlinear\ncon\nv\nolutional\nco\ndes\n\nd\nfree\n\nc\ndB\nK\nb\n\ne\n(dB)\n1.75\n2.4\n2.4\n2.5\n4.0\n3.8\n3.25\n5.1\n4.7\n6.0\n5.6\n4.5\n6.5\n6.0\n7.0\n6.0\n5.5\n7.4\n7.2\n7.8\n7.6\n\nProblem\nF.1\n(60\np\no i n\nts)\nIn\nthis\nproblem\nw\ne\nconsider\na\ncon\nv\nolutional\nco\nde\nC\no\nv\ner\nthe\nquaternary\neld\nF\n.\nThe\nele-\ng\nmen\nts\nof\nF\nma\ny\nb\ne\ndenoted\nas\nf00;\n01;\n10;\n11g\n(additiv\ne\nrepresen\ntation)\nor\nas\nf0;\n1;\n;\n\n(m\nultiplicativ\ne\nrepresen\ntation),\nwhere\n\nis\na\nprimitiv\ne\nelemen\nt\no f\nF\nand\na\nro\not\nof\nx\n+\nx\n+\n1.\nY\nou\nmigh\nt\nwish\nto\njot\ndo\nwn\nthe\naddition\nand\nm\nultiplication\ntables\nof\nF\n.\nThe\ncon\nv\nolutional\nco\nde\nC\nis\ngenerated\nb\ny\nthe\nenco\nder\nsho\nwn\nb\ne l o\nw.\ny\n1k\n-\nn\n-\nu\nk\nt-\nD\nu\nk\nt\n?-\n?\n-\n\nn\n\n-\n\nn\n\n?\ny\n2k\n-\nn\n-\nThe\ninput\nu\nk\nat\ntime\nk\nis\nan\nelemen\nt\no f\nF\n,\nand\nthe\ndela\ny\nelemen\nt\n(denoted\nb\ny\nD\n)\nstores\nthe\nprevious\ninput\nu\nk\n.\nThere\nare\nt\nw\no\nF\noutputs\nat\neac\nh\ntime\nk\n,\nwhose\nequations\nare\ny\n1k\n=\nu\nk\n+\nu\nk\n;\ny\n2k\n=\n\nu\n\nk\n+\n\nu\nk\n:\n(a)\nSho\nw\nthat\nthe\ncon\nv\nolutional\nco\nde\nC\nis\nlinear\no\nv\ner\nF\n.\n(b)\nLet\nu(D\n);\ny\n(D\n)\nand\ny\n(D\n)\nb\ne\nthe\nD\n-transforms\nof\nthe\nsequences\nfu\nk\ng;\nfy\n1k\ng\nand\nfy\n2k\ng,\nresp\nectiv\nely\n.\nGiv\ne\nexpressions\nfor\ny\n(D\n)\nand\ny\n(D\n)\nin\nterms\nof\nu(D\n).\n(c)\nSp\necify\nthe\nn\num\nb\ner\nof\nstates\nin\nthis\nenco\nder.\nDra\nw\na\nsingle\nsection\nof\na\ntrellis\ndiagram\nfor\nC\n,\nlab\nelling\neac\nh\nbranc\nh\nwith\na\nquaternary\n2-tuple\n(y\n1k\n;\ny\n2k\n)\n(F\n)\n.\n(d)\nSho\nw\nthat\nthis\nenco\nder\nfor\nC\nis\nnoncatastrophic.\n(e)\nFind\nthe\nminim\num\nHamming\ndistance\nd\nfree\n(C\n),\nand\nthe\na\nv\nerage\nn\num\nb\ne r\nof\nnearest\nneigh\nb\no rs\nK\nmin\n(C\n)\np\ner\nunit\ntime.\nNo\nw\ndene\nthe\nbinary\nimage\nof\nC\nas\nthe\nbinary\ncon\nv\nolutional\nco\nde\nC\nobtained\nb\ny\nmap-\nping\nthe\noutputs\ny\nj\nF\nin\nto\nthe\nadditiv\ne\nrepresen\ntation\nf00;\n01;\n10;\n11g,\nwhere\neac\nh\nrepresen\ntativ\ne\nis\na\npair\nof\nelemen\nts\nof\nF\n.\nk\nu\n(f\n)\nRep\neat\nparts\n(a)-(e)\nfor\nC\n,\nreplacing\nF\nb\ny\nF\nwhere\nappropriate.\n(F\nor\npart\n(b),\nmap\nk\nF\nto\nits\nbinary\nimage.)\n(g)\nCompute\nthe\nnominal\nsp\nectral\neÆciency\n(C\n)\nand\nthe\nnominal\nco\nding\ngain\n\nc\n(C\n),\nand\nestimate\nthe\neectiv\ne\nco\nding\ngain\n\ne\n(C\n)\nusing\nour\nusual\nrule\nof\nth\num\nb.\nCompare\nthe\np\nerformance\nof\nC\nto\nthat\nof\nthe\nb\nest\nrate-1=n\nbinary\nlinear\ncon\nv\nolutional\nco\nde\nwith\nthe\nsame\nsp\nectral\neÆciency\nand\nn\num\nb\ne r\nof\nstates\n(see\ntables\nab\no\nv\ne).\n\nNo\nw\ndene\nanother\nbinary\ncon\nv\nolutional\nco\nde\nC\nas\nthe\nco\nde\nobtained\nb\ny\nmapping\nthe\noutputs\ny\nj\nF\nin\nto\nthe\nco\ndew\nords\nf000;\n011;\n101;\n110g\nin\nthe\n(3;\n2;\n2)\nbinary\nSPC\nco\nde,\nwhere\neac\nh\nrepresen\ntativ\ne\nis\nno\nw\na\n3-tuple\nof\nelemen\nts\nof\nF\n.\nk\nu\n(h)\nRep\neat\nparts\n(a)-(e)\nfor\nC\n,\nreplacing\nF\nb\ny\nF\nwhere\nappropriate.\n(F\nor\npart\n(b),\nmap\nk\nF\nto\nits\nbinary\nimage.)\n(i)\nCompute\n(C\n)\nand\n\nc\n(C\n),\nand\nestimate\n\ne\n(C\n).\nCompare\nthe\np\nerformance\nof\nC\nto\nthat\nof\nthe\nb\nest\nrate-1=n\nbinary\nlinear\ncon\nv\nolutional\nco\nde\nwith\nthe\nsame\nsp\nectral\neÆciency\nand\nn\num\nb\ne r\nof\nstates\n(see\ntables\nab\no\nv\ne).\nProblem\nF.2\n(40\np\no i n\nts)\nIn\nthis\nproblem\nw\ne\nconsider\ngraphical\nrepresen\ntations\nand\ndeco\nding\nof\nthe\n(32;\n16;\n8)\nbinary\nReed-Muller\nco\nde\nRM(2;\n5).\n(a)\nSho\nw\nthat\nthere\nis\na\npartition\nof\nthe\nsym\nb\no l s\nof\nthis\nco\nde\nin\nto\nfour\n8-tuples\nsuc\nh\nthat\nthe\npro\njection\nof\nRM(2;\n5)\non\nto\nan\ny\n8-tuple\nis\nthe\n(8;\n7;\n2)\nbinary\nSPC\nco\nde,\nand\nthe\nsub\nco\nde\ncorresp\nonding\nto\neac\nh\n8-tuple\nis\nthe\n(8;\n1;\n8)\nbinary\nrep\netition\nco\nde;\nmoreo\nv\ner,\nthe\n8-tuples\nma\ny\nb\ne\npaired\nsuc\nh\nthat\nthe\npro\njection\non\nto\neac\nh\nresulting\n16-tuple\nis\nthe\n(16;\n11;\n4)\nextended\nHamming\nco\nde,\nand\nthe\nsub\nco\nde\ncorresp\nonding\nto\neac\nh\nresulting\n16-\ntuple\nis\nthe\n(16;\n5;\n8)\nbiorthogonal\nco\nde.\n(b)\nUsing\npart\n(a),\nsho\nw\nthat\nthere\nis\na\nnormal\nrealization\nof\nRM(2;\n5)\nwhose\ngraph\nis\nas\nfollo\nws:\n/\n/\n/\n/\n(14;\n7)\n(14;\n7)\n(14;\n7)\n(14;\n7)\n/\n/\n/\n/\n(18;\n9)\n/\n(18;\n9)\n[Tip:\nto\nnd\nthe\nconstrain\nt\nco\nde\ndimensions,\ny\nou\nma\ny\nuse\nthe\nfact\n(not\npro\nv\ned\nin\n6.451)\nthat\nthe\nconstrain\nt\nco\ndes\nin\na\ncycle-free\nrepresen\ntation\nof\na\nself-dual\nco\nde\nare\nself-dual.]\n(c)\nUsing\npart\n(b),\ngiv\ne\na\nhigh-lev\nel\ndescription\nof\nan\neÆcien\nt\nalgorithm\nfor\nmaxim\num-\nlik\neliho\no\nd\ndeco\nding\nof\nRM(2;\n5)\non\nan\narbitrary\nmemoryless\nc\nhannel.\n(d)\nCompare\nthe\np\nerformance\n(probabilit\ny\nof\nerror)\nand\ncomplexit\ny\n( n\num\nb\ner\nof\narithmetic\nop\nerations,\nroughly)\nof\nthe\nalgorithm\nof\npart\n(c)\nto\nthat\nof\nthe\nViterbi\nalgorithm\napplied\nto\nan\neÆcien\nt\ntrellis\nrealization\nof\nRM(2;\n5).\n[Hin\nt:\nstart\nb\ny\nnding\na\ntrellis-orien\nted\ngenerator\nmatrix\nfor\nRM(2;\n5),\nand\nthen\nnd\nan\neÆcien\nt\nsectionalization.]\n\nProblem\nF.3\n(50\np\no i n\nts)\nF\nor\neac\nh\nof\nthe\nprop\nositions\nb\nelo\nw,\nstate\nwhether\nthe\nprop\nosition\nis\ntrue\nor\nfalse,\nand\ngiv\ne\na\nbrief\npro\nof.\nIf\na\nprop\nosition\nis\nfalse,\nthe\npro\nof\nwill\nusually\nb\ne\na\ncoun\nterexample.\nF\null\ncredit\nwill\nnot\nb\ne\ngiv\nen\nfor\ncorrect\nansw\ners\nwithout\nan\nadequate\nexplanation.\n(a)\nThe\nEuclidean\nimage\nof\nan\n(n;\nk\n;\nd)\nbinary\nlinear\nblo\nc\nk\nco\nde\nis\nan\northogonal\nsignal\nset\nif\nand\nonly\nif\nk\n=\nlog\nn\nand\nd\n=\nn=2.\n(b)\nEv\nery\nelemen\nt\n\nF\nis\nthe\nro\not\nof\na\nbinary\np\nolynomial\nf\n(x)\nF\n[x]\nof\ndegree\nless\nthan\nor\nequal\nto\n5.\n(c)\nIf\nco\ndew\nords\nin\nan\n(n;\nk\n;\nd)\nbinary\nlinear\nblo\nc\nk\nco\nde\nwith\nd\nev\nen\nare\ntransmitted\nequiprobably\no\nv\ner\nan\nA\nW\nGN\nc\nhannel\nusing\na\nstandard\n2-P\nAM\nmap\nand\nare\noptimally\ndetected,\nthen\nthe\nminim\num\nsquared\ndistance\nto\nan\ny\ndecision\nb\noundary\nis\nt\nwice\nthe\nminim\num\nsquared\ndistance\nthat\nis\nac\nhiev\ned\nif\nbinary\nhard\ndecisions\nare\nmade\nrst\non\neac\nh\nsym\nb\nol\nand\nthen\nthe\nresulting\nbinary\nreceiv\ned\nw\nord\nis\noptimally\ndeco\nded.\n(d)\nCapacit\ny-approac\nhing\nco\ndes\nm\nust\nha\nv\ne\ntrellis\ncomplexit\ny\nparameters\nthat\nb\necome\narbitrarily\nlarge\nas\nthe\nShannon\nlimit\nis\napproac\nhed\narbitrarily\nclosely\n.\n(e)\nIf\nthe\np\noin\nts\nx\nin\na\nlattice\n\nare\ntransmitted\nwith\nunequal\nprobabilities\nfp(x);\nx\ng\np\no\nv\ner\nan\nA\nW\nGN\nc\nhannel\nand\noptimally\ndetected,\nthen\nPr\n(E\n)\n\nK\nmin\n()Q\n(d\n()=4\n),\nmin\nwhere\nd\n()\nis\nthe\nminim\num\nsquared\ndistance\nb\net\nw\neen\np\no i n\nts\nin\n,\nand\nK\nmin\n()\nis\nthe\nmin\na\nv\nerage\nn\num\nb\ne r\nof\nnearest\nneigh\nb\nors\nto\neac\nh\ntransmitted\np\noin\nt."
    },
    {
      "category": "Resource",
      "title": "final02solns2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/f96e2d910415543f5f3fe39a80d006ba_final02solns2.pdf",
      "content": "′\n′\n′\nFinal Exam Solutions\nProblem F.1 (60 points)\nIn this problem we consider a convolutional code C over the quaternary field F4. The ele\nments of F4 may be denoted as {00, 01, 10, 11} (additive representation) or as {0, 1, α, α2}\n(multiplicative representation), where α is a primitive element of F4 and a root of x2+x+1.\nYou might wish to jot down the addition and multiplication tables of F4.\nThe convolutional code C is generated by the encoder shown below.\ny1k\nuk\nuk-1\n\nD\n\nα\n×\n\nα2\n×\n\ny2k\n\nThe input uk at time k is an element of F4, and the delay element (denoted by D) stores\nthe previous input uk-1. There are two F4 outputs at each time k, whose equations are\ny1k = uk + uk-1;\ny2k = αuk + α2 uk-1.\n(a) Show that the convolutional code C is linear over F4.\nIf {(y1k, y2k)} ∈ C and {(y1\n′\nk, y′ )} ∈ C are the output sequences corresponding to the\n2k\ninput sequences {uk} and {u′ }, respectively, then the input sequence {uk + u′ } generates\nk\nk\nthe output sequence {(y1k + y1\n′\nk, y2k + y′ }, since\n2k\ny1k + y1k = uk + uk + uk-1 + uk-1;\ny2k + y2\n′\nk = α(uk + u ′\nk) + α2(uk-1 + u ′\nk-1).\nThus if {(y1k, y2k)} and {(y1\n′\nk, y′ )} are in C, then {(y1k + y1\n′\nk, y2k + y′ } is in C. Also, for\n2k\n2k\nany β ∈F4, the input sequence {βuk} generates the output sequence {β(y1k, y2k)}, since\nβy1k = βuk + βuk-1;\nβy2k = α(βuk) + α2(βuk-1).\nThus if {(y1k, y2k)} ∈C, then {β(y1k, y2k)} ∈C. So C is a vector space over F4.\nAlternatively, after doing part (b), we can verify that if y(D) = u(D)g(D) and y′(D) =\nu′(D)g(D) are in C, then y(D)+ y′(D) = (u(D)+ u′(D))g(D) is in C, and so is βy(D) =\nβu(D)g(D).\n\n(b) Let u(D), y1(D) and y2(D) be the D-transforms of the sequences {uk}, {y1k} and {y2k},\nrespectively. Give expressions for y1(D) and y2(D) in terms of u(D).\nIt is straightforward to verify that y1(D) = (1+D)u(D) and y2(D) = (α+α2D)u(D), since\n{y1k} and {y2k} are the convolution of {uk} with the finite sequences (g10 = 1, g11 = 1)\nand (g10 = α, g11 = α2), respectively.\nIn other words, y(D) = u(D)g(D), where\ng(D) = (1 + D, α + α2D).\n(c) Specify the number of states in this encoder. Draw a single section of a trellis diagram\nfor C, labelling each branch with a quaternary 2-tuple (y1k, y2k) ∈ (F4)2 .\nThe encoder has 4 states, corresponding to the 4 possible values of uk-1.\nEvery state transition is possible, so there are 16 branches. The equations of the encoder\ndetermine the output 2-tuple associated with each branch. A trellis section for this encoder\nis therefore as follows:\n1α\nαα2\n\nα21 0\n\n1α2\n\n01 �\n\nα20\n\nαα 1\n� 1\n\nα1\n�\n\n�\n\nα2α2\n\n0α\nα\n\nα\n\nα2α\n\nα0\nα2\n\nα2\n\n0α2\n(d) Show that this encoder for C is noncatastrophic.\nThe simplest way to show that no infinite input sequence can lead to a finite output\nsequence is to observe that there is only one branch labeled with 00, namely the branch\nfrom the zero state to the zero state, so any nonzero input uk must give a nonzero output.\nNoncatastrophicity also follows algebraically from the fact that 1 + D and α + α2D have\nno common factors.\n(e) Find the minimum Hamming distance dfree(C), and the average number of nearest\nneighbors Kmin(C) per unit time.\nSince C is linear, it suffices to find the minimum-weight nonzero code sequences. Since\nit is noncatastrophic, the finite-weight code sequences are those generated by finite input\nsequences.\nThe impulse response g(D) = (1+D, α+α2D) is a code sequence with Hamming weight 4,\nand so are αg(D) and α2g(D). All finite code sequences start with a weight-2 2-tuple and\nend with a weight-2 2-tuple, and have 2-tuples of weight at least 1 in between. Therefore\nthe nonzero scalar multiples of g(D) are the only nonzero minimum-weight sequences, the\nfree distance is dfree(C) = 4, and Kmin(C) = 3.\n\nNow define the binary image of C as the binary convolutional code C′ obtained by map\nping the outputs yjk ∈ F4 into the additive representation {00, 01, 10, 11}, where each\nrepresentative is a pair of elements of F2.\n(f) Repeat parts (a)-(e) for C′, replacing F4 by F2 where appropriate. (For part (b), map\nuk ∈ F4 to its binary image.)\nIt is easy to verify that the binary image map f : F4 → (F2)2 is linear; i.e., f(β + γ) =\nf(β) + f(γ). Therefore if f(y(D)) and f(y′(D)) are code sequences in C′, then so is\nf(y(D)) + f(y′(D)) = f(y(D) + y′(D)), since y(D) + y′(D) ∈ C. So C′ is a vector space\nover F2.\nThe trellis for C′ is the same as that for C with quaternary labels mapped to binary\nlabels. In other words, C′ is a rate-2/4, 4-state binary linear convolutional code. The\nencoder is still noncatastrophic because there are no branches other than the zero branch\nthat have the all-zero label 0000.\nIf the input sequence is zero except for 10 (α) at time zero, then the output sequence is\n(αα2, α1) = (1011, 1001); i.e., the impulse response is (1 + D, 0, 1, 1 + D). Similarly the\nimpulse response to an input 01 at time zero is (0110, 0111) = (0, 1 + D, 1 + D, D). Thus\nthe generator matrix of this binary rate-2/4 code is\nG′(D) = 1 + D\n1 +\n\nD\n1 +\n\nD 1 + D\nD\nEach of the two generator sequences has Hamming weight 5, and their sum has weight\n6. By noting that every finite sequence starts and ends with a 4-tuple of weight at least\n2 and examining the low-weight continuations through the trellis, we can quickly verify\nthat these are the only weight-5 sequences in the code. Therefore dfree(C′) = 5 and\nKmin(C) = 2.\n(g) Compute the nominal spectral efficiency ρ(C′) and the nominal coding gain γc(C′),\nand estimate the effective coding gain γeff (C′) using our usual rule of thumb. Compare\nthe performance of C′ to that of the best rate-1/n binary linear convolutional code with\nthe same spectral efficiency and number of states (see tables above).\nThe nominal spectral efficiency is ρ(C′) = 2k/n = 1 b/2D. The nominal coding gain is\nγc(C′) = dfreek/n = 5/2 (3.98 dB). The number of nearest neighbors per information bit is\nKb(C′) = Kmin(C′)/k = 1. Therefore the effective coding gain is the same as the nominal\ncoding gain.\nIn fact, these parameters are precisely the same as those of the best rate-1/2 4-state\nbinary linear convolutional code, namely our standard example code. Therefore the two\ncodes will have the same performance, to the accuracy of the union bound estimate.\n\nNow define another binary convolutional code C′′ as the code obtained by mapping the\noutputs yjk ∈ F4 into the codewords {000, 011, 101, 110} in the (3, 2, 2) binary SPC code,\nwhere each representative is now a 3-tuple of elements of F2.\n(h) Repeat parts (a)-(e) for C′′, replacing F4 by F2 where appropriate. (For part (b), map\nuk ∈ F4 to its binary image.)\nAgain, it is easy to verify that this map g : F4 → (F2)3 is linear, and thereby to prove\nthat C′′ is linear.\nThe trellis for C′′ is the same as that for C with quaternary 2-tuples mapped to binary\n6-tuples. In other words, C′′ is a rate-2/6, 4-state binary linear convolutional code. The\nencoder is still noncatastrophic because there are no branches other than the zero branch\nthat have the all-zero label 000000.\nThe impulse response to an input 10 at time zero is now (101110, 101011). Similarly, the\nimpulse response to an input 01 at time zero is now (011101, 011110). Thus the generator\nmatrix of this binary rate-2/6 code is\nG′′(D) = 1 + D\n1 +\n\nD\n1 +\n\nD\nD\n\n1 +\n\nD 1 + D 1 + D\nD\nNote that the map g : F4 → (F2)3 maps every nonzero element of F4 into a binary 3-tuple\nof Hamming weight 2. Therefore the weight of any binary image sequence is just twice the\nweight of the corresponding quaternary sequence. Therefore, using our previous results\nfor C in part (e), we have dfree(C′′) = 8 and Kmin(C′′) = 3.\n(i) Compute ρ(C′′) and γc(C′′), and estimate γeff (C′′). Compare the performance of C′′ to\nthat of the best rate-1/n binary linear convolutional code with the same spectral efficiency\nand number of states (see tables above).\nThe nominal spectral efficiency is ρ(C′′) = 2k/n = 2/3 b/2D. The nominal coding gain is\nγc(C′′) = dfreek/n = 8/3 (4.26 dB). The number of nearest neighbors per information bit\nis Kb(C′′) = Kmin(C′′)/k = 3/2. Therefore the effective coding gain is about 0.1 dB less\nthan the nominal coding gain; i.e., γeff (C′′) ≈ 4.15 dB.\nBy comparing with Table 2, we see that the nominal coding gain is precisely the same\nas that of the best rate-1/3 4-state binary linear convolutional code. Moreover, Kb is\nactually a factor of 2 better, so the effective coding gain of C′′ is about 0.2 dB better.\n\nProblem F.2 (40 points)\nIn this problem we consider graphical realizations and decoding of the (32, 16, 8) binary\nReed-Muller code RM(2, 5).\n(a) Show that there is a partition of the 32 symbols of this code into four 8-tuples such\nthat the projection of RM(2, 5) onto any 8-tuple is the (8, 7, 2) binary SPC code, and the\nsubcode corresponding to each 8-tuple is the (8, 1, 8) binary repetition code; moreover, the\n8-tuples may be paired such that the projection onto each resulting 16-tuple is the (16, 11, 4)\nextended Hamming code, and the subcode corresponding to each resulting 16-tuple is the\n(16, 5, 8) biorthogonal code.\nThe (32, 16, 8) code RM(2, 5) may be constructed by the |u|u + v| construction from the\n(16, 11, 4) code RM(2, 4) and the (16, 5, 8) code RM(1, 4) as follows:\nRM(2, 5) = {(u, u + v) | u ∈ RM(2, 4), v ∈ RM(1, 4)}.\nThus the projection onto either 16-tuple is RM(2, 4) (since RM(1, 4) is a subcode of\nRM(2, 4)). A codeword has the form (u, 0) if and only if u = v ∈ RM(1, 4), so the\nsubcode of codewords equal to zero on the second 16-tuple is equal to RM(1, 4) on the\nfirst 16-tuple. Similarly, a codeword has the form (0, u + v) if and only if u = 0, which\nimplies that the second 16-tuple is a codeword v ∈ RM(1, 4).\nSimilarly, the (16, 11, 4) code RM(2, 4) may be constructed by the |u|u + v| construction\nfrom the (8, 7, 2) code RM(2, 3) and the (8, 4, 4) code RM(1, 3); by an argument similar\nto that above, this shows that the projection onto any corresponding 8-tuple is RM(2, 3).\nAlso, the (16, 5, 8) code RM(1, 4) may be constructed by the |u|u + v| construction from\nthe (8, 4, 4) code RM(1, 3) and the (8, 1, 8) code RM(0, 3); by an argument similar to that\nabove, this shows that the subcode corresponding to any 8-tuple is RM(0, 3).\n(b) Using part (a), show that there is a normal realization of RM(2, 5) whose graph is as\nfollows:\n(14, 7)\n/ 8\n/ 6\n(18, 9)\n(18, 9)\n(14, 7)\n/ 8\n/ 6\n/\n(14, 7)\n/ 8\n/ 6\n/ 6\n(14, 7)\n/ 8\n[Tip: to find the constraint code dimensions, you may use the fact (not proved in 6.451)\nthat the constraint codes in a cycle-free realization of a self-dual code are self-dual.]\nIf the 32 symbols are partitioned into 8-tuples as in part (a), then the projection and\nsubcode corresponding to each such 8-tuple are the (8, 7, 2) and (8, 1, 8) code, respectively.\nBy the state space theorem, the dimension of the state space corresponding to a partition\nof the time axis into one of these 8-tuples and the remaining 24-tuple is equal to the\n\ndifference of the dimensions between this projection and subcode, which is 6. Similarly,\nthe dimension of the state space corresponding to the partition of the time axis into two\n16-tuples is the difference of the dimensions of the projection (16, 11, 4) and the subcode\n(16, 5, 8), which is also 6. This accounts for the dimensions of all state spaces shown in\nthe normal graph above.\nThe lengths of the constraint codes are simply the sums of the dimensions of the incident\nvariables, which are 8 + 6 = 14 for the top constraint codes, and 6 + 6 + 6 = 18 for the\nbottom constraint codes. Using the tip and the fact that the (32, 16, 8) RM code is a\nself-dual code, the constraint codes are self-dual and therefore must have dimension equal\nto half their length.\n(c) Using part (b), give a high-level description of an efficient algorithm for maximum-\nlikelihood decoding of RM(2, 5) on an arbitrary memoryless channel.\nMaximum-likelihood decoding of a code defined on a cycle-free graph over any memoryless\nchannel may be performed by the max-product algorithm using likelihood weights, or\nequivalently by the min-sum algorithm using negative log likelihood weights.\nA high-level description of how the max-product algorithm would work on the above\ngraph is as follows. The inputs are the received likelihood vectors (2-tuples) for each\nof the 32 received symbols. For the top constraint codes, the max-product update rule\namounts to taking the maximum of each pair of 8-tuple \"intrinsic\" likelihoods (which are\neach the product of the appropriate 8 received likelihoods) corresponding to each of the\n64 states (cosets of the (8, 1, 8) code in the (8, 7, 2) code). Next, for the bottom constraint\ncodes, the max-product update rule amounts to taking the products of two incoming\nstate likelihoods corresponding to 12 of the 18 bits in each of the 512 codewords of the\n(18, 9) constraint code, and then taking the maximum of each of the 8 such products that\ncorrespond to each of the 64 states specified by the third 6-tuple in each codeword. The\nresult is a vector of 64 likelihoods, one for each state in the central state space.\nIn a generalized Viterbi algorithm, the \"past\" and \"future\" likelihoods of each central\nstate could then be combined (multiplied), and the maximum selected. The result will\nbe the maximum likelihood of any codeword. If the sequence of decisions leading to this\nmaximum have been remembered, or are retraced, then the entire maximum-likelihood\ncodeword can be reconstructed.\nHowever, you could continue to straightforwardly apply the max-product algorithm. The\nmax-product update rule can now be applied twice more to each bottom constraint code,\nresulting in \"extrinsic\" likelihood vectors for all 4 upper-level state spaces. At each of the\ntop-level constraint codes, the max-product update rule amounts to fanning out each of\nthese 64 \"extrinsic\" likelihoods, 2 at a time, to the 128 possible input 8-tuples. These\nlikelihoods may then be combined (multiplied) with each of the corresponding \"intrinsic\"\nlikelihoods, and the maximum of these 128 products chosen; this maximum corresponds\nto the ML-decoded 8-tuple. The four ML-decoded 8-tuples then form the ML-decoded\ncodeword.\n(d) Compare the performance (probability of error) and complexity (number of arithmetic\noperations, roughly) of the algorithm of part (c) to that of the Viterbi algorithm applied\nto an efficient trellis realization of RM(2, 5). [Hint: start by finding a trellis-oriented\n\ngenerator matrix for RM(2, 5), and then find an efficient sectionalization.]\nThe max-product decoder and the Viterbi algorithm applied to a trellis are both exact\nML decoding algorithms, so both will have the same performance (probability of error).\nTo compare complexity, we first need to find an efficient trellis realization for RM(2, 5).\nThe standard coordinate ordering for RM codes yields an efficient trellis realization. A\nstandard generator matrix corresponding to the standard coordinate ordering is obtained\nby taking the 16 rows of weight 8 or more from the \"universal\" 32 × 32 generator matrix\nU32 = U ⊗5\n2 , namely\n⎡\n⎤\n11111111 00000000 00000000 00000000\n⎢ 11110000 11110000 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 11001100 11001100 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 10101010 10101010 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 11111111 11111111 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 11110000 00000000 11110000 00000000 ⎥\n⎢\n⎥\n⎢ 11001100 00000000 11001100 00000000 ⎥\n⎢\n⎥\nG =\n⎢\n⎢\n⎢\n⎥\n⎥\n⎥ .\n⎢\n⎥\n⎢ 11000000 11000000 11000000 11000000 ⎥\n⎢\n⎥\n⎢ 10100000 10100000 10100000 10100000 ⎥\n⎢\n⎥\n⎢ 11110000 11110000 11110000 11110000 ⎥\n⎢\n⎥\n⎢ 10001000 10001000 10001000 10001000 ⎥\n⎢\n⎥\n⎢ 11001100 11001100 11001100 11001100 ⎥\n⎢\n⎥\n⎣ 10101010 10101010 10101010 10101010 ⎦\n11111111 11111111 11111111 11111111\nThese generators already have distinct stopping times. Reducing G to trellis-oriented\nform, we obtain symmetrical starting times:\n⎡\n⎤\n11111111 00000000 00000000 00000000\n⎢ 00001111 11110000 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 00110011 11001100 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 01010101 10101010 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 00000000 11111111 00000000 00000000 ⎥\n⎢\n⎥\n⎢ 00000000 00001111 11110000 00000000 ⎥\n⎢\n⎥\n⎢ 00000000 00110011 11001100 00000000 ⎥\n⎢\n⎥\nG′ =\n⎢ 00000000 01010101 10101010 00000000 ⎥\n.\n⎢\n⎥\n⎢ 00000000 00000000 11111111 00000000 ⎥\n⎢\n⎥\n⎢ 00000011 00000011 11000000 11000000 ⎥\n⎢\n⎥\n⎢ 00000101 00000101 10100000 10100000 ⎥\n⎢\n⎥\n⎢ 00000000 00000000 00001111 11110000 ⎥\n⎢\n⎥\n⎢ 00010001 00010001 10001000 10001000 ⎥\n⎢\n⎥\n⎢ 00000000 00000000 00110011 11001100 ⎥\n⎢\n⎥\n⎣ 00000000 00000000 01010101 10101010 ⎦\n00000000 00000000 00000000 11111111\n\nFrom this trellis-oriented generator matrix, we can read off the dimensions sk and bk of the\nstate and branch spaces of an unsectionalized trellis, namely (first half only; the second\nhalf is symmetric):\nk\n\nsk 0\nbk\n<\n<\n<\n<\n<\n<\n<\n>\n<\n<\n<\n>\n<\n>\n>\n>\nThe last row of the table above shows starting times (<) and stopping times (>). From\nthese times and either our heuristic clustering rule or the LV optimal clustering rule,\nwe obtain section boundaries at k = {0, 8, 12, 16, 20, 24, 32}, state complexity profile\n{0, 64, 256, 64, 256, 64, 0} and branch complexity profile {128, 512, 512, 512, 512, 128} (very\nsimilar to the optimal sectionalization and profiles of the Golay code).\nA normal graph of the resulting sectionalized trellis is as follows:\n(14, 7)\n/ 8\n/\n(18, 9)\n/ 4\n/\n(18, 9)\n/ 4\n/\n(18, 9)\n/ 4\n/\n(14, 7)\n/ 8\n(18, 9)\n/ 4\n/\nComparing this graph to the earlier graph, we see that they differ only in the second and\nthird sections (8-tuples), where the trellis realization has two (18, 9) sections (constraint\ncodes) in series, whereas the earlier realization has one (14, 7) constraint code and one\n(18, 9) constraint code.\nSince the complexity of the max-product update rule or Viterbi algorithm update is\nroughly proportional to the constraint code (branch space) size, the total complexity\nof the Viterbi algorithm is roughly proportional to 2 · 128 + 4 · 512 = 2304, whereas\nthe complexity of the max-product algorithm with survivor memory (generalized Viterbi\nalgorithm) on the earlier graph is roughly proportional to 4 · 128 + 2 · 512 = 1536. (This\nassumes either that we use survivor memory in both algorithms to avoid decoding the same\nsections twice, or equivalently that we do not use survivor memory in either algorithm.)\nThus decoding the earlier graph is roughly 2/3 as complex as decoding the optimum\nsectionalized trellis. Again, we see that optimal cycle-free graphs can be more efficient\nthan optimal sectionalized trellises, but not by very much.\n\nProblem F.3 (50 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na brief proof. If a proposition is false, the proof will usually be a counterexample. Full\ncredit will not be given for correct answers without an adequate explanation.\n(a) The Euclidean image of an (n, k, d) binary linear block code is an orthogonal signal\nset if and only if k = log2 n and d = n/2.\nFalse. The Euclidean images of two binary words are orthogonal if and only if d = n/2,\nso all codewords must be at distance d = n/2 from each other. In a linear code, this\nhappens if and only if all nonzero codewords have weight d = n/2. However, all that is\nspecified is that the minimum distance is d = n/2, which is necessary but not sufficient.\nThe smallest counterexample is a (4, 2, 2) code; e.g., {0000, 1100, 0011, 1111}.\n(b) Every element β ∈F32 is the root of a binary polynomial f(x) ∈F2[x] of degree less\nthan or equal to 5.\nTrue. Every element β ∈F32 is a root of x32 + x, which factors into the product of all\nbinary irreducible polynomials whose degrees divide 5; i.e., of degree 1 or 5. So β must\nbe a root of a binary irreducible polynomial of degree 1 or 5.\n(In fact, 0 and 1 are the roots of the two binary irreducible polynomials of degree 1,\nnamely x and x + 1, so the remaining 30 elements of F32 must be roots of irreducible\npolynomials of degree 5, and there must be 6 such polynomials.)\n(c) If codewords in an (n, k, d) binary linear block code with d even are transmitted\nequiprobably over an AWGN channel using a standard 2-PAM map and are optimally\ndetected, then the minimum squared distance to any decision boundary is twice the min\nimum squared distance that is achieved if binary hard decisions are made first on each\nsymbol and then the resulting binary received word is optimally decoded.\nTrue. With a standard 2-PAM map {0, 1} →{±α}, the minimum squared distance\nbetween codewords is d2\n= 4α2d, so with optimum (minimum-distance) detection the\nmin\nminimum squared distance to any decision boundary is (dmin/2)2 = α2d. On the other\nhand, if hard decisions are made first and there are d/2 noise components of magnitude α\nin coordinates where the transmitted codeword differs from another codeword at Hamming\ndistance d (and 0 in all other coordinates), then a decoding error may be made, so such a\nvector reaches the decision boundary. The squared norm of such a noise vector is (d/2)α2 .\n(d) Capacity-approaching codes must have trellis complexity parameters that become arbi\ntrarily large as the Shannon limit is approached arbitrarily closely.\nMost likely true. The average dimension bound shows that the sizes of the maxi\nmal branch and state spaces are essentially lowerbounded by 2γc , which goes to infinity\nexponentially with n for any \"good\" sequence of codes. However, the effective coding\ngain needed to get to the Shannon limit is finite, so the question of whether the nominal\ncoding gain γc has to become arbitrarily large to get arbitrarily close to the Shannon limit\nremains open. However, all empirical evidence indicates that it does. (Credit based on\nthe quality of your discussion.)\n\n(e) If the points x in a lattice Λ are transmitted with unequal probabilities {p(x), x ∈ Λ}\n√\nover an AWGN channel and optimally detected, then Pr(E) ≈ Kmin(Λ)Q (dmin\n2 (Λ)/4σ2),\nwhere dmin\n2 (Λ) is the minimum squared distance between points in Λ, and Kmin(Λ) is the\naverage number of nearest neighbors to each transmitted point.\nFalse. The non-equiprobable condition does not exclude using only a subset A of the\nlattice points with minimum distance d2\nmin\n2 (Λ); e.g., a sublattice Λ′ ⊂ Λ. In\nmin(A) > d\n√\nthis case the argument of the Q (·) function in the UBE will be at least d2 (A)/4σ2 .\nmin\nAlso, with non-equiprobable signals, minimum-distance (Voronoi) decision regions are in\ngeneral not optimum, so the argument that justifies the UBE no longer holds."
    },
    {
      "category": "Resource",
      "title": "midterm02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/e22f07b085a00cadb730d1f66da290e8_midterm02.pdf",
      "content": "Midterm Quiz\n- You have 110 minutes to complete the quiz.\n- This is a closed-book quiz, except that three 8.5′′ × 11′′ sheets of notes are allowed.\n- Calculators are allowed (provided that erasable memory is cleared), but will probably\nnot be useful.\n- There are two problems on the quiz. The first is a seven-part problem, each part\nworth 10 points. The second consists of three unrelated true-false questions, each\nworth 10 points.\n- The problems are not necessarily in order of difficulty.\n- A correct answer does not guarantee full credit and a wrong answer does not guarantee\nloss of credit. You should concisely indicate your reasoning and show all relevant work.\nThe grade on each problem is based on our judgment of your level of understanding\nas reflected by what you have written.\n- If we can't read it, we can't grade it.\n- If you don't understand a problem, please ask.\nProblem M.1 (70 points)\nRecall that an M -simplex signal set is a set of M signals A = {aj ∈ RM -1 , 1 ≤ j ≤ M }\nin an (M - 1)-dimensional real space RM -1, such that, for some EA > 0,\nEA,\nif i = j;\nai, aj =\nE\n⟨\n⟩\n- M -\nA\n1 , if i = j.\nInitially we will assume that M is a power of 2, M = 2m, for some integer m.\n(a) Compute the nominal spectral efficiency ρ(A) and the nominal coding gain γc(A) of\nan M -simplex signal set A on an AWGN channel as a function of M = 2m .\n(b) What is the limit of the effective coding gain γeff (A) of an M -simplex signal set A as\nM →inf, at a target error rate of Pr(E) ≈ 10-5?\n(c) Give a method of implementing an (M = 2m)-simplex signal set A in which each\nsignal aj is a sequence of points from a 2-PAM signal set {±α}.\n\nNow consider a concatenated coding scheme in which\n- the outer code is an (n, k, d) linear code C over a finite field Fq with q = 2m, which\nhas Nd codewords of minimum nonzero weight;\n- outer q-ary code symbols are mapped into a q-simplex signal set A via a one-to-one\nmap s : F\n.\nq →A\nIf x = (x1, x2, . . . , xn) is an n-tuple in (Fq )n, then s(x) = (s(x1), s(x2), . . . , s(xn)) will\nbe called the Euclidean image of x. Let A′ = s(C) = {s(x), x ∈C} denote the signal set\nconsisting of the Euclidean images of all codewords x ∈C.\n(d) Compute the nominal spectral efficiency ρ(A′) of the concatenated signal set A′ on\nan AWGN channel. Is this signal set appropriate for the power-limited or the bandwidth-\nlimited regime?\n(e) Compute d2\nmin(A′), Kmin(A′), and γc(A′). Give a good estimate of an appropriately\nnormalized error probability for A′.\nNow consider the case in which C is an (n = q + 1, k = 2, d = q) linear code over Fq .\nm\n(f) Show that a code C with these parameters exists whenever q is a prime power, q = p .\nShow that all nonzero codewords in C have the same Hamming weight.\n(g) Show that the Euclidean image A′ = s(C) of C is a q2-simplex signal set.\nProblem M.2 (30 points)\nFor each of the propositions below, state whether the proposition is true or false, and\ngive a proof of not more than a few sentences. No credit will be given for correct answers\nwithout an adequate explanation.\n(a) Let p(t) be a complex L2 signal with Fourier transform P(f). If the set of time shifts\n{p(t -kT), k ∈Z} is orthonormal for some T > 0, then P(0) = 0.\n(b) Let s(C) be the Euclidean-space image of a binary linear block code C under a 2-PAM\nmap s : {0, 1} →{±α}. Then the mean m of the signal set s(C) is 0, unless there is some\ncoordinate in which all codewords of C have the value 0.\n(c) A polynomial f(z) ∈ Fq [z] satisfies f(β) = 0 for all β ∈ Fq if and only if f(z) is a\nmultiple of zq -z."
    },
    {
      "category": "Resource",
      "title": "midtermsol02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/233218e32dcdaa3019f0aba50e98414b_midtermsol02.pdf",
      "content": "Æ\n\nÆ\n\n!\n\n!\n\n!\n\n\"\n\n!\n\n#\n\n$\n\n!\n\n!\n\n%\n\n%\n\n$\n\n!\n\n!\n!\n\n&\n\n!\n\n\"\n\n#\n\n'\n\n$\n&\n\n!\n\n($)\n\n*\n\n+\n\n!\n'\n\n),\n-\n.\n\n'\n&/\n\n'\n\n!\n\n,(\n-\n\n-\n\n!\n\n!\"\n\n#\n\n$\n%\n\n&\n\n'\n(\"\n\n$\n%\n\n)$*+,\n\n-\n$\n\n!\n\n\"\n\n#\n\n#$\n\n%\n\n$\n\n&\n\n'\n\n&\n\n(\n\nÆ#\n\n)\n\n%\n\n$\n\n*\n.\n/\n\n$\n.\n\n$\n\n(\n\n#\n\n!\n\n$\n$ #\n\n.\n&\n\n-\n\n,\n$\n.\n\n$\n\n*\n\n*\n\n$\n\n!\n\n\"\n\n#\n\n$\n\n!\n\n#\n\n#\n\n%\n$\n\n#\n\"\n\n#\n\n#\n\n&\n'\n\n'\n\n#\n\n!\n!\n\n(\n\n#\n\n%\n$\n\n#\n\n%\n$\n) *\n!\n\n'\n\n$\n\n%\n$\n\n\"\n\n!\n\n+\n\n#\n$\n\n$\n\n$\n\n$\n\n,\n\n#\n,\n\n$\n\n$\n+\n\n-\n\n\"\n\n\"\n*.\n!\n\n/\n\n\"\n\n-\n\n!\n\n/\n\n\"\n\n!\n\n&\n\n%\n$\n\n\"\n\n!\n\n45*\n\n%\n$\n\n#\n\n$\n#\n\n$\n#\n\n%\n$\n\n$\n#\n\n$\n\n$\n\n!\n\n#\n\n#\n\n#\n\n#\n\n!\n\n7!\n\n%\n$\n\n#\n\n%\n$\n\n#\n\n%\n$\n\n$\n#\n\n$\n\n!\n\n#\n\n%\n$\n\n*!\n\n!\n\n$\n\n!\n\n!\n\n!\n\n#\n\n#\n\n#\n\n$\n\n$\n\n$\n\n!\n\n!\n\n\"\n\n$8\n\n\"38\n\n\"\n\n\"\n&4\n\n%\n$\n\n\"\n\n) *\n!\n\n*!\n\n$+\n\n9:\n\n$:;/\n<\n\n9\",\n\n\":;/\n\n9-,\n\n-:;/\n<\n\n90,\n\n0:;/\n\"\n\n93,\n\n8,;\n-\n\n!\"\n\n#\n\n$\n\n$\n%\n\nÆ\n\n!\n\n\"\n\n#\n\n$\n\n!\n\n!\n\n%\n&\n&\n\n'\n\n(\n\n)\n\n*\n\n+\n\n,\n\n-\n\n.\n\n/\n\n%\n0&1\n\n%\n0&1\n\n%\n)&\n-\n\n%\n0&1\n\n%\n0&1\n\n%)\n)0&1\n\n%4\n40&1\n)\n\n%3\n30&1\n\n%/\n/0&1\n)\n\n%+\n+0&1\n\n%5\n0&\n+36\n\n/4\n\n)/"
    },
    {
      "category": "Resource",
      "title": "final03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/072b278828c13d55ce164af1d98da21b_final03.pdf",
      "content": "Final Exam\n- You have 3 hours (9:00-12:00) to complete the test.\n- This is a closed-book test, except that five 8.5′′ × 11′′ sheets of notes are allowed.\n- Calculators are allowed (provided that erasable memory is cleared).\n- There are three problems on the quiz. The first is a 7-part problem, each part worth\n10 points. The second is a 5-part problem, each part worth 10 points. The third\nproblem consists of 4 unrelated true-false questions, each worth 10 points.\n- The problems are not necessarily in order of difficulty.\n- A correct answer does not guarantee full credit and a wrong answer does not guarantee\nloss of credit. You should concisely indicate your reasoning and show all relevant work.\nThe grade on each problem is based on our judgment of your level of understanding\nas reflected by what you have written.\n- If we can't read it, we can't grade it.\n- If you don't understand a problem, please ask.\n\nUncoded 2-PAM\n-1\n-2\nb (E)\nUncoded 2-PAM\nShannon Limit\nShannon Limit for ρ = 2\n-3\nP (E)\nP\ns\n-6\n-5\n-4\n-3\n-2\n-1\nUncoded QAM\nShannon Limit\n-4\n-5\n-6\n-2\n-1\nEb/No [dB]\nFigure 1. Pb(E) vs. Eb/N0 for uncoded binary PAM.\nUncoded QAM\nnorm\nSNR\n[dB]\nFigure 2. Ps(E) vs. SNRnorm for uncoded (M × M )-QAM.\n\nα\ndB\ndB\n(round numbers) (two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n\n6.02\n6.99\n9.03\n10.00\nTable 1. Values of certain small factors α in dB.\ncode\nρ\nγc\n(dB)\nNd\nKb\nγeff (dB)\ns\nt\n(8,7,2)\n1.75\n7/4\n2.43\n2.0\n(8,4,4)\n1.00\n3.01\n2.6\n(16,15,2)\n1.88\n15/8\n2.73\n2.1\n(16,11,4)\n1.38\n11/4\n4.39\n3.7\n(16, 5,8)\n0.63\n5/2\n3.98\n3.5\n(32,31, 2)\n1.94 31/16\n2.87\n2.1\n(32,26, 4)\n1.63\n13/4\n5.12\n4.0\n(32,16, 8)\n1.00\n6.02\n4.9\n(32, 6,16)\n0.37\n4.77\n4.2\n(64,63, 2)\n1.97 63/32\n2.94\n1.9\n(64,57, 4)\n1.78 57/16\n5.52 10416 183\n4.0\n(64,42, 8)\n1.31\n21/4\n7.20 11160 266\n5.6\n10 16\n(64,22,16) 0.69\n11/2\n7.40\n2604 118\n6.0\n10 14\n(64, 7,32)\n0.22\n7/2\n5.44\n4.6\nTable 2. Parameters of RM codes with lengths n ≤ 64.\n\n′\nProblem F.1 (70 points)\nIn this problem we will consider coded modulation schemes based on a one-to-one mapping\nt : F3 →A from the finite field F3 to a 3-simplex signal set A in R2 with energy E(A) per\nsymbol. The symbols from A will be transmitted by QAM modulation over a passband\nAWGN channel with single-sided power spectral density N0. In everything that follows,\nwe assume that the receiver performs optimal detection.\nThe amount of information that can be conveyed in one ternary symbol will be called one\ntrit. We will normalize everything \"per information trit;\" i.e.,\n- we will use Et/N0 as our normalized signal-to-noise ratio, where Et is the average\nenergy per information trit;\n- we will define the nominal spectral efficiency ρt as the number of information trits\nper two dimensions (t/2D) conveyed by a given transmission scheme; and\n- we will define Pt(E) as the probability of error per information trit.\n(a) What is the ultimate Shannon limit on Et/N0 in dB?\n(b) What is the baseline performance (Pt(E) vs. Et/N0) of the signal set A?\n(c) How far is this baseline performance from the ultimate Shannon limit at Pt(E) ≈10-5?\nLet C be the (4, 2, 3) linear \"tetracode\" over F3, and let t(C) be the Euclidean image of C\nunder the map t : F3 →A.\n(d) What are the state and branch complexities of a minimal trellis for C?\n(e) What is the performance (Pt(E) vs. Et/N0) of the signal set t(C)?\nNow let C′ be a linear rate-1/2 convolutional code over F3 with generator 2-tuple g(D) =\n(1 + D, 1 + 2D), and let t(C′) be the Euclidean image of C under the map t.\n(f) What are the state and branch complexities of a minimal trellis for C′?\n(g) What is the performance (Pt(E) vs. Et/N0) of t(C′)?\n\nProblem F.2 (50 points)\nConsider the (16, 7, 6) binary linear block code C generated by the following generator\nmatrix:\n⎡\n⎤\n1111 1100 0000 0000\n⎥\n⎢ 0101 1011 1000 0000\n⎢\n⎥\n⎥\n⎢ 1100 1001 0110 0000\n⎢\n⎥\n⎥\n⎢ 1001 1111 0101 0000\n.\n⎢\n⎥\n⎥\n⎢ 1010 0001 0100 1100\n⎢\n⎥\n⎦\n⎣ 1100 0101 0000 1010\n0011 0010 0100 1001\n(a) It is known that kmax(n, 6) = {0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 4, 4, 5, 6, 7} for 1 ≤ n ≤ 16.\nShow that there exist shortened codes of C that meet this bound for every n ≤ 16.\n(b) Give the state complexity profile and the branch complexity profile of a 16-section\nminimal trellis for C.\n(c) From the information given, is it possible to say whether another coordinate ordering\nmight give a less complex trellis for C?\n(d) Find the sectionalization that gives the minimum number of sections without increas\ning the maximum branch complexity. Give the state complexity profile and the branch\ncomplexity profile of the resulting trellis.\n(e) Count the number of arithmetic operations required by decoding using a straightfor\nward Viterbi algorithm of the trellises of parts (b) and (d). Which is less complex?\nProblem F.3 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) There exist sequences of Reed-Muller codes which can approach the Shannon limit\narbitrarily closely, but the trellis complexity of such a sequence of codes necessarily grows\nwithout limit.\n(b) Let G be a finite abelian group of order |G|, and let X and N be independent random\nvariables defined on G, where the probability distribution of N is uniform:\npN (n) = 1/|G|, ∀n ∈ G.\nThen Y = X + N is uniformly distributed over G and independent of X, regardless of\nthe distribution of X.\n(c) There exists no MDS binary linear block code with block length greater than 3.\n(d) Given an (n, k, d) linear block code over a finite field Fq and optimal erasure correction:\n(i) up to d - 1 erasures can always be corrected;\n(ii) up to n - k erasures may be able to be corrected;\n(iii) more than n - k erasures can never be corrected."
    },
    {
      "category": "Resource",
      "title": "final03solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/9bd6ff59c3b9bfb93032ada9e0fe07c1_final03solns.pdf",
      "content": "Final solutions\nProblem F.1 (70 points)\nIn this problem we will consider coded modulation schemes based on a one-to-one mapping\nt : F3 →A from the finite field F3 to a 3-simplex signal set A in R2 with energy E(A)\nper symbol. The symbols from A will be transmitted by QAM modulation over a passband\nAWGN channel with single-sided power spectral density N0. In everything that follows,\nwe assume that the receiver performs optimal detection.\nThe amount of information that can be conveyed in one ternary symbol will be called one\ntrit. We will normalize everything \"per information trit;\" i.e.,\n- we will use Et/N0 as our normalized signal-to-noise ratio, where Et is the average\nenergy per information trit;\n- we will define the nominal spectral efficiency ρt as the number of information trits\nper two dimensions (t/2D) conveyed by a given transmission scheme; and\n- we will define Pt(E) as the probability of error per information trit.\n(a) What is the ultimate Shannon limit on Et/N0 in dB?\nThe amount of information conveyed by one equiprobable ternary symbol is log2 3 bits.\nThus one trit is equal to log2 3 = log10 3/ log10 2 = 4.77/3.01 = 1.58 bits. Alternatively,\nwe can just take logarithms to the base 3 to measure information-theoretic quantities\ndirectly in trits; i.e., the amount of information conveyed by one equiprobable ternary\nsymbol is log3 3 = 1 trit.\nThe capacity of an AWGN channel is thus\nlog2(1 + SNR)\nCt =\n= log3(1 + SNR) t/2D.\nlog2 3\nThe signal energy per two dimensions is ρtEt, so SNR = ρtEt/N0. Thus for reliable\ntransmission\nρt ≤log3(1 + ρtEt/N0),\nwhich is equivalent to\n3ρt -1\nρ\nEt/N0 ≥\n.\nt\nAs ρt → 0, we have 3ρt = exp(ρt ln 3) → 1 + ρt ln 3, so this lower bound decreases\nmonotonically to\nlog10 3\n4.77\nln 3 =\n=\n= 1.10 (0.41 dB).\nlog10 e\n4.34\nAlternatively, since the ultimate Shannon limit on Eb/N0 is ln 2 (-1.59 dB), the ultimate\nShannon limit on Et/N0 is (ln 2)(log2 3) = ln 3 = 1.10 (0.41 dB).\n\n(b) What is the baseline performance (Pt(E) vs. Et/N0) of the signal set A?\nA\nA 3-simplex signal set A may be constructed by starting with a 3-orthogonal signal set\n′ and subtracting out the mean m(A′): A = A′ - m(A′). Then d2\nmin(A′), and\nmin(A) = d2\nbecause m(A) = 0, we have E(A′) = E(A) + ||m(A′)||2, or E(A) = E(A′) -||m(A′)||2 .\nTake A′ = {(α, 0, 0), (0, α, 0), (0, 0, α)}; then d2\nmin(A′) = 2α2 , E(A′) = α2, and m(A′) =\n(α, α, α)/3. Thus A = A′ - m(A′) has d2\nmin(A′) = 2α2, and E(A) = E(A′) -\nmin(A) = d2\n||m(A′)||2 = 2α2/3. We conclude that d2\nmin(A) = 3E(A).\nThe same conclusion could be reached by taking A′ to be the vertices of an equilateral\ntriangle in R2 centered on the origin, or from our general formulas for the inner products\nof an M -simplex signal set, namely ||aj ||2 = E(A); ⟨aj , aj ⟩ = -E(A)/(M - 1) if j = j′ .\nSince the energy per symbol or per trit is Et = E(A), and each signal in A has Kmin(A) =\n2 nearest neighbors, the union bound estimate (UBE) of the probability of error per\nsymbol (or per trit) is\n√\nd2\n\n√\nmin(A)\nPt(E) ≈ 2Q\n= 2Q\nEt/N0 .\n2N0\n(c) How far is this baseline performance from the ultimate Shannon limit at Pt(E) ≈ 10-5?\nThe baseline ternary curve of part (b) may by obtained by moving the baseline binary\n√\ncurve Pb(E) = Q (2Eb/N0) of Figure 1 to the right by the \"coding loss\" of 3 (-1.25 dB)\nand up by a factor of 2, which costs about 0.2 dB at Pt(E) ≈ 10-5. Thus we obtain\nPt(E) ≈ 10-5 when Et/N0 ≈ 9.6 + 1.25 + 0.2 ≈ 11 dB. This is about 10.6 dB from the\nultimate Shannon limit on Et/N0 of 0.4 dB.\nLet C be the (4, 2, 3) linear \"tetracode\" over F3, and let t(C) be the Euclidean image of C\nunder the map t : F3 →A.\n(d) What are the state and branch complexities of a minimal trellis for C?\nThe tetracode C meets the Singleton bound d + k ≤ n + 1 with equality, and therefore is\nMDS. Its trellis-oriented generator matrix thus must have the following form:\nxxx0\n.\n0xxx\nFrom this matrix we see that the state complexity profile of a minimal trellis for C is\n{1, 3, 9, 3, 1}, and the branch complexity profile is {3, 9, 9, 3}.\n(e) What is the performance (Pt(E) vs. Et/N0) of the signal set t(C)?\nWe first note that the minimum Hamming distance of C is 3, and that all 8 nonzero\ncodewords have weight 3, since C is MDS and thus Nd = 3 (3 - 1) = 8.\nThe minimum squared distance of t(C) is therefore 3d2\nmin(A), since every sequence in\nt(C) differs from every other by d2\nK\nmin(A) in 3 places. The number of nearest neighbors is\nmin(t(C)) = 8. (In fact, t(C) is a 9-simplex.)\n\nFinally, Et = 4E(A)/2 = 2E(A), and Pt(E) = 2 Pr(E). The union bound estimate\n(UBE) of the probability of error per information trit is thus\n√\n3d2\n\n√\nmin(A)\nPt(E) ≈ Kmin(t(C))Q\n= 4Q\nEt/N0 .\n2N0\n√\nIn other words, the nominal coding gain over the baseline curve Pt(E) ≈ 2Q ( 3 Et/N0)\nis γc(C) = kd/n = 3/2 (1.76 dB). Because of the doubling of the error coefficient, the\neffective coding gain at Pt(E) ≈ 10-5 is about 0.2 dB less, or about 1.55 dB.\nNow let C′ be a linear rate-1/2 convolutional code over F3 with generator 2-tuple g(D) =\n(1 + D, 1 + 2D), and let t(C′) be the Euclidean image of C′ under the map t.\n(f ) What are the state and branch complexities of a minimal trellis for C′?\nThe encoder for C′ has one memory element storing one trit. It therefore has 3 states.\nThere is a 3-way branch out of every state, so its branch complexity is 9. (The VA\ndecoding complexity of C′ is very nearly the same as that of C.)\n(g) What is the performance (Pt(E) vs. Et/N0) of t(C′)?\nK\nWe will first establish that the minimum Hamming distance of C′ is 4, and that there are\nt = 2 error events of weight 4 per unit time (per information trit).\nIn the trellis diagram of C′, the branch from the zero state to the zero state is labelled\n00. The branches leaving the zero state to nonzero states are labelled 11 and 22, and\nthe branches arriving at the zero state from nonzero states are labelled 12 and 21. The\nlabels of the 9 branches run through the 9 ternary linear combinations of 11 and 12,\nwhich comprise all of (F3)2 since 11 and 12 are linearly independent. The labels of the 4\nbranches from nonzero states to nonzero states therefore have Hamming weight 1. Thus\nevery nonzero trellis path from the zero state to the zero state has Hamming weight 2 in\nits first branch, 2 in its last branch, and 1 in every branch in between. We conclude that\nthe minimum Hamming weight is 4, and that only the 2 error events of length 2 (i.e.,\ng(D) and 2g(D)) have the minimum weight.\nThe minimum squared distance of t(C′) is therefore 4d2\nmin(A), since every sequence in t(C′)\ndiffers from every other by d2\nmin(A) in at least 4 places. The number of nearest neighbors\nper information trit is Kt = 2. The energy per information trit is Et = 2E(A).\nThe union bound estimate (UBE) of the probability of error per information trit is thus\n√\n4d2\n\n√\nmin(A)\nPt(E) ≈ KtQ\n= 2Q (3Et/N0) .\n2N0\n√\nIn other words, the nominal coding gain over the baseline curve Pt(E) ≈ 2Q ( 3 Et/N0) is\nγc(C) = kd/n = 2 (3.01 dB). Because the error coefficient is the same, the effective coding\ngain is also 3 dB at all Pt(E).\nIn summary, even though the block and convolutional codes have about the same VA\ndecoding complexity and the block code is as good as possible (MDS), the effective coding\ngain of the convolutional code is about 1.5 dB greater.\n\nProblem F.2 (50 points)\nConsider the (16, 7, 6) binary linear block code C generated by the following generator\nmatrix:\n⎡\n⎤\n1111 1100 0000 0000\n⎥\n⎢ 0101 1011 1000 0000\n⎢\n⎥\n⎥\n⎢ 1100 1001 0110 0000\n⎢\n⎥\n⎥\n⎢ 1001 1111 0101 0000\n.\n⎢\n⎥\n⎥\n⎢ 1010 0001 0100 1100\n⎢\n⎥\n⎦\n⎣ 1100 0101 0000 1010\n0011 0010 0100 1001\n(a) It is known that kmax(n, 6) = {0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 4, 4, 5, 6, 7} for 1 ≤ n ≤ 16.\nShow that there exist shortened codes of C that meet this bound for every n ≤ 16.\nBy inspection, for 1 ≤ n ≤ 16, the first kmax(n, 6) generators of C, shortened to the first\nn coordinates, generate an (n, kmax(n, 6)) binary linear block code. Since these codes are\neach a shortened code of C, which has minimum distance d = 6, each shortened code must\nhave minimum distance at least 6.\n(b) Give the state complexity profile and the branch complexity profile of a 16-section\nminimal trellis for C.\nWe first reduce the generator matrix above to trellis-oriented form, obtaining:\n⎡\n⎤\n1111 1100 0000 0000\n⎢ 0101 1011 1000 0000 ⎥\n⎢\n⎥\n⎢ 0011 0101 0110 0000 ⎥\n⎢\n⎥\n⎢ 0000 1101 1011 0000 ⎥ .\n⎢\n⎥\n⎢ 0000 0110 1100 1100 ⎥\n⎢\n⎥\n⎣ 0000 0001 1101 1010 ⎦\n0000 0000 0011 1111\nNote that this generator matrix is symmetrical, and that the stopping times\n(6, 9, 11, 12, 14, 15, 16) of the generators are the same as in the original generator ma\ntrix. The starting times are symmetrical, (1, 2, 3, 5, 6, 8, 11), and thus the shortened codes\ngenerated by the last k generators also meet the bound of part (a).\nFrom this trellis-oriented generator matrix, we find that the state complexity profile is\n{1, 2, 4, 8, 8, 16, 16, 16, 32, 16, 16, 16, 8, 8, 4, 2, 1},\nand that the branch complexity profile is\n{2, 4, 8, 8, 16, 32, 16, 32, 32, 16, 32, 16, 8, 8, 4, 2}.\n\n(c) From the information given, is it possible to say whether another coordinate ordering\nmight give a less complex trellis for C?\nThe past subcode at time n is generated by the generators that stop by time n, and\nthe future subcode at time n is generated by the generators that start after time n. By\npart (a), the past subcode always has the largest dimension kmax(n, 6) that it could have,\nand by part (b) the same is true of the future subcodes. Therefore the Muder bound\ndim Sn ≥dim C -kmax(n, 6) -kmax(17 -n, 6) is met at all times, so no (16, 7, 6) code\ncould have a better state or branch complexity profile with any coordinate ordering.\n(d) Find the sectionalization that gives the minimum number of sections without increas\ning the maximum branch complexity. Give the state complexity profile and the branch\ncomplexity profile of the resulting trellis.\nUsing the heuristic clustering rule of Chapter 10 (or the LV rule), the section at time 8\nmay be extended back to time 7 and forward to time 10 before meeting the first or last\ngenerator, giving a central section of length 4 with branch complexity 32. Similarly, the\nsection at time 6 may be extended back to the beginning and the symmetrical section\nat time 11 may be extended to the end, giving first and last sections of length 6 with\nbranch complexity 32. In short, this sectionalization gives a 3-section trellis with state\ncomplexity profile {1, 16, 16, 1} and branch complexity profile {32, 32, 32}.\n(e) Count the number of arithmetic operations required by decoding using a straightforward\nViterbi algorithm of the trellises of parts (b) and (d). Which is less complex?\nThe 16 branch types, sizes and corresponding number of add and compare operations in\na standard Viterbi algorithm decoding of the 16-section trellis are as follows\ntime\n\ntype\n<\n<\n<\n= <\n▷◁\n=\n<\n>\n= ▷◁\n>\n=\n>\n>\n>\nsize\n\nadds\n\ncomps\n\nThere are therefore a total of 234 additions (of two variables) and 63 2-way comparisons.\nThe three-section trellis has 32 branches of length 6 in the first section, which require\n32 × 5 = 160 additions of single-symbol metrics in a straightforward implementation. At\nthe end of the first section, 16 2-way comparisons are required. In the second section there\nare 32 branches of length 4, which require 32 × 4 = 128 additions, followed by 16 2-way\ncomparisons. The final section has 32 branches of length 6, which require 32 × 6 = 192\nadditions, followed by a single 32-way comparison, equivalent to 31 2-way comparisons.\nThus the total number of 2-way comparisons is 63, the same as for the unsectionalized\ntrellis, but the total number of additions is 480, about twice as much. Evidently the\nunsectionalized trellis organizes the metric additions more efficiently.\nHowever, the metric additions could be organized in a similar way in the sectionalized\ntrellis, and the sectionalized trellis logic is less complex.\n\nProblem F.3 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) There exist sequences of Reed-Muller codes which can approach the Shannon limit\narbitrarily closely, but the trellis complexity of such a sequence of codes necessarily grows\nwithout limit.\nTrue. For m ≥0, the Euclidean image of the first-order Reed-Muller code RM(1, m) =\n(2m, m + 1, 2m-2) is a 2m+1-biorthogonal signal set. It is known that as M →inf the\nprobability of decoding error with M-biorthogonal signal sets goes to zero whenever\nEb/N0 > ln 2 (-1.59 dB), the ultimate Shannon limit on Eb/N0.\nA minimal trellis for the RM(1, m) = (2m, m+1, 2m-2) code has 2m-1 states at the central\nstate space, since the |u|u + v| construction gives the optimal coordinate ordering for RM\ncodes, and under the |u|u + v| construction the dimension of the central state space S is\ndim S = dim RM(1, m -1) -dim RM(0, m -1) = m -1.\nThus the trellis complexity of the RM(1, m) codes goes to infas m →inf.\nIt is possible that there exist other sequences of RM codes whose performance approaches\nthe relevant Shannon limit. However, the length of these codes must go to infinity in order\nto approach the Shannon limit arbitrarily closely, and the trellis complexity of an RM\ncode RM(r, m) other than the universe, SPC and repetition codes (whose performance\ndoes not approach the Shannon limit) is lowerbounded by the trellis complexity of the\nRM(1, m) code.\n(b) Let G be a finite abelian group of order |G|, and let X and N be independent random\nvariables defined on G, where the probability distribution of N is uniform:\npN (n) = 1/|G|, ∀n ∈G.\nThen Y = X + N is uniformly distributed over G and independent of X, regardless of the\ndistribution of X.\nTrue. The conditional probability distribution of Y given x is then uniform:\npY |X (y | x) = pN (y -x) = 1/|G|, ∀x, y ∈G;\ni.e., pY |X (y | x) is uniform independent of x. Thus Y is uniform:\npY (y) =\npY |X (y | x)pX (x) = 1/|G|, ∀y ∈G.\nx∈G\nMoreover, since pY |X (y | x) = pY (y), ∀x ∈G, Y is independent of X.\nThis is the principle of the \"one-time pad\" in cryptography, which ensures that the en\ncrypted text Y is independent of the plaintext X. This principle is also the basis of\nvarious \"scrambling\" and \"dither\" processes used in data communications to ensure that\nthe transmitted signal is quasi-random, regardless of the actual data.\n\n′′\n(c) There exists no MDS binary linear block code with block length greater than 3.\nFalse. An (n, k, d) linear code is MDS if it meets the Singleton bound, d + k ≤ n + 1.\nThe (n, n, 1) binary universe code, the (n, n - 1, 2) binary single-parity-check code, and\nthe (n, 1, n) binary repetition code are thus all MDS for any n ≥ 1.\n(d) Given an (n, k, d) linear block code over a finite field Fq and optimal erasure correc\ntion:\n(i) up to d - 1 erasures can always be corrected;\n(ii) up to n - k erasures may be able to be corrected;\n(iii) more than n - k erasures can never be corrected.\nTrue. Optimal erasure correction is a matter of finding a codeword c that agrees with\nthe received word r in the set J of unerased places; i.e., such that the projections onto J\nagree: c|J = r|J . Since the transmitted codeword c always satisfies this condition, optimal\nerasure correction fails only if there is some other codeword c′ such that c′\n|J = c|J , in\nwhich case there is no way to choose between c and c′ . By linearity, such a codeword c′\nexists if and only if there exists a nonzero codeword c′′ = c - c′ such that c|J = 0|J .\nIn case (i), if there are fewer than d erasures, then no such ambiguous case can arise, since\nthe minimum Hamming weight of any nonzero codeword is d.\nIn case (ii), erasure correction is possible if and only if the projection of the code onto J\nis one-to-one; i.e., if and only if J includes an information set for the code. (In the case of\nMDS codes every set of size k is an information set, but on the other hand n - k = d - 1.)\nIn case (iii), unambiguous erasure correction is never possible, since the dimension of the\ncode is k, so if there are fewer than k unerased places in J then the projection of the code\nonto J cannot possibly be one-to-one; i.e., a set J of size less than k cannot possibly\ninclude an information set."
    },
    {
      "category": "Resource",
      "title": "midterm03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/13a4ba8a120cea693cbc1959a80c025b_midterm03.pdf",
      "content": "Midterm\n- You have 110 minutes (9:05-10:55 am) to complete the test.\n- This is a closed-book test, except that three 8.5′′ × 11′′ sheets of notes are allowed.\n- Calculators are allowed (provided that erasable memory is cleared).\n- There are two problems on the quiz. The first is a six-part problem, each part worth\n10 points. The second problem consists of four unrelated true-false questions, each\nworth 10 points.\n- The problems are not necessarily in order of difficulty.\n- A correct answer does not guarantee full credit and a wrong answer does not guarantee\nloss of credit. You should concisely indicate your reasoning and show all relevant work.\nThe grade on each problem is based on our judgment of your level of understanding\nas reflected by what you have written.\n- If we can't read it, we can't grade it.\n- If you don't understand a problem, please ask.\n\nUncoded 2-PAM\n-1\n-2\nb (E)\nUncoded 2-PAM\nShannon Limit\nShannon Limit for ρ = 2\n-3\nP (E)\nP\ns\n-6\n-5\n-4\n-3\n-2\n-1\nUncoded QAM\nShannon Limit\n-4\n-5\n-6\n-2\n-1\n\nEb/No [dB]\nFigure 1. Pb(E) vs. Eb/N0 for uncoded binary PAM.\nUncoded QAM\n\nnorm\nSNR\n[dB]\nFigure 2. Ps(E) vs. SNRnorm for uncoded (M × M )-QAM.\n\nα\ndB\ndB\n(round numbers) (two decimal places)\n0.00\n1.25\n0.97\n3.01\n2.5\n3.98\ne\n4.3\n4.34\n4.8\n4.77\nπ\n4.97\n6.02\n6.99\n9.03\n10.00\nTable 1. Values of certain small factors α in dB.\ncode\nρ\nγc\n(dB)\nNd\nKb\nγeff (dB)\ns\nt\n(8,7,2)\n1.75\n7/4\n2.43\n2.0\n(8,4,4)\n1.00\n3.01\n2.6\n(16,15,2)\n1.88\n15/8\n2.73\n2.1\n(16,11,4)\n1.38\n11/4\n4.39\n3.7\n(16, 5,8)\n0.63\n5/2\n3.98\n3.5\n(32,31, 2) 1.94 31/16\n2.87\n2.1\n(32,26, 4) 1.63\n13/4\n5.12\n4.0\n(32,16, 8) 1.00\n6.02\n4.9\n(32, 6,16) 0.37\n4.77\n4.2\n(64,63, 2) 1.97 63/32\n2.94\n1.9\n(64,57, 4) 1.78 57/16\n5.52 10416 183\n4.0\n(64,42, 8) 1.31\n21/4\n7.20 11160 266\n5.6\n10 16\n(64,22,16) 0.69\n11/2\n7.40\n2604 118\n6.0\n10 14\n(64, 7,32) 0.22\n7/2\n5.44\n4.6\nTable 2. Parameters of RM codes with lengths n ≤ 64.\n\nProblem M.1 (60 points)\nYour boss wants you to do a feasibility study for a digital communication system with\nthe following characteristics.\nYou are allowed to use the frequency band B between 953 and 954 MHz. The allowed\nsignal power is P = 106 power units. The noise in the band is additive white Gaussian\nnoise with single-sided power spectral density N0 = 1 power units per Hz.\nFor the purposes of the feasibility study, you may assume optimally bandwidth-efficient\nmodulation, ideal brick-wall (zero-rolloff) filters, perfect receiver synchronization, etc.\n(a) What is the Shannon limit on the achievable data rate R in bits per second (b/s)?\n(b) What is the maximum data rate R that can be achieved with uncoded modulation, if\nthe target error rate is of the order of 10-5?\n(c) Suppose that for complexity reasons you are restricted to using Reed-Muller codes\nwith block length n ≤ 64. What is the maximum data rate R that can be achieved, if the\ntarget error rate is of the order of 10-5?\nNow let the allowed signal power be only P = 105 power units, with all else the same.\n(d) What is the Shannon limit on the achievable data rate R in bits per second (b/s)?\n(e) What is the maximum data rate R that can be achieved with uncoded modulation, if\nthe target error rate is of the order of 10-5?\n(f) Suppose that you are allowed to use any code that has been introduced in this course\nso far. What is the maximum data rate R that can be achieved, if the target error rate is\nof the order of 10-5?\n\nProblem M.2 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) Let A = {aj (t), 1 ≤j ≤M} be a set of M real L2 signals, and let the received signal\nbe r(t) = x(t) + n(t), where x(t) is a signal in A, and n(t) is additive (independent) white\nGaussian noise. Then, regardless of whether the signals in A are equiprobable or not, it\nis possible to do optimal detection on r(t) by first computing from r(t) a real M-tuple\nr = (r1, r2, . . . , rM ), and then doing optimal detection on r.\n(b) Let A be an arbitrary M-point, N-dimensional signal constellation, and let A′ =\nαUAK be the constellation obtained by taking the K-fold Cartesian product AK , scaling\nby α > 0, and applying an orthogonal transformation U. Then the effective coding gain\nof A′ is the same as that of A.\n(c) Let {Cj , j = 1, 2, . . . } be an infinite set of binary linear (nj , kj , dj ) block codes Cj with\nnj →infas j →inf. Then in order for the performance of these codes in AWGN to\napproach the Shannon limit as j →inf, it is necessary that either limj→inf kj /nj > 0 or\nlimj→inf dj /nj > 0.\n(d) The Euclidean-space image s(C) of a binary linear block code C under the 2-PAM map\n{s(0) = +α, s(1) = -α} has zero mean, m(s(C)) = 0, unless there is some coordinate\nposition in which all codewords in C have value 0."
    },
    {
      "category": "Resource",
      "title": "midterm03solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/a05a253379aba3efbd400e8b82dde603_midterm03solns.pdf",
      "content": "Midterm solutions\nProblem M.1 (60 points)\nYour boss wants you to do a feasibility study for a digital communication system with the\nfollowing characteristics.\nYou are allowed to use the frequency band B between 953 and 954 MHz. The allowed\nsignal power is P = 106 power units. The noise in the band is additive white Gaussian\nnoise with single-sided power spectral density N0 = 1 power units per Hz.\nFor the purposes of the feasibility study, you may assume optimally bandwidth-efficient\nmodulation, ideal brick-wall (zero-rolloff ) filters, perfect receiver synchronization, etc.\n(a) What is the Shannon limit on the achievable data rate R in bits per second (b/s)?\nAn AWGN channel is completely specified by its bandwidth W and signal-to-noise ratio\nSNR. The channel bandwidth is W = |B| = 106 Hz, and the SNR is SNR = P/(N0W) = 1\n(0 dB). The channel capacity in b/s is therefore\nCb/s = W log2(1 + SNR) = 106 b/s.\nWe see that the nominal spectral efficiency is limited to ρ < Cb/2D = 1 b/2D (or (b/s)/Hz),\nso we are in the power-limited regime.\n(b) What is the maximum data rate R that can be achieved with uncoded modulation, if\nthe target error rate is of the order of 10-5?\nIn the power-limited regime, we use 2-PAM or (2 × 2)-QAM for uncoded modulation.\nFrom Figure 1, to achieve Pb(E) ≈ 10-5 requires Eb/N0 ≈ 9.6 dB, or Eb/N0 ≈ 9. Since\nEb = P/R and P = 106 , N0 = 1, this implies that\nR ≈ P/9 ≈ 110, 000 b/s,\na factor of about 9 (9.6 dB) less than capacity.\nA number of you attacked this problem by varying ρ rather than R. Although this gives\nthe right answer, it is not correct in principle because the nominal spectral efficiency of\n2-PAM is a constant, ρ = 2 b/2D.\nNote that the Nyquist (nominal) bandwidth at R = 110, 000 b/s is only about 55 KHz;\ni.e., with bandwidth-efficient modulation, only about 1/18 of the available channel band-\nwidth W = 1 MHz will be used.\n\n(c) Suppose that for complexity reasons you are restricted to using Reed-Muller codes with\nblock length n ≤ 64. What is the maximum data rate R that can be achieved, if the target\nerror rate is of the order of 10-5?\nFrom Table 2, we see that RM codes of length 64 can achieve up to about 6 dB of\neffective coding gain at Pb(E) ≈ 10-5 . Thus to achieve Pb(E) ≈ 10-5 could require only\nEb/N0 ≈ 3.6 dB, or Eb/N0 ≈ 9/4 = 2.25. Again using Eb = P/R and P = 106 , N0 = 1,\nthis implies that\nR ≈ 4P/9 ≈ 440, 000 b/s,\n4 times the achievable rate of uncoded modulation. The required spectral efficiency at\nthis data rate is ρ ≥ 0.44 (b/s)/Hz.\nThus we can use the (64, 22, 16) RM code, which has a nominal spectral efficiency of\nρ = 2k/n = 11/16 = 0.6875 b/2D, and an effective coding gain of 6.0 dB. (We could not\nuse the (64, 7, 32) biorthogonal code, whose spectral efficiency is only 3/16 = 0.1875.)\nUsing the (64, 22, 16) code at a rate of R ≈ 440, 000 b/s, we will operate at Eb/N0 ≈ 2.25\n(3.6 dB) and thus obtain Pb(E) ≈ 10-5, according to the union bound estimate. The\nnominal (Nyquist) bandwidth will be\n16 440, 000\n·\nW = R/ρ ≈\n= 640 KHz,\nwhich is well within the 1 MHz available.\nNow let the allowed signal power be only P = 105 power units, with all else the same.\n(d) What is the Shannon limit on the achievable data rate R in bits per second (b/s)?\nThe channel bandwidth is still W = 106 Hz, but the SNR is now SNR = 0.1 (-10 dB).\nThe channel capacity in b/s is therefore\nCb/s = W log2(1 + SNR) = W (log2 1.1) ≈ (0.1375)W = 137, 500 b/s.\nSince ρ ≤ 0.1375 b/2D, we are now deep into the power-limited regime.\n(e) What is the maximum data rate R that can be achieved with uncoded modulation, if\nthe target error rate is of the order of 10-5?\nAgain using 2-PAM or (2 × 2)-QAM, to achieve Pb(E) ≈ 10-5 requires Eb/N0 ≈ 9.6 dB,\nor Eb/N0 ≈ 9. Since Eb = P/R and P = 105 , N0 = 1, this implies that\nR ≈ P/9 ≈ 11, 000 b/s,\na factor of about 9 log2 e (11.2 dB) less than capacity. Since we still have ρ = 2 b/2D, the\nNyquist (nominal) bandwidth is now only about 5.5 KHz.\n\n(f) Suppose that you are allowed to use any code that has been introduced in this course\nso far. What is the maximum data rate R that can be achieved, if the target error rate is\nof the order of 10-5?\nThe best RM code in Table 2 is the (64, 22, 16) code, which has an effective coding gain\nof 6.0 dB. Repeating the calculations in part (c) above for one tenth the power, we find\nthat this code can support a rate of about\nR ≈4P/9 ≈44, 000 b/s,\n4 times (6 dB) more than what we can achieve with uncoded modulation, but 3.3 times\n(5.2 dB) less than capacity.\nThe only potentially better codes that we know of so far are the orthogonal-simplex-\nbiorthogonal family, whose performance we know approaches the ultimate Shannon limit\non Eb/N0 as M →inf, albeit with ρ →0.\nLet us therefore try biorthogonal codes, the most bandwidth-efficient in this family. We\nknow that the binary image of a (2m, m + 1, 2m-1) binary linear block code is a 2m+1-\nbiorthogonal code.\nIf we are going to achieve a rate higher than 44 kb/s, then we are going to need an effective\ncoding gain greater than 6 dB and a spectral efficiency greater than 0.044 b/2D.\nThe nominal coding gain of a (2m, m + 1, 2m-1) code is γc = kd/n = (m + 1)/2, so we\nare going to need to choose m ≥8. The spectral efficiency is ρ = 2k/n = (m + 1)2-m+1 ,\nwhich limits us to m ≤8, since for m = 9, ρ = 10/256 = 0.039 b/2D.\nLet us therefore try the (256, 9, 128) biorthogonal code. This code has a nominal coding\ngain of γc = kd/n = 4.5 (6.53 dB). It has Nd = 29 -2 = 510 weight-128 codewords,\nso Kb = Nd/k ≈57. Since this is about 6 factors of two, its effective coding gain is\nonly about 5.3 dB, which is not good enough to improve on the (64, 22, 16) code. So we\nconclude that no biorthogonal code can improve on the (64, 22, 16) code in this problem.\nAnother approach is to examine RM codes of greater length. However, you were not\nexpected to go further in this problem; consideration only of RM codes of lengths n ≤64\nand biorthogonal codes suffices for full credit.\n[Extra credit] However, if you do go further, you will find for example that there\nis a (128, 29, 32) RM code which can be constructed from the (64, 22, 16) code and the\n(64, 7, 32) code. This code has a nominal coding gain of γc = kd/n = 29/4 = 7.25 (8.6\ndB), and a nominal spectral efficiency of ρ = 2k/n = 29/64 = 0.45, which are both fine.\nIts number of nearest neighbors is\n127 63 31 15 7\n·\n·\n·\n·\nNd = 4\n= 10668,\n31 15 7 3 1\n·\n·\n·\n·\n\nγ\nso Kb = Nd/k ≈368, which is about 8.5 binary orders of magnitude. Its effective coding\ngain by our rule of thumb (which is questionable for such large numbers) is therefore about\neff = 8.6 -1.7 ≈6.9 dB. If this estimate is accurate, then the data rate can be improved\nby 0.9 dB, or about 23%; i.e., to R ≈54, 000 b/s. However, the decoding complexity\nincreases very significantly (trellis complexity = 215 states).\nGrade distribution on Problem 1 (N = 10): {27, 28, 29, 34, 37, 41, 41, 47, 49, 52}.\n\nProblem M.2 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) Let A = {aj (t), 1 ≤j ≤M} be a set of M real L2 signals, and let the received signal\nbe r(t) = x(t) + n(t), where x(t) is a signal in A, and n(t) is additive (independent) white\nGaussian noise. Then, regardless of whether the signals in A are equiprobable or not,\nit is possible to do optimal detection on r(t) by first computing from r(t) a real M-tuple\nr = (r1, r2, . . . , rM ), and then doing optimal detection on r.\nTrue. The signal space S spanned by A (i.e., the set of all real linear combinations\nof signals in A) is a real vector space with at most M dimensions (with equality if and\nonly if the signals in A are linearly independent). It therefore has an orthonormal basis\n{φj (t), 1 ≤j ≤dim S} consisting of dim S ≤M orthonormal signals φj (t) ∈L2. By\nthe theorem of irrelevance, the set of dim S ≤M inner products {rj = ⟨r(t), φj (t)⟩, 1 ≤\nj ≤dim S} is a set of sufficient statistics for detection of signals in S in the presence of\nadditive white Gaussian noise, regardless of their statistics.\nIt is also true that the set of M inner products {r′ = ⟨r(t), aj (t)⟩, 1 ≤j ≤M} (the\nj\noutputs of a bank of matched filters matched to each signal in A) is another set of\nsufficient statistics for detection of signals in A in AWGN.\n(b) Let A be an arbitrary M-point, N-dimensional signal constellation, and let A′ =\nαUAK be the constellation obtained by taking the K-fold Cartesian product AK , scaling\nby α > 0, and applying an orthogonal transformation U. Then the effective coding gain\nof A′ is the same as that of A.\nTrue. The effective coding gain at any target error rate is determined by the curve of\nPb(E) vs. Eb/N0 in the power-limited regime, or of Ps(E) vs. SNRnorm in the bandwidth-\nlimited regime. Both of these curves have been shown in homework problems to be\ninvariant to scaling, orthogonal transformations, and the taking of Cartesian products.\n(c) Let {Cj , j = 1, 2, . . . } be an infinite set of binary linear (nj , kj , dj ) block codes Cj with\nnj →infas j →inf. Then in order for the performance of these codes in AWGN to\napproach the Shannon limit as j →inf, it is necessary that either limj→inf kj /nj > 0 or\nlimj→inf dj /nj > 0.\nMost probably false. The nominal coding gain is γc = kd/n, and therefore we could\nhave γc →infwhile both k/n →0 and d/n →0; e.g., if k ∝n2/3 and d ∝n2/3. We\nknow that biorthogonal codes approach the Shannon limit for ρ →0 with k/n →0 while\nd/n →2 , and we believe that rate-1/2 RM codes approach the Shannon limit for ρ = 1\nwith d/n →0 while k/n →2 . So there is every reason to believe that there should be\nsequences of intermediate codes that approach the Shannon limit for ρ →0 with k/n →0\nand d/n →0. [Credit given for the quality of your discussion.]\n\n(d) The Euclidean-space image s(C) of a binary linear block code C under the 2-PAM map\n{s(0) = +α, s(1) = -α} has zero mean, m(s(C)) = 0, unless there is some coordinate\nposition in which all codewords in C have value 0.\nTrue. This proposition holds if and only if half the codewords in any binary linear code\nC\nC have a 0 in any coordinate position, and half have a 1 (unless all are 0).\nTo show this, take a given coordinate position, and assume that there is some codeword\nx ∈C that has a 1 in that coordinate position. Then we can partition the codewords\nC into |C|/2 sets of pairs (y, x + y), y ∈C (the cosets of the one-dimensional subcode\n′ = {0, x} in C). If one member of a pair (y, x + y) has a 1 in the given coordinate\nposition, then the other has a 0, and vice versa. Hence precisely half the codewords in\nC have a 1 in any given coordinate position, unless none do. This argument holds for all\ncoordinate positions.\nGrade distribution on Problem 2 (N = 10): {10, 14, 16, 28, 30, 30, 30, 30, 30, 32}.\nGrade distribution on Midterm (N = 10): {43, 44, 57, 57, 60, 65, 71, 77, 79, 82}."
    },
    {
      "category": "Resource",
      "title": "final05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/0faea0dcb211a416b56436600236a0a7_final05.pdf",
      "content": "6.451 Principles of Digital Communication II\nTuesday, May 17, 2005\nMIT, Spring 2005\nHandout #25\nFinal\n- You have 180 minutes (9:00-12:00 am) to complete the test.\n- This is a closed-book test, except that five 8.5′′ × 11′′ sheets of notes are allowed.\n- Calculators are allowed (provided that erasable memory is cleared).\n- There are three problems on the quiz. The first is a five-part problem, each part\nworth 10 points. The second is a six-part problem, each part worth 10 points. There\nis also an optional seventh part, for which you can receive up to 10 points of extra\ncredit. The third problem consists of four unrelated true-false questions, each worth\n10 points.\n- The problems are not necessarily in order of difficulty.\n- Even if you can't prove a proposition stated in one part of a problem, you may assume\nthat it is true in subsequent parts.\n- A correct answer does not guarantee full credit and a wrong answer does not guarantee\nloss of credit. You should concisely indicate your reasoning and show all relevant work.\nThe grade on each problem is based on our judgment of your level of understanding\nas reflected by what you have written.\n- If we can't read it, we can't grade it.\n- If you don't understand a problem, please ask.\n\nProblem F.1 (50 points)\nIn this problem, we will investigate ML decoding using the standard VA on a minimal\nconventional trellis, or using a modified VA on a tail-biting trellis.\nConsider the (6, 3, 3) binary linear code C that is generated by the following generator\nmatrix:\nG =\n\n(a) Find the state and branch complexity profiles of an unsectionalized minimal trellis for\nC. Draw the corresponding minimal trellis for C, and label each branch with the corre-\nsponding output symbol. Verify that there is a one-to-one map between the codewords of\nC and the paths through the trellis.\n(b) Show that the following unsectionalized 2-state tail-biting trellis (TBT) realizes C.\n(Recall that in a TBT there may be more than one state in Σ0 = Σ6, the starting and\nending state space, and that the valid paths are those that start and end in the same state\nin Σ0 = Σ6.) Verify that there is a one-to-one map between the codewords of C and the\nvalid paths through the tail-biting trellis. Draw a normal graph of this tail-biting trellis\nrealization of C.\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\nn\nHHHH\n\nn\nn\nFigure 1. Tail-biting trellis for C.\n(c) Propose a modification to the Viterbi algorithm that finds the maximum-likelihood\n(ML) codeword in C, using the tail-biting trellis of part (b). Compare the complexity\nof your modified VA to that of the standard VA operating on the minimal conventional\ntrellis of part (a).\n(d) For a general (n, k) linear code C with a given coordinate ordering, is it possible to find\na minimal unsectionalized tail-biting trellis that simultaneously minimizes the complexity\nof each of the n state spaces over all tail-biting trellises? Explain.\n(e) Given a general linear code C and an unsectionalized tail-biting trellis for C, propose\na modified VA that performs ML decoding of C using the tail-biting trellis. In view of\nthe cut-set bound, is it possible to achieve a significant complexity reduction over the\nstandard VA by using such a modified VA?\n\nProblem F.2 (60 points)\nIn this problem, we will analyze the performance of iterative decoding of a rate-1/3 repeat-\naccumulate (RA) code on a binary erasure channel (BEC) with erasure probability p, in\nthe limit as the code becomes very long (n →inf).\n(3, 1, 3)\n-\ninfo bits\n- Π\n-\nu(D)\n1+D\n-\ny(D)\nFigure 2. Rate- 1\n3 RA encoder.\nThe encoder for the rate-1/3 RA code is shown in Figure 2 above, and works as follows.\nA sequence of information bits is first encoded by an encoder for a (3, 1, 3) repetition\ncode, which simply repeats each information bit three times. The resulting sequence is\nthen permuted by a large pseudo-random permutation Π. The permuted sequence u(D)\nis then encoded by a rate-1/1 2-state convolutional encoder with input/output relation\ny(D) = u(D)/(1 + D); i.e., the input/output equation is yk = uk + yk-1, so the output\nbit is simply the \"accumulation\" of all previous input bits (mod 2).\n(a) Show that this rate- 1\n3 RA code has the normal graph of Figure 3.\nΠ\n. . .\n. . .\n+\n=\n+\n=\n+\n=\n+\n=\n+\n=\n+\n=\n=@\n@\n@\n=@\n@\n@\n. . .\n. . .\nFigure 3. Normal graph of rate- 1\n3 RA code.\n(b) Suppose that the encoded bits are sent over a BEC with erasure probability p. Explain\nhow iterative decoding works in this case, using a schedule that alternates between the\nleft constraints and the right constraints.\n(c) Show that, as n →inf, if the probability that a left-going iterative decoding message\nis erased is qr→l, then the probability that a right-going message is erased after a left-side\nupdate is given by\nql→r = (qr→l)2.\n\n(d) Similarly, show that if the probability that a right-going iterative decoding message\nis erased is ql→r, then the probability that a left-going message is erased after a right-side\nupdate is given by\nqr→l= 1 -\n(1 -p)2\n(1 -p + pql→r)2.\n[Hint: observe that as n →inf, the right-side message probability distributions become\ninvariant to a shift of one time unit.]\n(e) Using a version of the area theorem that is appropriate for this scenario, show that\niterative decoding cannot succeed if p ≥2\n3.\n(f) The two curves given in parts (c) and (d) are plotted in the EXIT chart below for\np = 0.5. Show that iterative decoding succeeds in this case.\nqr→l\nql→r\n0.75\np = 0.5\nFigure 4. EXIT chart for iterative decoding of a rate- 1\n3 RA code on a BEC with p = 0.5.\n(g) [Optional; extra credit.] Determine whether or not iterative decoding succeeds for\np = 0.6.\n\nProblem F.3 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) The Euclidean image of an (n, k, d) binary linear block code is an orthogonal signal\nset if and only if k = log2 n and d = n/2.\n(b) If a possibly catastrophic binary rate-1/n linear convolutional code with polynomial\nencoder g(D) is terminated to a block code Cμ = {u(D)g(D) | deg u(D) < μ}, then a set\nof μ shifts {Dkg(D) | 0 ≤k < μ} of g(D) is a trellis-oriented generator matrix for Cμ.\n(c) Suppose that a codeword y = (y1, y2, . . . , yn) in some code C is sent over a memoryless\nchannel, such that the probability of an output r = (r1, r2, . . . , rn) is given by p(r | y) =\nQn\n1 p(ri | yi). Then the a posteriori probability p(Y1 = y1, Y2 = y2 | r) is given by\np(Y1 = y1, Y2 = y2 | r) ∝p(Y1 = y1 | r1)p(Y2 = y2 | r2)p(Y1 = y1, Y2 = y2 | r3, r4, . . . , rn).\n(d) Let C be the binary block code whose normal graph is shown in the figure below. All\nleft constraints are repetition constraints and have the same degree dλ (not counting the\ndongle); all right constraints have the same degree dρ and are given by a binary linear\n(dρ, κdρ) constraint code Cc. Assuming that all constraints are independent, the rate of C\nis\nR = 1 -dλ(1 -κ).\nCc\nCc\nCc\n=\n=\n=\n=\n=\n=\nH\nHP\nPh\nh(\n(\n\n. . .\nH\nHP\nPh\nh(\n(\n\nH\nHP\nPh\nh(\n(\n\nPP\n\nPP\n. . .\n\nPP\n\nPP\n\nPP\n\nPP\nΠ\nFigure 5. Normal graph realization of C, with left repetition constraints of degree dλ\nand right constraint codes Cc of degree dρ."
    },
    {
      "category": "Resource",
      "title": "final05solns.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-451-principles-of-digital-communication-ii-spring-2005/b3a4aba6e9aff98b7fefd0b83bcadd1f_final05solns.pdf",
      "content": "6.451 Principles of Digital Communication II\nTuesday, May 17, 2005\nMIT, Spring 2005\nHandout #26\nFinal solutions\nProblem F.1 (50 points)\nIn this problem, we will investigate ML decoding using the standard VA on a minimal\nconventional trellis, or using a modified VA on a tail-biting trellis.\nConsider the (6, 3, 3) binary linear code C that is generated by the following generator\nmatrix:\nG =\n\n(a) Find the state and branch complexity profiles of an unsectionalized minimal trellis\nfor C.\nDraw the corresponding minimal trellis for C, and label each branch with the\ncorresponding output symbol. Verify that there is a one-to-one map between the codewords\nof C and the paths through the trellis.\nWe first find a trellis-oriented generator matrix for C. Replacing the third generator by\nthe sum of the first and third generator, we arrive at the following generator matrix:\nG′ =\n\nSince the starting times and ending times of all generators are distinct, G′ is trellis-\noriented.\nUsing the active intervals of the generators in G′, we find that the state dimension pro-\nfile of a minimal trellis for C is {0, 1, 2, 2, 2, 1, 0}, and the branch dimension profile is\n{1, 2, 3, 2, 2, 1}. Explicitly, a minimal trellis is\nn\nn\nn\nHHHH\nn\nn\nn\nn\n@\n@\n@@\n@\n@\n@@\nn\nn\nn\nn\n\nHHHH\nHHHH\n\nn\nn\nn\nn\nn\nn\n\nn\nWe verify that there are eight paths through the trellis corresponding to the codewords\n{000000, 111000, 001110, 110110, 011011, 100011, 010101, 101101}.\n\n(b) Show that the following unsectionalized 2-state tail-biting trellis (TBT) realizes C.\n(Recall that in a TBT there may be more than one state in Σ0 = Σ6, the starting and\nending state space, and that the valid paths are those that start and end in the same state\nin Σ0 = Σ6.) Verify that there is a one-to-one map between the codewords of C and the\nvalid paths through the tail-biting trellis.\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\nn\nHHHH\n\nn\nn\nFigure 1. Tail-biting trellis for C.\nHere the two states at state time 6 should be regarded as the same as the two states at\nstate time 0. The valid paths are those that start and end in state 0, corresponding to the\nfour codewords {000000, 111000, 001110, 110110}, and those that start and end in state\n1, corresponding to the four codewords {011011, 100011, 010101, 101101}. Thus we verify\nthat this TBT realizes C.\nDraw a normal graph of this tail-biting trellis realization of C.\nThe normal graph of a tail-biting trellis looks the same as that of a conventional trellis,\nexcept that the two ends are joined so that the ending state variable is the same as the\nstarting state variable. In this case all states are binary, and the constraints are alternately\nsingle-parity-check and repetition constraints. So the normal graph looks like this:\n+\n+\n+\n=\n=\n=\n\nNormal graph of a tail-biting trellis for C.\nNote that G is a TBT-oriented generator matrix for C, in the following sense. If the active\nintervals of the generators in G are viewed on an end-around basis-- i.e., [0, 2], [2, 4], and\n[4, 0]-- then there is only one generator active at each state time, so the corresponding\nstate dimension profile is {1, 1, 1, 1, 1, 1}. Similarly, these end-around activity intervals\nimply a branch dimension profile of {2, 1, 2, 1, 2, 1}. The TBT above is generated by these\nthree end-around generators in the same way as a conventional trellis is generated by\nconventional trellis-oriented generators.\n(c) Propose a modification to the Viterbi algorithm that finds the maximum-likelihood\n(ML) codeword in C, using the tail-biting trellis of part (b). Compare the complexity of\nyour modified VA to that of the standard VA operating on the minimal conventional trellis\nof part (a).\nWe can run the standard Viterbi algorithm on the two subtrellises consisting of the subsets\nof valid paths that start and end in states 0 and 1, respectively. This will find the ML\ncodeword among the two corresponding subsets of 4 codewords. We can then choose the\nbest of these two survivors as the ML codeword.\n\nRoughly, this requires running the VA twice on two two-state trellises, compared to one\nfour-state trellis, so in terms of state complexity the complexity is roughly the same. The\nmaximum branch complexity of the two-state trellises is 4, whereas that of the four-state\ntrellis is 8, so again the branch complexity is roughly the same. Finally, if we do a detailed\ncount of additions and comparisons, we find that both methods require 22 additions and 7\ncomparisons-- exactly the same. We conclude that the simpler tail-biting trellis requires\nthe same number of operations for ML decoding, if we use this modified VA.\n(d) For a general (n, k) linear code C with a given coordinate ordering, is it possible to find\na minimal unsectionalized tail-biting trellis that simultaneously minimizes the complexity\nof each of the n state spaces over all tail-biting trellises? Explain.\nThe example above shows that this is not possible.\nThe conventional trellis for C is\nalso a tail-biting trellis for C with a state space Σ0 = Σ6 = {0} of size 1.\nThus, as\na TBT, the conventional trellis minimizes the state space size at state time 0, but at\nthe cost of state spaces larger than 2 at other times. We have already noted that it is\nnot possible to construct a TBT for C that achieves the state space dimension profile\n{0, 1, 1, 1, 1, 1}, which is what would be necessary to achieve the minima of these two\nprofiles simultaneously at all state times.\nMoreover, we can similarly obtain a tail-biting trellis with a state space of size 1 at\nany state time, by cyclically shifting the generators in order to construct a minimal\nconventional trellis that starts and ends at that state time. But there is obviously no way\nto obtain a minimal state space size of 1 at all state times simultaneously.\n(e) Given a general linear code C and an unsectionalized tail-biting trellis for C, propose\na modified VA that performs ML decoding of C using the tail-biting trellis. In view of the\ncut-set bound, is it possible to achieve a significant complexity reduction over the standard\nVA by using such a modified VA?\nIn general, we can run the standard VA on the |Σ0| subtrellises consisting of the subsets\nof valid paths that start and end in each state in Σ0. This will find the ML codeword\namong each of the |Σ0| corresponding codeword subsets. We can then choose the best of\nthese |Σ0| survivors as the ML codeword.\nThe complexity of this modified VA is of the order of |Σ0| times the complexity of VA\ndecoding a subset trellis.\nThe subset trellis state complexity is the state complexity\nmaxk |ΣC′\nk | for a conventional trellis for the code C′ that is generated by the generators\nof the TBT-oriented generator matrix for C that do not span state time 0. Choose a\ncut set consisting of state time 0 and state time k for any other k, 0 < k < n. By the\ncut-set bound, the total number of generators that span state time either 0 or k is not\nless than the minimal total number of generators that span state time k in a conventional\ntrellis for C. Therefore |Σ0||ΣC′\nk | ≥|ΣC\nk| for any k, 0 < k < n. But this implies that\nmaxk |Σ0||ΣC′\nk | ≥maxk |ΣC\nk|.\nThus no reduction in aggregate state complexity can be\nobtained by the modified VA.\nAn easier way of seeing this is to notice that the operation of the modified VA is the same\nas that of a standard VA on a nonminimal conventional trellis consisting of |Σ0| parallel\nsubtrellises, joined only at the starting and ending nodes. For example, for part (c) we\ncan think of a standard VA operating on the following nonminimal conventional trellis:\n\nn\nn\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\n\nn\nn\nn\nHHHH\n@\n@\n@@\nJ\nJ\nJ\nJ\nJJ\nn\nn\nn\nn\nHHHH\n\nn\nn\nn\n\nn\nSince this is merely another conventional trellis, in general nonminimal, the modified VA\noperating on this trellis must clearly be at least as complex as the standard VA operating\non the minimal conventional trellis.\nWe conclude that if we use the modified VA on a TBT to achieve ML decoding, then we\ncannot achieve any savings in decoding complexity. On the other hand, iterative decoding\non the TBT may be less complex, but in general will not give ML decoding performance.\nProblem F.2 (60 points)\nIn this problem, we will analyze the performance of iterative decoding of a rate-1/3 repeat-\naccumulate (RA) code on a binary erasure channel (BEC) with erasure probability p, in\nthe limit as the code becomes very long (n →inf).\n(3, 1, 3)\n-\ninfo bits\n- Π\n-\nu(D)\n1+D\n-\ny(D)\nFigure 2. Rate- 1\n3 RA encoder.\nThe encoder for the rate-1/3 RA code is shown in Figure 2 above, and works as follows.\nA sequence of information bits is first encoded by an encoder for a (3, 1, 3) repetition\ncode, which simply repeats each information bit three times. The resulting sequence is\nthen permuted by a large pseudo-random permutation Π. The permuted sequence u(D)\nis then encoded by a rate-1/1 2-state convolutional encoder with input/output relation\ny(D) = u(D)/(1 + D); i.e., the input/output equation is yk = uk + yk-1, so the output bit\nis simply the \"accumulation\" of all previous input bits (mod 2).\n\n(a) Show that this rate-1\n3 RA code has the normal graph of Figure 3.\nΠ\n. . .\n. . .\n+\n=\n+\n=\n+\n=\n+\n=\n+\n=\n+\n=\n=@\n@\n@\n=@\n@\n@\n. . .\n. . .\nFigure 3. Normal graph of rate- 1\n3 RA code.\nThe left-side nodes of Figure 2 represent the repetition code. Since the original information\nbits are not transmitted, they are regarded as hidden state variables, repeated three times.\nThe repeated bits are permuted in the permutation Π. On the right side, the permuted\nbits uk are the input bits and the yk are the output bits of a 2-state trellis, whose states\nare the output bits yk. The trellis constraints are represented explicitly by zero-sum nodes\nthat enforce the constraints yk + uk + yk-1 = 0.\n(b) Suppose that the encoded bits are sent over a BEC with erasure probability p. Explain\nhow iterative decoding works in this case, using a schedule that alternates between the left\nconstraints and the right constraints.\nThe outputs of a BEC are either known with certainty or completely unknown (erased).\nThe sum-product algorithm reduces to propagation of known variables through the code\ngraph. If any variable incident on a repetition node becomes known, then all become\nknown. On the other hand, for a zero-sum node, all but one incident variable must be\nknown in order to determine the last incident variable; otherwise all unknown incident\nvariables remain unknown.\nIn detail, we see that initially an input bit uk becomes known if and only if the two\nadjoining received symbols, yk and yk-1, are unerased. After passage through Π, these\nknown bits propagate through the repetition nodes to make all equal variables known.\nAfter passage through Π, the right-going known bits are propagated through the trellis,\nwith additional input or output bits becoming known whenever two of the three bits in\nany set {yk-1, uk, yk} become known. Known input bits uk are then propagated back\nthrough Π, and so forth.\n\n(c) Show that, as n →inf, if the probability that a left-going iterative decoding message is\nerased is qr→l, then the probability that a right-going messages is erased after a left-side\nupdate is given by\nql→r = (qr→l)2.\nAs n →inf, for any fixed number m of iterations, we may assume that all variables in the\nm-level iteration tree are independent.\nA right-going message is erased if and only if both left-going messages that are incident\non the same repetition node is erased. Thus if these two variables are independent, each\nwith probability qr→lof erasure, then ql→r = (qr→l)2.\n(d) Similarly, show that if the probability that a right-going iterative decoding message is\nerased is ql→r, then the probability that a left-going message is erased after a right-side\nupdate is given by\nqr→l= 1 -\n(1 -p)2\n(1 -p + pql→r)2.\n[Hint: observe that as n →inf, the right-side message probability distributions become\ninvariant to a shift of one time unit.]\nFor a particular input bit, the messages that contribute to the calculation of the outgoing\nmessage look like this:\n+\n+\n+\n+\n+\n=\n=\n=\n=\nqout\n↓\np\np\np\np\nqin\nqin\nqin\nqin\ny\ny\ny\ny\nx\nx\nx\nx\nx\nx\n↓\n↓\n↓\n↓\n↓\n↓\n↓\n↓\n→\n→\n←\n←\n→\n→\n→\n←\n←\n←\n. . .\n. . .\nHere p denotes the probability that an output bit yk will be erased on the channel, qin\ndenotes the probability ql→r that an input bit uk will still be erased after the previous\niteration, and qout denotes the probability qr→lthat the bit that we are interested in\nwill be erased after this iteration. Again, as n →inf, we may assume that all of these\nprobabilities are independent.\nFollowing the hint, we use the symmetry and time-invariance of the trellises on either\nside of uk to assert that as n →infthe probability of erasure x in all of the messages\nmarked with x will be the same, and similarly that the probability of erasure y in all of\nthe messages marked with y will be the same.\nThe relations between these probabilities are then evidently as follows:\nx = py;\n1 -y = (1 -qin)(1 -x);\n1 -qout = (1 -x)2.\nSolving the first two equations, we obtain\ny =\nqin\n1 -p + pqin,\nx =\npqin\n1 -p + pqin,\nand thus\nqout = 1 -\n(1 -p)2\n(1 -p + pqin)2.\n\n(e) Using a version of the area theorem that is appropriate for this scenario, show that\niterative decoding cannot succeed if p ≥2\n3.\nThe area under the curve of part (c) is\nZ 1\nq2dq = 1\n3.\nThe area under the curve of part (d) is\n1 -\nZ 1\n(1 -p)2\n(1 -p + pq)2dq = 1 -(1 -p)2\np\n·\n1 -p + pq\n= p.\nIterative decoding will succeed if and only if the two curves do not cross. In order for\nthe two curves not to cross, the sum of these two areas must be less than the area of the\nEXIT chart; i.e.,\n3 + p < 1,\nwhich is equivalent to p < 2\n3; i.e., the capacity 1 -p of the BEC, namely 1 -p, must be\ngreater than the rate of the RA code, namely 1\n3.\n(For example, in Figure 4 below, the area above the top curve is 1\n3, whereas the area\nbelow the bottom curve is 1\n2.)\n(f) The two curves given in parts (c) and (d) are plotted in the EXIT chart below for\np = 0.5. Show that iterative decoding succeeds in this case.\nqr→l\nql→r\n0.75\np = 0.5\nFigure 3. EXIT chart for iterative decoding of a rate- 1\n3 RA code on a BEC with p = 0.5.\nThe two curves do not cross, so iterative decoding starting at (qr→l, ql→r) = (1, 1) must\nsucceed (reach (qr→l, ql→r) = (0, 0)).\n\n(g) [Optional; extra credit.]\nDetermine whether or not iterative decoding succeeds for\np = 0.6.\nThe easiest way to determine whether iterative decoding succeeds for p = 0.6 is to simulate\nit using the equations above. We obtain\nql→r\nqr→l\n1.0\n0.84\n0.706\n0.764\n0.584\n0.716\n0.512\n0.680\n0.463\n0.652\n0.425\n0.627\n0.393\n0.604\n0.365\n0.582\n0.339\n0.561\n0.314\n0.538\n0.290\n0.514\n0.264\n0.487\n0.237\n0.456\n0.208\n0.419\n0.176\n0.373\n0.139\n0.316\n0.100\n0.244\n0.059\n0.157\n0.025\n0.070\n0.005\n0.015\n0.000\n0.001\n. . .\n. . .\nThus iterative decoding succeeds fairly easily for p = 0.6, even though the capacity of a\nBEC with p = 0.6 is only 0.4, not much greater than the rate of the RA code.\nIt is possible therefore that irregular RA codes may be capacity-approaching for the BEC.\nEven with regular codes, the two EXIT curves are already quite well matched. However,\nnote that only the left degrees can be made irregular, which limits design flexibility.\n\nProblem F.3 (40 points)\nFor each of the propositions below, state whether the proposition is true or false, and give\na proof of not more than a few sentences, or a counterexample. No credit will be given\nfor a correct answer without an adequate explanation.\n(a) The Euclidean image of an (n, k, d) binary linear block code is an orthogonal signal\nset if and only if k = log2 n and d = n/2.\nFalse. The Euclidean images of two binary words are orthogonal if and only if d = n/2,\nso all codewords must be at distance d = n/2 from each other. In a linear code, this\nhappens if and only if all nonzero codewords have weight d = n/2. However, all that is\nspecified is that the minimum distance is d = n/2, which is necessary but not sufficient.\nThe smallest counterexample is a (4, 2, 2) code; e.g., {0000, 1100, 0011, 1111}.\n(b) If a possibly catastrophic binary rate-1/n linear convolutional code with polynomial\nencoder g(D) is terminated to a block code Cμ = {u(D)g(D) | deg u(D) < μ}, then a set\nof μ shifts {Dkg(D) | 0 ≤k < μ} of g(D) is a trellis-oriented generator matrix for Cμ.\nTrue.\nThe μ shifts G = {Dkg(D) | 0 ≤k < μ} form a set of generators for Cμ,\nsince every codeword may be written as Pμ-1\nuk(Dkg(D)) for some binary μ-tuple\n(u0, u1, . . . , uμ-1). If the starting and ending times of g(D) occur during n-tuple times\ndel g(D) and deg g(D), respectively, then the starting time of Dkg(D) occurs during n-\ntuple time del g(D) + k, and its ending time occurs during n-tuple time k + deg g(D).\nThus the starting times of all generators are distinct, and so are all ending times, so the\nset G is trellis-oriented.\n(c) Suppose that a codeword y = (y1, y2, . . . , yn) in some code C is sent over a memoryless\nchannel, such that the probability of an output r = (r1, r2, . . . , rn) is given by p(r | y) =\nQn\n1 p(ri | yi). Then the a posteriori probability p(Y1 = y1, Y2 = y2 | r) is given by\np(Y1 = y1, Y2 = y2 | r) ∝p(Y1 = y1 | r1)p(Y2 = y2 | r2)p(Y1 = y1, Y2 = y2 | r3, . . . , rn).\nTrue. This may be viewed as an instance of Equation (12.6) of the course notes, where\nwe take (Y1, Y2) as a single symbol variable. Alternatively, this equation may be derived\nfrom first principles. We have\np(Y1 = y1, Y2 = y2 | r) =\nX\ny∈C(y1,y2)\np(y | r) ∝\nX\ny∈C(y1,y2)\np(r | y) =\nX\ny∈C(y1,y2)\nY\ni∈I\np(ri | yi),\nwhere C(y1, y2) denotes the subset of codewords with Y1 = y1, Y2 = y2. We may factor\np(r1 | y1)p(r2 | y2) out of every term on the right, yielding\np(Yi = yi | r) ∝p(r1 | y1)p(r2 | y2)\n\nX\ny∈C(y1,y2)\nn\nY\ni=3\np(ri | yi)\n\n.\nBut now by Bayes' rule p(Y1 = y1 | r1) ∝p(r1 | y1), p(Y2 = y2 | r2) ∝p(r2 | y2)\n(assuming equiprobable symbols), and we recognize that the last term is proportional to\nthe a posteriori probability p(Y1 = y1, Y2 = y2 | r3, . . . , rn).\n\n(d) Let C be the binary block code whose normal graph is shown in the figure below. All\nleft constraints are repetition constraints and have the same degree dλ (not counting the\ndongle); all right constraints have the same degree dρ and are given by a binary linear\n(dρ, κdρ) constraint code Cc. Assuming that all constraints are independent, the rate of C\nis\nR = 1 -dλ(1 -κ).\nCc\nCc\nCc\n=\n=\n=\n=\n=\n=\nH\nHP\nPh\nh(\n(\n\n. . .\nH\nHP\nPh\nh(\n(\n\nH\nHP\nPh\nh(\n(\n\nPP\n\nPP\n. . .\n\nPP\n\nPP\n\nPP\n\nPP\nΠ\nFigure 5. Normal graph realization of C, with left repetition constraints of degree dλ\nand right constraint codes Cc of degree dρ.\nTrue. First, if the length of C is n, then there are n left constraints and ndλ left edges.\nIf there are m right constraints, then there are mdρ right edges, which must equal the\nnumber of left edges. Thus m = ndλ/dρ.\nSecond, each constraint code Cc consists of the set of all binary dρ-tuples that satisfy a\nset of (1 -κ)dρ parity-check equations. Thus C is equivalent to a regular LDPC code\nof length n with m(1 -κ)dρ = ndλ(1 -κ) parity checks. Assuming that all checks are\nindependent, the rate of such a code is\nR = 1 -ndλ(1 -κ)\nn\n= 1 -dλ(1 -κ)."
    }
  ]
}