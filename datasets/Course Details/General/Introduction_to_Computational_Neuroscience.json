{
  "course_name": "Introduction to Computational Neuroscience",
  "course_description": "This course gives a mathematical introduction to neural coding and dynamics. Topics include convolution, correlation, linear systems, game theory, signal detection theory, probability theory, information theory, and reinforcement learning. Applications to neural coding, focusing on the visual system are covered, as well as Hodgkin-Huxley and other related models of neural excitability, stochastic models of ion channels, cable theory, and models of synaptic transmission.\nVisit the Seung Lab Web site.",
  "topics": [
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Science",
    "Biology",
    "Neuroscience",
    "Cognitive Science",
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Science",
    "Biology",
    "Neuroscience",
    "Cognitive Science"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / sessions\n\nCourse Philosophy\n\nThe central assumption of computational neuroscience is that the brain computes. What does that mean? Generally speaking, a computer is a dynamical system whose state variables encode information about the external world. In short, computation equals coding plus dynamics. Some neuroscientists study the way that information is encoded in neural activity and other dynamical variables of the brain. Others try to characterize how these dynamical variables evolve with time. The study of neural dynamics can be further subdivided into two separate strands. One tradition, exemplified by the work of Hodgkin and Huxley, focuses on the biophysics of single neurons. The other focuses on the dynamics of networks, concerning itself with phenomena that emerge from the interactions between neurons. Therefore computational neuroscience can be divided into three subspecialties: neural coding, biophysics of neurons, and neural networks.\n\nPrerequisites\n\nBasic biology, chemistry, and physics.\n\nDifferential equations or permission of instructor. Linear algebra is also desirable.\n\nKnowledge of MATLAB(r) or willingness to learn.\n\nCourse Requirements\n\nWeekly problem sets\n\nMidterm project\n\nFinal project\n\nTextbook\n\nWe will follow the first six chapters of the book very closely, and the later chapters more sketchily.\n\nDayan, Peter, and L. F. Abbott.\nTheoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems\n. Cambridge, MA: MIT Press, 2001. ISBN: 9780262041997.",
  "files": [
    {
      "category": "Resource",
      "title": "ps01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/37ca9085fc34456859d2fc3d7a670dd7_ps01.pdf",
      "content": "Problem Set 1 (due Feb. 13)\nLinear regression, convolution, and correlation\nFeb. 5, 2003\nThis assignment is based on data from experiments described in R. Wessel, C. Koch, and F. Gabbiani, Coding of\ntime-varying electric field amplitude modulations in a wave-type electric fish. J Neurophysiol 75:2280-93 (1996). The\nweakly electric fish Eigenmannia has a special organ that generates an oscillating electric field with a frequency of\nseveral hundred Hz. It also has an electrosensory organ, with which it is able to sense electric fields.\nDownload the data file: fish.mat\nStart up MATLAB and load the file by typing\n>> load fish.mat\nTyping whos will list the three vectors in the MATLAB workspace:\n\ntime contains the sampling times in milliseconds.\n\nrho is a binary vector representing the spike train of an electrosensory neuron\n\nstim is the stimulus, a random amplitude modulation of an oscillating electric field that was applied to the fish.\nNote: If your data link is too slow, you can use the abridged data file fish10000.mat for this assignment instead. (Available\n1. For each of the following questions, give one or two MATLAB commands that answer the question, along with\nthe answer itself. Try to make your MATLAB code short and elegant. No loops allowed!\n(a) How many spikes are in the whole experiment?\n(b) How long is the whole experiment, in seconds?\n(c) What is the firing rate in Hz, averaged over the whole experiment?\n(d) How many spikes are in the first half of the experiment?\n(e) What is the firing rate in Hz, averaged over the first half of the experiment?\n(f) What is the maximum value of the stimulus?\n(g) What is the minimum value of the stimulus?\n(h) At what time (in milliseconds) did the hundredth spike occur?\n(i) What is the mean of the spike train?\n(j) What is the variance of the spike train?\n2. Note that the variance of the spike train was equal to the mean minus the square of the mean. Prove that this\nholds true for any binary signal.\n(Available in the assignments section.)\nin the assignment section.)\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\n\n3. Write a program that plots the first 1000 samples of the spike train and the stimulus like this:\nvoltage modulation\n-200 0\ntime (ms)\nHint: use the plot command to graph the stimulus, and then use the line command to draw the spikes. Again,\nno loops allowed! Submit your code, along with the graph it produces.\n4. Convolution with a spike train.\nf\nf\ng1\ng2\n\n(a) Sketch\n.\n\n(b) Sketch\n.\n( is the convolution operator; all\n-axes are on the same scale.) The purpose of this question is to build your\nintuition about convolution. To that end, you may find the \"Joy of Convolution\" applets at\nhttp://www.jhu.edu/ signals/convolve/ and\nhttp://www.jhu.edu/ signals/discreteconv2/ useful to play with.\n5. Estimating firing rate by convolution with a boxcar filter. In the following, use only the first 10000 samples of\nthe data. Let's define the probability of firing at time\nto be the number of spikes in a window centered at time\n, divided by the length of the window. For example, in MATLAB this can be computed by convolving rho\nwith ones(101,1)/101.\nTake the central section of the resulting vector (dropping the first 50 and last 50 elements) to obtain a vector of\nthe same length as rho, and call this vector prob. (Even after omitting these elements, there are some suspect\nestimates of probability of firing at the edges, because the windows stick out past the edge of the spike train, but\nwe'll ignore that problem.)\nWith what number should prob be multiplied, to obtain an estimate of firing rate in Hz? Do the multiplication\nand call the result rate. Submit your code, along with a plot of firing rate and the stimulus versus time on the\nsame graph.\n\n6. Linear model of firing rate versus stimulus amplitude. Again, use only the first 10000 samples of the data in the\nfollowing. Try approximating the firing rate as a linear function of the stimulus. Using the function polyfit,\nfind the coefficients a and b such that a*stim+b best approximates prob. Submit your code, along with a\nplot of a*stim+b and prob versus time on the same graph. You should see a nice fit. Also plot prob versus\nstim (without connecting the data points) and a*stim+b versus stim on the same graph. This graph will\nshow the straight line that you fit to the data points. Use the corrcoef command to compute the correlation\ncoefficient.\nNow try the same thing, but calculate firing rate using windows of length 1001 samples and length 11 samples.\nJudging from the correlation coefficient, which of the three windows gives the best fit? Explain in words why.\n7. Linear model of the spike train versus stimulus amplitude. Using the function polyfit, find the coefficients a\nand b such that a*stim+b best approximates rho. Use the corrcoef command to compute the correlation\ncoefficient, which will indicate that the fit is bad. Clearly, no linear function of the stimulus can ever reproduce\nthe spikiness of the spike train. Nevertheless, the coefficients\nand\nare very similar to the ones calculated by\napplying polyfit to stim and prob. Explain why.\n8. Calculate the autocorrelation or autocovariance of the stimulus using the xcorr or xcov commands. Use a\nmaximum lag of\n. Plot your result. Roughly how wide is the central peak, in milliseconds?\nNow do the same thing for the spike train. Your result should look very different, with many peaks. Explain\nwhat property of the spike train causes this. Extra credit: by reading the Wessel et al. paper cited above and\navailable on the class web site, can you figure out why the spike train has this property?\n9. Calculate the cross-correlation or cross-covariance of the stimulus and the spike train. Use a maximum lag of\n. Plot your result. This graphs tells you what the stimulus is like on average, near the time of a spike.\nOn average, is the stimulus increasing or decreasing at the time of a spike? Explain your reasoning!"
    },
    {
      "category": "Resource",
      "title": "ps02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/641b482eea31bd64c11e5834646430f7_ps02.pdf",
      "content": "X\nP\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\n9.29 Problem Set 2 (due Feb. 19)\nMore convolution and correlation\nFeb. 12, 2003\n1. Examples of convolution. In class, we studied the boxcar filter pi = (ρi-1 +ρi +ρi+1)/3 as a way of estimating\nprobability of firing from a spike train. This can be written as p = ρ ∗ h, where h1 = h0 = h-1 = 1/3. For the\nfollowing, fully specify the h such that f = g ∗ h.\n(a) fi = gi - gi-1 (discrete approximation to the derivative)\n(b) fi = gi+1 - gi (another discrete approximation to the derivative)\n(c) fi = gi-5 (5 step time delay)\n(d) fi = gi+1 - 2gi + gi-1 (discrete approximation to the second derivative). For this example, also write\ndown a matrix H such that f = gH. Assume that f and g have lengths 5 and 3 respectively, regard them\nas row vectors (note that the matrix given in the lecture notes is for column vectors).\n2. The conv function is built in to MATLAB, while the xcorr function is part of the Signal Processing Toolbox.\nImagine that you are a starving grad student (a modernday counterpart of Abe Lincoln), and cannot afford to\npurchase the Toolbox. Nevertheless, your fervent desire to study the art of signal processing drives you to write\nyour own version of xcorr.\nType help xcorr in MATLAB, and read the description of the first invocation C=XCORR(A,B). Duplicate\nthis with your own code. It shouldn't take more than a few lines, if you make use of the conv function. (This\nexercise is supposed to teach you the relationship between correlation and convolution).\n3. Show that the convolution\nZ inf\nx(t) =\ndt0g(t - t0)h(t0)\nis the solution of the linear firstorder differential equation\ndx\nτ\n+ x = g\ndt\nwhere h(t) = τ -1e-t/τ for t ≥ 0, and g(t) is an arbitrary function of time. (Hint: integrate by parts). Now\ndefine h(t) = 0 for t < 0 (a kind of zero padding), and show that the convolutional formula above implies\ndh\nτ\n+ h = δ\ndt\nwhere δ(t) is the Dirac delta function. This is why h is regarded as the impulse response for the differential\nequation.\n4. The optimal stimulus. Consider the linear filter\nri =\ndi-j sj\nj\ntransforming stimulus s into response r using the kernel d. Suppose d is given, and consider a fixed i. The\noptimal stimulus is defined as the s that maximizes ri, given the constraint that\nj sj = 1. The constraint\n\nX\nX\nis necessary in the definition because the response can be made arbitrarily large by scaling up s by a constant\nfactor.\nProve that the optimal stimulus is proportional to the kernel, sj ∝di-j . This gives an interesting interpretation\nof the kernel as the stimulus that is most effective at producing a large response.\nHint: Use the method of Lagrange multipliers from multivariate calculus (read the textbook for more about this).\n5. WienerHopf equations. In class we discussed the problem of optimizing the approximation\nM2\nyi ≈\nhj xi-j\n(1)\nj=M1\nwith respect to the filter hi. It was stated without proof that hj is the solution of the WienerHopf equations\nM2\nCxy\nk =\nhj Cxx\nk-j ,\nk = M1, . . . , M2\n(2)\nj=M1\nwhere the correlations are defined by\nX\nX\nCxy\nk =\nxiyi+k\nCxx\nl\n=\nxixi+l\ni\ni\nIf not otherwise noted, all summations are from -infto inf, and assumed to be finite.\nDerive the WienerHopf equations by minimizing the cost function\n⎛\n⎞2\nM2\nX 1\nX\nE =\n⎝yi -\nhj xi-j ⎠\n(3)\ni\nj=M1\nwith respect to hj , for j = M1 to M2. You will need to compute the partial derivatives ∂E/∂hk , set them to\nzero, and play around with summations.\n6. Stimulus reconstruction from spike trains. In class, we discussed using the WienerHopf equations to model\nneural response as a filtered version of the stimulus. In this exercise, we'll work in the opposite direction: the\nstimulus will be modeled as a filtered version of the spike train. This method was invented by Bill Bialek, Rob\nde Ruyter van Steveninck, and coworkers, and is described in Section 3.4 of Dayan and Abbott. We'll apply it\nto the same Eigenmannia data that was used in the first problem set. Define yi = si -hsi and xi\n,\n= ρi -hρi\nwhere si is the stimulus and ρi is the spike train. Use the model of Eq. (1) with M1 = -100 and M2 = 300.\nIf the spike train were white noise, the optimal filter would be of the form hk ∝Cxy . This turns out to be a good\nk\napproximation, even though the spike train is not really white noise. Compute the crosscovariance of the spike\ntrain and the stimulus, and normalize by the number of spikes (this is similar to the spiketriggered average of\nthe stimulus). Plot the filter h. Make sure to only plot the elements corresponding to hM1 , . . . , hM2 .\n7. Compute h ∗x. Again, the challenge here is to discard the proper elements of the convolution so that h ∗x lines\nup with the stimulus y. Plot the first 1000 elements of h ∗x and y on the same graph. If you've done everything\nright, you should see very good agreement. Calculate the squared error of the approximation, as defined in Eq.\n(3).\n8. While the preceding filter is good, the optimal filter is found by solving the WienerHopf equations (2). Compute\nthe appropriate elements of the autocovariance Cxx, and transform them into a matrix of the form Cxx\nj-k using\ni\nthe toeplitz command. Also compute the appropriate elements of the crosscovariance Cxy . Then solve the\ni\nWienerHopf equations for h using the backslash (\\) command. Plot your result for h.\n9. Now for the Wiener filter, plot the first 1000 elements of h∗x and y on the same graph. If you've done everything\nright, the agreement should be even better than before. Calculate the squared error of the approximation, as\ndefined in Eq. (3). This number should be lower than before."
    },
    {
      "category": "Resource",
      "title": "ps03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/4c28f6a00f59d6fdd02e87c0ec4ba988_ps03.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nProblem Set 3 (due Thursday, Feb. 27)\nVisual receptive fields and least squares\nFeb. 20, 2003\n1. Difference of Gaussians in one dimension. In class I told you about some of the properties of Gaussians, and\nhow they are used to model the spatial receptive fields with center-surround structure.\n(a) In a future assignment, you will show that the integral of a Gaussian satisfies\n\nFor now, take this result for granted and use it to prove that\n\n\"!\n#$\n%\n\"\n!\n\n(b) Consider the one-dimension difference of Gaussians\n\n*\n&$'\n)(\n\n%\n!\n$\n%\n\"\n!\n\n% +,\n\n+\n\nwhere\n*.-0/ . For what value of\nis it true that\n\n*\n&$'\n\n\"(\n/5476\n\n/ ?\n&2'\n*\n)!\n\n+\n\n(c) Using MATLAB, plot\nfor\n,\n, and\n. Make sure to choose your axes to give\nthe best visualization of the curve. On your own, you can play around with the parameters to see what\n\"(\nhappens.\n&2'\n(d) For what value of is the maximum of\nachieved? Derive a formula for the values of that minimize\n. You can check your formula by comparing with your MATLAB plots.\n&$'\n\"(\n\n\"(\n\n2. Gabor function in one dimension. The function\n&$'\n)(\n\n% 8\n\n%\n\"\n9:;\n'=<\n?>\nis named after Dennis Gabor, the inventor of holography.\n\n(\n/ ,\n>\n\nBAC\n, and\n>\n\nBA%\n(a) Using MATLAB, plot\n&2'\n\"(\nwith\n<\n\nand\n\n@ . Try the values\n>\n.\n(b) Show that the local maxima or minima satisfy the equation\n<\n\n8DFE%G\n'H<\n\nI>\n(\n(c) How many local maxima/minima exist?\n\n(d) Does increasing make them more or less visible? How about increasing ? Explain why, in terms of the\n<\n&2'\nroles played by the exponential and cosine functions in\n.\n3. Is\n9:;\n'=<\n2JLK\n(\nspace-time separable? How about\n9:M;\n'=<\n2JLK\n\"(\n9#:;\n'=<\nJLK ? What about\n\n'H<\nPJLK ?\nExplain your answers.\n\n(BN\nON\n(\n\n(\n\ng\nh\nFigure 1: Problem 4(b).\n4. More fun with convolution.\n\n'\nK\n\n'\n'\n'\n'\n\n'\nK\n\n'\n'\n\n'\n(a) Prove that\n\n(b) Sketch\n, where\n( and\nare as shown in Fig. 1.\n(\n(\nK\n(\n.\n(\n(\n(\n(\n(\n5. Retinal ganglion cell. Download the data file: ganglion.mat (available in the assignments section.)\nThese data were collected in the course of experiments described in P. Kara, P. Reinagel, and R. C. Reid, Neuron\n27, 635-46 (2000), and generously provided by Prof. Clay Reid of Harvard Medical School.\nThere are two arrays in the data file:\n\nstim contains a time series of random\n\nimages, which were displayed at a rate of roughly 64 Hz. It\n\nhas dimensions\n, and contains 32767 images, each of size\n. I'll refer to such a\nthree-dimensional array as an image deck. It's stored as an int8 array to save space, so you will have to\nuse the double operation to convert it to floating point before performing any mathematical operations.\n\nIf you have limited memory, you won't want to convert it all at once, because that takes up\nMbytes.\nInstead, you'll want to operate on it piece by piece.\n\ncounts is a vector containing the spike counts in each time bin. Since 64 Hz is not very fast, often there\nis more than one spike in a time bin, so this vector is not binary.\nCompute the spike-triggered average of the stimulus. The spike-triggered average is defined as follows:\n\nIt is an image deck c.\n\nThe first image in c is the weighted average of all the images, where each image is weighted by the\nnumber of spikes in the same time bin. Note that images in time bins with zero spikes do not contribute to\nthe average at all.\n\nThe second image in c is the weighted average of all the images, where each image is weighted by the\nnumber of spikes that are one time bin in the future.\n\nThe third image in c is the weighted average of all the images, where each image is weighted by the\nnumber of spikes that are two time bins in the future.\n$\n\nAnd so on for the rest of the images in c. Make your answer a\nimage deck. There's no point\nin using larger time lags, for which the weighted average results in pure noise.\nThere are many ways to accomplish the above in a MATLAB program. Feel free to use whatever method you\nwant, as long as it gets the job done (loops are fine). Your only constraints will be execution time and memory.\nSubmit your code, along with a figure or figures of the images in c (this can be either hard copy or a computer\nfile). In grading, we will look for both a center and surround, as well as a reversal in their signs.\nTo display a single image, use the imagesc command. Experiment with statements like colormap gray or\ncolormap hot to change the color map. Note that imagesc automatically normalizes the image to use the\n\nfull colormap. To display many images in one figure, you can use imagesc in combination with subplot.\nOr you can use the montage command, but to get good results you will have to normalize the images yourself\nso that the minimum value is zero and the maximum is one.\nYou will have to be creative in finding a good way of visualizing c. The chief problem is that the surround is\nweak, and may not show up if you don't normalize correctly. One trick might be to display the positive and\nnegative pixels of c separately, but you may be able to think of better solutions.\n6. The first few lectures were devoted towards explaining how to estimate the parameters of linear models. We\nstarted with the equations for linear regression and moved on to the more complex Wiener-Hopf equations. In\nthis problem, you'll see that both are instances of the normal equations, which are associated with least squares\nproblems.\n(a) Generate a synthetic data set in MATLAB via the commands\nx=rand(100,1);\ny=x+0.1*randn(100,1);\nThese two vectors contain a set of ordered pairs\n'\n\n(\n\n4#4#4\n\n'\n\n(\n, where\n\n/\n/ . By construc-\ntion, these ordered pairs are scattered about a straight line. Use the command polyfit to fit a straight\nline to the data. Do the slope and intercept of the line correspond to what you would expect, given the way\nthat data was generated? Plot the data points and the line on a single graph.\n(b) The fit can also be performed by using the general matrix operations of MATLAB, rather than the special-\nized function polyfit. Convince yourself that a least squares fit to the straight line\n\nN\n\ncan be\nexpressed as the optimization\nE\nG\n!\n\n, where the matrix\n\nis defined as\n\n...\n...\n\n!#\" ,\n\n$\n%&\n\n4#4\n\n!#\" , and the function\nE'(\nG*),+\n'\n\"(\nreturns the value of\n\nthat minimizes\n+\n'\n)(\n. Returning to the MATLAB example, create the matrix\n, solve the least squares optimization with\nthe command A\\y, and compare with the previous result of polyfit.\n(c) In general, the least squares optimization is equivalent to solving the normal equations\n\n\"\n\n\" .\nShow that these general equations take the specific form\n\n-/.\n\n.\n-0.\n\n.\n-/.\n\"\n.\n\n-/.\n\n.\n-0.\n\n.\n\n.\n\n(1)\nfor the linear regression problem, by calculating\n\n\"\n\nand\n\n\"1 . These are the linear regression equations\nthat you find in textbooks. Now you see that they are just a special case of the normal equations.\n(d) Suppose that you would like to use a quadratic model\n\nN\n\nN\n\n\"\ninstead of a linear model. Find\nthe matrix\n\nsuch that the least squares optimization can be expressed as\n\nG\n!\n\n. Write down the\nspecific form of the normal equations in this case, which is a\n\nsystem of linear equations analogous\nto (1).\n(e) The Wiener-Hopf equations are also a special case of the normal equations. Look back to your derivation\nof these equations, and figure out what\n,\n,\n\n\"\n, and\n\n\"1 correspond to."
    },
    {
      "category": "Assignment",
      "title": "ps4_matchpset.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/69eda6678c88d2f2ccf0ffefe0ce725d_ps4_matchpset.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nProblem Set 4 (due Tues Mar. 9)\nOperant matching and games\nMar. 2, 2004\n1. Variable interval schedule, discrete time. Consider a single target with two possible states: baited or unbaited.\nIf the animal chooses an unbaited target, it receives nothing. If it chooses a baited target, it harvests the reward,\nand the target switches to the unbaited state.\nIf a target is baited at the end of a trial, it remains baited for the next trial. If a target is unbaited at the end of a\ntrial (either because reward was harvested, or because the target was unbaited to begin with), then it is rebaited\naccording to the toss of a coin with bias P .\n(a) Suppose that a target is unbaited at the end of a trial. Show that rebaiting occurs after a number of trials n\ndrawn from the geometrical distribution P(n) = (1 - P )n-1P .\n(b) Show that the average of n is given by 1/P .\nNote that this last result makes sense in two limiting cases. If P = 1, then the target is rebaited immediately, so\nthat the average n is one. In the limit P\n0 the time until rebaiting diverges to infinity.\n→\n2. Concurrent VI schedule, discrete time. Suppose there are two targets, both running independent VI schedules\nwith baiting probabilities P and P . Note that these two probabilities are generally chosen rather small (low\noverall rate of reward), so that P + P < 1. In practice, the schedules would not be entirely independent,\nbecause a changeover delay would be imposed. But here no changeover delay will be assumed, so as to simplify\nthe mathematics.\nConsider the probabilistic strategy of choosing between the targets by tossing a biased coin with odds p : p,\np\nwhere p + = 1. Within this class of strategies, what is the optimal p? We will refer to this as the \"optimal\nstrategy\", though there are other strategies outside this class that are superior.\nTo solve this problem, let's calculate the expected reward as a function of p, and then maximize with respect to\np. The total expected reward can be written as\n\nHtot = H + H\n\nwhere H and H are the expected rewards at the two targets. Assuming that all rewards have unity magnitude,\nthese can be written as\n\nH =\nH =\nT\nT\nwhere T and T are the average time intervals between harvestings at the two targets.\n(a) Show that\n1 - p\nT\n= 1 + 1 - P +\n(1)\nP\np\n\nP\n1 - p\nT\n= 1 + 1 -\n+\nP\np\n(2)\nHint: The waiting time T is the sum of two times, the time required for another reward to appear at the\ntarget, and then the time required for the target to be chosen,\n\n(b) Consider Htot as a function of p, and assume that it is smooth and differentiable. Suppose that Htot is\nmaximized for 0 < p < 1. Show that this implies\n\nH\nH\n=\np\np\nIn other words, the matching and maximizing strategies are one and the same for concurrent VI in discrete\ntime. This is a special case; in general, matching and maximizing are different.\n(c) Show that the matching/maximizing solution p∗ satisfies\np∗\n= P 1 - P\np ∗\nP 1 - P\nThis means that the optimal choice probabilities are approximately in the same ratio as the baiting proba\nbilities P and P , when the baiting probabilities are small. In general, this is not the case.\n3. Learning matching. Consider a simple learning model for operant matching behavior. Let the action taken in\ntrial t be given by the binary variables at and\nat = 1. After each action, the animal\nat, which satisfy at +\nreceives reward ht, which is also a binary variable. The incomes in the recent past are computed by\nHt+1\n= βHt + (1 - β)htat\n(3)\n\nHt+1\n= βHt + (1 - β)htat\n(4)\nwhere 0 < β < 1 is a discount factor. Based on these historical incomes, the model chooses its action randomly,\nwith odds given by\npt = Ht\n\np t\nHt\n(a) Program a concurrent VI schedule on two targets with baiting probabilities 0.1 and 0.2. Simulate the\nlearning model for various values of β. What value of β is good for convergence to the optimal?\n(b) Graph the results of the learner by plotting cumulative choices of target 2 vs. cumulative choices of target\n1. Also plot cumulative rewards harvested from target 2 vs. cumulative rewards harvested from target 1.\nMatching behavior corresponds to equal slopes of these two curves.\n(c) Suppose that halfway through your simulation, you exchange the baiting probabilities of the two targets.\nHow quickly can the learning model adapt to this situation? How does your answer depend on β?\nExplain your answers with words, figures, and/or equations. Also submit your MATLAB code.\n4. Pure strategy Nash equilibria\n(a) Solve the following game by eliminating dominated strategies. In what order are they eliminated? What is\nthe Nash equilibrium?\nN\nN\n73,25\nC\n80,26\nJ\n28,27\nC\n57,42\n35,12\n63,31\nJ\n66,32\n32,54\n54,29\n(b) Battle of the sexes. Robert (row) and Cecilia (column) are in love. They desperately want to spend every\nmoment together, and are considering watching a video together this evening. Robert would like to see the\nhorror flick Attack of the Killer Tomatoes, while Cecilia prefers Babette's Feast, based on the Isak Dinesen\nnovel. The payoff matrix for this strategic interaction is\nA\nB\nA\n2,1\n0,0\nB\n0,0\n1,2\n\nWhat are the pure strategy Nash equilibria? Is the game dominance solvable?\n5. Mixed strategy Nash equilibria. The battle of the sexes also has a mixed strategy Nash equilibrium, in which\nRobert chooses A with probability 0 < p < 1 and Cecilia chooses A with probability 0 < q < 1.\n(a) Find p and q. Hint: From the Fundamental theorem of mixed strategy Nash equilibria (see textbooks if\nyou want to know it), Robert is indifferent at equilibrium, i.e. his expected payoffs from his two choices\nare the same. The same holds for Cecilia. From these conditions, solve for p and q.\n(b) What are the average payoffs for Robert and Cecilia? Are they better or worse than the payoffs for the\npure strategy Nash equilibria?"
    },
    {
      "category": "Resource",
      "title": "ps05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/8fc2657a8c8e1a3862e408f46841d39e_ps05.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nProblem Set 5 (due Tues. Apr. 13)\nPassive models of neurons\nApril 7, 2004\n1. The membrane capacitance of a typical cell is 1 μF/cm2 (i.e., 10-6 uncompensated coulombs of charge on each\nside of 1 cm2 of membrane are needed to produce 1 V across the membrane). Suppose that the concentrations\nof ions inside and outside the cell are about 0.1 M, and that the cell is spherical with radius 10 μm.\n(a) How many ions of surface charge are required to produce a transmembrane potential of 100 mV? These\nare called \"uncompensated ions\", because they violate spacecharge neutrality.\n(b) What is the total number of ions inside the cell?\n(c) What is the ratio of the number of uncompensated ions to the total number of ions?\nThe last number should be small, justifying my statement in lecture that spacecharge neutrality is a very good\napproximation in neurons. Also, it means that we can treat ionic concentrations as constant in time, even though\nthe membrane potential and amount of surface charge may vary rapidly in time.\n2. A membrane is permeable to K+ and Cl-, but not to a large organic ion R+ . Inside the membrane, the initial\nconcentrations of RCl and KCl are both 150 mM. Outside the membrane, the initial concentration of KCl is 300\nmM, while that of RCl is zero.\n(a) What are the final concentrations of R+, K+, and Cl- on each side of the membrane at equilibrium?\n(b) What is Vm at equilibrium?\n(c) Will there be any osmotic pressure? If so, in which direction?\nHint: At Donnan equilibrium, the Nernst potentials of potassium and chloride must be the same.\n3. The drawing below depicts the business end of a glass capillary microelectrode. The hole at the tip has radius\nr0, and the radius r increases linearly away from the hole.\ndx\nr\nr0\n(a) What is the resistance dR of the slab shown in the figure, in terms of the thickness dx, radius r, and\nresistivity ρ of the electrolyte? From this result, it should be clear why the tip dominates the total resistance\nof the electrode.\n(b) Let r = kx, so that k represents the steepness of the taper of the pipette. Calculate the total resistance R\nby integrating dR from x = r0 to x = inf. You can make the approximation that the cone above extends\nto infinity, rather than turning into a cylinder, which would be more realistic. This makes little difference\nto the result, since the resistance is dominated by the tip. Your answer should be inversely proportional to\nthe hole radius r0 and to the steepness of the taper k.\n\n(c) Typical values for a patch electrode are ρ = 100 Ω cm, r0 = 0.5 μm and k = 0.2. Substitute these values\ninto your expression for R, to find the electrode resistance. Your result should be on the order of M Ω.\n4. The integrateandfire neuron is a simple model of spiking behavior that sacrifices biophysical realism for\nmathematical simplicity. Below threshold, the membrane potential V obeys the differential equation\ndV\nCm\n= -gL(V - VL) - gsyn(V - Vsyn) + Iapp\n(1)\ndt\nIf V reaches a threshold Vθ, then the neuron is said to spike, and V is instantaneously reset to a value of V0,\nwhere V0 < Vθ . The term Iapp models the current injected through an experimenter's microelectrode. The\nequation above is like Eq. (5.7) in the Dayan and Abbott book, though with slightly different notation. Also,\nI've added an extra term that models synaptic input as the product of a synaptic conductance gsyn, and a driving\nforce that depends on the reversal potential Vsyn. Let's consider the case of an excitatory synapse, so that Vsyn\nis above Vθ .\n(a) Response as a function of applied current Iapp, with no synaptic input (gsyn = 0).\ni. Determine the threshold current Iθ (or rheobase) below which the neuron is inactive, and above which\nthe neuron fires repetitively. The sign of Iθ should depend on whether Vθ is above or below VL.\nii. If Iapp is held constant in time above threshold, the neuron should fire action potentials repetitively.\nFind the relationship between frequency of firing ν and Iapp above threshold.\niii. Show that ν behaves roughly linearly for large Iapp, and determine the slope.\niv. Make a sketch of ν as a function of Iapp, annotating the important features.\n(b) Response as a function of synaptic input gsyn, with no applied current (Iapp = 0). Suppose that the\nsynaptic conductance gsyn is constant in time. This could be approximately true in vivo when a neuron\nreceives a constant barrage of inputs from many other sources, so that the summed input is approximately\nconstant.\ni. Determine the threshold synaptic conductance gsyn,θ below which the neuron is inactive, and above\nwhich the neuron fires repetitively.\nii. Find the relationship between frequency of firing ν and gsyn above threshold.\niii. Show that the ν behaves roughly linearly for large gsyn, and determine the slope.\niv. Make a sketch of ν as a function of gsyn, annotating the important features.\n5. Simulate an integrateandfire neuron in MATLAB with gsyn = 0, Iapp = 1 nA, Cm = 500 pF, gL = 25 nS,\nVL = -70 mV, Vθ = -54 mV, and V0 = -60 mV. To perform your numerical integration, use a step size\nof dt = 0.2 ms, and follow the instructions around Eqs. (5.46) to (5.48) in Dayan and Abbott. As soon as V\nexceeds Vθ , reset it to V0 in the next time step. Graph a train of 10 spikes, starting from the initial condition\nV = VL. Just for looks, you can draw a vertical line whenever a spike happens, to make your graph look more\nrealistic (as in Figure 5.5 of Dayan and Abbott). Compare your interspike interval with the formula for ν that\nyou derived above."
    },
    {
      "category": "Resource",
      "title": "ps06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/8ec643f14881ad9d5e6bedc85836da39_ps06.pdf",
      "content": "Problem Set 6 (due Thurs Apr 22)\nExtensions of the Hodgkin-Huxley model\nApril 13, 2003\nDownload the MATLAB script: cc.m,\nwhich is for simulating the Hodgkin-Huxley model in current clamp. You can read Chapter 5 of Dayan and Ab-\nbott for an explanation of the model equations. Note that conductances, capacitances, and currents are given per unit\narea. More specifically, capacitances are in μF/mm2, conductances in mS/mm2, and current in μA/mm2. Furthermore,\ntime is in msec, and voltage in mV.\n1. Firing frequency vs. applied current. Modify the code so that you can simulate the response of the HH model to\na step change from zero applied current to positive current.\n(a) Simulate the behavior for applied currents of 0.01, 0.05, 0.1, and 5 μA/mm2. Start from the initial condi-\ntions given in the code. For each case, graph the voltage vs. time, and describe in words the behaviors that\nyou see.\n(b) You described four qualitatively different behaviors above. Find the three threshold values of the current\nthat separate these four types of behavior. In other words, there is some value of the applied current below\nwhich behavior 1 holds, and above which behavior 2 holds. And so on for the other thresholds.\n(c) One of the four types of behavior you should have described above is convergence to repetitive firing of\naction potentials that continues indefinitely. What is the minimum frequency of such firing? What is the\nmaximum frequency? Can you explain why repetitive firing is impossible above this maximum?\n2. The MATLAB code is provided with initial conditions for V , m, h, and n, which are approximately the fixed\npoint of the HH model at zero applied current. In this problem, you'll explore the effects of varying the initial\nconditions.\n(a) Change the initial condition for V so that it is 5 mV higher the value given in the code, and simulate for\n100 msec. Graph the resulting V as a function of time. You will see that the model settles back to the fixed\npoint without much ado.\n(b) Now change the initial condition for V so that it is 10 mV higher the value given in the code, and simulate\nfor 100 msec. Graph the resulting V as a function of time. Now you will see an action potential before the\nmodel settles back to the fixed point.\n(c) Approximately determine the threshold voltage above which an action potential is evoked, and below\nwhich one is not evoked.\n3. Post-inhibitory rebound. Experiment with the following stimulation: start with zero applied current, step to a\nnegative value, and then step back to zero. Find an amplitude and duration of the negative step such that the HH\nmodel fires an action potential afterwards, a phenomenon known as post-inhibitory rebound. Plot V , m, h, and\nn as a function of time. Explain in words why this phenomenon happens.\n4. Write MATLAB code to simulate the Connor-Stevens model and reproduce Figure 6.1 of Dayan and Abbott.\nYou can use the above code for the Hodgkin-Huxley model as a starting point. To debug your code, use the\nfact that the model should converge to the steady state V = -68 mV, m = 0.0101, h = 0.9659, n = 0.1559,\na = 0.5404, and b = 0.2887, if the applied current is zero.\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\n(Available in the assignment section.)\n\n5. Download B. Ermentrout, Linearization of FI curves by adaptation, Neural Comput. 10:17219 (1998)\n\nWrite\n\nMATLAB\n\ncode\n\nto\n\nsimulate\n\nthe\n\nmodel\n\ndescribed\n\nin\n\nSection\n\n4.1,\n\nand\n\nreproduce\n\nFigure\n\n2A.\n\nThe\n\npaper\n\ngives\n\ncertain\n\nparameters\n\nper\n\ncm2,\n\nrather\n\nthan\n\nper\n\nmm2\n\n.\n\nThis\n\nis\n\na\n\ndifferent\n\nconvention\n\nthan\n\nDayan\n\nand\n\nAbbott--don't\n\nget\n\nconfused\n\nby\n\nit.\n\nAlso\n\ngraph\n\nz\n\nversus\n\ntime,\n\nand\n\nexplain\n\nin\n\nwords\n\nhow\n\nits\n\nbehavior\n\nis\n\nrelated\n\nto\n\nthe\n\nspike\n\nfrequency\n\nadaptation\n\nin\n\nFigure\n\n2A.\n\nNote: The basic model described in Section 4.1 has the same equations as the HodgkinHuxley model, but\nwith different parameters. It is inspired by Traub's model of hippocampal neurons. The adaptation current can\nbe interpreted in a number of ways. Ermentrout calls it an Mcurrent, after the muscarinesensitive potassium\ncurrent. It could also be interpreted as a slow inhibitory autapse with a time constant of 100 ms. An autapse is a\nsynapse made by a neuron onto itself. Finally, it could be regarded as a simplified model of calciumdependent\npotassium current. In that case, the variable z would represent calcium concentration. An explicit model for the\naccumulation of calcium is given in Section 4.2 of the paper.\n6.\n\nChange the reversal potential of the adaptation current to 0, and let g = 0.01 mS/mm2. Show that the resulting\nmodel is bistable with zero applied current. This means that there are two stable states, one quiescent and the\nother firing. More specifically, you can demonstrate bistability by showing that it is possible to toggle between\nthe two states using transient current pulses. For example, you could start with the neuron at rest, and then\napply a transient depolarizing current pulse that causes the neuron to fire repetitively even after the pulse is over.\nThen you could apply a transient hyperpolarizing current pulse that causes the neuron to stop firing and remain\nquiescent even after the pulse is over. Explain in words the reason for this bistability.\nNote: Here the extra current can be interpreted more than one way. There is some resemblance to a persistent\ncalcium current (though the reversal potential is too low). It is also like a slow excitatory autapse with a time\nconstant of 100 ms."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/36a5f16c3dc68f39afdbb16c93835f60_lecture01.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nNeural coding: linear models\nSebastian Seung\n9.29 Lecture 1: February 4, 2003\n1 What is computational neuroscience?\nThe term \"computational neuroscience\" has two different definitions:\n1. using a computer to study the brain\n2. studying the brain as a computer\nIn the first, the field is defined by a technique. In the second, it is defined by an idea.\nLet's discuss these two definitions in more depth.\nWhy use a computer to study the brain? The most compelling reason is the tor\nrential flow of data generated by neurophysiology experiments. Today it is common to\nsimultaneously record the signals generated by tens of neurons in an awake behaving\nanimal. Once the measurement is done, the neuroscientist must analyze the data to\nfigure out what it means, and computers are necessary for this task. Computers are also\nused to simulate neural systems. This is important when the models are complex, so\nthat their behaviors are not obvious from mere verbal reasoning.\nOn to the second definition. What does it mean to say that the brain is a computer?\nTo grasp this idea we must think beyond our desktop computers with their glowing\nscreens. The abacus is a computer, and so is a slide rule. What do these examples\nhave in common? They are all dynamical systems, but they are of a special class.\nWhat's special is that the state of a computer represents something else. The states of\ntransistors in your computer's display memory represent the words and pictures that\nare displayed on its screen. The locations of the beads on a abacus represent the money\npassing through a shopkeeper's hands. And the activities of neurons in our brains\nrepresent the things that we sense and think about. In short,\ncomputation = coding + dynamics\nThe two terms on the right hand side of this equation are the two great questions\nfor computational neuroscience. How are computational variables are encoded in neu\nral activity? How do the dynamical behaviors of neural networks emerge from the\nproperties of neurons?\nThe first half of this course will address the problem of encoding, or representa\ntion. The second half of the course will address the issue of brain dynamics, but only\nincompletely. The biophysics of single neurons will be discussed, but the collective\nbehaviors of networks are left for my other class 9.641 Neural Networks.\n\n2 Neural coding\nAs an introduction to the problem of neural coding, let me show you a video of a\nneurophysiology experiment. This video comes from the laboratory of David Hubel,\nwho won the Nobel prize with his colleague Torsten Wiesel for their discoveries in the\nmammalian visual system.\nIn the video, you will see a visual stimulus, a flashed or moving bar of light pro\njected onto a screen. This is the stimulus that is being presented to the cat. You will\nalso hear the activity of a neuron recorded from the cat's brain. I should also describe\nwhat you will not see and hear. A cat has been anesthetized and placed in front of the\nscreen, with its eyelids held open. The tip of a tungsten wire has been placed inside the\nskull, and lodged next to a neuron in a visual area of the brain. Although the cat is not\nconscious, neurons in this area are still responsive to visual stimuli. The tungsten wire\nis connected to an amplifier, so that the weak electrical signals from the neuron can be\nrecorded. The amplified signal is also used to drive a loudspeaker, and that is the sound\nthat you will hear.\nAs played on the loudspeaker, the response of the neuron consists of brief clicking\nsounds. These clicks are due to spikes in the waveform of the electrical signal from\nthe neuron. The more technical term for spike is action potential. Almost without\nexception, such spikes are characteristic of neural activity in the vertebrate brain.\nAs you can see and hear, the frequency of spiking is dependent on the properties of\nthe stimulus. The neuron is activated only when the bar is placed at a particular location\nin the visual field. Furthermore, it is most strongly activated when the bar is presented\nat a particular orientation. Arriving at such a verbal model of neural coding is more\ndifficult than it may seem from the video. David Hubel has recounted his feelings of\nfrustration during his initial studies of the visual cortex. For a long time, he used spots\nof light as visual stimuli, because that had worked well in his previous studies of other\nvisual areas of the brain. But spots of light evoked only feeble responses from cortical\nneurons. The spots of light were produced by a kind of slide projector. One day Hubel\nwas wrapping up yet another unsuccessful experiment. As he pulled the slide out of\nthe projector, he heard an eruption of spikes from the neuron. It was that observation\nthat led to the discovery that cortical neurons were most sensitive to oriented stimuli\nlike edges or bars.\nThe study of neural coding is not restricted to sensory processing. One can also\ninvestigate the neural coding of motor variables. In this video, you will see the move\nments of a goldfish eye, and hear the activity of a neuron involved in control of these\nmovements. The oculomotor behavior consists of periods of static fixation, punctuated\nby rapid saccadic movements. The rate of action potential firing during the fixation\nperiods is correlated with the horizontal position of the eye.\nFinally, some neuroscientists study the encoding of computational variables that\ncan't be classified as either sensory nor motor. This video shows a recording of a\nneuron in a rat as it moves about a circular arena. Neurons like this are sensitive to the\ndirection of the rat's head relative to the arena, and are thought to be important for the\nrat's ability to navigate.\nVerbal models are the first step towards understanding neural coding. But compu\ntational neuroscientists do not stop there. They strive for a deeper understanding by\n\nconstructing mathematically precise, quantitative models of neural coding. In the next\nfew lectures, you will learn how to construct such models. But first you have to become\nfamiliar with the format of data from neurophysiological experiments.\n3 Outline of the first part of the class\n1. convolution and correlation\n2. WienerHopf equations\n3. visual receptive fields\n4. fourier analysis and the auditory system\n5. probabilistic models of spike trains\n4 Discretely sampled data\nFor your first homework assignment, you will be given data from an experiment on\nthe weakly electric fish Eigenmannia. The fish has a special organ that generates an\noscillating electric field with a frequency of several hundred Hz. It also has an elec\ntrosensory organ, with which it is able to sense its electric field and the fields of other\nfish. The electric field is used for electrolocation and communication.\nIn the experiment, the fish was stimulated with an artificial electric field, and the\nactivity of a neuron in the electrosensory organ was recorded. The artificial electric\nfield was an amplitudemodulated sine wave, much like the natural electric field of the\nfish. The stimulus vector si in the dataset contains the modulation signal sampled every\n0.5 ms.\nThe extracellular voltage waveform was an analog signal. In such a recording,\naction potentials are submillivolt events. Some sort of pattern detector is applied to the\nwaveform to locate action potentials. A primitive choice is a threshold detector, but\nmore complex detectors make use of the detailed shape of the waveform. In this way,\nan analog voltage signal is processed into a binary response vector ρi that contains the\nspike train of the neuron. Its components are either zero or one, indicating whether or\nnot a spike occurred during each 0.5 ms time bin.\n5 Firing rate\nIn Figure 1 the stimulus amplitude is plotted versus time. Overlaid on top of this graph\nis the spike train. Inspection of this figure reveals that the frequency of action potential\nfiring increases when the stimulus increases.\nTo quantify this relationship, it is necessary to transform the binary vector ρi into a\nfiring rate or related quantity. For example, one can compute the number of spikes in a\n\n-200\ntime (ms)\nvoltage modulation\nFigure 1: Spike train and stimulus amplitude versus time.\n\nX\nX\nwindow of length 2M + 1 centered at i.\nM\nni =\nρi+j\nj=-M\nThis is like dragging a window across the spike train, and keeping track of the number\nof spikes in the window at each time.\nDividing ni by the length of the window 2M + 1 yields the probability of firing\nM\npi = 2M + 1\nρi+j\nj=-M\nper time bin. Alternatively, one can define the firing rate\nνi =\nM\nX\n(2M + 1)Δt j=-M\nρi+j\nwhere Δt is the sampling interval. Assuming that the sampling interval is given in\nseconds, this gives rate in spikes per second, or Hz.\nThe above computations can be implemented using the conv operation in MAT\nLAB, as will be explained in a later lecture and in the homework assignment.\nNote that there are many other definitions of firing rate. This is just a simple one.\n6 A linear model\nIf the pairs (si, pi) are plotted as points on a graph, a linear relationship can be seen.\nThe slope and intercept of the line can be found by optimizing the approximation pi ≈\na + bsi with respect to the parameters a and b.\nSo in this case, the neural coding problem can be addressed by simply fitting a\nstraight line to data points. This is probably the most common way to fit experimental\ndata in all of the sciences. Before we describe the technique below, let's pause to note\nthat this is a very simple dataset. The stimulus is a scalar signal that varies with time.\nMore generally, a vector might be required to describe the stimulus at a given time, as\nin the case of a dynamically varying image. The neural response might also be more\ncomplicated, if the experiment involved simultaneous recording of many neurons. But\neven in these more complex cases, it is sometimes possible to construct a linear model.\nWhen we do so later, we will see that some of the simple concepts introduced below\ncan be generalized.\n7 Fitting a straight line to data points\nSuppose that we are given measurements (xi, yi), where the index i runs from 1 to\nm. In the context of the previous experiment, the measurements are (si, pi). We have\n\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nsimply switched notation to emphasize the generality of the problem. Our task is to\nfind parameters a and b so that the approximation\nyi ≈ a + bxi\n(1)\nis as accurate as possible. Note that it is not generally possible to find a and b so that the\nerror vanishes completely. There are two reasons for this. First, measurement are not\nexact, but suffer from experimental error. Second, while linear models are often used\nin computational neuroscience, the underlying behavior is not truly linear. The linear\nmodel is just an approximation. Note that this is unlike the case of physics, where the\nproportionality of force and acceleration (F = ma) is considered a true \"law.\"\nWhile there are many ways of finding an optimal a and b, the canonical one is the\nmethod of least squares. Its starting point is the squared error function\nm\nX 1\nE =\n2(a + bxi - yi)\n(2)\ni=1\nwhich quantifies the accuracy of the model in Eq. (1). If E = 0 the model is perfect.\nMinimizing E with respect to a and b is a reasonable way of finding the best approxi\nmation. Since E is quadratic in a and b, its minimum can be found by setting the partial\nderivatives with respect to a and b equal to zero.\nSetting ∂E/∂a = 0 yields\n0 = ma + b\nxi -\nyi\ni\ni\nwhile setting ∂E/∂b = 0 produces\n=\n(a + bxi - yi)xi\n(3)\ni\n= a\nxi + b\nxi -\nyixi\n(4)\ni\ni\ni\nRearranging slightly, we obtain two simultaneous linear equations in two unknowns,\nma + b\nxi\n=\nyi\n(5)\ni\ni\na\nxi + b\nxi\n=\nyixi\n(6)\ni\ni\ni\nAs a shorthand for the coefficients of these linear equations, it is helpful to define\nm\nhxi =\nxi\nhx 2i = 1\nm\nxi\n(7)\nm\nm\ni=1\ni=1\nm\nX\nm\nhxyi =\nxiyi\n(8)\nhyi =\nyi\nm\nm\ni=1\ni=1\n\ni\ni\nThe quantity hxi is known as the mean or first moment of x, while hx i is known as\nthe second moment. The quantity hxyi is called the correlation of x and y.\nWith this new notation, the equations for a and b take the compact form\na + bhxi = hyi\n(9)\nahxi + bhx 2\n= hxyi\n(10)\nWe can solve for a in terms of b via\na =\n(11)\nhyi - bhxi\nThis can be used to eliminate a completely, yielding\nb = hxyi - hxihyi\n(12)\nhx2i - hxi2\nBacksubstituting this expression in Eq. (11) allows us to solve for a.\nThe numerator and denominator in Eq. (12) have special names. The denominator\nhx i - hxi2 is called the variance of x, because it measures how much x fluctuates.\nNote that if all the xi are equal to a large constant C, the second moment hx2i = C2 is\nlarge also. In contrast, the variance vanishes completely. The meaning of the variance\nis also evident in the identity\nh(δx)2i = hx i - hxi\nwhich you should verify for yourself. This equation says that the variance is the second\nmoment of δx = x - hxi, which is the deviation of x from its mean. The standard\ndeviation is another term that you should learn. It is defined as the square root of the\nvariance.\nThe numerator hxyi - hxihyi in Eq. (12) is called the covariance of x and y. It is\nequal to the correlation of the fluctuations δx and δy,\nhδxδyi = hxyi - hxihyi\nAgain, I recommend that you verify this identity on your own.\nIn summary, we have a simple recipe for a linear fit. Compute the covariance\nCov(x, y) of x and y, and the variance Var(x) of x. The ratio of these two quantities\ngives the slope b of the linear fit. Then compute a by Eq. (11).\nSubstituting Eq. (11) in the linear approximation of Eq. (1) yields\nyi - hyi ≈ b(xi - hxi)\nIn other words, the constant a is unnecessary, if the linear fit is done to δx and δy,\nrather than to x and y. Given this fact, one approach is to compute the means hxi and\nhyi first, and subtract them from the data to get δx and δy. Then apply the formula\nhδxδyi\nb = h(δx)2\n\np\nwhich is equivalent to Eq. (12). The trick of subtracting the mean comes up over and\nover again in linear modeling.\nSome of you may already have encountered the correlation coefficient r, which is\ndefined by\nr = p\nhxyi - hxihyi\nhx2\nhy2\ni - hxi2\ni - hyi2\nYou may have learned that r close to ±1 means that the linear approximation is a good\none. The correlation coefficient is similar to the covariance, except for the presence of\nthe standard deviations of x and y in the denominator. The denominator normalizes\nthe correlation coefficient, so that it must lie between -1 and 1, unlike the covariance,\nwhich can take on any value in principle. If you know the CauchySchwarz inequality,\nyou can use it to prove that -1 ≤ r ≤ 1, but this is not so illuminating.\nThe correlation coefficient can be interpreted as measuring the reduction in variance\nthat comes from taking a linear (firstorder) model of the data, as opposed to a constant\n(zerothorder) model. Recall that the squared error of Eq. (2) measures the variance\nof the deviation of the data points from the straight line. This variance vanishes only\nwhen the model is perfect.\nFor the best zerothorder model, we constrain b = 0 in Eq. (2), so that E is min\nimized when a = hyi, taking a value proportional to the variance of y. For the best\nfirstorder model, E is minimized with respect to both a and b, so that its optimal value\nis further reduced. The ratio of the new E to the old E is 1 - r2. Another way of saying\nit is that r2 is the fraction of the variance in y that is explained by the linear term in the\nmodel."
    },
    {
      "category": "Resource",
      "title": "optional01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/ccd4841531479d369a3e669edff5c65d_optional01.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nMATLAB, Statistics, and Linear Regression\nJustin Werfel\n9.29 Optional Lecture #1, 2/09/04\n1 MATLAB\nMATLAB is a powerful software package for matrix manipulation. It's a very useful\nlanguage not only for this class, but for a variety of scientific applications, and is used\nwidely thoughout industry. Just as when you have a hammer, everything looks like a\nnail, so when you have MATLAB, everything looks like a matrix. You may learn to\napproach this Zen-like state by the end of the class.\n1.1 Free your mind\nThe basic MATLAB data type is a matrix, an array of values (by default, double-\nprecision floating point numbers), typically arranged as a two-dimensional1 grid (though\narrangements of more or fewer dimensions are possible).\nThe simplest matrix consists of a single element. We can assign a value to a variable\nwith a statement like2\ni = 4\nNotice that when you enter this command, MATLAB echoes the result back to you.\nYou can suppress that echo by ending the statement with a semicolon:\ni = 4;\nA vector (one-dimensional matrix) can be created with the colon operator. To make\na row vector of successive integers, use a statement like x=2:9. The default increment\nbetween successive entries is 1; you can specify a different increment by putting it be\ntween two colons, e.g., x=9:-2:3 creates a row vector with the elements (9, 7, 5, 3).\nThe transpose operator in MATLAB is the single-quote; the easiest way to create\na column vector, then, is to transpose a row vector, e.g., x=(1:8)'. Note that paren\ntheses in MATLAB, like in C and other languages, can be used to specify the order in\n1There are at least two ways we can use the word 'dimension' when talking about matrices. One refers to\nthe layout, in the sense that a line is one-dimensional, a square two-dimensional, etc. The other is in the sense\nof dimensionality of a matrix, e.g., (x1 , x2 , x3, x4) is four-dimensional (although as a vector, its elements\nare arranged in a line when it's written; that is, a vector is one-dimensional in the first sense). MATLAB's\nhelp system tends to use the first sense, Sebastian tends to use the second. Context should hopefully always\nmake it clear which sense is meant. If you have questions about any specific situation, ask.\n2I'm not going to display the results of every operation here; try them out for yourself and see what\nhappens.\n\nwhich you want operations performed; if you type x=1:8', MATLAB first transposes\nthe element 3 (with no effect), then applies the colon operator and gives you a row\nvector as before.\nTo enter a two-dimensional matrix, use commas or spaces to separate entries on\nthe same row, and semicolons or carriage returns to separate rows. For example, the\nstatement\nA = [1, 5,3 6\n2 7 4,4; 3, 8 3 8\n]\nspecifies the matrix\n⎣\nParentheses are used to index matrices, and commas to separate dimensions; so for\ninstance, with the last definition we used for x, x(5) is 5, and A(1,4) is 6.3 You can\nspecify ranges of indices with the colon operator; a colon by itself means all entries\nin that dimension; the keyword end specifies the last entry in a dimension. See what\nthese statements give you:\nA(2:3,1:3)\nA(:,2)\nA(3,2:end)\nA(1:end-1,3)\nSome operators have special meanings when applied to matrices (notably multi\nplication, *, and raising to a power, ˆ); if you want to apply an operation to ev\nery element of a matrix separately, put a period before the operator. For instance,\n(1:8)*(1:8) gives an error because the inner dimensions of the matrices don't\nmatch; (1:8)*(1:8)' is an inner product, returning a single value; (1:8).*(1:8)\ngives you a row vector of the squares of the first eight counting numbers; so does\n(1:8).ˆ2.\n1.2 M-files, data sets, looping and why you should never do it\nSuppose we've got some set of N sensors from which we can read measurements (tem\nperature, voltage, whatever) all at the same time; and we sample from them repeatedly\nM times, so we have M sets of N values. Here's our first look at MATLAB as ham\nmer: one natural way to store these data points is in a matrix A, say M rows by N\ncolumns. Then each row is a snapshot of the sensor readings at a given time; A(i, j) is\nthe value the jth sensor had at time i. Let's further suppose that we want to calculate\nsome weighted sum of the sensor readings at each time step; the weights are given by\nvalues stored in the column vector b.\n3MATLAB stores matrices in memory such that the order of entries in columns is preserved. Thus A(2)\n= 2, A(5) = 7, etc. You can drop the geometry and get all the entries of a matrix as a single column vector\nwith the syntax A(:). Sometimes that knowledge or that operation comes in handy.\n\nThe loop-based way to think about doing this would be to loop over all times, and\nfor each time, loop over all sensor values, multiply each by its weight, and add up those\nproducts. Because it would be a pain to type in all the separate statements every time\nwe wanted to do this, let's write a program. With your favorite text editor, create the\nfollowing file, and name it sensor.m:\nM = 5000; N = 500;\n% Let's choose M and N reasonably large.\nA = rand(M,N); b = rand(N,1);\nC = zeros(5000,1);\nfor i=1:5000\nfor j=1:500\nC(i) = C(i) + A(i,j) * b(j);\nend\nend\nA few notes about this program:\n1. This is a script, a collection of MATLAB commands put into a file with the\n.m extension; typing the filename is then equivalent to typing in all those com\nmands separately at the command line. The other type of M-file is a function.\nCreating those is slightly more complicated; you can read about it by typing\nhelp function at the command line. The use of functions is also different\nfrom the use of scripts in various subtle ways that I'm not going to get into here,\nmostly having to do with the scope of variables in memory.\n2. In the first line, you'll see that you can put more than one statement on the same\nline; semicolons separate statements and suppress echoing, commas separate\nstatements and echo the result.\n3. Everything after the % character on a line is treated as a comment and ignored\nby the interpreter.\n4. The rand(p,q) command creates a p × q matrix and fills it with random num\nbers between 0 and 1 drawn from a uniform distribution (every value is equally\nlikely). randn has the same syntax, but draws its random numbers from a Gaus\nsian distribution with mean 0 and standard deviation 1. Other commands in the\nsame family include zeros and ones, which fill the matrices they create with\nzeros and ones, respectively. MATLAB's intuitive that way. Be warned: if you\nprovide only one argument to any of these commands, it won't give you a vector\nof that length, as you might expect, but rather a square matrix.\n5. for looping is done according to the syntax\nfor <index> = <expression>; <statements>; end.\n<expression> is typically a vector; as the loop executes, <index> takes on\neach of the values in that vector in turn. So you can have a loop like\nfor i=[6 3 12 -1 7], if you want; if it's not clear what that does, try it\nout, having it print the value of i on each iteration.\n\nAll right, with all that said, we can now run the program. Type sensor at the com\nmand line. Pay attention to how long it takes to execute.\nYou may have noticed that this situation was constructed such that we can do ex\nactly the same thing without looping, just by multiplying A and b together (compare\n⎡\nthe definition of matrix multiplication: if C = A b, then Cij =\nAik bkj . Try\nk\nC=A*b, and compare its execution time to the looping case. This is why the problem\nset stresses so much the need to avoid loops; not only does it make your code much\nmore elegant and readable, but MATLAB is full of optimizations to make these kinds\nof operations on matrices take place much more quickly.\n1.3 Plotting and printing\nOne of the great things about MATLAB is the ease and control with which you can use\nit to make plots. The command help plot will tell you all about it, but I'll give you\na few examples to give you an idea of what you can do:\nt = 0:0.1:10; % say we take samples every .1 seconds from 0 to 10 seconds\ny = sin(2*pi*t/5); % make y a sine wave with period 5 seconds\nplot(y)\n% the simplest way to plot; notice that the horizontal axis\n% is not 0 to 10 as we would like, but rather corresponds to\n% the indices of y, by default\nplot(t,y) % specify both the horizontal and vertical quantities to\n% plot against one another\nplot(t,y,'go--') % change the appearance of the plot with an optional\n% third argument, a string that can specify color\n% (here, green), point appearance (circles), and\n% line style (dashed); look in 'help plot' for more\n% options\nx = cos(2*PI*x/5);\nplot(x,y) % parametric plots are equally possible\nplot(t,x,'g-.',t,y,'r*') % more than one plot can be put on the same\n% axes, by adding more triplets of arguments\n% to the plot command\nplot(t,x,'k:') % note that making a new plot by default clears the\n% old one\nhold on % after this command, future plots will appear on the same\n% axes without clearing them\nplot(t,y,'m-.')\nhold off % and now future plots will clear the axes again\nplot(x,y)\nsemilogx(x+2,y+2) % make the x-axis logarithmic\nsemilogy(x+2,y+2) % or the y-axis\nloglog(x+2,y+2)\n% or both axes\nTo print out your masterpiece, MATLAB helpfully provides a print command,\nwhich has a longer help file than any other command I can think of. Here are the\noptions you're most likely to use:\n\nprint % send current figure to the default printer\nprint -Pname % send figure to printer called 'name'\nprint -dps file.ps % print to a PostScript file called 'file.ps'\nprint -dpsc file.ps % or color PostScript\nprint -depsc2 file.eps % or color encapsulated PostScript\nprint -djpeg file.jpeg % or JPEG\n1.4 How to learn MATLAB\nIt's actually pretty difficult to go about explaining how to use MATLAB in a methodical\nway. The best way to learn, and the primary way I learned, is to use its excellent help\nsystem, and play around with commands.\nTo get help on the syntax for any command, type help <command>. A lot\nof commands have intuitive names; if you want to know how to find the mean of\na vector, for instance, try help mean, and there it is. If you want a command\nthat performs some function but you can't think of the name of the command, use\nlookfor <keyword>; for instance, try lookfor variance.\nHere's a set of commands that you might find especially useful; I'll let you read\nabout how to use them. As always, ask any questions you like.\nwhile: the other kind of loop\nif/elseif/else: the other use of conditional statements\nsize(A) : returns M and N for an M-by-N matrix A\nrepmat(a,b,c) : tiles matrix A, repeating it b times vertically and c\ntimes horizontally; try 'repmat(rand(2,2),2,3)'\nmax, min : note that 'm = max(x)' returns in m the maximum value in\nthe vector x; '[m,i]' = max(x)' returns the same thing, plus the\nindex of that value in i!\nmean, sum\nfind : _very_ useful; read the help file\nwho : returns the names of all the variables in memory\nwhos : like who, but returns more detailed information\nclear : clears all variables from memory; 'clear x y z' clears just\nvariables x, y, and z\nwhy : answers any question (try it the next time you get an error!)\n2 Variance and covariance\nIn class, Sebastian explained that variance is a measure of how much a function or\ntime series fluctuates about its mean. It differs from second moment, ∗x2i, in that the\nmean is subtracted; thus with x(t) = 10 + sin(t)/10 and y(t) = 2 sin(t), the second\nmoment of x will be much greater than that of y, but the variance of y will be much\ngreater--which fits with our intuitive notion of what variance should be.\nSebastian gave the definition Var[x] ∗x2i -∗xi2 . I find it easier to see how this\ncorresponds to subtracting the mean by starting from a formulation where the mean is\n\nsubtracted more explicitly:\nVar(x)\n= ∗(x -∗xi)2i\n= ∗(x 2 -2x∗xi + ∗xi )\nX\nX\n1 X\n=\nxi -2\nxi ∗xi +\n∗xi2\nN\nN\nN\ni\ni\ni\nX\n1 X\n= ∗x 2i -2∗xi\nxi + ∗xi2\nN\nN\ni\ni\n= ∗x 2i -2∗xi + ∗xi2\n= ∗x 2i -∗xi .\nHere we used the definition ∗xi = 1 ⎡ xi, and relied on the fact that ∗xi is a fixed\nN\ni\nquantity which does not depend on the index i and thus can be taken outside the sum.\nCovariance is a measure of the extent to which two variables fluctuate together; if\nboth tend to increase at the same time and decrease at the same time, the covariance\nwill be high. You can see that if x and y are each large and positive at the same time\nas one another, and large and negative at the same time as one another, the term ∗xyi\nwill be large and positive, reflecting the high covariance. However, this term is not\nenough on its own; for the same reasons as with variance, to avoid getting artificially\nhigh or low values for the result, we want to subtract the mean from each variable.\nAgain starting from an expression where we do so explicitly,\nCov(x, y) = ∗(x -∗xi)(y -∗yi)i\n= ∗xyi -∗x∗yii -∗∗xiyi + ∗∗xi∗yii\n1 X\n1 X\n1 X\n= ∗xyi - N\nxi ∗yi - N\n∗xiyi + N\n∗xi∗yi\ni\ni\ni\n1 X\n1 X\n1 X\n= ∗xyi -∗yi N\nxi -∗xi N\nyi + ∗xi∗yi N\ni\ni\ni\n= ∗xyi -∗yi∗xi -∗xi∗yi + ∗xi∗yi\n= ∗xyi -∗xi∗yi\nwhich is the definition Sebastian gave in class.\n3 Linear regression\nYou should be familiar with the idea of solving systems of linear equations. For in\nstance, if you have the equations\n3x + 2y =\n8 and\nx -4y = -3,\nand you want to solve for x and y, you can write this in matrix form:\n⎤\n⎦⎤⎦\n⎤\n⎦\nx\n=\n1 -4\ny\n-3\n\nX\nIf we define\n⎤\n⎦\n⎤ ⎦\n⎤\n⎦\nA = 3\n-4 , x = x\ny , d =\n-3 ,\nthe equation becomes Ax = d, which has solution x = A-1d. In MATLAB, you can\nfind this solution with the command x = inv(A)*d or x = A\\d.\nGeometrically, if you think of x and y as coordinates, we're finding the location\nwhere these two lines meet. Or the same approach can describe a different situation:\nwith the familiar equation for a line y = mx + b, the equations\n=\n7m + b and\n=\n3m + b\ngive the matrix equation\n⎤\n⎦⎤\n⎦\n⎤⎦\nm\n=\nb\nThe interpretation suggested here is that we're finding the line connecting the two\npoints (7, 3) and (3, 1).\nIf you know much about matrix algebra, you know that things aren't always this\nsimple. This is a case with a unique solution, one and only one. Sometimes the matrix\nisn't invertible, e.g.,\n⎤\n⎦⎤⎦\n⎤⎦\nx\n=\ny\nThis is the case where the two lines are identical, or the two points are the same; you\ndon't have enough information to lock the solution down exactly. In this case the\nproblem is called underconstrained. If your matrix A has fewer rows than columns,\nit'll necessarily be underconstrained. If you've got two equations and four unknowns,\nyou're stuck; there's no unique solution, there's an infinity of solutions.\nThe opposite situation is when you're overconstrained. Here you also have no\nunique solution, but in this case it's not that you have infinitely many solutions, but\nrather that you have none; there are too many constraints and they can't all be satisfied\nat once. Your lines don't intersect in a single point, or (and this is clearly the case I'm\nworking up to) your points don't all lie on a single line. In that case, the best you can do\nin trying to fit a line to those points is to minimize the total error, for some convenient\nmeasure of error. The one that's always used, and this is the one Sebastian talked about\nin class, is the total squared error, the sum of the squares of the vertical distances from\neach point to the line:\nE =\n(yi - (mxi + b))2\ni\nMinimizing this squared error gives rise to the familiar term 'least-squares'.\nFor the problem to be overconstrained, A must have more rows than columns.4\nSince A isn't square, you can't simply take its inverse. But what you can do is multiply\nboth sides of the equation on the left by its transpose:\nAT Ax = AT d\n4More rows than columns doesn't necessarily mean the problem is overconstrained; it's a necessary but\nnot sufficient condition. The data set could have a unique solution or be underconstrained.\n\nThis is called the normal equation. And this is the equation that, for a given A and\nd, minimizes that squared error. In that sense, it makes the left and right sides of the\nequation as close as they can get.\nNow look at this matrix AT A. If A is M × N , then AT A is (N × M )(M × N ) =\nN × N ; so it's square, which means we can at least try to take an inverse, and in almost\nall real-world cases an inverse exists. Then the least-squares solution is\nx = (AT A)-1AT d.\n-1AT\nThis (AT A)\nis called the pseudoinverse.\nYou can find this solution in MATLAB by typing in the expression\nx = inv(A'*A)*A'*d, by using pinv, or by writing A\\d. I think all of these are\nactually calculated differently, but in real-world cases of real-valued data they should\nall give the same result."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/de8f5ce7b83ee51bf8b9ab12ea92eb29_lecture02.pdf",
      "content": "X\nX\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nConvolution and correlation\nSebastian Seung\n9.29 Lecture 2: February 6, 2003\nIn this lecture, we'll learn about two mathematical operations that are commonly\nused in signal processing, convolution and correlation. The convolution is used to\nlinearly filter a signal, for example to smooth a spike train to estimate probability of\nfiring. The correlation is used to characterize the statistical dependencies between two\nsignals.\nA few words about the big picture. The previous lecture discussed how to construct\na linear model relating firing rate and stimulus at a single time. In the next lecture,\nconvolution and correlation will be used to construct linear models that relate neural\nfiring rate to a stimulus, but at multiple times.\n1 Convolution\nLet's consider two time series, gi and hi, where the index i runs from -inf to inf. The\nconvolution of these two time series is defined as\ninf\n(g ∗ h)i =\ngi-j hj\n(1)\nj=-inf\nThis definition is applicable to time series of infinite length. If g and h are finite, they\ncan be extended to infinite length by adding zeros at both ends. After this trick, called\nzero padding, the definition in Eq. (1) becomes applicable. For example, the sum in\nEq. (1) becomes\nn-1\n(g ∗ h)i =\ngi-j hj\n(2)\nj=0\nfor the finite time series h0, . . . , hn-1.\nExercise 1 Convolution is commutative and associative. Prove that g ∗ h = h ∗ g and\nf ∗ (g ∗ h) = (f ∗ g) ∗ h.\nExercise 2 Convolution is distributive over addition. Prove that (g1 + g2) ∗ h =\ng1 ∗ h + g2 ∗ h. This means that filtering a signal via convolution is a linear operation.\nAlthough g and h are treated symmetrically by the convolution, they generally have\nvery different natures. Typically, one is a signal that goes on indefinitely in time. The\n\nother is concentrated near time zero, and is called a filter. The output of the convolution\nis also a signal, a filtered version of the input signal.\nNote that filtering a signal via convolution is a linear operation. This is an important\nproperty, because it simplifies the mathematics. There are also nonlinear methods of\nfiltering, but they involve more technical difficulties. Because of time limitations, this\nclass will cover linear filters only. Accordingly, we will discuss only neurobiological\nexamples for which linear models work well. But these examples are exceptions to the\nrule that most everything in biology is nonlinear. Don't jump to the conclusion that\nlinear models are always sufficient.\nIn Eq. (2), we chose hi to be zero for all negative i. This is called a causal filter,\nbecause g ∗ h is affected by h in the present and past, but not in the future. In some\ncontexts, the causality constraint is not important, and one can take h-M , . . . , hM to\nbe nonzero, for example.\nFormulas are nice and compact, but now let's draw some diagrams to see how\nconvolution works. To take a concrete example, assume a causal filter (h0, . . . , hn-1).\nThen the ith component of the convolution (g ∗h)i involves aligning g and h this way:\n· · ·\ngi-m-1\ngi-m\ngi-m+1\n· · ·\ngi-2\ngi-1\ngi\ngi+1\ngi+2\n· · ·\nhm-1\nh2\nh1\nh0\n· · ·\n· · ·\n· · ·\nIn words, (g ∗h)i is computed by looking at the signal g through a window of length\nm starting at time i and extending back to time i -m + 1. The weighted sum of the\nsignals in the window is taken, using the coefficients given by h.\nIf the signal g has a finite length, then the diagram looks different when the window\nsticks out over the edges. Consider a signal g0, . . . , gm-1. Let's consider the two\nextreme cases where the window includes only one time bin in the signal. One extreme\nis (g ∗h)0, which can be visualized as\ng0\ng1\ngn-1\n· · ·\n· · ·\n· · ·\n· · ·\nhm-1\nh1\nh0\n· · ·\n· · ·\n· · ·\nThe other extreme is (g ∗h)m+n-2, which can be visualized as\ngn-1\n· · ·\ng0\ng1\n· · ·\n· · ·\n0 · · ·\nhm-1\nh1\nh0\n· · ·\n· · ·\n· · ·\n· · ·\nTherefore g ∗h has m + n -1 nonvanishing components.\n2 Using the MATLAB conv function\nIf g0, g1, . . . , gM -1 and h0, h1 . . . , hN -1 are given as arguments to the conv func\ntion, then the output is f0, f1, . . . , fM +N -2, where f = g ∗ h. More generally, if\ngM1 , . . . , gM2 and hN1 , . . . , hN2 are given as arguments to the conv function, then\nthe output is fM1 +N1 , . . . , fM2 +N2 . In other words, shifting either g or h in time is\nequivalent to shifting g ∗h in time.\nFor example, suppose that g is a signal, and h represents an acausal filter, with\nN1 < 0 and N2 > 0. Then the first element of f returned by conv is fM1 +N1 , and the\n\nX\nX\nP\nlast is fM2 +N2 . So discarding the first |N1 and last N2 elements of f leaves us with\n|\nfM1 , . . . , fM2 . This is timealigned with the signal gM1 , . . . , gM2 , and has the same\nlength.\nAnother motivation for discarding elements at the beginning and end is that they\nmay be corrupted by edge effects. If you are really worried about edge effects, you\nmay have to discard even more elements, which will leave f shorter than g.\n3 Firing rate\nConsider a spike train ρ1, . . . , ρN . One estimate of the probability of firing is\np =\nρi\n(3)\nN\ni\nThis estimate is satisfactory, as long as it makes sense to describe the whole spike train\nby a single probability that does not vary with time. This is an assumption of statistical\nstationarity.\nMore commonly, it's a better model to assume that the probability varies slowly\nwith time (is nonstationary). Then it's better to apply something like Eq. (3) to small\nsegments of the spike train, rather than to the whole spike train. For example, the\nformula\npi = (ρi+1 + ρi + ρi-1)/3\n(4)\nestimates the probability at time i by counting the number of spikes in three time bins,\nand then dividing by three. In the first problem set, you were instructed to smooth the\nspike train like this, but to use a much wider window. In general, choosing the size of\nwindow involves a tradeoff. A larger window minimizes the effects of statistical sam\npling error (like flipping a coin many times to more accurately determine its probability\nof coming up heads). But a larger window also reduces the ability to follow more rapid\nchanges in the probability as a function of time.\nNote that Eq. (4) isn't to be trusted near the edges of the signal, as the filter operates\non the zeros that surround the signal.\nThere are other methods for estimating probability of firing, many of which can be\nexpressed in the convolutional form,\npi =\nρi-j wj\nj\nwhere w satisfies the constraint\nj wj = 1. According to this formula, pi is the\nweighted average of ρi and its neighbors, so that 0 ≤ pi ≤ 1. Closely related is the\nfiring rate per unit time,\npi\nνi = Δt\nwhere Δt is the length of a time bin, or sampling interval. Probabilistic models of neu\nral activity will be treated more formally in a later lecture, and we'll example concepts\nlike firing rate more critically.\n\nX\nThere are many different ways to choose w, depending on the particulars of the\napplication. Previously we chose w be of length n, with nonzero values equal to 1/n.\nThis is sometimes called a \"boxcar\" filter. MATLAB comes with a lot of other filter\nshapes. Try typing help bartlett, and you'll find more information about the\nBartlett and other types of windows that are good for smoothing. Depending on the\ncontext, you might want a causal or a noncausal filter for estimating probability of\nfiring.\nAnother option is to choose the kernel to be a decaying exponential,\n0,\nj < 0\nwj =\nγ(1 - γ)j ,\nj ≥ 0\nThis is causal, but has infinite duration.\nExercise 3 Prove that the exponential filter is equivalent to\npi = (1 - γ)pi-1 + γρi\n4 Impulse response\nConsider the signal consisting of a single impulse at time zero,\n1,\nj = 0\nδj =\n0,\nj = 0\nThe convolution of this signal with a filter h is\n(δ ∗ h)i =\nδj-khk = hj\nk\nwhich is just the filter h again. In other words h, is the response of the filter to an\nimpulse, or the impulse response function. If the impulse is displaced from time 0 to\ntime i, then the result of the convolution is the filter h, displaced by i time steps.\nA spike train is just a superposition of impulses at different times. Therefore, con\nvolving a spike train with a filter gives a superposition of filters at different times.\nElsewhere you may have seen the \"Kronecker delta\" notation δij , which is equiv\nalent to δi-j . The Kronecker delta is just the identity matrix, since it is equal to one\nonly for the diagonal elements i = j. You can think about convolution with δj as mul\ntiplication by the identity matrix δij . More generally, the following exercise shows that\nconvolution is equivalent to multiplication of a matrix and a vector.\nExercise 4 Matrix form of convolution. Show that the convolution of g0, g1, g2 and\nh0, h1, h2 can be written as\n= Gh\ng ∗ h\nwhere the matrix G is defined by\n⎞\n⎛\nG =\n⎜\n⎜\n⎜\n⎜\n⎝\ng0\ng1\ng0\ng2\ng1\ng0\ng2\ng1\ng2\n⎟\n⎟\n⎟\n⎟\n⎠\n(5)\n\nX\nand g ∗h and h are treated as column vectors.\nExercise 5 Each column of G is the same time series, but shifted by a different amount.\nUse the MATLAB function convmtx to create matrices like G from time series like g.\nThis function is found in the Signal Processing Toolbox.\nIf you don't have this toolbox installed, you can make use of the fact that Eq. (5) is\na Toeplitz matrix, and can be constructed by giving its first column and first row to the\ntoeplitz command in MATLAB.\nExercise 6 Convolution as polynomial multiplication. If the second degree polynomi\nals g0 + g1z + g2z2 and h0 + h1z + h2z2 are multiplied together, the result is a fourth\ndegree polynomial. Let's call this polynomial f0 + f1z + f2z2 + f3z3 + f4z4. Show\nthat this is equivalent to f = g ∗h.\n5 Correlation\nThe correlation of two time series is\ninf\nCorr[g, h]j =\ngihi+j\n(6)\ni=-inf\nThe case j = 0 corresponds to the correlation that was defined in the first lecture. The\ndifference here is that g and h are correlated at times separated by the lag j. Note that\nis the convention followed by Dayan and Abbott. Some other books, like Numerical\nRecipes, call the above sum Corr[h, g]j . This can be confusing.\nAs with the convolution, this definition can be applied to finite time series by using\nzero padding. Note that Corr[g, h]j = Corr[h, g]-j , so that the correlation operation\nis not commutative. Typically, the correlation is applied to two signals, while its output\nis concentrated near zero.\nThe zero lag case looks like\n0 g1\ng2\ngn\n· · ·\n· · ·\n· · ·\n0 h1\nh2\nhn\n· · ·\n· · ·\n· · ·\nand the other lags correspond to sliding h right or left.\nThe autocorrelation is a special case of the correlation, with g = h. If g = h, the\ncorrelation is sometimes called the crosscorrelation to distinguish it from the autocor\nrelation. In the first lecture, we distinguished between correlation and covariance. The\ncovariance was defined as the correlation with the means subtracted out. Similarly, the\ncrosscovariance can be defined as the correlation left between two time series after\nsubtracting out the means. The autocovariance is a special case. The command xcov\ncan be used for this purpose.\nExercise 7 Prove that the autocorrelation is the same for equal and opposite time lags\n±j.\n\nX\nX\nP\n6 Using the MATLAB xcorr function\nIf g and h are ndimensional vectors, then the MATLAB command xcorr(g,h)\nreturns a 2n - 1 dimensional vector, corresponding to the lags j = -(n - 1) to n + 1.\nLags beyond this range are not included, as the correlation trivially vanishes. The nth\nelement of the result corresponds to zero lag.\nOne irritation is that MATLAB 5 and 6 follow different conventions. MATLAB 5\ncalls Eq. (6) xcorr(g,h), while MATLAB 6 calls it xcorr(h,g).\nA maximum lag can also be given as an argument, xcorr(g,h,maxlag), to\nrestrict the range of lags computed to maxlag to maxlag. Then the maxlag+1\nelement corresponds to zero lag.\nThe default is the unnormalized correlation given above, but there is also a normal\nized version that looks like\nQxy\nm\nj =\nxiyi+j\nm i=1\nTo compensate for boundary effects, the form\nQxy\nm\nj =\nxiyi+j\nj\ni=1\nm -| |\nis sometimes preferred. Both forms can be obtained through the appropriate options to\nthe xcorr command.\n7 Two examples\nThe autocorrelation of a sine wave. The period should be evident.\nThe autocorrelation of Gaussian random noise. There is a peak at zero, while the\nrest is small. As the length of the signal goes to infinity, the ratio of the sides to the\ncenter vanishes. If the autocorrelation of a signal vanishes, except at lag zero, we'll\ncall it white noise. The origin of this term will become clear when we study Fourier\nanalysis.\n8 Spiketriggered average\nI define the spiketriggered average as\nρisi+j\ni\nCj =\nP ρk\nk\nThis is the crosscorrelation of the spike train and the stimulus, normalized by the num\nber of spikes. Note that Dayan and Abbott define the spiketriggered average with the\nopposite sign in the time lag. However, they also graph it with the time axis reversed,\nso their graph looks the same as mine would!\nYou can think about this as taking a snapshot of the stimulus triggered by each\nspike, and then averaging the snapshots together. This gives an idea of what stimulus\nwaveform is most effective at causing a spike."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/83c72498fa048bfcee6f24e8b88f05d9_lecture03.pdf",
      "content": "X\nX\nX\nX\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nWienerHopf equations. Convolution and\ncorrelation in continuous time\nSebastian Seung\n9.29 Lecture 3: February 11, 2003\nWhen analyzing neural data, the firing rate of a neuron is sometimes modeled as a\nlinear filtering of the stimulus. Alternatively, the stimulus is modeled as a linear filter\ning of the spike train. To construct such models, the optimal filter must be determined\nfrom the data. This problem was studied by the famous mathematician Norbert Wiener\nin the 1940s. It requires the solution of the famous WienerHopf equations.\n1 The WienerHopf equations\nSuppose that we'd like to model the time series yi as a filtered version of xi, i.e. find\nthe h that optimizes the approximation\nyi ≈\nhj xi-j\nj\nWe assume that both x and y have had their means subtracted out, so that no additive\nconstant is needed in the model. Also, hj is assumed to be zero for j < M1 or j > M2.\nThis constrains how far forward or backward in time the kernel extends. For example,\nM1 = 0 corresponds to the case of a causal filter.\nThe best approximation in the least squares sense is obtained by minimizing the\nsquared error\n⎛\n⎞2\nM2\nE =\n⎝yi -\nhj xi-j ⎠\ni\nj=M1\nrelative to hj for j = M1 to M2. This is analogous to the squared error function for\nlinear regression, which we saw in the first lecture.\nThe minimum is given by the equations, ∂E/∂hk = 0, for k = M1 to M2. These\nare the famous WienerHopf equations,\nM2\nCxy\nhj Cxx\n=\nk-j\nk = M1, . . . , M2\n(1)\nk\nj=M1\n\nX\nX\nwhere the shorthand notation\nCxy =\nxiyi+k\nCxx =\nxixi+l\nk\nl\ni\ni\nhas been used for the crosscovariance and autocovariance. You'll be asked to prove\nthis in the homework. This is a set of M2 - M1 + 1 linear equations in M2 - M1 + 1\nunknowns, so it typically has a unique solution. For our purposes, it will be sufficient to\nsolve them using the backslash (\\) and toeplitz commands in MATLAB. If you're\nworried about minimizing computation time, there are more efficient methods, like\nLevinsonDurbin recursion.\nRecall that in simple linear regression, the slope of the optimal line times the vari\nance of x is equal to the covariance of x and y. This is a special case of the WienerHopf\nequations. In particular, linear regression corresponds to the case M1 = M2 = 0, for\nwhich\n= Cxy /Cxx\nh0\n2 White noise analysis\nIf the input x is Gaussian white noise, then the solution of the WienerHopf equation is\ntrivial, because Cxx = C0\nxxδkj . Therefore\nk-j\nCxy\nhk =\nk\n(2)\nCxx\nSo a simple way to model a linear system is to stimulate it with white noise, and\ncorrelate the input with the output. This method is called reverse correlation or white\nnoise analysis.\nIf the input x is not white noise, then you must actually do some work to solve the\nWienerHopf equations. But if the input x is close to being white noise, you might\nget away with being lazy. Just choose the filter to be proportional to the xy cross\ncorrelation, hk =\nk\nCxy /γ, as in the formula (2). The optimal choice of the normaliza\ntion factor γ is\nP Cxy Cxx Cxy\njl\nj\nj-l\nl\nγ =\nP\nm Cxy\nCxy\nm\nm\nCxx\nwhere the summations run from M1 to M2. Note this reduces to γ =\n0 in the case\nof white noise, as in Eq. (2).\n3 Discrete versus continuous time\nIn the previous lecture, the convolution, correlation, and the WienerHopf equations\nwere defined for data sampled at discrete time points. In the remainder of this lecture,\nthe parallel definitions will be given for continuous time.\nBefore the advent of the digital computer, the continuous time formulation was\nmore important, because of its convenience for symbolic calculations. But for numeri\ncal analysis of experimental data, it is the discrete time formulation that is essential.\n\nZ\nX\n4 Convolution\nConsider two functions g and h defined on the real line. Their convolution g ∗ h is\ndefined as\nZ inf\n(g ∗ h)(t) =\ndt0g(t - t0)h(t0)\n-inf\nThe continuous variables t and t0 have taken the place of the discrete indices i and j.\nAgain, you should verify commutativity and associativity.\nIf g and h are only defined on finite intervals, they can be extended to the entire\nreal line using the zero padding trick. For example, if h vanishes outside the interval\n[0, T ], then\nZ T\n(g ∗ h)(t) =\ndt0g(t - t0)h(t0)\n5 Firing rate\nTo define the continuoustime representation of a spike train, we need to make use of\na mathematical construct called the Dirac delta function. The delta function is zero\neverywhere, except at the origin, where it is infinite. You can imagine it as a box of\nwidth Δt and height 1/Δt centered around the origin, with the limit Δt\n0. The\n→\ndelta function is defined by the identity\ninf\nh(t) =\ndt0δ(t - t0)h(t0)\n-inf\nIn other words, when the delta function is convolved with a function, the result is the\nsame function, or h = δ ∗ h. A special case of this formula is the normalization\ncondition\nZ inf\n1 =\ndt0δ(t - t0)\n-inf\nNote that the delta function has dimensions of inverse time.\nThe delta function represents a single spike at the origin. A spike train with spikes\nat times ta can be written as a sum of delta functions,\nρ(t) =\nδ(t - ta)\na\nA standard way of estimating firing rate from a spike train is to convolve it with a\n\nZ\nZ\nX\nX\nX\nZ\nresponse function w\nν(t)\n=\ndt w(t - t0)ρ(t0)\n(3)\n=\ndt w(t - t0)\nδ(t0 - ta)\n(4)\nZ\na\n=\ndt w(t - t0)δ(t0 - ta)\n(5)\na\n=\nw(t - ta)\n(6)\na\nSo the convolution simply adds up copies of the response function centered around the\nspike times. Note that it's important to choose a kernel satisfying\ndt w(t) = 1\nso that\nZ\nZ\ndt ν(t) =\ndt ρ(t)\nSince the Dirac delta function has dimensions of inverse time, smoothing ρ(t) results\nin an estimate of firing rate. In contrast, the discrete spike train ρi is dimensionless, so\nsmoothing it results in an estimate of probability of firing. You can think of ρ(t) as the\nΔt\n0 limit of ρi/Δt.\n→\n6 Lowpass filter\nTo see the convolution in action, consider the differential equation\ndx\nτ\n+ x = h\ndt\nThis is an equation for a lowpass filter with time constant τ . Given a signal h, the\noutput of the filter is a signal x that is smoothed over the time scale τ . The solution\ncan be written as the convolution x = g ∗ h, where the \"impulse response function\" g\nis defined as\n1 e-t/τ θ(\ng(t) =\nt)\nτ\nand we have defined the Heaviside step function θ(t), which is zero for all negative\ntime and one for all positive time. The response function g is zero for all negative time,\njumps to a nonzero value at time zero, and then decays exponentially for positive time.\nTo construct the function x, the convolution places a copy of the response function\ng(t - t0) at every time t0. Each copy gets weighted by h(t0), and they are all summed to\nobtain x(t). The response function is sometimes called the kernel of the convolution.\n\nZ\nX\nTo see another application of the delta function, note that the impulse response\nfunction for the lowpass filter satisfies the differential equation\ndg\nτ\n+ g = δ(t)\ndt\nIn other words, g is the response to driving the lowpass filter with an \"impulse\" δ(t),\nwhich is why it's called the impulse response.\n7 Correlation\nThe correlation is defined as\ninf\nCorr[g, h](t) =\ndt0g(t0)h(t + t0)\n-inf\nThis compares g and h at times separated by the lag t.1 Note that Corr[g, h](t) =\nCorr[h, g](-t), so that the correlation operation is not commutative.\nAs before, if g and h are only defined on the interval [0, T ], they can be extended\nby defining them to be zero outside the interval. Then the above definition is equivalent\nto\nZ T\nCorr[g, h](t) =\ndt0g(t0)h(t + t0)\nThis is the unnormalized version of the correlation. In the Dayan and Abbott textbook,\nQgh(t) = (1/T ) Corr[g, h](t), which is the normalized correlation.\n8 The spiketriggered average\nDayan and Abbott define the spiketriggered average of the stimulus as the average\nvalue of the stimulus at time τ before a spike,\nC(τ) =\ns(ta - τ)\nn a\nwhere n is the number of spikes. Then in Figure 1.9 they plot C(τ) with the positive τ\naxis pointing left. This sign convention may be standard, but it is certainly confusing.\nExactly the same graph would be produced by the alternative convention of taking\nC(τ) to be the average value of the stimulus at time τ after a spike, and plotting it with\nthe positive τ axis pointing right. Note that in this convention, C(τ) would have the\n1The expression above is the definition used in the Dayan and Abbott book, but take note that the opposite\nconvention is used in other books like Numerical Recipes, which call the above integral Corr[h, g](t).\n\nZ\nZ\nX\nX\nX\nX\nX\nsame shape as the crosscorrelation of ρ and s,\nCorr[ρ, s](τ )\n=\ndt ρ(t)s(t + τ )\n(7)\n=\ndt\nδ(t - ta)s(t + τ )\n(8)\na\n=\ns(ta + τ )\n(9)\na\n9 Visual images\nSo far we've discussed situations where the neural response encodes a single time\nvarying scalar variable. In the case of visual images, the stimulus is a function of\nspace as well as time. This means that a more complex linear model is necessary for\nmodeling the relationship between stimulus and response. Let the stimulus be denoted\nby sab, where the indices a and b specify pixel location in the twodimensional image.\ni\nab\nab\nab\nConstruct xi = si\ni by subtracting out the pixel means. Similarly, let yi denote\n-hs\nthe neural response with the mean subtracted out. Then consider the linear model\nyi ≈\nhab ab\nj xi-j\njab\nWe won't derive the WienerHopf equations for this case, as the indices get messy.\nBut for white noise the optimal filter is given by the cross correlation\nhab\nab\nj ∝\nxi yi+j\ni\nWhite noise is defined so that\nm\nab cd\nxi xi+j = σ2\nxδj δacδbd\nm i=1"
    },
    {
      "category": "Lecture Notes",
      "title": "lec6_match1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/3880fbebbc36c507aa303bc4dd4fad5c_lec6_match1.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nOperant matching\nSebastian Seung\n9.29 Lecture 6: February 24, 2004\n1 The law of effect\nThorndike was a pioneer in the scientific study of animal learning. He devised the\npuzzle box as an experimental method. A special cage was constructed, such that a\nspecial sequence of actions was required to open its door. The animal was repeatedly\nplaced in the cage, and the time to escape was tabulated. A graph of the escape time\nas a function of the number of trials was called a learning curve. Thorndike observed\nthat the animals seemed to learn by a process of trial and error. By generating random\nactions while in the cage, occasionally they would happen to generate the specific\nsequence of actions that caused the door to open. This sequence became reinforced by\nsuccess, so that it became more likely to recur in the future. By this process, the animal\nlearned to reduce its escape time.\nThorndike sought to place psychology on an axiomatic foundation, and wrote down\nseveral laws of learning. One of them was the law of effect:\nOf several responses made to the same situation those which are accom\npanied or closely followed by satisfaction to the animal will, other things\nbeing equal, be more firmly connected with the situation, so that, when\nit recurs, they will be more likely to recur; those which are accompanied\nor closely followed by discomfort to the animal will, other things being\nequal, have their connections to the situation weakened, so that, when it\nrecurs, they will be less likely to occur. The greater the satisfaction or\ndiscomfort, the greater the strengthening or weakening of the bond.\nThe law of effect inspired further work on animal learning that eventually led to\nthe behaviorist school of psychology. The study of learning from reinforcement was\neventually formalized in the paradigm of operant or instrumental conditioning.\n2 Variableinterval (VI) schedule\nIn operant conditioning, an animal is trained by making reward contingent on a desired\nbehavior. In the laboratory, there are many methods of creating such contingency. B. F.\nSkinner and collaborators called these reinforcement schedules, and catalogued many\nof them.\n\nIn a variableinterval (VI) reinforcement schedule, rewards are made available at a\ntarget that is chosen by an animal. For example, pigeons choose a key by pecking at it.\nSome of these pecks are reinforced by food reward. In a VI schedule, the rewards have\nconstant amplitude and are made available at the target at random times.\nA target has two possible states: baited or unbaited. If the animal chooses an\nunbaited target, it receives nothing. If it chooses a baited target, it harvests the reward,\nand the target switches to the unbaited state. The target is rebaited after a random time\ninterval t drawn from the exponential distribution P (t) = τ -1e-t/τ . The average time\ninterval is set by the parameter τ .\nRecall that the time intervals between events for a Poisson process are also expo\nnentially distributed. In its use of an exponential distribution, the VI schedule has some\nsimilarity to a Poisson process. However, note that the baiting times are not Poisson,\nand neither are the harvesting times. It is the time interval between harvesting and\nrebaiting that is exponentially distributed.\nNote that in the special case where the animal chooses the target very frequently,\nit will harvest each reward immediately after it becames available. Then the baiting\ntimes are approximately Poisson, as are the harvesting times.\n3 Concurrent VI schedule\nIn a concurrent VI schedule, the animal is presented with a choice between multiple\ntargets. Each target is baited according to a VI schedule. In general, the average\nrebaiting time is different for the targets. Suppose that there are two targets, and that\nthe average rebaiting time is shorter for the first target than for the second. Then the\nfirst target is \"rich,\" while the second target is \"lean.\"\nThe schedules are all independent, except for one complication, a fixed changeover\ndelay. When the animal switches from one target to another, choosing the new target\ndoes not result in harvesting reward during the changeover delay.\nA changeover delay is natural and inevitable if the targets are placed far apart, so\nthat the animal has to traverse some distance in order to switch targets. When the\ntargets are close together, a changeover delay can be artificially imposed. Effectively,\nthe changeover delay imposes a penalty on switching.\nWithout the changeover delay, animals tend to alternate between the targets, allo\ncating their choices equally between them. When the changeover delay is larger than\nsome threshold value, then the animal tends to prefer the rich target over the lean. A\nmathematical theory of such preference is given by the matching law. A typical value\nfor the changeover delay is 1.5 seconds.\n4 The matching law\nRichard Herrnstein and collaborators trained pigeons using concurrent VI schedules.\nIn such an experiment, the pecks of the pigeon at each key were recorded. Also, the\nrewards harvested by the pigeon were recorded. The number of pecks at each key was\na measure of the pigeon's preference for that key. Therefore, the experiment made\n\nit possible to study how the pigeon's preferences depended on how its choices were\nreinforced.\nLet N and\nN be the number of pecks at the two targets, and H and\nH be the\nnumber of rewards harvested at the two targets. Herrnstein and collaborators found the\nfollowing empirical law:\nH\n=\nN\n\nN\n\nH\nN +\nH +\nAlternatively, this can be expressed as\nN\n\nN\nH\n\nH\n=\nThey called this the matching law, as the preferences of the pigeon \"match\" the re\nwards.\nIn the language of economics, the number of rewards harvested can be called \"in\ncome.\" Then the matching law can be stated as:\nChoices are in the same ratio as the incomes of the targets.\nFor another statement in economic language, rewrite the equation as\n\nH\n\nN\n=\nH\nN\nThe income from a target divided by the number of pecks can be called the \"return,\" as\nin \"return on investment.\" Then the matching law can be stated as:\nThe returns from the targets are equal.\nNote that the matching law is not interesting, unless the number of pecks far ex\nceeds the number of rewards. To see why, suppose the opposite is true. Suppose that\nthe VI schedules rebait the targets more quickly than the pigeons can peck. Then every\npeck would be reinforced, so that H = N and\n\nN\n=\n\nH\n. The matching law would be\ntrivially satisfied.\nIn a typical experiment on the matching law, the number of pecks might be one\nhundred times larger than the number of rewards. In this case, large deviations from\nmatching are possible in principle, but empirically such deviations are not observed.\n5 VI schedule for discrete trials\nThe VI schedule can be formulated for discrete trials. Rewards are made available at a\ntarget that is chosen by an animal in discrete trials.\nA target has two possible states: baited or unbaited. If the animal chooses an\nunbaited target, it receives nothing. If it chooses a baited target, it harvests the reward,\nand the target switches to the unbaited state.\nIf a target is baited at the end of a trial, it remains baited for the next trial. If a\ntarget is unbaited at the end of a trial (either because reward was harvested, or because\n\nthe target was unbaited to begin with), then it is rebaited according to the toss of a coin\nwith bias p.\nThis means that a target is rebaited after a number of trials n drawn from the geo\nmetrical distribution P (n) = (1 - p)n-1p. The average n is given by 1/p. If p = 1,\nthen the target is rebaited immediately, so that the average n is one. In the limit p → 0\nthe time until rebaiting diverges to infinity.\n6 Concurrent VI schedule for discrete trials\nTo implement a concurrent VI schedule for discrete trials, a changeover delay is needed.\nThis is done by not allowing harvesting of reward in the first trial after an animal\nswitches.\nThis kind of experiment has been implemented by Sugrue, Corrado, and Newsome.\nGreen and red visual targets are presented to a monkey in repeated trials. The monkey\nchooses one of these targets by making a saccadic eye movement. The monkey is\nrewarded with juice. In the experiment, the VI schedule is nonstationary. That is, the\nbaiting probability for both targets changes every few hundred trials. This requires the\nmonkey to rapidly change its relative preferences for the targets.\n7 A simple learning model\nIn the matching law, the choice probabilities are in the same ratio as the incomes. In the\noriginal research, the matching law was only applied to data aggregated from a single\nlong experiment.\nHowever, the matching law can also be applied to the immediate past to produce a\nsimple learning model. In each trial, the model chooses between targets with probabil\nities set by the ratio of target incomes in the immediate past.\nMore formally, suppose there are two targets. The action taken in trial t is indicated\nat, which satisfy at +\nby the binary variables at and\nat = 1. After each action, the\nanimal receives reward ht. The incomes in the recent past can be computed by by\nHt+1\n= βHt + (1 - β)htat\n(1)\n\nHt+1\n= βHt + (1 - β)ht at\n(2)\nThis is like convolving the time series htat by an exponential filter, where the discount\nfactor β sets the time constant of the exponential.\nBased on these income histories, the model chooses its action randomly, with odds\ngiven by\npt = Ht\n\np t\nHt\nHere pt is the probability that at = 1 and p t the probability that at = 1. These choice\nprobabilities satisfy the constraint pt + pt = 1.\nSugrue et al. found that monkey behavior could be modeled using a relatively short\ntime constant of less than ten trials.\n\nThe model was earlier applied by Erev and Roth to a model of learning to play\ngames."
    },
    {
      "category": "Resource",
      "title": "whnotes.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/cb4507ee2c59db4dd2343672492a252a_whnotes.pdf",
      "content": "_Baa \",\n_\nO¡£¢¤¥¦ § (c)¡\na«¥_¤¬+¢¤¢¦\n9(r)+ ;°k±325 4μ¶°b·± \" \"1oO »S± a°!1⁄4 1⁄2r1bμ¶o1T· ·1⁄2¶μ¶ ¿3⁄4A°<1T \"(r)+ ¿°k1T \"μ¶AX2[μ¶2 \"(r)+ A ATATACÆE;EXEXEaE{EJEJI\nIInI<ÐNI>OXOOO\nO×\nØ\nO\nI\n×\nInI>OpO\nInI«ÐNI\nO\nO\n×\nUBU\nIUUpO\nY\no+± \"(r)+ X 9»n± B°I\nIInI Þaß>O,O'IInIO_Þ£I>aeInI_OUß(ÞaI>a\naaInIO\nßa\na\nÞaI>a\na\naaInIO\nßa\næ\nÞ8cXcXc\nI\na\nO\n9(r) μr2AX1oe 1 5 B1⁄4 o AX1T 5 ¿3⁄4 5±<e3μee 21b \" X· \" ¿25 ¿oO \"1T \"μe±3o ±»\nI\nμ¶oi \"(r)+ 1»n± B° ±»1b·±31⁄2ei^o+±3°kμ¶11⁄2{I\nIInIbÞZß O,OIInIOÞZI\na\nInIOUß ÞaI\na\na\nIni(c)O\nß\na\na\nIai3O\nð\n(r)+ X B\nIn£i n£IÞ<ßoqo\n» \"(r)+ 25 X Bμe ¿2qμ¶2$ 5 B1⁄4 oAX1T 5 ¿3⁄4b1T»S 5 X ,±3o 5 X a°o±3o+ e X \"2 \"(r)+ [oEBAToh÷ATAuøE Au^EBÆTE5EXu\n±»q ¿1⁄2e ¿°b ¿oO \"1T \"iiAX11⁄2¶AX1⁄4 1⁄2¶1⁄4 2\no\nY\no 11⁄2¶ 5 X Bo 1T \"μee\nð\n1¿ii 5±\nð\nBμe 5 1 \"(r) μ¶2μr2μ¶oyu4ECþ=yu2»n± B°IInI Þaß>O,O'IInIO_Þ£I\na\nInI_OUß(Þ<Iaß\na\nO\nIOO\nð\n(r) μrAB(r)°b ¿1o 2~ \"(r) 1T ~ \"(r)+ X \" \"± 13⁄4 1⁄4 ! 5±i 5 B1⁄4 oAX1T \"μ¶o+eA \"(r)+ b25 X Bμe ¿2~μ¶2~25±3°b ^· \" ¿2\"2Bμe±3oy \"(r)1T te±M ¿2~ 5± i12\n»e125 412\nß\na\nß\n\no\n1⁄4 μeeT11⁄2e ¿oO \"1⁄2ei^ \"(r)+ ; X \" B± 4μ¶2\nn2ß\na\n»n± 425±3°b 2AD±3o25 \"1o(c)\n\nð\n(r)+ ¿o\nß\nμ¶292B°k11⁄2¶1⁄2\no\n9(r) ;1piM1⁄2e± t25 X Bμe ¿2μr2\n\n1⁄4μeeP11⁄2¶ ¿o(c) 5±k \"(r) 1·>±\nð\nX t25 X Bμe ¿2 B X· \" ¿25 ¿oO \"1T \"μe±3o ±»$»e1⁄4 o AD \"μe±3o 2¿I\n\nOoUÞhI ÞhI\na\nÞ\nI a\næ\nÞ\nI\na\n\nÞ8cXcXc\nI3O\nU\nUÐNI\nO\nUÞhI ÞZI\na\nÞhI\na\nÞZI\n\nÞ8cXcXc\nI\næ\nO\n9(r)+ 11⁄2¶1T 5 5 X [μ¶2 \"(r)+ ;25 \"1o 3⁄4 1T a3⁄4«»n± B°(1⁄4 1⁄2¶1(»n± \"(r)+ 12\"1⁄4 °\n±»1 e X±3°k X 5 Bμ¶A125 X Bμ¶ ¿2\no\n±3°b·1T aμ¶o+e\n\no\na 1o 3⁄4\n\nð\n2e X ~1<o μe»n ii»n± B°k11⁄2 M· B ¿2\"2\"μe±3oy»S± t(r)+±\nð\n5±«13⁄4+eT1o AD (»n1⁄4o AD \"μe±3oeT11⁄2¶1⁄4+ ¿24»S \"±3°\nI\n5±\nIkÞ£ß\nμ¶o 5 X a°k2\n±»3⁄4+ X BμeeT1T \"μee ¿21T\nI\nIInI(Þaß>O,O\n\nIInIO\nI! O\nð\n(r)+ X B #\"\nμ¶2 \"(r) 23⁄4 μ%$ X \" ¿oO \"μ¶1T \"μe±3oi±· X B1T 5±\no\n_'&\n($¢, (c)¬*)¦y§,+h¢¡) .-¬^¥_¤0/ 3 3\nY\ne±O±^3⁄4.e ¿o+ X a11⁄2 \" X»n X \" ¿o AD <μ¶221\næ43\no65\nE\"E87¿ErEJEeÆo°b ¿1o 22(r)+±\nð\n°k1o(c)i=3⁄4 μee3μ¶ \"2;AX1o.e b \" X· \" ¿2\" ¿o(c) 5 ¿3⁄4mμ¶o= \"(r)+\nAD±3°b·1⁄4 5 X\no9\nμ¶o AD t \"(r)+ ;oM1⁄4 °!e> X ±»$3⁄4 μee3μe \"2μ¶2: o μ¶ 5 tμ¶oA· a1AD \"μ¶AD\nI\n11⁄2e \"(r)+±31⁄4+e3(r)2\"±3°b ~· \"±e B1°k21⁄2rμ<; 2oNA=nu?>\nEDukA=eE7BA@1\nU\n3tAX1oa· X \"»S± a°\n±·> X a1T \"μe±3o 2kμ¶oa1T Beμe 5 B1T \"im· \" ¿AXμ¶2Bμe±3o\nO\nμe \" B1T \"μ¶±3o 11⁄2oO1⁄4 °!e X B2b1⁄2¶μ<; BA\na.1o 3⁄4\n5 B1o 2BAD ¿o 3⁄4+ ¿oO \"11⁄2oO1⁄4 °!e X B2 1⁄2¶μ<; C\n1o 3⁄4\n\nAX1o o ± be> i +1AD \"1⁄2eih \" X· \" ¿25 ¿oO 5 ¿3⁄4\noED\n¿μe \"(r) X kAX1oa \" X· ¿1T \"μ¶o+e\n3⁄4+ ¿AXμ¶°<11⁄2¶21⁄2¶μ<;\nUFTi\nμro=e125\nU\n\noHG\nμ¶o 11⁄2¶1⁄2¶i> Xe ¿omμ¶o o+±^AD ¿oO t±· X B1T \"μ¶±3o 2~e X\nð\nX ¿o. ^1AD \"1⁄2¶i B X· \" ¿25 ¿oO 5 ¿3⁄4\nB1T \"μe±3o11⁄2oO1⁄4 °!e X B2~AX1oN1⁄2e ¿13⁄4. 5± 1⁄2¶±32\"2~±»· \" ¿AXμ¶2\"μe±3o\noIG\n± 2 ^1°b· 1⁄2e μ¶o=1 (r)OiO·± \"(r)+ X \"μrAX11⁄2°k1AB(r) μro+\nð\nμe \"(r)\nU\n\ne125\nU\n1T Bμ¶ \"(r) °b X \"μ¶A 1o 3⁄4\n\n3⁄4+ ¿AXμ¶°k11⁄23⁄4μee3μe \"2\noJ\n\nK ML\no\naN T\nU\nμr2[ B±31⁄4 o 3⁄4+ ¿3⁄4 5±\no¶U\n\nU\n\nU\n$2\"± \"(r)+ b1⁄2¶125\n3⁄4 μee3μ¶ μr29μ¶o AD± \" B ¿AD\no\nO\n\" ¿AXμ¶2\"μe±3oAX1oe> 2 M· B ¿2\"25 ¿3⁄4μ¶o 5 X B°<2[±» \"(r) «ukAP7Xu(c)ESoE<ERQ EaEnACÆTo\nIS8TU\n\nO\n> \"(r)+ 2\"°k11⁄2¶1⁄2¶ ¿25 4oM1⁄4 °2e X\nS\n2\"1⁄4 Aa(r)i \"(r) 1T\nUÞVSXW\nUTo9U,ÞVS\nAX1o\nOoU\nAD±3°b·1⁄4+ \"1T \"μ¶±3o 11⁄2¶1⁄2eike ¿AX11⁄4 2\" t±»$ \"(r)+ 1o+ X ¿3⁄4i 5±k1⁄2¶μro+ [1⁄4+· \"(r) ~3⁄4 ¿AXμ¶°k11⁄2\nI\n± ,eμro 1T \"i\nO\n·±3μ¶oO\nð\n(r)+ ¿ob13⁄4 3⁄4μ¶o+eZY±31T \"μ¶o+e1·±3μ¶oO oO1⁄4°2e X B2\no\\[\n1⁄4+ (r)OiO·± \"(r)+ X \"μrAX11⁄2 °k1AB(r)μ¶o+ 913⁄4 3⁄4 2\no¶U\nT\nU\n1o 3⁄4\no]?^^^\n_\n\n129»n±31⁄2¶1⁄2e±\nð\n2XI\n` acbdb.bfeRb.bdb.bdgha\n`ib.bdb.bfekjdldl.ldgha\nmdm.mdm.mdmdm.mdm.mdmdm\n` acbdb.bdgha\nð\n(r)+ X B ( \"(r) 3⁄4μee3μe \"2t 5±i \"(r)+ b Bμee3(r)O [±» \"(r)+\ne\n1T B 1⁄2¶±325 13⁄4 1⁄4+ ( 5± \"±31⁄4 o 3⁄4 μro+e\no\n9(r)M1⁄4 2SnToU\n2p\ncJ\n\nÐq\n± ;(r) 11⁄2¶»\n\"(r)+ 21⁄2r125 \" X \"1μ¶o+ ¿3⁄4y3⁄4 μee3μe\no\niO· μ¶AX11⁄2\nð\n± 8;^25 \"1T \"μe±3o 2~1⁄4 25 2e125 bak1o 3⁄4(r) 1¿e 1\ni\nac_{eμe\nð\n± B3⁄4»S± ~2\"μ¶o e31⁄2e _{· \" ¿AXμ¶2Bμe±3oi \" ¿11⁄2$oO1⁄4 °!e X B21o 3⁄4 æ\n\ne μe \"2»S± [3⁄4+±31⁄4+e1⁄2e _{· \" ¿AXμ¶2\"μe±3o\no\nY\nAD \"1⁄4 11⁄2¶1⁄2ei 3⁄4 ±31⁄4+e1⁄2e _{· B ¿AXμ¶2\"μe±3oA(r) 129°b± B 1 \"(r) 1o\nð\nμ¶AD ; \"(r)+ 1· B ¿AXμ¶2\"μe±3o\n±»_2\"μ¶o+e31⁄2¶ _{· \" ¿AXμ¶2\"μ¶±3o!e ¿AX11⁄4 2\" 9 \"(r)+ 4oM1⁄4 °!e> X ±»>e μe \"2,3⁄4+ Xe± 5 ¿3⁄4« 5±; \"(r)+ 9»n B1AD \"μe±3o11⁄2 ·1T \"\nI\n12,±· ·±325 ¿3⁄4b 5±! \"(r)+\n^·>±3o ¿o(c)\nO\nμ¶2°b± B 4 \"(r) 1oA3⁄4+±31⁄4+e1⁄2¶ ¿3⁄4\no\niO· μ¶AX11⁄2 eT11⁄2¶1⁄4+ ¿21T \" ~aa;eμe \"2»S± 2\"μ¶o+e31⁄2¶ _{· \" ¿AXμ¶2\"μ¶±3o\nISnTU\n\nO\na.r\na5a\nO\nUTc\na6L\nU\n\nrs\nO\n1o 3⁄4\n^U\neμe \"2~»n± ;2\"μ¶o+e31⁄2¶ _{· \" ¿AXμ¶2\"μ¶±3o\nIS8TU\n\nO\na\nrut\na O\na\nc\na6L\nU\n\nrwvkx\nOao\n9(r)+ B1⁄4 1⁄2e (±» \"(r)M1⁄4 °!e\nμ¶2XI11⁄2\nð\n1¿i^291⁄4 2\" ;3⁄4+±31⁄4+e1⁄2e _{· \" ¿AXμ¶2\"μe±3o<±3o 1\ni\nac_{eμe 9°k1Aa(r) μ¶o+ ~»n± 4oM1⁄4 °b X Bμ¶AX11⁄2\nð\n± 8;\no\nB1¿ii2\"1⁄4+· X BAD±3°k·1⁄4+ 5 X B2\n(r) 1pe !1kæ\n\n_{eμe\nð\n± B3⁄4y2\"μ<yX 25±<2Bμ¶o+e31⁄2e _{· B ¿AXμ¶2\"μe±3oiμ¶2o: o\no\nz\n7{7¿øME5AP7¿Ai°k ¿1o 29(r)+±\nð\nAX1⁄2e±325 !1kAD±3°b·1⁄4+ 5 ¿3⁄41o 2\nð\nX 4μ¶2 5±« \"(r)+ 1 5 B1⁄4+ !1o 2\nð\nX\nof[\neMe^μe±31⁄4 2\"1⁄2ei^ \"(r)+ !1AXAX1⁄4*_\nB1ADi<AX1o«e [o+±!e X 5 5 X \"(r) 1o« \"(r)+ [· \" ¿AXμ¶2\"μ¶±3ow(c)e1⁄4 O12 \"(r)+ [1Te±He [ +1°b·1⁄2e ¿22\"(r)+±\nð\n.: o μe 5 9· \" ¿AXμ¶2\"μe±3o«AX1o\n1⁄2e ¿13⁄4 5±1«1⁄2e±32B24±»1AXAX1⁄4 B1ADiy \"(r)+ B±31⁄4+e3(r)E5ÆTø^o|N>5Æi}' X \" B±\no4o\no1⁄2¶ ¿2\"24 ^ 5 \" ¿°b bAX125 ¿2K13⁄43⁄4 μ¶o+e«± t°(1⁄4 1⁄2e \"μe· 1⁄2ei?_\nμ¶o+eb\nð\n±«2\"μ¶°kμ¶1⁄2r1T n_2\"μ<yX ¿3⁄4ioM1⁄4 °2e X B2e3μ¶e ¿241b \" ¿1⁄2r1T \"μee 1 X \" \"± 9±»\n<ISnTU\n\nO\nð\n(r) μ¶Aa(r)μ¶29AD±3o 2Bμ¶3⁄4+ X \" ¿3⁄41AXAD X· \"1Te 1⁄2e\nI\ne ¿AX11⁄4 25 2μ¶ 9μ¶21⁄4 o 1pe±3μ¶3⁄4 1Te1⁄2e\nOao\n±31⁄2¶eMμ¶o e[3⁄4 μ%$ X \" ¿oO \"μ¶11⁄23\n\n1⁄4 1T \"μe±3o2 B\n\n1⁄4 μ¶ \" ¿2oM1⁄4 °b X B±31⁄4 2μ¶ 5 X B1T 5 ¿3⁄4bAX11⁄2¶AX1⁄4 1⁄2¶1T \"μ¶±3o 225±t \"(r)+ \"±31⁄4 o 3⁄4*_{±N$k X B \"±\nAX1oi1AXAX1⁄4 °(1⁄4 1⁄2¶1T 5\no\\~\n\nð\nμ¶1⁄2¶1⁄2 ^1°kμro+ [ \"(r)μ¶21⁄2¶1T 5 X\no\n¿o+ X B11⁄2¶1⁄2¶iM(r)+±\nð\nXe X 121⁄2¶±3o+e!12 \"(r)+ [ \"±31⁄4 o 3⁄4*_{±N$ X B \"±\nμ¶2o+± 1°k·1⁄2¶μ%: ¿3⁄4 eMib \"(r)+ ~11⁄2ee± Bμe \"(r)°Oμe μ¶2o+± 1;e μee2· B±e1⁄2e ¿°\no\nY\noO1⁄4°b X Bμ¶AX11⁄2 °k X \"(r)+±M3⁄4\nð\n(r) μrAB(r)<3⁄4 ±O ¿2o+±\n1°b·1⁄2rμe»Si< X \" \"± B29μr29AX11⁄2¶1⁄2e ¿3⁄4E ={A?DA¶E 1o3⁄4\nð\n\nð\nμ¶1⁄2¶1⁄2AD±3o 2\"μr3⁄4+ X \"(r)+ 225 \"1Te μ¶1⁄2¶μe\ni<±»q25 Xe X B11⁄211⁄2¶e± Bμe \"(r) °k21⁄2r1T 5 X\no\n9(r) X \" (μr24±3o+ bAX125\nð\n(r) X \" ! B±31⁄4 o 3⁄4*_{±N$AX1o=e !μr°b·± \" \"1o(c) XI\nð\n(r)+ ¿o\nð\n± o+ ¿1T B1⁄2ei\n\n1⁄4 11⁄2qoO1⁄4°2e X B2t1T \"\n2\"1⁄4+e 5 B1AD 5 ¿3⁄4< \"(r) X \" [μ¶2123⁄4 B125 \"μ¶A[1⁄2e±32\"2,±»· \" ¿AXμ¶2\"μ¶±3okAX11⁄2¶1⁄27BA={APE =eEUÆQu(c)E77BATo7JEDAnACAP=eEeÆTo\noG\n± ^1°k·1⁄2e o¶UUUU\nÐ8o¶UUU\n\nOo\n\nUTo\\G\n±31⁄4+ 3⁄4 μ¶e3μe \"2±» · \" ¿AXμ¶2\"μe±3o21T \" B ¿3⁄4 1⁄4 AD ¿3⁄4; 5±\nUTo\n9(r) μ¶2AD±3°b ¿2q1⁄4+·!μro21⁄4 2\"μro+e9 \"(r)+\n\n1⁄4 13⁄4 B1T \"μ¶A\n»n± B°!1⁄41⁄2¶1*+ 5±: o 3⁄41k \"±O± \"(r)1T 4μ¶29o+ ¿1T\noG\n± 4 +1°b·1⁄2¶ IiO\nÐZÐ\nA\n\na\nÐVP.\na\n\nI3O\nð\n(r)+ ¿o\n\nμ¶22B°k11⁄2¶1⁄2\no\n9(r) μ¶2AX1oAe t1pe±3μ¶3⁄4+ ¿3⁄4AeOiA1⁄4 2\"μ¶o e; \"(r)+ 1°k1T \"(r)+ ¿°k1T \"μrAX11⁄2¶1⁄2eib\n\n1⁄4 μeeT11⁄2e ¿o(c) ^e1⁄4 oM1⁄4 °b X Bμ¶AX11⁄2r1⁄2ei\n2\"1⁄4+· X Bμ¶± KO»n± B°(1⁄4 1⁄2¶1\nIO\nÐ\na\n\n,Þ\nA\n\na\nÐVP.\nI^3O\nY\no+± \"(r)+ X +1°b·1⁄2e 1μ¶2AD±3°b· 1⁄4+ \"μ¶o+e\n\nrut\nð\nμe \"(r)A \"(r)+ ~·±\nð\nX 425 X aμe ¿2±»\noMo\n9(r) t 5 X a°k2±» \"(r)+ ;2\" X Bμe ¿2\n11⁄2e 5 X Bo1T 5 2μ¶o2\"μee3o^2\"±k°k12\"2\"μee !AX1o AD ¿1⁄2¶1⁄2¶1T \"μe±3oμr2o+ X ¿3⁄4+ ¿3⁄4 5±ki^μe ¿1⁄2¶3⁄41b2B°k11⁄2¶1⁄2_oM1⁄4 °2e X\no\nY\ne X 5 5 X 4°b X \"(r)+±^3⁄4\nμ¶2 5±AAD±3°b·1⁄4+ 5\nUF\n\nt\no\na\n\n9(r) y1Te±pe 1T B y +1°b·1⁄2e ¿2b±»1 a1⁄4 μ¶o μ¶o e1.e±M±^3⁄4£· \"±e 1⁄2e ¿°\nð\nμe \"(r)\n1.e13⁄4\n°k X \"(r)+±M3⁄4\no\n9(r)+ : M ¿2«1T \"\n· \"±e 1⁄2e ¿°_3⁄4+ X· ¿o 3⁄4+ ¿oO\no\\9\n±3°b X \"μ¶°b ¿2K (r)+±\nð\nXe X K1(· \"±e 1⁄2e ¿°\nμ¶251⁄4 2\" μro(c) 5 Bμro 2\"μ¶AX11⁄2¶1⁄2¶i((r) 1T a3⁄4« 5±k2\"±31⁄2ee 1e ¿AX11⁄4 25\nμe $μ¶2e X \"i;2\" ¿o 2\"μe \"μee 5±[ X \" B±\nI\nEnASA>n7BÆTo|E=eEaÆTo>E{|\nOao\nY\no! +1°b·1⁄2e μ¶2w: o3⁄4 μ¶o+e \"(r)+ 3⁄4+±31⁄4+e1⁄2¶ \"±O± $±»\nI aOÐI$Þ,+o\nY\n2\"°k11⁄2r1⁄2, X \" \"±\nkISJO\nμ¶omAD±3°b·1⁄4+ \"μro+ey \"(r)+ AAD±O AAXμe ¿o(c) \"2 1⁄2e ¿13⁄422 5±1yeμ¶e X \" \"±\nkIS\nO\nt\nO\nμ¶o \"(r)+ «25±31⁄2r1⁄4+ \"μe±3o\ne ¿AX11⁄4 25 A \"(r)+ iAX1⁄4+ \"e iμ¶22 \"1o e ¿o(c) 5± \"(r)+\nI\n_14 ^μr2\no\nY\no+± \"(r)+ X +1°b·1⁄2e Aμ¶2! 5 \"iMμro+ey 5±.25±31⁄2ee i125iM2\" 5 ¿°\n±»\n1⁄2¶μ¶o ¿1T 1\n\n1⁄4 1T \"μ¶±3o 2\nð\n(r) ±325 k°k1T 5 Bμ% Nμ¶22o+ ¿1T B1⁄2ei=2\"μ¶o+e31⁄41⁄2¶1T\no\n9(r) μr21μ¶21e X±3°b X 5 BμrAX11⁄2¶1⁄2ei\n\n1⁄4 μ¶eP11⁄2e ¿oO ; 5±:>o 3⁄4 μ¶o+e\n\"(r)+ 2μro(c) 5 X B2\" ¿AD \"μe±3o ±»1⁄2¶μ¶o ¿2\nI\n(r)(c)iM· X \"·1⁄2¶1o ¿2\nO\n\"(r) 1T [1T \" !o+ ¿1T B1⁄2eiA·1T B11⁄2¶1⁄2¶ ¿1⁄2\noo[\no+ !AX1o o+± [AX1⁄4 \" 22\"1⁄4 Aa(r) AX125 ¿2tμ¶o\ne ¿o+ X B11⁄2! \"(r)+ 1e ¿25 ±3o+ 2AX1oy3⁄4+±<μ¶2e ;1\nð\n1T \" !±» \"(r)+ ¿°\no\no\noy \"(r) (AX125 ±»2\"±31⁄2eeMμro+e2\nIO\n±3o (AX1o ¿25 \"μ¶°<1T 5 (r) ±\nð\nμ¶1⁄2¶1⁄2%_AD±3o 3⁄4μe \"μe±3o+ ¿3⁄4 \"(r) !· B±e1⁄2e ¿°μr24μ¶o \"(r)+\n»n±31⁄2¶1⁄2e±\nð\nμ¶o+e\nð\n1pi\no~\nμe \"(r) 1k2B1⁄4 μe \"1Te1⁄2ei«3⁄4+ : o+ ¿3⁄4o+± a°1\næc30*4 5±«°b ¿12\"1⁄4+ \" 1 \"(r) 22\"μ<yX 1±»\\\nI\nXV\nI\n\n9(r)+ (o 1T \"1⁄4+ B11⁄2\nð\n1¿i 5±i 5 ¿25 ~1«oM1⁄4 °b X aμ¶AX11⁄22\"±31⁄2¶1⁄4+ \"μe±3ow\nI\nμ¶2[ 5±«·1⁄2r1⁄4+e<μe ~μ¶oO 5±< \"(r)+ (\n\n1⁄41T \"μe±3o1o3⁄4AX11⁄2rAX1⁄4 1⁄2¶1T 5\n\nO\n\nI\n1o 3⁄4A \"(r)+ bE\"EJEJE|ø+AA\n,Ð\n\no~\n\nð\nμ¶1⁄2¶1⁄2 e ~(r)1T· ·Oi«μe» \"(r)+ ~ \" ¿2Bμ¶3⁄4 1⁄4 11⁄2>μ¶22\"°k11⁄2r1⁄2Oe 1⁄4+\nð\n(r) 1T 3⁄4+±M ¿2 \"(r) μ¶2\n5 ¿1⁄2¶1⁄21⁄421Te>±31⁄4 \"(r)+ ; X \" \"±\nI«Ð\n\nI\nt2\"μ¶o e\n\nI«Ð\n\nI\n0{¡\nrwv\n\nÐ\n\n1o 3⁄4\n\nX{¡¢\nI\n\nð\n;e X 9 \"(r)+ ; ¿2\" \"μ¶°k1T 5\n\nI<Ð\n\nI\n\nI\n\n{¡¢{¡\nrwv\n\nÐ\n\n25±\nð\n;3⁄4+ : o 1 \"(r)+ 27BÆo|TE=eEeÆTo.oøMu¢aEDE±»\nAD±3o3⁄4\nI\n\nO,O\n{¡¢{¡\nrwv\n\nc\nIUU\n\nO\no\n»$AD±3o 3⁄4\nI\n\nO\nμ¶21⁄2¶1T \"e \"(r) ; X \" \"± 4°<1¿iAe> ;1⁄2r1T \"e ; Xe ¿o \"(r) ±31⁄4+e3(r) \"(r)+ 1 \" ¿2Bμ¶3⁄4 1⁄4 11⁄2_μ¶22B°k11⁄2¶1⁄2\no\nY\n°b± B <25 X Bμe±31⁄42\nð\n1¿i. \"(r) 1T £: o μe 5 k B ¿25±31⁄4+ BAD ¿2\nI\nAD±3°b·1⁄4+ \"μ¶o e e1⁄4 3⁄4+e X ,3⁄4μ¶2n;=25·1AD\nO\n1o3⁄4¤: o μ¶ 5 b \"μ¶°b\nI\n1⁄2¶μe»n X \"μ¶°b _ \"μr°b 5±e X ; 5 ¿oO1⁄4 \" \"μ¶°b 5± : oμ¶2\"(r)\nO\n(r)\noI¥!o\n\"(r) ¿2\"μ¶2\nO\nAD±3o(c) 5 aμee1⁄4+ 5 5± X \" \"± ;μr21μ¶o \"(r)+ I: o μe 5\n1T· · B±K +μ¶°k1T \"μe±3o=±»μ¶o*: oμe 5 !1⁄2¶μr°kμe \"μ¶o+e«· \"±^AD ¿2\"25 ¿2XI;3⁄4 X BμeeT1T \"μee ¿211T \" b \" X·1⁄2r1AD ¿3⁄4.eOi3⁄4 μ %$ X \" ¿o AD\n\n1⁄4+± \"μe ¿oO \"2\nμ¶oO 5 Xe B11⁄2¶2~1T \" \" X·1⁄2r1AD ¿3⁄4=eOi§¦[μe ¿°k1o o=2\"1⁄4°k2_1o 3⁄4.μ¶o*:>o μe 5 225\n\n1⁄4+ ¿o AD ¿211T B ( 5 a1⁄4 o AX1T 5 ¿3⁄4\no\n4(r) μ¶2tμ¶2~AX11⁄2¶1⁄2e ¿3⁄4\n=eEJøMoh7BA=eEaÆTo\nEDEJE5ÆTEk± ¤|TErE7XE\"E=eE(c) ¿A=eEaÆToEXEaE5ÆTE\no\n9(r) 1T Aμ¶2 \"(r)+ 3⁄4 μr2\"AD \" X \"μ<yX ¿3⁄4a· \"±e1⁄2e ¿°\nAX1oe y25±31⁄2ee ¿3⁄4\n5±\n°k1Aa(r) μ¶o+ «· \" ¿AXμ¶2\"μ¶±3owe 1⁄4+ 2 \"(r) 1T °k1piNe <±3o 1⁄2ei.1oZ1T· · B±K +μ¶°k1T \"μe±3o 5± \"(r)+ A \" ¿11⁄2AD±3o(c) \"μroO1⁄4+±31⁄422· B±e1⁄2e ¿°\no\n9(r) μr2[25± B t±» X \" \"± 1AX1o o+± 1e ( ¿1⁄2¶μ¶°kμ¶o1T 5 ¿3⁄4we1⁄4 [±31⁄4+ ~e±311⁄2μ¶2t 5±°k1N; b «AXμ¶ ¿o(c) 11⁄4 25 (±» \"(r) 1peP1μr1⁄2¶1Te1⁄2e\n\" ¿25±31⁄4 BAD ¿2\no\n9(r)1T\nð\nμ¶1⁄2¶1⁄2>e ~ \"(r)+ 2°k1μ¶oi»n±^AX1⁄4 2e ¿1⁄2e±\nð\no\ni\n\na\n«w¬kB (r)°\nn±i,_Z2_\"9 3Eμ£¶ o2{·[\n9(r)+ 1e ¿o X B11⁄2μ¶o μ¶ \"μ¶11⁄2>eP11⁄2¶1⁄4 1· \"±e1⁄2e ¿°\nð\n\nð\n1o(c) [ 5±<25±31⁄2ee ;μ¶2\nP1\n?o\nO'II\n\no\nO\nIUUUpO\nð\nμe \"(r)=μ¶oμe \"μ¶11⁄2$AD±3o 3⁄4 μe \"μe±3o\nI\n\nO~O\nO\no\n9(r) μ¶2~μ¶211»,EaE=>5ÆTEn|(c)EDE13⁄4 μ<$> X \" ¿oO \"μ¶11⁄2$\n\n1⁄4 1T \"μe±3o\no\n1o3⁄4\nII\n\no\nO\nAX1o\ne «e ¿AD 5± a2\nð\n(r)+ ¿o\nð\n(r) 1pe 1§: B25 i_{± B3⁄4+ X b25i^25 5 ¿°\no¤G\n± < ^1°b· 1⁄2e \"(r)+ 1⁄44±^3⁄4+e;^μ¶o*_k1⁄4t1⁄4* ^1⁄2¶ Xi.\n\n1⁄4 1T \"μ¶±3o 2\n(r) 1pe\nOI!1⁄2\\n3⁄4¤Bßwn¿Oao\n9(r)+\no\n3⁄4 X·> ¿o3⁄4+ ¿o AD y°k1pi£ \" Y ¿AD A ^·> X aμ¶°b ¿oO \"11⁄24°k1o μ¶·1⁄4 1⁄2¶1T \"μe±3o22B1⁄4 AB(r)812\n\"1⁄4+ Boμ¶o+e[1ob1T··1⁄2¶μe ¿3⁄4(AX1⁄4+ \" \" ¿oO q±3ob1o 3⁄4(±N$A(c)± ± \"(r)+ X , ^ 5 X Bo 11⁄2+μ¶o*Y>1⁄4+ ¿o AD ¿2T2\"1⁄4 Aa(r)b121obμ¶°b·±325 ¿3⁄4 25i^o 1T· \"μ¶A\nAD±3o 3⁄4 1⁄4AD \"1o AD Aa(r) 1o+e »n \"±3°\n1o+± \"(r)+ X AD ¿1⁄2r1⁄2\no~\n\nð\nμ¶1⁄2¶1⁄22\"1⁄4 · · \" ¿2\"2 \"(r)\no\n3⁄4+ X· ¿o 3⁄4+ ¿o AD »n± 2\"μr°b·1⁄2¶μ¶AXμ¶ i4μ¶o2°k1o(c)i\nAX125 ¿24e> ¿1⁄2¶±\nð\no\nG\nμe B2\" ± B3⁄4+ X 425i^25 5 ¿°k2[1T \" 2o 1T \"1⁄4 B11⁄2μ¶oo+ ¿1⁄4 \"±eμe±31⁄2e±ei\noo\n»qAD±3o+»n \"±3o(c) 5 ¿3⁄4\nð\nμe \"(r)1k(r)μee3(r)+ X ± B3⁄4+ X t25i^25 5 ¿°AD±3oOe X \" «μe < 5±¤: B25 b± a3⁄4+ X K2\"μ¶o AD °b±32\" <25±31⁄2¶1⁄4+ \"μ¶±3oZ·1A8;T1Te ¿2i12\"2\"1⁄4 °b \"(r)μ¶2b»n± B°\noMG\n± A +1°b·1⁄2e \"(r)+\n25 ¿AD±3o 3⁄4 ± B3⁄4+ X 9\n\n1⁄4 1T \"μe±3o\nA\na\na\nÞ\nU\n\nU\nA\na\nÞ\nU\n\nA\nO\n\nIUU\na\nO\nAX1o e 1AD±3oOe X \" 5 ¿3⁄4yeMii \"(r)+ 1 5 B1o 2\"»S± B°<1T \"μe±3o\nIiO\nA\n\nO\nA\na\n5±\n\n?o\nA\nI\n1AA\nO\nA\n\nU\nÐ!U\n\nÐ!U\n\nU\nA\nA\nI\n1A\nIUU¿i3O\n&q\n/b¢¤k) Ar¦ ÆC)EEPE,IE\n9(r)+ ;2Bμ¶°b·1⁄2e ¿2\" °b X \"(r) ±M3⁄4±»25±31⁄2¶eMμ¶o e\n[¥Z\n29μ¶2\n\n1⁄4 1⁄2e X KI\n2°b X \"(r) ±M3⁄4wð\n(r) μ¶Aa(r)3⁄4 μe \" ¿AD \"1⁄2eiA1T· ·1⁄2¶μ¶ ¿2\no\nI\n1II\nv\nO\n1I\nÞaß II\n1I\nOac\nIUU OO\no\noN± B3⁄4+ X ! 5±μro(c) 5 Xe B1T 5 A»n \"±3°) \"(r)+ Aμ¶o μe \"μr11⁄23⁄4 1T \"11T\no\nO\ny1⁄4 ·N 5±\no\nONÐ\nq3⁄4μeeMμr3⁄4+ \"(r)+ «μ¶oO 5 X \"eT11⁄2μ¶oO 5±O\n\n1⁄4 11⁄225 5 X· 2;±»92\"μ<yX\nßhOOÐXF\nO1o 3⁄41T·· \"±K +μ¶°k1T 5\nI\no\nI\nOO¿ß O1O\nI\no\n9(r)μ¶2;°b X \"(r)+±^3⁄4\nð\n± 8;^221o 3⁄4mμ¶2\n25±3°b X \"μr°b ¿291⁄4 25 ¿3⁄4μroi· B1AD \"μ¶AD +e1⁄4 9°!1⁄4 Aa(r)e X 5 5 X 411⁄2e 5 X Bo1T \"μee ¿291T \" ;3⁄4+ ¿2BAD Bμee ¿3⁄4Ae> ¿1⁄2¶±\nð\no\\D\n±3o+ X \"(r)+ ¿1⁄2e ¿2B2 μe\nμ¶2t \"(r)+ bAD±3o AD X· \"1⁄411⁄2qe12\"μ¶2t±»11⁄2r1⁄2± \"(r)+ X !°b X \"(r)+±^3⁄4 2_1o 3⁄4=1i1⁄2¶μe 5 \"1⁄2e 1o 11⁄2ei^2\"μ¶2[e3μee ¿2;μ¶o2\"μee3(r)O [μ¶oO 5±(r)+±\nð\n\"(r)+ Xi\n11⁄2¶1⁄2\nð\n± 8;\no\n&q'&\na«OO) P×)uO¬*)h¥wOEUØ.¬+¬+¢, 3¥_¬dU\n§#/b¢¤!) Ar¦ ÆU)EEIE\no\n!μ¶22 ¿125i. 5±2\" X < \"(r) 1T\n\n1⁄4 1⁄2e X I\n22°b X \"(r)+±^3⁄4AD±3oOe X \"e ¿2 »n± 2 \"(r)+ A25· ¿AXμ¶11⁄2,AX125 A±» \"(r)+ «\n\n1⁄4 1T \"μ¶±3o\na\nOCU\nð\nμe \"(r)2\"±31⁄2¶1⁄4+ \"μe±3o\nI'Ð~O,O\nO\nY Þ\noG\n± [ \"(r) μ¶2 +1°b·1⁄2e 1?ß\nO\nOIUUÞaßU>O\nß\nO\nOIUUÞqÐaUhF\nO\nO\nß\nIUUK3O\n¦9 ¿AX11⁄2r1⁄2¶μ¶o+e \"(r) 1T 41⁄2rμ¶°\nßa\nO\nIUUÞ\nUF\nO\nO\nß\nO\n\nð\n!25 X ; \"(r) 1T 41⁄2rμ¶°\nßa\nO\n1?ß\nO\nO\n\nY Þ\no\n9(r) b»S±31⁄2¶1⁄2¶±\nð\nμ¶o ei1T Be31⁄4 °b ¿oO 1±31⁄4+ \"1⁄2¶μ¶o+ ¿2~ \"(r) b· \"±M±»»n± ;e ¿o+ X B11⁄2,· \"±e1⁄2e ¿°<2t1o3⁄4.e3μee ¿221oN ¿25 \"μ¶°k1T 5 k±»\n\"(r)+ ; X B \"±\noa\n±31⁄4y°<1¿i2n;^μe· 5±k \"(r)+ ;AD±3o AX1⁄2r1⁄4 2\"μe±3o 2e ¿1⁄2e±\nð\nμe»i±31⁄4y1⁄2¶μ<;\no\n\nG\nB±3°\no\nð\ny25 X \"(r) 1T k \"(r)+ X B \"± «μ¶oAX1⁄4+ \" \" ¿3⁄4Zμ¶oZe±3μ¶o eN»S B±3°\noRI\n5±\noaINI\nv\nμ¶2\nkIaß apOaoA¥\n1⁄4 5±\n1AXAX1⁄4 °(1⁄4 1⁄2¶1T \"μe±3oA±» \"(r) ¿25 ACÆK7aATAM X \" \"± B2±pe X O\nOoUFTß\n25 5 X·2^ \"(r)+ ~e31⁄2e±e11⁄2 X \" \"± 1T \"μ¶°b\nÐ\nμ¶2\nkIaß>Oao\n9(r)+\n· \"±M±»$»S±31⁄2¶1⁄2¶±\nð\n2XI\na\nX\n\nI\nO\nI\no\nI\nOÐ\nI\ne 1 \"(r)+ 1 X \" B± 41T\no\nI\no\n±<25 X ;(r)+±\nð\n\nI\ne \"±\nð\nð\nμ¶ \"(r)\n¿\n2\"1⁄4+e 5 B1AD\nII\nv\nO\nI\nÞaß II\nI\nOac\n»n \"±3°\nI\no\nÞaß>O,O\nI\no\nO_Þaß II\nI\no\nO5OÞ<Iaß\na\nO\n5±ke X\n\nII\nv\nO\n\nI\nÞaßIaII\nI\no\nO5OÐII\nI\nO5OÞa<Iaß\na\nO\nY\n· ·1⁄2ei^μ¶o+e( \"(r)+ £æ ¿1o c,11⁄2¶1⁄4 ;9(r)+ X± \" ¿° 5±\nI\n\nII\nv\nn\n\nI\nÞaße\n\nI\nÞ<Iaß\na\nO\nð\n(r)+ X B\ne\nμ¶29 \"(r)+ ;°k14 +μ¶°!1⁄4° ±»\nII\nO\n±pe X t \"(r)+ ;μ¶oO 5 X \"eT11⁄2_±»μ¶oO 5 X \" ¿25\no\n9(r) μr2(r) 12 \"(r)+ ;»n± B°\n\nII\nv\nnMe\n\nI\nÞEe\\\nð\n(r)+ X B\nehO\nUÞ\nße\n>1o 3⁄4\neOe2ß\na\no2IGa~o\nI9(r) μ¶243⁄4 μ%$ X \" ¿o AD ;\n\n1⁄4 1T \"μe±3oμ¶29 \"(r) !2\"1°b (±3o+ ! \"(r) 1T t1T aμ¶25 ¿2\nμ¶oAD±3°b· 1⁄4+ \"μ¶o+ek°b± B 5e31Te ¿2[1o 3⁄41ooO1⁄4 μ¶ \"μe ¿2\nouO\nY\n2\"2\"1⁄4 °kμro+e \"(r)+ ;μ¶o μe \"μr11⁄2 X B \"± 4μ¶2 * \"(r) ¿o\n\nv\nnMe\n\na\nneeiÞEe\n1o 3⁄4\n\nI\nneIUUÞ8cXcXcTÞEe\nI\nrwv\nO\n1⁄4 °<°kμ¶o+e( \"(r)+ ;e X±3°b X 5 aμ¶A;25 X Bμ¶ ¿2\nI\nne\nIe\nI\nÐUpO\neÐ\nU\nne\ne\nI\neÐ£U\n¦9 X· 1⁄2¶1AXμ¶o+e\ne\n1o 3⁄4\ne\neOiA \"(r)+ ¿μe 93⁄4+ :>o μe \"μe±3o 2K\nß\nn\n;ß\ne\nIUUÞZße\nO\nß\nn\n;ß\ne\nKi\nß\nO\n;ß\ne\ni\nÞ\nc\nIUU\næ\nO\ni,ihið?noooiioo\n9(r)μ¶2 X \" \"± 9 ¿25 \"μr°k1T 5 22\"(r)+±\nð\no\n4(r)+ 1e31⁄2e±e11⁄2_ X B \"± 41T\nÐ\nμr2\nkIaß O2I\n»,EaE=ÆTEi|OEDEbAP7{7¿øME5AP7¿A\nOao\no\n4(r)+ 1 X \" \"± 9e B±\nð\n24 M·±3o+ ¿oO \"μ¶11⁄2¶1⁄2eiAμ¶oi \"μ¶°b\no\no\n4(r)+ ~ X \" \"± 9μ¶o AD B ¿125 ¿2\nð\nμe \"(r)\ne\no\n9(r) μ¶22\"1⁄4 ee ¿25 \"2 \"(r) 1T 9±3o ~2B(r)+±31⁄4 1⁄2¶3⁄4A \"1N; 22\"°<11⁄2¶1⁄2e X 25 5 X·2\nð\n(r)+ X \" ~ \"(r)+\n2\"±31⁄2¶1⁄4+ \"μe±3oμ¶24AB(r) 1o+e3μro+ek°b± \" 1 B1T· μ¶3⁄4 1⁄2ei\no~\n\nð\nμ¶1⁄2¶1⁄2 \" X \"1⁄4+ Bo 5±< \"(r) μ¶2e ¿1⁄2e±\nð\no\n9(r) 4 X \" \"± 1o 11⁄2ei^2\"μ¶21Te±pe 1μee3o+± \" ¿2 \"±31⁄4 o3⁄4*_{±N$ X \" \"±\noqo\n»_±3o+ ~12\"2\"1⁄4 °b ¿2 \"(r) 1T 1#:u M ¿3⁄4« X \" \"± μ¶213⁄4 3⁄4+ ¿3⁄4\n1T , ¿1Aa(r)k \"μ¶°b 92\" 5 X·(c) \"(r)+ ¿ok \"(r)+ X \" \"± , ¿2\" \"μ¶°k1T 5 9±»\noU\næ;μ¶2°b±^3⁄4 μ%: ¿3⁄4! 5±\nkIaß OÞ§kISnToU\n\nß\nrwv\nOao\n9(r)1T μ¶2 \"1N;^μ¶o+e1°b± \" 925 5 X· 2q \" ¿3⁄4 1⁄4 AD ¿2q \"(r)+ 93⁄4μ¶2\"AD \" X \"μ<y¿1T \"μ¶±3o! X B \"± K3e1⁄4 $μ¶oAD \" ¿125 ¿2q \"(r)+ \"±31⁄4o 3⁄4*_{±N$« X B \"±\no\n9(r)+ X \" X»n± \" \"(r)+ X \" bμ¶2~1A·±3μ¶oO [±»3⁄4 μ¶°kμ¶oμ¶2\"(r) μ¶o e( B X \"1⁄4+ Bo 2\nð\n(r)+ X \" \"(r)+ 5± \"11⁄2q X \" B± ;μ¶o AD \" ¿125 ¿2~12\nß\n3⁄4+ ¿AD \" ¿125 ¿2\no,÷\nX 5 5 X\n\" ¿2\"1⁄41⁄2e \"2q \"\n\n1⁄4 μe \" o+± ,°b± \" $± \" Oe1⁄4+ °b± \" «AXμ¶ ¿o ADi\no\n9(r)+ ; Xi μ¶2q 5±; \"1N; 4°b± B 5 X B°k2±»> \"(r)+ 9$1¿i^1⁄2e±\n25 X Bμ¶ ¿291o 3⁄4i \" ¿3⁄4 1⁄4AD ; \"(r)+ ;3⁄4 μ¶2BAD \" X \"μ<y¿1T \"μe±3o X B \"± 5±\n<IaßPø(c)O\nð\nμ¶ \"(r)Iu\nW\nUTo\n\n&q'u\nuE-!×E) yu ?E) BÆU)EEPE,IE¦dþ2y=¢O\\×)h¢IENE¥\n\n1⁄41⁄2e X KI\n2°b X \"(r)+±^3⁄4bμ¶21o 11⁄2¶±e±31⁄4 2, 5±21⁄42\"μ¶o+e~1#¦4μ¶ ¿°k1o ob2\"1⁄4 °o 5±2 XeP11⁄2¶1⁄41T 5 91okμ¶oO 5 Xe B11⁄2\nI\n1⁄4 13⁄4+ B1T \"1⁄4+ B\nOI\n1o 3⁄4\nμ¶2μ¶o«»n1AD\n\n1⁄4 μeeT11⁄2e ¿oO μe» \"(r)\n[a¥a\nμ¶2\na\nOII\no\nO5O\nμ¶o« \"(r) 1T\nI\nμ¶2 XeT11⁄2¶1⁄4 1T 5 ¿3⁄41T ±3o 1⁄2eib±3o+ ~ ¿o 3⁄4 ·>±3μro(c) ,±» \"(r)+\n\"μ¶°b 1μ¶oO 5 X \"eT11⁄2\no\\÷\niA1o11⁄2e±ei\nð\nμe \"(r)A \"(r)+ 1 5 B1T· yX±3μ¶3⁄4 11⁄2> B1⁄4 1⁄2¶ t»n±\n\n1⁄4 13⁄4+ B1T \"1⁄4+ B t±3o ;AX1o 1¿e X B1Te ! \"(r)+ 1eT11⁄2¶1⁄4+ ¿2\n±»\nI\n1T 9 \"(r)+ 21⁄2e X»n 91o 3⁄4 aμee3(r)(c) ¿o3⁄4+·±3μ¶o(c) \"21o 3⁄4 ±e \"1μ¶o 25 ¿AD±3o3⁄4*_{± B3⁄4+ X 41AXAX1⁄4 B1ADi I\nII\nv\nO\nI\nÞ\nß\na\nI{II\no\nI\n\nI\nOÞaII\no\nII\nv\n\nII\nv\nO5O>c\nIUU O\nY\noμ¶°<°b ¿3⁄4 μ¶1T 5 !· \"±e1⁄2e ¿°\nμ¶2[ \"(r) 1T\n1II\nv\n1T··> ¿1T a24±3oe± \"(r)y \"(r) 2 Bμee3(r)O ~1o 3⁄41⁄2¶ X»S t2\"μ¶3⁄4+ ¿2[±», \"(r)+ (\n\n1⁄41T \"μe±3ow\n\"(r) μ¶2$μ¶2 \"(r)+ X \" X»n± \" 2\"1μ¶3⁄4! 5±~e> 1oAESuQ A\nE7¿E=^°b X \"(r) ±M3⁄4\no\\[\no+ 1T· · \"±31Aa(r)bμ¶2 5±~1⁄425 1o\n\n1⁄41⁄2e X $2\" 5 X·( 5±~ ¿25 \"μr°k1T 5\nII\nv\no\n4(r)+ 1 \" ¿2\"1⁄4 1⁄2¶ \"μ¶o+eb°b X \"(r)+±^3⁄4 μ¶2AX11⁄2¶1⁄2¶ ¿3⁄4!EXø^o\nEtu<E=nu^ÆK|TI\nA\nII\nv\nO\nI\nÞaß II\no\nI\n\nI\nO\nIUUK3O\n1II\nv\nO\n1I\nÞ\nß\na\nIaII\noaI\n\n1I\nOÞ£II\noRII\nv\n\nA\nINI\nv\nO5Oc\n9(r) μr2 \"1⁄4+ Bo 29±31⁄4+ [11⁄2¶2\"±< 5±<e 22\" ¿AD±3o 3⁄4y± a3⁄4+ X [1AXAX1⁄4+ a1T 5 11⁄2¶ \"(r)+±31⁄4+e3(r)yo ± [12t25 \"1Te1⁄2e 212[ \"(r)+ !e ¿oO1⁄4 μro+ ; 5 B1T· _\nyX±3μ¶3⁄4 11⁄2 B1⁄4 1⁄2e\no\nY\no+± \"(r)+ X 42\" ¿AD±3o 3⁄4 ± B3⁄4 X 4°b X \"(r)+±^3⁄4 μ¶2 \"(r)+ <u(E|Q^ÆESou=u<E=nu^ÆK|TI\nA\nII\nv\nO\nI\nÞ\nß\na\nII\no\nI\n\nI\nO\nIUUK^3O\nII\nv\nO\nI\nÞaß II\no\nII\nv\n\nA\nII\nv\nOac\n÷\n± \"(r)1⁄4[ ¿1⁄4 o1o 3⁄4y°kμr3⁄4+·±3μ¶o(c) e ¿1⁄2e±3o+e< 5±< \"(r)+ !»n1°kμr1⁄2ei«±»,2\" ¿AD±3o 3⁄4 ± B3⁄4 X 4ø^oOþOE> !øE=={A2°b X \"(r)+±^3⁄4 241o 3⁄41T \"\nAD±3o 2\"μr3⁄4+ X \" ¿3⁄4\nÆTo>E >\nE=EaQm°b X \"(r)+±^3⁄4 2(2\"μ¶o AD A \"(r)+ XiN \"\n\n1⁄4 μe B <±3o 1⁄2ei. \"(r) «eT11⁄2¶1⁄4+ A±»\n1T \"(r)+ i1⁄2¶125 ( \"μ¶°b i25 5 X·Z 5±\n25 \"1T \"\noo÷\n± \"(r) \"\n\n1⁄4 μe \" ~\nð\n±«»n1⁄4 oAD \"μe±3oi XeP11⁄2r1⁄4 1T \"μe±3o 2· X 42\" 5 X· ¿E,±3o ;»S±\n\n1⁄4 1⁄2¶ X\no\n1⁄44±\nð\nXe X K> \"(r)+ ;e31μro μ¶o\n1AXAX1⁄4+ B1ADi.»e1T ;±31⁄4+\nð\n¿μ¶e3(r) 2; \"(r)+ < M 5 B1 $>± \"\nI\n25 X\n\nM X aAXμ¶25\n3Oao2\ne ¿oN»e1⁄4+ \" \"(r)+ X 1e31μro 221T \" «1AB(r) μe Xe ¿3⁄4eOi\ne±3μ¶o+ei 5± \"(r)+ »n±31⁄4+ \" \"(r)± B3⁄4+ X #¦[1⁄4 o+e _;1⁄4 5 \"1A°k X \"(r)+±M3⁄4\nI\n¦\nOO\nð\n(r) μrAB(r)=2\"1°b·1⁄2¶ ¿2\nI\n»n±31⁄4+ 1 \"μ¶°b ¿2t· X ~ \"μ¶°b\n25 5 X·I\n3⁄4\nv\nO'II\noaI\n\n1I\nO\nI\naN\nO\n3⁄4\na\nOII\no\nII\n\n1I\nÞ\nß\na\n3⁄4\nv\nO\n3⁄4\na\nOII\no\nII\n\n1I\nÞ\nß\na\n3⁄4\na\nO\n3⁄4\n\nO'II\no\nINI\nv\n\nI\nÞ£ßE3⁄4\na\nO\nINI\nv\nO\nI\nÞ\nß\næ\nI3⁄4\nv\nÞ\na\n3⁄4\na\nÞ\na\n3⁄4\na\nÞ@3⁄4\n\nO\nð\n(r)+ X B\no\nII\n\nO\no\nI\nÞßhF\na\noD\n± 5 [ \"(r) 1T\nI\nμ¶2· \"±e ¿3⁄4k1T ¿1Aa(r)A ¿o 3⁄4k·±3μ¶oO 1o 3⁄4<\nð\nμ¶AD tμ¶ok \"(r) [°kμ¶3⁄43⁄4 1⁄2e ±»_ \"(r)+\nμ¶oO 5 X \"eT11⁄2\no\n9(r)+ 21o 11⁄2¶±e±31⁄4 2\n\n1⁄4 13⁄4 B1T \"1⁄4+ \" 2°k X \"(r)+±M3⁄4μ¶2\nμ¶°b·25±3oI\n2 B1⁄4 1⁄2e ð\n(r) μ¶Aa(r) μ¶241\nð\n¿μ¶e3(r)(c) 5 ¿3⁄41¿e X B1Te b±»\n\"(r)+ 1 5 B1T· yX±3μ¶3⁄411⁄21o 3⁄4°kμ¶3⁄4 ·>±3μro(c)\n\n1⁄413⁄4+ B1T \"1⁄4+ \" ~ B1⁄41⁄2e ¿2\no\n9(r) μ¶2μr21bAD±3°k°b±3o 1⁄2eii1⁄4 25 ¿3⁄4μro(c) 5 Xe B1T \"μ¶±3o °b X \"(r)+±^3⁄4we1⁄4+ bo+± b \"(r) Ae ¿25\no@G\n± <°<1o(c)iZ2\"AXμe ¿oO \"μ¶25 \"2\nð\naμe \"μ¶o+e \"(r)+ ¿μe ±\nð\no£AD±M3⁄4 ¿2μe b1T· · ¿1T B2 5±. B X· \" ¿25 ¿oO b \"(r)+\næ\n\n·25i^AB(r) ±31⁄2e±e3μ¶AX11⁄2e \" ¿1N;._{ Xe ¿oa·±3μ¶oO e> X\nð\nX ¿o\nμ¶o(c)e ¿2\" \"μ¶o+e=°b± \" $± \" kμ¶oh· \"±e B1°<°kμ¶o+e ¿EZμ¶o(c)e ¿2\" \"μ¶o+e\n°b± \"\n\nO\n\n\"μ¶°b\no\n¦\n\nμ¶2~11⁄2¶25±1ie \" ¿1N;._{ Xe ¿o.·±3μ¶oO tμ¶o \"(r)+ b25 ¿o25 \"(r) 1T 1±3o+ bAX1o o+± 1e X\n\n\"(r)=± B3⁄4 X ;1AXAX1⁄4+ B1ADi\nð\nμe \"(r)\n\n»n1⁄4o AD \"μe±3o XeP11⁄2r1⁄4 1T \"μe±3o 21i°kμro μ¶°(1⁄4 °\n±»æ«1T B ( B\n\n1⁄4 μ¶ \" ¿3⁄4\no\næy± B X±pe X Kð\nμe \"(r).æ«»n1⁄4 oAD \"μe±3oy XeT11⁄2¶1⁄4 1T \"μ¶±3o 2 \"(r)+ X \" i1T \" °k1oOiAD±3°!eμ¶o 1T \"μe±3o22 \"(r) 1T e3μee\n\n\"(r)Z±\n\n\"(r)h± B3⁄4+ X b1AXAX1⁄4+ B1ADi\no\n9(r)μ¶2;±· ¿o 2 1⁄4+·Z1\nð\n1¿im 5±\n3⁄4+ B1°<1T \"μ¶AX11⁄2¶1⁄2eiNμ¶°k· \"±pe ¦[1⁄4 o+e _;1⁄4+ 5 \"1eMim°b±3oμe 5± Bμ¶o+e \"(r)+ 1⁄2e±^AX11⁄2 X B \"± b1o 3⁄4ZeP1T BiMμ¶o ey \"(r)+ 25 5 X· E_2Bμ<yX\no\n[\no+ 1⁄4 ·3⁄41T 5 i°<1T \"AB(r)+ ¿2< \"(r)+ 1piM1⁄2¶± <25 X Bμe ¿2<1⁄4+·a 5±\n\n\"(r)£± B3⁄4 X K1o 3⁄4Z \"(r) ± \"(r)+ X < 5±\n\n\"(r)\n± B3⁄4+ X\no\n9(r)M1⁄4 2 \"(r)+ (3⁄4 μ%$ X \" ¿o AD 2e X\nð\nX ¿o \"(r)+ !\nð\n±iμr241ke±M±^3⁄41T· · \"±K +μ¶°k1T \"μe±3oy 5±« \"(r) ! X \" B± [μ¶oy \"(r)+\n\n\"(r)± B3⁄4+ X t1⁄4+·_3⁄4 1T 5\no\n9(r)+\n\n\"(r)i± B3⁄4+ X 1⁄4+·_3⁄4 1T 5 ~μ¶21⁄4 25 ¿3⁄4A 5±k13⁄4+eT1o AD ~ \"(r)+ 125±31⁄2¶1⁄4+ \"μ¶±3owO1o 3⁄4A \"(r)+ ~ X \" B± ¿2\" \"μ¶°k1T 5 ;μ¶21⁄4 25 ¿3⁄4A 5±k13⁄44\"1⁄4 25\n\"(r)+ 25 5 X·a2\"μ<yX\no\n4(r) μ¶2; 5 aμ¶A8;1o3⁄4h1=25 X ±»41⁄42\"1Te1⁄2e iAD±O AAXμe ¿o(c) \"2b1T B i3⁄41⁄4+ < 5±\nG\n¿(r) 1⁄2¶e> X BeEe1⁄4+ ( \"(r)+ X \" 1T \"\n°k1oOi eT1T Bμ¶1oO \"2\no4o\n°b·1⁄2e ¿°k ¿o(c) \"1T \"μe±3o3⁄4+ X \"1μ¶1⁄2¶2t1o 3⁄4AD±M3⁄4 (AX1oe 2»n±31⁄4 o 3⁄4μ¶oq1\nU\na43\no\nY\n°b± \" o 1μee (°b X \"(r)+±^3⁄4\ne125 ¿3⁄4i±3oA \"1N;^μ¶o+eb25 5 X·2±»$2\"μ<yX\nß\n1o 3⁄4a\nß\n5±(e X 91oi X B \"± ¿25 \"μ¶°k1T 5 ;11⁄2¶2\"±\nð\n± 8;^2^e1⁄4+ \"\n\n1⁄4 μe \" ¿2o+ ¿1T B1⁄2¶i\n\nð\nμ¶AD k12;°k1oOi»e1⁄4 o AD \"μ¶±3o= XeP11⁄2¶1⁄41T \"μe±3o 2~· X ;25 5 X·\noH÷\n± \"(r)N2\"μr°b·1⁄2e 1o 3⁄4.13⁄4 1T· \"μ¶e be X B2\"μe±3o 2;±»o¦\n\n1T \"\nμ¶°b· 1⁄2e ¿°b ¿oO 5 ¿3⁄4μ¶ob1o 3⁄4 \"!#!\no\n&q%$\n&< ?)uE-\n¬dET 'pa« 3 P)¬dE §ÆC)EEPE,IE¦\n1⁄4[ ¿1⁄4 owI\n29°b X \"(r) ±M3⁄4\nIUUK3O\nμ¶2911⁄2¶2\"±k1o +1°b·1⁄2e 1±»1,Q E\"E8|TE7K={ÆE>n7BÆTEJE\"E87K={ÆE~°k X \"(r)+±M3⁄4I,1o\n\n1⁄4 1⁄2¶ X 425 5 X·yμ¶241⁄4 25 ¿3⁄4\n5±(· \" ¿3⁄4 μ¶AD\nINI\nv\n+1o 3⁄4<1! 5 B1T· yX±3μ¶3⁄4 11⁄2 B1⁄4 1⁄2¶ 425 5 X·AD± \" \" ¿AD \"2μe\nI\nμ¶°b· \"±He ¿2 \"(r) t1AXAX1⁄4+ a1ADi\nOao\n± μ¶°b·1⁄2e ¿°k ¿o(c)\n\"(r)+ »e1⁄4 1⁄2¶1⁄2 5 a1T·> yX±3μr3⁄4 11⁄2$ B1⁄4 1⁄2e ±3o+ b°(1⁄4 25 ~25±31⁄2ee \"(r)+ bo+±3o*_1⁄2¶μro+ ¿1T [\n\n1⁄4 1T \"μe±3o»n±\nINI\nv\no,[\no\nð\n1¿i=μ¶2t 5±1⁄4 25\nD\n\nð\n5±3owI\n2°k X \"(r)+±M3⁄4\no\nY\n2\"μ¶°b· 1⁄2e X 1T· · \"±31Aa(r)kμ¶2q 5±2μe 5 X a1T 5 9 \"(r)+ 9AD± \" \" ¿AD 5± 25 5 X·\no[\noAD 91okμ¶o μe \"μr11⁄2(c) ¿25 \"μr°k1T 5\n1)(\nO+*\nII\nv\nμr2±e \"1μ¶o+ ¿3⁄4 1⁄42\"μ¶o+e\n\n1⁄41⁄2e X K+μe 9AX1o e ;1⁄4 2\" ¿3⁄4i 5±ke X [1be X 5 5 X 9 ¿2\" \"μ¶°k1T 5 1)(\nv\n*\nII\nv\nO\nI\nÞ\nß\na\n,\nII\no\nI\n\nI\nOÞaII\no\nII\nv\n\n1)(\nO+*\nII\nv\nO.-\nI\na\nUpO\n1o 3⁄4N2\"± ±3oN1⁄4 oO \"μ¶1⁄2q \"(r)+ k2\"1⁄4AXAD ¿2\"2\"μee keT11⁄2¶1⁄4+ ¿2;±»\n1)(\nø\n*\nII\nv\n3⁄4 μ%$ X ;eOi=1⁄2e ¿2\"2; \"(r) 1oN1· B ¿2\"AD Bμee ¿3⁄4 5±31⁄2e X B1o AD\no\n9(r)+\nμe 5 X B1T \"μ¶±3oμr24e31⁄4 1T a1o(c) 5 X ¿3⁄4= 5±AD±3oOe X \"e k· \"±HeMμr3⁄4+ ¿3⁄4 \"(r) 1T\nß\nμr2[2\"°<11⁄2¶1⁄2 ¿o ±31⁄4+e3(r)\noa~\n\nð\nμ¶1⁄2¶1⁄2o+ M ~3⁄4+ ¿2\"AD aμee !1\n°b± \" ;2\"±·(r) μ¶25 \"μrAX1T 5 ¿3⁄4· \" ¿3⁄4 μrAD 5± n_AD± \" \" ¿AD 5± 4°k X \"(r)+±M3⁄4 »S±\n[¥Z\n2+e1⁄4+ 5 a1T·> yX±3μr3⁄4 11⁄2 B1⁄4 1⁄2e ~e12\" ¿3⁄4 °b X \"(r)+±^3⁄4 2\n1T \" ;±»n 5 ¿oy1⁄425 ¿3⁄4i»S±\nO\n¥a\no\n&q0/\nÆ\n¢¤EP-1X¦E)32ÆU)EEIE¦\nY\n1⁄2e \"(r)+±31⁄4+e3(r) ¦41⁄4 o e _;1⁄4+ 5 \"1\nð\nμe \"(r)y13⁄41T· \"μee 25 5 X· ·μ¶o e\nð\n± {;M2[· \" X 5\ni\nð\n¿1⁄2¶1⁄2»n± t· \"±e1⁄2e ¿°<2\nð\nμe \"(r)AD±3°b·1⁄2e\nI\nI\n»e1⁄4 o AD \"μ¶±3oi XeP11⁄2¶1⁄41T \"μe±3o 2· X 9 \"μ¶°b ;2\" 5 X·y°k1¿iie ;1k25 5 X X·y· Bμ¶AD 1 5±k· 1¿i\no\no.ø^A=eE>E =ERQA°k X \"(r)+±M3⁄421⁄4 25\n\"(r)+ ;eT11⁄2¶1⁄4+ ¿2±»\n1T 425 Xe X B11⁄2· \" Xe^μe±31⁄4 2 \"μ¶°k ;25 5 X·2\no\n9(r)\nY\n3⁄4 1°<29°b X \"(r)+±^3⁄4 29(r) 1pe 2 \"(r)+ 1e ¿o X B11⁄2»n± B°I\nII\nv\nO\nI\nÞaß465%78\n\nu\nI\no\nO\nPo\n\nI\naa\nO\nð\n(r)+ X B u\nI\no\nO\nμ¶oO 5 X \"·±31⁄2¶1T 5 ¿2\nII\no\n\nI\no\nO5Oao\n9(r)+ ~ ^·1⁄2¶μ¶AXμe :94_25 5 X·\nY\n3⁄41°k2i_\n÷\n12\"(r)+»n± \" \"(r)\nI\nY\n÷O\n°k X \"(r)+±M3⁄4μro(c) 5 X \"·±N_\n1⁄2¶1T 5 ¿2[1T\noRI\n\noRI\nrwv\nXcXcXc\noaI\nr3;\n25±\nINI\nv\nO\nI\nÞaß\nJOXI\nI\nÞ@\nv\nI\nI\nrwv\nÞcececPÞ@\n;rwv\nI\nI\nr3;\nI\nv\n\nI\na\ni3O\n\nð\n(r)+ X B\nI\nI\nO\nII\noaI\n\nI\noRI\nO5Oao\n9(r)+ iμ¶°b·1⁄2rμ¶AXμe <94_25 5 X·\nY\n3⁄4 1°k2i_kæ±31⁄4 1⁄2e 5±3o\nI\nY\næ\nO\n°b X \"(r) ±M3⁄4Zμ¶oO 5 X \"·±31⁄2¶1T 5 ¿2 1T\no\nINI\nv\n\no\nI\nXcXcXc\no\nII\nvnr3;\n25±\nINI\nv\nO\nI\nÞaß\n1%=\nJOXI\nII\nv\nÞ\n=\n\nv\nI\nI\nÞcececPÞ\n=\n\n;rwv\nI\nI\nr3;\nI\na\nc\nI\na\nOO\n9(r)+ 94_25 5 X·°b X \"(r)+±^3⁄4 29(r) 1pe ; 5 B1⁄4 o AX1T \"μ¶±3o X \" \"±\n<Iaß\n;\nOao\nY\n÷\n1o 3⁄4\nY\næ\n1T \" e ¿o+ X B11⁄2¶1⁄2¶it1⁄4 2\" ¿3⁄4;121· B ¿3⁄4 μ¶AD 5± n_AD± \" B ¿AD 5± ·1μe\no\nY\næ\nAX1o2e> ,μ¶ 5 X B1T 5 ¿3⁄41 \" X· X \"μe \"μee ¿1⁄2¶i\n1⁄4 oO \"μ¶1⁄2μe 2AD±3oOe X \"e ¿2qe1⁄4 ;μ¶o.· B1AD \"μrAD k±3o+\nO\n\n25 5 X·hμ¶2;AD±3o 2\"μr3⁄4+ X \" ¿3⁄4=e ¿25\no<o\n»μ¶o 2\"1⁄4E«AXμe ¿oO ~1AXAX1⁄4 B1ADiNμ¶2\n±e \"1μ¶o ¿3⁄4w μe 9μ¶2e X 5 5 X 4 5±< \" ¿3⁄4 1⁄4 AD 1 \"(r)+ 2 \"μ¶°b ;25 5 X· \"(r)1o 5±<μe 5 X B1T 5 2»n1⁄4+ B \"(r)+ X\no\n9(r) ;°b±325 4AD±3°k°k±3o 1⁄2eii1⁄4 25 ¿3⁄4°b ¿°!e X B2±»$ \"(r)+ ;»e1°kμ¶1⁄2eiA1T \" 1 \"(r)+\n\n_25 5 X·\nY\n÷\nI\n1INI\nv\nO\n1I\nÞ\nß\na\n\nI\nI\nÐE^I\nI\nrwv\nÞaiP TI\n1I\nr\na\nÐq^I\nI\nr\na\n\nI\na\n3O\n1o 3⁄4\n\n_2\" 5 X·\nY\næI\nINI\nv\nO\nI\nÞ\nß\na\n\n^I\nII\nv\nÞ'UK^I\nI\nÐqI\nI\nrwv\nÞaI\nI\nr\na\nc\nI\naæ\nO\nG\n± t1k3⁄4+ X BμeeT1T \"μe±3oi±»$ \"(r)+ 225 5 a1o+e _1⁄2e±M±;Mμ¶o ekAD±O «AXμ¶ ¿o(c) \"2 25 X 21\næ43\no\n9(r) k25 5 \"±3o+e 2\"1⁄4μe ~±»\nY\n÷?>\nY\næ\nμ¶2~ \"(r)1T ;±3o 1⁄2ei=ai»n1⁄4o AD \"μe±3o= XeT11⁄2¶1⁄4 1T \"μe±3o 211T B <o+ X ¿3⁄4+ ¿3⁄4.· X ;25 5 X· ¿1Aa(r)\neT11⁄2¶1⁄4+ «μr2!1⁄4 2\" ¿3⁄4\n\n\"μ¶°b ¿2K,o+± (3⁄4μ¶2\"AX1T B3⁄4+ ¿3⁄4m12 μ¶oq¦\n+o\n9(r) μ¶2!μ¶2(11⁄2¶25± \"(r)+\nY\nAB(r)μ¶1⁄2¶1⁄2e ¿2!(r)+ X ¿1⁄2,(r)+±\nð\nXe X ¿I« 5±\n25 \"1T \"\neP11⁄2r1⁄4+ ¿2±»\n1T \" !o+ X ¿3⁄4+ ¿3⁄4\no\n9(r) ¿25 21T \" 1e ¿o+ X a11⁄2¶1⁄2eiA· \"±pe^μ¶3⁄4+ ¿3⁄4ieMi ¦@\n+o\nY\n:u M ¿3⁄4A25 5 X·E_2\"μ(c)yX [e X B2Bμe±3o<±»\nY\n÷?>\nY\næμ¶2μ¶o AX1⁄2¶1⁄43⁄4+ ¿3⁄4kμ¶oA B!#!w(c)e1⁄4+ \"(r)+ ~°b X \"(r)+±^3⁄4«μ¶2°b±325 1⁄4 25 X»e1⁄4 1⁄2\nð\nμe \"(r)\neT1T Bμ¶1Te1⁄2e 25 5 X·2\no\n9(r)+ I; Xi=μ¶2t \"(r) 1T 1 \"(r)+ X \" \"± ;AX1o=e ( ¿25 \"μ¶°k1T 5 ¿3⁄4.»n \"±3°w \"(r)+\nO\n1o 3⁄4\n\neT11⁄2¶1⁄4+ ¿2~±»\nINI\nv\no\n9(r)+ ;1⁄2¶±MAX11⁄2_ X B \"± »S± [e± \"(r)\nY\n÷\n1o 3⁄4\nY\næ\nμ¶2\n<Iaß\nt\nO\ne1⁄4\nð\nμe \"(r)3⁄4 μ%$ X \" ¿oO 9AD±3o 25 \"1oO \"2XI\nO\n1\"C\nÞ@\nC\nß\nt\nI\na\nO\nO\n1ED\nÞ@\nD\nß\nt\nI\na\n3O\n1⁄4+e 5 B1AD \"μ¶o+e\nð\n2(r) 1¿e\n\nO\n1\"C\nÐ\n1ED\nÞI\nC\nÐV\nD\nOUß\nt\nI\na\n^3O\nt2\"μ¶o+e\no\na\n\nð\n2AX1oy ¿1⁄2¶μ¶°kμ¶o1T 5\nß\nt\n5±ke X 41oy ¿25 \"μ¶°k1T 5 ;»n± 9 \"(r)+ 2 X \" \"± 4μ¶oi \"(r) ;AD± \" \" ¿AD 5± teP11⁄2¶1⁄4 1ED\nI\nÐ\nD\np\nI\nCÐ\nDO\n\nD\n\nD\nÐy\nC\nc\nIai\n\nO\nt2\"μ¶o+ekeT11⁄2¶1⁄4+ ¿2\nD\n1o 3⁄4\nC\n±e \"1μ¶o ¿3⁄4 »n \"±3°25 \"1o3⁄4 1T B3⁄4 »n± B°(1⁄4 1⁄2¶12»n± 4 \"(r) 2 X \" \"± tμ¶o·±31⁄2ei^o+±3°kμ¶11⁄2_μro(c) 5 X \"·± N_\n1⁄2¶1T \"μe±3o\nð\n1 ¿o 3⁄4y1⁄4+·\nð\nμe \"(r)\nF\nÐ\n1ED\nF\np\nUK^\na\n\nF\n1\"C\nÐ\n1ED\nF\nc\nIai^UpO\no\n»\nμ¶21be ¿AD 5± >1k2BAX11⁄2e ¿3⁄4 2\"1⁄4 °\n±»$ \"(r) ;AD±3°b·±3o+ ¿o(c) \"2[2\"(r)+±31⁄4 1⁄2r3⁄4ie> ;1⁄425 ¿3⁄4\noo\n»$ \"(r) 1 X \" \"± [μr2e \" ¿1T 5 X [ \"(r)1oy1\n· \" ¿2\" X [°k14 +μ¶°(1⁄4 °ß\nμ¶2[(r) 11⁄2ee ¿3⁄4wμe»$ \"(r)+ 2 X \" B± tμ¶2[1⁄2e ¿2\"24 \"(r) 1o \"(r)+ (°kμ¶o μr°!1⁄4 ° ß\nμr243⁄4+±31⁄4+e 1⁄2e ¿3⁄4\noo~\n(r)+ ¿o\nß\nμ¶2\n(r) 11⁄2ee ¿3⁄4o\nð\n25 \"1T B \"μ¶o+eAeP11⁄2¶1⁄4 ¿2tAX1oe (±e \"1μ¶o+ ¿3⁄4eOiμ¶o(c) 5 X B·>±31⁄2r1T \"μ¶o+e\nð\nμe \"(r)¦\n+oa~\n(r) ¿o\nß\nμr2t3⁄4 ±31⁄4+e1⁄2e ¿3⁄4w\no+\nð\n25 \"1T \" \"μro+e eP11⁄2¶1⁄4 ¿2AX1oie t±e \"1μ¶o+ ¿3⁄4ieMi«1⁄42\"μ¶o+e! Xe X \"i«± \"(r) X ±31⁄2¶3⁄4i25 5 X· +· \"±HeMμr3⁄4+ ¿3⁄4\n\n±31⁄2r3⁄4«·±3μ¶oO \"21T \"\n2\"1pe ¿3⁄4\no\nY\n3⁄4 3⁄4 μ¶ \"μe±3o 11⁄2$μ¶°b·1⁄2e ¿°k ¿o(c) \"1T \"μe±3o3⁄4+ X \"1μ¶1⁄2r21AX1o=e> (»n±31⁄4 o 3⁄4=μ¶oa1\nU\na43!11⁄2e±3o e\nð\nμe \"(r)=1«·±31⁄2e ¿°<μ¶A!1Te31μro 25\nO\n\n°b X \"(r)+±^3⁄4 2\no\n\n&q0G\n(E¥IH-\n¤k-!EKU'¥(r)OE\n(EP-KJO/ML$¢¥hE-O¦\n±!»n1T KO±31⁄4+ AB(r)+±3μ¶AD [±»\nß\n(r) 12,e X ¿o«3⁄4 μ¶AD \"1T 5 ¿3⁄4«±3o 1⁄2ei eOik1AXAX1⁄4 B1ADi\no\\~\n\nð\nμ¶1⁄2¶1⁄2 o ±\nð\n25 X 4 \"(r) 1T 25 \"1Teμr1⁄2¶μe i °(1⁄4 25\n11⁄2¶25±be ;AD±3o 2Bμ¶3⁄4+ X \" ¿3⁄4\no\n4(r) μ¶2μ¶2 ¿25· ¿AXμ¶11⁄2r1⁄2ei<AD Bμe \"μrAX11⁄2_μ¶o· \"±e 1⁄2e ¿°k2\nð\nμe \"(r)°(1⁄4 1⁄2e \"μe· 1⁄2e [ \"μr°b ;2\"AX11⁄2e ¿2\no\nY\nð\nk2\"1\nð\nμro\noiUK\n1⁄4 1⁄2e X 11T· · 1⁄2¶μe ¿3⁄4 5±\na\nO\nU\ne3μ¶e ¿2\n1?ß\nO\nOIUU9Þ8ßU>O\nß\nð\n(r) μ¶Aa(r).AD±3o(c)e X \"e ¿2\n5±\nO\n\nY\nßE\n\no2~\nA11⁄2¶25±· \"±He ¿3⁄4N \"(r) 1T ! \"(r)+ k X \" B± 23⁄4+ ¿AD \" ¿125 ¿2(1⁄2¶μ<;\nßo\n1⁄4[±\nð\nXe X Kμe»\nUZn\n*$ \"(r)+ ¿o\no+± b±3o 1⁄2¶i\nð\nμ¶1⁄2r1⁄2 \"(r) X \" \"± be 1⁄2¶1T \"e μe»\nß\nμ¶2 \"1N; ¿o\n5±O±eμeeEe 1⁄4+ \"(r)+ oO1⁄4°b X Bμ¶AX11⁄225±31⁄2¶1⁄4 \"μe±3o\nð\nμ¶1⁄2¶1⁄2e \"±\nð\n^·>±3o ¿o(c) \"μ¶11⁄2r1⁄2ei<μ¶o 2\" 5 ¿13⁄4«±»q3⁄4+ ¿AX1piMμ¶o eb M·±3o+ ¿oO \"μ¶11⁄2¶1⁄2eik1⁄2rμ<; ~ \"(r)+ 1 5 B1⁄4+ 125±31⁄2¶1⁄4 \"μe±3o\noqo\noi± a3⁄4+ X 5±ke31⁄4 1T B1oO 5 X 21\n3⁄4+ ¿AX1piMμ¶o e<25±31⁄2¶1⁄4+ \"μ¶±3o\nß\n°(1⁄4 25 42\"1T \"μr25»Si\nF\nUÞaßhU\nF\nn\nU\nIai\na\nO\n±\n\nn\nß n\nÐ\na\nU\nIaii3O\nI\næ±3o+± 5±3o μ¶A;3⁄4 ¿AX1¿i \"\n\n1⁄4 μe \" ¿2\nß nÐ2UFUouO\nY\no 11⁄2e 5 X Bo1T \"μee ; \"(r) 1T 41pe±3μ¶3⁄4 24 \"(r) μ¶23⁄4 μ%AAX1⁄4 1⁄2e iAμ¶2 5±<1⁄4 2\" ;1oyμ¶°b· 1⁄2¶μ¶AXμe °b X \"(r)+±^3⁄4w\"AP7NPOATEn|RQøMA¶EDEJI\nII\nv\nO\nI\nÞZßII\no\nII\nv\n\nII\nv\nOac\nIai4OO\no\noe ¿o X B11⁄21io+±3o*_1⁄2rμ¶o+ ¿1T t\n\n1⁄4 1T \"μ¶±3o.°!1⁄4 2\" te b25±31⁄2ee ¿3⁄4=»n±\nINI\nv\n_e1⁄4 t»n± ~±31⁄4+ ;1⁄2¶μ¶o ¿1T [ +1°b·1⁄2e\nð\nbe X\n\"(r)+ ;»n±31⁄2¶1⁄2e±\nð\nμ¶o+e( \" ¿AX1⁄4+ a2\"μe±3o_I\n1II\nv\nO\nU\nUÐmßhU\n1I\nIai3O\nG\n\"±3°\n\"(r)+ (1piM1⁄2e± ~25 X Bμ¶ ¿2\nI oX3O\nð\n!2\" X 2 \"(r) 1T t \"(r) μ¶291Te B X ¿2\nð\nμe \"(r)»n±\nð\n1T a3⁄4\n\n1⁄41⁄2e X 9 5±2: B25 9± a3⁄4+ X K 2\"±<μe\n5±M±\nð\nμr1⁄2¶1⁄2 AD±3oOe X \"e 1 5±! \"(r) t25±31⁄2r1⁄4+ \"μe±3o\nð\nμe \"(r)ke31⁄2e±e 11⁄2 X \" B±\n<Iaß OaoG\n1⁄4 \" \"(r)+ X B°b± B O \"(r)+ t25±31⁄2¶1⁄4+ \"μ¶±3o\nð\nμr1⁄2¶1⁄2+11⁄2\nð\n1¿i^2\n3⁄4+ ¿AX1pi »S± ~1oOi\nU\nð\n(r) μ¶Aa(r)yμ¶2[o+ Xe31T \"μee\no9\n±E \"(r) !25±31⁄2¶1⁄4 \"μe±3o °k1pi e !μ¶o 1AXAX1⁄4 B1T 5 e 1⁄4+ 4μe\nð\nμ¶1⁄2¶1⁄2_o+ Xe X ~e1⁄2¶±\nð\n1⁄4+·\noio\noN»e1AD μe»\nß\nμr2;e X \"iN1⁄2¶1T Be $ \"(r) «25±31⁄2¶1⁄4 \"μe±3o\nð\nμ¶1⁄2r1⁄2qe> «3⁄4 1°b· ¿3⁄4. Xe ¿oZ°b± \" « B1T·μ¶3⁄4 1⁄2¶i\no\n9(r) 1T (μ¶2$ \"(r)+\n°b X \"(r)+±^3⁄4·1⁄42\"(r)+ ¿2 \"(r)+ ;3⁄4+ ¿AX1piMμro+e<25±31⁄2¶1⁄4 \"μe±3oi· \" ¿°k1T \"1⁄4 \" ¿1⁄2ei< 5±\nð\n1T a3⁄4 24μe \"2425 5 ¿13⁄4+i._25 \"1T 5 !eT11⁄2¶1⁄4+ ~±»I\no\n9(r)M1⁄4 2(c)e1A8;\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X μ¶21⁄4 o AD±3o 3⁄4 μ¶ \"μe±3o 11⁄2¶1⁄2ei;2\" \"1Te1⁄2e 9»n± 1oOib\n\n1⁄41T \"μe±3o\nð\nμ¶ \"(r)b3⁄4+ ¿AX1¿i^μ¶o+e! M·±3o+ ¿oO \"μ¶11⁄2\n25±31⁄2¶1⁄4 \"μe±3o 2ð\n(r)+ X B ¿121»S±\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X 1μ¶2125 \"1Te 1⁄2e bAD±3o 3⁄4 μe \"μe±3o ¿3⁄4=±3o= \" ¿25 5 Bμ¶AD \"μro+e\nß_o I![\no= \"(r)+ b± \"(r)+ X !(r) 1o 3⁄4w \"(r)+ e1A{;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X b25±31⁄2¶1⁄4 \"μe±3ohe \"±\nð\nð\n(r)+ ¿o\nU\nW\n*e1⁄4 2\"±N3⁄4+±O ¿2 \"(r) \" ¿11⁄225±31⁄2¶1⁄4+ \"μ¶±3ow,25±\nð\nAX1owI\n\nAD±3°b·1⁄2r1μ¶o\nouO\nY\n· ·1⁄2ei^μ¶o+e( \"(r)+ ; 5 a1T·> yX±3μr3⁄4 11⁄2 B1⁄4 1⁄2e\nIUU O\n5±k \"(r)+ 22\"1°k 2AX125 ;e3μee ¿2\nINI\nv\nO\nUÞ£ßUhF\na\nUÐhßUhF\na\nI\nc\nIai\næ\nO\na\nμ(c); ;e1A8;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X K+ \"(r) μr293⁄4+ ¿AX1¿i^2[»S± t11⁄2¶1⁄2\nUn\n* e1⁄4+ \"(r)+ !°!1⁄4 1⁄2¶ \"μe·1⁄2ei^μ¶o+e(»n1AD 5±\n\nÐ!U\nU ÐTS\n25±k \"(r) 1 5 B1T· yX±3μ¶3⁄4 11⁄2AX1o 2B1⁄4*$> X 4»S \"±3°2\"1⁄2e±\nð\n1⁄2ei«3⁄4 1°k·> ¿3⁄42\"·1⁄4+ Bμe±31⁄42±32BAXμ¶1⁄2¶1⁄2¶1T \"μe±3o2\no\nY\no= M· 1⁄2¶μ¶AXμe ~11⁄2e 5 X Bo 1T \"μ¶e k°b X \"(r)+±^3⁄4wE+U Q^ÆTo>EXou=eEeAAVQø^A¶EDE25±3°b X \"μr°b ¿211⁄4 25 ¿3⁄4= 5± 1pe±3μ¶3⁄4Nμ¶o 2\" \"1Teμ¶1⁄2¶μe\ni μ¶o\n1⁄2¶μ¶o ¿1T\n\n1⁄4 1T \"μ¶±3o 2±»$ \"(r)+ ;»n± B°\nP1\n?o\nOÐ\n¡\nI\no\nO\nÞXWI\no\nO\nIaiP O\n^\n\n°k1N; ¿24 \"(r)+ 2o+ ^ 9μe 5 X B1T 5 21<AD±3o(c)e yAD±3°!eμ¶o 1T \"μ¶±3o±» \"(r) 2AX1⁄4+ \" \" ¿oO eT11⁄2¶1⁄4+ ;1o 3⁄4 \"(r) ;25 5 ¿13⁄4+i._25 \"1T 5 I\n1II\nv\nO\n1I\n\nrY\n\nÞ\nW\n¡\nIUUÐ\n\nrY\n\nO\nIai3O\nX 21\n^\n·\no\na\n^U[Z\na\no\nY\nß\n* \"(r)μ¶2 \" ¿3⁄4 1⁄4 AD ¿29 5±< \" Xe31⁄4 1⁄2r1T\n\n1⁄4 1⁄2e X\noG\n± t1⁄2¶1T Be\nß\n\"(r)+ ;2\"±31⁄2¶1⁄4+ \"μe±3oi \" ¿°<1μ¶o 2\ne±31⁄4 o 3⁄4+ ¿3⁄4e1⁄4+ ,μe μr2,o+± AX1⁄2¶ ¿1T\nð\n(r) 1T \"(r)+ 41AXAX1⁄4+ B1ADikμr2\no~\n\nð\nμ¶1⁄2r1⁄2^ \" X \"1⁄4+ Bob 5±2 \"(r) μ¶2,°b X \"(r) ±M3⁄4«μ¶ob \"(r)+ 4AD±3oO 5 ^\n±»\nO\n¥a\no\nD\nM\nð\n2e3μee (1oy +1°b·1⁄2e 2±»,1<°b X \"(r)+±^3⁄4 \"(r)1T [1⁄2e±M±;M2t°b± \" !1T 5 5 B1AD \"μee \"(r) 1o»S±\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X e1⁄4+\nμ¶21⁄4 o25 \"1Te1⁄2e 1»n± !ATo A1eT11⁄2¶1⁄4+ 1±»\nß\nI\nII\nv\nO\nI\nrwv\nÞ\na\nß II\nI\nO\nIai^3O\n9(r) μr2μ¶21oi ^·1⁄2¶μ¶AXμ¶ O\nð\n±N_25 5 X·°!1⁄41⁄2e \"μ¶25 5 X·i°b X \"(r)+±^3⁄4\nð\n(r) μ¶Aa(r)«μr22\" ¿AD±3o 3⁄4i± B3⁄4+ X 1AXAX1⁄4+ B1T 5\nI\n25 X\no\næ\nO\nμe\nμ¶225±3°k X \"μ¶°b ¿24AX11⁄2r1⁄2e ¿3⁄4i \"(r)+ kA¶E\"AQ\\aE5Æ\"þ °b X \"(r)+±^3⁄4\no\nY\n· ·1⁄2ei^μ¶o+e μe 5±k \"(r) 1\n\n1⁄4 1T \"μ¶±3o\na\nOÐ\ne3μee ¿2\nINI\nv\nOÐ\na\nß\nI\nÞ\nI\nrwv\nc\nI\n\nO\n9(r) μr21μ¶211 1⁄2¶μ¶o ¿1T ;25 ¿AD±3o 3⁄4*_{± B3⁄4 X !3⁄4 μ%$ X \" ¿o AD b\n\n1⁄4 1T \"μe±3o1o 3⁄4NAX1oNe k25±31⁄2¶e ¿3⁄4N1o 11⁄2eiM \"μ¶AX11⁄2¶1⁄2ei=μ¶o.1 °<1o o+ X\n1o 11⁄2e±e±31⁄42 5±21⁄2¶μ¶o ¿1T q2\" ¿AD±3o 3⁄4*_{± B3⁄4+ X 3⁄4 μ%$ X \" ¿o(c) \"μr11⁄2O\n\n1⁄4 1T \"μe±3o2X1\næ43\no\n1⁄44 X B\nð\n\"1⁄4 25 25 \"1T 5\nð\n(r) 1T \"(r)+ 925±31⁄2r1⁄4+ \"μe±3o\nμ¶2\no\n±N°k1N; \"(r) oO1⁄4 °!e X B2\nð\n± {;±31⁄4+ b2\"μ¶°k·1⁄2ei\nð\nAB(r)+±M±325\nß\nO\niPFc+o\n4(r)+ ¿owμe»\nO\nO\nUN\nv\nO\n\ncJ\nI\nO\na.r\nI\no\n4(r) μ¶2; ^·±3o+ ¿oO \"μ¶11⁄2¶1⁄2ei.3⁄4+ ¿AX1piMμ¶o e25±31⁄2r1⁄4+ \"μe±3ohμ¶221 \" ¿125±3o 1Te 1⁄2e A1T·· \"± K +μ¶°k1T \"μ¶±3o 5± \"(r)+\n[a¥a\ne3μee ¿o! \"(r)+ AD±31T a25 \"μr°b 25 5 X·\no\n1⁄4[±\nð\nXe X K(c)μe»\nO9OoUÞIS\n\"(r)+ ¿o\nI\nO\nIUUÞ\n+]\nt\nO\na\nr\nI\nÐ\n]\nt\nIÐ\na\nO\nI\noI[\no1⁄2ei11o\n<ISaO\nX \" \"± !μ¶2;μ¶oO 5 \"±^3⁄4 1⁄4 AD ¿3⁄4.μ¶oO 5± \"(r)+ <AD±M «AXμe ¿oO \"2$e1⁄4+ 1±pe X ( \"μ¶°b b \"(r) k M·±3o+ ¿oO \"μ¶11⁄2¶1⁄2¶i e \"±\nð\nμ¶o+eyAD±3°b·±3o+ ¿oO\nð\nμ¶1⁄2r1⁄23⁄4+±3°<μ¶o 1T 5 μe»\nS\nμ¶2 o+± b ^1AD \"1⁄2¶iE\no\næ1N;^μ¶o+e\nß\n2\"°k11⁄2¶1⁄2e X \" ¿3⁄41⁄4 AD ¿2 \"(r)+ 3⁄4+ Xe \" X ±»~μ¶o 2\" \"1Teμ¶1⁄2¶μe\ni.e1⁄4+\n3⁄4+±M ¿2~o+± [ ¿1⁄2rμ¶°kμ¶o 1T 5 !μe\no\n4(r)+ (3⁄4 ¿AX1¿i^μ¶o+ei25±31⁄2¶1⁄4+ \"μe±3o 5±A \"(r)+\n[¥Z\nAX1oo+± ~e> (25 \"1Te1⁄2ei AD±3°k·1⁄4+ 5 ¿3⁄4eOiy \"(r) μ¶2\n11⁄2ee± Bμ¶ \"(r) °\noD\n± 5 ( \"(r) 1T 9»n±\nð\n1T a3⁄4\n\n1⁄41⁄2e X 4μ¶292\" \"1Te1⁄2e ;»n± 4 \"(r) μr2\n\n1⁄41T \"μe±3o\nð\nμ¶ \"(r)i \"(r) μ¶2\nßo\nY\n1⁄2e \"(r)+±31⁄4 e3(r) 1⁄2e ¿1T· »n \"±e\neMiAμe \"2\" ¿1⁄2e»μ¶241⁄4 o 25 \"1Te1⁄2¶ ^μ¶ 9μ¶291; Xiie1⁄4 μ¶1⁄2r3⁄4 μ¶o+e!e1⁄2e±^A8;A±»$1oy $> ¿AD \"μee (°b X \"(r)+±^3⁄4w÷\n1⁄4 1⁄2¶μe a2\"AB(r)*_\n5±O X £1\nU\na43\no\n9(r) 91Te>±He 4 +1°b·1⁄2e ¿2°k1¿ik25 X ¿°\n1T \" \"μ%: AXμr11⁄2\no\nY\n»S 5 X 11⁄2¶1⁄23 \"(r) 425 \"1Teμ¶1⁄2rμe i(AD±3o 3⁄4 μe \"μe±3ok»S± ,»n±\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X\n°b X \" ¿1⁄2¶i2\"1piM2! \"(r) 1T\nß\nAX1owI\n!e <°b± \" k \"(r)1oN\nð\nμ¶AD < \"(r)+ < \"μ¶°b <AD±3o 2\" \"1o(c)\no\n±y \" ¿25±31⁄2¶e ke ¿(r) 1¿e^μe± 2±3o \"(r)+\n± B3⁄4+ X t±», \"(r)+ ( \"μ¶°b (AD±3o 25 \"1oO ~ \"\n\n1⁄4μe \" ¿241A2\"°k11⁄2r1⁄2 \"μ¶°b (25 5 X·.1o(c)i\nð\n1pi\no[o\n» \"(r)+ (e> ¿(r)1¿e^μe± [»n± ~1⁄2e±3o+e< \"μr°b ¿2\nμ¶23⁄4+ ¿2Bμe \" ¿3⁄4w+μe 9μ¶2°k± \" 225 ¿o 2Bμee1⁄2e 51⁄425 5±<1⁄2e±M±;«1T 9 \"(r) 225 5 ¿13⁄4+i._25 \"1T 5 !2\"±31⁄2¶1⁄4+ \"μe±3o 2\no\nY\n\" ¿11⁄2· \"±e1⁄2e ¿°\n1T Bμr25 ¿2(r)+±\nð\nXe X Kð\n(r)+ ¿o=1o=\n\n1⁄4 1T \"μe±3o.(r) 1211T ;1⁄2e ¿12\" 1\nð\n± 3⁄4 μ¶25·1T a1T 5 ( \"μr°b bAD±3o 25 \"1oO \"2\noIoAU\naiμ¶1⁄2¶1⁄2¶1⁄425 5 B1T 5 ¿2t \"(r)+\n3⁄4 μ%AAX1⁄4 1⁄2e i\no\n4(r)+ ~ ¿μee ¿o(c)eT11⁄2¶1⁄4+ ¿24±» \"(r) 225i^25 5 ¿°\n1T \"\nÐ!U\nk1o 3⁄4\nÐ2U\n25±b \"(r)+ ;e ¿o+ X a11⁄225±31⁄2¶1⁄4+ \"μ¶±3oμ¶2\nA\nI\no\nO,O\n\nv\n\nrwv\nO5O\nÞ\na\n\nr\nI UpO\nð\nμe \"(r) \"(r)\n\n×\n3⁄4+ X 5 X a°kμ¶o+ ¿3⁄4 eMi! \"(r) 9μ¶o μe \"μ¶11⁄2MAD±3o 3⁄4 μ¶ \"μe±3o 2\nA\nI\n\nO\nA\na\nI\n\nOao\n9(r) 925±31⁄2¶1⁄4+ \"μe±3ob(r)121;2\"1⁄2e±\nð\n1⁄2ei!3⁄4 ¿AX1¿i^μ¶o+e\nAD±3°b·±3o+ ¿oO 1o 3⁄4(19 a1T·μ¶3⁄4 1⁄2ei~3⁄4+ ¿AX1piMμ¶o e4±3o+\no\nY\n»S 5 X q142\"(r) ± \" \"μ¶°k P \"(r) »e125 $AD±3°k·>±3o ¿o(c) $μ¶2o+ Xe31⁄2rμee3μee1⁄2e He1⁄4+\n1o M·1⁄2rμ¶AXμe 4°b X \"(r) ±M3⁄41⁄2¶μ<; ;»n±\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X t°!1⁄425 4 \"1N; b1« \"μ¶°b !2\" 5 X·3⁄4 μrAD \"1T 5 ¿3⁄4eMi \"(r)+ (»n12\" [AD±3°b·±3o+ ¿oO\no\nY\no μ¶°b·1⁄2rμ¶AXμe °b X \"(r)+±^3⁄4\nð\nμ¶1⁄2¶1⁄2> $ ¿AD \"μee ¿1⁄2eii \"1N; 2 \"(r) ;»n12\" 9· \"±^AD ¿2\"2 5±«\n\n1⁄4 μr1⁄2¶μee Bμr1⁄4 °\no\n9(r)1T 4AD±3°b·±3o+ ¿oO\nð\nμ¶1⁄2¶1⁄2\ne 2\"±31⁄2ee ¿3⁄4 μ¶o 1AXAX1⁄4+ B1T 5 ¿1⁄2¶ie1⁄4+ qμe $μr22\"°<11⁄2¶1⁄21o 3⁄4( \"(r)+ ±pe X B11⁄2¶1⁄2+2\"±31⁄2¶1⁄4+ \"μe±3o\nð\nμ¶1⁄2¶1⁄2(c)e e±M±M3⁄4\noI[\no AX1obμ¶o »S± B°<11⁄2¶1⁄2ei\n°b ¿12\"1⁄4 \" ~ \"(r)+ ;25 \"μ%$_o+ ¿2\"2eMi« \"(r)+ ~ B1T \"μ¶±b±» \"(r)+ ;1⁄2¶1T Be ¿25 5±b \"(r)+ ;2\"°k11⁄2¶1⁄2¶ ¿25 ¿μ¶e ¿o(c)eT11⁄2¶1⁄4+ ;μ¶oi°k1Te3oμe \"1⁄4 3⁄4+\no\n1⁄4[ X \"\nμe 9μ¶2\nU\n* e1⁄4+ 9μ¶o2\"±3°b 225i^25 5 ¿°k2+ ¿2\"·> ¿AXμr11⁄2¶1⁄2ei«Aa(r)+ ¿°kμ¶AX11⁄2 \" ¿1AD \"μe±3oy2\"iM25 5 ¿°<2 \"(r)+ 1 B1T \"μ¶±<AX1oe\nU\nx\no\n÷\n1A8;\nð\n1T B3⁄4\n\n1⁄41⁄2e X\nð\n123⁄4+ ¿°k±3o 25 5 B1T 5 ¿3⁄4k(r) X \" 5±;μr1⁄2¶1⁄2¶1⁄4 25 5 a1T 5 \"(r)+ · aμ¶o AXμe·1⁄2¶ pe1⁄4+ qμ¶o(· B1AD \"μrAD ±3o o X ¿3⁄4 2\n(r) μee3(r) X ± B3⁄4+ X 9°b X \"(r)+±^3⁄4 2\no\\G\n±\nð\n1T a3⁄4 1o 3⁄4ie1A{;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X AX1oe 1AD±3°2e μ¶o+ ¿3⁄4 μ¶oi1 · B ¿3⁄4 μ¶AD 5± n_AD± \" B ¿AD 5±\nU\n\n·1μe\no[\no+ yμ¶ 5 X B1T \"μe±3o£e3μ¶e ¿2¢1⁄4[ ¿1⁄4 owI\n2<°k X \"(r)+±M3⁄4ð\n(r)μ¶AB(r)£μr2k25 ¿AD±3o 3⁄4\n± B3⁄4+ X Ke1⁄4+ <1⁄2r1A8;^2k \"(r)+ y2\" \"1Teμ¶1⁄2¶μe\nim±»\n\"(r)+ (»n1⁄41⁄2¶1⁄2eiμ¶°b·1⁄2rμ¶AXμe 5 B1T· yX±3μ¶3⁄4 11⁄2 B1⁄4 1⁄2¶\no[\no !AD±31⁄4 1⁄2¶3⁄4μe 5 X B1T 5 5±AD±3oOe X \"e ¿o AD e1⁄4 tAD±3oOe X \"e ¿o AD kμ¶2t2\"1⁄2e±\nð\n· \" ¿AXμr25 ¿1⁄2ei\nð\n(r)+ ¿oi \"(r)+ 225i^25 5 ¿°\nμr2925 \"μ%$\no\nY\noy11⁄2e 5 X Bo 1T \"μee !μ¶2 5±<1⁄4 25\nD\n\nð\n5±3owI\n2[°b X \"(r)+±^3⁄4\no\nY\n»e1°kμ¶1⁄2ei(±»μ¶°b· 1⁄2¶μ¶AXμe °b X \"(r)+±^3⁄4 2±»_± B3⁄4+ X 1⁄4+·k 5±\n\n± æ;μ¶21⁄4 25 ¿3⁄4<μrok \"(r)+ t11⁄2ee± Bμe \"(r) °\n±»\n\n¿1T #1\n\no\n9(r)+\n: B2\" ± B3⁄4+ X 4°b X \"(r) ±M3⁄4μ¶29e1A8;\nð\n1T a3⁄4\n\n1⁄41⁄2e X K1\nII\nv\nO\nI\nÞaß II\nINI\nv\nO\nI\na\nO\n9(r)+ ;°k X \"(r)+±M3⁄4 ±»± B3⁄4 X ^μ¶2\nINI\nv\nÐVMO\nI\nÐV\nv\nI\nrwv\nÐ£cXcXc(c)ÐV_\nrwv\nI\nr\n(\n_\nrwv\n*\nO'ß[_TII\nII\nv\nO\nI(c)i3O\n9(r)M1⁄4 2>1⁄2¶μ<;\nY\n3⁄4 1°<2i_kæy±31⁄41⁄2e 5±3ow> \"(r)+ ¿25 1T \" (μr°b·1⁄2¶μ¶AXμ¶ 9°!1⁄4 1⁄2¶ \"μ¶25 5 X·°k X \"(r)+±M3⁄42e1⁄4 4 \"(r)+ Xiy1⁄425 2±31⁄2¶3⁄4eT11⁄2¶1⁄4+ ¿2\n±»\nB1T \"(r)+ X \"(r)1o\nII\nO\n1o 3⁄4i \"(r) Xi< XeT11⁄2¶1⁄4 1T 5\nI\n±3o 1⁄2eiA1T \"(r)+ ~ Bμee3(r)O ¿o3⁄4+·±3μ¶o(c) ±»$ \"(r)+ ~ \"μ¶°b ¿25 5 X·yμro(c) 5 X \"eT11⁄2\no\n9(r)+ 1AD±M «AXμe ¿oO \"2AX1oe t3⁄4 X Bμee ¿3⁄4i1⁄4 2\"μ¶o e! \"(r)+ ~ ¿1⁄2e Xe31oO 9»n± B°k11⁄2¶μr2\"°\n±»±· X B1T 5± 425 X Bμ¶ ¿2\nI\n\no\na\ni3O\n129AX1o\n\"(r)+ 22\" 5 B1o+e 2AD±M «AXμe ¿oO \"29±»$ \"(r)+\nY\n3⁄4 1°k2»e1°kμ¶1⁄2¶i<±»q°b X \"(r)+±^3⁄4 2£1\n\n· ·\no9U\n\nZ UU\no\no\no=· B1AD \"μ¶AD 5± 25±31⁄2ee k25 \"μ<$Z25i^25 5 ¿°k21μ¶ ;μ¶2~e ¿25 ~ 5± 1⁄4 25 b±3o+ b±» \"(r)+ k°k1oOi· 1A8;T1Te ¿2!1T \"±31⁄4o 3⁄4 \"(r) 1T\nμ¶°b· 1⁄2e ¿°b ¿oO \"(r)\n\n¿1T ~°b X \"(r)+±^3⁄4V1\n\no\n\"!#!(r) 1241\n\n¿1T t±· \"μe±3o\no,o\n»\nð\nBμe \"μ¶o e(i±31⁄4 4±\nð\noy3⁄4+ Bμ¶e X 9AD±M3⁄4+ i±31⁄4\nAX1oy1⁄425 1 \"(r)+\no\næ\naa`bcd\"e\n2\"1⁄4+e \"±31⁄4 \"μ¶o+ t± 4 \"(r)+ 1·1⁄4 e1⁄2¶μ¶At3⁄4+±3°k1μ¶o2B1⁄4+e \"±31⁄4+ \"μro+ f3g#h\n`c\no\n&q1i\nØENE 3¥_¬dE (c)¦i¥wOE\na E,¥¦\n[\no+\nð\n1¿i=μ¶o\nð\n(r) μ¶Aa(r)e μe±31⁄2e±eiyμ¶2~ ¿12Bμe X 1 \"(r) 1o·(r)OiM2\"μrAX2_μ¶2t \"(r) 1T 1 \"(r)+ b3⁄4+i^o 1°kμ¶AX11⁄2q25i^25 5 ¿°k211⁄4 2B1⁄4 11⁄2¶1⁄2ei (r) 1pe\n25 \"1Te1⁄2¶ 1T 5 5 B1AD 5± B2\no\nG\n± A ^1°b· 1⁄2e μ¶ kμ¶2 ¿12\"μe X « 5±AX11⁄2¶AX1⁄4 1⁄2¶1T 5 1N1⁄2¶μ¶°kμ¶ (ADi^AX1⁄2e yoM1⁄4 °b X BμrAX11⁄2¶1⁄2ei, \"(r) 1o\n\"(r)+\n± \"eμ¶ b±»t1N2Bμ¶°b·1⁄2e (r) 1T a°b±3o μ¶AA±32\"AXμ¶1⁄2r1⁄2¶1T 5± Ke> ¿AX11⁄425 X \" \"± B2<μ¶oa \"(r)+ oM1⁄4 °b X BμrAX11⁄225±31⁄2¶1⁄4+ \"μe±3oa1T B y3⁄4 1°b· ¿3⁄4\neMia \"(r)+ y 5 B1c5 ¿AD 5± \"ihI\n2i1T· · \"±31Aa(r)\n5±Z125 \"1Te 1⁄2e y1T 5 5 B1AD 5±\no\n9(r)+ X \" \"± < ¿2\" \"μ¶°k1T 5 y»n±\n\n1⁄4 1⁄2e X I\n2k°b X \"(r)+±^3⁄4\nI oU\næ\nO\n2\"1⁄4+ee ¿25 \"22 \"(r) 1T ; \"(r)+ < X \" \"± ;e \"±\nð\n2; ^·±3o+ ¿oO \"μ¶11⁄2¶1⁄2eiμ¶o. \"μ¶°b\no\n9(r) μ¶2~±^AXAX1⁄4+ B2;μro=· \"±e1⁄2e ¿°<2\nð\nμe \"(r)\no+ ¿1⁄4+ 5 a11⁄2¶1⁄2ei225 \"1Te 1⁄2e ± \"eμe \"2Ke1⁄4+ ,o+± \"(r)+±32\"\nð\nμe \"(r) 1⁄2¶μ¶°<μe $ADi^AX1⁄2e ¿2\nI\nAD»\no\\\n\noUUpOao9\nμ¶°kμr1⁄2¶1T B1⁄2eið\n(r)+ ¿obAD±3°b· 1⁄4+ \"μ¶o+e\neμe»e1⁄4+ BAX1T \"μ¶±3o<3⁄4 μ¶1Te a1°k2^ \"(r)+ ~25 5 B1⁄4 AD \"1⁄4+ a11⁄2>2\" \"1Teμ¶1⁄2¶μe\nib±»»n ¿1T \"1⁄4+ \" ¿22\"1⁄4 Aa(r)i121⁄44±· »eμe»e1⁄4+ BAX1T \"μe±3o 2°b ¿1o 2 \"(r)+ Xi\n· \" ¿2\" X \"e ¿3⁄4yμ¶o \"(r)+ ;»e1AD 2±»,2\"°k11⁄2r1⁄2·> X B \"1⁄4+ \"e1T \"μe±3o23⁄4 1⁄4+ ; 5±« \"±31⁄4 o 3⁄4 *_{±N$= X \" \"± 11⁄2e \"(r)+±31⁄4 e3(r) \"(r)+ !1⁄2¶±MAX1T \"μe±3o\nð\nμ¶1⁄2¶1⁄2\ne ;25±3°b\nð\n(r)1T 9μ¶o X \" \"±\no\n[\nAXAX12Bμe±3o 11⁄2¶1⁄2ei8±3o+ . B1⁄4o 2μ¶oO 5±\n°b±^3⁄4+ ¿1⁄2¶2\nð\nμe \"(r)'Aa(r) 1T±32ð\n(r) μ¶AB(r)'2B1⁄4*$> X y»S \"±3°&25 ¿o 2\"μe \"μ¶e N3⁄4+ X· ¿o 3⁄4+ ¿o AD\n±3o\nμ¶o μe \"μr11⁄24AD±3o 3⁄4 μ¶ \"μe±3o 2\nI\n\no\nU\na\nOaoO[\no+ =°kμee3(r)O « \"(r) μroE;Z \"(r) 1T μe\nð\n±31⁄4 1⁄2¶3⁄4e μr°b·±32\"2\"μee1⁄2¶ 5±ZAD±3°k·1⁄4+ 5\n1N°b ¿1oμ¶o+e»e1⁄4 1⁄225±31⁄2¶1⁄4+ \"μ¶±3oae ¿AX11⁄4 25 y1oOia X \" \"±\nð\n±31⁄4 1⁄2¶3⁄4£e> °k1Te3o μ%: ¿3⁄4£ M·±3o+ ¿oO \"μ¶11⁄2¶1⁄2ei\no\n1⁄44±\nð\nXe X K4 Xe ¿o\nAB(r)1T± \"μ¶A 25±31⁄2¶1⁄4+ \"μe±3o2[1T \" 1T 5 5 B1AD 5 ¿3⁄4. 5±1A25 \"1Te1⁄2e 25 X 12\"iM°b· 5± \"μ¶AX11⁄2¶1⁄2ei \"(r)+ (± \"eμ¶ \"2[1T \" ±3o 1⁄2¶i\nð\nμ¶1⁄2¶3⁄4\nð\nμe \"(r) μ¶o\n\"(r)+ ~AD±3o*: o+ ¿2±» \"(r) t1T 5 5 B1AD 5±\noo\n\"1⁄4+ ao 2,±31⁄4+ \"(r) 1T \"(r)+ ~oO1⁄4°b X Bμ¶AX11⁄2¶1⁄2¶i AD±3°k·1⁄4+ 5 ¿3⁄4« 5 B1cU ¿AD 5± Bið\n(r) μ¶1⁄2¶ 4o+±\n»e1μe \"(r)+»e1⁄4 1⁄2$ 5± \"(r)+ k 5 B1⁄4 b 5 B1cU ¿AD 5± Bi\nð\nμe \"(r)= \"(r)+ be3μee ¿omμ¶o μe \"μr11⁄2AD±3o 3⁄4μe \"μe±3o 2μr2;2\"(r) 13⁄4+±\nð\n¿3⁄4NeOi.1o+± \"(r)+ X 2 5 B1⁄4+\n5 B1c5 ¿AD 5± \"i\nð\nμe \"(r) 3⁄4 μ%$ X \" ¿oO [μ¶oμe \"μ¶11⁄2AD±3o 3⁄4μe \"μe±3o 2\no\n9(r)M1⁄4 2>1AXAD X· \"1Te1⁄2e 1o 2\nð\nX B2t1T \" (±e \"1μ¶o+ ¿3⁄4w Xe ¿o\nð\nμe \"(r)\n2\"μ¶°k·1⁄2e ~°b X \"(r)+±^3⁄4 291⁄2¶μ<; #¦[1⁄4 o+e _;1⁄4 5 \"1* · \"±HeMμ¶3⁄4 ¿3⁄4\nð\n1 \" ¿1⁄2r14 A±31⁄4+ [25 \"1o 3⁄4 1T B3⁄42»S \"±3°kj\na\n¿1¿e\nð\nμe \"(r)i \"(r) ;±3o+\n\"(r) 1T 9e \"±31⁄4+e3(r)O i±31⁄43lb 5±mj\no\n»qi±31⁄4 AX1owI\n4e\nð\nμ¶ \"(r)i \"(r)+ 1±3o+ ;i±31⁄41⁄2e±pe 1⁄2¶±pe ; \"(r)+ 2±3o+ ;i±31⁄4wI\n\"\nð\nμe \"(r)\no\nl\n[\n»>AD±31⁄4+ B25 O \"(r)+ X \" 91T \" 93⁄4 ¿1⁄2¶μ¶AX1T 5 · \"±e 1⁄2e ¿°k2$ \"(r) 1T ,1T \" 93⁄4 μ<«AX1⁄4 1⁄2e $ 5±1 \" ¿25±31⁄2ee 9oM1⁄4 °b X aμ¶AX11⁄2¶1⁄2eie1⁄4 $ \"(r) 9°b X \"\n+μ¶25 5 ¿o AD ;±»qAB(r)1T±32[3⁄4+±M ¿29o+± [o+ ¿AD ¿2\"2\"1T Bμr1⁄2ei«μ¶oOeT11⁄2¶μ¶3⁄4 1T 5 1oM1⁄4 °b X Bμ¶AX11⁄2_°k X \"(r)+±M3⁄42\no\nUU\n\nhttp://www.netlib.org\n\n{\nhh2B9§\nn±i,_a2\"9B3Eμ£¶ o2_8·[\nuq\na«¥IH¤k)@/|Lq¢¥EP-O\nO\n¥a\nI\n2(1T B i°(1⁄4 Aa(r)h°k± \" AAD±3°k·1⁄2¶μ¶AX1T 5 ¿3⁄4h \"(r) 1o\n[a¥a\nI\n2K,1o 3⁄4h ¿1AB(r)Z±»9 \"(r)+ i25 Xe X B11⁄2AX1⁄2r12\"25 ¿2( \"\n\n1⁄4 μe \" ¿2(μe \"2\n±\nð\noy25±31⁄2¶1⁄4 \"μe±3o °b X \"(r)+±^3⁄4 2\nofG\n± \" \"1⁄4o 1T 5 ¿1⁄2ei \"(r)+ ;\nð\n±i°k1μ¶o\n\n1⁄4 1T \"μ¶±3o 29 \"(r) 1T tAD±3°b !1⁄4 ·μ¶o o ¿1⁄4+ \"±32\"AXμe ¿oAD 21T \"\n\" ¿1⁄2¶1T \"μ¶e ¿1⁄2ei« ¿12\"iA 5±«(r) 1o 3⁄4 1⁄2e\no\n9(r) XiA1T \" 2 \"(r)+ ;1⁄2¶μ¶o+ ¿1T 9AX1Te 1⁄2e 1\n\n1⁄41T \"μe±3ow}v~3\n~\no\nO\nU\na\n~\na\n\n~\nI\na\nÐ\n\nIOO\nð\n(r)+ X B\n}\nOoUF^I\nT\n\nT\nO\nUa4O\nT\nF\n×\nH1o 3⁄41 \"(r) o+±3o 1⁄2¶μro+ ¿1T w1⁄44±^3⁄4+e;^μ¶o*_k1⁄4t1⁄4* ^1⁄2¶ Xi4\n\n1⁄4 1T \"μe±3o 21o3⁄4;μe \"2_eT1T Bμ¶1oO \"2}\n~\n~\no\nO\nU\na\n~\na\n\n~\nI\na\nÐm\n×\nI\nI3⁄4¤n¿IBß>O\nI?3O\n}\nI\n\nO\n~)\n~\no\nO\n\nO\nI\n\nOqÐ\n\nð\n(r)+ X B\n\nO\n3⁄4¤n¿IBß\n1o 3⁄4\n\n×\nI\nμ¶2k2BAX11⁄2e ¿3⁄4aeMih1.\niO· μ¶AX11⁄29AD±3o 3⁄4 1⁄4AD \"1o AD »n± «AD±3°b·1T aμ¶25±3oZ 5±N \"(r)+ AX1Te1⁄2e\n\n1⁄4 1T \"μe±3o\noD\n± 5 \"(r) 1T k \"(r)+ e31T \"μro+eNeT1T Bμ¶1Te1⁄2e ¿2\n\n(r) 1pe o+±N3⁄4μe \" ¿AD k25·1T \"μr11⁄293⁄4+ X· ¿o 3⁄4+ ¿o AD e1⁄4+ beT1T \"iZμ¶o\n25·1AD (e ¿AX11⁄4 25\n\neP1T Bμ¶ ¿2\no\n9(r)+ (AX1⁄2e±325 ( \" ¿1⁄2¶1T \"μe±3o 2B(r) μe·±», \"(r)+ ¿2\" !\n\n1⁄4 1T \"μe±3o 2[ \" ¿25· ¿AD \"μee ¿1⁄2¶iA 5±A \"(r)+ 3⁄4 μ%$_1⁄4 2\"μe±3o\n1o 3⁄4 B ¿1AD \"μe±3o*_3⁄4 μ%$_1⁄4 2\"μ¶±3oi\n\n1⁄4 1T \"μ¶±3o 29±»AB(r) ¿°kμ¶25 5 \"i1o 3⁄4 ·(r)(c)i^2\"μ¶AX24°b ¿1o 29 \"(r)1T 9 \"(r)+ X \" 2μr291k BμrAB(r) 1⁄2e Xe31ADi±»\n5 ¿AB(r)o μ\n\n1⁄4+ ¿2 5±<3⁄4+ a1\nð\n±3o\no\næ±325 4±»$ \"(r)+ 213⁄43⁄4 μe \"μe±3o 11⁄2_3⁄4μ%«AX1⁄4 1⁄2e \"μ¶ ¿2μroie±3μ¶o+ek»n \"±3°\n[a¥a\n2 5±\nO\n¥Z\n2911⁄2e \" ¿13⁄4 iA1T Bμr25 ;μ¶o \"(r)+ !1⁄2¶μ¶o+ ¿1T\nAX125 >25±\nð\n\nð\nμ¶1⁄2¶1⁄2>»n±MAX1⁄4 29±3o \"(r) 1T : a25\no\nuq'&\n(E) ¥wE\\Uc(E¥hE)8a«¥IH¤!)E/MLq¢,¥hEP-{OXþRi¢OE,¥ U<¥_¤¢\\)&< (c) H¤k)¡\n¦\nG\n± ~2\"μ¶°b· 1⁄2¶μ¶AXμe\ni\nð\n(AD±3o 2Bμ¶3⁄4+ X : a25 9 \"(r)+ (25 5 ¿13⁄4+i._25 \"1T 5 bAX1Te1⁄2e 2\n\n1⁄4 1T \"μe±3oy»S±\nð\n(r) μrAB(r) \"μ¶°k 23⁄4+ X BμeeT1T \"μee ¿2t1T \"\nk1o 3⁄4 \"(r)+ 1\n\n1⁄41T \"μe±3o \" ¿3⁄4 1⁄4AD ¿2 5±<1o\n[¥Z\ne±31⁄4 o 3⁄4 1T \"i<eP11⁄2¶1⁄4 1· \"±e1⁄2e ¿°\nI÷\nc\nO\nO\nU\na\n~\na\n\n~\nI\na\nÐ\n\nO\n\nI\næ\nO\n±3o<1!AX1Te1⁄2e [ B1⁄4 o o μro+e[»n \"±3°\nIO\n; 5±\nIO9o\n±!AD±3°b·1⁄2¶ X 5 ¿1⁄2eib25· ¿AXμe»Si( \"(r)+ [· \"±e1⁄2e ¿°\nð\nto+ X ¿3⁄4ke±31⁄4 o 3⁄41T \"i\nAD±3o 3⁄4 μ¶ \"μe±3o 2\no\n9(r)+ ;2Bμ¶°b·1⁄2e ¿2\" AX12\" 2μ¶2 5±«AX1⁄2r1°b·\n\n1T 4 \"(r)+ ; ¿o 3⁄4+·±3μ¶oO \"2XI\n\nInIO\n\nO,O\n1⁄2ON\n\nInI OOO\n1⁄2c\nI. O\n[\no+ 1AD±31⁄4 1⁄2r3⁄425±31⁄2ee ; \"(r) μr2eMiA \"(r)+ EBuMÆpÆ=eESoMþ!°k X \"(r)+±M3⁄4I \"1⁄4+ Boμe μ¶oO 5±k1b25i^25 5 ¿° »n±\n\n1o 3⁄4\n\n1o 3⁄425±31⁄2¶e ; \"(r)+\nμ¶o μ¶ \"μ¶11⁄2eP11⁄2r1⁄4+ 2· B±e1⁄2e ¿°\nð\nμ¶ \"(r)y1H;^o+±\nð\noμro μe \"μ¶11⁄2_AD±3o 3⁄4μe \"μe±3o »n±\n\n1o3⁄4y1o1⁄4 oE;^o+±\nð\no\n\no\n1⁄4+ ¿2\"2t1keT11⁄2¶1⁄4+\n»n±\n\n1o 3⁄4<μro(c) 5 Xe B1T 5 [ 5±\nIO9o\n9(r)+ 4e±311⁄2 μ¶2, 5±#: o 3⁄4\nð\n(r) 1T ,eT11⁄2¶1⁄4+ 9±»\n\n1T !°k1N; ¿2\n\nO\n\n1T\nIiO9o\n9(r) μr2μ¶2(r)+±\nð\nB!#!3⁄4 ±O ¿29μ¶\nI\n\no\naN\nOao\n~\n; \"1N; !1b3⁄4 μ%$ X \" ¿o(c) 1T· · \"±31AB(r)\nð\n(r)μ¶AB(r)ie ¿o+ X a11⁄2¶μ<yX ¿2 5±b \"(r)+ 1 \"μ¶°k _3⁄4+ X· ¿o 3⁄4+ ¿oO AX125\no~\n;3⁄4 μee^μ¶3⁄4+ t \"(r)+\nμ¶oO 5 X \"eT11⁄21\n\n+\n31T <\nÞ\nab·±3μ¶oO \"2\nI~O\n^\n«O\n\nXcXcXcc\n\nÞ'UN\n^\nOoF^I\n\nÞUpO;I~\n( \" ¿25 X \"e\nß\n»S± [ \"μ¶°b\n25 5 X·y1o3⁄4 1⁄4 25 z^i»n± 425· 1AD\nOaoD\n± 5 ; \"(r) 1T\nI\nO\nO\nk1o 3⁄4\nI\nI\nv\nO9oI~\n(3⁄4+ : o+\n\nO\n\nInI\n\nOao\nU¿i\n\no\no 25 5 ¿13⁄4±»qAD±3oOe X \" \"μ¶o+ek 5±k1I: B25 i_{± B3⁄4+ X 92\"iM25 5 ¿°\nð\n23⁄4μe \" ¿AD \"1⁄2ei<3⁄4μ¶2\"AD \" X \"μ<yX ~ \"(r)+ ;2\" ¿AD±3o 3⁄4 3⁄4+ X Bμ¶eP1T \"μee 1±»\n\no\n[2\"μro+eb \"(r)+ ;$1¿i^1⁄2e± [25 X aμe ¿2»S±\n\n1T\nI\n5±k \"(r) 21⁄2e X»n 91o 3⁄4 \"(r)+ 1 Bμ¶e3(r)(c)\nð\n2(r) 1pe\n\nI\nv\nO\n\nÞ\n\nInIHO\n^\nÞ\n\nInI#PO\n^\na\na\nÞ\n\nInIpO\n^\na\næ\nÞ\n\nInI#pO\n^\n\na\n\nÞ8cXcXc\nI?3O\n\nrwv\nO\n\nÐ\n\nInIHO\n^\nÞ\n\nInI#PO\n^\na\na\nÐ\n\nInIpO\n^\na\næ\nÞ\n\nInI#pO\n^\n\na\n\nÞ8cXcXc\nY\n3⁄4 3⁄4 μ¶o+eb1o 3⁄42\"±31⁄2eeMμro+eb»S±\n\nInI\n\nO\nð\n;e X 41k25 ¿AD±3o3⁄4*_{± B3⁄4+ X 41AXAX1⁄4 B1T 5 !1T· · \"±K +μ¶°k1T \"μe±3oI\n\nInIHO,O\n\nrwv\nÐ\na\n\nÞ\n\nI\nv\n^\na\nÞMkI\n^\na\nO\nI?^3O\n9(r)M1⁄4 2ð\n; \" X· 1⁄2¶1AD\noI\næ\nð\nμ¶ \"(r)1b1⁄2¶μ¶o+ ¿1T 925i^25 5 ¿°±»$11⁄2ee Xe B1μ¶A~\n\n1⁄4 1T \"μe±3o 2XI\nUa\n^\na\n\nrwv\nÐ\na\n\nÞ\n\nI\nv\nÐ\n\n[O\n\nI\n\nO\n»n±\n(O\nUNXcXcXc\nO\no\no\n»\nð\n; \"(r) μroE;<±»$ ¿1AB(r)e Bμ¶3⁄4μ¶oO 5 X \"eT11⁄21291kAD±3°k·1T \" \"°b ¿oO \"(r)+ ¿o\no\nkAX1o e 1 \"\nð\naμe 5 5 ¿o\nU\n^#9\n×\nI\n\n9Ð\n\nI\nv\nO$Ð\nI\n\nrwv\nÐ\n\npO\nÞ\n^\n\nT\nO\n\nc\n1⁄4 2\"μro+e\nU\na\nO\nT\nF\n×\no\n4(r) μ¶2$(r) 12 \"(r)+ 2\"1T \"μ¶2\"»Si^μ¶o+et·(r)OiM2\"μrAX11⁄2Mμro(c) 5 X \"· \" X \"1T \"μe±3o \"(r) 1T M1T ,25 5 ¿13⁄4+i._25 \"1T 5 O \"(r) 92\"1⁄4 °\n±»,AX1⁄4+ \" B ¿o(c) \"2[μ¶o(c) 5±« \"(r)+ !AD±3°k·1T \" \"°b ¿oO [»n \"±3° \"(r)+ !o+ ¿μ¶e3(r)(c)e± Bμ¶o e AD±3°k·1T \" \"°b ¿oO \"2~1o 3⁄4 \"(r)+ (AX1⁄4+ \" \" ¿oO 41AD \"±32B2\n\"(r)+ 2°k ¿°2e a1o+ ;μ¶2\no\nD\n± 5 y11⁄2¶25±. \"(r)1T beOih3⁄4 μ¶2\"AD \" X \"μ(c)y¿μ¶o+e\nð\n(r) 1pe yμ¶oZ1.25 ¿o 2\" iAD±3oOe X \" 5 ¿3⁄4\n\"(r)+ AD±3o(c) \"μroO1⁄4+±31⁄42\nO\n¥a\nμ¶oO 5±N1\nAD±3°b·1T B \"°b ¿o(c) \"11⁄2O \" X· B ¿25 ¿o(c) \"1T \"μ¶±3o;±»^ \"(r)+ 14 ^±3o\noI\n±3o(c)e X a25 ¿1⁄2eiT \"(r)+ 25±31⁄2r1⁄4+ \"μe±3o;°b X \"(r) ±M3⁄4 2(r)+ X \" 1T· · 1⁄2ei[\n\n1⁄4 11⁄2¶1⁄2¶i\n5±<AD±3°b· 1T \" \"°b ¿oO \"11⁄2°b±^3⁄4+ ¿1⁄2¶2\no\n~\nAX1o\nð\nBμe 5\no\n=°b± \" AAD±3°b·1AD \"1⁄2eihμ¶o°k1T 5 aμ% N»S± a°\no\na\nX\n1⁄2)O\nI\n\nv\nXcXcXcK\n\nO\n\nð\ni3⁄4+±=o+±\nμ¶o AX1⁄2r1⁄4 3⁄4+\n\nO\n±\n\nI\nv\ne ¿AX11⁄4 25 \"(r)+ Xi 1T \" ;Mo ±\nð\nob1o 3⁄4 e X ·1⁄4+ μ¶oO 5±t \"(r) Bμee3(r)O q(r)1o 3⁄4b2\"μ¶3⁄4+ ±»> \"(r)+\n\n1⁄41T \"μe±3o\no\n9(r)+ ¿o\n\n1⁄2Oc\nI^UpO\n.O\nIÐ\nY\n\n_\n\nON\n\nXcXcXc\n\nXÐ\nY\n\n_\n\n>O\nwμ¶241=eEaE|TEaA¿þ(c)ÆTo>ATA°k1T 5 Bμ< >I\n\nO\nU\na\n^\na)\nÐm\nI\na\nO\nð\n(r)+ X B\n\nμ¶2 \"(r) ;μ¶3⁄4+ ¿oO \"μe ii°k1T 5 Bμ% 1o 3⁄4\n\nμ¶2\n\nÐ\na\nU\n\nU\n\nU\nÐ\na\nU\nU\n\no\no\no\no\no\no\no\no\no\n\no\no\no\no\no\no\no\no\no\n\nU\nÐ\na\nU\n\nU\nU\nÐ\na\n\nIi3O\nU\n\n9(r) μr2b°k1T 5 Bμ% a\n\n1⁄4 1T \"μe±3o\nAX1o\ne ¿12\"μ¶1⁄2eia25±31⁄2ee ¿3⁄4\neOi\n\n11⁄42\"2\"μ¶1oa ¿1⁄2rμ¶°kμ¶o 1T \"μ¶±3ohμro\nkI\n\nO\n\"μr°b\no\nX \"(r)+\nY\n· · ¿o 3⁄4 μ% <»n± 9»n± B°(1⁄4 1⁄2¶121o 3⁄4 11⁄2r25±§1\næ*U\nN3\no\no\n$μr2°b± \" AD±3°k°b±3o! 5±125· ¿AXμe»nit \"(r) e±31⁄4 o 3⁄4 1T Bi~AD±3o3⁄4 μe \"μe±3o 2μ¶o2 5 X a°k2±» AX1⁄4 \" \" ¿oO $μ¶o25 5 ¿13⁄4;±» e±31⁄2¶ \"1Te\no\nG\n± +1°b·1⁄2e $±3o+ «°<μee3(r)(c) (μ¶o45 ¿AD 21yAX1⁄4 \" \" ¿oO\n\n1T\nI£O\n1o 3⁄4(r) 1pe Ao ±AX1⁄4+ \" B ¿o(c) Y ±\nð\n1AD \"±32\"2\nIZO\nI\n25 ¿11⁄2e ¿3⁄4y ¿o 3⁄4\nOao\n9(r) 1\n\n1⁄4 1T \"μ¶±3o»S± [14 ^μ¶11⁄2AX1⁄4+ \" \" ¿oO fY±\nð\nI![\n(r) °I\na\nð\nO\ne3μee ¿2\n~\n~\nI\nInIiO\n\nOOÐ\n×\n\nI4OO\n~\n~\nI\nInIiOO,O\n\nc\nI3O\n~\n1e X 1 25 ¿AD±3o 3⁄4*_{± B3⁄4 X 91AXAX1⁄4+ B1T 5 ;3⁄4 μ¶2BAD \" X \"μ<y¿1T \"μe±3o«eOiA2\"1⁄4+e 5 a1AD \"μ¶o+e 1o 3⁄4i25±31⁄2ee^μ¶o+e2»n±\n\nInIHO\nμ¶o\noI?\nI\n\nInI#HOO\n\nI\nv\nÐ\n\nrwv\naB^\nÞ<I\n^\na\nOac\nI\næ\nO\n9(r)+ 25 ¿11⁄2e ¿3⁄4 ¿o 3⁄4\nI=O\nAX1o= \"(r) ¿oye 2\"1T \"μ¶2i: ¿3⁄4eMiy25 X 5 \"μ¶o+e\n\nI\na\nO\n\no\n9(r)μ¶241T· · ¿o 3⁄4241o\n\n1⁄41T \"μe±3o\n»n±\n\nI\nv\n5±\no\n^I\nU a\n^\na\na\n\nÐ\na\n\nI\nv\nÐ\n\nI\nv\nO\n\nIP O\nμ¶°<μ¶1⁄2¶1T B1⁄2eið\n!1T· · ¿o 3⁄41o\n\n1⁄4 1T \"μe±3o»n±\n\nO\nI\nU\na\n^\na\na\n\nv\nÐ\na\n\nOÞ\naB^#9\n×\n\nÐ\n\nO4O\n\nc\nI3O\nuq'u\n¡6-¡)u¢¤)2)uOE)uOEa«¥IH¤!)E/|Lq¢¥EP-O0þM£4O-EP-¥¤i¢OE,¥ U <¥_¤¢)m&< (c) H¤k)¡\n[\no+ 1T· · B±31AB(r) 5±1 \"(r)+ \"μ¶°b 3⁄4+ X· ¿o 3⁄4+ ¿oO qAX125\nI\n25 \"μ¶1⁄2¶1⁄2M1⁄2¶μ¶o ¿1T\nO\nμr2 5±1e^μe\nð\n\"(r)+\nO\n¥a\n121~25i^25 5 ¿°o±»\n[¥Z\nI\n»n± 9 \"(r)+ ;e ¿AD 5±\n1⁄2\n±»\n\n×\nI\n2XI\n}\n\n1⁄2\nPo\nO\n\n1⁄2c\nI^3O\n\n1⁄4 μeeT11⁄2e ¿oO \"1⁄2ei(c)±3o ~AX1oA \"(r) μroE;b±» \"(r) μ¶2121 AD±3°b·1T \" \"°b ¿oO \"11⁄2_°b±^3⁄4+ ¿1⁄2\no[\no+ tAD±31⁄4 1⁄2¶3⁄4A25±31⁄2ee ~ \"(r) μ¶225i^25 5 ¿°\neOi\n»n±\nð\n1T a3⁄4\n\n1⁄4 1⁄2¶ X K1⁄2\nINI\nv\nO\n1⁄2\nI\nÞ\nß\n}\n\n1⁄2\nI\n\nI\næN\nO\n1T !1yAD±325 2±»±3o+\nkI\n\nO\n°k1T 5 Bμ< ._{e ¿AD 5± b°(1⁄4 1⁄2e \"μe·1⁄2rμ¶AX1T \"μe±3o· X ; \"μ¶°b _25 5 X·\noI~\nio+±\nð\n1⁄4 25 <2\"1⁄4 ·> X a2\"AD Bμe· \"2\n5±i3⁄4 μ¶25 \"μ¶o e31⁄4 μ¶2\"(r)i \"μ¶°k 225 5 X·2t»n \"±3°25· 1T \"μ¶11⁄2e aμ¶3⁄4y2\"μ(c)yX\nouO [\no+ (AX1oe31⁄4 ¿2\"2\nI\nAD± B \" ¿AD \"1⁄2ei\nO\n\"(r) 1T t \"(r)+ 1AXAX1⁄4+ B1ADi\n±»~ \"(r) μ¶2«°b X \"(r)+±^3⁄4£μ¶2\n<Iaß OÞe<I\n^\na\nOao\n9(r)+\n\n1⁄4 1T \"μe±3o 2«1T < \"(r)+ e>±31⁄4o 3⁄4 1T Bμe ¿2 1T \" °b±^3⁄4 μ%: ¿3⁄4a12«»S± « \"(r)+\n25 5 ¿13⁄4+i._25 \"1T 5 (·1⁄4+ \" ~e±31⁄4 o 3⁄41T \"ikeT11⁄2¶1⁄4+ 1· \"±e 1⁄2e ¿°\no\nY\nð\nb2\"1\nð\nμ¶o \"(r)+ b25 ¿AD \"μe±3o=±3o\n[a¥a\nI\n1⁄4 1⁄2e X ~μ¶2~25 \"1Te1⁄2e (»n± 1\n\n1⁄4 1T \"μ¶±3o 2\nð\nμe \"(r)=3⁄4+ ¿AX1¿i^μ¶o+e25±31⁄2r1⁄4+ \"μe±3o 2\nI\n1⁄2¶μ<; b \"(r)μ¶2~±3o+\nO\n±3o1⁄2eiμe»\nßan\nÐ\na\nF\n°kμro\nT\nI!U\nT\nOao\n9(r) b ¿μee ¿o(c)eT11⁄2¶1⁄4+ ¿2;±»\n\nAX1oNe> kAD±3°k·1⁄4+ 5 ¿3⁄4. ^·1⁄2¶μ¶AXμ¶ \"1⁄2ei\nI\n\no4UK^3O\n5±«e\nU\nT\nO\na|¤(c)AD±32¤\n3⁄4\nC\n\nÞ\nU¥\nÐU\n¥\nOÐ0\n2Bμ¶o\na\n3⁄4\nC\na\nI\n\nÞ\nUpO\nc\nI\næ\nUpO\nUK\n\n9(r)+ ¿oi \"(r) ; ¿μee ¿oOeP11⁄2¶1⁄4 ¿2±»\\\nF\n}\n1T \"\n¦\nT\nO\nÐX.U a\n^\na\n}\n2\"μro\na\n3⁄4\nC\na\nI\n\nÞ'UpO\nÐ\nU\n}\nc\nI\næa\nO\n9(r)+ Z: a25 5 X a° μ¶23⁄4 1⁄4+ ~ 5±b \"(r)+ 13⁄4 μ%$_1⁄4 2\"μe±3o(c) \"(r)+ 125 ¿AD±3o 3⁄4i 5±b \"(r) Z;^μ¶o+ X \"μrAX2\noo\n» \"(r)+ ;Mμro+ X \"μ¶AX21T \" 1°k13⁄4+ 12\"1⁄2e±\nð\neMiy3⁄4+ ¿AD \" ¿12\"μro+e §u}\nμ¶o AD B ¿125 ¿2\nð\n(r) μ¶1⁄2e\nUaF\n}\n\" ¿°k1μ¶o 2[AD±3o 25 \"1oO\no\n9(r)+ ¿ow>±3o 1⁄2ei \"(r) £: a25 4 5 X B°\n°k1T 5 5 X a21o 3⁄4= \"(r)+ b»e125 5 ¿25 !AD±3°b·±3o+ ¿o(c) ;μr2\n¦\n\n»S±\nð\n(r)μ¶AB(r)= \"(r)+ k2Bμ¶o»n1AD 5±\np\nUTo\n9(r)M1⁄4 2_ \"(r) k25 \"1Teμ¶1⁄2¶μ¶ iyAD±3o 3⁄4μe \"μe±3o\n»n± 9»S±\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X »n± 9 \"(r) μr2AX125 !μ¶2\nß n\nÐ\na\nÐX.U\na\nF\n^\na\n}\nO\n^\na\n}\na\nU\na\nc\nI\næ\ni3O\n9(r) μr2μ¶21\na(c)3«¬P\noi (r)\nI 5±(1Aa(r) μe Xe ~e \" ¿1T 5 X 91AXAX1⁄4+ B1ADi\nð\n~°(1⁄4 25 °k1N; <^k1o 3⁄4\nß\n2\"°k11⁄2¶1⁄2¶ X\no\n± °k1μ¶oO \"1μ¶o\n25 \"1Teμr1⁄2¶μe iAX1⁄4+ 5 \"μ¶o e°^μ¶o(r) 11⁄2e», \"\n\n1⁄4 μe B ¿24AX1⁄4+ 5 \"μ¶o e\nß\neMi\n+oG\n± ;AD±3°b· 1T \" \"°b ¿oO ;°b±M3⁄4 ¿1⁄2¶2 \"(r) 2B1°b ((r) ±31⁄2¶3⁄4 2\nð\n;μ¶o AD B ¿125 ; \"(r)+ ;oM1⁄4 °!e> X ±»AD±3°b· 1T \" \"°b ¿oO \"2\no\nY\no+± \"(r)+ X\nð\n1¿ii 5±be^μe\nð\n\"(r) μ¶22\"μe \"1⁄41T \"μe±3oiμ¶2 5±b ¿25 \"μ¶°k1T 5 1 \"(r) ~2\" \"μ%$o ¿2\"2±»A\neMi«1⁄2e±M±;Mμro+e(1T \"(r) 1 B1T \"μe±\n±»°kμ¶o μr°!1⁄4 °\n1o 3⁄4 °<14 ^μ¶°(1⁄4 °\n¿μee ¿oOeT11⁄2¶1⁄4+ ¿2\nI\næ*U\nN3{I\n¦\n\n¦\nv\nO\nr\n\nY\n\n_\n2±\n2Bμ¶o\na\nP3\na\n(\n\nI\nv\n*\nÐUv\n±\nr\n\nY\n\n_\n2±\n2Bμ¶o\na\nv\na\n(\n\nI\nv\n*\nÐUv\n±\nI\næ\nOO\nG\n± k1⁄2¶1T Be\n}\n1o 3⁄4Z1⁄2¶1T \"e\n\"(r) μ¶2! B1T \"μe±=μ¶2\nkI\n\na\nO\nμ\no\n\no\n, \"(r)+ 25 \"μ<$o+ ¿2B2!μ¶oAD \" ¿125 ¿2 12 \"(r)+ i2\n\n1⁄41T \" «±»[ \"(r)+\noM1⁄4 °2e X ~±»e aμ¶3⁄4·>±3μro(c) \"2t± 1 \"(r)+ koM1⁄4 °!e> X ~±»AD±3°b·1T \" \"°b ¿oO \"2\no\n9(r)+\nO\n¥a\nð\n(r) μ¶AB(r)=AD± \" B ¿25·±3o 3⁄4 2t 5± \"(r)+\n1⁄2¶μ¶°<μe T\nkS\n>AX1o e 1 \"(r)+±31⁄4+e3(r)O 4±»124μro*: o μe 5 ¿1⁄2¶i<25 \"μ%$\no÷\n¿AX11⁄4 25\nμ¶2925i^°k°b X 5 aμ¶A~ \"(r)+ 2 a1T \"μe±\n¦\n\nF\n¦\nv\nμ¶2\n\n1⁄4 μeeT11⁄2e ¿o(c) 5±< \"(r)+ ;AD±3o 3⁄4 μe \"μ¶±3ooO1⁄4 °!e X 9±»I\nIo[U\n* 25 X 211⁄2r25±§1\næ43\nOao\n÷\ni13⁄4+±3μ¶o+e193⁄4μ¶2\"AD \" X 5\nG\n±31⁄4 Bμe X 5 B1o25»S± a°'±»^ \"(r)+ ,μe 5 X B1T \"μe±3o;\n\n1⁄4 1T \"μ¶±3o 2\nI\næN\nO\n±3o AX1o225 X \"(r) 1T °k1N;^μ¶o+e\n^=2\"°k11⁄2r1⁄2e X 1μ¶o(c) 5 B±M3⁄4 1⁄4AD ¿2t°k± \"\nG\n±31⁄4+ Bμ¶ X ;AD±3°b·±3o+ ¿o(c) \"2\nð\nμe \"(r)=2\"(r)+± \" 5 X\nð\n1¿e ¿1⁄2¶ ¿o+e \"(r)μ¶oO 5± \"(r)+ k25±31⁄2r1⁄4+ \"μe±3oa1\næc3\no\nY\n1⁄2e \"(r)+±31⁄4+e3(r) \"(r)+ ¿25 bAD±3oO 5 Bμee1⁄4+ 5 1⁄2¶μe 5 \"1⁄2¶ 2 5±i \"(r)+ b25±31⁄2¶1⁄4+ \"μ¶±3ow \"(r)+ Xi M· 1⁄2e±M3⁄4 !μe» \"(r)+ b25 \"1Te μ¶1⁄2¶μe\ni AD±3o 3⁄4 μe \"μe±3oμ¶2~o+±\n2\"1T \"μ¶2n: ¿3⁄4\no\n[\noy \"(r)+ 2± \"(r) X t(r) 1o3⁄4wEno7XE\"E\"APEJESoMþ §i°k1N; ¿2[ \"(r)+ (AX1Te1⁄2e 2\n\n1⁄4 1T \"μe±3oAeEDEBE925 \"μ%$\no\n9(r)+ ¿o\n}\n2\"(r)+ aμ¶oE;^2\nð\n(r) μ¶1⁄2e\nUaF\n}\n25 \"1¿i^2: ^ ¿3⁄4k1o 3⁄4k \"(r) [25 \"μ%$_o+ ¿2\"2, B1T \"μe±\n7UTo\\[\n»AD±31⁄4 B25 M \"(r)+ 0;^μ¶o+ X \"μ¶AX2,2\"·> X ¿3⁄4«1⁄4+·(c)1o3⁄4\nð\n[25 \"μ¶1⁄2r1⁄2+o+ X ¿3⁄4\n1i2\"°k11⁄2¶1⁄2$ \"μ¶°k (25 5 X·. 5± B ¿25±31⁄2ee \"(r)+ »n12\" Z;^μ¶o+ X \"μrAX2e1⁄4+ ~1T ;1⁄2e ¿125 ~ \"(r)+ bAB(r)+±3μrAD b±»\nß\nμ¶2~o+± ~e±31⁄4 o 3⁄4μ¶o1o\n13⁄4+e X B2\"\nð\n1pii 5±k \"(r)+ 2Aa(r)+±3μ¶AD ;±»v^\no\nuq%$\n£H¡μ2¤k-¬E-!EA&A¢¤/\nÆU)EEPEE¦\no\n» \"(r)+ · \"±e 1⁄2e ¿°\n1T ~(r)1o 3⁄4=μ¶2125 \"μ%$±3o+ k°!1⁄425 ~ \" ¿25± \" ~ 5± μ¶°b· 1⁄2¶μ¶AXμe 4°k X \"(r)+±M3⁄42\no,G\n± 1 +1°b·1⁄2¶ _e1A8;\nð\n1T a3⁄4\n\n1⁄41⁄2e X »S± 4 \"(r)+\nO\n¥a\nμ¶2\n1⁄2\nINI\nv\nO\n1⁄2\nI\nÞ\nß\n}\n\n1⁄2\nINI\nv\nc\nI\næ\n3O\n9(r) μr2μ¶21⁄2¶μ¶o+ ¿1T 91o3⁄4 \" ¿13⁄4 μ¶1⁄2¶i<25±31⁄2ee ¿3⁄4_I\nI1Ð\nß\n}\n\nOn1⁄2\nII\nv\nO\n1⁄2\nI\nc\nI\nææ\nO\nU\næ\n\n9(r) μr21°b X \"(r)+±^3⁄4Nμ¶211⁄4 o AD±3o3⁄4 μe \"μe±3o 11⁄2r1⁄2ei 25 \"1Te1⁄2e $1⁄2¶μ<; b \"(r)+ kAD± B \" ¿25·±3o 3⁄4 μ¶o e\n[¥Z\n°b X \"(r)+±^3⁄4\no\nY\n; ¿1Aa(r)m2\" 5 X·m1\n5 Bμ¶3⁄4μ¶1Te±3o 11⁄2,25i^25 5 ¿°\n2\"μ¶°kμr1⁄2¶1T 1 5±\nI^UpO\n°(1⁄4 25 2e <2\"±31⁄2ee ¿3⁄4\no\n9(r)+ <2B1°b AAD±3o2\"μ¶3⁄4+ X B1T \"μ¶±3o 2;1Te±31⁄4+ !(r)1o 3⁄4 1⁄2¶μ¶o e\ne±31⁄4 o 3⁄4 1T Bik\n\n1⁄41T \"μe±3o 21T· ·1⁄2¶i\no\n9(r)+ 125±31⁄2¶1⁄4+ \"μ¶±3oA1T ¿1Aa(r)y25 5 X·AX1oe t±e \"1μ¶o+ ¿3⁄4μ¶o\nkI\n\nO\n±· X B1T \"μe±3o2+ \"(r)+\n2\"1°b «122»n±\nð\n1T B3⁄4\n\n1⁄41⁄2e X\no\n4(r)+ k1AXAX1⁄4+ B1ADiμr2;11⁄2¶25± \"(r) <2\"1°b : B25 2± B3⁄4+ X 2μ¶oN \"μ¶°b <1o 3⁄425 ¿AD±3o 3⁄4N± a3⁄4+ X\nμ¶o25·1AD\no1o\noy $ ¿AD 1 \"(r) μ¶2t°b ¿1o 2t \"(r) 1T ~ Xe ¿o= \"(r)+±31⁄4 e3(r)2\" \"1Teμ¶1⁄2¶μe\ni μ¶2to+± 1AD±3°b· \"±3°kμr25 ¿3⁄4wß\n°!1⁄4 2\" t25 \"μr1⁄2¶1⁄2e\n· \"±·± \" \"μ¶±3o 11⁄2> 5±¶^\na\n»S± 4 «AXμe ¿o ADi>Iμ¶oO \"1⁄4 μe \"μee ¿1⁄2¶i^μ¶ 9°k1N; ¿2[o ±k25 ¿o 25 1 5±<μro(c)e ¿25 [1o ¿o+± B°b±31⁄42 $>± B 4μ¶o\n\" ¿3⁄4 1⁄4AXμ¶o+e \"(r)+ ; X \" B± 93⁄4 1⁄4+ 1 5±¶^\nð\n(r) μ¶1⁄2e t \"(r)+ ; X \" B± 43⁄4 1⁄4+ ~ 5±\nß\nμ¶2925 \"μr1⁄2¶1⁄21⁄2¶1T \"e\nI!9\nX 211⁄2r25±\n\no\naa\nOao\nY\ne> X 5 5 X !°b X \"(r)+±^3⁄4 \"(r)1T ;μ¶2125 ¿AD±3o 3⁄4=± B3⁄4 X ;1AXAX1⁄4+ B1T 5 kμ¶oe± \"(r).25·1AD k1o 3⁄4 \"μ¶°k μr2[ \"(r) b 5 B1T· yX±3μ¶3⁄4 11⁄2\nB1⁄4 1⁄2¶\nI\n11⁄2r25±H;^o+±\nð\noy12R·$EUAoNc>Kt1E7BÆAEXÆTo\nO\n1⁄2\nII\nv\nO\n1⁄2\nI\nÞ\nß\na\n}\nI\n\n1⁄2\nI\nÞ\n\n1⁄2\nINI\nv\nO\nI\næ\nO\nð\n(r) μrAB(r)AX1oy1Te31μroe ;μ¶°b·1⁄2e ¿°k ¿o(c) 5 ¿3⁄4ieMiA2\"±31⁄2eeMμro+eb1b 5 Bμ¶3⁄4 μr1Te±3o 11⁄2_25i^25 5 ¿° I1Ð\nß\na\n}\n\nOn1⁄2\nII\nv\nO\nI1Þ\nß\na\n}\n\nOn1⁄2\nI\nc\nI\næ\n3O\nI\nAD»\no oi\næ\nO\n9(r) μr2 \"1N; ¿2[o+±b°b± \"\nð\n± 8;< \"(r) 1o e1A8;\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X 1o 3⁄4μr211⁄2¶25±b1⁄4 o AD±3o 3⁄4μe \"μe±3o 11⁄2¶1⁄2¶i 2\" \"1Te1⁄2e 25±ke ¿o X B11⁄2¶1⁄2eiAμe 9μ¶2· B X»S X \" B ¿3⁄4\no\n\ne ¿o\n\nB1oE;._\nD\nμ¶AD±31⁄2r25±3oyAX1oo+± t ¿2\"AX1T· ! \"(r)+ (AX1⁄2¶1⁄4+ \"Aa(r)+ ¿2t±» \"(r)+ !»e1⁄4 o 3⁄41°b ¿o(c) \"11⁄2 B1T \"μe±32\n}\nFU\na\nI\nAD±3o(c) \"μro*_\n1⁄4+±31⁄4 2\nO\n1o 3⁄4\nßF\n^\naI\n3⁄4μ¶2\"AD \" X 5\nOaoo\n»\nßF\n^\na¶\nU\nß\n¦\n\nð\nμr1⁄2¶1⁄2qe «o+ Xe31T \"μee i1o 3⁄4e X \"iN1⁄2¶1T \"e Aμ¶o°k1Te3oμe \"1⁄4 3⁄4+ e3μee^μ¶o+e1 Bμr25 5±!(r) μ¶e3(r) »S \"\n\n1⁄4+ ¿o ADi ±32\"AXμr1⁄2¶1⁄2¶1T \"μe±3o 2K325±3°b X \"μ¶°b ¿2AX11⁄2¶1⁄2e ¿3⁄4ajU aμ¶o+e3μ¶o+e\"l\no\n9(r)+ 4 5 B1T· yX±3μ¶3⁄4 11⁄2+ B1⁄41⁄2e\nð\nμ¶1⁄2¶1⁄2\n3⁄4 1°b·< \"(r)+ ¿25 4±31⁄4 (c)e1⁄4+ ,2B1⁄2e±\nð\n1⁄2¶i\nI\n\" ¿AX11⁄2r1⁄2\no,i\næ\nOao\n9(r) μ¶2AD±3°b ¿21⁄4+·< ¿25· ¿AXμ¶11⁄2¶1⁄2ei μ¶o \"(r)+ [· \" ¿25 ¿o AD 4±»3⁄4μ¶2\"AD±3o*_\n\"μ¶oM1⁄4 μe \"μe ¿2K12\nð\n(r)+ ¿o.2\"μ¶°(1⁄4 1⁄2¶1T \"μ¶o e«1ie±31⁄2e \"1Te _AX1⁄2¶1°k·\no o\no· B1AD \"μrAD kμe 1μ¶2~2\"1⁄4*«AXμe ¿oO ~ 5± \"1N;\nßF\n^\nnfF\nCð\n(r)+ X B\n\nμ¶2, \"(r)+ ~1⁄2e ¿o+e \"(r)<±»_ \"(r)+ [AX1Te 1⁄2e I1\nUK\nM·\no¶U¿i\nac3!+ \"(r) μr2, ¿o 2\"1⁄4 \" ¿2, \"(r) 1T \"(r)+ ~(r) μee3(r)b»n \"\n\n1⁄4+ ¿o ADikAD±3°b·±3o+ ¿oO \"2\n1T \" (3⁄4 1°b· ¿3⁄4y°b± B 2 B1T·μr3⁄4 1⁄2eiA \"(r) 1o \"(r)+ 1⁄2e±\nð\n»n \"\n\n1⁄4 ¿o ADi\no\nY\n1⁄2e 5 X Bo 1T \"μee ¿1⁄2¶iA±3o (AX1o1⁄4 25 !e1A8;\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X Kð\n(r) μrAB(r)3⁄4+±O ¿2~o+± ~2\"1⁄4*$ X t»S \"±3°w \"(r) μr24· B±e1⁄2e ¿°>11⁄2e \"(r)+±31⁄4+e3(r)=μe [ 5±O±\nð\nμ¶1⁄2r1⁄2e3μee μ¶o 1AXAX1⁄4+ a1T 5 1o2\nð\nX a2[μe»\nßF\n^\na\nμ¶2 \"1N; ¿o 5±O±«1⁄2¶1T \"e\no\n9(r) t· \"±e B1°\nb\ng\nw\ng\nx2μ¶o AX1⁄2¶1⁄43⁄4+ ¿21 e X a2\"μe±3oi±» ^·±3o+ ¿oO \"μ¶11⁄2\n\n1⁄4 1⁄2¶ X 1241o11⁄2e 5 X Bo 1T \"μee 1 5±kμr°b·1⁄2¶μ¶AXμ¶\n°b X \"(r)+±^3⁄4 2ð\n(r) μ¶AB(r)y3⁄4+±<o+± :\nð\n¿1⁄2¶1⁄2\nð\nμe \"(r) \"(r)+ 2±pe X B11⁄2¶1⁄225 5 B1⁄4 AD \"1⁄4 \" 1±»q \"(r)+ ;·1A{;P1Te\noo\n4μ¶291T· · 1⁄2¶μe ¿3⁄4A 5±< ¿1Aa(r)\n1⁄2¶μ¶o t±»\no^\n\nPo\nO\nU a\n}\n^\na\n,\n\nI\n\nrwv\nÐ\na\n\nI\n\nÞ\n\nI\n\nI\nv\n-\nÐ\nU\n}\n\nI\n\nI\næ\n^3O\n5±ke3μee 2 \"(r)+ 2μe 5 X B1T \"μ¶±3o\n\nII\nv\n\nO\n\nI\n\nr1\nÞXoI\n\nI\n\nrwv\nÞ\n\nI\nv\nODIUUÐ\n\nr1\nO\nð\n(r)+ X B\n»\nO\na\nU\na\nß\n}\n^\na\nÐ\nß\n}\n1o 3⁄4\no=O\nU aKF\n}\n^\na\na\nU\na\nF\n}\n^\na\nÐUF\n}\nc\no\no!1t·1⁄4+ B 3⁄4 μ%$_1⁄4 2\"μ¶±3o1\n\n1⁄41T \"μe±3o\nI\no+±13⁄4+ ¿AX1¿i! 5 X B°oμ¶o\noAOO\no=OoUF\na\no\n9(r)+ ¿owP»n± q1⁄2r1T \"e\n»\nM·±3o+ ¿oO \"μ¶11⁄2\n\n1⁄41⁄2e X 925 X \"2 ¿1Aa(r) eT11⁄2¶1⁄4+ 1 5±k \"(r)+ ;1pe X B1Te (±»$μe \"2\nð\n±Ao+ ¿μee3(r)Oe± B2\no\n9(r) μ¶2\nð\nμr1⁄2¶1⁄2> Xe ¿o(c) \"1⁄4 11⁄2r1⁄2eiAAD±3oOe X \"e ! 5±k \"(r)+\nU\n\n25±31⁄2¶1⁄4 \"μe±3o;±» \"(r) ±3o+ _3⁄4μ¶°b ¿o 2\"μ¶±3o 11⁄2\na\n1T· 1⁄2¶1AD\n\n1⁄4 1T \"μe±3oME eEq \"(r)+ 25 5 ¿13⁄4+i._25 \"1T 5\no\n9(r)M1⁄4 2P \"(r)+ 25±31⁄2¶1⁄4+ \"μ¶±3o23⁄4+±M ¿2\no+± te1⁄2e±\nð\n1⁄4+· Xe ¿o\nð\nμe \"(r)y1⁄2r1T \"e\nßo\n1⁄4[±\nð\nXe X Kμ¶oe ¿o+ X B11⁄2μe [±3o 1⁄2eiAD±3o(c)e X Be ¿21 5±« \"(r) (AD± \" B ¿AD t25±31⁄2r1⁄4+ \"μe±3o\nß\n?^h1o3⁄4\nßF\n^\na¤\n*1N°(1⁄4 Aa(r)\n°b± \" y \" ¿25 5 Bμ¶AD \"μ¶e yAD±3o 3⁄4 μe \"μ¶±3oZ \"(r) 1oa \"(r)+ y2\" \"1Teμ¶1⁄2¶μe\nihAD±3o3⁄4 μe \"μe±3oZ»n±\n»n±\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X\noqo\n°k1¿iAe t°b± B ~1AXAX1⁄4 B1T 5 21o 3⁄4i1⁄2e ¿2\"2 ^· ¿o 2\"μee t 5±k1⁄4 25 t»n±\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X\noA\nM·±3o+ ¿oO \"μ¶11⁄2\n\n1⁄41⁄2e X\nð\n±31⁄41⁄2¶3⁄4ie ;1T 4μe \"2e ¿25 9μ¶oo ±3o*_25 \"μ%$· \"±e1⁄2e ¿°<2\nI\n»S\nð\nAD±3°b·1T \" \"°b ¿oO \"2\n>\n1⁄2¶1T Be ^\nO\nð\nμe \"(r) »n125 1⁄2¶μ¶o+ ¿1T\n;^μ¶o+ X \"μ¶AX2\no\nuq0/\n+mO¤R-kO\\)¥> a«¥IH¤k)q/MLq¢,¥hEP-{O¦\n~\n.1T \" :>o 11⁄2¶1⁄2eih \" ¿13⁄4+i£ 5±Z25±31⁄2ee \"(r)+ »n1⁄4 1⁄2r1⁄29o+±3o*_1⁄2¶μro+ ¿1T ¢1⁄4[±M3⁄4+e;^μ¶o*_k1⁄4t1⁄4* +1⁄2e XiZ\n\n1⁄41T \"μe±3o 2\nI o\n?3O\nð\nμe \"(r)\n1=°b±^3⁄4 μ%: ¿3⁄4\n\nB1oE;._\nD\nμrAD±31⁄2¶25±3oZ2\"AB(r) ¿°b\noNo\nome ¿o X B11⁄2μ¶°k·1⁄2¶μ¶AXμe ;°k X \"(r)+±M3⁄42(1T B i3⁄4μ%«AX1⁄4 1⁄2e 2»S± ko+±3oE_1⁄2¶μ¶o+ ¿1T\n· \"±e 1⁄2e ¿°k2 e1⁄4+\nð\nbAX1o M· 1⁄2e±3μe ~1<·1T B \"μ¶AX1⁄4 1⁄2¶1T t»S ¿1T \"1⁄4 \" !±»o1⁄44±^3⁄4+e;^μ¶o*_k1⁄4t1⁄4* +1⁄2e Xi\nI\n±e25 X \"e ¿3⁄4=eMi§1⁄4tμ¶o+ ¿21\n\nO\n5±!°<1N; t1⁄2¶μ¶»S ¿12\"μ¶ X ¿I$4(r)+ 9\n\n1⁄4 1T \"μe±3ok»n±\n\nμ¶21⁄2¶μ¶o+ ¿1T ,μro\n\nμe» \"(r)+ 9e31T \"μ¶o+e!eP1T aμ¶1Te1⁄2e ¿2\n\n1T \" t(r)+ ¿1⁄2¶3⁄4:u M ¿3⁄4wM1o 3⁄4\n\"(r)+ [\n\n1⁄4 1T \"μ¶±3o 2»n± \"(r)+\n\n1T B [1⁄2¶μ¶o ¿1T μ¶o\n\nμe»\n\nμr2(r)+ ¿1⁄2r3⁄4: ^ ¿3⁄4\no\n9(r)M1⁄4 2ð\n[AX1oi1⁄4 2\" 4 \"(r)+ [»S±31⁄2¶1⁄2¶±\nð\nμ¶o e22\"AB(r) ¿°b I\n}\nß\n,\n1⁄2\nINI\nv\nÐE1⁄2\nI\n-kO\nU\na\nU a\n^\naq1⁄41⁄2\n1⁄2\nII\nv\nÞ\n\n1⁄2\nI\"3⁄4\nÞ\nU\na\n1⁄4\n\n×\nI\nIK¿\nII\nv2A\na\n1⁄2\nINI\nv\nO_ÞX\n×\nI\nIK¿\nII\nv2A\na\n1⁄2\nI\nO\n3⁄4\nI!\n\nO\n}\nI!1⁄2\nI\nO\nß\n,\n¿\nINI\nv2A\na\nÐu¿\nI\nrwv2A\na\n-\nO\nU\na\n1⁄4\nI\n\nO\nI!1⁄2\nI\nOÐ¿\nII\nv2A\na\nOÞ'I\n\nO\nI!1⁄2\nI\nOÐ¿\nI\nrwv2A\na\nO\n3⁄4\nI! MUpO\nD\n± 5 i \"(r)+ «25 \"1Tee X aμ¶o+e±»9 \"(r)+ < \"μr°b <e Bμr3⁄4 2;»n±\n1⁄2\n1o3⁄4\n¿oB[\no+ «3⁄4 ±O ¿2(1y 5 a1T·> yX±3μr3⁄4 11⁄2, B1⁄41⁄2e <25 5 X·h 5±\n13⁄4+eT1o AD\n¿\n»n \"±3° 25 5 X·\n¿AÐ£UF\na( 5±\n¿kÞ\nUF\nak1⁄4 2\"μ¶o+e\n1⁄2\nI\n1o 3⁄4i3⁄4+±M ¿291 5 B1T· yX±3μ¶3⁄4 11⁄2> B1⁄41⁄2e t2\" 5 X· 5±k13⁄4+eT1o AD\n1⁄2\n»n \"±3°\n¿\n5±\n¿AÞU\n1⁄4 2\"μro+e\n¿\nII\nv2A\na\no\n9(r)+ X \" XeMii±3o 1⁄2ei1⁄2¶μ¶o+ ¿1T 4 5 Bμ¶3⁄4 μ¶1Te±3o11⁄2\n\n1⁄4 1T \"μe±3o 2[(r) 1¿e ( 5±«e 22\"±31⁄2ee ¿3⁄4\n»n±\n\n1T ¿1AB(r) 25 5 X·\no[I\n9(r)+ t\n\n1⁄41T \"μe±3o 2»n±\n\nAX1oie t25±31⁄2¶e ¿3⁄41o 11⁄2eiM \"μ¶AX11⁄2¶1⁄2eike ¿AX11⁄4 2\"\n\n×\n3⁄4+±M ¿2o+± 3⁄4+ X· ¿o 3⁄4\n±3o\n\nð\n(r)+ ¿o\n\nμ¶2\\:u M ¿3⁄4\nouO\n4(r)+ t2\" \"1Tee X Bμ¶o+e\nI\n1o 11⁄2e±e±31⁄42 5±( \"(r) ~°<μ¶3⁄4+·±3μ¶oO ,°b X \"(r)+±^3⁄4\nO\n°k1N; ¿2 \"(r)+ t· \"±^AD ¿2\"2\n25 ¿AD±3o 3⁄4 ± B3⁄4+ X 41AXAX1⁄4+ a1T 5 !μ¶oi \"μ¶°k\no\n±«°b± \"μ¶eP1T 5 ( \"(r)+ 225 \"1Tee X aμ¶o+eA±»$ \"μr°b 2AD±3o 2\"μr3⁄4+ X 4 \"(r)+ !»S±31⁄2r1⁄2e±\nð\nμro+ek2\"μ¶°b· 1⁄2e\n[a¥a\n+1°b·1⁄2¶ 21\naduc±31⁄2\noo\n\n(r) 1T·\no^\no\nOa\nI\n?o\na\nOÐ9Ic\nI!\na\nO\n~\n!AX1o\nð\nBμe 5 1 \"(r) μr21241I: B2\" ± B3⁄4+ X 425i^25 5 ¿°AA\nI\nO\n\nI! Ti3O\nA\n\nO\nÐ9Ic\nI! cOO\n9(r) μr2AX1oe ;3⁄4 μ¶2\"AD \" X \"μ(c)yX ¿3⁄41⁄4 2\"μ¶o+e »n±\nð\n1T B3⁄4\n\n1⁄41⁄2e X 912\nI\nII\nv\nO\nI\nI\nÞaß\n\nI\nI! N3O\n\nII\nv\nO\n\nI\nÐmß I\nI\n\nI!\næ\nO\ne1⁄4+ \"(r) 1»S±31⁄2¶1⁄2¶±\nð\nμ¶o e 11⁄2¶ 5 X Bo 1T \"μee\nI\nINI\nv\nO\nI\nI\nÞaß\n\nINI\nv2A\na\nI! O\n\nINI\nv2A\na\nO\n\nI\nrwv2A\na\nÐmß+I\nI\nI! N3O\nUK\n\nμ¶2\n\n1⁄4 μeeT11⁄2e ¿oO 5±k \"(r)+ 225 ¿AD±3o3⁄4± B3⁄4+ X 41AXAX1⁄4 B1T 5 !3⁄4 μr2\"AD \" X \"μ<y¿1T \"μe±3oi±»\noo\na\nI\nII\nv\nÐ\na\nI\nI\nÞhI\nII\nv\nO\nInI\nII\nv\nÐNI\nI\nOÐInI\nI\nÐNI\nI\nrwv\nO\nI! N^3O\nO\nßI\n\nII\nv2A\na\nÐ\n\nI\nrwv2A\na\nO\nI\n\nO\nO\nÐ[ß\na\nI\nI\nc\nI^UpO\n9(r) (1Te±pe b 5 aμ¶A8;y3⁄4 ±O ¿2~o+±\nð\n± 8;y»n± 1⁄44±^3⁄4+e;^μ¶o*_k1⁄4t1⁄4* ^1⁄2¶ Xi μ¶oAX12\" ¿2\nð\n(r)+ X \" ( \"(r)+\n\n1⁄41T \"μe±3o.μ¶2to+±3o*_\n1⁄2¶μ¶o ¿1T μ¶o\n\no\\G\n± ^1°b· 1⁄2e ^±3o+ [±»n 5 ¿o25 X \"2\n3⁄4\nO3⁄4\nO\nI\n\nO\n5± ¿1⁄2¶μr°kμ¶o 1T 5 [ \"(r)+ ~»n125 \"μ¶°b ~2\"AX11⁄2e ~±»\n3⁄4\n»n \"±3°\n\"(r)+ · \"±e 1⁄2e ¿°o1o 3⁄4 \" ¿3⁄4 1⁄4 AD \"(r)+ 3⁄4 μr°b ¿o 2\"μe±3o!±» \"(r) 2\"iM25 5 ¿°\no\nY\n1⁄2¶25±E(c) 5 \" ¿1T \"μ¶o+e~ \"(r)+ 93⁄4+ aμeeMμro+e9»n± BAD 91⁄2¶μ¶o+ ¿1T a1⁄2ei;μ¶2\no+± $11⁄2\nð\n1piM2$eT11⁄2¶μ¶3⁄4wP ¿25· ¿AXμ¶11⁄2¶1⁄2¶i[»n±\n\na\nI\nAX1⁄4+ \" \" ¿oO \"2ð\n(r)+ X B ±3o+ °kμee3(r)O 1⁄4 25 \"(r)+\n\n±31⁄2¶3⁄4 °k1o*_k1⁄4[±^3⁄4+e;Mμro*_;1T ny\n»n± B°!1⁄41⁄2¶1\no\\G\nμ¶o 11⁄2¶1⁄2ei+25±3°k X \"μ¶°b ¿2±3o+ !1⁄4 25 ¿2·±31⁄2ei^o+±3°kμ¶11⁄2<_{e125 ¿3⁄4i°b±^3⁄4+ ¿1⁄2¶21⁄2¶μ<;\nG\nμe ny¿(r)M1⁄4+e3(r)*_\nD\n1Te31⁄4 °b±\no\no\noN \"(r)+±32\" «AX125 ¿2!±3o+ «°(1⁄4 25 !25±31⁄2ee < \"(r) <o+±3o 1⁄2¶μro+ ¿1T 1\n\n1⁄41T \"μe±3o 2!eOi.μe 5 X B1T \"μ¶±3oN± 2eMi.1⁄4 2\"μ¶o e\nD\n\nð\n5±3oI\n°b X \"(r)+±^3⁄4\no9\nμro AD [ \"(r) ~2\" \"μ%$o ¿2\"2±» \"(r)+ ¿2\" 1\n\n1⁄4 1T \"μ¶±3o 225 5 ¿°k2»n \"±3°\n\"(r)+ 13⁄4 μ%$_1⁄4 2\"μ¶±3o< 5 X B°<2o+± \"(r)+ ;Mμro+ X \"μ¶AX2μe 5 X B1T \"μ¶e 2°b X \"(r)+±^3⁄4 29AD±3oOe X \"e !1T»n 5 X [1k»S\nð\nμ¶ 5 X B1T \"μe±3o 2\no\n[\no ;2\"AB(r)+ ¿°k ;»S± [μe 5 X B1T \"μe±3o μr2XI\n}\nß\n,\n1⁄2\nINI\nvAA\nø\nI\nv\nÐ@1⁄2\nI\n-<O\nU\na\nU a\n^\na61⁄41⁄2\n1⁄2\nINI\nvAA\nø\nI\nv\nÞ\n\n1⁄2\nI\n3⁄4\nÞ\nU\na\n1⁄4\n\n×\nI\nIK¿\nINI\nvAA\nø\n1⁄2\nINI\nvAA\nø\nOÞA\n×1⁄2\nI\nIK¿\nI\n1⁄2\nI\nO\n3⁄4\nI\na\nO\n}\nI!1⁄2\nII\nvAA\nø\nI\nv\nO\nß\n,\n¿\nINI\nvAA\nø\nI\nv\nÐ¿\nI\n-<O\nU\na\n1⁄4\nI\n\nO\nI!1⁄2\nII\nvAA\nø\nI\nv\nOÐ¿\nII\nvAA\nø\nI\nv\nOÞ'I\n\nO\nI!1⁄2\nI\nO$Ð¿\nI\nO\n3⁄4\nIi3O\nð\n(r)+ X B 9 \"(r)+ 4\n\n1⁄4 1T \"μe±3o21T \" tμe 5 X B1T 5 ¿3⁄4k»n± u\nO\n\n¿UNXcXcXc\n1⁄4 oO \"μ¶1⁄2+AD±3o(c)e X Be ¿o AD 1o 3⁄4\n1⁄2\nII\nvAA\nO\nO\n1⁄2\nI\n¿\nII\nvAA\nO\nO\n¿\nI\no\n9(r) ,1⁄2¶μ¶o+ ¿1T · 1T \" \"2±»^ \"(r)+ Bμee3(r)O (r) 1o 3⁄4;2\"μr3⁄4+ 1T \" , 5 B ¿1T 5 ¿3⁄42»e1⁄4 1⁄2¶1⁄2eitμ¶°b·1⁄2rμ¶AXμe \"1⁄2ei ð\n(r)μ¶1⁄2e q \"(r)+ o ±3o*_1⁄2¶μ¶o+ ¿1T _· 1T \" \"2\n1T \" ( 5 \" ¿1T 5 ¿3⁄4=eMi · \" ¿3⁄4 μrAD 5± n_AD± \" \" ¿AD 5±\no£[\no ! 5 Bμr3⁄4 μ¶1Te±3o 11⁄225iM2\" 5 ¿°w°(1⁄4 25 te !25±31⁄2¶e ¿3⁄4»n± ~ ¿1AB(r).μe 5 X a1T \"μe±3o\no\nD\n± 5 ; \"(r) 1T \"(r)+ ;°k1T 5 aμ% AAD±M «AXμe ¿oO \"2±3o \"(r)+ ;3⁄4μ¶1Te±3o 11⁄2_°(1⁄4 25 e ~1⁄4 ·3⁄41T 5 ¿3⁄4\nð\nμe \"(r)A \"(r)+ ;o+\nð\neT11⁄2¶1⁄4+ ¿2±»$ \"(r)+\ne31T \"μ¶o+eieT1T Bμ¶1Te1⁄2¶ ¿2\nð\nμ¶ \"(r) ¿1Aa(r)Nμe 5 X B1T \"μ¶±3o\no,[\n»n 5 ¿ow \"1N;Mμ¶o eA±3o (· \" ¿3⁄4 μ¶AD 5± ~25 5 X·N1o 3⁄4±3o+ bAD± \" \" ¿AD 5± !25 5 X·\nμ¶213⁄4+\n\n1⁄4 1T 5\no\nuq0G\nuE-!×E) Æ¢V-¡)uO¦N-O¦\nY\n1⁄2e \"(r)+±31⁄4+e3(r)AX1Te1⁄2e (· \"±e1⁄2e ¿°<291T \" μ¶o (r)+ X \" ¿oO \"1⁄2eiA±3o+ _3⁄4 μ¶°k ¿o 2\"μe±3o 11⁄2! ±3o+ 25±3°b X \"μ¶°b ¿2to+ X ¿3⁄4 2[ 5±i25±31⁄2ee (· \"±eE_\n1⁄2e ¿°k2μ¶o\nð\n±2± \"(r)+ \" X 425· 1AD 43⁄4 μ¶°b ¿o2\"μe±3o 2\noI9\n±3°b +1°b·1⁄2e ¿21T \" 4AX1T B3⁄4μ¶1A\nð\n1pe [· \"±·1Te31T \"μe±3ok1o3⁄4\n\na\nI\n3⁄4 μ%$_1⁄4 2\"μ¶±3oμ¶o 1k B±31⁄4 o 3⁄4AD ¿1⁄2¶1⁄2\noo~\n(e3μee \"1⁄4 25 41Y 1¿e± ~±»q \"(r)+ !3⁄4μ%«AX1⁄4 1⁄2e \"μ¶ ¿2μ¶o(c)e±31⁄2¶e ¿3⁄4\nð\nμe \"(r) \"(r)+ 22\"μ¶°k·1⁄2e 2ac_\n¥\n1⁄2¶μ¶o ¿1T · \"±e1⁄2e ¿° I\nC\nO\nC\n\nÞ\nC)EEE\nc\nI4OO\nG\n± ;1A1⁄4 o μe t2\n\n1⁄4 1T \" ( \" Xe3μe±3ow(r)\nn'In\nUN\n\nn\nn\nU\nð\nAX1o.1⁄2e X\nC\n×\n2O\nC\nI\nU\n»\nI\n»\nO\nU\nO\nUNXcXcXc\nI±\nC\n×\n~O\nC\nI\nU2^\n\n^\nO\nð\nμe \"(r)\n»\nIyO\n»\nO\n^\noD\n± 5 2 \"(r) 1T\nC\n×\n\nμr2 5±«e ;μ¶oO 5 X \"· B X 5 ¿3⁄4y1241<e ¿AD 5± ~±»,1⁄2e ¿o+e \"(r)\n\na\no+± 41k°<1T 5 Bμ%\no\n9(r)+ ¿oi \"(r)+ ;»n±\nð\n1T B3⁄4\n\n1⁄41⁄2e X 93⁄4 μ¶2\"AD B X \"μ<y¿1T \"μe±3o±»\no4\nμ¶2\nC\nII\nv\n×\n\nO\nC\nI\n×\n\nÞ\nß\n^\na1⁄4\nI\nC\nI\n×\nI\nvAA\n\nÐ\na\nC\nI\n×\n\nÞ\nC\nI\n×\nrwvAA\n\nOÞ'I\nC\nI\n×\nA\n\nI\nv\nÐ\na\nC\nI\n×\n\nÞ\nC\nI\n×\nA\n\nrwv\nO\n3⁄4\nI3O\nUK^\n\n±\nE\nII\nv\nO\n¤\nÞ\nß\n^\na\ni\n¥\nE\nI\nI\næ\nO\nð\n(r)+ X B\ni\nμ¶2to+±\nð\n1o\na\nLa\na\nQ+EDo={AP|TEaA¿þ(c)ÆTo>ATA°k1T 5 Bμ<\no1o\n»\nC\n×\n\nμr24± B3⁄4 X \" ¿3⁄4.2\nð\nX X·μ¶o+eA \"±\nð\n_\nð\nμ¶25 μ¶o \"(r)+\nd_3⁄4 μe \" ¿AD \"μe±3o\nI\n1⁄2rμ<; 1 \" ¿13⁄4 μ¶o ek1b·1Te ;»n \"±3° e± 5 5±3° 5±k 5±·\nO\ni\n(r) 12[1b25 5 B1⁄4 AD \"1⁄4+ B ;1⁄2¶μ<;\nEI\nI\nI\nI\nIII\n\nI\nI\nI\n\nI\n\nI\nI\nI\n\nI\n\nI\nI\nI\n\nI\n\nI\nI\nI\n\nI\nÐEN\nN\nN\nN\nN\nO\n\nIP O\nð\n(r)+ X B \"(r) e1⁄2e±^A8;^2$1T B OL I\nμ¶2$ \"(r)+ 9μ¶3⁄4+ ¿oO \"μe\ni!°k1T 5 Bμ< hI\nμ¶2$ \"(r)+ yX X \"±;°k1T 5 aμ% h31o3⁄4\nI\nμ¶21t 5 Bμ¶3⁄4μ¶1Te±3o 11⁄2\n°k1T 5 Bμ< i1⁄2¶μ(c);\n\nμro\no[Ii3O\ne1⁄4\nð\nμe \"(r)\nÐ0\n±3o \"(r) 23⁄4 μ¶1Te±3o 11⁄2_μro 25 5 ¿13⁄4±»\nÐ\na\no\n9(r) !25 \"1Teμr1⁄2¶μe i AD±3o3⁄4 μe \"μe±3oμ¶2\nßNn\n^\naKFc+o\n4(r)+ 2e1A{;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X [°k X \"(r)+±M3⁄4μ¶o \"(r)+ 2\"1°b o+± \"1T \"μe±3o=μ¶2\n\"(r)+ ¿o\n¤\nÐ\nß\n^\na\ni\n¥\nE\nINI\nv\nO\nE\nI\nI3O\n1o 3⁄4\n\nB1oE;._\nD\nμ¶AD±31⁄2r25±3oμ¶2\n¤\nÐ\nU\na\nß\n^\na\ni\n¥\nE\nII\nv\nO\n¤\nÞ\nU\na\nß\n^\na\ni\n¥\nE\nI\nI^3O\n9(r)+ ¿2\" <1T \" «2\" 5 B1μee3(r)O 5»S±\nð\n1T B3⁄4me ¿o+ X B11⁄2¶μ(c)y¿1T \"μe±3o 2;±» \"(r)+\nU\n_\n¥\n°b X \"(r) ±M3⁄4 2Ke1⁄4 1⁄4 o+»n± \" \"1⁄4 o1T 5 ¿1⁄2ei$μ¶»±3o+ A1T i_\n5 ¿°b· \"2( 5±.25±31⁄2ee A \"(r)+ i1Te±pe °k1T 5 Bμ% N\n\n1⁄4 1T \"μe±3o2!eMi\n\n11⁄42\"2\"μ¶1om ¿1⁄2¶μ¶°kμro 1T \"μe±3ow \"(r)+ 2yX X B±=3⁄4μ¶1Te±3o 11⁄2¶2£: 1⁄2¶1⁄2\nμ¶owq \" ¿2B1⁄4 1⁄2e \"μ¶o+eμ¶o1⁄4 o »S ¿12\"μ¶e1⁄2ei=1⁄2¶1T \"e i25 5± B1Te \"\n\n1⁄4 μe \" ¿°k ¿o(c) \"2\no\n9(r)+ X \" Aμ¶2(1yeT125 b1⁄2¶μe 5 X B1T \"1⁄4 \" k±»4μe 5 X a1T \"μee\n°b X \"(r)+±^3⁄4 24»S± ~25±31⁄2ee^μ¶o+e<2B1⁄4 AB(r)°k1T 5 Bμ%\n\n1⁄4 1T \"μe±3o 2Ke1⁄4+ [ \"(r)+ Xi 1T \" (2\"1⁄2e±\nð\n¿25· ¿AXμ¶11⁄2¶1⁄2¶iiμ¶»$±3o !(r) 12[ 5±« B ¿25±31⁄2ee\n1T 9 Xe X \"ii \"μ¶°k ~2\" 5 X·\no\n9(r)+ ;1⁄2r1T 5 5 X\nð\nμ¶1⁄2¶1⁄2 e 1 \"(r)+ ;AX125 2μ¶»o+±3o 1⁄2rμ¶o+ ¿1T μe±3o μ¶A~AX1⁄4+ B \" ¿o(c) 5 X a°k291T \" ;μ¶oAX1⁄2¶1⁄4 3⁄4+ ¿3⁄4Aμ¶o\n\"(r)+\nO\n¥a\ne ¿AX11⁄4 25 b \"(r)+ ¿o \"(r)+ k3⁄4 μ¶1Te±3o11⁄2 5 X B°k21±» \"(r)+ k°<1T 5 Bμ%\nð\nμ¶1⁄2¶1⁄2qAB(r)1o+e <1T ; Xe X \"i= \"μ¶°k <25 5 X·\no\nY\no\n11⁄2e 5 X Bo1T \"μee ;μ¶2 \"(r)+\nz\nA%=EDEJo A=eEnoOþ O2EnE5E{7K=eEaÆTo6OauXQ>AuE7XE=O\nz\nOOO×[°k X \"(r)+±M3⁄4i±»\nO\n¿1AD ¿°k1o1o 3⁄4¦41Aa(r)+»S± a3⁄4w μ¶o\nð\n(r) μrAB(r)±3o+ k25±31⁄2ee ¿2;\nð\n±yAD±3o 25 ¿AX1⁄4+ \"μ¶e b 5 Bμ¶3⁄4 μ¶1Te±3o11⁄2· \"±e1⁄2e ¿°k2KAD± \" \" ¿25·±3o 3⁄4 μro+e< 5± \"(r)\nI\n1o 3⁄4\n· 1T \" \"μ¶11⁄2\n3⁄4+ X Bμ¶eP1T \"μee ¿2\no[\no+ 1e X B2\"μ¶±3oV1\nU¿i\n3e±M ¿2412»n±31⁄2¶1⁄2e±\nð\n2XI\n¤\nÐ\nU\na\nß\n^\naØ\na\n\n¥\nE\nINI\nv2A\na\nO\n¤\nÞ\nU\na\nß\n^\naØ\na\nE\n¥\nE\nI\nI^\n\nO\n¤\nÐ\nU\na\nß\n^\na\nØ\na\nE\n¥\nE\nII\nv\nO\n¤\nÞ\nU\na\nß\n^\na\nØ\na\n\n¥\nE\nINI\nv2A\na\nI^^UpO\nð\n(r)+ X B\nØ\na\n\nE\nI\nO\nC\nI\n×\nI\nvAA\n\nÐ\na\nC\nI\n×\n\nÞ\nC\nI\n×\nrwvAA\n\nØ\na\nE\nE\nI\nO\nC\nI\n×\nA\n\nI\nv\nÐ\na\nC\nI\n×\n\nÞ\nC\nI\n×\nA\n\nrwv\no\noZ± \"(r)+ X\nð\n± a3⁄4 2Ø\na\n\nÞ\nØ\na\nE\nO\ni\noo\nkAX1oZ ¿12\"μ¶1⁄2eime> i2\"(r)+±\nð\no\nI\n25 X\nY\n· · ¿o 3⁄4 μ%\nO\n\"(r)1T b \"(r) μ¶2 25·1⁄2¶μ¶ 5 \"μ¶o+ey±»\n\"(r)+ ac_\n¥\na\n1T· 1⁄2¶1AXμ¶1o ±· X B1T 5± ~μ¶oO 5±<2\"1⁄4 AXAD ¿2B2\"μee\nU\n_\n¥\na\n1T·1⁄2r1AXμ¶1o 24μr29\n\n1⁄4 μeeT11⁄2e ¿o(c) 4 5±\n\na1oE;?_\nD\nμ¶AD±31⁄2¶2\"±3o 5±\nkIaßapOao\n9(r)+ X \" X»n± \" \"(r) μ¶21°b X \"(r) ±M3⁄4.(r) 12~ 5 a1⁄4 o AX1T \"μe±3o= X \" B±\nkIaß aPOÞ\nkI\n^\napO\n1o 3⁄4= \"(r) k2\"1°b k25 \"1Teμr1⁄2¶μe i12\naN\n\nB1o;?_\nD\nμ¶AD±31⁄2¶25±3o\noo\n,11⁄2¶25±1(r) 12q \"(r)+ 3⁄4+ ¿2\"μ¶ B1Te1⁄2e »S ¿1T \"1⁄4 \" \"(r) 1T ,μ¶o( ¿1AB(r)k2\" 5 X·\nI¿\n¿9Þ\nv\na\n\n¿9Þ\nv\na\n\n¿9ÞyUpO\n\n1⁄4 o AD±31⁄4 ·1⁄2e ¿3⁄4~ 5 Bμ¶3⁄4μ¶1Te±3o 11⁄2325i^25 5 ¿°k21T \" 2\"±31⁄2ee ¿3⁄4wT25±9 \"(r)+ °b X \"(r)+±^3⁄4;AX1o2e ¿12\"μr1⁄2ei[e ¿AD 5± aμ<yX ¿3⁄4(± ·1T B11⁄2r1⁄2e ¿1⁄2¶μ<yX ¿3⁄4\no\n9(r)+ ;°k X \"(r)+±M3⁄4yAX1o 11⁄2¶25±ke ~ ^ 5 ¿o 3⁄4+ ¿3⁄4 5±\niN¥!o(r)[\no+ ;25±31⁄4+ aAD ;»S± 4»n1⁄4+ B \"(r)+ X 93⁄4+ X \"1μ¶1⁄2¶2μr2£1\nU¿i\no\nuq1i\n¡6E-kO×¦6UX) §aE¤uy¢IE\n~\n(r)1¿e yAD±He X \" ¿3⁄4(r) ±\nð\n5±m25±31⁄2ee y25· 1AD _AX1⁄2¶1°b· ¿3⁄4a· \"±e 1⁄2e ¿°k2(eOia25±31⁄2¶eMμ¶o e\n[¥Z\n2k1o 3⁄4£(r) ±\nð\n5±h25±31⁄2ee\n25·1AD A1o 3⁄4. \"μ¶°k _3⁄4+ X· ¿o 3⁄4+ ¿oO\nO\n¥a\n22±3o1 2\"μro+e31⁄2e kAX1Te1⁄2e\no[\n»n 5 ¿oN±3o+\nð\n1oO \"2; 5±2\"±31⁄2ee k±3o1ye B1o Aa(r)+ ¿3⁄4\n25 5 B1⁄4AD \"1⁄4+ \"\noo\no21o!1⁄4 oOe B1o Aa(r)+ ¿3⁄4;AX1Te1⁄2e P ¿1AB(r) o+±^3⁄4+ (r)12\nð\n±[o ¿μee3(r)(c)e± B2e3μeeMμro+e \"(r)+ °<1T 5 Bμ% 119 5 Bμ%_3⁄4μ¶1Te±3o 11⁄2\n25 5 B1⁄4AD \"1⁄4+ \"\no\\~\n(r)+ ¿o \"(r)+ ;AX1Te 1⁄2e 2μ¶2e a1o AB(r)+ ¿3⁄4 (r)+±\nð\nXe X K \"(r) 2o+±^3⁄4+ ¿291T 9 \"(r)+ 2e B1o Aa(r)i·>±3μro(c) \"29(r)1¿e 21T t1⁄2e ¿125\n\"(r)+ \" X ;o ¿μee3(r)(c)e± B2K^μro(c) 5 \"±^3⁄4 1⁄4 AXμro+e!»e1T ±N$h_3⁄4 μ¶1Te±3o 11⁄2_ ¿1⁄2¶ ¿°b ¿o(c) \"2\no,o\n»$±3o+ ;μ¶2o ± 9AX1T \" X»n1⁄41⁄2_1Te>±31⁄4 \"(r)+ 1± B3⁄4+ X 9±»\n\"(r)+ (o+±M3⁄4 ¿2\n11⁄4 2\"2\"μ¶1oy ¿1⁄2¶μ¶°kμro 1T \"μe±3o\nð\nμr1⁄2¶1⁄2_μ¶oO 5 \"±M3⁄41⁄4 AD 213⁄4 3⁄4 μ¶ \"μe±3o 11⁄2_±N$h_3⁄4 μ¶1Te±3o 11⁄2$ ¿1⁄2e ¿°k ¿o(c) \"2AD±3°b·1⁄2rμ¶AX1T \"μ¶o+e\n\"(r)+ 25±31⁄2¶1⁄4 \"μe±3oh· \"±^AD ¿2\"2\no\n1⁄4tμ¶o+ ¿21\n\n392\"(r) ±\nð\n2b(r)+±\nð\n5±NoO1⁄4°2e X \"(r)+ ie B1o Aa(r)+ ¿2b1o 3⁄4ao+±^3⁄4+ ¿2( 5±N1¿e±3μr3⁄4@: 1⁄2¶1⁄2%_\nμ¶ow,1o3⁄4h11⁄2r25±=e3μee ¿2 \"(r)+ i»S± a°!1⁄4 1⁄2r122»n± k25·1T \"μ¶11⁄2r1⁄2ei.eP1T BiMμ¶o eAX1Te1⁄2¶ «· B±·> X B \"μe ¿2\no\nY\no+± \"(r)+ X b1T· · \"±31Aa(r)Z 5±\ne B1oAB(r) μ¶o e(μ¶2 \"(r)1T ±»æ12\"AX1Te3o μ1\nUU\no\n9(r)+ X \" ~±3o+ 1e B ¿1N;M21⁄4+· \"(r) ~2\" 5 B1⁄4 AD \"1⁄4+ \"\nI\n1bo+ ¿1⁄4 \"±3oi± 91bo+ X\nð\n± 8;\n±»o+ ¿1⁄4 \"±3o 2\nO\nμ¶oO 5±(2\"1⁄4 eE_{·μe ¿AD ¿2\noIG\nμe B2\" \"(r)+ ~\n\n1⁄4 1T \"μe±3o 21T \" ~25±31⁄2ee ¿3⁄4i12μe»_ \"(r)+ t·μe ¿AD ¿2\nð\nX \" 1μ¶o 3⁄4+ X· ¿o 3⁄4 ¿o(c) ,±»\n¿1AB(r)N± \"(r)+ X K_1o 3⁄4 \"(r)+ ¿o= \"(r)+ 25±31⁄2¶1⁄4+ \"μ¶±3o 2[1T B °<1T \"AB(r)+ ¿3⁄4.1T 1 \"(r)+ (e±31⁄4 o 3⁄4 1T aμe ¿2\no\n9(r)+ (eMμe B \"1⁄4+ !±» \"(r) μ¶2~μ¶2[ \"(r) 1T\n\"(r)+ b2\"1⁄4+e_{·μe ¿AD ¿2~AX1o=e (25±31⁄2¶e ¿3⁄4= «AXμe ¿oO \"1⁄2ei ±3o.1Ae ¿AD 5± 2± ~·1T B11⁄2r1⁄2e ¿1⁄2AD±3°k·1⁄4+ 5 X\no;o\no»n1AD \"(r) μ¶2~25· X ¿3⁄4 2\n1⁄4+· ^ ¿AX1⁄4+ \"μe±3o25±k°!1⁄4AB(r) \"(r)1T 4μe\nð\n±31⁄4 1⁄2¶3⁄4i·1piA 5±< \"1N; !1k1⁄4 oμe \"1T \"i<AX1Te1⁄2¶ 21o 3⁄4 1T \" \"μ<: AXμ¶11⁄2¶1⁄2ei«25·1⁄2¶μe μe 91⁄4+·\no\na\nU\n\nU\nU\na 9oU\n·[Z2\n~\n(r) 1pe 2\"1⁄4+ Be Xi ¿3⁄4\n\"(r)+ NAX1⁄4+ \" \" ¿oO\nð\nμ¶2B3⁄4+±3°\n±3o\n\"(r)+ .e ¿25 25±31⁄2¶1⁄4+ \"μe±3o2i 5±\nð\n(r) 1T °kμee3(r)O e =AX11⁄2r1⁄2e ¿3⁄4\n\"(r)+\n¿125i· \"±e1⁄2e ¿°<2\no9\n°k11⁄2¶1⁄2t25iM2\" 5 ¿°k2A±»\n[a¥a\nI\n2K4 Xe ¿o'25 \"μ%$\n±3o+ ¿2[AX1o8e =25±31⁄2ee ¿3⁄48e X \"i\nAAXμe ¿o(c) \"1⁄2¶i£ 5±\n(r) μee3(r)1AXAX1⁄4+ B1ADi\no\nO\n¥a\nI\n2t1T \" o 1T \"1⁄4+ B11⁄2r1⁄2eiA°k± \" (3⁄4μ%«AX1⁄4 1⁄2e +e 1⁄4+ 4 \" ¿12\"±3o 1Te1⁄2e (°b X \"(r)+±^3⁄4 2\nI\nμ\no\n\no\n25 ¿AD±3o 3⁄4*_{± a3⁄4+ X\n1AXAX1⁄4+ B1T 5 Y\nI1⁄2Þ\nO\nð\n± 8;\nO\n1T \" 91¿eT1μ¶1⁄2¶1Te 1⁄2e »S± ±3o+ _3⁄4μ¶°b ¿o 2\"μ¶±3o 11⁄2(c)· \"±e1⁄2e ¿°k2KTμro AX1⁄2¶1⁄4 3⁄4 μro+e9e B1oAB(r)+ ¿3⁄4bo+ ¿1⁄4 \"±3o 2\no\no\n~μr24±»AD±31⁄4+ B25 o+± 13⁄4 μ%«AX1⁄41⁄2e 4 5±AD±3°b b1⁄4+·\nð\nμe \"(r)· \"±e 1⁄2e ¿°k2[ \"(r) 1T\nð\nμ¶1⁄2¶1⁄2AD±3o+»S±31⁄4o 3⁄4y \"(r) (e ¿25 ~11⁄2ee± Bμ¶ \"(r) °k2\n±3ok \"(r) 9»n12\" 5 ¿25 AD±3°b· 1⁄4+ 5 X B2O\no\ne\no\n1o(c)i · B±e1⁄2e ¿°\nð\nμ¶ \"(r)k25 \"μ%$ ;Mμro+ X \"μ¶AX2,μ¶ok\nð\n±2± \"(r) \" X [25· 1AD 43⁄4 μ¶°b ¿o2\"μe±3o 2\no\n~\n«(r)1¿e «AD±3oAXμe±31⁄4 2\"1⁄2ei=1¿e±3μr3⁄4+ ¿3⁄4Ne ¿oO \"1⁄4+ Bμ¶o eyμ¶oO 5± \"(r)+ ¿25 «1T B ¿12 e± \"(r).e ¿AX11⁄4 2\" k±»±31⁄4+ ;±\nð\no1⁄2¶μ¶°<μe \"1T \"μe±3o 2\n1o 3⁄4 \"(r) ;1⁄2¶μ¶°kμe \"1T \"μ¶±3o 2±»$ \"(r) a: ¿1⁄2¶3⁄41241\nð\n(r)+±31⁄2e\no\no\noy13⁄4 3⁄4μe \"μe±3o 5±« \"(r)+ (·1T \" \"μ¶AX1⁄4 1⁄2r1T 413⁄4+e^μ¶AD\nð\n!(r) 1pe (2\"· Bμ¶oE;^1⁄2e ¿3⁄4i \"(r)+ B±31⁄4+e3(r)+±31⁄4+ ð\n(AD±3o AX1⁄2¶1⁄4 3⁄4 2(r)+ X \"\nð\nμe \"(r)\n25±3°b =e ¿o+ X a11⁄2tAD±3oAD X· \"2i \"(r) 1T 1T \" = \" ¿1⁄2e XeT1o(c) A 5±Z· \"±e1⁄2¶ ¿°k2<±3o\n11⁄2¶1⁄2t2\"AX11⁄2e ¿2A±»(3⁄4 μ%«AX1⁄4 1⁄2¶ i\noD\n1⁄4 °k X Bμ¶AX11⁄2\n°b X \"(r)+±^3⁄4 2;1T B <»e11⁄2¶1⁄2¶μee1⁄2¶\noH9\n±3°b <°k1piN(r) 1¿e AAD±3o 2\"μ¶3⁄4+ X a1Te1⁄2e b1T \" \"μ%: AXμr11⁄2,μ¶oO 5 ¿1⁄2¶1⁄2¶μee ¿o AD e1⁄4μ¶1⁄2e 1μ¶oO 5± \"(r)+ ¿°$e1⁄4+\nμ¶o \"(r)+ b ¿o 3⁄4 \"(r)+ X B kμ¶2~o+±11⁄2e 5 X Bo 1T \"μee b 5± 13⁄4 X X·¤;Mo+±\nð\n1⁄2e ¿3⁄4+e b±» \"(r)+ b·1T \" \"μrAX1⁄4 1⁄2¶1T t·(r)(c)i^2\"μ¶AX11⁄2$· B±e1⁄2e ¿°w±3o\n\"(r)+ 1·1T B ±»$ \"(r)+ ;μ¶oOe ¿25 \"μee31T 5±\noo\n¿o+ X B11⁄23⁄4+iMo1°kμ¶AX11⁄225i^25 5 ¿°k2 \"(r)+ X± BiAAX1oe ~e X \"ii(r)+ ¿1⁄2e· »e1⁄4 1⁄2 e ¿AX11⁄4 25 1μe\nAX1T 5 Xe± Bμ<yX ¿2[·±32\"2\"μee 1⁄2e t1o3⁄4 μ¶°b·±32\"2\"μ¶e1⁄2e 4e ¿(r) 1peMμ¶± B2\no\n9(r) X \" (μr2to ±A11⁄2ee± aμe \"(r) °\n\"(r) 1T 125±31⁄2ee ¿2111⁄2¶1⁄2$· \"±e 1⁄2e ¿°k2>1o 3⁄4 \"(r)+ b1⁄4 25 X 1°!1⁄425 ;^o+±\nð\n¿o ±31⁄4+e3(r)= 5±13⁄4 1T·\n\"(r)+ t 5±O±31⁄2 5±( \"(r)+ I5±e\noo\n11⁄2¶25±(·1piM2 5±(2\"±31⁄2ee t1!· \"±e 1⁄2e ¿°\neMik°b± \" t \"(r) 1o«°k X \"(r)+±M3⁄4\no\n4(r) 1T °b ¿1o22B1⁄4+· ·1⁄2e _\n°b ¿oO \"μ¶o+ekoM1⁄4 °b X BμrAX11⁄2°b X \"(r)+±^3⁄4 2\nð\nμ¶ \"(r)1o 11⁄2eiM \"μ¶AX11⁄2°b X \"(r)+±^3⁄4 2[1o 3⁄4 11⁄2¶25±k1⁄42\"μ¶o+ek°b± B ; \"(r) 1o±3o+ !oO1⁄4 °k X Bμ¶AX11⁄2\n°b X \"(r)+±^3⁄4\no=o\noh13⁄43⁄4 μe \"μe±3o 5±=AX1T \"Aa(r) μ¶o+e. B±31⁄4+ \"μ¶o+ « X \" \"± B2 \"(r)μ¶2!°<1¿im1⁄4o AD±pe X be X \"ih2\"1⁄4+e \"1⁄2¶ <±3o+ ¿2\no.o\noh±3o+\n2\"°k11⁄2r1⁄2e1⁄4 ~μr1⁄2¶1⁄2¶1⁄4 °kμro 1T \"μ¶o+e +1°b·1⁄2e \"(r) 1T\nð\n;Mo+±\nð\n±»R1o.μro 25 \"1Teμ¶1⁄2rμe iμ¶o \"(r)+ b3⁄4+i^o 1°kμ¶AX11⁄2$25i^25 5 ¿°\nð\n25 ¿o 2Bμe \"μee ~ 5±koO1⁄4°b X Bμ¶AX11⁄2> X \" \"± 9μro(c) 5 \"±^3⁄4 1⁄4 AD ¿3⁄4AeMi< \"(r)+\n\n¿1T 4°b X \"(r)+±^3⁄4w^e1⁄4+ o+± 0¦[1⁄4 o+e _;1⁄4+ 5 \"1* 1⁄2¶ ¿13⁄4 μ¶o+e( 5±\n3⁄4 μ¶2BAD±pe X \"i.±»41o+\nð\nAX1⁄2¶12\"2!±»·(r)+ ¿o+±3°b ¿o1\nI!9\n(r)+ X B°<1o1o 3⁄4E¦4μ¶oyX ¿1⁄2UK^^\na\nOaoBD\n±AD±3°b·1⁄4 5 X !· B±e B1°\nAX1o.e M· ¿AD 5 ¿3⁄4. 5± 1oO \"μ¶AXμe· 1T 5 k2\"1⁄4 AB(r).AX12\" ¿2\no\n[1⁄2e \"μr°k1T 5 ¿1⁄2eiyAD±3°b· 1⁄4+ \"1T \"μe±3o 11⁄2,2BAXμe ¿o AD bμ¶21μ¶25±3°k± \"·(r) μ¶A2 5±\n11⁄2¶1⁄2±»2\"AXμe ¿oAD 1o 3⁄4AX1oyo ±k°b± \" ; \"(r) 1oy11⁄2¶1⁄2±»2\"AXμe ¿o AD 1 Xe X [e> ;AD±3°k·1⁄2e X 5\no\naa\n\n$,\nØo2ß2)OE-Ka\na a aa1a\nIaa\no\n«\no\n(c)\n(r)uii\n(c)\nnæIco\n¬Aee\no\n1⁄4[ X \" Aμr2; \"(r)+ A11⁄2ee± Bμe \"(r) °\n»n± (25±31⁄2¶eMμ¶o e 5 Bμ¶3⁄4 μr1Te±3o 11⁄2,25i^25 5 ¿°k2(»n \"±3°\nU\nP3!$ B X· \"±^3⁄4 1⁄4 AD ¿3⁄4N»n± AD±3o(c)e ¿o μ¶ ¿o AD\no\n9(r)+ ;2\"iM25 5 ¿°\n±»$\n\n1⁄4 1T \"μe±3o2 5±<25±31⁄2ee !μ¶2\nc1⁄2\nrwv\nÞ\n\"\n41⁄2EÞ\nE\n41⁄2E\nI\nv\nOes\nOoUN\na\nXcXcXcc\n\nð\nμe \"(r)\n\nv\nO\nE\nO\n\no\n9(r)+ qjU»n±\nð\n1T a3⁄4 ¿1⁄2¶μ¶°kμro 1T \"μe±3o3l(25 5 X·I\nE\nv\nO\nE\nv\nF\n\"\nv\ne\nv\nO\ne\nv\nF\n\"\nv\n\"\n\nO\n\"\n9Ða\nE\n\nrwv\n\nO\na\nBidXcXcXc4\n\nes\nO\nI1es9Ðmes\nrwv\nOnF\n\"\n\nO\na\nBidXcXcXc\n\nE\n\nO\nE\nF\n\"\n\n(O\na\nBidXcXcXc4\n\nÐ\nU\n9(r)+ qjUe 1A8;\nð\n1T B3⁄42\"1⁄4+e2\" \"μe \"1⁄4+ \"μe±3o3l 25 5 X·I\n1⁄2\n\nO\ne\n\n1⁄2E\nO\nesÐ\nE\n1⁄2\nI\nv\n\nO\n\nÐ\nUN\n\nÐ\na\nXcXcXcc¿U\n9(r)+ (25±31⁄2¶1⁄4+ \"μ¶±3o μ¶29 B X \"1⁄4+ Bo+ ¿3⁄4μ¶o\n1⁄2\n>11⁄2r1⁄2 \"(r)+ !± \"(r)+ X t1T B B1¿i^2~1T \" 2±He X\nð\nBμe 5 5 ¿o\no9o\n» \"(r)μ¶24μ¶2[1AXAD X· \"1Te1⁄2e ±3o 1⁄2¶i\n\n1T \" a1¿i^2<±»11⁄2e ¿o+e \"(r)\n1T \" \"\n\n1⁄4 μe \" ¿3⁄4\no\n9(r)+ yoM1⁄4 °!e X b±»~1T aμe \"(r) °b X \"μ¶Ai±· X B1T \"μ¶±3o 2<μ¶2\nkI\n\nO\nð\n(r) μ¶Aa(r)£μ¶2\n±· \"μ¶°<11⁄2\no\na a aa%e\nAi\nMiTi\noAo0i\n(c)\nn\ne\nið\neÆ¬\ni@i\na\n(c)\niðnEooðPihn'oii\na\nX 5 \"μro+e\neNO\nv\na\n\n_\n\nB1oE;._\nD\nμrAD±31⁄2¶25±3o\nI o^3O\nμ¶2\nI1Ðqe\ni\nO\nE\nII\nv\nO\nI1Þ@e\ni\nO\nE\nI\n\nð\n(r) μr1⁄2e\nY\n¥to;I o^\n\nZd^^UpO\nμr2\n,\nÐVe\nØ\na\n\n-\nE\nII\nv2A\na\nO\n,\nÞ@e\nØ\na\nE\n-\nE\nI\n,\nÐqe\nØ\na\nE\n-\nE\nII\nv\nO\n,\nÞ@e\nØ\na\n\n-\nE\nII\nv2A\na\nc\n\n±3°!eμ¶o μro+e! \"(r) 21⁄2¶125\nð\n±«\n\n1⁄4 1T \"μ¶±3o 2e3μee ¿2\n,\nÐVe\nØ\na\nE\n-\nE\nII\nv\nO\n,\nÞae\nØ\na\n\n-\n,\nÐVe\nØ\na\n\n-\nrwv\n,\nÞEe\nØ\na\nE\n-\nE\nI\nc\nD\n±\nð\ne\nØ\na\nE\n1o 3⁄4\ne\nØ\na\n\n1T \"\n<Iaß O\n2\"±\nð\n2AX1oy \"\nð\nBμ¶ 5 t \"(r) 21Te±pe ;»n± B°k11⁄2¶1⁄2¶i«12\n,\nÐqe\nØ\na\nE\n-\n,\nÐVe\nØ\na\n\n-\nE\nII\nv\nO\n,\nÞae\nØ\na\n\n-\n,\nÞ@e\nØ\na\nE\n-\nE\nI\nc\na\ni\n\n^·1o 3⁄4μ¶o+e(e3μee ¿2\n,\nÐVe\nØ\na\n\nÐVe\nØ\na\nE\nÞkIaß\na\nO\n-\nE\nII\nv\nO\n,\nÞae\nØ\na\n\nÞ@e\nØ\na\nE\nÞkIaß\na\nO\n-\nE\nI\n\nð\n(r) μrAB(r)μ¶2\n\n1⁄4 μeeT11⁄2e ¿oO 5±\n\nB1oE;._\nD\nμ¶AD±31⁄2¶25±3o 1⁄4 · 5±\nkIaßapO\ne ¿AX11⁄4 25\ni\nO\nØ\na\n\nÞ\nØ\na\nE\no\na\n\no\n3mo!,o,B\n\n+1°b·1⁄2¶ f: 1⁄2e ¿2o+ X ¿3⁄4+ ¿3⁄4k»S± , \"(r) M X BAXμr25 ¿2AX1o<e> »n±31⁄4 o 3⁄4b1T ?oP÷2÷uøuuBuu yKþy\nBy\noy\nBu o 'yKu. 11⁄2e±3o+e\nð\nμe \"(r)1⁄4 ·3⁄41T 5 ¿3⁄4i± 413⁄4 3⁄4 μe \"μ¶±3o 11⁄2_AD±·μe ¿2±»q \"(r)+ ¿2\" ;o+± 5 ¿2\no\nUTo9o\n»_ \"(r)+\n\n1T \" \"(r)\nI\n\"1N; ¿o121!25·(r)+ X \"\nð\nμe \"(r)k a13⁄4 μ¶1⁄4 2\nO\næ\niP N\n;^°\nO\nð\nX \" [AD±He X \" ¿3⁄4\nð\nμe \"(r)<1\nU\n¦\n°\n1⁄2¶1pi X\n±»e±31⁄2¶3⁄4wð\n(r) 1T\nð\n±31⁄4 1⁄2¶3⁄4«e 9 \"(r)+ ~μ¶o AD \" ¿12\" [μ¶oA2\"1⁄4+ \"»e1AD [1T B ¿161\nU\næ3\n\n±3°b· 1T \" [ \"(r) [1o 2\nð\nX B2i±31⁄4Ae X μ¶»\ni±31⁄4ZAu×~ \"1N; ! \"(r) 23⁄4 μ%$ X \" ¿o AD ;μ¶o \"(r)+ 21T \" ¿1<e> X»n± \" ;1o 3⁄4y1T»S 5 X t±\n×~1⁄4 2\" ;3⁄4 μ%$ X \" ¿o(c) \"μr11⁄2¶2 5±< ¿25 \"μr°k1T 5\n\"(r) 2AB(r) 1o e\no~\n(r) μ¶AB(r)yμ¶2°b± \" 21AXAX1⁄4 B1T 5 \"(r)+ 1 +1AD [°k X \"(r)+±M3⁄4 ± 9 \"(r)+ 21T· · \"±K +μ¶°k1T 5 ;°b X \"(r) ±M3⁄4\na\no\noA=nu^EXubAP=eE7aA;^o+±\nð\n2($1¿i^1⁄2e± (2\" X Bμe ¿2XI t25 < \"(r)+ g\ng\nx\ng\ni»n1⁄4 oAD \"μe±3oN 5±e X ( \"(r) H: B2\"\n\n5 X a°k22±»\nc\n\"!2 gxPw uP1o 3⁄4\naa\nm\nM·1o3⁄4+ ¿3⁄421T \"±31⁄4o 3⁄4\nb\n1o 3⁄4! \"2 [ ^·1o 3⁄4+ ¿3⁄4!1T \"±31⁄4 o 3⁄4$#\no\n1⁄4[μ¶oO XI\ni±31⁄4AAX1oke X \"(r)+ [2\"iMoO \"14 beOi iM·μ¶o e&%\"g\ng\nx\ng\n\no\n±3°b·1T \" 9 \"(r) 425 X Bμe ¿2,»S±\nc\n\"!('\n!1o 3⁄4Ap)\n*\n'gxPw2\nI\n',+g-\n\n.\nm\na\n\nOao\niMoa\n±31⁄4yAX1o 1⁄4 25 \"!!i 5±b 5 ¿25 \"(r)+ 1· \" ¿AXμr2\"μe±3oA±»i±31⁄4+\nð\n± 8;^25 \"1T \"μe±3o\no\\~\nBμe 5 ;1\n`\n3x/ : 1⁄2¶ [ 5±<AX11⁄2rAX1⁄4 1⁄2¶1T 5\nIUUqÞ£I\n\ncJ3O\n×\nO>ÐhUTo[\n· \"μe±3o11⁄2aIA t25 2oNAP=nu^EDukA=eE7BA2 5±bAX11⁄2¶AX1⁄4 1⁄2¶1T 5 ~ \"(r)+ ~2\"1°b ~»e1⁄4 o AD \"μe±3oA1o 3⁄4AeP1T Bik \"(r)+\n· \" ¿AXμ¶2\"μe±3o\nð\nμe \"(r)i \"(r)+ 10iAD±3°k°k1o 3⁄4\no\n+oZ\nμee ¿o\n\"(r)1T < \"(r)+ X \" \"± K93⁄4 μr2\"AD \" X \"μ<y¿1T \"μe±3o£·1⁄2¶1⁄4 2b B±31⁄4 o 3⁄4*_{±N$±»;1u± a3⁄4+ X A°k X \"(r)+±M3⁄4\n»S±\n[a¥a\n2Aμ¶2\nß\nø\nÞqSnToU\n\nß\nrwv\n: o3⁄4< \"(r)+ ;°<μ¶o μ¶°(1⁄4 °\nX B \"± 91T 5 \"1μ¶o 1Te1⁄2¶\noI~\n(r) 1T 9(r)1T· · ¿o 2 5±b \"(r) ~°<μ¶o μ¶°(1⁄4 °\nß\n12Au\nμro AD \" ¿125 ¿2y9(r)+ ;°<μ¶o μ¶°(1⁄4 °\nX \" \"±\nMoZ\n±3o 2\"μ¶3⁄4+ X \"(r) \" X 9°b X \"(r)+±^3⁄4 2\nð\nμ¶ \"(r) e31⁄2e±e11⁄2+ X \" B±\nkIaß>OkIaß\na\nO\n1o 3⁄4\n<Iaß\n\nO\n1o 3⁄4k \"\n\n1⁄4μe Bμ¶o+e\nU\n^adM1o 3⁄4\n\nXeT11⁄2¶1⁄4 1T \"μe±3o 2!±»\nI\n·> X ! \"μ¶°b i25 5 X·q \" ¿25· ¿AD \"μee ¿1⁄2¶i\no¤\n11⁄2rAX1⁄4 1⁄2¶1T 5 < \"(r) « 5± \"11⁄2oM1⁄4 °2e X !±»9»n1⁄4o AD \"μe±3o\nXeT11⁄2¶1⁄4 1T \"μ¶±3o 29o+ X ¿3⁄4+ ¿3⁄4 5±<1AB(r) μ¶ Xe !1be31⁄2e±e 11⁄2 X \" \"±\nS\n»S± [ ¿1AB(r)y°k X \"(r)+±M3⁄4\no\næ\no0~\naμe 5 k3⁄4+±\nð\no.1o 11⁄2eiM \"μ¶AX11⁄2¶1⁄2ei \"(r) b \" ¿2\"1⁄4 1⁄2e t±»±3o+ b25 5 X·N±»¦\n\n1T· ·1⁄2rμe ¿3⁄4y 5± \"(r) b\n\n1⁄4 1T \"μ¶±3o\na\nO\nU\n1o3⁄4N2\"(r)+±\nð\n\"(r) 1T 1 \"(r)+ k1⁄2e±^AX11⁄2 5 B1⁄4o AX1T \"μe±3o= X \" \"± !μ¶2\nkIaß\nt\nO\neOi=AD±3°b·1T Bμro+e\nð\nμe \"(r)= \"(r)+ k$1¿i^1⁄2e± !2\" X Bμe ¿2\n»n±\n\nY\n\no\noNAP=nu^EDukA=eE7BAbAX1o(r)+ ¿1⁄2e·\nð\nμ¶ \"(r)i \"(r)+ ;11⁄2ee Xe B1\no\nOo\n¿25 ! \"(r)+ «1AXAX1⁄4+ a1ADi.±»\n\n1⁄4 1⁄2e X ;1o3⁄4V¦@\n\n±3oN \"(r)+ «\n\n1⁄41T \"μe±3o\na\nO\nU\nð\nμe \"(r)\nUaW\n1⁄4 2\"μ¶o eA ¿μ¶ \"(r)+ X\n\"!#!± T\no\nO\n1⁄2e± 9± [ \"1Te1⁄4 1⁄2¶1T 5 ! \"(r)+ ; X \" B± [1T [1: M ¿3⁄4y \"μ¶°b\nÐ\n¿E\nß_o\nc X Bμe»niA \"(r)1T 4 \"(r)+ ; X B \"±\nμro AD \" ¿125 ¿2 ^·±3o+ ¿oO \"μ¶11⁄2¶1⁄2eiAμ¶oi \"μ¶°b\nð\nμ¶ \"(r)\n\n1⁄4 1⁄2e X\no\nMo\n¿25 ~»n±\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X ~±3o\na\nOUU\nð\nμ¶ \"(r)\nUNn\ni1o 3⁄4e X Bμe»ni \"(r) 1T ~ \"(r)+ b25±31⁄2r1⁄4+ \"μe±3oe1⁄2e±\nð\n2t1⁄4 ·=μ¶»\nß\nμ¶2\no ± 2\"°<11⁄2¶1⁄2 ¿o ±31⁄4+e3(r)\no\nB!#!«3⁄4+±M ¿2\"owI\n(r) 1¿e 11!e1A8;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X °b ¿oM1⁄4Aμ¶ 5 ¿°Oe 1⁄4+ i±31⁄4iAX1oA»n1N; 1μe eOi\nð\nBμe \"μ¶o+eb±31⁄4 9 \"(r)+ 2 B ¿AX1⁄4+ B2\"μe±3oyμ¶o 1\no\n3⁄4μe»:>1⁄2e ;1o 3⁄4y1⁄42\"μ¶o+eb \"(r)+ (3⁄4 μ%$ X \" ¿o AD ;\n\n1⁄4 1T \"μe±3o25±31⁄2ee X\nooa\n±31⁄4AX1o\nAD±3°k·1T \" ;eMiA3⁄4 ±3μ¶o+ebe± \"(r)»n±\nð\n1T a3⁄4\n\n1⁄4 1⁄2e X 91o3⁄4e1A8;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X 9μ¶oi \"(r)+ ;2B1°b : 1⁄2e\no\n^Mo\n4(r)+ !oNA=nu+EDukA=eE7BAbAD±3°k°<1o 3⁄4\n`\n\"32 4657+\nm\na`98\na\n*\nb;:\nb\n+<4\na=:\na\n+>465\n:@?\nxPw.A45CB)D2\na4b\nB ^ ¿AX1⁄4+ 5 ¿2q \"(r)+ 1⁄4o 25 \"14_\ne 1⁄2e ~ \" ¿AX1⁄4+ B2\"μ¶±3oi±» \"(r)+ ;1⁄2¶ ¿1T· »S B±ek°b X \"(r)+±^3⁄4\nI oi^3O~U\nb \"μ¶°k ¿2\no\n¿2\" 4μe\nð\nμe \"(r)\nO\nOoUÞ@SK\nv\nO\n\ncJ\n»n± 1eT1T Bμe±31⁄4 2[eP11⁄2r1⁄4+ ¿2t±»\nS\nμ¶o AX1⁄2r1⁄4 3⁄4 μ¶o+eH\noAI![\n· \"μe±3o 11⁄2{I42\"μ¶°(1⁄4 1⁄2¶1T 5 1⁄2e ¿1T· »n \"±e\nð\nμe \"(r)Æ B!#!± Aμ¶o\n3⁄4μ%$> X B ¿o AD 1\n\n1⁄41T \"μe±3o °b±^3⁄4+ ;1o 3⁄4e X aμe»SiA \"(r) 1T 4μ¶ 9μ¶211⁄2\nð\n1¿i^241⁄4 o 2\" \"1Te1⁄2e\nouO\na\n\nU\n\no\noA=nu^EXubAP=eE7aAkAX1o11⁄2¶25±«AX11⁄2rAX1⁄4 1⁄2¶1T 5\no4i^\n\" ¿AX1⁄4 B2\"μee ¿1⁄2ei\no¥\n: o+ E F\nb\nHGI+>4\nbJ:\nF\na\nKGI+>4\na=:\nF wMLKGI+\n5N \" w\nm\na\n\nm\nF w\nm\n5 ~1o 3⁄4!25 X (r)+±\nð\n1⁄2e±3o+etμe \"1N; ¿2q 5±4 XeT11⁄2¶1⁄4 1T 5 F F35\nb\n\no9\n(r)+±\nð\n\"(r) 1T\nXeT11⁄2¶1⁄4 1T \"μro+eO \" w6y \"1N; ¿2QP\nI\n»e1⁄4 o AD \"μe±3o= XeT11⁄2¶1⁄4 1T \"μ¶±3o 2ð\n(r)+ X B EP\nI\nμr2~ \"(r)+\n¿\n\"(r)\nG\nμee±3o 1AXAXμoO1⁄4°2e X\no\n\n1o!i±31⁄4b2\" X\nð\n(r)(c)i; B ¿AX1⁄4+ B2\"μee 11⁄2ee± Bμe \"(r)°k2ð\n(r) μ¶1⁄2¶ ¿1⁄2e Xe31oO (c)1T \" o+± AD±3°k°b±3o 1⁄2¶i21⁄4 25 ¿3⁄4(μ¶o(oO1⁄4 °k X Bμ¶AX11⁄2\nð\n± 8;\nUUToZ9\n±31⁄2ee ; \"(r)+ 1· B ¿3⁄4 1T 5± n_{· \" Xii25i^25 5 ¿°\nA\nI\nO\n(c)IbÞaaI\nI^\na\nO\nA\nO\n\nÞ\n\nI\nð\nμe \"(r) ·1T a1°b X 5 X B2\n<O\n\nc\na\nd{~O\nÐ;c\n\nUN8~O\nÐ2UTc\n\nO\nc\n\nU\n1o3⁄4yμ¶o μ¶ \"μ¶11⁄2_AD±3o 3⁄4 μe \"μ¶±3o 2\nIOe\n\nO\ni\n\no\n9(r)+ A25i^25 5 ¿°\nð\nμ¶1⁄2¶1⁄2±32\"AXμr1⁄2¶1⁄2¶1T 5\nð\nμe \"(r)N \"(r)+ ¿2\" «eT11⁄2¶1⁄4+ ¿2\no\n[25\n\n1⁄41⁄2e X\nð\nμe \"(r)h1 a1o+e «±»[25 5 X·h2Bμ<yX ¿2\no\n[\ne25 X \"e ! \"(r)+ !e ¿(r) 1peMμ¶± [μ¶oy \"(r)+\nI\n_\n·(r) 125 !·1⁄2¶1o+\no\n1⁄44±\nð\n2B°k11⁄2¶1⁄21<eP11⁄2r1⁄4+ 2±»\n»\no\nμr2[o+ X ¿3⁄4 ¿3⁄4y 5±Ae X\ne±M±^3⁄4.1o 2\nð\nX B2\nð\nμe \"(r)\n\n1⁄4 1⁄2e X a¦@\n\nY\n»S 5 X a: o3⁄4 μ¶o+eke±M±^3⁄4.oO1⁄4 °k X Bμ¶AX11⁄2·1T B1°b X 5 X B2K_ M· X Bμ¶°k ¿o(c)\nð\nμe \"(r)Z3⁄4 μ%$ X \" ¿o(c) μ¶o μ¶ \"μ¶11⁄2AD±3o 3⁄4μe \"μe±3o 2\noV\n±3°k·1T \" A \"(r)+ o 1T \"1⁄4+ \" i±»9 \"(r)+ i±32\"AXμr1⁄2¶1⁄2¶1T \"μe±3o 2\nð\nμe \"(r)h \"(r)+±32\" A±»\n\"(r) k25iM2\" 5 ¿°)μ¶o= \"(r)+ : 1⁄2¶ E-\"ySR\n`\nE\ng\nI\n\"(r)+ «°b±M3⁄4 ¿1⁄2$μr213⁄4+ ¿2\"AD Bμee ¿3⁄4=μ¶o1\nU\n3!$i±31⁄4NAX1oN1⁄425 b \"(r)+ k3⁄4+ X»e11⁄4 1⁄2e\n· 1T B1°b X 5 X B2[μ¶oi \"(r)+ : 1⁄2¶\nOaoI~\n(r) μrAB(r) 25i^25 5 ¿°\nμr29°b± \" ;3⁄4+ ¿1⁄2rμ¶AX1T 5 1 5±«μ¶oO 5 Xe B1T 5 2oM1⁄4 °b X aμ¶AX11⁄2¶1⁄2ei\n~\n(r)(c)i\nU\na\no!I\n$1N; ¿o(»S B±3°\nj\n[\nBeμe \"2\n~\n± \" \"(r)\n÷\nX 5 \"μ¶o+e\n[\no3l*¦9±e\n;o 1T· ·(1o 3⁄4\n\"1o\n~\n1Te±3owX[a¥aBD\n\nð\n2\"1⁄2e X i_\n5 X ~\nμro(c) 5 X\nUK^^\næ\nouO\n9(r)+ 1»n± BAD ¿3⁄4\n¥\n1⁄4E«o+eb\n\n1⁄4 1T \"μe±3o\nI a\na(c)Þ\n\nc¶UKPI a+ÐNI ÞZI\na\nO\n\nciNUT\n\nI\no\nO\nμr2beμ¶25 \"1Te 1⁄2e ie> X\nð\nX ¿o1N1⁄2¶μ¶°<μe (ADi^AX1⁄2e y1o3⁄4£1NAB(r)1T± \"μ¶A 1T 5 5 B1AD 5±\no\n\n±3°b· 1T \" \"(r)+ 25 ¿o2\"μe \"μee^μe im 5±\nμro μe \"μ¶11⁄2AD±3o 3⁄4 μ¶ \"μe±3o 2\nð\n(r)+ ¿o2\" \"1T \" \"μ¶o+eb»n \"±3°\nI5I\na\nO\nI\n\nc\næ\n¿UTci3O\n1o 3⁄4\nI5I\na\nO\nIÐ2UTc\n\n¿UTc\n\nOao\nU¿iMoZ9\nμ¶°!1⁄41⁄2¶1T 5 41;e±31⁄2¶ \"1Te tAX1⁄2¶1°k·\nð\nμe \"(r)¶ B!#!<μ¶ob\nð\n±\nð\n1piM2Kð\nμ¶ \"(r)<1£1⁄4[ ¿1¿e^μ¶2\"μr3⁄4+ 9»e1⁄4 o AD \"μe±3o\nI\n1⁄4 2\"μ¶o+e;(r) ¿1¿e\nI{O5O\n1o3⁄4\nð\nμe \"(r)1be31⁄2¶±e11⁄2(r)Y 1TebeT1T Bμ¶1Te1⁄2¶\no\n\"iAe± \"(r)¦41⁄4 o e _;1⁄4+ 5 \"1k1o 3⁄4\n\n¿1T\no\nU +oZ9\n±31⁄2ee « \"(r) A25 \"μ<$\n25iM2\" 5 ¿°\nIUU\na\nO\nð\nμ¶ \"(r)μ¶o μ¶ \"μ¶11⁄2,AD±3o 3⁄4 μ¶ \"μe±3o 2\nIqI\n\nO O)UTc\n\nUN\nI\n\nObO%Ð\nay1⁄4 2Bμ¶o+e\n\n1⁄4 1⁄2e X K¦\n\nY\n3⁄4 1°k21o 3⁄4\n\n¿1T\no\n1⁄44±\nð\n2\"°k11⁄2r1⁄23⁄4+±M ¿2\nß\n(r) 1pe b 5±e> (»n± t ¿1Aa(r)N°b X \"(r)+±^3⁄4= 5±1¿e±3μ¶3⁄4.μro 25 \"14_\ne μ¶1⁄2¶μe\niE\n\n±3°b·1T \" 1 \"(r) 1»S± B°\n\"(r)+ 2μ¶o 2\" \"1Teμ¶1⁄2¶μe\nik \"1N; ¿29e X\nð\nX ¿o°b X \"(r)+±^3⁄4 2\no\nUKMoZ\n±3°b·1T \" ~ \"(r)+ 125· X ¿3⁄4i±»\nY\n3⁄4 1°k21o3⁄4\n\n¿1T 9±3o¶yV\n\nP\n`\n\ng\nð\nμe \"(r)\nI\nO0U«O\n\ncJ^\nI\ne\nOfU«O\nU\n*1o 3⁄4\nI\nA\nO¿OA¿\nO\nI\n\nOao[I!\nI\nA\nO\nð\nμ¶1⁄2¶1⁄2> \"\n\n1⁄4 μe \" ~ \"\nð\naμe \"μ¶o+e( \"(r)+ 225i^25 5 ¿°\nouO\nU\næ\no\n1⁄4[ X \" <μr211o. ^1°k·1⁄2e b±»\n\n1⁄4 11⁄2rμe \"1T \"μee ¿1⁄2ei\nð\n\"±3o+e 1o 2\nð\nX B2;±e \"1μ¶o+ ¿3⁄4.e ¿AX11⁄4 2\"\nß\nμ¶21 5±M±eμeeE Xe ¿o\n\"(r) ±31⁄4+e3(r)h \"(r)+ A2\"±31⁄2¶1⁄4+ \"μe±3o 2!1⁄2e±M±;μ¶oO 5 X Bo 11⁄2¶1⁄2ei.AD±3o 2Bμ¶25 5 ¿oO\noo\noO 5 Xe B1T 5 i \"(r)+ 6: 1⁄2e MyV\n\n`\n\ng\nð\nμe \"(r)m \"(r)+\ne3μ¶e ¿ok·1T B1°k X 5 X B2\no\\\n2\" \"1Te1⁄2¶μ¶2\"(r) 11e±31⁄2r3⁄4<25 \"1o 3⁄41T B3⁄4k25±31⁄2¶1⁄4 \"μe±3o\nð\nμe \"(r)\n\n¿1T\noa\n±31⁄4<2\"(r)+±31⁄41⁄2¶3⁄4b25 X 4· X Bμ¶±M3⁄4 μrA\ne 1⁄4+ B25 \"μ¶o e\nð\nμe \"(r)\n\n2\"·μ<; ¿29· X 9e 1⁄4+ B25\nof\n+1°kμ¶o μro+e \"(r)+\n1⁄2\n_\n¿\n·(r)125 ;·1⁄2¶1o ;AD±3o*: B°k24 \"(r)+ 225±31⁄2r1⁄4+ \"μe±3o\n5±ie ;· X Bμe±^3⁄4 μ¶A\no\n¦4 X B1⁄4 o\nð\nμe \"(r)\n\n1⁄4 1⁄2e X t1⁄4 2\"μ¶o eREW+\n`a\n\n`98\n\na`ib\no0a\n±31⁄4=°k1¿i11⁄2¶25±< 5 ¿2\" ~2\"±3°b !±»\n\"(r) ;± \"(r)+ X 4°b X \"(r) ±M3⁄4 29 5±<25 X 2(r)+±\nð\neμ¶e|iAX1o e ~»S± 9 \"(r) ¿°\no\nU Oo\n4(r) μ¶2t ^1°k·1⁄2e μ¶2\nð\n± B2\" b \"(r) 1o= \"(r)+ · \" Xe^μe±31⁄4 2t±3o+ I~9(r)+ · \"±e 1⁄2e ¿°\nAX1oo+± 1e £: ^ ¿3⁄4=eMi°k1N;^μ¶o+e\n\"(r) y25 5 X·\n2\"μ(c)yX 2\"°k11⁄2¶1⁄2e X\nomo\nk2\"(r)+±\nð\n2k(r)+±\nð\n±3o+ yAX1oae 3⁄4+ ¿AD ¿μ¶e ¿3⁄4aeOiZ1NoM1⁄4 °b X BμrAX11⁄21o2\nð\nX < \"(r) 1T\naæ\n\n1T··> ¿1T a2 5±£e .e X \"i'1AXAX1⁄4+ B1T 5 h1⁄4 o 1⁄2e ¿2\"2i±3o y;Mo ±\nð\n2 μro'13⁄4+eP1oAD\nð\n(r) 1T \"(r) .e> ¿(r)1¿e^μe± ±» \"(r)+\n2\"iM25 5 ¿°\n2\"(r) ±31⁄4 1⁄2¶3⁄4e\no4o\noO 5 Xe B1T 5 \"(r)+ (AD±31⁄4 ·1⁄2e ¿3⁄4 ±32BAXμ¶1⁄2¶1⁄2¶1T 5± ~25i^25 5 ¿°\nμ¶o \"(r)+ ,: 1⁄2e\n\ne_AD\nð\n±\no\n±^3⁄4+\no\n±O±^3⁄4\n· 1T B1°b X 5 X B291T B\n\nO\n\nc¶UKd\n§\n~O\n\nc\n\n1o 3⁄4A \"(r)+ 1 B ¿25 1T \"(r)+ ¿μe 3⁄4+ X»e11⁄4 1⁄2e \"2\no~\nμe \"(r)μ¶o μe \"μr11⁄2AD±3o 3⁄4 μ¶ \"μe±3o 2\nX\n+\nX\n5N+\nm\njCY\nAwZ+w[5>+\n`ib8\n\"(r)+ ~AD ¿1⁄2¶1⁄2¶2\nð\nμ¶1⁄2¶1⁄2^±32BAXμ¶1⁄2¶1⁄2¶1T 5 tμ¶ok·(r)125\no\\D\n±\nð\n11⁄2¶1⁄2e±\nð\ni±31⁄4+ B2\" ¿1⁄2e» 5±(e\n1⁄2¶ ¿3⁄4 1⁄4+· \"(r)+ ;e31T a3⁄4+ ¿o·1T \"(r)_I\no\no\noO 5 Xe B1T 5\nð\nμ¶ \"(r)¦@\nð\nμ¶ \"(r)E\\+\na\n»n± [aN k \"μ¶°k ;1⁄4 o μe \"2\no\no\n\n(r)+ ¿A8;i±31⁄4+ 21o 2\nð\nX\nð\nμ¶ \"(r)\n\n¿1T\nð\nμe \"(r) \"(r)+ k3⁄4 X»n11⁄4 1⁄2¶ toM1⁄4 °b X BμrAX11⁄2q·1T B1°b X 5 X B2\no\nc X Bμ¶»Si \"(r) 1T\n\"(r) ;\nð\n±«°b X \"(r) ±M3⁄4 21Te B X !AX1⁄2e±325 ¿1⁄2¶i\no\no\n¦4 ¿25 \"1T \"\nð\nμe \"(r)\n\n¿1T ,1⁄4 2Bμ¶o+e4 \"(r)\na\n125 μ¶o μe \"μ¶11⁄2(c)AD±3o 3⁄4 μe \"μe±3o2»n± ,aN 1°b± \" \"μ¶°b 1⁄4 o μe \"2\no\n¦9 X· ¿1T\n2\" Xe X B11⁄2 \"μ¶°b ¿21⁄4 oO \"μ¶1⁄2 i±31⁄425 X 11bo+\nð\n·1T 5 5 X Boi ¿°b X Be I, \"(r)+ 1AD ¿1⁄2¶1⁄2¶21T \" ;o+±\nð\n±32BAXμ¶1⁄2¶1⁄2¶1T \"μ¶o e(1oO \"μ%_\n· (r) 125\no\no\n¦[1⁄4 o ¦o»S± t1b1⁄2e±3o+eb \"μ¶°b\noo\n\nð\nμr1⁄2¶1⁄2o+ Xe X 4· B±M3⁄4 1⁄4AD ~1oO \"μ%_{·(r)125 1±32\"AXμ¶1⁄2¶1⁄2r1T \"μe±3o 2\no\no\n¦4 _{ B1⁄4 o ¦+e1⁄4+ · X \" \"1⁄4 \"ei \"(r)+ 2μ¶o μ¶ \"μ¶11⁄2 AD±3o3⁄4 μe \"μe±3oi»n± 9e+a 5±\nÐ0. Oo\n\nUTo\nO\n±3o 3⁄4 X ti±31⁄4 1±e25 X \"eT1T \"μe±3o 2t1o 3⁄4 5 \"i 5±i ^·1⁄2¶1μ¶o \"(r)+ ¿°\noa~\n(r) μ¶AB(r)e ¿(r) 1peMμe± ~μ¶2t \"(r)+ AD± \" \" ¿AD 1±3o+ c\n~\n(r)Oii3⁄4 ±b \"(r)+ 2oM1⁄4 °b X Bμ¶AX11⁄2_°k X \"(r)+±M3⁄423⁄4 μ%$ X\næ± B11⁄2aI9(r)+ ;AD±3°b·1⁄4 5 X 43⁄4+±M ¿2\"owI\n9(r) 1pe !1be a1μ¶ow^e1⁄4+ i±31⁄4y3⁄4 ±\no\nUKMoZ9\nμ¶o AD 11kaTo 3⁄43⁄4+ X Bμ¶eP1T \"μee ;μr21b3⁄4 X BμeeT1T \"μee 1±»$1b3⁄4 X BμeeT1T \"μee ^ \"(r)+ ¿o μe μ¶2o 1T \"1⁄4 B11⁄2 5±k3⁄4 μ¶2\"AD B X \"μ<yX 1μe 912\n1<3⁄4 μ%$ X \" ¿o AD 1±»q1k3⁄4 μ%$ X \" ¿o AD\no\\¥\nX Bμee\no\\?^\neMiA25 \"1T B \"μ¶o+e\nð\nμe \"(r)\noo\næb1o 3⁄4 1⁄4 2Bμ¶o+e\n\nInIHOO\n\n,\nI\n\nI\n\n-!Ð\n\n,\nI\n\nr\n\n-\n^\nUK^MoZ9\nX !11⁄2¶25± 1\næc3\no\nc X aμe»SiA \"(r) 1T [ \"(r)+ 2 ¿μee ¿oOeT11⁄2¶1⁄4+ ¿2±»\n\n1T \" !e3μee ¿o eMi\nI\næ\nUpO\neMi2\"(r)+±\nð\nμ¶o+eb \"(r) 1T\n\no\nT\nO\nU\nT\no\nT\n3⁄4\nO\nUNXcXcXc\n\nI\n\" ¿AX11⁄2¶1⁄2 \"(r) 1T\n\nμ¶2TNeMi6\nO\nð\n(r)+ X \"\no\nT\nμ¶2 \"(r)+ 1 ¿μ¶e ¿o(c)e ¿AD 5±\nI\n2Bμ¶o\n3⁄4\n»\nI\n2Bμ¶o9a\n3⁄4\n»\nIXcXcXcc\n2\"μ¶oT\n3⁄4\n»\nI_O\n»\nIAOoUF^I\n\nÞ\nUpOac\næ± \"μeeT1T \"μe±3o_I9(r)+ 1\n\n1⁄4 1T \"μe±3o\n\no]\nO\nU\no]\nμ¶21<3⁄4 μ¶2\"AD \" X \"μ(c)y¿1T \"μe±3o±»$ \"(r)+\n[a¥aM÷\nc\nO\nC\na\na\nOeU\nC\n±3o\n*U\nð\nμ¶ \"(r)N1⁄4\nI\n\nO(O\n1⁄4\nIUUpO(O\n*, \"(r)\no\nT\n1T \" A \"(r)+ A3⁄4 μ¶2\"AD \" X 5 < \" X· \" ¿25 ¿oO \"1T \"μe±3o 2!±»92\"μ¶o\nI3⁄4I_O\n±3om \"(r)+\n\n¿U\n3!ð\n(r) μrAB(r)1T \" ; \"(r) 1 ¿μee ¿o+»e1⁄4 o AD \"μe±3o 2±»q \"(r)+\n[¥Z4o\naN\noZ9\n±31⁄2ee < \"(r)+ i25 5 ¿13⁄4+i._25 \"1T 5 AX1Te1⁄2e «\n\n1⁄41T \"μe±3o\n\nÐ\n\nO\n1⁄4 2\"μ¶o e \"(r)+ «e±31⁄4 o3⁄4 1T \"ieP11⁄2¶1⁄4 <±· \"μe±3omμ¶o\n\"!#!\no\na\nX\n\nO\nU\n1T\nIiO\nk1o 3⁄4 3⁄4+±b \"(r) 2AX125 ¿2\n\nO\nk1o3⁄4\n\nO\nk1T\nI O\nð\nμ¶ \"(r)\nhOcJd¿UN\na\nBiMo\n4(r) μ¶2 \" X· B±M3⁄4 1⁄4AD ¿2\nG\nμ¶e\no\na\nU\nμro¦411⁄2¶1⁄2I\n21⁄4t1o 3⁄4+e±M±;«±»\nO\n(r)(c)i^2\"μe±31⁄2e±eii1T \" \"μ¶AX1⁄2¶\no\nY\n1⁄2r25±<3⁄4+±b \"(r)+ !AX125 2±»1\nAD±3o25 \"1o(c) t1T· ·1⁄2¶μ¶ ¿3⁄4AAX1⁄4 \" \" ¿oO 91T\nIO\n\nð\nμe \"(r)1k25 ¿11⁄2¶ ¿3⁄4 ¿o 3⁄4 1T\nIOoUTo\na\nUToZ9\n±31⁄2ee \"(r) y \"μ¶°b _3⁄4 X·> ¿o3⁄4+ ¿o(c) AAX1Te1⁄2e\n\n1⁄4 1T \"μ¶±3o\n\nO\n\nÐ\n\nI\n\no\nOyO\nÐ2U\n\nIUU\n\no\nOO\n*\nInI\n\nOiO\n1⁄4 2\"μ¶o+e\nMR\noa~\nBμe 5 i±31⁄4 <±\nð\no£μ¶o+·1⁄4 £: 1⁄2¶ ± <1⁄4 25 \"(r)+ : 1⁄2e ^R/#By)\ng`\n[R\no\n(r)+±M±325\n\ne Bμ¶3⁄4a·±3μ¶oO \"225±N \"(r) 1T °^\nO\n\no\na\no\n¿25 < \"(r) »S±\nð\n1T B3⁄4\n\n1⁄4 1⁄2¶ X <25 \"1Teμ¶1⁄2rμe ihAD±3o 3⁄4 μe \"μe±3oo\næ\niMo\n[\ne25 X \"e 2μro \"(r)+ 21⁄4 o25 \"1Te1⁄2e ;AX125 ¿2[ \"(r) 1T [ \"(r)+ ;»e125 5 ¿25\nG\n±31⁄4+ Bμe X tAD±3°b·±3o+ ¿o(c) [3⁄4+±3°kμ¶o 1T 5 ¿2[1T»S 5 X ~1b»n\nð\na\n\n\"μr°b 25 5 X·2\no\nY\n1⁄2¶25±= 5 \"ie 1A8;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X ¦41⁄4 o e _;1⁄4+ 5 \"1*1o3⁄4\n\n¿1T\noEa\n±31⁄4£AX1o£11⁄2¶2\"±N1⁄4 25 i \"(r)+\n2\" 5 ¿13⁄4+i?_2\" \"1T 5 <25±31⁄2¶1⁄4+ \"μ¶±3o ±e \"1μ¶o ¿3⁄4\nð\nμe \"(r)y \"(r) !e±31⁄4 o 3⁄41T \"iieP11⁄2¶1⁄4 (25±31⁄2¶e X t±»: B!#!1211«Aa(r)+ ¿A8;±3o \"(r)+\n2\"±31⁄2¶1⁄4+ \"μe±3o1T\no\nO\naN\no\naa\no\næμ¶o μr°kμ<yX 4 \"(r) [ X \" B± »n± ¿μe \"(r)+ X »n±\nð\n1T a3⁄4«± e1A{;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X »S±\nO\n¥a\n2»n± 1(e3μee ¿oi1°b±31⁄4 oO ±»\nð\n± 8;\no\n1N; \"(r)+ ! X \" \"± [ 5±«e\n^ß(ÞM\n^\naTo\n9(r)+\nð\n± 8; μ¶2[μ¶o(c)e X a25 ¿1⁄2eii· \"±·± \" \"μe±3o11⁄2_ 5±< \"(r)+ (oO1⁄4°2e X\n±»qe Bμr3⁄4i·>±3μro(c) \"21o 3⁄4 5±k \"(r)+ 2oM1⁄4 °!e> X ±»$ \"μ¶°b ;2\" 5 X·225±b ¿2\" \"μ¶°k1T 5 !μ¶ 12\nD\n\n_\no\na\niMo\n4(r)+ [e 1A8;\nð\n1T B3⁄43⁄4μ%$> X B ¿o(c) \"μ¶1T \"μ¶±3o«°b X \"(r) ±M3⁄4 2\nI\ne\no\\(c)i3O\nAX1oAe>\nð\nBμ¶ 5 5 ¿oAμro< 5 X B°<2±» \"(r)+ te1A8;\nð\n1T a3⁄4\n3⁄4μ%$> X B ¿o AD 1±· X B1T 5± K_\nI\nO\nI\nÐ\nI\nrwv\n\n1o3⁄4 \"(r)+ 23⁄4 μ<$> X \" ¿oO \"μ¶1T \"μe±3oi±· X B1T 5± u\"\no\\÷\n1A{;\nð\n1T B3⁄4\n\n1⁄4 1⁄2e X 9μr2 \"(r)+ ¿o\n_\nII\nv\nO'ß\n\"\nINI\nv\nc\nI^i3O\n¦4\nð\nBμe \"μro+e\no\n\nO\nU\nUÐ\n_\n\noo^i\nAX1o e ~ M· 1o 3⁄4+ ¿3⁄4»n± B°k11⁄2¶1⁄2¶i«12\n_\nII\nv\nO\nA\n_\nÞ\n_\na\na\nÞ\n_\na\ni\nÞ8cXcXc\nA\nII\nv\nc\n1⁄4tμee3(r)+ X 8_{± B3⁄4+ X ;°b X \"(r)+±^3⁄4 211T \" b \"(r) ¿o=±e \"1μ¶o+ ¿3⁄4=eMi°k1T \"Aa(r) μ¶o+e °k± \" 5 X B°k21±» \"(r)+ kμ¶o *: oμe 5 !25 X aμe ¿2\no\n4(r)+ ;25 ¿AD±3o 3⁄4*_{± a3⁄4+ X 4°b X \"(r)+±^3⁄4 μ¶2\nA\n_\nÞ\n_\na\na\nA\nINI\nv\nO'ß II\nINI\nv\nOac\n(r)+±\nð\n\"(r)1T 9 \"(r) μ¶2μ¶29\n\n1⁄4 μ¶eP11⁄2e ¿oO 5±\n1INI\nv\nO\n\ni\n1I\nÐ\nU\ni\n1I\nrwv\nÞ\na\ni\nßII\n1II\nv\nOac\na\n\n`\n;a¿_|o,\nU\n÷to\\\nB°b ¿oO 5 \"±31⁄4+\no@5\nuMAPE¿E\nAeATo>EcbAu^EAO2ATo ATu E7aATA.dAPE =EDu(ENA Æ¿ÆTAA÷EDEaEaEaÆTo7egf\no@÷\n\"±M±;M2\n>\n±31⁄2e O\n1AXμ%:>A\n\n\"±He\n11⁄2¶μ¶»S± Boμ¶1*UK^^\n\no\nac3I¦\noG\nXi^o °k1o\no\nAu^EihEDATo ukATokjE87=eø^E5EDEiÆTo\nu(c)APEaE7DE\no\nY\n3⁄43⁄4 μ¶25±3o*_\n~\n¿2B1⁄2e XiI¦9 ¿3⁄4\nð\n±O±^3⁄4\n\nμe i\nY\nUK^\næ\niMo\ni\n3(\noto\n1T B3⁄4\no\nOaou=eEUÆ|Tø7K=eEaÆToV={Æld={ÆK7XuMAPE=eE7 O2E\n}tEDE\"EDo=eEeATAIQmXø A=eEaÆTo+E\no\næ1T BAD ¿1⁄2\n¥\n;d; X KD\n\nð\na\n± 8;\n1o3⁄4\n÷\n125 ¿1⁄2UK^Mo\n\n~o~\no\n¿1T\no\n4(r)+ !oM1⁄4 °b X BμrAX11⁄2μ¶oO 5 Xe B1T \"μe±3o±»± B3⁄4 μ¶o 1T BiA3⁄4 μ<$> X \" ¿oO \"μ¶11⁄2_\n\n1⁄4 1T \"μe±3o2\no\noNA=nu#T·ÆTuQI\na\nU\nI\nU\næ\nZ UK\nædUK^\næ\nOo\n\n~o£~o\n¿1T\no\nt1ø^u<EDEJE7aATA\nOao E=eEaATAm÷ ATA\nøE\nE5Æ?DA¶EDu(E\nEnowyEn|TEno ATEJAO2E\n}tEDE\"EDo=eEeATA|QmXø A=eEaÆTo+E\no\nO\n\" ¿oO \"μ¶AD _k1⁄4[11⁄2r1⁄2\no+e31⁄2¶\nð\n±M±M3⁄4\n\n1⁄2¶μ%$_2D\n\nðon\nX B25 XiUK^P MUTo\næc3\no\n1⁄4\noN\n±31⁄2¶1⁄4 e!1o 3⁄4\nn\no\næ\noP[\n\" 5 Xe31\no\nd(r)7XE{EDou=eE\n»7z·ÆuXQ>ø*=eEnoOþ(ATo|@O2E\n}tEDE\"EDo=eEeATA\"Qm¿ø A=eEeÆo+E\no\nY\nAX13⁄4 ¿°kμ¶A\nO\n\" ¿2\"2÷\n±325 5±3oUK^^\na\no\n\n¥(o\n1⁄4t1o 25 ¿1⁄2!(o\næ1T 5±Eto\næ ¿1⁄4 o μe X K 1o3⁄4\na\noD\n¿1⁄2e \"o X\nof[\noyoO1⁄4 °k X Bμ¶AX11⁄2_2\"μ¶°(1⁄4 1⁄2¶1T \"μ¶±3o 2±»μ¶o(c) 5 Xe a1T 5 _\n1o3⁄4*_: \" 1o+ ¿1⁄4+ a11⁄2_o+ X\nð\n± 8;^2\no\nt(EDø^E5ATA:·ÆTuQ øE=K\nμ¶oi· B ¿2\"2XI\nd\nZ\nd hUK^^P Oo\n\n3Iæ\no\n1⁄4[μ¶o ¿2\no\n«AXμe ¿oO [AD±3°b·1⁄4 \"1T \"μe±3o±»,e B1o Aa(r)+ ¿3⁄4o+ X \"e (\n\n1⁄4 1T \"μ¶±3o 2\no\nOao=K;pu4EaÆN>aomE8|TE7BAAs·ÆuI>\nQ>ø*=eEnoOþPUK\nIæ\n^AZ*\nædUK^4+o\n^\n3I¦\no\nn\no\næ1A\n\n\" Xe±\no\nt(EDø^EUAAATo|ku4E5ATESooÆK|(c)EXAuEnoOþ\no\nY\nAX13⁄4+ ¿°<μ¶A\nO\n\" ¿2B29\n1o\n¥\nμe Xe±EUK^P Oo\nU\n43Iæ\no\nc\no\næ12\"AX1Te3o μ\no D\n1⁄4 °b X aμ¶AX11⁄2,°b X \"(r)+±^3⁄4 22»S± (o+ ¿1⁄4+ \"±3o 11⁄2°b±M3⁄4 ¿1⁄2¶μ¶o+e\noko\no\n~o\n1±^AB(r)h1o 3⁄4\noJoA9\nXe Xeh ¿3⁄4μe 5± B2+oE=nu^ÆK|PE~ESoAt(EDø^E5ÆTo ATAoÆK|(c)EDA\nEnoOþM·1Te ¿2\n(c)i^AZ??4+o\n9(r)+ æ\no\n\nO\n\" ¿2\"2\n1°!e Bμ¶3⁄4 e .æ12i_\n2B1AB(r)M1⁄4 25 X 5 \"2UK^^Mo\nUU\n3Iæ\no\nc\no\næ12\"AX1Te3oμ\no\nY\n·1T B11⁄2¶1⁄2e ¿1⁄2rμ<y¿μ¶o+e411⁄2¶e± Bμe \"(r) °o»n± ,AD±3°b·1⁄4+ \"μro+e125±31⁄2¶1⁄4+ \"μe±3o2 5±;1T Beμe 5 B1T Bμr1⁄2ei1e B1o Aa(r)+ ¿3⁄4\nAX1Te 1⁄2e ;o+ ¿1⁄4+ \"±3o°k±M3⁄4+ ¿1⁄2r2\no\np t(EDø^E5ÆPE 7¿E1oE=nu^ÆK|PEi\næMI\nU\n\nAZ UU\nUK^^^UTo\nU\nac3\n~o\n1⁄4\no\nO\n\" ¿2\"2÷to\nO\nowG\n1⁄2¶1o o X \"i9>o\nY\no\n¿1⁄4E;±31⁄2¶2n;Mi1o 3⁄4\n~\no\n\no\nc X 5 5 X Bμ¶o e\no\nt1ø^u<EDEJE7BATA 9E{7XEJQ+EJE\no\n\n1°2e aμ¶3⁄4+e [oμee X B2\"μe\ni\nO\n\" ¿2\"2\n1°2e aμ¶3⁄4+e 25 ¿AD±3o 3⁄4 ¿3⁄4 μe \"μe±3owUK^^\na\no\nU¿i\n3I¦\no0¥(o\n¦4μ¶Aa(r)(c) \"°!i X 1o3⁄4\no~\no\næy± \" 5±3o\no\nO2E\n}tEDE\"EDoh7aEoE=nu^ÆK|PE \\XÆTE6Oao E=eEaATA>T÷ATAuøE\nE5Æ?XAeEXu!E\no\no\noO 5 X B2\"AXμe ¿o AD\nO\n1⁄4+e1⁄2rμ¶2\"(r)+ X B2KD\n\nð\na\n± 8;hUK^\næ\nOo\nU\nY\no.9\n(r)+ X a°k1o 1o 3⁄4\nn\no\n¦4μroEyX ¿1⁄2\no\n¦4(r)OiO \"(r)°b±e ¿o μ¶A $ ¿AD \"2±»\nð\n¿1N;( ¿1⁄2e ¿AD 5 \"± 5±3o μrA9AD±31⁄4+·1⁄2¶μro+e[μ¶o o+ ¿1⁄4 \"±3o 11⁄2\n°k±M3⁄4+ ¿1⁄2r2\no5\nE5ÆK7Aat2A=eA\nz\n7BAP|\")dh7XE\n^\nIa\n. MU[Z\na\n. c\nUK^^\na\no\nUK\noo¥(oX9\n°kμe \"(r)\no\nt;øMu<EXEaE7BATAd>ÆTA\nøE=eEeÆTooÆ\n\\\nATE =eEeATA@O2E\n}tEDE\"EDou=eEaATAsQmXø A=eEaÆTo+Eqbkh,Eno E=EqO2E\n}~EDE\"EDo7JE\nomE=nuMÆ|PE\no[\nM»n± B3⁄4 to μee X B2Bμe i\nO\n\" ¿2B2[\n^»S± B3⁄4UK^Mo\na\n^\n\nU\næc3\n~ouGo\nc,1o\na\n±31o\no\n[2Bμ¶o+ek +1°b·1⁄2e ¿29 5±<e1⁄4 μr1⁄2¶3⁄4AD±3°b·1⁄4+ \"1T \"μ¶±3o 11⁄2μ¶oO \"1⁄4 μe \"μ¶±3o\no\ndO\nz\no\nt!EOqE>a\n+I3O\nI\nU\nUK^^Mo\nU\n9oI~\n±31⁄2e»S a1°\no\noA=nu^EDukA=eE7BAb\nz\ndAPE=EDu\n\\DÆE¶O!ÆESoOþ.oA=nu^EDukA=eE7DE¤DA·ÆTuQ øE=EXE\no\nY\n3⁄4 3⁄4μ¶25±3o*_\n~\n¿2B1⁄2e Xi¦4 ¿3⁄4\nð\n±M±^3⁄4\n\nμe i\nY\nUK^^^UTo\ni"
    },
    {
      "category": "Resource",
      "title": "hss_assocmem1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/bf699be0528543856f9daac4f88e29f6_hss_assocmem1.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nThe Hopfield model\nSebastian Seung\n9.641 Lecture 15: November 7, 2002\n1 The Hebbian paradigm\nIn his 1949 book The Organization of Behavior, Donald Hebb predicted a form of\nsynaptic plasticity with the following property\nWhen an axon of cell A is near enough to excite a cell B and repeatedly\nor persistently takes part in firing it, some growth process or metabolic\nchange takes place in one or both cells such that A's efficiency, as one of\nthe cells firing B, is increased.\nIn 1973, the phenomenon of long-term potentiation (LTP) was discovered. The study of\nLTP is now a small industry within the field of neuroscience. Among the various forms\nof LTP, those that depend on the NMDA subtype of glutamate receptor are regarded\nas \"Hebbian.\" These forms depend on temporal contiguity of presynaptic and postsy\nnaptic activity. The requirement of temporal contiguity is expressed in the following\nditty\nNeurons that fire together, wire together.\nNeurons that fire out of sync, fail to link.\nHebbian synaptic plasticity is remarkable for having been predicted on theoretical\ngrounds well before it was discovered experimentally. This is not a common occur\nrence in biology.\nHow did Hebb make his remarkable prediction? He was motivated by an old tra\ndition in Western thought known as associationism. This is the idea that the brain is\nnothing more than an engine for storing and retrieving associations. In the late 19th\ncentury, it was found that the brain consisted of neurons connected by an intricate web\nof synapses. This transformed the older associationist tradition into connectionism,\nthe doctrine that associations are stored as synaptic connections. This is an example\nof the idea that structure determines function in biology. Hume and other empiricist\nphilosophers had already expressed the idea that associations were learned from tem\nporal contiguity. It only stood to reason that connections should also be learned from\ntemporal contiguity.\nWhile these ideas are very suggestive, they are admittedly rather vague and metaphor\nical. The challenge of connectionism is to make these ideas precise.\nA number of theorists have formulated neural network models with the goal of\nexplaining how Hebbian synaptic plasticity could be used to store memories.\n\nX\nA\nX\nμ μ\nBinary model neurons\nFor the most part, we have studied neural network models in which the activity of each\nneuron is described by a single analog variable. A more drastic simplification is to\nuse binary neurons, which are either active (si = 1) or inactive (si = -1). In such a\ndynamics, each neuron updates itself according to the rule\ni = sgn\n⎝\n@\nWij sj\n,\n(1)\ns\nj\nwhere s0 is the state of neuron i after the update. Note that sometimes s0 = si, in which\ni\ni\ncase the update results in no change. If the argument of the sgn function happens to be\nzero, there is an ambiguity to the definition, which could be resolved by a coin flip or\narbitrarily defining sgn(0) = 1.\nEquation (1) can be used to define several types of network dynamics, depending\non the exact manner in which it is applied. In a sequential dynamics, the neurons are\nupdated one at a time, typically in a random order. In a parallel dynamics, Eq. (1) is\napplied to all neurons simultaneously. It is often easier to prove theorems about the\nsequential case, but the parallel case is sometimes easier to implement (for example in\nMATLAB it is more natural).\nIn a simulation on a digital computer, it is natural to make the updates at discrete\ntimes. However, the update times could in principle be continuous. For example, they\ncould occur at random for each neuron according to a Poisson process with some mean\nrate.\nAs we shall see, all these version of network dynamics behave qualitatively the\nsame in general, but there can be subtle differences.\nHopfield model\nSuppose that synapses change according to the learning rule\nWij = sisj\nwhere > 0 is a learning rate. This could be called Hebbian, although it has some\nweird features. The synapse is potentiated if both neurons are active, or both are inac\ntive. The synapse is depressed if one neuron is active, and the other is inactive.\nSuppose further that the network is exposed to P binary patterns i\n1, . . . , P during\ni\na training phase. More specifically, the state si is set to each pattern in turn, and the\nsynaptic weights are changed by the Hebbian update. If Wij = 0 at the beginning of\nthe training phase, Hebbian learning will result in\nWij = 1\nN\nμ\n(2)\ni\nj\nThe prefactor 1/N corresponds to a particular choice of learning rate = 1/N. It is a\nconvenient choice for the calculations we are about to perform, but is otherwise arbi\n\nX\nA\n\nX\n\nA\nX\n\ntrary, as Wij can be multiplied by any positive prefactor without affecting the dynamics\n(1).\nThis synaptic weight matrix is the famous Hopfield model, along with the dynamics\n(1), and the assumption that the patterns are chosen at random. This last assumption\nmakes sense if we assume that there is a data compression stage that encodes sensory\ndata efficiently before it reaches the Hopfield network.\nIn his original paper, Hopfield set the diagonal terms of the weight matrix to be\nzero (Wii = 0). If this is not done, the dynamics takes the form\ni = sgn\n⎝\n@\nij sj\n,\ni +\nW\nP\ns\ns\nN\nj,j=i\n\nThe network still basically works, but there are some subtle differences, which we will\ndiscuss later.\nAs we shall see in the following, the Hopfield model functions as an associative\nmemory, because the patterns are stored as dynamical attractors. If the network is ini\ntialized with a corrupted or incomplete version of a pattern, convergence to an attractor\ncan recall the correct pattern.\nSingle pattern\nLet's start with a simple case, a single pattern\nWij =\nij\n(3)\nN\nThe dynamics takes the form\n⎝\n@\nj\ni = sgn\ns\nj s\ni N\nj\nIn terms of the overlap\nm =\nj sj\nN\nj\nbetween and s, we can write\nsi\n0 = sgn (im)\nSince m = 1 when s = , it is a steady state of the dynamics. Similarly, m = -1 when\ns = -, so it is also a steady state of the dynamics. If m > 0, updates can only cause\nm to increase. Likewise if m < 0, updates can only cause m to decrease. Therefore\nthese steady states are attractors of the dynamics.\n\nX\nX X\nμ μ\nX X\nμ μ\n\nX X\nA\n\nX\nA\n5 Many patterns\nThe case of many patterns is more interesting. The weight matrix (2) is just the su\nperposition of single pattern outer products (3). The danger here is that the patterns\nmight interfere with each other. If the patterns were exactly orthogonal to each other,\nthere would be no interference. If they are chosen randomly, there is some possibility\nof crosstalk, which is quantified below.\nTo check whether the patterns are steady states, we calculate\nWij j\n\n=\n(4)\ni\nj\nj\nN\nj,j=i\nj,j=i\nμ\n\n+ 1\ni\n(5)\ni\nj\nj\n=\nN j,j=i μ,μ=\n\ni\n⎝\n@\ni\nμi\nj\nμj\n\n(6)\n1 +\n=\nN j,j=i μ,μ=\n\nThe last term is called the crosstalk or interference term. If it is greater than -1 for all\ni, then the th pattern is a steady state\ni = sgn\n⎝\n@\nWij j\n\nj\nIf we try to store too many patterns, then the interference between them becomes large,\nand storage is not possible.\nTo study the stability of a pattern, imagine that the network is initialized at the\npattern, and try to estimate how many bit flips take place. We can approximate the\ncrosstalk term as the sum of Np independent coin flips. There is an error when the sum\nis more negative than -N. The derivation in the book uses the Gaussian approximation\nto the binomial distribution. We will do something coarser, which is the Hoeffding\nbound on the deviation between frequency and probability. According to the one-sided\nHoeffding bound for m coin tosses\nProb[ˆp < p - ] < exp(-22 m)\nHere we are interested in deviations of the frequency of heads from 1\n2 by more than 2p .\nWe find that\n⎞\n⎛\nN\nPerror < exp(-2Np(1/2p)2) = exp - 2p\nThere are several definitions of capacity.\n- (fraction of bits are corrupted, Perror < 0.01) Suppose that we would like less\nthan one percent of the bits corrupted. Then we need exp(-N/(2p)) < 0.01, or\np < -N/(2 log(0.01)) = 0.11N. The problem with this calculation is that the\nflipping of the first bits could cause an avalanche. In reality, the capacity is about\np = 0.14N. Calculating this number requires some heavy mathematics, like the\nreplica trick.\n\n- (fraction of patterns are corrupted, Perror < 0.01/N ) We could impose a more\nstringent requirement, which is than less than one percent of the patterns have a\nbit corrupted.\nN\np = 2 log N\n- (fraction of samples are corrupted, Perror < 0.01/(pN ). The most stringent\nrequirement is that no pattern in the sample is corrupted, with confidence greater\nthan 99%. Taking log p log N , we obtain\nN\np = 4 log N\nIs this good? According to the Cover argument, the maximum should be p = 2N .\nWe can store patterns as attractors of the network dynamics (with some corruption).\nHowever, there can be other attractors, or spurious states. These are of three types.\n- There are reversed states - due to the ± symmetry of the network dynamics.\n- There are mixture states, which are a superposition of an odd number of patterns.\nFor example,\nmix = sgn(±μ1 ± μ2 ± μ3 )\ni\ni\ni\ni\nAn even number won't work, because the sum works out to zero on some sites.\n- There are spin glass states for large p, which are not correlated with any finite\nnumber of the patterns .\nEnergy function\nIf the interactions Wij are symmetric, then\nE =\nX\nWij sisj\n- 2 ij\nis nonincreasing under the dynamics (1), assuming asynchronous update.\nTo prove this, note that the i = j terms in the sum are unchanging, since s2\ni = 1.\nTherefore the change in E due to the update (1) is given by\nE\n= - 2(si\n0 - si)\nX\nWij sj\n(7)\nj,j=i\n\n= - 2(si\n0 - si)\nX\nWij sj +\n(s0\ni - si)Wiisi\n(8)\nj\nIf s0 = si, then E = 0 and we are done. In the other case, s0 = -si, and we can\ni\ni\nwrite\nE = -si\n0 X\nWij sj - Wiis 2\ni 0\nj\n\nTherefore the dynamics can be understood as descent on an energy landscape.\nStatistical physicists like binary neurons because they are analogous to magnetic\nspins. The synaptic interaction Wij can be compared to a magnetic interaction. If\nWij > 0, the spins want to line up in the same direction, as in a ferromagnet. If\nWij < 0, the interaction is called antiferromagnetic.\n7 Energy function\nLet's consider the case Wij = 1 for all i and j. This corresponds to a pure ferromagnet,\nin which all interactions try to make the spins line up in the same direction. In this case,\n⎠2\nX\nE =\nsi\n- 2\ni\nObviously there are two minima, the fully magnetized states si = 1 for all i and\nsi = -1 for all i. Any initial condition with more up spins than down spins will\nconverge to the all up state, and a similar statement can be made about the down state.\nSuppose that we would like to store a single pattern as an attractor of the dynam\nics. This is basically the same as the previous case. If we choose\nWij = ij\nthen the energy function is\nE\n= - 2\nX\nij sisj\n(9)\nij\nX\n⎠\n=\nisi\n(10)\n- 2\ni\nThe attractors of this dynamics are si = i and si = -i, as required. In statistical\nmechanics, this is known as the Mattis model.\nNote that the energy function for this network is\n⎠2\nE =\nX X\nsii\nμ\n- 2N\nμ\ni\nIt's plausible that the patterns should be local minima, but they might interfere with\neach other.\n8 Content-addressable memory\nThe input is encoded in the initial condition of the network dynamics. Convergence to\nan attractor corresponds to recall of the closest stored memory."
    },
    {
      "category": "Resource",
      "title": "hss_assocmem2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/614c4ed033780401f65843a8d6583562_hss_assocmem2.pdf",
      "content": "Models of delay activity\nSebastian Seung\n9.641 Lecture 16: November 12, 2002\nMiyashita experiment\nThe monkey was trained on a task involving visual short-term memory. The sample and\nmatch stimuli were one of 97 randomly generated \"fractal\" color patterns. Both sample\nand match were presented for 200 ms. During the 16 second delay between sample and\nmatch, no visual stimulus was presented. During training, the sample cycled through a\nfixed sequence, while the match was chosen at random.\nDuring the test phase, single unit recordings were made in anterior ventral temporal\ncortex. Now the sample stimulus was presented in random sequence. For the learned\nstimuli, there was strong delay activity. Neural response was selective for only a few\nlearned stimuli. The delay activity patterns for stimuli that were adjacent in time during\ntraining were correlated with each other.\nCaveat: The delay activity was weak for the 97 new stimuli. However, short-term\nmemory performance on new patterns was as good as on old.\nThe delay activity patterns could be the fixed point attractors of the Hopfield model.\nBut the attractors in the Hopfield model are uncorrelated with each other, so some\nmodification of the model is necessary.\nThe GTA model\nTo model the Miyashita experiment, Griniasty, Tsodyks, and Amit proposed a modifi-\ncation of the Hopfield model with the following weight matrix:\nWij = 1\nN\nP\nX\nμ=1\n(ξμ\ni ξμ\nj + aξμ+1\ni\nξμ\nj + aξμ-1\ni\nξμ\nj )\nThe ordering of the patterns is supposed to be the order of presentation during training.\nAnd for simplicity, the ordering is assumed to be cyclic, so that ξN+1\ni\n= ξ1\ni .\nThe natural order parameters are the overlaps\nmμ = 1\nN\nN\nX\ni=1\nξμ\ni si\nMIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\n\nSince\nX\nj\nWijsj = 1\nN\nP\nX\nμ=1\n(ξμ\ni + aξμ+1\ni\n+ aξμ-1\ni\n)\nX\nj\nξμ\nj sj\nIt follows from the update rule\ns′\ni = sgn\n\nX\nj\nWijsj\n\nthat the stationary states satisfy\nmμ = 1\nN\nX\ni\nξμ\ni sgn\nX\nν\nmν(ξν\ni + aξν+1\ni\n+ aξν-1\ni\n)\n!\nIf we now average this by randomizing the patterns, then the average at each neuron is\nthe same, so that we can drop the index i:\nmμ = 2-P X\nξ\nξμ sgn\nX\nν\nmν(ξν + aξν+1 + aξν-1)\n!\nLet's consider the stability of a pure pattern solution, for example m2 = 1 with all\nother overlaps zero. If a < 0.5, then the right hand side of the above equation is\nN\nX\ni\nξμ\ni sgn(ξ2\ni + aξ3\ni + aξ1\ni ) = δμ2\nin the N →inflimit, since sgn(ξ2\ni + aξ3\ni + aξ1\ni ) = ξ2\ni . If a > 0.5, then the situation\nis different. On average, ξ3\ni = ξ1\ni = -ξ2\ni for one quarter of the neurons, so that\nsgn(ξ2\ni + aξ3\ni + aξ1\ni ) = -ξ2\ni . Therefore the right hand side is 0.5(δμ1 + δμ2 + δμ3).\nWe can numerically solve by iterating the fixed point equations to a steady state.\nFor 0.5 < a < 1, this converges to a state with a radius of five nonzero overlaps. If\na > 1, the stable states have overlap with all stored patterns.\nSuppose that mμ is the vector of overlaps at a steady state. Then the steady state is\nof the form\nsi = sgn\nX\nμ\nmμ(ξμ\ni + aξμ+1\ni\n+ aξμ-1\ni\n)\n!\nSo if each attractor has overlap with several patterns, then the attractors associated with\ndifferent patterns must also overlap.\nAssociative memory with sparse patterns\nThe ±1 symmetry in the Hopfield model is rather artificial. The activity patterns in\nthe brain are generally sparse. That is, the number of active neurons is much smaller\n\nthan the total number of neurons. To treat the case of sparse patterns, it is convenient\nto switch to si = 0 or 1, and write the dynamics as\nsi(t + 1) = H\n\nX\nj\nWijsj(t) -θ\n\n(1)\nwhere θ is a threshold, H is the Heaviside step function, and Wij are the synaptic\nweights.\nLet P random patterns be given, ξμ\ni with μ = 1 to P. Each component is one\nwith probability f and zero with probability 1 -f. How should W be chosen so as to\nembed the patterns as attractors of the network dynamics? A number of Hebbian rules\nare considered below.\nCovariance rule\nThe covariance rule is\nWij =\nNf(1 -f)\nX\nμ\n(ξμ\ni -f)(ξμ\nj -f)\nBefore we go into a detailed analysis of the covariance rule, let's obtain some rough\nintuition as to why it works. The basic reason is that the formula\nX\nj\nWijξμ\nj ≈ξμ\ni -f\nis approximately true. If this formula were exactly true, ξμ would be a steady state of\nthe dynamics (1), provided that the threshold θ were set somewhere between 1 -f and\n-f.\nThis approximate formula is true in turn, provided that the following \"orthogonal-\nity\" condition is approximately satisfied:\nNf(1 -f)\nX\nj\n(ξμ\nj -f)ξν\nj ≈δμν\nThe μ = ν term is small because ξμ\nj -f is a zero mean random variable, and uncorre-\nlated with ξν\nj . The approximation becomes better as N →infwith f held fixed.\nNow let's do a calculation to see when the above approximations are accurate. In\nthe following we'll take the limits N →infand P →infwith their ratio α = P/N\nheld fixed.\nX\nj\nWijξν\nj\n≈\nNf(1 -f)\nX\nμ\n(ξμ\ni -f)\nX\nj,j=i\n(ξμ\nj -f)ξν\nj\n(2)\n≈\n(ξν\ni -f) +\nNf(1 -f)\nX\nμ,μ=ν\n(ξμ\ni -f)\nX\nj,j=i\n(ξμ\nj -f)ξν\nj\n(3)\n\nThe first term is the \"signal,\" while the second term is the \"noise.\" If the noise term\nwere zero, ξν would be a steady state for any threshold θ between 1 -f and -f.\nThis will still be true with high likelihood if the noise is much smaller than the signal.\nTherefore it is important to estimate the size of the noise.\nThe noise term has zero mean and variance\nσ2 = αf\nSince the signal is order unity, we expect that αf ≪1 should be a sufficient condition\nfor faithful storage. In other words, we estimate the capacity of the system to be αc ∼\n1/f.\nA more complicated calculation gives the result\nαc ≈\n2f| log f|\nThis has a logarithmic correction relative to the result derived by our simpler argument.\nThe capacity increases as the patterns become more sparse (f →0). This makes\nsense, as each pattern contains less information as f decreases.\nSeparating excitation and inhibition\nThe covariance rule is mathematically nice, but does not implement the biological con-\nstraint that excitatory and inhibitory neurons are distinct. An alternative is\nWij =\nNf(1 -f)\nX\nμ\n(ξμ\ni ξμ\nj -f 2)\nThis can be implemented by a network of N excitatory neurons and a single global\ninhibitory neuron."
    },
    {
      "category": "Resource",
      "title": "hss_spectrum2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-29j-introduction-to-computational-neuroscience-spring-2004/184d7272a8bf6f1a33165d05feb118d9_hss_spectrum2.pdf",
      "content": "MIT Department of Brain and Cognitive Sciences\n9.29J, Spring 2004 - Introduction to Computational Neuroscience\nInstructor: Professor Sebastian Seung\nMatching and maximizing are two ends of a\nspectrum of policy search algorithms\nJanuary 2, 2004\nAbstract\nAccording to the matching law, when an animal makes many repeated choices\nbetween alternatives, its preferences are in the ratio of the incomes derived from\nthe alternatives. Because matching behavior does not maximize reward, it has\nbeen difficult to explain using optimal foraging theory or rational choice theory.\nHere I show that matching and maximizing can be regarded as two ends of a spec\ntrum of policy search algorithms from reinforcement learning. The algorithms are\nparametrized by the time horizon within which past choices are correlated with\npresent reward. Maximization corresponds to the case of a long time horizon,\nwhile matching corresponds to a short horizon. From this viewpoint, matching is\nan approximation to maximizing, with the advantage of faster learning and more\nrobust performance in nonstationary environments. Between these two ends of the\nspectrum lie many strategies intermediate between matching and maximizing.\nIf an animal's relative preferences for alternatives are in the ratio of the incomes\nderived from them, then its behavior is said to be \"matching.\" Matching behavior has\nbeen observed for certain types of reinforcement schedules, in particular those that\nrandomize the interval between reward. The matching law was important because it\ngave the law of effect a quantitative formulation.\nGiven the matching law as an empirical observation about behavior, two questions\nimmediately come to mind. The first question is functional: why is matching a good\npolicy for animals to follow? The second is mechanistic: what neural mechanisms un\nderlie the production of matching behavior? This note mainly addresses the first ques\ntion, by elucidating the function of matching from the viewpoint of the mathematical\ntheory of reinforcement learning. However, the second question is also peripherally ad\ndressed through mathematical developments that are shared by recent neural network\nmodels of matching behavior.\nOne of the most common ways to explain the function of a behavior is to argue that\nit has been adapted by evolution to be optimal. Such an explanation for matching has\nbeen elusive, because matching does not generally maximize the animal's overall rate\nof reward. In this respect, the matching law is suboptimal. This enables it to be used as\nan explanation for \"irrational\" human behaviors, such as addiction and other behaviors\nattributed to lack of \"selfcontrol.\" Nevertheless, it would be hasty to completely reject\noptimality as an explanation of matching. Often matching is close to optimal, even if it\nis not exactly so.\n\nX\nX\nThe purpose of this note is to point out that matching and maximizing are two ends\nof a spectrum of behaviors generated by the REINFORCE class of machine learning\nalgorithms. Such algorithms learn by correlating past actions with reward, through\nan eligibility trace that maintains a memory of past actions. The time constant of\nthe eligibility trace is an important parameter of the learner. If the time constant is\nsufficiently long, then the learner converges to maximizing behavior. On the other\nhand, if the time constant is extremely short, then the learner converges to matching\nbehavior. This type of behavior is similar to the melioration model of Herrnstein,\nPrelec, and Vaughan [].\nWhy would a learner pursue a strategy that converges to matching rather than max\nimizing? In general, a biasvariance tradeoff is inherent to REINFORCE algorithms.\nThey work by computing a stochastic approximation to the gradient of the expected\nreward. When the time constant of the eligibility trace is long, the bias of the gradient\nestimate is small, but the variance is large. Reducing the time constant of the eligibility\ntrace increases the bias but decreases the variance, and can therefore be advantageous\nfor fast learning and robust performance in nonstationary environments.\n1 The matching law\nSuppose that an animal is presented with repeated choices between two actions. For\nsimplicity, it's assumed that the choices occur in discrete trials, though many matching\nexperiments have been performed using continuous time. This section defines some\nnotation, and uses it to express the matching law in mathematical form.\nThe action taken in trial t is indicated by the binary variables at and at, which\nsatisfy at + at = 1. After each action, the animal receives reward ht. The average\nincome derived from action a is\nT\nH =\nhtat\nT t=1\nSimilarly, the average income derived from action a is\nT\n\nH =\nhtat\nT t=1\nThe frequencies of actions a and a are respectively\nf = 1\nT\nT\nX\nat\n\nf = 1\nT\nT\nX\nat\nt=1\nt=1\nand satisfy f +\nf = 1.\nAccording to the matching law, the frequencies are in the same ratio as the incomes,\n\nH\nH\n=\nf\nf\n\nX\nEquivalently, the matching law can be stated as equality of the returns from the two\nactions, where the return from action a is defined as H/f , the reward averaged only\nover those trials in which action a was chosen.\n2 Matching is not equivalent to maximizing\nIn the field of reinforcement learning, a method of choosing actions is called a policy.\nIs matching an optimal policy?\nTo address this question, we consider the class of policies in which actions a and a\nare chosen with probabilities p and p respectively, with p + p = 1. Each trial's action is\nassumed to be statistically independent of previous trials. This case is easy to analyze;\ngeneralization to more complex policies is left to the technical appendix.\n\nFor the policy indexed by p, we define H(p) and H(p) as the incomes derived from\nthe two actions, averaged over time. It is assumed that the reward process has some\nsort of statistical stationarity, so that these time averages are welldefined.\nThe total average income is given by the sum H(p)+\nH(p), and the optimal policy\nis found by maximizing with respect to p. Assuming smoothness, the optimal p should\nbe a stationary point of the sum, assuming that it is strictly between 0 and 1. This\nmeans that\n\ndH\ndH\ndH\n=\ndp\n- dp = dp\nIn other words, the optimal p is given by the condition that the marginal incomes\n\ndH/dp and dH/dp are equal. In general, this is not the same as the matching law,\n\nwhich is the condition that the returns H/p and H/p are equal. In short, matching is\nnot typically the same as maximizing.\n3 REINFORCE learning\n\nSuppose that the learner does not know the functions H(p) and H(p), so it cannot\ncompute the optimal p directly. Based on observations of its actions and rewards, how\ncan the learner find the optimal p? One method is provided by the REINFORCE class\nof learning rules, which are an important class of policy search algorithms.\nDefine the eligibility at trial t to be\n\net = p tat - ptat\n(1)\nThis is positive for action a and negative for action a, with zero mean.\nThe eligibility trace is a shortterm memory of recent eligibilities, and can be de\nfined in various ways. For example, it could be the sum of recent eligibilities,\nτc\neˆt =\net-τ\nτ =0\nThis will be called \"sharp discounting,\" because of the sharp cutoff at τ = τc. This\nparameter will be called the consequential horizon, because it determines the temporal\nrange over which the learner correlates actions with their consequences.\n\nX\nOr the eligibility trace could be the infinite series\ninf\neˆt =\nβτ et-τ\nτ =0\nwhere the discount factor 0 ≤β < 1 makes past eligibilities count less. This exponen\ntially discounted eligibility trace can be computed in an \"online\" way by\neˆt = ˆet-1 + βet\nFor either type of discounting, the learning rule is\nΔqt = ηhteˆt\nwhere η > 0 is the learning rate, and the log odds q = log(p/p ) is the learned param\neter. This is related to p by the monotone increasing function, p = 1/(1 + exp(-q)).\nThe choice of q as the learned parameter is advantageous for a number of reasons. One\nis that it implements the constraint that p must lie between 0 and 1.\n4 How it works\nFor an intuitive explanation of how the learning rule works, consider the case of sharp\ndiscounting. If the probability p is fixed, then the eligibility trace eˆ is equal to (τc +\n1)(f -p), where f is the frequency with which action a was chosen in the last τc + 1\ntrials. This is the deviation between the actual and expected number of a actions.\nSuppose that positive reward is received. If action a was chosen more times than ex\npected, then p is increased. If action a was chosen fewer times than expected, then\np is decreased. Intuitively, by monitoring how reward is correlated to fluctuations in\nthe frequency of action a, the learner is sensitive to the marginal incomes, and such\nsensitivity is necessary for finding the optimal policy.\n5 Maximizing behavior\nThere are some theoretical assurances that REINFORCE learning rules do indeed max\nimize average reward, though these assurances are not unconditional guarantees. For\nexample, Baxter and Bartlett have proved the following result []. Suppose that the\nworld plus learner forms an ergodic Markov chain. If q is held fixed. then the time\n\naverage of hteˆt is approximately equal to the derivative of the average income H + H\nwith respect to p. This implies that the learning rule is driven by a stochastic approx\nimation to the gradient, and therefore tends to change q in the direction that increases\nthe average income.\nThe gradient approximation contains two types of error, systematic bias and ran\ndom variance. The bias vanishes in the limit of β →1 or τc →inf, which corresponds\nto an eligibility trace with infinite time scale. However, the variance diverges in this\nlimit. Therefore it is best to use a finite time scale, to reduce the variance at the ex\npense of increasing the bias. The bias will be small, as long as the time scale is much\n\nlonger than the mixing time of the Markov chain. Reducing variance has the effect of\nspeeding up the initial stages of learning, but makes the final policy suboptimal.\n6 Matching behavior\nThe previous section discussed the case of a long time scale for the eligibility trace.\nThe opposite extreme is to make this time scale as short as possible, β = 0 or τc = 0.\nThen the eligibility trace is equal to the present eligibility, eˆt = et, and the learning\nrule takes the form\nΔq = ηhtet\n(2)\nptat - ptat)\n(3)\n= ηht(\n\n= ηht(at - pt)\n(4)\nSuppose that the learning rule approaches some stationary probability density as time\nincreases. In the limit of small η, this stationary density will be concentrated around a\nvalue of q satisfying hhteti = 0. Substituting the expression (1) yields\n\nhhtati\nhhtati\n=\np\np\n\nThe quantities hhtati and hhtati are the average incomes H and H. They are in the\nsame ratio as the action probabilities, which is precisely the matching law.\nThere are some equivalent ways of writing the learning rule (2). For example,\nsuppose that the learner maintains two numbers z and z , which are unnormalized prob\nabilities, so that the choice probabilities are\nz\nz\np =\np =\nz +\nz +\nz\nz\nThen the learning rule can be written as a multiplicative update\nzt+1 = zt exp(ηp thtat)\nz t+1 =\nat)\nzt exp(ptht\nTo prove this, simply compute the log odds q = log(z/z )\nAlternatively, suppose that the learner maintains two numbers u and u, determining\nthe choice probabilities via\nu\nu\ne\ne\np =\nu\np = eu + e\neu + e\nu\nThen the learning rule can be written as an additive update\nΔut = ηp thtat\nΔ\nat\nut = ηptht\nA modification of (2) is to make the update directly in p, rather than in the log odds\nq.\nΔpt = ηhtet = ηht(at - pt)\nIf ht, η ≤ 1, then this respects the constraint 0 < p < 1. Then the above additive\nlearning rule holds with\nu\np = u -\n\nP\nP\nP\n7 Questions for further research\nThe two extreme cases τc = 0 and τc →infwere considered above. What happens for\nintermediate τc? For example, suppose that τc = 1. Then the learning rule is\nΔθ = ηht(et + et-1)\nThe stationary point satisfies hhteti + hhtet-1i = 0, which implies that\n\nhhtati + hhtat-1i\nhhtati + hhtat-1i\n=\np\np\nThis deviates from matching. Can we characterize exactly how?\nSuppose that the actions are generated by a Markov chain, instead of being statisti\ncally independent from previous trials. How does these results generalize?\nA REINFORCE for n actions\nFor simplicity, the main text assumed that there are only two possible actions, and that\nthe learned parameter was the log odds of the two actions. More generally, there could\nbe n actions, and the probability vector might be parametrized in some other way.\nSuppose that action ai has probability pi, where i runs from 1 to n. The probability\nvector is parametrized by the mdimensional vector θ. Let r denote the gradient with\nrespect to θ. Then the eligibility is defined by\nX ai\nt\net =\npi rp i\ni\nand the eligibility trace eˆt is similar to before. The REINFORCE learning rule is\nΔθ = ηhteˆt\nwhere η > 0 is the learning rate and ht is the reward in trial t.\nB Matching behavior for n actions\ni\nDefine the m × n matrix Aαi = rαp . We'll have to assume that the rank of this\nmatrix is n -1, in order to derive matching behavior. This assumption is important,\nbecause it guarantees that any vector vi satisfying\ni Aαivi = 0 is proportional to\nthe vector of all ones. To see this, note that\ni Aαi = 0 follows from differentiation\nof the identity\ni pi = 1, and apply the fundamental theorem of linear algebra. The\nassumption should hold generically, provided that θ contains n-1 or more parameters.\nSuppose that the learning rule depends only on the present eligibility,\nΔθ = ηhtet\n\nThen a stationary point satisfies hhteti = 0, or\nX Hi\npi rp i = 0\ni\ni\nwhere Hi = hhtati is the average income derived from action i. Therefore the vector\nHi/pi should be proportional to the vector of all ones, which is the matching law."
    }
  ]
}